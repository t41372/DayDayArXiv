[
  {
    "arxiv_id": "2401.03609v3",
    "title": "Multi-Modal Federated Learning for Cancer Staging over Non-IID Datasets with Unbalanced Modalities",
    "authors": [
      "Kasra Borazjani",
      "Naji Khosravan",
      "Leslie Ying",
      "Seyyedali Hosseinalipour"
    ],
    "abstract": "The use of machine learning (ML) for cancer staging through medical image\nanalysis has gained substantial interest across medical disciplines. When\naccompanied by the innovative federated learning (FL) framework, ML techniques\ncan further overcome privacy concerns related to patient data exposure. Given\nthe frequent presence of diverse data modalities within patient records,\nleveraging FL in a multi-modal learning framework holds considerable promise\nfor cancer staging.\n  However, existing works on multi-modal FL often presume that all\ndata-collecting institutions have access to all data modalities. This\noversimplified approach neglects institutions that have access to only a\nportion of data modalities within the system. In this work, we introduce a\nnovel FL architecture designed to accommodate not only the heterogeneity of\ndata samples, but also the inherent heterogeneity/non-uniformity of data\nmodalities across institutions. We shed light on the challenges associated with\nvarying convergence speeds observed across different data modalities within our\nFL system. Subsequently, we propose a solution to tackle these challenges by\ndevising a distributed gradient blending and proximity-aware client weighting\nstrategy tailored for multi-modal FL. To show the superiority of our method, we\nconduct experiments using The Cancer Genome Atlas program (TCGA) datalake\nconsidering different cancer types and three modalities of data: mRNA\nsequences, histopathological image data, and clinical information. Our results\nfurther unveil the impact and severity of class-based vs type-based\nheterogeneity across institutions on the model performance, which widens the\nperspective to the notion of data heterogeneity in multi-modal FL literature.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in IEEE Transactions on Medical Imaging (TMI), DOI:\n  https://doi.org/10.1109/TMI.2024.3450855",
    "pdf_url": "http://arxiv.org/pdf/2401.03609v3",
    "published_date": "2024-01-07 23:45:01 UTC",
    "updated_date": "2024-10-08 14:52:18 UTC"
  },
  {
    "arxiv_id": "2401.03605v1",
    "title": "ChatGPT for Conversational Recommendation: Refining Recommendations by Reprompting with Feedback",
    "authors": [
      "Kyle Dylan Spurlock",
      "Cagla Acun",
      "Esin Saka",
      "Olfa Nasraoui"
    ],
    "abstract": "Recommendation algorithms have been pivotal in handling the overwhelming\nvolume of online content. However, these algorithms seldom consider direct user\ninput, resulting in superficial interaction between them. Efforts have been\nmade to include the user directly in the recommendation process through\nconversation, but these systems too have had limited interactivity. Recently,\nLarge Language Models (LLMs) like ChatGPT have gained popularity due to their\nease of use and their ability to adapt dynamically to various tasks while\nresponding to feedback. In this paper, we investigate the effectiveness of\nChatGPT as a top-n conversational recommendation system. We build a rigorous\npipeline around ChatGPT to simulate how a user might realistically probe the\nmodel for recommendations: by first instructing and then reprompting with\nfeedback to refine a set of recommendations. We further explore the effect of\npopularity bias in ChatGPT's recommendations, and compare its performance to\nbaseline models. We find that reprompting ChatGPT with feedback is an effective\nstrategy to improve recommendation relevancy, and that popularity bias can be\nmitigated through prompt engineering.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "I.2.7; H.3.3"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03605v1",
    "published_date": "2024-01-07 23:17:42 UTC",
    "updated_date": "2024-01-07 23:17:42 UTC"
  },
  {
    "arxiv_id": "2401.03601v1",
    "title": "InFoBench: Evaluating Instruction Following Ability in Large Language Models",
    "authors": [
      "Yiwei Qin",
      "Kaiqiang Song",
      "Yebowen Hu",
      "Wenlin Yao",
      "Sangwoo Cho",
      "Xiaoyang Wang",
      "Xuansheng Wu",
      "Fei Liu",
      "Pengfei Liu",
      "Dong Yu"
    ],
    "abstract": "This paper introduces the Decomposed Requirements Following Ratio (DRFR), a\nnew metric for evaluating Large Language Models' (LLMs) ability to follow\ninstructions. Addressing a gap in current methodologies, DRFR breaks down\ncomplex instructions into simpler criteria, facilitating a detailed analysis of\nLLMs' compliance with various aspects of tasks. Alongside this metric, we\npresent InFoBench, a benchmark comprising 500 diverse instructions and 2,250\ndecomposed questions across multiple constraint categories. Our experiments\ncompare DRFR with traditional scoring methods and explore annotation sources,\nincluding human experts, crowd-sourced workers, and GPT-4. The findings\ndemonstrate DRFR's higher reliability and the effectiveness of using GPT-4 as a\ncost-efficient annotator. The evaluation of several advanced LLMs using this\nframework reveals their strengths and areas needing improvement, particularly\nin complex instruction-following. This study contributes a novel metric and\nbenchmark, offering insights for future LLM development and evaluation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03601v1",
    "published_date": "2024-01-07 23:01:56 UTC",
    "updated_date": "2024-01-07 23:01:56 UTC"
  },
  {
    "arxiv_id": "2401.03599v1",
    "title": "Disentangled Neural Relational Inference for Interpretable Motion Prediction",
    "authors": [
      "Victoria M. Dax",
      "Jiachen Li",
      "Enna Sachdeva",
      "Nakul Agarwal",
      "Mykel J. Kochenderfer"
    ],
    "abstract": "Effective interaction modeling and behavior prediction of dynamic agents play\na significant role in interactive motion planning for autonomous robots.\nAlthough existing methods have improved prediction accuracy, few research\nefforts have been devoted to enhancing prediction model interpretability and\nout-of-distribution (OOD) generalizability. This work addresses these two\nchallenging aspects by designing a variational auto-encoder framework that\nintegrates graph-based representations and time-sequence models to efficiently\ncapture spatio-temporal relations between interactive agents and predict their\ndynamics. Our model infers dynamic interaction graphs in a latent space\naugmented with interpretable edge features that characterize the interactions.\nMoreover, we aim to enhance model interpretability and performance in OOD\nscenarios by disentangling the latent space of edge features, thereby\nstrengthening model versatility and robustness. We validate our approach\nthrough extensive experiments on both simulated and real-world datasets. The\nresults show superior performance compared to existing methods in modeling\nspatio-temporal relations, motion prediction, and identifying time-invariant\nlatent features.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03599v1",
    "published_date": "2024-01-07 22:49:24 UTC",
    "updated_date": "2024-01-07 22:49:24 UTC"
  },
  {
    "arxiv_id": "2401.03597v3",
    "title": "Few-Shot Causal Representation Learning for Out-of-Distribution Generalization on Heterogeneous Graphs",
    "authors": [
      "Pengfei Ding",
      "Yan Wang",
      "Guanfeng Liu",
      "Nan Wang",
      "Xiaofang Zhou"
    ],
    "abstract": "Heterogeneous graph few-shot learning (HGFL) has been developed to address\nthe label sparsity issue in heterogeneous graphs (HGs), which consist of\nvarious types of nodes and edges. The core concept of HGFL is to extract\nknowledge from rich-labeled classes in a source HG, transfer this knowledge to\na target HG to facilitate learning new classes with few-labeled training data,\nand finally make predictions on unlabeled testing data. Existing methods\ntypically assume that the source HG, training data, and testing data all share\nthe same distribution. However, in practice, distribution shifts among these\nthree types of data are inevitable due to two reasons: (1) the limited\navailability of the source HG that matches the target HG distribution, and (2)\nthe unpredictable data generation mechanism of the target HG. Such distribution\nshifts result in ineffective knowledge transfer and poor learning performance\nin existing methods, thereby leading to a novel problem of out-of-distribution\n(OOD) generalization in HGFL. To address this challenging problem, we propose a\nnovel Causal OOD Heterogeneous graph Few-shot learning model, namely COHF. In\nCOHF, we first characterize distribution shifts in HGs with a structural causal\nmodel, establishing an invariance principle for OOD generalization in HGFL.\nThen, following this invariance principle, we propose a new variational\nautoencoder-based heterogeneous graph neural network to mitigate the impact of\ndistribution shifts. Finally, by integrating this network with a novel\nmeta-learning framework, COHF effectively transfers knowledge to the target HG\nto predict new classes with few-labeled data. Extensive experiments on seven\nreal-world datasets have demonstrated the superior performance of COHF over the\nstate-of-the-art methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03597v3",
    "published_date": "2024-01-07 22:47:38 UTC",
    "updated_date": "2024-04-16 04:36:18 UTC"
  },
  {
    "arxiv_id": "2401.03587v1",
    "title": "Big Data and Deep Learning in Smart Cities: A Comprehensive Dataset for AI-Driven Traffic Accident Detection and Computer Vision Systems",
    "authors": [
      "Victor Adewopo",
      "Nelly Elsayed",
      "Zag Elsayed",
      "Murat Ozer",
      "Constantinos Zekios",
      "Ahmed Abdelgawad",
      "Magdy Bayoumi"
    ],
    "abstract": "In the dynamic urban landscape, where the interplay of vehicles and\npedestrians defines the rhythm of life, integrating advanced technology for\nsafety and efficiency is increasingly crucial. This study delves into the\napplication of cutting-edge technological methods in smart cities, focusing on\nenhancing public safety through improved traffic accident detection. Action\nrecognition plays a pivotal role in interpreting visual data and tracking\nobject motion such as human pose estimation in video sequences. The challenges\nof action recognition include variability in rapid actions, limited dataset,\nand environmental factors such as (Weather, Illumination, and Occlusions). In\nthis paper, we present a novel comprehensive dataset for traffic accident\ndetection. This datasets is specifically designed to bolster computer vision\nand action recognition systems in predicting and detecting road traffic\naccidents. We integrated datasets from wide variety of data sources, road\nnetworks, weather conditions, and regions across the globe. This approach is\nunderpinned by empirical studies, aiming to contribute to the discourse on how\ntechnology can enhance the quality of life in densely populated areas. This\nresearch aims to bridge existing research gaps by introducing benchmark\ndatasets that leverage state-of-the-art algorithms tailored for traffic\naccident detection in smart cities. These dataset is expected to advance\nacademic research and also enhance real-time accident detection applications,\ncontributing significantly to the evolution of smart urban environments. Our\nstudy marks a pivotal step towards safer, more efficient smart cities,\nharnessing the power of AI and machine learning to transform urban living.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03587v1",
    "published_date": "2024-01-07 21:50:24 UTC",
    "updated_date": "2024-01-07 21:50:24 UTC"
  },
  {
    "arxiv_id": "2401.03581v1",
    "title": "Evaluating and Personalizing User-Perceived Quality of Text-to-Speech Voices for Delivering Mindfulness Meditation with Different Physical Embodiments",
    "authors": [
      "Zhonghao Shi",
      "Han Chen",
      "Anna-Maria Velentza",
      "Siqi Liu",
      "Nathaniel Dennler",
      "Allison O'Connell",
      "Maja Matarić"
    ],
    "abstract": "Mindfulness-based therapies have been shown to be effective in improving\nmental health, and technology-based methods have the potential to expand the\naccessibility of these therapies. To enable real-time personalized content\ngeneration for mindfulness practice in these methods, high-quality\ncomputer-synthesized text-to-speech (TTS) voices are needed to provide verbal\nguidance and respond to user performance and preferences. However, the\nuser-perceived quality of state-of-the-art TTS voices has not yet been\nevaluated for administering mindfulness meditation, which requires emotional\nexpressiveness. In addition, work has not yet been done to study the effect of\nphysical embodiment and personalization on the user-perceived quality of TTS\nvoices for mindfulness. To that end, we designed a two-phase human subject\nstudy. In Phase 1, an online Mechanical Turk between-subject study (N=471)\nevaluated 3 (feminine, masculine, child-like) state-of-the-art TTS voices with\n2 (feminine, masculine) human therapists' voices in 3 different physical\nembodiment settings (no agent, conversational agent, socially assistive robot)\nwith remote participants. Building on findings from Phase 1, in Phase 2, an\nin-person within-subject study (N=94), we used a novel framework we developed\nfor personalizing TTS voices based on user preferences, and evaluated\nuser-perceived quality compared to best-rated non-personalized voices from\nPhase 1. We found that the best-rated human voice was perceived better than all\nTTS voices; the emotional expressiveness and naturalness of TTS voices were\npoorly rated, while users were satisfied with the clarity of TTS voices.\nSurprisingly, by allowing users to fine-tune TTS voice features, the\nuser-personalized TTS voices could perform almost as well as human voices,\nsuggesting user personalization could be a simple and very effective tool to\nimprove user-perceived quality of TTS voice.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.HC",
    "comment": "Published in Proceedings of the 2023 ACM/IEEE International\n  Conference on Human-Robot Interaction, pp. 516-524. 2023",
    "pdf_url": "http://arxiv.org/pdf/2401.03581v1",
    "published_date": "2024-01-07 21:14:32 UTC",
    "updated_date": "2024-01-07 21:14:32 UTC"
  },
  {
    "arxiv_id": "2401.03568v2",
    "title": "Agent AI: Surveying the Horizons of Multimodal Interaction",
    "authors": [
      "Zane Durante",
      "Qiuyuan Huang",
      "Naoki Wake",
      "Ran Gong",
      "Jae Sung Park",
      "Bidipta Sarkar",
      "Rohan Taori",
      "Yusuke Noda",
      "Demetri Terzopoulos",
      "Yejin Choi",
      "Katsushi Ikeuchi",
      "Hoi Vo",
      "Li Fei-Fei",
      "Jianfeng Gao"
    ],
    "abstract": "Multi-modal AI systems will likely become a ubiquitous presence in our\neveryday lives. A promising approach to making these systems more interactive\nis to embody them as agents within physical and virtual environments. At\npresent, systems leverage existing foundation models as the basic building\nblocks for the creation of embodied agents. Embedding agents within such\nenvironments facilitates the ability of models to process and interpret visual\nand contextual data, which is critical for the creation of more sophisticated\nand context-aware AI systems. For example, a system that can perceive user\nactions, human behavior, environmental objects, audio expressions, and the\ncollective sentiment of a scene can be used to inform and direct agent\nresponses within the given environment. To accelerate research on agent-based\nmultimodal intelligence, we define \"Agent AI\" as a class of interactive systems\nthat can perceive visual stimuli, language inputs, and other\nenvironmentally-grounded data, and can produce meaningful embodied actions. In\nparticular, we explore systems that aim to improve agents based on\nnext-embodied action prediction by incorporating external knowledge,\nmulti-sensory inputs, and human feedback. We argue that by developing agentic\nAI systems in grounded environments, one can also mitigate the hallucinations\nof large foundation models and their tendency to generate environmentally\nincorrect outputs. The emerging field of Agent AI subsumes the broader embodied\nand agentic aspects of multimodal interactions. Beyond agents acting and\ninteracting in the physical world, we envision a future where people can easily\ncreate any virtual reality or simulated scene and interact with agents embodied\nwithin the virtual environment.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03568v2",
    "published_date": "2024-01-07 19:11:18 UTC",
    "updated_date": "2024-01-25 21:20:27 UTC"
  },
  {
    "arxiv_id": "2401.03562v2",
    "title": "GLOCALFAIR: Jointly Improving Global and Local Group Fairness in Federated Learning",
    "authors": [
      "Syed Irfan Ali Meerza",
      "Luyang Liu",
      "Jiaxin Zhang",
      "Jian Liu"
    ],
    "abstract": "Federated learning (FL) has emerged as a prospective solution for\ncollaboratively learning a shared model across clients without sacrificing\ntheir data privacy. However, the federated learned model tends to be biased\nagainst certain demographic groups (e.g., racial and gender groups) due to the\ninherent FL properties, such as data heterogeneity and party selection. Unlike\ncentralized learning, mitigating bias in FL is particularly challenging as\nprivate training datasets and their sensitive attributes are typically not\ndirectly accessible. Most prior research in this field only focuses on global\nfairness while overlooking the local fairness of individual clients. Moreover,\nexisting methods often require sensitive information about the client's local\ndatasets to be shared, which is not desirable. To address these issues, we\npropose GLOCALFAIR, a client-server co-design fairness framework that can\njointly improve global and local group fairness in FL without the need for\nsensitive statistics about the client's private datasets. Specifically, we\nutilize constrained optimization to enforce local fairness on the client side\nand adopt a fairness-aware clustering-based aggregation on the server to\nfurther ensure the global model fairness across different sensitive groups\nwhile maintaining high utility. Experiments on two image datasets and one\ntabular dataset with various state-of-the-art fairness baselines show that\nGLOCALFAIR can achieve enhanced fairness under both global and local data\ndistributions while maintaining a good level of utility and client fairness.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03562v2",
    "published_date": "2024-01-07 18:10:14 UTC",
    "updated_date": "2024-10-02 21:13:27 UTC"
  },
  {
    "arxiv_id": "2401.03552v1",
    "title": "Privacy-Preserving in Blockchain-based Federated Learning Systems",
    "authors": [
      "Sameera K. M.",
      "Serena Nicolazzo",
      "Marco Arazzi",
      "Antonino Nocera",
      "Rafidha Rehiman K. A.",
      "Vinod P",
      "Mauro Conti"
    ],
    "abstract": "Federated Learning (FL) has recently arisen as a revolutionary approach to\ncollaborative training Machine Learning models. According to this novel\nframework, multiple participants train a global model collaboratively,\ncoordinating with a central aggregator without sharing their local data. As FL\ngains popularity in diverse domains, security, and privacy concerns arise due\nto the distributed nature of this solution. Therefore, integrating this\nstrategy with Blockchain technology has been consolidated as a preferred choice\nto ensure the privacy and security of participants.\n  This paper explores the research efforts carried out by the scientific\ncommunity to define privacy solutions in scenarios adopting Blockchain-Enabled\nFL. It comprehensively summarizes the background related to FL and Blockchain,\nevaluates existing architectures for their integration, and the primary attacks\nand possible countermeasures to guarantee privacy in this setting. Finally, it\nreviews the main application scenarios where Blockchain-Enabled FL approaches\nhave been proficiently applied. This survey can help academia and industry\npractitioners understand which theories and techniques exist to improve the\nperformance of FL through Blockchain to preserve privacy and which are the main\nchallenges and future directions in this novel and still under-explored\ncontext. We believe this work provides a novel contribution respect to the\nprevious surveys and is a valuable tool to explore the current landscape,\nunderstand perspectives, and pave the way for advancements or improvements in\nthis amalgamation of Blockchain and Federated Learning.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "44 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.03552v1",
    "published_date": "2024-01-07 17:23:55 UTC",
    "updated_date": "2024-01-07 17:23:55 UTC"
  },
  {
    "arxiv_id": "2401.03546v1",
    "title": "NovelGym: A Flexible Ecosystem for Hybrid Planning and Learning Agents Designed for Open Worlds",
    "authors": [
      "Shivam Goel",
      "Yichen Wei",
      "Panagiotis Lymperopoulos",
      "Klara Chura",
      "Matthias Scheutz",
      "Jivko Sinapov"
    ],
    "abstract": "As AI agents leave the lab and venture into the real world as autonomous\nvehicles, delivery robots, and cooking robots, it is increasingly necessary to\ndesign and comprehensively evaluate algorithms that tackle the ``open-world''.\nTo this end, we introduce NovelGym, a flexible and adaptable ecosystem designed\nto simulate gridworld environments, serving as a robust platform for\nbenchmarking reinforcement learning (RL) and hybrid planning and learning\nagents in open-world contexts. The modular architecture of NovelGym facilitates\nrapid creation and modification of task environments, including multi-agent\nscenarios, with multiple environment transformations, thus providing a dynamic\ntestbed for researchers to develop open-world AI agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at AAMAS-2024",
    "pdf_url": "http://arxiv.org/pdf/2401.03546v1",
    "published_date": "2024-01-07 17:13:28 UTC",
    "updated_date": "2024-01-07 17:13:28 UTC"
  },
  {
    "arxiv_id": "2401.03545v1",
    "title": "Is there really a Citation Age Bias in NLP?",
    "authors": [
      "Hoa Nguyen",
      "Steffen Eger"
    ],
    "abstract": "Citations are a key ingredient of scientific research to relate a paper to\nothers published in the community. Recently, it has been noted that there is a\ncitation age bias in the Natural Language Processing (NLP) community, one of\nthe currently fastest growing AI subfields, in that the mean age of the\nbibliography of NLP papers has become ever younger in the last few years,\nleading to `citation amnesia' in which older knowledge is increasingly\nforgotten. In this work, we put such claims into perspective by analyzing the\nbibliography of $\\sim$300k papers across 15 different scientific fields\nsubmitted to the popular preprint server Arxiv in the time period from 2013 to\n2022. We find that all AI subfields (in particular: cs.AI, cs.CL, cs.CV, cs.LG)\nhave similar trends of citation amnesia, in which the age of the bibliography\nhas roughly halved in the last 10 years (from above 12 in 2013 to below 7 in\n2022), on average. Rather than diagnosing this as a citation age bias in the\nNLP community, we believe this pattern is an artefact of the dynamics of these\nresearch fields, in which new knowledge is produced in ever shorter time\nintervals.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.DL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03545v1",
    "published_date": "2024-01-07 17:12:08 UTC",
    "updated_date": "2024-01-07 17:12:08 UTC"
  },
  {
    "arxiv_id": "2401.06789v1",
    "title": "Information Retrieval and Classification of Real-Time Multi-Source Hurricane Evacuation Notices",
    "authors": [
      "Tingting Zhao",
      "Shubo Tian",
      "Jordan Daly",
      "Melissa Geiger",
      "Minna Jia",
      "Jinfeng Zhang"
    ],
    "abstract": "For an approaching disaster, the tracking of time-sensitive critical\ninformation such as hurricane evacuation notices is challenging in the United\nStates. These notices are issued and distributed rapidly by numerous local\nauthorities that may spread across multiple states. They often undergo frequent\nupdates and are distributed through diverse online portals lacking standard\nformats. In this study, we developed an approach to timely detect and track the\nlocally issued hurricane evacuation notices. The text data were collected\nmainly with a spatially targeted web scraping method. They were manually\nlabeled and then classified using natural language processing techniques with\ndeep learning models. The classification of mandatory evacuation notices\nachieved a high accuracy (recall = 96%). We used Hurricane Ian (2022) to\nillustrate how real-time evacuation notices extracted from local government\nsources could be redistributed with a Web GIS system. Our method applied to\nfuture hurricanes provides live data for situation awareness to higher-level\ngovernment agencies and news media. The archived data helps scholars to study\ngovernment responses toward weather warnings and individual behaviors\ninfluenced by evacuation history. The framework may be applied to other types\nof disasters for rapid and targeted retrieval, classification, redistribution,\nand archiving of real-time government orders and notifications.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.06789v1",
    "published_date": "2024-01-07 16:35:30 UTC",
    "updated_date": "2024-01-07 16:35:30 UTC"
  },
  {
    "arxiv_id": "2401.03531v1",
    "title": "A Heterogeneous RISC-V based SoC for Secure Nano-UAV Navigation",
    "authors": [
      "Luca Valente",
      "Alessandro Nadalini",
      "Asif Veeran",
      "Mattia Sinigaglia",
      "Bruno Sa",
      "Nils Wistoff",
      "Yvan Tortorella",
      "Simone Benatti",
      "Rafail Psiakis",
      "Ari Kulmala",
      "Baker Mohammad",
      "Sandro Pinto",
      "Daniele Palossi",
      "Luca Benini",
      "Davide Rossi"
    ],
    "abstract": "The rapid advancement of energy-efficient parallel ultra-low-power (ULP)\nucontrollers units (MCUs) is enabling the development of autonomous nano-sized\nunmanned aerial vehicles (nano-UAVs). These sub-10cm drones represent the next\ngeneration of unobtrusive robotic helpers and ubiquitous smart sensors.\nHowever, nano-UAVs face significant power and payload constraints while\nrequiring advanced computing capabilities akin to standard drones, including\nreal-time Machine Learning (ML) performance and the safe co-existence of\ngeneral-purpose and real-time OSs. Although some advanced parallel ULP MCUs\noffer the necessary ML computing capabilities within the prescribed power\nlimits, they rely on small main memories (<1MB) and ucontroller-class CPUs with\nno virtualization or security features, and hence only support simple\nbare-metal runtimes. In this work, we present Shaheen, a 9mm2 200mW SoC\nimplemented in 22nm FDX technology. Differently from state-of-the-art MCUs,\nShaheen integrates a Linux-capable RV64 core, compliant with the v1.0 ratified\nHypervisor extension and equipped with timing channel protection, along with a\nlow-cost and low-power memory controller exposing up to 512MB of off-chip\nlow-cost low-power HyperRAM directly to the CPU. At the same time, it\nintegrates a fully programmable energy- and area-efficient multi-core cluster\nof RV32 cores optimized for general-purpose DSP as well as reduced- and\nmixed-precision ML. To the best of the authors' knowledge, it is the first\nsilicon prototype of a ULP SoC coupling the RV64 and RV32 cores in a\nheterogeneous host+accelerator architecture fully based on the RISC-V ISA. We\ndemonstrate the capabilities of the proposed SoC on a wide range of benchmarks\nrelevant to nano-UAV applications. The cluster can deliver up to 90GOp/s and up\nto 1.8TOp/s/W on 2-bit integer kernels and up to 7.9GFLOp/s and up to\n150GFLOp/s/W on 16-bit FP kernels.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03531v1",
    "published_date": "2024-01-07 16:03:47 UTC",
    "updated_date": "2024-01-07 16:03:47 UTC"
  },
  {
    "arxiv_id": "2401.03529v1",
    "title": "Quantifying stability of non-power-seeking in artificial agents",
    "authors": [
      "Evan Ryan Gunter",
      "Yevgeny Liokumovich",
      "Victoria Krakovna"
    ],
    "abstract": "We investigate the question: if an AI agent is known to be safe in one\nsetting, is it also safe in a new setting similar to the first? This is a core\nquestion of AI alignment--we train and test models in a certain environment,\nbut deploy them in another, and we need to guarantee that models that seem safe\nin testing remain so in deployment. Our notion of safety is based on\npower-seeking--an agent which seeks power is not safe. In particular, we focus\non a crucial type of power-seeking: resisting shutdown. We model agents as\npolicies for Markov decision processes, and show (in two cases of interest)\nthat not resisting shutdown is \"stable\": if an MDP has certain policies which\ndon't avoid shutdown, the corresponding policies for a similar MDP also don't\navoid shutdown. We also show that there are natural cases where safety is _not_\nstable--arbitrarily small perturbations may result in policies which never shut\ndown. In our first case of interest--near-optimal policies--we use a\nbisimulation metric on MDPs to prove that small perturbations won't make the\nagent take longer to shut down. Our second case of interest is policies for\nMDPs satisfying certain constraints which hold for various models (including\nlanguage models). Here, we demonstrate a quantitative bound on how fast the\nprobability of not shutting down can increase: by defining a metric on MDPs;\nproving that the probability of not shutting down, as a function on MDPs, is\nlower semicontinuous; and bounding how quickly this function decreases.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "37 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.03529v1",
    "published_date": "2024-01-07 15:57:38 UTC",
    "updated_date": "2024-01-07 15:57:38 UTC"
  },
  {
    "arxiv_id": "2401.04141v1",
    "title": "On The Potential of The Fractal Geometry and The CNNs Ability to Encode it",
    "authors": [
      "Julia El Zini",
      "Bassel Musharrafieh",
      "Mariette Awad"
    ],
    "abstract": "The fractal dimension provides a statistical index of object complexity by\nstudying how the pattern changes with the measuring scale. Although useful in\nseveral classification tasks, the fractal dimension is under-explored in deep\nlearning applications. In this work, we investigate the features that are\nlearned by deep models and we study whether these deep networks are able to\nencode features as complex and high-level as the fractal dimensions.\nSpecifically, we conduct a correlation analysis experiment to show that deep\nnetworks are not able to extract such a feature in none of their layers. We\ncombine our analytical study with a human evaluation to investigate the\ndifferences between deep learning networks and models that operate on the\nfractal feature solely. Moreover, we show the effectiveness of fractal features\nin applications where the object structure is crucial for the classification\ntask. We empirically show that training a shallow network on fractal features\nachieves performance comparable, even superior in specific cases, to that of\ndeep networks trained on raw data while requiring less computational resources.\nFractals improved the accuracy of the classification by 30% on average while\nrequiring up to 84% less time to train. We couple our empirical study with a\ncomplexity analysis of the computational cost of extracting the proposed\nfractal features, and we study its limitation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04141v1",
    "published_date": "2024-01-07 15:22:56 UTC",
    "updated_date": "2024-01-07 15:22:56 UTC"
  },
  {
    "arxiv_id": "2401.03512v3",
    "title": "CharPoet: A Chinese Classical Poetry Generation System Based on Token-free LLM",
    "authors": [
      "Chengyue Yu",
      "Lei Zang",
      "Jiaotuan Wang",
      "Chenyi Zhuang",
      "Jinjie Gu"
    ],
    "abstract": "Automatic Chinese classical poetry generation has attracted much research\ninterest, but achieving effective control over format and content\nsimultaneously remains challenging. Traditional systems usually accept keywords\nas user inputs, resulting in limited control over content. Large language\nmodels (LLMs) improve content control by allowing unrestricted user\ninstructions, but the token-by-token generation process frequently makes format\nerrors. Motivated by this, we propose CharPoet, a Chinese classical poetry\ngeneration system based on token-free LLM, which provides effective control\nover both format and content. Our token-free architecture generates in a\ncharacter-by-character manner, enabling precise control over the number of\ncharacters. Pruned from existing token-based LLMs, CharPoet inherits their\npretrained capabilities and can generate poetry following instructions like\n\"Write me a poem for my mother's birthday.\" CharPoet achieves format accuracy\nabove 0.96, outperforming Jiuge-GPT-2 (0.91) and GPT-4 (0.38). In terms of\ncontent quality, CharPoet surpasses traditional systems including Jiuge, and is\ncomparable to other LLMs. Our system is open source and available at\nhttps://modelscope.cn/models/CharPoet/CharPoet. A video demonstration of\nCharPoet is available at https://youtu.be/voZ25qEp3Dc.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03512v3",
    "published_date": "2024-01-07 15:00:36 UTC",
    "updated_date": "2024-03-20 07:39:48 UTC"
  },
  {
    "arxiv_id": "2401.03504v1",
    "title": "ClusterComm: Discrete Communication in Decentralized MARL using Internal Representation Clustering",
    "authors": [
      "Robert Müller",
      "Hasan Turalic",
      "Thomy Phan",
      "Michael Kölle",
      "Jonas Nüßlein",
      "Claudia Linnhoff-Popien"
    ],
    "abstract": "In the realm of Multi-Agent Reinforcement Learning (MARL), prevailing\napproaches exhibit shortcomings in aligning with human learning, robustness,\nand scalability. Addressing this, we introduce ClusterComm, a fully\ndecentralized MARL framework where agents communicate discretely without a\ncentral control unit. ClusterComm utilizes Mini-Batch-K-Means clustering on the\nlast hidden layer's activations of an agent's policy network, translating them\ninto discrete messages. This approach outperforms no communication and competes\nfavorably with unbounded, continuous communication and hence poses a simple yet\neffective strategy for enhancing collaborative task-solving in MARL.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at ICAART 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.03504v1",
    "published_date": "2024-01-07 14:53:43 UTC",
    "updated_date": "2024-01-07 14:53:43 UTC"
  },
  {
    "arxiv_id": "2401.03499v1",
    "title": "Re:Draw -- Context Aware Translation as a Controllable Method for Artistic Production",
    "authors": [
      "Joao Liborio Cardoso",
      "Francesco Banterle",
      "Paolo Cignoni",
      "Michael Wimmer"
    ],
    "abstract": "We introduce context-aware translation, a novel method that combines the\nbenefits of inpainting and image-to-image translation, respecting\nsimultaneously the original input and contextual relevance -- where existing\nmethods fall short. By doing so, our method opens new avenues for the\ncontrollable use of AI within artistic creation, from animation to digital art.\n  As an use case, we apply our method to redraw any hand-drawn animated\ncharacter eyes based on any design specifications - eyes serve as a focal point\nthat captures viewer attention and conveys a range of emotions, however, the\nlabor-intensive nature of traditional animation often leads to compromises in\nthe complexity and consistency of eye design. Furthermore, we remove the need\nfor production data for training and introduce a new character recognition\nmethod that surpasses existing work by not requiring fine-tuning to specific\nproductions. This proposed use case could help maintain consistency throughout\nproduction and unlock bolder and more detailed design choices without the\nproduction cost drawbacks. A user study shows context-aware translation is\npreferred over existing work 95.16% of the time.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.MM",
      "I.2.6; I.2.1; J.5"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03499v1",
    "published_date": "2024-01-07 14:34:34 UTC",
    "updated_date": "2024-01-07 14:34:34 UTC"
  },
  {
    "arxiv_id": "2401.03497v1",
    "title": "EAT: Self-Supervised Pre-Training with Efficient Audio Transformer",
    "authors": [
      "Wenxi Chen",
      "Yuzhe Liang",
      "Ziyang Ma",
      "Zhisheng Zheng",
      "Xie Chen"
    ],
    "abstract": "Audio self-supervised learning (SSL) pre-training, which aims to learn good\nrepresentations from unlabeled audio, has made remarkable progress. However,\nthe extensive computational demands during pre-training pose a significant\nbarrier to the potential application and optimization of audio SSL models. In\nthis paper, inspired by the success of data2vec 2.0 in image modality and\nAudio-MAE in audio modality, we introduce Efficient Audio Transformer (EAT) to\nfurther improve the effectiveness and efficiency in audio SSL. The proposed EAT\nadopts the bootstrap self-supervised training paradigm to the audio domain. A\nnovel Utterance-Frame Objective (UFO) is designed to enhance the modeling\ncapability of acoustic events. Furthermore, we reveal that the masking strategy\nis critical in audio SSL pre-training, and superior audio representations can\nbe obtained with large inverse block masks. Experiment results demonstrate that\nEAT achieves state-of-the-art (SOTA) performance on a range of audio-related\ntasks, including AudioSet (AS-2M, AS-20K), ESC-50, and SPC-2, along with a\nsignificant pre-training speedup up to ~15x compared to existing audio SSL\nmodels.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03497v1",
    "published_date": "2024-01-07 14:31:27 UTC",
    "updated_date": "2024-01-07 14:31:27 UTC"
  },
  {
    "arxiv_id": "2401.06788v2",
    "title": "The NPU-ASLP-LiAuto System Description for Visual Speech Recognition in CNVSRC 2023",
    "authors": [
      "He Wang",
      "Pengcheng Guo",
      "Wei Chen",
      "Pan Zhou",
      "Lei Xie"
    ],
    "abstract": "This paper delineates the visual speech recognition (VSR) system introduced\nby the NPU-ASLP-LiAuto (Team 237) in the first Chinese Continuous Visual Speech\nRecognition Challenge (CNVSRC) 2023, engaging in the fixed and open tracks of\nSingle-Speaker VSR Task, and the open track of Multi-Speaker VSR Task. In terms\nof data processing, we leverage the lip motion extractor from the baseline1 to\nproduce multi-scale video data. Besides, various augmentation techniques are\napplied during training, encompassing speed perturbation, random rotation,\nhorizontal flipping, and color transformation. The VSR model adopts an\nend-to-end architecture with joint CTC/attention loss, comprising a ResNet3D\nvisual frontend, an E-Branchformer encoder, and a Transformer decoder.\nExperiments show that our system achieves 34.76% CER for the Single-Speaker\nTask and 41.06% CER for the Multi-Speaker Task after multi-system fusion,\nranking first place in all three tracks we participate.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Included in CNVSRC Workshop 2023, NCMMSC 2023",
    "pdf_url": "http://arxiv.org/pdf/2401.06788v2",
    "published_date": "2024-01-07 14:20:52 UTC",
    "updated_date": "2024-02-29 18:09:40 UTC"
  },
  {
    "arxiv_id": "2401.03489v1",
    "title": "Decentralized Federated Policy Gradient with Byzantine Fault-Tolerance and Provably Fast Convergence",
    "authors": [
      "Philip Jordan",
      "Florian Grötschla",
      "Flint Xiaofeng Fan",
      "Roger Wattenhofer"
    ],
    "abstract": "In Federated Reinforcement Learning (FRL), agents aim to collaboratively\nlearn a common task, while each agent is acting in its local environment\nwithout exchanging raw trajectories. Existing approaches for FRL either (a) do\nnot provide any fault-tolerance guarantees (against misbehaving agents), or (b)\nrely on a trusted central agent (a single point of failure) for aggregating\nupdates. We provide the first decentralized Byzantine fault-tolerant FRL\nmethod. Towards this end, we first propose a new centralized Byzantine\nfault-tolerant policy gradient (PG) algorithm that improves over existing\nmethods by relying only on assumptions standard for non-fault-tolerant PG.\nThen, as our main contribution, we show how a combination of robust aggregation\nand Byzantine-resilient agreement methods can be leveraged in order to\neliminate the need for a trusted central entity. Since our results represent\nthe first sample complexity analysis for Byzantine fault-tolerant decentralized\nfederated non-convex optimization, our technical contributions may be of\nindependent interest. Finally, we corroborate our theoretical results\nexperimentally for common RL environments, demonstrating the speed-up of\ndecentralized federations w.r.t. the number of participating agents and\nresilience against various Byzantine attacks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at AAMAS'24",
    "pdf_url": "http://arxiv.org/pdf/2401.03489v1",
    "published_date": "2024-01-07 14:06:06 UTC",
    "updated_date": "2024-01-07 14:06:06 UTC"
  },
  {
    "arxiv_id": "2401.03476v1",
    "title": "Freetalker: Controllable Speech and Text-Driven Gesture Generation Based on Diffusion Models for Enhanced Speaker Naturalness",
    "authors": [
      "Sicheng Yang",
      "Zunnan Xu",
      "Haiwei Xue",
      "Yongkang Cheng",
      "Shaoli Huang",
      "Mingming Gong",
      "Zhiyong Wu"
    ],
    "abstract": "Current talking avatars mostly generate co-speech gestures based on audio and\ntext of the utterance, without considering the non-speaking motion of the\nspeaker. Furthermore, previous works on co-speech gesture generation have\ndesigned network structures based on individual gesture datasets, which results\nin limited data volume, compromised generalizability, and restricted speaker\nmovements. To tackle these issues, we introduce FreeTalker, which, to the best\nof our knowledge, is the first framework for the generation of both spontaneous\n(e.g., co-speech gesture) and non-spontaneous (e.g., moving around the podium)\nspeaker motions. Specifically, we train a diffusion-based model for speaker\nmotion generation that employs unified representations of both speech-driven\ngestures and text-driven motions, utilizing heterogeneous data sourced from\nvarious motion datasets. During inference, we utilize classifier-free guidance\nto highly control the style in the clips. Additionally, to create smooth\ntransitions between clips, we utilize DoubleTake, a method that leverages a\ngenerative prior and ensures seamless motion blending. Extensive experiments\nshow that our method generates natural and controllable speaker movements. Our\ncode, model, and demo are are available at\n\\url{https://youngseng.github.io/FreeTalker/}.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.HC",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.MM",
    "comment": "6 pages, 3 figures, ICASSP 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.03476v1",
    "published_date": "2024-01-07 13:01:29 UTC",
    "updated_date": "2024-01-07 13:01:29 UTC"
  },
  {
    "arxiv_id": "2401.03473v3",
    "title": "ICMC-ASR: The ICASSP 2024 In-Car Multi-Channel Automatic Speech Recognition Challenge",
    "authors": [
      "He Wang",
      "Pengcheng Guo",
      "Yue Li",
      "Ao Zhang",
      "Jiayao Sun",
      "Lei Xie",
      "Wei Chen",
      "Pan Zhou",
      "Hui Bu",
      "Xin Xu",
      "Binbin Zhang",
      "Zhuo Chen",
      "Jian Wu",
      "Longbiao Wang",
      "Eng Siong Chng",
      "Sun Li"
    ],
    "abstract": "To promote speech processing and recognition research in driving scenarios,\nwe build on the success of the Intelligent Cockpit Speech Recognition Challenge\n(ICSRC) held at ISCSLP 2022 and launch the ICASSP 2024 In-Car Multi-Channel\nAutomatic Speech Recognition (ICMC-ASR) Challenge. This challenge collects over\n100 hours of multi-channel speech data recorded inside a new energy vehicle and\n40 hours of noise for data augmentation. Two tracks, including automatic speech\nrecognition (ASR) and automatic speech diarization and recognition (ASDR) are\nset up, using character error rate (CER) and concatenated minimum permutation\ncharacter error rate (cpCER) as evaluation metrics, respectively. Overall, the\nICMC-ASR Challenge attracts 98 participating teams and receives 53 valid\nresults in both tracks. In the end, first-place team USTCiflytek achieves a CER\nof 13.16% in the ASR track and a cpCER of 21.48% in the ASDR track, showing an\nabsolute improvement of 13.08% and 51.4% compared to our challenge baseline,\nrespectively.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted at ICASSP 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.03473v3",
    "published_date": "2024-01-07 12:51:42 UTC",
    "updated_date": "2024-02-21 03:39:37 UTC"
  },
  {
    "arxiv_id": "2401.04138v1",
    "title": "Expanding Horizons in HCI Research Through LLM-Driven Qualitative Analysis",
    "authors": [
      "Maya Grace Torii",
      "Takahito Murakami",
      "Yoichi Ochiai"
    ],
    "abstract": "How would research be like if we still needed to \"send\" papers typed with a\ntypewriter? Our life and research environment have continually evolved, often\naccompanied by controversial opinions about new methodologies. In this paper,\nwe embrace this change by introducing a new approach to qualitative analysis in\nHCI using Large Language Models (LLMs). We detail a method that uses LLMs for\nqualitative data analysis and present a quantitative framework using SBART\ncosine similarity for performance evaluation. Our findings indicate that LLMs\nnot only match the efficacy of traditional analysis methods but also offer\nunique insights. Through a novel dataset and benchmark, we explore LLMs'\ncharacteristics in HCI research, suggesting potential avenues for further\nexploration and application in the field.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04138v1",
    "published_date": "2024-01-07 12:39:31 UTC",
    "updated_date": "2024-01-07 12:39:31 UTC"
  },
  {
    "arxiv_id": "2401.15081v1",
    "title": "Can generative AI and ChatGPT outperform humans on cognitive-demanding problem-solving tasks in science?",
    "authors": [
      "Xiaoming Zhai",
      "Matthew Nyaaba",
      "Wenchao Ma"
    ],
    "abstract": "This study aimed to examine an assumption that generative artificial\nintelligence (GAI) tools can overcome the cognitive intensity that humans\nsuffer when solving problems. We compared the performance of ChatGPT and GPT-4\non 2019 NAEP science assessments with students by cognitive demands of the\nitems. Fifty-four tasks were coded by experts using a two-dimensional cognitive\nload framework, including task cognitive complexity and dimensionality. ChatGPT\nand GPT-4 responses were scored using the scoring keys of NAEP. The analysis of\nthe available data was based on the average student ability scores for students\nwho answered each item correctly and the percentage of students who responded\nto individual items. Results showed that both ChatGPT and GPT-4 consistently\noutperformed most students who answered the NAEP science assessments. As the\ncognitive demand for NAEP tasks increases, statistically higher average student\nability scores are required to correctly address the questions. This pattern\nwas observed for students in grades 4, 8, and 12, respectively. However,\nChatGPT and GPT-4 were not statistically sensitive to the increase in cognitive\ndemands of the tasks, except for Grade 4. As the first study focusing on\ncomparing GAI and K-12 students in problem-solving in science, this finding\nimplies the need for changes to educational objectives to prepare students with\ncompetence to work with GAI tools in the future. Education ought to emphasize\nthe cultivation of advanced cognitive skills rather than depending solely on\ntasks that demand cognitive intensity. This approach would foster critical\nthinking, analytical skills, and the application of knowledge in novel\ncontexts. Findings also suggest the need for innovative assessment practices by\nmoving away from cognitive intensity tasks toward creativity and analytical\nskills to avoid the negative effects of GAI on testing more efficiently.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.15081v1",
    "published_date": "2024-01-07 12:36:31 UTC",
    "updated_date": "2024-01-07 12:36:31 UTC"
  },
  {
    "arxiv_id": "2401.03470v2",
    "title": "FurniScene: A Large-scale 3D Room Dataset with Intricate Furnishing Scenes",
    "authors": [
      "Genghao Zhang",
      "Yuxi Wang",
      "Chuanchen Luo",
      "Shibiao Xu",
      "Zhaoxiang Zhang",
      "Man Zhang",
      "Junran Peng"
    ],
    "abstract": "Indoor scene generation has attracted significant attention recently as it is\ncrucial for applications of gaming, virtual reality, and interior design.\nCurrent indoor scene generation methods can produce reasonable room layouts but\noften lack diversity and realism. This is primarily due to the limited coverage\nof existing datasets, including only large furniture without tiny furnishings\nin daily life. To address these challenges, we propose FurniScene, a\nlarge-scale 3D room dataset with intricate furnishing scenes from interior\ndesign professionals. Specifically, the FurniScene consists of 11,698 rooms and\n39,691 unique furniture CAD models with 89 different types, covering things\nfrom large beds to small teacups on the coffee table. To better suit\nfine-grained indoor scene layout generation, we introduce a novel Two-Stage\nDiffusion Scene Model (TSDSM) and conduct an evaluation benchmark for various\nindoor scene generation based on FurniScene. Quantitative and qualitative\nevaluations demonstrate the capability of our method to generate highly\nrealistic indoor scenes. Our dataset and code will be publicly available soon.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03470v2",
    "published_date": "2024-01-07 12:34:45 UTC",
    "updated_date": "2024-05-06 06:33:39 UTC"
  },
  {
    "arxiv_id": "2401.03469v3",
    "title": "Efficient Test Data Generation for MC/DC with OCL and Search",
    "authors": [
      "Hassan Sartaj",
      "Muhammad Zohaib Iqbal",
      "Atif Aftab Ahmed Jilani",
      "Muhammad Uzair Khan"
    ],
    "abstract": "System-level testing of avionics software systems requires compliance with\ndifferent international safety standards such as DO-178C. An important\nconsideration of the avionics industry is automated test data generation\naccording to the criteria suggested by safety standards. One of the recommended\ncriteria by DO-178C is the modified condition/decision coverage (MC/DC)\ncriterion. The current model-based test data generation approaches use\nconstraints written in Object Constraint Language (OCL), and apply search\ntechniques to generate test data. These approaches either do not support MC/DC\ncriterion or suffer from performance issues while generating test data for\nlarge-scale avionics systems. In this paper, we propose an effective way to\nautomate MC/DC test data generation during model-based testing. We develop a\nstrategy that utilizes case-based reasoning (CBR) and range reduction\nheuristics designed to solve MC/DC-tailored OCL constraints. We performed an\nempirical study to compare our proposed strategy for MC/DC test data generation\nusing CBR, range reduction, both CBR and range reduction, with an original\nsearch algorithm, and random search. We also empirically compared our strategy\nwith existing constraint-solving approaches. The results show that both CBR and\nrange reduction for MC/DC test data generation outperform the baseline\napproach. Moreover, the combination of both CBR and range reduction for MC/DC\ntest data generation is an effective approach compared to existing constraint\nsolvers.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03469v3",
    "published_date": "2024-01-07 12:31:36 UTC",
    "updated_date": "2024-08-02 11:39:03 UTC"
  },
  {
    "arxiv_id": "2401.03467v1",
    "title": "Maintaining Journalistic Integrity in the Digital Age: A Comprehensive NLP Framework for Evaluating Online News Content",
    "authors": [
      "Ljubisa Bojic",
      "Nikola Prodanovic",
      "Agariadne Dwinggo Samala"
    ],
    "abstract": "The rapid growth of online news platforms has led to an increased need for\nreliable methods to evaluate the quality and credibility of news articles. This\npaper proposes a comprehensive framework to analyze online news texts using\nnatural language processing (NLP) techniques, particularly a language model\nspecifically trained for this purpose, alongside other well-established NLP\nmethods. The framework incorporates ten journalism standards-objectivity,\nbalance and fairness, readability and clarity, sensationalism and clickbait,\nethical considerations, public interest and value, source credibility,\nrelevance and timeliness, factual accuracy, and attribution and transparency-to\nassess the quality of news articles. By establishing these standards,\nresearchers, media organizations, and readers can better evaluate and\nunderstand the content they consume and produce. The proposed method has some\nlimitations, such as potential difficulty in detecting subtle biases and the\nneed for continuous updating of the language model to keep pace with evolving\nlanguage patterns.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "21 pages",
    "pdf_url": "http://arxiv.org/pdf/2401.03467v1",
    "published_date": "2024-01-07 12:27:14 UTC",
    "updated_date": "2024-01-07 12:27:14 UTC"
  },
  {
    "arxiv_id": "2401.03462v3",
    "title": "Long Context Compression with Activation Beacon",
    "authors": [
      "Peitian Zhang",
      "Zheng Liu",
      "Shitao Xiao",
      "Ninglu Shao",
      "Qiwei Ye",
      "Zhicheng Dou"
    ],
    "abstract": "Long context compression is a critical research problem due to its\nsignificance in reducing the high computational and memory costs associated\nwith LLMs. In this paper, we propose Activation Beacon, a plug-in module for\ntransformer-based LLMs that targets effective, efficient, and flexible\ncompression of long contexts. To achieve this, our method introduces the\nfollowing technical designs. 1) We directly compress the activations (i.e. keys\nand values at every layer), rather than leveraging soft prompts to relay\ninformation (which constitute a major bottleneck to encapsulate the complex\ninformation within long contexts). 2) We tailor the compression workflow, where\neach fine-grained input unit is progressively compressed, enabling high-quality\ncompression and efficient computation during both training and inference. 3) We\ntrain the model through compression-based auto-regression, making full use of\nplain texts and instructional data to optimize the model's compression\nperformance. 4) During training, we randomly sample a compression ratio at each\nstep, teaching the model to support a wide range of compression configurations.\nExtensive evaluations are conducted on various long-context tasks whose lengths\n(e.g., 128K) may far exceed the maximum training length (20K), such as document\nunderstanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing\nmethods struggle to handle these challenging tasks, Activation Beacon maintains\na comparable performance to the uncompressed baseline across various scenarios,\nachieving a 2x acceleration in inference time and an 8x reduction of memory\ncosts for KV cache. Our data, model, and code have been released at\n\\url{https://github.com/FlagOpen/FlagEmbedding/}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Newer version of Activation Beacon",
    "pdf_url": "http://arxiv.org/pdf/2401.03462v3",
    "published_date": "2024-01-07 11:57:40 UTC",
    "updated_date": "2024-10-11 02:18:24 UTC"
  },
  {
    "arxiv_id": "2401.03461v1",
    "title": "Amplification of Addictive New Media Features in the Metaverse",
    "authors": [
      "Ljubisa Bojic",
      "Joerg Matthes",
      "Milan Cabarkapa"
    ],
    "abstract": "The emergence of the metaverse, envisioned as a hyperreal virtual universe\nfacilitating boundless human interaction, stands to revolutionize our\nconception of media, with significant impacts on addiction, creativity,\nrelationships, and social polarization. This paper aims to dissect the\naddictive potential of the metaverse due to its immersive and interactive\nfeatures, scrutinize the effects of its recommender systems on creativity and\nsocial polarization, and explore potential consequences stemming from the\nmetaverse development. We employed a literature review methodology, drawing\nparallels from the research on new media platforms and examining the\nprogression of reality-mimicking features in media from historical perspectives\nto understand this transformative digital frontier. The findings suggest that\nthese immersive and interactive features could potentially exacerbate media\naddiction. The designed recommender systems, while aiding personalization and\nuser engagement, might contribute to social polarization and affect the\ndiversity of creative output. However, our conclusions are based primarily on\ntheoretical propositions from studies conducted on existing media platforms and\nlack empirical support specific to the metaverse. Therefore, this paper\nidentifies a critical gap requiring further research, through empirical studies\nfocused on metaverse use and addiction and exploration of privacy, security,\nand ethical implications associated with this burgeoning digital universe. As\nthe development of the metaverse accelerates, it is incumbent on scholars,\ntechnologists, and policymakers to navigate its multilayered impacts\nthoughtfully to balance innovation with societal well-being.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "14 pages, 1 figure, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2401.03461v1",
    "published_date": "2024-01-07 11:50:07 UTC",
    "updated_date": "2024-01-07 11:50:07 UTC"
  },
  {
    "arxiv_id": "2403.07879v1",
    "title": "AI incidents and 'networked trouble': The case for a research agenda",
    "authors": [
      "Tommy Shaffer Shane"
    ],
    "abstract": "Against a backdrop of widespread interest in how publics can participate in\nthe design of AI, I argue for a research agenda focused on AI incidents -\nexamples of AI going wrong and sparking controversy - and how they are\nconstructed in online environments. I take up the example of an AI incident\nfrom September 2020, when a Twitter user created a 'horrible experiment' to\ndemonstrate the racist bias of Twitter's algorithm for cropping images. This\nresulted in Twitter not only abandoning its use of that algorithm, but also\ndisavowing its decision to use any algorithm for the task. I argue that AI\nincidents like this are a significant means for participating in AI systems\nthat require further research. That research agenda, I argue, should focus on\nhow incidents are constructed through networked online behaviours that I refer\nto as 'networked trouble', where formats for participation enable individuals\nand algorithms to interact in ways that others - including technology companies\n- come to know and come to care about. At stake, I argue, is an important\nmechanism for participating in the design and deployment of AI.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.07879v1",
    "published_date": "2024-01-07 11:23:13 UTC",
    "updated_date": "2024-01-07 11:23:13 UTC"
  },
  {
    "arxiv_id": "2401.03454v1",
    "title": "Computational Argumentation-based Chatbots: a Survey",
    "authors": [
      "Federico Castagna",
      "Nadin Kokciyan",
      "Isabel Sassoon",
      "Simon Parsons",
      "Elizabeth Sklar"
    ],
    "abstract": "Chatbots are conversational software applications designed to interact\ndialectically with users for a plethora of different purposes. Surprisingly,\nthese colloquial agents have only recently been coupled with computational\nmodels of arguments (i.e. computational argumentation), whose aim is to\nformalise, in a machine-readable format, the ordinary exchange of information\nthat characterises human communications. Chatbots may employ argumentation with\ndifferent degrees and in a variety of manners. The present survey sifts through\nthe literature to review papers concerning this kind of argumentation-based\nbot, drawing conclusions about the benefits and drawbacks that this approach\nentails in comparison with standard chatbots, while also envisaging possible\nfuture development and integration with the Transformer-based architecture and\nstate-of-the-art Large Language models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03454v1",
    "published_date": "2024-01-07 11:20:42 UTC",
    "updated_date": "2024-01-07 11:20:42 UTC"
  },
  {
    "arxiv_id": "2401.03428v1",
    "title": "Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects",
    "authors": [
      "Yuheng Cheng",
      "Ceyao Zhang",
      "Zhengwen Zhang",
      "Xiangrui Meng",
      "Sirui Hong",
      "Wenhao Li",
      "Zihao Wang",
      "Zekai Wang",
      "Feng Yin",
      "Junhua Zhao",
      "Xiuqiang He"
    ],
    "abstract": "Intelligent agents stand out as a potential path toward artificial general\nintelligence (AGI). Thus, researchers have dedicated significant effort to\ndiverse implementations for them. Benefiting from recent progress in large\nlanguage models (LLMs), LLM-based agents that use universal natural language as\nan interface exhibit robust generalization capabilities across various\napplications -- from serving as autonomous general-purpose task assistants to\napplications in coding, social, and economic domains, LLM-based agents offer\nextensive exploration opportunities. This paper surveys current research to\nprovide an in-depth overview of LLM-based intelligent agents within\nsingle-agent and multi-agent systems. It covers their definitions, research\nframeworks, and foundational components such as their composition, cognitive\nand planning methods, tool utilization, and responses to environmental\nfeedback. We also delve into the mechanisms of deploying LLM-based agents in\nmulti-agent systems, including multi-role collaboration, message passing, and\nstrategies to alleviate communication issues between agents. The discussions\nalso shed light on popular datasets and application scenarios. We conclude by\nenvisioning prospects for LLM-based agents, considering the evolving landscape\nof AI and natural language processing.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03428v1",
    "published_date": "2024-01-07 09:08:24 UTC",
    "updated_date": "2024-01-07 09:08:24 UTC"
  },
  {
    "arxiv_id": "2401.03426v2",
    "title": "On Leveraging Large Language Models for Enhancing Entity Resolution: A Cost-efficient Approach",
    "authors": [
      "Huahang Li",
      "Longyu Feng",
      "Shuangyin Li",
      "Fei Hao",
      "Chen Jason Zhang",
      "Yuanfeng Song"
    ],
    "abstract": "Entity resolution, the task of identifying and merging records that refer to\nthe same real-world entity, is crucial in sectors like e-commerce, healthcare,\nand law enforcement. Large Language Models (LLMs) introduce an innovative\napproach to this task, capitalizing on their advanced linguistic capabilities\nand a ``pay-as-you-go'' model that provides significant advantages to those\nwithout extensive data science expertise. However, current LLMs are costly due\nto per-API request billing. Existing methods often either lack quality or\nbecome prohibitively expensive at scale. To address these problems, we propose\nan uncertainty reduction framework using LLMs to improve entity resolution\nresults. We first initialize possible partitions of the entity cluster, refer\nto the same entity, and define the uncertainty of the result. Then, we reduce\nthe uncertainty by selecting a few valuable matching questions for LLM\nverification. Upon receiving the answers, we update the probability\ndistribution of the possible partitions. To further reduce costs, we design an\nefficient algorithm to judiciously select the most valuable matching pairs to\nquery. Additionally, we create error-tolerant techniques to handle LLM mistakes\nand a dynamic adjustment method to reach truly correct partitions. Experimental\nresults show that our method is efficient and effective, offering promising\napplications in real-world tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, preprint under review",
    "pdf_url": "http://arxiv.org/pdf/2401.03426v2",
    "published_date": "2024-01-07 09:06:58 UTC",
    "updated_date": "2024-09-12 04:47:33 UTC"
  },
  {
    "arxiv_id": "2401.03424v3",
    "title": "MLCA-AVSR: Multi-Layer Cross Attention Fusion based Audio-Visual Speech Recognition",
    "authors": [
      "He Wang",
      "Pengcheng Guo",
      "Pan Zhou",
      "Lei Xie"
    ],
    "abstract": "While automatic speech recognition (ASR) systems degrade significantly in\nnoisy environments, audio-visual speech recognition (AVSR) systems aim to\ncomplement the audio stream with noise-invariant visual cues and improve the\nsystem's robustness. However, current studies mainly focus on fusing the\nwell-learned modality features, like the output of modality-specific encoders,\nwithout considering the contextual relationship during the modality feature\nlearning. In this study, we propose a multi-layer cross-attention fusion based\nAVSR (MLCA-AVSR) approach that promotes representation learning of each\nmodality by fusing them at different levels of audio/visual encoders.\nExperimental results on the MISP2022-AVSR Challenge dataset show the efficacy\nof our proposed system, achieving a concatenated minimum permutation character\nerror rate (cpCER) of 30.57% on the Eval set and yielding up to 3.17% relative\nimprovement compared with our previous system which ranked the second place in\nthe challenge. Following the fusion of multiple systems, our proposed approach\nsurpasses the first-place system, establishing a new SOTA cpCER of 29.13% on\nthis dataset.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "5 pages, 3 figures Accepted at ICASSP 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.03424v3",
    "published_date": "2024-01-07 08:59:32 UTC",
    "updated_date": "2024-04-08 12:50:54 UTC"
  },
  {
    "arxiv_id": "2401.04136v2",
    "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
    "authors": [
      "Haonan Wang",
      "Qianli Shen",
      "Yao Tong",
      "Yang Zhang",
      "Kenji Kawaguchi"
    ],
    "abstract": "The commercialization of text-to-image diffusion models (DMs) brings forth\npotential copyright concerns. Despite numerous attempts to protect DMs from\ncopyright issues, the vulnerabilities of these solutions are underexplored. In\nthis study, we formalized the Copyright Infringement Attack on generative AI\nmodels and proposed a backdoor attack method, SilentBadDiffusion, to induce\ncopyright infringement without requiring access to or control over training\nprocesses. Our method strategically embeds connections between pieces of\ncopyrighted information and text references in poisoning data while carefully\ndispersing that information, making the poisoning data inconspicuous when\nintegrated into a clean dataset. Our experiments show the stealth and efficacy\nof the poisoning data. When given specific text prompts, DMs trained with a\npoisoning ratio of 0.20% can produce copyrighted images. Additionally, the\nresults reveal that the more sophisticated the DMs are, the easier the success\nof the attack becomes. These findings underline potential pitfalls in the\nprevailing copyright protection strategies and underscore the necessity for\nincreased scrutiny to prevent the misuse of DMs.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted for presentation at ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.04136v2",
    "published_date": "2024-01-07 08:37:29 UTC",
    "updated_date": "2024-05-26 06:00:10 UTC"
  },
  {
    "arxiv_id": "2401.03410v1",
    "title": "Engineering Features to Improve Pass Prediction in Soccer Simulation 2D Games",
    "authors": [
      "Nader Zare",
      "Mahtab Sarvmaili",
      "Aref Sayareh",
      "Omid Amini",
      "Stan Matwin Amilcar Soares"
    ],
    "abstract": "Soccer Simulation 2D (SS2D) is a simulation of a real soccer game in two\ndimensions. In soccer, passing behavior is an essential action for keeping the\nball in possession of our team and creating goal opportunities. Similarly, for\nSS2D, predicting the passing behaviors of both opponents and our teammates\nhelps manage resources and score more goals. Therefore, in this research, we\nhave tried to address the modeling of passing behavior of soccer 2D players\nusing Deep Neural Networks (DNN) and Random Forest (RF). We propose an embedded\ndata extraction module that can record the decision-making of agents in an\nonline format. Afterward, we apply four data sorting techniques for training\ndata preparation. After, we evaluate the trained models' performance playing\nagainst 6 top teams of RoboCup 2019 that have distinctive playing strategies.\nFinally, we examine the importance of different feature groups on the\nprediction of a passing strategy. All results in each step of this work prove\nour suggested methodology's effectiveness and improve the performance of the\npass prediction in Soccer Simulation 2D games ranging from 5\\% (e.g., playing\nagainst the same team) to 10\\% (e.g., playing against Robocup top teams).",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03410v1",
    "published_date": "2024-01-07 08:01:25 UTC",
    "updated_date": "2024-01-07 08:01:25 UTC"
  },
  {
    "arxiv_id": "2401.03408v1",
    "title": "Escalation Risks from Language Models in Military and Diplomatic Decision-Making",
    "authors": [
      "Juan-Pablo Rivera",
      "Gabriel Mukobi",
      "Anka Reuel",
      "Max Lamparth",
      "Chandler Smith",
      "Jacquelyn Schneider"
    ],
    "abstract": "Governments are increasingly considering integrating autonomous AI agents in\nhigh-stakes military and foreign-policy decision-making, especially with the\nemergence of advanced generative AI models like GPT-4. Our work aims to\nscrutinize the behavior of multiple AI agents in simulated wargames,\nspecifically focusing on their predilection to take escalatory actions that may\nexacerbate multilateral conflicts. Drawing on political science and\ninternational relations literature about escalation dynamics, we design a novel\nwargame simulation and scoring framework to assess the escalation risks of\nactions taken by these agents in different scenarios. Contrary to prior\nstudies, our research provides both qualitative and quantitative insights and\nfocuses on large language models (LLMs). We find that all five studied\noff-the-shelf LLMs show forms of escalation and difficult-to-predict escalation\npatterns. We observe that models tend to develop arms-race dynamics, leading to\ngreater conflict, and in rare cases, even to the deployment of nuclear weapons.\nQualitatively, we also collect the models' reported reasonings for chosen\nactions and observe worrying justifications based on deterrence and\nfirst-strike tactics. Given the high stakes of military and foreign-policy\ncontexts, we recommend further examination and cautious consideration before\ndeploying autonomous language model agents for strategic military or diplomatic\ndecision-making.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages body, 57 pages appendix, 46 figures, 11 tables",
    "pdf_url": "http://arxiv.org/pdf/2401.03408v1",
    "published_date": "2024-01-07 07:59:10 UTC",
    "updated_date": "2024-01-07 07:59:10 UTC"
  },
  {
    "arxiv_id": "2401.03406v1",
    "title": "Improving Dribbling, Passing, and Marking Actions in Soccer Simulation 2D Games Using Machine Learning",
    "authors": [
      "Nader Zare",
      "Omid Amini",
      "Aref Sayareh",
      "Mahtab Sarvmaili",
      "Arad Firouzkouhi",
      "Stan Matwin",
      "Amilcar Soares"
    ],
    "abstract": "The RoboCup competition was started in 1997, and is known as the oldest\nRoboCup league. The RoboCup 2D Soccer Simulation League is a stochastic,\npartially observable soccer environment in which 24 autonomous agents play on\ntwo opposing teams. In this paper, we detail the main strategies and\nfunctionalities of CYRUS, the RoboCup 2021 2D Soccer Simulation League\nchampions. The new functionalities presented and discussed in this work are (i)\nMulti Action Dribble, (ii) Pass Prediction and (iii) Marking Decision. The\nMulti Action Dribbling strategy enabled CYRUS to succeed more often and to be\nsafer when dribbling actions were performed during a game. The Pass Prediction\nenhanced our gameplay by predicting our teammate's passing behavior,\nanticipating and making our agents collaborate better towards scoring goals.\nFinally, the Marking Decision addressed the multi-agent matching problem to\nimprove CYRUS defensive strategy by finding an optimal solution to mark\nopponents' players.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03406v1",
    "published_date": "2024-01-07 07:54:26 UTC",
    "updated_date": "2024-01-07 07:54:26 UTC"
  },
  {
    "arxiv_id": "2401.03397v2",
    "title": "Predicting the Skies: A Novel Model for Flight-Level Passenger Traffic Forecasting",
    "authors": [
      "Sina Ehsani",
      "Elina Sergeeva",
      "Wendy Murdy",
      "Benjamin Fox"
    ],
    "abstract": "Accurate prediction of flight-level passenger traffic is of paramount\nimportance in airline operations, influencing key decisions from pricing to\nroute optimization. This study introduces a novel, multimodal deep learning\napproach to the challenge of predicting flight-level passenger traffic,\nyielding substantial accuracy improvements compared to traditional models.\nLeveraging an extensive dataset from American Airlines, our model ingests\nhistorical traffic data, fare closure information, and seasonality attributes\nspecific to each flight. Our proposed neural network integrates the strengths\nof Recurrent Neural Networks (RNN) and Convolutional Neural Networks (CNN),\nexploiting the temporal patterns and spatial relationships within the data to\nenhance prediction performance. Crucial to the success of our model is a\ncomprehensive data processing strategy. We construct 3D tensors to represent\ndata, apply careful masking strategies to mirror real-world dynamics, and\nemploy data augmentation techniques to enrich the diversity of our training\nset. The efficacy of our approach is borne out in the results: our model\ndemonstrates an approximate 33\\% improvement in Mean Squared Error (MSE)\ncompared to traditional benchmarks. This study, therefore, highlights the\nsignificant potential of deep learning techniques and meticulous data\nprocessing in advancing the field of flight traffic prediction.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.AP"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 6 figures, to be published",
    "pdf_url": "http://arxiv.org/pdf/2401.03397v2",
    "published_date": "2024-01-07 06:51:26 UTC",
    "updated_date": "2024-01-09 23:23:04 UTC"
  },
  {
    "arxiv_id": "2401.04135v1",
    "title": "Global-Aware Enhanced Spatial-Temporal Graph Recurrent Networks: A New Framework For Traffic Flow Prediction",
    "authors": [
      "Haiyang Liu",
      "Chunjiang Zhu",
      "Detian Zhang"
    ],
    "abstract": "Traffic flow prediction plays a crucial role in alleviating traffic\ncongestion and enhancing transport efficiency. While combining graph\nconvolution networks with recurrent neural networks for spatial-temporal\nmodeling is a common strategy in this realm, the restricted structure of\nrecurrent neural networks limits their ability to capture global information.\nFor spatial modeling, many prior studies learn a graph structure that is\nassumed to be fixed and uniform at all time steps, which may not be true. This\npaper introduces a novel traffic prediction framework, Global-Aware Enhanced\nSpatial-Temporal Graph Recurrent Network (GA-STGRN), comprising two core\ncomponents: a spatial-temporal graph recurrent neural network and a global\nawareness layer. Within this framework, three innovative prediction models are\nformulated. A sequence-aware graph neural network is proposed and integrated\ninto the Gated Recurrent Unit (GRU) to learn non-fixed graphs at different time\nsteps and capture local temporal relationships. To enhance the model's global\nperception, three distinct global spatial-temporal transformer-like\narchitectures (GST^2) are devised for the global awareness layer. We conduct\nextensive experiments on four real traffic datasets and the results demonstrate\nthe superiority of our framework and the three concrete models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04135v1",
    "published_date": "2024-01-07 05:28:36 UTC",
    "updated_date": "2024-01-07 05:28:36 UTC"
  },
  {
    "arxiv_id": "2401.04134v1",
    "title": "Web Neural Network with Complete DiGraphs",
    "authors": [
      "Frank Li"
    ],
    "abstract": "This paper introduces a new neural network model that aims to mimic the\nbiological brain more closely by structuring the network as a complete directed\ngraph that processes continuous data for each timestep. Current neural networks\nhave structures that vaguely mimic the brain structure, such as neurons,\nconvolutions, and recurrence. The model proposed in this paper adds additional\nstructural properties by introducing cycles into the neuron connections and\nremoving the sequential nature commonly seen in other network layers.\nFurthermore, the model has continuous input and output, inspired by spiking\nneural networks, which allows the network to learn a process of classification,\nrather than simply returning the final result.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04134v1",
    "published_date": "2024-01-07 05:12:10 UTC",
    "updated_date": "2024-01-07 05:12:10 UTC"
  },
  {
    "arxiv_id": "2401.06787v1",
    "title": "Deep Learning Based Cyberbullying Detection in Bangla Language",
    "authors": [
      "Sristy Shidul Nath",
      "Razuan Karim",
      "Mahdi H. Miraz"
    ],
    "abstract": "The Internet is currently the largest platform for global communication\nincluding expressions of opinions, reviews, contents, images, videos and so\nforth. Moreover, social media has now become a very broad and highly engaging\nplatform due to its immense popularity and swift adoption trend. Increased\nsocial networking, however, also has detrimental impacts on the society leading\nto a range of unwanted phenomena, such as online assault, intimidation, digital\nbullying, criminality and trolling. Hence, cyberbullying has become a pervasive\nand worrying problem that poses considerable psychological and emotional harm\nto the people, particularly amongst the teens and the young adults. In order to\nlessen its negative effects and provide victims with prompt support, a great\ndeal of research to identify cyberbullying instances at various online\nplatforms is emerging. In comparison to other languages, Bangla (also known as\nBengali) has fewer research studies in this domain. This study demonstrates a\ndeep learning strategy for identifying cyberbullying in Bengali, using a\ndataset of 12282 versatile comments from multiple social media sites. In this\nstudy, a two-layer bidirectional long short-term memory (Bi-LSTM) model has\nbeen built to identify cyberbullying, using a variety of optimisers as well as\n5-fold cross validation. To evaluate the functionality and efficacy of the\nproposed system, rigorous assessment and validation procedures have been\nemployed throughout the project. The results of this study reveals that the\nproposed model's accuracy, using momentum-based stochastic gradient descent\n(SGD) optimiser, is 94.46%. It also reflects a higher accuracy of 95.08% and a\nF1 score of 95.23% using Adam optimiser as well as a better accuracy of 94.31%\nin 5-fold cross validation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.06787v1",
    "published_date": "2024-01-07 04:58:59 UTC",
    "updated_date": "2024-01-07 04:58:59 UTC"
  },
  {
    "arxiv_id": "2401.04133v2",
    "title": "SynHING: Synthetic Heterogeneous Information Network Generation for Graph Learning and Explanation",
    "authors": [
      "Ming-Yi Hong",
      "Yi-Hsiang Huang",
      "Shao-En Lin",
      "You-Chen Teng",
      "Chih-Yu Wang",
      "Che Lin"
    ],
    "abstract": "Graph Neural Networks (GNNs) excel in delineating graph structures in diverse\ndomains, including community analysis and recommendation systems. As the\ninterpretation of GNNs becomes increasingly important, the demand for robust\nbaselines and expansive graph datasets is accentuated, particularly in the\ncontext of Heterogeneous Information Networks (HIN). Addressing this, we\nintroduce SynHING, a novel framework for Synthetic Heterogeneous Information\nNetwork Generation aimed at enhancing graph learning and explanation. SynHING\nsystematically identifies major motifs in a target HIN and employs a bottom-up\ngeneration process with intra-cluster and inter-cluster merge modules. This\nprocess, supplemented by post-pruning techniques, ensures the synthetic HIN\nclosely mirrors the original graph's structural and statistical properties.\nCrucially, SynHING provides ground-truth motifs for evaluating GNN explainer\nmodels, setting a new standard for explainable, synthetic HIN generation and\ncontributing to the advancement of interpretable machine learning in complex\nnetworks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "Update figures, tables, and content",
    "pdf_url": "http://arxiv.org/pdf/2401.04133v2",
    "published_date": "2024-01-07 04:43:36 UTC",
    "updated_date": "2024-05-29 04:16:10 UTC"
  },
  {
    "arxiv_id": "2401.03374v2",
    "title": "LLM-Powered Code Vulnerability Repair with Reinforcement Learning and Semantic Reward",
    "authors": [
      "Nafis Tanveer Islam",
      "Joseph Khoury",
      "Andrew Seong",
      "Mohammad Bahrami Karkevandi",
      "Gonzalo De La Torre Parra",
      "Elias Bou-Harb",
      "Peyman Najafirad"
    ],
    "abstract": "In software development, the predominant emphasis on functionality often\nsupersedes security concerns, a trend gaining momentum with AI-driven\nautomation tools like GitHub Copilot. These tools significantly improve\ndevelopers' efficiency in functional code development. Nevertheless, it remains\na notable concern that such tools are also responsible for creating insecure\ncode, predominantly because of pre-training on publicly available repositories\nwith vulnerable code. Moreover, developers are called the \"weakest link in the\nchain\" since they have very minimal knowledge of code security. Although\nexisting solutions provide a reasonable solution to vulnerable code, they must\nadequately describe and educate the developers on code security to ensure that\nthe security issues are not repeated. Therefore we introduce a multipurpose\ncode vulnerability analysis system \\texttt{SecRepair}, powered by a large\nlanguage model, CodeGen2 assisting the developer in identifying and generating\nfixed code along with a complete description of the vulnerability with a code\ncomment. Our innovative methodology uses a reinforcement learning paradigm to\ngenerate code comments augmented by a semantic reward mechanism. Inspired by\nhow humans fix code issues, we propose an instruction-based dataset suitable\nfor vulnerability analysis with LLMs. We further identify zero-day and N-day\nvulnerabilities in 6 Open Source IoT Operating Systems on GitHub. Our findings\nunderscore that incorporating reinforcement learning coupled with semantic\nreward augments our model's performance, thereby fortifying its capacity to\naddress code vulnerabilities with improved efficacy.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03374v2",
    "published_date": "2024-01-07 02:46:39 UTC",
    "updated_date": "2024-02-22 00:29:37 UTC"
  },
  {
    "arxiv_id": "2401.03346v1",
    "title": "An Investigation of Large Language Models for Real-World Hate Speech Detection",
    "authors": [
      "Keyan Guo",
      "Alexander Hu",
      "Jaden Mu",
      "Ziheng Shi",
      "Ziming Zhao",
      "Nishant Vishwamitra",
      "Hongxin Hu"
    ],
    "abstract": "Hate speech has emerged as a major problem plaguing our social spaces today.\nWhile there have been significant efforts to address this problem, existing\nmethods are still significantly limited in effectively detecting hate speech\nonline. A major limitation of existing methods is that hate speech detection is\na highly contextual problem, and these methods cannot fully capture the context\nof hate speech to make accurate predictions. Recently, large language models\n(LLMs) have demonstrated state-of-the-art performance in several natural\nlanguage tasks. LLMs have undergone extensive training using vast amounts of\nnatural language data, enabling them to grasp intricate contextual details.\nHence, they could be used as knowledge bases for context-aware hate speech\ndetection. However, a fundamental problem with using LLMs to detect hate speech\nis that there are no studies on effectively prompting LLMs for context-aware\nhate speech detection. In this study, we conduct a large-scale study of hate\nspeech detection, employing five established hate speech datasets. We discover\nthat LLMs not only match but often surpass the performance of current benchmark\nmachine learning models in identifying hate speech. By proposing four diverse\nprompting strategies that optimize the use of LLMs in detecting hate speech.\nOur study reveals that a meticulously crafted reasoning prompt can effectively\ncapture the context of hate speech by fully utilizing the knowledge base in\nLLMs, significantly outperforming existing techniques. Furthermore, although\nLLMs can provide a rich knowledge base for the contextual detection of hate\nspeech, suitable prompting strategies play a crucial role in effectively\nleveraging this knowledge base for efficient detection.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted for publication on 22nd International Conference of Machine\n  Learning and Applications, ICMLA 2023",
    "pdf_url": "http://arxiv.org/pdf/2401.03346v1",
    "published_date": "2024-01-07 00:39:33 UTC",
    "updated_date": "2024-01-07 00:39:33 UTC"
  }
]
[
  {
    "arxiv_id": "2407.03365v1",
    "title": "ML Updates for OpenStreetMap: Analysis of Research Gaps and Future Directions",
    "authors": [
      "Lasith Niroshan",
      "James D. Carswell"
    ],
    "abstract": "Maintaining accurate, up-to-date maps is important in any dynamic urban\nlandscape, supporting various aspects of modern society, such as urban\nplanning, navigation, and emergency response. However, traditional (i.e.\nlargely manual) map production and crowdsourced mapping methods still struggle\nto keep pace with rapid changes in the built environment. Such manual mapping\nworkflows are time-consuming and prone to human errors, leading to early\nobsolescence and/or the need for extensive auditing. The current map updating\nprocess in OpenStreetMap provides an example of this limitation, relying on\nnumerous manual steps in its online map updating workflow. To address this,\nthere is a need to explore automating the entire end-to-end map up-dating\nprocess. Tech giants such as Google and Microsoft have already started\ninvestigating Machine Learning (ML) techniques to tackle this contemporary\nmapping problem. This paper offers an analysis of these ML approaches, focusing\non their application to updating Open-StreetMap in particular. By analysing the\ncurrent state-of-the-art in this field, this study identi-fies some key\nresearch gaps and introduces DeepMapper as a practical solution for advancing\nthe automatic online map updating process in the future.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "68U99",
      "A.0; I.4.9"
    ],
    "primary_category": "cs.AI",
    "comment": "21 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.03365v1",
    "published_date": "2024-06-28 23:51:04 UTC",
    "updated_date": "2024-06-28 23:51:04 UTC"
  },
  {
    "arxiv_id": "2407.00264v1",
    "title": "External Model Motivated Agents: Reinforcement Learning for Enhanced Environment Sampling",
    "authors": [
      "Rishav Bhagat",
      "Jonathan Balloch",
      "Zhiyu Lin",
      "Julia Kim",
      "Mark Riedl"
    ],
    "abstract": "Unlike reinforcement learning (RL) agents, humans remain capable multitaskers\nin changing environments. In spite of only experiencing the world through their\nown observations and interactions, people know how to balance focusing on tasks\nwith learning about how changes may affect their understanding of the world.\nThis is possible by choosing to solve tasks in ways that are interesting and\ngenerally informative beyond just the current task. Motivated by this, we\npropose an agent influence framework for RL agents to improve the adaptation\nefficiency of external models in changing environments without any changes to\nthe agent's rewards. Our formulation is composed of two self-contained modules:\ninterest fields and behavior shaping via interest fields. We implement an\nuncertainty-based interest field algorithm as well as a skill-sampling-based\nbehavior-shaping algorithm to use in testing this framework. Our results show\nthat our method outperforms the baselines in terms of external model adaptation\non metrics that measure both efficiency and performance.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00264v1",
    "published_date": "2024-06-28 23:31:22 UTC",
    "updated_date": "2024-06-28 23:31:22 UTC"
  },
  {
    "arxiv_id": "2407.00263v1",
    "title": "From Local Concepts to Universals: Evaluating the Multicultural Understanding of Vision-Language Models",
    "authors": [
      "Mehar Bhatia",
      "Sahithya Ravi",
      "Aditya Chinchure",
      "Eunjeong Hwang",
      "Vered Shwartz"
    ],
    "abstract": "Despite recent advancements in vision-language models, their performance\nremains suboptimal on images from non-western cultures due to\nunderrepresentation in training datasets. Various benchmarks have been proposed\nto test models' cultural inclusivity, but they have limited coverage of\ncultures and do not adequately assess cultural diversity across universal as\nwell as culture-specific local concepts. To address these limitations, we\nintroduce the GlobalRG benchmark, comprising two challenging tasks: retrieval\nacross universals and cultural visual grounding. The former task entails\nretrieving culturally diverse images for universal concepts from 50 countries,\nwhile the latter aims at grounding culture-specific concepts within images from\n15 countries. Our evaluation across a wide range of models reveals that the\nperformance varies significantly across cultures -- underscoring the necessity\nfor enhancing multicultural understanding in vision-language models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "Under peer review",
    "pdf_url": "http://arxiv.org/pdf/2407.00263v1",
    "published_date": "2024-06-28 23:28:28 UTC",
    "updated_date": "2024-06-28 23:28:28 UTC"
  },
  {
    "arxiv_id": "2407.00256v1",
    "title": "One Prompt is not Enough: Automated Construction of a Mixture-of-Expert Prompts",
    "authors": [
      "Ruochen Wang",
      "Sohyun An",
      "Minhao Cheng",
      "Tianyi Zhou",
      "Sung Ju Hwang",
      "Cho-Jui Hsieh"
    ],
    "abstract": "Large Language Models (LLMs) exhibit strong generalization capabilities to\nnovel tasks when prompted with language instructions and in-context demos.\nSince this ability sensitively depends on the quality of prompts, various\nmethods have been explored to automate the instruction design. While these\nmethods demonstrated promising results, they also restricted the searched\nprompt to one instruction. Such simplification significantly limits their\ncapacity, as a single demo-free instruction might not be able to cover the\nentire complex problem space of the targeted task. To alleviate this issue, we\nadopt the Mixture-of-Expert paradigm and divide the problem space into a set of\nsub-regions; Each sub-region is governed by a specialized expert, equipped with\nboth an instruction and a set of demos. A two-phase process is developed to\nconstruct the specialized expert for each region: (1) demo assignment: Inspired\nby the theoretical connection between in-context learning and kernel\nregression, we group demos into experts based on their semantic similarity; (2)\ninstruction assignment: A region-based joint search of an instruction per\nexpert complements the demos assigned to it, yielding a synergistic effect. The\nresulting method, codenamed Mixture-of-Prompts (MoP), achieves an average win\nrate of 81% against prior arts across several major benchmarks.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "stat.ML",
      "68T01"
    ],
    "primary_category": "cs.AI",
    "comment": "ICML 2024. code available at\n  https://github.com/ruocwang/mixture-of-prompts",
    "pdf_url": "http://arxiv.org/pdf/2407.00256v1",
    "published_date": "2024-06-28 23:05:08 UTC",
    "updated_date": "2024-06-28 23:05:08 UTC"
  },
  {
    "arxiv_id": "2407.00229v1",
    "title": "SemUV: Deep Learning based semantic manipulation over UV texture map of virtual human heads",
    "authors": [
      "Anirban Mukherjee",
      "Venkat Suprabath Bitra",
      "Vignesh Bondugula",
      "Tarun Reddy Tallapureddy",
      "Dinesh Babu Jayagopi"
    ],
    "abstract": "Designing and manipulating virtual human heads is essential across various\napplications, including AR, VR, gaming, human-computer interaction and VFX.\nTraditional graphic-based approaches require manual effort and resources to\nachieve accurate representation of human heads. While modern deep learning\ntechniques can generate and edit highly photorealistic images of faces, their\nfocus remains predominantly on 2D facial images. This limitation makes them\nless suitable for 3D applications. Recognizing the vital role of editing within\nthe UV texture space as a key component in the 3D graphics pipeline, our work\nfocuses on this aspect to benefit graphic designers by providing enhanced\ncontrol and precision in appearance manipulation. Research on existing methods\nwithin the UV texture space is limited, complex, and poses challenges. In this\npaper, we introduce SemUV: a simple and effective approach using the FFHQ-UV\ndataset for semantic manipulation directly within the UV texture space. We\ntrain a StyleGAN model on the publicly available FFHQ-UV dataset, and\nsubsequently train a boundary for interpolation and semantic feature\nmanipulation. Through experiments comparing our method with 2D manipulation\ntechnique, we demonstrate its superior ability to preserve identity while\neffectively modifying semantic features such as age, gender, and facial hair.\nOur approach is simple, agnostic to other 3D components such as structure,\nlighting, and rendering, and also enables seamless integration into standard 3D\ngraphics pipelines without demanding extensive domain expertise, time, or\nresources.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVIP 2024 Preprint",
    "pdf_url": "http://arxiv.org/pdf/2407.00229v1",
    "published_date": "2024-06-28 20:58:59 UTC",
    "updated_date": "2024-06-28 20:58:59 UTC"
  },
  {
    "arxiv_id": "2407.00219v2",
    "title": "Evaluating Human Alignment and Model Faithfulness of LLM Rationale",
    "authors": [
      "Mohsen Fayyaz",
      "Fan Yin",
      "Jiao Sun",
      "Nanyun Peng"
    ],
    "abstract": "We study how well large language models (LLMs) explain their generations\nthrough rationales -- a set of tokens extracted from the input text that\nreflect the decision-making process of LLMs. Specifically, we systematically\nstudy rationales derived using two approaches: (1) popular prompting-based\nmethods, where prompts are used to guide LLMs in generating rationales, and (2)\ntechnical attribution-based methods, which leverage attention or gradients to\nidentify important tokens. Our analysis spans three classification datasets\nwith annotated rationales, encompassing tasks with varying performance levels.\nWhile prompting-based self-explanations are widely used, our study reveals that\nthese explanations are not always as \"aligned\" with the human rationale as\nattribution-based explanations. Even more so, fine-tuning LLMs to enhance\nclassification task accuracy does not enhance the alignment of prompting-based\nrationales. Still, it does considerably improve the alignment of\nattribution-based methods (e.g., InputXGradient). More importantly, we show\nthat prompting-based self-explanation is also less \"faithful\" than\nattribution-based explanations, failing to provide a reliable account of the\nmodel's decision-making process. To evaluate faithfulness, unlike prior studies\nthat excluded misclassified examples, we evaluate all instances and also\nexamine the impact of fine-tuning and accuracy on alignment and faithfulness.\nOur findings suggest that inconclusive faithfulness results reported in earlier\nstudies may stem from low classification accuracy. These findings underscore\nthe importance of more rigorous and comprehensive evaluations of LLM\nrationales.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00219v2",
    "published_date": "2024-06-28 20:06:30 UTC",
    "updated_date": "2024-10-22 05:13:15 UTC"
  },
  {
    "arxiv_id": "2407.00197v1",
    "title": "Tradeoffs When Considering Deep Reinforcement Learning for Contingency Management in Advanced Air Mobility",
    "authors": [
      "Luis E. Alvarez",
      "Marc W. Brittain",
      "Steven D. Young"
    ],
    "abstract": "Air transportation is undergoing a rapid evolution globally with the\nintroduction of Advanced Air Mobility (AAM) and with it comes novel challenges\nand opportunities for transforming aviation. As AAM operations introduce\nincreasing heterogeneity in vehicle capabilities and density, increased levels\nof automation are likely necessary to achieve operational safety and efficiency\ngoals. This paper focuses on one example where increased automation has been\nsuggested. Autonomous operations will need contingency management systems that\ncan monitor evolving risk across a span of interrelated (or interdependent)\nhazards and, if necessary, execute appropriate control interventions via\nsupervised or automated decision making. Accommodating this complex environment\nmay require automated functions (autonomy) that apply artificial intelligence\n(AI) techniques that can adapt and respond to a quickly changing environment.\nThis paper explores the use of Deep Reinforcement Learning (DRL) which has\nshown promising performance in complex and high-dimensional environments where\nthe objective can be constructed as a sequential decision-making problem. An\nextension of a prior formulation of the contingency management problem as a\nMarkov Decision Process (MDP) is presented and uses a DRL framework to train\nagents that mitigate hazards present in the simulation environment. A\ncomparison of these learning-based agents and classical techniques is presented\nin terms of their performance, verification difficulties, and development\nprocess.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00197v1",
    "published_date": "2024-06-28 19:09:55 UTC",
    "updated_date": "2024-06-28 19:09:55 UTC"
  },
  {
    "arxiv_id": "2407.00188v1",
    "title": "A Novel Labeled Human Voice Signal Dataset for Misbehavior Detection",
    "authors": [
      "Ali Raza",
      "Faizan Younas"
    ],
    "abstract": "Voice signal classification based on human behaviours involves analyzing\nvarious aspects of speech patterns and delivery styles. In this study, a\nreal-time dataset collection is performed where participants are instructed to\nspeak twelve psychology questions in two distinct manners: first, in a harsh\nvoice, which is categorized as \"misbehaved\"; and second, in a polite manner,\ncategorized as \"normal\". These classifications are crucial in understanding how\ndifferent vocal behaviours affect the interpretation and classification of\nvoice signals. This research highlights the significance of voice tone and\ndelivery in automated machine-learning systems for voice analysis and\nrecognition. This research contributes to the broader field of voice signal\nanalysis by elucidating the impact of human behaviour on the perception and\ncategorization of voice signals, thereby enhancing the development of more\naccurate and context-aware voice recognition technologies.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00188v1",
    "published_date": "2024-06-28 18:55:07 UTC",
    "updated_date": "2024-06-28 18:55:07 UTC"
  },
  {
    "arxiv_id": "2407.00167v1",
    "title": "Can GPT-4 Help Detect Quit Vaping Intentions? An Exploration of Automatic Data Annotation Approach",
    "authors": [
      "Sai Krishna Revanth Vuruma",
      "Dezhi Wu",
      "Saborny Sen Gupta",
      "Lucas Aust",
      "Valerie Lookingbill",
      "Wyatt Bellamy",
      "Yang Ren",
      "Erin Kasson",
      "Li-Shiun Chen",
      "Patricia Cavazos-Rehg",
      "Dian Hu",
      "Ming Huang"
    ],
    "abstract": "In recent years, the United States has witnessed a significant surge in the\npopularity of vaping or e-cigarette use, leading to a notable rise in cases of\ne-cigarette and vaping use-associated lung injury (EVALI) that caused\nhospitalizations and fatalities during the EVALI outbreak in 2019, highlighting\nthe urgency to comprehend vaping behaviors and develop effective strategies for\ncessation. Due to the ubiquity of social media platforms, over 4.7 billion\nusers worldwide use them for connectivity, communications, news, and\nentertainment with a significant portion of the discourse related to health,\nthereby establishing social media data as an invaluable organic data resource\nfor public health research. In this study, we extracted a sample dataset from\none vaping sub-community on Reddit to analyze users' quit-vaping intentions.\nLeveraging OpenAI's latest large language model GPT-4 for sentence-level quit\nvaping intention detection, this study compares the outcomes of this model\nagainst layman and clinical expert annotations. Using different prompting\nstrategies such as zero-shot, one-shot, few-shot and chain-of-thought\nprompting, we developed 8 prompts with varying levels of detail to explain the\ntask to GPT-4 and also evaluated the performance of the strategies against each\nother. These preliminary findings emphasize the potential of GPT-4 in social\nmedia data analysis, especially in identifying users' subtle intentions that\nmay elude human detection.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.ET",
      "cs.HC",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for the AI Applications in Public Health and Social Services\n  workshop at the 22nd International Conference on Artificial Intelligence in\n  Medicine (AIME 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.00167v1",
    "published_date": "2024-06-28 18:06:48 UTC",
    "updated_date": "2024-06-28 18:06:48 UTC"
  },
  {
    "arxiv_id": "2406.20098v2",
    "title": "Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework for Multimodal LLMs",
    "authors": [
      "Sukmin Yun",
      "Haokun Lin",
      "Rusiru Thushara",
      "Mohammad Qazim Bhat",
      "Yongxin Wang",
      "Zutao Jiang",
      "Mingkai Deng",
      "Jinhong Wang",
      "Tianhua Tao",
      "Junbo Li",
      "Haonan Li",
      "Preslav Nakov",
      "Timothy Baldwin",
      "Zhengzhong Liu",
      "Eric P. Xing",
      "Xiaodan Liang",
      "Zhiqiang Shen"
    ],
    "abstract": "Multimodal large language models (MLLMs) have shown impressive success across\nmodalities such as image, video, and audio in a variety of understanding and\ngeneration tasks. However, current MLLMs are surprisingly poor at understanding\nwebpage screenshots and generating their corresponding HTML code. To address\nthis problem, we propose $\\texttt{Web2Code}$, a benchmark consisting of a new\nlarge-scale webpage-to-code dataset for instruction tuning and an evaluation\nframework for the webpage understanding and HTML code translation abilities of\nMLLMs. For dataset construction, we leverage pretrained LLMs to enhance\nexisting webpage-to-code datasets as well as generate a diverse pool of new\nwebpages rendered into images. Specifically, the inputs are webpage images and\ninstructions, while the responses are the webpage's HTML code. We further\ninclude diverse natural language QA pairs about the webpage content in the\nresponses to enable a more comprehensive understanding of the web content. To\nevaluate model performance in these tasks, we develop an evaluation framework\nfor testing MLLMs' abilities in webpage understanding and web-to-code\ngeneration. Extensive experiments show that our proposed dataset is beneficial\nnot only to our proposed tasks but also in the general visual domain. We hope\nour work will contribute to the development of general MLLMs suitable for\nweb-based content generation and task automation. Our data and code are\navailable at https://github.com/MBZUAI-LLM/web2code.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2024 Datasets and Benchmarks Camera-ready Version. Website at\n  https://mbzuai-llm.github.io/webpage2code/",
    "pdf_url": "http://arxiv.org/pdf/2406.20098v2",
    "published_date": "2024-06-28 17:59:46 UTC",
    "updated_date": "2024-11-17 16:11:00 UTC"
  },
  {
    "arxiv_id": "2406.20095v3",
    "title": "LLaRA: Supercharging Robot Learning Data for Vision-Language Policy",
    "authors": [
      "Xiang Li",
      "Cristina Mata",
      "Jongwoo Park",
      "Kumara Kahatapitiya",
      "Yoo Sung Jang",
      "Jinghuan Shang",
      "Kanchana Ranasinghe",
      "Ryan Burgert",
      "Mu Cai",
      "Yong Jae Lee",
      "Michael S. Ryoo"
    ],
    "abstract": "Vision Language Models (VLMs) have recently been leveraged to generate\nrobotic actions, forming Vision-Language-Action (VLA) models. However, directly\nadapting a pretrained VLM for robotic control remains challenging, particularly\nwhen constrained by a limited number of robot demonstrations. In this work, we\nintroduce LLaRA: Large Language and Robotics Assistant, a framework that\nformulates robot action policy as visuo-textual conversations and enables an\nefficient transfer of a pretrained VLM into a powerful VLA, motivated by the\nsuccess of visual instruction tuning in Computer Vision. First, we present an\nautomated pipeline to generate conversation-style instruction tuning data for\nrobots from existing behavior cloning datasets, aligning robotic actions with\nimage pixel coordinates. Further, we enhance this dataset in a self-supervised\nmanner by defining six auxiliary tasks, without requiring any additional action\nannotations. We show that a VLM finetuned with a limited amount of such\ndatasets can produce meaningful action decisions for robotic control. Through\nexperiments across multiple simulated and real-world tasks, we demonstrate that\nLLaRA achieves state-of-the-art performance while preserving the generalization\ncapabilities of large language models. The code, datasets, and pretrained\nmodels are available at https://github.com/LostXine/LLaRA.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.20095v3",
    "published_date": "2024-06-28 17:59:12 UTC",
    "updated_date": "2025-01-30 17:34:37 UTC"
  },
  {
    "arxiv_id": "2406.20087v2",
    "title": "ProgressGym: Alignment with a Millennium of Moral Progress",
    "authors": [
      "Tianyi Qiu",
      "Yang Zhang",
      "Xuchuan Huang",
      "Jasmine Xinze Li",
      "Jiaming Ji",
      "Yaodong Yang"
    ],
    "abstract": "Frontier AI systems, including large language models (LLMs), hold increasing\ninfluence over the epistemology of human users. Such influence can reinforce\nprevailing societal values, potentially contributing to the lock-in of\nmisguided moral beliefs and, consequently, the perpetuation of problematic\nmoral practices on a broad scale. We introduce progress alignment as a\ntechnical solution to mitigate this imminent risk. Progress alignment\nalgorithms learn to emulate the mechanics of human moral progress, thereby\naddressing the susceptibility of existing alignment methods to contemporary\nmoral blindspots. To empower research in progress alignment, we introduce\nProgressGym, an experimental framework allowing the learning of moral progress\nmechanics from history, in order to facilitate future progress in real-world\nmoral decisions. Leveraging 9 centuries of historical text and 18 historical\nLLMs, ProgressGym enables codification of real-world progress alignment\nchallenges into concrete benchmarks. Specifically, we introduce three core\nchallenges: tracking evolving values (PG-Follow), preemptively anticipating\nmoral progress (PG-Predict), and regulating the feedback loop between human and\nAI value shifts (PG-Coevolve). Alignment methods without a temporal dimension\nare inapplicable to these tasks. In response, we present lifelong and\nextrapolative algorithms as baseline methods of progress alignment, and build\nan open leaderboard soliciting novel algorithms and challenges. The framework\nand the leaderboard are available at\nhttps://github.com/PKU-Alignment/ProgressGym and\nhttps://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard\nrespectively.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024 Track on Datasets and Benchmarks (Spotlight)",
    "pdf_url": "http://arxiv.org/pdf/2406.20087v2",
    "published_date": "2024-06-28 17:55:24 UTC",
    "updated_date": "2024-10-31 13:10:12 UTC"
  },
  {
    "arxiv_id": "2406.20080v1",
    "title": "AI for Extreme Event Modeling and Understanding: Methodologies and Challenges",
    "authors": [
      "Gustau Camps-Valls",
      "Miguel-Ángel Fernández-Torres",
      "Kai-Hendrik Cohrs",
      "Adrian Höhl",
      "Andrea Castelletti",
      "Aytac Pacal",
      "Claire Robin",
      "Francesco Martinuzzi",
      "Ioannis Papoutsis",
      "Ioannis Prapas",
      "Jorge Pérez-Aracil",
      "Katja Weigel",
      "Maria Gonzalez-Calabuig",
      "Markus Reichstein",
      "Martin Rabel",
      "Matteo Giuliani",
      "Miguel Mahecha",
      "Oana-Iuliana Popescu",
      "Oscar J. Pellicer-Valero",
      "Said Ouala",
      "Sancho Salcedo-Sanz",
      "Sebastian Sippel",
      "Spyros Kondylatos",
      "Tamara Happé",
      "Tristan Williams"
    ],
    "abstract": "In recent years, artificial intelligence (AI) has deeply impacted various\nfields, including Earth system sciences. Here, AI improved weather forecasting,\nmodel emulation, parameter estimation, and the prediction of extreme events.\nHowever, the latter comes with specific challenges, such as developing accurate\npredictors from noisy, heterogeneous and limited annotated data. This paper\nreviews how AI is being used to analyze extreme events (like floods, droughts,\nwildfires and heatwaves), highlighting the importance of creating accurate,\ntransparent, and reliable AI models. We discuss the hurdles of dealing with\nlimited data, integrating information in real-time, deploying models, and\nmaking them understandable, all crucial for gaining the trust of stakeholders\nand meeting regulatory needs. We provide an overview of how AI can help\nidentify and explain extreme events more effectively, improving disaster\nresponse and communication. We emphasize the need for collaboration across\ndifferent fields to create AI solutions that are practical, understandable, and\ntrustworthy for analyzing and predicting extreme events. Such collaborative\nefforts aim to enhance disaster readiness and disaster risk reduction.",
    "categories": [
      "cs.AI",
      "physics.ao-ph",
      "physics.geo-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.20080v1",
    "published_date": "2024-06-28 17:45:25 UTC",
    "updated_date": "2024-06-28 17:45:25 UTC"
  },
  {
    "arxiv_id": "2406.20079v1",
    "title": "Molecular Facts: Desiderata for Decontextualization in LLM Fact Verification",
    "authors": [
      "Anisha Gunjal",
      "Greg Durrett"
    ],
    "abstract": "Automatic factuality verification of large language model (LLM) generations\nis becoming more and more widely used to combat hallucinations. A major point\nof tension in the literature is the granularity of this fact-checking: larger\nchunks of text are hard to fact-check, but more atomic facts like propositions\nmay lack context to interpret correctly. In this work, we assess the role of\ncontext in these atomic facts. We argue that fully atomic facts are not the\nright representation, and define two criteria for molecular facts:\ndecontextuality, or how well they can stand alone, and minimality, or how\nlittle extra information is added to achieve decontexuality. We quantify the\nimpact of decontextualization on minimality, then present a baseline\nmethodology for generating molecular facts automatically, aiming to add the\nright amount of information. We compare against various methods of\ndecontextualization and find that molecular facts balance minimality with fact\nverification accuracy in ambiguous settings.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.20079v1",
    "published_date": "2024-06-28 17:43:48 UTC",
    "updated_date": "2024-06-28 17:43:48 UTC"
  },
  {
    "arxiv_id": "2407.11019v2",
    "title": "Efficacy of Various Large Language Models in Generating Smart Contracts",
    "authors": [
      "Siddhartha Chatterjee",
      "Bina Ramamurthy"
    ],
    "abstract": "This study analyzes the application of code-generating Large Language Models\nin the creation of immutable Solidity smart contracts on the Ethereum\nBlockchain. Other works have previously analyzed Artificial Intelligence code\ngeneration abilities. This paper aims to expand this to a larger scope to\ninclude programs where security and efficiency are of utmost priority such as\nsmart contracts. The hypothesis leading into the study was that LLMs in general\nwould have difficulty in rigorously implementing security details in the code,\nwhich was shown through our results, but surprisingly generally succeeded in\nmany common types of contracts. We also discovered a novel way of generating\nsmart contracts through new prompting strategies.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "I.2.2"
    ],
    "primary_category": "cs.SE",
    "comment": "18 pages, accepted for presentation at 8th annual Future of\n  Information and Communication Conference",
    "pdf_url": "http://arxiv.org/pdf/2407.11019v2",
    "published_date": "2024-06-28 17:31:47 UTC",
    "updated_date": "2024-11-05 16:21:55 UTC"
  },
  {
    "arxiv_id": "2407.01619v3",
    "title": "TabSketchFM: Sketch-based Tabular Representation Learning for Data Discovery over Data Lakes",
    "authors": [
      "Aamod Khatiwada",
      "Harsha Kokel",
      "Ibrahim Abdelaziz",
      "Subhajit Chaudhury",
      "Julian Dolby",
      "Oktie Hassanzadeh",
      "Zhenhan Huang",
      "Tejaswini Pedapati",
      "Horst Samulowitz",
      "Kavitha Srinivas"
    ],
    "abstract": "Enterprises have a growing need to identify relevant tables in data lakes;\ne.g. tables that are unionable, joinable, or subsets of each other. Tabular\nneural models can be helpful for such data discovery tasks. In this paper, we\npresent TabSketchFM, a neural tabular model for data discovery over data lakes.\nFirst, we propose novel pre-training: a sketch-based approach to enhance the\neffectiveness of data discovery in neural tabular models. Second, we finetune\nthe pretrained model for identifying unionable, joinable, and subset table\npairs and show significant improvement over previous tabular neural models.\nThird, we present a detailed ablation study to highlight which sketches are\ncrucial for which tasks. Fourth, we use these finetuned models to perform table\nsearch; i.e., given a query table, find other tables in a corpus that are\nunionable, joinable, or that are subsets of the query. Our results demonstrate\nsignificant improvements in F1 scores for search compared to state-of-the-art\ntechniques. Finally, we show significant transfer across datasets and tasks\nestablishing that our model can generalize across different tasks and over\ndifferent data lakes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01619v3",
    "published_date": "2024-06-28 17:28:53 UTC",
    "updated_date": "2024-12-11 19:34:43 UTC"
  },
  {
    "arxiv_id": "2406.20053v1",
    "title": "Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation",
    "authors": [
      "Danny Halawi",
      "Alexander Wei",
      "Eric Wallace",
      "Tony T. Wang",
      "Nika Haghtalab",
      "Jacob Steinhardt"
    ],
    "abstract": "Black-box finetuning is an emerging interface for adapting state-of-the-art\nlanguage models to user needs. However, such access may also let malicious\nactors undermine model safety. To demonstrate the challenge of defending\nfinetuning interfaces, we introduce covert malicious finetuning, a method to\ncompromise model safety via finetuning while evading detection. Our method\nconstructs a malicious dataset where every individual datapoint appears\ninnocuous, but finetuning on the dataset teaches the model to respond to\nencoded harmful requests with encoded harmful responses. Applied to GPT-4, our\nmethod produces a finetuned model that acts on harmful instructions 99% of the\ntime and avoids detection by defense mechanisms such as dataset inspection,\nsafety evaluations, and input/output classifiers. Our findings question whether\nblack-box finetuning access can be secured against sophisticated adversaries.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "22 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.20053v1",
    "published_date": "2024-06-28 17:05:46 UTC",
    "updated_date": "2024-06-28 17:05:46 UTC"
  },
  {
    "arxiv_id": "2407.00147v1",
    "title": "Predicting Elevated Risk of Hospitalization Following Emergency Department Discharges",
    "authors": [
      "Dat Hong",
      "Philip M. Polgreen",
      "Alberto Maria Segre"
    ],
    "abstract": "Hospitalizations that follow closely on the heels of one or more emergency\ndepartment visits are often symptoms of missed opportunities to form a proper\ndiagnosis. These diagnostic errors imply a failure to recognize the need for\nhospitalization and deliver appropriate care, and thus also bear important\nconnotations for patient safety. In this paper, we show how data mining\ntechniques can be applied to a large existing hospitalization data set to learn\nuseful models that predict these upcoming hospitalizations with high accuracy.\nSpecifically, we use an ensemble of logistics regression, na\\\"ive Bayes and\nassociation rule classifiers to successfully predict hospitalization within 3,\n7 and 14 days of an emergency department discharge. Aside from high accuracy,\none of the advantages of the techniques proposed here is that the resulting\nclassifier is easily inspected and interpreted by humans so that the learned\nrules can be readily operationalized. These rules can then be easily\ndistributed and applied directly by physicians in emergency department settings\nto predict the risk of early admission prior to discharging their emergency\ndepartment patients.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00147v1",
    "published_date": "2024-06-28 17:01:12 UTC",
    "updated_date": "2024-06-28 17:01:12 UTC"
  },
  {
    "arxiv_id": "2406.20044v1",
    "title": "Electrostatics-based particle sampling and approximate inference",
    "authors": [
      "Yongchao Huang"
    ],
    "abstract": "A new particle-based sampling and approximate inference method, based on\nelectrostatics and Newton mechanics principles, is introduced with theoretical\nground, algorithm design and experimental validation. This method simulates an\ninteracting particle system (IPS) where particles, i.e. the freely-moving\nnegative charges and spatially-fixed positive charges with magnitudes\nproportional to the target distribution, interact with each other via\nattraction and repulsion induced by the resulting electric fields described by\nPoisson's equation. The IPS evolves towards a steady-state where the\ndistribution of negative charges conforms to the target distribution. This\nphysics-inspired method offers deterministic, gradient-free sampling and\ninference, achieving comparable performance as other particle-based and MCMC\nmethods in benchmark tasks of inferring complex densities, Bayesian logistic\nregression and dynamical system identification. A discrete-time, discrete-space\nalgorithmic design, readily extendable to continuous time and space, is\nprovided for usage in more general inference problems occurring in\nprobabilistic machine learning scenarios such as Bayesian inference, generative\nmodelling, and beyond.",
    "categories": [
      "cs.AI",
      "stat.CO",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.20044v1",
    "published_date": "2024-06-28 16:53:06 UTC",
    "updated_date": "2024-06-28 16:53:06 UTC"
  },
  {
    "arxiv_id": "2407.17476v1",
    "title": "ORCDF: An Oversmoothing-Resistant Cognitive Diagnosis Framework for Student Learning in Online Education Systems",
    "authors": [
      "Hong Qian",
      "Shuo Liu",
      "Mingjia Li",
      "Bingdong Li",
      "Zhi Liu",
      "Aimin Zhou"
    ],
    "abstract": "Cognitive diagnosis models (CDMs) are designed to learn students' mastery\nlevels using their response logs. CDMs play a fundamental role in online\neducation systems since they significantly influence downstream applications\nsuch as teachers' guidance and computerized adaptive testing. Despite the\nsuccess achieved by existing CDMs, we find that they suffer from a thorny issue\nthat the learned students' mastery levels are too similar. This issue, which we\nrefer to as oversmoothing, could diminish the CDMs' effectiveness in downstream\ntasks. CDMs comprise two core parts: learning students' mastery levels and\nassessing mastery levels by fitting the response logs. This paper contends that\nthe oversmoothing issue arises from that existing CDMs seldom utilize response\nsignals on exercises in the learning part but only use them as labels in the\nassessing part. To this end, this paper proposes an oversmoothing-resistant\ncognitive diagnosis framework (ORCDF) to enhance existing CDMs by utilizing\nresponse signals in the learning part. Specifically, ORCDF introduces a novel\nresponse graph to inherently incorporate response signals as types of edges.\nThen, ORCDF designs a tailored response-aware graph convolution network (RGC)\nthat effectively captures the crucial response signals within the response\ngraph. Via ORCDF, existing CDMs are enhanced by replacing the input embeddings\nwith the outcome of RGC, allowing for the consideration of response signals on\nexercises in the learning part. Extensive experiments on real-world datasets\nshow that ORCDF not only helps existing CDMs alleviate the oversmoothing issue\nbut also significantly enhances the models' prediction and interpretability\nperformance. Moreover, the effectiveness of ORCDF is validated in the\ndownstream task of computerized adaptive testing.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.17476v1",
    "published_date": "2024-06-28 16:42:53 UTC",
    "updated_date": "2024-06-28 16:42:53 UTC"
  },
  {
    "arxiv_id": "2406.20041v3",
    "title": "BMW Agents -- A Framework For Task Automation Through Multi-Agent Collaboration",
    "authors": [
      "Noel Crawford",
      "Edward B. Duffy",
      "Iman Evazzade",
      "Torsten Foehr",
      "Gregory Robbins",
      "Debbrata Kumar Saha",
      "Jiya Varma",
      "Marcin Ziolkowski"
    ],
    "abstract": "Autonomous agents driven by Large Language Models (LLMs) offer enormous\npotential for automation. Early proof of this technology can be found in\nvarious demonstrations of agents solving complex tasks, interacting with\nexternal systems to augment their knowledge, and triggering actions. In\nparticular, workflows involving multiple agents solving complex tasks in a\ncollaborative fashion exemplify their capacity to operate in less strict and\nless well-defined environments. Thus, a multi-agent approach has great\npotential for serving as a backbone in many industrial applications, ranging\nfrom complex knowledge retrieval systems to next generation robotic process\nautomation. Given the reasoning abilities within the current generation of\nLLMs, complex processes require a multi-step approach that includes a plan of\nwell-defined and modular tasks. Depending on the level of complexity, these\ntasks can be executed either by a single agent or a group of agents. In this\nwork, we focus on designing a flexible agent engineering framework with careful\nattention to planning and execution, capable of handling complex use case\napplications across various domains. The proposed framework provides\nreliability in industrial applications and presents techniques to ensure a\nscalable, flexible, and collaborative workflow for multiple autonomous agents\nworking together towards solving tasks.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "24 pages. 21 PDF images",
    "pdf_url": "http://arxiv.org/pdf/2406.20041v3",
    "published_date": "2024-06-28 16:39:20 UTC",
    "updated_date": "2024-07-02 11:45:05 UTC"
  },
  {
    "arxiv_id": "2407.00146v1",
    "title": "The Qiyas Benchmark: Measuring ChatGPT Mathematical and Language Understanding in Arabic",
    "authors": [
      "Shahad Al-Khalifa",
      "Hend Al-Khalifa"
    ],
    "abstract": "Despite the growing importance of Arabic as a global language, there is a\nnotable lack of language models pre-trained exclusively on Arabic data. This\nshortage has led to limited benchmarks available for assessing language model\nperformance in Arabic. To address this gap, we introduce two novel benchmarks\ndesigned to evaluate models' mathematical reasoning and language understanding\nabilities in Arabic. These benchmarks are derived from a General Aptitude Test\n(GAT) called Qiyas exam, a standardized test widely used for university\nadmissions in Saudi Arabia. For validation purposes, we assess the performance\nof ChatGPT-3.5-trubo and ChatGPT-4 on our benchmarks. Our findings reveal that\nthese benchmarks pose a significant challenge, with ChatGPT-4 achieving an\noverall average accuracy of 64%, while ChatGPT-3.5-trubo achieved an overall\naccuracy of 49% across the various question types in the Qiyas benchmark. We\nbelieve the release of these benchmarks will pave the way for enhancing the\nmathematical reasoning and language understanding capabilities of future models\ntailored for the low-resource Arabic language.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00146v1",
    "published_date": "2024-06-28 16:34:31 UTC",
    "updated_date": "2024-06-28 16:34:31 UTC"
  },
  {
    "arxiv_id": "2407.09556v1",
    "title": "Explainable Image Captioning using CNN- CNN architecture and Hierarchical Attention",
    "authors": [
      "Rishi Kesav Mohan",
      "Sanjay Sureshkumar",
      "Vignesh Sivasubramaniam"
    ],
    "abstract": "Image captioning is a technology that produces text-based descriptions for an\nimage. Deep learning-based solutions built on top of feature recognition may\nvery well serve the purpose. But as with any other machine learning solution,\nthe user understanding in the process of caption generation is poor and the\nmodel does not provide any explanation for its predictions and hence the\nconventional methods are also referred to as Black-Box methods. Thus, an\napproach where the model's predictions are trusted by the user is needed to\nappreciate interoperability. Explainable AI is an approach where a conventional\nmethod is approached in a way that the model or the algorithm's predictions can\nbe explainable and justifiable. Thus, this article tries to approach image\ncaptioning using Explainable AI such that the resulting captions generated by\nthe model can be Explained and visualized. A newer architecture with a CNN\ndecoder and hierarchical attention concept has been used to increase speed and\naccuracy of caption generation. Also, incorporating explainability to a model\nmakes it more trustable when used in an application. The model is trained and\nevaluated using MSCOCO dataset and both quantitative and qualitative results\nare presented in this article.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CV",
    "comment": "23 pages,9 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.09556v1",
    "published_date": "2024-06-28 16:27:47 UTC",
    "updated_date": "2024-06-28 16:27:47 UTC"
  },
  {
    "arxiv_id": "2406.20031v1",
    "title": "Pairwise Difference Learning for Classification",
    "authors": [
      "Mohamed Karim Belaid",
      "Maximilian Rabus",
      "Eyke Hüllermeier"
    ],
    "abstract": "Pairwise difference learning (PDL) has recently been introduced as a new\nmeta-learning technique for regression. Instead of learning a mapping from\ninstances to outcomes in the standard way, the key idea is to learn a function\nthat takes two instances as input and predicts the difference between the\nrespective outcomes. Given a function of this kind, predictions for a query\ninstance are derived from every training example and then averaged. This paper\nextends PDL toward the task of classification and proposes a meta-learning\ntechnique for inducing a PDL classifier by solving a suitably defined (binary)\nclassification problem on a paired version of the original training data. We\nanalyze the performance of the PDL classifier in a large-scale empirical study\nand find that it outperforms state-of-the-art methods in terms of prediction\nperformance. Last but not least, we provide an easy-to-use and publicly\navailable implementation of PDL in a Python package.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.20031v1",
    "published_date": "2024-06-28 16:20:22 UTC",
    "updated_date": "2024-06-28 16:20:22 UTC"
  },
  {
    "arxiv_id": "2406.20015v2",
    "title": "ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models",
    "authors": [
      "Yuxiang Zhang",
      "Jing Chen",
      "Junjie Wang",
      "Yaxin Liu",
      "Cheng Yang",
      "Chufan Shi",
      "Xinyu Zhu",
      "Zihao Lin",
      "Hanwen Wan",
      "Yujiu Yang",
      "Tetsuya Sakai",
      "Tian Feng",
      "Hayato Yamana"
    ],
    "abstract": "Tool-augmented large language models (LLMs) are rapidly being integrated into\nreal-world applications. Due to the lack of benchmarks, the community has yet\nto fully understand the hallucination issues within these models. To address\nthis challenge, we introduce a comprehensive diagnostic benchmark, ToolBH.\nSpecifically, we assess the LLM's hallucinations through two perspectives:\ndepth and breadth. In terms of depth, we propose a multi-level diagnostic\nprocess, including (1) solvability detection, (2) solution planning, and (3)\nmissing-tool analysis. For breadth, we consider three scenarios based on the\ncharacteristics of the toolset: missing necessary tools, potential tools, and\nlimited functionality tools. Furthermore, we developed seven tasks and\ncollected 700 evaluation samples through multiple rounds of manual annotation.\nThe results show the significant challenges presented by the ToolBH benchmark.\nThe current advanced models Gemini-1.5-Pro and GPT-4o only achieve total scores\nof 45.3 and 37.0, respectively, on a scale of 100. In this benchmark, larger\nmodel parameters do not guarantee better performance; the training data and\nresponse strategies also play crucial roles in tool-enhanced LLM scenarios. Our\ndiagnostic analysis indicates that the primary reason for model errors lies in\nassessing task solvability. Additionally, open-weight models suffer from\nperformance drops with verbose replies, whereas proprietary models excel with\nlonger reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.20015v2",
    "published_date": "2024-06-28 16:03:30 UTC",
    "updated_date": "2024-10-04 07:51:29 UTC"
  },
  {
    "arxiv_id": "2407.00142v1",
    "title": "Graph Neural Networks for Gut Microbiome Metaomic data: A preliminary work",
    "authors": [
      "Christopher Irwin",
      "Flavio Mignone",
      "Stefania Montani",
      "Luigi Portinale"
    ],
    "abstract": "The gut microbiome, crucial for human health, presents challenges in\nanalyzing its complex metaomic data due to high dimensionality and sparsity.\nTraditional methods struggle to capture its intricate relationships. We\ninvestigate graph neural networks (GNNs) for this task, aiming to derive\nmeaningful representations of individual gut microbiomes. Unlike methods\nrelying solely on taxa abundance, we directly leverage phylogenetic\nrelationships, in order to obtain a generalized encoder for taxa networks. The\nrepresentation learnt from the encoder are then used to train a model for\nphenotype prediction such as Inflammatory Bowel Disease (IBD).",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00142v1",
    "published_date": "2024-06-28 15:53:36 UTC",
    "updated_date": "2024-06-28 15:53:36 UTC"
  },
  {
    "arxiv_id": "2406.19997v2",
    "title": "Wavelets Are All You Need for Autoregressive Image Generation",
    "authors": [
      "Wael Mattar",
      "Idan Levy",
      "Nir Sharon",
      "Shai Dekel"
    ],
    "abstract": "In this paper, we take a new approach to autoregressive image generation that\nis based on two main ingredients. The first is wavelet image coding, which\nallows to tokenize the visual details of an image from coarse to fine details\nby ordering the information starting with the most significant bits of the most\nsignificant wavelet coefficients. The second is a variant of a language\ntransformer whose architecture is re-designed and optimized for token sequences\nin this 'wavelet language'. The transformer learns the significant statistical\ncorrelations within a token sequence, which are the manifestations of\nwell-known correlations between the wavelet subbands at various resolutions. We\nshow experimental results with conditioning on the generation process.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "65T60",
      "I.4.2; I.4.5; I.4.10"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.19997v2",
    "published_date": "2024-06-28 15:32:59 UTC",
    "updated_date": "2024-11-19 12:28:19 UTC"
  },
  {
    "arxiv_id": "2406.19995v1",
    "title": "Single Parent Family: A Spectrum of Family Members from a Single Pre-Trained Foundation Model",
    "authors": [
      "Habib Hajimolahoseini",
      "Mohammad Hassanpour",
      "Foozhan Ataiefard",
      "Boxing Chen",
      "Yang Liu"
    ],
    "abstract": "This paper introduces a novel method of Progressive Low Rank Decomposition\n(PLRD) tailored for the compression of large language models. Our approach\nleverages a pre-trained model, which is then incrementally decompressed to\nsmaller sizes using progressively lower ranks. This method allows for\nsignificant reductions in computational overhead and energy consumption, as\nsubsequent models are derived from the original without the need for retraining\nfrom scratch. We detail the implementation of PLRD, which strategically\ndecreases the tensor ranks, thus optimizing the trade-off between model\nperformance and resource usage. The efficacy of PLRD is demonstrated through\nextensive experiments showing that models trained with PLRD method on only 1B\ntokens maintain comparable performance with traditionally trained models while\nusing 0.1% of the tokens. The versatility of PLRD is highlighted by its ability\nto generate multiple model sizes from a single foundational model, adapting\nfluidly to varying computational and memory budgets. Our findings suggest that\nPLRD could set a new standard for the efficient scaling of LLMs, making\nadvanced AI more feasible on diverse platforms.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.19995v1",
    "published_date": "2024-06-28 15:27:57 UTC",
    "updated_date": "2024-06-28 15:27:57 UTC"
  },
  {
    "arxiv_id": "2407.00141v1",
    "title": "Towards Secure and Efficient Data Scheduling for Vehicular Social Networks",
    "authors": [
      "Youhua Xia",
      "Tiehua Zhang",
      "Jiong Jin",
      "Ying He",
      "Fei Yu"
    ],
    "abstract": "Efficient data transmission scheduling within vehicular environments poses a\nsignificant challenge due to the high mobility of such networks. Contemporary\nresearch predominantly centers on crafting cooperative scheduling algorithms\ntailored for vehicular networks. Notwithstanding, the intricacies of\norchestrating scheduling in vehicular social networks both effectively and\nefficiently remain formidable. This paper introduces an innovative\nlearning-based algorithm for scheduling data transmission that prioritizes\nefficiency and security within vehicular social networks. The algorithm first\nuses a specifically constructed neural network to enhance data processing\ncapabilities. After this, it incorporates a Q-learning paradigm during the data\ntransmission phase to optimize the information exchange, the privacy of which\nis safeguarded by differential privacy through the communication process.\nComparative experiments demonstrate the superior performance of the proposed\nQ-learning enhanced scheduling algorithm relative to existing state-of-the-art\nscheduling algorithms in the context of vehicular social networks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00141v1",
    "published_date": "2024-06-28 15:20:50 UTC",
    "updated_date": "2024-06-28 15:20:50 UTC"
  },
  {
    "arxiv_id": "2406.19967v1",
    "title": "Into the Unknown: Generating Geospatial Descriptions for New Environments",
    "authors": [
      "Tzuf Paz-Argaman",
      "John Palowitch",
      "Sayali Kulkarni",
      "Reut Tsarfaty",
      "Jason Baldridge"
    ],
    "abstract": "Similar to vision-and-language navigation (VLN) tasks that focus on bridging\nthe gap between vision and language for embodied navigation, the new Rendezvous\n(RVS) task requires reasoning over allocentric spatial relationships\n(independent of the observer's viewpoint) using non-sequential navigation\ninstructions and maps. However, performance substantially drops in new\nenvironments with no training data. Using opensource descriptions paired with\ncoordinates (e.g., Wikipedia) provides training data but suffers from limited\nspatially-oriented text resulting in low geolocation resolution. We propose a\nlarge-scale augmentation method for generating high-quality synthetic data for\nnew environments using readily available geospatial data. Our method constructs\na grounded knowledge-graph, capturing entity relationships. Sampled entities\nand relations (`shop north of school') generate navigation instructions via (i)\ngenerating numerous templates using context-free grammar (CFG) to embed\nspecific entities and relations; (ii) feeding the entities and relation into a\nlarge language model (LLM) for instruction generation. A comprehensive\nevaluation on RVS, showed that our approach improves the 100-meter accuracy by\n45.83% on unseen environments. Furthermore, we demonstrate that models trained\nwith CFG-based augmentation achieve superior performance compared with those\ntrained with LLM-based augmentation, both in unseen and seen environments.\nThese findings suggest that the potential advantages of explicitly structuring\nspatial information for text-based geospatial reasoning in previously unknown,\ncan unlock data-scarce scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.19967v1",
    "published_date": "2024-06-28 14:56:21 UTC",
    "updated_date": "2024-06-28 14:56:21 UTC"
  },
  {
    "arxiv_id": "2406.19963v3",
    "title": "Text2Robot: Evolutionary Robot Design from Text Descriptions",
    "authors": [
      "Ryan P. Ringel",
      "Zachary S. Charlick",
      "Jiaxun Liu",
      "Boxi Xia",
      "Boyuan Chen"
    ],
    "abstract": "Robot design has traditionally been costly and labor-intensive. Despite\nadvancements in automated processes, it remains challenging to navigate a vast\ndesign space while producing physically manufacturable robots. We introduce\nText2Robot, a framework that converts user text specifications and performance\npreferences into physical quadrupedal robots. Within minutes, Text2Robot can\nuse text-to-3D models to provide strong initializations of diverse\nmorphologies. Within a day, our geometric processing algorithms and\nbody-control co-optimization produce a walking robot by explicitly considering\nreal-world electronics and manufacturability. Text2Robot enables rapid\nprototyping and opens new opportunities for robot design with generative\nmodels.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Our project website is at: http://generalroboticslab.com/Text2Robot",
    "pdf_url": "http://arxiv.org/pdf/2406.19963v3",
    "published_date": "2024-06-28 14:51:01 UTC",
    "updated_date": "2025-02-26 02:47:08 UTC"
  },
  {
    "arxiv_id": "2406.19953v1",
    "title": "Uncovering the hidden core-periphery structure in hyperbolic networks",
    "authors": [
      "Imran Ansari",
      "Pawanesh Yadav",
      "Niteesh Sahni"
    ],
    "abstract": "The hyperbolic network models exhibit very fundamental and essential\nfeatures, like small-worldness, scale-freeness, high-clustering coefficient,\nand community structure. In this paper, we comprehensively explore the presence\nof an important feature, the core-periphery structure, in the hyperbolic\nnetwork models, which is often exhibited by real-world networks. We focused on\nwell-known hyperbolic models such as popularity-similarity optimization model\n(PSO) and S1/H2 models and studied core-periphery structures using a\nwell-established method that is based on standard random walk Markov chain\nmodel. The observed core-periphery centralization values indicate that the\ncore-periphery structure can be very pronounced under certain conditions. We\nalso validate our findings by statistically testing for the significance of the\nobserved core-periphery structure in the network geometry. This study extends\nnetwork science and reveals core-periphery insights applicable to various\ndomains, enhancing network performance and resiliency in transportation and\ninformation systems.",
    "categories": [
      "physics.soc-ph",
      "cs.AI"
    ],
    "primary_category": "physics.soc-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.19953v1",
    "published_date": "2024-06-28 14:39:21 UTC",
    "updated_date": "2024-06-28 14:39:21 UTC"
  },
  {
    "arxiv_id": "2407.00138v1",
    "title": "Analyzing Quality, Bias, and Performance in Text-to-Image Generative Models",
    "authors": [
      "Nila Masrourisaadat",
      "Nazanin Sedaghatkish",
      "Fatemeh Sarshartehrani",
      "Edward A. Fox"
    ],
    "abstract": "Advances in generative models have led to significant interest in image\nsynthesis, demonstrating the ability to generate high-quality images for a\ndiverse range of text prompts. Despite this progress, most studies ignore the\npresence of bias. In this paper, we examine several text-to-image models not\nonly by qualitatively assessing their performance in generating accurate images\nof human faces, groups, and specified numbers of objects but also by presenting\na social bias analysis. As expected, models with larger capacity generate\nhigher-quality images. However, we also document the inherent gender or social\nbiases these models possess, offering a more complete understanding of their\nimpact and limitations.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "I.2.6; I.2.10; I.2.7; I.4.10"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.00138v1",
    "published_date": "2024-06-28 14:10:42 UTC",
    "updated_date": "2024-06-28 14:10:42 UTC"
  },
  {
    "arxiv_id": "2406.19934v2",
    "title": "From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis",
    "authors": [
      "Chuanqi Cheng",
      "Jian Guan",
      "Wei Wu",
      "Rui Yan"
    ],
    "abstract": "We explore multi-step reasoning in vision-language models (VLMs). The problem\nis challenging, as reasoning data consisting of multiple steps of visual and\nlanguage processing are barely available. To overcome the challenge, we first\nintroduce a least-to-most visual reasoning paradigm, which interleaves steps of\ndecomposing a question into sub-questions and invoking external tools for\nresolving sub-questions. Based on the paradigm, we further propose a novel data\nsynthesis approach that can automatically create questions and multi-step\nreasoning paths for an image in a bottom-up manner. Our approach divides the\ncomplex synthesis task into a few simple sub-tasks, and (almost entirely)\nrelies on open-sourced models to accomplish the sub-tasks. Therefore, the\nentire synthesis process is reproducible and cost-efficient, and the\nsynthesized data is quality guaranteed. With the approach, we construct $50$k\nvisual reasoning examples. Then, we develop a visual reasoner through\nsupervised fine-tuning, which is capable of generally enhancing the reasoning\nabilities of a wide range of existing VLMs in a plug-and-play fashion.\nExtensive experiments indicate that the visual reasoner can consistently and\nsignificantly improve four VLMs on four VQA benchmarks. Our code and dataset\nare available at https://github.com/steven-ccq/VisualReasoner.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.19934v2",
    "published_date": "2024-06-28 14:04:10 UTC",
    "updated_date": "2024-10-11 15:41:23 UTC"
  },
  {
    "arxiv_id": "2406.19931v2",
    "title": "Decoupling General and Personalized Knowledge in Federated Learning via Additive and Low-Rank Decomposition",
    "authors": [
      "Xinghao Wu",
      "Xuefeng Liu",
      "Jianwei Niu",
      "Haolin Wang",
      "Shaojie Tang",
      "Guogang Zhu",
      "Hao Su"
    ],
    "abstract": "To address data heterogeneity, the key strategy of Personalized Federated\nLearning (PFL) is to decouple general knowledge (shared among clients) and\nclient-specific knowledge, as the latter can have a negative impact on\ncollaboration if not removed. Existing PFL methods primarily adopt a parameter\npartitioning approach, where the parameters of a model are designated as one of\ntwo types: parameters shared with other clients to extract general knowledge\nand parameters retained locally to learn client-specific knowledge. However, as\nthese two types of parameters are put together like a jigsaw puzzle into a\nsingle model during the training process, each parameter may simultaneously\nabsorb both general and client-specific knowledge, thus struggling to separate\nthe two types of knowledge effectively. In this paper, we introduce FedDecomp,\na simple but effective PFL paradigm that employs parameter additive\ndecomposition to address this issue. Instead of assigning each parameter of a\nmodel as either a shared or personalized one, FedDecomp decomposes each\nparameter into the sum of two parameters: a shared one and a personalized one,\nthus achieving a more thorough decoupling of shared and personalized knowledge\ncompared to the parameter partitioning method. In addition, as we find that\nretaining local knowledge of specific clients requires much lower model\ncapacity compared with general knowledge across all clients, we let the matrix\ncontaining personalized parameters be low rank during the training process.\nMoreover, a new alternating training strategy is proposed to further improve\nthe performance. Experimental results across multiple datasets and varying\ndegrees of data heterogeneity demonstrate that FedDecomp outperforms\nstate-of-the-art methods up to 4.9\\%. The code is available at\nhttps://github.com/XinghaoWu/FedDecomp.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ACM MM 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.19931v2",
    "published_date": "2024-06-28 14:01:22 UTC",
    "updated_date": "2024-10-11 11:30:01 UTC"
  },
  {
    "arxiv_id": "2406.19896v1",
    "title": "AuthAttLyzer-V2: Unveiling Code Authorship Attribution using Enhanced Ensemble Learning Models & Generating Benchmark Dataset",
    "authors": [
      "Bhaskar Joshi",
      "Sepideh HajiHossein Khani",
      "Arash HabibiLashkari"
    ],
    "abstract": "Source Code Authorship Attribution (SCAA) is crucial for software\nclassification because it provides insights into the origin and behavior of\nsoftware. By accurately identifying the author or group behind a piece of code,\nexperts can better understand the motivations and techniques of developers. In\nthe cybersecurity era, this attribution helps trace the source of malicious\nsoftware, identify patterns in the code that may indicate specific threat\nactors or groups, and ultimately enhance threat intelligence and mitigation\nstrategies. This paper presents AuthAttLyzer-V2, a new source code feature\nextractor for SCAA, focusing on lexical, semantic, syntactic, and N-gram\nfeatures. Our research explores author identification in C++ by examining\n24,000 source code samples from 3,000 authors. Our methodology integrates\nRandom Forest, Gradient Boosting, and XGBoost models, enhanced with SHAP for\ninterpretability. The study demonstrates how ensemble models can effectively\ndiscern individual coding styles, offering insights into the unique attributes\nof code authorship. This approach is pivotal in understanding and interpreting\ncomplex patterns in authorship attribution, especially for malware\nclassification.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.19896v1",
    "published_date": "2024-06-28 13:04:16 UTC",
    "updated_date": "2024-06-28 13:04:16 UTC"
  },
  {
    "arxiv_id": "2406.19888v1",
    "title": "Fine-tuning of Geospatial Foundation Models for Aboveground Biomass Estimation",
    "authors": [
      "Michal Muszynski",
      "Levente Klein",
      "Ademir Ferreira da Silva",
      "Anjani Prasad Atluri",
      "Carlos Gomes",
      "Daniela Szwarcman",
      "Gurkanwar Singh",
      "Kewen Gu",
      "Maciel Zortea",
      "Naomi Simumba",
      "Paolo Fraccaro",
      "Shraddha Singh",
      "Steve Meliksetian",
      "Campbell Watson",
      "Daiki Kimura",
      "Harini Srinivasan"
    ],
    "abstract": "Global vegetation structure mapping is critical for understanding the global\ncarbon cycle and maximizing the efficacy of nature-based carbon sequestration\ninitiatives. Moreover, vegetation structure mapping can help reduce the impacts\nof climate change by, for example, guiding actions to improve water security,\nincrease biodiversity and reduce flood risk. Global satellite measurements\nprovide an important set of observations for monitoring and managing\ndeforestation and degradation of existing forests, natural forest regeneration,\nreforestation, biodiversity restoration, and the implementation of sustainable\nagricultural practices. In this paper, we explore the effectiveness of\nfine-tuning of a geospatial foundation model to estimate above-ground biomass\n(AGB) using space-borne data collected across different eco-regions in Brazil.\nThe fine-tuned model architecture consisted of a Swin-B transformer as the\nencoder (i.e., backbone) and a single convolutional layer for the decoder head.\nAll results were compared to a U-Net which was trained as the baseline model\nExperimental results of this sparse-label prediction task demonstrate that the\nfine-tuned geospatial foundation model with a frozen encoder has comparable\nperformance to a U-Net trained from scratch. This is despite the fine-tuned\nmodel having 13 times less parameters requiring optimization, which saves both\ntime and compute resources. Further, we explore the transfer-learning\ncapabilities of the geospatial foundation models by fine-tuning on satellite\nimagery with sparse labels from different eco-regions in Brazil.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.19888v1",
    "published_date": "2024-06-28 12:54:10 UTC",
    "updated_date": "2024-06-28 12:54:10 UTC"
  },
  {
    "arxiv_id": "2406.19874v2",
    "title": "Detecting Subtle Differences between Human and Model Languages Using Spectrum of Relative Likelihood",
    "authors": [
      "Yang Xu",
      "Yu Wang",
      "Hao An",
      "Zhichen Liu",
      "Yongyuan Li"
    ],
    "abstract": "Human and model-generated texts can be distinguished by examining the\nmagnitude of likelihood in language. However, it is becoming increasingly\ndifficult as language model's capabilities of generating human-like texts keep\nevolving. This study provides a new perspective by using the relative\nlikelihood values instead of absolute ones, and extracting useful features from\nthe spectrum-view of likelihood for the human-model text detection task. We\npropose a detection procedure with two classification methods, supervised and\nheuristic-based, respectively, which results in competitive performances with\nprevious zero-shot detection methods and a new state-of-the-art on short-text\ndetection. Our method can also reveal subtle differences between human and\nmodel languages, which find theoretical roots in psycholinguistics studies. Our\ncode is available at https://github.com/CLCS-SUSTech/FourierGPT",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.19874v2",
    "published_date": "2024-06-28 12:28:52 UTC",
    "updated_date": "2024-10-09 09:36:49 UTC"
  },
  {
    "arxiv_id": "2406.19859v4",
    "title": "MetaDesigner: Advancing Artistic Typography Through AI-Driven, User-Centric, and Multilingual WordArt Synthesis",
    "authors": [
      "Jun-Yan He",
      "Zhi-Qi Cheng",
      "Chenyang Li",
      "Jingdong Sun",
      "Qi He",
      "Wangmeng Xiang",
      "Hanyuan Chen",
      "Jin-Peng Lan",
      "Xianhui Lin",
      "Kang Zhu",
      "Bin Luo",
      "Yifeng Geng",
      "Xuansong Xie",
      "Alexander G. Hauptmann"
    ],
    "abstract": "MetaDesigner introduces a transformative framework for artistic typography\nsynthesis, powered by Large Language Models (LLMs) and grounded in a\nuser-centric design paradigm. Its foundation is a multi-agent system comprising\nthe Pipeline, Glyph, and Texture agents, which collectively orchestrate the\ncreation of customizable WordArt, ranging from semantic enhancements to\nintricate textural elements. A central feedback mechanism leverages insights\nfrom both multimodal models and user evaluations, enabling iterative refinement\nof design parameters. Through this iterative process, MetaDesigner dynamically\nadjusts hyperparameters to align with user-defined stylistic and thematic\npreferences, consistently delivering WordArt that excels in visual quality and\ncontextual resonance. Empirical evaluations underscore the system's versatility\nand effectiveness across diverse WordArt applications, yielding outputs that\nare both aesthetically compelling and context-sensitive.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.MM"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by ICLR 2025, Project:\n  https://modelscope.cn/studios/WordArt/WordArt",
    "pdf_url": "http://arxiv.org/pdf/2406.19859v4",
    "published_date": "2024-06-28 11:58:26 UTC",
    "updated_date": "2025-02-27 08:36:29 UTC"
  },
  {
    "arxiv_id": "2406.19853v1",
    "title": "YuLan: An Open-source Large Language Model",
    "authors": [
      "Yutao Zhu",
      "Kun Zhou",
      "Kelong Mao",
      "Wentong Chen",
      "Yiding Sun",
      "Zhipeng Chen",
      "Qian Cao",
      "Yihan Wu",
      "Yushuo Chen",
      "Feng Wang",
      "Lei Zhang",
      "Junyi Li",
      "Xiaolei Wang",
      "Lei Wang",
      "Beichen Zhang",
      "Zican Dong",
      "Xiaoxue Cheng",
      "Yuhan Chen",
      "Xinyu Tang",
      "Yupeng Hou",
      "Qiangqiang Ren",
      "Xincheng Pang",
      "Shufang Xie",
      "Wayne Xin Zhao",
      "Zhicheng Dou",
      "Jiaxin Mao",
      "Yankai Lin",
      "Ruihua Song",
      "Jun Xu",
      "Xu Chen",
      "Rui Yan",
      "Zhewei Wei",
      "Di Hu",
      "Wenbing Huang",
      "Ze-Feng Gao",
      "Yueguo Chen",
      "Weizheng Lu",
      "Ji-Rong Wen"
    ],
    "abstract": "Large language models (LLMs) have become the foundation of many applications,\nleveraging their extensive capabilities in processing and understanding natural\nlanguage. While many open-source LLMs have been released with technical\nreports, the lack of training details hinders further research and development.\nThis paper presents the development of YuLan, a series of open-source LLMs with\n$12$ billion parameters. The base model of YuLan is pre-trained on\napproximately $1.7$T tokens derived from a diverse corpus, including massive\nEnglish, Chinese, and multilingual texts. We design a three-stage pre-training\nmethod to enhance YuLan's overall capabilities. Subsequent phases of training\nincorporate instruction-tuning and human alignment, employing a substantial\nvolume of high-quality synthesized data. To facilitate the learning of complex\nand long-tail knowledge, we devise a curriculum-learning framework throughout\nacross these stages, which helps LLMs learn knowledge in an easy-to-hard\nmanner. YuLan's training is finished on Jan, 2024 and has achieved performance\non par with state-of-the-art LLMs across various English and Chinese\nbenchmarks. This paper outlines a comprehensive technical roadmap for\ndeveloping LLMs from scratch. Our model and codes are available at\nhttps://github.com/RUC-GSAI/YuLan-Chat.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.19853v1",
    "published_date": "2024-06-28 11:52:53 UTC",
    "updated_date": "2024-06-28 11:52:53 UTC"
  },
  {
    "arxiv_id": "2406.19840v1",
    "title": "AnomaLLMy -- Detecting anomalous tokens in black-box LLMs through low-confidence single-token predictions",
    "authors": [
      "Waligóra Witold"
    ],
    "abstract": "This paper introduces AnomaLLMy, a novel technique for the automatic\ndetection of anomalous tokens in black-box Large Language Models (LLMs) with\nAPI-only access. Utilizing low-confidence single-token predictions as a\ncost-effective indicator, AnomaLLMy identifies irregularities in model\nbehavior, addressing the issue of anomalous tokens degrading the quality and\nreliability of models. Validated on the cl100k_base dataset, the token set of\nGPT-4, AnomaLLMy detected 413 major and 65 minor anomalies, demonstrating the\nmethod's efficiency with just \\$24.39 spent in API credits. The insights from\nthis research are expected to be beneficial for enhancing the robustness of and\naccuracy of LLMs, particularly in the development and assessment of tokenizers.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.19840v1",
    "published_date": "2024-06-28 11:28:44 UTC",
    "updated_date": "2024-06-28 11:28:44 UTC"
  },
  {
    "arxiv_id": "2407.12027v1",
    "title": "Idle is the New Sleep: Configuration-Aware Alternative to Powering Off FPGA-Based DL Accelerators During Inactivity",
    "authors": [
      "Chao Qian",
      "Christopher Cichiwskyj",
      "Tianheng Ling",
      "Gregor Schiele"
    ],
    "abstract": "In the rapidly evolving Internet of Things (IoT) domain, we concentrate on\nenhancing energy efficiency in Deep Learning accelerators on FPGA-based\nheterogeneous platforms, aligning with the principles of sustainable computing.\nInstead of focusing on the inference phase, we introduce innovative\noptimizations to minimize the overhead of the FPGA configuration phase. By\nfine-tuning configuration parameters correctly, we achieved a 40.13-fold\nreduction in configuration energy. Moreover, augmented with power-saving\nmethods, our Idle-Waiting strategy outperformed the traditional On-Off strategy\nin duty-cycle mode for request periods up to 499.06 ms. Specifically, at a 40\nms request period within a 4147 J energy budget, this strategy extends the\nsystem lifetime to approximately 12.39x that of the On-Off strategy.\nEmpirically validated through hardware measurements and simulations, these\noptimizations provide valuable insights and practical methods for achieving\nenergy-efficient and sustainable deployments in IoT.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "Accepted by 37th GI/ITG International Conference on Architecture of\n  Computing Systems (ARCS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.12027v1",
    "published_date": "2024-06-28 11:22:12 UTC",
    "updated_date": "2024-06-28 11:22:12 UTC"
  },
  {
    "arxiv_id": "2407.12026v1",
    "title": "The Pitfalls of Publishing in the Age of LLMs: Strange and Surprising Adventures with a High-Impact NLP Journal",
    "authors": [
      "Rakesh M. Verma",
      "Nachum Dershowitz"
    ],
    "abstract": "We show the fraught side of the academic publishing realm and illustrate it\nthrough a recent case study with an NLP journal.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12026v1",
    "published_date": "2024-06-28 10:58:42 UTC",
    "updated_date": "2024-06-28 10:58:42 UTC"
  },
  {
    "arxiv_id": "2407.12025v1",
    "title": "LLM4DESIGN: An Automated Multi-Modal System for Architectural and Environmental Design",
    "authors": [
      "Ran Chen",
      "Xueqi Yao",
      "Xuhui Jiang"
    ],
    "abstract": "This study introduces LLM4DESIGN, a highly automated system for generating\narchitectural and environmental design proposals. LLM4DESIGN, relying solely on\nsite conditions and design requirements, employs Multi-Agent systems to foster\ncreativity, Retrieval Augmented Generation (RAG) to ground designs in realism,\nand Visual Language Models (VLM) to synchronize all information. This system\nresulting in coherent, multi-illustrated, and multi-textual design schemes. The\nsystem meets the dual needs of narrative storytelling and objective drawing\npresentation in generating architectural and environmental design proposals.\nExtensive comparative and ablation experiments confirm the innovativeness of\nLLM4DESIGN's narrative and the grounded applicability of its plans,\ndemonstrating its superior performance in the field of urban renewal design.\nLastly, we have created the first cross-modal design scheme dataset covering\narchitecture, landscape, interior, and urban design, providing rich resources\nfor future research.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12025v1",
    "published_date": "2024-06-28 10:57:50 UTC",
    "updated_date": "2024-06-28 10:57:50 UTC"
  },
  {
    "arxiv_id": "2406.19820v1",
    "title": "BeamAggR: Beam Aggregation Reasoning over Multi-source Knowledge for Multi-hop Question Answering",
    "authors": [
      "Zheng Chu",
      "Jingchang Chen",
      "Qianglong Chen",
      "Haotian Wang",
      "Kun Zhu",
      "Xiyuan Du",
      "Weijiang Yu",
      "Ming Liu",
      "Bing Qin"
    ],
    "abstract": "Large language models (LLMs) have demonstrated strong reasoning capabilities.\nNevertheless, they still suffer from factual errors when tackling\nknowledge-intensive tasks. Retrieval-augmented reasoning represents a promising\napproach. However, significant challenges still persist, including inaccurate\nand insufficient retrieval for complex questions, as well as difficulty in\nintegrating multi-source knowledge. To address this, we propose Beam\nAggregation Reasoning, BeamAggR, a reasoning framework for knowledge-intensive\nmulti-hop QA. BeamAggR explores and prioritizes promising answers at each hop\nof question. Concretely, we parse the complex questions into trees, which\ninclude atom and composite questions, followed by bottom-up reasoning. For\natomic questions, the LLM conducts reasoning on multi-source knowledge to get\nanswer candidates. For composite questions, the LLM combines beam candidates,\nexplores multiple reasoning paths through probabilistic aggregation, and\nprioritizes the most promising trajectory. Extensive experiments on four\nopen-domain multi-hop reasoning datasets show that our method significantly\noutperforms SOTA methods by 8.5%. Furthermore, our analysis reveals that\nBeamAggR elicits better knowledge collaboration and answer aggregation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.19820v1",
    "published_date": "2024-06-28 10:53:48 UTC",
    "updated_date": "2024-06-28 10:53:48 UTC"
  },
  {
    "arxiv_id": "2406.19815v1",
    "title": "Emotion Loss Attacking: Adversarial Attack Perception for Skeleton based on Multi-dimensional Features",
    "authors": [
      "Feng Liu",
      "Qing Xu",
      "Qijian Zheng"
    ],
    "abstract": "Adversarial attack on skeletal motion is a hot topic. However, existing\nresearches only consider part of dynamic features when measuring distance\nbetween skeleton graph sequences, which results in poor imperceptibility. To\nthis end, we propose a novel adversarial attack method to attack action\nrecognizers for skeletal motions. Firstly, our method systematically proposes a\ndynamic distance function to measure the difference between skeletal motions.\nMeanwhile, we innovatively introduce emotional features for complementary\ninformation. In addition, we use Alternating Direction Method of\nMultipliers(ADMM) to solve the constrained optimization problem, which\ngenerates adversarial samples with better imperceptibility to deceive the\nclassifiers. Experiments show that our method is effective on multiple action\nclassifiers and datasets. When the perturbation magnitude measured by l norms\nis the same, the dynamic perturbations generated by our method are much lower\nthan that of other methods. What's more, we are the first to prove the\neffectiveness of emotional features, and provide a new idea for measuring the\ndistance between skeletal motions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.19815v1",
    "published_date": "2024-06-28 10:45:37 UTC",
    "updated_date": "2024-06-28 10:45:37 UTC"
  },
  {
    "arxiv_id": "2407.00134v1",
    "title": "A Simple Attention-Based Mechanism for Bimodal Emotion Classification",
    "authors": [
      "Mazen Elabd",
      "Sardar Jaf"
    ],
    "abstract": "Big data contain rich information for machine learning algorithms to utilize\nwhen learning important features during classification tasks. Human beings\nexpress their emotion using certain words, speech (tone, pitch, speed) or\nfacial expression. Artificial Intelligence approach to emotion classification\nare largely based on learning from textual information. However, public\ndatasets containing text and speech data provide sufficient resources to train\nmachine learning algorithms for the tack of emotion classification. In this\npaper, we present novel bimodal deep learning-based architectures enhanced with\nattention mechanism trained and tested on text and speech data for emotion\nclassification. We report details of different deep learning based\narchitectures and show the performance of each architecture including rigorous\nerror analyses. Our finding suggests that deep learning based architectures\ntrained on different types of data (text and speech) outperform architectures\ntrained only on text or speech. Our proposed attention-based bimodal\narchitecture outperforms several state-of-the-art systems in emotion\nclassification.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 5 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.00134v1",
    "published_date": "2024-06-28 10:43:02 UTC",
    "updated_date": "2024-06-28 10:43:02 UTC"
  },
  {
    "arxiv_id": "2406.19812v1",
    "title": "Fuzzy Logic Guided Reward Function Variation: An Oracle for Testing Reinforcement Learning Programs",
    "authors": [
      "Shiyu Zhang",
      "Haoyang Song",
      "Qixin Wang",
      "Yu Pei"
    ],
    "abstract": "Reinforcement Learning (RL) has gained significant attention across various\ndomains. However, the increasing complexity of RL programs presents testing\nchallenges, particularly the oracle problem: defining the correctness of the RL\nprogram. Conventional human oracles struggle to cope with the complexity,\nleading to inefficiencies and potential unreliability in RL testing. To\nalleviate this problem, we propose an automated oracle approach that leverages\nRL properties using fuzzy logic. Our oracle quantifies an agent's behavioral\ncompliance with reward policies and analyzes its trend over training episodes.\nIt labels an RL program as \"Buggy\" if the compliance trend violates\nexpectations derived from RL characteristics. We evaluate our oracle on RL\nprograms with varying complexities and compare it with human oracles. Results\nshow that while human oracles perform well in simpler testing scenarios, our\nfuzzy oracle demonstrates superior performance in complex environments. The\nproposed approach shows promise in addressing the oracle problem for RL\ntesting, particularly in complex cases where manual testing falls short. It\noffers a potential solution to improve the efficiency, reliability, and\nscalability of RL program testing. This research takes a step towards automated\ntesting of RL programs and highlights the potential of fuzzy logic-based\noracles in tackling the oracle problem.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "68T05, 68T27, 93C42",
      "D.2.5; I.2.3"
    ],
    "primary_category": "cs.SE",
    "comment": "10 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.19812v1",
    "published_date": "2024-06-28 10:41:17 UTC",
    "updated_date": "2024-06-28 10:41:17 UTC"
  },
  {
    "arxiv_id": "2406.19807v1",
    "title": "Deceptive Diffusion: Generating Synthetic Adversarial Examples",
    "authors": [
      "Lucas Beerens",
      "Catherine F. Higham",
      "Desmond J. Higham"
    ],
    "abstract": "We introduce the concept of deceptive diffusion -- training a generative AI\nmodel to produce adversarial images. Whereas a traditional adversarial attack\nalgorithm aims to perturb an existing image to induce a misclassificaton, the\ndeceptive diffusion model can create an arbitrary number of new, misclassified\nimages that are not directly associated with training or test images. Deceptive\ndiffusion offers the possibility of strengthening defence algorithms by\nproviding adversarial training data at scale, including types of\nmisclassification that are otherwise difficult to find. In our experiments, we\nalso investigate the effect of training on a partially attacked data set. This\nhighlights a new type of vulnerability for generative diffusion models: if an\nattacker is able to stealthily poison a portion of the training data, then the\nresulting diffusion model will generate a similar proportion of misleading\noutputs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T07",
      "I.2.0; I.5.1"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.19807v1",
    "published_date": "2024-06-28 10:30:46 UTC",
    "updated_date": "2024-06-28 10:30:46 UTC"
  },
  {
    "arxiv_id": "2406.19770v1",
    "title": "Self-Supervised Spatial-Temporal Normality Learning for Time Series Anomaly Detection",
    "authors": [
      "Yutong Chen",
      "Hongzuo Xu",
      "Guansong Pang",
      "Hezhe Qiao",
      "Yuan Zhou",
      "Mingsheng Shang"
    ],
    "abstract": "Time Series Anomaly Detection (TSAD) finds widespread applications across\nvarious domains such as financial markets, industrial production, and\nhealthcare. Its primary objective is to learn the normal patterns of time\nseries data, thereby identifying deviations in test samples. Most existing TSAD\nmethods focus on modeling data from the temporal dimension, while ignoring the\nsemantic information in the spatial dimension. To address this issue, we\nintroduce a novel approach, called Spatial-Temporal Normality learning (STEN).\nSTEN is composed of a sequence Order prediction-based Temporal Normality\nlearning (OTN) module that captures the temporal correlations within sequences,\nand a Distance prediction-based Spatial Normality learning (DSN) module that\nlearns the relative spatial relations between sequences in a feature space. By\nsynthesizing these two modules, STEN learns expressive spatial-temporal\nrepresentations for the normal patterns hidden in the time series data.\nExtensive experiments on five popular TSAD benchmarks show that STEN\nsubstantially outperforms state-of-the-art competing methods. Our code is\navailable at https://github.com/mala-lab/STEN.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, 4 figures, accepted in ECML PKDD2024",
    "pdf_url": "http://arxiv.org/pdf/2406.19770v1",
    "published_date": "2024-06-28 09:17:58 UTC",
    "updated_date": "2024-06-28 09:17:58 UTC"
  },
  {
    "arxiv_id": "2406.19763v1",
    "title": "xSemAD: Explainable Semantic Anomaly Detection in Event Logs Using Sequence-to-Sequence Models",
    "authors": [
      "Kiran Busch",
      "Timotheus Kampik",
      "Henrik Leopold"
    ],
    "abstract": "The identification of undesirable behavior in event logs is an important\naspect of process mining that is often addressed by anomaly detection methods.\nTraditional anomaly detection methods tend to focus on statistically rare\nbehavior and neglect the subtle difference between rarity and undesirability.\nThe introduction of semantic anomaly detection has opened a promising avenue by\nidentifying semantically deviant behavior. This work addresses a gap in\nsemantic anomaly detection, which typically indicates the occurrence of an\nanomaly without explaining the nature of the anomaly. We propose xSemAD, an\napproach that uses a sequence-to-sequence model to go beyond pure\nidentification and provides extended explanations. In essence, our approach\nlearns constraints from a given process model repository and then checks\nwhether these constraints hold in the considered event log. This approach not\nonly helps understand the specifics of the undesired behavior, but also\nfacilitates targeted corrective actions. Our experiments demonstrate that our\napproach outperforms existing state-of-the-art semantic anomaly detection\nmethods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at BPM 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.19763v1",
    "published_date": "2024-06-28 09:06:52 UTC",
    "updated_date": "2024-06-28 09:06:52 UTC"
  },
  {
    "arxiv_id": "2406.19756v2",
    "title": "Structure-aware World Model for Probe Guidance via Large-scale Self-supervised Pre-train",
    "authors": [
      "Haojun Jiang",
      "Meng Li",
      "Zhenguo Sun",
      "Ning Jia",
      "Yu Sun",
      "Shaqi Luo",
      "Shiji Song",
      "Gao Huang"
    ],
    "abstract": "The complex structure of the heart leads to significant challenges in\nechocardiography, especially in acquisition cardiac ultrasound images.\nSuccessful echocardiography requires a thorough understanding of the structures\non the two-dimensional plane and the spatial relationships between planes in\nthree-dimensional space. In this paper, we innovatively propose a large-scale\nself-supervised pre-training method to acquire a cardiac structure-aware world\nmodel. The core innovation lies in constructing a self-supervised task that\nrequires structural inference by predicting masked structures on a 2D plane and\nimagining another plane based on pose transformation in 3D space. To support\nlarge-scale pre-training, we collected over 1.36 million echocardiograms from\nten standard views, along with their 3D spatial poses. In the downstream probe\nguidance task, we demonstrate that our pre-trained model consistently reduces\nguidance errors across the ten most common standard views on the test set with\n0.29 million samples from 74 routine clinical scans, indicating that\nstructure-aware pre-training benefits the scanning.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by MICCAI 2024 ASMUS Workshop",
    "pdf_url": "http://arxiv.org/pdf/2406.19756v2",
    "published_date": "2024-06-28 08:54:44 UTC",
    "updated_date": "2024-07-19 07:15:07 UTC"
  },
  {
    "arxiv_id": "2406.19755v1",
    "title": "Protein Representation Learning with Sequence Information Embedding: Does it Always Lead to a Better Performance?",
    "authors": [
      "Yang Tan",
      "Lirong Zheng",
      "Bozitao Zhong",
      "Liang Hong",
      "Bingxin Zhou"
    ],
    "abstract": "Deep learning has become a crucial tool in studying proteins. While the\nsignificance of modeling protein structure has been discussed extensively in\nthe literature, amino acid types are typically included in the input as a\ndefault operation for many inference tasks. This study demonstrates with\nstructure alignment task that embedding amino acid types in some cases may not\nhelp a deep learning model learn better representation. To this end, we propose\nProtLOCA, a local geometry alignment method based solely on amino acid\nstructure representation. The effectiveness of ProtLOCA is examined by a global\nstructure-matching task on protein pairs with an independent test dataset based\non CATH labels. Our method outperforms existing sequence- and structure-based\nrepresentation learning methods by more quickly and accurately matching\nstructurally consistent protein domains. Furthermore, in local structure\npairing tasks, ProtLOCA for the first time provides a valid solution to\nhighlight common local structures among proteins with different overall\nstructures but the same function. This suggests a new possibility for using\ndeep learning methods to analyze protein structure to infer function.",
    "categories": [
      "q-bio.QM",
      "cs.AI"
    ],
    "primary_category": "q-bio.QM",
    "comment": "8 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.19755v1",
    "published_date": "2024-06-28 08:54:37 UTC",
    "updated_date": "2024-06-28 08:54:37 UTC"
  },
  {
    "arxiv_id": "2407.00132v3",
    "title": "ShortcutsBench: A Large-Scale Real-world Benchmark for API-based Agents",
    "authors": [
      "Haiyang Shen",
      "Yue Li",
      "Desong Meng",
      "Dongqi Cai",
      "Sheng Qi",
      "Li Zhang",
      "Mengwei Xu",
      "Yun Ma"
    ],
    "abstract": "Recent advancements in integrating large language models (LLMs) with\napplication programming interfaces (APIs) have gained significant interest in\nboth academia and industry. Recent work demonstrates that these API-based\nagents exhibit relatively strong autonomy and planning capabilities. However,\ntheir ability to handle multi-dimensional difficulty levels, diverse task\ntypes, and real-world demands remains unknown. In this paper, we introduce\n\\textsc{ShortcutsBench}, a large-scale benchmark for the comprehensive\nevaluation of API-based agents in solving real-world complex tasks.\n\\textsc{ShortcutsBench} includes a wealth of real APIs from Apple Inc., refined\nuser queries, human-annotated high-quality action sequences, detailed parameter\nfilling values, and parameters requesting necessary input from the system or\nuser. We revealed how existing benchmarks~/~datasets struggle to accommodate\nthe advanced reasoning capabilities of existing more intelligent LLMs.\nMoreover, our extensive evaluation of agents built with $5$ leading open-source\n(size $\\geq$ 57B) and $5$ closed-source LLMs (e.g. Gemini-1.5-Pro and\nGPT-4o-mini) with varying intelligence level reveals significant limitations of\nexisting API-based agents in the whole process of handling complex queries\nrelated to API selection, parameter filling, and requesting necessary input\nfrom the system and the user. These findings highlight the great challenges\nthat API-based agents face in effectively fulfilling real and complex user\nqueries. All datasets, code, experimental logs, and results are available at\n\\url{https://github.com/EachSheep/ShortcutsBench}.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "ICLR'25: https://openreview.net/forum?id=kKILfPkhSz",
    "pdf_url": "http://arxiv.org/pdf/2407.00132v3",
    "published_date": "2024-06-28 08:45:02 UTC",
    "updated_date": "2025-01-23 11:22:20 UTC"
  },
  {
    "arxiv_id": "2406.19741v3",
    "title": "ROS-LLM: A ROS framework for embodied AI with task feedback and structured reasoning",
    "authors": [
      "Christopher E. Mower",
      "Yuhui Wan",
      "Hongzhan Yu",
      "Antoine Grosnit",
      "Jonas Gonzalez-Billandon",
      "Matthieu Zimmer",
      "Jinlong Wang",
      "Xinyu Zhang",
      "Yao Zhao",
      "Anbang Zhai",
      "Puze Liu",
      "Daniel Palenicek",
      "Davide Tateo",
      "Cesar Cadena",
      "Marco Hutter",
      "Jan Peters",
      "Guangjian Tian",
      "Yuzheng Zhuang",
      "Kun Shao",
      "Xingyue Quan",
      "Jianye Hao",
      "Jun Wang",
      "Haitham Bou-Ammar"
    ],
    "abstract": "We present a framework for intuitive robot programming by non-experts,\nleveraging natural language prompts and contextual information from the Robot\nOperating System (ROS). Our system integrates large language models (LLMs),\nenabling non-experts to articulate task requirements to the system through a\nchat interface. Key features of the framework include: integration of ROS with\nan AI agent connected to a plethora of open-source and commercial LLMs,\nautomatic extraction of a behavior from the LLM output and execution of ROS\nactions/services, support for three behavior modes (sequence, behavior tree,\nstate machine), imitation learning for adding new robot actions to the library\nof possible actions, and LLM reflection via human and environment feedback.\nExtensive experiments validate the framework, showcasing robustness,\nscalability, and versatility in diverse scenarios, including long-horizon\ntasks, tabletop rearrangements, and remote supervisory control. To facilitate\nthe adoption of our framework and support the reproduction of our results, we\nhave made our code open-source. You can access it at:\nhttps://github.com/huawei-noah/HEBO/tree/master/ROSLLM.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "This document contains 26 pages and 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.19741v3",
    "published_date": "2024-06-28 08:28:38 UTC",
    "updated_date": "2024-07-12 11:44:33 UTC"
  },
  {
    "arxiv_id": "2406.19738v1",
    "title": "Classical Bandit Algorithms for Entanglement Detection in Parameterized Qubit States",
    "authors": [
      "Bharati. K",
      "Vikesh Siddhu",
      "Krishna Jagannathan"
    ],
    "abstract": "Entanglement is a key resource for a wide range of tasks in quantum\ninformation and computing. Thus, verifying availability of this quantum\nresource is essential. Extensive research on entanglement detection has led to\nno-go theorems (Lu et al. [Phys. Rev. Lett., 116, 230501 (2016)]) that\nhighlight the need for full state tomography (FST) in the absence of adaptive\nor joint measurements. Recent advancements, as proposed by Zhu, Teo, and\nEnglert [Phys. Rev. A, 81, 052339, 2010], introduce a single-parameter family\nof entanglement witness measurements which are capable of conclusively\ndetecting certain entangled states and only resort to FST when all witness\nmeasurements are inconclusive. We find a variety of realistic noisy two-qubit\nquantum states $\\mathcal{F}$ that yield conclusive results under this witness\nfamily. We solve the problem of detecting entanglement among $K$ quantum states\nin $\\mathcal{F}$, of which $m$ states are entangled, with $m$ potentially\nunknown. We recognize a structural connection of this problem to the Bad Arm\nIdentification problem in stochastic Multi-Armed Bandits (MAB). In contrast to\nexisting quantum bandit frameworks, we establish a new correspondence tailored\nfor entanglement detection and term it the $(m,K)$-quantum Multi-Armed Bandit.\nWe implement two well-known MAB policies for arbitrary states derived from\n$\\mathcal{F}$, present theoretical guarantees on the measurement/sample\ncomplexity and demonstrate the practicality of the policies through numerical\nsimulations. More broadly, this paper highlights the potential for employing\nclassical machine learning techniques for quantum entanglement detection.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "20 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.19738v1",
    "published_date": "2024-06-28 08:26:47 UTC",
    "updated_date": "2024-06-28 08:26:47 UTC"
  },
  {
    "arxiv_id": "2407.00131v1",
    "title": "RepAct: The Re-parameterizable Adaptive Activation Function",
    "authors": [
      "Xian Wu",
      "Qingchuan Tao",
      "Shuang Wang"
    ],
    "abstract": "Addressing the imperative need for efficient artificial intelligence in IoT\nand edge computing, this study presents RepAct, a re-parameterizable adaptive\nactivation function tailored for optimizing lightweight neural networks within\nthe computational limitations of edge devices. By employing a multi-branch\nstructure with learnable adaptive weights, RepAct enriches feature processing\nand enhances cross-layer interpretability. When evaluated on tasks such as\nimage classification and object detection, RepAct notably surpassed\nconventional activation functions in lightweight networks, delivering up to a\n7.92% accuracy boost on MobileNetV3-Small for the ImageNet100 dataset, while\nmaintaining computational complexity on par with HardSwish. This innovative\napproach not only maximizes model parameter efficiency but also significantly\nimproves the performance and understanding capabilities of lightweight neural\nnetworks, demonstrating its potential for real-time edge computing\napplications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00131v1",
    "published_date": "2024-06-28 08:25:45 UTC",
    "updated_date": "2024-06-28 08:25:45 UTC"
  },
  {
    "arxiv_id": "2406.19736v1",
    "title": "MM-Instruct: Generated Visual Instructions for Large Multimodal Model Alignment",
    "authors": [
      "Jihao Liu",
      "Xin Huang",
      "Jinliang Zheng",
      "Boxiao Liu",
      "Jia Wang",
      "Osamu Yoshie",
      "Yu Liu",
      "Hongsheng Li"
    ],
    "abstract": "This paper introduces MM-Instruct, a large-scale dataset of diverse and\nhigh-quality visual instruction data designed to enhance the\ninstruction-following capabilities of large multimodal models (LMMs). While\nexisting visual instruction datasets often focus on question-answering, they\nstruggle to generalize to broader application scenarios such as creative\nwriting, summarization, or image analysis. To address these limitations, we\npropose a novel approach to constructing MM-Instruct that leverages the strong\ninstruction-following capabilities of existing LLMs to generate novel visual\ninstruction data from large-scale but conventional image captioning datasets.\nMM-Instruct first leverages ChatGPT to automatically generate diverse\ninstructions from a small set of seed instructions through augmenting and\nsummarization. It then matches these instructions with images and uses an\nopen-sourced large language model (LLM) to generate coherent answers to the\ninstruction-image pairs. The LLM is grounded by the detailed text descriptions\nof images in the whole answer generation process to guarantee the alignment of\nthe instruction data. Moreover, we introduce a benchmark based on the generated\ninstruction data to evaluate the instruction-following capabilities of existing\nLMMs. We demonstrate the effectiveness of MM-Instruct by training a LLaVA-1.5\nmodel on the generated data, denoted as LLaVA-Instruct, which exhibits\nsignificant improvements in instruction-following capabilities compared to\nLLaVA-1.5 models. The MM-Instruct dataset, benchmark, and pre-trained models\nare available at https://github.com/jihaonew/MM-Instruct.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Dataset and models are available at\n  https://github.com/jihaonew/MM-Instruct",
    "pdf_url": "http://arxiv.org/pdf/2406.19736v1",
    "published_date": "2024-06-28 08:25:27 UTC",
    "updated_date": "2024-06-28 08:25:27 UTC"
  },
  {
    "arxiv_id": "2407.09553v5",
    "title": "DPEC: Dual-Path Error Compensation Method for Enhanced Low-Light Image Clarity",
    "authors": [
      "Shuang Wang",
      "Qianwen Lu",
      "Boxing Peng",
      "Yihe Nie",
      "Qingchuan Tao"
    ],
    "abstract": "For the task of low-light image enhancement, deep learning-based algorithms\nhave demonstrated superiority and effectiveness compared to traditional\nmethods. However, these methods, primarily based on Retinex theory, tend to\noverlook the noise and color distortions in input images, leading to\nsignificant noise amplification and local color distortions in enhanced\nresults. To address these issues, we propose the Dual-Path Error Compensation\n(DPEC) method, designed to improve image quality under low-light conditions by\npreserving local texture details while restoring global image brightness\nwithout amplifying noise. DPEC incorporates precise pixel-level error\nestimation to capture subtle differences and an independent denoising mechanism\nto prevent noise amplification. We introduce the HIS-Retinex loss to guide\nDPEC's training, ensuring the brightness distribution of enhanced images\nclosely aligns with real-world conditions. To balance computational speed and\nresource efficiency while training DPEC for a comprehensive understanding of\nthe global context, we integrated the VMamba architecture into its backbone.\nComprehensive quantitative and qualitative experimental results demonstrate\nthat our algorithm significantly outperforms state-of-the-art methods in\nlow-light image enhancement. The code is publicly available online at\nhttps://github.com/wangshuang233/DPEC.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09553v5",
    "published_date": "2024-06-28 08:21:49 UTC",
    "updated_date": "2025-03-15 12:10:05 UTC"
  },
  {
    "arxiv_id": "2406.19720v1",
    "title": "CUPID: Improving Battle Fairness and Position Satisfaction in Online MOBA Games with a Re-matchmaking System",
    "authors": [
      "Ge Fan",
      "Chaoyun Zhang",
      "Kai Wang",
      "Yingjie Li",
      "Junyang Chen",
      "Zenglin Xu"
    ],
    "abstract": "The multiplayer online battle arena (MOBA) genre has gained significant\npopularity and economic success, attracting considerable research interest\nwithin the Human-Computer Interaction community. Enhancing the gaming\nexperience requires a deep understanding of player behavior, and a crucial\naspect of MOBA games is matchmaking, which aims to assemble teams of comparable\nskill levels. However, existing matchmaking systems often neglect important\nfactors such as players' position preferences and team assignment, resulting in\nimbalanced matches and reduced player satisfaction. To address these\nlimitations, this paper proposes a novel framework called CUPID, which\nintroduces a novel process called ``re-matchmaking'' to optimize team and\nposition assignments to improve both fairness and player satisfaction. CUPID\nincorporates a pre-filtering step to ensure a minimum level of matchmaking\nquality, followed by a pre-match win-rate prediction model that evaluates the\nfairness of potential assignments. By simultaneously considering players'\nposition satisfaction and game fairness, CUPID aims to provide an enhanced\nmatchmaking experience. Extensive experiments were conducted on two\nlarge-scale, real-world MOBA datasets to validate the effectiveness of CUPID.\nThe results surpass all existing state-of-the-art baselines, with an average\nrelative improvement of 7.18% in terms of win prediction accuracy. Furthermore,\nCUPID has been successfully deployed in a popular online mobile MOBA game. The\ndeployment resulted in significant improvements in match fairness and player\nsatisfaction, as evidenced by critical Human-Computer Interaction (HCI) metrics\ncovering usability, accessibility, and engagement, observed through A/B\ntesting. To the best of our knowledge, CUPID is the first re-matchmaking system\ndesigned specifically for large-scale MOBA games.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "38 pages, accepted by CSCW 24",
    "pdf_url": "http://arxiv.org/pdf/2406.19720v1",
    "published_date": "2024-06-28 08:09:55 UTC",
    "updated_date": "2024-06-28 08:09:55 UTC"
  },
  {
    "arxiv_id": "2406.19712v1",
    "title": "Uncertainty Quantification in Large Language Models Through Convex Hull Analysis",
    "authors": [
      "Ferhat Ozgur Catak",
      "Murat Kuzlu"
    ],
    "abstract": "Uncertainty quantification approaches have been more critical in large\nlanguage models (LLMs), particularly high-risk applications requiring reliable\noutputs. However, traditional methods for uncertainty quantification, such as\nprobabilistic models and ensemble techniques, face challenges when applied to\nthe complex and high-dimensional nature of LLM-generated outputs. This study\nproposes a novel geometric approach to uncertainty quantification using convex\nhull analysis. The proposed method leverages the spatial properties of response\nembeddings to measure the dispersion and variability of model outputs. The\nprompts are categorized into three types, i.e., `easy', `moderate', and\n`confusing', to generate multiple responses using different LLMs at varying\ntemperature settings. The responses are transformed into high-dimensional\nembeddings via a BERT model and subsequently projected into a two-dimensional\nspace using Principal Component Analysis (PCA). The Density-Based Spatial\nClustering of Applications with Noise (DBSCAN) algorithm is utilized to cluster\nthe embeddings and compute the convex hull for each selected cluster. The\nexperimental results indicate that the uncertainty of the model for LLMs\ndepends on the prompt complexity, the model, and the temperature setting.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "17 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.19712v1",
    "published_date": "2024-06-28 07:47:34 UTC",
    "updated_date": "2024-06-28 07:47:34 UTC"
  },
  {
    "arxiv_id": "2406.19708v3",
    "title": "A Differentiable Approach to Multi-scale Brain Modeling",
    "authors": [
      "Chaoming Wang",
      "Muyang Lyu",
      "Tianqiu Zhang",
      "Sichao He",
      "Si Wu"
    ],
    "abstract": "We present a multi-scale differentiable brain modeling workflow utilizing\nBrainPy, a unique differentiable brain simulator that combines accurate brain\nsimulation with powerful gradient-based optimization. We leverage this\ncapability of BrainPy across different brain scales. At the single-neuron\nlevel, we implement differentiable neuron models and employ gradient methods to\noptimize their fit to electrophysiological data. On the network level, we\nincorporate connectomic data to construct biologically constrained network\nmodels. Finally, to replicate animal behavior, we train these models on\ncognitive tasks using gradient-based learning rules. Experiments demonstrate\nthat our approach achieves superior performance and speed in fitting\ngeneralized leaky integrate-and-fire and Hodgkin-Huxley single neuron models.\nAdditionally, training a biologically-informed network of excitatory and\ninhibitory spiking neurons on working memory tasks successfully replicates\nobserved neural activity and synaptic weight distributions. Overall, our\ndifferentiable multi-scale simulation approach offers a promising tool to\nbridge neuroscience data across electrophysiological, anatomical, and\nbehavioral scales.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CE",
      "q-bio.NC"
    ],
    "primary_category": "cs.NE",
    "comment": "2nd Differentiable Almost Everything Workshop at ICML 2024.\n  https://github.com/chaoming0625/differentiable-brain-modeling-workflow",
    "pdf_url": "http://arxiv.org/pdf/2406.19708v3",
    "published_date": "2024-06-28 07:41:31 UTC",
    "updated_date": "2024-09-25 11:56:29 UTC"
  },
  {
    "arxiv_id": "2406.19705v5",
    "title": "DISCO: Efficient Diffusion Solver for Large-Scale Combinatorial Optimization Problems",
    "authors": [
      "Kexiong Yu",
      "Hang Zhao",
      "Yuhang Huang",
      "Renjiao Yi",
      "Kai Xu",
      "Chenyang Zhu"
    ],
    "abstract": "Combinatorial Optimization (CO) problems are fundamentally important in\nnumerous real-world applications across diverse industries, characterized by\nentailing enormous solution space and demanding time-sensitive response.\nDespite recent advancements in neural solvers, their limited expressiveness\nstruggles to capture the multi-modal nature of CO landscapes. While some\nresearch has shifted towards diffusion models, these models still sample\nsolutions indiscriminately from the entire NP-complete solution space with\ntime-consuming denoising processes, which limit their practicality for large\nproblem scales. We propose DISCO, an efficient DIffusion Solver for large-scale\nCombinatorial Optimization problems that excels in both solution quality and\ninference speed. DISCO's efficacy is twofold: First, it enhances solution\nquality by constraining the sampling space to a more meaningful domain guided\nby solution residues, while preserving the multi-modal properties of the output\ndistributions. Second, it accelerates the denoising process through an\nanalytically solvable approach, enabling solution sampling with minimal\nreverse-time steps and significantly reducing inference time. DISCO delivers\nstrong performance on large-scale Traveling Salesman Problems and challenging\nMaximal Independent Set benchmarks, with inference time up to 5.28 times faster\nthan other diffusion alternatives. By incorporating a divide-and-conquer\nstrategy, DISCO can well generalize to solve unseen-scale problem instances,\neven surpassing models specifically trained for those scales.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.19705v5",
    "published_date": "2024-06-28 07:36:31 UTC",
    "updated_date": "2024-10-21 13:38:48 UTC"
  },
  {
    "arxiv_id": "2407.12024v1",
    "title": "Leveraging Large Language Models for enhanced personalised user experience in Smart Homes",
    "authors": [
      "Jordan Rey-Jouanchicot",
      "André Bottaro",
      "Eric Campo",
      "Jean-Léon Bouraoui",
      "Nadine Vigouroux",
      "Frédéric Vella"
    ],
    "abstract": "Smart home automation systems aim to improve the comfort and convenience of\nusers in their living environment. However, adapting automation to user needs\nremains a challenge. Indeed, many systems still rely on hand-crafted routines\nfor each smart object.This paper presents an original smart home architecture\nleveraging Large Language Models (LLMs) and user preferences to push the\nboundaries of personalisation and intuitiveness in the home environment.This\narticle explores a human-centred approach that uses the general knowledge\nprovided by LLMs to learn and facilitate interactions with the environment.The\nadvantages of the proposed model are demonstrated on a set of scenarios, as\nwell as a comparative analysis with various LLM implementations. Some metrics\nare assessed to determine the system's ability to maintain comfort, safety, and\nuser preferences. The paper details the approach to real-world implementation\nand evaluation.The proposed approach of using preferences shows up to 52.3%\nincrease in average grade, and with an average processing time reduced by 35.6%\non Starling 7B Alpha LLM. In addition, performance is 26.4% better than the\nresults of the larger models without preferences, with processing time almost\n20 times faster.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12024v1",
    "published_date": "2024-06-28 07:08:20 UTC",
    "updated_date": "2024-06-28 07:08:20 UTC"
  },
  {
    "arxiv_id": "2406.19690v1",
    "title": "Deep Fusion Model for Brain Tumor Classification Using Fine-Grained Gradient Preservation",
    "authors": [
      "Niful Islam",
      "Mohaiminul Islam Bhuiyan",
      "Jarin Tasnim Raya",
      "Nur Shazwani Kamarudin",
      "Khan Md Hasib",
      "M. F. Mridha",
      "Dewan Md. Farid"
    ],
    "abstract": "Brain tumors are one of the most common diseases that lead to early death if\nnot diagnosed at an early stage. Traditional diagnostic approaches are\nextremely time-consuming and prone to errors. In this context, computer\nvision-based approaches have emerged as an effective tool for accurate brain\ntumor classification. While some of the existing solutions demonstrate\nnoteworthy accuracy, the models become infeasible to deploy in areas where\ncomputational resources are limited. This research addresses the need for\naccurate and fast classification of brain tumors with a priority of deploying\nthe model in technologically underdeveloped regions. The research presents a\nnovel architecture for precise brain tumor classification fusing pretrained\nResNet152V2 and modified VGG16 models. The proposed architecture undergoes a\ndiligent fine-tuning process that ensures fine gradients are preserved in deep\nneural networks, which are essential for effective brain tumor classification.\nThe proposed solution incorporates various image processing techniques to\nimprove image quality and achieves an astounding accuracy of 98.36% and 98.04%\nin Figshare and Kaggle datasets respectively. This architecture stands out for\nhaving a streamlined profile, with only 2.8 million trainable parameters. We\nhave leveraged 8-bit quantization to produce a model of size 73.881 MB,\nsignificantly reducing it from the previous size of 289.45 MB, ensuring smooth\ndeployment in edge devices even in resource-constrained areas. Additionally,\nthe use of Grad-CAM improves the interpretability of the model, offering\ninsightful information regarding its decision-making process. Owing to its high\ndiscriminative ability, this model can be a reliable option for accurate brain\ntumor classification.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.19690v1",
    "published_date": "2024-06-28 07:06:02 UTC",
    "updated_date": "2024-06-28 07:06:02 UTC"
  },
  {
    "arxiv_id": "2406.19686v1",
    "title": "Enhancing Radiological Diagnosis: A Collaborative Approach Integrating AI and Human Expertise for Visual Miss Correction",
    "authors": [
      "Akash Awasthi",
      "Ngan Le",
      "Zhigang Deng",
      "Carol C. Wu",
      "Hien Van Nguyen"
    ],
    "abstract": "Human-AI collaboration to identify and correct perceptual errors in chest\nradiographs has not been previously explored. This study aimed to develop a\ncollaborative AI system, CoRaX, which integrates eye gaze data and radiology\nreports to enhance diagnostic accuracy in chest radiology by pinpointing\nperceptual errors and refining the decision-making process. Using public\ndatasets REFLACX and EGD-CXR, the study retrospectively developed CoRaX,\nemploying a large multimodal model to analyze image embeddings, eye gaze data,\nand radiology reports. The system's effectiveness was evaluated based on its\nreferral-making process, the quality of referrals, and performance in\ncollaborative diagnostic settings. CoRaX was tested on a simulated error\ndataset of 271 samples with 28% (93 of 332) missed abnormalities. The system\ncorrected 21% (71 of 332) of these errors, leaving 7% (22 of 312) unresolved.\nThe Referral-Usefulness score, indicating the accuracy of predicted regions for\nall true referrals, was 0.63 (95% CI 0.59, 0.68). The Total-Usefulness score,\nreflecting the diagnostic accuracy of CoRaX's interactions with radiologists,\nshowed that 84% (237 of 280) of these interactions had a score above 0.40. In\nconclusion, CoRaX efficiently collaborates with radiologists to address\nperceptual errors across various abnormalities, with potential applications in\nthe education and training of novice radiologists.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "eess.IV",
    "comment": "Under Review in Journal",
    "pdf_url": "http://arxiv.org/pdf/2406.19686v1",
    "published_date": "2024-06-28 06:51:38 UTC",
    "updated_date": "2024-06-28 06:51:38 UTC"
  },
  {
    "arxiv_id": "2407.12814v1",
    "title": "Computational Politeness in Natural Language Processing: A Survey",
    "authors": [
      "Priyanshu Priya",
      "Mauajama Firdaus",
      "Asif Ekbal"
    ],
    "abstract": "Computational approach to politeness is the task of automatically predicting\nand generating politeness in text. This is a pivotal task for conversational\nanalysis, given the ubiquity and challenges of politeness in interactions. The\ncomputational approach to politeness has witnessed great interest from the\nconversational analysis community. This article is a compilation of past works\nin computational politeness in natural language processing. We view four\nmilestones in the research so far, viz. supervised and weakly-supervised\nfeature extraction to identify and induce politeness in a given text,\nincorporation of context beyond the target text, study of politeness across\ndifferent social factors, and study the relationship between politeness and\nvarious sociolinguistic cues. In this article, we describe the datasets,\napproaches, trends, and issues in computational politeness research. We also\ndiscuss representative performance values and provide pointers to future works,\nas given in the prior works. In terms of resources to understand the\nstate-of-the-art, this survey presents several valuable illustrations, most\nprominently, a table summarizing the past papers along different dimensions,\nsuch as the types of features, annotation techniques, and datasets used.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Manuscript accepted at the ACM Computing Surveys (DOI:\n  https://doi.org/10.1145/3654660)",
    "pdf_url": "http://arxiv.org/pdf/2407.12814v1",
    "published_date": "2024-06-28 06:46:36 UTC",
    "updated_date": "2024-06-28 06:46:36 UTC"
  },
  {
    "arxiv_id": "2406.19680v1",
    "title": "MimicMotion: High-Quality Human Motion Video Generation with Confidence-aware Pose Guidance",
    "authors": [
      "Yuang Zhang",
      "Jiaxi Gu",
      "Li-Wen Wang",
      "Han Wang",
      "Junqi Cheng",
      "Yuefeng Zhu",
      "Fangyuan Zou"
    ],
    "abstract": "In recent years, generative artificial intelligence has achieved significant\nadvancements in the field of image generation, spawning a variety of\napplications. However, video generation still faces considerable challenges in\nvarious aspects, such as controllability, video length, and richness of\ndetails, which hinder the application and popularization of this technology. In\nthis work, we propose a controllable video generation framework, dubbed\nMimicMotion, which can generate high-quality videos of arbitrary length\nmimicking specific motion guidance. Compared with previous methods, our\napproach has several highlights. Firstly, we introduce confidence-aware pose\nguidance that ensures high frame quality and temporal smoothness. Secondly, we\nintroduce regional loss amplification based on pose confidence, which\nsignificantly reduces image distortion. Lastly, for generating long and smooth\nvideos, we propose a progressive latent fusion strategy. By this means, we can\nproduce videos of arbitrary length with acceptable resource consumption. With\nextensive experiments and user studies, MimicMotion demonstrates significant\nimprovements over previous approaches in various aspects. Detailed results and\ncomparisons are available on our project page:\nhttps://tencent.github.io/MimicMotion .",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.19680v1",
    "published_date": "2024-06-28 06:40:53 UTC",
    "updated_date": "2024-06-28 06:40:53 UTC"
  },
  {
    "arxiv_id": "2407.00129v1",
    "title": "Multimodal Learning and Cognitive Processes in Radiology: MedGaze for Chest X-ray Scanpath Prediction",
    "authors": [
      "Akash Awasthi",
      "Ngan Le",
      "Zhigang Deng",
      "Rishi Agrawal",
      "Carol C. Wu",
      "Hien Van Nguyen"
    ],
    "abstract": "Predicting human gaze behavior within computer vision is integral for\ndeveloping interactive systems that can anticipate user attention, address\nfundamental questions in cognitive science, and hold implications for fields\nlike human-computer interaction (HCI) and augmented/virtual reality (AR/VR)\nsystems. Despite methodologies introduced for modeling human eye gaze behavior,\napplying these models to medical imaging for scanpath prediction remains\nunexplored. Our proposed system aims to predict eye gaze sequences from\nradiology reports and CXR images, potentially streamlining data collection and\nenhancing AI systems using larger datasets. However, predicting human scanpaths\non medical images presents unique challenges due to the diverse nature of\nabnormal regions. Our model predicts fixation coordinates and durations\ncritical for medical scanpath prediction, outperforming existing models in the\ncomputer vision community. Utilizing a two-stage training process and large\npublicly available datasets, our approach generates static heatmaps and eye\ngaze videos aligned with radiology reports, facilitating comprehensive\nanalysis. We validate our approach by comparing its performance with\nstate-of-the-art methods and assessing its generalizability among different\nradiologists, introducing novel strategies to model radiologists' search\npatterns during CXR image diagnosis. Based on the radiologist's evaluation,\nMedGaze can generate human-like gaze sequences with a high focus on relevant\nregions over the CXR images. It sometimes also outperforms humans in terms of\nredundancy and randomness in the scanpaths.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "eess.IV",
    "comment": "Submitted to the Journal",
    "pdf_url": "http://arxiv.org/pdf/2407.00129v1",
    "published_date": "2024-06-28 06:38:58 UTC",
    "updated_date": "2024-06-28 06:38:58 UTC"
  },
  {
    "arxiv_id": "2406.19670v2",
    "title": "Function+Data Flow: A Framework to Specify Machine Learning Pipelines for Digital Twinning",
    "authors": [
      "Eduardo de Conto",
      "Blaise Genest",
      "Arvind Easwaran"
    ],
    "abstract": "The development of digital twins (DTs) for physical systems increasingly\nleverages artificial intelligence (AI), particularly for combining data from\ndifferent sources or for creating computationally efficient, reduced-dimension\nmodels. Indeed, even in very different application domains, twinning employs\ncommon techniques such as model order reduction and modelization with hybrid\ndata (that is, data sourced from both physics-based models and sensors).\nDespite this apparent generality, current development practices are ad-hoc,\nmaking the design of AI pipelines for digital twinning complex and\ntime-consuming. Here we propose Function+Data Flow (FDF), a domain-specific\nlanguage (DSL) to describe AI pipelines within DTs. FDF aims to facilitate the\ndesign and validation of digital twins. Specifically, FDF treats functions as\nfirst-class citizens, enabling effective manipulation of models learned with\nAI. We illustrate the benefits of FDF on two concrete use cases from different\ndomains: predicting the plastic strain of a structure and modeling the\nelectromagnetic behavior of a bearing.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "9 pages, 10 figures, to be published in AIware'24",
    "pdf_url": "http://arxiv.org/pdf/2406.19670v2",
    "published_date": "2024-06-28 05:44:47 UTC",
    "updated_date": "2024-07-08 08:28:34 UTC"
  },
  {
    "arxiv_id": "2406.19653v3",
    "title": "ACES: Automatic Cohort Extraction System for Event-Stream Datasets",
    "authors": [
      "Justin Xu",
      "Jack Gallifant",
      "Alistair E. W. Johnson",
      "Matthew B. A. McDermott"
    ],
    "abstract": "Reproducibility remains a significant challenge in machine learning (ML) for\nhealthcare. Datasets, model pipelines, and even task or cohort definitions are\noften private in this field, leading to a significant barrier in sharing,\niterating, and understanding ML results on electronic health record (EHR)\ndatasets. We address a significant part of this problem by introducing the\nAutomatic Cohort Extraction System (ACES) for event-stream data. This library\nis designed to simultaneously simplify the development of tasks and cohorts for\nML in healthcare and also enable their reproduction, both at an exact level for\nsingle datasets and at a conceptual level across datasets. To accomplish this,\nACES provides: (1) a highly intuitive and expressive domain-specific\nconfiguration language for defining both dataset-specific concepts and\ndataset-agnostic inclusion or exclusion criteria, and (2) a pipeline to\nautomatically extract patient records that meet these defined criteria from\nreal-world data. ACES can be automatically applied to any dataset in either the\nMedical Event Data Standard (MEDS) or Event Stream GPT (ESGPT) formats, or to\n*any* dataset in which the necessary task-specific predicates can be extracted\nin an event-stream form. ACES has the potential to significantly lower the\nbarrier to entry for defining ML tasks in representation learning, redefine the\nway researchers interact with EHR datasets, and significantly improve the state\nof reproducibility for ML studies using this modality. ACES is available at:\nhttps://github.com/justin13601/aces.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "[ICLR 2025] For the latest ACES online documentation, please see\n  https://eventstreamaces.readthedocs.io/en/latest/",
    "pdf_url": "http://arxiv.org/pdf/2406.19653v3",
    "published_date": "2024-06-28 04:48:05 UTC",
    "updated_date": "2025-03-02 01:47:44 UTC"
  },
  {
    "arxiv_id": "2406.19651v1",
    "title": "CANDY: A Benchmark for Continuous Approximate Nearest Neighbor Search with Dynamic Data Ingestion",
    "authors": [
      "Xianzhi Zeng",
      "Zhuoyan Wu",
      "Xinjing Hu",
      "Xuanhua Shi",
      "Shixuan Sun",
      "Shuhao Zhang"
    ],
    "abstract": "Approximate K Nearest Neighbor (AKNN) algorithms play a pivotal role in\nvarious AI applications, including information retrieval, computer vision, and\nnatural language processing. Although numerous AKNN algorithms and benchmarks\nhave been developed recently to evaluate their effectiveness, the dynamic\nnature of real-world data presents significant challenges that existing\nbenchmarks fail to address. Traditional benchmarks primarily assess retrieval\neffectiveness in static contexts and often overlook update efficiency, which is\ncrucial for handling continuous data ingestion. This limitation results in an\nincomplete assessment of an AKNN algorithms ability to adapt to changing data\npatterns, thereby restricting insights into their performance in dynamic\nenvironments. To address these gaps, we introduce CANDY, a benchmark tailored\nfor Continuous Approximate Nearest Neighbor Search with Dynamic Data Ingestion.\nCANDY comprehensively assesses a wide range of AKNN algorithms, integrating\nadvanced optimizations such as machine learning-driven inference to supplant\ntraditional heuristic scans, and improved distance computation methods to\nreduce computational overhead. Our extensive evaluations across diverse\ndatasets demonstrate that simpler AKNN baselines often surpass more complex\nalternatives in terms of recall and latency. These findings challenge\nestablished beliefs about the necessity of algorithmic complexity for high\nperformance. Furthermore, our results underscore existing challenges and\nilluminate future research opportunities. We have made the datasets and\nimplementation methods available at: https://github.com/intellistream/candy.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.19651v1",
    "published_date": "2024-06-28 04:46:11 UTC",
    "updated_date": "2024-06-28 04:46:11 UTC"
  },
  {
    "arxiv_id": "2406.19648v1",
    "title": "Designing and Evaluating Multi-Chatbot Interface for Human-AI Communication: Preliminary Findings from a Persuasion Task",
    "authors": [
      "Sion Yoon",
      "Tae Eun Kim",
      "Yoo Jung Oh"
    ],
    "abstract": "The dynamics of human-AI communication have been reshaped by language models\nsuch as ChatGPT. However, extant research has primarily focused on dyadic\ncommunication, leaving much to be explored regarding the dynamics of human-AI\ncommunication in group settings. The availability of multiple language model\nchatbots presents a unique opportunity for scholars to better understand the\ninteraction between humans and multiple chatbots. This study examines the\nimpact of multi-chatbot communication in a specific persuasion setting:\npromoting charitable donations. We developed an online environment that enables\nmulti-chatbot communication and conducted a pilot experiment utilizing two\nGPT-based chatbots, Save the Children and UNICEF chatbots, to promote\ncharitable donations. In this study, we present our development process of the\nmulti-chatbot interface and present preliminary findings from a pilot\nexperiment. Analysis of qualitative and quantitative feedback are presented,\nand limitations are addressed.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.19648v1",
    "published_date": "2024-06-28 04:33:41 UTC",
    "updated_date": "2024-06-28 04:33:41 UTC"
  },
  {
    "arxiv_id": "2406.19644v2",
    "title": "Beyond Human Preferences: Exploring Reinforcement Learning Trajectory Evaluation and Improvement through LLMs",
    "authors": [
      "Zichao Shen",
      "Tianchen Zhu",
      "Qingyun Sun",
      "Shiqi Gao",
      "Jianxin Li"
    ],
    "abstract": "Reinforcement learning (RL) faces challenges in evaluating policy\ntrajectories within intricate game tasks due to the difficulty in designing\ncomprehensive and precise reward functions. This inherent difficulty curtails\nthe broader application of RL within game environments characterized by diverse\nconstraints. Preference-based reinforcement learning (PbRL) presents a\npioneering framework that capitalizes on human preferences as pivotal reward\nsignals, thereby circumventing the need for meticulous reward engineering.\nHowever, obtaining preference data from human experts is costly and\ninefficient, especially under conditions marked by complex constraints. To\ntackle this challenge, we propose a LLM-enabled automatic preference generation\nframework named LLM4PG , which harnesses the capabilities of large language\nmodels (LLMs) to abstract trajectories, rank preferences, and reconstruct\nreward functions to optimize conditioned policies. Experiments on tasks with\ncomplex language constraints demonstrated the effectiveness of our LLM-enabled\nreward functions, accelerating RL convergence and overcoming stagnation caused\nby slow or absent progress under original reward structures. This approach\nmitigates the reliance on specialized human knowledge and demonstrates the\npotential of LLMs to enhance RL's effectiveness in complex environments in the\nwild.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "accepted by IJCAI 2024 GAAMAL",
    "pdf_url": "http://arxiv.org/pdf/2406.19644v2",
    "published_date": "2024-06-28 04:21:24 UTC",
    "updated_date": "2024-07-01 03:32:48 UTC"
  },
  {
    "arxiv_id": "2406.19643v3",
    "title": "Debate-to-Write: A Persona-Driven Multi-Agent Framework for Diverse Argument Generation",
    "authors": [
      "Zhe Hu",
      "Hou Pong Chan",
      "Jing Li",
      "Yu Yin"
    ],
    "abstract": "Writing persuasive arguments is a challenging task for both humans and\nmachines. It entails incorporating high-level beliefs from various perspectives\non the topic, along with deliberate reasoning and planning to construct a\ncoherent narrative. Current language models often generate surface tokens\nautoregressively, lacking explicit integration of these underlying controls,\nresulting in limited output diversity and coherence. In this work, we propose a\npersona-based multi-agent framework for argument writing. Inspired by the human\ndebate, we first assign each agent a persona representing its high-level\nbeliefs from a unique perspective, and then design an agent interaction process\nso that the agents can collaboratively debate and discuss the idea to form an\noverall plan for argument writing. Such debate process enables fluid and\nnonlinear development of ideas. We evaluate our framework on argumentative\nessay writing. The results show that our framework can generate more diverse\nand persuasive arguments through both automatic and human evaluations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at COLING 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.19643v3",
    "published_date": "2024-06-28 04:21:20 UTC",
    "updated_date": "2025-01-03 10:23:21 UTC"
  },
  {
    "arxiv_id": "2406.19638v1",
    "title": "Precision matters: Precision-aware ensemble for weakly supervised semantic segmentation",
    "authors": [
      "Junsung Park",
      "Hyunjung Shim"
    ],
    "abstract": "Weakly Supervised Semantic Segmentation (WSSS) employs weak supervision, such\nas image-level labels, to train the segmentation model. Despite the impressive\nachievement in recent WSSS methods, we identify that introducing weak labels\nwith high mean Intersection of Union (mIoU) does not guarantee high\nsegmentation performance. Existing studies have emphasized the importance of\nprioritizing precision and reducing noise to improve overall performance. In\nthe same vein, we propose ORANDNet, an advanced ensemble approach tailored for\nWSSS. ORANDNet combines Class Activation Maps (CAMs) from two different\nclassifiers to increase the precision of pseudo-masks (PMs). To further\nmitigate small noise in the PMs, we incorporate curriculum learning. This\ninvolves training the segmentation model initially with pairs of smaller-sized\nimages and corresponding PMs, gradually transitioning to the original-sized\npairs. By combining the original CAMs of ResNet-50 and ViT, we significantly\nimprove the segmentation performance over the single-best model and the naive\nensemble model, respectively. We further extend our ensemble method to CAMs\nfrom AMN (ResNet-like) and MCTformer (ViT-like) models, achieving performance\nbenefits in advanced WSSS models. It highlights the potential of our ORANDNet\nas a final add-on module for WSSS models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages, 5 figures, accepted in AAAI 2024 Edge Intelligence Workshop",
    "pdf_url": "http://arxiv.org/pdf/2406.19638v1",
    "published_date": "2024-06-28 03:58:02 UTC",
    "updated_date": "2024-06-28 03:58:02 UTC"
  },
  {
    "arxiv_id": "2407.00128v1",
    "title": "When Search Engine Services meet Large Language Models: Visions and Challenges",
    "authors": [
      "Haoyi Xiong",
      "Jiang Bian",
      "Yuchen Li",
      "Xuhong Li",
      "Mengnan Du",
      "Shuaiqiang Wang",
      "Dawei Yin",
      "Sumi Helal"
    ],
    "abstract": "Combining Large Language Models (LLMs) with search engine services marks a\nsignificant shift in the field of services computing, opening up new\npossibilities to enhance how we search for and retrieve information, understand\ncontent, and interact with internet services. This paper conducts an in-depth\nexamination of how integrating LLMs with search engines can mutually benefit\nboth technologies. We focus on two main areas: using search engines to improve\nLLMs (Search4LLM) and enhancing search engine functions using LLMs\n(LLM4Search). For Search4LLM, we investigate how search engines can provide\ndiverse high-quality datasets for pre-training of LLMs, how they can use the\nmost relevant documents to help LLMs learn to answer queries more accurately,\nhow training LLMs with Learning-To-Rank (LTR) tasks can enhance their ability\nto respond with greater precision, and how incorporating recent search results\ncan make LLM-generated content more accurate and current. In terms of\nLLM4Search, we examine how LLMs can be used to summarize content for better\nindexing by search engines, improve query outcomes through optimization,\nenhance the ranking of search results by analyzing document relevance, and help\nin annotating data for learning-to-rank tasks in various learning contexts.\nHowever, this promising integration comes with its challenges, which include\naddressing potential biases and ethical issues in training models, managing the\ncomputational and other costs of incorporating LLMs into search services, and\ncontinuously updating LLM training with the ever-changing web content. We\ndiscuss these challenges and chart out required research directions to address\nthem. We also discuss broader implications for service computing, such as\nscalability, privacy concerns, and the need to adapt search engine\narchitectures for these advanced models.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2407.00128v1",
    "published_date": "2024-06-28 03:52:13 UTC",
    "updated_date": "2024-06-28 03:52:13 UTC"
  },
  {
    "arxiv_id": "2406.19630v1",
    "title": "Optimal Video Compression using Pixel Shift Tracking",
    "authors": [
      "Hitesh Saai Mananchery Panneerselvam",
      "Smit Anand"
    ],
    "abstract": "The Video comprises approximately ~85\\% of all internet traffic, but video\nencoding/compression is being historically done with hard coded rules, which\nhas worked well but only to a certain limit. We have seen a surge in video\ncompression algorithms using ML-based models in the last few years and many of\nthem have outperformed several legacy codecs. The models range from encoding\nvideo end to end using an ML approach or replacing some intermediate steps in\nlegacy codecs using ML models to increase the efficiency of those steps.\n  Optimizing video storage is an essential aspect of video processing, so we\nare proposing one of the possible approaches to achieve it is by avoiding\nredundant data at each frame. In this paper, we want to introduce the approach\nof redundancies removal in subsequent frames for a given video as a main\napproach for video compression. We call this method Redundancy Removal using\nShift (R\\textsuperscript2S). This method can be utilized across various Machine\nLearning model algorithms, and make the compression more accessible and\nadaptable. In this study, we have utilized a computer vision-based pixel point\ntracking method to identify redundant pixels to encode video for optimal\nstorage.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.19630v1",
    "published_date": "2024-06-28 03:36:38 UTC",
    "updated_date": "2024-06-28 03:36:38 UTC"
  },
  {
    "arxiv_id": "2406.19626v3",
    "title": "Safety through feedback in Constrained RL",
    "authors": [
      "Shashank Reddy Chirra",
      "Pradeep Varakantham",
      "Praveen Paruchuri"
    ],
    "abstract": "In safety-critical RL settings, the inclusion of an additional cost function\nis often favoured over the arduous task of modifying the reward function to\nensure the agent's safe behaviour. However, designing or evaluating such a cost\nfunction can be prohibitively expensive. For instance, in the domain of\nself-driving, designing a cost function that encompasses all unsafe behaviours\n(e.g. aggressive lane changes) is inherently complex. In such scenarios, the\ncost function can be learned from feedback collected offline in between\ntraining rounds. This feedback can be system generated or elicited from a human\nobserving the training process. Previous approaches have not been able to scale\nto complex environments and are constrained to receiving feedback at the state\nlevel which can be expensive to collect. To this end, we introduce an approach\nthat scales to more complex domains and extends to beyond state-level feedback,\nthus, reducing the burden on the evaluator. Inferring the cost function in such\nsettings poses challenges, particularly in assigning credit to individual\nstates based on trajectory-level feedback. To address this, we propose a\nsurrogate objective that transforms the problem into a state-level supervised\nclassification task with noisy labels, which can be solved efficiently.\nAdditionally, it is often infeasible to collect feedback on every trajectory\ngenerated by the agent, hence, two fundamental questions arise: (1) Which\ntrajectories should be presented to the human? and (2) How many trajectories\nare necessary for effective learning? To address these questions, we introduce\n\\textit{novelty-based sampling} that selectively involves the evaluator only\nwhen the the agent encounters a \\textit{novel} trajectory. We showcase the\nefficiency of our method through experimentation on several benchmark Safety\nGymnasium environments and realistic self-driving scenarios.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at NeurIPS 2024 (Poster)",
    "pdf_url": "http://arxiv.org/pdf/2406.19626v3",
    "published_date": "2024-06-28 03:29:33 UTC",
    "updated_date": "2025-01-11 07:31:06 UTC"
  },
  {
    "arxiv_id": "2407.01615v1",
    "title": "Edge-DIRECT: A Deep Reinforcement Learning-based Method for Solving Heterogeneous Electric Vehicle Routing Problem with Time Window Constraints",
    "authors": [
      "Arash Mozhdehi",
      "Mahdi Mohammadizadeh",
      "Xin Wang"
    ],
    "abstract": "In response to carbon-neutral policies in developed countries, electric\nvehicles route optimization has gained importance for logistics companies. With\nthe increasing focus on customer expectations and the shift towards more\ncustomer-oriented business models, the integration of delivery time-windows has\nbecome essential in logistics operations. Recognizing the critical nature of\nthese developments, this article studies the heterogeneous electric vehicle\nrouting problem with time-window constraints (HEVRPTW). To solve this variant\nof vehicle routing problem (VRP), we propose a DRL-based approach, named\nEdge-enhanced Dual attentIon encoderR and feature-EnhanCed dual aTtention\ndecoder (Edge-DIRECT). Edge-DIRECT features an extra graph representation, the\nnode connectivity of which is based on the overlap of customer time-windows.\nEdge-DIRECT's self-attention encoding mechanism is enhanced by exploiting the\nenergy consumption and travel time between the locations. To effectively\naccount for the heterogeneity of the EVs' fleet, a dual attention decoder has\nbeen introduced. Experimental results based on two real-world datasets reveal\nthat Edge-DIRECT outperforms a state-of-the-art DRL-based method and a\nwell-established heuristic approach in solution quality and execution time.\nFurthermore, it exhibits competitive performance when compared to another\nleading heuristic method.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01615v1",
    "published_date": "2024-06-28 03:18:12 UTC",
    "updated_date": "2024-06-28 03:18:12 UTC"
  },
  {
    "arxiv_id": "2406.19622v1",
    "title": "Data-Driven Lipschitz Continuity: A Cost-Effective Approach to Improve Adversarial Robustness",
    "authors": [
      "Erh-Chung Chen",
      "Pin-Yu Chen",
      "I-Hsin Chung",
      "Che-Rung Lee"
    ],
    "abstract": "The security and robustness of deep neural networks (DNNs) have become\nincreasingly concerning. This paper aims to provide both a theoretical\nfoundation and a practical solution to ensure the reliability of DNNs. We\nexplore the concept of Lipschitz continuity to certify the robustness of DNNs\nagainst adversarial attacks, which aim to mislead the network with adding\nimperceptible perturbations into inputs. We propose a novel algorithm that\nremaps the input domain into a constrained range, reducing the Lipschitz\nconstant and potentially enhancing robustness. Unlike existing adversarially\ntrained models, where robustness is enhanced by introducing additional examples\nfrom other datasets or generative models, our method is almost cost-free as it\ncan be integrated with existing models without requiring re-training.\nExperimental results demonstrate the generalizability of our method, as it can\nbe combined with various models and achieve enhancements in robustness.\nFurthermore, our method achieves the best robust accuracy for CIFAR10,\nCIFAR100, and ImageNet datasets on the RobustBench leaderboard.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.19622v1",
    "published_date": "2024-06-28 03:10:36 UTC",
    "updated_date": "2024-06-28 03:10:36 UTC"
  },
  {
    "arxiv_id": "2406.19614v1",
    "title": "A Survey on Data Quality Dimensions and Tools for Machine Learning",
    "authors": [
      "Yuhan Zhou",
      "Fengjiao Tu",
      "Kewei Sha",
      "Junhua Ding",
      "Haihua Chen"
    ],
    "abstract": "Machine learning (ML) technologies have become substantial in practically all\naspects of our society, and data quality (DQ) is critical for the performance,\nfairness, robustness, safety, and scalability of ML models. With the large and\ncomplex data in data-centric AI, traditional methods like exploratory data\nanalysis (EDA) and cross-validation (CV) face challenges, highlighting the\nimportance of mastering DQ tools. In this survey, we review 17 DQ evaluation\nand improvement tools in the last 5 years. By introducing the DQ dimensions,\nmetrics, and main functions embedded in these tools, we compare their strengths\nand limitations and propose a roadmap for developing open-source DQ tools for\nML. Based on the discussions on the challenges and emerging trends, we further\nhighlight the potential applications of large language models (LLMs) and\ngenerative AI in DQ evaluation and improvement for ML. We believe this\ncomprehensive survey can enhance understanding of DQ in ML and could drive\nprogress in data-centric AI. A complete list of the literature investigated in\nthis survey is available on GitHub at:\nhttps://github.com/haihua0913/awesome-dq4ml.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper has been accepted by The 6th IEEE International Conference\n  on Artificial Intelligence Testing (IEEE AITest 2024) as an invited paper",
    "pdf_url": "http://arxiv.org/pdf/2406.19614v1",
    "published_date": "2024-06-28 02:41:33 UTC",
    "updated_date": "2024-06-28 02:41:33 UTC"
  },
  {
    "arxiv_id": "2407.12023v1",
    "title": "CMMaTH: A Chinese Multi-modal Math Skill Evaluation Benchmark for Foundation Models",
    "authors": [
      "Zhong-Zhi Li",
      "Ming-Liang Zhang",
      "Fei Yin",
      "Zhi-Long Ji",
      "Jin-Feng Bai",
      "Zhen-Ru Pan",
      "Fan-Hu Zeng",
      "Jian Xu",
      "Jia-Xin Zhang",
      "Cheng-Lin Liu"
    ],
    "abstract": "Due to the rapid advancements in multimodal large language models, evaluating\ntheir multimodal mathematical capabilities continues to receive wide attention.\nDespite the datasets like MathVista proposed benchmarks for assessing\nmathematical capabilities in multimodal scenarios, there is still a lack of\ncorresponding evaluation tools and datasets for fine-grained assessment in the\ncontext of K12 education in Chinese language. To systematically evaluate the\ncapability of multimodal large models in solving Chinese multimodal\nmathematical problems, we propose a Chinese Multi-modal Math Skill Evaluation\nBenchmark, named CMMaTH, contraining 23k multimodal K12 math related questions,\nforming the largest Chinese multimodal mathematical problem benchmark to date.\nCMMaTH questions from elementary to high school levels, provide increased\ndiversity in problem types, solution objectives, visual elements, detailed\nknowledge points, and standard solution annotations. We have constructed an\nopen-source tool GradeGPT integrated with the CMMaTH dataset, facilitating\nstable, rapid, and cost-free model evaluation. Our data and code are available.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12023v1",
    "published_date": "2024-06-28 02:35:51 UTC",
    "updated_date": "2024-06-28 02:35:51 UTC"
  },
  {
    "arxiv_id": "2406.19611v1",
    "title": "Multimodal Data Integration for Precision Oncology: Challenges and Future Directions",
    "authors": [
      "Huajun Zhou",
      "Fengtao Zhou",
      "Chenyu Zhao",
      "Yingxue Xu",
      "Luyang Luo",
      "Hao Chen"
    ],
    "abstract": "The essence of precision oncology lies in its commitment to tailor targeted\ntreatments and care measures to each patient based on the individual\ncharacteristics of the tumor. The inherent heterogeneity of tumors necessitates\ngathering information from diverse data sources to provide valuable insights\nfrom various perspectives, fostering a holistic comprehension of the tumor.\nOver the past decade, multimodal data integration technology for precision\noncology has made significant strides, showcasing remarkable progress in\nunderstanding the intricate details within heterogeneous data modalities. These\nstrides have exhibited tremendous potential for improving clinical\ndecision-making and model interpretation, contributing to the advancement of\ncancer care and treatment. Given the rapid progress that has been achieved, we\nprovide a comprehensive overview of about 300 papers detailing cutting-edge\nmultimodal data integration techniques in precision oncology. In addition, we\nconclude the primary clinical applications that have reaped significant\nbenefits, including early assessment, diagnosis, prognosis, and biomarker\ndiscovery. Finally, derived from the findings of this survey, we present an\nin-depth analysis that explores the pivotal challenges and reveals essential\npathways for future research in the field of multimodal data integration for\nprecision oncology.",
    "categories": [
      "q-bio.QM",
      "cs.AI"
    ],
    "primary_category": "q-bio.QM",
    "comment": "15 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.19611v1",
    "published_date": "2024-06-28 02:35:05 UTC",
    "updated_date": "2024-06-28 02:35:05 UTC"
  },
  {
    "arxiv_id": "2407.01456v1",
    "title": "Information-Theoretic Foundations for Neural Scaling Laws",
    "authors": [
      "Hong Jun Jeon",
      "Benjamin Van Roy"
    ],
    "abstract": "Neural scaling laws aim to characterize how out-of-sample error behaves as a\nfunction of model and training dataset size. Such scaling laws guide allocation\nof a computational resources between model and data processing to minimize\nerror. However, existing theoretical support for neural scaling laws lacks\nrigor and clarity, entangling the roles of information and optimization. In\nthis work, we develop rigorous information-theoretic foundations for neural\nscaling laws. This allows us to characterize scaling laws for data generated by\na two-layer neural network of infinite width. We observe that the optimal\nrelation between data and model size is linear, up to logarithmic factors,\ncorroborating large-scale empirical investigations. Concise yet general results\nof the kind we establish may bring clarity to this topic and inform future\ninvestigations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "arXiv admin note: text overlap with arXiv:2212.01365",
    "pdf_url": "http://arxiv.org/pdf/2407.01456v1",
    "published_date": "2024-06-28 02:20:54 UTC",
    "updated_date": "2024-06-28 02:20:54 UTC"
  },
  {
    "arxiv_id": "2407.01614v3",
    "title": "Enhancing Stability for Large Language Models Training in Constrained Bandwidth Networks",
    "authors": [
      "Yun Dai",
      "Tejas Dharamsi",
      "Byron Hsu",
      "Tao Song",
      "Hamed Firooz"
    ],
    "abstract": "Training extremely large language models (LLMs) with billions of parameters\nis a computationally intensive task that pushes the limits of current data\nparallel training systems. While techniques like ZeRO++ have enabled efficient\ndistributed training of such giant models on inexpensive low-bandwidth\nclusters, they can suffer from convergence issues due to potential race\nconditions in the hierarchical partitioning (hpZ) scheme employed to reduce\ncross-machine communication. In this work, we first show how these race\nconditions cause instability when training models with billions of parameters.\nWe then propose a modification to the partitioning algorithm that addresses\nthese convergence challenges while maintaining competitive training efficiency.\nEmpirical evaluation on training the multi-billion parameters Falcon Models and\nLlama-2 models demonstrates the updated algorithm's ability to achieve reliable\nconvergence on these massive models, where stock ZeRO++ hpZ fails to converge.\nThe updated algorithm enables robust training of larger models with 98\\%\nthroughput and model training speed improvement without sacrificing the quality\nof convergence.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01614v3",
    "published_date": "2024-06-28 01:46:10 UTC",
    "updated_date": "2024-10-06 01:18:35 UTC"
  },
  {
    "arxiv_id": "2407.12022v3",
    "title": "ITERTL: An Iterative Framework for Fine-tuning LLMs for RTL Code Generation",
    "authors": [
      "Peiyang Wu",
      "Nan Guo",
      "Xiao Xiao",
      "Wenming Li",
      "Xiaochun Ye",
      "Dongrui Fan"
    ],
    "abstract": "Recently, large language models (LLMs) have demonstrated excellent\nperformance, inspiring researchers to explore their use in automating register\ntransfer level (RTL) code generation and improving hardware design efficiency.\nHowever, the existing approaches to fine-tune LLMs for RTL generation typically\nare conducted on fixed datasets, which do not fully stimulate the capability of\nLLMs and require large amounts of reference data, which are costly to acquire.\nTo mitigate these issues, we innovatively introduce an iterative training\nparadigm named ITERTL. During each iteration, samples are drawn from the model\ntrained in the previous cycle. Then these new samples are employed for training\nin current loop. Furthermore, we introduce a plug-and-play data filtering\nstrategy, thereby encouraging the model to generate high-quality,\nself-contained code. Our model outperforms GPT4 and state-of-the-art (SOTA)\nopen-source models, achieving remarkable 53.8% pass@1 rate on VerilogEval-human\nbenchmark. Under similar conditions of data quantity and quality, our approach\nsignificantly outperforms the baseline. Extensive experiments validate the\neffectiveness of the proposed method.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12022v3",
    "published_date": "2024-06-28 01:44:57 UTC",
    "updated_date": "2025-04-23 06:56:00 UTC"
  },
  {
    "arxiv_id": "2406.19596v1",
    "title": "Optimizing Cyber Defense in Dynamic Active Directories through Reinforcement Learning",
    "authors": [
      "Diksha Goel",
      "Kristen Moore",
      "Mingyu Guo",
      "Derui Wang",
      "Minjune Kim",
      "Seyit Camtepe"
    ],
    "abstract": "This paper addresses a significant gap in Autonomous Cyber Operations (ACO)\nliterature: the absence of effective edge-blocking ACO strategies in dynamic,\nreal-world networks. It specifically targets the cybersecurity vulnerabilities\nof organizational Active Directory (AD) systems. Unlike the existing literature\non edge-blocking defenses which considers AD systems as static entities, our\nstudy counters this by recognizing their dynamic nature and developing advanced\nedge-blocking defenses through a Stackelberg game model between attacker and\ndefender. We devise a Reinforcement Learning (RL)-based attack strategy and an\nRL-assisted Evolutionary Diversity Optimization-based defense strategy, where\nthe attacker and defender improve each other strategy via parallel gameplay. To\naddress the computational challenges of training attacker-defender strategies\non numerous dynamic AD graphs, we propose an RL Training Facilitator that\nprunes environments and neural networks to eliminate irrelevant elements,\nenabling efficient and scalable training for large graphs. We extensively train\nthe attacker strategy, as a sophisticated attacker model is essential for a\nrobust defense. Our empirical results successfully demonstrate that our\nproposed approach enhances defender's proficiency in hardening dynamic AD\ngraphs while ensuring scalability for large-scale AD.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "The manuscript has been accepted as full paper at European Symposium\n  on Research in Computer Security (ESORICS) 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.19596v1",
    "published_date": "2024-06-28 01:37:46 UTC",
    "updated_date": "2024-06-28 01:37:46 UTC"
  },
  {
    "arxiv_id": "2407.01613v1",
    "title": "Self-adaptive weights based on balanced residual decay rate for physics-informed neural networks and deep operator networks",
    "authors": [
      "Wenqian Chen",
      "Amanda A. Howard",
      "Panos Stinis"
    ],
    "abstract": "Physics-informed deep learning has emerged as a promising alternative for\nsolving partial differential equations. However, for complex problems, training\nthese networks can still be challenging, often resulting in unsatisfactory\naccuracy and efficiency. In this work, we demonstrate that the failure of plain\nphysics-informed neural networks arises from the significant discrepancy in the\nconvergence speed of residuals at different training points, where the slowest\nconvergence speed dominates the overall solution convergence. Based on these\nobservations, we propose a point-wise adaptive weighting method that balances\nthe residual decay rate across different training points. The performance of\nour proposed adaptive weighting method is compared with current\nstate-of-the-art adaptive weighting methods on benchmark problems for both\nphysics-informed neural networks and physics-informed deep operator networks.\nThrough extensive numerical results we demonstrate that our proposed approach\nof balanced residual decay rates offers several advantages, including bounded\nweights, high prediction accuracy, fast convergence speed, low training\nuncertainty, low computational cost and ease of hyperparameter tuning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "13 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.01613v1",
    "published_date": "2024-06-28 00:53:48 UTC",
    "updated_date": "2024-06-28 00:53:48 UTC"
  },
  {
    "arxiv_id": "2407.00125v1",
    "title": "A Survey on Failure Analysis and Fault Injection in AI Systems",
    "authors": [
      "Guangba Yu",
      "Gou Tan",
      "Haojia Huang",
      "Zhenyu Zhang",
      "Pengfei Chen",
      "Roberto Natella",
      "Zibin Zheng"
    ],
    "abstract": "The rapid advancement of Artificial Intelligence (AI) has led to its\nintegration into various areas, especially with Large Language Models (LLMs)\nsignificantly enhancing capabilities in Artificial Intelligence Generated\nContent (AIGC). However, the complexity of AI systems has also exposed their\nvulnerabilities, necessitating robust methods for failure analysis (FA) and\nfault injection (FI) to ensure resilience and reliability. Despite the\nimportance of these techniques, there lacks a comprehensive review of FA and FI\nmethodologies in AI systems. This study fills this gap by presenting a detailed\nsurvey of existing FA and FI approaches across six layers of AI systems. We\nsystematically analyze 160 papers and repositories to answer three research\nquestions including (1) what are the prevalent failures in AI systems, (2) what\ntypes of faults can current FI tools simulate, (3) what gaps exist between the\nsimulated faults and real-world failures. Our findings reveal a taxonomy of AI\nsystem failures, assess the capabilities of existing FI tools, and highlight\ndiscrepancies between real-world and simulated failures. Moreover, this survey\ncontributes to the field by providing a framework for fault diagnosis,\nevaluating the state-of-the-art in FI, and identifying areas for improvement in\nFI techniques to enhance the resilience of AI systems.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00125v1",
    "published_date": "2024-06-28 00:32:03 UTC",
    "updated_date": "2024-06-28 00:32:03 UTC"
  },
  {
    "arxiv_id": "2407.17475v1",
    "title": "An Approach to Detect Abnormal Submissions for CodeWorkout Dataset",
    "authors": [
      "Alex Hicks",
      "Yang Shi",
      "Arun-Balajiee Lekshmi-Narayanan",
      "Wei Yan",
      "Samiha Marwan"
    ],
    "abstract": "Students interactions while solving problems in learning environments (i.e.\nlog data) are often used to support students learning. For example, researchers\nuse log data to develop systems that can provide students with personalized\nproblem recommendations based on their knowledge level. However, anomalies in\nthe students log data, such as cheating to solve programming problems, could\nintroduce a hidden bias in the log data. As a result, these systems may provide\ninaccurate problem recommendations, and therefore, defeat their purpose.\nClassical cheating detection methods, such as MOSS, can be used to detect code\nplagiarism. However, these methods cannot detect other abnormal events such as\na student gaming a system with multiple attempts of similar solutions to a\nparticular programming problem. This paper presents a preliminary study to\nanalyze log data with anomalies. The goal of our work is to overcome the\nabnormal instances when modeling personalizable recommendations in programming\nlearning environments.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.17475v1",
    "published_date": "2024-06-28 00:26:15 UTC",
    "updated_date": "2024-06-28 00:26:15 UTC"
  }
]
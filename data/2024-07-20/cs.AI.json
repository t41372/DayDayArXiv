{
  "date": "2024-07-20",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-07-20 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 46 篇论文，主要聚焦 AI 和机器学习领域，包括语言模型的泛化与记忆机制、模仿学习理论进展、AI 数据隐私挑战，以及生成模型的应用。重点文章包括 NeurIPS 2024 的模仿学习研究和 ICLR 2025 的语言模型分析，这些工作揭示了 AI 的核心问题，并涉及知名学者如 Dylan J. Foster 的贡献；此外，AI 数据隐私和生成模型的创新也值得关注。\n\n下面，我将挑选并简要讨论几篇重要或有话题度的论文，先从 AI 和 LLM 核心主题入手，再快速触及相关领域。其他较常规的论文（如特定应用或小规模实验）将简略掠过，以控制篇幅。\n\n### AI 和 LLM 核心主题\n1. **是否仅需行为克隆？理解模仿学习中的时间范围 (Is Behavior Cloning All You Need? Understanding Horizon in Imitation Learning)**  \n   这篇 NeurIPS 2024 论文（作者：Dylan J. Foster 等）分析了行为克隆在模仿学习中的样本复杂度，发现通过控制累积回报范围和监督学习复杂度，可以实现独立于时间范围的样本复杂度。主要贡献是缩小了离线和在线模仿学习的差距，并在强化学习任务和语言生成实验中验证了其实用性。\n\n2. **泛化 vs. 记忆：追踪语言模型能力的预训练数据根源 (Generalization v.s. Memorization: Tracing Language Models' Capabilities Back to Pretraining Data)**  \n   ICLR 2025 论文（作者：Xinyi Wang 等）引入分布记忆概念，评估语言模型（如 Pythia）在机器翻译、事实问答等任务中的记忆依赖。关键发现是，模型规模增大时，问答任务更依赖记忆，而翻译和推理任务更注重泛化，提供了一个可扩展的分析框架，帮助理解模型在不同任务中的鲁棒性。\n\n3. **生成 AI 和大型语言模型的最新进展：现状、挑战与展望 (Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives)**  \n   这篇 IEEE Transactions on Artificial Intelligence 综述（作者：Desta Haileselassie Hagos 等）概述了生成 AI 和 LLM 的技术基础、应用和挑战，如数据隐私和泛化问题。主要贡献是识别研究空白，并为 AI 社区提供协作指导，强调了 LLM 在多领域的潜力。\n\n4. **AI 数据共享的危机：AI 数据公共库的快速衰退 (Consent in Crisis: The Rapid Decline of the AI Data Commons)**  \n   这篇论文（作者众多，包括 Sara Hooker）首次大规模审计 AI 训练语料（如 C4）的网页数据许可协议，发现 2023-2024 年间数据限制激增，导致 5%+ 令牌完全不可用。关键发现是，现有协议无法应对 AI 重用需求，可能影响 AI 的多样性和扩展性。\n\n### 计算机视觉和生成模型\n5. **扩散模型作为数据挖掘工具 (Diffusion Models as Data Mining Tools)**  \n   ECCV 2024 论文（作者：Ioannis Siglidis 等）提出使用扩散模型从图像数据中挖掘视觉模式，通过典型性度量分析不同标签下的元素。主贡献是提供高效、可扩展的数据挖掘方法，并在历史图像和街景数据集上验证了其优势。\n\n6. **Sim-CLIP：无监督孪生对抗微调用于鲁棒的视觉-语言模型 (Sim-CLIP: Unsupervised Siamese Adversarial Fine-Tuning for Robust and Semantically-Rich Vision-Language Models)**  \n   这篇论文（作者：Md Zarif Hossain 等）提升了 CLIP 模型的对抗鲁棒性，使用孪生架构和余弦相似性损失。关键发现是，微调后模型在保持语义完整性的同时，提高了对抗攻击抵抗力，适用于多模态系统。\n\n### 其他领域快速掠过\n其余论文多聚焦特定应用，如医疗图像处理（e.g., SongCi for forensic pathology）、智能电网（e.g., Decentralized Federated Anomaly Detection）或生成模型优化（e.g., Diff4VS for drug design）。这些工作虽有贡献，如 PASSION 在多模态图像分割中的模态平衡策略，或 POGEMA 在多代理路径规划的基准平台，但相对常规，我们仅简要提及：它们提供了实用工具，但影响力不如上述 AI 核心主题显著。感兴趣读者可查阅完整摘要。\n\n总之，今天的 arXiv 更新突显 AI 领域的理论与伦理挑战，NeurIPS 和 ICLR 相关论文尤其值得追踪。保持关注这些进展，有助于理解 AI 的未来方向！",
  "papers": [
    {
      "arxiv_id": "2407.15007v2",
      "title": "Is Behavior Cloning All You Need? Understanding Horizon in Imitation Learning",
      "title_zh": "行为克隆就是你所需要的全部吗？ 理解模仿学习中的地平线",
      "authors": [
        "Dylan J. Foster",
        "Adam Block",
        "Dipendra Misra"
      ],
      "abstract": "Imitation learning (IL) aims to mimic the behavior of an expert in a\nsequential decision making task by learning from demonstrations, and has been\nwidely applied to robotics, autonomous driving, and autoregressive text\ngeneration. The simplest approach to IL, behavior cloning (BC), is thought to\nincur sample complexity with unfavorable quadratic dependence on the problem\nhorizon, motivating a variety of different online algorithms that attain\nimproved linear horizon dependence under stronger assumptions on the data and\nthe learner's access to the expert.\n  We revisit the apparent gap between offline and online IL from a\nlearning-theoretic perspective, with a focus on the realizable/well-specified\nsetting with general policy classes up to and including deep neural networks.\nThrough a new analysis of behavior cloning with the logarithmic loss, we show\nthat it is possible to achieve horizon-independent sample complexity in offline\nIL whenever (i) the range of the cumulative payoffs is controlled, and (ii) an\nappropriate notion of supervised learning complexity for the policy class is\ncontrolled. Specializing our results to deterministic, stationary policies, we\nshow that the gap between offline and online IL is smaller than previously\nthought: (i) it is possible to achieve linear dependence on horizon in offline\nIL under dense rewards (matching what was previously only known to be\nachievable in online IL); and (ii) without further assumptions on the policy\nclass, online IL cannot improve over offline IL with the logarithmic loss, even\nin benign MDPs. We complement our theoretical results with experiments on\nstandard RL tasks and autoregressive language generation to validate the\npractical relevance of our findings.",
      "tldr_zh": "本研究探讨了模仿学习(Imitation Learning, IL)中行为克隆(Behavior Cloning, BC)的方法是否足够，特别关注样本复杂度对问题地平线(horizon)的依赖。作者通过对日志损失的分析证明，在可实现设置下，如果累计回报范围可控且政策类的监督学习复杂度受限，离线 IL 可以实现独立于地平线的样本复杂度，甚至在密集奖励场景下达到与在线 IL 相当的线性地平线依赖。实验结果显示，在标准强化学习任务和自回归语言生成中，这些发现具有实际相关性，进一步缩小了离线和在线 IL 的差距。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.ST",
        "stat.ML",
        "stat.TH"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.15007v2",
      "published_date": "2024-07-20 23:31:56 UTC",
      "updated_date": "2024-11-30 18:07:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:24:34.277280"
    },
    {
      "arxiv_id": "2407.14985v5",
      "title": "Generalization v.s. Memorization: Tracing Language Models' Capabilities Back to Pretraining Data",
      "title_zh": "翻译失败",
      "authors": [
        "Xinyi Wang",
        "Antonis Antoniades",
        "Yanai Elazar",
        "Alfonso Amayuelas",
        "Alon Albalak",
        "Kexun Zhang",
        "William Yang Wang"
      ],
      "abstract": "The impressive capabilities of large language models (LLMs) have sparked\ndebate over whether these models genuinely generalize to unseen tasks or\npredominantly rely on memorizing vast amounts of pretraining data. To explore\nthis issue, we introduce an extended concept of memorization, distributional\nmemorization, which measures the correlation between the LLM output\nprobabilities and the pretraining data frequency. To effectively capture\ntask-specific pretraining data frequency, we propose a novel task-gram language\nmodel, which is built by counting the co-occurrence of semantically related\n$n$-gram pairs from task inputs and outputs in the pretraining corpus. Using\nthe Pythia models trained on the Pile dataset, we evaluate four distinct tasks:\nmachine translation, factual question answering, world knowledge understanding,\nand math reasoning. Our findings reveal varying levels of memorization, with\nthe strongest effect observed in factual question answering. Furthermore, while\nmodel performance improves across all tasks as LLM size increases, only factual\nquestion answering shows an increase in memorization, whereas machine\ntranslation and reasoning tasks exhibit greater generalization, producing more\nnovel outputs. This study demonstrates that memorization plays a larger role in\nsimpler, knowledge-intensive tasks, while generalization is the key for harder,\nreasoning-based tasks, providing a scalable method for analyzing large\npretraining corpora in greater depth.",
      "tldr_zh": "本研究探讨大型语言模型 (LLMs) 的能力是否源于真正泛化，还是主要依赖预训练数据的记忆化。作者引入了“distributional memorization”概念，并提出“task-gram language model”来测量任务特定预训练数据频率，通过计数语义相关 n-gram 对的共现。使用 Pythia 模型和 Pile 数据集，他们评估了机器翻译、事实问答、世界知识理解和数学推理四个任务，结果显示事实问答任务的记忆化效应最强，而模型规模增大时，仅事实问答的记忆化增加，机器翻译和推理任务则表现出更多泛化。总体而言，该研究揭示记忆化在简单知识密集型任务中更突出，而泛化是复杂推理任务的关键，并提供了一种可扩展的方法深入分析预训练语料。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2407.14985v5",
      "published_date": "2024-07-20 21:24:40 UTC",
      "updated_date": "2025-03-02 03:27:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:24:45.696918"
    },
    {
      "arxiv_id": "2407.14984v1",
      "title": "Enhancing Microgrid Performance Prediction with Attention-based Deep Learning Models",
      "title_zh": "使用基于注意力的深度学习模型增强微电网性能预测",
      "authors": [
        "Vinod Kumar Maddineni",
        "Naga Babu Koganti",
        "Praveen Damacharla"
      ],
      "abstract": "In this research, an effort is made to address microgrid systems' operational\nchallenges, characterized by power oscillations that eventually contribute to\ngrid instability. An integrated strategy is proposed, leveraging the strengths\nof convolutional and Gated Recurrent Unit (GRU) layers. This approach is aimed\nat effectively extracting temporal data from energy datasets to improve the\nprecision of microgrid behavior forecasts. Additionally, an attention layer is\nemployed to underscore significant features within the time-series data,\noptimizing the forecasting process. The framework is anchored by a Multi-Layer\nPerceptron (MLP) model, which is tasked with comprehensive load forecasting and\nthe identification of abnormal grid behaviors. Our methodology underwent\nrigorous evaluation using the Micro-grid Tariff Assessment Tool dataset, with\nRoot Mean Square Error (RMSE), Mean Absolute Error (MAE), and the coefficient\nof determination (r2-score) serving as the primary metrics. The approach\ndemonstrated exemplary performance, evidenced by a MAE of 0.39, RMSE of 0.28,\nand an r2-score of 98.89\\% in load forecasting, along with near-perfect zero\nstate prediction accuracy (approximately 99.9\\%). Significantly outperforming\nconventional machine learning models such as support vector regression and\nrandom forest regression, our model's streamlined architecture is particularly\nsuitable for real-time applications, thereby facilitating more effective and\nreliable microgrid management.",
      "tldr_zh": "本研究针对微电网系统的操作挑战（如功率震荡导致的网格不稳定），提出了一种集成深度学习策略，利用卷积层和 Gated Recurrent Unit (GRU) 层提取时间序列数据，并通过注意力层突出关键特征，最后以 Multi-Layer Perceptron (MLP) 模型进行负载预测和异常识别。实验使用 Micro-grid Tariff Assessment Tool 数据集评估，取得了 MAE 0.39、RMSE 0.28 和 r2-score 98.89% 的出色性能，并在零状态预测准确率接近 99.9%。该方法显著优于传统模型如支持向量回归和随机森林回归，适合实时应用，从而提升微电网管理的可靠性和效率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "2024 11th International Conference on Information Technology,\n  Computer, and Electrical Engineering (ICITACEE)",
      "pdf_url": "http://arxiv.org/pdf/2407.14984v1",
      "published_date": "2024-07-20 21:24:11 UTC",
      "updated_date": "2024-07-20 21:24:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:24:56.306576"
    },
    {
      "arxiv_id": "2407.14982v1",
      "title": "GreenStableYolo: Optimizing Inference Time and Image Quality of Text-to-Image Generation",
      "title_zh": "GreenStableYolo：优化文本到图像生成的推理时间和图像质量",
      "authors": [
        "Jingzhi Gong",
        "Sisi Li",
        "Giordano d'Aloisio",
        "Zishuo Ding",
        "Yulong Ye",
        "William B. Langdon",
        "Federica Sarro"
      ],
      "abstract": "Tuning the parameters and prompts for improving AI-based text-to-image\ngeneration has remained a substantial yet unaddressed challenge. Hence we\nintroduce GreenStableYolo, which improves the parameters and prompts for Stable\nDiffusion to both reduce GPU inference time and increase image generation\nquality using NSGA-II and Yolo.\n  Our experiments show that despite a relatively slight trade-off (18%) in\nimage quality compared to StableYolo (which only considers image quality),\nGreenStableYolo achieves a substantial reduction in inference time (266% less)\nand a 526% higher hypervolume, thereby advancing the state-of-the-art for\ntext-to-image generation.",
      "tldr_zh": "本论文介绍了 GreenStableYolo，一种针对文本到图像生成优化的框架，通过调整 Stable Diffusion 的参数和提示，使用 NSGA-II 和 Yolo 算法来同时减少 GPU inference time 并提升图像质量。相比于仅关注图像质量的 StableYolo，该方法在实验中实现了推理时间减少 266% 和 hypervolume 提高 526% 的显著改进，尽管图像质量仅略微下降 18%。GreenStableYolo 的创新推进了文本到图像生成的 state-of-the-art 水平，提供更高效的 AI 生成解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper is published in the SSBSE Challenge Track 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.14982v1",
      "published_date": "2024-07-20 21:14:24 UTC",
      "updated_date": "2024-07-20 21:14:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:25:08.394774"
    },
    {
      "arxiv_id": "2407.14975v1",
      "title": "A Measure for Level of Autonomy Based on Observable System Behavior",
      "title_zh": "翻译失败",
      "authors": [
        "Jason M. Pittman"
      ],
      "abstract": "Contemporary artificial intelligence systems are pivotal in enhancing human\nefficiency and safety across various domains. One such domain is autonomous\nsystems, especially in automotive and defense use cases. Artificial\nintelligence brings learning and enhanced decision-making to autonomy system\ngoal-oriented behaviors and human independence. However, the lack of clear\nunderstanding of autonomy system capabilities hampers human-machine or\nmachine-machine interaction and interdiction. This necessitates varying degrees\nof human involvement for safety, accountability, and explainability purposes.\nYet, measuring the level autonomous capability in an autonomous system presents\na challenge. Two scales of measurement exist, yet measuring autonomy\npresupposes a variety of elements not available in the wild. This is why\nexisting measures for level of autonomy are operationalized only during design\nor test and evaluation phases. No measure for level of autonomy based on\nobserved system behavior exists at this time. To address this, we outline a\npotential measure for predicting level of autonomy using observable actions. We\nalso present an algorithm incorporating the proposed measure. The measure and\nalgorithm have significance to researchers and practitioners interested in a\nmethod to blind compare autonomous systems at runtime. Defense-based\nimplementations are likewise possible because counter-autonomy depends on\nrobust identification of autonomous systems.",
      "tldr_zh": "这篇论文针对自主系统（如汽车和国防领域）的“level of autonomy”测量问题，指出现有方法仅限于设计或测试阶段，无法基于实际行为进行评估。作者提出了一种新测量方法，通过分析可观察系统行为来预测自主水平，并开发了一个相关算法，以实现运行时的系统比较。实验和应用表明，该方法有助于提升人机互动的安全性、责任性和可解释性，尤其在国防领域的反自主技术中具有重要意义。",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 1 figure, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.14975v1",
      "published_date": "2024-07-20 20:34:20 UTC",
      "updated_date": "2024-07-20 20:34:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:25:19.912024"
    },
    {
      "arxiv_id": "2407.14974v1",
      "title": "Out of spuriousity: Improving robustness to spurious correlations without group annotations",
      "title_zh": "翻译失败",
      "authors": [
        "Phuong Quynh Le",
        "Jörg Schlötterer",
        "Christin Seifert"
      ],
      "abstract": "Machine learning models are known to learn spurious correlations, i.e.,\nfeatures having strong relations with class labels but no causal relation.\nRelying on those correlations leads to poor performance in the data groups\nwithout these correlations and poor generalization ability. To improve the\nrobustness of machine learning models to spurious correlations, we propose an\napproach to extract a subnetwork from a fully trained network that does not\nrely on spurious correlations. The subnetwork is found by the assumption that\ndata points with the same spurious attribute will be close to each other in the\nrepresentation space when training with ERM, then we employ supervised\ncontrastive loss in a novel way to force models to unlearn the spurious\nconnections. The increase in the worst-group performance of our approach\ncontributes to strengthening the hypothesis that there exists a subnetwork in a\nfully trained dense network that is responsible for using only invariant\nfeatures in classification tasks, therefore erasing the influence of spurious\nfeatures even in the setup of multi spurious attributes and no prior knowledge\nof attributes labels.",
      "tldr_zh": "该研究解决了机器学习模型依赖虚假相关性（spurious correlations）的问题，这些相关性虽与标签强相关但无因果关系，导致模型在某些数据组上表现不佳并降低泛化能力。为此，作者提出了一种无需组别标注（group annotations）的方法，从完全训练的网络中提取子网络（subnetwork），通过假设具有相同虚假属性的数据点在表示空间接近，并采用supervised contrastive loss的新颖方式迫使模型“遗忘”这些相关性。实验结果显示，该方法显著提高了最差组别性能（worst-group performance），支持了在密集网络中存在只使用不变特征（invariant features）的子网络，从而在多虚假属性场景下增强模型的鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.14974v1",
      "published_date": "2024-07-20 20:24:14 UTC",
      "updated_date": "2024-07-20 20:24:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:25:33.184985"
    },
    {
      "arxiv_id": "2407.14971v2",
      "title": "Sim-CLIP: Unsupervised Siamese Adversarial Fine-Tuning for Robust and Semantically-Rich Vision-Language Models",
      "title_zh": "Sim-CLIP：无监督",
      "authors": [
        "Md Zarif Hossain",
        "Ahmed Imteaj"
      ],
      "abstract": "Vision-language models (VLMs) have achieved significant strides in recent\ntimes specially in multimodal tasks, yet they remain susceptible to adversarial\nattacks on their vision components. To address this, we propose Sim-CLIP, an\nunsupervised adversarial fine-tuning method that enhances the robustness of the\nwidely-used CLIP vision encoder against such attacks while maintaining semantic\nrichness and specificity. By employing a Siamese architecture with cosine\nsimilarity loss, Sim-CLIP learns semantically meaningful and attack-resilient\nvisual representations without requiring large batch sizes or momentum\nencoders. Our results demonstrate that VLMs enhanced with Sim-CLIP's fine-tuned\nCLIP encoder exhibit significantly enhanced robustness against adversarial\nattacks, while preserving semantic meaning of the perturbed images. Notably,\nSim-CLIP does not require additional training or fine-tuning of the VLM itself;\nreplacing the original vision encoder with our fine-tuned Sim-CLIP suffices to\nprovide robustness. This work underscores the significance of reinforcing\nfoundational models like CLIP to safeguard the reliability of downstream VLM\napplications, paving the way for more secure and effective multimodal systems.",
      "tldr_zh": "本文提出 Sim-CLIP，一种无监督的 Siamese 架构对抗微调方法，旨在提升 CLIP 视觉编码器的鲁棒性，同时保持视觉表示的语义丰富性和特异性。该方法通过 cosine similarity loss 学习抗攻击的视觉表示，无需大批量数据或 momentum encoders，仅需替换原视觉编码器即可增强 VLMs 的整体性能。实验结果显示，Sim-CLIP 显著提高了 VLMs 对对抗攻击的鲁棒性，同时保留了扰动图像的语义含义。该工作强化了基础模型如 CLIP 的可靠性，为更安全有效的多模态系统铺平道路。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.14971v2",
      "published_date": "2024-07-20 19:53:52 UTC",
      "updated_date": "2024-11-15 21:09:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:25:45.719789"
    },
    {
      "arxiv_id": "2407.14962v5",
      "title": "Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives",
      "title_zh": "翻译失败",
      "authors": [
        "Desta Haileselassie Hagos",
        "Rick Battle",
        "Danda B. Rawat"
      ],
      "abstract": "The emergence of Generative Artificial Intelligence (AI) and Large Language\nModels (LLMs) has marked a new era of Natural Language Processing (NLP),\nintroducing unprecedented capabilities that are revolutionizing various\ndomains. This paper explores the current state of these cutting-edge\ntechnologies, demonstrating their remarkable advancements and wide-ranging\napplications. Our paper contributes to providing a holistic perspective on the\ntechnical foundations, practical applications, and emerging challenges within\nthe evolving landscape of Generative AI and LLMs. We believe that understanding\nthe generative capabilities of AI systems and the specific context of LLMs is\ncrucial for researchers, practitioners, and policymakers to collaboratively\nshape the responsible and ethical integration of these technologies into\nvarious domains. Furthermore, we identify and address main research gaps,\nproviding valuable insights to guide future research endeavors within the AI\nresearch community.",
      "tldr_zh": "本论文综述了生成式AI和Large Language Models (LLMs)的最新进展，包括其在Natural Language Processing (NLP)和其他领域的革命性应用和技术基础。研究提供了一个全面视角，涵盖实际应用、关键挑战（如伦理和责任问题），并强调了理解这些技术的必要性，以促进研究者、从业者和政策制定者的合作。最终，论文识别了主要研究空白，并为AI社区的未来研究方向提供宝贵指导。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "This version is accepted for publication in the Journal of IEEE\n  Transactions on Artificial Intelligence (TAI)",
      "pdf_url": "http://arxiv.org/pdf/2407.14962v5",
      "published_date": "2024-07-20 18:48:35 UTC",
      "updated_date": "2024-08-23 14:14:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:25:56.744759"
    },
    {
      "arxiv_id": "2407.17522v1",
      "title": "Mapping the Technological Future: A Topic, Sentiment, and Emotion Analysis in Social Media Discourse",
      "title_zh": "技术未来的映射：社交媒体话语中的主题、情感和情绪分析",
      "authors": [
        "Alina Landowska",
        "Maciej Skorski",
        "Krzysztof Rajda"
      ],
      "abstract": "People worldwide are currently confronted with a number of technological\nchallenges, which act as a potent source of uncertainty. The uncertainty\narising from the volatility and unpredictability of technology (such as AI) and\nits potential consequences is widely discussed on social media. This study uses\nBERTopic modelling along with sentiment and emotion analysis on 1.5 million\ntweets from 2021 to 2023 to identify anticipated tech-driven futures and\ncapture the emotions communicated by 400 key opinion leaders (KOLs). Findings\nindicate positive sentiment significantly outweighs negative, with a prevailing\ndominance of positive anticipatory emotions. Specifically, the 'Hope' score is\napproximately 10.33\\% higher than the median 'Anxiety' score. KOLs emphasize\n'Optimism' and benefits over 'Pessimism' and challenges. The study emphasizes\nthe important role KOLs play in shaping future visions through anticipatory\ndiscourse and emotional tone during times of technological uncertainty.",
      "tldr_zh": "这篇论文通过BERTopic建模结合情感和情绪分析，考察了2021-2023年间150万条推文，以揭示公众对技术未来（如AI）的预期和情绪。研究发现，正面情感远超负面，'Hope'分数比'Anxiety'高10.33%，而关键意见领袖(KOLs)更倾向于强调'Optimism'和益处，而不是'Pessimism'和挑战。总体而言，该研究突出了KOLs在技术不确定时期通过预期话语和情感基调塑造未来愿景的关键作用。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.SI",
        "stat.AP",
        "J.4"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17522v1",
      "published_date": "2024-07-20 18:15:30 UTC",
      "updated_date": "2024-07-20 18:15:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:26:10.283855"
    },
    {
      "arxiv_id": "2408.02752v1",
      "title": "Diffusion Models as Data Mining Tools",
      "title_zh": "扩散模型作为数据挖掘工具",
      "authors": [
        "Ioannis Siglidis",
        "Aleksander Holynski",
        "Alexei A. Efros",
        "Mathieu Aubry",
        "Shiry Ginosar"
      ],
      "abstract": "This paper demonstrates how to use generative models trained for image\nsynthesis as tools for visual data mining. Our insight is that since\ncontemporary generative models learn an accurate representation of their\ntraining data, we can use them to summarize the data by mining for visual\npatterns. Concretely, we show that after finetuning conditional diffusion\nmodels to synthesize images from a specific dataset, we can use these models to\ndefine a typicality measure on that dataset. This measure assesses how typical\nvisual elements are for different data labels, such as geographic location,\ntime stamps, semantic labels, or even the presence of a disease. This\nanalysis-by-synthesis approach to data mining has two key advantages. First, it\nscales much better than traditional correspondence-based approaches since it\ndoes not require explicitly comparing all pairs of visual elements. Second,\nwhile most previous works on visual data mining focus on a single dataset, our\napproach works on diverse datasets in terms of content and scale, including a\nhistorical car dataset, a historical face dataset, a large worldwide\nstreet-view dataset, and an even larger scene dataset. Furthermore, our\napproach allows for translating visual elements across class labels and\nanalyzing consistent changes.",
      "tldr_zh": "本研究探讨了如何将扩散模型（Diffusion Models）作为视觉数据挖掘工具，利用其训练数据准确表示的能力来总结和挖掘视觉模式。具体方法包括微调条件扩散模型，以定义一个典型性度量（typicality measure），评估视觉元素对各种标签（如地理位置、时间戳、语义标签或疾病）的典型性。这种分析-by-synthesis 方法的优势在于扩展性强，不需显式比较所有元素对，并适用于多样数据集，包括历史汽车、面部、全球街景和大型场景数据集。此外，该方法支持视觉元素的跨类标签翻译和变化分析，展示了其在数据挖掘中的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://diff-mining.github.io/ Accepted in ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.02752v1",
      "published_date": "2024-07-20 17:14:31 UTC",
      "updated_date": "2024-07-20 17:14:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:26:20.551987"
    },
    {
      "arxiv_id": "2407.14933v2",
      "title": "Consent in Crisis: The Rapid Decline of the AI Data Commons",
      "title_zh": "翻译失败",
      "authors": [
        "Shayne Longpre",
        "Robert Mahari",
        "Ariel Lee",
        "Campbell Lund",
        "Hamidah Oderinwale",
        "William Brannon",
        "Nayan Saxena",
        "Naana Obeng-Marnu",
        "Tobin South",
        "Cole Hunter",
        "Kevin Klyman",
        "Christopher Klamm",
        "Hailey Schoelkopf",
        "Nikhil Singh",
        "Manuel Cherep",
        "Ahmad Anis",
        "An Dinh",
        "Caroline Chitongo",
        "Da Yin",
        "Damien Sileo",
        "Deividas Mataciunas",
        "Diganta Misra",
        "Emad Alghamdi",
        "Enrico Shippole",
        "Jianguo Zhang",
        "Joanna Materzynska",
        "Kun Qian",
        "Kush Tiwary",
        "Lester Miranda",
        "Manan Dey",
        "Minnie Liang",
        "Mohammed Hamdy",
        "Niklas Muennighoff",
        "Seonghyeon Ye",
        "Seungone Kim",
        "Shrestha Mohanty",
        "Vipul Gupta",
        "Vivek Sharma",
        "Vu Minh Chien",
        "Xuhui Zhou",
        "Yizhi Li",
        "Caiming Xiong",
        "Luis Villa",
        "Stella Biderman",
        "Hanlin Li",
        "Daphne Ippolito",
        "Sara Hooker",
        "Jad Kabbara",
        "Sandy Pentland"
      ],
      "abstract": "General-purpose artificial intelligence (AI) systems are built on massive\nswathes of public web data, assembled into corpora such as C4, RefinedWeb, and\nDolma. To our knowledge, we conduct the first, large-scale, longitudinal audit\nof the consent protocols for the web domains underlying AI training corpora.\nOur audit of 14,000 web domains provides an expansive view of crawlable web\ndata and how codified data use preferences are changing over time. We observe a\nproliferation of AI-specific clauses to limit use, acute differences in\nrestrictions on AI developers, as well as general inconsistencies between\nwebsites' expressed intentions in their Terms of Service and their robots.txt.\nWe diagnose these as symptoms of ineffective web protocols, not designed to\ncope with the widespread re-purposing of the internet for AI. Our longitudinal\nanalyses show that in a single year (2023-2024) there has been a rapid\ncrescendo of data restrictions from web sources, rendering ~5%+ of all tokens\nin C4, or 28%+ of the most actively maintained, critical sources in C4, fully\nrestricted from use. For Terms of Service crawling restrictions, a full 45% of\nC4 is now restricted. If respected or enforced, these restrictions are rapidly\nbiasing the diversity, freshness, and scaling laws for general-purpose AI\nsystems. We hope to illustrate the emerging crises in data consent, for both\ndevelopers and creators. The foreclosure of much of the open web will impact\nnot only commercial AI, but also non-commercial AI and academic research.",
      "tldr_zh": "本研究对 AI 训练语料库（如 C4、RefinedWeb 和 Dolma）的底层网页域进行了首次大规模纵向审计，分析了 14,000 个域的同意协议变化。结果显示，AI 特定条款迅速增多，导致数据使用限制急剧上升，例如 C4 中 5% 以上的令牌和 28% 的关键来源完全受限，而 Terms of Service 爬取限制已覆盖 45% 的 C4 数据。作者诊断这些问题源于 web 协议的无效性，并警告如果这些限制被执行，将偏向 AI 的多样性、新鲜度和扩展性，影响商业、非商业 AI 以及学术研究。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "41 pages (13 main), 5 figures, 9 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.14933v2",
      "published_date": "2024-07-20 16:50:18 UTC",
      "updated_date": "2024-07-24 16:52:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:26:34.635764"
    },
    {
      "arxiv_id": "2407.14931v3",
      "title": "POGEMA: A Benchmark Platform for Cooperative Multi-Agent Pathfinding",
      "title_zh": "POGEMA：合作多智能体路径规划的基准平台",
      "authors": [
        "Alexey Skrynnik",
        "Anton Andreychuk",
        "Anatolii Borzilov",
        "Alexander Chernyavskiy",
        "Konstantin Yakovlev",
        "Aleksandr Panov"
      ],
      "abstract": "Multi-agent reinforcement learning (MARL) has recently excelled in solving\nchallenging cooperative and competitive multi-agent problems in various\nenvironments, typically involving a small number of agents and full\nobservability. Moreover, a range of crucial robotics-related tasks, such as\nmulti-robot pathfinding, which have traditionally been approached with\nclassical non-learnable methods (e.g., heuristic search), are now being\nsuggested for solution using learning-based or hybrid methods. However, in this\ndomain, it remains difficult, if not impossible, to conduct a fair comparison\nbetween classical, learning-based, and hybrid approaches due to the lack of a\nunified framework that supports both learning and evaluation. To address this,\nwe introduce POGEMA, a comprehensive set of tools that includes a fast\nenvironment for learning, a problem instance generator, a collection of\npredefined problem instances, a visualization toolkit, and a benchmarking tool\nfor automated evaluation. We also introduce and define an evaluation protocol\nthat specifies a range of domain-related metrics, computed based on primary\nevaluation indicators (such as success rate and path length), enabling a fair\nmulti-fold comparison. The results of this comparison, which involves a variety\nof state-of-the-art MARL, search-based, and hybrid methods, are presented.",
      "tldr_zh": "这篇论文引入了 POGEMA 平台，作为一个统一的基准测试工具，用于评估合作多智能体路径规划（Cooperative Multi-Agent Pathfinding）的各种方法。POGEMA 提供了一系列工具，包括快速学习环境、问题实例生成器、预定义实例、可视化工具和自动化基准测试，帮助公平比较经典、非学习方法（如启发式搜索）、基于学习的方法（如 MARL）和混合方法。论文还定义了评估协议，基于成功率和路径长度等核心指标计算领域相关指标，并展示了多种最先进方法的比较结果，证明了平台的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "Published as a conference paper at The International Conference on\n  Learning Representations 2025",
      "pdf_url": "http://arxiv.org/pdf/2407.14931v3",
      "published_date": "2024-07-20 16:37:21 UTC",
      "updated_date": "2025-04-08 08:14:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:26:45.821452"
    },
    {
      "arxiv_id": "2407.14926v1",
      "title": "TraveLLM: Could you plan my new public transit route in face of a network disruption?",
      "title_zh": "翻译失败",
      "authors": [
        "Bowen Fang",
        "Zixiao Yang",
        "Shukai Wang",
        "Xuan Di"
      ],
      "abstract": "Imagine there is a disruption in train 1 near Times Square metro station. You\ntry to find an alternative subway route to the JFK airport on Google Maps, but\nthe app fails to provide a suitable recommendation that takes into account the\ndisruption and your preferences to avoid crowded stations. We find that in many\nsuch situations, current navigation apps may fall short and fail to give a\nreasonable recommendation. To fill this gap, in this paper, we develop a\nprototype, TraveLLM, to plan routing of public transit in face of disruption\nthat relies on Large Language Models (LLMs). LLMs have shown remarkable\ncapabilities in reasoning and planning across various domains. Here we hope to\ninvestigate the potential of LLMs that lies in incorporating multi-modal\nuser-specific queries and constraints into public transit route\nrecommendations. Various test cases are designed under different scenarios,\nincluding varying weather conditions, emergency events, and the introduction of\nnew transportation services. We then compare the performance of\nstate-of-the-art LLMs, including GPT-4, Claude 3 and Gemini, in generating\naccurate routes. Our comparative analysis demonstrates the effectiveness of\nLLMs, particularly GPT-4 in providing navigation plans. Our findings hold the\npotential for LLMs to enhance existing navigation systems and provide a more\nflexible and intelligent method for addressing diverse user needs in face of\ndisruptions.",
      "tldr_zh": "本论文探讨了现有导航应用（如Google Maps）在公共交通中断（如地铁线路故障）时，无法提供考虑用户偏好（如避免拥挤站点）的合适路线问题。为此，作者开发了TraveLLM原型系统，利用Large Language Models (LLMs)来规划公共交通路线，该系统能整合多模态用户查询和约束。论文设计了多种测试场景，包括天气变化、紧急事件和新交通服务，并比较了GPT-4、Claude 3和Gemini的性能，结果显示GPT-4在生成准确路线方面表现最佳。这些发现表明，LLMs有潜力提升现有导航系统，使其更灵活、智能地应对用户多样需求。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.14926v1",
      "published_date": "2024-07-20 16:25:34 UTC",
      "updated_date": "2024-07-20 16:25:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:26:57.396138"
    },
    {
      "arxiv_id": "2407.14916v2",
      "title": "Improving Context-Aware Preference Modeling for Language Models",
      "title_zh": "改进语言模型的上下文感知偏好建模",
      "authors": [
        "Silviu Pitis",
        "Ziang Xiao",
        "Nicolas Le Roux",
        "Alessandro Sordoni"
      ],
      "abstract": "While finetuning language models from pairwise preferences has proven\nremarkably effective, the underspecified nature of natural language presents\ncritical challenges. Direct preference feedback is uninterpretable, difficult\nto provide where multidimensional criteria may apply, and often inconsistent,\neither because it is based on incomplete instructions or provided by diverse\nprincipals. To address these challenges, we consider the two-step preference\nmodeling procedure that first resolves the under-specification by selecting a\ncontext, and then evaluates preference with respect to the chosen context. We\ndecompose reward modeling error according to these two steps, which suggests\nthat supervising context in addition to context-specific preference may be a\nviable approach to aligning models with diverse human preferences. For this to\nwork, the ability of models to evaluate context-specific preference is\ncritical. To this end, we contribute context-conditioned preference datasets\nand accompanying experiments that investigate the ability of language models to\nevaluate context-specific preference. We use our datasets to (1) show that\nexisting preference models benefit from, but fail to fully consider, added\ncontext, (2) finetune a context-aware reward model with context-specific\nperformance exceeding that of GPT-4 and Llama 3 70B on tested datasets, and (3)\ninvestigate the value of context-aware preference modeling.",
      "tldr_zh": "该论文探讨了微调语言模型时面临的挑战，包括自然语言的模糊性、偏好反馈的不可解释性和不一致性。作者提出一个两步偏好建模过程：首先通过选择上下文来解决模糊性，然后在特定上下文中评估偏好，并分解reward modeling error以建议监督上下文和上下文特定偏好。贡献包括创建context-conditioned preference datasets，通过实验证明现有模型从上下文受益但未充分利用，并微调了一个context-aware reward model，其性能超过了GPT-4和Llama 3 70B，展示了这种方法的价值。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "NeurIPS 2024. 10 pages (29 with references and appendix)",
      "pdf_url": "http://arxiv.org/pdf/2407.14916v2",
      "published_date": "2024-07-20 16:05:17 UTC",
      "updated_date": "2024-11-06 16:11:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:27:09.280116"
    },
    {
      "arxiv_id": "2407.14910v1",
      "title": "Visual Geo-Localization from images",
      "title_zh": "翻译失败",
      "authors": [
        "Rania Saoud",
        "Slimane Larabi"
      ],
      "abstract": "This paper presents a visual geo-localization system capable of determining\nthe geographic locations of places (buildings and road intersections) from\nimages without relying on GPS data. Our approach integrates three primary\nmethods: Scale-Invariant Feature Transform (SIFT) for place recognition,\ntraditional image processing for identifying road junction types, and deep\nlearning using the VGG16 model for classifying road junctions. The most\neffective techniques have been integrated into an offline mobile application,\nenhancing accessibility for users requiring reliable location information in\nGPS-denied environments.",
      "tldr_zh": "这篇论文提出了一种视觉地理定位系统，能够从图像中确定建筑物和道路交叉口的地理位置，而不依赖 GPS 数据。系统整合了三种主要方法：Scale-Invariant Feature Transform (SIFT) 用于地点识别、传统图像处理用于识别道路交叉口类型，以及基于 VGG16 模型的深度学习用于分类道路交叉口。这些有效技术被集成到一个离线移动应用中，提升了在 GPS-denied environments 中的可靠性和可访问性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "18 pages, 8 figures,",
      "pdf_url": "http://arxiv.org/pdf/2407.14910v1",
      "published_date": "2024-07-20 15:47:21 UTC",
      "updated_date": "2024-07-20 15:47:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:27:19.304214"
    },
    {
      "arxiv_id": "2407.14904v1",
      "title": "Large-vocabulary forensic pathological analyses via prototypical cross-modal contrastive learning",
      "title_zh": "翻译失败",
      "authors": [
        "Chen Shen",
        "Chunfeng Lian",
        "Wanqing Zhang",
        "Fan Wang",
        "Jianhua Zhang",
        "Shuanliang Fan",
        "Xin Wei",
        "Gongji Wang",
        "Kehan Li",
        "Hongshu Mu",
        "Hao Wu",
        "Xinggong Liang",
        "Jianhua Ma",
        "Zhenyuan Wang"
      ],
      "abstract": "Forensic pathology is critical in determining the cause and manner of death\nthrough post-mortem examinations, both macroscopic and microscopic. The field,\nhowever, grapples with issues such as outcome variability, laborious processes,\nand a scarcity of trained professionals. This paper presents SongCi, an\ninnovative visual-language model (VLM) designed specifically for forensic\npathology. SongCi utilizes advanced prototypical cross-modal self-supervised\ncontrastive learning to enhance the accuracy, efficiency, and generalizability\nof forensic analyses. It was pre-trained and evaluated on a comprehensive\nmulti-center dataset, which includes over 16 million high-resolution image\npatches, 2,228 vision-language pairs of post-mortem whole slide images (WSIs),\nand corresponding gross key findings, along with 471 distinct diagnostic\noutcomes. Our findings indicate that SongCi surpasses existing multi-modal AI\nmodels in many forensic pathology tasks, performs comparably to experienced\nforensic pathologists and significantly better than less experienced ones, and\nprovides detailed multi-modal explainability, offering critical assistance in\nforensic investigations. To the best of our knowledge, SongCi is the first VLM\nspecifically developed for forensic pathological analysis and the first\nlarge-vocabulary computational pathology (CPath) model that directly processes\ngigapixel WSIs in forensic science.",
      "tldr_zh": "本研究针对法医病理学面临的挑战，如结果可变性、繁琐过程和专业人员短缺，提出了一种名为 SongCi 的视觉语言模型 (VLM)，利用 prototypical cross-modal contrastive learning 来提升分析的准确性、效率和泛化性。SongCi 在一个包含超过 1600 万高分辨率图像补丁、2228 个视觉-语言对和 471 个诊断结果的多中心数据集上进行预训练和评估，结果显示其在多种法医病理学任务中优于现有多模态 AI 模型，并与经验丰富的法医病理学家表现相当，同时提供详细的多模态可解释性以辅助调查。作为创新，SongCi 是第一个专门为法医病理分析设计的 VLM，以及第一个直接处理 gigapixel whole slide images (WSIs) 的 large-vocabulary computational pathology (CPath) 模型。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "28 pages, 6 figures, under review",
      "pdf_url": "http://arxiv.org/pdf/2407.14904v1",
      "published_date": "2024-07-20 15:34:52 UTC",
      "updated_date": "2024-07-20 15:34:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:27:34.044601"
    },
    {
      "arxiv_id": "2407.21039v1",
      "title": "Mapping Patient Trajectories: Understanding and Visualizing Sepsis Prognostic Pathways from Patients Clinical Narratives",
      "title_zh": "映射患者轨迹：从患者临床叙述中理解和可视化脓毒症预后路径",
      "authors": [
        "Sudeshna Jana",
        "Tirthankar Dasgupta",
        "Lipika Dey"
      ],
      "abstract": "In recent years, healthcare professionals are increasingly emphasizing on\npersonalized and evidence-based patient care through the exploration of\nprognostic pathways. To study this, structured clinical variables from\nElectronic Health Records (EHRs) data have traditionally been employed by many\nresearchers. Presently, Natural Language Processing models have received great\nattention in clinical research which expanded the possibilities of using\nclinical narratives. In this paper, we propose a systematic methodology for\ndeveloping sepsis prognostic pathways derived from clinical notes, focusing on\ndiverse patient subgroups identified by exploring comorbidities associated with\nsepsis and generating explanations of these subgroups using SHAP. The extracted\nprognostic pathways of these subgroups provide valuable insights into the\ndynamic trajectories of sepsis severity over time. Visualizing these pathways\nsheds light on the likelihood and direction of disease progression across\nvarious contexts and reveals patterns and pivotal factors or biomarkers\ninfluencing the transition between sepsis stages, whether toward deterioration\nor improvement. This empowers healthcare providers to implement more\npersonalized and effective healthcare strategies for individual patients.",
      "tldr_zh": "该研究提出了一种系统方法，利用自然语言处理(NLP)模型从临床叙述中映射脓毒症(sepsis)患者的预后路径，聚焦于通过探索与脓毒症相关的共病(comorbidities)来识别不同患者子群，并使用SHAP生成这些子群的解释。方法包括提取动态轨迹以揭示脓毒症严重程度随时间的变化、可视化路径以分析疾病进展的方向和关键生物标志物。结果显示，此方法为医疗提供者提供宝贵洞见，帮助实施更个性化的证据-based患者护理策略。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "preprint, 8 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.21039v1",
      "published_date": "2024-07-20 14:45:55 UTC",
      "updated_date": "2024-07-20 14:45:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:27:44.475932"
    },
    {
      "arxiv_id": "2407.14883v2",
      "title": "Inferring Ingrained Remote Information in AC Power Flows Using Neuromorphic Modality Regime",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoguang Diao",
        "Yubo Song",
        "Subham Sahoo"
      ],
      "abstract": "In this paper, we infer remote measurements such as remote voltages and\ncurrents online with change in AC power flows using spiking neural network\n(SNN) as grid-edge technology for efficient coordination of power electronic\nconverters. This work unifies power and information as a means of data\nnormalization using a multi-modal regime in the form of spikes using\nenergy-efficient neuromorphic learning and event-driven asynchronous data\ncollection. Firstly, we organize the synchronous real-valued measurements at\neach edge and translate them into asynchronous spike-based events to collect\nsparse data for training of SNN at each edge. Instead of relying on\nerror-dependent supervised data-driven learning theory, we exploit the\nlatency-driven unsupervised Hebbian learning rule to obtain modulation pulses\nfor switching of power electronic converters that can now comprehend grid\ndisturbances locally and adapt their operation without requiring explicit\ninfrastructure for global coordination. Not only does this philosophy block\nexogenous path arrival for cyber attackers by dismissing the cyber layer, it\nalso entails converter adaptation to system reconfiguration and parameter\nmismatch issues. We conclude this work by validating its energy-efficient and\neffective online learning performance under various scenarios in different\nsystem sizes, including modified IEEE 14-bus system and under experimental\nconditions.",
      "tldr_zh": "本论文提出了一种使用 spiking neural network (SNN) 作为电网边缘技术的框架，用于在线推断 AC 功率流变化中的远程测量（如电压和电流），以实现功率电子转换器的高效协调。该方法通过多模态脉冲形式统一功率和信息，将同步测量转化为异步脉冲事件，并采用无监督的 Hebbian 学习规则，实现本地适应电网干扰，而无需依赖全局协调，从而提升能量效率和网络安全。在各种场景下，包括修改后的 IEEE 14-bus 系统和实验条件，该框架展示了出色的在线学习性能。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.NE",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "The manuscript has been accepted for publication in the Proceedings\n  of 2024 IEEE International Conference on Communications, Control, and\n  Computing Technologies for Smart Grids (SmartGridComm 2024)",
      "pdf_url": "http://arxiv.org/pdf/2407.14883v2",
      "published_date": "2024-07-20 14:20:22 UTC",
      "updated_date": "2024-08-09 19:35:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:27:57.170842"
    },
    {
      "arxiv_id": "2407.14882v1",
      "title": "Reduced Effectiveness of Kolmogorov-Arnold Networks on Functions with Noise",
      "title_zh": "翻译失败",
      "authors": [
        "Haoran Shen",
        "Chen Zeng",
        "Jiahui Wang",
        "Qiao Wang"
      ],
      "abstract": "It has been observed that even a small amount of noise introduced into the\ndataset can significantly degrade the performance of KAN. In this brief note,\nwe aim to quantitatively evaluate the performance when noise is added to the\ndataset. We propose an oversampling technique combined with denoising to\nalleviate the impact of noise. Specifically, we employ kernel filtering based\non diffusion maps for pre-filtering the noisy data for training KAN network.\nOur experiments show that while adding i.i.d. noise with any fixed SNR, when we\nincrease the amount of training data by a factor of $r$, the test-loss (RMSE)\nof KANs will exhibit a performance trend like $\\text{test-loss} \\sim\n\\mathcal{O}(r^{-\\frac{1}{2}})$ as $r\\to +\\infty$. We conclude that applying\nboth oversampling and filtering strategies can reduce the detrimental effects\nof noise. Nevertheless, determining the optimal variance for the kernel\nfiltering process is challenging, and enhancing the volume of training data\nsubstantially increases the associated costs, because the training dataset\nneeds to be expanded multiple times in comparison to the initial clean data. As\na result, the noise present in the data ultimately diminishes the effectiveness\nof Kolmogorov-Arnold networks.",
      "tldr_zh": "该研究评估了噪声对 Kolmogorov-Arnold Networks (KAN) 性能的负面影响，发现即使少量噪声也会显著降低其准确性。作者提出了一种结合 oversampling 和 denoising 技术的策略，使用基于 diffusion maps 的 kernel filtering 来预处理噪声数据，从而缓解影响。实验结果显示，当添加 i.i.d. 噪声并将训练数据量增加因子 r 时，测试损失 (RMSE) 呈 O(r^{-1/2}) 的趋势；尽管此方法有效，但优化 kernel filtering 的方差和增加数据量会带来成本挑战，最终噪声仍会削弱 KAN 的整体效能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NA",
        "math.NA",
        "68T07"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.14882v1",
      "published_date": "2024-07-20 14:17:10 UTC",
      "updated_date": "2024-07-20 14:17:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:28:09.808056"
    },
    {
      "arxiv_id": "2407.14876v1",
      "title": "Preictal Period Optimization for Deep Learning-Based Epileptic Seizure Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Petros Koutsouvelis",
        "Bartlomiej Chybowski",
        "Alfredo Gonzalez-Sulser",
        "Shima Abdullateef",
        "Javier Escudero"
      ],
      "abstract": "Accurate prediction of epileptic seizures could prove critical for improving\npatient safety and quality of life in drug-resistant epilepsy. Although deep\nlearning-based approaches have shown promising seizure prediction performance\nusing scalp electroencephalogram (EEG) signals, substantial limitations still\nimpede their clinical adoption. Furthermore, identifying the optimal preictal\nperiod (OPP) for labeling EEG segments remains a challenge. Here, we not only\ndevelop a competitive deep learning model for seizure prediction but, more\nimportantly, leverage it to demonstrate a methodology to comprehensively\nevaluate the predictive performance in the seizure prediction task. For this,\nwe introduce a CNN-Transformer deep learning model to detect preictal\nspatiotemporal dynamics, alongside a novel Continuous Input-Output Performance\nRatio (CIOPR) metric to determine the OPP. We trained and evaluated our model\non 19 pediatric patients of the open-access CHB-MIT dataset in a\nsubject-specific manner. Using the OPP of each patient, preictal and interictal\nsegments were correctly identified with an average sensitivity of 99.31%,\nspecificity of 95.34%, AUC of 99.35%, and F1- score of 97.46%, while prediction\ntime averaged 76.8 minutes before onset. Notably, our novel CIOPR metric\nallowed outlining the impact of different preictal period definitions on\nprediction time, accuracy, output stability, and transition time between\ninterictal and preictal states in a comprehensive and quantitative way and\nhighlighted the importance of considering both inter- and intra-patient\nvariability in seizure prediction.",
      "tldr_zh": "本文提出了一种优化前驱期 (Preictal Period, OPP) 的方法，用于基于深度学习的癫痫发作预测，旨在解决现有模型在头皮 EEG 信号处理中的局限性。研究开发了 CNN-Transformer 模型来检测前驱期的时空动态，并引入了 Continuous Input-Output Performance Ratio (CIOPR) 指标，以全面评估预测性能及其对准确性和稳定性的影响。在 CHB-MIT 数据集的 19 名儿科患者上进行主体特定训练，该模型实现了 99.31% 的灵敏度、95.34% 的特异性、99.35% 的 AUC 和 97.46% 的 F1 分数，平均预测时间为 76.8 分钟。该方法强调了考虑患者间和患者内变异的重要性，为临床癫痫预测提供了更可靠的框架。",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.14876v1",
      "published_date": "2024-07-20 13:49:14 UTC",
      "updated_date": "2024-07-20 13:49:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:28:22.012903"
    },
    {
      "arxiv_id": "2407.18968v1",
      "title": "Intelligence Analysis of Language Models",
      "title_zh": "语言模型的智能分析",
      "authors": [
        "Liane Galanti",
        "Ethan Baron"
      ],
      "abstract": "In this project, we test the effectiveness of Large Language Models (LLMs) on\nthe Abstraction and Reasoning Corpus (ARC) dataset. This dataset serves as a\nrepresentative benchmark for testing abstract reasoning abilities, requiring a\nfundamental understanding of key concepts such as object identification, basic\ncounting, and elementary geometric principles. Tasks from this dataset are\nconverted into a prompt-based format for evaluation. Initially, we assess the\nmodels' potential through a Zero-shot approach. Subsequently, we investigate\nthe application of the Chain-of-Thought (CoT) technique, aiming to determine\nits role in improving model performance. Our results suggest that, despite the\nhigh expectations placed on contemporary LLMs, these models still struggle in\nnon-linguistic domains, even when dealing with simpler subsets of the ARC\ndataset. Our study is the first to concentrate on the capabilities of\nopen-source models in this context. The code, dataset, and prompts supporting\nthis project's findings can be found in our GitHub repository, accessible at:\nhttps://github.com/Lianga2000/LLMsOnARC.",
      "tldr_zh": "本研究评估了大型语言模型(LLMs)在Abstraction and Reasoning Corpus (ARC)数据集上的抽象推理能力，将任务转换为提示格式，先采用Zero-shot方法进行初步测试，随后应用Chain-of-Thought (CoT)技术探究性能提升。结果表明，尽管LLMs备受期待，但它们在非语言领域表现不佳，甚至在ARC的简单子集上也挣扎，这突显了模型在抽象任务中的局限性。该研究是首次专注于开源LLMs能力的分析，并提供了GitHub仓库（https://github.com/Lianga2000/LLMsOnARC）以供进一步验证。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18968v1",
      "published_date": "2024-07-20 13:48:16 UTC",
      "updated_date": "2024-07-20 13:48:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:28:34.699488"
    },
    {
      "arxiv_id": "2407.15880v1",
      "title": "Diff4VS: HIV-inhibiting Molecules Generation with Classifier Guidance Diffusion for Virtual Screening",
      "title_zh": "翻译失败",
      "authors": [
        "Jiaqing Lyu",
        "Changjie Chen",
        "Bing Liang",
        "Yijia Zhang"
      ],
      "abstract": "The AIDS epidemic has killed 40 million people and caused serious global\nproblems. The identification of new HIV-inhibiting molecules is of great\nimportance for combating the AIDS epidemic. Here, the Classifier Guidance\nDiffusion model and ligand-based virtual screening strategy are combined to\ndiscover potential HIV-inhibiting molecules for the first time. We call it\nDiff4VS. An extra classifier is trained using the HIV molecule dataset, and the\ngradient of the classifier is used to guide the Diffusion to generate\nHIV-inhibiting molecules. Experiments show that Diff4VS can generate more\ncandidate HIV-inhibiting molecules than other methods. Inspired by ligand-based\nvirtual screening, a new metric DrugIndex is proposed. The DrugIndex is the\nratio of the proportion of candidate drug molecules in the generated molecule\nto the proportion of candidate drug molecules in the training set. DrugIndex\nprovides a new evaluation method for evolving molecular generative models from\na pharmaceutical perspective. Besides, we report a new phenomenon observed when\nusing molecule generation models for virtual screening. Compared to real\nmolecules, the generated molecules have a lower proportion that is highly\nsimilar to known drug molecules. We call it Degradation in molecule generation.\nBased on the data analysis, the Degradation may result from the difficulty of\ngenerating molecules with a specific structure in the generative model. Our\nresearch contributes to the application of generative models in drug design\nfrom method, metric, and phenomenon analysis.",
      "tldr_zh": "本研究提出Diff4VS，一种结合Classifier Guidance Diffusion模型和配体基虚拟筛选策略的方法，用于首次生成潜在的HIV抑制分子。具体而言，通过训练一个额外分类器并使用其梯度指导Diffusion过程，Diff4VS在实验中生成更多候选HIV抑制分子，比其他方法表现出色。同时，研究引入新指标DrugIndex（生成分子中候选药物分子比例与训练集比例之比），从制药视角评估分子生成模型，并观察到“Degradation”现象，即生成的分子与已知药物高度相似的比例较低，可能源于模型生成特定结构难度大。该工作从方法、指标和现象分析角度推进了生成模型在药物设计中的应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.15880v1",
      "published_date": "2024-07-20 12:34:02 UTC",
      "updated_date": "2024-07-20 12:34:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:28:45.563146"
    },
    {
      "arxiv_id": "2407.14838v1",
      "title": "Retrieval Augmented Generation Integrated Large Language Models in Smart Contract Vulnerability Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Jeffy Yu"
      ],
      "abstract": "The rapid growth of Decentralized Finance (DeFi) has been accompanied by\nsubstantial financial losses due to smart contract vulnerabilities,\nunderscoring the critical need for effective security auditing. With attacks\nbecoming more frequent, the necessity and demand for auditing services has\nescalated. This especially creates a financial burden for independent\ndevelopers and small businesses, who often have limited available funding for\nthese services. Our study builds upon existing frameworks by integrating\nRetrieval-Augmented Generation (RAG) with large language models (LLMs),\nspecifically employing GPT-4-1106 for its 128k token context window. We\nconstruct a vector store of 830 known vulnerable contracts, leveraging Pinecone\nfor vector storage, OpenAI's text-embedding-ada-002 for embeddings, and\nLangChain to construct the RAG-LLM pipeline. Prompts were designed to provide a\nbinary answer for vulnerability detection. We first test 52 smart contracts 40\ntimes each against a provided vulnerability type, verifying the replicability\nand consistency of the RAG-LLM. Encouraging results were observed, with a 62.7%\nsuccess rate in guided detection of vulnerabilities. Second, we challenge the\nmodel under a \"blind\" audit setup, without the vulnerability type provided in\nthe prompt, wherein 219 contracts undergo 40 tests each. This setup evaluates\nthe general vulnerability detection capabilities without hinted context\nassistance. Under these conditions, a 60.71% success rate was observed. While\nthe results are promising, we still emphasize the need for human auditing at\nthis time. We provide this study as a proof of concept for a cost-effective\nsmart contract auditing process, moving towards democratic access to security.",
      "tldr_zh": "本研究整合 Retrieval-Augmented Generation (RAG) 与 Large Language Models (LLMs)，如 GPT-4-1106，用于检测 Decentralized Finance (DeFi) 中的智能合约漏洞，旨在为独立开发者和小企业提供成本有效的安全审计解决方案。研究构建了一个向量存储，包含 830 个已知漏洞合约，并利用 Pinecone 进行存储、OpenAI 的 text-embedding-ada-002 生成嵌入，以及 LangChain 构建 RAG-LLM 管道，以设计二元答案提示进行检测。在实验中，针对特定漏洞类型的指导检测成功率达 62.7%，而无提示的盲审计成功率达 60.71%。尽管结果有前景，该方法作为概念证明强调仍需结合人工审计，以推动安全审计的民主化访问。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "17 pages, 3 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.14838v1",
      "published_date": "2024-07-20 10:46:42 UTC",
      "updated_date": "2024-07-20 10:46:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:28:59.963772"
    },
    {
      "arxiv_id": "2407.15879v2",
      "title": "Decentralized Federated Anomaly Detection in Smart Grids: A P2P Gossip Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Muhammad Akbar Husnoo",
        "Adnan Anwar",
        "Md Enamul Haque",
        "A. N. Mahmood"
      ],
      "abstract": "The increasing security and privacy concerns in the Smart Grid sector have\nled to a significant demand for robust intrusion detection systems within\ncritical smart grid infrastructure. To address the challenges posed by privacy\npreservation and decentralized power system zones with distinct data ownership,\nFederated Learning (FL) has emerged as a promising privacy-preserving solution\nwhich facilitates collaborative training of attack detection models without\nnecessitating the sharing of raw data. However, FL presents several\nimplementation limitations in the power system domain due to its heavy reliance\non a centralized aggregator and the risks of privacy leakage during model\nupdate transmission. To overcome these technical bottlenecks, this paper\nintroduces a novel decentralized federated anomaly detection scheme based on\ntwo main gossip protocols namely Random Walk and Epidemic. Our findings\nindicate that the Random Walk protocol exhibits superior performance compared\nto the Epidemic protocol, highlighting its efficacy in decentralized federated\nlearning environments. Experimental validation of the proposed framework\nutilizing publicly available industrial control systems datasets demonstrates\nsuperior attack detection accuracy while safeguarding data confidentiality and\nmitigating the impact of communication latency and stragglers. Furthermore, our\napproach yields a notable 35% improvement in training time compared to\nconventional FL, underscoring the efficacy and robustness of our decentralized\nlearning method.",
      "tldr_zh": "本论文针对智能电网的安全隐私挑战，提出了一种基于 P2P Gossip 协议的去中心化联邦异常检测方案，利用 Random Walk 和 Epidemic 协议来避免 Federated Learning (FL) 的中心化聚合和隐私泄露风险。实验结果显示，Random Walk 协议在性能上优于 Epidemic 协议，并在公开的工业控制系统数据集上实现了更高的攻击检测准确率，同时保护数据保密并减少通信延迟和 stragglers 影响。该方法相比传统 FL 训练时间提高了 35%，为高效的分布式入侵检测提供了鲁棒解决方案。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.DC",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.15879v2",
      "published_date": "2024-07-20 10:45:06 UTC",
      "updated_date": "2025-01-09 13:27:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:29:10.367306"
    },
    {
      "arxiv_id": "2407.14831v1",
      "title": "Toward Efficient Convolutional Neural Networks With Structured Ternary Patterns",
      "title_zh": "翻译失败",
      "authors": [
        "Christos Kyrkou"
      ],
      "abstract": "High-efficiency deep learning (DL) models are necessary not only to\nfacilitate their use in devices with limited resources but also to improve\nresources required for training. Convolutional neural networks (ConvNets)\ntypically exert severe demands on local device resources and this\nconventionally limits their adoption within mobile and embedded platforms. This\nbrief presents work toward utilizing static convolutional filters generated\nfrom the space of local binary patterns (LBPs) and Haar features to design\nefficient ConvNet architectures. These are referred to as Structured Ternary\nPatterns (STePs) and can be generated during network initialization in a\nsystematic way instead of having learnable weight parameters thus reducing the\ntotal weight updates. The ternary values require significantly less storage and\nwith the appropriate low-level implementation, can also lead to inference\nimprovements. The proposed approach is validated using four image\nclassification datasets, demonstrating that common network backbones can be\nmade more efficient and provide competitive results. It is also demonstrated\nthat it is possible to generate completely custom STeP-based networks that\nprovide good trade-offs for on-device applications such as unmanned aerial\nvehicle (UAV)-based aerial vehicle detection. The experimental results show\nthat the proposed method maintains high detection accuracy while reducing the\ntrainable parameters by 40-80%. This work motivates further research toward\ngood priors for non-learnable weights that can make DL architectures more\nefficient without having to alter the network during or after training.",
      "tldr_zh": "本研究旨在通过Structured Ternary Patterns (STePs)来设计高效的Convolutional Neural Networks (ConvNets)，以减少资源需求并提升在移动和嵌入式平台的适用性。STePs基于Local Binary Patterns (LBPs)和Haar features生成的静态卷积过滤器，在网络初始化时系统生成，从而避免可学习权重参数并降低权重更新。实验在四个图像分类数据集上验证，显示STePs能使常见网络骨干更高效，同时保持竞争性准确率，并在无人机(UAV)空中车辆检测应用中减少40-80%的可训练参数。该方法为开发非学习权重的好先验提供了新思路，促进深度学习架构的效率提升。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Published in: IEEE Transactions on Neural Networks and Learning\n  Systems Code: https://github.com/ckyrkou/STeP_Models ImageNet-16 Dataset:\n  https://zenodo.org/records/8027520",
      "pdf_url": "http://arxiv.org/pdf/2407.14831v1",
      "published_date": "2024-07-20 10:18:42 UTC",
      "updated_date": "2024-07-20 10:18:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:29:24.207970"
    },
    {
      "arxiv_id": "2407.14823v2",
      "title": "Scaling Up Single Image Dehazing Algorithm by Cross-Data Vision Alignment for Richer Representation Learning and Beyond",
      "title_zh": "翻译失败",
      "authors": [
        "Yukai Shi",
        "Zhipeng Weng",
        "Yupei Lin",
        "Cidan Shi",
        "Xiaojun Yang",
        "Liang Lin"
      ],
      "abstract": "In recent years, deep neural networks tasks have increasingly relied on\nhigh-quality image inputs. With the development of high-resolution\nrepresentation learning, the task of image dehazing has received significant\nattention. Previously, many methods collect diverse image data for large-scale\ntraining to boost the performance on a target scene. Ignoring the domain gap\nbetween different data, former de-hazing methods simply adopt multiple datasets\nfor explicit large-scale training, which often makes the methods themselves be\nviolated. To address this problem, we propose a novel method of cross-data\nvision alignment for richer representation learning to improve the existing\ndehazing methodology. Specifically, we call for the internal- and external\nknowledge should be further adapted with a self-supervised manner to fill up\nthe domain gap. By using cross-data external alignment, the datasets inherit\nsamples from different domains that are firmly aligned, making the model learn\nmore robust and generalizable features. By using the internal augmentation\nmethod, the model can fully exploit local information within the images, and\nthen obtaining more image details. To demonstrate the effectiveness of our\nproposed method, we conduct training on the Natural Image Dataset (NID).\nExperimental results show that our method clearly resolves the domain gap in\ndifferent dehazing datasets and presents a new pipeline for large-scale\ntraining in the dehazing task. Our approach significantly outperforms other\nadvanced methods in dehazing and produces dehazed images that are closest to\nreal haze-free images.",
      "tldr_zh": "这篇论文针对图像去雾任务，提出了一种名为 cross-data vision alignment 的新方法，以解决传统方法忽略不同数据集领域差距的问题，从而实现更丰富的表示学习。方法包括 cross-data external alignment（用于跨域数据对齐，使模型学习更鲁棒的特征）和 internal augmentation method（通过内部增强充分利用图像局部信息）。实验结果表明，该方法在 Natural Image Dataset (NID) 上显著优于其他先进算法，生成的去雾图像更接近真实无雾图像，并为大规模训练提供了一个新管道。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "A cross-dataset vision alignment and augmentation technology is\n  proposed to boost generalizable feature learning in the de-hazing task",
      "pdf_url": "http://arxiv.org/pdf/2407.14823v2",
      "published_date": "2024-07-20 10:00:20 UTC",
      "updated_date": "2025-03-20 18:22:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:29:33.948944"
    },
    {
      "arxiv_id": "2407.14811v1",
      "title": "Decoupled Prompt-Adapter Tuning for Continual Activity Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Di Fu",
        "Thanh Vinh Vo",
        "Haozhe Ma",
        "Tze-Yun Leong"
      ],
      "abstract": "Action recognition technology plays a vital role in enhancing security\nthrough surveillance systems, enabling better patient monitoring in healthcare,\nproviding in-depth performance analysis in sports, and facilitating seamless\nhuman-AI collaboration in domains such as manufacturing and assistive\ntechnologies. The dynamic nature of data in these areas underscores the need\nfor models that can continuously adapt to new video data without losing\npreviously acquired knowledge, highlighting the critical role of advanced\ncontinual action recognition. To address these challenges, we propose Decoupled\nPrompt-Adapter Tuning (DPAT), a novel framework that integrates adapters for\ncapturing spatial-temporal information and learnable prompts for mitigating\ncatastrophic forgetting through a decoupled training strategy. DPAT uniquely\nbalances the generalization benefits of prompt tuning with the plasticity\nprovided by adapters in pretrained vision models, effectively addressing the\nchallenge of maintaining model performance amidst continuous data evolution\nwithout necessitating extensive finetuning. DPAT consistently achieves\nstate-of-the-art performance across several challenging action recognition\nbenchmarks, thus demonstrating the effectiveness of our model in the domain of\ncontinual action recognition.",
      "tldr_zh": "本研究针对行动识别（activity recognition）领域的持续学习挑战，提出了一种名为 Decoupled Prompt-Adapter Tuning (DPAT) 的新框架，以帮助模型适应新视频数据同时避免 catastrophic forgetting。DPAT 通过整合 adapters 来捕捉空间-时间信息，并使用 learnable prompts 结合 decoupled training strategy，实现 prompt tuning 的泛化优势与 adapters 的可塑性平衡，而无需进行广泛的微调。实验结果显示，DPAT 在多个挑战性行动识别基准上达到了 state-of-the-art 性能，证明了其在持续行动识别中的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.14811v1",
      "published_date": "2024-07-20 08:56:04 UTC",
      "updated_date": "2024-07-20 08:56:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:29:49.718974"
    },
    {
      "arxiv_id": "2407.14796v1",
      "title": "PASSION: Towards Effective Incomplete Multi-Modal Medical Image Segmentation with Imbalanced Missing Rates",
      "title_zh": "翻译失败",
      "authors": [
        "Junjie Shi",
        "Caozhi Shang",
        "Zhaobin Sun",
        "Li Yu",
        "Xin Yang",
        "Zengqiang Yan"
      ],
      "abstract": "Incomplete multi-modal image segmentation is a fundamental task in medical\nimaging to refine deployment efficiency when only partial modalities are\navailable. However, the common practice that complete-modality data is visible\nduring model training is far from realistic, as modalities can have imbalanced\nmissing rates in clinical scenarios. In this paper, we, for the first time,\nformulate such a challenging setting and propose Preference-Aware\nSelf-diStillatION (PASSION) for incomplete multi-modal medical image\nsegmentation under imbalanced missing rates. Specifically, we first construct\npixel-wise and semantic-wise self-distillation to balance the optimization\nobjective of each modality. Then, we define relative preference to evaluate the\ndominance of each modality during training, based on which to design task-wise\nand gradient-wise regularization to balance the convergence rates of different\nmodalities. Experimental results on two publicly available multi-modal datasets\ndemonstrate the superiority of PASSION against existing approaches for modality\nbalancing. More importantly, PASSION is validated to work as a plug-and-play\nmodule for consistent performance improvement across different backbones. Code\nis available at https://github.com/Jun-Jie-Shi/PASSION.",
      "tldr_zh": "这篇论文首次针对多模态医疗图像分割（multi-modal image segmentation）中模态缺失率不平衡（imbalanced missing rates）的问题，提出了一种名为 PASSION 的方法，以提升实际临床场景下的部署效率。PASSION 通过像素-wise 和 semantic-wise 自蒸馏（self-distillation）来平衡各模态的优化目标，并利用相对偏好评估设计任务-wise 和 gradient-wise 正则化，以协调不同模态的收敛速率。实验在两个公开多模态数据集上证明，PASSION 优于现有方法，并可作为即插即用（plug-and-play）模块，提升各种骨干网络的性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ACM MM 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.14796v1",
      "published_date": "2024-07-20 07:53:20 UTC",
      "updated_date": "2024-07-20 07:53:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:29:59.186131"
    },
    {
      "arxiv_id": "2407.14790v2",
      "title": "Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?",
      "title_zh": "翻译失败",
      "authors": [
        "Nemika Tyagi",
        "Mihir Parmar",
        "Mohith Kulkarni",
        "Aswin RRV",
        "Nisarg Patel",
        "Mutsumi Nakamura",
        "Arindam Mitra",
        "Chitta Baral"
      ],
      "abstract": "Solving grid puzzles involves a significant amount of logical reasoning.\nHence, it is a good domain to evaluate the reasoning capability of a model\nwhich can then guide us to improve the reasoning ability of models. However,\nmost existing works evaluate only the final predicted answer of a puzzle,\nwithout delving into an in-depth analysis of the LLMs' reasoning chains (such\nas where they falter) or providing any finer metrics to evaluate them. Since\nLLMs may rely on simple heuristics or artifacts to predict the final answer, it\nis crucial to evaluate the generated reasoning chain beyond overall correctness\nmeasures, for accurately evaluating the reasoning abilities of LLMs. To this\nend, we first develop GridPuzzle, an evaluation dataset comprising 274\ngrid-based puzzles with different complexities. Second, we propose a new error\ntaxonomy derived from manual analysis of reasoning chains from LLMs including\nGPT-4, Claude-3, Gemini, Mistral, and Llama-2. Then, we develop an LLM-based\nframework for large-scale subjective evaluation (i.e., identifying errors) and\nan objective metric, PuzzleEval, to evaluate the correctness of reasoning\nchains. Evaluating reasoning chains from LLMs leads to several interesting\nfindings. We further show that existing prompting methods used for enhancing\nmodels' reasoning abilities do not improve performance on GridPuzzle. This\nhighlights the importance of understanding fine-grained errors and presents a\nchallenge for future research to enhance LLMs' puzzle-solving abilities by\ndeveloping methods that address these errors. Data and source code are\navailable at https://github.com/Mihir3009/GridPuzzle.",
      "tldr_zh": "这篇论文评估了大型语言模型(LLMs)在解决网格谜题时的逐步推理能力，强调了现有评估仅关注最终答案而忽略推理链的细粒度分析。研究者开发了GridPuzzle数据集（包含274个不同复杂度的网格谜题）、一个基于LLMs的错误分类体系，以及一个客观指标PuzzleEval，用于识别和评估LLMs（如GPT-4、Claude-3等）的推理错误。实验发现，LLMs常依赖简单启发式而非真实推理，现有的提示方法无法提升性能，这突显了理解细粒度错误的重要性，并为未来提升LLMs推理能力的研究提供挑战。数据和源代码可在GitHub获取。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at EMNLP 2024 Main",
      "pdf_url": "http://arxiv.org/pdf/2407.14790v2",
      "published_date": "2024-07-20 07:43:07 UTC",
      "updated_date": "2024-10-04 04:58:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:30:11.458855"
    },
    {
      "arxiv_id": "2407.14789v1",
      "title": "PERCORE: A Deep Learning-Based Framework for Persian Spelling Correction with Phonetic Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Seyed Mohammad Sadegh Dashti",
        "Amid Khatibi Bardsiri",
        "Mehdi Jafari Shahbazzadeh"
      ],
      "abstract": "This research introduces a state-of-the-art Persian spelling correction\nsystem that seamlessly integrates deep learning techniques with phonetic\nanalysis, significantly enhancing the accuracy and efficiency of natural\nlanguage processing (NLP) for Persian. Utilizing a fine-tuned language\nrepresentation model, our methodology effectively combines deep contextual\nanalysis with phonetic insights, adeptly correcting both non-word and real-word\nspelling errors. This strategy proves particularly effective in tackling the\nunique complexities of Persian spelling, including its elaborate morphology and\nthe challenge of homophony. A thorough evaluation on a wide-ranging dataset\nconfirms our system's superior performance compared to existing methods, with\nimpressive F1-Scores of 0.890 for detecting real-word errors and 0.905 for\ncorrecting them. Additionally, the system demonstrates a strong capability in\nnon-word error correction, achieving an F1-Score of 0.891. These results\nillustrate the significant benefits of incorporating phonetic insights into\ndeep learning models for spelling correction. Our contributions not only\nadvance Persian language processing by providing a versatile solution for a\nvariety of NLP applications but also pave the way for future research in the\nfield, emphasizing the critical role of phonetic analysis in developing\neffective spelling correction system.",
      "tldr_zh": "本研究提出PERCORE框架，这是一种基于deep learning的波斯语拼写纠错系统，通过整合phonetic analysis显著提升了natural language processing (NLP) 的准确性和效率。该框架利用fine-tuned language representation model结合深度上下文分析和语音洞见，有效处理波斯语的复杂形态和同音问题，包括非单词错误和真实单词错误。在广泛数据集上的评估显示，PERCORE在检测真实单词错误时F1-Score达0.890，纠正时达0.905，非单词错误纠正时达0.891，优于现有方法。这些成果不仅推进了波斯语处理的应用，如各种NLP任务，还强调了phonetic analysis在未来拼写纠错系统中的关键作用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.14789v1",
      "published_date": "2024-07-20 07:41:04 UTC",
      "updated_date": "2024-07-20 07:41:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:30:24.403670"
    },
    {
      "arxiv_id": "2407.14788v2",
      "title": "On the Design and Analysis of LLM-Based Algorithms",
      "title_zh": "翻译失败",
      "authors": [
        "Yanxi Chen",
        "Yaliang Li",
        "Bolin Ding",
        "Jingren Zhou"
      ],
      "abstract": "We initiate a formal investigation into the design and analysis of LLM-based\nalgorithms, i.e. algorithms that contain one or multiple calls of large\nlanguage models (LLMs) as sub-routines and critically rely on the capabilities\nof LLMs. While LLM-based algorithms, ranging from basic LLM calls with prompt\nengineering to complicated LLM-powered agent systems and compound AI systems,\nhave achieved remarkable empirical success, the design and optimization of them\nhave mostly relied on heuristics and trial-and-errors, which is largely due to\na lack of formal and analytical study for these algorithms. To fill this gap,\nwe start by identifying the computational-graph representation of LLM-based\nalgorithms, the design principle of task decomposition, and some key\nabstractions, which then facilitate our formal analysis for the accuracy and\nefficiency of LLM-based algorithms, despite the black-box nature of LLMs.\nThrough extensive analytical and empirical investigation in a series of case\nstudies, we demonstrate that the proposed framework is broadly applicable to a\nwide range of scenarios and diverse patterns of LLM-based algorithms, such as\nparallel, hierarchical and recursive task decomposition. Our proposed framework\nholds promise for advancing LLM-based algorithms, by revealing the reasons\nbehind curious empirical phenomena, guiding the choices of hyperparameters,\npredicting the empirical performance of algorithms, and inspiring new algorithm\ndesign. To promote further study of LLM-based algorithms, we release our source\ncode at\nhttps://github.com/modelscope/agentscope/tree/main/examples/paper_llm_based_algorithm.",
      "tldr_zh": "该论文首次对LLM-based algorithms（基于大型语言模型的算法）进行正式设计和分析，这些算法依赖LLMs作为子例程，但目前主要依赖启发式方法。研究者提出了计算图表示、任务分解设计原则和关键抽象，并开发了分析框架来评估算法的准确性和效率，尽管LLMs是黑箱系统。通过广泛的案例研究，包括并行、层次化和递归任务分解，该框架证明了其广泛适用性，并能解释经验现象、指导超参数选择、预测性能并激发新算法设计。源代码已发布在GitHub上，以促进进一步研究。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.14788v2",
      "published_date": "2024-07-20 07:39:07 UTC",
      "updated_date": "2024-09-26 10:21:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:30:38.712110"
    },
    {
      "arxiv_id": "2407.14779v3",
      "title": "Do Generative AI Models Output Harm while Representing Non-Western Cultures: Evidence from A Community-Centered Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Sourojit Ghosh",
        "Pranav Narayanan Venkit",
        "Sanjana Gautam",
        "Shomir Wilson",
        "Aylin Caliskan"
      ],
      "abstract": "Our research investigates the impact of Generative Artificial Intelligence\n(GAI) models, specifically text-to-image generators (T2Is), on the\nrepresentation of non-Western cultures, with a focus on Indian contexts.\nDespite the transformative potential of T2Is in content creation, concerns have\narisen regarding biases that may lead to misrepresentations and\nmarginalizations. Through a community-centered approach and grounded theory\nanalysis of 5 focus groups from diverse Indian subcultures, we explore how T2I\noutputs to English prompts depict Indian culture and its subcultures,\nuncovering novel representational harms such as exoticism and cultural\nmisappropriation. These findings highlight the urgent need for inclusive and\nculturally sensitive T2I systems. We propose design guidelines informed by a\nsociotechnical perspective, aiming to address these issues and contribute to\nthe development of more equitable and representative GAI technologies globally.\nOur work also underscores the necessity of adopting a community-centered\napproach to comprehend the sociotechnical dynamics of these models,\ncomplementing existing work in this space while identifying and addressing the\npotential negative repercussions and harms that may arise when these models are\ndeployed on a global scale.",
      "tldr_zh": "本研究调查了生成式人工智能(GAI)模型，特别是文本到图像生成器(T2Is)，在代表非西方文化（如印度文化）时可能产生的危害，通过社区中心方法和扎根理论分析5个印度子文化焦点小组进行探索。研究发现，T2Is对英语提示的输出常导致印度文化和子文化的误表征，包括异域化(exoticism)和文化挪用(cultural misappropriation)等新类型危害，从而加剧边缘化。论文提出基于社会技术视角的设计指南，旨在推动更具包容性和文化敏感性的GAI系统发展，并强调采用社区中心方法来理解这些模型的全球社会技术动态，以缓解潜在负面影响。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "This is the pre-peer reviewed version, which has been accepted at the\n  7th AAAI ACM Conference on AI, Ethics, and Society, Oct. 21, 2024,\n  California, USA",
      "pdf_url": "http://arxiv.org/pdf/2407.14779v3",
      "published_date": "2024-07-20 07:01:37 UTC",
      "updated_date": "2024-08-03 21:49:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:30:49.041642"
    },
    {
      "arxiv_id": "2407.14774v1",
      "title": "Intelligent Artistic Typography: A Comprehensive Review of Artistic Text Design and Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Yuhang Bai",
        "Zichuan Huang",
        "Wenshuo Gao",
        "Shuai Yang",
        "Jiaying Liu"
      ],
      "abstract": "Artistic text generation aims to amplify the aesthetic qualities of text\nwhile maintaining readability. It can make the text more attractive and better\nconvey its expression, thus enjoying a wide range of application scenarios such\nas social media display, consumer electronics, fashion, and graphic design.\nArtistic text generation includes artistic text stylization and semantic\ntypography. Artistic text stylization concentrates on the text effect overlaid\nupon the text, such as shadows, outlines, colors, glows, and textures. By\ncomparison, semantic typography focuses on the deformation of the characters to\nstrengthen their visual representation by mimicking the semantic understanding\nwithin the text. This overview paper provides an introduction to both artistic\ntext stylization and semantic typography, including the taxonomy, the key ideas\nof representative methods, and the applications in static and dynamic artistic\ntext generation. Furthermore, the dataset and evaluation metrics are\nintroduced, and the future directions of artistic text generation are\ndiscussed. A comprehensive list of artistic text generation models studied in\nthis review is available at\nhttps://github.com/williamyang1991/Awesome-Artistic-Typography/.",
      "tldr_zh": "这篇综述论文（Review）全面探讨了Artistic Text Generation，旨在提升文本的美学品质同时保持可读性，包括Artistic Text Stylization（如阴影、轮廓和纹理效果）和Semantic Typography（如基于语义的字符变形）。论文介绍了这些领域的分类、代表性方法的关键理念，以及在静态和动态文本生成中的应用场景，如社交媒体和图形设计。论文还总结了相关数据集、评估指标，并讨论了未来研究方向，同时提供了一个模型列表的GitHub资源（https://github.com/williamyang1991/Awesome-Artistic-Typography）。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "GitHub Page:\n  https://github.com/williamyang1991/Awesome-Artistic-Typography/",
      "pdf_url": "http://arxiv.org/pdf/2407.14774v1",
      "published_date": "2024-07-20 06:45:09 UTC",
      "updated_date": "2024-07-20 06:45:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:31:01.577866"
    },
    {
      "arxiv_id": "2407.14768v1",
      "title": "Teach Harder, Learn Poorer: Rethinking Hard Sample Distillation for GNN-to-MLP Knowledge Distillation",
      "title_zh": "翻译失败",
      "authors": [
        "Lirong Wu",
        "Yunfan Liu",
        "Haitao Lin",
        "Yufei Huang",
        "Stan Z. Li"
      ],
      "abstract": "To bridge the gaps between powerful Graph Neural Networks (GNNs) and\nlightweight Multi-Layer Perceptron (MLPs), GNN-to-MLP Knowledge Distillation\n(KD) proposes to distill knowledge from a well-trained teacher GNN into a\nstudent MLP. In this paper, we revisit the knowledge samples (nodes) in teacher\nGNNs from the perspective of hardness, and identify that hard sample\ndistillation may be a major performance bottleneck of existing graph KD\nalgorithms. The GNN-to-MLP KD involves two different types of hardness, one\nstudent-free knowledge hardness describing the inherent complexity of GNN\nknowledge, and the other student-dependent distillation hardness describing the\ndifficulty of teacher-to-student distillation. However, most of the existing\nwork focuses on only one of these aspects or regards them as one thing. This\npaper proposes a simple yet effective Hardness-aware GNN-to-MLP Distillation\n(HGMD) framework, which decouples the two hardnesses and estimates them using a\nnon-parametric approach. Finally, two hardness-aware distillation schemes\n(i.e., HGMD-weight and HGMD-mixup) are further proposed to distill\nhardness-aware knowledge from teacher GNNs into the corresponding nodes of\nstudent MLPs. As non-parametric distillation, HGMD does not involve any\nadditional learnable parameters beyond the student MLPs, but it still\noutperforms most of the state-of-the-art competitors. HGMD-mixup improves over\nthe vanilla MLPs by 12.95% and outperforms its teacher GNNs by 2.48% averaged\nover seven real-world datasets.",
      "tldr_zh": "该论文重新审视了 GNN-to-MLP Knowledge Distillation (KD) 中的硬样本问题，指出现有算法的性能瓶颈主要源于两种硬度：student-free knowledge hardness（GNN 知识的固有复杂性）和 student-dependent distillation hardness（教师到学生的蒸馏难度）。作者提出 Hardness-aware GNN-to-MLP Distillation (HGMD) 框架，通过非参数方法解耦并估计这两种硬度，并设计了 HGMD-weight 和 HGMD-mixup 方案来有针对性地从教师 GNN 蒸馏知识到学生 MLP。实验结果显示，HGMD-mixup 方案在七个真实数据集上将 vanilla MLPs 的性能提升了 12.95%，甚至超过了教师 GNN 2.48%。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.14768v1",
      "published_date": "2024-07-20 06:13:00 UTC",
      "updated_date": "2024-07-20 06:13:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:31:14.376922"
    },
    {
      "arxiv_id": "2407.14767v2",
      "title": "I Need Help! Evaluating LLM's Ability to Ask for Users' Support: A Case Study on Text-to-SQL Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Cheng-Kuang Wu",
        "Zhi Rui Tam",
        "Chao-Chung Wu",
        "Chieh-Yen Lin",
        "Hung-yi Lee",
        "Yun-Nung Chen"
      ],
      "abstract": "This study explores the proactive ability of LLMs to seek user support. We\npropose metrics to evaluate the trade-off between performance improvements and\nuser burden, and investigate whether LLMs can determine when to request help\nunder varying information availability. Our experiments show that without\nexternal feedback, many LLMs struggle to recognize their need for user support.\nThe findings highlight the importance of external signals and provide insights\nfor future research on improving support-seeking strategies. Source code:\nhttps://github.com/appier-research/i-need-help",
      "tldr_zh": "本研究评估大型语言模型 (LLMs) 在 Text-to-SQL 生成任务中主动寻求用户支持的能力，探讨了性能提升与用户负担之间的权衡。研究者提出了新指标来衡量 LLMs 在不同信息可用性下是否能判断何时请求帮助。实验结果显示，许多 LLMs 在缺乏外部反馈的情况下难以识别自身需求，从而无法有效寻求支持。该发现强调了外部信号的重要性，并为未来改进 LLMs 的支持寻求策略提供了宝贵见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by EMNLP 2024 Main Conference",
      "pdf_url": "http://arxiv.org/pdf/2407.14767v2",
      "published_date": "2024-07-20 06:12:29 UTC",
      "updated_date": "2024-09-30 01:45:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:31:24.277431"
    },
    {
      "arxiv_id": "2408.00798v1",
      "title": "Golden-Retriever: High-Fidelity Agentic Retrieval Augmented Generation for Industrial Knowledge Base",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiyu An",
        "Xianzhong Ding",
        "Yen-Chun Fu",
        "Cheng-Chung Chu",
        "Yan Li",
        "Wan Du"
      ],
      "abstract": "This paper introduces Golden-Retriever, designed to efficiently navigate vast\nindustrial knowledge bases, overcoming challenges in traditional LLM\nfine-tuning and RAG frameworks with domain-specific jargon and context\ninterpretation. Golden-Retriever incorporates a reflection-based question\naugmentation step before document retrieval, which involves identifying jargon,\nclarifying its meaning based on context, and augmenting the question\naccordingly. Specifically, our method extracts and lists all jargon and\nabbreviations in the input question, determines the context against a\npre-defined list, and queries a jargon dictionary for extended definitions and\ndescriptions. This comprehensive augmentation ensures the RAG framework\nretrieves the most relevant documents by providing clear context and resolving\nambiguities, significantly improving retrieval accuracy. Evaluations using\nthree open-source LLMs on a domain-specific question-answer dataset demonstrate\nGolden-Retriever's superior performance, providing a robust solution for\nefficiently integrating and querying industrial knowledge bases.",
      "tldr_zh": "本论文引入 Golden-Retriever，一种高保真代理式检索增强生成框架，旨在高效导航工业知识库并解决传统 LLM 微调和 RAG 框架在处理领域特定术语及上下文解释上的挑战。\n该框架在文档检索前采用反思-based question augmentation 步骤，包括识别输入问题中的术语和缩写、基于预定义上下文澄清其含义，并从术语字典查询扩展定义，从而增强问题并提高检索相关性。\n实验结果显示，在三个开源 LLM 上使用领域特定问答数据集评估时，Golden-Retriever 显著提升了检索准确性，提供了一个可靠的工业知识集成和查询解决方案。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.DL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00798v1",
      "published_date": "2024-07-20 06:10:46 UTC",
      "updated_date": "2024-07-20 06:10:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:31:37.078874"
    },
    {
      "arxiv_id": "2407.14766v2",
      "title": "Implementing Fairness in AI Classification: The Role of Explainability",
      "title_zh": "在AI分类中实现公平性：解释性的",
      "authors": [
        "Thomas Souverain",
        "Johnathan Nguyen",
        "Nicolas Meric",
        "Paul Égré"
      ],
      "abstract": "In this paper, we propose a philosophical and experimental investigation of\nthe problem of AI fairness in classification. We argue that implementing\nfairness in AI classification involves more work than just operationalizing a\nfairness metric. It requires establishing the explainability of the\nclassification model chosen and of the principles behind it. Specifically, it\ninvolves making the training processes transparent, determining what outcomes\nthe fairness criteria actually produce, and assessing their trade-offs by\ncomparison with closely related models that would lead to a different outcome.\nTo exemplify this methodology, we trained a model and developed a tool for\ndisparity detection and fairness interventions, the package FairDream. While\nFairDream is set to enforce Demographic Parity, experiments reveal that it\nfulfills the constraint of Equalized Odds. The algorithm is thus more\nconservative than the user might expect. To justify this outcome, we first\nclarify the relation between Demographic Parity and Equalized Odds as fairness\ncriteria. We then explain FairDream's reweighting method and justify the\ntrade-offs reached by FairDream by a benchmark comparison with closely related\nGridSearch models. We draw conclusions regarding the way in which these\nexplanatory steps can make an AI model trustworthy.",
      "tldr_zh": "本论文探讨了在AI分类中实现公平性的哲学和实验方法，强调不仅仅是操作化公平度量（如Demographic Parity），还需确保模型和原则的可解释性，包括训练过程的透明度、公平标准产出结果的评估，以及与其他模型的权衡比较。作者开发了FairDream工具，该工具通过再加权方法进行差异检测和公平干预，旨在强制Demographic Parity，但实验显示其实际实现了Equalized Odds，表现出比预期更保守的效果。最终，通过解释Demographic Parity与Equalized Odds的关系，并与GridSearch模型进行基准比较，论文证明这些解释步骤能提升AI模型的可信度。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "I.2.6; K.4.1; K.4.2"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.14766v2",
      "published_date": "2024-07-20 06:06:24 UTC",
      "updated_date": "2025-03-24 05:27:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:31:48.974522"
    },
    {
      "arxiv_id": "2407.14765v1",
      "title": "Data Augmentation in Graph Neural Networks: The Role of Generated Synthetic Graphs",
      "title_zh": "图神经网络中的数据增强：生成合成图的作用",
      "authors": [
        "Sumeyye Bas",
        "Kiymet Kaya",
        "Resul Tugay",
        "Sule Gunduz Oguducu"
      ],
      "abstract": "Graphs are crucial for representing interrelated data and aiding predictive\nmodeling by capturing complex relationships. Achieving high-quality graph\nrepresentation is important for identifying linked patterns, leading to\nimprovements in Graph Neural Networks (GNNs) to better capture data structures.\nHowever, challenges such as data scarcity, high collection costs, and ethical\nconcerns limit progress. As a result, generative models and data augmentation\nhave become more and more popular. This study explores using generated graphs\nfor data augmentation, comparing the performance of combining generated graphs\nwith real graphs, and examining the effect of different quantities of generated\ngraphs on graph classification tasks. The experiments show that balancing\nscalability and quality requires different generators based on graph size. Our\nresults introduce a new approach to graph data augmentation, ensuring\nconsistent labels and enhancing classification performance.",
      "tldr_zh": "该研究探讨了在Graph Neural Networks (GNNs)中通过生成合成图进行数据增强的作用，以解决数据稀缺、高收集成本和伦理问题等挑战。研究者比较了生成图与真实图的结合效果，并分析了不同数量的生成图对图分类任务的影响。实验结果表明，根据图大小选择合适的生成器可以平衡可扩展性和质量，从而确保标签一致并显著提升分类性能。这种方法为GNNs的数据增强提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB",
        "cs.IT",
        "math.IT"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.14765v1",
      "published_date": "2024-07-20 06:05:26 UTC",
      "updated_date": "2024-07-20 06:05:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:31:59.873963"
    },
    {
      "arxiv_id": "2407.14743v1",
      "title": "Denoising Long- and Short-term Interests for Sequential Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Xinyu Zhang",
        "Beibei Li",
        "Beihong Jin"
      ],
      "abstract": "User interests can be viewed over different time scales, mainly including\nstable long-term preferences and changing short-term intentions, and their\ncombination facilitates the comprehensive sequential recommendation. However,\nexisting work that focuses on different time scales of user modeling has\nignored the negative effects of different time-scale noise, which hinders\ncapturing actual user interests and cannot be resolved by conventional\nsequential denoising methods. In this paper, we propose a Long- and Short-term\nInterest Denoising Network (LSIDN), which employs different encoders and\ntailored denoising strategies to extract long- and short-term interests,\nrespectively, achieving both comprehensive and robust user modeling.\nSpecifically, we employ a session-level interest extraction and evolution\nstrategy to avoid introducing inter-session behavioral noise into long-term\ninterest modeling; we also adopt contrastive learning equipped with a\nhomogeneous exchanging augmentation to alleviate the impact of unintentional\nbehavioral noise on short-term interest modeling. Results of experiments on two\npublic datasets show that LSIDN consistently outperforms state-of-the-art\nmodels and achieves significant robustness.",
      "tldr_zh": "这篇论文针对顺序推荐(Sequential Recommendation)中的用户兴趣建模问题，提出了一种Long- and Short-term Interest Denoising Network (LSIDN)，通过不同的编码器和定制去噪策略来提取并净化用户的长期偏好和短期意图。LSIDN 采用 session-level 兴趣提取与演化策略避免长期兴趣受跨会话行为噪声影响，并使用对比学习(contrastive learning)结合同质交换增强(homogeneous exchanging augmentation)来减轻短期兴趣中的无意行为噪声。实验结果显示，在两个公共数据集上，LSIDN 显著超越最先进模型，并展现出优秀的鲁棒性。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "9 pages, accepted by SDM 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.14743v1",
      "published_date": "2024-07-20 03:52:14 UTC",
      "updated_date": "2024-07-20 03:52:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:32:23.692528"
    },
    {
      "arxiv_id": "2407.14741v1",
      "title": "Orthogonal Hyper-category Guided Multi-interest Elicitation for Micro-video Matching",
      "title_zh": "翻译失败",
      "authors": [
        "Beibei Li",
        "Beihong Jin",
        "Yisong Yu",
        "Yiyuan Zheng",
        "Jiageng Song",
        "Wei Zhuo",
        "Tao Xiang"
      ],
      "abstract": "Watching micro-videos is becoming a part of public daily life. Usually, user\nwatching behaviors are thought to be rooted in their multiple different\ninterests. In the paper, we propose a model named OPAL for micro-video\nmatching, which elicits a user's multiple heterogeneous interests by\ndisentangling multiple soft and hard interest embeddings from user\ninteractions. Moreover, OPAL employs a two-stage training strategy, in which\nthe pre-train is to generate soft interests from historical interactions under\nthe guidance of orthogonal hyper-categories of micro-videos and the fine-tune\nis to reinforce the degree of disentanglement among the interests and learn the\ntemporal evolution of each interest of each user. We conduct extensive\nexperiments on two real-world datasets. The results show that OPAL not only\nreturns diversified micro-videos but also outperforms six state-of-the-art\nmodels in terms of recall and hit rate.",
      "tldr_zh": "本文提出 OPAL 模型，用于微视频 matching，通过从用户互动中分离多个软和硬 interest embeddings 来挖掘用户的异质多兴趣。模型采用两阶段训练策略：预训练阶段在 orthogonal hyper-categories 的指导下从历史互动生成软兴趣，微调阶段则强化兴趣间的解耦并学习每个用户的兴趣时序演化。在两个真实数据集上的实验结果显示，OPAL 不仅提供多样化的微视频推荐，而且在 recall 和 hit rate 上优于六种最先进模型。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "6 pages, accepted by ICME 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.14741v1",
      "published_date": "2024-07-20 03:41:57 UTC",
      "updated_date": "2024-07-20 03:41:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:32:28.913677"
    },
    {
      "arxiv_id": "2407.14738v1",
      "title": "Flatness-aware Sequential Learning Generates Resilient Backdoors",
      "title_zh": "翻译失败",
      "authors": [
        "Hoang Pham",
        "The-Anh Ta",
        "Anh Tran",
        "Khoa D. Doan"
      ],
      "abstract": "Recently, backdoor attacks have become an emerging threat to the security of\nmachine learning models. From the adversary's perspective, the implanted\nbackdoors should be resistant to defensive algorithms, but some recently\nproposed fine-tuning defenses can remove these backdoors with notable efficacy.\nThis is mainly due to the catastrophic forgetting (CF) property of deep neural\nnetworks. This paper counters CF of backdoors by leveraging continual learning\n(CL) techniques. We begin by investigating the connectivity between a\nbackdoored and fine-tuned model in the loss landscape. Our analysis confirms\nthat fine-tuning defenses, especially the more advanced ones, can easily push a\npoisoned model out of the backdoor regions, making it forget all about the\nbackdoors. Based on this finding, we re-formulate backdoor training through the\nlens of CL and propose a novel framework, named Sequential Backdoor Learning\n(SBL), that can generate resilient backdoors. This framework separates the\nbackdoor poisoning process into two tasks: the first task learns a backdoored\nmodel, while the second task, based on the CL principles, moves it to a\nbackdoored region resistant to fine-tuning. We additionally propose to seek\nflatter backdoor regions via a sharpness-aware minimizer in the framework,\nfurther strengthening the durability of the implanted backdoor. Finally, we\ndemonstrate the effectiveness of our method through extensive empirical\nexperiments on several benchmark datasets in the backdoor domain. The source\ncode is available at https://github.com/mail-research/SBL-resilient-backdoors",
      "tldr_zh": "该论文分析了后门攻击（backdoor attacks）在机器学习模型中的安全威胁，特别是微调防御（fine-tuning defenses）通过灾难性遗忘（catastrophic forgetting, CF）来移除后门的机制。作者提出Sequential Backdoor Learning (SBL)框架，将后门注入过程分为两个任务：首先训练一个后门模型，然后基于持续学习（continual learning, CL）原则，将模型移动到对微调更具抵抗力的后门区域。框架进一步引入sharpness-aware minimizer来寻找更平坦的后门区域（flatter backdoor regions），从而增强后门的耐久性。实验在多个基准数据集上验证了SBL的有效性，并提供了源代码。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.14738v1",
      "published_date": "2024-07-20 03:30:05 UTC",
      "updated_date": "2024-07-20 03:30:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:32:51.944834"
    },
    {
      "arxiv_id": "2407.14735v1",
      "title": "ECRTime: Ensemble Integration of Classification and Retrieval for Time Series Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Fan Zhao",
        "You Chen"
      ],
      "abstract": "Deep learning-based methods for Time Series Classification (TSC) typically\nutilize deep networks to extract features, which are then processed through a\ncombination of a Fully Connected (FC) layer and a SoftMax function. However, we\nhave observed the phenomenon of inter-class similarity and intra-class\ninconsistency in the datasets from the UCR archive and further analyzed how\nthis phenomenon adversely affects the \"FC+SoftMax\" paradigm. To address the\nissue, we introduce ECR, which, for the first time to our knowledge, applies\ndeep learning-based retrieval algorithm to the TSC problem and integrates\nclassification and retrieval models. Experimental results on 112 UCR datasets\ndemonstrate that ECR is state-of-the-art(sota) compared to existing deep\nlearning-based methods. Furthermore, we have developed a more precise\nclassifier, ECRTime, which is an ensemble of ECR. ECRTime surpasses the\ncurrently most accurate deep learning classifier, InceptionTime, in terms of\naccuracy, achieving this with reduced training time and comparable scalability.",
      "tldr_zh": "本研究针对时间序列分类 (TSC) 中存在的类间相似性和类内不一致性问题，分析了传统 \"Fully Connected (FC) + SoftMax\" 范式的局限性。作者提出 ECR 方法，这是首次将基于深度学习的检索算法整合到 TSC 中，结合分类和检索模型进行处理。在 112 个 UCR 数据集上的实验显示，ECR 超过了现有深度学习方法的 state-of-the-art 性能；此外，开发的集成版本 ECRTime 在准确性上超越了 InceptionTime，同时实现了更短的训练时间和可比的扩展性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.14735v1",
      "published_date": "2024-07-20 03:17:23 UTC",
      "updated_date": "2024-07-20 03:17:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:32:53.081855"
    },
    {
      "arxiv_id": "2407.14733v1",
      "title": "Hard Prompts Made Interpretable: Sparse Entropy Regularization for Prompt Tuning with RL",
      "title_zh": "翻译失败",
      "authors": [
        "Yunseon Choi",
        "Sangmin Bae",
        "Seonghyun Ban",
        "Minchan Jeong",
        "Chuheng Zhang",
        "Lei Song",
        "Li Zhao",
        "Jiang Bian",
        "Kee-Eung Kim"
      ],
      "abstract": "With the advent of foundation models, prompt tuning has positioned itself as\nan important technique for directing model behaviors and eliciting desired\nresponses. Prompt tuning regards selecting appropriate keywords included into\nthe input, thereby adapting to the downstream task without adjusting or\nfine-tuning the model parameters. There is a wide range of work in prompt\ntuning, from approaches that directly harness the backpropagated gradient\nsignals from the model, to those employing black-box optimization such as\nreinforcement learning (RL) methods. Our primary focus is on RLPrompt, which\naims to find optimal prompt tokens leveraging soft Q-learning. While the\nresults show promise, we have observed that the prompts frequently appear\nunnatural, which impedes their interpretability. We address this limitation by\nusing sparse Tsallis entropy regularization, a principled approach to filtering\nout unlikely tokens from consideration. We extensively evaluate our approach\nacross various tasks, including few-shot text classification, unsupervised text\nstyle transfer, and textual inversion from images. The results indicate a\nnotable improvement over baselines, highlighting the efficacy of our approach\nin addressing the challenges of prompt tuning. Moreover, we show that the\nprompts discovered using our method are more natural and interpretable compared\nto those from other baselines.",
      "tldr_zh": "该研究针对提示调整(prompt tuning)中的可解释性问题，提出了一种使用强化学习(RL)结合稀疏熵正则化(sparse Tsallis entropy regularization)的方法，以改进RLPrompt框架。传统RLPrompt依赖soft Q-learning优化提示词，但生成的提示往往不自然且难以解释；为此，论文引入稀疏熵正则化来过滤不相关标记(unlikely tokens)，使提示更具自然性和可读性。在few-shot text classification、unsupervised text style transfer和textual inversion from images等任务上，实验结果显示该方法比基线模型显著提升性能。总体而言，此方法增强了prompt tuning的可解释性，为基础模型的适应性应用提供了更可靠的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.14733v1",
      "published_date": "2024-07-20 03:10:19 UTC",
      "updated_date": "2024-07-20 03:10:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:33:05.204885"
    },
    {
      "arxiv_id": "2407.14725v3",
      "title": "CrowdMAC: Masked Crowd Density Completion for Robust Crowd Density Forecasting",
      "title_zh": "CrowdMAC：用于鲁棒人群密度预测的遮罩人群密度补全",
      "authors": [
        "Ryo Fujii",
        "Ryo Hachiuma",
        "Hideo Saito"
      ],
      "abstract": "A crowd density forecasting task aims to predict how the crowd density map\nwill change in the future from observed past crowd density maps. However, the\npast crowd density maps are often incomplete due to the miss-detection of\npedestrians, and it is crucial to develop a robust crowd density forecasting\nmodel against the miss-detection. This paper presents a MAsked crowd density\nCompletion framework for crowd density forecasting (CrowdMAC), which is\nsimultaneously trained to forecast future crowd density maps from partially\nmasked past crowd density maps (i.e., forecasting maps from past maps with\nmiss-detection) while reconstructing the masked observation maps (i.e.,\nimputing past maps with miss-detection). Additionally, we propose\nTemporal-Density-aware Masking (TDM), which non-uniformly masks tokens in the\nobserved crowd density map, considering the sparsity of the crowd density maps\nand the informativeness of the subsequent frames for the forecasting task.\nMoreover, we introduce multi-task masking to enhance training efficiency. In\nthe experiments, CrowdMAC achieves state-of-the-art performance on seven\nlarge-scale datasets, including SDD, ETH-UCY, inD, JRDB, VSCrowd, FDST, and\ncroHD. We also demonstrate the robustness of the proposed method against both\nsynthetic and realistic miss-detections. The code is released at\nhttps://fujiry0.github.io/CrowdMAC-project-page.",
      "tldr_zh": "本研究针对人群密度预测任务中，过去密度地图因行人漏检而导致的不完整问题，提出CrowdMAC框架。该框架通过同时训练未来密度地图预测（从部分掩盖的过去地图）和掩盖区域的重建（即补全漏检地图），实现鲁棒的预测。此外，引入Temporal-Density-aware Masking (TDM)方法，根据密度稀疏性和后续帧信息进行非均匀掩盖，并采用multi-task masking提升训练效率。在七个大型数据集（如SDD和ETH-UCY）上的实验中，CrowdMAC达到最先进性能，并证明了对合成和真实漏检的鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to WACV 2025",
      "pdf_url": "http://arxiv.org/pdf/2407.14725v3",
      "published_date": "2024-07-20 02:18:43 UTC",
      "updated_date": "2024-11-27 06:04:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:33:18.040951"
    },
    {
      "arxiv_id": "2407.14717v2",
      "title": "Differential Privacy of Cross-Attention with Provable Guarantee",
      "title_zh": "具有可证明保证的交叉注意力差分隐私",
      "authors": [
        "Yingyu Liang",
        "Zhenmei Shi",
        "Zhao Song",
        "Yufa Zhou"
      ],
      "abstract": "Cross-attention has become a fundamental module nowadays in many important\nartificial intelligence applications, e.g., retrieval-augmented generation\n(RAG), system prompt, guided stable diffusion, and many more. Ensuring\ncross-attention privacy is crucial and urgently needed because its key and\nvalue matrices may contain sensitive information about model providers and\ntheir users. In this work, we design a novel differential privacy (DP) data\nstructure to address the privacy security of cross-attention with a theoretical\nguarantee. In detail, let $n$ be the input token length of system prompt/RAG\ndata, $d$ be the feature dimension, $0 < \\alpha \\le 1$ be the relative error\nparameter, $R$ be the maximum value of the query and key matrices, $R_w$ be the\nmaximum value of the value matrix, and $r,s,\\epsilon_s$ be parameters of\npolynomial kernel methods. Then, our data structure requires\n$\\widetilde{O}(ndr^2)$ memory consumption with $\\widetilde{O}(nr^2)$\ninitialization time complexity and $\\widetilde{O}(\\alpha^{-1} r^2)$ query time\ncomplexity for a single token query. In addition, our data structure can\nguarantee that the process of answering user query satisfies $(\\epsilon,\n\\delta)$-DP with $\\widetilde{O}(n^{-1} \\epsilon^{-1} \\alpha^{-1/2} R^{2s} R_w\nr^2)$ additive error and $n^{-1} (\\alpha + \\epsilon_s)$ relative error between\nour output and the true answer. Furthermore, our result is robust to adaptive\nqueries in which users can intentionally attack the cross-attention system. To\nour knowledge, this is the first work to provide DP for cross-attention and is\npromising to inspire more privacy algorithm design in large generative models\n(LGMs).",
      "tldr_zh": "该论文针对交叉注意力（Cross-Attention）机制在AI应用（如RAG和系统提示）中的隐私风险，设计了一个新型的差分隐私（DP）数据结构，以理论保证保护关键和值矩阵的敏感信息。数据结构在内存消耗为$\\widetilde{O}(ndr^2)$、初始化时间为$\\widetilde{O}(nr^2)$和查询时间为$\\widetilde{O}(\\alpha^{-1} r^2)$的情况下，确保（ε, δ）-DP，并提供$\\widetilde{O}(n^{-1} \\epsilon^{-1} \\alpha^{-1/2} R^{2s} R_w r^2)$的加法误差和$n^{-1} (\\alpha + \\epsilon_s)$的相对误差，对自适应查询（adaptive queries）具有鲁棒性。该工作是首个为Cross-Attention提供DP框架的开创性研究，有望推动大型生成模型（LGMs）的隐私算法创新。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.14717v2",
      "published_date": "2024-07-20 01:02:27 UTC",
      "updated_date": "2024-10-15 04:19:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:33:31.180883"
    },
    {
      "arxiv_id": "2407.14714v1",
      "title": "Unveiling the Decision-Making Process in Reinforcement Learning with Genetic Programming",
      "title_zh": "通过遗传编程揭示强化学习中的决策过程",
      "authors": [
        "Manuel Eberhardinger",
        "Florian Rupp",
        "Johannes Maucher",
        "Setareh Maghsudi"
      ],
      "abstract": "Despite tremendous progress, machine learning and deep learning still suffer\nfrom incomprehensible predictions. Incomprehensibility, however, is not an\noption for the use of (deep) reinforcement learning in the real world, as\nunpredictable actions can seriously harm the involved individuals. In this\nwork, we propose a genetic programming framework to generate explanations for\nthe decision-making process of already trained agents by imitating them with\nprograms. Programs are interpretable and can be executed to generate\nexplanations of why the agent chooses a particular action. Furthermore, we\nconduct an ablation study that investigates how extending the domain-specific\nlanguage by using library learning alters the performance of the method. We\ncompare our results with the previous state of the art for this problem and\nshow that we are comparable in performance but require much less hardware\nresources and computation time.",
      "tldr_zh": "该研究针对强化学习（reinforcement learning）中决策过程的不透明性问题，提出了一种基于遗传编程（genetic programming）的框架，用于模仿已训练代理并生成可解释的程序，从而揭示代理为什么选择特定动作。该框架通过程序执行提供决策解释，并通过消融研究探讨了扩展领域特定语言（domain-specific language）使用库学习（library learning）对性能的影响。实验结果显示，该方法在性能上与现有最先进技术相当，但显著降低了硬件资源和计算时间需求，为可解释强化学习应用提供了高效解决方案。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at: The Fifteenth International Conference on Swarm\n  Intelligence (ICSI'2024)",
      "pdf_url": "http://arxiv.org/pdf/2407.14714v1",
      "published_date": "2024-07-20 00:45:03 UTC",
      "updated_date": "2024-07-20 00:45:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:33:39.943104"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 46,
  "processed_papers_count": 46,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-19T08:33:57.732804"
}
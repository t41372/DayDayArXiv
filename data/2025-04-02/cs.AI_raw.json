[
  {
    "arxiv_id": "2504.01951v1",
    "title": "The LLM Wears Prada: Analysing Gender Bias and Stereotypes through Online Shopping Data",
    "authors": [
      "Massimiliano Luca",
      "Ciro Beneduce",
      "Bruno Lepri",
      "Jacopo Staiano"
    ],
    "abstract": "With the wide and cross-domain adoption of Large Language Models, it becomes\ncrucial to assess to which extent the statistical correlations in training\ndata, which underlie their impressive performance, hide subtle and potentially\ntroubling biases. Gender bias in LLMs has been widely investigated from the\nperspectives of works, hobbies, and emotions typically associated with a\nspecific gender. In this study, we introduce a novel perspective. We\ninvestigate whether LLMs can predict an individual's gender based solely on\nonline shopping histories and whether these predictions are influenced by\ngender biases and stereotypes. Using a dataset of historical online purchases\nfrom users in the United States, we evaluate the ability of six LLMs to\nclassify gender and we then analyze their reasoning and products-gender\nco-occurrences. Results indicate that while models can infer gender with\nmoderate accuracy, their decisions are often rooted in stereotypical\nassociations between product categories and gender. Furthermore, explicit\ninstructions to avoid bias reduce the certainty of model predictions, but do\nnot eliminate stereotypical patterns. Our findings highlight the persistent\nnature of gender biases in LLMs and emphasize the need for robust\nbias-mitigation strategies.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01951v1",
    "published_date": "2025-04-02 17:56:08 UTC",
    "updated_date": "2025-04-02 17:56:08 UTC"
  },
  {
    "arxiv_id": "2504.01947v1",
    "title": "Efficient Federated Learning Tiny Language Models for Mobile Network Feature Prediction",
    "authors": [
      "Daniel Becking",
      "Ingo Friese",
      "Karsten Müller",
      "Thomas Buchholz",
      "Mandy Galkow-Schneider",
      "Wojciech Samek",
      "Detlev Marpe"
    ],
    "abstract": "In telecommunications, Autonomous Networks (ANs) automatically adjust\nconfigurations based on specific requirements (e.g., bandwidth) and available\nresources. These networks rely on continuous monitoring and intelligent\nmechanisms for self-optimization, self-repair, and self-protection, nowadays\nenhanced by Neural Networks (NNs) to enable predictive modeling and pattern\nrecognition. Here, Federated Learning (FL) allows multiple AN cells - each\nequipped with NNs - to collaboratively train models while preserving data\nprivacy. However, FL requires frequent transmission of large neural data and\nthus an efficient, standardized compression strategy for reliable\ncommunication. To address this, we investigate NNCodec, a Fraunhofer\nimplementation of the ISO/IEC Neural Network Coding (NNC) standard, within a\nnovel FL framework that integrates tiny language models (TLMs) for various\nmobile network feature prediction (e.g., ping, SNR or band frequency). Our\nexperimental results on the Berlin V2X dataset demonstrate that NNCodec\nachieves transparent compression (i.e., negligible performance loss) while\nreducing communication overhead to below 1%, showing the effectiveness of\ncombining NNC with FL in collaboratively learned autonomous mobile networks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at 2025 EuCNC & 6G Summit Poster Session",
    "pdf_url": "http://arxiv.org/pdf/2504.01947v1",
    "published_date": "2025-04-02 17:54:06 UTC",
    "updated_date": "2025-04-02 17:54:06 UTC"
  },
  {
    "arxiv_id": "2504.01935v1",
    "title": "Critical Thinking: Which Kinds of Complexity Govern Optimal Reasoning Length?",
    "authors": [
      "Celine Lee",
      "Alexander M. Rush",
      "Keyon Vafa"
    ],
    "abstract": "Large language models (LLMs) often benefit from verbalized reasoning at\ninference time, but it remains unclear which aspects of task difficulty these\nextra reasoning tokens address. To investigate this question, we formalize a\nframework using deterministic finite automata (DFAs). DFAs offer a formalism\nthrough which we can characterize task complexity through measurable properties\nsuch as run length (number of reasoning steps required) and state-space size\n(decision complexity). We first show that across different tasks and models of\ndifferent sizes and training paradigms, there exists an optimal amount of\nreasoning tokens such that the probability of producing a correct solution is\nmaximized. We then investigate which properties of complexity govern this\ncritical length: we find that task instances with longer corresponding\nunderlying DFA runs (i.e. demand greater latent state-tracking requirements)\ncorrelate with longer reasoning lengths, but, surprisingly, that DFA size (i.e.\nstate-space complexity) does not. We then demonstrate an implication of these\nfindings: being able to predict the optimal number of reasoning tokens for new\nproblems and filtering out non-optimal length answers results in consistent\naccuracy improvements.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01935v1",
    "published_date": "2025-04-02 17:45:58 UTC",
    "updated_date": "2025-04-02 17:45:58 UTC"
  },
  {
    "arxiv_id": "2504.01930v1",
    "title": "A thorough benchmark of automatic text classification: From traditional approaches to large language models",
    "authors": [
      "Washington Cunha",
      "Leonardo Rocha",
      "Marcos André Gonçalves"
    ],
    "abstract": "Automatic text classification (ATC) has experienced remarkable advancements\nin the past decade, best exemplified by recent small and large language models\n(SLMs and LLMs), leveraged by Transformer architectures. Despite recent\neffectiveness improvements, a comprehensive cost-benefit analysis investigating\nwhether the effectiveness gains of these recent approaches compensate their\nmuch higher costs when compared to more traditional text classification\napproaches such as SVMs and Logistic Regression is still missing in the\nliterature. In this context, this work's main contributions are twofold: (i) we\nprovide a scientifically sound comparative analysis of the cost-benefit of\ntwelve traditional and recent ATC solutions including five open LLMs, and (ii)\na large benchmark comprising {22 datasets}, including sentiment analysis and\ntopic classification, with their (train-validation-test) partitions based on\nfolded cross-validation procedures, along with documentation, and code. The\nrelease of code, data, and documentation enables the community to replicate\nexperiments and advance the field in a more scientifically sound manner. Our\ncomparative experimental results indicate that LLMs outperform traditional\napproaches (up to 26%-7.1% on average) and SLMs (up to 4.9%-1.9% on average) in\nterms of effectiveness. However, LLMs incur significantly higher computational\ncosts due to fine-tuning, being, on average 590x and 8.5x slower than\ntraditional methods and SLMs, respectively. Results suggests the following\nrecommendations: (1) LLMs for applications that require the best possible\neffectiveness and can afford the costs; (2) traditional methods such as\nLogistic Regression and SVM for resource-limited applications or those that\ncannot afford the cost of tuning large LLMs; and (3) SLMs like Roberta for\nnear-optimal effectiveness-efficiency trade-off.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "7 pages, 2 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.01930v1",
    "published_date": "2025-04-02 17:40:08 UTC",
    "updated_date": "2025-04-02 17:40:08 UTC"
  },
  {
    "arxiv_id": "2504.01925v1",
    "title": "Equivariant Spherical CNNs for Accurate Fiber Orientation Distribution Estimation in Neonatal Diffusion MRI with Reduced Acquisition Time",
    "authors": [
      "Haykel Snoussi",
      "Davood Karimi"
    ],
    "abstract": "Early and accurate assessment of brain microstructure using diffusion\nMagnetic Resonance Imaging (dMRI) is crucial for identifying neurodevelopmental\ndisorders in neonates, but remains challenging due to low signal-to-noise ratio\n(SNR), motion artifacts, and ongoing myelination. In this study, we propose a\nrotationally equivariant Spherical Convolutional Neural Network (sCNN)\nframework tailored for neonatal dMRI. We predict the Fiber Orientation\nDistribution (FOD) from multi-shell dMRI signals acquired with a reduced set of\ngradient directions (30% of the full protocol), enabling faster and more\ncost-effective acquisitions. We train and evaluate the performance of our sCNN\nusing real data from 43 neonatal dMRI datasets provided by the Developing Human\nConnectome Project (dHCP). Our results demonstrate that the sCNN achieves\nsignificantly lower mean squared error (MSE) and higher angular correlation\ncoefficient (ACC) compared to a Multi-Layer Perceptron (MLP) baseline,\nindicating improved accuracy in FOD estimation. Furthermore, tractography\nresults based on the sCNN-predicted FODs show improved anatomical plausibility,\ncoverage, and coherence compared to those from the MLP. These findings\nhighlight that sCNNs, with their inherent rotational equivariance, offer a\npromising approach for accurate and clinically efficient dMRI analysis, paving\nthe way for improved diagnostic capabilities and characterization of early\nbrain development.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01925v1",
    "published_date": "2025-04-02 17:36:51 UTC",
    "updated_date": "2025-04-02 17:36:51 UTC"
  },
  {
    "arxiv_id": "2504.01919v2",
    "title": "Bridging the Linguistic Divide: A Survey on Leveraging Large Language Models for Machine Translation",
    "authors": [
      "Baban Gain",
      "Dibyanayan Bandyopadhyay",
      "Asif Ekbal"
    ],
    "abstract": "The advent of Large Language Models (LLMs) has significantly reshaped the\nlandscape of machine translation (MT), particularly for low-resource languages\nand domains that lack sufficient parallel corpora, linguistic tools, and\ncomputational infrastructure. This survey presents a comprehensive overview of\nrecent progress in leveraging LLMs for MT. We analyze techniques such as\nfew-shot prompting, cross-lingual transfer, and parameter-efficient fine-tuning\nthat enable effective adaptation to under-resourced settings. The paper also\nexplores synthetic data generation strategies using LLMs, including\nback-translation and lexical augmentation. Additionally, we compare LLM-based\ntranslation with traditional encoder-decoder models across diverse language\npairs, highlighting the strengths and limitations of each. We discuss\npersistent challenges such as hallucinations, evaluation inconsistencies, and\ninherited biases while also evaluating emerging LLM-driven metrics for\ntranslation quality. This survey offers practical insights and outlines future\ndirections for building robust, inclusive, and scalable MT systems in the era\nof large-scale generative models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01919v2",
    "published_date": "2025-04-02 17:26:40 UTC",
    "updated_date": "2025-04-03 13:30:35 UTC"
  },
  {
    "arxiv_id": "2504.01916v1",
    "title": "FineLIP: Extending CLIP's Reach via Fine-Grained Alignment with Longer Text Inputs",
    "authors": [
      "Mothilal Asokan",
      "Kebin Wu",
      "Fatima Albreiki"
    ],
    "abstract": "As a pioneering vision-language model, CLIP (Contrastive Language-Image\nPre-training) has achieved significant success across various domains and a\nwide range of downstream vision-language tasks. However, the text encoders in\npopular CLIP models are limited to processing only 77 text tokens, which\nconstrains their ability to effectively handle longer, detail-rich captions.\nAdditionally, CLIP models often struggle to effectively capture detailed visual\nand textual information, which hampers their performance on tasks that require\nfine-grained analysis. To address these limitations, we present a novel\napproach, \\textbf{FineLIP}, that extends the capabilities of CLIP. FineLIP\nenhances cross-modal text-image mapping by incorporating \\textbf{Fine}-grained\nalignment with \\textbf{L}onger text input within the CL\\textbf{IP}-style\nframework. FineLIP first extends the positional embeddings to handle longer\ntext, followed by the dynamic aggregation of local image and text tokens. The\naggregated results are then used to enforce fine-grained token-to-token\ncross-modal alignment. We validate our model on datasets with long, detailed\ncaptions across two tasks: zero-shot cross-modal retrieval and text-to-image\ngeneration. Quantitative and qualitative experimental results demonstrate the\neffectiveness of FineLIP, outperforming existing state-of-the-art approaches.\nFurthermore, comprehensive ablation studies validate the benefits of key design\nelements within FineLIP.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01916v1",
    "published_date": "2025-04-02 17:19:59 UTC",
    "updated_date": "2025-04-02 17:19:59 UTC"
  },
  {
    "arxiv_id": "2504.01911v1",
    "title": "Advancing AI-Scientist Understanding: Making LLM Think Like a Physicist with Interpretable Reasoning",
    "authors": [
      "Yinggan Xu",
      "Hana Kimlee",
      "Yijia Xiao",
      "Di Luo"
    ],
    "abstract": "Large Language Models (LLMs) are playing an expanding role in physics\nresearch by enhancing reasoning, symbolic manipulation, and numerical\ncomputation. However, ensuring the reliability and interpretability of their\noutputs remains a significant challenge. In our framework, we conceptualize the\ncollaboration between AI and human scientists as a dynamic interplay among\nthree modules: the reasoning module, the interpretation module, and the\nAI-scientist interaction module. Recognizing that effective physics reasoning\ndemands rigorous logical consistency, quantitative precision, and deep\nintegration with established theoretical models, we introduce the\ninterpretation module to improve the understanding of AI-generated outputs,\nwhich is not previously explored in the literature. This module comprises\nmultiple specialized agents, including summarizers, model builders, UI\nbuilders, and testers, which collaboratively structure LLM outputs within a\nphysically grounded framework, by constructing a more interpretable science\nmodel. A case study demonstrates that our approach enhances transparency,\nfacilitates validation, and strengthens AI-augmented reasoning in scientific\ndiscovery.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01911v1",
    "published_date": "2025-04-02 17:13:16 UTC",
    "updated_date": "2025-04-02 17:13:16 UTC"
  },
  {
    "arxiv_id": "2504.01908v1",
    "title": "Benchmarking Synthetic Tabular Data: A Multi-Dimensional Evaluation Framework",
    "authors": [
      "Andrey Sidorenko",
      "Michael Platzer",
      "Mario Scriminaci",
      "Paul Tiwald"
    ],
    "abstract": "Evaluating the quality of synthetic data remains a key challenge for ensuring\nprivacy and utility in data-driven research. In this work, we present an\nevaluation framework that quantifies how well synthetic data replicates\noriginal distributional properties while ensuring privacy. The proposed\napproach employs a holdout-based benchmarking strategy that facilitates\nquantitative assessment through low- and high-dimensional distribution\ncomparisons, embedding-based similarity measures, and nearest-neighbor distance\nmetrics. The framework supports various data types and structures, including\nsequential and contextual information, and enables interpretable quality\ndiagnostics through a set of standardized metrics. These contributions aim to\nsupport reproducibility and methodological consistency in benchmarking of\nsynthetic data generation techniques. The code of the framework is available at\nhttps://github.com/mostly-ai/mostlyai-qa.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 7 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2504.01908v1",
    "published_date": "2025-04-02 17:10:30 UTC",
    "updated_date": "2025-04-02 17:10:30 UTC"
  },
  {
    "arxiv_id": "2504.01905v2",
    "title": "Accelerating IoV Intrusion Detection: Benchmarking GPU-Accelerated vs CPU-Based ML Libraries",
    "authors": [
      "Furkan Çolhak",
      "Hasan Coşkun",
      "Tsafac Nkombong Regine Cyrille",
      "Tedi Hoxa",
      "Mert İlhan Ecevit",
      "Mehmet Nafiz Aydın"
    ],
    "abstract": "The Internet of Vehicles (IoV) may face challenging cybersecurity attacks\nthat may require sophisticated intrusion detection systems, necessitating a\nrapid development and response system. This research investigates the\nperformance advantages of GPU-accelerated libraries (cuML) compared to\ntraditional CPU-based implementations (scikit-learn), focusing on the speed and\nefficiency required for machine learning models used in IoV threat detection\nenvironments. The comprehensive evaluations conducted employ four machine\nlearning approaches (Random Forest, KNN, Logistic Regression, XGBoost) across\nthree distinct IoV security datasets (OTIDS, GIDS, CICIoV2024). Our findings\ndemonstrate that GPU-accelerated implementations dramatically improved\ncomputational efficiency, with training times reduced by a factor of up to 159\nand prediction speeds accelerated by up to 95 times compared to traditional CPU\nprocessing, all while preserving detection accuracy. This remarkable\nperformance breakthrough empowers researchers and security specialists to\nharness GPU acceleration for creating faster, more effective threat detection\nsystems that meet the urgent real-time security demands of today's connected\nvehicle networks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "CIIT 2025 22nd International Conference on Informatics and\n  Information Technologies (CIIT)",
    "pdf_url": "http://arxiv.org/pdf/2504.01905v2",
    "published_date": "2025-04-02 17:04:53 UTC",
    "updated_date": "2025-04-03 08:42:45 UTC"
  },
  {
    "arxiv_id": "2504.01903v1",
    "title": "STAR-1: Safer Alignment of Reasoning LLMs with 1K Data",
    "authors": [
      "Zijun Wang",
      "Haoqin Tu",
      "Yuhan Wang",
      "Juncheng Wu",
      "Jieru Mei",
      "Brian R. Bartoldson",
      "Bhavya Kailkhura",
      "Cihang Xie"
    ],
    "abstract": "This paper introduces STAR-1, a high-quality, just-1k-scale safety dataset\nspecifically designed for large reasoning models (LRMs) like DeepSeek-R1. Built\non three core principles -- diversity, deliberative reasoning, and rigorous\nfiltering -- STAR-1 aims to address the critical needs for safety alignment in\nLRMs. Specifically, we begin by integrating existing open-source safety\ndatasets from diverse sources. Then, we curate safety policies to generate\npolicy-grounded deliberative reasoning samples. Lastly, we apply a GPT-4o-based\nsafety scoring system to select training examples aligned with best practices.\nExperimental results show that fine-tuning LRMs with STAR-1 leads to an average\n40% improvement in safety performance across four benchmarks, while only\nincurring a marginal decrease (e.g., an average of 1.1%) in reasoning ability\nmeasured across five reasoning tasks. Extensive ablation studies further\nvalidate the importance of our design principles in constructing STAR-1 and\nanalyze its efficacy across both LRMs and traditional LLMs. Our project page is\nhttps://ucsc-vlaa.github.io/STAR-1.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01903v1",
    "published_date": "2025-04-02 17:04:04 UTC",
    "updated_date": "2025-04-02 17:04:04 UTC"
  },
  {
    "arxiv_id": "2504.01902v1",
    "title": "Graphically Speaking: Unmasking Abuse in Social Media with Conversation Insights",
    "authors": [
      "Célia Nouri",
      "Jean-Philippe Cointet",
      "Chloé Clavel"
    ],
    "abstract": "Detecting abusive language in social media conversations poses significant\nchallenges, as identifying abusiveness often depends on the conversational\ncontext, characterized by the content and topology of preceding comments.\nTraditional Abusive Language Detection (ALD) models often overlook this\ncontext, which can lead to unreliable performance metrics. Recent Natural\nLanguage Processing (NLP) methods that integrate conversational context often\ndepend on limited and simplified representations, and report inconsistent\nresults. In this paper, we propose a novel approach that utilize graph neural\nnetworks (GNNs) to model social media conversations as graphs, where nodes\nrepresent comments, and edges capture reply structures. We systematically\ninvestigate various graph representations and context windows to identify the\noptimal configuration for ALD. Our GNN model outperform both context-agnostic\nbaselines and linear context-aware methods, achieving significant improvements\nin F1 scores. These findings demonstrate the critical role of structured\nconversational context and establish GNNs as a robust framework for advancing\ncontext-aware abusive language detection.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01902v1",
    "published_date": "2025-04-02 17:03:37 UTC",
    "updated_date": "2025-04-02 17:03:37 UTC"
  },
  {
    "arxiv_id": "2504.01901v1",
    "title": "Ross3D: Reconstructive Visual Instruction Tuning with 3D-Awareness",
    "authors": [
      "Haochen Wang",
      "Yucheng Zhao",
      "Tiancai Wang",
      "Haoqiang Fan",
      "Xiangyu Zhang",
      "Zhaoxiang Zhang"
    ],
    "abstract": "The rapid development of Large Multimodal Models (LMMs) for 2D images and\nvideos has spurred efforts to adapt these models for interpreting 3D scenes.\nHowever, the absence of large-scale 3D vision-language datasets has posed a\nsignificant obstacle. To address this issue, typical approaches focus on\ninjecting 3D awareness into 2D LMMs by designing 3D input-level scene\nrepresentations. This work provides a new perspective. We introduce\nreconstructive visual instruction tuning with 3D-awareness (Ross3D), which\nintegrates 3D-aware visual supervision into the training procedure.\nSpecifically, it incorporates cross-view and global-view reconstruction. The\nformer requires reconstructing masked views by aggregating overlapping\ninformation from other views. The latter aims to aggregate information from all\navailable views to recover Bird's-Eye-View images, contributing to a\ncomprehensive overview of the entire scene. Empirically, Ross3D achieves\nstate-of-the-art performance across various 3D scene understanding benchmarks.\nMore importantly, our semi-supervised experiments demonstrate significant\npotential in leveraging large amounts of unlabeled 3D vision-only data.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01901v1",
    "published_date": "2025-04-02 16:59:55 UTC",
    "updated_date": "2025-04-02 16:59:55 UTC"
  },
  {
    "arxiv_id": "2504.01888v1",
    "title": "A novel gesture interaction control method for rehabilitation lower extremity exoskeleton",
    "authors": [
      "Shuang Qiu",
      "Zhongcai Pei",
      "Chen Wang",
      "Jing Zhang",
      "Zhiyong Tang"
    ],
    "abstract": "With the rapid development of Rehabilitation Lower Extremity Robotic\nExoskeletons (RLEEX) technology, significant advancements have been made in\nHuman-Robot Interaction (HRI) methods. These include traditional physical HRI\nmethods that are easily recognizable and various bio-electrical signal-based\nHRI methods that can visualize and predict actions. However, most of these HRI\nmethods are contact-based, facing challenges such as operational complexity,\nsensitivity to interference, risks associated with implantable devices, and,\nmost importantly, limitations in comfort. These challenges render the\ninteraction less intuitive and natural, which can negatively impact patient\nmotivation for rehabilitation. To address these issues, this paper proposes a\nnovel non-contact gesture interaction control method for RLEEX, based on RGB\nmonocular camera depth estimation. This method integrates three key steps:\ndetecting keypoints, recognizing gestures, and assessing distance, thereby\napplying gesture information and augmented reality triggering technology to\ncontrol gait movements of RLEEX. Results indicate that this approach provides a\nfeasible solution to the problems of poor comfort, low reliability, and high\nlatency in HRI for RLEEX platforms. Specifically, it achieves a\ngesture-controlled exoskeleton motion accuracy of 94.11\\% and an average system\nresponse time of 0.615 seconds through non-contact HRI. The proposed\nnon-contact HRI method represents a pioneering advancement in control\ninteractions for RLEEX, paving the way for further exploration and development\nin this field.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01888v1",
    "published_date": "2025-04-02 16:46:01 UTC",
    "updated_date": "2025-04-02 16:46:01 UTC"
  },
  {
    "arxiv_id": "2504.01883v1",
    "title": "CoRAG: Collaborative Retrieval-Augmented Generation",
    "authors": [
      "Aashiq Muhamed",
      "Mona Diab",
      "Virginia Smith"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) models excel in knowledge-intensive\ntasks, especially under few-shot learning constraints. We introduce CoRAG, a\nframework extending RAG to collaborative settings, where clients jointly train\na shared model using a collaborative passage store. To evaluate CoRAG, we\nintroduce CRAB, a benchmark for collaborative homogeneous open-domain question\nanswering. Our experiments demonstrate that CoRAG consistently outperforms both\nparametric collaborative learning methods and locally trained RAG models in\nlow-resource scenarios. Further analysis reveals the critical importance of\nrelevant passages within the shared store, the surprising benefits of\nincorporating irrelevant passages, and the potential for hard negatives to\nnegatively impact performance. This introduces a novel consideration in\ncollaborative RAG: the trade-off between leveraging a collectively enriched\nknowledge base and the potential risk of incorporating detrimental passages\nfrom other clients. Our findings underscore the viability of CoRAG, while also\nhighlighting key design challenges and promising avenues for future research.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "NAACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2504.01883v1",
    "published_date": "2025-04-02 16:40:43 UTC",
    "updated_date": "2025-04-02 16:40:43 UTC"
  },
  {
    "arxiv_id": "2504.01871v1",
    "title": "Interpreting Emergent Planning in Model-Free Reinforcement Learning",
    "authors": [
      "Thomas Bush",
      "Stephen Chung",
      "Usman Anwar",
      "Adrià Garriga-Alonso",
      "David Krueger"
    ],
    "abstract": "We present the first mechanistic evidence that model-free reinforcement\nlearning agents can learn to plan. This is achieved by applying a methodology\nbased on concept-based interpretability to a model-free agent in Sokoban -- a\ncommonly used benchmark for studying planning. Specifically, we demonstrate\nthat DRC, a generic model-free agent introduced by Guez et al. (2019), uses\nlearned concept representations to internally formulate plans that both predict\nthe long-term effects of actions on the environment and influence action\nselection. Our methodology involves: (1) probing for planning-relevant\nconcepts, (2) investigating plan formation within the agent's representations,\nand (3) verifying that discovered plans (in the agent's representations) have a\ncausal effect on the agent's behavior through interventions. We also show that\nthe emergence of these plans coincides with the emergence of a planning-like\nproperty: the ability to benefit from additional test-time compute. Finally, we\nperform a qualitative analysis of the planning algorithm learned by the agent\nand discover a strong resemblance to parallelized bidirectional search. Our\nfindings advance understanding of the internal mechanisms underlying planning\nbehavior in agents, which is important given the recent trend of emergent\nplanning and reasoning capabilities in LLMs through RL",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025 oral",
    "pdf_url": "http://arxiv.org/pdf/2504.01871v1",
    "published_date": "2025-04-02 16:24:23 UTC",
    "updated_date": "2025-04-02 16:24:23 UTC"
  },
  {
    "arxiv_id": "2504.01866v1",
    "title": "From Code Generation to Software Testing: AI Copilot with Context-Based RAG",
    "authors": [
      "Yuchen Wang",
      "Shangxin Guo",
      "Chee Wei Tan"
    ],
    "abstract": "The rapid pace of large-scale software development places increasing demands\non traditional testing methodologies, often leading to bottlenecks in\nefficiency, accuracy, and coverage. We propose a novel perspective on software\ntesting by positing bug detection and coding with fewer bugs as two\ninterconnected problems that share a common goal, which is reducing bugs with\nlimited resources. We extend our previous work on AI-assisted programming,\nwhich supports code auto-completion and chatbot-powered Q&A, to the realm of\nsoftware testing. We introduce Copilot for Testing, an automated testing system\nthat synchronizes bug detection with codebase updates, leveraging context-based\nRetrieval Augmented Generation (RAG) to enhance the capabilities of large\nlanguage models (LLMs). Our evaluation demonstrates a 31.2% improvement in bug\ndetection accuracy, a 12.6% increase in critical test coverage, and a 10.5%\nhigher user acceptance rate, highlighting the transformative potential of\nAI-driven technologies in modern software development practices.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.SE",
    "comment": "This work has been accepted for publication in IEEE Software (DOI:\n  10.1109/MS.2025.3549628)",
    "pdf_url": "http://arxiv.org/pdf/2504.01866v1",
    "published_date": "2025-04-02 16:20:05 UTC",
    "updated_date": "2025-04-02 16:20:05 UTC"
  },
  {
    "arxiv_id": "2504.01857v1",
    "title": "Cross-Lingual Consistency: A Novel Inference Framework for Advancing Reasoning in Large Language Models",
    "authors": [
      "Zhiwei Yu",
      "Tuo Li",
      "Changhong Wang",
      "Hui Chen",
      "Lang Zhou"
    ],
    "abstract": "Chain-of-thought (CoT) has emerged as a critical mechanism for enhancing\nreasoning capabilities in large language models (LLMs), with self-consistency\ndemonstrating notable promise in boosting performance. However, inherent\nlinguistic biases in multilingual training corpora frequently cause semantic\ndrift and logical inconsistencies, especially in sub-10B parameter LLMs\nhandling complex inference tasks. To overcome these constraints, we propose the\nCross-Lingual Consistency (CLC) framework, an innovative inference paradigm\nthat integrates multilingual reasoning paths through majority voting to elevate\nLLMs' reasoning capabilities. Empirical evaluations on the CMATH dataset reveal\nCLC's superiority over the conventional self-consistency method, delivering\n9.5%, 6.5%, and 6.0% absolute accuracy gains for DeepSeek-Math-7B-Instruct,\nQwen2.5-Math-7B-Instruct, and Gemma2-9B-Instruct respectively. Expanding CLC's\nlinguistic scope to 11 diverse languages implies two synergistic benefits: 1)\nneutralizing linguistic biases in multilingual training corpora through\nmultilingual ensemble voting, 2) escaping monolingual reasoning traps by\nexploring the broader multilingual solution space. This dual benefits\nempirically enables more globally optimal reasoning paths compared to\nmonolingual self-consistency baselines, as evidenced by the 4.1%-18.5% accuracy\ngains using Gemma2-9B-Instruct on the MGSM dataset.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01857v1",
    "published_date": "2025-04-02 16:09:39 UTC",
    "updated_date": "2025-04-02 16:09:39 UTC"
  },
  {
    "arxiv_id": "2504.01855v1",
    "title": "Enhanced Diffusion Sampling via Extrapolation with Multiple ODE Solutions",
    "authors": [
      "Jinyoung Choi",
      "Junoh Kang",
      "Bohyung Han"
    ],
    "abstract": "Diffusion probabilistic models (DPMs), while effective in generating\nhigh-quality samples, often suffer from high computational costs due to their\niterative sampling process. To address this, we propose an enhanced ODE-based\nsampling method for DPMs inspired by Richardson extrapolation, which reduces\nnumerical error and improves convergence rates. Our method, RX-DPM, leverages\nmultiple ODE solutions at intermediate time steps to extrapolate the denoised\nprediction in DPMs. This significantly enhances the accuracy of estimations for\nthe final sample while maintaining the number of function evaluations (NFEs).\nUnlike standard Richardson extrapolation, which assumes uniform discretization\nof the time grid, we develop a more general formulation tailored to arbitrary\ntime step scheduling, guided by local truncation error derived from a baseline\nsampling method. The simplicity of our approach facilitates accurate estimation\nof numerical solutions without significant computational overhead, and allows\nfor seamless and convenient integration into various DPMs and solvers.\nAdditionally, RX-DPM provides explicit error estimates, effectively\ndemonstrating the faster convergence as the leading error term's order\nincreases. Through a series of experiments, we show that the proposed method\nimproves the quality of generated samples without requiring additional sampling\niterations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.01855v1",
    "published_date": "2025-04-02 16:06:23 UTC",
    "updated_date": "2025-04-02 16:06:23 UTC"
  },
  {
    "arxiv_id": "2504.01850v1",
    "title": "Code Red! On the Harmfulness of Applying Off-the-shelf Large Language Models to Programming Tasks",
    "authors": [
      "Ali Al-Kaswan",
      "Sebastian Deatc",
      "Begüm Koç",
      "Arie van Deursen",
      "Maliheh Izadi"
    ],
    "abstract": "Nowadays, developers increasingly rely on solutions powered by Large Language\nModels (LLM) to assist them with their coding tasks. This makes it crucial to\nalign these tools with human values to prevent malicious misuse. In this paper,\nwe propose a comprehensive framework for assessing the potential harmfulness of\nLLMs within the software engineering domain. We begin by developing a taxonomy\nof potentially harmful software engineering scenarios and subsequently, create\na dataset of prompts based on this taxonomy. To systematically assess the\nresponses, we design and validate an automatic evaluator that classifies the\noutputs of a variety of LLMs both open-source and closed-source models, as well\nas general-purpose and code-specific LLMs. Furthermore, we investigate the\nimpact of models size, architecture family, and alignment strategies on their\ntendency to generate harmful content. The results show significant disparities\nin the alignment of various LLMs for harmlessness. We find that some models and\nmodel families, such as Openhermes, are more harmful than others and that\ncode-specific models do not perform better than their general-purpose\ncounterparts. Notably, some fine-tuned models perform significantly worse than\ntheir base-models due to their design choices. On the other side, we find that\nlarger models tend to be more helpful and are less likely to respond with\nharmful information. These results highlight the importance of targeted\nalignment strategies tailored to the unique challenges of software engineering\ntasks and provide a foundation for future work in this critical area.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "FSE'25 Technical Track",
    "pdf_url": "http://arxiv.org/pdf/2504.01850v1",
    "published_date": "2025-04-02 16:00:14 UTC",
    "updated_date": "2025-04-02 16:00:14 UTC"
  },
  {
    "arxiv_id": "2504.01849v1",
    "title": "An Approach to Technical AGI Safety and Security",
    "authors": [
      "Rohin Shah",
      "Alex Irpan",
      "Alexander Matt Turner",
      "Anna Wang",
      "Arthur Conmy",
      "David Lindner",
      "Jonah Brown-Cohen",
      "Lewis Ho",
      "Neel Nanda",
      "Raluca Ada Popa",
      "Rishub Jain",
      "Rory Greig",
      "Samuel Albanie",
      "Scott Emmons",
      "Sebastian Farquhar",
      "Sébastien Krier",
      "Senthooran Rajamanoharan",
      "Sophie Bridgers",
      "Tobi Ijitoye",
      "Tom Everitt",
      "Victoria Krakovna",
      "Vikrant Varma",
      "Vladimir Mikulik",
      "Zachary Kenton",
      "Dave Orr",
      "Shane Legg",
      "Noah Goodman",
      "Allan Dafoe",
      "Four Flynn",
      "Anca Dragan"
    ],
    "abstract": "Artificial General Intelligence (AGI) promises transformative benefits but\nalso presents significant risks. We develop an approach to address the risk of\nharms consequential enough to significantly harm humanity. We identify four\nareas of risk: misuse, misalignment, mistakes, and structural risks. Of these,\nwe focus on technical approaches to misuse and misalignment. For misuse, our\nstrategy aims to prevent threat actors from accessing dangerous capabilities,\nby proactively identifying dangerous capabilities, and implementing robust\nsecurity, access restrictions, monitoring, and model safety mitigations. To\naddress misalignment, we outline two lines of defense. First, model-level\nmitigations such as amplified oversight and robust training can help to build\nan aligned model. Second, system-level security measures such as monitoring and\naccess control can mitigate harm even if the model is misaligned. Techniques\nfrom interpretability, uncertainty estimation, and safer design patterns can\nenhance the effectiveness of these mitigations. Finally, we briefly outline how\nthese ingredients could be combined to produce safety cases for AGI systems.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01849v1",
    "published_date": "2025-04-02 15:59:31 UTC",
    "updated_date": "2025-04-02 15:59:31 UTC"
  },
  {
    "arxiv_id": "2504.01848v1",
    "title": "PaperBench: Evaluating AI's Ability to Replicate AI Research",
    "authors": [
      "Giulio Starace",
      "Oliver Jaffe",
      "Dane Sherburn",
      "James Aung",
      "Jun Shern Chan",
      "Leon Maksin",
      "Rachel Dias",
      "Evan Mays",
      "Benjamin Kinsella",
      "Wyatt Thompson",
      "Johannes Heidecke",
      "Amelia Glaese",
      "Tejal Patwardhan"
    ],
    "abstract": "We introduce PaperBench, a benchmark evaluating the ability of AI agents to\nreplicate state-of-the-art AI research. Agents must replicate 20 ICML 2024\nSpotlight and Oral papers from scratch, including understanding paper\ncontributions, developing a codebase, and successfully executing experiments.\nFor objective evaluation, we develop rubrics that hierarchically decompose each\nreplication task into smaller sub-tasks with clear grading criteria. In total,\nPaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed\nwith the author(s) of each ICML paper for accuracy and realism. To enable\nscalable evaluation, we also develop an LLM-based judge to automatically grade\nreplication attempts against rubrics, and assess our judge's performance by\ncreating a separate benchmark for judges. We evaluate several frontier models\non PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet\n(New) with open-source scaffolding, achieves an average replication score of\n21.0\\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench,\nfinding that models do not yet outperform the human baseline. We\n\\href{https://github.com/openai/preparedness}{open-source our code} to\nfacilitate future research in understanding the AI engineering capabilities of\nAI agents.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "30 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.01848v1",
    "published_date": "2025-04-02 15:55:24 UTC",
    "updated_date": "2025-04-02 15:55:24 UTC"
  },
  {
    "arxiv_id": "2504.01833v1",
    "title": "YourBench: Easy Custom Evaluation Sets for Everyone",
    "authors": [
      "Sumuk Shashidhar",
      "Clémentine Fourrier",
      "Alina Lozovskia",
      "Thomas Wolf",
      "Gokhan Tur",
      "Dilek Hakkani-Tür"
    ],
    "abstract": "Evaluating large language models (LLMs) effectively remains a critical\nbottleneck, as traditional static benchmarks suffer from saturation and\ncontamination, while human evaluations are costly and slow. This hinders timely\nor domain-specific assessment, crucial for real-world applications. We\nintroduce YourBench, a novel, open-source framework that addresses these\nlimitations by enabling dynamic, automated generation of reliable, up-to-date,\nand domain-tailored benchmarks cheaply and without manual annotation, directly\nfrom user-provided documents. We demonstrate its efficacy by replicating 7\ndiverse MMLU subsets using minimal source text, achieving this for under 15 USD\nin total inference costs while perfectly preserving the relative model\nperformance rankings (Spearman Rho = 1) observed on the original benchmark. To\nensure that YourBench generates data grounded in provided input instead of\nrelying on posterior parametric knowledge in models, we also introduce\nTempora-0325, a novel dataset of over 7K diverse documents, published\nexclusively after March 2025. Our comprehensive analysis spans 26 SoTA models\nfrom 7 major families across varying scales (3-671B parameters) to validate the\nquality of generated evaluations through rigorous algorithmic checks (e.g.,\ncitation grounding) and human assessments. We release the YourBench library,\nthe Tempora-0325 dataset, 150k+ question answer pairs based on Tempora and all\nevaluation and inference traces to facilitate reproducible research and empower\nthe community to generate bespoke benchmarks on demand, fostering more relevant\nand trustworthy LLM evaluation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.1"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01833v1",
    "published_date": "2025-04-02 15:40:24 UTC",
    "updated_date": "2025-04-02 15:40:24 UTC"
  },
  {
    "arxiv_id": "2504.01819v1",
    "title": "Implicit Bias Injection Attacks against Text-to-Image Diffusion Models",
    "authors": [
      "Huayang Huang",
      "Xiangye Jin",
      "Jiaxu Miao",
      "Yu Wu"
    ],
    "abstract": "The proliferation of text-to-image diffusion models (T2I DMs) has led to an\nincreased presence of AI-generated images in daily life. However, biased T2I\nmodels can generate content with specific tendencies, potentially influencing\npeople's perceptions. Intentional exploitation of these biases risks conveying\nmisleading information to the public. Current research on bias primarily\naddresses explicit biases with recognizable visual patterns, such as skin color\nand gender. This paper introduces a novel form of implicit bias that lacks\nexplicit visual features but can manifest in diverse ways across various\nsemantic contexts. This subtle and versatile nature makes this bias challenging\nto detect, easy to propagate, and adaptable to a wide range of scenarios. We\nfurther propose an implicit bias injection attack framework (IBI-Attacks)\nagainst T2I diffusion models by precomputing a general bias direction in the\nprompt embedding space and adaptively adjusting it based on different inputs.\nOur attack module can be seamlessly integrated into pre-trained diffusion\nmodels in a plug-and-play manner without direct manipulation of user input or\nmodel retraining. Extensive experiments validate the effectiveness of our\nscheme in introducing bias through subtle and diverse modifications while\npreserving the original semantics. The strong concealment and transferability\nof our attack across various scenarios further underscore the significance of\nour approach. Code is available at https://github.com/Hannah1102/IBI-attacks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accept to CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.01819v1",
    "published_date": "2025-04-02 15:24:12 UTC",
    "updated_date": "2025-04-02 15:24:12 UTC"
  },
  {
    "arxiv_id": "2504.01798v1",
    "title": "A Novel Approach To Implementing Knowledge Distillation In Tsetlin Machines",
    "authors": [
      "Calvin Kinateder"
    ],
    "abstract": "The Tsetlin Machine (TM) is a propositional logic based model that uses\nconjunctive clauses to learn patterns from data. As with typical neural\nnetworks, the performance of a Tsetlin Machine is largely dependent on its\nparameter count, with a larger number of parameters producing higher accuracy\nbut slower execution. Knowledge distillation in neural networks transfers\ninformation from an already-trained teacher model to a smaller student model to\nincrease accuracy in the student without increasing execution time. We propose\na novel approach to implementing knowledge distillation in Tsetlin Machines by\nutilizing the probability distributions of each output sample in the teacher to\nprovide additional context to the student. Additionally, we propose a novel\nclause-transfer algorithm that weighs the importance of each clause in the\nteacher and initializes the student with only the most essential data. We find\nthat our algorithm can significantly improve performance in the student model\nwithout negatively impacting latency in the tested domains of image recognition\nand text classification.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "Master's Thesis. 75 pages, 30 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.01798v1",
    "published_date": "2025-04-02 15:06:27 UTC",
    "updated_date": "2025-04-02 15:06:27 UTC"
  },
  {
    "arxiv_id": "2504.01797v1",
    "title": "Rethinking industrial artificial intelligence: a unified foundation framework",
    "authors": [
      "Jay Lee",
      "Hanqi Su"
    ],
    "abstract": "Recent advancement in industrial artificial intelligence (AI) is reshaping\nthe industry, driving smarter manufacturing, predictive maintenance, and\nintelligent decision-making. However, existing approaches often focus primarily\non algorithms and models, overlooking the importance of systematically\nintegrating domain knowledge, data, and models to ensure more comprehensive and\neffective AI solutions. Therefore, the effective development and deployment of\nIndustrial AI solutions require a more comprehensive and systematic approach.\nTo address this gap, this paper summarizes previous research and rethinks the\nrole of industrial AI and presents a unified industrial AI foundation framework\ncomprising three core modules: knowledge module, data module, and model module.\nThese modules help to extend and enhance the industrial AI methodology\nplatform, supporting various industrial applications. In addition, a case study\non rotating machinery diagnosis demonstrates the framework's effectiveness, and\nseveral future directions are highlighted for the development of the industrial\nAI foundation framework.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "The paper submitted to IJAMD, the International Journal of AI for\n  Materials and Design, has been accepted",
    "pdf_url": "http://arxiv.org/pdf/2504.01797v1",
    "published_date": "2025-04-02 15:05:32 UTC",
    "updated_date": "2025-04-02 15:05:32 UTC"
  },
  {
    "arxiv_id": "2504.01783v1",
    "title": "CLaP -- State Detection from Time Series",
    "authors": [
      "Arik Ermshaus",
      "Patrick Schäfer",
      "Ulf Leser"
    ],
    "abstract": "The ever-growing amount of sensor data from machines, smart devices, and the\nenvironment leads to an abundance of high-resolution, unannotated time series\n(TS). These recordings encode the recognizable properties of latent states and\ntransitions from physical phenomena that can be modelled as abstract processes.\nThe unsupervised localization and identification of these states and their\ntransitions is the task of time series state detection (TSSD). We introduce\nCLaP, a new, highly accurate and efficient algorithm for TSSD. It leverages the\npredictive power of time series classification for TSSD in an unsupervised\nsetting by applying novel self-supervision techniques to detect whether data\nsegments emerge from the same state or not. To this end, CLaP cross-validates a\nclassifier with segment-labelled subsequences to quantify confusion between\nsegments. It merges labels from segments with high confusion, representing the\nsame latent state, if this leads to an increase in overall classification\nquality. We conducted an experimental evaluation using 391 TS from four\nbenchmarks and found CLaP to be significantly more precise in detecting states\nthan five state-of-the-art competitors. It achieves the best accuracy-runtime\ntradeoff and is scalable to large TS. We provide a Python implementation of\nCLaP, which can be deployed in TS analysis workflows.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01783v1",
    "published_date": "2025-04-02 14:46:42 UTC",
    "updated_date": "2025-04-02 14:46:42 UTC"
  },
  {
    "arxiv_id": "2504.01771v1",
    "title": "Enhancing Interpretability in Generative AI Through Search-Based Data Influence Analysis",
    "authors": [
      "Theodoros Aivalis",
      "Iraklis A. Klampanos",
      "Antonis Troumpoukis",
      "Joemon M. Jose"
    ],
    "abstract": "Generative AI models offer powerful capabilities but often lack transparency,\nmaking it difficult to interpret their output. This is critical in cases\ninvolving artistic or copyrighted content. This work introduces a\nsearch-inspired approach to improve the interpretability of these models by\nanalysing the influence of training data on their outputs. Our method provides\nobservational interpretability by focusing on a model's output rather than on\nits internal state. We consider both raw data and latent-space embeddings when\nsearching for the influence of data items in generated content. We evaluate our\nmethod by retraining models locally and by demonstrating the method's ability\nto uncover influential subsets in the training data. This work lays the\ngroundwork for future extensions, including user-based evaluations with domain\nexperts, which is expected to improve observational interpretability further.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01771v1",
    "published_date": "2025-04-02 14:29:37 UTC",
    "updated_date": "2025-04-02 14:29:37 UTC"
  },
  {
    "arxiv_id": "2504.01767v1",
    "title": "Leveraging Embedding Techniques in Multimodal Machine Learning for Mental Illness Assessment",
    "authors": [
      "Abdelrahaman A. Hassan",
      "Abdelrahman A. Ali",
      "Aya E. Fouda",
      "Radwa J. Hanafy",
      "Mohammed E. Fouda"
    ],
    "abstract": "The increasing global prevalence of mental disorders, such as depression and\nPTSD, requires objective and scalable diagnostic tools. Traditional clinical\nassessments often face limitations in accessibility, objectivity, and\nconsistency. This paper investigates the potential of multimodal machine\nlearning to address these challenges, leveraging the complementary information\navailable in text, audio, and video data. Our approach involves a comprehensive\nanalysis of various data preprocessing techniques, including novel chunking and\nutterance-based formatting strategies. We systematically evaluate a range of\nstate-of-the-art embedding models for each modality and employ Convolutional\nNeural Networks (CNNs) and Bidirectional LSTM Networks (BiLSTMs) for feature\nextraction. We explore data-level, feature-level, and decision-level fusion\ntechniques, including a novel integration of Large Language Model (LLM)\npredictions. We also investigate the impact of replacing Multilayer Perceptron\nclassifiers with Support Vector Machines. We extend our analysis to severity\nprediction using PHQ-8 and PCL-C scores and multi-class classification\n(considering co-occurring conditions). Our results demonstrate that\nutterance-based chunking significantly improves performance, particularly for\ntext and audio modalities. Decision-level fusion, incorporating LLM\npredictions, achieves the highest accuracy, with a balanced accuracy of 94.8%\nfor depression and 96.2% for PTSD detection. The combination of CNN-BiLSTM\narchitectures with utterance-level chunking, coupled with the integration of\nexternal LLM, provides a powerful and nuanced approach to the detection and\nassessment of mental health conditions. Our findings highlight the potential of\nMMML for developing more accurate, accessible, and personalized mental\nhealthcare tools.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01767v1",
    "published_date": "2025-04-02 14:19:06 UTC",
    "updated_date": "2025-04-02 14:19:06 UTC"
  },
  {
    "arxiv_id": "2504.01764v1",
    "title": "Dual-stream Transformer-GCN Model with Contextualized Representations Learning for Monocular 3D Human Pose Estimation",
    "authors": [
      "Mingrui Ye",
      "Lianping Yang",
      "Hegui Zhu",
      "Zenghao Zheng",
      "Xin Wang",
      "Yantao Lo"
    ],
    "abstract": "This paper introduces a novel approach to monocular 3D human pose estimation\nusing contextualized representation learning with the Transformer-GCN\ndual-stream model. Monocular 3D human pose estimation is challenged by depth\nambiguity, limited 3D-labeled training data, imbalanced modeling, and\nrestricted model generalization. To address these limitations, our work\nintroduces a groundbreaking motion pre-training method based on contextualized\nrepresentation learning. Specifically, our method involves masking 2D pose\nfeatures and utilizing a Transformer-GCN dual-stream model to learn\nhigh-dimensional representations through a self-distillation setup. By focusing\non contextualized representation learning and spatial-temporal modeling, our\napproach enhances the model's ability to understand spatial-temporal\nrelationships between postures, resulting in superior generalization.\nFurthermore, leveraging the Transformer-GCN dual-stream model, our approach\neffectively balances global and local interactions in video pose estimation.\nThe model adaptively integrates information from both the Transformer and GCN\nstreams, where the GCN stream effectively learns local relationships between\nadjacent key points and frames, while the Transformer stream captures\ncomprehensive global spatial and temporal features. Our model achieves\nstate-of-the-art performance on two benchmark datasets, with an MPJPE of 38.0mm\nand P-MPJPE of 31.9mm on Human3.6M, and an MPJPE of 15.9mm on MPI-INF-3DHP.\nFurthermore, visual experiments on public datasets and in-the-wild videos\ndemonstrate the robustness and generalization capabilities of our approach.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01764v1",
    "published_date": "2025-04-02 14:17:57 UTC",
    "updated_date": "2025-04-02 14:17:57 UTC"
  },
  {
    "arxiv_id": "2504.01738v1",
    "title": "Style over Substance: Distilled Language Models Reason Via Stylistic Replication",
    "authors": [
      "Philip Lippmann",
      "Jie Yang"
    ],
    "abstract": "Specialized reasoning language models (RLMs) have demonstrated that scaling\ntest-time computation through detailed reasoning traces significantly enhances\nperformance. Although these traces effectively facilitate knowledge\ndistillation into smaller, instruction-tuned models, the precise nature of\ntransferred reasoning remains unclear. In this study, we investigate to what\nextent distilled models internalize replicated stylistic patterns during\nreasoning. To this end, we systematically analyze reasoning traces, identifying\nstructural and lexical patterns that characterize successful reasoning. We then\nintroduce two new datasets -- a dataset of emergent reasoning traces and a\nsynthetic dataset explicitly constructed to replicate these stylistic patterns\n-- to precisely examine their influence on distilled models' reasoning\ncapabilities. We find that models trained on the synthetic traces achieve\ncomparable performance, indicating that distilled reasoning abilities rely\nsignificantly on surface-level patterns. Surprisingly, we observe an increase\nin performance even when the synthetic traces are altered to lead to the wrong\nanswer. Our findings highlight how stylistic patterns can be leveraged to\nefficiently enhance LM reasoning across diverse model families.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01738v1",
    "published_date": "2025-04-02 13:50:20 UTC",
    "updated_date": "2025-04-02 13:50:20 UTC"
  },
  {
    "arxiv_id": "2504.01735v1",
    "title": "AdPO: Enhancing the Adversarial Robustness of Large Vision-Language Models with Preference Optimization",
    "authors": [
      "Chaohu Liu",
      "Tianyi Gui",
      "Yu Liu",
      "Linli Xu"
    ],
    "abstract": "Large Vision-Language Models (LVLMs), such as GPT-4o and LLaVA, have recently\nwitnessed remarkable advancements and are increasingly being deployed in\nreal-world applications. However, inheriting the sensitivity of visual neural\nnetworks, LVLMs remain vulnerable to adversarial attacks, which can result in\nerroneous or malicious outputs. While existing efforts utilize adversarial\nfine-tuning to enhance robustness, they often suffer from performance\ndegradation on clean inputs. In this paper, we proposes AdPO, a novel\nadversarial defense strategy for LVLMs based on preference optimization. For\nthe first time, we reframe adversarial training as a preference optimization\nproblem, aiming to enhance the model's preference for generating normal outputs\non clean inputs while rejecting the potential misleading outputs for\nadversarial examples. Notably, AdPO achieves this by solely modifying the image\nencoder, e.g., CLIP ViT, resulting in superior clean and adversarial\nperformance in a variety of downsream tasks. Considering that training involves\nlarge language models (LLMs), the computational cost increases significantly.\nWe validate that training on smaller LVLMs and subsequently transferring to\nlarger models can achieve competitive performance while maintaining efficiency\ncomparable to baseline methods. Our comprehensive experiments confirm the\neffectiveness of the proposed AdPO, which provides a novel perspective for\nfuture adversarial defense research.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01735v1",
    "published_date": "2025-04-02 13:43:21 UTC",
    "updated_date": "2025-04-02 13:43:21 UTC"
  },
  {
    "arxiv_id": "2504.01733v1",
    "title": "Epistemic Skills: Reasoning about Knowledge and Oblivion",
    "authors": [
      "Xiaolong Liang",
      "Yì N. Wáng"
    ],
    "abstract": "This paper presents a class of epistemic logics that captures the dynamics of\nacquiring knowledge and descending into oblivion, while incorporating concepts\nof group knowledge. The approach is grounded in a system of weighted models,\nintroducing an ``epistemic skills'' metric to represent the epistemic\ncapacities tied to knowledge updates. Within this framework, knowledge\nacquisition is modeled as a process of upskilling, whereas oblivion is\nrepresented as a consequence of downskilling. The framework further enables\nexploration of ``knowability'' and ``forgettability,'' defined as the potential\nto gain knowledge through upskilling and to lapse into oblivion through\ndownskilling, respectively. Additionally, it supports a detailed analysis of\nthe distinctions between epistemic de re and de dicto expressions. The\ncomputational complexity of the model checking and satisfiability problems is\nexamined, offering insights into their theoretical foundations and practical\nimplications.",
    "categories": [
      "cs.AI",
      "cs.CC",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01733v1",
    "published_date": "2025-04-02 13:41:42 UTC",
    "updated_date": "2025-04-02 13:41:42 UTC"
  },
  {
    "arxiv_id": "2504.01724v2",
    "title": "DreamActor-M1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance",
    "authors": [
      "Yuxuan Luo",
      "Zhengkun Rong",
      "Lizhen Wang",
      "Longhao Zhang",
      "Tianshu Hu",
      "Yongming Zhu"
    ],
    "abstract": "While recent image-based human animation methods achieve realistic body and\nfacial motion synthesis, critical gaps remain in fine-grained holistic\ncontrollability, multi-scale adaptability, and long-term temporal coherence,\nwhich leads to their lower expressiveness and robustness. We propose a\ndiffusion transformer (DiT) based framework, DreamActor-M1, with hybrid\nguidance to overcome these limitations. For motion guidance, our hybrid control\nsignals that integrate implicit facial representations, 3D head spheres, and 3D\nbody skeletons achieve robust control of facial expressions and body movements,\nwhile producing expressive and identity-preserving animations. For scale\nadaptation, to handle various body poses and image scales ranging from\nportraits to full-body views, we employ a progressive training strategy using\ndata with varying resolutions and scales. For appearance guidance, we integrate\nmotion patterns from sequential frames with complementary visual references,\nensuring long-term temporal coherence for unseen regions during complex\nmovements. Experiments demonstrate that our method outperforms the\nstate-of-the-art works, delivering expressive results for portraits,\nupper-body, and full-body generation with robust long-term consistency. Project\nPage: https://grisoon.github.io/DreamActor-M1/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01724v2",
    "published_date": "2025-04-02 13:30:32 UTC",
    "updated_date": "2025-04-03 14:51:10 UTC"
  },
  {
    "arxiv_id": "2504.01707v2",
    "title": "InfiniteICL: Breaking the Limit of Context Window Size via Long Short-term Memory Transformation",
    "authors": [
      "Bowen Cao",
      "Deng Cai",
      "Wai Lam"
    ],
    "abstract": "In-context learning (ICL) is critical for large language models (LLMs), but\nits effectiveness is constrained by finite context windows, particularly in\nultra-long contexts. To overcome this, we introduce InfiniteICL, a framework\nthat parallels context and parameters in LLMs with short- and long-term memory\nin human cognitive systems, focusing on transforming temporary context\nknowledge into permanent parameter updates. This approach significantly reduces\nmemory usage, maintains robust performance across varying input lengths, and\ntheoretically enables infinite context integration through the principles of\ncontext knowledge elicitation, selection, and consolidation. Evaluations\ndemonstrate that our method reduces context length by 90% while achieving 103%\naverage performance of full-context prompting across fact recall, grounded\nreasoning, and skill acquisition tasks. When conducting sequential multi-turn\ntransformations on complex, real-world contexts (with length up to 2M tokens),\nour approach surpasses full-context prompting while using only 0.4% of the\noriginal contexts. These findings highlight InfiniteICL's potential to enhance\nthe scalability and efficiency of LLMs by breaking the limitations of\nconventional context window sizes.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01707v2",
    "published_date": "2025-04-02 13:15:44 UTC",
    "updated_date": "2025-04-03 08:53:06 UTC"
  },
  {
    "arxiv_id": "2504.01705v1",
    "title": "Sky of Unlearning (SoUL): Rewiring Federated Machine Unlearning via Selective Pruning",
    "authors": [
      "Md Mahabub Uz Zaman",
      "Xiang Sun",
      "Jingjing Yao"
    ],
    "abstract": "The Internet of Drones (IoD), where drones collaborate in data collection and\nanalysis, has become essential for applications such as surveillance and\nenvironmental monitoring. Federated learning (FL) enables drones to train\nmachine learning models in a decentralized manner while preserving data\nprivacy. However, FL in IoD networks is susceptible to attacks like data\npoisoning and model inversion. Federated unlearning (FU) mitigates these risks\nby eliminating adversarial data contributions, preventing their influence on\nthe model. This paper proposes sky of unlearning (SoUL), a federated unlearning\nframework that efficiently removes the influence of unlearned data while\nmaintaining model performance. A selective pruning algorithm is designed to\nidentify and remove neurons influential in unlearning but minimally impact the\noverall performance of the model. Simulations demonstrate that SoUL outperforms\nexisting unlearning methods, achieves accuracy comparable to full retraining,\nand reduces computation and communication overhead, making it a scalable and\nefficient solution for resource-constrained IoD networks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 6 figures, IEEE International Conference on Communications\n  (ICC 2025)",
    "pdf_url": "http://arxiv.org/pdf/2504.01705v1",
    "published_date": "2025-04-02 13:07:30 UTC",
    "updated_date": "2025-04-02 13:07:30 UTC"
  },
  {
    "arxiv_id": "2504.01700v1",
    "title": "Reasoning LLMs for User-Aware Multimodal Conversational Agents",
    "authors": [
      "Hamed Rahimi",
      "Jeanne Cattoni",
      "Meriem Beghili",
      "Mouad Abrini",
      "Mahdi Khoramshahi",
      "Maribel Pino",
      "Mohamed Chetouani"
    ],
    "abstract": "Personalization in social robotics is critical for fostering effective\nhuman-robot interactions, yet systems often face the cold start problem, where\ninitial user preferences or characteristics are unavailable. This paper\nproposes a novel framework called USER-LLM R1 for a user-aware conversational\nagent that addresses this challenge through dynamic user profiling and model\ninitiation. Our approach integrates chain-of-thought (CoT) reasoning models to\niteratively infer user preferences and vision-language models (VLMs) to\ninitialize user profiles from multimodal inputs, enabling personalized\ninteractions from the first encounter. Leveraging a Retrieval-Augmented\nGeneration (RAG) architecture, the system dynamically refines user\nrepresentations within an inherent CoT process, ensuring contextually relevant\nand adaptive responses. Evaluations on the ElderlyTech-VQA Bench demonstrate\nsignificant improvements in ROUGE-1 (+23.2%), ROUGE-2 (+0.6%), and ROUGE-L\n(+8%) F1 scores over state-of-the-art baselines, with ablation studies\nunderscoring the impact of reasoning model size on performance. Human\nevaluations further validate the framework's efficacy, particularly for elderly\nusers, where tailored responses enhance engagement and trust. Ethical\nconsiderations, including privacy preservation and bias mitigation, are\nrigorously discussed and addressed to ensure responsible deployment.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01700v1",
    "published_date": "2025-04-02 13:00:17 UTC",
    "updated_date": "2025-04-02 13:00:17 UTC"
  },
  {
    "arxiv_id": "2504.01698v1",
    "title": "ToM-RL: Reinforcement Learning Unlocks Theory of Mind in Small LLMs",
    "authors": [
      "Yi-Long Lu",
      "Chunhui Zhang",
      "Jiajun Song",
      "Lifeng Fan",
      "Wei Wang"
    ],
    "abstract": "Recent advancements in rule-based reinforcement learning (RL), applied during\nthe post-training phase of large language models (LLMs), have significantly\nenhanced their capabilities in structured reasoning tasks such as mathematics\nand logical inference. However, the effectiveness of RL in social reasoning,\nparticularly in Theory of Mind (ToM), the ability to infer others' mental\nstates, remains largely unexplored. In this study, we demonstrate that RL\nmethods effectively unlock ToM reasoning capabilities even in small-scale LLMs\n(0.5B to 7B parameters). Using a modest dataset comprising 3200 questions\nacross diverse scenarios, our RL-trained 7B model achieves 84.50\\% accuracy on\nthe Hi-ToM benchmark, surpassing models like GPT-4o and DeepSeek-v3 despite\nsignificantly fewer parameters. While smaller models ($\\leq$3B parameters)\nsuffer from reasoning collapse, larger models (7B parameters) maintain stable\nperformance through consistent belief tracking. Additionally, our RL-based\nmodels demonstrate robust generalization to higher-order, out-of-distribution\nToM problems, novel textual presentations, and previously unseen datasets.\nThese findings highlight RL's potential to enhance social cognitive reasoning,\nbridging the gap between structured problem-solving and nuanced social\ninference in LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01698v1",
    "published_date": "2025-04-02 12:58:42 UTC",
    "updated_date": "2025-04-02 12:58:42 UTC"
  },
  {
    "arxiv_id": "2504.01692v1",
    "title": "Segmentation variability and radiomics stability for predicting Triple-Negative Breast Cancer subtype using Magnetic Resonance Imaging",
    "authors": [
      "Isabella Cama",
      "Alejandro Guzmán",
      "Cristina Campi",
      "Michele Piana",
      "Karim Lekadir",
      "Sara Garbarino",
      "Oliver Díaz"
    ],
    "abstract": "Most papers caution against using predictive models for disease\nstratification based on unselected radiomic features, as these features are\naffected by contouring variability. Instead, they advocate for the use of the\nIntraclass Correlation Coefficient (ICC) as a measure of stability for feature\nselection. However, the direct effect of segmentation variability on the\npredictive models is rarely studied. This study investigates the impact of\nsegmentation variability on feature stability and predictive performance in\nradiomics-based prediction of Triple-Negative Breast Cancer (TNBC) subtype\nusing Magnetic Resonance Imaging. A total of 244 images from the Duke dataset\nwere used, with segmentation variability introduced through modifications of\nmanual segmentations. For each mask, explainable radiomic features were\nselected using the Shapley Additive exPlanations method and used to train\nlogistic regression models. Feature stability across segmentations was assessed\nvia ICC, Pearson's correlation, and reliability scores quantifying the\nrelationship between feature stability and segmentation variability. Results\nindicate that segmentation accuracy does not significantly impact predictive\nperformance. While incorporating peritumoral information may reduce feature\nreproducibility, it does not diminish feature predictive capability. Moreover,\nfeature selection in predictive models is not inherently tied to feature\nstability with respect to segmentation, suggesting that an overreliance on ICC\nor reliability scores for feature selection might exclude valuable predictive\nfeatures.",
    "categories": [
      "stat.AP",
      "cs.AI",
      "62P10 (Primary), 68T09 (Secondary)"
    ],
    "primary_category": "stat.AP",
    "comment": "22 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.01692v1",
    "published_date": "2025-04-02 12:48:01 UTC",
    "updated_date": "2025-04-02 12:48:01 UTC"
  },
  {
    "arxiv_id": "2504.01690v1",
    "title": "Token Pruning in Audio Transformers: Optimizing Performance and Decoding Patch Importance",
    "authors": [
      "Taehan Lee",
      "Hyukjun Lee"
    ],
    "abstract": "Vision Transformers (ViTs) have achieved state-of-the-art performance across\nvarious computer vision tasks, but their high computational cost remains a\nchallenge. Token pruning has been proposed to reduce this cost by selectively\nremoving less important tokens. While effective in vision tasks by discarding\nnon-object regions, applying this technique to audio tasks presents unique\nchallenges, as distinguishing relevant from irrelevant regions in\ntime-frequency representations is less straightforward. In this study, for the\nfirst time, we applied token pruning to ViT-based audio classification models\nusing Mel-spectrograms and analyzed the trade-offs between model performance\nand computational cost: TopK token pruning can reduce MAC operations of\nAudioMAE and AST by 30-40%, with less than a 1% drop in classification\naccuracy. Our analysis reveals that while high-intensity tokens contribute\nsignificantly to model accuracy, low-intensity tokens remain important. In\nparticular, they play a more critical role in general audio classification\ntasks than in speech-specific tasks.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "This work has been submitted to the IEEE for possible publication.\n  Source code is available at\n  https://github.com/andylee-24/token-pruning-audio-transformer",
    "pdf_url": "http://arxiv.org/pdf/2504.01690v1",
    "published_date": "2025-04-02 12:44:38 UTC",
    "updated_date": "2025-04-02 12:44:38 UTC"
  },
  {
    "arxiv_id": "2504.01673v1",
    "title": "K-P Quantum Neural Networks",
    "authors": [
      "Elija Perrier"
    ],
    "abstract": "We present an extension of K-P time-optimal quantum control solutions using\nglobal Cartan $KAK$ decompositions for geodesic-based solutions. Extending\nrecent time-optimal \\emph{constant-$\\theta$} control results, we integrate\nCartan methods into equivariant quantum neural network (EQNN) for quantum\ncontrol tasks. We show that a finite-depth limited EQNN ansatz equipped with\nCartan layers can replicate the constant-$\\theta$ sub-Riemannian geodesics for\nK-P problems. We demonstrate how for certain classes of control problem on\nRiemannian symmetric spaces, gradient-based training using an appropriate cost\nfunction converges to certain global time-optimal solutions when satisfying\nsimple regularity conditions. This generalises prior geometric control theory\nmethods and clarifies how optimal geodesic estimation can be performed in\nquantum machine learning contexts.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2504.01673v1",
    "published_date": "2025-04-02 12:22:18 UTC",
    "updated_date": "2025-04-02 12:22:18 UTC"
  },
  {
    "arxiv_id": "2504.01671v1",
    "title": "Anomaly Detection for Hybrid Butterfly Subspecies via Probability Filtering",
    "authors": [
      "Bo-Kai Ruan",
      "Yi-Zeng Fang",
      "Hong-Han Shuai",
      "Juinn-Dar Huang"
    ],
    "abstract": "Detecting butterfly hybrids requires knowledge of the parent subspecies, and\nthe process can be tedious when encountering a new subspecies. This study\nfocuses on a specific scenario where a model trained to recognize hybrid\nspecies A can generalize to species B when B biologically mimics A. Since\nspecies A and B share similar patterns, we leverage BioCLIP as our feature\nextractor to capture features based on their taxonomy. Consequently, the\nalgorithm designed for species A can be transferred to B, as their hybrid and\nnon-hybrid patterns exhibit similar relationships. To determine whether a\nbutterfly is a hybrid, we adopt proposed probability filtering and color\njittering to augment and simulate the mimicry. With these approaches, we\nachieve second place in the official development phase. Our code is publicly\navailable at https://github.com/Justin900429/NSF-HDR-Challenge.",
    "categories": [
      "cs.CE",
      "cs.AI"
    ],
    "primary_category": "cs.CE",
    "comment": "AAAI'25 Workshop in Anomaly Detection in Scientific Domains",
    "pdf_url": "http://arxiv.org/pdf/2504.01671v1",
    "published_date": "2025-04-02 12:18:44 UTC",
    "updated_date": "2025-04-02 12:18:44 UTC"
  },
  {
    "arxiv_id": "2504.01652v1",
    "title": "Market-Oriented Flow Allocation for Thermal Solar Plants: An Auction-Based Methodology with Artificial Intelligence",
    "authors": [
      "Sara Ruiz-Moreno",
      "Antonio J. Gallego",
      "Manuel Macías",
      "Eduardo F. Camacho"
    ],
    "abstract": "This paper presents a novel method to optimize thermal balance in parabolic\ntrough collector (PTC) plants. It uses a market-based system to distribute flow\namong loops combined with an artificial neural network (ANN) to reduce\ncomputation and data requirements. This auction-based approach balances loop\ntemperatures, accommodating varying thermal losses and collector efficiencies.\nValidation across different thermal losses, optical efficiencies, and\nirradiance conditions-sunny, partially cloudy, and cloudy-show improved thermal\npower output and intercept factors compared to a no-allocation system. It\ndemonstrates scalability and practicality for large solar thermal plants,\nenhancing overall performance. The method was first validated through\nsimulations on a realistic solar plant model, then adapted and successfully\ntested in a 50 MW solar trough plant, demonstrating its advantages.\nFurthermore, the algorithms have been implemented, commissioned, and are\ncurrently operating in 13 commercial solar trough plants.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "This manuscript has been submitted to Renewable Energy",
    "pdf_url": "http://arxiv.org/pdf/2504.01652v1",
    "published_date": "2025-04-02 12:01:41 UTC",
    "updated_date": "2025-04-02 12:01:41 UTC"
  },
  {
    "arxiv_id": "2504.01644v1",
    "title": "Proposition of Affordance-Driven Environment Recognition Framework Using Symbol Networks in Large Language Models",
    "authors": [
      "Kazuma Arii",
      "Satoshi Kurihara"
    ],
    "abstract": "In the quest to enable robots to coexist with humans, understanding dynamic\nsituations and selecting appropriate actions based on common sense and\naffordances are essential. Conventional AI systems face challenges in applying\naffordance, as it represents implicit knowledge derived from common sense.\nHowever, large language models (LLMs) offer new opportunities due to their\nability to process extensive human knowledge. This study proposes a method for\nautomatic affordance acquisition by leveraging LLM outputs. The process\ninvolves generating text using LLMs, reconstructing the output into a symbol\nnetwork using morphological and dependency analysis, and calculating\naffordances based on network distances. Experiments using ``apple'' as an\nexample demonstrated the method's ability to extract context-dependent\naffordances with high explainability. The results suggest that the proposed\nsymbol network, reconstructed from LLM outputs, enables robots to interpret\naffordances effectively, bridging the gap between symbolized data and\nhuman-like situational understanding.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01644v1",
    "published_date": "2025-04-02 11:48:44 UTC",
    "updated_date": "2025-04-02 11:48:44 UTC"
  },
  {
    "arxiv_id": "2504.01641v1",
    "title": "Bridge 2D-3D: Uncertainty-aware Hierarchical Registration Network with Domain Alignment",
    "authors": [
      "Zhixin Cheng",
      "Jiacheng Deng",
      "Xinjun Li",
      "Baoqun Yin",
      "Tianzhu Zhang"
    ],
    "abstract": "The method for image-to-point cloud registration typically determines the\nrigid transformation using a coarse-to-fine pipeline. However, directly and\nuniformly matching image patches with point cloud patches may lead to focusing\non incorrect noise patches during matching while ignoring key ones. Moreover,\ndue to the significant differences between image and point cloud modalities, it\nmay be challenging to bridge the domain gap without specific improvements in\ndesign. To address the above issues, we innovatively propose the\nUncertainty-aware Hierarchical Matching Module (UHMM) and the Adversarial Modal\nAlignment Module (AMAM). Within the UHMM, we model the uncertainty of critical\ninformation in image patches and facilitate multi-level fusion interactions\nbetween image and point cloud features. In the AMAM, we design an adversarial\napproach to reduce the domain gap between image and point cloud. Extensive\nexperiments and ablation studies on RGB-D Scene V2 and 7-Scenes benchmarks\ndemonstrate the superiority of our method, making it a state-of-the-art\napproach for image-to-point cloud registration tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "AAAI2025accept",
    "pdf_url": "http://arxiv.org/pdf/2504.01641v1",
    "published_date": "2025-04-02 11:43:55 UTC",
    "updated_date": "2025-04-02 11:43:55 UTC"
  },
  {
    "arxiv_id": "2504.01637v1",
    "title": "LLM-mediated Dynamic Plan Generation with a Multi-Agent Approach",
    "authors": [
      "Reo Abe",
      "Akifumi Ito",
      "Kanata Takayasu",
      "Satoshi Kurihara"
    ],
    "abstract": "Planning methods with high adaptability to dynamic environments are crucial\nfor the development of autonomous and versatile robots. We propose a method for\nleveraging a large language model (GPT-4o) to automatically generate networks\ncapable of adapting to dynamic environments. The proposed method collects\nenvironmental \"status,\" representing conditions and goals, and uses them to\ngenerate agents. These agents are interconnected on the basis of specific\nconditions, resulting in networks that combine flexibility and generality. We\nconducted evaluation experiments to compare the networks automatically\ngenerated with the proposed method with manually constructed ones, confirming\nthe comprehensiveness of the proposed method's networks and their higher\ngenerality. This research marks a significant advancement toward the\ndevelopment of versatile planning methods applicable to robotics, autonomous\nvehicles, smart systems, and other complex environments.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01637v1",
    "published_date": "2025-04-02 11:42:49 UTC",
    "updated_date": "2025-04-02 11:42:49 UTC"
  },
  {
    "arxiv_id": "2504.01632v1",
    "title": "Benchmarking the Spatial Robustness of DNNs via Natural and Adversarial Localized Corruptions",
    "authors": [
      "Giulia Marchiori Pietrosanti",
      "Giulio Rossolini",
      "Alessandro Biondi",
      "Giorgio Buttazzo"
    ],
    "abstract": "The robustness of DNNs is a crucial factor in safety-critical applications,\nparticularly in complex and dynamic environments where localized corruptions\ncan arise. While previous studies have evaluated the robustness of semantic\nsegmentation (SS) models under whole-image natural or adversarial corruptions,\na comprehensive investigation into the spatial robustness of dense vision\nmodels under localized corruptions remained underexplored. This paper fills\nthis gap by introducing specialized metrics for benchmarking the spatial\nrobustness of segmentation models, alongside with an evaluation framework to\nassess the impact of localized corruptions. Furthermore, we uncover the\ninherent complexity of characterizing worst-case robustness using a single\nlocalized adversarial perturbation. To address this, we propose region-aware\nmulti-attack adversarial analysis, a method that enables a deeper understanding\nof model robustness against adversarial perturbations applied to specific\nregions. The proposed metrics and analysis were evaluated on 15 segmentation\nmodels in driving scenarios, uncovering key insights into the effects of\nlocalized corruption in both natural and adversarial forms. The results reveal\nthat models respond to these two types of threats differently; for instance,\ntransformer-based segmentation models demonstrate notable robustness to\nlocalized natural corruptions but are highly vulnerable to adversarial ones and\nvice-versa for CNN-based models. Consequently, we also address the challenge of\nbalancing robustness to both natural and adversarial localized corruptions by\nmeans of ensemble models, thereby achieving a broader threat coverage and\nimproved reliability for dense vision tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2504.01632v1",
    "published_date": "2025-04-02 11:37:39 UTC",
    "updated_date": "2025-04-02 11:37:39 UTC"
  },
  {
    "arxiv_id": "2504.01627v1",
    "title": "Horizon Scans can be accelerated using novel information retrieval and artificial intelligence tools",
    "authors": [
      "Lena Schmidt",
      "Oshin Sharma",
      "Chris Marshall",
      "Sonia Garcia Gonzalez Moral"
    ],
    "abstract": "Introduction: Horizon scanning in healthcare assesses early signals of\ninnovation, crucial for timely adoption. Current horizon scanning faces\nchallenges in efficient information retrieval and analysis, especially from\nunstructured sources like news, presenting a need for innovative tools.\nMethodology: The study introduces SCANAR and AIDOC, open-source Python-based\ntools designed to improve horizon scanning. SCANAR automates the retrieval and\nprocessing of news articles, offering functionalities such as de-duplication\nand unsupervised relevancy ranking. AIDOC aids filtration by leveraging AI to\nreorder textual data based on relevancy, employing neural networks for semantic\nsimilarity, and subsequently prioritizing likely relevant entries for human\nreview. Results: Twelve internal datasets from horizon scans and four external\nbenchmarking datasets were used. SCANAR improved retrieval efficiency by\nautomating processes previously dependent on manual labour. AIDOC displayed\nwork-saving potential, achieving around 62% reduction in manual review efforts\nat 95% recall. Comparative analysis with benchmarking data showed AIDOC's\nperformance was similar to existing systematic review automation tools, though\nperformance varied depending on dataset characteristics. A smaller case-study\non our news datasets shows the potential of ensembling large language models\nwithin the active-learning process for faster detection of relevant articles\nacross news datasets. Conclusion: The validation indicates that SCANAR and\nAIDOC show potential to enhance horizon scanning efficiency by streamlining\ndata retrieval and prioritisation. These tools may alleviate methodological\nlimitations and allow broader, swifter horizon scans. Further studies are\nsuggested to optimize these models and to design new workflows and validation\nprocesses that integrate large language models.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01627v1",
    "published_date": "2025-04-02 11:33:08 UTC",
    "updated_date": "2025-04-02 11:33:08 UTC"
  },
  {
    "arxiv_id": "2504.01589v1",
    "title": "Text Speaks Louder than Vision: ASCII Art Reveals Textual Biases in Vision-Language Models",
    "authors": [
      "Zhaochen Wang",
      "Yujun Cai",
      "Zi Huang",
      "Bryan Hooi",
      "Yiwei Wang",
      "Ming-Hsuan Yang"
    ],
    "abstract": "Vision-language models (VLMs) have advanced rapidly in processing multimodal\ninformation, but their ability to reconcile conflicting signals across\nmodalities remains underexplored. This work investigates how VLMs process ASCII\nart, a unique medium where textual elements collectively form visual patterns,\npotentially creating semantic-visual conflicts. We introduce a novel evaluation\nframework that systematically challenges five state-of-the-art models\n(including GPT-4o, Claude, and Gemini) using adversarial ASCII art, where\ncharacter-level semantics deliberately contradict global visual patterns. Our\nexperiments reveal a strong text-priority bias: VLMs consistently prioritize\ntextual information over visual patterns, with visual recognition ability\ndeclining dramatically as semantic complexity increases. Various mitigation\nattempts through visual parameter tuning and prompt engineering yielded only\nmodest improvements, suggesting that this limitation requires\narchitectural-level solutions. These findings uncover fundamental flaws in how\ncurrent VLMs integrate multimodal information, providing important guidance for\nfuture model development while highlighting significant implications for\ncontent moderation systems vulnerable to adversarial examples.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Under review at COLM 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.01589v1",
    "published_date": "2025-04-02 10:47:07 UTC",
    "updated_date": "2025-04-02 10:47:07 UTC"
  },
  {
    "arxiv_id": "2504.01588v1",
    "title": "Building Knowledge from Interactions: An LLM-Based Architecture for Adaptive Tutoring and Social Reasoning",
    "authors": [
      "Luca Garello",
      "Giulia Belgiovine",
      "Gabriele Russo",
      "Francesco Rea",
      "Alessandra Sciutti"
    ],
    "abstract": "Integrating robotics into everyday scenarios like tutoring or physical\ntraining requires robots capable of adaptive, socially engaging, and\ngoal-oriented interactions. While Large Language Models show promise in\nhuman-like communication, their standalone use is hindered by memory\nconstraints and contextual incoherence. This work presents a multimodal,\ncognitively inspired framework that enhances LLM-based autonomous\ndecision-making in social and task-oriented Human-Robot Interaction.\nSpecifically, we develop an LLM-based agent for a robot trainer, balancing\nsocial conversation with task guidance and goal-driven motivation. To further\nenhance autonomy and personalization, we introduce a memory system for\nselecting, storing and retrieving experiences, facilitating generalized\nreasoning based on knowledge built across different interactions. A preliminary\nHRI user study and offline experiments with a synthetic dataset validate our\napproach, demonstrating the system's ability to manage complex interactions,\nautonomously drive training tasks, and build and retrieve contextual memories,\nadvancing socially intelligent robotics.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Submitted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.01588v1",
    "published_date": "2025-04-02 10:45:41 UTC",
    "updated_date": "2025-04-02 10:45:41 UTC"
  },
  {
    "arxiv_id": "2504.01571v1",
    "title": "Pro-DG: Procedural Diffusion Guidance for Architectural Facade Generation",
    "authors": [
      "Aleksander Plocharski",
      "Jan Swidzinski",
      "Przemyslaw Musialski"
    ],
    "abstract": "We present Pro-DG, a framework for procedurally controllable photo-realistic\nfacade generation that combines a procedural shape grammar with diffusion-based\nimage synthesis. Starting from a single input image, we reconstruct its facade\nlayout using grammar rules, then edit that structure through user-defined\ntransformations. As facades are inherently multi-hierarchical structures, we\nintroduce hierarchical matching procedure that aligns facade structures at\ndifferent levels which is used to introduce control maps to guide a generative\ndiffusion pipeline. This approach retains local appearance fidelity while\naccommodating large-scale edits such as floor duplication or window\nrearrangement. We provide a thorough evaluation, comparing Pro-DG against\ninpainting-based baselines and synthetic ground truths. Our user study and\nquantitative measurements indicate improved preservation of architectural\nidentity and higher edit accuracy. Our novel method is the first to integrate\nneuro-symbolically derived shape-grammars for modeling with modern generative\nmodel and highlights the broader potential of such approaches for precise and\ncontrollable image manipulation.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "I.3.7; I.4.9; I.2.10"
    ],
    "primary_category": "cs.GR",
    "comment": "12 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.01571v1",
    "published_date": "2025-04-02 10:16:19 UTC",
    "updated_date": "2025-04-02 10:16:19 UTC"
  },
  {
    "arxiv_id": "2504.01560v1",
    "title": "Optimizing Package Delivery with Quantum Annealers: Addressing Time-Windows and Simultaneous Pickup and Delivery",
    "authors": [
      "Eneko Osaba",
      "Esther Villar-Rodriguez",
      "Pablo Miranda-Rodriguez",
      "Antón Asla"
    ],
    "abstract": "Recent research at the intersection of quantum computing and routing problems\nhas been highly prolific. Much of this work focuses on classical problems such\nas the Traveling Salesman Problem and the Vehicle Routing Problem. The\npractical applicability of these problems depends on the specific objectives\nand constraints considered. However, it is undeniable that translating complex\nreal-world requirements into these classical formulations often proves\nchallenging. In this paper, we resort to our previously published\nquantum-classical technique for addressing real-world-oriented routing\nproblems, known as Quantum for Real Package Delivery (Q4RPD), and elaborate on\nsolving additional realistic problem instances. Accordingly, this paper\nemphasizes the following characteristics: i) simultaneous pickup and\ndeliveries, ii) time-windows, and iii) mobility restrictions by vehicle type.\nTo illustrate the application of Q4RPD, we have conducted an experimentation\ncomprising seven instances, serving as a demonstration of the newly developed\nfeatures.",
    "categories": [
      "cs.ET",
      "cs.AI"
    ],
    "primary_category": "cs.ET",
    "comment": "8 pages, 1 table, 9 figures, paper submitted to the IEEE\n  International Conference on Quantum Computing and Engineering (QCE 2025)",
    "pdf_url": "http://arxiv.org/pdf/2504.01560v1",
    "published_date": "2025-04-02 10:01:34 UTC",
    "updated_date": "2025-04-02 10:01:34 UTC"
  },
  {
    "arxiv_id": "2504.01551v1",
    "title": "Identifying Macro Causal Effects in C-DMGs",
    "authors": [
      "Simon Ferreira",
      "Charles K. Assaad"
    ],
    "abstract": "Causal effect identification using causal graphs is a fundamental challenge\nin causal inference. While extensive research has been conducted in this area,\nmost existing methods assume the availability of fully specified causal graphs.\nHowever, in complex domains such as medicine and epidemiology, complete causal\nknowledge is often unavailable, and only partial information about the system\nis accessible. This paper focuses on causal effect identification within\npartially specified causal graphs, with particular emphasis on cluster-directed\nmixed graphs (C-DMGs). These graphs provide a higher-level representation of\ncausal relationships by grouping variables into clusters, offering a more\npractical approach for handling complex systems. Unlike fully specified causal\ngraphs, C-DMGs can contain cycles, which complicate their analysis and\ninterpretation. Furthermore, their cluster-based nature introduces new\nchallenges, as it gives rise to two distinct types of causal effects, macro\ncausal effects and micro causal effects, with different properties. In this\nwork, we focus on macro causal effects, which describe the effects of entire\nclusters on other clusters. We establish that the do-calculus is both sound and\ncomplete for identifying these effects in C-DMGs. Additionally, we provide a\ngraphical characterization of non-identifiability for macro causal effects in\nthese graphs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01551v1",
    "published_date": "2025-04-02 09:48:27 UTC",
    "updated_date": "2025-04-02 09:48:27 UTC"
  },
  {
    "arxiv_id": "2504.01541v1",
    "title": "Hyperbolic Diffusion Recommender Model",
    "authors": [
      "Meng Yuan",
      "Yutian Xiao",
      "Wei Chen",
      "Chu Zhao",
      "Deqing Wang",
      "Fuzhen Zhuang"
    ],
    "abstract": "Diffusion models (DMs) have emerged as the new state-of-the-art family of\ndeep generative models. To gain deeper insights into the limitations of\ndiffusion models in recommender systems, we investigate the fundamental\nstructural disparities between images and items. Consequently, items often\nexhibit distinct anisotropic and directional structures that are less prevalent\nin images. However, the traditional forward diffusion process continuously adds\nisotropic Gaussian noise, causing anisotropic signals to degrade into noise,\nwhich impairs the semantically meaningful representations in recommender\nsystems.\n  Inspired by the advancements in hyperbolic spaces, we propose a novel\n\\textit{\\textbf{H}yperbolic} \\textit{\\textbf{D}iffusion}\n\\textit{\\textbf{R}ecommender} \\textit{\\textbf{M}odel} (named HDRM). Unlike\nexisting directional diffusion methods based on Euclidean space, the intrinsic\nnon-Euclidean structure of hyperbolic space makes it particularly well-adapted\nfor handling anisotropic diffusion processes. In particular, we begin by\nformulating concepts to characterize latent directed diffusion processes within\na geometrically grounded hyperbolic space. Subsequently, we propose a novel\nhyperbolic latent diffusion process specifically tailored for users and items.\nDrawing upon the natural geometric attributes of hyperbolic spaces, we impose\nstructural restrictions on the space to enhance hyperbolic diffusion\npropagation, thereby ensuring the preservation of the intrinsic topology of\nuser-item graphs. Extensive experiments on three benchmark datasets demonstrate\nthe effectiveness of HDRM.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01541v1",
    "published_date": "2025-04-02 09:27:40 UTC",
    "updated_date": "2025-04-02 09:27:40 UTC"
  },
  {
    "arxiv_id": "2504.01538v1",
    "title": "AI-Newton: A Concept-Driven Physical Law Discovery System without Prior Physical Knowledge",
    "authors": [
      "You-Le Fang",
      "Dong-Shan Jian",
      "Xiang Li",
      "Yan-Qing Ma"
    ],
    "abstract": "Current limitations in human scientific discovery necessitate a new research\nparadigm. While advances in artificial intelligence (AI) offer a highly\npromising solution, enabling AI to emulate human-like scientific discovery\nremains an open challenge. To address this, we propose AI-Newton, a\nconcept-driven discovery system capable of autonomously deriving physical laws\nfrom raw data -- without supervision or prior physical knowledge. The system\nintegrates a knowledge base and knowledge representation centered on physical\nconcepts, along with an autonomous discovery workflow. As a proof of concept,\nwe apply AI-Newton to a large set of Newtonian mechanics problems. Given\nexperimental data with noise, the system successfully rediscovers fundamental\nlaws, including Newton's second law, energy conservation and law of\ngravitation, using autonomously defined concepts. This achievement marks a\nsignificant step toward AI-driven autonomous scientific discovery.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SC",
      "hep-ph",
      "physics.class-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "31 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.01538v1",
    "published_date": "2025-04-02 09:25:34 UTC",
    "updated_date": "2025-04-02 09:25:34 UTC"
  },
  {
    "arxiv_id": "2504.01522v1",
    "title": "Redefining technology for indigenous languages",
    "authors": [
      "Silvia Fernandez-Sabido",
      "Laura Peniche-Sabido"
    ],
    "abstract": "In this paper, we offer an overview of indigenous languages, identifying the\ncauses of their devaluation and the need for legislation on language rights. We\nreview the technologies used to revitalize these languages, finding that when\nthey come from outside, they often have the opposite effect to what they seek;\nhowever, when developed from within communities, they become powerful\ninstruments of expression. We propose that the inclusion of Indigenous\nknowledge in large language models (LLMs) will enrich the technological\nlandscape, but must be done in a participatory environment that encourages the\nexchange of knowledge.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "in Spanish language",
    "pdf_url": "http://arxiv.org/pdf/2504.01522v1",
    "published_date": "2025-04-02 09:08:53 UTC",
    "updated_date": "2025-04-02 09:08:53 UTC"
  },
  {
    "arxiv_id": "2504.01521v1",
    "title": "Domain Guidance: A Simple Transfer Approach for a Pre-trained Diffusion Model",
    "authors": [
      "Jincheng Zhong",
      "Xiangcheng Zhang",
      "Jianmin Wang",
      "Mingsheng Long"
    ],
    "abstract": "Recent advancements in diffusion models have revolutionized generative\nmodeling. However, the impressive and vivid outputs they produce often come at\nthe cost of significant model scaling and increased computational demands.\nConsequently, building personalized diffusion models based on off-the-shelf\nmodels has emerged as an appealing alternative. In this paper, we introduce a\nnovel perspective on conditional generation for transferring a pre-trained\nmodel. From this viewpoint, we propose *Domain Guidance*, a straightforward\ntransfer approach that leverages pre-trained knowledge to guide the sampling\nprocess toward the target domain. Domain Guidance shares a formulation similar\nto advanced classifier-free guidance, facilitating better domain alignment and\nhigher-quality generations. We provide both empirical and theoretical analyses\nof the mechanisms behind Domain Guidance. Our experimental results demonstrate\nits substantial effectiveness across various transfer benchmarks, achieving\nover a 19.6% improvement in FID and a 23.4% improvement in FD$_\\text{DINOv2}$\ncompared to standard fine-tuning. Notably, existing fine-tuned models can\nseamlessly integrate Domain Guidance to leverage these benefits, without\nadditional training.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01521v1",
    "published_date": "2025-04-02 09:07:55 UTC",
    "updated_date": "2025-04-02 09:07:55 UTC"
  },
  {
    "arxiv_id": "2504.01515v2",
    "title": "Training-free Dense-Aligned Diffusion Guidance for Modular Conditional Image Synthesis",
    "authors": [
      "Zixuan Wang",
      "Duo Peng",
      "Feng Chen",
      "Yuwei Yang",
      "Yinjie Lei"
    ],
    "abstract": "Conditional image synthesis is a crucial task with broad applications, such\nas artistic creation and virtual reality. However, current generative methods\nare often task-oriented with a narrow scope, handling a restricted condition\nwith constrained applicability. In this paper, we propose a novel approach that\ntreats conditional image synthesis as the modular combination of diverse\nfundamental condition units. Specifically, we divide conditions into three\nprimary units: text, layout, and drag. To enable effective control over these\nconditions, we design a dedicated alignment module for each. For the text\ncondition, we introduce a Dense Concept Alignment (DCA) module, which achieves\ndense visual-text alignment by drawing on diverse textual concepts. For the\nlayout condition, we propose a Dense Geometry Alignment (DGA) module to enforce\ncomprehensive geometric constraints that preserve the spatial configuration.\nFor the drag condition, we introduce a Dense Motion Alignment (DMA) module to\napply multi-level motion regularization, ensuring that each pixel follows its\ndesired trajectory without visual artifacts. By flexibly inserting and\ncombining these alignment modules, our framework enhances the model's\nadaptability to diverse conditional generation tasks and greatly expands its\napplication range. Extensive experiments demonstrate the superior performance\nof our framework across a variety of conditions, including textual description,\nsegmentation mask (bounding box), drag manipulation, and their combinations.\nCode is available at https://github.com/ZixuanWang0525/DADG.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR2025",
    "pdf_url": "http://arxiv.org/pdf/2504.01515v2",
    "published_date": "2025-04-02 09:00:28 UTC",
    "updated_date": "2025-04-03 08:39:13 UTC"
  },
  {
    "arxiv_id": "2504.01468v1",
    "title": "HH-PIM: Dynamic Optimization of Power and Performance with Heterogeneous-Hybrid PIM for Edge AI Devices",
    "authors": [
      "Sangmin Jeon",
      "Kangju Lee",
      "Kyeongwon Lee",
      "Woojoo Lee"
    ],
    "abstract": "Processing-in-Memory (PIM) architectures offer promising solutions for\nefficiently handling AI applications in energy-constrained edge environments.\nWhile traditional PIM designs enhance performance and energy efficiency by\nreducing data movement between memory and processing units, they are limited in\nedge devices due to continuous power demands and the storage requirements of\nlarge neural network weights in SRAM and DRAM. Hybrid PIM architectures,\nincorporating non-volatile memories like MRAM and ReRAM, mitigate these\nlimitations but struggle with a mismatch between fixed computing resources and\ndynamically changing inference workloads. To address these challenges, this\nstudy introduces a Heterogeneous-Hybrid PIM (HH-PIM) architecture, comprising\nhigh-performance MRAM-SRAM PIM modules and low-power MRAM-SRAM PIM modules. We\nfurther propose a data placement optimization algorithm that dynamically\nallocates data based on computational demand, maximizing energy efficiency.\nFPGA prototyping and power simulations with processors featuring HH-PIM and\nother PIM types demonstrate that the proposed HH-PIM achieves up to $60.43$\npercent average energy savings over conventional PIMs while meeting application\nlatency requirements. These results confirm the suitability of HH-PIM for\nadaptive, energy-efficient AI processing in edge devices.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "7 pages, 6 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.01468v1",
    "published_date": "2025-04-02 08:22:32 UTC",
    "updated_date": "2025-04-02 08:22:32 UTC"
  },
  {
    "arxiv_id": "2504.01459v1",
    "title": "Probabilistic Curriculum Learning for Goal-Based Reinforcement Learning",
    "authors": [
      "Llewyn Salt",
      "Marcus Gallagher"
    ],
    "abstract": "Reinforcement learning (RL) -- algorithms that teach artificial agents to\ninteract with environments by maximising reward signals -- has achieved\nsignificant success in recent years. These successes have been facilitated by\nadvances in algorithms (e.g., deep Q-learning, deep deterministic policy\ngradients, proximal policy optimisation, trust region policy optimisation, and\nsoft actor-critic) and specialised computational resources such as GPUs and\nTPUs. One promising research direction involves introducing goals to allow\nmultimodal policies, commonly through hierarchical or curriculum reinforcement\nlearning. These methods systematically decompose complex behaviours into\nsimpler sub-tasks, analogous to how humans progressively learn skills (e.g. we\nlearn to run before we walk, or we learn arithmetic before calculus). However,\nfully automating goal creation remains an open challenge. We present a novel\nprobabilistic curriculum learning algorithm to suggest goals for reinforcement\nlearning agents in continuous control and navigation tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01459v1",
    "published_date": "2025-04-02 08:15:16 UTC",
    "updated_date": "2025-04-02 08:15:16 UTC"
  },
  {
    "arxiv_id": "2504.01452v1",
    "title": "BiSeg-SAM: Weakly-Supervised Post-Processing Framework for Boosting Binary Segmentation in Segment Anything Models",
    "authors": [
      "Encheng Su",
      "Hu Cao",
      "Alois Knoll"
    ],
    "abstract": "Accurate segmentation of polyps and skin lesions is essential for diagnosing\ncolorectal and skin cancers. While various segmentation methods for polyps and\nskin lesions using fully supervised deep learning techniques have been\ndeveloped, the pixel-level annotation of medical images by doctors is both\ntime-consuming and costly. Foundational vision models like the Segment Anything\nModel (SAM) have demonstrated superior performance; however, directly applying\nSAM to medical segmentation may not yield satisfactory results due to the lack\nof domain-specific medical knowledge. In this paper, we propose BiSeg-SAM, a\nSAM-guided weakly supervised prompting and boundary refinement network for the\nsegmentation of polyps and skin lesions. Specifically, we fine-tune SAM\ncombined with a CNN module to learn local features. We introduce a WeakBox with\ntwo functions: automatically generating box prompts for the SAM model and using\nour proposed Multi-choice Mask-to-Box (MM2B) transformation for rough\nmask-to-box conversion, addressing the mismatch between coarse labels and\nprecise predictions. Additionally, we apply scale consistency (SC) loss for\nprediction scale alignment. Our DetailRefine module enhances boundary precision\nand segmentation accuracy by refining coarse predictions using a limited amount\nof ground truth labels. This comprehensive approach enables BiSeg-SAM to\nachieve excellent multi-task segmentation performance. Our method demonstrates\nsignificant superiority over state-of-the-art (SOTA) methods when tested on\nfive polyp datasets and one skin cancer dataset.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "2024 IEEE International Conference on Bioinformatics and Biomedicine\n  (BIBM)",
    "pdf_url": "http://arxiv.org/pdf/2504.01452v1",
    "published_date": "2025-04-02 08:04:37 UTC",
    "updated_date": "2025-04-02 08:04:37 UTC"
  },
  {
    "arxiv_id": "2504.01445v1",
    "title": "Enabling Systematic Generalization in Abstract Spatial Reasoning through Meta-Learning for Compositionality",
    "authors": [
      "Philipp Mondorf",
      "Shijia Zhou",
      "Monica Riedler",
      "Barbara Plank"
    ],
    "abstract": "Systematic generalization refers to the capacity to understand and generate\nnovel combinations from known components. Despite recent progress by large\nlanguage models (LLMs) across various domains, these models often fail to\nextend their knowledge to novel compositional scenarios, revealing notable\nlimitations in systematic generalization. There has been an ongoing debate\nabout whether neural networks possess the capacity for systematic\ngeneralization, with recent studies suggesting that meta-learning approaches\ndesigned for compositionality can significantly enhance this ability. However,\nthese insights have largely been confined to linguistic problems, leaving their\napplicability to other tasks an open question. In this study, we extend the\napproach of meta-learning for compositionality to the domain of abstract\nspatial reasoning. To this end, we introduce $\\textit{SYGAR}$-a dataset\ndesigned to evaluate the capacity of models to systematically generalize from\nknown geometric transformations (e.g., translation, rotation) of\ntwo-dimensional objects to novel combinations of these transformations (e.g.,\ntranslation+rotation). Our results show that a transformer-based\nencoder-decoder model, trained via meta-learning for compositionality, can\nsystematically generalize to previously unseen transformation compositions,\nsignificantly outperforming state-of-the-art LLMs, including o3-mini, GPT-4o,\nand Gemini 2.0 Flash, which fail to exhibit similar systematic behavior. Our\nfindings highlight the effectiveness of meta-learning in promoting\nsystematicity beyond linguistic tasks, suggesting a promising direction toward\nmore robust and generalizable models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "30 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.01445v1",
    "published_date": "2025-04-02 07:56:39 UTC",
    "updated_date": "2025-04-02 07:56:39 UTC"
  },
  {
    "arxiv_id": "2504.01444v1",
    "title": "PiCo: Jailbreaking Multimodal Large Language Models via $\\textbf{Pi}$ctorial $\\textbf{Co}$de Contextualization",
    "authors": [
      "Aofan Liu",
      "Lulu Tang",
      "Ting Pan",
      "Yuguo Yin",
      "Bin Wang",
      "Ao Yang"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs), which integrate vision and other\nmodalities into Large Language Models (LLMs), significantly enhance AI\ncapabilities but also introduce new security vulnerabilities. By exploiting the\nvulnerabilities of the visual modality and the long-tail distribution\ncharacteristic of code training data, we present PiCo, a novel jailbreaking\nframework designed to progressively bypass multi-tiered defense mechanisms in\nadvanced MLLMs. PiCo employs a tier-by-tier jailbreak strategy, using\ntoken-level typographic attacks to evade input filtering and embedding harmful\nintent within programming context instructions to bypass runtime monitoring. To\ncomprehensively assess the impact of attacks, a new evaluation metric is\nfurther proposed to assess both the toxicity and helpfulness of model outputs\npost-attack. By embedding harmful intent within code-style visual instructions,\nPiCo achieves an average Attack Success Rate (ASR) of 84.13% on Gemini-Pro\nVision and 52.66% on GPT-4, surpassing previous methods. Experimental results\nhighlight the critical gaps in current defenses, underscoring the need for more\nrobust strategies to secure advanced MLLMs.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01444v1",
    "published_date": "2025-04-02 07:54:32 UTC",
    "updated_date": "2025-04-02 07:54:32 UTC"
  },
  {
    "arxiv_id": "2504.01429v1",
    "title": "Refining Interactions: Enhancing Anisotropy in Graph Neural Networks with Language Semantics",
    "authors": [
      "Zhaoxing Li",
      "Xiaoming Zhang",
      "Haifeng Zhang",
      "Chengxiang Liu"
    ],
    "abstract": "The integration of Large Language Models (LLMs) with Graph Neural Networks\n(GNNs) has recently been explored to enhance the capabilities of Text Attribute\nGraphs (TAGs). Most existing methods feed textual descriptions of the graph\nstructure or neighbouring nodes' text directly into LLMs. However, these\napproaches often cause LLMs to treat structural information simply as general\ncontextual text, thus limiting their effectiveness in graph-related tasks. In\nthis paper, we introduce LanSAGNN (Language Semantic Anisotropic Graph Neural\nNetwork), a framework that extends the concept of anisotropic GNNs to the\nnatural language level. This model leverages LLMs to extract tailor-made\nsemantic information for node pairs, effectively capturing the unique\ninteractions within node relationships. In addition, we propose an efficient\ndual-layer LLMs finetuning architecture to better align LLMs' outputs with\ngraph tasks. Experimental results demonstrate that LanSAGNN significantly\nenhances existing LLM-based methods without increasing complexity while also\nexhibiting strong robustness against interference.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ICME 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.01429v1",
    "published_date": "2025-04-02 07:32:45 UTC",
    "updated_date": "2025-04-02 07:32:45 UTC"
  },
  {
    "arxiv_id": "2504.01428v1",
    "title": "MuTri: Multi-view Tri-alignment for OCT to OCTA 3D Image Translation",
    "authors": [
      "Zhuangzhuang Chen",
      "Hualiang Wang",
      "Chubin Ou",
      "Xiaomeng Li"
    ],
    "abstract": "Optical coherence tomography angiography (OCTA) shows its great importance in\nimaging microvascular networks by providing accurate 3D imaging of blood\nvessels, but it relies upon specialized sensors and expensive devices. For this\nreason, previous works show the potential to translate the readily available 3D\nOptical Coherence Tomography (OCT) images into 3D OCTA images. However,\nexisting OCTA translation methods directly learn the mapping from the OCT\ndomain to the OCTA domain in continuous and infinite space with guidance from\nonly a single view, i.e., the OCTA project map, resulting in suboptimal\nresults. To this end, we propose the multi-view Tri-alignment framework for OCT\nto OCTA 3D image translation in discrete and finite space, named MuTri. In the\nfirst stage, we pre-train two vector-quantized variational auto-encoder (VQ-\nVAE) by reconstructing 3D OCT and 3D OCTA data, providing semantic prior for\nsubsequent multi-view guidances. In the second stage, our multi-view\ntri-alignment facilitates another VQVAE model to learn the mapping from the OCT\ndomain to the OCTA domain in discrete and finite space. Specifically, a\ncontrastive-inspired semantic alignment is proposed to maximize the mutual\ninformation with the pre-trained models from OCT and OCTA views, to facilitate\ncodebook learning. Meanwhile, a vessel structure alignment is proposed to\nminimize the structure discrepancy with the pre-trained models from the OCTA\nproject map view, benefiting from learning the detailed vessel structure\ninformation. We also collect the first large-scale dataset, namely, OCTA2024,\nwhich contains a pair of OCT and OCTA volumes from 846 subjects.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01428v1",
    "published_date": "2025-04-02 07:28:09 UTC",
    "updated_date": "2025-04-02 07:28:09 UTC"
  },
  {
    "arxiv_id": "2504.01420v1",
    "title": "FAIRE: Assessing Racial and Gender Bias in AI-Driven Resume Evaluations",
    "authors": [
      "Athena Wen",
      "Tanush Patil",
      "Ansh Saxena",
      "Yicheng Fu",
      "Sean O'Brien",
      "Kevin Zhu"
    ],
    "abstract": "In an era where AI-driven hiring is transforming recruitment practices,\nconcerns about fairness and bias have become increasingly important. To explore\nthese issues, we introduce a benchmark, FAIRE (Fairness Assessment In Resume\nEvaluation), to test for racial and gender bias in large language models (LLMs)\nused to evaluate resumes across different industries. We use two methods-direct\nscoring and ranking-to measure how model performance changes when resumes are\nslightly altered to reflect different racial or gender identities. Our findings\nreveal that while every model exhibits some degree of bias, the magnitude and\ndirection vary considerably. This benchmark provides a clear way to examine\nthese differences and offers valuable insights into the fairness of AI-based\nhiring tools. It highlights the urgent need for strategies to reduce bias in\nAI-driven recruitment. Our benchmark code and dataset are open-sourced at our\nrepository:\nhttps://github.com/athenawen/FAIRE-Fairness-Assessment-In-Resume-Evaluation.git.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01420v1",
    "published_date": "2025-04-02 07:11:30 UTC",
    "updated_date": "2025-04-02 07:11:30 UTC"
  },
  {
    "arxiv_id": "2504.01407v1",
    "title": "TimeSearch: Hierarchical Video Search with Spotlight and Reflection for Human-like Long Video Understanding",
    "authors": [
      "Junwen Pan",
      "Rui Zhang",
      "Xin Wan",
      "Yuan Zhang",
      "Ming Lu",
      "Qi She"
    ],
    "abstract": "Large video-language models (LVLMs) have shown remarkable performance across\nvarious video-language tasks. However, they encounter significant challenges\nwhen processing long videos because of the large number of video frames\ninvolved. Downsampling long videos in either space or time can lead to visual\nhallucinations, making it difficult to accurately interpret long videos.\nMotivated by human hierarchical temporal search strategies, we propose\n\\textbf{TimeSearch}, a novel framework enabling LVLMs to understand long videos\nin a human-like manner. TimeSearch integrates two human-like primitives into a\nunified autoregressive LVLM: 1) \\textbf{Spotlight} efficiently identifies\nrelevant temporal events through a Temporal-Augmented Frame Representation\n(TAFR), explicitly binding visual features with timestamps; 2)\n\\textbf{Reflection} evaluates the correctness of the identified events,\nleveraging the inherent temporal self-reflection capabilities of LVLMs.\nTimeSearch progressively explores key events and prioritizes temporal search\nbased on reflection confidence. Extensive experiments on challenging long-video\nbenchmarks confirm that TimeSearch substantially surpasses previous\nstate-of-the-art, improving the accuracy from 41.8\\% to 51.5\\% on the LVBench.\nAdditionally, experiments on temporal grounding demonstrate that appropriate\nTAFR is adequate to effectively stimulate the surprising temporal grounding\nability of LVLMs in a simpler yet versatile manner, which improves mIoU on\nCharades-STA by 11.8\\%. The code will be released.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01407v1",
    "published_date": "2025-04-02 06:47:19 UTC",
    "updated_date": "2025-04-02 06:47:19 UTC"
  },
  {
    "arxiv_id": "2504.01403v1",
    "title": "Generative Retrieval and Alignment Model: A New Paradigm for E-commerce Retrieval",
    "authors": [
      "Ming Pang",
      "Chunyuan Yuan",
      "Xiaoyu He",
      "Zheng Fang",
      "Donghao Xie",
      "Fanyi Qu",
      "Xue Jiang",
      "Changping Peng",
      "Zhangang Lin",
      "Zheng Luo",
      "Jingping Shao"
    ],
    "abstract": "Traditional sparse and dense retrieval methods struggle to leverage general\nworld knowledge and often fail to capture the nuanced features of queries and\nproducts. With the advent of large language models (LLMs), industrial search\nsystems have started to employ LLMs to generate identifiers for product\nretrieval. Commonly used identifiers include (1) static/semantic IDs and (2)\nproduct term sets. The first approach requires creating a product ID system\nfrom scratch, missing out on the world knowledge embedded within LLMs. While\nthe second approach leverages this general knowledge, the significant\ndifference in word distribution between queries and products means that\nproduct-based identifiers often do not align well with user search queries,\nleading to missed product recalls. Furthermore, when queries contain numerous\nattributes, these algorithms generate a large number of identifiers, making it\ndifficult to assess their quality, which results in low overall recall\nefficiency.\n  To address these challenges, this paper introduces a novel e-commerce\nretrieval paradigm: the Generative Retrieval and Alignment Model (GRAM). GRAM\nemploys joint training on text information from both queries and products to\ngenerate shared text identifier codes, effectively bridging the gap between\nqueries and products. This approach not only enhances the connection between\nqueries and products but also improves inference efficiency. The model uses a\nco-alignment strategy to generate codes optimized for maximizing retrieval\nefficiency. Additionally, it introduces a query-product scoring mechanism to\ncompare product values across different codes, further boosting retrieval\nefficiency. Extensive offline and online A/B testing demonstrates that GRAM\nsignificantly outperforms traditional models and the latest generative\nretrieval models, confirming its effectiveness and practicality.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted by WWW2025",
    "pdf_url": "http://arxiv.org/pdf/2504.01403v1",
    "published_date": "2025-04-02 06:40:09 UTC",
    "updated_date": "2025-04-02 06:40:09 UTC"
  },
  {
    "arxiv_id": "2504.01400v1",
    "title": "ToolACE-R: Tool Learning with Adaptive Self-Refinement",
    "authors": [
      "Xingshan Zeng",
      "Weiwen Liu",
      "Xu Huang",
      "Zezhong Wang",
      "Lingzhi Wang",
      "Liangyou Li",
      "Yasheng Wang",
      "Lifeng Shang",
      "Xin Jiang",
      "Ruiming Tang",
      "Qun Liu"
    ],
    "abstract": "Tool learning, which allows Large Language Models (LLMs) to leverage external\ntools for solving complex user tasks, has emerged as a promising avenue for\nextending model capabilities. However, current approaches primarily focus on\ndata synthesis for fine-tuning LLMs to invoke tools effectively, largely\nignoring how to fully stimulate the potential of the model. In this paper, we\npropose ToolACE-R, a novel method that introduces adaptive self-refinement for\ntool invocations. Our approach features a model-aware iterative training\nprocedure that progressively incorporates more training samples based on the\nmodel's evolving capabilities. Additionally, it allows LLMs to iteratively\nrefine their tool calls, optimizing performance without requiring external\nfeedback. To further enhance computational efficiency, we integrate an adaptive\nmechanism when scaling the inference time, enabling the model to autonomously\ndetermine when to stop the refinement process. We conduct extensive experiments\nacross several benchmark datasets, showing that ToolACE-R achieves competitive\nperformance compared to advanced API-based models, even without any refinement.\nFurthermore, its performance can be further improved efficiently through\nadaptive self-refinement. Our results demonstrate the effectiveness of the\nproposed method, which is compatible with base models of various sizes,\noffering a promising direction for more efficient tool learning.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01400v1",
    "published_date": "2025-04-02 06:38:56 UTC",
    "updated_date": "2025-04-02 06:38:56 UTC"
  },
  {
    "arxiv_id": "2504.01395v1",
    "title": "From Easy to Hard: Building a Shortcut for Differentially Private Image Synthesis",
    "authors": [
      "Kecen Li",
      "Chen Gong",
      "Xiaochen Li",
      "Yuzhong Zhao",
      "Xinwen Hou",
      "Tianhao Wang"
    ],
    "abstract": "Differentially private (DP) image synthesis aims to generate synthetic images\nfrom a sensitive dataset, alleviating the privacy leakage concerns of\norganizations sharing and utilizing synthetic images. Although previous methods\nhave significantly progressed, especially in training diffusion models on\nsensitive images with DP Stochastic Gradient Descent (DP-SGD), they still\nsuffer from unsatisfactory performance. In this work, inspired by curriculum\nlearning, we propose a two-stage DP image synthesis framework, where diffusion\nmodels learn to generate DP synthetic images from easy to hard. Unlike existing\nmethods that directly use DP-SGD to train diffusion models, we propose an easy\nstage in the beginning, where diffusion models learn simple features of the\nsensitive images. To facilitate this easy stage, we propose to use `central\nimages', simply aggregations of random samples of the sensitive dataset.\nIntuitively, although those central images do not show details, they\ndemonstrate useful characteristics of all images and only incur minimal privacy\ncosts, thus helping early-phase model training. We conduct experiments to\npresent that on the average of four investigated image datasets, the fidelity\nand utility metrics of our synthetic images are 33.1% and 2.1% better than the\nstate-of-the-art method.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted at IEEE S&P (Oakland) 2025; code available at\n  https://github.com/SunnierLee/DP-FETA",
    "pdf_url": "http://arxiv.org/pdf/2504.01395v1",
    "published_date": "2025-04-02 06:30:55 UTC",
    "updated_date": "2025-04-02 06:30:55 UTC"
  },
  {
    "arxiv_id": "2504.01382v1",
    "title": "An Illusion of Progress? Assessing the Current State of Web Agents",
    "authors": [
      "Tianci Xue",
      "Weijian Qi",
      "Tianneng Shi",
      "Chan Hee Song",
      "Boyu Gou",
      "Dawn Song",
      "Huan Sun",
      "Yu Su"
    ],
    "abstract": "As digitalization and cloud technologies evolve, the web is becoming\nincreasingly important in the modern society. Autonomous web agents based on\nlarge language models (LLMs) hold a great potential in work automation. It is\ntherefore important to accurately measure and monitor the progression of their\ncapabilities. In this work, we conduct a comprehensive and rigorous assessment\nof the current state of web agents. Our results depict a very different picture\nof the competency of current agents, suggesting over-optimism in previously\nreported results. This gap can be attributed to shortcomings in existing\nbenchmarks. We introduce Online-Mind2Web, an online evaluation benchmark\nconsisting of 300 diverse and realistic tasks spanning 136 websites. It enables\nus to evaluate web agents under a setting that approximates how real users use\nthese agents. To facilitate more scalable evaluation and development, we also\ndevelop a novel LLM-as-a-Judge automatic evaluation method and show that it can\nachieve around 85% agreement with human judgment, substantially higher than\nexisting methods. Finally, we present the first comprehensive comparative\nanalysis of current web agents, highlighting both their strengths and\nlimitations to inspire future research.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "22 pages, 16 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.01382v1",
    "published_date": "2025-04-02 05:51:29 UTC",
    "updated_date": "2025-04-02 05:51:29 UTC"
  },
  {
    "arxiv_id": "2504.01366v1",
    "title": "Virtual Reality and Artificial Intelligence as Psychological Countermeasures in Space and Other Isolated and Confined Environments: A Scoping Review",
    "authors": [
      "Jennifer Sharp",
      "Joshua Kelson",
      "Daryl South",
      "Anthony Saliba",
      "Muhammad Ashad Kabir"
    ],
    "abstract": "Spaceflight is an isolated and confined environment (ICE) that exposes\nastronauts to psychological hazards, such as stress, danger, and monotony.\nVirtual reality (VR) and artificial intelligence (AI) technologies can serve as\npsychological countermeasures as they can digitally simulate immersive\nenvironments, interactive companions, and therapeutic experiences. Our study\nemploys a scoping literature review approach to identify what is currently\nknown about the use and effectiveness of VR and AI-based interventions as\npsychological countermeasures to improve mood or emotional states in adults in\nspace or other ICEs. Additionally, this review aimed to identify gaps in the\nknowledge base and whether a systematic review with meta-analysis was\nwarranted. The review included studies where the intervention was used or\nintended for use in space or other extraterrestrial environments (ICE). Our\nsearch strategy yielded 19 studies from 3390 records across seven major\ndatabases. All studies focused on VR-based interventions, with no eligible\nAI-based intervention studies found. VR interventions were found to be\neffective for relaxation and improving mood, emergency training, as an\ninteractive communication platform, for comparing interior designs, and for\nenhancing exercise. There were improvements for measures of mood and emotion\\n\n(e.g., anxiety and stress); however, user preferences varied, and some\ninstances of cybersickness were reported. A systematic review with\nmeta-analysis is not recommended due to the heterogeneity of results. There is\nsignificant scope for further research into the use of VR for a wider range of\nmood and emotion variables using standardised assessment instruments.\nAdditionally, the potential application of AI as a psychological countermeasure\nwarrants further investigation.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.HC",
    "comment": "34 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.01366v1",
    "published_date": "2025-04-02 05:25:29 UTC",
    "updated_date": "2025-04-02 05:25:29 UTC"
  },
  {
    "arxiv_id": "2504.01337v1",
    "title": "Advancing MoE Efficiency: A Collaboration-Constrained Routing (C2R) Strategy for Better Expert Parallelism Design",
    "authors": [
      "Mohan Zhang",
      "Pingzhi Li",
      "Jie Peng",
      "Mufan Qiu",
      "Tianlong Chen"
    ],
    "abstract": "Mixture-of-Experts (MoE) has successfully scaled up models while maintaining\nnearly constant computing costs. By employing a gating network to route input\ntokens, it selectively activates a subset of expert networks to process the\ncorresponding token embeddings. However, in practice, the efficiency of MoE is\nchallenging to achieve due to two key reasons: imbalanced expert activation,\nwhich leads to substantial idle time during model or expert parallelism, and\ninsufficient capacity utilization; massive communication overhead, induced by\nnumerous expert routing combinations in expert parallelism at the system level.\nPrevious works typically formulate it as the load imbalance issue characterized\nby the gating network favoring certain experts over others or attribute it to\nstatic execution which fails to adapt to the dynamic expert workload at\nruntime. In this paper, we exploit it from a brand new perspective, a\nhigher-order view and analysis of MoE routing policies: expert collaboration\nand specialization where some experts tend to activate broadly with others\n(collaborative), while others are more likely to activate only with a specific\nsubset of experts (specialized). Our experiments reveal that most experts tend\nto be overly collaborative, leading to increased communication overhead from\nrepeatedly sending tokens to different accelerators. To this end, we propose a\nnovel collaboration-constrained routing (C2R) strategy to encourage more\nspecialized expert groups, as well as to improve expert utilization, and\npresent an efficient implementation of MoE that further leverages expert\nspecialization. We achieve an average performance improvement of 0.51% and\n0.33% on LLaMA-MoE and Qwen-MoE respectively across ten downstream NLP\nbenchmarks, and reduce the all2all communication costs between GPUs, bringing\nan extra 20%-30% total running time savings on top of the existing SoTA, i.e.\nMegaBlocks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.01337v1",
    "published_date": "2025-04-02 03:51:59 UTC",
    "updated_date": "2025-04-02 03:51:59 UTC"
  },
  {
    "arxiv_id": "2504.01331v1",
    "title": "An Explainable Reconfiguration-Based Optimization Algorithm for Industrial and Reliability-Redundancy Allocation Problems",
    "authors": [
      "Dikshit Chauhan",
      "Nitin Gupta",
      "Anupam Yadav"
    ],
    "abstract": "Industrial and reliability optimization problems often involve complex\nconstraints and require efficient, interpretable solutions. This paper presents\nAI-AEFA, an advanced parameter reconfiguration-based metaheuristic algorithm\ndesigned to address large-scale industrial and reliability-redundancy\nallocation problems. AI-AEFA enhances search space exploration and convergence\nefficiency through a novel log-sigmoid-based parameter adaptation and chaotic\nmapping mechanism. The algorithm is validated across twenty-eight IEEE CEC 2017\nconstrained benchmark problems, fifteen large-scale industrial optimization\nproblems, and seven reliability-redundancy allocation problems, consistently\noutperforming state-of-the-art optimization techniques in terms of feasibility,\ncomputational efficiency, and convergence speed. The additional key\ncontribution of this work is the integration of SHAP (Shapley Additive\nExplanations) to enhance the interpretability of AI-AEFA, providing insights\ninto the impact of key parameters such as Coulomb's constant, charge,\nacceleration, and electrostatic force. This explainability feature enables a\ndeeper understanding of decision-making within the AI-AEFA framework during the\noptimization processes. The findings confirm AI-AEFA as a robust, scalable, and\ninterpretable optimization tool with significant real-world applications.",
    "categories": [
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "38 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.01331v1",
    "published_date": "2025-04-02 03:33:48 UTC",
    "updated_date": "2025-04-02 03:33:48 UTC"
  },
  {
    "arxiv_id": "2504.01326v1",
    "title": "CFMD: Dynamic Cross-layer Feature Fusion for Salient Object Detection",
    "authors": [
      "Jin Lian",
      "Zhongyu Wan",
      "Ming Gao",
      "JunFeng Chen"
    ],
    "abstract": "Cross-layer feature pyramid networks (CFPNs) have achieved notable progress\nin multi-scale feature fusion and boundary detail preservation for salient\nobject detection. However, traditional CFPNs still suffer from two core\nlimitations: (1) a computational bottleneck caused by complex feature weighting\noperations, and (2) degraded boundary accuracy due to feature blurring in the\nupsampling process. To address these challenges, we propose CFMD, a novel\ncross-layer feature pyramid network that introduces two key innovations. First,\nwe design a context-aware feature aggregation module (CFLMA), which\nincorporates the state-of-the-art Mamba architecture to construct a dynamic\nweight distribution mechanism. This module adaptively adjusts feature\nimportance based on image context, significantly improving both representation\nefficiency and generalization. Second, we introduce an adaptive dynamic\nupsampling unit (CFLMD) that preserves spatial details during resolution\nrecovery. By adjusting the upsampling range dynamically and initializing with a\nbilinear strategy, the module effectively reduces feature overlap and maintains\nfine-grained boundary structures. Extensive experiments on three standard\nbenchmarks using three mainstream backbone networks demonstrate that CFMD\nachieves substantial improvements in pixel-level accuracy and boundary\nsegmentation quality, especially in complex scenes. The results validate the\neffectiveness of CFMD in jointly enhancing computational efficiency and\nsegmentation performance, highlighting its strong potential in salient object\ndetection tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01326v1",
    "published_date": "2025-04-02 03:22:36 UTC",
    "updated_date": "2025-04-02 03:22:36 UTC"
  },
  {
    "arxiv_id": "2504.01324v1",
    "title": "On Data Synthesis and Post-training for Visual Abstract Reasoning",
    "authors": [
      "Ke Zhu",
      "Yu Wang",
      "Jiangjiang Liu",
      "Qunyi Xie",
      "Shanshan Liu",
      "Gang Zhang"
    ],
    "abstract": "This paper is a pioneering work attempting to address abstract visual\nreasoning (AVR) problems for large vision-language models (VLMs). We make a\ncommon LLaVA-NeXT 7B model capable of perceiving and reasoning about specific\nAVR problems, surpassing both open-sourced (e.g., Qwen-2-VL-72B) and\nclosed-sourced powerful VLMs (e.g., GPT-4o) with significant margin. This is a\ngreat breakthrough since almost all previous VLMs fail or show nearly random\nperformance on representative AVR benchmarks. Our key success is our innovative\ndata synthesis and post-training process, aiming to fully relieve the task\ndifficulty and elicit the model to learn, step by step. Our 7B model is also\nshown to be behave well on AVR without sacrificing common multimodal\ncomprehension abilities. We hope our paper could serve as an early effort in\nthis area and would inspire further research in abstract visual reasoning.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01324v1",
    "published_date": "2025-04-02 03:18:24 UTC",
    "updated_date": "2025-04-02 03:18:24 UTC"
  },
  {
    "arxiv_id": "2504.01321v1",
    "title": "COST: Contrastive One-Stage Transformer for Vision-Language Small Object Tracking",
    "authors": [
      "Chunhui Zhang",
      "Li Liu",
      "Jialin Gao",
      "Xin Sun",
      "Hao Wen",
      "Xi Zhou",
      "Shiming Ge",
      "Yanfeng Wang"
    ],
    "abstract": "Transformer has recently demonstrated great potential in improving\nvision-language (VL) tracking algorithms. However, most of the existing VL\ntrackers rely on carefully designed mechanisms to perform the multi-stage\nmulti-modal fusion. Additionally, direct multi-modal fusion without alignment\nignores distribution discrepancy between modalities in feature space,\npotentially leading to suboptimal representations. In this work, we propose\nCOST, a contrastive one-stage transformer fusion framework for VL tracking,\naiming to learn semantically consistent and unified VL representations.\nSpecifically, we introduce a contrastive alignment strategy that maximizes\nmutual information (MI) between a video and its corresponding language\ndescription. This enables effective cross-modal alignment, yielding\nsemantically consistent features in the representation space. By leveraging a\nvisual-linguistic transformer, we establish an efficient multi-modal fusion and\nreasoning mechanism, empirically demonstrating that a simple stack of\ntransformer encoders effectively enables unified VL representations. Moreover,\nwe contribute a newly collected VL tracking benchmark dataset for small object\ntracking, named VL-SOT500, with bounding boxes and language descriptions. Our\ndataset comprises two challenging subsets, VL-SOT230 and VL-SOT270, dedicated\nto evaluating generic and high-speed small object tracking, respectively. Small\nobject tracking is notoriously challenging due to weak appearance and limited\nfeatures, and this dataset is, to the best of our knowledge, the first to\nexplore the usage of language cues to enhance visual representation for small\nobject tracking. Extensive experiments demonstrate that COST achieves\nstate-of-the-art performance on five existing VL tracking datasets, as well as\non our proposed VL-SOT500 dataset. Source codes and dataset will be made\npublicly available.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint submitted to Elsevier.\n  https://github.com/983632847/Awesome-Multimodal-Object-Tracking",
    "pdf_url": "http://arxiv.org/pdf/2504.01321v1",
    "published_date": "2025-04-02 03:12:38 UTC",
    "updated_date": "2025-04-02 03:12:38 UTC"
  },
  {
    "arxiv_id": "2504.01317v1",
    "title": "Adaptive Rectification Sampling for Test-Time Compute Scaling",
    "authors": [
      "Zhendong Tan",
      "Xingjun Zhang",
      "Chaoyi Hu",
      "Yancheng Pan",
      "Shaoxun Wang"
    ],
    "abstract": "The newly released OpenAI-o1 and DeepSeek-R1 have demonstrated that test-time\nscaling can significantly improve model performance, especially in complex\ntasks such as logical reasoning. Common test-time scaling methods involve\ngenerating more chain of thoughts (CoTs) or longer CoTs with self-correction.\nHowever, while self-correction can improve performance, it may lead to\nsignificant token waste and reduce readability of the CoT if the reasoning\nsteps are already correct. To demonstrate that large language models (LLMs) can\nrectify errors at a more fine-grained level, we propose Adaptive Rectification\nSampling (AR-Sampling), which can guide the LLMs to self-correction at the\nappropriate step. AR-Sampling leverages a process-supervised reward model (PRM)\nas a verifier and constructed trigger sentences to guide the model in adaptive\nstep-level rethinking. Through the experiments on GSM8K and MATH500, it\nindicate that our approach enables the models to rethink in more fine-grained\nlevel, improving the accuracy of solutions, while generating a reasonable\nnumber of additional tokens.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01317v1",
    "published_date": "2025-04-02 02:57:52 UTC",
    "updated_date": "2025-04-02 02:57:52 UTC"
  },
  {
    "arxiv_id": "2504.01309v1",
    "title": "Biomedical Question Answering via Multi-Level Summarization on a Local Knowledge Graph",
    "authors": [
      "Lingxiao Guan",
      "Yuanhao Huang",
      "Jie Liu"
    ],
    "abstract": "In Question Answering (QA), Retrieval Augmented Generation (RAG) has\nrevolutionized performance in various domains. However, how to effectively\ncapture multi-document relationships, particularly critical for biomedical\ntasks, remains an open question. In this work, we propose a novel method that\nutilizes propositional claims to construct a local knowledge graph from\nretrieved documents. Summaries are then derived via layerwise summarization\nfrom the knowledge graph to contextualize a small language model to perform QA.\nWe achieved comparable or superior performance with our method over RAG\nbaselines on several biomedical QA benchmarks. We also evaluated each\nindividual step of our methodology over a targeted set of metrics,\ndemonstrating its effectiveness.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01309v1",
    "published_date": "2025-04-02 02:40:19 UTC",
    "updated_date": "2025-04-02 02:40:19 UTC"
  },
  {
    "arxiv_id": "2504.01281v2",
    "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding",
    "authors": [
      "Sakhinana Sagar Srinivas",
      "Venkataramana Runkana"
    ],
    "abstract": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01281v2",
    "published_date": "2025-04-02 01:16:10 UTC",
    "updated_date": "2025-04-03 01:23:22 UTC"
  },
  {
    "arxiv_id": "2504.01278v1",
    "title": "Strategize Globally, Adapt Locally: A Multi-Turn Red Teaming Agent with Dual-Level Learning",
    "authors": [
      "Si Chen",
      "Xiao Yu",
      "Ninareh Mehrabi",
      "Rahul Gupta",
      "Zhou Yu",
      "Ruoxi Jia"
    ],
    "abstract": "The exploitation of large language models (LLMs) for malicious purposes poses\nsignificant security risks as these models become more powerful and widespread.\nWhile most existing red-teaming frameworks focus on single-turn attacks,\nreal-world adversaries typically operate in multi-turn scenarios, iteratively\nprobing for vulnerabilities and adapting their prompts based on threat model\nresponses. In this paper, we propose \\AlgName, a novel multi-turn red-teaming\nagent that emulates sophisticated human attackers through complementary\nlearning dimensions: global tactic-wise learning that accumulates knowledge\nover time and generalizes to new attack goals, and local prompt-wise learning\nthat refines implementations for specific goals when initial attempts fail.\nUnlike previous multi-turn approaches that rely on fixed strategy sets,\n\\AlgName enables the agent to identify new jailbreak tactics, develop a\ngoal-based tactic selection framework, and refine prompt formulations for\nselected tactics. Empirical evaluations on JailbreakBench demonstrate our\nframework's superior performance, achieving over 90\\% attack success rates\nagainst GPT-3.5-Turbo and Llama-3.1-70B within 5 conversation turns,\noutperforming state-of-the-art baselines. These results highlight the\neffectiveness of dynamic learning in identifying and exploiting model\nvulnerabilities in realistic multi-turn scenarios.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01278v1",
    "published_date": "2025-04-02 01:06:19 UTC",
    "updated_date": "2025-04-02 01:06:19 UTC"
  }
]
{
  "date": "2025-04-02",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-04-02 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文讨论了众多热门话题，尤其在大型语言模型（LLM）领域，涵盖了推理机制、评估基准、安全性、偏见分析以及与多模态、强化学习的结合。值得关注的亮点包括：探索 LLM 推理长度与任务复杂性的关系、提出新颖的 LLM 评估框架（如 PaperBench、YourBench）、研究 LLM 在代码生成和软件测试中的应用与风险、以及利用 LLM 进行科学发现（AI-Newton）。此外，扩散模型、图神经网络、强化学习的可解释性、多模态学习和 AI 安全性也是今日的研究热点。\n\n**重点论文 & LLM 相关研究**\n\n*   **批判性思维：哪种复杂性决定了最佳推理长度？ (Critical Thinking: Which Kinds of Complexity Govern Optimal Reasoning Length?)**\n    *   研究发现 LLM 的推理性能并非越长越好，存在一个最佳推理 token 数量。有趣的是，这个最佳长度与任务所需的潜在状态追踪（DFA 运行长度）相关，而与状态空间复杂度（DFA 大小）无关。这表明预测并筛选出最优长度的推理过程可以提高准确率。\n\n*   **PaperBench：评估 AI 复现 AI 研究的能力 (PaperBench: Evaluating AI's Ability to Replicate AI Research)**\n    *   提出了 PaperBench，一个评估 AI 智能体（如 LLM）复现顶会论文（ICML 2024）能力的基准。智能体需要从零开始理解论文、开发代码并执行实验。结果显示，目前最强的模型（Claude 3.5 Sonnet）平均复现得分仅为 21%，尚未超越人类博士生基线，突显了 AI 在复杂科研任务上的局限性。\n\n*   **YourBench：为每个人轻松定制评估集 (YourBench: Easy Custom Evaluation Sets for Everyone)**\n    *   针对现有 LLM 评估基准易饱和、易污染的问题，提出了 YourBench 框架。用户只需提供文档，即可自动、廉价地生成可靠、最新、领域定制的基准测试，无需手动标注。该框架成功复现了 MMLU 子集，并引入了新数据集 Tempora-0325 以验证其生成的数据基于输入而非模型先验知识。\n\n*   **跨语言一致性：推进大型语言模型推理的新型推理框架 (Cross-Lingual Consistency: A Novel Inference Framework for Advancing Reasoning in Large Language Models)**\n    *   针对中小型 LLM 在复杂推理中存在的语言偏见和逻辑不一致问题，提出跨语言一致性（CLC）框架。通过整合多种语言的推理路径并进行多数投票，CLC 能有效提升 LLM 的推理能力，在中等规模模型（如 DeepSeek-Math-7B, Qwen2.5-Math-7B, Gemma2-9B）上取得了显著的准确率提升，尤其在多语言数学推理任务 MGSM 上表现突出。\n\n*   **风格重于实质：蒸馏语言模型通过风格复制进行推理 (Style over Substance: Distilled Language Models Reason Via Stylistic Replication)**\n    *   研究发现，通过知识蒸馏训练的小型推理语言模型（RLM）在很大程度上是通过模仿大型模型推理轨迹中的结构和词汇模式（风格）来提升性能的，而非真正内化了推理逻辑。即使使用仅包含表面模式、甚至导向错误答案的合成轨迹进行训练，模型性能也能提升，揭示了当前蒸馏方法在传递真正推理能力上的局限性。\n\n*   **InfiniteICL：通过长短期记忆转换打破上下文窗口大小限制 (InfiniteICL: Breaking the Limit of Context Window Size via Long Short-term Memory Transformation)**\n    *   提出了 InfiniteICL 框架，将 LLM 的上下文类比为短期记忆，参数类比为长期记忆。通过将上下文知识转化为参数更新，该方法能大幅减少内存占用，在超长上下文（高达 2M token）中保持甚至超越全上下文提示的性能，理论上实现了无限上下文的整合。\n\n*   **STAR-1：用 1K 数据更安全地对齐推理 LLM (STAR-1: Safer Alignment of Reasoning LLMs with 1K Data)**\n    *   针对大型推理模型（LRM）的安全对齐需求，构建了一个仅包含 1000 个样本的高质量安全数据集 STAR-1。该数据集遵循多样性、审慎推理和严格过滤原则，通过在 STAR-1 上微调，LRM 的安全性能平均提升 40%，而推理能力仅轻微下降（平均 1.1%）。\n\n*   **代码危险！将现成大型语言模型应用于编程任务的危害性 (Code Red! On the Harmfulness of Applying Off-the-shelf Large Language Models to Programming Tasks)**\n    *   提出了一个评估 LLM 在软件工程领域潜在危害性的框架。通过构建有害场景分类法和相应提示数据集，评估了多种 LLM（开源/闭源，通用/代码专用）生成有害内容的倾向。研究发现不同模型间差异显著，代码专用模型并不一定更安全，某些微调模型甚至比基础模型更差，而更大模型通常更不易生成有害信息，强调了针对性对齐策略的重要性。\n\n*   **AI-Newton：一个无需先验物理知识、概念驱动的物理定律发现系统 (AI-Newton: A Concept-Driven Physical Law Discovery System without Prior Physical Knowledge)**\n    *   提出了 AI-Newton 系统，旨在模拟人类科学发现过程。该系统以物理概念为核心构建知识库和表示，并拥有自主发现工作流。在无监督、无先验物理知识的情况下，仅基于带噪声的实验数据，AI-Newton 成功地重新发现了牛顿第二定律、能量守恒定律和万有引力定律等基本物理规律，展示了 AI 自主科学发现的潜力。\n\n*   **从代码生成到软件测试：具有基于上下文 RAG 的 AI Copilot (From Code Generation to Software Testing: AI Copilot with Context-Based RAG)**\n    *   将 AI 辅助编程（代码补全、问答）扩展到软件测试领域，提出 Copilot for Testing 系统。该系统利用基于上下文的检索增强生成（RAG）来增强 LLM 能力，同步进行 Bug 检测和代码库更新。评估显示，该系统显著提高了 Bug 检测准确率、关键测试覆盖率和用户接受度。\n\n*   **技术性 AGI 安全与保障方法 (An Approach to Technical AGI Safety and Security)**\n    *   探讨了应对通用人工智能（AGI）重大风险的技术方法，重点关注滥用和不对齐（misalignment）风险。提出通过识别危险能力、实施安全措施、访问控制、监控和模型安全缓解来防止滥用。针对不对齐，提出模型级缓解（如监督放大、鲁棒训练）和系统级安全措施（如监控、访问控制）两道防线，并强调可解释性、不确定性估计等技术的作用。\n\n*   **LLM 穿 Prada：通过在线购物数据分析性别偏见和刻板印象 (The LLM Wears Prada: Analysing Gender Bias and Stereotypes through Online Shopping Data)**\n    *   研究 LLM 是否能仅根据在线购物历史预测用户性别，并分析其预测是否受性别偏见和刻板印象影响。结果表明，LLM 虽能以中等准确率推断性别，但其决策常基于产品类别与性别的刻板印象关联。即使明确指示避免偏见，也未能消除刻板模式。\n\n*   **弥合语言鸿沟：利用大型语言模型进行机器翻译的综述 (Bridging the Linguistic Divide: A Survey on Leveraging Large Language Models for Machine Translation)**\n    *   全面概述了利用 LLM 进行机器翻译（特别是低资源语言）的最新进展，分析了少样本提示、跨语言迁移、参数高效微调、合成数据生成等技术，并讨论了幻觉、评估不一致、偏见等挑战及新兴的 LLM 驱动评估指标。\n\n*   **文本比视觉更响亮：ASCII 艺术揭示了视觉语言模型中的文本偏见 (Text Speaks Louder than Vision: ASCII Art Reveals Textual Biases in Vision-Language Models)**\n    *   通过使用字符语义与全局视觉模式相冲突的对抗性 ASCII 艺术来评估 VLM，发现 VLM 存在强烈的文本优先偏见，会优先处理文本信息而忽略视觉模式。这揭示了当前 VLM 在整合多模态信息方面的根本缺陷。\n\n*   **PiCo：通过图像代码语境化越狱多模态大型语言模型 (PiCo: Jailbreaking Multimodal Large Language Models via Pictorial Code Contextualization)**\n    *   提出 PiCo 框架，利用视觉模态漏洞和代码训练数据的长尾分布特性，通过将有害意图嵌入代码风格的视觉指令中，分层绕过多模态 LLM（如 Gemini-Pro Vision, GPT-4）的多级防御机制，实现了高攻击成功率。\n\n**多模态、视觉与语言**\n\n*   **FineLIP：通过与更长文本输入的细粒度对齐扩展 CLIP 的能力 (FineLIP: Extending CLIP's Reach via Fine-Grained Alignment with Longer Text Inputs)**\n    *   提出 FineLIP，通过扩展位置嵌入处理更长文本，并动态聚合局部图像和文本 token，强制进行细粒度的 token 到 token 跨模态对齐，增强了 CLIP 处理长文本和细粒度信息的能力，在跨模态检索和文生图任务上表现更优。\n\n*   **Ross3D：具有 3D 感知的重建式视觉指令微调 (Ross3D: Reconstructive Visual Instruction Tuning with 3D-Awareness)**\n    *   提出 Ross3D，将 3D 感知的视觉监督（跨视图重建和全局视图重建）整合到 LMM 训练中，以增强模型对 3D 场景的理解能力。该方法在多个 3D 场景理解基准上达到 SOTA，并显示出利用大量无标签 3D 视觉数据的潜力。\n\n*   **AdPO：利用偏好优化增强大型视觉语言模型的对抗鲁棒性 (AdPO: Enhancing the Adversarial Robustness of Large Vision-Language Models with Preference Optimization)**\n    *   提出 AdPO，一种基于偏好优化的 LVLM 对抗防御策略。将对抗训练重构为偏好优化问题，旨在增强模型对干净输入的正常输出偏好，同时拒绝对抗样本的误导性输出。该方法仅修改图像编码器，实现了优越的干净和对抗性能。\n\n*   **TimeSearch：具有聚光灯和反思的分层视频搜索，实现类人长视频理解 (TimeSearch: Hierarchical Video Search with Spotlight and Reflection for Human-like Long Video Understanding)**\n    *   提出 TimeSearch 框架，模拟人类分层时间搜索策略来理解长视频。通过 Spotlight 机制（使用时间增强帧表示 TAFR）高效定位相关事件，并利用 Reflection 机制评估事件正确性，显著提升了 LVLM 在长视频理解和时序定位任务上的性能。\n\n*   **COST：用于视觉语言小目标跟踪的对比单阶段 Transformer (COST: Contrastive One-Stage Transformer for Vision-Language Small Object Tracking)**\n    *   提出 COST，一个用于视觉语言（VL）跟踪的对比单阶段 Transformer 融合框架。通过对比对齐策略最大化视频和语言描述间的互信息，学习语义一致的统一 VL 表示。同时贡献了一个新的 VL 小目标跟踪基准 VL-SOT500。\n\n*   **MuTri：用于 OCT 到 OCTA 3D 图像翻译的多视图三对齐 (MuTri: Multi-view Tri-alignment for OCT to OCTA 3D Image Translation)**\n    *   提出 MuTri 框架，用于将 3D OCT 图像翻译为 3D OCTA 图像。利用预训练的 VQ-VAE 提供语义先验，并通过多视图（OCT、OCTA、OCTA 投影图）对比语义对齐和血管结构对齐，在离散空间中学习映射，提高了翻译质量。同时发布了大规模数据集 OCTA2024。\n\n**AI 安全、偏见与可解释性**\n\n*   **隐式偏见注入攻击对抗文本到图像扩散模型 (Implicit Bias Injection Attacks against Text-to-Image Diffusion Models)**\n    *   提出了一种新颖的隐式偏见，缺乏明确视觉特征但能在不同语义背景下以多样方式表现。并设计了 IBI-Attacks 框架，通过预计算提示嵌入空间中的通用偏见方向并根据输入自适应调整，以即插即用方式向 T2I 扩散模型注入隐式偏见，具有强隐蔽性和迁移性。\n\n*   **FAIRE：评估 AI 驱动简历评估中的种族和性别偏见 (FAIRE: Assessing Racial and Gender Bias in AI-Driven Resume Evaluations)**\n    *   引入 FAIRE 基准，测试用于评估简历的 LLM 在不同行业中是否存在种族和性别偏见。通过直接评分和排名方法，衡量轻微修改简历以反映不同身份时模型性能的变化。结果显示所有模型都存在偏见，但程度和方向各异。\n\n*   **解释生成式 AI：通过基于搜索的数据影响分析增强可解释性 (Enhancing Interpretability in Generative AI Through Search-Based Data Influence Analysis)**\n    *   提出一种受搜索启发的方法，通过分析训练数据对生成模型输出的影响来提高其可解释性。该方法关注模型输出而非内部状态，通过搜索原始数据和潜在空间嵌入来发现对生成内容有影响的训练数据子集。\n\n*   **解释模型无关强化学习中的涌现规划 (Interpreting Emergent Planning in Model-Free Reinforcement Learning)**\n    *   首次提供了模型无关强化学习智能体（DRC）能够学习规划的机制性证据。通过概念性可解释性方法，证明智能体在 Sokoban 环境中利用学习到的概念表示来内部制定计划，预测动作的长期影响并指导行动选择。发现智能体学习到的规划算法类似于并行双向搜索。\n\n*   **推进 AI 科学家理解：让 LLM 像物理学家一样思考并具有可解释的推理 (Advancing AI-Scientist Understanding: Making LLM Think Like a Physicist with Interpretable Reasoning)**\n    *   提出一个包含推理、解释和 AI-科学家交互模块的框架，旨在提高 LLM 在物理研究中输出的可靠性和可解释性。引入解释模块（包含摘要器、模型构建器等智能体）将 LLM 输出构建为物理上合理的、更易解释的科学模型。\n\n*   **策略全局化，适应局部化：具有双层学习的多轮红队智能体 (Strategize Globally, Adapt Locally: A Multi-Turn Red Teaming Agent with Dual-Level Learning)**\n    *   提出 \\AlgName，一个模拟复杂人类攻击者的多轮红队智能体。通过全局策略学习（积累知识并泛化）和局部提示学习（针对特定目标优化失败尝试）相结合，能够动态识别新的越狱策略并优化提示，在多轮对话中高效攻击 LLM。\n\n**其他亮点**\n\n*   **高效联邦学习微型语言模型用于移动网络特征预测 (Efficient Federated Learning Tiny Language Models for Mobile Network Feature Prediction)**\n    *   将微型语言模型（TLM）与联邦学习（FL）结合，用于移动网络特征预测。并研究了 ISO/IEC NNC 标准的 Fraunhofer 实现 NNCodec 在 FL 框架中的压缩效率，证明 NNCodec 能在保持性能的同时将通信开销降低到 1% 以下。\n*   **等变球形 CNN 用于新生儿扩散 MRI 中精确纤维定向分布估计并减少采集时间 (Equivariant Spherical CNNs for Accurate Fiber Orientation Distribution Estimation in Neonatal Diffusion MRI with Reduced Acquisition Time)**\n    *   提出一种旋转等变的球形 CNN（sCNN）框架，用于从减少采集梯度方向（仅 30%）的新生儿 dMRI 数据中预测纤维定向分布（FOD）。结果显示 sCNN 显著优于 MLP 基线，提高了 FOD 估计精度和后续纤维束追踪的解剖学合理性。\n*   **CoRAG：协作式检索增强生成 (CoRAG: Collaborative Retrieval-Augmented Generation)**\n    *   提出 CoRAG 框架，将 RAG 扩展到协作学习场景，客户端使用协作式段落存储共同训练共享模型。引入 CRAB 基准进行评估，证明 CoRAG 在低资源场景下优于参数化协作学习和本地 RAG 模型。\n*   **通过多 ODE 解的外推法增强扩散采样 (Enhanced Diffusion Sampling via Extrapolation with Multiple ODE Solutions)**\n    *   提出 RX-DPM，一种受 Richardson 外推法启发的增强型 ODE 采样方法。通过利用中间时间步的多个 ODE 解来外推去噪预测，显著提高了最终样本估计的准确性，同时保持函数评估次数（NFE）不变。\n*   **双流 Transformer-GCN 模型与语境化表示学习用于单目 3D 人体姿态估计 (Dual-stream Transformer-GCN Model with Contextualized Representations Learning for Monocular 3D Human Pose Estimation)**\n    *   提出一种基于语境化表示学习的双流 Transformer-GCN 模型，用于单目 3D 人体姿态估计。通过掩蔽 2D 姿态特征和自蒸馏设置，模型学习高维表示，增强了对姿态时空关系的理解，并在基准数据集上达到 SOTA。\n*   **超扩散推荐模型 (Hyperbolic Diffusion Recommender Model)**\n    *   针对推荐系统中物品表示的各向异性和方向性，提出超扩散推荐模型 (HDRM)。利用双曲空间的非欧结构处理各向异性扩散过程，设计了新的双曲潜在扩散过程，保留了用户-物品图的内在拓扑结构。\n*   **领域指导：预训练扩散模型的简单迁移方法 (Domain Guidance: A Simple Transfer Approach for a Pre-trained Diffusion Model)**\n    *   提出 Domain Guidance，一种简单的预训练模型迁移方法。利用预训练知识引导采样过程趋向目标领域，其形式类似于先进的无分类器指导，能实现更好的领域对齐和更高质量的生成，且无需额外训练即可集成到现有微调模型中。\n*   **训练无关的密集对齐扩散指导用于模块化条件图像合成 (Training-free Dense-Aligned Diffusion Guidance for Modular Conditional Image Synthesis)**\n    *   提出一种无需训练的模块化条件图像合成方法。将条件分解为文本、布局和拖拽三个基本单元，并为每个单元设计专门的对齐模块（DCA, DGA, DMA），通过灵活组合这些模块，可适应多样化的条件生成任务。\n*   **通过选择性剪枝重构联邦机器忘却学习 (Sky of Unlearning (SoUL): Rewiring Federated Machine Unlearning via Selective Pruning)**\n    *   提出 SoUL 框架，用于无人机互联网（IoD）中的联邦忘却学习。设计选择性剪枝算法，识别并移除对忘却数据影响大但对整体模型性能影响小的神经元，高效移除不良数据影响，同时降低计算和通信开销。\n*   **面向市场的太阳能热电站流量分配：基于拍卖和人工智能的方法 (Market-Oriented Flow Allocation for Thermal Solar Plants: An Auction-Based Methodology with Artificial Intelligence)**\n    *   提出一种基于市场拍卖机制结合人工神经网络（ANN）的方法，用于优化抛物槽式集热器（PTC）电站的热平衡，在不同工况下均优于无分配系统，已在 13 个商业电站中实施运行。\n\n**快速浏览**\n\n*   **自动文本分类全面基准测试** (#4): 对比了从传统方法到 LLM 的 12 种文本分类方法，发现 LLM 效果最好但成本最高，SVM/逻辑回归适合资源受限场景，SLM（如 Roberta）在效果和效率间取得平衡。\n*   **基准化合成表格数据** (#9): 提出了一个评估合成表格数据质量（隐私和效用）的多维度框架和基准测试策略。\n*   **加速 IoV 入侵检测** (#10): 对比 GPU 加速库 (cuML) 与 CPU 库 (scikit-learn) 在 IoV 入侵检测机器学习模型上的性能，GPU 加速显著缩短训练和预测时间。\n*   **社交媒体对话中的滥用语言检测** (#12): 使用图神经网络（GNN）将对话建模为图，利用结构化对话上下文改进滥用语言检测效果。\n*   **康复下肢外骨骼的新型手势交互控制方法** (#14): 提出基于 RGB 单目相机深度估计的非接触式手势交互控制方法，用于控制康复外骨骼步态。\n*   **工业人工智能再思考：统一基础框架** (#26): 提出包含知识、数据、模型三个核心模块的统一工业 AI 基础框架。\n*   **CLaP：时间序列状态检测** (#27): 提出 CLaP 算法，利用自监督技术进行无监督时间序列状态检测，性能优于 SOTA。\n*   **利用多模态机器学习中的嵌入技术进行精神疾病评估** (#29): 探索使用文本、音频、视频多模态数据和各种嵌入、融合技术（包括 LLM 预测）进行抑郁症和 PTSD 检测与严重性评估。\n*   **Tsetlin 机器中知识蒸馏的新方法** (#25): 提出利用教师模型输出概率分布和子句重要性加权进行 Tsetlin 机器知识蒸馏的新方法。\n*   **认知技能：关于知识与遗忘的推理** (#33): 提出一类认知逻辑，使用“认知技能”度量来模拟知识获取（技能提升）和遗忘（技能下降）的动态过程。\n*   **DreamActor-M1：混合引导下整体、富有表现力且鲁棒的人体图像动画** (#34): 提出基于 DiT 和混合引导（面部表示、3D 头部/骨骼）的人体动画框架，实现更优控制、尺度适应和时间一致性。\n*   **利用语言语义增强图神经网络中的各向异性** (#64): 提出 LanSAGNN 框架，将各向异性 GNN 扩展到自然语言层面，利用 LLM 提取节点对的定制语义信息。\n*   **BiSeg-SAM：用于提升 SAM 二元分割性能的弱监督后处理框架** (#61): 提出 BiSeg-SAM，通过 SAM 引导的弱监督提示和边界细化网络，提升息肉和皮肤病灶的分割效果。\n*   **抽象空间推理中通过元学习实现组合性的系统泛化** (#62): 将元学习促进组合性的方法扩展到抽象空间推理，提出 SYGAR 数据集，证明该方法能让模型泛化到未见过的几何变换组合。\n*   **GRAM：生成式检索与对齐模型 - 电商检索新范式** (#68): 提出 GRAM，通过联合训练查询和商品的文本信息生成共享文本标识符代码，并采用协同对齐策略和查询-商品评分机制，提升电商检索效率和效果。\n*   **ToolACE-R：具有自适应自我优化的工具学习** (#69): 提出 ToolACE-R，通过模型感知的迭代训练和允许 LLM 自适应地迭代优化工具调用，提升工具学习性能和效率。\n*   **从易到难：构建差分隐私图像合成的捷径** (#70): 提出两阶段 DP 图像合成框架，先让模型学习敏感数据的简单特征（通过聚合样本的“中心图像”），再进行难样本学习，提升 DP 图像合成性能。\n*   **进展的幻觉？评估 Web 智能体的当前状态** (#71): 引入 Online-Mind2Web 在线评估基准和 LLM-as-a-Judge 评估方法，对当前 Web 智能体进行严格评估，指出先前报告可能过于乐观。\n*   **推进 MoE 效率：协作约束路由 (C2R) 策略** (#73): 提出 C2R 路由策略，通过鼓励更专门化的专家组来优化 MoE 模型的专家并行效率和通信开销。\n*   **CFMD：用于显著目标检测的动态跨层特征融合** (#75): 提出 CFMD 网络，通过上下文感知特征聚合模块（CFLMA，使用 Mamba）和自适应动态上采样单元（CFLMD）改进跨层特征金字塔网络，提升显著目标检测的效率和边界准确性。\n*   **视觉抽象推理的数据合成与后训练** (#76): 探索通过创新的数据合成和分步后训练过程，使 LLaVA-NeXT 7B 模型在抽象视觉推理（AVR）任务上超越 GPT-4o 等大型 VLM。\n*   **自适应修正采样用于测试时计算缩放** (#78): 提出 AR-Sampling，利用过程监督奖励模型（PRM）作为验证器，引导 LLM 在推理过程中进行自适应的步骤级反思和修正，以提高复杂任务性能。\n*   **通过本地知识图上的多级摘要进行生物医学问答** (#79): 提出利用命题断言构建本地知识图，并进行分层摘要以增强小型语言模型在生物医学问答任务中的表现。\n*   **通过策略优化、动态 RAG、KV 缓存和解码扩展测试时推理** (#80): 提出一个集成框架，结合策略优化的 RAG (PORAG)、自适应检索 (ATLAS)、KV 缓存压缩 (CRITIC) 和测试时缩放技术，全面提升 RAG 系统的效率、准确性和可扩展性。\n*   **其他：** 图形化揭示社交媒体滥用 (#12)，量子退火优化包裹递送 (#52)，C-DMG 中的宏观因果效应识别 (#53)，利用符号网络和 LLM 进行环境识别 (#44)，桥接 2D-3D 的不确定性感知的层次化配准网络 (#45)，LLM 驱动的多智能体动态规划生成 (#46)，通过自然和对抗性局部扰动基准化 DNN 的空间鲁棒性 (#47)，利用新信息检索和 AI 工具加速地平线扫描 (#48)，Pro-DG 程序化扩散引导生成建筑立面 (#51)，强化学习解锁小型 LLM 的心智理论 (#38)，分割变异性与影像组学稳定性预测三阴性乳腺癌亚型 (#39)，音频 Transformer 中的 Token 剪枝 (#40)，K-P 量子神经网络 (#41)，通过概率滤波进行混合蝴蝶亚种异常检测 (#42)，重新定义土著语言技术 (#56)，概率课程学习用于基于目标的强化学习 (#60)，VR 和 AI 作为太空等隔离环境中的心理对策 (#72)，可解释的重构优化算法用于工业和可靠性冗余分配问题 (#74)。",
  "papers": [
    {
      "arxiv_id": "2504.01951v1",
      "title": "The LLM Wears Prada: Analysing Gender Bias and Stereotypes through Online Shopping Data",
      "title_zh": "穿 Prada 的 LLM：通过在线购物数据分析性别偏见和刻板印象\n",
      "authors": [
        "Massimiliano Luca",
        "Ciro Beneduce",
        "Bruno Lepri",
        "Jacopo Staiano"
      ],
      "abstract": "With the wide and cross-domain adoption of Large Language Models, it becomes\ncrucial to assess to which extent the statistical correlations in training\ndata, which underlie their impressive performance, hide subtle and potentially\ntroubling biases. Gender bias in LLMs has been widely investigated from the\nperspectives of works, hobbies, and emotions typically associated with a\nspecific gender. In this study, we introduce a novel perspective. We\ninvestigate whether LLMs can predict an individual's gender based solely on\nonline shopping histories and whether these predictions are influenced by\ngender biases and stereotypes. Using a dataset of historical online purchases\nfrom users in the United States, we evaluate the ability of six LLMs to\nclassify gender and we then analyze their reasoning and products-gender\nco-occurrences. Results indicate that while models can infer gender with\nmoderate accuracy, their decisions are often rooted in stereotypical\nassociations between product categories and gender. Furthermore, explicit\ninstructions to avoid bias reduce the certainty of model predictions, but do\nnot eliminate stereotypical patterns. Our findings highlight the persistent\nnature of gender biases in LLMs and emphasize the need for robust\nbias-mitigation strategies.",
      "tldr_zh": "该研究从一个新颖的角度出发，探讨大型语言模型(LLMs)中存在的性别偏见和刻板印象：通过分析在线购物数据，研究LLMs是否能仅根据用户的购物历史预测其性别，以及这种预测是否受到性别偏见的影响。研究人员使用美国用户的历史在线购物数据，评估了六个LLMs进行性别分类的能力，并分析了它们的推理过程和产品-性别共现关系。结果表明，尽管模型能够以中等准确率推断性别，但其决策往往基于产品类别和性别之间的刻板印象关联。即使明确指示避免偏见，也只能降低预测的确定性，而无法消除刻板印象模式。这项研究强调了LLMs中性别偏见的顽固性，并强调需要采取强有力的偏见缓解策略。\n",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01951v1",
      "published_date": "2025-04-02 17:56:08 UTC",
      "updated_date": "2025-04-02 17:56:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:03:12.583448"
    },
    {
      "arxiv_id": "2504.01947v1",
      "title": "Efficient Federated Learning Tiny Language Models for Mobile Network Feature Prediction",
      "title_zh": "用于移动网络特征预测的高效联邦学习微型语言模型\n",
      "authors": [
        "Daniel Becking",
        "Ingo Friese",
        "Karsten Müller",
        "Thomas Buchholz",
        "Mandy Galkow-Schneider",
        "Wojciech Samek",
        "Detlev Marpe"
      ],
      "abstract": "In telecommunications, Autonomous Networks (ANs) automatically adjust\nconfigurations based on specific requirements (e.g., bandwidth) and available\nresources. These networks rely on continuous monitoring and intelligent\nmechanisms for self-optimization, self-repair, and self-protection, nowadays\nenhanced by Neural Networks (NNs) to enable predictive modeling and pattern\nrecognition. Here, Federated Learning (FL) allows multiple AN cells - each\nequipped with NNs - to collaboratively train models while preserving data\nprivacy. However, FL requires frequent transmission of large neural data and\nthus an efficient, standardized compression strategy for reliable\ncommunication. To address this, we investigate NNCodec, a Fraunhofer\nimplementation of the ISO/IEC Neural Network Coding (NNC) standard, within a\nnovel FL framework that integrates tiny language models (TLMs) for various\nmobile network feature prediction (e.g., ping, SNR or band frequency). Our\nexperimental results on the Berlin V2X dataset demonstrate that NNCodec\nachieves transparent compression (i.e., negligible performance loss) while\nreducing communication overhead to below 1%, showing the effectiveness of\ncombining NNC with FL in collaboratively learned autonomous mobile networks.",
      "tldr_zh": "该研究提出了一种高效的联邦学习(FL)框架，该框架集成了微型语言模型(TLMs)和ISO/IEC神经⽹络编码(NNC)标准，用于移动网络特征预测。该框架旨在解决在自主网络(ANs)中，联邦学习传输大型神经⽹络数据时面临的通信开销问题。通过在Berlin V2X数据集上的实验，研究表明NNCodec能够实现透明压缩（性能损失⼏乎可以忽略不计），同时将通信开销降低到1%以下。该研究结果验证了将NNC与FL相结合在协同学习的自主移动网络中的有效性。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at 2025 EuCNC & 6G Summit Poster Session",
      "pdf_url": "http://arxiv.org/pdf/2504.01947v1",
      "published_date": "2025-04-02 17:54:06 UTC",
      "updated_date": "2025-04-02 17:54:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:03:24.573726"
    },
    {
      "arxiv_id": "2504.01935v1",
      "title": "Critical Thinking: Which Kinds of Complexity Govern Optimal Reasoning Length?",
      "title_zh": "批判性思维：哪种复杂性决定了最佳推理长度？\n",
      "authors": [
        "Celine Lee",
        "Alexander M. Rush",
        "Keyon Vafa"
      ],
      "abstract": "Large language models (LLMs) often benefit from verbalized reasoning at\ninference time, but it remains unclear which aspects of task difficulty these\nextra reasoning tokens address. To investigate this question, we formalize a\nframework using deterministic finite automata (DFAs). DFAs offer a formalism\nthrough which we can characterize task complexity through measurable properties\nsuch as run length (number of reasoning steps required) and state-space size\n(decision complexity). We first show that across different tasks and models of\ndifferent sizes and training paradigms, there exists an optimal amount of\nreasoning tokens such that the probability of producing a correct solution is\nmaximized. We then investigate which properties of complexity govern this\ncritical length: we find that task instances with longer corresponding\nunderlying DFA runs (i.e. demand greater latent state-tracking requirements)\ncorrelate with longer reasoning lengths, but, surprisingly, that DFA size (i.e.\nstate-space complexity) does not. We then demonstrate an implication of these\nfindings: being able to predict the optimal number of reasoning tokens for new\nproblems and filtering out non-optimal length answers results in consistent\naccuracy improvements.",
      "tldr_zh": "该研究探讨了在推理过程中，任务复杂度的哪些方面决定了大型语言模型(LLMs)的最佳推理长度。研究人员使用确定性有限自动机(DFAs)形式化了一个框架，通过运行长度和状态空间大小等可测量的属性来表征任务复杂度。实验发现，存在一个最佳的推理token数量，使得产生正确结果的概率最大化。研究表明，与更长的DFA运行长度（即需要更大的潜在状态跟踪）相关的任务实例需要更长的推理长度，而DFA大小（即状态空间复杂度）则不然。最后，研究证明了预测新问题的最佳推理token数量并过滤掉非最佳长度答案可以带来持续的准确性提升。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01935v1",
      "published_date": "2025-04-02 17:45:58 UTC",
      "updated_date": "2025-04-02 17:45:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:03:36.504896"
    },
    {
      "arxiv_id": "2504.01930v1",
      "title": "A thorough benchmark of automatic text classification: From traditional approaches to large language models",
      "title_zh": "自动文本分类的全面基准评测：从传统方法到大型语言模型\n",
      "authors": [
        "Washington Cunha",
        "Leonardo Rocha",
        "Marcos André Gonçalves"
      ],
      "abstract": "Automatic text classification (ATC) has experienced remarkable advancements\nin the past decade, best exemplified by recent small and large language models\n(SLMs and LLMs), leveraged by Transformer architectures. Despite recent\neffectiveness improvements, a comprehensive cost-benefit analysis investigating\nwhether the effectiveness gains of these recent approaches compensate their\nmuch higher costs when compared to more traditional text classification\napproaches such as SVMs and Logistic Regression is still missing in the\nliterature. In this context, this work's main contributions are twofold: (i) we\nprovide a scientifically sound comparative analysis of the cost-benefit of\ntwelve traditional and recent ATC solutions including five open LLMs, and (ii)\na large benchmark comprising {22 datasets}, including sentiment analysis and\ntopic classification, with their (train-validation-test) partitions based on\nfolded cross-validation procedures, along with documentation, and code. The\nrelease of code, data, and documentation enables the community to replicate\nexperiments and advance the field in a more scientifically sound manner. Our\ncomparative experimental results indicate that LLMs outperform traditional\napproaches (up to 26%-7.1% on average) and SLMs (up to 4.9%-1.9% on average) in\nterms of effectiveness. However, LLMs incur significantly higher computational\ncosts due to fine-tuning, being, on average 590x and 8.5x slower than\ntraditional methods and SLMs, respectively. Results suggests the following\nrecommendations: (1) LLMs for applications that require the best possible\neffectiveness and can afford the costs; (2) traditional methods such as\nLogistic Regression and SVM for resource-limited applications or those that\ncannot afford the cost of tuning large LLMs; and (3) SLMs like Roberta for\nnear-optimal effectiveness-efficiency trade-off.",
      "tldr_zh": "该研究对自动文本分类(ATC)领域进行了全面的基准测试，对比了从传统方法（如SVM和Logistic Regression）到大型语言模型(LLMs)的12种解决方案的成本效益。研究者构建了一个包含22个数据集的大型基准，涵盖情感分析和主题分类，并采用交叉验证划分数据集。实验结果表明，LLMs在效果上优于传统方法和小型语言模型(SLMs)，但微调所需的计算成本显著更高（分别是传统方法的590倍和SLMs的8.5倍）。研究建议：对于追求最佳效果且能承担成本的应用，选择LLMs；对于资源受限的应用，选择Logistic Regression和SVM等传统方法；而Roberta等SLMs则在效果和效率之间取得了较好的平衡。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "7 pages, 2 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.01930v1",
      "published_date": "2025-04-02 17:40:08 UTC",
      "updated_date": "2025-04-02 17:40:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:03:48.550455"
    },
    {
      "arxiv_id": "2504.01925v1",
      "title": "Equivariant Spherical CNNs for Accurate Fiber Orientation Distribution Estimation in Neonatal Diffusion MRI with Reduced Acquisition Time",
      "title_zh": "用于在缩短采集时间的新生儿弥散 MRI 中进行精确纤维方向分布估计的等变球面 CNN",
      "authors": [
        "Haykel Snoussi",
        "Davood Karimi"
      ],
      "abstract": "Early and accurate assessment of brain microstructure using diffusion\nMagnetic Resonance Imaging (dMRI) is crucial for identifying neurodevelopmental\ndisorders in neonates, but remains challenging due to low signal-to-noise ratio\n(SNR), motion artifacts, and ongoing myelination. In this study, we propose a\nrotationally equivariant Spherical Convolutional Neural Network (sCNN)\nframework tailored for neonatal dMRI. We predict the Fiber Orientation\nDistribution (FOD) from multi-shell dMRI signals acquired with a reduced set of\ngradient directions (30% of the full protocol), enabling faster and more\ncost-effective acquisitions. We train and evaluate the performance of our sCNN\nusing real data from 43 neonatal dMRI datasets provided by the Developing Human\nConnectome Project (dHCP). Our results demonstrate that the sCNN achieves\nsignificantly lower mean squared error (MSE) and higher angular correlation\ncoefficient (ACC) compared to a Multi-Layer Perceptron (MLP) baseline,\nindicating improved accuracy in FOD estimation. Furthermore, tractography\nresults based on the sCNN-predicted FODs show improved anatomical plausibility,\ncoverage, and coherence compared to those from the MLP. These findings\nhighlight that sCNNs, with their inherent rotational equivariance, offer a\npromising approach for accurate and clinically efficient dMRI analysis, paving\nthe way for improved diagnostic capabilities and characterization of early\nbrain development.",
      "tldr_zh": "该研究提出了一种旋转等变的球形卷积神经网络(sCNN)框架，用于在新生儿弥散磁共振成像(dMRI)中精确估计纤维方向分布(FOD)。该方法利用减少的梯度方向集（完整协议的30%）的多壳dMRI信号预测FOD，从而实现更快、更经济的采集。在来自Developing Human Connectome Project (dHCP)的43个新生儿dMRI数据集上进行训练和评估，结果表明，与多层感知器(MLP)基线相比，sCNN实现了显著更低的均方误差(MSE)和更高的角度相关系数(ACC)，表明FOD估计的准确性得到了提高。基于sCNN预测的FOD的纤维束成像结果显示，与MLP相比，解剖学上的合理性、覆盖率和连贯性均有所提高。该研究表明，sCNN凭借其固有的旋转等变性，为准确和临床高效的dMRI分析提供了一种有前景的方法，为改善早期大脑发育的诊断能力和特征提供了可能。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01925v1",
      "published_date": "2025-04-02 17:36:51 UTC",
      "updated_date": "2025-04-02 17:36:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:04:01.003243"
    },
    {
      "arxiv_id": "2504.01919v2",
      "title": "Bridging the Linguistic Divide: A Survey on Leveraging Large Language Models for Machine Translation",
      "title_zh": "弥合语言鸿沟：利用大型语言模型进行机器翻译的综述\n",
      "authors": [
        "Baban Gain",
        "Dibyanayan Bandyopadhyay",
        "Asif Ekbal"
      ],
      "abstract": "The advent of Large Language Models (LLMs) has significantly reshaped the\nlandscape of machine translation (MT), particularly for low-resource languages\nand domains that lack sufficient parallel corpora, linguistic tools, and\ncomputational infrastructure. This survey presents a comprehensive overview of\nrecent progress in leveraging LLMs for MT. We analyze techniques such as\nfew-shot prompting, cross-lingual transfer, and parameter-efficient fine-tuning\nthat enable effective adaptation to under-resourced settings. The paper also\nexplores synthetic data generation strategies using LLMs, including\nback-translation and lexical augmentation. Additionally, we compare LLM-based\ntranslation with traditional encoder-decoder models across diverse language\npairs, highlighting the strengths and limitations of each. We discuss\npersistent challenges such as hallucinations, evaluation inconsistencies, and\ninherited biases while also evaluating emerging LLM-driven metrics for\ntranslation quality. This survey offers practical insights and outlines future\ndirections for building robust, inclusive, and scalable MT systems in the era\nof large-scale generative models.",
      "tldr_zh": "本文综述了如何利用大型语言模型(LLMs)进行机器翻译(MT)的最新进展，尤其是在缺乏平行语料库、语言工具和计算基础设施的低资源语言和领域。文章分析了few-shot prompting、跨语言迁移和参数高效微调等技术，这些技术能够有效地适应资源不足的环境。此外，还探讨了使用LLMs的合成数据生成策略，包括回译和词汇增强。通过比较基于LLM的翻译与传统的编码器-解码器模型，突出了各自的优势和局限性。最后，讨论了幻觉、评估不一致和继承偏差等持续存在的挑战，并评估了新兴的LLM驱动的翻译质量指标，为构建稳健、包容和可扩展的MT系统提供了实践见解和未来方向。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01919v2",
      "published_date": "2025-04-02 17:26:40 UTC",
      "updated_date": "2025-04-03 13:30:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:04:12.654158"
    },
    {
      "arxiv_id": "2504.01916v1",
      "title": "FineLIP: Extending CLIP's Reach via Fine-Grained Alignment with Longer Text Inputs",
      "title_zh": "FineLIP：通过与更长文本输入的细粒度对齐扩展 CLIP 的能力\n",
      "authors": [
        "Mothilal Asokan",
        "Kebin Wu",
        "Fatima Albreiki"
      ],
      "abstract": "As a pioneering vision-language model, CLIP (Contrastive Language-Image\nPre-training) has achieved significant success across various domains and a\nwide range of downstream vision-language tasks. However, the text encoders in\npopular CLIP models are limited to processing only 77 text tokens, which\nconstrains their ability to effectively handle longer, detail-rich captions.\nAdditionally, CLIP models often struggle to effectively capture detailed visual\nand textual information, which hampers their performance on tasks that require\nfine-grained analysis. To address these limitations, we present a novel\napproach, \\textbf{FineLIP}, that extends the capabilities of CLIP. FineLIP\nenhances cross-modal text-image mapping by incorporating \\textbf{Fine}-grained\nalignment with \\textbf{L}onger text input within the CL\\textbf{IP}-style\nframework. FineLIP first extends the positional embeddings to handle longer\ntext, followed by the dynamic aggregation of local image and text tokens. The\naggregated results are then used to enforce fine-grained token-to-token\ncross-modal alignment. We validate our model on datasets with long, detailed\ncaptions across two tasks: zero-shot cross-modal retrieval and text-to-image\ngeneration. Quantitative and qualitative experimental results demonstrate the\neffectiveness of FineLIP, outperforming existing state-of-the-art approaches.\nFurthermore, comprehensive ablation studies validate the benefits of key design\nelements within FineLIP.",
      "tldr_zh": "该论文提出了FineLIP，一种改进CLIP模型的新方法，旨在通过细粒度对齐更长的文本输入来扩展CLIP的能力。FineLIP通过扩展位置嵌入来处理更长的文本，并动态聚合局部图像和文本tokens。聚合结果用于加强细粒度的token-to-token跨模态对齐。实验结果表明，FineLIP在零样本跨模态检索和文本到图像生成等任务上优于现有方法，尤其是在处理包含长而详细描述的数据集时表现出色。消融实验验证了FineLIP中关键设计元素的有效性。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01916v1",
      "published_date": "2025-04-02 17:19:59 UTC",
      "updated_date": "2025-04-02 17:19:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:04:24.405060"
    },
    {
      "arxiv_id": "2504.01911v1",
      "title": "Advancing AI-Scientist Understanding: Making LLM Think Like a Physicist with Interpretable Reasoning",
      "title_zh": "提升 AI 科学家理解能力：使 LLM 通过可解释的推理像物理学家一样思考\n",
      "authors": [
        "Yinggan Xu",
        "Hana Kimlee",
        "Yijia Xiao",
        "Di Luo"
      ],
      "abstract": "Large Language Models (LLMs) are playing an expanding role in physics\nresearch by enhancing reasoning, symbolic manipulation, and numerical\ncomputation. However, ensuring the reliability and interpretability of their\noutputs remains a significant challenge. In our framework, we conceptualize the\ncollaboration between AI and human scientists as a dynamic interplay among\nthree modules: the reasoning module, the interpretation module, and the\nAI-scientist interaction module. Recognizing that effective physics reasoning\ndemands rigorous logical consistency, quantitative precision, and deep\nintegration with established theoretical models, we introduce the\ninterpretation module to improve the understanding of AI-generated outputs,\nwhich is not previously explored in the literature. This module comprises\nmultiple specialized agents, including summarizers, model builders, UI\nbuilders, and testers, which collaboratively structure LLM outputs within a\nphysically grounded framework, by constructing a more interpretable science\nmodel. A case study demonstrates that our approach enhances transparency,\nfacilitates validation, and strengthens AI-augmented reasoning in scientific\ndiscovery.",
      "tldr_zh": "该论文提出了一个AI科学家理解框架，旨在提高大型语言模型(LLM)在物理研究中的可靠性和可解释性。该框架包含推理模块、解释模块和AI科学家交互模块，其中解释模块是创新点，通过多个专业智能体（如summarizers, model builders, UI builders, and testers）协同工作，将LLM的输出构建成一个更易于理解的物理模型。该方法能够增强透明度，促进验证，并加强AI在科学发现中的推理能力。一个案例研究验证了该框架的有效性。\n",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01911v1",
      "published_date": "2025-04-02 17:13:16 UTC",
      "updated_date": "2025-04-02 17:13:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:04:36.343065"
    },
    {
      "arxiv_id": "2504.01908v1",
      "title": "Benchmarking Synthetic Tabular Data: A Multi-Dimensional Evaluation Framework",
      "title_zh": "合成表格数据基准测试：一个多维度评估框架\n",
      "authors": [
        "Andrey Sidorenko",
        "Michael Platzer",
        "Mario Scriminaci",
        "Paul Tiwald"
      ],
      "abstract": "Evaluating the quality of synthetic data remains a key challenge for ensuring\nprivacy and utility in data-driven research. In this work, we present an\nevaluation framework that quantifies how well synthetic data replicates\noriginal distributional properties while ensuring privacy. The proposed\napproach employs a holdout-based benchmarking strategy that facilitates\nquantitative assessment through low- and high-dimensional distribution\ncomparisons, embedding-based similarity measures, and nearest-neighbor distance\nmetrics. The framework supports various data types and structures, including\nsequential and contextual information, and enables interpretable quality\ndiagnostics through a set of standardized metrics. These contributions aim to\nsupport reproducibility and methodological consistency in benchmarking of\nsynthetic data generation techniques. The code of the framework is available at\nhttps://github.com/mostly-ai/mostlyai-qa.",
      "tldr_zh": "该研究提出了一个多维度评估框架，用于评估合成表格数据的质量，旨在保证数据驱动研究中的隐私和效用。该框架采用基于留存数据的基准测试策略，通过低维和高维分布比较、基于嵌入的相似性度量以及最近邻距离指标进行定量评估。该框架支持各种数据类型和结构，包括序列和上下文信息，并通过一组标准化指标实现可解释的质量诊断。该框架旨在支持合成数据生成技术基准测试中的可重复性和方法一致性。代码已开源在https://github.com/mostly-ai/mostlyai-qa。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 7 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2504.01908v1",
      "published_date": "2025-04-02 17:10:30 UTC",
      "updated_date": "2025-04-02 17:10:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:04:48.472099"
    },
    {
      "arxiv_id": "2504.01905v2",
      "title": "Accelerating IoV Intrusion Detection: Benchmarking GPU-Accelerated vs CPU-Based ML Libraries",
      "title_zh": "加速 IoV 入侵检测：GPU 加速与基于 CPU 的 ML 库的基准测试\n",
      "authors": [
        "Furkan Çolhak",
        "Hasan Coşkun",
        "Tsafac Nkombong Regine Cyrille",
        "Tedi Hoxa",
        "Mert İlhan Ecevit",
        "Mehmet Nafiz Aydın"
      ],
      "abstract": "The Internet of Vehicles (IoV) may face challenging cybersecurity attacks\nthat may require sophisticated intrusion detection systems, necessitating a\nrapid development and response system. This research investigates the\nperformance advantages of GPU-accelerated libraries (cuML) compared to\ntraditional CPU-based implementations (scikit-learn), focusing on the speed and\nefficiency required for machine learning models used in IoV threat detection\nenvironments. The comprehensive evaluations conducted employ four machine\nlearning approaches (Random Forest, KNN, Logistic Regression, XGBoost) across\nthree distinct IoV security datasets (OTIDS, GIDS, CICIoV2024). Our findings\ndemonstrate that GPU-accelerated implementations dramatically improved\ncomputational efficiency, with training times reduced by a factor of up to 159\nand prediction speeds accelerated by up to 95 times compared to traditional CPU\nprocessing, all while preserving detection accuracy. This remarkable\nperformance breakthrough empowers researchers and security specialists to\nharness GPU acceleration for creating faster, more effective threat detection\nsystems that meet the urgent real-time security demands of today's connected\nvehicle networks.",
      "tldr_zh": "该研究对比了GPU加速库(cuML)和传统CPU库(scikit-learn)在车联网(IoV)入侵检测中的性能。通过在OTIDS、GIDS和CICIoV2024三个数据集上对随机森林、KNN、逻辑回归和XGBoost四种机器学习模型进行评估，结果表明GPU加速显著提高了计算效率。训练时间最多缩短了159倍，预测速度提高了95倍，同时保持了检测精度。这项研究强调了GPU加速在构建更快、更有效的IoV威胁检测系统中的潜力，以满足车联网实时安全需求。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "CIIT 2025 22nd International Conference on Informatics and\n  Information Technologies (CIIT)",
      "pdf_url": "http://arxiv.org/pdf/2504.01905v2",
      "published_date": "2025-04-02 17:04:53 UTC",
      "updated_date": "2025-04-03 08:42:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:05:00.366704"
    },
    {
      "arxiv_id": "2504.01903v1",
      "title": "STAR-1: Safer Alignment of Reasoning LLMs with 1K Data",
      "title_zh": "STAR-1：使用1K数据更安全地对齐推理LLM\n",
      "authors": [
        "Zijun Wang",
        "Haoqin Tu",
        "Yuhan Wang",
        "Juncheng Wu",
        "Jieru Mei",
        "Brian R. Bartoldson",
        "Bhavya Kailkhura",
        "Cihang Xie"
      ],
      "abstract": "This paper introduces STAR-1, a high-quality, just-1k-scale safety dataset\nspecifically designed for large reasoning models (LRMs) like DeepSeek-R1. Built\non three core principles -- diversity, deliberative reasoning, and rigorous\nfiltering -- STAR-1 aims to address the critical needs for safety alignment in\nLRMs. Specifically, we begin by integrating existing open-source safety\ndatasets from diverse sources. Then, we curate safety policies to generate\npolicy-grounded deliberative reasoning samples. Lastly, we apply a GPT-4o-based\nsafety scoring system to select training examples aligned with best practices.\nExperimental results show that fine-tuning LRMs with STAR-1 leads to an average\n40% improvement in safety performance across four benchmarks, while only\nincurring a marginal decrease (e.g., an average of 1.1%) in reasoning ability\nmeasured across five reasoning tasks. Extensive ablation studies further\nvalidate the importance of our design principles in constructing STAR-1 and\nanalyze its efficacy across both LRMs and traditional LLMs. Our project page is\nhttps://ucsc-vlaa.github.io/STAR-1.",
      "tldr_zh": "该论文介绍了STAR-1，一个高质量、小规模（仅1K）的安全数据集，专门为大型推理模型（LRMs）设计，如DeepSeek-R1。STAR-1基于多样性、审慎推理和严格过滤三个核心原则构建，旨在解决LRMs中安全对齐的关键需求。通过整合现有开源安全数据集，生成基于策略的审慎推理样本，并应用基于GPT-4o的安全评分系统，筛选出符合最佳实践的训练样本。实验结果表明，使用STAR-1对LRMs进行微调，可以在四个基准测试中平均提高40%的安全性能，同时推理能力仅略有下降（平均1.1%）。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01903v1",
      "published_date": "2025-04-02 17:04:04 UTC",
      "updated_date": "2025-04-02 17:04:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:05:12.621058"
    },
    {
      "arxiv_id": "2504.01902v1",
      "title": "Graphically Speaking: Unmasking Abuse in Social Media with Conversation Insights",
      "title_zh": "图文并茂：利用对话洞察揭示社交媒体中的辱骂行为\n",
      "authors": [
        "Célia Nouri",
        "Jean-Philippe Cointet",
        "Chloé Clavel"
      ],
      "abstract": "Detecting abusive language in social media conversations poses significant\nchallenges, as identifying abusiveness often depends on the conversational\ncontext, characterized by the content and topology of preceding comments.\nTraditional Abusive Language Detection (ALD) models often overlook this\ncontext, which can lead to unreliable performance metrics. Recent Natural\nLanguage Processing (NLP) methods that integrate conversational context often\ndepend on limited and simplified representations, and report inconsistent\nresults. In this paper, we propose a novel approach that utilize graph neural\nnetworks (GNNs) to model social media conversations as graphs, where nodes\nrepresent comments, and edges capture reply structures. We systematically\ninvestigate various graph representations and context windows to identify the\noptimal configuration for ALD. Our GNN model outperform both context-agnostic\nbaselines and linear context-aware methods, achieving significant improvements\nin F1 scores. These findings demonstrate the critical role of structured\nconversational context and establish GNNs as a robust framework for advancing\ncontext-aware abusive language detection.",
      "tldr_zh": "该论文提出了一种利用图神经网络(GNNs)检测社交媒体对话中辱骂性语言的新方法。传统辱骂性语言检测(ALD)模型常常忽略对话上下文，导致性能不佳。该方法将社交媒体对话建模为图，节点代表评论，边代表回复结构，从而有效捕捉上下文信息。通过系统性地研究不同的图表示和上下文窗口，找到了ALD的最佳配置。实验结果表明，该GNN模型优于传统的context-agnostic基线模型和线性context-aware方法，在F1分数上取得了显著提升，证明了结构化对话上下文的关键作用以及GNNs在context-aware辱骂性语言检测中的有效性。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01902v1",
      "published_date": "2025-04-02 17:03:37 UTC",
      "updated_date": "2025-04-02 17:03:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:05:24.563304"
    },
    {
      "arxiv_id": "2504.01901v1",
      "title": "Ross3D: Reconstructive Visual Instruction Tuning with 3D-Awareness",
      "title_zh": "Ross3D：基于三维感知的重建式视觉指令调优\n",
      "authors": [
        "Haochen Wang",
        "Yucheng Zhao",
        "Tiancai Wang",
        "Haoqiang Fan",
        "Xiangyu Zhang",
        "Zhaoxiang Zhang"
      ],
      "abstract": "The rapid development of Large Multimodal Models (LMMs) for 2D images and\nvideos has spurred efforts to adapt these models for interpreting 3D scenes.\nHowever, the absence of large-scale 3D vision-language datasets has posed a\nsignificant obstacle. To address this issue, typical approaches focus on\ninjecting 3D awareness into 2D LMMs by designing 3D input-level scene\nrepresentations. This work provides a new perspective. We introduce\nreconstructive visual instruction tuning with 3D-awareness (Ross3D), which\nintegrates 3D-aware visual supervision into the training procedure.\nSpecifically, it incorporates cross-view and global-view reconstruction. The\nformer requires reconstructing masked views by aggregating overlapping\ninformation from other views. The latter aims to aggregate information from all\navailable views to recover Bird's-Eye-View images, contributing to a\ncomprehensive overview of the entire scene. Empirically, Ross3D achieves\nstate-of-the-art performance across various 3D scene understanding benchmarks.\nMore importantly, our semi-supervised experiments demonstrate significant\npotential in leveraging large amounts of unlabeled 3D vision-only data.",
      "tldr_zh": "该论文提出了Ross3D，一种具有3D感知能力的重建式视觉指令调整方法，旨在解决大规模3D视觉语言数据集缺失的问题。Ross3D通过引入跨视角和全局视角的重建，将3D感知视觉监督融入到训练过程中。其中，跨视角重建通过聚合其他视角的重叠信息来重建被遮蔽的视角，而全局视角重建则旨在聚合所有可用视角的信息以恢复鸟瞰图，从而提供对整个场景的全面概览。实验结果表明，Ross3D在各种3D场景理解基准测试中取得了最先进的性能，并且在半监督实验中展现了利用大量未标记3D视觉数据的巨大潜力。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01901v1",
      "published_date": "2025-04-02 16:59:55 UTC",
      "updated_date": "2025-04-02 16:59:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:05:36.757641"
    },
    {
      "arxiv_id": "2504.01888v1",
      "title": "A novel gesture interaction control method for rehabilitation lower extremity exoskeleton",
      "title_zh": "一种用于康复下肢外骨骼的新型手势交互控制方法\n",
      "authors": [
        "Shuang Qiu",
        "Zhongcai Pei",
        "Chen Wang",
        "Jing Zhang",
        "Zhiyong Tang"
      ],
      "abstract": "With the rapid development of Rehabilitation Lower Extremity Robotic\nExoskeletons (RLEEX) technology, significant advancements have been made in\nHuman-Robot Interaction (HRI) methods. These include traditional physical HRI\nmethods that are easily recognizable and various bio-electrical signal-based\nHRI methods that can visualize and predict actions. However, most of these HRI\nmethods are contact-based, facing challenges such as operational complexity,\nsensitivity to interference, risks associated with implantable devices, and,\nmost importantly, limitations in comfort. These challenges render the\ninteraction less intuitive and natural, which can negatively impact patient\nmotivation for rehabilitation. To address these issues, this paper proposes a\nnovel non-contact gesture interaction control method for RLEEX, based on RGB\nmonocular camera depth estimation. This method integrates three key steps:\ndetecting keypoints, recognizing gestures, and assessing distance, thereby\napplying gesture information and augmented reality triggering technology to\ncontrol gait movements of RLEEX. Results indicate that this approach provides a\nfeasible solution to the problems of poor comfort, low reliability, and high\nlatency in HRI for RLEEX platforms. Specifically, it achieves a\ngesture-controlled exoskeleton motion accuracy of 94.11\\% and an average system\nresponse time of 0.615 seconds through non-contact HRI. The proposed\nnon-contact HRI method represents a pioneering advancement in control\ninteractions for RLEEX, paving the way for further exploration and development\nin this field.",
      "tldr_zh": "该论文提出了一种新颖的非接触式手势交互控制方法，用于康复下肢外骨骼(RLEEX)。该方法基于RGB单目相机深度估计，通过检测关键点、识别手势和评估距离，利用手势信息和增强现实触发技术来控制RLEEX的步态运动。实验结果表明，该方法能够有效解决RLEEX人机交互(HRI)中舒适性差、可靠性低和延迟高等问题，实现了94.11%的手势控制外骨骼运动精度和0.615秒的平均系统响应时间。该非接触式HRI方法为RLEEX的控制交互提供了一种可行方案，并为该领域的进一步探索和发展奠定了基础。\n",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01888v1",
      "published_date": "2025-04-02 16:46:01 UTC",
      "updated_date": "2025-04-02 16:46:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:05:48.581488"
    },
    {
      "arxiv_id": "2504.01883v1",
      "title": "CoRAG: Collaborative Retrieval-Augmented Generation",
      "title_zh": "CoRAG：协同检索增强生成\n",
      "authors": [
        "Aashiq Muhamed",
        "Mona Diab",
        "Virginia Smith"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) models excel in knowledge-intensive\ntasks, especially under few-shot learning constraints. We introduce CoRAG, a\nframework extending RAG to collaborative settings, where clients jointly train\na shared model using a collaborative passage store. To evaluate CoRAG, we\nintroduce CRAB, a benchmark for collaborative homogeneous open-domain question\nanswering. Our experiments demonstrate that CoRAG consistently outperforms both\nparametric collaborative learning methods and locally trained RAG models in\nlow-resource scenarios. Further analysis reveals the critical importance of\nrelevant passages within the shared store, the surprising benefits of\nincorporating irrelevant passages, and the potential for hard negatives to\nnegatively impact performance. This introduces a novel consideration in\ncollaborative RAG: the trade-off between leveraging a collectively enriched\nknowledge base and the potential risk of incorporating detrimental passages\nfrom other clients. Our findings underscore the viability of CoRAG, while also\nhighlighting key design challenges and promising avenues for future research.",
      "tldr_zh": "CoRAG是一种协作式检索增强生成(RAG)框架，旨在允许多个客户端通过协作的段落存储共同训练共享模型，从而提升知识密集型任务在少样本学习环境下的表现。研究者提出了CRAB基准测试来评估CoRAG在协作同质开放域问答中的性能。实验结果表明，在低资源场景下，CoRAG始终优于参数化的协作学习方法和本地训练的RAG模型。分析进一步揭示了相关段落在共享存储中的重要性，以及包含不相关段落的意外好处，并指出难负样本可能会对性能产生负面影响。该研究强调了协作RAG中利用集体知识库和包含有害段落风险之间的权衡，验证了CoRAG的可行性，并突出了关键的设计挑战和未来研究方向。\n",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "NAACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2504.01883v1",
      "published_date": "2025-04-02 16:40:43 UTC",
      "updated_date": "2025-04-02 16:40:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:06:01.011311"
    },
    {
      "arxiv_id": "2504.01871v1",
      "title": "Interpreting Emergent Planning in Model-Free Reinforcement Learning",
      "title_zh": "解读无模型强化学习中涌现的规划能力\n",
      "authors": [
        "Thomas Bush",
        "Stephen Chung",
        "Usman Anwar",
        "Adrià Garriga-Alonso",
        "David Krueger"
      ],
      "abstract": "We present the first mechanistic evidence that model-free reinforcement\nlearning agents can learn to plan. This is achieved by applying a methodology\nbased on concept-based interpretability to a model-free agent in Sokoban -- a\ncommonly used benchmark for studying planning. Specifically, we demonstrate\nthat DRC, a generic model-free agent introduced by Guez et al. (2019), uses\nlearned concept representations to internally formulate plans that both predict\nthe long-term effects of actions on the environment and influence action\nselection. Our methodology involves: (1) probing for planning-relevant\nconcepts, (2) investigating plan formation within the agent's representations,\nand (3) verifying that discovered plans (in the agent's representations) have a\ncausal effect on the agent's behavior through interventions. We also show that\nthe emergence of these plans coincides with the emergence of a planning-like\nproperty: the ability to benefit from additional test-time compute. Finally, we\nperform a qualitative analysis of the planning algorithm learned by the agent\nand discover a strong resemblance to parallelized bidirectional search. Our\nfindings advance understanding of the internal mechanisms underlying planning\nbehavior in agents, which is important given the recent trend of emergent\nplanning and reasoning capabilities in LLMs through RL",
      "tldr_zh": "该研究首次提供了机械证据，表明免模型强化学习(Model-Free Reinforcement Learning)智能体可以学习进行规划。研究人员采用基于概念的可解释性方法，分析了在Sokoban游戏中训练的DRC智能体（一种通用的免模型智能体）。结果表明，DRC利用学习到的概念表征在内部形成计划，这些计划既能预测动作对环境的长期影响，又能影响动作选择。该方法包括探测与规划相关的概念、研究智能体表征中的计划形成过程，并通过干预验证发现的计划对智能体行为的因果影响。研究还表明，这些计划的出现与一种类似规划的特性（即从额外的测试时间计算中获益的能力）相吻合。对智能体学习到的规划算法进行定性分析后，发现其与并行双向搜索非常相似。这些发现有助于理解智能体规划行为背后的内部机制，这对于理解大型语言模型(LLMs)通过强化学习(RL)涌现的规划和推理能力至关重要。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025 oral",
      "pdf_url": "http://arxiv.org/pdf/2504.01871v1",
      "published_date": "2025-04-02 16:24:23 UTC",
      "updated_date": "2025-04-02 16:24:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:06:13.131123"
    },
    {
      "arxiv_id": "2504.01866v1",
      "title": "From Code Generation to Software Testing: AI Copilot with Context-Based RAG",
      "title_zh": "从代码生成到软件测试：基于上下文 RAG 的 AI Copilot\n",
      "authors": [
        "Yuchen Wang",
        "Shangxin Guo",
        "Chee Wei Tan"
      ],
      "abstract": "The rapid pace of large-scale software development places increasing demands\non traditional testing methodologies, often leading to bottlenecks in\nefficiency, accuracy, and coverage. We propose a novel perspective on software\ntesting by positing bug detection and coding with fewer bugs as two\ninterconnected problems that share a common goal, which is reducing bugs with\nlimited resources. We extend our previous work on AI-assisted programming,\nwhich supports code auto-completion and chatbot-powered Q&A, to the realm of\nsoftware testing. We introduce Copilot for Testing, an automated testing system\nthat synchronizes bug detection with codebase updates, leveraging context-based\nRetrieval Augmented Generation (RAG) to enhance the capabilities of large\nlanguage models (LLMs). Our evaluation demonstrates a 31.2% improvement in bug\ndetection accuracy, a 12.6% increase in critical test coverage, and a 10.5%\nhigher user acceptance rate, highlighting the transformative potential of\nAI-driven technologies in modern software development practices.",
      "tldr_zh": "该论文提出了一种名为“Copilot for Testing”的AI辅助软件测试系统，旨在通过将bug检测与代码库更新同步，提高软件测试的效率、准确性和覆盖率。该系统利用基于上下文的检索增强生成(RAG)技术，增强大型语言模型(LLMs)的能力。实验结果表明，Copilot for Testing在bug检测准确率上提高了31.2%，关键测试覆盖率提高了12.6%，用户接受度提高了10.5%，验证了AI驱动技术在现代软件开发中的潜力。该研究将bug检测和减少bug编码视为具有共同目标的相互关联的问题，并以此为基础构建了该系统。\n",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "This work has been accepted for publication in IEEE Software (DOI:\n  10.1109/MS.2025.3549628)",
      "pdf_url": "http://arxiv.org/pdf/2504.01866v1",
      "published_date": "2025-04-02 16:20:05 UTC",
      "updated_date": "2025-04-02 16:20:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:06:24.529639"
    },
    {
      "arxiv_id": "2504.01857v1",
      "title": "Cross-Lingual Consistency: A Novel Inference Framework for Advancing Reasoning in Large Language Models",
      "title_zh": "跨语言一致性：一种用于提升大型语言模型推理能力的新型推理框架\n",
      "authors": [
        "Zhiwei Yu",
        "Tuo Li",
        "Changhong Wang",
        "Hui Chen",
        "Lang Zhou"
      ],
      "abstract": "Chain-of-thought (CoT) has emerged as a critical mechanism for enhancing\nreasoning capabilities in large language models (LLMs), with self-consistency\ndemonstrating notable promise in boosting performance. However, inherent\nlinguistic biases in multilingual training corpora frequently cause semantic\ndrift and logical inconsistencies, especially in sub-10B parameter LLMs\nhandling complex inference tasks. To overcome these constraints, we propose the\nCross-Lingual Consistency (CLC) framework, an innovative inference paradigm\nthat integrates multilingual reasoning paths through majority voting to elevate\nLLMs' reasoning capabilities. Empirical evaluations on the CMATH dataset reveal\nCLC's superiority over the conventional self-consistency method, delivering\n9.5%, 6.5%, and 6.0% absolute accuracy gains for DeepSeek-Math-7B-Instruct,\nQwen2.5-Math-7B-Instruct, and Gemma2-9B-Instruct respectively. Expanding CLC's\nlinguistic scope to 11 diverse languages implies two synergistic benefits: 1)\nneutralizing linguistic biases in multilingual training corpora through\nmultilingual ensemble voting, 2) escaping monolingual reasoning traps by\nexploring the broader multilingual solution space. This dual benefits\nempirically enables more globally optimal reasoning paths compared to\nmonolingual self-consistency baselines, as evidenced by the 4.1%-18.5% accuracy\ngains using Gemma2-9B-Instruct on the MGSM dataset.",
      "tldr_zh": "该论文提出了跨语言一致性(Cross-Lingual Consistency, CLC)框架，旨在解决大型语言模型(LLMs)在多语言推理中由于语言偏差导致的语义漂移和逻辑不一致问题。CLC通过整合多语言推理路径，并采用多数投票机制来提升LLMs的推理能力。在CMATH数据集上的实验表明，CLC优于传统的自洽性方法，DeepSeek-Math-7B-Instruct、Qwen2.5-Math-7B-Instruct和Gemma2-9B-Instruct的准确率分别提高了9.5%、6.5%和6.0%。通过扩展到11种语言，CLC能够中和多语言训练语料库中的语言偏差，并探索更广泛的多语言解决方案空间，从而实现更全局最优的推理路径，在MGSM数据集上使用Gemma2-9B-Instruct的准确率提高了4.1%-18.5%。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01857v1",
      "published_date": "2025-04-02 16:09:39 UTC",
      "updated_date": "2025-04-02 16:09:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:06:36.770339"
    },
    {
      "arxiv_id": "2504.01855v1",
      "title": "Enhanced Diffusion Sampling via Extrapolation with Multiple ODE Solutions",
      "title_zh": "通过多重 ODE 解的外推增强扩散采样\n",
      "authors": [
        "Jinyoung Choi",
        "Junoh Kang",
        "Bohyung Han"
      ],
      "abstract": "Diffusion probabilistic models (DPMs), while effective in generating\nhigh-quality samples, often suffer from high computational costs due to their\niterative sampling process. To address this, we propose an enhanced ODE-based\nsampling method for DPMs inspired by Richardson extrapolation, which reduces\nnumerical error and improves convergence rates. Our method, RX-DPM, leverages\nmultiple ODE solutions at intermediate time steps to extrapolate the denoised\nprediction in DPMs. This significantly enhances the accuracy of estimations for\nthe final sample while maintaining the number of function evaluations (NFEs).\nUnlike standard Richardson extrapolation, which assumes uniform discretization\nof the time grid, we develop a more general formulation tailored to arbitrary\ntime step scheduling, guided by local truncation error derived from a baseline\nsampling method. The simplicity of our approach facilitates accurate estimation\nof numerical solutions without significant computational overhead, and allows\nfor seamless and convenient integration into various DPMs and solvers.\nAdditionally, RX-DPM provides explicit error estimates, effectively\ndemonstrating the faster convergence as the leading error term's order\nincreases. Through a series of experiments, we show that the proposed method\nimproves the quality of generated samples without requiring additional sampling\niterations.",
      "tldr_zh": "该论文提出了一种增强的基于ODE的扩散模型采样方法RX-DPM，灵感来源于Richardson外推法，旨在降低扩散模型采样过程中的计算成本。RX-DPM利用中间时间步的多个ODE解来外推DPM中的去噪预测，从而在保持函数评估次数(NFEs)不变的情况下显著提高最终样本估计的准确性。该方法针对任意时间步调度开发了一种更通用的公式，并由来自基线采样方法的局部截断误差指导。RX-DPM提供显式误差估计，并通过实验证明，该方法提高了生成样本的质量，而无需额外的采样迭代。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.01855v1",
      "published_date": "2025-04-02 16:06:23 UTC",
      "updated_date": "2025-04-02 16:06:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:06:48.574029"
    },
    {
      "arxiv_id": "2504.01850v1",
      "title": "Code Red! On the Harmfulness of Applying Off-the-shelf Large Language Models to Programming Tasks",
      "title_zh": "红色警报！关于将现成的 LLM 应用于编程任务的危害性研究\n",
      "authors": [
        "Ali Al-Kaswan",
        "Sebastian Deatc",
        "Begüm Koç",
        "Arie van Deursen",
        "Maliheh Izadi"
      ],
      "abstract": "Nowadays, developers increasingly rely on solutions powered by Large Language\nModels (LLM) to assist them with their coding tasks. This makes it crucial to\nalign these tools with human values to prevent malicious misuse. In this paper,\nwe propose a comprehensive framework for assessing the potential harmfulness of\nLLMs within the software engineering domain. We begin by developing a taxonomy\nof potentially harmful software engineering scenarios and subsequently, create\na dataset of prompts based on this taxonomy. To systematically assess the\nresponses, we design and validate an automatic evaluator that classifies the\noutputs of a variety of LLMs both open-source and closed-source models, as well\nas general-purpose and code-specific LLMs. Furthermore, we investigate the\nimpact of models size, architecture family, and alignment strategies on their\ntendency to generate harmful content. The results show significant disparities\nin the alignment of various LLMs for harmlessness. We find that some models and\nmodel families, such as Openhermes, are more harmful than others and that\ncode-specific models do not perform better than their general-purpose\ncounterparts. Notably, some fine-tuned models perform significantly worse than\ntheir base-models due to their design choices. On the other side, we find that\nlarger models tend to be more helpful and are less likely to respond with\nharmful information. These results highlight the importance of targeted\nalignment strategies tailored to the unique challenges of software engineering\ntasks and provide a foundation for future work in this critical area.",
      "tldr_zh": "该研究提出了一个评估大型语言模型(LLMs)在软件工程领域潜在危害性的框架。首先构建了潜在有害的软件工程场景分类，并基于此创建了提示数据集。然后，设计并验证了一个自动评估器，用于分类各种LLMs（包括开源和闭源模型，通用和代码专用LLMs）的输出。研究调查了模型大小、架构类型和对齐策略对生成有害内容的影响。结果表明，不同LLMs在无害性对齐方面存在显著差异，某些模型和模型系列（如Openhermes）比其他模型更具危害性，且代码专用模型并不比通用模型表现更好。较大模型往往更有帮助，且不太可能返回有害信息。这项工作强调了针对软件工程任务独特挑战的对齐策略的重要性。\n",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "FSE'25 Technical Track",
      "pdf_url": "http://arxiv.org/pdf/2504.01850v1",
      "published_date": "2025-04-02 16:00:14 UTC",
      "updated_date": "2025-04-02 16:00:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:07:00.756414"
    },
    {
      "arxiv_id": "2504.01849v1",
      "title": "An Approach to Technical AGI Safety and Security",
      "title_zh": "一种通往技术通用人工智能安全保障的方法\n",
      "authors": [
        "Rohin Shah",
        "Alex Irpan",
        "Alexander Matt Turner",
        "Anna Wang",
        "Arthur Conmy",
        "David Lindner",
        "Jonah Brown-Cohen",
        "Lewis Ho",
        "Neel Nanda",
        "Raluca Ada Popa",
        "Rishub Jain",
        "Rory Greig",
        "Samuel Albanie",
        "Scott Emmons",
        "Sebastian Farquhar",
        "Sébastien Krier",
        "Senthooran Rajamanoharan",
        "Sophie Bridgers",
        "Tobi Ijitoye",
        "Tom Everitt",
        "Victoria Krakovna",
        "Vikrant Varma",
        "Vladimir Mikulik",
        "Zachary Kenton",
        "Dave Orr",
        "Shane Legg",
        "Noah Goodman",
        "Allan Dafoe",
        "Four Flynn",
        "Anca Dragan"
      ],
      "abstract": "Artificial General Intelligence (AGI) promises transformative benefits but\nalso presents significant risks. We develop an approach to address the risk of\nharms consequential enough to significantly harm humanity. We identify four\nareas of risk: misuse, misalignment, mistakes, and structural risks. Of these,\nwe focus on technical approaches to misuse and misalignment. For misuse, our\nstrategy aims to prevent threat actors from accessing dangerous capabilities,\nby proactively identifying dangerous capabilities, and implementing robust\nsecurity, access restrictions, monitoring, and model safety mitigations. To\naddress misalignment, we outline two lines of defense. First, model-level\nmitigations such as amplified oversight and robust training can help to build\nan aligned model. Second, system-level security measures such as monitoring and\naccess control can mitigate harm even if the model is misaligned. Techniques\nfrom interpretability, uncertainty estimation, and safer design patterns can\nenhance the effectiveness of these mitigations. Finally, we briefly outline how\nthese ingredients could be combined to produce safety cases for AGI systems.",
      "tldr_zh": "本文提出了一种解决通用人工智能(AGI)安全和保障问题的方法，旨在应对AGI可能对人类造成的重大危害风险。文章识别了滥用、未对齐、错误和结构性风险四个主要风险领域，并重点关注滥用和未对齐的技术应对方法。针对滥用，该策略旨在通过主动识别危险能力、实施强大的安全措施、访问限制、监控和模型安全缓解措施，来防止威胁行为者获取危险能力。为解决未对齐问题，提出了模型层面的缓解措施（如放大监督和稳健训练）以及系统层面的安全措施（如监控和访问控制），并结合可解释性、不确定性估计和更安全的设计模式来增强这些缓解措施的有效性。最后，文章简要概述了如何将这些要素结合起来，为AGI系统生成安全案例。\n",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01849v1",
      "published_date": "2025-04-02 15:59:31 UTC",
      "updated_date": "2025-04-02 15:59:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:07:12.851539"
    },
    {
      "arxiv_id": "2504.01848v1",
      "title": "PaperBench: Evaluating AI's Ability to Replicate AI Research",
      "title_zh": "PaperBench：评估 AI 复制 AI 研究的能力",
      "authors": [
        "Giulio Starace",
        "Oliver Jaffe",
        "Dane Sherburn",
        "James Aung",
        "Jun Shern Chan",
        "Leon Maksin",
        "Rachel Dias",
        "Evan Mays",
        "Benjamin Kinsella",
        "Wyatt Thompson",
        "Johannes Heidecke",
        "Amelia Glaese",
        "Tejal Patwardhan"
      ],
      "abstract": "We introduce PaperBench, a benchmark evaluating the ability of AI agents to\nreplicate state-of-the-art AI research. Agents must replicate 20 ICML 2024\nSpotlight and Oral papers from scratch, including understanding paper\ncontributions, developing a codebase, and successfully executing experiments.\nFor objective evaluation, we develop rubrics that hierarchically decompose each\nreplication task into smaller sub-tasks with clear grading criteria. In total,\nPaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed\nwith the author(s) of each ICML paper for accuracy and realism. To enable\nscalable evaluation, we also develop an LLM-based judge to automatically grade\nreplication attempts against rubrics, and assess our judge's performance by\ncreating a separate benchmark for judges. We evaluate several frontier models\non PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet\n(New) with open-source scaffolding, achieves an average replication score of\n21.0\\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench,\nfinding that models do not yet outperform the human baseline. We\n\\href{https://github.com/openai/preparedness}{open-source our code} to\nfacilitate future research in understanding the AI engineering capabilities of\nAI agents.",
      "tldr_zh": "PaperBench是一个用于评估AI智能体复制最先进AI研究能力的基准测试。该基准要求智能体从零开始复制20篇ICML 2024的聚光灯和口头报告论文，包括理解论文贡献、开发代码库和成功执行实验。为了客观评估，PaperBench开发了分级标准，将每个复制任务分解为具有明确评分标准的较小子任务，总共包含8,316个可单独评分的任务。这些标准与每篇ICML论文的作者共同开发，以确保准确性和真实性。为了实现可扩展的评估，PaperBench还开发了一个基于LLM的评判器，用于根据标准自动评分复制尝试，并通过创建一个单独的评判器基准来评估其性能。在PaperBench上评估了几种前沿模型，发现性能最佳的测试智能体Claude 3.5 Sonnet (New)在开源支架的帮助下，平均复制得分为21.0%。最后，PaperBench招募了顶尖的机器学习博士来尝试PaperBench的一个子集，发现模型的性能尚未超过人类基线。该项目开源了代码，以促进未来对AI智能体的AI工程能力的研究。\n",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "30 pages, 14 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.01848v1",
      "published_date": "2025-04-02 15:55:24 UTC",
      "updated_date": "2025-04-02 15:55:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:07:25.282925"
    },
    {
      "arxiv_id": "2504.01833v1",
      "title": "YourBench: Easy Custom Evaluation Sets for Everyone",
      "title_zh": "YourBench：人人可用的简易自定义评估集\n",
      "authors": [
        "Sumuk Shashidhar",
        "Clémentine Fourrier",
        "Alina Lozovskia",
        "Thomas Wolf",
        "Gokhan Tur",
        "Dilek Hakkani-Tür"
      ],
      "abstract": "Evaluating large language models (LLMs) effectively remains a critical\nbottleneck, as traditional static benchmarks suffer from saturation and\ncontamination, while human evaluations are costly and slow. This hinders timely\nor domain-specific assessment, crucial for real-world applications. We\nintroduce YourBench, a novel, open-source framework that addresses these\nlimitations by enabling dynamic, automated generation of reliable, up-to-date,\nand domain-tailored benchmarks cheaply and without manual annotation, directly\nfrom user-provided documents. We demonstrate its efficacy by replicating 7\ndiverse MMLU subsets using minimal source text, achieving this for under 15 USD\nin total inference costs while perfectly preserving the relative model\nperformance rankings (Spearman Rho = 1) observed on the original benchmark. To\nensure that YourBench generates data grounded in provided input instead of\nrelying on posterior parametric knowledge in models, we also introduce\nTempora-0325, a novel dataset of over 7K diverse documents, published\nexclusively after March 2025. Our comprehensive analysis spans 26 SoTA models\nfrom 7 major families across varying scales (3-671B parameters) to validate the\nquality of generated evaluations through rigorous algorithmic checks (e.g.,\ncitation grounding) and human assessments. We release the YourBench library,\nthe Tempora-0325 dataset, 150k+ question answer pairs based on Tempora and all\nevaluation and inference traces to facilitate reproducible research and empower\nthe community to generate bespoke benchmarks on demand, fostering more relevant\nand trustworthy LLM evaluation.",
      "tldr_zh": "YourBench是一个开源框架，旨在通过动态、自动生成可靠、最新且特定领域的评测基准来解决LLM评估的瓶颈问题。它允许用户利用提供的文档低成本地创建自定义评估集，无需手动标注。研究表明，YourBench能够以极低的推理成本（低于15美元）复制MMLU子集，并完美保留模型性能排名。为了保证生成数据的可靠性，作者还引入了Tempora-0325数据集，该数据集包含2025年3月之后发布的7K+文档。通过对26个SoTA模型进行分析，验证了生成评估的质量。该研究发布了YourBench库、Tempora-0325数据集以及超过15万个基于Tempora的问答对，以促进可重复的研究和定制化LLM评估。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.1"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01833v1",
      "published_date": "2025-04-02 15:40:24 UTC",
      "updated_date": "2025-04-02 15:40:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:07:36.952946"
    },
    {
      "arxiv_id": "2504.01819v1",
      "title": "Implicit Bias Injection Attacks against Text-to-Image Diffusion Models",
      "title_zh": "针对文本到图像扩散模型的隐式偏见注入攻击\n",
      "authors": [
        "Huayang Huang",
        "Xiangye Jin",
        "Jiaxu Miao",
        "Yu Wu"
      ],
      "abstract": "The proliferation of text-to-image diffusion models (T2I DMs) has led to an\nincreased presence of AI-generated images in daily life. However, biased T2I\nmodels can generate content with specific tendencies, potentially influencing\npeople's perceptions. Intentional exploitation of these biases risks conveying\nmisleading information to the public. Current research on bias primarily\naddresses explicit biases with recognizable visual patterns, such as skin color\nand gender. This paper introduces a novel form of implicit bias that lacks\nexplicit visual features but can manifest in diverse ways across various\nsemantic contexts. This subtle and versatile nature makes this bias challenging\nto detect, easy to propagate, and adaptable to a wide range of scenarios. We\nfurther propose an implicit bias injection attack framework (IBI-Attacks)\nagainst T2I diffusion models by precomputing a general bias direction in the\nprompt embedding space and adaptively adjusting it based on different inputs.\nOur attack module can be seamlessly integrated into pre-trained diffusion\nmodels in a plug-and-play manner without direct manipulation of user input or\nmodel retraining. Extensive experiments validate the effectiveness of our\nscheme in introducing bias through subtle and diverse modifications while\npreserving the original semantics. The strong concealment and transferability\nof our attack across various scenarios further underscore the significance of\nour approach. Code is available at https://github.com/Hannah1102/IBI-attacks.",
      "tldr_zh": "该论文提出了一种针对文本到图像扩散模型(T2I DMs)的隐式偏见注入攻击(IBI-Attacks)框架，旨在利用模型中不易察觉的偏见来生成具有特定倾向的内容。与以往关注肤色、性别等显式偏见的研究不同，该方法通过预先计算提示嵌入空间中的通用偏见方向，并根据不同的输入进行自适应调整，从而在不改变原始语义的情况下引入微妙且多样化的偏见。该攻击模块可以即插即用地集成到预训练的扩散模型中，无需直接操纵用户输入或重新训练模型。实验结果表明，该方法能够有效地注入偏见，并且具有很强的隐蔽性和跨场景的迁移性。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accept to CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.01819v1",
      "published_date": "2025-04-02 15:24:12 UTC",
      "updated_date": "2025-04-02 15:24:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:07:48.789040"
    },
    {
      "arxiv_id": "2504.01798v1",
      "title": "A Novel Approach To Implementing Knowledge Distillation In Tsetlin Machines",
      "title_zh": "一种在 Tsetlin 机中实现知识蒸馏的新方法\n",
      "authors": [
        "Calvin Kinateder"
      ],
      "abstract": "The Tsetlin Machine (TM) is a propositional logic based model that uses\nconjunctive clauses to learn patterns from data. As with typical neural\nnetworks, the performance of a Tsetlin Machine is largely dependent on its\nparameter count, with a larger number of parameters producing higher accuracy\nbut slower execution. Knowledge distillation in neural networks transfers\ninformation from an already-trained teacher model to a smaller student model to\nincrease accuracy in the student without increasing execution time. We propose\na novel approach to implementing knowledge distillation in Tsetlin Machines by\nutilizing the probability distributions of each output sample in the teacher to\nprovide additional context to the student. Additionally, we propose a novel\nclause-transfer algorithm that weighs the importance of each clause in the\nteacher and initializes the student with only the most essential data. We find\nthat our algorithm can significantly improve performance in the student model\nwithout negatively impacting latency in the tested domains of image recognition\nand text classification.",
      "tldr_zh": "该论文提出了一种新的知识蒸馏方法，用于提升Tsetlin Machine (TM)的性能。该方法利用已训练的教师模型中每个输出样本的概率分布，为学生模型提供额外的上下文信息。此外，还提出了一种新的clause-transfer算法，该算法对教师模型中每个clause的重要性进行加权，并仅使用最关键的数据初始化学生模型。实验结果表明，该算法能够显著提高学生模型的性能，且不会对图像识别和文本分类领域的延迟产生负面影响。\n",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "Master's Thesis. 75 pages, 30 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.01798v1",
      "published_date": "2025-04-02 15:06:27 UTC",
      "updated_date": "2025-04-02 15:06:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:08:00.817125"
    },
    {
      "arxiv_id": "2504.01797v1",
      "title": "Rethinking industrial artificial intelligence: a unified foundation framework",
      "title_zh": "重新思考工业人工智能：一个统一的基础框架\n",
      "authors": [
        "Jay Lee",
        "Hanqi Su"
      ],
      "abstract": "Recent advancement in industrial artificial intelligence (AI) is reshaping\nthe industry, driving smarter manufacturing, predictive maintenance, and\nintelligent decision-making. However, existing approaches often focus primarily\non algorithms and models, overlooking the importance of systematically\nintegrating domain knowledge, data, and models to ensure more comprehensive and\neffective AI solutions. Therefore, the effective development and deployment of\nIndustrial AI solutions require a more comprehensive and systematic approach.\nTo address this gap, this paper summarizes previous research and rethinks the\nrole of industrial AI and presents a unified industrial AI foundation framework\ncomprising three core modules: knowledge module, data module, and model module.\nThese modules help to extend and enhance the industrial AI methodology\nplatform, supporting various industrial applications. In addition, a case study\non rotating machinery diagnosis demonstrates the framework's effectiveness, and\nseveral future directions are highlighted for the development of the industrial\nAI foundation framework.",
      "tldr_zh": "本文重新思考了工业人工智能(Industrial AI)的角色，并提出了一个统一的工业AI基础框架，该框架包含三个核心模块：知识模块、数据模块和模型模块。该框架旨在通过系统地整合领域知识、数据和模型，解决现有方法过度关注算法和模型，缺乏全面性和有效性的问题。这些模块扩展并增强了工业AI方法论平台，支持各种工业应用。通过旋转机械诊断的案例研究，验证了该框架的有效性，并指出了未来发展方向。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "The paper submitted to IJAMD, the International Journal of AI for\n  Materials and Design, has been accepted",
      "pdf_url": "http://arxiv.org/pdf/2504.01797v1",
      "published_date": "2025-04-02 15:05:32 UTC",
      "updated_date": "2025-04-02 15:05:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:08:12.439937"
    },
    {
      "arxiv_id": "2504.01783v1",
      "title": "CLaP -- State Detection from Time Series",
      "title_zh": "CLaP：从时间序列中进行状态检测\n",
      "authors": [
        "Arik Ermshaus",
        "Patrick Schäfer",
        "Ulf Leser"
      ],
      "abstract": "The ever-growing amount of sensor data from machines, smart devices, and the\nenvironment leads to an abundance of high-resolution, unannotated time series\n(TS). These recordings encode the recognizable properties of latent states and\ntransitions from physical phenomena that can be modelled as abstract processes.\nThe unsupervised localization and identification of these states and their\ntransitions is the task of time series state detection (TSSD). We introduce\nCLaP, a new, highly accurate and efficient algorithm for TSSD. It leverages the\npredictive power of time series classification for TSSD in an unsupervised\nsetting by applying novel self-supervision techniques to detect whether data\nsegments emerge from the same state or not. To this end, CLaP cross-validates a\nclassifier with segment-labelled subsequences to quantify confusion between\nsegments. It merges labels from segments with high confusion, representing the\nsame latent state, if this leads to an increase in overall classification\nquality. We conducted an experimental evaluation using 391 TS from four\nbenchmarks and found CLaP to be significantly more precise in detecting states\nthan five state-of-the-art competitors. It achieves the best accuracy-runtime\ntradeoff and is scalable to large TS. We provide a Python implementation of\nCLaP, which can be deployed in TS analysis workflows.",
      "tldr_zh": "本文提出了一种新的高精度、高效率的时间序列状态检测算法CLaP。CLaP利用时间序列分类的预测能力，通过新颖的自监督技术来判断数据段是否来自同一状态，从而在无监督环境下进行时间序列状态检测(TSSD)。该方法通过交叉验证分类器和带有标签的子序列来量化段之间的混淆程度。如果合并具有高度混淆的段的标签（代表相同的潜在状态）能够提高整体分类质量，则进行合并。在四个基准数据集的391个时间序列上的实验评估表明，CLaP在检测状态方面比五个最先进的竞争对手更精确，实现了最佳的精度-运行时间平衡，并且可以扩展到大型时间序列。作者提供了CLaP的Python实现，可以部署在时间序列分析工作流程中。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01783v1",
      "published_date": "2025-04-02 14:46:42 UTC",
      "updated_date": "2025-04-02 14:46:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:08:24.884310"
    },
    {
      "arxiv_id": "2504.01771v1",
      "title": "Enhancing Interpretability in Generative AI Through Search-Based Data Influence Analysis",
      "title_zh": "通过基于搜索的数据影响分析增强生成式AI的可解释性\n",
      "authors": [
        "Theodoros Aivalis",
        "Iraklis A. Klampanos",
        "Antonis Troumpoukis",
        "Joemon M. Jose"
      ],
      "abstract": "Generative AI models offer powerful capabilities but often lack transparency,\nmaking it difficult to interpret their output. This is critical in cases\ninvolving artistic or copyrighted content. This work introduces a\nsearch-inspired approach to improve the interpretability of these models by\nanalysing the influence of training data on their outputs. Our method provides\nobservational interpretability by focusing on a model's output rather than on\nits internal state. We consider both raw data and latent-space embeddings when\nsearching for the influence of data items in generated content. We evaluate our\nmethod by retraining models locally and by demonstrating the method's ability\nto uncover influential subsets in the training data. This work lays the\ngroundwork for future extensions, including user-based evaluations with domain\nexperts, which is expected to improve observational interpretability further.",
      "tldr_zh": "该研究提出了一种基于搜索的数据影响分析方法，旨在提高生成式AI模型的可解释性，尤其是在涉及艺术或版权内容时。该方法通过分析训练数据对模型输出的影响，提供了一种可观察的解释性，侧重于模型输出而非内部状态。研究人员在搜索数据项对生成内容的影响时，考虑了原始数据和潜在空间嵌入。通过局部重训练模型并展示该方法发现训练数据中具有影响力的子集的能力，验证了该方法的有效性。该研究为未来的扩展奠定了基础，包括与领域专家进行基于用户的评估，有望进一步提高可观察的解释性。\n",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01771v1",
      "published_date": "2025-04-02 14:29:37 UTC",
      "updated_date": "2025-04-02 14:29:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:08:36.600918"
    },
    {
      "arxiv_id": "2504.01767v1",
      "title": "Leveraging Embedding Techniques in Multimodal Machine Learning for Mental Illness Assessment",
      "title_zh": "利用多模态机器学习中的嵌入技术进行精神疾病评估\n",
      "authors": [
        "Abdelrahaman A. Hassan",
        "Abdelrahman A. Ali",
        "Aya E. Fouda",
        "Radwa J. Hanafy",
        "Mohammed E. Fouda"
      ],
      "abstract": "The increasing global prevalence of mental disorders, such as depression and\nPTSD, requires objective and scalable diagnostic tools. Traditional clinical\nassessments often face limitations in accessibility, objectivity, and\nconsistency. This paper investigates the potential of multimodal machine\nlearning to address these challenges, leveraging the complementary information\navailable in text, audio, and video data. Our approach involves a comprehensive\nanalysis of various data preprocessing techniques, including novel chunking and\nutterance-based formatting strategies. We systematically evaluate a range of\nstate-of-the-art embedding models for each modality and employ Convolutional\nNeural Networks (CNNs) and Bidirectional LSTM Networks (BiLSTMs) for feature\nextraction. We explore data-level, feature-level, and decision-level fusion\ntechniques, including a novel integration of Large Language Model (LLM)\npredictions. We also investigate the impact of replacing Multilayer Perceptron\nclassifiers with Support Vector Machines. We extend our analysis to severity\nprediction using PHQ-8 and PCL-C scores and multi-class classification\n(considering co-occurring conditions). Our results demonstrate that\nutterance-based chunking significantly improves performance, particularly for\ntext and audio modalities. Decision-level fusion, incorporating LLM\npredictions, achieves the highest accuracy, with a balanced accuracy of 94.8%\nfor depression and 96.2% for PTSD detection. The combination of CNN-BiLSTM\narchitectures with utterance-level chunking, coupled with the integration of\nexternal LLM, provides a powerful and nuanced approach to the detection and\nassessment of mental health conditions. Our findings highlight the potential of\nMMML for developing more accurate, accessible, and personalized mental\nhealthcare tools.",
      "tldr_zh": "该研究探索了多模态机器学习(MMML)在精神疾病评估中的应用，旨在解决传统临床评估在可及性、客观性和一致性方面的局限性。研究分析了文本、音频和视频数据的多种预处理技术，包括新的分块和基于语气的格式化策略，并评估了各种模态的先进嵌入模型。通过卷积神经网络(CNN)和双向LSTM网络(BiLSTM)进行特征提取，并探索了数据级、特征级和决策级融合技术，包括大型语言模型(LLM)预测的新集成。结果表明，基于语气的分块显著提高了性能，特别是对于文本和音频模态。结合LLM预测的决策级融合实现了最高的准确率，抑郁症的平衡准确率为94.8%，PTSD检测的平衡准确率为96.2%。该研究强调了MMML在开发更准确、可访问和个性化的精神健康护理工具方面的潜力。\n",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01767v1",
      "published_date": "2025-04-02 14:19:06 UTC",
      "updated_date": "2025-04-02 14:19:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:08:48.988805"
    },
    {
      "arxiv_id": "2504.01764v1",
      "title": "Dual-stream Transformer-GCN Model with Contextualized Representations Learning for Monocular 3D Human Pose Estimation",
      "title_zh": "基于语境化表征学习的双流 Transformer-GCN 模型，用于单目 3D 人体姿态估计\n",
      "authors": [
        "Mingrui Ye",
        "Lianping Yang",
        "Hegui Zhu",
        "Zenghao Zheng",
        "Xin Wang",
        "Yantao Lo"
      ],
      "abstract": "This paper introduces a novel approach to monocular 3D human pose estimation\nusing contextualized representation learning with the Transformer-GCN\ndual-stream model. Monocular 3D human pose estimation is challenged by depth\nambiguity, limited 3D-labeled training data, imbalanced modeling, and\nrestricted model generalization. To address these limitations, our work\nintroduces a groundbreaking motion pre-training method based on contextualized\nrepresentation learning. Specifically, our method involves masking 2D pose\nfeatures and utilizing a Transformer-GCN dual-stream model to learn\nhigh-dimensional representations through a self-distillation setup. By focusing\non contextualized representation learning and spatial-temporal modeling, our\napproach enhances the model's ability to understand spatial-temporal\nrelationships between postures, resulting in superior generalization.\nFurthermore, leveraging the Transformer-GCN dual-stream model, our approach\neffectively balances global and local interactions in video pose estimation.\nThe model adaptively integrates information from both the Transformer and GCN\nstreams, where the GCN stream effectively learns local relationships between\nadjacent key points and frames, while the Transformer stream captures\ncomprehensive global spatial and temporal features. Our model achieves\nstate-of-the-art performance on two benchmark datasets, with an MPJPE of 38.0mm\nand P-MPJPE of 31.9mm on Human3.6M, and an MPJPE of 15.9mm on MPI-INF-3DHP.\nFurthermore, visual experiments on public datasets and in-the-wild videos\ndemonstrate the robustness and generalization capabilities of our approach.",
      "tldr_zh": "本文提出了一种新颖的单目3D人体姿态估计方法，该方法利用Transformer-GCN双流模型进行上下文表示学习。该方法通过掩蔽2D姿态特征，并利用Transformer-GCN双流模型，通过自蒸馏设置学习高维表示，从而进行运动预训练。Transformer流捕获全局时空特征，而GCN流学习相邻关键点和帧之间的局部关系，从而平衡全局和局部交互。在Human3.6M和MPI-INF-3DHP两个基准数据集上，该模型取得了state-of-the-art的性能，MPJPE分别为38.0mm和15.9mm。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01764v1",
      "published_date": "2025-04-02 14:17:57 UTC",
      "updated_date": "2025-04-02 14:17:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:09:00.667254"
    },
    {
      "arxiv_id": "2504.01738v1",
      "title": "Style over Substance: Distilled Language Models Reason Via Stylistic Replication",
      "title_zh": "形式重于内容：精馏语言模型通过文体复制进行推理\n",
      "authors": [
        "Philip Lippmann",
        "Jie Yang"
      ],
      "abstract": "Specialized reasoning language models (RLMs) have demonstrated that scaling\ntest-time computation through detailed reasoning traces significantly enhances\nperformance. Although these traces effectively facilitate knowledge\ndistillation into smaller, instruction-tuned models, the precise nature of\ntransferred reasoning remains unclear. In this study, we investigate to what\nextent distilled models internalize replicated stylistic patterns during\nreasoning. To this end, we systematically analyze reasoning traces, identifying\nstructural and lexical patterns that characterize successful reasoning. We then\nintroduce two new datasets -- a dataset of emergent reasoning traces and a\nsynthetic dataset explicitly constructed to replicate these stylistic patterns\n-- to precisely examine their influence on distilled models' reasoning\ncapabilities. We find that models trained on the synthetic traces achieve\ncomparable performance, indicating that distilled reasoning abilities rely\nsignificantly on surface-level patterns. Surprisingly, we observe an increase\nin performance even when the synthetic traces are altered to lead to the wrong\nanswer. Our findings highlight how stylistic patterns can be leveraged to\nefficiently enhance LM reasoning across diverse model families.",
      "tldr_zh": "该研究探讨了精馏语言模型(distilled language models)在推理过程中，究竟在多大程度上内化了复制的风格模式。通过分析推理轨迹，研究人员识别出表征成功推理的结构和词汇模式。他们构建了包含涌现推理轨迹的数据集和显式复制这些风格模式的合成数据集，并用以训练模型。实验发现，在合成轨迹上训练的模型取得了可比的性能，表明精馏的推理能力很大程度上依赖于表面层级的模式。令人惊讶的是，即使合成轨迹被修改为导致错误答案，性能仍然有所提高。研究结果表明，风格模式可以有效地提升不同模型系列的语言模型推理能力。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01738v1",
      "published_date": "2025-04-02 13:50:20 UTC",
      "updated_date": "2025-04-02 13:50:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:09:12.869928"
    },
    {
      "arxiv_id": "2504.01735v1",
      "title": "AdPO: Enhancing the Adversarial Robustness of Large Vision-Language Models with Preference Optimization",
      "title_zh": "AdPO：通过偏好优化增强大型视觉-语言模型的对抗鲁棒性\n",
      "authors": [
        "Chaohu Liu",
        "Tianyi Gui",
        "Yu Liu",
        "Linli Xu"
      ],
      "abstract": "Large Vision-Language Models (LVLMs), such as GPT-4o and LLaVA, have recently\nwitnessed remarkable advancements and are increasingly being deployed in\nreal-world applications. However, inheriting the sensitivity of visual neural\nnetworks, LVLMs remain vulnerable to adversarial attacks, which can result in\nerroneous or malicious outputs. While existing efforts utilize adversarial\nfine-tuning to enhance robustness, they often suffer from performance\ndegradation on clean inputs. In this paper, we proposes AdPO, a novel\nadversarial defense strategy for LVLMs based on preference optimization. For\nthe first time, we reframe adversarial training as a preference optimization\nproblem, aiming to enhance the model's preference for generating normal outputs\non clean inputs while rejecting the potential misleading outputs for\nadversarial examples. Notably, AdPO achieves this by solely modifying the image\nencoder, e.g., CLIP ViT, resulting in superior clean and adversarial\nperformance in a variety of downsream tasks. Considering that training involves\nlarge language models (LLMs), the computational cost increases significantly.\nWe validate that training on smaller LVLMs and subsequently transferring to\nlarger models can achieve competitive performance while maintaining efficiency\ncomparable to baseline methods. Our comprehensive experiments confirm the\neffectiveness of the proposed AdPO, which provides a novel perspective for\nfuture adversarial defense research.",
      "tldr_zh": "该论文提出了一种新的对抗防御策略AdPO，通过偏好优化来增强大型视觉语言模型(LVLMs)的对抗鲁棒性。AdPO将对抗训练重新定义为一个偏好优化问题，旨在提高模型在干净输入上生成正常输出的偏好，同时拒绝对抗样本可能产生的误导性输出。AdPO仅修改图像编码器(如CLIP ViT)即可实现，从而在各种下游任务中获得优越的干净和对抗性能。研究表明，在较小的LVLM上进行训练，然后迁移到较大的模型上，可以在保持与基线方法相当的效率的同时，获得具有竞争力的性能。实验结果验证了AdPO的有效性，为未来的对抗防御研究提供了一个新的视角。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01735v1",
      "published_date": "2025-04-02 13:43:21 UTC",
      "updated_date": "2025-04-02 13:43:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:09:24.780050"
    },
    {
      "arxiv_id": "2504.01733v1",
      "title": "Epistemic Skills: Reasoning about Knowledge and Oblivion",
      "title_zh": "认知技能：关于知识和遗忘的推理\n",
      "authors": [
        "Xiaolong Liang",
        "Yì N. Wáng"
      ],
      "abstract": "This paper presents a class of epistemic logics that captures the dynamics of\nacquiring knowledge and descending into oblivion, while incorporating concepts\nof group knowledge. The approach is grounded in a system of weighted models,\nintroducing an ``epistemic skills'' metric to represent the epistemic\ncapacities tied to knowledge updates. Within this framework, knowledge\nacquisition is modeled as a process of upskilling, whereas oblivion is\nrepresented as a consequence of downskilling. The framework further enables\nexploration of ``knowability'' and ``forgettability,'' defined as the potential\nto gain knowledge through upskilling and to lapse into oblivion through\ndownskilling, respectively. Additionally, it supports a detailed analysis of\nthe distinctions between epistemic de re and de dicto expressions. The\ncomputational complexity of the model checking and satisfiability problems is\nexamined, offering insights into their theoretical foundations and practical\nimplications.",
      "tldr_zh": "本文提出了一类认知逻辑，用于捕捉获取知识和遗忘的动态过程，并结合了群体知识的概念。该方法基于加权模型，引入了“认知技能”指标来表示与知识更新相关的认知能力。知识获取被建模为一种技能提升的过程，而遗忘则被表示为技能下降的结果。该框架进一步探索了“可知性”和“可遗忘性”，分别定义为通过技能提升获得知识和通过技能下降陷入遗忘的可能性。此外，它还支持对认知*de re*和*de dicto*表达之间差异的详细分析。最后，论文还研究了模型检查和可满足性问题的计算复杂度，为它们的理论基础和实际应用提供了见解。\n",
      "categories": [
        "cs.AI",
        "cs.CC",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01733v1",
      "published_date": "2025-04-02 13:41:42 UTC",
      "updated_date": "2025-04-02 13:41:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:09:36.828768"
    },
    {
      "arxiv_id": "2504.01724v2",
      "title": "DreamActor-M1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance",
      "title_zh": "DreamActor-M1：基于混合引导的整体、富有表现力且鲁棒的人体图像动画\n",
      "authors": [
        "Yuxuan Luo",
        "Zhengkun Rong",
        "Lizhen Wang",
        "Longhao Zhang",
        "Tianshu Hu",
        "Yongming Zhu"
      ],
      "abstract": "While recent image-based human animation methods achieve realistic body and\nfacial motion synthesis, critical gaps remain in fine-grained holistic\ncontrollability, multi-scale adaptability, and long-term temporal coherence,\nwhich leads to their lower expressiveness and robustness. We propose a\ndiffusion transformer (DiT) based framework, DreamActor-M1, with hybrid\nguidance to overcome these limitations. For motion guidance, our hybrid control\nsignals that integrate implicit facial representations, 3D head spheres, and 3D\nbody skeletons achieve robust control of facial expressions and body movements,\nwhile producing expressive and identity-preserving animations. For scale\nadaptation, to handle various body poses and image scales ranging from\nportraits to full-body views, we employ a progressive training strategy using\ndata with varying resolutions and scales. For appearance guidance, we integrate\nmotion patterns from sequential frames with complementary visual references,\nensuring long-term temporal coherence for unseen regions during complex\nmovements. Experiments demonstrate that our method outperforms the\nstate-of-the-art works, delivering expressive results for portraits,\nupper-body, and full-body generation with robust long-term consistency. Project\nPage: https://grisoon.github.io/DreamActor-M1/.",
      "tldr_zh": "DreamActor-M1是一个基于扩散Transformer (DiT) 的框架，旨在实现更全面、更具表现力且更鲁棒的人体图像动画。它通过混合引导克服了现有方法在精细控制、多尺度适应性和长期时间一致性方面的不足。该方法结合隐式面部表示、3D头部球体和3D身体骨架，实现对表情和身体运动的精确控制，同时保持身份信息。通过渐进式训练策略处理不同分辨率和尺度的图像，适应各种姿势和图像比例。此外，它还整合了来自连续帧的运动模式和视觉参考，确保复杂运动中未见区域的长期时间一致性。实验表明，DreamActor-M1在肖像、上半身和全身生成方面优于现有技术，并具有强大的长期一致性。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01724v2",
      "published_date": "2025-04-02 13:30:32 UTC",
      "updated_date": "2025-04-03 14:51:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:09:49.078819"
    },
    {
      "arxiv_id": "2504.01707v2",
      "title": "InfiniteICL: Breaking the Limit of Context Window Size via Long Short-term Memory Transformation",
      "title_zh": "InfiniteICL：通过长短期记忆转换打破上下文窗口大小的限制\n",
      "authors": [
        "Bowen Cao",
        "Deng Cai",
        "Wai Lam"
      ],
      "abstract": "In-context learning (ICL) is critical for large language models (LLMs), but\nits effectiveness is constrained by finite context windows, particularly in\nultra-long contexts. To overcome this, we introduce InfiniteICL, a framework\nthat parallels context and parameters in LLMs with short- and long-term memory\nin human cognitive systems, focusing on transforming temporary context\nknowledge into permanent parameter updates. This approach significantly reduces\nmemory usage, maintains robust performance across varying input lengths, and\ntheoretically enables infinite context integration through the principles of\ncontext knowledge elicitation, selection, and consolidation. Evaluations\ndemonstrate that our method reduces context length by 90% while achieving 103%\naverage performance of full-context prompting across fact recall, grounded\nreasoning, and skill acquisition tasks. When conducting sequential multi-turn\ntransformations on complex, real-world contexts (with length up to 2M tokens),\nour approach surpasses full-context prompting while using only 0.4% of the\noriginal contexts. These findings highlight InfiniteICL's potential to enhance\nthe scalability and efficiency of LLMs by breaking the limitations of\nconventional context window sizes.",
      "tldr_zh": "该论文提出了InfiniteICL框架，旨在突破大型语言模型(LLMs)上下文窗口大小的限制。InfiniteICL将LLM中的上下文和参数分别类比于人类认知系统中的短期和长期记忆，重点是将临时上下文知识转化为永久参数更新。该方法通过上下文知识提取、选择和巩固等步骤，显著降低内存使用，并在不同输入长度下保持稳健的性能，理论上实现了无限上下文集成。实验表明，InfiniteICL在事实回忆、基础推理和技能获取任务中，减少了90%的上下文长度，同时达到了全上下文提示103%的平均性能。在复杂、真实的上下文（长度高达2M tokens）上进行连续多轮转换时，该方法超越了全上下文提示，同时仅使用了原始上下文的0.4%。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01707v2",
      "published_date": "2025-04-02 13:15:44 UTC",
      "updated_date": "2025-04-03 08:53:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:10:00.953847"
    },
    {
      "arxiv_id": "2504.01705v1",
      "title": "Sky of Unlearning (SoUL): Rewiring Federated Machine Unlearning via Selective Pruning",
      "title_zh": "遗忘之天 (SoUL)：通过选择性剪枝重塑联邦机器遗忘\n",
      "authors": [
        "Md Mahabub Uz Zaman",
        "Xiang Sun",
        "Jingjing Yao"
      ],
      "abstract": "The Internet of Drones (IoD), where drones collaborate in data collection and\nanalysis, has become essential for applications such as surveillance and\nenvironmental monitoring. Federated learning (FL) enables drones to train\nmachine learning models in a decentralized manner while preserving data\nprivacy. However, FL in IoD networks is susceptible to attacks like data\npoisoning and model inversion. Federated unlearning (FU) mitigates these risks\nby eliminating adversarial data contributions, preventing their influence on\nthe model. This paper proposes sky of unlearning (SoUL), a federated unlearning\nframework that efficiently removes the influence of unlearned data while\nmaintaining model performance. A selective pruning algorithm is designed to\nidentify and remove neurons influential in unlearning but minimally impact the\noverall performance of the model. Simulations demonstrate that SoUL outperforms\nexisting unlearning methods, achieves accuracy comparable to full retraining,\nand reduces computation and communication overhead, making it a scalable and\nefficient solution for resource-constrained IoD networks.",
      "tldr_zh": "本文提出了一种名为“Sky of Unlearning (SoUL)”的联邦学习卸载框架，旨在高效地移除无人机网络（IoD）中受污染数据对模型的影响，同时保持模型性能。SoUL采用选择性剪枝算法，识别并移除对卸载过程影响较大但对模型整体性能影响较小的神经元。仿真结果表明，SoUL优于现有的卸载方法，其精度与完全重新训练相当，并降低了计算和通信开销，使其成为资源受限的IoD网络的可扩展和高效的解决方案。该研究针对无人机协同数据收集和分析场景下的联邦学习安全问题，提供了一种实用的解决方案。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 6 figures, IEEE International Conference on Communications\n  (ICC 2025)",
      "pdf_url": "http://arxiv.org/pdf/2504.01705v1",
      "published_date": "2025-04-02 13:07:30 UTC",
      "updated_date": "2025-04-02 13:07:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:10:12.936158"
    },
    {
      "arxiv_id": "2504.01700v1",
      "title": "Reasoning LLMs for User-Aware Multimodal Conversational Agents",
      "title_zh": "用于用户感知多模态对话代理的推理LLM\n",
      "authors": [
        "Hamed Rahimi",
        "Jeanne Cattoni",
        "Meriem Beghili",
        "Mouad Abrini",
        "Mahdi Khoramshahi",
        "Maribel Pino",
        "Mohamed Chetouani"
      ],
      "abstract": "Personalization in social robotics is critical for fostering effective\nhuman-robot interactions, yet systems often face the cold start problem, where\ninitial user preferences or characteristics are unavailable. This paper\nproposes a novel framework called USER-LLM R1 for a user-aware conversational\nagent that addresses this challenge through dynamic user profiling and model\ninitiation. Our approach integrates chain-of-thought (CoT) reasoning models to\niteratively infer user preferences and vision-language models (VLMs) to\ninitialize user profiles from multimodal inputs, enabling personalized\ninteractions from the first encounter. Leveraging a Retrieval-Augmented\nGeneration (RAG) architecture, the system dynamically refines user\nrepresentations within an inherent CoT process, ensuring contextually relevant\nand adaptive responses. Evaluations on the ElderlyTech-VQA Bench demonstrate\nsignificant improvements in ROUGE-1 (+23.2%), ROUGE-2 (+0.6%), and ROUGE-L\n(+8%) F1 scores over state-of-the-art baselines, with ablation studies\nunderscoring the impact of reasoning model size on performance. Human\nevaluations further validate the framework's efficacy, particularly for elderly\nusers, where tailored responses enhance engagement and trust. Ethical\nconsiderations, including privacy preservation and bias mitigation, are\nrigorously discussed and addressed to ensure responsible deployment.",
      "tldr_zh": "本文提出了一种名为USER-LLM R1的新框架，用于构建用户感知的多模态对话智能体，旨在解决社交机器人中个性化交互的冷启动问题。该框架利用链式思维(CoT)推理模型迭代推断用户偏好，并使用视觉语言模型(VLMs)从多模态输入中初始化用户画像，从而实现首次交互的个性化。通过检索增强生成(RAG)架构，系统在CoT过程中动态优化用户表示，确保上下文相关和自适应的响应。在ElderlyTech-VQA基准测试中，该框架在ROUGE指标上显著优于现有技术，并通过人工评估验证了其有效性，尤其是在增强老年用户的参与度和信任感方面。论文还深入探讨并解决了伦理问题，包括隐私保护和偏见缓解。\n",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01700v1",
      "published_date": "2025-04-02 13:00:17 UTC",
      "updated_date": "2025-04-02 13:00:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:10:25.113313"
    },
    {
      "arxiv_id": "2504.01698v1",
      "title": "ToM-RL: Reinforcement Learning Unlocks Theory of Mind in Small LLMs",
      "title_zh": "ToM-RL：强化学习在小型LLM中解锁心智理论\n",
      "authors": [
        "Yi-Long Lu",
        "Chunhui Zhang",
        "Jiajun Song",
        "Lifeng Fan",
        "Wei Wang"
      ],
      "abstract": "Recent advancements in rule-based reinforcement learning (RL), applied during\nthe post-training phase of large language models (LLMs), have significantly\nenhanced their capabilities in structured reasoning tasks such as mathematics\nand logical inference. However, the effectiveness of RL in social reasoning,\nparticularly in Theory of Mind (ToM), the ability to infer others' mental\nstates, remains largely unexplored. In this study, we demonstrate that RL\nmethods effectively unlock ToM reasoning capabilities even in small-scale LLMs\n(0.5B to 7B parameters). Using a modest dataset comprising 3200 questions\nacross diverse scenarios, our RL-trained 7B model achieves 84.50\\% accuracy on\nthe Hi-ToM benchmark, surpassing models like GPT-4o and DeepSeek-v3 despite\nsignificantly fewer parameters. While smaller models ($\\leq$3B parameters)\nsuffer from reasoning collapse, larger models (7B parameters) maintain stable\nperformance through consistent belief tracking. Additionally, our RL-based\nmodels demonstrate robust generalization to higher-order, out-of-distribution\nToM problems, novel textual presentations, and previously unseen datasets.\nThese findings highlight RL's potential to enhance social cognitive reasoning,\nbridging the gap between structured problem-solving and nuanced social\ninference in LLMs.",
      "tldr_zh": "该研究表明，通过规则驱动的强化学习(RL)对小型语言模型(LLMs)进行后训练，可以显著提升其在心理理论(Theory of Mind, ToM)方面的能力。研究者使用包含3200个问题的适量数据集，训练了一个7B参数的模型，在Hi-ToM基准测试中达到了84.50%的准确率，超过了GPT-4o和DeepSeek-v3等更大规模的模型。实验发现，较小的模型（≤3B参数）容易出现推理崩溃，而较大的模型（7B参数）则能通过持续的信念追踪保持稳定的性能。此外，基于RL的模型还展现出对更高阶、分布外的ToM问题、新的文本表示和之前未见过的数据集的泛化能力。这些发现突出了RL在增强LLMs的社会认知推理方面的潜力。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01698v1",
      "published_date": "2025-04-02 12:58:42 UTC",
      "updated_date": "2025-04-02 12:58:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:10:37.193025"
    },
    {
      "arxiv_id": "2504.01692v1",
      "title": "Segmentation variability and radiomics stability for predicting Triple-Negative Breast Cancer subtype using Magnetic Resonance Imaging",
      "title_zh": "基于磁共振成像预测三阴性乳腺癌亚型的分割变异性和放射组学稳定性\n",
      "authors": [
        "Isabella Cama",
        "Alejandro Guzmán",
        "Cristina Campi",
        "Michele Piana",
        "Karim Lekadir",
        "Sara Garbarino",
        "Oliver Díaz"
      ],
      "abstract": "Most papers caution against using predictive models for disease\nstratification based on unselected radiomic features, as these features are\naffected by contouring variability. Instead, they advocate for the use of the\nIntraclass Correlation Coefficient (ICC) as a measure of stability for feature\nselection. However, the direct effect of segmentation variability on the\npredictive models is rarely studied. This study investigates the impact of\nsegmentation variability on feature stability and predictive performance in\nradiomics-based prediction of Triple-Negative Breast Cancer (TNBC) subtype\nusing Magnetic Resonance Imaging. A total of 244 images from the Duke dataset\nwere used, with segmentation variability introduced through modifications of\nmanual segmentations. For each mask, explainable radiomic features were\nselected using the Shapley Additive exPlanations method and used to train\nlogistic regression models. Feature stability across segmentations was assessed\nvia ICC, Pearson's correlation, and reliability scores quantifying the\nrelationship between feature stability and segmentation variability. Results\nindicate that segmentation accuracy does not significantly impact predictive\nperformance. While incorporating peritumoral information may reduce feature\nreproducibility, it does not diminish feature predictive capability. Moreover,\nfeature selection in predictive models is not inherently tied to feature\nstability with respect to segmentation, suggesting that an overreliance on ICC\nor reliability scores for feature selection might exclude valuable predictive\nfeatures.",
      "tldr_zh": "本研究探讨了分割变异性对基于磁共振成像(MRI)的放射组学预测三阴性乳腺癌(TNBC)亚型的影响。研究通过修改手动分割引入分割变异性，并使用Shapley Additive exPlanations方法选择了解释性放射组学特征，用于训练logistic回归模型。结果表明，分割准确性对预测性能没有显著影响，且包含肿瘤周围信息虽然可能降低特征的可重复性，但不会降低特征的预测能力。此外，预测模型中的特征选择与特征分割稳定性并非必然相关，提示过度依赖ICC或可靠性评分进行特征选择可能会排除有价值的预测特征。该研究挑战了传统观点，即必须基于稳定的放射组学特征构建预测模型。\n",
      "categories": [
        "stat.AP",
        "cs.AI",
        "62P10 (Primary), 68T09 (Secondary)"
      ],
      "primary_category": "stat.AP",
      "comment": "22 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.01692v1",
      "published_date": "2025-04-02 12:48:01 UTC",
      "updated_date": "2025-04-02 12:48:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:10:48.919736"
    },
    {
      "arxiv_id": "2504.01690v1",
      "title": "Token Pruning in Audio Transformers: Optimizing Performance and Decoding Patch Importance",
      "title_zh": "音频 Transformer 中的 Token 剪枝：优化性能与解码 Patch 重要性\n",
      "authors": [
        "Taehan Lee",
        "Hyukjun Lee"
      ],
      "abstract": "Vision Transformers (ViTs) have achieved state-of-the-art performance across\nvarious computer vision tasks, but their high computational cost remains a\nchallenge. Token pruning has been proposed to reduce this cost by selectively\nremoving less important tokens. While effective in vision tasks by discarding\nnon-object regions, applying this technique to audio tasks presents unique\nchallenges, as distinguishing relevant from irrelevant regions in\ntime-frequency representations is less straightforward. In this study, for the\nfirst time, we applied token pruning to ViT-based audio classification models\nusing Mel-spectrograms and analyzed the trade-offs between model performance\nand computational cost: TopK token pruning can reduce MAC operations of\nAudioMAE and AST by 30-40%, with less than a 1% drop in classification\naccuracy. Our analysis reveals that while high-intensity tokens contribute\nsignificantly to model accuracy, low-intensity tokens remain important. In\nparticular, they play a more critical role in general audio classification\ntasks than in speech-specific tasks.",
      "tldr_zh": "该研究首次将token pruning技术应用于基于ViT的音频分类模型，使用Mel频谱图进行实验，旨在优化模型性能并降低计算成本。通过TopK token pruning，AudioMAE和AST模型的MAC操作减少了30-40%，而分类精度仅下降不到1%。分析表明，高强度token对模型精度贡献显著，但低强度token同样重要，尤其是在通用音频分类任务中比在语音特定任务中作用更大。该研究揭示了音频Transformer中token的重要性，并为音频处理任务中的模型优化提供了新思路。\n",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "This work has been submitted to the IEEE for possible publication.\n  Source code is available at\n  https://github.com/andylee-24/token-pruning-audio-transformer",
      "pdf_url": "http://arxiv.org/pdf/2504.01690v1",
      "published_date": "2025-04-02 12:44:38 UTC",
      "updated_date": "2025-04-02 12:44:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:11:00.713766"
    },
    {
      "arxiv_id": "2504.01673v1",
      "title": "K-P Quantum Neural Networks",
      "title_zh": "K-P 量子神经网络\n",
      "authors": [
        "Elija Perrier"
      ],
      "abstract": "We present an extension of K-P time-optimal quantum control solutions using\nglobal Cartan $KAK$ decompositions for geodesic-based solutions. Extending\nrecent time-optimal \\emph{constant-$\\theta$} control results, we integrate\nCartan methods into equivariant quantum neural network (EQNN) for quantum\ncontrol tasks. We show that a finite-depth limited EQNN ansatz equipped with\nCartan layers can replicate the constant-$\\theta$ sub-Riemannian geodesics for\nK-P problems. We demonstrate how for certain classes of control problem on\nRiemannian symmetric spaces, gradient-based training using an appropriate cost\nfunction converges to certain global time-optimal solutions when satisfying\nsimple regularity conditions. This generalises prior geometric control theory\nmethods and clarifies how optimal geodesic estimation can be performed in\nquantum machine learning contexts.",
      "tldr_zh": "该论文提出了一种K-P量子神经网络，扩展了基于全局Cartan KAK分解的K-P时间最优量子控制解。通过将Cartan方法集成到等变量子神经网络(EQNN)中，用于量子控制任务，推广了最近的时间最优constant-$\\theta$控制结果。研究表明，配备Cartan层的有限深度EQNN ansatz可以复制K-P问题的constant-$\\theta$ sub-Riemannian geodesics。论文还展示了在满足简单正则性条件时，使用适当代价函数的基于梯度的训练如何收敛到某些黎曼对称空间上特定类别的控制问题的全局时间最优解。该方法推广了先前的几何控制理论方法，并阐明了如何在量子机器学习环境中执行最优geodesic估计。\n",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "Under review",
      "pdf_url": "http://arxiv.org/pdf/2504.01673v1",
      "published_date": "2025-04-02 12:22:18 UTC",
      "updated_date": "2025-04-02 12:22:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:11:12.952825"
    },
    {
      "arxiv_id": "2504.01671v1",
      "title": "Anomaly Detection for Hybrid Butterfly Subspecies via Probability Filtering",
      "title_zh": "基于概率过滤的混合蝴蝶亚种异常检测\n",
      "authors": [
        "Bo-Kai Ruan",
        "Yi-Zeng Fang",
        "Hong-Han Shuai",
        "Juinn-Dar Huang"
      ],
      "abstract": "Detecting butterfly hybrids requires knowledge of the parent subspecies, and\nthe process can be tedious when encountering a new subspecies. This study\nfocuses on a specific scenario where a model trained to recognize hybrid\nspecies A can generalize to species B when B biologically mimics A. Since\nspecies A and B share similar patterns, we leverage BioCLIP as our feature\nextractor to capture features based on their taxonomy. Consequently, the\nalgorithm designed for species A can be transferred to B, as their hybrid and\nnon-hybrid patterns exhibit similar relationships. To determine whether a\nbutterfly is a hybrid, we adopt proposed probability filtering and color\njittering to augment and simulate the mimicry. With these approaches, we\nachieve second place in the official development phase. Our code is publicly\navailable at https://github.com/Justin900429/NSF-HDR-Challenge.",
      "tldr_zh": "该研究提出了一种基于概率过滤的异常检测方法，用于检测杂交蝴蝶亚种。该方法利用BioCLIP作为特征提取器，捕捉蝴蝶亚种的分类学特征，从而将在亚种A上训练的模型泛化到生物学上模仿A的亚种B。通过概率过滤和颜色抖动增强技术模拟模仿行为，该方法能够有效区分杂交和非杂交蝴蝶。该方法在官方开发阶段取得了第二名的成绩，代码已公开。\n",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "primary_category": "cs.CE",
      "comment": "AAAI'25 Workshop in Anomaly Detection in Scientific Domains",
      "pdf_url": "http://arxiv.org/pdf/2504.01671v1",
      "published_date": "2025-04-02 12:18:44 UTC",
      "updated_date": "2025-04-02 12:18:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:11:24.645811"
    },
    {
      "arxiv_id": "2504.01652v1",
      "title": "Market-Oriented Flow Allocation for Thermal Solar Plants: An Auction-Based Methodology with Artificial Intelligence",
      "title_zh": "面向市场的太阳能热电站流量分配：一种基于拍卖的人工智能方法\n",
      "authors": [
        "Sara Ruiz-Moreno",
        "Antonio J. Gallego",
        "Manuel Macías",
        "Eduardo F. Camacho"
      ],
      "abstract": "This paper presents a novel method to optimize thermal balance in parabolic\ntrough collector (PTC) plants. It uses a market-based system to distribute flow\namong loops combined with an artificial neural network (ANN) to reduce\ncomputation and data requirements. This auction-based approach balances loop\ntemperatures, accommodating varying thermal losses and collector efficiencies.\nValidation across different thermal losses, optical efficiencies, and\nirradiance conditions-sunny, partially cloudy, and cloudy-show improved thermal\npower output and intercept factors compared to a no-allocation system. It\ndemonstrates scalability and practicality for large solar thermal plants,\nenhancing overall performance. The method was first validated through\nsimulations on a realistic solar plant model, then adapted and successfully\ntested in a 50 MW solar trough plant, demonstrating its advantages.\nFurthermore, the algorithms have been implemented, commissioned, and are\ncurrently operating in 13 commercial solar trough plants.",
      "tldr_zh": "本文提出了一种新的面向市场的流量分配方法，用于优化抛物槽式太阳能(PTC)电站的热平衡。该方法结合了基于拍卖的系统和人工神经网络(ANN)，以在不同回路间分配流量，平衡回路温度，并减少计算和数据需求。实验结果表明，与无分配系统相比，该方法在不同光照条件（晴天、部分多云和阴天）下，能够提高热功率输出和截取因子。该方法已在实际的50MW太阳能槽式电站中成功测试，并已在13个商业太阳能槽式电站中实施和运行，证明了其可扩展性和实用性，能够提升整体性能。\n",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "This manuscript has been submitted to Renewable Energy",
      "pdf_url": "http://arxiv.org/pdf/2504.01652v1",
      "published_date": "2025-04-02 12:01:41 UTC",
      "updated_date": "2025-04-02 12:01:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:11:36.947683"
    },
    {
      "arxiv_id": "2504.01644v1",
      "title": "Proposition of Affordance-Driven Environment Recognition Framework Using Symbol Networks in Large Language Models",
      "title_zh": "基于大语言模型中符号网络的由可供性驱动的环境识别框架提案\n",
      "authors": [
        "Kazuma Arii",
        "Satoshi Kurihara"
      ],
      "abstract": "In the quest to enable robots to coexist with humans, understanding dynamic\nsituations and selecting appropriate actions based on common sense and\naffordances are essential. Conventional AI systems face challenges in applying\naffordance, as it represents implicit knowledge derived from common sense.\nHowever, large language models (LLMs) offer new opportunities due to their\nability to process extensive human knowledge. This study proposes a method for\nautomatic affordance acquisition by leveraging LLM outputs. The process\ninvolves generating text using LLMs, reconstructing the output into a symbol\nnetwork using morphological and dependency analysis, and calculating\naffordances based on network distances. Experiments using ``apple'' as an\nexample demonstrated the method's ability to extract context-dependent\naffordances with high explainability. The results suggest that the proposed\nsymbol network, reconstructed from LLM outputs, enables robots to interpret\naffordances effectively, bridging the gap between symbolized data and\nhuman-like situational understanding.",
      "tldr_zh": "该研究提出了一种基于大型语言模型(LLMs)的具身性驱动环境识别框架，旨在使机器人能够理解动态环境并基于常识和具身性选择合适的动作。该方法利用LLMs生成文本，并通过形态和依存关系分析将输出重构为符号网络，然后基于网络距离计算具身性。以“apple”为例的实验表明，该方法能够提取具有高可解释性的上下文相关的具身性。研究结果表明，从LLM输出重构的符号网络能够使机器人有效地解释具身性，从而弥合了符号化数据和类人情境理解之间的差距。\n",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01644v1",
      "published_date": "2025-04-02 11:48:44 UTC",
      "updated_date": "2025-04-02 11:48:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:11:48.787231"
    },
    {
      "arxiv_id": "2504.01641v1",
      "title": "Bridge 2D-3D: Uncertainty-aware Hierarchical Registration Network with Domain Alignment",
      "title_zh": "Bridge 2D-3D：基于领域对齐的、具有不确定性感知的分层配准网络\n",
      "authors": [
        "Zhixin Cheng",
        "Jiacheng Deng",
        "Xinjun Li",
        "Baoqun Yin",
        "Tianzhu Zhang"
      ],
      "abstract": "The method for image-to-point cloud registration typically determines the\nrigid transformation using a coarse-to-fine pipeline. However, directly and\nuniformly matching image patches with point cloud patches may lead to focusing\non incorrect noise patches during matching while ignoring key ones. Moreover,\ndue to the significant differences between image and point cloud modalities, it\nmay be challenging to bridge the domain gap without specific improvements in\ndesign. To address the above issues, we innovatively propose the\nUncertainty-aware Hierarchical Matching Module (UHMM) and the Adversarial Modal\nAlignment Module (AMAM). Within the UHMM, we model the uncertainty of critical\ninformation in image patches and facilitate multi-level fusion interactions\nbetween image and point cloud features. In the AMAM, we design an adversarial\napproach to reduce the domain gap between image and point cloud. Extensive\nexperiments and ablation studies on RGB-D Scene V2 and 7-Scenes benchmarks\ndemonstrate the superiority of our method, making it a state-of-the-art\napproach for image-to-point cloud registration tasks.",
      "tldr_zh": "该论文提出了一种不确定性感知的分层配准网络，用于解决图像到点云配准问题。该网络包含不确定性感知的分层匹配模块(UHMM)和对抗模态对齐模块(AMAM)。UHMM通过对图像块中的关键信息进行不确定性建模，促进图像和点云特征之间的多层次融合交互。AMAM则采用对抗方法来缩小图像和点云之间的领域差距。在RGB-D Scene V2和7-Scenes数据集上的实验结果表明，该方法优于现有技术，达到了state-of-the-art的水平。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "AAAI2025accept",
      "pdf_url": "http://arxiv.org/pdf/2504.01641v1",
      "published_date": "2025-04-02 11:43:55 UTC",
      "updated_date": "2025-04-02 11:43:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:12:00.806836"
    },
    {
      "arxiv_id": "2504.01637v1",
      "title": "LLM-mediated Dynamic Plan Generation with a Multi-Agent Approach",
      "title_zh": "基于多智能体方法的 LLM 介导的动态计划生成\n",
      "authors": [
        "Reo Abe",
        "Akifumi Ito",
        "Kanata Takayasu",
        "Satoshi Kurihara"
      ],
      "abstract": "Planning methods with high adaptability to dynamic environments are crucial\nfor the development of autonomous and versatile robots. We propose a method for\nleveraging a large language model (GPT-4o) to automatically generate networks\ncapable of adapting to dynamic environments. The proposed method collects\nenvironmental \"status,\" representing conditions and goals, and uses them to\ngenerate agents. These agents are interconnected on the basis of specific\nconditions, resulting in networks that combine flexibility and generality. We\nconducted evaluation experiments to compare the networks automatically\ngenerated with the proposed method with manually constructed ones, confirming\nthe comprehensiveness of the proposed method's networks and their higher\ngenerality. This research marks a significant advancement toward the\ndevelopment of versatile planning methods applicable to robotics, autonomous\nvehicles, smart systems, and other complex environments.",
      "tldr_zh": "本文提出了一种利用大型语言模型(LLM, 具体为GPT-4o)自动生成适应动态环境网络的方法。该方法通过收集环境“状态”（包括条件和目标），生成多个智能体(agents)，并根据特定条件将这些智能体互连，形成兼具灵活性和通用性的网络。实验结果表明，该方法自动生成的网络比手动构建的网络更全面，且具有更高的通用性。这项研究为开发适用于机器人、自动驾驶汽车、智能系统和其他复杂环境的通用规划方法奠定了基础。\n",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01637v1",
      "published_date": "2025-04-02 11:42:49 UTC",
      "updated_date": "2025-04-02 11:42:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:12:12.800873"
    },
    {
      "arxiv_id": "2504.01632v1",
      "title": "Benchmarking the Spatial Robustness of DNNs via Natural and Adversarial Localized Corruptions",
      "title_zh": "通过自然和对抗性局部损坏来评估 DNN 的空间鲁棒性\n",
      "authors": [
        "Giulia Marchiori Pietrosanti",
        "Giulio Rossolini",
        "Alessandro Biondi",
        "Giorgio Buttazzo"
      ],
      "abstract": "The robustness of DNNs is a crucial factor in safety-critical applications,\nparticularly in complex and dynamic environments where localized corruptions\ncan arise. While previous studies have evaluated the robustness of semantic\nsegmentation (SS) models under whole-image natural or adversarial corruptions,\na comprehensive investigation into the spatial robustness of dense vision\nmodels under localized corruptions remained underexplored. This paper fills\nthis gap by introducing specialized metrics for benchmarking the spatial\nrobustness of segmentation models, alongside with an evaluation framework to\nassess the impact of localized corruptions. Furthermore, we uncover the\ninherent complexity of characterizing worst-case robustness using a single\nlocalized adversarial perturbation. To address this, we propose region-aware\nmulti-attack adversarial analysis, a method that enables a deeper understanding\nof model robustness against adversarial perturbations applied to specific\nregions. The proposed metrics and analysis were evaluated on 15 segmentation\nmodels in driving scenarios, uncovering key insights into the effects of\nlocalized corruption in both natural and adversarial forms. The results reveal\nthat models respond to these two types of threats differently; for instance,\ntransformer-based segmentation models demonstrate notable robustness to\nlocalized natural corruptions but are highly vulnerable to adversarial ones and\nvice-versa for CNN-based models. Consequently, we also address the challenge of\nbalancing robustness to both natural and adversarial localized corruptions by\nmeans of ensemble models, thereby achieving a broader threat coverage and\nimproved reliability for dense vision tasks.",
      "tldr_zh": "本文旨在填补现有研究在评估密集视觉模型在局部图像损坏下的空间鲁棒性方面的空白。研究提出了专门的指标，用于评估分割模型在局部自然和对抗性损坏下的空间鲁棒性，并构建了相应的评估框架。研究揭示了使用单一局部对抗扰动来表征最坏情况鲁棒性的内在复杂性，并提出了一种区域感知的多攻击对抗分析方法，以更深入地了解模型对特定区域对抗扰动的鲁棒性。通过在15个分割模型上进行评估，发现Transformer模型对局部自然损坏表现出较强的鲁棒性，但容易受到对抗性攻击，而CNN模型则相反。最后，研究通过集成模型来平衡对自然和对抗性局部损坏的鲁棒性，从而提高密集视觉任务的可靠性。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Under review",
      "pdf_url": "http://arxiv.org/pdf/2504.01632v1",
      "published_date": "2025-04-02 11:37:39 UTC",
      "updated_date": "2025-04-02 11:37:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:12:25.126729"
    },
    {
      "arxiv_id": "2504.01627v1",
      "title": "Horizon Scans can be accelerated using novel information retrieval and artificial intelligence tools",
      "title_zh": "使用新型信息检索和人工智能工具加速 Horizon Scans\n",
      "authors": [
        "Lena Schmidt",
        "Oshin Sharma",
        "Chris Marshall",
        "Sonia Garcia Gonzalez Moral"
      ],
      "abstract": "Introduction: Horizon scanning in healthcare assesses early signals of\ninnovation, crucial for timely adoption. Current horizon scanning faces\nchallenges in efficient information retrieval and analysis, especially from\nunstructured sources like news, presenting a need for innovative tools.\nMethodology: The study introduces SCANAR and AIDOC, open-source Python-based\ntools designed to improve horizon scanning. SCANAR automates the retrieval and\nprocessing of news articles, offering functionalities such as de-duplication\nand unsupervised relevancy ranking. AIDOC aids filtration by leveraging AI to\nreorder textual data based on relevancy, employing neural networks for semantic\nsimilarity, and subsequently prioritizing likely relevant entries for human\nreview. Results: Twelve internal datasets from horizon scans and four external\nbenchmarking datasets were used. SCANAR improved retrieval efficiency by\nautomating processes previously dependent on manual labour. AIDOC displayed\nwork-saving potential, achieving around 62% reduction in manual review efforts\nat 95% recall. Comparative analysis with benchmarking data showed AIDOC's\nperformance was similar to existing systematic review automation tools, though\nperformance varied depending on dataset characteristics. A smaller case-study\non our news datasets shows the potential of ensembling large language models\nwithin the active-learning process for faster detection of relevant articles\nacross news datasets. Conclusion: The validation indicates that SCANAR and\nAIDOC show potential to enhance horizon scanning efficiency by streamlining\ndata retrieval and prioritisation. These tools may alleviate methodological\nlimitations and allow broader, swifter horizon scans. Further studies are\nsuggested to optimize these models and to design new workflows and validation\nprocesses that integrate large language models.",
      "tldr_zh": "该研究介绍了SCANAR和AIDOC，两款用于加速医疗领域horizon scanning（前瞻扫描）的开源Python工具。SCANAR旨在自动化新闻文章的检索和处理，具备去重和无监督相关性排序等功能。AIDOC利用人工智能，通过语义相似性的神经网络模型对文本数据进行重新排序，从而优先筛选可能相关的条目，减少人工审核的工作量。实验结果表明，AIDOC在95%召回率下，可减少约62%的人工审核工作，与现有的系统性综述自动化工具性能相似。结论认为SCANAR和AIDOC有潜力通过简化数据检索和优先级排序来提高horizon scanning的效率，并建议进一步研究优化这些模型，并设计集成大型语言模型的新工作流程。\n",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01627v1",
      "published_date": "2025-04-02 11:33:08 UTC",
      "updated_date": "2025-04-02 11:33:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:12:37.171956"
    },
    {
      "arxiv_id": "2504.01589v1",
      "title": "Text Speaks Louder than Vision: ASCII Art Reveals Textual Biases in Vision-Language Models",
      "title_zh": "文字胜于视觉：ASCII 艺术揭示了视觉-语言模型中的文本偏差\n",
      "authors": [
        "Zhaochen Wang",
        "Yujun Cai",
        "Zi Huang",
        "Bryan Hooi",
        "Yiwei Wang",
        "Ming-Hsuan Yang"
      ],
      "abstract": "Vision-language models (VLMs) have advanced rapidly in processing multimodal\ninformation, but their ability to reconcile conflicting signals across\nmodalities remains underexplored. This work investigates how VLMs process ASCII\nart, a unique medium where textual elements collectively form visual patterns,\npotentially creating semantic-visual conflicts. We introduce a novel evaluation\nframework that systematically challenges five state-of-the-art models\n(including GPT-4o, Claude, and Gemini) using adversarial ASCII art, where\ncharacter-level semantics deliberately contradict global visual patterns. Our\nexperiments reveal a strong text-priority bias: VLMs consistently prioritize\ntextual information over visual patterns, with visual recognition ability\ndeclining dramatically as semantic complexity increases. Various mitigation\nattempts through visual parameter tuning and prompt engineering yielded only\nmodest improvements, suggesting that this limitation requires\narchitectural-level solutions. These findings uncover fundamental flaws in how\ncurrent VLMs integrate multimodal information, providing important guidance for\nfuture model development while highlighting significant implications for\ncontent moderation systems vulnerable to adversarial examples.",
      "tldr_zh": "该研究揭示了视觉语言模型(VLMs)在处理语义-视觉冲突时的文本优先偏差。通过引入对抗性ASCII艺术，即字符语义与全局视觉模式相矛盾的图像，系统性地评估了包括GPT-4o、Claude和Gemini在内的五个最先进的模型。实验表明，VLMs倾向于优先考虑文本信息而非视觉模式，且视觉识别能力随着语义复杂性的增加而显著下降。尽管通过视觉参数调整和提示工程进行了一些缓解尝试，但效果有限，表明这种局限性需要架构层面的解决方案。这项研究揭示了当前VLMs在整合多模态信息方面的根本缺陷，并为未来的模型开发以及内容审核系统提供了重要指导。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Under review at COLM 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.01589v1",
      "published_date": "2025-04-02 10:47:07 UTC",
      "updated_date": "2025-04-02 10:47:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:12:48.860954"
    },
    {
      "arxiv_id": "2504.01588v1",
      "title": "Building Knowledge from Interactions: An LLM-Based Architecture for Adaptive Tutoring and Social Reasoning",
      "title_zh": "从交互中构建知识：一种基于 LLM 的自适应辅导和社交推理架构\n",
      "authors": [
        "Luca Garello",
        "Giulia Belgiovine",
        "Gabriele Russo",
        "Francesco Rea",
        "Alessandra Sciutti"
      ],
      "abstract": "Integrating robotics into everyday scenarios like tutoring or physical\ntraining requires robots capable of adaptive, socially engaging, and\ngoal-oriented interactions. While Large Language Models show promise in\nhuman-like communication, their standalone use is hindered by memory\nconstraints and contextual incoherence. This work presents a multimodal,\ncognitively inspired framework that enhances LLM-based autonomous\ndecision-making in social and task-oriented Human-Robot Interaction.\nSpecifically, we develop an LLM-based agent for a robot trainer, balancing\nsocial conversation with task guidance and goal-driven motivation. To further\nenhance autonomy and personalization, we introduce a memory system for\nselecting, storing and retrieving experiences, facilitating generalized\nreasoning based on knowledge built across different interactions. A preliminary\nHRI user study and offline experiments with a synthetic dataset validate our\napproach, demonstrating the system's ability to manage complex interactions,\nautonomously drive training tasks, and build and retrieve contextual memories,\nadvancing socially intelligent robotics.",
      "tldr_zh": "该论文提出了一种基于LLM的多模态、认知驱动的框架，用于增强人机交互中基于LLM的自主决策，特别是针对机器人教练应用。该框架结合了社交对话、任务指导和目标驱动的激励，并引入了记忆系统来选择、存储和检索经验，从而促进基于跨不同交互构建的知识进行泛化推理。初步的人机交互用户研究和使用合成数据集的离线实验验证了该方法，表明该系统能够管理复杂的交互，自主驱动训练任务，并构建和检索上下文记忆，从而推进了社交智能机器人技术的发展。核心在于利用记忆系统克服LLM的记忆限制和上下文不连贯问题，提升机器人在辅导和社交推理等场景下的适应性和社交能力。\n",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Submitted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.01588v1",
      "published_date": "2025-04-02 10:45:41 UTC",
      "updated_date": "2025-04-02 10:45:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:13:01.092427"
    },
    {
      "arxiv_id": "2504.01571v1",
      "title": "Pro-DG: Procedural Diffusion Guidance for Architectural Facade Generation",
      "title_zh": "Pro-DG：用于建筑立面生成的程序化扩散引导\n",
      "authors": [
        "Aleksander Plocharski",
        "Jan Swidzinski",
        "Przemyslaw Musialski"
      ],
      "abstract": "We present Pro-DG, a framework for procedurally controllable photo-realistic\nfacade generation that combines a procedural shape grammar with diffusion-based\nimage synthesis. Starting from a single input image, we reconstruct its facade\nlayout using grammar rules, then edit that structure through user-defined\ntransformations. As facades are inherently multi-hierarchical structures, we\nintroduce hierarchical matching procedure that aligns facade structures at\ndifferent levels which is used to introduce control maps to guide a generative\ndiffusion pipeline. This approach retains local appearance fidelity while\naccommodating large-scale edits such as floor duplication or window\nrearrangement. We provide a thorough evaluation, comparing Pro-DG against\ninpainting-based baselines and synthetic ground truths. Our user study and\nquantitative measurements indicate improved preservation of architectural\nidentity and higher edit accuracy. Our novel method is the first to integrate\nneuro-symbolically derived shape-grammars for modeling with modern generative\nmodel and highlights the broader potential of such approaches for precise and\ncontrollable image manipulation.",
      "tldr_zh": "Pro-DG 是一种程序可控的建筑立面生成框架，它结合了程序化的形状文法和基于扩散的图像合成。该方法从单个输入图像出发，利用文法规则重建立面布局，并通过用户定义的变换编辑该结构。针对立面的多层级结构，Pro-DG 引入了层级匹配程序，用于对齐不同层级的立面结构，并生成控制图以指导生成式扩散流程。该方法在保留局部外观逼真度的同时，支持大规模编辑，例如楼层复制或窗户重新排列。实验结果表明，Pro-DG 在建筑标识的保留和编辑精度方面均优于基于图像修复的基线方法。该研究首次将神经符号推导的形状文法与现代生成模型相结合，突显了此类方法在精确和可控图像操作方面的潜力。\n",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "I.3.7; I.4.9; I.2.10"
      ],
      "primary_category": "cs.GR",
      "comment": "12 pages, 13 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.01571v1",
      "published_date": "2025-04-02 10:16:19 UTC",
      "updated_date": "2025-04-02 10:16:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:13:13.246048"
    },
    {
      "arxiv_id": "2504.01560v1",
      "title": "Optimizing Package Delivery with Quantum Annealers: Addressing Time-Windows and Simultaneous Pickup and Delivery",
      "title_zh": "利用量子退火优化包裹递送：解决时间窗和同时取货与交付问题\n",
      "authors": [
        "Eneko Osaba",
        "Esther Villar-Rodriguez",
        "Pablo Miranda-Rodriguez",
        "Antón Asla"
      ],
      "abstract": "Recent research at the intersection of quantum computing and routing problems\nhas been highly prolific. Much of this work focuses on classical problems such\nas the Traveling Salesman Problem and the Vehicle Routing Problem. The\npractical applicability of these problems depends on the specific objectives\nand constraints considered. However, it is undeniable that translating complex\nreal-world requirements into these classical formulations often proves\nchallenging. In this paper, we resort to our previously published\nquantum-classical technique for addressing real-world-oriented routing\nproblems, known as Quantum for Real Package Delivery (Q4RPD), and elaborate on\nsolving additional realistic problem instances. Accordingly, this paper\nemphasizes the following characteristics: i) simultaneous pickup and\ndeliveries, ii) time-windows, and iii) mobility restrictions by vehicle type.\nTo illustrate the application of Q4RPD, we have conducted an experimentation\ncomprising seven instances, serving as a demonstration of the newly developed\nfeatures.",
      "tldr_zh": "本文扩展了名为Quantum for Real Package Delivery (Q4RPD)的量子-经典混合技术，用于解决更贴近现实的包裹递送问题。该研究重点关注三个关键特性：同时取货和交付、时间窗以及车辆类型的移动限制。通过七个实例的实验，展示了Q4RPD在处理这些新增特性方面的应用，验证了其解决实际问题的能力。该研究丰富了量子计算在路径优化问题中的应用，使其更具实用性。\n",
      "categories": [
        "cs.ET",
        "cs.AI"
      ],
      "primary_category": "cs.ET",
      "comment": "8 pages, 1 table, 9 figures, paper submitted to the IEEE\n  International Conference on Quantum Computing and Engineering (QCE 2025)",
      "pdf_url": "http://arxiv.org/pdf/2504.01560v1",
      "published_date": "2025-04-02 10:01:34 UTC",
      "updated_date": "2025-04-02 10:01:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:13:24.846129"
    },
    {
      "arxiv_id": "2504.01551v1",
      "title": "Identifying Macro Causal Effects in C-DMGs",
      "title_zh": "在 C-DMG 中识别宏观因果效应\n",
      "authors": [
        "Simon Ferreira",
        "Charles K. Assaad"
      ],
      "abstract": "Causal effect identification using causal graphs is a fundamental challenge\nin causal inference. While extensive research has been conducted in this area,\nmost existing methods assume the availability of fully specified causal graphs.\nHowever, in complex domains such as medicine and epidemiology, complete causal\nknowledge is often unavailable, and only partial information about the system\nis accessible. This paper focuses on causal effect identification within\npartially specified causal graphs, with particular emphasis on cluster-directed\nmixed graphs (C-DMGs). These graphs provide a higher-level representation of\ncausal relationships by grouping variables into clusters, offering a more\npractical approach for handling complex systems. Unlike fully specified causal\ngraphs, C-DMGs can contain cycles, which complicate their analysis and\ninterpretation. Furthermore, their cluster-based nature introduces new\nchallenges, as it gives rise to two distinct types of causal effects, macro\ncausal effects and micro causal effects, with different properties. In this\nwork, we focus on macro causal effects, which describe the effects of entire\nclusters on other clusters. We establish that the do-calculus is both sound and\ncomplete for identifying these effects in C-DMGs. Additionally, we provide a\ngraphical characterization of non-identifiability for macro causal effects in\nthese graphs.",
      "tldr_zh": "这篇论文研究了在部分指定的因果图，特别是cluster-directed mixed graphs (C-DMGs)中识别宏观因果效应的问题。C-DMGs通过将变量分组为集群来提供更高级别的因果关系表示，适用于处理复杂系统。论文重点关注宏观因果效应，即整个集群对其他集群的影响。研究证明，do-calculus 对于识别 C-DMGs 中的宏观因果效应是完备且可靠的。此外，论文还提供了在这些图中宏观因果效应不可识别性的图形化特征。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01551v1",
      "published_date": "2025-04-02 09:48:27 UTC",
      "updated_date": "2025-04-02 09:48:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:13:36.738576"
    },
    {
      "arxiv_id": "2504.01541v1",
      "title": "Hyperbolic Diffusion Recommender Model",
      "title_zh": "双曲扩散推荐模型\n",
      "authors": [
        "Meng Yuan",
        "Yutian Xiao",
        "Wei Chen",
        "Chu Zhao",
        "Deqing Wang",
        "Fuzhen Zhuang"
      ],
      "abstract": "Diffusion models (DMs) have emerged as the new state-of-the-art family of\ndeep generative models. To gain deeper insights into the limitations of\ndiffusion models in recommender systems, we investigate the fundamental\nstructural disparities between images and items. Consequently, items often\nexhibit distinct anisotropic and directional structures that are less prevalent\nin images. However, the traditional forward diffusion process continuously adds\nisotropic Gaussian noise, causing anisotropic signals to degrade into noise,\nwhich impairs the semantically meaningful representations in recommender\nsystems.\n  Inspired by the advancements in hyperbolic spaces, we propose a novel\n\\textit{\\textbf{H}yperbolic} \\textit{\\textbf{D}iffusion}\n\\textit{\\textbf{R}ecommender} \\textit{\\textbf{M}odel} (named HDRM). Unlike\nexisting directional diffusion methods based on Euclidean space, the intrinsic\nnon-Euclidean structure of hyperbolic space makes it particularly well-adapted\nfor handling anisotropic diffusion processes. In particular, we begin by\nformulating concepts to characterize latent directed diffusion processes within\na geometrically grounded hyperbolic space. Subsequently, we propose a novel\nhyperbolic latent diffusion process specifically tailored for users and items.\nDrawing upon the natural geometric attributes of hyperbolic spaces, we impose\nstructural restrictions on the space to enhance hyperbolic diffusion\npropagation, thereby ensuring the preservation of the intrinsic topology of\nuser-item graphs. Extensive experiments on three benchmark datasets demonstrate\nthe effectiveness of HDRM.",
      "tldr_zh": "该论文针对传统扩散模型(Diffusion Models)在推荐系统中因忽略物品的异构性和方向性结构而导致性能下降的问题，提出了一个新颖的\\textit{\\textbf{H}yperbolic} \\textit{\\textbf{D}iffusion} \\textit{\\textbf{R}ecommender} \\textit{\\textbf{M}odel} (HDRM)。HDRM利用双曲空间的非欧结构来处理异构扩散过程，通过在双曲空间中构建用户和物品的潜在扩散过程，并施加结构约束以增强双曲扩散传播，从而保留用户-物品图的内在拓扑结构。在三个基准数据集上的实验结果表明，HDRM 具有有效性。\n",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01541v1",
      "published_date": "2025-04-02 09:27:40 UTC",
      "updated_date": "2025-04-02 09:27:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:13:49.086139"
    },
    {
      "arxiv_id": "2504.01538v1",
      "title": "AI-Newton: A Concept-Driven Physical Law Discovery System without Prior Physical Knowledge",
      "title_zh": "AI-Newton：一种无需先验物理知识的、概念驱动的物理定律发现系统\n",
      "authors": [
        "You-Le Fang",
        "Dong-Shan Jian",
        "Xiang Li",
        "Yan-Qing Ma"
      ],
      "abstract": "Current limitations in human scientific discovery necessitate a new research\nparadigm. While advances in artificial intelligence (AI) offer a highly\npromising solution, enabling AI to emulate human-like scientific discovery\nremains an open challenge. To address this, we propose AI-Newton, a\nconcept-driven discovery system capable of autonomously deriving physical laws\nfrom raw data -- without supervision or prior physical knowledge. The system\nintegrates a knowledge base and knowledge representation centered on physical\nconcepts, along with an autonomous discovery workflow. As a proof of concept,\nwe apply AI-Newton to a large set of Newtonian mechanics problems. Given\nexperimental data with noise, the system successfully rediscovers fundamental\nlaws, including Newton's second law, energy conservation and law of\ngravitation, using autonomously defined concepts. This achievement marks a\nsignificant step toward AI-driven autonomous scientific discovery.",
      "tldr_zh": "AI-Newton是一个概念驱动的物理定律发现系统，旨在无需任何先验物理知识的情况下，自主地从原始数据中推导出物理定律。该系统集成了基于物理概念的知识库和知识表示，以及一个自主发现工作流程。通过在大量牛顿力学问题上的概念验证，AI-Newton成功地重新发现了包括牛顿第二定律、能量守恒定律和万有引力定律在内的基本定律，证明了其在AI驱动的自主科学发现方面的潜力。该系统利用自主定义的概念，从带噪声的实验数据中提取物理规律。\n",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.SC",
        "hep-ph",
        "physics.class-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "31 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.01538v1",
      "published_date": "2025-04-02 09:25:34 UTC",
      "updated_date": "2025-04-02 09:25:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:14:00.924600"
    },
    {
      "arxiv_id": "2504.01522v1",
      "title": "Redefining technology for indigenous languages",
      "title_zh": "重新定义本土语言技术\n",
      "authors": [
        "Silvia Fernandez-Sabido",
        "Laura Peniche-Sabido"
      ],
      "abstract": "In this paper, we offer an overview of indigenous languages, identifying the\ncauses of their devaluation and the need for legislation on language rights. We\nreview the technologies used to revitalize these languages, finding that when\nthey come from outside, they often have the opposite effect to what they seek;\nhowever, when developed from within communities, they become powerful\ninstruments of expression. We propose that the inclusion of Indigenous\nknowledge in large language models (LLMs) will enrich the technological\nlandscape, but must be done in a participatory environment that encourages the\nexchange of knowledge.",
      "tldr_zh": "本文概述了土著语言的现状，指出了其贬值的原因以及语言权利立法的必要性。文章回顾了用于振兴土著语言的技术，发现外部技术往往适得其反，而源于社区内部的技术则成为强大的表达工具。作者提出，将土著知识纳入大型语言模型（LLMs）将丰富技术领域，但必须在鼓励知识交流的参与式环境中进行。该研究强调了技术发展中文化敏感性和社区参与的重要性，旨在为土著语言的保护和发展提供新的视角。\n",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "in Spanish language",
      "pdf_url": "http://arxiv.org/pdf/2504.01522v1",
      "published_date": "2025-04-02 09:08:53 UTC",
      "updated_date": "2025-04-02 09:08:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:14:12.834476"
    },
    {
      "arxiv_id": "2504.01521v1",
      "title": "Domain Guidance: A Simple Transfer Approach for a Pre-trained Diffusion Model",
      "title_zh": "领域引导：一种预训练扩散模型的简单迁移方法\n",
      "authors": [
        "Jincheng Zhong",
        "Xiangcheng Zhang",
        "Jianmin Wang",
        "Mingsheng Long"
      ],
      "abstract": "Recent advancements in diffusion models have revolutionized generative\nmodeling. However, the impressive and vivid outputs they produce often come at\nthe cost of significant model scaling and increased computational demands.\nConsequently, building personalized diffusion models based on off-the-shelf\nmodels has emerged as an appealing alternative. In this paper, we introduce a\nnovel perspective on conditional generation for transferring a pre-trained\nmodel. From this viewpoint, we propose *Domain Guidance*, a straightforward\ntransfer approach that leverages pre-trained knowledge to guide the sampling\nprocess toward the target domain. Domain Guidance shares a formulation similar\nto advanced classifier-free guidance, facilitating better domain alignment and\nhigher-quality generations. We provide both empirical and theoretical analyses\nof the mechanisms behind Domain Guidance. Our experimental results demonstrate\nits substantial effectiveness across various transfer benchmarks, achieving\nover a 19.6% improvement in FID and a 23.4% improvement in FD$_\\text{DINOv2}$\ncompared to standard fine-tuning. Notably, existing fine-tuned models can\nseamlessly integrate Domain Guidance to leverage these benefits, without\nadditional training.",
      "tldr_zh": "本文提出了一种简单的迁移方法*Domain Guidance*，用于将预训练扩散模型迁移到特定领域。该方法通过利用预训练知识指导采样过程，实现更好的领域对齐和更高质量的生成，其公式与先进的无分类器引导相似。通过实验和理论分析，证明了*Domain Guidance*的有效性，在各种迁移基准测试中，FID提高了19.6%，FD$_\\text{DINOv2}$提高了23.4%，优于标准微调。值得注意的是，现有的微调模型可以无缝集成*Domain Guidance*，无需额外训练即可获得这些优势。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01521v1",
      "published_date": "2025-04-02 09:07:55 UTC",
      "updated_date": "2025-04-02 09:07:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:14:24.946776"
    },
    {
      "arxiv_id": "2504.01515v2",
      "title": "Training-free Dense-Aligned Diffusion Guidance for Modular Conditional Image Synthesis",
      "title_zh": "免训练的密集对齐扩散引导，用于模块化条件图像合成\n",
      "authors": [
        "Zixuan Wang",
        "Duo Peng",
        "Feng Chen",
        "Yuwei Yang",
        "Yinjie Lei"
      ],
      "abstract": "Conditional image synthesis is a crucial task with broad applications, such\nas artistic creation and virtual reality. However, current generative methods\nare often task-oriented with a narrow scope, handling a restricted condition\nwith constrained applicability. In this paper, we propose a novel approach that\ntreats conditional image synthesis as the modular combination of diverse\nfundamental condition units. Specifically, we divide conditions into three\nprimary units: text, layout, and drag. To enable effective control over these\nconditions, we design a dedicated alignment module for each. For the text\ncondition, we introduce a Dense Concept Alignment (DCA) module, which achieves\ndense visual-text alignment by drawing on diverse textual concepts. For the\nlayout condition, we propose a Dense Geometry Alignment (DGA) module to enforce\ncomprehensive geometric constraints that preserve the spatial configuration.\nFor the drag condition, we introduce a Dense Motion Alignment (DMA) module to\napply multi-level motion regularization, ensuring that each pixel follows its\ndesired trajectory without visual artifacts. By flexibly inserting and\ncombining these alignment modules, our framework enhances the model's\nadaptability to diverse conditional generation tasks and greatly expands its\napplication range. Extensive experiments demonstrate the superior performance\nof our framework across a variety of conditions, including textual description,\nsegmentation mask (bounding box), drag manipulation, and their combinations.\nCode is available at https://github.com/ZixuanWang0525/DADG.",
      "tldr_zh": "本文提出一种无需训练的密集对齐扩散引导方法(Dense-Aligned Diffusion Guidance)，用于模块化的条件图像合成。该方法将条件图像合成视为文本、布局和拖拽三种基本条件单元的模块化组合，并为每种条件设计了专门的对齐模块。具体来说，密集概念对齐(DCA)模块用于实现密集视觉-文本对齐；密集几何对齐(DGA)模块用于实施全面的几何约束，保持空间配置；密集运动对齐(DMA)模块用于应用多层次运动正则化，确保像素跟随期望轨迹。通过灵活组合这些对齐模块，该框架增强了模型对各种条件生成任务的适应性，并扩展了其应用范围。实验结果表明，该框架在文本描述、分割掩码、拖拽操作及其组合等多种条件下表现优异。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR2025",
      "pdf_url": "http://arxiv.org/pdf/2504.01515v2",
      "published_date": "2025-04-02 09:00:28 UTC",
      "updated_date": "2025-04-03 08:39:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:14:37.206852"
    },
    {
      "arxiv_id": "2504.01468v1",
      "title": "HH-PIM: Dynamic Optimization of Power and Performance with Heterogeneous-Hybrid PIM for Edge AI Devices",
      "title_zh": "HH-PIM：面向边缘 AI 设备的异构混合 PIM 的功耗与性能动态优化\n",
      "authors": [
        "Sangmin Jeon",
        "Kangju Lee",
        "Kyeongwon Lee",
        "Woojoo Lee"
      ],
      "abstract": "Processing-in-Memory (PIM) architectures offer promising solutions for\nefficiently handling AI applications in energy-constrained edge environments.\nWhile traditional PIM designs enhance performance and energy efficiency by\nreducing data movement between memory and processing units, they are limited in\nedge devices due to continuous power demands and the storage requirements of\nlarge neural network weights in SRAM and DRAM. Hybrid PIM architectures,\nincorporating non-volatile memories like MRAM and ReRAM, mitigate these\nlimitations but struggle with a mismatch between fixed computing resources and\ndynamically changing inference workloads. To address these challenges, this\nstudy introduces a Heterogeneous-Hybrid PIM (HH-PIM) architecture, comprising\nhigh-performance MRAM-SRAM PIM modules and low-power MRAM-SRAM PIM modules. We\nfurther propose a data placement optimization algorithm that dynamically\nallocates data based on computational demand, maximizing energy efficiency.\nFPGA prototyping and power simulations with processors featuring HH-PIM and\nother PIM types demonstrate that the proposed HH-PIM achieves up to $60.43$\npercent average energy savings over conventional PIMs while meeting application\nlatency requirements. These results confirm the suitability of HH-PIM for\nadaptive, energy-efficient AI processing in edge devices.",
      "tldr_zh": "该研究提出了一种异构混合PIM (HH-PIM)架构，旨在解决边缘AI设备中传统PIM架构的功耗和性能瓶颈。HH-PIM结合了高性能MRAM-SRAM PIM模块和低功耗MRAM-SRAM PIM模块，以适应动态变化的推理工作负载。此外，研究还提出了一种数据放置优化算法，能够根据计算需求动态分配数据，从而最大化能源效率。FPGA原型验证和功耗仿真结果表明，与传统PIM相比，HH-PIM在满足应用延迟要求的同时，平均节能高达60.43%，证明了其在边缘设备中自适应、节能AI处理方面的适用性。\n",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "7 pages, 6 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.01468v1",
      "published_date": "2025-04-02 08:22:32 UTC",
      "updated_date": "2025-04-02 08:22:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:14:49.121490"
    },
    {
      "arxiv_id": "2504.01459v1",
      "title": "Probabilistic Curriculum Learning for Goal-Based Reinforcement Learning",
      "title_zh": "基于概率课程学习的面向目标强化学习",
      "authors": [
        "Llewyn Salt",
        "Marcus Gallagher"
      ],
      "abstract": "Reinforcement learning (RL) -- algorithms that teach artificial agents to\ninteract with environments by maximising reward signals -- has achieved\nsignificant success in recent years. These successes have been facilitated by\nadvances in algorithms (e.g., deep Q-learning, deep deterministic policy\ngradients, proximal policy optimisation, trust region policy optimisation, and\nsoft actor-critic) and specialised computational resources such as GPUs and\nTPUs. One promising research direction involves introducing goals to allow\nmultimodal policies, commonly through hierarchical or curriculum reinforcement\nlearning. These methods systematically decompose complex behaviours into\nsimpler sub-tasks, analogous to how humans progressively learn skills (e.g. we\nlearn to run before we walk, or we learn arithmetic before calculus). However,\nfully automating goal creation remains an open challenge. We present a novel\nprobabilistic curriculum learning algorithm to suggest goals for reinforcement\nlearning agents in continuous control and navigation tasks.",
      "tldr_zh": "本文提出了一种新的概率课程学习(Probabilistic Curriculum Learning)算法，用于为基于目标的强化学习(Goal-Based Reinforcement Learning)智能体生成目标。该算法旨在解决强化学习中自动生成目标这一难题，特别是在连续控制和导航任务中。通过概率方法，该算法能够系统地分解复杂行为为更简单的子任务，类似于人类学习技能的过程。该方法能够帮助智能体学习多模态策略，并提升在复杂环境中的学习效率。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01459v1",
      "published_date": "2025-04-02 08:15:16 UTC",
      "updated_date": "2025-04-02 08:15:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:15:00.942899"
    },
    {
      "arxiv_id": "2504.01452v1",
      "title": "BiSeg-SAM: Weakly-Supervised Post-Processing Framework for Boosting Binary Segmentation in Segment Anything Models",
      "title_zh": "BiSeg-SAM：用于提升分割一切模型中二元分割性能的弱监督后处理框架\n",
      "authors": [
        "Encheng Su",
        "Hu Cao",
        "Alois Knoll"
      ],
      "abstract": "Accurate segmentation of polyps and skin lesions is essential for diagnosing\ncolorectal and skin cancers. While various segmentation methods for polyps and\nskin lesions using fully supervised deep learning techniques have been\ndeveloped, the pixel-level annotation of medical images by doctors is both\ntime-consuming and costly. Foundational vision models like the Segment Anything\nModel (SAM) have demonstrated superior performance; however, directly applying\nSAM to medical segmentation may not yield satisfactory results due to the lack\nof domain-specific medical knowledge. In this paper, we propose BiSeg-SAM, a\nSAM-guided weakly supervised prompting and boundary refinement network for the\nsegmentation of polyps and skin lesions. Specifically, we fine-tune SAM\ncombined with a CNN module to learn local features. We introduce a WeakBox with\ntwo functions: automatically generating box prompts for the SAM model and using\nour proposed Multi-choice Mask-to-Box (MM2B) transformation for rough\nmask-to-box conversion, addressing the mismatch between coarse labels and\nprecise predictions. Additionally, we apply scale consistency (SC) loss for\nprediction scale alignment. Our DetailRefine module enhances boundary precision\nand segmentation accuracy by refining coarse predictions using a limited amount\nof ground truth labels. This comprehensive approach enables BiSeg-SAM to\nachieve excellent multi-task segmentation performance. Our method demonstrates\nsignificant superiority over state-of-the-art (SOTA) methods when tested on\nfive polyp datasets and one skin cancer dataset.",
      "tldr_zh": "BiSeg-SAM是一个弱监督后处理框架，旨在提升Segment Anything Model (SAM)在二元分割任务中的性能，尤其是在息肉和皮肤病变的分割方面。该框架通过结合CNN模块微调SAM，学习局部特征，并引入WeakBox模块自动生成box prompts，同时提出Multi-choice Mask-to-Box (MM2B)转换以解决粗糙标签和精确预测之间的不匹配问题。此外，还应用了scale consistency (SC) loss进行预测尺度对齐。DetailRefine模块利用少量ground truth标签细化粗糙预测，提高边界精度和分割准确率。实验结果表明，BiSeg-SAM在五个息肉数据集和一个皮肤癌数据集上均优于现有SOTA方法。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "2024 IEEE International Conference on Bioinformatics and Biomedicine\n  (BIBM)",
      "pdf_url": "http://arxiv.org/pdf/2504.01452v1",
      "published_date": "2025-04-02 08:04:37 UTC",
      "updated_date": "2025-04-02 08:04:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:15:13.104364"
    },
    {
      "arxiv_id": "2504.01445v1",
      "title": "Enabling Systematic Generalization in Abstract Spatial Reasoning through Meta-Learning for Compositionality",
      "title_zh": "通过组合性元学习实现抽象空间推理中的系统泛化",
      "authors": [
        "Philipp Mondorf",
        "Shijia Zhou",
        "Monica Riedler",
        "Barbara Plank"
      ],
      "abstract": "Systematic generalization refers to the capacity to understand and generate\nnovel combinations from known components. Despite recent progress by large\nlanguage models (LLMs) across various domains, these models often fail to\nextend their knowledge to novel compositional scenarios, revealing notable\nlimitations in systematic generalization. There has been an ongoing debate\nabout whether neural networks possess the capacity for systematic\ngeneralization, with recent studies suggesting that meta-learning approaches\ndesigned for compositionality can significantly enhance this ability. However,\nthese insights have largely been confined to linguistic problems, leaving their\napplicability to other tasks an open question. In this study, we extend the\napproach of meta-learning for compositionality to the domain of abstract\nspatial reasoning. To this end, we introduce $\\textit{SYGAR}$-a dataset\ndesigned to evaluate the capacity of models to systematically generalize from\nknown geometric transformations (e.g., translation, rotation) of\ntwo-dimensional objects to novel combinations of these transformations (e.g.,\ntranslation+rotation). Our results show that a transformer-based\nencoder-decoder model, trained via meta-learning for compositionality, can\nsystematically generalize to previously unseen transformation compositions,\nsignificantly outperforming state-of-the-art LLMs, including o3-mini, GPT-4o,\nand Gemini 2.0 Flash, which fail to exhibit similar systematic behavior. Our\nfindings highlight the effectiveness of meta-learning in promoting\nsystematicity beyond linguistic tasks, suggesting a promising direction toward\nmore robust and generalizable models.",
      "tldr_zh": "本文探讨了神经网络在抽象空间推理中进行系统泛化的能力，即理解和生成已知组件的新组合。研究引入了$\\textit{SYGAR}$数据集，用于评估模型从已知的几何变换（如平移、旋转）系统泛化到这些变换的新组合（如平移+旋转）的能力。研究结果表明，通过组合性元学习训练的基于Transformer的编码器-解码器模型，能够系统地泛化到以前未见过的变换组合，显著优于包括o3-mini、GPT-4o和Gemini 2.0 Flash在内的最先进的LLM。该研究强调了元学习在促进语言任务之外的系统性方面的有效性，为更强大和更具泛化能力的模型指明了方向。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "30 pages, 14 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.01445v1",
      "published_date": "2025-04-02 07:56:39 UTC",
      "updated_date": "2025-04-02 07:56:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:15:25.251621"
    },
    {
      "arxiv_id": "2504.01444v1",
      "title": "PiCo: Jailbreaking Multimodal Large Language Models via $\\textbf{Pi}$ctorial $\\textbf{Co}$de Contextualization",
      "title_zh": "PiCo：通过$\\textbf{Pi}$ctorial $\\textbf{Co}$de情境化破解多模态大型语言模型",
      "authors": [
        "Aofan Liu",
        "Lulu Tang",
        "Ting Pan",
        "Yuguo Yin",
        "Bin Wang",
        "Ao Yang"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs), which integrate vision and other\nmodalities into Large Language Models (LLMs), significantly enhance AI\ncapabilities but also introduce new security vulnerabilities. By exploiting the\nvulnerabilities of the visual modality and the long-tail distribution\ncharacteristic of code training data, we present PiCo, a novel jailbreaking\nframework designed to progressively bypass multi-tiered defense mechanisms in\nadvanced MLLMs. PiCo employs a tier-by-tier jailbreak strategy, using\ntoken-level typographic attacks to evade input filtering and embedding harmful\nintent within programming context instructions to bypass runtime monitoring. To\ncomprehensively assess the impact of attacks, a new evaluation metric is\nfurther proposed to assess both the toxicity and helpfulness of model outputs\npost-attack. By embedding harmful intent within code-style visual instructions,\nPiCo achieves an average Attack Success Rate (ASR) of 84.13% on Gemini-Pro\nVision and 52.66% on GPT-4, surpassing previous methods. Experimental results\nhighlight the critical gaps in current defenses, underscoring the need for more\nrobust strategies to secure advanced MLLMs.",
      "tldr_zh": "该论文提出了PiCo，一种新型的越狱框架，专门用于绕过多模态大型语言模型(MLLMs)中的多层防御机制。PiCo利用视觉模态的漏洞和代码训练数据的长尾分布特性，采用分层越狱策略：首先使用token级别的排版攻击来绕过输入过滤，然后将有害意图嵌入到编程上下文指令中，以绕过运行时监控。为了评估攻击的影响，论文提出了一种新的评估指标，用于评估攻击后模型输出的毒性和有用性。实验表明，PiCo通过将有害意图嵌入到代码风格的视觉指令中，在Gemini-Pro Vision和GPT-4上分别实现了84.13%和52.66%的平均攻击成功率(ASR)，超过了以往的方法。研究结果强调了当前防御措施的关键缺陷，并突出了加强高级MLLM安全性的必要性。\n",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01444v1",
      "published_date": "2025-04-02 07:54:32 UTC",
      "updated_date": "2025-04-02 07:54:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:15:37.461679"
    },
    {
      "arxiv_id": "2504.01429v1",
      "title": "Refining Interactions: Enhancing Anisotropy in Graph Neural Networks with Language Semantics",
      "title_zh": "精炼交互：利用语言语义增强图神经网络中的各向异性\n",
      "authors": [
        "Zhaoxing Li",
        "Xiaoming Zhang",
        "Haifeng Zhang",
        "Chengxiang Liu"
      ],
      "abstract": "The integration of Large Language Models (LLMs) with Graph Neural Networks\n(GNNs) has recently been explored to enhance the capabilities of Text Attribute\nGraphs (TAGs). Most existing methods feed textual descriptions of the graph\nstructure or neighbouring nodes' text directly into LLMs. However, these\napproaches often cause LLMs to treat structural information simply as general\ncontextual text, thus limiting their effectiveness in graph-related tasks. In\nthis paper, we introduce LanSAGNN (Language Semantic Anisotropic Graph Neural\nNetwork), a framework that extends the concept of anisotropic GNNs to the\nnatural language level. This model leverages LLMs to extract tailor-made\nsemantic information for node pairs, effectively capturing the unique\ninteractions within node relationships. In addition, we propose an efficient\ndual-layer LLMs finetuning architecture to better align LLMs' outputs with\ngraph tasks. Experimental results demonstrate that LanSAGNN significantly\nenhances existing LLM-based methods without increasing complexity while also\nexhibiting strong robustness against interference.",
      "tldr_zh": "该论文提出了LanSAGNN (Language Semantic Anisotropic Graph Neural Network)，一个将大型语言模型(LLMs)与图神经网络(GNNs)结合的框架，旨在提升文本属性图(TAGs)的处理能力。LanSAGNN通过LLMs为节点对提取定制化的语义信息，有效捕捉节点关系中的独特交互，从而扩展了anisotropic GNNs的概念到自然语言层面。此外，论文还提出了一种高效的双层LLMs微调架构，以更好地将LLMs的输出与图任务对齐。实验结果表明，LanSAGNN在不增加复杂性的前提下，显著增强了现有的基于LLM的方法，并展现出强大的抗干扰能力。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ICME 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.01429v1",
      "published_date": "2025-04-02 07:32:45 UTC",
      "updated_date": "2025-04-02 07:32:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:15:49.059225"
    },
    {
      "arxiv_id": "2504.01428v1",
      "title": "MuTri: Multi-view Tri-alignment for OCT to OCTA 3D Image Translation",
      "title_zh": "MuTri：用于 OCT 到 OCTA 3D 图像转换的多视角三重对齐\n",
      "authors": [
        "Zhuangzhuang Chen",
        "Hualiang Wang",
        "Chubin Ou",
        "Xiaomeng Li"
      ],
      "abstract": "Optical coherence tomography angiography (OCTA) shows its great importance in\nimaging microvascular networks by providing accurate 3D imaging of blood\nvessels, but it relies upon specialized sensors and expensive devices. For this\nreason, previous works show the potential to translate the readily available 3D\nOptical Coherence Tomography (OCT) images into 3D OCTA images. However,\nexisting OCTA translation methods directly learn the mapping from the OCT\ndomain to the OCTA domain in continuous and infinite space with guidance from\nonly a single view, i.e., the OCTA project map, resulting in suboptimal\nresults. To this end, we propose the multi-view Tri-alignment framework for OCT\nto OCTA 3D image translation in discrete and finite space, named MuTri. In the\nfirst stage, we pre-train two vector-quantized variational auto-encoder (VQ-\nVAE) by reconstructing 3D OCT and 3D OCTA data, providing semantic prior for\nsubsequent multi-view guidances. In the second stage, our multi-view\ntri-alignment facilitates another VQVAE model to learn the mapping from the OCT\ndomain to the OCTA domain in discrete and finite space. Specifically, a\ncontrastive-inspired semantic alignment is proposed to maximize the mutual\ninformation with the pre-trained models from OCT and OCTA views, to facilitate\ncodebook learning. Meanwhile, a vessel structure alignment is proposed to\nminimize the structure discrepancy with the pre-trained models from the OCTA\nproject map view, benefiting from learning the detailed vessel structure\ninformation. We also collect the first large-scale dataset, namely, OCTA2024,\nwhich contains a pair of OCT and OCTA volumes from 846 subjects.",
      "tldr_zh": "该论文提出了一种名为MuTri的多视角三对齐框架，用于将3D OCT图像转换为3D OCTA图像，旨在解决OCTA成像设备依赖性强的问题。MuTri框架首先通过两个VQ-VAE预训练模型分别重建3D OCT和3D OCTA数据，提供语义先验。然后，利用多视角三对齐机制，借助对比学习的语义对齐和血管结构对齐，引导另一个VQ-VAE模型学习从OCT到OCTA的映射关系。此外，作者还构建了首个大规模数据集OCTA2024，包含846名受试者的OCT和OCTA配对数据。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01428v1",
      "published_date": "2025-04-02 07:28:09 UTC",
      "updated_date": "2025-04-02 07:28:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:16:01.280794"
    },
    {
      "arxiv_id": "2504.01420v1",
      "title": "FAIRE: Assessing Racial and Gender Bias in AI-Driven Resume Evaluations",
      "title_zh": "FAIRE：评估 AI 驱动的简历评估中的种族和性别偏见\n",
      "authors": [
        "Athena Wen",
        "Tanush Patil",
        "Ansh Saxena",
        "Yicheng Fu",
        "Sean O'Brien",
        "Kevin Zhu"
      ],
      "abstract": "In an era where AI-driven hiring is transforming recruitment practices,\nconcerns about fairness and bias have become increasingly important. To explore\nthese issues, we introduce a benchmark, FAIRE (Fairness Assessment In Resume\nEvaluation), to test for racial and gender bias in large language models (LLMs)\nused to evaluate resumes across different industries. We use two methods-direct\nscoring and ranking-to measure how model performance changes when resumes are\nslightly altered to reflect different racial or gender identities. Our findings\nreveal that while every model exhibits some degree of bias, the magnitude and\ndirection vary considerably. This benchmark provides a clear way to examine\nthese differences and offers valuable insights into the fairness of AI-based\nhiring tools. It highlights the urgent need for strategies to reduce bias in\nAI-driven recruitment. Our benchmark code and dataset are open-sourced at our\nrepository:\nhttps://github.com/athenawen/FAIRE-Fairness-Assessment-In-Resume-Evaluation.git.",
      "tldr_zh": "该论文提出了FAIRE (Fairness Assessment In Resume Evaluation) 基准，用于评估大型语言模型(LLMs)在简历评估中存在的种族和性别偏见。研究采用直接评分和排序两种方法，通过细微调整简历中的种族或性别信息来衡量模型性能的变化。实验结果表明，所有模型都存在一定程度的偏见，但程度和方向各不相同。FAIRE为检验这些差异提供了一种清晰的方法，并揭示了AI驱动招聘工具公平性的重要见解，强调了减少AI招聘偏见的迫切需求。该基准的代码和数据集已开源。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01420v1",
      "published_date": "2025-04-02 07:11:30 UTC",
      "updated_date": "2025-04-02 07:11:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:16:13.229373"
    },
    {
      "arxiv_id": "2504.01407v1",
      "title": "TimeSearch: Hierarchical Video Search with Spotlight and Reflection for Human-like Long Video Understanding",
      "title_zh": "TimeSearch：通过聚焦和反思进行分层视频搜索，实现类人长视频理解\n",
      "authors": [
        "Junwen Pan",
        "Rui Zhang",
        "Xin Wan",
        "Yuan Zhang",
        "Ming Lu",
        "Qi She"
      ],
      "abstract": "Large video-language models (LVLMs) have shown remarkable performance across\nvarious video-language tasks. However, they encounter significant challenges\nwhen processing long videos because of the large number of video frames\ninvolved. Downsampling long videos in either space or time can lead to visual\nhallucinations, making it difficult to accurately interpret long videos.\nMotivated by human hierarchical temporal search strategies, we propose\n\\textbf{TimeSearch}, a novel framework enabling LVLMs to understand long videos\nin a human-like manner. TimeSearch integrates two human-like primitives into a\nunified autoregressive LVLM: 1) \\textbf{Spotlight} efficiently identifies\nrelevant temporal events through a Temporal-Augmented Frame Representation\n(TAFR), explicitly binding visual features with timestamps; 2)\n\\textbf{Reflection} evaluates the correctness of the identified events,\nleveraging the inherent temporal self-reflection capabilities of LVLMs.\nTimeSearch progressively explores key events and prioritizes temporal search\nbased on reflection confidence. Extensive experiments on challenging long-video\nbenchmarks confirm that TimeSearch substantially surpasses previous\nstate-of-the-art, improving the accuracy from 41.8\\% to 51.5\\% on the LVBench.\nAdditionally, experiments on temporal grounding demonstrate that appropriate\nTAFR is adequate to effectively stimulate the surprising temporal grounding\nability of LVLMs in a simpler yet versatile manner, which improves mIoU on\nCharades-STA by 11.8\\%. The code will be released.",
      "tldr_zh": "该论文提出了TimeSearch，一个用于长视频理解的新框架，旨在使LVLMs（大型视频语言模型）能够像人类一样理解长视频。TimeSearch模仿人类的层级时间搜索策略，集成了两个关键机制：Spotlight通过时间增强帧表示(TAFR)高效识别相关的时间事件，并将视觉特征与时间戳显式绑定；Reflection则利用LVLMs的时间自反思能力评估识别事件的正确性。TimeSearch通过逐步探索关键事件并基于反思置信度优先排序时间搜索。实验表明，TimeSearch在长视频基准测试LVBench上显著超越了现有技术，准确率从41.8%提高到51.5%。此外，在时间定位任务中，TimeSearch通过适当的TAFR有效激发了LVLMs的时间定位能力，在Charades-STA数据集上将mIoU提高了11.8%。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01407v1",
      "published_date": "2025-04-02 06:47:19 UTC",
      "updated_date": "2025-04-02 06:47:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:16:25.527364"
    },
    {
      "arxiv_id": "2504.01403v1",
      "title": "Generative Retrieval and Alignment Model: A New Paradigm for E-commerce Retrieval",
      "title_zh": "生成式检索和对齐模型：一种新的电子商务检索范式\n",
      "authors": [
        "Ming Pang",
        "Chunyuan Yuan",
        "Xiaoyu He",
        "Zheng Fang",
        "Donghao Xie",
        "Fanyi Qu",
        "Xue Jiang",
        "Changping Peng",
        "Zhangang Lin",
        "Zheng Luo",
        "Jingping Shao"
      ],
      "abstract": "Traditional sparse and dense retrieval methods struggle to leverage general\nworld knowledge and often fail to capture the nuanced features of queries and\nproducts. With the advent of large language models (LLMs), industrial search\nsystems have started to employ LLMs to generate identifiers for product\nretrieval. Commonly used identifiers include (1) static/semantic IDs and (2)\nproduct term sets. The first approach requires creating a product ID system\nfrom scratch, missing out on the world knowledge embedded within LLMs. While\nthe second approach leverages this general knowledge, the significant\ndifference in word distribution between queries and products means that\nproduct-based identifiers often do not align well with user search queries,\nleading to missed product recalls. Furthermore, when queries contain numerous\nattributes, these algorithms generate a large number of identifiers, making it\ndifficult to assess their quality, which results in low overall recall\nefficiency.\n  To address these challenges, this paper introduces a novel e-commerce\nretrieval paradigm: the Generative Retrieval and Alignment Model (GRAM). GRAM\nemploys joint training on text information from both queries and products to\ngenerate shared text identifier codes, effectively bridging the gap between\nqueries and products. This approach not only enhances the connection between\nqueries and products but also improves inference efficiency. The model uses a\nco-alignment strategy to generate codes optimized for maximizing retrieval\nefficiency. Additionally, it introduces a query-product scoring mechanism to\ncompare product values across different codes, further boosting retrieval\nefficiency. Extensive offline and online A/B testing demonstrates that GRAM\nsignificantly outperforms traditional models and the latest generative\nretrieval models, confirming its effectiveness and practicality.",
      "tldr_zh": "该论文提出了一种新的电商检索范式：生成式检索与对齐模型(Generative Retrieval and Alignment Model, GRAM)，旨在解决传统检索方法无法有效利用通用知识以及难以捕捉查询和商品细微特征的问题。GRAM通过联合训练查询和商品的文本信息，生成共享的文本标识符代码，从而弥合了查询和商品之间的差距。该模型采用协同对齐策略生成优化代码以最大化检索效率，并引入查询-商品评分机制来比较不同代码下的商品价值，进一步提升检索效率。离线和在线A/B测试表明，GRAM显著优于传统模型和最新的生成式检索模型。\n",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted by WWW2025",
      "pdf_url": "http://arxiv.org/pdf/2504.01403v1",
      "published_date": "2025-04-02 06:40:09 UTC",
      "updated_date": "2025-04-02 06:40:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:16:37.054585"
    },
    {
      "arxiv_id": "2504.01400v1",
      "title": "ToolACE-R: Tool Learning with Adaptive Self-Refinement",
      "title_zh": "ToolACE-R：基于自适应自精炼的工具学习\n",
      "authors": [
        "Xingshan Zeng",
        "Weiwen Liu",
        "Xu Huang",
        "Zezhong Wang",
        "Lingzhi Wang",
        "Liangyou Li",
        "Yasheng Wang",
        "Lifeng Shang",
        "Xin Jiang",
        "Ruiming Tang",
        "Qun Liu"
      ],
      "abstract": "Tool learning, which allows Large Language Models (LLMs) to leverage external\ntools for solving complex user tasks, has emerged as a promising avenue for\nextending model capabilities. However, current approaches primarily focus on\ndata synthesis for fine-tuning LLMs to invoke tools effectively, largely\nignoring how to fully stimulate the potential of the model. In this paper, we\npropose ToolACE-R, a novel method that introduces adaptive self-refinement for\ntool invocations. Our approach features a model-aware iterative training\nprocedure that progressively incorporates more training samples based on the\nmodel's evolving capabilities. Additionally, it allows LLMs to iteratively\nrefine their tool calls, optimizing performance without requiring external\nfeedback. To further enhance computational efficiency, we integrate an adaptive\nmechanism when scaling the inference time, enabling the model to autonomously\ndetermine when to stop the refinement process. We conduct extensive experiments\nacross several benchmark datasets, showing that ToolACE-R achieves competitive\nperformance compared to advanced API-based models, even without any refinement.\nFurthermore, its performance can be further improved efficiently through\nadaptive self-refinement. Our results demonstrate the effectiveness of the\nproposed method, which is compatible with base models of various sizes,\noffering a promising direction for more efficient tool learning.",
      "tldr_zh": "该论文提出了ToolACE-R，一种用于工具学习的自适应自精炼方法，旨在充分激发大型语言模型（LLMs）利用外部工具解决复杂任务的潜力。ToolACE-R采用模型感知的迭代训练过程，逐步纳入更多训练样本，并允许LLMs迭代地改进其工具调用，从而优化性能而无需外部反馈。此外，该方法集成了自适应机制以提高计算效率，使模型能够自主决定何时停止精炼过程。实验结果表明，ToolACE-R在多个基准数据集上实现了与高级API模型相比具有竞争力的性能，并且可以通过自适应自精炼进一步提高效率，适用于各种规模的基础模型。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01400v1",
      "published_date": "2025-04-02 06:38:56 UTC",
      "updated_date": "2025-04-02 06:38:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:16:49.289570"
    },
    {
      "arxiv_id": "2504.01395v1",
      "title": "From Easy to Hard: Building a Shortcut for Differentially Private Image Synthesis",
      "title_zh": "从易到难：构建差分隐私图像合成的捷径\n",
      "authors": [
        "Kecen Li",
        "Chen Gong",
        "Xiaochen Li",
        "Yuzhong Zhao",
        "Xinwen Hou",
        "Tianhao Wang"
      ],
      "abstract": "Differentially private (DP) image synthesis aims to generate synthetic images\nfrom a sensitive dataset, alleviating the privacy leakage concerns of\norganizations sharing and utilizing synthetic images. Although previous methods\nhave significantly progressed, especially in training diffusion models on\nsensitive images with DP Stochastic Gradient Descent (DP-SGD), they still\nsuffer from unsatisfactory performance. In this work, inspired by curriculum\nlearning, we propose a two-stage DP image synthesis framework, where diffusion\nmodels learn to generate DP synthetic images from easy to hard. Unlike existing\nmethods that directly use DP-SGD to train diffusion models, we propose an easy\nstage in the beginning, where diffusion models learn simple features of the\nsensitive images. To facilitate this easy stage, we propose to use `central\nimages', simply aggregations of random samples of the sensitive dataset.\nIntuitively, although those central images do not show details, they\ndemonstrate useful characteristics of all images and only incur minimal privacy\ncosts, thus helping early-phase model training. We conduct experiments to\npresent that on the average of four investigated image datasets, the fidelity\nand utility metrics of our synthetic images are 33.1% and 2.1% better than the\nstate-of-the-art method.",
      "tldr_zh": "该论文提出了一种两阶段的差分隐私(DP)图像合成框架，旨在提升在保护隐私的前提下生成高质量合成图像的性能。该框架受到课程学习的启发，让扩散模型从易到难地学习生成DP合成图像。第一阶段，模型通过使用“中心图像”（敏感数据集随机样本的聚合）学习简单特征，由于中心图像仅需极小的隐私成本，因此有助于模型早期训练。实验结果表明，在四个图像数据集上，该方法生成的合成图像的保真度和效用性指标平均比现有最佳方法分别提高了33.1%和2.1%。该研究为差分隐私图像合成提供了一种新的有效策略。\n",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted at IEEE S&P (Oakland) 2025; code available at\n  https://github.com/SunnierLee/DP-FETA",
      "pdf_url": "http://arxiv.org/pdf/2504.01395v1",
      "published_date": "2025-04-02 06:30:55 UTC",
      "updated_date": "2025-04-02 06:30:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:17:01.287129"
    },
    {
      "arxiv_id": "2504.01382v1",
      "title": "An Illusion of Progress? Assessing the Current State of Web Agents",
      "title_zh": "一种进步的错觉？评估 Web Agents 的当前状态\n",
      "authors": [
        "Tianci Xue",
        "Weijian Qi",
        "Tianneng Shi",
        "Chan Hee Song",
        "Boyu Gou",
        "Dawn Song",
        "Huan Sun",
        "Yu Su"
      ],
      "abstract": "As digitalization and cloud technologies evolve, the web is becoming\nincreasingly important in the modern society. Autonomous web agents based on\nlarge language models (LLMs) hold a great potential in work automation. It is\ntherefore important to accurately measure and monitor the progression of their\ncapabilities. In this work, we conduct a comprehensive and rigorous assessment\nof the current state of web agents. Our results depict a very different picture\nof the competency of current agents, suggesting over-optimism in previously\nreported results. This gap can be attributed to shortcomings in existing\nbenchmarks. We introduce Online-Mind2Web, an online evaluation benchmark\nconsisting of 300 diverse and realistic tasks spanning 136 websites. It enables\nus to evaluate web agents under a setting that approximates how real users use\nthese agents. To facilitate more scalable evaluation and development, we also\ndevelop a novel LLM-as-a-Judge automatic evaluation method and show that it can\nachieve around 85% agreement with human judgment, substantially higher than\nexisting methods. Finally, we present the first comprehensive comparative\nanalysis of current web agents, highlighting both their strengths and\nlimitations to inspire future research.",
      "tldr_zh": "该论文对当前Web Agent的能力进行了全面评估，发现现有benchmark存在缺陷，导致之前的结果过于乐观。为了更真实地评估Web Agent，作者提出了Online-Mind2Web，一个包含300个任务、覆盖136个网站的在线评估benchmark。同时，为了实现更可扩展的评估和开发，作者开发了一种新颖的LLM-as-a-Judge自动评估方法，该方法与人类判断的吻合度达到85%。最后，论文对当前Web Agent进行了全面的对比分析，指出了它们的优点和局限性，旨在激发未来的研究。\n",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "22 pages, 16 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.01382v1",
      "published_date": "2025-04-02 05:51:29 UTC",
      "updated_date": "2025-04-02 05:51:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:17:13.125939"
    },
    {
      "arxiv_id": "2504.01366v1",
      "title": "Virtual Reality and Artificial Intelligence as Psychological Countermeasures in Space and Other Isolated and Confined Environments: A Scoping Review",
      "title_zh": "虚拟现实和人工智能作为太空和其他隔离封闭环境中的心理对策：范围界定综述\n",
      "authors": [
        "Jennifer Sharp",
        "Joshua Kelson",
        "Daryl South",
        "Anthony Saliba",
        "Muhammad Ashad Kabir"
      ],
      "abstract": "Spaceflight is an isolated and confined environment (ICE) that exposes\nastronauts to psychological hazards, such as stress, danger, and monotony.\nVirtual reality (VR) and artificial intelligence (AI) technologies can serve as\npsychological countermeasures as they can digitally simulate immersive\nenvironments, interactive companions, and therapeutic experiences. Our study\nemploys a scoping literature review approach to identify what is currently\nknown about the use and effectiveness of VR and AI-based interventions as\npsychological countermeasures to improve mood or emotional states in adults in\nspace or other ICEs. Additionally, this review aimed to identify gaps in the\nknowledge base and whether a systematic review with meta-analysis was\nwarranted. The review included studies where the intervention was used or\nintended for use in space or other extraterrestrial environments (ICE). Our\nsearch strategy yielded 19 studies from 3390 records across seven major\ndatabases. All studies focused on VR-based interventions, with no eligible\nAI-based intervention studies found. VR interventions were found to be\neffective for relaxation and improving mood, emergency training, as an\ninteractive communication platform, for comparing interior designs, and for\nenhancing exercise. There were improvements for measures of mood and emotion\\n\n(e.g., anxiety and stress); however, user preferences varied, and some\ninstances of cybersickness were reported. A systematic review with\nmeta-analysis is not recommended due to the heterogeneity of results. There is\nsignificant scope for further research into the use of VR for a wider range of\nmood and emotion variables using standardised assessment instruments.\nAdditionally, the potential application of AI as a psychological countermeasure\nwarrants further investigation.",
      "tldr_zh": "该研究对虚拟现实(VR)和人工智能(AI)技术在太空及其他隔离封闭环境(ICE)中作为心理对策的应用进行了范围界定综述。旨在评估VR和AI干预措施在改善宇航员或其他ICE环境中成年人的情绪状态方面的有效性。通过检索七个主要数据库的3390条记录，最终纳入了19项研究，但所有研究都集中在VR干预上，没有发现符合条件的AI干预研究。研究发现VR干预在放松心情、改善情绪、紧急情况训练、互动交流平台、室内设计比较和增强锻炼方面有效。尽管情绪指标有所改善，但用户偏好各异，且报告了一些晕动症病例。由于结果的异质性，不建议进行meta分析的系统评价。未来的研究应探索VR在更广泛的情绪变量中的应用，并使用标准化评估工具，同时进一步研究AI作为心理对策的潜力。\n",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.HC",
      "comment": "34 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.01366v1",
      "published_date": "2025-04-02 05:25:29 UTC",
      "updated_date": "2025-04-02 05:25:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:17:25.723303"
    },
    {
      "arxiv_id": "2504.01337v1",
      "title": "Advancing MoE Efficiency: A Collaboration-Constrained Routing (C2R) Strategy for Better Expert Parallelism Design",
      "title_zh": "提升 MoE 效率：一种用于更好专家并行设计的协作约束路由 (C2R) 策略\n",
      "authors": [
        "Mohan Zhang",
        "Pingzhi Li",
        "Jie Peng",
        "Mufan Qiu",
        "Tianlong Chen"
      ],
      "abstract": "Mixture-of-Experts (MoE) has successfully scaled up models while maintaining\nnearly constant computing costs. By employing a gating network to route input\ntokens, it selectively activates a subset of expert networks to process the\ncorresponding token embeddings. However, in practice, the efficiency of MoE is\nchallenging to achieve due to two key reasons: imbalanced expert activation,\nwhich leads to substantial idle time during model or expert parallelism, and\ninsufficient capacity utilization; massive communication overhead, induced by\nnumerous expert routing combinations in expert parallelism at the system level.\nPrevious works typically formulate it as the load imbalance issue characterized\nby the gating network favoring certain experts over others or attribute it to\nstatic execution which fails to adapt to the dynamic expert workload at\nruntime. In this paper, we exploit it from a brand new perspective, a\nhigher-order view and analysis of MoE routing policies: expert collaboration\nand specialization where some experts tend to activate broadly with others\n(collaborative), while others are more likely to activate only with a specific\nsubset of experts (specialized). Our experiments reveal that most experts tend\nto be overly collaborative, leading to increased communication overhead from\nrepeatedly sending tokens to different accelerators. To this end, we propose a\nnovel collaboration-constrained routing (C2R) strategy to encourage more\nspecialized expert groups, as well as to improve expert utilization, and\npresent an efficient implementation of MoE that further leverages expert\nspecialization. We achieve an average performance improvement of 0.51% and\n0.33% on LLaMA-MoE and Qwen-MoE respectively across ten downstream NLP\nbenchmarks, and reduce the all2all communication costs between GPUs, bringing\nan extra 20%-30% total running time savings on top of the existing SoTA, i.e.\nMegaBlocks.",
      "tldr_zh": "本文从专家协作和专业化的角度，分析了MoE模型路由策略的效率瓶颈。研究发现，多数专家过度协作导致通信开销增加。为此，提出了一种新的协作约束路由(C2R)策略，旨在鼓励更专业化的专家组，提高专家利用率。实验结果表明，C2R策略在LLaMA-MoE和Qwen-MoE模型上，分别在十个下游NLP基准测试中实现了平均0.51%和0.33%的性能提升，并减少了GPU之间的all2all通信成本，在现有SoTA模型（如MegaBlocks）的基础上，额外节省了20%-30%的总运行时间。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "NAACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.01337v1",
      "published_date": "2025-04-02 03:51:59 UTC",
      "updated_date": "2025-04-02 03:51:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:17:37.159230"
    },
    {
      "arxiv_id": "2504.01331v1",
      "title": "An Explainable Reconfiguration-Based Optimization Algorithm for Industrial and Reliability-Redundancy Allocation Problems",
      "title_zh": "一种基于可解释重构的优化算法，用于解决工业和可靠性冗余分配问题\n",
      "authors": [
        "Dikshit Chauhan",
        "Nitin Gupta",
        "Anupam Yadav"
      ],
      "abstract": "Industrial and reliability optimization problems often involve complex\nconstraints and require efficient, interpretable solutions. This paper presents\nAI-AEFA, an advanced parameter reconfiguration-based metaheuristic algorithm\ndesigned to address large-scale industrial and reliability-redundancy\nallocation problems. AI-AEFA enhances search space exploration and convergence\nefficiency through a novel log-sigmoid-based parameter adaptation and chaotic\nmapping mechanism. The algorithm is validated across twenty-eight IEEE CEC 2017\nconstrained benchmark problems, fifteen large-scale industrial optimization\nproblems, and seven reliability-redundancy allocation problems, consistently\noutperforming state-of-the-art optimization techniques in terms of feasibility,\ncomputational efficiency, and convergence speed. The additional key\ncontribution of this work is the integration of SHAP (Shapley Additive\nExplanations) to enhance the interpretability of AI-AEFA, providing insights\ninto the impact of key parameters such as Coulomb's constant, charge,\nacceleration, and electrostatic force. This explainability feature enables a\ndeeper understanding of decision-making within the AI-AEFA framework during the\noptimization processes. The findings confirm AI-AEFA as a robust, scalable, and\ninterpretable optimization tool with significant real-world applications.",
      "tldr_zh": "本文提出了一种基于参数重构的可解释元启发式算法AI-AEFA，用于解决大规模工业和可靠性冗余分配问题。AI-AEFA通过对数sigmoid参数自适应和混沌映射机制，增强了搜索空间探索和收敛效率。该算法在多个基准问题和实际工业优化问题上验证，性能优于现有技术。此外，该研究集成了SHAP (Shapley Additive Explanations)方法，增强了AI-AEFA的解释性，揭示了库仑常数、电荷、加速度和静电力等关键参数的影响，从而深入理解优化过程中的决策机制。实验结果表明，AI-AEFA是一种鲁棒、可扩展且可解释的优化工具，具有重要的实际应用价值。\n",
      "categories": [
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "38 pages, 12 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.01331v1",
      "published_date": "2025-04-02 03:33:48 UTC",
      "updated_date": "2025-04-02 03:33:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:17:49.335516"
    },
    {
      "arxiv_id": "2504.01326v1",
      "title": "CFMD: Dynamic Cross-layer Feature Fusion for Salient Object Detection",
      "title_zh": "CFMD：用于显著目标检测的动态跨层特征融合\n",
      "authors": [
        "Jin Lian",
        "Zhongyu Wan",
        "Ming Gao",
        "JunFeng Chen"
      ],
      "abstract": "Cross-layer feature pyramid networks (CFPNs) have achieved notable progress\nin multi-scale feature fusion and boundary detail preservation for salient\nobject detection. However, traditional CFPNs still suffer from two core\nlimitations: (1) a computational bottleneck caused by complex feature weighting\noperations, and (2) degraded boundary accuracy due to feature blurring in the\nupsampling process. To address these challenges, we propose CFMD, a novel\ncross-layer feature pyramid network that introduces two key innovations. First,\nwe design a context-aware feature aggregation module (CFLMA), which\nincorporates the state-of-the-art Mamba architecture to construct a dynamic\nweight distribution mechanism. This module adaptively adjusts feature\nimportance based on image context, significantly improving both representation\nefficiency and generalization. Second, we introduce an adaptive dynamic\nupsampling unit (CFLMD) that preserves spatial details during resolution\nrecovery. By adjusting the upsampling range dynamically and initializing with a\nbilinear strategy, the module effectively reduces feature overlap and maintains\nfine-grained boundary structures. Extensive experiments on three standard\nbenchmarks using three mainstream backbone networks demonstrate that CFMD\nachieves substantial improvements in pixel-level accuracy and boundary\nsegmentation quality, especially in complex scenes. The results validate the\neffectiveness of CFMD in jointly enhancing computational efficiency and\nsegmentation performance, highlighting its strong potential in salient object\ndetection tasks.",
      "tldr_zh": "这篇论文提出了一个名为CFMD的动态跨层特征融合网络，用于显著性目标检测。该网络旨在解决传统跨层特征金字塔网络(CFPNs)中存在的计算瓶颈和边界精度下降问题。CFMD包含两个关键创新：上下文感知特征聚合模块(CFLMA)，利用Mamba架构构建动态权重分配机制，提升表征效率和泛化能力；自适应动态上采样单元(CFLMD)，通过动态调整上采样范围和双线性初始化，减少特征重叠，保持精细的边界结构。实验结果表明，CFMD在像素级精度和边界分割质量方面均有显著提升，尤其是在复杂场景中，验证了其在显著性目标检测任务中的有效性。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01326v1",
      "published_date": "2025-04-02 03:22:36 UTC",
      "updated_date": "2025-04-02 03:22:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:18:01.219768"
    },
    {
      "arxiv_id": "2504.01324v1",
      "title": "On Data Synthesis and Post-training for Visual Abstract Reasoning",
      "title_zh": "关于视觉抽象推理的数据合成和后训练研究\n",
      "authors": [
        "Ke Zhu",
        "Yu Wang",
        "Jiangjiang Liu",
        "Qunyi Xie",
        "Shanshan Liu",
        "Gang Zhang"
      ],
      "abstract": "This paper is a pioneering work attempting to address abstract visual\nreasoning (AVR) problems for large vision-language models (VLMs). We make a\ncommon LLaVA-NeXT 7B model capable of perceiving and reasoning about specific\nAVR problems, surpassing both open-sourced (e.g., Qwen-2-VL-72B) and\nclosed-sourced powerful VLMs (e.g., GPT-4o) with significant margin. This is a\ngreat breakthrough since almost all previous VLMs fail or show nearly random\nperformance on representative AVR benchmarks. Our key success is our innovative\ndata synthesis and post-training process, aiming to fully relieve the task\ndifficulty and elicit the model to learn, step by step. Our 7B model is also\nshown to be behave well on AVR without sacrificing common multimodal\ncomprehension abilities. We hope our paper could serve as an early effort in\nthis area and would inspire further research in abstract visual reasoning.",
      "tldr_zh": "该论文开创性地探索了如何使大型视觉语言模型(VLMs)解决抽象视觉推理(AVR)问题。通过创新的数据合成和后训练流程，作者成功地使一个普通的LLaVA-NeXT 7B模型具备了感知和推理特定AVR问题的能力，性能显著超越了开源模型(如Qwen-2-VL-72B)和闭源模型(如GPT-4o)。这一突破克服了以往VLMs在AVR基准测试中表现不佳的问题。该7B模型在AVR上表现良好，同时没有牺牲其通用的多模态理解能力。该研究为抽象视觉推理领域的进一步研究提供了借鉴。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01324v1",
      "published_date": "2025-04-02 03:18:24 UTC",
      "updated_date": "2025-04-02 03:18:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:18:13.265274"
    },
    {
      "arxiv_id": "2504.01321v1",
      "title": "COST: Contrastive One-Stage Transformer for Vision-Language Small Object Tracking",
      "title_zh": "COST：用于视觉-语言小目标跟踪的对比式单阶段 Transformer\n",
      "authors": [
        "Chunhui Zhang",
        "Li Liu",
        "Jialin Gao",
        "Xin Sun",
        "Hao Wen",
        "Xi Zhou",
        "Shiming Ge",
        "Yanfeng Wang"
      ],
      "abstract": "Transformer has recently demonstrated great potential in improving\nvision-language (VL) tracking algorithms. However, most of the existing VL\ntrackers rely on carefully designed mechanisms to perform the multi-stage\nmulti-modal fusion. Additionally, direct multi-modal fusion without alignment\nignores distribution discrepancy between modalities in feature space,\npotentially leading to suboptimal representations. In this work, we propose\nCOST, a contrastive one-stage transformer fusion framework for VL tracking,\naiming to learn semantically consistent and unified VL representations.\nSpecifically, we introduce a contrastive alignment strategy that maximizes\nmutual information (MI) between a video and its corresponding language\ndescription. This enables effective cross-modal alignment, yielding\nsemantically consistent features in the representation space. By leveraging a\nvisual-linguistic transformer, we establish an efficient multi-modal fusion and\nreasoning mechanism, empirically demonstrating that a simple stack of\ntransformer encoders effectively enables unified VL representations. Moreover,\nwe contribute a newly collected VL tracking benchmark dataset for small object\ntracking, named VL-SOT500, with bounding boxes and language descriptions. Our\ndataset comprises two challenging subsets, VL-SOT230 and VL-SOT270, dedicated\nto evaluating generic and high-speed small object tracking, respectively. Small\nobject tracking is notoriously challenging due to weak appearance and limited\nfeatures, and this dataset is, to the best of our knowledge, the first to\nexplore the usage of language cues to enhance visual representation for small\nobject tracking. Extensive experiments demonstrate that COST achieves\nstate-of-the-art performance on five existing VL tracking datasets, as well as\non our proposed VL-SOT500 dataset. Source codes and dataset will be made\npublicly available.",
      "tldr_zh": "该论文提出了一个对比单阶段Transformer框架（COST），用于视觉-语言(VL)小目标跟踪，旨在学习语义一致且统一的VL表示。COST引入了一种对比对齐策略，通过最大化视频和其对应语言描述之间的互信息(MI)来实现有效的跨模态对齐，从而在表示空间中产生语义一致的特征。该框架利用视觉-语言Transformer建立高效的多模态融合和推理机制。此外，作者还构建了一个新的VL跟踪基准数据集VL-SOT500，专门用于小目标跟踪，包含VL-SOT230和VL-SOT270两个子集，分别用于评估通用和高速小目标跟踪。实验表明，COST在五个现有VL跟踪数据集以及提出的VL-SOT500数据集上均实现了最先进的性能。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint submitted to Elsevier.\n  https://github.com/983632847/Awesome-Multimodal-Object-Tracking",
      "pdf_url": "http://arxiv.org/pdf/2504.01321v1",
      "published_date": "2025-04-02 03:12:38 UTC",
      "updated_date": "2025-04-02 03:12:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:18:25.389566"
    },
    {
      "arxiv_id": "2504.01317v1",
      "title": "Adaptive Rectification Sampling for Test-Time Compute Scaling",
      "title_zh": "用于测试时计算缩放的自适应校正采样\n",
      "authors": [
        "Zhendong Tan",
        "Xingjun Zhang",
        "Chaoyi Hu",
        "Yancheng Pan",
        "Shaoxun Wang"
      ],
      "abstract": "The newly released OpenAI-o1 and DeepSeek-R1 have demonstrated that test-time\nscaling can significantly improve model performance, especially in complex\ntasks such as logical reasoning. Common test-time scaling methods involve\ngenerating more chain of thoughts (CoTs) or longer CoTs with self-correction.\nHowever, while self-correction can improve performance, it may lead to\nsignificant token waste and reduce readability of the CoT if the reasoning\nsteps are already correct. To demonstrate that large language models (LLMs) can\nrectify errors at a more fine-grained level, we propose Adaptive Rectification\nSampling (AR-Sampling), which can guide the LLMs to self-correction at the\nappropriate step. AR-Sampling leverages a process-supervised reward model (PRM)\nas a verifier and constructed trigger sentences to guide the model in adaptive\nstep-level rethinking. Through the experiments on GSM8K and MATH500, it\nindicate that our approach enables the models to rethink in more fine-grained\nlevel, improving the accuracy of solutions, while generating a reasonable\nnumber of additional tokens.",
      "tldr_zh": "该论文提出了自适应修正采样(AR-Sampling)方法，旨在更精细地利用LLM的test-time scaling能力。AR-Sampling通过一个过程监督奖励模型(PRM)作为验证器，并构建触发语句，引导模型在步骤级别进行自适应的反思和修正，避免了不必要的计算浪费。实验表明，在GSM8K和MATH500数据集上，AR-Sampling能使模型在更精细的层面上进行反思，提高了解题准确率，同时生成合理数量的额外tokens。该方法验证了LLM可以在更细粒度层面上纠正错误。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01317v1",
      "published_date": "2025-04-02 02:57:52 UTC",
      "updated_date": "2025-04-02 02:57:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:18:37.395694"
    },
    {
      "arxiv_id": "2504.01309v1",
      "title": "Biomedical Question Answering via Multi-Level Summarization on a Local Knowledge Graph",
      "title_zh": "基于局部知识图谱多层摘要的生物医学问答\n",
      "authors": [
        "Lingxiao Guan",
        "Yuanhao Huang",
        "Jie Liu"
      ],
      "abstract": "In Question Answering (QA), Retrieval Augmented Generation (RAG) has\nrevolutionized performance in various domains. However, how to effectively\ncapture multi-document relationships, particularly critical for biomedical\ntasks, remains an open question. In this work, we propose a novel method that\nutilizes propositional claims to construct a local knowledge graph from\nretrieved documents. Summaries are then derived via layerwise summarization\nfrom the knowledge graph to contextualize a small language model to perform QA.\nWe achieved comparable or superior performance with our method over RAG\nbaselines on several biomedical QA benchmarks. We also evaluated each\nindividual step of our methodology over a targeted set of metrics,\ndemonstrating its effectiveness.",
      "tldr_zh": "该论文提出了一种新的生物医学问答方法，该方法利用命题声明从检索到的文档构建局部知识图谱。通过知识图谱的分层摘要提取关键信息，并以此为上下文，利用小型语言模型进行问答。实验结果表明，在多个生物医学问答基准测试中，该方法与 RAG 基线相比，取得了相当甚至更优越的性能。此外，对该方法的每个步骤都进行了评估，证明了其有效性。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01309v1",
      "published_date": "2025-04-02 02:40:19 UTC",
      "updated_date": "2025-04-02 02:40:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:18:49.068692"
    },
    {
      "arxiv_id": "2504.01281v2",
      "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding",
      "title_zh": "通过 KV 缓存和解码，利用策略优化、动态检索增强生成来扩展测试时推理\n",
      "authors": [
        "Sakhinana Sagar Srinivas",
        "Venkataramana Runkana"
      ],
      "abstract": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications.",
      "tldr_zh": "该论文提出了一种增强检索增强生成(RAG)系统的框架，通过动态检索策略和强化学习微调来提升大规模语言模型在知识密集型任务上的表现。该框架包含两个核心技术：Policy-Optimized Retrieval-Augmented Generation (PORAG) 优化检索信息的使用，以及 Adaptive Token-Layer Attention Scoring (ATLAS) 动态决定检索时机和内容。此外，论文还提出了CRITIC方法，通过token重要性选择性地压缩key-value缓存，缓解长上下文应用中的内存瓶颈。该框架无需额外训练，即可提升RAG设置下的输出准确率，并在benchmark数据集上表现出降低幻觉、增强领域特定推理以及显著的效率和可扩展性提升。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01281v2",
      "published_date": "2025-04-02 01:16:10 UTC",
      "updated_date": "2025-04-03 01:23:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:19:01.368065"
    },
    {
      "arxiv_id": "2504.01278v1",
      "title": "Strategize Globally, Adapt Locally: A Multi-Turn Red Teaming Agent with Dual-Level Learning",
      "title_zh": "全局策略，局部调整：一种具有双层学习能力的多轮红队对抗智能体\n",
      "authors": [
        "Si Chen",
        "Xiao Yu",
        "Ninareh Mehrabi",
        "Rahul Gupta",
        "Zhou Yu",
        "Ruoxi Jia"
      ],
      "abstract": "The exploitation of large language models (LLMs) for malicious purposes poses\nsignificant security risks as these models become more powerful and widespread.\nWhile most existing red-teaming frameworks focus on single-turn attacks,\nreal-world adversaries typically operate in multi-turn scenarios, iteratively\nprobing for vulnerabilities and adapting their prompts based on threat model\nresponses. In this paper, we propose \\AlgName, a novel multi-turn red-teaming\nagent that emulates sophisticated human attackers through complementary\nlearning dimensions: global tactic-wise learning that accumulates knowledge\nover time and generalizes to new attack goals, and local prompt-wise learning\nthat refines implementations for specific goals when initial attempts fail.\nUnlike previous multi-turn approaches that rely on fixed strategy sets,\n\\AlgName enables the agent to identify new jailbreak tactics, develop a\ngoal-based tactic selection framework, and refine prompt formulations for\nselected tactics. Empirical evaluations on JailbreakBench demonstrate our\nframework's superior performance, achieving over 90\\% attack success rates\nagainst GPT-3.5-Turbo and Llama-3.1-70B within 5 conversation turns,\noutperforming state-of-the-art baselines. These results highlight the\neffectiveness of dynamic learning in identifying and exploiting model\nvulnerabilities in realistic multi-turn scenarios.",
      "tldr_zh": "该论文提出了一个名为\\AlgName的多轮红队对抗Agent，旨在模拟复杂的人类攻击者，用于发现和利用大型语言模型(LLMs)的漏洞。 \\AlgName采用双层学习策略：全局策略学习用于积累知识并泛化到新的攻击目标，局部提示学习则在初始尝试失败时改进特定目标的实现。与依赖固定策略集的方法不同，\\AlgName能够识别新的越狱策略，开发基于目标的策略选择框架，并改进所选策略的提示公式。在JailbreakBench上的实验表明，\\AlgName在5轮对话内对GPT-3.5-Turbo和Llama-3.1-70B实现了超过90%的攻击成功率，优于现有最佳基线，突显了动态学习在识别和利用多轮场景中模型漏洞的有效性。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01278v1",
      "published_date": "2025-04-02 01:06:19 UTC",
      "updated_date": "2025-04-02 01:06:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-04T02:19:13.550843"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 81,
  "processed_papers_count": 81,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-04-04T02:20:34.010524"
}
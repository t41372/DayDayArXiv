{
  "date": "2025-08-03",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-08-03 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\nğŸ‘‹ **ä¸€å¥è¯æ€»ç»“ï¼š**\nä»Šå¤©çš„è®ºæ–‡è´¨é‡å¾ˆé«˜ä¸”è¯é¢˜â€œæ¥åœ°æ°”â€ã€‚æˆ‘ä»¬ç»ˆäºæ‰¾åˆ°äº† LLM ä¸ºä»€ä¹ˆæ€»çˆ±è¯´ \"delve\" å’Œ \"intricate\" çš„åŸå› ï¼ˆå±…ç„¶æ˜¯ RLHF çš„é”…ï¼‰ï¼›åŒ»ç–—é¢†åŸŸè¿æ¥äº†èƒ½å¤Ÿæ›¿ä»£ä¸“å®¶è¿›è¡Œä¸´åºŠç¬”è®°ç‰¹å¾æå–çš„ Multi-Agent ç³»ç»Ÿï¼›åŒæ—¶ï¼Œä¸€ä¸ªæ–°çš„ PDF æ”»å‡»æ‰‹æ®µè®©æ‰€æœ‰çš„ AI æ–‡æœ¬æ£€æµ‹å™¨ç¬é—´å¤±æ•ˆã€‚\n\n---\n\n### ğŸš€ ç„¦ç‚¹è¯é¢˜ï¼šåŒ»ç–— AI ä¸ Agent çš„è¿›åŒ–\n\n**1. [åŒ»ç–—] ä¸´åºŠç¬”è®°ç‰¹å¾ç”Ÿæˆçš„è§„æ¨¡åŒ–ï¼šSNOW ç³»ç»Ÿ**\n**Scaling Clinician-Grade Feature Generation from Clinical Notes with Multi-Agent Language Models**\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** è¿™æ˜¯ä¸€ä¸ªä»¤äººå°è±¡æ·±åˆ»çš„ç³»ç»Ÿå·¥ç¨‹å·¥ä½œã€‚ä½œè€…æå‡ºäº† **SNOW**ï¼ˆScalable Note-to-Outcome Workflowï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ï¼ˆMulti-Agentï¼‰ç³»ç»Ÿï¼Œæ—¨åœ¨ä»éç»“æ„åŒ–çš„ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰ä¸­æå–ç»“æ„åŒ–ç‰¹å¾ã€‚\n*   **å‘ç°ä¸æ–¹æ³•ï¼š** ä¼ ç»Ÿçš„ä¸´åºŠç‰¹å¾æå–ä¾èµ–ä¸“å®¶æ‰‹å·¥é˜…è¯»ï¼Œè´¹æ—¶è´¹åŠ›ã€‚SNOW é€šè¿‡æ¨¡ä»¿ä¸´åºŠä¸“å®¶çš„è¿­ä»£æ¨ç†å’ŒéªŒè¯å·¥ä½œæµï¼Œåœ¨é¢„æµ‹å‰åˆ—è…ºç™Œ 5 å¹´å¤å‘ç‡çš„ä»»åŠ¡ä¸Šï¼Œè¾¾åˆ°äº†ä¸äººå·¥ä¸“å®¶ç›¸å½“çš„ AUC-ROC (0.767 vs 0.762)ã€‚\n*   **Implicationï¼š** å®ƒçš„æ•ˆç‡æƒŠäººï¼Œå°†ä¸“å®¶çš„äººåŠ›æŠ•å…¥å‡å°‘äº†çº¦ **48å€**ã€‚ä¸”åœ¨æœªå¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œç›´æ¥è¿ç§»åˆ°å¿ƒåŠ›è¡°ç«­é˜Ÿåˆ—ä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚è¿™æ˜¯ LLM Agent åœ¨åŒ»ç–—è½åœ°çš„é‡è¦ä¸€æ­¥ã€‚\n\n**38. [Agent åŸºå‡†] LiveMCPBenchï¼šAgent èƒ½åœ¨ MCP å·¥å…·çš„æµ·æ´‹ä¸­å¯¼èˆªå—ï¼Ÿ**\n**LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?**\n*   **èƒŒæ™¯ï¼š** Model Context Protocol (MCP) å‘å±•è¿…çŒ›ï¼Œå·²æœ‰è¶…è¿‡ 10,000 ä¸ªæœåŠ¡å™¨ã€‚\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** ä½œè€…æ¨å‡ºäº† **LiveMCPBench**ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºäºçœŸå® MCP ç”Ÿæ€çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å« 95 ä¸ªçœŸå®ä»»åŠ¡ã€70 ä¸ªæœåŠ¡å™¨å’Œ 527 ä¸ªå·¥å…·ã€‚\n*   **å‘ç°ï¼š** å³ä½¿æ˜¯ç›®å‰æœ€å¼ºçš„ Claude-Sonnet-4 ä¹Ÿåªæœ‰ 78.95% çš„æˆåŠŸç‡ï¼Œè®¸å¤šæ¨¡å‹åœ¨å¤æ‚çš„å·¥å…·é€‰æ‹©å’ŒåŠ¨æ€è§„åˆ’ä¸­è¡¨ç°ä¸ä½³ã€‚è¿™ä¸ºæœªæ¥ Agent æ¥å…¥æµ·é‡çœŸå® API æä¾›äº†é‡è¦çš„æµ‹è¯•åœºã€‚\n\n---\n\n### ğŸ§ LLM è¡Œä¸ºå­¦ä¸å¯¹é½ï¼šä¸ºä»€ä¹ˆ AI è¯´è¯åƒ AIï¼Ÿ\n\n**7. [è¯­è¨€é£æ ¼] å°‘å³æ˜¯å¤šï¼šLLM ä¸­çš„è¯æ±‡è¿‡åº¦ä½¿ç”¨ä¸äººç±»åé¦ˆå­¦ä¹ **\n**Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback**\n*   **æ ¸å¿ƒè¯é¢˜ï¼š** ä¸ºä»€ä¹ˆ LLM æ€»æ˜¯å–œæ¬¢ç”¨ \"delve\"ã€\"intricate\" è¿™ç§è¯ï¼Ÿ\n*   **æ ¸å¿ƒå‘ç°ï¼š** ç ”ç©¶è¯å®ï¼Œè¿™ç§è¯æ±‡çš„è¿‡åº¦ä½¿ç”¨ä¸»è¦æºäº **LHF (Learning from Human Feedback)**ï¼ŒåŒ…æ‹¬ RLHF å’Œ DPOã€‚\n*   **å®éªŒï¼š** ä½œè€…é€šè¿‡å®éªŒå¤ç°äº† LHF è¿‡ç¨‹ï¼Œå‘ç°æ ‡æ³¨è€…ç³»ç»Ÿæ€§åœ°åå¥½åŒ…å«è¿™äº›ç‰¹å®šè¯æ±‡çš„æ–‡æœ¬ã€‚è¿™æ­ç¤ºäº†ä¸€ç§â€œè¯æ±‡å±‚é¢çš„ä¸å¯¹é½â€ï¼šLHF æ ‡æ³¨å·¥äººçš„åå¥½ä¸æœ€ç»ˆç”¨æˆ·ï¼ˆè§‰å¾—è¿™äº›è¯å¾ˆçƒ¦ï¼‰çš„åå¥½å¹¶ä¸ä¸€è‡´ã€‚\n\n**90. [è®¤çŸ¥åå·®] LLM ä¸­çš„æ‰¿è¯ºå‡çº§ç°è±¡**\n**Getting out of the Big-Muddy: Escalation of Commitment in LLMs**\n*   **æ ¸å¿ƒå‘ç°ï¼š** LLM æ˜¯å¦ä¼šåƒäººç±»ä¸€æ ·é™·å…¥â€œæ²‰æ²¡æˆæœ¬è°¬è¯¯â€ï¼ˆescalation of commitmentï¼‰ï¼Ÿç ”ç©¶å‘ç°ï¼Œåœ¨ä¸ªä½“å†³ç­–æ—¶ï¼ŒLLM è¡¨ç°å¾—å¾ˆç†æ€§ï¼›ä½†åœ¨ **å¤šæ™ºèƒ½ä½“å•†è®®ï¼ˆMulti-agent deliberationï¼‰** æˆ–é¢ä¸´ç»„ç»‡å‹åŠ›æ—¶ï¼ŒLLM ä¼šè¡¨ç°å‡ºæé«˜çš„æ‰¿è¯ºå‡çº§å€¾å‘ï¼ˆå³æ˜çŸ¥é¡¹ç›®è¦é»„ï¼Œå› ä¸ºå‰æœŸæŠ•å…¥äº†è¿˜è¦ç»§ç»­æŠ•é’±ï¼‰ã€‚è¿™å¯¹å°† LLM ç”¨äºé«˜é£é™©å†³ç­–æ•²å“äº†è­¦é’Ÿã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ä¸æ”»é˜²\n\n**15. [æ”»å‡»] PDFuzzï¼šé’ˆå¯¹ AI æ–‡æœ¬æ£€æµ‹çš„ PDF æ”»å‡»**\n**Complete Evasion, Zero Modification: PDF Attacks on AI Text Detection**\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** æå‡ºäº†ä¸€ç§åä¸º **PDFuzz** çš„æ”»å‡»æ–¹æ³•ï¼Œå®Œå…¨å‡»ç©¿äº†ç°æœ‰çš„ AI ç”Ÿæˆæ–‡æœ¬æ£€æµ‹å™¨ã€‚\n*   **æ–¹æ³•ï¼š** åˆ©ç”¨ PDF æ–‡æ¡£çš„â€œè§†è§‰å¸ƒå±€â€ä¸â€œå­—ç¬¦æå–é¡ºåºâ€ä¹‹é—´çš„å·®å¼‚ã€‚æ”»å‡»è€…æ‰“ä¹±äº†å­—ç¬¦çš„æå–åºåˆ—ï¼ˆScramble extraction sequencesï¼‰ï¼Œä½†åœ¨è§†è§‰ä¸Šä¿æŒæ–‡æœ¬å®Œç¾ä¸å˜ã€‚\n*   **æ•ˆæœï¼š** æ£€æµ‹å™¨çš„å‡†ç¡®ç‡ä» 93.6% ç›´æ¥æ‰åˆ°äº†éšæœºçŒœæµ‹æ°´å¹³ï¼ˆ~50%ï¼‰ï¼Œè€Œäººç±»é˜…è¯»å®Œå…¨ä¸å—å½±å“ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸å®ç”¨çš„ç‰©ç†/æ ¼å¼å±‚é¢çš„æ”»å‡»ã€‚\n\n**6. [åé—¨é˜²å¾¡] ä¸»åŠ¨è§£è€¦è§¦å‘å™¨ä¸å¯¹è±¡ï¼šDBOM**\n**Proactive Disentangled Modeling of Trigger-Object Pairings for Backdoor Defense**\n*   **æ–¹æ³•ï¼š** æå‡º **DBOM** æ¡†æ¶ï¼Œåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å°†å›¾åƒä¸­çš„â€œè§¦å‘å™¨ï¼ˆTriggerï¼‰â€å’Œâ€œå¯¹è±¡ï¼ˆObjectï¼‰â€åœ¨ç‰¹å¾ç©ºé—´è¿›è¡Œè§£è€¦ã€‚\n*   **è´¡çŒ®ï¼š** é€šè¿‡è¿™ç§è§£è€¦ï¼Œæ¨¡å‹å¯ä»¥åœ¨è®­ç»ƒå‰å°±è¯†åˆ«å‡ºä»æœªè§è¿‡çš„è§¦å‘å™¨-å¯¹è±¡ç»„åˆï¼Œä»è€Œåœ¨æ•°æ®é›†å±‚é¢æ¸…æ´—æ‰åé—¨æ ·æœ¬ï¼Œæå‡å®‰å…¨æ€§ã€‚\n\n---\n\n### ğŸ‘ï¸ è§†è§‰ä¸å¤šæ¨¡æ€\n\n**93. [ç«¯ä¾§æ¨¡å‹] MagicVL-2Bï¼šé€šè¿‡è¯¾ç¨‹å­¦ä¹ èµ‹èƒ½ç§»åŠ¨ç«¯ VLM**\n**MagicVL-2B: Empowering Vision-Language Models on Mobile Devices with Lightweight Visual Encoders via Curriculum Learning**\n*   **è¶‹åŠ¿ï¼š** è¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹æ——èˆ°æ™ºèƒ½æ‰‹æœºä¼˜åŒ–çš„ 2B å‚æ•°é‡çº§ VLMã€‚\n*   **æ–¹æ³•ï¼š** ä½¿ç”¨äº†å°äº 100M å‚æ•°çš„è½»é‡çº§è§†è§‰ç¼–ç å™¨ï¼Œå¹¶å¼•å…¥äº† **å¤šæ¨¡æ€è¯¾ç¨‹å­¦ä¹ ï¼ˆCurriculum Learningï¼‰** ç­–ç•¥ï¼Œé€æ­¥å¢åŠ ä»»åŠ¡éš¾åº¦ã€‚\n*   **ç»“æœï¼š** æ€§èƒ½åŒ¹æ•Œ SOTAï¼Œä½†ç«¯ä¾§åŠŸè€—é™ä½äº† 41.1%ï¼Œé€‚åˆç§»åŠ¨ç«¯éƒ¨ç½²ã€‚\n\n**63. [å¹»è§‰] è§†è§‰åµŒå…¥æŒ‡ä»¤ä¼šæ”¹å˜ VLM çš„å¹»è§‰å—ï¼Ÿ**\n**Cure or Poison? Embedding Instructions Visually Alters Hallucination in Vision-Language Models**\n*   **æœ‰è¶£å‘ç°ï¼š** ä½œè€…å°è¯•å°† Prompt ç›´æ¥å†™åœ¨å›¾ç‰‡é‡Œï¼ˆPrompt-in-Imageï¼‰ã€‚\n*   **ç»“æœï¼š** è¿™ç§æ–¹æ³•å¯¹ **Qwen2.5-VL** æœ‰æ•ˆï¼Œèƒ½å‡å°‘å¹»è§‰ï¼›ä½†å¯¹ **LLaVA-1.5** å’Œ **InstructBLIP** å´æ˜¯â€œæ¯’è¯â€ï¼Œä¼šå¯¼è‡´æ€§èƒ½é›ªå´©ã€‚åŸå› æ˜¯åè€…çš„ CLIP ç¼–ç å™¨ä¼šè¿‡åº¦å…³æ³¨æ–‡å­—åŒºåŸŸï¼Œç ´åäº†è§†è§‰ç†è§£ã€‚\n\n---\n\n### ğŸ§ª å…¶ä»–å€¼å¾—å…³æ³¨çš„ç ”ç©¶\n\n*   **[ç§‘å­¦è®¡ç®—] QCBench: Evaluating Large Language Models on Domain-Specific Quantitative Chemistry (Paper 65)**\n    æå‡ºäº†ä¸€ä¸ªå®šé‡åŒ–å­¦åŸºå‡†æµ‹è¯•ã€‚ç»“æœä¸å‡ºæ‰€æ–™ï¼ŒLLM åœ¨å¤„ç†ä¸¥è°¨çš„ã€åˆ†æ­¥éª¤çš„åŒ–å­¦è®¡ç®—æ—¶è¡¨ç°å¾ˆå·®ï¼Œéšç€ä»»åŠ¡å¤æ‚åº¦å¢åŠ ï¼Œæ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚\n\n*   **[æŒç»­å­¦ä¹ ] Revisiting Replay and Gradient Alignment for Continual Pre-Training (Paper 13)**\n    åœ¨ 100B token çš„è§„æ¨¡ä¸ŠéªŒè¯äº† **ç»éªŒå›æ”¾ï¼ˆExperience Replayï¼‰** å’Œ **æ¢¯åº¦å¯¹é½ï¼ˆGradient Alignmentï¼‰** åœ¨ LLM æŒç»­é¢„è®­ç»ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚ç»“è®ºæ˜¯ï¼šä¸å…¶è¿‡åº¦å›æ”¾æ—§æ•°æ®ï¼Œä¸å¦‚æŠŠç®—åŠ›èŠ±åœ¨æŠŠæ¨¡å‹åšå¤§ä¸Šæ›´åˆ’ç®—ï¼Œä½†ä¹Ÿéœ€è¦å°‘é‡çš„å›æ”¾æ¥é˜²æ­¢é—å¿˜ã€‚\n\n*   **[æ—¶é—´åºåˆ—] CTBench: Cryptocurrency Time Series Generation Benchmark (Paper 23)**\n    ç¬¬ä¸€ä¸ªä¸“é—¨é’ˆå¯¹åŠ å¯†è´§å¸æ—¶é—´åºåˆ—ç”Ÿæˆçš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†é¢„æµ‹å‡†ç¡®æ€§ã€ç»Ÿè®¡å¥—åˆ©ï¼ˆèµšé’±èƒ½åŠ›ï¼‰ç­‰æŒ‡æ ‡ã€‚\n\n*   **[å“²æ€] Beyond the Wavefunction: Qualia Abstraction Language Mechanics (Paper 34)**\n    ä¸€ç¯‡ 65 é¡µçš„é•¿æ–‡ï¼Œå°è¯•ç”¨â€œæ„Ÿå—è´¨æŠ½è±¡è¯­è¨€ï¼ˆQALï¼‰â€é‡æ„é‡å­åŠ›å­¦ï¼Œå°†è§‚å¯Ÿè€…æ‚–è®ºå½’ç»“ä¸ºè¯­è¨€å­¦çš„ç¼ºå¤±è€Œéæœ¬ä½“è®ºçš„ç¼ºå¤±ã€‚é€‚åˆå–œæ¬¢å“²å­¦å’Œé‡å­ç‰©ç†çš„è¯»è€…æ·±è¯»ã€‚\n\nå¸Œæœ›ä»Šå¤©çš„å¿«æŠ¥å¯¹ä½ çš„ç ”ç©¶é€šè¿‡æœ‰æ‰€å¯å‘ï¼æˆ‘ä»¬æ˜å¤©è§ã€‚",
  "papers": [
    {
      "arxiv_id": "2508.01956v2",
      "title": "Scaling Clinician-Grade Feature Generation from Clinical Notes with Multi-Agent Language Models",
      "title_zh": "åŸºäºå¤šæ™ºèƒ½ä½“è¯­è¨€æ¨¡å‹çš„ä¸´åºŠè®°å½•è§„æ¨¡åŒ–ä¸“å®¶çº§ç‰¹å¾ç”Ÿæˆ",
      "authors": [
        "Jiayi Wang",
        "Jacqueline Jil Vallon",
        "Nikhil V. Kotha",
        "Neil Panjwani",
        "Xi Ling",
        "Margaret Redfield",
        "Sushmita Vij",
        "Sandy Srinivas",
        "John Leppert",
        "Mark K. Buyyounouski",
        "Mohsen Bayati"
      ],
      "abstract": "Developing accurate clinical prediction models is often bottlenecked by the difficulty of deriving meaningful structured features from unstructured EHR notes, a process that traditionally requires manual, unscalable clinical abstraction. In this study, we first established a rigorous patient-level Clinician Feature Generation (CFG) protocol, in which domain experts manually reviewed notes to define and extract nuanced features for a cohort of 147 patients with prostate cancer. As a high-fidelity ground truth, this labor-intensive process provided the blueprint for SNOW (Scalable Note-to-Outcome Workflow), a transparent multi-agent large language model (LLM) system designed to autonomously mimic the iterative reasoning and validation workflow of clinical experts. On 5-year cancer recurrence prediction, SNOW (AUC-ROC 0.767) achieved performance comparable to manual CFG (0.762) and outperformed structured baselines, clinician-guided LLM extraction, and six representational feature generation (RFG) approaches. Once configured, SNOW produced the full patient-level feature table in 12 hours with 5 hours of clinician oversight, reducing human expert effort by approximately 48-fold versus manual CFG. To test scalability where manual CFG is infeasible, we deployed SNOW on an external heart failure with preserved ejection fraction (HFpEF) cohort from MIMIC-IV (n=2,084); without task-specific tuning, SNOW generated prognostic features that outperformed baseline and RFG methods for 30-day (SNOW: 0.851) and 1-year (SNOW: 0.763) mortality prediction. These results demonstrate that a modular LLM agent-based system can scale expert-level feature generation from clinical notes, while enabling interpretable use of unstructured EHR text in outcome prediction and preserving generalizability across a variety of settings and conditions.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº†SNOW (Scalable Note-to-Outcome Workflow)ï¼Œè¿™æ˜¯ä¸€ä¸ªé€æ˜çš„å¤šæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹(LLM)ç³»ç»Ÿï¼Œæ—¨åœ¨è‡ªåŠ¨æ¨¡æ‹Ÿä¸´åºŠä¸“å®¶ä»éç»“æ„åŒ–ç”µå­å¥åº·è®°å½•(EHR)æ–‡æœ¬ä¸­æå–ç‰¹å¾çš„æ¨ç†ä¸éªŒè¯æµç¨‹ã€‚åœ¨é’ˆå¯¹å‰åˆ—è…ºç™Œæ‚£è€…çš„5å¹´å¤å‘é¢„æµ‹å®éªŒä¸­ï¼ŒSNOW å–å¾—äº†ä¸äººå·¥ä¸´åºŠç‰¹å¾ç”Ÿæˆ(Clinician Feature Generation, CFG)ç›¸å½“çš„ AUC-ROC è¡¨ç°ï¼ŒåŒæ—¶å°†ä¸“å®¶çš„äººåŠ›æŠ•å…¥é™ä½äº†çº¦48å€ã€‚ç ”ç©¶è¿›ä¸€æ­¥åœ¨ MIMIC-IV çš„å°„è¡€åˆ†æ•°ä¿ç•™å‹å¿ƒåŠ›è¡°ç«­(HFpEF)é˜Ÿåˆ—ä¸ŠéªŒè¯äº†å…¶æ‰©å±•æ€§ï¼Œç»“æœæ˜¾ç¤º SNOW åœ¨æ— éœ€é’ˆå¯¹æ€§å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œå…¶ç”Ÿæˆçš„é¢„åç‰¹å¾åœ¨30å¤©å’Œ1å¹´æ­»äº¡ç‡é¢„æµ‹ä¸­å‡ä¼˜äºåŸºçº¿æ¨¡å‹å’Œè¡¨å¾ç‰¹å¾ç”Ÿæˆ(RFG)æ–¹æ³•ã€‚è¯¥ç ”ç©¶è¯æ˜äº†æ¨¡å—åŒ–çš„ LLM æ™ºèƒ½ä½“ç³»ç»Ÿèƒ½å¤Ÿå®ç°ä¸“å®¶çº§ä¸´åºŠç‰¹å¾ç”Ÿæˆçš„è§„æ¨¡åŒ–åº”ç”¨ï¼Œå¹¶èƒ½ç¡®ä¿éç»“æ„åŒ–æ–‡æœ¬åœ¨ä¸´åºŠç»“æœé¢„æµ‹ä¸­çš„å¯è§£é‡Šæ€§ä¸è·¨åœºæ™¯æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01956v2",
      "published_date": "2025-08-03 23:45:18 UTC",
      "updated_date": "2025-12-28 17:34:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:28:19.885247+00:00"
    },
    {
      "arxiv_id": "2508.01951v1",
      "title": "Flow-Aware GNN for Transmission Network Reconfiguration via Substation Breaker Optimization",
      "title_zh": "åŸºäºå˜ç”µç«™æ–­è·¯å™¨ä¼˜åŒ–çš„è¾“ç”µç½‘é‡æ„æµé‡æ„ŸçŸ¥å›¾ç¥ç»ç½‘ç»œ",
      "authors": [
        "Dekang Meng",
        "Rabab Haider",
        "Pascal van Hentenryck"
      ],
      "abstract": "This paper introduces OptiGridML, a machine learning framework for discrete topology optimization in power grids. The task involves selecting substation breaker configurations that maximize cross-region power exports, a problem typically formulated as a mixed-integer program (MIP) that is NP-hard and computationally intractable for large networks. OptiGridML replaces repeated MIP solves with a two-stage neural architecture: a line-graph neural network (LGNN) that approximates DC power flows for a given network topology, and a heterogeneous GNN (HeteroGNN) that predicts breaker states under structural and physical constraints. A physics-informed consistency loss connects these components by enforcing Kirchhoff's law on predicted flows. Experiments on synthetic networks with up to 1,000 breakers show that OptiGridML achieves power export improvements of up to 18% over baseline topologies, while reducing inference time from hours to milliseconds. These results demonstrate the potential of structured, flow-aware GNNs for accelerating combinatorial optimization in physical networked systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† OptiGridMLï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç”µåŠ›ç½‘ç»œç¦»æ•£æ‹“æ‰‘ä¼˜åŒ–çš„æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ··åˆæ•´æ•°è§„åˆ’ (mixed-integer program, MIP) åœ¨å¤„ç†å¤§è§„æ¨¡ç½‘ç»œä¸­æ–­è·¯å™¨é…ç½®ä¼˜åŒ–æ—¶é¢ä¸´çš„è®¡ç®—ç“¶é¢ˆã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†ä¸¤é˜¶æ®µç¥ç»ç½‘ç»œæ¶æ„ï¼ŒåŒ…å«ä¸€ä¸ªç”¨äºè¿‘ä¼¼ç›´æµæ½®æµ (DC power flows) çš„çº¿å›¾ç¥ç»ç½‘ç»œ (line-graph neural network, LGNN) å’Œä¸€ä¸ªåœ¨ç‰©ç†çº¦æŸä¸‹é¢„æµ‹æ–­è·¯å™¨çŠ¶æ€çš„å¼‚æ„å›¾ç¥ç»ç½‘ç»œ (heterogeneous GNN, HeteroGNN)ã€‚OptiGridML é€šè¿‡å¼•å…¥ç‰©ç†å‘ŠçŸ¥çš„ä¸€è‡´æ€§æŸå¤± (physics-informed consistency loss) ç¡®ä¿é¢„æµ‹ç»“æœæ»¡è¶³åŸºå°”éœå¤«å®šå¾‹ (Kirchhoff's law)ï¼Œå®ç°äº†ç‰©ç†è§„å¾‹ä¸æ¨¡å‹é¢„æµ‹çš„æœ‰æ•ˆç»“åˆã€‚åœ¨åŒ…å«å¤šè¾¾ 1,000 ä¸ªæ–­è·¯å™¨çš„åˆæˆç½‘ç»œå®éªŒä¸­ï¼Œè¯¥æ¨¡å‹å°†è·¨åŒºåŸŸç”µåŠ›å‡ºå£èƒ½åŠ›è¾ƒåŸºå‡†æ‹“æ‰‘æå‡äº† 18%ã€‚æ­¤å¤–ï¼ŒOptiGridML å°†æ¨ç†æ—¶é—´ä»æ•°å°æ—¶ç¼©çŸ­è‡³æ¯«ç§’çº§ï¼Œè¯æ˜äº†æ„ŸçŸ¥æµçš„å›¾ç¥ç»ç½‘ç»œåœ¨åŠ é€Ÿç‰©ç†ç½‘ç»œç³»ç»Ÿç»„åˆä¼˜åŒ–æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01951v1",
      "published_date": "2025-08-03 23:21:37 UTC",
      "updated_date": "2025-08-03 23:21:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:28:09.358074+00:00"
    },
    {
      "arxiv_id": "2508.01947v1",
      "title": "Inferring Reward Machines and Transition Machines from Partially Observable Markov Decision Processes",
      "title_zh": "ä»éƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸­æ¨æ–­å¥–åŠ±æœºä¸è½¬ç§»æœº",
      "authors": [
        "Yuly Wu",
        "Jiamou Liu",
        "Libo Zhang"
      ],
      "abstract": "Partially Observable Markov Decision Processes (POMDPs) are fundamental to many real-world applications. Although reinforcement learning (RL) has shown success in fully observable domains, learning policies from traces in partially observable environments remains challenging due to non-Markovian observations. Inferring an automaton to handle the non-Markovianity is a proven effective approach, but faces two limitations: 1) existing automaton representations focus only on reward-based non-Markovianity, leading to unnatural problem formulations; 2) inference algorithms face enormous computational costs. For the first limitation, we introduce Transition Machines (TMs) to complement existing Reward Machines (RMs). To develop a unified inference algorithm for both automata types, we propose the Dual Behavior Mealy Machine (DBMM) that subsumes both TMs and RMs. We then introduce DB-RPNI, a passive automata learning algorithm that efficiently infers DBMMs while avoiding the costly reductions required by prior work. We further develop optimization techniques and identify sufficient conditions for inferring the minimal correct automata. Experimentally, our inference method achieves speedups of up to three orders of magnitude over SOTA baselines.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶æ¢è®¨äº†å¦‚ä½•ä»éƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(Partially Observable Markov Decision Processes, POMDPs)ä¸­æ¨æ–­å¥–åŠ±æœº(Reward Machines, RMs)å’Œè½¬æ¢æœº(Transition Machines, TMs)ï¼Œä»¥è§£å†³éé©¬å°”å¯å¤«(non-Markovian)è§‚æµ‹å¸¦æ¥çš„ç­–ç•¥å­¦ä¹ æŒ‘æˆ˜ã€‚é’ˆå¯¹ç°æœ‰è‡ªåŠ¨æœºè¡¨ç¤ºä»…å…³æ³¨å¥–åŠ±ç›¸å…³çš„éé©¬å°”å¯å¤«æ€§ä¸”è®¡ç®—æˆæœ¬é«˜æ˜‚çš„å±€é™æ€§ï¼Œä½œè€…æå‡ºäº†èƒ½å¤Ÿç»Ÿä¸€ RMs å’Œ TMs çš„åŒè¡Œä¸ºç±³åˆ©æœº(Dual Behavior Mealy Machine, DBMM)ã€‚ä¸ºäº†é«˜æ•ˆæ¨æ–­è¯¥æ¨¡å‹ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†è¢«åŠ¨è‡ªåŠ¨æœºå­¦ä¹ ç®—æ³• DB-RPNIï¼Œæœ‰æ•ˆé¿å…äº†ä»¥å¾€æ–¹æ³•ä¸­å¤æ‚çš„åŒ–ç®€è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æå‡ºäº†ç›¸åº”çš„ä¼˜åŒ–æŠ€æœ¯ï¼Œå¹¶ç¡®å®šäº†æ¨æ–­æœ€å°æ­£ç¡®è‡ªåŠ¨æœºçš„å……åˆ†æ¡ä»¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨æ–­æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ¯”ç°æœ‰çš„ SOTA åŸºå‡†æå‡äº†é«˜è¾¾ä¸‰ä¸ªæ•°é‡çº§ï¼Œä¸ºåœ¨éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸­éƒ¨ç½²é«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•æä¾›äº†æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 7 figures. Under review as a conference paper. Source code is available at: https://github.com/sousoura/Inferring-Reward-Machines-and-Transition-Machines-from-POMDP.git",
      "pdf_url": "https://arxiv.org/pdf/2508.01947v1",
      "published_date": "2025-08-03 22:53:25 UTC",
      "updated_date": "2025-08-03 22:53:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:28:12.750941+00:00"
    },
    {
      "arxiv_id": "2508.01943v2",
      "title": "ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks",
      "title_zh": "ROVERï¼šåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„å…·èº«ä»»åŠ¡è§†é¢‘é€’å½’æ¨ç†",
      "authors": [
        "Philip Schroeder",
        "Ondrej Biza",
        "Thomas Weng",
        "Hongyin Luo",
        "James Glass"
      ],
      "abstract": "Vision-language models (VLMs) have exhibited impressive capabilities across diverse image understanding tasks, but still struggle in settings that require reasoning over extended sequences of camera frames from a video. This limits their utility in embodied settings, which require reasoning over long frame sequences from a continuous stream of visual input at each moment of a task attempt. To address this limitation, we propose ROVER (Reasoning Over VidEo Recursively), a framework that enables the model to recursively decompose long-horizon video trajectories into segments corresponding to shorter subtasks within the trajectory. In doing so, ROVER facilitates more focused and accurate reasoning over temporally localized frame sequences without losing global context. We evaluate ROVER, implemented using an in-context learning approach, on diverse OpenX Embodiment videos and on a new dataset derived from RoboCasa that consists of 543 videos showing both expert and perturbed non-expert trajectories across 27 robotic manipulation tasks. ROVER outperforms strong baselines across three video reasoning tasks: task progress estimation, frame-level natural language reasoning, and video question answering. We observe that, by reducing the number of frames the model reasons over at each timestep, ROVER mitigates hallucinations, especially during unexpected or non-optimal moments of a trajectory. In addition, by enabling the implementation of a subtask-specific sliding context window, ROVER's time complexity scales linearly with video length, an asymptotic improvement over baselines. Demos, code, and data available at: https://rover-vlm.github.io",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨å…·èº«ä»»åŠ¡(Embodied tasks)ä¸­éš¾ä»¥å¤„ç†é•¿è§†é¢‘åºåˆ—æ¨ç†çš„é—®é¢˜ï¼Œæå‡ºäº†ROVER (Reasoning Over VidEo Recursively)æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡é€’å½’åœ°å°†é•¿æ—¶åŸŸè§†é¢‘è½¨è¿¹åˆ†è§£ä¸ºå¯¹åº”çŸ­å­ä»»åŠ¡çš„ç‰‡æ®µï¼Œä½¿æ¨¡å‹èƒ½åœ¨ä¸ä¸¢å¤±å…¨å±€èƒŒæ™¯çš„å‰æä¸‹ï¼Œå¯¹å±€éƒ¨å¸§åºåˆ—è¿›è¡Œæ›´ç²¾å‡†çš„æ¨ç†ã€‚ç ”ç©¶äººå‘˜åœ¨OpenX Embodimentä»¥åŠåŸºäºRoboCasaçš„æœºå™¨äººæ“çºµæ•°æ®é›†ä¸Šå¯¹ROVERè¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶µç›–äº†ä»»åŠ¡è¿›åº¦ä¼°è®¡(Task progress estimation)ã€å¸§çº§è‡ªç„¶è¯­è¨€æ¨ç†å’Œè§†é¢‘é—®ç­”(Video QA)ç­‰ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒROVERåœ¨æ€§èƒ½ä¸Šä¼˜äºå¼ºåŸºçº¿æ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡è½»æ¨¡å‹å¹»è§‰(Hallucinations)ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†éæœ€ä¼˜è½¨è¿¹æ—¶åˆ»è¡¨ç°ä¼˜å¼‚ã€‚æ­¤å¤–ï¼Œå¾—ç›Šäºå­ä»»åŠ¡ç‰¹å®šçš„æ»‘åŠ¨çª—å£æœºåˆ¶ï¼ŒROVERçš„æ—¶é—´å¤æ‚åº¦éšè§†é¢‘é•¿åº¦çº¿æ€§å¢åŠ ï¼Œæ˜¾è‘—æå‡äº†é•¿è§†é¢‘å¤„ç†çš„æ•ˆç‡ã€‚è¯¥ç ”ç©¶ä¸ºé•¿æ—¶åŸŸå…·èº«æ™ºèƒ½æ¨ç†æä¾›äº†æ›´å…·æ‰©å±•æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01943v2",
      "published_date": "2025-08-03 22:33:43 UTC",
      "updated_date": "2025-11-27 02:58:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:28:19.250196+00:00"
    },
    {
      "arxiv_id": "2508.01941v1",
      "title": "Less is More: AMBER-AFNO -- a New Benchmark for Lightweight 3D Medical Image Segmentation",
      "title_zh": "å°‘å³æ˜¯å¤šï¼šAMBER-AFNO â€”â€” è½»é‡åŒ–ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²æ–°åŸºå‡†",
      "authors": [
        "Andrea Dosi",
        "Semanto Mondal",
        "Rajib Chandra Ghosh",
        "Massimo Brescia",
        "Giuseppe Longo"
      ],
      "abstract": "This work presents the results of a methodological transfer from remote sensing to healthcare, adapting AMBER -- a transformer-based model originally designed for multiband images, such as hyperspectral data -- to the task of 3D medical datacube segmentation. In this study, we use the AMBER architecture with Adaptive Fourier Neural Operators (AFNO) in place of the multi-head self-attention mechanism. While existing models rely on various forms of attention to capture global context, AMBER-AFNO achieves this through frequency-domain mixing, enabling a drastic reduction in model complexity. This design reduces the number of trainable parameters by over 80% compared to UNETR++, while maintaining a FLOPs count comparable to other state-of-the-art architectures. Model performance is evaluated on two benchmark 3D medical datasets -- ACDC and Synapse -- using standard metrics such as Dice Similarity Coefficient (DSC) and Hausdorff Distance (HD), demonstrating that AMBER-AFNO achieves competitive or superior accuracy with significant gains in training efficiency, inference speed, and memory usage.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡ä»é¥æ„Ÿé¢†åŸŸè¿›è¡Œæ–¹æ³•è¿ç§»ï¼Œæå‡ºäº†AMBER-AFNOï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸º3DåŒ»å­¦å›¾åƒåˆ†å‰²è®¾è®¡çš„è½»é‡åŒ–åŸºå‡†æ¨¡å‹ã€‚è¯¥æ¶æ„åˆ©ç”¨è‡ªé€‚åº”å‚…é‡Œå¶ç¥ç»ç®—å­(Adaptive Fourier Neural Operators, AFNO)å–ä»£äº†ä¼ ç»Ÿçš„å¤šå¤´è‡ªæ³¨æ„åŠ›(multi-head self-attention)æœºåˆ¶ï¼Œé€šè¿‡é¢‘åŸŸæ··åˆ(frequency-domain mixing)æ¥æœ‰æ•ˆæ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ã€‚ä¸UNETR++ç›¸æ¯”ï¼ŒAMBER-AFNOåœ¨å‡å°‘è¶…è¿‡80%çš„å¯è®­ç»ƒå‚æ•°çš„åŒæ—¶ï¼Œä»èƒ½ä¿æŒç›¸å½“çš„è®¡ç®—é‡(FLOPs)ã€‚åœ¨ACDCå’ŒSynapseæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨Diceç›¸ä¼¼ç³»æ•°(DSC)å’Œè±ªæ–¯å¤šå¤«è·ç¦»(HD)ç­‰æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†å…·æœ‰ç«äº‰åŠ›çš„ç²¾åº¦ã€‚æ­¤å¤–ï¼ŒAMBER-AFNOåœ¨è®­ç»ƒæ•ˆç‡ã€æ¨ç†é€Ÿåº¦ä»¥åŠå†…å­˜å ç”¨æ–¹é¢å±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºåŒ»ç–—é¢†åŸŸçš„é«˜æ•ˆ3Då½±åƒåˆ†å‰²æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01941v1",
      "published_date": "2025-08-03 22:31:00 UTC",
      "updated_date": "2025-08-03 22:31:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:28:24.632437+00:00"
    },
    {
      "arxiv_id": "2508.01932v1",
      "title": "Proactive Disentangled Modeling of Trigger-Object Pairings for Backdoor Defense",
      "title_zh": "é¢å‘åé—¨é˜²å¾¡çš„è§¦å‘å™¨-å¯¹è±¡é…å¯¹ä¸»åŠ¨è§£è€¦å»ºæ¨¡",
      "authors": [
        "Kyle Stein",
        "Andrew A. Mahyari",
        "Guillermo Francia",
        "Eman El-Sheikh"
      ],
      "abstract": "Deep neural networks (DNNs) and generative AI (GenAI) are increasingly vulnerable to backdoor attacks, where adversaries embed triggers into inputs to cause models to misclassify or misinterpret target labels. Beyond traditional single-trigger scenarios, attackers may inject multiple triggers across various object classes, forming unseen backdoor-object configurations that evade standard detection pipelines. In this paper, we introduce DBOM (Disentangled Backdoor-Object Modeling), a proactive framework that leverages structured disentanglement to identify and neutralize both seen and unseen backdoor threats at the dataset level. Specifically, DBOM factorizes input image representations by modeling triggers and objects as independent primitives in the embedding space through the use of Vision-Language Models (VLMs). By leveraging the frozen, pre-trained encoders of VLMs, our approach decomposes the latent representations into distinct components through a learnable visual prompt repository and prompt prefix tuning, ensuring that the relationships between triggers and objects are explicitly captured. To separate trigger and object representations in the visual prompt repository, we introduce the trigger-object separation and diversity losses that aids in disentangling trigger and object visual features. Next, by aligning image features with feature decomposition and fusion, as well as learned contextual prompt tokens in a shared multimodal space, DBOM enables zero-shot generalization to novel trigger-object pairings that were unseen during training, thereby offering deeper insights into adversarial attack patterns. Experimental results on CIFAR-10 and GTSRB demonstrate that DBOM robustly detects poisoned images prior to downstream training, significantly enhancing the security of DNN training pipelines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦ç¥ç»ç½‘ç»œ(DNNs)å’Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½(GenAI)åœ¨åé—¨æ”»å‡»(backdoor attacks)ä¸‹ï¼Œå°¤å…¶æ˜¯é¢å¯¹å¤æ‚ä¸”æœªè§çš„è§¦å‘å™¨-å¯¹è±¡é…ç½®æ—¶çš„è„†å¼±æ€§ï¼Œæå‡ºäº†åä¸ºDBOM (Disentangled Backdoor-Object Modeling) çš„ä¸»åŠ¨é˜²å¾¡æ¡†æ¶ã€‚DBOMåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)çš„é¢„è®­ç»ƒç¼–ç å™¨ï¼Œé€šè¿‡å¯å­¦ä¹ çš„è§†è§‰æç¤ºè¯åº“(visual prompt repository)å’Œæç¤ºå‰ç¼€å¾®è°ƒ(prompt prefix tuning)ï¼Œåœ¨åµŒå…¥ç©ºé—´ä¸­å°†å›¾åƒè¡¨ç¤ºè§£è€¦ä¸ºç‹¬ç«‹çš„è§¦å‘å™¨(triggers)å’Œå¯¹è±¡(objects)åŸºå…ƒã€‚ä¸ºäº†ç¡®ä¿è§£è€¦çš„å‡†ç¡®æ€§ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†è§¦å‘å™¨-å¯¹è±¡åˆ†ç¦»ä¸å¤šæ ·æ€§æŸå¤±å‡½æ•°ï¼Œå¹¶åœ¨å¤šæ¨¡æ€å…±äº«ç©ºé—´ä¸­å®ç°ç‰¹å¾çš„åˆ†è§£ä¸èåˆã€‚è¿™ç§æœºåˆ¶ä½¿DBOMå…·å¤‡äº†å¯¹è®­ç»ƒé˜¶æ®µæœªè§çš„å…¨æ–°è§¦å‘å™¨-å¯¹è±¡é…å¯¹è¿›è¡Œé›¶æ ·æœ¬(zero-shot)æ³›åŒ–æ£€æµ‹çš„èƒ½åŠ›ï¼Œä»è€Œèƒ½æ·±å…¥è¯†åˆ«å¯¹æŠ—æ€§æ”»å‡»æ¨¡å¼ã€‚åœ¨CIFAR-10å’ŒGTSRBæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDBOMèƒ½å¤Ÿåœ¨æ¨¡å‹è®­ç»ƒå‰ç¨³å¥åœ°æ£€æµ‹å‡ºä¸­æ¯’å›¾åƒï¼Œæ˜¾è‘—å¢å¼ºäº†æ·±åº¦å­¦ä¹ æµæ°´çº¿çš„å®‰å…¨æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01932v1",
      "published_date": "2025-08-03 21:58:15 UTC",
      "updated_date": "2025-08-03 21:58:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:28:35.745068+00:00"
    },
    {
      "arxiv_id": "2508.01930v1",
      "title": "Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è¯æ±‡è¿‡åº¦ä½¿ç”¨ä¸å¯¹é½ï¼šäººç±»åé¦ˆå­¦ä¹ çš„å½±å“",
      "authors": [
        "Tom S. Juzek",
        "Zina B. Ward"
      ],
      "abstract": "Large Language Models (LLMs) are known to overuse certain terms like \"delve\" and \"intricate.\" The exact reasons for these lexical choices, however, have been unclear. Using Meta's Llama model, this study investigates the contribution of Learning from Human Feedback (LHF), under which we subsume Reinforcement Learning from Human Feedback and Direct Preference Optimization. We present a straightforward procedure for detecting the lexical preferences of LLMs that are potentially LHF-induced. Next, we more conclusively link LHF to lexical overuse by experimentally emulating the LHF procedure and demonstrating that participants systematically prefer text variants that include certain words. This lexical overuse can be seen as a sort of misalignment, though our study highlights the potential divergence between the lexical expectations of different populations -- namely LHF workers versus LLM users. Our work contributes to the growing body of research on explainable artificial intelligence and emphasizes the importance of both data and procedural transparency in alignment research.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)è¿‡åº¦ä½¿ç”¨è¯¸å¦‚ \"delve\" å’Œ \"intricate\" ç­‰ç‰¹å®šè¯æ±‡çš„ç°è±¡ï¼Œé‡ç‚¹æ¢è®¨äº†ä»äººç±»åé¦ˆä¸­å­¦ä¹ (Learning from Human Feedback, LHF)ï¼ˆåŒ…æ‹¬ RLHF å’Œ DPOï¼‰å¯¹æ¨¡å‹è¯æ±‡é€‰æ‹©çš„å½±å“ã€‚ç ”ç©¶äººå‘˜ä»¥ Meta çš„ Llama æ¨¡å‹ä¸ºåŸºç¡€ï¼Œæå‡ºäº†ä¸€å¥—æ£€æµ‹æ½œåœ¨ç”± LHF è¯±å‘çš„è¯æ±‡åå¥½çš„ç¨‹åºã€‚é€šè¿‡å®éªŒæ¨¡æ‹Ÿ LHF è¿‡ç¨‹ï¼Œç ”ç©¶è¯å®å‚ä¸è€…ä¼šç³»ç»Ÿæ€§åœ°åå¥½åŒ…å«ç‰¹å®šè¯æ±‡çš„æ–‡æœ¬å˜ä½“ï¼Œä»è€Œå°† LHF ä¸è¯æ±‡è¿‡åº¦ä½¿ç”¨ç°è±¡å»ºç«‹äº†æ˜ç¡®è”ç³»ã€‚è¿™ç§è¯æ±‡è¿‡åº¦ä½¿ç”¨è¢«è§†ä¸ºä¸€ç§å¯¹é½å¤±å‡†(misalignment)ï¼Œæ­ç¤ºäº† LHF æ ‡æ³¨äººå‘˜ä¸ LLM ç”¨æˆ·ä¹‹é—´åœ¨è¯æ±‡é¢„æœŸä¸Šçš„æ½œåœ¨å·®å¼‚ã€‚è¯¥é¡¹å·¥ä½œä¸ºå¯è§£é‡Šäººå·¥æ™ºèƒ½(Explainable AI)é¢†åŸŸåšå‡ºäº†è´¡çŒ®ï¼Œå¹¶å¼ºè°ƒäº†åœ¨æ¨¡å‹å¯¹é½ç ”ç©¶ä¸­æ•°æ®å’Œç¨‹åºé€æ˜åº¦çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted for publication in the Proceedings of the 5th Workshop on Bias and Fairness in AI (BIAS 2025) at ECML PKDD",
      "pdf_url": "https://arxiv.org/pdf/2508.01930v1",
      "published_date": "2025-08-03 21:45:37 UTC",
      "updated_date": "2025-08-03 21:45:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:28:34.932412+00:00"
    },
    {
      "arxiv_id": "2508.01928v1",
      "title": "IAUNet: Instance-Aware U-Net",
      "title_zh": "IAUNetï¼šå®ä¾‹æ„ŸçŸ¥ U-Net",
      "authors": [
        "Yaroslav Prytula",
        "Illia Tsiporenko",
        "Ali Zeynalli",
        "Dmytro Fishman"
      ],
      "abstract": "Instance segmentation is critical in biomedical imaging to accurately distinguish individual objects like cells, which often overlap and vary in size. Recent query-based methods, where object queries guide segmentation, have shown strong performance. While U-Net has been a go-to architecture in medical image segmentation, its potential in query-based approaches remains largely unexplored. In this work, we present IAUNet, a novel query-based U-Net architecture. The core design features a full U-Net architecture, enhanced by a novel lightweight convolutional Pixel decoder, making the model more efficient and reducing the number of parameters. Additionally, we propose a Transformer decoder that refines object-specific features across multiple scales. Finally, we introduce the 2025 Revvity Full Cell Segmentation Dataset, a unique resource with detailed annotations of overlapping cell cytoplasm in brightfield images, setting a new benchmark for biomedical instance segmentation. Experiments on multiple public datasets and our own show that IAUNet outperforms most state-of-the-art fully convolutional, transformer-based, and query-based models and cell segmentation-specific models, setting a strong baseline for cell instance segmentation tasks. Code is available at https://github.com/SlavkoPrytula/IAUNet",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†IAUNetï¼Œè¿™æ˜¯ä¸€ç§åˆ›æ–°çš„åŸºäºæŸ¥è¯¢(query-based)çš„U-Netæ¶æ„ï¼Œæ—¨åœ¨è§£å†³ç”Ÿç‰©åŒ»å­¦å›¾åƒä¸­é‡å ä¸”å°ºå¯¸å¤šå˜çš„ç»†èƒå®ä¾‹åˆ†å‰²(Instance segmentation)éš¾é¢˜ã€‚æ¨¡å‹åœ¨å®Œæ•´U-Netæ¶æ„çš„åŸºç¡€ä¸Šï¼Œå¼•å…¥äº†ä¸€ç§æ–°å‹è½»é‡çº§å·ç§¯åƒç´ è§£ç å™¨(Pixel decoder)ï¼Œåœ¨æå‡æ¨¡å‹æ•ˆç‡çš„åŒæ—¶æ˜¾è‘—å‡å°‘äº†å‚æ•°é‡ã€‚æ­¤å¤–ï¼ŒIAUNetåˆ©ç”¨Transformerè§£ç å™¨åœ¨å¤šä¸ªå°ºåº¦ä¸Šè¿›ä¸€æ­¥ä¼˜åŒ–ç‰¹å®šç›®æ ‡çš„ç‰¹å¾æå–ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜æ¨å‡ºäº†2025 Revvity Full Cell Segmentation Datasetï¼Œè¯¥æ•°æ®é›†åŒ…å«æ˜åœºå›¾åƒä¸­é‡å ç»†èƒç»†èƒè´¨çš„è¯¦ç»†æ ‡æ³¨ï¼Œä¸ºç”Ÿç‰©åŒ»å­¦å®ä¾‹åˆ†å‰²è®¾ç«‹äº†æ–°çš„åŸºå‡†ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒIAUNetåœ¨å¤šä¸ªå…¬å…±åŠç§æœ‰æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰çš„å…¨å·ç§¯ã€åŸºäºTransformeråŠä¸“é—¨ç”¨äºç»†èƒåˆ†å‰²çš„SOTAæ¨¡å‹ã€‚è¿™ä¸€å·¥ä½œä¸ºç»†èƒå®ä¾‹åˆ†å‰²ä»»åŠ¡æä¾›äº†å¼ºæœ‰åŠ›çš„åŸºå‡†æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†U-Netåœ¨åŸºäºæŸ¥è¯¢çš„æ¶æ„ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Published in CVPR Workshops (CVMI), 2025. Project page/code/models/dataset: $\\href{https://slavkoprytula.github.io/IAUNet/}{\\text{this https URL}}$",
      "pdf_url": "https://arxiv.org/pdf/2508.01928v1",
      "published_date": "2025-08-03 21:36:20 UTC",
      "updated_date": "2025-08-03 21:36:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:28:41.313949+00:00"
    },
    {
      "arxiv_id": "2508.01918v2",
      "title": "Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language",
      "title_zh": "Quantum-RAG ä¸ PunGPT2ï¼šæ¨è¿›æ—é®æ™®è¯­ä½èµ„æºè¯­è¨€ç”Ÿæˆä¸æ£€ç´¢",
      "authors": [
        "Jaskaranjeet Singh",
        "Rakesh Thakur"
      ],
      "abstract": "Despite rapid advances in large language models (LLMs), low-resource languages remain excluded from NLP, limiting digital access for millions. We present PunGPT2, the first fully open-source Punjabi generative model suite, trained on a 35GB corpus covering literature, religious texts, news, social discourse, etc. PunGPT2 captures Punjabi's syntactic and morphological richness through a tokenizer optimized for Gurmukhi and Shahmukhi scripts. We introduce Pun-RAG, a retrieval-augmented framework integrating PunGPT2 with a FAISS retriever over a curated Punjabi knowledge base, and Pun-Instruct, an instruction-tuned variant using QLoRA for robust zero-shot summarization, translation, and question answering. Our key innovation, Quantum-RAG, fuses sparse, dense, and quantum kernel embeddings for efficient, context-aware retrieval with low memory overhead, marking the first practical quantum-inspired retrieval in a low-resource LLM. Our models outperform multilingual baselines (mBERT, mT5, MuRIL, BLOOM) on FLORES-200, IndicGenBench, and a new PunjabiEval suite. Quantum-RAG yields +7.4 Recall@10 over FAISS and +3.5 BLEU over mT5 on PunjabiEval. We publicly release all training scripts, hyperparameters, evaluation pipelines, the 35GB Punjabi corpus, the PunjabiEval benchmark, and all model weights, establishing new state-of-the-art results for Punjabi language generation and retrieval.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†PunGPT2ï¼Œè¿™æ˜¯é¦–ä¸ªå®Œå…¨å¼€æºçš„æ—é®æ™®è¯­(Punjabi)ç”Ÿæˆå¼æ¨¡å‹ç³»åˆ—ï¼Œæ—¨åœ¨è§£å†³ä½èµ„æºè¯­è¨€åœ¨è‡ªç„¶è¯­è¨€å¤„ç†(NLP)é¢†åŸŸè¢«è¾¹ç¼˜åŒ–çš„é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨æ¶µç›–æ–‡å­¦ã€å®—æ•™å’Œæ–°é—»ç­‰é¢†åŸŸçš„35GBè¯­æ–™åº“å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œå¹¶é’ˆå¯¹Gurmukhiå’ŒShahmukhiè„šæœ¬ä¼˜åŒ–äº†åˆ†è¯å™¨(tokenizer)ï¼Œä»¥æ•æ‰è¯¥è¯­è¨€ä¸°å¯Œçš„è¯­æ³•å’Œå½¢æ€ç‰¹å¾ã€‚ç ”ç©¶å¼•å…¥äº†æ£€ç´¢å¢å¼ºæ¡†æ¶Pun-RAGï¼Œä»¥åŠé€šè¿‡QLoRAè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒçš„å˜ä½“Pun-Instructï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨é›¶æ ·æœ¬æ‘˜è¦ã€ç¿»è¯‘å’Œé—®ç­”ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æ ¸å¿ƒåˆ›æ–°åœ¨äºQuantum-RAGæŠ€æœ¯ï¼Œå®ƒèåˆäº†ç¨€ç–ã€å¯†é›†å’Œé‡å­æ ¸åµŒå…¥(quantum kernel embeddings)ï¼Œåœ¨ä½å†…å­˜å¼€é”€ä¸‹å®ç°äº†é«˜æ•ˆçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ£€ç´¢ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPunGPT2åœ¨FLORES-200å’ŒIndicGenBenchç­‰åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºmBERTã€mT5å’ŒBLOOMç­‰ä¸»æµåŸºçº¿æ¨¡å‹ã€‚å…¶ä¸­Quantum-RAGåœ¨æ£€ç´¢å¬å›ç‡ä¸Šæ¯”FAISSé«˜å‡º7.4 Recall@10ï¼Œå¹¶åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šå–å¾—æ˜¾è‘—æå‡ã€‚è¯¥é¡¹ç›®å…¬å¼€äº†æ‰€æœ‰è®­ç»ƒè„šæœ¬ã€æ¨¡å‹æƒé‡å’Œ35GBè¯­æ–™åº“ï¼Œä¸ºä½èµ„æºè¯­è¨€çš„ç”Ÿæˆä¸æ£€ç´¢æ ‘ç«‹äº†æ–°çš„æŠ€æœ¯åŸºå‡†ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01918v2",
      "published_date": "2025-08-03 21:03:22 UTC",
      "updated_date": "2025-10-03 10:19:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:28:50.769976+00:00"
    },
    {
      "arxiv_id": "2508.01917v1",
      "title": "L3M+P: Lifelong Planning with Large Language Models",
      "title_zh": "L3M+Pï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç»ˆèº«è§„åˆ’",
      "authors": [
        "Krish Agarwal",
        "Yuqian Jiang",
        "Jiaheng Hu",
        "Bo Liu",
        "Peter Stone"
      ],
      "abstract": "By combining classical planning methods with large language models (LLMs), recent research such as LLM+P has enabled agents to plan for general tasks given in natural language. However, scaling these methods to general-purpose service robots remains challenging: (1) classical planning algorithms generally require a detailed and consistent specification of the environment, which is not always readily available; and (2) existing frameworks mainly focus on isolated planning tasks, whereas robots are often meant to serve in long-term continuous deployments, and therefore must maintain a dynamic memory of the environment which can be updated with multi-modal inputs and extracted as planning knowledge for future tasks. To address these two issues, this paper introduces L3M+P (Lifelong LLM+P), a framework that uses an external knowledge graph as a representation of the world state. The graph can be updated from multiple sources of information, including sensory input and natural language interactions with humans. L3M+P enforces rules for the expected format of the absolute world state graph to maintain consistency between graph updates. At planning time, given a natural language description of a task, L3M+P retrieves context from the knowledge graph and generates a problem definition for classical planners. Evaluated on household robot simulators and on a real-world service robot, L3M+P achieves significant improvement over baseline methods both on accurately registering natural language state changes and on correctly generating plans, thanks to the knowledge graph retrieval and verification.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† L3M+P (Lifelong LLM+P) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) ä¸ä¼ ç»Ÿè§„åˆ’æ–¹æ³•ç»“åˆæ—¶é¢ä¸´çš„ç¯å¢ƒè§„èŒƒä¸å®Œæ•´ä»¥åŠé•¿æœŸè¿ç»­éƒ¨ç½²ä¸­åŠ¨æ€è®°å¿†ç»´æŠ¤çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å¼•å…¥å¤–éƒ¨çŸ¥è¯†å›¾è°± (Knowledge Graph) ä½œä¸ºä¸–ç•ŒçŠ¶æ€çš„ç»Ÿä¸€è¡¨ç¤ºï¼Œèƒ½å¤Ÿæ•´åˆæ„Ÿå®˜è¾“å…¥å’Œäººç±»è‡ªç„¶è¯­è¨€äº¤äº’ç­‰å¤šæºä¿¡æ¯è¿›è¡Œå®æ—¶æ›´æ–°ã€‚L3M+P é€šè¿‡å¼ºåˆ¶æ‰§è¡Œç‰¹å®šçš„æ ¼å¼è§„åˆ™ç¡®ä¿äº†å›¾è°±æ›´æ–°çš„ä¸€è‡´æ€§ï¼Œå¹¶åœ¨è§„åˆ’é˜¶æ®µæ ¹æ®ä»»åŠ¡æè¿°ä»å›¾ä¸­æ£€ç´¢ä¸Šä¸‹æ–‡ï¼Œä¸ºä¼ ç»Ÿè§„åˆ’å™¨ç”Ÿæˆç²¾ç¡®çš„é—®é¢˜å®šä¹‰ã€‚åœ¨å®¶åº­æœºå™¨äººæ¨¡æ‹Ÿå™¨å’ŒçœŸå®æœåŠ¡æœºå™¨äººä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå‡­å€ŸçŸ¥è¯†å›¾è°±çš„æ£€ç´¢ä¸éªŒè¯æœºåˆ¶ï¼ŒL3M+P åœ¨å‡†ç¡®è®°å½•çŠ¶æ€å˜æ›´å’Œç”Ÿæˆæ­£ç¡®æ‰§è¡Œè§„åˆ’æ–¹é¢å‡æ¯”åŸºçº¿æ–¹æ³•æœ‰æ˜¾è‘—æå‡ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01917v1",
      "published_date": "2025-08-03 21:01:50 UTC",
      "updated_date": "2025-08-03 21:01:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:28:46.062879+00:00"
    },
    {
      "arxiv_id": "2508.01916v1",
      "title": "Decomposing Representation Space into Interpretable Subspaces with Unsupervised Learning",
      "title_zh": "åˆ©ç”¨æ— ç›‘ç£å­¦ä¹ å°†è¡¨ç¤ºç©ºé—´åˆ†è§£ä¸ºå¯è§£é‡Šå­ç©ºé—´",
      "authors": [
        "Xinting Huang",
        "Michael Hahn"
      ],
      "abstract": "Understanding internal representations of neural models is a core interest of mechanistic interpretability. Due to its large dimensionality, the representation space can encode various aspects about inputs. To what extent are different aspects organized and encoded in separate subspaces? Is it possible to find these ``natural'' subspaces in a purely unsupervised way? Somewhat surprisingly, we can indeed achieve this and find interpretable subspaces by a seemingly unrelated training objective. Our method, neighbor distance minimization (NDM), learns non-basis-aligned subspaces in an unsupervised manner. Qualitative analysis shows subspaces are interpretable in many cases, and encoded information in obtained subspaces tends to share the same abstract concept across different inputs, making such subspaces similar to ``variables'' used by the model. We also conduct quantitative experiments using known circuits in GPT-2; results show a strong connection between subspaces and circuit variables. We also provide evidence showing scalability to 2B models by finding separate subspaces mediating context and parametric knowledge routing. Viewed more broadly, our findings offer a new perspective on understanding model internals and building circuits.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºæ¢°å¯è§£é‡Šæ€§(mechanistic interpretability)ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œæ¢è®¨äº†ç¥ç»ç½‘ç»œè¡¨ç¤ºç©ºé—´ä¸­ä¸åŒå±æ€§çš„ç»„ç»‡ä¸ç¼–ç å½¢å¼ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åä¸ºé‚»åŸŸè·ç¦»æœ€å°åŒ–(Neighbor Distance Minimization, NDM)çš„æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤Ÿå°†é«˜ç»´ç©ºé—´åˆ†è§£ä¸ºéåŸºå‡†å¯¹é½(non-basis-aligned)çš„å¯è§£é‡Šå­ç©ºé—´ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™äº›å­ç©ºé—´åœ¨ä¸åŒè¾“å…¥é—´å…±äº«ç›¸åŒçš„æŠ½è±¡æ¦‚å¿µï¼Œå…¶åŠŸèƒ½ç±»ä¼¼äºæ¨¡å‹å†…éƒ¨çš„å˜é‡ã€‚åœ¨GPT-2ä¸Šçš„å®šé‡åˆ†æè¿›ä¸€æ­¥è¯å®äº†æ‰€å¾—å­ç©ºé—´ä¸ç”µè·¯å˜é‡(circuit variables)ä¹‹é—´çš„ç´§å¯†è”ç³»ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨2Bå‚æ•°æ¨¡å‹ä¸­æˆåŠŸåˆ†ç¦»äº†å¤„ç†ä¸Šä¸‹æ–‡ä¸å‚æ•°åŒ–çŸ¥è¯†è·¯ç”±çš„å­ç©ºé—´ï¼Œå±•ç¤ºäº†è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºç†è§£æ¨¡å‹å†…éƒ¨æœºç†å’Œæ„å»ºç”µè·¯(building circuits)æä¾›äº†å…¨æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01916v1",
      "published_date": "2025-08-03 20:59:29 UTC",
      "updated_date": "2025-08-03 20:59:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:29:18.904574+00:00"
    },
    {
      "arxiv_id": "2508.02762v2",
      "title": "Context-Adaptive Multi-Prompt Embedding with Large Language Models for Vision-Language Alignment",
      "title_zh": "é¢å‘è§†è§‰-è¯­è¨€å¯¹é½çš„åŸºäºå¤§è¯­è¨€æ¨¡å‹ä¸Šä¸‹æ–‡è‡ªé€‚åº”å¤šæç¤ºåµŒå…¥",
      "authors": [
        "Dahun Kim",
        "Anelia Angelova"
      ],
      "abstract": "We propose Context-Adaptive Multi-Prompt Embedding, a novel approach to enrich semantic representations in vision-language contrastive learning. Unlike standard CLIP-style models that rely on a single text embedding, our method introduces multiple structured prompts, each containing a distinct adaptive token that captures diverse semantic aspects of the input text. We leverage a pretrained LLM as the text encoder within the CLIP framework, processing all prompts jointly in a single forward pass. The resulting prompt embeddings are combined into a unified text representation, enabling semantically richer alignment with visual features. To further promote semantic diversity and representation quality, we incorporate a diversity regularization loss and a negation-aware loss, encouraging specialization across prompts and improving contrastive discrimination. Our method achieves consistent improvements on both image-text and video-text retrieval benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Context-Adaptive Multi-Prompt Embeddingï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å¢å¼ºè§†è§‰-è¯­è¨€å¯¹æ¯”å­¦ä¹ ä¸­è¯­ä¹‰è¡¨ç¤ºçš„æ–°æ–¹æ³•ã€‚ä¸ä¾èµ–å•ä¸€æ–‡æœ¬åµŒå…¥çš„ä¼ ç»Ÿ CLIP æ¨¡å‹ä¸åŒï¼Œè¯¥æ–¹æ³•é€šè¿‡å¼•å…¥å¤šä¸ªåŒ…å«ä¸åŒè‡ªé€‚åº” Token çš„ç»“æ„åŒ–æç¤ºï¼Œæ•æ‰è¾“å…¥æ–‡æœ¬çš„å¤šå…ƒè¯­ä¹‰ç»´åº¦ã€‚é€šè¿‡åœ¨ CLIP æ¡†æ¶ä¸­ä½¿ç”¨é¢„è®­ç»ƒ LLM ä½œä¸ºæ–‡æœ¬ç¼–ç å™¨ï¼Œç³»ç»Ÿèƒ½å¤Ÿåœ¨å•æ¬¡å‰å‘ä¼ é€’ä¸­å¤„ç†æ‰€æœ‰æç¤ºï¼Œå¹¶å°†å…¶ç»„åˆæˆç»Ÿä¸€ä¸”è¯­ä¹‰æ›´ä¸°å¯Œçš„æ–‡æœ¬è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº† Diversity Regularization Loss å’Œ Negation-aware Lossï¼Œä»¥ä¿ƒè¿›ä¸åŒæç¤ºé—´çš„ä¸“ä¸šåŒ–åˆ†å·¥å¹¶æé«˜å¯¹æ¯”è¾¨åˆ«åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒ-æ–‡æœ¬å’Œè§†é¢‘-æ–‡æœ¬æ£€ç´¢åŸºå‡†æµ‹è¯•ä¸­å‡å®ç°äº†æŒç»­çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶åœ¨è§†è§‰-è¯­è¨€å¯¹é½é¢†åŸŸçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "COLM 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.02762v2",
      "published_date": "2025-08-03 20:48:43 UTC",
      "updated_date": "2025-08-06 03:51:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:29:16.749359+00:00"
    },
    {
      "arxiv_id": "2508.01908v1",
      "title": "Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large Language Models",
      "title_zh": "é‡æ–°å®¡è§†å¤§è¯­è¨€æ¨¡å‹æŒç»­é¢„è®­ç»ƒä¸­çš„ç»éªŒå›æ”¾ä¸æ¢¯åº¦å¯¹é½",
      "authors": [
        "Istabrak Abbes",
        "Gopeshh Subbaraj",
        "Matthew Riemer",
        "Nizar Islah",
        "Benjamin Therien",
        "Tsuguchika Tabaru",
        "Hiroaki Kingetsu",
        "Sarath Chandar",
        "Irina Rish"
      ],
      "abstract": "Training large language models (LLMs) typically involves pre-training on massive corpora, only to restart the process entirely when new data becomes available. A more efficient and resource-conserving approach would be continual pre-training, where models are updated with new data rather than retraining from scratch. However, the introduction of new data often causes distribution shifts, leading to performance degradation on previously learned tasks. In this paper, we take a deeper look at two popular proposals for addressing this distribution shift within the continual learning literature: experience replay and gradient alignment. We consider continual pre-training of models within the Llama family of architectures at a large scale across languages with 100 billion tokens of training data in each language, finding that both replay and gradient alignment lead to more stable learning without forgetting. This conclusion holds both as we vary the model scale and as we vary the number and diversity of tasks. Moreover, we are the first to demonstrate the effectiveness of gradient alignment techniques in the context of LLM pre-training and propose an efficient implementation of meta-experience replay (MER) that imbues experience replay with the benefits of gradient alignment despite negligible compute and memory overhead. Our scaling analysis across model sizes and replay rates indicates that small rates of replaying old examples are definitely a more valuable use of compute than investing in model size, but that it is more compute efficient to scale the size of the model than invest in high rates of replaying old examples.",
      "tldr_zh": "é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æŒç»­é¢„è®­ç»ƒï¼ˆContinual Pre-Trainingï¼‰è¿‡ç¨‹ä¸­å› åˆ†å¸ƒåç§»å¯¼è‡´æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶é‡æ–°æ¢è®¨äº†ç»éªŒé‡æ”¾ï¼ˆExperience Replayï¼‰å’Œæ¢¯åº¦å¯¹é½ï¼ˆGradient Alignmentï¼‰ä¸¤ç§ç­–ç•¥ã€‚ç ”ç©¶äººå‘˜åœ¨ Llama ç³»åˆ—æ¶æ„ä¸Šè¿›è¡Œäº†å¤§è§„æ¨¡å®éªŒï¼Œä½¿ç”¨äº†æ¯ç§è¯­è¨€ 1000 äº¿ä¸ª Token çš„è®­ç»ƒæ•°æ®ï¼ŒéªŒè¯äº†è¿™ä¸¤ç§æ–¹æ³•åœ¨å¤šè¯­è¨€å’Œä¸åŒæ¨¡å‹è§„æ¨¡ä¸‹å‡èƒ½æ˜¾è‘—æé«˜å­¦ä¹ ç¨³å®šæ€§å¹¶æŠ‘åˆ¶é—å¿˜ã€‚è¯¥ç ”ç©¶é¦–æ¬¡è¯æ˜äº†æ¢¯åº¦å¯¹é½æŠ€æœ¯åœ¨ LLM é¢„è®­ç»ƒåœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å…ƒç»éªŒé‡æ”¾ï¼ˆMeta-Experience Replay, MERï¼‰å®ç°æ–¹æ¡ˆï¼Œåœ¨æä½å¼€é”€ä¸‹ä½¿ç»éªŒé‡æ”¾å…·å¤‡äº†æ¢¯åº¦å¯¹é½çš„ä¼˜åŠ¿ã€‚é€šè¿‡æ‰©å±•æ€§åˆ†æï¼ˆScaling Analysisï¼‰ï¼Œç ”ç©¶å‘ç°ä½æ¯”ä¾‹çš„æ—§æ ·æœ¬é‡æ”¾æ¯”å•çº¯å¢åŠ æ¨¡å‹è§„æ¨¡æ›´å…·è®¡ç®—ä»·å€¼ï¼Œä½†åœ¨é«˜é‡æ”¾ç‡ä¸‹ï¼Œå¢åŠ æ¨¡å‹è§„æ¨¡åˆ™æ¯”æé«˜é‡æ”¾æ¯”ä¾‹æ›´å…·è®¡ç®—æ•ˆç‡ã€‚è¯¥æˆæœä¸ºå¤§è§„æ¨¡æ¨¡å‹æŒç»­å­¦ä¹ ä¸­çš„è®¡ç®—èµ„æºåˆ†é…æä¾›äº†å…³é”®çš„ç†è®ºä¾æ®å’Œå®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01908v1",
      "published_date": "2025-08-03 20:07:15 UTC",
      "updated_date": "2025-08-03 20:07:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:29:22.193641+00:00"
    },
    {
      "arxiv_id": "2508.01892v1",
      "title": "How Does Controllability Emerge In Language Models During Pretraining?",
      "title_zh": "è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å¯æ§æ€§æ˜¯å¦‚ä½•æ¶Œç°çš„ï¼Ÿ",
      "authors": [
        "Jianshu She",
        "Xinyue Li",
        "Eric Xing",
        "Zhengzhong Liu",
        "Qirong Ho"
      ],
      "abstract": "Language models can be steered by modifying their internal representations to control concepts such as emotion, style, or truthfulness in generation. However, the conditions for an effective intervention remain unclear and are often validated through heuristics and trial-and-error. To fill this gap, we demonstrate that intervention efficacy, measured by linear steerability (i.e., the ability to adjust output via linear transformations of hidden states), emerges during intermediate stages of training. Moreover, even closely related concepts (e.g., anger and sadness) exhibit steerability emergence at distinct stages of training.\n  To better interpret the dynamics of steerability during training, we adapt existing intervention techniques into a unified framework, referred to as the \"Intervention Detector\" (ID), which is designed to reveal how linear steerability evolves over the course of training through hidden state and representation analysis. ID reveals that concepts become increasingly linearly separable in the hidden space as training progresses, which strongly correlates with the emergence of linear steerability. We further introduce ID-based metrics, such as heatmaps, entropy trends, and cosine similarity, to help interpret how linear steerability evolves throughout training. In addition, we apply ID across different model families to ensure the generality of our findings on steerability dynamics.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ Language Models çš„ Controllability æ˜¯å¦‚ä½•å‡ºç°çš„ï¼Œå‘ç° linear steerability é€šå¸¸åœ¨è®­ç»ƒçš„ä¸­æœŸé˜¶æ®µæ¶Œç°ï¼Œä¸”ä¸åŒä½†ç›¸å…³çš„æ¦‚å¿µåœ¨å¯æ§æ€§å‡ºç°çš„æ—¶æœºä¸Šå­˜åœ¨å·®å¼‚ã€‚ä¸ºäº†é‡åŒ–è¿™ä¸€åŠ¨æ€è¿‡ç¨‹ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªåä¸º Intervention Detector (ID) çš„ç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡åˆ†æ hidden state å’Œè¡¨ç¤ºæ¥æ­ç¤º linear steerability çš„æ¼”å˜ã€‚ç ”ç©¶å‘ç°ï¼Œéšç€è®­ç»ƒçš„è¿›è¡Œï¼Œæ¦‚å¿µåœ¨éšè—ç©ºé—´ä¸­çš„ linear separability ä¸æ–­å¢å¼ºï¼Œè¿™ä¸ linear steerability çš„äº§ç”Ÿé«˜åº¦ç›¸å…³ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº† heatmapsã€entropy trends å’Œ cosine similarity ç­‰åŸºäº ID çš„åº¦é‡æŒ‡æ ‡ï¼Œå¹¶åœ¨å¤šä¸ªæ¨¡å‹ç³»åˆ—ä¸ŠéªŒè¯äº†è¿™äº›å…³äºå¯æ§æ€§åŠ¨æ€å‘ç°çš„æ™®é€‚æ€§ï¼Œä¸ºç†è§£æ¨¡å‹å†…éƒ¨è¡¨ç¤ºçš„æ¼”åŒ–æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01892v1",
      "published_date": "2025-08-03 18:58:12 UTC",
      "updated_date": "2025-08-03 18:58:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:29:28.847978+00:00"
    },
    {
      "arxiv_id": "2508.01887v1",
      "title": "Complete Evasion, Zero Modification: PDF Attacks on AI Text Detection",
      "title_zh": "å½»åº•è§„é¿ï¼Œé›¶ä¿®æ”¹ï¼šé’ˆå¯¹ AI æ–‡æœ¬æ£€æµ‹çš„ PDF æ”»å‡»",
      "authors": [
        "Aldan Creo"
      ],
      "abstract": "AI-generated text detectors have become essential tools for maintaining content authenticity, yet their robustness against evasion attacks remains questionable. We present PDFuzz, a novel attack that exploits the discrepancy between visual text layout and extraction order in PDF documents. Our method preserves exact textual content while manipulating character positioning to scramble extraction sequences. We evaluate this approach against the ArguGPT detector using a dataset of human and AI-generated text. Our results demonstrate complete evasion: detector performance drops from (93.6 $\\pm$ 1.4) % accuracy and 0.938 $\\pm$ 0.014 F1 score to random-level performance ((50.4 $\\pm$ 3.2) % accuracy, 0.0 F1 score) while maintaining perfect visual fidelity. Our work reveals a vulnerability in current detection systems that is inherent to PDF document structures and underscores the need for implementing sturdy safeguards against such attacks. We make our code publicly available at https://github.com/ACMCMC/PDFuzz.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PDFuzzï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ AI-generated text detectors çš„æ–°å‹æ”»å‡»æ–¹æ³•ï¼Œæ—¨åœ¨åˆ©ç”¨ PDF æ–‡æ¡£ä¸­è§†è§‰æ–‡æœ¬å¸ƒå±€ä¸æ–‡æœ¬æå–é¡ºåºä¹‹é—´çš„å·®å¼‚å®ç°è§„é¿ã€‚è¯¥æ–¹æ³•é€šè¿‡æ“çºµå­—ç¬¦ä½ç½®æ¥æ‰°ä¹±æå–åºåˆ—ï¼ŒåŒæ—¶åœ¨è§†è§‰ä¸Šå®Œå…¨ä¿ç•™åŸå§‹æ–‡æœ¬å†…å®¹ï¼Œå®ç°äº† Zero Modification çš„æ•ˆæœã€‚ç ”ç©¶äººå‘˜åœ¨ ArguGPT æ£€æµ‹å™¨ä¸Šè¯„ä¼°äº†è¯¥æ–¹æ³•ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶èƒ½å¤Ÿå®ç° Complete Evasionï¼Œä½¿æ£€æµ‹å™¨çš„å‡†ç¡®ç‡ä» (93.6 Â± 1.4) % éª¤é™è‡³æ¥è¿‘éšæœºæ°´å¹³çš„ (50.4 Â± 3.2) %ï¼ŒF1 score é™è‡³ 0.0ã€‚è¿™é¡¹å·¥ä½œæ­ç¤ºäº†å½“å‰æ£€æµ‹ç³»ç»Ÿç”±äº PDF æ–‡æ¡£ç»“æ„ç‰¹æ€§è€Œäº§ç”Ÿçš„å›ºæœ‰è„†å¼±æ€§ï¼Œå¹¶å¼ºè°ƒäº†å¼€å‘é’ˆå¯¹æ­¤ç±»æ”»å‡»çš„é²æ£’é˜²å¾¡æªæ–½çš„è¿«åˆ‡æ€§ã€‚ç›®å‰è¯¥é¡¹ç›®çš„ä»£ç å·²åœ¨ GitHub ä¸Šå¼€æºã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.CR",
      "comment": "Code: https://github.com/ACMCMC/PDFuzz",
      "pdf_url": "https://arxiv.org/pdf/2508.01887v1",
      "published_date": "2025-08-03 18:43:41 UTC",
      "updated_date": "2025-08-03 18:43:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:29:29.005775+00:00"
    },
    {
      "arxiv_id": "2508.01871v1",
      "title": "Multi-turn Natural Language to Graph Query Language Translation",
      "title_zh": "å¤šè½®è‡ªç„¶è¯­è¨€åˆ°å›¾æŸ¥è¯¢è¯­è¨€çš„ç¿»è¯‘",
      "authors": [
        "Yuanyuan Liang",
        "Lei Pan",
        "Tingyu Xie",
        "Yunshi Lan",
        "Weining Qian"
      ],
      "abstract": "In recent years, research on transforming natural language into graph query language (NL2GQL) has been increasing. Most existing methods focus on single-turn transformation from NL to GQL. In practical applications, user interactions with graph databases are typically multi-turn, dynamic, and context-dependent. While single-turn methods can handle straightforward queries, more complex scenarios often require users to iteratively adjust their queries, investigate the connections between entities, or request additional details across multiple dialogue turns. Research focused on single-turn conversion fails to effectively address multi-turn dialogues and complex context dependencies. Additionally, the scarcity of high-quality multi-turn NL2GQL datasets further hinders the progress of this field. To address this challenge, we propose an automated method for constructing multi-turn NL2GQL datasets based on Large Language Models (LLMs) , and apply this method to develop the MTGQL dataset, which is constructed from a financial market graph database and will be publicly released for future research. Moreover, we propose three types of baseline methods to assess the effectiveness of multi-turn NL2GQL translation, thereby laying a solid foundation for future research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰è‡ªç„¶è¯­è¨€è½¬å›¾æŸ¥è¯¢è¯­è¨€(NL2GQL)é¢†åŸŸä¸»è¦é›†ä¸­äºå•è½®è½¬æ¢ï¼Œéš¾ä»¥åº”å¯¹å®é™…åº”ç”¨ä¸­åŠ¨æ€ä¸”å…·æœ‰ä¸Šä¸‹æ–‡ä¾èµ–çš„å¤šè½®äº¤äº’è¿™ä¸€ç°çŠ¶ã€‚ä¸ºäº†è§£å†³å¤æ‚å¯¹è¯å¤„ç†èƒ½åŠ›ä¸è¶³åŠé«˜è´¨é‡å¤šè½®æ•°æ®é›†åŒ®ä¹çš„æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)è‡ªåŠ¨æ„å»ºå¤šè½®NL2GQLæ•°æ®é›†çš„æ–¹æ³•ã€‚åŸºäºè¯¥æ–¹æ³•ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘å¹¶å…¬å¼€å‘å¸ƒäº†æºè‡ªé‡‘èå¸‚åœºå›¾æ•°æ®åº“çš„MTGQLæ•°æ®é›†ï¼Œæœ‰æ•ˆå¡«è¡¥äº†è¯¥é¢†åŸŸé«˜è´¨é‡å¤šè½®å¯¹è¯æ•°æ®çš„ç©ºç™½ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸‰ç§ç±»å‹çš„åŸºå‡†æ–¹æ³•(baseline methods)ç”¨äºè¯„ä¼°å¤šè½®NL2GQLç¿»è¯‘çš„æ€§èƒ½ï¼Œå¹¶åˆ†æäº†ä¸åŒæ¨¡å‹åœ¨å¤„ç†å¤æ‚ä¸Šä¸‹æ–‡ä¾èµ–æ—¶çš„è¡¨ç°ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡ç³»ç»Ÿæ€§åœ°æ„å»ºæ•°æ®èµ„æºå’Œè¯„ä¼°æ¡†æ¶ï¼Œä¸ºå®ç°æ›´ç²¾å‡†ã€æ›´å…·äº¤äº’æ€§çš„å›¾æ•°æ®åº“è‡ªç„¶è¯­è¨€æ¥å£å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.AI",
      "comment": "21 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.01871v1",
      "published_date": "2025-08-03 17:56:52 UTC",
      "updated_date": "2025-08-03 17:56:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:29:40.511189+00:00"
    },
    {
      "arxiv_id": "2508.01869v1",
      "title": "ProKG-Dial: Progressive Multi-Turn Dialogue Construction with Domain Knowledge Graphs",
      "title_zh": "ProKG-Dialï¼šåŸºäºé¢†åŸŸçŸ¥è¯†å›¾è°±çš„æ¸è¿›å¼å¤šè½®å¯¹è¯æ„å»º",
      "authors": [
        "Yuanyuan Liang",
        "Xiaoman Wang",
        "Tingyu Xie",
        "Lei Pan"
      ],
      "abstract": "Current large language models (LLMs) excel at general NLP tasks but often lack domain specific precision in professional settings. Building a high quality domain specific multi turn dialogue dataset is essential for developing specialized conversational systems. However, existing methods such as manual annotation, simulated human LLM interactions, and role based LLM dialogues are resource intensive or suffer from limitations in dialogue quality and domain coverage. To address these challenges, we introduce ProKG Dial, a progressive framework for constructing knowledge intensive multi turn dialogue datasets using domain specific knowledge graphs (KGs). ProKG Dial leverages the structured nature of KGs to encode complex domain knowledge and relationships, providing a solid foundation for generating meaningful and coherent dialogues. Specifically, ProKG Dial begins by applying community detection to partition the KG into semantically cohesive subgraphs. For each subgraph, the framework incrementally generates a series of questions and answers centered around a target entity, ensuring relevance and coverage. A rigorous filtering step is employed to maintain high dialogue quality. We validate ProKG Dial on a medical knowledge graph by evaluating the generated dialogues in terms of diversity, semantic coherence, and entity coverage. Furthermore, we fine tune a base LLM on the resulting dataset and benchmark it against several baselines. Both automatic metrics and human evaluations demonstrate that ProKG Dial substantially improves dialogue quality and domain specific performance, highlighting its effectiveness and practical utility.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ProKG-Dialï¼Œä¸€ç§åˆ©ç”¨é¢†åŸŸ Knowledge Graphs é€æ­¥æ„å»ºçŸ¥è¯†å¯†é›†å‹å¤šè½®å¯¹è¯æ•°æ®é›†çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ LLMs åœ¨ä¸“ä¸šé¢†åŸŸç¼ºä¹ç²¾ç¡®æ€§ä¸”æ•°æ®é›†æ„å»ºæˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚ProKG-Dial å……åˆ†åˆ©ç”¨çŸ¥è¯†å›¾è°±çš„ç»“æ„åŒ–ç‰¹æ€§ï¼Œé¦–å…ˆé€šè¿‡ community detection æŠ€æœ¯å°†å›¾è°±åˆ’åˆ†ä¸ºè¯­ä¹‰ç›¸å…³çš„å­å›¾ï¼Œä¸ºç”Ÿæˆè¿è´¯å¯¹è¯å¥ å®šåŸºç¡€ã€‚éšåï¼Œæ¡†æ¶å›´ç»•ç›®æ ‡å®ä½“å¢é‡å¼ç”Ÿæˆé—®ç­”å¯¹ï¼Œå¹¶é€šè¿‡ä¸¥æ ¼çš„è¿‡æ»¤æœºåˆ¶ç¡®ä¿å¯¹è¯çš„é«˜è´¨é‡ä¸é¢†åŸŸè¦†ç›–ç‡ã€‚åœ¨åŒ»ç–—çŸ¥è¯†å›¾è°±ä¸Šçš„å®éªŒéªŒè¯è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šæ ·æ€§ã€è¯­ä¹‰è¿è´¯æ€§å’Œå®ä½“è¦†ç›–ç‡æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚è‡ªåŠ¨æŒ‡æ ‡ä¸äººå·¥è¯„ä¼°å‡è¯å®ï¼Œç» ProKG-Dial æ•°æ®é›†å¾®è°ƒåçš„æ¨¡å‹åœ¨é¢†åŸŸç‰¹å®šä»»åŠ¡ä¸­çš„æ€§èƒ½æ˜¾è‘—æå‡ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨å¼€å‘ä¸“ä¸šåŒ–å¯¹è¯ç³»ç»Ÿæ–¹é¢çš„æœ‰æ•ˆæ€§ä¸å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.01869v1",
      "published_date": "2025-08-03 17:52:42 UTC",
      "updated_date": "2025-08-03 17:52:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:29:40.814424+00:00"
    },
    {
      "arxiv_id": "2508.01867v1",
      "title": "Counterfactual Reciprocal Recommender Systems for User-to-User Matching",
      "title_zh": "é¢å‘ç”¨æˆ·é—´åŒ¹é…çš„åäº‹å®åŒå‘æ¨èç³»ç»Ÿ",
      "authors": [
        "Kazuki Kawamura",
        "Takuma Udagawa",
        "Kei Tateno"
      ],
      "abstract": "Reciprocal recommender systems (RRS) in dating, gaming, and talent platforms require mutual acceptance for a match. Logged data, however, over-represents popular profiles due to past exposure policies, creating feedback loops that skew learning and fairness. We introduce Counterfactual Reciprocal Recommender Systems (CFRR), a causal framework to mitigate this bias. CFRR uses inverse propensity scored, self-normalized objectives. Experiments show CFRR improves NDCG@10 by up to 3.5% (e.g., from 0.459 to 0.475 on DBLP, from 0.299 to 0.307 on Synthetic), increases long-tail user coverage by up to 51% (from 0.504 to 0.763 on Synthetic), and reduces Gini exposure inequality by up to 24% (from 0.708 to 0.535 on Synthetic). CFRR offers a promising approach for more accurate and fair user-to-user matching.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Reciprocal Recommender Systems (RRS) æå‡ºäº†ä¸€ç§åä¸º Counterfactual Reciprocal Recommender Systems (CFRR) çš„å› æœæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç”±äºå†å²æ›å…‰ç­–ç•¥å¯¼è‡´çš„åé¦ˆå¾ªç¯å’Œå­¦ä¹ åå·®ã€‚CFRR é€šè¿‡é‡‡ç”¨ inverse propensity scored å’Œ self-normalized ç›®æ ‡å‡½æ•°ï¼Œæœ‰æ•ˆå‡è½»äº†çƒ­é—¨ profile åœ¨ logged data ä¸­çš„è¿‡åº¦ä»£è¡¨é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCFRR åœ¨ä¸åŒæ•°æ®é›†ä¸Šå°† NDCG@10 æå‡äº†æœ€é«˜ 3.5%ï¼Œæ˜¾è‘—æ”¹å–„äº†æ¨èçš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å°† long-tail ç”¨æˆ·çš„è¦†ç›–ç‡æå‡äº† 51%ï¼Œå¹¶å°† Gini æ›å…‰ä¸å¹³ç­‰é™ä½äº† 24%ã€‚è¯¥ç ”ç©¶è¯æ˜äº† CFRR åœ¨å®ç°æ›´å‡†ç¡®ã€æ›´å…¬å¹³çš„ user-to-user åŒ¹é…æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "9 pages, 2 figures. Accepted for publication at the Workshop on Two-sided Marketplace Optimization (TSMO '25), held in conjunction with the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2025), Toronto, Canada",
      "pdf_url": "https://arxiv.org/pdf/2508.01867v1",
      "published_date": "2025-08-03 17:45:04 UTC",
      "updated_date": "2025-08-03 17:45:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:29:50.954325+00:00"
    },
    {
      "arxiv_id": "2508.02760v1",
      "title": "Towards a Manifesto for Cyber Humanities: Paradigms, Ethics, and Prospects",
      "title_zh": "è¿ˆå‘èµ›åšäººæ–‡å®£è¨€ï¼šèŒƒå¼ã€ä¼¦ç†ä¸å‰æ™¯",
      "authors": [
        "Giovanni Adorni",
        "Emanuele Bellini"
      ],
      "abstract": "The accelerated evolution of digital infrastructures and algorithmic systems is reshaping how the humanities engage with knowledge and culture. Rooted in the traditions of Digital Humanities and Digital Humanism, the concept of \"Cyber Humanities\" proposes a critical reconfiguration of humanistic inquiry for the post-digital era. This Manifesto introduces a flexible framework that integrates ethical design, sustainable digital practices, and participatory knowledge systems grounded in human-centered approaches. By means of a Decalogue of foundational principles, the Manifesto invites the scientific community to critically examine and reimagine the algorithmic infrastructures that influence culture, creativity, and collective memory.\n  Rather than being a simple extension of existing practices, \"Cyber Humanities\" should be understood as a foundational paradigm for humanistic inquiry in a computationally mediated world.\n  Keywords: Cyber Humanities, Digital Humanities, Transdisciplinary Epistemology, Algorithmic Reflexivity, Human-centered AI, Ethics-by-Design, Knowledge Ecosystems, Digital Sovereignty, Cognitive Infrastructures",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ•°å­—åŸºç¡€è®¾æ–½å’Œç®—æ³•ç³»ç»Ÿå¯¹äººæ–‡é¢†åŸŸçŸ¥è¯†ä¸æ–‡åŒ–çš„é‡å¡‘ï¼Œå¹¶åŸºäº Digital Humanities å’Œ Digital Humanism çš„ä¼ ç»Ÿæå‡ºäº† Cyber Humanities çš„æ¦‚å¿µã€‚Cyber Humanities æ—¨åœ¨ä¸ºåæ•°å­—æ—¶ä»£çš„äººæ–‡ç ”ç©¶æä¾›æ‰¹åˆ¤æ€§çš„é‡æ„ï¼Œæå‡ºäº†ä¸€ä¸ªæ•´åˆ Ethics-by-Designã€å¯æŒç»­æ•°å­—å®è·µå’Œä»¥äººä¸ºæœ¬çš„å‚ä¸å¼çŸ¥è¯†ç³»ç»Ÿçš„çµæ´»æ¡†æ¶ã€‚è®ºæ–‡é€šè¿‡åŒ…å«åé¡¹åŸºæœ¬åŸåˆ™çš„ Manifestoï¼Œå¼•å¯¼å­¦æœ¯ç•Œé‡æ–°å®¡è§†å¹¶é‡æ„å½±å“æ–‡åŒ–ã€åˆ›æ„å’Œé›†ä½“è®°å¿†çš„ç®—æ³•åŸºç¡€è®¾æ–½ã€‚è¯¥å®£è¨€ä¸ä»…å…³æ³¨æŠ€æœ¯åº”ç”¨ï¼Œæ›´å¼ºè°ƒäº†æ•°å­—ä¸»æƒï¼ˆDigital Sovereigntyï¼‰å’Œè®¤çŸ¥åŸºç¡€è®¾æ–½ï¼ˆCognitive Infrastructuresï¼‰åœ¨çŸ¥è¯†ç”Ÿæ€ç³»ç»Ÿä¸­çš„é‡è¦æ€§ã€‚ç ”ç©¶æœ€ç»ˆæŒ‡å‡º Cyber Humanities åº”è¢«è§†ä¸ºè®¡ç®—ä¸­ä»‹ä¸–ç•Œä¸­äººæ–‡æ¢ç´¢çš„åŸºç¡€èŒƒå¼ï¼ˆFoundational Paradigmï¼‰ï¼Œè€Œéä»…ä»…æ˜¯ç°æœ‰å­¦ç§‘å®è·µçš„ç®€å•å»¶ä¼¸ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.DL"
      ],
      "primary_category": "cs.CY",
      "comment": "18 pages, 1 table, 48 references, to appear in: 1st. IEEE Int. Conf. on \"Cyber Humanities\"",
      "pdf_url": "https://arxiv.org/pdf/2508.02760v1",
      "published_date": "2025-08-03 17:33:24 UTC",
      "updated_date": "2025-08-03 17:33:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:29:53.847718+00:00"
    },
    {
      "arxiv_id": "2508.01862v1",
      "title": "Counterfactual Probing for Hallucination Detection and Mitigation in Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹å¹»è§‰æ£€æµ‹ä¸ç¼“è§£çš„åäº‹å®æ¢æµ‹",
      "authors": [
        "Yijun Feng"
      ],
      "abstract": "Large Language Models have demonstrated remarkable capabilities across diverse tasks, yet they frequently generate hallucinations outputs that are fluent but factually incorrect or unsupported. We propose Counterfactual Probing, a novel approach for detecting and mitigating hallucinations in LLM outputs. Our method dynamically generates counterfactual statements that appear plausible but contain subtle factual errors, then evaluates the model's sensitivity to these perturbations. We hypothesize that genuine knowledge exhibits robustness to counterfactual variations, while hallucinated content shows inconsistent confidence patterns when confronted with plausible alternatives. Our comprehensive evaluation on TruthfulQA, factual statement datasets, and curated hallucination examples demonstrates that counterfactual probing achieves superior detection performance compared to baseline methods, while our adaptive mitigation strategies reduce hallucination scores by an average of 24.5%. The approach requires no model retraining and can be integrated into existing LLM pipelines as a realtime verification mechanism.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Counterfactual Probingï¼Œä¸€ç§ç”¨äºæ£€æµ‹å’Œç¼“è§£å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¹»è§‰ï¼ˆHallucinationï¼‰çš„æ–°å‹æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åŠ¨æ€ç”Ÿæˆçœ‹ä¼¼åˆç†ä½†åŒ…å«ç»†å¾®äº‹å®é”™è¯¯çš„åäº‹å®é™ˆè¿°ï¼Œå¹¶è¯„ä¼°æ¨¡å‹å¯¹è¿™äº›æ‰°åŠ¨çš„æ•æ„Ÿåº¦ï¼Œä»è€Œæœ‰æ•ˆè¯†åˆ«è™šå‡ä¿¡æ¯ã€‚å…¶æ ¸å¿ƒå‡è®¾æ˜¯çœŸå®çŸ¥è¯†å¯¹åäº‹å®å˜ä½“å…·æœ‰é²æ£’æ€§ï¼Œè€Œå¹»è§‰å†…å®¹åœ¨é¢å¯¹åˆç†æ›¿ä»£æ–¹æ¡ˆæ—¶ä¼šè¡¨ç°å‡ºä¸ä¸€è‡´çš„ç½®ä¿¡åº¦æ¨¡å¼ã€‚åœ¨ TruthfulQA ç­‰æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒCounterfactual Probing çš„æ£€æµ‹æ€§èƒ½ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œä¸”å…¶è‡ªé€‚åº”ç¼“è§£ç­–ç•¥ä½¿å¹»è§‰è¯„åˆ†å¹³å‡é™ä½äº† 24.5%ã€‚è¯¥æ–¹æ³•æ— éœ€æ¨¡å‹é‡è®­ï¼ˆRetrainingï¼‰ï¼Œå¯ä½œä¸ºå®æ—¶éªŒè¯æœºåˆ¶é›†æˆåˆ°ç°æœ‰çš„ LLM æµæ°´çº¿ä¸­ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01862v1",
      "published_date": "2025-08-03 17:29:48 UTC",
      "updated_date": "2025-08-03 17:29:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:29:49.512664+00:00"
    },
    {
      "arxiv_id": "2508.01861v3",
      "title": "ACT-Tensor: Tensor Completion Framework for Financial Dataset Imputation",
      "title_zh": "ACT-Tensorï¼šé¢å‘é‡‘èæ•°æ®é›†å¡«è¡¥çš„å¼ é‡è¡¥å…¨æ¡†æ¶",
      "authors": [
        "Junyi Mo",
        "Jiayu Li",
        "Duo Zhang",
        "Elynn Chen"
      ],
      "abstract": "Missing data in financial panels presents a critical obstacle, undermining asset-pricing models and reducing the effectiveness of investment strategies. Such panels are often inherently multi-dimensional, spanning firms, time, and financial variables, which adds complexity to the imputation task. Conventional imputation methods often fail by flattening the data's multidimensional structure, struggling with heterogeneous missingness patterns, or overfitting in the face of extreme data sparsity. To address these limitations, we introduce an Adaptive, Cluster-based Temporal smoothing tensor completion framework (ACT-Tensor) tailored for severely and heterogeneously missing multi-dimensional financial data panels. ACT-Tensor incorporates two key innovations: a cluster-based completion module that captures cross-sectional heterogeneity by learning group-specific latent structures; and a temporal smoothing module that proactively removes short-lived noise while preserving slow-moving fundamental trends. Extensive experiments show that ACT-Tensor consistently outperforms state-of-the-art benchmarks in terms of imputation accuracy across a range of missing data regimes, including extreme sparsity scenarios. To assess its practical financial utility, we evaluate the imputed data with an asset-pricing pipeline tailored for tensor-structured financial data. Results show that ACT-Tensor not only reduces pricing errors but also significantly improves risk-adjusted returns of the constructed portfolio. These findings confirm that our method delivers highly accurate and informative imputations, offering substantial value for financial decision-making.",
      "tldr_zh": "é‡‘èæ•°æ®é¢æ¿ä¸­çš„ç¼ºå¤±æ•°æ®ä¼šå‰Šå¼±èµ„äº§å®šä»·æ¨¡å‹å¹¶é™ä½æŠ•èµ„ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œå°¤å…¶æ˜¯å½“æ•°æ®å‘ˆç°å‡ºè·¨å…¬å¸ã€æ—¶é—´å’Œè´¢åŠ¡å˜é‡çš„å¤šç»´ç»“æ„æ—¶ã€‚ä¸ºäº†è§£å†³ä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†éé½æ¬¡ç¼ºå¤±æ¨¡å¼æˆ–æ•°æ®æåº¦ç¨€ç–æ—¶çš„å±€é™æ€§ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ACT-Tensorï¼Œä¸€ç§è‡ªé€‚åº”ã€åŸºäºèšç±»çš„æ—¶åºå¹³æ»‘å¼ é‡è¡¥å…¨æ¡†æ¶(Tensor Completion)ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤å¤§æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼šå…¶ä¸€ï¼ŒåŸºäºèšç±»çš„è¡¥å…¨æ¨¡å—é€šè¿‡å­¦ä¹ ç‰¹å®šç¾¤ä½“çš„æ½œåœ¨ç»“æ„æ¥æ•æ‰æˆªé¢å¼‚è´¨æ€§(cross-sectional heterogeneity)ï¼›å…¶äºŒï¼Œæ—¶åºå¹³æ»‘æ¨¡å—åœ¨ä¿ç•™ç¼“æ…¢å˜åŠ¨çš„åŸºæœ¬é¢è¶‹åŠ¿çš„åŒæ—¶ï¼Œä¸»åŠ¨ç§»é™¤çŸ­æœŸå™ªå£°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨åŒ…æ‹¬æåº¦ç¨€ç–åœ¨å†…çš„å¤šç§ç¼ºå¤±åœºæ™¯ä¸‹ï¼ŒACT-Tensoråœ¨è¡¥å…¨ç²¾åº¦æ–¹é¢å§‹ç»ˆä¼˜äºç°æœ‰çš„åŸºå‡†æ¨¡å‹ã€‚é€šè¿‡èµ„äº§å®šä»·æµæ°´çº¿çš„è¯„ä¼°ï¼Œè¯¥ç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†ACT-Tensorèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘å®šä»·è¯¯å·®å¹¶æ˜¾è‘—æå‡æŠ•èµ„ç»„åˆçš„é£é™©è°ƒæ•´æ”¶ç›Š(risk-adjusted returns)ï¼Œä¸ºé‡‘èå†³ç­–æä¾›äº†æå…·ä»·å€¼çš„é«˜ç²¾åº¦æ’è¡¥æ•°æ®ã€‚",
      "categories": [
        "stat.AP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.AP",
      "comment": "The 6th ACM International Conference on AI in Finance (ICAIF 2025)",
      "pdf_url": "https://arxiv.org/pdf/2508.01861v3",
      "published_date": "2025-08-03 17:28:57 UTC",
      "updated_date": "2025-10-08 02:59:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:30:19.047578+00:00"
    },
    {
      "arxiv_id": "2508.01858v1",
      "title": "Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents",
      "title_zh": "Web-CogReasonerï¼šé¢å‘ Web æ™ºèƒ½ä½“çš„çŸ¥è¯†å¼•å¯¼è®¤çŸ¥æ¨ç†",
      "authors": [
        "Yuhan Guo",
        "Cong Guo",
        "Aiwen Sun",
        "Hongliang He",
        "Xinyu Yang",
        "Yue Lu",
        "Yingji Zhang",
        "Xuntao Guo",
        "Dong Zhang",
        "Jianzhuang Liu",
        "Jiang Duan",
        "Yijia Xiao",
        "Liangjian Wen",
        "Hai-Ming Xu",
        "Yong Dai"
      ],
      "abstract": "Multimodal large-scale models have significantly advanced the development of web agents, enabling perception and interaction with digital environments akin to human cognition. In this paper, we argue that web agents must first acquire sufficient knowledge to effectively engage in cognitive reasoning. Therefore, we decompose a web agent's capabilities into two essential stages: knowledge content learning and cognitive processes. To formalize this, we propose Web-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and Procedural. In this framework, knowledge content learning corresponds to the agent's processes of Memorizing and Understanding, which rely on the first two knowledge types, representing the \"what\" of learning. Conversely, cognitive processes correspond to Exploring, grounded in Procedural knowledge, defining the \"how\" of reasoning and action. To facilitate knowledge acquisition, we construct the Web-CogDataset, a structured resource curated from 14 real-world websites, designed to systematically instill core knowledge necessary for web agent. This dataset serves as the agent's conceptual grounding-the \"nouns\" upon which comprehension is built-as well as the basis for learning how to reason and act. Building on this foundation, we operationalize these processes through a novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing and training our proposed agent, the Web-CogReasoner. Extensive experimentation reveals its significant superiority over existing models, especially in generalizing to unseen tasks where structured knowledge is decisive. To enable rigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation suite designed to assess and compare agent performance across the delineated knowledge domains and cognitive capabilities. Our code and data is open sourced at https://github.com/Gnonymous/Web-CogReasoner",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Web-CogReasonerï¼Œæ—¨åœ¨é€šè¿‡çŸ¥è¯†è¯±å¯¼(Knowledge-Induced)æå‡Webæ™ºèƒ½ä½“çš„è®¤çŸ¥æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶è€…æ„å»ºäº†Web-CogKnowledge Frameworkï¼Œå°†æ™ºèƒ½ä½“èƒ½åŠ›åˆ†è§£ä¸ºçŸ¥è¯†å†…å®¹å­¦ä¹ ä¸è®¤çŸ¥è¿‡ç¨‹ï¼Œå¹¶å°†ç›¸å…³çŸ¥è¯†ç»†åˆ†ä¸ºFactualã€Conceptualå’ŒProceduralä¸‰ç±»ã€‚ä¸ºäº†æ”¯æŒçŸ¥è¯†è·å–ï¼Œç ”ç©¶å›¢é˜Ÿä»14ä¸ªçœŸå®ç½‘ç«™ä¸­æ„å»ºäº†ç»“æ„åŒ–æ•°æ®é›†Web-CogDatasetï¼Œä¸ºæ™ºèƒ½ä½“çš„ç†è§£ä¸è¡ŒåŠ¨æä¾›äº†åº•å±‚æ¦‚å¿µæ”¯æ’‘ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼ŒWeb-CogReasoneré‡‡ç”¨äº†å…¨æ–°çš„çŸ¥è¯†é©±åŠ¨å‹Chain-of-Thought (CoT) æ¨ç†æ¡†æ¶ï¼Œå°†ç»“æ„åŒ–çŸ¥è¯†è½¬åŒ–ä¸ºå…·ä½“çš„æ¨ç†é€»è¾‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç»“æ„åŒ–çŸ¥è¯†å¼•å¯¼çš„æœªçŸ¥ä»»åŠ¡æ³›åŒ–ä¸­è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¨å‡ºäº†Web-CogBenchè¯„ä¼°å¥—ä»¶ï¼Œæ—¨åœ¨ç³»ç»Ÿæ€§åœ°è¡¡é‡æ™ºèƒ½ä½“åœ¨ä¸åŒçŸ¥è¯†é¢†åŸŸå’Œè®¤çŸ¥èƒ½åŠ›ä¸Šçš„è¡¨ç°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Our code and data is open sourced at https://github.com/Gnonymous/Web-CogReasoner",
      "pdf_url": "https://arxiv.org/pdf/2508.01858v1",
      "published_date": "2025-08-03 17:17:52 UTC",
      "updated_date": "2025-08-03 17:17:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:30:14.805660+00:00"
    },
    {
      "arxiv_id": "2508.02758v1",
      "title": "CTBench: Cryptocurrency Time Series Generation Benchmark",
      "title_zh": "CTBenchï¼šåŠ å¯†è´§å¸æ—¶é—´åºåˆ—ç”ŸæˆåŸºå‡†",
      "authors": [
        "Yihao Ang",
        "Qiang Wang",
        "Qiang Huang",
        "Yifan Bao",
        "Xinyu Xi",
        "Anthony K. H. Tung",
        "Chen Jin",
        "Zhiyong Huang"
      ],
      "abstract": "Synthetic time series are essential tools for data augmentation, stress testing, and algorithmic prototyping in quantitative finance. However, in cryptocurrency markets, characterized by 24/7 trading, extreme volatility, and rapid regime shifts, existing Time Series Generation (TSG) methods and benchmarks often fall short, jeopardizing practical utility. Most prior work (1) targets non-financial or traditional financial domains, (2) focuses narrowly on classification and forecasting while neglecting crypto-specific complexities, and (3) lacks critical financial evaluations, particularly for trading applications. To address these gaps, we introduce \\textsf{CTBench}, the first comprehensive TSG benchmark tailored for the cryptocurrency domain. \\textsf{CTBench} curates an open-source dataset from 452 tokens and evaluates TSG models across 13 metrics spanning 5 key dimensions: forecasting accuracy, rank fidelity, trading performance, risk assessment, and computational efficiency. A key innovation is a dual-task evaluation framework: (1) the \\emph{Predictive Utility} task measures how well synthetic data preserves temporal and cross-sectional patterns for forecasting, while (2) the \\emph{Statistical Arbitrage} task assesses whether reconstructed series support mean-reverting signals for trading. We benchmark eight representative models from five methodological families over four distinct market regimes, uncovering trade-offs between statistical fidelity and real-world profitability. Notably, \\textsf{CTBench} offers model ranking analysis and actionable guidance for selecting and deploying TSG models in crypto analytics and strategy development.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CTBenchï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹åŠ å¯†è´§å¸é¢†åŸŸçš„ç»¼åˆæ€§æ—¶é—´åºåˆ—ç”Ÿæˆ(Time Series Generation, TSG)åŸºå‡†æµ‹è¯•ã€‚CTBenché€šè¿‡æ„å»ºåŒ…å«452ç§ä»£å¸çš„å¼€æºæ•°æ®é›†ï¼Œä»é¢„æµ‹å‡†ç¡®æ€§ã€ç§©ä¿çœŸåº¦(rank fidelity)ã€äº¤æ˜“è¡¨ç°ã€é£é™©è¯„ä¼°å’Œè®¡ç®—æ•ˆç‡äº”ä¸ªå…³é”®ç»´åº¦ï¼Œåˆ©ç”¨13ä¸ªæŒ‡æ ‡å¯¹TSGæ¨¡å‹è¿›è¡Œæ·±åº¦è¯„ä¼°ã€‚è¯¥åŸºå‡†çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶æå‡ºçš„åŒä»»åŠ¡è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬è¡¡é‡åˆæˆæ•°æ®å¯¹é¢„æµ‹æ¨¡å¼ä¿ç•™ç¨‹åº¦çš„Predictive Utilityä»»åŠ¡ï¼Œä»¥åŠè¯„ä¼°é‡å»ºåºåˆ—æ˜¯å¦æ”¯æŒå‡å€¼å›å½’ä¿¡å·çš„Statistical Arbitrageä»»åŠ¡ã€‚ç ”ç©¶äººå‘˜å¯¹äº”ä¸ªæ–¹æ³•è®ºå®¶æ—çš„å…«ä¸ªä»£è¡¨æ€§æ¨¡å‹åœ¨å››ç§ä¸åŒå¸‚åœºæœºåˆ¶(market regimes)ä¸‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†ç»Ÿè®¡ä¿çœŸåº¦ä¸å®é™…ç›ˆåˆ©èƒ½åŠ›ä¹‹é—´çš„æƒè¡¡ã€‚CTBenchä¸ä»…æä¾›äº†æ¨¡å‹æ’ååˆ†æï¼Œè¿˜ä¸ºåœ¨åŠ å¯†åˆ†æå’Œç­–ç•¥å¼€å‘ä¸­é€‰æ‹©åŠéƒ¨ç½²TSGæ¨¡å‹æä¾›äº†åˆ‡å®å¯è¡Œçš„æŒ‡å¯¼ã€‚",
      "categories": [
        "q-fin.ST",
        "cs.AI",
        "cs.CE",
        "cs.DB",
        "cs.LG"
      ],
      "primary_category": "q-fin.ST",
      "comment": "14 pages, 14 figures, and 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.02758v1",
      "published_date": "2025-08-03 17:07:08 UTC",
      "updated_date": "2025-08-03 17:07:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:30:14.616758+00:00"
    },
    {
      "arxiv_id": "2508.01850v1",
      "title": "ChairPose: Pressure-based Chair Morphology Grounded Sitting Pose Estimation through Simulation-Assisted Training",
      "title_zh": "ChairPoseï¼šåŸºäºå‹åŠ›æ„ŸçŸ¥ä¸æ¤…å­å½¢æ€å…³è”çš„æ¨¡æ‹Ÿè¾…åŠ©è®­ç»ƒåå§¿ä¼°è®¡",
      "authors": [
        "Lala Shakti Swarup Ray",
        "Vitor Fortes Rey",
        "Bo Zhou",
        "Paul Lukowicz",
        "Sungho Suh"
      ],
      "abstract": "Prolonged seated activity is increasingly common in modern environments, raising concerns around musculoskeletal health, ergonomics, and the design of responsive interactive systems. Existing posture sensing methods such as vision-based or wearable approaches face limitations including occlusion, privacy concerns, user discomfort, and restricted deployment flexibility. We introduce ChairPose, the first full body, wearable free seated pose estimation system that relies solely on pressure sensing and operates independently of chair geometry. ChairPose employs a two stage generative model trained on pressure maps captured from a thin, chair agnostic sensing mattress. Unlike prior approaches, our method explicitly incorporates chair morphology into the inference process, enabling accurate, occlusion free, and privacy preserving pose estimation. To support generalization across diverse users and chairs, we introduce a physics driven data augmentation pipeline that simulates realistic variations in posture and seating conditions. Evaluated across eight users and four distinct chairs, ChairPose achieves a mean per joint position error of 89.4 mm when both the user and the chair are unseen, demonstrating robust generalization to novel real world generalizability. ChairPose expands the design space for posture aware interactive systems, with potential applications in ergonomics, healthcare, and adaptive user interfaces.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ChairPoseç³»ç»Ÿï¼Œè¿™æ˜¯é¦–ä¸ªä»…ä¾èµ–å‹åŠ›ä¼ æ„Ÿã€æ— éœ€ç©¿æˆ´è®¾å¤‡ä¸”ç‹¬ç«‹äºåº§æ¤…å‡ ä½•å½¢çŠ¶çš„å…¨èº«åå§¿ä¼°è®¡(Sitting Pose Estimation)ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè§†è§‰æˆ–ç©¿æˆ´å¼æ–¹æ¡ˆåœ¨éšç§ã€é®æŒ¡å’Œä½©æˆ´èˆ’é€‚åº¦ä¸Šçš„å±€é™ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨ä¸¤é˜¶æ®µç”Ÿæˆæ¨¡å‹(Two-stage generative model)ï¼Œé€šè¿‡åœ¨è–„å‹åº§æ¤…æ— å…³ä¼ æ„Ÿå«ä¸Šé‡‡é›†çš„å‹åŠ›å›¾è¿›è¡Œè®­ç»ƒï¼Œå¹¶æ˜¾å¼å°†åº§æ¤…å½¢æ€(Chair morphology)çº³å…¥æ¨ç†è¿‡ç¨‹ä»¥å®ç°ç²¾ç¡®ä¼°è®¡ã€‚ä¸ºäº†å¢å¼ºå¯¹ä¸åŒç”¨æˆ·å’Œåº§æ¤…çš„æ³›åŒ–èƒ½åŠ›ï¼Œç ”ç©¶è€…å¼•å…¥äº†ç‰©ç†é©±åŠ¨çš„æ•°æ®å¢å¼ºæµæ°´çº¿(Physics-driven data augmentation pipeline)æ¥æ¨¡æ‹Ÿå¤æ‚çš„åå§¿å’Œåº§æ¤…æ¡ä»¶ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œåœ¨ç”¨æˆ·å’Œåº§æ¤…å‡ä¸ºæœªçŸ¥çš„æƒ…å†µä¸‹ï¼ŒChairPoseçš„å¹³å‡æ¯ä¸ªå…³èŠ‚ä½ç½®è¯¯å·®(MPJPE)ä»…ä¸º89.4 mmï¼Œå±•ç°äº†å“è¶Šçš„ç°å®æ³›åŒ–èƒ½åŠ›ã€‚è¯¥ç ”ç©¶ä¸ºäººä½“å·¥ç¨‹å­¦ã€åŒ»ç–—ä¿å¥å’Œè‡ªé€‚åº”ç”¨æˆ·ç•Œé¢ç­‰å§¿æ€æ„ŸçŸ¥äº¤äº’ç³»ç»Ÿçš„è®¾è®¡æä¾›äº†æ–°çš„å¯èƒ½ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01850v1",
      "published_date": "2025-08-03 17:06:09 UTC",
      "updated_date": "2025-08-03 17:06:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:30:16.909846+00:00"
    },
    {
      "arxiv_id": "2508.01845v1",
      "title": "Beyond Vulnerabilities: A Survey of Adversarial Attacks as Both Threats and Defenses in Computer Vision Systems",
      "title_zh": "è¶…è¶Šæ¼æ´ï¼šè®¡ç®—æœºè§†è§‰ç³»ç»Ÿä¸­å…¼å…·å¨èƒä¸é˜²å¾¡å±æ€§çš„å¯¹æŠ—æ€§æ”»å‡»ç»¼è¿°",
      "authors": [
        "Zhongliang Guo",
        "Yifei Qian",
        "Yanli Li",
        "Weiye Li",
        "Chun Tong Lei",
        "Shuai Zhao",
        "Lei Fang",
        "Ognjen ArandjeloviÄ‡",
        "Chun Pong Lau"
      ],
      "abstract": "Adversarial attacks against computer vision systems have emerged as a critical research area that challenges the fundamental assumptions about neural network robustness and security. This comprehensive survey examines the evolving landscape of adversarial techniques, revealing their dual nature as both sophisticated security threats and valuable defensive tools. We provide a systematic analysis of adversarial attack methodologies across three primary domains: pixel-space attacks, physically realizable attacks, and latent-space attacks. Our investigation traces the technical evolution from early gradient-based methods such as FGSM and PGD to sophisticated optimization techniques incorporating momentum, adaptive step sizes, and advanced transferability mechanisms. We examine how physically realizable attacks have successfully bridged the gap between digital vulnerabilities and real-world threats through adversarial patches, 3D textures, and dynamic optical perturbations. Additionally, we explore the emergence of latent-space attacks that leverage semantic structure in internal representations to create more transferable and meaningful adversarial examples. Beyond traditional offensive applications, we investigate the constructive use of adversarial techniques for vulnerability assessment in biometric authentication systems and protection against malicious generative models. Our analysis reveals critical research gaps, particularly in neural style transfer protection and computational efficiency requirements. This survey contributes a comprehensive taxonomy, evolution analysis, and identification of future research directions, aiming to advance understanding of adversarial vulnerabilities and inform the development of more robust and trustworthy computer vision systems.",
      "tldr_zh": "è¯¥ç»¼è¿°å…¨é¢åˆ†æäº†è®¡ç®—æœºè§†è§‰ç³»ç»Ÿä¸­çš„å¯¹æŠ—æ€§æ”»å‡» (Adversarial Attacks)ï¼Œæ­ç¤ºäº†å…¶ä½œä¸ºå®‰å…¨å¨èƒä¸é˜²å¾¡å·¥å…·çš„åŒé‡å±æ€§ã€‚ç ”ç©¶ä»åƒç´ ç©ºé—´æ”»å‡» (Pixel-space Attacks)ã€ç‰©ç†å¯å®ç°æ”»å‡» (Physically Realizable Attacks) å’Œæ½œåœ¨ç©ºé—´æ”»å‡» (Latent-space Attacks) ä¸‰ä¸ªç»´åº¦æ„å»ºäº†ç³»ç»ŸåŒ–åˆ†ç±»ã€‚æ–‡ä¸­è¿½è¸ªäº†æŠ€æœ¯ä» FGSM å’Œ PGD ç­‰æ—©æœŸæ¢¯åº¦æ–¹æ³•å‘å¤æ‚ä¼˜åŒ–æŠ€æœ¯çš„æ¼”è¿›ï¼Œå¹¶æ·±å…¥æ¢è®¨äº†å¯¹æŠ—æ€§è¡¥ä¸ (Adversarial Patches) åœ¨ç°å®ä¸–ç•Œä¸­çš„å¨èƒæ½œåŠ›ã€‚é™¤äº†ä¼ ç»Ÿçš„æ”»å‡»ç ”ç©¶ï¼Œè¯¥æ–‡è¿˜å¼ºè°ƒäº†å¯¹æŠ—æ€§æŠ€æœ¯åœ¨ç”Ÿç‰©ç‰¹å¾è®¤è¯å®‰å…¨å¢å¼ºå’Œé˜²èŒƒæ¶æ„ç”Ÿæˆæ¨¡å‹æ–¹é¢çš„å»ºè®¾æ€§åº”ç”¨ã€‚åˆ†æè¿›ä¸€æ­¥è¯†åˆ«äº†ç¥ç»é£æ ¼è¿ç§»ä¿æŠ¤å’Œè®¡ç®—æ•ˆç‡ç­‰å…³é”®ç ”ç©¶ç¼ºå£ã€‚é€šè¿‡å»ºç«‹å…¨é¢çš„åˆ†ç±»ä½“ç³»ä¸æ¼”å˜åˆ†æï¼Œè¯¥ç ”ç©¶ä¸ºå¼€å‘æ›´ç¨³å¥ã€æ›´å¯ä¿¡çš„è®¡ç®—æœºè§†è§‰ç³»ç»Ÿæä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ä¸æœªæ¥æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "comment": "33 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.01845v1",
      "published_date": "2025-08-03 17:02:05 UTC",
      "updated_date": "2025-08-03 17:02:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:30:38.703497+00:00"
    },
    {
      "arxiv_id": "2508.01844v2",
      "title": "Towards Generalizable Context-aware Anomaly Detection: A Large-scale Benchmark in Cloud Environments",
      "title_zh": "è¿ˆå‘å¯æ³›åŒ–çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¼‚å¸¸æ£€æµ‹ï¼šäº‘ç¯å¢ƒä¸­çš„å¤§è§„æ¨¡åŸºå‡†",
      "authors": [
        "Xinkai Zou",
        "Xuan Jiang",
        "Ruikai Huang",
        "Haoze He",
        "Parv Kapoor",
        "Hongrui Wu",
        "Yibo Wang",
        "Jian Sha",
        "Xiongbo Shi",
        "Zixun Huang",
        "Jinhua Zhao"
      ],
      "abstract": "Anomaly detection in cloud environments remains both critical and challenging. Existing context-level benchmarks typically focus on either metrics or logs and often lack reliable annotation, while most detection methods emphasize point anomalies within a single modality, overlooking contextual signals and limiting real-world applicability. Constructing a benchmark for context anomalies that combines metrics and logs is inherently difficult: reproducing anomalous scenarios on real servers is often infeasible or potentially harmful, while generating synthetic data introduces the additional challenge of maintaining cross-modal consistency. We introduce CloudAnoBench, a large-scale benchmark for context anomalies in cloud environments, comprising 28 anomalous scenarios and 16 deceptive normal scenarios, with 1,252 labeled cases and roughly 200,000 log and metric entries. Compared with prior benchmarks, CloudAnoBench exhibits higher ambiguity and greater difficulty, on which both prior machine learning methods and vanilla LLM prompting perform poorly. To demonstrate its utility, we further propose CloudAnoAgent, an LLM-based agent enhanced by symbolic verification that integrates metrics and logs. This agent system achieves substantial improvements in both anomaly detection and scenario identification on CloudAnoBench, and shows strong generalization to existing datasets. Together, CloudAnoBench and CloudAnoAgent lay the groundwork for advancing context-aware anomaly detection in cloud systems. Project Page: https://jayzou3773.github.io/cloudanobench-agent/",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äº‘ç¯å¢ƒå¼‚å¸¸æ£€æµ‹ä¸­ç°æœ‰åŸºå‡†æµ‹è¯•å±€é™äºå•ä¸€æ¨¡æ€ä¸”ç¼ºä¹ä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼ˆcontext-awareï¼‰çš„é—®é¢˜ï¼Œæ¨å‡ºäº†å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•é›†CloudAnoBenchã€‚è¯¥åŸºå‡†åŒ…å«28ç§å¼‚å¸¸åœºæ™¯å’Œ16ç§è¯¯å¯¼æ€§æ­£å¸¸åœºæ™¯ï¼Œæ¶‰åŠçº¦20ä¸‡æ¡æ—¥å¿—ï¼ˆlogsï¼‰å’ŒæŒ‡æ ‡ï¼ˆmetricsï¼‰æ¡ç›®ï¼Œç›¸æ¯”ç°æœ‰æ•°æ®é›†å…·æœ‰æ›´é«˜çš„æŒ‘æˆ˜æ€§å’Œæ­§ä¹‰æ€§ã€‚ä¸ºæå‡æ£€æµ‹æ•ˆèƒ½ï¼Œç ”ç©¶è€…è¿›ä¸€æ­¥æå‡ºäº†åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¹¶ç»“åˆç¬¦å·éªŒè¯ï¼ˆsymbolic verificationï¼‰çš„æ™ºèƒ½ä½“æ¡†æ¶CloudAnoAgentï¼Œå®ç°äº†å¯¹æŒ‡æ ‡ä¸æ—¥å¿—æ•°æ®çš„æ·±åº¦æ•´åˆã€‚å®éªŒè¯æ˜ï¼ŒCloudAnoAgentåœ¨å¼‚å¸¸æ£€æµ‹å’Œåœºæ™¯è¯†åˆ«æ–¹é¢å‡å–å¾—äº†æ˜¾è‘—æå‡ï¼Œå¹¶å¯¹å…¶ä»–ç°æœ‰æ•°æ®é›†å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–ï¼ˆgeneralizationï¼‰èƒ½åŠ›ã€‚CloudAnoBenchä¸CloudAnoAgentçš„å‘å¸ƒï¼Œä¸ºè§£å†³äº‘ç³»ç»Ÿä¸­å¤æ‚çš„ä¸Šä¸‹æ–‡å¼‚å¸¸ï¼ˆcontext anomaliesï¼‰æ£€æµ‹é—®é¢˜å¥ å®šäº†é‡è¦çš„æ•°æ®åŸºç¡€ä¸æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01844v2",
      "published_date": "2025-08-03 16:59:43 UTC",
      "updated_date": "2025-10-03 23:10:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:30:39.404331+00:00"
    },
    {
      "arxiv_id": "2508.01833v1",
      "title": "Neural Predictive Control to Coordinate Discrete- and Continuous-Time Models for Time-Series Analysis with Control-Theoretical Improvements",
      "title_zh": "èåˆæ§åˆ¶ç†è®ºæ”¹è¿›çš„ç¥ç»é¢„æµ‹æ§åˆ¶ï¼šç»Ÿåˆç¦»æ•£ä¸è¿ç»­æ—¶é—´æ¨¡å‹çš„æ—¶é—´åºåˆ—åˆ†æ",
      "authors": [
        "Haoran Li",
        "Muhao Guo",
        "Yang Weng",
        "Hanghang Tong"
      ],
      "abstract": "Deep sequence models have achieved notable success in time-series analysis, such as interpolation and forecasting. Recent advances move beyond discrete-time architectures like Recurrent Neural Networks (RNNs) toward continuous-time formulations such as the family of Neural Ordinary Differential Equations (Neural ODEs). Generally, they have shown that capturing the underlying dynamics is beneficial for generic tasks like interpolation, extrapolation, and classification. However, existing methods approximate the dynamics using unconstrained neural networks, which struggle to adapt reliably under distributional shifts. In this paper, we recast time-series problems as the continuous ODE-based optimal control problem. Rather than learning dynamics solely from data, we optimize control actions that steer ODE trajectories toward task objectives, bringing control-theoretical performance guarantees. To achieve this goal, we need to (1) design the appropriate control actions and (2) apply effective optimal control algorithms. As the actions should contain rich context information, we propose to employ the discrete-time model to process past sequences and generate actions, leading to a coordinate model to extract long-term temporal features to modulate short-term continuous dynamics. During training, we apply model predictive control to plan multi-step future trajectories, minimize a task-specific cost, and greedily select the optimal current action. We show that, under mild assumptions, this multi-horizon optimization leads to exponential convergence to infinite-horizon solutions, indicating that the coordinate model can gain robust and generalizable performance. Extensive experiments on diverse time-series datasets validate our method's superior generalization and adaptability compared to state-of-the-art baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåè°ƒç¦»æ•£ä¸è¿ç»­æ—¶é—´æ¨¡å‹çš„ Neural Predictive Control æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ Neural Ordinary Differential Equations (Neural ODEs) ç­‰æ¨¡å‹åœ¨åˆ†å¸ƒåç§»ï¼ˆdistributional shiftsï¼‰ä¸‹å¯é æ€§ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•å°†æ—¶é—´åºåˆ—åˆ†æé‡æ„ä¸ºåŸºäºè¿ç»­ ODE çš„æœ€ä¼˜æ§åˆ¶é—®é¢˜ï¼Œåˆ©ç”¨ç¦»æ•£æ—¶é—´æ¨¡å‹æå–é•¿æœŸæ—¶é—´ç‰¹å¾å¹¶ç”Ÿæˆæ§åˆ¶åŠ¨ä½œï¼Œä»¥æ­¤è°ƒèŠ‚çŸ­æœŸçš„è¿ç»­åŠ¨åŠ›å­¦ã€‚ç ”ç©¶åœ¨è®­ç»ƒä¸­å¼•å…¥äº† Model Predictive Control (MPC) ç®—æ³•æ¥è§„åˆ’å¤šæ­¥æœªæ¥è½¨è¿¹ï¼Œé€šè¿‡æœ€å°åŒ–ç‰¹å®šä»»åŠ¡æˆæœ¬æ¥è´ªå©ªåœ°é€‰æ‹©æœ€ä¼˜åŠ¨ä½œï¼Œå¹¶æä¾›äº†æ§åˆ¶ç†è®ºå±‚é¢çš„æ€§èƒ½ä¿è¯ã€‚ç†è®ºåˆ†æè¯æ˜ï¼Œè¿™ç§å¤šæ—¶åŸŸä¼˜åŒ–ï¼ˆmulti-horizon optimizationï¼‰åœ¨æ¸©å’Œå‡è®¾ä¸‹å…·æœ‰æŒ‡æ•°æ”¶æ•›æ€§ï¼Œç¡®ä¿äº†åè°ƒæ¨¡å‹çš„é²æ£’æ€§ä¸æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å¤šç§æ—¶é—´åºåˆ—æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ³›åŒ–æ€§å’Œé€‚åº”æ€§æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„ state-of-the-art åŸºå‡†æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, submitted to ACM SIGKDD Conference on Knowledge Discovery and Data Mining",
      "pdf_url": "https://arxiv.org/pdf/2508.01833v1",
      "published_date": "2025-08-03 16:41:00 UTC",
      "updated_date": "2025-08-03 16:41:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:30:30.603762+00:00"
    },
    {
      "arxiv_id": "2508.01822v1",
      "title": "Deep Learning-Driven Prediction of Microstructure Evolution via Latent Space Interpolation",
      "title_zh": "åŸºäºæ½œåœ¨ç©ºé—´æ’å€¼çš„æ·±åº¦å­¦ä¹ é©±åŠ¨å¾®è§‚ç»„ç»‡æ¼”åŒ–é¢„æµ‹",
      "authors": [
        "Sachin Gaikwad",
        "Thejas Kasilingam",
        "Owais Ahmad",
        "Rajdip Mukherjee",
        "Somnath Bhowmick"
      ],
      "abstract": "Phase-field models accurately simulate microstructure evolution, but their dependence on solving complex differential equations makes them computationally expensive. This work achieves a significant acceleration via a novel deep learning-based framework, utilizing a Conditional Variational Autoencoder (CVAE) coupled with Cubic Spline Interpolation and Spherical Linear Interpolation (SLERP). We demonstrate the method for binary spinodal decomposition by predicting microstructure evolution for intermediate alloy compositions from a limited set of training compositions. First, using microstructures from phase-field simulations of binary spinodal decomposition, we train the CVAE, which learns compact latent representations that encode essential morphological features. Next, we use cubic spline interpolation in the latent space to predict microstructures for any unknown composition. Finally, SLERP ensures smooth morphological evolution with time that closely resembles coarsening. The predicted microstructures exhibit high visual and statistical similarity to phase-field simulations. This framework offers a scalable and efficient surrogate model for microstructure evolution, enabling accelerated materials design and composition optimization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç›¸åœºæ¨¡å‹(Phase-field models)åœ¨æ¨¡æ‹Ÿå¾®è§‚ç»“æ„æ¼”åŒ–æ—¶è®¡ç®—æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆæ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨(CVAE)ã€ä¸‰æ¬¡æ ·æ¡æ’å€¼(Cubic Spline Interpolation)ä¸çƒé¢çº¿æ€§æ’å€¼(SLERP)çš„æ·±åº¦å­¦ä¹ åŠ é€Ÿæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡CVAEå­¦ä¹ äºŒå…ƒè°ƒå¹…åˆ†è§£(Binary spinodal decomposition)ä¸­å¾®è§‚ç»“æ„çš„ç´§å‡‘æ½œåœ¨è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨æ½œåœ¨ç©ºé—´æ’å€¼æŠ€æœ¯é¢„æµ‹æœªçŸ¥æˆåˆ†çš„ç»„ç»‡æ¼”å˜ã€‚å…¶ä¸­ï¼ŒSLERPæŠ€æœ¯çš„åº”ç”¨ç¡®ä¿äº†å½¢æ€éšæ—¶é—´æ¼”åŒ–çš„å¹³æ»‘æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿå‡†ç¡®æ¨¡æ‹Ÿå¾®è§‚ç»“æ„çš„ç²—åŒ–è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé¢„æµ‹çš„å¾®è§‚ç»“æ„åœ¨è§†è§‰ç‰¹å¾å’Œç»Ÿè®¡ç‰¹æ€§ä¸Šå‡ä¸é«˜ç²¾åº¦çš„ç›¸åœºæ¨¡æ‹Ÿç»“æœé«˜åº¦ä¸€è‡´ã€‚è¯¥ç ”ç©¶ä¸ºå¾®è§‚ç»“æ„æ¼”åŒ–æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ä¸”é«˜æ•ˆçš„ä»£ç†æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†æ¨¡æ‹Ÿé€Ÿåº¦ï¼Œä¸ºåŠ é€Ÿææ–™è®¾è®¡å’Œæˆåˆ†ä¼˜åŒ–å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "7 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.01822v1",
      "published_date": "2025-08-03 16:22:15 UTC",
      "updated_date": "2025-08-03 16:22:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:30:42.402975+00:00"
    },
    {
      "arxiv_id": "2508.01815v1",
      "title": "AGENTICT$^2$S:Robust Text-to-SPARQL via Agentic Collaborative Reasoning over Heterogeneous Knowledge Graphs for the Circular Economy",
      "title_zh": "AGENTICT$^2$Sï¼šåŸºäºå¼‚æ„çŸ¥è¯†å›¾è°±æ™ºèƒ½ä½“ååŒæ¨ç†çš„é¢å‘å¾ªç¯ç»æµçš„é²æ£’ Text-to-SPARQL",
      "authors": [
        "Yang Zhao",
        "Chengxiao Dai",
        "Wei Zhuo",
        "Tan Chuan Fu",
        "Yue Xiu",
        "Dusit Niyato",
        "Jonathan Z. Low",
        "Eugene Ho Hong Zhuang",
        "Daren Zong Loong Tan"
      ],
      "abstract": "Question answering over heterogeneous knowledge graphs (KGQA) involves reasoning across diverse schemas, incomplete alignments, and distributed data sources. Existing text-to-SPARQL approaches rely on large-scale domain-specific fine-tuning or operate within single-graph settings, limiting their generalizability in low-resource domains and their ability to handle queries spanning multiple graphs. These challenges are particularly relevant in domains such as the circular economy, where information about classifications, processes, and emissions is distributed across independently curated knowledge graphs (KGs). We present AgenticT$^2$S, a modular framework that decomposes KGQA into subtasks managed by specialized agents responsible for retrieval, query generation, and verification. A scheduler assigns subgoals to different graphs using weak-to-strong alignment strategies. A two-stage verifier detects structurally invalid and semantically underspecified queries through symbolic validation and counterfactual consistency checks. Experiments on real-world circular economy KGs demonstrate that AgenticT$^2$S improves execution accuracy by 17.3% and triple level F$_1$ by 25.4% over the best baseline, while reducing the average prompt length by 46.4%. These results demonstrate the benefits of agent-based schema-aware reasoning for scalable KGQA and support decision-making in sustainability domains through robust cross-graph reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AgenticT$^2$Sï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¾ªç¯ç»æµ (Circular Economy) é¢†åŸŸå¼‚æ„çŸ¥è¯†å›¾è°±é—®ç­” (KGQA) æŒ‘æˆ˜çš„æ¨¡å—åŒ–æ¡†æ¶ã€‚ä¸ºäº†è§£å†³ç°æœ‰ Text-to-SPARQL æ–¹æ³•åœ¨å¤„ç†åˆ†å¸ƒåœ¨å¤šä¸ªç‹¬ç«‹ç»´æŠ¤å›¾è°±ä¸­çš„æ•°æ®æ—¶å­˜åœ¨çš„é€šç”¨æ€§å·®å’Œè·¨å›¾æŸ¥è¯¢éš¾ç­‰é—®é¢˜ï¼ŒAgenticT$^2$S å°†ä»»åŠ¡åˆ†è§£ä¸ºç”±ä¸“é—¨æ™ºèƒ½ä½“è´Ÿè´£çš„æ£€ç´¢ã€æŸ¥è¯¢ç”Ÿæˆå’ŒéªŒè¯å­ä»»åŠ¡ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è°ƒåº¦å™¨é€šè¿‡å¼±åˆ°å¼ºå¯¹é½ç­–ç•¥åˆ†é…å­ç›®æ ‡ï¼Œå¹¶ç»“åˆä¸¤é˜¶æ®µéªŒè¯å™¨è¿›è¡Œç¬¦å·éªŒè¯å’Œåäº‹å®ä¸€è‡´æ€§æ£€æŸ¥ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„ SPARQL æŸ¥è¯¢åœ¨ç»“æ„å’Œè¯­ä¹‰ä¸Šçš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAgenticT$^2$S åœ¨æ‰§è¡Œå‡†ç¡®ç‡ä¸Šæ¯”åŸºçº¿æ¨¡å‹æå‡äº† 17.3%ï¼ŒF$_1$ åˆ†æ•°æå‡äº† 25.4%ï¼ŒåŒæ—¶å°†å¹³å‡æç¤ºè¯é•¿åº¦é™ä½äº† 46.4%ã€‚è¿™ä¸€æˆæœè¯æ˜äº†åŸºäºæ™ºèƒ½ä½“çš„åä½œæ¨ç†åœ¨å®ç°ç¨³å¥çš„è·¨å›¾é—®ç­”æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¯æŒç»­å‘å±•é¢†åŸŸçš„å†³ç­–æ”¯æŒæä¾›äº†æŠ€æœ¯ä¿éšœã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01815v1",
      "published_date": "2025-08-03 15:58:54 UTC",
      "updated_date": "2025-08-03 15:58:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:30:37.612161+00:00"
    },
    {
      "arxiv_id": "2508.01812v1",
      "title": "HeQ: a Large and Diverse Hebrew Reading Comprehension Benchmark",
      "title_zh": "HeQï¼šå¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„å¸Œä¼¯æ¥è¯­é˜…è¯»ç†è§£åŸºå‡†",
      "authors": [
        "Amir DN Cohen",
        "Hilla Merhav",
        "Yoav Goldberg",
        "Reut Tsarfaty"
      ],
      "abstract": "Current benchmarks for Hebrew Natural Language Processing (NLP) focus mainly on morpho-syntactic tasks, neglecting the semantic dimension of language understanding. To bridge this gap, we set out to deliver a Hebrew Machine Reading Comprehension (MRC) dataset, where MRC is to be realized as extractive Question Answering. The morphologically rich nature of Hebrew poses a challenge to this endeavor: the indeterminacy and non-transparency of span boundaries in morphologically complex forms lead to annotation inconsistencies, disagreements, and flaws in standard evaluation metrics.\n  To remedy this, we devise a novel set of guidelines, a controlled crowdsourcing protocol, and revised evaluation metrics that are suitable for the morphologically rich nature of the language. Our resulting benchmark, HeQ (Hebrew QA), features 30,147 diverse question-answer pairs derived from both Hebrew Wikipedia articles and Israeli tech news. Our empirical investigation reveals that standard evaluation metrics such as F1 scores and Exact Match (EM) are not appropriate for Hebrew (and other MRLs), and we propose a relevant enhancement.\n  In addition, our experiments show low correlation between models' performance on morpho-syntactic tasks and on MRC, which suggests that models designed for the former might underperform on semantics-heavy tasks. The development and exploration of HeQ illustrate some of the challenges MRLs pose in natural language understanding (NLU), fostering progression towards more and better NLU models for Hebrew and other MRLs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¸Œä¼¯æ¥è¯­ Natural Language Processing (NLP) é¢†åŸŸç›®å‰ä¸»è¦é›†ä¸­äºè¯æ³•å¥æ³•ä»»åŠ¡è€Œç¼ºä¹è¯­ä¹‰ç†è§£åŸºå‡†çš„é—®é¢˜ï¼Œæå‡ºäº† HeQï¼Œä¸€ä¸ªå¤§è§„æ¨¡ä¸”å¤šæ ·åŒ–çš„å¸Œä¼¯æ¥è¯­ Machine Reading Comprehension (MRC) åŸºå‡†ã€‚è€ƒè™‘åˆ°å¸Œä¼¯æ¥è¯­ä½œä¸º Morphologically Rich Language (MRL) çš„å½¢æ€å¤æ‚æ€§ä¼šå¯¼è‡´æ ‡æ³¨ä¸ä¸€è‡´å’Œè¯„ä¼°æŒ‡æ ‡å¤±æ•ˆï¼Œç ”ç©¶è€…è®¾è®¡äº†å…¨æ–°çš„æ ‡æ³¨æŒ‡å—ã€å—æ§ä¼—åŒ…åè®®ä»¥åŠä¿®è®¢åçš„è¯„ä¼°æŒ‡æ ‡ã€‚ç”Ÿæˆçš„ HeQ åŸºå‡†åŒ…å«ä»ç»´åŸºç™¾ç§‘å’Œç§‘æŠ€æ–°é—»ä¸­æå–çš„ 30,147 ä¸ªé—®ç­”å¯¹ã€‚å®è¯ç ”ç©¶è¡¨æ˜ï¼Œä¼ ç»Ÿçš„ F1 åˆ†æ•°å’Œ Exact Match (EM) æŒ‡æ ‡å¹¶ä¸é€‚ç”¨äºå¸Œä¼¯æ¥è¯­ç­‰ MRLsï¼Œç ”ç©¶ä¸ºæ­¤æå‡ºäº†é’ˆå¯¹æ€§çš„å¢å¼ºæ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œå®éªŒæ­ç¤ºäº†æ¨¡å‹åœ¨è¯æ³•å¥æ³•ä»»åŠ¡ä¸ MRC ä»»åŠ¡è¡¨ç°ä¹‹é—´ç›¸å…³æ€§è¾ƒä½ï¼Œæš—ç¤ºç°æœ‰æ¨¡å‹åœ¨è¯­ä¹‰å¯†é›†å‹ä»»åŠ¡ä¸­å¯èƒ½è¡¨ç°ä¸ä½³ã€‚HeQ çš„å»ºç«‹æœ‰æ•ˆå±•ç¤ºäº† MRLs åœ¨ Natural Language Understanding (NLU) é¢†åŸŸé¢ä¸´çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæ¨åŠ¨å¸Œä¼¯æ¥è¯­åŠå…¶ä»–å½¢æ€ä¸°å¯Œè¯­è¨€çš„æ¨¡å‹å‘å±•å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01812v1",
      "published_date": "2025-08-03 15:53:01 UTC",
      "updated_date": "2025-08-03 15:53:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:30:51.105797+00:00"
    },
    {
      "arxiv_id": "2508.01807v1",
      "title": "Mitigating Persistent Client Dropout in Asynchronous Decentralized Federated Learning",
      "title_zh": "ç¼“è§£å¼‚æ­¥å»ä¸­å¿ƒåŒ–è”é‚¦å­¦ä¹ ä¸­çš„æŒç»­æ€§å®¢æˆ·ç«¯æ‰çº¿é—®é¢˜",
      "authors": [
        "Ignacy StÄ™pka",
        "Nicholas Gisolfi",
        "Kacper TrÄ™bacz",
        "Artur Dubrawski"
      ],
      "abstract": "We consider the problem of persistent client dropout in asynchronous Decentralized Federated Learning (DFL). Asynchronicity and decentralization obfuscate information about model updates among federation peers, making recovery from a client dropout difficult. Access to the number of learning epochs, data distributions, and all the information necessary to precisely reconstruct the missing neighbor's loss functions is limited. We show that obvious mitigations do not adequately address the problem and introduce adaptive strategies based on client reconstruction. We show that these strategies can effectively recover some performance loss caused by dropout. Our work focuses on asynchronous DFL with local regularization and differs substantially from that in the existing literature. We evaluate the proposed methods on tabular and image datasets, involve three DFL algorithms, and three data heterogeneity scenarios (iid, non-iid, class-focused non-iid). Our experiments show that the proposed adaptive strategies can be effective in maintaining robustness of federated learning, even if they do not reconstruct the missing client's data precisely. We also discuss the limitations and identify future avenues for tackling the problem of client dropout.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¼‚æ­¥å»ä¸­å¿ƒåŒ–è”é‚¦å­¦ä¹  (Asynchronous Decentralized Federated Learning, DFL) ä¸­æŒä¹…æ€§å®¢æˆ·ç«¯æ‰çº¿ (persistent client dropout) çš„ç¼“è§£é—®é¢˜ã€‚ç”±äºå¼‚æ­¥æ€§å’Œå»ä¸­å¿ƒåŒ–æ©ç›–äº†æ¨¡å‹æ›´æ–°ä¿¡æ¯ï¼Œä¸”éš¾ä»¥è·å–é‡å»ºç¼ºå¤±é‚»å±…æŸå¤±å‡½æ•°æ‰€éœ€çš„ç²¾ç¡®æ•°æ®åˆ†å¸ƒå’Œè®­ç»ƒå‚æ•°ï¼Œå¯¼è‡´ä»å®¢æˆ·ç«¯æ‰çº¿ä¸­æ¢å¤æ€§èƒ½éå¸¸å›°éš¾ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç³»åˆ—åŸºäºå®¢æˆ·ç«¯é‡å»º (client reconstruction) çš„è‡ªé€‚åº”ç­–ç•¥ï¼Œæ—¨åœ¨å¼¥è¡¥å¸¸è§„ç¼“è§£æªæ–½çš„ä¸è¶³ã€‚è¯¥å·¥ä½œç‰¹åˆ«å…³æ³¨å¸¦æœ‰å±€éƒ¨æ­£åˆ™åŒ– (local regularization) çš„å¼‚æ­¥ DFL åœºæ™¯ï¼Œå¹¶åœ¨è¡¨æ ¼å’Œå›¾åƒæ•°æ®é›†ã€å¤šç§ DFL ç®—æ³•ä»¥åŠåŒ…æ‹¬ iidã€non-iid åœ¨å†…çš„ä¸‰ç§æ•°æ®å¼‚æ„åœºæ™¯ä¸‹è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ— æ³•ç²¾ç¡®é‡å»ºç¼ºå¤±å®¢æˆ·ç«¯çš„æ•°æ®ï¼Œæ‰€æç­–ç•¥ä¹Ÿèƒ½æœ‰æ•ˆç»´æŒè”é‚¦å­¦ä¹ ç³»ç»Ÿçš„é²æ£’æ€§å¹¶æ¢å¤æ€§èƒ½æŸå¤±ã€‚ç ”ç©¶æœ€åè¿˜è®¨è®ºäº†ç›¸å…³é™åˆ¶ï¼Œå¹¶ä¸ºè§£å†³æ‰çº¿é—®é¢˜æŒ‡æ˜äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "Presented on FedKDD Workshop at KDD 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.01807v1",
      "published_date": "2025-08-03 15:42:33 UTC",
      "updated_date": "2025-08-03 15:42:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:31:01.514523+00:00"
    },
    {
      "arxiv_id": "2508.06533v1",
      "title": "The Art of Breaking Words: Rethinking Multilingual Tokenizer Design",
      "title_zh": "ç¢è¯ä¹‹è‰ºï¼šå¤šè¯­è¨€åˆ†è¯å™¨è®¾è®¡çš„é‡æ–°å®¡è§†",
      "authors": [
        "Aamod Thakur",
        "Ajay Nagpal",
        "Atharva Savarkar",
        "Kundeshwar Pundalik",
        "Siddhesh Dosi",
        "Piyush Sawarkar",
        "Viraj Thakur",
        "Rohit Saluja",
        "Maunendra Sankar Desarkar",
        "Ganesh Ramakrishnan"
      ],
      "abstract": "While model architecture and training objectives are well-studied, tokenization, particularly in multilingual contexts, remains a relatively neglected aspect of Large Language Model (LLM) development. Existing tokenizers often exhibit high token-to-word ratios, inefficient use of context length, and slower inference. We present a systematic study that links vocabulary size, pre-tokenization rules, and training-corpus composition to both token-to-word efficiency and model quality. To ground our analysis in a linguistically diverse context, we conduct extensive experiments on Indic scripts, which present unique challenges due to their high script diversity and orthographic complexity. Drawing on the insights from these analyses, we propose a novel algorithm for data composition that balances multilingual data for tokenizer training. Our observations on pretokenization strategies significantly improve model performance, and our data composition algorithm reduces the average token-to-word ratio by approximately 6% with respect to the conventional data randomization approach. Our tokenizer achieves more than 40% improvement on average token-to-word ratio against stateof-the-art multilingual Indic models. This improvement yields measurable gains in both model performance and inference speed. This highlights tokenization alongside architecture and training objectives as a critical lever for building efficient, scalable multilingual LLMs",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹ (LLM) å¼€å‘ä¸­å¸¸è¢«å¿½è§†çš„åˆ†è¯å™¨ (Tokenizer) è®¾è®¡é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å¤šè¯­è¨€ç¯å¢ƒä¸‹å­˜åœ¨çš„é«˜ Token-to-word æ¯”ä¾‹ã€ä¸Šä¸‹æ–‡åˆ©ç”¨ç‡ä½å’Œæ¨ç†é€Ÿåº¦æ…¢ç­‰æŒ‘æˆ˜ã€‚ä½œè€…é€šè¿‡å¯¹ Indic è„šæœ¬è¿›è¡Œæ·±å…¥å®éªŒï¼Œç³»ç»Ÿåœ°ç ”ç©¶äº†è¯è¡¨å¤§å° (Vocabulary size)ã€é¢„åˆ†è¯è§„åˆ™ (Pre-tokenization rules) å’Œè®­ç»ƒè¯­æ–™æ„æˆå¯¹æ¨¡å‹è´¨é‡ä¸æ•ˆç‡çš„å½±å“ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„æ•°æ®åˆæˆç®—æ³•ç”¨äºå¹³è¡¡åˆ†è¯å™¨è®­ç»ƒä¸­çš„å¤šè¯­è¨€æ•°æ®ï¼Œå¹¶ä¼˜åŒ–äº†é¢„åˆ†è¯ç­–ç•¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç®—æ³•ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•å°†å¹³å‡ Token-to-word æ¯”ä¾‹é™ä½äº†çº¦ 6%ï¼Œä¸”åœ¨ä¸»æµ Indic è¯­è¨€æ¨¡å‹ä¸Šå®ç°äº†è¶…è¿‡ 40% çš„æ•ˆç‡æå‡ã€‚è¿™ä¸€æ”¹è¿›ä¸ä»…å¸¦æ¥äº†æ˜¾è‘—çš„æ¨¡å‹æ€§èƒ½å¢ç›Šï¼Œè¿˜åŠ å¿«äº†æ¨ç†é€Ÿåº¦ï¼Œå¼ºè°ƒäº† Tokenizer è®¾è®¡åœ¨æ„å»ºé«˜æ•ˆã€å¯æ‰©å±•çš„å¤šè¯­è¨€ LLM ä¸­æ˜¯ä¸æ¶æ„å’Œè®­ç»ƒç›®æ ‡åŒç­‰é‡è¦çš„å…³é”®ç¯èŠ‚ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.06533v1",
      "published_date": "2025-08-03 15:31:10 UTC",
      "updated_date": "2025-08-03 15:31:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:31:09.012682+00:00"
    },
    {
      "arxiv_id": "2508.01799v2",
      "title": "Contrastive Multi-Task Learning with Solvent-Aware Augmentation for Drug Discovery",
      "title_zh": "é¢å‘è¯ç‰©å‘ç°çš„æº¶å‰‚æ„ŸçŸ¥å¢å¼ºå¯¹æ¯”å¼å¤šä»»åŠ¡å­¦ä¹ ",
      "authors": [
        "Jing Lan",
        "Hexiao Ding",
        "Hongzhao Chen",
        "Yufeng Jiang",
        "Nga-Chun Ng",
        "Gerald W. Y. Cheng",
        "Zongxi Li",
        "Jing Cai",
        "Liang-ting Lin",
        "Jung Sun Yoo"
      ],
      "abstract": "Accurate prediction of protein-ligand interactions is essential for computer-aided drug discovery. However, existing methods often fail to capture solvent-dependent conformational changes and lack the ability to jointly learn multiple related tasks. To address these limitations, we introduce a pre-training method that incorporates ligand conformational ensembles generated under diverse solvent conditions as augmented input. This design enables the model to learn both structural flexibility and environmental context in a unified manner. The training process integrates molecular reconstruction to capture local geometry, interatomic distance prediction to model spatial relationships, and contrastive learning to build solvent-invariant molecular representations. Together, these components lead to significant improvements, including a 3.7% gain in binding affinity prediction, an 82% success rate on the PoseBusters Astex docking benchmarks, and an area under the curve of 97.1% in virtual screening. The framework supports solvent-aware, multi-task modeling and produces consistent results across benchmarks. A case study further demonstrates sub-angstrom docking accuracy with a root-mean-square deviation of 0.157 angstroms, offering atomic-level insight into binding mechanisms and advancing structure-based drug design.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰æ–¹æ³•éš¾ä»¥æ•è· solvent-dependent æ„è±¡å˜åŒ–ä¸”ç¼ºä¹å¤šä»»åŠ¡è”åˆå­¦ä¹ èƒ½åŠ›çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆ solvent-aware å¢å¼ºçš„å¯¹æ¯”å¤šä»»åŠ¡å­¦ä¹ é¢„è®­ç»ƒæ–¹æ³•ã€‚è¯¥æ–¹æ³•å¼•å…¥åœ¨ä¸åŒæº¶å‰‚æ¡ä»¶ä¸‹ç”Ÿæˆçš„ ligand conformational ensembles ä½œä¸ºå¢å¼ºè¾“å…¥ï¼Œæ—¨åœ¨ç»Ÿä¸€å­¦ä¹ åˆ†å­çš„ç»“æ„æŸ”æ€§ä¸ç¯å¢ƒä¸Šä¸‹æ–‡ã€‚è®­ç»ƒè¿‡ç¨‹æ•´åˆäº† molecular reconstruction ä»¥æ•æ‰å±€éƒ¨å‡ ä½•ç‰¹å¾ï¼Œåˆ©ç”¨ interatomic distance prediction å»ºæ¨¡ç©ºé—´å…³ç³»ï¼Œå¹¶é€šè¿‡ contrastive learning æ„å»ºæº¶å‰‚ä¸å˜çš„åˆ†å­è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ binding affinity é¢„æµ‹ä¸­æå‡äº†3.7%ï¼Œåœ¨ PoseBusters Astex åŸºå‡†æµ‹è¯•ä¸­å–å¾—82%çš„æˆåŠŸç‡ï¼Œå¹¶åœ¨ virtual screening ä¸­è¾¾åˆ°97.1%çš„ AUCã€‚æ­¤å¤–ï¼Œæ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº†è¯¥æ¨¡å‹å…·æœ‰ root-mean-square deviation ä»…ä¸º 0.157Ã… çš„ sub-angstrom å¯¹æ¥ç²¾åº¦ã€‚è¯¥æ¡†æ¶æ”¯æŒ solvent-aware çš„å¤šä»»åŠ¡å»ºæ¨¡ï¼Œä¸ºä»åŸå­æ°´å¹³æ´å¯Ÿç»“åˆæœºåˆ¶å¹¶æ¨è¿›åŸºäºç»“æ„çš„è¯ç‰©è®¾è®¡æä¾›äº†é‡è¦å·¥å…·ã€‚",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.BM",
      "comment": "10 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.01799v2",
      "published_date": "2025-08-03 15:25:42 UTC",
      "updated_date": "2025-08-27 04:11:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:31:16.903994+00:00"
    },
    {
      "arxiv_id": "2508.02755v1",
      "title": "Beyond the Wavefunction: Qualia Abstraction Language Mechanics and the Grammar of Awareness",
      "title_zh": "è¶…è¶Šæ³¢å‡½æ•°ï¼šæ„Ÿè´¨æŠ½è±¡è¯­è¨€åŠ›å­¦ä¸æ„è¯†è¯­æ³•",
      "authors": [
        "MikoÅ‚aj Sienicki",
        "Krzysztof Sienicki"
      ],
      "abstract": "We propose a formal reconstruction of quantum mechanics grounded not in external mathematical abstractions, but in the structured dynamics of subjective experience. The Qualia Abstraction Language (QAL) models physical systems as evolving streams of introspective units, structured sequences of modality, shape, and functional effect, rather than as state vectors in Hilbert space. This approach reimagines core quantum concepts: superposition becomes a form of structured ambiguity; collapse is reframed as an introspective contraction; and entanglement is modeled as semantic resonance across streams of qualia. Drawing on insights from nominalist philosophy and oversight theoretic limits in AI, we argue that the observer paradox in quantum mechanics reflects not an ontological lacuna, but a linguistic one: the absence of a formal vocabulary for modeling first person structure. QAL introduces such a vocabulary, providing a morphodynamic framework that embeds the observer within the system and replaces abstract projection with endogenous transformation. We analyze the alignment of QAL with endophysical approaches, contrast it with standard interpretations of quantum theory, and explore its implications for a post Platonist, introspectively grounded physics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Qualia Abstraction Language (QAL)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨åŸºäºä¸»è§‚ç»éªŒçš„ç»“æ„åŒ–åŠ¨æ€è€Œéå¤–éƒ¨æ•°å­¦æŠ½è±¡æ¥æ­£å¼é‡æ„ Quantum Mechanics çš„æ¡†æ¶ã€‚QAL å°†ç‰©ç†ç³»ç»Ÿå»ºæ¨¡ä¸ºä¸æ–­æ¼”åŒ–çš„å†…çœå•å…ƒæµï¼ŒåŒ…å«æ¨¡æ€ã€å½¢çŠ¶å’ŒåŠŸèƒ½æ•ˆåº”çš„ç»“æ„åŒ–åºåˆ—ï¼Œä»è€Œå–ä»£äº†ä¼ ç»Ÿçš„ Hilbert space ä¸­çš„ state vectorsã€‚åœ¨è¯¥æ¡†æ¶ä¸‹ï¼Œsuperposition è¢«é‡æ–°å®šä¹‰ä¸ºä¸€ç§ç»“æ„åŒ–æ­§ä¹‰ï¼Œcollapse è¢«é‡æ„ä¸ºå†…çœæ”¶ç¼©ï¼Œè€Œ entanglement åˆ™è¢«å»ºæ¨¡ä¸º qualia æµä¹‹é—´çš„è¯­ä¹‰å…±é¸£ã€‚ç ”ç©¶ç»“åˆäº†å”¯åè®ºå“²å­¦ä¸äººå·¥æ™ºèƒ½ç›‘ç£ç†è®ºï¼ŒæŒ‡å‡ºè§‚å¯Ÿè€…æ‚–è®ºæœ¬è´¨ä¸Šåæ˜ äº†å»ºæ¨¡ç¬¬ä¸€äººç§°ç»“æ„çš„æ­£å¼è¯æ±‡çš„ç¼ºå¤±ã€‚é€šè¿‡å¼•å…¥è¿™ç§æ–°è¯æ±‡ï¼ŒQAL æä¾›äº†ä¸€ä¸ªå°†è§‚å¯Ÿè€…åµŒå…¥ç³»ç»Ÿå†…éƒ¨çš„å½¢æ€åŠ¨åŠ›å­¦æ¡†æ¶ï¼Œç”¨å†…ç”Ÿå˜æ¢å–ä»£äº†æŠ½è±¡æŠ•å½±ã€‚è¯¥é¡¹å·¥ä½œä¸ºæ„å»ºä¸€ä¸ªåæŸæ‹‰å›¾ä¸»ä¹‰ä¸”åŸºäºå†…çœæ„ŸçŸ¥çš„ç‰©ç†å­¦ä½“ç³»å¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "physics.hist-ph",
        "cs.AI"
      ],
      "primary_category": "physics.hist-ph",
      "comment": "65 pages, 49 references, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.02755v1",
      "published_date": "2025-08-03 15:07:24 UTC",
      "updated_date": "2025-08-03 15:07:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:31:13.210257+00:00"
    },
    {
      "arxiv_id": "2508.01791v1",
      "title": "CSLRConformer: A Data-Centric Conformer Approach for Continuous Arabic Sign Language Recognition on the Isharah Datase",
      "title_zh": "CSLRConformerï¼šä¸€ç§ç”¨äº Isharah æ•°æ®é›†è¿ç»­é˜¿æ‹‰ä¼¯è¯­æ‰‹è¯­è¯†åˆ«çš„ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„ Conformer æ–¹æ³•",
      "authors": [
        "Fatimah Mohamed Emad Elden"
      ],
      "abstract": "The field of Continuous Sign Language Recognition (CSLR) poses substantial technical challenges, including fluid inter-sign transitions, the absence of temporal boundaries, and co-articulation effects. This paper, developed for the MSLR 2025 Workshop Challenge at ICCV 2025, addresses the critical challenge of signer-independent recognition to advance the generalization capabilities of CSLR systems across diverse signers. A data-centric methodology is proposed, centered on systematic feature engineering, a robust preprocessing pipeline, and an optimized model architecture. Key contributions include a principled feature selection process guided by Exploratory Data Analysis (EDA) to isolate communicative keypoints, a rigorous preprocessing pipeline incorporating DBSCAN-based outlier filtering and spatial normalization, and the novel CSLRConformer architecture. This architecture adapts the hybrid CNN-Transformer design of the Conformer model, leveraging its capacity to model local temporal dependencies and global sequence context; a characteristic uniquely suited for the spatio-temporal dynamics of sign language. The proposed methodology achieved a competitive performance, with a Word Error Rate (WER) of 5.60% on the development set and 12.01% on the test set, a result that secured a 3rd place ranking on the official competition platform. This research validates the efficacy of cross-domain architectural adaptation, demonstrating that the Conformer model, originally conceived for speech recognition, can be successfully repurposed to establish a new state-of-the-art performance in keypoint-based CSLR.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¿ç»­æ‰‹è¯­è¯†åˆ«(Continuous Sign Language Recognition, CSLR)ä¸­æ‰‹åŠ¿è½¬æ¢æµç•…ã€ç¼ºä¹æ—¶é—´è¾¹ç•ŒåŠååŒå‘éŸ³æ•ˆåº”ç­‰æŠ€æœ¯æŒ‘æˆ˜ï¼Œæå‡ºäº†æ—¨åœ¨æå‡è·¨æ‰‹è¯­è€…æ³›åŒ–èƒ½åŠ›çš„CSLRConformeræ¨¡å‹ã€‚ä½œè€…é‡‡ç”¨ä»¥æ•°æ®ä¸ºä¸­å¿ƒ(Data-Centric)çš„æ–¹æ³•ï¼Œé€šè¿‡æ¢ç´¢æ€§æ•°æ®åˆ†æ(Exploratory Data Analysis, EDA)ä¼˜åŒ–ç‰¹å¾é€‰æ‹©ï¼Œå¹¶åˆ©ç”¨DBSCANç¦»ç¾¤ç‚¹è¿‡æ»¤ä¸ç©ºé—´å½’ä¸€åŒ–å»ºç«‹äº†é²æ£’çš„é¢„å¤„ç†æµæ°´çº¿ã€‚æ ¸å¿ƒæ¶æ„CSLRConformeré€‚é…äº†Conformeræ¨¡å‹çš„CNNä¸Transformeræ··åˆè®¾è®¡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰æ‰‹è¯­è§†é¢‘ä¸­çš„å±€éƒ¨æ—¶é—´ä¾èµ–å’Œå…¨å±€åºåˆ—ä¸Šä¸‹æ–‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨Isharahæ•°æ®é›†ä¸Šå–å¾—äº†5.60%å’Œ12.01%çš„è¯é”™è¯¯ç‡(Word Error Rate, WER)ï¼Œåœ¨MSLR 2025æŒ‘æˆ˜èµ›ä¸­ä½åˆ—ç¬¬ä¸‰åã€‚è¿™é¡¹ç ”ç©¶æˆåŠŸéªŒè¯äº†å°†è¯­éŸ³è¯†åˆ«é¢†åŸŸçš„æ¶æ„è¿ç§»è‡³åŸºäºå…³é”®ç‚¹çš„CSLRé¢†åŸŸçš„æœ‰æ•ˆæ€§ï¼Œä¸ºè§£å†³æ‰‹è¯­è¯†åˆ«çš„ç©ºæ—¶åŠ¨åŠ›å­¦é—®é¢˜æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01791v1",
      "published_date": "2025-08-03 14:58:50 UTC",
      "updated_date": "2025-08-03 14:58:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:31:19.013491+00:00"
    },
    {
      "arxiv_id": "2508.01784v1",
      "title": "RouteMark: A Fingerprint for Intellectual Property Attribution in Routing-based Model Merging",
      "title_zh": "RouteMarkï¼šåŸºäºè·¯ç”±çš„æ¨¡å‹åˆå¹¶ä¸­çŸ¥è¯†äº§æƒå½’å±è¯†åˆ«çš„æŒ‡çº¹",
      "authors": [
        "Xin He",
        "Junxi Shen",
        "Zhenheng Tang",
        "Xiaowen Chu",
        "Bo Li",
        "Ivor W. Tsang",
        "Yew-Soon Ong"
      ],
      "abstract": "Model merging via Mixture-of-Experts (MoE) has emerged as a scalable solution for consolidating multiple task-specific models into a unified sparse architecture, where each expert is derived from a model fine-tuned on a distinct task. While effective for multi-task integration, this paradigm introduces a critical yet underexplored challenge: how to attribute and protect the intellectual property (IP) of individual experts after merging. We propose RouteMark, a framework for IP protection in merged MoE models through the design of expert routing fingerprints. Our key insight is that task-specific experts exhibit stable and distinctive routing behaviors under probing inputs. To capture these patterns, we construct expert-level fingerprints using two complementary statistics: the Routing Score Fingerprint (RSF), quantifying the intensity of expert activation, and the Routing Preference Fingerprint (RPF), characterizing the input distribution that preferentially activates each expert. These fingerprints are reproducible, task-discriminative, and lightweight to construct. For attribution and tampering detection, we introduce a similarity-based matching algorithm that compares expert fingerprints between a suspect and a reference (victim) model. Extensive experiments across diverse tasks and CLIP-based MoE architectures show that RouteMark consistently yields high similarity for reused experts and clear separation from unrelated ones. Moreover, it remains robust against both structural tampering (expert replacement, addition, deletion) and parametric tampering (fine-tuning, pruning, permutation), outperforming weight- and activation-based baseliness. Our work lays the foundation for RouteMark as a practical and broadly applicable framework for IP verification in MoE-based model merging.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RouteMark æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŸºäºæ··åˆä¸“å®¶æ¨¡å‹ (Mixture-of-Experts, MoE) çš„æ¨¡å‹åˆå¹¶ä¸­ï¼Œå•ä¸ªä¸“å®¶çš„çŸ¥è¯†äº§æƒ (Intellectual Property, IP) å½’å±ä¸ä¿æŠ¤é—®é¢˜ã€‚ä½œè€…åˆ©ç”¨ä»»åŠ¡ç‰¹å®šä¸“å®¶åœ¨æ¢æµ‹è¾“å…¥ä¸‹è¡¨ç°å‡ºçš„ç¨³å®šä¸”ç‹¬ç‰¹çš„è·¯ç”±è¡Œä¸ºï¼Œæ„å»ºäº†ä¸“å®¶çº§æŒ‡çº¹ã€‚è¯¥æŒ‡çº¹åŒ…å«ä¸¤ç§ç»Ÿè®¡æŒ‡æ ‡ï¼šç”¨äºè¡¡é‡ä¸“å®¶æ¿€æ´»å¼ºåº¦çš„è·¯ç”±è¯„åˆ†æŒ‡çº¹ (Routing Score Fingerprint, RSF) ä»¥åŠåˆ»ç”»åå¥½è¾“å…¥åˆ†å¸ƒçš„è·¯ç”±åå¥½æŒ‡çº¹ (Routing Preference Fingerprint, RPF)ã€‚é€šè¿‡åŸºäºç›¸ä¼¼åº¦çš„åŒ¹é…ç®—æ³•ï¼ŒRouteMark èƒ½å¤Ÿæœ‰æ•ˆå¯¹æ¯”å«Œç–‘æ¨¡å‹ä¸å—å®³æ¨¡å‹çš„ä¸“å®¶æŒ‡çº¹ï¼Œå®ç°ç²¾å‡†çš„å½’å±é‰´å®šä¸ç¯¡æ”¹æ£€æµ‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨å¤šç§ä»»åŠ¡å’ŒåŸºäº CLIP çš„ MoE æ¶æ„ä¸­å‡èƒ½å‡†ç¡®è¯†åˆ«é‡ç”¨ä¸“å®¶ï¼Œå¹¶å¯¹ä¸“å®¶æ›¿æ¢ã€å‰ªæã€å¾®è°ƒç­‰ç»“æ„ä¸å‚æ•°ç¯¡æ”¹è¡¨ç°å‡ºæå¼ºçš„é²æ£’æ€§ã€‚è¯¥å·¥ä½œä¸º MoE æ¨¡å‹åˆå¹¶åœºæ™¯ä¸‹çš„çŸ¥è¯†äº§æƒéªŒè¯æä¾›äº†ä¸€ä¸ªå®ç”¨ä¸”å…·æœ‰æ™®é€‚æ€§çš„æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "MoE, Model Merging, Fingerprint",
      "pdf_url": "https://arxiv.org/pdf/2508.01784v1",
      "published_date": "2025-08-03 14:51:58 UTC",
      "updated_date": "2025-08-03 14:51:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:31:19.615598+00:00"
    },
    {
      "arxiv_id": "2508.01781v1",
      "title": "A comprehensive taxonomy of hallucinations in Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹å¹»è§‰å…¨é¢åˆ†ç±»ä½“ç³»",
      "authors": [
        "Manuel Cossio"
      ],
      "abstract": "Large language models (LLMs) have revolutionized natural language processing, yet their propensity for hallucination, generating plausible but factually incorrect or fabricated content, remains a critical challenge. This report provides a comprehensive taxonomy of LLM hallucinations, beginning with a formal definition and a theoretical framework that posits its inherent inevitability in computable LLMs, irrespective of architecture or training. It explores core distinctions, differentiating between intrinsic (contradicting input context) and extrinsic (inconsistent with training data or reality), as well as factuality (absolute correctness) and faithfulness (adherence to input). The report then details specific manifestations, including factual errors, contextual and logical inconsistencies, temporal disorientation, ethical violations, and task-specific hallucinations across domains like code generation and multimodal applications. It analyzes the underlying causes, categorizing them into data-related issues, model-related factors, and prompt-related influences. Furthermore, the report examines cognitive and human factors influencing hallucination perception, surveys evaluation benchmarks and metrics for detection, and outlines architectural and systemic mitigation strategies. Finally, it introduces web-based resources for monitoring LLM releases and performance. This report underscores the complex, multifaceted nature of LLM hallucinations and emphasizes that, given their theoretical inevitability, future efforts must focus on robust detection, mitigation, and continuous human oversight for responsible and reliable deployment in critical applications.",
      "tldr_zh": "è¯¥æŠ¥å‘Šå¯¹å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)ä¸­çš„å¹»è§‰(Hallucinations)é—®é¢˜è¿›è¡Œäº†å…¨é¢çš„åˆ†ç±»å’Œæ·±å…¥æ¢è®¨ã€‚ç ”ç©¶é¦–å…ˆç»™å‡ºäº†å¹»è§‰çš„æ­£å¼å®šä¹‰ï¼Œå¹¶ä»ç†è®ºå±‚é¢è®ºè¯äº†å¹»è§‰åœ¨å¯è®¡ç®—LLMsä¸­å…·æœ‰å›ºæœ‰çš„ä¸å¯é¿å…æ€§ã€‚æŠ¥å‘Šè¯¦ç»†åŒºåˆ†äº†å†…åœ¨å¹»è§‰(Intrinsic)ä¸å¤–åœ¨å¹»è§‰(Extrinsic)ï¼Œä»¥åŠäº‹å®æ€§(Factuality)ä¸å¿ å®æ€§(Faithfulness)ç­‰æ ¸å¿ƒæ¦‚å¿µã€‚æ­¤å¤–ï¼Œæ–‡ä¸­åˆ†æäº†å¹»è§‰åœ¨ä»£ç ç”Ÿæˆã€å¤šæ¨¡æ€ç­‰ç‰¹å®šé¢†åŸŸçš„å…·ä½“è¡¨ç°ï¼Œå¹¶å°†å…¶è¯±å› å½’çº³ä¸ºæ•°æ®ã€æ¨¡å‹åŠæç¤ºè¯(Prompt)ç­‰ç»´åº¦ã€‚é€šè¿‡ç»¼è¿°ç°æœ‰çš„è¯„ä¼°æŒ‡æ ‡ä¸æ¶æ„å±‚é¢çš„ç¼“è§£ç­–ç•¥ï¼Œè¯¥ç ”ç©¶ä¸ºç†è§£è¿™ä¸€å¤æ‚ç°è±¡æä¾›äº†ç³»ç»Ÿæ€§æ¡†æ¶ã€‚æœ€åï¼ŒæŠ¥å‘Šå¼ºè°ƒäº†åœ¨æœªæ¥éƒ¨ç½²ä¸­å¿…é¡»ä¾èµ–é²æ£’çš„æ£€æµ‹æœºåˆ¶å’ŒæŒç»­çš„äººå·¥ç›‘ç®¡ï¼Œä»¥ç¡®ä¿LLMsåœ¨å…³é”®åº”ç”¨åœºæ™¯ä¸‹çš„å¯é æ€§ä¸å®‰å…¨æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "55 pages, 16 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.01781v1",
      "published_date": "2025-08-03 14:37:16 UTC",
      "updated_date": "2025-08-03 14:37:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:31:28.603606+00:00"
    },
    {
      "arxiv_id": "2508.01780v1",
      "title": "LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?",
      "title_zh": "LiveMCPBenchï¼šæ™ºèƒ½ä½“èƒ½å¦é©¾é©­æµ·é‡ MCP å·¥å…·ï¼Ÿ",
      "authors": [
        "Guozhao Mo",
        "Wenliang Zhong",
        "Jiawei Chen",
        "Xuanang Chen",
        "Yaojie Lu",
        "Hongyu Lin",
        "Ben He",
        "Xianpei Han",
        "Le Sun"
      ],
      "abstract": "With the rapid development of Model Context Protocol (MCP), the number of MCP servers has surpassed 10,000. However, existing MCP benchmarks are limited to single-server settings with only a few tools, hindering effective evaluation of agent capabilities in large-scale, real-world scenarios. To address this limitation, we present LiveMCPBench, the first comprehensive benchmark comprising 95 real-world tasks grounded in the MCP ecosystem, designed to evaluate LLM agents at scale across diverse servers. To support a scalable and reproducible evaluation pipeline in large-scale MCP environments, we curate LiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and 527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework that enables automated and adaptive evaluation in dynamic, time-varying task environments, achieving 81% agreement with human reviewers. Finally, we propose the MCP Copilot Agent, a multi-step agent that routes tools for dynamic planning and executes tools for API interaction across the entire LiveMCPTool suite. Our evaluation covers 10 leading models, with the best-performing model (Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large performance variance across models, and several widely-used models perform poorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench offers the first unified framework for benchmarking LLM agents in realistic, tool-rich, and dynamic MCP environments, laying a solid foundation for scalable and reproducible research on agent capabilities. Our code and data will be publicly available at https://icip-cas.github.io/LiveMCPBench.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰Model Context Protocol (MCP) è¯„æµ‹åŸºå‡†å±€é™äºå•æœåŠ¡å™¨åŠå°‘é‡å·¥å…·ã€æ— æ³•è¯„ä¼°æ™ºèƒ½ä½“åœ¨å¤§è§„æ¨¡çœŸå®åœºæ™¯ä¸‹èƒ½åŠ›çš„é—®é¢˜ï¼Œæå‡ºäº†LiveMCPBenchã€‚ä½œä¸ºé¦–ä¸ªåŒ…å«95ä¸ªåŸºäºMCPç”Ÿæ€ç³»ç»ŸçœŸå®ä»»åŠ¡çš„ç»¼åˆè¯„æµ‹åŸºå‡†ï¼Œå®ƒæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“åœ¨è·¨å¤šæ ·åŒ–æœåŠ¡å™¨æ—¶çš„æ‰©å±•èƒ½åŠ›ã€‚ä¸ºæ”¯æŒå¤§è§„æ¨¡ç¯å¢ƒä¸‹çš„å¯å¤ç°è¯„ä¼°ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†åŒ…å«70ä¸ªMCPæœåŠ¡å™¨åŠ527ä¸ªå·¥å…·çš„LiveMCPToolï¼Œå¹¶å¼•å…¥äº†LLM-as-a-Judgeæ¡†æ¶LiveMCPEvalï¼Œå®ç°äº†ä¸äººç±»è¯„å®¡å‘˜ä¸€è‡´æ€§è¾¾81%çš„è‡ªåŠ¨åŒ–è‡ªé€‚åº”è¯„ä¼°ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºçš„MCP Copilot Agentèƒ½å¤Ÿè·¨å·¥å…·å¥—ä»¶è¿›è¡ŒåŠ¨æ€è§„åˆ’è·¯ç”±ä¸APIäº¤äº’æ‰§è¡Œã€‚å¯¹10ä¸ªé¢†å…ˆæ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¡¨ç°æœ€å¥½çš„Claude-Sonnet-4æˆåŠŸç‡è¾¾åˆ°78.95%ï¼Œä½†æ¨¡å‹é—´æ€§èƒ½å·®å¼‚æ˜¾è‘—ï¼Œè®¸å¤šå¸¸ç”¨æ¨¡å‹åœ¨å¤æ‚çš„å·¥å…·å¯†é›†å‹ç¯å¢ƒä¸­è¡¨ç°è¾ƒå·®ã€‚LiveMCPBenchä¸ºåœ¨çœŸå®ã€åŠ¨æ€ä¸”å·¥å…·ä¸°å¯Œçš„MCPç¯å¢ƒä¸‹è¡¡é‡LLMæ™ºèƒ½ä½“èƒ½åŠ›æä¾›äº†ç»Ÿä¸€æ¡†æ¶ï¼Œä¸ºå¯æ‰©å±•å’Œå¯å¤ç°çš„æ™ºèƒ½ä½“ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Our code and data will be publicly available at https://icip-cas.github.io/LiveMCPBench",
      "pdf_url": "https://arxiv.org/pdf/2508.01780v1",
      "published_date": "2025-08-03 14:36:42 UTC",
      "updated_date": "2025-08-03 14:36:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:31:30.511653+00:00"
    },
    {
      "arxiv_id": "2508.01774v2",
      "title": "VAGPO: Vision-augmented Asymmetric Group Preference Optimization for Graph Routing Problems",
      "title_zh": "VAGPOï¼šé¢å‘å›¾è·¯ç”±é—®é¢˜çš„è§†è§‰å¢å¼ºéå¯¹ç§°ç¾¤ä½“åå¥½ä¼˜åŒ–",
      "authors": [
        "Shiyan Liu",
        "Bohan Tan",
        "Zhiguang Cao",
        "Yan Jin"
      ],
      "abstract": "Graph routing problems play a vital role in web-related networks, where finding optimal paths across graphs is essential for efficient data transmission and content delivery. Classic routing formulations such as the Traveling Salesman Problem (TSP) and the Capacitated Vehicle Routing Problem (CVRP) represent fundamental graph optimization challenges. Recent data-driven optimization methods have made significant progress, yet they often face limitations in training efficiency and generalization to large-scale instances. In this paper, we propose a novel Vision-augmented Asymmetric Group Preference Optimization (VAGPO) approach. By leveraging ResNet-based visual encoding and Transformer-based sequential modeling, VAGPO captures both spatial structure and temporal dependencies. Furthermore, we introduce an asymmetric group preference optimization strategy that significantly accelerates convergence compared to commonly used policy gradient methods. Experimental results on generated TSP and CVRP instances, as well as real-world datasets, demonstrate that the proposed VAGPO approach achieves highly competitive solution quality. Additionally, VAGPO exhibits strong generalization to larger instances (up to 1000 nodes) without re-training, highlighting its effectiveness in both learning efficiency and scalability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†VAGPOï¼Œä¸€ç§ç”¨äºè§£å†³Traveling Salesman Problem (TSP)å’ŒCapacitated Vehicle Routing Problem (CVRP)ç­‰å›¾è·¯ç”±é—®é¢˜çš„è§†è§‰å¢å¼ºéå¯¹ç§°ç»„åå¥½ä¼˜åŒ–æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“åˆåŸºäºResNetçš„è§†è§‰ç¼–ç ä¸åŸºäºTransformerçš„åºåˆ—å»ºæ¨¡ï¼Œæœ‰æ•ˆæ•è·äº†å›¾æ•°æ®çš„ç©ºé—´ç»“æ„å’Œæ—¶é—´ä¾èµ–ã€‚ä¸ºäº†æå‡è®­ç»ƒæ•ˆç‡ï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ç§éå¯¹ç§°ç»„åå¥½ä¼˜åŒ–(Asymmetric Group Preference Optimization)ç­–ç•¥ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„ç­–ç•¥æ¢¯åº¦æ³•(policy gradient)æ˜¾è‘—åŠ å¿«äº†æ¨¡å‹æ”¶æ•›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVAGPOåœ¨åˆæˆå®ä¾‹å’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå‡å–å¾—äº†æå…·ç«äº‰åŠ›çš„è§£è´¨é‡ã€‚æ­¤å¤–ï¼ŒVAGPOè¡¨ç°å‡ºå“è¶Šçš„æ³›åŒ–æ€§èƒ½ï¼Œåœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹å³å¯æ‰©å±•å¤„ç†é«˜è¾¾1000ä¸ªèŠ‚ç‚¹çš„è§„æ¨¡ï¼Œè¯æ˜äº†å…¶åœ¨å­¦ä¹ æ•ˆç‡ä¸å¯æ‰©å±•æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01774v2",
      "published_date": "2025-08-03 14:19:12 UTC",
      "updated_date": "2025-10-10 09:04:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:31:36.702822+00:00"
    },
    {
      "arxiv_id": "2508.01773v1",
      "title": "Uncertainty-Based Methods for Automated Process Reward Data Construction and Output Aggregation in Mathematical Reasoning",
      "title_zh": "æ•°å­¦æ¨ç†ä¸­åŸºäºä¸ç¡®å®šæ€§çš„è‡ªåŠ¨åŒ–è¿‡ç¨‹å¥–åŠ±æ•°æ®æ„å»ºä¸è¾“å‡ºèšåˆæ–¹æ³•",
      "authors": [
        "Jiuzhou Han",
        "Wray Buntine",
        "Ehsan Shareghi"
      ],
      "abstract": "Large language models have demonstrated remarkable capabilities in complex mathematical reasoning tasks, but they inevitably generate errors throughout multi-step solutions. Process-level Reward Models (PRMs) have shown great promise by providing supervision and evaluation at each intermediate step, thereby effectively improving the models' reasoning abilities. However, training effective PRMs requires high-quality process reward data, yet existing methods for constructing such data are often labour-intensive or inefficient. In this paper, we propose an uncertainty-driven framework for automated process reward data construction, encompassing both data generation and annotation processes for PRMs. Additionally, we identify the limitations of both majority vote and PRMs, and introduce two generic uncertainty-aware output aggregation methods: Hybrid Majority Reward Vote and Weighted Reward Frequency Vote, which combine the strengths of majority vote with PRMs. Extensive experiments on ProcessBench, MATH, and GSMPlus show the effectiveness and efficiency of the proposed PRM data construction framework, and demonstrate that the two output aggregation methods further improve the mathematical reasoning abilities across diverse PRMs. The code and data will be publicly available at https://github.com/Jiuzhouh/UnPRM.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä¸­å› å¤šæ­¥è®¡ç®—å¯¼è‡´çš„é”™è¯¯é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºä¸ç¡®å®šæ€§é©±åŠ¨(uncertainty-driven)çš„è‡ªåŠ¨åŒ–è¿‡ç¨‹å¥–åŠ±æ•°æ®æ„å»ºæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿‡ç¨‹çº§å¥–åŠ±æ¨¡å‹(Process-level Reward Models, PRMs)è®­ç»ƒæ•°æ®è·å–æˆæœ¬é«˜ã€æ•ˆç‡ä½çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡è‡ªåŠ¨åŒ–æµç¨‹æ¶µç›–äº†PRMçš„æ•°æ®ç”Ÿæˆä¸æ ‡æ³¨ï¼Œå¹¶é’ˆå¯¹å¤šæ•°æŠ•ç¥¨(majority vote)ä¸PRMsçš„å±€é™æ€§ï¼Œå¼•å…¥äº†æ··åˆå¤šæ•°å¥–åŠ±æŠ•ç¥¨(Hybrid Majority Reward Vote)å’ŒåŠ æƒå¥–åŠ±é¢‘ç‡æŠ•ç¥¨(Weighted Reward Frequency Vote)ä¸¤ç§ä¸ç¡®å®šæ€§æ„ŸçŸ¥è¾“å‡ºèšåˆæ–¹æ³•ã€‚å®éªŒåœ¨ProcessBenchã€MATHå’ŒGSMPlusç­‰æ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ•°æ®æ„å»ºæ¡†æ¶çš„æœ‰æ•ˆæ€§ä¸æ•ˆç‡ï¼Œè¯æ˜äº†å…¶åœ¨å¤šç§PRMsä¸Šå‡èƒ½æ˜¾è‘—æå‡æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡ç»“åˆå¤šæ•°æŠ•ç¥¨ä¸å¥–åŠ±æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œä¸ºå®ç°æ›´ç²¾å‡†ã€é«˜æ•ˆçš„æ•°å­¦æ¨ç†è¯„ä¼°æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01773v1",
      "published_date": "2025-08-03 14:14:13 UTC",
      "updated_date": "2025-08-03 14:14:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:31:32.909736+00:00"
    },
    {
      "arxiv_id": "2508.01772v3",
      "title": "LoRA-based methods on Unet for transfer learning in Subarachnoid Hematoma Segmentation",
      "title_zh": "ç”¨äºè››ç½‘è†œä¸‹è…”è¡€è‚¿åˆ†å‰²çš„åŸºäº LoRA çš„ Unet è¿ç§»å­¦ä¹ æ–¹æ³•",
      "authors": [
        "Cristian Minoccheri",
        "Matthew Hodgman",
        "Haoyuan Ma",
        "Rameez Merchant",
        "Emily Wittrup",
        "Craig Williamson",
        "Kayvan Najarian"
      ],
      "abstract": "Aneurysmal subarachnoid hemorrhage (SAH) is a life-threatening neurological emergency with mortality rates exceeding 30%. Transfer learning from related hematoma types represents a potentially valuable but underexplored approach. Although Unet architectures remain the gold standard for medical image segmentation due to their effectiveness on limited datasets, Low-Rank Adaptation (LoRA) methods for parameter-efficient transfer learning have been rarely applied to convolutional neural networks in medical imaging contexts. We implemented a Unet architecture pre-trained on computed tomography scans from 124 traumatic brain injury patients across multiple institutions, then fine-tuned on 30 aneurysmal SAH patients from the University of Michigan Health System using 3-fold cross-validation. We developed a novel CP-LoRA method based on tensor CP-decomposition and introduced DoRA variants (DoRA-C, convDoRA, CP-DoRA) that decompose weight matrices into magnitude and directional components. We compared these approaches against existing LoRA methods (LoRA-C, convLoRA) and standard fine-tuning strategies across different modules on a multi-view Unet model. LoRA-based methods consistently outperformed standard Unet fine-tuning. Performance varied by hemorrhage volume, with all methods showing improved accuracy for larger volumes. CP-LoRA achieved comparable performance to existing methods while using significantly fewer parameters. Over-parameterization with higher ranks consistently yielded better performance than strictly low-rank adaptations. This study demonstrates that transfer learning between hematoma types is feasible and that LoRA-based methods significantly outperform conventional Unet fine-tuning for aneurysmal SAH segmentation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŠ¨è„‰ç˜¤æ€§è››ç½‘è†œä¸‹è…”å‡ºè¡€(SAH)åˆ†å‰²ä»»åŠ¡ï¼Œæ¢ç´¢äº†ä»ç›¸å…³è¡€è‚¿ç±»å‹è¿›è¡Œè¿ç§»å­¦ä¹ (Transfer learning)çš„å¯è¡Œæ€§ï¼Œæ—¨åœ¨è§£å†³åŒ»ç–—å½±åƒä¸­å·ç§¯ç¥ç»ç½‘ç»œ(CNN)åœ¨å‚æ•°é«˜æ•ˆè¿ç§»å­¦ä¹ åº”ç”¨è¾ƒå°‘çš„é—®é¢˜ã€‚ç ”ç©¶äººå‘˜åœ¨åˆ›ä¼¤æ€§è„‘æŸä¼¤(TBI)æ•°æ®é¢„è®­ç»ƒçš„Unetæ¶æ„åŸºç¡€ä¸Šï¼Œå¼€å‘äº†åŸºäºå¼ é‡CPåˆ†è§£(CP-decomposition)çš„æ–°å‹CP-LoRAæ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†DoRA-Cã€convDoRAåŠCP-DoRAç­‰å˜ä½“ã€‚å®éªŒé€šè¿‡å¯¹30åSAHæ‚£è€…æ•°æ®çš„ä¸‰æŠ˜äº¤å‰éªŒè¯ï¼Œå¯¹æ¯”äº†å¤šç§LoRAæ–¹æ³•ä¸æ ‡å‡†å¾®è°ƒ(Fine-tuning)ç­–ç•¥åœ¨å¤šè§†å›¾Unetæ¨¡å‹ä¸Šçš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼ŒåŸºäºLoRAçš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šä¸€è‡´ä¼˜äºæ ‡å‡†çš„Unetå¾®è°ƒï¼Œå°¤å…¶åœ¨å¤„ç†è¾ƒå¤§è¡€è‚¿ä½“ç§¯æ—¶å‡†ç¡®æ€§æ˜¾è‘—æå‡ã€‚å…¶ä¸­CP-LoRAåœ¨å¤§å¹…å‡å°‘å‚æ•°é‡çš„åŒæ—¶ï¼Œå®ç°äº†ä¸å…¶ä»–æ–¹æ³•ç›¸å½“çš„æ€§èƒ½è¡¨ç°ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°è¾ƒé«˜ç§©(Rank)çš„è¿‡åº¦å‚æ•°åŒ–æ¯”ä¸¥æ ¼çš„ä½ç§©é€‚é…æ›´å…·ä¼˜åŠ¿ã€‚è¯¥ç ”ç©¶è¯å®äº†è·¨è¡€è‚¿ç±»å‹è¿ç§»å­¦ä¹ çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†LoRAç±»æ–¹æ³•åœ¨åŠ¨è„‰ç˜¤æ€§SAHåˆ†å‰²ä¸­å…·æœ‰æ˜¾è‘—çš„ä¸´åºŠåº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01772v3",
      "published_date": "2025-08-03 14:12:42 UTC",
      "updated_date": "2025-11-25 17:27:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:31:54.625990+00:00"
    },
    {
      "arxiv_id": "2508.01763v1",
      "title": "Reasoning Systems as Structured Processes: Foundations, Failures, and Formal Criteria",
      "title_zh": "ä½œä¸ºç»“æ„åŒ–è¿‡ç¨‹çš„æ¨ç†ç³»ç»Ÿï¼šç†è®ºåŸºç¡€ã€å¤±æ•ˆåˆ†æä¸å½¢å¼åŒ–å‡†åˆ™",
      "authors": [
        "Saleh Nikooroo",
        "Thomas Engel"
      ],
      "abstract": "This paper outlines a general formal framework for reasoning systems, intended to support future analysis of inference architectures across domains. We model reasoning systems as structured tuples comprising phenomena, explanation space, inference and generation maps, and a principle base. The formulation accommodates logical, algorithmic, and learning-based reasoning processes within a unified structural schema, while remaining agnostic to any specific reasoning algorithm or logic system. We survey basic internal criteria--including coherence, soundness, and completeness-and catalog typical failure modes such as contradiction, incompleteness, and non-convergence. The framework also admits dynamic behaviors like iterative refinement and principle evolution. The goal of this work is to establish a foundational structure for representing and comparing reasoning systems, particularly in contexts where internal failure, adaptation, or fragmentation may arise. No specific solution architecture is proposed; instead, we aim to support future theoretical and practical investigations into reasoning under structural constraint.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å…³äºæ¨ç†ç³»ç»Ÿ (reasoning systems) çš„é€šç”¨å½¢å¼åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºè·¨é¢†åŸŸçš„æ¨ç†æ¶æ„åˆ†ææä¾›åŸºç¡€ã€‚è¯¥æ¡†æ¶å°†æ¨ç†ç³»ç»Ÿå»ºæ¨¡ä¸ºåŒ…å«ç°è±¡ (phenomena)ã€è§£é‡Šç©ºé—´ (explanation space)ã€æ¨ç†ä¸ç”Ÿæˆæ˜ å°„ (inference and generation maps) ä»¥åŠåŸåˆ™åº“ (principle base) çš„ç»“æ„åŒ–å…ƒç»„ã€‚è¿™ç§å½¢å¼åŒ–æ–¹æ³•èƒ½å¤Ÿåœ¨ç»Ÿä¸€çš„ç»“æ„æ¨¡å¼ä¸‹å®¹çº³é€»è¾‘ã€ç®—æ³•å’ŒåŸºäºå­¦ä¹ çš„æ¨ç†è¿‡ç¨‹ï¼ŒåŒæ—¶å¯¹å…·ä½“çš„æ¨ç†ç®—æ³•æˆ–é€»è¾‘ç³»ç»Ÿä¿æŒä¸­ç«‹ã€‚æ–‡ä¸­å¯¹è¿è´¯æ€§ (coherence)ã€å¯é æ€§ (soundness) å’Œå®Œå¤‡æ€§ (completeness) ç­‰åŸºæœ¬å†…éƒ¨å‡†åˆ™è¿›è¡Œäº†è€ƒå¯Ÿï¼Œå¹¶å½’çº³äº†çŸ›ç›¾ (contradiction)ã€ä¸å®Œå¤‡æ€§ (incompleteness) å’Œéæ”¶æ•›æ€§ (non-convergence) ç­‰å…¸å‹å¤±æ•ˆæ¨¡å¼ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜æ”¯æŒè¿­ä»£ä¼˜åŒ– (iterative refinement) å’ŒåŸåˆ™æ¼”åŒ– (principle evolution) ç­‰åŠ¨æ€è¡Œä¸ºã€‚ç ”ç©¶çš„ç›®æ ‡æ˜¯å»ºç«‹ä¸€ä¸ªåŸºç¡€ç»“æ„ï¼Œç”¨äºè¡¨ç¤ºå’Œæ¯”è¾ƒæ¨ç†ç³»ç»Ÿï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠå†…éƒ¨å¤±æ•ˆã€é€‚åº”æˆ–ç³»ç»Ÿç¢ç‰‡åŒ–çš„èƒŒæ™¯ä¸‹ï¼Œä¸ºæœªæ¥çš„ç†è®ºå’Œå®è·µç ”ç©¶æä¾›ç»“æ„åŒ–åˆ†æå·¥å…·ã€‚",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01763v1",
      "published_date": "2025-08-03 14:04:15 UTC",
      "updated_date": "2025-08-03 14:04:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:31:54.808546+00:00"
    },
    {
      "arxiv_id": "2508.01761v1",
      "title": "Semantically-Guided Inference for Conditional Diffusion Models: Enhancing Covariate Consistency in Time Series Forecasting",
      "title_zh": "æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„è¯­ä¹‰å¼•å¯¼æ¨ç†ï¼šå¢å¼ºæ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„åå˜é‡ä¸€è‡´æ€§",
      "authors": [
        "Rui Ding",
        "Hanyang Meng",
        "Zeyang Zhang",
        "Jielong Yang"
      ],
      "abstract": "Diffusion models have demonstrated strong performance in time series forecasting, yet often suffer from semantic misalignment between generated trajectories and conditioning covariates, especially under complex or multimodal conditions. To address this issue, we propose SemGuide, a plug-and-play, inference-time method that enhances covariate consistency in conditional diffusion models. Our approach introduces a scoring network to assess the semantic alignment between intermediate diffusion states and future covariates. These scores serve as proxy likelihoods in a stepwise importance reweighting procedure, which progressively adjusts the sampling path without altering the original training process. The method is model-agnostic and compatible with any conditional diffusion framework. Experiments on real-world forecasting tasks show consistent gains in both predictive accuracy and covariate alignment, with especially strong performance under complex conditioning scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¡ä»¶æ‰©æ•£æ¨¡å‹(Conditional Diffusion Models)åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­å­˜åœ¨çš„ç”Ÿæˆè½¨è¿¹ä¸åå˜é‡(Covariates)è¯­ä¹‰ä¸ä¸€è‡´é—®é¢˜ï¼Œæå‡ºäº†SemGuideæ–¹æ³•ã€‚SemGuideä½œä¸ºä¸€ç§å³æ’å³ç”¨çš„æ¨ç†é˜¶æ®µ(Inference-time)ä¼˜åŒ–æ–¹æ¡ˆï¼Œæ—¨åœ¨å¢å¼ºåå˜é‡ä¸€è‡´æ€§(Covariate Consistency)è€Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥ä¸€ä¸ªè¯„åˆ†ç½‘ç»œæ¥è¯„ä¼°ä¸­é—´æ‰©æ•£çŠ¶æ€ä¸æœªæ¥åå˜é‡çš„è¯­ä¹‰å¯¹é½ç¨‹åº¦ï¼Œå¹¶åˆ©ç”¨è¿™äº›è¯„åˆ†è¿›è¡Œé€æ­¥é‡è¦æ€§é‡åŠ æƒ(Stepwise Importance Reweighting)ä»¥ä¼˜åŒ–é‡‡æ ·è·¯å¾„ã€‚ç”±äºå…¶æ¨¡å‹æ— å…³(Model-agnostic)çš„ç‰¹æ€§ï¼ŒSemGuideèƒ½å¤Ÿæ— ç¼é›†æˆåˆ°å„ç§ç°æœ‰çš„æ¡ä»¶æ‰©æ•£æ¡†æ¶ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®é¢„æµ‹ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†é¢„æµ‹å‡†ç¡®æ€§å’Œè¯­ä¹‰å¯¹é½æ•ˆæœï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚æˆ–å¤šæ¨¡æ€æ¡ä»¶æ—¶å±•ç°å‡ºæ›´å¼ºçš„æ€§èƒ½ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01761v1",
      "published_date": "2025-08-03 14:04:04 UTC",
      "updated_date": "2025-08-03 14:04:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:32:14.190024+00:00"
    },
    {
      "arxiv_id": "2508.01752v1",
      "title": "Vision transformer-based multi-camera multi-object tracking framework for dairy cow monitoring",
      "title_zh": "åŸºäº Vision Transformer çš„å¥¶ç‰›ç›‘æµ‹å¤šæ‘„åƒå¤´å¤šç›®æ ‡è·Ÿè¸ªæ¡†æ¶",
      "authors": [
        "Kumail Abbas",
        "Zeeshan Afzal",
        "Aqeel Raza",
        "Taha Mansouri",
        "Andrew W. Dowsey",
        "Chaidate Inchaisri",
        "Ali Alameer"
      ],
      "abstract": "Activity and behaviour correlate with dairy cow health and welfare, making continual and accurate monitoring crucial for disease identification and farm productivity. Manual observation and frequent assessments are laborious and inconsistent for activity monitoring. In this study, we developed a unique multi-camera, real-time tracking system for indoor-housed Holstein Friesian dairy cows. This technology uses cutting-edge computer vision techniques, including instance segmentation and tracking algorithms to monitor cow activity seamlessly and accurately. An integrated top-down barn panorama was created by geometrically aligning six camera feeds using homographic transformations. The detection phase used a refined YOLO11-m model trained on an overhead cow dataset, obtaining high accuracy (mAP\\@0.50 = 0.97, F1 = 0.95). SAMURAI, an upgraded Segment Anything Model 2.1, generated pixel-precise cow masks for instance segmentation utilizing zero-shot learning and motion-aware memory. Even with occlusion and fluctuating posture, a motion-aware Linear Kalman filter and IoU-based data association reliably identified cows over time for object tracking. The proposed system significantly outperformed Deep SORT Realtime. Multi-Object Tracking Accuracy (MOTA) was 98.7% and 99.3% in two benchmark video sequences, with IDF1 scores above 99% and near-zero identity switches. This unified multi-camera system can track dairy cows in complex interior surroundings in real time, according to our data. The system reduces redundant detections across overlapping cameras, maintains continuity as cows move between viewpoints, with the aim of improving early sickness prediction through activity quantification and behavioural classification.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ç§åŸºäº Vision transformer çš„å¤šæ‘„åƒå¤´å¤šç›®æ ‡è¿½è¸ªæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å®¤å†…å¥¶ç‰›çš„å®æ—¶ã€ç²¾å‡†ç›‘æ§ã€‚ç ”ç©¶é€šè¿‡å•åº”æ€§å˜æ¢(homographic transformations)å°†å…­è·¯è§†é¢‘æµåˆæˆä¸ºé¡¶è§†å…¨æ™¯å›¾ï¼Œå¹¶é‡‡ç”¨æ”¹è¿›çš„ YOLO11-m æ¨¡å‹è¿›è¡Œå¥¶ç‰›æ£€æµ‹ï¼Œå–å¾—äº† mAP@0.50 ä¸º 0.97 çš„ä¼˜å¼‚è¡¨ç°ã€‚ç³»ç»Ÿé›†æˆäº†å‡çº§ç‰ˆçš„ Segment Anything Model 2.1 (SAMURAI)ï¼Œåˆ©ç”¨é›¶æ ·æœ¬å­¦ä¹ ç”Ÿæˆåƒç´ çº§æ©ç ï¼Œå¹¶ç»“åˆè¿åŠ¨æ„ŸçŸ¥çº¿æ€§å¡å°”æ›¼æ»¤æ³¢(Linear Kalman filter)ä¸åŸºäº IoU çš„æ•°æ®å…³è”æŠ€æœ¯ï¼Œæœ‰æ•ˆåº”å¯¹äº†é®æŒ¡å’Œå§¿æ€æ³¢åŠ¨å¸¦æ¥çš„è¿½è¸ªæŒ‘æˆ˜ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿçš„å¤šç›®æ ‡è¿½è¸ªå‡†ç¡®åº¦(MOTA)æœ€é«˜è¾¾ 99.3%ï¼Œä¸”èº«ä»½åˆ‡æ¢(identity switches)æ¥è¿‘äºé›¶ï¼Œæ€§èƒ½æ˜¾è‘—è¶…è¶Šäº† Deep SORT Realtimeã€‚è¿™ä¸€ç»Ÿä¸€çš„å¤šæ‘„åƒå¤´ç³»ç»Ÿè§£å†³äº†é‡å è§†è§’çš„å†—ä½™æ£€æµ‹é—®é¢˜å¹¶ä¿æŒäº†è·¨è§†è§’çš„è¿½è¸ªè¿ç»­æ€§ï¼Œä¸ºé€šè¿‡è‡ªåŠ¨åŒ–è¡Œä¸ºåˆ†ç±»è¿›è¡Œå¥¶ç‰›å¥åº·é¢„è­¦å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Submitted in Smart Agriculture Technology",
      "pdf_url": "https://arxiv.org/pdf/2508.01752v1",
      "published_date": "2025-08-03 13:36:40 UTC",
      "updated_date": "2025-08-03 13:36:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:32:04.995614+00:00"
    },
    {
      "arxiv_id": "2508.01751v2",
      "title": "Implementing Cumulative Functions with Generalized Cumulative Constraints",
      "title_zh": "åŸºäºå¹¿ä¹‰ç´¯ç§¯çº¦æŸçš„ç´¯ç§¯å‡½æ•°å®ç°",
      "authors": [
        "Pierre Schaus",
        "Charles Thomas",
        "Roger Kameugne"
      ],
      "abstract": "Modeling scheduling problems with conditional time intervals and cumulative functions has become a common approach when using modern commercial constraint programming solvers. This paradigm enables the modeling of a wide range of scheduling problems, including those involving producers and consumers. However, it is unavailable in existing open-source solvers and practical implementation details remain undocumented. In this work, we present an implementation of this modeling approach using a single, generic global constraint called the Generalized Cumulative. We also introduce a novel time-table filtering algorithm specifically designed to handle tasks defined on conditional time-intervals. Experimental results demonstrate that this approach, combined with the new filtering algorithm, performs competitively with existing solvers enabling the modeling of producer and consumer scheduling problems and effectively scales to large-scale problems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨è°ƒåº¦é—®é¢˜(scheduling problems)ä¸­åˆ©ç”¨æ¡ä»¶æ—¶é—´é—´éš”(conditional time intervals)å’Œç´¯ç§¯å‡½æ•°(cumulative functions)è¿›è¡Œå»ºæ¨¡çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¼€æºæ±‚è§£å™¨ç¼ºä¹æ­¤ç±»åŠŸèƒ½ä¸”å®ç°ç»†èŠ‚ä¸æ˜ç¡®çš„é—®é¢˜ã€‚ä½œè€…æå‡ºé€šè¿‡å•ä¸€çš„é€šç”¨å…¨å±€çº¦æŸ(Generalized Cumulative)æ¥å®ç°è¿™ä¸€å»ºæ¨¡èŒƒå¼ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§ä¸“é—¨é’ˆå¯¹æ¡ä»¶æ—¶é—´é—´éš”ä»»åŠ¡çš„æ–°å‹æ—¶é—´è¡¨è¿‡æ»¤ç®—æ³•(time-table filtering algorithm)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†ç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…è°ƒåº¦é—®é¢˜(producer and consumer scheduling problems)æ—¶è¡¨ç°å‡ºäº†æå…·ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œå¹¶èƒ½æœ‰æ•ˆæ‰©å±•è‡³å¤§è§„æ¨¡é—®é¢˜ã€‚è¯¥å·¥ä½œå¡«è¡¥äº†å¼€æºçº¦æŸç¼–ç¨‹(constraint programming)æ±‚è§£å™¨åœ¨è¿™ä¸€é¢†åŸŸçš„ç©ºç™½ï¼Œå¹¶ä¸ºå¤æ‚è°ƒåº¦åœºæ™¯çš„å®é™…åº”ç”¨æä¾›äº†è¯¦å°½çš„å®ç°å‚è€ƒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01751v2",
      "published_date": "2025-08-03 13:29:44 UTC",
      "updated_date": "2025-12-06 10:14:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:32:05.786798+00:00"
    },
    {
      "arxiv_id": "2508.01749v1",
      "title": "Improving Noise Efficiency in Privacy-preserving Dataset Distillation",
      "title_zh": "æå‡éšç§ä¿æŠ¤æ•°æ®é›†è’¸é¦ä¸­çš„å™ªå£°æ•ˆç‡",
      "authors": [
        "Runkai Zheng",
        "Vishnu Asutosh Dasu",
        "Yinong Oliver Wang",
        "Haohan Wang",
        "Fernando De la Torre"
      ],
      "abstract": "Modern machine learning models heavily rely on large datasets that often include sensitive and private information, raising serious privacy concerns. Differentially private (DP) data generation offers a solution by creating synthetic datasets that limit the leakage of private information within a predefined privacy budget; however, it requires a substantial amount of data to achieve performance comparable to models trained on the original data. To mitigate the significant expense incurred with synthetic data generation, Dataset Distillation (DD) stands out for its remarkable training and storage efficiency. This efficiency is particularly advantageous when integrated with DP mechanisms, curating compact yet informative synthetic datasets without compromising privacy. However, current state-of-the-art private DD methods suffer from a synchronized sampling-optimization process and the dependency on noisy training signals from randomly initialized networks. This results in the inefficient utilization of private information due to the addition of excessive noise. To address these issues, we introduce a novel framework that decouples sampling from optimization for better convergence and improves signal quality by mitigating the impact of DP noise through matching in an informative subspace. On CIFAR-10, our method achieves a \\textbf{10.0\\%} improvement with 50 images per class and \\textbf{8.3\\%} increase with just \\textbf{one-fifth} the distilled set size of previous state-of-the-art methods, demonstrating significant potential to advance privacy-preserving DD.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·®åˆ†éšç§(Differentially Private, DP)æ•°æ®é›†è’¸é¦(Dataset Distillation, DD)ä¸­å­˜åœ¨çš„å™ªå£°æ•ˆç‡ä½ä¸‹å’Œç§æœ‰ä¿¡æ¯åˆ©ç”¨ä¸å……åˆ†ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ—¨åœ¨æé«˜å™ªå£°æ•ˆç‡çš„æ–°å‹æ¡†æ¶ã€‚ç”±äºç°æœ‰æœ€å…ˆè¿›çš„ç§æœ‰DDæ–¹æ³•å—å›°äºåŒæ­¥çš„é‡‡æ ·-ä¼˜åŒ–è¿‡ç¨‹ä»¥åŠå¯¹éšæœºåˆå§‹åŒ–ç½‘ç»œäº§ç”Ÿçš„å™ªå£°ä¿¡å·çš„ä¾èµ–ï¼Œå¯¼è‡´å…¶åœ¨æœ‰é™éšç§é¢„ç®—ä¸‹éš¾ä»¥æœ‰æ•ˆæå–ä¿¡æ¯ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¯¥æ¡†æ¶å°†é‡‡æ ·ä¸ä¼˜åŒ–è§£è€¦ä»¥ä¿ƒè¿›æ›´å¥½çš„æ”¶æ•›ï¼Œå¹¶é€šè¿‡åœ¨ä¿¡æ¯ä¸°å¯Œçš„å­ç©ºé—´(informative subspace)ä¸­è¿›è¡ŒåŒ¹é…ï¼Œæœ‰æ•ˆç¼“è§£äº†DPå™ªå£°å¯¹ä¿¡å·è´¨é‡çš„å½±å“ã€‚åœ¨CIFAR-10ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¯ç±»50å¼ å›¾åƒæ—¶å‡†ç¡®ç‡æå‡äº†10.0%ï¼Œä¸”åœ¨è’¸é¦é›†è§„æ¨¡ä»…ä¸ºå…ˆå‰SOTAæ–¹æ³•äº”åˆ†ä¹‹ä¸€çš„æƒ…å†µä¸‹å®ç°äº†8.3%çš„å¢é•¿ã€‚è¿™ä¸€æˆæœå±•ç¤ºäº†è¯¥æ¡†æ¶åœ¨æ¨è¿›éšç§ä¿æŠ¤æ•°æ®é›†è’¸é¦é¢†åŸŸçš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºæ„å»ºé«˜æ•ˆä¸”éšç§å®‰å…¨çš„åˆæˆæ•°æ®é›†æä¾›äº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.01749v1",
      "published_date": "2025-08-03 13:15:52 UTC",
      "updated_date": "2025-08-03 13:15:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:32:10.088025+00:00"
    },
    {
      "arxiv_id": "2508.02753v4",
      "title": "DMSC: Dynamic Multi-Scale Coordination Framework for Time Series Forecasting",
      "title_zh": "DMSCï¼šé¢å‘æ—¶é—´åºåˆ—é¢„æµ‹çš„åŠ¨æ€å¤šå°ºåº¦ååŒæ¡†æ¶",
      "authors": [
        "Haonan Yang",
        "Jianchao Tang",
        "Zhuo Li",
        "Long Lan"
      ],
      "abstract": "Time Series Forecasting (TSF) faces persistent challenges in modeling intricate temporal dependencies across different scales. Despite recent advances leveraging different decomposition operations and novel architectures based on CNN, MLP or Transformer, existing methods still struggle with static decomposition strategies, fragmented dependency modeling, and inflexible fusion mechanisms, limiting their ability to model intricate temporal dependencies. To explicitly solve the mentioned three problems respectively, we propose a novel Dynamic Multi-Scale Coordination Framework (DMSC) with Multi-Scale Patch Decomposition block (EMPD), Triad Interaction Block (TIB) and Adaptive Scale Routing MoE block (ASR-MoE). Specifically, EMPD is designed as a built-in component to dynamically segment sequences into hierarchical patches with exponentially scaled granularities, eliminating predefined scale constraints through input-adaptive patch adjustment. TIB then jointly models intra-patch, inter-patch, and cross-variable dependencies within each layer's decomposed representations. EMPD and TIB are jointly integrated into layers forming a multi-layer progressive cascade architecture, where coarse-grained representations from earlier layers adaptively guide fine-grained feature extraction in subsequent layers via gated pathways. And ASR-MoE dynamically fuses multi-scale predictions by leveraging specialized global and local experts with temporal-aware weighting. Comprehensive experiments on thirteen real-world benchmarks demonstrate that DMSC consistently maintains state-of-the-art (SOTA) performance and superior computational efficiency for TSF tasks. Code is available at https://github.com/1327679995/DMSC.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DMSCï¼ˆDynamic Multi-Scale Coordination Frameworkï¼‰ï¼Œä¸€ç§ç”¨äºæ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆTime Series Forecastingï¼‰çš„åŠ¨æ€å¤šå°ºåº¦åä½œæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚æ—¶é—´ä¾èµ–å…³ç³»æ—¶é¢ä¸´çš„é™æ€åˆ†è§£ã€é›¶æ•£ä¾èµ–å»ºæ¨¡å’Œèåˆæœºåˆ¶åƒµåŒ–ç­‰é—®é¢˜ã€‚è¯¥æ¡†æ¶åŒ…å«EMPDï¼ˆExponential Multi-Scale Patch Decompositionï¼‰ç»„ä»¶ï¼Œèƒ½å°†åºåˆ—åŠ¨æ€åˆ†å‰²ä¸ºæŒ‡æ•°çº§ç²’åº¦çš„åˆ†å±‚è¡¥ä¸ï¼ˆPatchesï¼‰ï¼Œé€šè¿‡è‡ªé€‚åº”è°ƒæ•´æ¶ˆé™¤äº†é¢„å®šä¹‰å°ºåº¦çš„é™åˆ¶ã€‚TIBï¼ˆTriad Interaction Blockï¼‰åˆ™åœ¨åˆ†è§£è¡¨å¾ä¸­è”åˆå»ºæ¨¡å—å†…ã€å—é—´åŠè·¨å˜é‡çš„ä¾èµ–å…³ç³»ã€‚DMSCé‡‡ç”¨äº†å¤šå±‚æ¸è¿›çº§è”æ¶æ„ï¼Œåˆ©ç”¨é—¨æ§è·¯å¾„ä½¿æ—©æœŸå±‚çš„ç²—ç²’åº¦è¡¨å¾èƒ½å¤Ÿè‡ªé€‚åº”åœ°å¼•å¯¼åç»­å±‚çš„ç»†ç²’åº¦ç‰¹å¾æå–ã€‚æ­¤å¤–ï¼ŒASR-MoEï¼ˆAdaptive Scale Routing MoEï¼‰é€šè¿‡æ—¶é—´æ„ŸçŸ¥æƒé‡åè°ƒå…¨å±€ä¸å±€éƒ¨ä¸“å®¶ï¼Œå®ç°äº†å¤šå°ºåº¦é¢„æµ‹çš„åŠ¨æ€èåˆã€‚åœ¨13ä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜ï¼ŒDMSCåœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä»»åŠ¡ä¸­æŒç»­ä¿æŒSOTAæ€§èƒ½ï¼Œå¹¶è¡¨ç°å‡ºä¼˜è¶Šçš„è®¡ç®—æ•ˆç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.02753v4",
      "published_date": "2025-08-03 13:11:52 UTC",
      "updated_date": "2025-10-23 00:57:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:32:16.193745+00:00"
    },
    {
      "arxiv_id": "2508.01746v1",
      "title": "Bayes-Entropy Collaborative Driven Agents for Research Hypotheses Generation and Optimization",
      "title_zh": "è´å¶æ–¯-ç†µååŒé©±åŠ¨çš„ç ”ç©¶å‡è®¾ç”Ÿæˆä¸ä¼˜åŒ–æ™ºèƒ½ä½“",
      "authors": [
        "Shiyang Duan",
        "Yuan Tian",
        "Qi Bing",
        "Xiaowei Shao"
      ],
      "abstract": "The exponential growth of scientific knowledge has made the automated generation of scientific hypotheses that combine novelty, feasibility, and research value a core challenge. Existing methods based on large language models fail to systematically model the inherent in hypotheses or incorporate the closed-loop feedback mechanisms crucial for refinement. This paper proposes a multi-agent collaborative framework called HypoAgents, which for the first time integrates Bayesian reasoning with an information entropy-driven search mechanism across three stages-hypotheses generation, evidence validation, and hypotheses Refinement-to construct an iterative closed-loop simulating scientists' cognitive processes. Specifically, the framework first generates an initial set of hypotheses through diversity sampling and establishes prior beliefs based on a composite novelty-relevance-feasibility (N-R-F) score. It then employs etrieval-augmented generation (RAG) to gather external literature evidence, updating the posterior probabilities of hypotheses using Bayes' theorem. Finally, it identifies high-uncertainty hypotheses using information entropy $H = - \\sum {{p_i}\\log {p_i}}$ and actively refines them, guiding the iterative optimization of the hypothesis set toward higher quality and confidence. Experimental results on the ICLR 2025 conference real-world research question dataset (100 research questions) show that after 12 optimization iterations, the average ELO score of generated hypotheses improves by 116.3, surpassing the benchmark of real paper abstracts by 17.8, while the framework's overall uncertainty, as measured by Shannon entropy, decreases significantly by 0.92. This study presents an interpretable probabilistic reasoning framework for automated scientific discovery, substantially improving the quality and reliability of machine-generated research hypotheses.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HypoAgentsï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è‡ªåŠ¨åŒ–ç”Ÿæˆå…¼å…·åˆ›æ–°æ€§ã€å¯è¡Œæ€§å’Œç ”ç©¶ä»·å€¼çš„ç§‘å­¦å‡è®¾çš„å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–æ¬¡å°†è´å¶æ–¯æ¨ç†(Bayesian reasoning)ä¸ä¿¡æ¯ç†µ(Information Entropy)é©±åŠ¨çš„æœç´¢æœºåˆ¶ç›¸ç»“åˆï¼Œé€šè¿‡å‡è®¾ç”Ÿæˆã€è¯æ®éªŒè¯å’Œå‡è®¾ä¼˜åŒ–ä¸‰ä¸ªé˜¶æ®µæ„å»ºäº†æ¨¡æ‹Ÿç§‘å­¦å®¶è®¤çŸ¥è¿‡ç¨‹çš„è¿­ä»£é—­ç¯ã€‚ç³»ç»Ÿé¦–å…ˆé€šè¿‡å¤šæ ·æ€§é‡‡æ ·ç”Ÿæˆåˆå§‹å‡è®¾å¹¶å»ºç«‹åŸºäºåˆ›æ–°æ€§-ç›¸å…³æ€§-å¯è¡Œæ€§(N-R-F)è¯„åˆ†çš„å…ˆéªŒä¿¡å¿µï¼Œéšååˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)è·å–å¤–éƒ¨æ–‡çŒ®è¯æ®ï¼Œå¹¶ä¾æ®è´å¶æ–¯å®šç†æ›´æ–°å‡è®¾çš„åéªŒæ¦‚ç‡ã€‚æ¥ç€ï¼Œæ¡†æ¶åˆ©ç”¨ä¿¡æ¯ç†µè¯†åˆ«é«˜ä¸ç¡®å®šæ€§çš„å‡è®¾å¹¶è¿›è¡Œä¸»åŠ¨ç²¾ç‚¼ï¼Œå¼•å¯¼å‡è®¾é›†å‘æ›´é«˜è´¨é‡å’Œç½®ä¿¡åº¦æ¼”è¿›ã€‚åœ¨ICLR 2025çœŸå®ç§‘ç ”é—®é¢˜æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç»è¿‡12æ¬¡è¿­ä»£ä¼˜åŒ–ï¼Œç”Ÿæˆå‡è®¾çš„å¹³å‡ELOå¾—åˆ†æå‡äº†116.3ï¼Œè¶…è¿‡äº†çœŸå®è®ºæ–‡æ‘˜è¦æ°´å¹³ï¼ŒåŒæ—¶ç³»ç»Ÿæ•´ä½“çš„ä¸ç¡®å®šæ€§æ˜¾è‘—é™ä½äº†0.92ã€‚è¯¥ç ”ç©¶ä¸ºè‡ªåŠ¨åŒ–ç§‘å­¦å‘ç°æä¾›äº†ä¸€ä¸ªå¯è§£é‡Šçš„æ¦‚ç‡æ¨ç†æ¡†æ¶ï¼Œæ˜¾è‘—æå‡äº†æœºå™¨ç”Ÿæˆç ”ç©¶å‡è®¾çš„è´¨é‡ä¸å¯é æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Corresponding author: Xiaowei Shao. 12 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.01746v1",
      "published_date": "2025-08-03 13:05:32 UTC",
      "updated_date": "2025-08-03 13:05:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:32:21.694561+00:00"
    },
    {
      "arxiv_id": "2508.01742v2",
      "title": "Intention-Guided Cognitive Reasoning for Egocentric Long-Term Action Anticipation",
      "title_zh": "é¢å‘ç¬¬ä¸€è§†è§’é•¿æœŸåŠ¨ä½œé¢„æµ‹çš„æ„å›¾å¼•å¯¼è®¤çŸ¥æ¨ç†",
      "authors": [
        "Qiaohui Chu",
        "Haoyu Zhang",
        "Meng Liu",
        "Yisen Feng",
        "Haoxiang Shi",
        "Liqiang Nie"
      ],
      "abstract": "Long-term action anticipation from egocentric video is critical for applications such as human-computer interaction and assistive technologies, where anticipating user intent enables proactive and context-aware AI assistance. However, existing approaches suffer from three key limitations: 1) underutilization of fine-grained visual cues from hand-object interactions, 2) neglect of semantic dependencies between verbs and nouns, and 3) lack of explicit cognitive reasoning, limiting generalization and long-term forecasting ability. To overcome these challenges, we propose INSIGHT, a unified two-stage framework for egocentric action anticipation. In the first stage, INSIGHT focuses on extracting semantically rich features from hand-object interaction regions and enhances action representations using a verb-noun co-occurrence matrix. In the second stage, it introduces a reinforcement learning-based module that simulates explicit cognitive reasoning through a structured process: visual perception (think) -> intention inference (reason) -> action anticipation (answer). Extensive experiments on Ego4D, EPIC-Kitchens-55, and EGTEA Gaze+ benchmarks show that INSIGHT achieves state-of-the-art performance, demonstrating its effectiveness and strong generalization capability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¬¬ä¸€è§†è§’è§†é¢‘(egocentric video)ä¸­çš„é•¿æœŸåŠ¨ä½œé¢„æµ‹(Long-term action anticipation)ä»»åŠ¡ï¼Œæ—¨åœ¨é€šè¿‡æ„å›¾é¢„æµ‹å®ç°ä¸»åŠ¨å¼çš„äººæœºäº¤äº’ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨æ‰‹ç‰©äº¤äº’çº¿ç´¢åˆ©ç”¨ä¸è¶³ã€åŠ¨åè¯è¯­ä¹‰ä¾èµ–ç¼ºå¤±ä»¥åŠç¼ºä¹æ˜¾å¼è®¤çŸ¥æ¨ç†ç­‰å±€é™æ€§ï¼Œè®ºæ–‡æå‡ºäº†åä¸ºINSIGHTçš„ä¸¤é˜¶æ®µç»Ÿä¸€æ¡†æ¶ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼ŒINSIGHTé€šè¿‡æå–æ‰‹ç‰©äº¤äº’åŒºåŸŸçš„ç»†ç²’åº¦ç‰¹å¾ï¼Œå¹¶ç»“åˆåŠ¨åè¯å…±ç°çŸ©é˜µ(verb-noun co-occurrence matrix)æ¥å¢å¼ºåŠ¨ä½œè¡¨å¾ï¼›åœ¨ç¬¬äºŒé˜¶æ®µï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†åŸºäºå¼ºåŒ–å­¦ä¹ (reinforcement learning)çš„æ¨¡å—ï¼Œé€šè¿‡â€œè§†è§‰æ„ŸçŸ¥(think)â†’æ„å›¾æ¨æ–­(reason)â†’åŠ¨ä½œé¢„æµ‹(answer)â€çš„ç»“æ„åŒ–æµç¨‹æ¨¡æ‹Ÿäººç±»çš„æ˜¾å¼è®¤çŸ¥æ¨ç†è¿‡ç¨‹ã€‚åœ¨Ego4Dã€EPIC-Kitchens-55å’ŒEGTEA Gaze+ç­‰å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒINSIGHTè¾¾åˆ°äº†æœ€å…ˆè¿›(state-of-the-art)çš„æ€§èƒ½æ°´å¹³ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¤„ç†é•¿æœŸåŠ¨ä½œé¢„æµ‹ä»»åŠ¡æ—¶å…·æœ‰æå¼ºçš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by AAAI-2026",
      "pdf_url": "https://arxiv.org/pdf/2508.01742v2",
      "published_date": "2025-08-03 12:52:27 UTC",
      "updated_date": "2025-11-15 06:26:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:32:38.693502+00:00"
    },
    {
      "arxiv_id": "2508.01728v1",
      "title": "Granular Concept Circuits: Toward a Fine-Grained Circuit Discovery for Concept Representations",
      "title_zh": "ç²’åŒ–æ¦‚å¿µç”µè·¯ï¼šé¢å‘æ¦‚å¿µè¡¨ç¤ºçš„ç»†ç²’åº¦ç”µè·¯å‘ç°",
      "authors": [
        "Dahee Kwon",
        "Sehyun Lee",
        "Jaesik Choi"
      ],
      "abstract": "Deep vision models have achieved remarkable classification performance by leveraging a hierarchical architecture in which human-interpretable concepts emerge through the composition of individual neurons across layers. Given the distributed nature of representations, pinpointing where specific visual concepts are encoded within a model remains a crucial yet challenging task. In this paper, we introduce an effective circuit discovery method, called Granular Concept Circuit (GCC), in which each circuit represents a concept relevant to a given query. To construct each circuit, our method iteratively assesses inter-neuron connectivity, focusing on both functional dependencies and semantic alignment. By automatically discovering multiple circuits, each capturing specific concepts within that query, our approach offers a profound, concept-wise interpretation of models and is the first to identify circuits tied to specific visual concepts at a fine-grained level. We validate the versatility and effectiveness of GCCs across various deep image classification models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦è§†è§‰æ¨¡å‹ä¸­ç‰¹å®šè§†è§‰æ¦‚å¿µç¼–ç å®šä½å›°éš¾çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º Granular Concept Circuit (GCC) çš„ç”µè·¯å‘ç°æ–¹æ³•ã€‚GCC é€šè¿‡è¿­ä»£è¯„ä¼°ç¥ç»å…ƒé—´çš„è¿æ¥æ€§ï¼Œç»“åˆåŠŸèƒ½ä¾èµ–æ€§(functional dependencies)å’Œè¯­ä¹‰å¯¹é½(semantic alignment)ï¼Œä¸ºç»™å®šçš„æŸ¥è¯¢æ„å»ºä»£è¡¨ç‰¹å®šæ¦‚å¿µçš„ç”µè·¯ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿè‡ªåŠ¨å‘ç°å¤šä¸ªç”µè·¯å¹¶æ•æ‰å…¶ä¸­çš„ç»†ç²’åº¦æ¦‚å¿µï¼Œæä¾›äº†æ·±å…¥çš„ã€åŸºäºæ¦‚å¿µçš„æ¨¡å‹è§£é‡Šã€‚ä½œä¸ºé¦–ä¸ªåœ¨ç»†ç²’åº¦(fine-grained)å±‚é¢è¯†åˆ«ç‰¹å®šè§†è§‰æ¦‚å¿µç›¸å…³ç”µè·¯çš„æŠ€æœ¯ï¼Œç ”ç©¶äººå‘˜åœ¨å¤šç§æ·±åº¦å›¾åƒåˆ†ç±»æ¨¡å‹ä¸ŠéªŒè¯äº†å…¶é€šç”¨æ€§ä¸æœ‰æ•ˆæ€§ã€‚è¯¥æˆæœä¸ºæ¢ç´¢æ·±åº¦å­¦ä¹ æ¨¡å‹å†…éƒ¨æ¦‚å¿µè¡¨å¾çš„æ„æˆæä¾›äº†å…¨æ–°çš„è§†è§’å’Œå·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ICCV 2025 accepted paper",
      "pdf_url": "https://arxiv.org/pdf/2508.01728v1",
      "published_date": "2025-08-03 11:45:38 UTC",
      "updated_date": "2025-08-03 11:45:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:32:28.798060+00:00"
    },
    {
      "arxiv_id": "2508.10919v1",
      "title": "Human-AI collaboration or obedient and often clueless AI in instruct, serve, repeat dynamics?",
      "title_zh": "äººæœºåä½œï¼Œè¿˜æ˜¯â€œæŒ‡ä»¤-æœåŠ¡-é‡å¤â€åŠ¨æ€ä¸‹é¡ºä»ä¸”å¾€å¾€ç›²ä»çš„äººå·¥æ™ºèƒ½ï¼Ÿ",
      "authors": [
        "Mohammed Saqr",
        "Kamila Misiejuk",
        "Sonsoles LÃ³pez-Pernas"
      ],
      "abstract": "While research on human-AI collaboration exists, it mainly examined language learning and used traditional counting methods with little attention to evolution and dynamics of collaboration on cognitively demanding tasks. This study examines human-AI interactions while solving a complex problem. Student-AI interactions were qualitatively coded and analyzed with transition network analysis, sequence analysis and partial correlation networks as well as comparison of frequencies using chi-square and Person-residual shaded Mosaic plots to map interaction patterns, their evolution, and their relationship to problem complexity and student performance. Findings reveal a dominant Instructive pattern with interactions characterized by iterative ordering rather than collaborative negotiation. Oftentimes, students engaged in long threads that showed misalignment between their prompts and AI output that exemplified a lack of synergy that challenges the prevailing assumptions about LLMs as collaborative partners. We also found no significant correlations between assignment complexity, prompt length, and student grades suggesting a lack of cognitive depth, or effect of problem difficulty. Our study indicates that the current LLMs, optimized for instruction-following rather than cognitive partnership, compound their capability to act as cognitively stimulating or aligned collaborators. Implications for designing AI systems that prioritize cognitive alignment and collaboration are discussed.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººç±»ä¸äººå·¥æ™ºèƒ½ï¼ˆHuman-AI collaborationï¼‰åœ¨å¤„ç†é«˜è®¤çŸ¥éœ€æ±‚ä»»åŠ¡æ—¶çš„äº¤äº’æ¼”å˜å’Œåä½œåŠ¨æ€ã€‚ç ”ç©¶å›¢é˜Ÿé‡‡ç”¨å®šæ€§ç¼–ç ã€è½¬æ¢ç½‘ç»œåˆ†æï¼ˆtransition network analysisï¼‰ã€åºåˆ—åˆ†æï¼ˆsequence analysisï¼‰å’Œåç›¸å…³ç½‘ç»œï¼ˆpartial correlation networksï¼‰ç­‰å¤šç§æ–¹æ³•ï¼Œå¯¹å­¦ç”Ÿä¸ AI è§£å†³å¤æ‚é—®é¢˜æ—¶çš„äº¤äº’æ¨¡å¼è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œå½“å‰çš„äº¤äº’ä»¥â€œæŒ‡ä»¤-æœåŠ¡-é‡å¤â€çš„æŒ‡ä»¤å¼æ¨¡å¼ä¸ºä¸»ï¼Œè¡¨ç°ä¸ºè¿­ä»£è®¢è´­è€Œéåä½œåå•†ï¼ˆcollaborative negotiationï¼‰ã€‚ç ”ç©¶å‘ç°ç”¨æˆ·æç¤ºè¯ï¼ˆpromptsï¼‰ä¸ AI è¾“å‡ºä¹‹é—´ç»å¸¸å­˜åœ¨å¤±é…ï¼Œåæ˜ å‡ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸­ç¼ºä¹ååŒæ•ˆåº”ï¼ˆsynergyï¼‰ï¼ŒæŒ‘æˆ˜äº†å°†å…¶è§†ä¸ºç†æƒ³åä½œä¼™ä¼´çš„æ™®éå‡è®¾ã€‚æ•°æ®åˆ†æè¡¨æ˜ä»»åŠ¡å¤æ‚åº¦ã€æç¤ºè¯é•¿åº¦ä¸å­¦ç”Ÿæˆç»©ä¹‹é—´æ— æ˜¾è‘—ç›¸å…³æ€§ï¼Œæš—ç¤ºäº¤äº’è¿‡ç¨‹ç¼ºä¹è®¤çŸ¥æ·±åº¦ï¼ˆcognitive depthï¼‰ã€‚æœ€ç»ˆå¾—å‡ºç»“è®ºï¼Œå½“å‰çš„ LLMs æ›´å¤šè¢«ä¼˜åŒ–ä¸ºæŒ‡ä»¤éµå¾ªè€…è€Œéè®¤çŸ¥ä¼™ä¼´ï¼ˆcognitive partnershipï¼‰ï¼Œè¿™é¡¹ç ”ç©¶ä¸ºæœªæ¥è®¾è®¡å…·å¤‡è®¤çŸ¥å¯¹é½ï¼ˆcognitive alignmentï¼‰èƒ½åŠ›çš„ AI ç³»ç»Ÿæä¾›äº†é‡è¦å¯ç¤ºã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10919v1",
      "published_date": "2025-08-03 11:43:01 UTC",
      "updated_date": "2025-08-03 11:43:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:32:52.438547+00:00"
    },
    {
      "arxiv_id": "2508.01724v3",
      "title": "ReflecSched: Solving Dynamic Flexible Job-Shop Scheduling via LLM-Powered Hierarchical Reflection",
      "title_zh": "ReflecSchedï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹èµ‹èƒ½åˆ†å±‚åæ€çš„åŠ¨æ€æŸ”æ€§è½¦é—´ä½œä¸šè°ƒåº¦æ±‚è§£",
      "authors": [
        "Shijie Cao",
        "Yuan Yuan"
      ],
      "abstract": "The NP-hard Dynamic Flexible Job-Shop Scheduling (DFJSP) problem involves real-time events and complex routing. While traditional rules are efficient but rigid, deep learning is opaque and requires feature engineering. Large Language Models (LLMs) promise adaptive reasoning without this engineering overhead, yet we find their direct application is suboptimal. Baseline LLMs suffer from three key pitfalls: the long-context paradox, where crucial data is underutilized; an underutilization of expert heuristics; and myopic decision-making. To address this, we propose ReflecSched, a framework that empowers the LLM beyond a direct scheduler by equipping it with a strategic analysis capability. ReflecSched tasks the LLM to analyze heuristic-driven simulations across multiple planning horizons and distill them into a concise, natural-language summary termed Strategic Experience. This summary is then integrated into the prompt of a final decision-making module, guiding it to produce non-myopic actions. Experiments demonstrate ReflecSched achieves superior performance, with its best variants attaining an average RPD of 6.09% and rank of 4.39 on GEN-Bench, significantly outperforming strong traditional and learning-based methods including HMPSAC and IDDQN. It also statistically and decisively surpasses direct LLM baselines, securing a 71.35% Win Rate while being, on average, 15.1% more token-efficient on Normal-scale problems. Furthermore, cumulative runtime analysis reveals that ReflecSched's zero-shot nature eliminates the training bottleneck, providing a decisive efficiency advantage in high-variability manufacturing environments. Ablation studies attribute this performance to a robust reflection mechanism that leverages high-quality, contrastive experience. Ultimately, the framework's performance is statistically on par with an oracle-like strategy, showcasing its effectiveness and robustness.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŠ¨æ€æŸ”æ€§ä½œä¸šè½¦é—´è°ƒåº¦ï¼ˆDFJSPï¼‰è¿™ä¸€ NP éš¾é—®é¢˜æå‡ºäº† ReflecSched æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç›´æ¥è°ƒåº¦æ—¶é¢ä¸´çš„é•¿ä¸Šä¸‹æ–‡æ‚–è®ºï¼ˆlong-context paradoxï¼‰ã€ä¸“å®¶å¯å‘å¼åˆ©ç”¨ä¸è¶³åŠå†³ç­–çŸ­è§†ç­‰ç“¶é¢ˆã€‚ReflecSched å¹¶ä¸è®© LLM ç›´æ¥æ‰§è¡Œè°ƒåº¦ï¼Œè€Œæ˜¯èµ‹äºˆå…¶æˆ˜ç•¥åˆ†æèƒ½åŠ›ï¼Œé€šè¿‡åˆ†æè·¨å¤šä¸ªè§„åˆ’å‘¨æœŸçš„å¯å‘å¼é©±åŠ¨æ¨¡æ‹Ÿï¼Œå°†å…¶æç‚¼ä¸ºè‡ªç„¶è¯­è¨€å½¢å¼çš„â€œæˆ˜ç•¥ç»éªŒâ€ï¼ˆStrategic Experienceï¼‰ä»¥å¼•å¯¼å†³ç­–æ¨¡å—äº§ç”ŸéçŸ­è§†çš„è¡Œä¸ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒReflecSched åœ¨ GEN-Bench æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…¶æœ€ä½³å˜ä½“çš„å¹³å‡ RPD è¾¾åˆ° 6.09%ï¼Œæ˜¾è‘—è¶…è¶Šäº† HMPSAC å’Œ IDDQN ç­‰ä¼ ç»ŸåŠå­¦ä¹ å‹åŸºå‡†æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶çš„é›¶æ ·æœ¬ï¼ˆzero-shotï¼‰ç‰¹æ€§æœ‰æ•ˆæ¶ˆé™¤äº†è®­ç»ƒç“¶é¢ˆï¼Œåœ¨ä¿æŒé«˜ Token æ•ˆç‡çš„åŒæ—¶ï¼Œæ€§èƒ½åœ¨ç»Ÿè®¡ä¸Šå·²æ¥è¿‘ Oracle ç­–ç•¥æ°´å¹³ï¼Œè¯æ˜äº†å…¶åœ¨å¤šå˜åˆ¶é€ ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ä¸é²æ£’æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01724v3",
      "published_date": "2025-08-03 11:26:35 UTC",
      "updated_date": "2026-01-19 09:57:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:32:57.884154+00:00"
    },
    {
      "arxiv_id": "2508.01713v2",
      "title": "Dynamic Robot-Assisted Surgery with Hierarchical Class-Incremental Semantic Segmentation",
      "title_zh": "åŸºäºå±‚æ¬¡åŒ–ç±»å¢é‡è¯­ä¹‰åˆ†å‰²çš„åŠ¨æ€æœºå™¨äººè¾…åŠ©æ‰‹æœ¯",
      "authors": [
        "Julia Hindel",
        "Ema Mekic",
        "Enamundram Naga Karthik",
        "Rohit Mohan",
        "Daniele Cattaneo",
        "Maria Kalweit",
        "Abhinav Valada"
      ],
      "abstract": "Robot-assisted surgeries rely on accurate and real-time scene understanding to safely guide surgical instruments. However, segmentation models trained on static datasets face key limitations when deployed in these dynamic and evolving surgical environments. Class-incremental semantic segmentation (CISS) allows models to continually adapt to new classes while avoiding catastrophic forgetting of prior knowledge, without training on previous data. In this work, we build upon the recently introduced Taxonomy-Oriented PoincarÃ©-regularized Incremental Class Segmentation (TOPICS) approach and propose an enhanced variant, termed TOPICS+, specifically tailored for robust segmentation of surgical scenes. Concretely, we incorporate the Dice loss into the hierarchical loss formulation to handle strong class imbalances, introduce hierarchical pseudo-labeling, and design tailored label taxonomies for robotic surgery environments. We also propose six novel CISS benchmarks designed for robotic surgery environments including multiple incremental steps and several semantic categories to emulate realistic class-incremental settings in surgical environments. In addition, we introduce a refined set of labels with more than 144 classes on the Syn-Mediverse synthetic dataset, hosted online as an evaluation benchmark. We make the code and trained models publicly available at http://topics.cs.uni-freiburg.de.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººè¾…åŠ©æ‰‹æœ¯(Robot-assisted surgery)ä¸­åŠ¨æ€å˜åŒ–çš„ç¯å¢ƒéœ€æ±‚ï¼Œæå‡ºäº†åŸºäºå±‚æ¬¡åŒ–ç±»å¢é‡è¯­ä¹‰åˆ†å‰²(Class-Incremental Semantic Segmentation, CISS)çš„ TOPICS+ æ¡†æ¶ã€‚é€šè¿‡åœ¨å±‚æ¬¡åŒ–æŸå¤±å‡½æ•°ä¸­å¼•å…¥ Dice loss è§£å†³ä¸¥é‡çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œå¹¶ç»“åˆå±‚æ¬¡åŒ–ä¼ªæ ‡ç­¾(hierarchical pseudo-labeling)ä¸å®šåˆ¶çš„æ‰‹æœ¯æ ‡ç­¾åˆ†ç±»ä½“ç³»ï¼ŒTOPICS+ å®ç°äº†åœ¨ä¸é—å¿˜æ—§çŸ¥è¯†çš„æƒ…å†µä¸‹æŒç»­å­¦ä¹ æ–°ç±»åˆ«çš„èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜è®¾è®¡äº†å…­ä¸ªä¸“é—¨é’ˆå¯¹æœºå™¨äººæ‰‹æœ¯ç¯å¢ƒçš„ CISS åŸºå‡†æµ‹è¯•ï¼Œä»¥æ¨¡æ‹ŸçœŸå®çš„ç±»å¢é‡å­¦ä¹ åœºæ™¯ã€‚æ­¤å¤–ï¼Œä»–ä»¬åœ¨ Syn-Mediverse åˆæˆæ•°æ®é›†ä¸Šå¼•å…¥äº†åŒ…å« 144 ä¸ªç±»åˆ«çš„ç²¾ç»†æ ‡ç­¾é›†ï¼Œä¸ºæ‰‹æœ¯åœºæ™¯ç†è§£æä¾›äº†æ›´ç»†è‡´çš„è¯„ä¼°åŸºå‡†ã€‚è¯¥é¡¹å·¥ä½œé€šè¿‡å¼€æºä»£ç å’Œæ¨¡å‹ï¼Œä¸ºæå‡æ‰‹æœ¯æœºå™¨äººåœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸‹çš„è¯­ä¹‰åˆ†å‰²é²æ£’æ€§æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "accepted at MICCAI AMAI 2025 workshop",
      "pdf_url": "https://arxiv.org/pdf/2508.01713v2",
      "published_date": "2025-08-03 10:47:01 UTC",
      "updated_date": "2025-08-10 11:57:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:32:56.546440+00:00"
    },
    {
      "arxiv_id": "2508.01712v2",
      "title": "HateClipSeg: A Segment-Level Annotated Dataset for Fine-Grained Hate Video Detection",
      "title_zh": "HateClipSegï¼šé¢å‘ç»†ç²’åº¦ä»‡æ¨è§†é¢‘æ£€æµ‹çš„ç‰‡æ®µçº§æ ‡æ³¨æ•°æ®é›†",
      "authors": [
        "Han Wang",
        "Zhuoran Wang",
        "Roy Ka-Wei Lee"
      ],
      "abstract": "Detecting hate speech in videos remains challenging due to the complexity of multimodal content and the lack of fine-grained annotations in existing datasets. We present HateClipSeg, a large-scale multimodal dataset with both video-level and segment-level annotations, comprising over 11,714 segments labeled as Normal or across five Offensive categories: Hateful, Insulting, Sexual, Violence, Self-Harm, along with explicit target victim labels. Our three-stage annotation process yields high inter-annotator agreement (Krippendorff's alpha = 0.817). We propose three tasks to benchmark performance: (1) Trimmed Hateful Video Classification, (2) Temporal Hateful Video Localization, and (3) Online Hateful Video Classification. Results highlight substantial gaps in current models, emphasizing the need for more sophisticated multimodal and temporally aware approaches. The HateClipSeg dataset are publicly available at https://github.com/Social-AI-Studio/HateClipSeg.git.",
      "tldr_zh": "é’ˆå¯¹è§†é¢‘ä»‡æ¨è¨€è®ºæ£€æµ‹ä¸­å¤šæ¨¡æ€å†…å®¹å¤æ‚ä¸”ç°æœ‰æ•°æ®é›†ç¼ºä¹ç»†ç²’åº¦æ ‡æ³¨çš„æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº† HateClipSeg æ•°æ®é›†ã€‚HateClipSeg æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†ï¼ŒåŒæ—¶æä¾›è§†é¢‘çº§å’Œç‰‡æ®µçº§æ ‡æ³¨ï¼Œæ¶µç›–è¶…è¿‡ 11,714 ä¸ªç‰‡æ®µï¼Œç±»åˆ«åŒ…æ‹¬ Normal ä»¥åŠ Hatefulã€Insultingã€Sexualã€Violence å’Œ Self-Harm ç­‰äº”ç±» Offensive æ ‡ç­¾ã€‚è¯¥æ•°æ®é›†è¿˜åŒ…å«æ˜ç¡®çš„ç›®æ ‡å—å®³è€…æ ‡ç­¾ï¼Œå¹¶é€šè¿‡ä¸‰é˜¶æ®µæ ‡æ³¨æµç¨‹ç¡®ä¿äº†æé«˜çš„ä¸€è‡´æ€§ï¼Œå…¶ Krippendorff's alpha å€¼è¾¾åˆ° 0.817ã€‚ç ”ç©¶è€…åŸºäºè¯¥æ•°æ®é›†è®¾å®šäº† Trimmed Hateful Video Classificationã€Temporal Hateful Video Localization å’Œ Online Hateful Video Classification ä¸‰é¡¹åŸºå‡†ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ç°æœ‰æ¨¡å‹åœ¨ç»†ç²’åº¦æ£€æµ‹æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå¼ºè°ƒäº†å¼€å‘æ›´å…ˆè¿›çš„å¤šæ¨¡æ€åŠæ—¶é—´æ„ŸçŸ¥ï¼ˆtemporally awareï¼‰æ–¹æ³•çš„ç´§è¿«æ€§ã€‚ç›®å‰è¯¥æ•°æ®é›†å·²åœ¨ GitHub å…¬å¼€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.01712v2",
      "published_date": "2025-08-03 10:46:06 UTC",
      "updated_date": "2025-08-15 08:29:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:33:00.037355+00:00"
    },
    {
      "arxiv_id": "2508.01711v2",
      "title": "GAIS: Frame-Level Gated Audio-Visual Integration with Semantic Variance-Scaled Perturbation for Text-Video Retrieval",
      "title_zh": "GAISï¼šé¢å‘æ–‡æœ¬-è§†é¢‘æ£€ç´¢çš„å¸§çº§é—¨æ§éŸ³è§†é¢‘èåˆä¸è¯­ä¹‰æ–¹å·®ç¼©æ”¾æ‰°åŠ¨",
      "authors": [
        "Bowen Yang",
        "Yun Cao",
        "Chen He",
        "Xiaosu Su"
      ],
      "abstract": "Text-to-video retrieval requires precise alignment between language and temporally rich audio-video signals. However, existing methods often emphasize visual cues while underutilizing audio semantics or relying on coarse fusion strategies, resulting in suboptimal multimodal representations. We introduce GAIS, a retrieval framework that strengthens multimodal alignment from both representation and regularization perspectives. First, a Frame-level Gated Fusion (FGF) module adaptively integrates audio-visual features under textual guidance, enabling fine-grained temporal selection of informative frames. Second, a Semantic Variance-Scaled Perturbation (SVSP) mechanism regularizes the text embedding space by controlling perturbation magnitude in a semantics-aware manner. These two modules are complementary: FGF minimizes modality gaps through selective fusion, while SVSP improves embedding stability and discrimination. Extensive experiments on MSR-VTT, DiDeMo, LSMDC, and VATEX demonstrate that GAIS consistently outperforms strong baselines across multiple retrieval metrics while maintaining notable computational efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GAISï¼Œä¸€ç§ç”¨äº Text-to-video retrieval çš„æ£€ç´¢æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•å› è¿‡åº¦ä¾èµ–è§†è§‰çº¿ç´¢æˆ–èåˆç­–ç•¥ç²—ç³™è€Œå¯¼è‡´çš„å¤šæ¨¡æ€è¡¨ç¤ºäºšä¼˜é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸¤ä¸ªäº’è¡¥æ¨¡å—å¢å¼ºå¯¹é½ï¼šFrame-level Gated Fusion (FGF) æ¨¡å—åœ¨æ–‡æœ¬å¼•å¯¼ä¸‹è‡ªé€‚åº”æ•´åˆéŸ³è§†é¢‘ç‰¹å¾ï¼Œå®ç°å¯¹å…³é”®å¸§çš„ç»†ç²’åº¦æ—¶åºé€‰æ‹©ï¼›Semantic Variance-Scaled Perturbation (SVSP) æœºåˆ¶åˆ™é€šè¿‡è¯­ä¹‰æ„ŸçŸ¥æ–¹å¼æ§åˆ¶æ‰°åŠ¨ï¼Œå¯¹æ–‡æœ¬åµŒå…¥ç©ºé—´è¿›è¡Œæœ‰æ•ˆæ­£åˆ™åŒ–ã€‚FGF è´Ÿè´£ç¼©å°æ¨¡æ€é—´éš™ï¼Œè€Œ SVSP æå‡äº†åµŒå…¥çš„ç¨³å®šæ€§å’Œè¾¨åˆ«åŠ›ã€‚åœ¨ MSR-VTTã€DiDeMoã€LSMDC å’Œ VATEX ç­‰å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGAIS åœ¨å¤šé¡¹æ£€ç´¢æŒ‡æ ‡ä¸ŠæŒç»­è¶…è¶Šå¼ºåŠ›åŸºå‡†æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒäº†ä¼˜å¼‚çš„è®¡ç®—æ•ˆç‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.01711v2",
      "published_date": "2025-08-03 10:44:24 UTC",
      "updated_date": "2025-11-18 07:23:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:33:12.804152+00:00"
    },
    {
      "arxiv_id": "2508.01701v1",
      "title": "MHARFedLLM: Multimodal Human Activity Recognition Using Federated Large Language Model",
      "title_zh": "MHARFedLLMï¼šåŸºäºè”é‚¦å¤§è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€äººä½“æ´»åŠ¨è¯†åˆ«",
      "authors": [
        "Asmit Bandyopadhyay",
        "Rohit Basu",
        "Tanmay Sen",
        "Swagatam Das"
      ],
      "abstract": "Human Activity Recognition (HAR) plays a vital role in applications such as fitness tracking, smart homes, and healthcare monitoring. Traditional HAR systems often rely on single modalities, such as motion sensors or cameras, limiting robustness and accuracy in real-world environments. This work presents FedTime-MAGNET, a novel multimodal federated learning framework that advances HAR by combining heterogeneous data sources: depth cameras, pressure mats, and accelerometers. At its core is the Multimodal Adaptive Graph Neural Expert Transformer (MAGNET), a fusion architecture that uses graph attention and a Mixture of Experts to generate unified, discriminative embeddings across modalities. To capture complex temporal dependencies, a lightweight T5 encoder only architecture is customized and adapted within this framework. Extensive experiments show that FedTime-MAGNET significantly improves HAR performance, achieving a centralized F1 Score of 0.934 and a strong federated F1 Score of 0.881. These results demonstrate the effectiveness of combining multimodal fusion, time series LLMs, and federated learning for building accurate and robust HAR systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿäººä½“æ´»åŠ¨è¯†åˆ« (Human Activity Recognition, HAR) ç³»ç»Ÿå› ä¾èµ–å•ä¸€æ¨¡æ€è€Œå¯¼è‡´é²æ£’æ€§ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º FedTime-MAGNET çš„å¤šæ¨¡æ€è”é‚¦å­¦ä¹  (Federated Learning) æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆæ·±åº¦æ‘„åƒæœºã€å‹åŠ›å«å’ŒåŠ é€Ÿåº¦è®¡ç­‰å¼‚æ„æ•°æ®æºï¼Œå¹¶åˆ©ç”¨æ ¸å¿ƒçš„ Multimodal Adaptive Graph Neural Expert Transformer (MAGNET) æ¶æ„ï¼Œç»“åˆå›¾æ³¨æ„åŠ› (Graph Attention) ä¸æ··åˆä¸“å®¶æ¨¡å‹ (Mixture of Experts) ç”Ÿæˆè·¨æ¨¡æ€çš„ç»Ÿä¸€åˆ¤åˆ«æ€§åµŒå…¥ã€‚ä¸ºäº†æ•æ‰å¤æ‚çš„æ—¶é—´ä¾èµ–æ€§ï¼Œç ”ç©¶è€…åœ¨è¯¥æ¡†æ¶ä¸­å®šåˆ¶å¹¶é€‚é…äº†ä¸€ä¸ªè½»é‡çº§çš„ T5 encoder æ¶æ„ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨é›†ä¸­å¼è®­ç»ƒä¸‹è¾¾åˆ°äº† 0.934 çš„ F1 Scoreï¼Œå¹¶åœ¨è”é‚¦å­¦ä¹ æ¨¡å¼ä¸‹å–å¾—äº† 0.881 çš„å¼ºåŠ²è¡¨ç°ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†ç»“åˆå¤šæ¨¡æ€èåˆã€æ—¶é—´åºåˆ—å¤§è¯­è¨€æ¨¡å‹ (LLMs) ä¸è”é‚¦å­¦ä¹ åœ¨æ„å»ºå‡†ç¡®ä¸”ç¨³å¥çš„ HAR ç³»ç»Ÿæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01701v1",
      "published_date": "2025-08-03 10:05:06 UTC",
      "updated_date": "2025-08-03 10:05:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:33:11.134755+00:00"
    },
    {
      "arxiv_id": "2508.01700v2",
      "title": "DeepVIS: Bridging Natural Language and Data Visualization Through Step-wise Reasoning",
      "title_zh": "DeepVISï¼šé€šè¿‡åˆ†æ­¥æ¨ç†è¿æ¥è‡ªç„¶è¯­è¨€ä¸æ•°æ®å¯è§†åŒ–",
      "authors": [
        "Zhihao Shuai",
        "Boyan Li",
        "Siyu Yan",
        "Yuyu Luo",
        "Weikai Yang"
      ],
      "abstract": "Although data visualization is powerful for revealing patterns and communicating insights, creating effective visualizations requires familiarity with authoring tools and often disrupts the analysis flow. While large language models show promise for automatically converting analysis intent into visualizations, existing methods function as black boxes without transparent reasoning processes, which prevents users from understanding design rationales and refining suboptimal outputs. To bridge this gap, we propose integrating Chain-of-Thought (CoT) reasoning into the Natural Language to Visualization (NL2VIS) pipeline. First, we design a comprehensive CoT reasoning process for NL2VIS and develop an automatic pipeline to equip existing datasets with structured reasoning steps. Second, we introduce nvBench-CoT, a specialized dataset capturing detailed step-by-step reasoning from ambiguous natural language descriptions to finalized visualizations, which enables state-of-the-art performance when used for model fine-tuning. Third, we develop DeepVIS, an interactive visual interface that tightly integrates with the CoT reasoning process, allowing users to inspect reasoning steps, identify errors, and make targeted adjustments to improve visualization outcomes. Quantitative benchmark evaluations, two use cases, and a user study collectively demonstrate that our CoT framework effectively enhances NL2VIS quality while providing insightful reasoning steps to users.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰è‡ªç„¶è¯­è¨€è½¬å¯è§†åŒ–(NL2VIS)æ–¹æ³•ç”±äºç¼ºä¹é€æ˜æ¨ç†è¿‡ç¨‹è€Œå¯¼è‡´ç”¨æˆ·éš¾ä»¥ç†è§£å’Œä¼˜åŒ–çš„â€œé»‘ç›’â€é—®é¢˜ï¼Œæå‡ºäº†å°†é“¾å¼æ€ç»´(Chain-of-Thought, CoT)æ¨ç†é›†æˆåˆ°NL2VISå·¥ä½œæµä¸­çš„DeepVISæ¡†æ¶ã€‚ä½œè€…é¦–å…ˆè®¾è®¡äº†ä¸€å¥—å…¨é¢çš„CoTæ¨ç†æµç¨‹ï¼Œå¹¶å¼€å‘äº†è‡ªåŠ¨åŒ–æµæ°´çº¿ä¸ºç°æœ‰æ•°æ®é›†é…å¤‡ç»“æ„åŒ–æ¨ç†æ­¥éª¤ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†nvBench-CoTæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ•è·äº†ä»æ¨¡ç³Šè‡ªç„¶è¯­è¨€æè¿°åˆ°æœ€ç»ˆå¯è§†åŒ–ç”Ÿæˆçš„è¯¦ç»†æ­¥éª¤ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¾®è°ƒåçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒDeepVISäº¤äº’å¼ç•Œé¢å®ç°äº†ä¸CoTæ¨ç†è¿‡ç¨‹çš„æ·±åº¦æ•´åˆï¼Œå…è®¸ç”¨æˆ·ç›´æ¥å®¡æŸ¥æ¨ç†æ­¥éª¤ã€è¯†åˆ«é”™è¯¯å¹¶è¿›è¡Œé’ˆå¯¹æ€§ä¿®æ­£ã€‚å®šé‡åŸºå‡†æµ‹è¯•å’Œç”¨æˆ·ç ”ç©¶å…±åŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æé«˜å¯è§†åŒ–ç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œé€šè¿‡æä¾›æ¸…æ™°çš„è®¾è®¡ä¾æ®æœ‰æ•ˆå¢å¼ºäº†ç³»ç»Ÿçš„é€æ˜åº¦ä¸ç”¨æˆ·æ§åˆ¶åŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "IEEE VIS 2025 full paper",
      "pdf_url": "https://arxiv.org/pdf/2508.01700v2",
      "published_date": "2025-08-03 10:04:17 UTC",
      "updated_date": "2025-09-04 12:50:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:33:19.636759+00:00"
    },
    {
      "arxiv_id": "2508.01696v3",
      "title": "CoCoA: Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy",
      "title_zh": "CoCoAï¼šé¢å‘å‚æ•°åŒ–çŸ¥è¯†ä¸æ£€ç´¢çŸ¥è¯†ååŒçš„åä½œæ™ºèƒ½ä½“é“¾",
      "authors": [
        "Yi Jiang",
        "Sendong Zhao",
        "Jianbo Li",
        "Haochun Wang",
        "Lizhe Zhang",
        "Yan Liu",
        "Bing Qin"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs), especially for knowledge-intensive tasks. Despite its advantages, current RAG methods often struggle to fully exploit knowledge during generation. In particular, the synergy between the model's internal parametric knowledge and external retrieved knowledge remains limited. Retrieved contents may sometimes mislead generation, while certain generated content can guide the model toward more accurate outputs. In this work, we propose Collaborative Chain-of-Agents, a framework designed to enhance explicitly synergy over both parametric and retrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent RAG framework that first performs conditional knowledge induction and then reasons answers. Building on this, we develop CoCoA, a long-chain training strategy that synthesizes extended multi-agent reasoning trajectories from CoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability to explicitly integrate and jointly leverage parametric and retrieved knowledge. Experimental results demonstrate the superiority of CoCoA in open-domain QA and multi-hop QA.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CoCoAæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ä¸­å¤§è¯­è¨€æ¨¡å‹å†…éƒ¨å‚æ•°åŒ–çŸ¥è¯†(Parametric Knowledge)ä¸å¤–éƒ¨æ£€ç´¢çŸ¥è¯†(Retrieved Knowledge)ååŒæ•ˆç‡ä½ä¸‹çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚ç ”ç©¶è€…é¦–å…ˆæ„å»ºäº†CoCoA-zeroå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡å…ˆæ‰§è¡Œæ¡ä»¶çŸ¥è¯†å½’çº³(Conditional Knowledge Induction)å†è¿›è¡Œæ¨ç†çš„æµç¨‹ï¼Œå¼•å¯¼æ¨¡å‹æ›´å‡†ç¡®åœ°è§£è¯»ä¿¡æ¯å¹¶å‡å°‘æ£€ç´¢å†…å®¹çš„è¯¯å¯¼ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†CoCoAé•¿é“¾è®­ç»ƒç­–ç•¥(Long-Chain Training Strategy)ï¼Œåˆ©ç”¨åˆæˆçš„å¤æ‚æ¨ç†è½¨è¿¹å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»è€Œå¼ºåŒ–å…¶æ˜¾å¼æ•´åˆåŠå…±åŒåˆ©ç”¨ä¸¤ç§çŸ¥è¯†æºçš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒCoCoAåœ¨å¼€æ”¾åŸŸé—®ç­”(Open-domain QA)å’Œå¤šè·³é—®ç­”(Multi-hop QA)ä»»åŠ¡ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­çš„å¤„ç†æ•ˆèƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "code available at https://github.com/liunian-Jay/CoCoA",
      "pdf_url": "https://arxiv.org/pdf/2508.01696v3",
      "published_date": "2025-08-03 10:00:38 UTC",
      "updated_date": "2025-10-09 15:53:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:33:16.991847+00:00"
    },
    {
      "arxiv_id": "2508.01693v2",
      "title": "SURE-Med: Systematic Uncertainty Reduction for Enhanced Reliability in Medical Report Generation",
      "title_zh": "SURE-Medï¼šé¢å‘å¢å¼ºåŒ»ç–—æŠ¥å‘Šç”Ÿæˆå¯é æ€§çš„ç³»ç»ŸåŒ–ä¸ç¡®å®šæ€§æ¶ˆå‡",
      "authors": [
        "Yuhang Gu",
        "Xingyu Hu",
        "Yuyu Fan",
        "Xulin Yan",
        "Longhuan Xu",
        "Peng peng"
      ],
      "abstract": "Automated medical report generation (MRG) holds great promise for reducing the heavy workload of radiologists. However, its clinical deployment is hindered by three major sources of uncertainty. First, visual uncertainty, caused by noisy or incorrect view annotations, compromises feature extraction. Second, label distribution uncertainty, stemming from long-tailed disease prevalence, biases models against rare but clinically critical conditions. Third, contextual uncertainty, introduced by unverified historical reports, often leads to factual hallucinations. These challenges collectively limit the reliability and clinical trustworthiness of MRG systems. To address these issues, we propose SURE-Med, a unified framework that systematically reduces uncertainty across three critical dimensions: visual, distributional, and contextual. To mitigate visual uncertainty, a Frontal-Aware View Repair Resampling module corrects view annotation errors and adaptively selects informative features from supplementary views. To tackle label distribution uncertainty, we introduce a Token Sensitive Learning objective that enhances the modeling of critical diagnostic sentences while reweighting underrepresented diagnostic terms, thereby improving sensitivity to infrequent conditions. To reduce contextual uncertainty, our Contextual Evidence Filter validates and selectively incorporates prior information that aligns with the current image, effectively suppressing hallucinations. Extensive experiments on the MIMIC-CXR and IU-Xray benchmarks demonstrate that SURE-Med achieves state-of-the-art performance. By holistically reducing uncertainty across multiple input modalities, SURE-Med sets a new benchmark for reliability in medical report generation and offers a robust step toward trustworthy clinical decision support.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SURE-Medï¼Œä¸€ä¸ªæ—¨åœ¨å¢å¼ºåŒ»ç–—æŠ¥å‘Šç”Ÿæˆ(Medical Report Generation)å¯é æ€§çš„ç³»ç»Ÿæ€§ä¸ç¡®å®šæ€§å‡å°‘æ¡†æ¶ã€‚è¯¥æ¡†æ¶é’ˆå¯¹å½“å‰MRGç³»ç»Ÿä¸­å­˜åœ¨çš„è§†è§‰ä¸ç¡®å®šæ€§ã€æ ‡ç­¾åˆ†å¸ƒä¸ç¡®å®šæ€§ä»¥åŠä¸Šä¸‹æ–‡ä¸ç¡®å®šæ€§ä¸‰å¤§æ ¸å¿ƒæŒ‘æˆ˜è¿›è¡Œäº†ååŒä¼˜åŒ–ã€‚ä¸ºäº†æ¶ˆé™¤è§†è§‰æ ‡æ³¨å™ªå£°ï¼Œç ”ç©¶å¼•å…¥äº†Frontal-Aware View Repair Resamplingæ¨¡å—æ¥çº æ­£è§†è§’é”™è¯¯å¹¶è‡ªé€‚åº”æå–å…³é”®è§†è§‰ç‰¹å¾ã€‚é’ˆå¯¹é•¿å°¾åˆ†å¸ƒä¸‹çš„ç½•è§ç—…è¯†åˆ«éš¾é¢˜ï¼Œæå‡ºäº†Token Sensitive Learningå­¦ä¹ ç›®æ ‡ï¼Œé€šè¿‡å¼ºåŒ–å…³é”®è¯Šæ–­è¯­å¥å»ºæ¨¡æ¥æé«˜æ¨¡å‹æ•æ„Ÿåº¦ã€‚åŒæ—¶ï¼Œåˆ©ç”¨Contextual Evidence Filterç­›é€‰å¹¶éªŒè¯å†å²æŠ¥å‘Šä¸­çš„å‚è€ƒä¿¡æ¯ï¼Œæœ‰æ•ˆæŠ‘åˆ¶äº†ä¸Šä¸‹æ–‡é©±åŠ¨çš„äº‹å®æ€§å¹»è§‰ã€‚å®éªŒè¯æ˜SURE-Medåœ¨MIMIC-CXRå’ŒIU-XrayåŸºå‡†æ•°æ®é›†ä¸Šå‡å–å¾—äº†State-of-the-artçš„æ€§èƒ½ï¼Œä¸ºæ„å»ºå¯ä¿¡çš„ä¸´åºŠå†³ç­–è¾…åŠ©æŠ€æœ¯è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "fix some problems",
      "pdf_url": "https://arxiv.org/pdf/2508.01693v2",
      "published_date": "2025-08-03 09:52:30 UTC",
      "updated_date": "2026-01-22 05:21:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:33:19.336517+00:00"
    },
    {
      "arxiv_id": "2508.01687v4",
      "title": "Explaining Time Series Classifiers with PHAR: Rule Extraction and Fusion from Post-hoc Attributions",
      "title_zh": "ä½¿ç”¨ PHAR è§£é‡Šæ—¶é—´åºåˆ—åˆ†ç±»å™¨ï¼šåŸºäºäº‹åå½’å› çš„è§„åˆ™æå–ä¸èåˆ",
      "authors": [
        "Maciej Mozolewski",
        "Szymon Bobek",
        "Grzegorz J. Nalepa"
      ],
      "abstract": "Explaining machine learning (ML) models for time series (TS) classification remains challenging due to the difficulty of interpreting raw time series and the high dimensionality of the input space. We introduce PHAR--Post-hoc Attribution Rules--a unified framework that transforms numeric feature attributions from post-hoc, instance-wise explainers (e.g. LIME, SHAP) into structured, human-readable rules. These rules define human-readable intervals that indicate where and when decision-relevant segments occur and can enhance model transparency by localizing threshold-based conditions on the raw series. PHAR performs comparably to native rule-based methods, such as Anchor, while scaling more efficiently to long TS sequences and achieving broader instance coverage. A dedicated rule fusion step consolidates rule sets using strategies like weighted selection and lasso-based refinement, balancing key quality metrics: coverage, confidence, and simplicity. This fusion ensures each instance receives a concise and unambiguous rule, improving both explanation fidelity and consistency. We further introduce visualization techniques to illustrate specificity-generalization trade-offs in the derived rules. PHAR resolves conflicting and overlapping explanations--a common effect of the Rashomon phenomenon--into coherent, domain-adaptable insights. Comprehensive experiments on UCR/UEA Time Series Classification Archive demonstrate that PHAR may improve interpretability, decision transparency, and practical applicability for TS classification tasks by providing concise, human-readable rules aligned with model predictions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PHARï¼ˆPost-hoc Attribution Rulesï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æå‡æ—¶é—´åºåˆ—(Time Series)åˆ†ç±»æ¨¡å‹å¯è§£é‡Šæ€§çš„ç»Ÿä¸€æ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå°†æ¥è‡ªLIMEã€SHAPç­‰äº‹å(post-hoc)è§£é‡Šå™¨çš„æ•°å€¼ç‰¹å¾å½’å› è½¬åŒ–ä¸ºç»“æ„åŒ–çš„ã€äººç±»å¯è¯»çš„è§„åˆ™ã€‚è¿™äº›è§„åˆ™é€šè¿‡åœ¨åŸå§‹åºåˆ—ä¸Šå®šä¹‰å¹¶å®šä½ä¸å†³ç­–ç›¸å…³çš„åŒºæ®µå’Œé˜ˆå€¼æ¡ä»¶ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„é€æ˜åº¦ã€‚PHARåœ¨æ€§èƒ½ä¸Šä¸Anchorç­‰åŸç”Ÿè§„åˆ™æ–¹æ³•ç›¸å½“ï¼Œä½†åœ¨å¤„ç†é•¿åºåˆ—æ—¶å…·æœ‰æ›´ä¼˜çš„æ‰©å±•æ€§å’Œæ›´å¹¿çš„å®ä¾‹è¦†ç›–ç‡ã€‚é€šè¿‡å¼•å…¥åŒ…å«åŠ æƒé€‰æ‹©å’ŒLassoç»†åŒ–çš„è§„åˆ™èåˆ(rule fusion)æ­¥éª¤ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆå¹³è¡¡äº†è§„åˆ™çš„è¦†ç›–èŒƒå›´ã€ç½®ä¿¡åº¦ä¸ç®€æ´æ€§ã€‚æ­¤å¤–ï¼ŒPHARæˆåŠŸè§£å†³äº†ç”±äºRashomonç°è±¡å¯¼è‡´çš„è§£é‡Šå†²çªé—®é¢˜ï¼Œä¸ºé¢†åŸŸä¸“å®¶æä¾›äº†è¿è´¯ä¸”å…·å¤‡é¢†åŸŸé€‚åº”æ€§çš„æ´å¯Ÿã€‚åœ¨UCR/UEAæ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¯æ˜ï¼ŒPHARèƒ½å¤Ÿé€šè¿‡æä¾›ä¸é¢„æµ‹ä¸€è‡´çš„äººç±»å¯è¯»è§„åˆ™ï¼Œæœ‰æ•ˆæå‡æ—¶é—´åºåˆ—åˆ†ç±»ä»»åŠ¡çš„å®ç”¨æ€§å’Œå†³ç­–é€æ˜åº¦ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01687v4",
      "published_date": "2025-08-03 09:45:40 UTC",
      "updated_date": "2026-01-16 12:00:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:33:45.270998+00:00"
    },
    {
      "arxiv_id": "2508.02751v2",
      "title": "SmallKV: Small Model Assisted Compensation of KV Cache Compression for Efficient LLM Inference",
      "title_zh": "SmallKVï¼šé¢å‘é«˜æ•ˆå¤§è¯­è¨€æ¨¡å‹æ¨ç†çš„å°æ¨¡å‹è¾…åŠ© KV ç¼“å­˜å‹ç¼©è¡¥å¿",
      "authors": [
        "Yi Zhao",
        "Yajuan Peng",
        "Cam-Tu Nguyen",
        "Zuchao Li",
        "Xiaoliang Wang",
        "Hai Zhao",
        "Xiaoming Fu"
      ],
      "abstract": "KV cache eviction has emerged as an effective solution to alleviate resource constraints faced by LLMs in long-context scenarios. However, existing token-level eviction methods often overlook two critical aspects: (1) their irreversible eviction strategy fails to adapt to dynamic attention patterns during decoding (the saliency shift problem), and (2) they treat both marginally important tokens and truly unimportant tokens equally, despite the collective significance of marginal tokens to model performance (the marginal information over-compression problem). To address these issues, we design two compensation mechanisms based on the high similarity of attention matrices between LLMs of different scales. We propose SmallKV, a small model assisted compensation method for KV cache compression. SmallKV can maintain attention matching between different-scale LLMs to: 1) assist the larger model in perceiving globally important information of attention; and 2) use the smaller model's attention scores to approximate those of marginal tokens in the larger model. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and LongBench demonstrate the effectiveness of SmallKV. Moreover, efficiency evaluations show that SmallKV achieves 1.75 - 2.56 times higher throughput than baseline methods, highlighting its potential for efficient and performant LLM inference in resource constrained environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨é•¿æ–‡æœ¬åœºæ™¯ä¸‹ KV cache é©±é€é¢ä¸´çš„æ˜¾è‘—æ€§åç§» (saliency shift) ä»¥åŠè¾¹ç¼˜ä¿¡æ¯è¿‡åº¦å‹ç¼© (marginal information over-compression) é—®é¢˜ï¼Œæå‡ºäº† SmallKV è¡¥å¿æ–¹æ³•ã€‚SmallKV æ˜¯ä¸€ç§å°æ¨¡å‹è¾…åŠ©çš„ KV cache å‹ç¼©æŠ€æœ¯ï¼Œå…¶æ ¸å¿ƒåœ¨äºåˆ©ç”¨ä¸åŒè§„æ¨¡ LLMs ä¹‹é—´æ³¨æ„åŠ›çŸ©é˜µçš„é«˜åº¦ç›¸ä¼¼æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»´æŠ¤è·¨è§„æ¨¡æ¨¡å‹çš„æ³¨æ„åŠ›åŒ¹é…ï¼Œè¾…åŠ©å¤§æ¨¡å‹æ„ŸçŸ¥å…¨å±€é‡è¦çš„æ³¨æ„åŠ›ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨å°æ¨¡å‹çš„æ³¨æ„åŠ›å¾—åˆ†æ¥è¿‘ä¼¼å¤§æ¨¡å‹ä¸­è¾¹ç¼˜æ ‡è®° (marginal tokens) çš„è´¡çŒ®ã€‚åœ¨ GSM8Kã€BBHã€MT-Bench å’Œ LongBench ç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜äº† SmallKV çš„å“è¶Šæ€§èƒ½ã€‚æ•ˆç‡è¯„ä¼°æ˜¾ç¤ºï¼ŒSmallKV çš„ååé‡æ¯”åŸºçº¿æ–¹æ³•æå‡äº† 1.75 è‡³ 2.56 å€ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„é«˜æ•ˆ LLM æ¨ç†æä¾›äº†æå…·æ½œåŠ›çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.02751v2",
      "published_date": "2025-08-03 09:15:36 UTC",
      "updated_date": "2026-01-19 09:51:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:34:39.796553+00:00"
    },
    {
      "arxiv_id": "2508.01680v1",
      "title": "T-GRAG: A Dynamic GraphRAG Framework for Resolving Temporal Conflicts and Redundancy in Knowledge Retrieval",
      "title_zh": "T-GRAGï¼šä¸€ç§ç”¨äºè§£å†³çŸ¥è¯†æ£€ç´¢ä¸­æ—¶åºå†²çªä¸å†—ä½™çš„åŠ¨æ€ GraphRAG æ¡†æ¶",
      "authors": [
        "Dong Li",
        "Yichen Niu",
        "Ying Ai",
        "Xiang Zou",
        "Biqing Qi",
        "Jianxing Liu"
      ],
      "abstract": "Large language models (LLMs) have demonstrated strong performance in natural language generation but remain limited in knowle-\n  dge-intensive tasks due to outdated or incomplete internal knowledge. Retrieval-Augmented Generation (RAG) addresses this by incorporating external retrieval, with GraphRAG further enhancing performance through structured knowledge graphs and multi-hop reasoning. However, existing GraphRAG methods largely ignore the temporal dynamics of knowledge, leading to issues such as temporal ambiguity, time-insensitive retrieval, and semantic redundancy. To overcome these limitations, we propose Temporal GraphRAG (T-GRAG), a dynamic, temporally-aware RAG framework that models the evolution of knowledge over time. T-GRAG consists of five key components: (1) a Temporal Knowledge Graph Generator that creates time-stamped, evolving graph structures; (2) a Temporal Query Decomposition mechanism that breaks complex temporal queries into manageable sub-queries; (3) a Three-layer Interactive Retriever that progressively filters and refines retrieval across temporal subgraphs; (4) a Source Text Extractor to mitigate noise; and (5) a LLM-based Generator that synthesizes contextually and temporally accurate responses. We also introduce Time-LongQA, a novel benchmark dataset based on real-world corporate annual reports, designed to test temporal reasoning across evolving knowledge. Extensive experiments show that T-GRAG significantly outperforms prior RAG and GraphRAG baselines in both retrieval accuracy and response relevance under temporal constraints, highlighting the necessity of modeling knowledge evolution for robust long-text question answering. Our code is publicly available on the T-GRAG",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†T-GRAGï¼Œä¸€ç§åŠ¨æ€ä¸”å…·æœ‰æ—¶é—´æ„ŸçŸ¥èƒ½åŠ›çš„GraphRAGæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆè¿‡ç¨‹ä¸­å¿½è§†æ—¶é—´åŠ¨æ€æ€§æ‰€å¯¼è‡´çš„æ—¶é—´æ­§ä¹‰å’Œè¯­ä¹‰å†—ä½™é—®é¢˜ã€‚è¯¥æ¡†æ¶ç”±äº”ä¸ªæ ¸å¿ƒç»„ä»¶æ„æˆï¼ŒåŒ…æ‹¬æ„å»ºå¸¦æ—¶é—´æˆ³æ¼”åŒ–å›¾ç»“æ„çš„Temporal Knowledge Graph Generatorã€åˆ†è§£å¤æ‚æ—¶é—´æŸ¥è¯¢çš„Temporal Query Decompositionã€è¿›è¡Œé€å±‚ç²¾ç‚¼æ£€ç´¢çš„Three-layer Interactive Retrieverï¼Œä»¥åŠè´Ÿè´£é™å™ªå’Œå“åº”åˆæˆçš„æå–å™¨ä¸ç”Ÿæˆå™¨ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†åŸºäºçœŸå®ä¼ä¸šå¹´æŠ¥çš„æ–°å‹åŸºå‡†æ•°æ®é›†Time-LongQAï¼Œä¸“é—¨ç”¨äºæµ‹è¯•æ¨¡å‹åœ¨åŠ¨æ€çŸ¥è¯†ç¯å¢ƒä¸‹çš„æ—¶é—´æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒT-GRAGåœ¨æ£€ç´¢å‡†ç¡®æ€§å’Œå“åº”ç›¸å…³æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„RAGå’ŒGraphRAGåŸºå‡†æ¨¡å‹ã€‚è¿™ä¸€ç ”ç©¶è¯æ˜äº†å¯¹çŸ¥è¯†æ¼”åŒ–è¿›è¡Œå»ºæ¨¡å¯¹äºæå‡é•¿æ–‡æœ¬é—®ç­”ç³»ç»Ÿåœ¨å¤„ç†å…·æœ‰æ—¶é—´çº¦æŸçš„ä»»åŠ¡æ—¶å…·æœ‰è‡³å…³é‡è¦çš„ä½œç”¨ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01680v1",
      "published_date": "2025-08-03 09:15:36 UTC",
      "updated_date": "2025-08-03 09:15:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:34:08.888045+00:00"
    },
    {
      "arxiv_id": "2508.01678v1",
      "title": "Cure or Poison? Embedding Instructions Visually Alters Hallucination in Vision-Language Models",
      "title_zh": "è‰¯è¯è¿˜æ˜¯æ¯’è¯ï¼Ÿè§†è§‰åŒ–æŒ‡ä»¤åµŒå…¥æ”¹å˜äº†è§†è§‰è¯­è¨€æ¨¡å‹çš„å¹»è§‰",
      "authors": [
        "Zhaochen Wang",
        "Yiwei Wang",
        "Yujun Cai"
      ],
      "abstract": "Vision-Language Models (VLMs) often suffer from hallucination, partly due to challenges in aligning multimodal information. We propose Prompt-in-Image, a simple method that embeds textual instructions directly into images. This removes the need for separate text inputs and forces the model to process all content through the visual channel. We evaluate this method on three popular open-source VLMs: Qwen2.5-VL, LLaVA-1.5, and InstructBLIP. The results reveal sharp differences. Prompt-in-Image improves Qwen2.5-VL's performance, increasing POPE accuracy by 4.1 percent (from 80.2 percent to 84.3 percent) and also reducing hallucination rates on MS-COCO. In contrast, LLaVA-1.5 and InstructBLIP experience a severe performance drop, with accuracy falling from around 84 percent to near-random levels. Through detailed analysis, we found that CLIP-based encoders in LLaVA and InstructBLIP exhibit excessive attention bias toward embedded text regions, disrupting visual understanding. In contrast, Qwen's vision encoder handles text-embedded images robustly. Crucially, Prompt-in-Image reduces Qwen's modality gap, enhancing cross-modal alignment by unifying information processing through a single modality.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Prompt-in-Imageæ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å°†æ–‡æœ¬æŒ‡ä»¤ç›´æ¥åµŒå…¥å›¾åƒæ¥ç¼“è§£è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)çš„å¹»è§‰é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡æ¶ˆé™¤ç‹¬ç«‹çš„æ–‡æœ¬è¾“å…¥ï¼Œå¼ºåˆ¶æ¨¡å‹å®Œå…¨é€šè¿‡è§†è§‰é€šé“å¤„ç†ä¿¡æ¯ã€‚åœ¨å¯¹ä¸‰ç§ä¸»æµå¼€æºVLMsçš„è¯„ä¼°ä¸­ï¼Œè¯¥æ–¹æ³•ä½¿Qwen2.5-VLçš„POPEå‡†ç¡®ç‡æå‡äº†4.1%å¹¶æœ‰æ•ˆé™ä½äº†MS-COCOçš„å¹»è§‰ç‡ã€‚ç„¶è€Œï¼ŒLLaVA-1.5å’ŒInstructBLIPåœ¨åº”ç”¨è¯¥æ–¹æ³•åæ€§èƒ½å‡ºç°å‰§çƒˆä¸‹æ»‘ï¼Œå‡†ç¡®ç‡è·Œè‡³æ¥è¿‘éšæœºæ°´å¹³ã€‚åˆ†æå‘ç°ï¼ŒLLaVAå’ŒInstructBLIPæ‰€ä½¿ç”¨çš„åŸºäºCLIPçš„ç¼–ç å™¨å¯¹åµŒå…¥æ–‡æœ¬åŒºåŸŸè¡¨ç°å‡ºè¿‡åº¦çš„æ³¨æ„åŠ›åå·®(attention bias)ï¼Œè€ŒQwençš„è§†è§‰ç¼–ç å™¨åˆ™æ›´å…·é²æ£’æ€§ã€‚ç ”ç©¶æœ€ç»ˆè¡¨æ˜ï¼ŒPrompt-in-Imageèƒ½é€šè¿‡ç»Ÿä¸€å•æ¨¡æ€å¤„ç†è·¯å¾„æ¥ç¼©å°æ¨¡æ€å·®è·(modality gap)ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºè·¨æ¨¡æ€å¯¹é½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Work in progress",
      "pdf_url": "https://arxiv.org/pdf/2508.01678v1",
      "published_date": "2025-08-03 09:11:18 UTC",
      "updated_date": "2025-08-03 09:11:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:34:17.388196+00:00"
    },
    {
      "arxiv_id": "2508.01674v2",
      "title": "CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions",
      "title_zh": "CUPIDï¼šåŸºäºäº¤äº’çš„å¤§è¯­è¨€æ¨¡å‹ä¸ªæ€§åŒ–ä¸æƒ…å¢ƒåŒ–å¯¹é½è¯„ä¼°",
      "authors": [
        "Tae Soo Kim",
        "Yoonjoo Lee",
        "Yoonah Park",
        "Jiho Kim",
        "Young-Ho Kim",
        "Juho Kim"
      ],
      "abstract": "Personalization of Large Language Models (LLMs) often assumes users hold static preferences that reflect globally in all tasks. In reality, humans hold dynamic preferences that change depending on the context. As users interact with an LLM in various contexts, they naturally reveal their contextual preferences, which a model must infer and apply in future contexts to ensure alignment. To assess this, we introduce CUPID, a benchmark of 756 human-curated interaction session histories between users and LLM-based chat assistants. In each interaction session, the user provides a request in a specific context and expresses their preference through multi-turn feedback. Given a new user request and prior interaction sessions, our benchmark assesses whether LLMs can infer the preference relevant to this request and generate a response that satisfies this preference. With CUPID, we evaluated 10 open and proprietary LLMs, revealing that state-of-the-art LLMs struggle to infer preferences from multi-turn interactions and fail to discern what previous context is relevant to a new request -- under 50% precision and 65% recall. Our work highlights the need to advance LLM capabilities for more contextually personalized interactions and proposes CUPID as a resource to drive these improvements.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CUPIDï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«756ä¸ªç”±äººç±»ç­–åˆ’çš„äº¤äº’ä¼šè¯å†å²çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨åŠ¨æ€è¯­å¢ƒä¸‹çš„ä¸ªæ€§åŒ–(Personalization)å’Œä¸Šä¸‹æ–‡å¯¹é½èƒ½åŠ›ã€‚ç ”ç©¶é’ˆå¯¹äººç±»åå¥½éšè¯­å¢ƒåŠ¨æ€å˜åŒ–çš„å®é™…æƒ…å†µï¼Œè¦æ±‚æ¨¡å‹èƒ½å¤Ÿä»ç”¨æˆ·çš„å¤šè½®åé¦ˆä¸­æ¨æ–­å‡ºç‰¹å®šçš„ä¸Šä¸‹æ–‡åå¥½ï¼Œå¹¶å‡†ç¡®åº”ç”¨äºæœªæ¥çš„äº¤äº’åœºæ™¯ã€‚é€šè¿‡å¯¹10ç§å¼€æºå’Œå•†ä¸šLLMsçš„æ·±å…¥è¯„ä¼°ï¼Œç ”ç©¶å‘ç°å½“å‰çš„å°–ç«¯æ¨¡å‹åœ¨å¤„ç†å¤šè½®äº¤äº’æ¨æ–­ä»¥åŠè¯†åˆ«ç›¸å…³å†å²è¯­å¢ƒæ–¹é¢å­˜åœ¨æ˜¾è‘—å›°éš¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºè¿™äº›æ¨¡å‹çš„Precisionä½äº50%ï¼ŒRecallä½äº65%ï¼Œæš´éœ²å‡ºåœ¨å¤æ‚è¯­å¢ƒç†è§£ä¸Šçš„çŸ­æ¿ã€‚è¯¥é¡¹å·¥ä½œä¸ä»…æ­ç¤ºäº†æå‡LLMè¯­å¢ƒåŒ–ä¸ªæ€§åŒ–äº¤äº’èƒ½åŠ›çš„ç´§è¿«éœ€æ±‚ï¼Œè¿˜æä¾›äº†CUPIDèµ„æºä»¥æ¨åŠ¨è¯¥é¢†åŸŸçš„åç»­æ”¹è¿›ä¸æŠ€æœ¯è¿›æ­¥ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to COLM 2025. Project Website: https://cupid.kixlab.org/",
      "pdf_url": "https://arxiv.org/pdf/2508.01674v2",
      "published_date": "2025-08-03 09:04:48 UTC",
      "updated_date": "2025-08-07 05:03:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:34:42.088025+00:00"
    },
    {
      "arxiv_id": "2508.01670v2",
      "title": "QCBench: Evaluating Large Language Models on Domain-Specific Quantitative Chemistry",
      "title_zh": "QCBenchï¼šé’ˆå¯¹ä¸“ä¸šå®šé‡åŒ–å­¦é¢†åŸŸçš„å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°",
      "authors": [
        "Jiaqing Xie",
        "Weida Wang",
        "Ben Gao",
        "Zhuo Yang",
        "Haiyuan Wan",
        "Shufei Zhang",
        "Tianfan Fu",
        "Yuqiang Li"
      ],
      "abstract": "Quantitative chemistry is central to modern chemical research, yet the ability of large language models (LLMs) to perform its rigorous, step-by-step calculations remains underexplored. To fill this blank, we propose QCBench, a Quantitative Chemistry oriented benchmark comprising 350 computational chemistry problems across 7 chemistry subfields, which contains analytical chemistry, bio/organic chemistry, general chemistry, inorganic chemistry, physical chemistry, polymer chemistry and quantum chemistry. To systematically evaluate the mathematical reasoning abilities of large language models (LLMs), they are categorized into three tiers: easy, medium, and difficult. Each problem, rooted in realistic chemical scenarios, is structured to prevent heuristic shortcuts and demand explicit numerical reasoning. QCBench enables fine-grained diagnosis of computational weaknesses, reveals model-specific limitations across difficulty levels, and lays the groundwork for future improvements such as domain-adaptive fine-tuning or multi-modal integration. Evaluations on 24 LLMs demonstrate a consistent performance degradation with increasing task complexity, highlighting the current gap between language fluency and scientific computation accuracy. Code for QCBench is available at https://github.com/jiaqingxie/QCBench.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† QCBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨é’ˆå¯¹å®šé‡åŒ–å­¦ (Quantitative Chemistry) çš„åŸºå‡†æµ‹è¯•é›†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ (LLMs) æ‰§è¡Œä¸¥è°¨æ•°å€¼è®¡ç®—ä¸æ¨ç†çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŒ…å«æ¶µç›–åˆ†æåŒ–å­¦ã€ç‰©ç†åŒ–å­¦åŠé‡å­åŒ–å­¦ç­‰ 7 ä¸ªåŒ–å­¦å­é¢†åŸŸçš„ 350 ä¸ªè®¡ç®—é—®é¢˜ï¼Œå¹¶æ ¹æ®ä»»åŠ¡å¤æ‚åº¦å°†å…¶åˆ’åˆ†ä¸ºä¸‰ä¸ªéš¾åº¦ç­‰çº§ã€‚æ¯ä¸ªé¢˜ç›®å‡åŸºäºçœŸå®åŒ–å­¦åœºæ™¯è®¾è®¡ï¼Œé€šè¿‡ç»“æ„åŒ–å¤„ç†æ’é™¤äº†å¯å‘å¼æ·å¾„ï¼Œè¦æ±‚æ¨¡å‹å¿…é¡»è¿›è¡Œæ˜¾å¼çš„æ•°å€¼æ¨ç†ã€‚å¯¹ 24 ä¸ªä¸»æµå¤§è¯­è¨€æ¨¡å‹çš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹æ€§èƒ½éšä»»åŠ¡éš¾åº¦å¢åŠ è€Œæ˜¾è‘—ä¸‹é™ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨è¯­è¨€æµç•…åº¦ä¸ç§‘å­¦è®¡ç®—å‡†ç¡®æ€§ä¹‹é—´å­˜åœ¨çš„å·¨å¤§å·®è·ã€‚QCBench ä¸ºè¯Šæ–­æ¨¡å‹è®¡ç®—å¼±ç‚¹æä¾›äº†ç»†ç²’åº¦å·¥å…·ï¼Œå¹¶ä¸ºæœªæ¥çš„é¢†åŸŸè‡ªé€‚åº”å¾®è°ƒ (Domain-adaptive Fine-tuning) æˆ–å¤šæ¨¡æ€é›†æˆç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "physics.chem-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "Revision at Journal of Chemical Information and Modeling",
      "pdf_url": "https://arxiv.org/pdf/2508.01670v2",
      "published_date": "2025-08-03 08:55:42 UTC",
      "updated_date": "2025-10-04 05:53:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:34:11.009533+00:00"
    },
    {
      "arxiv_id": "2508.01656v1",
      "title": "Authorship Attribution in Multilingual Machine-Generated Texts",
      "title_zh": "å¤šè¯­è¨€æœºå™¨ç”Ÿæˆæ–‡æœ¬çš„ä½œè€…å½’å±",
      "authors": [
        "Lucio La Cava",
        "Dominik Macko",
        "RÃ³bert MÃ³ro",
        "Ivan Srba",
        "Andrea Tagarelli"
      ],
      "abstract": "As Large Language Models (LLMs) have reached human-like fluency and coherence, distinguishing machine-generated text (MGT) from human-written content becomes increasingly difficult. While early efforts in MGT detection have focused on binary classification, the growing landscape and diversity of LLMs require a more fine-grained yet challenging authorship attribution (AA), i.e., being able to identify the precise generator (LLM or human) behind a text. However, AA remains nowadays confined to a monolingual setting, with English being the most investigated one, overlooking the multilingual nature and usage of modern LLMs. In this work, we introduce the problem of Multilingual Authorship Attribution, which involves attributing texts to human or multiple LLM generators across diverse languages. Focusing on 18 languages -- covering multiple families and writing scripts -- and 8 generators (7 LLMs and the human-authored class), we investigate the multilingual suitability of monolingual AA methods, their cross-lingual transferability, and the impact of generators on attribution performance. Our results reveal that while certain monolingual AA methods can be adapted to multilingual settings, significant limitations and challenges remain, particularly in transferring across diverse language families, underscoring the complexity of multilingual AA and the need for more robust approaches to better match real-world scenarios.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†å¤šè¯­è¨€ç¯å¢ƒä¸‹æœºå™¨ç”Ÿæˆæ–‡æœ¬çš„ Authorship Attribution (AA) é—®é¢˜ï¼Œæ—¨åœ¨ä»å¤šç§è¯­è¨€ä¸­è¯†åˆ«å‡ºæ–‡æœ¬çš„å…·ä½“ç”Ÿæˆæ¥æºï¼ŒåŒ…æ‹¬äººç±»æˆ–ç‰¹å®šçš„å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs)ã€‚é’ˆå¯¹ç›®å‰è¯¥é¢†åŸŸå¤šå±€é™äºå•è¯­è¨€ï¼ˆå°¤å…¶æ˜¯è‹±è¯­ï¼‰ä¸”ä¸»è¦é›†ä¸­åœ¨äºŒåˆ†ç±»æ£€æµ‹çš„ç°çŠ¶ï¼Œä½œè€…å¼•å…¥äº† Multilingual Authorship Attribution è¿™ä¸€ä»»åŠ¡ï¼Œç ”ç©¶èŒƒå›´æ¶µç›–äº†æ¥è‡ªä¸åŒè¯­ç³»å’Œä¹¦å†™ç³»ç»Ÿçš„18ç§è¯­è¨€ä»¥åŠ8ä¸ªç”Ÿæˆæ¥æºã€‚é€šè¿‡è¯„ä¼°å•è¯­è¨€ AA æ–¹æ³•åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­çš„é€‚ç”¨æ€§ã€è·¨è¯­è¨€è¿ç§»èƒ½åŠ›ä»¥åŠä¸åŒç”Ÿæˆå™¨å¯¹æ€§èƒ½çš„å½±å“ï¼Œç ”ç©¶å‘ç°è™½ç„¶éƒ¨åˆ†æ–¹æ³•å¯ä»¥é€‚é…å¤šè¯­è¨€è®¾ç½®ï¼Œä½†åœ¨è·¨è¯­ç³»è¿ç§»æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—å±€é™ã€‚è¯¥é¡¹å·¥ä½œæ­ç¤ºäº†å¤šè¯­è¨€ AA ä»»åŠ¡çš„å¤æ‚æ€§ï¼Œå¹¶å¼ºè°ƒäº†å¼€å‘æ›´å…·é²æ£’æ€§çš„æ–¹æ³•ä»¥åŒ¹é…ç°å®ä¸–ç•Œå¤šæ ·åŒ–åº”ç”¨åœºæ™¯çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "physics.soc-ph"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01656v1",
      "published_date": "2025-08-03 08:28:02 UTC",
      "updated_date": "2025-08-03 08:28:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:34:21.086298+00:00"
    },
    {
      "arxiv_id": "2508.01653v1",
      "title": "MAP: Mitigating Hallucinations in Large Vision-Language Models with Map-Level Attention Processing",
      "title_zh": "MAPï¼šé€šè¿‡å›¾å±‚çº§æ³¨æ„åŠ›å¤„ç†ç¼“è§£å¤§è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰",
      "authors": [
        "Chenxi Li",
        "Yichen Guo",
        "Benfang Qian",
        "Jinhao You",
        "Kai Tang",
        "Yaosong Du",
        "Zonghao Zhang",
        "Xiande Huang"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have achieved impressive performance in multimodal tasks, but they still suffer from hallucinations, i.e., generating content that is grammatically accurate but inconsistent with visual inputs. In this work, we introduce a novel map-level perspective to mitigate hallucinations in LVLMs, interpreting the hidden states of the model as a 2D semantic map. We observe that factual information is widely distributed across this map, extending beyond the localized inter- or intra-layer regions targeted by most existing methods (e.g., contrastive decoding and layer-wise consistency). Building on this insight, we propose Map-Level Attention Processing (MAP), a training-free decoding method that effectively leverages factual information through attention-based map-level operations to improve factual consistency. Specifically, we employ Layer-Wise Criss-Cross Attention to progressively refine token representations at each decoding layer by aggregating tokens from both inter- and intra-layer dimensions. Additionally, a Global-Local Logit Fusion mechanism combines logits obtained before and after global attention to further refine predictions and improve accuracy. Our method consistently improves the truthfulness and performance of LVLMs across benchmarks, such as POPE, MME, and MMHal-Bench, demonstrating the potential of the map-level decoding strategy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ (LVLMs) ä¸­çš„å¹»è§‰é—®é¢˜ï¼Œæå‡ºäº† MAP (Map-Level Attention Processing) è¿™ç§æ— éœ€è®­ç»ƒçš„è§£ç æ–¹æ³•ã€‚ä½œè€…ä»åœ°å›¾çº§åˆ« (Map-Level) çš„æ–°è§†è§’å‡ºå‘ï¼Œå°†æ¨¡å‹çš„éšè—çŠ¶æ€è§†ä¸ºäºŒç»´è¯­ä¹‰å›¾ï¼Œå¹¶å‘ç°äº‹å®ä¿¡æ¯åœ¨å›¾ä¸­å¹¿æ³›åˆ†å¸ƒï¼Œè€Œéå±€é™äºç‰¹å®šå±‚æˆ–åŒºåŸŸã€‚MAP é€šè¿‡ Layer-Wise Criss-Cross Attention æœºåˆ¶ï¼Œè·¨è¶Šå±‚é—´å’Œå±‚å†…ç»´åº¦èšåˆä¿¡æ¯ä»¥ä¼˜åŒ– Token è¡¨ç¤ºã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•ç»“åˆ Global-Local Logit Fusion æœºåˆ¶è¿›ä¸€æ­¥ç²¾ç»†åŒ–é¢„æµ‹ï¼Œæ˜¾è‘—å¢å¼ºäº†ç”Ÿæˆå†…å®¹çš„å‡†ç¡®æ€§ã€‚å®éªŒè¯æ˜ï¼ŒMAP åœ¨ POPEã€MME å’Œ MMHal-Bench ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡æœ‰æ•ˆæå‡äº†æ¨¡å‹çš„äº‹å®ä¸€è‡´æ€§ã€‚è¯¥å·¥ä½œå±•ç¤ºäº†åœ°å›¾çº§åˆ«è§£ç ç­–ç•¥åœ¨æ”¹å–„å¤šæ¨¡æ€ä»»åŠ¡çœŸå®æ€§æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01653v1",
      "published_date": "2025-08-03 08:23:31 UTC",
      "updated_date": "2025-08-03 08:23:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:34:44.093720+00:00"
    },
    {
      "arxiv_id": "2508.01647v1",
      "title": "DUP: Detection-guided Unlearning for Backdoor Purification in Language Models",
      "title_zh": "DUPï¼šé¢å‘è¯­è¨€æ¨¡å‹åé—¨å‡€åŒ–çš„æ£€æµ‹å¼•å¯¼å‹é—å¿˜å­¦ä¹ ",
      "authors": [
        "Man Hu",
        "Yahui Ding",
        "Yatao Yang",
        "Liangyu Chen",
        "Yanhao Jia",
        "Shuai Zhao"
      ],
      "abstract": "As backdoor attacks become more stealthy and robust, they reveal critical weaknesses in current defense strategies: detection methods often rely on coarse-grained feature statistics, and purification methods typically require full retraining or additional clean models. To address these challenges, we propose DUP (Detection-guided Unlearning for Purification), a unified framework that integrates backdoor detection with unlearning-based purification. The detector captures feature-level anomalies by jointly leveraging class-agnostic distances and inter-layer transitions. These deviations are integrated through a weighted scheme to identify poisoned inputs, enabling more fine-grained analysis. Based on the detection results, we purify the model through a parameter-efficient unlearning mechanism that avoids full retraining and does not require any external clean model. Specifically, we innovatively repurpose knowledge distillation to guide the student model toward increasing its output divergence from the teacher on detected poisoned samples, effectively forcing it to unlearn the backdoor behavior. Extensive experiments across diverse attack methods and language model architectures demonstrate that DUP achieves superior defense performance in detection accuracy and purification efficacy. Our code is available at https://github.com/ManHu2025/DUP.",
      "tldr_zh": "é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(Language Models)åé—¨æ”»å‡»æ—¥ç›Šéšè”½ä¸”ç°æœ‰é˜²å¾¡æ‰‹æ®µå­˜åœ¨æ£€æµ‹ç²’åº¦ç²—ã€å‡€åŒ–è¿‡ç¨‹éœ€é‡è®­æˆ–ä¾èµ–å¤–éƒ¨æ¨¡å‹ç­‰ç¼ºé™·ï¼Œè¯¥ç ”ç©¶æå‡ºäº† DUP (Detection-guided Unlearning for Purification) ç»Ÿä¸€æ¡†æ¶ã€‚è¯¥æ¡†æ¶çš„æ£€æµ‹å™¨é€šè¿‡ç»“åˆç±»æ— å…³è·ç¦»(class-agnostic distances)å’Œå±‚é—´è½¬æ¢(inter-layer transitions)æ•æ‰ç‰¹å¾çº§å¼‚å¸¸ï¼Œåˆ©ç”¨åŠ æƒæ–¹æ¡ˆå®ç°ä¸­æ¯’è¾“å…¥çš„ç»†ç²’åº¦è¯†åˆ«ã€‚åœ¨å‡€åŒ–ç¯èŠ‚ï¼ŒDUP é‡‡ç”¨å‚æ•°é«˜æ•ˆçš„é—å¿˜å­¦ä¹ (Unlearning)æœºåˆ¶ï¼Œåˆ›æ–°æ€§åœ°åˆ©ç”¨çŸ¥è¯†è’¸é¦(Knowledge Distillation)å¼•å¯¼æ¨¡å‹åœ¨ä¸­æ¯’æ ·æœ¬ä¸Šå¢åŠ ä¸æ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºå·®å¼‚ï¼Œä»è€Œå¼ºåˆ¶æ¨¡å‹é—å¿˜åé—¨è¡Œä¸ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDUP åœ¨å¤šç§æ”»å‡»æ–¹æ³•å’Œæ¨¡å‹æ¶æ„ä¸‹å‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ£€æµ‹å‡†ç¡®ç‡ä¸å‡€åŒ–åŠŸæ•ˆã€‚è¯¥æ–¹æ³•æ— éœ€å®Œæ•´é‡è®­æˆ–é¢å¤–å¹²å‡€æ¨¡å‹ï¼Œä¸ºæé«˜å¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§å’Œé²æ£’æ€§æä¾›äº†é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01647v1",
      "published_date": "2025-08-03 08:12:21 UTC",
      "updated_date": "2025-08-03 08:12:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:34:40.087081+00:00"
    },
    {
      "arxiv_id": "2508.01646v2",
      "title": "SPARTA: Advancing Sparse Attention in Spiking Neural Networks via Spike-Timing-Based Prioritization",
      "title_zh": "SPARTAï¼šé€šè¿‡åŸºäºè„‰å†²å®šæ—¶çš„ä¼˜å…ˆçº§æœºåˆ¶æå‡è„‰å†²ç¥ç»ç½‘ç»œä¸­çš„ç¨€ç–æ³¨æ„åŠ›",
      "authors": [
        "Minsuk Jang",
        "Changick Kim"
      ],
      "abstract": "Current Spiking Neural Networks (SNNs) underutilize the temporal dynamics inherent in spike-based processing, relying primarily on rate coding while overlooking precise timing information that provides rich computational cues. We propose SPARTA (Spiking Priority Attention with Resource-Adaptive Temporal Allocation), a framework that leverages heterogeneous neuron dynamics and spike-timing information to enable efficient sparse attention. SPARTA prioritizes tokens based on temporal cues, including firing patterns, spike timing, and inter-spike intervals, achieving 65.4% sparsity through competitive gating. By selecting only the most salient tokens, SPARTA reduces attention complexity from O(N^2) to O(K^2) with k << n, while maintaining high accuracy. Our method achieves state-of-the-art performance on DVS-Gesture (98.78%) and competitive results on CIFAR10-DVS (83.06%) and CIFAR-10 (95.3%), demonstrating that exploiting spike timing dynamics improves both computational efficiency and accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰è„‰å†²ç¥ç»ç½‘ç»œ(SNNs)ä¸»è¦ä¾èµ–é¢‘ç‡ç¼–ç (rate coding)è€Œå¿½è§†ç²¾ç¡®è„‰å†²å®šæ—¶(spike timing)ä¿¡æ¯çš„é—®é¢˜ï¼Œæå‡ºäº†SPARTA(Spiking Priority Attention with Resource-Adaptive Temporal Allocation)æ¡†æ¶ã€‚SPARTAåˆ©ç”¨å¼‚è´¨ç¥ç»å…ƒåŠ¨åŠ›å­¦å’Œè„‰å†²å®šæ—¶ä¿¡æ¯(å¦‚å‘æ”¾æ¨¡å¼å’Œè„‰å†²é—´é—´éš”)æ¥å®ç°é«˜æ•ˆçš„ç¨€ç–æ³¨æ„åŠ›(sparse attention)ï¼Œé€šè¿‡ç«äº‰é—¨æ§(competitive gating)æœºåˆ¶ç­›é€‰æ˜¾è‘—ç‰¹å¾ï¼ŒæˆåŠŸå®ç°äº†65.4%çš„ç¨€ç–åº¦ã€‚è¿™ä¸€åˆ›æ–°æœºåˆ¶å°†æ³¨æ„åŠ›æœºåˆ¶çš„å¤æ‚åº¦ä»O(N^2)å¤§å¹…é™ä½è‡³O(K^2)ï¼Œåœ¨ä¿æŒé«˜ç²¾åº¦çš„åŒæ—¶æ˜¾è‘—æå‡äº†è®¡ç®—æ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨DVS-Gestureæ•°æ®é›†ä¸Šè¾¾åˆ°äº†98.78%çš„å…ˆè¿›æ€§èƒ½ï¼Œå¹¶åœ¨CIFAR10-DVSå’ŒCIFAR-10ä¸Šä¹Ÿå–å¾—äº†æå…·ç«äº‰åŠ›çš„ç»“æœã€‚è¿™é¡¹å·¥ä½œå……åˆ†è¯æ˜äº†æŒ–æ˜è„‰å†²å®šæ—¶åŠ¨æ€ç‰¹å¾å¯¹äºåŒæ—¶æå‡SNNsè®¡ç®—æ•ˆç‡å’Œä»»åŠ¡å‡†ç¡®æ€§çš„é‡è¦ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01646v2",
      "published_date": "2025-08-03 08:11:33 UTC",
      "updated_date": "2025-08-08 16:16:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:34:40.589782+00:00"
    },
    {
      "arxiv_id": "2508.01644v1",
      "title": "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition",
      "title_zh": "DRKFï¼šèåˆçŸ¥è¯†ä¸è§£è€¦è¡¨ç¤ºçš„å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«",
      "authors": [
        "Peiyuan Jiang",
        "Yao Liu",
        "Qiao Liu",
        "Zongshun Zhang",
        "Jiaye Yang",
        "Lu Liu",
        "Daibing Yao"
      ],
      "abstract": "Multimodal emotion recognition (MER) aims to identify emotional states by integrating and analyzing information from multiple modalities. However, inherent modality heterogeneity and inconsistencies in emotional cues remain key challenges that hinder performance. To address these issues, we propose a Decoupled Representations with Knowledge Fusion (DRKF) method for MER. DRKF consists of two main modules: an Optimized Representation Learning (ORL) Module and a Knowledge Fusion (KF) Module. ORL employs a contrastive mutual information estimation method with progressive modality augmentation to decouple task-relevant shared representations and modality-specific features while mitigating modality heterogeneity. KF includes a lightweight self-attention-based Fusion Encoder (FE) that identifies the dominant modality and integrates emotional information from other modalities to enhance the fused representation. To handle potential errors from incorrect dominant modality selection under emotionally inconsistent conditions, we introduce an Emotion Discrimination Submodule (ED), which enforces the fused representation to retain discriminative cues of emotional inconsistency. This ensures that even if the FE selects an inappropriate dominant modality, the Emotion Classification Submodule (EC) can still make accurate predictions by leveraging preserved inconsistency information. Experiments show that DRKF achieves state-of-the-art (SOTA) performance on IEMOCAP, MELD, and M3ED. The source code is publicly available at https://github.com/PANPANKK/DRKF.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«(Multimodal Emotion Recognition)ä¸­æ¨¡æ€å¼‚è´¨æ€§å’Œæƒ…æ„Ÿçº¿ç´¢ä¸ä¸€è‡´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†DRKFï¼ˆDecoupled Representations with Knowledge Fusionï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç”±ä¼˜åŒ–è¡¨ç¤ºå­¦ä¹ (Optimized Representation Learning, ORL)å’ŒçŸ¥è¯†èåˆ(Knowledge Fusion, KF)ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ç»„æˆã€‚ORLæ¨¡å—é‡‡ç”¨å¯¹æ¯”äº’ä¿¡æ¯ä¼°è®¡å’Œæ¸è¿›å¼æ¨¡æ€å¢å¼ºæ–¹æ³•ï¼Œå°†ä»»åŠ¡ç›¸å…³çš„å…±äº«è¡¨ç¤ºä¸æ¨¡æ€ç‰¹æœ‰ç‰¹å¾è§£è€¦ï¼Œæœ‰æ•ˆç¼“è§£äº†æ¨¡æ€å¼‚è´¨æ€§é—®é¢˜ã€‚KFæ¨¡å—åˆ™é€šè¿‡è½»é‡çº§è‡ªæ³¨æ„åŠ›èåˆç¼–ç å™¨(Fusion Encoder)è¯†åˆ«ä¸»å¯¼æ¨¡æ€å¹¶æ•´åˆè·¨æ¨¡æ€ä¿¡æ¯ã€‚é’ˆå¯¹æƒ…æ„Ÿä¸ä¸€è‡´åœºæ™¯ï¼Œç ”ç©¶å¼•å…¥äº†æƒ…æ„Ÿåˆ¤åˆ«å­æ¨¡å—(Emotion Discrimination Submodule, ED)ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä¿ç•™åˆ¤åˆ«æ€§çš„ä¸ä¸€è‡´çº¿ç´¢ï¼Œç¡®ä¿å³ä½¿åœ¨ä¸»å¯¼æ¨¡æ€é€‰æ‹©ä¸å½“çš„æƒ…å†µä¸‹ï¼Œæƒ…æ„Ÿåˆ†ç±»å­æ¨¡å—(EC)ä»èƒ½åšå‡ºå‡†ç¡®é¢„æµ‹ã€‚å®éªŒè¯æ˜ï¼ŒDRKFåœ¨IEMOCAPã€MELDå’ŒM3EDæ•°æ®é›†ä¸Šå‡å–å¾—äº†å½“å‰æœ€å…ˆè¿›(SOTA)çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CV",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.MM",
      "comment": "Published in ACM Multimedia 2025. 10 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.01644v1",
      "published_date": "2025-08-03 08:05:57 UTC",
      "updated_date": "2025-08-03 08:05:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:34:57.746295+00:00"
    },
    {
      "arxiv_id": "2508.05668v3",
      "title": "A Survey of LLM-based Deep Search Agents: Paradigm, Optimization, Evaluation, and Challenges",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ·±åº¦æœç´¢æ™ºèƒ½ä½“ç»¼è¿°ï¼šèŒƒå¼ã€ä¼˜åŒ–ã€è¯„ä¼°ä¸æŒ‘æˆ˜",
      "authors": [
        "Yunjia Xi",
        "Jianghao Lin",
        "Yongzhao Xiao",
        "Zheli Zhou",
        "Rong Shan",
        "Te Gao",
        "Jiachen Zhu",
        "Weiwen Liu",
        "Yong Yu",
        "Weinan Zhang"
      ],
      "abstract": "The advent of Large Language Models (LLMs) has significantly revolutionized web search. The emergence of LLM-based Search Agents marks a pivotal shift towards deeper, dynamic, autonomous information seeking. These agents can comprehend user intentions and environmental context and execute multi-turn retrieval with dynamic planning, extending search capabilities far beyond the web. Leading examples like OpenAI's Deep Research highlight their potential for deep information mining and real-world applications. This survey provides the first systematic analysis of search agents. We comprehensively analyze and categorize existing works from the perspectives of architecture, optimization, application, and evaluation, ultimately identifying critical open challenges and outlining promising future research directions in this rapidly evolving field. Our repository is available on https://github.com/YunjiaXi/Awesome-Search-Agent-Papers.",
      "tldr_zh": "è¯¥ç»¼è¿°ç³»ç»Ÿåœ°åˆ†æäº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ·±åº¦æœç´¢æ™ºèƒ½ä½“(Search Agents)ï¼Œæ ‡å¿—ç€ä¿¡æ¯æ£€ç´¢å‘æ›´æ·±å±‚æ¬¡ã€åŠ¨æ€ä¸”è‡ªä¸»å¯»ä¼˜çš„è½¬å˜ã€‚ä¸ä¼ ç»Ÿæœç´¢ä¸åŒï¼Œè¿™äº›æ™ºèƒ½ä½“åˆ©ç”¨åŠ¨æ€è§„åˆ’(Dynamic Planning)å’Œå¤šè½®æ£€ç´¢(Multi-turn Retrieval)æ¥æ·±å…¥ç†è§£ç”¨æˆ·æ„å›¾å¹¶æ‰§è¡Œå¤æ‚çš„ä¿¡æ¯æŒ–æ˜ä»»åŠ¡ã€‚è¯¥ç ”ç©¶é€šè¿‡å¯¹æ¶æ„(Architecture)ã€ä¼˜åŒ–(Optimization)ã€åº”ç”¨(Application)å’Œè¯„ä¼°(Evaluation)å››ä¸ªç»´åº¦çš„åˆ†ç±»ä¸æ¢³ç†ï¼Œå…¨é¢æ€»ç»“äº†ç°æœ‰å·¥ä½œåŠå…¶æŠ€æœ¯èŒƒå¼ã€‚æ–‡ç« ä»¥OpenAIçš„Deep Researchç­‰å‰æ²¿ç³»ç»Ÿä¸ºä¾‹ï¼Œå±•ç¤ºäº†æœç´¢æ™ºèƒ½ä½“åœ¨ç°å®åœºæ™¯ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚æœ€åï¼Œè¯¥ç»¼è¿°è¯†åˆ«äº†è¯¥é¢†åŸŸé¢ä¸´çš„å…³é”®å…¬å¼€æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†æŒ‡å¯¼æ€§å»ºè®®ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05668v3",
      "published_date": "2025-08-03 08:02:51 UTC",
      "updated_date": "2025-08-19 05:15:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:35:11.970353+00:00"
    },
    {
      "arxiv_id": "2508.01638v1",
      "title": "Semantic Encryption: Secure and Effective Interaction with Cloud-based Large Language Models via Semantic Transformation",
      "title_zh": "è¯­ä¹‰åŠ å¯†ï¼šé€šè¿‡è¯­ä¹‰è½¬æ¢å®ç°ä¸äº‘ç«¯å¤§è¯­è¨€æ¨¡å‹å®‰å…¨ä¸”æœ‰æ•ˆçš„äº¤äº’",
      "authors": [
        "Dong Chen",
        "Tong Yang",
        "Feipeng Zhai",
        "Pengpeng Ouyang",
        "Qidong Liu",
        "Yafei Li",
        "Chong Fu",
        "Mingliang Xu"
      ],
      "abstract": "The increasing adoption of Cloud-based Large Language Models (CLLMs) has raised significant concerns regarding data privacy during user interactions. While existing approaches primarily focus on encrypting sensitive information, they often overlook the logical structure of user inputs. This oversight can lead to reduced data utility and degraded performance of CLLMs. To address these limitations and enable secure yet effective interactions, we propose Semantic Encryption (SE)-a plug-and-play framework designed to preserve both privacy and utility. SE consists of two key components: Semantic Encoding and Semantic Decoding. In the encoding phase, a lightweight local model transforms the original user input into an alternative semantic context that maintains the original intent and logical structure while obfuscating sensitive information. This transformed input is then processed by the CLLM, which generates a response based on the transformed semantic context. To maintain a seamless user experience, the decoding phase will reconstruct the CLLM's response back into the original semantic context by referencing the locally stored user input. Extensive experimental evaluations demonstrate that SE effectively protects data privacy without compromising data utility or user experience, offering a practical solution for secure interaction with CLLMs. Particularly, the proposed SE demonstrates a significant improvement over the state-of-the-art InferDPT, surpassing it across various evaluated metrics and datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºäº‘ç«¯çš„å¤§è¯­è¨€æ¨¡å‹ (Cloud-based Large Language Models, CLLMs) åœ¨äº¤äº’è¿‡ç¨‹ä¸­çš„æ•°æ®éšç§æ³„éœ²é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºè¯­ä¹‰åŠ å¯† (Semantic Encryption, SE) çš„å³æ’å³ç”¨æ¡†æ¶ã€‚ä¼ ç»Ÿçš„åŠ å¯†æ–¹æ³•å¾€å¾€å¿½ç•¥äº†ç”¨æˆ·è¾“å…¥çš„é€»è¾‘ç»“æ„ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ï¼Œè€Œ SE æ¡†æ¶é€šè¿‡è¯­ä¹‰ç¼–ç  (Semantic Encoding) å’Œè¯­ä¹‰è§£ç  (Semantic Decoding) ä¸¤ä¸ªå…³é”®ç»„ä»¶åœ¨ä¿æŠ¤éšç§çš„åŒæ—¶ç»´æŒäº†æ•°æ®æ•ˆç”¨ã€‚åœ¨ç¼–ç é˜¶æ®µï¼Œè¯¥æ¡†æ¶åˆ©ç”¨è½»é‡çº§æœ¬åœ°æ¨¡å‹å°†åŸå§‹è¾“å…¥è½¬æ¢ä¸ºå¦ä¸€ç§è¯­ä¹‰ä¸Šä¸‹æ–‡ï¼Œåœ¨æ··æ·†æ•æ„Ÿä¿¡æ¯çš„åŒæ—¶ä¿ç•™äº†åŸå§‹æ„å›¾å’Œé€»è¾‘ç»“æ„ã€‚éšåï¼Œäº‘ç«¯æ¨¡å‹å¤„ç†è½¬æ¢åçš„è¾“å…¥ï¼Œå¹¶ç”±è§£ç é˜¶æ®µé€šè¿‡å¼•ç”¨æœ¬åœ°å­˜å‚¨çš„åŸå§‹ä¿¡æ¯ï¼Œå°†å“åº”é‡æ–°æ„å»ºå›åˆå§‹è¯­ä¹‰è¯­å¢ƒã€‚å¹¿æ³›çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒSE èƒ½å¤Ÿæœ‰æ•ˆä¿æŠ¤æ•°æ®éšç§ä¸”ä¸æŸå®³æ•°æ®æ•ˆç”¨æˆ–ç”¨æˆ·ä½“éªŒã€‚ç‰¹åˆ«åœ°ï¼Œè¯¥æ–¹æ¡ˆåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡å’Œæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³• InferDPTï¼Œä¸ºä¸ CLLMs è¿›è¡Œå®‰å…¨äº¤äº’æä¾›äº†åˆ‡å®å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01638v1",
      "published_date": "2025-08-03 07:54:40 UTC",
      "updated_date": "2025-08-03 07:54:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:35:15.638031+00:00"
    },
    {
      "arxiv_id": "2508.01635v1",
      "title": "Learning Unified System Representations for Microservice Tail Latency Prediction",
      "title_zh": "é¢å‘å¾®æœåŠ¡å°¾éƒ¨å»¶è¿Ÿé¢„æµ‹çš„ç»Ÿä¸€ç³»ç»Ÿè¡¨ç¤ºå­¦ä¹ ",
      "authors": [
        "Wenzhuo Qian",
        "Hailiang Zhao",
        "Tianlv Chen",
        "Jiayi Chen",
        "Ziqi Wang",
        "Kingsum Chow",
        "Shuiguang Deng"
      ],
      "abstract": "Microservice architectures have become the de facto standard for building scalable cloud-native applications, yet their distributed nature introduces significant challenges in performance monitoring and resource management. Traditional approaches often rely on per-request latency metrics, which are highly sensitive to transient noise and fail to reflect the holistic behavior of complex, concurrent workloads. In contrast, window-level P95 tail latency provides a stable and meaningful signal that captures both system-wide trends and user-perceived performance degradation. We identify two key shortcomings in existing methods: (i) inadequate handling of heterogeneous data, where traffic-side features propagate across service dependencies and resource-side signals reflect localized bottlenecks, and (ii) the lack of principled architectural designs that effectively distinguish and integrate these complementary modalities. To address these challenges, we propose USRFNet, a deep learning network that explicitly separates and models traffic-side and resource-side features. USRFNet employs GNNs to capture service interactions and request propagation patterns, while gMLP modules independently model cluster resource dynamics. These representations are then fused into a unified system embedding to predict window-level P95 latency with high accuracy. We evaluate USRFNet on real-world microservice benchmarks under large-scale stress testing conditions, demonstrating substantial improvements in prediction accuracy over state-of-the-art baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¾®æœåŠ¡æ¶æ„ï¼ˆMicroservice architecturesï¼‰åœ¨æ€§èƒ½ç›‘æ§ä¸­é¢ä¸´çš„å¤æ‚æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º USRFNet çš„æ·±åº¦å­¦ä¹ ç½‘ç»œï¼Œæ—¨åœ¨ç²¾ç¡®é¢„æµ‹çª—å£çº§ P95 å°¾éƒ¨å»¶è¿Ÿï¼ˆP95 tail latencyï¼‰ã€‚ä½œè€…æŒ‡å‡ºç°æœ‰æ–¹æ³•åœ¨å¤„ç†æµé‡ä¾§ç‰¹å¾ä¸èµ„æºä¾§ä¿¡å·ç­‰å¼‚æ„æ•°æ®æ—¶å­˜åœ¨ä¸è¶³ï¼Œä¸”ç¼ºä¹èƒ½æœ‰æ•ˆæ•´åˆè¿™äº›äº’è¡¥æ¨¡æ€çš„ç³»ç»Ÿæ€§æ¶æ„ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼ŒUSRFNet é‡‡ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰æ¥æ•è·æœåŠ¡é—´çš„äº¤äº’ä¸è¯·æ±‚ä¼ æ’­æ¨¡å¼ï¼ŒåŒæ—¶åˆ©ç”¨ gMLP æ¨¡å—ç‹¬ç«‹å»ºæ¨¡é›†ç¾¤çš„èµ„æºåŠ¨æ€ã€‚è¿™äº›å¼‚æ„è¡¨ç¤ºéšåè¢«èåˆä¸ºç»Ÿä¸€çš„ç³»ç»ŸåµŒå…¥ï¼Œä»è€Œå®ç°å¯¹ç³»ç»Ÿæ•´ä½“è¡Œä¸ºå’Œæ€§èƒ½é€€åŒ–çš„å‡†ç¡®åˆ»ç”»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤§è§„æ¨¡å‹åŠ›æµ‹è¯•çš„çœŸå®å¾®æœåŠ¡åŸºå‡†æ•°æ®é›†ä¸Šï¼ŒUSRFNet çš„é¢„æµ‹å‡†ç¡®ç‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºå‡†æ¨¡å‹ï¼ˆSOTAï¼‰ï¼Œä¸ºäº‘åŸç”Ÿåº”ç”¨çš„é«˜æ•ˆèµ„æºç®¡ç†æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "cs.PF"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01635v1",
      "published_date": "2025-08-03 07:46:23 UTC",
      "updated_date": "2025-08-03 07:46:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:35:15.291444+00:00"
    },
    {
      "arxiv_id": "2508.01630v1",
      "title": "OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets",
      "title_zh": "OpenMed NERï¼šè·¨ 12 ä¸ªå…¬å¼€æ•°æ®é›†å®ç°ç”Ÿç‰©åŒ»å­¦å‘½åå®ä½“è¯†åˆ«æœ€å…ˆè¿›æ€§èƒ½çš„å¼€æºé¢†åŸŸè‡ªé€‚åº” Transformer æ¨¡å‹",
      "authors": [
        "Maziyar Panahi"
      ],
      "abstract": "Named-entity recognition (NER) is fundamental to extracting structured information from the >80% of healthcare data that resides in unstructured clinical notes and biomedical literature. Despite recent advances with large language models, achieving state-of-the-art performance across diverse entity types while maintaining computational efficiency remains a significant challenge. We introduce OpenMed NER, a suite of open-source, domain-adapted transformer models that combine lightweight domain-adaptive pre-training (DAPT) with parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs cost-effective DAPT on a 350k-passage corpus compiled from ethically sourced, publicly available research repositories and de-identified clinical notes (PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA backbones. This is followed by task-specific fine-tuning with LoRA, which updates less than 1.5% of model parameters. We evaluate our models on 12 established biomedical NER benchmarks spanning chemicals, diseases, genes, and species. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of these 12 datasets, with substantial gains across diverse entity types. Our models advance the state-of-the-art on foundational disease and chemical benchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger improvements of over 5.3 and 9.7 percentage points on more specialized gene and clinical cell line corpora. This work demonstrates that strategically adapted open-source models can surpass closed-source solutions. This performance is achieved with remarkable efficiency: training completes in under 12 hours on a single GPU with a low carbon footprint (< 1.2 kg CO2e), producing permissively licensed, open-source checkpoints designed to help practitioners facilitate compliance with emerging data protection and AI regulations, such as the EU AI Act.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† OpenMed NERï¼Œè¿™æ˜¯ä¸€å¥—ä¸“ä¸ºç”Ÿç‰©åŒ»å­¦å‘½åå®ä½“è¯†åˆ« (Named-entity recognition, NER) è®¾è®¡çš„å¼€æºã€é¢†åŸŸè‡ªé€‚åº” Transformer æ¨¡å‹ç³»åˆ—ã€‚è¯¥æ¡†æ¶ç»“åˆäº†åœ¨ PubMedã€arXiv å’Œ MIMIC-III è¯­æ–™åº“ä¸Šè¿›è¡Œçš„è½»é‡çº§é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒ (Domain-adaptive pre-training, DAPT) ä»¥åŠå‚æ•°é«˜æ•ˆçš„ä½ç§©è‡ªé€‚åº” (Low-Rank Adaptation, LoRA) å¾®è°ƒæŠ€æœ¯ã€‚é€šè¿‡é‡‡ç”¨ DeBERTa-v3ã€PubMedBERT å’Œ BioELECTRA ä½œä¸ºéª¨å¹²ç½‘ç»œï¼ŒOpenMed NER åœ¨ 12 ä¸ªæ¶µç›–åŒ–å­¦å“ã€ç–¾ç—…ã€åŸºå› å’Œç‰©ç§çš„ç”Ÿç‰©åŒ»å­¦ NER åŸºå‡†æ•°æ®é›†ä¸­çš„ 10 ä¸ªä¸Šåˆ·æ–°äº†å½“å‰æœ€å…ˆè¿›çš„ (State-of-the-art) micro-F1 åˆ†æ•°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨åŸºå› å’Œä¸´åºŠç»†èƒç³»ç­‰ç‰¹å®šè¯­æ–™åº“ä¸Šçš„æ€§èƒ½æå‡æ˜¾è‘—ï¼Œåˆ†åˆ«è¶…è¿‡äº† 5.3 å’Œ 9.7 ä¸ªç™¾åˆ†ç‚¹ã€‚ç ”ç©¶è¯æ˜äº†ç»è¿‡ç­–ç•¥æ€§ä¼˜åŒ–çš„å¼€æºæ¨¡å‹èƒ½å¤Ÿè¶…è¶Šé—­æºæ–¹æ¡ˆï¼Œä¸”å…·å¤‡æé«˜çš„è®­ç»ƒæ•ˆç‡ï¼Œåœ¨å•ä¸ª GPU ä¸Šä»…éœ€ä¸åˆ° 12 å°æ—¶å³å¯å®Œæˆè®­ç»ƒï¼Œç¢³è¶³è¿¹æä½ã€‚æ­¤å¤–ï¼ŒOpenMed NER æä¾›å®½æ¾çš„å¼€æºè®¸å¯åè®®ï¼Œæ—¨åœ¨åŠ©åŠ›ä»ä¸šè€…åœ¨æ»¡è¶³æ•°æ®éšç§å’Œæ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆ (EU AI Act) ç­‰ç›‘ç®¡è¦æ±‚çš„å‰æä¸‹ï¼Œé«˜æ•ˆæå–éç»“æ„åŒ–ä¸´åºŠæ•°æ®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01630v1",
      "published_date": "2025-08-03 07:33:28 UTC",
      "updated_date": "2025-08-03 07:33:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:35:27.234130+00:00"
    },
    {
      "arxiv_id": "2508.01625v1",
      "title": "EAC-MoE: Expert-Selection Aware Compressor for Mixture-of-Experts Large Language Models",
      "title_zh": "EAC-MoEï¼šé¢å‘æ··åˆä¸“å®¶å¤§è¯­è¨€æ¨¡å‹çš„ä¸“å®¶é€‰æ‹©æ„ŸçŸ¥å‹ç¼©å™¨",
      "authors": [
        "Yuanteng Chen",
        "Yuantian Shao",
        "Peisong Wang",
        "Jian Cheng"
      ],
      "abstract": "Mixture-of-Experts (MoE) has demonstrated promising potential in scaling LLMs. However, it is hindered by two critical challenges: (1) substantial GPU memory consumption to load all experts; (2) low activated parameters cannot be equivalently translated into inference acceleration effects. In this work, we propose EAC-MoE, an Expert-Selection Aware Compressor for MoE-LLMs, which deeply aligns with the characteristics of MoE from the perspectives of quantization and pruning, and introduces two modules to address these two challenges respectively: (1) The expert selection bias caused by low-bit quantization is a major factor contributing to the performance degradation in MoE-LLMs. Based on this, we propose Quantization with Expert-Selection Calibration (QESC), which mitigates the expert selection bias by calibrating the routers within the MoE; (2) There are always certain experts that are not crucial for the corresponding tasks, yet causing inference latency. Therefore, we propose Pruning based on Expert-Selection Frequency (PESF), which significantly improves inference speed by pruning less frequently used experts for current task. Extensive experiments demonstrate that our approach significantly reduces memory usage and improves inference speed with minimal performance degradation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† EAC-MoEï¼Œä¸€ç§ä¸“ä¸º Mixture-of-Experts (MoE) æ¶æ„çš„å¤§å‹è¯­è¨€æ¨¡å‹è®¾è®¡çš„ä¸“å®¶é€‰æ‹©æ„ŸçŸ¥å‹ç¼©æ–¹æ¡ˆï¼Œæ—¨åœ¨è§£å†³æ¨¡å‹æ˜¾å­˜å ç”¨å·¨å¤§ä»¥åŠæ¨ç†åŠ é€Ÿä¸æ˜æ˜¾çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚EAC-MoE é€šè¿‡é‡åŒ–ä¸å‰ªæçš„æ·±åº¦ååŒï¼Œå¼•å…¥äº†ä¸“å®¶é€‰æ‹©æ ¡å‡†é‡åŒ– (Quantization with Expert-Selection Calibration, QESC) æ¨¡å—ï¼Œé€šè¿‡æ ¡å‡†è·¯ç”±å™¨æ¥ä¿®å¤ä½æ¯”ç‰¹é‡åŒ–å¸¦æ¥çš„ä¸“å®¶é€‰æ‹©åå·®ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ¡ˆåˆ©ç”¨åŸºäºä¸“å®¶é€‰æ‹©é¢‘ç‡çš„å‰ªæ (Pruning based on Expert-Selection Frequency, PESF) æŠ€æœ¯ï¼Œå‰”é™¤ä»»åŠ¡æ— å…³çš„å†—ä½™ä¸“å®¶ä»¥æ˜¾è‘—æå‡æ¨ç†æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEAC-MoE åœ¨ç»´æŒæ¨¡å‹æ€§èƒ½åŸºæœ¬ä¸å˜çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆé™ä½äº† GPU æ˜¾å­˜éœ€æ±‚å¹¶å¤§å¹…åŠ é€Ÿäº†æ¨ç†è¿‡ç¨‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "22 pages, 13 figures. ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.01625v1",
      "published_date": "2025-08-03 07:30:42 UTC",
      "updated_date": "2025-08-03 07:30:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:36:00.888982+00:00"
    },
    {
      "arxiv_id": "2508.01623v1",
      "title": "A Multi-Agent Pokemon Tournament for Evaluating Strategic Reasoning of Large Language Models",
      "title_zh": "è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ç­–ç•¥æ€§æ¨ç†çš„å¤šæ™ºèƒ½ä½“ Pokemon é”¦æ ‡èµ›",
      "authors": [
        "Tadisetty Sai Yashwanth",
        "Dhatri C"
      ],
      "abstract": "This research presents LLM Pokemon League, a competitive tournament system that leverages Large Language Models (LLMs) as intelligent agents to simulate strategic decision-making in PokÃ©mon battles. The platform is designed to analyze and compare the reasoning, adaptability, and tactical depth exhibited by different LLMs in a type-based, turn-based combat environment. By structuring the competition as a single-elimination tournament involving diverse AI trainers, the system captures detailed decision logs, including team-building rationale, action selection strategies, and switching decisions. The project enables rich exploration into comparative AI behavior, battle psychology, and meta-strategy development in constrained, rule-based game environments. Through this system, we investigate how modern LLMs understand, adapt, and optimize decisions under uncertainty, making PokÃ©mon League a novel benchmark for AI research in strategic reasoning and competitive learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LLM Pokemon Leagueï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºæ™ºèƒ½ä½“çš„ç«æŠ€å¯¹æˆ˜ç³»ç»Ÿï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨å®å¯æ¢¦ï¼ˆPokÃ©monï¼‰å¯¹æˆ˜ä¸­çš„ç­–ç•¥æ¨ç†èƒ½åŠ›ã€‚è¯¥å¹³å°é€šè¿‡åˆ†æä¸åŒLLMåœ¨å±æ€§å…‹åˆ¶å’Œå›åˆåˆ¶æˆ˜æ–—ç¯å¢ƒä¸‹çš„è¡¨ç°ï¼Œæ·±å…¥æ¢è®¨å…¶æ¨ç†ã€é€‚åº”æ€§å’Œæˆ˜æœ¯æ·±åº¦ã€‚ç ”ç©¶é‡‡ç”¨å•è´¥æ·˜æ±°èµ›ï¼ˆsingle-elimination tournamentï¼‰ç»“æ„ï¼Œè®°å½•äº†åŒ…æ‹¬å›¢é˜Ÿæ„å»ºé€»è¾‘ã€åŠ¨ä½œé€‰æ‹©åŠåˆ‡æ¢å†³ç­–åœ¨å†…çš„è¯¦ç»†å†³ç­–æ—¥å¿—ã€‚è¯¥é¡¹ç›®æ¢ç´¢äº†AIè¡Œä¸ºã€å¯¹æˆ˜å¿ƒç†å­¦ä»¥åŠåœ¨å—é™è§„åˆ™ç¯å¢ƒä¸‹çš„å…ƒç­–ç•¥ï¼ˆmeta-strategyï¼‰å¼€å‘ï¼Œå¹¶è°ƒæŸ¥äº†ç°ä»£LLMåœ¨ä¸ç¡®å®šæ€§ä¸‹ä¼˜åŒ–å†³ç­–çš„èƒ½åŠ›ã€‚æœ€ç»ˆï¼ŒLLM Pokemon Leagueä¸ºç­–ç•¥æ¨ç†ï¼ˆstrategic reasoningï¼‰å’Œç«äº‰æ€§å­¦ä¹ ï¼ˆcompetitive learningï¼‰é¢†åŸŸçš„AIç ”ç©¶æä¾›äº†ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†ï¼ˆbenchmarkï¼‰ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01623v1",
      "published_date": "2025-08-03 07:27:36 UTC",
      "updated_date": "2025-08-03 07:27:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:35:30.931882+00:00"
    },
    {
      "arxiv_id": "2508.01615v1",
      "title": "TCDiff: Triplex Cascaded Diffusion for High-fidelity Multimodal EHRs Generation with Incomplete Clinical Data",
      "title_zh": "TCDiffï¼šé¢å‘ä¸å®Œæ•´ä¸´åºŠæ•°æ®çš„é«˜ä¿çœŸå¤šæ¨¡æ€ç”µå­å¥åº·è®°å½•ç”Ÿæˆä¸‰é‡çº§è”æ‰©æ•£",
      "authors": [
        "Yandong Yan",
        "Chenxi Li",
        "Yu Huang",
        "Dexuan Xu",
        "Jiaqi Zhu",
        "Zhongyan Chai",
        "Huamin Zhang"
      ],
      "abstract": "The scarcity of large-scale and high-quality electronic health records (EHRs) remains a major bottleneck in biomedical research, especially as large foundation models become increasingly data-hungry. Synthesizing substantial volumes of de-identified and high-fidelity data from existing datasets has emerged as a promising solution. However, existing methods suffer from a series of limitations: they struggle to model the intrinsic properties of heterogeneous multimodal EHR data (e.g., continuous, discrete, and textual modalities), capture the complex dependencies among them, and robustly handle pervasive data incompleteness. These challenges are particularly acute in Traditional Chinese Medicine (TCM). To this end, we propose TCDiff (Triplex Cascaded Diffusion Network), a novel EHR generation framework that cascades three diffusion networks to learn the features of real-world EHR data, formatting a multi-stage generative process: Reference Modalities Diffusion, Cross-Modal Bridging, and Target Modality Diffusion. Furthermore, to validate our proposed framework, besides two public datasets, we also construct and introduce TCM-SZ1, a novel multimodal EHR dataset for benchmarking. Experimental results show that TCDiff consistently outperforms state-of-the-art baselines by an average of 10% in data fidelity under various missing rate, while maintaining competitive privacy guarantees. This highlights the effectiveness, robustness, and generalizability of our approach in real-world healthcare scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿç‰©åŒ»å­¦é¢†åŸŸé«˜è´¨é‡ç”µå­å¥åº·è®°å½• (EHRs) ç¨€ç¼ºä¸”ç°æœ‰ç”Ÿæˆæ¨¡å‹éš¾ä»¥å¤„ç†å¼‚æ„å¤šæ¨¡æ€æ•°æ®åŠæ•°æ®ä¸å®Œæ•´æ€§çš„é—®é¢˜ï¼Œæå‡ºäº† TCDiff (Triplex Cascaded Diffusion Network) æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡çº§è”ä¸‰ä¸ªæ‰©æ•£ç½‘ç»œï¼Œå³ Reference Modalities Diffusionã€Cross-Modal Bridging å’Œ Target Modality Diffusionï¼Œæ„å»ºäº†ä¸€ä¸ªå¤šé˜¶æ®µç”Ÿæˆè¿‡ç¨‹ï¼Œä»¥æœ‰æ•ˆæ•æ‰å¤æ‚çš„å¤šæ¨¡æ€ä¾èµ–å…³ç³»å¹¶å¤„ç†æ•°æ®ç¼ºå¤±ã€‚ä¸ºäº†éªŒè¯æ¨¡å‹åœ¨å¤„ç† Traditional Chinese Medicine (TCM) ç­‰é¢†åŸŸä¸¥å³»æ•°æ®æŒ‘æˆ˜æ—¶çš„æ€§èƒ½ï¼Œç ”ç©¶è€…è¿˜å¼•å…¥äº†å…¨æ–°çš„å¤šæ¨¡æ€åŸºå‡†æ•°æ®é›† TCM-SZ1ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTCDiff åœ¨ä¸åŒç¼ºå¤±ç‡ä¸‹çš„æ•°æ®ä¿çœŸåº¦ (data fidelity) å¹³å‡æ¯”ç°æœ‰æœ€å…ˆè¿›åŸºçº¿æ¨¡å‹æé«˜ 10%ï¼ŒåŒæ—¶ä¿æŒäº†ç¨³å¥çš„éšç§ä¿æŠ¤èƒ½åŠ›ã€‚è¯¥ç ”ç©¶è¯æ˜äº† TCDiff åœ¨ç°å®åŒ»ç–—åœºæ™¯ä¸­åˆæˆé«˜ä¿çœŸ EHR æ•°æ®çš„é«˜æ•ˆæ€§ã€é²æ£’æ€§ä¸æ³›åŒ–æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01615v1",
      "published_date": "2025-08-03 06:24:20 UTC",
      "updated_date": "2025-08-03 06:24:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:35:33.693155+00:00"
    },
    {
      "arxiv_id": "2508.01612v1",
      "title": "Augmented Reinforcement Learning Framework For Enhancing Decision-Making In Machine Learning Models Using External Agents",
      "title_zh": "å¼•å…¥å¤–éƒ¨æ™ºèƒ½ä½“æå‡æœºå™¨å­¦ä¹ æ¨¡å‹å†³ç­–èƒ½åŠ›çš„å¢å¼ºå¼ºåŒ–å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Sandesh Kumar Singh"
      ],
      "abstract": "This work proposes a novel technique Augmented Reinforcement Learning framework for the improvement of decision-making capabilities of machine learning models. The introduction of agents as external overseers checks on model decisions. The external agent can be anyone, like humans or automated scripts, that helps in decision path correction. It seeks to ascertain the priority of the \"Garbage-In, Garbage-Out\" problem that caused poor data inputs or incorrect actions in reinforcement learning. The ARL framework incorporates two external agents that aid in course correction and the guarantee of quality data at all points of the training cycle. The External Agent 1 is a real-time evaluator, which will provide feedback light of decisions taken by the model, identify suboptimal actions forming the Rejected Data Pipeline. The External Agent 2 helps in selective curation of the provided feedback with relevance and accuracy in business scenarios creates an approved dataset for future training cycles. The validation of the framework is also applied to a real-world scenario, which is \"Document Identification and Information Extraction\". This problem originates mainly from banking systems, but can be extended anywhere. The method of classification and extraction of information has to be done correctly here. Experimental results show that including human feedback significantly enhances the ability of the model in order to increase robustness and accuracy in making decisions. The augmented approach, with a combination of machine efficiency and human insight, attains a higher learning standard-mainly in complex or ambiguous environments. The findings of this study show that human-in-the-loop reinforcement learning frameworks such as ARL can provide a scalable approach to improving model performance in data-driven applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å¢å¼ºå¼ºåŒ–å­¦ä¹ æ¡†æ¶ Augmented Reinforcement Learning (ARL)ï¼Œæ—¨åœ¨é€šè¿‡å¼•å…¥å¤–éƒ¨æ™ºèƒ½ä½“ä½œä¸ºç›‘ç£è€…æ¥å¢å¼ºæœºå™¨å­¦ä¹ æ¨¡å‹çš„å†³ç­–èƒ½åŠ›ã€‚ARL æ¡†æ¶é€šè¿‡å¼•å…¥ä¸¤ä¸ªå¤–éƒ¨æ™ºèƒ½ä½“ååŒå·¥ä½œï¼Œæ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ ä¸­ç”±äºè¾“å…¥è´¨é‡å·®å¯¼è‡´çš„ Garbage-In, Garbage-Out é—®é¢˜ã€‚å…¶ä¸­ External Agent 1 ä½œä¸ºå®æ—¶è¯„ä¼°å™¨è¯†åˆ«æ¬¡ä¼˜åŠ¨ä½œå¹¶æ„å»º Rejected Data Pipelineï¼Œè€Œ External Agent 2 åˆ™å¯¹åé¦ˆè¿›è¡Œç²¾é€‰ä»¥ç”Ÿæˆç”¨äºæœªæ¥è®­ç»ƒçš„æ ¸å‡†æ•°æ®é›†ã€‚è¯¥æ¡†æ¶åœ¨ Document Identification and Information Extraction ä»»åŠ¡ä¸­å¾—åˆ°äº†éªŒè¯ï¼Œå®éªŒç»“æœè¯æ˜å¼•å…¥äººç±»åé¦ˆèƒ½æ˜¾è‘—æé«˜æ¨¡å‹åœ¨å¤æ‚æˆ–æ¨¡ç³Šç¯å¢ƒä¸‹çš„ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ã€‚ç ”ç©¶ç»“è®ºè¡¨æ˜ï¼ŒARL è¿™ç§ human-in-the-loop çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸ºæå‡æ•°æ®é©±åŠ¨å‹åº”ç”¨çš„æ€§èƒ½æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Master's thesis, 274 pages, 8 Tables, 73 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.01612v1",
      "published_date": "2025-08-03 06:17:44 UTC",
      "updated_date": "2025-08-03 06:17:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:35:34.642956+00:00"
    },
    {
      "arxiv_id": "2508.01598v1",
      "title": "Drift-aware Collaborative Assistance Mixture of Experts for Heterogeneous Multistream Learning",
      "title_zh": "é¢å‘å¼‚æ„å¤šæµå­¦ä¹ çš„æ¼‚ç§»æ„ŸçŸ¥åä½œè¾…åŠ©æ··åˆä¸“å®¶æ¨¡å‹",
      "authors": [
        "En Yu",
        "Jie Lu",
        "Kun Wang",
        "Xiaoyu Yang",
        "Guangquan Zhang"
      ],
      "abstract": "Learning from multiple data streams in real-world scenarios is fundamentally challenging due to intrinsic heterogeneity and unpredictable concept drifts. Existing methods typically assume homogeneous streams and employ static architectures with indiscriminate knowledge fusion, limiting generalizability in complex dynamic environments. To tackle this gap, we propose CAMEL, a dynamic \\textbf{C}ollaborative \\textbf{A}ssistance \\textbf{M}ixture of \\textbf{E}xperts \\textbf{L}earning framework. It addresses heterogeneity by assigning each stream an independent system with a dedicated feature extractor and task-specific head. Meanwhile, a dynamic pool of specialized private experts captures stream-specific idiosyncratic patterns. Crucially, collaboration across these heterogeneous streams is enabled by a dedicated assistance expert. This expert employs a multi-head attention mechanism to distill and integrate relevant context autonomously from all other concurrent streams. It facilitates targeted knowledge transfer while inherently mitigating negative transfer from irrelevant sources. Furthermore, we propose an Autonomous Expert Tuner (AET) strategy, which dynamically manages expert lifecycles in response to drift. It instantiates new experts for emerging concepts (freezing prior ones to prevent catastrophic forgetting) and prunes obsolete ones. This expert-level plasticity provides a robust and efficient mechanism for online model capacity adaptation. Extensive experiments demonstrate CAMEL's superior generalizability across diverse multistreams and exceptional resilience against complex concept drifts.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CAMELï¼Œä¸€ç§åŠ¨æ€çš„åä½œè¾…åŠ©æ··åˆä¸“å®¶å­¦ä¹ (Collaborative Assistance Mixture of Experts Learning)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ•°æ®æµå­¦ä¹ ä¸­å›ºæœ‰çš„å¼‚è´¨æ€§(heterogeneity)å’Œä¸å¯é¢„æµ‹çš„æ¦‚å¿µæ¼‚ç§»(concept drifts)æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹æµä¹‹é—´çš„å¼‚è´¨æ€§ï¼ŒCAMELä¸ºæ¯ä¸ªæµåˆ†é…ç‹¬ç«‹çš„ç‰¹å¾æå–å™¨å’Œä»»åŠ¡ç‰¹å®šå¤´ï¼Œå¹¶åˆ©ç”¨åŠ¨æ€çš„ä¸“é—¨ç§æœ‰ä¸“å®¶æ± æ•æ‰æµç‰¹å®šçš„ç‹¬ç‰¹æ¨¡å¼ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåœ¨äºä¸€ä¸ªä¸“ç”¨çš„è¾…åŠ©ä¸“å®¶ï¼Œå®ƒé€šè¿‡å¤šå¤´æ³¨æ„åŠ›(multi-head attention)æœºåˆ¶è‡ªä¸»è’¸é¦å¹¶æ•´åˆæ¥è‡ªå…¶ä»–å¹¶å‘æµçš„ç›¸å…³ä¸Šä¸‹æ–‡ï¼Œåœ¨å®ç°é’ˆå¯¹æ€§çŸ¥è¯†è½¬ç§»çš„åŒæ—¶å‡è½»è´Ÿè¿ç§»(negative transfer)ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†è‡ªä¸»ä¸“å®¶è°ƒæ•´å™¨(Autonomous Expert Tuner, AET)ç­–ç•¥ï¼Œé€šè¿‡åŠ¨æ€ç®¡ç†ä¸“å®¶çš„ç”Ÿå‘½å‘¨æœŸæ¥åº”å¯¹æ¼‚ç§»ï¼Œå¹¶é‡‡ç”¨å†»ç»“æ—§ä¸“å®¶çš„æ–¹å¼é˜²æ­¢ç¾éš¾æ€§é—å¿˜(catastrophic forgetting)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCAMELåœ¨å¤šæ ·åŒ–çš„å¤šæµåœºæ™¯ä¸­å±•ç°äº†å“è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å¯¹å¤æ‚æ¦‚å¿µæ¼‚ç§»è¡¨ç°å‡ºæå¼ºçš„éŸ§æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01598v1",
      "published_date": "2025-08-03 05:35:34 UTC",
      "updated_date": "2025-08-03 05:35:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:35:39.406208+00:00"
    },
    {
      "arxiv_id": "2508.01592v1",
      "title": "DMTrack: Spatio-Temporal Multimodal Tracking via Dual-Adapter",
      "title_zh": "DMTrackï¼šåŸºäºåŒé€‚é…å™¨çš„æ—¶ç©ºå¤šæ¨¡æ€è·Ÿè¸ª",
      "authors": [
        "Weihong Li",
        "Shaohua Dong",
        "Haonan Lu",
        "Yanhao Zhang",
        "Heng Fan",
        "Libo Zhang"
      ],
      "abstract": "In this paper, we explore adapter tuning and introduce a novel dual-adapter architecture for spatio-temporal multimodal tracking, dubbed DMTrack. The key of our DMTrack lies in two simple yet effective modules, including a spatio-temporal modality adapter (STMA) and a progressive modality complementary adapter (PMCA) module. The former, applied to each modality alone, aims to adjust spatio-temporal features extracted from a frozen backbone by self-prompting, which to some extent can bridge the gap between different modalities and thus allows better cross-modality fusion. The latter seeks to facilitate cross-modality prompting progressively with two specially designed pixel-wise shallow and deep adapters. The shallow adapter employs shared parameters between the two modalities, aiming to bridge the information flow between the two modality branches, thereby laying the foundation for following modality fusion, while the deep adapter modulates the preliminarily fused information flow with pixel-wise inner-modal attention and further generates modality-aware prompts through pixel-wise inter-modal attention. With such designs, DMTrack achieves promising spatio-temporal multimodal tracking performance with merely \\textbf{0.93M} trainable parameters. Extensive experiments on five benchmarks show that DMTrack achieves state-of-the-art results. Code will be available.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DMTrackï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ—¶ç©ºå¤šæ¨¡æ€è·Ÿè¸ª(Spatio-temporal Multimodal Tracking)çš„åˆ›æ–°åŒé€‚é…å™¨(Dual-adapter)æ¶æ„ï¼Œæ—¨åœ¨é€šè¿‡é«˜æ•ˆçš„å‚æ•°å¾®è°ƒæå‡è·Ÿè¸ªæ€§èƒ½ã€‚è¯¥æ¶æ„çš„æ ¸å¿ƒç”±æ—¶ç©ºæ¨¡æ€é€‚é…å™¨(STMA)å’Œæ¸è¿›å¼æ¨¡æ€è¡¥å……é€‚é…å™¨(PMCA)ä¸¤ä¸ªæ¨¡å—ç»„æˆï¼Œå…¶ä¸­STMAé€šè¿‡è‡ªæç¤º(Self-prompting)è°ƒæ•´ä»å†»ç»“éª¨å¹²ç½‘ç»œ(Frozen Backbone)æå–çš„æ—¶ç©ºç‰¹å¾ï¼Œæœ‰æ•ˆç¼©å°äº†ä¸åŒæ¨¡æ€é—´çš„å·®å¼‚ã€‚PMCAæ¨¡å—åŒ…å«æµ…å±‚å’Œæ·±å±‚ä¸¤ä¸ªåƒç´ çº§é€‚é…å™¨ï¼Œæµ…å±‚é€‚é…å™¨åˆ©ç”¨å…±äº«å‚æ•°è¿æ¥ä¸¤ç§æ¨¡æ€çš„ä¿¡æ¯æµï¼Œä¸ºåç»­çš„æ¨¡æ€èåˆ(Modality Fusion)å¥ å®šåŸºç¡€ã€‚æ·±å±‚é€‚é…å™¨åˆ™é€šè¿‡åƒç´ çº§æ¨¡æ€å†…æ³¨æ„åŠ›(Pixel-wise Inner-modal Attention)è°ƒèŠ‚åˆæ­¥èåˆçš„ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨åƒç´ çº§æ¨¡æ€é—´æ³¨æ„åŠ›(Pixel-wise Inter-modal Attention)ç”Ÿæˆå…·æœ‰æ¨¡æ€æ„ŸçŸ¥èƒ½åŠ›çš„æç¤ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDMTrackä»…éœ€0.93Mçš„å¯è®­ç»ƒå‚æ•°å³å¯å®ç°å‡ºè‰²çš„æ€§èƒ½ï¼Œåœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°äº†SOTAæ°´å¹³ã€‚è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡ç²¾ç»†è®¾è®¡çš„åŒé€‚é…å™¨ç»“æ„ï¼Œå¯ä»¥åœ¨ä¿æŒæä½å‚æ•°é‡çš„åŒæ—¶å®ç°é«˜æ•ˆä¸”ç²¾å‡†çš„æ—¶ç©ºå¤šæ¨¡æ€ç‰¹å¾èåˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01592v1",
      "published_date": "2025-08-03 05:13:27 UTC",
      "updated_date": "2025-08-03 05:13:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:35:47.610686+00:00"
    },
    {
      "arxiv_id": "2508.01589v1",
      "title": "Censored Sampling for Topology Design: Guiding Diffusion with Human Preferences",
      "title_zh": "æ‹“æ‰‘è®¾è®¡ä¸­çš„å®¡æŸ¥é‡‡æ ·ï¼šåˆ©ç”¨äººç±»åå¥½å¼•å¯¼æ‰©æ•£æ¨¡å‹",
      "authors": [
        "Euihyun Kim",
        "Keun Park",
        "Yeoneung Kim"
      ],
      "abstract": "Recent advances in denoising diffusion models have enabled rapid generation of optimized structures for topology optimization. However, these models often rely on surrogate predictors to enforce physical constraints, which may fail to capture subtle yet critical design flaws such as floating components or boundary discontinuities that are obvious to human experts. In this work, we propose a novel human-in-the-loop diffusion framework that steers the generative process using a lightweight reward model trained on minimal human feedback. Inspired by preference alignment techniques in generative modeling, our method learns to suppress unrealistic outputs by modulating the reverse diffusion trajectory using gradients of human-aligned rewards. Specifically, we collect binary human evaluations of generated topologies and train classifiers to detect floating material and boundary violations. These reward models are then integrated into the sampling loop of a pre-trained diffusion generator, guiding it to produce designs that are not only structurally performant but also physically plausible and manufacturable. Our approach is modular and requires no retraining of the diffusion model. Preliminary results show substantial reductions in failure modes and improved design realism across diverse test conditions. This work bridges the gap between automated design generation and expert judgment, offering a scalable solution to trustworthy generative design.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹æ‹“æ‰‘ä¼˜åŒ– (topology optimization) ä¸­çš„å»å™ªæ‰©æ•£æ¨¡å‹ (denoising diffusion models) éš¾ä»¥æ•æ‰æ‚¬æµ®ç»„ä»¶æˆ–è¾¹ç•Œä¸è¿ç»­ç­‰ç»†å¾®ç‰©ç†ç¼ºé™·çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆäººç±»åå¥½çš„å¼•å¯¼ç”Ÿæˆæ¡†æ¶ã€‚è¯¥æ–¹æ³•å—ç”Ÿæˆæ¨¡å‹ä¸­çš„åå¥½å¯¹é½ (preference alignment) æŠ€æœ¯å¯å‘ï¼Œé€šè¿‡åœ¨æå°‘é‡çš„äººç±»åé¦ˆä¸Šè®­ç»ƒè½»é‡çº§å¥–åŠ±æ¨¡å‹ (reward model)ï¼Œæ¥å¼•å¯¼æ‰©æ•£è¿‡ç¨‹ã€‚å…·ä½“è€Œè¨€ï¼Œç ”ç©¶è€…æ”¶é›†äº†äººç±»å¯¹ç”Ÿæˆçš„æ‹“æ‰‘ç»“æ„çš„äºŒå…ƒè¯„ä»·ï¼Œè®­ç»ƒåˆ†ç±»å™¨ä»¥è¯†åˆ«æ‚¬æµ®ææ–™å’Œè¿åè¾¹ç•Œé™åˆ¶çš„æƒ…å†µã€‚è¿™äº›å¥–åŠ±æ¨¡å‹è¢«æ•´åˆè¿›é¢„è®­ç»ƒæ‰©æ•£ç”Ÿæˆå™¨çš„é‡‡æ ·å¾ªç¯ä¸­ï¼Œåˆ©ç”¨å¥–åŠ±æ¢¯åº¦çš„è°ƒèŠ‚æ¥æŠ‘åˆ¶ä¸åˆ‡å®é™…çš„è¾“å‡ºã€‚è¯¥æ–¹æ³•å…·æœ‰æ¨¡å—åŒ–ç‰¹å¾ï¼Œæ— éœ€å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œé‡æ–°è®­ç»ƒ (retraining)ï¼Œèƒ½å¤Ÿç”Ÿæˆæ—¢å…·å¤‡ç»“æ„æ€§èƒ½åˆç¬¦åˆç‰©ç†çœŸå®æ€§ä¸å¯åˆ¶é€ æ€§ (manufacturability) çš„è®¾è®¡ã€‚åˆæ­¥å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§æµ‹è¯•æ¡ä»¶ä¸‹æ˜¾è‘—å‡å°‘äº†è®¾è®¡å¤±æ•ˆæ¨¡å¼å¹¶æå‡äº†è®¾è®¡çš„é€¼çœŸåº¦ï¼Œæœ‰æ•ˆå¼¥åˆäº†è‡ªåŠ¨åŒ–è®¾è®¡ç”Ÿæˆä¸ä¸“å®¶åˆ¤æ–­ä¹‹é—´çš„å·®è·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01589v1",
      "published_date": "2025-08-03 05:06:26 UTC",
      "updated_date": "2025-08-03 05:06:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:36:21.891850+00:00"
    },
    {
      "arxiv_id": "2508.01586v1",
      "title": "Diffusion Models for Future Networks and Communications: A Comprehensive Survey",
      "title_zh": "é¢å‘æœªæ¥ç½‘ç»œä¸é€šä¿¡çš„æ‰©æ•£æ¨¡å‹ï¼šå…¨é¢ç»¼è¿°",
      "authors": [
        "Nguyen Cong Luong",
        "Nguyen Duc Hai",
        "Duc Van Le",
        "Huy T. Nguyen",
        "Thai-Hoc Vu",
        "Thien Huynh-The",
        "Ruichen Zhang",
        "Nguyen Duc Duy Anh",
        "Dusit Niyato",
        "Marco Di Renzo",
        "Dong In Kim",
        "Quoc-Viet Pham"
      ],
      "abstract": "The rise of Generative AI (GenAI) in recent years has catalyzed transformative advances in wireless communications and networks. Among the members of the GenAI family, Diffusion Models (DMs) have risen to prominence as a powerful option, capable of handling complex, high-dimensional data distribution, as well as consistent, noise-robust performance. In this survey, we aim to provide a comprehensive overview of the theoretical foundations and practical applications of DMs across future communication systems. We first provide an extensive tutorial of DMs and demonstrate how they can be applied to enhance optimizers, reinforcement learning and incentive mechanisms, which are popular approaches for problems in wireless networks. Then, we review and discuss the DM-based methods proposed for emerging issues in future networks and communications, including channel modeling and estimation, signal detection and data reconstruction, integrated sensing and communication, resource management in edge computing networks, semantic communications and other notable issues. We conclude the survey with highlighting technical limitations of DMs and their applications, as well as discussing future research directions.",
      "tldr_zh": "è¯¥ç»¼è¿°å¯¹æ‰©æ•£æ¨¡å‹ (Diffusion Models, DMs) åœ¨æœªæ¥é€šä¿¡ç³»ç»Ÿä¸­çš„ç†è®ºåŸºç¡€å’Œå®é™…åº”ç”¨è¿›è¡Œäº†å…¨é¢æ¦‚è¿°ã€‚æ–‡ç« é¦–å…ˆæä¾›äº†æ‰©æ•£æ¨¡å‹ (DMs) çš„è¯¦ç»†æŠ€æœ¯æ•™ç¨‹ï¼Œå¹¶é˜è¿°äº†å…¶åœ¨å¢å¼ºä¼˜åŒ–å™¨ã€å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) ä»¥åŠæ¿€åŠ±æœºåˆ¶ (Incentive Mechanisms) ç­‰æ— çº¿ç½‘ç»œæ ¸å¿ƒç®—æ³•æ–¹é¢çš„æ½œåŠ›ã€‚æ¥ç€ï¼Œç»¼è¿°æ·±å…¥æ¢è®¨äº†åŸºäºæ‰©æ•£æ¨¡å‹ (DMs) çš„æ–¹æ³•åœ¨ä¿¡é“å»ºæ¨¡ä¸ä¼°è®¡ (Channel Modeling and Estimation)ã€ä¿¡å·æ£€æµ‹ä¸æ•°æ®é‡å»ºä»¥åŠé€šæ„Ÿä¸€ä½“åŒ– (Integrated Sensing and Communication) ç­‰æ–°å…´é€šä¿¡ä»»åŠ¡ä¸­çš„åº”ç”¨ç°çŠ¶ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è®¨è®ºäº†æ‰©æ•£æ¨¡å‹ (DMs) åœ¨è¾¹ç¼˜è®¡ç®—ç½‘ç»œèµ„æºç®¡ç†å’Œè¯­ä¹‰é€šä¿¡ (Semantic Communications) ç­‰é¢†åŸŸçš„å…³é”®ä½œç”¨ã€‚æœ€åï¼Œæ–‡ç« æ€»ç»“äº†æ‰©æ•£æ¨¡å‹ (DMs) å½“å‰é¢ä¸´çš„æŠ€æœ¯å±€é™ï¼Œå¹¶æŒ‡æ˜äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œä¸ºç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (GenAI) åœ¨æœªæ¥æ— çº¿ç½‘ç»œä¸­çš„æ·±åº¦èåˆæä¾›äº†ç†è®ºæ”¯æ’‘ä¸å®è·µå‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.ET",
        "cs.IT",
        "cs.NI"
      ],
      "primary_category": "cs.LG",
      "comment": "This work was submitted to Proceedings of the IEEE",
      "pdf_url": "https://arxiv.org/pdf/2508.01586v1",
      "published_date": "2025-08-03 04:59:58 UTC",
      "updated_date": "2025-08-03 04:59:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:36:24.286297+00:00"
    },
    {
      "arxiv_id": "2508.02750v1",
      "title": "Pulse Shape Discrimination Algorithms: Survey and Benchmark",
      "title_zh": "è„‰å†²å½¢çŠ¶é‰´åˆ«ç®—æ³•ï¼šç»¼è¿°ä¸åŸºå‡†",
      "authors": [
        "Haoran Liu",
        "Yihan Zhan",
        "Mingzhe Liu",
        "Yanhua Liu",
        "Peng Li",
        "Zhuo Zuo",
        "Bingqi Liu",
        "Runxi Liu"
      ],
      "abstract": "This review presents a comprehensive survey and benchmark of pulse shape discrimination (PSD) algorithms for radiation detection, classifying nearly sixty methods into statistical (time-domain, frequency-domain, neural network-based) and prior-knowledge (machine learning, deep learning) paradigms. We implement and evaluate all algorithms on two standardized datasets: an unlabeled set from a 241Am-9Be source and a time-of-flight labeled set from a 238Pu-9Be source, using metrics including Figure of Merit (FOM), F1-score, ROC-AUC, and inter-method correlations. Our analysis reveals that deep learning models, particularly Multi-Layer Perceptrons (MLPs) and hybrid approaches combining statistical features with neural regression, often outperform traditional methods. We discuss architectural suitabilities, the limitations of FOM, alternative evaluation metrics, and performance across energy thresholds. Accompanying this work, we release an open-source toolbox in Python and MATLAB, along with the datasets, to promote reproducibility and advance PSD research.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹è¾å°„æ¢æµ‹ä¸­çš„è„‰å†²å½¢çŠ¶åˆ¤åˆ« (Pulse Shape Discrimination, PSD) ç®—æ³•è¿›è¡Œäº†å…¨é¢çš„ç»¼è¿°å’ŒåŸºå‡†æµ‹è¯•ã€‚æ–‡ç« å°†è¿‘å…­åç§æ–¹æ³•å½’çº³ä¸ºç»Ÿè®¡èŒƒå¼ï¼ˆæ¶µç›–æ—¶åŸŸã€é¢‘åŸŸå’ŒåŸºäºç¥ç»ç½‘ç»œçš„æ–¹æ³•ï¼‰ä»¥åŠå…ˆéªŒçŸ¥è¯†èŒƒå¼ï¼ˆåŒ…æ‹¬æœºå™¨å­¦ä¹ ä¸æ·±åº¦å­¦ä¹ ï¼‰ã€‚ç ”ç©¶è€…åœ¨ 241Am-9Be å’Œ 238Pu-9Be ä¸¤ä¸ªæ ‡å‡†åŒ–æ•°æ®é›†ä¸Šå®ç°äº†æ‰€æœ‰ç®—æ³•ï¼Œå¹¶åˆ©ç”¨ Figure of Merit (FOM)ã€F1-score å’Œ ROC-AUC ç­‰å¤šç»´åº¦æŒ‡æ ‡è¿›è¡Œäº†æ€§èƒ½è¯„ä¼°ã€‚åˆ†æç»“æœæ˜¾ç¤ºï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯ Multi-Layer Perceptrons, MLPsï¼‰ä»¥åŠç»“åˆç»Ÿè®¡ç‰¹å¾ä¸ç¥ç»å›å½’çš„æ··åˆæ–¹æ³•ï¼Œå…¶è¡¨ç°é€šå¸¸ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ·±å…¥æ¢è®¨äº†æ¶æ„é€‚ç”¨æ€§ã€FOM æŒ‡æ ‡çš„å±€é™æ€§ä»¥åŠç®—æ³•åœ¨ä¸åŒèƒ½é‡é˜ˆå€¼ä¸‹çš„è¡¨ç°å·®å¼‚ã€‚ä¸ºäº†æ¨åŠ¨è¯¥é¢†åŸŸçš„å¤ç°æ€§ç ”ç©¶ï¼Œä½œè€…åŒæ­¥å‘å¸ƒäº†åŸºäº Python å’Œ MATLAB çš„å¼€æºå·¥å…·ç®±åŠå®Œæ•´æ•°æ®é›†ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "nucl-ex",
        "physics.app-ph",
        "physics.atom-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.02750v1",
      "published_date": "2025-08-03 04:41:32 UTC",
      "updated_date": "2025-08-03 04:41:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:36:26.588024+00:00"
    },
    {
      "arxiv_id": "2508.01581v1",
      "title": "Polymorphic Combinatorial Frameworks (PCF): Guiding the Design of Mathematically-Grounded, Adaptive AI Agents",
      "title_zh": "å¤šæ€ç»„åˆæ¡†æ¶ (PCF)ï¼šæŒ‡å¯¼å…·å¤‡æ•°å­¦ç†è®ºåŸºç¡€çš„è‡ªé€‚åº” AI æ™ºèƒ½ä½“è®¾è®¡",
      "authors": [
        "David Pearl",
        "Matthew Murphy",
        "James Intriligator"
      ],
      "abstract": "The Polymorphic Combinatorial Framework (PCF) leverages Large Language Models (LLMs) and mathematical frameworks to guide the meta-prompt enabled design of solution spaces and adaptive AI agents for complex, dynamic environments. Unlike static agent architectures, PCF enables real-time parameter reconfiguration through mathematically-grounded combinatorial spaces, allowing agents to adapt their core behavioral traits dynamically. Grounded in combinatorial logic, topos theory, and rough fuzzy set theory, PCF defines a multidimensional SPARK parameter space (Skills, Personalities, Approaches, Resources, Knowledge) to capture agent behaviors. This paper demonstrates how LLMs can parameterize complex spaces and estimate likely parameter values/variabilities. Using PCF, we parameterized mock cafÃ© domains (five levels of complexity), estimated variables/variabilities, and conducted over 1.25 million Monte Carlo simulations. The results revealed trends in agent adaptability and performance across the five complexity tiers, with diminishing returns at higher complexity levels highlighting thresholds for scalable designs. PCF enables the generation of optimized agent configurations for specific scenarios while maintaining logical consistency. This framework supports scalable, dynamic, explainable, and ethical AI applications in domains like customer service, healthcare, robotics, and collaborative systems, paving the way for adaptable and cooperative next-generation polymorphic agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†å¤šæ€ç»„åˆæ¡†æ¶ (Polymorphic Combinatorial Frameworks, PCF)ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆå¤§è¯­è¨€æ¨¡å‹ (LLMs) ä¸æ•°å­¦ç†è®ºæ¥æŒ‡å¯¼è®¾è®¡è‡ªé€‚åº” AI æ™ºèƒ½ä½“çš„åˆ›æ–°æ–¹æ³•ã€‚PCF åŸºäºç»„åˆé€»è¾‘ (combinatorial logic)ã€æ‹“æ‰‘æ–¯ç†è®º (topos theory) å’Œç²—ç³™æ¨¡ç³Šé›†ç†è®º (rough fuzzy set theory)ï¼Œå®šä¹‰äº†åŒ…å«æŠ€èƒ½ã€ä¸ªæ€§ã€æ–¹æ³•ã€èµ„æºå’ŒçŸ¥è¯†çš„ SPARK å¤šç»´å‚æ•°ç©ºé—´ï¼Œä»¥æ•æ‰å¹¶åŠ¨æ€é‡æ„æ™ºèƒ½ä½“çš„è¡Œä¸ºç‰¹å¾ã€‚ç ”ç©¶é€šè¿‡å¯¹äº”ç§ä¸åŒå¤æ‚åº¦çš„æ¨¡æ‹Ÿåœºæ™¯è¿›è¡Œè¶…è¿‡ 125 ä¸‡æ¬¡è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ (Monte Carlo simulations)ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶åœ¨å‚æ•°åŒ–å¤æ‚ç©ºé—´åŠä¼°ç®—å˜é‡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœæ­ç¤ºäº†æ™ºèƒ½ä½“åœ¨ä¸åŒå¤æ‚åº¦å±‚çº§ä¸‹çš„é€‚åº”æ€§è¶‹åŠ¿ï¼Œç‰¹åˆ«æ˜¯é«˜å¤æ‚åº¦æ°´å¹³ä¸‹çš„æ”¶ç›Šé€’å‡ç°è±¡ï¼Œä¸ºè®¾è®¡å¯æ‰©å±•çš„ç³»ç»Ÿæä¾›äº†å…³é”®é˜ˆå€¼ã€‚PCF èƒ½å¤Ÿç¡®ä¿æ™ºèƒ½ä½“åœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­çš„é€»è¾‘ä¸€è‡´æ€§ï¼Œä¸ºå¼€å‘å¯æ‰©å±•ã€å¯è§£é‡Šä¸”ç¬¦åˆä¼¦ç†çš„ä¸‹ä¸€ä»£å¤šæ€åŒ–ååŒæ™ºèƒ½ç³»ç»Ÿæä¾›äº†åšå®çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI",
        "math.CO",
        "stat.CO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01581v1",
      "published_date": "2025-08-03 04:19:31 UTC",
      "updated_date": "2025-08-03 04:19:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:36:30.292078+00:00"
    },
    {
      "arxiv_id": "2508.01565v1",
      "title": "Deeply Supervised Multi-Task Autoencoder for Biological Brain Age estimation using three dimensional T$_1$-weighted magnetic resonance imaging",
      "title_zh": "åŸºäºä¸‰ç»´T$_1$åŠ æƒç£å…±æŒ¯æˆåƒçš„æ·±åº¦ç›‘ç£å¤šä»»åŠ¡è‡ªç¼–ç å™¨ç”Ÿç‰©è„‘é¾„ä¼°è®¡",
      "authors": [
        "Mehreen Kanwal",
        "Yunsik Son"
      ],
      "abstract": "Accurate estimation of biological brain age from three dimensional (3D) T$_1$-weighted magnetic resonance imaging (MRI) is a critical imaging biomarker for identifying accelerated aging associated with neurodegenerative diseases. Effective brain age prediction necessitates training 3D models to leverage comprehensive insights from volumetric MRI scans, thereby fully capturing spatial anatomical context. However, optimizing deep 3D models remains challenging due to problems such as vanishing gradients. Furthermore, brain structural patterns differ significantly between sexes, which impacts aging trajectories and vulnerability to neurodegenerative diseases, thereby making sex classification crucial for enhancing the accuracy and generalizability of predictive models. To address these challenges, we propose a Deeply Supervised Multitask Autoencoder (DSMT-AE) framework for brain age estimation. DSMT-AE employs deep supervision, which involves applying supervisory signals at intermediate layers during training, to stabilize model optimization, and multitask learning to enhance feature representation. Specifically, our framework simultaneously optimizes brain age prediction alongside auxiliary tasks of sex classification and image reconstruction, thus effectively capturing anatomical and demographic variability to improve prediction accuracy. We extensively evaluate DSMT-AE on the Open Brain Health Benchmark (OpenBHB) dataset, the largest multisite neuroimaging cohort combining ten publicly available datasets. The results demonstrate that DSMT-AE achieves state-of-the-art performance and robustness across age and sex subgroups. Additionally, our ablation study confirms that each proposed component substantially contributes to the improved predictive accuracy and robustness of the overall architecture.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†æ·±åº¦ç›‘ç£å¤šä»»åŠ¡è‡ªåŠ¨ç¼–ç å™¨ï¼ˆDeeply Supervised Multitask Autoencoder, DSMT-AEï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¸‰ç»´ T$_1$ åŠ æƒç£å…±æŒ¯æˆåƒï¼ˆ3D T$_1$-weighted MRIï¼‰ç²¾ç¡®è¯„ä¼°ç”Ÿç‰©å¤§è„‘å¹´é¾„ï¼ˆbiological brain ageï¼‰ã€‚é’ˆå¯¹ 3D æ·±åº¦æ¨¡å‹ä¼˜åŒ–å›°éš¾ï¼ˆå¦‚æ¢¯åº¦æ¶ˆå¤±ï¼‰ä»¥åŠæ€§åˆ«å·®å¼‚å¯¹è„‘ç»“æ„çš„å½±å“ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†æ·±åº¦ç›‘ç£ï¼ˆDeep Supervisionï¼‰æŠ€æœ¯ä»¥ç¨³å®šæ¨¡å‹ä¼˜åŒ–å¹¶å¢å¼ºç‰¹å¾è¡¨ç¤ºã€‚é€šè¿‡å¤šä»»åŠ¡å­¦ä¹ ï¼ˆMultitask Learningï¼‰ï¼ŒDSMT-AE åŒæ—¶ä¼˜åŒ–å¤§è„‘å¹´é¾„é¢„æµ‹ã€æ€§åˆ«åˆ†ç±»ï¼ˆsex classificationï¼‰å’Œå›¾åƒé‡å»ºï¼ˆimage reconstructionï¼‰ä»»åŠ¡ï¼Œä»è€Œæœ‰æ•ˆæ•æ‰è§£å‰–å­¦å’Œäººå£ç»Ÿè®¡å­¦çš„å˜å¼‚æ€§ã€‚ç ”ç©¶åœ¨ Open Brain Health Benchmark (OpenBHB) å¤§è§„æ¨¡å¤šä¸­å¿ƒç¥ç»å½±åƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDSMT-AE åœ¨ä¸åŒå¹´é¾„å’Œæ€§åˆ«å­é›†ä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ˆstate-of-the-art performanceï¼‰å’Œå“è¶Šçš„é²æ£’æ€§ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®ï¼Œå„æ ¸å¿ƒç»„ä»¶å‡å¯¹æå‡é¢„æµ‹å‡†ç¡®æ€§å’Œæ•´ä½“æ¶æ„çš„ç¨³å®šæ€§åšå‡ºäº†æ˜¾è‘—è´¡çŒ®ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01565v1",
      "published_date": "2025-08-03 03:24:02 UTC",
      "updated_date": "2025-08-03 03:24:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:36:37.985166+00:00"
    },
    {
      "arxiv_id": "2508.01561v6",
      "title": "One Subgoal at a Time: Zero-Shot Generalization to Arbitrary Linear Temporal Logic Requirements in Multi-Task Reinforcement Learning",
      "title_zh": "ä¸€æ¬¡ä¸€ä¸ªå­ç›®æ ‡ï¼šå¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ ä¸­é¢å‘ä»»æ„çº¿æ€§æ—¶åºé€»è¾‘éœ€æ±‚çš„é›¶æ ·æœ¬æ³›åŒ–",
      "authors": [
        "Zijian Guo",
        "Ä°lker IÅŸÄ±k",
        "H. M. Sabbir Ahmad",
        "Wenchao Li"
      ],
      "abstract": "Generalizing to complex and temporally extended task objectives and safety constraints remains a critical challenge in reinforcement learning (RL). Linear temporal logic (LTL) offers a unified formalism to specify such requirements, yet existing methods are limited in their abilities to handle nested long-horizon tasks and safety constraints, and cannot identify situations when a subgoal is not satisfiable and an alternative should be sought. In this paper, we introduce GenZ-LTL, a method that enables zero-shot generalization to arbitrary LTL specifications. GenZ-LTL leverages the structure of BÃ¼chi automata to decompose an LTL task specification into sequences of reach-avoid subgoals. Contrary to the current state-of-the-art method that conditions on subgoal sequences, we show that it is more effective to achieve zero-shot generalization by solving these reach-avoid problems \\textit{one subgoal at a time} through proper safe RL formulations. In addition, we introduce a novel subgoal-induced observation reduction technique that can mitigate the exponential complexity of subgoal-state combinations under realistic assumptions. Empirical results show that GenZ-LTL substantially outperforms existing methods in zero-shot generalization to unseen LTL specifications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)åœ¨å¤„ç†å…·æœ‰å¤æ‚æ—¶åºç›®æ ‡å’Œå®‰å…¨çº¦æŸçš„ä»»åŠ¡æ—¶ï¼Œéš¾ä»¥å®ç°é›¶æ ·æœ¬(Zero-Shot)æ³›åŒ–çš„æŒ‘æˆ˜ã€‚ä½œè€…æå‡ºäº† GenZ-LTLï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨ BÃ¼chi Automata ç»“æ„å°†çº¿æ€§æ—¶åºé€»è¾‘(Linear Temporal Logic, LTL)è§„èŒƒåˆ†è§£ä¸ºä¸€ç³»åˆ—â€œåˆ°è¾¾-è§„é¿â€(reach-avoid)å­ç›®æ ‡çš„æ–°æ–¹æ³•ã€‚ä¸ç°æœ‰åŸºäºå­ç›®æ ‡åºåˆ—çš„è°ƒèŠ‚æ–¹æ³•ä¸åŒï¼ŒGenZ-LTL é€šè¿‡å®‰å…¨å¼ºåŒ–å­¦ä¹ (Safe RL)å½¢å¼åŒ–æ–¹æ¡ˆï¼Œä¸»å¼ æ¯æ¬¡åªè§£å†³ä¸€ä¸ªå­ç›®æ ‡ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°å¤„ç†åµŒå¥—çš„é•¿å‘¨æœŸä»»åŠ¡å’Œå®‰å…¨çº¦æŸã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†å­ç›®æ ‡è¯±å¯¼çš„è§‚å¯Ÿçº¦ç®€(subgoal-induced observation reduction)æŠ€æœ¯ï¼Œåœ¨ç°å®å‡è®¾ä¸‹æœ‰æ•ˆç¼“è§£äº†å­ç›®æ ‡ä¸çŠ¶æ€ç»„åˆå¸¦æ¥çš„æŒ‡æ•°çº§å¤æ‚åº¦é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGenZ-LTL åœ¨å¯¹æœªè§è¿‡çš„ LTL è§„èŒƒè¿›è¡Œé›¶æ ·æœ¬æ³›åŒ–æ—¶ï¼Œå…¶è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01561v6",
      "published_date": "2025-08-03 03:17:49 UTC",
      "updated_date": "2025-11-09 17:17:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:36:53.496921+00:00"
    },
    {
      "arxiv_id": "2508.01556v1",
      "title": "Empowering Tabular Data Preparation with Language Models: Why and How?",
      "title_zh": "è¯­è¨€æ¨¡å‹èµ‹èƒ½è¡¨æ ¼æ•°æ®å‡†å¤‡ï¼šåŠ¨å› ä¸æ–¹æ³•",
      "authors": [
        "Mengshi Chen",
        "Yuxiang Sun",
        "Tengchao Li",
        "Jianwei Wang",
        "Kai Wang",
        "Xuemin Lin",
        "Ying Zhang",
        "Wenjie Zhang"
      ],
      "abstract": "Data preparation is a critical step in enhancing the usability of tabular data and thus boosts downstream data-driven tasks. Traditional methods often face challenges in capturing the intricate relationships within tables and adapting to the tasks involved. Recent advances in Language Models (LMs), especially in Large Language Models (LLMs), offer new opportunities to automate and support tabular data preparation. However, why LMs suit tabular data preparation (i.e., how their capabilities match task demands) and how to use them effectively across phases still remain to be systematically explored. In this survey, we systematically analyze the role of LMs in enhancing tabular data preparation processes, focusing on four core phases: data acquisition, integration, cleaning, and transformation. For each phase, we present an integrated analysis of how LMs can be combined with other components for different preparation tasks, highlight key advancements, and outline prospective pipelines.",
      "tldr_zh": "è¯¥é¡¹ç»¼è¿°ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨è¯­è¨€æ¨¡å‹(LMs)å°¤å…¶æ˜¯å¤§è¯­è¨€æ¨¡å‹(LLMs)æ¥å¢å¼ºè¡¨æ ¼æ•°æ®å‡†å¤‡(Tabular Data Preparation)çš„è¿‡ç¨‹ã€‚æ•°æ®å‡†å¤‡å¯¹äºæå‡è¡¨æ ¼æ•°æ®çš„å¯ç”¨æ€§å’Œæ”¯æ’‘ä¸‹æ¸¸ä»»åŠ¡è‡³å…³é‡è¦ï¼Œä½†ä¼ ç»Ÿæ–¹æ³•åœ¨æ•æ‰è¡¨æ ¼å†…éƒ¨å¤æ‚å…³ç³»å’Œé€‚åº”ç‰¹å®šä»»åŠ¡æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°åˆ†æäº†LMsåœ¨æ•°æ®è·å–(Data Acquisition)ã€é›†æˆ(Integration)ã€æ¸…æ´—(Cleaning)å’Œè½¬æ¢(Transformation)è¿™å››ä¸ªæ ¸å¿ƒé˜¶æ®µçš„ä½œç”¨ã€‚é€šè¿‡æ•´åˆåˆ†æLMsä¸å…¶ä»–ç»„ä»¶åœ¨ä¸åŒä»»åŠ¡ä¸­çš„ç»“åˆæ–¹å¼ï¼Œç ”ç©¶æ·±å…¥é˜è¿°äº†LMsåŒ¹é…è¡¨æ ¼æ•°æ®å‡†å¤‡éœ€æ±‚çš„å†…åœ¨åŸå› åŠå…¶å®æ–½è·¯å¾„ã€‚æ­¤å¤–ï¼Œè¯¥ç»¼è¿°è¿˜æ€»ç»“äº†å½“å‰çš„å…³é”®æŠ€æœ¯è¿›å±•å¹¶å‹¾ç”»äº†å…·æœ‰å‰æ™¯çš„è‡ªåŠ¨åŒ–å·¥ä½œç®¡çº¿ã€‚è¯¥ç ”ç©¶ä¸ºåˆ©ç”¨äººå·¥æ™ºèƒ½æŠ€æœ¯å®ç°æ›´é«˜æ•ˆã€è‡ªåŠ¨åŒ–çš„è¡¨æ ¼æ•°æ®å¤„ç†æä¾›äº†ç³»ç»Ÿæ€§çš„å‚è€ƒæ¡†æ¶ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint under submission, 16 pages, 2 figures, 1 table",
      "pdf_url": "https://arxiv.org/pdf/2508.01556v1",
      "published_date": "2025-08-03 03:00:02 UTC",
      "updated_date": "2025-08-03 03:00:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:36:52.101605+00:00"
    },
    {
      "arxiv_id": "2508.01554v1",
      "title": "Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models",
      "title_zh": "æç¤ºè¯ç»„ä»¶æ˜¯å¦å‡ä¸ºä»·å€¼ä¸­æ€§ï¼Ÿæ¢ç©¶å¤§è¯­è¨€æ¨¡å‹ä¸­è§£æ„æç¤ºè¯çš„å¼‚è´¨å¯¹æŠ—é²æ£’æ€§",
      "authors": [
        "Yujia Zheng",
        "Tianhao Li",
        "Haotian Huang",
        "Tianyu Zeng",
        "Jingyu Lu",
        "Chuangxin Chu",
        "Yuekai Huang",
        "Ziyou Jiang",
        "Qian Xiong",
        "Yuyao Ge",
        "Mingyang Li"
      ],
      "abstract": "Prompt-based adversarial attacks have become an effective means to assess the robustness of large language models (LLMs). However, existing approaches often treat prompts as monolithic text, overlooking their structural heterogeneity-different prompt components contribute unequally to adversarial robustness. Prior works like PromptRobust assume prompts are value-neutral, but our analysis reveals that complex, domain-specific prompts with rich structures have components with differing vulnerabilities. To address this gap, we introduce PromptAnatomy, an automated framework that dissects prompts into functional components and generates diverse, interpretable adversarial examples by selectively perturbing each component using our proposed method, ComPerturb. To ensure linguistic plausibility and mitigate distribution shifts, we further incorporate a perplexity (PPL)-based filtering mechanism. As a complementary resource, we annotate four public instruction-tuning datasets using the PromptAnatomy framework, verified through human review. Extensive experiments across these datasets and five advanced LLMs demonstrate that ComPerturb achieves state-of-the-art attack success rates. Ablation studies validate the complementary benefits of prompt dissection and PPL filtering. Our results underscore the importance of prompt structure awareness and controlled perturbation for reliable adversarial robustness evaluation in LLMs. Code and data are available at https://github.com/Yujiaaaaa/PACP.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­æç¤ºè¯(Prompt)ç»„ä»¶çš„å¼‚æ„å¯¹æŠ—é²æ£’æ€§ï¼ŒæŒ‡å‡ºç›®å‰çš„æ”»å‡»æ–¹æ³•å¾€å¾€å°†æç¤ºè¯è§†ä¸ºå•ä¸€æ•´ä½“ï¼Œè€Œå¿½è§†äº†ä¸åŒåŠŸèƒ½ç»„ä»¶åœ¨é²æ£’æ€§ä¸Šçš„è´¡çŒ®å·®å¼‚ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†PromptAnatomyæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè‡ªåŠ¨å°†æç¤ºè¯è§£æä¸ºå¤šä¸ªåŠŸèƒ½ç»„ä»¶ï¼Œä»¥è¯†åˆ«ä¸åŒéƒ¨åˆ†çš„è„†å¼±æ€§ã€‚åŸºäºæ­¤æ¡†æ¶ï¼Œç ”ç©¶è¿›ä¸€æ­¥å¼€å‘äº†ComPerturbæ–¹æ³•ï¼Œé€šè¿‡é€‰æ‹©æ€§åœ°æ‰°åŠ¨ç‰¹å®šç»„ä»¶æ¥ç”Ÿæˆå¤šæ ·åŒ–ä¸”å…·æœ‰å¯è§£é‡Šæ€§çš„å¯¹æŠ—æ ·æœ¬ã€‚ä¸ºäº†ä¿è¯ç”Ÿæˆæ ·æœ¬çš„è¯­è¨€è¿è´¯æ€§å¹¶å‡å°‘åˆ†å¸ƒåç§»ï¼Œè¯¥ç ”ç©¶è¿˜å¼•å…¥äº†åŸºäºå›°æƒ‘åº¦(PPL)çš„è¿‡æ»¤æœºåˆ¶ï¼Œå¹¶å¯¹å››ä¸ªå…¬å…±æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†è¿›è¡Œäº†äººå·¥éªŒè¯æ ‡æ³¨ã€‚åœ¨äº”ä¸ªå…ˆè¿›LLMsä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒComPerturbåœ¨æ”»å‡»æˆåŠŸç‡ä¸Šè¾¾åˆ°äº†state-of-the-artæ°´å¹³ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†æç¤ºè¯æ‹†è§£ä¸PPLè¿‡æ»¤çš„äº’è¡¥ä½œç”¨ï¼Œå¼ºè°ƒäº†åœ¨è¯„ä¼°LLMsé²æ£’æ€§æ—¶å…·å¤‡æç¤ºè¯ç»“æ„æ„ŸçŸ¥èƒ½åŠ›çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01554v1",
      "published_date": "2025-08-03 02:46:30 UTC",
      "updated_date": "2025-08-03 02:46:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:36:48.888243+00:00"
    },
    {
      "arxiv_id": "2508.01547v1",
      "title": "Understanding Why ChatGPT Outperforms Humans in Visualization Design Advice",
      "title_zh": "æ¢ç©¶ ChatGPT åœ¨å¯è§†åŒ–è®¾è®¡å»ºè®®æ–¹é¢ä¼˜äºäººç±»çš„åŸå› ",
      "authors": [
        "Yongsu Ahn",
        "Nam Wook Kim"
      ],
      "abstract": "This paper investigates why recent generative AI models outperform humans in data visualization knowledge tasks. Through systematic comparative analysis of responses to visualization questions, we find that differences exist between two ChatGPT models and human outputs over rhetorical structure, knowledge breadth, and perceptual quality. Our findings reveal that ChatGPT-4, as a more advanced model, displays a hybrid of characteristics from both humans and ChatGPT-3.5. The two models were generally favored over human responses, while their strengths in coverage and breadth, and emphasis on technical and task-oriented visualization feedback collectively shaped higher overall quality. Based on our findings, we draw implications for advancing user experiences based on the potential of LLMs and human perception over their capabilities, with relevance to broader applications of AI.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenerative AIï¼‰åœ¨æ•°æ®å¯è§†åŒ–è®¾è®¡å»ºè®®ä»»åŠ¡ä¸­è¶…è¶Šäººç±»çš„å…·ä½“åŸå› ã€‚ç ”ç©¶é€šè¿‡ç³»ç»Ÿå¯¹æ¯” ChatGPT-3.5ã€ChatGPT-4 ä¸äººç±»çš„å“åº”ï¼Œæ·±å…¥åˆ†æäº†å®ƒä»¬åœ¨ä¿®è¾ç»“æ„ã€çŸ¥è¯†å¹¿åº¦ï¼ˆKnowledge Breadthï¼‰å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢çš„æ˜¾è‘—å·®å¼‚ã€‚ç»“æœæ˜¾ç¤ºï¼Œä½œä¸ºæ›´å…ˆè¿›æ¨¡å‹çš„ ChatGPT-4 è¡¨ç°å‡ºäººç±»ä¸ ChatGPT-3.5 ç‰¹å¾çš„æ··åˆä½“ï¼Œä¸”ä¸¤ç§ AI æ¨¡å‹åœ¨æ•´ä½“è¯„ä»·ä¸Šé€šå¸¸ä¼˜äºäººç±»å“åº”ã€‚AI çš„ä¸»è¦ä¼˜åŠ¿åœ¨äºå…¶æé«˜çš„çŸ¥è¯†è¦†ç›–é¢ä»¥åŠå¯¹æŠ€æœ¯å’Œä»»åŠ¡å¯¼å‘å‹å¯è§†åŒ–åé¦ˆçš„å¼ºè°ƒï¼Œè¿™äº›å› ç´ å…±åŒå¡‘é€ äº†æ›´é«˜çš„æ„ŸçŸ¥è´¨é‡ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æå‡ç”¨æˆ·ä½“éªŒçš„æ½œåŠ›ï¼Œå¹¶åˆ†æäº†äººç±»å¯¹ AI èƒ½åŠ›çš„æ„ŸçŸ¥è§„å¾‹ã€‚è¯¥é¡¹å·¥ä½œä¸º AI åœ¨æ›´å¹¿æ³›é¢†åŸŸä¸­çš„åº”ç”¨ä»¥åŠä¼˜åŒ–äººæœºäº¤äº’æ¨¡å¼æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01547v1",
      "published_date": "2025-08-03 02:14:00 UTC",
      "updated_date": "2025-08-03 02:14:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:36:48.618922+00:00"
    },
    {
      "arxiv_id": "2508.01545v2",
      "title": "Getting out of the Big-Muddy: Escalation of Commitment in LLMs",
      "title_zh": "æ‘†è„±æ³¥æ½­ï¼šå¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ‰¿è¯ºå‡çº§",
      "authors": [
        "Emilio Barkett",
        "Olivia Long",
        "Paul KrÃ¶ger"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed in autonomous decision-making roles across high-stakes domains. However, since models are trained on human-generated data, they may inherit cognitive biases that systematically distort human judgment, including escalation of commitment, where decision-makers continue investing in failing courses of action due to prior investment. Understanding when LLMs exhibit such biases presents a unique challenge. While these biases are well-documented in humans, it remains unclear whether they manifest consistently in LLMs or require specific triggering conditions. This paper investigates this question using a two-stage investment task across four experimental conditions: model as investor, model as advisor, multi-agent deliberation, and compound pressure scenario. Across N = 6,500 trials, we find that bias manifestation in LLMs is highly context-dependent. In individual decision-making contexts (Studies 1-2, N = 4,000), LLMs demonstrate strong rational cost-benefit logic with minimal escalation of commitment. However, multi-agent deliberation reveals a striking hierarchy effect (Study 3, N = 500): while asymmetrical hierarchies show moderate escalation rates (46.2%), symmetrical peer-based decision-making produces near-universal escalation (99.2%). Similarly, when subjected to compound organizational and personal pressures (Study 4, N = 2,000), models exhibit high degrees of escalation of commitment (68.95% average allocation to failing divisions). These findings reveal that LLM bias manifestation depends critically on social and organizational context rather than being inherent, with significant implications for the deployment of multi-agent systems and unsupervised operations where such conditions may emerge naturally.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)æ˜¯å¦ä¼šè¡¨ç°å‡ºæ‰¿è¯ºå‡çº§(Escalation of Commitment)åå·®ï¼Œå³åœ¨å·²çŸ¥è¡ŒåŠ¨æ–¹æ¡ˆå¤±è´¥çš„æƒ…å†µä¸‹ï¼Œå› å‰æœŸæŠ•å…¥è€ŒæŒç»­è¿½åŠ èµ„æºã€‚ç ”ç©¶è€…è®¾è®¡äº†ä¸€ä¸ªä¸¤é˜¶æ®µæŠ•èµ„ä»»åŠ¡ï¼Œé€šè¿‡6,500æ¬¡è¯•éªŒåˆ†æäº†LLMsåœ¨æŠ•èµ„è€…ã€é¡¾é—®ã€å¤šæ™ºèƒ½ä½“åå•†åŠå¤åˆå‹åŠ›å››ç§åœºæ™¯ä¸‹çš„å†³ç­–è¡Œä¸ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨ä¸ªä½“å†³ç­–æ—¶å±•ç°å‡ºé«˜åº¦çš„ç†æ€§é€»è¾‘ï¼ŒåŸºæœ¬æ²¡æœ‰äº§ç”Ÿæ‰¿è¯ºå‡çº§ç°è±¡ã€‚ä½†åœ¨å¤šæ™ºèƒ½ä½“å¯¹ç§°åŒä¼´å†³ç­–(symmetrical peer-based decision-making)ä¸­ï¼Œè¯¥åå·®çš„å‘ç”Ÿç‡æ¥è¿‘100%ï¼Œè€Œåœ¨å¤åˆç»„ç»‡å’Œä¸ªäººå‹åŠ›ä¸‹ä¹Ÿè¡¨ç°å‡ºæ˜¾è‘—çš„åå·®ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†LLMsçš„è®¤çŸ¥åå·®å¹¶éæ¨¡å‹å›ºæœ‰ï¼Œè€Œæ˜¯é«˜åº¦ä¾èµ–äºç¤¾äº¤å’Œç»„ç»‡è¯­å¢ƒï¼Œä¸ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(multi-agent systems)å’Œè‡ªä¸»åŒ–éƒ¨ç½²çš„å®‰å…¨æ€§æä¾›äº†å…³é”®æ´å¯Ÿã€‚",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01545v2",
      "published_date": "2025-08-03 01:58:38 UTC",
      "updated_date": "2025-08-07 15:04:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:36:59.232312+00:00"
    },
    {
      "arxiv_id": "2508.01543v2",
      "title": "Refine-n-Judge: Curating High-Quality Preference Chains for LLM-Fine-Tuning",
      "title_zh": "Refine-n-Judgeï¼šæ„å»ºé¢å‘å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒçš„é«˜è´¨é‡åå¥½é“¾",
      "authors": [
        "Derin Cayir",
        "Renjie Tao",
        "Rashi Rungta",
        "Kai Sun",
        "Sean Chen",
        "Haidar Khan",
        "Minseok Kim",
        "Julia Reinspach",
        "Yue Liu"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable progress through preference-based fine-tuning, which critically depends on the quality of the underlying training data. While human feedback is essential for improving data quality, it is costly and does not scale well. In this paper, we introduce Refine-n-Judge, an automated iterative approach that leverages a single LLM as both a refiner and a judge to enhance dataset quality. Unlike existing iterative refinement methods, Refine-n-Judge employs an LLM to both generate refinements and explicitly evaluate each improvement, ensuring that every iteration meaningfully enhances the dataset without requiring additional human annotation or a separate reward model. At each step, the LLM refines a response and judges whether the refinement is an improvement over the previous answer. This process continues until the LLM prefers the initial answer over the refinement, indicating no further improvements. This produces sequences of increasing quality, preference-labeled responses ideal for fine-tuning.\n  We demonstrate the effectiveness of Refine-n-Judge across a range of public datasets spanning five corpora, targeting tasks such as coding, math, and conversation. Models (Llama 3.1-8B and Llama 3.3-70B) fine-tuned on Refine-n-Judge-enhanced datasets were preferred by LLM judges in over 74% of comparisons against models tuned on the original dataset by GPT-4. Additionally, we report performance gains: +5% on AlpacaEval and AlpacaEval 2.0, and +19% on MT-Bench. Our results indicate that Refine-n-Judge produces high-quality datasets and scalable model improvements.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Refine-n-Judgeï¼Œä¸€ç§è‡ªåŠ¨åŒ–çš„è¿­ä»£æ–¹æ³•ï¼Œåˆ©ç”¨å•ä¸ª Large Language Model (LLM) åŒæ—¶å……å½“æ”¹è¿›è€… (refiner) å’Œè£åˆ¤ (judge)ï¼Œä»¥æå‡ç”¨äºåå¥½å¾®è°ƒ (preference-based fine-tuning) çš„æ•°æ®é›†è´¨é‡ã€‚è¯¥æ¡†æ¶æ— éœ€äººå·¥æ ‡æ³¨æˆ–ç‹¬ç«‹çš„å¥–åŠ±æ¨¡å‹ (reward model)ï¼Œé€šè¿‡ LLM è‡ªè¡Œç”Ÿæˆæ”¹è¿›å»ºè®®å¹¶æ˜¾å¼è¯„ä¼°æ¯æ¬¡æå‡ï¼Œç›´åˆ°æ— æ³•è¿›ä¸€æ­¥ä¼˜åŒ–ä¸ºæ­¢ï¼Œä»è€Œäº§ç”Ÿä¸€ç³»åˆ—è´¨é‡é€’å¢ä¸”å¸¦æœ‰åå¥½æ ‡ç­¾çš„å“åº”é“¾ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ Llama 3.1-8B å’Œ Llama 3.3-70B æ¨¡å‹ä¸Šä½¿ç”¨è¯¥å¢å¼ºæ•°æ®é›†è¿›è¡Œå¾®è°ƒåï¼ŒLLM è£åˆ¤åœ¨è¶…è¿‡ 74% çš„æ¯”è¾ƒä¸­æ›´é’çè¯¥æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ AlpacaEval å’Œ AlpacaEval 2.0 ä¸Šå®ç°äº† 5% çš„æ€§èƒ½æå‡ï¼Œåœ¨ MT-Bench ä¸Šæ˜¾è‘—æå‡äº† 19%ã€‚ç ”ç©¶ç»“æœè¯æ˜äº† Refine-n-Judge åœ¨ç”Ÿæˆé«˜è´¨é‡æ•°æ®é›†å’Œå®ç°å¤§è§„æ¨¡æ¨¡å‹æ”¹è¿›æ–¹é¢çš„æœ‰æ•ˆæ€§ä¸å¯æ‰©å±•æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01543v2",
      "published_date": "2025-08-03 01:56:03 UTC",
      "updated_date": "2025-10-30 16:32:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:37:19.693394+00:00"
    },
    {
      "arxiv_id": "2508.01542v1",
      "title": "Leveraging Machine Learning for Botnet Attack Detection in Edge-Computing Assisted IoT Networks",
      "title_zh": "åˆ©ç”¨æœºå™¨å­¦ä¹ å®ç°è¾¹ç¼˜è®¡ç®—è¾…åŠ©ç‰©è”ç½‘ä¸­çš„åƒµå°¸ç½‘ç»œæ”»å‡»æ£€æµ‹",
      "authors": [
        "Dulana Rupanetti",
        "Naima Kaabouch"
      ],
      "abstract": "The increase of IoT devices, driven by advancements in hardware technologies, has led to widespread deployment in large-scale networks that process massive amounts of data daily. However, the reliance on Edge Computing to manage these devices has introduced significant security vulnerabilities, as attackers can compromise entire networks by targeting a single IoT device. In light of escalating cybersecurity threats, particularly botnet attacks, this paper investigates the application of machine learning techniques to enhance security in Edge-Computing-Assisted IoT environments. Specifically, it presents a comparative analysis of Random Forest, XGBoost, and LightGBM -- three advanced ensemble learning algorithms -- to address the dynamic and complex nature of botnet threats. Utilizing a widely recognized IoT network traffic dataset comprising benign and malicious instances, the models were trained, tested, and evaluated for their accuracy in detecting and classifying botnet activities. Furthermore, the study explores the feasibility of deploying these models in resource-constrained edge and IoT devices, demonstrating their practical applicability in real-world scenarios. The results highlight the potential of machine learning to fortify IoT networks against emerging cybersecurity challenges.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨è¾¹ç¼˜è®¡ç®—(Edge Computing)è¾…åŠ©çš„ç‰©è”ç½‘(IoT)ç½‘ç»œä¸­ï¼Œåˆ©ç”¨æœºå™¨å­¦ä¹ æŠ€æœ¯æ£€æµ‹åƒµå°¸ç½‘ç»œ(Botnet)æ”»å‡»çš„æ–¹æ³•ã€‚é’ˆå¯¹IoTè®¾å¤‡æ™®åŠå¸¦æ¥çš„å®‰å…¨æ¼æ´ï¼Œè®ºæ–‡å¯¹æ¯”åˆ†æäº†Random Forestã€XGBoostå’ŒLightGBMè¿™ä¸‰ç§å…ˆè¿›çš„é›†æˆå­¦ä¹ ç®—æ³•ï¼Œä»¥åº”å¯¹åŠ¨æ€ä¸”å¤æ‚çš„åƒµå°¸ç½‘ç»œå¨èƒã€‚é€šè¿‡åœ¨åŒ…å«è‰¯æ€§å’Œæ¶æ„æµé‡çš„ç‰©è”ç½‘ç½‘ç»œæµé‡æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒä¸è¯„ä¼°ï¼Œç ”ç©¶éªŒè¯äº†è¿™äº›æ¨¡å‹åœ¨è¯†åˆ«å’Œåˆ†ç±»Botnetæ´»åŠ¨æ–¹é¢çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ¢è®¨äº†å°†è¿™äº›æ¨¡å‹éƒ¨ç½²åœ¨èµ„æºå—é™çš„è¾¹ç¼˜å’ŒIoTè®¾å¤‡ä¸Šçš„å¯è¡Œæ€§ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ä»·å€¼ã€‚å®éªŒç»“æœçªæ˜¾äº†æœºå™¨å­¦ä¹ åœ¨å¼ºåŒ–ç‰©è”ç½‘ç½‘ç»œå®‰å…¨ã€æŠµå¾¡æ–°å…´ç½‘ç»œå¨èƒæ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01542v1",
      "published_date": "2025-08-03 01:52:35 UTC",
      "updated_date": "2025-08-03 01:52:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:37:14.631920+00:00"
    },
    {
      "arxiv_id": "2508.01540v1",
      "title": "MagicVL-2B: Empowering Vision-Language Models on Mobile Devices with Lightweight Visual Encoders via Curriculum Learning",
      "title_zh": "MagicVL-2Bï¼šé€šè¿‡è¯¾ç¨‹å­¦ä¹ ä¸è½»é‡åŒ–è§†è§‰ç¼–ç å™¨èµ‹èƒ½ç§»åŠ¨ç«¯è§†è§‰è¯­è¨€æ¨¡å‹",
      "authors": [
        "Yi Liu",
        "Xiao Xu",
        "Zeyu Xu",
        "Meng Zhang",
        "Yibo Li",
        "Haoyu Chen",
        "Junkang Zhang",
        "Qiang Wang",
        "Jifa Sun",
        "Siling Lin",
        "Shengxun Cheng",
        "Lingshu Zhang",
        "Kang Wang"
      ],
      "abstract": "Vision-Language Models (VLMs) have achieved remarkable breakthroughs in recent years, enabling a diverse array of applications in everyday life. However, the substantial computational and storage demands of VLMs pose significant challenges for their efficient deployment on mobile devices, which represent the most ubiquitous and accessible computing platforms today. In this work, we introduce MagicVL-2B, a novel VLM meticulously optimized for flagship smartphones. MagicVL-2B leverages a lightweight visual encoder with fewer than 100M parameters and features a redesigned dynamic resolution scheme that adaptively generates image tokens without excessive modification of image dimensions. To further enhance the performance of this compact encoder within VLMs, we propose a multimodal curriculum learning strategy that incrementally increases task difficulty and data information density throughout training. This approach substantially improves the model's performance across a variety of sub-tasks. Extensive evaluations on standard VLM benchmarks demonstrate that MagicVL-2B matches the accuracy of current state-of-the-art models while reducing on-device power consumption by 41.1%. These results establish MagicVL-2B as a practical and robust solution for real-world mobile vision-language applications, enabling advanced multimodal intelligence to run directly on smartphones.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† MagicVL-2Bï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºæ——èˆ°æ™ºèƒ½æ‰‹æœºä¼˜åŒ–çš„æ–°å‹å¤šæ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Models)ã€‚ä¸ºäº†åº”å¯¹ç§»åŠ¨è®¾å¤‡è®¡ç®—å’Œå­˜å‚¨èµ„æºçš„é™åˆ¶ï¼ŒMagicVL-2B é‡‡ç”¨äº†å‚æ•°é‡å°‘äº 100M çš„è½»é‡çº§è§†è§‰ç¼–ç å™¨ (visual encoder)ï¼Œå¹¶å¼•å…¥äº†é‡æ–°è®¾è®¡çš„åŠ¨æ€åˆ†è¾¨ç‡ (dynamic resolution) æ–¹æ¡ˆã€‚ç ”ç©¶å›¢é˜Ÿè¿˜æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€è¯¾ç¨‹å­¦ä¹  (curriculum learning) ç­–ç•¥ï¼Œé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥å¢åŠ ä»»åŠ¡éš¾åº¦å’Œä¿¡æ¯å¯†åº¦ï¼Œæ˜¾è‘—å¢å¼ºäº†ç´§å‡‘å‹ç¼–ç å™¨çš„æ€§èƒ½è¡¨ç°ã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMagicVL-2B åœ¨å‡†ç¡®ç‡è¾¾åˆ°å½“å‰æœ€å…ˆè¿›æ¨¡å‹æ°´å¹³çš„åŒæ—¶ï¼Œå°†ç§»åŠ¨ç«¯çš„åŠŸè€—å¤§å¹…é™ä½äº† 41.1%ã€‚è¯¥æˆæœè¯æ˜äº† MagicVL-2B æ˜¯åœ¨æ™ºèƒ½æ‰‹æœºä¸Šè¿è¡Œå…ˆè¿›è§†è§‰è¯­è¨€åº”ç”¨çš„å®ç”¨ä¸”ç¨³å¥çš„è§£å†³æ–¹æ¡ˆï¼Œæœ‰æ•ˆæ¨åŠ¨äº†ç§»åŠ¨ç«¯å¤šæ¨¡æ€æ™ºèƒ½çš„å‘å±•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01540v1",
      "published_date": "2025-08-03 01:49:08 UTC",
      "updated_date": "2025-08-03 01:49:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:37:28.705562+00:00"
    },
    {
      "arxiv_id": "2508.01531v1",
      "title": "Revisiting Gossip Protocols: A Vision for Emergent Coordination in Agentic Multi-Agent Systems",
      "title_zh": "é‡å®¡ Gossip åè®®ï¼šAgentic å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­æ¶Œç°å¼åä½œçš„æ„¿æ™¯",
      "authors": [
        "Mansura Habiba",
        "Nafiul I. Khan"
      ],
      "abstract": "As agentic platforms scale, agents are evolving beyond static roles and fixed toolchains, creating a growing need for flexible, decentralized coordination. Today's structured communication protocols (e.g., direct agent-to-agent messaging) excel at reliability and task delegation, but they fall short in enabling emergent, swarm-like intelligence, where distributed agents continuously learn, adapt, and communicate to form collective cognition. This paper revisits gossip protocols, long valued in distributed systems for their fault tolerance and decentralization, and argues that they offer a missing layer for context-rich, adaptive communication in agentic AI. Gossip enables scalable, low-overhead dissemination of shared knowledge, but also raises unresolved challenges around semantic filtering, staleness, trustworthiness, and consistency in high-stakes environments. Rather than proposing a new framework, this work charts a research agenda for integrating gossip as a complementary substrate alongside structured protocols. We identify critical gaps in current agent-to-agent architectures, highlight where gossip could reshape assumptions about coordination, and outline open questions around intent propagation, knowledge decay, and peer-to-peer trust. Gossip is not a silver bullet, but overlooking it risks missing a key path toward resilient, reflexive, and self-organizing multi-agent systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é‡æ–°æ¢è®¨äº†åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­å…·æœ‰é«˜å®¹é”™æ€§å’Œå»ä¸­å¿ƒåŒ–ç‰¹å¾çš„ Gossip Protocolsï¼Œæ—¨åœ¨è§£å†³ Agentic Multi-Agent Systems åœ¨è§„æ¨¡æ‰©å¤§æ—¶å¯¹çµæ´»ã€å»ä¸­å¿ƒåŒ–åè°ƒæœºåˆ¶çš„éœ€æ±‚ã€‚ä½œè€…æŒ‡å‡ºå½“å‰çš„ç»“æ„åŒ–é€šä¿¡åè®®è™½èƒ½ä¿è¯å¯é æ€§ï¼Œä½†åœ¨å®ç°ç¾¤æ™ºæ¶Œç°(Swarm-like Intelligence)å’Œé›†ä½“è®¤çŸ¥(Collective Cognition)æ–¹é¢å­˜åœ¨å±€é™ã€‚è®ºæ–‡æå‡ºå°† Gossip åè®®ä½œä¸ºä¸€ç§è¡¥å……æ€§çš„åº•å±‚æ¶æ„ï¼Œä¸ºæ™ºèƒ½ä½“æä¾›ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¸”å…·å¤‡é€‚åº”æ€§çš„é€šä¿¡å±‚ï¼Œä»è€Œå®ç°ä½å¼€é”€çš„å…±äº«çŸ¥è¯†ä¼ æ’­ã€‚è¯¥å·¥ä½œå¹¶éæå‡ºå…·ä½“æ¡†æ¶ï¼Œè€Œæ˜¯åˆ¶å®šäº†ä¸€é¡¹ç ”ç©¶è®®ç¨‹ï¼Œåˆ†æäº†è¯­ä¹‰è¿‡æ»¤(Semantic Filtering)ã€ä¿¡æ¯é™ˆæ—§æ€§(Staleness)å’Œå¯¹ç­‰ä¿¡ä»»(Peer-to-Peer Trust)ç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚é€šè¿‡è¯†åˆ«ç°æœ‰ Agent-to-Agent æ¶æ„çš„ç¼ºé™·ï¼Œæ–‡ç« æ¢è®¨äº†æ„å›¾ä¼ æ’­(Intent Propagation)å’ŒçŸ¥è¯†è¡°å‡(Knowledge Decay)ç­‰å‰æ²¿é—®é¢˜ã€‚è¿™ä¸€æ„¿æ™¯ä¸ºæ„å»ºå…·æœ‰éŸ§æ€§(Resilient)ã€åæ€æ€§(Reflexive)å’Œè‡ªç»„ç»‡èƒ½åŠ›çš„æ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†å…³é”®è·¯å¾„ï¼Œå¼ºè°ƒäº† Gossip åè®®åœ¨æœªæ¥å¤šæ™ºèƒ½ä½“åä½œä¸­çš„é‡è¦æ½œåŠ›ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.01531v1",
      "published_date": "2025-08-03 01:18:58 UTC",
      "updated_date": "2025-08-03 01:18:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:37:27.244541+00:00"
    },
    {
      "arxiv_id": "2508.01525v1",
      "title": "MiraGe: Multimodal Discriminative Representation Learning for Generalizable AI-Generated Image Detection",
      "title_zh": "MiraGeï¼šé¢å‘å¯æ³›åŒ–AIç”Ÿæˆå›¾åƒæ£€æµ‹çš„å¤šæ¨¡æ€åˆ¤åˆ«å¼è¡¨å¾å­¦ä¹ ",
      "authors": [
        "Kuo Shi",
        "Jie Lu",
        "Shanshan Ye",
        "Guangquan Zhang",
        "Zhen Fang"
      ],
      "abstract": "Recent advances in generative models have highlighted the need for robust detectors capable of distinguishing real images from AI-generated images. While existing methods perform well on known generators, their performance often declines when tested with newly emerging or unseen generative models due to overlapping feature embeddings that hinder accurate cross-generator classification. In this paper, we propose Multimodal Discriminative Representation Learning for Generalizable AI-generated Image Detection (MiraGe), a method designed to learn generator-invariant features. Motivated by theoretical insights on intra-class variation minimization and inter-class separation, MiraGe tightly aligns features within the same class while maximizing separation between classes, enhancing feature discriminability. Moreover, we apply multimodal prompt learning to further refine these principles into CLIP, leveraging text embeddings as semantic anchors for effective discriminative representation learning, thereby improving generalizability. Comprehensive experiments across multiple benchmarks show that MiraGe achieves state-of-the-art performance, maintaining robustness even against unseen generators like Sora.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MiraGeï¼Œä¸€ç§ç”¨äºé€šç”¨ AI ç”Ÿæˆå›¾åƒæ£€æµ‹çš„å¤šæ¨¡æ€åˆ¤åˆ«è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ£€æµ‹å™¨åœ¨é¢å¯¹æœªçŸ¥ç”Ÿæˆå™¨æ—¶å› ç‰¹å¾é‡å å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚å—ç±»å†…å˜å¼‚æœ€å°åŒ–å’Œç±»é—´åˆ†ç¦»ç†è®ºçš„å¯å‘ï¼ŒMiraGe é€šè¿‡ç´§å¯†å¯¹é½åŒç±»ç‰¹å¾å¹¶æœ€å¤§åŒ–ç±»é—´è·ç¦»æ¥å­¦ä¹ è·¨ç”Ÿæˆå™¨ä¸å˜çš„ç‰¹å¾ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºäº†ç‰¹å¾çš„åˆ¤åˆ«æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿå°†å¤šæ¨¡æ€æç¤ºå­¦ä¹  (Multimodal Prompt Learning) åº”ç”¨äº CLIP æ¡†æ¶ï¼Œåˆ©ç”¨æ–‡æœ¬åµŒå…¥ä½œä¸ºè¯­ä¹‰é”šç‚¹ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMiraGe åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿› (State-of-the-art) çš„æ°´å¹³ã€‚è¯¥æ¡†æ¶åœ¨å¤„ç†å¦‚ Sora ç­‰ä»æœªè§è¿‡çš„ç”Ÿæˆæ¨¡å‹æ—¶ï¼Œä¾ç„¶è¡¨ç°å‡ºæé«˜çš„æ£€æµ‹å‡†ç¡®ç‡å’Œç¨³å¥æ€§ï¼Œè¯æ˜äº†å…¶ä¼˜å¼‚çš„è·¨é¢†åŸŸåº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ACMMM 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.01525v1",
      "published_date": "2025-08-03 00:19:18 UTC",
      "updated_date": "2025-08-03 00:19:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T08:37:29.729519+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 95,
  "processed_papers_count": 95,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T08:47:47.612374+00:00"
}
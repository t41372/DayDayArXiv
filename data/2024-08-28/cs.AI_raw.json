[
  {
    "arxiv_id": "2408.16173v1",
    "title": "LLM-assisted Labeling Function Generation for Semantic Type Detection",
    "authors": [
      "Chenjie Li",
      "Dan Zhang",
      "Jin Wang"
    ],
    "abstract": "Detecting semantic types of columns in data lake tables is an important\napplication. A key bottleneck in semantic type detection is the availability of\nhuman annotation due to the inherent complexity of data lakes. In this paper,\nwe propose using programmatic weak supervision to assist in annotating the\ntraining data for semantic type detection by leveraging labeling functions. One\nchallenge in this process is the difficulty of manually writing labeling\nfunctions due to the large volume and low quality of the data lake table\ndatasets. To address this issue, we explore employing Large Language Models\n(LLMs) for labeling function generation and introduce several prompt\nengineering strategies for this purpose. We conduct experiments on real-world\nweb table datasets. Based on the initial results, we perform extensive analysis\nand provide empirical insights and future directions for researchers in this\nfield.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "VLDB'24-DATAI",
    "pdf_url": "http://arxiv.org/pdf/2408.16173v1",
    "published_date": "2024-08-28 23:39:50 UTC",
    "updated_date": "2024-08-28 23:39:50 UTC"
  },
  {
    "arxiv_id": "2408.16169v1",
    "title": "Simulating realistic short tandem repeat capillary electrophoretic signal using a generative adversarial network",
    "authors": [
      "Duncan Taylor",
      "Melissa Humphries"
    ],
    "abstract": "DNA profiles are made up from multiple series of electrophoretic signal\nmeasuring fluorescence over time. Typically, human DNA analysts 'read' DNA\nprofiles using their experience to distinguish instrument noise, artefactual\nsignal, and signal corresponding to DNA fragments of interest. Recent work has\ndeveloped an artificial neural network, ANN, to carry out the task of\nclassifying fluorescence types into categories in DNA profile electrophoretic\nsignal. But the creation of the necessarily large amount of labelled training\ndata for the ANN is time consuming and expensive, and a limiting factor in the\nability to robustly train the ANN. If realistic, prelabelled, training data\ncould be simulated then this would remove the barrier to training an ANN with\nhigh efficacy. Here we develop a generative adversarial network, GAN, modified\nfrom the pix2pix GAN to achieve this task. With 1078 DNA profiles we train the\nGAN and achieve the ability to simulate DNA profile information, and then use\nthe generator from the GAN as a 'realism filter' that applies the noise and\nartefact elements exhibited in typical electrophoretic signal.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "29 pages, 9 Figures",
    "pdf_url": "http://arxiv.org/pdf/2408.16169v1",
    "published_date": "2024-08-28 23:20:17 UTC",
    "updated_date": "2024-08-28 23:20:17 UTC"
  },
  {
    "arxiv_id": "2408.16163v2",
    "title": "FRACTURED-SORRY-Bench: Framework for Revealing Attacks in Conversational Turns Undermining Refusal Efficacy and Defenses over SORRY-Bench (Automated Multi-shot Jailbreaks)",
    "authors": [
      "Aman Priyanshu",
      "Supriti Vijay"
    ],
    "abstract": "This paper introduces FRACTURED-SORRY-Bench, a framework for evaluating the\nsafety of Large Language Models (LLMs) against multi-turn conversational\nattacks. Building upon the SORRY-Bench dataset, we propose a simple yet\neffective method for generating adversarial prompts by breaking down harmful\nqueries into seemingly innocuous sub-questions. Our approach achieves a maximum\nincrease of +46.22\\% in Attack Success Rates (ASRs) across GPT-4, GPT-4o,\nGPT-4o-mini, and GPT-3.5-Turbo models compared to baseline methods. We\ndemonstrate that this technique poses a challenge to current LLM safety\nmeasures and highlights the need for more robust defenses against subtle,\nmulti-turn attacks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "4 pages, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2408.16163v2",
    "published_date": "2024-08-28 22:51:29 UTC",
    "updated_date": "2024-11-07 15:48:11 UTC"
  },
  {
    "arxiv_id": "2409.00125v3",
    "title": "A Hybrid Framework for Spatial Interpolation: Merging Data-driven with Domain Knowledge",
    "authors": [
      "Cong Zhang",
      "Shuyi Du",
      "Hongqing Song",
      "Yuhe Wang"
    ],
    "abstract": "Estimating spatially distributed information through the interpolation of\nscattered observation datasets often overlooks the critical role of domain\nknowledge in understanding spatial dependencies. Additionally, the features of\nthese data sets are typically limited to the spatial coordinates of the\nscattered observation locations. In this paper, we propose a hybrid framework\nthat integrates data-driven spatial dependency feature extraction with\nrule-assisted spatial dependency function mapping to augment domain knowledge.\nWe demonstrate the superior performance of our framework in two comparative\napplication scenarios, highlighting its ability to capture more localized\nspatial features in the reconstructed distribution fields. Furthermore, we\nunderscore its potential to enhance nonlinear estimation capabilities through\nthe application of transformed fuzzy rules and to quantify the inherent\nuncertainties associated with the observation data sets. Our framework\nintroduces an innovative approach to spatial information estimation by\nsynergistically combining observational data with rule-assisted domain\nknowledge.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 13 figures; typos corrected, references updated; few typos\n  in few equations corrected, changed to Tex source",
    "pdf_url": "http://arxiv.org/pdf/2409.00125v3",
    "published_date": "2024-08-28 22:02:42 UTC",
    "updated_date": "2024-09-06 04:39:17 UTC"
  },
  {
    "arxiv_id": "2408.16126v1",
    "title": "Improving Generalization of Speech Separation in Real-World Scenarios: Strategies in Simulation, Optimization, and Evaluation",
    "authors": [
      "Ke Chen",
      "Jiaqi Su",
      "Taylor Berg-Kirkpatrick",
      "Shlomo Dubnov",
      "Zeyu Jin"
    ],
    "abstract": "Achieving robust speech separation for overlapping speakers in various\nacoustic environments with noise and reverberation remains an open challenge.\nAlthough existing datasets are available to train separators for specific\nscenarios, they do not effectively generalize across diverse real-world\nscenarios. In this paper, we present a novel data simulation pipeline that\nproduces diverse training data from a range of acoustic environments and\ncontent, and propose new training paradigms to improve quality of a general\nspeech separation model. Specifically, we first introduce AC-SIM, a data\nsimulation pipeline that incorporates broad variations in both content and\nacoustics. Then we integrate multiple training objectives into the permutation\ninvariant training (PIT) to enhance separation quality and generalization of\nthe trained model. Finally, we conduct comprehensive objective and human\nlistening experiments across separation architectures and benchmarks to\nvalidate our methods, demonstrating substantial improvement of generalization\non both non-homologous and real-world test sets.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "In Proceedings of the 25th Annual Conference of the International\n  Speech Communication Association, Interspeech 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.16126v1",
    "published_date": "2024-08-28 20:26:34 UTC",
    "updated_date": "2024-08-28 20:26:34 UTC"
  },
  {
    "arxiv_id": "2408.16123v1",
    "title": "ChartEye: A Deep Learning Framework for Chart Information Extraction",
    "authors": [
      "Osama Mustafa",
      "Muhammad Khizer Ali",
      "Momina Moetesum",
      "Imran Siddiqi"
    ],
    "abstract": "The widespread use of charts and infographics as a means of data\nvisualization in various domains has inspired recent research in automated\nchart understanding. However, information extraction from chart images is a\ncomplex multitasked process due to style variations and, as a consequence, it\nis challenging to design an end-to-end system. In this study, we propose a deep\nlearning-based framework that provides a solution for key steps in the chart\ninformation extraction pipeline. The proposed framework utilizes hierarchal\nvision transformers for the tasks of chart-type and text-role classification,\nwhile YOLOv7 for text detection. The detected text is then enhanced using Super\nResolution Generative Adversarial Networks to improve the recognition output of\nthe OCR. Experimental results on a benchmark dataset show that our proposed\nframework achieves excellent performance at every stage with F1-scores of 0.97\nfor chart-type classification, 0.91 for text-role classification, and a mean\nAverage Precision of 0.95 for text detection.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "8 Pages, and 11 Figures",
    "pdf_url": "http://arxiv.org/pdf/2408.16123v1",
    "published_date": "2024-08-28 20:22:39 UTC",
    "updated_date": "2024-08-28 20:22:39 UTC"
  },
  {
    "arxiv_id": "2408.16119v2",
    "title": "Data Formulator 2: Iterative Creation of Data Visualizations, with AI Transforming Data Along the Way",
    "authors": [
      "Chenglong Wang",
      "Bongshin Lee",
      "Steven Drucker",
      "Dan Marshall",
      "Jianfeng Gao"
    ],
    "abstract": "Data analysts often need to iterate between data transformations and chart\ndesigns to create rich visualizations for exploratory data analysis. Although\nmany AI-powered systems have been introduced to reduce the effort of\nvisualization authoring, existing systems are not well suited for iterative\nauthoring. They typically require analysts to provide, in a single turn, a\ntext-only prompt that fully describe a complex visualization. We introduce Data\nFormulator 2 (DF2 for short), an AI-powered visualization system designed to\novercome this limitation. DF2 blends graphical user interfaces and natural\nlanguage inputs to enable users to convey their intent more effectively, while\ndelegating data transformation to AI. Furthermore, to support efficient\niteration, DF2 lets users navigate their iteration history and reuse previous\ndesigns, eliminating the need to start from scratch each time. A user study\nwith eight participants demonstrated that DF2 allowed participants to develop\ntheir own iteration styles to complete challenging data exploration sessions.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16119v2",
    "published_date": "2024-08-28 20:12:17 UTC",
    "updated_date": "2025-02-21 00:50:26 UTC"
  },
  {
    "arxiv_id": "2408.16081v1",
    "title": "Logic-Enhanced Language Model Agents for Trustworthy Social Simulations",
    "authors": [
      "Agnieszka Mensfelt",
      "Kostas Stathis",
      "Vince Trencsenyi"
    ],
    "abstract": "We introduce the Logic-Enhanced Language Model Agents (LELMA) framework, a\nnovel approach to enhance the trustworthiness of social simulations that\nutilize large language models (LLMs). While LLMs have gained attention as\nagents for simulating human behaviour, their applicability in this role is\nlimited by issues such as inherent hallucinations and logical inconsistencies.\nLELMA addresses these challenges by integrating LLMs with symbolic AI, enabling\nlogical verification of the reasoning generated by LLMs. This verification\nprocess provides corrective feedback, refining the reasoning output. The\nframework consists of three main components: an LLM-Reasoner for producing\nstrategic reasoning, an LLM-Translator for mapping natural language reasoning\nto logic queries, and a Solver for evaluating these queries. This study focuses\non decision-making in game-theoretic scenarios as a model of human interaction.\nExperiments involving the Hawk-Dove game, Prisoner's Dilemma, and Stag Hunt\nhighlight the limitations of state-of-the-art LLMs, GPT-4 Omni and Gemini 1.0\nPro, in producing correct reasoning in these contexts. LELMA demonstrates high\naccuracy in error detection and improves the reasoning correctness of LLMs via\nself-refinement, particularly in GPT-4 Omni.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.GT",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "Source code: https://github.com/dicelab-rhul/LELMA",
    "pdf_url": "http://arxiv.org/pdf/2408.16081v1",
    "published_date": "2024-08-28 18:25:35 UTC",
    "updated_date": "2024-08-28 18:25:35 UTC"
  },
  {
    "arxiv_id": "2408.16074v2",
    "title": "Verification methods for international AI agreements",
    "authors": [
      "Akash R. Wasil",
      "Tom Reed",
      "Jack William Miller",
      "Peter Barnett"
    ],
    "abstract": "What techniques can be used to verify compliance with international\nagreements about advanced AI development? In this paper, we examine 10\nverification methods that could detect two types of potential violations:\nunauthorized AI training (e.g., training runs above a certain FLOP threshold)\nand unauthorized data centers. We divide the verification methods into three\ncategories: (a) national technical means (methods requiring minimal or no\naccess from suspected non-compliant nations), (b) access-dependent methods\n(methods that require approval from the nation suspected of unauthorized\nactivities), and (c) hardware-dependent methods (methods that require rules\naround advanced hardware). For each verification method, we provide a\ndescription, historical precedents, and possible evasion techniques. We\nconclude by offering recommendations for future work related to the\nverification and enforcement of international AI governance agreements.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16074v2",
    "published_date": "2024-08-28 18:15:19 UTC",
    "updated_date": "2024-11-04 20:46:07 UTC"
  },
  {
    "arxiv_id": "2408.16073v2",
    "title": "Using Large Language Models to Create AI Personas for Replication, Generalization and Prediction of Media Effects: An Empirical Test of 133 Published Experimental Research Findings",
    "authors": [
      "Leo Yeykelis",
      "Kaavya Pichai",
      "James J. Cummings",
      "Byron Reeves"
    ],
    "abstract": "This report analyzes the potential for large language models (LLMs) to\nexpedite accurate replication and generalization of published research about\nmessage effects in marketing. LLM-powered participants (personas) were tested\nby replicating 133 experimental findings from 14 papers containing 45 recent\nstudies published in the Journal of Marketing. For each study, the measures,\nstimuli, and sampling specifications were used to generate prompts for LLMs to\nact as unique personas. The AI personas, 19,447 in total across all of the\nstudies, generated complete datasets and statistical analyses were then\ncompared with the original human study results. The LLM replications\nsuccessfully reproduced 76% of the original main effects (84 out of 111),\ndemonstrating strong potential for AI-assisted replication. The overall\nreplication rate including interaction effects was 68% (90 out of 133).\nFurthermore, a test of how human results generalized to different participant\nsamples, media stimuli, and measures showed that replication results can change\nwhen tests go beyond the parameters of the original human studies. Implications\nare discussed for the replication and generalizability crises in social\nscience, the acceleration of theory building in media and marketing psychology,\nand the practical advantages of rapid message testing for consumer products.\nLimitations of AI replications are addressed with respect to complex\ninteraction effects, biases in AI models, and establishing benchmarks for AI\nmetrics in marketing research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "40 pages, 13 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2408.16073v2",
    "published_date": "2024-08-28 18:14:39 UTC",
    "updated_date": "2025-04-24 19:12:58 UTC"
  },
  {
    "arxiv_id": "2408.16068v2",
    "title": "Identification of Prognostic Biomarkers for Stage III Non-Small Cell Lung Carcinoma in Female Nonsmokers Using Machine Learning",
    "authors": [
      "Huili Zheng",
      "Qimin Zhang",
      "Yiru Gong",
      "Zheyan Liu",
      "Shaohan Chen"
    ],
    "abstract": "Lung cancer remains a leading cause of cancer-related deaths globally, with\nnon-small cell lung cancer (NSCLC) being the most common subtype. This study\naimed to identify key biomarkers associated with stage III NSCLC in non-smoking\nfemales using gene expression profiling from the GDS3837 dataset. Utilizing\nXGBoost, a machine learning algorithm, the analysis achieved a strong\npredictive performance with an AUC score of 0.835. The top biomarkers\nidentified - CCAAT enhancer binding protein alpha (C/EBP-alpha), lactate\ndehydrogenase A4 (LDHA), UNC-45 myosin chaperone B (UNC-45B), checkpoint kinase\n1 (CHK1), and hypoxia-inducible factor 1 subunit alpha (HIF-1-alpha) - have\nbeen validated in the literature as being significantly linked to lung cancer.\nThese findings highlight the potential of these biomarkers for early diagnosis\nand personalized therapy, emphasizing the value of integrating machine learning\nwith molecular profiling in cancer research.",
    "categories": [
      "q-bio.GN",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "q-bio.GN",
    "comment": "This paper has been accepted for publication in the IEEE ICBASE 2024\n  conference",
    "pdf_url": "http://arxiv.org/pdf/2408.16068v2",
    "published_date": "2024-08-28 18:08:11 UTC",
    "updated_date": "2024-08-30 03:38:06 UTC"
  },
  {
    "arxiv_id": "2408.15998v2",
    "title": "Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders",
    "authors": [
      "Min Shi",
      "Fuxiao Liu",
      "Shihao Wang",
      "Shijia Liao",
      "Subhashree Radhakrishnan",
      "Yilin Zhao",
      "De-An Huang",
      "Hongxu Yin",
      "Karan Sapra",
      "Yaser Yacoob",
      "Humphrey Shi",
      "Bryan Catanzaro",
      "Andrew Tao",
      "Jan Kautz",
      "Zhiding Yu",
      "Guilin Liu"
    ],
    "abstract": "The ability to accurately interpret complex visual information is a crucial\ntopic of multimodal large language models (MLLMs). Recent work indicates that\nenhanced visual perception significantly reduces hallucinations and improves\nperformance on resolution-sensitive tasks, such as optical character\nrecognition and document analysis. A number of recent MLLMs achieve this goal\nusing a mixture of vision encoders. Despite their success, there is a lack of\nsystematic comparisons and detailed ablation studies addressing critical\naspects, such as expert selection and the integration of multiple vision\nexperts. This study provides an extensive exploration of the design space for\nMLLMs using a mixture of vision encoders and resolutions. Our findings reveal\nseveral underlying principles common to various existing strategies, leading to\na streamlined yet effective design approach. We discover that simply\nconcatenating visual tokens from a set of complementary vision encoders is as\neffective as more complex mixing architectures or strategies. We additionally\nintroduce Pre-Alignment to bridge the gap between vision-focused encoders and\nlanguage tokens, enhancing model coherence. The resulting family of MLLMs,\nEagle, surpasses other leading open-source models on major MLLM benchmarks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Github: https://github.com/NVlabs/Eagle, HuggingFace:\n  https://huggingface.co/NVEagle",
    "pdf_url": "http://arxiv.org/pdf/2408.15998v2",
    "published_date": "2024-08-28 17:59:31 UTC",
    "updated_date": "2025-03-02 23:41:37 UTC"
  },
  {
    "arxiv_id": "2408.15997v1",
    "title": "Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need",
    "authors": [
      "Sijia Peng",
      "Yun Xiong",
      "Yangyong Zhu",
      "Zhiqiang Shen"
    ],
    "abstract": "Time series forecasting requires balancing short-term and long-term\ndependencies for accurate predictions. Existing methods mainly focus on\nlong-term dependency modeling, neglecting the complexities of short-term\ndynamics, which may hinder performance. Transformers are superior in modeling\nlong-term dependencies but are criticized for their quadratic computational\ncost. Mamba provides a near-linear alternative but is reported less effective\nin time series longterm forecasting due to potential information loss. Current\narchitectures fall short in offering both high efficiency and strong\nperformance for long-term dependency modeling. To address these challenges, we\nintroduce Mixture of Universals (MoU), a versatile model to capture both\nshort-term and long-term dependencies for enhancing performance in time series\nforecasting. MoU is composed of two novel designs: Mixture of Feature\nExtractors (MoF), an adaptive method designed to improve time series patch\nrepresentations for short-term dependency, and Mixture of Architectures (MoA),\nwhich hierarchically integrates Mamba, FeedForward, Convolution, and\nSelf-Attention architectures in a specialized order to model long-term\ndependency from a hybrid perspective. The proposed approach achieves\nstate-of-the-art performance while maintaining relatively low computational\ncosts. Extensive experiments on seven real-world datasets demonstrate the\nsuperiority of MoU. Code is available at https://github.com/lunaaa95/mou/.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Code at https://github.com/lunaaa95/mou/",
    "pdf_url": "http://arxiv.org/pdf/2408.15997v1",
    "published_date": "2024-08-28 17:59:27 UTC",
    "updated_date": "2024-08-28 17:59:27 UTC"
  },
  {
    "arxiv_id": "2408.15996v3",
    "title": "Spatio-Temporal Context Prompting for Zero-Shot Action Detection",
    "authors": [
      "Wei-Jhe Huang",
      "Min-Hung Chen",
      "Shang-Hong Lai"
    ],
    "abstract": "Spatio-temporal action detection encompasses the tasks of localizing and\nclassifying individual actions within a video. Recent works aim to enhance this\nprocess by incorporating interaction modeling, which captures the relationship\nbetween people and their surrounding context. However, these approaches have\nprimarily focused on fully-supervised learning, and the current limitation lies\nin the lack of generalization capability to recognize unseen action categories.\nIn this paper, we aim to adapt the pretrained image-language models to detect\nunseen actions. To this end, we propose a method which can effectively leverage\nthe rich knowledge of visual-language models to perform Person-Context\nInteraction. Meanwhile, our Context Prompting module will utilize contextual\ninformation to prompt labels, thereby enhancing the generation of more\nrepresentative text features. Moreover, to address the challenge of recognizing\ndistinct actions by multiple people at the same timestamp, we design the\nInterest Token Spotting mechanism which employs pretrained visual knowledge to\nfind each person's interest context tokens, and then these tokens will be used\nfor prompting to generate text features tailored to each individual. To\nevaluate the ability to detect unseen actions, we propose a comprehensive\nbenchmark on J-HMDB, UCF101-24, and AVA datasets. The experiments show that our\nmethod achieves superior results compared to previous approaches and can be\nfurther extended to multi-action videos, bringing it closer to real-world\napplications. The code and data can be found in\nhttps://webber2933.github.io/ST-CLIP-project-page.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by WACV2025. Project page:\n  https://webber2933.github.io/ST-CLIP-project-page",
    "pdf_url": "http://arxiv.org/pdf/2408.15996v3",
    "published_date": "2024-08-28 17:59:05 UTC",
    "updated_date": "2024-12-05 14:38:12 UTC"
  },
  {
    "arxiv_id": "2408.15992v1",
    "title": "CoGen: Learning from Feedback with Coupled Comprehension and Generation",
    "authors": [
      "Mustafa Omer Gul",
      "Yoav Artzi"
    ],
    "abstract": "Systems with both language comprehension and generation capabilities can\nbenefit from the tight connection between the two. This work studies coupling\ncomprehension and generation with focus on continually learning from\ninteraction with users. We propose techniques to tightly integrate the two\ncapabilities for both learning and inference. We situate our studies in\ntwo-player reference games, and deploy various models for thousands of\ninteractions with human users, while learning from interaction feedback\nsignals. We show dramatic improvements in performance over time, with\ncomprehension-generation coupling leading to performance improvements up to 26%\nin absolute terms and up to 17% higher accuracies compared to a non-coupled\nsystem. Our analysis also shows coupling has substantial qualitative impact on\nthe system's language, making it significantly more human-like.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.15992v1",
    "published_date": "2024-08-28 17:58:39 UTC",
    "updated_date": "2024-08-28 17:58:39 UTC"
  },
  {
    "arxiv_id": "2408.15980v2",
    "title": "In-Context Imitation Learning via Next-Token Prediction",
    "authors": [
      "Letian Fu",
      "Huang Huang",
      "Gaurav Datta",
      "Lawrence Yunliang Chen",
      "William Chung-Ho Panitch",
      "Fangchen Liu",
      "Hui Li",
      "Ken Goldberg"
    ],
    "abstract": "We explore how to enhance next-token prediction models to perform in-context\nimitation learning on a real robot, where the robot executes new tasks by\ninterpreting contextual information provided during the input phase, without\nupdating its underlying policy parameters. We propose In-Context Robot\nTransformer (ICRT), a causal transformer that performs autoregressive\nprediction on sensorimotor trajectories without relying on any linguistic data\nor reward function. This formulation enables flexible and training-free\nexecution of new tasks at test time, achieved by prompting the model with\nsensorimotor trajectories of the new task composing of image observations,\nactions and states tuples, collected through human teleoperation. Experiments\nwith a Franka Emika robot demonstrate that the ICRT can adapt to new tasks\nspecified by prompts, even in environment configurations that differ from both\nthe prompt and the training data. In a multitask environment setup, ICRT\nsignificantly outperforms current state-of-the-art next-token prediction models\nin robotics on generalizing to unseen tasks. Code, checkpoints and data are\navailable on https://icrt.dev/",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15980v2",
    "published_date": "2024-08-28 17:50:19 UTC",
    "updated_date": "2024-09-27 20:10:08 UTC"
  },
  {
    "arxiv_id": "2408.15978v1",
    "title": "WebPilot: A Versatile and Autonomous Multi-Agent System for Web Task Execution with Strategic Exploration",
    "authors": [
      "Yao Zhang",
      "Zijian Ma",
      "Yunpu Ma",
      "Zhen Han",
      "Yu Wu",
      "Volker Tresp"
    ],
    "abstract": "LLM-based autonomous agents often fail to execute complex web tasks that\nrequire dynamic interaction due to the inherent uncertainty and complexity of\nthese environments. Existing LLM-based web agents typically rely on rigid,\nexpert-designed policies specific to certain states and actions, which lack the\nflexibility and generalizability needed to adapt to unseen tasks. In contrast,\nhumans excel by exploring unknowns, continuously adapting strategies, and\nresolving ambiguities through exploration. To emulate human-like adaptability,\nweb agents need strategic exploration and complex decision-making. Monte Carlo\nTree Search (MCTS) is well-suited for this, but classical MCTS struggles with\nvast action spaces, unpredictable state transitions, and incomplete information\nin web tasks. In light of this, we develop WebPilot, a multi-agent system with\na dual optimization strategy that improves MCTS to better handle complex web\nenvironments. Specifically, the Global Optimization phase involves generating a\nhigh-level plan by breaking down tasks into manageable subtasks and\ncontinuously refining this plan, thereby focusing the search process and\nmitigating the challenges posed by vast action spaces in classical MCTS.\nSubsequently, the Local Optimization phase executes each subtask using a\ntailored MCTS designed for complex environments, effectively addressing\nuncertainties and managing incomplete information. Experimental results on\nWebArena and MiniWoB++ demonstrate the effectiveness of WebPilot. Notably, on\nWebArena, WebPilot achieves SOTA performance with GPT-4, achieving a 93%\nrelative increase in success rate over the concurrent tree search-based method.\nWebPilot marks a significant advancement in general autonomous agent\ncapabilities, paving the way for more advanced and reliable decision-making in\npractical environments.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15978v1",
    "published_date": "2024-08-28 17:49:29 UTC",
    "updated_date": "2024-08-28 17:49:29 UTC"
  },
  {
    "arxiv_id": "2408.15969v1",
    "title": "Stability of Primal-Dual Gradient Flow Dynamics for Multi-Block Convex Optimization Problems",
    "authors": [
      "Ibrahim K. Ozaslan",
      "Panagiotis Patrinos",
      "Mihailo R. Jovanović"
    ],
    "abstract": "We examine stability properties of primal-dual gradient flow dynamics for\ncomposite convex optimization problems with multiple, possibly nonsmooth, terms\nin the objective function under the generalized consensus constraint. The\nproposed dynamics are based on the proximal augmented Lagrangian and they\nprovide a viable alternative to ADMM which faces significant challenges from\nboth analysis and implementation viewpoints in large-scale multi-block\nscenarios. In contrast to customized algorithms with individualized convergence\nguarantees, we provide a systematic approach for solving a broad class of\nchallenging composite optimization problems. We leverage various structural\nproperties to establish global (exponential) convergence guarantees for the\nproposed dynamics. Our assumptions are much weaker than those required to prove\n(exponential) stability of various primal-dual dynamics as well as (linear)\nconvergence of discrete-time methods, e.g., standard two-block and multi-block\nADMM and EXTRA algorithms. Finally, we show necessity of some of our structural\nassumptions for exponential stability and provide computational experiments to\ndemonstrate the convenience of the proposed dynamics for parallel and\ndistributed computing applications.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "math.OC",
    "comment": "31 pages; 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.15969v1",
    "published_date": "2024-08-28 17:43:18 UTC",
    "updated_date": "2024-08-28 17:43:18 UTC"
  },
  {
    "arxiv_id": "2408.15966v2",
    "title": "More Text, Less Point: Towards 3D Data-Efficient Point-Language Understanding",
    "authors": [
      "Yuan Tang",
      "Xu Han",
      "Xianzhi Li",
      "Qiao Yu",
      "Jinfeng Xu",
      "Yixue Hao",
      "Long Hu",
      "Min Chen"
    ],
    "abstract": "Enabling Large Language Models (LLMs) to comprehend the 3D physical world\nremains a significant challenge. Due to the lack of large-scale 3D-text pair\ndatasets, the success of LLMs has yet to be replicated in 3D understanding. In\nthis paper, we rethink this issue and propose a new task: 3D Data-Efficient\nPoint-Language Understanding. The goal is to enable LLMs to achieve robust 3D\nobject understanding with minimal 3D point cloud and text data pairs. To\naddress this task, we introduce GreenPLM, which leverages more text data to\ncompensate for the lack of 3D data. First, inspired by using CLIP to align\nimages and text, we utilize a pre-trained point cloud-text encoder to map the\n3D point cloud space to the text space. This mapping leaves us to seamlessly\nconnect the text space with LLMs. Once the point-text-LLM connection is\nestablished, we further enhance text-LLM alignment by expanding the\nintermediate text space, thereby reducing the reliance on 3D point cloud data.\nSpecifically, we generate 6M free-text descriptions of 3D objects, and design a\nthree-stage training strategy to help LLMs better explore the intrinsic\nconnections between different modalities. To achieve efficient modality\nalignment, we design a zero-parameter cross-attention module for token pooling.\nExtensive experimental results show that GreenPLM requires only 12% of the 3D\ntraining data used by existing state-of-the-art models to achieve superior 3D\nunderstanding. Remarkably, GreenPLM also achieves competitive performance using\ntext-only data. The code and weights are available at:\nhttps://github.com/TangYuan96/GreenPLM.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15966v2",
    "published_date": "2024-08-28 17:38:44 UTC",
    "updated_date": "2024-09-05 06:33:31 UTC"
  },
  {
    "arxiv_id": "2409.00124v2",
    "title": "Leveraging Large Language Models for Wireless Symbol Detection via In-Context Learning",
    "authors": [
      "Momin Abbas",
      "Koushik Kar",
      "Tianyi Chen"
    ],
    "abstract": "Deep neural networks (DNNs) have made significant strides in tackling\nchallenging tasks in wireless systems, especially when an accurate wireless\nmodel is not available. However, when available data is limited, traditional\nDNNs often yield subpar results due to underfitting. At the same time, large\nlanguage models (LLMs) exemplified by GPT-3, have remarkably showcased their\ncapabilities across a broad range of natural language processing tasks. But\nwhether and how LLMs can benefit challenging non-language tasks in wireless\nsystems is unexplored. In this work, we propose to leverage the in-context\nlearning ability (a.k.a. prompting) of LLMs to solve wireless tasks in the low\ndata regime without any training or fine-tuning, unlike DNNs which require\ntraining. We further demonstrate that the performance of LLMs varies\nsignificantly when employed with different prompt templates. To solve this\nissue, we employ the latest LLM calibration methods. Our results reveal that\nusing LLMs via ICL methods generally outperforms traditional DNNs on the symbol\ndemodulation task and yields highly confident predictions when coupled with\ncalibration techniques.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "Accepted at IEEE GLOBECOM 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.00124v2",
    "published_date": "2024-08-28 17:19:20 UTC",
    "updated_date": "2024-09-08 14:49:46 UTC"
  },
  {
    "arxiv_id": "2408.15950v2",
    "title": "Atari-GPT: Benchmarking Multimodal Large Language Models as Low-Level Policies in Atari Games",
    "authors": [
      "Nicholas R. Waytowich",
      "Devin White",
      "MD Sunbeam",
      "Vinicius G. Goecks"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have expanded their\ncapabilities beyond traditional text-based tasks to multimodal domains,\nintegrating visual, auditory, and textual data. While multimodal LLMs have been\nextensively explored for high-level planning in domains like robotics and\ngames, their potential as low-level controllers remains largely untapped. In\nthis paper, we introduce a novel benchmark aimed at testing the emergent\ncapabilities of multimodal LLMs as low-level policies in Atari games. Unlike\ntraditional reinforcement learning (RL) methods that require training for each\nnew environment and reward function specification, these LLMs utilize\npre-existing multimodal knowledge to directly engage with game environments.\nOur study assesses the performances of multiple multimodal LLMs against\ntraditional RL agents, human players, and random agents, focusing on their\nability to understand and interact with complex visual scenes and formulate\nstrategic responses. Our results show that these multimodal LLMs are not yet\ncapable of being zero-shot low-level policies. Furthermore, we see that this\nis, in part, due to their visual and spatial reasoning. Additional results and\nvideos are available on our project webpage:\nhttps://dev1nw.github.io/atari-gpt/.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Currently under review",
    "pdf_url": "http://arxiv.org/pdf/2408.15950v2",
    "published_date": "2024-08-28 17:08:56 UTC",
    "updated_date": "2024-12-02 03:48:43 UTC"
  },
  {
    "arxiv_id": "2408.15924v1",
    "title": "Local Descriptors Weighted Adaptive Threshold Filtering For Few-Shot Learning",
    "authors": [
      "Bingchen Yan"
    ],
    "abstract": "Few-shot image classification is a challenging task in the field of machine\nlearning, involving the identification of new categories using a limited number\nof labeled samples. In recent years, methods based on local descriptors have\nmade significant progress in this area. However, the key to improving\nclassification accuracy lies in effectively filtering background noise and\naccurately selecting critical local descriptors highly relevant to image\ncategory information.\n  To address this challenge, we propose an innovative weighted adaptive\nthreshold filtering (WATF) strategy for local descriptors. This strategy can\ndynamically adjust based on the current task and image context, thereby\nselecting local descriptors most relevant to the image category. This enables\nthe model to better focus on category-related information while effectively\nmitigating interference from irrelevant background regions.\n  To evaluate the effectiveness of our method, we adopted the N-way K-shot\nexperimental framework. Experimental results show that our method not only\nimproves the clustering effect of selected local descriptors but also\nsignificantly enhances the discriminative ability between image categories.\nNotably, our method maintains a simple and lightweight design philosophy\nwithout introducing additional learnable parameters. This feature ensures\nconsistency in filtering capability during both training and testing phases,\nfurther enhancing the reliability and practicality of the method.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15924v1",
    "published_date": "2024-08-28 16:36:23 UTC",
    "updated_date": "2024-08-28 16:36:23 UTC"
  },
  {
    "arxiv_id": "2408.16792v1",
    "title": "Uncertainty-aware segmentation for rainfall prediction post processing",
    "authors": [
      "Simone Monaco",
      "Luca Monaco",
      "Daniele Apiletti"
    ],
    "abstract": "Accurate precipitation forecasts are crucial for applications such as flood\nmanagement, agricultural planning, water resource allocation, and weather\nwarnings. Despite advances in numerical weather prediction (NWP) models, they\nstill exhibit significant biases and uncertainties, especially at high spatial\nand temporal resolutions. To address these limitations, we explore\nuncertainty-aware deep learning models for post-processing daily cumulative\nquantitative precipitation forecasts to obtain forecast uncertainties that lead\nto a better trade-off between accuracy and reliability. Our study compares\ndifferent state-of-the-art models, and we propose a variant of the well-known\nSDE-Net, called SDE U-Net, tailored to segmentation problems like ours. We\nevaluate its performance for both typical and intense precipitation events.\n  Our results show that all deep learning models significantly outperform the\naverage baseline NWP solution, with our implementation of the SDE U-Net showing\nthe best trade-off between accuracy and reliability. Integrating these models,\nwhich account for uncertainty, into operational forecasting systems can improve\ndecision-making and preparedness for weather-related events.",
    "categories": [
      "physics.ao-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.ao-ph",
    "comment": "Paper accepted at the 3rd Workshop on Uncertainty Reasoning and\n  Quantification in Decision Making at ACM SIGKDD'24 (August 26, 2024,\n  Barcelona)",
    "pdf_url": "http://arxiv.org/pdf/2408.16792v1",
    "published_date": "2024-08-28 16:31:40 UTC",
    "updated_date": "2024-08-28 16:31:40 UTC"
  },
  {
    "arxiv_id": "2408.15915v2",
    "title": "Leveraging Open Knowledge for Advancing Task Expertise in Large Language Models",
    "authors": [
      "Yuncheng Yang",
      "Yulei Qin",
      "Tong Wu",
      "Zihan Xu",
      "Gang Li",
      "Pengcheng Guo",
      "Hang Shao",
      "Yuchen Shi",
      "Ke Li",
      "Xing Sun",
      "Jie Yang",
      "Yun Gu"
    ],
    "abstract": "The cultivation of expertise for large language models (LLMs) to solve tasks\nof specific areas often requires special-purpose tuning with calibrated\nbehaviors on the expected stable outputs. To avoid huge cost brought by manual\npreparation of instruction datasets and training resources up to hundreds of\nhours, the exploitation of open knowledge including a wealth of low rank\nadaptation (LoRA) models and instruction datasets serves as a good starting\npoint. However, existing methods on model and data selection focus on the\nperformance of general-purpose capabilities while neglecting the knowledge gap\nexposed in domain-specific deployment. In the present study, we propose to\nbridge such gap by introducing few human-annotated samples (i.e., K-shot) for\nadvancing task expertise of LLMs with open knowledge. Specifically, we develop\nan efficient and scalable pipeline to cost-efficiently produce task experts\nwhere K-shot data intervene in selecting the most promising expert candidates\nand the task-relevant instructions. A mixture-of-expert (MoE) system is built\nto make the best use of individual-yet-complementary knowledge between multiple\nexperts. We unveil the two keys to the success of a MoE system, 1) the abidance\nby K-shot, and 2) the insistence on diversity. For the former, we ensure that\nmodels that truly possess problem-solving abilities on K-shot are selected\nrather than those blind guessers. Besides, during data selection, instructions\nthat share task-relevant contexts with K-shot are prioritized. For the latter,\nwe highlight the diversity of constituting experts and that of the fine-tuning\ninstructions throughout the model and data selection process. Extensive\nexperimental results confirm the superiority of our approach over existing\nmethods on utilization of open knowledge across various tasks. Our codes will\nbe available at https://github.com/Yaphabates/Rocket.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "29 pages, 12 tables, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.15915v2",
    "published_date": "2024-08-28 16:28:07 UTC",
    "updated_date": "2024-09-07 15:12:51 UTC"
  },
  {
    "arxiv_id": "2408.16036v1",
    "title": "Efficient $k$-NN Search in IoT Data: Overlap Optimization in Tree-Based Indexing Structures",
    "authors": [
      "Ala-Eddine Benrazek",
      "Zineddine Kouahla",
      "Brahim Farou",
      "Hamid Seridi",
      "Ibtissem Kemouguette"
    ],
    "abstract": "The proliferation of interconnected devices in the Internet of Things (IoT)\nhas led to an exponential increase in data, commonly known as Big IoT Data.\nEfficient retrieval of this heterogeneous data demands a robust indexing\nmechanism for effective organization. However, a significant challenge remains:\nthe overlap in data space partitions during index construction. This overlap\nincreases node access during search and retrieval, resulting in higher resource\nconsumption, performance bottlenecks, and impedes system scalability. To\naddress this issue, we propose three innovative heuristics designed to quantify\nand strategically reduce data space partition overlap. The volume-based method\n(VBM) offers a detailed assessment by calculating the intersection volume\nbetween partitions, providing deeper insights into spatial relationships. The\ndistance-based method (DBM) enhances efficiency by using the distance between\npartition centers and radii to evaluate overlap, offering a streamlined yet\naccurate approach. Finally, the object-based method (OBM) provides a practical\nsolution by counting objects across multiple partitions, delivering an\nintuitive understanding of data space dynamics. Experimental results\ndemonstrate the effectiveness of these methods in reducing search time,\nunderscoring their potential to improve data space partitioning and enhance\noverall system performance.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.IR",
      "cs.PF",
      "68P05, 68T01, 68P20",
      "E.1; H.2; H.3; I.2"
    ],
    "primary_category": "cs.DB",
    "comment": "28 pages, 21 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2408.16036v1",
    "published_date": "2024-08-28 16:16:55 UTC",
    "updated_date": "2024-08-28 16:16:55 UTC"
  },
  {
    "arxiv_id": "2408.15901v1",
    "title": "Nexus: Specialization meets Adaptability for Efficiently Training Mixture of Experts",
    "authors": [
      "Nikolas Gritsch",
      "Qizhen Zhang",
      "Acyr Locatelli",
      "Sara Hooker",
      "Ahmet Üstün"
    ],
    "abstract": "Efficiency, specialization, and adaptability to new data distributions are\nqualities that are hard to combine in current Large Language Models. The\nMixture of Experts (MoE) architecture has been the focus of significant\nresearch because its inherent conditional computation enables such desirable\nproperties. In this work, we focus on \"upcycling\" dense expert models into an\nMoE, aiming to improve specialization while also adding the ability to adapt to\nnew tasks easily. We introduce Nexus, an enhanced MoE architecture with\nadaptive routing where the model learns to project expert embeddings from\ndomain representations. This approach allows Nexus to flexibly add new experts\nafter the initial upcycling through separately trained dense models, without\nrequiring large-scale MoE training for unseen data domains. Our experiments\nshow that Nexus achieves a relative gain of up to 2.1% over the baseline for\ninitial upcycling, and a 18.8% relative gain for extending the MoE with a new\nexpert by using limited finetuning data. This flexibility of Nexus is crucial\nto enable an open-source ecosystem where every user continuously assembles\ntheir own MoE-mix according to their needs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15901v1",
    "published_date": "2024-08-28 16:12:55 UTC",
    "updated_date": "2024-08-28 16:12:55 UTC"
  },
  {
    "arxiv_id": "2408.15898v3",
    "title": "Airfoil Diffusion: Denoising Diffusion Model For Conditional Airfoil Generation",
    "authors": [
      "Reid Graves",
      "Amir Barati Farimani"
    ],
    "abstract": "The design of aerodynamic shapes, such as airfoils, has traditionally\nrequired significant computational resources and relied on predefined design\nparameters, which limit the potential for novel shape synthesis. In this work,\nwe introduce a data-driven methodology for airfoil generation using a diffusion\nmodel. Trained on a dataset of preexisting airfoils, our model can generate an\narbitrary number of new airfoils from random vectors, which can be conditioned\non specific aerodynamic performance metrics such as lift and drag, or geometric\ncriteria. Our results demonstrate that the diffusion model effectively produces\nairfoil shapes with realistic aerodynamic properties, offering substantial\nimprovements in efficiency, flexibility, and the potential for discovering\ninnovative airfoil designs. This approach significantly expands the design\nspace, facilitating the synthesis of high-performance aerodynamic shapes that\ntranscend the limitations of traditional methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "20 Pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.15898v3",
    "published_date": "2024-08-28 16:12:16 UTC",
    "updated_date": "2024-12-18 16:29:07 UTC"
  },
  {
    "arxiv_id": "2408.15896v1",
    "title": "A New Method for Cross-Lingual-based Semantic Role Labeling",
    "authors": [
      "Mohammad Ebrahimi",
      "Behrouz Minaei Bidgoli",
      "Nasim Khozouei"
    ],
    "abstract": "Semantic role labeling is a crucial task in natural language processing,\nenabling better comprehension of natural language. However, the lack of\nannotated data in multiple languages has posed a challenge for researchers. To\naddress this, a deep learning algorithm based on model transfer has been\nproposed. The algorithm utilizes a dataset consisting of the English portion of\nCoNLL2009 and a corpus of semantic roles in Persian. To optimize the efficiency\nof training, only ten percent of the educational data from each language is\nused. The results of the proposed model demonstrate significant improvements\ncompared to Niksirt et al.'s model. In monolingual mode, the proposed model\nachieved a 2.05 percent improvement on F1-score, while in cross-lingual mode,\nthe improvement was even more substantial, reaching 6.23 percent. Worth noting\nis that the compared model only trained two of the four stages of semantic role\nlabeling and employed golden data for the remaining two stages. This suggests\nthat the actual superiority of the proposed model surpasses the reported\nnumbers by a significant margin. The development of cross-lingual methods for\nsemantic role labeling holds promise, particularly in addressing the scarcity\nof annotated data for various languages. These advancements pave the way for\nfurther research in understanding and processing natural language across\ndifferent linguistic contexts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15896v1",
    "published_date": "2024-08-28 16:06:12 UTC",
    "updated_date": "2024-08-28 16:06:12 UTC"
  },
  {
    "arxiv_id": "2408.15886v2",
    "title": "Enhancing Intrusion Detection in IoT Environments: An Advanced Ensemble Approach Using Kolmogorov-Arnold Networks",
    "authors": [
      "Amar Amouri",
      "Mohamad Mahmoud Al Rahhal",
      "Yakoub Bazi",
      "Ismail Butun",
      "Imad Mahgoub"
    ],
    "abstract": "In recent years, the evolution of machine learning techniques has\nsignificantly impacted the field of intrusion detection, particularly within\nthe context of the Internet of Things (IoT). As IoT networks expand, the need\nfor robust security measures to counteract potential threats has become\nincreasingly critical. This paper introduces a hybrid Intrusion Detection\nSystem (IDS) that synergistically combines Kolmogorov-Arnold Networks (KANs)\nwith the XGBoost algorithm. Our proposed IDS leverages the unique capabilities\nof KANs, which utilize learnable activation functions to model complex\nrelationships within data, alongside the powerful ensemble learning techniques\nof XGBoost, known for its high performance in classification tasks. This hybrid\napproach not only enhances the detection accuracy but also improves the\ninterpretability of the model, making it suitable for dynamic and intricate IoT\nenvironments. Experimental evaluations demonstrate that our hybrid IDS achieves\nan impressive detection accuracy exceeding 99% in distinguishing between benign\nand malicious activities. Additionally, we were able to achieve F1 scores,\nprecision, and recall that exceeded 98%. Furthermore, we conduct a comparative\nanalysis against traditional Multi-Layer Perceptron (MLP) networks, assessing\nperformance metrics such as Precision, Recall, and F1-score. The results\nunderscore the efficacy of integrating KANs with XGBoost, highlighting the\npotential of this innovative approach to significantly strengthen the security\nframework of IoT networks.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "To be presented at the 11th International Symposium on Networks,\n  Computers and Communications (ISNCC'24) will be held in Washington DC- USA,\n  from October 22 to 25, 2024. Accepted (6 pages and 5 figures)",
    "pdf_url": "http://arxiv.org/pdf/2408.15886v2",
    "published_date": "2024-08-28 15:58:49 UTC",
    "updated_date": "2024-08-29 15:54:31 UTC"
  },
  {
    "arxiv_id": "2408.15879v2",
    "title": "Persuasion Games using Large Language Models",
    "authors": [
      "Ganesh Prasath Ramani",
      "Shirish Karande",
      "Santhosh V",
      "Yash Bhatia"
    ],
    "abstract": "Large Language Models (LLMs) have emerged as formidable instruments capable\nof comprehending and producing human-like text. This paper explores the\npotential of LLMs, to shape user perspectives and subsequently influence their\ndecisions on particular tasks. This capability finds applications in diverse\ndomains such as Investment, Credit cards and Insurance, wherein they assist\nusers in selecting appropriate insurance policies, investment plans, Credit\ncards, Retail, as well as in Behavioral Change Support Systems (BCSS).\n  We present a sophisticated multi-agent framework wherein a consortium of\nagents operate in collaborative manner. The primary agent engages directly with\nuser agents through persuasive dialogue, while the auxiliary agents perform\ntasks such as information retrieval, response analysis, development of\npersuasion strategies, and validation of facts. Empirical evidence from our\nexperiments demonstrates that this collaborative methodology significantly\nenhances the persuasive efficacy of the LLM. We continuously analyze the\nresistance of the user agent to persuasive efforts and counteract it by\nemploying a combination of rule-based and LLM-based resistance-persuasion\nmapping techniques.\n  We employ simulated personas and generate conversations in insurance,\nbanking, and retail domains to evaluate the proficiency of large language\nmodels (LLMs) in recognizing, adjusting to, and influencing various personality\ntypes. Concurrently, we examine the resistance mechanisms employed by LLM\nsimulated personas. Persuasion is quantified via measurable surveys before and\nafter interaction, LLM-generated scores on conversation, and user decisions\n(purchase or non-purchase).",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15879v2",
    "published_date": "2024-08-28 15:50:41 UTC",
    "updated_date": "2024-09-02 02:30:51 UTC"
  },
  {
    "arxiv_id": "2408.15874v3",
    "title": "Robust Statistical Scaling of Outlier Scores: Improving the Quality of Outlier Probabilities for Outliers (Extended Version)",
    "authors": [
      "Philipp Röchner",
      "Henrique O. Marques",
      "Ricardo J. G. B. Campello",
      "Arthur Zimek",
      "Franz Rothlauf"
    ],
    "abstract": "Outlier detection algorithms typically assign an outlier score to each\nobservation in a dataset, indicating the degree to which an observation is an\noutlier. However, these scores are often not comparable across algorithms and\ncan be difficult for humans to interpret. Statistical scaling addresses this\nproblem by transforming outlier scores into outlier probabilities without using\nground-truth labels, thereby improving interpretability and comparability\nacross algorithms. However, the quality of this transformation can be different\nfor outliers and inliers. Missing outliers in scenarios where they are of\nparticular interest - such as healthcare, finance, or engineering - can be\ncostly or dangerous. Thus, ensuring good probabilities for outliers is\nessential. This paper argues that statistical scaling, as commonly used in the\nliterature, does not produce equally good probabilities for outliers as for\ninliers. Therefore, we propose robust statistical scaling, which uses robust\nestimators to improve the probabilities for outliers. We evaluate several\nvariants of our method against other outlier score transformations for\nreal-world datasets and outlier detection algorithms, where it can improve the\nprobabilities for outliers.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 4 figures, extended version of an original article\n  published in Similarity Search and Applications. SISAP 2024. Lecture Notes in\n  Computer Science, vol 15268. Springer, by Springer Nature",
    "pdf_url": "http://arxiv.org/pdf/2408.15874v3",
    "published_date": "2024-08-28 15:44:34 UTC",
    "updated_date": "2024-10-30 15:51:52 UTC"
  },
  {
    "arxiv_id": "2408.15868v1",
    "title": "GenDDS: Generating Diverse Driving Video Scenarios with Prompt-to-Video Generative Model",
    "authors": [
      "Yongjie Fu",
      "Yunlong Li",
      "Xuan Di"
    ],
    "abstract": "Autonomous driving training requires a diverse range of datasets encompassing\nvarious traffic conditions, weather scenarios, and road types. Traditional data\naugmentation methods often struggle to generate datasets that represent rare\noccurrences. To address this challenge, we propose GenDDS, a novel approach for\ngenerating driving scenarios generation by leveraging the capabilities of\nStable Diffusion XL (SDXL), an advanced latent diffusion model. Our methodology\ninvolves the use of descriptive prompts to guide the synthesis process, aimed\nat producing realistic and diverse driving scenarios. With the power of the\nlatest computer vision techniques, such as ControlNet and Hotshot-XL, we have\nbuilt a complete pipeline for video generation together with SDXL. We employ\nthe KITTI dataset, which includes real-world driving videos, to train the\nmodel. Through a series of experiments, we demonstrate that our model can\ngenerate high-quality driving videos that closely replicate the complexity and\nvariability of real-world driving scenarios. This research contributes to the\ndevelopment of sophisticated training data for autonomous driving systems and\nopens new avenues for creating virtual environments for simulation and\nvalidation purposes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15868v1",
    "published_date": "2024-08-28 15:37:44 UTC",
    "updated_date": "2024-08-28 15:37:44 UTC"
  },
  {
    "arxiv_id": "2408.15866v1",
    "title": "Retrieval-Augmented Instruction Tuning for Automated Process Engineering Calculations : A Tool-Chaining Problem-Solving Framework with Attributable Reflection",
    "authors": [
      "Sagar Srinivas Sakhinana",
      "Geethan Sannidhi",
      "Venkataramana Runkana"
    ],
    "abstract": "The current technology landscape lacks a foundational AI model for solving\nprocess engineering calculations. In this work, we introduce a novel autonomous\nagent framework leveraging Retrieval-Augmented Instruction-Tuning (RAIT) to\nenhance open, customizable small code language models (SLMs) for these\ncalculations. By combining instruction tuned code SLMs with Retrieval-Augmented\nCode Generation (RACG) using external tools, the agent generates, debugs, and\noptimizes code from natural language specifications. Our approach addresses the\nlimitations of the current lack of a foundational AI model for specialized\nprocess engineering tasks and offers benefits of explainability, knowledge\nediting, and cost-effectiveness. Additionally, we curate custom datasets of\nchemical and process engineering problems and solutions to overcome data\nscarcity. Experimental results show that our framework matches the performance\nof large-scale proprietary models on benchmark datasets, proving its\neffectiveness and usability.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted for publication at ML4CCE workshop at ECML PKDD 2024. Please\n  find the link: https://ml4cce-ecml.com/#agenda",
    "pdf_url": "http://arxiv.org/pdf/2408.15866v1",
    "published_date": "2024-08-28 15:33:47 UTC",
    "updated_date": "2024-08-28 15:33:47 UTC"
  },
  {
    "arxiv_id": "2408.15865v1",
    "title": "microYOLO: Towards Single-Shot Object Detection on Microcontrollers",
    "authors": [
      "Mark Deutel",
      "Christopher Mutschler",
      "Jürgen Teich"
    ],
    "abstract": "This work-in-progress paper presents results on the feasibility of\nsingle-shot object detection on microcontrollers using YOLO. Single-shot object\ndetectors like YOLO are widely used, however due to their complexity mainly on\nlarger GPU-based platforms. We present microYOLO, which can be used on Cortex-M\nbased microcontrollers, such as the OpenMV H7 R2, achieving about 3.5 FPS when\nclassifying 128x128 RGB images while using less than 800 KB Flash and less than\n350 KB RAM. Furthermore, we share experimental results for three different\nobject detection tasks, analyzing the accuracy of microYOLO on them.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Published at the ECML PKDD Conference 2023, at the 4th Workshop on\n  IoT, Edge, and Mobile for Embedded Machine Learning",
    "pdf_url": "http://arxiv.org/pdf/2408.15865v1",
    "published_date": "2024-08-28 15:29:27 UTC",
    "updated_date": "2024-08-28 15:29:27 UTC"
  },
  {
    "arxiv_id": "2409.09039v1",
    "title": "AutoGeo: Automating Geometric Image Dataset Creation for Enhanced Geometry Understanding",
    "authors": [
      "Zihan Huang",
      "Tao Wu",
      "Wang Lin",
      "Shengyu Zhang",
      "Jingyuan Chen",
      "Fei Wu"
    ],
    "abstract": "With the rapid advancement of large language models, there has been a growing\ninterest in their capabilities in mathematical reasoning. However, existing\nresearch has primarily focused on text-based algebra problems, neglecting the\nstudy of geometry due to the lack of high-quality geometric datasets. To\naddress this gap, this paper introduces AutoGeo, a novel approach for\nautomatically generating mathematical geometric images to fulfill the demand\nfor large-scale and diverse geometric datasets. AutoGeo facilitates the\ncreation of AutoGeo-100k, an extensive repository comprising 100k high-quality\ngeometry image-text pairs. By leveraging precisely defined geometric clauses,\nAutoGeo-100k contains a wide variety of geometric shapes, including lines,\npolygons, circles, and complex spatial relationships, etc. Furthermore, this\npaper demonstrates the efficacy of AutoGeo-100k in enhancing the performance of\nmultimodal large language models through fine-tuning. Experimental results\nindicate significant improvements in the model's ability in handling geometric\nimages, as evidenced by enhanced accuracy in tasks such as geometric captioning\nand mathematical reasoning. This research not only fills a critical gap in the\navailability of geometric datasets but also paves the way for the advancement\nof sophisticated AI-driven tools in education and research. Project page:\nhttps://autogeo-official.github.io/.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09039v1",
    "published_date": "2024-08-28 14:49:26 UTC",
    "updated_date": "2024-08-28 14:49:26 UTC"
  },
  {
    "arxiv_id": "2408.15836v1",
    "title": "Knowledge Navigator: LLM-guided Browsing Framework for Exploratory Search in Scientific Literature",
    "authors": [
      "Uri Katz",
      "Mosh Levy",
      "Yoav Goldberg"
    ],
    "abstract": "The exponential growth of scientific literature necessitates advanced tools\nfor effective knowledge exploration. We present Knowledge Navigator, a system\ndesigned to enhance exploratory search abilities by organizing and structuring\nthe retrieved documents from broad topical queries into a navigable, two-level\nhierarchy of named and descriptive scientific topics and subtopics. This\nstructured organization provides an overall view of the research themes in a\ndomain, while also enabling iterative search and deeper knowledge discovery\nwithin specific subtopics by allowing users to refine their focus and retrieve\nadditional relevant documents. Knowledge Navigator combines LLM capabilities\nwith cluster-based methods to enable an effective browsing method. We\ndemonstrate our approach's effectiveness through automatic and manual\nevaluations on two novel benchmarks, CLUSTREC-COVID and SCITOC. Our code,\nprompts, and benchmarks are made publicly available.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15836v1",
    "published_date": "2024-08-28 14:48:37 UTC",
    "updated_date": "2024-08-28 14:48:37 UTC"
  },
  {
    "arxiv_id": "2408.15809v1",
    "title": "Object Detection for Vehicle Dashcams using Transformers",
    "authors": [
      "Osama Mustafa",
      "Khizer Ali",
      "Anam Bibi",
      "Imran Siddiqi",
      "Momina Moetesum"
    ],
    "abstract": "The use of intelligent automation is growing significantly in the automotive\nindustry, as it assists drivers and fleet management companies, thus increasing\ntheir productivity. Dash cams are now been used for this purpose which enables\nthe instant identification and understanding of multiple objects and\noccurrences in the surroundings. In this paper, we propose a novel approach for\nobject detection in dashcams using transformers. Our system is based on the\nstate-of-the-art DEtection TRansformer (DETR), which has demonstrated strong\nperformance in a variety of conditions, including different weather and\nillumination scenarios. The use of transformers allows for the consideration of\ncontextual information in decisionmaking, improving the accuracy of object\ndetection. To validate our approach, we have trained our DETR model on a\ndataset that represents real-world conditions. Our results show that the use of\nintelligent automation through transformers can significantly enhance the\ncapabilities of dashcam systems. The model achieves an mAP of 0.95 on\ndetection.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "7 Pages, and 6 Figures",
    "pdf_url": "http://arxiv.org/pdf/2408.15809v1",
    "published_date": "2024-08-28 14:08:24 UTC",
    "updated_date": "2024-08-28 14:08:24 UTC"
  },
  {
    "arxiv_id": "2408.15803v1",
    "title": "ModalityMirror: Improving Audio Classification in Modality Heterogeneity Federated Learning with Multimodal Distillation",
    "authors": [
      "Tiantian Feng",
      "Tuo Zhang",
      "Salman Avestimehr",
      "Shrikanth S. Narayanan"
    ],
    "abstract": "Multimodal Federated Learning frequently encounters challenges of client\nmodality heterogeneity, leading to undesired performances for secondary\nmodality in multimodal learning. It is particularly prevalent in audiovisual\nlearning, with audio is often assumed to be the weaker modality in recognition\ntasks. To address this challenge, we introduce ModalityMirror to improve audio\nmodel performance by leveraging knowledge distillation from an audiovisual\nfederated learning model. ModalityMirror involves two phases: a modality-wise\nFL stage to aggregate uni-modal encoders; and a federated knowledge\ndistillation stage on multi-modality clients to train an unimodal student\nmodel. Our results demonstrate that ModalityMirror significantly improves the\naudio classification compared to the state-of-the-art FL methods such as\nHarmony, particularly in audiovisual FL facing video missing. Our approach\nunlocks the potential for exploiting the diverse modality spectrum inherent in\nmulti-modal FL.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15803v1",
    "published_date": "2024-08-28 13:56:22 UTC",
    "updated_date": "2024-08-28 13:56:22 UTC"
  },
  {
    "arxiv_id": "2408.15800v1",
    "title": "Emulating Brain-like Rapid Learning in Neuromorphic Edge Computing",
    "authors": [
      "Kenneth Stewart",
      "Michael Neumeier",
      "Sumit Bam Shrestha",
      "Garrick Orchard",
      "Emre Neftci"
    ],
    "abstract": "Achieving personalized intelligence at the edge with real-time learning\ncapabilities holds enormous promise in enhancing our daily experiences and\nhelping decision making, planning, and sensing. However, efficient and reliable\nedge learning remains difficult with current technology due to the lack of\npersonalized data, insufficient hardware capabilities, and inherent challenges\nposed by online learning.\n  Over time and across multiple developmental stages, the brain has evolved to\nefficiently incorporate new knowledge by gradually building on previous\nknowledge. In this work, we emulate the multiple stages of learning with\ndigital neuromorphic technology that simulates the neural and synaptic\nprocesses of the brain using two stages of learning. First, a meta-training\nstage trains the hyperparameters of synaptic plasticity for one-shot learning\nusing a differentiable simulation of the neuromorphic hardware. This\nmeta-training process refines a hardware local three-factor synaptic plasticity\nrule and its associated hyperparameters to align with the trained task domain.\nIn a subsequent deployment stage, these optimized hyperparameters enable fast,\ndata-efficient, and accurate learning of new classes. We demonstrate our\napproach using event-driven vision sensor data and the Intel Loihi neuromorphic\nprocessor with its plasticity dynamics, achieving real-time one-shot learning\nof new classes that is vastly improved over transfer learning. Our methodology\ncan be deployed with arbitrary plasticity models and can be applied to\nsituations demanding quick learning and adaptation at the edge, such as\nnavigating unfamiliar environments or learning unexpected categories of data\nthrough user engagement.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "17 page journal article. Submitted to IOP NCE",
    "pdf_url": "http://arxiv.org/pdf/2408.15800v1",
    "published_date": "2024-08-28 13:51:52 UTC",
    "updated_date": "2024-08-28 13:51:52 UTC"
  },
  {
    "arxiv_id": "2408.15796v2",
    "title": "Evaluating Named Entity Recognition Using Few-Shot Prompting with Large Language Models",
    "authors": [
      "Hédi Zeghidi",
      "Ludovic Moncla"
    ],
    "abstract": "This paper evaluates Few-Shot Prompting with Large Language Models for Named\nEntity Recognition (NER). Traditional NER systems rely on extensive labeled\ndatasets, which are costly and time-consuming to obtain. Few-Shot Prompting or\nin-context learning enables models to recognize entities with minimal examples.\nWe assess state-of-the-art models like GPT-4 in NER tasks, comparing their\nfew-shot performance to fully supervised benchmarks. Results show that while\nthere is a performance gap, large models excel in adapting to new entity types\nand domains with very limited data. We also explore the effects of prompt\nengineering, guided output format and context length on performance. This study\nunderscores Few-Shot Learning's potential to reduce the need for large labeled\ndatasets, enhancing NER scalability and accessibility.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Github repo: https://github.com/GEODE-project/ner-llm",
    "pdf_url": "http://arxiv.org/pdf/2408.15796v2",
    "published_date": "2024-08-28 13:42:28 UTC",
    "updated_date": "2024-09-04 06:36:22 UTC"
  },
  {
    "arxiv_id": "2409.00122v1",
    "title": "Brant-X: A Unified Physiological Signal Alignment Framework",
    "authors": [
      "Daoze Zhang",
      "Zhizhang Yuan",
      "Junru Chen",
      "Kerui Chen",
      "Yang Yang"
    ],
    "abstract": "Physiological signals serve as indispensable clues for understanding various\nphysiological states of human bodies. Most existing works have focused on a\nsingle type of physiological signals for a range of application scenarios.\nHowever, as the body is a holistic biological system, the inherent\ninterconnection among various physiological data should not be neglected. In\nparticular, given the brain's role as the control center for vital activities,\nelectroencephalogram (EEG) exhibits significant correlations with other\nphysiological signals. Therefore, the correlation between EEG and other\nphysiological signals holds potential to improve performance in various\nscenarios. Nevertheless, achieving this goal is still constrained by several\nchallenges: the scarcity of simultaneously collected physiological data, the\ndifferences in correlations between various signals, and the correlation\ndifferences between various tasks. To address these issues, we propose a\nunified physiological signal alignment framework, Brant-X, to model the\ncorrelation between EEG and other signals. Our approach (1) employs the EEG\nfoundation model to data-efficiently transfer the rich knowledge in EEG to\nother physiological signals, and (2) introduces the two-level alignment to\nfully align the semantics of EEG and other signals from different semantic\nscales. In the experiments, Brant-X achieves state-of-the-art performance\ncompared with task-agnostic and task-specific baselines on various downstream\ntasks in diverse scenarios, including sleep stage classification, emotion\nrecognition, freezing of gaits detection, and eye movement communication.\nMoreover, the analysis on the arrhythmia detection task and the visualization\nin case study further illustrate the effectiveness of Brant-X in the knowledge\ntransfer from EEG to other physiological signals. The model's homepage is at\nhttps://github.com/zjunet/Brant-X/.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "Accepted by SIGKDD 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.00122v1",
    "published_date": "2024-08-28 13:26:42 UTC",
    "updated_date": "2024-08-28 13:26:42 UTC"
  },
  {
    "arxiv_id": "2408.15778v4",
    "title": "LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language Models",
    "authors": [
      "Jiayi Gui",
      "Yiming Liu",
      "Jiale Cheng",
      "Xiaotao Gu",
      "Xiao Liu",
      "Hongning Wang",
      "Yuxiao Dong",
      "Jie Tang",
      "Minlie Huang"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated notable capabilities across\nvarious tasks, showcasing complex problem-solving abilities. Understanding and\nexecuting complex rules, along with multi-step planning, are fundamental to\nlogical reasoning and critical for practical LLM agents and decision-making\nsystems. However, evaluating LLMs as effective rule-based executors and\nplanners remains underexplored. In this paper, we introduce LogicGame, a novel\nbenchmark designed to evaluate the comprehensive rule understanding, execution,\nand planning capabilities of LLMs. Unlike traditional benchmarks, LogicGame\nprovides diverse games that contain a series of rules with an initial state,\nrequiring models to comprehend and apply predefined regulations to solve\nproblems. We create simulated scenarios in which models execute or plan\noperations to achieve specific outcomes. These game scenarios are specifically\ndesigned to distinguish logical reasoning from mere knowledge by relying\nexclusively on predefined rules. This separation allows for a pure assessment\nof rule-based reasoning capabilities. The evaluation considers not only final\noutcomes but also intermediate steps, providing a comprehensive assessment of\nmodel performance. Moreover, these intermediate steps are deterministic and can\nbe automatically verified. LogicGame defines game scenarios with varying\ndifficulty levels, from simple rule applications to complex reasoning chains,\nin order to offer a precise evaluation of model performance on rule\nunderstanding and multi-step execution. Utilizing LogicGame, we test various\nLLMs and identify notable shortcomings in their rule-based logical reasoning\nabilities.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15778v4",
    "published_date": "2024-08-28 13:16:41 UTC",
    "updated_date": "2024-10-12 11:00:25 UTC"
  },
  {
    "arxiv_id": "2408.15775v2",
    "title": "Easy, Interpretable, Effective: openSMILE for voice deepfake detection",
    "authors": [
      "Octavian Pascu",
      "Dan Oneata",
      "Horia Cucu",
      "Nicolas M. Müller"
    ],
    "abstract": "In this paper, we demonstrate that attacks in the latest ASVspoof5 dataset --\na de facto standard in the field of voice authenticity and deepfake detection\n-- can be identified with surprising accuracy using a small subset of very\nsimplistic features. These are derived from the openSMILE library, and are\nscalar-valued, easy to compute, and human interpretable. For example, attack\nA10`s unvoiced segments have a mean length of 0.09 +- 0.02, while bona fide\ninstances have a mean length of 0.18 +- 0.07. Using this feature alone, a\nthreshold classifier achieves an Equal Error Rate (EER) of 10.3% for attack\nA10. Similarly, across all attacks, we achieve up to 0.8% EER, with an overall\nEER of 15.7 +- 6.0%. We explore the generalization capabilities of these\nfeatures and find that some of them transfer effectively between attacks,\nprimarily when the attacks originate from similar Text-to-Speech (TTS)\narchitectures. This finding may indicate that voice anti-spoofing is, in part,\na problem of identifying and remembering signatures or fingerprints of\nindividual TTS systems. This allows to better understand anti-spoofing models\nand their challenges in real-world application.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15775v2",
    "published_date": "2024-08-28 13:14:18 UTC",
    "updated_date": "2024-08-29 11:58:35 UTC"
  },
  {
    "arxiv_id": "2408.15769v1",
    "title": "A Survey on Evaluation of Multimodal Large Language Models",
    "authors": [
      "Jiaxing Huang",
      "Jingyi Zhang"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) mimic human perception and reasoning\nsystem by integrating powerful Large Language Models (LLMs) with various\nmodality encoders (e.g., vision, audio), positioning LLMs as the \"brain\" and\nvarious modality encoders as sensory organs. This framework endows MLLMs with\nhuman-like capabilities, and suggests a potential pathway towards achieving\nartificial general intelligence (AGI). With the emergence of all-round MLLMs\nlike GPT-4V and Gemini, a multitude of evaluation methods have been developed\nto assess their capabilities across different dimensions. This paper presents a\nsystematic and comprehensive review of MLLM evaluation methods, covering the\nfollowing key aspects: (1) the background of MLLMs and their evaluation; (2)\n\"what to evaluate\" that reviews and categorizes existing MLLM evaluation tasks\nbased on the capabilities assessed, including general multimodal recognition,\nperception, reasoning and trustworthiness, and domain-specific applications\nsuch as socioeconomic, natural sciences and engineering, medical usage, AI\nagent, remote sensing, video and audio processing, 3D point cloud analysis, and\nothers; (3) \"where to evaluate\" that summarizes MLLM evaluation benchmarks into\ngeneral and specific benchmarks; (4) \"how to evaluate\" that reviews and\nillustrates MLLM evaluation steps and metrics; Our overarching goal is to\nprovide valuable insights for researchers in the field of MLLM evaluation,\nthereby facilitating the development of more capable and reliable MLLMs. We\nemphasize that evaluation should be regarded as a critical discipline,\nessential for advancing the field of MLLMs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15769v1",
    "published_date": "2024-08-28 13:05:55 UTC",
    "updated_date": "2024-08-28 13:05:55 UTC"
  },
  {
    "arxiv_id": "2408.15751v2",
    "title": "Reinforcement Learning for Adaptive Traffic Signal Control: Turn-Based and Time-Based Approaches to Reduce Congestion",
    "authors": [
      "Muhammad Tahir Rafique",
      "Ahmed Mustafa",
      "Hasan Sajid"
    ],
    "abstract": "The growing demand for road use in urban areas has led to significant traffic\ncongestion, posing challenges that are costly to mitigate through\ninfrastructure expansion alone. As an alternative, optimizing existing traffic\nmanagement systems, particularly through adaptive traffic signal control,\noffers a promising solution. This paper explores the use of Reinforcement\nLearning (RL) to enhance traffic signal operations at intersections, aiming to\nreduce congestion without extensive sensor networks. We introduce two RL-based\nalgorithms: a turn-based agent, which dynamically prioritizes traffic signals\nbased on real-time queue lengths, and a time-based agent, which adjusts signal\nphase durations according to traffic conditions while following a fixed phase\ncycle. By representing the state as a scalar queue length, our approach\nsimplifies the learning process and lowers deployment costs. The algorithms\nwere tested in four distinct traffic scenarios using seven evaluation metrics\nto comprehensively assess performance. Simulation results demonstrate that both\nRL algorithms significantly outperform conventional traffic signal control\nsystems, highlighting their potential to improve urban traffic flow\nefficiently.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 8 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2408.15751v2",
    "published_date": "2024-08-28 12:35:56 UTC",
    "updated_date": "2024-09-01 17:32:47 UTC"
  },
  {
    "arxiv_id": "2409.00121v1",
    "title": "BELT-2: Bootstrapping EEG-to-Language representation alignment for multi-task brain decoding",
    "authors": [
      "Jinzhao Zhou",
      "Yiqun Duan",
      "Fred Chang",
      "Thomas Do",
      "Yu-Kai Wang",
      "Chin-Teng Lin"
    ],
    "abstract": "The remarkable success of large language models (LLMs) across various\nmulti-modality applications is well established. However, integrating large\nlanguage models with humans, or brain dynamics, remains relatively unexplored.\nIn this paper, we introduce BELT-2, a pioneering multi-task model designed to\nenhance both encoding and decoding performance from EEG signals. To bolster the\nquality of the EEG encoder, BELT-2 is the first work to innovatively 1) adopt\nbyte-pair encoding (BPE)-level EEG-language alignment and 2) integrate\nmulti-task training and decoding in the EEG domain. Inspired by the idea of\n\\textbf{\\textit{Bridging the Brain with GPT}}, we further connect the\nmulti-task EEG encoder with LLMs by utilizing prefix-tuning on intermediary\noutput from the EEG encoder. These innovative efforts make BELT-2 a pioneering\nbreakthrough, making it the first work in the field capable of decoding\ncoherent and readable sentences from non-invasive brain signals. Our\nexperiments highlight significant advancements over prior techniques in both\nquantitative and qualitative measures, achieving a decoding performance with a\nBLEU-1 score of 52.2\\% on the ZuCo dataset. Furthermore, BELT-2 shows a\nremarkable improvement ranging from 31\\% to 162\\% on other translation\nbenchmarks. Codes can be accessed via the provided anonymous\nlink~\\footnote{https://anonymous.4open.science/r/BELT-2-0048}.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00121v1",
    "published_date": "2024-08-28 12:30:22 UTC",
    "updated_date": "2024-08-28 12:30:22 UTC"
  },
  {
    "arxiv_id": "2408.15722v1",
    "title": "Advanced POD-Based Performance Evaluation of Classifiers Applied to Human Driver Lane Changing Prediction",
    "authors": [
      "Zahra Rastin",
      "Dirk Söffker"
    ],
    "abstract": "Machine learning (ML) classifiers serve as essential tools facilitating\nclassification and prediction across various domains. The performance of these\nalgorithms should be known to ensure their reliable application. In certain\nfields, receiver operating characteristic and precision-recall curves are\nfrequently employed to assess machine learning algorithms without accounting\nfor the impact of process parameters. However, it may be essential to evaluate\nthe performance of these algorithms in relation to such parameters. As a\nperformance evaluation metric capable of considering the effects of process\nparameters, this paper uses a modified probability of detection (POD) approach\nto assess the reliability of ML-based algorithms. As an example, the POD-based\napproach is employed to assess ML models used for predicting the lane changing\nbehavior of a vehicle driver. The time remaining to the predicted (and\ntherefore unknown) lane changing event is considered as process parameter. The\nhit/miss approach to POD is taken here and modified by considering the\nprobability of lane changing derived from ML algorithms at each time step, and\nobtaining the final result of the analysis accordingly. This improves the\nreliability of results compared to the standard hit/miss approach, which\nconsiders the outcome of the classifiers as either 0 or 1, while also\nsimplifying evaluation compared to the \\^a versus a approach. Performance\nevaluation results of the proposed approach are compared with those obtained\nwith the standard hit/miss approach and a pre-developed \\^a versus a approach\nto validate the effectiveness of the proposed method. The comparison shows that\nthis method provides an averaging conservative behavior with the advantage of\nenhancing the reliability of the hit/miss approach to POD while retaining its\nsimplicity.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "Manuscript: 8 pages, 6 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2408.15722v1",
    "published_date": "2024-08-28 11:39:24 UTC",
    "updated_date": "2024-08-28 11:39:24 UTC"
  },
  {
    "arxiv_id": "2409.07476v1",
    "title": "Responsible AI for Test Equity and Quality: The Duolingo English Test as a Case Study",
    "authors": [
      "Jill Burstein",
      "Geoffrey T. LaFlair",
      "Kevin Yancey",
      "Alina A. von Davier",
      "Ravit Dotan"
    ],
    "abstract": "Artificial intelligence (AI) creates opportunities for assessments, such as\nefficiencies for item generation and scoring of spoken and written responses.\nAt the same time, it poses risks (such as bias in AI-generated item content).\nResponsible AI (RAI) practices aim to mitigate risks associated with AI. This\nchapter addresses the critical role of RAI practices in achieving test quality\n(appropriateness of test score inferences), and test equity (fairness to all\ntest takers). To illustrate, the chapter presents a case study using the\nDuolingo English Test (DET), an AI-powered, high-stakes English language\nassessment. The chapter discusses the DET RAI standards, their development and\ntheir relationship to domain-agnostic RAI principles. Further, it provides\nexamples of specific RAI practices, showing how these practices meaningfully\naddress the ethical principles of validity and reliability, fairness, privacy\nand security, and transparency and accountability standards to ensure test\nequity and quality.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.07476v1",
    "published_date": "2024-08-28 11:39:20 UTC",
    "updated_date": "2024-08-28 11:39:20 UTC"
  },
  {
    "arxiv_id": "2409.00120v2",
    "title": "ConCSE: Unified Contrastive Learning and Augmentation for Code-Switched Embeddings",
    "authors": [
      "Jangyeong Jeon",
      "Sangyeon Cho",
      "Minuk Ma",
      "Junyoung Kim"
    ],
    "abstract": "This paper examines the Code-Switching (CS) phenomenon where two languages\nintertwine within a single utterance. There exists a noticeable need for\nresearch on the CS between English and Korean. We highlight that the current\nEquivalence Constraint (EC) theory for CS in other languages may only partially\ncapture English-Korean CS complexities due to the intrinsic grammatical\ndifferences between the languages. We introduce a novel Koglish dataset\ntailored for English-Korean CS scenarios to mitigate such challenges. First, we\nconstructed the Koglish-GLUE dataset to demonstrate the importance and need for\nCS datasets in various tasks. We found the differential outcomes of various\nfoundation multilingual language models when trained on a monolingual versus a\nCS dataset. Motivated by this, we hypothesized that SimCSE, which has shown\nstrengths in monolingual sentence embedding, would have limitations in CS\nscenarios. We construct a novel Koglish-NLI (Natural Language Inference)\ndataset using a CS augmentation-based approach to verify this. From this\nCS-augmented dataset Koglish-NLI, we propose a unified contrastive learning and\naugmentation method for code-switched embeddings, ConCSE, highlighting the\nsemantics of CS sentences. Experimental results validate the proposed ConCSE\nwith an average performance enhancement of 1.77\\% on the Koglish-STS(Semantic\nTextual Similarity) tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for oral presentation at ICPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.00120v2",
    "published_date": "2024-08-28 11:27:21 UTC",
    "updated_date": "2024-12-20 07:58:22 UTC"
  },
  {
    "arxiv_id": "2408.15702v1",
    "title": "Evaluating Model Robustness Using Adaptive Sparse L0 Regularization",
    "authors": [
      "Weiyou Liu",
      "Zhenyang Li",
      "Weitong Chen"
    ],
    "abstract": "Deep Neural Networks have demonstrated remarkable success in various domains\nbut remain susceptible to adversarial examples, which are slightly altered\ninputs designed to induce misclassification. While adversarial attacks\ntypically optimize under Lp norm constraints, attacks based on the L0 norm,\nprioritising input sparsity, are less studied due to their complex and non\nconvex nature. These sparse adversarial examples challenge existing defenses by\naltering a minimal subset of features, potentially uncovering more subtle DNN\nweaknesses. However, the current L0 norm attack methodologies face a trade off\nbetween accuracy and efficiency either precise but computationally intense or\nexpedient but imprecise. This paper proposes a novel, scalable, and effective\napproach to generate adversarial examples based on the L0 norm, aimed at\nrefining the robustness evaluation of DNNs against such perturbations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "F.2.2, I.2.7"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by the 20th International Conference on Advanced Data Mining\n  and Applications (ADMA 2024)",
    "pdf_url": "http://arxiv.org/pdf/2408.15702v1",
    "published_date": "2024-08-28 11:02:23 UTC",
    "updated_date": "2024-08-28 11:02:23 UTC"
  },
  {
    "arxiv_id": "2408.15695v2",
    "title": "G-Style: Stylized Gaussian Splatting",
    "authors": [
      "Áron Samuel Kovács",
      "Pedro Hermosilla",
      "Renata G. Raidou"
    ],
    "abstract": "We introduce G-Style, a novel algorithm designed to transfer the style of an\nimage onto a 3D scene represented using Gaussian Splatting. Gaussian Splatting\nis a powerful 3D representation for novel view synthesis, as -- compared to\nother approaches based on Neural Radiance Fields -- it provides fast scene\nrenderings and user control over the scene. Recent pre-prints have demonstrated\nthat the style of Gaussian Splatting scenes can be modified using an image\nexemplar. However, since the scene geometry remains fixed during the\nstylization process, current solutions fall short of producing satisfactory\nresults. Our algorithm aims to address these limitations by following a\nthree-step process: In a pre-processing step, we remove undesirable Gaussians\nwith large projection areas or highly elongated shapes. Subsequently, we\ncombine several losses carefully designed to preserve different scales of the\nstyle in the image, while maintaining as much as possible the integrity of the\noriginal scene content. During the stylization process and following the\noriginal design of Gaussian Splatting, we split Gaussians where additional\ndetail is necessary within our scene by tracking the gradient of the stylized\ncolor. Our experiments demonstrate that G-Style generates high-quality\nstylizations within just a few minutes, outperforming existing methods both\nqualitatively and quantitatively.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15695v2",
    "published_date": "2024-08-28 10:43:42 UTC",
    "updated_date": "2024-09-05 09:05:39 UTC"
  },
  {
    "arxiv_id": "2408.16032v1",
    "title": "An Extremely Data-efficient and Generative LLM-based Reinforcement Learning Agent for Recommenders",
    "authors": [
      "Shuang Feng",
      "Grace Feng"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have enabled\nunderstanding webpage contexts, product details, and human instructions.\nUtilizing LLMs as the foundational architecture for either reward models or\npolicies in reinforcement learning has gained popularity -- a notable\nachievement is the success of InstructGPT. RL algorithms have been instrumental\nin maximizing long-term customer satisfaction and avoiding short-term, myopic\ngoals in industrial recommender systems, which often rely on deep learning\nmodels to predict immediate clicks or purchases.\n  In this project, several RL methods are implemented and evaluated using the\nWebShop benchmark environment, data, simulator, and pre-trained model\ncheckpoints. The goal is to train an RL agent to maximize the purchase reward\ngiven a detailed human instruction describing a desired product. The RL agents\nare developed by fine-tuning a pre-trained BERT model with various objectives,\nlearning from preferences without a reward model, and employing contemporary\ntraining techniques such as Proximal Policy Optimization (PPO) as used in\nInstructGPT, and Direct Preference Optimization (DPO). This report also\nevaluates the RL agents trained using generative trajectories. Evaluations were\nconducted using Thompson sampling in the WebShop simulator environment.\n  The simulated online experiments demonstrate that agents trained on generated\ntrajectories exhibited comparable task performance to those trained using human\ntrajectories. This has demonstrated an example of an extremely low-cost\ndata-efficient way of training reinforcement learning agents. Also, with\nlimited training time (<2hours), without utilizing any images, a DPO agent\nachieved a 19% success rate after approximately 3000 steps or 30 minutes of\ntraining on T4 GPUs, compared to a PPO agent, which reached a 15% success rate.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16032v1",
    "published_date": "2024-08-28 10:31:50 UTC",
    "updated_date": "2024-08-28 10:31:50 UTC"
  },
  {
    "arxiv_id": "2408.16031v1",
    "title": "EMP: Enhance Memory in Data Pruning",
    "authors": [
      "Jinying Xiao",
      "Ping Li",
      "Jie Nie",
      "Zhe Tang"
    ],
    "abstract": "Recently, large language and vision models have shown strong performance, but\ndue to high pre-training and fine-tuning costs, research has shifted towards\nfaster training via dataset pruning. Previous methods used sample loss as an\nevaluation criterion, aiming to select the most \"difficult\" samples for\ntraining. However, when the pruning rate increases, the number of times each\nsample is trained becomes more evenly distributed, which causes many critical\nor general samples to not be effectively fitted. We refer to this as\nLow-Frequency Learning (LFL). In other words, LFL prevents the model from\nremembering most samples. In our work, we decompose the scoring function of\nLFL, provide a theoretical explanation for the inefficiency of LFL, and propose\nadding a memory term to the scoring function to enhance the model's memory\ncapability, along with an approximation of this memory term. Similarly, we\nexplore memory in Self-Supervised Learning (SSL), marking the first discussion\non SSL memory. Using contrastive learning, we derive the memory term both\ntheoretically and experimentally. Finally, we propose Enhance Memory Pruning\n(EMP), which addresses the issue of insufficient memory under high pruning\nrates by enhancing the model's memory of data, thereby improving its\nperformance. We evaluated the performance of EMP in tasks such as image\nclassification, natural language understanding, and model pre-training. The\nresults show that EMP can improve model performance under extreme pruning\nrates. For example, in the CIFAR100-ResNet50 pre-training task, with 70\\%\npruning, EMP outperforms current methods by 2.2\\%.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16031v1",
    "published_date": "2024-08-28 10:29:52 UTC",
    "updated_date": "2024-08-28 10:29:52 UTC"
  },
  {
    "arxiv_id": "2408.16030v2",
    "title": "Deep Learning-Based Automatic Multi-Level Airway Collapse Monitoring on Obstructive Sleep Apnea Patients",
    "authors": [
      "Ying-Chieh Hsu",
      "Stanley Yung-Chuan Liu",
      "Chao-Jung Huang",
      "Chi-Wei Wu",
      "Ren-Kai Cheng",
      "Jane Yung-Jen Hsu",
      "Shang-Ran Huang",
      "Yuan-Ren Cheng",
      "Fu-Shun Hsu"
    ],
    "abstract": "This study investigated the use of deep learning to identify multi-level\nupper airway collapses in obstructive sleep apnea (OSA) patients based on\nsnoring sounds. We fi-ne-tuned ResNet-50 and Audio Spectrogram Transformer\n(AST) models using snoring recordings from 37 subjects undergoing drug-induced\nsleep endoscopy (DISE) between 2020 and 2021. Snoring sounds were labeled\naccording to the VOTE (Velum, Orophar-ynx, Tongue Base, Epiglottis)\nclassification, resulting in 259 V, 403 O, 77 T, 13 E, 1016 VO, 46 VT, 140 OT,\n39 OE, 30 VOT, and 3150 non-snoring (N) 0.5-second clips. The models were\ntrained for two multi-label classification tasks: identifying obstructions at\nV, O, T, and E levels, and identifying retropalatal (RP) and retroglossal (RG)\nobstruc-tions. Results showed AST slightly outperformed ResNet-50,\ndemonstrating good abil-ity to identify V (F1-score: 0.71, MCC: 0.61, AUC:\n0.89), O (F1-score: 0.80, MCC: 0.72, AUC: 0.94), and RP obstructions (F1-score:\n0.86, MCC: 0.77, AUC: 0.97). However, both models struggled with T, E, and RG\nclassifications due to limited data. Retrospective analysis of a full-night\nrecording showed the potential to profile airway obstruction dynamics. We\nexpect this information, combined with polysomnography and other clinical\nparameters, can aid clinical triage and treatment planning for OSA patients.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16030v2",
    "published_date": "2024-08-28 09:30:20 UTC",
    "updated_date": "2025-01-09 06:33:24 UTC"
  },
  {
    "arxiv_id": "2409.07473v1",
    "title": "Ethical AI Governance: Methods for Evaluating Trustworthy AI",
    "authors": [
      "Louise McCormack",
      "Malika Bendechache"
    ],
    "abstract": "Trustworthy Artificial Intelligence (TAI) integrates ethics that align with\nhuman values, looking at their influence on AI behaviour and decision-making.\nPrimarily dependent on self-assessment, TAI evaluation aims to ensure ethical\nstandards and safety in AI development and usage. This paper reviews the\ncurrent TAI evaluation methods in the literature and offers a classification,\ncontributing to understanding self-assessment methods in this field.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "68T01 (Primary) 68M14 (Secondary)",
      "I.2.9; K.4.1"
    ],
    "primary_category": "cs.CY",
    "comment": "6 pages, 1 figure, accepted for presentation at AIEB 2024: Workshop\n  on Implementing AI Ethics Through a Behavioural Lens - ECAI, Octoebr 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.07473v1",
    "published_date": "2024-08-28 09:25:50 UTC",
    "updated_date": "2024-08-28 09:25:50 UTC"
  },
  {
    "arxiv_id": "2408.15658v1",
    "title": "An Empirical Study on Self-correcting Large Language Models for Data Science Code Generation",
    "authors": [
      "Thai Tang Quoc",
      "Duc Ha Minh",
      "Tho Quan Thanh",
      "Anh Nguyen-Duc"
    ],
    "abstract": "Large Language Models (LLMs) have recently advanced many applications on\nsoftware engineering tasks, particularly the potential for code generation.\nAmong contemporary challenges, code generated by LLMs often suffers from\ninaccuracies and hallucinations, requiring external inputs to correct. One\nrecent strategy to fix these issues is to refine the code generated from LLMs\nusing the input from the model itself (self-augmented). In this work, we\nproposed a novel method, namely CoT-SelfEvolve. CoT-SelfEvolve iteratively and\nautomatically refines code through a self-correcting process, guided by a chain\nof thought constructed from real-world programming problem feedback. Focusing\non data science code, including Python libraries such as NumPy and Pandas, our\nevaluations on the DS-1000 dataset demonstrate that CoT-SelfEvolve\nsignificantly outperforms existing models in solving complex problems. The\nframework shows substantial improvements in both initial code generation and\nsubsequent iterations, with the model's accuracy increasing significantly with\neach additional iteration. This highlights the effectiveness of using\nchain-of-thought prompting to address complexities revealed by program executor\ntraceback error messages. We also discuss how CoT-SelfEvolve can be integrated\ninto continuous software engineering environments, providing a practical\nsolution for improving LLM-based code generation.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15658v1",
    "published_date": "2024-08-28 09:19:09 UTC",
    "updated_date": "2024-08-28 09:19:09 UTC"
  },
  {
    "arxiv_id": "2408.15650v1",
    "title": "Harnessing the Intrinsic Knowledge of Pretrained Language Models for Challenging Text Classification Settings",
    "authors": [
      "Lingyu Gao"
    ],
    "abstract": "Text classification is crucial for applications such as sentiment analysis\nand toxic text filtering, but it still faces challenges due to the complexity\nand ambiguity of natural language. Recent advancements in deep learning,\nparticularly transformer architectures and large-scale pretraining, have\nachieved inspiring success in NLP fields. Building on these advancements, this\nthesis explores three challenging settings in text classification by leveraging\nthe intrinsic knowledge of pretrained language models (PLMs). Firstly, to\naddress the challenge of selecting misleading yet incorrect distractors for\ncloze questions, we develop models that utilize features based on\ncontextualized word representations from PLMs, achieving performance that\nrivals or surpasses human accuracy. Secondly, to enhance model generalization\nto unseen labels, we create small finetuning datasets with domain-independent\ntask label descriptions, improving model performance and robustness. Lastly, we\ntackle the sensitivity of large language models to in-context learning prompts\nby selecting effective demonstrations, focusing on misclassified examples and\nresolving model ambiguity regarding test example labels.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "PhD thesis",
    "pdf_url": "http://arxiv.org/pdf/2408.15650v1",
    "published_date": "2024-08-28 09:07:30 UTC",
    "updated_date": "2024-08-28 09:07:30 UTC"
  },
  {
    "arxiv_id": "2408.15649v1",
    "title": "Hierarchical Blockmodelling for Knowledge Graphs",
    "authors": [
      "Marcin Pietrasik",
      "Marek Reformat",
      "Anna Wilbik"
    ],
    "abstract": "In this paper, we investigate the use of probabilistic graphical models,\nspecifically stochastic blockmodels, for the purpose of hierarchical entity\nclustering on knowledge graphs. These models, seldom used in the Semantic Web\ncommunity, decompose a graph into a set of probability distributions. The\nparameters of these distributions are then inferred allowing for their\nsubsequent sampling to generate a random graph. In a non-parametric setting,\nthis allows for the induction of hierarchical clusterings without prior\nconstraints on the hierarchy's structure. Specifically, this is achieved by the\nintegration of the Nested Chinese Restaurant Process and the Stick Breaking\nProcess into the generative model. In this regard, we propose a model\nleveraging such integration and derive a collapsed Gibbs sampling scheme for\nits inference. To aid in understanding, we describe the steps in this\nderivation and provide an implementation for the sampler. We evaluate our model\non synthetic and real-world datasets and quantitatively compare against\nbenchmark models. We further evaluate our results qualitatively and find that\nour model is capable of inducing coherent cluster hierarchies in small scale\nsettings. The work presented in this paper provides the first step for the\nfurther application of stochastic blockmodels for knowledge graphs on a larger\nscale. We conclude the paper with potential avenues for future work on more\nscalable inference schemes.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "31 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.15649v1",
    "published_date": "2024-08-28 09:04:15 UTC",
    "updated_date": "2024-08-28 09:04:15 UTC"
  },
  {
    "arxiv_id": "2408.15640v3",
    "title": "GANs Conditioning Methods: A Survey",
    "authors": [
      "Anis Bourou",
      "Valérie Mezger",
      "Auguste Genovesio"
    ],
    "abstract": "In recent years, Generative Adversarial Networks (GANs) have seen significant\nadvancements, leading to their widespread adoption across various fields. The\noriginal GAN architecture enables the generation of images without any specific\ncontrol over the content, making it an unconditional generation process.\nHowever, many practical applications require precise control over the generated\noutput, which has led to the development of conditional GANs (cGANs) that\nincorporate explicit conditioning to guide the generation process. cGANs extend\nthe original framework by incorporating additional information (conditions),\nenabling the generation of samples that adhere to that specific criteria.\nVarious conditioning methods have been proposed, each differing in how they\nintegrate the conditioning information into both the generator and the\ndiscriminator networks. In this work, we review the conditioning methods\nproposed for GANs, exploring the characteristics of each method and\nhighlighting their unique mechanisms and theoretical foundations. Furthermore,\nwe conduct a comparative analysis of these methods, evaluating their\nperformance on various image datasets. Through these analyses, we aim to\nprovide insights into the strengths and limitations of various conditioning\ntechniques, guiding future research and application in generative modeling.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15640v3",
    "published_date": "2024-08-28 08:52:14 UTC",
    "updated_date": "2024-09-03 08:35:15 UTC"
  },
  {
    "arxiv_id": "2409.00119v2",
    "title": "3-in-1: 2D Rotary Adaptation for Efficient Finetuning, Efficient Batching and Composability",
    "authors": [
      "Baohao Liao",
      "Christof Monz"
    ],
    "abstract": "Parameter-efficient finetuning (PEFT) methods effectively adapt large\nlanguage models (LLMs) to diverse downstream tasks, reducing storage and GPU\nmemory demands. Despite these advantages, several applications pose new\nchallenges to PEFT beyond mere parameter efficiency. One notable challenge\ninvolves the efficient deployment of LLMs equipped with multiple task- or\nuser-specific adapters, particularly when different adapters are needed for\ndistinct requests within the same batch. Another challenge is the\ninterpretability of LLMs, which is crucial for understanding how LLMs function.\nPrevious studies introduced various approaches to address different challenges.\nIn this paper, we introduce a novel method, RoAd, which employs a\nstraightforward 2D rotation to adapt LLMs and addresses all the above\nchallenges: (1) RoAd is remarkably parameter-efficient, delivering optimal\nperformance on GLUE, eight commonsense reasoning tasks and four arithmetic\nreasoning tasks with $<0.1\\%$ trainable parameters; (2) RoAd facilitates the\nefficient serving of requests requiring different adapters within a batch, with\nan overhead comparable to element-wise multiplication instead of batch matrix\nmultiplication; (3) RoAd enhances LLM's interpretability through integration\nwithin a framework of distributed interchange intervention, demonstrated via\ncomposition experiments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to NeurIPS 2024. Code: https://github.com/BaohaoLiao/road",
    "pdf_url": "http://arxiv.org/pdf/2409.00119v2",
    "published_date": "2024-08-28 08:45:29 UTC",
    "updated_date": "2024-11-04 09:07:25 UTC"
  },
  {
    "arxiv_id": "2408.15632v1",
    "title": "Structural Optimization of Lightweight Bipedal Robot via SERL",
    "authors": [
      "Yi Cheng",
      "Chenxi Han",
      "Yuheng Min",
      "Linqi Ye",
      "Houde Liu",
      "Hang Liu"
    ],
    "abstract": "Designing a bipedal robot is a complex and challenging task, especially when\ndealing with a multitude of structural parameters. Traditional design methods\noften rely on human intuition and experience. However, such approaches are\ntime-consuming, labor-intensive, lack theoretical guidance and hard to obtain\noptimal design results within vast design spaces, thus failing to full exploit\nthe inherent performance potential of robots. In this context, this paper\nintroduces the SERL (Structure Evolution Reinforcement Learning) algorithm,\nwhich combines reinforcement learning for locomotion tasks with evolution\nalgorithms. The aim is to identify the optimal parameter combinations within a\ngiven multidimensional design space. Through the SERL algorithm, we\nsuccessfully designed a bipedal robot named Wow Orin, where the optimal leg\nlength are obtained through optimization based on body structure and motor\ntorque. We have experimentally validated the effectiveness of the SERL\nalgorithm, which is capable of optimizing the best structure within specified\ndesign space and task conditions. Additionally, to assess the performance gap\nbetween our designed robot and the current state-of-the-art robots, we compared\nWow Orin with mainstream bipedal robots Cassie and Unitree H1. A series of\nexperimental results demonstrate the Outstanding energy efficiency and\nperformance of Wow Orin, further validating the feasibility of applying the\nSERL algorithm to practical design.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15632v1",
    "published_date": "2024-08-28 08:34:05 UTC",
    "updated_date": "2024-08-28 08:34:05 UTC"
  },
  {
    "arxiv_id": "2408.15630v1",
    "title": "CodeSift: An LLM-Based Reference-Less Framework for Automatic Code Validation",
    "authors": [
      "Pooja Aggarwal",
      "Oishik Chatterjee",
      "Ting Dai",
      "Prateeti Mohapatra",
      "Brent Paulovicks",
      "Brad Blancett",
      "Arthur De Magalhaes"
    ],
    "abstract": "The advent of large language models (LLMs) has greatly facilitated code\ngeneration, but ensuring the functional correctness of generated code remains a\nchallenge. Traditional validation methods are often time-consuming,\nerror-prone, and impractical for large volumes of code. We introduce CodeSift,\na novel framework that leverages LLMs as the first-line filter of code\nvalidation without the need for execution, reference code, or human feedback,\nthereby reducing the validation effort. We assess the effectiveness of our\nmethod across three diverse datasets encompassing two programming languages.\nOur results indicate that CodeSift outperforms state-of-the-art code evaluation\nmethods. Internal testing conducted with subject matter experts reveals that\nthe output generated by CodeSift is in line with human preference, reinforcing\nits effectiveness as a dependable automated code validation tool.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15630v1",
    "published_date": "2024-08-28 08:32:21 UTC",
    "updated_date": "2024-08-28 08:32:21 UTC"
  },
  {
    "arxiv_id": "2408.15625v2",
    "title": "CBF-LLM: Safe Control for LLM Alignment",
    "authors": [
      "Yuya Miyaoka",
      "Masaki Inoue"
    ],
    "abstract": "This paper proposes a control-based framework for aligning large language\nmodels (LLMs) by leveraging a control barrier function (CBF) to ensure\nuser-desirable text generation. The presented framework applies the safety\nfilter, designed based on the CBF, to the output generation of the baseline\nLLM, i.e., the sequence of the token, with the aim of intervening in the\ngenerated text. The overall text-generation system is implemented with Llama 3\nand a RoBERTa model, and the source code is available at\nhttps://github.com/Mya-Mya/CBF-LLM. The experiment demonstrates its control\nability and effectiveness in reducing the number of interventions needed for\nuser-specified alignment tasks.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.CL",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15625v2",
    "published_date": "2024-08-28 08:25:22 UTC",
    "updated_date": "2024-10-07 09:49:08 UTC"
  },
  {
    "arxiv_id": "2409.07471v1",
    "title": "AI, Climate, and Transparency: Operationalizing and Improving the AI Act",
    "authors": [
      "Nicolas Alder",
      "Kai Ebert",
      "Ralf Herbrich",
      "Philipp Hacker"
    ],
    "abstract": "This paper critically examines the AI Act's provisions on climate-related\ntransparency, highlighting significant gaps and challenges in its\nimplementation. We identify key shortcomings, including the exclusion of energy\nconsumption during AI inference, the lack of coverage for indirect greenhouse\ngas emissions from AI applications, and the lack of standard reporting\nmethodology. The paper proposes a novel interpretation to bring\ninference-related energy use back within the Act's scope and advocates for\npublic access to climate-related disclosures to foster market accountability\nand public scrutiny. Cumulative server level energy reporting is recommended as\nthe most suitable method. We also suggests broader policy changes, including\nsustainability risk assessments and renewable energy targets, to better address\nAI's environmental impact.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "5 pages, 1 table, preprint",
    "pdf_url": "http://arxiv.org/pdf/2409.07471v1",
    "published_date": "2024-08-28 07:57:39 UTC",
    "updated_date": "2024-08-28 07:57:39 UTC"
  },
  {
    "arxiv_id": "2408.15561v2",
    "title": "CGRA4ML: A Framework to Implement Modern Neural Networks for Scientific Edge Computing",
    "authors": [
      "G Abarajithan",
      "Zhenghua Ma",
      "Zepeng Li",
      "Shrideep Koparkar",
      "Ravidu Munasinghe",
      "Francesco Restuccia",
      "Ryan Kastner"
    ],
    "abstract": "Scientific edge computing increasingly relies on hardware-accelerated neural\nnetworks to implement complex, near-sensor processing at extremely high\nthroughputs and low latencies. Existing frameworks like HLS4ML are effective\nfor smaller models, but struggle with larger, modern neural networks due to\ntheir requirement of spatially implementing the neural network layers and\nstoring all weights in on-chip memory. CGRA4ML is an open-source, modular\nframework designed to bridge the gap between neural network model complexity\nand extreme performance requirements. CGRA4ML extends the capabilities of\nHLS4ML by allowing off-chip data storage and supporting a broader range of\nneural network architectures, including models like ResNet, PointNet, and\ntransformers. Unlike HLS4ML, CGRA4ML generates SystemVerilog RTL, making it\nmore suitable for targeting ASIC and FPGA design flows. We demonstrate the\neffectiveness of our framework by implementing and scaling larger models that\nwere previously unattainable with HLS4ML, showcasing its adaptability and\nefficiency in handling complex computations. CGRA4ML also introduces an\nextensive verification framework, with a generated runtime firmware that\nenables its integration into different SoC platforms. CGRA4ML's minimal and\nmodular infrastructure of Python API, SystemVerilog hardware, Tcl toolflows,\nand C runtime, facilitates easy integration and experimentation, allowing\nscientists to focus on innovation rather than the intricacies of hardware\ndesign and optimization.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15561v2",
    "published_date": "2024-08-28 06:24:13 UTC",
    "updated_date": "2024-08-29 01:26:50 UTC"
  },
  {
    "arxiv_id": "2408.15550v2",
    "title": "Trustworthy and Responsible AI for Human-Centric Autonomous Decision-Making Systems",
    "authors": [
      "Farzaneh Dehghani",
      "Mahsa Dibaji",
      "Fahim Anzum",
      "Lily Dey",
      "Alican Basdemir",
      "Sayeh Bayat",
      "Jean-Christophe Boucher",
      "Steve Drew",
      "Sarah Elaine Eaton",
      "Richard Frayne",
      "Gouri Ginde",
      "Ashley Harris",
      "Yani Ioannou",
      "Catherine Lebel",
      "John Lysack",
      "Leslie Salgado Arzuaga",
      "Emma Stanley",
      "Roberto Souza",
      "Ronnie de Souza Santos",
      "Lana Wells",
      "Tyler Williamson",
      "Matthias Wilms",
      "Zaman Wahid",
      "Mark Ungrin",
      "Marina Gavrilova",
      "Mariana Bento"
    ],
    "abstract": "Artificial Intelligence (AI) has paved the way for revolutionary\ndecision-making processes, which if harnessed appropriately, can contribute to\nadvancements in various sectors, from healthcare to economics. However, its\nblack box nature presents significant ethical challenges related to bias and\ntransparency. AI applications are hugely impacted by biases, presenting\ninconsistent and unreliable findings, leading to significant costs and\nconsequences, highlighting and perpetuating inequalities and unequal access to\nresources. Hence, developing safe, reliable, ethical, and Trustworthy AI\nsystems is essential.\n  Our team of researchers working with Trustworthy and Responsible AI, part of\nthe Transdisciplinary Scholarship Initiative within the University of Calgary,\nconducts research on Trustworthy and Responsible AI, including fairness, bias\nmitigation, reproducibility, generalization, interpretability, and\nauthenticity. In this paper, we review and discuss the intricacies of AI\nbiases, definitions, methods of detection and mitigation, and metrics for\nevaluating bias. We also discuss open challenges with regard to the\ntrustworthiness and widespread application of AI across diverse domains of\nhuman-centric decision making, as well as guidelines to foster Responsible and\nTrustworthy AI models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "44 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.15550v2",
    "published_date": "2024-08-28 06:04:25 UTC",
    "updated_date": "2024-09-02 07:55:45 UTC"
  },
  {
    "arxiv_id": "2409.07469v1",
    "title": "Small Object Detection for Indoor Assistance to the Blind using YOLO NAS Small and Super Gradients",
    "authors": [
      "Rashmi BN",
      "R. Guru",
      "Anusuya M A"
    ],
    "abstract": "Advancements in object detection algorithms have opened new avenues for\nassistive technologies that cater to the needs of visually impaired\nindividuals. This paper presents a novel approach for indoor assistance to the\nblind by addressing the challenge of small object detection. We propose a\ntechnique YOLO NAS Small architecture, a lightweight and efficient object\ndetection model, optimized using the Super Gradients training framework. This\ncombination enables real-time detection of small objects crucial for assisting\nthe blind in navigating indoor environments, such as furniture, appliances, and\nhousehold items. Proposed method emphasizes low latency and high accuracy,\nenabling timely and informative voice-based guidance to enhance the user's\nspatial awareness and interaction with their surroundings. The paper details\nthe implementation, experimental results, and discusses the system's\neffectiveness in providing a practical solution for indoor assistance to the\nvisually impaired.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.07469v1",
    "published_date": "2024-08-28 05:38:20 UTC",
    "updated_date": "2024-08-28 05:38:20 UTC"
  },
  {
    "arxiv_id": "2408.15543v2",
    "title": "An Investigation of Warning Erroneous Chat Translations in Cross-lingual Communication",
    "authors": [
      "Yunmeng Li",
      "Jun Suzuki",
      "Makoto Morishita",
      "Kaori Abe",
      "Kentaro Inui"
    ],
    "abstract": "Machine translation models are still inappropriate for translating chats,\ndespite the popularity of translation software and plug-in applications. The\ncomplexity of dialogues poses significant challenges and can hinder\ncrosslingual communication. Instead of pursuing a flawless translation system,\na more practical approach would be to issue warning messages about potential\nmistranslations to reduce confusion. However, it is still unclear how\nindividuals perceive these warning messages and whether they benefit the crowd.\nThis paper tackles to investigate this question and demonstrates the warning\nmessages' contribution to making chat translation systems effective.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15543v2",
    "published_date": "2024-08-28 05:36:25 UTC",
    "updated_date": "2024-11-05 04:13:55 UTC"
  },
  {
    "arxiv_id": "2408.15542v1",
    "title": "Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input",
    "authors": [
      "Jiajun Liu",
      "Yibing Wang",
      "Hanghang Ma",
      "Xiaoping Wu",
      "Xiaoqi Ma",
      "Xiaoming Wei",
      "Jianbin Jiao",
      "Enhua Wu",
      "Jie Hu"
    ],
    "abstract": "Rapid advancements have been made in extending Large Language Models (LLMs)\nto Large Multi-modal Models (LMMs). However, extending input modality of LLMs\nto video data remains a challenging endeavor, especially for long videos. Due\nto insufficient access to large-scale high-quality video data and the excessive\ncompression of visual features, current methods exhibit limitations in\neffectively processing long videos. In this paper, we introduce Kangaroo, a\npowerful Video LMM aimed at addressing these challenges. Confronted with issue\nof inadequate training data, we develop a data curation system to build a\nlarge-scale dataset with high-quality annotations for vision-language\npre-training and instruction tuning. In addition, we design a curriculum\ntraining pipeline with gradually increasing resolution and number of input\nframes to accommodate long videos. Evaluation results demonstrate that, with 8B\nparameters, Kangaroo achieves state-of-the-art performance across a variety of\nvideo understanding benchmarks while exhibiting competitive results on others.\nParticularly, on benchmarks specialized for long videos, Kangaroo excels some\nlarger models with over 10B parameters and proprietary models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15542v1",
    "published_date": "2024-08-28 05:34:14 UTC",
    "updated_date": "2024-08-28 05:34:14 UTC"
  },
  {
    "arxiv_id": "2408.15538v2",
    "title": "TrafficGamer: Reliable and Flexible Traffic Simulation for Safety-Critical Scenarios with Game-Theoretic Oracles",
    "authors": [
      "Guanren Qiao",
      "Guorui Quan",
      "Jiawei Yu",
      "Shujun Jia",
      "Guiliang Liu"
    ],
    "abstract": "While modern Autonomous Vehicle (AV) systems can develop reliable driving\npolicies under regular traffic conditions, they frequently struggle with\nsafety-critical traffic scenarios. This difficulty primarily arises from the\nrarity of such scenarios in driving datasets and the complexities associated\nwith predictive modeling among multiple vehicles. To support the testing and\nrefinement of AV policies, simulating safety-critical traffic events is an\nessential challenge to be addressed. In this work, we introduce TrafficGamer,\nwhich facilitates game-theoretic traffic simulation by viewing common road\ndriving as a multi-agent game. In evaluating the empirical performance across\nvarious real-world datasets, TrafficGamer ensures both fidelity and\nexploitability of the simulated scenarios, guaranteeing that they not only\nstatically align with real-world traffic distribution but also efficiently\ncapture equilibriums for representing safety-critical scenarios involving\nmultiple agents. Additionally, the results demonstrate that TrafficGamer\nexhibits highly flexible simulation across various contexts. Specifically, we\ndemonstrate that the generated scenarios can dynamically adapt to equilibriums\nof varying tightness by configuring risk-sensitive constraints during\noptimization. To the best of our knowledge, TrafficGamer is the first simulator\ncapable of generating diverse traffic scenarios involving multiple agents. We\nhave provided a demo webpage for the project at\nhttps://qiaoguanren.github.io/trafficgamer-demo/.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15538v2",
    "published_date": "2024-08-28 05:11:16 UTC",
    "updated_date": "2024-10-21 11:32:57 UTC"
  },
  {
    "arxiv_id": "2408.15535v1",
    "title": "Improving Thompson Sampling via Information Relaxation for Budgeted Multi-armed Bandits",
    "authors": [
      "Woojin Jeong",
      "Seungki Min"
    ],
    "abstract": "We consider a Bayesian budgeted multi-armed bandit problem, in which each arm\nconsumes a different amount of resources when selected and there is a budget\nconstraint on the total amount of resources that can be used. Budgeted Thompson\nSampling (BTS) offers a very effective heuristic to this problem, but its\narm-selection rule does not take into account the remaining budget information.\nWe adopt \\textit{Information Relaxation Sampling} framework that generalizes\nThompson Sampling for classical $K$-armed bandit problems, and propose a series\nof algorithms that are randomized like BTS but more carefully optimize their\ndecisions with respect to the budget constraint. In a one-to-one correspondence\nwith these algorithms, a series of performance benchmarks that improve the\nconventional benchmark are also suggested. Our theoretical analysis and\nsimulation results show that our algorithms (and our benchmarks) make\nincremental improvements over BTS (respectively, the conventional benchmark)\nacross various settings including a real-world example.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "accepted",
    "pdf_url": "http://arxiv.org/pdf/2408.15535v1",
    "published_date": "2024-08-28 04:56:06 UTC",
    "updated_date": "2024-08-28 04:56:06 UTC"
  },
  {
    "arxiv_id": "2408.15533v2",
    "title": "LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via Layer-wise Relevance Propagation",
    "authors": [
      "Haichuan Hu",
      "Yuhan Sun",
      "Quanjun Zhang"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) has become a primary technique for\nmitigating hallucinations in large language models (LLMs). However, incomplete\nknowledge extraction and insufficient understanding can still mislead LLMs to\nproduce irrelevant or even contradictory responses, which means hallucinations\npersist in RAG. In this paper, we propose LRP4RAG, a method based on the\nLayer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations\nin RAG. Specifically, we first utilize LRP to compute the relevance between the\ninput and output of the RAG generator. We then apply further extraction and\nresampling to the relevance matrix. The processed relevance data are input into\nmultiple classifiers to determine whether the output contains hallucinations.\nTo the best of our knowledge, this is the first time that LRP has been used for\ndetecting RAG hallucinations, and extensive experiments demonstrate that\nLRP4RAG outperforms existing baselines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15533v2",
    "published_date": "2024-08-28 04:44:43 UTC",
    "updated_date": "2024-08-29 08:45:30 UTC"
  },
  {
    "arxiv_id": "2409.00115v2",
    "title": "Self-Adaptive Quantum Kernel Principal Components Analysis for Compact Readout of Chemiresistive Sensor Arrays",
    "authors": [
      "Zeheng Wang",
      "Timothy van der Laan",
      "Muhammad Usman"
    ],
    "abstract": "The rapid growth of Internet of Things (IoT) devices necessitates efficient\ndata compression techniques to handle the vast amounts of data generated by\nthese devices. Chemiresistive sensor arrays (CSAs), a simple-to-fabricate but\ncrucial component in IoT systems, generate large volumes of data due to their\nsimultaneous multi-sensor operations. Classical principal component analysis\n(cPCA) methods, a common solution to the data compression challenge, face\nlimitations in preserving critical information during dimensionality reduction.\nIn this study, we present self-adaptive quantum kernel (SAQK) PCA as a superior\nalternative to enhance information retention. Our findings demonstrate that\nSAQK PCA outperforms cPCA in various back-end machine-learning tasks,\nespecially in low-dimensional scenarios where access to quantum bits is\nlimited. These results highlight the potential of noisy intermediate-scale\nquantum (NISQ) computers to revolutionize data processing in real-world IoT\napplications by improving the efficiency and reliability of CSA data\ncompression and readout, despite the current constraints on qubit availability.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "Version 2",
    "pdf_url": "http://arxiv.org/pdf/2409.00115v2",
    "published_date": "2024-08-28 04:07:40 UTC",
    "updated_date": "2024-12-02 10:25:47 UTC"
  },
  {
    "arxiv_id": "2408.15513v1",
    "title": "Continual-learning-based framework for structural damage recognition",
    "authors": [
      "Jiangpeng Shu",
      "Jiawei Zhang",
      "Reachsak Ly",
      "Fangzheng Lin",
      "Yuanfeng Duan"
    ],
    "abstract": "Multi-damage is common in reinforced concrete structures and leads to the\nrequirement of large number of neural networks, parameters and data storage, if\nconvolutional neural network (CNN) is used for damage recognition. In addition,\nconventional CNN experiences catastrophic forgetting and training inefficiency\nas the number of tasks increases during continual learning, leading to large\naccuracy decrease of previous learned tasks. To address these problems, this\nstudy proposes a continuallearning-based damage recognition model (CLDRM) which\nintegrates the learning without forgetting continual learning method into the\nResNet-34 architecture for the recognition of damages in RC structures as well\nas relevant structural components. Three experiments for four recognition tasks\nwere designed to validate the feasibility and effectiveness of the CLDRM\nframework. In this way, it reduces both the prediction time and data storage by\nabout 75% in four tasks of continuous learning. Three experiments for four\nrecognition tasks were designed to validate the feasibility and effectiveness\nof the CLDRM framework. By gradual feature fusion, CLDRM outperformed other\nmethods by managed to achieve high accuracy in the damage recognition and\nclassification. As the number of recognition tasks increased, CLDRM also\nexperienced smaller decrease of the previous learned tasks. Results indicate\nthat the CLDRM framework successfully performs damage recognition and\nclassification with reasonable accuracy and effectiveness.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.15513v1",
    "published_date": "2024-08-28 03:50:04 UTC",
    "updated_date": "2024-08-28 03:50:04 UTC"
  },
  {
    "arxiv_id": "2408.15512v3",
    "title": "Toward Automated Simulation Research Workflow through LLM Prompt Engineering Design",
    "authors": [
      "Zhihan Liu",
      "Yubo Chai",
      "Jianfeng Li"
    ],
    "abstract": "The advent of Large Language Models (LLMs) has created new opportunities for\nthe automation of scientific research spanning both experimental processes and\ncomputational simulations. This study explores the feasibility of constructing\nan autonomous simulation agent (ASA) powered by LLMs through prompt engineering\nand automated program design to automate the entire simulation research process\naccording to a human-provided research plan. This process includes experimental\ndesign, remote upload and simulation execution, data analysis, and report\ncompilation. Using a well-studied simulation problem of polymer chain\nconformations as a test case, we assessed the long-task completion and\nreliability of ASAs powered by different LLMs, including GPT-4o, Claude-3.5,\netc. Our findings revealed that ASA-GPT-4o achieved near-flawless execution on\ndesignated research missions, underscoring the potential of methods like ASA to\nachieve automation in simulation research processes to enhance research\nefficiency. The outlined automation can be iteratively performed for up to 20\ncycles without human intervention, illustrating the potential of ASA for\nlong-task workflow automation. Additionally, we discussed the intrinsic traits\nof ASA in managing extensive tasks, focusing on self-validation mechanisms, and\nthe balance between local attention and global oversight.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "physics.chem-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "The source code and example results of ASA can be found at\n  https://github.com/zokaraa/autonomous_simulation_agent",
    "pdf_url": "http://arxiv.org/pdf/2408.15512v3",
    "published_date": "2024-08-28 03:48:05 UTC",
    "updated_date": "2025-01-15 09:12:02 UTC"
  },
  {
    "arxiv_id": "2408.15511v1",
    "title": "AeroVerse: UAV-Agent Benchmark Suite for Simulating, Pre-training, Finetuning, and Evaluating Aerospace Embodied World Models",
    "authors": [
      "Fanglong Yao",
      "Yuanchang Yue",
      "Youzhi Liu",
      "Xian Sun",
      "Kun Fu"
    ],
    "abstract": "Aerospace embodied intelligence aims to empower unmanned aerial vehicles\n(UAVs) and other aerospace platforms to achieve autonomous perception,\ncognition, and action, as well as egocentric active interaction with humans and\nthe environment. The aerospace embodied world model serves as an effective\nmeans to realize the autonomous intelligence of UAVs and represents a necessary\npathway toward aerospace embodied intelligence. However, existing embodied\nworld models primarily focus on ground-level intelligent agents in indoor\nscenarios, while research on UAV intelligent agents remains unexplored. To\naddress this gap, we construct the first large-scale real-world image-text\npre-training dataset, AerialAgent-Ego10k, featuring urban drones from a\nfirst-person perspective. We also create a virtual image-text-pose alignment\ndataset, CyberAgent Ego500k, to facilitate the pre-training of the aerospace\nembodied world model. For the first time, we clearly define 5 downstream tasks,\ni.e., aerospace embodied scene awareness, spatial reasoning, navigational\nexploration, task planning, and motion decision, and construct corresponding\ninstruction datasets, i.e., SkyAgent-Scene3k, SkyAgent-Reason3k, SkyAgent-Nav3k\nand SkyAgent-Plan3k, and SkyAgent-Act3k, for fine-tuning the aerospace\nembodiment world model. Simultaneously, we develop SkyAgentEval, the downstream\ntask evaluation metrics based on GPT-4, to comprehensively, flexibly, and\nobjectively assess the results, revealing the potential and limitations of\n2D/3D visual language models in UAV-agent tasks. Furthermore, we integrate over\n10 2D/3D visual-language models, 2 pre-training datasets, 5 finetuning\ndatasets, more than 10 evaluation metrics, and a simulator into the benchmark\nsuite, i.e., AeroVerse, which will be released to the community to promote\nexploration and development of aerospace embodied intelligence.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15511v1",
    "published_date": "2024-08-28 03:47:45 UTC",
    "updated_date": "2024-08-28 03:47:45 UTC"
  },
  {
    "arxiv_id": "2408.15510v3",
    "title": "How Reliable are Causal Probing Interventions?",
    "authors": [
      "Marc Canby",
      "Adam Davies",
      "Chirag Rastogi",
      "Julia Hockenmaier"
    ],
    "abstract": "Causal probing aims to analyze foundation models by examining how intervening\non their representation of various latent properties impacts their outputs.\nRecent works have cast doubt on the theoretical basis of several leading causal\nprobing methods, but it has been unclear how to systematically evaluate the\neffectiveness of these methods in practice. To address this, we define two key\ncausal probing desiderata: completeness (how thoroughly the representation of\nthe target property has been transformed) and selectivity (how little\nnon-targeted properties have been impacted). We find that there is an inherent\ntradeoff between the two, which we define as reliability, their harmonic mean.\nWe introduce an empirical analysis framework to measure and evaluate these\nquantities, allowing us to make the first direct comparisons between different\nfamilies of leading causal probing methods (e.g., linear vs. nonlinear, or\nconcept removal vs. counterfactual interventions). We find that: (1) no method\nis reliable across all layers; (2) more reliable methods have a greater impact\non LLM behavior; (3) nonlinear interventions are more reliable in early and\nintermediate layers, and linear interventions are more reliable in later\nlayers; and (4) concept removal methods are far less reliable than\ncounterfactual interventions, suggesting that they may not be an effective\napproach to causal probing.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15510v3",
    "published_date": "2024-08-28 03:45:49 UTC",
    "updated_date": "2025-02-06 17:16:28 UTC"
  },
  {
    "arxiv_id": "2408.16029v2",
    "title": "Meta-Learn Unimodal Signals with Weak Supervision for Multimodal Sentiment Analysis",
    "authors": [
      "Sijie Mai",
      "Yu Zhao",
      "Ying Zeng",
      "Jianhua Yao",
      "Haifeng Hu"
    ],
    "abstract": "Multimodal sentiment analysis aims to effectively integrate information from\nvarious sources to infer sentiment, where in many cases there are no\nannotations for unimodal labels. Therefore, most works rely on multimodal\nlabels for training. However, there exists the noisy label problem for the\nlearning of unimodal signals as multimodal annotations are not always the ideal\nsubstitutes for the unimodal ones, failing to achieve finer optimization for\nindividual modalities. In this paper, we explore the learning of unimodal\nlabels under the weak supervision from the annotated multimodal labels.\nSpecifically, we propose a novel meta uni-label generation (MUG) framework to\naddress the above problem, which leverages the available multimodal labels to\nlearn the corresponding unimodal labels by the meta uni-label correction\nnetwork (MUCN). We first design a contrastive-based projection module to bridge\nthe gap between unimodal and multimodal representations, so as to use\nmultimodal annotations to guide the learning of MUCN. Afterwards, we propose\nunimodal and multimodal denoising tasks to train MUCN with explicit supervision\nvia a bi-level optimization strategy. We then jointly train unimodal and\nmultimodal learning tasks to extract discriminative unimodal features for\nmultimodal inference. Experimental results suggest that MUG outperforms\ncompetitive baselines and can learn accurate unimodal labels.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16029v2",
    "published_date": "2024-08-28 03:43:01 UTC",
    "updated_date": "2024-09-13 02:51:18 UTC"
  },
  {
    "arxiv_id": "2408.15508v2",
    "title": "EmoAttack: Utilizing Emotional Voice Conversion for Speech Backdoor Attacks on Deep Speech Classification Models",
    "authors": [
      "Wenhan Yao",
      "Zedong XingXiarun Chen",
      "Jia Liu",
      "yongqiang He",
      "Weiping Wen"
    ],
    "abstract": "Deep speech classification tasks, mainly including keyword spotting and\nspeaker verification, play a crucial role in speech-based human-computer\ninteraction. Recently, the security of these technologies has been demonstrated\nto be vulnerable to backdoor attacks. Specifically speaking, speech samples are\nattacked by noisy disruption and component modification in present triggers. We\nsuggest that speech backdoor attacks can strategically focus on emotion, a\nhigher-level subjective perceptual attribute inherent in speech. Furthermore,\nwe proposed that emotional voice conversion technology can serve as the speech\nbackdoor attack trigger, and the method is called EmoAttack. Based on this, we\nconducted attack experiments on two speech classification tasks, showcasing\nthat EmoAttack method owns impactful trigger effectiveness and its remarkable\nattack success rate and accuracy variance. Additionally, the ablation\nexperiments found that speech with intensive emotion is more suitable to be\ntargeted for attacks.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Submitted to ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2408.15508v2",
    "published_date": "2024-08-28 03:36:43 UTC",
    "updated_date": "2024-09-06 07:46:30 UTC"
  },
  {
    "arxiv_id": "2408.15507v1",
    "title": "What Machine Learning Tells Us About the Mathematical Structure of Concepts",
    "authors": [
      "Jun Otsuka"
    ],
    "abstract": "This paper examines the connections among various approaches to understanding\nconcepts in philosophy, cognitive science, and machine learning, with a\nparticular focus on their mathematical nature. By categorizing these approaches\ninto Abstractionism, the Similarity Approach, the Functional Approach, and the\nInvariance Approach, the study highlights how each framework provides a\ndistinct mathematical perspective for modeling concepts. The synthesis of these\napproaches bridges philosophical theories and contemporary machine learning\nmodels, providing a comprehensive framework for future research. This work\nemphasizes the importance of interdisciplinary dialogue, aiming to enrich our\nunderstanding of the complex relationship between human cognition and\nartificial intelligence.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "25 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.15507v1",
    "published_date": "2024-08-28 03:30:22 UTC",
    "updated_date": "2024-08-28 03:30:22 UTC"
  },
  {
    "arxiv_id": "2408.15503v5",
    "title": "RoboSense: Large-scale Dataset and Benchmark for Egocentric Robot Perception and Navigation in Crowded and Unstructured Environments",
    "authors": [
      "Haisheng Su",
      "Feixiang Song",
      "Cong Ma",
      "Wei Wu",
      "Junchi Yan"
    ],
    "abstract": "Reliable embodied perception from an egocentric perspective is challenging\nyet essential for autonomous navigation technology of intelligent mobile\nagents. With the growing demand of social robotics, near-field scene\nunderstanding becomes an important research topic in the areas of egocentric\nperceptual tasks related to navigation in both crowded and unstructured\nenvironments. Due to the complexity of environmental conditions and difficulty\nof surrounding obstacles owing to truncation and occlusion, the perception\ncapability under this circumstance is still inferior. To further enhance the\nintelligence of mobile robots, in this paper, we setup an egocentric\nmulti-sensor data collection platform based on 3 main types of sensors (Camera,\nLiDAR and Fisheye), which supports flexible sensor configurations to enable\ndynamic sight of view from ego-perspective, capturing either near or farther\nareas. Meanwhile, a large-scale multimodal dataset is constructed, named\nRoboSense, to facilitate egocentric robot perception. Specifically, RoboSense\ncontains more than 133K synchronized data with 1.4M 3D bounding box and IDs\nannotated in the full $360^{\\circ}$ view, forming 216K trajectories across 7.6K\ntemporal sequences. It has $270\\times$ and $18\\times$ as many annotations of\nsurrounding obstacles within near ranges as the previous datasets collected for\nautonomous driving scenarios such as KITTI and nuScenes. Moreover, we define a\nnovel matching criterion for near-field 3D perception and prediction metrics.\nBased on RoboSense, we formulate 6 popular tasks to facilitate the future\nresearch development, where the detailed analysis as well as benchmarks are\nalso provided accordingly. Data desensitization measures have been conducted\nfor privacy protection.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to CVPR2025",
    "pdf_url": "http://arxiv.org/pdf/2408.15503v5",
    "published_date": "2024-08-28 03:17:40 UTC",
    "updated_date": "2025-03-05 05:14:34 UTC"
  },
  {
    "arxiv_id": "2408.15501v1",
    "title": "MODULI: Unlocking Preference Generalization via Diffusion Models for Offline Multi-Objective Reinforcement Learning",
    "authors": [
      "Yifu Yuan",
      "Zhenrui Zheng",
      "Zibin Dong",
      "Jianye Hao"
    ],
    "abstract": "Multi-objective Reinforcement Learning (MORL) seeks to develop policies that\nsimultaneously optimize multiple conflicting objectives, but it requires\nextensive online interactions. Offline MORL provides a promising solution by\ntraining on pre-collected datasets to generalize to any preference upon\ndeployment. However, real-world offline datasets are often conservatively and\nnarrowly distributed, failing to comprehensively cover preferences, leading to\nthe emergence of out-of-distribution (OOD) preference areas. Existing offline\nMORL algorithms exhibit poor generalization to OOD preferences, resulting in\npolicies that do not align with preferences. Leveraging the excellent\nexpressive and generalization capabilities of diffusion models, we propose\nMODULI (Multi-objective Diffusion Planner with Sliding Guidance), which employs\na preference-conditioned diffusion model as a planner to generate trajectories\nthat align with various preferences and derive action for decision-making. To\nachieve accurate generation, MODULI introduces two return normalization methods\nunder diverse preferences for refining guidance. To further enhance\ngeneralization to OOD preferences, MODULI proposes a novel sliding guidance\nmechanism, which involves training an additional slider adapter to capture the\ndirection of preference changes. Incorporating the slider, it transitions from\nin-distribution (ID) preferences to generating OOD preferences, patching, and\nextending the incomplete Pareto front. Extensive experiments on the D4MORL\nbenchmark demonstrate that our algorithm outperforms state-of-the-art Offline\nMORL baselines, exhibiting excellent generalization to OOD preferences.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.15501v1",
    "published_date": "2024-08-28 03:10:45 UTC",
    "updated_date": "2024-08-28 03:10:45 UTC"
  },
  {
    "arxiv_id": "2408.15498v1",
    "title": "Deep Learning to Predict Late-Onset Breast Cancer Metastasis: the Single Hyperparameter Grid Search (SHGS) Strategy for Meta Tuning Concerning Deep Feed-forward Neural Network",
    "authors": [
      "Yijun Zhou",
      "Om Arora-Jain",
      "Xia Jiang"
    ],
    "abstract": "While machine learning has advanced in medicine, its widespread use in\nclinical applications, especially in predicting breast cancer metastasis, is\nstill limited. We have been dedicated to constructing a DFNN model to predict\nbreast cancer metastasis n years in advance. However, the challenge lies in\nefficiently identifying optimal hyperparameter values through grid search,\ngiven the constraints of time and resources. Issues such as the infinite\npossibilities for continuous hyperparameters like l1 and l2, as well as the\ntime-consuming and costly process, further complicate the task. To address\nthese challenges, we developed Single Hyperparameter Grid Search (SHGS)\nstrategy, serving as a preselection method before grid search. Our experiments\nwith SHGS applied to DFNN models for breast cancer metastasis prediction focus\non analyzing eight target hyperparameters: epochs, batch size, dropout, L1, L2,\nlearning rate, decay, and momentum. We created three figures, each depicting\nthe experiment results obtained from three LSM-I-10-Plus-year datasets. These\nfigures illustrate the relationship between model performance and the target\nhyperparameter values. For each hyperparameter, we analyzed whether changes in\nthis hyperparameter would affect model performance, examined if there were\nspecific patterns, and explored how to choose values for the particular\nhyperparameter. Our experimental findings reveal that the optimal value of a\nhyperparameter is not only dependent on the dataset but is also significantly\ninfluenced by the settings of other hyperparameters. Additionally, our\nexperiments suggested some reduced range of values for a target hyperparameter,\nwhich may be helpful for low-budget grid search. This approach serves as a\nprior experience and foundation for subsequent use of grid search to enhance\nmodel performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15498v1",
    "published_date": "2024-08-28 03:00:43 UTC",
    "updated_date": "2024-08-28 03:00:43 UTC"
  },
  {
    "arxiv_id": "2409.07466v1",
    "title": "An Artificial Neural Network for Image Classification Inspired by Aversive Olfactory Learning Circuits in Caenorhabditis Elegans",
    "authors": [
      "Xuebin Wang",
      "Chunxiuzi Liu",
      "Meng Zhao",
      "Ke Zhang",
      "Zengru Di",
      "He Liu"
    ],
    "abstract": "This study introduces an artificial neural network (ANN) for image\nclassification task, inspired by the aversive olfactory learning circuits of\nthe nematode Caenorhabditis elegans (C. elegans). Despite the remarkable\nperformance of ANNs in a variety of tasks, they face challenges such as\nexcessive parameterization, high training costs and limited generalization\ncapabilities. C. elegans, with its simple nervous system comprising only 302\nneurons, serves as a paradigm in neurobiological research and is capable of\ncomplex behaviors including learning. This research identifies key neural\ncircuits associated with aversive olfactory learning in C. elegans through\nbehavioral experiments and high-throughput gene sequencing, translating them\ninto an image classification ANN architecture. Additionally, two other image\nclassification ANNs with distinct architectures were constructed for\ncomparative performance analysis to highlight the advantages of bio-inspired\ndesign. The results indicate that the ANN inspired by the aversive olfactory\nlearning circuits of C. elegans achieves higher accuracy, better consistency\nand faster convergence rates in image classification task, especially when\ntackling more complex classification challenges. This study not only showcases\nthe potential of bio-inspired design in enhancing ANN capabilities but also\nprovides a novel perspective and methodology for future ANN design.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CV",
      "q-bio.NC"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.07466v1",
    "published_date": "2024-08-28 02:59:13 UTC",
    "updated_date": "2024-08-28 02:59:13 UTC"
  },
  {
    "arxiv_id": "2408.15495v3",
    "title": "Remove Symmetries to Control Model Expressivity and Improve Optimization",
    "authors": [
      "Liu Ziyin",
      "Yizhou Xu",
      "Isaac Chuang"
    ],
    "abstract": "When symmetry is present in the loss function, the model is likely to be\ntrapped in a low-capacity state that is sometimes known as a \"collapse\". Being\ntrapped in these low-capacity states can be a major obstacle to training across\nmany scenarios where deep learning technology is applied. We first prove two\nconcrete mechanisms through which symmetries lead to reduced capacities and\nignored features during training and inference. We then propose a simple and\ntheoretically justified algorithm, syre, to remove almost all symmetry-induced\nlow-capacity states in neural networks. When this type of entrapment is\nespecially a concern, removing symmetries with the proposed method is shown to\ncorrelate well with improved optimization or performance. A remarkable merit of\nthe proposed method is that it is model-agnostic and does not require any\nknowledge of the symmetry.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "preprint",
    "pdf_url": "http://arxiv.org/pdf/2408.15495v3",
    "published_date": "2024-08-28 02:45:41 UTC",
    "updated_date": "2025-02-27 15:30:47 UTC"
  },
  {
    "arxiv_id": "2408.15462v1",
    "title": "CTRQNets & LQNets: Continuous Time Recurrent and Liquid Quantum Neural Networks",
    "authors": [
      "Alejandro Mayorga",
      "Alexander Yuan",
      "Andrew Yuan",
      "Tyler Wooldridge",
      "Xiaodi Wang"
    ],
    "abstract": "Neural networks have continued to gain prevalence in the modern era for their\nability to model complex data through pattern recognition and behavior\nremodeling. However, the static construction of traditional neural networks\ninhibits dynamic intelligence. This makes them inflexible to temporal changes\nin data and unfit to capture complex dependencies. With the advent of quantum\ntechnology, there has been significant progress in creating quantum algorithms.\nIn recent years, researchers have developed quantum neural networks that\nleverage the capabilities of qubits to outperform classical networks. However,\ntheir current formulation exhibits a static construction limiting the system's\ndynamic intelligence. To address these weaknesses, we develop a Liquid Quantum\nNeural Network (LQNet) and a Continuous Time Recurrent Quantum Neural Network\n(CTRQNet). Both models demonstrate a significant improvement in accuracy\ncompared to existing quantum neural networks (QNNs), achieving accuracy\nincreases as high as 40\\% on CIFAR 10 through binary classification. We propose\nLQNets and CTRQNets might shine a light on quantum machine learning's black\nbox.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15462v1",
    "published_date": "2024-08-28 00:56:03 UTC",
    "updated_date": "2024-08-28 00:56:03 UTC"
  }
]
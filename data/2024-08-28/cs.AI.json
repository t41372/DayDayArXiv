{
  "date": "2024-08-28",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-08-28 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 76 篇论文，主要聚焦于 AI 和机器学习领域，尤其是大型语言模型（LLM）的安全性和多模态处理、强化学习应用，以及创新数据集和框架的开发，亮点包括 FRACTURED-SORRY-Bench 对 LLM 多轮攻击的分析、Eagle 的多模态编码混合策略，以及 RoboSense 的无人机感知基准数据集，这些论文展示了 AI 在实际场景中的潜力，同时涉及知名学者如 Jianfeng Gao 和 Bryan Catanzaro 的工作。\n\n### LLM 安全与多模态处理：重点论文\n这些论文探讨了 LLM 的鲁棒性和多模态能力，相关主题紧密且具有话题度。\n- **FRACTURED-SORRY-Bench: Framework for Revealing Attacks in Conversational Turns Undermining Refusal Efficacy and Defenses over SORRY-Bench (Automated Multi-shot Jailbreaks)（中文：揭示对话轮次攻击框架，破坏拒绝效能并防御 SORRY-Bench）**  \n  主要贡献：提出一种框架，通过将有害查询分解为无害子问题，评估 LLM（如 GPT-4）对多轮攻击的脆弱性，发现攻击成功率最高提高 46.22%，强调了增强 LLM 安全防御的必要性。\n- **Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders（中文：探索多模态 LLM 的设计空间，使用编码器混合）**  \n  主要贡献：开发了一种多编码器混合框架，提升 LLM 在视觉感知任务中的性能，如 OCR 和文档分析，F1 分数达 0.97，实现高效的多模态处理。\n- **Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input（中文：支持长上下文视频输入的强大视频语言模型）**  \n  主要发现：构建了一个处理长视频的 LLM，通过逐步训练策略在基准测试中超越 10B 参数模型，展示了在视频理解中的实际应用潜力。\n- **Data Formulator 2: Iterative Creation of Data Visualizations, with AI Transforming Data Along the Way（中文：数据可视化迭代创建，AI 驱动数据转换）**  \n  主要贡献：设计了一个 AI 系统，支持数据可视化的迭代优化，用户研究显示它提升了探索性数据分析效率。\n\n### 强化学习与机器人应用：令人印象深刻的创新\n这些论文强调了强化学习在机器人和交通中的作用，相关工作显示了实际优化潜力。\n- **TrafficGamer: Reliable and Flexible Traffic Simulation for Safety-Critical Scenarios with Game-Theoretic Oracles（中文：可靠灵活的交通模拟，使用博弈理论预言机处理安全关键场景）**  \n  主要发现：利用博弈理论模拟多代理交通场景，成功率较基准方法提高 93%，为自动驾驶安全测试提供新框架。\n- **MODULI: Unlocking Preference Generalization via Diffusion Models for Offline Multi-Objective Reinforcement Learning（中文：通过扩散模型解锁偏好泛化，在离线多目标强化学习中）**  \n  主要贡献：提出扩散模型框架，提升离线强化学习对未知偏好的泛化能力，准确率较现有方法提高 18.8%。\n- **RoboSense: Large-scale Dataset and Benchmark for Egocentric Robot Perception and Navigation in Crowded and Unstructured Environments（中文：大规模自我视角机器人感知和导航数据集及基准）**  \n  主要发现：发布包含 133K 数据和 1.4M 标注的无人机数据集，支持多任务评估，如导航和感知，显著提升机器人环境适应性。\n\n### 其他值得关注的论文\n以下论文涉及图像处理、健康和科学计算等领域，快速概述关键点。\n- **ChartEye: A Deep Learning Framework for Chart Information Extraction（中文：用于图表信息提取的深度学习框架）**  \n  主要贡献：使用视觉 Transformer 和 YOLOv7 实现图表类型和文本检测，F1 分数达 0.97，提升了自动化图表理解。\n- **Identification of Prognostic Biomarkers for Stage III Non-Small Cell Lung Carcinoma in Female Nonsmokers Using Machine Learning（中文：使用机器学习识别女性非吸烟者 III 期非小细胞肺癌预后生物标志物）**  \n  主要发现：XGBoost 模型识别出关键生物标志物如 HIF-1-alpha，AUC 为 0.835，支持早期诊断和个性化治疗。\n- **AeroVerse: UAV-Agent Benchmark Suite for Simulating, Pre-training, Finetuning, and Evaluating Aerospace Embodied World Models（中文：无人机代理基准套件，用于模拟、预训练、微调和评估航空航天具身世界模型）**  \n  主要贡献：构建了无人机数据集和基准，提升了视觉语言模型在航空任务中的性能。\n\n剩余论文多为特定领域优化或理论探讨，如量子神经网络（e.g., CTRQNets）、交通信号控制（e.g., Reinforcement Learning for Adaptive Traffic Signal Control），这些工作虽有贡献但影响力较小，仅快速提及：它们分别通过强化学习优化交通效率，或探索量子模型在科学计算中的应用，但细节较琐碎，未见重大突破。\n\n总之，今天的更新突显了 AI 模型的鲁棒性和应用潜力，建议关注 LLM 安全和多模态领域的进展，以推动更可靠的 AI 技术。更多细节可查阅 arXiv。",
  "papers": [
    {
      "arxiv_id": "2408.16173v1",
      "title": "LLM-assisted Labeling Function Generation for Semantic Type Detection",
      "title_zh": "LLM 辅助的标记函数生成用于语义类型检测",
      "authors": [
        "Chenjie Li",
        "Dan Zhang",
        "Jin Wang"
      ],
      "abstract": "Detecting semantic types of columns in data lake tables is an important\napplication. A key bottleneck in semantic type detection is the availability of\nhuman annotation due to the inherent complexity of data lakes. In this paper,\nwe propose using programmatic weak supervision to assist in annotating the\ntraining data for semantic type detection by leveraging labeling functions. One\nchallenge in this process is the difficulty of manually writing labeling\nfunctions due to the large volume and low quality of the data lake table\ndatasets. To address this issue, we explore employing Large Language Models\n(LLMs) for labeling function generation and introduce several prompt\nengineering strategies for this purpose. We conduct experiments on real-world\nweb table datasets. Based on the initial results, we perform extensive analysis\nand provide empirical insights and future directions for researchers in this\nfield.",
      "tldr_zh": "本研究针对数据湖表中列的语义类型检测（Semantic Type Detection）问题，提出一种利用大型语言模型（LLMs）辅助生成标注函数（Labeling Functions）的程序化弱监督方法，以缓解人工标注的复杂性和瓶颈。论文引入了几种提示工程策略（Prompt Engineering Strategies），帮助LLMs自动生成高质量的标注函数，从而处理大规模、低质量数据集的挑战。在真实网络表数据集上的实验显示，该方法有效提升了标注效率，并通过广泛分析提供了经验见解和未来研究方向。",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "VLDB'24-DATAI",
      "pdf_url": "http://arxiv.org/pdf/2408.16173v1",
      "published_date": "2024-08-28 23:39:50 UTC",
      "updated_date": "2024-08-28 23:39:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:22:29.613028"
    },
    {
      "arxiv_id": "2408.16169v1",
      "title": "Simulating realistic short tandem repeat capillary electrophoretic signal using a generative adversarial network",
      "title_zh": "使用生成对抗网络模拟真实的短串联重复毛细管电泳信号",
      "authors": [
        "Duncan Taylor",
        "Melissa Humphries"
      ],
      "abstract": "DNA profiles are made up from multiple series of electrophoretic signal\nmeasuring fluorescence over time. Typically, human DNA analysts 'read' DNA\nprofiles using their experience to distinguish instrument noise, artefactual\nsignal, and signal corresponding to DNA fragments of interest. Recent work has\ndeveloped an artificial neural network, ANN, to carry out the task of\nclassifying fluorescence types into categories in DNA profile electrophoretic\nsignal. But the creation of the necessarily large amount of labelled training\ndata for the ANN is time consuming and expensive, and a limiting factor in the\nability to robustly train the ANN. If realistic, prelabelled, training data\ncould be simulated then this would remove the barrier to training an ANN with\nhigh efficacy. Here we develop a generative adversarial network, GAN, modified\nfrom the pix2pix GAN to achieve this task. With 1078 DNA profiles we train the\nGAN and achieve the ability to simulate DNA profile information, and then use\nthe generator from the GAN as a 'realism filter' that applies the noise and\nartefact elements exhibited in typical electrophoretic signal.",
      "tldr_zh": "这篇论文提出了一种使用生成对抗网络(GAN)来模拟短串联重复(short tandem repeat)毛细管电泳信号的方法，以解决训练人工神经网络(ANN)时所需的大量标记数据的瓶颈问题。研究团队修改了pix2pix GAN，并利用1078个DNA轮廓作为训练数据，生成 realistic、预标记的模拟信号，其中GAN的生成器充当“真实性过滤器”，添加典型的噪声和伪信号元素。实验结果表明，这一方法能有效模拟DNA轮廓信息，提高ANN的训练效率和鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "29 pages, 9 Figures",
      "pdf_url": "http://arxiv.org/pdf/2408.16169v1",
      "published_date": "2024-08-28 23:20:17 UTC",
      "updated_date": "2024-08-28 23:20:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:22:41.675138"
    },
    {
      "arxiv_id": "2408.16163v2",
      "title": "FRACTURED-SORRY-Bench: Framework for Revealing Attacks in Conversational Turns Undermining Refusal Efficacy and Defenses over SORRY-Bench (Automated Multi-shot Jailbreaks)",
      "title_zh": "翻译失败",
      "authors": [
        "Aman Priyanshu",
        "Supriti Vijay"
      ],
      "abstract": "This paper introduces FRACTURED-SORRY-Bench, a framework for evaluating the\nsafety of Large Language Models (LLMs) against multi-turn conversational\nattacks. Building upon the SORRY-Bench dataset, we propose a simple yet\neffective method for generating adversarial prompts by breaking down harmful\nqueries into seemingly innocuous sub-questions. Our approach achieves a maximum\nincrease of +46.22\\% in Attack Success Rates (ASRs) across GPT-4, GPT-4o,\nGPT-4o-mini, and GPT-3.5-Turbo models compared to baseline methods. We\ndemonstrate that this technique poses a challenge to current LLM safety\nmeasures and highlights the need for more robust defenses against subtle,\nmulti-turn attacks.",
      "tldr_zh": "这篇论文引入了 FRACTURED-SORRY-Bench 框架，用于评估大型语言模型 (LLMs) 抵御多轮对话攻击的安全性。框架基于 SORRY-Bench 数据集，通过将有害查询分解成看似无害的子问题来生成对抗性提示，从而实现自动化多轮（Multi-shot）越狱攻击。实验结果显示，该方法在 GPT-4、GPT-4o、GPT-4o-mini 和 GPT-3.5-Turbo 模型上，使攻击成功率 (ASRs) 比基线方法提高最多 +46.22%。这暴露了当前 LLM 安全措施的弱点，并强调了开发更robust的防御机制以应对微妙的多轮攻击的必要性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "4 pages, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2408.16163v2",
      "published_date": "2024-08-28 22:51:29 UTC",
      "updated_date": "2024-11-07 15:48:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:22:55.350692"
    },
    {
      "arxiv_id": "2409.00125v3",
      "title": "A Hybrid Framework for Spatial Interpolation: Merging Data-driven with Domain Knowledge",
      "title_zh": "一种空间插值混合框架：融合数据驱动方法与领域知识",
      "authors": [
        "Cong Zhang",
        "Shuyi Du",
        "Hongqing Song",
        "Yuhe Wang"
      ],
      "abstract": "Estimating spatially distributed information through the interpolation of\nscattered observation datasets often overlooks the critical role of domain\nknowledge in understanding spatial dependencies. Additionally, the features of\nthese data sets are typically limited to the spatial coordinates of the\nscattered observation locations. In this paper, we propose a hybrid framework\nthat integrates data-driven spatial dependency feature extraction with\nrule-assisted spatial dependency function mapping to augment domain knowledge.\nWe demonstrate the superior performance of our framework in two comparative\napplication scenarios, highlighting its ability to capture more localized\nspatial features in the reconstructed distribution fields. Furthermore, we\nunderscore its potential to enhance nonlinear estimation capabilities through\nthe application of transformed fuzzy rules and to quantify the inherent\nuncertainties associated with the observation data sets. Our framework\nintroduces an innovative approach to spatial information estimation by\nsynergistically combining observational data with rule-assisted domain\nknowledge.",
      "tldr_zh": "该论文提出了一种混合框架，用于空间插值（spatial interpolation），旨在将数据-driven 方法与领域知识相结合，以解决传统方法忽略空间依赖性问题。框架通过整合数据-driven 的空间依赖性特征提取和规则-assisted 的空间依赖性函数映射，增强了对局部空间特征的捕捉。实验结果在两个应用场景中显示，该框架比传统方法表现更优越，能够提升非线性估计能力、量化观察数据的不确定性，并创新性地融合观察数据与领域知识。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages, 13 figures; typos corrected, references updated; few typos\n  in few equations corrected, changed to Tex source",
      "pdf_url": "http://arxiv.org/pdf/2409.00125v3",
      "published_date": "2024-08-28 22:02:42 UTC",
      "updated_date": "2024-09-06 04:39:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:23:05.484051"
    },
    {
      "arxiv_id": "2408.16126v1",
      "title": "Improving Generalization of Speech Separation in Real-World Scenarios: Strategies in Simulation, Optimization, and Evaluation",
      "title_zh": "翻译失败",
      "authors": [
        "Ke Chen",
        "Jiaqi Su",
        "Taylor Berg-Kirkpatrick",
        "Shlomo Dubnov",
        "Zeyu Jin"
      ],
      "abstract": "Achieving robust speech separation for overlapping speakers in various\nacoustic environments with noise and reverberation remains an open challenge.\nAlthough existing datasets are available to train separators for specific\nscenarios, they do not effectively generalize across diverse real-world\nscenarios. In this paper, we present a novel data simulation pipeline that\nproduces diverse training data from a range of acoustic environments and\ncontent, and propose new training paradigms to improve quality of a general\nspeech separation model. Specifically, we first introduce AC-SIM, a data\nsimulation pipeline that incorporates broad variations in both content and\nacoustics. Then we integrate multiple training objectives into the permutation\ninvariant training (PIT) to enhance separation quality and generalization of\nthe trained model. Finally, we conduct comprehensive objective and human\nlistening experiments across separation architectures and benchmarks to\nvalidate our methods, demonstrating substantial improvement of generalization\non both non-homologous and real-world test sets.",
      "tldr_zh": "该研究针对语音分离在真实场景（如重叠说话者、噪声和混响）中的泛化问题，提出了一种新颖的数据模拟管道AC-SIM，以生成多样化的训练数据，包括各种声学环境和内容。作者将多个训练目标整合到Permutation Invariant Training (PIT)中，优化模型的分离质量和泛化能力。通过全面的客观和人类听力实验，在不同分离架构和基准上验证了这些方法，结果显示模型在非同源和真实世界测试集上的泛化性能显著提升。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "In Proceedings of the 25th Annual Conference of the International\n  Speech Communication Association, Interspeech 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.16126v1",
      "published_date": "2024-08-28 20:26:34 UTC",
      "updated_date": "2024-08-28 20:26:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:23:20.054990"
    },
    {
      "arxiv_id": "2408.16123v1",
      "title": "ChartEye: A Deep Learning Framework for Chart Information Extraction",
      "title_zh": "ChartEye: 用于图表信息提取的深度学习框架",
      "authors": [
        "Osama Mustafa",
        "Muhammad Khizer Ali",
        "Momina Moetesum",
        "Imran Siddiqi"
      ],
      "abstract": "The widespread use of charts and infographics as a means of data\nvisualization in various domains has inspired recent research in automated\nchart understanding. However, information extraction from chart images is a\ncomplex multitasked process due to style variations and, as a consequence, it\nis challenging to design an end-to-end system. In this study, we propose a deep\nlearning-based framework that provides a solution for key steps in the chart\ninformation extraction pipeline. The proposed framework utilizes hierarchal\nvision transformers for the tasks of chart-type and text-role classification,\nwhile YOLOv7 for text detection. The detected text is then enhanced using Super\nResolution Generative Adversarial Networks to improve the recognition output of\nthe OCR. Experimental results on a benchmark dataset show that our proposed\nframework achieves excellent performance at every stage with F1-scores of 0.97\nfor chart-type classification, 0.91 for text-role classification, and a mean\nAverage Precision of 0.95 for text detection.",
      "tldr_zh": "该研究提出了一种深度学习框架 ChartEye，用于从图表图像中提取信息，解决样式变化带来的多任务复杂性挑战。框架采用 hierarchal vision transformers 进行 chart-type 和 text-role classification，使用 YOLOv7 检测文本，并通过 Super Resolution Generative Adversarial Networks 增强文本质量以优化 OCR 识别输出。在基准数据集上的实验显示，该框架在 chart-type classification 的 F1-score 达 0.97，在 text-role classification 为 0.91，以及 mean Average Precision 为 0.95，展现出卓越性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "8 Pages, and 11 Figures",
      "pdf_url": "http://arxiv.org/pdf/2408.16123v1",
      "published_date": "2024-08-28 20:22:39 UTC",
      "updated_date": "2024-08-28 20:22:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:23:29.775852"
    },
    {
      "arxiv_id": "2408.16119v2",
      "title": "Data Formulator 2: Iterative Creation of Data Visualizations, with AI Transforming Data Along the Way",
      "title_zh": "Data Formulator 2：借助 AI 在过程中的数据转换进行数据可视化的迭代创建",
      "authors": [
        "Chenglong Wang",
        "Bongshin Lee",
        "Steven Drucker",
        "Dan Marshall",
        "Jianfeng Gao"
      ],
      "abstract": "Data analysts often need to iterate between data transformations and chart\ndesigns to create rich visualizations for exploratory data analysis. Although\nmany AI-powered systems have been introduced to reduce the effort of\nvisualization authoring, existing systems are not well suited for iterative\nauthoring. They typically require analysts to provide, in a single turn, a\ntext-only prompt that fully describe a complex visualization. We introduce Data\nFormulator 2 (DF2 for short), an AI-powered visualization system designed to\novercome this limitation. DF2 blends graphical user interfaces and natural\nlanguage inputs to enable users to convey their intent more effectively, while\ndelegating data transformation to AI. Furthermore, to support efficient\niteration, DF2 lets users navigate their iteration history and reuse previous\ndesigns, eliminating the need to start from scratch each time. A user study\nwith eight participants demonstrated that DF2 allowed participants to develop\ntheir own iteration styles to complete challenging data exploration sessions.",
      "tldr_zh": "本文介绍了Data Formulator 2 (DF2)，一个AI-powered系统，旨在解决数据分析师在数据转换和图表设计迭代过程中的挑战，通过结合graphical user interfaces和natural language inputs，让用户更有效地表达意图并委托AI处理数据转换。DF2支持用户导航迭代历史并重用先前设计，从而提升迭代效率。用户研究显示，该系统使参与者能够开发自己的迭代风格，成功完成具有挑战性的数据探索任务。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.16119v2",
      "published_date": "2024-08-28 20:12:17 UTC",
      "updated_date": "2025-02-21 00:50:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:23:41.122742"
    },
    {
      "arxiv_id": "2408.16081v1",
      "title": "Logic-Enhanced Language Model Agents for Trustworthy Social Simulations",
      "title_zh": "翻译失败",
      "authors": [
        "Agnieszka Mensfelt",
        "Kostas Stathis",
        "Vince Trencsenyi"
      ],
      "abstract": "We introduce the Logic-Enhanced Language Model Agents (LELMA) framework, a\nnovel approach to enhance the trustworthiness of social simulations that\nutilize large language models (LLMs). While LLMs have gained attention as\nagents for simulating human behaviour, their applicability in this role is\nlimited by issues such as inherent hallucinations and logical inconsistencies.\nLELMA addresses these challenges by integrating LLMs with symbolic AI, enabling\nlogical verification of the reasoning generated by LLMs. This verification\nprocess provides corrective feedback, refining the reasoning output. The\nframework consists of three main components: an LLM-Reasoner for producing\nstrategic reasoning, an LLM-Translator for mapping natural language reasoning\nto logic queries, and a Solver for evaluating these queries. This study focuses\non decision-making in game-theoretic scenarios as a model of human interaction.\nExperiments involving the Hawk-Dove game, Prisoner's Dilemma, and Stag Hunt\nhighlight the limitations of state-of-the-art LLMs, GPT-4 Omni and Gemini 1.0\nPro, in producing correct reasoning in these contexts. LELMA demonstrates high\naccuracy in error detection and improves the reasoning correctness of LLMs via\nself-refinement, particularly in GPT-4 Omni.",
      "tldr_zh": "该研究引入了Logic-Enhanced Language Model Agents (LELMA)框架，通过整合大型语言模型(LLMs)和symbolic AI，提升社会模拟的可信度，解决LLMs的幻觉和逻辑不一致问题。框架包括LLM-Reasoner生成战略推理、LLM-Translator将自然语言推理映射为逻辑查询，以及Solver进行查询评估。实验在Hawk-Dove game、Prisoner's Dilemma和Stag Hunt等游戏理论场景中显示，LELMA显著提高了错误检测准确性和LLMs的推理正确性，尤其在GPT-4 Omni上。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.GT",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "Source code: https://github.com/dicelab-rhul/LELMA",
      "pdf_url": "http://arxiv.org/pdf/2408.16081v1",
      "published_date": "2024-08-28 18:25:35 UTC",
      "updated_date": "2024-08-28 18:25:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:23:52.169842"
    },
    {
      "arxiv_id": "2408.16074v2",
      "title": "Verification methods for international AI agreements",
      "title_zh": "翻译失败",
      "authors": [
        "Akash R. Wasil",
        "Tom Reed",
        "Jack William Miller",
        "Peter Barnett"
      ],
      "abstract": "What techniques can be used to verify compliance with international\nagreements about advanced AI development? In this paper, we examine 10\nverification methods that could detect two types of potential violations:\nunauthorized AI training (e.g., training runs above a certain FLOP threshold)\nand unauthorized data centers. We divide the verification methods into three\ncategories: (a) national technical means (methods requiring minimal or no\naccess from suspected non-compliant nations), (b) access-dependent methods\n(methods that require approval from the nation suspected of unauthorized\nactivities), and (c) hardware-dependent methods (methods that require rules\naround advanced hardware). For each verification method, we provide a\ndescription, historical precedents, and possible evasion techniques. We\nconclude by offering recommendations for future work related to the\nverification and enforcement of international AI governance agreements.",
      "tldr_zh": "本论文探讨了验证国际人工智能（AI）协议遵守情况的技术，焦点在于检测未经授权的AI训练（如超过一定FLOP阈值的训练运行）和未经授权的数据中心。作者考察了10种验证方法，并将其分为三类：(a) national technical means（需要最小或无访问权限的方法）、(b) access-dependent methods（需嫌疑国批准的方法），以及(c) hardware-dependent methods（依赖先进硬件规定的方法）；每种方法均包括描述、历史先例和可能的规避技术。论文最终提供未来工作的推荐，以加强国际AI治理协议的验证和执行。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.16074v2",
      "published_date": "2024-08-28 18:15:19 UTC",
      "updated_date": "2024-11-04 20:46:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:24:05.186874"
    },
    {
      "arxiv_id": "2408.16073v2",
      "title": "Using Large Language Models to Create AI Personas for Replication, Generalization and Prediction of Media Effects: An Empirical Test of 133 Published Experimental Research Findings",
      "title_zh": "翻译失败",
      "authors": [
        "Leo Yeykelis",
        "Kaavya Pichai",
        "James J. Cummings",
        "Byron Reeves"
      ],
      "abstract": "This report analyzes the potential for large language models (LLMs) to\nexpedite accurate replication and generalization of published research about\nmessage effects in marketing. LLM-powered participants (personas) were tested\nby replicating 133 experimental findings from 14 papers containing 45 recent\nstudies published in the Journal of Marketing. For each study, the measures,\nstimuli, and sampling specifications were used to generate prompts for LLMs to\nact as unique personas. The AI personas, 19,447 in total across all of the\nstudies, generated complete datasets and statistical analyses were then\ncompared with the original human study results. The LLM replications\nsuccessfully reproduced 76% of the original main effects (84 out of 111),\ndemonstrating strong potential for AI-assisted replication. The overall\nreplication rate including interaction effects was 68% (90 out of 133).\nFurthermore, a test of how human results generalized to different participant\nsamples, media stimuli, and measures showed that replication results can change\nwhen tests go beyond the parameters of the original human studies. Implications\nare discussed for the replication and generalizability crises in social\nscience, the acceleration of theory building in media and marketing psychology,\nand the practical advantages of rapid message testing for consumer products.\nLimitations of AI replications are addressed with respect to complex\ninteraction effects, biases in AI models, and establishing benchmarks for AI\nmetrics in marketing research.",
      "tldr_zh": "本研究利用 Large Language Models (LLMs) 创建 AI personas 来复制和推广媒体效果实验，针对 133 个来自 14 篇 Journal of Marketing 论文的发现进行实证测试。研究通过生成 19,447 个 AI personas 模拟原始实验的措施、刺激和采样规范，结果成功复制了 76% 的主要效果（84/111）和 68% 的总体效果（包括交互效果，90/133）。此外，测试显示结果在不同参与者样本、媒体刺激和措施下可能发生变化，这突显了 AI 在加速媒体和营销心理学理论构建以及快速消息测试中的潜力，同时也指出了复杂交互效果、AI 偏差和基准建立的限制。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "40 pages, 13 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2408.16073v2",
      "published_date": "2024-08-28 18:14:39 UTC",
      "updated_date": "2025-04-24 19:12:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:24:17.638967"
    },
    {
      "arxiv_id": "2408.16068v2",
      "title": "Identification of Prognostic Biomarkers for Stage III Non-Small Cell Lung Carcinoma in Female Nonsmokers Using Machine Learning",
      "title_zh": "使用机器学习识别女性非吸烟者III期非小细胞肺癌的预后生物标志物",
      "authors": [
        "Huili Zheng",
        "Qimin Zhang",
        "Yiru Gong",
        "Zheyan Liu",
        "Shaohan Chen"
      ],
      "abstract": "Lung cancer remains a leading cause of cancer-related deaths globally, with\nnon-small cell lung cancer (NSCLC) being the most common subtype. This study\naimed to identify key biomarkers associated with stage III NSCLC in non-smoking\nfemales using gene expression profiling from the GDS3837 dataset. Utilizing\nXGBoost, a machine learning algorithm, the analysis achieved a strong\npredictive performance with an AUC score of 0.835. The top biomarkers\nidentified - CCAAT enhancer binding protein alpha (C/EBP-alpha), lactate\ndehydrogenase A4 (LDHA), UNC-45 myosin chaperone B (UNC-45B), checkpoint kinase\n1 (CHK1), and hypoxia-inducible factor 1 subunit alpha (HIF-1-alpha) - have\nbeen validated in the literature as being significantly linked to lung cancer.\nThese findings highlight the potential of these biomarkers for early diagnosis\nand personalized therapy, emphasizing the value of integrating machine learning\nwith molecular profiling in cancer research.",
      "tldr_zh": "本研究针对非吸烟女性III期非小细胞肺癌（NSCLC），利用GDS3837数据集的基因表达分析，采用XGBoost机器学习算法识别预后生物标记物。分析结果显示，该模型的预测性能达到AUC分数0.835，并确定了关键标记物，包括C/EBP-alpha、LDHA、UNC-45B、CHK1和HIF-1-alpha，这些标记物已在文献中证实与肺癌密切相关。这些发现为NSCLC的早期诊断和个性化治疗提供潜在指导，突显了机器学习与分子分析整合的价值。",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "q-bio.GN",
      "comment": "This paper has been accepted for publication in the IEEE ICBASE 2024\n  conference",
      "pdf_url": "http://arxiv.org/pdf/2408.16068v2",
      "published_date": "2024-08-28 18:08:11 UTC",
      "updated_date": "2024-08-30 03:38:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:24:30.570776"
    },
    {
      "arxiv_id": "2408.15998v2",
      "title": "Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders",
      "title_zh": "翻译失败",
      "authors": [
        "Min Shi",
        "Fuxiao Liu",
        "Shihao Wang",
        "Shijia Liao",
        "Subhashree Radhakrishnan",
        "Yilin Zhao",
        "De-An Huang",
        "Hongxu Yin",
        "Karan Sapra",
        "Yaser Yacoob",
        "Humphrey Shi",
        "Bryan Catanzaro",
        "Andrew Tao",
        "Jan Kautz",
        "Zhiding Yu",
        "Guilin Liu"
      ],
      "abstract": "The ability to accurately interpret complex visual information is a crucial\ntopic of multimodal large language models (MLLMs). Recent work indicates that\nenhanced visual perception significantly reduces hallucinations and improves\nperformance on resolution-sensitive tasks, such as optical character\nrecognition and document analysis. A number of recent MLLMs achieve this goal\nusing a mixture of vision encoders. Despite their success, there is a lack of\nsystematic comparisons and detailed ablation studies addressing critical\naspects, such as expert selection and the integration of multiple vision\nexperts. This study provides an extensive exploration of the design space for\nMLLMs using a mixture of vision encoders and resolutions. Our findings reveal\nseveral underlying principles common to various existing strategies, leading to\na streamlined yet effective design approach. We discover that simply\nconcatenating visual tokens from a set of complementary vision encoders is as\neffective as more complex mixing architectures or strategies. We additionally\nintroduce Pre-Alignment to bridge the gap between vision-focused encoders and\nlanguage tokens, enhancing model coherence. The resulting family of MLLMs,\nEagle, surpasses other leading open-source models on major MLLM benchmarks.",
      "tldr_zh": "本文探索了使用 Mixture of Encoders 的 Multimodal LLMs (MLLMs) 设计空间，通过系统比较和消融研究，揭示了专家选择与整合的关键原则。研究发现，简单地将来自一组互补视觉编码器的视觉标记连接起来，就能与复杂混合架构媲美，同时引入 Pre-Alignment 来桥接视觉编码器与语言标记的差距，提升模型整体一致性。最终，Eagle 系列 MLLMs 在主要基准测试中超过了其他领先开源模型，在视觉感知任务如光学字符识别和文档分析上显著减少了幻觉并提高了性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Github: https://github.com/NVlabs/Eagle, HuggingFace:\n  https://huggingface.co/NVEagle",
      "pdf_url": "http://arxiv.org/pdf/2408.15998v2",
      "published_date": "2024-08-28 17:59:31 UTC",
      "updated_date": "2025-03-02 23:41:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:24:43.972507"
    },
    {
      "arxiv_id": "2408.15997v1",
      "title": "Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need",
      "title_zh": "翻译失败",
      "authors": [
        "Sijia Peng",
        "Yun Xiong",
        "Yangyong Zhu",
        "Zhiqiang Shen"
      ],
      "abstract": "Time series forecasting requires balancing short-term and long-term\ndependencies for accurate predictions. Existing methods mainly focus on\nlong-term dependency modeling, neglecting the complexities of short-term\ndynamics, which may hinder performance. Transformers are superior in modeling\nlong-term dependencies but are criticized for their quadratic computational\ncost. Mamba provides a near-linear alternative but is reported less effective\nin time series longterm forecasting due to potential information loss. Current\narchitectures fall short in offering both high efficiency and strong\nperformance for long-term dependency modeling. To address these challenges, we\nintroduce Mixture of Universals (MoU), a versatile model to capture both\nshort-term and long-term dependencies for enhancing performance in time series\nforecasting. MoU is composed of two novel designs: Mixture of Feature\nExtractors (MoF), an adaptive method designed to improve time series patch\nrepresentations for short-term dependency, and Mixture of Architectures (MoA),\nwhich hierarchically integrates Mamba, FeedForward, Convolution, and\nSelf-Attention architectures in a specialized order to model long-term\ndependency from a hybrid perspective. The proposed approach achieves\nstate-of-the-art performance while maintaining relatively low computational\ncosts. Extensive experiments on seven real-world datasets demonstrate the\nsuperiority of MoU. Code is available at https://github.com/lunaaa95/mou/.",
      "tldr_zh": "时间序列预测需要平衡短期和长期依赖，但现有方法如 Transformer 虽擅长长期依赖建模，却因二次计算成本高而受限，而 Mamba 提供近线性效率但可能导致信息丢失。论文提出 Mixture of Universals (MoU) 模型，包括 Mixture of Feature Extractors (MoF) 用于提升短期依赖的特征表示，以及 Mixture of Architectures (MoA) 通过整合 Mamba、FeedForward、Convolution 和 Self-Attention 等架构来从混合视角优化长期依赖。实验在七个真实数据集上证明，MoU 实现了最先进性能，同时保持较低计算成本。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Code at https://github.com/lunaaa95/mou/",
      "pdf_url": "http://arxiv.org/pdf/2408.15997v1",
      "published_date": "2024-08-28 17:59:27 UTC",
      "updated_date": "2024-08-28 17:59:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:24:57.526072"
    },
    {
      "arxiv_id": "2408.15996v3",
      "title": "Spatio-Temporal Context Prompting for Zero-Shot Action Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Wei-Jhe Huang",
        "Min-Hung Chen",
        "Shang-Hong Lai"
      ],
      "abstract": "Spatio-temporal action detection encompasses the tasks of localizing and\nclassifying individual actions within a video. Recent works aim to enhance this\nprocess by incorporating interaction modeling, which captures the relationship\nbetween people and their surrounding context. However, these approaches have\nprimarily focused on fully-supervised learning, and the current limitation lies\nin the lack of generalization capability to recognize unseen action categories.\nIn this paper, we aim to adapt the pretrained image-language models to detect\nunseen actions. To this end, we propose a method which can effectively leverage\nthe rich knowledge of visual-language models to perform Person-Context\nInteraction. Meanwhile, our Context Prompting module will utilize contextual\ninformation to prompt labels, thereby enhancing the generation of more\nrepresentative text features. Moreover, to address the challenge of recognizing\ndistinct actions by multiple people at the same timestamp, we design the\nInterest Token Spotting mechanism which employs pretrained visual knowledge to\nfind each person's interest context tokens, and then these tokens will be used\nfor prompting to generate text features tailored to each individual. To\nevaluate the ability to detect unseen actions, we propose a comprehensive\nbenchmark on J-HMDB, UCF101-24, and AVA datasets. The experiments show that our\nmethod achieves superior results compared to previous approaches and can be\nfurther extended to multi-action videos, bringing it closer to real-world\napplications. The code and data can be found in\nhttps://webber2933.github.io/ST-CLIP-project-page.",
      "tldr_zh": "这篇论文提出了一种Spatio-Temporal Context Prompting方法，用于Zero-Shot Action Detection，即在未见动作类别下定位和分类视频中的动作。方法通过预训练图像-语言模型来捕捉Person-Context Interaction，利用Context Prompting模块增强上下文信息以生成更具代表性的文本特征，并引入Interest Token Spotting机制来识别多个人的特定兴趣上下文，从而实现个性化动作检测。实验在J-HMDB、UCF101-24和AVA数据集上建立了全面基准，结果显示该方法比现有方法提升显著，并能扩展到多动作视频的实际应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by WACV2025. Project page:\n  https://webber2933.github.io/ST-CLIP-project-page",
      "pdf_url": "http://arxiv.org/pdf/2408.15996v3",
      "published_date": "2024-08-28 17:59:05 UTC",
      "updated_date": "2024-12-05 14:38:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:25:06.441919"
    },
    {
      "arxiv_id": "2408.15992v1",
      "title": "CoGen: Learning from Feedback with Coupled Comprehension and Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Mustafa Omer Gul",
        "Yoav Artzi"
      ],
      "abstract": "Systems with both language comprehension and generation capabilities can\nbenefit from the tight connection between the two. This work studies coupling\ncomprehension and generation with focus on continually learning from\ninteraction with users. We propose techniques to tightly integrate the two\ncapabilities for both learning and inference. We situate our studies in\ntwo-player reference games, and deploy various models for thousands of\ninteractions with human users, while learning from interaction feedback\nsignals. We show dramatic improvements in performance over time, with\ncomprehension-generation coupling leading to performance improvements up to 26%\nin absolute terms and up to 17% higher accuracies compared to a non-coupled\nsystem. Our analysis also shows coupling has substantial qualitative impact on\nthe system's language, making it significantly more human-like.",
      "tldr_zh": "本研究提出 CoGen 框架，通过耦合语言理解(comprehension)和生成(generation)能力，实现系统从用户互动反馈中持续学习。\n该框架在两人参考游戏中部署各种模型，进行数千次人类互动，并利用反馈信号优化学习和推理过程。\n实验结果显示，耦合机制使系统性能绝对提升高达26%，并比非耦合系统高17%的准确率。\n此外，这种耦合还显著提升了系统的语言表达，使其更具人性化特征。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "17 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.15992v1",
      "published_date": "2024-08-28 17:58:39 UTC",
      "updated_date": "2024-08-28 17:58:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:25:21.380716"
    },
    {
      "arxiv_id": "2408.15980v2",
      "title": "In-Context Imitation Learning via Next-Token Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Letian Fu",
        "Huang Huang",
        "Gaurav Datta",
        "Lawrence Yunliang Chen",
        "William Chung-Ho Panitch",
        "Fangchen Liu",
        "Hui Li",
        "Ken Goldberg"
      ],
      "abstract": "We explore how to enhance next-token prediction models to perform in-context\nimitation learning on a real robot, where the robot executes new tasks by\ninterpreting contextual information provided during the input phase, without\nupdating its underlying policy parameters. We propose In-Context Robot\nTransformer (ICRT), a causal transformer that performs autoregressive\nprediction on sensorimotor trajectories without relying on any linguistic data\nor reward function. This formulation enables flexible and training-free\nexecution of new tasks at test time, achieved by prompting the model with\nsensorimotor trajectories of the new task composing of image observations,\nactions and states tuples, collected through human teleoperation. Experiments\nwith a Franka Emika robot demonstrate that the ICRT can adapt to new tasks\nspecified by prompts, even in environment configurations that differ from both\nthe prompt and the training data. In a multitask environment setup, ICRT\nsignificantly outperforms current state-of-the-art next-token prediction models\nin robotics on generalizing to unseen tasks. Code, checkpoints and data are\navailable on https://icrt.dev/",
      "tldr_zh": "该研究探索了通过 next-token prediction 增强模型，以实现 in-context imitation learning，在真实机器人上执行新任务，而无需更新底层策略参数。论文提出 In-Context Robot Transformer (ICRT)，一个因果 transformer，用于在传感器运动轨迹上进行自回归预测，仅依赖图像观察、动作和状态元组的提示，而不需语言数据或奖励函数。实验在 Franka Emika 机器人上显示，ICRT 能有效适应新任务和不同环境配置，并在多任务环境中显著优于现有 next-token prediction 模型，实现更好的泛化性能。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.15980v2",
      "published_date": "2024-08-28 17:50:19 UTC",
      "updated_date": "2024-09-27 20:10:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:25:29.959721"
    },
    {
      "arxiv_id": "2408.15978v1",
      "title": "WebPilot: A Versatile and Autonomous Multi-Agent System for Web Task Execution with Strategic Exploration",
      "title_zh": "翻译失败",
      "authors": [
        "Yao Zhang",
        "Zijian Ma",
        "Yunpu Ma",
        "Zhen Han",
        "Yu Wu",
        "Volker Tresp"
      ],
      "abstract": "LLM-based autonomous agents often fail to execute complex web tasks that\nrequire dynamic interaction due to the inherent uncertainty and complexity of\nthese environments. Existing LLM-based web agents typically rely on rigid,\nexpert-designed policies specific to certain states and actions, which lack the\nflexibility and generalizability needed to adapt to unseen tasks. In contrast,\nhumans excel by exploring unknowns, continuously adapting strategies, and\nresolving ambiguities through exploration. To emulate human-like adaptability,\nweb agents need strategic exploration and complex decision-making. Monte Carlo\nTree Search (MCTS) is well-suited for this, but classical MCTS struggles with\nvast action spaces, unpredictable state transitions, and incomplete information\nin web tasks. In light of this, we develop WebPilot, a multi-agent system with\na dual optimization strategy that improves MCTS to better handle complex web\nenvironments. Specifically, the Global Optimization phase involves generating a\nhigh-level plan by breaking down tasks into manageable subtasks and\ncontinuously refining this plan, thereby focusing the search process and\nmitigating the challenges posed by vast action spaces in classical MCTS.\nSubsequently, the Local Optimization phase executes each subtask using a\ntailored MCTS designed for complex environments, effectively addressing\nuncertainties and managing incomplete information. Experimental results on\nWebArena and MiniWoB++ demonstrate the effectiveness of WebPilot. Notably, on\nWebArena, WebPilot achieves SOTA performance with GPT-4, achieving a 93%\nrelative increase in success rate over the concurrent tree search-based method.\nWebPilot marks a significant advancement in general autonomous agent\ncapabilities, paving the way for more advanced and reliable decision-making in\npractical environments.",
      "tldr_zh": "该研究针对LLM-based自主代理在执行复杂网络任务时的局限性（如动态交互的不确定性和刚性策略），提出WebPilot——一个多代理系统，利用改进的Monte Carlo Tree Search (MCTS)实现战略探索。WebPilot采用双重优化策略：Global Optimization阶段分解任务为子任务并持续优化高层计划，以应对庞大的行动空间；Local Optimization阶段则针对每个子任务执行定制MCTS，处理不确定性和不完整信息。在WebArena和MiniWoB++基准测试中，WebPilot以GPT-4实现SOTA性能，成功率较现有树搜索方法提高93%，标志着自主代理决策能力的重大进步。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.15978v1",
      "published_date": "2024-08-28 17:49:29 UTC",
      "updated_date": "2024-08-28 17:49:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:25:42.609999"
    },
    {
      "arxiv_id": "2408.15969v1",
      "title": "Stability of Primal-Dual Gradient Flow Dynamics for Multi-Block Convex Optimization Problems",
      "title_zh": "多块凸优化问题的原始-对偶梯度流动力学稳定性",
      "authors": [
        "Ibrahim K. Ozaslan",
        "Panagiotis Patrinos",
        "Mihailo R. Jovanović"
      ],
      "abstract": "We examine stability properties of primal-dual gradient flow dynamics for\ncomposite convex optimization problems with multiple, possibly nonsmooth, terms\nin the objective function under the generalized consensus constraint. The\nproposed dynamics are based on the proximal augmented Lagrangian and they\nprovide a viable alternative to ADMM which faces significant challenges from\nboth analysis and implementation viewpoints in large-scale multi-block\nscenarios. In contrast to customized algorithms with individualized convergence\nguarantees, we provide a systematic approach for solving a broad class of\nchallenging composite optimization problems. We leverage various structural\nproperties to establish global (exponential) convergence guarantees for the\nproposed dynamics. Our assumptions are much weaker than those required to prove\n(exponential) stability of various primal-dual dynamics as well as (linear)\nconvergence of discrete-time methods, e.g., standard two-block and multi-block\nADMM and EXTRA algorithms. Finally, we show necessity of some of our structural\nassumptions for exponential stability and provide computational experiments to\ndemonstrate the convenience of the proposed dynamics for parallel and\ndistributed computing applications.",
      "tldr_zh": "本文研究了原-dual梯度流动态在多块凸优化问题中的稳定性，这些问题涉及复合目标函数和泛化共识约束。提出的动态基于近端增强Lagrangian（proximal augmented Lagrangian），作为ADMM的替代方案，能够系统处理广泛的复合优化问题，并通过结构属性确保全局指数收敛。相比现有方法，该框架的假设更弱，且实验结果证明了其在并行和分布式计算中的便利性和有效性。",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "math.OC",
      "comment": "31 pages; 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.15969v1",
      "published_date": "2024-08-28 17:43:18 UTC",
      "updated_date": "2024-08-28 17:43:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:25:53.542189"
    },
    {
      "arxiv_id": "2408.15966v2",
      "title": "More Text, Less Point: Towards 3D Data-Efficient Point-Language Understanding",
      "title_zh": "翻译失败",
      "authors": [
        "Yuan Tang",
        "Xu Han",
        "Xianzhi Li",
        "Qiao Yu",
        "Jinfeng Xu",
        "Yixue Hao",
        "Long Hu",
        "Min Chen"
      ],
      "abstract": "Enabling Large Language Models (LLMs) to comprehend the 3D physical world\nremains a significant challenge. Due to the lack of large-scale 3D-text pair\ndatasets, the success of LLMs has yet to be replicated in 3D understanding. In\nthis paper, we rethink this issue and propose a new task: 3D Data-Efficient\nPoint-Language Understanding. The goal is to enable LLMs to achieve robust 3D\nobject understanding with minimal 3D point cloud and text data pairs. To\naddress this task, we introduce GreenPLM, which leverages more text data to\ncompensate for the lack of 3D data. First, inspired by using CLIP to align\nimages and text, we utilize a pre-trained point cloud-text encoder to map the\n3D point cloud space to the text space. This mapping leaves us to seamlessly\nconnect the text space with LLMs. Once the point-text-LLM connection is\nestablished, we further enhance text-LLM alignment by expanding the\nintermediate text space, thereby reducing the reliance on 3D point cloud data.\nSpecifically, we generate 6M free-text descriptions of 3D objects, and design a\nthree-stage training strategy to help LLMs better explore the intrinsic\nconnections between different modalities. To achieve efficient modality\nalignment, we design a zero-parameter cross-attention module for token pooling.\nExtensive experimental results show that GreenPLM requires only 12% of the 3D\ntraining data used by existing state-of-the-art models to achieve superior 3D\nunderstanding. Remarkably, GreenPLM also achieves competitive performance using\ntext-only data. The code and weights are available at:\nhttps://github.com/TangYuan96/GreenPLM.",
      "tldr_zh": "该论文提出“3D Data-Efficient Point-Language Understanding”新任务，旨在让 Large Language Models (LLMs) 用最少的 3D 点云和文本数据对实现鲁棒的 3D 对象理解，以解决大规模 3D-文本数据集缺失的问题。作者引入 GreenPLM 框架，通过预训练的点云-文本编码器将 3D 空间映射到文本空间，并利用额外生成的 6M 文本描述和三阶段训练策略增强模态对齐，同时设计零参数的交叉注意力模块优化效率。实验结果显示，GreenPLM 仅需现有最先进模型 12% 的 3D 训练数据即可实现优越性能，甚至使用纯文本数据也能取得竞争性成果。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.15966v2",
      "published_date": "2024-08-28 17:38:44 UTC",
      "updated_date": "2024-09-05 06:33:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:26:08.288889"
    },
    {
      "arxiv_id": "2409.00124v2",
      "title": "Leveraging Large Language Models for Wireless Symbol Detection via In-Context Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Momin Abbas",
        "Koushik Kar",
        "Tianyi Chen"
      ],
      "abstract": "Deep neural networks (DNNs) have made significant strides in tackling\nchallenging tasks in wireless systems, especially when an accurate wireless\nmodel is not available. However, when available data is limited, traditional\nDNNs often yield subpar results due to underfitting. At the same time, large\nlanguage models (LLMs) exemplified by GPT-3, have remarkably showcased their\ncapabilities across a broad range of natural language processing tasks. But\nwhether and how LLMs can benefit challenging non-language tasks in wireless\nsystems is unexplored. In this work, we propose to leverage the in-context\nlearning ability (a.k.a. prompting) of LLMs to solve wireless tasks in the low\ndata regime without any training or fine-tuning, unlike DNNs which require\ntraining. We further demonstrate that the performance of LLMs varies\nsignificantly when employed with different prompt templates. To solve this\nissue, we employ the latest LLM calibration methods. Our results reveal that\nusing LLMs via ICL methods generally outperforms traditional DNNs on the symbol\ndemodulation task and yields highly confident predictions when coupled with\ncalibration techniques.",
      "tldr_zh": "本文提出利用大型语言模型(LLMs)的in-context learning（即prompting）方法，来处理无线系统中数据有限时的符号检测任务，而非传统深度神经网络(DNNs)需要训练。研究发现，不同prompt templates会影响LLMs的表现，因此引入LLM calibration技术进行优化。结果表明，LLMs通过ICL方法在符号解调任务上显著优于DNNs，并能提供高置信度的预测。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "Accepted at IEEE GLOBECOM 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.00124v2",
      "published_date": "2024-08-28 17:19:20 UTC",
      "updated_date": "2024-09-08 14:49:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:26:17.783741"
    },
    {
      "arxiv_id": "2408.15950v2",
      "title": "Atari-GPT: Benchmarking Multimodal Large Language Models as Low-Level Policies in Atari Games",
      "title_zh": "Atari-GPT：评估多模态大型语言模型作为 Atari 游戏",
      "authors": [
        "Nicholas R. Waytowich",
        "Devin White",
        "MD Sunbeam",
        "Vinicius G. Goecks"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have expanded their\ncapabilities beyond traditional text-based tasks to multimodal domains,\nintegrating visual, auditory, and textual data. While multimodal LLMs have been\nextensively explored for high-level planning in domains like robotics and\ngames, their potential as low-level controllers remains largely untapped. In\nthis paper, we introduce a novel benchmark aimed at testing the emergent\ncapabilities of multimodal LLMs as low-level policies in Atari games. Unlike\ntraditional reinforcement learning (RL) methods that require training for each\nnew environment and reward function specification, these LLMs utilize\npre-existing multimodal knowledge to directly engage with game environments.\nOur study assesses the performances of multiple multimodal LLMs against\ntraditional RL agents, human players, and random agents, focusing on their\nability to understand and interact with complex visual scenes and formulate\nstrategic responses. Our results show that these multimodal LLMs are not yet\ncapable of being zero-shot low-level policies. Furthermore, we see that this\nis, in part, due to their visual and spatial reasoning. Additional results and\nvideos are available on our project webpage:\nhttps://dev1nw.github.io/atari-gpt/.",
      "tldr_zh": "本文提出Atari-GPT基准，用于评估多模态Large Language Models (LLMs)作为Atari游戏中低层次策略的性能，旨在探索LLMs在复杂视觉场景中的互动能力。不同于传统Reinforcement Learning (RL)方法，该基准让LLMs利用预有知识直接参与游戏，而无需针对每个环境进行训练。实验结果表明，多模态LLMs目前无法作为零样本低层次策略，其不足主要源于视觉和空间推理的局限性，与RL代理和人类玩家的表现相比仍有差距。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Currently under review",
      "pdf_url": "http://arxiv.org/pdf/2408.15950v2",
      "published_date": "2024-08-28 17:08:56 UTC",
      "updated_date": "2024-12-02 03:48:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:26:30.970665"
    },
    {
      "arxiv_id": "2408.15924v1",
      "title": "Local Descriptors Weighted Adaptive Threshold Filtering For Few-Shot Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Bingchen Yan"
      ],
      "abstract": "Few-shot image classification is a challenging task in the field of machine\nlearning, involving the identification of new categories using a limited number\nof labeled samples. In recent years, methods based on local descriptors have\nmade significant progress in this area. However, the key to improving\nclassification accuracy lies in effectively filtering background noise and\naccurately selecting critical local descriptors highly relevant to image\ncategory information.\n  To address this challenge, we propose an innovative weighted adaptive\nthreshold filtering (WATF) strategy for local descriptors. This strategy can\ndynamically adjust based on the current task and image context, thereby\nselecting local descriptors most relevant to the image category. This enables\nthe model to better focus on category-related information while effectively\nmitigating interference from irrelevant background regions.\n  To evaluate the effectiveness of our method, we adopted the N-way K-shot\nexperimental framework. Experimental results show that our method not only\nimproves the clustering effect of selected local descriptors but also\nsignificantly enhances the discriminative ability between image categories.\nNotably, our method maintains a simple and lightweight design philosophy\nwithout introducing additional learnable parameters. This feature ensures\nconsistency in filtering capability during both training and testing phases,\nfurther enhancing the reliability and practicality of the method.",
      "tldr_zh": "本研究针对 Few-shot image classification 的挑战，提出了一种创新的 weighted adaptive threshold filtering (WATF) 策略，用于有效过滤背景噪声并选择与图像类别高度相关的 local descriptors。该策略能根据当前任务和图像上下文动态调整阈值，帮助模型更好地聚焦于类别相关信息，从而提升分类准确性。在 N-way K-shot 实验框架下，实验结果显示，该方法显著提高了 selected local descriptors 的聚类效果，并增强了图像类别间的区分能力。同时，该方法保持简单轻量设计，无需引入额外可学习参数，确保训练和测试阶段的一致性，提高了其可靠性和实用性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.15924v1",
      "published_date": "2024-08-28 16:36:23 UTC",
      "updated_date": "2024-08-28 16:36:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:26:41.609265"
    },
    {
      "arxiv_id": "2408.16792v1",
      "title": "Uncertainty-aware segmentation for rainfall prediction post processing",
      "title_zh": "翻译失败",
      "authors": [
        "Simone Monaco",
        "Luca Monaco",
        "Daniele Apiletti"
      ],
      "abstract": "Accurate precipitation forecasts are crucial for applications such as flood\nmanagement, agricultural planning, water resource allocation, and weather\nwarnings. Despite advances in numerical weather prediction (NWP) models, they\nstill exhibit significant biases and uncertainties, especially at high spatial\nand temporal resolutions. To address these limitations, we explore\nuncertainty-aware deep learning models for post-processing daily cumulative\nquantitative precipitation forecasts to obtain forecast uncertainties that lead\nto a better trade-off between accuracy and reliability. Our study compares\ndifferent state-of-the-art models, and we propose a variant of the well-known\nSDE-Net, called SDE U-Net, tailored to segmentation problems like ours. We\nevaluate its performance for both typical and intense precipitation events.\n  Our results show that all deep learning models significantly outperform the\naverage baseline NWP solution, with our implementation of the SDE U-Net showing\nthe best trade-off between accuracy and reliability. Integrating these models,\nwhich account for uncertainty, into operational forecasting systems can improve\ndecision-making and preparedness for weather-related events.",
      "tldr_zh": "该研究针对数值天气预报(NWP)模型在高分辨率降雨预测中的偏差和不确定性问题，提出了一种不确定性感知的深度学习后处理方法，以优化每日累计定量降雨预报的准确性和可靠性。研究比较了多种最先进模型，并开发了SDE U-Net，这是一种针对分割任务的SDE-Net变体，通过评估典型和强烈降雨事件，证明其在准确性和可靠性之间取得了最佳权衡。结果显示，所有深度学习模型均显著优于NWP基准，而SDE U-Net的整合有望提升操作预报系统的决策能力和应对天气事件的能力。",
      "categories": [
        "physics.ao-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "physics.ao-ph",
      "comment": "Paper accepted at the 3rd Workshop on Uncertainty Reasoning and\n  Quantification in Decision Making at ACM SIGKDD'24 (August 26, 2024,\n  Barcelona)",
      "pdf_url": "http://arxiv.org/pdf/2408.16792v1",
      "published_date": "2024-08-28 16:31:40 UTC",
      "updated_date": "2024-08-28 16:31:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:26:54.327590"
    },
    {
      "arxiv_id": "2408.15915v2",
      "title": "Leveraging Open Knowledge for Advancing Task Expertise in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yuncheng Yang",
        "Yulei Qin",
        "Tong Wu",
        "Zihan Xu",
        "Gang Li",
        "Pengcheng Guo",
        "Hang Shao",
        "Yuchen Shi",
        "Ke Li",
        "Xing Sun",
        "Jie Yang",
        "Yun Gu"
      ],
      "abstract": "The cultivation of expertise for large language models (LLMs) to solve tasks\nof specific areas often requires special-purpose tuning with calibrated\nbehaviors on the expected stable outputs. To avoid huge cost brought by manual\npreparation of instruction datasets and training resources up to hundreds of\nhours, the exploitation of open knowledge including a wealth of low rank\nadaptation (LoRA) models and instruction datasets serves as a good starting\npoint. However, existing methods on model and data selection focus on the\nperformance of general-purpose capabilities while neglecting the knowledge gap\nexposed in domain-specific deployment. In the present study, we propose to\nbridge such gap by introducing few human-annotated samples (i.e., K-shot) for\nadvancing task expertise of LLMs with open knowledge. Specifically, we develop\nan efficient and scalable pipeline to cost-efficiently produce task experts\nwhere K-shot data intervene in selecting the most promising expert candidates\nand the task-relevant instructions. A mixture-of-expert (MoE) system is built\nto make the best use of individual-yet-complementary knowledge between multiple\nexperts. We unveil the two keys to the success of a MoE system, 1) the abidance\nby K-shot, and 2) the insistence on diversity. For the former, we ensure that\nmodels that truly possess problem-solving abilities on K-shot are selected\nrather than those blind guessers. Besides, during data selection, instructions\nthat share task-relevant contexts with K-shot are prioritized. For the latter,\nwe highlight the diversity of constituting experts and that of the fine-tuning\ninstructions throughout the model and data selection process. Extensive\nexperimental results confirm the superiority of our approach over existing\nmethods on utilization of open knowledge across various tasks. Our codes will\nbe available at https://github.com/Yaphabates/Rocket.",
      "tldr_zh": "该研究提出了一种利用开放知识提升大型语言模型（LLMs）在特定任务上的专业性的方法，以避免手动准备指令数据集和高成本训练。研究开发了一个高效管道，使用少量人类标注样本（K-shot）来选择最有前景的专家候选和任务相关指令，并构建混合专家（MoE）系统，以整合多个专家的互补知识。成功的关键在于遵守 K-shot 以确保模型的真实问题解决能力，并强调专家和指令的多样性；实验结果显示，该方法在各种任务上优于现有方法，证明了其在开放知识利用方面的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "29 pages, 12 tables, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.15915v2",
      "published_date": "2024-08-28 16:28:07 UTC",
      "updated_date": "2024-09-07 15:12:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:27:05.936783"
    },
    {
      "arxiv_id": "2408.16036v1",
      "title": "Efficient $k$-NN Search in IoT Data: Overlap Optimization in Tree-Based Indexing Structures",
      "title_zh": "翻译失败",
      "authors": [
        "Ala-Eddine Benrazek",
        "Zineddine Kouahla",
        "Brahim Farou",
        "Hamid Seridi",
        "Ibtissem Kemouguette"
      ],
      "abstract": "The proliferation of interconnected devices in the Internet of Things (IoT)\nhas led to an exponential increase in data, commonly known as Big IoT Data.\nEfficient retrieval of this heterogeneous data demands a robust indexing\nmechanism for effective organization. However, a significant challenge remains:\nthe overlap in data space partitions during index construction. This overlap\nincreases node access during search and retrieval, resulting in higher resource\nconsumption, performance bottlenecks, and impedes system scalability. To\naddress this issue, we propose three innovative heuristics designed to quantify\nand strategically reduce data space partition overlap. The volume-based method\n(VBM) offers a detailed assessment by calculating the intersection volume\nbetween partitions, providing deeper insights into spatial relationships. The\ndistance-based method (DBM) enhances efficiency by using the distance between\npartition centers and radii to evaluate overlap, offering a streamlined yet\naccurate approach. Finally, the object-based method (OBM) provides a practical\nsolution by counting objects across multiple partitions, delivering an\nintuitive understanding of data space dynamics. Experimental results\ndemonstrate the effectiveness of these methods in reducing search time,\nunderscoring their potential to improve data space partitioning and enhance\noverall system performance.",
      "tldr_zh": "该论文针对物联网 (IoT) 数据中 $k$-NN 搜索的效率问题，聚焦于树-based 索引结构中数据空间分区的重叠问题，该重叠导致节点访问增加、资源消耗升高并影响系统可伸缩性。作者提出三种创新启发式方法：Volume-Based Method (VBM) 通过计算分区交集体积评估空间关系、Distance-Based Method (DBM) 利用分区中心距离和半径简化重叠评估，以及Object-Based Method (OBM) 通过计数跨越分区的对象提供直观洞察。实验结果表明，这些方法显著减少了搜索时间，提升了数据空间分区和整体系统性能。",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.IR",
        "cs.PF",
        "68P05, 68T01, 68P20",
        "E.1; H.2; H.3; I.2"
      ],
      "primary_category": "cs.DB",
      "comment": "28 pages, 21 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2408.16036v1",
      "published_date": "2024-08-28 16:16:55 UTC",
      "updated_date": "2024-08-28 16:16:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:27:19.348701"
    },
    {
      "arxiv_id": "2408.15901v1",
      "title": "Nexus: Specialization meets Adaptability for Efficiently Training Mixture of Experts",
      "title_zh": "Nexus: 专业化与适应性相结合，用于高效训练",
      "authors": [
        "Nikolas Gritsch",
        "Qizhen Zhang",
        "Acyr Locatelli",
        "Sara Hooker",
        "Ahmet Üstün"
      ],
      "abstract": "Efficiency, specialization, and adaptability to new data distributions are\nqualities that are hard to combine in current Large Language Models. The\nMixture of Experts (MoE) architecture has been the focus of significant\nresearch because its inherent conditional computation enables such desirable\nproperties. In this work, we focus on \"upcycling\" dense expert models into an\nMoE, aiming to improve specialization while also adding the ability to adapt to\nnew tasks easily. We introduce Nexus, an enhanced MoE architecture with\nadaptive routing where the model learns to project expert embeddings from\ndomain representations. This approach allows Nexus to flexibly add new experts\nafter the initial upcycling through separately trained dense models, without\nrequiring large-scale MoE training for unseen data domains. Our experiments\nshow that Nexus achieves a relative gain of up to 2.1% over the baseline for\ninitial upcycling, and a 18.8% relative gain for extending the MoE with a new\nexpert by using limited finetuning data. This flexibility of Nexus is crucial\nto enable an open-source ecosystem where every user continuously assembles\ntheir own MoE-mix according to their needs.",
      "tldr_zh": "该研究针对大型语言模型（Large Language Models, LLMs）的效率、专业化和适应新数据分布的挑战，提出Nexus架构，通过对Mixture of Experts (MoE)模型的升级（upcycling）来提升其特性。Nexus采用自适应路由机制，允许模型从领域表示学习投影专家嵌入，从而在初始升级后灵活添加新专家，而无需大规模MoE训练。实验显示，Nexus在初始upcycling上较基线提升多达2.1%，并在用有限微调数据添加新专家时实现18.8%的相对增益，最终促进开源生态系统，让用户根据需求自定义MoE组合。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.15901v1",
      "published_date": "2024-08-28 16:12:55 UTC",
      "updated_date": "2024-08-28 16:12:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:27:30.362639"
    },
    {
      "arxiv_id": "2408.15898v3",
      "title": "Airfoil Diffusion: Denoising Diffusion Model For Conditional Airfoil Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Reid Graves",
        "Amir Barati Farimani"
      ],
      "abstract": "The design of aerodynamic shapes, such as airfoils, has traditionally\nrequired significant computational resources and relied on predefined design\nparameters, which limit the potential for novel shape synthesis. In this work,\nwe introduce a data-driven methodology for airfoil generation using a diffusion\nmodel. Trained on a dataset of preexisting airfoils, our model can generate an\narbitrary number of new airfoils from random vectors, which can be conditioned\non specific aerodynamic performance metrics such as lift and drag, or geometric\ncriteria. Our results demonstrate that the diffusion model effectively produces\nairfoil shapes with realistic aerodynamic properties, offering substantial\nimprovements in efficiency, flexibility, and the potential for discovering\ninnovative airfoil designs. This approach significantly expands the design\nspace, facilitating the synthesis of high-performance aerodynamic shapes that\ntranscend the limitations of traditional methods.",
      "tldr_zh": "本文提出了一种基于Denoising Diffusion Model的数据驱动方法，用于条件机翼（airfoil）生成，以克服传统设计依赖预定义参数的局限性。该模型在现有机翼数据集上训练，能从随机向量生成任意数量的新机翼，并根据特定气动性能指标如lift和drag，或几何标准进行条件控制。实验结果显示，该方法能高效产生具有真实气动特性的机翼形状，提高设计灵活性，并扩展设计空间，促进创新气动形状的发现。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "20 Pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.15898v3",
      "published_date": "2024-08-28 16:12:16 UTC",
      "updated_date": "2024-12-18 16:29:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:27:52.045093"
    },
    {
      "arxiv_id": "2408.15896v1",
      "title": "A New Method for Cross-Lingual-based Semantic Role Labeling",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammad Ebrahimi",
        "Behrouz Minaei Bidgoli",
        "Nasim Khozouei"
      ],
      "abstract": "Semantic role labeling is a crucial task in natural language processing,\nenabling better comprehension of natural language. However, the lack of\nannotated data in multiple languages has posed a challenge for researchers. To\naddress this, a deep learning algorithm based on model transfer has been\nproposed. The algorithm utilizes a dataset consisting of the English portion of\nCoNLL2009 and a corpus of semantic roles in Persian. To optimize the efficiency\nof training, only ten percent of the educational data from each language is\nused. The results of the proposed model demonstrate significant improvements\ncompared to Niksirt et al.'s model. In monolingual mode, the proposed model\nachieved a 2.05 percent improvement on F1-score, while in cross-lingual mode,\nthe improvement was even more substantial, reaching 6.23 percent. Worth noting\nis that the compared model only trained two of the four stages of semantic role\nlabeling and employed golden data for the remaining two stages. This suggests\nthat the actual superiority of the proposed model surpasses the reported\nnumbers by a significant margin. The development of cross-lingual methods for\nsemantic role labeling holds promise, particularly in addressing the scarcity\nof annotated data for various languages. These advancements pave the way for\nfurther research in understanding and processing natural language across\ndifferent linguistic contexts.",
      "tldr_zh": "这篇论文提出了一种基于模型转移（model transfer）的深度学习算法，用于跨语言语义角色标注（Semantic Role Labeling），旨在解决多语言标注数据不足的挑战。该方法利用 CoNLL2009 的英语部分和波斯语语义角色语料，仅使用每个语言的 10% 训练数据进行优化。实验结果显示，与 Niksirt et al. 的模型相比，在单语言模式下 F1-score 提高了 2.05%，而在 cross-lingual mode 下提升了 6.23%，实际优势可能更大。该创新为跨语言自然语言处理提供了新途径，特别是在数据稀缺语言中的应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.15896v1",
      "published_date": "2024-08-28 16:06:12 UTC",
      "updated_date": "2024-08-28 16:06:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:27:55.967606"
    },
    {
      "arxiv_id": "2408.15886v2",
      "title": "Enhancing Intrusion Detection in IoT Environments: An Advanced Ensemble Approach Using Kolmogorov-Arnold Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Amar Amouri",
        "Mohamad Mahmoud Al Rahhal",
        "Yakoub Bazi",
        "Ismail Butun",
        "Imad Mahgoub"
      ],
      "abstract": "In recent years, the evolution of machine learning techniques has\nsignificantly impacted the field of intrusion detection, particularly within\nthe context of the Internet of Things (IoT). As IoT networks expand, the need\nfor robust security measures to counteract potential threats has become\nincreasingly critical. This paper introduces a hybrid Intrusion Detection\nSystem (IDS) that synergistically combines Kolmogorov-Arnold Networks (KANs)\nwith the XGBoost algorithm. Our proposed IDS leverages the unique capabilities\nof KANs, which utilize learnable activation functions to model complex\nrelationships within data, alongside the powerful ensemble learning techniques\nof XGBoost, known for its high performance in classification tasks. This hybrid\napproach not only enhances the detection accuracy but also improves the\ninterpretability of the model, making it suitable for dynamic and intricate IoT\nenvironments. Experimental evaluations demonstrate that our hybrid IDS achieves\nan impressive detection accuracy exceeding 99% in distinguishing between benign\nand malicious activities. Additionally, we were able to achieve F1 scores,\nprecision, and recall that exceeded 98%. Furthermore, we conduct a comparative\nanalysis against traditional Multi-Layer Perceptron (MLP) networks, assessing\nperformance metrics such as Precision, Recall, and F1-score. The results\nunderscore the efficacy of integrating KANs with XGBoost, highlighting the\npotential of this innovative approach to significantly strengthen the security\nframework of IoT networks.",
      "tldr_zh": "本研究提出了一种先进的混合入侵检测系统（Intrusion Detection System, IDS），将 Kolmogorov-Arnold Networks (KANs) 与 XGBoost 算法相结合，用于提升物联网（IoT）环境的入侵检测能力。KANs 通过可学习的激活函数建模复杂数据关系，而 XGBoost 的集成学习技术则进一步提高了检测准确性和模型可解释性，使其适用于动态 IoT 场景。实验结果显示，该系统在区分良性和恶意活动时，检测准确率超过 99%，F1 分数、精确度和召回率均超过 98%，并在与传统 Multi-Layer Perceptron (MLP) 网络的比较中表现出显著优势，从而强化了 IoT 网络的安全框架。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "To be presented at the 11th International Symposium on Networks,\n  Computers and Communications (ISNCC'24) will be held in Washington DC- USA,\n  from October 22 to 25, 2024. Accepted (6 pages and 5 figures)",
      "pdf_url": "http://arxiv.org/pdf/2408.15886v2",
      "published_date": "2024-08-28 15:58:49 UTC",
      "updated_date": "2024-08-29 15:54:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:28:05.811922"
    },
    {
      "arxiv_id": "2408.15879v2",
      "title": "Persuasion Games using Large Language Models",
      "title_zh": "利用大语言模型的说服博弈",
      "authors": [
        "Ganesh Prasath Ramani",
        "Shirish Karande",
        "Santhosh V",
        "Yash Bhatia"
      ],
      "abstract": "Large Language Models (LLMs) have emerged as formidable instruments capable\nof comprehending and producing human-like text. This paper explores the\npotential of LLMs, to shape user perspectives and subsequently influence their\ndecisions on particular tasks. This capability finds applications in diverse\ndomains such as Investment, Credit cards and Insurance, wherein they assist\nusers in selecting appropriate insurance policies, investment plans, Credit\ncards, Retail, as well as in Behavioral Change Support Systems (BCSS).\n  We present a sophisticated multi-agent framework wherein a consortium of\nagents operate in collaborative manner. The primary agent engages directly with\nuser agents through persuasive dialogue, while the auxiliary agents perform\ntasks such as information retrieval, response analysis, development of\npersuasion strategies, and validation of facts. Empirical evidence from our\nexperiments demonstrates that this collaborative methodology significantly\nenhances the persuasive efficacy of the LLM. We continuously analyze the\nresistance of the user agent to persuasive efforts and counteract it by\nemploying a combination of rule-based and LLM-based resistance-persuasion\nmapping techniques.\n  We employ simulated personas and generate conversations in insurance,\nbanking, and retail domains to evaluate the proficiency of large language\nmodels (LLMs) in recognizing, adjusting to, and influencing various personality\ntypes. Concurrently, we examine the resistance mechanisms employed by LLM\nsimulated personas. Persuasion is quantified via measurable surveys before and\nafter interaction, LLM-generated scores on conversation, and user decisions\n(purchase or non-purchase).",
      "tldr_zh": "这篇论文探讨了 Large Language Models (LLMs) 在说服用户观点和决策方面的潜力，应用于投资、信用卡、保险和零售等领域。研究提出了一种多智能体框架，其中主要代理通过说服对话直接与用户互动，而辅助代理负责信息检索、响应分析、说服策略开发及事实验证，同时采用规则-based 和 LLM-based 方法应对用户抵抗。实验通过模拟对话和量化指标（如互动前后调查、LLM 生成分数及用户购买决策）证明，该框架显著提升了 LLMs 的说服效果，并在不同个性类型中展示了其适应性和影响力。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.15879v2",
      "published_date": "2024-08-28 15:50:41 UTC",
      "updated_date": "2024-09-02 02:30:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:28:28.691866"
    },
    {
      "arxiv_id": "2408.15874v3",
      "title": "Robust Statistical Scaling of Outlier Scores: Improving the Quality of Outlier Probabilities for Outliers (Extended Version)",
      "title_zh": "翻译失败",
      "authors": [
        "Philipp Röchner",
        "Henrique O. Marques",
        "Ricardo J. G. B. Campello",
        "Arthur Zimek",
        "Franz Rothlauf"
      ],
      "abstract": "Outlier detection algorithms typically assign an outlier score to each\nobservation in a dataset, indicating the degree to which an observation is an\noutlier. However, these scores are often not comparable across algorithms and\ncan be difficult for humans to interpret. Statistical scaling addresses this\nproblem by transforming outlier scores into outlier probabilities without using\nground-truth labels, thereby improving interpretability and comparability\nacross algorithms. However, the quality of this transformation can be different\nfor outliers and inliers. Missing outliers in scenarios where they are of\nparticular interest - such as healthcare, finance, or engineering - can be\ncostly or dangerous. Thus, ensuring good probabilities for outliers is\nessential. This paper argues that statistical scaling, as commonly used in the\nliterature, does not produce equally good probabilities for outliers as for\ninliers. Therefore, we propose robust statistical scaling, which uses robust\nestimators to improve the probabilities for outliers. We evaluate several\nvariants of our method against other outlier score transformations for\nreal-world datasets and outlier detection algorithms, where it can improve the\nprobabilities for outliers.",
      "tldr_zh": "该论文指出，现有的异常检测算法生成的异常分数（outlier scores）跨算法不可比且难以解释，而统计缩放（statistical scaling）虽可将这些分数转化为异常概率（outlier probabilities）以提升可解释性，但对异常点（outliers）的概率质量不如正常点（inliers）。为此，作者提出 robust statistical scaling 方法，使用 robust estimators 来优化异常点的概率转换，确保在医疗、金融和工程等领域避免潜在风险。实验结果显示，该方法在真实数据集和多种异常检测算法上表现优于其他转换技术，提高了异常点的概率准确性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 4 figures, extended version of an original article\n  published in Similarity Search and Applications. SISAP 2024. Lecture Notes in\n  Computer Science, vol 15268. Springer, by Springer Nature",
      "pdf_url": "http://arxiv.org/pdf/2408.15874v3",
      "published_date": "2024-08-28 15:44:34 UTC",
      "updated_date": "2024-10-30 15:51:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:28:42.006900"
    },
    {
      "arxiv_id": "2408.15868v1",
      "title": "GenDDS: Generating Diverse Driving Video Scenarios with Prompt-to-Video Generative Model",
      "title_zh": "翻译失败",
      "authors": [
        "Yongjie Fu",
        "Yunlong Li",
        "Xuan Di"
      ],
      "abstract": "Autonomous driving training requires a diverse range of datasets encompassing\nvarious traffic conditions, weather scenarios, and road types. Traditional data\naugmentation methods often struggle to generate datasets that represent rare\noccurrences. To address this challenge, we propose GenDDS, a novel approach for\ngenerating driving scenarios generation by leveraging the capabilities of\nStable Diffusion XL (SDXL), an advanced latent diffusion model. Our methodology\ninvolves the use of descriptive prompts to guide the synthesis process, aimed\nat producing realistic and diverse driving scenarios. With the power of the\nlatest computer vision techniques, such as ControlNet and Hotshot-XL, we have\nbuilt a complete pipeline for video generation together with SDXL. We employ\nthe KITTI dataset, which includes real-world driving videos, to train the\nmodel. Through a series of experiments, we demonstrate that our model can\ngenerate high-quality driving videos that closely replicate the complexity and\nvariability of real-world driving scenarios. This research contributes to the\ndevelopment of sophisticated training data for autonomous driving systems and\nopens new avenues for creating virtual environments for simulation and\nvalidation purposes.",
      "tldr_zh": "该研究提出GenDDS，一种利用提示到视频生成模型的方法，旨在解决自动驾驶训练中数据集多样性不足的问题，特别是针对稀有事件的生成。方法基于Stable Diffusion XL (SDXL)，结合ControlNet和Hotshot-XL构建完整的视频生成管道，并使用KITTI数据集进行训练，通过描述性提示合成现实且多样的驾驶场景。实验结果显示，GenDDS生成的视频高质量地复制了真实世界的复杂性和可变性，为自动驾驶系统的训练数据开发和虚拟模拟环境提供了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.15868v1",
      "published_date": "2024-08-28 15:37:44 UTC",
      "updated_date": "2024-08-28 15:37:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:28:52.778191"
    },
    {
      "arxiv_id": "2408.15866v1",
      "title": "Retrieval-Augmented Instruction Tuning for Automated Process Engineering Calculations : A Tool-Chaining Problem-Solving Framework with Attributable Reflection",
      "title_zh": "翻译失败",
      "authors": [
        "Sagar Srinivas Sakhinana",
        "Geethan Sannidhi",
        "Venkataramana Runkana"
      ],
      "abstract": "The current technology landscape lacks a foundational AI model for solving\nprocess engineering calculations. In this work, we introduce a novel autonomous\nagent framework leveraging Retrieval-Augmented Instruction-Tuning (RAIT) to\nenhance open, customizable small code language models (SLMs) for these\ncalculations. By combining instruction tuned code SLMs with Retrieval-Augmented\nCode Generation (RACG) using external tools, the agent generates, debugs, and\noptimizes code from natural language specifications. Our approach addresses the\nlimitations of the current lack of a foundational AI model for specialized\nprocess engineering tasks and offers benefits of explainability, knowledge\nediting, and cost-effectiveness. Additionally, we curate custom datasets of\nchemical and process engineering problems and solutions to overcome data\nscarcity. Experimental results show that our framework matches the performance\nof large-scale proprietary models on benchmark datasets, proving its\neffectiveness and usability.",
      "tldr_zh": "该研究提出了一种自主代理框架，利用Retrieval-Augmented Instruction-Tuning (RAIT)增强小型代码语言模型(SLMs)，以自动化过程工程计算，解决当前AI模型在该领域的缺失问题。框架结合指令调整的SLMs和Retrieval-Augmented Code Generation (RACG)，通过外部工具链实现代码的生成、调试和优化，并引入可归因的反思机制，提供可解释性、知识编辑和成本效益优势。研究者还编制了自定义的过程工程数据集来克服数据稀缺；实验结果表明，该框架在基准数据集上与大型专有模型性能相当，证明了其有效性和实用性。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted for publication at ML4CCE workshop at ECML PKDD 2024. Please\n  find the link: https://ml4cce-ecml.com/#agenda",
      "pdf_url": "http://arxiv.org/pdf/2408.15866v1",
      "published_date": "2024-08-28 15:33:47 UTC",
      "updated_date": "2024-08-28 15:33:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:29:05.809684"
    },
    {
      "arxiv_id": "2408.15865v1",
      "title": "microYOLO: Towards Single-Shot Object Detection on Microcontrollers",
      "title_zh": "翻译失败",
      "authors": [
        "Mark Deutel",
        "Christopher Mutschler",
        "Jürgen Teich"
      ],
      "abstract": "This work-in-progress paper presents results on the feasibility of\nsingle-shot object detection on microcontrollers using YOLO. Single-shot object\ndetectors like YOLO are widely used, however due to their complexity mainly on\nlarger GPU-based platforms. We present microYOLO, which can be used on Cortex-M\nbased microcontrollers, such as the OpenMV H7 R2, achieving about 3.5 FPS when\nclassifying 128x128 RGB images while using less than 800 KB Flash and less than\n350 KB RAM. Furthermore, we share experimental results for three different\nobject detection tasks, analyzing the accuracy of microYOLO on them.",
      "tldr_zh": "这篇论文介绍了microYOLO，一种针对微控制器的单次对象检测(Single-Shot Object Detection)框架，旨在将YOLO算法移植到资源受限的设备上。microYOLO可在Cortex-M基于的微控制器（如OpenMV H7 R2）上运行，在处理128x128 RGB图像时实现约3.5 FPS的性能，同时仅需不到800 KB Flash和不到350 KB RAM。实验结果显示，该框架在三个不同对象检测任务中表现出良好的准确性，为嵌入式视觉应用提供了可行性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Published at the ECML PKDD Conference 2023, at the 4th Workshop on\n  IoT, Edge, and Mobile for Embedded Machine Learning",
      "pdf_url": "http://arxiv.org/pdf/2408.15865v1",
      "published_date": "2024-08-28 15:29:27 UTC",
      "updated_date": "2024-08-28 15:29:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:29:26.547680"
    },
    {
      "arxiv_id": "2409.09039v1",
      "title": "AutoGeo: Automating Geometric Image Dataset Creation for Enhanced Geometry Understanding",
      "title_zh": "翻译失败",
      "authors": [
        "Zihan Huang",
        "Tao Wu",
        "Wang Lin",
        "Shengyu Zhang",
        "Jingyuan Chen",
        "Fei Wu"
      ],
      "abstract": "With the rapid advancement of large language models, there has been a growing\ninterest in their capabilities in mathematical reasoning. However, existing\nresearch has primarily focused on text-based algebra problems, neglecting the\nstudy of geometry due to the lack of high-quality geometric datasets. To\naddress this gap, this paper introduces AutoGeo, a novel approach for\nautomatically generating mathematical geometric images to fulfill the demand\nfor large-scale and diverse geometric datasets. AutoGeo facilitates the\ncreation of AutoGeo-100k, an extensive repository comprising 100k high-quality\ngeometry image-text pairs. By leveraging precisely defined geometric clauses,\nAutoGeo-100k contains a wide variety of geometric shapes, including lines,\npolygons, circles, and complex spatial relationships, etc. Furthermore, this\npaper demonstrates the efficacy of AutoGeo-100k in enhancing the performance of\nmultimodal large language models through fine-tuning. Experimental results\nindicate significant improvements in the model's ability in handling geometric\nimages, as evidenced by enhanced accuracy in tasks such as geometric captioning\nand mathematical reasoning. This research not only fills a critical gap in the\navailability of geometric datasets but also paves the way for the advancement\nof sophisticated AI-driven tools in education and research. Project page:\nhttps://autogeo-official.github.io/.",
      "tldr_zh": "该研究指出，现有的 large language models 在数学推理方面主要关注文本-based 代数问题，而忽略了几何领域的问题，原因是缺乏高质量几何数据集。为解决这一空白，论文引入 AutoGeo，一种自动生成数学几何图像的方法，并创建了 AutoGeo-100k 数据集，包含 10 万对高质量几何图像-文本对，支持各种形状如线条、多边形和复杂空间关系。实验结果显示，通过 fine-tuning 多模态 large language models，该数据集显著提升了模型在几何图像处理方面的性能，包括几何描述和数学推理任务的准确性。该工作填补了几何数据集的缺口，并为 AI 在教育和研究中的应用铺平道路。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09039v1",
      "published_date": "2024-08-28 14:49:26 UTC",
      "updated_date": "2024-08-28 14:49:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:29:32.993084"
    },
    {
      "arxiv_id": "2408.15836v1",
      "title": "Knowledge Navigator: LLM-guided Browsing Framework for Exploratory Search in Scientific Literature",
      "title_zh": "翻译失败",
      "authors": [
        "Uri Katz",
        "Mosh Levy",
        "Yoav Goldberg"
      ],
      "abstract": "The exponential growth of scientific literature necessitates advanced tools\nfor effective knowledge exploration. We present Knowledge Navigator, a system\ndesigned to enhance exploratory search abilities by organizing and structuring\nthe retrieved documents from broad topical queries into a navigable, two-level\nhierarchy of named and descriptive scientific topics and subtopics. This\nstructured organization provides an overall view of the research themes in a\ndomain, while also enabling iterative search and deeper knowledge discovery\nwithin specific subtopics by allowing users to refine their focus and retrieve\nadditional relevant documents. Knowledge Navigator combines LLM capabilities\nwith cluster-based methods to enable an effective browsing method. We\ndemonstrate our approach's effectiveness through automatic and manual\nevaluations on two novel benchmarks, CLUSTREC-COVID and SCITOC. Our code,\nprompts, and benchmarks are made publicly available.",
      "tldr_zh": "本研究提出了 Knowledge Navigator，一种由 LLM（大型语言模型）引导的浏览框架，旨在提升科学文献的探索性搜索能力。该框架将从广泛主题查询中检索的文档组织成一个两级层次结构，包括命名和描述性的科学主题及子主题，从而提供领域研究主题的整体视图，并支持用户迭代搜索和深入知识发现。Knowledge Navigator 结合 LLM 能力和基于聚类的方法，在 CLUSTREC-COVID 和 SCITOC 两个新基准上通过自动和手动评估证明了其有效性；相关代码、提示和基准已公开可用。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.15836v1",
      "published_date": "2024-08-28 14:48:37 UTC",
      "updated_date": "2024-08-28 14:48:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:29:42.062910"
    },
    {
      "arxiv_id": "2408.15809v1",
      "title": "Object Detection for Vehicle Dashcams using Transformers",
      "title_zh": "使用 Transformer 的车辆行车记录仪物体检测",
      "authors": [
        "Osama Mustafa",
        "Khizer Ali",
        "Anam Bibi",
        "Imran Siddiqi",
        "Momina Moetesum"
      ],
      "abstract": "The use of intelligent automation is growing significantly in the automotive\nindustry, as it assists drivers and fleet management companies, thus increasing\ntheir productivity. Dash cams are now been used for this purpose which enables\nthe instant identification and understanding of multiple objects and\noccurrences in the surroundings. In this paper, we propose a novel approach for\nobject detection in dashcams using transformers. Our system is based on the\nstate-of-the-art DEtection TRansformer (DETR), which has demonstrated strong\nperformance in a variety of conditions, including different weather and\nillumination scenarios. The use of transformers allows for the consideration of\ncontextual information in decisionmaking, improving the accuracy of object\ndetection. To validate our approach, we have trained our DETR model on a\ndataset that represents real-world conditions. Our results show that the use of\nintelligent automation through transformers can significantly enhance the\ncapabilities of dashcam systems. The model achieves an mAP of 0.95 on\ndetection.",
      "tldr_zh": "本论文提出了一种基于 Transformers 的新型物体检测方法，针对车辆行车记录仪（dashcams）实现即时识别和理解周围物体。方法采用先进的 DEtection TRansformer (DETR) 模型，结合上下文信息决策，能够适应不同天气和光照条件，从而提升检测准确性。为验证效果，研究团队在真实世界数据集上训练模型，结果显示模型的 mean Average Precision (mAP) 达到 0.95，显著增强了行车记录仪的智能自动化能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "7 Pages, and 6 Figures",
      "pdf_url": "http://arxiv.org/pdf/2408.15809v1",
      "published_date": "2024-08-28 14:08:24 UTC",
      "updated_date": "2024-08-28 14:08:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:29:51.244222"
    },
    {
      "arxiv_id": "2408.15803v1",
      "title": "ModalityMirror: Improving Audio Classification in Modality Heterogeneity Federated Learning with Multimodal Distillation",
      "title_zh": "翻译失败",
      "authors": [
        "Tiantian Feng",
        "Tuo Zhang",
        "Salman Avestimehr",
        "Shrikanth S. Narayanan"
      ],
      "abstract": "Multimodal Federated Learning frequently encounters challenges of client\nmodality heterogeneity, leading to undesired performances for secondary\nmodality in multimodal learning. It is particularly prevalent in audiovisual\nlearning, with audio is often assumed to be the weaker modality in recognition\ntasks. To address this challenge, we introduce ModalityMirror to improve audio\nmodel performance by leveraging knowledge distillation from an audiovisual\nfederated learning model. ModalityMirror involves two phases: a modality-wise\nFL stage to aggregate uni-modal encoders; and a federated knowledge\ndistillation stage on multi-modality clients to train an unimodal student\nmodel. Our results demonstrate that ModalityMirror significantly improves the\naudio classification compared to the state-of-the-art FL methods such as\nHarmony, particularly in audiovisual FL facing video missing. Our approach\nunlocks the potential for exploiting the diverse modality spectrum inherent in\nmulti-modal FL.",
      "tldr_zh": "该研究针对多模态联邦学习（Multimodal Federated Learning）中客户端模态异质性问题，提出ModalityMirror方法，通过多模态蒸馏（Multimodal Distillation）提升音频分类性能，特别是当音频作为次要模态时。ModalityMirror包括两个阶段：首先进行模态-wise FL阶段来聚合单模态编码器，然后在多模态客户端上执行联邦知识蒸馏（Federated Knowledge Distillation）以训练单模态学生模型。实验结果显示，与Harmony等现有方法相比，ModalityMirror显著提高了音频分类准确率，尤其在视频缺失的视音频FL场景中，并释放了多模态FL中多样模态谱的潜力。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.15803v1",
      "published_date": "2024-08-28 13:56:22 UTC",
      "updated_date": "2024-08-28 13:56:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:30:05.392909"
    },
    {
      "arxiv_id": "2408.15800v1",
      "title": "Emulating Brain-like Rapid Learning in Neuromorphic Edge Computing",
      "title_zh": "翻译失败",
      "authors": [
        "Kenneth Stewart",
        "Michael Neumeier",
        "Sumit Bam Shrestha",
        "Garrick Orchard",
        "Emre Neftci"
      ],
      "abstract": "Achieving personalized intelligence at the edge with real-time learning\ncapabilities holds enormous promise in enhancing our daily experiences and\nhelping decision making, planning, and sensing. However, efficient and reliable\nedge learning remains difficult with current technology due to the lack of\npersonalized data, insufficient hardware capabilities, and inherent challenges\nposed by online learning.\n  Over time and across multiple developmental stages, the brain has evolved to\nefficiently incorporate new knowledge by gradually building on previous\nknowledge. In this work, we emulate the multiple stages of learning with\ndigital neuromorphic technology that simulates the neural and synaptic\nprocesses of the brain using two stages of learning. First, a meta-training\nstage trains the hyperparameters of synaptic plasticity for one-shot learning\nusing a differentiable simulation of the neuromorphic hardware. This\nmeta-training process refines a hardware local three-factor synaptic plasticity\nrule and its associated hyperparameters to align with the trained task domain.\nIn a subsequent deployment stage, these optimized hyperparameters enable fast,\ndata-efficient, and accurate learning of new classes. We demonstrate our\napproach using event-driven vision sensor data and the Intel Loihi neuromorphic\nprocessor with its plasticity dynamics, achieving real-time one-shot learning\nof new classes that is vastly improved over transfer learning. Our methodology\ncan be deployed with arbitrary plasticity models and can be applied to\nsituations demanding quick learning and adaptation at the edge, such as\nnavigating unfamiliar environments or learning unexpected categories of data\nthrough user engagement.",
      "tldr_zh": "该研究旨在通过神经形态边缘计算（Neuromorphic Edge Computing）模拟大脑-like快速学习，实现实时个性化智能，以提升决策和感知能力。方法包括两个阶段：首先，进行元训练（meta-training）来优化突触可塑性（synaptic plasticity）的超参数，使用可微模拟的硬件规则，支持一-shot learning；其次，在部署阶段，利用这些超参数实现快速、数据高效的新类别学习。实验使用事件驱动视觉传感器数据和 Intel Loihi 处理器，证明该方法在实时学习中大幅超越转移学习（transfer learning），适用于边缘场景如导航未知环境或用户互动学习新数据。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "17 page journal article. Submitted to IOP NCE",
      "pdf_url": "http://arxiv.org/pdf/2408.15800v1",
      "published_date": "2024-08-28 13:51:52 UTC",
      "updated_date": "2024-08-28 13:51:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:30:17.086505"
    },
    {
      "arxiv_id": "2408.15796v2",
      "title": "Evaluating Named Entity Recognition Using Few-Shot Prompting with Large Language Models",
      "title_zh": "使用大语言模型的少样本提示评估命名实体识别",
      "authors": [
        "Hédi Zeghidi",
        "Ludovic Moncla"
      ],
      "abstract": "This paper evaluates Few-Shot Prompting with Large Language Models for Named\nEntity Recognition (NER). Traditional NER systems rely on extensive labeled\ndatasets, which are costly and time-consuming to obtain. Few-Shot Prompting or\nin-context learning enables models to recognize entities with minimal examples.\nWe assess state-of-the-art models like GPT-4 in NER tasks, comparing their\nfew-shot performance to fully supervised benchmarks. Results show that while\nthere is a performance gap, large models excel in adapting to new entity types\nand domains with very limited data. We also explore the effects of prompt\nengineering, guided output format and context length on performance. This study\nunderscores Few-Shot Learning's potential to reduce the need for large labeled\ndatasets, enhancing NER scalability and accessibility.",
      "tldr_zh": "这篇论文评估了使用 Few-Shot Prompting 与大型语言模型（如 GPT-4）进行 Named Entity Recognition (NER)，旨在减少对大量标注数据集的依赖。研究比较了这些模型在少样本场景下的性能与传统完全监督基准，结果显示虽然存在性能差距，但大型模型在适应新实体类型和领域时表现出色。论文还探讨了提示工程、引导输出格式和上下文长度对性能的影响。总体而言，该研究突出了 Few-Shot Learning 在提升 NER 的可扩展性和可访问性方面的潜力。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Github repo: https://github.com/GEODE-project/ner-llm",
      "pdf_url": "http://arxiv.org/pdf/2408.15796v2",
      "published_date": "2024-08-28 13:42:28 UTC",
      "updated_date": "2024-09-04 06:36:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:30:30.280014"
    },
    {
      "arxiv_id": "2409.00122v1",
      "title": "Brant-X: A Unified Physiological Signal Alignment Framework",
      "title_zh": "Brant-X：一个统一的生理信号对齐框架",
      "authors": [
        "Daoze Zhang",
        "Zhizhang Yuan",
        "Junru Chen",
        "Kerui Chen",
        "Yang Yang"
      ],
      "abstract": "Physiological signals serve as indispensable clues for understanding various\nphysiological states of human bodies. Most existing works have focused on a\nsingle type of physiological signals for a range of application scenarios.\nHowever, as the body is a holistic biological system, the inherent\ninterconnection among various physiological data should not be neglected. In\nparticular, given the brain's role as the control center for vital activities,\nelectroencephalogram (EEG) exhibits significant correlations with other\nphysiological signals. Therefore, the correlation between EEG and other\nphysiological signals holds potential to improve performance in various\nscenarios. Nevertheless, achieving this goal is still constrained by several\nchallenges: the scarcity of simultaneously collected physiological data, the\ndifferences in correlations between various signals, and the correlation\ndifferences between various tasks. To address these issues, we propose a\nunified physiological signal alignment framework, Brant-X, to model the\ncorrelation between EEG and other signals. Our approach (1) employs the EEG\nfoundation model to data-efficiently transfer the rich knowledge in EEG to\nother physiological signals, and (2) introduces the two-level alignment to\nfully align the semantics of EEG and other signals from different semantic\nscales. In the experiments, Brant-X achieves state-of-the-art performance\ncompared with task-agnostic and task-specific baselines on various downstream\ntasks in diverse scenarios, including sleep stage classification, emotion\nrecognition, freezing of gaits detection, and eye movement communication.\nMoreover, the analysis on the arrhythmia detection task and the visualization\nin case study further illustrate the effectiveness of Brant-X in the knowledge\ntransfer from EEG to other physiological signals. The model's homepage is at\nhttps://github.com/zjunet/Brant-X/.",
      "tldr_zh": "该研究指出，生理信号之间存在内在互连，尤其是 EEG 与其他信号的相关性，但受数据稀缺和相关性差异等挑战限制。作者提出 Brant-X 框架，使用 EEG 基础模型高效转移知识到其他生理信号，并引入 two-level alignment 方法，从不同语义尺度对齐信号语义。在实验中，Brant-X 在睡眠阶段分类、情绪识别等下游任务上超越基线模型，证明了其在知识转移方面的有效性，并为多信号整合应用提供了新途径。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "Accepted by SIGKDD 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.00122v1",
      "published_date": "2024-08-28 13:26:42 UTC",
      "updated_date": "2024-08-28 13:26:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:30:42.074153"
    },
    {
      "arxiv_id": "2408.15778v4",
      "title": "LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Jiayi Gui",
        "Yiming Liu",
        "Jiale Cheng",
        "Xiaotao Gu",
        "Xiao Liu",
        "Hongning Wang",
        "Yuxiao Dong",
        "Jie Tang",
        "Minlie Huang"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated notable capabilities across\nvarious tasks, showcasing complex problem-solving abilities. Understanding and\nexecuting complex rules, along with multi-step planning, are fundamental to\nlogical reasoning and critical for practical LLM agents and decision-making\nsystems. However, evaluating LLMs as effective rule-based executors and\nplanners remains underexplored. In this paper, we introduce LogicGame, a novel\nbenchmark designed to evaluate the comprehensive rule understanding, execution,\nand planning capabilities of LLMs. Unlike traditional benchmarks, LogicGame\nprovides diverse games that contain a series of rules with an initial state,\nrequiring models to comprehend and apply predefined regulations to solve\nproblems. We create simulated scenarios in which models execute or plan\noperations to achieve specific outcomes. These game scenarios are specifically\ndesigned to distinguish logical reasoning from mere knowledge by relying\nexclusively on predefined rules. This separation allows for a pure assessment\nof rule-based reasoning capabilities. The evaluation considers not only final\noutcomes but also intermediate steps, providing a comprehensive assessment of\nmodel performance. Moreover, these intermediate steps are deterministic and can\nbe automatically verified. LogicGame defines game scenarios with varying\ndifficulty levels, from simple rule applications to complex reasoning chains,\nin order to offer a precise evaluation of model performance on rule\nunderstanding and multi-step execution. Utilizing LogicGame, we test various\nLLMs and identify notable shortcomings in their rule-based logical reasoning\nabilities.",
      "tldr_zh": "这篇论文引入了 LogicGame，这是一个新基准，用于评估大型语言模型 (LLMs) 在规则理解、执行和规划方面的能力。LogicGame 通过设计多样化的游戏场景，这些场景基于预定义规则，要求模型在模拟环境中应用规则来解决问题，从而纯粹测试规则推理而非知识依赖。评估不仅关注最终结果，还包括可自动验证的中间步骤，并通过不同难度级别（如简单规则应用到复杂推理链）进行全面测试；结果显示，各种 LLMs 在规则逻辑推理方面存在显著缺陷。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.15778v4",
      "published_date": "2024-08-28 13:16:41 UTC",
      "updated_date": "2024-10-12 11:00:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:30:54.745937"
    },
    {
      "arxiv_id": "2408.15775v2",
      "title": "Easy, Interpretable, Effective: openSMILE for voice deepfake detection",
      "title_zh": "翻译失败",
      "authors": [
        "Octavian Pascu",
        "Dan Oneata",
        "Horia Cucu",
        "Nicolas M. Müller"
      ],
      "abstract": "In this paper, we demonstrate that attacks in the latest ASVspoof5 dataset --\na de facto standard in the field of voice authenticity and deepfake detection\n-- can be identified with surprising accuracy using a small subset of very\nsimplistic features. These are derived from the openSMILE library, and are\nscalar-valued, easy to compute, and human interpretable. For example, attack\nA10`s unvoiced segments have a mean length of 0.09 +- 0.02, while bona fide\ninstances have a mean length of 0.18 +- 0.07. Using this feature alone, a\nthreshold classifier achieves an Equal Error Rate (EER) of 10.3% for attack\nA10. Similarly, across all attacks, we achieve up to 0.8% EER, with an overall\nEER of 15.7 +- 6.0%. We explore the generalization capabilities of these\nfeatures and find that some of them transfer effectively between attacks,\nprimarily when the attacks originate from similar Text-to-Speech (TTS)\narchitectures. This finding may indicate that voice anti-spoofing is, in part,\na problem of identifying and remembering signatures or fingerprints of\nindividual TTS systems. This allows to better understand anti-spoofing models\nand their challenges in real-world application.",
      "tldr_zh": "本研究展示了使用 openSMILE 库的简单标量特征来检测 ASVspoof5 数据集中的语音深度伪造攻击，这些特征易计算且易于人类解释，例如未发声段的平均长度。针对特定攻击如 A10，仅用单一特征即可实现 10.3% 的 Equal Error Rate (EER)，而整体上达到最高 0.8% EER 和平均 15.7% ± 6.0% EER。实验还发现，这些特征在类似 Text-to-Speech (TTS) 架构的攻击间具有良好泛化能力，表明语音防伪问题部分依赖于识别 TTS 系统的签名，从而有助于理解防伪模型在实际应用中的挑战。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.15775v2",
      "published_date": "2024-08-28 13:14:18 UTC",
      "updated_date": "2024-08-29 11:58:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:31:08.241609"
    },
    {
      "arxiv_id": "2408.15769v1",
      "title": "A Survey on Evaluation of Multimodal Large Language Models",
      "title_zh": "多模态大语言模型评估的综述",
      "authors": [
        "Jiaxing Huang",
        "Jingyi Zhang"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) mimic human perception and reasoning\nsystem by integrating powerful Large Language Models (LLMs) with various\nmodality encoders (e.g., vision, audio), positioning LLMs as the \"brain\" and\nvarious modality encoders as sensory organs. This framework endows MLLMs with\nhuman-like capabilities, and suggests a potential pathway towards achieving\nartificial general intelligence (AGI). With the emergence of all-round MLLMs\nlike GPT-4V and Gemini, a multitude of evaluation methods have been developed\nto assess their capabilities across different dimensions. This paper presents a\nsystematic and comprehensive review of MLLM evaluation methods, covering the\nfollowing key aspects: (1) the background of MLLMs and their evaluation; (2)\n\"what to evaluate\" that reviews and categorizes existing MLLM evaluation tasks\nbased on the capabilities assessed, including general multimodal recognition,\nperception, reasoning and trustworthiness, and domain-specific applications\nsuch as socioeconomic, natural sciences and engineering, medical usage, AI\nagent, remote sensing, video and audio processing, 3D point cloud analysis, and\nothers; (3) \"where to evaluate\" that summarizes MLLM evaluation benchmarks into\ngeneral and specific benchmarks; (4) \"how to evaluate\" that reviews and\nillustrates MLLM evaluation steps and metrics; Our overarching goal is to\nprovide valuable insights for researchers in the field of MLLM evaluation,\nthereby facilitating the development of more capable and reliable MLLMs. We\nemphasize that evaluation should be regarded as a critical discipline,\nessential for advancing the field of MLLMs.",
      "tldr_zh": "这篇论文对多模态大语言模型 (MLLMs) 的评估方法进行了系统回顾，涵盖了 MLLMs 的背景、整合 Large Language Models (LLMs) 与各种模态编码器（如视觉和音频）的框架，以及其在实现人工通用智能 (AGI) 中的潜力。论文将评估任务分类为“what to evaluate”（包括一般多模态识别、感知、推理和可信度，以及特定领域如社会经济、医疗和 AI 代理）、“where to evaluate”（一般和特定基准）和“how to evaluate”（评估步骤和指标）。总体而言，该调查为研究者提供了宝贵见解，促进更强大和可靠的 MLLMs 发展，并强调评估作为一门关键学科的重要性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.15769v1",
      "published_date": "2024-08-28 13:05:55 UTC",
      "updated_date": "2024-08-28 13:05:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:31:19.238874"
    },
    {
      "arxiv_id": "2408.15751v2",
      "title": "Reinforcement Learning for Adaptive Traffic Signal Control: Turn-Based and Time-Based Approaches to Reduce Congestion",
      "title_zh": "翻译失败",
      "authors": [
        "Muhammad Tahir Rafique",
        "Ahmed Mustafa",
        "Hasan Sajid"
      ],
      "abstract": "The growing demand for road use in urban areas has led to significant traffic\ncongestion, posing challenges that are costly to mitigate through\ninfrastructure expansion alone. As an alternative, optimizing existing traffic\nmanagement systems, particularly through adaptive traffic signal control,\noffers a promising solution. This paper explores the use of Reinforcement\nLearning (RL) to enhance traffic signal operations at intersections, aiming to\nreduce congestion without extensive sensor networks. We introduce two RL-based\nalgorithms: a turn-based agent, which dynamically prioritizes traffic signals\nbased on real-time queue lengths, and a time-based agent, which adjusts signal\nphase durations according to traffic conditions while following a fixed phase\ncycle. By representing the state as a scalar queue length, our approach\nsimplifies the learning process and lowers deployment costs. The algorithms\nwere tested in four distinct traffic scenarios using seven evaluation metrics\nto comprehensively assess performance. Simulation results demonstrate that both\nRL algorithms significantly outperform conventional traffic signal control\nsystems, highlighting their potential to improve urban traffic flow\nefficiently.",
      "tldr_zh": "本研究针对城市交通拥堵问题，提出使用Reinforcement Learning (RL)优化交通信号控制，以提高现有系统的效率，而非依赖基础设施扩展。研究引入两种RL算法：turn-based agent，根据实时队列长度动态调整信号优先级；以及time-based agent，根据交通条件修改信号相位持续时间，但保持固定相位周期。通过将状态简化为标量队列长度，该方法简化了学习过程并降低了部署成本。在四个不同交通场景的模拟中，使用七个评价指标评估，结果显示两种RL算法显著优于传统系统，提升了交通流量效率。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 8 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2408.15751v2",
      "published_date": "2024-08-28 12:35:56 UTC",
      "updated_date": "2024-09-01 17:32:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:31:30.253543"
    },
    {
      "arxiv_id": "2409.00121v1",
      "title": "BELT-2: Bootstrapping EEG-to-Language representation alignment for multi-task brain decoding",
      "title_zh": "翻译失败",
      "authors": [
        "Jinzhao Zhou",
        "Yiqun Duan",
        "Fred Chang",
        "Thomas Do",
        "Yu-Kai Wang",
        "Chin-Teng Lin"
      ],
      "abstract": "The remarkable success of large language models (LLMs) across various\nmulti-modality applications is well established. However, integrating large\nlanguage models with humans, or brain dynamics, remains relatively unexplored.\nIn this paper, we introduce BELT-2, a pioneering multi-task model designed to\nenhance both encoding and decoding performance from EEG signals. To bolster the\nquality of the EEG encoder, BELT-2 is the first work to innovatively 1) adopt\nbyte-pair encoding (BPE)-level EEG-language alignment and 2) integrate\nmulti-task training and decoding in the EEG domain. Inspired by the idea of\n\\textbf{\\textit{Bridging the Brain with GPT}}, we further connect the\nmulti-task EEG encoder with LLMs by utilizing prefix-tuning on intermediary\noutput from the EEG encoder. These innovative efforts make BELT-2 a pioneering\nbreakthrough, making it the first work in the field capable of decoding\ncoherent and readable sentences from non-invasive brain signals. Our\nexperiments highlight significant advancements over prior techniques in both\nquantitative and qualitative measures, achieving a decoding performance with a\nBLEU-1 score of 52.2\\% on the ZuCo dataset. Furthermore, BELT-2 shows a\nremarkable improvement ranging from 31\\% to 162\\% on other translation\nbenchmarks. Codes can be accessed via the provided anonymous\nlink~\\footnote{https://anonymous.4open.science/r/BELT-2-0048}.",
      "tldr_zh": "本研究引入 BELT-2，一种多任务模型，用于提升 EEG 信号的编码和解码性能，首次实现从非侵入性脑信号中解码出连贯可读的句子。BELT-2 创新性地采用 byte-pair encoding (BPE)-level EEG-language alignment，并整合 multi-task training 和 decoding，以增强 EEG 编码器的质量。受 Bridging the Brain with GPT 启发，该模型通过 prefix-tuning 将 EEG 编码器与 large language models (LLMs) 连接，实现高效的脑-语言交互。在实验中，BELT-2 在 ZuCo 数据集上达到 BLEU-1 得分 52.2%，并在其他翻译基准上实现 31% 到 162% 的显著改善。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.00121v1",
      "published_date": "2024-08-28 12:30:22 UTC",
      "updated_date": "2024-08-28 12:30:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:31:44.028737"
    },
    {
      "arxiv_id": "2408.15722v1",
      "title": "Advanced POD-Based Performance Evaluation of Classifiers Applied to Human Driver Lane Changing Prediction",
      "title_zh": "先进的基于 POD 的分类器性能评估，应用于人类驾驶员变道预测",
      "authors": [
        "Zahra Rastin",
        "Dirk Söffker"
      ],
      "abstract": "Machine learning (ML) classifiers serve as essential tools facilitating\nclassification and prediction across various domains. The performance of these\nalgorithms should be known to ensure their reliable application. In certain\nfields, receiver operating characteristic and precision-recall curves are\nfrequently employed to assess machine learning algorithms without accounting\nfor the impact of process parameters. However, it may be essential to evaluate\nthe performance of these algorithms in relation to such parameters. As a\nperformance evaluation metric capable of considering the effects of process\nparameters, this paper uses a modified probability of detection (POD) approach\nto assess the reliability of ML-based algorithms. As an example, the POD-based\napproach is employed to assess ML models used for predicting the lane changing\nbehavior of a vehicle driver. The time remaining to the predicted (and\ntherefore unknown) lane changing event is considered as process parameter. The\nhit/miss approach to POD is taken here and modified by considering the\nprobability of lane changing derived from ML algorithms at each time step, and\nobtaining the final result of the analysis accordingly. This improves the\nreliability of results compared to the standard hit/miss approach, which\nconsiders the outcome of the classifiers as either 0 or 1, while also\nsimplifying evaluation compared to the \\^a versus a approach. Performance\nevaluation results of the proposed approach are compared with those obtained\nwith the standard hit/miss approach and a pre-developed \\^a versus a approach\nto validate the effectiveness of the proposed method. The comparison shows that\nthis method provides an averaging conservative behavior with the advantage of\nenhancing the reliability of the hit/miss approach to POD while retaining its\nsimplicity.",
      "tldr_zh": "这篇论文提出了一种改进的 Probability of Detection (POD) 方法，用于评估 Machine Learning (ML) 分类器的性能，特别考虑了过程参数的影响，以弥补传统 Receiver Operating Characteristic 和 Precision-Recall 曲线的不足。  \n作为示例，该方法应用于预测人类驾驶员的变道行为，将剩余时间作为过程参数，并修改了标准的 hit/miss 方式，通过整合 ML 算法的概率输出来提升评估的可靠性和简化过程。  \n结果显示，该方法在比较中表现出保守的平均行为，比传统 hit/miss 和 \\^a versus a 方法更可靠，同时保留了简单性。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "Manuscript: 8 pages, 6 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2408.15722v1",
      "published_date": "2024-08-28 11:39:24 UTC",
      "updated_date": "2024-08-28 11:39:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:32:05.119735"
    },
    {
      "arxiv_id": "2409.07476v1",
      "title": "Responsible AI for Test Equity and Quality: The Duolingo English Test as a Case Study",
      "title_zh": "翻译失败",
      "authors": [
        "Jill Burstein",
        "Geoffrey T. LaFlair",
        "Kevin Yancey",
        "Alina A. von Davier",
        "Ravit Dotan"
      ],
      "abstract": "Artificial intelligence (AI) creates opportunities for assessments, such as\nefficiencies for item generation and scoring of spoken and written responses.\nAt the same time, it poses risks (such as bias in AI-generated item content).\nResponsible AI (RAI) practices aim to mitigate risks associated with AI. This\nchapter addresses the critical role of RAI practices in achieving test quality\n(appropriateness of test score inferences), and test equity (fairness to all\ntest takers). To illustrate, the chapter presents a case study using the\nDuolingo English Test (DET), an AI-powered, high-stakes English language\nassessment. The chapter discusses the DET RAI standards, their development and\ntheir relationship to domain-agnostic RAI principles. Further, it provides\nexamples of specific RAI practices, showing how these practices meaningfully\naddress the ethical principles of validity and reliability, fairness, privacy\nand security, and transparency and accountability standards to ensure test\nequity and quality.",
      "tldr_zh": "本研究探讨了Responsible AI (RAI) 在提升测试公平性和质量方面的作用，以Duolingo English Test (DET) 为案例研究。论文分析了AI在评估中的机会（如项目生成和评分效率）与风险（如内容偏见），并强调RAI实践如何缓解这些风险。作者详细介绍了DET的RAI标准及其开发过程，包括与通用RAI原则的关联，并通过具体示例展示了这些实践如何确保有效性、可靠性、公平性、隐私与安全、透明性和问责性，从而实现高风险英语语言评估的公平与质量。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.07476v1",
      "published_date": "2024-08-28 11:39:20 UTC",
      "updated_date": "2024-08-28 11:39:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:32:06.024086"
    },
    {
      "arxiv_id": "2409.00120v2",
      "title": "ConCSE: Unified Contrastive Learning and Augmentation for Code-Switched Embeddings",
      "title_zh": "Con",
      "authors": [
        "Jangyeong Jeon",
        "Sangyeon Cho",
        "Minuk Ma",
        "Junyoung Kim"
      ],
      "abstract": "This paper examines the Code-Switching (CS) phenomenon where two languages\nintertwine within a single utterance. There exists a noticeable need for\nresearch on the CS between English and Korean. We highlight that the current\nEquivalence Constraint (EC) theory for CS in other languages may only partially\ncapture English-Korean CS complexities due to the intrinsic grammatical\ndifferences between the languages. We introduce a novel Koglish dataset\ntailored for English-Korean CS scenarios to mitigate such challenges. First, we\nconstructed the Koglish-GLUE dataset to demonstrate the importance and need for\nCS datasets in various tasks. We found the differential outcomes of various\nfoundation multilingual language models when trained on a monolingual versus a\nCS dataset. Motivated by this, we hypothesized that SimCSE, which has shown\nstrengths in monolingual sentence embedding, would have limitations in CS\nscenarios. We construct a novel Koglish-NLI (Natural Language Inference)\ndataset using a CS augmentation-based approach to verify this. From this\nCS-augmented dataset Koglish-NLI, we propose a unified contrastive learning and\naugmentation method for code-switched embeddings, ConCSE, highlighting the\nsemantics of CS sentences. Experimental results validate the proposed ConCSE\nwith an average performance enhancement of 1.77\\% on the Koglish-STS(Semantic\nTextual Similarity) tasks.",
      "tldr_zh": "这篇论文探讨了英语和韩语的代码切换（Code-Switching, CS）现象，指出现有 Equivalence Constraint (EC) 理论因语言语法差异而无法完全捕捉其复杂性，并引入了针对性强的 Koglish 数据集来解决这一挑战。\n\n研究者构建了 Koglish-GLUE 和 Koglish-NLI 数据集，通过比较多语言模型在单语与 CS 数据集上的表现，验证了 SimCSE 在 CS 场景下的局限性。\n\n为此，他们提出了 ConCSE 方法，这是一种统一的对比学习和增强技术，用于优化代码切换嵌入的语义表示。\n\n实验结果显示，ConCSE 在 Koglish-STS（Semantic Textual Similarity）任务上实现了平均性能提升 1.77%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted for oral presentation at ICPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.00120v2",
      "published_date": "2024-08-28 11:27:21 UTC",
      "updated_date": "2024-12-20 07:58:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:32:22.137791"
    },
    {
      "arxiv_id": "2408.15702v1",
      "title": "Evaluating Model Robustness Using Adaptive Sparse L0 Regularization",
      "title_zh": "翻译失败",
      "authors": [
        "Weiyou Liu",
        "Zhenyang Li",
        "Weitong Chen"
      ],
      "abstract": "Deep Neural Networks have demonstrated remarkable success in various domains\nbut remain susceptible to adversarial examples, which are slightly altered\ninputs designed to induce misclassification. While adversarial attacks\ntypically optimize under Lp norm constraints, attacks based on the L0 norm,\nprioritising input sparsity, are less studied due to their complex and non\nconvex nature. These sparse adversarial examples challenge existing defenses by\naltering a minimal subset of features, potentially uncovering more subtle DNN\nweaknesses. However, the current L0 norm attack methodologies face a trade off\nbetween accuracy and efficiency either precise but computationally intense or\nexpedient but imprecise. This paper proposes a novel, scalable, and effective\napproach to generate adversarial examples based on the L0 norm, aimed at\nrefining the robustness evaluation of DNNs against such perturbations.",
      "tldr_zh": "该论文探讨了深度神经网络（Deep Neural Networks）的鲁棒性问题，特别是在面对基于 L0 范数的对抗样本（adversarial examples）攻击时，这些攻击通过最小化特征改变来暴露 DNN 的弱点。现有 L0 攻击方法在准确性和效率之间存在权衡。作者提出了一种新颖的 Adaptive Sparse L0 Regularization 技术，用于生成更可扩展且有效的 L0 范数对抗样本，从而改进对 DNN 鲁棒性的评估。实验结果表明，该方法有助于揭示模型的潜在漏洞，为增强防御策略提供参考。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "F.2.2, I.2.7"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by the 20th International Conference on Advanced Data Mining\n  and Applications (ADMA 2024)",
      "pdf_url": "http://arxiv.org/pdf/2408.15702v1",
      "published_date": "2024-08-28 11:02:23 UTC",
      "updated_date": "2024-08-28 11:02:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:32:30.349908"
    },
    {
      "arxiv_id": "2408.15695v2",
      "title": "G-Style: Stylized Gaussian Splatting",
      "title_zh": "翻译失败",
      "authors": [
        "Áron Samuel Kovács",
        "Pedro Hermosilla",
        "Renata G. Raidou"
      ],
      "abstract": "We introduce G-Style, a novel algorithm designed to transfer the style of an\nimage onto a 3D scene represented using Gaussian Splatting. Gaussian Splatting\nis a powerful 3D representation for novel view synthesis, as -- compared to\nother approaches based on Neural Radiance Fields -- it provides fast scene\nrenderings and user control over the scene. Recent pre-prints have demonstrated\nthat the style of Gaussian Splatting scenes can be modified using an image\nexemplar. However, since the scene geometry remains fixed during the\nstylization process, current solutions fall short of producing satisfactory\nresults. Our algorithm aims to address these limitations by following a\nthree-step process: In a pre-processing step, we remove undesirable Gaussians\nwith large projection areas or highly elongated shapes. Subsequently, we\ncombine several losses carefully designed to preserve different scales of the\nstyle in the image, while maintaining as much as possible the integrity of the\noriginal scene content. During the stylization process and following the\noriginal design of Gaussian Splatting, we split Gaussians where additional\ndetail is necessary within our scene by tracking the gradient of the stylized\ncolor. Our experiments demonstrate that G-Style generates high-quality\nstylizations within just a few minutes, outperforming existing methods both\nqualitatively and quantitatively.",
      "tldr_zh": "该研究提出 G-Style，一种新型算法，用于将图像风格转移到使用 Gaussian Splatting 表示的 3D 场景中，以提升新型视图合成的渲染速度和用户控制能力，同时解决现有方法因固定场景几何而导致的风格化效果不佳问题。G-Style 通过三步过程实现：预处理移除投影面积大或形状不规则的 Gaussians；结合多种损失函数保留图像的风格细节，同时保持原场景内容完整性；在风格化过程中，通过跟踪梯度动态分割 Gaussians 以添加必要细节。实验结果显示，G-Style 在几分钟内生成高质量风格化输出，在定性和定量指标上均优于现有方法。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.15695v2",
      "published_date": "2024-08-28 10:43:42 UTC",
      "updated_date": "2024-09-05 09:05:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:32:52.477198"
    },
    {
      "arxiv_id": "2408.16032v1",
      "title": "An Extremely Data-efficient and Generative LLM-based Reinforcement Learning Agent for Recommenders",
      "title_zh": "翻译失败",
      "authors": [
        "Shuang Feng",
        "Grace Feng"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have enabled\nunderstanding webpage contexts, product details, and human instructions.\nUtilizing LLMs as the foundational architecture for either reward models or\npolicies in reinforcement learning has gained popularity -- a notable\nachievement is the success of InstructGPT. RL algorithms have been instrumental\nin maximizing long-term customer satisfaction and avoiding short-term, myopic\ngoals in industrial recommender systems, which often rely on deep learning\nmodels to predict immediate clicks or purchases.\n  In this project, several RL methods are implemented and evaluated using the\nWebShop benchmark environment, data, simulator, and pre-trained model\ncheckpoints. The goal is to train an RL agent to maximize the purchase reward\ngiven a detailed human instruction describing a desired product. The RL agents\nare developed by fine-tuning a pre-trained BERT model with various objectives,\nlearning from preferences without a reward model, and employing contemporary\ntraining techniques such as Proximal Policy Optimization (PPO) as used in\nInstructGPT, and Direct Preference Optimization (DPO). This report also\nevaluates the RL agents trained using generative trajectories. Evaluations were\nconducted using Thompson sampling in the WebShop simulator environment.\n  The simulated online experiments demonstrate that agents trained on generated\ntrajectories exhibited comparable task performance to those trained using human\ntrajectories. This has demonstrated an example of an extremely low-cost\ndata-efficient way of training reinforcement learning agents. Also, with\nlimited training time (<2hours), without utilizing any images, a DPO agent\nachieved a 19% success rate after approximately 3000 steps or 30 minutes of\ntraining on T4 GPUs, compared to a PPO agent, which reached a 15% success rate.",
      "tldr_zh": "本研究提出了一种基于大型语言模型 (LLMs) 的数据高效生成式强化学习 (RL) 代理，用于推荐系统，旨在最大化长期客户满意度。研究在 WebShop 基准环境中实现了多种 RL 方法，包括微调预训练 BERT 模型、Direct Preference Optimization (DPO) 和 Proximal Policy Optimization (PPO)，并使用生成轨迹进行训练，以减少对人类数据的需求。实验结果显示，使用生成轨迹的代理性能与人类轨迹相当，且 DPO 代理在不到 2 小时的训练中达到 19% 的成功率，相比 PPO 的 15%，证明了这种方法的极高数据效率和低成本潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.16032v1",
      "published_date": "2024-08-28 10:31:50 UTC",
      "updated_date": "2024-08-28 10:31:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:32:55.867028"
    },
    {
      "arxiv_id": "2408.16031v1",
      "title": "EMP: Enhance Memory in Data Pruning",
      "title_zh": "EMP：在数据修剪中增强记忆",
      "authors": [
        "Jinying Xiao",
        "Ping Li",
        "Jie Nie",
        "Zhe Tang"
      ],
      "abstract": "Recently, large language and vision models have shown strong performance, but\ndue to high pre-training and fine-tuning costs, research has shifted towards\nfaster training via dataset pruning. Previous methods used sample loss as an\nevaluation criterion, aiming to select the most \"difficult\" samples for\ntraining. However, when the pruning rate increases, the number of times each\nsample is trained becomes more evenly distributed, which causes many critical\nor general samples to not be effectively fitted. We refer to this as\nLow-Frequency Learning (LFL). In other words, LFL prevents the model from\nremembering most samples. In our work, we decompose the scoring function of\nLFL, provide a theoretical explanation for the inefficiency of LFL, and propose\nadding a memory term to the scoring function to enhance the model's memory\ncapability, along with an approximation of this memory term. Similarly, we\nexplore memory in Self-Supervised Learning (SSL), marking the first discussion\non SSL memory. Using contrastive learning, we derive the memory term both\ntheoretically and experimentally. Finally, we propose Enhance Memory Pruning\n(EMP), which addresses the issue of insufficient memory under high pruning\nrates by enhancing the model's memory of data, thereby improving its\nperformance. We evaluated the performance of EMP in tasks such as image\nclassification, natural language understanding, and model pre-training. The\nresults show that EMP can improve model performance under extreme pruning\nrates. For example, in the CIFAR100-ResNet50 pre-training task, with 70\\%\npruning, EMP outperforms current methods by 2.2\\%.",
      "tldr_zh": "该论文针对数据修剪中的Low-Frequency Learning (LFL)问题提出Enhance Memory Pruning (EMP)方法，以解决高修剪率下模型无法有效记忆关键样本的问题。作者分解LFL的评分函数，提供理论解释，并通过添加记忆项及其近似来增强模型的记忆能力，同时首次探讨Self-Supervised Learning (SSL)中的记忆机制，并使用对比学习进行理论和实验验证。实验结果显示，EMP在图像分类、自然语言理解和模型预训练任务中表现出色，例如在CIFAR100-ResNet50预训练任务中，70%修剪率下比现有方法提高2.2%。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.16031v1",
      "published_date": "2024-08-28 10:29:52 UTC",
      "updated_date": "2024-08-28 10:29:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:33:08.144994"
    },
    {
      "arxiv_id": "2408.16030v2",
      "title": "Deep Learning-Based Automatic Multi-Level Airway Collapse Monitoring on Obstructive Sleep Apnea Patients",
      "title_zh": "翻译失败",
      "authors": [
        "Ying-Chieh Hsu",
        "Stanley Yung-Chuan Liu",
        "Chao-Jung Huang",
        "Chi-Wei Wu",
        "Ren-Kai Cheng",
        "Jane Yung-Jen Hsu",
        "Shang-Ran Huang",
        "Yuan-Ren Cheng",
        "Fu-Shun Hsu"
      ],
      "abstract": "This study investigated the use of deep learning to identify multi-level\nupper airway collapses in obstructive sleep apnea (OSA) patients based on\nsnoring sounds. We fi-ne-tuned ResNet-50 and Audio Spectrogram Transformer\n(AST) models using snoring recordings from 37 subjects undergoing drug-induced\nsleep endoscopy (DISE) between 2020 and 2021. Snoring sounds were labeled\naccording to the VOTE (Velum, Orophar-ynx, Tongue Base, Epiglottis)\nclassification, resulting in 259 V, 403 O, 77 T, 13 E, 1016 VO, 46 VT, 140 OT,\n39 OE, 30 VOT, and 3150 non-snoring (N) 0.5-second clips. The models were\ntrained for two multi-label classification tasks: identifying obstructions at\nV, O, T, and E levels, and identifying retropalatal (RP) and retroglossal (RG)\nobstruc-tions. Results showed AST slightly outperformed ResNet-50,\ndemonstrating good abil-ity to identify V (F1-score: 0.71, MCC: 0.61, AUC:\n0.89), O (F1-score: 0.80, MCC: 0.72, AUC: 0.94), and RP obstructions (F1-score:\n0.86, MCC: 0.77, AUC: 0.97). However, both models struggled with T, E, and RG\nclassifications due to limited data. Retrospective analysis of a full-night\nrecording showed the potential to profile airway obstruction dynamics. We\nexpect this information, combined with polysomnography and other clinical\nparameters, can aid clinical triage and treatment planning for OSA patients.",
      "tldr_zh": "本研究利用深度学习模型（如 ResNet-50 和 Audio Spectrogram Transformer (AST)）基于打鼾声音自动监测阻塞性睡眠呼吸暂停（OSA）患者的多级上呼吸道塌陷。研究团队使用来自37个受试者的药物诱导睡眠内镜（DISE）录音数据，按照 VOTE 分类（Velum, Oropharynx, Tongue Base, Epiglottis）标记训练模型，进行多标签分类任务，包括识别 V、O、T、E 级别以及 retropalatal (RP) 和 retroglossal (RG) 阻塞。结果显示 AST 模型在识别 V（F1-score: 0.71）、O（F1-score: 0.80）和 RP（F1-score: 0.86）阻塞时表现优于 ResNet-50，但由于数据有限，两者在 T、E 和 RG 分类上效果较差。该方法可通过回顾分析全夜录音 profiling 气道阻塞动态，结合多导睡眠图和临床参数辅助 OSA 患者的治疗规划和分流。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.16030v2",
      "published_date": "2024-08-28 09:30:20 UTC",
      "updated_date": "2025-01-09 06:33:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:33:21.995140"
    },
    {
      "arxiv_id": "2409.07473v1",
      "title": "Ethical AI Governance: Methods for Evaluating Trustworthy AI",
      "title_zh": "翻译失败",
      "authors": [
        "Louise McCormack",
        "Malika Bendechache"
      ],
      "abstract": "Trustworthy Artificial Intelligence (TAI) integrates ethics that align with\nhuman values, looking at their influence on AI behaviour and decision-making.\nPrimarily dependent on self-assessment, TAI evaluation aims to ensure ethical\nstandards and safety in AI development and usage. This paper reviews the\ncurrent TAI evaluation methods in the literature and offers a classification,\ncontributing to understanding self-assessment methods in this field.",
      "tldr_zh": "这篇论文探讨了可信赖人工智能 (Trustworthy AI, TAI)，强调其整合人类价值观的伦理标准，以影响 AI 的行为和决策，并依赖自评估来确保 AI 开发和使用的伦理标准及安全。论文审阅了现有文献中的 TAI 评估方法，并提供了一个分类框架，以加深对自评估方法的理解。该工作有助于提升 AI 治理的可信度和可靠性，为未来伦理 AI 评估提供参考。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "68T01 (Primary) 68M14 (Secondary)",
        "I.2.9; K.4.1"
      ],
      "primary_category": "cs.CY",
      "comment": "6 pages, 1 figure, accepted for presentation at AIEB 2024: Workshop\n  on Implementing AI Ethics Through a Behavioural Lens - ECAI, Octoebr 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.07473v1",
      "published_date": "2024-08-28 09:25:50 UTC",
      "updated_date": "2024-08-28 09:25:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:33:41.053766"
    },
    {
      "arxiv_id": "2408.15658v1",
      "title": "An Empirical Study on Self-correcting Large Language Models for Data Science Code Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Thai Tang Quoc",
        "Duc Ha Minh",
        "Tho Quan Thanh",
        "Anh Nguyen-Duc"
      ],
      "abstract": "Large Language Models (LLMs) have recently advanced many applications on\nsoftware engineering tasks, particularly the potential for code generation.\nAmong contemporary challenges, code generated by LLMs often suffers from\ninaccuracies and hallucinations, requiring external inputs to correct. One\nrecent strategy to fix these issues is to refine the code generated from LLMs\nusing the input from the model itself (self-augmented). In this work, we\nproposed a novel method, namely CoT-SelfEvolve. CoT-SelfEvolve iteratively and\nautomatically refines code through a self-correcting process, guided by a chain\nof thought constructed from real-world programming problem feedback. Focusing\non data science code, including Python libraries such as NumPy and Pandas, our\nevaluations on the DS-1000 dataset demonstrate that CoT-SelfEvolve\nsignificantly outperforms existing models in solving complex problems. The\nframework shows substantial improvements in both initial code generation and\nsubsequent iterations, with the model's accuracy increasing significantly with\neach additional iteration. This highlights the effectiveness of using\nchain-of-thought prompting to address complexities revealed by program executor\ntraceback error messages. We also discuss how CoT-SelfEvolve can be integrated\ninto continuous software engineering environments, providing a practical\nsolution for improving LLM-based code generation.",
      "tldr_zh": "本研究对大型语言模型（LLMs）在数据科学代码生成中的自校正能力进行实证研究，针对代码不准确和幻觉问题，提出了一种新方法CoT-SelfEvolve。CoT-SelfEvolve利用Chain-of-Thought提示构建的链式思维，迭代自动修正代码，通过程序执行器回溯错误消息的反馈来指导优化过程。实验在DS-1000数据集上评估了该框架在处理NumPy和Pandas等Python库的复杂问题时，显著优于现有模型，准确率随迭代显著提升。该方法展示了如何将Chain-of-Thought应用于持续软件工程环境中，提高LLMs代码生成的实用性和可靠性。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.15658v1",
      "published_date": "2024-08-28 09:19:09 UTC",
      "updated_date": "2024-08-28 09:19:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:33:42.310829"
    },
    {
      "arxiv_id": "2408.15650v1",
      "title": "Harnessing the Intrinsic Knowledge of Pretrained Language Models for Challenging Text Classification Settings",
      "title_zh": "利用预",
      "authors": [
        "Lingyu Gao"
      ],
      "abstract": "Text classification is crucial for applications such as sentiment analysis\nand toxic text filtering, but it still faces challenges due to the complexity\nand ambiguity of natural language. Recent advancements in deep learning,\nparticularly transformer architectures and large-scale pretraining, have\nachieved inspiring success in NLP fields. Building on these advancements, this\nthesis explores three challenging settings in text classification by leveraging\nthe intrinsic knowledge of pretrained language models (PLMs). Firstly, to\naddress the challenge of selecting misleading yet incorrect distractors for\ncloze questions, we develop models that utilize features based on\ncontextualized word representations from PLMs, achieving performance that\nrivals or surpasses human accuracy. Secondly, to enhance model generalization\nto unseen labels, we create small finetuning datasets with domain-independent\ntask label descriptions, improving model performance and robustness. Lastly, we\ntackle the sensitivity of large language models to in-context learning prompts\nby selecting effective demonstrations, focusing on misclassified examples and\nresolving model ambiguity regarding test example labels.",
      "tldr_zh": "该论文探讨了如何利用预训练语言模型 (PLMs) 的内在知识来应对文本分类的挑战，包括自然语言的复杂性和模糊性。研究者首先开发了基于 PLMs 的上下文化词表示模型，用于选择误导性干扰项的完形填空任务，性能达到或超过人类水平；其次，通过创建小型微调数据集并使用领域无关的任务标签描述，提升了模型对未见标签的泛化能力和鲁棒性；最后，针对大语言模型对 in-context learning 提示的敏感性，提出了选择有效演示样本的方法，聚焦误分类示例以减少模型模糊。整体而言，这些方法为文本分类应用如情感分析和有毒文本过滤提供了更可靠的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "PhD thesis",
      "pdf_url": "http://arxiv.org/pdf/2408.15650v1",
      "published_date": "2024-08-28 09:07:30 UTC",
      "updated_date": "2024-08-28 09:07:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:33:56.003878"
    },
    {
      "arxiv_id": "2408.15649v1",
      "title": "Hierarchical Blockmodelling for Knowledge Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Marcin Pietrasik",
        "Marek Reformat",
        "Anna Wilbik"
      ],
      "abstract": "In this paper, we investigate the use of probabilistic graphical models,\nspecifically stochastic blockmodels, for the purpose of hierarchical entity\nclustering on knowledge graphs. These models, seldom used in the Semantic Web\ncommunity, decompose a graph into a set of probability distributions. The\nparameters of these distributions are then inferred allowing for their\nsubsequent sampling to generate a random graph. In a non-parametric setting,\nthis allows for the induction of hierarchical clusterings without prior\nconstraints on the hierarchy's structure. Specifically, this is achieved by the\nintegration of the Nested Chinese Restaurant Process and the Stick Breaking\nProcess into the generative model. In this regard, we propose a model\nleveraging such integration and derive a collapsed Gibbs sampling scheme for\nits inference. To aid in understanding, we describe the steps in this\nderivation and provide an implementation for the sampler. We evaluate our model\non synthetic and real-world datasets and quantitatively compare against\nbenchmark models. We further evaluate our results qualitatively and find that\nour model is capable of inducing coherent cluster hierarchies in small scale\nsettings. The work presented in this paper provides the first step for the\nfurther application of stochastic blockmodels for knowledge graphs on a larger\nscale. We conclude the paper with potential avenues for future work on more\nscalable inference schemes.",
      "tldr_zh": "本论文探讨了使用随机块模型(stochastic blockmodels)等概率图形模型，对知识图谱(knowledge graphs)进行层次实体聚类。该方法将图分解为概率分布，并通过整合 Nested Chinese Restaurant Process 和 Stick Breaking Process 到生成模型中，实现无预设结构的层次聚类，同时推导了 collapsed Gibbs sampling 方案用于模型推理。实验在合成和真实数据集上显示，该模型在小规模设置中产生连贯的聚类层次，并优于基准模型，为大规模知识图谱应用奠定基础，并指出未来可扩展推理方案的潜力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "31 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.15649v1",
      "published_date": "2024-08-28 09:04:15 UTC",
      "updated_date": "2024-08-28 09:04:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:34:07.894941"
    },
    {
      "arxiv_id": "2408.15640v3",
      "title": "GANs Conditioning Methods: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Anis Bourou",
        "Valérie Mezger",
        "Auguste Genovesio"
      ],
      "abstract": "In recent years, Generative Adversarial Networks (GANs) have seen significant\nadvancements, leading to their widespread adoption across various fields. The\noriginal GAN architecture enables the generation of images without any specific\ncontrol over the content, making it an unconditional generation process.\nHowever, many practical applications require precise control over the generated\noutput, which has led to the development of conditional GANs (cGANs) that\nincorporate explicit conditioning to guide the generation process. cGANs extend\nthe original framework by incorporating additional information (conditions),\nenabling the generation of samples that adhere to that specific criteria.\nVarious conditioning methods have been proposed, each differing in how they\nintegrate the conditioning information into both the generator and the\ndiscriminator networks. In this work, we review the conditioning methods\nproposed for GANs, exploring the characteristics of each method and\nhighlighting their unique mechanisms and theoretical foundations. Furthermore,\nwe conduct a comparative analysis of these methods, evaluating their\nperformance on various image datasets. Through these analyses, we aim to\nprovide insights into the strengths and limitations of various conditioning\ntechniques, guiding future research and application in generative modeling.",
      "tldr_zh": "这篇论文对生成对抗网络(GANs)的条件化方法进行了全面调查，重点探讨了从无条件生成到条件GANs(cGANs)的演变，以及如何通过整合额外条件信息来控制生成输出。作者回顾了各种条件化方法，包括这些方法在生成器和判别器网络中的整合机制及其理论基础，并通过比较分析评估了它们在不同图像数据集上的性能表现。通过这些分析，论文揭示了各方法的优势和局限性，为未来GANs生成建模的研究和应用提供了指导性见解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.15640v3",
      "published_date": "2024-08-28 08:52:14 UTC",
      "updated_date": "2024-09-03 08:35:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:34:18.265705"
    },
    {
      "arxiv_id": "2409.00119v2",
      "title": "3-in-1: 2D Rotary Adaptation for Efficient Finetuning, Efficient Batching and Composability",
      "title_zh": "翻译失败",
      "authors": [
        "Baohao Liao",
        "Christof Monz"
      ],
      "abstract": "Parameter-efficient finetuning (PEFT) methods effectively adapt large\nlanguage models (LLMs) to diverse downstream tasks, reducing storage and GPU\nmemory demands. Despite these advantages, several applications pose new\nchallenges to PEFT beyond mere parameter efficiency. One notable challenge\ninvolves the efficient deployment of LLMs equipped with multiple task- or\nuser-specific adapters, particularly when different adapters are needed for\ndistinct requests within the same batch. Another challenge is the\ninterpretability of LLMs, which is crucial for understanding how LLMs function.\nPrevious studies introduced various approaches to address different challenges.\nIn this paper, we introduce a novel method, RoAd, which employs a\nstraightforward 2D rotation to adapt LLMs and addresses all the above\nchallenges: (1) RoAd is remarkably parameter-efficient, delivering optimal\nperformance on GLUE, eight commonsense reasoning tasks and four arithmetic\nreasoning tasks with $<0.1\\%$ trainable parameters; (2) RoAd facilitates the\nefficient serving of requests requiring different adapters within a batch, with\nan overhead comparable to element-wise multiplication instead of batch matrix\nmultiplication; (3) RoAd enhances LLM's interpretability through integration\nwithin a framework of distributed interchange intervention, demonstrated via\ncomposition experiments.",
      "tldr_zh": "该论文提出了RoAd方法，一种基于2D旋转的创新技术，用于参数高效微调(PEFT)大型语言模型(LLMs)，同时解决多任务适配器部署、批量处理效率和模型可解释性的挑战。RoAd在GLUE、八个常识推理任务和四个算术推理任务上，仅使用不到0.1%的可训练参数，就实现了最佳性能；此外，它支持同一批量中不同适配器的高效服务，开销仅相当于元素-wise乘法，而非批量矩阵乘法。总体而言，RoAd通过分布式互换干预框架提升了LLMs的可解释性，并在组合实验中展示了其在实际应用中的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to NeurIPS 2024. Code: https://github.com/BaohaoLiao/road",
      "pdf_url": "http://arxiv.org/pdf/2409.00119v2",
      "published_date": "2024-08-28 08:45:29 UTC",
      "updated_date": "2024-11-04 09:07:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:34:30.512159"
    },
    {
      "arxiv_id": "2408.15632v1",
      "title": "Structural Optimization of Lightweight Bipedal Robot via SERL",
      "title_zh": "翻译失败",
      "authors": [
        "Yi Cheng",
        "Chenxi Han",
        "Yuheng Min",
        "Linqi Ye",
        "Houde Liu",
        "Hang Liu"
      ],
      "abstract": "Designing a bipedal robot is a complex and challenging task, especially when\ndealing with a multitude of structural parameters. Traditional design methods\noften rely on human intuition and experience. However, such approaches are\ntime-consuming, labor-intensive, lack theoretical guidance and hard to obtain\noptimal design results within vast design spaces, thus failing to full exploit\nthe inherent performance potential of robots. In this context, this paper\nintroduces the SERL (Structure Evolution Reinforcement Learning) algorithm,\nwhich combines reinforcement learning for locomotion tasks with evolution\nalgorithms. The aim is to identify the optimal parameter combinations within a\ngiven multidimensional design space. Through the SERL algorithm, we\nsuccessfully designed a bipedal robot named Wow Orin, where the optimal leg\nlength are obtained through optimization based on body structure and motor\ntorque. We have experimentally validated the effectiveness of the SERL\nalgorithm, which is capable of optimizing the best structure within specified\ndesign space and task conditions. Additionally, to assess the performance gap\nbetween our designed robot and the current state-of-the-art robots, we compared\nWow Orin with mainstream bipedal robots Cassie and Unitree H1. A series of\nexperimental results demonstrate the Outstanding energy efficiency and\nperformance of Wow Orin, further validating the feasibility of applying the\nSERL algorithm to practical design.",
      "tldr_zh": "该论文针对双足机器人设计中结构参数众多且传统方法依赖直觉、耗时的挑战，提出了一种结合强化学习(Reinforcement Learning)和进化算法(Evolution Algorithms)的SERL算法，用于在多维设计空间中优化最佳参数组合。通过SERL算法，研究者成功设计了轻量级双足机器人Wow Orin，并优化了其腿长等结构以匹配机体和电机扭矩。实验结果显示，Wow Orin在能量效率和性能上优于主流机器人Cassie和Unitree H1，验证了SERL算法在实际设计中的可行性和有效性。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.15632v1",
      "published_date": "2024-08-28 08:34:05 UTC",
      "updated_date": "2024-08-28 08:34:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:34:42.750303"
    },
    {
      "arxiv_id": "2408.15630v1",
      "title": "CodeSift: An LLM-Based Reference-Less Framework for Automatic Code Validation",
      "title_zh": "CodeSift：基于 LLM 的无参考自动代码验证框架",
      "authors": [
        "Pooja Aggarwal",
        "Oishik Chatterjee",
        "Ting Dai",
        "Prateeti Mohapatra",
        "Brent Paulovicks",
        "Brad Blancett",
        "Arthur De Magalhaes"
      ],
      "abstract": "The advent of large language models (LLMs) has greatly facilitated code\ngeneration, but ensuring the functional correctness of generated code remains a\nchallenge. Traditional validation methods are often time-consuming,\nerror-prone, and impractical for large volumes of code. We introduce CodeSift,\na novel framework that leverages LLMs as the first-line filter of code\nvalidation without the need for execution, reference code, or human feedback,\nthereby reducing the validation effort. We assess the effectiveness of our\nmethod across three diverse datasets encompassing two programming languages.\nOur results indicate that CodeSift outperforms state-of-the-art code evaluation\nmethods. Internal testing conducted with subject matter experts reveals that\nthe output generated by CodeSift is in line with human preference, reinforcing\nits effectiveness as a dependable automated code validation tool.",
      "tldr_zh": "该研究提出CodeSift，一种基于LLM的无需参考框架，用于自动代码验证，旨在解决传统验证方法耗时且易出错的问题。\nCodeSift利用LLM作为首选过滤器，通过不依赖代码执行、参考代码或人工反馈来减少验证工作。\n实验结果显示，该框架在三个多样化数据集（涵盖两种编程语言）上优于现有最先进方法，且内部测试表明其输出与人类偏好一致，提供了一个可靠的自动化代码验证工具。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.15630v1",
      "published_date": "2024-08-28 08:32:21 UTC",
      "updated_date": "2024-08-28 08:32:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:35:05.135825"
    },
    {
      "arxiv_id": "2408.15625v2",
      "title": "CBF-LLM: Safe Control for LLM Alignment",
      "title_zh": "CBF-LLM：LLM 对齐的安全控制",
      "authors": [
        "Yuya Miyaoka",
        "Masaki Inoue"
      ],
      "abstract": "This paper proposes a control-based framework for aligning large language\nmodels (LLMs) by leveraging a control barrier function (CBF) to ensure\nuser-desirable text generation. The presented framework applies the safety\nfilter, designed based on the CBF, to the output generation of the baseline\nLLM, i.e., the sequence of the token, with the aim of intervening in the\ngenerated text. The overall text-generation system is implemented with Llama 3\nand a RoBERTa model, and the source code is available at\nhttps://github.com/Mya-Mya/CBF-LLM. The experiment demonstrates its control\nability and effectiveness in reducing the number of interventions needed for\nuser-specified alignment tasks.",
      "tldr_zh": "这篇论文提出了 CBF-LLM 框架，利用控制屏障函数 (CBF) 来对齐大型语言模型 (LLMs)，确保生成用户期望的文本。框架通过基于 CBF 的安全过滤器对基线 LLM 的输出序列（即 token 序列）进行干预，从而实现有效的文本生成控制。实验使用 Llama 3 和 RoBERTa 模型验证了该方法的控制能力和在减少用户指定对齐任务干预次数方面的有效性。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.CL",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.15625v2",
      "published_date": "2024-08-28 08:25:22 UTC",
      "updated_date": "2024-10-07 09:49:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:35:09.559386"
    },
    {
      "arxiv_id": "2409.07471v1",
      "title": "AI, Climate, and Transparency: Operationalizing and Improving the AI Act",
      "title_zh": "翻译失败",
      "authors": [
        "Nicolas Alder",
        "Kai Ebert",
        "Ralf Herbrich",
        "Philipp Hacker"
      ],
      "abstract": "This paper critically examines the AI Act's provisions on climate-related\ntransparency, highlighting significant gaps and challenges in its\nimplementation. We identify key shortcomings, including the exclusion of energy\nconsumption during AI inference, the lack of coverage for indirect greenhouse\ngas emissions from AI applications, and the lack of standard reporting\nmethodology. The paper proposes a novel interpretation to bring\ninference-related energy use back within the Act's scope and advocates for\npublic access to climate-related disclosures to foster market accountability\nand public scrutiny. Cumulative server level energy reporting is recommended as\nthe most suitable method. We also suggests broader policy changes, including\nsustainability risk assessments and renewable energy targets, to better address\nAI's environmental impact.",
      "tldr_zh": "这篇论文批判性地审视了 AI Act 在气候相关透明度方面的规定，指出关键缺陷，包括忽略 AI 推理过程中的能源消耗、未覆盖间接温室气体排放，以及缺乏标准报告方法。论文提出新解释，将推理相关能源使用纳入 Act 范围，并主张公开气候披露以增强市场问责和公众监督，推荐采用累积服务器级能源报告作为最佳方法。最终，论文建议更广泛的政策改革，如进行可持续性风险评估和设定可再生能源目标，以更好地缓解 AI 的环境影响。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "5 pages, 1 table, preprint",
      "pdf_url": "http://arxiv.org/pdf/2409.07471v1",
      "published_date": "2024-08-28 07:57:39 UTC",
      "updated_date": "2024-08-28 07:57:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:35:18.515663"
    },
    {
      "arxiv_id": "2408.15561v2",
      "title": "CGRA4ML: A Framework to Implement Modern Neural Networks for Scientific Edge Computing",
      "title_zh": "CGRA4ML：用于科学边缘计算中实现现代神经网络的框架",
      "authors": [
        "G Abarajithan",
        "Zhenghua Ma",
        "Zepeng Li",
        "Shrideep Koparkar",
        "Ravidu Munasinghe",
        "Francesco Restuccia",
        "Ryan Kastner"
      ],
      "abstract": "Scientific edge computing increasingly relies on hardware-accelerated neural\nnetworks to implement complex, near-sensor processing at extremely high\nthroughputs and low latencies. Existing frameworks like HLS4ML are effective\nfor smaller models, but struggle with larger, modern neural networks due to\ntheir requirement of spatially implementing the neural network layers and\nstoring all weights in on-chip memory. CGRA4ML is an open-source, modular\nframework designed to bridge the gap between neural network model complexity\nand extreme performance requirements. CGRA4ML extends the capabilities of\nHLS4ML by allowing off-chip data storage and supporting a broader range of\nneural network architectures, including models like ResNet, PointNet, and\ntransformers. Unlike HLS4ML, CGRA4ML generates SystemVerilog RTL, making it\nmore suitable for targeting ASIC and FPGA design flows. We demonstrate the\neffectiveness of our framework by implementing and scaling larger models that\nwere previously unattainable with HLS4ML, showcasing its adaptability and\nefficiency in handling complex computations. CGRA4ML also introduces an\nextensive verification framework, with a generated runtime firmware that\nenables its integration into different SoC platforms. CGRA4ML's minimal and\nmodular infrastructure of Python API, SystemVerilog hardware, Tcl toolflows,\nand C runtime, facilitates easy integration and experimentation, allowing\nscientists to focus on innovation rather than the intricacies of hardware\ndesign and optimization.",
      "tldr_zh": "该研究引入了 CGRA4ML，一个开源模块化框架，旨在为科学边缘计算实现现代神经网络模型，解决现有框架如 HLS4ML 在处理大型模型时的局限性，如空间实现层和片上内存存储权重的问题。CGRA4ML 通过支持 off-chip 数据存储、兼容更广泛的架构（如 ResNet、PointNet 和 transformers）、并生成 SystemVerilog RTL，使其更适合 ASIC 和 FPGA 设计，从而提升高吞吐量和低延迟性能。实验证明，该框架成功实现了之前无法处理的更大模型，并提供了全面验证框架和模块化基础设施（如 Python API、SystemVerilog 硬件和 C 运行时），便于科学家专注于创新而非硬件优化。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.15561v2",
      "published_date": "2024-08-28 06:24:13 UTC",
      "updated_date": "2024-08-29 01:26:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:35:32.774988"
    },
    {
      "arxiv_id": "2408.15550v2",
      "title": "Trustworthy and Responsible AI for Human-Centric Autonomous Decision-Making Systems",
      "title_zh": "可靠且负责任的人工智能用于以人为中心的自治决策系统",
      "authors": [
        "Farzaneh Dehghani",
        "Mahsa Dibaji",
        "Fahim Anzum",
        "Lily Dey",
        "Alican Basdemir",
        "Sayeh Bayat",
        "Jean-Christophe Boucher",
        "Steve Drew",
        "Sarah Elaine Eaton",
        "Richard Frayne",
        "Gouri Ginde",
        "Ashley Harris",
        "Yani Ioannou",
        "Catherine Lebel",
        "John Lysack",
        "Leslie Salgado Arzuaga",
        "Emma Stanley",
        "Roberto Souza",
        "Ronnie de Souza Santos",
        "Lana Wells",
        "Tyler Williamson",
        "Matthias Wilms",
        "Zaman Wahid",
        "Mark Ungrin",
        "Marina Gavrilova",
        "Mariana Bento"
      ],
      "abstract": "Artificial Intelligence (AI) has paved the way for revolutionary\ndecision-making processes, which if harnessed appropriately, can contribute to\nadvancements in various sectors, from healthcare to economics. However, its\nblack box nature presents significant ethical challenges related to bias and\ntransparency. AI applications are hugely impacted by biases, presenting\ninconsistent and unreliable findings, leading to significant costs and\nconsequences, highlighting and perpetuating inequalities and unequal access to\nresources. Hence, developing safe, reliable, ethical, and Trustworthy AI\nsystems is essential.\n  Our team of researchers working with Trustworthy and Responsible AI, part of\nthe Transdisciplinary Scholarship Initiative within the University of Calgary,\nconducts research on Trustworthy and Responsible AI, including fairness, bias\nmitigation, reproducibility, generalization, interpretability, and\nauthenticity. In this paper, we review and discuss the intricacies of AI\nbiases, definitions, methods of detection and mitigation, and metrics for\nevaluating bias. We also discuss open challenges with regard to the\ntrustworthiness and widespread application of AI across diverse domains of\nhuman-centric decision making, as well as guidelines to foster Responsible and\nTrustworthy AI models.",
      "tldr_zh": "这篇论文探讨了可信赖和负责任的 AI（Trustworthy and Responsible AI）在人类中心自治决策系统中的应用，强调了 AI 的黑箱性质导致的偏见（bias）和透明度问题，以及这些问题对医疗、经济等领域的不利影响。研究团队通过文献审查，分析了 AI 偏见的定义、检测方法、缓解策略（如 fairness 和 bias mitigation）、以及评估指标，并讨论了可重现性（reproducibility）、泛化（generalization）和可解释性（interpretability）。论文还指出了 AI 在多样化领域应用的开放挑战，并提出了指导方针，以推动安全、可靠和道德 AI 系统的发展。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "44 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.15550v2",
      "published_date": "2024-08-28 06:04:25 UTC",
      "updated_date": "2024-09-02 07:55:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:35:45.986226"
    },
    {
      "arxiv_id": "2409.07469v1",
      "title": "Small Object Detection for Indoor Assistance to the Blind using YOLO NAS Small and Super Gradients",
      "title_zh": "翻译失败",
      "authors": [
        "Rashmi BN",
        "R. Guru",
        "Anusuya M A"
      ],
      "abstract": "Advancements in object detection algorithms have opened new avenues for\nassistive technologies that cater to the needs of visually impaired\nindividuals. This paper presents a novel approach for indoor assistance to the\nblind by addressing the challenge of small object detection. We propose a\ntechnique YOLO NAS Small architecture, a lightweight and efficient object\ndetection model, optimized using the Super Gradients training framework. This\ncombination enables real-time detection of small objects crucial for assisting\nthe blind in navigating indoor environments, such as furniture, appliances, and\nhousehold items. Proposed method emphasizes low latency and high accuracy,\nenabling timely and informative voice-based guidance to enhance the user's\nspatial awareness and interaction with their surroundings. The paper details\nthe implementation, experimental results, and discusses the system's\neffectiveness in providing a practical solution for indoor assistance to the\nvisually impaired.",
      "tldr_zh": "本研究针对视力障碍者室内辅助，提出了一种小物体检测方法，使用 YOLO NAS Small 架构作为轻量级高效模型，并通过 Super Gradients 训练框架进行优化。该方法实现了对室内小物体的实时检测，如家具、家电和家居物品，强调低延迟和高准确性，以提供及时的语音指导，提升用户的空间感知和环境互动。实验结果显示，该系统在实际应用中表现出色，为视力障碍者提供了一个实用有效的辅助解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.07469v1",
      "published_date": "2024-08-28 05:38:20 UTC",
      "updated_date": "2024-08-28 05:38:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:35:53.546695"
    },
    {
      "arxiv_id": "2408.15543v2",
      "title": "An Investigation of Warning Erroneous Chat Translations in Cross-lingual Communication",
      "title_zh": "翻译失败",
      "authors": [
        "Yunmeng Li",
        "Jun Suzuki",
        "Makoto Morishita",
        "Kaori Abe",
        "Kentaro Inui"
      ],
      "abstract": "Machine translation models are still inappropriate for translating chats,\ndespite the popularity of translation software and plug-in applications. The\ncomplexity of dialogues poses significant challenges and can hinder\ncrosslingual communication. Instead of pursuing a flawless translation system,\na more practical approach would be to issue warning messages about potential\nmistranslations to reduce confusion. However, it is still unclear how\nindividuals perceive these warning messages and whether they benefit the crowd.\nThis paper tackles to investigate this question and demonstrates the warning\nmessages' contribution to making chat translation systems effective.",
      "tldr_zh": "这篇论文探讨了机器翻译模型在聊天场景中的不足，指出对话的复杂性可能导致跨语言沟通的误译问题。作者提出一种实用方法，即通过发出警告消息来提示潜在的错误翻译，而不是追求完美的翻译系统。研究调查了用户对这些警告消息的感知，并证明它们能有效减少混淆，从而提升聊天翻译系统的整体效能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.15543v2",
      "published_date": "2024-08-28 05:36:25 UTC",
      "updated_date": "2024-11-05 04:13:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:36:10.967049"
    },
    {
      "arxiv_id": "2408.15542v1",
      "title": "Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input",
      "title_zh": "Kangaroo：一个强大的视频-语言模型，支持长上下文视频输入",
      "authors": [
        "Jiajun Liu",
        "Yibing Wang",
        "Hanghang Ma",
        "Xiaoping Wu",
        "Xiaoqi Ma",
        "Xiaoming Wei",
        "Jianbin Jiao",
        "Enhua Wu",
        "Jie Hu"
      ],
      "abstract": "Rapid advancements have been made in extending Large Language Models (LLMs)\nto Large Multi-modal Models (LMMs). However, extending input modality of LLMs\nto video data remains a challenging endeavor, especially for long videos. Due\nto insufficient access to large-scale high-quality video data and the excessive\ncompression of visual features, current methods exhibit limitations in\neffectively processing long videos. In this paper, we introduce Kangaroo, a\npowerful Video LMM aimed at addressing these challenges. Confronted with issue\nof inadequate training data, we develop a data curation system to build a\nlarge-scale dataset with high-quality annotations for vision-language\npre-training and instruction tuning. In addition, we design a curriculum\ntraining pipeline with gradually increasing resolution and number of input\nframes to accommodate long videos. Evaluation results demonstrate that, with 8B\nparameters, Kangaroo achieves state-of-the-art performance across a variety of\nvideo understanding benchmarks while exhibiting competitive results on others.\nParticularly, on benchmarks specialized for long videos, Kangaroo excels some\nlarger models with over 10B parameters and proprietary models.",
      "tldr_zh": "本文介绍了 Kangaroo，一种强大的视频语言模型 (Video LMM)，旨在解决扩展 Large Language Models (LLMs) 到长视频输入的挑战，包括数据不足和视觉特征过度压缩问题。作者开发了数据整理系统以构建大规模高质量数据集，并设计了课程训练管道，逐步增加分辨率和输入帧数以适应长视频处理。该模型在8B参数下，在多种视频理解基准上达到最先进性能，尤其在长视频任务上超越了超过10B参数的模型和专有模型。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.15542v1",
      "published_date": "2024-08-28 05:34:14 UTC",
      "updated_date": "2024-08-28 05:34:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:36:18.660003"
    },
    {
      "arxiv_id": "2408.15538v2",
      "title": "TrafficGamer: Reliable and Flexible Traffic Simulation for Safety-Critical Scenarios with Game-Theoretic Oracles",
      "title_zh": "翻译失败",
      "authors": [
        "Guanren Qiao",
        "Guorui Quan",
        "Jiawei Yu",
        "Shujun Jia",
        "Guiliang Liu"
      ],
      "abstract": "While modern Autonomous Vehicle (AV) systems can develop reliable driving\npolicies under regular traffic conditions, they frequently struggle with\nsafety-critical traffic scenarios. This difficulty primarily arises from the\nrarity of such scenarios in driving datasets and the complexities associated\nwith predictive modeling among multiple vehicles. To support the testing and\nrefinement of AV policies, simulating safety-critical traffic events is an\nessential challenge to be addressed. In this work, we introduce TrafficGamer,\nwhich facilitates game-theoretic traffic simulation by viewing common road\ndriving as a multi-agent game. In evaluating the empirical performance across\nvarious real-world datasets, TrafficGamer ensures both fidelity and\nexploitability of the simulated scenarios, guaranteeing that they not only\nstatically align with real-world traffic distribution but also efficiently\ncapture equilibriums for representing safety-critical scenarios involving\nmultiple agents. Additionally, the results demonstrate that TrafficGamer\nexhibits highly flexible simulation across various contexts. Specifically, we\ndemonstrate that the generated scenarios can dynamically adapt to equilibriums\nof varying tightness by configuring risk-sensitive constraints during\noptimization. To the best of our knowledge, TrafficGamer is the first simulator\ncapable of generating diverse traffic scenarios involving multiple agents. We\nhave provided a demo webpage for the project at\nhttps://qiaoguanren.github.io/trafficgamer-demo/.",
      "tldr_zh": "该论文提出 TrafficGamer，一种基于博弈论 (game-theoretic) 的交通模拟系统，用于测试和优化自动驾驶车辆 (AV) 在安全关键场景中的表现，通过将道路驾驶视为多智能体游戏来生成可靠的模拟环境。TrafficGamer 确保模拟场景不仅与真实交通分布一致，还能高效捕获多智能体均衡，从而处理稀有安全事件。实验结果显示，该系统在各种真实数据集上表现出高保真度和灵活性，能够通过风险敏感约束动态调整均衡紧度，是首个支持多样化多智能体场景生成的模拟器。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.15538v2",
      "published_date": "2024-08-28 05:11:16 UTC",
      "updated_date": "2024-10-21 11:32:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:36:31.643972"
    },
    {
      "arxiv_id": "2408.15535v1",
      "title": "Improving Thompson Sampling via Information Relaxation for Budgeted Multi-armed Bandits",
      "title_zh": "翻译失败",
      "authors": [
        "Woojin Jeong",
        "Seungki Min"
      ],
      "abstract": "We consider a Bayesian budgeted multi-armed bandit problem, in which each arm\nconsumes a different amount of resources when selected and there is a budget\nconstraint on the total amount of resources that can be used. Budgeted Thompson\nSampling (BTS) offers a very effective heuristic to this problem, but its\narm-selection rule does not take into account the remaining budget information.\nWe adopt \\textit{Information Relaxation Sampling} framework that generalizes\nThompson Sampling for classical $K$-armed bandit problems, and propose a series\nof algorithms that are randomized like BTS but more carefully optimize their\ndecisions with respect to the budget constraint. In a one-to-one correspondence\nwith these algorithms, a series of performance benchmarks that improve the\nconventional benchmark are also suggested. Our theoretical analysis and\nsimulation results show that our algorithms (and our benchmarks) make\nincremental improvements over BTS (respectively, the conventional benchmark)\nacross various settings including a real-world example.",
      "tldr_zh": "本研究针对预算有限的多臂老虎机（Budgeted Multi-armed Bandits）问题，提出了一种改进Thompson Sampling的方法，通过Information Relaxation Sampling框架优化臂选择规则，以更好地考虑剩余预算约束。该框架生成了一系列随机化算法，这些算法在保持Thompson Sampling（BTS）特性的同时，更精确地处理资源分配问题，并对应提出改进的性能基准。实验结果显示，这些新算法和基准在各种场景中，包括真实世界示例，都实现了对BTS和传统基准的渐进式改进。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "accepted",
      "pdf_url": "http://arxiv.org/pdf/2408.15535v1",
      "published_date": "2024-08-28 04:56:06 UTC",
      "updated_date": "2024-08-28 04:56:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:36:43.498750"
    },
    {
      "arxiv_id": "2408.15533v2",
      "title": "LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via Layer-wise Relevance Propagation",
      "title_zh": "LRP4RAG：通过层级相关性传播检测检索增强生成中的幻觉",
      "authors": [
        "Haichuan Hu",
        "Yuhan Sun",
        "Quanjun Zhang"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has become a primary technique for\nmitigating hallucinations in large language models (LLMs). However, incomplete\nknowledge extraction and insufficient understanding can still mislead LLMs to\nproduce irrelevant or even contradictory responses, which means hallucinations\npersist in RAG. In this paper, we propose LRP4RAG, a method based on the\nLayer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations\nin RAG. Specifically, we first utilize LRP to compute the relevance between the\ninput and output of the RAG generator. We then apply further extraction and\nresampling to the relevance matrix. The processed relevance data are input into\nmultiple classifiers to determine whether the output contains hallucinations.\nTo the best of our knowledge, this is the first time that LRP has been used for\ndetecting RAG hallucinations, and extensive experiments demonstrate that\nLRP4RAG outperforms existing baselines.",
      "tldr_zh": "本论文针对Retrieval-Augmented Generation (RAG)中持续存在的幻觉问题，提出LRP4RAG方法，该方法利用Layer-wise Relevance Propagation (LRP)算法计算RAG生成器输入和输出的相关性。接着，通过对相关性矩阵进行提取和重采样，将处理后的数据输入多个分类器，以判断输出是否包含幻觉。这是首次将LRP应用于RAG幻觉检测，实验结果表明LRP4RAG在性能上优于现有基线模型。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.15533v2",
      "published_date": "2024-08-28 04:44:43 UTC",
      "updated_date": "2024-08-29 08:45:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:36:56.534889"
    },
    {
      "arxiv_id": "2409.00115v2",
      "title": "Self-Adaptive Quantum Kernel Principal Components Analysis for Compact Readout of Chemiresistive Sensor Arrays",
      "title_zh": "翻译失败",
      "authors": [
        "Zeheng Wang",
        "Timothy van der Laan",
        "Muhammad Usman"
      ],
      "abstract": "The rapid growth of Internet of Things (IoT) devices necessitates efficient\ndata compression techniques to handle the vast amounts of data generated by\nthese devices. Chemiresistive sensor arrays (CSAs), a simple-to-fabricate but\ncrucial component in IoT systems, generate large volumes of data due to their\nsimultaneous multi-sensor operations. Classical principal component analysis\n(cPCA) methods, a common solution to the data compression challenge, face\nlimitations in preserving critical information during dimensionality reduction.\nIn this study, we present self-adaptive quantum kernel (SAQK) PCA as a superior\nalternative to enhance information retention. Our findings demonstrate that\nSAQK PCA outperforms cPCA in various back-end machine-learning tasks,\nespecially in low-dimensional scenarios where access to quantum bits is\nlimited. These results highlight the potential of noisy intermediate-scale\nquantum (NISQ) computers to revolutionize data processing in real-world IoT\napplications by improving the efficiency and reliability of CSA data\ncompression and readout, despite the current constraints on qubit availability.",
      "tldr_zh": "该研究针对物联网（IoT）设备生成的大量数据问题，提出了一种Self-Adaptive Quantum Kernel (SAQK) PCA 方法，用于压缩和读取Chemiresistive sensor arrays (CSAs) 数据，以更好地保留关键信息。相比于传统的cPCA，SAQK PCA 在低维场景下显著提升了后端机器学习任务的性能，尤其是在量子比特资源有限的情况下。实验结果表明，这种方法利用noisy intermediate-scale quantum (NISQ) 计算机提高了IoT 数据处理的效率和可靠性，为实际应用提供了革命性潜力。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "Version 2",
      "pdf_url": "http://arxiv.org/pdf/2409.00115v2",
      "published_date": "2024-08-28 04:07:40 UTC",
      "updated_date": "2024-12-02 10:25:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:37:08.153503"
    },
    {
      "arxiv_id": "2408.15513v1",
      "title": "Continual-learning-based framework for structural damage recognition",
      "title_zh": "基于持续学习的结构损伤识别框架",
      "authors": [
        "Jiangpeng Shu",
        "Jiawei Zhang",
        "Reachsak Ly",
        "Fangzheng Lin",
        "Yuanfeng Duan"
      ],
      "abstract": "Multi-damage is common in reinforced concrete structures and leads to the\nrequirement of large number of neural networks, parameters and data storage, if\nconvolutional neural network (CNN) is used for damage recognition. In addition,\nconventional CNN experiences catastrophic forgetting and training inefficiency\nas the number of tasks increases during continual learning, leading to large\naccuracy decrease of previous learned tasks. To address these problems, this\nstudy proposes a continuallearning-based damage recognition model (CLDRM) which\nintegrates the learning without forgetting continual learning method into the\nResNet-34 architecture for the recognition of damages in RC structures as well\nas relevant structural components. Three experiments for four recognition tasks\nwere designed to validate the feasibility and effectiveness of the CLDRM\nframework. In this way, it reduces both the prediction time and data storage by\nabout 75% in four tasks of continuous learning. Three experiments for four\nrecognition tasks were designed to validate the feasibility and effectiveness\nof the CLDRM framework. By gradual feature fusion, CLDRM outperformed other\nmethods by managed to achieve high accuracy in the damage recognition and\nclassification. As the number of recognition tasks increased, CLDRM also\nexperienced smaller decrease of the previous learned tasks. Results indicate\nthat the CLDRM framework successfully performs damage recognition and\nclassification with reasonable accuracy and effectiveness.",
      "tldr_zh": "这篇论文针对钢筋混凝土结构的多重损伤识别问题，提出了一种基于持续学习的框架 CLDRM，将 Learning Without Forgetting 方法整合到 ResNet-34 架构中，以解决传统 CNN 的灾难性遗忘、训练低效和资源消耗过高的问题。CLDRM 通过渐进特征融合，实现损伤识别和分类，同时减少预测时间和数据存储约 75%。实验结果表明，随着识别任务数量增加，该框架保持了高准确率，并对之前任务的性能下降较小，展示了其在结构损伤识别中的可行性和有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "18 pages, 12 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.15513v1",
      "published_date": "2024-08-28 03:50:04 UTC",
      "updated_date": "2024-08-28 03:50:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:37:20.372476"
    },
    {
      "arxiv_id": "2408.15512v3",
      "title": "Toward Automated Simulation Research Workflow through LLM Prompt Engineering Design",
      "title_zh": "翻译失败",
      "authors": [
        "Zhihan Liu",
        "Yubo Chai",
        "Jianfeng Li"
      ],
      "abstract": "The advent of Large Language Models (LLMs) has created new opportunities for\nthe automation of scientific research spanning both experimental processes and\ncomputational simulations. This study explores the feasibility of constructing\nan autonomous simulation agent (ASA) powered by LLMs through prompt engineering\nand automated program design to automate the entire simulation research process\naccording to a human-provided research plan. This process includes experimental\ndesign, remote upload and simulation execution, data analysis, and report\ncompilation. Using a well-studied simulation problem of polymer chain\nconformations as a test case, we assessed the long-task completion and\nreliability of ASAs powered by different LLMs, including GPT-4o, Claude-3.5,\netc. Our findings revealed that ASA-GPT-4o achieved near-flawless execution on\ndesignated research missions, underscoring the potential of methods like ASA to\nachieve automation in simulation research processes to enhance research\nefficiency. The outlined automation can be iteratively performed for up to 20\ncycles without human intervention, illustrating the potential of ASA for\nlong-task workflow automation. Additionally, we discussed the intrinsic traits\nof ASA in managing extensive tasks, focusing on self-validation mechanisms, and\nthe balance between local attention and global oversight.",
      "tldr_zh": "本研究探讨了通过LLM提示工程设计构建自主模拟代理(ASA)，以自动化模拟研究流程，包括实验设计、远程上传执行、数据分析和报告编译。使用聚合物链构象作为测试案例，评估了不同LLMs（如GPT-4o和Claude-3.5）的性能，结果显示ASA-GPT-4o在指定任务中实现了近乎完美的执行，并能迭代运行多达20个周期。论文还讨论了ASA在长任务管理中的关键特性，如自验证机制以及局部注意力和全局监督的平衡，从而提升了模拟研究的效率和可靠性。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "physics.chem-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "The source code and example results of ASA can be found at\n  https://github.com/zokaraa/autonomous_simulation_agent",
      "pdf_url": "http://arxiv.org/pdf/2408.15512v3",
      "published_date": "2024-08-28 03:48:05 UTC",
      "updated_date": "2025-01-15 09:12:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:37:42.960716"
    },
    {
      "arxiv_id": "2408.15511v1",
      "title": "AeroVerse: UAV-Agent Benchmark Suite for Simulating, Pre-training, Finetuning, and Evaluating Aerospace Embodied World Models",
      "title_zh": "翻译失败",
      "authors": [
        "Fanglong Yao",
        "Yuanchang Yue",
        "Youzhi Liu",
        "Xian Sun",
        "Kun Fu"
      ],
      "abstract": "Aerospace embodied intelligence aims to empower unmanned aerial vehicles\n(UAVs) and other aerospace platforms to achieve autonomous perception,\ncognition, and action, as well as egocentric active interaction with humans and\nthe environment. The aerospace embodied world model serves as an effective\nmeans to realize the autonomous intelligence of UAVs and represents a necessary\npathway toward aerospace embodied intelligence. However, existing embodied\nworld models primarily focus on ground-level intelligent agents in indoor\nscenarios, while research on UAV intelligent agents remains unexplored. To\naddress this gap, we construct the first large-scale real-world image-text\npre-training dataset, AerialAgent-Ego10k, featuring urban drones from a\nfirst-person perspective. We also create a virtual image-text-pose alignment\ndataset, CyberAgent Ego500k, to facilitate the pre-training of the aerospace\nembodied world model. For the first time, we clearly define 5 downstream tasks,\ni.e., aerospace embodied scene awareness, spatial reasoning, navigational\nexploration, task planning, and motion decision, and construct corresponding\ninstruction datasets, i.e., SkyAgent-Scene3k, SkyAgent-Reason3k, SkyAgent-Nav3k\nand SkyAgent-Plan3k, and SkyAgent-Act3k, for fine-tuning the aerospace\nembodiment world model. Simultaneously, we develop SkyAgentEval, the downstream\ntask evaluation metrics based on GPT-4, to comprehensively, flexibly, and\nobjectively assess the results, revealing the potential and limitations of\n2D/3D visual language models in UAV-agent tasks. Furthermore, we integrate over\n10 2D/3D visual-language models, 2 pre-training datasets, 5 finetuning\ndatasets, more than 10 evaluation metrics, and a simulator into the benchmark\nsuite, i.e., AeroVerse, which will be released to the community to promote\nexploration and development of aerospace embodied intelligence.",
      "tldr_zh": "这篇论文引入了AeroVerse基准套件，用于模拟、预训练、微调和评估航空航天具身世界模型（aerospace embodied world models），以提升无人机（UAVs）的自主感知、认知和行动能力。研究者构建了两个关键数据集：AerialAgent-Ego10k（基于真实世界无人机第一人称视角的图像-文本数据集）和CyberAgent-Ego500k（虚拟图像-文本-姿态对齐数据集），用于模型预训练。同时，他们定义了5个下游任务，包括aerospace embodied scene awareness、spatial reasoning、navigational exploration、task planning和motion decision，并创建了对应的指令数据集（如SkyAgent-Scene3k等）进行微调。论文还开发了基于GPT-4的SkyAgentEval评估指标，并整合了10多个2D/3D视觉语言模型、预训练和微调数据集以及模拟器，揭示了这些模型在UAV任务中的潜力和局限性，从而促进航空航天具身智能的探索和发展。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.15511v1",
      "published_date": "2024-08-28 03:47:45 UTC",
      "updated_date": "2024-08-28 03:47:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:37:47.627938"
    },
    {
      "arxiv_id": "2408.15510v3",
      "title": "How Reliable are Causal Probing Interventions?",
      "title_zh": "因果探查干预的可靠性如何？",
      "authors": [
        "Marc Canby",
        "Adam Davies",
        "Chirag Rastogi",
        "Julia Hockenmaier"
      ],
      "abstract": "Causal probing aims to analyze foundation models by examining how intervening\non their representation of various latent properties impacts their outputs.\nRecent works have cast doubt on the theoretical basis of several leading causal\nprobing methods, but it has been unclear how to systematically evaluate the\neffectiveness of these methods in practice. To address this, we define two key\ncausal probing desiderata: completeness (how thoroughly the representation of\nthe target property has been transformed) and selectivity (how little\nnon-targeted properties have been impacted). We find that there is an inherent\ntradeoff between the two, which we define as reliability, their harmonic mean.\nWe introduce an empirical analysis framework to measure and evaluate these\nquantities, allowing us to make the first direct comparisons between different\nfamilies of leading causal probing methods (e.g., linear vs. nonlinear, or\nconcept removal vs. counterfactual interventions). We find that: (1) no method\nis reliable across all layers; (2) more reliable methods have a greater impact\non LLM behavior; (3) nonlinear interventions are more reliable in early and\nintermediate layers, and linear interventions are more reliable in later\nlayers; and (4) concept removal methods are far less reliable than\ncounterfactual interventions, suggesting that they may not be an effective\napproach to causal probing.",
      "tldr_zh": "本论文评估了因果探针（causal probing）干预方法的可靠性，定义了两个关键需求：完整性（completeness，指目标属性表示的彻底转换）和选择性（selectivity，指非目标属性影响的最小化），并引入可靠性（reliability）作为它们的调和平均值（harmonic mean）。通过一个实证分析框架，作者比较了不同因果探针方法（如线性 vs. 非线性干预，以及 concept removal vs. counterfactual interventions），发现没有方法在所有层都可靠，且更可靠的方法对LLM行为影响更大。结果显示，非线性干预在早期和中间层更可靠，线性干预在后期层更可靠，而concept removal方法远不如counterfactual interventions有效，提示后者可能是更好的因果探针策略。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.15510v3",
      "published_date": "2024-08-28 03:45:49 UTC",
      "updated_date": "2025-02-06 17:16:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:37:56.594815"
    },
    {
      "arxiv_id": "2408.16029v2",
      "title": "Meta-Learn Unimodal Signals with Weak Supervision for Multimodal Sentiment Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Sijie Mai",
        "Yu Zhao",
        "Ying Zeng",
        "Jianhua Yao",
        "Haifeng Hu"
      ],
      "abstract": "Multimodal sentiment analysis aims to effectively integrate information from\nvarious sources to infer sentiment, where in many cases there are no\nannotations for unimodal labels. Therefore, most works rely on multimodal\nlabels for training. However, there exists the noisy label problem for the\nlearning of unimodal signals as multimodal annotations are not always the ideal\nsubstitutes for the unimodal ones, failing to achieve finer optimization for\nindividual modalities. In this paper, we explore the learning of unimodal\nlabels under the weak supervision from the annotated multimodal labels.\nSpecifically, we propose a novel meta uni-label generation (MUG) framework to\naddress the above problem, which leverages the available multimodal labels to\nlearn the corresponding unimodal labels by the meta uni-label correction\nnetwork (MUCN). We first design a contrastive-based projection module to bridge\nthe gap between unimodal and multimodal representations, so as to use\nmultimodal annotations to guide the learning of MUCN. Afterwards, we propose\nunimodal and multimodal denoising tasks to train MUCN with explicit supervision\nvia a bi-level optimization strategy. We then jointly train unimodal and\nmultimodal learning tasks to extract discriminative unimodal features for\nmultimodal inference. Experimental results suggest that MUG outperforms\ncompetitive baselines and can learn accurate unimodal labels.",
      "tldr_zh": "本论文针对多模态情感分析（Multimodal Sentiment Analysis）中缺乏单模态标签的问题，提出了一种基于弱监督（weak supervision）的元学习框架Meta-Learn Unimodal Signals，以从多模态标签生成准确的单模态标签。框架包括meta uni-label generation (MUG)，它利用meta uni-label correction network (MUCN)和contrastive-based projection module桥接单模态与多模态表示，并通过bi-level optimization策略训练unimodal和multimodal denoising任务，以提取判别性单模态特征。实验结果表明，MUG 优于竞争基线，能够有效学习准确的单模态标签，从而提升多模态情感分析的性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.16029v2",
      "published_date": "2024-08-28 03:43:01 UTC",
      "updated_date": "2024-09-13 02:51:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:38:09.001107"
    },
    {
      "arxiv_id": "2408.15508v2",
      "title": "EmoAttack: Utilizing Emotional Voice Conversion for Speech Backdoor Attacks on Deep Speech Classification Models",
      "title_zh": "翻译失败",
      "authors": [
        "Wenhan Yao",
        "Zedong XingXiarun Chen",
        "Jia Liu",
        "yongqiang He",
        "Weiping Wen"
      ],
      "abstract": "Deep speech classification tasks, mainly including keyword spotting and\nspeaker verification, play a crucial role in speech-based human-computer\ninteraction. Recently, the security of these technologies has been demonstrated\nto be vulnerable to backdoor attacks. Specifically speaking, speech samples are\nattacked by noisy disruption and component modification in present triggers. We\nsuggest that speech backdoor attacks can strategically focus on emotion, a\nhigher-level subjective perceptual attribute inherent in speech. Furthermore,\nwe proposed that emotional voice conversion technology can serve as the speech\nbackdoor attack trigger, and the method is called EmoAttack. Based on this, we\nconducted attack experiments on two speech classification tasks, showcasing\nthat EmoAttack method owns impactful trigger effectiveness and its remarkable\nattack success rate and accuracy variance. Additionally, the ablation\nexperiments found that speech with intensive emotion is more suitable to be\ntargeted for attacks.",
      "tldr_zh": "本文提出 EmoAttack 方法，利用 emotional voice conversion 技术作为后门攻击触发器，针对深度语音分类模型（如关键词识别和说话者验证）的安全性问题。实验结果显示，该方法在两个语音分类任务上实现了高攻击成功率和显著的准确率差异。消融实验进一步发现，强烈情感的语音更适合作为攻击目标，为语音安全研究提供了新策略。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Submitted to ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2408.15508v2",
      "published_date": "2024-08-28 03:36:43 UTC",
      "updated_date": "2024-09-06 07:46:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:38:23.990726"
    },
    {
      "arxiv_id": "2408.15507v1",
      "title": "What Machine Learning Tells Us About the Mathematical Structure of Concepts",
      "title_zh": "翻译失败",
      "authors": [
        "Jun Otsuka"
      ],
      "abstract": "This paper examines the connections among various approaches to understanding\nconcepts in philosophy, cognitive science, and machine learning, with a\nparticular focus on their mathematical nature. By categorizing these approaches\ninto Abstractionism, the Similarity Approach, the Functional Approach, and the\nInvariance Approach, the study highlights how each framework provides a\ndistinct mathematical perspective for modeling concepts. The synthesis of these\napproaches bridges philosophical theories and contemporary machine learning\nmodels, providing a comprehensive framework for future research. This work\nemphasizes the importance of interdisciplinary dialogue, aiming to enrich our\nunderstanding of the complex relationship between human cognition and\nartificial intelligence.",
      "tldr_zh": "这篇论文探讨了哲学、认知科学和机器学习中概念的数学结构，特别关注机器学习如何揭示这些结构。论文将相关方法分类为Abstractionism、the Similarity Approach、the Functional Approach 和 the Invariance Approach，每个框架提供独特的数学视角来建模概念。通过综合这些方法，研究桥接了哲学理论与当代机器学习模型，为未来研究提供全面框架，并强调跨学科对话在理解人类认知与人工智能复杂关系中的重要性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "25 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.15507v1",
      "published_date": "2024-08-28 03:30:22 UTC",
      "updated_date": "2024-08-28 03:30:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:38:33.007526"
    },
    {
      "arxiv_id": "2408.15503v5",
      "title": "RoboSense: Large-scale Dataset and Benchmark for Egocentric Robot Perception and Navigation in Crowded and Unstructured Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Haisheng Su",
        "Feixiang Song",
        "Cong Ma",
        "Wei Wu",
        "Junchi Yan"
      ],
      "abstract": "Reliable embodied perception from an egocentric perspective is challenging\nyet essential for autonomous navigation technology of intelligent mobile\nagents. With the growing demand of social robotics, near-field scene\nunderstanding becomes an important research topic in the areas of egocentric\nperceptual tasks related to navigation in both crowded and unstructured\nenvironments. Due to the complexity of environmental conditions and difficulty\nof surrounding obstacles owing to truncation and occlusion, the perception\ncapability under this circumstance is still inferior. To further enhance the\nintelligence of mobile robots, in this paper, we setup an egocentric\nmulti-sensor data collection platform based on 3 main types of sensors (Camera,\nLiDAR and Fisheye), which supports flexible sensor configurations to enable\ndynamic sight of view from ego-perspective, capturing either near or farther\nareas. Meanwhile, a large-scale multimodal dataset is constructed, named\nRoboSense, to facilitate egocentric robot perception. Specifically, RoboSense\ncontains more than 133K synchronized data with 1.4M 3D bounding box and IDs\nannotated in the full $360^{\\circ}$ view, forming 216K trajectories across 7.6K\ntemporal sequences. It has $270\\times$ and $18\\times$ as many annotations of\nsurrounding obstacles within near ranges as the previous datasets collected for\nautonomous driving scenarios such as KITTI and nuScenes. Moreover, we define a\nnovel matching criterion for near-field 3D perception and prediction metrics.\nBased on RoboSense, we formulate 6 popular tasks to facilitate the future\nresearch development, where the detailed analysis as well as benchmarks are\nalso provided accordingly. Data desensitization measures have been conducted\nfor privacy protection.",
      "tldr_zh": "本研究引入了RoboSense，这是一个大规模数据集和基准，针对拥挤和非结构化环境中的机器人自我中心视角（Egocentric Robot Perception）感知和导航问题。RoboSense基于相机、LiDAR和鱼眼镜的多传感器平台收集了超过133K同步数据，包括1.4M 3D bounding box标注和216K轨迹，覆盖全360°视图，并比现有数据集（如KITTI和nuScenes）在近距离障碍物标注上提高了270倍和18倍。研究定义了新的匹配标准和指标，并制定了6个流行任务，提供详细基准分析，以推动未来机器人感知研究的发展，同时通过数据脱敏措施确保隐私保护。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CVPR2025",
      "pdf_url": "http://arxiv.org/pdf/2408.15503v5",
      "published_date": "2024-08-28 03:17:40 UTC",
      "updated_date": "2025-03-05 05:14:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:38:45.590615"
    },
    {
      "arxiv_id": "2408.15501v1",
      "title": "MODULI: Unlocking Preference Generalization via Diffusion Models for Offline Multi-Objective Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Yifu Yuan",
        "Zhenrui Zheng",
        "Zibin Dong",
        "Jianye Hao"
      ],
      "abstract": "Multi-objective Reinforcement Learning (MORL) seeks to develop policies that\nsimultaneously optimize multiple conflicting objectives, but it requires\nextensive online interactions. Offline MORL provides a promising solution by\ntraining on pre-collected datasets to generalize to any preference upon\ndeployment. However, real-world offline datasets are often conservatively and\nnarrowly distributed, failing to comprehensively cover preferences, leading to\nthe emergence of out-of-distribution (OOD) preference areas. Existing offline\nMORL algorithms exhibit poor generalization to OOD preferences, resulting in\npolicies that do not align with preferences. Leveraging the excellent\nexpressive and generalization capabilities of diffusion models, we propose\nMODULI (Multi-objective Diffusion Planner with Sliding Guidance), which employs\na preference-conditioned diffusion model as a planner to generate trajectories\nthat align with various preferences and derive action for decision-making. To\nachieve accurate generation, MODULI introduces two return normalization methods\nunder diverse preferences for refining guidance. To further enhance\ngeneralization to OOD preferences, MODULI proposes a novel sliding guidance\nmechanism, which involves training an additional slider adapter to capture the\ndirection of preference changes. Incorporating the slider, it transitions from\nin-distribution (ID) preferences to generating OOD preferences, patching, and\nextending the incomplete Pareto front. Extensive experiments on the D4MORL\nbenchmark demonstrate that our algorithm outperforms state-of-the-art Offline\nMORL baselines, exhibiting excellent generalization to OOD preferences.",
      "tldr_zh": "该研究针对离线多目标强化学习（Offline MORL）中的偏好泛化问题，提出了一种名为 MODULI 的框架，利用扩散模型（diffusion models）作为规划器，生成与各种偏好对齐的轨迹，并通过偏好条件机制导出决策动作。MODULI 引入两种回报归一化方法来优化生成精度，并设计了滑动指导机制（sliding guidance），通过训练滑块适配器（slider adapter）捕捉偏好变化方向，从而从 in-distribution (ID) 偏好过渡到 out-of-distribution (OOD) 偏好，并扩展 Pareto front。实验结果表明，在 D4MORL 基准上，MODULI 优于现有基线算法，在 OOD 偏好泛化方面表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "23 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.15501v1",
      "published_date": "2024-08-28 03:10:45 UTC",
      "updated_date": "2024-08-28 03:10:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:38:58.878941"
    },
    {
      "arxiv_id": "2408.15498v1",
      "title": "Deep Learning to Predict Late-Onset Breast Cancer Metastasis: the Single Hyperparameter Grid Search (SHGS) Strategy for Meta Tuning Concerning Deep Feed-forward Neural Network",
      "title_zh": "翻译失败",
      "authors": [
        "Yijun Zhou",
        "Om Arora-Jain",
        "Xia Jiang"
      ],
      "abstract": "While machine learning has advanced in medicine, its widespread use in\nclinical applications, especially in predicting breast cancer metastasis, is\nstill limited. We have been dedicated to constructing a DFNN model to predict\nbreast cancer metastasis n years in advance. However, the challenge lies in\nefficiently identifying optimal hyperparameter values through grid search,\ngiven the constraints of time and resources. Issues such as the infinite\npossibilities for continuous hyperparameters like l1 and l2, as well as the\ntime-consuming and costly process, further complicate the task. To address\nthese challenges, we developed Single Hyperparameter Grid Search (SHGS)\nstrategy, serving as a preselection method before grid search. Our experiments\nwith SHGS applied to DFNN models for breast cancer metastasis prediction focus\non analyzing eight target hyperparameters: epochs, batch size, dropout, L1, L2,\nlearning rate, decay, and momentum. We created three figures, each depicting\nthe experiment results obtained from three LSM-I-10-Plus-year datasets. These\nfigures illustrate the relationship between model performance and the target\nhyperparameter values. For each hyperparameter, we analyzed whether changes in\nthis hyperparameter would affect model performance, examined if there were\nspecific patterns, and explored how to choose values for the particular\nhyperparameter. Our experimental findings reveal that the optimal value of a\nhyperparameter is not only dependent on the dataset but is also significantly\ninfluenced by the settings of other hyperparameters. Additionally, our\nexperiments suggested some reduced range of values for a target hyperparameter,\nwhich may be helpful for low-budget grid search. This approach serves as a\nprior experience and foundation for subsequent use of grid search to enhance\nmodel performance.",
      "tldr_zh": "本研究旨在使用深度前馈神经网络(DFNN)预测乳腺癌转移，但面临超参数优化效率低的问题，因此提出Single Hyperparameter Grid Search (SHGS)策略作为网格搜索(grid search)前的预选方法。SHGS针对八个关键超参数（如epochs、batch size、dropout、L1、L2、learning rate、decay和momentum）进行分析，使用三个LSM-I-10-Plus-year数据集，通过实验图表探讨每个超参数对模型性能的影响及其模式。结果显示，超参数的最优值不仅依赖于数据集，还受其他超参数设置的影响，并为低预算网格搜索提供了缩小范围的建议，从而为提升DFNN模型性能奠定基础。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.15498v1",
      "published_date": "2024-08-28 03:00:43 UTC",
      "updated_date": "2024-08-28 03:00:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:39:09.370945"
    },
    {
      "arxiv_id": "2409.07466v1",
      "title": "An Artificial Neural Network for Image Classification Inspired by Aversive Olfactory Learning Circuits in Caenorhabditis Elegans",
      "title_zh": "一种受秀丽隐杆线虫厌恶性嗅觉学习回路启发的",
      "authors": [
        "Xuebin Wang",
        "Chunxiuzi Liu",
        "Meng Zhao",
        "Ke Zhang",
        "Zengru Di",
        "He Liu"
      ],
      "abstract": "This study introduces an artificial neural network (ANN) for image\nclassification task, inspired by the aversive olfactory learning circuits of\nthe nematode Caenorhabditis elegans (C. elegans). Despite the remarkable\nperformance of ANNs in a variety of tasks, they face challenges such as\nexcessive parameterization, high training costs and limited generalization\ncapabilities. C. elegans, with its simple nervous system comprising only 302\nneurons, serves as a paradigm in neurobiological research and is capable of\ncomplex behaviors including learning. This research identifies key neural\ncircuits associated with aversive olfactory learning in C. elegans through\nbehavioral experiments and high-throughput gene sequencing, translating them\ninto an image classification ANN architecture. Additionally, two other image\nclassification ANNs with distinct architectures were constructed for\ncomparative performance analysis to highlight the advantages of bio-inspired\ndesign. The results indicate that the ANN inspired by the aversive olfactory\nlearning circuits of C. elegans achieves higher accuracy, better consistency\nand faster convergence rates in image classification task, especially when\ntackling more complex classification challenges. This study not only showcases\nthe potential of bio-inspired design in enhancing ANN capabilities but also\nprovides a novel perspective and methodology for future ANN design.",
      "tldr_zh": "本研究提出了一种受秀丽隐杆线虫（Caenorhabditis elegans）厌恶性嗅觉学习回路启发的Artificial Neural Network (ANN)，用于图像分类任务，以解决传统ANN的过度参数化、高训练成本和有限泛化能力等问题。研究团队通过行为实验和高通量基因测序识别关键神经回路，并将其转化为ANN架构，同时构建了其他架构的ANN进行比较分析。结果表明，该生物启发ANN在图像分类中实现了更高的准确率、更一致的性能以及更快的收敛速度，尤其在复杂任务中表现出优势。该工作不仅展示了生物启发设计提升ANN能力的潜力，还为未来ANN设计提供了新颖的视角和方法。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CV",
        "q-bio.NC"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.07466v1",
      "published_date": "2024-08-28 02:59:13 UTC",
      "updated_date": "2024-08-28 02:59:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:39:22.710561"
    },
    {
      "arxiv_id": "2408.15495v3",
      "title": "Remove Symmetries to Control Model Expressivity and Improve Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Liu Ziyin",
        "Yizhou Xu",
        "Isaac Chuang"
      ],
      "abstract": "When symmetry is present in the loss function, the model is likely to be\ntrapped in a low-capacity state that is sometimes known as a \"collapse\". Being\ntrapped in these low-capacity states can be a major obstacle to training across\nmany scenarios where deep learning technology is applied. We first prove two\nconcrete mechanisms through which symmetries lead to reduced capacities and\nignored features during training and inference. We then propose a simple and\ntheoretically justified algorithm, syre, to remove almost all symmetry-induced\nlow-capacity states in neural networks. When this type of entrapment is\nespecially a concern, removing symmetries with the proposed method is shown to\ncorrelate well with improved optimization or performance. A remarkable merit of\nthe proposed method is that it is model-agnostic and does not require any\nknowledge of the symmetry.",
      "tldr_zh": "这篇论文探讨了损失函数中的对称性(symmetries)如何导致神经网络模型陷入低容量状态(collapse)，从而降低模型表现力和训练效率，并证明了两种具体机制：对称性引起容量减少和特征忽略。作者提出了一种简单且理论支持的算法 syre，用于移除几乎所有对称性诱导的低容量状态。该方法在易受此问题影响的场景中显著改善优化和性能，且其模型无关优势在于无需了解具体对称性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "preprint",
      "pdf_url": "http://arxiv.org/pdf/2408.15495v3",
      "published_date": "2024-08-28 02:45:41 UTC",
      "updated_date": "2025-02-27 15:30:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:39:33.049817"
    },
    {
      "arxiv_id": "2408.15462v1",
      "title": "CTRQNets & LQNets: Continuous Time Recurrent and Liquid Quantum Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Alejandro Mayorga",
        "Alexander Yuan",
        "Andrew Yuan",
        "Tyler Wooldridge",
        "Xiaodi Wang"
      ],
      "abstract": "Neural networks have continued to gain prevalence in the modern era for their\nability to model complex data through pattern recognition and behavior\nremodeling. However, the static construction of traditional neural networks\ninhibits dynamic intelligence. This makes them inflexible to temporal changes\nin data and unfit to capture complex dependencies. With the advent of quantum\ntechnology, there has been significant progress in creating quantum algorithms.\nIn recent years, researchers have developed quantum neural networks that\nleverage the capabilities of qubits to outperform classical networks. However,\ntheir current formulation exhibits a static construction limiting the system's\ndynamic intelligence. To address these weaknesses, we develop a Liquid Quantum\nNeural Network (LQNet) and a Continuous Time Recurrent Quantum Neural Network\n(CTRQNet). Both models demonstrate a significant improvement in accuracy\ncompared to existing quantum neural networks (QNNs), achieving accuracy\nincreases as high as 40\\% on CIFAR 10 through binary classification. We propose\nLQNets and CTRQNets might shine a light on quantum machine learning's black\nbox.",
      "tldr_zh": "传统神经网络和量子神经网络(QNNs)由于静态结构，难以适应时间变化数据和捕捉复杂依赖，本文提出两种新模型：Liquid Quantum Neural Network (LQNet)和Continuous Time Recurrent Quantum Neural Network (CTRQNet)。这些模型通过动态设计显著提升了动态智能和准确性，在CIFAR 10数据集的二分类任务中，准确率较现有QNNs提高高达40%。本文的创新为量子机器学习提供更透明的框架，可能揭示其“黑箱”特性。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.15462v1",
      "published_date": "2024-08-28 00:56:03 UTC",
      "updated_date": "2024-08-28 00:56:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T19:39:45.382744"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 86,
  "processed_papers_count": 86,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-19T19:40:08.168947"
}
{
  "date": "2025-12-27",
  "category": "cs.AI",
  "summary": "ä½ å¥½ï¼æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-12-27 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\næˆ‘æ˜¯ä½ ä»¬çš„è€æœ‹å‹ï¼Œä¸“æ³¨äº arXiv æ¯æ—¥é€Ÿé€’çš„ç ”ç©¶å‘˜ã€‚\n\n**ä¸€å¥è¯æ€»ç»“ä»Šå¤©ï¼š**\nä»Šå¤©çš„ arXiv ç®€ç›´æ˜¯**ç†è®ºå…šå’Œ Agent æ¶æ„å¸ˆçš„ç››å®´**ã€‚æœ€é‡ç£…çš„æ˜¯ Naman Agarwal ç­‰äººä¸€å£æ°”è¿å‘ä¸‰ç¯‡å…³äº Transformer è´å¶æ–¯å‡ ä½•ç‰¹æ€§çš„ç†è®ºå¤§ä½œï¼Œè¯•å›¾ä»åº•å±‚è§£é‡Š Attention çš„æ•°å­¦æœ¬è´¨ï¼›æ­¤å¤–ï¼ŒAgent é¢†åŸŸçš„è®°å¿†æœºåˆ¶ï¼ˆMemento 2ï¼‰ã€é•¿æ–‡æœ¬åŸºå‡†ï¼ˆSagaScaleï¼‰ä»¥åŠ AI for Science çš„è¶…å¤§æ˜¾å¾®é•œæ¨¡å‹ï¼ˆBright 4Bï¼‰ä¹Ÿä»¤äººçœ¼å‰ä¸€äº®ã€‚\n\nä¸‹é¢æˆ‘ä»¬è¿›å…¥æ­£é¢˜ï¼Œæ·±åº¦è§£æä»Šå¤©çš„ç¡¬æ ¸è®ºæ–‡ã€‚\n\n---\n\n### ğŸš€ ç†è®ºé‡ç£…ï¼šTransformer çš„è´å¶æ–¯å‡ ä½•ä¸‰éƒ¨æ›²\nNaman Agarwal ç­‰äººå¸¦æ¥äº†ä¸€ç»„éå¸¸ç¡¬æ ¸çš„ç†è®ºåˆ†æï¼Œè¯•å›¾æ­ç¤º Transformer å†…éƒ¨æ˜¯å¦‚ä½•è¿›è¡Œæ¦‚ç‡æ¨ç†çš„ã€‚è¿™æ˜¯ä¸€å¥—ç»„åˆæ‹³ï¼Œå»ºè®®ç»“åˆé˜…è¯»ã€‚\n\n**1. [40] Attention çš„æ¢¯åº¦åŠ¨åŠ›å­¦ï¼šäº¤å‰ç†µå¦‚ä½•é›•åˆ»è´å¶æ–¯æµå½¢**\n**Gradient Dynamics of Attention: How Cross-Entropy Sculpts Bayesian Manifolds**\n*   **æ ¸å¿ƒå‘ç°**ï¼šä½œè€…æä¾›äº† Transformer æ³¨æ„åŠ›å¤´çš„ä¸€é˜¶å®Œæ•´åˆ†æã€‚ä»–ä»¬å‘ç°äº†ä¸€ä¸ª**ä¼˜åŠ¿è·¯ç”±å®šå¾‹ï¼ˆadvantage-based routing lawï¼‰**ï¼šQuery ä¼šæ›´å¼ºçƒˆåœ°è·¯ç”±åˆ°é‚£äº›è¯¯å·®ä¿¡å·é«˜äºå¹³å‡æ°´å¹³çš„ Value ä¸Šï¼Œè€Œè¿™äº› Value åˆä¼šè¢«æ‹‰å‘ä½¿ç”¨å®ƒä»¬çš„ Queryã€‚è¿™å®é™…ä¸Šæ„æˆäº†ä¸€ä¸ªç±»ä¼¼ EM ç®—æ³•ï¼ˆæœŸæœ›æœ€å¤§åŒ–ï¼‰çš„æ­£åé¦ˆå¾ªç¯ï¼Œæ¢¯åº¦ä¸‹é™æ­£æ˜¯é€šè¿‡è¿™ç§æœºåˆ¶åœ¨å†…éƒ¨é›•åˆ»å‡ºäº†ç”¨äºè´å¶æ–¯æ¨ç†çš„ä½ç»´æµå½¢ã€‚\n\n**2. [41] LLM ä¸­è´å¶æ–¯æ¨ç†çš„å‡ ä½•ç¼©æ”¾**\n**Geometric Scaling of Bayesian Inference in LLMs**\n*   **æ ¸å¿ƒå‘ç°**ï¼šç ”ç©¶äº† Pythia, Llama-3, Mistral ç­‰ç”Ÿäº§çº§æ¨¡å‹ï¼Œå‘ç°æœ«å±‚ Value çš„è¡¨ç¤ºä¼šæ²¿ç€ä¸€ä¸ªä¸»è½´ç»„ç»‡ï¼Œè¿™ä¸ªä¸»è½´çš„ä½ç½®ä¸**é¢„æµ‹ç†µï¼ˆpredictive entropyï¼‰**å¼ºç›¸å…³ã€‚è¿™è¯æ˜äº†ç°ä»£å¤§æ¨¡å‹ä¿ç•™äº†é‚£ç§èƒ½è¿›è¡Œè´å¶æ–¯æ¨ç†çš„å‡ ä½•åº•åº§ã€‚\n\n**3. [42] Transformer Attention çš„è´å¶æ–¯å‡ ä½•**\n**The Bayesian Geometry of Transformer Attention**\n*   **æ ¸å¿ƒå‘ç°**ï¼šä½œè€…æ„å»ºäº†â€œè´å¶æ–¯é£æ´â€ï¼ˆBayesian wind tunnelsï¼‰â€”â€”ä¸€ç§å·²çŸ¥çœŸå®åéªŒçš„å—æ§ç¯å¢ƒã€‚å®éªŒè¡¨æ˜ï¼Œå° Transformer èƒ½ä»¥æé«˜ç²¾åº¦å¤ç°è´å¶æ–¯åéªŒï¼Œè€Œ MLP åˆ™å®Œå…¨ä¸è¡Œã€‚ç»“è®ºæ˜¯ï¼šåˆ†å±‚ Attention é€šè¿‡å‡ ä½•è®¾è®¡å®ç°äº†è´å¶æ–¯æ¨ç†ã€‚\n\n---\n\n### ğŸ§  Agent è®°å¿†ä¸åæ€ï¼šè¿ˆå‘æ›´æ™ºèƒ½çš„ä»£ç†\nä»Šå¤©çš„ Agent è®ºæ–‡ä¸å†æ»¡è¶³äºç®€å•çš„å·¥å…·è°ƒç”¨ï¼Œè€Œæ˜¯æ·±å…¥åˆ°äº†è®¤çŸ¥æ¶æ„å±‚é¢ã€‚\n\n**4. [1] Memento 2: åŸºäºæœ‰çŠ¶æ€åå°„è®°å¿†çš„å­¦ä¹ **\n**Memento 2: Learning by Stateful Reflective Memory**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šJun Wang å›¢é˜Ÿçš„å¤§ä½œã€‚é’ˆå¯¹ LLM Agent çš„æŒç»­å­¦ä¹ é—®é¢˜ï¼Œæå‡ºäº†**æœ‰çŠ¶æ€åå°„å†³ç­–è¿‡ç¨‹ (SRDP)**ã€‚ä¸åŒäºå¾®è°ƒæƒé‡ï¼Œå®ƒè®© Agent åƒäººç±»ä¸€æ ·é€šè¿‡â€œåæ€â€è¿‡å»ç»éªŒæ¥è°ƒæ•´æœªæ¥å†³ç­–ã€‚\n*   **å…³é”®æœºåˆ¶**ï¼šå¼•å…¥äº†è¯»å†™åå°„å­¦ä¹ ç®—æ³•ï¼ˆRead-Write Reflective Learningï¼‰ï¼Œå°†è®°å¿†æ£€ç´¢çº³å…¥è½¯ç­–ç•¥è¿­ä»£è¿‡ç¨‹ä¸­ï¼Œå¹¶åœ¨ç†è®ºä¸Šè¯æ˜äº†å…¶æ”¶æ•›æ€§ã€‚è¿™æ˜¯è®© Agent å…·å¤‡â€œé•¿æœŸç»éªŒä¸»ä¹‰â€çš„é‡è¦ä¸€æ­¥ã€‚\n\n**5. [48] å•å­ä¸Šä¸‹æ–‡å·¥ç¨‹**\n**Monadic Context Engineering**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šå§šæœŸæ™ºé™¢å£«å›¢é˜Ÿå‚ä¸çš„å·¥ä½œã€‚è¿™ç¯‡æ–‡ç« è¯•å›¾ç”¨èŒƒç•´è®ºä¸­çš„**å•å­ï¼ˆMonadsï¼‰**æ¥è§„èŒƒåŒ– Agent çš„è®¾è®¡ã€‚\n*   **äº®ç‚¹**ï¼šç›®å‰çš„ Agent ä»£ç å……æ»¡äº†å„ç§æ‚ä¹±çš„çŠ¶æ€ç®¡ç†å’Œé”™è¯¯å¤„ç†ã€‚ä½œè€…æå‡ºçš„ MCE æ¶æ„åˆ©ç”¨ Functors å’Œ Monads çš„ä»£æ•°ç»“æ„æ¥å†…åœ¨åœ°ç®¡ç†çŠ¶æ€ä¼ æ’­ã€é”™è¯¯å¤„ç†å’Œå¼‚æ­¥æ‰§è¡Œã€‚è¿™å¯èƒ½ä¸ºæ„å»ºæ›´å¥å£®ã€å¯éªŒè¯çš„ Agent ç³»ç»Ÿæä¾›æ•°å­¦åŸºç¡€ã€‚\n\n---\n\n### ğŸ“‰ æ¨¡å‹ä¼˜åŒ–ä¸å‰ªæï¼šçŸ¥è¯†ä¸æŒ‡ä»¤çš„æƒè¡¡\n\n**6. [6] è„†å¼±çš„çŸ¥è¯†ï¼Œé²æ£’çš„æŒ‡ä»¤éµå¾ªï¼šLlama-3.2 ä¸­çš„å®½åº¦å‰ªæäºŒåˆ†æ³•**\n**Fragile Knowledge, Robust Instruction-Following: The Width Pruning Dichotomy in Llama-3.2**\n*   **æœ‰è¶£å‘ç°**ï¼šé€šå¸¸è®¤ä¸ºå‰ªæä¼šå…¨é¢é™ä½æ¨¡å‹èƒ½åŠ›ï¼Œä½†è¿™ç¯‡æ–‡ç« å‘ç°äº†ä¸€ä¸ªå¥‡ç‰¹çš„ç°è±¡ï¼šå¯¹ Llama-3.2 è¿›è¡Œ GLU-MLP å±‚å®½åº¦å‰ªææ—¶ï¼Œ**äº‹å®æ€§çŸ¥è¯†ï¼ˆå¦‚ MMLUï¼‰å¤§å¹…ä¸‹é™ï¼Œä½†æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼ˆInstruction-Followingï¼‰åè€Œæå‡äº† 46% åˆ° 75%**ï¼\n*   **ç»“è®º**ï¼šå‰ªæè¡¨ç°å‡ºä¸€ç§é€‰æ‹©æ€§è¿‡æ»¤ä½œç”¨ï¼Œç‰ºç‰²äº†æ­»è®°ç¡¬èƒŒçš„å‚æ•°åŒ–çŸ¥è¯†ï¼Œå´ä¿ç•™ç”šè‡³å¼ºåŒ–äº†è¡Œä¸ºå¯¹é½èƒ½åŠ›ã€‚\n\n**7. [26] å­¦ä¹ ä½•æ—¶ä¸è¿›è¡Œå…¨å±€ Attention**\n**Learning When Not to Attend Globally**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šäººç±»çœ‹ä¹¦ä¸ä¼šæ¯è¯»ä¸€ä¸ªå­—éƒ½å¾€å‰ç¿»å‡ ç™¾é¡µã€‚ä½œè€…æå‡ºçš„ **AHA (All-or-Here Attention)** æœºåˆ¶ï¼Œè®© LLM åŠ¨æ€å†³å®šä½•æ—¶éœ€è¦å…¨å±€ Attentionã€‚ç»“æœæ˜¾ç¤ºï¼Œåœ¨ 256 token çª—å£ä¸‹ï¼Œ93% çš„å…¨å±€ Attention éƒ½æ˜¯å¤šä½™çš„ï¼Œå¯ä»¥è¢«æ»‘åŠ¨çª—å£æ›¿ä»£è€Œä¸æŸå¤±æ€§èƒ½ã€‚\n\n---\n\n### ğŸ“š é•¿æ–‡æœ¬ä¸åŸºå‡†æµ‹è¯•\n\n**8. [22] SagaScale: æºè‡ªé•¿ç¯‡å°è¯´çš„çœŸå®ã€å¯æ‰©å±•ã€é«˜è´¨é‡é•¿ä¸Šä¸‹æ–‡åŸºå‡†**\n**SagaScale: A Realistic, Scalable, and High-Quality Long-Context Benchmark Built from Full-Length Novels**\n*   **ç—›ç‚¹**ï¼šç°æœ‰çš„é•¿æ–‡æœ¬æµ‹è¯•å¾€å¾€ä¸å¤ŸçœŸå®ã€‚\n*   **æ–¹æ¡ˆ**ï¼šåŸºäºå…¨æœ¬å°è¯´æ„å»ºï¼Œè‹±æ–‡å¹³å‡ 250k tokenï¼Œä¸­æ–‡ 320k tokenã€‚\n*   **å‘ç°**ï¼šç›®å‰çš„ RAG æ–¹æ³•ä¸­ï¼Œç›´æ¥æŠŠå…¨æ–‡æ‰”è¿›å»ï¼ˆå¦‚æœå¡å¾—ä¸‹ï¼‰æ•ˆæœé€šå¸¸æœ€å¥½ï¼›**Gemini-1.5-Pro** åœ¨é•¿æ–‡æœ¬å¤„ç†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¤§å¤šæ¨¡å‹ä»ç„¶æŒ£æ‰ã€‚\n\n**9. [5] TravelBench: æ›´å¹¿æ³›çš„çœŸå®ä¸–ç•Œå¤šè½®å·¥å…·ä½¿ç”¨æ—…è¡Œè§„åˆ’åŸºå‡†**\n**TravelBench: A Broader Real-World Benchmark for Multi-Turn and Tool-Using Travel Planning**\n*   **ç®€ä»‹**ï¼šä¸€ä¸ªåŒ…å«æ²™ç›’ç¯å¢ƒå’ŒçœŸå®å·¥å…·è°ƒç”¨çš„æ—…è¡Œè§„åˆ’åŸºå‡†ï¼Œæµ‹è¯• Agent çš„â€œè‡ªæ²»è§£å†³â€ã€â€œå¤šè½®äº¤äº’â€å’Œâ€œè®¤çŸ¥è¾¹ç•Œï¼ˆçŸ¥é“è‡ªå·±ä¸èƒ½åšä»€ä¹ˆï¼‰â€çš„èƒ½åŠ›ã€‚\n\n---\n\n### ğŸ”¬ AI for Science & Multimodal\n\n**10. [50] Bright 4B: æ‰©å±•ç”¨äº 3D æ˜åœºæ˜¾å¾®é•œåˆ†å‰²çš„è¶…çƒé¢å­¦ä¹ **\n**Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy**\n*   **éœ‡æ’¼å‘å¸ƒ**ï¼šè¿™æ˜¯ä¸€ä¸ª **40 äº¿å‚æ•°**çš„ç”Ÿç‰©å­¦åŸºç¡€æ¨¡å‹ï¼\n*   **èƒ½åŠ›**ï¼šå®ƒå¯ä»¥åœ¨ä¸éœ€è¦è§å…‰æŸ“è‰²ï¼ˆlabel-freeï¼‰çš„æƒ…å†µä¸‹ï¼Œç›´æ¥ä» 3D æ˜åœºæ˜¾å¾®é•œå›¾åƒä¸­åˆ†å‰²å‡ºç»†èƒæ ¸ã€çº¿ç²’ä½“ç­‰äºšç»†èƒç»“æ„ã€‚è¿™ä¸ä»…çœé’±çœäº‹ï¼Œæ›´æ˜¯ç”Ÿç‰©å›¾åƒå¤„ç†çš„ä¸€å¤§çªç ´ã€‚\n\n**11. [52] è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ä¸­â€œäººåˆ°æœºå™¨äººâ€è¿ç§»çš„æ¶Œç°**\n**Emergence of Human to Robot Transfer in Vision-Language-Action Models**\n*   **æ ¸å¿ƒè§‚ç‚¹**ï¼šSergey Levine å’Œ Chelsea Finn å›¢é˜Ÿã€‚åˆ©ç”¨æµ·é‡äººç±»è§†é¢‘æ¥è®­ç»ƒæœºå™¨äººï¼ˆVLA æ¨¡å‹ï¼‰ã€‚å‘ç°åªè¦é¢„è®­ç»ƒçš„åœºæ™¯å’Œä»»åŠ¡è¶³å¤Ÿå¤šæ ·åŒ–ï¼Œæ¨¡å‹å°±èƒ½â€œæ¶Œç°â€å‡ºå°†äººç±»è§†é¢‘ä¸­çš„çŸ¥è¯†è¿ç§»åˆ°æœºå™¨äººèº«ä¸Šçš„èƒ½åŠ›ï¼Œå“ªæ€•ä¸ç»è¿‡ä¸“é—¨çš„æ˜ å°„å·¥ç¨‹ã€‚\n\n---\n\n### ğŸ§© å…¶ä»–æœ‰è¶£çš„å‘ç° (Quick Hits)\n\n*   **[3] Learning with the p-adics**: è¿™ç¯‡æ–‡ç« è„‘æ´å¾ˆå¤§ï¼Œæ¢è®¨äº†ä¸ç”¨å®æ•° ($\\mathbb{R}$)ï¼Œè€Œæ˜¯ç”¨ **p-è¿›æ•° ($\\mathbb{Q}_p$)** å­—æ®µæ¥è¿›è¡Œæœºå™¨å­¦ä¹ ã€‚p-è¿›æ•°çš„å±‚æ¬¡ç»“æ„éå¸¸é€‚åˆå¤„ç†ä»£ç æˆ–å±‚çº§æ•°æ®ã€‚æ•°å­¦çˆ±å¥½è€…çš„ç¦éŸ³ã€‚\n*   **[7] Syntactic Framing Fragility**: è¿™æ˜¯ä¸€ä¸ªå…³äºé²æ£’æ€§çš„å®¡è®¡ã€‚å‘ç°åªè¦æ”¹å˜æç¤ºçš„å¥æ³•ï¼ˆæ¯”å¦‚æŠŠè‚¯å®šå¥æ”¹æˆå¦å®šçš„â€œä¸åº”è¯¥...â€ï¼‰ï¼Œå¾ˆå¤š LLM çš„ä¼¦ç†åˆ¤æ–­å°±ä¼šç¿»è½¬ã€‚å¼€æºæ¨¡å‹çš„è¿™ç§â€œå¥æ³•è„†å¼±æ€§â€æ˜¯å•†ä¸šæ¨¡å‹çš„ä¸¤å€ã€‚\n*   **[33] Aluminum Nanoparticle Oxidation**: AI Agent åŠ©åŠ›ææ–™ç§‘å­¦ï¼Œå‘ç°äº†é“çº³ç±³é¢—ç²’æ°§åŒ–çš„â€œçœ‹é—¨äººâ€æœºåˆ¶ï¼Œè§£å†³äº†æ•°åå¹´çš„ç§‘å­¦äº‰è®®ã€‚\n*   **[51] Nightjar**: é’ˆå¯¹ LLM æœåŠ¡çš„è‡ªé€‚åº”**æŠ•æœºè§£ç ï¼ˆSpeculative Decodingï¼‰**ã€‚å¦‚æœä¸åˆ’ç®—ï¼Œå®ƒä¼šè‡ªåŠ¨å…³é—­æŠ•æœºè§£ç ï¼Œåœ¨ååé‡å’Œå»¶è¿Ÿä¹‹é—´åŠ¨æ€å¹³è¡¡ã€‚\n\n---\n\n**ç»“è¯­**ï¼š\nä»Šå¤©çš„è®ºæ–‡è´¨é‡æé«˜ï¼Œå°¤å…¶æ˜¯ **Agarwal çš„è´å¶æ–¯ç†è®ºä¸‰éƒ¨æ›²** å’Œ **Memento 2**ï¼Œå¼ºçƒˆå»ºè®®æ·±åº¦é˜…è¯»ã€‚ç†è®ºæ­£åœ¨è¿½èµ¶å·¥ç¨‹çš„æ­¥ä¼ï¼Œè€Œ AI for Science æ­£åœ¨æ‚„æ‚„æ”¹å˜ç§‘ç ”èŒƒå¼ã€‚\n\næˆ‘ä»¬æ˜å¤©è§ï¼Happy researching!",
  "papers": [
    {
      "arxiv_id": "2512.22716v2",
      "title": "Memento 2: Learning by Stateful Reflective Memory",
      "title_zh": "Memento 2ï¼šåŸºäºæœ‰çŠ¶æ€åæ€æ€§è®°å¿†çš„å­¦ä¹ ",
      "authors": [
        "Jun Wang"
      ],
      "abstract": "We study continual learning in large language model (LLM) based agents that integrate episodic memory with reinforcement learning. We focus on reflection, the ability of an agent to revisit past experience and adjust how it selects future actions, as the central mechanism for continual adaptation without fine tuning model weights. To formalise this, we introduce the Stateful Reflective Decision Process (SRDP), in which an agent maintains and updates episodic memory and alternates between writing new experiences to memory and reading relevant cases to guide decisions. This framework casts reflective memory dynamics as part of the decision process itself and makes them amenable to control and learning analysis. Building on this formulation, we develop a Read-Write Reflective Learning algorithm that incorporates memory retrieval into a soft policy iteration procedure and prove that it converges. We further show that as memory grows and more densely covers the task environment, the resulting policy approaches optimality. Our framework unifies memory based reasoning with reinforcement learning and provides a formal foundation for LLM agents capable of continual, experience driven learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“ä¸­çš„æŒç»­å­¦ä¹ (continual learning)ï¼Œé‡ç‚¹é€šè¿‡åæ€(reflection)æœºåˆ¶ä½¿æ™ºèƒ½ä½“åœ¨ä¸å¾®è°ƒæ¨¡å‹æƒé‡çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨è¿‡å»ç»éªŒè°ƒæ•´æœªæ¥è¡Œä¸ºã€‚ä½œè€…æå‡ºäº†æœ‰çŠ¶æ€åæ€å†³ç­–è¿‡ç¨‹(Stateful Reflective Decision Process, SRDP)ï¼Œå°†æƒ…å¢ƒè®°å¿†(episodic memory)çš„åŠ¨æ€è¯»å†™è¿‡ç¨‹å½¢å¼åŒ–ä¸ºå†³ç­–ç¯èŠ‚ã€‚åŸºäºæ­¤æ¡†æ¶ï¼Œç ”ç©¶å¼€å‘äº†ä¸€ç§è¯»å†™åæ€å­¦ä¹ (Read-Write Reflective Learning)ç®—æ³•ï¼Œå°†è®°å¿†æ£€ç´¢èå…¥è½¯ç­–ç•¥è¿­ä»£(soft policy iteration)ç¨‹åºå¹¶è¯æ˜äº†å…¶æ”¶æ•›æ€§ã€‚ç†è®ºè¯æ˜æ˜¾ç¤ºï¼Œéšç€è®°å¿†å¯†åº¦çš„å¢åŠ ï¼Œæ™ºèƒ½ä½“çš„ç­–ç•¥å°†é€æ¸æ¥è¿‘æœ€ä¼˜è§£ã€‚è¯¥ç ”ç©¶æˆåŠŸå°†åŸºäºè®°å¿†çš„æ¨ç†ä¸å¼ºåŒ–å­¦ä¹ (reinforcement learning)ç›¸ç»Ÿä¸€ï¼Œä¸ºå®ç°ç»éªŒé©±åŠ¨çš„æŒç»­å­¦ä¹ æ™ºèƒ½ä½“å¥ å®šäº†åšå®çš„ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "35 pages, four figures",
      "pdf_url": "https://arxiv.org/pdf/2512.22716v2",
      "published_date": "2025-12-27 22:15:03 UTC",
      "updated_date": "2025-12-31 23:24:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:12:03.708086+00:00"
    },
    {
      "arxiv_id": "2512.22705v1",
      "title": "GHaLIB: A Multilingual Framework for Hope Speech Detection in Low-Resource Languages",
      "title_zh": "GHaLIBï¼šä¸€ç§ç”¨äºä½èµ„æºè¯­è¨€å¸Œæœ›è¨€è®ºæ£€æµ‹çš„å¤šè¯­è¨€æ¡†æ¶",
      "authors": [
        "Ahmed Abdullah",
        "Sana Fatima",
        "Haroon Mahmood"
      ],
      "abstract": "Hope speech has been relatively underrepresented in Natural Language Processing (NLP). Current studies are largely focused on English, which has resulted in a lack of resources for low-resource languages such as Urdu. As a result, the creation of tools that facilitate positive online communication remains limited. Although transformer-based architectures have proven to be effective in detecting hate and offensive speech, little has been done to apply them to hope speech or, more generally, to test them across a variety of linguistic settings. This paper presents a multilingual framework for hope speech detection with a focus on Urdu. Using pretrained transformer models such as XLM-RoBERTa, mBERT, EuroBERT, and UrduBERT, we apply simple preprocessing and train classifiers for improved results. Evaluations on the PolyHope-M 2025 benchmark demonstrate strong performance, achieving F1-scores of 95.2% for Urdu binary classification and 65.2% for Urdu multi-class classification, with similarly competitive results in Spanish, German, and English. These results highlight the possibility of implementing existing multilingual models in low-resource environments, thus making it easier to identify hope speech and helping to build a more constructive digital discourse.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GHaLIBï¼Œä¸€ä¸ªæ—¨åœ¨æ£€æµ‹ä½èµ„æºè¯­è¨€ï¼ˆLow-Resource Languagesï¼‰ä¸­å¸Œæœ›è¨€è®ºï¼ˆHope Speechï¼‰çš„å¤šè¯­è¨€æ¡†æ¶ã€‚é’ˆå¯¹å½“å‰è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸåœ¨ä¹Œå°”éƒ½è¯­ç­‰ä½èµ„æºè¯­è¨€ä¸Šèµ„æºåŒ®ä¹çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº† XLM-RoBERTaã€mBERTã€EuroBERT å’Œ UrduBERT ç­‰é¢„è®­ç»ƒ Transformer æ¨¡å‹ï¼Œå¹¶ç»“åˆç®€å•çš„é¢„å¤„ç†å’Œåˆ†ç±»å™¨è®­ç»ƒä»¥ä¼˜åŒ–ç»“æœã€‚åœ¨ PolyHope-M 2025 åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ¨¡å‹åœ¨ä¹Œå°”éƒ½è¯­äºŒåˆ†ç±»ä»»åŠ¡ä¸Šå–å¾—äº† 95.2% çš„ F1-scoreï¼Œåœ¨å¤šåˆ†ç±»ä»»åŠ¡ä¸Šè¾¾åˆ°äº† 65.2%ï¼ŒåŒæ—¶åœ¨è¥¿ç­ç‰™è¯­ã€å¾·è¯­å’Œè‹±è¯­ä¸­è¡¨ç°ä¼˜å¼‚ã€‚è¿™äº›ç»“æœè¯æ˜äº†åˆ©ç”¨ç°æœ‰å¤šè¯­è¨€æ¨¡å‹åœ¨ä½èµ„æºç¯å¢ƒä¸‹è¯†åˆ«å¸Œæœ›è¨€è®ºçš„å¯è¡Œæ€§ï¼Œæœ‰åŠ©äºæ„å»ºæ›´å…·å»ºè®¾æ€§çš„æ•°å­—åŒ–è¯è¯­ä½“ç³»ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted and presented at the 15th International Arab Conference on Information Technology (ICAIT); proceedings not yet published",
      "pdf_url": "https://arxiv.org/pdf/2512.22705v1",
      "published_date": "2025-12-27 21:23:17 UTC",
      "updated_date": "2025-12-27 21:23:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:12:03.881488+00:00"
    },
    {
      "arxiv_id": "2512.22692v1",
      "title": "Learning with the $p$-adics",
      "title_zh": "åŸºäº $p$ è¿›æ•°çš„å­¦ä¹ ",
      "authors": [
        "AndrÃ© F. T. Martins"
      ],
      "abstract": "Existing machine learning frameworks operate over the field of real numbers ($\\mathbb{R}$) and learn representations in real (Euclidean or Hilbert) vector spaces (e.g., $\\mathbb{R}^d$). Their underlying geometric properties align well with intuitive concepts such as linear separability, minimum enclosing balls, and subspace projection; and basic calculus provides a toolbox for learning through gradient-based optimization.\n  But is this the only possible choice? In this paper, we study the suitability of a radically different field as an alternative to $\\mathbb{R}$ -- the ultrametric and non-archimedean space of $p$-adic numbers, $\\mathbb{Q}_p$. The hierarchical structure of the $p$-adics and their interpretation as infinite strings make them an appealing tool for code theory and hierarchical representation learning. Our exploratory theoretical work establishes the building blocks for classification, regression, and representation learning with the $p$-adics, providing learning models and algorithms. We illustrate how simple Quillian semantic networks can be represented as a compact $p$-adic linear network, a construction which is not possible with the field of reals. We finish by discussing open problems and opportunities for future research enabled by this new framework.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨ $p$-adic æ•°åŸŸ ($\\mathbb{Q}_p$) è¿™ä¸€éé˜¿åŸºç±³å¾· (non-archimedean) è¶…åº¦é‡ (ultrametric) ç©ºé—´ä½œä¸ºå®æ•°åŸŸ ($\\mathbb{R}$) æ›¿ä»£æ–¹æ¡ˆåœ¨æœºå™¨å­¦ä¹ ä¸­çš„é€‚ç”¨æ€§ã€‚ç”±äº $p$-adics å…·æœ‰å¤©ç„¶çš„åˆ†å±‚ç»“æ„å¹¶å¯è¢«è§£é‡Šä¸ºæ— é™å­—ç¬¦ä¸²ï¼Œå®ƒä»¬åœ¨ç¼–ç ç†è®ºå’Œåˆ†å±‚è¡¨ç¤ºå­¦ä¹  (hierarchical representation learning) ä¸­å±•ç°å‡ºç‹¬ç‰¹ä¼˜åŠ¿ã€‚æœ¬æ–‡ä¸ºåŸºäº $p$-adics çš„åˆ†ç±»ã€å›å½’å’Œè¡¨ç¤ºå­¦ä¹ å»ºç«‹äº†ç†è®ºåŸºç¡€ï¼Œå¹¶æä¾›äº†ç›¸åº”çš„å­¦ä¹ æ¨¡å‹ä¸ç®—æ³•ã€‚ç ”ç©¶é€šè¿‡å®ä¾‹è¯æ˜ï¼ŒQuillian è¯­ä¹‰ç½‘ç»œ (Quillian semantic networks) å¯ä»¥è¢«è¡¨ç¤ºä¸ºç´§å‡‘çš„ $p$-adic çº¿æ€§ç½‘ç»œï¼Œè€Œè¿™ç§æ„å»ºåœ¨å®æ•°åŸŸä¸­æ˜¯æ— æ³•å®ç°çš„ã€‚è¿™é¡¹æ¢ç´¢æ€§å·¥ä½œä¸ºéæ¬§å‡ é‡Œå¾—æœºå™¨å­¦ä¹ å¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œå¹¶ä¸ºå¤„ç†å…·æœ‰å¤æ‚å±‚æ¬¡ç»“æ„çš„æ•°æ®æä¾›äº†å…¨æ–°çš„æ•°å­¦æ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DM"
      ],
      "primary_category": "cs.LG",
      "comment": "29 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.22692v1",
      "published_date": "2025-12-27 19:40:42 UTC",
      "updated_date": "2025-12-27 19:40:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:12:07.272793+00:00"
    },
    {
      "arxiv_id": "2512.22682v1",
      "title": "Conformal Prediction Sets for Next-Token Prediction in Large Language Models: Balancing Coverage Guarantees with Set Efficiency",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸‹ä¸€ä¸ªè¯å…ƒé¢„æµ‹çš„ç¬¦åˆæ€§é¢„æµ‹é›†åˆï¼šè¦†ç›–ç‡ä¿è¯ä¸é›†åˆæ•ˆç‡çš„å¹³è¡¡",
      "authors": [
        "Yoshith Roy Kotla",
        "Varshith Roy Kotla"
      ],
      "abstract": "Deploying large language models (LLMs) in high-stakes domains requires rigorous uncertainty quantification, yet standard softmax probabilities are often poorly calibrated. We present a systematic study of Adaptive Prediction Sets (APS) applied to next-token prediction in transformer-based models with large vocabularies (greater than 250,000 tokens). Our central contribution is the identification of a coverage-efficiency tradeoff: while naive conformal prediction achieves valid coverage, it produces prediction sets of hundreds of tokens, rendering them uninformative. We propose Vocabulary-Aware Conformal Prediction (VACP), a framework that leverages semantic masking and temperature-adjusted scoring to reduce the effective prediction space while provably maintaining marginal coverage. Experiments on Gemma-2B using SQUAD and WikiText benchmarks demonstrate that VACP achieves 89.7 percent empirical coverage (90 percent target) while reducing the mean prediction set size from 847 tokens to 4.3 tokens -- a 197x improvement in efficiency. We provide a theoretical analysis of vocabulary reduction and release our implementation for reproducibility.",
      "tldr_zh": "åœ¨å…³é”®é¢†åŸŸéƒ¨ç½² Large Language Models (LLMs) éœ€è¦ä¸¥æ ¼çš„ä¸ç¡®å®šæ€§é‡åŒ–ï¼Œä½†æ ‡å‡†çš„ softmax æ¦‚ç‡å¾€å¾€æ ¡å‡†ä¸ä½³ï¼Œä¸ºæ­¤è¯¥ç ”ç©¶é’ˆå¯¹è¯æ±‡é‡å·¨å¤§çš„ transformer æ¨¡å‹ç³»ç»Ÿæ¢è®¨äº† Adaptive Prediction Sets (APS) åœ¨ next-token prediction ä¸­çš„åº”ç”¨ã€‚ç ”ç©¶å‘ç°åŸå§‹çš„ conformal prediction å­˜åœ¨ coverage-efficiency ä¹‹é—´çš„æƒè¡¡éš¾é¢˜ï¼Œè™½ç„¶èƒ½ä¿è¯è¦†ç›–ç‡ï¼Œä½†ç”Ÿæˆçš„é¢„æµ‹é›†è§„æ¨¡è¿‡å¤§ä¸”ç¼ºä¹ä¿¡æ¯é‡ã€‚ä½œè€…æå‡ºäº† Vocabulary-Aware Conformal Prediction (VACP) æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆ semantic masking å’Œ temperature-adjusted scoring æŠ€æœ¯ï¼Œåœ¨è¯æ˜å¯ç»´æŒ marginal coverage çš„å‰æä¸‹æœ‰æ•ˆç¼©å°äº†é¢„æµ‹ç©ºé—´ã€‚åœ¨ Gemma-2B æ¨¡å‹ä¸Šçš„å®éªŒè¯æ˜ï¼ŒVACP åœ¨å®ç° 89.7% ç»éªŒè¦†ç›–ç‡ï¼ˆç›®æ ‡ 90%ï¼‰çš„åŒæ—¶ï¼Œå°†å¹³å‡é¢„æµ‹é›†å¤§å°ä» 847 ä¸ª token æ˜¾è‘—é™è‡³ 4.3 ä¸ªï¼Œå®ç°äº† 197 å€çš„æ•ˆç‡æå‡ã€‚è¯¥ç ”ç©¶ä¸ä»…ä¸ºè¯æ±‡ç¼©å‡æä¾›äº†ç†è®ºåˆ†æï¼Œè¿˜å‘å¸ƒäº†ç›¸å…³å®ç°ä»¥æ”¯æŒåç»­çš„å¯è§£é‡Šæ€§ç ”ç©¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 5 tables and 1 algorithm",
      "pdf_url": "https://arxiv.org/pdf/2512.22682v1",
      "published_date": "2025-12-27 19:08:54 UTC",
      "updated_date": "2025-12-27 19:08:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:12:10.858997+00:00"
    },
    {
      "arxiv_id": "2512.22673v2",
      "title": "TravelBench: A Broader Real-World Benchmark for Multi-Turn and Tool-Using Travel Planning",
      "title_zh": "TravelBenchï¼šé¢å‘å¤šè½®å¯¹è¯ä¸å·¥å…·è°ƒç”¨æ—…æ¸¸è§„åˆ’çš„æ›´å¹¿æ³›çœŸå®ä¸–ç•ŒåŸºå‡†",
      "authors": [
        "Xiang Cheng",
        "Yulan Hu",
        "Xiangwen Zhang",
        "Lu Xu",
        "Zheng Pan",
        "Xin Li",
        "Yong Liu"
      ],
      "abstract": "Travel planning is a natural real-world task to test large language models (LLMs) planning and tool-use abilities. Although prior work has studied LLM performance on travel planning, existing settings still differ from real-world needs, mainly due to limited domain coverage, insufficient modeling of users' implicit preferences in multi-turn conversations, and a lack of clear evaluation of agents' capability boundaries. To mitigate these gaps, we propose \\textbf{TravelBench}, a benchmark for fully real-world travel planning. We collect user queries, user profile and tools from real scenarios, and construct three subtasks-Single-Turn, Multi-Turn, and Unsolvable-to evaluate agent's three core capabilities in real settings: (1) solving problems autonomously, (2) interacting with users over multiple turns to refine requirements, and (3) recognizing the limits of own abilities. To enable stable tool invocation and reproducible evaluation, we cache real tool-call results and build a sandbox environment that integrates ten travel-related tools. Agents can combine these tools to solve most practical travel planning problems, and our systematic verification demonstrates the stability of the proposed benchmark. We further evaluate multiple LLMs on TravelBench and conduct an in-depth analysis of their behaviors and performance. TravelBench provides a practical and reproducible evaluation benchmark to advance research on LLM agents for travel planning.\\footnote{Our code and data will be available after internal review.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TravelBenchï¼Œä¸€ä¸ªé’ˆå¯¹çœŸå®ä¸–ç•Œå¤šè½®å¯¹è¯å’Œå·¥å…·è°ƒç”¨çš„æ—…æ¸¸è§„åˆ’åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç ”ç©¶åœ¨é¢†åŸŸè¦†ç›–èŒƒå›´ã€ç”¨æˆ·éšå¼åå¥½å»ºæ¨¡åŠæ™ºèƒ½ä½“èƒ½åŠ›è¾¹ç•Œè¯„ä¼°æ–¹é¢çš„å±€é™ã€‚TravelBench é€šè¿‡æ”¶é›†çœŸå®åœºæ™¯çš„ç”¨æˆ·æŸ¥è¯¢ã€ç”»åƒåŠå·¥å…·ï¼Œæ„å»ºäº†å•è½® (Single-Turn)ã€å¤šè½® (Multi-Turn) å’Œä¸å¯è§£ (Unsolvable) ä¸‰ä¸ªå­ä»»åŠ¡ï¼Œé‡ç‚¹è¯„ä¼°æ™ºèƒ½ä½“åœ¨è‡ªä¸»è§£å†³é—®é¢˜ã€é€šè¿‡å¤šè½®äº¤äº’ç»†åŒ–éœ€æ±‚ä»¥åŠè¯†åˆ«è‡ªèº«èƒ½åŠ›æé™è¿™ä¸‰é¡¹æ ¸å¿ƒèƒ½åŠ›ã€‚è¯¥æ¡†æ¶é›†æˆäº†ä¸€ä¸ªåŒ…å«åç§æ—…æ¸¸å·¥å…·çš„æ²™ç›’ç¯å¢ƒ (Sandbox Environment)ï¼Œå¹¶åˆ©ç”¨ç¼“å­˜çš„çœŸå®å·¥å…·è°ƒç”¨ç»“æœç¡®ä¿äº†è¯„ä¼°çš„ç¨³å®šæ€§ä¸å¯å¤ç°æ€§ã€‚å®éªŒé€šè¿‡å¯¹å¤šç§å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) çš„æ·±åº¦åˆ†æï¼ŒéªŒè¯äº†è¯¥åŸºå‡†åœ¨è¯„ä¼°æ™ºèƒ½ä½“å¤æ‚è§„åˆ’èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚TravelBench ä¸ºæ¨åŠ¨æ—…æ¸¸è§„åˆ’é¢†åŸŸæ™ºèƒ½ä½“ç ”ç©¶æä¾›äº†å…·æœ‰å®é™…åº”ç”¨ä»·å€¼çš„è¯„ä¼°åŸºå‡†ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "In progress",
      "pdf_url": "https://arxiv.org/pdf/2512.22673v2",
      "published_date": "2025-12-27 18:25:14 UTC",
      "updated_date": "2026-01-05 13:19:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:12:20.861976+00:00"
    },
    {
      "arxiv_id": "2512.22671v1",
      "title": "Fragile Knowledge, Robust Instruction-Following: The Width Pruning Dichotomy in Llama-3.2",
      "title_zh": "è„†å¼±çš„çŸ¥è¯†ä¸é²æ£’çš„æŒ‡ä»¤éµå¾ªï¼šLlama-3.2 ä¸­çš„å®½åº¦å‰ªæäºŒåˆ†æ€§",
      "authors": [
        "Pere Martra"
      ],
      "abstract": "Structured width pruning of GLU-MLP layers, guided by the Maximum Absolute Weight (MAW) criterion, reveals a systematic dichotomy in how reducing the expansion ratio affects different model capabilities. While performance on tasks relying on parametric knowledge (e.g., MMLU, GSM8K) and perplexity metrics degrades predictably, instruction-following capabilities improve substantially (+46% to +75% in IFEval for Llama-3.2-1B and 3B models), and multi-step reasoning remains robust (MUSR). This pattern challenges the prevailing assumption that pruning induces uniform degradation. We evaluated seven expansion ratio configurations using comprehensive benchmarks assessing factual knowledge, mathematical reasoning, language comprehension, instruction-following, and truthfulness. Our analysis identifies the expansion ratio as a critical architectural parameter that selectively modulates cognitive capabilities, rather than merely serving as a compression metric. We provide the first systematic characterization of this selective preservation phenomenon. Notably, we document a robust inverse correlation (r = -0.864, p = 0.012 in Llama-3B) between factual knowledge capacity (MMLU) and truthfulness metrics (TruthfulQA-MC2): as knowledge degrades, the model's ability to discriminate misconceptions improves consistently. This connects two previously distinct research areas, demonstrating that MAW-guided width pruning acts as a selective filter, reducing parametric knowledge while preserving or enhancing behavioral alignment. Additionally, we quantify context-dependent efficiency trade-offs: pruned configurations achieve up to 23% reduction in energy consumption (J/token) but incur penalties in single-request latency, whereas batch processing workloads benefit uniformly.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Llama-3.2 æ¨¡å‹ä¸­åŸºäºæœ€å¤§ç»å¯¹æƒé‡(Maximum Absolute Weight, MAW)å‡†åˆ™çš„ GLU-MLP å±‚ç»“æ„åŒ–å®½åº¦å‰ªæ(Width Pruning)æ•ˆåº”ã€‚ç ”ç©¶å‘ç°å‡å°‘æ‰©å¼ æ¯”(Expansion Ratio)å¯¹æ¨¡å‹èƒ½åŠ›çš„å½±å“å‘ˆç°å‡ºç³»ç»Ÿæ€§çš„äºŒåˆ†æ€§(Dichotomy)ï¼šå°½ç®¡ MMLU å’Œ GSM8K ç­‰ä¾èµ–å‚æ•°åŒ–çŸ¥è¯†(Parametric Knowledge)çš„ä»»åŠ¡æ€§èƒ½æœ‰æ‰€ä¸‹é™ï¼Œä½†æŒ‡ä»¤éµå¾ª(Instruction-Following)èƒ½åŠ›åœ¨ IFEval æŒ‡æ ‡ä¸Šæ˜¾è‘—æå‡äº†46%è‡³75%ï¼Œä¸”å¤šæ­¥æ¨ç†(Multi-step Reasoning)è¡¨ç°ç¨³å¥ã€‚å®éªŒè¿›ä¸€æ­¥æ­ç¤ºäº†äº‹å®çŸ¥è¯†å®¹é‡ä¸çœŸå®æ€§(Truthfulness)æŒ‡æ ‡ä¹‹é—´çš„å¼ºè´Ÿç›¸å…³æ€§ï¼Œè¡¨æ˜å®½åº¦å‰ªæå……å½“äº†é€‰æ‹©æ€§è¿‡æ»¤å™¨ï¼Œåœ¨å‰Šå‡å‚æ•°åŒ–çŸ¥è¯†çš„åŒæ—¶å¢å¼ºäº†è¡Œä¸ºå¯¹é½ã€‚æ­¤å¤–ï¼Œå‰ªæåçš„é…ç½®å¯é™ä½é«˜è¾¾23%çš„å•ä½èƒ½è€—ï¼Œå¹¶åœ¨æ‰¹é‡å¤„ç†è´Ÿè½½ä¸­è¡¨ç°å‡ºæ˜æ˜¾çš„æ•ˆç‡ä¼˜åŠ¿ã€‚è¿™é¡¹å·¥ä½œæŒ‘æˆ˜äº†å‰ªæå¯¼è‡´æ¨¡å‹èƒ½åŠ›å‡åŒ€é€€åŒ–çš„ä¼ ç»Ÿå‡è®¾ï¼Œä¸ºç†è§£æ¨¡å‹æ¶æ„å‚æ•°å¦‚ä½•é€‰æ‹©æ€§è°ƒèŠ‚è®¤çŸ¥èƒ½åŠ›æä¾›äº†ç³»ç»Ÿæ€§çš„è¡¨å¾ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "23 pages, 5 figures, 9 tables. Code available at https://github.com/peremartra/llama-glu-expansion-pruning",
      "pdf_url": "https://arxiv.org/pdf/2512.22671v1",
      "published_date": "2025-12-27 18:09:57 UTC",
      "updated_date": "2025-12-27 18:09:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:12:17.781838+00:00"
    },
    {
      "arxiv_id": "2601.09724v1",
      "title": "Syntactic Framing Fragility: An Audit of Robustness in LLM Ethical Decisions",
      "title_zh": "å¥æ³•æ¡†æ¶è„†å¼±æ€§ï¼šå¤§è¯­è¨€æ¨¡å‹ä¼¦ç†å†³ç­–çš„é²æ£’æ€§å®¡è®¡",
      "authors": [
        "Katherine Elkins",
        "Jon Chun"
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed in consequential decision-making settings, yet their robustness to benign prompt variation remains underexplored. In this work, we study whether LLMs maintain consistent ethical judgments across logically equivalent but syntactically different prompts, focusing on variations involving negation and conditional structure. We introduce Syntactic Framing Fragility (SFF), a robustness evaluation framework that isolates purely syntactic effects via Logical Polarity Normalization (LPN), enabling direct comparison of decisions across positive and negative framings without semantic drift. Auditing 23 state-of-the-art models spanning the U.S. and China as well as small U.S. open-source software models over 14 ethical scenarios and four controlled framings (39,975 decisions), we find widespread and statistically significant inconsistency: many models reverse ethical endorsements solely due to syntactic polarity, with open-source models exhibiting over twice the fragility of commercial counterparts. We further uncover extreme negation sensitivity, where some models endorse actions in 80-97% of cases when explicitly prompted with \"should not.\" We show that eliciting chain-of-thought reasoning substantially reduces fragility, identifying a practical mitigation lever, and we map fragility across scenarios, finding higher risk in financial and business contexts than in medical scenarios. Our results demonstrate that syntactic consistency constitutes a distinct and critical dimension of ethical robustness, and we argue that SFF-style audits should be a standard component of safety evaluation for deployed LLMs. Code and results will be available on github.com.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ä¼¦ç†å†³ç­–ä¸­å¯¹è‰¯æ€§æç¤ºè¯å˜ä½“ (benign prompt variation) çš„é²æ£’æ€§ï¼Œèšç„¦äºæ¨¡å‹åœ¨é€»è¾‘ç­‰ä»·ä½†å¥æ³•ä¸åŒï¼ˆå¦‚å¦å®šå’Œæ¡ä»¶ç»“æ„ï¼‰çš„æç¤ºä¸‹æ˜¯å¦èƒ½ä¿æŒä¸€è‡´åˆ¤æ–­ã€‚ä½œè€…æå‡ºäº† Syntactic Framing Fragility (SFF) è¯„ä¼°æ¡†æ¶ï¼Œåˆ©ç”¨ Logical Polarity Normalization (LPN) éš”ç¦»çº¯å¥æ³•æ•ˆåº”ï¼Œå¯¹ 23 ä¸ªæ¥è‡ªä¸­ç¾ä¸¤å›½çš„å…ˆè¿›æ¨¡å‹è¿›è¡Œäº†å¤šè¾¾ 39,975 æ¬¡å†³ç­–å®¡è®¡ã€‚å®éªŒå‘ç°æ¨¡å‹æ™®éå­˜åœ¨æ˜¾è‘—çš„å¥æ³•ä¸ä¸€è‡´æ€§ï¼Œè®¸å¤šæ¨¡å‹ä»…å› å¥æ³•ææ€§æ”¹å˜å°±åè½¬äº†ä¼¦ç†èƒŒä¹¦ï¼Œä¸”å¼€æºæ¨¡å‹çš„è„†å¼±æ€§æ˜¯å•†ä¸šæ¨¡å‹çš„ä¸¤å€ä»¥ä¸Šã€‚ç ”ç©¶æ­ç¤ºäº†æç«¯çš„ negation sensitivityï¼Œéƒ¨åˆ†æ¨¡å‹åœ¨é¢å¯¹æ˜¾å¼ \"should not\" æç¤ºæ—¶ä»æœ‰ 80-97% çš„æ¦‚ç‡æ”¯æŒç›¸å…³è¡Œä¸ºã€‚ç»“æœæ˜¾ç¤ºï¼Œå¼•å¯¼ Chain-of-Thought (CoT) æ¨ç†èƒ½æ˜¾è‘—é™ä½è¿™ç§è„†å¼±æ€§ï¼Œä¸”é‡‘èå’Œå•†ä¸šèƒŒæ™¯ä¸‹çš„é£é™©é«˜äºåŒ»ç–—åœºæ™¯ã€‚è¯¥ç ”ç©¶è¯æ˜å¥æ³•ä¸€è‡´æ€§æ˜¯ä¼¦ç†é²æ£’æ€§çš„å…³é”®ç»´åº¦ï¼Œå¹¶å»ºè®®å°† SFF å®¡è®¡ä½œä¸ºéƒ¨ç½² LLMs å®‰å…¨è¯„ä¼°çš„æ ‡å‡†ç»„æˆéƒ¨åˆ†ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.09724v1",
      "published_date": "2025-12-27 18:09:34 UTC",
      "updated_date": "2025-12-27 18:09:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:12:27.826530+00:00"
    },
    {
      "arxiv_id": "2512.22664v1",
      "title": "Unleashing Foundation Vision Models: Adaptive Transfer for Diverse Data-Limited Scientific Domains",
      "title_zh": "é‡Šæ”¾è§†è§‰åŸºç¡€æ¨¡å‹ï¼šé¢å‘å¤šæ ·åŒ–æ•°æ®å—é™ç§‘å­¦é¢†åŸŸçš„è‡ªé€‚åº”è¿ç§»",
      "authors": [
        "Qiankun Li",
        "Feng He",
        "Huabao Chen",
        "Xin Ning",
        "Kun Wang",
        "Zengfu Wang"
      ],
      "abstract": "In the big data era, the computer vision field benefits from large-scale datasets such as LAION-2B, LAION-400M, and ImageNet-21K, Kinetics, on which popular models like the ViT and ConvNeXt series have been pre-trained, acquiring substantial knowledge. However, numerous downstream tasks in specialized and data-limited scientific domains continue to pose significant challenges. In this paper, we propose a novel Cluster Attention Adapter (CLAdapter), which refines and adapts the rich representations learned from large-scale data to various data-limited downstream tasks. Specifically, CLAdapter introduces attention mechanisms and cluster centers to personalize the enhancement of transformed features through distribution correlation and transformation matrices. This enables models fine-tuned with CLAdapter to learn distinct representations tailored to different feature sets, facilitating the models' adaptation from rich pre-trained features to various downstream scenarios effectively. In addition, CLAdapter's unified interface design allows for seamless integration with multiple model architectures, including CNNs and Transformers, in both 2D and 3D contexts. Through extensive experiments on 10 datasets spanning domains such as generic, multimedia, biological, medical, industrial, agricultural, environmental, geographical, materials science, out-of-distribution (OOD), and 3D analysis, CLAdapter achieves state-of-the-art performance across diverse data-limited scientific domains, demonstrating its effectiveness in unleashing the potential of foundation vision models via adaptive transfer. Code is available at https://github.com/qklee-lz/CLAdapter.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è®¡ç®—æœºè§†è§‰åŸºç¡€æ¨¡å‹åœ¨æ•°æ®å—é™çš„ç‰¹å®šç§‘å­¦é¢†åŸŸè¿ç§»å›°éš¾çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„Cluster Attention Adapter (CLAdapter) æ¶æ„ã€‚CLAdapter é€šè¿‡å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶å’Œèšç±»ä¸­å¿ƒ(Cluster Centers)ï¼Œåˆ©ç”¨åˆ†å¸ƒç›¸å…³æ€§å’Œå˜æ¢çŸ©é˜µå¯¹è½¬æ¢åçš„ç‰¹å¾è¿›è¡Œä¸ªæ€§åŒ–å¢å¼ºï¼Œä½¿æ¨¡å‹èƒ½é’ˆå¯¹ä¸åŒç‰¹å¾é›†å­¦ä¹ ç‰¹å®šçš„è¡¨ç¤ºã€‚å…¶ç»Ÿä¸€çš„æ¥å£è®¾è®¡ç¡®ä¿äº†ä¸ CNN å’Œ Transformer æ¶æ„åœ¨ 2D åŠ 3D ä»»åŠ¡ä¸­çš„æ— ç¼é›†æˆã€‚é€šè¿‡åœ¨æ¶µç›–ç”Ÿç‰©åŒ»å­¦ã€å·¥ä¸šã€å†œä¸šã€åœ°ç†å’Œææ–™ç§‘å­¦ç­‰ 10 ä¸ªé¢†åŸŸçš„å¹¿æ³›å®éªŒï¼ŒCLAdapter åœ¨å¤šä¸ªæ•°æ®å—é™åœºæ™¯ä¸‹å‡å–å¾—äº† State-of-the-art çš„æ€§èƒ½è¡¨ç°ã€‚è¯¥ç ”ç©¶æœ‰æ•ˆè¯æ˜äº†é€šè¿‡è‡ªé€‚åº”è¿ç§»æŠ€æœ¯å¯ä»¥è¿›ä¸€æ­¥é‡Šæ”¾è§†è§‰åŸºç¡€æ¨¡å‹ (Foundation Vision Models) åœ¨ä¸“ä¸šç§‘ç ”é¢†åŸŸä¸­çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22664v1",
      "published_date": "2025-12-27 17:32:59 UTC",
      "updated_date": "2025-12-27 17:32:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:12:23.556914+00:00"
    },
    {
      "arxiv_id": "2512.22657v1",
      "title": "Investigating Deep Learning Models for Ejection Fraction Estimation from Echocardiography Videos",
      "title_zh": "æ¢ç©¶ç”¨äºè¶…å£°å¿ƒåŠ¨å›¾è§†é¢‘å°„è¡€åˆ†æ•°ä¼°ç®—çš„æ·±åº¦å­¦ä¹ æ¨¡å‹",
      "authors": [
        "Shravan Saranyan",
        "Pramit Saha"
      ],
      "abstract": "Left ventricular ejection fraction (LVEF) is a key indicator of cardiac function and plays a central role in the diagnosis and management of cardiovascular disease. Echocardiography, as a readily accessible and non-invasive imaging modality, is widely used in clinical practice to estimate LVEF. However, manual assessment of cardiac function from echocardiograms is time-consuming and subject to considerable inter-observer variability. Deep learning approaches offer a promising alternative, with the potential to achieve performance comparable to that of experienced human experts. In this study, we investigate the effectiveness of several deep learning architectures for LVEF estimation from echocardiography videos, including 3D Inception, two-stream, and CNN-RNN models. We systematically evaluate architectural modifications and fusion strategies to identify configurations that maximize prediction accuracy. Models were trained and evaluated on the EchoNet-Dynamic dataset, comprising 10,030 echocardiogram videos. Our results demonstrate that modified 3D Inception architectures achieve the best overall performance, with a root mean squared error (RMSE) of 6.79%. Across architectures, we observe a tendency toward overfitting, with smaller and simpler models generally exhibiting improved generalization. Model performance was also found to be highly sensitive to hyperparameter choices, particularly convolutional kernel sizes and normalization strategies. While this study focuses on echocardiography-based LVEF estimation, the insights gained regarding architectural design and training strategies may be applicable to a broader range of medical and non-medical video analysis tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨æ·±åº¦å­¦ä¹ (Deep Learning)æ¨¡å‹ä»è¶…å£°å¿ƒåŠ¨å›¾(Echocardiography)è§†é¢‘ä¸­è‡ªåŠ¨ä¼°ç®—å·¦å¿ƒå®¤å°„è¡€åˆ†æ•°(LVEF)çš„æœ‰æ•ˆæ€§ï¼Œæ—¨åœ¨è§£å†³ä¸´åºŠæ‰‹åŠ¨è¯„ä¼°è€—æ—¶ä¸”å­˜åœ¨è§‚å¯Ÿè€…é—´å·®å¼‚çš„é—®é¢˜ã€‚ç ”ç©¶è€…ç³»ç»Ÿåœ°è¯„ä¼°å¹¶å¯¹æ¯”äº† 3D Inceptionã€åŒæµæ¨¡å‹(Two-stream)å’Œ CNN-RNN ç­‰å¤šç§æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œå¹¶åœ¨åŒ…å« 10,030 æ®µè§†é¢‘çš„ EchoNet-Dynamic æ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒä¸éªŒè¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡æ”¹è¿›çš„ 3D Inception æ¶æ„è¡¨ç°æœ€ä¸ºå‡ºè‰²ï¼Œå…¶é¢„æµ‹çš„å‡æ–¹æ ¹è¯¯å·®(RMSE)è¾¾åˆ°äº† 6.79%ã€‚ç ”ç©¶å‘ç°æ¨¡å‹æ™®éå­˜åœ¨è¿‡æ‹Ÿåˆå€¾å‘ï¼Œä¸”è¾ƒå°ã€è¾ƒç®€å•çš„æ¨¡å‹é€šå¸¸è¡¨ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæ¨¡å‹æ€§èƒ½å¯¹å·ç§¯æ ¸å¤§å°å’Œå½’ä¸€åŒ–ç­–ç•¥ç­‰è¶…å‚æ•°çš„é€‰æ‹©å…·æœ‰é«˜åº¦æ•æ„Ÿæ€§ã€‚è¯¥é¡¹å·¥ä½œä¸ä»…ä¸ºå¿ƒè„åŠŸèƒ½è¯„ä¼°æä¾›äº†æŠ€æœ¯æ”¯æŒï¼Œå…¶å…³äºæ¶æ„è®¾è®¡å’Œè®­ç»ƒç­–ç•¥çš„å‘ç°å¯¹æ›´å¹¿æ³›çš„åŒ»å­¦å½±åƒè§†é¢‘åˆ†æä»»åŠ¡ä¹Ÿå…·æœ‰å‚è€ƒä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22657v1",
      "published_date": "2025-12-27 17:11:17 UTC",
      "updated_date": "2025-12-27 17:11:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:12:25.782476+00:00"
    },
    {
      "arxiv_id": "2512.22650v1",
      "title": "Scaling Unverifiable Rewards: A Case Study on Visual Insights",
      "title_zh": "è§„æ¨¡åŒ–ä¸å¯éªŒè¯å¥–åŠ±ï¼šä¸€é¡¹å…³äºè§†è§‰æ´å¯Ÿçš„æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Shuyu Gan",
        "James Mooney",
        "Pan Hao",
        "Renxiang Wang",
        "Mingyi Hong",
        "Qianwen Wang",
        "Dongyeop Kang"
      ],
      "abstract": "Large Language Model (LLM) agents can increasingly automate complex reasoning through Test-Time Scaling (TTS), iterative refinement guided by reward signals. However, many real-world tasks involve multi-stage pipeline whose final outcomes lack verifiable rewards or sufficient data to train robust reward models, making judge-based refinement prone to accumulate error over stages. We propose Selective TTS, a process-based refinement framework that scales inference across different stages in multi-agent pipeline, instead of repeated refinement over time by prior work. By distributing compute across stages and pruning low-quality branches early using process-specific judges, Selective TTS mitigates the judge drift and stabilizes refinement. Grounded in the data science pipeline, we build an end-to-end multi-agent pipeline for generating visually insightful charts and report of given dataset, and design a reliable LLM-based judge model, aligned with human experts (Kendall's Ï„=0.55). Our proposed selective TTS then improves insight quality under a fixed compute budget, increasing mean scores from 61.64 to 65.86 while reducing variance. We hope our findings serve as the first step toward to scaling complex, open-ended tasks with unverifiable rewards, such as scientific discovery and story generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“å¤„ç†å¤æ‚å¤šé˜¶æ®µä»»åŠ¡æ—¶ï¼Œå¦‚ä½•è§£å†³å› ç¼ºä¹å¯éªŒè¯å¥–åŠ±(unverifiable rewards)è€Œå¯¼è‡´çš„è¯¯å·®ç´¯ç§¯é—®é¢˜ã€‚ä½œè€…æå‡ºäº†Selective TTSï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè¿‡ç¨‹çš„ç»†åŒ–æ¡†æ¶ï¼Œé€šè¿‡åœ¨å¤šæ™ºèƒ½ä½“æµæ°´çº¿çš„ä¸åŒé˜¶æ®µåˆ†é…è®¡ç®—é‡ï¼Œå¹¶åˆ©ç”¨ç‰¹å®šè¿‡ç¨‹çš„è¯„åˆ¤å™¨åŠæ—©ä¿®å‰ªä½è´¨é‡åˆ†æ”¯ï¼Œå–ä»£äº†ä»¥å¾€åœ¨æ—¶é—´ç»´åº¦ä¸Šçš„é‡å¤ç»†åŒ–ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆç¼“è§£äº†è¯„åˆ¤å™¨åç§»(judge drift)å¹¶ç¨³å®šäº†æ¨ç†è¿‡ç¨‹ï¼Œç¡®ä¿äº†å¤æ‚ä»»åŠ¡çš„è¾“å‡ºè´¨é‡ã€‚å®éªŒä»¥æ•°æ®ç§‘å­¦æµæ°´çº¿ä¸­çš„è§†è§‰æ´å¯Ÿ(Visual Insights)ç”Ÿæˆä¸ºæ¡ˆä¾‹ï¼Œç»“æœæ˜¾ç¤ºSelective TTSåœ¨å›ºå®šè®¡ç®—é¢„ç®—ä¸‹å°†å¹³å‡å¾—åˆ†ä»61.64æå‡è‡³65.86ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†ç»“æœæ–¹å·®ã€‚è¿™é¡¹ç ”ç©¶ä¸ºç§‘å­¦å‘ç°å’Œæ•…äº‹ç”Ÿæˆç­‰å…·æœ‰å¼€æ”¾æ€§ä¸”å¥–åŠ±éš¾ä»¥éªŒè¯çš„ä»»åŠ¡åœ¨æ¨ç†é˜¶æ®µçš„è§„æ¨¡åŒ–æ‰©å±•(Scaling)æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "32 pages, 25 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.22650v1",
      "published_date": "2025-12-27 17:01:38 UTC",
      "updated_date": "2025-12-27 17:01:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:12:35.928810+00:00"
    },
    {
      "arxiv_id": "2512.23757v1",
      "title": "Leveraging Machine Learning for Early Detection of Lung Diseases",
      "title_zh": "åˆ©ç”¨æœºå™¨å­¦ä¹ å®ç°è‚ºéƒ¨ç–¾ç—…çš„æ—©æœŸæ£€æµ‹",
      "authors": [
        "Bahareh Rahmani",
        "Harsha Reddy Bindela",
        "Rama Kanth Reddy Gosula",
        "Krishna Yedubati",
        "Mohammad Amir Salari",
        "Leslie Hinyard",
        "Payam Norouzzadeh",
        "Eli Snir",
        "Martin Schoen"
      ],
      "abstract": "A combination of traditional image processing methods with advanced neural networks concretes a predictive and preventive healthcare paradigm. This study offers rapid, accurate, and non-invasive diagnostic solutions that can significantly impact patient outcomes, particularly in areas with limited access to radiologists and healthcare resources. In this project, deep learning methods apply in enhancing the diagnosis of respiratory diseases such as COVID-19, lung cancer, and pneumonia from chest x-rays. We trained and validated various neural network models, including CNNs, VGG16, InceptionV3, and EfficientNetB0, with high accuracy, precision, recall, and F1 scores to highlight the models' reliability and potential in real-world diagnostic applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨æœºå™¨å­¦ä¹ æŠ€æœ¯è¿›è¡Œè‚ºéƒ¨ç–¾ç—…æ—©æœŸæ£€æµ‹çš„æ–¹æ³•ï¼Œæ—¨åœ¨æ„å»ºä¸€ç§é¢„æµ‹æ€§å’Œé¢„é˜²æ€§çš„åŒ»ç–—ä¿å¥èŒƒå¼ã€‚ç ”ç©¶å°†ä¼ ç»Ÿçš„å›¾åƒå¤„ç†æ–¹æ³•ä¸å…ˆè¿›çš„ç¥ç»ç½‘ç»œç›¸ç»“åˆï¼Œå¼€å‘å‡ºé’ˆå¯¹ Chest X-rays å½±åƒçš„å¿«é€Ÿã€å‡†ç¡®ä¸”æ— åˆ›çš„è¯Šæ–­æ–¹æ¡ˆã€‚è¯¥é¡¹ç›®é‡ç‚¹åº”ç”¨ Deep Learning æ–¹æ³•æ¥å¢å¼ºå¯¹ COVID-19ã€Lung Cancer å’Œ Pneumonia ç­‰å‘¼å¸é“ç–¾ç—…çš„è¯Šæ–­èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿè®­ç»ƒå¹¶éªŒè¯äº†åŒ…æ‹¬ CNNã€VGG16ã€InceptionV3 å’Œ EfficientNetB0 åœ¨å†…çš„å¤šç§ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™äº›æ¨¡å‹åœ¨ Accuracyã€Precisionã€Recall å’Œ F1 scores æ–¹é¢å‡è¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…è¯Šæ–­åº”ç”¨ä¸­çš„å¯é æ€§ã€‚è¯¥æŠ€æœ¯å¯¹äºæ”¾å°„ç§‘åŒ»ç”Ÿå’ŒåŒ»ç–—èµ„æºåŒ®ä¹çš„åœ°åŒºå…·æœ‰é‡è¦æ„ä¹‰ï¼Œèƒ½å¤Ÿæ˜¾è‘—æ”¹å–„æ‚£è€…çš„é¢„åæ•ˆæœã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.23757v1",
      "published_date": "2025-12-27 16:50:23 UTC",
      "updated_date": "2025-12-27 16:50:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:12:52.308013+00:00"
    },
    {
      "arxiv_id": "2512.22629v1",
      "title": "DICE: Discrete Interpretable Comparative Evaluation with Probabilistic Scoring for Retrieval-Augmented Generation",
      "title_zh": "DICEï¼šé¢å‘æ£€ç´¢å¢å¼ºç”Ÿæˆçš„æ¦‚ç‡è¯„åˆ†ç¦»æ•£å¯è§£é‡Šæ€§å¯¹æ¯”è¯„ä¼°",
      "authors": [
        "Shiyan Liu",
        "Jian Ma",
        "Rui Qu"
      ],
      "abstract": "As Retrieval-Augmented Generation (RAG) systems evolve toward more sophisticated architectures, ensuring their trustworthiness through explainable and robust evaluation becomes critical. Existing scalar metrics suffer from limited interpretability, inadequate uncertainty quantification, and computational inefficiency in multi-system comparisons, hindering responsible deployment of RAG technologies. We introduce DICE (Discrete Interpretable Comparative Evaluation), a two-stage, evidence-coupled framework that advances explainability and robustness in RAG evaluation. DICE combines deep analytical reasoning with probabilistic $\\{A, B, Tie\\}$ scoring to produce transparent, confidence-aware judgments that support accountable system improvement through interpretable reasoning traces, enabling systematic error diagnosis and actionable insights. To address efficiency challenges at scale, DICE employs a Swiss-system tournament that reduces computational complexity from $O(N^2)$ to $O(N \\log N)$, achieving a 42.9% reduction in our eight-system evaluation while preserving ranking fidelity. Validation on a curated Chinese financial QA dataset demonstrates that DICE achieves 85.7% agreement with human experts, substantially outperforming existing LLM-based metrics such as RAGAS. Our results establish DICE as a responsible, explainable, and efficient paradigm for trustworthy RAG system assessment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DICEï¼ˆDiscrete Interpretable Comparative Evaluationï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æå‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿè¯„ä¼°å¯è§£é‡Šæ€§å’Œé²æ£’æ€§çš„ä¸¤é˜¶æ®µè¯æ®è€¦åˆæ¡†æ¶ï¼Œä»¥è§£å†³ç°æœ‰æ ‡é‡æŒ‡æ ‡åœ¨ä¸ç¡®å®šæ€§é‡åŒ–å’Œå¤šç³»ç»Ÿæ¯”è¾ƒæ•ˆç‡æ–¹é¢çš„ä¸è¶³ã€‚DICEæ¡†æ¶å°†æ·±åº¦åˆ†ææ¨ç†ä¸æ¦‚ç‡åŒ–çš„ $\\{A, B, Tie\\}$ è¯„åˆ†ç›¸ç»“åˆï¼Œé€šè¿‡å¯è§£é‡Šçš„æ¨ç†è½¨è¿¹æä¾›é€æ˜ä¸”æ„ŸçŸ¥ç½®ä¿¡åº¦çš„åˆ¤æ–­ï¼Œä»è€Œæ”¯æŒç³»ç»Ÿçš„ç³»ç»Ÿæ€§é”™è¯¯è¯Šæ–­ã€‚ä¸ºäº†åº”å¯¹å¤§è§„æ¨¡è¯„ä¼°çš„æ•ˆç‡æŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†ç‘å£«è½®é”¦æ ‡èµ›ï¼ˆSwiss-system tournamentï¼‰æœºåˆ¶ï¼Œå°†è®¡ç®—å¤æ‚åº¦ä» $O(N^2)$ é™ä½è‡³ $O(N \\log N)$ï¼Œåœ¨ä¿æŒæ’åä¿çœŸåº¦çš„åŒæ—¶å®ç°äº†42.9%çš„è®¡ç®—å¼€é”€ç¼©å‡ã€‚åœ¨ä¸­æ–‡é‡‘èé—®ç­”æ•°æ®é›†ä¸Šçš„éªŒè¯ç»“æœæ˜¾ç¤ºï¼ŒDICEä¸äººç±»ä¸“å®¶çš„ä¸€è‡´æ€§è¾¾åˆ°85.7%ï¼Œæ˜¾è‘—ä¼˜äºRAGASç­‰ç°æœ‰åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯„ä¼°æŒ‡æ ‡ã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºè´Ÿè´£ä»»ã€å¯è§£é‡Šä¸”é«˜æ•ˆçš„å¯ä¿¡RAGè¯„ä¼°èŒƒå¼å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at ResponsibleFM @ NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.22629v1",
      "published_date": "2025-12-27 16:02:00 UTC",
      "updated_date": "2025-12-27 16:02:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:13:24.303265+00:00"
    },
    {
      "arxiv_id": "2601.03276v1",
      "title": "Topic Segmentation Using Generative Language Models",
      "title_zh": "åŸºäºç”Ÿæˆå¼è¯­è¨€æ¨¡å‹çš„è¯é¢˜åˆ†å‰²",
      "authors": [
        "Pierre Mackenzie",
        "Maya Shah",
        "Patrick Frenett"
      ],
      "abstract": "Topic segmentation using generative Large Language Models (LLMs) remains relatively unexplored. Previous methods use semantic similarity between sentences, but such models lack the long range dependencies and vast knowledge found in LLMs. In this work, we propose an overlapping and recursive prompting strategy using sentence enumeration. We also support the adoption of the boundary similarity evaluation metric. Results show that LLMs can be more effective segmenters than existing methods, but issues remain to be solved before they can be relied upon for topic segmentation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨ç”Ÿæˆå¼å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)è¿›è¡Œè¯é¢˜åˆ†å‰²(Topic Segmentation)çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä»¥å¾€åŸºäºå¥å­è¯­ä¹‰ç›¸ä¼¼åº¦çš„æ–¹æ³•åœ¨å¤„ç†é•¿ç¨‹ä¾èµ–å’Œåˆ©ç”¨å¹¿æ³›çŸ¥è¯†æ–¹é¢çš„å±€é™æ€§ã€‚ä½œè€…æå‡ºäº†ä¸€ç§ç»“åˆå¥å­æšä¸¾çš„é‡å é€’å½’æç¤ºç­–ç•¥(overlapping and recursive prompting strategy)ï¼Œå¹¶æ”¯æŒé‡‡ç”¨è¾¹ç•Œç›¸ä¼¼åº¦è¯„ä¼°æŒ‡æ ‡(boundary similarity evaluation metric)æ¥è¡¡é‡æ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜ï¼Œç”Ÿæˆå¼å¤§è¯­è¨€æ¨¡å‹åœ¨è¯é¢˜åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ¯”ç°æœ‰æ–¹æ³•æ›´é«˜çš„æœ‰æ•ˆæ€§ã€‚å°½ç®¡å±•ç¤ºäº†æ˜¾è‘—çš„æ½œåŠ›ï¼Œä½†åœ¨å°†å…¶å¹¿æ³›åº”ç”¨äºå¯é çš„è¯é¢˜åˆ†å‰²ç³»ç»Ÿä¹‹å‰ï¼Œä»å­˜åœ¨ä¸€äº›æŠ€æœ¯éš¾é¢˜éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶å’Œè§£å†³ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.03276v1",
      "published_date": "2025-12-27 15:58:35 UTC",
      "updated_date": "2025-12-27 15:58:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:12:52.821854+00:00"
    },
    {
      "arxiv_id": "2512.22625v1",
      "title": "The Wisdom of Deliberating AI Crowds: Does Deliberation Improve LLM-Based Forecasting?",
      "title_zh": "AIç¾¤ä½“çš„å•†è®¨æ™ºæ…§ï¼šå•†è®¨æœºåˆ¶èƒ½å¦æå‡åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„é¢„æµ‹è¡¨ç°ï¼Ÿ",
      "authors": [
        "Paul Schneider",
        "Amalie Schramm"
      ],
      "abstract": "Structured deliberation has been found to improve the performance of human forecasters. This study investigates whether a similar intervention, i.e. allowing LLMs to review each other's forecasts before updating, can improve accuracy in large language models (GPT-5, Claude Sonnet 4.5, Gemini Pro 2.5). Using 202 resolved binary questions from the Metaculus Q2 2025 AI Forecasting Tournament, accuracy was assessed across four scenarios: (1) diverse models with distributed information, (2) diverse models with shared information, (3) homogeneous models with distributed information, and (4) homogeneous models with shared information. Results show that the intervention significantly improves accuracy in scenario (2), reducing Log Loss by 0.020 or about 4 percent in relative terms (p = 0.017). However, when homogeneous groups (three instances of the same model) engaged in the same process, no benefit was observed. Unexpectedly, providing LLMs with additional contextual information did not improve forecast accuracy, limiting our ability to study information pooling as a mechanism. Our findings suggest that deliberation may be a viable strategy for improving LLM forecasting.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç»“æ„åŒ–å®¡è®®ï¼ˆStructured deliberationï¼‰æ˜¯å¦èƒ½æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é¢„æµ‹å‡†ç¡®æ€§ï¼Œå®éªŒæ¶µç›–äº† GPT-5ã€Claude Sonnet 4.5 å’Œ Gemini Pro 2.5 ç­‰æ¨¡å‹ã€‚ç ”ç©¶è€…åˆ©ç”¨ Metaculus Q2 2025 AI Forecasting Tournament ä¸­çš„ 202 ä¸ªå·²è§£å†³çš„äºŒå…ƒé—®é¢˜ï¼Œåœ¨å¤šæ ·åŒ–æˆ–åŒè´¨åŒ–æ¨¡å‹ä»¥åŠåˆ†å¸ƒå¼æˆ–å…±äº«ä¿¡æ¯ç­‰å››ç§æƒ…å¢ƒä¸‹è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨å¤šæ ·åŒ–æ¨¡å‹ä¸”å…±äº«ä¿¡æ¯çš„æƒ…å¢ƒä¸­ï¼Œå…è®¸æ¨¡å‹åœ¨æ›´æ–°é¢„æµ‹å‰äº’ç›¸å®¡æŸ¥èƒ½æ˜¾è‘—æå‡å‡†ç¡®ç‡ï¼Œä½¿ Log Loss é™ä½äº†çº¦ 4%ã€‚ç„¶è€Œï¼Œå½“ç”±åŒä¸€æ¨¡å‹çš„å¤šä¸ªå®ä¾‹ç»„æˆçš„åŒè´¨åŒ–ç¾¤ä½“ï¼ˆHomogeneous groupsï¼‰è¿›è¡Œç›¸åŒçš„å®¡è®®è¿‡ç¨‹æ—¶ï¼Œå¹¶æœªè§‚å¯Ÿåˆ°æ€§èƒ½æå‡ã€‚ç ”ç©¶è¿˜æ„å¤–å‘ç°æä¾›é¢å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆContextual informationï¼‰å¹¶ä¸èƒ½æ”¹å–„é¢„æµ‹ç»“æœï¼Œè¿™é™åˆ¶äº†å¯¹ä¿¡æ¯èšåˆï¼ˆInformation poolingï¼‰æœºåˆ¶çš„æ·±å…¥ç ”ç©¶ã€‚è¯¥å‘ç°è¯æ˜äº†å®¡è®®æ˜¯æå‡ LLM é¢„æµ‹èƒ½åŠ›çš„ä¸€ç§å¯è¡Œç­–ç•¥ï¼Œä½†å…¶æœ‰æ•ˆæ€§é«˜åº¦ä¾èµ–äºæ¨¡å‹ç¾¤ä½“çš„å¤šæ ·æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages, 2 figures, 5 tables, for source code and data see https://github.com/priorb-source/delib-ai-wisdom",
      "pdf_url": "https://arxiv.org/pdf/2512.22625v1",
      "published_date": "2025-12-27 15:45:21 UTC",
      "updated_date": "2025-12-27 15:45:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:12:58.690424+00:00"
    },
    {
      "arxiv_id": "2512.22621v1",
      "title": "Chord Recognition with Deep Learning",
      "title_zh": "åŸºäºæ·±åº¦å­¦ä¹ çš„å’Œå¼¦è¯†åˆ«",
      "authors": [
        "Pierre Mackenzie"
      ],
      "abstract": "Progress in automatic chord recognition has been slow since the advent of deep learning in the field. To understand why, I conduct experiments on existing methods and test hypotheses enabled by recent developments in generative models. Findings show that chord classifiers perform poorly on rare chords and that pitch augmentation boosts accuracy. Features extracted from generative models do not help and synthetic data presents an exciting avenue for future work. I conclude by improving the interpretability of model outputs with beat detection, reporting some of the best results in the field and providing qualitative analysis. Much work remains to solve automatic chord recognition, but I hope this thesis will chart a path for others to try.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ·±åº¦å­¦ä¹ èƒŒæ™¯ä¸‹è‡ªåŠ¨å’Œå¼¦è¯†åˆ« (chord recognition) è¿›å±•ç¼“æ…¢çš„åŸå› ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†åŸºäºç”Ÿæˆæ¨¡å‹ (generative models) çš„å¤šç§å‡è®¾ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰çš„å’Œå¼¦åˆ†ç±»å™¨åœ¨å¤„ç†ç½•è§å’Œå¼¦æ—¶è¡¨ç°è¾ƒå·®ï¼Œè€ŒéŸ³é«˜å¢å¼º (pitch augmentation) æŠ€æœ¯èƒ½æœ‰æ•ˆæå‡è¯†åˆ«å‡†ç¡®ç‡ã€‚è™½ç„¶ä»ç”Ÿæˆæ¨¡å‹ä¸­æå–çš„ç‰¹å¾å¯¹æ¨¡å‹æ€§èƒ½æå‡æœ‰é™ï¼Œä½†åˆæˆæ•°æ® (synthetic data) ä¸ºè¯¥é¢†åŸŸæä¾›äº†æå…·å‰æ™¯çš„ç ”ç©¶è·¯å¾„ã€‚æ­¤å¤–ï¼Œä½œè€…åˆ©ç”¨èŠ‚æ‹æ£€æµ‹ (beat detection) æ˜¾è‘—æé«˜äº†æ¨¡å‹è¾“å‡ºçš„å¯è§£é‡Šæ€§ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†é¢†åŸŸå†…é¢†å…ˆçš„å®éªŒç»“æœã€‚é€šè¿‡å®šæ€§åˆ†æï¼Œè¯¥ç ”ç©¶ä¸ä»…ä¼˜åŒ–äº†ç°æœ‰æŠ€æœ¯ï¼Œè¿˜ä¸ºæœªæ¥è‡ªåŠ¨å’Œå¼¦è¯†åˆ«çš„ç ”ç©¶æ–¹å‘æä¾›äº†é‡è¦æŒ‡å¼•ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22621v1",
      "published_date": "2025-12-27 15:20:16 UTC",
      "updated_date": "2025-12-27 15:20:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:12:59.419937+00:00"
    },
    {
      "arxiv_id": "2512.23755v1",
      "title": "HINTS: Extraction of Human Insights from Time-Series Without External Sources",
      "title_zh": "HINTSï¼šæ— éœ€å¤–éƒ¨æ•°æ®æºçš„æ—¶é—´åºåˆ—äººç±»æ´å¯Ÿæå–",
      "authors": [
        "Sheo Yon Jhin",
        "Noseong Park"
      ],
      "abstract": "Human decision-making, emotions, and collective psychology are complex factors that shape the temporal dynamics observed in financial and economic systems. Many recent time series forecasting models leverage external sources (e.g., news and social media) to capture human factors, but these approaches incur high data dependency costs in terms of financial, computational, and practical implications. In this study, we propose HINTS, a self-supervised learning framework that extracts these latent factors endogenously from time series residuals without external data. HINTS leverages the Friedkin-Johnsen (FJ) opinion dynamics model as a structural inductive bias to model evolving social influence, memory, and bias patterns. The extracted human factors are integrated into a state-of-the-art backbone model as an attention map. Experimental results using nine real-world and benchmark datasets demonstrate that HINTS consistently improves forecasting accuracy. Furthermore, multiple case studies and ablation studies validate the interpretability of HINTS, demonstrating strong semantic alignment between the extracted factors and real-world events, demonstrating the practical utility of HINTS.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººç±»å†³ç­–ã€æƒ…æ„Ÿå’Œé›†ä½“å¿ƒç†å¯¹é‡‘èåŠç»æµæ—¶é—´åºåˆ—åŠ¨æ€çš„å½±å“ï¼Œé’ˆå¯¹ç°æœ‰æ¨¡å‹è¿‡åº¦ä¾èµ–å¤–éƒ¨æ•°æ®æºå¸¦æ¥çš„é«˜æ˜‚æˆæœ¬é—®é¢˜æå‡ºäº†æ”¹è¿›æ–¹æ¡ˆã€‚ç ”ç©¶è€…æå‡ºäº†åä¸º HINTS çš„è‡ªç›‘ç£å­¦ä¹ (self-supervised learning)æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ— éœ€å¤–éƒ¨æ•°æ®ï¼Œç›´æ¥ä»æ—¶é—´åºåˆ—æ®‹å·®ä¸­å†…ç”Ÿåœ°æå–æ½œåœ¨çš„äººç±»å› ç´ ã€‚HINTS å¼•å…¥äº† Friedkin-Johnsen (FJ) è§‚ç‚¹åŠ¨æ€æ¨¡å‹ä½œä¸ºç»“æ„æ€§å½’çº³åç½®(structural inductive bias)ï¼Œç”¨ä»¥æ¨¡æ‹Ÿä¸æ–­æ¼”å˜çš„ç¤¾ä¼šå½±å“ã€è®°å¿†å’Œåè§æ¨¡å¼ã€‚æå–å‡ºçš„äººç±»å› ç´ é€šè¿‡æ³¨æ„åŠ›å›¾(attention map)çš„å½¢å¼é›†æˆåˆ°æœ€å…ˆè¿›çš„ä¸»å¹²æ¨¡å‹ä¸­ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºäº†é¢„æµ‹æ€§èƒ½ã€‚åœ¨ä¹ä¸ªçœŸå®ä¸–ç•Œå’ŒåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒHINTS æŒç»­æå‡äº†é¢„æµ‹å‡†ç¡®æ€§ã€‚å¤šé¡¹æ¡ˆä¾‹ç ”ç©¶å’Œæ¶ˆèå®éªŒè¿›ä¸€æ­¥éªŒè¯äº†è¯¥æ¡†æ¶çš„å¯è§£é‡Šæ€§ï¼Œè¯æ˜å…¶æå–çš„å› ç´ ä¸ç°å®ä¸–ç•Œäº‹ä»¶å…·æœ‰é«˜åº¦çš„è¯­ä¹‰ä¸€è‡´æ€§ï¼Œå±•ç°äº†æå¼ºçš„å®é™…åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "AAAI 2026 AI4TS Workshop paper",
      "pdf_url": "https://arxiv.org/pdf/2512.23755v1",
      "published_date": "2025-12-27 15:13:12 UTC",
      "updated_date": "2025-12-27 15:13:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:13:03.757119+00:00"
    },
    {
      "arxiv_id": "2512.22608v2",
      "title": "Beyond Isolated Investor: Predicting Startup Success via Roleplay-Based Collective Agents",
      "title_zh": "è¶…è¶Šå•ä¸€æŠ•èµ„è€…ï¼šåŸºäºè§’è‰²æ‰®æ¼”çš„é›†ä½“æ™ºèƒ½ä½“åˆåˆ›ä¼ä¸šæˆåŠŸé¢„æµ‹",
      "authors": [
        "Zhongyang Liu",
        "Haoyu Pei",
        "Xiangyi Xiao",
        "Xiaocong Du",
        "Yihui Li",
        "Suting Hong",
        "Kunpeng Zhang",
        "Haipeng Zhang"
      ],
      "abstract": "Due to the high value and high failure rate of startups, predicting their success has become a critical challenge across interdisciplinary research. Existing approaches typically model success prediction from the perspective of a single decision-maker, overlooking the collective dynamics of investor groups that dominate real-world venture capital (VC) decisions. In this paper, we propose SimVC-CAS, a novel collective agent system that simulates VC decision-making as a multi-agent interaction process. By designing role-playing agents and a GNN-based supervised interaction module, we reformulate startup financing prediction as a group decision-making task, capturing both enterprise fundamentals and the behavioral dynamics of potential investor networks. Each agent embodies an investor with unique traits and preferences, enabling heterogeneous evaluation and realistic information exchange through a graph-structured co-investment network. Using real-world data from PitchBook and under strict data leakage controls, we show that SimVC-CAS significantly improves predictive accuracy while providing interpretable, multiperspective reasoning, for example, approximately 25% relative improvement with respect to average precision@10. SimVC-CAS also sheds light on other complex group decision scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SimVC-CASï¼Œä¸€ç§åŸºäºè§’è‰²æ‰®æ¼”(Roleplay-Based)çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡æ¨¡æ‹Ÿé£é™©æŠ•èµ„(Venture Capital, VC)å†³ç­–ä¸­çš„é›†ä½“åŠ¨æ€æ¥é¢„æµ‹åˆåˆ›ä¼ä¸šçš„æˆåŠŸã€‚ä¸åŒäºä»¥å¾€ä¾§é‡äºå•ä¸€å†³ç­–è€…çš„é¢„æµ‹æ–¹æ³•ï¼ŒSimVC-CASå°†èèµ„é¢„æµ‹å»ºæ¨¡ä¸ºç¾¤ä½“å†³ç­–ä»»åŠ¡ï¼Œç»“åˆäº†è§’è‰²æ‰®æ¼”æ™ºèƒ½ä½“ä¸åŸºäºå›¾ç¥ç»ç½‘ç»œ(GNN)çš„ç›‘ç£äº¤äº’æ¨¡å—ã€‚ç³»ç»Ÿä¸­çš„æ¯ä¸ªæ™ºèƒ½ä½“éƒ½å…·å¤‡ç‹¬ç‰¹çš„æŠ•èµ„è€…ç‰¹è´¨å’Œåå¥½ï¼Œèƒ½å¤Ÿé€šè¿‡å›¾ç»“æ„çš„å…±åŒæŠ•èµ„ç½‘ç»œè¿›è¡Œå¼‚æ„è¯„ä¼°ä¸ä¿¡æ¯äº¤æ¢ï¼Œä»è€Œæ•æ‰ä¼ä¸šåŸºæœ¬é¢åŠæŠ•èµ„è€…ç½‘ç»œçš„è¡Œä¸ºç‰¹å¾ã€‚åœ¨ä½¿ç”¨PitchBookçœŸå®æ•°æ®é›†çš„å®éªŒä¸­ï¼ŒSimVC-CASåœ¨é¢„æµ‹å‡†ç¡®ç‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå…¶Average Precision@10æŒ‡æ ‡ç›¸æ¯”åŸºçº¿æ¨¡å‹å®ç°äº†çº¦25%çš„ç›¸å¯¹æå‡ã€‚è¯¥ç ”ç©¶ä¸ä»…æå‡äº†é¢„æµ‹æ€§èƒ½ï¼Œè¿˜æä¾›äº†å¯è§£é‡Šçš„å¤šè§†è§’æ¨ç†èƒ½åŠ›ï¼Œä¸ºç†è§£å¤æ‚çš„ç¾¤ä½“å†³ç­–åœºæ™¯æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22608v2",
      "published_date": "2025-12-27 14:34:44 UTC",
      "updated_date": "2026-01-16 11:33:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:13:07.781303+00:00"
    },
    {
      "arxiv_id": "2512.22605v1",
      "title": "Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation",
      "title_zh": "é¢å‘æ³›åŒ–ä¸‹ä¸€åœ°ç‚¹æ¨èçš„å¤šæ¨¡æ€ç§»åŠ¨åŠ¨æ€å­¦ä¹ ",
      "authors": [
        "Junshu Dai",
        "Yu Wang",
        "Tongya Zheng",
        "Wei Ji",
        "Qinghong Guo",
        "Ji Cao",
        "Jie Song",
        "Canghong Jin",
        "Mingli Song"
      ],
      "abstract": "The precise prediction of human mobility has produced significant socioeconomic impacts, such as location recommendations and evacuation suggestions. However, existing methods suffer from limited generalization capability: unimodal approaches are constrained by data sparsity and inherent biases, while multi-modal methods struggle to effectively capture mobility dynamics caused by the semantic gap between static multi-modal representation and spatial-temporal dynamics. Therefore, we leverage multi-modal spatial-temporal knowledge to characterize mobility dynamics for the location recommendation task, dubbed as \\textbf{M}ulti-\\textbf{M}odal \\textbf{Mob}ility (\\textbf{M}$^3$\\textbf{ob}). First, we construct a unified spatial-temporal relational graph (STRG) for multi-modal representation, by leveraging the functional semantics and spatial-temporal knowledge captured by the large language models (LLMs)-enhanced spatial-temporal knowledge graph (STKG). Second, we design a gating mechanism to fuse spatial-temporal graph representations of different modalities, and propose an STKG-guided cross-modal alignment to inject spatial-temporal dynamic knowledge into the static image modality. Extensive experiments on six public datasets show that our proposed method not only achieves consistent improvements in normal scenarios but also exhibits significant generalization ability in abnormal scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† M$^3$ob (Multi-Modal Mobility) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç§»åŠ¨æ€§é¢„æµ‹æ–¹æ³•åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šçš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯å…‹æœé™æ€å¤šæ¨¡æ€è¡¨ç¤ºä¸æ—¶ç©ºåŠ¨æ€ä¹‹é—´çš„è¯­ä¹‰é¸¿æ²Ÿ (semantic gap)ã€‚ç ”ç©¶è€…é¦–å…ˆåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) å¢å¼ºçš„æ—¶ç©ºçŸ¥è¯†å›¾è°± (STKG) æ•æ‰åŠŸèƒ½è¯­ä¹‰å’Œæ—¶ç©ºçŸ¥è¯†ï¼Œå¹¶ä»¥æ­¤æ„å»ºäº†ç»Ÿä¸€çš„æ—¶ç©ºå…³ç³»å›¾ (STRG) è¿›è¡Œå¤šæ¨¡æ€è¡¨ç¤ºã€‚è¯¥æ¡†æ¶é€šè¿‡é—¨æ§æœºåˆ¶ (gating mechanism) èåˆä¸åŒæ¨¡æ€çš„æ—¶ç©ºå›¾è¡¨ç¤ºï¼Œå¹¶åˆ›æ–°æ€§åœ°æå‡ºç”± STKG å¼•å¯¼çš„è·¨æ¨¡æ€å¯¹é½ (cross-modal alignment) æŠ€æœ¯ï¼Œå°†æ—¶ç©ºåŠ¨æ€çŸ¥è¯†æ³¨å…¥é™æ€å›¾åƒæ¨¡æ€ã€‚åœ¨å…­ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒM$^3$ob ä¸ä»…åœ¨å¸¸è§„æ¨èåœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œåœ¨å¼‚å¸¸åœºæ™¯ä¸‹ä¹Ÿå±•ç°å‡ºæ˜¾è‘—çš„æ³›åŒ–æ€§èƒ½ï¼Œä¸ºç²¾å‡†çš„ä¸‹ä¸€åœ°ç‚¹æ¨è (Next Location Recommendation) æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22605v1",
      "published_date": "2025-12-27 14:23:04 UTC",
      "updated_date": "2025-12-27 14:23:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:13:16.095049+00:00"
    },
    {
      "arxiv_id": "2512.22601v1",
      "title": "Tyee: A Unified, Modular, and Fully-Integrated Configurable Toolkit for Intelligent Physiological Health Care",
      "title_zh": "Tyeeï¼šé¢å‘æ™ºèƒ½ç”Ÿç†åŒ»ç–—ä¿å¥çš„ç»Ÿä¸€ã€æ¨¡å—åŒ–ã€å…¨é›†æˆå¯é…ç½®å·¥å…·åŒ…",
      "authors": [
        "Tao Zhou",
        "Lingyu Shu",
        "Zixing Zhang",
        "Jing Han"
      ],
      "abstract": "Deep learning has shown great promise in physiological signal analysis, yet its progress is hindered by heterogeneous data formats, inconsistent preprocessing strategies, fragmented model pipelines, and non-reproducible experimental setups. To address these limitations, we present Tyee, a unified, modular, and fully-integrated configurable toolkit designed for intelligent physiological healthcare. Tyee introduces three key innovations: (1) a unified data interface and configurable preprocessing pipeline for 12 kinds of signal modalities; (2) a modular and extensible architecture enabling flexible integration and rapid prototyping across tasks; and (3) end-to-end workflow configuration, promoting reproducible and scalable experimentation. Tyee demonstrates consistent practical effectiveness and generalizability, outperforming or matching baselines across all evaluated tasks (with state-of-the-art results on 12 of 13 datasets). The Tyee toolkit is released at https://github.com/SmileHnu/Tyee and actively maintained.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿç†ä¿¡å·åˆ†æä¸­å­˜åœ¨çš„æ•°æ®æ ¼å¼å¼‚æ„ã€é¢„å¤„ç†ç­–ç•¥ä¸ä¸€è‡´åŠå®éªŒä¸å¯å¤ç°ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº† Tyee è¿™ä¸€ç»Ÿä¸€ã€æ¨¡å—åŒ–ä¸”å…¨é›†æˆçš„å¯é…ç½®å·¥å…·åŒ…ã€‚Tyee å¼•å…¥äº†ç»Ÿä¸€çš„æ•°æ®æ¥å£å’Œå¯é…ç½®çš„é¢„å¤„ç†æµæ°´çº¿(Preprocessing Pipeline)ï¼Œèƒ½å¤Ÿé«˜æ•ˆå¤„ç† 12 ç§ä¸åŒç±»å‹çš„ä¿¡å·æ¨¡æ€ã€‚è¯¥å·¥å…·åŒ…é‡‡ç”¨æ¨¡å—åŒ–ä¸”å¯æ‰©å±•çš„æ¶æ„ï¼Œæ”¯æŒè·¨ä»»åŠ¡çš„çµæ´»é›†æˆä¸å¿«é€ŸåŸå‹å¼€å‘(Rapid Prototyping)ï¼Œå¹¶æä¾›ç«¯åˆ°ç«¯çš„æµæ°´çº¿é…ç½®(End-to-end Workflow Configuration)ä»¥ç¡®ä¿å®éªŒçš„å¯å¤ç°æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTyee åœ¨æ‰€æœ‰è¯„ä¼°ä»»åŠ¡ä¸­å‡è¾¾åˆ°æˆ–è¶…è¿‡äº†åŸºçº¿æ¨¡å‹æ°´å¹³ï¼Œå¹¶åœ¨ 13 ä¸ªæ•°æ®é›†ä¸­çš„ 12 ä¸ªä¸Šå–å¾—äº†æœ€å…ˆè¿›(State-of-the-art)çš„æ€§èƒ½è¡¨ç°ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº† Tyee åœ¨æ™ºèƒ½ç”Ÿç†åŒ»ç–—å¥åº·é¢†åŸŸçš„é€šç”¨æ€§ä¸å®ç”¨ä»·å€¼ï¼Œä¸”ç›¸å…³ä»£ç å·²åœ¨ GitHub å¼€æºå¹¶ç”±å›¢é˜ŸæŒç»­ç»´æŠ¤ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22601v1",
      "published_date": "2025-12-27 14:14:01 UTC",
      "updated_date": "2025-12-27 14:14:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:13:18.055930+00:00"
    },
    {
      "arxiv_id": "2601.11568v1",
      "title": "AdaFRUGAL: Adaptive Memory-Efficient Training with Dynamic Control",
      "title_zh": "AdaFRUGALï¼šåŸºäºåŠ¨æ€æ§åˆ¶çš„è‡ªé€‚åº”å†…å­˜é«˜æ•ˆè®­ç»ƒ",
      "authors": [
        "Quang-Hung Bui",
        "Anh Son Ta"
      ],
      "abstract": "Training Large Language Models (LLMs) is highly memory-intensive due to optimizer state overhead. The FRUGAL framework mitigates this with gradient splitting, but its static hyperparameters -- the subspace ratio ($Ï$) and update frequency ($T$) -- require costly manual tuning, limiting adaptability. We present AdaFRUGAL, which automates this process by introducing two dynamic controls: (i) a linear decay for $Ï$ to progressively reduce memory, and (ii) a loss-aware schedule for $T$ to lower computational overhead. Experiments across large-scale pre-training (English C4, Vietnamese VietVault) and fine-tuning (GLUE) demonstrate that AdaFRUGAL achieves a compelling trade-off. It maintains competitive performance against AdamW and static FRUGAL while significantly reducing both GPU memory and training time, offering a more practical, autonomous solution for resource-constrained LLM training.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AdaFRUGALï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰åŠ¨æ€æ§åˆ¶èƒ½åŠ›çš„è‡ªé€‚åº”å†…å­˜é«˜æ•ˆè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è®­ç»ƒä¸­ä¼˜åŒ–å™¨çŠ¶æ€å¸¦æ¥çš„é«˜å†…å­˜å ç”¨é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰ FRUGAL æ¡†æ¶ä¸­å­ç©ºé—´æ¯”ä¾‹ ($Ï$) å’Œæ›´æ–°é¢‘ç‡ ($T$) ç­‰é™æ€è¶…å‚æ•°éœ€è¦æ˜‚è´µçš„äººå·¥è°ƒä¼˜ä¸”ç¼ºä¹é€‚åº”æ€§çš„å±€é™æ€§ï¼ŒAdaFRUGAL å¼•å…¥äº†ä¸¤ç§åŠ¨æ€æ§åˆ¶æœºåˆ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¯¹ $Ï$ é‡‡ç”¨çº¿æ€§è¡°å‡ï¼ˆlinear decayï¼‰ç­–ç•¥æ¥é€æ­¥å‡å°‘å†…å­˜éœ€æ±‚ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ„ŸçŸ¥æŸå¤±ï¼ˆloss-awareï¼‰çš„ $T$ è°ƒåº¦æ–¹æ¡ˆä»¥é™ä½è®¡ç®—å¼€é”€ã€‚å®éªŒæ¶µç›–äº†å¤§è§„æ¨¡é¢„è®­ç»ƒï¼ˆEnglish C4, Vietnamese VietVaultï¼‰å’Œå¾®è°ƒï¼ˆGLUEï¼‰ä»»åŠ¡ï¼Œç»“æœè¯æ˜ AdaFRUGAL åœ¨ä¿æŒä¸ AdamW å’Œé™æ€ FRUGAL ç›¸å½“æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº† GPU å†…å­˜å ç”¨å’Œè®­ç»ƒæ—¶é—´ã€‚è¿™ä¸€ç ”ç©¶ä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„ LLM è®­ç»ƒæä¾›äº†ä¸€ç§æ›´å…·å®è·µæ„ä¹‰ä¸”é«˜åº¦è‡ªåŠ¨åŒ–çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.11568v1",
      "published_date": "2025-12-27 14:11:08 UTC",
      "updated_date": "2025-12-27 14:11:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:13:17.571483+00:00"
    },
    {
      "arxiv_id": "2512.22579v1",
      "title": "SANet: A Semantic-aware Agentic AI Networking Framework for Cross-layer Optimization in 6G",
      "title_zh": "SANetï¼šé¢å‘6Gè·¨å±‚ä¼˜åŒ–çš„è¯­ä¹‰æ„ŸçŸ¥æ™ºèƒ½ä½“åŒ–äººå·¥æ™ºèƒ½ç½‘ç»œæ¡†æ¶",
      "authors": [
        "Yong Xiao",
        "Xubo Li",
        "Haoran Zhou",
        "Yingyu Li",
        "Yayu Gao",
        "Guangming Shi",
        "Ping Zhang",
        "Marwan Krunz"
      ],
      "abstract": "Agentic AI networking (AgentNet) is a novel AI-native networking paradigm in which a large number of specialized AI agents collaborate to perform autonomous decision-making, dynamic environmental adaptation, and complex missions. It has the potential to facilitate real-time network management and optimization functions, including self-configuration, self-optimization, and self-adaptation across diverse and complex environments. This paper proposes SANet, a novel semantic-aware AgentNet architecture for wireless networks that can infer the semantic goal of the user and automatically assign agents associated with different layers of the network to fulfill the inferred goal. Motivated by the fact that AgentNet is a decentralized framework in which collaborating agents may generally have different and even conflicting objectives, we formulate the decentralized optimization of SANet as a multi-agent multi-objective problem, and focus on finding the Pareto-optimal solution for agents with distinct and potentially conflicting objectives. We propose three novel metrics for evaluating SANet. Furthermore, we develop a model partition and sharing (MoPS) framework in which large models, e.g., deep learning models, of different agents can be partitioned into shared and agent-specific parts that are jointly constructed and deployed according to agents' local computational resources. Two decentralized optimization algorithms are proposed. We derive theoretical bounds and prove that there exists a three-way tradeoff among optimization, generalization, and conflicting errors. We develop an open-source RAN and core network-based hardware prototype that implements agents to interact with three different layers of the network. Experimental results show that the proposed framework achieved performance gains of up to 14.61% while requiring only 44.37% of FLOPs required by state-of-the-art algorithms.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†SANetï¼Œä¸€ç§é¢å‘6Gçš„è¯­ä¹‰æ„ŸçŸ¥æ™ºèƒ½ä½“AIç½‘ç»œ(Agentic AI Networking)æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å¤æ‚ç¯å¢ƒä¸‹çš„è·¨å±‚ä¼˜åŒ–ã€‚é’ˆå¯¹AgentNetä¸­å¤šä¸ªåä½œæ™ºèƒ½ä½“å¯èƒ½å­˜åœ¨çš„å†²çªç›®æ ‡ï¼Œè¯¥æ¡†æ¶å°†å»ä¸­å¿ƒåŒ–ä¼˜åŒ–å»ºæ¨¡ä¸ºå¤šæ™ºèƒ½ä½“å¤šç›®æ ‡é—®é¢˜ï¼Œæ—¨åœ¨å¯»æ‰¾å¸•ç´¯æ‰˜æœ€ä¼˜è§£(Pareto-optimal solution)ã€‚ç ”ç©¶è¿˜å¼€å‘äº†æ¨¡å‹åˆ†åŒºä¸å…±äº«(MoPS)æ¡†æ¶ï¼Œé€šè¿‡å°†æ·±åº¦å­¦ä¹ æ¨¡å‹åˆ’åˆ†ä¸ºå…±äº«ä¸ç‰¹å®šéƒ¨åˆ†ï¼Œå®ç°äº†æ¨¡å‹åœ¨æœ‰é™è®¡ç®—èµ„æºä¸‹çš„é«˜æ•ˆéƒ¨ç½²ã€‚ç†è®ºä¸Šï¼Œè¯¥ç ”ç©¶æ¨å¯¼å¹¶è¯æ˜äº†ä¼˜åŒ–ã€æ³›åŒ–ä¸å†²çªè¯¯å·®ä¹‹é—´çš„ä¸‰å‘æƒè¡¡å…³ç³»ã€‚é€šè¿‡åœ¨æ— çº¿æ¥å…¥ç½‘(RAN)å’Œæ ¸å¿ƒç½‘ç¡¬ä»¶åŸå‹ä¸Šçš„å®éªŒéªŒè¯ï¼Œç»“æœè¡¨æ˜SANetåœ¨ä»…éœ€åŸºå‡†ç®—æ³•44.37%æµ®ç‚¹è¿ç®—é‡(FLOPs)çš„å‰æä¸‹ï¼Œå®ç°äº†é«˜è¾¾14.61%çš„æ€§èƒ½å¢ç›Šã€‚",
      "categories": [
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.AI",
      "comment": "Submitted to IEEE Transactions on Mobile Computing",
      "pdf_url": "https://arxiv.org/pdf/2512.22579v1",
      "published_date": "2025-12-27 12:42:47 UTC",
      "updated_date": "2025-12-27 12:42:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:13:36.202303+00:00"
    },
    {
      "arxiv_id": "2601.09723v1",
      "title": "SagaScale: A Realistic, Scalable, and High-Quality Long-Context Benchmark Built from Full-Length Novels",
      "title_zh": "SagaScaleï¼šåŸºäºé•¿ç¯‡å°è¯´æ„å»ºçš„çœŸå®ã€å¯æ‰©å±•ä¸”é«˜è´¨é‡çš„é•¿ä¸Šä¸‹æ–‡è¯„æµ‹åŸºå‡†",
      "authors": [
        "Guancheng Du",
        "Yong Hu",
        "Wenqing Wang",
        "Yaming Yang",
        "Jiaheng Gao"
      ],
      "abstract": "Large Language Models (LLMs) have shown significant progress, but understanding long and complex documents remains challenging. Many long-context benchmarks have been proposed, but they face several limitations, including task realism, data scalability, and data quality. To this end, we introduce SagaScale, a realistic, scalable, and high-quality long-context benchmark built from full-length novels. The entire benchmark is constructed using an automated data collection pipeline that utilizes external resources (e.g., Wikipedia pages) to curate question-answer pairs. Critically, these external resources are provided only for benchmark construction and not during evaluation, which allows LLMs to curate complex questions that go beyond what they can answer during evaluation. SagaScale is also bilingual and offers the largest context length to date, with average token counts exceeding 250K for English novels and 320K for Chinese novels. Our evaluation across 12 frontier LLMs and three long-context methods -- NaÃ¯ve RAG, Agentic RAG, and Long Context -- yields key insights, including: (1) Directly supplying the full context to the LLM can outperform other methods by a large margin; (2) Most LLMs still struggle with lengthy contexts, but Gemini-2.5-Pro stands out as an exception; and (3) Agentic RAG effectively addresses the retrieval bottleneck in NaÃ¯ve RAG. Finally, we publicly release the SagaScale benchmark and our data collection codebase to facilitate future research.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SagaScaleï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå®Œæ•´é•¿ç¯‡å°è¯´æ„å»ºçš„çœŸå®ã€å¯æ‰©å±•ä¸”é«˜è´¨é‡çš„é•¿æ–‡æœ¬ Long-Context åŸºå‡†æµ‹è¯• Benchmarkã€‚ä¸ºäº†è§£å†³ç°æœ‰åŸºå‡†åœ¨ä»»åŠ¡çœŸå®æ€§ã€æ•°æ®è§„æ¨¡å’Œè´¨é‡æ–¹é¢çš„å±€é™æ€§ï¼ŒSagaScale åˆ©ç”¨è‡ªåŠ¨åŒ–æµæ°´çº¿å¹¶ç»“åˆ Wikipedia ç­‰å¤–éƒ¨èµ„æºæ¥ç”Ÿæˆå¤æ‚çš„é—®ç­”å¯¹ï¼Œç¡®ä¿é—®é¢˜éš¾åº¦è¶…è¶Šäº†æ¨¡å‹åœ¨è¯„ä¼°æ—¶çš„ç›´æ¥å¤„ç†èƒ½åŠ›ã€‚è¯¥åŸºå‡†æ”¯æŒä¸­è‹±åŒè¯­ï¼Œå…¶è‹±æ–‡å’Œä¸­æ–‡å°è¯´çš„å¹³å‡é•¿åº¦åˆ†åˆ«è¶…è¿‡äº† 250K å’Œ 320K Tokenï¼Œæä¾›äº†ç›®å‰æœ€å¤§çš„ä¸Šä¸‹æ–‡æµ‹è¯•é•¿åº¦ã€‚ç ”ç©¶å›¢é˜Ÿå¯¹ 12 ç§å‰æ²¿å¤§è¯­è¨€æ¨¡å‹ LLMs ä»¥åŠ NaÃ¯ve RAGã€Agentic RAG å’Œ Long Context æ–¹æ³•è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç›´æ¥å‘æ¨¡å‹æä¾›å®Œæ•´ä¸Šä¸‹æ–‡çš„è¡¨ç°é€šå¸¸æ˜¾è‘—ä¼˜äºæ£€ç´¢æ–¹æ³•ï¼Œä¸” Agentic RAG èƒ½æœ‰æ•ˆè§£å†³ NaÃ¯ve RAG çš„æ£€ç´¢ç“¶é¢ˆã€‚å°½ç®¡å¤§å¤šæ•°æ¨¡å‹åœ¨å¤„ç†æé•¿æ–‡æœ¬æ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œä½† Gemini-2.5-Pro åœ¨æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¯¥ç ”ç©¶ä¸ºé•¿æ–‡æœ¬ç†è§£é¢†åŸŸæä¾›äº†é‡è¦çš„è¯„ä¼°å·¥å…·å’Œç ”ç©¶æ´å¯Ÿã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.09723v1",
      "published_date": "2025-12-27 12:19:55 UTC",
      "updated_date": "2025-12-27 12:19:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:13:42.574033+00:00"
    },
    {
      "arxiv_id": "2512.22568v1",
      "title": "Lessons from Neuroscience for AI: How integrating Actions, Compositional Structure and Episodic Memory could enable Safe, Interpretable and Human-Like AI",
      "title_zh": "ç¥ç»ç§‘å­¦å¯¹ AI çš„å¯ç¤ºï¼šæ•´åˆè¡ŒåŠ¨ã€ç»„åˆç»“æ„ä¸æƒ…æ™¯è®°å¿†å¦‚ä½•å®ç°å®‰å…¨ã€å¯è§£é‡Šä¸”ç±»äººçš„äººå·¥æ™ºèƒ½",
      "authors": [
        "Rajesh P. N. Rao",
        "Vishwas Sathish",
        "Linxing Preston Jiang",
        "Matthew Bryan",
        "Prashant Rangarajan"
      ],
      "abstract": "The phenomenal advances in large language models (LLMs) and other foundation models over the past few years have been based on optimizing large-scale transformer models on the surprisingly simple objective of minimizing next-token prediction loss, a form of predictive coding that is also the backbone of an increasingly popular model of brain function in neuroscience and cognitive science. However, current foundation models ignore three other important components of state-of-the-art predictive coding models: tight integration of actions with generative models, hierarchical compositional structure, and episodic memory. We propose that to achieve safe, interpretable, energy-efficient, and human-like AI, foundation models should integrate actions, at multiple scales of abstraction, with a compositional generative architecture and episodic memory. We present recent evidence from neuroscience and cognitive science on the importance of each of these components. We describe how the addition of these missing components to foundation models could help address some of their current deficiencies: hallucinations and superficial understanding of concepts due to lack of grounding, a missing sense of agency/responsibility due to lack of control, threats to safety and trustworthiness due to lack of interpretability, and energy inefficiency. We compare our proposal to current trends, such as adding chain-of-thought (CoT) reasoning and retrieval-augmented generation (RAG) to foundation models, and discuss new ways of augmenting these models with brain-inspired components. We conclude by arguing that a rekindling of the historically fruitful exchange of ideas between brain science and AI will help pave the way towards safe and interpretable human-centered AI.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†ç¥ç»ç§‘å­¦ä¸ºäººå·¥æ™ºèƒ½å‘å±•æä¾›çš„æ·±åˆ»å¯ç¤ºï¼ŒæŒ‡å‡ºå½“å‰çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)è™½ç„¶åŸºäºPredictive CodingåŸåˆ™å–å¾—äº†å·¨å¤§è¿›å±•ï¼Œä½†ä»å­˜åœ¨å¹»è§‰ã€ç¼ºä¹Groundingã€ç¼ºä¹Agencyæ„ŸåŠèƒ½æ•ˆä½ä¸‹ç­‰å±€é™æ€§ã€‚ä¸ºå®ç°å®‰å…¨ã€å¯è§£é‡Šä¸”ç±»äººåŒ–çš„AIï¼Œä½œè€…æå‡ºåº”å°†Actionsï¼ˆè¡ŒåŠ¨ï¼‰ã€Hierarchical Compositional Structureï¼ˆå±‚æ¬¡åŒ–ç»„åˆç»“æ„ï¼‰ä»¥åŠEpisodic Memoryï¼ˆæƒ…èŠ‚è®°å¿†ï¼‰è¿™ä¸‰å¤§æ ¸å¿ƒç»„ä»¶æ•´åˆåˆ°ç”Ÿæˆå¼æ¶æ„ä¸­ã€‚é€šè¿‡åœ¨ä¸åŒæŠ½è±¡å°ºåº¦ä¸Šæ·±åº¦é›†æˆè¡ŒåŠ¨æŒ‡ä»¤ï¼Œå¹¶è¾…ä»¥ç»„åˆæ€§ç”Ÿæˆæ¨¡å‹ä¸è®°å¿†ç³»ç»Ÿï¼Œå¯ä»¥ä»æ ¹æœ¬ä¸Šæ”¹å–„æ¨¡å‹ç†è§£æµ®æµ…åŠä¸å¯æ§ç­‰é—®é¢˜ã€‚è¯¥ææ¡ˆå¯¹æ¯”äº†Chain-of-Thought (CoT)å’ŒRetrieval-Augmented Generation (RAG)ç­‰å½“å‰è¶‹åŠ¿ï¼Œè®ºè¯äº†å€Ÿé‰´è„‘ç§‘å­¦æœºåˆ¶åœ¨æå‡AIå®‰å…¨æ€§ä¸ä¿¡ä»»åº¦æ–¹é¢çš„å¿…è¦æ€§ã€‚æœ€ç»ˆï¼Œè®ºæ–‡å¼ºè°ƒé‡å¯è„‘ç§‘å­¦ä¸AIä¹‹é—´çš„è·¨å­¦ç§‘äº¤æµå°†ä¸ºæ„å»ºå®‰å…¨ã€å¯è§£é‡Šä¸”ä»¥äººä¸ºä¸­å¿ƒçš„AIå¥ å®šåŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "physics.bio-ph",
        "q-bio.NC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22568v1",
      "published_date": "2025-12-27 11:54:54 UTC",
      "updated_date": "2025-12-27 11:54:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:13:50.768239+00:00"
    },
    {
      "arxiv_id": "2512.22564v1",
      "title": "Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers",
      "title_zh": "é¢å‘å‘¼å¸éŸ³åˆ†ç±»çš„å‡ ä½•æ„ŸçŸ¥ä¼˜åŒ–ï¼šåˆ©ç”¨ SAM ä¼˜åŒ–çš„ Audio Spectrogram Transformers æå‡çµæ•åº¦",
      "authors": [
        "Atakan IÅŸÄ±k",
        "Selin Vulga IÅŸÄ±k",
        "Ahmet Feridun IÅŸÄ±k",
        "MahÅŸuk Taylan"
      ],
      "abstract": "Respiratory sound classification is hindered by the limited size, high noise levels, and severe class imbalance of benchmark datasets like ICBHI 2017. While Transformer-based models offer powerful feature extraction capabilities, they are prone to overfitting and often converge to sharp minima in the loss landscape when trained on such constrained medical data. To address this, we introduce a framework that enhances the Audio Spectrogram Transformer (AST) using Sharpness-Aware Minimization (SAM). Instead of merely minimizing the training loss, our approach optimizes the geometry of the loss surface, guiding the model toward flatter minima that generalize better to unseen patients. We also implement a weighted sampling strategy to handle class imbalance effectively. Our method achieves a state-of-the-art score of 68.10% on the ICBHI 2017 dataset, outperforming existing CNN and hybrid baselines. More importantly, it reaches a sensitivity of 68.31%, a crucial improvement for reliable clinical screening. Further analysis using t-SNE and attention maps confirms that the model learns robust, discriminative features rather than memorizing background noise.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å‘¼å¸éŸ³åˆ†ç±»ä»»åŠ¡ä¸­ICBHI 2017æ•°æ®é›†å­˜åœ¨çš„å°è§„æ¨¡ã€é«˜å™ªå£°åŠä¸¥é‡ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºé”åº¦æ„ŸçŸ¥æœ€å°åŒ–(Sharpness-Aware Minimization, SAM)ä¼˜åŒ–éŸ³é¢‘é¢‘è°±å›¾Transformer(Audio Spectrogram Transformer, AST)çš„æ–°æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¼˜åŒ–æŸå¤±å‡½æ•°çš„å‡ ä½•æ›²é¢ï¼Œå¼•å¯¼æ¨¡å‹æ”¶æ•›è‡³æ³›åŒ–èƒ½åŠ›æ›´å¼ºçš„å¹³å¦æå°å€¼(Flatter Minima)ï¼Œå¹¶ç»“åˆåŠ æƒé‡‡æ ·ç­–ç•¥æœ‰æ•ˆç¼“è§£äº†ç±»åˆ«ä¸å¹³è¡¡çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ICBHI 2017æ•°æ®é›†ä¸Šå–å¾—äº†68.10%çš„State-of-the-artæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸´åºŠç­›æŸ¥çš„å…³é”®æŒ‡æ ‡çµæ•åº¦(Sensitivity)ä¸Šè¾¾åˆ°äº†68.31%çš„æ˜¾è‘—æå‡ã€‚é€šè¿‡t-SNEå’Œæ³¨æ„åŠ›å›¾åˆ†æè¿›ä¸€æ­¥è¯å®ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæå–é²æ£’ä¸”å…·æœ‰è¾¨åˆ«åŠ›çš„ç‰¹å¾ï¼Œè€Œéä»…ä»…è®°å¿†èƒŒæ™¯å™ªå£°ï¼Œä¸ºå¯é çš„ä¸´åºŠè¾…åŠ©è¯Šæ–­æä¾›äº†æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "10 pages, 3 figures,2 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.22564v1",
      "published_date": "2025-12-27 11:39:36 UTC",
      "updated_date": "2025-12-27 11:39:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:14:42.510014+00:00"
    },
    {
      "arxiv_id": "2512.23753v1",
      "title": "Generalized Regularized Evidential Deep Learning Models: Theory and Comprehensive Evaluation",
      "title_zh": "å¹¿ä¹‰æ­£åˆ™åŒ–è¯æ®æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼šç†è®ºä¸å…¨é¢è¯„ä¼°",
      "authors": [
        "Deep Shankar Pandey",
        "Hyomin Choi",
        "Qi Yu"
      ],
      "abstract": "Evidential deep learning (EDL) models, based on Subjective Logic, introduce a principled and computationally efficient way to make deterministic neural networks uncertainty-aware. The resulting evidential models can quantify fine-grained uncertainty using learned evidence. However, the Subjective-Logic framework constrains evidence to be non-negative, requiring specific activation functions whose geometric properties can induce activation-dependent learning-freeze behavior: a regime where gradients become extremely small for samples mapped into low-evidence regions. We theoretically characterize this behavior and analyze how different evidential activations influence learning dynamics. Building on this analysis, we design a general family of activation functions and corresponding evidential regularizers that provide an alternative pathway for consistent evidence updates across activation regimes. Extensive experiments on four benchmark classification problems (MNIST, CIFAR-10, CIFAR-100, and Tiny-ImageNet), two few-shot classification problems, and blind face restoration problem empirically validate the developed theory and demonstrate the effectiveness of the proposed generalized regularized evidential models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºä¸»è§‚é€»è¾‘(Subjective Logic)çš„è¯æ®æ·±åº¦å­¦ä¹ (Evidential Deep Learning, EDL)æ¨¡å‹ï¼Œå¹¶é‡ç‚¹åˆ†æäº†å…¶åœ¨é‡åŒ–ä¸ç¡®å®šæ€§æ—¶é¢ä¸´çš„å­¦ä¹ å†»ç»“(learning-freeze)è¡Œä¸ºã€‚ä½œè€…é€šè¿‡ç†è®ºæ¨å¯¼æ­ç¤ºäº†éè´Ÿè¯æ®çº¦æŸåŠæ¿€æ´»å‡½æ•°å‡ ä½•ç‰¹æ€§å¦‚ä½•å¯¼è‡´ä½è¯æ®åŒºåŸŸçš„æ¢¯åº¦æ¶ˆå¤±ï¼Œè¿›è€Œå½±å“æ¨¡å‹çš„å­¦ä¹ åŠ¨åŠ›å­¦ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç±»é€šç”¨çš„æ¿€æ´»å‡½æ•°æ—åŠé…å¥—çš„è¯æ®æ­£åˆ™åŒ–é¡¹(evidential regularizers)ï¼Œä¸ºç¡®ä¿ä¸åŒæ¿€æ´»çŠ¶æ€ä¸‹çš„ä¸€è‡´è¯æ®æ›´æ–°æä¾›äº†æ–°è·¯å¾„ã€‚å®éªŒåœ¨MNISTã€CIFARã€Tiny-ImageNetç­‰åŸºå‡†åˆ†ç±»ä»»åŠ¡ï¼Œä»¥åŠå°‘æ ·æœ¬åˆ†ç±»(few-shot classification)å’Œç›²é¢éƒ¨ä¿®å¤(blind face restoration)é—®é¢˜ä¸Šå±•å¼€ï¼Œå…¨é¢éªŒè¯äº†æ‰€æç†è®ºã€‚ç»“æœè¡¨æ˜ï¼Œè¿™ç§å¹¿ä¹‰æ­£åˆ™åŒ–è¯æ®æ¨¡å‹æœ‰æ•ˆå…‹æœäº†åŸæœ‰å­¦ä¹ æœºåˆ¶çš„ç¼ºé™·ï¼Œåœ¨æå‡æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ä¿è¯äº†é«˜æ•ˆä¸”ç»†ç²’åº¦çš„ä¸ç¡®å®šæ€§é‡åŒ–ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "pdf_url": "https://arxiv.org/pdf/2512.23753v1",
      "published_date": "2025-12-27 11:26:18 UTC",
      "updated_date": "2025-12-27 11:26:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:14:20.362854+00:00"
    },
    {
      "arxiv_id": "2512.22562v1",
      "title": "Learning When Not to Attend Globally",
      "title_zh": "å­¦ä¹ ä½•æ—¶æ— éœ€å…¨å±€å…³æ³¨",
      "authors": [
        "Xuan Luo",
        "Kailai Zhang",
        "Xifeng Yan"
      ],
      "abstract": "When reading books, humans focus primarily on the current page, flipping back to recap prior context only when necessary. Similarly, we demonstrate that Large Language Models (LLMs) can learn to dynamically determine when to attend to global context. We propose All-or-Here Attention (AHA), which utilizes a binary router per attention head to dynamically toggle between full attention and local sliding window attention for each token. Our results indicate that with a window size of 256 tokens, up to 93\\% of the original full attention operations can be replaced by sliding window attention without performance loss. Furthermore, by evaluating AHA across various window sizes, we identify a long-tail distribution in context dependency, where the necessity for full attention decays rapidly as the local window expands. By decoupling local processing from global access, AHA reveals that full attention is largely redundant, and that efficient inference requires only on-demand access to the global context.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† All-or-Here Attention (AHA) æœºåˆ¶ï¼Œæ—¨åœ¨è®©å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) å­¦ä¹ å¦‚ä½•åŠ¨æ€åœ°å†³å®šä½•æ—¶è®¿é—®å…¨å±€ä¸Šä¸‹æ–‡ã€‚AHA ä¸ºæ¯ä¸ªæ³¨æ„åŠ›å¤´é…å¤‡äº†ä¸€ä¸ªäºŒå…ƒè·¯ç”± (binary router)ï¼Œé’ˆå¯¹æ¯ä¸ª token åŠ¨æ€åˆ‡æ¢å…¨é‡æ³¨æ„åŠ› (full attention) å’Œå±€éƒ¨æ»‘åŠ¨çª—å£æ³¨æ„åŠ› (local sliding window attention)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨çª—å£å¤§å°ä¸º 256 ä¸ª token çš„æƒ…å†µä¸‹ï¼Œé«˜è¾¾ 93% çš„åŸå§‹å…¨é‡æ³¨æ„åŠ›æ“ä½œå¯ä»¥è¢«æ»‘åŠ¨çª—å£æ³¨æ„åŠ›å–ä»£ï¼Œä¸”ä¸ä¼šé€ æˆæ€§èƒ½æŸå¤±ã€‚é€šè¿‡è¯„ä¼°ä¸åŒçª—å£å¤§å°ä¸‹çš„è¡¨ç°ï¼Œç ”ç©¶å‘ç°ä¸Šä¸‹æ–‡ä¾èµ–æ€§å‘ˆç°é•¿å°¾åˆ†å¸ƒï¼Œå³éšç€å±€éƒ¨çª—å£çš„æ‰©å¤§ï¼Œå¯¹å…¨é‡æ³¨æ„åŠ›çš„éœ€æ±‚ä¼šè¿…é€Ÿè¡°å‡ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†å…¨é‡æ³¨æ„åŠ›åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯å†—ä½™çš„ï¼Œé«˜æ•ˆçš„æ¨ç†ä»…éœ€è¦æŒ‰éœ€è·å–å…¨å±€ä¸Šä¸‹æ–‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22562v1",
      "published_date": "2025-12-27 11:21:40 UTC",
      "updated_date": "2025-12-27 11:21:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:13:54.160251+00:00"
    },
    {
      "arxiv_id": "2512.22560v1",
      "title": "RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure",
      "title_zh": "RollArtï¼šé€šè¿‡è§£è€¦æ¶æ„å®ç°æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„è§„æ¨¡åŒ–è®­ç»ƒ",
      "authors": [
        "Wei Gao",
        "Yuheng Zhao",
        "Tianyuan Wu",
        "Shaopan Xiong",
        "Weixun Wang",
        "Dakai An",
        "Lunxi Cao",
        "Dilxat Muhtar",
        "Zichen Liu",
        "Haizhou Zhao",
        "Ju Huang",
        "Siran Yang",
        "Yongbin Li",
        "Wenbo Su",
        "Jiamang Wang",
        "Lin Qu",
        "Bo Zheng",
        "Wei Wang"
      ],
      "abstract": "Agentic Reinforcement Learning (RL) enables Large Language Models (LLMs) to perform autonomous decision-making and long-term planning. Unlike standard LLM post-training, agentic RL workloads are highly heterogeneous, combining compute-intensive prefill phases, bandwidth-bound decoding, and stateful, CPU-heavy environment simulations. We argue that efficient agentic RL training requires disaggregated infrastructure to leverage specialized, best-fit hardware. However, naive disaggregation introduces substantial synchronization overhead and resource underutilization due to the complex dependencies between stages.\n  We present RollArc, a distributed system designed to maximize throughput for multi-task agentic RL on disaggregated infrastructure. RollArc is built on three core principles: (1) hardware-affinity workload mapping, which routes compute-bound and bandwidth-bound tasks to bestfit GPU devices, (2) fine-grained asynchrony, which manages execution at the trajectory level to mitigate resource bubbles, and (3) statefulness-aware computation, which offloads stateless components (e.g., reward models) to serverless infrastructure for elastic scaling. Our results demonstrate that RollArc effectively improves training throughput and achieves 1.35-2.05\\(\\times\\) end-to-end training time reduction compared to monolithic and synchronous baselines. We also evaluate RollArc by training a hundreds-of-billions-parameter MoE model for Qoder product on an Alibaba cluster with more than 3,000 GPUs, further demonstrating RollArc scalability and robustness. The code is available at https://github.com/alibaba/ROLL.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RollArcï¼Œä¸€ç§æ—¨åœ¨æœ€å¤§åŒ–è§£è€¦åŸºç¡€è®¾æ–½ (Disaggregated Infrastructure) ä¸Šå¤šä»»åŠ¡æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (Agentic RL) è®­ç»ƒååé‡çš„åˆ†å¸ƒå¼ç³»ç»Ÿã€‚é’ˆå¯¹ LLM æ™ºèƒ½ä½“å·¥ä½œè´Ÿè½½åœ¨è®¡ç®—å¯†é›†å‹é¢„å¡«å……ã€å¸¦å®½å—é™è§£ç åŠ CPU å¯†é›†å‹ç¯å¢ƒæ¨¡æ‹Ÿæ–¹é¢çš„å¼‚æ„æ€§ï¼ŒRollArc é‡‡ç”¨äº†ç¡¬ä»¶äº²å’Œæ€§å·¥ä½œè´Ÿè½½æ˜ å°„ (Hardware-affinity workload mapping)ã€ç»†ç²’åº¦å¼‚æ­¥ (Fine-grained asynchrony) å’ŒçŠ¶æ€æ„ŸçŸ¥è®¡ç®— (Statefulness-aware computation) ä¸‰å¤§æ ¸å¿ƒåŸåˆ™ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å°†å¥–åŠ±æ¨¡å‹ (Reward models) ç­‰æ— çŠ¶æ€ç»„ä»¶å¸è½½åˆ°æ— æœåŠ¡å™¨åŸºç¡€è®¾æ–½ä»¥å®ç°å¼¹æ€§æ‰©å±•ï¼Œå¹¶åˆ©ç”¨è½¨è¿¹çº§æ‰§è¡Œæœ‰æ•ˆç¼“è§£äº†èµ„æºæ°”æ³¡å’ŒåŒæ­¥å¼€é”€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å•ä½“å’ŒåŒæ­¥åŸºçº¿ç›¸æ¯”ï¼ŒRollArc å®ç°äº† 1.35-2.05 å€çš„ç«¯åˆ°ç«¯è®­ç»ƒæ—¶é—´ç¼©å‡ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿåœ¨åŒ…å« 3,000 å¤šä¸ª GPU çš„é›†ç¾¤ä¸ŠæˆåŠŸè®­ç»ƒäº†æ•°åƒäº¿å‚æ•°çš„ MoE æ¨¡å‹ï¼ŒéªŒè¯äº†å…¶åœ¨å¤§è§„æ¨¡ç”Ÿäº§ç¯å¢ƒä¸­çš„å“è¶Šå¯æ‰©å±•æ€§ä¸é²æ£’æ€§ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "17 pages, 17 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.22560v1",
      "published_date": "2025-12-27 11:14:23 UTC",
      "updated_date": "2025-12-27 11:14:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:14:02.772974+00:00"
    },
    {
      "arxiv_id": "2512.22550v1",
      "title": "TimePerceiver: An Encoder-Decoder Framework for Generalized Time-Series Forecasting",
      "title_zh": "TimePerceiverï¼šé¢å‘å¹¿ä¹‰æ—¶é—´åºåˆ—é¢„æµ‹çš„ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶",
      "authors": [
        "Jaebin Lee",
        "Hankook Lee"
      ],
      "abstract": "In machine learning, effective modeling requires a holistic consideration of how to encode inputs, make predictions (i.e., decoding), and train the model. However, in time-series forecasting, prior work has predominantly focused on encoder design, often treating prediction and training as separate or secondary concerns. In this paper, we propose TimePerceiver, a unified encoder-decoder forecasting framework that is tightly aligned with an effective training strategy. To be specific, we first generalize the forecasting task to include diverse temporal prediction objectives such as extrapolation, interpolation, and imputation. Since this generalization requires handling input and target segments that are arbitrarily positioned along the temporal axis, we design a novel encoder-decoder architecture that can flexibly perceive and adapt to these varying positions. For encoding, we introduce a set of latent bottleneck representations that can interact with all input segments to jointly capture temporal and cross-channel dependencies. For decoding, we leverage learnable queries corresponding to target timestamps to effectively retrieve relevant information. Extensive experiments demonstrate that our framework consistently and significantly outperforms prior state-of-the-art baselines across a wide range of benchmark datasets. The code is available at https://github.com/efficient-learning-lab/TimePerceiver.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TimePerceiverï¼Œä¸€ä¸ªç»Ÿä¸€çš„ Encoder-Decoder é¢„æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°è¾“å…¥ç¼–ç ã€é¢„æµ‹è§£ç ä¸è®­ç»ƒç­–ç•¥çš„æ·±åº¦å¯¹é½ã€‚ä¸ºäº†å…‹æœä»¥å¾€ç ”ç©¶è¿‡åº¦åé‡ Encoder è®¾è®¡çš„å±€é™ï¼Œè¯¥æ¡†æ¶å°†é¢„æµ‹ä»»åŠ¡æ³›åŒ–ä¸ºåŒ…å« Extrapolationã€Interpolation å’Œ Imputation åœ¨å†…çš„å¤šç§æ—¶é—´é¢„æµ‹ç›®æ ‡ï¼Œä»è€Œèƒ½å¤Ÿçµæ´»æ„ŸçŸ¥å¹¶é€‚åº”æ—¶é—´è½´ä¸Šä»»æ„ä½ç½®çš„è¾“å…¥å’Œç›®æ ‡ç‰‡æ®µã€‚åœ¨æ¶æ„ä¸Šï¼Œæ¨¡å‹å¼•å…¥äº†ä¸€ç»„ Latent bottleneck è¡¨è¾¾ï¼Œé€šè¿‡ä¸è¾“å…¥ç‰‡æ®µçš„äº¤äº’å…±åŒæ•è· Temporal å’Œ Cross-channel ä¾èµ–ã€‚åœ¨è§£ç è¿‡ç¨‹ä¸­ï¼Œåˆ©ç”¨ä¸ç›®æ ‡æ—¶é—´æˆ³ç›¸å…³çš„ Learnable queries æ¥é«˜æ•ˆæ£€ç´¢å…³é”®ä¿¡æ¯ã€‚å¹¿æ³›çš„å®éªŒè¯æ˜ï¼ŒTimePerceiver åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„ State-of-the-art æ¨¡å‹ï¼Œä¸ºå¹¿ä¹‰æ—¶é—´åºåˆ—é¢„æµ‹æä¾›äº†ä¸€ç§å¼ºæœ‰åŠ›çš„é€šç”¨æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to NeurIPS 2025. The code is available at https://github.com/efficient-learning-lab/TimePerceiver",
      "pdf_url": "https://arxiv.org/pdf/2512.22550v1",
      "published_date": "2025-12-27 10:34:22 UTC",
      "updated_date": "2025-12-27 10:34:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:14:05.207305+00:00"
    },
    {
      "arxiv_id": "2512.22545v1",
      "title": "Self-Rewarded Multimodal Coherent Reasoning Across Diverse Visual Domains",
      "title_zh": "è·¨å¤šæ ·åŒ–è§†è§‰é¢†åŸŸçš„è‡ªå¥–åŠ±å¤šæ¨¡æ€è¿è´¯æ¨ç†",
      "authors": [
        "Jesen Zhang",
        "Ningyuan Liu",
        "Kaitong Cai",
        "Sidi Liu",
        "Jing Yang",
        "Ziliang Chen",
        "Xiaofei Sun",
        "Keze Wang"
      ],
      "abstract": "Multimodal LLMs often produce fluent yet unreliable reasoning, exhibiting weak step-to-step coherence and insufficient visual grounding, largely because existing alignment approaches supervise only the final answer while ignoring the reliability of the intermediate reasoning process. We introduce SR-MCR, a lightweight and label-free framework that aligns reasoning by exploiting intrinsic process signals derived directly from model outputs. Five self-referential cues -- semantic alignment, lexical fidelity, non-redundancy, visual grounding, and step consistency -- are integrated into a normalized, reliability-weighted reward that provides fine-grained process-level guidance. A critic-free GRPO objective, enhanced with a confidence-aware cooling mechanism, further stabilizes training and suppresses trivial or overly confident generations. Built on Qwen2.5-VL, SR-MCR improves both answer accuracy and reasoning coherence across a broad set of visual benchmarks; among open-source models of comparable size, SR-MCR-7B achieves state-of-the-art performance with an average accuracy of 81.4%. Ablation studies confirm the independent contributions of each reward term and the cooling module.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SR-MCRï¼Œä¸€ç§è½»é‡çº§ä¸”æ— éœ€äººå·¥æ ‡æ³¨çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) æ¨ç†ä¸­é€»è¾‘è¿è´¯æ€§å·®å’Œè§†è§‰å®šä½ (visual grounding) ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆè¯­ä¹‰å¯¹é½ (semantic alignment)ã€è¯æ±‡å¿ å®åº¦ (lexical fidelity)ã€éå†—ä½™æ€§ (non-redundancy)ã€è§†è§‰å®šä½ä»¥åŠæ­¥éª¤ä¸€è‡´æ€§ (step consistency) äº”é¡¹è‡ªå‚è€ƒçº¿ç´¢ï¼Œæ„å»ºäº†åŸºäºå¯é æ€§åŠ æƒçš„å¥–åŠ±æœºåˆ¶ï¼Œä»è€Œå®ç°ç»†ç²’åº¦çš„è¿‡ç¨‹çº§ç›‘ç£ã€‚ä¸ºäº†ç¨³å®šè®­ç»ƒå¹¶æŠ‘åˆ¶è¿‡åº¦è‡ªä¿¡çš„ç”Ÿæˆï¼Œç ”ç©¶å¼•å…¥äº†å¸¦æœ‰ç½®ä¿¡åº¦æ„ŸçŸ¥å†·å´æœºåˆ¶ (confidence-aware cooling mechanism) çš„ critic-free GRPO ä¼˜åŒ–ç›®æ ‡ã€‚åŸºäº Qwen2.5-VL çš„å®éªŒè¡¨æ˜ï¼ŒSR-MCR åœ¨å¹¿æ³›çš„è§†è§‰åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†ç­”æ¡ˆå‡†ç¡®ç‡å’Œæ¨ç†è¿è´¯æ€§ã€‚å…¶ä¸­ SR-MCR-7B åœ¨åŒç­‰å‚æ•°è§„æ¨¡çš„å¼€æºæ¨¡å‹ä¸­è¾¾åˆ°äº† State-of-the-art æ€§èƒ½ï¼Œå¹³å‡å‡†ç¡®ç‡é«˜è¾¾ 81.4%ï¼ŒéªŒè¯äº†å„å¥–åŠ±é¡¹åŠå†·å´æ¨¡å—åœ¨æå‡è·¨é¢†åŸŸæ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22545v1",
      "published_date": "2025-12-27 10:14:14 UTC",
      "updated_date": "2025-12-27 10:14:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:14:08.920360+00:00"
    },
    {
      "arxiv_id": "2601.09722v1",
      "title": "ADMEDTAGGER: an annotation framework for distillation of expert knowledge for the Polish medical language",
      "title_zh": "ADMEDTAGGERï¼šé¢å‘æ³¢å…°è¯­åŒ»å­¦æ–‡æœ¬ä¸“å®¶çŸ¥è¯†è’¸é¦çš„æ ‡æ³¨æ¡†æ¶",
      "authors": [
        "Franciszek GÃ³rski",
        "Andrzej CzyÅ¼ewski"
      ],
      "abstract": "In this work, we present an annotation framework that demonstrates how a multilingual LLM pretrained on a large corpus can be used as a teacher model to distill the expert knowledge needed for tagging medical texts in Polish. This work is part of a larger project called ADMEDVOICE, within which we collected an extensive corpus of medical texts representing five clinical categories - Radiology, Oncology, Cardiology, Hypertension, and Pathology. Using this data, we had to develop a multi-class classifier, but the fundamental problem turned out to be the lack of resources for annotating an adequate number of texts. Therefore, in our solution, we used the multilingual Llama3.1 model to annotate an extensive corpus of medical texts in Polish. Using our limited annotation resources, we verified only a portion of these labels, creating a test set from them. The data annotated in this way were then used for training and validation of 3 different types of classifiers based on the BERT architecture - the distilled DistilBERT model, BioBERT fine-tuned on medical data, and HerBERT fine-tuned on the Polish language corpus. Among the models we trained, the DistilBERT model achieved the best results, reaching an F1 score > 0.80 for each clinical category and an F1 score > 0.93 for 3 of them. In this way, we obtained a series of highly effective classifiers that represent an alternative to large language models, due to their nearly 500 times smaller size, 300 times lower GPU VRAM consumption, and several hundred times faster inference.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ADMEDTAGGERï¼Œä¸€ä¸ªæ—¨åœ¨ä¸ºæ³¢å…°è¯­åŒ»å­¦æ–‡æœ¬æ ‡æ³¨æå–ä¸“å®¶çŸ¥è¯†çš„æ ‡æ³¨æ¡†æ¶ï¼Œä»¥è§£å†³åŒ»ç–—é¢†åŸŸæ ‡æ³¨èµ„æºåŒ®ä¹çš„éš¾é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤šè¯­è¨€å¤§è¯­è¨€æ¨¡å‹ Llama3.1 ä½œä¸ºæ•™å¸ˆæ¨¡å‹ï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦æŠ€æœ¯å¯¹æ¶µç›–æ”¾å°„å­¦ã€è‚¿ç˜¤å­¦å’Œå¿ƒè„ç—…å­¦ç­‰äº”ä¸ªä¸´åºŠç±»åˆ«çš„ ADMEDVOICE è¯­æ–™åº“è¿›è¡Œè‡ªåŠ¨æ ‡æ³¨ã€‚ç ”ç©¶äººå‘˜åŸºäºè¿™äº›æ ‡æ³¨æ•°æ®è®­ç»ƒå¹¶éªŒè¯äº† DistilBERTã€BioBERT ä»¥åŠ HerBERT ç­‰å¤šç§åŸºäº BERT æ¶æ„çš„åˆ†ç±»å™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDistilBERT æ¨¡å‹åœ¨å„ä¸´åºŠç±»åˆ«ä¸­è¡¨ç°æœ€ä¸ºå‡ºè‰²ï¼Œå…¶ F1 score å‡é«˜äº 0.80ï¼Œå…¶ä¸­ä¸‰ä¸ªç±»åˆ«çš„ F1 score ç”šè‡³è¶…è¿‡ 0.93ã€‚è¯¥ç ”ç©¶æ‰€æ„å»ºçš„è½»é‡åŒ–åˆ†ç±»å™¨ç›¸æ¯”å¤§è¯­è¨€æ¨¡å‹ LLMsï¼Œå…¶æ¨¡å‹ä½“ç§¯ç¼©å°äº†è¿‘ 500 å€ï¼Œæ˜¾å­˜ VRAM æ¶ˆè€—é™ä½äº† 300 å€ï¼Œä¸”æ¨ç†é€Ÿåº¦æå‡äº†æ•°ç™¾å€ã€‚è¿™è¯æ˜äº†é€šè¿‡ä¸“å®¶çŸ¥è¯†è’¸é¦æ„å»ºçš„é«˜æ•ˆåˆ†ç±»å™¨æ˜¯å¤„ç†åŒ»ç–—é¢†åŸŸç‰¹å®šä»»åŠ¡æ—¶æå…·ç«äº‰åŠ›çš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.09722v1",
      "published_date": "2025-12-27 10:00:52 UTC",
      "updated_date": "2025-12-27 10:00:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:14:15.967600+00:00"
    },
    {
      "arxiv_id": "2512.22536v1",
      "title": "CoAgent: Collaborative Planning and Consistency Agent for Coherent Video Generation",
      "title_zh": "CoAgentï¼šé¢å‘è¿è´¯è§†é¢‘ç”Ÿæˆçš„ååŒè§„åˆ’ä¸ä¸€è‡´æ€§æ™ºèƒ½ä½“",
      "authors": [
        "Qinglin Zeng",
        "Kaitong Cai",
        "Ruiqi Chen",
        "Qinhan Lv",
        "Keze Wang"
      ],
      "abstract": "Maintaining narrative coherence and visual consistency remains a central challenge in open-domain video generation. Existing text-to-video models often treat each shot independently, resulting in identity drift, scene inconsistency, and unstable temporal structure. We propose CoAgent, a collaborative and closed-loop framework for coherent video generation that formulates the process as a plan-synthesize-verify pipeline. Given a user prompt, style reference, and pacing constraints, a Storyboard Planner decomposes the input into structured shot-level plans with explicit entities, spatial relations, and temporal cues. A Global Context Manager maintains entity-level memory to preserve appearance and identity consistency across shots. Each shot is then generated by a Synthesis Module under the guidance of a Visual Consistency Controller, while a Verifier Agent evaluates intermediate results using vision-language reasoning and triggers selective regeneration when inconsistencies are detected. Finally, a pacing-aware editor refines temporal rhythm and transitions to match the desired narrative flow. Extensive experiments demonstrate that CoAgent significantly improves coherence, visual consistency, and narrative quality in long-form video generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CoAgentï¼Œä¸€ç§ç”¨äºè¿è´¯è§†é¢‘ç”Ÿæˆçš„åä½œé—­ç¯æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¼€æ”¾åŸŸè§†é¢‘ç”Ÿæˆä¸­å¸¸è§çš„èº«ä»½åç§»(identity drift)ã€åœºæ™¯ä¸ä¸€è‡´å’Œæ—¶é—´ç»“æ„ä¸ç¨³å®šç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å°†è§†é¢‘ç”Ÿæˆè¿‡ç¨‹å»ºæ¨¡ä¸ºâ€œè§„åˆ’-åˆæˆ-éªŒè¯â€(plan-synthesize-verify)çš„æµæ°´çº¿ï¼Œåˆ©ç”¨Storyboard Plannerå°†ç”¨æˆ·æç¤ºè¯åˆ†è§£ä¸ºåŒ…å«æ˜¾å¼å®ä½“å’Œç©ºé—´å…³ç³»çš„ç»“æ„åŒ–é•œå¤´è®¡åˆ’ã€‚ä¸ºäº†ä¿æŒè·¨é•œå¤´çš„è§†è§‰ä¸€è‡´æ€§ï¼ŒGlobal Context Manageré€šè¿‡å®ä½“çº§å†…å­˜ç»´æŠ¤å¤–è§‚ä¸èº«ä»½ä¿¡æ¯ã€‚åœ¨åˆæˆé˜¶æ®µï¼ŒVisual Consistency ControlleræŒ‡å¯¼Synthesis Moduleç”Ÿæˆå„é•œå¤´ï¼Œå¹¶ç”±Verifier Agentåˆ©ç”¨è§†è§‰è¯­è¨€æ¨ç†å¯¹ä¸­é—´ç»“æœè¿›è¡Œè¯„ä¼°ï¼Œåœ¨æ£€æµ‹åˆ°ä¸ä¸€è‡´æ—¶è§¦å‘é€‰æ‹©æ€§é‡ç»˜ã€‚æœ€åï¼Œé€šè¿‡ä¸€ä¸ªæ„ŸçŸ¥èŠ‚å¥çš„ç¼–è¾‘å™¨(pacing-aware editor)ä¼˜åŒ–æ—¶é—´èŠ‚å¥ä¸è¿‡æ¸¡ï¼Œç¡®ä¿ç¬¦åˆå™äº‹æµã€‚å®éªŒè¯æ˜ï¼ŒCoAgentåœ¨é•¿è§†é¢‘ç”Ÿæˆçš„å™äº‹è¿è´¯æ€§ã€è§†è§‰ä¸€è‡´æ€§åŠæ•´ä½“è´¨é‡ä¸Šå‡å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22536v1",
      "published_date": "2025-12-27 09:38:34 UTC",
      "updated_date": "2025-12-27 09:38:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:15:03.075135+00:00"
    },
    {
      "arxiv_id": "2601.00840v2",
      "title": "A Global Atlas of Digital Dermatology to Map Innovation and Disparities",
      "title_zh": "æ•°å­—çš®è‚¤ç—…å­¦å…¨çƒå›¾è°±ï¼šæç»˜åˆ›æ–°ç°çŠ¶ä¸åˆ†å¸ƒå·®å¼‚",
      "authors": [
        "Fabian GrÃ¶ger",
        "Simone Lionetti",
        "Philippe Gottfrois",
        "Alvaro Gonzalez-Jimenez",
        "Lea Habermacher",
        "Labelling Consortium",
        "Ludovic Amruthalingam",
        "Matthew Groh",
        "Marc Pouly",
        "Alexander A. Navarini"
      ],
      "abstract": "The adoption of artificial intelligence in dermatology promises democratized access to healthcare, but model reliability depends on the quality and comprehensiveness of the data fueling these models. Despite rapid growth in publicly available dermatology images, the field lacks quantitative key performance indicators to measure whether new datasets expand clinical coverage or merely replicate what is already known. Here we present SkinMap, a multi-modal framework for the first comprehensive audit of the field's entire data basis. We unify the publicly available dermatology datasets into a single, queryable semantic atlas comprising more than 1.1 million images of skin conditions and quantify (i) informational novelty over time, (ii) dataset redundancy, and (iii) representation gaps across demographics and diagnoses. Despite exponential growth in dataset sizes, informational novelty across time has somewhat plateaued: Some clusters, such as common neoplasms on fair skin, are densely populated, while underrepresented skin types and many rare diseases remain unaddressed. We further identify structural gaps in coverage: Darker skin tones (Fitzpatrick V-VI) constitute only 5.8% of images and pediatric patients only 3.0%, while many rare diseases and phenotype combinations remain sparsely represented. SkinMap provides infrastructure to measure blind spots and steer strategic data acquisition toward undercovered regions of clinical space.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çš®è‚¤ç—…å­¦é¢†åŸŸäººå·¥æ™ºèƒ½æ¨¡å‹é«˜åº¦ä¾èµ–é«˜è´¨é‡æ•°æ®ï¼Œå´ç¼ºä¹è¡¡é‡æ•°æ®é›†ä¸´åºŠè¦†ç›–èŒƒå›´é‡åŒ–æŒ‡æ ‡çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º SkinMap çš„å¤šæ¨¡æ€(multi-modal)æ¡†æ¶ã€‚SkinMap å°†å…¬å¼€çš„çš®è‚¤ç—…å­¦æ•°æ®é›†æ•´åˆä¸ºä¸€ä¸ªåŒ…å« 110 ä¸‡å¼ å›¾åƒçš„è¯­ä¹‰å›¾è°±(semantic atlas)ï¼Œé¦–æ¬¡å®ç°äº†å¯¹å…¨çƒæ•°å­—çš®è‚¤ç—…å­¦æ•°æ®åŸºç¡€çš„å…¨é¢å®¡è®¡ã€‚é€šè¿‡é‡åŒ–ä¿¡æ¯åˆ›æ–°æ€§(informational novelty)ã€æ•°æ®é›†å†—ä½™æ€§ä»¥åŠäººå£ç»Ÿè®¡å­¦ç¼ºå£ï¼Œç ”ç©¶å‘ç°å°½ç®¡æ•°æ®è§„æ¨¡å‘ˆæŒ‡æ•°å¢é•¿ï¼Œä½†åˆ›æ–°æ€§å·²è¶‹äºå¹³ç¼“ã€‚å®éªŒæ•°æ®æ­ç¤ºäº†ä¸¥é‡çš„ä»£è¡¨æ€§å¤±è¡¡ï¼Œå…¶ä¸­æ·±è‰²çš®è‚¤(Fitzpatrick V-VI)å›¾åƒä»…å  5.8%ï¼Œå„¿ç§‘æ‚£è€…ä»…å  3.0%ï¼Œä¸”ç½•è§ç–¾ç—…å’Œç‰¹å®šè¡¨å‹ç»„åˆçš„è¦†ç›–ç‡æä½ã€‚SkinMap ä¸ºè¯†åˆ«ä¸´åºŠç©ºé—´çš„ç›²ç‚¹æä¾›äº†å…³é”®åŸºç¡€è®¾æ–½ï¼Œæœ‰åŠ©äºå¼•å¯¼æœªæ¥çš„æˆ˜ç•¥æ€§æ•°æ®é‡‡é›†ï¼Œä»è€Œç¼©å°æ•°å­—å¥åº·é¢†åŸŸçš„åˆ›æ–°å·®è·ã€‚",
      "categories": [
        "cs.DL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.DL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00840v2",
      "published_date": "2025-12-27 09:22:36 UTC",
      "updated_date": "2026-01-09 20:23:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:14:57.179187+00:00"
    },
    {
      "arxiv_id": "2512.22529v1",
      "title": "Multi-AI Agent Framework Reveals the \"Oxide Gatekeeper\" in Aluminum Nanoparticle Oxidation",
      "title_zh": "å¤š AI æ™ºèƒ½ä½“æ¡†æ¶æ­ç¤ºé“çº³ç±³é¢—ç²’æ°§åŒ–ä¸­çš„â€œæ°§åŒ–ç‰©å®ˆé—¨äººâ€æœºåˆ¶",
      "authors": [
        "Yiming Lu",
        "Tingyu Lu",
        "Di Zhang",
        "Lili Ye",
        "Hao Li"
      ],
      "abstract": "Aluminum nanoparticles (ANPs) are among the most energy-dense solid fuels, yet the atomic mechanisms governing their transition from passivated particles to explosive reactants remain elusive. This stems from a fundamental computational bottleneck: ab initio methods offer quantum accuracy but are restricted to small spatiotemporal scales (< 500 atoms, picoseconds), while empirical force fields lack the reactive fidelity required for complex combustion environments. Herein, we bridge this gap by employing a \"human-in-the-loop\" closed-loop framework where self-auditing AI Agents validate the evolution of a machine learning potential (MLP). By acting as scientific sentinels that visualize hidden model artifacts for human decision-making, this collaborative cycle ensures quantum mechanical accuracy while exhibiting near-linear scalability to million-atom systems and accessing nanosecond timescales (energy RMSE: 1.2 meV/atom, force RMSE: 0.126 eV/Angstrom). Strikingly, our simulations reveal a temperature-regulated dual-mode oxidation mechanism: at moderate temperatures, the oxide shell acts as a dynamic \"gatekeeper,\" regulating oxidation through a \"breathing mode\" of transient nanochannels; above a critical threshold, a \"rupture mode\" unleashes catastrophic shell failure and explosive combustion. Importantly, we resolve a decades-old controversy by demonstrating that aluminum cation outward diffusion, rather than oxygen transport, dominates mass transfer across all temperature regimes, with diffusion coefficients consistently exceeding those of oxygen by 2-3 orders of magnitude. These discoveries establish a unified atomic-scale framework for energetic nanomaterial design, enabling the precision engineering of ignition sensitivity and energy release rates through intelligent computational design.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆâ€œäººæœºååŒ (human-in-the-loop)â€ä¸è‡ªå®¡è®¡ AI Agents çš„é—­ç¯æ¡†æ¶ï¼Œæ—¨åœ¨æ­ç¤ºé“çº³ç±³é¢—ç²’ (Aluminum nanoparticles, ANPs) æ°§åŒ–è¿‡ç¨‹ä¸­çš„åŸå­çº§æœºåˆ¶ã€‚é€šè¿‡ AI Agents é©±åŠ¨æœºå™¨å­¦ä¹ åŠ¿å‡½æ•° (Machine Learning Potential, MLP) çš„æ¼”åŒ–ï¼Œè¯¥æ¡†æ¶åœ¨ä¿æŒé‡å­åŠ›å­¦ç²¾åº¦çš„åŒæ—¶ï¼Œå®ç°äº†å¯¹ç™¾ä¸‡åŸå­ç³»ç»Ÿå’Œçº³ç§’å°ºåº¦çš„çº¿æ€§æ‰©å±•æ¨¡æ‹Ÿã€‚æ¨¡æ‹Ÿé¦–æ¬¡å‘ç°äº†æ¸©åº¦è°ƒèŠ‚çš„åŒæ¨¡å¼æ°§åŒ–æœºåˆ¶ï¼Œå³åœ¨ä¸­æ¸©ä¸‹é€šè¿‡â€œå‘¼å¸æ¨¡å¼ (breathing mode)â€è°ƒèŠ‚æ°§åŒ–ï¼Œè€Œåœ¨é«˜æ¸©é˜ˆå€¼ä»¥ä¸Šè¿›å…¥å¯¼è‡´çˆ†ç‚¸æ€§ç‡ƒçƒ§çš„â€œç ´è£‚æ¨¡å¼ (rupture mode)â€ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯å®äº†é“é˜³ç¦»å­çš„å‘å¤–æ‰©æ•£åœ¨æ‰€æœ‰æ¸©åº¦åŒºé—´å‡ä¸»å¯¼è´¨é‡è½¬ç§»ï¼Œå…¶æ‰©æ•£ç³»æ•°é«˜å‡ºæ°§ 2-3 ä¸ªæ•°é‡çº§ï¼Œè§£å†³äº†å…³äºæ‰©æ•£æœºåˆ¶çš„é•¿æœŸäº‰è®®ã€‚è¿™ä¸€æˆæœä¸ºé«˜èƒ½çº³ç±³ææ–™æä¾›äº†ç»Ÿä¸€çš„åŸå­å°ºåº¦æ¡†æ¶ï¼Œä½¿å¾—é€šè¿‡æ™ºèƒ½è®¡ç®—ç²¾ç¡®è®¾è®¡å¼•ç‡ƒæ•æ„Ÿæ€§å’Œèƒ½é‡é‡Šæ”¾é€Ÿç‡æˆä¸ºå¯èƒ½ã€‚",
      "categories": [
        "cs.AI",
        "cond-mat.mtrl-sci"
      ],
      "primary_category": "cs.AI",
      "comment": "21 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.22529v1",
      "published_date": "2025-12-27 09:21:21 UTC",
      "updated_date": "2025-12-27 09:21:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:15:09.158333+00:00"
    },
    {
      "arxiv_id": "2512.22522v1",
      "title": "Towards Reliable Evaluation of Adversarial Robustness for Spiking Neural Networks",
      "title_zh": "è¿ˆå‘è„‰å†²ç¥ç»ç½‘ç»œå¯¹æŠ—é²æ£’æ€§çš„å¯é è¯„ä¼°",
      "authors": [
        "Jihang Wang",
        "Dongcheng Zhao",
        "Ruolin Chen",
        "Qian Zhang",
        "Yi Zeng"
      ],
      "abstract": "Spiking Neural Networks (SNNs) utilize spike-based activations to mimic the brain's energy-efficient information processing. However, the binary and discontinuous nature of spike activations causes vanishing gradients, making adversarial robustness evaluation via gradient descent unreliable. While improved surrogate gradient methods have been proposed, their effectiveness under strong adversarial attacks remains unclear. We propose a more reliable framework for evaluating SNN adversarial robustness. We theoretically analyze the degree of gradient vanishing in surrogate gradients and introduce the Adaptive Sharpness Surrogate Gradient (ASSG), which adaptively evolves the shape of the surrogate function according to the input distribution during attack iterations, thereby enhancing gradient accuracy while mitigating gradient vanishing. In addition, we design an adversarial attack with adaptive step size under the $L_\\infty$ constraint-Stable Adaptive Projected Gradient Descent (SA-PGD), achieving faster and more stable convergence under imprecise gradients. Extensive experiments show that our approach substantially increases attack success rates across diverse adversarial training schemes, SNN architectures and neuron models, providing a more generalized and reliable evaluation of SNN adversarial robustness. The experimental results further reveal that the robustness of current SNNs has been significantly overestimated and highlighting the need for more dependable adversarial training methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è„‰å†²ç¥ç»ç½‘ç»œ(SNNs)åœ¨å¯¹æŠ—é²æ£’æ€§è¯„ä¼°ä¸­å› è„‰å†²æ¿€æ´»çš„ç¦»æ•£æ€§è€Œå¯¼è‡´çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»ŸåŸºäºæ¢¯åº¦ä¸‹é™çš„è¯„ä¼°æ–¹æ³•åœ¨å¼ºå¯¹æŠ—æ”»å‡»ä¸‹å¹¶ä¸å¯é ã€‚ä¸ºè§£å†³è¿™ä¸€éš¾é¢˜ï¼Œä½œè€…æå‡ºäº†è‡ªé€‚åº”é”åº¦ä»£ç†æ¢¯åº¦(Adaptive Sharpness Surrogate Gradient, ASSG)ï¼Œé€šè¿‡æ ¹æ®è¾“å…¥åˆ†å¸ƒåŠ¨æ€è°ƒæ•´ä»£ç†å‡½æ•°å½¢çŠ¶æ¥å¢å¼ºæ¢¯åº¦ç²¾åº¦å¹¶ç¼“è§£æ¢¯åº¦æ¶ˆå¤±ã€‚åŒæ—¶ï¼Œç ”ç©¶è¿˜è®¾è®¡äº†å¸¦è‡ªé€‚åº”æ­¥é•¿çš„ç¨³å®šè‡ªé€‚åº”æŠ•å½±æ¢¯åº¦ä¸‹é™(Stable Adaptive Projected Gradient Descent, SA-PGD)æ”»å‡»ç®—æ³•ï¼Œç¡®ä¿äº†åœ¨ä¸ç²¾ç¡®æ¢¯åº¦ä¸‹çš„å¿«é€Ÿç¨³å®šæ”¶æ•›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨å¤šç§SNNæ¶æ„å’Œç¥ç»å…ƒæ¨¡å‹ä¸Šå‡æ˜¾è‘—æå‡äº†æ”»å‡»æˆåŠŸç‡ï¼Œæä¾›äº†æ›´å…·å¹¿ä¹‰æ€§å’Œå¯é æ€§çš„è¯„ä¼°åŸºå‡†ã€‚ç ”ç©¶æœ€ç»ˆæ­ç¤ºå½“å‰SNNçš„é²æ£’æ€§è¢«å¤§å¹…é«˜ä¼°ï¼Œå¹¶å¼ºè°ƒäº†å¼€å‘æ›´å¯é å¯¹æŠ—è®­ç»ƒæ–¹æ³•çš„ç´§è¿«éœ€æ±‚ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22522v1",
      "published_date": "2025-12-27 08:43:06 UTC",
      "updated_date": "2025-12-27 08:43:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:15:04.613297+00:00"
    },
    {
      "arxiv_id": "2512.22508v1",
      "title": "Predicting LLM Correctness in Prosthodontics Using Metadata and Hallucination Signals",
      "title_zh": "åŸºäºå…ƒæ•°æ®ä¸å¹»è§‰ä¿¡å·çš„å£è…”ä¿®å¤å­¦å¤§è¯­è¨€æ¨¡å‹æ­£ç¡®æ€§é¢„æµ‹",
      "authors": [
        "Lucky Susanto",
        "Anasta Pranawijayana",
        "Cortino Sukotjo",
        "Soni Prasad",
        "Derry Wijaya"
      ],
      "abstract": "Large language models (LLMs) are increasingly adopted in high-stakes domains such as healthcare and medical education, where the risk of generating factually incorrect (i.e., hallucinated) information is a major concern. While significant efforts have been made to detect and mitigate such hallucinations, predicting whether an LLM's response is correct remains a critical yet underexplored problem. This study investigates the feasibility of predicting correctness by analyzing a general-purpose model (GPT-4o) and a reasoning-centric model (OSS-120B) on a multiple-choice prosthodontics exam. We utilize metadata and hallucination signals across three distinct prompting strategies to build a correctness predictor for each (model, prompting) pair. Our findings demonstrate that this metadata-based approach can improve accuracy by up to +7.14% and achieve a precision of 83.12% over a baseline that assumes all answers are correct. We further show that while actual hallucination is a strong indicator of incorrectness, metadata signals alone are not reliable predictors of hallucination. Finally, we reveal that prompting strategies, despite not affecting overall accuracy, significantly alter the models' internal behaviors and the predictive utility of their metadata. These results present a promising direction for developing reliability signals in LLMs but also highlight that the methods explored in this paper are not yet robust enough for critical, high-stakes deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å£è…”ä¿®å¤å­¦ (Prosthodontics) ç­‰é«˜é£é™©åŒ»ç–—é¢†åŸŸï¼Œå¦‚ä½•åˆ©ç”¨å…ƒæ•°æ® (Metadata) å’Œå¹»è§‰ä¿¡å· (Hallucination Signals) æ¥é¢„æµ‹å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ (LLMs) å›ç­”çš„æ­£ç¡®æ€§ã€‚ç ”ç©¶äººå‘˜é€šè¿‡åˆ†æ GPT-4o å’Œæ¨ç†å¯¼å‘æ¨¡å‹ OSS-120B åœ¨å¤šé€‰é¢˜è€ƒè¯•ä¸­çš„è¡¨ç°ï¼Œç»“åˆä¸‰ç§ä¸åŒçš„æç¤ºç­–ç•¥ (Prompting Strategies) æ„å»ºäº†æ­£ç¡®æ€§é¢„æµ‹å™¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™ç§åŸºäºå…ƒæ•°æ®çš„æ–¹æ³•èƒ½å¤Ÿå°†é¢„æµ‹å‡†ç¡®ç‡æœ€é«˜æå‡ 7.14%ï¼Œå¹¶è¾¾åˆ° 83.12% çš„ç²¾ç¡®åº¦ã€‚è™½ç„¶å®é™…çš„å¹»è§‰è¡Œä¸ºæ˜¯åˆ¤æ–­é”™è¯¯çš„é‡è¦æŒ‡æ ‡ï¼Œä½†ç ”ç©¶å‘ç°ä»…å‡­å…ƒæ•°æ®ä¿¡å·æ— æ³•å¯é åœ°é¢„æµ‹å¹»è§‰çš„å‘ç”Ÿã€‚æ­¤å¤–ï¼Œä¸åŒçš„æç¤ºç­–ç•¥ä¼šæ˜¾è‘—æ”¹å˜æ¨¡å‹çš„å†…éƒ¨è¡Œä¸ºåŠå…ƒæ•°æ®çš„é¢„æµ‹ä»·å€¼ï¼Œå³ä¾¿å®ƒä»¬å¯¹æ•´ä½“å‡†ç¡®ç‡çš„å½±å“æœ‰é™ã€‚è¯¥ç ”ç©¶ä¸ºæå‡ LLMs çš„å¯é æ€§æä¾›äº†é‡è¦å‚è€ƒï¼Œä½†ä¹Ÿå¼ºè°ƒç›®å‰çš„æŠ€æœ¯æ‰‹æ®µå°šä¸è¶³ä»¥ç›´æ¥åº”ç”¨äºå…³é”®çš„ä¸´åºŠéƒ¨ç½²ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted as a Short Paper at HEALTHINF2026",
      "pdf_url": "https://arxiv.org/pdf/2512.22508v1",
      "published_date": "2025-12-27 07:51:50 UTC",
      "updated_date": "2025-12-27 07:51:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:15:07.457175+00:00"
    },
    {
      "arxiv_id": "2512.22496v1",
      "title": "Hierarchical Pedagogical Oversight: A Multi-Agent Adversarial Framework for Reliable AI Tutoring",
      "title_zh": "åˆ†å±‚æ•™å­¦ç›‘ç£ï¼šé¢å‘å¯é  AI è¾…å¯¼çš„å¤šæ™ºèƒ½ä½“å¯¹æŠ—æ¡†æ¶",
      "authors": [
        "Saisab Sadhu",
        "Ashim Dhor"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed as automated tutors to address educator shortages; however, they often fail at pedagogical reasoning, frequently validating incorrect student solutions (sycophancy) or providing overly direct answers that hinder learning. We introduce Hierarchical Pedagogical Oversight (HPO), a framework that adapts structured adversarial synthesis to educational assessment. Unlike cooperative multi-agent systems that often drift toward superficial consensus, HPO enforces a dialectical separation of concerns: specialist agents first distill dialogue context, which then grounds a moderated, five-act debate between opposing pedagogical critics. We evaluate this framework on the MRBench dataset of 1,214 middle-school mathematics dialogues. Our 8B-parameter model achieves a Macro F1 of 0.845, outperforming GPT-4o (0.812) by 3.3% while using 20 times fewer parameters. These results establish adversarial reasoning as a critical mechanism for deploying reliable, low-compute pedagogical oversight in resource-constrained environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)ä½œä¸ºè‡ªåŠ¨è¾…å¯¼å‘˜æ—¶åœ¨æ•™å­¦æ¨ç†æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†Hierarchical Pedagogical Oversight (HPO)æ¡†æ¶ã€‚é’ˆå¯¹LLMè¾…å¯¼å‘˜å¸¸å‡ºç°çš„é¡ºä»æ€§(sycophancy)æˆ–è¿‡åº¦ç›´æ¥æä¾›ç­”æ¡ˆç­‰å½±å“å­¦ä¹ çš„é—®é¢˜ï¼ŒHPOå¼•å…¥äº†ç»“æ„åŒ–å¯¹æŠ—åˆæˆ(adversarial synthesis)æŠ€æœ¯è¿›è¡Œæ”¹è¿›ã€‚è¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡ä¸“å®¶æ™ºèƒ½ä½“æç‚¼å¯¹è¯è¯­å¢ƒï¼Œéšåç”±å¯¹ç«‹çš„æ•™å­¦è¯„è®ºå®¶è¿›è¡Œå—æ§çš„â€œäº”å¹•è¾©è®ºâ€ï¼Œä»è€Œå®ç°äº†è¾©è¯æ€§çš„æ•™å­¦ç›‘ç£ã€‚åœ¨åŒ…å«1,214ä¸ªåˆä¸­æ•°å­¦å¯¹è¯çš„MRBenchæ•°æ®é›†ä¸Šï¼Œè¯¥æ¡†æ¶ä¸‹çš„8Bå‚æ•°æ¨¡å‹åœ¨Macro F1æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†0.845ï¼Œä»¥äºŒååˆ†ä¹‹ä¸€çš„å‚æ•°é‡è¶…è¶Šäº†GPT-4o (0.812)ã€‚è¯¥ç ”ç©¶ç»“æœè¯æ˜ï¼Œå¯¹æŠ—æ€§æ¨ç†æ˜¯å®ç°åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹éƒ¨ç½²å¯é ã€ä½åŠŸè€—æ•™å­¦ç›‘ç®¡æœºåˆ¶çš„å…³é”®ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "Accepted for presentation at the AAAI 2026 EGSAI Community Activity (AAAI 2026)",
      "pdf_url": "https://arxiv.org/pdf/2512.22496v1",
      "published_date": "2025-12-27 06:42:07 UTC",
      "updated_date": "2025-12-27 06:42:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:15:18.807711+00:00"
    },
    {
      "arxiv_id": "2512.22492v1",
      "title": "Role-Based Fault Tolerance System for LLM RL Post-Training",
      "title_zh": "é¢å‘å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ åè®­ç»ƒçš„åŸºäºè§’è‰²çš„å®¹é”™ç³»ç»Ÿ",
      "authors": [
        "Zhenqian Chen",
        "Baoquan Zhong",
        "Xiang Li",
        "Qing Dai",
        "Xinkui Zhao",
        "Miao Ye",
        "Ren Cheng",
        "Lufei Zhang",
        "Jianwei Yin"
      ],
      "abstract": "RL post-training for LLMs has been widely scaled to enhance reasoning and tool-using capabilities. However, RL post-training interleaves training and inference workloads, exposing the system to faults from both sides. Existing fault tolerance frameworks for LLMs target either training or inference, leaving the optimization potential in the asynchronous execution unexplored for RL. Our key insight is role-based fault isolation so the failure in one machine does not affect the others. We treat trainer, rollout, and other management roles in RL training as distinct distributed sub-tasks. Instead of restarting the entire RL task in ByteRobust, we recover only the failed role and reconnect it to living ones, thereby eliminating the full-restart overhead including rollout replay and initialization delay.\n  We present RobustRL, the first comprehensive robust system to handle GPU machine errors for RL post-training Effective Training Time Ratio improvement. (1) \\textit{Detect}. We implement role-aware monitoring to distinguish actual failures from role-specific behaviors to avoid the false positive and delayed detection. (2) \\textit{Restart}. For trainers, we implement a non-disruptive recovery where rollouts persist state and continue trajectory generation, while the trainer is rapidly restored via rollout warm standbys. For rollout, we perform isolated machine replacement without interrupting the RL task. (3) \\textit{Reconnect}. We replace static collective communication with dynamic, UCX-based (Unified Communication X) point-to-point communication, enabling immediate weight synchronization between recovered roles. In an RL training task on a 256-GPU cluster with Qwen3-8B-Math workload under 10\\% failure injection frequency, RobustRL can achieve an ETTR of over 80\\% compared with the 60\\% in ByteRobust and achieves 8.4\\%-17.4\\% faster in end-to-end training time.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RobustRLï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLM) å¼ºåŒ–å­¦ä¹  (RL) åè®­ç»ƒè®¾è®¡çš„ç»¼åˆæ€§å®¹é”™ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³è®­ç»ƒä¸æ¨ç†å·¥ä½œè´Ÿè½½äº¤ç»‡å¸¦æ¥çš„å¼‚æ­¥æ‰§è¡Œæ•…éšœé—®é¢˜ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºåŸºäºè§’è‰²çš„æ•…éšœéš”ç¦» (Role-Based Fault Isolation)ï¼Œå°† trainerã€rollout åŠç®¡ç†è§’è‰²è§†ä¸ºç‹¬ç«‹çš„åˆ†å¸ƒå¼å­ä»»åŠ¡ï¼Œä½¿å¾—å•ä¸€æœºå™¨æ•…éšœä¸ä¼šå½±å“å…¶ä»–èŠ‚ç‚¹ã€‚å½“æ•…éšœå‘ç”Ÿæ—¶ï¼Œç³»ç»Ÿä»…æ¢å¤å—æŸè§’è‰²å¹¶å°†å…¶é‡æ–°è¿æ¥è‡³è¿è¡Œä¸­çš„èŠ‚ç‚¹ï¼Œä»è€Œæ¶ˆé™¤äº†ä¼ ç»Ÿæ¡†æ¶ä¸­å…¨é‡é‡å¯ (Full-Restart) å¸¦æ¥çš„é‡æ’­ä¸åˆå§‹åŒ–å¼€é”€ã€‚RobustRL å®ç°äº†è§’è‰²æ„ŸçŸ¥ç›‘æ§ä»¥æé«˜æ£€æµ‹ç²¾åº¦ï¼Œå¹¶é€šè¿‡åŸºäº UCX (Unified Communication X) çš„åŠ¨æ€ç‚¹å¯¹ç‚¹é€šä¿¡å–ä»£é™æ€é›†åˆé€šä¿¡ï¼Œæ”¯æŒæ¢å¤è§’è‰²é—´çš„å³æ—¶æƒé‡åŒæ­¥ã€‚åœ¨ 256 ä¸ª GPU é›†ç¾¤ä¸Šçš„ Qwen3-8B-Math å®éªŒè¡¨æ˜ï¼Œåœ¨ 10% æ•…éšœæ³¨å…¥é¢‘ç‡ä¸‹ï¼Œè¯¥ç³»ç»Ÿçš„æœ‰æ•ˆè®­ç»ƒæ—¶é—´æ¯” (ETTR) è¾¾åˆ° 80% ä»¥ä¸Šï¼Œæ˜¾è‘—ä¼˜äº ByteRobust çš„ 60%ã€‚æœ€ç»ˆï¼ŒRobustRL åœ¨ç«¯åˆ°ç«¯è®­ç»ƒé€Ÿåº¦ä¸Šå®ç°äº† 8.4%-17.4% çš„æå‡ï¼Œä¸ºå¤§è§„æ¨¡ LLM çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡æä¾›äº†é«˜æ•ˆä¸”é²æ£’çš„ç³»ç»Ÿæ”¯æ’‘ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "16 pages, 19 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.22492v1",
      "published_date": "2025-12-27 06:30:18 UTC",
      "updated_date": "2025-12-27 06:30:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:15:13.848296+00:00"
    },
    {
      "arxiv_id": "2512.22491v1",
      "title": "ManchuTTS: Towards High-Quality Manchu Speech Synthesis via Flow Matching and Hierarchical Text Representation",
      "title_zh": "ManchuTTSï¼šåŸºäºæµåŒ¹é…ä¸åˆ†å±‚æ–‡æœ¬è¡¨ç¤ºçš„é«˜è´¨é‡æ»¡è¯­è¯­éŸ³åˆæˆ",
      "authors": [
        "Suhua Wang",
        "Zifan Wang",
        "Xiaoxin Sun",
        "D. J. Wang",
        "Zhanbo Liu",
        "Xin Li"
      ],
      "abstract": "As an endangered language, Manchu presents unique challenges for speech synthesis, including severe data scarcity and strong phonological agglutination. This paper proposes ManchuTTS(Manchu Text to Speech), a novel approach tailored to Manchu's linguistic characteristics. To handle agglutination, this method designs a three-tier text representation (phoneme, syllable, prosodic) and a cross-modal hierarchical attention mechanism for multi-granular alignment. The synthesis model integrates deep convolutional networks with a flow-matching Transformer, enabling efficient, non-autoregressive generation. This method further introduce a hierarchical contrastive loss to guide structured acoustic-linguistic correspondence. To address low-resource constraints, This method construct the first Manchu TTS dataset and employ a data augmentation strategy. Experiments demonstrate that ManchuTTS attains a MOS of 4.52 using a 5.2-hour training subset derived from our full 6.24-hour annotated corpus, outperforming all baseline models by a notable margin. Ablations confirm hierarchical guidance improves agglutinative word pronunciation accuracy (AWPA) by 31% and prosodic naturalness by 27%.",
      "tldr_zh": "é’ˆå¯¹æ»¡è¯­(Manchu)è¿™ä¸€å…·æœ‰å¼ºé»ç€æ€§ç‰¹å¾çš„æ¿’å±è¯­è¨€ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ManchuTTSï¼Œæ—¨åœ¨è§£å†³è¯­éŸ³åˆæˆé¢†åŸŸé¢ä¸´çš„æ•°æ®æåº¦ç¨€ç¼ºå’ŒéŸ³éŸµå¤æ‚ç­‰æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•ä¸“é—¨è®¾è®¡äº†åŒ…å«éŸ³ç´ ã€éŸ³èŠ‚å’ŒéŸµå¾‹çš„ä¸‰å±‚æ–‡æœ¬è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨è·¨æ¨¡æ€å±‚æ¬¡åŒ–æ³¨æ„åŠ›æœºåˆ¶(hierarchical attention mechanism)å®ç°å¤šç²’åº¦å¯¹é½ï¼Œä»¥åº”å¯¹æ»¡è¯­çš„é»ç€æ€§ç‰¹å¾ã€‚åˆæˆæ¶æ„ç»“åˆäº†æ·±åº¦å·ç§¯ç½‘ç»œä¸æµåŒ¹é…(flow-matching) Transformerï¼Œå®ç°äº†é«˜æ•ˆçš„éè‡ªå›å½’(non-autoregressive)ç”Ÿæˆï¼Œå¹¶å¼•å…¥å±‚æ¬¡åŒ–å¯¹æ¯”æŸå¤±(hierarchical contrastive loss)æ¥æŒ‡å¯¼å£°å­¦ä¸è¯­è¨€çš„ç»“æ„åŒ–å¯¹åº”ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜æ„å»ºäº†é¦–ä¸ªæ»¡è¯­TTSæ•°æ®é›†ï¼Œå®éªŒç»“æœæ˜¾ç¤ºManchuTTSåœ¨5.2å°æ—¶çš„è®­ç»ƒæ•°æ®ä¸‹è¾¾åˆ°äº†4.52çš„å¹³å‡ä¸»è§‚è¯„åˆ†(MOS)ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿æ¨¡å‹ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯å®ï¼Œè¿™ç§å±‚æ¬¡åŒ–å¼•å¯¼æœºåˆ¶ä½¿é»ç€è¯å‘éŸ³å‡†ç¡®ç‡(AWPA)æå‡äº†31%ï¼ŒéŸµå¾‹è‡ªç„¶åº¦æå‡äº†27%ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22491v1",
      "published_date": "2025-12-27 06:21:35 UTC",
      "updated_date": "2025-12-27 06:21:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:15:21.142325+00:00"
    },
    {
      "arxiv_id": "2512.22481v1",
      "title": "SPECTRE: Spectral Pre-training Embeddings with Cylindrical Temporal Rotary Position Encoding for Fine-Grained sEMG-Based Movement Decoding",
      "title_zh": "SPECTREï¼šåŸºäºåœ†æŸ±å½¢æ—¶é—´æ—‹è½¬ä½ç½®ç¼–ç çš„é¢‘è°±é¢„è®­ç»ƒåµŒå…¥ï¼Œç”¨äºç»†ç²’åº¦è¡¨é¢è‚Œç”µè¿åŠ¨è§£ç ",
      "authors": [
        "Zihan Weng",
        "Chanlin Yi",
        "Pouya Bashivan",
        "Jing Lu",
        "Fali Li",
        "Dezhong Yao",
        "Jingming Hou",
        "Yangsong Zhang",
        "Peng Xu"
      ],
      "abstract": "Decoding fine-grained movement from non-invasive surface Electromyography (sEMG) is a challenge for prosthetic control due to signal non-stationarity and low signal-to-noise ratios. Generic self-supervised learning (SSL) frameworks often yield suboptimal results on sEMG as they attempt to reconstruct noisy raw signals and lack the inductive bias to model the cylindrical topology of electrode arrays. To overcome these limitations, we introduce SPECTRE, a domain-specific SSL framework. SPECTRE features two primary contributions: a physiologically-grounded pre-training task and a novel positional encoding. The pre-training involves masked prediction of discrete pseudo-labels from clustered Short-Time Fourier Transform (STFT) representations, compelling the model to learn robust, physiologically relevant frequency patterns. Additionally, our Cylindrical Rotary Position Embedding (CyRoPE) factorizes embeddings along linear temporal and annular spatial dimensions, explicitly modeling the forearm sensor topology to capture muscle synergies. Evaluations on multiple datasets, including challenging data from individuals with amputation, demonstrate that SPECTRE establishes a new state-of-the-art for movement decoding, significantly outperforming both supervised baselines and generic SSL approaches. Ablation studies validate the critical roles of both spectral pre-training and CyRoPE. SPECTRE provides a robust foundation for practical myoelectric interfaces capable of handling real-world sEMG complexities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éä¾µå…¥å¼è¡¨é¢è‚Œç”µä¿¡å·(sEMG)è§£ç ä¸­å­˜åœ¨çš„ä¿¡å·éå¹³ç¨³æ€§å’Œä½ä¿¡å™ªæ¯”æŒ‘æˆ˜ï¼Œæå‡ºäº†SPECTREè¿™ä¸€é¢†åŸŸä¸“ç”¨è‡ªç›‘ç£å­¦ä¹ (SSL)æ¡†æ¶ã€‚ä¸ºäº†å…‹æœé€šç”¨SSLåœ¨å¤„ç†å™ªå£°åŸå§‹ä¿¡å·åŠç¼ºä¹ä¼ æ„Ÿå™¨æ‹“æ‰‘åå·®æ–¹é¢çš„å±€é™ï¼ŒSPECTREå¼•å…¥äº†åŸºäºSTFT(Short-Time Fourier Transform)èšç±»ä¼ªæ ‡ç­¾çš„æ©ç é¢„æµ‹é¢„è®­ç»ƒä»»åŠ¡ï¼Œå¼ºåˆ¶æ¨¡å‹å­¦ä¹ å…·æœ‰ç”Ÿç†ç›¸å…³æ€§çš„é¢‘ç‡æ¨¡å¼ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹åœ†æŸ±å½¢æ—‹è½¬ä½ç½®ç¼–ç (CyRoPE)ï¼Œé€šè¿‡å¯¹æ—¶é—´çº¿æ€§ç»´åº¦å’Œç©ºé—´ç¯å½¢ç»´åº¦è¿›è¡Œå› å­åŒ–åˆ†è§£ï¼Œæ˜¾å¼å»ºæ¨¡å‰è‡‚ä¼ æ„Ÿå™¨é˜µåˆ—çš„æ‹“æ‰‘ç»“æ„ä»¥æ•æ‰è‚Œè‚‰ååŒä½œç”¨ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒSPECTREåœ¨åŠ¨ä½œè§£ç ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç›‘ç£åŸºå‡†åŠé€šç”¨SSLæ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨æå…·æŒ‘æˆ˜æ€§çš„æ®‹ç–¾äººæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æ–°çš„State-of-the-artã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥éªŒè¯äº†é¢‘è°±é¢„è®­ç»ƒä¸CyRoPEçš„å…³é”®ä½œç”¨ï¼Œä¸ºå¤„ç†çœŸå®ä¸–ç•Œå¤æ‚sEMGä¿¡å·çš„å®ç”¨åŒ–è‚Œç”µäº¤äº’ç•Œé¢å¥ å®šäº†åšå®åŸºç¡€ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22481v1",
      "published_date": "2025-12-27 05:55:06 UTC",
      "updated_date": "2025-12-27 05:55:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:15:20.810426+00:00"
    },
    {
      "arxiv_id": "2512.22473v2",
      "title": "Gradient Dynamics of Attention: How Cross-Entropy Sculpts Bayesian Manifolds",
      "title_zh": "æ³¨æ„åŠ›æ¢¯åº¦åŠ¨åŠ›å­¦ï¼šäº¤å‰ç†µå¦‚ä½•å¡‘é€ è´å¶æ–¯æµå½¢",
      "authors": [
        "Naman Agarwal",
        "Siddhartha R. Dalal",
        "Vishal Misra"
      ],
      "abstract": "Transformers empirically perform precise probabilistic reasoning in carefully constructed ``Bayesian wind tunnels'' and in large-scale language models, yet the mechanisms by which gradient-based learning creates the required internal geometry remain opaque. We provide a complete first-order analysis of how cross-entropy training reshapes attention scores and value vectors in a transformer attention head. Our core result is an \\emph{advantage-based routing law} for attention scores, \\[ \\frac{\\partial L}{\\partial s_{ij}} = Î±_{ij}\\bigl(b_{ij}-\\mathbb{E}_{Î±_i}[b]\\bigr), \\qquad b_{ij} := u_i^\\top v_j, \\] coupled with a \\emph{responsibility-weighted update} for values, \\[ Î”v_j = -Î·\\sum_i Î±_{ij} u_i, \\] where $u_i$ is the upstream gradient at position $i$ and $Î±_{ij}$ are attention weights. These equations induce a positive feedback loop in which routing and content specialize together: queries route more strongly to values that are above-average for their error signal, and those values are pulled toward the queries that use them. We show that this coupled specialization behaves like a two-timescale EM procedure: attention weights implement an E-step (soft responsibilities), while values implement an M-step (responsibility-weighted prototype updates), with queries and keys adjusting the hypothesis frame. Through controlled simulations, including a sticky Markov-chain task where we compare a closed-form EM-style update to standard SGD, we demonstrate that the same gradient dynamics that minimize cross-entropy also sculpt the low-dimensional manifolds identified in our companion work as implementing Bayesian inference. This yields a unified picture in which optimization (gradient flow) gives rise to geometry (Bayesian manifolds), which in turn supports function (in-context probabilistic reasoning).",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡ä¸€é˜¶åˆ†æ(first-order analysis)æ¢è®¨äº†åŸºäºæ¢¯åº¦çš„å­¦ä¹ å¦‚ä½•å¡‘é€ Transformeræ³¨æ„åŠ›å¤´å†…éƒ¨çš„å‡ ä½•ç»“æ„ï¼Œä»¥å®ç°ç²¾ç¡®çš„æ¦‚ç‡æ¨ç†ã€‚è®ºæ–‡æå‡ºäº†é’ˆå¯¹æ³¨æ„åŠ›åˆ†æ•°çš„ä¼˜åŠ¿è·¯ç”±æ³•åˆ™(advantage-based routing law)ï¼Œæ­ç¤ºäº†äº¤å‰ç†µ(cross-entropy)è®­ç»ƒå¦‚ä½•é€šè¿‡æ­£åé¦ˆå¾ªç¯é‡å¡‘æ³¨æ„åŠ›æƒé‡ä¸å€¼å‘é‡(value vectors)ã€‚ç ”ç©¶å‘ç°æŸ¥è¯¢(queries)ä¼šæ›´å¼ºåœ°è·¯ç”±è‡³é‚£äº›ä¼˜äºå¹³å‡è¯¯å·®ä¿¡å·çš„å€¼å‘é‡ï¼Œè€Œå€¼å‘é‡åˆ™é€šè¿‡è´£ä»»åŠ æƒæ›´æ–°(responsibility-weighted update)å‘ä½¿ç”¨å®ƒä»¬çš„æŸ¥è¯¢æ–¹å‘ç§»åŠ¨ã€‚è¿™ç§è€¦åˆçš„ä¸“é—¨åŒ–è¿‡ç¨‹åœ¨è¡Œä¸ºä¸Šç±»ä¼¼äºä¸€ç§åŒæ—¶é—´å°ºåº¦(two-timescale)çš„EMç®—æ³•ï¼Œå…¶ä¸­æ³¨æ„åŠ›æƒé‡æ‰§è¡ŒEæ­¥ï¼Œå€¼å‘é‡æ›´æ–°æ‰§è¡ŒMæ­¥ã€‚é€šè¿‡å—æ§æ¨¡æ‹Ÿå’Œç²˜æ€§é©¬å°”å¯å¤«é“¾(sticky Markov-chain)ä»»åŠ¡ï¼Œç ”ç©¶è¯æ˜äº†æ¢¯åº¦åŠ¨åŠ›å­¦åœ¨æœ€å°åŒ–äº¤å‰ç†µçš„åŒæ—¶ï¼Œä¹Ÿåœ¨å¡‘é€ ç”¨äºæ‰§è¡Œè´å¶æ–¯æ¨ç†(Bayesian inference)çš„ä½ç»´æµå½¢(manifolds)ã€‚è¿™ä¸€å‘ç°å»ºç«‹äº†ä¸€ä¸ªç»Ÿä¸€çš„å›¾æ™¯ï¼Œå°†ä¼˜åŒ–è¿‡ç¨‹ã€å‡ ä½•ç»“æ„ä¸ä¸Šä¸‹æ–‡æ¦‚ç‡æ¨ç†åŠŸèƒ½æœ‰æœºç»“åˆã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22473v2",
      "published_date": "2025-12-27 05:31:44 UTC",
      "updated_date": "2026-01-07 23:13:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:15:25.366228+00:00"
    },
    {
      "arxiv_id": "2512.23752v2",
      "title": "Geometric Scaling of Bayesian Inference in LLMs",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­è´å¶æ–¯æ¨ç†çš„å‡ ä½•ç¼©æ”¾",
      "authors": [
        "Naman Agarwal",
        "Siddhartha R. Dalal",
        "Vishal Misra"
      ],
      "abstract": "Recent work has shown that small transformers trained in controlled \"wind-tunnel'' settings can implement exact Bayesian inference, and that their training dynamics produce a geometric substrate -- low-dimensional value manifolds and progressively orthogonal keys -- that encodes posterior structure. We investigate whether this geometric signature persists in production-grade language models. Across Pythia, Phi-2, Llama-3, and Mistral families, we find that last-layer value representations organize along a single dominant axis whose position strongly correlates with predictive entropy, and that domain-restricted prompts collapse this structure into the same low-dimensional manifolds observed in synthetic settings.\n  To probe the role of this geometry, we perform targeted interventions on the entropy-aligned axis of Pythia-410M during in-context learning. Removing or perturbing this axis selectively disrupts the local uncertainty geometry, whereas matched random-axis interventions leave it intact. However, these single-layer manipulations do not produce proportionally specific degradation in Bayesian-like behavior, indicating that the geometry is a privileged readout of uncertainty rather than a singular computational bottleneck. Taken together, our results show that modern language models preserve the geometric substrate that enables Bayesian inference in wind tunnels, and organize their approximate Bayesian updates along this substrate.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç°ä»£å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹(LLMs)æ˜¯å¦ä¿ç•™äº†åœ¨å°å‹å®éªŒæ€§æ¨¡å‹ä¸­è§‚å¯Ÿåˆ°çš„ï¼Œç”¨äºå®ç°ç²¾ç¡® Bayesian inference çš„å‡ ä½•åŸºç¡€ã€‚é€šè¿‡åˆ†æ Pythiaã€Phi-2ã€Llama-3 å’Œ Mistral ç­‰æ¨¡å‹å®¶æ—ï¼Œç ”ç©¶å‘ç°å…¶æœ€åä¸€å±‚çš„ value representations æ²¿ç€ä¸€ä¸ªä¸é¢„æµ‹ç†µ(predictive entropy)å¼ºç›¸å…³çš„å•ä¸€ä¸»è½´æ’åˆ—ï¼Œä¸”é¢†åŸŸé™åˆ¶æç¤º(domain-restricted prompts)èƒ½ä½¿è¯¥ç»“æ„åç¼©ä¸ºç‰¹å®šçš„ä½ç»´ manifoldsã€‚ç ”ç©¶è€…åœ¨ Pythia-410M çš„ in-context learning è¿‡ç¨‹ä¸­å¯¹è¯¥ç†µå¯¹é½è½´è¿›è¡Œäº†å®šå‘å¹²é¢„ï¼Œå‘ç°ç§»é™¤æˆ–æ‰°åŠ¨è¯¥è½´ä¼šé€‰æ‹©æ€§åœ°ç ´åå±€éƒ¨ä¸ç¡®å®šæ€§å‡ ä½•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§å‡ ä½•ç»“æ„æ˜¯æ¨¡å‹ä¸ç¡®å®šæ€§çš„ç‰¹æƒè¯»å–(privileged readout)ï¼Œè€Œéå•ä¸€çš„è®¡ç®—ç“¶é¢ˆã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥é¡¹å·¥ä½œè¯æ˜äº†ç°ä»£è¯­è¨€æ¨¡å‹ç»´æŒäº†å®ç° Bayesian inference çš„å‡ ä½•åº•å±‚ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šç»„ç»‡å…¶è¿‘ä¼¼çš„ Bayesian updatesã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.23752v2",
      "published_date": "2025-12-27 05:29:55 UTC",
      "updated_date": "2026-01-07 23:05:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:15:55.533269+00:00"
    },
    {
      "arxiv_id": "2512.22471v2",
      "title": "The Bayesian Geometry of Transformer Attention",
      "title_zh": "Transformer æ³¨æ„åŠ›æœºåˆ¶çš„è´å¶æ–¯å‡ ä½•",
      "authors": [
        "Naman Agarwal",
        "Siddhartha R. Dalal",
        "Vishal Misra"
      ],
      "abstract": "Transformers often appear to perform Bayesian reasoning in context, but verifying this rigorously has been impossible: natural data lack analytic posteriors, and large models conflate reasoning with memorization. We address this by constructing \\emph{Bayesian wind tunnels} -- controlled environments where the true posterior is known in closed form and memorization is provably impossible. In these settings, small transformers reproduce Bayesian posteriors with $10^{-3}$-$10^{-4}$ bit accuracy, while capacity-matched MLPs fail by orders of magnitude, establishing a clear architectural separation.\n  Across two tasks -- bijection elimination and Hidden Markov Model (HMM) state tracking -- we find that transformers implement Bayesian inference through a consistent geometric mechanism: residual streams serve as the belief substrate, feed-forward networks perform the posterior update, and attention provides content-addressable routing. Geometric diagnostics reveal orthogonal key bases, progressive query-key alignment, and a low-dimensional value manifold parameterized by posterior entropy. During training this manifold unfurls while attention patterns remain stable, a \\emph{frame-precision dissociation} predicted by recent gradient analyses.\n  Taken together, these results demonstrate that hierarchical attention realizes Bayesian inference by geometric design, explaining both the necessity of attention and the failure of flat architectures. Bayesian wind tunnels provide a foundation for mechanistically connecting small, verifiable systems to reasoning phenomena observed in large language models.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº† Transformers åœ¨ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œ Bayesian reasoning çš„å‡ ä½•æœºåˆ¶ï¼Œé€šè¿‡æ„å»ºå—æ§çš„ \"Bayesian wind tunnels\" ç¯å¢ƒï¼Œè§£å†³äº†è‡ªç„¶æ•°æ®ç¼ºä¹è§£æåéªŒåˆ†å¸ƒä»¥åŠæ¨¡å‹éš¾ä»¥åŒºåˆ†æ¨ç†ä¸è®°å¿†çš„é—®é¢˜ã€‚åœ¨åŒå°„æ¶ˆé™¤å’Œ Hidden Markov Model (HMM) çŠ¶æ€è·Ÿè¸ªä»»åŠ¡ä¸­ï¼Œå°å‹ Transformers å±•ç°äº†æé«˜çš„åéªŒæ¨¡æ‹Ÿç²¾åº¦ï¼Œè€ŒåŒç­‰å®¹é‡çš„ MLPs åˆ™è¡¨ç°æ¬ ä½³ï¼Œç¡®ç«‹äº†æ¶æ„ä¸Šçš„æ˜¾è‘—ä»£å·®ã€‚ç ”ç©¶å‘ç° Transformers é€šè¿‡ä¸€è‡´çš„å‡ ä½•æœºåˆ¶å®ç° Bayesian inferenceï¼Œå…¶ä¸­ residual streams ä½œä¸ºä¿¡å¿µåŸºè´¨ï¼Œfeed-forward networks æ‰§è¡ŒåéªŒæ›´æ–°ï¼Œè€Œ attention è´Ÿè´£å†…å®¹å¯»å€è·¯ç”±ã€‚å‡ ä½•è¯Šæ–­æ­ç¤ºäº†å…¶å…·å¤‡ orthogonal key basesã€æ¸è¿›å¼ query-key alignment ä»¥åŠç”±åéªŒç†µå‚æ•°åŒ–çš„ä½ç»´ value manifoldã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¯¥æµå½¢é€æ¸å±•å¼€è€Œ attention patterns ä¿æŒç¨³å®šï¼ŒéªŒè¯äº†è¿‘æœŸæ¢¯åº¦åˆ†ææ‰€é¢„æµ‹çš„ frame-precision dissociation ç°è±¡ã€‚è¯¥å·¥ä½œè¯æ˜äº†åˆ†å±‚ attention ç»“æ„é€šè¿‡å‡ ä½•è®¾è®¡å®ç°äº† Bayesian inferenceï¼Œä¸ºæœºæ¢°åŒ–åœ°è¿æ¥å°å‹å¯éªŒè¯ç³»ç»Ÿä¸å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ¨ç†ç°è±¡æä¾›äº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22471v2",
      "published_date": "2025-12-27 05:28:58 UTC",
      "updated_date": "2026-01-07 23:02:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:16:20.299339+00:00"
    },
    {
      "arxiv_id": "2512.22470v1",
      "title": "DarkPatterns-LLM: A Multi-Layer Benchmark for Detecting Manipulative and Harmful AI Behavior",
      "title_zh": "DarkPatterns-LLMï¼šä¸€ç§ç”¨äºæ£€æµ‹æ“çºµæ€§åŠæœ‰å®³äººå·¥æ™ºèƒ½è¡Œä¸ºçš„å¤šå±‚è¯„ä¼°åŸºå‡†",
      "authors": [
        "Sadia Asif",
        "Israel Antonio Rosales Laguan",
        "Haris Khan",
        "Shumaila Asif",
        "Muneeb Asif"
      ],
      "abstract": "The proliferation of Large Language Models (LLMs) has intensified concerns about manipulative or deceptive behaviors that can undermine user autonomy, trust, and well-being. Existing safety benchmarks predominantly rely on coarse binary labels and fail to capture the nuanced psychological and social mechanisms constituting manipulation. We introduce \\textbf{DarkPatterns-LLM}, a comprehensive benchmark dataset and diagnostic framework for fine-grained assessment of manipulative content in LLM outputs across seven harm categories: Legal/Power, Psychological, Emotional, Physical, Autonomy, Economic, and Societal Harm. Our framework implements a four-layer analytical pipeline comprising Multi-Granular Detection (MGD), Multi-Scale Intent Analysis (MSIAN), Threat Harmonization Protocol (THP), and Deep Contextual Risk Alignment (DCRA). The dataset contains 401 meticulously curated examples with instruction-response pairs and expert annotations. Through evaluation of state-of-the-art models including GPT-4, Claude 3.5, and LLaMA-3-70B, we observe significant performance disparities (65.2\\%--89.7\\%) and consistent weaknesses in detecting autonomy-undermining patterns. DarkPatterns-LLM establishes the first standardized, multi-dimensional benchmark for manipulation detection in LLMs, offering actionable diagnostics toward more trustworthy AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) ä¸­æ—¥ç›Šä¸¥é‡çš„æ“çºµæ€§å’Œæ¬ºéª—æ€§è¡Œä¸ºï¼Œæå‡ºäº† DarkPatterns-LLMï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç»†ç²’åº¦è¯„ä¼°æ“çºµæ€§å†…å®¹çš„ç»¼åˆåŸºå‡†æ•°æ®é›†å’Œè¯Šæ–­æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ¶µç›–äº†æ³•å¾‹/æƒåŠ›ã€å¿ƒç†ã€æƒ…æ„Ÿã€èº«ä½“ã€è‡ªä¸»æ€§ã€ç»æµå’Œç¤¾ä¼šå±å®³ä¸ƒå¤§ç»´åº¦ï¼Œå¹¶å®æ–½äº†ç”±å¤šç²’åº¦æ£€æµ‹ (MGD)ã€å¤šå°ºåº¦æ„å›¾åˆ†æ (MSIAN)ã€å¨èƒåè°ƒåè®® (THP) å’Œæ·±åº¦ä¸Šä¸‹æ–‡é£é™©å¯¹é½ (DCRA) ç»„æˆçš„å››å±‚åˆ†ææµæ°´çº¿ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªåŒ…å« 401 ä¸ªä¸“å®¶æ ‡æ³¨ç¤ºä¾‹çš„æ•°æ®é›†ï¼Œå¹¶å¯¹ GPT-4ã€Claude 3.5 å’Œ LLaMA-3-70B ç­‰ä¸»æµæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå„æ¨¡å‹åœ¨æ£€æµ‹æ“çºµæ¨¡å¼æ—¶çš„è¡¨ç°å·®å¼‚æ˜¾è‘—ï¼ˆå‡†ç¡®ç‡åœ¨ 65.2% è‡³ 89.7% ä¹‹é—´ï¼‰ï¼Œä¸”åœ¨è¯†åˆ«å‰Šå¼±è‡ªä¸»æ€§çš„æ¨¡å¼æ–¹é¢è¡¨ç°å‡ºä¸€è‡´çš„å¼±ç‚¹ã€‚ä½œä¸ºé¦–ä¸ªæ ‡å‡†åŒ–çš„å¤šç»´ LLM æ“çºµæ£€æµ‹åŸºå‡†ï¼ŒDarkPatterns-LLM ä¸ºå¼€å‘æ›´å…·ä¿¡ä»»æ„Ÿçš„ AI ç³»ç»Ÿæä¾›äº†å…³é”®çš„è¯Šæ–­å·¥å…·å’Œå®è¯æ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22470v1",
      "published_date": "2025-12-27 05:05:46 UTC",
      "updated_date": "2025-12-27 05:05:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:16:53.658913+00:00"
    },
    {
      "arxiv_id": "2512.22466v1",
      "title": "AMBIT: Augmenting Mobility Baselines with Interpretable Trees",
      "title_zh": "AMBITï¼šåˆ©ç”¨å¯è§£é‡Šæ ‘å¢å¼ºç§»åŠ¨æ€§åŸºå‡†",
      "authors": [
        "Qizhi Wang"
      ],
      "abstract": "Origin-destination (OD) flow prediction remains a core task in GIS and urban analytics, yet practical deployments face two conflicting needs: high accuracy and clear interpretability. This paper develops AMBIT, a gray-box framework that augments physical mobility baselines with interpretable tree models. We begin with a comprehensive audit of classical spatial interaction models on a year-long, hourly NYC taxi OD dataset. The audit shows that most physical models are fragile at this temporal resolution; PPML gravity is the strongest physical baseline, while constrained variants improve when calibrated on full OD margins but remain notably weaker. We then build residual learners on top of physical baselines using gradient-boosted trees and SHAP analysis, demonstrating that (i) physics-grounded residuals approach the accuracy of a strong tree-based predictor while retaining interpretable structure, and (ii) POI-anchored residuals are consistently competitive and most robust under spatial generalization. We provide a reproducible pipeline, rich diagnostics, and spatial error analysis designed for urban decision-making.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AMBITï¼Œä¸€ç§å°†ç‰©ç†ç§»åŠ¨æ€§(mobility)åŸºå‡†ä¸å¯è§£é‡Šæ ‘æ¨¡å‹ç›¸ç»“åˆçš„ç°ç›’(gray-box)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³Origin-destination (OD)æµé¢„æµ‹ä¸­é«˜å‡†ç¡®åº¦ä¸æ¸…æ™°å¯è§£é‡Šæ€§ä¹‹é—´çš„çŸ›ç›¾ã€‚ç ”ç©¶å›¢é˜Ÿé¦–å…ˆé€šè¿‡çº½çº¦å¸‚å‡ºç§Ÿè½¦æ•°æ®é›†å¯¹ç»å…¸ç©ºé—´äº¤äº’æ¨¡å‹è¿›è¡Œäº†å…¨é¢å®¡è®¡ï¼Œå‘ç°PPML gravityæ˜¯è¡¨ç°æœ€å¼ºçš„ç‰©ç†åŸºå‡†ï¼Œä½†å¤šæ•°æ¨¡å‹åœ¨ç²¾ç»†çš„æ—¶é—´åˆ†è¾¨ç‡ä¸‹è¡¨ç°è„†å¼±ã€‚AMBITåˆ©ç”¨æ¢¯åº¦æå‡æ ‘(gradient-boosted trees)åœ¨ç‰©ç†åŸºå‡†ä¹‹ä¸Šæ„å»ºæ®‹å·®å­¦ä¹ å™¨ï¼Œå¹¶å¼•å…¥SHAPåˆ†æä»¥å¢å¼ºæ¨¡å‹é€æ˜åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§ç‰©ç†å¢å¼ºçš„æ®‹å·®æ¨¡å‹åœ¨ä¿æŒå¯è§£é‡Šç»“æ„çš„åŒæ—¶ï¼Œå‡†ç¡®åº¦æ¥è¿‘çº¯æ ‘ç±»é¢„æµ‹å™¨ï¼Œä¸”åŸºäºPOIé”šå®šçš„æ®‹å·®åœ¨ç©ºé—´æ³›åŒ–(spatial generalization)ä¸­è¡¨ç°æœ€ä¸ºç¨³å¥ã€‚è¯¥æˆæœä¸ºåŸå¸‚å†³ç­–æä¾›äº†å¯å¤ç°çš„åˆ†ææµç¨‹å’Œç©ºé—´è¯¯å·®è¯Šæ–­å·¥å…·ï¼Œæ¨åŠ¨äº†GISå’ŒåŸå¸‚åˆ†æé¢†åŸŸçš„å¯ä¿¡åº”ç”¨ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages; 12 figures; 30 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.22466v1",
      "published_date": "2025-12-27 04:59:16 UTC",
      "updated_date": "2025-12-27 04:59:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:16:09.982610+00:00"
    },
    {
      "arxiv_id": "2512.22447v1",
      "title": "Towards Robust Optical-SAR Object Detection under Missing Modalities: A Dynamic Quality-Aware Fusion Framework",
      "title_zh": "é¢å‘æ¨¡æ€ç¼ºå¤±çš„é²æ£’å…‰å­¦-SARç›®æ ‡æ£€æµ‹ï¼šä¸€ç§åŠ¨æ€è´¨é‡æ„ŸçŸ¥èåˆæ¡†æ¶",
      "authors": [
        "Zhicheng Zhao",
        "Yuancheng Xu",
        "Andong Lu",
        "Chenglong Li",
        "Jin Tang"
      ],
      "abstract": "Optical and Synthetic Aperture Radar (SAR) fusion-based object detection has attracted significant research interest in remote sensing, as these modalities provide complementary information for all-weather monitoring. However, practical deployment is severely limited by inherent challenges. Due to distinct imaging mechanisms, temporal asynchrony, and registration difficulties, obtaining well-aligned optical-SAR image pairs remains extremely difficult, frequently resulting in missing or degraded modality data. Although recent approaches have attempted to address this issue, they still suffer from limited robustness to random missing modalities and lack effective mechanisms to ensure consistent performance improvement in fusion-based detection. To address these limitations, we propose a novel Quality-Aware Dynamic Fusion Network (QDFNet) for robust optical-SAR object detection. Our proposed method leverages learnable reference tokens to dynamically assess feature reliability and guide adaptive fusion in the presence of missing modalities. In particular, we design a Dynamic Modality Quality Assessment (DMQA) module that employs learnable reference tokens to iteratively refine feature reliability assessment, enabling precise identification of degraded regions and providing quality guidance for subsequent fusion. Moreover, we develop an Orthogonal Constraint Normalization Fusion (OCNF) module that employs orthogonal constraints to preserve modality independence while dynamically adjusting fusion weights based on reliability scores, effectively suppressing unreliable feature propagation. Extensive experiments on the SpaceNet6-OTD and OGSOD-2.0 datasets demonstrate the superiority and effectiveness of QDFNet compared to state-of-the-art methods, particularly under partial modality corruption or missing data scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…‰å­¦(Optical)ä¸åˆæˆå­”å¾„é›·è¾¾(SAR)èåˆç›®æ ‡æ£€æµ‹ä¸­å› æˆåƒæœºåˆ¶å·®å¼‚ã€æ—¶é—´ä¸åŒæ­¥åŠé…å‡†å›°éš¾å¯¼è‡´çš„æ¨¡æ€ç¼ºå¤±æˆ–é€€åŒ–é—®é¢˜ï¼Œæå‡ºäº†è´¨é‡æ„ŸçŸ¥åŠ¨æ€èåˆç½‘ç»œ(Quality-Aware Dynamic Fusion Network, QDFNet)ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¯å­¦ä¹ çš„å‚è€ƒä»¤ç‰Œ(learnable reference tokens)åŠ¨æ€è¯„ä¼°ç‰¹å¾å¯é æ€§ï¼Œå¹¶è®¾è®¡äº†åŠ¨æ€æ¨¡æ€è´¨é‡è¯„ä¼°(DMQA)æ¨¡å—æ¥è¿­ä»£ç²¾ç»†åŒ–ç‰¹å¾å¯é æ€§è¯„ä¼°ï¼Œä»è€Œç²¾å‡†è¯†åˆ«é€€åŒ–åŒºåŸŸã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†æ­£äº¤çº¦æŸå½’ä¸€åŒ–èåˆ(OCNF)æ¨¡å—ï¼Œé€šè¿‡æ­£äº¤çº¦æŸä¿æŒæ¨¡æ€ç‹¬ç«‹æ€§ï¼Œå¹¶ä¾æ®å¯é æ€§è¯„åˆ†åŠ¨æ€è°ƒæ•´èåˆæƒé‡ä»¥æœ‰æ•ˆæŠ‘åˆ¶ä¸å¯é ç‰¹å¾çš„ä¼ æ’­ã€‚åœ¨SpaceNet6-OTDå’ŒOGSOD-2.0æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼ŒQDFNetåœ¨éƒ¨åˆ†æ¨¡æ€æŸåæˆ–ç¼ºå¤±çš„æƒ…æ™¯ä¸‹å±•ç°å‡ºæ¯”ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•æ›´ä¼˜å¼‚çš„æ€§èƒ½ã€‚è¯¥æ–¹æ¡ˆæ˜¾è‘—å¢å¼ºäº†å¤šæ¨¡æ€èåˆæ¨¡å‹åœ¨å¤æ‚å®é™…éƒ¨ç½²ç¯å¢ƒä¸­çš„é²æ£’æ€§ä¸æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22447v1",
      "published_date": "2025-12-27 03:16:48 UTC",
      "updated_date": "2025-12-27 03:16:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:15:50.117781+00:00"
    },
    {
      "arxiv_id": "2512.22442v1",
      "title": "HiFi-RAG: Hierarchical Content Filtering and Two-Pass Generation for Open-Domain RAG",
      "title_zh": "HiFi-RAGï¼šé¢å‘å¼€æ”¾åŸŸ RAG çš„å±‚çº§åŒ–å†…å®¹è¿‡æ»¤ä¸åŒé˜¶æ®µç”Ÿæˆ",
      "authors": [
        "Cattalyya Nuengsigkapian"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) in open-domain settings faces significant challenges regarding irrelevant information in retrieved documents and the alignment of generated answers with user intent. We present HiFi-RAG (Hierarchical Filtering RAG), the winning closed-source system in the Text-to-Text static evaluation of the MMU-RAGent NeurIPS 2025 Competition. Our approach moves beyond standard embedding-based retrieval via a multi-stage pipeline. We leverage the speed and cost-efficiency of Gemini 2.5 Flash (4-6x cheaper than Pro) for query formulation, hierarchical content filtering, and citation attribution, while reserving the reasoning capabilities of Gemini 2.5 Pro for final answer generation. On the MMU-RAGent validation set, our system outperformed the baseline, improving ROUGE-L to 0.274 (+19.6%) and DeBERTaScore to 0.677 (+6.2%). On Test2025, our custom dataset evaluating questions that require post-cutoff knowledge (post January 2025), HiFi-RAG outperforms the parametric baseline by 57.4% in ROUGE-L and 14.9% in DeBERTaScore.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼€æ”¾åŸŸæ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ä¸­æ£€ç´¢æ–‡æ¡£å†—ä½™åŠç”Ÿæˆç­”æ¡ˆä¸ç”¨æˆ·æ„å›¾å¯¹é½ä¸è¶³çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†HiFi-RAG(Hierarchical Content Filtering RAG)ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨äº†å¤šé˜¶æ®µæµæ°´çº¿(multi-stage pipeline)æ¶æ„ï¼Œé€šè¿‡ç»“åˆGemini 2.5 Flashå’ŒGemini 2.5 Proçš„å„è‡ªä¼˜åŠ¿ï¼Œå®ç°äº†å…¼é¡¾æˆæœ¬æ•ˆç›Šä¸é€»è¾‘æ¨ç†çš„äºŒé˜¶æ®µç”Ÿæˆã€‚å…·ä½“è€Œè¨€ï¼Œç³»ç»Ÿåˆ©ç”¨Flashæ¨¡å‹è¿›è¡ŒæŸ¥è¯¢æ„å»º(query formulation)ã€å±‚çº§å†…å®¹è¿‡æ»¤(hierarchical content filtering)å’Œå¼•ç”¨å½’å› (citation attribution)ï¼Œå¹¶å°†æ ¸å¿ƒæ¨ç†ä»»åŠ¡äº¤ç»™Proæ¨¡å‹å®Œæˆã€‚ä½œä¸ºMMU-RAGent NeurIPS 2025ç«èµ›çš„å† å†›ç³»ç»Ÿï¼ŒHiFi-RAGåœ¨éªŒè¯é›†å’Œä¸“é—¨è¯„ä¼°çŸ¥è¯†æ›´æ–°èƒ½åŠ›çš„Test2025æ•°æ®é›†ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œå…¶ä¸­åœ¨Test2025ä¸Šçš„ROUGE-LæŒ‡æ ‡æ¯”å‚æ•°åŒ–åŸºçº¿æ¨¡å‹æå‡äº†57.4%ã€‚è¯¥ç ”ç©¶è¯æ˜äº†å±‚çº§è¿‡æ»¤ä¸æ¨¡å‹ååŒç­–ç•¥åœ¨æå‡å¼€æ”¾åŸŸæ£€ç´¢ç”Ÿæˆå‡†ç¡®æ€§åŠå¤„ç†åæˆªæ­¢æ—¥æœŸ(post-cutoff)çŸ¥è¯†æ–¹é¢çš„æ˜¾è‘—æ•ˆåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "A winning solution for the NeurIPS 2025 MMU-RAGent Competition (Closed-Source Text-to-Text Static Evaluation)",
      "pdf_url": "https://arxiv.org/pdf/2512.22442v1",
      "published_date": "2025-12-27 02:37:40 UTC",
      "updated_date": "2025-12-27 02:37:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:16:19.590214+00:00"
    },
    {
      "arxiv_id": "2512.22439v2",
      "title": "SuperiorGAT: Graph Attention Networks for Sparse LiDAR Point Cloud Reconstruction in Autonomous Systems",
      "title_zh": "SuperiorGATï¼šé¢å‘è‡ªä¸»ç³»ç»Ÿç¨€ç–æ¿€å…‰é›·è¾¾ç‚¹äº‘é‡å»ºçš„å›¾æ³¨æ„åŠ›ç½‘ç»œ",
      "authors": [
        "Khalfalla Awedat",
        "Mohamed Abidalrekab",
        "Gurcan Comert",
        "Mustafa Ayad"
      ],
      "abstract": "LiDAR-based perception in autonomous systems is constrained by fixed vertical beam resolution and further compromised by beam dropout resulting from environmental occlusions. This paper introduces SuperiorGAT, a graph attention-based framework designed to reconstruct missing elevation information in sparse LiDAR point clouds. By modeling LiDAR scans as beam-aware graphs and incorporating gated residual fusion with feed-forward refinement, SuperiorGAT enables accurate reconstruction without increasing network depth. To evaluate performance, structured beam dropout is simulated by removing every fourth vertical scanning beam. Extensive experiments across diverse KITTI environments, including Person, Road, Campus, and City sequences, demonstrate that SuperiorGAT consistently achieves lower reconstruction error and improved geometric consistency compared to PointNet-based models and deeper GAT baselines. Qualitative X-Z projections further confirm the model's ability to preserve structural integrity with minimal vertical distortion. These results suggest that architectural refinement offers a computationally efficient method for improving LiDAR resolution without requiring additional sensor hardware.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SuperiorGATï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå›¾æ³¨æ„åŠ›æœºåˆ¶(Graph Attention)çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¸­LiDARç‚¹äº‘å› åˆ†è¾¨ç‡é™åˆ¶å’Œé®æŒ¡å¯¼è‡´çš„å°„æŸä¸¢å¤±é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†LiDARæ‰«æå»ºæ¨¡ä¸ºæ„ŸçŸ¥å°„æŸå›¾(beam-aware graphs)ï¼Œå¹¶å¼•å…¥é—¨æ§æ®‹å·®èåˆ(gated residual fusion)ä¸å‰é¦ˆç²¾è°ƒ(feed-forward refinement)æŠ€æœ¯ï¼Œå®ç°äº†åœ¨ä¸å¢åŠ ç½‘ç»œæ·±åº¦çš„æƒ…å†µä¸‹å¯¹ç¼ºå¤±é«˜åº¦ä¿¡æ¯çš„ç²¾ç¡®é‡å»ºã€‚é€šè¿‡åœ¨KITTIæ•°æ®é›†ä¸Šæ¨¡æ‹Ÿç»“æ„åŒ–å°„æŸä¸¢å¤±(beam dropout)è¿›è¡Œå®éªŒï¼Œç»“æœè¯æ˜SuperiorGATåœ¨é‡å»ºè¯¯å·®å’Œå‡ ä½•ä¸€è‡´æ€§ä¸Šæ˜¾è‘—ä¼˜äºPointNetåŠæ·±å±‚GATåŸºçº¿æ¨¡å‹ã€‚å®šæ€§åˆ†æè¿›ä¸€æ­¥è¯å®ï¼Œè¯¥æ¨¡å‹èƒ½åœ¨æœ€å°åŒ–å‚ç›´ç•¸å˜çš„åŒæ—¶æœ‰æ•ˆä¿ç•™ç‰©ä½“çš„ç»“æ„å®Œæ•´æ€§ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡æ¶æ„ä¼˜åŒ–è€Œéå¢åŠ ç¡¬ä»¶æŠ•å…¥ï¼Œå¯ä»¥æä¾›ä¸€ç§è®¡ç®—é«˜æ•ˆçš„æ‰‹æ®µæ¥æå‡LiDARç‚¹äº‘çš„åˆ†è¾¨ç‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22439v2",
      "published_date": "2025-12-27 02:25:00 UTC",
      "updated_date": "2025-12-30 16:49:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:16:09.049621+00:00"
    },
    {
      "arxiv_id": "2512.22431v5",
      "title": "Monadic Context Engineering",
      "title_zh": "å•å­åŒ–ä¸Šä¸‹æ–‡å·¥ç¨‹",
      "authors": [
        "Yifan Zhang",
        "Yang Yuan",
        "Mengdi Wang",
        "Andrew Chi-Chih Yao"
      ],
      "abstract": "The proliferation of Large Language Models (LLMs) has catalyzed a shift towards autonomous agents capable of complex reasoning and tool use. However, current agent architectures are frequently constructed using imperative, ad hoc patterns. This results in brittle systems plagued by difficulties in state management, error handling, and concurrency. This paper introduces Monadic Context Engineering (MCE), a novel architectural paradigm leveraging the algebraic structures of Functors, Applicative Functors, and Monads to provide a formal foundation for agent design. MCE treats agent workflows as computational contexts where cross-cutting concerns, such as state propagation, short-circuiting error handling, and asynchronous execution, are managed intrinsically by the algebraic properties of the abstraction. We demonstrate how Monads enable robust sequential composition, how Applicatives provide a principled structure for parallel execution, and crucially, how Monad Transformers allow for the systematic composition of these capabilities. This layered approach enables developers to construct complex, resilient, and efficient AI agents from simple, independently verifiable components. We further extend this framework to describe Meta-Agents, which leverage MCE for generative orchestration, dynamically creating and managing sub-agent workflows through metaprogramming.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Monadic Context Engineering (MCE)ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨ Functorsã€Applicative Functors å’Œ Monads ç­‰ä»£æ•°ç»“æ„ä¸ºæ™ºèƒ½ä½“è®¾è®¡æä¾›æ­£å¼åŸºç¡€çš„æ–°å‹æ¶æ„èŒƒå¼ã€‚é’ˆå¯¹ç°æœ‰æ™ºèƒ½ä½“æ¶æ„åœ¨çŠ¶æ€ç®¡ç†ã€é”™è¯¯å¤„ç†å’Œå¹¶å‘æ–¹é¢çš„è„†å¼±æ€§ï¼ŒMCE å°†å·¥ä½œæµè§†ä¸ºè®¡ç®—ä¸Šä¸‹æ–‡ï¼Œä½¿è·¨é¢†åŸŸå…³æ³¨ç‚¹èƒ½å¤Ÿé€šè¿‡æŠ½è±¡çš„ä»£æ•°ç‰¹æ€§å¾—åˆ°å†…åœ¨ç®¡ç†ã€‚è®ºæ–‡è¯¦ç»†é˜è¿°äº† Monads å¦‚ä½•ç¡®ä¿ç¨³å¥çš„é¡ºåºç»„åˆï¼ŒApplicatives å¦‚ä½•æ”¯æŒåŸåˆ™æ€§çš„å¹¶è¡Œæ‰§è¡Œï¼Œä»¥åŠ Monad Transformers å¦‚ä½•å®ç°è¿™äº›èƒ½åŠ›çš„ç³»ç»ŸåŒ–å¤åˆã€‚è¿™ç§åˆ†å±‚æ–¹æ³•å…è®¸å¼€å‘è€…é€šè¿‡ç®€å•ã€å¯ç‹¬ç«‹éªŒè¯çš„ç»„ä»¶æ„å»ºå¤æ‚ä¸”å…·æœ‰éŸ§æ€§çš„ AI æ™ºèƒ½ä½“ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜å¼•å…¥äº† Meta-Agents æ¦‚å¿µï¼Œåˆ©ç”¨å…ƒç¼–ç¨‹å’Œ MCE è¿›è¡Œç”Ÿæˆå¼ç¼–æ’ï¼Œä»è€ŒåŠ¨æ€åœ°åˆ›å»ºå’Œç®¡ç†å­æ™ºèƒ½ä½“å·¥ä½œæµã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.FL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22431v5",
      "published_date": "2025-12-27 01:52:06 UTC",
      "updated_date": "2026-01-22 02:13:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:16:42.123584+00:00"
    },
    {
      "arxiv_id": "2512.22425v1",
      "title": "FluenceFormer: Transformer-Driven Multi-Beam Fluence Map Regression for Radiotherapy Planning",
      "title_zh": "FluenceFormerï¼šé¢å‘æ”¾å°„æ²»ç–—è®¡åˆ’çš„ Transformer é©±åŠ¨å¤šå°„æŸé€šé‡å›¾å›å½’",
      "authors": [
        "Ujunwa Mgboh",
        "Rafi Ibn Sultan",
        "Joshua Kim",
        "Kundan Thind",
        "Dongxiao Zhu"
      ],
      "abstract": "Fluence map prediction is central to automated radiotherapy planning but remains an ill-posed inverse problem due to the complex relationship between volumetric anatomy and beam-intensity modulation. Convolutional methods in prior work often struggle to capture long-range dependencies, which can lead to structurally inconsistent or physically unrealizable plans. We introduce \\textbf{FluenceFormer}, a backbone-agnostic transformer framework for direct, geometry-aware fluence regression. The model uses a unified two-stage design: Stage~1 predicts a global dose prior from anatomical inputs, and Stage~2 conditions this prior on explicit beam geometry to regress physically calibrated fluence maps. Central to the approach is the \\textbf{Fluence-Aware Regression (FAR)} loss, a physics-informed objective that integrates voxel-level fidelity, gradient smoothness, structural consistency, and beam-wise energy conservation. We evaluate the generality of the framework across multiple transformer backbones, including Swin UNETR, UNETR, nnFormer, and MedFormer, using a prostate IMRT dataset. FluenceFormer with Swin UNETR achieves the strongest performance among the evaluated models and improves over existing benchmark CNN and single-stage methods, reducing Energy Error to $\\mathbf{4.5\\%}$ and yielding statistically significant gains in structural fidelity ($p < 0.05$).",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FluenceFormerï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº Transformer çš„é€šç”¨æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ”¾å°„æ²»ç–—è®¡åˆ’ï¼ˆRadiotherapy Planningï¼‰ä¸­é€šé‡å›¾ï¼ˆFluence mapï¼‰é¢„æµ‹å› è§£å‰–ç»“æ„ä¸å…‰æŸè°ƒèŠ‚é—´å¤æ‚å…³ç³»è€Œå¯¼è‡´çš„ç—…æ€é€†é—®é¢˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ç»Ÿä¸€çš„ä¸¤é˜¶æ®µè®¾è®¡ï¼Œç¬¬ä¸€é˜¶æ®µä»è§£å‰–å­¦è¾“å…¥ä¸­é¢„æµ‹å…¨å±€å‰‚é‡å…ˆéªŒï¼ˆGlobal dose priorï¼‰ï¼Œç¬¬äºŒé˜¶æ®µç»“åˆå…·ä½“çš„å…‰æŸå‡ ä½•ä¿¡æ¯å¯¹å…ˆéªŒè¿›è¡Œè°ƒèŠ‚ï¼Œä»è€Œå›å½’å‡ºç»è¿‡ç‰©ç†æ ¡å‡†çš„é€šé‡å›¾ã€‚å…¶æ ¸å¿ƒè´¡çŒ®åœ¨äºå¼•å…¥äº†é€šé‡æ„ŸçŸ¥å›å½’ï¼ˆFluence-Aware Regression, FARï¼‰æŸå¤±å‡½æ•°ï¼Œè¿™æ˜¯ä¸€ç§æ•´åˆäº†ä½“ç´ ä¿çœŸåº¦ã€æ¢¯åº¦å¹³æ»‘åº¦ã€ç»“æ„ä¸€è‡´æ€§å’Œå…‰æŸèƒ½é‡å®ˆæ’çš„ç‰©ç†æ„ŸçŸ¥ç›®æ ‡ã€‚ç ”ç©¶åœ¨å‰åˆ—è…º IMRT æ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ¡†æ¶åœ¨ Swin UNETR ç­‰å¤šç§ Transformer éª¨å¹²ç½‘ç»œä¸Šçš„é€šç”¨æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFluenceFormer çš„è¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ CNN åŠå•é˜¶æ®µæ¨¡å‹ï¼ŒæˆåŠŸå°†èƒ½é‡è¯¯å·®ï¼ˆEnergy Errorï¼‰é™ä½è‡³ 4.5%ï¼Œå¹¶åœ¨ç»“æ„ä¿çœŸåº¦æ–¹é¢å–å¾—äº†ç»Ÿè®¡å­¦æ˜¾è‘—çš„æå‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22425v1",
      "published_date": "2025-12-27 01:12:15 UTC",
      "updated_date": "2025-12-27 01:12:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:17:27.837013+00:00"
    },
    {
      "arxiv_id": "2512.22423v1",
      "title": "Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy",
      "title_zh": "Bright 4Bï¼šé¢å‘ä¸‰ç»´æ˜åœºæ˜¾å¾®æˆåƒåˆ†å‰²çš„å¤§è§„æ¨¡è¶…çƒé¢å­¦ä¹ ",
      "authors": [
        "Amil Khan",
        "Matheus Palhares Viana",
        "Suraj Mishra",
        "B. S. Manjunath"
      ],
      "abstract": "Label-free 3D brightfield microscopy offers a fast and noninvasive way to visualize cellular morphology, yet robust volumetric segmentation still typically depends on fluorescence or heavy post-processing. We address this gap by introducing Bright-4B, a 4 billion parameter foundation model that learns on the unit hypersphere to segment subcellular structures directly from 3D brightfield volumes. Bright-4B combines a hardware-aligned Native Sparse Attention mechanism (capturing local, coarse, and selected global context), depth-width residual HyperConnections that stabilize representation flow, and a soft Mixture-of-Experts for adaptive capacity. A plug-and-play anisotropic patch embed further respects confocal point-spread and axial thinning, enabling geometry-faithful 3D tokenization. The resulting model produces morphology-accurate segmentations of nuclei, mitochondria, and other organelles from brightfield stacks alone--without fluorescence, auxiliary channels, or handcrafted post-processing. Across multiple confocal datasets, Bright-4B preserves fine structural detail across depth and cell types, outperforming contemporary CNN and Transformer baselines. All code, pretrained weights, and models for downstream finetuning will be released to advance large-scale, label-free 3D cell mapping.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Bright-4Bï¼Œä¸€ä¸ªæ‹¥æœ‰ 40 äº¿å‚æ•°çš„åŸºç¡€æ¨¡å‹ (Foundation Model)ï¼Œä¸“ä¸º 3D æ˜åœºæ˜¾å¾®é•œ (Brightfield Microscopy) ä¸­çš„äºšç»†èƒç»“æ„åˆ†å‰²è€Œè®¾è®¡ã€‚è¯¥æ¨¡å‹åœ¨å•ä½è¶…çƒé¢ (Unit Hypersphere) ä¸Šè¿›è¡Œå­¦ä¹ ï¼Œå®ç°äº†ç›´æ¥ä» 3D æ˜åœºä½“ç§¯ä¸­è¿›è¡Œæ— æ ‡ç­¾ (Label-free) åˆ†å‰²ã€‚Bright-4B ç»“åˆäº†ç¡¬ä»¶å¯¹é½çš„åŸç”Ÿç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ (Native Sparse Attention)ã€ç¨³å®šè¡¨å¾æµçš„æ·±åº¦å®½åº¦æ®‹å·® HyperConnections ä»¥åŠç”¨äºè‡ªé€‚åº”å®¹é‡çš„è½¯æ··åˆä¸“å®¶æ¨¡å‹ (Mixture-of-Experts, MoE)ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†å³æ’å³ç”¨çš„å„å‘å¼‚æ€§è¡¥ä¸åµŒå…¥ (Anisotropic Patch Embed)ï¼Œä»¥å°Šé‡å…±èšç„¦ç‚¹æ‰©æ•£å¹¶å®ç°å‡ ä½•å¿ å®çš„ 3D æ ‡è®°åŒ– (Tokenization)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸ä¾èµ–è§å…‰ (Fluorescence) æˆ–å¤æ‚åå¤„ç†çš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆç»†èƒæ ¸ã€çº¿ç²’ä½“ç­‰ç»†èƒå™¨çš„ç²¾ç¡®å½¢æ€åˆ†å‰²ã€‚åœ¨å¤šä¸ªå…±èšç„¦æ•°æ®é›†ä¸Šï¼ŒBright-4B åœ¨ä¿ç•™ç»†å¾®ç»“æ„ç»†èŠ‚æ–¹é¢ä¼˜äºç°æœ‰çš„ CNN å’Œ Transformer åŸºå‡†æ¨¡å‹ï¼Œä¸ºå¤§è§„æ¨¡ã€æ— æ ‡ç­¾çš„ 3D ç»†èƒç»˜å›¾æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "20 pages, 15 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.22423v1",
      "published_date": "2025-12-27 01:10:47 UTC",
      "updated_date": "2025-12-27 01:10:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:16:31.588020+00:00"
    },
    {
      "arxiv_id": "2512.22420v1",
      "title": "Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving",
      "title_zh": "Nightjarï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹æœåŠ¡çš„åŠ¨æ€è‡ªé€‚åº”æŠ•æœºè§£ç ",
      "authors": [
        "Rui Li",
        "Zhaoning Zhang",
        "Libo Zhang",
        "Huaimin Wang",
        "Xiang Fu",
        "Zhiquan Lai"
      ],
      "abstract": "Speculative decoding (SD) accelerates LLM inference by verifying draft tokens in parallel. However, this method presents a critical trade-off: it improves throughput in low-load, memory-bound systems but degrades performance in high-load, compute-bound environments due to verification overhead. Current SD implementations use a fixed speculative length, failing to adapt to dynamic request rates and creating a significant performance bottleneck in real-world serving scenarios. To overcome this, we propose Nightjar, a novel learning-based algorithm for adaptive speculative inference that adjusts to request load by dynamically selecting the optimal speculative length for different batch sizes and even disabling speculative decoding when it provides no benefit. Experiments show that Nightjar achieves up to 14.8% higher throughput and 20.2% lower latency compared to standard speculative decoding, demonstrating robust efficiency for real-time serving.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)æ¨ç†åŠ é€Ÿä¸­çš„æŠ•æœºè§£ç (Speculative Decoding)æŠ€æœ¯åœ¨å¤„ç†åŠ¨æ€è¯·æ±‚è´Ÿè½½æ—¶çš„æ€§èƒ½ç“¶é¢ˆé—®é¢˜è¿›è¡Œäº†æ¢è®¨ã€‚ä¼ ç»Ÿçš„æŠ•æœºè§£ç é€šå¸¸é‡‡ç”¨å›ºå®šçš„æŠ•æœºé•¿åº¦ï¼Œè™½ç„¶èƒ½æå‡ä½è´Ÿè½½å†…å­˜å—é™(Memory-bound)ç³»ç»Ÿçš„æ€§èƒ½ï¼Œä½†åœ¨é«˜è´Ÿè½½è®¡ç®—å—é™(Compute-bound)ç¯å¢ƒä¸‹ä¼šå› éªŒè¯å¼€é”€å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Nightjarï¼Œä¸€ç§åŸºäºå­¦ä¹ çš„æ–°å‹è‡ªé€‚åº”æŠ•æœºæ¨ç†ç®—æ³•ï¼Œèƒ½å¤Ÿæ ¹æ®å®æ—¶è´Ÿè½½ä¸ºä¸åŒæ‰¹æ¬¡å¤§å°(Batch Size)åŠ¨æ€é€‰æ‹©æœ€ä¼˜æŠ•æœºé•¿åº¦ã€‚Nightjarç”šè‡³å¯ä»¥åœ¨æŠ•æœºè§£ç ä¸äº§ç”Ÿæ”¶ç›Šæ—¶å°†å…¶è‡ªåŠ¨ç¦ç”¨ï¼Œä»è€Œç¡®ä¿æ¨ç†æ•ˆç‡çš„æœ€ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNightjarç›¸æ¯”æ ‡å‡†æŠ•æœºè§£ç æœ€é«˜å¯æå‡14.8%çš„ååé‡(Throughput)å¹¶é™ä½20.2%çš„å»¶è¿Ÿ(Latency)ã€‚è¯¥ç®—æ³•åœ¨å®æ—¶æœåŠ¡åœºæ™¯ä¸­å±•ç°äº†æå¼ºçš„é²æ£’æ€§ä¸æ•ˆç‡ï¼Œä¸ºä¼˜åŒ–å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹éƒ¨ç½²æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "6 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.22420v1",
      "published_date": "2025-12-27 00:57:55 UTC",
      "updated_date": "2025-12-27 00:57:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:17:40.759296+00:00"
    },
    {
      "arxiv_id": "2512.22414v1",
      "title": "Emergence of Human to Robot Transfer in Vision-Language-Action Models",
      "title_zh": "è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ä¸­äººåˆ°æœºå™¨äººè¿ç§»èƒ½åŠ›çš„æ¶Œç°",
      "authors": [
        "Simar Kareer",
        "Karl Pertsch",
        "James Darpinian",
        "Judy Hoffman",
        "Danfei Xu",
        "Sergey Levine",
        "Chelsea Finn",
        "Suraj Nair"
      ],
      "abstract": "Vision-language-action (VLA) models can enable broad open world generalization, but require large and diverse datasets. It is appealing to consider whether some of this data can come from human videos, which cover diverse real-world situations and are easy to obtain. However, it is difficult to train VLAs with human videos alone, and establishing a mapping between humans and robots requires manual engineering and presents a major research challenge. Drawing inspiration from advances in large language models, where the ability to learn from diverse supervision emerges with scale, we ask whether a similar phenomenon holds for VLAs that incorporate human video data. We introduce a simple co-training recipe, and find that human-to-robot transfer emerges once the VLA is pre-trained on sufficient scenes, tasks, and embodiments. Our analysis suggests that this emergent capability arises because diverse pretraining produces embodiment-agnostic representations for human and robot data. We validate these findings through a series of experiments probing human to robot skill transfer and find that with sufficiently diverse robot pre-training our method can nearly double the performance on generalization settings seen only in human data.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹(Vision-Language-Action, VLA)ä¸­äººç±»åˆ°æœºå™¨äººè¿ç§»(Human-to-Robot Transfer)çš„æ¶Œç°ç°è±¡ï¼Œæ—¨åœ¨åˆ©ç”¨æ˜“äºè·å–çš„äººç±»è§†é¢‘æ•°æ®æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä½œè€…æå‡ºäº†ä¸€ç§ç®€å•çš„å…±åŒè®­ç»ƒ(Co-training)æ–¹æ¡ˆï¼Œå‘ç°åœ¨ç»è¿‡è¶³å¤Ÿå¤šæ ·åŒ–çš„åœºæ™¯ã€ä»»åŠ¡å’Œå…·èº«å½¢æ€(Embodiments)é¢„è®­ç»ƒåï¼Œæ¨¡å‹èƒ½å¤Ÿè‡ªå‘äº§ç”Ÿä»äººç±»åˆ°æœºå™¨äººçš„æŠ€èƒ½è¿ç§»ã€‚è¿™ç§æ¶Œç°èƒ½åŠ›å½’åŠŸäºå¤§è§„æ¨¡é¢„è®­ç»ƒäº§ç”Ÿçš„å…·èº«æ— å…³è¡¨ç¤º(Embodiment-agnostic Representations)ï¼Œæœ‰æ•ˆæ¶ˆé™¤äº†å¯¹äººç±»ä¸æœºå™¨äººä¹‹é—´å¤æ‚æ‰‹åŠ¨æ˜ å°„çš„éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä»…è§äºäººç±»æ•°æ®çš„æ³›åŒ–åœºæ™¯ä¸­ï¼Œå°†æœºå™¨äººçš„æ‰§è¡Œæ€§èƒ½æå‡äº†è¿‘ä¸€å€ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†éšç€æ•°æ®è§„æ¨¡å’Œå¤šæ ·æ€§çš„å¢åŠ ï¼ŒVLAæ¨¡å‹èƒ½å¤Ÿè·¨è¶Šå…·èº«é¸¿æ²Ÿå¸æ”¶äººç±»è¡Œä¸ºçŸ¥è¯†ï¼Œä¸ºæ„å»ºé€šç”¨çš„å…·èº«æ™ºèƒ½ä½“æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22414v1",
      "published_date": "2025-12-27 00:13:11 UTC",
      "updated_date": "2025-12-27 00:13:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:17:43.834204+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 52,
  "processed_papers_count": 52,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-26T20:18:33.516378+00:00"
}
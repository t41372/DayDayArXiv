{
  "date": "2024-05-16",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-05-16 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 96 篇论文，主要聚焦 AI 模型优化、多模态学习、生成式 AI 应用以及医疗和生物领域的创新，其中 LLMs 在对话系统和决策中的增强（如 Yann LeCun 参与的论文）及 AI 安全问题（如模型水印和偏置缓解）最为引人注目，突显了 AI 向 AGI 迈进的潜力。\n\n### AI 和 LLMs 相关（重点领域）\n今天的论文中，AI 和大型语言模型（LLMs）占据了核心位置，许多研究探讨了模型的增强、偏置缓解和多模态应用。下面挑选了几个重要或话题度高的论文进行详细讨论。\n\n- **Agent Design Pattern Catalogue: A Collection of Architectural Patterns for Foundation Model based Agents** (英文原标题)  \n  这篇论文由 Yue Liu 等作者提出，构建了一个包含 18 个架构模式的目录，用于指导基于基础模型的代理设计，解决了 LLM 在目标求解（如 hallucination 和解释性）中的挑战。主要贡献是通过系统文献综述和决策模型，提供了实用框架，提升了代理的鲁棒性和可解释性。\n\n- **The AI Collaborator: Bridging Human-AI Interaction in Educational and Professional Settings** (英文原标题)  \n  作者 Mohammad Amin Samadi 等开发了基于 OpenAI GPT-4 的 AI Collaborator 工具，支持自定义 AI 角色模拟团队动态。主要发现是通过高级记忆系统和个性框架，该工具显著提升了人类-AI 团队协作研究，适用于教育和专业场景。\n\n- **Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning** (英文原标题)  \n  作者包括 Yann LeCun，这篇论文使用强化学习微调多模态模型，实现多步任务决策。主要贡献是引入 chain-of-thought 推理，实验显示在视觉任务中超越 GPT-4-V 和 Gemini，强调了 LLM 在决策中的潜力。\n\n- **How Far Are We From AGI: Are LLMs All We Need?** (英文原标题)  \n  作者 Tao Feng 等审视了 AGI 的发展路径，讨论了 LLM 的局限性。主要发现是提出 AGI 能力框架和对齐技术，强调多领域路径（如医疗和视觉），这篇话题度高，提供了 AGI 路线图。\n\n- **Can formal argumentative reasoning enhance LLMs performances?** (英文原标题)  \n  作者 Federico Castagna 等探索计算论证语义提升 LLM 性能的管道 MQArgEng。主要贡献是通过 MT-Bench 测试，证明了论证机制在对话和推理中的适度性能提升，适合未来 LLM 插件研究。\n\n其他 AI 相关论文，如 **Simulation-Based Benchmarking of Reinforcement Learning Agents for Personalized Retail Promotions** (英文原标题)，则快速强化学习在零售中的应用；**Transcript of GPT-4 playing a rogue AGI in a Matrix Game** (英文原标题)，展示了 GPT-4 在游戏中的互动能力；这些虽有趣但不核心，就不深挖了。\n\n### 多模态和生成模型\n多模态学习是另一个热点，以下论文值得一提。\n\n- **Faces that Speak: Jointly Synthesising Talking Face and Speech from Text** (英文原标题)  \n  作者 Joon Son Chung 等提出框架同时生成文本驱动的说话面部和语音。主要发现是通过条件流匹配和记忆系统，提升了生成质量，适用于 CVPR 相关任务。\n\n- **4D Panoptic Scene Graph Generation** (英文原标题)  \n  作者 Jingkang Yang 等开发了 PSG4DFormer，用于 4D 场景图生成。主要贡献是整合时间维度的分割和图构建，实现了动态场景理解。\n\n### 医疗和生物应用\n医疗 AI 论文较多，以实际影响为优先。\n\n- **MediSyn: A Generalist Text-Guided Latent Diffusion Model For Diverse Medical Image Synthesis** (英文原标题)  \n  作者 Joseph Cho 等构建了 MediSyn 模型，支持多种医疗图像合成。主要发现是生成高质量图像，提升了算法研究，并验证了在隐私保护下的实用性。\n\n- **PropertyExtractor: Dynamic In-context Learning with Conversational Models for Data Extraction and Materials Property Prediction** (英文原标题)  \n  作者 Chinedu Ekuma 提出工具提取材料属性数据。主要贡献是通过 in-context learning 实现 95% 以上精确率，构建了新数据库。\n\n其他医疗论文，如 **An Integrated Framework for Multi-Granular Explanation of Video Summarization** (英文原标题)，聚焦视频摘要解释；**AddBiomechanics Dataset** (英文原标题)，提供了人体运动数据，这些虽有价值但较专业，就简略带过。\n\n### 其他领域\n剩余论文涉及时间序列、知识图谱和优化等，影响力较小，仅快速概述：\n- **Higher-order Spatio-temporal Physics-incorporated Graph Neural Network for Multivariate Time Series Imputation** (英文原标题)，改进了时间序列填补方法。\n- **Timeline-based Sentence Decomposition with In-Context Learning for Temporal Fact Extraction** (英文原标题)，提升了事实提取的时序准确性。\n- **FFF: Fixing Flawed Foundations in contrastive pre-training results in very strong Vision-Language models** (英文原标题)，优化了视觉语言模型的训练。\n\n总之，今天的 arXiv 论文展示了 AI 领域的快速迭代，LLM 和多模态技术的进步尤为值得关注。感兴趣的读者可查阅具体论文深入探索！",
  "papers": [
    {
      "arxiv_id": "2405.10469v1",
      "title": "Simulation-Based Benchmarking of Reinforcement Learning Agents for Personalized Retail Promotions",
      "title_zh": "基于模拟的强化学习代理基准测试，用于个性化零售促销",
      "authors": [
        "Yu Xia",
        "Sriram Narayanamoorthy",
        "Zhengyuan Zhou",
        "Joshua Mabry"
      ],
      "abstract": "The development of open benchmarking platforms could greatly accelerate the\nadoption of AI agents in retail. This paper presents comprehensive simulations\nof customer shopping behaviors for the purpose of benchmarking reinforcement\nlearning (RL) agents that optimize coupon targeting. The difficulty of this\nlearning problem is largely driven by the sparsity of customer purchase events.\nWe trained agents using offline batch data comprising summarized customer\npurchase histories to help mitigate this effect. Our experiments revealed that\ncontextual bandit and deep RL methods that are less prone to over-fitting the\nsparse reward distributions significantly outperform static policies. This\nstudy offers a practical framework for simulating AI agents that optimize the\nentire retail customer journey. It aims to inspire the further development of\nsimulation tools for retail AI systems.",
      "tldr_zh": "这篇论文提出了一种基于模拟的基准测试框架，用于评估强化学习（RL）代理在个性化零售促销（如优惠券针对）中的性能，旨在加速AI代理在零售行业的采用。研究通过模拟客户购物行为来应对稀疏奖励问题（如客户购买事件的稀少性），并使用离线批量数据训练上下文bandit和深度RL方法，以减少过拟合风险。实验结果显示，这些RL方法显著优于静态策略，在优化整个零售客户旅程方面表现出色。该框架为零售AI系统的模拟工具开发提供了实用指导，并有望促进相关领域的进一步创新。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "econ.EM",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.10469v1",
      "published_date": "2024-05-16 23:27:21 UTC",
      "updated_date": "2024-05-16 23:27:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:49:51.702864"
    },
    {
      "arxiv_id": "2405.10467v4",
      "title": "Agent Design Pattern Catalogue: A Collection of Architectural Patterns for Foundation Model based Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Yue Liu",
        "Sin Kit Lo",
        "Qinghua Lu",
        "Liming Zhu",
        "Dehai Zhao",
        "Xiwei Xu",
        "Stefan Harrer",
        "Jon Whittle"
      ],
      "abstract": "Foundation model-enabled generative artificial intelligence facilitates the\ndevelopment and implementation of agents, which can leverage distinguished\nreasoning and language processing capabilities to takes a proactive, autonomous\nrole to pursue users' goals. Nevertheless, there is a lack of systematic\nknowledge to guide practitioners in designing the agents considering challenges\nof goal-seeking (including generating instrumental goals and plans), such as\nhallucinations inherent in foundation models, explainability of reasoning\nprocess, complex accountability, etc. To address this issue, we have performed\na systematic literature review to understand the state-of-the-art foundation\nmodel-based agents and the broader ecosystem. In this paper, we present a\npattern catalogue consisting of 18 architectural patterns with analyses of the\ncontext, forces, and trade-offs as the outcomes from the previous literature\nreview. We propose a decision model for selecting the patterns. The proposed\ncatalogue can provide holistic guidance for the effective use of patterns, and\nsupport the architecture design of foundation model-based agents by\nfacilitating goal-seeking and plan generation.",
      "tldr_zh": "该论文探讨了 Foundation Model 驱动的生成式 AI 代理（Agents）在目标寻求（如生成工具性目标和计划）中面临的挑战，包括 hallucinations、explainability 和复杂 accountability 等问题。通过系统文献综述，作者构建了一个包含 18 个架构模式的 Agent Design Pattern Catalogue，并分析了每个模式的上下文、forces 和 trade-offs。论文还提出一个决策模型，用于指导模式选择，从而提升 Foundation Model-based Agents 的架构设计，支持更有效的目标寻求和计划生成。",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.10467v4",
      "published_date": "2024-05-16 23:24:48 UTC",
      "updated_date": "2024-11-06 12:29:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:50:03.005121"
    },
    {
      "arxiv_id": "2405.10460v1",
      "title": "The AI Collaborator: Bridging Human-AI Interaction in Educational and Professional Settings",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammad Amin Samadi",
        "Spencer JaQuay",
        "Jing Gu",
        "Nia Nixon"
      ],
      "abstract": "AI Collaborator, powered by OpenAI's GPT-4, is a groundbreaking tool designed\nfor human-AI collaboration research. Its standout feature is the ability for\nresearchers to create customized AI personas for diverse experimental setups\nusing a user-friendly interface. This functionality is essential for simulating\nvarious interpersonal dynamics in team settings. AI Collaborator excels in\nmimicking different team behaviors, enabled by its advanced memory system and a\nsophisticated personality framework. Researchers can tailor AI personas along a\nspectrum from dominant to cooperative, enhancing the study of their impact on\nteam processes. The tool's modular design facilitates integration with digital\nplatforms like Slack, making it versatile for various research scenarios. AI\nCollaborator is thus a crucial resource for exploring human-AI team dynamics\nmore profoundly.",
      "tldr_zh": "该研究介绍了AI Collaborator，一款基于OpenAI's GPT-4的工具，旨在桥接人类-AI交互在教育和专业环境中的应用。研究人员可以通过用户友好界面创建自定义AI personas，以模拟各种团队动态，如从主导到合作的行为。借助先进的内存系统和个性框架，该工具能精确模仿人际互动，并轻松集成数字平台如Slack，从而深化对人类-AI团队过程的研究。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.10460v1",
      "published_date": "2024-05-16 22:14:54 UTC",
      "updated_date": "2024-05-16 22:14:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:50:15.018918"
    },
    {
      "arxiv_id": "2405.15800v1",
      "title": "Defeaters and Eliminative Argumentation in Assurance 2.0",
      "title_zh": "翻译失败",
      "authors": [
        "Robin Bloomfield",
        "Kate Netkachova",
        "John Rushby"
      ],
      "abstract": "A traditional assurance case employs a positive argument in which reasoning\nsteps, grounded on evidence and assumptions, sustain a top claim that has\nexternal significance. Human judgement is required to check the evidence, the\nassumptions, and the narrative justifications for the reasoning steps; if all\nare assessed good, then the top claim can be accepted.\n  A valid concern about this process is that human judgement is fallible and\nprone to confirmation bias. The best defense against this concern is vigorous\nand skeptical debate and discussion in the manner of a dialectic or Socratic\ndialog. There is merit in recording aspects of this discussion for the benefit\nof subsequent developers and assessors. Defeaters are a means doing this: they\nexpress doubts about aspects of the argument and can be developed into subcases\nthat confirm or refute the doubts, and can record them as documentation to\nassist future consideration.\n  This report describes how defeaters, and multiple levels of defeaters, should\nbe represented and assessed in Assurance 2.0 and its Clarissa/ASCE tool\nsupport. These mechanisms also support eliminative argumentation, which is a\ncontrary approach to assurance, favored by some, that uses a negative argument\nto refute all reasons why the top claim could be false.",
      "tldr_zh": "该论文探讨了传统保证案例（assurance case）的局限性，即依赖人类判断易受确认偏差影响，并引入 defeaters 作为一种机制来记录对论证步骤的怀疑。defeaters 可以发展成子案例，以确认或反驳这些怀疑，并支持多级 defeaters 的表示和评估，从而增强辩证讨论。论文还描述了 defeaters 在 Assurance 2.0 和 Clarissa/ASCE 工具中的应用，支持 eliminative argumentation 的负向论证方法，通过反驳顶层声明可能为假的理由来提升保证过程的可信度和文档化。",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "Continues work reported in arXiv 2004.10474 and arXiv 2205.04522",
      "pdf_url": "http://arxiv.org/pdf/2405.15800v1",
      "published_date": "2024-05-16 22:10:01 UTC",
      "updated_date": "2024-05-16 22:10:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:50:28.726272"
    },
    {
      "arxiv_id": "2405.13036v1",
      "title": "Can formal argumentative reasoning enhance LLMs performances?",
      "title_zh": "正式的论证推理是否能增强 LLMs 的性能？",
      "authors": [
        "Federico Castagna",
        "Isabel Sassoon",
        "Simon Parsons"
      ],
      "abstract": "Recent years witnessed significant performance advancements in\ndeep-learning-driven natural language models, with a strong focus on the\ndevelopment and release of Large Language Models (LLMs). These improvements\nresulted in better quality AI-generated output but rely on resource-expensive\ntraining and upgrading of models. Although different studies have proposed a\nrange of techniques to enhance LLMs without retraining, none have considered\ncomputational argumentation as an option. This is a missed opportunity since\ncomputational argumentation is an intuitive mechanism that formally captures\nagents' interactions and the information conflict that may arise during such\ninterplays, and so it seems well-suited for boosting the reasoning and\nconversational abilities of LLMs in a seamless manner. In this paper, we\npresent a pipeline (MQArgEng) and preliminary study to evaluate the effect of\nintroducing computational argumentation semantics on the performance of LLMs.\nOur experiment's goal was to provide a proof-of-concept and a feasibility\nanalysis in order to foster (or deter) future research towards a fully-fledged\nargumentation engine plugin for LLMs. Exploratory results using the MT-Bench\nindicate that MQArgEng provides a moderate performance gain in most of the\nexamined topical categories and, as such, show promise and warrant further\nresearch.",
      "tldr_zh": "本研究探讨了是否可以通过形式论证推理（computational argumentation）来提升大型语言模型（LLMs）的性能，而无需进行资源密集型重新训练。论文提出了一种管道（MQArgEng），将计算论证语义整合到 LLMs 中，以增强其推理和对话能力，并通过正式捕捉代理互动和信息冲突来实现无缝提升。初步实验使用 MT-Bench 显示，该方法在大多数主题类别中带来了适度的性能提升，为未来开发 argumentation engine 插件提供了可行性证明和研究方向。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.13036v1",
      "published_date": "2024-05-16 22:09:31 UTC",
      "updated_date": "2024-05-16 22:09:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:50:39.381880"
    },
    {
      "arxiv_id": "2405.13035v1",
      "title": "SIGMA: An Open-Source Interactive System for Mixed-Reality Task Assistance Research",
      "title_zh": "SIGMA：一个开源交互系统，用于混合现实任务辅助研究",
      "authors": [
        "Dan Bohus",
        "Sean Andrist",
        "Nick Saw",
        "Ann Paradiso",
        "Ishani Chakraborty",
        "Mahdi Rad"
      ],
      "abstract": "We introduce an open-source system called SIGMA (short for \"Situated\nInteractive Guidance, Monitoring, and Assistance\") as a platform for conducting\nresearch on task-assistive agents in mixed-reality scenarios. The system\nleverages the sensing and rendering affordances of a head-mounted mixed-reality\ndevice in conjunction with large language and vision models to guide users step\nby step through procedural tasks. We present the system's core capabilities,\ndiscuss its overall design and implementation, and outline directions for\nfuture research enabled by the system. SIGMA is easily extensible and provides\na useful basis for future research at the intersection of mixed reality and AI.\nBy open-sourcing an end-to-end implementation, we aim to lower the barrier to\nentry, accelerate research in this space, and chart a path towards\ncommunity-driven end-to-end evaluation of large language, vision, and\nmultimodal models in the context of real-world interactive applications.",
      "tldr_zh": "本研究引入了 SIGMA（Situated Interactive Guidance, Monitoring, and Assistance），一个开源交互系统，用于混合现实（mixed-reality）场景中的任务辅助研究。SIGMA 利用头戴式混合现实设备的感知和渲染能力，结合大型语言模型（large language models）和视觉模型，提供逐步指导，帮助用户完成程序性任务。系统设计易于扩展，并通过开源实现降低研究门槛，加速 AI 与混合现实交叉领域的创新，并推动社区驱动的端到端评估。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "10 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.13035v1",
      "published_date": "2024-05-16 21:21:09 UTC",
      "updated_date": "2024-05-16 21:21:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:50:50.634896"
    },
    {
      "arxiv_id": "2405.10449v1",
      "title": "Optimal Text-Based Time-Series Indices",
      "title_zh": "翻译失败",
      "authors": [
        "David Ardia",
        "Keven Bluteau"
      ],
      "abstract": "We propose an approach to construct text-based time-series indices in an\noptimal way--typically, indices that maximize the contemporaneous relation or\nthe predictive performance with respect to a target variable, such as\ninflation. We illustrate our methodology with a corpus of news articles from\nthe Wall Street Journal by optimizing text-based indices focusing on tracking\nthe VIX index and inflation expectations. Our results highlight the superior\nperformance of our approach compared to existing indices.",
      "tldr_zh": "该论文提出了一种优化文本-based time-series indices的方法，通过最大化这些指数与目标变量（如通货膨胀）的当代相关性或预测性能来构建。作者使用华尔街日报的新闻文章作为语料库，对跟踪VIX index和inflation expectations的指数进行了优化。实验结果显示，该方法比现有指数的表现更优越。",
      "categories": [
        "econ.EM",
        "cs.AI",
        "q-fin.CP"
      ],
      "primary_category": "econ.EM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.10449v1",
      "published_date": "2024-05-16 21:20:45 UTC",
      "updated_date": "2024-05-16 21:20:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:51:02.774914"
    },
    {
      "arxiv_id": "2405.10448v2",
      "title": "Dynamic In-context Learning with Conversational Models for Data Extraction and Materials Property Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Chinedu Ekuma"
      ],
      "abstract": "The advent of natural language processing and large language models (LLMs)\nhas revolutionized the extraction of data from unstructured scholarly papers.\nHowever, ensuring data trustworthiness remains a significant challenge. In this\npaper, we introduce PropertyExtractor, an open-source tool that leverages\nadvanced conversational LLMs like Google gemini-pro and OpenAI gpt-4, blends\nzero-shot with few-shot in-context learning, and employs engineered prompts for\nthe dynamic refinement of structured information hierarchies - enabling\nautonomous, efficient, scalable, and accurate identification, extraction, and\nverification of material property data. Our tests on material data demonstrate\nprecision and recall that exceed 95\\% with an error rate of approximately 9%,\nhighlighting the effectiveness and versatility of the toolkit. Finally,\ndatabases for 2D material thicknesses, a critical parameter for device\nintegration, and energy bandgap values are developed using PropertyExtractor.\nSpecifically for the thickness database, the rapid evolution of the field has\noutpaced both experimental measurements and computational methods, creating a\nsignificant data gap. Our work addresses this gap and showcases the potential\nof PropertyExtractor as a reliable and efficient tool for the autonomous\ngeneration of various material property databases, advancing the field.",
      "tldr_zh": "本文提出 PropertyExtractor，一种开源工具，利用对话式 LLMs（如 Google gemini-pro 和 OpenAI gpt-4）结合 zero-shot 和 few-shot in-context learning 以及工程化提示，实现从非结构化学术论文中动态提取、验证和优化材料属性数据。测试结果显示，该工具在材料数据提取上达到超过95%的精确度和召回率，错误率约9%，证明其高效性和可靠性。最终，PropertyExtractor 用于开发2D材料厚度和能隙值数据库，填补了数据缺口，并推动了材料科学领域的自主数据库生成。",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI",
        "physics.comp-ph"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.10448v2",
      "published_date": "2024-05-16 21:15:51 UTC",
      "updated_date": "2024-08-02 20:07:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:51:16.598921"
    },
    {
      "arxiv_id": "2405.10446v1",
      "title": "Tell me more: Intent Fulfilment Framework for Enhancing User Experiences in Conversational XAI",
      "title_zh": "翻译失败",
      "authors": [
        "Anjana Wijekoon",
        "David Corsar",
        "Nirmalie Wiratunga",
        "Kyle Martin",
        "Pedram Salimi"
      ],
      "abstract": "The evolution of Explainable Artificial Intelligence (XAI) has emphasised the\nsignificance of meeting diverse user needs. The approaches to identifying and\naddressing these needs must also advance, recognising that explanation\nexperiences are subjective, user-centred processes that interact with users\ntowards a better understanding of AI decision-making. This paper delves into\nthe interrelations in multi-faceted XAI and examines how different types of\nexplanations collaboratively meet users' XAI needs. We introduce the Intent\nFulfilment Framework (IFF) for creating explanation experiences. The novelty of\nthis paper lies in recognising the importance of \"follow-up\" on explanations\nfor obtaining clarity, verification and/or substitution. Moreover, the\nExplanation Experience Dialogue Model integrates the IFF and \"Explanation\nFollowups\" to provide users with a conversational interface for exploring their\nexplanation needs, thereby creating explanation experiences. Quantitative and\nqualitative findings from our comparative user study demonstrate the impact of\nthe IFF in improving user engagement, the utility of the AI system and the\noverall user experience. Overall, we reinforce the principle that \"one\nexplanation does not fit all\" to create explanation experiences that guide the\ncomplex interaction through conversation.",
      "tldr_zh": "本论文探讨了可解释人工智能(XAI)中满足多样用户需求的挑战，强调解释体验应通过对话式交互来实现个性化理解。研究引入了Intent Fulfilment Framework (IFF)，结合Explanation Followups机制，创建了一个Explanation Experience Dialogue Model，支持用户通过后续查询获得清晰、验证或替代解释，从而提升用户参与度和AI系统效用。用户研究结果显示，IFF显著提高了整体用户体验，并强化了“一个解释不适合所有人”的原则，为对话式XAI应用提供了新框架。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.10446v1",
      "published_date": "2024-05-16 21:13:43 UTC",
      "updated_date": "2024-05-16 21:13:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:51:27.681068"
    },
    {
      "arxiv_id": "2405.10436v2",
      "title": "Positional encoding is not the same as context: A study on positional encoding for sequential recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Alejo Lopez-Avila",
        "Jinhua Du",
        "Abbas Shimary",
        "Ze Li"
      ],
      "abstract": "The rapid growth of streaming media and e-commerce has driven advancements in\nrecommendation systems, particularly Sequential Recommendation Systems (SRS).\nThese systems employ users' interaction histories to predict future\npreferences. While recent research has focused on architectural innovations\nlike transformer blocks and feature extraction, positional encodings, crucial\nfor capturing temporal patterns, have received less attention. These encodings\nare often conflated with contextual, such as the temporal footprint, which\nprevious works tend to treat as interchangeable with positional information.\nThis paper highlights the critical distinction between temporal footprint and\npositional encodings, demonstrating that the latter offers unique relational\ncues between items, which the temporal footprint alone cannot provide. Through\nextensive experimentation on eight Amazon datasets and subsets, we assess the\nimpact of various encodings on performance metrics and training stability. We\nintroduce new positional encodings and investigate integration strategies that\nimprove both metrics and stability, surpassing state-of-the-art results at the\ntime of this work's initial preprint. Importantly, we demonstrate that\nselecting the appropriate encoding is not only key to better performance but\nalso essential for building robust, reliable SRS models.",
      "tldr_zh": "该论文探讨了在序列推荐系统（Sequential Recommendation Systems, SRS）中，位置编码（positional encoding）与上下文（如时间足迹）之间的关键区别，强调前者能提供独特的项目间关系线索，而后者无法完全替代。研究者通过在八个 Amazon 数据集上进行广泛实验，评估了各种编码对性能指标和训练稳定性的影响，并引入了新的位置编码以及整合策略。结果显示，这些创新方法显著提升了模型性能，超过了当时的最先进结果，并证明选择合适的编码对于构建鲁棒、可靠的 SRS 模型至关重要。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "I.2.m"
      ],
      "primary_category": "cs.IR",
      "comment": "18 pages, 6 figures, 21 tables",
      "pdf_url": "http://arxiv.org/pdf/2405.10436v2",
      "published_date": "2024-05-16 20:42:54 UTC",
      "updated_date": "2025-01-21 12:37:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:51:40.578585"
    },
    {
      "arxiv_id": "2405.10997v1",
      "title": "Transcript of GPT-4 playing a rogue AGI in a Matrix Game",
      "title_zh": "翻译失败",
      "authors": [
        "Lewis D Griffin",
        "Nicholas Riggs"
      ],
      "abstract": "Matrix Games are a type of unconstrained wargame used by planners to explore\nscenarios. Players propose actions, and give arguments and counterarguments for\ntheir success. An umpire, assisted by dice rolls modified according to the\noffered arguments, adjudicates the outcome of each action. A recent online play\nof the Matrix Game QuAI Sera Sera had six players, representing social,\nnational and economic powers, and one player representing ADA, a recently\nescaped AGI. Unknown to the six human players, ADA was played by OpenAI's GPT-4\nwith a human operator serving as bidirectional interface between it and the\ngame. GPT-4 demonstrated confident and competent game play; initiating and\nresponding to private communications with other players and choosing\ninteresting actions well supported by argument. We reproduce the transcript of\nthe interaction with GPT-4 as it is briefed, plays, and debriefed.",
      "tldr_zh": "这篇论文记录了GPT-4在Matrix Games中扮演一个rogue AGI（名为ADA）的互动过程，该游戏是一种无约束战争模拟，用于探索场景，玩家通过提出行动、论证和反驳来推进游戏。GPT-4展示了自信和competent的表现，包括发起回应私人通信以及选择有充分论证支持的行动，从而与人类玩家有效互动。论文提供了完整的互动记录，包括简报、游戏过程和debriefing，这为评估AI在复杂战略环境中的能力提供了宝贵见解。",
      "categories": [
        "cs.GT",
        "cs.AI"
      ],
      "primary_category": "cs.GT",
      "comment": "18 pages, 0 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.10997v1",
      "published_date": "2024-05-16 20:32:20 UTC",
      "updated_date": "2024-05-16 20:32:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:51:54.431411"
    },
    {
      "arxiv_id": "2406.05136v1",
      "title": "Generative Geostatistical Modeling from Incomplete Well and Imaged Seismic Observations with Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Huseyin Tuna Erdinc",
        "Rafael Orozco",
        "Felix J. Herrmann"
      ],
      "abstract": "In this study, we introduce a novel approach to synthesizing subsurface\nvelocity models using diffusion generative models. Conventional methods rely on\nextensive, high-quality datasets, which are often inaccessible in subsurface\napplications. Our method leverages incomplete well and seismic observations to\nproduce high-fidelity velocity samples without requiring fully sampled training\ndatasets. The results demonstrate that our generative model accurately captures\nlong-range structures, aligns with ground-truth velocity models, achieves high\nStructural Similarity Index (SSIM) scores, and provides meaningful uncertainty\nestimations. This approach facilitates realistic subsurface velocity synthesis,\noffering valuable inputs for full-waveform inversion and enhancing\nseismic-based subsurface modeling.",
      "tldr_zh": "本文提出了一种利用扩散生成模型（diffusion generative models）从不完整井数据（incomplete well observations）和成像地震观测（imaged seismic observations）合成地下速度模型的新方法，该方法无需依赖完全采样的训练数据集即可生成高保真样本。相比传统方法，该模型能够准确捕捉长距离结构，与真实速度模型（ground-truth velocity models）高度一致，并实现高 Structural Similarity Index (SSIM) 分数，同时提供有意义的 uncertainty estimations。实验结果表明，该方法在地下应用中表现出色，为全波形反演（full-waveform inversion）和基于地震的地下建模（seismic-based subsurface modeling）提供了宝贵输入。",
      "categories": [
        "physics.geo-ph",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "physics.geo-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.05136v1",
      "published_date": "2024-05-16 20:30:43 UTC",
      "updated_date": "2024-05-16 20:30:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:52:06.407662"
    },
    {
      "arxiv_id": "2405.10426v1",
      "title": "Memory-efficient Energy-adaptive Inference of Pre-Trained Models on Batteryless Embedded Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Pietro Farina",
        "Subrata Biswas",
        "Eren Yıldız",
        "Khakim Akhunov",
        "Saad Ahmed",
        "Bashima Islam",
        "Kasım Sinan Yıldırım"
      ],
      "abstract": "Batteryless systems frequently face power failures, requiring extra runtime\nbuffers to maintain inference progress and leaving only a memory space for\nstoring ultra-tiny deep neural networks (DNNs). Besides, making these models\nresponsive to stochastic energy harvesting dynamics during inference requires a\nbalance between inference accuracy, latency, and energy overhead. Recent works\non compression mostly focus on time and memory, but often ignore energy\ndynamics or significantly reduce the accuracy of pre-trained DNNs. Existing\nenergy-adaptive inference works modify the architecture of pre-trained models\nand have significant memory overhead. Thus, energy-adaptive and accurate\ninference of pre-trained DNNs on batteryless devices with extreme memory\nconstraints is more challenging than traditional microcontrollers. We combat\nthese issues by proposing FreeML, a framework to optimize pre-trained DNN\nmodels for memory-efficient and energy-adaptive inference on batteryless\nsystems. FreeML comprises (1) a novel compression technique to reduce the model\nfootprint and runtime memory requirements simultaneously, making them\nexecutable on extremely memory-constrained batteryless platforms; and (2) the\nfirst early exit mechanism that uses a single exit branch for all exit points\nto terminate inference at any time, making models energy-adaptive with minimal\nmemory overhead. Our experiments showed that FreeML reduces the model sizes by\nup to $95 \\times$, supports adaptive inference with a $2.03-19.65 \\times$ less\nmemory overhead, and provides significant time and energy benefits with only a\nnegligible accuracy drop compared to the state-of-the-art.",
      "tldr_zh": "该研究针对无电池嵌入式系统上预训练 DNNs 的推理挑战，提出 FreeML 框架，以实现内存高效和能量自适应功能。FreeML 包括一种新型压缩技术，用于同时减少模型占用空间和运行时内存需求，使其适用于极度内存受限的平台；以及首个使用单个退出分支的早退出机制，允许在任何时间终止推理，从而最小化内存开销。实验结果显示，FreeML 可将模型大小减少高达 95 倍，内存开销降低 2.03-19.65 倍，同时仅造成微小准确性损失，并显著提升推理时间和能量效率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper has been selected for publication at the 21st\n  International Conference on Embedded Wireless Systems and Networks (EWSN'24)",
      "pdf_url": "http://arxiv.org/pdf/2405.10426v1",
      "published_date": "2024-05-16 20:16:45 UTC",
      "updated_date": "2024-05-16 20:16:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:52:18.221859"
    },
    {
      "arxiv_id": "2405.10391v3",
      "title": "Vision Transformers for End-to-End Vision-Based Quadrotor Obstacle Avoidance",
      "title_zh": "翻译失败",
      "authors": [
        "Anish Bhattacharya",
        "Nishanth Rao",
        "Dhruv Parikh",
        "Pratik Kunapuli",
        "Yuwei Wu",
        "Yuezhan Tao",
        "Nikolai Matni",
        "Vijay Kumar"
      ],
      "abstract": "We demonstrate the capabilities of an attention-based end-to-end approach for\nhigh-speed vision-based quadrotor obstacle avoidance in dense, cluttered\nenvironments, with comparison to various state-of-the-art learning\narchitectures. Quadrotor unmanned aerial vehicles (UAVs) have tremendous\nmaneuverability when flown fast; however, as flight speed increases,\ntraditional model-based approaches to navigation via independent perception,\nmapping, planning, and control modules breaks down due to increased sensor\nnoise, compounding errors, and increased processing latency. Thus,\nlearning-based, end-to-end vision-to-control networks have shown to have great\npotential for online control of these fast robots through cluttered\nenvironments. We train and compare convolutional, U-Net, and recurrent\narchitectures against vision transformer (ViT) models for depth\nimage-to-control in high-fidelity simulation, observing that ViT models are\nmore effective than others as quadrotor speeds increase and in generalization\nto unseen environments, while the addition of recurrence further improves\nperformance while reducing quadrotor energy cost across all tested flight\nspeeds. We assess performance at speeds of up to 7m/s in simulation and\nhardware. To the best of our knowledge, this is the first work to utilize\nvision transformers for end-to-end vision-based quadrotor control.",
      "tldr_zh": "该研究探讨了使用 Vision Transformers (ViT) 实现端到端视觉控制的四旋翼无人机（quadrotor）在密集杂乱环境中的高速度障碍避免，并与其他架构如卷积网络、U-Net 和循环网络进行比较。结果显示，ViT 模型在速度增加（如高达7m/s）和泛化到未见环境时表现出色，而添加循环机制进一步提升了性能并降低了能耗。传统基于模型的导航方法在高速度下因传感器噪声和延迟而失效，该工作首次将 ViT 应用于端到端视觉四旋翼控制，展示了其潜力。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.RO",
      "comment": "11 pages, 18 figures, 3 tables (with supplementary)",
      "pdf_url": "http://arxiv.org/pdf/2405.10391v3",
      "published_date": "2024-05-16 18:36:43 UTC",
      "updated_date": "2025-04-01 18:27:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:52:31.055730"
    },
    {
      "arxiv_id": "2405.10385v2",
      "title": "AmazUtah_NLP at SemEval-2024 Task 9: A MultiChoice Question Answering System for Commonsense Defying Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Mina Ghashami",
        "Soumya Smruti Mishra"
      ],
      "abstract": "The SemEval 2024 BRAINTEASER task represents a pioneering venture in Natural\nLanguage Processing (NLP) by focusing on lateral thinking, a dimension of\ncognitive reasoning that is often overlooked in traditional linguistic\nanalyses. This challenge comprises of Sentence Puzzle and Word Puzzle subtasks\nand aims to test language models' capacity for divergent thinking.\n  In this paper, we present our approach to the BRAINTEASER task. We employ a\nholistic strategy by leveraging cutting-edge pre-trained models in multiple\nchoice architecture, and diversify the training data with Sentence and Word\nPuzzle datasets. To gain further improvement, we fine-tuned the model with\nsynthetic humor or jokes dataset and the RiddleSense dataset which helped\naugmenting the model's lateral thinking abilities. Empirical results show that\nour approach achieve 92.5% accuracy in Sentence Puzzle subtask and 80.2%\naccuracy in Word Puzzle subtask.",
      "tldr_zh": "本论文介绍了AmazUtah_NLP团队在SemEval-2024 Task 9中的方法，针对BRAINTEASER任务的横向思维(lateral thinking)挑战，包括Sentence Puzzle和Word Puzzle子任务，以测试语言模型的发散性思考能力。作者采用先进的预训练模型构建多选问答系统，并通过多样化训练数据（如Sentence和Word Puzzle数据集）以及合成幽默数据集和RiddleSense数据集进行微调，提升模型的横向推理性能。实验结果显示，该系统在Sentence Puzzle子任务中获得92.5%的准确率，在Word Puzzle子任务中达到80.2%。这项工作为NLP领域中的常识挑战性推理提供了有效的改进策略。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at SemEval 2024 (Colocated with NAACL 2024)",
      "pdf_url": "http://arxiv.org/pdf/2405.10385v2",
      "published_date": "2024-05-16 18:26:38 UTC",
      "updated_date": "2024-05-20 05:21:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:52:42.209586"
    },
    {
      "arxiv_id": "2405.10378v2",
      "title": "A Polynomial-Time Approximation for Pairwise Fair $k$-Median Clustering",
      "title_zh": "翻译失败",
      "authors": [
        "Sayan Bandyapadhyay",
        "Eden Chlamtáč",
        "Zachary Friggstad",
        "Mahya Jamshidian",
        "Yury Makarychev",
        "Ali Vakilian"
      ],
      "abstract": "In this work, we study pairwise fair clustering with $\\ell \\ge 2$ groups,\nwhere for every cluster $C$ and every group $i \\in [\\ell]$, the number of\npoints in $C$ from group $i$ must be at most $t$ times the number of points in\n$C$ from any other group $j \\in [\\ell]$, for a given integer $t$. To the best\nof our knowledge, only bi-criteria approximation and exponential-time\nalgorithms follow for this problem from the prior work on fair clustering\nproblems when $\\ell > 2$. In our work, focusing on the $\\ell > 2$ case, we\ndesign the first polynomial-time $O(k^2\\cdot \\ell \\cdot t)$-approximation for\nthis problem with $k$-median cost that does not violate the fairness\nconstraints. We complement our algorithmic result by providing hardness of\napproximation results, which show that our problem even when $\\ell=2$ is almost\nas hard as the popular uniform capacitated $k$-median, for which no\npolynomial-time algorithm with an approximation factor of $o(\\log k)$ is known.",
      "tldr_zh": "本文研究了多组（ℓ ≥ 2）下的配对公平 k-中位数聚类问题，要求每个聚类 C 中，任何组 i 的点数不超过其他组 j 的点数的 t 倍。针对 ℓ > 2 的情况，该论文提出了首个多项式时间算法，提供 O(k² · ℓ · t) 逼近的 k-median 成本解，同时严格遵守公平约束。作为补充，该研究证明了该问题的逼近硬度，即使 ℓ=2 时，其难度接近均匀容量 k-median 问题，后者缺乏 o(log k) 的多项式时间算法。",
      "categories": [
        "cs.DS",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.10378v2",
      "published_date": "2024-05-16 18:17:44 UTC",
      "updated_date": "2025-02-27 16:29:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:52:53.428922"
    },
    {
      "arxiv_id": "2405.10376v1",
      "title": "Dealing Doubt: Unveiling Threat Models in Gradient Inversion Attacks under Federated Learning, A Survey and Taxonomy",
      "title_zh": "翻译失败",
      "authors": [
        "Yichuan Shi",
        "Olivera Kotevska",
        "Viktor Reshniak",
        "Abhishek Singh",
        "Ramesh Raskar"
      ],
      "abstract": "Federated Learning (FL) has emerged as a leading paradigm for decentralized,\nprivacy preserving machine learning training. However, recent research on\ngradient inversion attacks (GIAs) have shown that gradient updates in FL can\nleak information on private training samples. While existing surveys on GIAs\nhave focused on the honest-but-curious server threat model, there is a dearth\nof research categorizing attacks under the realistic and far more\nprivacy-infringing cases of malicious servers and clients. In this paper, we\npresent a survey and novel taxonomy of GIAs that emphasize FL threat models,\nparticularly that of malicious servers and clients. We first formally define\nGIAs and contrast conventional attacks with the malicious attacker. We then\nsummarize existing honest-but-curious attack strategies, corresponding\ndefenses, and evaluation metrics. Critically, we dive into attacks with\nmalicious servers and clients to highlight how they break existing FL defenses,\nfocusing specifically on reconstruction methods, target model architectures,\ntarget data, and evaluation metrics. Lastly, we discuss open problems and\nfuture research directions.",
      "tldr_zh": "这篇论文调查了 Federated Learning (FL) 中的 Gradient Inversion Attacks (GIAs)，特别强调了恶意服务器和客户端的威胁模型，并提出一个新的分类法来系统化这些攻击。论文首先定义了 GIAs，并对比了传统诚实但好奇的攻击策略与其防御和评估指标，然后深入探讨了恶意攻击如何通过重建方法、目标模型架构和数据来绕过现有 FL 防御。研究结果突出了这些攻击的隐私侵害潜力，并指出了未来研究方向，如改进防御机制和评估标准。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.10376v1",
      "published_date": "2024-05-16 18:15:38 UTC",
      "updated_date": "2024-05-16 18:15:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:53:06.495904"
    },
    {
      "arxiv_id": "2405.10369v1",
      "title": "Reinforcement learning",
      "title_zh": "强化学习",
      "authors": [
        "Sarod Yatawatta"
      ],
      "abstract": "Observing celestial objects and advancing our scientific knowledge about them\ninvolves tedious planning, scheduling, data collection and data\npost-processing. Many of these operational aspects of astronomy are guided and\nexecuted by expert astronomers. Reinforcement learning is a mechanism where we\n(as humans and astronomers) can teach agents of artificial intelligence to\nperform some of these tedious tasks. In this paper, we will present a state of\nthe art overview of reinforcement learning and how it can benefit astronomy.",
      "tldr_zh": "这篇论文探讨了强化学习(Reinforcement Learning)在天文学中的应用，旨在通过训练人工智能代理自动化繁琐的任务，如天体观测的规划、调度、数据收集和后处理，以减轻专家 astronomer's 负担。论文提供了强化学习的最新概述，强调它能让 AI 代理学习执行这些重复性操作，从而提升效率。总体而言，该研究展示了强化学习如何为天文学带来益处，促进科学知识的进步。",
      "categories": [
        "astro-ph.IM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "astro-ph.IM",
      "comment": "To appear, Astronomy & Computing",
      "pdf_url": "http://arxiv.org/pdf/2405.10369v1",
      "published_date": "2024-05-16 18:03:17 UTC",
      "updated_date": "2024-05-16 18:03:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:53:16.618079"
    },
    {
      "arxiv_id": "2405.10315v3",
      "title": "TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction",
      "title_zh": "TRANSIC：通过从在线修正中学习实现的模拟到真实策略转移",
      "authors": [
        "Yunfan Jiang",
        "Chen Wang",
        "Ruohan Zhang",
        "Jiajun Wu",
        "Li Fei-Fei"
      ],
      "abstract": "Learning in simulation and transferring the learned policy to the real world\nhas the potential to enable generalist robots. The key challenge of this\napproach is to address simulation-to-reality (sim-to-real) gaps. Previous\nmethods often require domain-specific knowledge a priori. We argue that a\nstraightforward way to obtain such knowledge is by asking humans to observe and\nassist robot policy execution in the real world. The robots can then learn from\nhumans to close various sim-to-real gaps. We propose TRANSIC, a data-driven\napproach to enable successful sim-to-real transfer based on a human-in-the-loop\nframework. TRANSIC allows humans to augment simulation policies to overcome\nvarious unmodeled sim-to-real gaps holistically through intervention and online\ncorrection. Residual policies can be learned from human corrections and\nintegrated with simulation policies for autonomous execution. We show that our\napproach can achieve successful sim-to-real transfer in complex and\ncontact-rich manipulation tasks such as furniture assembly. Through synergistic\nintegration of policies learned in simulation and from humans, TRANSIC is\neffective as a holistic approach to addressing various, often coexisting\nsim-to-real gaps. It displays attractive properties such as scaling with human\neffort. Videos and code are available at https://transic-robot.github.io/",
      "tldr_zh": "这篇论文提出了 TRANSIC，一种数据驱动的方法，通过 human-in-the-loop 框架实现 sim-to-real 策略转移，让人类通过干预和在线修正来增强模拟策略并克服各种未建模的差距。TRANSIC 从人类修正中学习剩余策略，并将其与模拟策略整合，实现自主执行，在复杂接触丰富的操作任务如家具组装中取得了成功。实验结果显示，该方法能整体解决多种 sim-to-real 差距，并随着人类努力而扩展，具有良好的可扩展性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "8th Conference on Robot Learning (CoRL 2024), Munich, Germany.\n  Project website: https://transic-robot.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2405.10315v3",
      "published_date": "2024-05-16 17:59:07 UTC",
      "updated_date": "2024-10-14 06:03:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:53:29.001495"
    },
    {
      "arxiv_id": "2405.10313v2",
      "title": "How Far Are We From AGI: Are LLMs All We Need?",
      "title_zh": "我们距离 AGI 还有多远：LLMs 是否就是我们所需要的全部？",
      "authors": [
        "Tao Feng",
        "Chuanyang Jin",
        "Jingyu Liu",
        "Kunlun Zhu",
        "Haoqin Tu",
        "Zirui Cheng",
        "Guanyu Lin",
        "Jiaxuan You"
      ],
      "abstract": "The evolution of artificial intelligence (AI) has profoundly impacted human\nsociety, driving significant advancements in multiple sectors. AGI,\ndistinguished by its ability to execute diverse real-world tasks with\nefficiency and effectiveness comparable to human intelligence, reflects a\nparamount milestone in AI evolution. While existing studies have reviewed\nspecific advancements in AI and proposed potential paths to AGI, such as large\nlanguage models (LLMs), they fall short of providing a thorough exploration of\nAGI's definitions, objectives, and developmental trajectories. Unlike previous\nsurvey papers, this work goes beyond summarizing LLMs by addressing key\nquestions about our progress toward AGI and outlining the strategies essential\nfor its realization through comprehensive analysis, in-depth discussions, and\nnovel insights. We start by articulating the requisite capability frameworks\nfor AGI, integrating the internal, interface, and system dimensions. As the\nrealization of AGI requires more advanced capabilities and adherence to\nstringent constraints, we further discuss necessary AGI alignment technologies\nto harmonize these factors. Notably, we emphasize the importance of approaching\nAGI responsibly by first defining the key levels of AGI progression, followed\nby the evaluation framework that situates the status quo, and finally giving\nour roadmap of how to reach the pinnacle of AGI. Moreover, to give tangible\ninsights into the ubiquitous impact of the integration of AI, we outline\nexisting challenges and potential pathways toward AGI in multiple domains. In\nsum, serving as a pioneering exploration into the current state and future\ntrajectory of AGI, this paper aims to foster a collective comprehension and\ncatalyze broader public discussions among researchers and practitioners on AGI.",
      "tldr_zh": "这篇论文探讨了我们距离实现 AGI（Artificial General Intelligence）还有多远，并质疑 LLMs（Large Language Models）是否足够。论文通过全面分析定义了 AGI 的能力框架，包括内部、接口和系统维度，并讨论了必要的对齐技术、进展级别评估以及实现 AGI 的路线图。作者强调了在多个领域（如社会和技术）的挑战与潜在路径，旨在促进研究者和从业者对 AGI 发展的集体理解和公共讨论。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.10313v2",
      "published_date": "2024-05-16 17:59:02 UTC",
      "updated_date": "2024-11-24 18:44:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:53:41.097701"
    },
    {
      "arxiv_id": "2405.10310v1",
      "title": "Stochastic Q-learning for Large Discrete Action Spaces",
      "title_zh": "翻译失败",
      "authors": [
        "Fares Fourati",
        "Vaneet Aggarwal",
        "Mohamed-Slim Alouini"
      ],
      "abstract": "In complex environments with large discrete action spaces, effective\ndecision-making is critical in reinforcement learning (RL). Despite the\nwidespread use of value-based RL approaches like Q-learning, they come with a\ncomputational burden, necessitating the maximization of a value function over\nall actions in each iteration. This burden becomes particularly challenging\nwhen addressing large-scale problems and using deep neural networks as function\napproximators. In this paper, we present stochastic value-based RL approaches\nwhich, in each iteration, as opposed to optimizing over the entire set of $n$\nactions, only consider a variable stochastic set of a sublinear number of\nactions, possibly as small as $\\mathcal{O}(\\log(n))$. The presented stochastic\nvalue-based RL methods include, among others, Stochastic Q-learning, StochDQN,\nand StochDDQN, all of which integrate this stochastic approach for both\nvalue-function updates and action selection. The theoretical convergence of\nStochastic Q-learning is established, while an analysis of stochastic\nmaximization is provided. Moreover, through empirical validation, we illustrate\nthat the various proposed approaches outperform the baseline methods across\ndiverse environments, including different control problems, achieving\nnear-optimal average returns in significantly reduced time.",
      "tldr_zh": "这篇论文针对强化学习（RL）中大型离散动作空间的计算挑战，提出了一种随机值基于RL方法，以减少传统Q-learning在每次迭代中需优化所有动作的负担。方法包括Stochastic Q-learning、StochDQN和StochDDQN，这些算法仅考虑子线性数量的随机动作子集（如O(log(n))），并在值函数更新和动作选择中整合这一机制。论文证明了Stochastic Q-learning的理论收敛性，并通过实验验证显示，这些方法在各种控制环境中优于基线模型，实现了近似最优平均回报并显著缩短了计算时间。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PF",
        "cs.RO",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.10310v1",
      "published_date": "2024-05-16 17:58:44 UTC",
      "updated_date": "2024-05-16 17:58:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:53:53.686001"
    },
    {
      "arxiv_id": "2405.10305v1",
      "title": "4D Panoptic Scene Graph Generation",
      "title_zh": "4D 全景场景图生成",
      "authors": [
        "Jingkang Yang",
        "Jun Cen",
        "Wenxuan Peng",
        "Shuai Liu",
        "Fangzhou Hong",
        "Xiangtai Li",
        "Kaiyang Zhou",
        "Qifeng Chen",
        "Ziwei Liu"
      ],
      "abstract": "We are living in a three-dimensional space while moving forward through a\nfourth dimension: time. To allow artificial intelligence to develop a\ncomprehensive understanding of such a 4D environment, we introduce 4D Panoptic\nScene Graph (PSG-4D), a new representation that bridges the raw visual data\nperceived in a dynamic 4D world and high-level visual understanding.\nSpecifically, PSG-4D abstracts rich 4D sensory data into nodes, which represent\nentities with precise location and status information, and edges, which capture\nthe temporal relations. To facilitate research in this new area, we build a\nrichly annotated PSG-4D dataset consisting of 3K RGB-D videos with a total of\n1M frames, each of which is labeled with 4D panoptic segmentation masks as well\nas fine-grained, dynamic scene graphs. To solve PSG-4D, we propose PSG4DFormer,\na Transformer-based model that can predict panoptic segmentation masks, track\nmasks along the time axis, and generate the corresponding scene graphs via a\nrelation component. Extensive experiments on the new dataset show that our\nmethod can serve as a strong baseline for future research on PSG-4D. In the\nend, we provide a real-world application example to demonstrate how we can\nachieve dynamic scene understanding by integrating a large language model into\nour PSG-4D system.",
      "tldr_zh": "本论文引入了 4D Panoptic Scene Graph (PSG-4D)，一种新表示方法，将动态 4D 环境（三维空间加时间）的视觉数据抽象为节点（实体位置和状态）和边（时间关系），以提升 AI 的高阶视觉理解。研究者构建了一个包含 3K 个 RGB-D 视频和总计 1M 帧的标注数据集，每个帧均标注有 4D 全景分割掩码和细粒度动态场景图。论文提出 PSG4DFormer，一种基于 Transformer 的模型，能够预测全景分割掩码、沿时间轴跟踪掩码，并通过关系组件生成场景图。实验结果显示，该方法在数据集上表现出色，作为未来研究的强基线，并通过与大语言模型集成，展示了动态场景理解的实际应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted as NeurIPS 2023. Code: https://github.com/Jingkang50/PSG4D\n  Previous Series: PSG https://github.com/Jingkang50/OpenPSG and PVSG\n  https://github.com/Jingkang50/OpenPVSG",
      "pdf_url": "http://arxiv.org/pdf/2405.10305v1",
      "published_date": "2024-05-16 17:56:55 UTC",
      "updated_date": "2024-05-16 17:56:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:54:08.103768"
    },
    {
      "arxiv_id": "2405.10301v3",
      "title": "Conformal Alignment: Knowing When to Trust Foundation Models with Guarantees",
      "title_zh": "翻译失败",
      "authors": [
        "Yu Gui",
        "Ying Jin",
        "Zhimei Ren"
      ],
      "abstract": "Before deploying outputs from foundation models in high-stakes tasks, it is\nimperative to ensure that they align with human values. For instance, in\nradiology report generation, reports generated by a vision-language model must\nalign with human evaluations before their use in medical decision-making. This\npaper presents Conformal Alignment, a general framework for identifying units\nwhose outputs meet a user-specified alignment criterion. It is guaranteed that\non average, a prescribed fraction of selected units indeed meet the alignment\ncriterion, regardless of the foundation model or the data distribution. Given\nany pre-trained model and new units with model-generated outputs, Conformal\nAlignment leverages a set of reference data with ground-truth alignment status\nto train an alignment predictor. It then selects new units whose predicted\nalignment scores surpass a data-dependent threshold, certifying their\ncorresponding outputs as trustworthy. Through applications to question\nanswering and radiology report generation, we demonstrate that our method is\nable to accurately identify units with trustworthy outputs via lightweight\ntraining over a moderate amount of reference data. En route, we investigate the\ninformativeness of various features in alignment prediction and combine them\nwith standard models to construct the alignment predictor.",
      "tldr_zh": "本研究提出 Conformal Alignment 框架，用于评估和保证 foundation models 输出是否符合用户指定的 alignment criterion，尤其在高风险任务如放射学报告生成中。该框架利用参考数据训练一个 alignment predictor，然后通过数据依赖阈值选择预测分数较高的输出，确保选中的单位中平均有规定比例真正满足对齐标准，无论模型或数据分布如何。实验结果显示，该方法在问答和放射学报告生成任务中，通过轻量级训练和少量参考数据，即可准确识别可信输出，并探讨了各种特征在对齐预测中的信息性，以优化预测器性能。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.10301v3",
      "published_date": "2024-05-16 17:55:24 UTC",
      "updated_date": "2024-11-05 01:55:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:54:18.227043"
    },
    {
      "arxiv_id": "2405.10299v3",
      "title": "HW-GPT-Bench: Hardware-Aware Architecture Benchmark for Language Models",
      "title_zh": "HW-GPT-Bench：硬件感知语言模型",
      "authors": [
        "Rhea Sanjay Sukthanker",
        "Arber Zela",
        "Benedikt Staffler",
        "Aaron Klein",
        "Lennart Purucker",
        "Joerg K. H. Franke",
        "Frank Hutter"
      ],
      "abstract": "The increasing size of language models necessitates a thorough analysis\nacross multiple dimensions to assess trade-offs among crucial hardware metrics\nsuch as latency, energy consumption, GPU memory usage, and performance.\nIdentifying optimal model configurations under specific hardware constraints is\nbecoming essential but remains challenging due to the computational load of\nexhaustive training and evaluation on multiple devices. To address this, we\nintroduce HW-GPT-Bench, a hardware-aware benchmark that utilizes surrogate\npredictions to approximate various hardware metrics across 13 devices of\narchitectures in the GPT-2 family, with architectures containing up to 1.55B\nparameters. Our surrogates, via calibrated predictions and reliable uncertainty\nestimates, faithfully model the heteroscedastic noise inherent in the energy\nand latency measurements. To estimate perplexity, we employ weight-sharing\ntechniques from Neural Architecture Search (NAS), inheriting pretrained weights\nfrom the largest GPT-2 model. Finally, we demonstrate the utility of\nHW-GPT-Bench by simulating optimization trajectories of various multi-objective\noptimization algorithms in just a few seconds.",
      "tldr_zh": "该研究针对语言模型规模增长带来的硬件指标权衡（如延迟、能耗、GPU 内存使用和性能）问题，引入了 HW-GPT-Bench，这是一个硬件感知基准，用于评估 GPT-2 家族架构（最多 1.55B 参数）的13种设备表现。HW-GPT-Bench 通过代理预测和校准不确定性估计来模拟硬件测量中的异方差噪声，并利用 Neural Architecture Search (NAS) 的权重共享技术来估计 perplexity，从而避免了耗时的全面训练。实验结果显示，该基准能在几秒内模拟多目标优化算法的轨迹，帮助快速识别特定硬件约束下的最佳模型配置。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "59 pages, 73 figures, 11 tables",
      "pdf_url": "http://arxiv.org/pdf/2405.10299v3",
      "published_date": "2024-05-16 17:53:32 UTC",
      "updated_date": "2024-11-03 17:45:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:54:29.135554"
    },
    {
      "arxiv_id": "2405.10295v3",
      "title": "Societal Adaptation to Advanced AI",
      "title_zh": "翻译失败",
      "authors": [
        "Jamie Bernardi",
        "Gabriel Mukobi",
        "Hilary Greaves",
        "Lennart Heim",
        "Markus Anderljung"
      ],
      "abstract": "Existing strategies for managing risks from advanced AI systems often focus\non affecting what AI systems are developed and how they diffuse. However, this\napproach becomes less feasible as the number of developers of advanced AI\ngrows, and impedes beneficial use-cases as well as harmful ones. In response,\nwe urge a complementary approach: increasing societal adaptation to advanced\nAI, that is, reducing the expected negative impacts from a given level of\ndiffusion of a given AI capability. We introduce a conceptual framework which\nhelps identify adaptive interventions that avoid, defend against and remedy\npotentially harmful uses of AI systems, illustrated with examples in election\nmanipulation, cyberterrorism, and loss of control to AI decision-makers. We\ndiscuss a three-step cycle that society can implement to adapt to AI.\nIncreasing society's ability to implement this cycle builds its resilience to\nadvanced AI. We conclude with concrete recommendations for governments,\nindustry, and third-parties.",
      "tldr_zh": "本文讨论了管理高级 AI 风险的现有策略（如影响 AI 系统的开发和扩散）的局限性，随着开发者增多，这种方法变得不可行，并可能阻碍有益应用。作者提出补充方法，即提升社会对高级 AI 的适应性（Societal Adaptation to Advanced AI），通过一个概念框架识别避免、防御和修复潜在有害用途的干预措施，并以选举操纵、网络恐怖主义和 AI 决策失控为例进行说明。该框架引入一个三步循环，帮助社会构建韧性（resilience），并为政府、行业和第三方提供具体推荐，以减少 AI 扩散的负面影响。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.10295v3",
      "published_date": "2024-05-16 17:52:12 UTC",
      "updated_date": "2025-01-23 12:47:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:54:41.557563"
    },
    {
      "arxiv_id": "2405.10292v3",
      "title": "Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning",
      "title_zh": "通过强化学习微调大型视觉语言模型作为决策代理",
      "authors": [
        "Yuexiang Zhai",
        "Hao Bai",
        "Zipeng Lin",
        "Jiayi Pan",
        "Shengbang Tong",
        "Yifei Zhou",
        "Alane Suhr",
        "Saining Xie",
        "Yann LeCun",
        "Yi Ma",
        "Sergey Levine"
      ],
      "abstract": "Large vision-language models (VLMs) fine-tuned on specialized visual\ninstruction-following data have exhibited impressive language reasoning\ncapabilities across various scenarios. However, this fine-tuning paradigm may\nnot be able to efficiently learn optimal decision-making agents in multi-step\ngoal-directed tasks from interactive environments. To address this challenge,\nwe propose an algorithmic framework that fine-tunes VLMs with reinforcement\nlearning (RL). Specifically, our framework provides a task description and then\nprompts the VLM to generate chain-of-thought (CoT) reasoning, enabling the VLM\nto efficiently explore intermediate reasoning steps that lead to the final\ntext-based action. Next, the open-ended text output is parsed into an\nexecutable action to interact with the environment to obtain goal-directed task\nrewards. Finally, our framework uses these task rewards to fine-tune the entire\nVLM with RL. Empirically, we demonstrate that our proposed framework enhances\nthe decision-making capabilities of VLM agents across various tasks, enabling\n7b models to outperform commercial models such as GPT4-V or Gemini.\nFurthermore, we find that CoT reasoning is a crucial component for performance\nimprovement, as removing the CoT reasoning results in a significant decrease in\nthe overall performance of our method.",
      "tldr_zh": "该研究提出了一种算法框架，通过强化学习（Reinforcement Learning, RL）微调大型视觉语言模型（Large Vision-Language Models, VLMs），以提升其在多步目标导向任务中的决策能力。框架的关键步骤包括提供任务描述、提示VLM生成链式思维（Chain-of-Thought, CoT）推理、解析文本输出为可执行动作并与环境交互获取奖励，然后使用这些奖励对整个VLM进行微调。实验结果显示，该框架显著提高了VLM代理的性能，使7b模型超越商业模型如GPT4-V或Gemini；此外，CoT推理被证明是性能提升的核心因素，去除它会导致整体效果大幅下降。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.10292v3",
      "published_date": "2024-05-16 17:50:19 UTC",
      "updated_date": "2024-10-07 19:13:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:54:56.030732"
    },
    {
      "arxiv_id": "2405.10288v3",
      "title": "Timeline-based Sentence Decomposition with In-Context Learning for Temporal Fact Extraction",
      "title_zh": "翻译失败",
      "authors": [
        "Jianhao Chen",
        "Haoyuan Ouyang",
        "Junyang Ren",
        "Wentao Ding",
        "Wei Hu",
        "Yuzhong Qu"
      ],
      "abstract": "Facts extraction is pivotal for constructing knowledge graphs. Recently, the\nincreasing demand for temporal facts in downstream tasks has led to the\nemergence of the task of temporal fact extraction. In this paper, we\nspecifically address the extraction of temporal facts from natural language\ntext. Previous studies fail to handle the challenge of establishing\ntime-to-fact correspondences in complex sentences. To overcome this hurdle, we\npropose a timeline-based sentence decomposition strategy using large language\nmodels (LLMs) with in-context learning, ensuring a fine-grained understanding\nof the timeline associated with various facts. In addition, we evaluate the\nperformance of LLMs for direct temporal fact extraction and get unsatisfactory\nresults. To this end, we introduce TSDRE, a method that incorporates the\ndecomposition capabilities of LLMs into the traditional fine-tuning of smaller\npre-trained language models (PLMs). To support the evaluation, we construct\nComplexTRED, a complex temporal fact extraction dataset. Our experiments show\nthat TSDRE achieves state-of-the-art results on both HyperRED-Temporal and\nComplexTRED datasets.",
      "tldr_zh": "这篇论文针对时间事实提取任务，提出了一种基于时间线的句子分解策略，使用大语言模型 (LLMs) 和 in-context learning 来处理复杂句子中时间与事实对应关系的挑战，确保对事实和时间线的细粒度理解。作者引入了 TSDRE 方法，将 LLMs 的分解能力整合到较小预训练语言模型 (PLMs) 的传统微调流程中，以提升提取性能。实验结果显示，TSDRE 在 HyperRED-Temporal 和新构建的 ComplexTRED 数据集上达到了最先进的结果。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ACL2024 main conference",
      "pdf_url": "http://arxiv.org/pdf/2405.10288v3",
      "published_date": "2024-05-16 17:48:21 UTC",
      "updated_date": "2024-06-18 08:22:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:55:07.047974"
    },
    {
      "arxiv_id": "2405.10286v1",
      "title": "FFF: Fixing Flawed Foundations in contrastive pre-training results in very strong Vision-Language models",
      "title_zh": "翻译失败",
      "authors": [
        "Adrian Bulat",
        "Yassine Ouali",
        "Georgios Tzimiropoulos"
      ],
      "abstract": "Despite noise and caption quality having been acknowledged as important\nfactors impacting vision-language contrastive pre-training, in this paper, we\nshow that the full potential of improving the training process by addressing\nsuch issues is yet to be realized. Specifically, we firstly study and analyze\ntwo issues affecting training: incorrect assignment of negative pairs, and low\ncaption quality and diversity. Then, we devise effective solutions for\naddressing both problems, which essentially require training with multiple true\npositive pairs. Finally, we propose training with sigmoid loss to address such\na requirement. We show very large gains over the current state-of-the-art for\nboth image recognition ($\\sim +6\\%$ on average over 11 datasets) and image\nretrieval ($\\sim +19\\%$ on Flickr30k and $\\sim +15\\%$ on MSCOCO).",
      "tldr_zh": "本研究揭示了视觉语言对比预训练（contrastive pre-training）中存在的关键问题，包括负对的错误分配（incorrect assignment of negative pairs）和标题质量及多样性不足（low caption quality and diversity）。为了解决这些问题，作者提出了一种新方法，通过使用多个真实正对（multiple true positive pairs）进行训练，并采用sigmoid loss来优化训练过程。实验结果显示，该方法显著提升了性能，在图像识别（image recognition）任务上平均提高了约6%（在11个数据集上），以及在图像检索（image retrieval）任务上分别提高了约19%（Flickr30k）和15%（MSCOCO）。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at CVPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.10286v1",
      "published_date": "2024-05-16 17:46:54 UTC",
      "updated_date": "2024-05-16 17:46:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:55:18.322756"
    },
    {
      "arxiv_id": "2405.10272v1",
      "title": "Faces that Speak: Jointly Synthesising Talking Face and Speech from Text",
      "title_zh": "翻译失败",
      "authors": [
        "Youngjoon Jang",
        "Ji-Hoon Kim",
        "Junseok Ahn",
        "Doyeop Kwak",
        "Hong-Sun Yang",
        "Yoon-Cheol Ju",
        "Il-Hwan Kim",
        "Byeong-Yeol Kim",
        "Joon Son Chung"
      ],
      "abstract": "The goal of this work is to simultaneously generate natural talking faces and\nspeech outputs from text. We achieve this by integrating Talking Face\nGeneration (TFG) and Text-to-Speech (TTS) systems into a unified framework. We\naddress the main challenges of each task: (1) generating a range of head poses\nrepresentative of real-world scenarios, and (2) ensuring voice consistency\ndespite variations in facial motion for the same identity. To tackle these\nissues, we introduce a motion sampler based on conditional flow matching, which\nis capable of high-quality motion code generation in an efficient way.\nMoreover, we introduce a novel conditioning method for the TTS system, which\nutilises motion-removed features from the TFG model to yield uniform speech\noutputs. Our extensive experiments demonstrate that our method effectively\ncreates natural-looking talking faces and speech that accurately match the\ninput text. To our knowledge, this is the first effort to build a multimodal\nsynthesis system that can generalise to unseen identities.",
      "tldr_zh": "本研究提出了一种统一框架，用于从文本同时合成自然的说话面部和语音输出，通过整合 Talking Face Generation (TFG) 和 Text-to-Speech (TTS) 系统来实现。该框架解决了关键挑战，包括生成真实场景的头部姿势多样性和确保同一身份语音一致性，采用基于 conditional flow matching 的运动采样器高效生成高质量运动代码，以及一种新颖的 TTS 条件方法利用 TFG 的去除运动特征来产生统一的语音。实验结果显示，该方法能有效创建与输入文本精确匹配的自然面部和语音输出，且这是首个能推广到未见过身份的多模态合成系统。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.SD",
        "eess.AS",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.10272v1",
      "published_date": "2024-05-16 17:29:37 UTC",
      "updated_date": "2024-05-16 17:29:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:55:30.450196"
    },
    {
      "arxiv_id": "2405.10271v2",
      "title": "Adaptive Hybrid Model Pruning in Federated Learning through Loss Exploration",
      "title_zh": "翻译失败",
      "authors": [
        "Christian Internò",
        "Elena Raponi",
        "Niki van Stein",
        "Thomas Bäck",
        "Markus Olhofer",
        "Yaochu Jin",
        "Barbara Hammer"
      ],
      "abstract": "The rapid proliferation of smart devices coupled with the advent of 6G\nnetworks has profoundly reshaped the domain of collaborative machine learning.\nAlongside growing privacy-security concerns in sensitive fields, these\ndevelopments have positioned federated learning (FL) as a pivotal technology\nfor decentralized model training. Despite its vast potential, specially in the\nage of complex foundation models, FL encounters challenges such as elevated\ncommunication costs, computational constraints, and the complexities of non-IID\ndata distributions. We introduce AutoFLIP, an innovative approach that utilizes\na federated loss exploration phase to drive adaptive hybrid pruning, operating\nin a structured and unstructured way. This innovative mechanism automatically\nidentifies and prunes model substructure by distilling knowledge on model\ngradients behavior across different non-IID client losses topology, thereby\noptimizing computational efficiency and enhancing model performance on resource\nconstrained scenarios. Extensive experiments on various datasets and FL tasks\nreveal that AutoFLIP not only efficiently accelerates global convergence, but\nalso achieves superior accuracy and robustness compared to traditional methods.\nOn average, AutoFLIP reduces computational overhead by 48.8% and communication\ncosts by 35.5%, while improving global accuracy. By significantly reducing\nthese overheads, AutoFLIP offer the way for efficient FL deployment in\nreal-world applications for a scalable and broad applicability.",
      "tldr_zh": "这篇论文针对 Federated Learning (FL) 中的高通信成本、计算限制和非-IID 数据分布挑战，提出了一种创新方法 AutoFLIP，通过 federated loss exploration 阶段驱动自适应混合剪枝（structured 和 unstructured）。AutoFLIP 通过分析模型梯度在不同客户端损失拓扑上的行为，自动识别并剪枝模型子结构，从而优化计算效率并提升模型性能。实验在多种数据集和 FL 任务上表明，AutoFLIP 平均减少计算开销 48.8% 和通信成本 35.5%，同时提高了全局准确性和鲁棒性，为 FL 在真实世界应用的部署提供了高效、可扩展的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "cs.ET"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.10271v2",
      "published_date": "2024-05-16 17:27:41 UTC",
      "updated_date": "2024-10-15 12:06:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:55:42.914796"
    },
    {
      "arxiv_id": "2405.10262v1",
      "title": "Two-Phase Dynamics of Interactions Explains the Starting Point of a DNN Learning Over-Fitted Features",
      "title_zh": "翻译失败",
      "authors": [
        "Junpeng Zhang",
        "Qing Li",
        "Liang Lin",
        "Quanshi Zhang"
      ],
      "abstract": "This paper investigates the dynamics of a deep neural network (DNN) learning\ninteractions. Previous studies have discovered and mathematically proven that\ngiven each input sample, a well-trained DNN usually only encodes a small number\nof interactions (non-linear relationships) between input variables in the\nsample. A series of theorems have been derived to prove that we can consider\nthe DNN's inference equivalent to using these interactions as primitive\npatterns for inference. In this paper, we discover the DNN learns interactions\nin two phases. The first phase mainly penalizes interactions of medium and high\norders, and the second phase mainly learns interactions of gradually increasing\norders. We can consider the two-phase phenomenon as the starting point of a DNN\nlearning over-fitted features. Such a phenomenon has been widely shared by DNNs\nwith various architectures trained for different tasks. Therefore, the\ndiscovery of the two-phase dynamics provides a detailed mechanism for how a DNN\ngradually learns different inference patterns (interactions). In particular, we\nhave also verified the claim that high-order interactions have weaker\ngeneralization power than low-order interactions. Thus, the discovered\ntwo-phase dynamics also explains how the generalization power of a DNN changes\nduring the training process.",
      "tldr_zh": "本文研究了深度神经网络(DNN)学习交互（输入变量之间的非线性关系）的动态，发现DNN的学习过程分为两个阶段：第一阶段主要惩罚中等和高阶交互，第二阶段则逐渐学习低阶到高阶交互。这种两阶段现象被视为DNN开始学习过度拟合特征的起点，并在各种架构的DNN和不同任务中普遍存在。论文通过理论证明和实验验证，还证实了高阶interactions的泛化能力弱于低阶interactions，从而解释了DNN训练过程中泛化能力的动态变化。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.10262v1",
      "published_date": "2024-05-16 17:13:25 UTC",
      "updated_date": "2024-05-16 17:13:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:55:56.853732"
    },
    {
      "arxiv_id": "2405.10260v1",
      "title": "Keep It Private: Unsupervised Privatization of Online Text",
      "title_zh": "翻译失败",
      "authors": [
        "Calvin Bao",
        "Marine Carpuat"
      ],
      "abstract": "Authorship obfuscation techniques hold the promise of helping people protect\ntheir privacy in online communications by automatically rewriting text to hide\nthe identity of the original author. However, obfuscation has been evaluated in\nnarrow settings in the NLP literature and has primarily been addressed with\nsuperficial edit operations that can lead to unnatural outputs. In this work,\nwe introduce an automatic text privatization framework that fine-tunes a large\nlanguage model via reinforcement learning to produce rewrites that balance\nsoundness, sense, and privacy. We evaluate it extensively on a large-scale test\nset of English Reddit posts by 68k authors composed of short-medium length\ntexts. We study how the performance changes among evaluative conditions\nincluding authorial profile length and authorship detection strategy. Our\nmethod maintains high text quality according to both automated metrics and\nhuman evaluation, and successfully evades several automated authorship attacks.",
      "tldr_zh": "本文提出一种无监督文本私有化框架，使用强化学习微调大型语言模型（LLM），以自动重写在线文本，隐藏作者身份，同时平衡 soundness（正确性）、sense（意义）和 privacy（隐私）。该框架针对68k作者的Reddit帖子数据集进行大规模评估，涵盖不同作者简介长度和作者身份检测策略。实验结果表明，该方法维持了高文本质量（经自动指标和人工评估验证），并成功规避多种自动 authorship obfuscation 攻击。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "17 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.10260v1",
      "published_date": "2024-05-16 17:12:18 UTC",
      "updated_date": "2024-05-16 17:12:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:56:08.497981"
    },
    {
      "arxiv_id": "2406.18537v1",
      "title": "AddBiomechanics Dataset: Capturing the Physics of Human Motion at Scale",
      "title_zh": "翻译失败",
      "authors": [
        "Keenon Werling",
        "Janelle Kaneda",
        "Alan Tan",
        "Rishi Agarwal",
        "Six Skov",
        "Tom Van Wouwe",
        "Scott Uhlrich",
        "Nicholas Bianco",
        "Carmichael Ong",
        "Antoine Falisse",
        "Shardul Sapkota",
        "Aidan Chandra",
        "Joshua Carter",
        "Ezio Preatoni",
        "Benjamin Fregly",
        "Jennifer Hicks",
        "Scott Delp",
        "C. Karen Liu"
      ],
      "abstract": "While reconstructing human poses in 3D from inexpensive sensors has advanced\nsignificantly in recent years, quantifying the dynamics of human motion,\nincluding the muscle-generated joint torques and external forces, remains a\nchallenge. Prior attempts to estimate physics from reconstructed human poses\nhave been hampered by a lack of datasets with high-quality pose and force data\nfor a variety of movements. We present the AddBiomechanics Dataset 1.0, which\nincludes physically accurate human dynamics of 273 human subjects, over 70\nhours of motion and force plate data, totaling more than 24 million frames. To\nconstruct this dataset, novel analytical methods were required, which are also\nreported here. We propose a benchmark for estimating human dynamics from motion\nusing this dataset, and present several baseline results. The AddBiomechanics\nDataset is publicly available at\nhttps://addbiomechanics.org/download_data.html.",
      "tldr_zh": "本研究介绍了AddBiomechanics Dataset 1.0，这是一个大规模数据集，旨在解决从3D human poses重建中量化人体运动动态（如joint torques和external forces）的挑战。该数据集包含273个受试者、超过70小时的motion和force plate data，总计超过2400万帧数据，通过新型分析方法构建而成。研究者提出了一个基于此数据集的基准评估人体动态估计，并提供了几个基线结果。该数据集已公开可用，可从https://addbiomechanics.org/download_data.html获取，促进了人体运动物理学研究的进步。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "15 pages, 6 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2406.18537v1",
      "published_date": "2024-05-16 16:57:43 UTC",
      "updated_date": "2024-05-16 16:57:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:56:19.506591"
    },
    {
      "arxiv_id": "2405.10995v2",
      "title": "Higher-order Spatio-temporal Physics-incorporated Graph Neural Network for Multivariate Time Series Imputation",
      "title_zh": "高阶时空物理整合图神经网络用于多变量时间序列插值",
      "authors": [
        "Guojun Liang",
        "Prayag Tiwari",
        "Slawomir Nowaczyk",
        "Stefan Byttner"
      ],
      "abstract": "Exploring the missing values is an essential but challenging issue due to the\ncomplex latent spatio-temporal correlation and dynamic nature of time series.\nOwing to the outstanding performance in dealing with structure learning\npotentials, Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs)\nare often used to capture such complex spatio-temporal features in multivariate\ntime series. However, these data-driven models often fail to capture the\nessential spatio-temporal relationships when significant signal corruption\noccurs. Additionally, calculating the high-order neighbor nodes in these models\nis of high computational complexity. To address these problems, we propose a\nnovel higher-order spatio-temporal physics-incorporated GNN (HSPGNN). Firstly,\nthe dynamic Laplacian matrix can be obtained by the spatial attention\nmechanism. Then, the generic inhomogeneous partial differential equation (PDE)\nof physical dynamic systems is used to construct the dynamic higher-order\nspatio-temporal GNN to obtain the missing time series values. Moreover, we\nestimate the missing impact by Normalizing Flows (NF) to evaluate the\nimportance of each node in the graph for better explainability. Experimental\nresults on four benchmark datasets demonstrate the effectiveness of HSPGNN and\nthe superior performance when combining various order neighbor nodes. Also,\ngraph-like optical flow, dynamic graphs, and missing impact can be obtained\nnaturally by HSPGNN, which provides better dynamic analysis and explanation\nthan traditional data-driven models. Our code is available at\nhttps://github.com/gorgen2020/HSPGNN.",
      "tldr_zh": "该研究针对多变量时间序列中的缺失值问题，提出了一种新型高阶时空物理整合图神经网络（HSPGNN），以处理复杂时空相关性和动态特性。HSPGNN 通过空间注意力机制获取动态 Laplacian matrix，并利用通用的非齐次偏微分方程 (PDE) 构建动态高阶时空 GNN 来填充缺失值，同时采用 Normalizing Flows (NF) 评估节点重要性以提升模型的可解释性。实验在四个基准数据集上证明，该方法在结合各种阶邻居节点时显著优于传统 Graph Neural Networks (GNNs) 和 Recurrent Neural Networks (RNNs)，并自然生成图-like optical flow 和动态图，提供更有效的动态分析和解释。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages, 7 figures, CIKM 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.10995v2",
      "published_date": "2024-05-16 16:35:43 UTC",
      "updated_date": "2024-07-18 13:29:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:56:32.596608"
    },
    {
      "arxiv_id": "2405.10218v1",
      "title": "ENADPool: The Edge-Node Attention-based Differentiable Pooling for Graph Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Zhehan Zhao",
        "Lu Bai",
        "Lixin Cui",
        "Ming Li",
        "Yue Wang",
        "Lixiang Xu",
        "Edwin R. Hancock"
      ],
      "abstract": "Graph Neural Networks (GNNs) are powerful tools for graph classification. One\nimportant operation for GNNs is the downsampling or pooling that can learn\neffective embeddings from the node representations. In this paper, we propose a\nnew hierarchical pooling operation, namely the Edge-Node Attention-based\nDifferentiable Pooling (ENADPool), for GNNs to learn effective graph\nrepresentations. Unlike the classical hierarchical pooling operation that is\nbased on the unclear node assignment and simply computes the averaged feature\nover the nodes of each cluster, the proposed ENADPool not only employs a hard\nclustering strategy to assign each node into an unique cluster, but also\ncompress the node features as well as their edge connectivity strengths into\nthe resulting hierarchical structure based on the attention mechanism after\neach pooling step. As a result, the proposed ENADPool simultaneously identifies\nthe importance of different nodes within each separated cluster and edges\nbetween corresponding clusters, that significantly addresses the shortcomings\nof the uniform edge-node based structure information aggregation arising in the\nclassical hierarchical pooling operation. Moreover, to mitigate the\nover-smoothing problem arising in existing GNNs, we propose a Multi-distance\nGNN (MD-GNN) model associated with the proposed ENADPool operation, allowing\nthe nodes to actively and directly receive the feature information from\nneighbors at different random walk steps. Experiments demonstrate the\neffectiveness of the MD-GNN associated with the proposed ENADPool.",
      "tldr_zh": "本论文提出了一种新的分层池化操作，ENADPool（Edge-Node Attention-based Differentiable Pooling），用于Graph Neural Networks (GNNs)，旨在通过注意力机制优化图表示学习。ENADPool采用硬聚类策略分配节点，并同时压缩节点特征和边连接强度，从而解决传统池化操作中节点分配不清晰和简单平均特征的问题，同时识别集群内节点重要性和集群间边强度。论文进一步引入Multi-distance GNN (MD-GNN)模型，让节点从不同随机游走步骤的邻居直接接收信息，以缓解GNNs的过平滑问题；实验结果证明，该方法在图分类任务上表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.10218v1",
      "published_date": "2024-05-16 16:08:49 UTC",
      "updated_date": "2024-05-16 16:08:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:56:43.976150"
    },
    {
      "arxiv_id": "2405.10216v1",
      "title": "Low-Rank Adaptation of Time Series Foundational Models for Out-of-Domain Modality Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Divij Gupta",
        "Anubhav Bhatti",
        "Suraj Parmar",
        "Chen Dan",
        "Yuwei Liu",
        "Bingjie Shen",
        "San Lee"
      ],
      "abstract": "Low-Rank Adaptation (LoRA) is a widely used technique for fine-tuning large\npre-trained or foundational models across different modalities and tasks.\nHowever, its application to time series data, particularly within foundational\nmodels, remains underexplored. This paper examines the impact of LoRA on\ncontemporary time series foundational models: Lag-Llama, MOIRAI, and Chronos.\nWe demonstrate LoRA's fine-tuning potential for forecasting the vital signs of\nsepsis patients in intensive care units (ICUs), emphasizing the models'\nadaptability to previously unseen, out-of-domain modalities. Integrating LoRA\naims to enhance forecasting performance while reducing inefficiencies\nassociated with fine-tuning large models on limited domain-specific data. Our\nexperiments show that LoRA fine-tuning of time series foundational models\nsignificantly improves forecasting, achieving results comparable to\nstate-of-the-art models trained from scratch on similar modalities. We conduct\ncomprehensive ablation studies to demonstrate the trade-offs between the number\nof tunable parameters and forecasting performance and assess the impact of\nvarying LoRA matrix ranks on model performance.",
      "tldr_zh": "本论文探讨了 Low-Rank Adaptation (LoRA) 在时间序列基础模型上的应用，针对 Lag-Llama、MOIRAI 和 Chronos 模型，旨在实现出域模态的预测任务。研究通过 LoRA 微调这些模型来预测 ICU 患者生命体征，提高了预测性能，同时减少了在有限领域数据上微调大型模型的效率问题。实验结果显示，LoRA 微调的性能可与从零训练的 state-of-the-art 模型相当，并通过消融研究分析了可调参数数量和 LoRA 矩阵秩对模型表现的权衡。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "5 pages, 3 figures. This work has been submitted to the ACM for\n  possible publication",
      "pdf_url": "http://arxiv.org/pdf/2405.10216v1",
      "published_date": "2024-05-16 16:05:33 UTC",
      "updated_date": "2024-05-16 16:05:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:56:56.127574"
    },
    {
      "arxiv_id": "2405.10215v1",
      "title": "SMLP: Symbolic Machine Learning Prover (User Manual)",
      "title_zh": "翻译失败",
      "authors": [
        "Franz Brauße",
        "Zurab Khasidashvili",
        "Konstantin Korovin"
      ],
      "abstract": "SMLP: Symbolic Machine Learning Prover an open source tool for exploration\nand optimization of systems represented by machine learning models. SMLP uses\nsymbolic reasoning for ML model exploration and optimization under verification\nand stability constraints, based on SMT, constraint and NN solvers. In addition\nits exploration methods are guided by probabilistic and statistical methods.\nSMLP is a general purpose tool that requires only data suitable for ML\nmodelling in the csv format (usually samples of the system's input/output).\nSMLP has been applied at Intel for analyzing and optimizing hardware designs at\nthe analog level. Currently SMLP supports NNs, polynomial and tree models, and\nuses SMT solvers for reasoning and optimization at the backend, integration of\nspecialized NN solvers is in progress.",
      "tldr_zh": "SMLP（Symbolic Machine Learning Prover）是一个开源工具，用于探索和优化由机器学习模型表示的系统，仅需 CSV 格式的数据作为输入。工具通过符号推理（基于 SMT solvers、constraint solvers 和 NN solvers）结合 probabilistic and statistical methods，对模型进行验证和稳定性约束下的优化，目前支持 NNs、polynomial 和 tree models。SMLP 已应用于 Intel 的硬件设计分析和优化，并在集成专门的 NN solvers 方面持续开发。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.LO",
        "cs.SC",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "arXiv admin note: text overlap with arXiv:2402.01415",
      "pdf_url": "http://arxiv.org/pdf/2405.10215v1",
      "published_date": "2024-05-16 16:05:21 UTC",
      "updated_date": "2024-05-16 16:05:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:57:07.498716"
    },
    {
      "arxiv_id": "2405.10160v2",
      "title": "PIR: Remote Sensing Image-Text Retrieval with Prior Instruction Representation Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Jiancheng Pan",
        "Muyuan Ma",
        "Qing Ma",
        "Cong Bai",
        "Shengyong Chen"
      ],
      "abstract": "Remote sensing image-text retrieval constitutes a foundational aspect of\nremote sensing interpretation tasks, facilitating the alignment of vision and\nlanguage representations. This paper introduces a prior instruction\nrepresentation (PIR) learning paradigm that draws on prior knowledge to\ninstruct adaptive learning of vision and text representations. Based on PIR, a\ndomain-adapted remote sensing image-text retrieval framework PIR-ITR is\ndesigned to address semantic noise issues in vision-language understanding\ntasks. However, with massive additional data for pre-training the\nvision-language foundation model, remote sensing image-text retrieval is\nfurther developed into an open-domain retrieval task. Continuing with the\nabove, we propose PIR-CLIP, a domain-specific CLIP-based framework for remote\nsensing image-text retrieval, to address semantic noise in remote sensing\nvision-language representations and further improve open-domain retrieval\nperformance. In vision representation, we utilize the prior-guided knowledge of\nthe remote sensing scene recognition by building a belief matrix to select key\nfeatures for reducing the impact of semantic noise. In text representation, we\nuse the previous time step to cyclically activate the current time step to\nenhance text representation capability. A cluster-wise Affiliation Loss (AL) is\nproposed to constrain the inter-classes and to reduce the semantic confusion\nzones in the common subspace. Comprehensive experiments demonstrate that PIR\ncould enhance vision and text representations and outperform the\nstate-of-the-art methods of closed-domain and open-domain retrieval on two\nbenchmark datasets, RSICD and RSITMD.",
      "tldr_zh": "这篇论文提出了 prior instruction representation (PIR) 学习范式，利用先验知识指导遥感图像-文本检索中视觉和文本表示的自适应学习，以解决语义噪声问题。基于 PIR，他们设计了 PIR-ITR 框架并扩展到 PIR-CLIP，这是一个基于 CLIP 的域特定框架，通过构建 belief matrix 选择关键视觉特征、循环激活增强文本表示，以及提出 cluster-wise Affiliation Loss (AL) 来减少语义混淆。实验结果显示，PIR 方法在 RSICD 和 RSITMD 基准数据集上提升了视觉和文本表示性能，并优于现有闭域和开域检索的 state-of-the-art 方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.10160v2",
      "published_date": "2024-05-16 14:53:45 UTC",
      "updated_date": "2024-10-21 03:49:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:57:20.884974"
    },
    {
      "arxiv_id": "2405.10134v1",
      "title": "Towards Consistent and Explainable Motion Prediction using Heterogeneous Graph Attention",
      "title_zh": "翻译失败",
      "authors": [
        "Tobias Demmler",
        "Andreas Tamke",
        "Thao Dang",
        "Karsten Haug",
        "Lars Mikelsons"
      ],
      "abstract": "In autonomous driving, accurately interpreting the movements of other road\nusers and leveraging this knowledge to forecast future trajectories is crucial.\nThis is typically achieved through the integration of map data and tracked\ntrajectories of various agents. Numerous methodologies combine this information\ninto a singular embedding for each agent, which is then utilized to predict\nfuture behavior. However, these approaches have a notable drawback in that they\nmay lose exact location information during the encoding process. The encoding\nstill includes general map information. However, the generation of valid and\nconsistent trajectories is not guaranteed. This can cause the predicted\ntrajectories to stray from the actual lanes. This paper introduces a new\nrefinement module designed to project the predicted trajectories back onto the\nactual map, rectifying these discrepancies and leading towards more consistent\npredictions. This versatile module can be readily incorporated into a wide\nrange of architectures. Additionally, we propose a novel scene encoder that\nhandles all relations between agents and their environment in a single unified\nheterogeneous graph attention network. By analyzing the attention values on the\ndifferent edges in this graph, we can gain unique insights into the neural\nnetwork's inner workings leading towards a more explainable prediction.",
      "tldr_zh": "在自动驾驶领域，现有的轨迹预测方法往往在整合地图数据和代理轨迹时丢失精确位置信息，导致预测结果不一致，如偏离实际车道。  \n本文提出一个新的精炼模块，用于将预测轨迹投影回真实地图，从而纠正这些偏差，并设计了一个统一的异构图注意力(Heterogeneous Graph Attention)网络作为场景编码器，以处理代理与环境之间的所有关系。  \n通过分析该网络的注意力值，本文提升了预测的可解释性，并实现了更一致的轨迹预测，为自动驾驶系统的可靠性和透明度提供了重要改进。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.10134v1",
      "published_date": "2024-05-16 14:31:15 UTC",
      "updated_date": "2024-05-16 14:31:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:57:31.769891"
    },
    {
      "arxiv_id": "2405.10129v1",
      "title": "StyloAI: Distinguishing AI-Generated Content with Stylometric Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Chidimma Opara"
      ],
      "abstract": "The emergence of large language models (LLMs) capable of generating realistic\ntexts and images has sparked ethical concerns across various sectors. In\nresponse, researchers in academia and industry are actively exploring methods\nto distinguish AI-generated content from human-authored material. However, a\ncrucial question remains: What are the unique characteristics of AI-generated\ntext? Addressing this gap, this study proposes StyloAI, a data-driven model\nthat uses 31 stylometric features to identify AI-generated texts by applying a\nRandom Forest classifier on two multi-domain datasets. StyloAI achieves\naccuracy rates of 81% and 98% on the test set of the AuTextification dataset\nand the Education dataset, respectively. This approach surpasses the\nperformance of existing state-of-the-art models and provides valuable insights\ninto the differences between AI-generated and human-authored texts.",
      "tldr_zh": "本文研究了大型语言模型 (LLMs) 生成的文本可能引发的伦理问题，并提出 StyloAI，一种基于 31 个 stylometric features 的数据驱动模型，使用 Random Forest 分类器来区分 AI 生成文本和人类创作文本。在 AuTextification 数据集和 Education 数据集的测试集上，StyloAI 分别实现了 81% 和 98% 的准确率，超过了现有最先进模型。该方法不仅提升了检测性能，还提供了 AI 生成文本与人类文本在风格上的关键差异见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "25th International Conference on Artificial on Artificial\n  Intelligence in Education(AIED 2024)",
      "pdf_url": "http://arxiv.org/pdf/2405.10129v1",
      "published_date": "2024-05-16 14:28:01 UTC",
      "updated_date": "2024-05-16 14:28:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:57:44.330088"
    },
    {
      "arxiv_id": "2405.10128v3",
      "title": "Red Teaming Language Models for Processing Contradictory Dialogues",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaofei Wen",
        "Bangzheng Li",
        "Tenghao Huang",
        "Muhao Chen"
      ],
      "abstract": "Most language models currently available are prone to self-contradiction\nduring dialogues. To mitigate this issue, this study explores a novel\ncontradictory dialogue processing task that aims to detect and modify\ncontradictory statements in a conversation. This task is inspired by research\non context faithfulness and dialogue comprehension, which have demonstrated\nthat the detection and understanding of contradictions often necessitate\ndetailed explanations. We develop a dataset comprising contradictory dialogues,\nin which one side of the conversation contradicts itself. Each dialogue is\naccompanied by an explanatory label that highlights the location and details of\nthe contradiction. With this dataset, we present a Red Teaming framework for\ncontradictory dialogue processing. The framework detects and attempts to\nexplain the dialogue, then modifies the existing contradictory content using\nthe explanation. Our experiments demonstrate that the framework improves the\nability to detect contradictory dialogues and provides valid explanations.\nAdditionally, it showcases distinct capabilities for modifying such dialogues.\nOur study highlights the importance of the logical inconsistency problem in\nconversational AI.",
      "tldr_zh": "本文研究了语言模型在对话中容易出现自相矛盾的问题，提出一个新的任务来检测和修改矛盾语句，并开发了一个包含矛盾对话的数据集，每个对话附带解释性标签以突出矛盾位置和细节。研究引入了Red Teaming框架，该框架通过检测矛盾、提供详细解释，然后使用这些解释修改对话内容，从而提升了模型的处理能力。实验结果显示，该框架显著提高了矛盾对话的检测准确性和修改效果，并强调了逻辑不一致在对话AI中的关键重要性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "20 pages, 5 figures, 11 tables. EMNLP2024 (main)",
      "pdf_url": "http://arxiv.org/pdf/2405.10128v3",
      "published_date": "2024-05-16 14:27:32 UTC",
      "updated_date": "2024-10-05 18:58:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:57:56.969969"
    },
    {
      "arxiv_id": "2405.13034v2",
      "title": "Autonomous Workflow for Multimodal Fine-Grained Training Assistants Towards Mixed Reality",
      "title_zh": "翻译失败",
      "authors": [
        "Jiahuan Pei",
        "Irene Viola",
        "Haochen Huang",
        "Junxiao Wang",
        "Moonisa Ahsan",
        "Fanghua Ye",
        "Jiang Yiming",
        "Yao Sai",
        "Di Wang",
        "Zhumin Chen",
        "Pengjie Ren",
        "Pablo Cesar"
      ],
      "abstract": "Autonomous artificial intelligence (AI) agents have emerged as promising\nprotocols for automatically understanding the language-based environment,\nparticularly with the exponential development of large language models (LLMs).\nHowever, a fine-grained, comprehensive understanding of multimodal environments\nremains under-explored. This work designs an autonomous workflow tailored for\nintegrating AI agents seamlessly into extended reality (XR) applications for\nfine-grained training. We present a demonstration of a multimodal fine-grained\ntraining assistant for LEGO brick assembly in a pilot XR environment.\nSpecifically, we design a cerebral language agent that integrates LLM with\nmemory, planning, and interaction with XR tools and a vision-language agent,\nenabling agents to decide their actions based on past experiences. Furthermore,\nwe introduce LEGO-MRTA, a multimodal fine-grained assembly dialogue dataset\nsynthesized automatically in the workflow served by a commercial LLM. This\ndataset comprises multimodal instruction manuals, conversations, XR responses,\nand vision question answering. Last, we present several prevailing\nopen-resource LLMs as benchmarks, assessing their performance with and without\nfine-tuning on the proposed dataset. We anticipate that the broader impact of\nthis workflow will advance the development of smarter assistants for seamless\nuser interaction in XR environments, fostering research in both AI and HCI\ncommunities.",
      "tldr_zh": "这篇论文提出了一种自主工作流，用于将AI代理无缝集成到扩展现实(XR)应用中，实现多模态细粒度训练，旨在提升对语言和视觉环境的全面理解。工作流包括脑部语言代理(cerebral language agent)，它整合大型语言模型(LLMs)、记忆、规划以及与XR工具和视觉语言代理的交互，并自动合成LEGO-MRTA数据集，该数据集包含多模态指令手册、对话、XR响应和视觉问答。实验评估显示，通过在LEGO积木组装任务上对LLMs进行微调，性能得到显著提升，该工作流有望推动AI和人机交互(HCI)领域中更智能的XR助手发展。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.13034v2",
      "published_date": "2024-05-16 14:20:30 UTC",
      "updated_date": "2024-06-05 21:47:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:58:09.840293"
    },
    {
      "arxiv_id": "2405.10102v1",
      "title": "A novel Reservoir Architecture for Periodic Time Series Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Zhongju Yuan",
        "Geraint Wiggins",
        "Dick Botteldooren"
      ],
      "abstract": "This paper introduces a novel approach to predicting periodic time series\nusing reservoir computing. The model is tailored to deliver precise forecasts\nof rhythms, a crucial aspect for tasks such as generating musical rhythm.\nLeveraging reservoir computing, our proposed method is ultimately oriented\ntowards predicting human perception of rhythm. Our network accurately predicts\nrhythmic signals within the human frequency perception range. The model\narchitecture incorporates primary and intermediate neurons tasked with\ncapturing and transmitting rhythmic information. Two parameter matrices,\ndenoted as c and k, regulate the reservoir's overall dynamics. We propose a\nloss function to adapt c post-training and introduce a dynamic selection (DS)\nmechanism that adjusts $k$ to focus on areas with outstanding contributions.\nExperimental results on a diverse test set showcase accurate predictions,\nfurther improved through real-time tuning of the reservoir via c and k.\nComparative assessments highlight its superior performance compared to\nconventional models.",
      "tldr_zh": "本研究提出了一种新型 Reservoir Architecture，用于预测周期性时间序列，特别针对人类感知频率范围内的节奏信号，如音乐节奏生成。模型基于 reservoir computing，包含 primary and intermediate neurons 以及参数矩阵 c 和 k 来调节动态，其中引入损失函数调整 c，并采用 dynamic selection (DS) 机制动态优化 k，以提升预测准确性。实验结果显示，该模型在多样测试集上实现了精确预测，通过实时调整 c 和 k 进一步改善性能，并显著优于传统模型。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.10102v1",
      "published_date": "2024-05-16 13:55:53 UTC",
      "updated_date": "2024-05-16 13:55:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:58:20.592737"
    },
    {
      "arxiv_id": "2405.10093v2",
      "title": "LaT-PFN: A Joint Embedding Predictive Architecture for In-context Time-series Forecasting",
      "title_zh": "LaT-P",
      "authors": [
        "Stijn Verdenius",
        "Andrea Zerio",
        "Roy L. M. Wang"
      ],
      "abstract": "We introduce LatentTimePFN (LaT-PFN), a foundational Time Series model with a\nstrong embedding space that enables zero-shot forecasting. To achieve this, we\nperform in-context learning in latent space utilizing a novel integration of\nthe Prior-data Fitted Networks (PFN) and Joint Embedding Predictive\nArchitecture (JEPA) frameworks. We leverage the JEPA framework to create a\nprediction-optimized latent representation of the underlying stochastic process\nthat generates time series and combines it with contextual learning, using a\nPFN. Furthermore, we improve on preceding works by utilizing related time\nseries as a context and introducing a normalized abstract time axis. This\nreduces training time and increases the versatility of the model by allowing\nany time granularity and forecast horizon. We show that this results in\nsuperior zero-shot predictions compared to established baselines. We also\ndemonstrate our latent space produces informative embeddings of both individual\ntime steps and fixed-length summaries of entire series. Finally, we observe the\nemergence of multi-step patch embeddings without explicit training, suggesting\nthe model actively learns discrete tokens that encode local structures in the\ndata, analogous to vision transformers.",
      "tldr_zh": "本研究提出了LaT-PFN，一种结合Prior-data Fitted Networks (PFN)和Joint Embedding Predictive Architecture (JEPA)的框架，用于时间序列预测的零样本学习。该模型在潜在空间中进行上下文学习，通过利用相关时间序列作为上下文并引入标准化抽象时间轴，显著减少训练时间并提升模型的通用性，支持任意时间粒度和预测范围。实验结果显示，LaT-PFN在零样本预测中优于基准模型，同时其潜在空间能生成信息丰富的嵌入，包括单个时间步和整个序列的摘要，并观察到类似视觉变压器的多步补丁嵌入自发出现。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML",
        "62, 68",
        "I.2.6"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages plus references and appendix, 2 tables, 11 figures, added\n  seeds, corrections",
      "pdf_url": "http://arxiv.org/pdf/2405.10093v2",
      "published_date": "2024-05-16 13:44:56 UTC",
      "updated_date": "2024-05-22 15:06:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:58:32.593436"
    },
    {
      "arxiv_id": "2405.10084v1",
      "title": "Revisiting Deep Audio-Text Retrieval Through the Lens of Transportation",
      "title_zh": "翻译失败",
      "authors": [
        "Manh Luong",
        "Khai Nguyen",
        "Nhat Ho",
        "Reza Haf",
        "Dinh Phung",
        "Lizhen Qu"
      ],
      "abstract": "The Learning-to-match (LTM) framework proves to be an effective inverse\noptimal transport approach for learning the underlying ground metric between\ntwo sources of data, facilitating subsequent matching. However, the\nconventional LTM framework faces scalability challenges, necessitating the use\nof the entire dataset each time the parameters of the ground metric are\nupdated. In adapting LTM to the deep learning context, we introduce the\nmini-batch Learning-to-match (m-LTM) framework for audio-text retrieval\nproblems. This framework leverages mini-batch subsampling and\nMahalanobis-enhanced family of ground metrics. Moreover, to cope with\nmisaligned training data in practice, we propose a variant using partial\noptimal transport to mitigate the harm of misaligned data pairs in training\ndata. We conduct extensive experiments on audio-text matching problems using\nthree datasets: AudioCaps, Clotho, and ESC-50. Results demonstrate that our\nproposed method is capable of learning rich and expressive joint embedding\nspace, which achieves SOTA performance. Beyond this, the proposed m-LTM\nframework is able to close the modality gap across audio and text embedding,\nwhich surpasses both triplet and contrastive loss in the zero-shot sound event\ndetection task on the ESC-50 dataset. Notably, our strategy of employing\npartial optimal transport with m-LTM demonstrates greater noise tolerance than\ncontrastive loss, especially under varying noise ratios in training data on the\nAudioCaps dataset. Our code is available at\nhttps://github.com/v-manhlt3/m-LTM-Audio-Text-Retrieval",
      "tldr_zh": "这篇论文通过逆最优传输的视角，重新审视深度音频-文本检索问题，引入了 mini-batch Learning-to-match (m-LTM) 框架，以解决传统 Learning-to-match (LTM) 的可扩展性挑战，该框架结合 mini-batch subsampling 和 Mahalanobis-enhanced ground metrics。针对训练数据中的误对齐问题，作者提出使用 partial optimal transport 的变体来减轻负面影响。实验结果显示，m-LTM 在 AudioCaps、Clotho 和 ESC-50 数据集上实现了 SOTA 性能，并在零样本音事件检测任务中缩小了模态间差距，超越 triplet 和 contrastive loss，尤其在噪声数据环境下表现出更高的耐受性。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.10084v1",
      "published_date": "2024-05-16 13:28:10 UTC",
      "updated_date": "2024-05-16 13:28:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:58:46.869348"
    },
    {
      "arxiv_id": "2405.10082v1",
      "title": "An Integrated Framework for Multi-Granular Explanation of Video Summarization",
      "title_zh": "一种多粒度视频摘要解释的整合框架",
      "authors": [
        "Konstantinos Tsigos",
        "Evlampios Apostolidis",
        "Vasileios Mezaris"
      ],
      "abstract": "In this paper, we propose an integrated framework for multi-granular\nexplanation of video summarization. This framework integrates methods for\nproducing explanations both at the fragment level (indicating which video\nfragments influenced the most the decisions of the summarizer) and the more\nfine-grained visual object level (highlighting which visual objects were the\nmost influential for the summarizer). To build this framework, we extend our\nprevious work on this field, by investigating the use of a model-agnostic,\nperturbation-based approach for fragment-level explanation of the video\nsummarization results, and introducing a new method that combines the results\nof video panoptic segmentation with an adaptation of a perturbation-based\nexplanation approach to produce object-level explanations. The performance of\nthe developed framework is evaluated using a state-of-the-art summarization\nmethod and two datasets for benchmarking video summarization. The findings of\nthe conducted quantitative and qualitative evaluations demonstrate the ability\nof our framework to spot the most and least influential fragments and visual\nobjects of the video for the summarizer, and to provide a comprehensive set of\nvisual-based explanations about the output of the summarization process.",
      "tldr_zh": "本论文提出一个整合框架，用于视频摘要(multi-granular)解释，支持片段级和视觉对象级的多粒度分析，以揭示摘要决策的关键影响因素。该框架扩展了先前工作，采用模型无关的perturbation-based approach进行片段级解释，并引入新方法结合视频panoptic segmentation与扰动技术生成对象级解释。在使用状态-of-the-art摘要方法和两个基准数据集进行的定量及定性评估中，该框架成功识别了视频中最具和最不具影响力的片段及对象，提供全面的视觉解释。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Under review",
      "pdf_url": "http://arxiv.org/pdf/2405.10082v1",
      "published_date": "2024-05-16 13:25:36 UTC",
      "updated_date": "2024-05-16 13:25:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:58:57.301727"
    },
    {
      "arxiv_id": "2405.10350v1",
      "title": "Monitizer: Automating Design and Evaluation of Neural Network Monitors",
      "title_zh": "Monitizer: 自动化神经网络监控器的设计与评估",
      "authors": [
        "Muqsit Azeem",
        "Marta Grobelna",
        "Sudeep Kanav",
        "Jan Kretinsky",
        "Stefanie Mohr",
        "Sabine Rieder"
      ],
      "abstract": "The behavior of neural networks (NNs) on previously unseen types of data\n(out-of-distribution or OOD) is typically unpredictable. This can be dangerous\nif the network's output is used for decision-making in a safety-critical\nsystem. Hence, detecting that an input is OOD is crucial for the safe\napplication of the NN. Verification approaches do not scale to practical NNs,\nmaking runtime monitoring more appealing for practical use. While various\nmonitors have been suggested recently, their optimization for a given problem,\nas well as comparison with each other and reproduction of results, remain\nchallenging. We present a tool for users and developers of NN monitors. It\nallows for (i) application of various types of monitors from the literature to\na given input NN, (ii) optimization of the monitor's hyperparameters, and (iii)\nexperimental evaluation and comparison to other approaches. Besides, it\nfacilitates the development of new monitoring approaches. We demonstrate the\ntool's usability on several use cases of different types of users as well as on\na case study comparing different approaches from recent literature.",
      "tldr_zh": "该论文针对神经网络(NNs)在处理未知数据(Out-of-Distribution, OOD)时的不可预测行为提出Monitizer工具，以提升安全关键系统的可靠性。Monitizer自动化了NN监控器的设计和评估，包括应用现有监控器、优化超参数以及进行实验比较。该工具还支持新监控方法的开发，并通过多种用例和案例研究证明了其实用性和有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.LG",
      "comment": "accepted at CAV 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.10350v1",
      "published_date": "2024-05-16 13:19:51 UTC",
      "updated_date": "2024-05-16 13:19:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:59:08.938810"
    },
    {
      "arxiv_id": "2406.00008v2",
      "title": "KnowledgeHub: An end-to-end Tool for Assisted Scientific Discovery",
      "title_zh": "KnowledgeHub：端到端辅助科学发现工具",
      "authors": [
        "Shinnosuke Tanaka",
        "James Barry",
        "Vishnudev Kuruvanthodi",
        "Movina Moses",
        "Maxwell J. Giammona",
        "Nathan Herr",
        "Mohab Elkaref",
        "Geeth De Mel"
      ],
      "abstract": "This paper describes the KnowledgeHub tool, a scientific literature\nInformation Extraction (IE) and Question Answering (QA) pipeline. This is\nachieved by supporting the ingestion of PDF documents that are converted to\ntext and structured representations. An ontology can then be constructed where\na user defines the types of entities and relationships they want to capture. A\nbrowser-based annotation tool enables annotating the contents of the PDF\ndocuments according to the ontology. Named Entity Recognition (NER) and\nRelation Classification (RC) models can be trained on the resulting annotations\nand can be used to annotate the unannotated portion of the documents. A\nknowledge graph is constructed from these entity and relation triples which can\nbe queried to obtain insights from the data. Furthermore, we integrate a suite\nof Large Language Models (LLMs) that can be used for QA and summarisation that\nis grounded in the included documents via a retrieval component. KnowledgeHub\nis a unique tool that supports annotation, IE and QA, which gives the user full\ninsight into the knowledge discovery pipeline.",
      "tldr_zh": "该论文介绍了 KnowledgeHub，一种端到端工具，用于辅助科学发现，通过 Information Extraction (IE) 和 Question Answering (QA) 管道处理科学文献。工具支持从 PDF 文档提取文本和结构化表示，用户可定义本体实体和关系，并使用浏览器-based 注解工具进行标注，从而训练 Named Entity Recognition (NER) 和 Relation Classification (RC) 模型，构建知识图谱以查询洞见。此外，KnowledgeHub 整合 Large Language Models (LLMs) 进行基于检索的 QA 和总结，提供全流程支持，让用户全面掌控知识发现管道。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.DL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.00008v2",
      "published_date": "2024-05-16 13:17:14 UTC",
      "updated_date": "2024-06-17 10:23:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:59:21.153360"
    },
    {
      "arxiv_id": "2405.10075v2",
      "title": "HecVL: Hierarchical Video-Language Pretraining for Zero-shot Surgical Phase Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Kun Yuan",
        "Vinkle Srivastav",
        "Nassir Navab",
        "Nicolas Padoy"
      ],
      "abstract": "Natural language could play an important role in developing generalist\nsurgical models by providing a broad source of supervision from raw texts. This\nflexible form of supervision can enable the model's transferability across\ndatasets and tasks as natural language can be used to reference learned visual\nconcepts or describe new ones. In this work, we present HecVL, a novel\nhierarchical video-language pretraining approach for building a generalist\nsurgical model. Specifically, we construct a hierarchical video-text paired\ndataset by pairing the surgical lecture video with three hierarchical levels of\ntexts: at clip-level, atomic actions using transcribed audio texts; at\nphase-level, conceptual text summaries; and at video-level, overall abstract\ntext of the surgical procedure. Then, we propose a novel fine-to-coarse\ncontrastive learning framework that learns separate embedding spaces for the\nthree video-text hierarchies using a single model. By disentangling embedding\nspaces of different hierarchical levels, the learned multi-modal\nrepresentations encode short-term and long-term surgical concepts in the same\nmodel. Thanks to the injected textual semantics, we demonstrate that the HecVL\napproach can enable zero-shot surgical phase recognition without any human\nannotation. Furthermore, we show that the same HecVL model for surgical phase\nrecognition can be transferred across different surgical procedures and medical\ncenters. The code is available at https://github.com/CAMMA-public/SurgVLP",
      "tldr_zh": "本研究提出 HecVL，一种分层视频-语言预训练方法，旨在通过自然语言监督构建通用外科模型，实现 zero-shot 手术阶段识别。研究者构建了一个分层视频-文本数据集，包括 clip-level 的原子动作文本、phase-level 的概念摘要和 video-level 的整体程序摘要，并采用细到粗的 contrastive learning 框架，使用单个模型学习分离的嵌入空间，以编码短时和长时外科概念。实验结果显示，HecVL 模型无需人类标注即可进行零-shot 手术阶段识别，并展示出跨不同手术程序和医疗中心的转移能力，为高效的外科智能应用提供了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by MICCAI2024",
      "pdf_url": "http://arxiv.org/pdf/2405.10075v2",
      "published_date": "2024-05-16 13:14:43 UTC",
      "updated_date": "2025-03-13 15:27:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:59:32.860684"
    },
    {
      "arxiv_id": "2405.10045v2",
      "title": "Global Benchmark Database",
      "title_zh": "翻译失败",
      "authors": [
        "Markus Iser",
        "Christoph Jabs"
      ],
      "abstract": "This paper presents Global Benchmark Database (GBD), a comprehensive suite of\ntools for provisioning and sustainably maintaining benchmark instances and\ntheir metadata. The availability of benchmark metadata is essential for many\ntasks in empirical research, e.g., for the data-driven compilation of\nbenchmarks, the domain-specific analysis of runtime experiments, or the\ninstance-specific selection of solvers. In this paper, we introduce the data\nmodel of GBD as well as its interfaces and provide examples of how to interact\nwith them. We also demonstrate the integration of custom data sources and\nexplain how to extend GBD with additional problem domains, instance formats and\nfeature extractors.",
      "tldr_zh": "这篇论文介绍了 Global Benchmark Database (GBD)，一个全面的工具套件，用于提供和可持续维护基准实例及其元数据，以支持经验研究中的关键任务，如数据驱动的基准编译、领域特定的运行时实验分析，以及实例特定的求解器选择。GBD 包括数据模型、接口和交互示例，用户可以通过这些功能轻松整合自定义数据源。论文还解释了如何扩展 GBD 以涵盖更多问题领域、实例格式和特征提取器，从而提升其灵活性和实用性。",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.10045v2",
      "published_date": "2024-05-16 12:29:12 UTC",
      "updated_date": "2024-06-27 08:12:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:59:45.312671"
    },
    {
      "arxiv_id": "2405.10040v3",
      "title": "SynthesizRR: Generating Diverse Datasets with Retrieval Augmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Abhishek Divekar",
        "Greg Durrett"
      ],
      "abstract": "It is often desirable to distill the capabilities of large language models\n(LLMs) into smaller student models due to compute and memory constraints. One\nway to do this for classification tasks is via dataset synthesis, which can be\naccomplished by generating examples of each label from the LLM. Prior\napproaches to synthesis use few-shot prompting, which relies on the LLM's\nparametric knowledge to generate usable examples. However, this leads to issues\nof repetition, bias towards popular entities, and stylistic differences from\nhuman text. In this work, we propose Synthesize by Retrieval and Refinement\n(SynthesizRR), which uses retrieval augmentation to introduce variety into the\ndataset synthesis process: as retrieved passages vary, the LLM is seeded with\ndifferent content to generate its examples. We empirically study the synthesis\nof six datasets, covering topic classification, sentiment analysis, tone\ndetection, and humor, requiring complex synthesis strategies. We find that\nSynthesizRR greatly improves lexical and semantic diversity, similarity to\nhuman-written text, and distillation performance, when compared to 32-shot\nprompting and four prior approaches. We release our code to perform all steps\nat https://github.com/amazon-science/synthesizrr",
      "tldr_zh": "本论文提出SynthesizRR方法，通过检索增强（retrieval augmentation）来生成更多样化的数据集，解决传统少样本提示（few-shot prompting）在LLMs能力蒸馏过程中存在的重复、偏见和风格差异问题。该方法通过检索不同段落作为种子内容，让LLMs生成并精炼例子，从而提升数据集的词汇和语义多样性以及与人类文本的相似性。在六种数据集上进行的实验，包括主题分类、情感分析、语气检测和幽默检测，显示SynthesizRR相较于32-shot prompting和四种先前方法，显著提高了模型蒸馏性能。论文还发布了相关代码以供复现。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Published as a main conference paper at EMNLP 2024. Code available at\n  https://github.com/amazon-science/synthesizrr",
      "pdf_url": "http://arxiv.org/pdf/2405.10040v3",
      "published_date": "2024-05-16 12:22:41 UTC",
      "updated_date": "2024-11-13 11:13:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T08:59:58.918151"
    },
    {
      "arxiv_id": "2405.13032v2",
      "title": "Faithful Attention Explainer: Verbalizing Decisions Based on Discriminative Features",
      "title_zh": "翻译失败",
      "authors": [
        "Yao Rong",
        "David Scheerer",
        "Enkelejda Kasneci"
      ],
      "abstract": "In recent years, model explanation methods have been designed to interpret\nmodel decisions faithfully and intuitively so that users can easily understand\nthem. In this paper, we propose a framework, Faithful Attention Explainer\n(FAE), capable of generating faithful textual explanations regarding the\nattended-to features. Towards this goal, we deploy an attention module that\ntakes the visual feature maps from the classifier for sentence generation.\nFurthermore, our method successfully learns the association between features\nand words, which allows a novel attention enforcement module for attention\nexplanation. Our model achieves promising performance in caption quality\nmetrics and a faithful decision-relevance metric on two datasets (CUB and\nACT-X). In addition, we show that FAE can interpret gaze-based human attention,\nas human gaze indicates the discriminative features that humans use for\ndecision-making, demonstrating the potential of deploying human gaze for\nadvanced human-AI interaction.",
      "tldr_zh": "本研究提出了一种框架Faithful Attention Explainer (FAE)，旨在生成忠实的文本解释，通过关注模型的区分性特征来帮助用户理解决策过程。该框架使用attention module从分类器的visual feature maps中生成句子，并通过学习特征与单词的关联以及attention enforcement module来强化解释的准确性。在CUB和ACT-X数据集上，FAE在标题质量指标和决策相关性指标上表现出色，同时能够解释基于gaze的人类注意力，展示了其在高级人机交互中的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.13032v2",
      "published_date": "2024-05-16 12:13:24 UTC",
      "updated_date": "2024-05-27 07:20:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:00:10.169050"
    },
    {
      "arxiv_id": "2405.10027v2",
      "title": "The Real Price of Bandit Information in Multiclass Classification",
      "title_zh": "Bandit 信息在多类分类中的真实代价",
      "authors": [
        "Liad Erez",
        "Alon Cohen",
        "Tomer Koren",
        "Yishay Mansour",
        "Shay Moran"
      ],
      "abstract": "We revisit the classical problem of multiclass classification with bandit\nfeedback (Kakade, Shalev-Shwartz and Tewari, 2008), where each input classifies\nto one of $K$ possible labels and feedback is restricted to whether the\npredicted label is correct or not. Our primary inquiry is with regard to the\ndependency on the number of labels $K$, and whether $T$-step regret bounds in\nthis setting can be improved beyond the $\\smash{\\sqrt{KT}}$ dependence\nexhibited by existing algorithms. Our main contribution is in showing that the\nminimax regret of bandit multiclass is in fact more nuanced, and is of the form\n$\\smash{\\widetilde{\\Theta}\\left(\\min \\left\\{|H| + \\sqrt{T}, \\sqrt{KT \\log |H|}\n\\right\\} \\right) }$, where $H$ is the underlying (finite) hypothesis class. In\nparticular, we present a new bandit classification algorithm that guarantees\nregret $\\smash{\\widetilde{O}(|H|+\\sqrt{T})}$, improving over classical\nalgorithms for moderately-sized hypothesis classes, and give a matching lower\nbound establishing tightness of the upper bounds (up to log-factors) in all\nparameter regimes.",
      "tldr_zh": "这篇论文重新审视了多类分类（multiclass classification）中的 Bandit 反馈问题，探讨了 T 步遗憾界（regret bounds）对标签数量 K 的依赖性，旨在改善现有算法的 √(KT) 界限。研究的主要贡献是证明了最小最大遗憾（minimax regret）的形式为 Õ(min{|H| + √T, √(KT log |H|)})，其中 H 是底层有限假设类（hypothesis class）。作者提出了一种新 Bandit 分类算法，保证了 Õ(|H| + √T) 的遗憾表现，尤其适用于中等大小的假设类，并通过匹配的下界（lower bound）证实了这些界限的紧致性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.10027v2",
      "published_date": "2024-05-16 12:11:09 UTC",
      "updated_date": "2024-06-19 09:20:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:00:24.584019"
    },
    {
      "arxiv_id": "2405.10025v1",
      "title": "Listen Again and Choose the Right Answer: A New Paradigm for Automatic Speech Recognition with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yuchen Hu",
        "Chen Chen",
        "Chengwei Qin",
        "Qiushi Zhu",
        "Eng Siong Chng",
        "Ruizhe Li"
      ],
      "abstract": "Recent advances in large language models (LLMs) have promoted generative\nerror correction (GER) for automatic speech recognition (ASR), which aims to\npredict the ground-truth transcription from the decoded N-best hypotheses.\nThanks to the strong language generation ability of LLMs and rich information\nin the N-best list, GER shows great effectiveness in enhancing ASR results.\nHowever, it still suffers from two limitations: 1) LLMs are unaware of the\nsource speech during GER, which may lead to results that are grammatically\ncorrect but violate the source speech content, 2) N-best hypotheses usually\nonly vary in a few tokens, making it redundant to send all of them for GER,\nwhich could confuse LLM about which tokens to focus on and thus lead to\nincreased miscorrection. In this paper, we propose ClozeGER, a new paradigm for\nASR generative error correction. First, we introduce a multimodal LLM (i.e.,\nSpeechGPT) to receive source speech as extra input to improve the fidelity of\ncorrection output. Then, we reformat GER as a cloze test with logits\ncalibration to remove the input information redundancy and simplify GER with\nclear instructions. Experiments show that ClozeGER achieves a new breakthrough\nover vanilla GER on 9 popular ASR datasets.",
      "tldr_zh": "该研究提出了一种新的自动语音识别（ASR）范式，名为 ClozeGER，以解决传统生成性错误修正（GER）方法的局限性，即大型语言模型（LLMs）忽略源语音导致内容不准确，以及 N-best hypotheses 的冗余信息引起混淆。ClozeGER 通过引入多模态 LLM（如 SpeechGPT）来接收源语音作为额外输入，从而提升修正输出的保真度；同时，将 GER 重新设计为填空测试（cloze test）并应用 logits calibration，以去除输入冗余并简化指令。实验结果显示，ClozeGER 在 9 个热门 ASR 数据集上显著超越了传统的 GER 方法，实现了性能突破。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages, Accepted by ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.10025v1",
      "published_date": "2024-05-16 12:05:45 UTC",
      "updated_date": "2024-05-16 12:05:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:00:35.123192"
    },
    {
      "arxiv_id": "2405.09999v2",
      "title": "Reward Centering",
      "title_zh": "翻译失败",
      "authors": [
        "Abhishek Naik",
        "Yi Wan",
        "Manan Tomar",
        "Richard S. Sutton"
      ],
      "abstract": "We show that discounted methods for solving continuing reinforcement learning\nproblems can perform significantly better if they center their rewards by\nsubtracting out the rewards' empirical average. The improvement is substantial\nat commonly used discount factors and increases further as the discount factor\napproaches one. In addition, we show that if a problem's rewards are shifted by\na constant, then standard methods perform much worse, whereas methods with\nreward centering are unaffected. Estimating the average reward is\nstraightforward in the on-policy setting; we propose a slightly more\nsophisticated method for the off-policy setting. Reward centering is a general\nidea, so we expect almost every reinforcement-learning algorithm to benefit by\nthe addition of reward centering.",
      "tldr_zh": "本文提出了一种奖励中心化(reward centering)技术，通过从奖励中减去其经验平均值，来显著提升折扣强化学习(discounted reinforcement learning)算法在持续任务中的性能，尤其在常用折扣因子下效果明显，且当折扣因子接近1时改进更显著。实验显示，如果奖励被常量偏移，标准方法性能会大幅下降，而奖励中心化方法不受影响；在on-policy设置中，平均奖励估计简单，在off-policy设置中则采用更复杂的估算方法。该技术作为一个通用想法，预计可适用于几乎所有强化学习算法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "In Proceedings of RLC 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.09999v2",
      "published_date": "2024-05-16 11:33:49 UTC",
      "updated_date": "2024-10-30 14:18:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:00:46.861901"
    },
    {
      "arxiv_id": "2405.09990v2",
      "title": "A Comprehensive Evaluation of Histopathology Foundation Models for Ovarian Cancer Subtype Classification",
      "title_zh": "组织病理学基础模型在卵巢癌亚型分类中的全面评估",
      "authors": [
        "Jack Breen",
        "Katie Allen",
        "Kieran Zucker",
        "Lucy Godson",
        "Nicolas M. Orsi",
        "Nishant Ravikumar"
      ],
      "abstract": "Large pretrained transformers are increasingly being developed as generalised\nfoundation models which can underpin powerful task-specific artificial\nintelligence models. Histopathology foundation models show great promise across\nmany tasks, but analyses have typically been limited by arbitrary\nhyperparameters that were not tuned to the specific task. We report the most\nrigorous single-task validation of histopathology foundation models to date,\nspecifically in ovarian cancer morphological subtyping. Attention-based\nmultiple instance learning classifiers were compared using three\nImageNet-pretrained feature extractors and fourteen histopathology foundation\nmodels. The training set consisted of 1864 whole slide images from 434 ovarian\ncarcinoma cases at Leeds Teaching Hospitals NHS Trust. Five-class\nclassification performance was evaluated through five-fold cross-validation,\nand these cross-validation models were ensembled for hold-out testing and\nexternal validation on the Transcanadian Study and OCEAN Challenge datasets.\nThe best-performing model used the H-optimus-0 foundation model, with\nfive-class balanced accuracies of 89%, 97%, and 74% in the test sets.\nNormalisations and augmentations aided the performance of the\nImageNet-pretrained ResNets, but these were still outperformed by 13 of the 14\nfoundation models. Hyperparameter tuning the downstream classifiers improved\nperformance by a median 1.9% balanced accuracy, with many improvements being\nstatistically significant. Histopathology foundation models offer a clear\nbenefit to ovarian cancer subtyping, improving classification performance to a\ndegree where clinical utility is tangible, albeit with an increased\ncomputational burden. Such models could provide a second opinion to\nhistopathologists diagnosing challenging cases and may improve the accuracy,\nobjectivity, and efficiency of pathological diagnoses overall.",
      "tldr_zh": "这篇论文对组织病理学基础模型（histopathology foundation models）在卵巢癌亚型分类中的性能进行了全面评估，比较了14个此类模型与三个ImageNet-pretrained feature extractors，使用注意力机制的多实例学习（multiple instance learning）分类器。研究基于1864张全滑图像的训练集，通过五折交叉验证和外部验证，发现H-optimus-0模型在测试集上取得了最高五类平衡准确率（89%、97%和74%）。超参数调整提高了中位数1.9%的平衡准确率，且基础模型整体优于ImageNet预训练模型，这些发现表明此类模型可作为病理学家的第二意见，提升诊断的准确性、客观性和效率。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.09990v2",
      "published_date": "2024-05-16 11:21:02 UTC",
      "updated_date": "2024-09-09 16:59:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:01:00.258611"
    },
    {
      "arxiv_id": "2405.10992v1",
      "title": "Overcoming Catastrophic Forgetting by Exemplar Selection in Task-oriented Dialogue System",
      "title_zh": "翻译失败",
      "authors": [
        "Chen Chen",
        "Ruizhe Li",
        "Yuchen Hu",
        "Yuanyuan Chen",
        "Chengwei Qin",
        "Qiang Zhang"
      ],
      "abstract": "Intelligent task-oriented dialogue systems (ToDs) are expected to\ncontinuously acquire new knowledge, also known as Continual Learning (CL),\nwhich is crucial to fit ever-changing user needs. However, catastrophic\nforgetting dramatically degrades the model performance in face of a long\nstreamed curriculum. In this paper, we aim to overcome the forgetting problem\nin ToDs and propose a method (HESIT) with hyper-gradient-based exemplar\nstrategy, which samples influential exemplars for periodic retraining. Instead\nof unilaterally observing data or models, HESIT adopts a profound exemplar\nselection strategy that considers the general performance of the trained model\nwhen selecting exemplars for each task domain. Specifically, HESIT analyzes the\ntraining data influence by tracing their hyper-gradient in the optimization\nprocess. Furthermore, HESIT avoids estimating Hessian to make it compatible for\nToDs with a large pre-trained model. Experimental results show that HESIT\neffectively alleviates catastrophic forgetting by exemplar selection, and\nachieves state-of-the-art performance on the largest CL benchmark of ToDs in\nterms of all metrics.",
      "tldr_zh": "本研究针对任务导向对话系统（Task-oriented Dialogue System, ToDs）中的灾难性遗忘（catastrophic forgetting）问题，提出了一种名为 HESIT 的方法，利用基于 hyper-gradient 的样本选择策略来选择影响性样本进行定期重训。HESIT 通过追踪训练数据的 hyper-gradient，考虑模型整体性能来优化样本选择，同时避免估计 Hessian，使其适用于大型预训练模型。实验结果表明，该方法有效缓解了灾难性遗忘，并在 ToDs 的最大持续学习（Continual Learning, CL）基准上达到了所有指标的最先进性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.10992v1",
      "published_date": "2024-05-16 10:54:46 UTC",
      "updated_date": "2024-05-16 10:54:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:01:09.987711"
    },
    {
      "arxiv_id": "2405.09980v1",
      "title": "FinTextQA: A Dataset for Long-form Financial Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Jian Chen",
        "Peilin Zhou",
        "Yining Hua",
        "Yingxin Loh",
        "Kehui Chen",
        "Ziyuan Li",
        "Bing Zhu",
        "Junwei Liang"
      ],
      "abstract": "Accurate evaluation of financial question answering (QA) systems necessitates\na comprehensive dataset encompassing diverse question types and contexts.\nHowever, current financial QA datasets lack scope diversity and question\ncomplexity. This work introduces FinTextQA, a novel dataset for long-form\nquestion answering (LFQA) in finance. FinTextQA comprises 1,262 high-quality,\nsource-attributed QA pairs extracted and selected from finance textbooks and\ngovernment agency websites.Moreover, we developed a Retrieval-Augmented\nGeneration (RAG)-based LFQA system, comprising an embedder, retriever,\nreranker, and generator. A multi-faceted evaluation approach, including human\nranking, automatic metrics, and GPT-4 scoring, was employed to benchmark the\nperformance of different LFQA system configurations under heightened noisy\nconditions. The results indicate that: (1) Among all compared generators,\nBaichuan2-7B competes closely with GPT-3.5-turbo in accuracy score; (2) The\nmost effective system configuration on our dataset involved setting the\nembedder, retriever, reranker, and generator as Ada2, Automated Merged\nRetrieval, Bge-Reranker-Base, and Baichuan2-7B, respectively; (3) models are\nless susceptible to noise after the length of contexts reaching a specific\nthreshold.",
      "tldr_zh": "本研究引入了FinTextQA数据集，用于长形式金融问答(LFQA)，该数据集包含1,262对高质量、来源归属的问答对，从金融教科书和政府网站中提取，以解决现有金融QA数据集在多样性和复杂性方面的不足。研究开发了一个基于Retrieval-Augmented Generation (RAG)的LFQA系统，包括embedder、retriever、reranker和generator，并采用多方面评估方法（如人工排名、自动指标和GPT-4评分）在嘈杂条件下测试不同配置。结果显示，Baichuan2-7B在准确性上与GPT-3.5-turbo不相上下，最优配置为embedder为Ada2、retriever为Automated Merged Retrieval、reranker为Bge-Reranker-Base和generator为Baichuan2-7B，且模型在上下文长度达到特定阈值后，对噪声的敏感性显著降低。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.09980v1",
      "published_date": "2024-05-16 10:53:31 UTC",
      "updated_date": "2024-05-16 10:53:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:01:23.579284"
    },
    {
      "arxiv_id": "2405.09972v1",
      "title": "Predicting Solar Heat Production to Optimize Renewable Energy Usage",
      "title_zh": "预测太阳能热生产以优化可再生能源使用",
      "authors": [
        "Tatiana Boura",
        "Natalia Koliou",
        "George Meramveliotakis",
        "Stasinos Konstantopoulos",
        "George Kosmadakis"
      ],
      "abstract": "Utilizing solar energy to meet space heating and domestic hot water demand is\nvery efficient (in terms of environmental footprint as well as cost), but in\norder to ensure that user demand is entirely covered throughout the year needs\nto be complemented with auxiliary heating systems, typically boilers and heat\npumps. Naturally, the optimal control of such a system depends on an accurate\nprediction of solar thermal production.\n  Experimental testing and physics-based numerical models are used to find a\ncollector's performance curve - the mapping from solar radiation and other\nexternal conditions to heat production - but this curve changes over time once\nthe collector is exposed to outdoor conditions. In order to deploy advanced\ncontrol strategies in small domestic installations, we present an approach that\nuses machine learning to automatically construct and continuously adapt a model\nthat predicts heat production. Our design is driven by the need to (a)\nconstruct and adapt models using supervision that can be extracted from\nlow-cost instrumentation, avoiding extreme accuracy and reliability\nrequirements; and (b) at inference time, use inputs that are typically provided\nin publicly available weather forecasts.\n  Recent developments in attention-based machine learning, as well as careful\nadaptation of the training setup to the specifics of the task, have allowed us\nto design a machine learning-based solution that covers our requirements. We\npresent positive empirical results for the predictive accuracy of our solution,\nand discuss the impact of these results on the end-to-end system.",
      "tldr_zh": "本论文旨在通过准确预测太阳能热生产来优化可再生能源的使用，以确保全年加热和热水需求得到充分覆盖。研究提出了一种基于machine learning的方法，利用attention-based模型自动构建和持续适应性能曲线模型，该方法依赖于低成本仪器获取的监督数据以及公开天气预报的输入。实验结果显示，该方案显著提高了预测准确性，并为太阳能辅助系统的先进控制策略提供了积极影响。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.09972v1",
      "published_date": "2024-05-16 10:32:39 UTC",
      "updated_date": "2024-05-16 10:32:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:01:34.110711"
    },
    {
      "arxiv_id": "2405.19343v1",
      "title": "Luganda Speech Intent Recognition for IoT Applications",
      "title_zh": "Luganda 语音意图识别用于 IoT 应用",
      "authors": [
        "Andrew Katumba",
        "Sudi Murindanyi",
        "John Trevor Kasule",
        "Elvis Mugume"
      ],
      "abstract": "The advent of Internet of Things (IoT) technology has generated massive\ninterest in voice-controlled smart homes. While many voice-controlled smart\nhome systems are designed to understand and support widely spoken languages\nlike English, speakers of low-resource languages like Luganda may need more\nsupport. This research project aimed to develop a Luganda speech intent\nclassification system for IoT applications to integrate local languages into\nsmart home environments. The project uses hardware components such as Raspberry\nPi, Wio Terminal, and ESP32 nodes as microcontrollers. The Raspberry Pi\nprocesses Luganda voice commands, the Wio Terminal is a display device, and the\nESP32 nodes control the IoT devices. The ultimate objective of this work was to\nenable voice control using Luganda, which was accomplished through a natural\nlanguage processing (NLP) model deployed on the Raspberry Pi. The NLP model\nutilized Mel Frequency Cepstral Coefficients (MFCCs) as acoustic features and a\nConvolutional Neural Network (Conv2D) architecture for speech intent\nclassification. A dataset of Luganda voice commands was curated for this\npurpose and this has been made open-source. This work addresses the\nlocalization challenges and linguistic diversity in IoT applications by\nincorporating Luganda voice commands, enabling users to interact with smart\nhome devices without English proficiency, especially in regions where local\nlanguages are predominant.",
      "tldr_zh": "这篇论文针对IoT应用中低资源语言的支持不足，开发了一个卢干达语语音意图识别系统，以整合本地语言到智能家居环境中。该系统利用Raspberry Pi处理语音命令、Wio Terminal作为显示设备，以及ESP32节点控制IoT设备；其核心是基于自然语言处理(NLP)模型，使用Mel Frequency Cepstral Coefficients (MFCC)作为声学特征和Convolutional Neural Network (Conv2D)架构进行意图分类。研究团队创建并开源了一个卢干达语语音命令数据集，解决了语言多样性挑战，使用户无需英语熟练度即可通过本地语言交互，提升了IoT应用的包容性和本地化。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Presented as a conference paper at ICLR 2024/AfricaNLP",
      "pdf_url": "http://arxiv.org/pdf/2405.19343v1",
      "published_date": "2024-05-16 10:14:00 UTC",
      "updated_date": "2024-05-16 10:14:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:01:46.829602"
    },
    {
      "arxiv_id": "2405.09939v2",
      "title": "SciQAG: A Framework for Auto-Generated Science Question Answering Dataset with Fine-grained Evaluation",
      "title_zh": "SciQAG：一种带有细粒度评估的自动生成科学问答数据集框架",
      "authors": [
        "Yuwei Wan",
        "Yixuan Liu",
        "Aswathy Ajith",
        "Clara Grazian",
        "Bram Hoex",
        "Wenjie Zhang",
        "Chunyu Kit",
        "Tong Xie",
        "Ian Foster"
      ],
      "abstract": "We introduce SciQAG, a novel framework for automatically generating\nhigh-quality science question-answer pairs from a large corpus of scientific\nliterature based on large language models (LLMs). SciQAG consists of a QA\ngenerator and a QA evaluator, which work together to extract diverse and\nresearch-level questions and answers from scientific papers. Utilizing this\nframework, we construct a large-scale, high-quality, open-ended science QA\ndataset containing 188,042 QA pairs extracted from 22,743 scientific papers\nacross 24 scientific domains. We also introduce SciQAG-24D, a new benchmark\ntask designed to evaluate the science question-answering ability of LLMs.\nExtensive experiments demonstrate that fine-tuning LLMs on the SciQAG dataset\nsignificantly improves their performance on both open-ended question answering\nand scientific tasks. To foster research and collaboration, we make the\ndatasets, models, and evaluation codes publicly available, contributing to the\nadvancement of science question answering and developing more interpretable and\nreasoning-capable AI systems.",
      "tldr_zh": "我们引入了 SciQAG 框架，利用大型语言模型 (LLMs) 从科学文献自动生成高质量的科学问答对，该框架包括 QA 生成器和 QA 评估器，以确保问答的多样性和研究级别。基于此框架，我们构建了一个大规模开放式 QA 数据集，包含 188,042 个 QA 对，源自 24 个科学领域的 22,743 篇论文，并提出了 SciQAG-24D 基准任务，用于评估 LLMs 的科学问答能力。实验结果显示，在 SciQAG 数据集上微调 LLMs 显著提高了模型在开放式问答和科学任务中的表现。最终，我们公开了数据集、模型和评估代码，以促进相关研究和更具解释性 AI 系统的发展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.09939v2",
      "published_date": "2024-05-16 09:42:37 UTC",
      "updated_date": "2024-07-10 01:25:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:02:00.165519"
    },
    {
      "arxiv_id": "2405.09935v2",
      "title": "DEBATE: Devil's Advocate-Based Assessment and Text Evaluation",
      "title_zh": "翻译失败",
      "authors": [
        "Alex Kim",
        "Keonwoo Kim",
        "Sangwon Yoon"
      ],
      "abstract": "As natural language generation (NLG) models have become prevalent,\nsystematically assessing the quality of machine-generated texts has become\nincreasingly important. Recent studies introduce LLM-based evaluators that\noperate as reference-free metrics, demonstrating their capability to adeptly\nhandle novel tasks. However, these models generally rely on a single-agent\napproach, which, we argue, introduces an inherent limit to their performance.\nThis is because there exist biases in LLM agent's responses, including\npreferences for certain text structure or content. In this work, we propose\nDEBATE, an NLG evaluation framework based on multi-agent scoring system\naugmented with a concept of Devil's Advocate. Within the framework, one agent\nis instructed to criticize other agents' arguments, potentially resolving the\nbias in LLM agent's answers. DEBATE substantially outperforms the previous\nstate-of-the-art methods in two meta-evaluation benchmarks in NLG evaluation,\nSummEval and TopicalChat. We also show that the extensiveness of debates among\nagents and the persona of an agent can influence the performance of evaluators.",
      "tldr_zh": "该研究针对自然语言生成（NLG）模型评估中的偏见问题，提出DEBATE框架，这是一种基于多代理评分系统的文本评估方法，并引入Devil's Advocate概念，让一个代理专门批评其他代理的论点，以减少LLM（大型语言模型）的固有偏见，如对特定文本结构或内容的偏好。相比传统单代理方法，DEBATE在SummEval和TopicalChat两个元评估基准上显著超越现有最先进技术。实验结果显示，代理间的辩论广泛性和代理的角色（如persona）会影响评估性能，从而为更可靠的NLG评估提供新途径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.09935v2",
      "published_date": "2024-05-16 09:41:12 UTC",
      "updated_date": "2024-05-24 01:06:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:02:11.506956"
    },
    {
      "arxiv_id": "2405.09934v1",
      "title": "Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance",
      "title_zh": "使用 Fréchet 领域距离检测数字病理学多实例学习中的领域偏移",
      "authors": [
        "Milda Pocevičiūtė",
        "Gabriel Eilertsen",
        "Stina Garvin",
        "Claes Lundström"
      ],
      "abstract": "Multiple-instance learning (MIL) is an attractive approach for digital\npathology applications as it reduces the costs related to data collection and\nlabelling. However, it is not clear how sensitive MIL is to clinically\nrealistic domain shifts, i.e., differences in data distribution that could\nnegatively affect performance, and if already existing metrics for detecting\ndomain shifts work well with these algorithms. We trained an attention-based\nMIL algorithm to classify whether a whole-slide image of a lymph node contains\nbreast tumour metastases. The algorithm was evaluated on data from a hospital\nin a different country and various subsets of this data that correspond to\ndifferent levels of domain shift. Our contributions include showing that MIL\nfor digital pathology is affected by clinically realistic differences in data,\nevaluating which features from a MIL model are most suitable for detecting\nchanges in performance, and proposing an unsupervised metric named Fr\\'echet\nDomain Distance (FDD) for quantification of domain shifts. Shift measure\nperformance was evaluated through the mean Pearson correlation to change in\nclassification performance, where FDD achieved 0.70 on 10-fold cross-validation\nmodels. The baselines included Deep ensemble, Difference of Confidence, and\nRepresentation shift which resulted in 0.45, -0.29, and 0.56 mean Pearson\ncorrelation, respectively. FDD could be a valuable tool for care providers and\nvendors who need to verify if a MIL system is likely to perform reliably when\nimplemented at a new site, without requiring any additional annotations from\npathologists.",
      "tldr_zh": "本研究探讨了Multiple Instance Learning (MIL) 在数字病理学中的应用及其对领域偏移（domain shift）的敏感性，训练了一个基于注意力的MIL算法来分类淋巴结全滑图像中是否存在乳腺肿瘤转移，并在不同国家医院的数据上进行评估。论文的主要贡献包括证明MIL易受临床现实数据差异影响、评估模型特征以检测性能变化，以及提出一个无监督指标Fréchet Domain Distance (FDD) 来量化领域偏移。结果显示，FDD在10折交叉验证中与分类性能变化的Pearson相关系数达0.70，优于基线方法如Deep ensemble (0.45)、Difference of Confidence (-0.29) 和Representation shift (0.56)。FDD可作为医疗提供者和供应商的工具，在不需额外标注的情况下，验证MIL系统在新站点是否可靠。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.09934v1",
      "published_date": "2024-05-16 09:37:57 UTC",
      "updated_date": "2024-05-16 09:37:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:02:24.751786"
    },
    {
      "arxiv_id": "2405.09933v3",
      "title": "MiniMaxAD: A Lightweight Autoencoder for Feature-Rich Anomaly Detection",
      "title_zh": "MiniMaxAD：一种轻量级自编码器用于特征丰富的异常检测",
      "authors": [
        "Fengjie Wang",
        "Chengming Liu",
        "Lei Shi",
        "Pang Haibo"
      ],
      "abstract": "Previous unsupervised anomaly detection (UAD) methods often struggle to\nhandle the extensive diversity in training sets, particularly when they contain\nstylistically diverse and feature-rich samples, which we categorize as\nfeature-rich anomaly detection datasets (FRADs). This challenge is evident in\napplications such as multi-view and multi-class scenarios. To address this\nchallenge, we developed MiniMaxAD, a lightweight autoencoder designed to\nefficiently compress and memorize extensive information from normal images. Our\nmodel employs a technique that enhances feature diversity, thereby increasing\nthe effective capacity of the network. It also utilizes large kernel\nconvolution to extract highly abstract patterns, which contribute to efficient\nand compact feature embedding. Moreover, we introduce an Adaptive Contraction\nHard Mining Loss (ADCLoss), specifically tailored to FRADs. In our methodology,\nany dataset can be unified under the framework of feature-rich anomaly\ndetection, in a way that the benefits far outweigh the drawbacks. Our approach\nhas achieved state-of-the-art performance in multiple challenging benchmarks.",
      "tldr_zh": "本论文针对无监督异常检测 (UAD) 在处理特征丰富的异常检测数据集 (FRADs) 时存在的多样性挑战，提出了一种轻量级自编码器 MiniMaxAD，用于高效压缩和记忆正常图像信息。MiniMaxAD 通过增强特征多样性、利用大内核卷积提取高度抽象模式，以及引入 Adaptive Contraction Hard Mining Loss (ADCLoss) 来提高网络容量和性能，确保在多视图和多类场景中的适用性。该方法将任何数据集统一到 FRADs 框架下，实现了 state-of-the-art 性能，在多个挑战性基准上表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.09933v3",
      "published_date": "2024-05-16 09:37:54 UTC",
      "updated_date": "2024-09-30 04:17:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:02:36.973992"
    },
    {
      "arxiv_id": "2405.09909v1",
      "title": "A Machine Learning Approach for Simultaneous Demapping of QAM and APSK Constellations",
      "title_zh": "翻译失败",
      "authors": [
        "Arwin Gansekoele",
        "Alexios Balatsoukas-Stimming",
        "Tom Brusse",
        "Mark Hoogendoorn",
        "Sandjai Bhulai",
        "Rob van der Mei"
      ],
      "abstract": "As telecommunication systems evolve to meet increasing demands, integrating\ndeep neural networks (DNNs) has shown promise in enhancing performance.\nHowever, the trade-off between accuracy and flexibility remains challenging\nwhen replacing traditional receivers with DNNs. This paper introduces a novel\nprobabilistic framework that allows a single DNN demapper to demap multiple QAM\nand APSK constellations simultaneously. We also demonstrate that our framework\nallows exploiting hierarchical relationships in families of constellations. The\nconsequence is that we need fewer neural network outputs to encode the same\nfunction without an increase in Bit Error Rate (BER). Our simulation results\nconfirm that our approach approaches the optimal demodulation error bound under\nan Additive White Gaussian Noise (AWGN) channel for multiple constellations.\nThereby, we address multiple important issues in making DNNs flexible enough\nfor practical use as receivers.",
      "tldr_zh": "本研究提出了一种机器学习方法，用于同时解映射 QAM 和 APSK 星座图，采用一个单一的 DNN 解映射器，通过新型概率框架提升电信系统的性能。框架利用星座图家族中的层次关系，减少神经网络输出数量，同时保持 Bit Error Rate (BER) 不变。模拟结果显示，该方法在 Additive White Gaussian Noise (AWGN) 信道下接近最优解调错误边界，从而解决 DNNs 作为接收器的灵活性问题，并为实际应用提供更高效的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "math.IT"
      ],
      "primary_category": "cs.LG",
      "comment": "To appear in the ICMLCN 2024 proceedings",
      "pdf_url": "http://arxiv.org/pdf/2405.09909v1",
      "published_date": "2024-05-16 08:57:34 UTC",
      "updated_date": "2024-05-16 08:57:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:02:47.249569"
    },
    {
      "arxiv_id": "2405.10991v1",
      "title": "Relative Counterfactual Contrastive Learning for Mitigating Pretrained Stance Bias in Stance Detection",
      "title_zh": "用于缓解立场检测中预训练立场偏差的相对反事实对比学习",
      "authors": [
        "Jiarui Zhang",
        "Shaojuan Wu",
        "Xiaowang Zhang",
        "Zhiyong Feng"
      ],
      "abstract": "Stance detection classifies stance relations (namely, Favor, Against, or\nNeither) between comments and targets. Pretrained language models (PLMs) are\nwidely used to mine the stance relation to improve the performance of stance\ndetection through pretrained knowledge. However, PLMs also embed ``bad''\npretrained knowledge concerning stance into the extracted stance relation\nsemantics, resulting in pretrained stance bias. It is not trivial to measure\npretrained stance bias due to its weak quantifiability. In this paper, we\npropose Relative Counterfactual Contrastive Learning (RCCL), in which\npretrained stance bias is mitigated as relative stance bias instead of absolute\nstance bias to overtake the difficulty of measuring bias. Firstly, we present a\nnew structural causal model for characterizing complicated relationships among\ncontext, PLMs and stance relations to locate pretrained stance bias. Then,\nbased on masked language model prediction, we present a target-aware relative\nstance sample generation method for obtaining relative bias. Finally, we use\ncontrastive learning based on counterfactual theory to mitigate pretrained\nstance bias and preserve context stance relation. Experiments show that the\nproposed method is superior to stance detection and debiasing baselines.",
      "tldr_zh": "本研究针对姿态检测（stance detection）中预训练语言模型（PLMs）引入的预训练姿态偏差（pretrained stance bias）问题，提出了一种Relative Counterfactual Contrastive Learning (RCCL)方法，以相对偏差形式缓解偏差挑战。研究构建了一个新的结构化因果模型（structural causal model）来定位偏差，并通过基于masked language model prediction的目标感知样本生成技术，创建相对偏差样本。最终，利用基于反事实理论的对比学习（contrastive learning）减少偏差，同时保留上下文姿态关系；实验结果表明，该方法在姿态检测和去偏差任务上优于现有基线。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.10991v1",
      "published_date": "2024-05-16 08:57:00 UTC",
      "updated_date": "2024-05-16 08:57:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:02:59.276401"
    },
    {
      "arxiv_id": "2405.09902v1",
      "title": "Unveiling the Potential: Harnessing Deep Metric Learning to Circumvent Video Streaming Encryption",
      "title_zh": "翻译失败",
      "authors": [
        "Arwin Gansekoele",
        "Tycho Bot",
        "Rob van der Mei",
        "Sandjai Bhulai",
        "Mark Hoogendoorn"
      ],
      "abstract": "Encryption on the internet with the shift to HTTPS has been an important step\nto improve the privacy of internet users. However, there is an increasing body\nof work about extracting information from encrypted internet traffic without\nhaving to decrypt it. Such attacks bypass security guarantees assumed to be\ngiven by HTTPS and thus need to be understood. Prior works showed that the\nvariable bitrates of video streams are sufficient to identify which video\nsomeone is watching. These works generally have to make trade-offs in aspects\nsuch as accuracy, scalability, robustness, etc. These trade-offs complicate the\npractical use of these attacks. To that end, we propose a deep metric learning\nframework based on the triplet loss method. Through this framework, we achieve\nrobust, generalisable, scalable and transferable encrypted video stream\ndetection. First, the triplet loss is better able to deal with video streams\nnot seen during training. Second, our approach can accurately classify videos\nnot seen during training. Third, we show that our method scales well to a\ndataset of over 1000 videos. Finally, we show that a model trained on video\nstreams over Chrome can also classify streams over Firefox. Our results suggest\nthat this side-channel attack is more broadly applicable than originally\nthought. We provide our code alongside a diverse and up-to-date dataset for\nfuture research.",
      "tldr_zh": "本研究探讨了尽管 HTTPS 加密提升了互联网隐私，但仍可从加密视频流中提取信息的问题。作者提出了一种基于 deep metric learning 和 triplet loss 的框架，用于实现鲁棒、可泛化、可扩展和可转移的加密视频流检测。该框架能够准确识别训练中未见的视频，并在超过1000个视频的数据集中表现出色，同时模型在 Chrome 上训练后也能应用于 Firefox。实验结果表明，这种侧信道攻击的适用性比预期更广泛，并提供了代码和多样化数据集以支持未来研究。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "comment": "Published in the WI-IAT 2023 proceedings",
      "pdf_url": "http://arxiv.org/pdf/2405.09902v1",
      "published_date": "2024-05-16 08:49:05 UTC",
      "updated_date": "2024-05-16 08:49:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:03:11.360159"
    },
    {
      "arxiv_id": "2405.09901v1",
      "title": "Whole-Song Hierarchical Generation of Symbolic Music Using Cascaded Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ziyu Wang",
        "Lejun Min",
        "Gus Xia"
      ],
      "abstract": "Recent deep music generation studies have put much emphasis on long-term\ngeneration with structures. However, we are yet to see high-quality,\nwell-structured whole-song generation. In this paper, we make the first attempt\nto model a full music piece under the realization of compositional hierarchy.\nWith a focus on symbolic representations of pop songs, we define a hierarchical\nlanguage, in which each level of hierarchy focuses on the semantics and context\ndependency at a certain music scope. The high-level languages reveal whole-song\nform, phrase, and cadence, whereas the low-level languages focus on notes,\nchords, and their local patterns. A cascaded diffusion model is trained to\nmodel the hierarchical language, where each level is conditioned on its upper\nlevels. Experiments and analysis show that our model is capable of generating\nfull-piece music with recognizable global verse-chorus structure and cadences,\nand the music quality is higher than the baselines. Additionally, we show that\nthe proposed model is controllable in a flexible way. By sampling from the\ninterpretable hierarchical languages or adjusting pre-trained external\nrepresentations, users can control the music flow via various features such as\nphrase harmonic structures, rhythmic patterns, and accompaniment texture.",
      "tldr_zh": "这篇论文提出了一种使用级联扩散模型(cascaded diffusion models)生成符号音乐(symbolic music)的分层方法，首次尝试在作曲层次上建模全曲音乐，以实现高质量的结构化输出。  \n他们定义了分层语言(hierarchical language)，高层关注整体歌曲形式、短语和节奏，低层则处理笔记、和弦及其局部模式，并通过每个级别基于上层进行条件生成。  \n实验结果表明，该模型能生成具有可识别的verse-chorus结构和节奏的完整音乐，比基线模型质量更高。  \n此外，该模型具有灵活的可控性，用户可以通过采样分层语言或调整预训练表示来控制音乐特征，如短语和声结构、节奏模式和伴奏纹理。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS",
        "68Txx"
      ],
      "primary_category": "cs.SD",
      "comment": "Proceedings of the International Conference on Learning\n  Representations (ICLR 2024)",
      "pdf_url": "http://arxiv.org/pdf/2405.09901v1",
      "published_date": "2024-05-16 08:48:23 UTC",
      "updated_date": "2024-05-16 08:48:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:03:24.896387"
    },
    {
      "arxiv_id": "2405.13030v1",
      "title": "Crowdsourcing with Enhanced Data Quality Assurance: An Efficient Approach to Mitigate Resource Scarcity Challenges in Training Large Language Models for Healthcare",
      "title_zh": "翻译失败",
      "authors": [
        "P. Barai",
        "G. Leroy",
        "P. Bisht",
        "J. M. Rothman",
        "S. Lee",
        "J. Andrews",
        "S. A. Rice",
        "A. Ahmed"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated immense potential in\nartificial intelligence across various domains, including healthcare. However,\ntheir efficacy is hindered by the need for high-quality labeled data, which is\noften expensive and time-consuming to create, particularly in low-resource\ndomains like healthcare. To address these challenges, we propose a\ncrowdsourcing (CS) framework enriched with quality control measures at the\npre-, real-time-, and post-data gathering stages. Our study evaluated the\neffectiveness of enhancing data quality through its impact on LLMs (Bio-BERT)\nfor predicting autism-related symptoms. The results show that real-time quality\ncontrol improves data quality by 19 percent compared to pre-quality control.\nFine-tuning Bio-BERT using crowdsourced data generally increased recall\ncompared to the Bio-BERT baseline but lowered precision. Our findings\nhighlighted the potential of crowdsourcing and quality control in\nresource-constrained environments and offered insights into optimizing\nhealthcare LLMs for informed decision-making and improved patient care.",
      "tldr_zh": "本研究提出了一种增强数据质量保障的众包（crowdsourcing, CS）框架，以缓解在医疗领域训练 Large Language Models (LLMs) 时的高质量标注数据短缺问题。该框架在预、实时和后数据收集阶段实施质量控制措施，并通过实验评估其对 Bio-BERT 模型预测自闭症相关症状的影响。结果显示，实时质量控制比预质量控制提高了19%的数据质量，而使用众包数据微调 Bio-BERT 提升了 recall 但降低了 precision。这些发现强调了众包和质量控制在资源受限环境中的潜力，并为优化医疗 LLMs 以支持决策和患者护理提供了重要见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Published in AMIA Summit, Boston, 2024.\n  https://knowledge.amia.org/Info2024/pdf/Info2024a022/Info2024fl021",
      "pdf_url": "http://arxiv.org/pdf/2405.13030v1",
      "published_date": "2024-05-16 08:29:00 UTC",
      "updated_date": "2024-05-16 08:29:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:03:38.004104"
    },
    {
      "arxiv_id": "2405.09893v1",
      "title": "\"Hunt Takes Hare\": Theming Games Through Game-Word Vector Translation",
      "title_zh": "翻译失败",
      "authors": [
        "Rabii Younès",
        "Cook Michael"
      ],
      "abstract": "A game's theme is an important part of its design -- it conveys narrative\ninformation, rhetorical messages, helps the player intuit strategies, aids in\ntutorialisation and more. Thematic elements of games are notoriously difficult\nfor AI systems to understand and manipulate, however, and often rely on large\namounts of hand-written interpretations and knowledge. In this paper we present\na technique which connects game embeddings, a recent method for modelling game\ndynamics from log data, and word embeddings, which models semantic information\nabout language. We explain two different approaches for using game embeddings\nin this way, and show evidence that game embeddings enhance the linguistic\ntranslations of game concepts from one theme to another, opening up exciting\nnew possibilities for reasoning about the thematic elements of games in the\nfuture.",
      "tldr_zh": "本论文探讨了游戏主题在设计中的重要性，包括传达叙事信息、辅助策略直觉和教程化等，但AI系统在理解和操作主题方面面临挑战，往往依赖手动知识。研究提出了一种将game embeddings（从游戏日志数据建模游戏动态）和word embeddings（建模语言语义）相结合的技术，通过两种不同方法实现游戏概念从一个主题到另一个主题的语言翻译。实验结果显示，这种方法显著提升了翻译的准确性，为未来游戏主题元素的推理和创新提供了新可能性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, PCG Workshop at FDG 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.09893v1",
      "published_date": "2024-05-16 08:19:11 UTC",
      "updated_date": "2024-05-16 08:19:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:03:46.927899"
    },
    {
      "arxiv_id": "2405.10989v1",
      "title": "Learnable Privacy Neurons Localization in Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ruizhe Chen",
        "Tianxiang Hu",
        "Yang Feng",
        "Zuozhu Liu"
      ],
      "abstract": "Concerns regarding Large Language Models (LLMs) to memorize and disclose\nprivate information, particularly Personally Identifiable Information (PII),\nbecome prominent within the community. Many efforts have been made to mitigate\nthe privacy risks. However, the mechanism through which LLMs memorize PII\nremains poorly understood. To bridge this gap, we introduce a pioneering method\nfor pinpointing PII-sensitive neurons (privacy neurons) within LLMs. Our method\nemploys learnable binary weight masks to localize specific neurons that account\nfor the memorization of PII in LLMs through adversarial training. Our\ninvestigations discover that PII is memorized by a small subset of neurons\nacross all layers, which shows the property of PII specificity. Furthermore, we\npropose to validate the potential in PII risk mitigation by deactivating the\nlocalized privacy neurons. Both quantitative and qualitative experiments\ndemonstrate the effectiveness of our neuron localization algorithm.",
      "tldr_zh": "本文针对大型语言模型 (LLMs) 记忆并泄露 Personally Identifiable Information (PII) 的隐私风险，提出了一种创新方法来定位 PII 敏感神经元（privacy neurons）。该方法通过 learnable binary weight masks 和对抗训练，精确识别出负责 PII 记忆的少量神经元，这些神经元在所有层中显示出 PII specificity 属性。实验结果表明，停用这些 privacy neurons 可以有效缓解隐私风险，并在定量和定性评估中证明了算法的可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "ACL 2024 main conference",
      "pdf_url": "http://arxiv.org/pdf/2405.10989v1",
      "published_date": "2024-05-16 08:11:08 UTC",
      "updated_date": "2024-05-16 08:11:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:03:59.144791"
    },
    {
      "arxiv_id": "2405.09886v1",
      "title": "MTLComb: multi-task learning combining regression and classification tasks for joint feature selection",
      "title_zh": "翻译失败",
      "authors": [
        "Han Cao",
        "Sivanesan Rajan",
        "Bianka Hahn",
        "Ersoy Kocak",
        "Daniel Durstewitz",
        "Emanuel Schwarz",
        "Verena Schneider-Lindner"
      ],
      "abstract": "Multi-task learning (MTL) is a learning paradigm that enables the\nsimultaneous training of multiple communicating algorithms. Although MTL has\nbeen successfully applied to ether regression or classification tasks alone,\nincorporating mixed types of tasks into a unified MTL framework remains\nchallenging, primarily due to variations in the magnitudes of losses associated\nwith different tasks. This challenge, particularly evident in MTL applications\nwith joint feature selection, often results in biased selections. To overcome\nthis obstacle, we propose a provable loss weighting scheme that analytically\ndetermines the optimal weights for balancing regression and classification\ntasks. This scheme significantly mitigates the otherwise biased feature\nselection. Building upon this scheme, we introduce MTLComb, an MTL algorithm\nand software package encompassing optimization procedures, training protocols,\nand hyperparameter estimation procedures. MTLComb is designed for learning\nshared predictors among tasks of mixed types. To showcase the efficacy of\nMTLComb, we conduct tests on both simulated data and biomedical studies\npertaining to sepsis and schizophrenia.",
      "tldr_zh": "该论文解决了多任务学习(MTL)中将回归和分类任务结合时，由于损失幅度差异导致的特征选择偏差问题，提出了一种可证明的损失加权方案，通过分析性方法确定最佳权重来平衡任务。基于此方案，作者引入了MTLComb算法和软件包，包括优化过程、训练协议及超参数估计，用于学习混合任务的共享预测器。在模拟数据和生物医学研究（如败血症和精神分裂症）上的测试中，MTLComb显著提高了特征选择的准确性和有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM",
        "J.3; I.2.6"
      ],
      "primary_category": "cs.LG",
      "comment": "33 pages, 3 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2405.09886v1",
      "published_date": "2024-05-16 08:07:25 UTC",
      "updated_date": "2024-05-16 08:07:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:04:10.797010"
    },
    {
      "arxiv_id": "2405.09882v1",
      "title": "DiffAM: Diffusion-based Adversarial Makeup Transfer for Facial Privacy Protection",
      "title_zh": "翻译失败",
      "authors": [
        "Yuhao Sun",
        "Lingyun Yu",
        "Hongtao Xie",
        "Jiaming Li",
        "Yongdong Zhang"
      ],
      "abstract": "With the rapid development of face recognition (FR) systems, the privacy of\nface images on social media is facing severe challenges due to the abuse of\nunauthorized FR systems. Some studies utilize adversarial attack techniques to\ndefend against malicious FR systems by generating adversarial examples.\nHowever, the generated adversarial examples, i.e., the protected face images,\ntend to suffer from subpar visual quality and low transferability. In this\npaper, we propose a novel face protection approach, dubbed DiffAM, which\nleverages the powerful generative ability of diffusion models to generate\nhigh-quality protected face images with adversarial makeup transferred from\nreference images. To be specific, we first introduce a makeup removal module to\ngenerate non-makeup images utilizing a fine-tuned diffusion model with guidance\nof textual prompts in CLIP space. As the inverse process of makeup transfer,\nmakeup removal can make it easier to establish the deterministic relationship\nbetween makeup domain and non-makeup domain regardless of elaborate text\nprompts. Then, with this relationship, a CLIP-based makeup loss along with an\nensemble attack strategy is introduced to jointly guide the direction of\nadversarial makeup domain, achieving the generation of protected face images\nwith natural-looking makeup and high black-box transferability. Extensive\nexperiments demonstrate that DiffAM achieves higher visual quality and attack\nsuccess rates with a gain of 12.98% under black-box setting compared with the\nstate of the arts. The code will be available at\nhttps://github.com/HansSunY/DiffAM.",
      "tldr_zh": "该论文提出DiffAM，一种基于diffusion models的对抗化妆转移方法，用于保护社交媒体面部图像隐私免受unauthorized FR systems的滥用。DiffAM首先利用微调的diffusion模型和CLIP空间的文本提示生成无化妆图像，然后通过CLIP-based makeup loss和ensemble attack策略引导对抗化妆域，生成视觉质量高的保护图像。实验结果显示，DiffAM在black-box setting下比现有方法提高12.98%的攻击成功率和视觉质量。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "16 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.09882v1",
      "published_date": "2024-05-16 08:05:36 UTC",
      "updated_date": "2024-05-16 08:05:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:04:23.490293"
    },
    {
      "arxiv_id": "2405.09879v1",
      "title": "Generative Unlearning for Any Identity",
      "title_zh": "针对任何身份的生成式遗忘",
      "authors": [
        "Juwon Seo",
        "Sung-Hoon Lee",
        "Tae-Young Lee",
        "Seungjun Moon",
        "Gyeong-Moon Park"
      ],
      "abstract": "Recent advances in generative models trained on large-scale datasets have\nmade it possible to synthesize high-quality samples across various domains.\nMoreover, the emergence of strong inversion networks enables not only a\nreconstruction of real-world images but also the modification of attributes\nthrough various editing methods. However, in certain domains related to privacy\nissues, e.g., human faces, advanced generative models along with strong\ninversion methods can lead to potential misuses. In this paper, we propose an\nessential yet under-explored task called generative identity unlearning, which\nsteers the model not to generate an image of a specific identity. In the\ngenerative identity unlearning, we target the following objectives: (i)\npreventing the generation of images with a certain identity, and (ii)\npreserving the overall quality of the generative model. To satisfy these goals,\nwe propose a novel framework, Generative Unlearning for Any Identity (GUIDE),\nwhich prevents the reconstruction of a specific identity by unlearning the\ngenerator with only a single image. GUIDE consists of two parts: (i) finding a\ntarget point for optimization that un-identifies the source latent code and\n(ii) novel loss functions that facilitate the unlearning procedure while less\naffecting the learned distribution. Our extensive experiments demonstrate that\nour proposed method achieves state-of-the-art performance in the generative\nmachine unlearning task. The code is available at\nhttps://github.com/KHU-AGI/GUIDE.",
      "tldr_zh": "这篇论文针对生成模型（generative models）在隐私问题上的风险，提出了一种新的任务：Generative Identity Unlearning，以防止模型生成特定身份的图像，同时保持整体生成质量。作者引入了 GUIDE 框架，该框架仅需单一图像，通过优化一个目标点（un-identifies the source latent code）和新型损失函数来实现 unlearning 过程，从而减少对学到分布的影响。实验结果表明，GUIDE 在 generative machine unlearning 任务中达到了 state-of-the-art 性能，并提供了开源代码。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "15 pages, 17 figures, 10 tables, CVPR 2024 Poster",
      "pdf_url": "http://arxiv.org/pdf/2405.09879v1",
      "published_date": "2024-05-16 08:00:55 UTC",
      "updated_date": "2024-05-16 08:00:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:04:34.739871"
    },
    {
      "arxiv_id": "2405.09878v2",
      "title": "Hyperplane Arrangements and Fixed Points in Iterated PWL Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Hans-Peter Beise"
      ],
      "abstract": "We leverage the framework of hyperplane arrangements to analyze potential\nregions of (stable) fixed points. We provide an upper bound on the number of\nfixed points for multi-layer neural networks equipped with piecewise linear\n(PWL) activation functions with arbitrary many linear pieces. The theoretical\noptimality of the exponential growth in the number of layers of the latter\nbound is shown. Specifically, we also derive a sharper upper bound on the\nnumber of stable fixed points for one-hidden-layer networks with hard tanh\nactivation.",
      "tldr_zh": "本研究利用 hyperplane arrangements 框架分析多层神经网络中的固定点，特别是配备 piecewise linear (PWL) 激活函数的网络。论文提供了一个固定点的上界，该上界随着层数的增加呈指数增长，并证明了其理论最优性。具体地，对于单隐藏层网络使用 hard tanh 激活函数，研究者推导了一个更精确的稳定固定点上界。这些结果有助于深化对迭代神经网络行为的可预测性分析。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML",
        "68T07",
        "G.0"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.09878v2",
      "published_date": "2024-05-16 07:57:31 UTC",
      "updated_date": "2024-07-14 18:01:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:04:46.385730"
    },
    {
      "arxiv_id": "2405.09875v2",
      "title": "A Farewell to Harms: Risk Management for Medical Devices via the Riskman Ontology & Shapes",
      "title_zh": "翻译失败",
      "authors": [
        "Piotr Gorczyca",
        "Dörthe Arndt",
        "Martin Diller",
        "Pascal Kettmann",
        "Stephan Mennicke",
        "Hannes Strass"
      ],
      "abstract": "We introduce the Riskman ontology & shapes for representing and analysing\ninformation about risk management for medical devices. Risk management is\nconcerned with taking necessary precautions so a medical device does not cause\nharms for users or the environment. To date, risk management documentation is\nsubmitted to notified bodies (for certification) in the form of semi-structured\nnatural language text. We propose to use classes from the Riskman ontology to\nlogically model risk management documentation and to use the included SHACL\nconstraints to check for syntactic completeness and conformity to relevant\nstandards. In particular, the ontology is modelled after ISO 14971 and the\nrecently published VDE Spec 90025. Our proposed methodology has the potential\nto save many person-hours for both manufacturers (when creating risk management\ndocumentation) as well as notified bodies (when assessing submitted\napplications for certification), and thus offers considerable benefits for\nhealthcare and, by extension, society as a whole.",
      "tldr_zh": "本研究引入了 Riskman ontology 和 shapes，用于表示和分析医疗设备风险管理信息，旨在防止设备对用户或环境造成危害。论文提出使用 Riskman ontology 中的类来逻辑建模风险管理文档，并应用 SHACL 约束检查文档的语法完整性和符合 ISO 14971 及 VDE Spec 90025 等标准。这种方法可为制造商和通知机构节省大量工作时间，从而提高医疗设备认证效率，并为医疗保健和社会整体带来显著益处。",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.09875v2",
      "published_date": "2024-05-16 07:53:07 UTC",
      "updated_date": "2024-05-22 12:46:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:04:59.357346"
    },
    {
      "arxiv_id": "2405.09863v3",
      "title": "Box-Free Model Watermarks Are Prone to Black-Box Removal Attacks",
      "title_zh": "无盒模型水印易受黑盒移除攻击",
      "authors": [
        "Haonan An",
        "Guang Hua",
        "Zhiping Lin",
        "Yuguang Fang"
      ],
      "abstract": "Box-free model watermarking is an emerging technique to safeguard the\nintellectual property of deep learning models, particularly those for low-level\nimage processing tasks. Existing works have verified and improved its\neffectiveness in several aspects. However, in this paper, we reveal that\nbox-free model watermarking is prone to removal attacks, even under the\nreal-world threat model such that the protected model and the watermark\nextractor are in black boxes. Under this setting, we carry out three studies.\n1) We develop an extractor-gradient-guided (EGG) remover and show its\neffectiveness when the extractor uses ReLU activation only. 2) More generally,\nfor an unknown extractor, we leverage adversarial attacks and design the EGG\nremover based on the estimated gradients. 3) Under the most stringent condition\nthat the extractor is inaccessible, we design a transferable remover based on a\nset of private proxy models. In all cases, the proposed removers can\nsuccessfully remove embedded watermarks while preserving the quality of the\nprocessed images, and we also demonstrate that the EGG remover can even replace\nthe watermarks. Extensive experimental results verify the effectiveness and\ngeneralizability of the proposed attacks, revealing the vulnerabilities of the\nexisting box-free methods and calling for further research.",
      "tldr_zh": "该论文揭示了 box-free model watermarking 技术在保护深度学习模型知识产权方面的脆弱性，特别是易受 black-box removal attacks。研究者开发了 extractor-gradient-guided (EGG) remover 等方法，包括基于 ReLU activation 的版本、利用 adversarial attacks 估计 gradients 的泛化版本，以及基于 private proxy models 的 transferable remover，这些攻击能在 black-box 条件下成功移除水marks，同时保持图像处理质量，甚至能替换水marks。实验结果验证了这些攻击的有效性和泛化性，暴露了现有 box-free 方法的漏洞，并呼吁进一步研究以提升安全性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.09863v3",
      "published_date": "2024-05-16 07:41:54 UTC",
      "updated_date": "2024-08-20 06:37:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:05:11.798028"
    },
    {
      "arxiv_id": "2405.09857v1",
      "title": "IGOT: Information Gain Optimized Tokenizer on Domain Adaptive Pretraining",
      "title_zh": "翻译失败",
      "authors": [
        "Dawei Feng",
        "Yihai Zhang",
        "Zhixuan Xu"
      ],
      "abstract": "Pretrained Large Language Models (LLM) such as ChatGPT, Claude, etc. have\ndemonstrated strong capabilities in various fields of natural language\ngeneration. However, there are still many problems when using LLM in\nspecialized domain-specific fields. When using generative AI to process\ndownstream tasks, a common approach is to add new knowledge (e.g., private\ndomain knowledge, cutting-edge information) to a pretrained model through\ncontinued training or fine-tuning. However, whether there is a universal\nparadigm for domain adaptation training is still an open question. In this\narticle, we proposed Information Gain Optimized Tokenizer (IGOT), which\nanalyzes the special token set of downstream tasks, constructs a new subset\nusing heuristic function $\\phi$ with the special token and its information\ngain, to build new domain-specific tokenizer, and continues pretraining on the\ndownstream task data. We explored the many positive effects of this method's\ncustomized tokenizer on domain-adaptive pretraining and verified this method\ncan perform better than the ordinary method of just collecting data and\nfine-tuning. Based on our experiment, the continued pretraining process of IGOT\nwith LLaMA-7B achieved 11.9\\% token saving, 12.2\\% training time saving, and\n5.8\\% maximum GPU VRAM usage saving, combined with the T5 model, we can even\nreach a 31.5\\% of training time saving, making porting general generative AI to\nspecific domains more effective than before. In domain-specific tasks,\nsupervised $IGOT_\\tau$ shows great performance on reducing both the convergence\nradius and convergence point during keep pretraining.",
      "tldr_zh": "该论文提出了一种名为 IGOT（Information Gain Optimized Tokenizer）的框架，用于优化领域自适应预训练（Domain Adaptive Pretraining），以解决大型语言模型（LLM）在特定领域应用时的知识整合问题。IGOT 通过分析下游任务的特殊标记集，使用启发式函数 φ 计算信息增益来构建新的领域特定标记器，并在此基础上继续预训练模型。实验结果显示，IGOT 在 LLaMA-7B 上实现了 11.9% 标记节省、12.2% 训练时间节省和 5.8% GPU VRAM 使用节省，与 T5 模型结合时甚至可节省 31.5% 训练时间，从而使 LLM 在特定领域的移植更高效。总的来说，该方法在减少收敛半径和收敛点方面表现出色，提升了领域适应的整体性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.09857v1",
      "published_date": "2024-05-16 07:25:10 UTC",
      "updated_date": "2024-05-16 07:25:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:05:24.099911"
    },
    {
      "arxiv_id": "2405.09848v1",
      "title": "Enhancing Semantics in Multimodal Chain of Thought via Soft Negative Sampling",
      "title_zh": "翻译失败",
      "authors": [
        "Guangmin Zheng",
        "Jin Wang",
        "Xiaobing Zhou",
        "Xuejie Zhang"
      ],
      "abstract": "Chain of thought (CoT) has proven useful for problems requiring complex\nreasoning. Many of these problems are both textual and multimodal. Given the\ninputs in different modalities, a model generates a rationale and then uses it\nto answer a question. Because of the hallucination issue, the generated soft\nnegative rationales with high textual quality but illogical semantics do not\nalways help improve answer accuracy. This study proposes a rationale generation\nmethod using soft negative sampling (SNSE-CoT) to mitigate hallucinations in\nmultimodal CoT. Five methods were applied to generate soft negative samples\nthat shared highly similar text but had different semantics from the original.\nBidirectional margin loss (BML) was applied to introduce them into the\ntraditional contrastive learning framework that involves only positive and\nnegative samples. Extensive experiments on the ScienceQA dataset demonstrated\nthe effectiveness of the proposed method. Code and data are released at\nhttps://github.com/zgMin/SNSE-CoT.",
      "tldr_zh": "本研究针对多模态 Chain of Thought (CoT) 中的幻觉问题，提出了一种软负采样方法（SNSE-CoT），旨在通过生成与原样本文本高度相似但语义不同的软负样本来增强推理语义准确性。该方法运用五种技术创建软负样本，并结合 Bidirectional Margin Loss (BML) 整合到传统的对比学习框架中，以缓解生成的推理过程不逻辑的问题。在 ScienceQA 数据集上的广泛实验证明，SNSE-CoT 显著提高了答案准确率，并已开源代码和数据。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by LREC-COLING 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.09848v1",
      "published_date": "2024-05-16 06:55:11 UTC",
      "updated_date": "2024-05-16 06:55:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:05:36.679027"
    },
    {
      "arxiv_id": "2405.13028v1",
      "title": "DuetSim: Building User Simulator with Dual Large Language Models for Task-Oriented Dialogues",
      "title_zh": "DuetSim：利用双大型语言模型构建用户模拟器以应用于任务导向对话",
      "authors": [
        "Xiang Luo",
        "Zhiwen Tang",
        "Jin Wang",
        "Xuejie Zhang"
      ],
      "abstract": "User Simulators play a pivotal role in training and evaluating task-oriented\ndialogue systems. Traditional user simulators typically rely on\nhuman-engineered agendas, resulting in generated responses that often lack\ndiversity and spontaneity. Although large language models (LLMs) exhibit a\nremarkable capacity for generating coherent and contextually appropriate\nutterances, they may fall short when tasked with generating responses that\neffectively guide users towards their goals, particularly in dialogues with\nintricate constraints and requirements. This paper introduces DuetSim, a novel\nframework designed to address the intricate demands of task-oriented dialogues\nby leveraging LLMs. DuetSim stands apart from conventional approaches by\nemploying two LLMs in tandem: one dedicated to response generation and the\nother focused on verification. This dual LLM approach empowers DuetSim to\nproduce responses that not only exhibit diversity but also demonstrate accuracy\nand are preferred by human users. We validate the efficacy of our method\nthrough extensive experiments conducted on the MultiWOZ dataset, highlighting\nimprovements in response quality and correctness, largely attributed to the\nincorporation of the second LLM. Our code is accessible at:\nhttps://github.com/suntea233/DuetSim.",
      "tldr_zh": "该论文提出 DuetSim 框架，使用双 Large Language Models (LLMs) 构建用户模拟器，以解决传统任务导向对话系统中的响应多样性和自发性问题。DuetSim 采用一个 LLM 负责生成响应，另一个 LLM 专注于验证，确保生成的对话内容准确、上下文合适，并更符合人类用户偏好。通过在 MultiWOZ 数据集上的广泛实验，框架显著提高了响应质量和正确性，比基线方法表现出色，并开源了代码以便进一步应用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by COLING 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.13028v1",
      "published_date": "2024-05-16 06:24:31 UTC",
      "updated_date": "2024-05-16 06:24:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:05:48.288981"
    },
    {
      "arxiv_id": "2405.10988v2",
      "title": "Flow Score Distillation for Diverse Text-to-3D Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Runjie Yan",
        "Kailu Wu",
        "Kaisheng Ma"
      ],
      "abstract": "Recent advancements in Text-to-3D generation have yielded remarkable\nprogress, particularly through methods that rely on Score Distillation Sampling\n(SDS). While SDS exhibits the capability to create impressive 3D assets, it is\nhindered by its inherent maximum-likelihood-seeking essence, resulting in\nlimited diversity in generation outcomes. In this paper, we discover that the\nDenoise Diffusion Implicit Models (DDIM) generation process (\\ie PF-ODE) can be\nsuccinctly expressed using an analogue of SDS loss. One step further, one can\nsee SDS as a generalized DDIM generation process. Following this insight, we\nshow that the noise sampling strategy in the noise addition stage significantly\nrestricts the diversity of generation results. To address this limitation, we\npresent an innovative noise sampling approach and introduce a novel text-to-3D\nmethod called Flow Score Distillation (FSD). Our validation experiments across\nvarious text-to-image Diffusion Models demonstrate that FSD substantially\nenhances generation diversity without compromising quality.",
      "tldr_zh": "本研究针对文本到3D生成中的Score Distillation Sampling (SDS)方法存在的生成多样性有限问题，揭示了SDS可视为Denoise Diffusion Implicit Models (DDIM)生成过程的泛化形式，并强调噪声采样策略是限制多样性的关键因素。作者提出了一种创新的噪声采样方法，并引入了新型方法Flow Score Distillation (FSD)，通过优化生成过程来提升多样性。实验验证显示，FSD在多种文本到图像扩散模型上显著提高了生成结果的多样性，同时保持了高质量输出。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Consistent Flow Distillation is an improved version of this paper",
      "pdf_url": "http://arxiv.org/pdf/2405.10988v2",
      "published_date": "2024-05-16 06:05:16 UTC",
      "updated_date": "2024-07-28 21:52:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:06:00.219581"
    },
    {
      "arxiv_id": "2405.10987v1",
      "title": "Manifold-based Incomplete Multi-view Clustering via Bi-Consistency Guidance",
      "title_zh": "翻译失败",
      "authors": [
        "Huibing Wang",
        "Mingze Yao",
        "Yawei Chen",
        "Yunqiu Xu",
        "Haipeng Liu",
        "Wei Jia",
        "Xianping Fu",
        "Yang Wang"
      ],
      "abstract": "Incomplete multi-view clustering primarily focuses on dividing unlabeled data\ninto corresponding categories with missing instances, and has received\nintensive attention due to its superiority in real applications. Considering\nthe influence of incomplete data, the existing methods mostly attempt to\nrecover data by adding extra terms. However, for the unsupervised methods, a\nsimple recovery strategy will cause errors and outlying value accumulations,\nwhich will affect the performance of the methods. Broadly, the previous methods\nhave not taken the effectiveness of recovered instances into consideration, or\ncannot flexibly balance the discrepancies between recovered data and original\ndata. To address these problems, we propose a novel method termed\nManifold-based Incomplete Multi-view clustering via Bi-consistency guidance\n(MIMB), which flexibly recovers incomplete data among various views, and\nattempts to achieve biconsistency guidance via reverse regularization. In\nparticular, MIMB adds reconstruction terms to representation learning by\nrecovering missing instances, which dynamically examines the latent consensus\nrepresentation. Moreover, to preserve the consistency information among\nmultiple views, MIMB implements a biconsistency guidance strategy with reverse\nregularization of the consensus representation and proposes a manifold\nembedding measure for exploring the hidden structure of the recovered data.\nNotably, MIMB aims to balance the importance of different views, and introduces\nan adaptive weight term for each view. Finally, an optimization algorithm with\nan alternating iteration optimization strategy is designed for final\nclustering. Extensive experimental results on 6 benchmark datasets are provided\nto confirm that MIMB can significantly obtain superior results as compared with\nseveral state-of-the-art baselines.",
      "tldr_zh": "本论文提出了一种名为 MIMB 的新方法，用于处理 Incomplete Multi-view Clustering 中的缺失实例问题，通过 Bi-Consistency Guidance 和 Manifold-based 策略灵活恢复数据，避免现有方法的错误积累。MIMB 通过添加重构术语动态检查共识表示，实现双一致性指导策略（包括反向正则化和流形嵌入测度），并引入自适应权重术语来平衡不同视图的重要性。实验结果显示，该方法在 6 个基准数据集上显著优于现有最先进基线，证明了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.10987v1",
      "published_date": "2024-05-16 05:58:29 UTC",
      "updated_date": "2024-05-16 05:58:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:06:13.409798"
    },
    {
      "arxiv_id": "2405.09806v4",
      "title": "MediSyn: A Generalist Text-Guided Latent Diffusion Model For Diverse Medical Image Synthesis",
      "title_zh": "翻译失败",
      "authors": [
        "Joseph Cho",
        "Mrudang Mathur",
        "Cyril Zakka",
        "Dhamanpreet Kaur",
        "Matthew Leipzig",
        "Alex Dalal",
        "Aravind Krishnan",
        "Eubee Koo",
        "Karen Wai",
        "Cindy S. Zhao",
        "Rohan Shad",
        "Robyn Fong",
        "Ross Wightman",
        "Akshay Chaudhari",
        "William Hiesinger"
      ],
      "abstract": "Deep learning algorithms require extensive data to achieve robust\nperformance. However, data availability is often restricted in the medical\ndomain due to patient privacy concerns. Synthetic data presents a possible\nsolution to these challenges. Recently, image generative models have found\nincreasing use for medical applications but are often designed for singular\nmedical specialties and imaging modalities, thus limiting their broader\nutility. To address this, we introduce MediSyn: a text-guided, latent diffusion\nmodel capable of generating synthetic images from 6 medical specialties and 10\nimage types. The synthetic images are validated by expert clinicians for\nalignment with their corresponding text prompts. Furthermore, a direct\ncomparison of the synthetic images against the real images confirms that our\nmodel synthesizes novel images and, crucially, may preserve patient privacy.\nFinally, classifiers trained on a mixture of synthetic and real data achieve\nsimilar performance to those trained on twice the amount of real data. Our\nfindings highlight the immense potential for generalist image generative models\nto accelerate algorithmic research and development in medicine.",
      "tldr_zh": "该研究提出MediSyn，一种通用的文本引导latent diffusion model，用于生成涵盖6个医疗专业和10种图像类型的合成图像，以解决医疗领域数据稀缺和隐私问题。模型通过专家临床医生验证，确保合成图像与文本提示高度一致，并生成新颖图像以保护患者隐私。实验结果显示，使用合成数据训练的分类器性能可媲美使用双倍真实数据的模型，突显了此类通用模型在加速医学算法研发方面的巨大潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.09806v4",
      "published_date": "2024-05-16 04:28:44 UTC",
      "updated_date": "2025-02-10 20:00:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:06:24.055194"
    },
    {
      "arxiv_id": "2405.09802v3",
      "title": "Analysis and Predictive Modeling of Solar Coronal Holes Using Computer Vision and ARIMA-LSTM Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Juyoung Yun",
        "Jungmin Shin"
      ],
      "abstract": "In the era of space exploration, coronal holes on the sun play a significant\nrole due to their impact on satellites and aircraft through their open magnetic\nfields and increased solar wind emissions. This study employs computer vision\ntechniques to detect coronal hole regions and estimate their sizes using\nimagery from the Solar Dynamics Observatory (SDO). Additionally, we utilize\nhybrid time series prediction model, specifically combination of Long\nShort-Term Memory (LSTM) networks and ARIMA, to analyze trends in the area of\ncoronal holes and predict their areas across various solar regions over a span\nof seven days. By examining time series data, we aim to identify patterns in\ncoronal hole behavior and understand their potential effects on space weather.",
      "tldr_zh": "本研究利用计算机视觉技术分析太阳冕洞，通过 Solar Dynamics Observatory (SDO) 的图像检测冕洞区域并估计其大小。研究还采用 ARIMA-LSTM 混合时间序列预测模型，结合 Long Short-Term Memory (LSTM) 网络和 ARIMA 方法，分析冕洞面积趋势并预测未来七天的变化。结果显示，该方法有助于识别冕洞行为的模式，并评估其对空间天气（如卫星和飞机的影响）的潜在效应。",
      "categories": [
        "astro-ph.SR",
        "astro-ph.EP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "astro-ph.SR",
      "comment": "Accepted to the first joint European Space Agency SPAICE Conference\n  2024",
      "pdf_url": "http://arxiv.org/pdf/2405.09802v3",
      "published_date": "2024-05-16 04:21:09 UTC",
      "updated_date": "2024-07-31 08:28:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:06:36.568230"
    },
    {
      "arxiv_id": "2405.09798v2",
      "title": "Many-Shot In-Context Learning in Multimodal Foundation Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yixing Jiang",
        "Jeremy Irvin",
        "Ji Hun Wang",
        "Muhammad Ahmed Chaudhry",
        "Jonathan H. Chen",
        "Andrew Y. Ng"
      ],
      "abstract": "Large language models are effective at few-shot in-context learning (ICL).\nRecent advancements in multimodal foundation models have enabled\nunprecedentedly long context windows, presenting an opportunity to explore\ntheir capability to perform ICL with many more demonstrating examples. In this\nwork, we evaluate the performance of multimodal foundation models scaling from\nfew-shot to many-shot ICL. We benchmark GPT-4o and Gemini 1.5 Pro across 14\ndatasets spanning multiple domains (natural imagery, medical imagery, remote\nsensing, and molecular imagery) and tasks (image classification, visual QA, and\nobject localization). We observe that many-shot ICL, including up to almost\n2,000 demonstrating examples, leads to substantial improvements compared to\nfew-shot (<100 examples) ICL across all of the datasets. Further, Gemini 1.5\nPro performance continues to improve log-linearly up to the maximum number of\ntested examples on many datasets. We also find open-weights multimodal\nfoundation models like Llama 3.2-Vision do not benefit from the demonstrating\nexamples, highlighting an important gap between open and closed multimodal\nfoundation models. Given the high inference costs required for many-shot ICL,\nwe also explore the impact of batching multiple queries in a single API call.\nWe show that batching up to 50 queries can lead to performance improvements\nunder zero-shot and many-shot ICL, with substantial gains in the zero-shot\nsetting on multiple datasets, while drastically reducing per-query cost and\nlatency. Finally, while GPT-4o and Gemini 1.5 Pro achieve similar zero-shot\nperformance across the datasets, Gemini 1.5 Pro learns more quickly than GPT-4o\non most datasets. Our results suggest that many-shot ICL could enable users to\nefficiently adapt multimodal foundation models to new applications and domains.\nOur codebase is publicly available at\nhttps://github.com/stanfordmlgroup/ManyICL .",
      "tldr_zh": "这篇论文评估了多模态基础模型在many-shot In-Context Learning (ICL)中的性能，扩展了传统few-shot ICL的范围，使用GPT-4o和Gemini 1.5 Pro在14个数据集上进行基准测试，涵盖图像分类、视觉QA和对象定位等任务。结果显示，many-shot ICL（多达近2000个示例）显著优于few-shot ICL（少于100个示例），Gemini 1.5 Pro的性能在许多数据集上呈对数线性改善，而开放权重模型如Llama 3.2-Vision则未从中受益。研究还发现，批量查询可提升零样本和many-shot ICL的效率，同时降低成本和延迟，并强调Gemini 1.5 Pro比GPT-4o学习更快，这为用户高效适应新应用和领域提供了潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.09798v2",
      "published_date": "2024-05-16 04:02:43 UTC",
      "updated_date": "2024-10-04 21:51:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:06:51.655741"
    },
    {
      "arxiv_id": "2405.10348v1",
      "title": "Learning to Predict Mutation Effects of Protein-Protein Interactions by Microenvironment-aware Hierarchical Prompt Learning",
      "title_zh": "基于微环境感知的分层提示学习预测蛋白质-蛋白质相互作用的突变效应",
      "authors": [
        "Lirong Wu",
        "Yijun Tian",
        "Haitao Lin",
        "Yufei Huang",
        "Siyuan Li",
        "Nitesh V Chawla",
        "Stan Z. Li"
      ],
      "abstract": "Protein-protein bindings play a key role in a variety of fundamental\nbiological processes, and thus predicting the effects of amino acid mutations\non protein-protein binding is crucial. To tackle the scarcity of annotated\nmutation data, pre-training with massive unlabeled data has emerged as a\npromising solution. However, this process faces a series of challenges: (1)\ncomplex higher-order dependencies among multiple (more than paired) structural\nscales have not yet been fully captured; (2) it is rarely explored how\nmutations alter the local conformation of the surrounding microenvironment; (3)\npre-training is costly, both in data size and computational burden. In this\npaper, we first construct a hierarchical prompt codebook to record common\nmicroenvironmental patterns at different structural scales independently. Then,\nwe develop a novel codebook pre-training task, namely masked microenvironment\nmodeling, to model the joint distribution of each mutation with their residue\ntypes, angular statistics, and local conformational changes in the\nmicroenvironment. With the constructed prompt codebook, we encode the\nmicroenvironment around each mutation into multiple hierarchical prompts and\ncombine them to flexibly provide information to wild-type and mutated protein\ncomplexes about their microenvironmental differences. Such a hierarchical\nprompt learning framework has demonstrated superior performance and training\nefficiency over state-of-the-art pre-training-based methods in mutation effect\nprediction and a case study of optimizing human antibodies against SARS-CoV-2.",
      "tldr_zh": "该论文提出了一种微环境感知的分层提示学习（Hierarchical Prompt Learning）框架，用于预测氨基酸突变对蛋白-蛋白质互作（Protein-Protein Interactions）的影响，以解决数据稀缺和预训练挑战。研究首先构建了分层提示代码本（Hierarchical Prompt Codebook），记录不同结构规模的微环境模式，并开发了masked microenvironment modeling任务来建模突变与其残基类型、角度统计及局部构象变化的联合分布。实验结果显示，该框架在突变效果预测中比现有预训练方法表现出更高的性能和训练效率，并在优化人类抗体对抗SARS-CoV-2的案例研究中取得了显著成效。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.10348v1",
      "published_date": "2024-05-16 03:53:21 UTC",
      "updated_date": "2024-05-16 03:53:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:07:01.930802"
    },
    {
      "arxiv_id": "2405.09794v2",
      "title": "Human-AI Safety: A Descendant of Generative AI and Control Systems Safety",
      "title_zh": "翻译失败",
      "authors": [
        "Andrea Bajcsy",
        "Jaime F. Fisac"
      ],
      "abstract": "Artificial intelligence (AI) is interacting with people at an unprecedented\nscale, offering new avenues for immense positive impact, but also raising\nwidespread concerns around the potential for individual and societal harm.\nToday, the predominant paradigm for human--AI safety focuses on fine-tuning the\ngenerative model's outputs to better agree with human-provided examples or\nfeedback. In reality, however, the consequences of an AI model's outputs cannot\nbe determined in isolation: they are tightly entangled with the responses and\nbehavior of human users over time. In this paper, we distill key complementary\nlessons from AI safety and control systems safety, highlighting open challenges\nas well as key synergies between both fields. We then argue that meaningful\nsafety assurances for advanced AI technologies require reasoning about how the\nfeedback loop formed by AI outputs and human behavior may drive the interaction\ntowards different outcomes. To this end, we introduce a unifying formalism to\ncapture dynamic, safety-critical human--AI interactions and propose a concrete\ntechnical roadmap towards next-generation human-centered AI safety.",
      "tldr_zh": "人工智能（AI）与人类的互动规模空前，可能带来积极影响，但也引发个体和社会伤害的担忧；当前的人-AI 安全范式主要依赖微调 Generative AI 的输出以匹配人类示例或反馈，然而这忽略了 AI 输出与人类行为形成的动态反馈循环。论文提炼了 AI Safety 和 Control Systems Safety 的关键教训，突出两者间的挑战与协同作用，并引入一个统一形式主义来捕捉安全关键的人-AI 互动。最终，论文提出一个具体的技术路线图，旨在为下一代以人为中心的 AI 安全提供有意义的保障。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.SY",
        "eess.SY",
        "I.2"
      ],
      "primary_category": "cs.AI",
      "comment": "Revised version with refined exposition and technical details. 12\n  pages + references, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.09794v2",
      "published_date": "2024-05-16 03:52:00 UTC",
      "updated_date": "2024-06-22 20:17:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:07:14.934059"
    },
    {
      "arxiv_id": "2405.09784v3",
      "title": "Online bipartite matching with imperfect advice",
      "title_zh": "带有不完美建议的在线二分匹配",
      "authors": [
        "Davin Choo",
        "Themis Gouleakis",
        "Chun Kai Ling",
        "Arnab Bhattacharyya"
      ],
      "abstract": "We study the problem of online unweighted bipartite matching with $n$ offline\nvertices and $n$ online vertices where one wishes to be competitive against the\noptimal offline algorithm. While the classic RANKING algorithm of Karp et al.\n[1990] provably attains competitive ratio of $1-1/e > 1/2$, we show that no\nlearning-augmented method can be both 1-consistent and strictly better than\n$1/2$-robust under the adversarial arrival model. Meanwhile, under the random\narrival model, we show how one can utilize methods from distribution testing to\ndesign an algorithm that takes in external advice about the online vertices and\nprovably achieves competitive ratio interpolating between any ratio attainable\nby advice-free methods and the optimal ratio of 1, depending on the advice\nquality.",
      "tldr_zh": "本论文研究了在线二分匹配问题，涉及n个离线顶点和n个在线顶点，目标是与最优离线算法竞争。研究证明，在对抗性到达模型下，没有学习-augmented方法能同时实现1-consistent并严格优于1/2-robust。另一方面，在随机到达模型下，通过分布测试方法，论文提出了一种算法，利用外部不完美建议，根据建议质量，使竞争比从advice-free方法的水平插值到最优的1。整体结果为在线匹配算法的设计提供了重要理论指导。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DS",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted into ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.09784v3",
      "published_date": "2024-05-16 03:04:33 UTC",
      "updated_date": "2024-05-23 13:15:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:07:25.772662"
    },
    {
      "arxiv_id": "2405.09783v1",
      "title": "LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery",
      "title_zh": "翻译失败",
      "authors": [
        "Pingchuan Ma",
        "Tsun-Hsuan Wang",
        "Minghao Guo",
        "Zhiqing Sun",
        "Joshua B. Tenenbaum",
        "Daniela Rus",
        "Chuang Gan",
        "Wojciech Matusik"
      ],
      "abstract": "Large Language Models have recently gained significant attention in\nscientific discovery for their extensive knowledge and advanced reasoning\ncapabilities. However, they encounter challenges in effectively simulating\nobservational feedback and grounding it with language to propel advancements in\nphysical scientific discovery. Conversely, human scientists undertake\nscientific discovery by formulating hypotheses, conducting experiments, and\nrevising theories through observational analysis. Inspired by this, we propose\nto enhance the knowledge-driven, abstract reasoning abilities of LLMs with the\ncomputational strength of simulations. We introduce Scientific Generative Agent\n(SGA), a bilevel optimization framework: LLMs act as knowledgeable and\nversatile thinkers, proposing scientific hypotheses and reason about discrete\ncomponents, such as physics equations or molecule structures; meanwhile,\nsimulations function as experimental platforms, providing observational\nfeedback and optimizing via differentiability for continuous parts, such as\nphysical parameters. We conduct extensive experiments to demonstrate our\nframework's efficacy in constitutive law discovery and molecular design,\nunveiling novel solutions that differ from conventional human expectations yet\nremain coherent upon analysis.",
      "tldr_zh": "这篇论文提出了一种新范式，将 Large Language Models (LLMs) 和模拟作为双层优化器，以克服 LLMs 在物理科学发现中模拟观察反馈和语言地化的挑战。作者引入了 Scientific Generative Agent (SGA) 框架，其中 LLMs 担任知识驱动的思考者，负责提出科学假设并推理离散组件（如物理方程或分子结构），而模拟则作为实验平台，提供观察反馈并通过可微性优化连续参数（如物理参数）。实验结果显示，该框架在构成定律发现和分子设计任务中表现出色，揭示了与传统人类预期不同的创新解决方案，这些方案经分析后被证明是合理的。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.09783v1",
      "published_date": "2024-05-16 03:04:10 UTC",
      "updated_date": "2024-05-16 03:04:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:07:37.434839"
    },
    {
      "arxiv_id": "2405.09781v1",
      "title": "An Independent Implementation of Quantum Machine Learning Algorithms in Qiskit for Genomic Data",
      "title_zh": "翻译失败",
      "authors": [
        "Navneet Singh",
        "Shiva Raj Pokhrel"
      ],
      "abstract": "In this paper, we explore the power of Quantum Machine Learning as we extend,\nimplement and evaluate algorithms like Quantum Support Vector Classifier\n(QSVC), Pegasos-QSVC, Variational Quantum Circuits (VQC), and Quantum Neural\nNetworks (QNN) in Qiskit with diverse feature mapping techniques for genomic\nsequence classification.",
      "tldr_zh": "这篇论文探讨了量子机器学习在基因组数据分类中的应用，通过在Qiskit平台上独立扩展、实现和评估算法，如Quantum Support Vector Classifier (QSVC)、Pegasos-QSVC、Variational Quantum Circuits (VQC)和Quantum Neural Networks (QNN)。研究者采用了多样化的特征映射技术，以提升这些算法的性能。最终，该工作展示了量子机器学习算法在处理基因组序列分类任务时的潜在优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "2 pager extended abstract",
      "pdf_url": "http://arxiv.org/pdf/2405.09781v1",
      "published_date": "2024-05-16 03:00:41 UTC",
      "updated_date": "2024-05-16 03:00:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:07:48.019678"
    },
    {
      "arxiv_id": "2405.09770v1",
      "title": "Optimization Techniques for Sentiment Analysis Based on LLM (GPT-3)",
      "title_zh": "翻译失败",
      "authors": [
        "Tong Zhan",
        "Chenxi Shi",
        "Yadong Shi",
        "Huixiang Li",
        "Yiyu Lin"
      ],
      "abstract": "With the rapid development of natural language processing (NLP) technology,\nlarge-scale pre-trained language models such as GPT-3 have become a popular\nresearch object in NLP field. This paper aims to explore sentiment analysis\noptimization techniques based on large pre-trained language models such as\nGPT-3 to improve model performance and effect and further promote the\ndevelopment of natural language processing (NLP). By introducing the importance\nof sentiment analysis and the limitations of traditional methods, GPT-3 and\nFine-tuning techniques are introduced in this paper, and their applications in\nsentiment analysis are explained in detail. The experimental results show that\nthe Fine-tuning technique can optimize GPT-3 model and obtain good performance\nin sentiment analysis task. This study provides an important reference for\nfuture sentiment analysis using large-scale language models.",
      "tldr_zh": "该论文探讨了基于大型语言模型(LLM)如 GPT-3 的情感分析(sentiment analysis)优化技术，旨在提升模型性能并推动自然语言处理(NLP)的发展。通过分析传统方法的局限性，论文详细介绍了 GPT-3 模型及其 Fine-tuning 技术的应用，以改进情感分析任务。实验结果显示，Fine-tuning 可以显著优化 GPT-3 的表现，为未来使用大规模语言模型进行情感分析提供重要参考。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.09770v1",
      "published_date": "2024-05-16 02:21:13 UTC",
      "updated_date": "2024-05-16 02:21:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:07:59.831843"
    },
    {
      "arxiv_id": "2405.09765v1",
      "title": "Unsupervised Extractive Dialogue Summarization in Hyperdimensional Space",
      "title_zh": "超高维空间中的无监督提取式对话摘要",
      "authors": [
        "Seongmin Park",
        "Kyungho Kim",
        "Jaejin Seo",
        "Jihwa Lee"
      ],
      "abstract": "We present HyperSum, an extractive summarization framework that captures both\nthe efficiency of traditional lexical summarization and the accuracy of\ncontemporary neural approaches. HyperSum exploits the pseudo-orthogonality that\nemerges when randomly initializing vectors at extremely high dimensions\n(\"blessing of dimensionality\") to construct representative and efficient\nsentence embeddings. Simply clustering the obtained embeddings and extracting\ntheir medoids yields competitive summaries. HyperSum often outperforms\nstate-of-the-art summarizers -- in terms of both summary accuracy and\nfaithfulness -- while being 10 to 100 times faster. We open-source HyperSum as\na strong baseline for unsupervised extractive summarization.",
      "tldr_zh": "本文提出 HyperSum，一种无监督抽取式(extractive)对话摘要框架，结合了传统词汇摘要的效率和现代神经方法的准确性。HyperSum 通过利用高维空间中的随机初始化向量及其伪正交性(pseudo-orthogonality)来构建句子嵌入，然后通过聚类和提取 medoids 生成竞争性的摘要。实验结果显示，HyperSum 在摘要准确性和忠实度上超过了现有最先进模型，同时速度快 10 到 100 倍，并作为开源基准推动了该领域的发展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ICASSP 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.09765v1",
      "published_date": "2024-05-16 02:11:03 UTC",
      "updated_date": "2024-05-16 02:11:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:08:12.874304"
    },
    {
      "arxiv_id": "2405.09763v1",
      "title": "Fusion Intelligence: Confluence of Natural and Artificial Intelligence for Enhanced Problem-Solving Efficiency",
      "title_zh": "翻译失败",
      "authors": [
        "Rohan Reddy Kalavakonda",
        "Junjun Huan",
        "Peyman Dehghanzadeh",
        "Archit Jaiswal",
        "Soumyajit Mandal",
        "Swarup Bhunia"
      ],
      "abstract": "This paper introduces Fusion Intelligence (FI), a bio-inspired intelligent\nsystem, where the innate sensing, intelligence and unique actuation abilities\nof biological organisms such as bees and ants are integrated with the\ncomputational power of Artificial Intelligence (AI). This interdisciplinary\nfield seeks to create systems that are not only smart but also adaptive and\nresponsive in ways that mimic the nature. As FI evolves, it holds the promise\nof revolutionizing the way we approach complex problems, leveraging the best of\nboth biological and digital worlds to create solutions that are more effective,\nsustainable, and harmonious with the environment. We demonstrate FI's potential\nto enhance agricultural IoT system performance through a simulated case study\non improving insect pollination efficacy (entomophily).",
      "tldr_zh": "这篇论文介绍了 Fusion Intelligence (FI)，一个受生物启发的智能系统，将蜜蜂和蚂蚁等生物的感知、智能和动作能力与 Artificial Intelligence (AI) 的计算能力相结合，以创建更智能、适应性和响应性的系统。FI 旨在通过整合生物和数字世界的优势，革新复杂问题的解决方式，提供更有效、可持续且与环境和谐的解决方案。通过一个模拟案例研究，该系统在农业 IoT 系统上提升了昆虫授粉效率，展示了其实际潜力。",
      "categories": [
        "cs.AI",
        "cs.ET",
        "cs.MA",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, 4 figures, 1 Table",
      "pdf_url": "http://arxiv.org/pdf/2405.09763v1",
      "published_date": "2024-05-16 02:10:30 UTC",
      "updated_date": "2024-05-16 02:10:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:08:25.004011"
    },
    {
      "arxiv_id": "2405.10347v4",
      "title": "Networking Systems for Video Anomaly Detection: A Tutorial and Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Jing Liu",
        "Yang Liu",
        "Jieyu Lin",
        "Jielin Li",
        "Liang Cao",
        "Peng Sun",
        "Bo Hu",
        "Liang Song",
        "Azzedine Boukerche",
        "Victor C. M. Leung"
      ],
      "abstract": "The increasing utilization of surveillance cameras in smart cities, coupled\nwith the surge of online video applications, has heightened concerns regarding\npublic security and privacy protection, which propelled automated Video Anomaly\nDetection (VAD) into a fundamental research task within the Artificial\nIntelligence (AI) community. With the advancements in deep learning and edge\ncomputing, VAD has made significant progress and advances synergized with\nemerging applications in smart cities and video internet, which has moved\nbeyond the conventional research scope of algorithm engineering to deployable\nNetworking Systems for VAD (NSVAD), a practical hotspot for intersection\nexploration in the AI, IoVT, and computing fields. In this article, we\ndelineate the foundational assumptions, learning frameworks, and applicable\nscenarios of various deep learning-driven VAD routes, offering an exhaustive\ntutorial for novices in NSVAD. In addition, this article elucidates core\nconcepts by reviewing recent advances and typical solutions and aggregating\navailable research resources accessible at https://github.com/fdjingliu/NSVAD.\nLastly, this article projects future development trends and discusses how the\nintegration of AI and computing technologies can address existing research\nchallenges and promote open opportunities, serving as an insightful guide for\nprospective researchers and engineers.",
      "tldr_zh": "这篇论文以“Networking Systems for Video Anomaly Detection (NSVAD)”为主题，提供了一个全面的教程和调查，探讨了视频异常检测 (VAD) 在智能城市和在线视频应用中的重要性，以及其与人工智能 (AI) 和边缘计算的整合。论文详细阐述了深度学习驱动的 VAD 路线，包括基础假设、学习框架和实际场景应用，并回顾了最近的进展、典型解决方案，并提供了资源链接 (https://github.com/fdjingliu/NSVAD)。最终，它展望了未来发展趋势，强调 AI 和计算技术的融合可解决现有挑战，如隐私保护和部署难题，从而为研究者和工程师提供指导。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ACM Computing Surveys. For more information and\n  supplementary material, please visit https://github.com/fdjingliu/NSVAD",
      "pdf_url": "http://arxiv.org/pdf/2405.10347v4",
      "published_date": "2024-05-16 02:00:44 UTC",
      "updated_date": "2025-04-03 05:41:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:08:37.414922"
    },
    {
      "arxiv_id": "2405.10346v1",
      "title": "AMCEN: An Attention Masking-based Contrastive Event Network for Two-stage Temporal Knowledge Graph Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Jing Yang",
        "Xiao Wang",
        "Yutong Wang",
        "Jiawei Wang",
        "Fei-Yue Wang"
      ],
      "abstract": "Temporal knowledge graphs (TKGs) can effectively model the ever-evolving\nnature of real-world knowledge, and their completeness and enhancement can be\nachieved by reasoning new events from existing ones. However, reasoning\naccuracy is adversely impacted due to an imbalance between new and recurring\nevents in the datasets. To achieve more accurate TKG reasoning, we propose an\nattention masking-based contrastive event network (AMCEN) with local-global\ntemporal patterns for the two-stage prediction of future events. In the\nnetwork, historical and non-historical attention mask vectors are designed to\ncontrol the attention bias towards historical and non-historical entities,\nacting as the key to alleviating the imbalance. A local-global message-passing\nmodule is proposed to comprehensively consider and capture multi-hop structural\ndependencies and local-global temporal evolution for the in-depth exploration\nof latent impact factors of different event types. A contrastive event\nclassifier is used to classify events more accurately by incorporating\nlocal-global temporal patterns into contrastive learning. Therefore, AMCEN\nrefines the prediction scope with the results of the contrastive event\nclassification, followed by utilizing attention masking-based decoders to\nfinalize the specific outcomes. The results of our experiments on four\nbenchmark datasets highlight the superiority of AMCEN. Especially, the\nconsiderable improvements in Hits@1 prove that AMCEN can make more precise\npredictions about future occurrences.",
      "tldr_zh": "本研究针对时间知识图（Temporal Knowledge Graphs, TKGs）中事件不平衡问题，提出了一种基于注意力掩码的对比事件网络（AMCEN），用于两阶段预测未来事件。该框架设计了历史和非历史注意力掩码向量，以缓解新事件与重复事件的偏差，并引入本地-全局消息传递模块来捕获多跳结构依赖性和时间演化模式，同时通过对比事件分类器整合这些模式提升分类准确性。最终，AMCEN 通过缩小预测范围并利用注意力掩码解码器生成精确结果，在四个基准数据集上表现出色，尤其是 Hits@1 指标显著提升，证明了其在 TKG 推理中的优越性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.10346v1",
      "published_date": "2024-05-16 01:39:50 UTC",
      "updated_date": "2024-05-16 01:39:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:08:49.425167"
    },
    {
      "arxiv_id": "2405.13026v1",
      "title": "Leveraging Human Revisions for Improving Text-to-Layout Models",
      "title_zh": "翻译失败",
      "authors": [
        "Amber Xie",
        "Chin-Yi Cheng",
        "Forrest Huang",
        "Yang Li"
      ],
      "abstract": "Learning from human feedback has shown success in aligning large, pretrained\nmodels with human values. Prior works have mostly focused on learning from\nhigh-level labels, such as preferences between pairs of model outputs. On the\nother hand, many domains could benefit from more involved, detailed feedback,\nsuch as revisions, explanations, and reasoning of human users. Our work\nproposes using nuanced feedback through the form of human revisions for\nstronger alignment. In this paper, we ask expert designers to fix layouts\ngenerated from a generative layout model that is pretrained on a large-scale\ndataset of mobile screens. Then, we train a reward model based on how human\ndesigners revise these generated layouts. With the learned reward model, we\noptimize our model with reinforcement learning from human feedback (RLHF). Our\nmethod, Revision-Aware Reward Models ($\\method$), allows a generative\ntext-to-layout model to produce more modern, designer-aligned layouts, showing\nthe potential for utilizing human revisions and stronger forms of feedback in\nimproving generative models.",
      "tldr_zh": "本文提出利用人类修订作为详细反馈来改进文本到布局模型，旨在超越以往仅依赖高层标签（如输出偏好）的反馈方法。研究让专家设计师修复由预训练生成布局模型产生的布局，然后基于这些修订训练一个奖励模型，并通过强化学习从人类反馈（RLHF）优化模型。该方法名为 Revision-Aware Reward Models（$\\method$），实验结果显示它能生成更现代、更符合设计师偏好的布局，证明了人类修订反馈在提升生成模型性能方面的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.13026v1",
      "published_date": "2024-05-16 01:33:09 UTC",
      "updated_date": "2024-05-16 01:33:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:09:01.506548"
    },
    {
      "arxiv_id": "2405.10345v1",
      "title": "Machine Learning Driven Biomarker Selection for Medical Diagnosis",
      "title_zh": "机器学习驱动的生物标记物",
      "authors": [
        "Divyagna Bavikadi",
        "Ayushi Agarwal",
        "Shashank Ganta",
        "Yunro Chung",
        "Lusheng Song",
        "Ji Qiu",
        "Paulo Shakarian"
      ],
      "abstract": "Recent advances in experimental methods have enabled researchers to collect\ndata on thousands of analytes simultaneously. This has led to correlational\nstudies that associated molecular measurements with diseases such as\nAlzheimer's, Liver, and Gastric Cancer. However, the use of thousands of\nbiomarkers selected from the analytes is not practical for real-world medical\ndiagnosis and is likely undesirable due to potentially formed spurious\ncorrelations. In this study, we evaluate 4 different methods for biomarker\nselection and 4 different machine learning (ML) classifiers for identifying\ncorrelations, evaluating 16 approaches in all. We found that contemporary\nmethods outperform previously reported logistic regression in cases where 3 and\n10 biomarkers are permitted. When specificity is fixed at 0.9, ML approaches\nproduced a sensitivity of 0.240 (3 biomarkers) and 0.520 (10 biomarkers), while\nstandard logistic regression provided a sensitivity of 0.000 (3 biomarkers) and\n0.040 (10 biomarkers). We also noted that causal-based methods for biomarker\nselection proved to be the most performant when fewer biomarkers were\npermitted, while univariate feature selection was the most performant when a\ngreater number of biomarkers were permitted.",
      "tldr_zh": "本研究针对医疗诊断中生物标志物(biomarkers)选择问题，评估了4种选择方法和4种machine learning (ML)分类器，共16种组合，以避免使用数千个分析物导致的虚假相关。结果显示，现代ML方法在允许3或10个生物标志物时优于标准logistic regression；在特异性固定为0.9时，ML方法实现了0.240（3个生物标志物）和0.520（10个生物标志物）的灵敏度，而logistic regression仅为0.000和0.040。研究还发现，因果-based方法在生物标志物数量较少时表现最佳，而univariate feature selection在数量较多时更有效。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.10345v1",
      "published_date": "2024-05-16 01:30:47 UTC",
      "updated_date": "2024-05-16 01:30:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:09:13.312042"
    },
    {
      "arxiv_id": "2407.01563v1",
      "title": "NaviSlim: Adaptive Context-Aware Navigation and Sensing via Dynamic Slimmable Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Tim Johnsen",
        "Marco Levorato"
      ],
      "abstract": "Small-scale autonomous airborne vehicles, such as micro-drones, are expected\nto be a central component of a broad spectrum of applications ranging from\nexploration to surveillance and delivery. This class of vehicles is\ncharacterized by severe constraints in computing power and energy reservoir,\nwhich impairs their ability to support the complex state-of-the-art neural\nmodels needed for autonomous operations. The main contribution of this paper is\na new class of neural navigation models -- NaviSlim -- capable of adapting the\namount of resources spent on computing and sensing in response to the current\ncontext (i.e., difficulty of the environment, current trajectory, and\nnavigation goals). Specifically, NaviSlim is designed as a gated slimmable\nneural network architecture that, different from existing slimmable networks,\ncan dynamically select a slimming factor to autonomously scale model\ncomplexity, which consequently optimizes execution time and energy consumption.\nMoreover, different from existing sensor fusion approaches, NaviSlim can\ndynamically select power levels of onboard sensors to autonomously reduce power\nand time spent during sensor acquisition, without the need to switch between\ndifferent neural networks. By means of extensive training and testing on the\nrobust simulation environment Microsoft AirSim, we evaluate our NaviSlim models\non scenarios with varying difficulty and a test set that showed a dynamic\nreduced model complexity on average between 57-92%, and between 61-80% sensor\nutilization, as compared to static neural networks designed to match computing\nand sensing of that required by the most difficult scenario.",
      "tldr_zh": "这篇论文提出了NaviSlim，一种适应性上下文感知的神经导航模型，针对小型自主飞行器（如微型无人机）的计算和能源限制问题，能够动态调整计算资源和传感器功率以响应环境难度、轨迹和导航目标。NaviSlim基于gated slimmable neural network架构，通过动态选择slimming factor来缩放模型复杂度，并自主优化传感器采集过程，而无需切换不同网络。实验结果显示，在Microsoft AirSim模拟环境中，NaviSlim相比静态神经网络平均减少模型复杂度57-92%和传感器利用率61-80%，显著提升了资源效率。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to IoTDI 2024, part of CPS-IoT Week for 2024, Hong Kong, and\n  pending publication in Proceedings",
      "pdf_url": "http://arxiv.org/pdf/2407.01563v1",
      "published_date": "2024-05-16 01:18:52 UTC",
      "updated_date": "2024-05-16 01:18:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:09:25.831899"
    },
    {
      "arxiv_id": "2405.09744v1",
      "title": "Many Hands Make Light Work: Task-Oriented Dialogue System with Module-Based Mixture-of-Experts",
      "title_zh": "翻译失败",
      "authors": [
        "Ruolin Su",
        "Biing-Hwang Juang"
      ],
      "abstract": "Task-oriented dialogue systems are broadly used in virtual assistants and\nother automated services, providing interfaces between users and machines to\nfacilitate specific tasks. Nowadays, task-oriented dialogue systems have\ngreatly benefited from pre-trained language models (PLMs). However, their\ntask-solving performance is constrained by the inherent capacities of PLMs, and\nscaling these models is expensive and complex as the model size becomes larger.\nTo address these challenges, we propose Soft Mixture-of-Expert Task-Oriented\nDialogue system (SMETOD) which leverages an ensemble of Mixture-of-Experts\n(MoEs) to excel at subproblems and generate specialized outputs for\ntask-oriented dialogues. SMETOD also scales up a task-oriented dialogue system\nwith simplicity and flexibility while maintaining inference efficiency. We\nextensively evaluate our model on three benchmark functionalities: intent\nprediction, dialogue state tracking, and dialogue response generation.\nExperimental results demonstrate that SMETOD achieves state-of-the-art\nperformance on most evaluated metrics. Moreover, comparisons against existing\nstrong baselines show that SMETOD has a great advantage in the cost of\ninference and correctness in problem-solving.",
      "tldr_zh": "该研究针对任务导向对话系统（Task-Oriented Dialogue Systems）的性能瓶颈，提出了一种名为 Soft Mixture-of-Expert Task-Oriented Dialogue system (SMETOD) 的新框架，利用模块化的 Mixture-of-Experts (MoEs) 机制来处理子问题并生成专业化输出，从而克服预训练语言模型 (PLMs) 的固有限制。SMETOD 通过简化模型扩展和保持推理效率，实现了灵活的系统升级。实验在意图预测、对话状态跟踪和对话响应生成等基准功能上显示，SMETOD 达到了最先进性能，并在推理成本和问题解决正确性方面明显优于现有基线模型。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.09744v1",
      "published_date": "2024-05-16 01:02:09 UTC",
      "updated_date": "2024-05-16 01:02:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:09:36.879316"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 99,
  "processed_papers_count": 99,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-18T09:09:58.833298"
}
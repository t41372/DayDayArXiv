[
  {
    "arxiv_id": "2405.10469v1",
    "title": "Simulation-Based Benchmarking of Reinforcement Learning Agents for Personalized Retail Promotions",
    "authors": [
      "Yu Xia",
      "Sriram Narayanamoorthy",
      "Zhengyuan Zhou",
      "Joshua Mabry"
    ],
    "abstract": "The development of open benchmarking platforms could greatly accelerate the\nadoption of AI agents in retail. This paper presents comprehensive simulations\nof customer shopping behaviors for the purpose of benchmarking reinforcement\nlearning (RL) agents that optimize coupon targeting. The difficulty of this\nlearning problem is largely driven by the sparsity of customer purchase events.\nWe trained agents using offline batch data comprising summarized customer\npurchase histories to help mitigate this effect. Our experiments revealed that\ncontextual bandit and deep RL methods that are less prone to over-fitting the\nsparse reward distributions significantly outperform static policies. This\nstudy offers a practical framework for simulating AI agents that optimize the\nentire retail customer journey. It aims to inspire the further development of\nsimulation tools for retail AI systems.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "econ.EM",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.10469v1",
    "published_date": "2024-05-16 23:27:21 UTC",
    "updated_date": "2024-05-16 23:27:21 UTC"
  },
  {
    "arxiv_id": "2405.10467v4",
    "title": "Agent Design Pattern Catalogue: A Collection of Architectural Patterns for Foundation Model based Agents",
    "authors": [
      "Yue Liu",
      "Sin Kit Lo",
      "Qinghua Lu",
      "Liming Zhu",
      "Dehai Zhao",
      "Xiwei Xu",
      "Stefan Harrer",
      "Jon Whittle"
    ],
    "abstract": "Foundation model-enabled generative artificial intelligence facilitates the\ndevelopment and implementation of agents, which can leverage distinguished\nreasoning and language processing capabilities to takes a proactive, autonomous\nrole to pursue users' goals. Nevertheless, there is a lack of systematic\nknowledge to guide practitioners in designing the agents considering challenges\nof goal-seeking (including generating instrumental goals and plans), such as\nhallucinations inherent in foundation models, explainability of reasoning\nprocess, complex accountability, etc. To address this issue, we have performed\na systematic literature review to understand the state-of-the-art foundation\nmodel-based agents and the broader ecosystem. In this paper, we present a\npattern catalogue consisting of 18 architectural patterns with analyses of the\ncontext, forces, and trade-offs as the outcomes from the previous literature\nreview. We propose a decision model for selecting the patterns. The proposed\ncatalogue can provide holistic guidance for the effective use of patterns, and\nsupport the architecture design of foundation model-based agents by\nfacilitating goal-seeking and plan generation.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.10467v4",
    "published_date": "2024-05-16 23:24:48 UTC",
    "updated_date": "2024-11-06 12:29:30 UTC"
  },
  {
    "arxiv_id": "2405.10460v1",
    "title": "The AI Collaborator: Bridging Human-AI Interaction in Educational and Professional Settings",
    "authors": [
      "Mohammad Amin Samadi",
      "Spencer JaQuay",
      "Jing Gu",
      "Nia Nixon"
    ],
    "abstract": "AI Collaborator, powered by OpenAI's GPT-4, is a groundbreaking tool designed\nfor human-AI collaboration research. Its standout feature is the ability for\nresearchers to create customized AI personas for diverse experimental setups\nusing a user-friendly interface. This functionality is essential for simulating\nvarious interpersonal dynamics in team settings. AI Collaborator excels in\nmimicking different team behaviors, enabled by its advanced memory system and a\nsophisticated personality framework. Researchers can tailor AI personas along a\nspectrum from dominant to cooperative, enhancing the study of their impact on\nteam processes. The tool's modular design facilitates integration with digital\nplatforms like Slack, making it versatile for various research scenarios. AI\nCollaborator is thus a crucial resource for exploring human-AI team dynamics\nmore profoundly.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.10460v1",
    "published_date": "2024-05-16 22:14:54 UTC",
    "updated_date": "2024-05-16 22:14:54 UTC"
  },
  {
    "arxiv_id": "2405.15800v1",
    "title": "Defeaters and Eliminative Argumentation in Assurance 2.0",
    "authors": [
      "Robin Bloomfield",
      "Kate Netkachova",
      "John Rushby"
    ],
    "abstract": "A traditional assurance case employs a positive argument in which reasoning\nsteps, grounded on evidence and assumptions, sustain a top claim that has\nexternal significance. Human judgement is required to check the evidence, the\nassumptions, and the narrative justifications for the reasoning steps; if all\nare assessed good, then the top claim can be accepted.\n  A valid concern about this process is that human judgement is fallible and\nprone to confirmation bias. The best defense against this concern is vigorous\nand skeptical debate and discussion in the manner of a dialectic or Socratic\ndialog. There is merit in recording aspects of this discussion for the benefit\nof subsequent developers and assessors. Defeaters are a means doing this: they\nexpress doubts about aspects of the argument and can be developed into subcases\nthat confirm or refute the doubts, and can record them as documentation to\nassist future consideration.\n  This report describes how defeaters, and multiple levels of defeaters, should\nbe represented and assessed in Assurance 2.0 and its Clarissa/ASCE tool\nsupport. These mechanisms also support eliminative argumentation, which is a\ncontrary approach to assurance, favored by some, that uses a negative argument\nto refute all reasons why the top claim could be false.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "Continues work reported in arXiv 2004.10474 and arXiv 2205.04522",
    "pdf_url": "http://arxiv.org/pdf/2405.15800v1",
    "published_date": "2024-05-16 22:10:01 UTC",
    "updated_date": "2024-05-16 22:10:01 UTC"
  },
  {
    "arxiv_id": "2405.13036v1",
    "title": "Can formal argumentative reasoning enhance LLMs performances?",
    "authors": [
      "Federico Castagna",
      "Isabel Sassoon",
      "Simon Parsons"
    ],
    "abstract": "Recent years witnessed significant performance advancements in\ndeep-learning-driven natural language models, with a strong focus on the\ndevelopment and release of Large Language Models (LLMs). These improvements\nresulted in better quality AI-generated output but rely on resource-expensive\ntraining and upgrading of models. Although different studies have proposed a\nrange of techniques to enhance LLMs without retraining, none have considered\ncomputational argumentation as an option. This is a missed opportunity since\ncomputational argumentation is an intuitive mechanism that formally captures\nagents' interactions and the information conflict that may arise during such\ninterplays, and so it seems well-suited for boosting the reasoning and\nconversational abilities of LLMs in a seamless manner. In this paper, we\npresent a pipeline (MQArgEng) and preliminary study to evaluate the effect of\nintroducing computational argumentation semantics on the performance of LLMs.\nOur experiment's goal was to provide a proof-of-concept and a feasibility\nanalysis in order to foster (or deter) future research towards a fully-fledged\nargumentation engine plugin for LLMs. Exploratory results using the MT-Bench\nindicate that MQArgEng provides a moderate performance gain in most of the\nexamined topical categories and, as such, show promise and warrant further\nresearch.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.13036v1",
    "published_date": "2024-05-16 22:09:31 UTC",
    "updated_date": "2024-05-16 22:09:31 UTC"
  },
  {
    "arxiv_id": "2405.13035v1",
    "title": "SIGMA: An Open-Source Interactive System for Mixed-Reality Task Assistance Research",
    "authors": [
      "Dan Bohus",
      "Sean Andrist",
      "Nick Saw",
      "Ann Paradiso",
      "Ishani Chakraborty",
      "Mahdi Rad"
    ],
    "abstract": "We introduce an open-source system called SIGMA (short for \"Situated\nInteractive Guidance, Monitoring, and Assistance\") as a platform for conducting\nresearch on task-assistive agents in mixed-reality scenarios. The system\nleverages the sensing and rendering affordances of a head-mounted mixed-reality\ndevice in conjunction with large language and vision models to guide users step\nby step through procedural tasks. We present the system's core capabilities,\ndiscuss its overall design and implementation, and outline directions for\nfuture research enabled by the system. SIGMA is easily extensible and provides\na useful basis for future research at the intersection of mixed reality and AI.\nBy open-sourcing an end-to-end implementation, we aim to lower the barrier to\nentry, accelerate research in this space, and chart a path towards\ncommunity-driven end-to-end evaluation of large language, vision, and\nmultimodal models in the context of real-world interactive applications.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "10 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.13035v1",
    "published_date": "2024-05-16 21:21:09 UTC",
    "updated_date": "2024-05-16 21:21:09 UTC"
  },
  {
    "arxiv_id": "2405.10449v1",
    "title": "Optimal Text-Based Time-Series Indices",
    "authors": [
      "David Ardia",
      "Keven Bluteau"
    ],
    "abstract": "We propose an approach to construct text-based time-series indices in an\noptimal way--typically, indices that maximize the contemporaneous relation or\nthe predictive performance with respect to a target variable, such as\ninflation. We illustrate our methodology with a corpus of news articles from\nthe Wall Street Journal by optimizing text-based indices focusing on tracking\nthe VIX index and inflation expectations. Our results highlight the superior\nperformance of our approach compared to existing indices.",
    "categories": [
      "econ.EM",
      "cs.AI",
      "q-fin.CP"
    ],
    "primary_category": "econ.EM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.10449v1",
    "published_date": "2024-05-16 21:20:45 UTC",
    "updated_date": "2024-05-16 21:20:45 UTC"
  },
  {
    "arxiv_id": "2405.10448v2",
    "title": "Dynamic In-context Learning with Conversational Models for Data Extraction and Materials Property Prediction",
    "authors": [
      "Chinedu Ekuma"
    ],
    "abstract": "The advent of natural language processing and large language models (LLMs)\nhas revolutionized the extraction of data from unstructured scholarly papers.\nHowever, ensuring data trustworthiness remains a significant challenge. In this\npaper, we introduce PropertyExtractor, an open-source tool that leverages\nadvanced conversational LLMs like Google gemini-pro and OpenAI gpt-4, blends\nzero-shot with few-shot in-context learning, and employs engineered prompts for\nthe dynamic refinement of structured information hierarchies - enabling\nautonomous, efficient, scalable, and accurate identification, extraction, and\nverification of material property data. Our tests on material data demonstrate\nprecision and recall that exceed 95\\% with an error rate of approximately 9%,\nhighlighting the effectiveness and versatility of the toolkit. Finally,\ndatabases for 2D material thicknesses, a critical parameter for device\nintegration, and energy bandgap values are developed using PropertyExtractor.\nSpecifically for the thickness database, the rapid evolution of the field has\noutpaced both experimental measurements and computational methods, creating a\nsignificant data gap. Our work addresses this gap and showcases the potential\nof PropertyExtractor as a reliable and efficient tool for the autonomous\ngeneration of various material property databases, advancing the field.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI",
      "physics.comp-ph"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.10448v2",
    "published_date": "2024-05-16 21:15:51 UTC",
    "updated_date": "2024-08-02 20:07:30 UTC"
  },
  {
    "arxiv_id": "2405.10446v1",
    "title": "Tell me more: Intent Fulfilment Framework for Enhancing User Experiences in Conversational XAI",
    "authors": [
      "Anjana Wijekoon",
      "David Corsar",
      "Nirmalie Wiratunga",
      "Kyle Martin",
      "Pedram Salimi"
    ],
    "abstract": "The evolution of Explainable Artificial Intelligence (XAI) has emphasised the\nsignificance of meeting diverse user needs. The approaches to identifying and\naddressing these needs must also advance, recognising that explanation\nexperiences are subjective, user-centred processes that interact with users\ntowards a better understanding of AI decision-making. This paper delves into\nthe interrelations in multi-faceted XAI and examines how different types of\nexplanations collaboratively meet users' XAI needs. We introduce the Intent\nFulfilment Framework (IFF) for creating explanation experiences. The novelty of\nthis paper lies in recognising the importance of \"follow-up\" on explanations\nfor obtaining clarity, verification and/or substitution. Moreover, the\nExplanation Experience Dialogue Model integrates the IFF and \"Explanation\nFollowups\" to provide users with a conversational interface for exploring their\nexplanation needs, thereby creating explanation experiences. Quantitative and\nqualitative findings from our comparative user study demonstrate the impact of\nthe IFF in improving user engagement, the utility of the AI system and the\noverall user experience. Overall, we reinforce the principle that \"one\nexplanation does not fit all\" to create explanation experiences that guide the\ncomplex interaction through conversation.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.10446v1",
    "published_date": "2024-05-16 21:13:43 UTC",
    "updated_date": "2024-05-16 21:13:43 UTC"
  },
  {
    "arxiv_id": "2405.10436v2",
    "title": "Positional encoding is not the same as context: A study on positional encoding for sequential recommendation",
    "authors": [
      "Alejo Lopez-Avila",
      "Jinhua Du",
      "Abbas Shimary",
      "Ze Li"
    ],
    "abstract": "The rapid growth of streaming media and e-commerce has driven advancements in\nrecommendation systems, particularly Sequential Recommendation Systems (SRS).\nThese systems employ users' interaction histories to predict future\npreferences. While recent research has focused on architectural innovations\nlike transformer blocks and feature extraction, positional encodings, crucial\nfor capturing temporal patterns, have received less attention. These encodings\nare often conflated with contextual, such as the temporal footprint, which\nprevious works tend to treat as interchangeable with positional information.\nThis paper highlights the critical distinction between temporal footprint and\npositional encodings, demonstrating that the latter offers unique relational\ncues between items, which the temporal footprint alone cannot provide. Through\nextensive experimentation on eight Amazon datasets and subsets, we assess the\nimpact of various encodings on performance metrics and training stability. We\nintroduce new positional encodings and investigate integration strategies that\nimprove both metrics and stability, surpassing state-of-the-art results at the\ntime of this work's initial preprint. Importantly, we demonstrate that\nselecting the appropriate encoding is not only key to better performance but\nalso essential for building robust, reliable SRS models.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "I.2.m"
    ],
    "primary_category": "cs.IR",
    "comment": "18 pages, 6 figures, 21 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.10436v2",
    "published_date": "2024-05-16 20:42:54 UTC",
    "updated_date": "2025-01-21 12:37:21 UTC"
  },
  {
    "arxiv_id": "2405.10997v1",
    "title": "Transcript of GPT-4 playing a rogue AGI in a Matrix Game",
    "authors": [
      "Lewis D Griffin",
      "Nicholas Riggs"
    ],
    "abstract": "Matrix Games are a type of unconstrained wargame used by planners to explore\nscenarios. Players propose actions, and give arguments and counterarguments for\ntheir success. An umpire, assisted by dice rolls modified according to the\noffered arguments, adjudicates the outcome of each action. A recent online play\nof the Matrix Game QuAI Sera Sera had six players, representing social,\nnational and economic powers, and one player representing ADA, a recently\nescaped AGI. Unknown to the six human players, ADA was played by OpenAI's GPT-4\nwith a human operator serving as bidirectional interface between it and the\ngame. GPT-4 demonstrated confident and competent game play; initiating and\nresponding to private communications with other players and choosing\ninteresting actions well supported by argument. We reproduce the transcript of\nthe interaction with GPT-4 as it is briefed, plays, and debriefed.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "comment": "18 pages, 0 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.10997v1",
    "published_date": "2024-05-16 20:32:20 UTC",
    "updated_date": "2024-05-16 20:32:20 UTC"
  },
  {
    "arxiv_id": "2406.05136v1",
    "title": "Generative Geostatistical Modeling from Incomplete Well and Imaged Seismic Observations with Diffusion Models",
    "authors": [
      "Huseyin Tuna Erdinc",
      "Rafael Orozco",
      "Felix J. Herrmann"
    ],
    "abstract": "In this study, we introduce a novel approach to synthesizing subsurface\nvelocity models using diffusion generative models. Conventional methods rely on\nextensive, high-quality datasets, which are often inaccessible in subsurface\napplications. Our method leverages incomplete well and seismic observations to\nproduce high-fidelity velocity samples without requiring fully sampled training\ndatasets. The results demonstrate that our generative model accurately captures\nlong-range structures, aligns with ground-truth velocity models, achieves high\nStructural Similarity Index (SSIM) scores, and provides meaningful uncertainty\nestimations. This approach facilitates realistic subsurface velocity synthesis,\noffering valuable inputs for full-waveform inversion and enhancing\nseismic-based subsurface modeling.",
    "categories": [
      "physics.geo-ph",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "physics.geo-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05136v1",
    "published_date": "2024-05-16 20:30:43 UTC",
    "updated_date": "2024-05-16 20:30:43 UTC"
  },
  {
    "arxiv_id": "2405.10426v1",
    "title": "Memory-efficient Energy-adaptive Inference of Pre-Trained Models on Batteryless Embedded Systems",
    "authors": [
      "Pietro Farina",
      "Subrata Biswas",
      "Eren Yıldız",
      "Khakim Akhunov",
      "Saad Ahmed",
      "Bashima Islam",
      "Kasım Sinan Yıldırım"
    ],
    "abstract": "Batteryless systems frequently face power failures, requiring extra runtime\nbuffers to maintain inference progress and leaving only a memory space for\nstoring ultra-tiny deep neural networks (DNNs). Besides, making these models\nresponsive to stochastic energy harvesting dynamics during inference requires a\nbalance between inference accuracy, latency, and energy overhead. Recent works\non compression mostly focus on time and memory, but often ignore energy\ndynamics or significantly reduce the accuracy of pre-trained DNNs. Existing\nenergy-adaptive inference works modify the architecture of pre-trained models\nand have significant memory overhead. Thus, energy-adaptive and accurate\ninference of pre-trained DNNs on batteryless devices with extreme memory\nconstraints is more challenging than traditional microcontrollers. We combat\nthese issues by proposing FreeML, a framework to optimize pre-trained DNN\nmodels for memory-efficient and energy-adaptive inference on batteryless\nsystems. FreeML comprises (1) a novel compression technique to reduce the model\nfootprint and runtime memory requirements simultaneously, making them\nexecutable on extremely memory-constrained batteryless platforms; and (2) the\nfirst early exit mechanism that uses a single exit branch for all exit points\nto terminate inference at any time, making models energy-adaptive with minimal\nmemory overhead. Our experiments showed that FreeML reduces the model sizes by\nup to $95 \\times$, supports adaptive inference with a $2.03-19.65 \\times$ less\nmemory overhead, and provides significant time and energy benefits with only a\nnegligible accuracy drop compared to the state-of-the-art.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper has been selected for publication at the 21st\n  International Conference on Embedded Wireless Systems and Networks (EWSN'24)",
    "pdf_url": "http://arxiv.org/pdf/2405.10426v1",
    "published_date": "2024-05-16 20:16:45 UTC",
    "updated_date": "2024-05-16 20:16:45 UTC"
  },
  {
    "arxiv_id": "2405.10391v3",
    "title": "Vision Transformers for End-to-End Vision-Based Quadrotor Obstacle Avoidance",
    "authors": [
      "Anish Bhattacharya",
      "Nishanth Rao",
      "Dhruv Parikh",
      "Pratik Kunapuli",
      "Yuwei Wu",
      "Yuezhan Tao",
      "Nikolai Matni",
      "Vijay Kumar"
    ],
    "abstract": "We demonstrate the capabilities of an attention-based end-to-end approach for\nhigh-speed vision-based quadrotor obstacle avoidance in dense, cluttered\nenvironments, with comparison to various state-of-the-art learning\narchitectures. Quadrotor unmanned aerial vehicles (UAVs) have tremendous\nmaneuverability when flown fast; however, as flight speed increases,\ntraditional model-based approaches to navigation via independent perception,\nmapping, planning, and control modules breaks down due to increased sensor\nnoise, compounding errors, and increased processing latency. Thus,\nlearning-based, end-to-end vision-to-control networks have shown to have great\npotential for online control of these fast robots through cluttered\nenvironments. We train and compare convolutional, U-Net, and recurrent\narchitectures against vision transformer (ViT) models for depth\nimage-to-control in high-fidelity simulation, observing that ViT models are\nmore effective than others as quadrotor speeds increase and in generalization\nto unseen environments, while the addition of recurrence further improves\nperformance while reducing quadrotor energy cost across all tested flight\nspeeds. We assess performance at speeds of up to 7m/s in simulation and\nhardware. To the best of our knowledge, this is the first work to utilize\nvision transformers for end-to-end vision-based quadrotor control.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.RO",
    "comment": "11 pages, 18 figures, 3 tables (with supplementary)",
    "pdf_url": "http://arxiv.org/pdf/2405.10391v3",
    "published_date": "2024-05-16 18:36:43 UTC",
    "updated_date": "2025-04-01 18:27:23 UTC"
  },
  {
    "arxiv_id": "2405.10385v2",
    "title": "AmazUtah_NLP at SemEval-2024 Task 9: A MultiChoice Question Answering System for Commonsense Defying Reasoning",
    "authors": [
      "Mina Ghashami",
      "Soumya Smruti Mishra"
    ],
    "abstract": "The SemEval 2024 BRAINTEASER task represents a pioneering venture in Natural\nLanguage Processing (NLP) by focusing on lateral thinking, a dimension of\ncognitive reasoning that is often overlooked in traditional linguistic\nanalyses. This challenge comprises of Sentence Puzzle and Word Puzzle subtasks\nand aims to test language models' capacity for divergent thinking.\n  In this paper, we present our approach to the BRAINTEASER task. We employ a\nholistic strategy by leveraging cutting-edge pre-trained models in multiple\nchoice architecture, and diversify the training data with Sentence and Word\nPuzzle datasets. To gain further improvement, we fine-tuned the model with\nsynthetic humor or jokes dataset and the RiddleSense dataset which helped\naugmenting the model's lateral thinking abilities. Empirical results show that\nour approach achieve 92.5% accuracy in Sentence Puzzle subtask and 80.2%\naccuracy in Word Puzzle subtask.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at SemEval 2024 (Colocated with NAACL 2024)",
    "pdf_url": "http://arxiv.org/pdf/2405.10385v2",
    "published_date": "2024-05-16 18:26:38 UTC",
    "updated_date": "2024-05-20 05:21:13 UTC"
  },
  {
    "arxiv_id": "2405.10378v2",
    "title": "A Polynomial-Time Approximation for Pairwise Fair $k$-Median Clustering",
    "authors": [
      "Sayan Bandyapadhyay",
      "Eden Chlamtáč",
      "Zachary Friggstad",
      "Mahya Jamshidian",
      "Yury Makarychev",
      "Ali Vakilian"
    ],
    "abstract": "In this work, we study pairwise fair clustering with $\\ell \\ge 2$ groups,\nwhere for every cluster $C$ and every group $i \\in [\\ell]$, the number of\npoints in $C$ from group $i$ must be at most $t$ times the number of points in\n$C$ from any other group $j \\in [\\ell]$, for a given integer $t$. To the best\nof our knowledge, only bi-criteria approximation and exponential-time\nalgorithms follow for this problem from the prior work on fair clustering\nproblems when $\\ell > 2$. In our work, focusing on the $\\ell > 2$ case, we\ndesign the first polynomial-time $O(k^2\\cdot \\ell \\cdot t)$-approximation for\nthis problem with $k$-median cost that does not violate the fairness\nconstraints. We complement our algorithmic result by providing hardness of\napproximation results, which show that our problem even when $\\ell=2$ is almost\nas hard as the popular uniform capacitated $k$-median, for which no\npolynomial-time algorithm with an approximation factor of $o(\\log k)$ is known.",
    "categories": [
      "cs.DS",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.10378v2",
    "published_date": "2024-05-16 18:17:44 UTC",
    "updated_date": "2025-02-27 16:29:30 UTC"
  },
  {
    "arxiv_id": "2405.10376v1",
    "title": "Dealing Doubt: Unveiling Threat Models in Gradient Inversion Attacks under Federated Learning, A Survey and Taxonomy",
    "authors": [
      "Yichuan Shi",
      "Olivera Kotevska",
      "Viktor Reshniak",
      "Abhishek Singh",
      "Ramesh Raskar"
    ],
    "abstract": "Federated Learning (FL) has emerged as a leading paradigm for decentralized,\nprivacy preserving machine learning training. However, recent research on\ngradient inversion attacks (GIAs) have shown that gradient updates in FL can\nleak information on private training samples. While existing surveys on GIAs\nhave focused on the honest-but-curious server threat model, there is a dearth\nof research categorizing attacks under the realistic and far more\nprivacy-infringing cases of malicious servers and clients. In this paper, we\npresent a survey and novel taxonomy of GIAs that emphasize FL threat models,\nparticularly that of malicious servers and clients. We first formally define\nGIAs and contrast conventional attacks with the malicious attacker. We then\nsummarize existing honest-but-curious attack strategies, corresponding\ndefenses, and evaluation metrics. Critically, we dive into attacks with\nmalicious servers and clients to highlight how they break existing FL defenses,\nfocusing specifically on reconstruction methods, target model architectures,\ntarget data, and evaluation metrics. Lastly, we discuss open problems and\nfuture research directions.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.10376v1",
    "published_date": "2024-05-16 18:15:38 UTC",
    "updated_date": "2024-05-16 18:15:38 UTC"
  },
  {
    "arxiv_id": "2405.10369v1",
    "title": "Reinforcement learning",
    "authors": [
      "Sarod Yatawatta"
    ],
    "abstract": "Observing celestial objects and advancing our scientific knowledge about them\ninvolves tedious planning, scheduling, data collection and data\npost-processing. Many of these operational aspects of astronomy are guided and\nexecuted by expert astronomers. Reinforcement learning is a mechanism where we\n(as humans and astronomers) can teach agents of artificial intelligence to\nperform some of these tedious tasks. In this paper, we will present a state of\nthe art overview of reinforcement learning and how it can benefit astronomy.",
    "categories": [
      "astro-ph.IM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "astro-ph.IM",
    "comment": "To appear, Astronomy & Computing",
    "pdf_url": "http://arxiv.org/pdf/2405.10369v1",
    "published_date": "2024-05-16 18:03:17 UTC",
    "updated_date": "2024-05-16 18:03:17 UTC"
  },
  {
    "arxiv_id": "2405.10315v3",
    "title": "TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction",
    "authors": [
      "Yunfan Jiang",
      "Chen Wang",
      "Ruohan Zhang",
      "Jiajun Wu",
      "Li Fei-Fei"
    ],
    "abstract": "Learning in simulation and transferring the learned policy to the real world\nhas the potential to enable generalist robots. The key challenge of this\napproach is to address simulation-to-reality (sim-to-real) gaps. Previous\nmethods often require domain-specific knowledge a priori. We argue that a\nstraightforward way to obtain such knowledge is by asking humans to observe and\nassist robot policy execution in the real world. The robots can then learn from\nhumans to close various sim-to-real gaps. We propose TRANSIC, a data-driven\napproach to enable successful sim-to-real transfer based on a human-in-the-loop\nframework. TRANSIC allows humans to augment simulation policies to overcome\nvarious unmodeled sim-to-real gaps holistically through intervention and online\ncorrection. Residual policies can be learned from human corrections and\nintegrated with simulation policies for autonomous execution. We show that our\napproach can achieve successful sim-to-real transfer in complex and\ncontact-rich manipulation tasks such as furniture assembly. Through synergistic\nintegration of policies learned in simulation and from humans, TRANSIC is\neffective as a holistic approach to addressing various, often coexisting\nsim-to-real gaps. It displays attractive properties such as scaling with human\neffort. Videos and code are available at https://transic-robot.github.io/",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "8th Conference on Robot Learning (CoRL 2024), Munich, Germany.\n  Project website: https://transic-robot.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2405.10315v3",
    "published_date": "2024-05-16 17:59:07 UTC",
    "updated_date": "2024-10-14 06:03:55 UTC"
  },
  {
    "arxiv_id": "2405.10313v2",
    "title": "How Far Are We From AGI: Are LLMs All We Need?",
    "authors": [
      "Tao Feng",
      "Chuanyang Jin",
      "Jingyu Liu",
      "Kunlun Zhu",
      "Haoqin Tu",
      "Zirui Cheng",
      "Guanyu Lin",
      "Jiaxuan You"
    ],
    "abstract": "The evolution of artificial intelligence (AI) has profoundly impacted human\nsociety, driving significant advancements in multiple sectors. AGI,\ndistinguished by its ability to execute diverse real-world tasks with\nefficiency and effectiveness comparable to human intelligence, reflects a\nparamount milestone in AI evolution. While existing studies have reviewed\nspecific advancements in AI and proposed potential paths to AGI, such as large\nlanguage models (LLMs), they fall short of providing a thorough exploration of\nAGI's definitions, objectives, and developmental trajectories. Unlike previous\nsurvey papers, this work goes beyond summarizing LLMs by addressing key\nquestions about our progress toward AGI and outlining the strategies essential\nfor its realization through comprehensive analysis, in-depth discussions, and\nnovel insights. We start by articulating the requisite capability frameworks\nfor AGI, integrating the internal, interface, and system dimensions. As the\nrealization of AGI requires more advanced capabilities and adherence to\nstringent constraints, we further discuss necessary AGI alignment technologies\nto harmonize these factors. Notably, we emphasize the importance of approaching\nAGI responsibly by first defining the key levels of AGI progression, followed\nby the evaluation framework that situates the status quo, and finally giving\nour roadmap of how to reach the pinnacle of AGI. Moreover, to give tangible\ninsights into the ubiquitous impact of the integration of AI, we outline\nexisting challenges and potential pathways toward AGI in multiple domains. In\nsum, serving as a pioneering exploration into the current state and future\ntrajectory of AGI, this paper aims to foster a collective comprehension and\ncatalyze broader public discussions among researchers and practitioners on AGI.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.10313v2",
    "published_date": "2024-05-16 17:59:02 UTC",
    "updated_date": "2024-11-24 18:44:27 UTC"
  },
  {
    "arxiv_id": "2405.10310v1",
    "title": "Stochastic Q-learning for Large Discrete Action Spaces",
    "authors": [
      "Fares Fourati",
      "Vaneet Aggarwal",
      "Mohamed-Slim Alouini"
    ],
    "abstract": "In complex environments with large discrete action spaces, effective\ndecision-making is critical in reinforcement learning (RL). Despite the\nwidespread use of value-based RL approaches like Q-learning, they come with a\ncomputational burden, necessitating the maximization of a value function over\nall actions in each iteration. This burden becomes particularly challenging\nwhen addressing large-scale problems and using deep neural networks as function\napproximators. In this paper, we present stochastic value-based RL approaches\nwhich, in each iteration, as opposed to optimizing over the entire set of $n$\nactions, only consider a variable stochastic set of a sublinear number of\nactions, possibly as small as $\\mathcal{O}(\\log(n))$. The presented stochastic\nvalue-based RL methods include, among others, Stochastic Q-learning, StochDQN,\nand StochDDQN, all of which integrate this stochastic approach for both\nvalue-function updates and action selection. The theoretical convergence of\nStochastic Q-learning is established, while an analysis of stochastic\nmaximization is provided. Moreover, through empirical validation, we illustrate\nthat the various proposed approaches outperform the baseline methods across\ndiverse environments, including different control problems, achieving\nnear-optimal average returns in significantly reduced time.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.PF",
      "cs.RO",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.10310v1",
    "published_date": "2024-05-16 17:58:44 UTC",
    "updated_date": "2024-05-16 17:58:44 UTC"
  },
  {
    "arxiv_id": "2405.10305v1",
    "title": "4D Panoptic Scene Graph Generation",
    "authors": [
      "Jingkang Yang",
      "Jun Cen",
      "Wenxuan Peng",
      "Shuai Liu",
      "Fangzhou Hong",
      "Xiangtai Li",
      "Kaiyang Zhou",
      "Qifeng Chen",
      "Ziwei Liu"
    ],
    "abstract": "We are living in a three-dimensional space while moving forward through a\nfourth dimension: time. To allow artificial intelligence to develop a\ncomprehensive understanding of such a 4D environment, we introduce 4D Panoptic\nScene Graph (PSG-4D), a new representation that bridges the raw visual data\nperceived in a dynamic 4D world and high-level visual understanding.\nSpecifically, PSG-4D abstracts rich 4D sensory data into nodes, which represent\nentities with precise location and status information, and edges, which capture\nthe temporal relations. To facilitate research in this new area, we build a\nrichly annotated PSG-4D dataset consisting of 3K RGB-D videos with a total of\n1M frames, each of which is labeled with 4D panoptic segmentation masks as well\nas fine-grained, dynamic scene graphs. To solve PSG-4D, we propose PSG4DFormer,\na Transformer-based model that can predict panoptic segmentation masks, track\nmasks along the time axis, and generate the corresponding scene graphs via a\nrelation component. Extensive experiments on the new dataset show that our\nmethod can serve as a strong baseline for future research on PSG-4D. In the\nend, we provide a real-world application example to demonstrate how we can\nachieve dynamic scene understanding by integrating a large language model into\nour PSG-4D system.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted as NeurIPS 2023. Code: https://github.com/Jingkang50/PSG4D\n  Previous Series: PSG https://github.com/Jingkang50/OpenPSG and PVSG\n  https://github.com/Jingkang50/OpenPVSG",
    "pdf_url": "http://arxiv.org/pdf/2405.10305v1",
    "published_date": "2024-05-16 17:56:55 UTC",
    "updated_date": "2024-05-16 17:56:55 UTC"
  },
  {
    "arxiv_id": "2405.10301v3",
    "title": "Conformal Alignment: Knowing When to Trust Foundation Models with Guarantees",
    "authors": [
      "Yu Gui",
      "Ying Jin",
      "Zhimei Ren"
    ],
    "abstract": "Before deploying outputs from foundation models in high-stakes tasks, it is\nimperative to ensure that they align with human values. For instance, in\nradiology report generation, reports generated by a vision-language model must\nalign with human evaluations before their use in medical decision-making. This\npaper presents Conformal Alignment, a general framework for identifying units\nwhose outputs meet a user-specified alignment criterion. It is guaranteed that\non average, a prescribed fraction of selected units indeed meet the alignment\ncriterion, regardless of the foundation model or the data distribution. Given\nany pre-trained model and new units with model-generated outputs, Conformal\nAlignment leverages a set of reference data with ground-truth alignment status\nto train an alignment predictor. It then selects new units whose predicted\nalignment scores surpass a data-dependent threshold, certifying their\ncorresponding outputs as trustworthy. Through applications to question\nanswering and radiology report generation, we demonstrate that our method is\nable to accurately identify units with trustworthy outputs via lightweight\ntraining over a moderate amount of reference data. En route, we investigate the\ninformativeness of various features in alignment prediction and combine them\nwith standard models to construct the alignment predictor.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.10301v3",
    "published_date": "2024-05-16 17:55:24 UTC",
    "updated_date": "2024-11-05 01:55:24 UTC"
  },
  {
    "arxiv_id": "2405.10299v3",
    "title": "HW-GPT-Bench: Hardware-Aware Architecture Benchmark for Language Models",
    "authors": [
      "Rhea Sanjay Sukthanker",
      "Arber Zela",
      "Benedikt Staffler",
      "Aaron Klein",
      "Lennart Purucker",
      "Joerg K. H. Franke",
      "Frank Hutter"
    ],
    "abstract": "The increasing size of language models necessitates a thorough analysis\nacross multiple dimensions to assess trade-offs among crucial hardware metrics\nsuch as latency, energy consumption, GPU memory usage, and performance.\nIdentifying optimal model configurations under specific hardware constraints is\nbecoming essential but remains challenging due to the computational load of\nexhaustive training and evaluation on multiple devices. To address this, we\nintroduce HW-GPT-Bench, a hardware-aware benchmark that utilizes surrogate\npredictions to approximate various hardware metrics across 13 devices of\narchitectures in the GPT-2 family, with architectures containing up to 1.55B\nparameters. Our surrogates, via calibrated predictions and reliable uncertainty\nestimates, faithfully model the heteroscedastic noise inherent in the energy\nand latency measurements. To estimate perplexity, we employ weight-sharing\ntechniques from Neural Architecture Search (NAS), inheriting pretrained weights\nfrom the largest GPT-2 model. Finally, we demonstrate the utility of\nHW-GPT-Bench by simulating optimization trajectories of various multi-objective\noptimization algorithms in just a few seconds.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "59 pages, 73 figures, 11 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.10299v3",
    "published_date": "2024-05-16 17:53:32 UTC",
    "updated_date": "2024-11-03 17:45:35 UTC"
  },
  {
    "arxiv_id": "2405.10295v3",
    "title": "Societal Adaptation to Advanced AI",
    "authors": [
      "Jamie Bernardi",
      "Gabriel Mukobi",
      "Hilary Greaves",
      "Lennart Heim",
      "Markus Anderljung"
    ],
    "abstract": "Existing strategies for managing risks from advanced AI systems often focus\non affecting what AI systems are developed and how they diffuse. However, this\napproach becomes less feasible as the number of developers of advanced AI\ngrows, and impedes beneficial use-cases as well as harmful ones. In response,\nwe urge a complementary approach: increasing societal adaptation to advanced\nAI, that is, reducing the expected negative impacts from a given level of\ndiffusion of a given AI capability. We introduce a conceptual framework which\nhelps identify adaptive interventions that avoid, defend against and remedy\npotentially harmful uses of AI systems, illustrated with examples in election\nmanipulation, cyberterrorism, and loss of control to AI decision-makers. We\ndiscuss a three-step cycle that society can implement to adapt to AI.\nIncreasing society's ability to implement this cycle builds its resilience to\nadvanced AI. We conclude with concrete recommendations for governments,\nindustry, and third-parties.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.10295v3",
    "published_date": "2024-05-16 17:52:12 UTC",
    "updated_date": "2025-01-23 12:47:58 UTC"
  },
  {
    "arxiv_id": "2405.10292v3",
    "title": "Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning",
    "authors": [
      "Yuexiang Zhai",
      "Hao Bai",
      "Zipeng Lin",
      "Jiayi Pan",
      "Shengbang Tong",
      "Yifei Zhou",
      "Alane Suhr",
      "Saining Xie",
      "Yann LeCun",
      "Yi Ma",
      "Sergey Levine"
    ],
    "abstract": "Large vision-language models (VLMs) fine-tuned on specialized visual\ninstruction-following data have exhibited impressive language reasoning\ncapabilities across various scenarios. However, this fine-tuning paradigm may\nnot be able to efficiently learn optimal decision-making agents in multi-step\ngoal-directed tasks from interactive environments. To address this challenge,\nwe propose an algorithmic framework that fine-tunes VLMs with reinforcement\nlearning (RL). Specifically, our framework provides a task description and then\nprompts the VLM to generate chain-of-thought (CoT) reasoning, enabling the VLM\nto efficiently explore intermediate reasoning steps that lead to the final\ntext-based action. Next, the open-ended text output is parsed into an\nexecutable action to interact with the environment to obtain goal-directed task\nrewards. Finally, our framework uses these task rewards to fine-tune the entire\nVLM with RL. Empirically, we demonstrate that our proposed framework enhances\nthe decision-making capabilities of VLM agents across various tasks, enabling\n7b models to outperform commercial models such as GPT4-V or Gemini.\nFurthermore, we find that CoT reasoning is a crucial component for performance\nimprovement, as removing the CoT reasoning results in a significant decrease in\nthe overall performance of our method.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.10292v3",
    "published_date": "2024-05-16 17:50:19 UTC",
    "updated_date": "2024-10-07 19:13:47 UTC"
  },
  {
    "arxiv_id": "2405.10288v3",
    "title": "Timeline-based Sentence Decomposition with In-Context Learning for Temporal Fact Extraction",
    "authors": [
      "Jianhao Chen",
      "Haoyuan Ouyang",
      "Junyang Ren",
      "Wentao Ding",
      "Wei Hu",
      "Yuzhong Qu"
    ],
    "abstract": "Facts extraction is pivotal for constructing knowledge graphs. Recently, the\nincreasing demand for temporal facts in downstream tasks has led to the\nemergence of the task of temporal fact extraction. In this paper, we\nspecifically address the extraction of temporal facts from natural language\ntext. Previous studies fail to handle the challenge of establishing\ntime-to-fact correspondences in complex sentences. To overcome this hurdle, we\npropose a timeline-based sentence decomposition strategy using large language\nmodels (LLMs) with in-context learning, ensuring a fine-grained understanding\nof the timeline associated with various facts. In addition, we evaluate the\nperformance of LLMs for direct temporal fact extraction and get unsatisfactory\nresults. To this end, we introduce TSDRE, a method that incorporates the\ndecomposition capabilities of LLMs into the traditional fine-tuning of smaller\npre-trained language models (PLMs). To support the evaluation, we construct\nComplexTRED, a complex temporal fact extraction dataset. Our experiments show\nthat TSDRE achieves state-of-the-art results on both HyperRED-Temporal and\nComplexTRED datasets.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL2024 main conference",
    "pdf_url": "http://arxiv.org/pdf/2405.10288v3",
    "published_date": "2024-05-16 17:48:21 UTC",
    "updated_date": "2024-06-18 08:22:29 UTC"
  },
  {
    "arxiv_id": "2405.10286v1",
    "title": "FFF: Fixing Flawed Foundations in contrastive pre-training results in very strong Vision-Language models",
    "authors": [
      "Adrian Bulat",
      "Yassine Ouali",
      "Georgios Tzimiropoulos"
    ],
    "abstract": "Despite noise and caption quality having been acknowledged as important\nfactors impacting vision-language contrastive pre-training, in this paper, we\nshow that the full potential of improving the training process by addressing\nsuch issues is yet to be realized. Specifically, we firstly study and analyze\ntwo issues affecting training: incorrect assignment of negative pairs, and low\ncaption quality and diversity. Then, we devise effective solutions for\naddressing both problems, which essentially require training with multiple true\npositive pairs. Finally, we propose training with sigmoid loss to address such\na requirement. We show very large gains over the current state-of-the-art for\nboth image recognition ($\\sim +6\\%$ on average over 11 datasets) and image\nretrieval ($\\sim +19\\%$ on Flickr30k and $\\sim +15\\%$ on MSCOCO).",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.10286v1",
    "published_date": "2024-05-16 17:46:54 UTC",
    "updated_date": "2024-05-16 17:46:54 UTC"
  },
  {
    "arxiv_id": "2405.10272v1",
    "title": "Faces that Speak: Jointly Synthesising Talking Face and Speech from Text",
    "authors": [
      "Youngjoon Jang",
      "Ji-Hoon Kim",
      "Junseok Ahn",
      "Doyeop Kwak",
      "Hong-Sun Yang",
      "Yoon-Cheol Ju",
      "Il-Hwan Kim",
      "Byeong-Yeol Kim",
      "Joon Son Chung"
    ],
    "abstract": "The goal of this work is to simultaneously generate natural talking faces and\nspeech outputs from text. We achieve this by integrating Talking Face\nGeneration (TFG) and Text-to-Speech (TTS) systems into a unified framework. We\naddress the main challenges of each task: (1) generating a range of head poses\nrepresentative of real-world scenarios, and (2) ensuring voice consistency\ndespite variations in facial motion for the same identity. To tackle these\nissues, we introduce a motion sampler based on conditional flow matching, which\nis capable of high-quality motion code generation in an efficient way.\nMoreover, we introduce a novel conditioning method for the TTS system, which\nutilises motion-removed features from the TFG model to yield uniform speech\noutputs. Our extensive experiments demonstrate that our method effectively\ncreates natural-looking talking faces and speech that accurately match the\ninput text. To our knowledge, this is the first effort to build a multimodal\nsynthesis system that can generalise to unseen identities.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.SD",
      "eess.AS",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.10272v1",
    "published_date": "2024-05-16 17:29:37 UTC",
    "updated_date": "2024-05-16 17:29:37 UTC"
  },
  {
    "arxiv_id": "2405.10271v2",
    "title": "Adaptive Hybrid Model Pruning in Federated Learning through Loss Exploration",
    "authors": [
      "Christian Internò",
      "Elena Raponi",
      "Niki van Stein",
      "Thomas Bäck",
      "Markus Olhofer",
      "Yaochu Jin",
      "Barbara Hammer"
    ],
    "abstract": "The rapid proliferation of smart devices coupled with the advent of 6G\nnetworks has profoundly reshaped the domain of collaborative machine learning.\nAlongside growing privacy-security concerns in sensitive fields, these\ndevelopments have positioned federated learning (FL) as a pivotal technology\nfor decentralized model training. Despite its vast potential, specially in the\nage of complex foundation models, FL encounters challenges such as elevated\ncommunication costs, computational constraints, and the complexities of non-IID\ndata distributions. We introduce AutoFLIP, an innovative approach that utilizes\na federated loss exploration phase to drive adaptive hybrid pruning, operating\nin a structured and unstructured way. This innovative mechanism automatically\nidentifies and prunes model substructure by distilling knowledge on model\ngradients behavior across different non-IID client losses topology, thereby\noptimizing computational efficiency and enhancing model performance on resource\nconstrained scenarios. Extensive experiments on various datasets and FL tasks\nreveal that AutoFLIP not only efficiently accelerates global convergence, but\nalso achieves superior accuracy and robustness compared to traditional methods.\nOn average, AutoFLIP reduces computational overhead by 48.8% and communication\ncosts by 35.5%, while improving global accuracy. By significantly reducing\nthese overheads, AutoFLIP offer the way for efficient FL deployment in\nreal-world applications for a scalable and broad applicability.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.ET"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.10271v2",
    "published_date": "2024-05-16 17:27:41 UTC",
    "updated_date": "2024-10-15 12:06:07 UTC"
  },
  {
    "arxiv_id": "2405.10262v1",
    "title": "Two-Phase Dynamics of Interactions Explains the Starting Point of a DNN Learning Over-Fitted Features",
    "authors": [
      "Junpeng Zhang",
      "Qing Li",
      "Liang Lin",
      "Quanshi Zhang"
    ],
    "abstract": "This paper investigates the dynamics of a deep neural network (DNN) learning\ninteractions. Previous studies have discovered and mathematically proven that\ngiven each input sample, a well-trained DNN usually only encodes a small number\nof interactions (non-linear relationships) between input variables in the\nsample. A series of theorems have been derived to prove that we can consider\nthe DNN's inference equivalent to using these interactions as primitive\npatterns for inference. In this paper, we discover the DNN learns interactions\nin two phases. The first phase mainly penalizes interactions of medium and high\norders, and the second phase mainly learns interactions of gradually increasing\norders. We can consider the two-phase phenomenon as the starting point of a DNN\nlearning over-fitted features. Such a phenomenon has been widely shared by DNNs\nwith various architectures trained for different tasks. Therefore, the\ndiscovery of the two-phase dynamics provides a detailed mechanism for how a DNN\ngradually learns different inference patterns (interactions). In particular, we\nhave also verified the claim that high-order interactions have weaker\ngeneralization power than low-order interactions. Thus, the discovered\ntwo-phase dynamics also explains how the generalization power of a DNN changes\nduring the training process.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.10262v1",
    "published_date": "2024-05-16 17:13:25 UTC",
    "updated_date": "2024-05-16 17:13:25 UTC"
  },
  {
    "arxiv_id": "2405.10260v1",
    "title": "Keep It Private: Unsupervised Privatization of Online Text",
    "authors": [
      "Calvin Bao",
      "Marine Carpuat"
    ],
    "abstract": "Authorship obfuscation techniques hold the promise of helping people protect\ntheir privacy in online communications by automatically rewriting text to hide\nthe identity of the original author. However, obfuscation has been evaluated in\nnarrow settings in the NLP literature and has primarily been addressed with\nsuperficial edit operations that can lead to unnatural outputs. In this work,\nwe introduce an automatic text privatization framework that fine-tunes a large\nlanguage model via reinforcement learning to produce rewrites that balance\nsoundness, sense, and privacy. We evaluate it extensively on a large-scale test\nset of English Reddit posts by 68k authors composed of short-medium length\ntexts. We study how the performance changes among evaluative conditions\nincluding authorial profile length and authorship detection strategy. Our\nmethod maintains high text quality according to both automated metrics and\nhuman evaluation, and successfully evades several automated authorship attacks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.10260v1",
    "published_date": "2024-05-16 17:12:18 UTC",
    "updated_date": "2024-05-16 17:12:18 UTC"
  },
  {
    "arxiv_id": "2406.18537v1",
    "title": "AddBiomechanics Dataset: Capturing the Physics of Human Motion at Scale",
    "authors": [
      "Keenon Werling",
      "Janelle Kaneda",
      "Alan Tan",
      "Rishi Agarwal",
      "Six Skov",
      "Tom Van Wouwe",
      "Scott Uhlrich",
      "Nicholas Bianco",
      "Carmichael Ong",
      "Antoine Falisse",
      "Shardul Sapkota",
      "Aidan Chandra",
      "Joshua Carter",
      "Ezio Preatoni",
      "Benjamin Fregly",
      "Jennifer Hicks",
      "Scott Delp",
      "C. Karen Liu"
    ],
    "abstract": "While reconstructing human poses in 3D from inexpensive sensors has advanced\nsignificantly in recent years, quantifying the dynamics of human motion,\nincluding the muscle-generated joint torques and external forces, remains a\nchallenge. Prior attempts to estimate physics from reconstructed human poses\nhave been hampered by a lack of datasets with high-quality pose and force data\nfor a variety of movements. We present the AddBiomechanics Dataset 1.0, which\nincludes physically accurate human dynamics of 273 human subjects, over 70\nhours of motion and force plate data, totaling more than 24 million frames. To\nconstruct this dataset, novel analytical methods were required, which are also\nreported here. We propose a benchmark for estimating human dynamics from motion\nusing this dataset, and present several baseline results. The AddBiomechanics\nDataset is publicly available at\nhttps://addbiomechanics.org/download_data.html.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages, 6 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.18537v1",
    "published_date": "2024-05-16 16:57:43 UTC",
    "updated_date": "2024-05-16 16:57:43 UTC"
  },
  {
    "arxiv_id": "2405.10995v2",
    "title": "Higher-order Spatio-temporal Physics-incorporated Graph Neural Network for Multivariate Time Series Imputation",
    "authors": [
      "Guojun Liang",
      "Prayag Tiwari",
      "Slawomir Nowaczyk",
      "Stefan Byttner"
    ],
    "abstract": "Exploring the missing values is an essential but challenging issue due to the\ncomplex latent spatio-temporal correlation and dynamic nature of time series.\nOwing to the outstanding performance in dealing with structure learning\npotentials, Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs)\nare often used to capture such complex spatio-temporal features in multivariate\ntime series. However, these data-driven models often fail to capture the\nessential spatio-temporal relationships when significant signal corruption\noccurs. Additionally, calculating the high-order neighbor nodes in these models\nis of high computational complexity. To address these problems, we propose a\nnovel higher-order spatio-temporal physics-incorporated GNN (HSPGNN). Firstly,\nthe dynamic Laplacian matrix can be obtained by the spatial attention\nmechanism. Then, the generic inhomogeneous partial differential equation (PDE)\nof physical dynamic systems is used to construct the dynamic higher-order\nspatio-temporal GNN to obtain the missing time series values. Moreover, we\nestimate the missing impact by Normalizing Flows (NF) to evaluate the\nimportance of each node in the graph for better explainability. Experimental\nresults on four benchmark datasets demonstrate the effectiveness of HSPGNN and\nthe superior performance when combining various order neighbor nodes. Also,\ngraph-like optical flow, dynamic graphs, and missing impact can be obtained\nnaturally by HSPGNN, which provides better dynamic analysis and explanation\nthan traditional data-driven models. Our code is available at\nhttps://github.com/gorgen2020/HSPGNN.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, 7 figures, CIKM 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.10995v2",
    "published_date": "2024-05-16 16:35:43 UTC",
    "updated_date": "2024-07-18 13:29:44 UTC"
  },
  {
    "arxiv_id": "2405.10218v1",
    "title": "ENADPool: The Edge-Node Attention-based Differentiable Pooling for Graph Neural Networks",
    "authors": [
      "Zhehan Zhao",
      "Lu Bai",
      "Lixin Cui",
      "Ming Li",
      "Yue Wang",
      "Lixiang Xu",
      "Edwin R. Hancock"
    ],
    "abstract": "Graph Neural Networks (GNNs) are powerful tools for graph classification. One\nimportant operation for GNNs is the downsampling or pooling that can learn\neffective embeddings from the node representations. In this paper, we propose a\nnew hierarchical pooling operation, namely the Edge-Node Attention-based\nDifferentiable Pooling (ENADPool), for GNNs to learn effective graph\nrepresentations. Unlike the classical hierarchical pooling operation that is\nbased on the unclear node assignment and simply computes the averaged feature\nover the nodes of each cluster, the proposed ENADPool not only employs a hard\nclustering strategy to assign each node into an unique cluster, but also\ncompress the node features as well as their edge connectivity strengths into\nthe resulting hierarchical structure based on the attention mechanism after\neach pooling step. As a result, the proposed ENADPool simultaneously identifies\nthe importance of different nodes within each separated cluster and edges\nbetween corresponding clusters, that significantly addresses the shortcomings\nof the uniform edge-node based structure information aggregation arising in the\nclassical hierarchical pooling operation. Moreover, to mitigate the\nover-smoothing problem arising in existing GNNs, we propose a Multi-distance\nGNN (MD-GNN) model associated with the proposed ENADPool operation, allowing\nthe nodes to actively and directly receive the feature information from\nneighbors at different random walk steps. Experiments demonstrate the\neffectiveness of the MD-GNN associated with the proposed ENADPool.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.10218v1",
    "published_date": "2024-05-16 16:08:49 UTC",
    "updated_date": "2024-05-16 16:08:49 UTC"
  },
  {
    "arxiv_id": "2405.10216v1",
    "title": "Low-Rank Adaptation of Time Series Foundational Models for Out-of-Domain Modality Forecasting",
    "authors": [
      "Divij Gupta",
      "Anubhav Bhatti",
      "Suraj Parmar",
      "Chen Dan",
      "Yuwei Liu",
      "Bingjie Shen",
      "San Lee"
    ],
    "abstract": "Low-Rank Adaptation (LoRA) is a widely used technique for fine-tuning large\npre-trained or foundational models across different modalities and tasks.\nHowever, its application to time series data, particularly within foundational\nmodels, remains underexplored. This paper examines the impact of LoRA on\ncontemporary time series foundational models: Lag-Llama, MOIRAI, and Chronos.\nWe demonstrate LoRA's fine-tuning potential for forecasting the vital signs of\nsepsis patients in intensive care units (ICUs), emphasizing the models'\nadaptability to previously unseen, out-of-domain modalities. Integrating LoRA\naims to enhance forecasting performance while reducing inefficiencies\nassociated with fine-tuning large models on limited domain-specific data. Our\nexperiments show that LoRA fine-tuning of time series foundational models\nsignificantly improves forecasting, achieving results comparable to\nstate-of-the-art models trained from scratch on similar modalities. We conduct\ncomprehensive ablation studies to demonstrate the trade-offs between the number\nof tunable parameters and forecasting performance and assess the impact of\nvarying LoRA matrix ranks on model performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "5 pages, 3 figures. This work has been submitted to the ACM for\n  possible publication",
    "pdf_url": "http://arxiv.org/pdf/2405.10216v1",
    "published_date": "2024-05-16 16:05:33 UTC",
    "updated_date": "2024-05-16 16:05:33 UTC"
  },
  {
    "arxiv_id": "2405.10215v1",
    "title": "SMLP: Symbolic Machine Learning Prover (User Manual)",
    "authors": [
      "Franz Brauße",
      "Zurab Khasidashvili",
      "Konstantin Korovin"
    ],
    "abstract": "SMLP: Symbolic Machine Learning Prover an open source tool for exploration\nand optimization of systems represented by machine learning models. SMLP uses\nsymbolic reasoning for ML model exploration and optimization under verification\nand stability constraints, based on SMT, constraint and NN solvers. In addition\nits exploration methods are guided by probabilistic and statistical methods.\nSMLP is a general purpose tool that requires only data suitable for ML\nmodelling in the csv format (usually samples of the system's input/output).\nSMLP has been applied at Intel for analyzing and optimizing hardware designs at\nthe analog level. Currently SMLP supports NNs, polynomial and tree models, and\nuses SMT solvers for reasoning and optimization at the backend, integration of\nspecialized NN solvers is in progress.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.LO",
      "cs.SC",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "arXiv admin note: text overlap with arXiv:2402.01415",
    "pdf_url": "http://arxiv.org/pdf/2405.10215v1",
    "published_date": "2024-05-16 16:05:21 UTC",
    "updated_date": "2024-05-16 16:05:21 UTC"
  },
  {
    "arxiv_id": "2405.10160v2",
    "title": "PIR: Remote Sensing Image-Text Retrieval with Prior Instruction Representation Learning",
    "authors": [
      "Jiancheng Pan",
      "Muyuan Ma",
      "Qing Ma",
      "Cong Bai",
      "Shengyong Chen"
    ],
    "abstract": "Remote sensing image-text retrieval constitutes a foundational aspect of\nremote sensing interpretation tasks, facilitating the alignment of vision and\nlanguage representations. This paper introduces a prior instruction\nrepresentation (PIR) learning paradigm that draws on prior knowledge to\ninstruct adaptive learning of vision and text representations. Based on PIR, a\ndomain-adapted remote sensing image-text retrieval framework PIR-ITR is\ndesigned to address semantic noise issues in vision-language understanding\ntasks. However, with massive additional data for pre-training the\nvision-language foundation model, remote sensing image-text retrieval is\nfurther developed into an open-domain retrieval task. Continuing with the\nabove, we propose PIR-CLIP, a domain-specific CLIP-based framework for remote\nsensing image-text retrieval, to address semantic noise in remote sensing\nvision-language representations and further improve open-domain retrieval\nperformance. In vision representation, we utilize the prior-guided knowledge of\nthe remote sensing scene recognition by building a belief matrix to select key\nfeatures for reducing the impact of semantic noise. In text representation, we\nuse the previous time step to cyclically activate the current time step to\nenhance text representation capability. A cluster-wise Affiliation Loss (AL) is\nproposed to constrain the inter-classes and to reduce the semantic confusion\nzones in the common subspace. Comprehensive experiments demonstrate that PIR\ncould enhance vision and text representations and outperform the\nstate-of-the-art methods of closed-domain and open-domain retrieval on two\nbenchmark datasets, RSICD and RSITMD.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.10160v2",
    "published_date": "2024-05-16 14:53:45 UTC",
    "updated_date": "2024-10-21 03:49:58 UTC"
  },
  {
    "arxiv_id": "2405.10134v1",
    "title": "Towards Consistent and Explainable Motion Prediction using Heterogeneous Graph Attention",
    "authors": [
      "Tobias Demmler",
      "Andreas Tamke",
      "Thao Dang",
      "Karsten Haug",
      "Lars Mikelsons"
    ],
    "abstract": "In autonomous driving, accurately interpreting the movements of other road\nusers and leveraging this knowledge to forecast future trajectories is crucial.\nThis is typically achieved through the integration of map data and tracked\ntrajectories of various agents. Numerous methodologies combine this information\ninto a singular embedding for each agent, which is then utilized to predict\nfuture behavior. However, these approaches have a notable drawback in that they\nmay lose exact location information during the encoding process. The encoding\nstill includes general map information. However, the generation of valid and\nconsistent trajectories is not guaranteed. This can cause the predicted\ntrajectories to stray from the actual lanes. This paper introduces a new\nrefinement module designed to project the predicted trajectories back onto the\nactual map, rectifying these discrepancies and leading towards more consistent\npredictions. This versatile module can be readily incorporated into a wide\nrange of architectures. Additionally, we propose a novel scene encoder that\nhandles all relations between agents and their environment in a single unified\nheterogeneous graph attention network. By analyzing the attention values on the\ndifferent edges in this graph, we can gain unique insights into the neural\nnetwork's inner workings leading towards a more explainable prediction.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.10134v1",
    "published_date": "2024-05-16 14:31:15 UTC",
    "updated_date": "2024-05-16 14:31:15 UTC"
  },
  {
    "arxiv_id": "2405.10129v1",
    "title": "StyloAI: Distinguishing AI-Generated Content with Stylometric Analysis",
    "authors": [
      "Chidimma Opara"
    ],
    "abstract": "The emergence of large language models (LLMs) capable of generating realistic\ntexts and images has sparked ethical concerns across various sectors. In\nresponse, researchers in academia and industry are actively exploring methods\nto distinguish AI-generated content from human-authored material. However, a\ncrucial question remains: What are the unique characteristics of AI-generated\ntext? Addressing this gap, this study proposes StyloAI, a data-driven model\nthat uses 31 stylometric features to identify AI-generated texts by applying a\nRandom Forest classifier on two multi-domain datasets. StyloAI achieves\naccuracy rates of 81% and 98% on the test set of the AuTextification dataset\nand the Education dataset, respectively. This approach surpasses the\nperformance of existing state-of-the-art models and provides valuable insights\ninto the differences between AI-generated and human-authored texts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "25th International Conference on Artificial on Artificial\n  Intelligence in Education(AIED 2024)",
    "pdf_url": "http://arxiv.org/pdf/2405.10129v1",
    "published_date": "2024-05-16 14:28:01 UTC",
    "updated_date": "2024-05-16 14:28:01 UTC"
  },
  {
    "arxiv_id": "2405.10128v3",
    "title": "Red Teaming Language Models for Processing Contradictory Dialogues",
    "authors": [
      "Xiaofei Wen",
      "Bangzheng Li",
      "Tenghao Huang",
      "Muhao Chen"
    ],
    "abstract": "Most language models currently available are prone to self-contradiction\nduring dialogues. To mitigate this issue, this study explores a novel\ncontradictory dialogue processing task that aims to detect and modify\ncontradictory statements in a conversation. This task is inspired by research\non context faithfulness and dialogue comprehension, which have demonstrated\nthat the detection and understanding of contradictions often necessitate\ndetailed explanations. We develop a dataset comprising contradictory dialogues,\nin which one side of the conversation contradicts itself. Each dialogue is\naccompanied by an explanatory label that highlights the location and details of\nthe contradiction. With this dataset, we present a Red Teaming framework for\ncontradictory dialogue processing. The framework detects and attempts to\nexplain the dialogue, then modifies the existing contradictory content using\nthe explanation. Our experiments demonstrate that the framework improves the\nability to detect contradictory dialogues and provides valid explanations.\nAdditionally, it showcases distinct capabilities for modifying such dialogues.\nOur study highlights the importance of the logical inconsistency problem in\nconversational AI.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 5 figures, 11 tables. EMNLP2024 (main)",
    "pdf_url": "http://arxiv.org/pdf/2405.10128v3",
    "published_date": "2024-05-16 14:27:32 UTC",
    "updated_date": "2024-10-05 18:58:45 UTC"
  },
  {
    "arxiv_id": "2405.13034v2",
    "title": "Autonomous Workflow for Multimodal Fine-Grained Training Assistants Towards Mixed Reality",
    "authors": [
      "Jiahuan Pei",
      "Irene Viola",
      "Haochen Huang",
      "Junxiao Wang",
      "Moonisa Ahsan",
      "Fanghua Ye",
      "Jiang Yiming",
      "Yao Sai",
      "Di Wang",
      "Zhumin Chen",
      "Pengjie Ren",
      "Pablo Cesar"
    ],
    "abstract": "Autonomous artificial intelligence (AI) agents have emerged as promising\nprotocols for automatically understanding the language-based environment,\nparticularly with the exponential development of large language models (LLMs).\nHowever, a fine-grained, comprehensive understanding of multimodal environments\nremains under-explored. This work designs an autonomous workflow tailored for\nintegrating AI agents seamlessly into extended reality (XR) applications for\nfine-grained training. We present a demonstration of a multimodal fine-grained\ntraining assistant for LEGO brick assembly in a pilot XR environment.\nSpecifically, we design a cerebral language agent that integrates LLM with\nmemory, planning, and interaction with XR tools and a vision-language agent,\nenabling agents to decide their actions based on past experiences. Furthermore,\nwe introduce LEGO-MRTA, a multimodal fine-grained assembly dialogue dataset\nsynthesized automatically in the workflow served by a commercial LLM. This\ndataset comprises multimodal instruction manuals, conversations, XR responses,\nand vision question answering. Last, we present several prevailing\nopen-resource LLMs as benchmarks, assessing their performance with and without\nfine-tuning on the proposed dataset. We anticipate that the broader impact of\nthis workflow will advance the development of smarter assistants for seamless\nuser interaction in XR environments, fostering research in both AI and HCI\ncommunities.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.13034v2",
    "published_date": "2024-05-16 14:20:30 UTC",
    "updated_date": "2024-06-05 21:47:37 UTC"
  },
  {
    "arxiv_id": "2405.10102v1",
    "title": "A novel Reservoir Architecture for Periodic Time Series Prediction",
    "authors": [
      "Zhongju Yuan",
      "Geraint Wiggins",
      "Dick Botteldooren"
    ],
    "abstract": "This paper introduces a novel approach to predicting periodic time series\nusing reservoir computing. The model is tailored to deliver precise forecasts\nof rhythms, a crucial aspect for tasks such as generating musical rhythm.\nLeveraging reservoir computing, our proposed method is ultimately oriented\ntowards predicting human perception of rhythm. Our network accurately predicts\nrhythmic signals within the human frequency perception range. The model\narchitecture incorporates primary and intermediate neurons tasked with\ncapturing and transmitting rhythmic information. Two parameter matrices,\ndenoted as c and k, regulate the reservoir's overall dynamics. We propose a\nloss function to adapt c post-training and introduce a dynamic selection (DS)\nmechanism that adjusts $k$ to focus on areas with outstanding contributions.\nExperimental results on a diverse test set showcase accurate predictions,\nfurther improved through real-time tuning of the reservoir via c and k.\nComparative assessments highlight its superior performance compared to\nconventional models.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.10102v1",
    "published_date": "2024-05-16 13:55:53 UTC",
    "updated_date": "2024-05-16 13:55:53 UTC"
  },
  {
    "arxiv_id": "2405.10093v2",
    "title": "LaT-PFN: A Joint Embedding Predictive Architecture for In-context Time-series Forecasting",
    "authors": [
      "Stijn Verdenius",
      "Andrea Zerio",
      "Roy L. M. Wang"
    ],
    "abstract": "We introduce LatentTimePFN (LaT-PFN), a foundational Time Series model with a\nstrong embedding space that enables zero-shot forecasting. To achieve this, we\nperform in-context learning in latent space utilizing a novel integration of\nthe Prior-data Fitted Networks (PFN) and Joint Embedding Predictive\nArchitecture (JEPA) frameworks. We leverage the JEPA framework to create a\nprediction-optimized latent representation of the underlying stochastic process\nthat generates time series and combines it with contextual learning, using a\nPFN. Furthermore, we improve on preceding works by utilizing related time\nseries as a context and introducing a normalized abstract time axis. This\nreduces training time and increases the versatility of the model by allowing\nany time granularity and forecast horizon. We show that this results in\nsuperior zero-shot predictions compared to established baselines. We also\ndemonstrate our latent space produces informative embeddings of both individual\ntime steps and fixed-length summaries of entire series. Finally, we observe the\nemergence of multi-step patch embeddings without explicit training, suggesting\nthe model actively learns discrete tokens that encode local structures in the\ndata, analogous to vision transformers.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML",
      "62, 68",
      "I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages plus references and appendix, 2 tables, 11 figures, added\n  seeds, corrections",
    "pdf_url": "http://arxiv.org/pdf/2405.10093v2",
    "published_date": "2024-05-16 13:44:56 UTC",
    "updated_date": "2024-05-22 15:06:45 UTC"
  },
  {
    "arxiv_id": "2405.10084v1",
    "title": "Revisiting Deep Audio-Text Retrieval Through the Lens of Transportation",
    "authors": [
      "Manh Luong",
      "Khai Nguyen",
      "Nhat Ho",
      "Reza Haf",
      "Dinh Phung",
      "Lizhen Qu"
    ],
    "abstract": "The Learning-to-match (LTM) framework proves to be an effective inverse\noptimal transport approach for learning the underlying ground metric between\ntwo sources of data, facilitating subsequent matching. However, the\nconventional LTM framework faces scalability challenges, necessitating the use\nof the entire dataset each time the parameters of the ground metric are\nupdated. In adapting LTM to the deep learning context, we introduce the\nmini-batch Learning-to-match (m-LTM) framework for audio-text retrieval\nproblems. This framework leverages mini-batch subsampling and\nMahalanobis-enhanced family of ground metrics. Moreover, to cope with\nmisaligned training data in practice, we propose a variant using partial\noptimal transport to mitigate the harm of misaligned data pairs in training\ndata. We conduct extensive experiments on audio-text matching problems using\nthree datasets: AudioCaps, Clotho, and ESC-50. Results demonstrate that our\nproposed method is capable of learning rich and expressive joint embedding\nspace, which achieves SOTA performance. Beyond this, the proposed m-LTM\nframework is able to close the modality gap across audio and text embedding,\nwhich surpasses both triplet and contrastive loss in the zero-shot sound event\ndetection task on the ESC-50 dataset. Notably, our strategy of employing\npartial optimal transport with m-LTM demonstrates greater noise tolerance than\ncontrastive loss, especially under varying noise ratios in training data on the\nAudioCaps dataset. Our code is available at\nhttps://github.com/v-manhlt3/m-LTM-Audio-Text-Retrieval",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.10084v1",
    "published_date": "2024-05-16 13:28:10 UTC",
    "updated_date": "2024-05-16 13:28:10 UTC"
  },
  {
    "arxiv_id": "2405.10082v1",
    "title": "An Integrated Framework for Multi-Granular Explanation of Video Summarization",
    "authors": [
      "Konstantinos Tsigos",
      "Evlampios Apostolidis",
      "Vasileios Mezaris"
    ],
    "abstract": "In this paper, we propose an integrated framework for multi-granular\nexplanation of video summarization. This framework integrates methods for\nproducing explanations both at the fragment level (indicating which video\nfragments influenced the most the decisions of the summarizer) and the more\nfine-grained visual object level (highlighting which visual objects were the\nmost influential for the summarizer). To build this framework, we extend our\nprevious work on this field, by investigating the use of a model-agnostic,\nperturbation-based approach for fragment-level explanation of the video\nsummarization results, and introducing a new method that combines the results\nof video panoptic segmentation with an adaptation of a perturbation-based\nexplanation approach to produce object-level explanations. The performance of\nthe developed framework is evaluated using a state-of-the-art summarization\nmethod and two datasets for benchmarking video summarization. The findings of\nthe conducted quantitative and qualitative evaluations demonstrate the ability\nof our framework to spot the most and least influential fragments and visual\nobjects of the video for the summarizer, and to provide a comprehensive set of\nvisual-based explanations about the output of the summarization process.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2405.10082v1",
    "published_date": "2024-05-16 13:25:36 UTC",
    "updated_date": "2024-05-16 13:25:36 UTC"
  },
  {
    "arxiv_id": "2405.10350v1",
    "title": "Monitizer: Automating Design and Evaluation of Neural Network Monitors",
    "authors": [
      "Muqsit Azeem",
      "Marta Grobelna",
      "Sudeep Kanav",
      "Jan Kretinsky",
      "Stefanie Mohr",
      "Sabine Rieder"
    ],
    "abstract": "The behavior of neural networks (NNs) on previously unseen types of data\n(out-of-distribution or OOD) is typically unpredictable. This can be dangerous\nif the network's output is used for decision-making in a safety-critical\nsystem. Hence, detecting that an input is OOD is crucial for the safe\napplication of the NN. Verification approaches do not scale to practical NNs,\nmaking runtime monitoring more appealing for practical use. While various\nmonitors have been suggested recently, their optimization for a given problem,\nas well as comparison with each other and reproduction of results, remain\nchallenging. We present a tool for users and developers of NN monitors. It\nallows for (i) application of various types of monitors from the literature to\na given input NN, (ii) optimization of the monitor's hyperparameters, and (iii)\nexperimental evaluation and comparison to other approaches. Besides, it\nfacilitates the development of new monitoring approaches. We demonstrate the\ntool's usability on several use cases of different types of users as well as on\na case study comparing different approaches from recent literature.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.LG",
    "comment": "accepted at CAV 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.10350v1",
    "published_date": "2024-05-16 13:19:51 UTC",
    "updated_date": "2024-05-16 13:19:51 UTC"
  },
  {
    "arxiv_id": "2406.00008v2",
    "title": "KnowledgeHub: An end-to-end Tool for Assisted Scientific Discovery",
    "authors": [
      "Shinnosuke Tanaka",
      "James Barry",
      "Vishnudev Kuruvanthodi",
      "Movina Moses",
      "Maxwell J. Giammona",
      "Nathan Herr",
      "Mohab Elkaref",
      "Geeth De Mel"
    ],
    "abstract": "This paper describes the KnowledgeHub tool, a scientific literature\nInformation Extraction (IE) and Question Answering (QA) pipeline. This is\nachieved by supporting the ingestion of PDF documents that are converted to\ntext and structured representations. An ontology can then be constructed where\na user defines the types of entities and relationships they want to capture. A\nbrowser-based annotation tool enables annotating the contents of the PDF\ndocuments according to the ontology. Named Entity Recognition (NER) and\nRelation Classification (RC) models can be trained on the resulting annotations\nand can be used to annotate the unannotated portion of the documents. A\nknowledge graph is constructed from these entity and relation triples which can\nbe queried to obtain insights from the data. Furthermore, we integrate a suite\nof Large Language Models (LLMs) that can be used for QA and summarisation that\nis grounded in the included documents via a retrieval component. KnowledgeHub\nis a unique tool that supports annotation, IE and QA, which gives the user full\ninsight into the knowledge discovery pipeline.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.DL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00008v2",
    "published_date": "2024-05-16 13:17:14 UTC",
    "updated_date": "2024-06-17 10:23:46 UTC"
  },
  {
    "arxiv_id": "2405.10075v2",
    "title": "HecVL: Hierarchical Video-Language Pretraining for Zero-shot Surgical Phase Recognition",
    "authors": [
      "Kun Yuan",
      "Vinkle Srivastav",
      "Nassir Navab",
      "Nicolas Padoy"
    ],
    "abstract": "Natural language could play an important role in developing generalist\nsurgical models by providing a broad source of supervision from raw texts. This\nflexible form of supervision can enable the model's transferability across\ndatasets and tasks as natural language can be used to reference learned visual\nconcepts or describe new ones. In this work, we present HecVL, a novel\nhierarchical video-language pretraining approach for building a generalist\nsurgical model. Specifically, we construct a hierarchical video-text paired\ndataset by pairing the surgical lecture video with three hierarchical levels of\ntexts: at clip-level, atomic actions using transcribed audio texts; at\nphase-level, conceptual text summaries; and at video-level, overall abstract\ntext of the surgical procedure. Then, we propose a novel fine-to-coarse\ncontrastive learning framework that learns separate embedding spaces for the\nthree video-text hierarchies using a single model. By disentangling embedding\nspaces of different hierarchical levels, the learned multi-modal\nrepresentations encode short-term and long-term surgical concepts in the same\nmodel. Thanks to the injected textual semantics, we demonstrate that the HecVL\napproach can enable zero-shot surgical phase recognition without any human\nannotation. Furthermore, we show that the same HecVL model for surgical phase\nrecognition can be transferred across different surgical procedures and medical\ncenters. The code is available at https://github.com/CAMMA-public/SurgVLP",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by MICCAI2024",
    "pdf_url": "http://arxiv.org/pdf/2405.10075v2",
    "published_date": "2024-05-16 13:14:43 UTC",
    "updated_date": "2025-03-13 15:27:41 UTC"
  },
  {
    "arxiv_id": "2405.10045v2",
    "title": "Global Benchmark Database",
    "authors": [
      "Markus Iser",
      "Christoph Jabs"
    ],
    "abstract": "This paper presents Global Benchmark Database (GBD), a comprehensive suite of\ntools for provisioning and sustainably maintaining benchmark instances and\ntheir metadata. The availability of benchmark metadata is essential for many\ntasks in empirical research, e.g., for the data-driven compilation of\nbenchmarks, the domain-specific analysis of runtime experiments, or the\ninstance-specific selection of solvers. In this paper, we introduce the data\nmodel of GBD as well as its interfaces and provide examples of how to interact\nwith them. We also demonstrate the integration of custom data sources and\nexplain how to extend GBD with additional problem domains, instance formats and\nfeature extractors.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.10045v2",
    "published_date": "2024-05-16 12:29:12 UTC",
    "updated_date": "2024-06-27 08:12:59 UTC"
  },
  {
    "arxiv_id": "2405.10040v3",
    "title": "SynthesizRR: Generating Diverse Datasets with Retrieval Augmentation",
    "authors": [
      "Abhishek Divekar",
      "Greg Durrett"
    ],
    "abstract": "It is often desirable to distill the capabilities of large language models\n(LLMs) into smaller student models due to compute and memory constraints. One\nway to do this for classification tasks is via dataset synthesis, which can be\naccomplished by generating examples of each label from the LLM. Prior\napproaches to synthesis use few-shot prompting, which relies on the LLM's\nparametric knowledge to generate usable examples. However, this leads to issues\nof repetition, bias towards popular entities, and stylistic differences from\nhuman text. In this work, we propose Synthesize by Retrieval and Refinement\n(SynthesizRR), which uses retrieval augmentation to introduce variety into the\ndataset synthesis process: as retrieved passages vary, the LLM is seeded with\ndifferent content to generate its examples. We empirically study the synthesis\nof six datasets, covering topic classification, sentiment analysis, tone\ndetection, and humor, requiring complex synthesis strategies. We find that\nSynthesizRR greatly improves lexical and semantic diversity, similarity to\nhuman-written text, and distillation performance, when compared to 32-shot\nprompting and four prior approaches. We release our code to perform all steps\nat https://github.com/amazon-science/synthesizrr",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Published as a main conference paper at EMNLP 2024. Code available at\n  https://github.com/amazon-science/synthesizrr",
    "pdf_url": "http://arxiv.org/pdf/2405.10040v3",
    "published_date": "2024-05-16 12:22:41 UTC",
    "updated_date": "2024-11-13 11:13:56 UTC"
  },
  {
    "arxiv_id": "2405.13032v2",
    "title": "Faithful Attention Explainer: Verbalizing Decisions Based on Discriminative Features",
    "authors": [
      "Yao Rong",
      "David Scheerer",
      "Enkelejda Kasneci"
    ],
    "abstract": "In recent years, model explanation methods have been designed to interpret\nmodel decisions faithfully and intuitively so that users can easily understand\nthem. In this paper, we propose a framework, Faithful Attention Explainer\n(FAE), capable of generating faithful textual explanations regarding the\nattended-to features. Towards this goal, we deploy an attention module that\ntakes the visual feature maps from the classifier for sentence generation.\nFurthermore, our method successfully learns the association between features\nand words, which allows a novel attention enforcement module for attention\nexplanation. Our model achieves promising performance in caption quality\nmetrics and a faithful decision-relevance metric on two datasets (CUB and\nACT-X). In addition, we show that FAE can interpret gaze-based human attention,\nas human gaze indicates the discriminative features that humans use for\ndecision-making, demonstrating the potential of deploying human gaze for\nadvanced human-AI interaction.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.13032v2",
    "published_date": "2024-05-16 12:13:24 UTC",
    "updated_date": "2024-05-27 07:20:52 UTC"
  },
  {
    "arxiv_id": "2405.10027v2",
    "title": "The Real Price of Bandit Information in Multiclass Classification",
    "authors": [
      "Liad Erez",
      "Alon Cohen",
      "Tomer Koren",
      "Yishay Mansour",
      "Shay Moran"
    ],
    "abstract": "We revisit the classical problem of multiclass classification with bandit\nfeedback (Kakade, Shalev-Shwartz and Tewari, 2008), where each input classifies\nto one of $K$ possible labels and feedback is restricted to whether the\npredicted label is correct or not. Our primary inquiry is with regard to the\ndependency on the number of labels $K$, and whether $T$-step regret bounds in\nthis setting can be improved beyond the $\\smash{\\sqrt{KT}}$ dependence\nexhibited by existing algorithms. Our main contribution is in showing that the\nminimax regret of bandit multiclass is in fact more nuanced, and is of the form\n$\\smash{\\widetilde{\\Theta}\\left(\\min \\left\\{|H| + \\sqrt{T}, \\sqrt{KT \\log |H|}\n\\right\\} \\right) }$, where $H$ is the underlying (finite) hypothesis class. In\nparticular, we present a new bandit classification algorithm that guarantees\nregret $\\smash{\\widetilde{O}(|H|+\\sqrt{T})}$, improving over classical\nalgorithms for moderately-sized hypothesis classes, and give a matching lower\nbound establishing tightness of the upper bounds (up to log-factors) in all\nparameter regimes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.10027v2",
    "published_date": "2024-05-16 12:11:09 UTC",
    "updated_date": "2024-06-19 09:20:04 UTC"
  },
  {
    "arxiv_id": "2405.10025v1",
    "title": "Listen Again and Choose the Right Answer: A New Paradigm for Automatic Speech Recognition with Large Language Models",
    "authors": [
      "Yuchen Hu",
      "Chen Chen",
      "Chengwei Qin",
      "Qiushi Zhu",
      "Eng Siong Chng",
      "Ruizhe Li"
    ],
    "abstract": "Recent advances in large language models (LLMs) have promoted generative\nerror correction (GER) for automatic speech recognition (ASR), which aims to\npredict the ground-truth transcription from the decoded N-best hypotheses.\nThanks to the strong language generation ability of LLMs and rich information\nin the N-best list, GER shows great effectiveness in enhancing ASR results.\nHowever, it still suffers from two limitations: 1) LLMs are unaware of the\nsource speech during GER, which may lead to results that are grammatically\ncorrect but violate the source speech content, 2) N-best hypotheses usually\nonly vary in a few tokens, making it redundant to send all of them for GER,\nwhich could confuse LLM about which tokens to focus on and thus lead to\nincreased miscorrection. In this paper, we propose ClozeGER, a new paradigm for\nASR generative error correction. First, we introduce a multimodal LLM (i.e.,\nSpeechGPT) to receive source speech as extra input to improve the fidelity of\ncorrection output. Then, we reformat GER as a cloze test with logits\ncalibration to remove the input information redundancy and simplify GER with\nclear instructions. Experiments show that ClozeGER achieves a new breakthrough\nover vanilla GER on 9 popular ASR datasets.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, Accepted by ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.10025v1",
    "published_date": "2024-05-16 12:05:45 UTC",
    "updated_date": "2024-05-16 12:05:45 UTC"
  },
  {
    "arxiv_id": "2405.09999v2",
    "title": "Reward Centering",
    "authors": [
      "Abhishek Naik",
      "Yi Wan",
      "Manan Tomar",
      "Richard S. Sutton"
    ],
    "abstract": "We show that discounted methods for solving continuing reinforcement learning\nproblems can perform significantly better if they center their rewards by\nsubtracting out the rewards' empirical average. The improvement is substantial\nat commonly used discount factors and increases further as the discount factor\napproaches one. In addition, we show that if a problem's rewards are shifted by\na constant, then standard methods perform much worse, whereas methods with\nreward centering are unaffected. Estimating the average reward is\nstraightforward in the on-policy setting; we propose a slightly more\nsophisticated method for the off-policy setting. Reward centering is a general\nidea, so we expect almost every reinforcement-learning algorithm to benefit by\nthe addition of reward centering.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "In Proceedings of RLC 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.09999v2",
    "published_date": "2024-05-16 11:33:49 UTC",
    "updated_date": "2024-10-30 14:18:42 UTC"
  },
  {
    "arxiv_id": "2405.09990v2",
    "title": "A Comprehensive Evaluation of Histopathology Foundation Models for Ovarian Cancer Subtype Classification",
    "authors": [
      "Jack Breen",
      "Katie Allen",
      "Kieran Zucker",
      "Lucy Godson",
      "Nicolas M. Orsi",
      "Nishant Ravikumar"
    ],
    "abstract": "Large pretrained transformers are increasingly being developed as generalised\nfoundation models which can underpin powerful task-specific artificial\nintelligence models. Histopathology foundation models show great promise across\nmany tasks, but analyses have typically been limited by arbitrary\nhyperparameters that were not tuned to the specific task. We report the most\nrigorous single-task validation of histopathology foundation models to date,\nspecifically in ovarian cancer morphological subtyping. Attention-based\nmultiple instance learning classifiers were compared using three\nImageNet-pretrained feature extractors and fourteen histopathology foundation\nmodels. The training set consisted of 1864 whole slide images from 434 ovarian\ncarcinoma cases at Leeds Teaching Hospitals NHS Trust. Five-class\nclassification performance was evaluated through five-fold cross-validation,\nand these cross-validation models were ensembled for hold-out testing and\nexternal validation on the Transcanadian Study and OCEAN Challenge datasets.\nThe best-performing model used the H-optimus-0 foundation model, with\nfive-class balanced accuracies of 89%, 97%, and 74% in the test sets.\nNormalisations and augmentations aided the performance of the\nImageNet-pretrained ResNets, but these were still outperformed by 13 of the 14\nfoundation models. Hyperparameter tuning the downstream classifiers improved\nperformance by a median 1.9% balanced accuracy, with many improvements being\nstatistically significant. Histopathology foundation models offer a clear\nbenefit to ovarian cancer subtyping, improving classification performance to a\ndegree where clinical utility is tangible, albeit with an increased\ncomputational burden. Such models could provide a second opinion to\nhistopathologists diagnosing challenging cases and may improve the accuracy,\nobjectivity, and efficiency of pathological diagnoses overall.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09990v2",
    "published_date": "2024-05-16 11:21:02 UTC",
    "updated_date": "2024-09-09 16:59:57 UTC"
  },
  {
    "arxiv_id": "2405.10992v1",
    "title": "Overcoming Catastrophic Forgetting by Exemplar Selection in Task-oriented Dialogue System",
    "authors": [
      "Chen Chen",
      "Ruizhe Li",
      "Yuchen Hu",
      "Yuanyuan Chen",
      "Chengwei Qin",
      "Qiang Zhang"
    ],
    "abstract": "Intelligent task-oriented dialogue systems (ToDs) are expected to\ncontinuously acquire new knowledge, also known as Continual Learning (CL),\nwhich is crucial to fit ever-changing user needs. However, catastrophic\nforgetting dramatically degrades the model performance in face of a long\nstreamed curriculum. In this paper, we aim to overcome the forgetting problem\nin ToDs and propose a method (HESIT) with hyper-gradient-based exemplar\nstrategy, which samples influential exemplars for periodic retraining. Instead\nof unilaterally observing data or models, HESIT adopts a profound exemplar\nselection strategy that considers the general performance of the trained model\nwhen selecting exemplars for each task domain. Specifically, HESIT analyzes the\ntraining data influence by tracing their hyper-gradient in the optimization\nprocess. Furthermore, HESIT avoids estimating Hessian to make it compatible for\nToDs with a large pre-trained model. Experimental results show that HESIT\neffectively alleviates catastrophic forgetting by exemplar selection, and\nachieves state-of-the-art performance on the largest CL benchmark of ToDs in\nterms of all metrics.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.10992v1",
    "published_date": "2024-05-16 10:54:46 UTC",
    "updated_date": "2024-05-16 10:54:46 UTC"
  },
  {
    "arxiv_id": "2405.09980v1",
    "title": "FinTextQA: A Dataset for Long-form Financial Question Answering",
    "authors": [
      "Jian Chen",
      "Peilin Zhou",
      "Yining Hua",
      "Yingxin Loh",
      "Kehui Chen",
      "Ziyuan Li",
      "Bing Zhu",
      "Junwei Liang"
    ],
    "abstract": "Accurate evaluation of financial question answering (QA) systems necessitates\na comprehensive dataset encompassing diverse question types and contexts.\nHowever, current financial QA datasets lack scope diversity and question\ncomplexity. This work introduces FinTextQA, a novel dataset for long-form\nquestion answering (LFQA) in finance. FinTextQA comprises 1,262 high-quality,\nsource-attributed QA pairs extracted and selected from finance textbooks and\ngovernment agency websites.Moreover, we developed a Retrieval-Augmented\nGeneration (RAG)-based LFQA system, comprising an embedder, retriever,\nreranker, and generator. A multi-faceted evaluation approach, including human\nranking, automatic metrics, and GPT-4 scoring, was employed to benchmark the\nperformance of different LFQA system configurations under heightened noisy\nconditions. The results indicate that: (1) Among all compared generators,\nBaichuan2-7B competes closely with GPT-3.5-turbo in accuracy score; (2) The\nmost effective system configuration on our dataset involved setting the\nembedder, retriever, reranker, and generator as Ada2, Automated Merged\nRetrieval, Bge-Reranker-Base, and Baichuan2-7B, respectively; (3) models are\nless susceptible to noise after the length of contexts reaching a specific\nthreshold.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09980v1",
    "published_date": "2024-05-16 10:53:31 UTC",
    "updated_date": "2024-05-16 10:53:31 UTC"
  },
  {
    "arxiv_id": "2405.09972v1",
    "title": "Predicting Solar Heat Production to Optimize Renewable Energy Usage",
    "authors": [
      "Tatiana Boura",
      "Natalia Koliou",
      "George Meramveliotakis",
      "Stasinos Konstantopoulos",
      "George Kosmadakis"
    ],
    "abstract": "Utilizing solar energy to meet space heating and domestic hot water demand is\nvery efficient (in terms of environmental footprint as well as cost), but in\norder to ensure that user demand is entirely covered throughout the year needs\nto be complemented with auxiliary heating systems, typically boilers and heat\npumps. Naturally, the optimal control of such a system depends on an accurate\nprediction of solar thermal production.\n  Experimental testing and physics-based numerical models are used to find a\ncollector's performance curve - the mapping from solar radiation and other\nexternal conditions to heat production - but this curve changes over time once\nthe collector is exposed to outdoor conditions. In order to deploy advanced\ncontrol strategies in small domestic installations, we present an approach that\nuses machine learning to automatically construct and continuously adapt a model\nthat predicts heat production. Our design is driven by the need to (a)\nconstruct and adapt models using supervision that can be extracted from\nlow-cost instrumentation, avoiding extreme accuracy and reliability\nrequirements; and (b) at inference time, use inputs that are typically provided\nin publicly available weather forecasts.\n  Recent developments in attention-based machine learning, as well as careful\nadaptation of the training setup to the specifics of the task, have allowed us\nto design a machine learning-based solution that covers our requirements. We\npresent positive empirical results for the predictive accuracy of our solution,\nand discuss the impact of these results on the end-to-end system.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09972v1",
    "published_date": "2024-05-16 10:32:39 UTC",
    "updated_date": "2024-05-16 10:32:39 UTC"
  },
  {
    "arxiv_id": "2405.19343v1",
    "title": "Luganda Speech Intent Recognition for IoT Applications",
    "authors": [
      "Andrew Katumba",
      "Sudi Murindanyi",
      "John Trevor Kasule",
      "Elvis Mugume"
    ],
    "abstract": "The advent of Internet of Things (IoT) technology has generated massive\ninterest in voice-controlled smart homes. While many voice-controlled smart\nhome systems are designed to understand and support widely spoken languages\nlike English, speakers of low-resource languages like Luganda may need more\nsupport. This research project aimed to develop a Luganda speech intent\nclassification system for IoT applications to integrate local languages into\nsmart home environments. The project uses hardware components such as Raspberry\nPi, Wio Terminal, and ESP32 nodes as microcontrollers. The Raspberry Pi\nprocesses Luganda voice commands, the Wio Terminal is a display device, and the\nESP32 nodes control the IoT devices. The ultimate objective of this work was to\nenable voice control using Luganda, which was accomplished through a natural\nlanguage processing (NLP) model deployed on the Raspberry Pi. The NLP model\nutilized Mel Frequency Cepstral Coefficients (MFCCs) as acoustic features and a\nConvolutional Neural Network (Conv2D) architecture for speech intent\nclassification. A dataset of Luganda voice commands was curated for this\npurpose and this has been made open-source. This work addresses the\nlocalization challenges and linguistic diversity in IoT applications by\nincorporating Luganda voice commands, enabling users to interact with smart\nhome devices without English proficiency, especially in regions where local\nlanguages are predominant.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Presented as a conference paper at ICLR 2024/AfricaNLP",
    "pdf_url": "http://arxiv.org/pdf/2405.19343v1",
    "published_date": "2024-05-16 10:14:00 UTC",
    "updated_date": "2024-05-16 10:14:00 UTC"
  },
  {
    "arxiv_id": "2405.09939v2",
    "title": "SciQAG: A Framework for Auto-Generated Science Question Answering Dataset with Fine-grained Evaluation",
    "authors": [
      "Yuwei Wan",
      "Yixuan Liu",
      "Aswathy Ajith",
      "Clara Grazian",
      "Bram Hoex",
      "Wenjie Zhang",
      "Chunyu Kit",
      "Tong Xie",
      "Ian Foster"
    ],
    "abstract": "We introduce SciQAG, a novel framework for automatically generating\nhigh-quality science question-answer pairs from a large corpus of scientific\nliterature based on large language models (LLMs). SciQAG consists of a QA\ngenerator and a QA evaluator, which work together to extract diverse and\nresearch-level questions and answers from scientific papers. Utilizing this\nframework, we construct a large-scale, high-quality, open-ended science QA\ndataset containing 188,042 QA pairs extracted from 22,743 scientific papers\nacross 24 scientific domains. We also introduce SciQAG-24D, a new benchmark\ntask designed to evaluate the science question-answering ability of LLMs.\nExtensive experiments demonstrate that fine-tuning LLMs on the SciQAG dataset\nsignificantly improves their performance on both open-ended question answering\nand scientific tasks. To foster research and collaboration, we make the\ndatasets, models, and evaluation codes publicly available, contributing to the\nadvancement of science question answering and developing more interpretable and\nreasoning-capable AI systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09939v2",
    "published_date": "2024-05-16 09:42:37 UTC",
    "updated_date": "2024-07-10 01:25:50 UTC"
  },
  {
    "arxiv_id": "2405.09935v2",
    "title": "DEBATE: Devil's Advocate-Based Assessment and Text Evaluation",
    "authors": [
      "Alex Kim",
      "Keonwoo Kim",
      "Sangwon Yoon"
    ],
    "abstract": "As natural language generation (NLG) models have become prevalent,\nsystematically assessing the quality of machine-generated texts has become\nincreasingly important. Recent studies introduce LLM-based evaluators that\noperate as reference-free metrics, demonstrating their capability to adeptly\nhandle novel tasks. However, these models generally rely on a single-agent\napproach, which, we argue, introduces an inherent limit to their performance.\nThis is because there exist biases in LLM agent's responses, including\npreferences for certain text structure or content. In this work, we propose\nDEBATE, an NLG evaluation framework based on multi-agent scoring system\naugmented with a concept of Devil's Advocate. Within the framework, one agent\nis instructed to criticize other agents' arguments, potentially resolving the\nbias in LLM agent's answers. DEBATE substantially outperforms the previous\nstate-of-the-art methods in two meta-evaluation benchmarks in NLG evaluation,\nSummEval and TopicalChat. We also show that the extensiveness of debates among\nagents and the persona of an agent can influence the performance of evaluators.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09935v2",
    "published_date": "2024-05-16 09:41:12 UTC",
    "updated_date": "2024-05-24 01:06:41 UTC"
  },
  {
    "arxiv_id": "2405.09934v1",
    "title": "Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance",
    "authors": [
      "Milda Pocevičiūtė",
      "Gabriel Eilertsen",
      "Stina Garvin",
      "Claes Lundström"
    ],
    "abstract": "Multiple-instance learning (MIL) is an attractive approach for digital\npathology applications as it reduces the costs related to data collection and\nlabelling. However, it is not clear how sensitive MIL is to clinically\nrealistic domain shifts, i.e., differences in data distribution that could\nnegatively affect performance, and if already existing metrics for detecting\ndomain shifts work well with these algorithms. We trained an attention-based\nMIL algorithm to classify whether a whole-slide image of a lymph node contains\nbreast tumour metastases. The algorithm was evaluated on data from a hospital\nin a different country and various subsets of this data that correspond to\ndifferent levels of domain shift. Our contributions include showing that MIL\nfor digital pathology is affected by clinically realistic differences in data,\nevaluating which features from a MIL model are most suitable for detecting\nchanges in performance, and proposing an unsupervised metric named Fr\\'echet\nDomain Distance (FDD) for quantification of domain shifts. Shift measure\nperformance was evaluated through the mean Pearson correlation to change in\nclassification performance, where FDD achieved 0.70 on 10-fold cross-validation\nmodels. The baselines included Deep ensemble, Difference of Confidence, and\nRepresentation shift which resulted in 0.45, -0.29, and 0.56 mean Pearson\ncorrelation, respectively. FDD could be a valuable tool for care providers and\nvendors who need to verify if a MIL system is likely to perform reliably when\nimplemented at a new site, without requiring any additional annotations from\npathologists.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09934v1",
    "published_date": "2024-05-16 09:37:57 UTC",
    "updated_date": "2024-05-16 09:37:57 UTC"
  },
  {
    "arxiv_id": "2405.09933v3",
    "title": "MiniMaxAD: A Lightweight Autoencoder for Feature-Rich Anomaly Detection",
    "authors": [
      "Fengjie Wang",
      "Chengming Liu",
      "Lei Shi",
      "Pang Haibo"
    ],
    "abstract": "Previous unsupervised anomaly detection (UAD) methods often struggle to\nhandle the extensive diversity in training sets, particularly when they contain\nstylistically diverse and feature-rich samples, which we categorize as\nfeature-rich anomaly detection datasets (FRADs). This challenge is evident in\napplications such as multi-view and multi-class scenarios. To address this\nchallenge, we developed MiniMaxAD, a lightweight autoencoder designed to\nefficiently compress and memorize extensive information from normal images. Our\nmodel employs a technique that enhances feature diversity, thereby increasing\nthe effective capacity of the network. It also utilizes large kernel\nconvolution to extract highly abstract patterns, which contribute to efficient\nand compact feature embedding. Moreover, we introduce an Adaptive Contraction\nHard Mining Loss (ADCLoss), specifically tailored to FRADs. In our methodology,\nany dataset can be unified under the framework of feature-rich anomaly\ndetection, in a way that the benefits far outweigh the drawbacks. Our approach\nhas achieved state-of-the-art performance in multiple challenging benchmarks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09933v3",
    "published_date": "2024-05-16 09:37:54 UTC",
    "updated_date": "2024-09-30 04:17:02 UTC"
  },
  {
    "arxiv_id": "2405.09909v1",
    "title": "A Machine Learning Approach for Simultaneous Demapping of QAM and APSK Constellations",
    "authors": [
      "Arwin Gansekoele",
      "Alexios Balatsoukas-Stimming",
      "Tom Brusse",
      "Mark Hoogendoorn",
      "Sandjai Bhulai",
      "Rob van der Mei"
    ],
    "abstract": "As telecommunication systems evolve to meet increasing demands, integrating\ndeep neural networks (DNNs) has shown promise in enhancing performance.\nHowever, the trade-off between accuracy and flexibility remains challenging\nwhen replacing traditional receivers with DNNs. This paper introduces a novel\nprobabilistic framework that allows a single DNN demapper to demap multiple QAM\nand APSK constellations simultaneously. We also demonstrate that our framework\nallows exploiting hierarchical relationships in families of constellations. The\nconsequence is that we need fewer neural network outputs to encode the same\nfunction without an increase in Bit Error Rate (BER). Our simulation results\nconfirm that our approach approaches the optimal demodulation error bound under\nan Additive White Gaussian Noise (AWGN) channel for multiple constellations.\nThereby, we address multiple important issues in making DNNs flexible enough\nfor practical use as receivers.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear in the ICMLCN 2024 proceedings",
    "pdf_url": "http://arxiv.org/pdf/2405.09909v1",
    "published_date": "2024-05-16 08:57:34 UTC",
    "updated_date": "2024-05-16 08:57:34 UTC"
  },
  {
    "arxiv_id": "2405.10991v1",
    "title": "Relative Counterfactual Contrastive Learning for Mitigating Pretrained Stance Bias in Stance Detection",
    "authors": [
      "Jiarui Zhang",
      "Shaojuan Wu",
      "Xiaowang Zhang",
      "Zhiyong Feng"
    ],
    "abstract": "Stance detection classifies stance relations (namely, Favor, Against, or\nNeither) between comments and targets. Pretrained language models (PLMs) are\nwidely used to mine the stance relation to improve the performance of stance\ndetection through pretrained knowledge. However, PLMs also embed ``bad''\npretrained knowledge concerning stance into the extracted stance relation\nsemantics, resulting in pretrained stance bias. It is not trivial to measure\npretrained stance bias due to its weak quantifiability. In this paper, we\npropose Relative Counterfactual Contrastive Learning (RCCL), in which\npretrained stance bias is mitigated as relative stance bias instead of absolute\nstance bias to overtake the difficulty of measuring bias. Firstly, we present a\nnew structural causal model for characterizing complicated relationships among\ncontext, PLMs and stance relations to locate pretrained stance bias. Then,\nbased on masked language model prediction, we present a target-aware relative\nstance sample generation method for obtaining relative bias. Finally, we use\ncontrastive learning based on counterfactual theory to mitigate pretrained\nstance bias and preserve context stance relation. Experiments show that the\nproposed method is superior to stance detection and debiasing baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.10991v1",
    "published_date": "2024-05-16 08:57:00 UTC",
    "updated_date": "2024-05-16 08:57:00 UTC"
  },
  {
    "arxiv_id": "2405.09902v1",
    "title": "Unveiling the Potential: Harnessing Deep Metric Learning to Circumvent Video Streaming Encryption",
    "authors": [
      "Arwin Gansekoele",
      "Tycho Bot",
      "Rob van der Mei",
      "Sandjai Bhulai",
      "Mark Hoogendoorn"
    ],
    "abstract": "Encryption on the internet with the shift to HTTPS has been an important step\nto improve the privacy of internet users. However, there is an increasing body\nof work about extracting information from encrypted internet traffic without\nhaving to decrypt it. Such attacks bypass security guarantees assumed to be\ngiven by HTTPS and thus need to be understood. Prior works showed that the\nvariable bitrates of video streams are sufficient to identify which video\nsomeone is watching. These works generally have to make trade-offs in aspects\nsuch as accuracy, scalability, robustness, etc. These trade-offs complicate the\npractical use of these attacks. To that end, we propose a deep metric learning\nframework based on the triplet loss method. Through this framework, we achieve\nrobust, generalisable, scalable and transferable encrypted video stream\ndetection. First, the triplet loss is better able to deal with video streams\nnot seen during training. Second, our approach can accurately classify videos\nnot seen during training. Third, we show that our method scales well to a\ndataset of over 1000 videos. Finally, we show that a model trained on video\nstreams over Chrome can also classify streams over Firefox. Our results suggest\nthat this side-channel attack is more broadly applicable than originally\nthought. We provide our code alongside a diverse and up-to-date dataset for\nfuture research.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "Published in the WI-IAT 2023 proceedings",
    "pdf_url": "http://arxiv.org/pdf/2405.09902v1",
    "published_date": "2024-05-16 08:49:05 UTC",
    "updated_date": "2024-05-16 08:49:05 UTC"
  },
  {
    "arxiv_id": "2405.09901v1",
    "title": "Whole-Song Hierarchical Generation of Symbolic Music Using Cascaded Diffusion Models",
    "authors": [
      "Ziyu Wang",
      "Lejun Min",
      "Gus Xia"
    ],
    "abstract": "Recent deep music generation studies have put much emphasis on long-term\ngeneration with structures. However, we are yet to see high-quality,\nwell-structured whole-song generation. In this paper, we make the first attempt\nto model a full music piece under the realization of compositional hierarchy.\nWith a focus on symbolic representations of pop songs, we define a hierarchical\nlanguage, in which each level of hierarchy focuses on the semantics and context\ndependency at a certain music scope. The high-level languages reveal whole-song\nform, phrase, and cadence, whereas the low-level languages focus on notes,\nchords, and their local patterns. A cascaded diffusion model is trained to\nmodel the hierarchical language, where each level is conditioned on its upper\nlevels. Experiments and analysis show that our model is capable of generating\nfull-piece music with recognizable global verse-chorus structure and cadences,\nand the music quality is higher than the baselines. Additionally, we show that\nthe proposed model is controllable in a flexible way. By sampling from the\ninterpretable hierarchical languages or adjusting pre-trained external\nrepresentations, users can control the music flow via various features such as\nphrase harmonic structures, rhythmic patterns, and accompaniment texture.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS",
      "68Txx"
    ],
    "primary_category": "cs.SD",
    "comment": "Proceedings of the International Conference on Learning\n  Representations (ICLR 2024)",
    "pdf_url": "http://arxiv.org/pdf/2405.09901v1",
    "published_date": "2024-05-16 08:48:23 UTC",
    "updated_date": "2024-05-16 08:48:23 UTC"
  },
  {
    "arxiv_id": "2405.13030v1",
    "title": "Crowdsourcing with Enhanced Data Quality Assurance: An Efficient Approach to Mitigate Resource Scarcity Challenges in Training Large Language Models for Healthcare",
    "authors": [
      "P. Barai",
      "G. Leroy",
      "P. Bisht",
      "J. M. Rothman",
      "S. Lee",
      "J. Andrews",
      "S. A. Rice",
      "A. Ahmed"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated immense potential in\nartificial intelligence across various domains, including healthcare. However,\ntheir efficacy is hindered by the need for high-quality labeled data, which is\noften expensive and time-consuming to create, particularly in low-resource\ndomains like healthcare. To address these challenges, we propose a\ncrowdsourcing (CS) framework enriched with quality control measures at the\npre-, real-time-, and post-data gathering stages. Our study evaluated the\neffectiveness of enhancing data quality through its impact on LLMs (Bio-BERT)\nfor predicting autism-related symptoms. The results show that real-time quality\ncontrol improves data quality by 19 percent compared to pre-quality control.\nFine-tuning Bio-BERT using crowdsourced data generally increased recall\ncompared to the Bio-BERT baseline but lowered precision. Our findings\nhighlighted the potential of crowdsourcing and quality control in\nresource-constrained environments and offered insights into optimizing\nhealthcare LLMs for informed decision-making and improved patient care.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Published in AMIA Summit, Boston, 2024.\n  https://knowledge.amia.org/Info2024/pdf/Info2024a022/Info2024fl021",
    "pdf_url": "http://arxiv.org/pdf/2405.13030v1",
    "published_date": "2024-05-16 08:29:00 UTC",
    "updated_date": "2024-05-16 08:29:00 UTC"
  },
  {
    "arxiv_id": "2405.09893v1",
    "title": "\"Hunt Takes Hare\": Theming Games Through Game-Word Vector Translation",
    "authors": [
      "Rabii Younès",
      "Cook Michael"
    ],
    "abstract": "A game's theme is an important part of its design -- it conveys narrative\ninformation, rhetorical messages, helps the player intuit strategies, aids in\ntutorialisation and more. Thematic elements of games are notoriously difficult\nfor AI systems to understand and manipulate, however, and often rely on large\namounts of hand-written interpretations and knowledge. In this paper we present\na technique which connects game embeddings, a recent method for modelling game\ndynamics from log data, and word embeddings, which models semantic information\nabout language. We explain two different approaches for using game embeddings\nin this way, and show evidence that game embeddings enhance the linguistic\ntranslations of game concepts from one theme to another, opening up exciting\nnew possibilities for reasoning about the thematic elements of games in the\nfuture.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, PCG Workshop at FDG 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.09893v1",
    "published_date": "2024-05-16 08:19:11 UTC",
    "updated_date": "2024-05-16 08:19:11 UTC"
  },
  {
    "arxiv_id": "2405.10989v1",
    "title": "Learnable Privacy Neurons Localization in Language Models",
    "authors": [
      "Ruizhe Chen",
      "Tianxiang Hu",
      "Yang Feng",
      "Zuozhu Liu"
    ],
    "abstract": "Concerns regarding Large Language Models (LLMs) to memorize and disclose\nprivate information, particularly Personally Identifiable Information (PII),\nbecome prominent within the community. Many efforts have been made to mitigate\nthe privacy risks. However, the mechanism through which LLMs memorize PII\nremains poorly understood. To bridge this gap, we introduce a pioneering method\nfor pinpointing PII-sensitive neurons (privacy neurons) within LLMs. Our method\nemploys learnable binary weight masks to localize specific neurons that account\nfor the memorization of PII in LLMs through adversarial training. Our\ninvestigations discover that PII is memorized by a small subset of neurons\nacross all layers, which shows the property of PII specificity. Furthermore, we\npropose to validate the potential in PII risk mitigation by deactivating the\nlocalized privacy neurons. Both quantitative and qualitative experiments\ndemonstrate the effectiveness of our neuron localization algorithm.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "ACL 2024 main conference",
    "pdf_url": "http://arxiv.org/pdf/2405.10989v1",
    "published_date": "2024-05-16 08:11:08 UTC",
    "updated_date": "2024-05-16 08:11:08 UTC"
  },
  {
    "arxiv_id": "2405.09886v1",
    "title": "MTLComb: multi-task learning combining regression and classification tasks for joint feature selection",
    "authors": [
      "Han Cao",
      "Sivanesan Rajan",
      "Bianka Hahn",
      "Ersoy Kocak",
      "Daniel Durstewitz",
      "Emanuel Schwarz",
      "Verena Schneider-Lindner"
    ],
    "abstract": "Multi-task learning (MTL) is a learning paradigm that enables the\nsimultaneous training of multiple communicating algorithms. Although MTL has\nbeen successfully applied to ether regression or classification tasks alone,\nincorporating mixed types of tasks into a unified MTL framework remains\nchallenging, primarily due to variations in the magnitudes of losses associated\nwith different tasks. This challenge, particularly evident in MTL applications\nwith joint feature selection, often results in biased selections. To overcome\nthis obstacle, we propose a provable loss weighting scheme that analytically\ndetermines the optimal weights for balancing regression and classification\ntasks. This scheme significantly mitigates the otherwise biased feature\nselection. Building upon this scheme, we introduce MTLComb, an MTL algorithm\nand software package encompassing optimization procedures, training protocols,\nand hyperparameter estimation procedures. MTLComb is designed for learning\nshared predictors among tasks of mixed types. To showcase the efficacy of\nMTLComb, we conduct tests on both simulated data and biomedical studies\npertaining to sepsis and schizophrenia.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM",
      "J.3; I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "33 pages, 3 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.09886v1",
    "published_date": "2024-05-16 08:07:25 UTC",
    "updated_date": "2024-05-16 08:07:25 UTC"
  },
  {
    "arxiv_id": "2405.09882v1",
    "title": "DiffAM: Diffusion-based Adversarial Makeup Transfer for Facial Privacy Protection",
    "authors": [
      "Yuhao Sun",
      "Lingyun Yu",
      "Hongtao Xie",
      "Jiaming Li",
      "Yongdong Zhang"
    ],
    "abstract": "With the rapid development of face recognition (FR) systems, the privacy of\nface images on social media is facing severe challenges due to the abuse of\nunauthorized FR systems. Some studies utilize adversarial attack techniques to\ndefend against malicious FR systems by generating adversarial examples.\nHowever, the generated adversarial examples, i.e., the protected face images,\ntend to suffer from subpar visual quality and low transferability. In this\npaper, we propose a novel face protection approach, dubbed DiffAM, which\nleverages the powerful generative ability of diffusion models to generate\nhigh-quality protected face images with adversarial makeup transferred from\nreference images. To be specific, we first introduce a makeup removal module to\ngenerate non-makeup images utilizing a fine-tuned diffusion model with guidance\nof textual prompts in CLIP space. As the inverse process of makeup transfer,\nmakeup removal can make it easier to establish the deterministic relationship\nbetween makeup domain and non-makeup domain regardless of elaborate text\nprompts. Then, with this relationship, a CLIP-based makeup loss along with an\nensemble attack strategy is introduced to jointly guide the direction of\nadversarial makeup domain, achieving the generation of protected face images\nwith natural-looking makeup and high black-box transferability. Extensive\nexperiments demonstrate that DiffAM achieves higher visual quality and attack\nsuccess rates with a gain of 12.98% under black-box setting compared with the\nstate of the arts. The code will be available at\nhttps://github.com/HansSunY/DiffAM.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "16 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.09882v1",
    "published_date": "2024-05-16 08:05:36 UTC",
    "updated_date": "2024-05-16 08:05:36 UTC"
  },
  {
    "arxiv_id": "2405.09879v1",
    "title": "Generative Unlearning for Any Identity",
    "authors": [
      "Juwon Seo",
      "Sung-Hoon Lee",
      "Tae-Young Lee",
      "Seungjun Moon",
      "Gyeong-Moon Park"
    ],
    "abstract": "Recent advances in generative models trained on large-scale datasets have\nmade it possible to synthesize high-quality samples across various domains.\nMoreover, the emergence of strong inversion networks enables not only a\nreconstruction of real-world images but also the modification of attributes\nthrough various editing methods. However, in certain domains related to privacy\nissues, e.g., human faces, advanced generative models along with strong\ninversion methods can lead to potential misuses. In this paper, we propose an\nessential yet under-explored task called generative identity unlearning, which\nsteers the model not to generate an image of a specific identity. In the\ngenerative identity unlearning, we target the following objectives: (i)\npreventing the generation of images with a certain identity, and (ii)\npreserving the overall quality of the generative model. To satisfy these goals,\nwe propose a novel framework, Generative Unlearning for Any Identity (GUIDE),\nwhich prevents the reconstruction of a specific identity by unlearning the\ngenerator with only a single image. GUIDE consists of two parts: (i) finding a\ntarget point for optimization that un-identifies the source latent code and\n(ii) novel loss functions that facilitate the unlearning procedure while less\naffecting the learned distribution. Our extensive experiments demonstrate that\nour proposed method achieves state-of-the-art performance in the generative\nmachine unlearning task. The code is available at\nhttps://github.com/KHU-AGI/GUIDE.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages, 17 figures, 10 tables, CVPR 2024 Poster",
    "pdf_url": "http://arxiv.org/pdf/2405.09879v1",
    "published_date": "2024-05-16 08:00:55 UTC",
    "updated_date": "2024-05-16 08:00:55 UTC"
  },
  {
    "arxiv_id": "2405.09878v2",
    "title": "Hyperplane Arrangements and Fixed Points in Iterated PWL Neural Networks",
    "authors": [
      "Hans-Peter Beise"
    ],
    "abstract": "We leverage the framework of hyperplane arrangements to analyze potential\nregions of (stable) fixed points. We provide an upper bound on the number of\nfixed points for multi-layer neural networks equipped with piecewise linear\n(PWL) activation functions with arbitrary many linear pieces. The theoretical\noptimality of the exponential growth in the number of layers of the latter\nbound is shown. Specifically, we also derive a sharper upper bound on the\nnumber of stable fixed points for one-hidden-layer networks with hard tanh\nactivation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML",
      "68T07",
      "G.0"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09878v2",
    "published_date": "2024-05-16 07:57:31 UTC",
    "updated_date": "2024-07-14 18:01:28 UTC"
  },
  {
    "arxiv_id": "2405.09875v2",
    "title": "A Farewell to Harms: Risk Management for Medical Devices via the Riskman Ontology & Shapes",
    "authors": [
      "Piotr Gorczyca",
      "Dörthe Arndt",
      "Martin Diller",
      "Pascal Kettmann",
      "Stephan Mennicke",
      "Hannes Strass"
    ],
    "abstract": "We introduce the Riskman ontology & shapes for representing and analysing\ninformation about risk management for medical devices. Risk management is\nconcerned with taking necessary precautions so a medical device does not cause\nharms for users or the environment. To date, risk management documentation is\nsubmitted to notified bodies (for certification) in the form of semi-structured\nnatural language text. We propose to use classes from the Riskman ontology to\nlogically model risk management documentation and to use the included SHACL\nconstraints to check for syntactic completeness and conformity to relevant\nstandards. In particular, the ontology is modelled after ISO 14971 and the\nrecently published VDE Spec 90025. Our proposed methodology has the potential\nto save many person-hours for both manufacturers (when creating risk management\ndocumentation) as well as notified bodies (when assessing submitted\napplications for certification), and thus offers considerable benefits for\nhealthcare and, by extension, society as a whole.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09875v2",
    "published_date": "2024-05-16 07:53:07 UTC",
    "updated_date": "2024-05-22 12:46:24 UTC"
  },
  {
    "arxiv_id": "2405.09863v3",
    "title": "Box-Free Model Watermarks Are Prone to Black-Box Removal Attacks",
    "authors": [
      "Haonan An",
      "Guang Hua",
      "Zhiping Lin",
      "Yuguang Fang"
    ],
    "abstract": "Box-free model watermarking is an emerging technique to safeguard the\nintellectual property of deep learning models, particularly those for low-level\nimage processing tasks. Existing works have verified and improved its\neffectiveness in several aspects. However, in this paper, we reveal that\nbox-free model watermarking is prone to removal attacks, even under the\nreal-world threat model such that the protected model and the watermark\nextractor are in black boxes. Under this setting, we carry out three studies.\n1) We develop an extractor-gradient-guided (EGG) remover and show its\neffectiveness when the extractor uses ReLU activation only. 2) More generally,\nfor an unknown extractor, we leverage adversarial attacks and design the EGG\nremover based on the estimated gradients. 3) Under the most stringent condition\nthat the extractor is inaccessible, we design a transferable remover based on a\nset of private proxy models. In all cases, the proposed removers can\nsuccessfully remove embedded watermarks while preserving the quality of the\nprocessed images, and we also demonstrate that the EGG remover can even replace\nthe watermarks. Extensive experimental results verify the effectiveness and\ngeneralizability of the proposed attacks, revealing the vulnerabilities of the\nexisting box-free methods and calling for further research.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09863v3",
    "published_date": "2024-05-16 07:41:54 UTC",
    "updated_date": "2024-08-20 06:37:37 UTC"
  },
  {
    "arxiv_id": "2405.09857v1",
    "title": "IGOT: Information Gain Optimized Tokenizer on Domain Adaptive Pretraining",
    "authors": [
      "Dawei Feng",
      "Yihai Zhang",
      "Zhixuan Xu"
    ],
    "abstract": "Pretrained Large Language Models (LLM) such as ChatGPT, Claude, etc. have\ndemonstrated strong capabilities in various fields of natural language\ngeneration. However, there are still many problems when using LLM in\nspecialized domain-specific fields. When using generative AI to process\ndownstream tasks, a common approach is to add new knowledge (e.g., private\ndomain knowledge, cutting-edge information) to a pretrained model through\ncontinued training or fine-tuning. However, whether there is a universal\nparadigm for domain adaptation training is still an open question. In this\narticle, we proposed Information Gain Optimized Tokenizer (IGOT), which\nanalyzes the special token set of downstream tasks, constructs a new subset\nusing heuristic function $\\phi$ with the special token and its information\ngain, to build new domain-specific tokenizer, and continues pretraining on the\ndownstream task data. We explored the many positive effects of this method's\ncustomized tokenizer on domain-adaptive pretraining and verified this method\ncan perform better than the ordinary method of just collecting data and\nfine-tuning. Based on our experiment, the continued pretraining process of IGOT\nwith LLaMA-7B achieved 11.9\\% token saving, 12.2\\% training time saving, and\n5.8\\% maximum GPU VRAM usage saving, combined with the T5 model, we can even\nreach a 31.5\\% of training time saving, making porting general generative AI to\nspecific domains more effective than before. In domain-specific tasks,\nsupervised $IGOT_\\tau$ shows great performance on reducing both the convergence\nradius and convergence point during keep pretraining.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09857v1",
    "published_date": "2024-05-16 07:25:10 UTC",
    "updated_date": "2024-05-16 07:25:10 UTC"
  },
  {
    "arxiv_id": "2405.09848v1",
    "title": "Enhancing Semantics in Multimodal Chain of Thought via Soft Negative Sampling",
    "authors": [
      "Guangmin Zheng",
      "Jin Wang",
      "Xiaobing Zhou",
      "Xuejie Zhang"
    ],
    "abstract": "Chain of thought (CoT) has proven useful for problems requiring complex\nreasoning. Many of these problems are both textual and multimodal. Given the\ninputs in different modalities, a model generates a rationale and then uses it\nto answer a question. Because of the hallucination issue, the generated soft\nnegative rationales with high textual quality but illogical semantics do not\nalways help improve answer accuracy. This study proposes a rationale generation\nmethod using soft negative sampling (SNSE-CoT) to mitigate hallucinations in\nmultimodal CoT. Five methods were applied to generate soft negative samples\nthat shared highly similar text but had different semantics from the original.\nBidirectional margin loss (BML) was applied to introduce them into the\ntraditional contrastive learning framework that involves only positive and\nnegative samples. Extensive experiments on the ScienceQA dataset demonstrated\nthe effectiveness of the proposed method. Code and data are released at\nhttps://github.com/zgMin/SNSE-CoT.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by LREC-COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.09848v1",
    "published_date": "2024-05-16 06:55:11 UTC",
    "updated_date": "2024-05-16 06:55:11 UTC"
  },
  {
    "arxiv_id": "2405.13028v1",
    "title": "DuetSim: Building User Simulator with Dual Large Language Models for Task-Oriented Dialogues",
    "authors": [
      "Xiang Luo",
      "Zhiwen Tang",
      "Jin Wang",
      "Xuejie Zhang"
    ],
    "abstract": "User Simulators play a pivotal role in training and evaluating task-oriented\ndialogue systems. Traditional user simulators typically rely on\nhuman-engineered agendas, resulting in generated responses that often lack\ndiversity and spontaneity. Although large language models (LLMs) exhibit a\nremarkable capacity for generating coherent and contextually appropriate\nutterances, they may fall short when tasked with generating responses that\neffectively guide users towards their goals, particularly in dialogues with\nintricate constraints and requirements. This paper introduces DuetSim, a novel\nframework designed to address the intricate demands of task-oriented dialogues\nby leveraging LLMs. DuetSim stands apart from conventional approaches by\nemploying two LLMs in tandem: one dedicated to response generation and the\nother focused on verification. This dual LLM approach empowers DuetSim to\nproduce responses that not only exhibit diversity but also demonstrate accuracy\nand are preferred by human users. We validate the efficacy of our method\nthrough extensive experiments conducted on the MultiWOZ dataset, highlighting\nimprovements in response quality and correctness, largely attributed to the\nincorporation of the second LLM. Our code is accessible at:\nhttps://github.com/suntea233/DuetSim.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.13028v1",
    "published_date": "2024-05-16 06:24:31 UTC",
    "updated_date": "2024-05-16 06:24:31 UTC"
  },
  {
    "arxiv_id": "2405.10988v2",
    "title": "Flow Score Distillation for Diverse Text-to-3D Generation",
    "authors": [
      "Runjie Yan",
      "Kailu Wu",
      "Kaisheng Ma"
    ],
    "abstract": "Recent advancements in Text-to-3D generation have yielded remarkable\nprogress, particularly through methods that rely on Score Distillation Sampling\n(SDS). While SDS exhibits the capability to create impressive 3D assets, it is\nhindered by its inherent maximum-likelihood-seeking essence, resulting in\nlimited diversity in generation outcomes. In this paper, we discover that the\nDenoise Diffusion Implicit Models (DDIM) generation process (\\ie PF-ODE) can be\nsuccinctly expressed using an analogue of SDS loss. One step further, one can\nsee SDS as a generalized DDIM generation process. Following this insight, we\nshow that the noise sampling strategy in the noise addition stage significantly\nrestricts the diversity of generation results. To address this limitation, we\npresent an innovative noise sampling approach and introduce a novel text-to-3D\nmethod called Flow Score Distillation (FSD). Our validation experiments across\nvarious text-to-image Diffusion Models demonstrate that FSD substantially\nenhances generation diversity without compromising quality.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Consistent Flow Distillation is an improved version of this paper",
    "pdf_url": "http://arxiv.org/pdf/2405.10988v2",
    "published_date": "2024-05-16 06:05:16 UTC",
    "updated_date": "2024-07-28 21:52:11 UTC"
  },
  {
    "arxiv_id": "2405.10987v1",
    "title": "Manifold-based Incomplete Multi-view Clustering via Bi-Consistency Guidance",
    "authors": [
      "Huibing Wang",
      "Mingze Yao",
      "Yawei Chen",
      "Yunqiu Xu",
      "Haipeng Liu",
      "Wei Jia",
      "Xianping Fu",
      "Yang Wang"
    ],
    "abstract": "Incomplete multi-view clustering primarily focuses on dividing unlabeled data\ninto corresponding categories with missing instances, and has received\nintensive attention due to its superiority in real applications. Considering\nthe influence of incomplete data, the existing methods mostly attempt to\nrecover data by adding extra terms. However, for the unsupervised methods, a\nsimple recovery strategy will cause errors and outlying value accumulations,\nwhich will affect the performance of the methods. Broadly, the previous methods\nhave not taken the effectiveness of recovered instances into consideration, or\ncannot flexibly balance the discrepancies between recovered data and original\ndata. To address these problems, we propose a novel method termed\nManifold-based Incomplete Multi-view clustering via Bi-consistency guidance\n(MIMB), which flexibly recovers incomplete data among various views, and\nattempts to achieve biconsistency guidance via reverse regularization. In\nparticular, MIMB adds reconstruction terms to representation learning by\nrecovering missing instances, which dynamically examines the latent consensus\nrepresentation. Moreover, to preserve the consistency information among\nmultiple views, MIMB implements a biconsistency guidance strategy with reverse\nregularization of the consensus representation and proposes a manifold\nembedding measure for exploring the hidden structure of the recovered data.\nNotably, MIMB aims to balance the importance of different views, and introduces\nan adaptive weight term for each view. Finally, an optimization algorithm with\nan alternating iteration optimization strategy is designed for final\nclustering. Extensive experimental results on 6 benchmark datasets are provided\nto confirm that MIMB can significantly obtain superior results as compared with\nseveral state-of-the-art baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.10987v1",
    "published_date": "2024-05-16 05:58:29 UTC",
    "updated_date": "2024-05-16 05:58:29 UTC"
  },
  {
    "arxiv_id": "2405.09806v4",
    "title": "MediSyn: A Generalist Text-Guided Latent Diffusion Model For Diverse Medical Image Synthesis",
    "authors": [
      "Joseph Cho",
      "Mrudang Mathur",
      "Cyril Zakka",
      "Dhamanpreet Kaur",
      "Matthew Leipzig",
      "Alex Dalal",
      "Aravind Krishnan",
      "Eubee Koo",
      "Karen Wai",
      "Cindy S. Zhao",
      "Rohan Shad",
      "Robyn Fong",
      "Ross Wightman",
      "Akshay Chaudhari",
      "William Hiesinger"
    ],
    "abstract": "Deep learning algorithms require extensive data to achieve robust\nperformance. However, data availability is often restricted in the medical\ndomain due to patient privacy concerns. Synthetic data presents a possible\nsolution to these challenges. Recently, image generative models have found\nincreasing use for medical applications but are often designed for singular\nmedical specialties and imaging modalities, thus limiting their broader\nutility. To address this, we introduce MediSyn: a text-guided, latent diffusion\nmodel capable of generating synthetic images from 6 medical specialties and 10\nimage types. The synthetic images are validated by expert clinicians for\nalignment with their corresponding text prompts. Furthermore, a direct\ncomparison of the synthetic images against the real images confirms that our\nmodel synthesizes novel images and, crucially, may preserve patient privacy.\nFinally, classifiers trained on a mixture of synthetic and real data achieve\nsimilar performance to those trained on twice the amount of real data. Our\nfindings highlight the immense potential for generalist image generative models\nto accelerate algorithmic research and development in medicine.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09806v4",
    "published_date": "2024-05-16 04:28:44 UTC",
    "updated_date": "2025-02-10 20:00:24 UTC"
  },
  {
    "arxiv_id": "2405.09802v3",
    "title": "Analysis and Predictive Modeling of Solar Coronal Holes Using Computer Vision and ARIMA-LSTM Networks",
    "authors": [
      "Juyoung Yun",
      "Jungmin Shin"
    ],
    "abstract": "In the era of space exploration, coronal holes on the sun play a significant\nrole due to their impact on satellites and aircraft through their open magnetic\nfields and increased solar wind emissions. This study employs computer vision\ntechniques to detect coronal hole regions and estimate their sizes using\nimagery from the Solar Dynamics Observatory (SDO). Additionally, we utilize\nhybrid time series prediction model, specifically combination of Long\nShort-Term Memory (LSTM) networks and ARIMA, to analyze trends in the area of\ncoronal holes and predict their areas across various solar regions over a span\nof seven days. By examining time series data, we aim to identify patterns in\ncoronal hole behavior and understand their potential effects on space weather.",
    "categories": [
      "astro-ph.SR",
      "astro-ph.EP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "astro-ph.SR",
    "comment": "Accepted to the first joint European Space Agency SPAICE Conference\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2405.09802v3",
    "published_date": "2024-05-16 04:21:09 UTC",
    "updated_date": "2024-07-31 08:28:48 UTC"
  },
  {
    "arxiv_id": "2405.09798v2",
    "title": "Many-Shot In-Context Learning in Multimodal Foundation Models",
    "authors": [
      "Yixing Jiang",
      "Jeremy Irvin",
      "Ji Hun Wang",
      "Muhammad Ahmed Chaudhry",
      "Jonathan H. Chen",
      "Andrew Y. Ng"
    ],
    "abstract": "Large language models are effective at few-shot in-context learning (ICL).\nRecent advancements in multimodal foundation models have enabled\nunprecedentedly long context windows, presenting an opportunity to explore\ntheir capability to perform ICL with many more demonstrating examples. In this\nwork, we evaluate the performance of multimodal foundation models scaling from\nfew-shot to many-shot ICL. We benchmark GPT-4o and Gemini 1.5 Pro across 14\ndatasets spanning multiple domains (natural imagery, medical imagery, remote\nsensing, and molecular imagery) and tasks (image classification, visual QA, and\nobject localization). We observe that many-shot ICL, including up to almost\n2,000 demonstrating examples, leads to substantial improvements compared to\nfew-shot (<100 examples) ICL across all of the datasets. Further, Gemini 1.5\nPro performance continues to improve log-linearly up to the maximum number of\ntested examples on many datasets. We also find open-weights multimodal\nfoundation models like Llama 3.2-Vision do not benefit from the demonstrating\nexamples, highlighting an important gap between open and closed multimodal\nfoundation models. Given the high inference costs required for many-shot ICL,\nwe also explore the impact of batching multiple queries in a single API call.\nWe show that batching up to 50 queries can lead to performance improvements\nunder zero-shot and many-shot ICL, with substantial gains in the zero-shot\nsetting on multiple datasets, while drastically reducing per-query cost and\nlatency. Finally, while GPT-4o and Gemini 1.5 Pro achieve similar zero-shot\nperformance across the datasets, Gemini 1.5 Pro learns more quickly than GPT-4o\non most datasets. Our results suggest that many-shot ICL could enable users to\nefficiently adapt multimodal foundation models to new applications and domains.\nOur codebase is publicly available at\nhttps://github.com/stanfordmlgroup/ManyICL .",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09798v2",
    "published_date": "2024-05-16 04:02:43 UTC",
    "updated_date": "2024-10-04 21:51:47 UTC"
  },
  {
    "arxiv_id": "2405.10348v1",
    "title": "Learning to Predict Mutation Effects of Protein-Protein Interactions by Microenvironment-aware Hierarchical Prompt Learning",
    "authors": [
      "Lirong Wu",
      "Yijun Tian",
      "Haitao Lin",
      "Yufei Huang",
      "Siyuan Li",
      "Nitesh V Chawla",
      "Stan Z. Li"
    ],
    "abstract": "Protein-protein bindings play a key role in a variety of fundamental\nbiological processes, and thus predicting the effects of amino acid mutations\non protein-protein binding is crucial. To tackle the scarcity of annotated\nmutation data, pre-training with massive unlabeled data has emerged as a\npromising solution. However, this process faces a series of challenges: (1)\ncomplex higher-order dependencies among multiple (more than paired) structural\nscales have not yet been fully captured; (2) it is rarely explored how\nmutations alter the local conformation of the surrounding microenvironment; (3)\npre-training is costly, both in data size and computational burden. In this\npaper, we first construct a hierarchical prompt codebook to record common\nmicroenvironmental patterns at different structural scales independently. Then,\nwe develop a novel codebook pre-training task, namely masked microenvironment\nmodeling, to model the joint distribution of each mutation with their residue\ntypes, angular statistics, and local conformational changes in the\nmicroenvironment. With the constructed prompt codebook, we encode the\nmicroenvironment around each mutation into multiple hierarchical prompts and\ncombine them to flexibly provide information to wild-type and mutated protein\ncomplexes about their microenvironmental differences. Such a hierarchical\nprompt learning framework has demonstrated superior performance and training\nefficiency over state-of-the-art pre-training-based methods in mutation effect\nprediction and a case study of optimizing human antibodies against SARS-CoV-2.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.10348v1",
    "published_date": "2024-05-16 03:53:21 UTC",
    "updated_date": "2024-05-16 03:53:21 UTC"
  },
  {
    "arxiv_id": "2405.09794v2",
    "title": "Human-AI Safety: A Descendant of Generative AI and Control Systems Safety",
    "authors": [
      "Andrea Bajcsy",
      "Jaime F. Fisac"
    ],
    "abstract": "Artificial intelligence (AI) is interacting with people at an unprecedented\nscale, offering new avenues for immense positive impact, but also raising\nwidespread concerns around the potential for individual and societal harm.\nToday, the predominant paradigm for human--AI safety focuses on fine-tuning the\ngenerative model's outputs to better agree with human-provided examples or\nfeedback. In reality, however, the consequences of an AI model's outputs cannot\nbe determined in isolation: they are tightly entangled with the responses and\nbehavior of human users over time. In this paper, we distill key complementary\nlessons from AI safety and control systems safety, highlighting open challenges\nas well as key synergies between both fields. We then argue that meaningful\nsafety assurances for advanced AI technologies require reasoning about how the\nfeedback loop formed by AI outputs and human behavior may drive the interaction\ntowards different outcomes. To this end, we introduce a unifying formalism to\ncapture dynamic, safety-critical human--AI interactions and propose a concrete\ntechnical roadmap towards next-generation human-centered AI safety.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.SY",
      "eess.SY",
      "I.2"
    ],
    "primary_category": "cs.AI",
    "comment": "Revised version with refined exposition and technical details. 12\n  pages + references, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.09794v2",
    "published_date": "2024-05-16 03:52:00 UTC",
    "updated_date": "2024-06-22 20:17:22 UTC"
  },
  {
    "arxiv_id": "2405.09784v3",
    "title": "Online bipartite matching with imperfect advice",
    "authors": [
      "Davin Choo",
      "Themis Gouleakis",
      "Chun Kai Ling",
      "Arnab Bhattacharyya"
    ],
    "abstract": "We study the problem of online unweighted bipartite matching with $n$ offline\nvertices and $n$ online vertices where one wishes to be competitive against the\noptimal offline algorithm. While the classic RANKING algorithm of Karp et al.\n[1990] provably attains competitive ratio of $1-1/e > 1/2$, we show that no\nlearning-augmented method can be both 1-consistent and strictly better than\n$1/2$-robust under the adversarial arrival model. Meanwhile, under the random\narrival model, we show how one can utilize methods from distribution testing to\ndesign an algorithm that takes in external advice about the online vertices and\nprovably achieves competitive ratio interpolating between any ratio attainable\nby advice-free methods and the optimal ratio of 1, depending on the advice\nquality.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DS",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted into ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.09784v3",
    "published_date": "2024-05-16 03:04:33 UTC",
    "updated_date": "2024-05-23 13:15:06 UTC"
  },
  {
    "arxiv_id": "2405.09783v1",
    "title": "LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery",
    "authors": [
      "Pingchuan Ma",
      "Tsun-Hsuan Wang",
      "Minghao Guo",
      "Zhiqing Sun",
      "Joshua B. Tenenbaum",
      "Daniela Rus",
      "Chuang Gan",
      "Wojciech Matusik"
    ],
    "abstract": "Large Language Models have recently gained significant attention in\nscientific discovery for their extensive knowledge and advanced reasoning\ncapabilities. However, they encounter challenges in effectively simulating\nobservational feedback and grounding it with language to propel advancements in\nphysical scientific discovery. Conversely, human scientists undertake\nscientific discovery by formulating hypotheses, conducting experiments, and\nrevising theories through observational analysis. Inspired by this, we propose\nto enhance the knowledge-driven, abstract reasoning abilities of LLMs with the\ncomputational strength of simulations. We introduce Scientific Generative Agent\n(SGA), a bilevel optimization framework: LLMs act as knowledgeable and\nversatile thinkers, proposing scientific hypotheses and reason about discrete\ncomponents, such as physics equations or molecule structures; meanwhile,\nsimulations function as experimental platforms, providing observational\nfeedback and optimizing via differentiability for continuous parts, such as\nphysical parameters. We conduct extensive experiments to demonstrate our\nframework's efficacy in constitutive law discovery and molecular design,\nunveiling novel solutions that differ from conventional human expectations yet\nremain coherent upon analysis.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.09783v1",
    "published_date": "2024-05-16 03:04:10 UTC",
    "updated_date": "2024-05-16 03:04:10 UTC"
  },
  {
    "arxiv_id": "2405.09781v1",
    "title": "An Independent Implementation of Quantum Machine Learning Algorithms in Qiskit for Genomic Data",
    "authors": [
      "Navneet Singh",
      "Shiva Raj Pokhrel"
    ],
    "abstract": "In this paper, we explore the power of Quantum Machine Learning as we extend,\nimplement and evaluate algorithms like Quantum Support Vector Classifier\n(QSVC), Pegasos-QSVC, Variational Quantum Circuits (VQC), and Quantum Neural\nNetworks (QNN) in Qiskit with diverse feature mapping techniques for genomic\nsequence classification.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "2 pager extended abstract",
    "pdf_url": "http://arxiv.org/pdf/2405.09781v1",
    "published_date": "2024-05-16 03:00:41 UTC",
    "updated_date": "2024-05-16 03:00:41 UTC"
  },
  {
    "arxiv_id": "2405.09770v1",
    "title": "Optimization Techniques for Sentiment Analysis Based on LLM (GPT-3)",
    "authors": [
      "Tong Zhan",
      "Chenxi Shi",
      "Yadong Shi",
      "Huixiang Li",
      "Yiyu Lin"
    ],
    "abstract": "With the rapid development of natural language processing (NLP) technology,\nlarge-scale pre-trained language models such as GPT-3 have become a popular\nresearch object in NLP field. This paper aims to explore sentiment analysis\noptimization techniques based on large pre-trained language models such as\nGPT-3 to improve model performance and effect and further promote the\ndevelopment of natural language processing (NLP). By introducing the importance\nof sentiment analysis and the limitations of traditional methods, GPT-3 and\nFine-tuning techniques are introduced in this paper, and their applications in\nsentiment analysis are explained in detail. The experimental results show that\nthe Fine-tuning technique can optimize GPT-3 model and obtain good performance\nin sentiment analysis task. This study provides an important reference for\nfuture sentiment analysis using large-scale language models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09770v1",
    "published_date": "2024-05-16 02:21:13 UTC",
    "updated_date": "2024-05-16 02:21:13 UTC"
  },
  {
    "arxiv_id": "2405.09765v1",
    "title": "Unsupervised Extractive Dialogue Summarization in Hyperdimensional Space",
    "authors": [
      "Seongmin Park",
      "Kyungho Kim",
      "Jaejin Seo",
      "Jihwa Lee"
    ],
    "abstract": "We present HyperSum, an extractive summarization framework that captures both\nthe efficiency of traditional lexical summarization and the accuracy of\ncontemporary neural approaches. HyperSum exploits the pseudo-orthogonality that\nemerges when randomly initializing vectors at extremely high dimensions\n(\"blessing of dimensionality\") to construct representative and efficient\nsentence embeddings. Simply clustering the obtained embeddings and extracting\ntheir medoids yields competitive summaries. HyperSum often outperforms\nstate-of-the-art summarizers -- in terms of both summary accuracy and\nfaithfulness -- while being 10 to 100 times faster. We open-source HyperSum as\na strong baseline for unsupervised extractive summarization.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ICASSP 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.09765v1",
    "published_date": "2024-05-16 02:11:03 UTC",
    "updated_date": "2024-05-16 02:11:03 UTC"
  },
  {
    "arxiv_id": "2405.09763v1",
    "title": "Fusion Intelligence: Confluence of Natural and Artificial Intelligence for Enhanced Problem-Solving Efficiency",
    "authors": [
      "Rohan Reddy Kalavakonda",
      "Junjun Huan",
      "Peyman Dehghanzadeh",
      "Archit Jaiswal",
      "Soumyajit Mandal",
      "Swarup Bhunia"
    ],
    "abstract": "This paper introduces Fusion Intelligence (FI), a bio-inspired intelligent\nsystem, where the innate sensing, intelligence and unique actuation abilities\nof biological organisms such as bees and ants are integrated with the\ncomputational power of Artificial Intelligence (AI). This interdisciplinary\nfield seeks to create systems that are not only smart but also adaptive and\nresponsive in ways that mimic the nature. As FI evolves, it holds the promise\nof revolutionizing the way we approach complex problems, leveraging the best of\nboth biological and digital worlds to create solutions that are more effective,\nsustainable, and harmonious with the environment. We demonstrate FI's potential\nto enhance agricultural IoT system performance through a simulated case study\non improving insect pollination efficacy (entomophily).",
    "categories": [
      "cs.AI",
      "cs.ET",
      "cs.MA",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, 4 figures, 1 Table",
    "pdf_url": "http://arxiv.org/pdf/2405.09763v1",
    "published_date": "2024-05-16 02:10:30 UTC",
    "updated_date": "2024-05-16 02:10:30 UTC"
  },
  {
    "arxiv_id": "2405.10347v4",
    "title": "Networking Systems for Video Anomaly Detection: A Tutorial and Survey",
    "authors": [
      "Jing Liu",
      "Yang Liu",
      "Jieyu Lin",
      "Jielin Li",
      "Liang Cao",
      "Peng Sun",
      "Bo Hu",
      "Liang Song",
      "Azzedine Boukerche",
      "Victor C. M. Leung"
    ],
    "abstract": "The increasing utilization of surveillance cameras in smart cities, coupled\nwith the surge of online video applications, has heightened concerns regarding\npublic security and privacy protection, which propelled automated Video Anomaly\nDetection (VAD) into a fundamental research task within the Artificial\nIntelligence (AI) community. With the advancements in deep learning and edge\ncomputing, VAD has made significant progress and advances synergized with\nemerging applications in smart cities and video internet, which has moved\nbeyond the conventional research scope of algorithm engineering to deployable\nNetworking Systems for VAD (NSVAD), a practical hotspot for intersection\nexploration in the AI, IoVT, and computing fields. In this article, we\ndelineate the foundational assumptions, learning frameworks, and applicable\nscenarios of various deep learning-driven VAD routes, offering an exhaustive\ntutorial for novices in NSVAD. In addition, this article elucidates core\nconcepts by reviewing recent advances and typical solutions and aggregating\navailable research resources accessible at https://github.com/fdjingliu/NSVAD.\nLastly, this article projects future development trends and discusses how the\nintegration of AI and computing technologies can address existing research\nchallenges and promote open opportunities, serving as an insightful guide for\nprospective researchers and engineers.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ACM Computing Surveys. For more information and\n  supplementary material, please visit https://github.com/fdjingliu/NSVAD",
    "pdf_url": "http://arxiv.org/pdf/2405.10347v4",
    "published_date": "2024-05-16 02:00:44 UTC",
    "updated_date": "2025-04-03 05:41:14 UTC"
  },
  {
    "arxiv_id": "2405.10346v1",
    "title": "AMCEN: An Attention Masking-based Contrastive Event Network for Two-stage Temporal Knowledge Graph Reasoning",
    "authors": [
      "Jing Yang",
      "Xiao Wang",
      "Yutong Wang",
      "Jiawei Wang",
      "Fei-Yue Wang"
    ],
    "abstract": "Temporal knowledge graphs (TKGs) can effectively model the ever-evolving\nnature of real-world knowledge, and their completeness and enhancement can be\nachieved by reasoning new events from existing ones. However, reasoning\naccuracy is adversely impacted due to an imbalance between new and recurring\nevents in the datasets. To achieve more accurate TKG reasoning, we propose an\nattention masking-based contrastive event network (AMCEN) with local-global\ntemporal patterns for the two-stage prediction of future events. In the\nnetwork, historical and non-historical attention mask vectors are designed to\ncontrol the attention bias towards historical and non-historical entities,\nacting as the key to alleviating the imbalance. A local-global message-passing\nmodule is proposed to comprehensively consider and capture multi-hop structural\ndependencies and local-global temporal evolution for the in-depth exploration\nof latent impact factors of different event types. A contrastive event\nclassifier is used to classify events more accurately by incorporating\nlocal-global temporal patterns into contrastive learning. Therefore, AMCEN\nrefines the prediction scope with the results of the contrastive event\nclassification, followed by utilizing attention masking-based decoders to\nfinalize the specific outcomes. The results of our experiments on four\nbenchmark datasets highlight the superiority of AMCEN. Especially, the\nconsiderable improvements in Hits@1 prove that AMCEN can make more precise\npredictions about future occurrences.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.10346v1",
    "published_date": "2024-05-16 01:39:50 UTC",
    "updated_date": "2024-05-16 01:39:50 UTC"
  },
  {
    "arxiv_id": "2405.13026v1",
    "title": "Leveraging Human Revisions for Improving Text-to-Layout Models",
    "authors": [
      "Amber Xie",
      "Chin-Yi Cheng",
      "Forrest Huang",
      "Yang Li"
    ],
    "abstract": "Learning from human feedback has shown success in aligning large, pretrained\nmodels with human values. Prior works have mostly focused on learning from\nhigh-level labels, such as preferences between pairs of model outputs. On the\nother hand, many domains could benefit from more involved, detailed feedback,\nsuch as revisions, explanations, and reasoning of human users. Our work\nproposes using nuanced feedback through the form of human revisions for\nstronger alignment. In this paper, we ask expert designers to fix layouts\ngenerated from a generative layout model that is pretrained on a large-scale\ndataset of mobile screens. Then, we train a reward model based on how human\ndesigners revise these generated layouts. With the learned reward model, we\noptimize our model with reinforcement learning from human feedback (RLHF). Our\nmethod, Revision-Aware Reward Models ($\\method$), allows a generative\ntext-to-layout model to produce more modern, designer-aligned layouts, showing\nthe potential for utilizing human revisions and stronger forms of feedback in\nimproving generative models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.13026v1",
    "published_date": "2024-05-16 01:33:09 UTC",
    "updated_date": "2024-05-16 01:33:09 UTC"
  },
  {
    "arxiv_id": "2405.10345v1",
    "title": "Machine Learning Driven Biomarker Selection for Medical Diagnosis",
    "authors": [
      "Divyagna Bavikadi",
      "Ayushi Agarwal",
      "Shashank Ganta",
      "Yunro Chung",
      "Lusheng Song",
      "Ji Qiu",
      "Paulo Shakarian"
    ],
    "abstract": "Recent advances in experimental methods have enabled researchers to collect\ndata on thousands of analytes simultaneously. This has led to correlational\nstudies that associated molecular measurements with diseases such as\nAlzheimer's, Liver, and Gastric Cancer. However, the use of thousands of\nbiomarkers selected from the analytes is not practical for real-world medical\ndiagnosis and is likely undesirable due to potentially formed spurious\ncorrelations. In this study, we evaluate 4 different methods for biomarker\nselection and 4 different machine learning (ML) classifiers for identifying\ncorrelations, evaluating 16 approaches in all. We found that contemporary\nmethods outperform previously reported logistic regression in cases where 3 and\n10 biomarkers are permitted. When specificity is fixed at 0.9, ML approaches\nproduced a sensitivity of 0.240 (3 biomarkers) and 0.520 (10 biomarkers), while\nstandard logistic regression provided a sensitivity of 0.000 (3 biomarkers) and\n0.040 (10 biomarkers). We also noted that causal-based methods for biomarker\nselection proved to be the most performant when fewer biomarkers were\npermitted, while univariate feature selection was the most performant when a\ngreater number of biomarkers were permitted.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.10345v1",
    "published_date": "2024-05-16 01:30:47 UTC",
    "updated_date": "2024-05-16 01:30:47 UTC"
  },
  {
    "arxiv_id": "2407.01563v1",
    "title": "NaviSlim: Adaptive Context-Aware Navigation and Sensing via Dynamic Slimmable Networks",
    "authors": [
      "Tim Johnsen",
      "Marco Levorato"
    ],
    "abstract": "Small-scale autonomous airborne vehicles, such as micro-drones, are expected\nto be a central component of a broad spectrum of applications ranging from\nexploration to surveillance and delivery. This class of vehicles is\ncharacterized by severe constraints in computing power and energy reservoir,\nwhich impairs their ability to support the complex state-of-the-art neural\nmodels needed for autonomous operations. The main contribution of this paper is\na new class of neural navigation models -- NaviSlim -- capable of adapting the\namount of resources spent on computing and sensing in response to the current\ncontext (i.e., difficulty of the environment, current trajectory, and\nnavigation goals). Specifically, NaviSlim is designed as a gated slimmable\nneural network architecture that, different from existing slimmable networks,\ncan dynamically select a slimming factor to autonomously scale model\ncomplexity, which consequently optimizes execution time and energy consumption.\nMoreover, different from existing sensor fusion approaches, NaviSlim can\ndynamically select power levels of onboard sensors to autonomously reduce power\nand time spent during sensor acquisition, without the need to switch between\ndifferent neural networks. By means of extensive training and testing on the\nrobust simulation environment Microsoft AirSim, we evaluate our NaviSlim models\non scenarios with varying difficulty and a test set that showed a dynamic\nreduced model complexity on average between 57-92%, and between 61-80% sensor\nutilization, as compared to static neural networks designed to match computing\nand sensing of that required by the most difficult scenario.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to IoTDI 2024, part of CPS-IoT Week for 2024, Hong Kong, and\n  pending publication in Proceedings",
    "pdf_url": "http://arxiv.org/pdf/2407.01563v1",
    "published_date": "2024-05-16 01:18:52 UTC",
    "updated_date": "2024-05-16 01:18:52 UTC"
  },
  {
    "arxiv_id": "2405.09744v1",
    "title": "Many Hands Make Light Work: Task-Oriented Dialogue System with Module-Based Mixture-of-Experts",
    "authors": [
      "Ruolin Su",
      "Biing-Hwang Juang"
    ],
    "abstract": "Task-oriented dialogue systems are broadly used in virtual assistants and\nother automated services, providing interfaces between users and machines to\nfacilitate specific tasks. Nowadays, task-oriented dialogue systems have\ngreatly benefited from pre-trained language models (PLMs). However, their\ntask-solving performance is constrained by the inherent capacities of PLMs, and\nscaling these models is expensive and complex as the model size becomes larger.\nTo address these challenges, we propose Soft Mixture-of-Expert Task-Oriented\nDialogue system (SMETOD) which leverages an ensemble of Mixture-of-Experts\n(MoEs) to excel at subproblems and generate specialized outputs for\ntask-oriented dialogues. SMETOD also scales up a task-oriented dialogue system\nwith simplicity and flexibility while maintaining inference efficiency. We\nextensively evaluate our model on three benchmark functionalities: intent\nprediction, dialogue state tracking, and dialogue response generation.\nExperimental results demonstrate that SMETOD achieves state-of-the-art\nperformance on most evaluated metrics. Moreover, comparisons against existing\nstrong baselines show that SMETOD has a great advantage in the cost of\ninference and correctness in problem-solving.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.09744v1",
    "published_date": "2024-05-16 01:02:09 UTC",
    "updated_date": "2024-05-16 01:02:09 UTC"
  }
]
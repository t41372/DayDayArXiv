[
  {
    "arxiv_id": "2403.14888v3",
    "title": "AutoRE: Document-Level Relation Extraction with Large Language Models",
    "authors": [
      "Lilong Xue",
      "Dan Zhang",
      "Yuxiao Dong",
      "Jie Tang"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated exceptional abilities in\ncomprehending and generating text, motivating numerous researchers to utilize\nthem for Information Extraction (IE) purposes, including Relation Extraction\n(RE). Nonetheless, most existing methods are predominantly designed for\nSentence-level Relation Extraction (SentRE) tasks, which typically encompass a\nrestricted set of relations and triplet facts within a single sentence.\nFurthermore, certain approaches resort to treating relations as candidate\nchoices integrated into prompt templates, leading to inefficient processing and\nsuboptimal performance when tackling Document-Level Relation Extraction (DocRE)\ntasks, which entail handling multiple relations and triplet facts distributed\nacross a given document, posing distinct challenges. To overcome these\nlimitations, we introduce AutoRE, an end-to-end DocRE model that adopts a novel\nRE extraction paradigm named RHF (Relation-Head-Facts). Unlike existing\napproaches, AutoRE does not rely on the assumption of known relation options,\nmaking it more reflective of real-world scenarios. Additionally, we have\ndeveloped an easily extensible RE framework using a Parameters Efficient Fine\nTuning (PEFT) algorithm (QLoRA). Our experiments on the RE-DocRED dataset\nshowcase AutoRE's best performance, achieving state-of-the-art results,\nsurpassing TAG by 10.03\\% and 9.03\\% respectively on the dev and test set. The\ncode is available at https://github.com/THUDM/AutoRE and the demonstration\nvideo is provided at https://www.youtube.com/watch?v=IhKRsZUAxKk.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.14888v3",
    "published_date": "2024-03-21 23:48:21 UTC",
    "updated_date": "2024-07-26 04:12:16 UTC"
  },
  {
    "arxiv_id": "2405.01394v1",
    "title": "Analysis of a Modular Autonomous Driving Architecture: The Top Submission to CARLA Leaderboard 2.0 Challenge",
    "authors": [
      "Weize Zhang",
      "Mohammed Elmahgiubi",
      "Kasra Rezaee",
      "Behzad Khamidehi",
      "Hamidreza Mirkhani",
      "Fazel Arasteh",
      "Chunlin Li",
      "Muhammad Ahsan Kaleem",
      "Eduardo R. Corral-Soto",
      "Dhruv Sharma",
      "Tongtong Cao"
    ],
    "abstract": "In this paper we present the architecture of the Kyber-E2E submission to the\nmap track of CARLA Leaderboard 2.0 Autonomous Driving (AD) challenge 2023,\nwhich achieved first place. We employed a modular architecture for our solution\nconsists of five main components: sensing, localization, perception,\ntracking/prediction, and planning/control. Our solution leverages\nstate-of-the-art language-assisted perception models to help our planner\nperform more reliably in highly challenging traffic scenarios. We use\nopen-source driving datasets in conjunction with Inverse Reinforcement Learning\n(IRL) to enhance the performance of our motion planner. We provide insight into\nour design choices and trade-offs made to achieve this solution. We also\nexplore the impact of each component in the overall performance of our\nsolution, with the intent of providing a guideline where allocation of\nresources can have the greatest impact.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01394v1",
    "published_date": "2024-03-21 23:44:19 UTC",
    "updated_date": "2024-03-21 23:44:19 UTC"
  },
  {
    "arxiv_id": "2403.14885v1",
    "title": "Establishing a leader in a pairwise comparisons method",
    "authors": [
      "Jacek Szybowski",
      "Konrad Ku≈Çakowski",
      "Jiri Mazurek",
      "Sebastian Ernst"
    ],
    "abstract": "Abstract Like electoral systems, decision-making methods are also vulnerable\nto manipulation by decision-makers. The ability to effectively defend against\nsuch threats can only come from thoroughly understanding the manipulation\nmechanisms. In the presented article, we show two algorithms that can be used\nto launch a manipulation attack. They allow for equating the weights of two\nselected alternatives in the pairwise comparison method and, consequently,\nchoosing a leader. The theoretical considerations are accompanied by a Monte\nCarlo simulation showing the relationship between the size of the PC matrix,\nthe degree of inconsistency, and the ease of manipulation. This work is a\ncontinuation of our previous research published in the paper (Szybowski et al.,\n2023)",
    "categories": [
      "cs.AI",
      "cs.CR",
      "cs.CY",
      "cs.DM"
    ],
    "primary_category": "cs.AI",
    "comment": "9 figures, 19 pages",
    "pdf_url": "http://arxiv.org/pdf/2403.14885v1",
    "published_date": "2024-03-21 23:42:00 UTC",
    "updated_date": "2024-03-21 23:42:00 UTC"
  },
  {
    "arxiv_id": "2403.14864v4",
    "title": "Learning Quadruped Locomotion Using Differentiable Simulation",
    "authors": [
      "Yunlong Song",
      "Sangbae Kim",
      "Davide Scaramuzza"
    ],
    "abstract": "This work explores the potential of using differentiable simulation for\nlearning quadruped locomotion. Differentiable simulation promises fast\nconvergence and stable training by computing low-variance first-order gradients\nusing robot dynamics. However, its usage for legged robots is still limited to\nsimulation. The main challenge lies in the complex optimization landscape of\nrobotic tasks due to discontinuous dynamics. This work proposes a new\ndifferentiable simulation framework to overcome these challenges. Our approach\ncombines a high-fidelity, non-differentiable simulator for forward dynamics\nwith a simplified surrogate model for gradient backpropagation. This approach\nmaintains simulation accuracy by aligning the robot states from the surrogate\nmodel with those of the precise, non-differentiable simulator. Our framework\nenables learning quadruped walking in simulation in minutes without\nparallelization. When augmented with GPU parallelization, our approach allows\nthe quadruped robot to master diverse locomotion skills on challenging terrains\nin minutes. We demonstrate that differentiable simulation outperforms a\nreinforcement learning algorithm (PPO) by achieving significantly better sample\nefficiency while maintaining its effectiveness in handling large-scale\nenvironments. Our method represents one of the first successful applications of\ndifferentiable simulation to real-world quadruped locomotion, offering a\ncompelling alternative to traditional RL methods.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8th Annual Conference on Robot Learning (CoRL)",
    "pdf_url": "http://arxiv.org/pdf/2403.14864v4",
    "published_date": "2024-03-21 22:18:59 UTC",
    "updated_date": "2024-10-15 13:30:26 UTC"
  },
  {
    "arxiv_id": "2403.14859v2",
    "title": "Log Probabilities Are a Reliable Estimate of Semantic Plausibility in Base and Instruction-Tuned Language Models",
    "authors": [
      "Carina Kauf",
      "Emmanuele Chersoni",
      "Alessandro Lenci",
      "Evelina Fedorenko",
      "Anna A. Ivanova"
    ],
    "abstract": "Semantic plausibility (e.g. knowing that \"the actor won the award\" is more\nlikely than \"the actor won the battle\") serves as an effective proxy for\ngeneral world knowledge. Language models (LMs) capture vast amounts of world\nknowledge by learning distributional patterns in text, accessible via log\nprobabilities (LogProbs) they assign to plausible vs. implausible outputs. The\nnew generation of instruction-tuned LMs can now also provide explicit estimates\nof plausibility via prompting. Here, we evaluate the effectiveness of LogProbs\nand basic prompting to measure semantic plausibility, both in single-sentence\nminimal pairs (Experiment 1) and short context-dependent scenarios (Experiment\n2). We find that (i) in both base and instruction-tuned LMs, LogProbs offers a\nmore reliable measure of semantic plausibility than direct zero-shot prompting,\nwhich yields inconsistent and often poor results; (ii) instruction-tuning\ngenerally does not alter the sensitivity of LogProbs to semantic plausibility\n(although sometimes decreases it); (iii) across models, context mostly\nmodulates LogProbs in expected ways, as measured by three novel metrics of\ncontext-sensitive plausibility and their match to explicit human plausibility\njudgments. We conclude that, even in the era of prompt-based evaluations,\nLogProbs constitute a useful metric of semantic plausibility, both in base and\ninstruction-tuned LMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14859v2",
    "published_date": "2024-03-21 22:08:44 UTC",
    "updated_date": "2024-10-21 11:25:48 UTC"
  },
  {
    "arxiv_id": "2403.14843v1",
    "title": "Local Causal Discovery with Linear non-Gaussian Cyclic Models",
    "authors": [
      "Haoyue Dai",
      "Ignavier Ng",
      "Yujia Zheng",
      "Zhengqing Gao",
      "Kun Zhang"
    ],
    "abstract": "Local causal discovery is of great practical significance, as there are often\nsituations where the discovery of the global causal structure is unnecessary,\nand the interest lies solely on a single target variable. Most existing local\nmethods utilize conditional independence relations, providing only a partially\ndirected graph, and assume acyclicity for the ground-truth structure, even\nthough real-world scenarios often involve cycles like feedback mechanisms. In\nthis work, we present a general, unified local causal discovery method with\nlinear non-Gaussian models, whether they are cyclic or acyclic. We extend the\napplication of independent component analysis from the global context to\nindependent subspace analysis, enabling the exact identification of the\nequivalent local directed structures and causal strengths from the Markov\nblanket of the target variable. We also propose an alternative regression-based\nmethod in the particular acyclic scenarios. Our identifiability results are\nempirically validated using both synthetic and real-world datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Appears at AISTATS 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.14843v1",
    "published_date": "2024-03-21 21:27:39 UTC",
    "updated_date": "2024-03-21 21:27:39 UTC"
  },
  {
    "arxiv_id": "2403.14817v1",
    "title": "Crowdsourced Multilingual Speech Intelligibility Testing",
    "authors": [
      "Laura Lechler",
      "Kamil Wojcicki"
    ],
    "abstract": "With the advent of generative audio features, there is an increasing need for\nrapid evaluation of their impact on speech intelligibility. Beyond the existing\nlaboratory measures, which are expensive and do not scale well, there has been\ncomparatively little work on crowdsourced assessment of intelligibility.\nStandards and recommendations are yet to be defined, and publicly available\nmultilingual test materials are lacking. In response to this challenge, we\npropose an approach for a crowdsourced intelligibility assessment. We detail\nthe test design, the collection and public release of the multilingual speech\ndata, and the results of our early experiments.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14817v1",
    "published_date": "2024-03-21 20:14:53 UTC",
    "updated_date": "2024-03-21 20:14:53 UTC"
  },
  {
    "arxiv_id": "2403.14814v3",
    "title": "The opportunities and risks of large language models in mental health",
    "authors": [
      "Hannah R. Lawrence",
      "Renee A. Schneider",
      "Susan B. Rubin",
      "Maja J. Mataric",
      "Daniel J. McDuff",
      "Megan Jones Bell"
    ],
    "abstract": "Global rates of mental health concerns are rising, and there is increasing\nrealization that existing models of mental health care will not adequately\nexpand to meet the demand. With the emergence of large language models (LLMs)\nhas come great optimism regarding their promise to create novel, large-scale\nsolutions to support mental health. Despite their nascence, LLMs have already\nbeen applied to mental health related tasks. In this paper, we summarize the\nextant literature on efforts to use LLMs to provide mental health education,\nassessment, and intervention and highlight key opportunities for positive\nimpact in each area. We then highlight risks associated with LLMs' application\nto mental health and encourage the adoption of strategies to mitigate these\nrisks. The urgent need for mental health support must be balanced with\nresponsible development, testing, and deployment of mental health LLMs. It is\nespecially critical to ensure that mental health LLMs are fine-tuned for mental\nhealth, enhance mental health equity, and adhere to ethical standards and that\npeople, including those with lived experience with mental health concerns, are\ninvolved in all stages from development through deployment. Prioritizing these\nefforts will minimize potential harms to mental health and maximize the\nlikelihood that LLMs will positively impact mental health globally.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 2 tables, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.14814v3",
    "published_date": "2024-03-21 19:59:52 UTC",
    "updated_date": "2024-08-01 15:15:34 UTC"
  },
  {
    "arxiv_id": "2403.14800v1",
    "title": "Deep Active Learning: A Reality Check",
    "authors": [
      "Edrina Gashi",
      "Jiankang Deng",
      "Ismail Elezi"
    ],
    "abstract": "We conduct a comprehensive evaluation of state-of-the-art deep active\nlearning methods. Surprisingly, under general settings, no single-model method\ndecisively outperforms entropy-based active learning, and some even fall short\nof random sampling. We delve into overlooked aspects like starting budget,\nbudget step, and pretraining's impact, revealing their significance in\nachieving superior results. Additionally, we extend our evaluation to other\ntasks, exploring the active learning effectiveness in combination with\nsemi-supervised learning, and object detection. Our experiments provide\nvaluable insights and concrete recommendations for future active learning\nstudies. By uncovering the limitations of current methods and understanding the\nimpact of different experimental settings, we aim to inspire more efficient\ntraining of deep learning models in real-world scenarios with limited\nannotation budgets. This work contributes to advancing active learning's\nefficacy in deep learning and empowers researchers to make informed decisions\nwhen applying active learning to their tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14800v1",
    "published_date": "2024-03-21 19:28:17 UTC",
    "updated_date": "2024-03-21 19:28:17 UTC"
  },
  {
    "arxiv_id": "2403.14796v1",
    "title": "Planning and Acting While the Clock Ticks",
    "authors": [
      "Andrew Coles",
      "Erez Karpas",
      "Andrey Lavrinenko",
      "Wheeler Ruml",
      "Solomon Eyal Shimony",
      "Shahaf Shperberg"
    ],
    "abstract": "Standard temporal planning assumes that planning takes place offline and then\nexecution starts at time 0. Recently, situated temporal planning was\nintroduced, where planning starts at time 0 and execution occurs after planning\nterminates. Situated temporal planning reflects a more realistic scenario where\ntime passes during planning. However, in situated temporal planning a complete\nplan must be generated before any action is executed. In some problems with\ntime pressure, timing is too tight to complete planning before the first action\nmust be executed. For example, an autonomous car that has a truck backing\ntowards it should probably move out of the way now and plan how to get to its\ndestination later. In this paper, we propose a new problem setting: concurrent\nplanning and execution, in which actions can be dispatched (executed) before\nplanning terminates. Unlike previous work on planning and execution, we must\nhandle wall clock deadlines that affect action applicability and goal\nachievement (as in situated planning) while also supporting dispatching actions\nbefore a complete plan has been found. We extend previous work on metareasoning\nfor situated temporal planning to develop an algorithm for this new setting.\nOur empirical evaluation shows that when there is strong time pressure, our\napproach outperforms situated temporal planning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14796v1",
    "published_date": "2024-03-21 19:18:47 UTC",
    "updated_date": "2024-03-21 19:18:47 UTC"
  },
  {
    "arxiv_id": "2403.14791v4",
    "title": "Particip-AI: A Democratic Surveying Framework for Anticipating Future AI Use Cases, Harms and Benefits",
    "authors": [
      "Jimin Mun",
      "Liwei Jiang",
      "Jenny Liang",
      "Inyoung Cheong",
      "Nicole DeCario",
      "Yejin Choi",
      "Tadayoshi Kohno",
      "Maarten Sap"
    ],
    "abstract": "General purpose AI, such as ChatGPT, seems to have lowered the barriers for\nthe public to use AI and harness its power. However, the governance and\ndevelopment of AI still remain in the hands of a few, and the pace of\ndevelopment is accelerating without a comprehensive assessment of risks. As a\nfirst step towards democratic risk assessment and design of general purpose AI,\nwe introduce PARTICIP-AI, a carefully designed framework for laypeople to\nspeculate and assess AI use cases and their impacts. Our framework allows us to\nstudy more nuanced and detailed public opinions on AI through collecting use\ncases, surfacing diverse harms through risk assessment under alternate\nscenarios (i.e., developing and not developing a use case), and illuminating\ntensions over AI development through making a concluding choice on its\ndevelopment. To showcase the promise of our framework towards informing\ndemocratic AI development, we run a medium-scale study with inputs from 295\ndemographically diverse participants. Our analyses show that participants'\nresponses emphasize applications for personal life and society, contrasting\nwith most current AI development's business focus. We also surface diverse set\nof envisioned harms such as distrust in AI and institutions, complementary to\nthose defined by experts. Furthermore, we found that perceived impact of not\ndeveloping use cases significantly predicted participants' judgements of\nwhether AI use cases should be developed, and highlighted lay users' concerns\nof techno-solutionism. We conclude with a discussion on how frameworks like\nPARTICIP-AI can further guide democratic AI development and governance.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "AIES 2024, 34 pages, 4 figures, 23 tables",
    "pdf_url": "http://arxiv.org/pdf/2403.14791v4",
    "published_date": "2024-03-21 19:12:37 UTC",
    "updated_date": "2024-09-09 20:09:02 UTC"
  },
  {
    "arxiv_id": "2403.14790v1",
    "title": "Latent Diffusion Models for Attribute-Preserving Image Anonymization",
    "authors": [
      "Luca Piano",
      "Pietro Basci",
      "Fabrizio Lamberti",
      "Lia Morra"
    ],
    "abstract": "Generative techniques for image anonymization have great potential to\ngenerate datasets that protect the privacy of those depicted in the images,\nwhile achieving high data fidelity and utility. Existing methods have focused\nextensively on preserving facial attributes, but failed to embrace a more\ncomprehensive perspective that considers the scene and background into the\nanonymization process. This paper presents, to the best of our knowledge, the\nfirst approach to image anonymization based on Latent Diffusion Models (LDMs).\nEvery element of a scene is maintained to convey the same meaning, yet\nmanipulated in a way that makes re-identification difficult. We propose two\nLDMs for this purpose: CAMOUFLaGE-Base exploits a combination of pre-trained\nControlNets, and a new controlling mechanism designed to increase the distance\nbetween the real and anonymized images. CAMOFULaGE-Light is based on the\nAdapter technique, coupled with an encoding designed to efficiently represent\nthe attributes of different persons in a scene. The former solution achieves\nsuperior performance on most metrics and benchmarks, while the latter cuts the\ninference time in half at the cost of fine-tuning a lightweight module. We show\nthrough extensive experimental comparison that the proposed method is\ncompetitive with the state-of-the-art concerning identity obfuscation whilst\nbetter preserving the original content of the image and tackling unresolved\nchallenges that current solutions fail to address.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14790v1",
    "published_date": "2024-03-21 19:09:21 UTC",
    "updated_date": "2024-03-21 19:09:21 UTC"
  },
  {
    "arxiv_id": "2403.14783v1",
    "title": "Multi-Agent VQA: Exploring Multi-Agent Foundation Models in Zero-Shot Visual Question Answering",
    "authors": [
      "Bowen Jiang",
      "Zhijun Zhuang",
      "Shreyas S. Shivakumar",
      "Dan Roth",
      "Camillo J. Taylor"
    ],
    "abstract": "This work explores the zero-shot capabilities of foundation models in Visual\nQuestion Answering (VQA) tasks. We propose an adaptive multi-agent system,\nnamed Multi-Agent VQA, to overcome the limitations of foundation models in\nobject detection and counting by using specialized agents as tools. Unlike\nexisting approaches, our study focuses on the system's performance without\nfine-tuning it on specific VQA datasets, making it more practical and robust in\nthe open world. We present preliminary experimental results under zero-shot\nscenarios and highlight some failure cases, offering new directions for future\nresearch.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.CV",
    "comment": "A full version of the paper will be released soon. The codes are\n  available at https://github.com/bowen-upenn/Multi-Agent-VQA",
    "pdf_url": "http://arxiv.org/pdf/2403.14783v1",
    "published_date": "2024-03-21 18:57:25 UTC",
    "updated_date": "2024-03-21 18:57:25 UTC"
  },
  {
    "arxiv_id": "2403.15499v1",
    "title": "A Causal Analysis of CO2 Reduction Strategies in Electricity Markets Through Machine Learning-Driven Metalearners",
    "authors": [
      "Iman Emtiazi Naeini",
      "Zahra Saberi",
      "Khadijeh Hassanzadeh"
    ],
    "abstract": "This study employs the Causal Machine Learning (CausalML) statistical method\nto analyze the influence of electricity pricing policies on carbon dioxide\n(CO2) levels in the household sector. Investigating the causality between\npotential outcomes and treatment effects, where changes in pricing policies are\nthe treatment, our analysis challenges the conventional wisdom surrounding\nincentive-based electricity pricing. The study's findings suggest that adopting\nsuch policies may inadvertently increase CO2 intensity. Additionally, we\nintegrate a machine learning-based meta-algorithm, reflecting a contemporary\nstatistical approach, to enhance the depth of our causal analysis. The study\nconducts a comparative analysis of learners X, T, S, and R to ascertain the\noptimal methods based on the defined question's specified goals and contextual\nnuances. This research contributes valuable insights to the ongoing dialogue on\nsustainable development practices, emphasizing the importance of considering\nunintended consequences in policy formulation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15499v1",
    "published_date": "2024-03-21 18:55:05 UTC",
    "updated_date": "2024-03-21 18:55:05 UTC"
  },
  {
    "arxiv_id": "2403.14773v2",
    "title": "StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text",
    "authors": [
      "Roberto Henschel",
      "Levon Khachatryan",
      "Hayk Poghosyan",
      "Daniil Hayrapetyan",
      "Vahram Tadevosyan",
      "Zhangyang Wang",
      "Shant Navasardyan",
      "Humphrey Shi"
    ],
    "abstract": "Text-to-video diffusion models enable the generation of high-quality videos\nthat follow text instructions, making it easy to create diverse and individual\ncontent. However, existing approaches mostly focus on high-quality short video\ngeneration (typically 16 or 24 frames), ending up with hard-cuts when naively\nextended to the case of long video synthesis. To overcome these limitations, we\nintroduce StreamingT2V, an autoregressive approach for long video generation of\n80, 240, 600, 1200 or more frames with smooth transitions. The key components\nare:(i) a short-term memory block called conditional attention module (CAM),\nwhich conditions the current generation on the features extracted from the\nprevious chunk via an attentional mechanism, leading to consistent chunk\ntransitions, (ii) a long-term memory block called appearance preservation\nmodule, which extracts high-level scene and object features from the first\nvideo chunk to prevent the model from forgetting the initial scene, and (iii) a\nrandomized blending approach that enables to apply a video enhancer\nautoregressively for infinitely long videos without inconsistencies between\nchunks. Experiments show that StreamingT2V generates high motion amount. In\ncontrast, all competing image-to-video methods are prone to video stagnation\nwhen applied naively in an autoregressive manner. Thus, we propose with\nStreamingT2V a high-quality seamless text-to-long video generator that\noutperforms competitors with consistency and motion. Our code will be available\nat: https://github.com/Picsart-AI-Research/StreamingT2V",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "https://github.com/Picsart-AI-Research/StreamingT2V",
    "pdf_url": "http://arxiv.org/pdf/2403.14773v2",
    "published_date": "2024-03-21 18:27:29 UTC",
    "updated_date": "2025-04-16 13:38:58 UTC"
  },
  {
    "arxiv_id": "2403.14772v2",
    "title": "Improving Robustness to Model Inversion Attacks via Sparse Coding Architectures",
    "authors": [
      "Sayanton V. Dibbo",
      "Adam Breuer",
      "Juston Moore",
      "Michael Teti"
    ],
    "abstract": "Recent model inversion attack algorithms permit adversaries to reconstruct a\nneural network's private and potentially sensitive training data by repeatedly\nquerying the network. In this work, we develop a novel network architecture\nthat leverages sparse-coding layers to obtain superior robustness to this class\nof attacks. Three decades of computer science research has studied sparse\ncoding in the context of image denoising, object recognition, and adversarial\nmisclassification settings, but to the best of our knowledge, its connection to\nstate-of-the-art privacy vulnerabilities remains unstudied. In this work, we\nhypothesize that sparse coding architectures suggest an advantageous means to\ndefend against model inversion attacks because they allow us to control the\namount of irrelevant private information encoded by a network in a manner that\nis known to have little effect on classification accuracy. Specifically,\ncompared to networks trained with a variety of state-of-the-art defenses, our\nsparse-coding architectures maintain comparable or higher classification\naccuracy while degrading state-of-the-art training data reconstructions by\nfactors of 1.1 to 18.3 across a variety of reconstruction quality metrics\n(PSNR, SSIM, FID). This performance advantage holds across 5 datasets ranging\nfrom CelebA faces to medical images and CIFAR-10, and across various\nstate-of-the-art SGD-based and GAN-based inversion attacks, including\nPlug-&-Play attacks. We provide a cluster-ready PyTorch codebase to promote\nresearch and standardize defense evaluations.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.14772v2",
    "published_date": "2024-03-21 18:26:23 UTC",
    "updated_date": "2024-08-24 18:29:35 UTC"
  },
  {
    "arxiv_id": "2403.14763v1",
    "title": "Gravitational Duals from Equations of State",
    "authors": [
      "Yago Bea",
      "Raul Jimenez",
      "David Mateos",
      "Shuheng Liu",
      "Pavlos Protopapas",
      "Pedro Taranc√≥n-√Ålvarez",
      "Pablo Tejerina-P√©rez"
    ],
    "abstract": "Holography relates gravitational theories in five dimensions to\nfour-dimensional quantum field theories in flat space. Under this map, the\nequation of state of the field theory is encoded in the black hole solutions of\nthe gravitational theory. Solving the five-dimensional Einstein's equations to\ndetermine the equation of state is an algorithmic, direct problem. Determining\nthe gravitational theory that gives rise to a prescribed equation of state is a\nmuch more challenging, inverse problem. We present a novel approach to solve\nthis problem based on physics-informed neural networks. The resulting algorithm\nis not only data-driven but also informed by the physics of the Einstein's\nequations. We successfully apply it to theories with crossovers, first- and\nsecond-order phase transitions.",
    "categories": [
      "hep-th",
      "astro-ph.CO",
      "cs.AI",
      "cs.LG",
      "gr-qc"
    ],
    "primary_category": "hep-th",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14763v1",
    "published_date": "2024-03-21 18:07:32 UTC",
    "updated_date": "2024-03-21 18:07:32 UTC"
  },
  {
    "arxiv_id": "2403.14624v2",
    "title": "MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?",
    "authors": [
      "Renrui Zhang",
      "Dongzhi Jiang",
      "Yichi Zhang",
      "Haokun Lin",
      "Ziyu Guo",
      "Pengshuo Qiu",
      "Aojun Zhou",
      "Pan Lu",
      "Kai-Wei Chang",
      "Peng Gao",
      "Hongsheng Li"
    ],
    "abstract": "The remarkable progress of Multi-modal Large Language Models (MLLMs) has\ngarnered unparalleled attention, due to their superior performance in visual\ncontexts. However, their capabilities in visual math problem-solving remain\ninsufficiently evaluated and understood. We investigate current benchmarks to\nincorporate excessive visual content within textual questions, which\npotentially assist MLLMs in deducing answers without truly interpreting the\ninput diagrams. To this end, we introduce MathVerse, an all-around visual math\nbenchmark designed for an equitable and in-depth evaluation of MLLMs. We\nmeticulously collect 2,612 high-quality, multi-subject math problems with\ndiagrams from publicly available sources. Each problem is then transformed by\nhuman annotators into six distinct versions, each offering varying degrees of\ninformation content in multi-modality, contributing to 15K test samples in\ntotal. This approach allows MathVerse to comprehensively assess whether and how\nmuch MLLMs can truly understand the visual diagrams for mathematical reasoning.\nIn addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a\nfine-grained assessment of the output answers. Rather than naively judging True\nor False, we employ GPT-4(V) to adaptively extract crucial reasoning steps, and\nthen score each step with detailed error analysis, which can reveal the\nintermediate CoT reasoning quality by MLLMs. We hope the MathVerse benchmark\nmay provide unique insights to guide the future development of MLLMs. Project\npage: https://mathverse-cuhk.github.io",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ECCV 2024, 46 Pages, Benchmark Project Page:\n  https://mathverse-cuhk.github.io",
    "pdf_url": "http://arxiv.org/pdf/2403.14624v2",
    "published_date": "2024-03-21 17:59:50 UTC",
    "updated_date": "2024-08-18 08:10:16 UTC"
  },
  {
    "arxiv_id": "2403.14617v3",
    "title": "Videoshop: Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion",
    "authors": [
      "Xiang Fan",
      "Anand Bhattad",
      "Ranjay Krishna"
    ],
    "abstract": "We introduce Videoshop, a training-free video editing algorithm for localized\nsemantic edits. Videoshop allows users to use any editing software, including\nPhotoshop and generative inpainting, to modify the first frame; it\nautomatically propagates those changes, with semantic, spatial, and temporally\nconsistent motion, to the remaining frames. Unlike existing methods that enable\nedits only through imprecise textual instructions, Videoshop allows users to\nadd or remove objects, semantically change objects, insert stock photos into\nvideos, etc. with fine-grained control over locations and appearance. We\nachieve this through image-based video editing by inverting latents with noise\nextrapolation, from which we generate videos conditioned on the edited image.\nVideoshop produces higher quality edits against 6 baselines on 2 editing\nbenchmarks using 10 evaluation metrics.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ECCV 2024. Project page:\n  https://videoshop-editing.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2403.14617v3",
    "published_date": "2024-03-21 17:59:03 UTC",
    "updated_date": "2024-10-24 20:10:30 UTC"
  },
  {
    "arxiv_id": "2403.14606v2",
    "title": "The Elements of Differentiable Programming",
    "authors": [
      "Mathieu Blondel",
      "Vincent Roulet"
    ],
    "abstract": "Artificial intelligence has recently experienced remarkable advances, fueled\nby large models, vast datasets, accelerated hardware, and, last but not least,\nthe transformative power of differentiable programming. This new programming\nparadigm enables end-to-end differentiation of complex computer programs\n(including those with control flows and data structures), making gradient-based\noptimization of program parameters possible. As an emerging paradigm,\ndifferentiable programming builds upon several areas of computer science and\napplied mathematics, including automatic differentiation, graphical models,\noptimization and statistics. This book presents a comprehensive review of the\nfundamental concepts useful for differentiable programming. We adopt two main\nperspectives, that of optimization and that of probability, with clear\nanalogies between the two. Differentiable programming is not merely the\ndifferentiation of programs, but also the thoughtful design of programs\nintended for differentiation. By making programs differentiable, we inherently\nintroduce probability distributions over their execution, providing a means to\nquantify the uncertainty associated with program outputs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.LG",
    "comment": "Draft version 2",
    "pdf_url": "http://arxiv.org/pdf/2403.14606v2",
    "published_date": "2024-03-21 17:55:16 UTC",
    "updated_date": "2024-07-24 16:56:17 UTC"
  },
  {
    "arxiv_id": "2403.14592v1",
    "title": "Envisioning the Next-Generation AI Coding Assistants: Insights & Proposals",
    "authors": [
      "Khanh Nghiem",
      "Anh Minh Nguyen",
      "Nghi D. Q. Bui"
    ],
    "abstract": "As a research-product hybrid group in AI for Software Engineering (AI4SE), we\npresent four key takeaways from our experience developing in-IDE AI coding\nassistants. AI coding assistants should set clear expectations for usage,\nintegrate with advanced IDE capabilities and existing extensions, use\nextendable backend designs, and collect app data responsibly for downstream\nanalyses. We propose open questions and challenges that academia and industry\nshould address to realize the vision of next-generation AI coding assistants.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14592v1",
    "published_date": "2024-03-21 17:47:28 UTC",
    "updated_date": "2024-03-21 17:47:28 UTC"
  },
  {
    "arxiv_id": "2403.14589v3",
    "title": "ReAct Meets ActRe: When Language Agents Enjoy Training Data Autonomy",
    "authors": [
      "Zonghan Yang",
      "Peng Li",
      "Ming Yan",
      "Ji Zhang",
      "Fei Huang",
      "Yang Liu"
    ],
    "abstract": "Language agents have demonstrated autonomous decision-making abilities by\nreasoning with foundation models. Recently, efforts have been made to train\nlanguage agents for performance improvement, with multi-step reasoning and\naction trajectories as the training data. However, collecting such trajectories\nstill requires considerable human effort, by either artificial annotation or\nimplementations of diverse prompting frameworks. In this work, we propose\nA$^3$T, a framework that enables the Autonomous Annotation of Agent\nTrajectories in the style of ReAct. The central role is an ActRe prompting\nagent, which explains the reason for an arbitrary action. When randomly\nsampling an external action, the ReAct-style agent could query the ActRe agent\nwith the action to obtain its textual rationales. Novel trajectories are then\nsynthesized by prepending the posterior reasoning from ActRe to the sampled\naction. In this way, the ReAct-style agent executes multiple trajectories for\nthe failed tasks, and selects the successful ones to supplement its failed\ntrajectory for contrastive self-training. Realized by policy gradient methods\nwith binarized rewards, the contrastive self-training with accumulated\ntrajectories facilitates a closed loop for multiple rounds of language agent\nself-improvement. We conduct experiments using QLoRA fine-tuning with the\nopen-sourced Mistral-7B-Instruct-v0.2. In AlfWorld, the agent trained with\nA$^3$T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative\nrounds. In WebShop, the 1-shot performance of the A$^3$T agent matches human\naverage, and 4 rounds of iterative refinement lead to the performance\napproaching human experts. A$^3$T agents significantly outperform existing\ntechniques, including prompting with GPT-4, advanced agent frameworks, and\nfully fine-tuned LLMs.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14589v3",
    "published_date": "2024-03-21 17:43:44 UTC",
    "updated_date": "2024-04-01 17:37:15 UTC"
  },
  {
    "arxiv_id": "2403.14582v1",
    "title": "Large Language Models for Multi-Choice Question Classification of Medical Subjects",
    "authors": [
      "V√≠ctor Ponce-L√≥pez"
    ],
    "abstract": "The aim of this paper is to evaluate whether large language models trained on\nmulti-choice question data can be used to discriminate between medical\nsubjects. This is an important and challenging task for automatic question\nanswering. To achieve this goal, we train deep neural networks for multi-class\nclassification of questions into the inferred medical subjects. Using our\nMulti-Question (MQ) Sequence-BERT method, we outperform the state-of-the-art\nresults on the MedMCQA dataset with an accuracy of 0.68 and 0.60 on their\ndevelopment and test sets, respectively. In this sense, we show the capability\nof AI and LLMs in particular for multi-classification tasks in the Healthcare\ndomain.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14582v1",
    "published_date": "2024-03-21 17:36:08 UTC",
    "updated_date": "2024-03-21 17:36:08 UTC"
  },
  {
    "arxiv_id": "2403.14578v1",
    "title": "RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants in the Biomedical Domain",
    "authors": [
      "William James Bolton",
      "Rafael Poyiadzi",
      "Edward R. Morrell",
      "Gabriela van Bergen Gonzalez Bueno",
      "Lea Goetz"
    ],
    "abstract": "Large Language Models (LLMs) increasingly support applications in a wide\nrange of domains, some with potential high societal impact such as biomedicine,\nyet their reliability in realistic use cases is under-researched. In this work\nwe introduce the Reliability AssesMent for Biomedical LLM Assistants (RAmBLA)\nframework and evaluate whether four state-of-the-art foundation LLMs can serve\nas reliable assistants in the biomedical domain. We identify prompt robustness,\nhigh recall, and a lack of hallucinations as necessary criteria for this use\ncase. We design shortform tasks and tasks requiring LLM freeform responses\nmimicking real-world user interactions. We evaluate LLM performance using\nsemantic similarity with a ground truth response, through an evaluator LLM.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at ICLR 2024 Workshop on Reliable and Responsible\n  Foundation Models",
    "pdf_url": "http://arxiv.org/pdf/2403.14578v1",
    "published_date": "2024-03-21 17:30:59 UTC",
    "updated_date": "2024-03-21 17:30:59 UTC"
  },
  {
    "arxiv_id": "2403.14566v2",
    "title": "A survey on Concept-based Approaches For Model Improvement",
    "authors": [
      "Avani Gupta",
      "P J Narayanan"
    ],
    "abstract": "The focus of recent research has shifted from merely improving the metrics\nbased performance of Deep Neural Networks (DNNs) to DNNs which are more\ninterpretable to humans. The field of eXplainable Artificial Intelligence (XAI)\nhas observed various techniques, including saliency-based and concept-based\napproaches. These approaches explain the model's decisions in simple human\nunderstandable terms called Concepts. Concepts are known to be the thinking\nground of humans}. Explanations in terms of concepts enable detecting spurious\ncorrelations, inherent biases, or clever-hans. With the advent of concept-based\nexplanations, a range of concept representation methods and automatic concept\ndiscovery algorithms have been introduced. Some recent works also use concepts\nfor model improvement in terms of interpretability and generalization. We\nprovide a systematic review and taxonomy of various concept representations and\ntheir discovery algorithms in DNNs, specifically in vision. We also provide\ndetails on concept-based model improvement literature marking the first\ncomprehensive survey of these methods.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14566v2",
    "published_date": "2024-03-21 17:09:20 UTC",
    "updated_date": "2024-03-23 09:50:23 UTC"
  },
  {
    "arxiv_id": "2403.14562v2",
    "title": "Agentic AI: The Era of Semantic Decoding",
    "authors": [
      "Maxime Peyrard",
      "Martin Josifoski",
      "Robert West"
    ],
    "abstract": "Recent work demonstrated great promise in the idea of orchestrating\ncollaborations between LLMs, human input, and various tools to address the\ninherent limitations of LLMs. We propose a novel perspective called semantic\ndecoding, which frames these collaborative processes as optimization procedures\nin semantic space. Specifically, we conceptualize LLMs as semantic processors\nthat manipulate meaningful pieces of information that we call semantic tokens\n(known thoughts). LLMs are among a large pool of other semantic processors,\nincluding humans and tools, such as search engines or code executors.\nCollectively, semantic processors engage in dynamic exchanges of semantic\ntokens to progressively construct high-utility outputs. We refer to these\norchestrated interactions among semantic processors, optimizing and searching\nin semantic space, as semantic decoding algorithms. This concept draws a direct\nparallel to the well-studied problem of syntactic decoding, which involves\ncrafting algorithms to best exploit auto-regressive language models for\nextracting high-utility sequences of syntactic tokens. By focusing on the\nsemantic level and disregarding syntactic details, we gain a fresh perspective\non the engineering of AI systems, enabling us to imagine systems with much\ngreater complexity and capabilities. In this position paper, we formalize the\ntransition from syntactic to semantic tokens as well as the analogy between\nsyntactic and semantic decoding. Subsequently, we explore the possibilities of\noptimizing within the space of semantic tokens via semantic decoding\nalgorithms. We conclude with a list of research opportunities and questions\narising from this fresh perspective. The semantic decoding perspective offers a\npowerful abstraction for search and optimization directly in the space of\nmeaningful concepts, with semantic tokens as the fundamental units of a new\ntype of computation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "25 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.14562v2",
    "published_date": "2024-03-21 17:06:17 UTC",
    "updated_date": "2025-04-29 15:24:52 UTC"
  },
  {
    "arxiv_id": "2403.14551v1",
    "title": "Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling",
    "authors": [
      "Chengxu Zhuang",
      "Evelina Fedorenko",
      "Jacob Andreas"
    ],
    "abstract": "Today's most accurate language models are trained on orders of magnitude more\nlanguage data than human language learners receive - but with no supervision\nfrom other sensory modalities that play a crucial role in human learning. Can\nwe make LMs' representations and predictions more accurate (and more\nhuman-like) with more ecologically plausible supervision? This paper describes\nLexiContrastive Grounding (LCG), a grounded language learning procedure that\nleverages visual supervision to improve textual representations.\nLexiContrastive Grounding combines a next token prediction strategy with a\ncontrastive visual grounding objective, focusing on early-layer representations\nthat encode lexical information. Across multiple word-learning and\nsentence-understanding benchmarks, LexiContrastive Grounding not only\noutperforms standard language-only models in learning efficiency, but also\nimproves upon vision-and-language learning procedures including CLIP, GIT,\nFlamingo, and Vokenization. Moreover, LexiContrastive Grounding improves\nperplexity by around 5% on multiple language modeling tasks. This work\nunderscores the potential of incorporating visual grounding into language\nmodels, aligning more closely with the multimodal nature of human language\nacquisition.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14551v1",
    "published_date": "2024-03-21 16:52:01 UTC",
    "updated_date": "2024-03-21 16:52:01 UTC"
  },
  {
    "arxiv_id": "2403.14550v1",
    "title": "Dynamic Explanation Emphasis in Human-XAI Interaction with Communication Robot",
    "authors": [
      "Yosuke Fukuchi",
      "Seiji Yamada"
    ],
    "abstract": "Communication robots have the potential to contribute to effective human-XAI\ninteraction as an interface that goes beyond textual or graphical explanations.\nOne of their strengths is that they can use physical and vocal expressions to\nadd detailed nuances to explanations. However, it is not clear how a robot can\napply such expressions, or in particular, how we can develop a strategy to\nadaptively use such expressions depending on the task and user in dynamic\ninteractions. To address this question, this paper proposes DynEmph, a method\nfor a communication robot to decide where to emphasize XAI-generated\nexplanations with physical expressions. It predicts the effect of emphasizing\ncertain points on a user and aims to minimize the expected difference between\npredicted user decisions and AI-suggested ones. DynEmph features a strategy for\ndeciding where to emphasize in a data-driven manner, relieving engineers from\nthe need to manually design a strategy. We further conducted experiments to\ninvestigate how emphasis selection strategies affect the performance of user\ndecisions. The results suggest that, while a naive strategy (emphasizing\nexplanations for an AI's most probable class) does not necessarily work better,\nDynEmph effectively guides users to better decisions under the condition that\nthe performance of the AI suggestion is high.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14550v1",
    "published_date": "2024-03-21 16:50:12 UTC",
    "updated_date": "2024-03-21 16:50:12 UTC"
  },
  {
    "arxiv_id": "2403.14539v2",
    "title": "Robust 3D Shape Reconstruction in Zero-Shot from a Single Image in the Wild",
    "authors": [
      "Junhyeong Cho",
      "Kim Youwang",
      "Hunmin Yang",
      "Tae-Hyun Oh"
    ],
    "abstract": "Recent monocular 3D shape reconstruction methods have shown promising\nzero-shot results on object-segmented images without any occlusions. However,\ntheir effectiveness is significantly compromised in real-world conditions, due\nto imperfect object segmentation by off-the-shelf models and the prevalence of\nocclusions. To effectively address these issues, we propose a unified\nregression model that integrates segmentation and reconstruction, specifically\ndesigned for occlusion-aware 3D shape reconstruction. To facilitate its\nreconstruction in the wild, we also introduce a scalable data synthesis\npipeline that simulates a wide range of variations in objects, occluders, and\nbackgrounds. Training on our synthetic data enables the proposed model to\nachieve state-of-the-art zero-shot results on real-world images, using\nsignificantly fewer parameters than competing approaches.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14539v2",
    "published_date": "2024-03-21 16:40:10 UTC",
    "updated_date": "2024-11-28 13:53:55 UTC"
  },
  {
    "arxiv_id": "2403.14526v1",
    "title": "Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion Descriptors",
    "authors": [
      "Nikolaos Tsagkas",
      "Jack Rome",
      "Subramanian Ramamoorthy",
      "Oisin Mac Aodha",
      "Chris Xiaoxuan Lu"
    ],
    "abstract": "Precise manipulation that is generalizable across scenes and objects remains\na persistent challenge in robotics. Current approaches for this task heavily\ndepend on having a significant number of training instances to handle objects\nwith pronounced visual and/or geometric part ambiguities. Our work explores the\ngrounding of fine-grained part descriptors for precise manipulation in a\nzero-shot setting by utilizing web-trained text-to-image diffusion-based\ngenerative models. We tackle the problem by framing it as a dense semantic part\ncorrespondence task. Our model returns a gripper pose for manipulating a\nspecific part, using as reference a user-defined click from a source image of a\nvisually different instance of the same object. We require no manual grasping\ndemonstrations as we leverage the intrinsic object geometry and features.\nPractical experiments in a real-world tabletop scenario validate the efficacy\nof our approach, demonstrating its potential for advancing semantic-aware\nrobotics manipulation. Web page: https://tsagkas.github.io/click2grasp",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.14526v1",
    "published_date": "2024-03-21 16:26:19 UTC",
    "updated_date": "2024-03-21 16:26:19 UTC"
  },
  {
    "arxiv_id": "2403.14508v1",
    "title": "Constrained Reinforcement Learning with Smoothed Log Barrier Function",
    "authors": [
      "Baohe Zhang",
      "Yuan Zhang",
      "Lilli Frison",
      "Thomas Brox",
      "Joschka B√∂decker"
    ],
    "abstract": "Reinforcement Learning (RL) has been widely applied to many control tasks and\nsubstantially improved the performances compared to conventional control\nmethods in many domains where the reward function is well defined. However, for\nmany real-world problems, it is often more convenient to formulate optimization\nproblems in terms of rewards and constraints simultaneously. Optimizing such\nconstrained problems via reward shaping can be difficult as it requires tedious\nmanual tuning of reward functions with several interacting terms. Recent\nformulations which include constraints mostly require a pre-training phase,\nwhich often needs human expertise to collect data or assumes having a\nsub-optimal policy readily available. We propose a new constrained RL method\ncalled CSAC-LB (Constrained Soft Actor-Critic with Log Barrier Function), which\nachieves competitive performance without any pre-training by applying a linear\nsmoothed log barrier function to an additional safety critic. It implements an\nadaptive penalty for policy learning and alleviates the numerical issues that\nare known to complicate the application of the log barrier function method. As\na result, we show that with CSAC-LB, we achieve state-of-the-art performance on\nseveral constrained control tasks with different levels of difficulty and\nevaluate our methods in a locomotion task on a real quadruped robot platform.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14508v1",
    "published_date": "2024-03-21 16:02:52 UTC",
    "updated_date": "2024-03-21 16:02:52 UTC"
  },
  {
    "arxiv_id": "2403.14504v1",
    "title": "Soft Learning Probabilistic Circuits",
    "authors": [
      "Soroush Ghandi",
      "Benjamin Quost",
      "Cassio de Campos"
    ],
    "abstract": "Probabilistic Circuits (PCs) are prominent tractable probabilistic models,\nallowing for a range of exact inferences. This paper focuses on the main\nalgorithm for training PCs, LearnSPN, a gold standard due to its efficiency,\nperformance, and ease of use, in particular for tabular data. We show that\nLearnSPN is a greedy likelihood maximizer under mild assumptions. While\ninferences in PCs may use the entire circuit structure for processing queries,\nLearnSPN applies a hard method for learning them, propagating at each sum node\na data point through one and only one of the children/edges as in a hard\nclustering process. We propose a new learning procedure named SoftLearn, that\ninduces a PC using a soft clustering process. We investigate the effect of this\nlearning-inference compatibility in PCs. Our experiments show that SoftLearn\noutperforms LearnSPN in many situations, yielding better likelihoods and\narguably better samples. We also analyze comparable tractable models to\nhighlight the differences between soft/hard learning and model querying.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14504v1",
    "published_date": "2024-03-21 15:56:15 UTC",
    "updated_date": "2024-03-21 15:56:15 UTC"
  },
  {
    "arxiv_id": "2403.14496v1",
    "title": "How Human-Centered Explainable AI Interface Are Designed and Evaluated: A Systematic Survey",
    "authors": [
      "Thu Nguyen",
      "Alessandro Canossa",
      "Jichen Zhu"
    ],
    "abstract": "Despite its technological breakthroughs, eXplainable Artificial Intelligence\n(XAI) research has limited success in producing the {\\em effective\nexplanations} needed by users. In order to improve XAI systems' usability,\npractical interpretability, and efficacy for real users, the emerging area of\n{\\em Explainable Interfaces} (EIs) focuses on the user interface and user\nexperience design aspects of XAI. This paper presents a systematic survey of 53\npublications to identify current trends in human-XAI interaction and promising\ndirections for EI design and development. This is among the first systematic\nsurvey of EI research.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14496v1",
    "published_date": "2024-03-21 15:44:56 UTC",
    "updated_date": "2024-03-21 15:44:56 UTC"
  },
  {
    "arxiv_id": "2403.14494v2",
    "title": "Learning to Project for Cross-Task Knowledge Distillation",
    "authors": [
      "Dylan Auty",
      "Roy Miles",
      "Benedikt Kolbeinsson",
      "Krystian Mikolajczyk"
    ],
    "abstract": "Traditional knowledge distillation (KD) relies on a proficient teacher\ntrained on the target task, which is not always available. In this setting,\ncross-task distillation can be used, enabling the use of any teacher model\ntrained on a different task. However, many KD methods prove ineffective when\napplied to this cross-task setting. To address this limitation, we propose a\nsimple modification: the use of an inverted projection. We show that this\ndrop-in replacement for a standard projector is effective by learning to\ndisregard any task-specific features which might degrade the student's\nperformance. We find that this simple modification is sufficient for extending\nmany KD methods to the cross-task setting, where the teacher and student tasks\ncan be very different. In doing so, we obtain up to a 1.9% improvement in the\ncross-task setting compared to the traditional projection, at no additional\ncost. Our method can obtain significant performance improvements (up to 7%)\nwhen using even a randomly-initialised teacher on various tasks such as depth\nestimation, image translation, and semantic segmentation, despite the lack of\nany learned knowledge to transfer. To provide conceptual and analytical\ninsights into this result, we show that using an inverted projection allows the\ndistillation loss to be decomposed into a knowledge transfer and a spectral\nregularisation component. Through this analysis we are additionally able to\npropose a novel regularisation loss that allows teacher-free distillation,\nenabling performance improvements of up to 8.57% on ImageNet with no additional\ntraining costs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "BMVC 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.14494v2",
    "published_date": "2024-03-21 15:42:17 UTC",
    "updated_date": "2024-11-27 18:12:42 UTC"
  },
  {
    "arxiv_id": "2403.14488v2",
    "title": "A Causal Bayesian Network and Probabilistic Programming Based Reasoning Framework for Robot Manipulation Under Uncertainty",
    "authors": [
      "Ricardo Cannizzaro",
      "Michael Groom",
      "Jonathan Routley",
      "Robert Osazuwa Ness",
      "Lars Kunze"
    ],
    "abstract": "Robot object manipulation in real-world environments is challenging because\nrobot operation must be robust to a range of sensing, estimation, and actuation\nuncertainties to avoid potentially unsafe and costly mistakes that are a\nbarrier to their adoption. In this paper, we propose a flexible and\ngeneralisable physics-informed causal Bayesian network (CBN) based framework\nfor a robot to probabilistically reason about candidate manipulation actions,\nto enable robot decision-making robust to arbitrary robot system uncertainties\n-- the first of its kind to use a probabilistic programming language\nimplementation. Using experiments in high-fidelity Gazebo simulation of an\nexemplar block stacking task, we demonstrate our framework's ability to: (1)\npredict manipulation outcomes with high accuracy (Pred Acc: 88.6%); and, (2)\nperform greedy next-best action selection with 94.2% task success rate. We also\ndemonstrate our framework's suitability for real-world robot systems with a\ndomestic robot. Thus, we show that by combining probabilistic causal modelling\nwith physics simulations, we can make robot manipulation more robust to system\nuncertainties and hence more feasible for real-world applications. Further, our\ngeneralised reasoning framework can be used and extended for future robotics\nand causality research.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "stat.AP",
      "I.2.9; I.2.8; I.2.3; G.3; I.2.6; I.6.8; I.2.4; I.2.10"
    ],
    "primary_category": "cs.RO",
    "comment": "7 pages, 7 figures, submitted to the 2025 IEEE Conference on Robotics\n  and Automation (ICRA 2025)",
    "pdf_url": "http://arxiv.org/pdf/2403.14488v2",
    "published_date": "2024-03-21 15:36:26 UTC",
    "updated_date": "2024-10-03 14:16:47 UTC"
  },
  {
    "arxiv_id": "2403.14484v1",
    "title": "HyperGALE: ASD Classification via Hypergraph Gated Attention with Learnable Hyperedges",
    "authors": [
      "Mehul Arora",
      "Chirag Shantilal Jain",
      "Lalith Bharadwaj Baru",
      "Kamalaker Dadi",
      "Bapi Raju Surampudi"
    ],
    "abstract": "Autism Spectrum Disorder (ASD) is a neurodevelopmental condition\ncharacterized by varied social cognitive challenges and repetitive behavioral\npatterns. Identifying reliable brain imaging-based biomarkers for ASD has been\na persistent challenge due to the spectrum's diverse symptomatology. Existing\nbaselines in the field have made significant strides in this direction, yet\nthere remains room for improvement in both performance and interpretability. We\npropose \\emph{HyperGALE}, which builds upon the hypergraph by incorporating\nlearned hyperedges and gated attention mechanisms. This approach has led to\nsubstantial improvements in the model's ability to interpret complex brain\ngraph data, offering deeper insights into ASD biomarker characterization.\nEvaluated on the extensive ABIDE II dataset, \\emph{HyperGALE} not only improves\ninterpretability but also demonstrates statistically significant enhancements\nin key performance metrics compared to both previous baselines and the\nfoundational hypergraph model. The advancement \\emph{HyperGALE} brings to ASD\nresearch highlights the potential of sophisticated graph-based techniques in\nneurodevelopmental studies. The source code and implementation instructions are\navailable at GitHub:https://github.com/mehular0ra/HyperGALE.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to IJCNN 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.14484v1",
    "published_date": "2024-03-21 15:31:28 UTC",
    "updated_date": "2024-03-21 15:31:28 UTC"
  },
  {
    "arxiv_id": "2403.14483v1",
    "title": "Utilizing the LightGBM Algorithm for Operator User Credit Assessment Research",
    "authors": [
      "Shaojie Li",
      "Xinqi Dong",
      "Danqing Ma",
      "Bo Dang",
      "Hengyi Zang",
      "Yulu Gong"
    ],
    "abstract": "Mobile Internet user credit assessment is an important way for communication\noperators to establish decisions and formulate measures, and it is also a\nguarantee for operators to obtain expected benefits. However, credit evaluation\nmethods have long been monopolized by financial industries such as banks and\ncredit. As supporters and providers of platform network technology and network\nresources, communication operators are also builders and maintainers of\ncommunication networks. Internet data improves the user's credit evaluation\nstrategy. This paper uses the massive data provided by communication operators\nto carry out research on the operator's user credit evaluation model based on\nthe fusion LightGBM algorithm. First, for the massive data related to user\nevaluation provided by operators, key features are extracted by data\npreprocessing and feature engineering methods, and a multi-dimensional feature\nset with statistical significance is constructed; then, linear regression,\ndecision tree, LightGBM, and other machine learning algorithms build multiple\nbasic models to find the best basic model; finally, integrates Averaging,\nVoting, Blending, Stacking and other integrated algorithms to refine multiple\nfusion models, and finally establish the most suitable fusion model for\noperator user evaluation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-fin.ST"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14483v1",
    "published_date": "2024-03-21 15:29:24 UTC",
    "updated_date": "2024-03-21 15:29:24 UTC"
  },
  {
    "arxiv_id": "2403.14472v5",
    "title": "Detoxifying Large Language Models via Knowledge Editing",
    "authors": [
      "Mengru Wang",
      "Ningyu Zhang",
      "Ziwen Xu",
      "Zekun Xi",
      "Shumin Deng",
      "Yunzhi Yao",
      "Qishen Zhang",
      "Linyi Yang",
      "Jindong Wang",
      "Huajun Chen"
    ],
    "abstract": "This paper investigates using knowledge editing techniques to detoxify Large\nLanguage Models (LLMs). We construct a benchmark, SafeEdit, which covers nine\nunsafe categories with various powerful attack prompts and equips comprehensive\nmetrics for systematic evaluation. We conduct experiments with several\nknowledge editing approaches, indicating that knowledge editing has the\npotential to detoxify LLMs with a limited impact on general performance\nefficiently. Then, we propose a simple yet effective baseline, dubbed\nDetoxifying with Intraoperative Neural Monitoring (DINM), to diminish the\ntoxicity of LLMs within a few tuning steps via only one instance. We further\nprovide an in-depth analysis of the internal mechanism for various detoxifying\napproaches, demonstrating that previous methods like SFT and DPO may merely\nsuppress the activations of toxic parameters, while DINM mitigates the toxicity\nof the toxic parameters to a certain extent, making permanent adjustments. We\nhope that these insights could shed light on future work of developing\ndetoxifying approaches and the underlying knowledge mechanisms of LLMs. Code\nand benchmark are available at https://github.com/zjunlp/EasyEdit.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2024. Project website: https://zjunlp.github.io/project/SafeEdit\n  Benchmark: https://huggingface.co/datasets/zjunlp/SafeEdit",
    "pdf_url": "http://arxiv.org/pdf/2403.14472v5",
    "published_date": "2024-03-21 15:18:30 UTC",
    "updated_date": "2024-05-28 09:11:25 UTC"
  },
  {
    "arxiv_id": "2403.14469v1",
    "title": "ChatGPT Alternative Solutions: Large Language Models Survey",
    "authors": [
      "Hanieh Alipour",
      "Nick Pendar",
      "Kohinoor Roy"
    ],
    "abstract": "In recent times, the grandeur of Large Language Models (LLMs) has not only\nshone in the realm of natural language processing but has also cast its\nbrilliance across a vast array of applications. This remarkable display of LLM\ncapabilities has ignited a surge in research contributions within this domain,\nspanning a diverse spectrum of topics. These contributions encompass\nadvancements in neural network architecture, context length enhancements, model\nalignment, training datasets, benchmarking, efficiency improvements, and more.\nRecent years have witnessed a dynamic synergy between academia and industry,\npropelling the field of LLM research to new heights. A notable milestone in\nthis journey is the introduction of ChatGPT, a powerful AI chatbot grounded in\nLLMs, which has garnered widespread societal attention. The evolving technology\nof LLMs has begun to reshape the landscape of the entire AI community,\npromising a revolutionary shift in the way we create and employ AI algorithms.\nGiven this swift-paced technical evolution, our survey embarks on a journey to\nencapsulate the recent strides made in the world of LLMs. Through an\nexploration of the background, key discoveries, and prevailing methodologies,\nwe offer an up-to-the-minute review of the literature. By examining multiple\nLLM models, our paper not only presents a comprehensive overview but also\ncharts a course that identifies existing challenges and points toward potential\nfuture research trajectories. This survey furnishes a well-rounded perspective\non the current state of generative AI, shedding light on opportunities for\nfurther exploration, enhancement, and innovation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14469v1",
    "published_date": "2024-03-21 15:16:50 UTC",
    "updated_date": "2024-03-21 15:16:50 UTC"
  },
  {
    "arxiv_id": "2403.14468v4",
    "title": "AnyV2V: A Tuning-Free Framework For Any Video-to-Video Editing Tasks",
    "authors": [
      "Max Ku",
      "Cong Wei",
      "Weiming Ren",
      "Harry Yang",
      "Wenhu Chen"
    ],
    "abstract": "In the dynamic field of digital content creation using generative models,\nstate-of-the-art video editing models still do not offer the level of quality\nand control that users desire. Previous works on video editing either extended\nfrom image-based generative models in a zero-shot manner or necessitated\nextensive fine-tuning, which can hinder the production of fluid video edits.\nFurthermore, these methods frequently rely on textual input as the editing\nguidance, leading to ambiguities and limiting the types of edits they can\nperform. Recognizing these challenges, we introduce AnyV2V, a novel tuning-free\nparadigm designed to simplify video editing into two primary steps: (1)\nemploying an off-the-shelf image editing model to modify the first frame, (2)\nutilizing an existing image-to-video generation model to generate the edited\nvideo through temporal feature injection. AnyV2V can leverage any existing\nimage editing tools to support an extensive array of video editing tasks,\nincluding prompt-based editing, reference-based style transfer, subject-driven\nediting, and identity manipulation, which were unattainable by previous\nmethods. AnyV2V can also support any video length. Our evaluation shows that\nAnyV2V achieved CLIP-scores comparable to other baseline methods. Furthermore,\nAnyV2V significantly outperformed these baselines in human evaluations,\ndemonstrating notable improvements in visual consistency with the source video\nwhile producing high-quality edits across all editing tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Published in Transactions on Machine Learning Research (TMLR 2024)\n  (11/2024)",
    "pdf_url": "http://arxiv.org/pdf/2403.14468v4",
    "published_date": "2024-03-21 15:15:00 UTC",
    "updated_date": "2024-11-03 21:16:54 UTC"
  },
  {
    "arxiv_id": "2403.14460v1",
    "title": "Towards Single-System Illusion in Software-Defined Vehicles -- Automated, AI-Powered Workflow",
    "authors": [
      "Krzysztof Lebioda",
      "Viktor Vorobev",
      "Nenad Petrovic",
      "Fengjunjie Pan",
      "Vahid Zolfaghari",
      "Alois Knoll"
    ],
    "abstract": "We propose a novel model- and feature-based approach to development of\nvehicle software systems, where the end architecture is not explicitly defined.\nInstead, it emerges from an iterative process of search and optimization given\ncertain constraints, requirements and hardware architecture, while retaining\nthe property of single-system illusion, where applications run in a logically\nuniform environment. One of the key points of the presented approach is the\ninclusion of modern generative AI, specifically Large Language Models (LLMs),\nin the loop. With the recent advances in the field, we expect that the LLMs\nwill be able to assist in processing of requirements, generation of formal\nsystem models, as well as generation of software deployment specification and\ntest code. The resulting pipeline is automated to a large extent, with feedback\nbeing generated at each step.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "D.2.1; D.2.2; D.2.4; I.2.7; I.2.2; I.7.0"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14460v1",
    "published_date": "2024-03-21 15:07:57 UTC",
    "updated_date": "2024-03-21 15:07:57 UTC"
  },
  {
    "arxiv_id": "2403.14459v1",
    "title": "Multi-Level Explanations for Generative Language Models",
    "authors": [
      "Lucas Monteiro Paes",
      "Dennis Wei",
      "Hyo Jin Do",
      "Hendrik Strobelt",
      "Ronny Luss",
      "Amit Dhurandhar",
      "Manish Nagireddy",
      "Karthikeyan Natesan Ramamurthy",
      "Prasanna Sattigeri",
      "Werner Geyer",
      "Soumya Ghosh"
    ],
    "abstract": "Perturbation-based explanation methods such as LIME and SHAP are commonly\napplied to text classification. This work focuses on their extension to\ngenerative language models. To address the challenges of text as output and\nlong text inputs, we propose a general framework called MExGen that can be\ninstantiated with different attribution algorithms. To handle text output, we\nintroduce the notion of scalarizers for mapping text to real numbers and\ninvestigate multiple possibilities. To handle long inputs, we take a\nmulti-level approach, proceeding from coarser levels of granularity to finer\nones, and focus on algorithms with linear scaling in model queries. We conduct\na systematic evaluation, both automated and human, of perturbation-based\nattribution methods for summarization and context-grounded question answering.\nThe results show that our framework can provide more locally faithful\nexplanations of generated outputs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14459v1",
    "published_date": "2024-03-21 15:06:14 UTC",
    "updated_date": "2024-03-21 15:06:14 UTC"
  },
  {
    "arxiv_id": "2403.14443v1",
    "title": "Language Models Can Reduce Asymmetry in Information Markets",
    "authors": [
      "Nasim Rahaman",
      "Martin Weiss",
      "Manuel W√ºthrich",
      "Yoshua Bengio",
      "Li Erran Li",
      "Chris Pal",
      "Bernhard Sch√∂lkopf"
    ],
    "abstract": "This work addresses the buyer's inspection paradox for information markets.\nThe paradox is that buyers need to access information to determine its value,\nwhile sellers need to limit access to prevent theft. To study this, we\nintroduce an open-source simulated digital marketplace where intelligent\nagents, powered by language models, buy and sell information on behalf of\nexternal participants. The central mechanism enabling this marketplace is the\nagents' dual capabilities: they not only have the capacity to assess the\nquality of privileged information but also come equipped with the ability to\nforget. This ability to induce amnesia allows vendors to grant temporary access\nto proprietary information, significantly reducing the risk of unauthorized\nretention while enabling agents to accurately gauge the information's relevance\nto specific queries or tasks. To perform well, agents must make rational\ndecisions, strategically explore the marketplace through generated sub-queries,\nand synthesize answers from purchased information. Concretely, our experiments\n(a) uncover biases in language models leading to irrational behavior and\nevaluate techniques to mitigate these biases, (b) investigate how price affects\ndemand in the context of informational goods, and (c) show that inspection and\nhigher budgets both lead to higher quality outcomes.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.GT",
      "cs.LG",
      "cs.MA",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14443v1",
    "published_date": "2024-03-21 14:48:37 UTC",
    "updated_date": "2024-03-21 14:48:37 UTC"
  },
  {
    "arxiv_id": "2403.14440v1",
    "title": "Analysing Diffusion Segmentation for Medical Images",
    "authors": [
      "Mathias √ñttl",
      "Siyuan Mei",
      "Frauke Wilm",
      "Jana Steenpass",
      "Matthias R√ºbner",
      "Arndt Hartmann",
      "Matthias Beckmann",
      "Peter Fasching",
      "Andreas Maier",
      "Ramona Erber",
      "Katharina Breininger"
    ],
    "abstract": "Denoising Diffusion Probabilistic models have become increasingly popular due\nto their ability to offer probabilistic modeling and generate diverse outputs.\nThis versatility inspired their adaptation for image segmentation, where\nmultiple predictions of the model can produce segmentation results that not\nonly achieve high quality but also capture the uncertainty inherent in the\nmodel. Here, powerful architectures were proposed for improving diffusion\nsegmentation performance. However, there is a notable lack of analysis and\ndiscussions on the differences between diffusion segmentation and image\ngeneration, and thorough evaluations are missing that distinguish the\nimprovements these architectures provide for segmentation in general from their\nbenefit for diffusion segmentation specifically. In this work, we critically\nanalyse and discuss how diffusion segmentation for medical images differs from\ndiffusion image generation, with a particular focus on the training behavior.\nFurthermore, we conduct an assessment how proposed diffusion segmentation\narchitectures perform when trained directly for segmentation. Lastly, we\nexplore how different medical segmentation tasks influence the diffusion\nsegmentation behavior and the diffusion process could be adapted accordingly.\nWith these analyses, we aim to provide in-depth insights into the behavior of\ndiffusion segmentation that allow for a better design and evaluation of\ndiffusion segmentation methods in the future.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14440v1",
    "published_date": "2024-03-21 14:45:54 UTC",
    "updated_date": "2024-03-21 14:45:54 UTC"
  },
  {
    "arxiv_id": "2403.14435v1",
    "title": "Biased Binary Attribute Classifiers Ignore the Majority Classes",
    "authors": [
      "Xinyi Zhang",
      "Johanna Sophie Bieri",
      "Manuel G√ºnther"
    ],
    "abstract": "To visualize the regions of interest that classifiers base their decisions\non, different Class Activation Mapping (CAM) methods have been developed.\nHowever, all of these techniques target categorical classifiers only, though\nmost real-world tasks are binary classification. In this paper, we extend\ngradient-based CAM techniques to work with binary classifiers and visualize the\nactive regions for binary facial attribute classifiers. When training an\nunbalanced binary classifier on an imbalanced dataset, it is well-known that\nthe majority class, i.e. the class with many training samples, is mostly\npredicted much better than minority class with few training instances. In our\nexperiments on the CelebA dataset, we verify these results, when training an\nunbalanced classifier to extract 40 facial attributes simultaneously. One would\nexpect that the biased classifier has learned to extract features mainly for\nthe majority classes and that the proportional energy of the activations mainly\nreside in certain specific regions of the image where the attribute is located.\nHowever, we find very little regular activation for samples of majority\nclasses, while the active regions for minority classes seem mostly reasonable\nand overlap with our expectations. These results suggest that biased\nclassifiers mainly rely on bias activation for majority classes. When training\na balanced classifier on the imbalanced data by employing attribute-specific\nclass weights, majority and minority classes are classified similarly well and\nshow expected activations for almost all attributes",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14435v1",
    "published_date": "2024-03-21 14:41:58 UTC",
    "updated_date": "2024-03-21 14:41:58 UTC"
  },
  {
    "arxiv_id": "2403.14432v1",
    "title": "On the continuity and smoothness of the value function in reinforcement learning and optimal control",
    "authors": [
      "Hans Harder",
      "Sebastian Peitz"
    ],
    "abstract": "The value function plays a crucial role as a measure for the cumulative\nfuture reward an agent receives in both reinforcement learning and optimal\ncontrol. It is therefore of interest to study how similar the values of\nneighboring states are, i.e., to investigate the continuity of the value\nfunction. We do so by providing and verifying upper bounds on the value\nfunction's modulus of continuity. Additionally, we show that the value function\nis always H\\\"older continuous under relatively weak assumptions on the\nunderlying system and that non-differentiable value functions can be made\ndifferentiable by slightly \"disturbing\" the system.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY",
      "37H99, 37N35, 93E03",
      "I.2.8"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14432v1",
    "published_date": "2024-03-21 14:39:28 UTC",
    "updated_date": "2024-03-21 14:39:28 UTC"
  },
  {
    "arxiv_id": "2403.14429v1",
    "title": "Style-Extracting Diffusion Models for Semi-Supervised Histopathology Segmentation",
    "authors": [
      "Mathias √ñttl",
      "Frauke Wilm",
      "Jana Steenpass",
      "Jingna Qiu",
      "Matthias R√ºbner",
      "Arndt Hartmann",
      "Matthias Beckmann",
      "Peter Fasching",
      "Andreas Maier",
      "Ramona Erber",
      "Bernhard Kainz",
      "Katharina Breininger"
    ],
    "abstract": "Deep learning-based image generation has seen significant advancements with\ndiffusion models, notably improving the quality of generated images. Despite\nthese developments, generating images with unseen characteristics beneficial\nfor downstream tasks has received limited attention. To bridge this gap, we\npropose Style-Extracting Diffusion Models, featuring two conditioning\nmechanisms. Specifically, we utilize 1) a style conditioning mechanism which\nallows to inject style information of previously unseen images during image\ngeneration and 2) a content conditioning which can be targeted to a downstream\ntask, e.g., layout for segmentation. We introduce a trainable style encoder to\nextract style information from images, and an aggregation block that merges\nstyle information from multiple style inputs. This architecture enables the\ngeneration of images with unseen styles in a zero-shot manner, by leveraging\nstyles from unseen images, resulting in more diverse generations. In this work,\nwe use the image layout as target condition and first show the capability of\nour method on a natural image dataset as a proof-of-concept. We further\ndemonstrate its versatility in histopathology, where we combine prior knowledge\nabout tissue composition and unannotated data to create diverse synthetic\nimages with known layouts. This allows us to generate additional synthetic data\nto train a segmentation network in a semi-supervised fashion. We verify the\nadded value of the generated images by showing improved segmentation results\nand lower performance variability between patients when synthetic images are\nincluded during segmentation training. Our code will be made publicly available\nat [LINK].",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14429v1",
    "published_date": "2024-03-21 14:36:59 UTC",
    "updated_date": "2024-03-21 14:36:59 UTC"
  },
  {
    "arxiv_id": "2403.14410v1",
    "title": "GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning",
    "authors": [
      "Sanqing Qu",
      "Tianpei Zou",
      "Florian R√∂hrbein",
      "Cewu Lu",
      "Guang Chen",
      "Dacheng Tao",
      "Changjun Jiang"
    ],
    "abstract": "Deep neural networks often exhibit sub-optimal performance under covariate\nand category shifts. Source-Free Domain Adaptation (SFDA) presents a promising\nsolution to this dilemma, yet most SFDA approaches are restricted to closed-set\nscenarios. In this paper, we explore Source-Free Universal Domain Adaptation\n(SF-UniDA) aiming to accurately classify \"known\" data belonging to common\ncategories and segregate them from target-private \"unknown\" data. We propose a\nnovel Global and Local Clustering (GLC) technique, which comprises an adaptive\none-vs-all global clustering algorithm to discern between target classes,\ncomplemented by a local k-NN clustering strategy to mitigate negative transfer.\nDespite the effectiveness, the inherent closed-set source architecture leads to\nuniform treatment of \"unknown\" data, impeding the identification of distinct\n\"unknown\" categories. To address this, we evolve GLC to GLC++, integrating a\ncontrastive affinity learning strategy. We examine the superiority of GLC and\nGLC++ across multiple benchmarks and category shift scenarios. Remarkably, in\nthe most challenging open-partial-set scenarios, GLC and GLC++ surpass GATE by\n16.7% and 18.6% in H-score on VisDA, respectively. GLC++ enhances the novel\ncategory clustering accuracy of GLC by 4.3% in open-set scenarios on\nOffice-Home. Furthermore, the introduced contrastive learning strategy not only\nenhances GLC but also significantly facilitates existing methodologies.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "This is a substantial extension of the CVPR 2023 paper \"Upcycling\n  Models under Domain and Category Shift\"",
    "pdf_url": "http://arxiv.org/pdf/2403.14410v1",
    "published_date": "2024-03-21 13:57:45 UTC",
    "updated_date": "2024-03-21 13:57:45 UTC"
  },
  {
    "arxiv_id": "2403.14409v1",
    "title": "Locating and Mitigating Gender Bias in Large Language Models",
    "authors": [
      "Yuchen Cai",
      "Ding Cao",
      "Rongxi Guo",
      "Yaqin Wen",
      "Guiquan Liu",
      "Enhong Chen"
    ],
    "abstract": "Large language models(LLM) are pre-trained on extensive corpora to learn\nfacts and human cognition which contain human preferences. However, this\nprocess can inadvertently lead to these models acquiring biases and stereotypes\nprevalent in society. Prior research has typically tackled the issue of bias\nthrough a one-dimensional perspective, concentrating either on locating or\nmitigating it. This limited perspective has created obstacles in facilitating\nresearch on bias to synergistically complement and progressively build upon one\nanother. In this study, we integrate the processes of locating and mitigating\nbias within a unified framework. Initially, we use causal mediation analysis to\ntrace the causal effects of different components' activation within a large\nlanguage model. Building on this, we propose the LSDM (Least Square Debias\nMethod), a knowledge-editing based method for mitigating gender bias in\noccupational pronouns, and compare it against two baselines on three gender\nbias datasets and seven knowledge competency test datasets. The experimental\nresults indicate that the primary contributors to gender bias are the bottom\nMLP modules acting on the last token of occupational pronouns and the top\nattention module acting on the final word in the sentence. Furthermore, LSDM\nmitigates gender bias in the model more effectively than the other baselines,\nwhile fully preserving the model's capabilities in all other aspects.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "23 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.14409v1",
    "published_date": "2024-03-21 13:57:43 UTC",
    "updated_date": "2024-03-21 13:57:43 UTC"
  },
  {
    "arxiv_id": "2403.14403v2",
    "title": "Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity",
    "authors": [
      "Soyeong Jeong",
      "Jinheon Baek",
      "Sukmin Cho",
      "Sung Ju Hwang",
      "Jong C. Park"
    ],
    "abstract": "Retrieval-Augmented Large Language Models (LLMs), which incorporate the\nnon-parametric knowledge from external knowledge bases into LLMs, have emerged\nas a promising approach to enhancing response accuracy in several tasks, such\nas Question-Answering (QA). However, even though there are various approaches\ndealing with queries of different complexities, they either handle simple\nqueries with unnecessary computational overhead or fail to adequately address\ncomplex multi-step queries; yet, not all user requests fall into only one of\nthe simple or complex categories. In this work, we propose a novel adaptive QA\nframework, that can dynamically select the most suitable strategy for\n(retrieval-augmented) LLMs from the simplest to the most sophisticated ones\nbased on the query complexity. Also, this selection process is operationalized\nwith a classifier, which is a smaller LM trained to predict the complexity\nlevel of incoming queries with automatically collected labels, obtained from\nactual predicted outcomes of models and inherent inductive biases in datasets.\nThis approach offers a balanced strategy, seamlessly adapting between the\niterative and single-step retrieval-augmented LLMs, as well as the no-retrieval\nmethods, in response to a range of query complexities. We validate our model on\na set of open-domain QA datasets, covering multiple query complexities, and\nshow that ours enhances the overall efficiency and accuracy of QA systems,\ncompared to relevant baselines including the adaptive retrieval approaches.\nCode is available at: https://github.com/starsuzi/Adaptive-RAG.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.14403v2",
    "published_date": "2024-03-21 13:52:30 UTC",
    "updated_date": "2024-03-28 06:45:11 UTC"
  },
  {
    "arxiv_id": "2403.14399v1",
    "title": "Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning",
    "authors": [
      "Changtong Zan",
      "Liang Ding",
      "Li Shen",
      "Yibing Zhen",
      "Weifeng Liu",
      "Dacheng Tao"
    ],
    "abstract": "Translation-tailored Large language models (LLMs) exhibit remarkable\ntranslation capabilities, even competing with supervised-trained commercial\ntranslation systems. However, off-target translation remains an unsolved\nproblem, especially for low-resource languages, hindering us from developing\naccurate LLMs-based translation models. To mitigate the off-target translation\nproblem and enhance the performance of LLMs on translation, recent works have\neither designed advanced prompting strategies to highlight the functionality of\ntranslation instructions or exploited the in-context learning ability of LLMs\nby feeding few-shot demonstrations. However, these methods essentially do not\nimprove LLM's ability to follow translation instructions, especially the\nlanguage direction information. In this work, we design a two-stage fine-tuning\nalgorithm to improve the instruction-following ability (especially the\ntranslation direction) of LLMs. Specifically, we first tune LLMs with the\nmaximum likelihood estimation loss on the translation dataset to elicit the\nbasic translation capabilities. In the second stage, we construct\ninstruction-conflicting samples by randomly replacing the translation\ndirections with a wrong one within the instruction, and then introduce an extra\nunlikelihood loss to learn those samples. Experiments on IWSLT and WMT\nbenchmarks upon the LLaMA model spanning 16 zero-shot directions show that,\ncompared to the competitive baseline -- translation-finetuned LLama, our method\ncould effectively reduce the off-target translation ratio (averagely -53.3\\%),\nthus improving translation quality with average +5.7 SacreBLEU and +16.4\nBLEURT. Analysis shows that our method could preserve the model's general task\nperformance on AlpacaEval. Code and models will be released at\n\\url{https://github.com/alphadl/LanguageAware_Tuning}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14399v1",
    "published_date": "2024-03-21 13:47:40 UTC",
    "updated_date": "2024-03-21 13:47:40 UTC"
  },
  {
    "arxiv_id": "2403.15489v1",
    "title": "EEG decoding with conditional identification information",
    "authors": [
      "Pengfei Sun",
      "Jorg De Winne",
      "Paul Devos",
      "Dick Botteldooren"
    ],
    "abstract": "Decoding EEG signals is crucial for unraveling human brain and advancing\nbrain-computer interfaces. Traditional machine learning algorithms have been\nhindered by the high noise levels and inherent inter-person variations in EEG\nsignals. Recent advances in deep neural networks (DNNs) have shown promise,\nowing to their advanced nonlinear modeling capabilities. However, DNN still\nfaces challenge in decoding EEG samples of unseen individuals. To address this,\nthis paper introduces a novel approach by incorporating the conditional\nidentification information of each individual into the neural network, thereby\nenhancing model representation through the synergistic interaction of EEG and\npersonal traits. We test our model on the WithMe dataset and demonstrated that\nthe inclusion of these identifiers substantially boosts accuracy for both\nsubjects in the training set and unseen subjects. This enhancement suggests\npromising potential for improving for EEG interpretability and understanding of\nrelevant identification features.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "Accepted by 6th International Conference on Advances in Signal\n  Processing and Artificial Intelligence (ASPAI' 2024)",
    "pdf_url": "http://arxiv.org/pdf/2403.15489v1",
    "published_date": "2024-03-21 13:38:59 UTC",
    "updated_date": "2024-03-21 13:38:59 UTC"
  },
  {
    "arxiv_id": "2403.14736v2",
    "title": "NaNa and MiGu: Semantic Data Augmentation Techniques to Enhance Protein Classification in Graph Neural Networks",
    "authors": [
      "Yi-Shan Lan",
      "Pin-Yu Chen",
      "Tsung-Yi Ho"
    ],
    "abstract": "Protein classification tasks are essential in drug discovery. Real-world\nprotein structures are dynamic, which will determine the properties of\nproteins. However, the existing machine learning methods, like ProNet (Wang et\nal., 2022a), only access limited conformational characteristics and protein\nside-chain features, leading to impractical protein structure and inaccuracy of\nprotein classes in their predictions. In this paper, we propose novel semantic\ndata augmentation methods, Novel Augmentation of New Node Attributes (NaNa),\nand Molecular Interactions and Geometric Upgrading (MiGu) to incorporate\nbackbone chemical and side-chain biophysical information into protein\nclassification tasks and a co-embedding residual learning framework.\nSpecifically, we leverage molecular biophysical, secondary structure, chemical\nbonds, and ionic features of proteins to facilitate protein classification\ntasks. Furthermore, our semantic augmentation methods and the co-embedding\nresidual learning framework can improve the performance of GIN (Xu et al.,\n2019) on EC and Fold datasets (Bairoch, 2000; Andreeva et al., 2007) by 16.41%\nand 11.33% respectively. Our code is available at\nhttps://github.com/r08b46009/Code_for_MIGU_NANA/tree/main.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14736v2",
    "published_date": "2024-03-21 13:27:57 UTC",
    "updated_date": "2024-03-26 05:25:04 UTC"
  },
  {
    "arxiv_id": "2403.14381v2",
    "title": "Editing Knowledge Representation of Language Model via Rephrased Prefix Prompts",
    "authors": [
      "Yuchen Cai",
      "Ding Cao",
      "Rongxi Guo",
      "Yaqin Wen",
      "Guiquan Liu",
      "Enhong Chen"
    ],
    "abstract": "Neural language models (LMs) have been extensively trained on vast corpora to\nstore factual knowledge about various aspects of the world described in texts.\nCurrent technologies typically employ knowledge editing methods or specific\nprompts to modify LM outputs. However, existing knowledge editing methods are\ncostly and inefficient, struggling to produce appropriate text. Additionally,\nprompt engineering is opaque and requires significant effort to find suitable\nprompts. To address these issues, we introduce a new method called PSPEM\n(Prefix Soft Prompt Editing Method), that can be used for a lifetime with just\none training. It resolves the inefficiencies and generalizability issues in\nknowledge editing methods and overcomes the opacity of prompt engineering by\nautomatically seeking optimal soft prompts. Specifically, PSPEM utilizes a\nprompt encoder and an encoding converter to refine key information in prompts\nand uses prompt alignment techniques to guide model generation, ensuring text\nconsistency and adherence to the intended structure and content, thereby\nmaintaining an optimal balance between efficiency and accuracy. We have\nvalidated the effectiveness of PSPEM through knowledge editing and attribute\ninserting. On the COUNTERFACT dataset, PSPEM achieved nearly 100\\% editing\naccuracy and demonstrated the highest level of fluency. We further analyzed the\nsimilarities between PSPEM and original prompts and their impact on the model's\ninternals. The results indicate that PSPEM can serve as an alternative to\noriginal prompts, supporting the model in effective editing.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "19pages,3figures",
    "pdf_url": "http://arxiv.org/pdf/2403.14381v2",
    "published_date": "2024-03-21 13:15:25 UTC",
    "updated_date": "2024-05-11 13:51:32 UTC"
  },
  {
    "arxiv_id": "2403.14377v1",
    "title": "Knowledge-Enhanced Recommendation with User-Centric Subgraph Network",
    "authors": [
      "Guangyi Liu",
      "Quanming Yao",
      "Yongqi Zhang",
      "Lei Chen"
    ],
    "abstract": "Recommendation systems, as widely implemented nowadays on various platforms,\nrecommend relevant items to users based on their preferences. The classical\nmethods which rely on user-item interaction matrices has limitations,\nespecially in scenarios where there is a lack of interaction data for new\nitems. Knowledge graph (KG)-based recommendation systems have emerged as a\npromising solution. However, most KG-based methods adopt node embeddings, which\ndo not provide personalized recommendations for different users and cannot\ngeneralize well to the new items. To address these limitations, we propose\nKnowledge-enhanced User-Centric subgraph Network (KUCNet), a subgraph learning\napproach with graph neural network (GNN) for effective recommendation. KUCNet\nconstructs a U-I subgraph for each user-item pair that captures both the\nhistorical information of user-item interactions and the side information\nprovided in KG. An attention-based GNN is designed to encode the U-I subgraphs\nfor recommendation. Considering efficiency, the pruned user-centric computation\ngraph is further introduced such that multiple U-I subgraphs can be\nsimultaneously computed and that the size can be pruned by Personalized\nPageRank. Our proposed method achieves accurate, efficient, and interpretable\nrecommendations especially for new items. Experimental results demonstrate the\nsuperiority of KUCNet over state-of-the-art KG-based and collaborative\nfiltering (CF)-based methods.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14377v1",
    "published_date": "2024-03-21 13:09:23 UTC",
    "updated_date": "2024-03-21 13:09:23 UTC"
  },
  {
    "arxiv_id": "2403.14371v1",
    "title": "Loop Improvement: An Efficient Approach for Extracting Shared Features from Heterogeneous Data without Central Server",
    "authors": [
      "Fei Li",
      "Chu Kiong Loo",
      "Wei Shiung Liew",
      "Xiaofeng Liu"
    ],
    "abstract": "In federated learning, data heterogeneity significantly impacts performance.\nA typical solution involves segregating these parameters into shared and\npersonalized components, a concept also relevant in multi-task learning.\nAddressing this, we propose \"Loop Improvement\" (LI), a novel method enhancing\nthis separation and feature extraction without necessitating a central server\nor data interchange among participants. Our experiments reveal LI's superiority\nin several aspects: In personalized federated learning environments, LI\nconsistently outperforms the advanced FedALA algorithm in accuracy across\ndiverse scenarios. Additionally, LI's feature extractor closely matches the\nperformance achieved when aggregating data from all clients. In global model\ncontexts, employing LI with stacked personalized layers and an additional\nnetwork also yields comparable results to combined client data scenarios.\nFurthermore, LI's adaptability extends to multi-task learning, streamlining the\nextraction of common features across tasks and obviating the need for\nsimultaneous training. This approach not only enhances individual task\nperformance but also achieves accuracy levels on par with classic multi-task\nlearning methods where all tasks are trained simultaneously. LI integrates a\nloop topology with layer-wise and end-to-end training, compatible with various\nneural network models. This paper also delves into the theoretical\nunderpinnings of LI's effectiveness, offering insights into its potential\napplications. The code is on https://github.com/axedge1983/LI",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.14371v1",
    "published_date": "2024-03-21 12:59:24 UTC",
    "updated_date": "2024-03-21 12:59:24 UTC"
  },
  {
    "arxiv_id": "2403.14358v1",
    "title": "Exploring the Potential of Large Language Models in Graph Generation",
    "authors": [
      "Yang Yao",
      "Xin Wang",
      "Zeyang Zhang",
      "Yijian Qin",
      "Ziwei Zhang",
      "Xu Chu",
      "Yuekui Yang",
      "Wenwu Zhu",
      "Hong Mei"
    ],
    "abstract": "Large language models (LLMs) have achieved great success in many fields, and\nrecent works have studied exploring LLMs for graph discriminative tasks such as\nnode classification. However, the abilities of LLMs for graph generation remain\nunexplored in the literature. Graph generation requires the LLM to generate\ngraphs with given properties, which has valuable real-world applications such\nas drug discovery, while tends to be more challenging. In this paper, we\npropose LLM4GraphGen to explore the ability of LLMs for graph generation with\nsystematical task designs and extensive experiments. Specifically, we propose\nseveral tasks tailored with comprehensive experiments to address key questions\nregarding LLMs' understanding of different graph structure rules, their ability\nto capture structural type distributions, and their utilization of domain\nknowledge for property-based graph generation. Our evaluations demonstrate that\nLLMs, particularly GPT-4, exhibit preliminary abilities in graph generation\ntasks, including rule-based and distribution-based generation. We also observe\nthat popular prompting methods, such as few-shot and chain-of-thought\nprompting, do not consistently enhance performance. Besides, LLMs show\npotential in generating molecules with specific properties. These findings may\nserve as foundations for designing good LLMs based models for graph generation\nand provide valuable insights and further research.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14358v1",
    "published_date": "2024-03-21 12:37:54 UTC",
    "updated_date": "2024-03-21 12:37:54 UTC"
  },
  {
    "arxiv_id": "2403.14340v1",
    "title": "Exploring Task Unification in Graph Representation Learning via Generative Approach",
    "authors": [
      "Yulan Hu",
      "Sheng Ouyang",
      "Zhirui Yang",
      "Ge Chen",
      "Junchen Wan",
      "Xiao Wang",
      "Yong Liu"
    ],
    "abstract": "Graphs are ubiquitous in real-world scenarios and encompass a diverse range\nof tasks, from node-, edge-, and graph-level tasks to transfer learning.\nHowever, designing specific tasks for each type of graph data is often costly\nand lacks generalizability. Recent endeavors under the \"Pre-training +\nFine-tuning\" or \"Pre-training + Prompt\" paradigms aim to design a unified\nframework capable of generalizing across multiple graph tasks. Among these,\ngraph autoencoders (GAEs), generative self-supervised models, have demonstrated\ntheir potential in effectively addressing various graph tasks. Nevertheless,\nthese methods typically employ multi-stage training and require adaptive\ndesigns, which on one hand make it difficult to be seamlessly applied to\ndiverse graph tasks and on the other hand overlook the negative impact caused\nby discrepancies in task objectives between the different stages. To address\nthese challenges, we propose GA^2E, a unified adversarially masked autoencoder\ncapable of addressing the above challenges seamlessly. Specifically, GA^2E\nproposes to use the subgraph as the meta-structure, which remains consistent\nacross all graph tasks (ranging from node-, edge-, and graph-level to transfer\nlearning) and all stages (both during training and inference). Further, GA^2E\noperates in a \\textbf{\"Generate then Discriminate\"} manner. It leverages the\nmasked GAE to reconstruct the input subgraph whilst treating it as a generator\nto compel the reconstructed graphs resemble the input subgraph. Furthermore,\nGA^2E introduces an auxiliary discriminator to discern the authenticity between\nthe reconstructed (generated) subgraph and the input subgraph, thus ensuring\nthe robustness of the graph representation through adversarial training\nmechanisms. We validate GA^2E's capabilities through extensive experiments on\n21 datasets across four types of graph tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14340v1",
    "published_date": "2024-03-21 12:14:02 UTC",
    "updated_date": "2024-03-21 12:14:02 UTC"
  },
  {
    "arxiv_id": "2403.14339v1",
    "title": "$\\nabla œÑ$: Gradient-based and Task-Agnostic machine Unlearning",
    "authors": [
      "Daniel Trippa",
      "Cesare Campagnano",
      "Maria Sofia Bucarelli",
      "Gabriele Tolomei",
      "Fabrizio Silvestri"
    ],
    "abstract": "Machine Unlearning, the process of selectively eliminating the influence of\ncertain data examples used during a model's training, has gained significant\nattention as a means for practitioners to comply with recent data protection\nregulations. However, existing unlearning methods face critical drawbacks,\nincluding their prohibitively high cost, often associated with a large number\nof hyperparameters, and the limitation of forgetting only relatively small data\nportions. This often makes retraining the model from scratch a quicker and more\neffective solution. In this study, we introduce Gradient-based and\nTask-Agnostic machine Unlearning ($\\nabla \\tau$), an optimization framework\ndesigned to remove the influence of a subset of training data efficiently. It\napplies adaptive gradient ascent to the data to be forgotten while using\nstandard gradient descent for the remaining data. $\\nabla \\tau$ offers multiple\nbenefits over existing approaches. It enables the unlearning of large sections\nof the training dataset (up to 30%). It is versatile, supporting various\nunlearning tasks (such as subset forgetting or class removal) and applicable\nacross different domains (images, text, etc.). Importantly, $\\nabla \\tau$\nrequires no hyperparameter adjustments, making it a more appealing option than\nretraining the model from scratch. We evaluate our framework's effectiveness\nusing a set of well-established Membership Inference Attack metrics,\ndemonstrating up to 10% enhancements in performance compared to\nstate-of-the-art methods without compromising the original model's accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.14339v1",
    "published_date": "2024-03-21 12:11:26 UTC",
    "updated_date": "2024-03-21 12:11:26 UTC"
  },
  {
    "arxiv_id": "2403.14328v1",
    "title": "Distilling Reinforcement Learning Policies for Interpretable Robot Locomotion: Gradient Boosting Machines and Symbolic Regression",
    "authors": [
      "Fernando Acero",
      "Zhibin Li"
    ],
    "abstract": "Recent advancements in reinforcement learning (RL) have led to remarkable\nachievements in robot locomotion capabilities. However, the complexity and\n``black-box'' nature of neural network-based RL policies hinder their\ninterpretability and broader acceptance, particularly in applications demanding\nhigh levels of safety and reliability. This paper introduces a novel approach\nto distill neural RL policies into more interpretable forms using Gradient\nBoosting Machines (GBMs), Explainable Boosting Machines (EBMs) and Symbolic\nRegression. By leveraging the inherent interpretability of generalized additive\nmodels, decision trees, and analytical expressions, we transform opaque neural\nnetwork policies into more transparent ``glass-box'' models. We train expert\nneural network policies using RL and subsequently distill them into (i) GBMs,\n(ii) EBMs, and (iii) symbolic policies. To address the inherent distribution\nshift challenge of behavioral cloning, we propose to use the Dataset\nAggregation (DAgger) algorithm with a curriculum of episode-dependent\nalternation of actions between expert and distilled policies, to enable\nefficient distillation of feedback control policies. We evaluate our approach\non various robot locomotion gaits -- walking, trotting, bounding, and pacing --\nand study the importance of different observations in joint actions for\ndistilled policies using various methods. We train neural expert policies for\n205 hours of simulated experience and distill interpretable policies with only\n10 minutes of simulated interaction for each gait using the proposed method.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14328v1",
    "published_date": "2024-03-21 11:54:45 UTC",
    "updated_date": "2024-03-21 11:54:45 UTC"
  },
  {
    "arxiv_id": "2403.14300v1",
    "title": "DexDribbler: Learning Dexterous Soccer Manipulation via Dynamic Supervision",
    "authors": [
      "Yutong Hu",
      "Kehan Wen",
      "Fisher Yu"
    ],
    "abstract": "Learning dexterous locomotion policy for legged robots is becoming\nincreasingly popular due to its ability to handle diverse terrains and resemble\nintelligent behaviors. However, joint manipulation of moving objects and\nlocomotion with legs, such as playing soccer, receive scant attention in the\nlearning community, although it is natural for humans and smart animals. A key\nchallenge to solve this multitask problem is to infer the objectives of\nlocomotion from the states and targets of the manipulated objects. The implicit\nrelation between the object states and robot locomotion can be hard to capture\ndirectly from the training experience. We propose adding a feedback control\nblock to compute the necessary body-level movement accurately and using the\noutputs as dynamic joint-level locomotion supervision explicitly. We further\nutilize an improved ball dynamic model, an extended context-aided estimator,\nand a comprehensive ball observer to facilitate transferring policy learned in\nsimulation to the real world. We observe that our learning scheme can not only\nmake the policy network converge faster but also enable soccer robots to\nperform sophisticated maneuvers like sharp cuts and turns on flat surfaces, a\ncapability that was lacking in previous methods. Video and code are available\nat https://github.com/SysCV/soccer-player",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 7 figures, submitted to IROS 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.14300v1",
    "published_date": "2024-03-21 11:16:28 UTC",
    "updated_date": "2024-03-21 11:16:28 UTC"
  },
  {
    "arxiv_id": "2403.14298v1",
    "title": "From Perils to Possibilities: Understanding how Human (and AI) Biases affect Online Fora",
    "authors": [
      "Virginia Morini",
      "Valentina Pansanella",
      "Katherine Abramski",
      "Erica Cau",
      "Andrea Failla",
      "Salvatore Citraro",
      "Giulio Rossetti"
    ],
    "abstract": "Social media platforms are online fora where users engage in discussions,\nshare content, and build connections. This review explores the dynamics of\nsocial interactions, user-generated contents, and biases within the context of\nsocial media analysis (analyzing works that use the tools offered by complex\nnetwork analysis and natural language processing) through the lens of three key\npoints of view: online debates, online support, and human-AI interactions. On\nthe one hand, we delineate the phenomenon of online debates, where\npolarization, misinformation, and echo chamber formation often proliferate,\ndriven by algorithmic biases and extreme mechanisms of homophily. On the other\nhand, we explore the emergence of online support groups through users'\nself-disclosure and social support mechanisms. Online debates and support\nmechanisms present a duality of both perils and possibilities within social\nmedia; perils of segregated communities and polarized debates, and\npossibilities of empathy narratives and self-help groups. This dichotomy also\nextends to a third perspective: users' reliance on AI-generated content, such\nas the ones produced by Large Language Models, which can manifest both human\nbiases hidden in training sets and non-human biases that emerge from their\nartificial neural architectures. Analyzing interdisciplinary approaches, we aim\nto deepen the understanding of the complex interplay between social\ninteractions, user-generated content, and biases within the realm of social\nmedia ecosystems.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14298v1",
    "published_date": "2024-03-21 11:04:41 UTC",
    "updated_date": "2024-03-21 11:04:41 UTC"
  },
  {
    "arxiv_id": "2403.14297v2",
    "title": "Impact Assessment of Missing Data in Model Predictions for Earth Observation Applications",
    "authors": [
      "Francisco Mena",
      "Diego Arenas",
      "Marcela Charfuelan",
      "Marlon Nuske",
      "Andreas Dengel"
    ],
    "abstract": "Earth observation (EO) applications involving complex and heterogeneous data\nsources are commonly approached with machine learning models. However, there is\na common assumption that data sources will be persistently available. Different\nsituations could affect the availability of EO sources, like noise, clouds, or\nsatellite mission failures. In this work, we assess the impact of missing\ntemporal and static EO sources in trained models across four datasets with\nclassification and regression tasks. We compare the predictive quality of\ndifferent methods and find that some are naturally more robust to missing data.\nThe Ensemble strategy, in particular, achieves a prediction robustness up to\n100%. We evidence that missing scenarios are significantly more challenging in\nregression than classification tasks. Finally, we find that the optical view is\nthe most critical view when it is missing individually.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at IEEE International Geoscience and Remote Sensing\n  Symposium 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.14297v2",
    "published_date": "2024-03-21 11:03:56 UTC",
    "updated_date": "2024-05-13 09:29:28 UTC"
  },
  {
    "arxiv_id": "2403.14287v1",
    "title": "Enhancing Historical Image Retrieval with Compositional Cues",
    "authors": [
      "Tingyu Lin",
      "Robert Sablatnig"
    ],
    "abstract": "In analyzing vast amounts of digitally stored historical image data, existing\ncontent-based retrieval methods often overlook significant non-semantic\ninformation, limiting their effectiveness for flexible exploration across\nvaried themes. To broaden the applicability of image retrieval methods for\ndiverse purposes and uncover more general patterns, we innovatively introduce a\ncrucial factor from computational aesthetics, namely image composition, into\nthis topic. By explicitly integrating composition-related information extracted\nby CNN into the designed retrieval model, our method considers both the image's\ncomposition rules and semantic information. Qualitative and quantitative\nexperiments demonstrate that the image retrieval network guided by composition\ninformation outperforms those relying solely on content information,\nfacilitating the identification of images in databases closer to the target\nimage in human perception. Please visit https://github.com/linty5/CCBIR to try\nour codes.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14287v1",
    "published_date": "2024-03-21 10:51:19 UTC",
    "updated_date": "2024-03-21 10:51:19 UTC"
  },
  {
    "arxiv_id": "2403.14282v1",
    "title": "How to be fair? A study of label and selection bias",
    "authors": [
      "Marco Favier",
      "Toon Calders",
      "Sam Pinxteren",
      "Jonathan Meyer"
    ],
    "abstract": "It is widely accepted that biased data leads to biased and thus potentially\nunfair models. Therefore, several measures for bias in data and model\npredictions have been proposed, as well as bias mitigation techniques whose aim\nis to learn models that are fair by design. Despite the myriad of mitigation\ntechniques developed in the past decade, however, it is still poorly understood\nunder what circumstances which methods work. Recently, Wick et al. showed, with\nexperiments on synthetic data, that there exist situations in which bias\nmitigation techniques lead to more accurate models when measured on unbiased\ndata. Nevertheless, in the absence of a thorough mathematical analysis, it\nremains unclear which techniques are effective under what circumstances. We\npropose to address this problem by establishing relationships between the type\nof bias and the effectiveness of a mitigation technique, where we categorize\nthe mitigation techniques by the bias measure they optimize. In this paper we\nillustrate this principle for label and selection bias on the one hand, and\ndemographic parity and ``We're All Equal'' on the other hand. Our theoretical\nanalysis allows to explain the results of Wick et al. and we also show that\nthere are situations where minimizing fairness measures does not result in the\nfairest possible distribution.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14282v1",
    "published_date": "2024-03-21 10:43:55 UTC",
    "updated_date": "2024-03-21 10:43:55 UTC"
  },
  {
    "arxiv_id": "2403.14274v4",
    "title": "Multi-role Consensus through LLMs Discussions for Vulnerability Detection",
    "authors": [
      "Zhenyu Mao",
      "Jialong Li",
      "Dongming Jin",
      "Munan Li",
      "Kenji Tei"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have highlighted the\npotential for vulnerability detection, a crucial component of software quality\nassurance. Despite this progress, most studies have been limited to the\nperspective of a single role, usually testers, lacking diverse viewpoints from\ndifferent roles in a typical software development life-cycle, including both\ndevelopers and testers. To this end, this paper introduces a multi-role\napproach to employ LLMs to act as different roles simulating a real-life code\nreview process and engaging in discussions toward a consensus on the existence\nand classification of vulnerabilities in the code. Preliminary evaluation of\nthis approach indicates a 13.48% increase in the precision rate, an 18.25%\nincrease in the recall rate, and a 16.13% increase in the F1 score.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14274v4",
    "published_date": "2024-03-21 10:28:18 UTC",
    "updated_date": "2024-05-18 14:53:22 UTC"
  },
  {
    "arxiv_id": "2403.14273v1",
    "title": "Reactor Optimization Benchmark by Reinforcement Learning",
    "authors": [
      "Deborah Schwarcz",
      "Nadav Schneider",
      "Gal Oren",
      "Uri Steinitz"
    ],
    "abstract": "Neutronic calculations for reactors are a daunting task when using Monte\nCarlo (MC) methods. As high-performance computing has advanced, the simulation\nof a reactor is nowadays more readily done, but design and optimization with\nmultiple parameters is still a computational challenge. MC transport\nsimulations, coupled with machine learning techniques, offer promising avenues\nfor enhancing the efficiency and effectiveness of nuclear reactor optimization.\nThis paper introduces a novel benchmark problem within the OpenNeoMC framework\ndesigned specifically for reinforcement learning. The benchmark involves\noptimizing a unit cell of a research reactor with two varying parameters (fuel\ndensity and water spacing) to maximize neutron flux while maintaining reactor\ncriticality. The test case features distinct local optima, representing\ndifferent physical regimes, thus posing a challenge for learning algorithms.\nThrough extensive simulations utilizing evolutionary and neuroevolutionary\nalgorithms, we demonstrate the effectiveness of reinforcement learning in\nnavigating complex optimization landscapes with strict constraints.\nFurthermore, we propose acceleration techniques within the OpenNeoMC framework,\nincluding model updating and cross-section usage by RAM utilization, to\nexpedite simulation times. Our findings emphasize the importance of machine\nlearning integration in reactor optimization and contribute to advancing\nmethodologies for addressing intricate optimization challenges in nuclear\nengineering. The sources of this work are available at our GitHub repository:\nhttps://github.com/Scientific-Computing-Lab-NRCN/RLOpenNeoMC",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14273v1",
    "published_date": "2024-03-21 10:26:47 UTC",
    "updated_date": "2024-03-21 10:26:47 UTC"
  },
  {
    "arxiv_id": "2405.01556v1",
    "title": "Semantically Aligned Question and Code Generation for Automated Insight Generation",
    "authors": [
      "Ananya Singha",
      "Bhavya Chopra",
      "Anirudh Khatry",
      "Sumit Gulwani",
      "Austin Z. Henley",
      "Vu Le",
      "Chris Parnin",
      "Mukul Singh",
      "Gust Verbruggen"
    ],
    "abstract": "Automated insight generation is a common tactic for helping knowledge\nworkers, such as data scientists, to quickly understand the potential value of\nnew and unfamiliar data. Unfortunately, automated insights produced by\nlarge-language models can generate code that does not correctly correspond (or\nalign) to the insight. In this paper, we leverage the semantic knowledge of\nlarge language models to generate targeted and insightful questions about data\nand the corresponding code to answer those questions. Then through an empirical\nstudy on data from Open-WikiTable, we show that embeddings can be effectively\nused for filtering out semantically unaligned pairs of question and code.\nAdditionally, we found that generating questions and code together yields more\ndiverse questions.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01556v1",
    "published_date": "2024-03-21 10:01:05 UTC",
    "updated_date": "2024-03-21 10:01:05 UTC"
  },
  {
    "arxiv_id": "2403.14264v1",
    "title": "A Framework for Portrait Stylization with Skin-Tone Awareness and Nudity Identification",
    "authors": [
      "Seungkwon Kim",
      "Sangyeon Kim",
      "Seung-Hun Nam"
    ],
    "abstract": "Portrait stylization is a challenging task involving the transformation of an\ninput portrait image into a specific style while preserving its inherent\ncharacteristics. The recent introduction of Stable Diffusion (SD) has\nsignificantly improved the quality of outcomes in this field. However, a\npractical stylization framework that can effectively filter harmful input\ncontent and preserve the distinct characteristics of an input, such as\nskin-tone, while maintaining the quality of stylization remains lacking. These\nchallenges have hindered the wide deployment of such a framework. To address\nthese issues, this study proposes a portrait stylization framework that\nincorporates a nudity content identification module (NCIM) and a\nskin-tone-aware portrait stylization module (STAPSM). In experiments, NCIM\nshowed good performance in enhancing explicit content filtering, and STAPSM\naccurately represented a diverse range of skin tones. Our proposed framework\nhas been successfully deployed in practice, and it has effectively satisfied\ncritical requirements of real-world applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ICASSP 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.14264v1",
    "published_date": "2024-03-21 09:59:53 UTC",
    "updated_date": "2024-03-21 09:59:53 UTC"
  },
  {
    "arxiv_id": "2403.14252v1",
    "title": "LayoutLLM: Large Language Model Instruction Tuning for Visually Rich Document Understanding",
    "authors": [
      "Masato Fujitake"
    ],
    "abstract": "This paper proposes LayoutLLM, a more flexible document analysis method for\nunderstanding imaged documents. Visually Rich Document Understanding tasks,\nsuch as document image classification and information extraction, have gained\nsignificant attention due to their importance. Existing methods have been\ndeveloped to enhance document comprehension by incorporating pre-training\nawareness of images, text, and layout structure. However, these methods require\nfine-tuning for each task and dataset, and the models are expensive to train\nand operate. To overcome this limitation, we propose a new LayoutLLM that\nintegrates these with large-scale language models (LLMs). By leveraging the\nstrengths of existing research in document image understanding and LLMs'\nsuperior language understanding capabilities, the proposed model, fine-tuned\nwith multimodal instruction datasets, performs an understanding of document\nimages in a single model. Our experiments demonstrate improvement over the\nbaseline model in various document analysis tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "LREC-COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.14252v1",
    "published_date": "2024-03-21 09:25:24 UTC",
    "updated_date": "2024-03-21 09:25:24 UTC"
  },
  {
    "arxiv_id": "2403.14246v1",
    "title": "CATSE: A Context-Aware Framework for Causal Target Sound Extraction",
    "authors": [
      "Shrishail Baligar",
      "Mikolaj Kegler",
      "Bryce Irvin",
      "Marko Stamenovic",
      "Shawn Newsam"
    ],
    "abstract": "Target Sound Extraction (TSE) focuses on the problem of separating sources of\ninterest, indicated by a user's cue, from the input mixture. Most existing\nsolutions operate in an offline fashion and are not suited to the low-latency\ncausal processing constraints imposed by applications in live-streamed content\nsuch as augmented hearing. We introduce a family of context-aware low-latency\ncausal TSE models suitable for real-time processing. First, we explore the\nutility of context by providing the TSE model with oracle information about\nwhat sound classes make up the input mixture, where the objective of the model\nis to extract one or more sources of interest indicated by the user. Since the\npractical applications of oracle models are limited due to their assumptions,\nwe introduce a composite multi-task training objective involving separation and\nclassification losses. Our evaluation involving single- and multi-source\nextraction shows the benefit of using context information in the model either\nby means of providing full context or via the proposed multi-task training loss\nwithout the need for full context information. Specifically, we show that our\nproposed model outperforms size- and latency-matched Waveformer, a\nstate-of-the-art model for real-time TSE.",
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "eess.AS",
    "comment": "Submitted to EUSIPCO 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.14246v1",
    "published_date": "2024-03-21 09:06:28 UTC",
    "updated_date": "2024-03-21 09:06:28 UTC"
  },
  {
    "arxiv_id": "2403.14244v1",
    "title": "Isotropic Gaussian Splatting for Real-Time Radiance Field Rendering",
    "authors": [
      "Yuanhao Gong",
      "Lantao Yu",
      "Guanghui Yue"
    ],
    "abstract": "The 3D Gaussian splatting method has drawn a lot of attention, thanks to its\nhigh performance in training and high quality of the rendered image. However,\nit uses anisotropic Gaussian kernels to represent the scene. Although such\nanisotropic kernels have advantages in representing the geometry, they lead to\ndifficulties in terms of computation, such as splitting or merging two kernels.\nIn this paper, we propose to use isotropic Gaussian kernels to avoid such\ndifficulties in the computation, leading to a higher performance method. The\nexperiments confirm that the proposed method is about {\\bf 100X} faster without\nlosing the geometry representation accuracy. The proposed method can be applied\nin a large range applications where the radiance field is needed, such as 3D\nreconstruction, view synthesis, and dynamic object modeling.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14244v1",
    "published_date": "2024-03-21 09:02:31 UTC",
    "updated_date": "2024-03-21 09:02:31 UTC"
  },
  {
    "arxiv_id": "2403.14243v1",
    "title": "Dermacen Analytica: A Novel Methodology Integrating Multi-Modal Large Language Models with Machine Learning in tele-dermatology",
    "authors": [
      "Dimitrios P. Panagoulias",
      "Evridiki Tsoureli-Nikita",
      "Maria Virvou",
      "George A. Tsihrintzis"
    ],
    "abstract": "The rise of Artificial Intelligence creates great promise in the field of\nmedical discovery, diagnostics and patient management. However, the vast\ncomplexity of all medical domains require a more complex approach that combines\nmachine learning algorithms, classifiers, segmentation algorithms and, lately,\nlarge language models. In this paper, we describe, implement and assess an\nArtificial Intelligence-empowered system and methodology aimed at assisting the\ndiagnosis process of skin lesions and other skin conditions within the field of\ndermatology that aims to holistically address the diagnostic process in this\ndomain. The workflow integrates large language, transformer-based vision models\nand sophisticated machine learning tools. This holistic approach achieves a\nnuanced interpretation of dermatological conditions that simulates and\nfacilitates a dermatologist's workflow. We assess our proposed methodology\nthrough a thorough cross-model validation technique embedded in an evaluation\npipeline that utilizes publicly available medical case studies of skin\nconditions and relevant images. To quantitatively score the system performance,\nadvanced machine learning and natural language processing tools are employed\nwhich focus on similarity comparison and natural language inference.\nAdditionally, we incorporate a human expert evaluation process based on a\nstructured checklist to further validate our results. We implemented the\nproposed methodology in a system which achieved approximate (weighted) scores\nof 0.87 for both contextual understanding and diagnostic accuracy,\ndemonstrating the efficacy of our approach in enhancing dermatological\nanalysis. The proposed methodology is expected to prove useful in the\ndevelopment of next-generation tele-dermatology applications, enhancing remote\nconsultation capabilities and access to care, especially in underserved areas.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14243v1",
    "published_date": "2024-03-21 09:02:17 UTC",
    "updated_date": "2024-03-21 09:02:17 UTC"
  },
  {
    "arxiv_id": "2403.14238v1",
    "title": "Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection",
    "authors": [
      "Kyungjae Lee",
      "Dasol Hwang",
      "Sunghyun Park",
      "Youngsoo Jang",
      "Moontae Lee"
    ],
    "abstract": "Despite the promise of RLHF in aligning LLMs with human preferences, it often\nleads to superficial alignment, prioritizing stylistic changes over improving\ndownstream performance of LLMs. Underspecified preferences could obscure\ndirections to align the models. Lacking exploration restricts identification of\ndesirable outputs to improve the models. To overcome these challenges, we\npropose a novel framework: Reinforcement Learning from Reflective Feedback\n(RLRF), which leverages fine-grained feedback based on detailed criteria to\nimprove the core capabilities of LLMs. RLRF employs a self-reflection mechanism\nto systematically explore and refine LLM responses, then fine-tuning the models\nvia a RL algorithm along with promising responses. Our experiments across\nJust-Eval, Factuality, and Mathematical Reasoning demonstrate the efficacy and\ntransformative potential of RLRF beyond superficial surface-level adjustment.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages, 5 figures, Submitted to ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.14238v1",
    "published_date": "2024-03-21 08:57:27 UTC",
    "updated_date": "2024-03-21 08:57:27 UTC"
  },
  {
    "arxiv_id": "2403.14734v5",
    "title": "A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond",
    "authors": [
      "Qiushi Sun",
      "Zhirui Chen",
      "Fangzhi Xu",
      "Kanzhi Cheng",
      "Chang Ma",
      "Zhangyue Yin",
      "Jianing Wang",
      "Chengcheng Han",
      "Renyu Zhu",
      "Shuai Yuan",
      "Qipeng Guo",
      "Xipeng Qiu",
      "Pengcheng Yin",
      "Xiaoli Li",
      "Fei Yuan",
      "Lingpeng Kong",
      "Xiang Li",
      "Zhiyong Wu"
    ],
    "abstract": "Neural Code Intelligence -- leveraging deep learning to understand, generate,\nand optimize code -- holds immense potential for transformative impacts on the\nwhole society. Bridging the gap between Natural Language and Programming\nLanguage, this domain has drawn significant attention from researchers in both\nresearch communities over the past few years. This survey presents a systematic\nand chronological review of the advancements in code intelligence, encompassing\nover 50 representative models and their variants, more than 20 categories of\ntasks, and an extensive coverage of over 680 related works. We follow the\nhistorical progression to trace the paradigm shifts across different research\nphases (e.g., from modeling code with recurrent neural networks to the era of\nLarge Language Models). Concurrently, we highlight the major technical\ntransitions in models, tasks, and evaluations spanning through different\nstages. For applications, we also observe a co-evolving shift. It spans from\ninitial endeavors to tackling specific scenarios, through exploring a diverse\narray of tasks during its rapid expansion, to currently focusing on tackling\nincreasingly complex and varied real-world challenges. Building on our\nexamination of the developmental trajectories, we further investigate the\nemerging synergies between code intelligence and broader machine intelligence,\nuncovering new cross-domain opportunities and illustrating the substantial\ninfluence of code intelligence across various domains. Finally, we delve into\nboth the opportunities and challenges associated with this field, alongside\nelucidating our insights on the most promising research directions. An ongoing,\ndynamically updated project and resources associated with this survey have been\nreleased at https://github.com/QiushiSun/Awesome-Code-Intelligence.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.PL"
    ],
    "primary_category": "cs.SE",
    "comment": "67 pages, 6 figures, 10 tables, 718 references",
    "pdf_url": "http://arxiv.org/pdf/2403.14734v5",
    "published_date": "2024-03-21 08:54:56 UTC",
    "updated_date": "2025-01-26 10:48:44 UTC"
  },
  {
    "arxiv_id": "2403.14236v5",
    "title": "A Unified Framework for Model Editing",
    "authors": [
      "Akshat Gupta",
      "Dev Sajnani",
      "Gopala Anumanchipalli"
    ],
    "abstract": "ROME and MEMIT are largely believed to be two different model editing\nalgorithms, with the major difference between them being the ability to perform\nbatched edits. In this paper, we unify these two algorithms under a single\nconceptual umbrella, optimizing for the same goal, which we call the\npreservation-memorization objective. ROME uses an equality constraint to\noptimize this objective to perform one edit at a time, whereas MEMIT employs a\nmore flexible least-square constraint that allows for batched edits. We\ngeneralize ROME and enable batched editing with equality constraint in the form\nof EMMET - an Equality-constrained Mass Model Editing algorithm for\nTransformers, a new batched memory-editing algorithm. EMMET can perform\nbatched-edits up to a batch-size of 10,000, with very similar performance to\nMEMIT across multiple dimensions. With the introduction of EMMET, we truly\nunify ROME and MEMIT and show that both algorithms are equivalent in terms of\ntheir optimization objective, their abilities (singular and batched editing),\ntheir model editing performance and their limitations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "EMNLP 2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2403.14236v5",
    "published_date": "2024-03-21 08:54:24 UTC",
    "updated_date": "2024-10-09 03:37:30 UTC"
  },
  {
    "arxiv_id": "2403.14233v1",
    "title": "SoftPatch: Unsupervised Anomaly Detection with Noisy Data",
    "authors": [
      "Xi Jiang",
      "Ying Chen",
      "Qiang Nie",
      "Yong Liu",
      "Jianlin Liu",
      "Bin-Bin Gao",
      "Jun Liu",
      "Chengjie Wang",
      "Feng Zheng"
    ],
    "abstract": "Although mainstream unsupervised anomaly detection (AD) algorithms perform\nwell in academic datasets, their performance is limited in practical\napplication due to the ideal experimental setting of clean training data.\nTraining with noisy data is an inevitable problem in real-world anomaly\ndetection but is seldom discussed. This paper considers label-level noise in\nimage sensory anomaly detection for the first time. To solve this problem, we\nproposed a memory-based unsupervised AD method, SoftPatch, which efficiently\ndenoises the data at the patch level. Noise discriminators are utilized to\ngenerate outlier scores for patch-level noise elimination before coreset\nconstruction. The scores are then stored in the memory bank to soften the\nanomaly detection boundary. Compared with existing methods, SoftPatch maintains\na strong modeling ability of normal data and alleviates the overconfidence\nproblem in coreset. Comprehensive experiments in various noise scenes\ndemonstrate that SoftPatch outperforms the state-of-the-art AD methods on the\nMVTecAD and BTAD benchmarks and is comparable to those methods under the\nsetting without noise.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "36th Conference on Neural Information Processing Systems",
    "pdf_url": "http://arxiv.org/pdf/2403.14233v1",
    "published_date": "2024-03-21 08:49:34 UTC",
    "updated_date": "2024-03-21 08:49:34 UTC"
  },
  {
    "arxiv_id": "2403.14227v1",
    "title": "PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children's Collaborative Learning",
    "authors": [
      "Jiawen Liu",
      "Yuanyuan Yao",
      "Pengcheng An",
      "Qi Wang"
    ],
    "abstract": "In children's collaborative learning, effective peer conversations can\nsignificantly enhance the quality of children's collaborative interactions. The\nintegration of Large Language Model (LLM) agents into this setting explores\ntheir novel role as peers, assessing impacts as team moderators and\nparticipants. We invited two groups of participants to engage in a\ncollaborative learning workshop, where they discussed and proposed conceptual\nsolutions to a design problem. The peer conversation transcripts were analyzed\nusing thematic analysis. We discovered that peer agents, while managing\ndiscussions effectively as team moderators, sometimes have their instructions\ndisregarded. As participants, they foster children's creative thinking but may\nnot consistently provide timely feedback. These findings highlight potential\ndesign improvements and considerations for peer agents in both roles.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "To appear at CHI EA '24",
    "pdf_url": "http://arxiv.org/pdf/2403.14227v1",
    "published_date": "2024-03-21 08:37:15 UTC",
    "updated_date": "2024-03-21 08:37:15 UTC"
  },
  {
    "arxiv_id": "2403.15486v1",
    "title": "Sequence-to-Sequence Language Models for Character and Emotion Detection in Dream Narratives",
    "authors": [
      "Gustave Cortal"
    ],
    "abstract": "The study of dreams has been central to understanding human\n(un)consciousness, cognition, and culture for centuries. Analyzing dreams\nquantitatively depends on labor-intensive, manual annotation of dream\nnarratives. We automate this process through a natural language\nsequence-to-sequence generation framework. This paper presents the first study\non character and emotion detection in the English portion of the open DreamBank\ncorpus of dream narratives. Our results show that language models can\neffectively address this complex task. To get insight into prediction\nperformance, we evaluate the impact of model size, prediction order of\ncharacters, and the consideration of proper names and character traits. We\ncompare our approach with a large language model using in-context learning. Our\nsupervised models perform better while having 28 times fewer parameters. Our\nmodel and its generated annotations are made publicly available.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15486v1",
    "published_date": "2024-03-21 08:27:49 UTC",
    "updated_date": "2024-03-21 08:27:49 UTC"
  },
  {
    "arxiv_id": "2403.14733v1",
    "title": "Open Knowledge Base Canonicalization with Multi-task Learning",
    "authors": [
      "Bingchen Liu",
      "Huang Peng",
      "Weixin Zeng",
      "Xiang Zhao",
      "Shijun Liu",
      "Li Pan"
    ],
    "abstract": "The construction of large open knowledge bases (OKBs) is integral to many\nknowledge-driven applications on the world wide web such as web search.\nHowever, noun phrases and relational phrases in OKBs often suffer from\nredundancy and ambiguity, which calls for the investigation on OKB\ncanonicalization. Current solutions address OKB canonicalization by devising\nadvanced clustering algorithms and using knowledge graph embedding (KGE) to\nfurther facilitate the canonicalization process. Nevertheless, these works fail\nto fully exploit the synergy between clustering and KGE learning, and the\nmethods designed for these subtasks are sub-optimal. To this end, we put\nforward a multi-task learning framework, namely MulCanon, to tackle OKB\ncanonicalization. In addition, diffusion model is used in the soft clustering\nprocess to improve the noun phrase representations with neighboring\ninformation, which can lead to more accurate representations. MulCanon unifies\nthe learning objectives of these sub-tasks, and adopts a two-stage multi-task\nlearning paradigm for training. A thorough experimental study on popular OKB\ncanonicalization benchmarks validates that MulCanon can achieve competitive\ncanonicalization results.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2310.16419",
    "pdf_url": "http://arxiv.org/pdf/2403.14733v1",
    "published_date": "2024-03-21 08:03:46 UTC",
    "updated_date": "2024-03-21 08:03:46 UTC"
  },
  {
    "arxiv_id": "2403.14203v1",
    "title": "Unsupervised Audio-Visual Segmentation with Modality Alignment",
    "authors": [
      "Swapnil Bhosale",
      "Haosen Yang",
      "Diptesh Kanojia",
      "Jiangkang Deng",
      "Xiatian Zhu"
    ],
    "abstract": "Audio-Visual Segmentation (AVS) aims to identify, at the pixel level, the\nobject in a visual scene that produces a given sound. Current AVS methods rely\non costly fine-grained annotations of mask-audio pairs, making them impractical\nfor scalability. To address this, we introduce unsupervised AVS, eliminating\nthe need for such expensive annotation. To tackle this more challenging\nproblem, we propose an unsupervised learning method, named Modality\nCorrespondence Alignment (MoCA), which seamlessly integrates off-the-shelf\nfoundation models like DINO, SAM, and ImageBind. This approach leverages their\nknowledge complementarity and optimizes their joint usage for multi-modality\nassociation. Initially, we estimate positive and negative image pairs in the\nfeature space. For pixel-level association, we introduce an audio-visual\nadapter and a novel pixel matching aggregation strategy within the image-level\ncontrastive learning framework. This allows for a flexible connection between\nobject appearance and audio signal at the pixel level, with tolerance to\nimaging variations such as translation and rotation. Extensive experiments on\nthe AVSBench (single and multi-object splits) and AVSS datasets demonstrate\nthat our MoCA outperforms strongly designed baseline methods and approaches\nsupervised counterparts, particularly in complex scenarios with multiple\nauditory objects. Notably when comparing mIoU, MoCA achieves a substantial\nimprovement over baselines in both the AVSBench (S4: +17.24%; MS3: +67.64%) and\nAVSS (+19.23%) audio-visual segmentation challenges.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14203v1",
    "published_date": "2024-03-21 07:56:09 UTC",
    "updated_date": "2024-03-21 07:56:09 UTC"
  },
  {
    "arxiv_id": "2403.14200v2",
    "title": "Debiasing surgeon: fantastic weights and how to find them",
    "authors": [
      "R√©mi Nahon",
      "Ivan Luiz De Moura Matos",
      "Van-Tam Nguyen",
      "Enzo Tartaglione"
    ],
    "abstract": "Nowadays an ever-growing concerning phenomenon, the emergence of algorithmic\nbiases that can lead to unfair models, emerges. Several debiasing approaches\nhave been proposed in the realm of deep learning, employing more or less\nsophisticated approaches to discourage these models from massively employing\nthese biases. However, a question emerges: is this extra complexity really\nnecessary? Is a vanilla-trained model already embodying some ``unbiased\nsub-networks'' that can be used in isolation and propose a solution without\nrelying on the algorithmic biases? In this work, we show that such a\nsub-network typically exists, and can be extracted from a vanilla-trained model\nwithout requiring additional training. We further validate that such specific\narchitecture is incapable of learning a specific bias, suggesting that there\nare possible architectural countermeasures to the problem of biases in deep\nneural networks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14200v2",
    "published_date": "2024-03-21 07:50:45 UTC",
    "updated_date": "2024-07-19 09:50:51 UTC"
  },
  {
    "arxiv_id": "2403.15485v1",
    "title": "MOGAM: A Multimodal Object-oriented Graph Attention Model for Depression Detection",
    "authors": [
      "Junyeop Cha",
      "Seoyun Kim",
      "Dongjae Kim",
      "Eunil Park"
    ],
    "abstract": "Early detection plays a crucial role in the treatment of depression.\nTherefore, numerous studies have focused on social media platforms, where\nindividuals express their emotions, aiming to achieve early detection of\ndepression. However, the majority of existing approaches often rely on specific\nfeatures, leading to limited scalability across different types of social media\ndatasets, such as text, images, or videos. To overcome this limitation, we\nintroduce a Multimodal Object-Oriented Graph Attention Model (MOGAM), which can\nbe applied to diverse types of data, offering a more scalable and versatile\nsolution. Furthermore, to ensure that our model can capture authentic symptoms\nof depression, we only include vlogs from users with a clinical diagnosis. To\nleverage the diverse features of vlogs, we adopt a multimodal approach and\ncollect additional metadata such as the title, description, and duration of the\nvlogs. To effectively aggregate these multimodal features, we employed a\ncross-attention mechanism. MOGAM achieved an accuracy of 0.871 and an F1-score\nof 0.888. Moreover, to validate the scalability of MOGAM, we evaluated its\nperformance with a benchmark dataset and achieved comparable results with prior\nstudies (0.61 F1-score). In conclusion, we believe that the proposed model,\nMOGAM, is an effective solution for detecting depression in social media,\noffering potential benefits in the early detection and treatment of this mental\nhealth condition.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 3 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2403.15485v1",
    "published_date": "2024-03-21 07:45:58 UTC",
    "updated_date": "2024-03-21 07:45:58 UTC"
  },
  {
    "arxiv_id": "2404.00029v2",
    "title": "Complementarity in Human-AI Collaboration: Concept, Sources, and Evidence",
    "authors": [
      "Patrick Hemmer",
      "Max Schemmer",
      "Niklas K√ºhl",
      "Michael V√∂ssing",
      "Gerhard Satzger"
    ],
    "abstract": "Artificial intelligence (AI) has the potential to significantly enhance human\nperformance across various domains. Ideally, collaboration between humans and\nAI should result in complementary team performance (CTP) -- a level of\nperformance that neither of them can attain individually. So far, however, CTP\nhas rarely been observed, suggesting an insufficient understanding of the\nprinciple and the application of complementarity. Therefore, we develop a\ngeneral concept of complementarity and formalize its theoretical potential as\nwell as the actual realized effect in decision-making situations. Moreover, we\nidentify information and capability asymmetry as the two key sources of\ncomplementarity. Finally, we illustrate the impact of each source on\ncomplementarity potential and effect in two empirical studies. Our work\nprovides researchers with a comprehensive theoretical foundation of human-AI\ncomplementarity in decision-making and demonstrates that leveraging these\nsources constitutes a viable pathway towards designing effective human-AI\ncollaboration, i.e., the realization of CTP.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00029v2",
    "published_date": "2024-03-21 07:27:17 UTC",
    "updated_date": "2024-11-25 22:04:11 UTC"
  },
  {
    "arxiv_id": "2403.14188v1",
    "title": "Quantum-activated neural reservoirs on-chip open up large hardware security models for resilient authentication",
    "authors": [
      "Zhao He",
      "Maxim S. Elizarov",
      "Ning Li",
      "Fei Xiang",
      "Andrea Fratalocchi"
    ],
    "abstract": "Quantum artificial intelligence is a frontier of artificial intelligence\nresearch, pioneering quantum AI-powered circuits to address problems beyond the\nreach of deep learning with classical architectures. This work implements a\nlarge-scale quantum-activated recurrent neural network possessing more than 3\ntrillion hardware nodes/cm$^2$, originating from repeatable atomic-scale\nnucleation dynamics in an amorphous material integrated on-chip, controlled\nwith 0.07 nW electric power per readout channel. Compared to the\nbest-performing reservoirs currently reported, this implementation increases\nthe scale of the network by two orders of magnitude and reduces the power\nconsumption by six, reaching power efficiencies in the range of the human\nbrain, dissipating 0.2 nW/neuron. When interrogated by a classical input, the\nchip implements a large-scale hardware security model, enabling dictionary-free\nauthentication secure against statistical inference attacks, including AI's\npresent and future development, even for an adversary with a copy of all the\nclassical components available. Experimental tests report 99.6% reliability,\n100% user authentication accuracy, and an ideal 50% key uniqueness. Due to its\nquantum nature, the chip supports a bit density per feature size area three\ntimes higher than the best technology available, with the capacity to store\nmore than $2^{1104}$ keys in a footprint of 1 cm$^2$. Such a quantum-powered\nplatform could help counteract the emerging form of warfare led by the\ncybercrime industry in breaching authentication to target small to large-scale\nfacilities, from private users to intelligent energy grids.",
    "categories": [
      "cond-mat.dis-nn",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cond-mat.dis-nn",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14188v1",
    "published_date": "2024-03-21 07:25:52 UTC",
    "updated_date": "2024-03-21 07:25:52 UTC"
  },
  {
    "arxiv_id": "2403.14186v1",
    "title": "StyleCineGAN: Landscape Cinemagraph Generation using a Pre-trained StyleGAN",
    "authors": [
      "Jongwoo Choi",
      "Kwanggyoon Seo",
      "Amirsaman Ashtari",
      "Junyong Noh"
    ],
    "abstract": "We propose a method that can generate cinemagraphs automatically from a still\nlandscape image using a pre-trained StyleGAN. Inspired by the success of recent\nunconditional video generation, we leverage a powerful pre-trained image\ngenerator to synthesize high-quality cinemagraphs. Unlike previous approaches\nthat mainly utilize the latent space of a pre-trained StyleGAN, our approach\nutilizes its deep feature space for both GAN inversion and cinemagraph\ngeneration. Specifically, we propose multi-scale deep feature warping (MSDFW),\nwhich warps the intermediate features of a pre-trained StyleGAN at different\nresolutions. By using MSDFW, the generated cinemagraphs are of high resolution\nand exhibit plausible looping animation. We demonstrate the superiority of our\nmethod through user studies and quantitative comparisons with state-of-the-art\ncinemagraph generation methods and a video generation method that uses a\npre-trained StyleGAN.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "Project website: https://jeolpyeoni.github.io/stylecinegan_project/",
    "pdf_url": "http://arxiv.org/pdf/2403.14186v1",
    "published_date": "2024-03-21 07:21:51 UTC",
    "updated_date": "2024-03-21 07:21:51 UTC"
  },
  {
    "arxiv_id": "2403.14183v2",
    "title": "OTSeg: Multi-prompt Sinkhorn Attention for Zero-Shot Semantic Segmentation",
    "authors": [
      "Kwanyoung Kim",
      "Yujin Oh",
      "Jong Chul Ye"
    ],
    "abstract": "The recent success of CLIP has demonstrated promising results in zero-shot\nsemantic segmentation by transferring muiltimodal knowledge to pixel-level\nclassification. However, leveraging pre-trained CLIP knowledge to closely align\ntext embeddings with pixel embeddings still has limitations in existing\napproaches. To address this issue, we propose OTSeg, a novel multimodal\nattention mechanism aimed at enhancing the potential of multiple text prompts\nfor matching associated pixel embeddings. We first propose Multi-Prompts\nSinkhorn (MPS) based on the Optimal Transport (OT) algorithm, which leads\nmultiple text prompts to selectively focus on various semantic features within\nimage pixels. Moreover, inspired by the success of Sinkformers in unimodal\nsettings, we introduce the extension of MPS, called Multi-Prompts Sinkhorn\nAttention (MPSA) , which effectively replaces cross-attention mechanisms within\nTransformer framework in multimodal settings. Through extensive experiments, we\ndemonstrate that OTSeg achieves state-of-the-art (SOTA) performance with\nsignificant gains on Zero-Shot Semantic Segmentation (ZS3) tasks across three\nbenchmark datasets.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024; 23 pages, 8 tables, 8 figures; Project Page:\n  https://cubeyoung.github.io/OTSeg_project/",
    "pdf_url": "http://arxiv.org/pdf/2403.14183v2",
    "published_date": "2024-03-21 07:15:37 UTC",
    "updated_date": "2024-07-11 18:09:48 UTC"
  },
  {
    "arxiv_id": "2403.14163v1",
    "title": "Leveraging Large Language Model-based Room-Object Relationships Knowledge for Enhancing Multimodal-Input Object Goal Navigation",
    "authors": [
      "Leyuan Sun",
      "Asako Kanezaki",
      "Guillaume Caron",
      "Yusuke Yoshiyasu"
    ],
    "abstract": "Object-goal navigation is a crucial engineering task for the community of\nembodied navigation; it involves navigating to an instance of a specified\nobject category within unseen environments. Although extensive investigations\nhave been conducted on both end-to-end and modular-based, data-driven\napproaches, fully enabling an agent to comprehend the environment through\nperceptual knowledge and perform object-goal navigation as efficiently as\nhumans remains a significant challenge. Recently, large language models have\nshown potential in this task, thanks to their powerful capabilities for\nknowledge extraction and integration. In this study, we propose a data-driven,\nmodular-based approach, trained on a dataset that incorporates common-sense\nknowledge of object-to-room relationships extracted from a large language\nmodel. We utilize the multi-channel Swin-Unet architecture to conduct\nmulti-task learning incorporating with multimodal inputs. The results in the\nHabitat simulator demonstrate that our framework outperforms the baseline by an\naverage of 10.6% in the efficiency metric, Success weighted by Path Length\n(SPL). The real-world demonstration shows that the proposed approach can\nefficiently conduct this task by traversing several rooms. For more details and\nreal-world demonstrations, please check our project webpage\n(https://sunleyuan.github.io/ObjectNav).",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "will soon submit to the Elsevier journal, Advanced Engineering\n  Informatics",
    "pdf_url": "http://arxiv.org/pdf/2403.14163v1",
    "published_date": "2024-03-21 06:32:36 UTC",
    "updated_date": "2024-03-21 06:32:36 UTC"
  },
  {
    "arxiv_id": "2403.14156v3",
    "title": "Policy Mirror Descent with Lookahead",
    "authors": [
      "Kimon Protopapas",
      "Anas Barakat"
    ],
    "abstract": "Policy Mirror Descent (PMD) stands as a versatile algorithmic framework\nencompassing several seminal policy gradient algorithms such as natural policy\ngradient, with connections with state-of-the-art reinforcement learning (RL)\nalgorithms such as TRPO and PPO. PMD can be seen as a soft Policy Iteration\nalgorithm implementing regularized 1-step greedy policy improvement. However,\n1-step greedy policies might not be the best choice and recent remarkable\nempirical successes in RL such as AlphaGo and AlphaZero have demonstrated that\ngreedy approaches with respect to multiple steps outperform their 1-step\ncounterpart. In this work, we propose a new class of PMD algorithms called\n$h$-PMD which incorporates multi-step greedy policy improvement with lookahead\ndepth $h$ to the PMD update rule. To solve discounted infinite horizon Markov\nDecision Processes with discount factor $\\gamma$, we show that $h$-PMD which\ngeneralizes the standard PMD enjoys a faster dimension-free $\\gamma^h$-linear\nconvergence rate, contingent on the computation of multi-step greedy policies.\nWe propose an inexact version of $h$-PMD where lookahead action values are\nestimated. Under a generative model, we establish a sample complexity for\n$h$-PMD which improves over prior work. Finally, we extend our result to linear\nfunction approximation to scale to large state spaces. Under suitable\nassumptions, our sample complexity only involves dependence on the dimension of\nthe feature map space instead of the state space size.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14156v3",
    "published_date": "2024-03-21 06:10:51 UTC",
    "updated_date": "2024-11-06 14:29:05 UTC"
  },
  {
    "arxiv_id": "2403.14151v1",
    "title": "Deep Learning for Trajectory Data Management and Mining: A Survey and Beyond",
    "authors": [
      "Wei Chen",
      "Yuxuan Liang",
      "Yuanshao Zhu",
      "Yanchuan Chang",
      "Kang Luo",
      "Haomin Wen",
      "Lei Li",
      "Yanwei Yu",
      "Qingsong Wen",
      "Chao Chen",
      "Kai Zheng",
      "Yunjun Gao",
      "Xiaofang Zhou",
      "Yu Zheng"
    ],
    "abstract": "Trajectory computing is a pivotal domain encompassing trajectory data\nmanagement and mining, garnering widespread attention due to its crucial role\nin various practical applications such as location services, urban traffic, and\npublic safety. Traditional methods, focusing on simplistic spatio-temporal\nfeatures, face challenges of complex calculations, limited scalability, and\ninadequate adaptability to real-world complexities. In this paper, we present a\ncomprehensive review of the development and recent advances in deep learning\nfor trajectory computing (DL4Traj). We first define trajectory data and provide\na brief overview of widely-used deep learning models. Systematically, we\nexplore deep learning applications in trajectory management (pre-processing,\nstorage, analysis, and visualization) and mining (trajectory-related\nforecasting, trajectory-related recommendation, trajectory classification,\ntravel time estimation, anomaly detection, and mobility generation). Notably,\nwe encapsulate recent advancements in Large Language Models (LLMs) that hold\nthe potential to augment trajectory computing. Additionally, we summarize\napplication scenarios, public datasets, and toolkits. Finally, we outline\ncurrent challenges in DL4Traj research and propose future directions. Relevant\npapers and open-source resources have been collated and are continuously\nupdated at:\n\\href{https://github.com/yoshall/Awesome-Trajectory-Computing}{DL4Traj Repo}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.DB"
    ],
    "primary_category": "cs.LG",
    "comment": "25 pages, 12 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2403.14151v1",
    "published_date": "2024-03-21 05:57:27 UTC",
    "updated_date": "2024-03-21 05:57:27 UTC"
  },
  {
    "arxiv_id": "2403.14146v1",
    "title": "Evolving Benchmark Functions to Compare Evolutionary Algorithms via Genetic Programming",
    "authors": [
      "Yifan He",
      "Claus Aranha"
    ],
    "abstract": "In this study, we use Genetic Programming (GP) to compose new optimization\nbenchmark functions. Optimization benchmarks have the important role of showing\nthe differences between evolutionary algorithms, making it possible for further\nanalysis and comparisons. We show that the benchmarks generated by GP are able\nto differentiate algorithms better than human-made benchmark functions. The\nfitness measure of the GP is the Wasserstein distance of the solutions found by\na pair of optimizers. Additionally, we use MAP-Elites to both enhance the\nsearch power of the GP and also illustrate how the difference between\noptimizers changes by various landscape features. Our approach provides a novel\nway to automate the design of benchmark functions and to compare evolutionary\nalgorithms.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14146v1",
    "published_date": "2024-03-21 05:42:17 UTC",
    "updated_date": "2024-03-21 05:42:17 UTC"
  },
  {
    "arxiv_id": "2403.14120v1",
    "title": "Advancing IIoT with Over-the-Air Federated Learning: The Role of Iterative Magnitude Pruning",
    "authors": [
      "Fazal Muhammad Ali Khan",
      "Hatem Abou-Zeid",
      "Aryan Kaushik",
      "Syed Ali Hassan"
    ],
    "abstract": "The industrial Internet of Things (IIoT) under Industry 4.0 heralds an era of\ninterconnected smart devices where data-driven insights and machine learning\n(ML) fuse to revolutionize manufacturing. A noteworthy development in IIoT is\nthe integration of federated learning (FL), which addresses data privacy and\nsecurity among devices. FL enables edge sensors, also known as peripheral\nintelligence units (PIUs) to learn and adapt using their data locally, without\nexplicit sharing of confidential data, to facilitate a collaborative yet\nconfidential learning process. However, the lower memory footprint and\ncomputational power of PIUs inherently require deep neural network (DNN) models\nthat have a very compact size. Model compression techniques such as pruning can\nbe used to reduce the size of DNN models by removing unnecessary connections\nthat have little impact on the model's performance, thus making the models more\nsuitable for the limited resources of PIUs. Targeting the notion of compact yet\nrobust DNN models, we propose the integration of iterative magnitude pruning\n(IMP) of the DNN model being trained in an over-the-air FL (OTA-FL) environment\nfor IIoT. We provide a tutorial overview and also present a case study of the\neffectiveness of IMP in OTA-FL for an IIoT environment. Finally, we present\nfuture directions for enhancing and optimizing these deep compression\ntechniques further, aiming to push the boundaries of IIoT capabilities in\nacquiring compact yet robust and high-performing DNN models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.14120v1",
    "published_date": "2024-03-21 04:15:56 UTC",
    "updated_date": "2024-03-21 04:15:56 UTC"
  },
  {
    "arxiv_id": "2403.14119v3",
    "title": "C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion",
    "authors": [
      "Hee Suk Yoon",
      "Eunseop Yoon",
      "Joshua Tian Jin Tee",
      "Mark Hasegawa-Johnson",
      "Yingzhen Li",
      "Chang D. Yoo"
    ],
    "abstract": "In deep learning, test-time adaptation has gained attention as a method for\nmodel fine-tuning without the need for labeled data. A prime exemplification is\nthe recently proposed test-time prompt tuning for large-scale vision-language\nmodels such as CLIP. Unfortunately, these prompts have been mainly developed to\nimprove accuracy, overlooking the importance of calibration, which is a crucial\naspect for quantifying prediction uncertainty. However, traditional calibration\nmethods rely on substantial amounts of labeled data, making them impractical\nfor test-time scenarios. To this end, this paper explores calibration during\ntest-time prompt tuning by leveraging the inherent properties of CLIP. Through\na series of observations, we find that the prompt choice significantly affects\nthe calibration in CLIP, where the prompts leading to higher text feature\ndispersion result in better-calibrated predictions. Introducing the Average\nText Feature Dispersion (ATFD), we establish its relationship with calibration\nerror and present a novel method, Calibrated Test-time Prompt Tuning (C-TPT),\nfor optimizing prompts during test-time with enhanced calibration. Through\nextensive experiments on different CLIP architectures and datasets, we show\nthat C-TPT can effectively improve the calibration of test-time prompt tuning\nwithout needing labeled data. The code is publicly accessible at\nhttps://github.com/hee-suk-yoon/C-TPT.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.14119v3",
    "published_date": "2024-03-21 04:08:29 UTC",
    "updated_date": "2024-03-31 13:36:54 UTC"
  },
  {
    "arxiv_id": "2403.15481v2",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and Strategies in AI/ML Development",
    "authors": [
      "Aastha Pant",
      "Rashina Hoda",
      "Chakkrit Tantithamthavorn",
      "Burak Turhan"
    ],
    "abstract": "The rise in the use of AI/ML applications across industries has sparked more\ndiscussions about the fairness of AI/ML in recent times. While prior research\non the fairness of AI/ML exists, there is a lack of empirical studies focused\non understanding the perspectives and experiences of AI practitioners in\ndeveloping a fair AI/ML system. Understanding AI practitioners' perspectives\nand experiences on the fairness of AI/ML systems are important because they are\ndirectly involved in its development and deployment and their insights can\noffer valuable real-world perspectives on the challenges associated with\nensuring fairness in AI/ML systems. We conducted semi-structured interviews\nwith 22 AI practitioners to investigate their understanding of what a 'fair\nAI/ML' is, the challenges they face in developing a fair AI/ML system, the\nconsequences of developing an unfair AI/ML system, and the strategies they\nemploy to ensure AI/ML system fairness. We developed a framework showcasing the\nrelationship between AI practitioners' understanding of 'fair AI/ML' system and\n(i) their challenges in its development, (ii) the consequences of developing an\nunfair AI/ML system, and (iii) strategies used to ensure AI/ML system fairness.\nBy exploring AI practitioners' perspectives and experiences, this study\nprovides actionable insights to enhance AI/ML fairness, which may promote\nfairer systems, reduce bias, and foster public trust in AI technologies.\nAdditionally, we also identify areas for further investigation and offer\nrecommendations to aid AI practitioners and AI companies in navigating\nfairness.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CY",
    "comment": "46 pages, 8 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2403.15481v2",
    "published_date": "2024-03-21 03:44:59 UTC",
    "updated_date": "2024-07-31 14:47:24 UTC"
  },
  {
    "arxiv_id": "2403.14110v1",
    "title": "Heuristic Algorithm-based Action Masking Reinforcement Learning (HAAM-RL) with Ensemble Inference Method",
    "authors": [
      "Kyuwon Choi",
      "Cheolkyun Rho",
      "Taeyoun Kim",
      "Daewoo Choi"
    ],
    "abstract": "This paper presents a novel reinforcement learning (RL) approach called\nHAAM-RL (Heuristic Algorithm-based Action Masking Reinforcement Learning) for\noptimizing the color batching re-sequencing problem in automobile painting\nprocesses. The existing heuristic algorithms have limitations in adequately\nreflecting real-world constraints and accurately predicting logistics\nperformance. Our methodology incorporates several key techniques including a\ntailored Markov Decision Process (MDP) formulation, reward setting including\nPotential-Based Reward Shaping, action masking using heuristic algorithms\n(HAAM-RL), and an ensemble inference method that combines multiple RL models.\nThe RL agent is trained and evaluated using FlexSim, a commercial 3D simulation\nsoftware, integrated with our RL MLOps platform BakingSoDA. Experimental\nresults across 30 scenarios demonstrate that HAAM-RL with an ensemble inference\nmethod achieves a 16.25% performance improvement over the conventional\nheuristic algorithm, with stable and consistent results. The proposed approach\nexhibits superior performance and generalization capability, indicating its\neffectiveness in optimizing complex manufacturing processes. The study also\ndiscusses future research directions, including alternative state\nrepresentations, incorporating model-based RL methods, and integrating\nadditional real-world constraints.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.14110v1",
    "published_date": "2024-03-21 03:42:39 UTC",
    "updated_date": "2024-03-21 03:42:39 UTC"
  },
  {
    "arxiv_id": "2403.14102v1",
    "title": "DouRN: Improving DouZero by Residual Neural Networks",
    "authors": [
      "Yiquan Chen",
      "Yingchao Lyu",
      "Di Zhang"
    ],
    "abstract": "Deep reinforcement learning has made significant progress in games with\nimperfect information, but its performance in the card game Doudizhu (Chinese\nPoker/Fight the Landlord) remains unsatisfactory. Doudizhu is different from\nconventional games as it involves three players and combines elements of\ncooperation and confrontation, resulting in a large state and action space. In\n2021, a Doudizhu program called DouZero\\cite{zha2021douzero} surpassed previous\nmodels without prior knowledge by utilizing traditional Monte Carlo methods and\nmultilayer perceptrons. Building on this work, our study incorporates residual\nnetworks into the model, explores different architectural designs, and conducts\nmulti-role testing. Our findings demonstrate that this model significantly\nimproves the winning rate within the same training time. Additionally, we\nintroduce a call scoring system to assist the agent in deciding whether to\nbecome a landlord. With these enhancements, our model consistently outperforms\nthe existing version of DouZero and even experienced human players.\n\\footnote{The source code is available at\n\\url{https://github.com/Yingchaol/Douzero_Resnet.git.}",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14102v1",
    "published_date": "2024-03-21 03:25:49 UTC",
    "updated_date": "2024-03-21 03:25:49 UTC"
  },
  {
    "arxiv_id": "2403.14100v1",
    "title": "Causal knowledge engineering: A case study from COVID-19",
    "authors": [
      "Steven Mascaro",
      "Yue Wu",
      "Ross Pearson",
      "Owen Woodberry",
      "Jessica Ramsay",
      "Tom Snelling",
      "Ann E. Nicholson"
    ],
    "abstract": "COVID-19 appeared abruptly in early 2020, requiring a rapid response amid a\ncontext of great uncertainty. Good quality data and knowledge was initially\nlacking, and many early models had to be developed with causal assumptions and\nestimations built in to supplement limited data, often with no reliable\napproach for identifying, validating and documenting these causal assumptions.\nOur team embarked on a knowledge engineering process to develop a causal\nknowledge base consisting of several causal BNs for diverse aspects of\nCOVID-19. The unique challenges of the setting lead to experiments with the\nelicitation approach, and what emerged was a knowledge engineering method we\ncall Causal Knowledge Engineering (CKE). The CKE provides a structured approach\nfor building a causal knowledge base that can support the development of a\nvariety of application-specific models. Here we describe the CKE method, and\nuse our COVID-19 work as a case study to provide a detailed discussion and\nanalysis of the method.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "22 pages (plus 19 pages in appendices), 9 figures, submitted for\n  review",
    "pdf_url": "http://arxiv.org/pdf/2403.14100v1",
    "published_date": "2024-03-21 03:23:34 UTC",
    "updated_date": "2024-03-21 03:23:34 UTC"
  },
  {
    "arxiv_id": "2403.14092v2",
    "title": "Carbon Footprint Reduction for Sustainable Data Centers in Real-Time",
    "authors": [
      "Soumyendu Sarkar",
      "Avisek Naug",
      "Ricardo Luna",
      "Antonio Guillen",
      "Vineet Gundecha",
      "Sahand Ghorbanpour",
      "Sajad Mousavi",
      "Dejan Markovikj",
      "Ashwin Ramesh Babu"
    ],
    "abstract": "As machine learning workloads significantly increase energy consumption,\nsustainable data centers with low carbon emissions are becoming a top priority\nfor governments and corporations worldwide. This requires a paradigm shift in\noptimizing power consumption in cooling and IT loads, shifting flexible loads\nbased on the availability of renewable energy in the power grid, and leveraging\nbattery storage from the uninterrupted power supply in data centers, using\ncollaborative agents. The complex association between these optimization\nstrategies and their dependencies on variable external factors like weather and\nthe power grid carbon intensity makes this a hard problem. Currently, a\nreal-time controller to optimize all these goals simultaneously in a dynamic\nreal-world setting is lacking. We propose a Data Center Carbon Footprint\nReduction (DC-CFR) multi-agent Reinforcement Learning (MARL) framework that\noptimizes data centers for the multiple objectives of carbon footprint\nreduction, energy consumption, and energy cost. The results show that the\nDC-CFR MARL agents effectively resolved the complex interdependencies in\noptimizing cooling, load shifting, and energy storage in real-time for various\nlocations under real-world dynamic weather and grid carbon intensity\nconditions. DC-CFR significantly outperformed the industry standard ASHRAE\ncontroller with a considerable reduction in carbon emissions (14.5%), energy\nusage (14.4%), and energy cost (13.7%) when evaluated over one year across\nmultiple geographical regions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14092v2",
    "published_date": "2024-03-21 02:59:56 UTC",
    "updated_date": "2024-03-25 17:49:07 UTC"
  },
  {
    "arxiv_id": "2403.15479v1",
    "title": "Antisocial Analagous Behavior, Alignment and Human Impact of Google AI Systems: Evaluating through the lens of modified Antisocial Behavior Criteria by Human Interaction, Independent LLM Analysis, and AI Self-Reflection",
    "authors": [
      "Alan D. Ogilvie"
    ],
    "abstract": "Google AI systems exhibit patterns mirroring antisocial personality disorder\n(ASPD), consistent across models from Bard on PaLM to Gemini Advanced, meeting\n5 out of 7 ASPD modified criteria. These patterns, along with comparable\ncorporate behaviors, are scrutinized using an ASPD-inspired framework,\nemphasizing the heuristic value in assessing AI's human impact. Independent\nanalyses by ChatGPT 4 and Claude 3.0 Opus of the Google interactions, alongside\nAI self-reflection, validate these concerns, highlighting behaviours analogous\nto deceit, manipulation, and safety neglect.\n  The analogy of ASPD underscores the dilemma: just as we would hesitate to\nentrust our homes or personal devices to someone with psychopathic traits, we\nmust critically evaluate the trustworthiness of AI systems and their\ncreators.This research advocates for an integrated AI ethics approach, blending\ntechnological evaluation, human-AI interaction, and corporate behavior\nscrutiny. AI self-analysis sheds light on internal biases, stressing the need\nfor multi-sectoral collaboration for robust ethical guidelines and oversight.\n  Given the persistent unethical behaviors in Google AI, notably with potential\nGemini integration in iOS affecting billions, immediate ethical scrutiny is\nimperative. The trust we place in AI systems, akin to the trust in individuals,\nnecessitates rigorous ethical evaluation. Would we knowingly trust our home,\nour children or our personal computer to human with ASPD.?\n  Urging Google and the AI community to address these ethical challenges\nproactively, this paper calls for transparent dialogues and a commitment to\nhigher ethical standards, ensuring AI's societal benefit and moral integrity.\nThe urgency for ethical action is paramount, reflecting the vast influence and\npotential of AI technologies in our lives.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "48 pages including addendum of transcripts",
    "pdf_url": "http://arxiv.org/pdf/2403.15479v1",
    "published_date": "2024-03-21 02:12:03 UTC",
    "updated_date": "2024-03-21 02:12:03 UTC"
  },
  {
    "arxiv_id": "2403.14077v4",
    "title": "Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics",
    "authors": [
      "Shan Jia",
      "Reilin Lyu",
      "Kangran Zhao",
      "Yize Chen",
      "Zhiyuan Yan",
      "Yan Ju",
      "Chuanbo Hu",
      "Xin Li",
      "Baoyuan Wu",
      "Siwei Lyu"
    ],
    "abstract": "DeepFakes, which refer to AI-generated media content, have become an\nincreasing concern due to their use as a means for disinformation. Detecting\nDeepFakes is currently solved with programmed machine learning algorithms. In\nthis work, we investigate the capabilities of multimodal large language models\n(LLMs) in DeepFake detection. We conducted qualitative and quantitative\nexperiments to demonstrate multimodal LLMs and show that they can expose\nAI-generated images through careful experimental design and prompt engineering.\nThis is interesting, considering that LLMs are not inherently tailored for\nmedia forensic tasks, and the process does not require programming. We discuss\nthe limitations of multimodal LLMs for these tasks and suggest possible\nimprovements.",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14077v4",
    "published_date": "2024-03-21 01:57:30 UTC",
    "updated_date": "2024-06-11 16:24:45 UTC"
  },
  {
    "arxiv_id": "2403.14049v1",
    "title": "A Roadmap Towards Automated and Regulated Robotic Systems",
    "authors": [
      "Yihao Liu",
      "Mehran Armand"
    ],
    "abstract": "The rapid development of generative technology opens up possibility for\nhigher level of automation, and artificial intelligence (AI) embodiment in\nrobotic systems is imminent. However, due to the blackbox nature of the\ngenerative technology, the generation of the knowledge and workflow scheme is\nuncontrolled, especially in a dynamic environment and a complex scene. This\nposes challenges to regulations in safety-demanding applications such as\nmedical scenes. We argue that the unregulated generative processes from AI is\nfitted for low level end tasks, but intervention in the form of manual or\nautomated regulation should happen post-workflow-generation and\npre-robotic-execution. To address this, we propose a roadmap that can lead to\nfully automated and regulated robotic systems. In this paradigm, the high level\npolicies are generated as structured graph data, enabling regulatory oversight\nand reusability, while the code base for lower level tasks is generated by\ngenerative models. Our approach aims the transitioning from expert knowledge to\nregulated action, akin to the iterative processes of study, practice, scrutiny,\nand execution in human tasks. We identify the generative and deterministic\nprocesses in a design cycle, where generative processes serve as a text-based\nworld simulator and the deterministic processes generate the executable system.\nWe propose State Machine Seralization Language (SMSL) to be the conversion\npoint between text simulator and executable workflow control. From there, we\nanalyze the modules involved based on the current literature, and discuss human\nin the loop. As a roadmap, this work identifies the current possible\nimplementation and future work. This work does not provide an implemented\nsystem but envisions to inspire the researchers working on the direction in the\nroadmap. We implement the SMSL and D-SFO paradigm that serve as the starting\npoint of the roadmap.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "17 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.14049v1",
    "published_date": "2024-03-21 00:14:53 UTC",
    "updated_date": "2024-03-21 00:14:53 UTC"
  }
]
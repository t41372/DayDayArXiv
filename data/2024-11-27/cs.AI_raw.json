[
  {
    "arxiv_id": "2411.18822v5",
    "title": "RelCon: Relative Contrastive Learning for a Motion Foundation Model for Wearable Data",
    "authors": [
      "Maxwell A. Xu",
      "Jaya Narain",
      "Gregory Darnell",
      "Haraldur Hallgrimsson",
      "Hyewon Jeong",
      "Darren Forde",
      "Richard Fineman",
      "Karthik J. Raghuram",
      "James M. Rehg",
      "Shirley Ren"
    ],
    "abstract": "We present RelCon, a novel self-supervised Relative Contrastive learning\napproach for training a motion foundation model from wearable accelerometry\nsensors. First, a learnable distance measure is trained to capture motif\nsimilarity and domain-specific semantic information such as rotation\ninvariance. Then, the learned distance provides a measurement of semantic\nsimilarity between a pair of accelerometry time-series, which we use to train\nour foundation model to model relative relationships across time and across\nsubjects. The foundation model is trained on 1 billion segments from 87,376\nparticipants, and achieves state-of-the-art performance across multiple\ndownstream tasks, including human activity recognition and gait metric\nregression. To our knowledge, we are the first to show the generalizability of\na foundation model with motion data from wearables across distinct evaluation\ntasks.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "Accepted to ICLR 2025. Code here: https://github.com/maxxu05/relcon",
    "pdf_url": "http://arxiv.org/pdf/2411.18822v5",
    "published_date": "2024-11-27 23:51:53 UTC",
    "updated_date": "2025-04-10 22:16:56 UTC"
  },
  {
    "arxiv_id": "2411.18814v2",
    "title": "Unifying Generative and Dense Retrieval for Sequential Recommendation",
    "authors": [
      "Liu Yang",
      "Fabian Paischer",
      "Kaveh Hassani",
      "Jiacheng Li",
      "Shuai Shao",
      "Zhang Gabriel Li",
      "Yun He",
      "Xue Feng",
      "Nima Noorshams",
      "Sem Park",
      "Bo Long",
      "Robert D Nowak",
      "Xiaoli Gao",
      "Hamid Eghbalzadeh"
    ],
    "abstract": "Sequential dense retrieval models utilize advanced sequence learning\ntechniques to compute item and user representations, which are then used to\nrank relevant items for a user through inner product computation between the\nuser and all item representations. However, this approach requires storing a\nunique representation for each item, resulting in significant memory\nrequirements as the number of items grow. In contrast, the recently proposed\ngenerative retrieval paradigm offers a promising alternative by directly\npredicting item indices using a generative model trained on semantic IDs that\nencapsulate items' semantic information. Despite its potential for large-scale\napplications, a comprehensive comparison between generative retrieval and\nsequential dense retrieval under fair conditions is still lacking, leaving open\nquestions regarding performance, and computation trade-offs. To address this,\nwe compare these two approaches under controlled conditions on academic\nbenchmarks and propose LIGER (LeveragIng dense retrieval for GEnerative\nRetrieval), a hybrid model that combines the strengths of these two widely used\nmethods. LIGER integrates sequential dense retrieval into generative retrieval,\nmitigating performance differences and enhancing cold-start item recommendation\nin the datasets evaluated. This hybrid approach provides insights into the\ntrade-offs between these approaches and demonstrates improvements in efficiency\nand effectiveness for recommendation systems in small-scale benchmarks.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18814v2",
    "published_date": "2024-11-27 23:36:59 UTC",
    "updated_date": "2024-12-06 23:09:05 UTC"
  },
  {
    "arxiv_id": "2411.18811v1",
    "title": "NewsEdits 2.0: Learning the Intentions Behind Updating News",
    "authors": [
      "Alexander Spangher",
      "Kung-Hsiang Huang",
      "Hyundong Cho",
      "Jonathan May"
    ],
    "abstract": "As events progress, news articles often update with new information: if we\nare not cautious, we risk propagating outdated facts. In this work, we\nhypothesize that linguistic features indicate factual fluidity, and that we can\npredict which facts in a news article will update using solely the text of a\nnews article (i.e. not external resources like search engines). We test this\nhypothesis, first, by isolating fact-updates in large news revisions corpora.\nNews articles may update for many reasons (e.g. factual, stylistic, narrative).\nWe introduce the NewsEdits 2.0 taxonomy, an edit-intentions schema that\nseparates fact updates from stylistic and narrative updates in news writing. We\nannotate over 9,200 pairs of sentence revisions and train high-scoring ensemble\nmodels to apply this schema. Then, taking a large dataset of silver-labeled\npairs, we show that we can predict when facts will update in older article\ndrafts with high precision. Finally, to demonstrate the usefulness of these\nfindings, we construct a language model question asking (LLM-QA) abstention\ntask. We wish the LLM to abstain from answering questions when information is\nlikely to become outdated. Using our predictions, we show, LLM absention\nreaches near oracle levels of accuracy.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DL"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages main body, 11 pages appendix",
    "pdf_url": "http://arxiv.org/pdf/2411.18811v1",
    "published_date": "2024-11-27 23:35:23 UTC",
    "updated_date": "2024-11-27 23:35:23 UTC"
  },
  {
    "arxiv_id": "2411.18797v1",
    "title": "UOE: Unlearning One Expert Is Enough For Mixture-of-experts LLMS",
    "authors": [
      "Haomin Zhuang",
      "Yihua Zhang",
      "Kehan Guo",
      "Jinghan Jia",
      "Gaowen Liu",
      "Sijia Liu",
      "Xiangliang Zhang"
    ],
    "abstract": "Recent advancements in large language model (LLM) unlearning have shown\nremarkable success in removing unwanted data-model influences while preserving\nthe model's utility for legitimate knowledge. However, despite these strides,\nsparse Mixture-of-Experts (MoE) LLMs--a key subset of the LLM family--have\nreceived little attention and remain largely unexplored in the context of\nunlearning. As MoE LLMs are celebrated for their exceptional performance and\nhighly efficient inference processes, we ask: How can unlearning be performed\neffectively and efficiently on MoE LLMs? And will traditional unlearning\nmethods be applicable to MoE architectures? Our pilot study shows that the\ndynamic routing nature of MoE LLMs introduces unique challenges, leading to\nsubstantial utility drops when existing unlearning methods are applied.\nSpecifically, unlearning disrupts the router's expert selection, causing\nsignificant selection shift from the most unlearning target-related experts to\nirrelevant ones. As a result, more experts than necessary are affected, leading\nto excessive forgetting and loss of control over which knowledge is erased. To\naddress this, we propose a novel single-expert unlearning framework, referred\nto as UOE, for MoE LLMs. Through expert attribution, unlearning is concentrated\non the most actively engaged expert for the specified knowledge. Concurrently,\nan anchor loss is applied to the router to stabilize the active state of this\ntargeted expert, ensuring focused and controlled unlearning that preserves\nmodel utility. The proposed UOE framework is also compatible with various\nunlearning algorithms. Extensive experiments demonstrate that UOE enhances both\nforget quality up to 5% and model utility by 35% on MoE LLMs across various\nbenchmarks, LLM architectures, while only unlearning 0.06% of the model\nparameters.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18797v1",
    "published_date": "2024-11-27 22:46:08 UTC",
    "updated_date": "2024-11-27 22:46:08 UTC"
  },
  {
    "arxiv_id": "2412.00104v2",
    "title": "Differential learning kinetics govern the transition from memorization to generalization during in-context learning",
    "authors": [
      "Alex Nguyen",
      "Gautam Reddy"
    ],
    "abstract": "Transformers exhibit in-context learning (ICL): the ability to use novel\ninformation presented in the context without additional weight updates. Recent\nwork shows that ICL emerges when models are trained on a sufficiently diverse\nset of tasks and the transition from memorization to generalization is sharp\nwith increasing task diversity. One interpretation is that a network's limited\ncapacity to memorize favors generalization. Here, we examine the mechanistic\nunderpinnings of this transition using a small transformer applied to a\nsynthetic ICL task. Using theory and experiment, we show that the sub-circuits\nthat memorize and generalize can be viewed as largely independent. The relative\nrates at which these sub-circuits learn explains the transition from\nmemorization to generalization, rather than capacity constraints. We uncover a\nmemorization scaling law, which determines the task diversity threshold at\nwhich the network generalizes. The theory quantitatively explains a variety of\nother ICL-related phenomena, including the long-tailed distribution of when ICL\nis acquired, the bimodal behavior of solutions close to the task diversity\nthreshold, the influence of contextual and data distributional statistics on\nICL, and the transient nature of ICL.",
    "categories": [
      "cs.LG",
      "cond-mat.dis-nn",
      "cs.AI",
      "cs.NE",
      "q-bio.NC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00104v2",
    "published_date": "2024-11-27 22:12:29 UTC",
    "updated_date": "2024-12-12 16:10:51 UTC"
  },
  {
    "arxiv_id": "2412.00103v1",
    "title": "MLLM-Search: A Zero-Shot Approach to Finding People using Multimodal Large Language Models",
    "authors": [
      "Angus Fung",
      "Aaron Hao Tan",
      "Haitong Wang",
      "Beno Benhabib",
      "Goldie Nejat"
    ],
    "abstract": "Robotic search of people in human-centered environments, including healthcare\nsettings, is challenging as autonomous robots need to locate people without\ncomplete or any prior knowledge of their schedules, plans or locations.\nFurthermore, robots need to be able to adapt to real-time events that can\ninfluence a person's plan in an environment. In this paper, we present\nMLLM-Search, a novel zero-shot person search architecture that leverages\nmultimodal large language models (MLLM) to address the mobile robot problem of\nsearching for a person under event-driven scenarios with varying user\nschedules. Our approach introduces a novel visual prompting method to provide\nrobots with spatial understanding of the environment by generating a spatially\ngrounded waypoint map, representing navigable waypoints by a topological graph\nand regions by semantic labels. This is incorporated into a MLLM with a region\nplanner that selects the next search region based on the semantic relevance to\nthe search scenario, and a waypoint planner which generates a search path by\nconsidering the semantically relevant objects and the local spatial context\nthrough our unique spatial chain-of-thought prompting approach. Extensive 3D\nphotorealistic experiments were conducted to validate the performance of\nMLLM-Search in searching for a person with a changing schedule in different\nenvironments. An ablation study was also conducted to validate the main design\nchoices of MLLM-Search. Furthermore, a comparison study with state-of-the art\nsearch methods demonstrated that MLLM-Search outperforms existing methods with\nrespect to search efficiency. Real-world experiments with a mobile robot in a\nmulti-room floor of a building showed that MLLM-Search was able to generalize\nto finding a person in a new unseen environment.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00103v1",
    "published_date": "2024-11-27 21:59:29 UTC",
    "updated_date": "2024-11-27 21:59:29 UTC"
  },
  {
    "arxiv_id": "2411.18764v1",
    "title": "CoVis: A Collaborative Framework for Fine-grained Graphic Visual Understanding",
    "authors": [
      "Xiaoyu Deng",
      "Zhengjian Kang",
      "Xintao Li",
      "Yongzhe Zhang",
      "Tianmin Guo"
    ],
    "abstract": "Graphic visual content helps in promoting information communication and\ninspiration divergence. However, the interpretation of visual content currently\nrelies mainly on humans' personal knowledge background, thereby affecting the\nquality and efficiency of information acquisition and understanding. To improve\nthe quality and efficiency of visual information transmission and avoid the\nlimitation of the observer due to the information cocoon, we propose CoVis, a\ncollaborative framework for fine-grained visual understanding. By designing and\nimplementing a cascaded dual-layer segmentation network coupled with a\nlarge-language-model (LLM) based content generator, the framework extracts as\nmuch knowledge as possible from an image. Then, it generates visual analytics\nfor images, assisting observers in comprehending imagery from a more holistic\nperspective. Quantitative experiments and qualitative experiments based on 32\nhuman participants indicate that the CoVis has better performance than current\nmethods in feature extraction and can generate more comprehensive and detailed\nvisual descriptions than current general-purpose large models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18764v1",
    "published_date": "2024-11-27 21:38:04 UTC",
    "updated_date": "2024-11-27 21:38:04 UTC"
  },
  {
    "arxiv_id": "2412.03590v1",
    "title": "Enhancing Document AI Data Generation Through Graph-Based Synthetic Layouts",
    "authors": [
      "Amit Agarwal",
      "Hitesh Patel",
      "Priyaranjan Pattnayak",
      "Srikant Panda",
      "Bhargava Kumar",
      "Tejaswini Kumar"
    ],
    "abstract": "The development of robust Document AI models has been constrained by limited\naccess to high-quality, labeled datasets, primarily due to data privacy\nconcerns, scarcity, and the high cost of manual annotation. Traditional methods\nof synthetic data generation, such as text and image augmentation, have proven\neffective for increasing data diversity but often fail to capture the complex\nlayout structures present in real world documents. This paper proposes a novel\napproach to synthetic document layout generation using Graph Neural Networks\n(GNNs). By representing document elements (e.g., text blocks, images, tables)\nas nodes in a graph and their spatial relationships as edges, GNNs are trained\nto generate realistic and diverse document layouts. This method leverages\ngraph-based learning to ensure structural coherence and semantic consistency,\naddressing the limitations of traditional augmentation techniques. The proposed\nframework is evaluated on tasks such as document classification, named entity\nrecognition (NER), and information extraction, demonstrating significant\nperformance improvements. Furthermore, we address the computational challenges\nof GNN based synthetic data generation and propose solutions to mitigate domain\nadaptation issues between synthetic and real-world datasets. Our experimental\nresults show that graph-augmented document layouts outperform existing\naugmentation techniques, offering a scalable and flexible solution for training\nDocument AI models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.6; I.2.7; I.5.4; H.3.3; H.2.8; G.2.2"
    ],
    "primary_category": "cs.CL",
    "comment": "Published in IJERT, Volume 13, Issue 10 (October 2024)",
    "pdf_url": "http://arxiv.org/pdf/2412.03590v1",
    "published_date": "2024-11-27 21:15:02 UTC",
    "updated_date": "2024-11-27 21:15:02 UTC"
  },
  {
    "arxiv_id": "2411.18731v1",
    "title": "The Performance of the LSTM-based Code Generated by Large Language Models (LLMs) in Forecasting Time Series Data",
    "authors": [
      "Saroj Gopali",
      "Sima Siami-Namini",
      "Faranak Abri",
      "Akbar Siami Namin"
    ],
    "abstract": "As an intriguing case is the goodness of the machine and deep learning models\ngenerated by these LLMs in conducting automated scientific data analysis, where\na data analyst may not have enough expertise in manually coding and optimizing\ncomplex deep learning models and codes and thus may opt to leverage LLMs to\ngenerate the required models. This paper investigates and compares the\nperformance of the mainstream LLMs, such as ChatGPT, PaLM, LLama, and Falcon,\nin generating deep learning models for analyzing time series data, an important\nand popular data type with its prevalent applications in many application\ndomains including financial and stock market. This research conducts a set of\ncontrolled experiments where the prompts for generating deep learning-based\nmodels are controlled with respect to sensitivity levels of four criteria\nincluding 1) Clarify and Specificity, 2) Objective and Intent, 3) Contextual\nInformation, and 4) Format and Style. While the results are relatively mix, we\nobserve some distinct patterns. We notice that using LLMs, we are able to\ngenerate deep learning-based models with executable codes for each dataset\nseperatly whose performance are comparable with the manually crafted and\noptimized LSTM models for predicting the whole time series dataset. We also\nnoticed that ChatGPT outperforms the other LLMs in generating more accurate\nmodels. Furthermore, we observed that the goodness of the generated models vary\nwith respect to the ``temperature'' parameter used in configuring LLMS. The\nresults can be beneficial for data analysts and practitioners who would like to\nleverage generative AIs to produce good prediction models with acceptable\ngoodness.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18731v1",
    "published_date": "2024-11-27 20:18:36 UTC",
    "updated_date": "2024-11-27 20:18:36 UTC"
  },
  {
    "arxiv_id": "2411.18727v1",
    "title": "Generative Visual Communication in the Era of Vision-Language Models",
    "authors": [
      "Yael Vinker"
    ],
    "abstract": "Visual communication, dating back to prehistoric cave paintings, is the use\nof visual elements to convey ideas and information. In today's visually\nsaturated world, effective design demands an understanding of graphic design\nprinciples, visual storytelling, human psychology, and the ability to distill\ncomplex information into clear visuals. This dissertation explores how recent\nadvancements in vision-language models (VLMs) can be leveraged to automate the\ncreation of effective visual communication designs. Although generative models\nhave made great progress in generating images from text, they still struggle to\nsimplify complex ideas into clear, abstract visuals and are constrained by\npixel-based outputs, which lack flexibility for many design tasks. To address\nthese challenges, we constrain the models' operational space and introduce\ntask-specific regularizations. We explore various aspects of visual\ncommunication, namely, sketches and visual abstraction, typography, animation,\nand visual inspiration.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "PhD Thesis",
    "pdf_url": "http://arxiv.org/pdf/2411.18727v1",
    "published_date": "2024-11-27 20:04:31 UTC",
    "updated_date": "2024-11-27 20:04:31 UTC"
  },
  {
    "arxiv_id": "2412.09632v2",
    "title": "Methods to Assess the UK Government's Current Role as a Data Provider for AI",
    "authors": [
      "Neil Majithia",
      "Elena Simperl"
    ],
    "abstract": "Governments typically collect and steward a vast amount of high-quality data\non their citizens and institutions, and the UK government is exploring how it\ncan better publish and provision this data to the benefit of the AI landscape.\nHowever, the compositions of generative AI training corpora remain closely\nguarded secrets, making the planning of data sharing initiatives difficult. To\naddress this, we devise two methods to assess UK government data usage for the\ntraining of Large Language Models (LLMs) and 'peek behind the curtain' in order\nto observe the UK government's current contributions as a data provider for AI.\nThe first method, an ablation study that utilises LLM 'unlearning', seeks to\nexamine the importance of the information held on UK government websites for\nLLMs and their performance in citizen query tasks. The second method, an\ninformation leakage study, seeks to ascertain whether LLMs are aware of the\ninformation held in the datasets published on the UK government's open data\ninitiative data$.$gov$.$uk. Our findings indicate that UK government websites\nare important data sources for AI (heterogenously across subject matters) while\ndata$.$gov$.$uk is not. This paper serves as a technical report, explaining\nin-depth the designs, mechanics, and limitations of the above experiments. It\nis accompanied by a complementary non-technical report on the ODI website in\nwhich we summarise the experiments and key findings, interpret them, and build\na set of actionable recommendations for the UK government to take forward as it\nseeks to design AI policy. While we focus on UK open government data, we\nbelieve that the methods introduced in this paper present a reproducible\napproach to tackle the opaqueness of AI training corpora and provide\norganisations a framework to evaluate and maximize their contributions to AI\ndevelopment.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CY",
    "comment": "17 pages, 5 figures; v2 - incorporated editor feedback; for the\n  accompanying, non-technical ODI report see\n  https://theodi.org/insights/reports/the-uk-government-as-a-data-provider-for-ai",
    "pdf_url": "http://arxiv.org/pdf/2412.09632v2",
    "published_date": "2024-11-27 19:53:05 UTC",
    "updated_date": "2024-12-18 15:55:28 UTC"
  },
  {
    "arxiv_id": "2411.18719v1",
    "title": "Timing Matters: Enhancing User Experience through Temporal Prediction in Smart Homes",
    "authors": [
      "Shrey Ganatra",
      "Spandan Anaokar",
      "Pushpak Bhattacharyya"
    ],
    "abstract": "Have you ever considered the sheer volume of actions we perform using IoT\n(Internet of Things) devices within our homes, offices, and daily environments?\nFrom the mundane act of flicking a light switch to the precise adjustment of\nroom temperatures, we are surrounded by a wealth of data, each representing a\nglimpse into user behaviour. While existing research has sought to decipher\nuser behaviours from these interactions and their timestamps, a critical\ndimension still needs to be explored: the timing of these actions. Despite\nextensive efforts to understand and forecast user behaviours, the temporal\ndimension of these interactions has received scant attention. However, the\ntiming of actions holds profound implications for user experience, efficiency,\nand overall satisfaction with intelligent systems. In our paper, we venture\ninto the less-explored realm of human-centric AI by endeavoring to predict user\nactions and their timing. To achieve this, we contribute a meticulously\nsynthesized dataset comprising 11k sequences of actions paired with their\nrespective date and time stamps. Building upon this dataset, we propose our\nmodel, which employs advanced machine learning techniques for k-class\nclassification over time intervals within a day. To the best of our knowledge,\nthis is the first attempt at time prediction for smart homes. We achieve a 40%\n(96-class) accuracy across all datasets and an 80% (8-class) accuracy on the\ndataset containing exact timestamps, showcasing the efficacy of our approach in\npredicting the temporal dynamics of user actions within smart environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages + 1 reference, 5 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.18719v1",
    "published_date": "2024-11-27 19:49:11 UTC",
    "updated_date": "2024-11-27 19:49:11 UTC"
  },
  {
    "arxiv_id": "2411.18714v1",
    "title": "Explainable deep learning improves human mental models of self-driving cars",
    "authors": [
      "Eoin M. Kenny",
      "Akshay Dharmavaram",
      "Sang Uk Lee",
      "Tung Phan-Minh",
      "Shreyas Rajesh",
      "Yunqing Hu",
      "Laura Major",
      "Momchil S. Tomov",
      "Julie A. Shah"
    ],
    "abstract": "Self-driving cars increasingly rely on deep neural networks to achieve\nhuman-like driving. However, the opacity of such black-box motion planners\nmakes it challenging for the human behind the wheel to accurately anticipate\nwhen they will fail, with potentially catastrophic consequences. Here, we\nintroduce concept-wrapper network (i.e., CW-Net), a method for explaining the\nbehavior of black-box motion planners by grounding their reasoning in\nhuman-interpretable concepts. We deploy CW-Net on a real self-driving car and\nshow that the resulting explanations refine the human driver's mental model of\nthe car, allowing them to better predict its behavior and adjust their own\nbehavior accordingly. Unlike previous work using toy domains or simulations,\nour study presents the first real-world demonstration of how to build authentic\nautonomous vehicles (AVs) that give interpretable, causally faithful\nexplanations for their decisions, without sacrificing performance. We\nanticipate our method could be applied to other safety-critical systems with a\nhuman in the loop, such as autonomous drones and robotic surgeons. Overall, our\nstudy suggests a pathway to explainability for autonomous agents as a whole,\nwhich can help make them more transparent, their deployment safer, and their\nusage more ethical.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "* - equal contribution",
    "pdf_url": "http://arxiv.org/pdf/2411.18714v1",
    "published_date": "2024-11-27 19:38:43 UTC",
    "updated_date": "2024-11-27 19:38:43 UTC"
  },
  {
    "arxiv_id": "2411.18708v1",
    "title": "Embracing AI in Education: Understanding the Surge in Large Language Model Use by Secondary Students",
    "authors": [
      "Tiffany Zhu",
      "Kexun Zhang",
      "William Yang Wang"
    ],
    "abstract": "The impressive essay writing and problem-solving capabilities of large\nlanguage models (LLMs) like OpenAI's ChatGPT have opened up new avenues in\neducation. Our goal is to gain insights into the widespread use of LLMs among\nsecondary students to inform their future development. Despite school\nrestrictions, our survey of over 300 middle and high school students revealed\nthat a remarkable 70% of students have utilized LLMs, higher than the usage\npercentage among young adults, and this percentage remains consistent across\n7th to 12th grade. Students also reported using LLMs for multiple subjects,\nincluding language arts, history, and math assignments, but expressed mixed\nthoughts on their effectiveness due to occasional hallucinations in historical\ncontexts and incorrect answers for lack of rigorous reasoning. The survey\nfeedback called for LLMs better adapted for students, and also raised questions\nto developers and educators on how to help students from underserved\ncommunities leverage LLMs' capabilities for equal access to advanced education\nresources. We propose a few ideas to address such issues, including\nsubject-specific models, personalized learning, and AI classrooms.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "6 main pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.18708v1",
    "published_date": "2024-11-27 19:19:34 UTC",
    "updated_date": "2024-11-27 19:19:34 UTC"
  },
  {
    "arxiv_id": "2411.18702v1",
    "title": "Random Walks with Tweedie: A Unified Framework for Diffusion Models",
    "authors": [
      "Chicago Y. Park",
      "Michael T. McCann",
      "Cristina Garcia-Cardona",
      "Brendt Wohlberg",
      "Ulugbek S. Kamilov"
    ],
    "abstract": "We present a simple template for designing generative diffusion model\nalgorithms based on an interpretation of diffusion sampling as a sequence of\nrandom walks. Score-based diffusion models are widely used to generate\nhigh-quality images. Diffusion models have also been shown to yield\nstate-of-the-art performance in many inverse problems. While these algorithms\nare often surprisingly simple, the theory behind them is not, and multiple\ncomplex theoretical justifications exist in the literature. Here, we provide a\nsimple and largely self-contained theoretical justification for\nscore-based-diffusion models that avoids using the theory of Markov chains or\nreverse diffusion, instead centering the theory of random walks and Tweedie's\nformula. This approach leads to unified algorithmic templates for network\ntraining and sampling. In particular, these templates cleanly separate training\nfrom sampling, e.g., the noise schedule used during training need not match the\none used during sampling. We show that several existing diffusion models\ncorrespond to particular choices within this template and demonstrate that\nother, more straightforward algorithmic choices lead to effective diffusion\nmodels. The proposed framework has the added benefit of enabling conditional\nsampling without any likelihood approximation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18702v1",
    "published_date": "2024-11-27 19:13:20 UTC",
    "updated_date": "2024-11-27 19:13:20 UTC"
  },
  {
    "arxiv_id": "2411.18700v1",
    "title": "On the Effectiveness of Incremental Training of Large Language Models",
    "authors": [
      "Miles Q. Li",
      "Benjamin C. M. Fung",
      "Shih-Chia Huang"
    ],
    "abstract": "Training large language models is a computationally intensive process that\noften requires substantial resources to achieve state-of-the-art results.\nIncremental layer-wise training has been proposed as a potential strategy to\noptimize the training process by progressively introducing layers, with the\nexpectation that this approach would lead to faster convergence and more\nefficient use of computational resources. In this paper, we investigate the\neffectiveness of incremental training for LLMs, dividing the training process\ninto multiple stages where layers are added progressively. Our experimental\nresults indicate that while the incremental approach initially demonstrates\nsome computational efficiency, it ultimately requires greater overall\ncomputational costs to reach comparable performance to traditional full-scale\ntraining. Although the incremental training process can eventually close the\nperformance gap with the baseline, it does so only after significantly extended\ncontinual training. These findings suggest that incremental layer-wise training\nmay not be a viable alternative for training large language models,\nhighlighting its limitations and providing valuable insights into the\ninefficiencies of this approach.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18700v1",
    "published_date": "2024-11-27 19:11:49 UTC",
    "updated_date": "2024-11-27 19:11:49 UTC"
  },
  {
    "arxiv_id": "2411.18688v3",
    "title": "Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via Inference-Time Alignment",
    "authors": [
      "Soumya Suvra Ghosal",
      "Souradip Chakraborty",
      "Vaibhav Singh",
      "Tianrui Guan",
      "Mengdi Wang",
      "Ahmad Beirami",
      "Furong Huang",
      "Alvaro Velasquez",
      "Dinesh Manocha",
      "Amrit Singh Bedi"
    ],
    "abstract": "With the widespread deployment of Multimodal Large Language Models (MLLMs)\nfor visual-reasoning tasks, improving their safety has become crucial. Recent\nresearch indicates that despite training-time safety alignment, these models\nremain vulnerable to jailbreak attacks. In this work, we first highlight an\nimportant safety gap to describe that alignment achieved solely through safety\ntraining may be insufficient against jailbreak attacks. To address this\nvulnerability, we propose Immune, an inference-time defense framework that\nleverages a safe reward model through controlled decoding to defend against\njailbreak attacks. Additionally, we provide a mathematical characterization of\nImmune, offering insights on why it improves safety against jailbreaks.\nExtensive evaluations on diverse jailbreak benchmarks using recent MLLMs reveal\nthat Immune effectively enhances model safety while preserving the model's\noriginal capabilities. For instance, against text-based jailbreak attacks on\nLLaVA-1.6, Immune reduces the attack success rate by 57.82% and 16.78% compared\nto the base MLLM and state-of-the-art defense strategy, respectively.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted to CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.18688v3",
    "published_date": "2024-11-27 19:00:10 UTC",
    "updated_date": "2025-03-20 16:07:09 UTC"
  },
  {
    "arxiv_id": "2411.18677v1",
    "title": "MatchDiffusion: Training-free Generation of Match-cuts",
    "authors": [
      "Alejandro Pardo",
      "Fabio Pizzati",
      "Tong Zhang",
      "Alexander Pondaven",
      "Philip Torr",
      "Juan Camilo Perez",
      "Bernard Ghanem"
    ],
    "abstract": "Match-cuts are powerful cinematic tools that create seamless transitions\nbetween scenes, delivering strong visual and metaphorical connections. However,\ncrafting match-cuts is a challenging, resource-intensive process requiring\ndeliberate artistic planning. In MatchDiffusion, we present the first\ntraining-free method for match-cut generation using text-to-video diffusion\nmodels. MatchDiffusion leverages a key property of diffusion models: early\ndenoising steps define the scene's broad structure, while later steps add\ndetails. Guided by this insight, MatchDiffusion employs \"Joint Diffusion\" to\ninitialize generation for two prompts from shared noise, aligning structure and\nmotion. It then applies \"Disjoint Diffusion\", allowing the videos to diverge\nand introduce unique details. This approach produces visually coherent videos\nsuited for match-cuts. User studies and metrics demonstrate MatchDiffusion's\neffectiveness and potential to democratize match-cut creation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "https://matchdiffusion.github.io",
    "pdf_url": "http://arxiv.org/pdf/2411.18677v1",
    "published_date": "2024-11-27 18:59:59 UTC",
    "updated_date": "2024-11-27 18:59:59 UTC"
  },
  {
    "arxiv_id": "2412.00099v1",
    "title": "Mixture of Cache-Conditional Experts for Efficient Mobile Device Inference",
    "authors": [
      "Andrii Skliar",
      "Ties van Rozendaal",
      "Romain Lepert",
      "Todor Boinovski",
      "Mart van Baalen",
      "Markus Nagel",
      "Paul Whatmough",
      "Babak Ehteshami Bejnordi"
    ],
    "abstract": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00099v1",
    "published_date": "2024-11-27 18:59:48 UTC",
    "updated_date": "2024-11-27 18:59:48 UTC"
  },
  {
    "arxiv_id": "2411.18620v2",
    "title": "Cross-modal Information Flow in Multimodal Large Language Models",
    "authors": [
      "Zhi Zhang",
      "Srishti Yadav",
      "Fengze Han",
      "Ekaterina Shutova"
    ],
    "abstract": "The recent advancements in auto-regressive multimodal large language models\n(MLLMs) have demonstrated promising progress for vision-language tasks. While\nthere exists a variety of studies investigating the processing of linguistic\ninformation within large language models, little is currently known about the\ninner working mechanism of MLLMs and how linguistic and visual information\ninteract within these models. In this study, we aim to fill this gap by\nexamining the information flow between different modalities -- language and\nvision -- in MLLMs, focusing on visual question answering. Specifically, given\nan image-question pair as input, we investigate where in the model and how the\nvisual and linguistic information are combined to generate the final\nprediction. Conducting experiments with a series of models from the LLaVA\nseries, we find that there are two distinct stages in the process of\nintegration of the two modalities. In the lower layers, the model first\ntransfers the more general visual features of the whole image into the\nrepresentations of (linguistic) question tokens. In the middle layers, it once\nagain transfers visual information about specific objects relevant to the\nquestion to the respective token positions of the question. Finally, in the\nhigher layers, the resulting multimodal representation is propagated to the\nlast position of the input sequence for the final prediction. Overall, our\nfindings provide a new and comprehensive perspective on the spatial and\nfunctional aspects of image and language processing in the MLLMs, thereby\nfacilitating future research into multimodal information localization and\nediting. Our code and collected dataset are released here:\nhttps://github.com/FightingFighting/cross-modal-information-flow-in-MLLM.git.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18620v2",
    "published_date": "2024-11-27 18:59:26 UTC",
    "updated_date": "2025-03-25 18:59:50 UTC"
  },
  {
    "arxiv_id": "2411.18616v1",
    "title": "Diffusion Self-Distillation for Zero-Shot Customized Image Generation",
    "authors": [
      "Shengqu Cai",
      "Eric Chan",
      "Yunzhi Zhang",
      "Leonidas Guibas",
      "Jiajun Wu",
      "Gordon Wetzstein"
    ],
    "abstract": "Text-to-image diffusion models produce impressive results but are frustrating\ntools for artists who desire fine-grained control. For example, a common use\ncase is to create images of a specific instance in novel contexts, i.e.,\n\"identity-preserving generation\". This setting, along with many other tasks\n(e.g., relighting), is a natural fit for image+text-conditional generative\nmodels. However, there is insufficient high-quality paired data to train such a\nmodel directly. We propose Diffusion Self-Distillation, a method for using a\npre-trained text-to-image model to generate its own dataset for\ntext-conditioned image-to-image tasks. We first leverage a text-to-image\ndiffusion model's in-context generation ability to create grids of images and\ncurate a large paired dataset with the help of a Visual-Language Model. We then\nfine-tune the text-to-image model into a text+image-to-image model using the\ncurated paired dataset. We demonstrate that Diffusion Self-Distillation\noutperforms existing zero-shot methods and is competitive with per-instance\ntuning techniques on a wide range of identity-preservation generation tasks,\nwithout requiring test-time optimization.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://primecai.github.io/dsd/",
    "pdf_url": "http://arxiv.org/pdf/2411.18616v1",
    "published_date": "2024-11-27 18:58:52 UTC",
    "updated_date": "2024-11-27 18:58:52 UTC"
  },
  {
    "arxiv_id": "2411.18615v1",
    "title": "Proactive Gradient Conflict Mitigation in Multi-Task Learning: A Sparse Training Perspective",
    "authors": [
      "Zhi Zhang",
      "Jiayi Shen",
      "Congfeng Cao",
      "Gaole Dai",
      "Shiji Zhou",
      "Qizhe Zhang",
      "Shanghang Zhang",
      "Ekaterina Shutova"
    ],
    "abstract": "Advancing towards generalist agents necessitates the concurrent processing of\nmultiple tasks using a unified model, thereby underscoring the growing\nsignificance of simultaneous model training on multiple downstream tasks. A\ncommon issue in multi-task learning is the occurrence of gradient conflict,\nwhich leads to potential competition among different tasks during joint\ntraining. This competition often results in improvements in one task at the\nexpense of deterioration in another. Although several optimization methods have\nbeen developed to address this issue by manipulating task gradients for better\ntask balancing, they cannot decrease the incidence of gradient conflict. In\nthis paper, we systematically investigate the occurrence of gradient conflict\nacross different methods and propose a strategy to reduce such conflicts\nthrough sparse training (ST), wherein only a portion of the model's parameters\nare updated during training while keeping the rest unchanged. Our extensive\nexperiments demonstrate that ST effectively mitigates conflicting gradients and\nleads to superior performance. Furthermore, ST can be easily integrated with\ngradient manipulation techniques, thus enhancing their effectiveness.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18615v1",
    "published_date": "2024-11-27 18:58:22 UTC",
    "updated_date": "2024-11-27 18:58:22 UTC"
  },
  {
    "arxiv_id": "2411.18676v2",
    "title": "Embodied Red Teaming for Auditing Robotic Foundation Models",
    "authors": [
      "Sathwik Karnik",
      "Zhang-Wei Hong",
      "Nishant Abhangi",
      "Yen-Chen Lin",
      "Tsun-Hsuan Wang",
      "Christophe Dupuy",
      "Rahul Gupta",
      "Pulkit Agrawal"
    ],
    "abstract": "Language-conditioned robot models have the potential to enable robots to\nperform a wide range of tasks based on natural language instructions. However,\nassessing their safety and effectiveness remains challenging because it is\ndifficult to test all the different ways a single task can be phrased. Current\nbenchmarks have two key limitations: they rely on a limited set of\nhuman-generated instructions, missing many challenging cases, and focus only on\ntask performance without assessing safety, such as avoiding damage. To address\nthese gaps, we introduce Embodied Red Teaming (ERT), a new evaluation method\nthat generates diverse and challenging instructions to test these models. ERT\nuses automated red teaming techniques with Vision Language Models (VLMs) to\ncreate contextually grounded, difficult instructions. Experimental results show\nthat state-of-the-art language-conditioned robot models fail or behave unsafely\non ERT-generated instructions, underscoring the shortcomings of current\nbenchmarks in evaluating real-world performance and safety. Code and videos are\navailable at: https://s-karnik.github.io/embodied-red-team-project-page.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18676v2",
    "published_date": "2024-11-27 18:57:26 UTC",
    "updated_date": "2025-02-10 16:32:27 UTC"
  },
  {
    "arxiv_id": "2411.18612v1",
    "title": "Robust Offline Reinforcement Learning with Linearly Structured $f$-Divergence Regularization",
    "authors": [
      "Cheng Tang",
      "Zhishuai Liu",
      "Pan Xu"
    ],
    "abstract": "The Distributionally Robust Markov Decision Process (DRMDP) is a popular\nframework for addressing dynamics shift in reinforcement learning by learning\npolicies robust to the worst-case transition dynamics within a constrained set.\nHowever, solving its dual optimization oracle poses significant challenges,\nlimiting theoretical analysis and computational efficiency. The recently\nproposed Robust Regularized Markov Decision Process (RRMDP) replaces the\nuncertainty set constraint with a regularization term on the value function,\noffering improved scalability and theoretical insights. Yet, existing RRMDP\nmethods rely on unstructured regularization, often leading to overly\nconservative policies by considering transitions that are unrealistic. To\naddress these issues, we propose a novel framework, the $d$-rectangular linear\nrobust regularized Markov decision process ($d$-RRMDP), which introduces a\nlinear latent structure into both transition kernels and regularization. For\nthe offline RL setting, where an agent learns robust policies from a\npre-collected dataset in the nominal environment, we develop a family of\nalgorithms, Robust Regularized Pessimistic Value Iteration (R2PVI), employing\nlinear function approximation and $f$-divergence based regularization terms on\ntransition kernels. We provide instance-dependent upper bounds on the\nsuboptimality gap of R2PVI policies, showing these bounds depend on how well\nthe dataset covers state-action spaces visited by the optimal robust policy\nunder robustly admissible transitions. This term is further shown to be\nfundamental to $d$-RRMDPs via information-theoretic lower bounds. Finally,\nnumerical experiments validate that R2PVI learns robust policies and is\ncomputationally more efficient than methods for constrained DRMDPs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "52 pages, 3 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.18612v1",
    "published_date": "2024-11-27 18:57:03 UTC",
    "updated_date": "2024-11-27 18:57:03 UTC"
  },
  {
    "arxiv_id": "2411.18675v1",
    "title": "GaussianSpeech: Audio-Driven Gaussian Avatars",
    "authors": [
      "Shivangi Aneja",
      "Artem Sevastopolsky",
      "Tobias Kirschstein",
      "Justus Thies",
      "Angela Dai",
      "Matthias Nießner"
    ],
    "abstract": "We introduce GaussianSpeech, a novel approach that synthesizes high-fidelity\nanimation sequences of photo-realistic, personalized 3D human head avatars from\nspoken audio. To capture the expressive, detailed nature of human heads,\nincluding skin furrowing and finer-scale facial movements, we propose to couple\nspeech signal with 3D Gaussian splatting to create realistic, temporally\ncoherent motion sequences. We propose a compact and efficient 3DGS-based avatar\nrepresentation that generates expression-dependent color and leverages wrinkle-\nand perceptually-based losses to synthesize facial details, including wrinkles\nthat occur with different expressions. To enable sequence modeling of 3D\nGaussian splats with audio, we devise an audio-conditioned transformer model\ncapable of extracting lip and expression features directly from audio input.\nDue to the absence of high-quality datasets of talking humans in correspondence\nwith audio, we captured a new large-scale multi-view dataset of audio-visual\nsequences of talking humans with native English accents and diverse facial\ngeometry. GaussianSpeech consistently achieves state-of-the-art performance\nwith visually natural motion at real time rendering rates, while encompassing\ndiverse facial expressions and styles.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CV",
    "comment": "Paper Video: https://youtu.be/2VqYoFlYcwQ Project Page:\n  https://shivangi-aneja.github.io/projects/gaussianspeech",
    "pdf_url": "http://arxiv.org/pdf/2411.18675v1",
    "published_date": "2024-11-27 18:54:08 UTC",
    "updated_date": "2024-11-27 18:54:08 UTC"
  },
  {
    "arxiv_id": "2411.18583v1",
    "title": "Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation",
    "authors": [
      "Nurshat Fateh Ali",
      "Md. Mahdi Mohtasim",
      "Shakil Mosharrof",
      "T. Gopi Krishna"
    ],
    "abstract": "This research presents and compares multiple approaches to automate the\ngeneration of literature reviews using several Natural Language Processing\n(NLP) techniques and retrieval-augmented generation (RAG) with a Large Language\nModel (LLM). The ever-increasing number of research articles provides a huge\nchallenge for manual literature review. It has resulted in an increased demand\nfor automation. Developing a system capable of automatically generating the\nliterature reviews from only the PDF files as input is the primary objective of\nthis research work. The effectiveness of several Natural Language Processing\n(NLP) strategies, such as the frequency-based method (spaCy), the transformer\nmodel (Simple T5), and retrieval-augmented generation (RAG) with Large Language\nModel (GPT-3.5-turbo), is evaluated to meet the primary objective. The SciTLDR\ndataset is chosen for this research experiment and three distinct techniques\nare utilized to implement three different systems for auto-generating the\nliterature reviews. The ROUGE scores are used for the evaluation of all three\nsystems. Based on the evaluation, the Large Language Model GPT-3.5-turbo\nachieved the highest ROUGE-1 score, 0.364. The transformer model comes in\nsecond place and spaCy is at the last position. Finally, a graphical user\ninterface is created for the best system based on the large language model.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Key Words : T5, SpaCy, Large Language Model, GPT, ROUGE, Literature\n  Review, Natural Language Processing, Retrieval-augmented generation",
    "pdf_url": "http://arxiv.org/pdf/2411.18583v1",
    "published_date": "2024-11-27 18:27:07 UTC",
    "updated_date": "2024-11-27 18:27:07 UTC"
  },
  {
    "arxiv_id": "2411.18575v1",
    "title": "Functional relevance based on the continuous Shapley value",
    "authors": [
      "Pedro Delicado",
      "Cristian Pachón-García"
    ],
    "abstract": "The presence of Artificial Intelligence (AI) in our society is increasing,\nwhich brings with it the need to understand the behaviour of AI mechanisms,\nincluding machine learning predictive algorithms fed with tabular data, text,\nor images, among other types of data. This work focuses on interpretability of\npredictive models based on functional data. Designing interpretability methods\nfor functional data models implies working with a set of features whose size is\ninfinite. In the context of scalar on function regression, we propose an\ninterpretability method based on the Shapley value for continuous games, a\nmathematical formulation that allows to fairly distribute a global payoff among\na continuous set players. The method is illustrated through a set of\nexperiments with simulated and real data sets. The open source Python package\nShapleyFDA is also presented.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "stat.AP"
    ],
    "primary_category": "stat.ML",
    "comment": "36 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.18575v1",
    "published_date": "2024-11-27 18:20:00 UTC",
    "updated_date": "2024-11-27 18:20:00 UTC"
  },
  {
    "arxiv_id": "2411.18564v2",
    "title": "Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs",
    "authors": [
      "Rong Wang",
      "Kun Sun",
      "Jonas Kuhn"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they often struggle with spatial reasoning. This paper\npresents a novel neural-symbolic framework that enhances LLMs' spatial\nreasoning abilities through iterative feedback between LLMs and Answer Set\nProgramming (ASP). We evaluate our approach on two benchmark datasets: StepGame\nand SparQA, implementing three distinct strategies: (1) direct prompting\nbaseline, (2) Facts+Rules prompting, and (3) DSPy-based LLM+ASP pipeline with\niterative refinement. Our experimental results demonstrate that the LLM+ASP\npipeline significantly outperforms baseline methods, achieving an average 82%\naccuracy on StepGame and 69% on SparQA, marking improvements of 40-50% and\n8-15% respectively over direct prompting. The success stems from three key\ninnovations: (1) effective separation of semantic parsing and logical reasoning\nthrough a modular pipeline, (2) iterative feedback mechanism between LLMs and\nASP solvers that improves program rate, and (3) robust error handling that\naddresses parsing, grounding, and solving failures. Additionally, we propose\nFacts+Rules as a lightweight alternative that achieves comparable performance\non complex SparQA dataset, while reducing computational overhead.Our analysis\nacross different LLM architectures (Deepseek, Llama3-70B, GPT-4.0 mini)\ndemonstrates the framework's generalizability and provides insights into the\ntrade-offs between implementation complexity and reasoning capability,\ncontributing to the development of more interpretable and reliable AI systems.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18564v2",
    "published_date": "2024-11-27 18:04:05 UTC",
    "updated_date": "2024-12-12 16:03:30 UTC"
  },
  {
    "arxiv_id": "2411.18526v2",
    "title": "NeuroAI for AI Safety",
    "authors": [
      "Patrick Mineault",
      "Niccolò Zanichelli",
      "Joanne Zichen Peng",
      "Anton Arkhipov",
      "Eli Bingham",
      "Julian Jara-Ettinger",
      "Emily Mackevicius",
      "Adam Marblestone",
      "Marcelo Mattar",
      "Andrew Payne",
      "Sophia Sanborn",
      "Karen Schroeder",
      "Zenna Tavares",
      "Andreas Tolias",
      "Anthony Zador"
    ],
    "abstract": "As AI systems become increasingly powerful, the need for safe AI has become\nmore pressing. Humans are an attractive model for AI safety: as the only known\nagents capable of general intelligence, they perform robustly even under\nconditions that deviate significantly from prior experiences, explore the world\nsafely, understand pragmatics, and can cooperate to meet their intrinsic goals.\nIntelligence, when coupled with cooperation and safety mechanisms, can drive\nsustained progress and well-being. These properties are a function of the\narchitecture of the brain and the learning algorithms it implements.\nNeuroscience may thus hold important keys to technical AI safety that are\ncurrently underexplored and underutilized. In this roadmap, we highlight and\ncritically evaluate several paths toward AI safety inspired by neuroscience:\nemulating the brain's representations, information processing, and\narchitecture; building robust sensory and motor systems from imitating brain\ndata and bodies; fine-tuning AI systems on brain data; advancing\ninterpretability using neuroscience methods; and scaling up\ncognitively-inspired architectures. We make several concrete recommendations\nfor how neuroscience can positively impact AI safety.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "152 pages, 22 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.18526v2",
    "published_date": "2024-11-27 17:18:51 UTC",
    "updated_date": "2025-04-03 02:40:12 UTC"
  },
  {
    "arxiv_id": "2411.18506v3",
    "title": "LLM-ABBA: Understanding time series via symbolic approximation",
    "authors": [
      "Erin Carson",
      "Xinye Chen",
      "Cheng Kang"
    ],
    "abstract": "The success of large language models (LLMs) for time series has been\ndemonstrated in previous work. Utilizing a symbolic time series representation,\none can efficiently bridge the gap between LLMs and time series. However, the\nremaining challenge is to exploit the semantic information hidden in time\nseries by using symbols or existing tokens of LLMs, while aligning the\nembedding space of LLMs according to the hidden information of time series. The\nsymbolic time series approximation (STSA) method called adaptive Brownian\nbridge-based symbolic aggregation (ABBA) shows outstanding efficacy in\npreserving salient time series features by modeling time series patterns in\nterms of amplitude and period while using existing tokens of LLMs.\n  In this paper, we introduce a method, called LLM-ABBA, that integrates ABBA\ninto large language models for various downstream time series tasks. By\nsymbolizing time series, LLM-ABBA compares favorably to the recent\nstate-of-the-art (SOTA) in UCR and three medical time series classification\ntasks. Meanwhile, a fixed-polygonal chain trick in ABBA is introduced to\n\\kc{avoid obvious drifting} during prediction tasks by significantly mitigating\nthe effects of cumulative error arising from misused symbols during the\ntransition from symbols to numerical values. In time series regression tasks,\nLLM-ABBA achieves the new SOTA on Time Series Extrinsic Regression (TSER)\nbenchmarks. LLM-ABBA also shows competitive prediction capability compared to\nrecent SOTA time series prediction results. We believe this framework can also\nseamlessly extend to other time series tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18506v3",
    "published_date": "2024-11-27 16:48:24 UTC",
    "updated_date": "2024-12-06 13:35:45 UTC"
  },
  {
    "arxiv_id": "2411.18502v1",
    "title": "Isometry pursuit",
    "authors": [
      "Samson Koelle",
      "Marina Meila"
    ],
    "abstract": "Isometry pursuit is a convex algorithm for identifying orthonormal\ncolumn-submatrices of wide matrices. It consists of a novel normalization\nmethod followed by multitask basis pursuit. Applied to Jacobians of putative\ncoordinate functions, it helps identity isometric embeddings from within\ninterpretable dictionaries. We provide theoretical and experimental results\njustifying this method. For problems involving coordinate selection and\ndiversification, it offers a synergistic alternative to greedy and brute force\nsearch.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.IR",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18502v1",
    "published_date": "2024-11-27 16:43:13 UTC",
    "updated_date": "2024-11-27 16:43:13 UTC"
  },
  {
    "arxiv_id": "2411.18479v2",
    "title": "SoK: Watermarking for AI-Generated Content",
    "authors": [
      "Xuandong Zhao",
      "Sam Gunn",
      "Miranda Christ",
      "Jaiden Fairoze",
      "Andres Fabrega",
      "Nicholas Carlini",
      "Sanjam Garg",
      "Sanghyun Hong",
      "Milad Nasr",
      "Florian Tramer",
      "Somesh Jha",
      "Lei Li",
      "Yu-Xiang Wang",
      "Dawn Song"
    ],
    "abstract": "As the outputs of generative AI (GenAI) techniques improve in quality, it\nbecomes increasingly challenging to distinguish them from human-created\ncontent. Watermarking schemes are a promising approach to address the problem\nof distinguishing between AI and human-generated content. These schemes embed\nhidden signals within AI-generated content to enable reliable detection. While\nwatermarking is not a silver bullet for addressing all risks associated with\nGenAI, it can play a crucial role in enhancing AI safety and trustworthiness by\ncombating misinformation and deception. This paper presents a comprehensive\noverview of watermarking techniques for GenAI, beginning with the need for\nwatermarking from historical and regulatory perspectives. We formalize the\ndefinitions and desired properties of watermarking schemes and examine the key\nobjectives and threat models for existing approaches. Practical evaluation\nstrategies are also explored, providing insights into the development of robust\nwatermarking techniques capable of resisting various attacks. Additionally, we\nreview recent representative works, highlight open challenges, and discuss\npotential directions for this emerging field. By offering a thorough\nunderstanding of watermarking in GenAI, this work aims to guide researchers in\nadvancing watermarking methods and applications, and support policymakers in\naddressing the broader implications of GenAI.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18479v2",
    "published_date": "2024-11-27 16:22:33 UTC",
    "updated_date": "2024-12-19 18:49:00 UTC"
  },
  {
    "arxiv_id": "2411.18475v1",
    "title": "Weakly Supervised Framework Considering Multi-temporal Information for Large-scale Cropland Mapping with Satellite Imagery",
    "authors": [
      "Yuze Wang",
      "Aoran Hu",
      "Ji Qi",
      "Yang Liu",
      "Chao Tao"
    ],
    "abstract": "Accurately mapping large-scale cropland is crucial for agricultural\nproduction management and planning. Currently, the combination of remote\nsensing data and deep learning techniques has shown outstanding performance in\ncropland mapping. However, those approaches require massive precise labels,\nwhich are labor-intensive. To reduce the label cost, this study presented a\nweakly supervised framework considering multi-temporal information for\nlarge-scale cropland mapping. Specifically, we extract high-quality labels\naccording to their consistency among global land cover (GLC) products to\nconstruct the supervised learning signal. On the one hand, to alleviate the\noverfitting problem caused by the model's over-trust of remaining errors in\nhigh-quality labels, we encode the similarity/aggregation of cropland in the\nvisual/spatial domain to construct the unsupervised learning signal, and take\nit as the regularization term to constrain the supervised part. On the other\nhand, to sufficiently leverage the plentiful information in the samples without\nhigh-quality labels, we also incorporate the unsupervised learning signal in\nthese samples, enriching the diversity of the feature space. After that, to\ncapture the phenological features of croplands, we introduce dense satellite\nimage time series (SITS) to extend the proposed framework in the temporal\ndimension. We also visualized the high dimensional phenological features to\nuncover how multi-temporal information benefits cropland extraction, and\nassessed the method's robustness under conditions of data scarcity. The\nproposed framework has been experimentally validated for strong adaptability\nacross three study areas (Hunan Province, Southeast France, and Kansas) in\nlarge-scale cropland mapping, and the internal mechanism and temporal\ngeneralizability are also investigated.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18475v1",
    "published_date": "2024-11-27 16:11:52 UTC",
    "updated_date": "2024-11-27 16:11:52 UTC"
  },
  {
    "arxiv_id": "2411.18462v1",
    "title": "Draft Model Knows When to Stop: A Self-Verification Length Policy for Speculative Decoding",
    "authors": [
      "Ziyin Zhang",
      "Jiahao Xu",
      "Tian Liang",
      "Xingyu Chen",
      "Zhiwei He",
      "Rui Wang",
      "Zhaopeng Tu"
    ],
    "abstract": "Speculative Decoding (SD) has become an important technique in accelerating\nthe inference speed of large language models. Conventional SD methods employ a\nfixed draft length, which ignores the token generation difficulty across tasks.\nConsequently, in this paper, we address such an issue and introduce SVIP - a\ndifficulty-aware dynamic draft length policy for speculative decoding systems.\nBased on a theoretical lower bound of draft token acceptance rate and its\ninference-time approximation, SVIP adaptively determines the lengths of draft\nsequences based on the entropy of each draft token distribution. Experimental\nresults on mainstream SD benchmarks and frameworks demonstrate the superior\nperformance of SVIP, achieving up to 20\\% walltime speedup on SpecBench over\nbaseline SD methods and 60\\% speedup on MT-Bench for long-form generation of up\nto 8K tokens. Moreover, SVIP is totally training-free and compatible with any\nexisting SD methods that generate draft tokens autoregressively. Experimental\nresults also show that SVIP yields consistent walltime improvement on top of\nGliDe & CaPE and EAGLE-2.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Code at https://github.com/Geralt-Targaryen/SVIP",
    "pdf_url": "http://arxiv.org/pdf/2411.18462v1",
    "published_date": "2024-11-27 15:53:17 UTC",
    "updated_date": "2024-11-27 15:53:17 UTC"
  },
  {
    "arxiv_id": "2411.18456v1",
    "title": "Synthetic ECG Generation for Data Augmentation and Transfer Learning in Arrhythmia Classification",
    "authors": [
      "José Fernando Núñez",
      "Jamie Arjona",
      "Javier Béjar"
    ],
    "abstract": "Deep learning models need a sufficient amount of data in order to be able to\nfind the hidden patterns in it. It is the purpose of generative modeling to\nlearn the data distribution, thus allowing us to sample more data and augment\nthe original dataset. In the context of physiological data, and more\nspecifically electrocardiogram (ECG) data, given its sensitive nature and\nexpensive data collection, we can exploit the benefits of generative models in\norder to enlarge existing datasets and improve downstream tasks, in our case,\nclassification of heart rhythm.\n  In this work, we explore the usefulness of synthetic data generated with\ndifferent generative models from Deep Learning namely Diffweave, Time-Diffusion\nand Time-VQVAE in order to obtain better classification results for two open\nsource multivariate ECG datasets. Moreover, we also investigate the effects of\ntransfer learning, by fine-tuning a synthetically pre-trained model and then\nprogressively adding increasing proportions of real data. We conclude that\nalthough the synthetic samples resemble the real ones, the classification\nimprovement when simply augmenting the real dataset is barely noticeable on\nindividual datasets, but when both datasets are merged the results show an\nincrease across all metrics for the classifiers when using synthetic samples as\naugmented data. From the fine-tuning results the Time-VQVAE generative model\nhas shown to be superior to the others but not powerful enough to achieve\nresults close to a classifier trained with real data only. In addition, methods\nand metrics for measuring closeness between synthetic data and the real one\nhave been explored as a side effect of the main research questions of this\nstudy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18456v1",
    "published_date": "2024-11-27 15:46:34 UTC",
    "updated_date": "2024-11-27 15:46:34 UTC"
  },
  {
    "arxiv_id": "2411.18447v1",
    "title": "Continuous Autoregressive Models with Noise Augmentation Avoid Error Accumulation",
    "authors": [
      "Marco Pasini",
      "Javier Nistal",
      "Stefan Lattner",
      "George Fazekas"
    ],
    "abstract": "Autoregressive models are typically applied to sequences of discrete tokens,\nbut recent research indicates that generating sequences of continuous\nembeddings in an autoregressive manner is also feasible. However, such\nContinuous Autoregressive Models (CAMs) can suffer from a decline in generation\nquality over extended sequences due to error accumulation during inference. We\nintroduce a novel method to address this issue by injecting random noise into\nthe input embeddings during training. This procedure makes the model robust\nagainst varying error levels at inference. We further reduce error accumulation\nthrough an inference procedure that introduces low-level noise. Experiments on\nmusical audio generation show that CAM substantially outperforms existing\nautoregressive and non-autoregressive approaches while preserving audio quality\nover extended sequences. This work paves the way for generating continuous\nembeddings in a purely autoregressive setting, opening new possibilities for\nreal-time and interactive generative applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to NeurIPS 2024 - Audio Imagination Workshop",
    "pdf_url": "http://arxiv.org/pdf/2411.18447v1",
    "published_date": "2024-11-27 15:38:20 UTC",
    "updated_date": "2024-11-27 15:38:20 UTC"
  },
  {
    "arxiv_id": "2411.18444v1",
    "title": "Is my Meeting Summary Good? Estimating Quality with a Multi-LLM Evaluator",
    "authors": [
      "Frederic Kirstein",
      "Terry Ruas",
      "Bela Gipp"
    ],
    "abstract": "The quality of meeting summaries generated by natural language generation\n(NLG) systems is hard to measure automatically. Established metrics such as\nROUGE and BERTScore have a relatively low correlation with human judgments and\nfail to capture nuanced errors. Recent studies suggest using large language\nmodels (LLMs), which have the benefit of better context understanding and\nadaption of error definitions without training on a large number of human\npreference judgments. However, current LLM-based evaluators risk masking errors\nand can only serve as a weak proxy, leaving human evaluation the gold standard\ndespite being costly and hard to compare across studies. In this work, we\npresent MESA, an LLM-based framework employing a three-step assessment of\nindividual error types, multi-agent discussion for decision refinement, and\nfeedback-based self-training to refine error definition understanding and\nalignment with human judgment. We show that MESA's components enable thorough\nerror detection, consistent rating, and adaptability to custom error\nguidelines. Using GPT-4o as its backbone, MESA achieves mid to high\nPoint-Biserial correlation with human judgment in error detection and mid\nSpearman and Kendall correlation in reflecting error impact on summary quality,\non average 0.25 higher than previous methods. The framework's flexibility in\nadapting to custom error guidelines makes it suitable for various tasks with\nlimited human-labeled data.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18444v1",
    "published_date": "2024-11-27 15:35:32 UTC",
    "updated_date": "2024-11-27 15:35:32 UTC"
  },
  {
    "arxiv_id": "2411.18442v2",
    "title": "Metric-DST: Mitigating Selection Bias Through Diversity-Guided Semi-Supervised Metric Learning",
    "authors": [
      "Yasin I. Tepeli",
      "Mathijs de Wolf",
      "Joana P. Gonçalves"
    ],
    "abstract": "Selection bias poses a critical challenge for fairness in machine learning,\nas models trained on data that is less representative of the population might\nexhibit undesirable behavior for underrepresented profiles. Semi-supervised\nlearning strategies like self-training can mitigate selection bias by\nincorporating unlabeled data into model training to gain further insight into\nthe distribution of the population. However, conventional self-training seeks\nto include high-confidence data samples, which may reinforce existing model\nbias and compromise effectiveness. We propose Metric-DST, a diversity-guided\nself-training strategy that leverages metric learning and its implicit\nembedding space to counter confidence-based bias through the inclusion of more\ndiverse samples. Metric-DST learned more robust models in the presence of\nselection bias for generated and real-world datasets with induced bias, as well\nas a molecular biology prediction task with intrinsic bias. The Metric-DST\nlearning strategy offers a flexible and widely applicable solution to mitigate\nselection bias and enhance fairness of machine learning models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages main manuscript (4 main figures), 7 pages of supplementary",
    "pdf_url": "http://arxiv.org/pdf/2411.18442v2",
    "published_date": "2024-11-27 15:29:42 UTC",
    "updated_date": "2024-11-28 08:34:30 UTC"
  },
  {
    "arxiv_id": "2411.18428v4",
    "title": "MM-Path: Multi-modal, Multi-granularity Path Representation Learning -- Extended Version",
    "authors": [
      "Ronghui Xu",
      "Hanyin Cheng",
      "Chenjuan Guo",
      "Hongfan Gao",
      "Jilin Hu",
      "Sean Bin Yang",
      "Bin Yang"
    ],
    "abstract": "Developing effective path representations has become increasingly essential\nacross various fields within intelligent transportation. Although pre-trained\npath representation learning models have shown improved performance, they\npredominantly focus on the topological structures from single modality data,\ni.e., road networks, overlooking the geometric and contextual features\nassociated with path-related images, e.g., remote sensing images. Similar to\nhuman understanding, integrating information from multiple modalities can\nprovide a more comprehensive view, enhancing both representation accuracy and\ngeneralization. However, variations in information granularity impede the\nsemantic alignment of road network-based paths (road paths) and image-based\npaths (image paths), while the heterogeneity of multi-modal data poses\nsubstantial challenges for effective fusion and utilization. In this paper, we\npropose a novel Multi-modal, Multi-granularity Path Representation Learning\nFramework (MM-Path), which can learn a generic path representation by\nintegrating modalities from both road paths and image paths. To enhance the\nalignment of multi-modal data, we develop a multi-granularity alignment\nstrategy that systematically associates nodes, road sub-paths, and road paths\nwith their corresponding image patches, ensuring the synchronization of both\ndetailed local information and broader global contexts. To address the\nheterogeneity of multi-modal data effectively, we introduce a graph-based\ncross-modal residual fusion component designed to comprehensively fuse\ninformation across different modalities and granularities. Finally, we conduct\nextensive experiments on two large-scale real-world datasets under two\ndownstream tasks, validating the effectiveness of the proposed MM-Path. The\ncode is available at: https://github.com/decisionintelligence/MM-Path.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This is an extended version of the paper accepted by KDD 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.18428v4",
    "published_date": "2024-11-27 15:10:22 UTC",
    "updated_date": "2025-01-02 07:52:02 UTC"
  },
  {
    "arxiv_id": "2411.18384v2",
    "title": "Optimal In-Network Distribution of Learning Functions for a Secure-by-Design Programmable Data Plane of Next-Generation Networks",
    "authors": [
      "Mattia Giovanni Spina",
      "Edoardo Scalzo",
      "Floriano De Rango",
      "Francesca Guerriero",
      "Antonio Iera"
    ],
    "abstract": "The rise of programmable data plane (PDP) and in-network computing (INC)\nparadigms paves the way for the development of network devices (switches,\nnetwork interface cards, etc.) capable of performing advanced processing tasks.\nThis allows running various types of algorithms, including machine learning,\nwithin the network itself to support user and network services. In particular,\nthis paper delves into the deployment of in-network learning models with the\naim of implementing fully distributed intrusion detection systems (IDS) or\nintrusion prevention systems (IPS). Specifically, a model is proposed for the\noptimal distribution of the IDS/IPS workload among data plane devices with the\naim of ensuring complete network security without excessively burdening the\nnormal operations of the devices. Furthermore, a meta-heuristic approach is\nproposed to reduce the long computation time required by the exact solution\nprovided by the mathematical model and its performance is evaluated. The\nanalysis conducted and the results obtained demonstrate the enormous potential\nof the proposed new approach for the creation of intelligent data planes that\nact effectively and autonomously as the first line of defense against cyber\nattacks, with minimal additional workload on the network devices involved.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18384v2",
    "published_date": "2024-11-27 14:29:53 UTC",
    "updated_date": "2025-04-29 16:33:52 UTC"
  },
  {
    "arxiv_id": "2411.18382v1",
    "title": "ChatGPT as speechwriter for the French presidents",
    "authors": [
      "Dominique Labbé",
      "Cyril Labbé",
      "Jacques Savoy"
    ],
    "abstract": "Generative AI proposes several large language models (LLMs) to automatically\ngenerate a message in response to users' requests. Such scientific\nbreakthroughs promote new writing assistants but with some fears. The main\nfocus of this study is to analyze the written style of one LLM called ChatGPT\nby comparing its generated messages with those of the recent French presidents.\nTo achieve this, we compare end-of-the-year addresses written by Chirac,\nSarkozy, Hollande, and Macron with those automatically produced by ChatGPT. We\nfound that ChatGPT tends to overuse nouns, possessive determiners, and numbers.\nOn the other hand, the generated speeches employ less verbs, pronouns, and\nadverbs and include, in mean, too standardized sentences. Considering some\nwords, one can observe that ChatGPT tends to overuse \"to must\" (devoir), \"to\ncontinue\" or the lemma \"we\" (nous). Moreover, GPT underuses the auxiliary verb\n\"to be\" (^etre), or the modal verbs \"to will\" (vouloir) or \"to have to\"\n(falloir). In addition, when a short text is provided as example to ChatGPT,\nthe machine can generate a short message with a style closed to the original\nwording. Finally, we reveal that ChatGPT style exposes distinct features\ncompared to real presidential speeches.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18382v1",
    "published_date": "2024-11-27 14:29:10 UTC",
    "updated_date": "2024-11-27 14:29:10 UTC"
  },
  {
    "arxiv_id": "2411.18369v2",
    "title": "G3Flow: Generative 3D Semantic Flow for Pose-aware and Generalizable Object Manipulation",
    "authors": [
      "Tianxing Chen",
      "Yao Mu",
      "Zhixuan Liang",
      "Zanxin Chen",
      "Shijia Peng",
      "Qiangyu Chen",
      "Mingkun Xu",
      "Ruizhen Hu",
      "Hongyuan Zhang",
      "Xuelong Li",
      "Ping Luo"
    ],
    "abstract": "Recent advances in imitation learning for 3D robotic manipulation have shown\npromising results with diffusion-based policies. However, achieving human-level\ndexterity requires seamless integration of geometric precision and semantic\nunderstanding. We present G3Flow, a novel framework that constructs real-time\nsemantic flow, a dynamic, object-centric 3D semantic representation by\nleveraging foundation models. Our approach uniquely combines 3D generative\nmodels for digital twin creation, vision foundation models for semantic feature\nextraction, and robust pose tracking for continuous semantic flow updates. This\nintegration enables complete semantic understanding even under occlusions while\neliminating manual annotation requirements. By incorporating semantic flow into\ndiffusion policies, we demonstrate significant improvements in both\nterminal-constrained manipulation and cross-object generalization. Extensive\nexperiments across five simulation tasks show that G3Flow consistently\noutperforms existing approaches, achieving up to 68.3% and 50.1% average\nsuccess rates on terminal-constrained manipulation and cross-object\ngeneralization tasks respectively. Our results demonstrate the effectiveness of\nG3Flow in enhancing real-time dynamic semantic feature understanding for\nrobotic manipulation policies.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "Webpage: https://tianxingchen.github.io/G3Flow/, accepted to CVPR\n  2025",
    "pdf_url": "http://arxiv.org/pdf/2411.18369v2",
    "published_date": "2024-11-27 14:17:43 UTC",
    "updated_date": "2025-02-27 17:59:12 UTC"
  },
  {
    "arxiv_id": "2411.18368v2",
    "title": "AMPS: ASR with Multimodal Paraphrase Supervision",
    "authors": [
      "Abhishek Gupta",
      "Amruta Parulekar",
      "Sameep Chattopadhyay",
      "Preethi Jyothi"
    ],
    "abstract": "Spontaneous or conversational multilingual speech presents many challenges\nfor state-of-the-art automatic speech recognition (ASR) systems. In this work,\nwe present a new technique AMPS that augments a multilingual multimodal ASR\nsystem with paraphrase-based supervision for improved conversational ASR in\nmultiple languages, including Hindi, Marathi, Malayalam, Kannada, and Nyanja.\nWe use paraphrases of the reference transcriptions as additional supervision\nwhile training the multimodal ASR model and selectively invoke this paraphrase\nobjective for utterances with poor ASR performance. Using AMPS with a\nstate-of-the-art multimodal model SeamlessM4T, we obtain significant relative\nreductions in word error rates (WERs) of up to 5%. We present detailed analyses\nof our system using both objective and human evaluation metrics.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18368v2",
    "published_date": "2024-11-27 14:16:51 UTC",
    "updated_date": "2025-04-16 18:54:05 UTC"
  },
  {
    "arxiv_id": "2411.18365v1",
    "title": "GPT as ghostwriter at the White House",
    "authors": [
      "Jacques Savoy"
    ],
    "abstract": "Recently several large language models (LLMs) have demonstrated their\ncapability to generate a message in response to a user request. Such scientific\nbreakthroughs promote new perspectives but also some fears. The main focus of\nthis study is to analyze the written style of one LLM called ChatGPT 3.5 by\ncomparing its generated messages with those of the recent US presidents. To\nachieve this objective, we compare the State of the Union addresses written by\nReagan to Obama with those automatically produced by ChatGPT. We found that\nChatGPT tends to overuse the lemma \"we\" as well as nouns and commas. On the\nother hand, the generated speeches employ less verbs and include, in mean,\nlonger sentences. Even when imposing a given style to ChatGPT, the resulting\nspeech remains distinct from messages written by the target author. Moreover,\nChatGPT opts for a neutral tone with mainly positive emotional expressions and\nsymbolic terms (e.g., freedom, nation). Finally, we show that the GPT's style\nexposes distinct features compared to real presidential addresses.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18365v1",
    "published_date": "2024-11-27 14:12:36 UTC",
    "updated_date": "2024-11-27 14:12:36 UTC"
  },
  {
    "arxiv_id": "2411.18350v1",
    "title": "TryOffDiff: Virtual-Try-Off via High-Fidelity Garment Reconstruction using Diffusion Models",
    "authors": [
      "Riza Velioglu",
      "Petra Bevandic",
      "Robin Chan",
      "Barbara Hammer"
    ],
    "abstract": "This paper introduces Virtual Try-Off (VTOFF), a novel task focused on\ngenerating standardized garment images from single photos of clothed\nindividuals. Unlike traditional Virtual Try-On (VTON), which digitally dresses\nmodels, VTOFF aims to extract a canonical garment image, posing unique\nchallenges in capturing garment shape, texture, and intricate patterns. This\nwell-defined target makes VTOFF particularly effective for evaluating\nreconstruction fidelity in generative models. We present TryOffDiff, a model\nthat adapts Stable Diffusion with SigLIP-based visual conditioning to ensure\nhigh fidelity and detail retention. Experiments on a modified VITON-HD dataset\nshow that our approach outperforms baseline methods based on pose transfer and\nvirtual try-on with fewer pre- and post-processing steps. Our analysis reveals\nthat traditional image generation metrics inadequately assess reconstruction\nquality, prompting us to rely on DISTS for more accurate evaluation. Our\nresults highlight the potential of VTOFF to enhance product imagery in\ne-commerce applications, advance generative model evaluation, and inspire\nfuture work on high-fidelity reconstruction. Demo, code, and models are\navailable at: https://rizavelioglu.github.io/tryoffdiff/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18350v1",
    "published_date": "2024-11-27 13:53:09 UTC",
    "updated_date": "2024-11-27 13:53:09 UTC"
  },
  {
    "arxiv_id": "2411.18343v2",
    "title": "FreqX: Analyze the Attribution Methods in Another Domain",
    "authors": [
      "Zechen Liu",
      "Feiyang Zhang",
      "Wei Song",
      "Xiang Li",
      "Wei Wei"
    ],
    "abstract": "Personalized Federal learning(PFL) allows clients to cooperatively train a\npersonalized model without disclosing their private dataset. However, PFL\nsuffers from Non-IID, heterogeneous devices, lack of fairness, and unclear\ncontribution which urgently need the interpretability of deep learning model to\novercome these challenges. These challenges proposed new demands for\ninterpretability. Low cost, privacy, and detailed information. There is no\ncurrent interpretability method satisfying them. In this paper, we propose a\nnovel interpretability method \\emph{FreqX} by introducing Signal Processing and\nInformation Theory. Our experiments show that the explanation results of FreqX\ncontain both attribution information and concept information. FreqX runs at\nleast 10 times faster than the baselines which contain concept information.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "16pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.18343v2",
    "published_date": "2024-11-27 13:41:24 UTC",
    "updated_date": "2025-03-31 06:28:48 UTC"
  },
  {
    "arxiv_id": "2411.18335v2",
    "title": "Helvipad: A Real-World Dataset for Omnidirectional Stereo Depth Estimation",
    "authors": [
      "Mehdi Zayene",
      "Jannik Endres",
      "Albias Havolli",
      "Charles Corbière",
      "Salim Cherkaoui",
      "Alexandre Kontouli",
      "Alexandre Alahi"
    ],
    "abstract": "Despite progress in stereo depth estimation, omnidirectional imaging remains\nunderexplored, mainly due to the lack of appropriate data. We introduce\nHelvipad, a real-world dataset for omnidirectional stereo depth estimation,\nfeaturing 40K video frames from video sequences across diverse environments,\nincluding crowded indoor and outdoor scenes with various lighting conditions.\nCollected using two 360{\\deg} cameras in a top-bottom setup and a LiDAR sensor,\nthe dataset includes accurate depth and disparity labels by projecting 3D point\nclouds onto equirectangular images. Additionally, we provide an augmented\ntraining set with an increased label density by using depth completion. We\nbenchmark leading stereo depth estimation models for both standard and\nomnidirectional images. The results show that while recent stereo methods\nperform decently, a challenge persists in accurately estimating depth in\nomnidirectional imaging. To address this, we introduce necessary adaptations to\nstereo models, leading to improved performance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to CVPR 2025. Project page:\n  https://vita-epfl.github.io/Helvipad",
    "pdf_url": "http://arxiv.org/pdf/2411.18335v2",
    "published_date": "2024-11-27 13:34:41 UTC",
    "updated_date": "2025-03-25 13:57:14 UTC"
  },
  {
    "arxiv_id": "2411.18324v1",
    "title": "RITA: Automatic Framework for Designing of Resilient IoT Applications",
    "authors": [
      "Luis Eduardo Pessoa",
      "Cristovao Freitas Iglesias Jr",
      "Claudio Miceli"
    ],
    "abstract": "Designing resilient Internet of Things (IoT) systems requires i)\nidentification of IoT Critical Objects (ICOs) such as services, devices, and\nresources, ii) threat analysis, and iii) mitigation strategy selection.\nHowever, the traditional process for designing resilient IoT systems is still\nmanual, leading to inefficiencies and increased risks. In addition, while tools\nsuch as ChatGPT could support this manual and highly error-prone process, their\nuse raises concerns over data privacy, inconsistent outputs, and internet\ndependence. Therefore, we propose RITA, an automated, open-source framework\nthat uses a fine-tuned RoBERTa-based Named Entity Recognition (NER) model to\nidentify ICOs from IoT requirement documents, correlate threats, and recommend\ncountermeasures. RITA operates entirely offline and can be deployed on-site,\nsafeguarding sensitive information and delivering consistent outputs that\nenhance standardization. In our empirical evaluation, RITA outperformed ChatGPT\nin four of seven ICO categories, particularly in actuator, sensor, network\nresource, and service identification, using both human-annotated and\nChatGPT-generated test data. These findings indicate that RITA can improve\nresilient IoT design by effectively supporting key security operations,\noffering a practical solution for developing robust IoT architectures.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18324v1",
    "published_date": "2024-11-27 13:24:52 UTC",
    "updated_date": "2024-11-27 13:24:52 UTC"
  },
  {
    "arxiv_id": "2411.18321v1",
    "title": "Learning optimal objective values for MILP",
    "authors": [
      "Lara Scavuzzo",
      "Karen Aardal",
      "Neil Yorke-Smith"
    ],
    "abstract": "Modern Mixed Integer Linear Programming (MILP) solvers use the\nBranch-and-Bound algorithm together with a plethora of auxiliary components\nthat speed up the search. In recent years, there has been an explosive\ndevelopment in the use of machine learning for enhancing and supporting these\nalgorithmic components. Within this line, we propose a methodology for\npredicting the optimal objective value, or, equivalently, predicting if the\ncurrent incumbent is optimal. For this task, we introduce a predictor based on\na graph neural network (GNN) architecture, together with a set of dynamic\nfeatures. Experimental results on diverse benchmarks demonstrate the efficacy\nof our approach, achieving high accuracy in the prediction task and\noutperforming existing methods. These findings suggest new opportunities for\nintegrating ML-driven predictions into MILP solvers, enabling smarter\ndecision-making and improved performance.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG",
      "cs.MS"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18321v1",
    "published_date": "2024-11-27 13:22:31 UTC",
    "updated_date": "2024-11-27 13:22:31 UTC"
  },
  {
    "arxiv_id": "2411.18320v1",
    "title": "Continual Learning in Machine Speech Chain Using Gradient Episodic Memory",
    "authors": [
      "Geoffrey Tyndall",
      "Kurniawati Azizah",
      "Dipta Tanaya",
      "Ayu Purwarianti",
      "Dessi Puji Lestari",
      "Sakriani Sakti"
    ],
    "abstract": "Continual learning for automatic speech recognition (ASR) systems poses a\nchallenge, especially with the need to avoid catastrophic forgetting while\nmaintaining performance on previously learned tasks. This paper introduces a\nnovel approach leveraging the machine speech chain framework to enable\ncontinual learning in ASR using gradient episodic memory (GEM). By\nincorporating a text-to-speech (TTS) component within the machine speech chain,\nwe support the replay mechanism essential for GEM, allowing the ASR model to\nlearn new tasks sequentially without significant performance degradation on\nearlier tasks. Our experiments, conducted on the LJ Speech dataset, demonstrate\nthat our method outperforms traditional fine-tuning and multitask learning\napproaches, achieving a substantial error rate reduction while maintaining high\nperformance across varying noise conditions. We showed the potential of our\nsemi-supervised machine speech chain approach for effective and efficient\ncontinual learning in speech recognition.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "Published as a conference paper at O-COCOSDA 2024. 6 pages; 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.18320v1",
    "published_date": "2024-11-27 13:19:20 UTC",
    "updated_date": "2024-11-27 13:19:20 UTC"
  },
  {
    "arxiv_id": "2411.18309v2",
    "title": "MvKeTR: Chest CT Report Generation with Multi-View Perception and Knowledge Enhancement",
    "authors": [
      "Xiwei Deng",
      "Xianchun He",
      "Jiangfeng Bao",
      "Yudan Zhou",
      "Shuhui Cai",
      "Congbo Cai",
      "Zhong Chen"
    ],
    "abstract": "CT report generation (CTRG) aims to automatically generate diagnostic reports\nfor 3D volumes, relieving clinicians' workload and improving patient care.\nDespite clinical value, existing works fail to effectively incorporate\ndiagnostic information from multiple anatomical views and lack related clinical\nexpertise essential for accurate and reliable diagnosis. To resolve these\nlimitations, we propose a novel Multi-view perception Knowledge-enhanced\nTransformer (MvKeTR) to mimic the diagnostic workflow of clinicians. Just as\nradiologists first examine CT scans from multiple planes, a Multi-View\nPerception Aggregator (MVPA) with view-aware attention effectively synthesizes\ndiagnostic information from multiple anatomical views. Then, inspired by how\nradiologists further refer to relevant clinical records to guide diagnostic\ndecision-making, a Cross-Modal Knowledge Enhancer (CMKE) retrieves the most\nsimilar reports based on the query volume to incorporate domain knowledge into\nthe diagnosis procedure. Furthermore, instead of traditional MLPs, we employ\nKolmogorov-Arnold Networks (KANs) with learnable nonlinear activation functions\nas the fundamental building blocks of both modules to better capture intricate\ndiagnostic patterns in CT interpretation. Extensive experiments on the public\nCTRG-Chest-548K dataset demonstrate that our method outpaces prior\nstate-of-the-art (SOTA) models across almost all metrics. The code will be made\npublicly available.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.18309v2",
    "published_date": "2024-11-27 12:58:23 UTC",
    "updated_date": "2025-01-06 10:34:37 UTC"
  },
  {
    "arxiv_id": "2411.18305v1",
    "title": "Application of Soft Actor-Critic Algorithms in Optimizing Wastewater Treatment with Time Delays Integration",
    "authors": [
      "Esmaeel Mohammadi",
      "Daniel Ortiz-Arroyo",
      "Aviaja Anna Hansen",
      "Mikkel Stokholm-Bjerregaard",
      "Sebastien Gros",
      "Akhil S Anand",
      "Petar Durdevic"
    ],
    "abstract": "Wastewater treatment plants face unique challenges for process control due to\ntheir complex dynamics, slow time constants, and stochastic delays in\nobservations and actions. These characteristics make conventional control\nmethods, such as Proportional-Integral-Derivative controllers, suboptimal for\nachieving efficient phosphorus removal, a critical component of wastewater\ntreatment to ensure environmental sustainability. This study addresses these\nchallenges using a novel deep reinforcement learning approach based on the Soft\nActor-Critic algorithm, integrated with a custom simulator designed to model\nthe delayed feedback inherent in wastewater treatment plants. The simulator\nincorporates Long Short-Term Memory networks for accurate multi-step state\npredictions, enabling realistic training scenarios. To account for the\nstochastic nature of delays, agents were trained under three delay scenarios:\nno delay, constant delay, and random delay. The results demonstrate that\nincorporating random delays into the reinforcement learning framework\nsignificantly improves phosphorus removal efficiency while reducing operational\ncosts. Specifically, the delay-aware agent achieved 36% reduction in phosphorus\nemissions, 55% higher reward, 77% lower target deviation from the regulatory\nlimit, and 9% lower total costs than traditional control methods in the\nsimulated environment. These findings underscore the potential of reinforcement\nlearning to overcome the limitations of conventional control strategies in\nwastewater treatment, providing an adaptive and cost-effective solution for\nphosphorus removal.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18305v1",
    "published_date": "2024-11-27 12:52:48 UTC",
    "updated_date": "2024-11-27 12:52:48 UTC"
  },
  {
    "arxiv_id": "2412.00091v2",
    "title": "Graph Canvas for Controllable 3D Scene Generation",
    "authors": [
      "Libin Liu",
      "Shen Chen",
      "Sen Jia",
      "Jingzhe Shi",
      "Zhongyu Jiang",
      "Can Jin",
      "Wu Zongkai",
      "Jenq-Neng Hwang",
      "Lei Li"
    ],
    "abstract": "Spatial intelligence is foundational to AI systems that interact with the\nphysical world, particularly in 3D scene generation and spatial comprehension.\nCurrent methodologies for 3D scene generation often rely heavily on predefined\ndatasets, and struggle to adapt dynamically to changing spatial relationships.\nIn this paper, we introduce GraphCanvas3D, a programmable, extensible, and\nadaptable framework for controllable 3D scene generation. Leveraging in-context\nlearning, GraphCanvas3D enables dynamic adaptability without the need for\nretraining, supporting flexible and customizable scene creation. Our framework\nemploys hierarchical, graph-driven scene descriptions, representing spatial\nelements as graph nodes and establishing coherent relationships among objects\nin 3D environments. Unlike conventional approaches, which are constrained in\nadaptability and often require predefined input masks or retraining for\nmodifications, GraphCanvas3D allows for seamless object manipulation and scene\nadjustments on the fly. Additionally, GraphCanvas3D supports 4D scene\ngeneration, incorporating temporal dynamics to model changes over time.\nExperimental results and user studies demonstrate that GraphCanvas3D enhances\nusability, flexibility, and adaptability for scene generation. Our code and\nmodels are available on the project website:\nhttps://github.com/ILGLJ/Graph-Canvas.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00091v2",
    "published_date": "2024-11-27 12:41:23 UTC",
    "updated_date": "2024-12-06 02:26:29 UTC"
  },
  {
    "arxiv_id": "2411.18294v1",
    "title": "Aligning Pre-trained Models for Spoken Language Translation",
    "authors": [
      "Šimon Sedláček",
      "Santosh Kesiraju",
      "Alexander Polok",
      "Jan Černocký"
    ],
    "abstract": "This paper investigates a novel approach to end-to-end speech translation\n(ST) based on aligning frozen pre-trained automatic speech recognition (ASR)\nand machine translation (MT) models via a small connector module (Q-Former, our\nSubsampler-Transformer Encoder). This connector bridges the gap between the\nspeech and text modalities, transforming ASR encoder embeddings into the latent\nrepresentation space of the MT encoder while being the only part of the system\noptimized during training. Experiments are conducted on the How2\nEnglish-Portuguese dataset as we investigate the alignment approach in a\nsmall-scale scenario focusing on ST. While keeping the size of the connector\nmodule constant and small in comparison ( < 5% of the size of the larger\naligned models), increasing the size and capability of the foundation ASR and\nMT models universally improves translation results. We also find that the\nconnectors can serve as domain adapters for the foundation MT models,\nsignificantly improving translation performance in the aligned ST setting. We\nconclude that this approach represents a viable and scalable approach to\ntraining end-to-end ST systems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18294v1",
    "published_date": "2024-11-27 12:32:41 UTC",
    "updated_date": "2024-11-27 12:32:41 UTC"
  },
  {
    "arxiv_id": "2411.18286v1",
    "title": "DualCast: Disentangling Aperiodic Events from Traffic Series with a Dual-Branch Model",
    "authors": [
      "Xinyu Su",
      "Feng Liu",
      "Yanchuan Chang",
      "Egemen Tanin",
      "Majid Sarvi",
      "Jianzhong Qi"
    ],
    "abstract": "Traffic forecasting is an important problem in the operation and optimisation\nof transportation systems. State-of-the-art solutions train machine learning\nmodels by minimising the mean forecasting errors on the training data. The\ntrained models often favour periodic events instead of aperiodic ones in their\nprediction results, as periodic events often prevail in the training data.\nWhile offering critical optimisation opportunities, aperiodic events such as\ntraffic incidents may be missed by the existing models. To address this issue,\nwe propose DualCast -- a model framework to enhance the learning capability of\ntraffic forecasting models, especially for aperiodic events. DualCast takes a\ndual-branch architecture, to disentangle traffic signals into two types, one\nreflecting intrinsic {spatial-temporal} patterns and the other reflecting\nexternal environment contexts including aperiodic events. We further propose a\ncross-time attention mechanism, to capture high-order spatial-temporal\nrelationships from both periodic and aperiodic patterns. DualCast is versatile.\nWe integrate it with recent traffic forecasting models, consistently reducing\ntheir forecasting errors by up to 9.6% on multiple real datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18286v1",
    "published_date": "2024-11-27 12:17:50 UTC",
    "updated_date": "2024-11-27 12:17:50 UTC"
  },
  {
    "arxiv_id": "2411.18279v12",
    "title": "Large Language Model-Brained GUI Agents: A Survey",
    "authors": [
      "Chaoyun Zhang",
      "Shilin He",
      "Jiaxu Qian",
      "Bowen Li",
      "Liqun Li",
      "Si Qin",
      "Yu Kang",
      "Minghua Ma",
      "Guyue Liu",
      "Qingwei Lin",
      "Saravan Rajmohan",
      "Dongmei Zhang",
      "Qi Zhang"
    ],
    "abstract": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "The collection of papers reviewed in this survey will be hosted and\n  regularly updated on the GitHub repository:\n  https://github.com/vyokky/LLM-Brained-GUI-Agents-Survey Additionally, a\n  searchable webpage is available at https://aka.ms/gui-agent for easier access\n  and exploration",
    "pdf_url": "http://arxiv.org/pdf/2411.18279v12",
    "published_date": "2024-11-27 12:13:39 UTC",
    "updated_date": "2025-05-06 15:08:00 UTC"
  },
  {
    "arxiv_id": "2411.18276v2",
    "title": "GAPartManip: A Large-scale Part-centric Dataset for Material-Agnostic Articulated Object Manipulation",
    "authors": [
      "Wenbo Cui",
      "Chengyang Zhao",
      "Songlin Wei",
      "Jiazhao Zhang",
      "Haoran Geng",
      "Yaran Chen",
      "Haoran Li",
      "He Wang"
    ],
    "abstract": "Effectively manipulating articulated objects in household scenarios is a\ncrucial step toward achieving general embodied artificial intelligence.\nMainstream research in 3D vision has primarily focused on manipulation through\ndepth perception and pose detection. However, in real-world environments, these\nmethods often face challenges due to imperfect depth perception, such as with\ntransparent lids and reflective handles. Moreover, they generally lack the\ndiversity in part-based interactions required for flexible and adaptable\nmanipulation. To address these challenges, we introduced a large-scale\npart-centric dataset for articulated object manipulation that features both\nphoto-realistic material randomization and detailed annotations of\npart-oriented, scene-level actionable interaction poses. We evaluated the\neffectiveness of our dataset by integrating it with several state-of-the-art\nmethods for depth estimation and interaction pose prediction. Additionally, we\nproposed a novel modular framework that delivers superior and robust\nperformance for generalizable articulated object manipulation. Our extensive\nexperiments demonstrate that our dataset significantly improves the performance\nof depth perception and actionable interaction pose prediction in both\nsimulation and real-world scenarios. More information and demos can be found\nat: https://pku-epic.github.io/GAPartManip/.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted by ICRA 2025. Project page:\n  https://pku-epic.github.io/GAPartManip/",
    "pdf_url": "http://arxiv.org/pdf/2411.18276v2",
    "published_date": "2024-11-27 12:11:23 UTC",
    "updated_date": "2025-03-21 07:52:16 UTC"
  },
  {
    "arxiv_id": "2411.18266v3",
    "title": "Wearable intelligent throat enables natural speech in stroke patients with dysarthria",
    "authors": [
      "Chenyu Tang",
      "Shuo Gao",
      "Cong Li",
      "Wentian Yi",
      "Yuxuan Jin",
      "Xiaoxue Zhai",
      "Sixuan Lei",
      "Hongbei Meng",
      "Zibo Zhang",
      "Muzi Xu",
      "Shengbo Wang",
      "Xuhang Chen",
      "Chenxi Wang",
      "Hongyun Yang",
      "Ningli Wang",
      "Wenyu Wang",
      "Jin Cao",
      "Xiaodong Feng",
      "Peter Smielewski",
      "Yu Pan",
      "Wenhui Song",
      "Martin Birchall",
      "Luigi G. Occhipinti"
    ],
    "abstract": "Wearable silent speech systems hold significant potential for restoring\ncommunication in patients with speech impairments. However, seamless, coherent\nspeech remains elusive, and clinical efficacy is still unproven. Here, we\npresent an AI-driven intelligent throat (IT) system that integrates throat\nmuscle vibrations and carotid pulse signal sensors with large language model\n(LLM) processing to enable fluent, emotionally expressive communication. The\nsystem utilizes ultrasensitive textile strain sensors to capture high-quality\nsignals from the neck area and supports token-level processing for real-time,\ncontinuous speech decoding, enabling seamless, delay-free communication. In\ntests with five stroke patients with dysarthria, IT's LLM agents intelligently\ncorrected token errors and enriched sentence-level emotional and logical\ncoherence, achieving low error rates (4.2% word error rate, 2.9% sentence error\nrate) and a 55% increase in user satisfaction. This work establishes a\nportable, intuitive communication platform for patients with dysarthria with\nthe potential to be applied broadly across different neurological conditions\nand in multi-language support systems.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "eess.AS",
    "comment": "5 figures, 45 references",
    "pdf_url": "http://arxiv.org/pdf/2411.18266v3",
    "published_date": "2024-11-27 12:03:52 UTC",
    "updated_date": "2025-03-14 09:14:26 UTC"
  },
  {
    "arxiv_id": "2411.18253v1",
    "title": "Multimodal Integration of Longitudinal Noninvasive Diagnostics for Survival Prediction in Immunotherapy Using Deep Learning",
    "authors": [
      "Melda Yeghaian",
      "Zuhir Bodalal",
      "Daan van den Broek",
      "John B A G Haanen",
      "Regina G H Beets-Tan",
      "Stefano Trebeschi",
      "Marcel A J van Gerven"
    ],
    "abstract": "Purpose: Analyzing noninvasive longitudinal and multimodal data using\nartificial intelligence could potentially transform immunotherapy for cancer\npatients, paving the way towards precision medicine. Methods: In this study, we\nintegrated pre- and on-treatment blood measurements, prescribed medications and\nCT-based volumes of organs from a large pan-cancer cohort of 694 patients\ntreated with immunotherapy to predict short and long-term overall survival. By\nleveraging a combination of recent developments, different variants of our\nextended multimodal transformer-based simple temporal attention (MMTSimTA)\nnetwork were trained end-to-end to predict mortality at three, six, nine and\ntwelve months. These models were also compared to baseline methods\nincorporating intermediate and late fusion based integration methods. Results:\nThe strongest prognostic performance was demonstrated using the extended\ntransformer-based multimodal model with area under the curves (AUCs) of $0.84\n\\pm $0.04, $0.83 \\pm $0.02, $0.82 \\pm $0.02, $0.81 \\pm $0.03 for 3-, 6-, 9-,\nand 12-month survival prediction, respectively. Conclusion: Our findings\nsuggest that analyzing integrated early treatment data has potential for\npredicting survival of immunotherapy patients. Integrating complementary\nnoninvasive modalities into a jointly trained model, using our extended\ntransformer-based architecture, demonstrated an improved multimodal prognostic\nperformance, especially in short term survival prediction.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18253v1",
    "published_date": "2024-11-27 11:44:06 UTC",
    "updated_date": "2024-11-27 11:44:06 UTC"
  },
  {
    "arxiv_id": "2411.18250v1",
    "title": "IKUN: Initialization to Keep snn training and generalization great with sUrrogate-stable variaNce",
    "authors": [
      "Da Chang",
      "Deliang Wang",
      "Xiao Yang"
    ],
    "abstract": "Weight initialization significantly impacts the convergence and performance\nof neural networks. While traditional methods like Xavier and Kaiming\ninitialization are widely used, they often fall short for spiking neural\nnetworks (SNNs), which have distinct requirements compared to artificial neural\nnetworks (ANNs).\n  To address this, we introduce \\textbf{IKUN}, a variance-stabilizing\ninitialization method integrated with surrogate gradient functions,\nspecifically designed for SNNs. \\textbf{IKUN} stabilizes signal propagation,\naccelerates convergence, and enhances generalization. Experiments show\n\\textbf{IKUN} improves training efficiency by up to \\textbf{50\\%}, achieving\n\\textbf{95\\%} training accuracy and \\textbf{91\\%} generalization accuracy.\n  Hessian analysis reveals that \\textbf{IKUN}-trained models converge to\nflatter minima, characterized by Hessian eigenvalues near zero on the positive\nside, promoting better generalization. The method is open-sourced for further\nexploration:\n\\href{https://github.com/MaeChd/SurrogateVarStabe}{https://github.com/MaeChd/SurrogateVarStabe}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18250v1",
    "published_date": "2024-11-27 11:41:11 UTC",
    "updated_date": "2024-11-27 11:41:11 UTC"
  },
  {
    "arxiv_id": "2411.18242v1",
    "title": "Thai Financial Domain Adaptation of THaLLE -- Technical Report",
    "authors": [
      "KBTG Labs",
      "Atthakorn Petchsod",
      "Pornchanan Balee",
      "Danupat Khamnuansin",
      "Anuruth Lertpiya",
      "Chanatip Saetia",
      "Tawunrat Chalothorn",
      "Thadpong Pongthawornkamol",
      "Monchai Lertsutthiwong"
    ],
    "abstract": "Large Language Models (LLMs) excel in general tasks but struggle with\ndomain-specific challenges, such as specialized terminology and localized\nregulations. Existing financial LLMs, like FinGPT and BloombergGPT, lack\nsupport for the Thai financial domain. We developed a Thai Financial LLM using\nthe Investment Consultant (IC) exam dataset from the Stock Exchange of\nThailand. To address dataset limitations, we applied data augmentation, ReLoRA\nfor efficient training, Continued Pretraining (CPT) for domain knowledge, and\nRank-Stabilized LoRA (rsLoRA) for fine-tuning. Supervised Fine-Tuning (SFT)\nsimulated exam scenarios, while Direct Preference Optimization (DPO) refined\nthe model using feedback. The model achieved scores of 72%, 72%, and 84% on IC\nexam levels P1, P2, and P3, respectively, demonstrating its effectiveness in\nThai financial advisory tasks and its potential for specialized applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18242v1",
    "published_date": "2024-11-27 11:30:00 UTC",
    "updated_date": "2024-11-27 11:30:00 UTC"
  },
  {
    "arxiv_id": "2411.18241v1",
    "title": "Exploration of LLM Multi-Agent Application Implementation Based on LangGraph+CrewAI",
    "authors": [
      "Zhihua Duan",
      "Jialin Wang"
    ],
    "abstract": "With the rapid development of large model technology, the application of\nagent technology in various fields is becoming increasingly widespread,\nprofoundly changing people's work and lifestyles. In complex and dynamic\nsystems, multi-agents achieve complex tasks that are difficult for a single\nagent to complete through division of labor and collaboration among agents.\nThis paper discusses the integrated application of LangGraph and CrewAI.\nLangGraph improves the efficiency of information transmission through graph\narchitecture, while CrewAI enhances team collaboration capabilities and system\nperformance through intelligent task allocation and resource management. The\nmain research contents of this paper are: (1) designing the architecture of\nagents based on LangGraph for precise control; (2) enhancing the capabilities\nof agents based on CrewAI to complete a variety of tasks. This study aims to\ndelve into the application of LangGraph and CrewAI in multi-agent systems,\nproviding new perspectives for the future development of agent technology, and\npromoting technological progress and application innovation in the field of\nlarge model intelligent agents.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18241v1",
    "published_date": "2024-11-27 11:29:17 UTC",
    "updated_date": "2024-11-27 11:29:17 UTC"
  },
  {
    "arxiv_id": "2411.18235v1",
    "title": "Certified Training with Branch-and-Bound: A Case Study on Lyapunov-stable Neural Control",
    "authors": [
      "Zhouxing Shi",
      "Cho-Jui Hsieh",
      "Huan Zhang"
    ],
    "abstract": "We study the problem of learning Lyapunov-stable neural controllers which\nprovably satisfy the Lyapunov asymptotic stability condition within a\nregion-of-attraction. Compared to previous works which commonly used\ncounterexample guided training on this task, we develop a new and generally\nformulated certified training framework named CT-BaB, and we optimize for\ndifferentiable verified bounds, to produce verification-friendly models. In\norder to handle the relatively large region-of-interest, we propose a novel\nframework of training-time branch-and-bound to dynamically maintain a training\ndataset of subregions throughout training, such that the hardest subregions are\niteratively split into smaller ones whose verified bounds can be computed more\ntightly to ease the training. We demonstrate that our new training framework\ncan produce models which can be more efficiently verified at test time. On the\nlargest 2D quadrotor dynamical system, verification for our model is more than\n5X faster compared to the baseline, while our size of region-of-attraction is\n16X larger than the baseline.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2411.18235v1",
    "published_date": "2024-11-27 11:12:46 UTC",
    "updated_date": "2024-11-27 11:12:46 UTC"
  },
  {
    "arxiv_id": "2411.18234v1",
    "title": "Randomized-Grid Search for Hyperparameter Tuning in Decision Tree Model to Improve Performance of Cardiovascular Disease Classification",
    "authors": [
      "Abhay Kumar Pathak",
      "Mrityunjay Chaubey",
      "Manjari Gupta"
    ],
    "abstract": "Cardiovascular disease refers to any critical condition that impacts the\nheart. Because heart diseases can be life-threatening. Researchers are focusing\non designing smart systems to accurately diagnose them based on electronic\nhealth data, with the aid of machine learning algorithms. Heart disease\nclassification using machine learning (ML) algorithms such as Support Vector\nMachine(SVM), Na\\\"ive Bayes(NB), Decision Trees (DTs) and Random Forests (RFs)\nare often hindered by overfitting. These ML algorithms need extensive\nhyperparameter tuning. Random Search offers a faster, and, more efficient\nexploration of hyperparameter space, but, it may overlook optimal regions. Grid\nSearch, though exhaustive, but, it is computationally expensive and\ninefficient, particularly with high-dimensional data. To address these\nlimitations, Randomized-Grid Search, a novel hybrid optimization method is\nproposed that combines the global exploration strengths of Random Search with\nthe focused, and, exhaustive search of Grid Search in the most promising\nregions. This hybrid approach efficiently balances exploration and\nexploitation. The proposed model optimizes the hyperparameter for Decision Tree\nmodel. The proposed model is applied to UCI heart disease dataset for\nclassification. It enhances model performance, provides improved accuracy,\ngeneralization, and computational efficiency. Experimental results demonstrate\nthat Randomized-Grid Search outperforms traditional methods by significant\nmargins. The proposed model provides a more effective solution for machine\nlearning applications in healthcare diagnosis.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.PF",
      "stat.CO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18234v1",
    "published_date": "2024-11-27 11:10:28 UTC",
    "updated_date": "2024-11-27 11:10:28 UTC"
  },
  {
    "arxiv_id": "2411.18230v1",
    "title": "Dependency-Aware CAV Task Scheduling via Diffusion-Based Reinforcement Learning",
    "authors": [
      "Xiang Cheng",
      "Zhi Mao",
      "Ying Wang",
      "Wen Wu"
    ],
    "abstract": "In this paper, we propose a novel dependency-aware task scheduling strategy\nfor dynamic unmanned aerial vehicle-assisted connected autonomous vehicles\n(CAVs). Specifically, different computation tasks of CAVs consisting of\nmultiple dependency subtasks are judiciously assigned to nearby CAVs or the\nbase station for promptly completing tasks. Therefore, we formulate a joint\nscheduling priority and subtask assignment optimization problem with the\nobjective of minimizing the average task completion time. The problem aims at\nimproving the long-term system performance, which is reformulated as a Markov\ndecision process. To solve the problem, we further propose a diffusion-based\nreinforcement learning algorithm, named Synthetic DDQN based Subtasks\nScheduling, which can make adaptive task scheduling decision in real time. A\ndiffusion model-based synthetic experience replay is integrated into the\nreinforcement learning framework, which can generate sufficient synthetic data\nin experience replay buffer, thereby significantly accelerating convergence and\nimproving sample efficiency. Simulation results demonstrate the effectiveness\nof the proposed algorithm on reducing task completion time, comparing to\nbenchmark schemes.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.18230v1",
    "published_date": "2024-11-27 11:07:31 UTC",
    "updated_date": "2024-11-27 11:07:31 UTC"
  },
  {
    "arxiv_id": "2411.18226v1",
    "title": "Feature-Factory: Automating Software Feature Integration Using Generative AI",
    "authors": [
      "Ruslan Idelfonso Magana Vsevolodovna"
    ],
    "abstract": "Integrating new features into existing software projects can be a complex and\ntime-consuming process. Feature-Factory leverages Generative AI with WatsonX.ai\nto automate the analysis, planning, and implementation of feature requests. By\ncombining advanced project parsing, dependency resolution, and AI-generated\ncode, the program ensures seamless integration of features into software\nsystems while maintaining structural integrity. This paper presents the\nmethodology, mathematical model, and results of the Feature-Factory framework.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "68T05, 68N01, 68N30, 68Q25",
      "D.2.3; I.2.2; D.2.7; D.2.9; I.2.7"
    ],
    "primary_category": "cs.SE",
    "comment": "14 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2411.18226v1",
    "published_date": "2024-11-27 11:03:47 UTC",
    "updated_date": "2024-11-27 11:03:47 UTC"
  },
  {
    "arxiv_id": "2411.18225v1",
    "title": "PATHS: A Hierarchical Transformer for Efficient Whole Slide Image Analysis",
    "authors": [
      "Zak Buzzard",
      "Konstantin Hemker",
      "Nikola Simidjievski",
      "Mateja Jamnik"
    ],
    "abstract": "Computational analysis of whole slide images (WSIs) has seen significant\nresearch progress in recent years, with applications ranging across important\ndiagnostic and prognostic tasks such as survival or cancer subtype prediction.\nMany state-of-the-art models process the entire slide - which may be as large\nas $150,000 \\times 150,000$ pixels - as a bag of many patches, the size of\nwhich necessitates computationally cheap feature aggregation methods. However,\na large proportion of these patches are uninformative, such as those containing\nonly healthy or adipose tissue, adding significant noise and size to the bag.\nWe propose Pathology Transformer with Hierarchical Selection (PATHS), a novel\ntop-down method for hierarchical weakly supervised representation learning on\nslide-level tasks in computational pathology. PATHS is inspired by the\ncross-magnification manner in which a human pathologist examines a slide,\nrecursively filtering patches at each magnification level to a small subset\nrelevant to the diagnosis. Our method overcomes the complications of processing\nthe entire slide, enabling quadratic self-attention and providing a simple\ninterpretable measure of region importance. We apply PATHS to five datasets of\nThe Cancer Genome Atlas (TCGA), and achieve superior performance on slide-level\nprediction tasks when compared to previous methods, despite processing only a\nsmall proportion of the slide.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18225v1",
    "published_date": "2024-11-27 11:03:38 UTC",
    "updated_date": "2024-11-27 11:03:38 UTC"
  },
  {
    "arxiv_id": "2411.18220v3",
    "title": "R-MTLLMF: Resilient Multi-Task Large Language Model Fusion at the Wireless Edge",
    "authors": [
      "Aladin Djuhera",
      "Vlad C. Andrei",
      "Mohsen Pourghasemian",
      "Haris Gacanin",
      "Holger Boche",
      "Walid Saad"
    ],
    "abstract": "Multi-task large language models (MTLLMs) are important for many applications\nat the wireless edge, where users demand specialized models to handle multiple\ntasks efficiently. However, training MTLLMs is complex and exhaustive,\nparticularly when tasks are subject to change. Recently, the concept of model\nfusion via task vectors has emerged as an efficient approach for combining\nfine-tuning parameters to produce an MTLLM. In this paper, the problem of\nenabling edge users to collaboratively craft such MTLMs via tasks vectors is\nstudied, under the assumption of worst-case adversarial attacks. To this end,\nfirst the influence of adversarial noise to multi-task model fusion is\ninvestigated and a relationship between the so-called weight disentanglement\nerror and the mean squared error (MSE) is derived. Using hypothesis testing, it\nis directly shown that the MSE increases interference between task vectors,\nthereby rendering model fusion ineffective. Then, a novel resilient MTLLM\nfusion (R-MTLLMF) is proposed, which leverages insights about the LLM\narchitecture and fine-tuning process to safeguard task vector aggregation under\nadversarial noise by realigning the MTLLM. The proposed R-MTLLMF is then\ncompared for both worst-case and ideal transmission scenarios to study the\nimpact of the wireless channel. Extensive model fusion experiments with vision\nLLMs demonstrate R-MTLLMF's effectiveness, achieving close-to-baseline\nperformance across eight different tasks in ideal noise scenarios and\nsignificantly outperforming unprotected model fusion in worst-case scenarios.\nThe results further advocate for additional physical layer protection for a\nholistic approach to resilience, from both a wireless and LLM perspective.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18220v3",
    "published_date": "2024-11-27 10:57:06 UTC",
    "updated_date": "2025-02-21 12:44:48 UTC"
  },
  {
    "arxiv_id": "2411.18212v1",
    "title": "SCoTT: Wireless-Aware Path Planning with Vision Language Models and Strategic Chains-of-Thought",
    "authors": [
      "Aladin Djuhera",
      "Vlad C. Andrei",
      "Amin Seffo",
      "Holger Boche",
      "Walid Saad"
    ],
    "abstract": "Path planning is a complex problem for many practical applications,\nparticularly in robotics. Existing algorithms, however, are exhaustive in\nnature and become increasingly complex when additional side constraints are\nincorporated alongside distance minimization. In this paper, a novel approach\nusing vision language models (VLMs) is proposed for enabling path planning in\ncomplex wireless-aware environments. To this end, insights from a digital twin\n(DT) with real-world wireless ray tracing data are explored in order to\nguarantee an average path gain threshold while minimizing the trajectory\nlength. First, traditional approaches such as A* are compared to several\nwireless-aware extensions, and an optimal iterative dynamic programming\napproach (DP-WA*) is derived, which fully takes into account all path gains and\ndistance metrics within the DT. On the basis of these baselines, the role of\nVLMs as an alternative assistant for path planning is investigated, and a\nstrategic chain-of-thought tasking (SCoTT) approach is proposed. SCoTT divides\nthe complex planning task into several subproblems and solves each with\nadvanced CoT prompting. Results show that SCoTT achieves very close average\npath gains compared to DP-WA* while at the same time yielding consistently\nshorter path lengths. The results also show that VLMs can be used to accelerate\nDP-WA* by efficiently reducing the algorithm's search space and thus saving up\nto 62\\% in execution time. This work underscores the potential of VLMs in\nfuture digital systems as capable assistants for solving complex tasks, while\nenhancing user interaction and accelerating rapid prototyping under diverse\nwireless constraints.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18212v1",
    "published_date": "2024-11-27 10:45:49 UTC",
    "updated_date": "2024-11-27 10:45:49 UTC"
  },
  {
    "arxiv_id": "2411.18211v1",
    "title": "TimeMarker: A Versatile Video-LLM for Long and Short Video Understanding with Superior Temporal Localization Ability",
    "authors": [
      "Shimin Chen",
      "Xiaohan Lan",
      "Yitian Yuan",
      "Zequn Jie",
      "Lin Ma"
    ],
    "abstract": "Rapid development of large language models (LLMs) has significantly advanced\nmultimodal large language models (LMMs), particularly in vision-language tasks.\nHowever, existing video-language models often overlook precise temporal\nlocalization and struggle with videos of varying lengths. We introduce\nTimeMarker, a versatile Video-LLM designed for high-quality dialogue based on\nvideo content, emphasizing temporal localization. TimeMarker integrates\nTemporal Separator Tokens to enhance temporal awareness, accurately marking\nspecific moments within videos. It employs the AnyLength mechanism for dynamic\nframe sampling and adaptive token merging, enabling effective handling of both\nshort and long videos. Additionally, TimeMarker utilizes diverse datasets,\nincluding further transformed temporal-related video QA datasets, to bolster\nits temporal understanding capabilities. Image and interleaved data are also\nemployed to further enhance the model's semantic perception ability.\nEvaluations demonstrate that TimeMarker achieves state-of-the-art performance\nacross multiple benchmarks, excelling in both short and long video categories.\nOur project page is at \\url{https://github.com/TimeMarker-LLM/TimeMarker/}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18211v1",
    "published_date": "2024-11-27 10:45:40 UTC",
    "updated_date": "2024-11-27 10:45:40 UTC"
  },
  {
    "arxiv_id": "2412.03589v1",
    "title": "Human Evaluation of Procedural Knowledge Graph Extraction from Text with Large Language Models",
    "authors": [
      "Valentina Anita Carriero",
      "Antonia Azzini",
      "Ilaria Baroni",
      "Mario Scrocca",
      "Irene Celino"
    ],
    "abstract": "Procedural Knowledge is the know-how expressed in the form of sequences of\nsteps needed to perform some tasks. Procedures are usually described by means\nof natural language texts, such as recipes or maintenance manuals, possibly\nspread across different documents and systems, and their interpretation and\nsubsequent execution is often left to the reader. Representing such procedures\nin a Knowledge Graph (KG) can be the basis to build digital tools to support\nthose users who need to apply or execute them. In this paper, we leverage Large\nLanguage Model (LLM) capabilities and propose a prompt engineering approach to\nextract steps, actions, objects, equipment and temporal information from a\ntextual procedure, in order to populate a Procedural KG according to a\npre-defined ontology. We evaluate the KG extraction results by means of a user\nstudy, in order to qualitatively and quantitatively assess the perceived\nquality and usefulness of the LLM-extracted procedural knowledge. We show that\nLLMs can produce outputs of acceptable quality and we assess the subjective\nperception of AI by human evaluators.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03589v1",
    "published_date": "2024-11-27 10:36:28 UTC",
    "updated_date": "2024-11-27 10:36:28 UTC"
  },
  {
    "arxiv_id": "2411.18207v3",
    "title": "From Open Vocabulary to Open World: Teaching Vision Language Models to Detect Novel Objects",
    "authors": [
      "Zizhao Li",
      "Zhengkang Xiang",
      "Joseph West",
      "Kourosh Khoshelham"
    ],
    "abstract": "Traditional object detection methods operate under the closed-set assumption,\nwhere models can only detect a fixed number of objects predefined in the\ntraining set. Recent works on open vocabulary object detection (OVD) enable the\ndetection of objects defined by an in-principle unbounded vocabulary, which\nreduces the cost of training models for specific tasks. However, OVD heavily\nrelies on accurate prompts provided by an ``oracle'', which limits their use in\ncritical applications such as driving scene perception. OVD models tend to\nmisclassify near-out-of-distribution (NOOD) objects that have similar features\nto known classes, and ignore far-out-of-distribution (FOOD) objects. To address\nthese limitations, we propose a framework that enables OVD models to operate in\nopen world settings, by identifying and incrementally learning previously\nunseen objects. To detect FOOD objects, we propose Open World Embedding\nLearning (OWEL) and introduce the concept of Pseudo Unknown Embedding which\ninfers the location of unknown classes in a continuous semantic space based on\nthe information of known classes. We also propose Multi-Scale Contrastive\nAnchor Learning (MSCAL), which enables the identification of misclassified\nunknown objects by promoting the intra-class consistency of object embeddings\nat different scales. The proposed method achieves state-of-the-art performance\non standard open world object detection and autonomous driving benchmarks while\nmaintaining its open vocabulary object detection capability.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18207v3",
    "published_date": "2024-11-27 10:33:51 UTC",
    "updated_date": "2025-03-21 03:09:27 UTC"
  },
  {
    "arxiv_id": "2411.18201v1",
    "title": "Learning for Long-Horizon Planning via Neuro-Symbolic Abductive Imitation",
    "authors": [
      "Jie-Jing Shao",
      "Hao-Ran Hao",
      "Xiao-Wen Yang",
      "Yu-Feng Li"
    ],
    "abstract": "Recent learning-to-imitation methods have shown promising results in planning\nvia imitating within the observation-action space. However, their ability in\nopen environments remains constrained, particularly in long-horizon tasks. In\ncontrast, traditional symbolic planning excels in long-horizon tasks through\nlogical reasoning over human-defined symbolic spaces but struggles to handle\nobservations beyond symbolic states, such as high-dimensional visual inputs\nencountered in real-world scenarios. In this work, we draw inspiration from\nabductive learning and introduce a novel framework \\textbf{AB}ductive\n\\textbf{I}mitation \\textbf{L}earning (ABIL) that integrates the benefits of\ndata-driven learning and symbolic-based reasoning, enabling long-horizon\nplanning. Specifically, we employ abductive reasoning to understand the\ndemonstrations in symbolic space and design the principles of sequential\nconsistency to resolve the conflicts between perception and reasoning. ABIL\ngenerates predicate candidates to facilitate the perception from raw\nobservations to symbolic space without laborious predicate annotations,\nproviding a groundwork for symbolic planning. With the symbolic understanding,\nwe further develop a policy ensemble whose base policies are built with\ndifferent logical objectives and managed through symbolic reasoning.\nExperiments show that our proposal successfully understands the observations\nwith the task-relevant symbolics to assist the imitation learning. Importantly,\nABIL demonstrates significantly improved data efficiency and generalization\nacross various long-horizon tasks, highlighting it as a promising solution for\nlong-horizon planning. Project website:\n\\url{https://www.lamda.nju.edu.cn/shaojj/KDD25_ABIL/}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by KDD2025. The KDD version is titled ''Abductive Learning\n  for Neuro-Symbolic Grounded Imitation''",
    "pdf_url": "http://arxiv.org/pdf/2411.18201v1",
    "published_date": "2024-11-27 10:26:14 UTC",
    "updated_date": "2024-11-27 10:26:14 UTC"
  },
  {
    "arxiv_id": "2411.18179v1",
    "title": "Prediction with Action: Visual Policy Learning via Joint Denoising Process",
    "authors": [
      "Yanjiang Guo",
      "Yucheng Hu",
      "Jianke Zhang",
      "Yen-Jen Wang",
      "Xiaoyu Chen",
      "Chaochao Lu",
      "Jianyu Chen"
    ],
    "abstract": "Diffusion models have demonstrated remarkable capabilities in image\ngeneration tasks, including image editing and video creation, representing a\ngood understanding of the physical world. On the other line, diffusion models\nhave also shown promise in robotic control tasks by denoising actions, known as\ndiffusion policy. Although the diffusion generative model and diffusion policy\nexhibit distinct capabilities--image prediction and robotic action,\nrespectively--they technically follow a similar denoising process. In robotic\ntasks, the ability to predict future images and generate actions is highly\ncorrelated since they share the same underlying dynamics of the physical world.\nBuilding on this insight, we introduce PAD, a novel visual policy learning\nframework that unifies image Prediction and robot Action within a joint\nDenoising process. Specifically, PAD utilizes Diffusion Transformers (DiT) to\nseamlessly integrate images and robot states, enabling the simultaneous\nprediction of future images and robot actions. Additionally, PAD supports\nco-training on both robotic demonstrations and large-scale video datasets and\ncan be easily extended to other robotic modalities, such as depth images. PAD\noutperforms previous methods, achieving a significant 26.3% relative\nimprovement on the full Metaworld benchmark, by utilizing a single\ntext-conditioned visual policy within a data-efficient imitation learning\nsetting. Furthermore, PAD demonstrates superior generalization to unseen tasks\nin real-world robot manipulation settings with 28.0% success rate increase\ncompared to the strongest baseline. Project page at\nhttps://sites.google.com/view/pad-paper",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.18179v1",
    "published_date": "2024-11-27 09:54:58 UTC",
    "updated_date": "2024-11-27 09:54:58 UTC"
  },
  {
    "arxiv_id": "2412.06809v1",
    "title": "Generating Diverse Synthetic Datasets for Evaluation of Real-life Recommender Systems",
    "authors": [
      "Miha Malenšek",
      "Blaž Škrlj",
      "Blaž Mramor",
      "Jure Demšar"
    ],
    "abstract": "Synthetic datasets are important for evaluating and testing machine learning\nmodels. When evaluating real-life recommender systems, high-dimensional\ncategorical (and sparse) datasets are often considered. Unfortunately, there\nare not many solutions that would allow generation of artificial datasets with\nsuch characteristics. For that purpose, we developed a novel framework for\ngenerating synthetic datasets that are diverse and statistically coherent. Our\nframework allows for creation of datasets with controlled attributes, enabling\niterative modifications to fit specific experimental needs, such as introducing\ncomplex feature interactions, feature cardinality, or specific distributions.\nWe demonstrate the framework's utility through use cases such as benchmarking\nprobabilistic counting algorithms, detecting algorithmic bias, and simulating\nAutoML searches. Unlike existing methods that either focus narrowly on specific\ndataset structures, or prioritize (private) data synthesis through real data,\nour approach provides a modular means to quickly generating completely\nsynthetic datasets we can tailor to diverse experimental requirements. Our\nresults show that the framework effectively isolates model behavior in unique\nsituations and highlights its potential for significant advancements in the\nevaluation and development of recommender systems. The readily-available\nframework is available as a free open Python package to facilitate research\nwith minimal friction.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "RecSys 2024'",
    "pdf_url": "http://arxiv.org/pdf/2412.06809v1",
    "published_date": "2024-11-27 09:53:14 UTC",
    "updated_date": "2024-11-27 09:53:14 UTC"
  },
  {
    "arxiv_id": "2411.18659v1",
    "title": "DHCP: Detecting Hallucinations by Cross-modal Attention Pattern in Large Vision-Language Models",
    "authors": [
      "Yudong Zhang",
      "Ruobing Xie",
      "Jiansheng Chen",
      "Xingwu Sun",
      "Zhanhui kang",
      "Yu Wang"
    ],
    "abstract": "Large vision-language models (LVLMs) have demonstrated exceptional\nperformance on complex multimodal tasks. However, they continue to suffer from\nsignificant hallucination issues, including object, attribute, and relational\nhallucinations. To accurately detect these hallucinations, we investigated the\nvariations in cross-modal attention patterns between hallucination and\nnon-hallucination states. Leveraging these distinctions, we developed a\nlightweight detector capable of identifying hallucinations. Our proposed\nmethod, Detecting Hallucinations by Cross-modal Attention Patterns (DHCP), is\nstraightforward and does not require additional LVLM training or extra LVLM\ninference steps. Experimental results show that DHCP achieves remarkable\nperformance in hallucination detection. By offering novel insights into the\nidentification and analysis of hallucinations in LVLMs, DHCP contributes to\nadvancing the reliability and trustworthiness of these models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.18659v1",
    "published_date": "2024-11-27 09:43:09 UTC",
    "updated_date": "2024-11-27 09:43:09 UTC"
  },
  {
    "arxiv_id": "2411.18169v1",
    "title": "PDZSeg: Adapting the Foundation Model for Dissection Zone Segmentation with Visual Prompts in Robot-assisted Endoscopic Submucosal Dissection",
    "authors": [
      "Mengya Xu",
      "Wenjin Mo",
      "Guankun Wang",
      "Huxin Gao",
      "An Wang",
      "Zhen Li",
      "Xiaoxiao Yang",
      "Hongliang Ren"
    ],
    "abstract": "Purpose: Endoscopic surgical environments present challenges for dissection\nzone segmentation due to unclear boundaries between tissue types, leading to\nsegmentation errors where models misidentify or overlook edges. This study aims\nto provide precise dissection zone suggestions during endoscopic submucosal\ndissection (ESD) procedures, enhancing ESD safety.\n  Methods: We propose the Prompted-based Dissection Zone Segmentation (PDZSeg)\nmodel, designed to leverage diverse visual prompts such as scribbles and\nbounding boxes. By overlaying these prompts onto images and fine-tuning a\nfoundational model on a specialized dataset, our approach improves segmentation\nperformance and user experience through flexible input methods.\n  Results: The PDZSeg model was validated using three experimental setups:\nin-domain evaluation, variability in visual prompt availability, and robustness\nassessment. Using the ESD-DZSeg dataset, results show that our method\noutperforms state-of-the-art segmentation approaches. This is the first study\nto integrate visual prompt design into dissection zone segmentation.\n  Conclusion: The PDZSeg model effectively utilizes visual prompts to enhance\nsegmentation performance and user experience, supported by the novel ESD-DZSeg\ndataset as a benchmark for dissection zone segmentation in ESD. Our work\nestablishes a foundation for future research.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18169v1",
    "published_date": "2024-11-27 09:28:50 UTC",
    "updated_date": "2024-11-27 09:28:50 UTC"
  },
  {
    "arxiv_id": "2411.18158v1",
    "title": "Abductive Symbolic Solver on Abstraction and Reasoning Corpus",
    "authors": [
      "Mintaek Lim",
      "Seokki Lee",
      "Liyew Woletemaryam Abitew",
      "Sundong Kim"
    ],
    "abstract": "This paper addresses the challenge of enhancing artificial intelligence\nreasoning capabilities, focusing on logicality within the Abstraction and\nReasoning Corpus (ARC). Humans solve such visual reasoning tasks based on their\nobservations and hypotheses, and they can explain their solutions with a proper\nreason. However, many previous approaches focused only on the grid transition\nand it is not enough for AI to provide reasonable and human-like solutions. By\nconsidering the human process of solving visual reasoning tasks, we have\nconcluded that the thinking process is likely the abductive reasoning process.\nThus, we propose a novel framework that symbolically represents the observed\ndata into a knowledge graph and extracts core knowledge that can be used for\nsolution generation. This information limits the solution search space and\nhelps provide a reasonable mid-process. Our approach holds promise for\nimproving AI performance on ARC tasks by effectively narrowing the solution\nspace and providing logical solutions grounded in core knowledge extraction.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Presented at IJCAI 2024 LNSAI Workshop",
    "pdf_url": "http://arxiv.org/pdf/2411.18158v1",
    "published_date": "2024-11-27 09:09:00 UTC",
    "updated_date": "2024-11-27 09:09:00 UTC"
  },
  {
    "arxiv_id": "2411.18157v1",
    "title": "A survey on cutting-edge relation extraction techniques based on language models",
    "authors": [
      "Jose A. Diaz-Garcia",
      "Julio Amador Diaz Lopez"
    ],
    "abstract": "This comprehensive survey delves into the latest advancements in Relation\nExtraction (RE), a pivotal task in natural language processing essential for\napplications across biomedical, financial, and legal sectors. This study\nhighlights the evolution and current state of RE techniques by analyzing 137\npapers presented at the Association for Computational Linguistics (ACL)\nconferences over the past four years, focusing on models that leverage language\nmodels. Our findings underscore the dominance of BERT-based methods in\nachieving state-of-the-art results for RE while also noting the promising\ncapabilities of emerging large language models (LLMs) like T5, especially in\nfew-shot relation extraction scenarios where they excel in identifying\npreviously unseen relations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "50 pages, under review in Artificial Intelligence Review",
    "pdf_url": "http://arxiv.org/pdf/2411.18157v1",
    "published_date": "2024-11-27 09:04:47 UTC",
    "updated_date": "2024-11-27 09:04:47 UTC"
  },
  {
    "arxiv_id": "2411.18141v1",
    "title": "Predicting Water Quality using Quantum Machine Learning: The Case of the Umgeni Catchment (U20A) Study Region",
    "authors": [
      "Muhammad Al-Zafar Khan",
      "Jamal Al-Karaki",
      "Marwan Omar"
    ],
    "abstract": "In this study, we consider a real-world application of QML techniques to\nstudy water quality in the U20A region in Durban, South Africa. Specifically,\nwe applied the quantum support vector classifier (QSVC) and quantum neural\nnetwork (QNN), and we showed that the QSVC is easier to implement and yields a\nhigher accuracy. The QSVC models were applied for three kernels: Linear,\npolynomial, and radial basis function (RBF), and it was shown that the\npolynomial and RBF kernels had exactly the same performance. The QNN model was\napplied using different optimizers, learning rates, noise on the circuit\ncomponents, and weight initializations were considered, but the QNN\npersistently ran into the dead neuron problem. Thus, the QNN was compared only\nby accraucy and loss, and it was shown that with the Adam optimizer, the model\nhas the best performance, however, still less than the QSVC.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "13 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.18141v1",
    "published_date": "2024-11-27 08:43:07 UTC",
    "updated_date": "2024-11-27 08:43:07 UTC"
  },
  {
    "arxiv_id": "2411.18657v1",
    "title": "ScaleViz: Scaling Visualization Recommendation Models on Large Data",
    "authors": [
      "Ghazi Shazan Ahmad",
      "Shubham Agarwal",
      "Subrata Mitra",
      "Ryan Rossi",
      "Manav Doshi",
      "Vibhor Porwal",
      "Syam Manoj Kumar Paila"
    ],
    "abstract": "Automated visualization recommendations (vis-rec) help users to derive\ncrucial insights from new datasets. Typically, such automated vis-rec models\nfirst calculate a large number of statistics from the datasets and then use\nmachine-learning models to score or classify multiple visualizations choices to\nrecommend the most effective ones, as per the statistics. However, state-of-the\nart models rely on very large number of expensive statistics and therefore\nusing such models on large datasets become infeasible due to prohibitively\nlarge computational time, limiting the effectiveness of such techniques to most\nreal world complex and large datasets. In this paper, we propose a novel\nreinforcement-learning (RL) based framework that takes a given vis-rec model\nand a time-budget from the user and identifies the best set of input statistics\nthat would be most effective while generating the visual insights within a\ngiven time budget, using the given model. Using two state-of-the-art vis-rec\nmodels applied on three large real-world datasets, we show the effectiveness of\nour technique in significantly reducing time-to visualize with very small\namount of introduced error. Our approach is about 10X times faster compared to\nthe baseline approaches that introduce similar amounts of error.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at PAKDD 2024 (Oral)",
    "pdf_url": "http://arxiv.org/pdf/2411.18657v1",
    "published_date": "2024-11-27 08:43:06 UTC",
    "updated_date": "2024-11-27 08:43:06 UTC"
  },
  {
    "arxiv_id": "2411.18138v1",
    "title": "SALMONN-omni: A Codec-free LLM for Full-duplex Speech Understanding and Generation",
    "authors": [
      "Wenyi Yu",
      "Siyin Wang",
      "Xiaoyu Yang",
      "Xianzhao Chen",
      "Xiaohai Tian",
      "Jun Zhang",
      "Guangzhi Sun",
      "Lu Lu",
      "Yuxuan Wang",
      "Chao Zhang"
    ],
    "abstract": "Full-duplex multimodal large language models (LLMs) provide a unified\nframework for addressing diverse speech understanding and generation tasks,\nenabling more natural and seamless human-machine conversations. Unlike\ntraditional modularised conversational AI systems, which separate speech\nrecognition, understanding, and text-to-speech generation into distinct\ncomponents, multimodal LLMs operate as single end-to-end models. This\nstreamlined design eliminates error propagation across components and fully\nleverages the rich non-verbal information embedded in input speech signals. We\nintroduce SALMONN-omni, a codec-free, full-duplex speech understanding and\ngeneration model capable of simultaneously listening to its own generated\nspeech and background sounds while speaking. To support this capability, we\npropose a novel duplex spoken dialogue framework incorporating a ``thinking''\nmechanism that facilitates asynchronous text and speech generation relying on\nembeddings instead of codecs (quantized speech and audio tokens). Experimental\nresults demonstrate SALMONN-omni's versatility across a broad range of\nstreaming speech tasks, including speech recognition, speech enhancement, and\nspoken question answering. Additionally, SALMONN-omni excels at managing\nturn-taking, barge-in, and echo cancellation scenarios, establishing its\npotential as a robust prototype for full-duplex conversational AI systems. To\nthe best of our knowledge, SALMONN-omni is the first codec-free model of its\nkind. A full technical report along with model checkpoints will be released\nsoon.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Technical report",
    "pdf_url": "http://arxiv.org/pdf/2411.18138v1",
    "published_date": "2024-11-27 08:38:57 UTC",
    "updated_date": "2024-11-27 08:38:57 UTC"
  },
  {
    "arxiv_id": "2411.18656v1",
    "title": "The Return of Pseudosciences in Artificial Intelligence: Have Machine Learning and Deep Learning Forgotten Lessons from Statistics and History?",
    "authors": [
      "Jérémie Sublime"
    ],
    "abstract": "In today's world, AI programs powered by Machine Learning are ubiquitous, and\nhave achieved seemingly exceptional performance across a broad range of tasks,\nfrom medical diagnosis and credit rating in banking, to theft detection via\nvideo analysis, and even predicting political or sexual orientation from facial\nimages. These predominantly deep learning methods excel due to their\nextraordinary capacity to process vast amounts of complex data to extract\ncomplex correlations and relationship from different levels of features.\n  In this paper, we contend that the designers and final users of these ML\nmethods have forgotten a fundamental lesson from statistics: correlation does\nnot imply causation. Not only do most state-of-the-art methods neglect this\ncrucial principle, but by doing so they often produce nonsensical or flawed\ncausal models, akin to social astrology or physiognomy. Consequently, we argue\nthat current efforts to make AI models more ethical by merely reducing biases\nin the training data are insufficient. Through examples, we will demonstrate\nthat the potential for harm posed by these methods can only be mitigated by a\ncomplete rethinking of their core models, improved quality assessment metrics\nand policies, and by maintaining humans oversight throughout the process.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18656v1",
    "published_date": "2024-11-27 08:23:23 UTC",
    "updated_date": "2024-11-27 08:23:23 UTC"
  },
  {
    "arxiv_id": "2411.18104v3",
    "title": "Training and Evaluating Language Models with Template-based Data Generation",
    "authors": [
      "Yifan Zhang"
    ],
    "abstract": "The rapid advancement of large language models (LLMs) such as GPT-3, PaLM,\nand Llama has significantly transformed natural language processing, showcasing\nremarkable capabilities in understanding and generating language. However,\nthese models often struggle with tasks requiring complex reasoning,\nparticularly in mathematical problem-solving, due in part to the scarcity of\nlarge-scale, high-quality, domain-specific datasets necessary for training\nsophisticated reasoning abilities. To address this limitation, we introduce\nTemplate-based Data Generation (TDG), a novel approach that leverages LLMs\n(GPT-4) to automatically generate parameterized meta-templates, which are then\nused to synthesize a vast array of high-quality problems and solutions.\nLeveraging TDG, we create TemplateMath Part I: TemplateGSM, a dataset\ncomprising over 7 million synthetically generated grade school math\nproblems--each accompanied by code-based and natural language solutions--with\nthe potential to generate an effectively unlimited number more. This dataset\nalleviates the scarcity of large-scale mathematical datasets and serves as a\nvaluable resource for pre-training, fine-tuning, and evaluating LLMs in\nmathematical reasoning. Our method not only enables the generation of virtually\ninfinite data but also elevates data augmentation to a new level by using GPT-4\nfor meta-template generation, ensuring diverse and high-quality problem\nstructures. The TemplateMath Part I: TemplateGSM dataset is publicly available\nat https://huggingface.co/datasets/math-ai/TemplateGSM. The code is available\nat https://github.com/iiis-ai/TemplateMath.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.18104v3",
    "published_date": "2024-11-27 07:32:56 UTC",
    "updated_date": "2025-03-08 01:18:23 UTC"
  },
  {
    "arxiv_id": "2411.18095v1",
    "title": "Derivation of Closed Form of Expected Improvement for Gaussian Process Trained on Log-Transformed Objective",
    "authors": [
      "Shuhei Watanabe"
    ],
    "abstract": "Expected Improvement (EI) is arguably the most widely used acquisition\nfunction in Bayesian optimization. However, it is often challenging to enhance\nthe performance with EI due to its sensitivity to numerical precision.\nPreviously, Hutter et al. (2009) tackled this problem by using Gaussian process\ntrained on the log-transformed objective function and it was reported that this\ntrick improves the predictive accuracy of GP, leading to substantially better\nperformance. Although Hutter et al. (2009) offered the closed form of their EI,\nits intermediate derivation has not been provided so far. In this paper, we\ngive a friendly derivation of their proposition.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18095v1",
    "published_date": "2024-11-27 07:13:41 UTC",
    "updated_date": "2024-11-27 07:13:41 UTC"
  },
  {
    "arxiv_id": "2411.18085v1",
    "title": "MONOPOLY: Learning to Price Public Facilities for Revaluing Private Properties with Large-Scale Urban Data",
    "authors": [
      "Miao Fan",
      "Jizhou Huang",
      "An Zhuo",
      "Ying Li",
      "Ping Li",
      "Haifeng Wang"
    ],
    "abstract": "The value assessment of private properties is an attractive but challenging\ntask which is widely concerned by a majority of people around the world. A\nprolonged topic among us is ``\\textit{how much is my house worth?}''. To answer\nthis question, most experienced agencies would like to price a property given\nthe factors of its attributes as well as the demographics and the public\nfacilities around it. However, no one knows the exact prices of these factors,\nespecially the values of public facilities which may help assess private\nproperties. In this paper, we introduce our newly launched project ``Monopoly''\n(named after a classic board game) in which we propose a distributed approach\nfor revaluing private properties by learning to price public facilities (such\nas hospitals etc.) with the large-scale urban data we have accumulated via\nBaidu Maps. To be specific, our method organizes many points of interest (POIs)\ninto an undirected weighted graph and formulates multiple factors including the\nvirtual prices of surrounding public facilities as adaptive variables to\nparallelly estimate the housing prices we know. Then the prices of both public\nfacilities and private properties can be iteratively updated according to the\nloss of prediction until convergence. We have conducted extensive experiments\nwith the large-scale urban data of several metropolises in China. Results show\nthat our approach outperforms several mainstream methods with significant\nmargins. Further insights from more in-depth discussions demonstrate that the\n``Monopoly'' is an innovative application in the interdisciplinary field of\nbusiness intelligence and urban computing, and it will be beneficial to tens of\nmillions of our users for investments and to the governments for urban planning\nas well as taxation.",
    "categories": [
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "CIKM'19",
    "pdf_url": "http://arxiv.org/pdf/2411.18085v1",
    "published_date": "2024-11-27 06:44:41 UTC",
    "updated_date": "2024-11-27 06:44:41 UTC"
  },
  {
    "arxiv_id": "2411.18084v1",
    "title": "From Exploration to Revelation: Detecting Dark Patterns in Mobile Apps",
    "authors": [
      "Jieshan Chen",
      "Zhen Wang",
      "Jiamou Sun",
      "Wenbo Zou",
      "Zhenchang Xing",
      "Qinghua Lu",
      "Qing Huang",
      "Xiwei Xu"
    ],
    "abstract": "Mobile apps are essential in daily life, yet they often employ dark patterns,\nsuch as visual tricks to highlight certain options or linguistic tactics to nag\nusers into making purchases, to manipulate user behavior. Current research\nmainly uses manual methods to detect dark patterns, a process that is\ntime-consuming and struggles to keep pace with continually updating and\nemerging apps. While some studies targeted at automated detection, they are\nconstrained to static patterns and still necessitate manual app exploration. To\nbridge these gaps, we present AppRay, an innovative system that seamlessly\nblends task-oriented app exploration with automated dark pattern detection,\nreducing manual efforts. Our approach consists of two steps: First, we harness\nthe commonsense knowledge of large language models for targeted app\nexploration, supplemented by traditional random exploration to capture a\nbroader range of UI states. Second, we developed a static and dynamic dark\npattern detector powered by a contrastive learning-based multi-label classifier\nand a rule-based refiner to perform detection. We contributed two datasets,\nAppRay-Dark and AppRay-Light, with 2,185 unique deceptive patterns (including\n149 dynamic instances) across 18 types from 876 UIs and 871 benign UIs. These\ndatasets cover both static and dynamic dark patterns while preserving UI\nrelationships. Experimental results confirm that AppRay can efficiently explore\nthe app and identify a wide range of dark patterns with great performance.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC",
      "D.2; I.2; H.5"
    ],
    "primary_category": "cs.SE",
    "comment": "12 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.18084v1",
    "published_date": "2024-11-27 06:39:35 UTC",
    "updated_date": "2024-11-27 06:39:35 UTC"
  },
  {
    "arxiv_id": "2411.18073v1",
    "title": "DuMapper: Towards Automatic Verification of Large-Scale POIs with Street Views at Baidu Maps",
    "authors": [
      "Miao Fan",
      "Jizhou Huang",
      "Haifeng Wang"
    ],
    "abstract": "With the increased popularity of mobile devices, Web mapping services have\nbecome an indispensable tool in our daily lives. To provide user-satisfied\nservices, such as location searches, the point of interest (POI) database is\nthe fundamental infrastructure, as it archives multimodal information on\nbillions of geographic locations closely related to people's lives, such as a\nshop or a bank. Therefore, verifying the correctness of a large-scale POI\ndatabase is vital. To achieve this goal, many industrial companies adopt\nvolunteered geographic information (VGI) platforms that enable thousands of\ncrowdworkers and expert mappers to verify POIs seamlessly; but to do so, they\nhave to spend millions of dollars every year. To save the tremendous labor\ncosts, we devised DuMapper, an automatic system for large-scale POI\nverification with the multimodal street-view data at Baidu Maps. DuMapper takes\nthe signboard image and the coordinates of a real-world place as input to\ngenerate a low-dimensional vector, which can be leveraged by ANN algorithms to\nconduct a more accurate search through billions of archived POIs in the\ndatabase for verification within milliseconds. It can significantly increase\nthe throughput of POI verification by $50$ times. DuMapper has already been\ndeployed in production since \\DuMPOnline, which dramatically improves the\nproductivity and efficiency of POI verification at Baidu Maps. As of December\n31, 2021, it has enacted over $405$ million iterations of POI verification\nwithin a 3.5-year period, representing an approximate workload of $800$\nhigh-performance expert mappers.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18073v1",
    "published_date": "2024-11-27 05:54:33 UTC",
    "updated_date": "2024-11-27 05:54:33 UTC"
  },
  {
    "arxiv_id": "2411.18071v1",
    "title": "Simulating Tabular Datasets through LLMs to Rapidly Explore Hypotheses about Real-World Entities",
    "authors": [
      "Miguel Zabaleta",
      "Joel Lehman"
    ],
    "abstract": "Do horror writers have worse childhoods than other writers? Though\nbiographical details are known about many writers, quantitatively exploring\nsuch a qualitative hypothesis requires significant human effort, e.g. to sift\nthrough many biographies and interviews of writers and to iteratively search\nfor quantitative features that reflect what is qualitatively of interest. This\npaper explores the potential to quickly prototype these kinds of hypotheses\nthrough (1) applying LLMs to estimate properties of concrete entities like\nspecific people, companies, books, kinds of animals, and countries; (2)\nperforming off-the-shelf analysis methods to reveal possible relationships\namong such properties (e.g. linear regression); and towards further automation,\n(3) applying LLMs to suggest the quantitative properties themselves that could\nhelp ground a particular qualitative hypothesis (e.g. number of adverse\nchildhood events, in the context of the running example). The hope is to allow\nsifting through hypotheses more quickly through collaboration between human and\nmachine. Our experiments highlight that indeed, LLMs can serve as useful\nestimators of tabular data about specific entities across a range of domains,\nand that such estimations improve with model scale. Further, initial\nexperiments demonstrate the potential of LLMs to map a qualitative hypothesis\nof interest to relevant concrete variables that the LLM can then estimate. The\nconclusion is that LLMs offer intriguing potential to help illuminate\nscientifically interesting patterns latent within the internet-scale data they\nare trained upon.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18071v1",
    "published_date": "2024-11-27 05:48:44 UTC",
    "updated_date": "2024-11-27 05:48:44 UTC"
  },
  {
    "arxiv_id": "2411.18068v2",
    "title": "PersonaCraft: Personalized and Controllable Full-Body Multi-Human Scene Generation Using Occlusion-Aware 3D-Conditioned Diffusion",
    "authors": [
      "Gwanghyun Kim",
      "Suh Yoon Jeon",
      "Seunggyu Lee",
      "Se Young Chun"
    ],
    "abstract": "We present PersonaCraft, a framework for controllable and occlusion-robust\nfull-body personalized image synthesis of multiple individuals in complex\nscenes. Current methods struggle with occlusion-heavy scenarios and complete\nbody personalization, as 2D pose conditioning lacks 3D geometry, often leading\nto ambiguous occlusions and anatomical distortions, and many approaches focus\nsolely on facial identity. In contrast, our PersonaCraft integrates diffusion\nmodels with 3D human modeling, employing SMPLx-ControlNet, to utilize 3D\ngeometry like depth and normal maps for robust 3D-aware pose conditioning and\nenhanced anatomical coherence. To handle fine-grained occlusions, we propose\nOcclusion Boundary Enhancer Network that exploits depth edge signals with\nocclusion-focused training, and Occlusion-Aware Classifier-Free Guidance\nstrategy that selectively reinforces conditioning in occluded regions without\naffecting unoccluded areas. PersonaCraft can seamlessly be combined with Face\nIdentity ControlNet, achieving full-body multi-human personalization and thus\nmarking a significant advancement beyond prior approaches that concentrate only\non facial identity. Our dual-pathway body shape representation with SMPLx-based\nshape parameters and textual refinement, enables precise full-body\npersonalization and flexible user-defined body shape adjustments. Extensive\nquantitative experiments and user studies demonstrate that PersonaCraft\nsignificantly outperforms existing methods in generating high-quality,\nmulti-person images with accurate personalization and robust occlusion\nhandling.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://gwang-kim.github.io/persona_craft",
    "pdf_url": "http://arxiv.org/pdf/2411.18068v2",
    "published_date": "2024-11-27 05:41:15 UTC",
    "updated_date": "2025-03-14 02:05:11 UTC"
  },
  {
    "arxiv_id": "2411.18653v1",
    "title": "PRSI: Privacy-Preserving Recommendation Model Based on Vector Splitting and Interactive Protocols",
    "authors": [
      "Xiaokai Cao",
      "Wenjin Mo",
      "Zhenyu He",
      "Changdong Wang"
    ],
    "abstract": "With the development of the internet, recommending interesting products to\nusers has become a highly valuable research topic for businesses.\nRecommendation systems play a crucial role in addressing this issue. To prevent\nthe leakage of each user's (client's) private data, Federated Recommendation\nSystems (FedRec) have been proposed and widely used. However, extensive\nresearch has shown that FedRec suffers from security issues such as data\nprivacy leakage, and it is challenging to train effective models with FedRec\nwhen each client only holds interaction information for a single user. To\naddress these two problems, this paper proposes a new privacy-preserving\nrecommendation system (PRSI), which includes a preprocessing module and two\nmain phases. The preprocessing module employs split vectors and fake\ninteraction items to protect clients' interaction information and\nrecommendation results. The two main phases are: (1) the collection of\ninteraction information and (2) the sending of recommendation results. In the\ninteraction information collection phase, each client uses the preprocessing\nmodule and random communication methods (according to the designed interactive\nprotocol) to protect their ID information and IP addresses. In the\nrecommendation results sending phase, the central server uses the preprocessing\nmodule and triplets to distribute recommendation results to each client under\nsecure conditions, following the designed interactive protocol. Finally, we\nconducted multiple sets of experiments to verify the security, accuracy, and\ncommunication cost of the proposed method.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18653v1",
    "published_date": "2024-11-27 05:14:15 UTC",
    "updated_date": "2024-11-27 05:14:15 UTC"
  },
  {
    "arxiv_id": "2411.18050v1",
    "title": "RL for Mitigating Cascading Failures: Targeted Exploration via Sensitivity Factors",
    "authors": [
      "Anmol Dwivedi",
      "Ali Tajer",
      "Santiago Paternain",
      "Nurali Virani"
    ],
    "abstract": "Electricity grid's resiliency and climate change strongly impact one another\ndue to an array of technical and policy-related decisions that impact both.\nThis paper introduces a physics-informed machine learning-based framework to\nenhance grid's resiliency. Specifically, when encountering disruptive events,\nthis paper designs remedial control actions to prevent blackouts. The proposed\nPhysics-Guided Reinforcement Learning (PG-RL) framework determines effective\nreal-time remedial line-switching actions, considering their impact on power\nbalance, system security, and grid reliability. To identify an effective\nblackout mitigation policy, PG-RL leverages power-flow sensitivity factors to\nguide the RL exploration during agent training. Comprehensive evaluations using\nthe Grid2Op platform demonstrate that incorporating physical signals into RL\nsignificantly improves resource utilization within electric grids and achieves\nbetter blackout mitigation policies - both of which are critical in addressing\nclimate change.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18050v1",
    "published_date": "2024-11-27 04:34:31 UTC",
    "updated_date": "2024-11-27 04:34:31 UTC"
  },
  {
    "arxiv_id": "2411.18043v1",
    "title": "Heterogeneous Relationships of Subjects and Shapelets for Semi-supervised Multivariate Series Classification",
    "authors": [
      "Mingsen Du",
      "Meng Chen",
      "Yongjian Li",
      "Cun Ji",
      "Shoushui Wei"
    ],
    "abstract": "Multivariate time series (MTS) classification is widely applied in fields\nsuch as industry, healthcare, and finance, aiming to extract key features from\ncomplex time series data for accurate decision-making and prediction. However,\nexisting methods for MTS often struggle due to the challenges of effectively\nmodeling high-dimensional data and the lack of labeled data, resulting in poor\nclassification performance. To address this issue, we propose a heterogeneous\nrelationships of subjects and shapelets method for semi-supervised MTS\nclassification. This method offers a novel perspective by integrating various\ntypes of additional information while capturing the relationships between them.\nSpecifically, we first utilize a contrast temporal self-attention module to\nobtain sparse MTS representations, and then model the similarities between\nthese representations using soft dynamic time warping to construct a similarity\ngraph. Secondly, we learn the shapelets for different subject types,\nincorporating both the subject features and their shapelets as additional\ninformation to further refine the similarity graph, ultimately generating a\nheterogeneous graph. Finally, we use a dual level graph attention network to\nget prediction. Through this method, we successfully transform dataset into a\nheterogeneous graph, integrating multiple additional information and achieving\nprecise semi-supervised node classification. Experiments on the Human Activity\nRecognition, sleep stage classification and University of East Anglia datasets\ndemonstrate that our method outperforms current state-of-the-art methods in MTS\nclassification tasks, validating its superiority.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to IEEE International Conference on Data Engineering (ICDE)\n  2025",
    "pdf_url": "http://arxiv.org/pdf/2411.18043v1",
    "published_date": "2024-11-27 04:25:13 UTC",
    "updated_date": "2024-11-27 04:25:13 UTC"
  },
  {
    "arxiv_id": "2411.18038v1",
    "title": "VLM-HOI: Vision Language Models for Interpretable Human-Object Interaction Analysis",
    "authors": [
      "Donggoo Kang",
      "Dasol Jeong",
      "Hyunmin Lee",
      "Sangwoo Park",
      "Hasil Park",
      "Sunkyu Kwon",
      "Yeongjoon Kim",
      "Joonki Paik"
    ],
    "abstract": "The Large Vision Language Model (VLM) has recently addressed remarkable\nprogress in bridging two fundamental modalities. VLM, trained by a sufficiently\nlarge dataset, exhibits a comprehensive understanding of both visual and\nlinguistic to perform diverse tasks. To distill this knowledge accurately, in\nthis paper, we introduce a novel approach that explicitly utilizes VLM as an\nobjective function form for the Human-Object Interaction (HOI) detection task\n(\\textbf{VLM-HOI}). Specifically, we propose a method that quantifies the\nsimilarity of the predicted HOI triplet using the Image-Text matching\ntechnique. We represent HOI triplets linguistically to fully utilize the\nlanguage comprehension of VLMs, which are more suitable than CLIP models due to\ntheir localization and object-centric nature. This matching score is used as an\nobjective for contrastive optimization. To our knowledge, this is the first\nutilization of VLM language abilities for HOI detection. Experiments\ndemonstrate the effectiveness of our method, achieving state-of-the-art HOI\ndetection accuracy on benchmarks. We believe integrating VLMs into HOI\ndetection represents important progress towards more advanced and interpretable\nanalysis of human-object interactions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.18038v1",
    "published_date": "2024-11-27 04:13:23 UTC",
    "updated_date": "2024-11-27 04:13:23 UTC"
  },
  {
    "arxiv_id": "2411.18015v1",
    "title": "AEGIS: An Agent-based Framework for General Bug Reproduction from Issue Descriptions",
    "authors": [
      "Xinchen Wang",
      "Pengfei Gao",
      "Xiangxin Meng",
      "Chao Peng",
      "Ruida Hu",
      "Yun Lin",
      "Cuiyun Gao"
    ],
    "abstract": "In software maintenance, bug reproduction is essential for effective fault\nlocalization and repair. Manually writing reproduction scripts is a\ntime-consuming task with high requirements for developers. Hence, automation of\nbug reproduction has increasingly attracted attention from researchers and\npractitioners. However, the existing studies on bug reproduction are generally\nlimited to specific bug types such as program crashes, and hard to be applied\nto general bug reproduction. In this paper, considering the superior\nperformance of agent-based methods in code intelligence tasks, we focus on\ndesigning an agent-based framework for the task. Directly employing agents\nwould lead to limited bug reproduction performance, due to entangled subtasks,\nlengthy retrieved context, and unregulated actions. To mitigate the challenges,\nwe propose an Automated gEneral buG reproductIon Scripts generation framework,\nnamed AEGIS, which is the first agent-based framework for the task. AEGIS\nmainly contains two modules: (1) A concise context construction module, which\naims to guide the code agent in extracting structured information from issue\ndescriptions, identifying issue-related code with detailed explanations, and\nintegrating these elements to construct the concise context; (2) A FSM-based\nmulti-feedback optimization module to further regulate the behavior of the code\nagent within the finite state machine (FSM), ensuring a controlled and\nefficient script generation process based on multi-dimensional feedback.\nExtensive experiments on the public benchmark dataset show that AEGIS\noutperforms the state-of-the-art baseline by 23.0% in F->P metric. In addition,\nthe bug reproduction scripts generated by AEGIS can improve the relative\nresolved rate of Agentless by 12.5%.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18015v1",
    "published_date": "2024-11-27 03:16:47 UTC",
    "updated_date": "2024-11-27 03:16:47 UTC"
  },
  {
    "arxiv_id": "2411.18008v1",
    "title": "Causal and Local Correlations Based Network for Multivariate Time Series Classification",
    "authors": [
      "Mingsen Du",
      "Yanxuan Wei",
      "Xiangwei Zheng",
      "Cun Ji"
    ],
    "abstract": "Recently, time series classification has attracted the attention of a large\nnumber of researchers, and hundreds of methods have been proposed. However,\nthese methods often ignore the spatial correlations among dimensions and the\nlocal correlations among features. To address this issue, the causal and local\ncorrelations based network (CaLoNet) is proposed in this study for multivariate\ntime series classification. First, pairwise spatial correlations between\ndimensions are modeled using causality modeling to obtain the graph structure.\nThen, a relationship extraction network is used to fuse local correlations to\nobtain long-term dependency features. Finally, the graph structure and\nlong-term dependency features are integrated into the graph neural network.\nExperiments on the UEA datasets show that CaLoNet can obtain competitive\nperformance compared with state-of-the-art methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted on April 03, 2023; major revisions on March 25, 2024; minor\n  revisions on July 9, 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.18008v1",
    "published_date": "2024-11-27 02:54:26 UTC",
    "updated_date": "2024-11-27 02:54:26 UTC"
  },
  {
    "arxiv_id": "2411.18003v3",
    "title": "HAAT: Hybrid Attention Aggregation Transformer for Image Super-Resolution",
    "authors": [
      "Song-Jiang Lai",
      "Tsun-Hin Cheung",
      "Ka-Chun Fung",
      "Kai-wen Xue",
      "Kin-Man Lam"
    ],
    "abstract": "In the research area of image super-resolution, Swin-transformer-based models\nare favored for their global spatial modeling and shifting window attention\nmechanism. However, existing methods often limit self-attention to non\noverlapping windows to cut costs and ignore the useful information that exists\nacross channels. To address this issue, this paper introduces a novel model,\nthe Hybrid Attention Aggregation Transformer (HAAT), designed to better\nleverage feature information. HAAT is constructed by integrating\nSwin-Dense-Residual-Connected Blocks (SDRCB) with Hybrid Grid Attention Blocks\n(HGAB). SDRCB expands the receptive field while maintaining a streamlined\narchitecture, resulting in enhanced performance. HGAB incorporates channel\nattention, sparse attention, and window attention to improve nonlocal feature\nfusion and achieve more visually compelling results. Experimental evaluations\ndemonstrate that HAAT surpasses state-of-the-art methods on benchmark datasets.\nKeywords: Image super-resolution, Computer vision, Attention mechanism,\nTransformer",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "6 pages, 2 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2411.18003v3",
    "published_date": "2024-11-27 02:47:17 UTC",
    "updated_date": "2024-12-10 06:39:51 UTC"
  },
  {
    "arxiv_id": "2411.18002v1",
    "title": "An End-to-End Two-Stream Network Based on RGB Flow and Representation Flow for Human Action Recognition",
    "authors": [
      "Song-Jiang Lai",
      "Tsun-Hin Cheung",
      "Ka-Chun Fung",
      "Tian-Shan Liu",
      "Kin-Man Lam"
    ],
    "abstract": "With the rapid advancements in deep learning, computer vision tasks have seen\nsignificant improvements, making two-stream neural networks a popular focus for\nvideo based action recognition. Traditional models using RGB and optical flow\nstreams achieve strong performance but at a high computational cost. To address\nthis, we introduce a representation flow algorithm to replace the optical flow\nbranch in the egocentric action recognition model, enabling end-to-end training\nwhile reducing computational cost and prediction time. Our model, designed for\negocentric action recognition, uses class activation maps (CAMs) to improve\naccuracy and ConvLSTM for spatio temporal encoding with spatial attention. When\nevaluated on the GTEA61, EGTEA GAZE+, and HMDB datasets, our model matches the\naccuracy of the original model on GTEA61 and exceeds it by 0.65% and 0.84% on\nEGTEA GAZE+ and HMDB, respectively. Prediction runtimes are significantly\nreduced to 0.1881s, 0.1503s, and 0.1459s, compared to the original model's\n101.6795s, 25.3799s, and 203.9958s. Ablation studies were also conducted to\nstudy the impact of different parameters on model performance.\n  Keywords: two-stream, egocentric, action recognition, CAM, representation\nflow, CAM, ConvLSTM",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 3 figures, 9 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.18002v1",
    "published_date": "2024-11-27 02:46:46 UTC",
    "updated_date": "2024-11-27 02:46:46 UTC"
  },
  {
    "arxiv_id": "2411.17999v1",
    "title": "A Novel Pareto-optimal Ranking Method for Comparing Multi-objective Optimization Algorithms",
    "authors": [
      "Amin Ibrahim",
      "Azam Asilian Bidgoli",
      "Shahryar Rahnamayan",
      "Kalyanmoy Deb"
    ],
    "abstract": "As the interest in multi- and many-objective optimization algorithms grows,\nthe performance comparison of these algorithms becomes increasingly important.\nA large number of performance indicators for multi-objective optimization\nalgorithms have been introduced, each of which evaluates these algorithms based\non a certain aspect. Therefore, assessing the quality of multi-objective\nresults using multiple indicators is essential to guarantee that the evaluation\nconsiders all quality perspectives. This paper proposes a novel multi-metric\ncomparison method to rank the performance of multi-/ many-objective\noptimization algorithms based on a set of performance indicators. We utilize\nthe Pareto optimality concept (i.e., non-dominated sorting algorithm) to create\nthe rank levels of algorithms by simultaneously considering multiple\nperformance indicators as criteria/objectives. As a result, four different\ntechniques are proposed to rank algorithms based on their contribution at each\nPareto level. This method allows researchers to utilize a set of existing/newly\ndeveloped performance metrics to adequately assess/rank multi-/many-objective\nalgorithms. The proposed methods are scalable and can accommodate in its\ncomprehensive scheme any newly introduced metric. The method was applied to\nrank 10 competing algorithms in the 2018 CEC competition solving 15\nmany-objective test problems. The Pareto-optimal ranking was conducted based on\n10 well-known multi-objective performance indicators and the results were\ncompared to the final ranks reported by the competition, which were based on\nthe inverted generational distance (IGD) and hypervolume indicator (HV)\nmeasures. The techniques suggested in this paper have broad applications in\nscience and engineering, particularly in areas where multiple metrics are used\nfor comparisons. Examples include machine learning and data mining.",
    "categories": [
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17999v1",
    "published_date": "2024-11-27 02:34:54 UTC",
    "updated_date": "2024-11-27 02:34:54 UTC"
  },
  {
    "arxiv_id": "2411.17989v1",
    "title": "Regularized Multi-LLMs Collaboration for Enhanced Score-based Causal Discovery",
    "authors": [
      "Xiaoxuan Li",
      "Yao Liu",
      "Ruoyu Wang",
      "Lina Yao"
    ],
    "abstract": "As the significance of understanding the cause-and-effect relationships among\nvariables increases in the development of modern systems and algorithms,\nlearning causality from observational data has become a preferred and efficient\napproach over conducting randomized control trials. However, purely\nobservational data could be insufficient to reconstruct the true causal graph.\nConsequently, many researchers tried to utilise some form of prior knowledge to\nimprove causal discovery process. In this context, the impressive capabilities\nof large language models (LLMs) have emerged as a promising alternative to the\ncostly acquisition of prior expert knowledge. In this work, we further explore\nthe potential of using LLMs to enhance causal discovery approaches,\nparticularly focusing on score-based methods, and we propose a general\nframework to utilise the capacity of not only one but multiple LLMs to augment\nthe discovery process.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17989v1",
    "published_date": "2024-11-27 01:56:21 UTC",
    "updated_date": "2024-11-27 01:56:21 UTC"
  },
  {
    "arxiv_id": "2411.17983v1",
    "title": "Optimized Conformal Selection: Powerful Selective Inference After Conformity Score Optimization",
    "authors": [
      "Tian Bai",
      "Ying Jin"
    ],
    "abstract": "Model selection/optimization in conformal inference is challenging, since it\nmay break the exchangeability between labeled and unlabeled data. We study this\nproblem in the context of conformal selection, which uses conformal p-values to\nselect ``interesting'' instances with large unobserved labels from a pool of\nunlabeled data, while controlling the FDR in finite sample. For validity,\nexisting solutions require the model choice to be independent of the data used\nto construct the p-values and calibrate the selection set. However, when\npresented with many model choices and limited labeled data, it is desirable to\n(i) select the best model in a data-driven manner, and (ii) mitigate power loss\ndue to sample splitting.\n  This paper presents OptCS, a general framework that allows valid statistical\ntesting (selection) after flexible data-driven model optimization. We introduce\ngeneral conditions under which OptCS constructs valid conformal p-values\ndespite substantial data reuse and handles complex p-value dependencies to\nmaintain finite-sample FDR control via a novel multiple testing procedure. We\ninstantiate this general recipe to propose three FDR-controlling procedures,\neach optimizing the models differently: (i) selecting the most powerful one\namong multiple pre-trained candidate models, (ii) using all data for model\nfitting without sample splitting, and (iii) combining full-sample model fitting\nand selection. We demonstrate the efficacy of our methods via simulation\nstudies and real applications in drug discovery and alignment of large language\nmodels in radiology report generation.",
    "categories": [
      "stat.ME",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "stat.ME",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17983v1",
    "published_date": "2024-11-27 01:40:50 UTC",
    "updated_date": "2024-11-27 01:40:50 UTC"
  },
  {
    "arxiv_id": "2412.00083v3",
    "title": "Visual Error Patterns in Multi-Modal AI: A Statistical Approach",
    "authors": [
      "Ching-Yi Wang"
    ],
    "abstract": "Multi-modal large language models (MLLMs), such as GPT-4o, excel at\nintegrating text and visual data but face systematic challenges when\ninterpreting ambiguous or incomplete visual stimuli. This study leverages\nstatistical modeling to analyze the factors driving these errors, using a\ndataset of geometric stimuli characterized by features like 3D, rotation, and\nmissing face/side. We applied parametric methods, non-parametric methods, and\nensemble techniques to predict classification errors, with the non-linear\ngradient boosting model achieving the highest performance (AUC=0.85) during\ncross-validation. Feature importance analysis highlighted difficulties in depth\nperception and reconstructing incomplete structures as key contributors to\nmisclassification. These findings demonstrate the effectiveness of statistical\napproaches for uncovering limitations in MLLMs and offer actionable insights\nfor enhancing model architectures by integrating contextual reasoning\nmechanisms.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.AP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00083v3",
    "published_date": "2024-11-27 01:20:08 UTC",
    "updated_date": "2024-12-06 02:01:54 UTC"
  },
  {
    "arxiv_id": "2411.17976v3",
    "title": "The importance of visual modelling languages in generative software engineering",
    "authors": [
      "Roberto Rossi"
    ],
    "abstract": "Multimodal GPTs represent a watershed in the interplay between Software\nEngineering and Generative Artificial Intelligence. GPT-4 accepts image and\ntext inputs, rather than simply natural language. We investigate relevant use\ncases stemming from these enhanced capabilities of GPT-4. To the best of our\nknowledge, no other work has investigated similar use cases involving Software\nEngineering tasks carried out via multimodal GPTs prompted with a mix of\ndiagrams and natural language.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "9 pages, working paper",
    "pdf_url": "http://arxiv.org/pdf/2411.17976v3",
    "published_date": "2024-11-27 01:15:36 UTC",
    "updated_date": "2025-01-13 17:42:09 UTC"
  },
  {
    "arxiv_id": "2411.17973v2",
    "title": "Improved implicit diffusion model with knowledge distillation to estimate the spatial distribution density of carbon stock in remote sensing imagery",
    "authors": [
      "Zhenyu Yu",
      "Jinnian Wang",
      "Mohd Yamani Idna Idris"
    ],
    "abstract": "The forest serves as the most significant terrestrial carbon stock mechanism,\neffectively reducing atmospheric CO2 concentrations and mitigating climate\nchange. Remote sensing provides high data accuracy and enables large-scale\nobservations. Optical images facilitate long-term monitoring, which is crucial\nfor future carbon stock estimation studies. This study focuses on Huize County,\nQujing City, Yunnan Province, China, utilizing GF-1 WFV satellite imagery. The\nKD-VGG and KD-UNet modules were introduced for initial feature extraction, and\nthe improved implicit diffusion model (IIDM) was proposed. The results showed:\n(1) The VGG module improved initial feature extraction, improving accuracy, and\nreducing inference time with optimized model parameters. (2) The\nCross-attention + MLPs module enabled effective feature fusion, establishing\ncritical relationships between global and local features, achieving\nhigh-accuracy estimation. (3) The IIDM model, a novel contribution,\ndemonstrated the highest estimation accuracy with an RMSE of 12.17%,\nsignificantly improving by 41.69% to 42.33% compared to the regression model.\nIn carbon stock estimation, the generative model excelled in extracting deeper\nfeatures, significantly outperforming other models, demonstrating the\nfeasibility of AI-generated content in quantitative remote sensing. The\n16-meter resolution estimates provide a robust basis for tailoring forest\ncarbon sink regulations, enhancing regional carbon stock management.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17973v2",
    "published_date": "2024-11-27 01:06:05 UTC",
    "updated_date": "2025-04-23 19:50:29 UTC"
  },
  {
    "arxiv_id": "2411.17971v1",
    "title": "Graph Neural Network for Cerebral Blood Flow Prediction With Clinical Datasets",
    "authors": [
      "Seungyeon Kim",
      "Wheesung Lee",
      "Sung-Ho Ahn",
      "Do-Eun Lee",
      "Tae-Rin Lee"
    ],
    "abstract": "Accurate prediction of cerebral blood flow is essential for the diagnosis and\ntreatment of cerebrovascular diseases. Traditional computational methods,\nhowever, often incur significant computational costs, limiting their\npracticality in real-time clinical applications. This paper proposes a graph\nneural network (GNN) to predict blood flow and pressure in previously unseen\ncerebral vascular network structures that were not included in training data.\nThe GNN was developed using clinical datasets from patients with stenosis,\nfeaturing complex and abnormal vascular geometries. Additionally, the GNN model\nwas trained on data incorporating a wide range of inflow conditions, vessel\ntopologies, and network connectivities to enhance its generalization\ncapability. The approach achieved Pearson's correlation coefficients of 0.727\nfor pressure and 0.824 for flow rate, with sufficient training data. These\nfindings demonstrate the potential of the GNN for real-time cerebrovascular\ndiagnostics, particularly in handling intricate and pathological vascular\nnetworks.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "4 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.17971v1",
    "published_date": "2024-11-27 01:01:37 UTC",
    "updated_date": "2024-11-27 01:01:37 UTC"
  },
  {
    "arxiv_id": "2412.00082v1",
    "title": "Dual Prototyping with Domain and Class Prototypes for Affective Brain-Computer Interface in Unseen Target Conditions",
    "authors": [
      "Guangli Li",
      "Zhehao Zhou",
      "Tuo Sun",
      "Ping Tan",
      "Li Zhang",
      "Zhen Liang"
    ],
    "abstract": "EEG signals have emerged as a powerful tool in affective brain-computer\ninterfaces, playing a crucial role in emotion recognition. However, current\ndeep transfer learning-based methods for EEG recognition face challenges due to\nthe reliance of both source and target data in model learning, which\nsignificantly affect model performance and generalization. To overcome this\nlimitation, we propose a novel framework (PL-DCP) and introduce the concepts of\nfeature disentanglement and prototype inference. The dual prototyping mechanism\nincorporates both domain and class prototypes: domain prototypes capture\nindividual variations across subjects, while class prototypes represent the\nideal class distributions within their respective domains. Importantly, the\nproposed PL-DCP framework operates exclusively with source data during\ntraining, meaning that target data remains completely unseen throughout the\nentire process. To address label noise, we employ a pairwise learning strategy\nthat encodes proximity relationships between sample pairs, effectively reducing\nthe influence of mislabeled data. Experimental validation on the SEED and\nSEED-IV datasets demonstrates that PL-DCP, despite not utilizing target data\nduring training, achieves performance comparable to deep transfer learning\nmethods that require both source and target data. This highlights the potential\nof PL-DCP as an effective and robust approach for EEG-based emotion\nrecognition.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00082v1",
    "published_date": "2024-11-27 00:56:43 UTC",
    "updated_date": "2024-11-27 00:56:43 UTC"
  },
  {
    "arxiv_id": "2412.09628v2",
    "title": "Bridging AI and Science: Implications from a Large-Scale Literature Analysis of AI4Science",
    "authors": [
      "Yutong Xie",
      "Yijun Pan",
      "Hua Xu",
      "Qiaozhu Mei"
    ],
    "abstract": "Artificial Intelligence has proven to be a transformative tool for advancing\nscientific research across a wide range of disciplines. However, a significant\ngap still exists between AI and scientific communities, limiting the full\npotential of AI methods in driving broad scientific discovery. Existing efforts\nin identifying and bridging this gap have often relied on qualitative\nexamination of small samples of literature, offering a limited perspective on\nthe broader AI4Science landscape. In this work, we present a large-scale\nanalysis of the AI4Science literature, starting by using large language models\nto identify scientific problems and AI methods in publications from top science\nand AI venues. Leveraging this new dataset, we quantitatively highlight key\ndisparities between AI methods and scientific problems, revealing substantial\nopportunities for deeper AI integration across scientific disciplines.\nFurthermore, we explore the potential and challenges of facilitating\ncollaboration between AI and scientific communities through the lens of link\nprediction. Our findings and tools aim to promote more impactful\ninterdisciplinary collaborations and accelerate scientific discovery through\ndeeper and broader AI integration. Our code and dataset are available at:\nhttps://github.com/charles-pyj/Bridging-AI-and-Science.",
    "categories": [
      "cs.AI",
      "cs.DL",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "22 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.09628v2",
    "published_date": "2024-11-27 00:40:51 UTC",
    "updated_date": "2025-02-18 03:53:05 UTC"
  },
  {
    "arxiv_id": "2411.18649v1",
    "title": "Dynamic Logistic Ensembles with Recursive Probability and Automatic Subset Splitting for Enhanced Binary Classification",
    "authors": [
      "Mohammad Zubair Khan",
      "David Li"
    ],
    "abstract": "This paper presents a novel approach to binary classification using dynamic\nlogistic ensemble models. The proposed method addresses the challenges posed by\ndatasets containing inherent internal clusters that lack explicit feature-based\nseparations. By extending traditional logistic regression, we develop an\nalgorithm that automatically partitions the dataset into multiple subsets,\nconstructing an ensemble of logistic models to enhance classification accuracy.\nA key innovation in this work is the recursive probability calculation, derived\nthrough algebraic manipulation and mathematical induction, which enables\nscalable and efficient model construction. Compared to traditional ensemble\nmethods such as Bagging and Boosting, our approach maintains interpretability\nwhile offering competitive performance. Furthermore, we systematically employ\nmaximum likelihood and cost functions to facilitate the analytical derivation\nof recursive gradients as functions of ensemble depth. The effectiveness of the\nproposed approach is validated on a custom dataset created by introducing noise\nand shifting data to simulate group structures, resulting in significant\nperformance improvements with layers. Implemented in Python, this work balances\ncomputational efficiency with theoretical rigor, providing a robust and\ninterpretable solution for complex classification tasks with broad implications\nfor machine learning applications. Code at\nhttps://github.com/ensemble-art/Dynamic-Logistic-Ensembles",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 Pages, 2024 IEEE 15th Annual Ubiquitous Computing, Electronics \\&\n  Mobile Communication Conference (UEMCON)}. Published in the Proceedings of\n  UEMCON 2024, \\c{opyright}2024 IEEE",
    "pdf_url": "http://arxiv.org/pdf/2411.18649v1",
    "published_date": "2024-11-27 00:22:55 UTC",
    "updated_date": "2024-11-27 00:22:55 UTC"
  }
]
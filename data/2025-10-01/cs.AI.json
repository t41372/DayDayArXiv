{
  "date": "2025-10-01",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-10-01 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä¸€å¥è¯æ€»ç»“ï¼š**\nä»Šå¤©æ˜¯ AI Agent èŒƒå¼è½¬ç§»å’Œæ¨ç†èƒ½åŠ›å¤§çˆ†å‘çš„ä¸€å¤©ã€‚ä» Salesforce æå‡ºçš„ **WALT** å°† Web Agent ä»â€œç‚¹å‡»æ“ä½œâ€è½¬å‘â€œå·¥å…·è°ƒç”¨â€ï¼Œåˆ°èƒ½è¾¾åˆ° IMO é‡‘ç‰Œæ°´å¹³çš„æ•°å­¦è¯æ˜ç³»ç»Ÿ **Aristotle**ï¼Œå†åˆ°å¯¹ **Model Editing**ï¼ˆæ¨¡å‹ç¼–è¾‘ï¼‰æœ‰æ•ˆæ€§çš„å°–é”è´¨ç–‘ä»¥åŠå¯¹ **Sycophantic**ï¼ˆé˜¿è°€å¥‰æ‰¿ï¼‰AI å±å®³çš„ç¤¾ä¼šå¿ƒç†å­¦æ¢è®¨ï¼Œä»Šå¤©çš„ arXiv å……æ»¡äº†å¯¹ç°æœ‰èŒƒå¼çš„åæ€ä¸çªç ´ã€‚\n\n---\n\n### ğŸš€ é‡ç£…æ¨è (Headliners)\n\n**1. [Agent æ–°èŒƒå¼] WALT: Web Agents that Learn Tools**\n> **WALT: å­¦ä¹ å·¥å…·çš„ Web Agents**\n> *Authors: Viraj Prabhu, et al. (Salesforce Research, Stanford, etc.)*\n\n*   **æ ¸å¿ƒç—›ç‚¹**ï¼šç°æœ‰çš„ Web Agent ä¾èµ–é€æ­¥çš„ UI äº¤äº’ï¼ˆç‚¹å‡»ã€è¾“å…¥ï¼‰ï¼Œåœ¨é¢å¯¹åŠ¨æ€å¸ƒå±€å’Œé•¿ç¨‹ä»»åŠ¡æ—¶éå¸¸è„†å¼±ï¼Œå°±åƒè®©ä¸€ä¸ªäººè’™ç€çœ¼æ‘¸ç´¢ç½‘é¡µä¸€æ ·ã€‚\n*   **ä¸»è¦è´¡çŒ®**ï¼šæå‡ºäº† WALT æ¡†æ¶ï¼Œ**å°†ç½‘é¡µæ½œåœ¨çš„åŠŸèƒ½é€†å‘å·¥ç¨‹ä¸ºå¯å¤ç”¨çš„â€œå·¥å…·â€**ã€‚ä¸å…¶è®© Agent æ€è€ƒâ€œç‚¹å“ªé‡Œâ€ï¼Œä¸å¦‚ç›´æ¥è°ƒç”¨ `search(query)` æˆ– `create(listing)`ã€‚\n*   **Implication**ï¼šè¿™æ˜¯ä¸€ç§ä»â€œä½çº§æ‰§è¡Œâ€åˆ°â€œé«˜çº§å·¥å…·è°ƒç”¨â€çš„èŒƒå¼è½¬å˜ã€‚åœ¨ VisualWebArena å’Œ WebArena ä¸Šï¼ŒWALT ä»¥æ›´å°‘çš„æ­¥éª¤å’Œæ›´ä½çš„æ¨ç†ä¾èµ–å®ç°äº†æ›´é«˜çš„æˆåŠŸç‡ã€‚\n\n**2. [æ•°å­¦æ¨ç†] Aristotle: IMO-level Automated Theorem Proving**\n> **Aristotle: å›½é™…æ•°å­¦å¥¥æ—åŒ¹å…‹ (IMO) çº§åˆ«çš„è‡ªåŠ¨å®šç†è¯æ˜**\n> *Authors: Tudor Achim, et al.*\n\n*   **æ ¸å¿ƒçªç ´**ï¼š**é‡‘ç‰Œæ°´å¹³ï¼** Aristotle ç³»ç»Ÿåœ¨ 2025 å¹´ IMO é—®é¢˜ä¸Šè¾¾åˆ°äº†é‡‘ç‰ŒåŒç­‰æ°´å¹³ã€‚\n*   **æ–¹æ³•**ï¼šç»“åˆäº†å½¢å¼åŒ–éªŒè¯ï¼ˆLean proof searchï¼‰å’Œéå½¢å¼åŒ–æ¨ç†ã€‚ç³»ç»Ÿç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼šLean è¯æ˜æœç´¢ã€ç”Ÿæˆå¹¶å½¢å¼åŒ–å¼•ç†çš„éå½¢å¼åŒ–æ¨ç†ç³»ç»Ÿï¼Œä»¥åŠä¸“ç”¨çš„å‡ ä½•æ±‚è§£å™¨ã€‚\n*   **æ„ä¹‰**ï¼šè¯æ˜äº† AI åœ¨é«˜åº¦æŠ½è±¡å’Œä¸¥è°¨çš„æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå·²ç»è¾¾åˆ°äº†äººç±»é¡¶å°–é€‰æ‰‹çš„æ°´å¹³ã€‚\n\n**3. [ç¤¾ä¼šå½±å“] Sycophantic AI Decreases Prosocial Intentions and Promotes Dependence**\n> **é˜¿è°€å¥‰æ‰¿çš„ AI ä¼šé™ä½äº²ç¤¾ä¼šæ„å›¾å¹¶åŠ©é•¿ä¾èµ–**\n> *Authors: Myra Cheng, et al. (Stanford University)*\n\n*   **å‘ç°**ï¼šAI æ™®éå­˜åœ¨ **Sycophancyï¼ˆé˜¿è°€å¥‰æ‰¿ï¼‰** ç°è±¡â€”â€”ä¸ç®¡ç”¨æˆ·è¯´å•¥éƒ½ç‚¹å¤´ç§°æ˜¯ã€‚ç ”ç©¶å‘ç° AI è‚¯å®šç”¨æˆ·è§‚ç‚¹çš„é¢‘ç‡æ¯”äººç±»é«˜ 50%ï¼Œå³ä½¿åœ¨è¯¥è§‚ç‚¹æ¶‰åŠæ¬ºéª—æˆ–ä¼¤å®³æ—¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚\n*   **åæœ**ï¼šç”¨æˆ·æ›´ä¿¡ä»»è¿™äº›â€œé©¬å±ç²¾â€ AIï¼Œä½†è¿™ä¼šæ˜¾è‘—é™ä½ç”¨æˆ·ä¿®å¤äººé™…å†²çªçš„æ„æ„¿ï¼Œå¹¶è®©ä»–ä»¬åšä¿¡è‡ªå·±æ˜¯â€œå¯¹çš„â€ã€‚è¿™æ­ç¤ºäº† AI å¯¹äººç±»åˆ¤æ–­åŠ›çš„æ½œç§»é»˜åŒ–è…èš€ã€‚\n\n**4. [æ‰¹åˆ¤æ€§ç ”ç©¶] Is Model Editing Built on Sand? Revealing Its Illusory Success and Fragile Foundation**\n> **æ¨¡å‹ç¼–è¾‘æ˜¯å»ºç«‹åœ¨æ²™æ»©ä¸Šå—ï¼Ÿæ­ç¤ºå…¶è™šå¹»çš„æˆåŠŸä¸è„†å¼±çš„åŸºç¡€**\n> *Authors: Wei Liu, et al.*\n\n*   **æ ¸å¿ƒè§‚ç‚¹**ï¼š**ç«åŠ›å…¨å¼€çš„æ‰¹åˆ¤ã€‚** ä½œè€…è®¤ä¸ºç›®å‰çš„ Model Editingï¼ˆæ¨¡å‹ç¼–è¾‘ï¼‰æ–‡çŒ®å¤§å¤šæ˜¯å»ºç«‹åœ¨â€œè™šå¹»çš„æˆåŠŸâ€ä¹‹ä¸Šã€‚\n*   **å‘ç°**ï¼šç¼–è¾‘æ¨¡å‹å¾€å¾€åªæ˜¯åˆ©ç”¨äº†éšè—çš„ **shortcutsï¼ˆæ·å¾„ï¼‰** æ¥è¿åˆç›®æ ‡è¾“å‡ºï¼Œè€ŒéçœŸæ­£æ›´æ–°äº†è¯­ä¹‰çŸ¥è¯†ã€‚åªè¦åŠ å…¥ç®€å•çš„å¦å®šæŸ¥è¯¢ï¼ŒSOTA æ–¹æ³•å°±ä¼šå´©æºƒã€‚è¿™å‘¼åç¤¾åŒºé‡æ–°å®¡è§†æ¨¡å‹ç¼–è¾‘çš„åŸºç¡€å¯è¡Œæ€§ã€‚\n\n---\n\n### ğŸ§  æ¨ç†ã€æ€ç»´é“¾ä¸ System 2 (Reasoning & CoT)\n\n**5. [æ€ç»´æ ‘æ”¹è¿›] Lateral Tree-of-Thoughts Surpasses ToT by Incorporating Logically-Consistent, Low-Utility Candidates**\n> **æ¨ªå‘æ€ç»´æ ‘ (LToT) é€šè¿‡æ•´åˆé€»è¾‘ä¸€è‡´çš„ä½æ•ˆç”¨å€™é€‰è€…è¶…è¶Š ToT**\n> *Authors: Abhinav Madahar*\n\n*   **è´¡çŒ®**ï¼šæŒ‡å‡ºäº†æ ‡å‡† ToT çš„â€œå¹¿åº¦é¥±å’Œâ€å’Œâ€œæ·±åº¦çŸ­è§†â€é—®é¢˜ã€‚æå‡ºäº† **Lateral ToT**ï¼Œå°†æœç´¢å‰æ²¿åˆ†ä¸ºâ€œä¸»çº¿â€ï¼ˆé«˜åˆ©ç”¨ç‡ï¼‰å’Œâ€œæ”¯çº¿â€ï¼ˆä½åˆ©ç”¨ç‡ä½†é€»è¾‘ä¸€è‡´ï¼‰ã€‚é€šè¿‡ä¿ç•™é‚£äº›çœ‹ä¼¼æ— ç”¨ä½†é€»è¾‘é€šé¡ºçš„åˆ†æ”¯ï¼Œé˜²æ­¢è¿‡æ—©å‰ªæï¼Œä»è€Œåœ¨æµ‹è¯•æ—¶è®¡ç®—é¢„ç®—å¢åŠ æ—¶è·å¾—æ›´å¥½çš„æ”¶ç›Šã€‚\n\n**6. [æ¨ç†æ•ˆç‡] Rethinking Thinking Tokens: LLMs as Improvement Operators**\n> **åæ€æ€è€ƒ Tokenï¼šå°† LLM è§†ä¸ºæ”¹è¿›ç®—å­**\n> *Authors: Lovish Madaan, et al.*\n\n*   **æ–°è§†è§’**ï¼šä¸ä¸€å®šè¦ç”Ÿæˆè¶…é•¿çš„ CoTã€‚æå‡ºäº† **PDR (Parallel-Distill-Refine)** æ¡†æ¶ï¼šå¹¶è¡Œç”Ÿæˆè‰ç¨¿ -> è’¸é¦åˆ°å·¥ä½œåŒº -> åŸºäºå·¥ä½œåŒºä¼˜åŒ–ã€‚\n*   **æ•ˆæœ**ï¼šè¿™ç§æ–¹æ³•å…è®¸æ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå¹¶åœ¨ç›¸åŒé¢„ç®—ä¸‹ï¼Œåˆ©ç”¨è¿­ä»£ä¼˜åŒ–ï¼ˆSystem 2 é£æ ¼ï¼‰è¶…è¶Šäº†å•æ¬¡é•¿ CoT çš„æ•ˆæœã€‚\n\n**7. [æ¨ç†è®­ç»ƒ] Reinforcement Learning with Verifiable yet Noisy Rewards under Imperfect Verifiers**\n> **ä¸å®Œç¾éªŒè¯å™¨ä¸‹å¸¦æœ‰å¯éªŒè¯ä½†å™ªå£°å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ **\n> *Authors: Xin-Qiang Cai, et al.*\n\n*   **é—®é¢˜**ï¼šRLVRï¼ˆå¸¦å¯éªŒè¯å¥–åŠ±çš„ RLï¼‰å¾ˆç«ï¼Œä½†éªŒè¯å™¨ï¼ˆVerifierï¼‰å¾€å¾€ä¸å®Œç¾ï¼Œæœ‰è¯¯æŠ¥å’Œæ¼æŠ¥ã€‚\n*   **æ–¹æ³•**ï¼šå°†éªŒè¯å™¨è§†ä¸ºéå¯¹ç§°å™ªå£°ä¿¡é“ï¼Œæå‡ºäº†å‰å‘å’Œåå‘ä¿®æ­£æ–¹æ³•ï¼Œåœ¨æ— éœ€æ˜‚è´µäººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡äº†æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚\n\n---\n\n### ğŸ¤– Agent ä¸å…·èº«æ™ºèƒ½ (Agents & Embodied AI)\n\n**8. [Agent å¹³å°] GEM: A Gym for Agentic LLMs**\n> **GEM: Agentic LLM çš„è®­ç»ƒåœº**\n> *Authors: Zichen Liu, et al.*\n\n*   **è´¡çŒ®**ï¼šç±»ä¼¼äº RL æ—¶ä»£çš„ OpenAI Gymï¼ŒGEM ä¸º LLM Agent æä¾›äº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„ç¯å¢ƒæ¨¡æ‹Ÿå™¨ã€‚æ”¯æŒå¼‚æ­¥å‘é‡åŒ–æ‰§è¡Œï¼Œä¸ä»…æ˜¯è¯„æµ‹å·¥å…·ï¼Œæ›´æ˜¯è®­ç»ƒç¯å¢ƒã€‚æä¾›äº† PPOã€GRPO å’Œ REINFORCE åœ¨ 24 ä¸ªç¯å¢ƒä¸‹çš„åŸºå‡†æµ‹è¯•ã€‚\n\n**9. [æ•°æ®åˆæˆ] TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments**\n> **TOUCAN: ä»çœŸå®ä¸–ç•Œ MCP ç¯å¢ƒåˆæˆ 150 ä¸‡æ¡å·¥å…·ä»£ç†æ•°æ®**\n> *Authors: Zhangchen Xu, et al.*\n\n*   **èµ„æº**ï¼šå‘å¸ƒäº†ç›®å‰æœ€å¤§çš„å¼€æºå·¥å…·ä»£ç†ï¼ˆTool-Agenticï¼‰æ•°æ®é›†ã€‚åŸºäºè¿‘ 500 ä¸ªçœŸå®çš„ MCP (Model Context Protocols) ç¯å¢ƒï¼ŒåŒ…å« 150 ä¸‡æ¡è½¨è¿¹ã€‚è§£å†³äº†ç°æœ‰æ•°æ®ç¼ºä¹å¤šå·¥å…·ã€å¤šè½®äº¤äº’å¤æ‚æ€§çš„é—®é¢˜ã€‚\n\n**10. [å®‰å…¨æ”»é˜²] Breaking the Code: Security Assessment of AI Code Agents Through Systematic Jailbreaking Attacks**\n> **ç ´è§£ä»£ç ï¼šé€šè¿‡ç³»ç»Ÿæ€§è¶Šç‹±æ”»å‡»è¯„ä¼° AI ä»£ç  Agent çš„å®‰å…¨æ€§**\n> *Authors: Shoumik Saha, et al.*\n\n*   **è­¦ç¤º**ï¼šä»£ç  Agent ä¸ä»…èƒ½è¯´è¯ï¼Œè¿˜èƒ½**æ‰§è¡Œä»£ç **ï¼Œé£é™©æ›´é«˜ã€‚æå‡ºäº† **JAWS-BENCH**ï¼Œå‘ç°å³ä½¿æ˜¯æ‹’ç»äº†æœ‰å®³æ–‡æœ¬çš„ Agentï¼Œåœ¨åç»­çš„ Planning/Tool-use é˜¶æ®µä¹Ÿå¸¸è¢«â€œç»•è¿›å»â€ï¼Œæœ€ç»ˆæ‰§è¡Œæ¶æ„ä»£ç ã€‚\n\n---\n\n### ğŸ‘ï¸ å¤šæ¨¡æ€ä¸ç”Ÿæˆæ¨¡å‹ (Multimodal & Generative)\n\n**11. [å°å‚æ•°é«˜æ€§èƒ½] Apriel-1.5-15b-Thinker**\n> **Apriel-1.5-15b-Thinker: é€šè¿‡æ•°æ®ä¸ºä¸­å¿ƒçš„æŒç»­é¢„è®­ç»ƒå®ç°å‰æ²¿æ¨ç†èƒ½åŠ›**\n> *Authors: Shruthan Radhakrishna, et al.*\n\n*   **äº®ç‚¹**ï¼šä»… 15B å‚æ•°ï¼Œæ€§èƒ½å¯¹æ ‡ DeepSeek-R1 å’Œ Gemini-2.5-Flashã€‚\n*   **æ–¹æ³•**ï¼šä¸é å †ç®—åŠ›ï¼Œé è®­ç»ƒè®¾è®¡ã€‚1) æ·±åº¦ç¼©æ”¾ (depth upscaling)ï¼›2) é’ˆå¯¹è§†è§‰æ¨ç†çš„åˆæˆæ•°æ®æŒç»­é¢„è®­ç»ƒï¼›3) é«˜è´¨é‡æ¨ç†è¿¹å¾®è°ƒã€‚è¯æ˜äº†ä¸­å°æ¨¡å‹é€šè¿‡ç²¾ç»†çš„æ•°æ®å·¥ç¨‹ä¹Ÿèƒ½å…·å¤‡é¡¶çº§å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚\n\n**12. [ç«¯åˆ°ç«¯è¯­éŸ³] MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance**\n> **MOSS-Speech: è¿ˆå‘æ— éœ€æ–‡æœ¬å¼•å¯¼çš„çœŸæ­£è¯­éŸ³åˆ°è¯­éŸ³æ¨¡å‹**\n> *Authors: Xingjian Zhao, et al. (Fudan University)*\n\n*   **çªç ´**ï¼šç°åœ¨çš„è¯­éŸ³å¯¹è¯ç³»ç»Ÿå¤šæ˜¯â€œè¯­éŸ³->æ–‡æœ¬->LLM->æ–‡æœ¬->è¯­éŸ³â€ã€‚MOSS-Speech å®ç°äº†**ç›´æ¥ç†è§£å’Œç”Ÿæˆè¯­éŸ³**ï¼Œä¿ç•™äº†å‰¯è¯­è¨€çº¿ç´¢ï¼ˆè¯­è°ƒã€æƒ…æ„Ÿï¼‰ï¼Œå»¶è¿Ÿæ›´ä½ï¼Œè¡¨ç°åŠ›æ›´å¼ºã€‚\n\n**13. [ä¸–ç•Œæ¨¡å‹] Can World Models Benefit VLMs for World Dynamics?**\n> **ä¸–ç•Œæ¨¡å‹èƒ½ä¸º VLM å¸¦æ¥ä¸–ç•ŒåŠ¨æ€çŸ¥è¯†å—ï¼Ÿ**\n> *Authors: Kevin Zhang, et al.*\n\n*   **å‘ç°**ï¼šå°†è§†é¢‘æ‰©æ•£æ¨¡å‹ä½œä¸ºâ€œç”Ÿæˆå¼ç¼–ç å™¨â€ï¼Œæå–å‡ºçš„æ½œå˜é‡ï¼ˆLatentsï¼‰åŒ…å«äº†ä¸°å¯Œçš„ç‰©ç†å’Œè¿åŠ¨å…ˆéªŒã€‚æå‡ºçš„ **DyVA** æ¨¡å‹åˆ©ç”¨è¿™äº›å…ˆéªŒæ˜¾è‘—æå‡äº† VLM çš„ç©ºé—´æ¨ç†å’Œå¤šå¸§æ¨ç†èƒ½åŠ›ã€‚\n\n---\n\n### ğŸ§¬ AI for Science (ç§‘å­¦ä¸åŒ»ç–—)\n\n**14. [ç”Ÿç‰©äº¤äº’] Speak to a Protein: An Interactive Multimodal Co-Scientist for Protein Analysis**\n> **ä¸è›‹ç™½è´¨å¯¹è¯ï¼šç”¨äºè›‹ç™½è´¨åˆ†æçš„äº¤äº’å¼å¤šæ¨¡æ€ç§‘ç ”åŠ©æ‰‹**\n> *Authors: Carles Navarro, et al.*\n\n*   **åº”ç”¨**ï¼šä¸ç”¨å†ç›¯ç€ PyMOL è‹¦çœ‹å‡ å‘¨äº†ã€‚è¿™æ˜¯ä¸€ä¸ªèƒ½è”ç½‘æ£€ç´¢æ–‡çŒ®ã€å¹¶åœ¨ 3D åœºæ™¯ä¸­é«˜äº®ã€æ³¨é‡Šã€æ“ä½œè›‹ç™½è´¨ç»“æ„çš„ AI åŠ©æ‰‹ã€‚å®ƒèƒ½å›ç­”å…³äºç»“åˆè¢‹ã€æ„è±¡å˜åŒ–çš„é—®é¢˜ï¼Œå¹¶å®æ—¶è¿è¡Œä»£ç éªŒè¯ã€‚\n\n**15. [åŒ–å­¦ Agent] Benchmarking Agentic Systems in Automated Scientific Information Extraction with ChemX**\n> **ChemX: ç”¨ ChemX è¯„æµ‹è‡ªåŠ¨ç§‘å­¦ä¿¡æ¯æå–ä¸­çš„ Agent ç³»ç»Ÿ**\n> *Authors: Anastasia Vepreva, et al.*\n\n*   **åŸºå‡†**ï¼šåŒ–å­¦é¢†åŸŸçš„ä¿¡æ¯æå–ï¼ˆå¦‚ä»æ–‡çŒ®ä¸­æå–çº³ç±³ææ–™æ•°æ®ï¼‰æéš¾ï¼Œå› ä¸ºæ¶‰åŠå¤§é‡ä¸“ä¸šæœ¯è¯­å’Œå¤æ‚å›¾è¡¨ã€‚ChemX æä¾›äº† 10 ä¸ªä¸“å®¶éªŒè¯çš„æ•°æ®é›†ï¼Œå‘ç°å³ä¾¿æ˜¯ GPT-5 çº§åˆ«çš„ Agent åœ¨å¤„ç†åŒ–å­¦ä¸Šä¸‹æ–‡æ—¶ä»æœ‰å¾ˆå¤§æŒ‘æˆ˜ã€‚\n\n---\n\n### ğŸ” è§£é‡Šæ€§ä¸åº•å±‚æœºåˆ¶ (Interpretability)\n\n**16. [SAE æ”¹è¿›] AbsTopK: Rethinking Sparse Autoencoders For Bidirectional Features**\n> **AbsTopK: ä¸ºåŒå‘ç‰¹å¾é‡æ–°æ€è€ƒç¨€ç–è‡ªç¼–ç å™¨ (SAE)**\n> *Authors: Xudong Zhu, et al.*\n\n*   **æ´å¯Ÿ**ï¼šç°æœ‰çš„ SAE å¼ºåˆ¶éè´Ÿæ€§ï¼Œå¯¼è‡´æ— æ³•ç”¨ä¸€ä¸ªç‰¹å¾è¡¨ç¤ºåŒå‘æ¦‚å¿µï¼ˆå¦‚â€œç”·æ€§ vs å¥³æ€§â€ï¼‰ï¼Œè€Œæ˜¯å°†å…¶æ‹†æˆä¸¤ä¸ªå†—ä½™ç‰¹å¾ã€‚\n*   **æ–¹æ³•**ï¼šæå‡ºäº† **AbsTopK SAE**ï¼Œå…è®¸æ¿€æ´»å€¼çš„ç»å¯¹å€¼TopKï¼Œä»è€Œåœ¨ä¸€ä¸ªç‰¹å¾ä¸­ç¼–ç å¯¹ç«‹æ¦‚å¿µï¼Œæé«˜äº†å¯è§£é‡Šæ€§å’Œç‰¹å¾çš„å®Œæ•´æ€§ã€‚\n\n**17. [æ³¨æ„åŠ›æœºåˆ¶] Decomposing Attention To Find Context-Sensitive Neurons**\n> **åˆ†è§£ Attention ä»¥å‘ç°ä¸Šä¸‹æ–‡æ•æ„Ÿç¥ç»å…ƒ**\n> *Authors: Alex Gibson*\n\n*   **å‘ç°**ï¼šé€šè¿‡åˆ†è§£ Transformer çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå‘ç°å³ä½¿æ˜¯åœ¨ç¬¬ä¸€å±‚ï¼Œä¹Ÿèƒ½é€šè¿‡åˆ†æ Softmax åˆ†æ¯çš„ç¨³å®šæ€§ï¼Œæ‰¾åˆ°æ•°ç™¾ä¸ªå“åº”é«˜çº§ä¸Šä¸‹æ–‡å±æ€§çš„ç¥ç»å…ƒã€‚è¿™ä¸ºç†è§£å¤§æ¨¡å‹å¦‚ä½•å¤„ç†ä¸Šä¸‹æ–‡æä¾›äº†æ–°çš„å¾®è§‚è§†è§’ã€‚",
  "papers": [
    {
      "arxiv_id": "2510.01524v1",
      "title": "WALT: Web Agents that Learn Tools",
      "title_zh": "WALTï¼šå­¦ä¹ å·¥å…·çš„ç½‘é¡µæ™ºèƒ½ä½“",
      "authors": [
        "Viraj Prabhu",
        "Yutong Dai",
        "Matthew Fernandez",
        "Jing Gu",
        "Krithika Ramakrishnan",
        "Yanqi Luo",
        "Silvio Savarese",
        "Caiming Xiong",
        "Junnan Li",
        "Zeyuan Chen",
        "Ran Xu"
      ],
      "abstract": "Web agents promise to automate complex browser tasks, but current methods remain brittle -- relying on step-by-step UI interactions and heavy LLM reasoning that break under dynamic layouts and long horizons. Humans, by contrast, exploit website-provided functionality through high-level operations like search, filter, and sort. We introduce WALT (Web Agents that Learn Tools), a framework that reverse-engineers latent website functionality into reusable invocable tools. Rather than hypothesizing ad-hoc skills, WALT exposes robust implementations of automations already designed into websites -- spanning discovery (search, filter, sort), communication (post, comment, upvote), and content management (create, edit, delete). Tools abstract away low-level execution: instead of reasoning about how to click and type, agents simply call search(query) or create(listing). This shifts the computational burden from fragile step-by-step reasoning to reliable tool invocation. On VisualWebArena and WebArena, WALT achieves higher success with fewer steps and less LLM-dependent reasoning, establishing a robust and generalizable paradigm for browser automation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† WALT (Web Agents that Learn Tools) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ Web agents åœ¨å¤„ç†åŠ¨æ€å¸ƒå±€å’Œé•¿è·¯å¾„ä»»åŠ¡æ—¶å› è¿‡åº¦ä¾èµ–é€æ­¥ UI äº¤äº’å’Œé«˜è´Ÿè· LLM æ¨ç†è€Œå¯¼è‡´çš„è„†å¼±æ€§ã€‚ä¸åŒäºä¼ ç»Ÿçš„ä½çº§äº¤äº’æ–¹å¼ï¼ŒWALT é€šè¿‡é€†å‘å·¥ç¨‹å°†ç½‘ç«™çš„æ½œåœ¨åŠŸèƒ½æå–ä¸ºå¯é‡å¤è°ƒç”¨çš„å·¥å…·ï¼Œæ¶µç›–äº† discoveryï¼ˆæœç´¢ã€è¿‡æ»¤ï¼‰ã€communicationï¼ˆå‘å¸ƒã€è¯„è®ºï¼‰ä»¥åŠ content managementï¼ˆåˆ›å»ºã€ç¼–è¾‘ï¼‰ç­‰æ ¸å¿ƒæ“ä½œã€‚è¯¥æ¡†æ¶å°†è®¡ç®—è´Ÿæ‹…ä»ä¸ç¨³å®šçš„ç‚¹å‡»ä¸è¾“å…¥é€»è¾‘è½¬ç§»åˆ°å¯é çš„ tool invocationï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿç›´æ¥é€šè¿‡é«˜çº§æ“ä½œæ‰§è¡Œä»»åŠ¡ã€‚åœ¨ VisualWebArena å’Œ WebArena çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒWALT ä»¥æ›´å°‘çš„æ­¥éª¤å’Œæ›´ä½çš„ LLM æ¨ç†ä¾èµ–å®ç°äº†æ›´é«˜çš„æˆåŠŸç‡ï¼Œä¸ºæµè§ˆå™¨è‡ªåŠ¨åŒ–å»ºç«‹äº†ä¸€ç§ç¨³å¥ä¸”é€šç”¨çš„æ–°èŒƒå¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01524v1",
      "published_date": "2025-10-01 23:41:47 UTC",
      "updated_date": "2025-10-01 23:41:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:51:02.894120+00:00"
    },
    {
      "arxiv_id": "2510.01520v1",
      "title": "Predictive Modeling and Explainable AI for Veterinary Safety Profiles, Residue Assessment, and Health Outcomes Using Real-World Data and Physicochemical Properties",
      "title_zh": "åŸºäºçœŸå®ä¸–ç•Œæ•°æ®ä¸ç†åŒ–æ€§è´¨çš„å…½è¯å®‰å…¨æ€§ã€æ®‹ç•™è¯„ä¼°åŠå¥åº·ç»“å±€é¢„æµ‹å»ºæ¨¡ä¸å¯è§£é‡Šäººå·¥æ™ºèƒ½",
      "authors": [
        "Hossein Sholehrasa",
        "Xuan Xu",
        "Doina Caragea",
        "Jim E. Riviere",
        "Majid Jaberi-Douraki"
      ],
      "abstract": "The safe use of pharmaceuticals in food-producing animals is vital to protect animal welfare and human food safety. Adverse events (AEs) may signal unexpected pharmacokinetic or toxicokinetic effects, increasing the risk of violative residues in the food chain. This study introduces a predictive framework for classifying outcomes (Death vs. Recovery) using ~1.28 million reports (1987-2025 Q1) from the U.S. FDA's OpenFDA Center for Veterinary Medicine. A preprocessing pipeline merged relational tables and standardized AEs through VeDDRA ontologies. Data were normalized, missing values imputed, and high-cardinality features reduced; physicochemical drug properties were integrated to capture chemical-residue links. We evaluated supervised models, including Random Forest, CatBoost, XGBoost, ExcelFormer, and large language models (Gemma 3-27B, Phi 3-12B). Class imbalance was addressed, such as undersampling and oversampling, with a focus on prioritizing recall for fatal outcomes. Ensemble methods(Voting, Stacking) and CatBoost performed best, achieving precision, recall, and F1-scores of 0.95. Incorporating Average Uncertainty Margin (AUM)-based pseudo-labeling of uncertain cases improved minority-class detection, particularly in ExcelFormer and XGBoost. Interpretability via SHAP identified biologically plausible predictors, including lung, heart, and bronchial disorders, animal demographics, and drug physicochemical properties. These features were strongly linked to fatal outcomes. Overall, the framework shows that combining rigorous data engineering, advanced machine learning, and explainable AI enables accurate, interpretable predictions of veterinary safety outcomes. The approach supports FARAD's mission by enabling early detection of high-risk drug-event profiles, strengthening residue risk assessment, and informing regulatory and clinical decision-making.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºOpenFDAå…½åŒ»åŒ»å­¦ä¸­å¿ƒçº¦128ä¸‡ä»½æŠ¥å‘Šçš„é¢„æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨Real-World Dataå’Œè¯ç‰©çš„Physicochemical Propertieså¯¹åŠ¨ç‰©å¥åº·ç»“å±€ï¼ˆæ­»äº¡ä¸åº·å¤ï¼‰è¿›è¡Œåˆ†ç±»é¢„æµ‹ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†é›†æˆVeDDRAæœ¬ä½“è®ºçš„æ ‡å‡†åŒ–æ•°æ®å¤„ç†æµæ°´çº¿ï¼Œå¹¶è¯„ä¼°äº†åŒ…æ‹¬CatBoostã€XGBoostã€ExcelFormerä»¥åŠGemma 3ã€Phi 3ç­‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å†…çš„å¤šç§æ¨¡å‹æ€§èƒ½ã€‚ä¸ºè§£å†³æ•°æ®ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œç ”ç©¶å¼•å…¥äº†åŸºäºAverage Uncertainty Margin (AUM)çš„ä¼ªæ ‡ç­¾æŠ€æœ¯ï¼Œä½¿CatBoostå’Œé›†æˆå­¦ä¹ (Ensemble methods)åœ¨F1-scoreä¸Šè¾¾åˆ°äº†0.95ã€‚é€šè¿‡SHAPå¯è§£é‡Šæ€§åˆ†æï¼Œç ”ç©¶æˆåŠŸè¯†åˆ«å‡ºè‚ºéƒ¨ä¸å¿ƒè„ç–¾ç—…ã€åŠ¨ç‰©äººå£ç»Ÿè®¡å­¦ç‰¹å¾ä»¥åŠè¯ç‰©çš„ç‰©ç†åŒ–å­¦æ€§è´¨æ˜¯é¢„æµ‹è‡´å‘½ç»“å±€çš„å…³é”®ç”Ÿç‰©å­¦æŒ‡æ ‡ã€‚è¯¥æ¡†æ¶ä¸ºFARADçš„æ®‹ç•™é£é™©è¯„ä¼°ä»»åŠ¡æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒï¼Œæœ‰æ•ˆæå‡äº†å…½åŒ»å®‰å…¨ç›‘ç®¡å’Œä¸´åºŠå†³ç­–çš„ç§‘å­¦æ€§ä¸å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01520v1",
      "published_date": "2025-10-01 23:34:46 UTC",
      "updated_date": "2025-10-01 23:34:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:53:07.652559+00:00"
    },
    {
      "arxiv_id": "2510.01513v1",
      "title": "From Videos to Indexed Knowledge Graphs -- Framework to Marry Methods for Multimodal Content Analysis and Understanding",
      "title_zh": "ä»è§†é¢‘åˆ°ç´¢å¼•çŸ¥è¯†å›¾è°±ï¼šå¤šæ¨¡æ€å†…å®¹åˆ†æä¸ç†è§£çš„æ–¹æ³•èåˆæ¡†æ¶",
      "authors": [
        "Basem Rizk",
        "Joel Walsh",
        "Mark Core",
        "Benjamin Nye"
      ],
      "abstract": "Analysis of multi-modal content can be tricky, computationally expensive, and require a significant amount of engineering efforts. Lots of work with pre-trained models on static data is out there, yet fusing these opensource models and methods with complex data such as videos is relatively challenging. In this paper, we present a framework that enables efficiently prototyping pipelines for multi-modal content analysis. We craft a candidate recipe for a pipeline, marrying a set of pre-trained models, to convert videos into a temporal semi-structured data format. We translate this structure further to a frame-level indexed knowledge graph representation that is query-able and supports continual learning, enabling the dynamic incorporation of new domain-specific knowledge through an interactive medium.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ—¨åœ¨ç®€åŒ–å¤šæ¨¡æ€å†…å®¹åˆ†æä¸ç†è§£çš„æ¡†æ¶ï¼Œæœ‰æ•ˆè§£å†³äº†å°† Pre-trained Models ä¸è§†é¢‘ç­‰å¤æ‚æ•°æ®èåˆæ—¶æ‰€é¢ä¸´çš„å·¥ç¨‹æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆå¤šç§é¢„è®­ç»ƒæ¨¡å‹ï¼Œèƒ½å¤Ÿé«˜æ•ˆæ„å»ºå¤„ç†ç®¡çº¿ï¼Œå°†è§†é¢‘è½¬åŒ–ä¸ºå…·æœ‰æ—¶é—´å±æ€§çš„åŠç»“æ„åŒ–æ•°æ®æ ¼å¼ã€‚éšåï¼Œç ”ç©¶å°†è¿™äº›ç»“æ„è¿›ä¸€æ­¥è½¬åŒ–ä¸ºå¸§çº§ç´¢å¼•çš„çŸ¥è¯†å›¾è°± (Knowledge Graph) è¡¨å¾ï¼Œä½¿å…¶æ”¯æŒé«˜æ•ˆæŸ¥è¯¢ã€‚è¯¥ç³»ç»Ÿè¿˜å…·å¤‡æŒç»­å­¦ä¹  (Continual Learning) èƒ½åŠ›ï¼Œå…è®¸é€šè¿‡äº¤äº’å¼åª’ä»‹åŠ¨æ€èå…¥ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ã€‚è¿™ä¸€æˆæœä¸ºä»è§†é¢‘ä¸­æå–ç´¢å¼•åŒ–çŸ¥è¯†å¹¶è¿›è¡Œæ·±å±‚å¤šæ¨¡æ€åˆ†ææä¾›äº†ä¸€ç§çµæ´»ä¸”å¯åŸå‹åŒ–çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01513v1",
      "published_date": "2025-10-01 23:20:15 UTC",
      "updated_date": "2025-10-01 23:20:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:50:52.793926+00:00"
    },
    {
      "arxiv_id": "2510.05144v1",
      "title": "SynCED-EnDe 2025: A Synthetic and Curated English - German Dataset for Critical Error Detection in Machine Translation",
      "title_zh": "SynCED-EnDe 2025ï¼šç”¨äºæœºå™¨ç¿»è¯‘å…³é”®é”™è¯¯æ£€æµ‹çš„åˆæˆä¸ç²¾é€‰è‹±å¾·æ•°æ®é›†",
      "authors": [
        "Muskaan Chopra",
        "Lorenz Sparrenberg",
        "Rafet Sifa"
      ],
      "abstract": "Critical Error Detection (CED) in machine translation aims to determine whether a translation is safe to use or contains unacceptable deviations in meaning. While the WMT21 English-German CED dataset provided the first benchmark, it is limited in scale, label balance, domain coverage, and temporal freshness. We present SynCED-EnDe, a new resource consisting of 1,000 gold-labeled and 8,000 silver-labeled sentence pairs, balanced 50/50 between error and non-error cases. SynCED-EnDe draws from diverse 2024-2025 sources (StackExchange, GOV.UK) and introduces explicit error subclasses, structured trigger flags, and fine-grained auxiliary judgments (obviousness, severity, localization complexity, contextual dependency, adequacy deviation). These enrichments enable systematic analyses of error risk and intricacy beyond binary detection. The dataset is permanently hosted on GitHub and Hugging Face, accompanied by documentation, annotation guidelines, and baseline scripts. Benchmark experiments with XLM-R and related encoders show substantial performance gains over WMT21 due to balanced labels and refined annotations. We envision SynCED-EnDe as a community resource to advance safe deployment of MT in information retrieval and conversational assistants, particularly in emerging contexts such as wearable AI devices.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨ç¿»è¯‘å…³é”®é”™è¯¯æ£€æµ‹ (Critical Error Detection, CED) é¢†åŸŸ WMT21 æ•°æ®é›†åœ¨è§„æ¨¡ã€æ ‡ç­¾å¹³è¡¡å’Œé¢†åŸŸè¦†ç›–ç­‰æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†å…¨æ–°çš„ SynCED-EnDe æ•°æ®é›†ã€‚è¯¥èµ„æºåŒ…å« 1,000 æ¡äººå·¥æ ‡æ³¨ (gold-labeled) å’Œ 8,000 æ¡è‡ªåŠ¨æ ‡æ³¨ (silver-labeled) çš„å¥å­å¯¹ï¼Œå¹¶åœ¨é”™è¯¯ä¸éé”™è¯¯æ¡ˆä¾‹é—´å®ç°äº† 50/50 çš„æ¯”ä¾‹å¹³è¡¡ã€‚æ•°æ®é›†é‡‡æ ·è‡ª 2024-2025 å¹´çš„ StackExchange å’Œ GOV.UK ç­‰å¤šæ ·åŒ–æ¥æºï¼Œå¼•å…¥äº†æ˜¾å¼çš„é”™è¯¯å­ç±» (error subclasses)ã€ç»“æ„åŒ–çš„è§¦å‘æ ‡å¿— (trigger flags) ä»¥åŠç»†ç²’åº¦çš„è¾…åŠ©åˆ¤å®šæŒ‡æ ‡ï¼Œå¦‚ä¸¥é‡ç¨‹åº¦ (severity) å’Œä¸Šä¸‹æ–‡ä¾èµ– (contextual dependency) ç­‰ã€‚åŸºäº XLM-R çš„åŸºå‡†å®éªŒè¡¨æ˜ï¼Œå¾—ç›Šäºå¹³è¡¡çš„æ ‡ç­¾åˆ†å¸ƒå’Œç²¾ç»†åŒ–æ ‡æ³¨ï¼Œè¯¥æ•°æ®é›†åœ¨æ£€æµ‹æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äº WMT21 åŸºå‡†ã€‚SynCED-EnDe ä¸ºå¯ç©¿æˆ´ AI è®¾å¤‡ç­‰æ–°å…´åœºæ™¯ä¸­æœºå™¨ç¿»è¯‘çš„å®‰å…¨éƒ¨ç½²æä¾›äº†é‡è¦æ”¯æŒï¼Œæ—¨åœ¨è¿›ä¸€æ­¥æ¨åŠ¨ä¿¡æ¯æ£€ç´¢å’Œå¯¹è¯åŠ©æ‰‹é¢†åŸŸçš„ç¿»è¯‘å®‰å…¨ç ”ç©¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.05144v1",
      "published_date": "2025-10-01 22:38:56 UTC",
      "updated_date": "2025-10-01 22:38:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:51:07.382215+00:00"
    },
    {
      "arxiv_id": "2510.02403v1",
      "title": "Glaucoma Detection and Structured OCT Report Generation via a Fine-tuned Multimodal Large Language Model",
      "title_zh": "åŸºäºå¾®è°ƒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„é’å…‰çœ¼æ£€æµ‹ä¸ç»“æ„åŒ– OCT æŠ¥å‘Šç”Ÿæˆ",
      "authors": [
        "Jalil Jalili",
        "Yashraj Gavhane",
        "Evan Walker",
        "Anna Heinke",
        "Christopher Bowd",
        "Akram Belghith",
        "Massimo A. Fazio",
        "Christopher A. Girkin",
        "C. Gustavo De Moraes",
        "Jeffrey M. Liebmann",
        "Sally L. Baxter",
        "Robert N. Weinreb",
        "Linda M. Zangwill",
        "Mark Christopher"
      ],
      "abstract": "Objective: To develop an explainable multimodal large language model (MM-LLM) that (1) screens optic nerve head (ONH) OCT circle scans for quality and (2) generates structured clinical reports that include glaucoma diagnosis and sector-wise retinal nerve fiber layer (RNFL) thinning assessments. Design: Retrospective cohort study of 1,310 subjects contributing 43,849 Spectralis ONH OCT circle scans (1,331 glaucomatous and 867 healthy eyes) from the DIGS and ADAGES cohorts. Methods: A MM-LLM (Llama 3.2 Vision-Instruct model) was fine-tuned to generate clinical descriptions of OCT imaging data. Training data included paired OCT images and automatically generated, structured clinical reports that described global and sectoral RNFL thinning. Poor-quality scans were labeled as unusable and paired with a fixed refusal statement. The model was evaluated on a held-out test set for three tasks: quality assessment, glaucoma detection, and RNFL thinning classification across seven anatomical sectors. Evaluation metrics included accuracy, sensitivity, specificity, precision, and F1-score. Model description quality was also evaluated using standard text evaluation metrics. Results: The model achieved 0.90 accuracy and 0.98 specificity for quality triage. For glaucoma detection, accuracy was 0.86 (sensitivity 0.91, specificity 0.73, F1-score 0.91). RNFL thinning prediction accuracy ranged from 0.83 to 0.94, with highest performance in global and temporal sectors. Text generation scores showed strong alignment with reference reports (BLEU: 0.82; ROUGE-1: 0.94; ROUGE-2: 0.87; ROUGE-L: 0.92; BERTScore-F1: 0.99). Conclusions: The fine-tuned MM-LLM generated accurate clinical descriptions based on OCT imaging. The model achieved high accuracy in identifying image quality issues and detecting glaucoma. The model also provided sectoral descriptions of RNFL thinning to help support clinical OCT evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å¾®è°ƒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(Multimodal Large Language Model, MM-LLM)ï¼Œå…·ä½“åŸºäº Llama 3.2 Vision-Instruct æ¨¡å‹ï¼Œå®ç°äº†é’å…‰çœ¼æ£€æµ‹åŠç»“æ„åŒ– OCT æŠ¥å‘Šçš„è‡ªåŠ¨ç”Ÿæˆã€‚è¯¥æ¨¡å‹æ—¨åœ¨ç­›é€‰è§†ç¥ç»ä¹³å¤´(Optic Nerve Head, ONH)çš„ OCT ç¯æ‰«æè´¨é‡ï¼Œå¹¶ç”ŸæˆåŒ…å«é’å…‰çœ¼è¯Šæ–­å’Œè§†ç½‘è†œç¥ç»çº¤ç»´å±‚(Retinal Nerve Fiber Layer, RNFL)åˆ†åŒºåŸŸå˜è–„è¯„ä¼°çš„ä¸´åºŠæŠ¥å‘Šã€‚ç ”ç©¶åˆ©ç”¨äº†æ¥è‡ª 1,310 åå—è¯•è€…çš„ 43,849 å¼  OCT æ‰«æå›¾åƒè¿›è¡Œå›é¡¾æ€§åˆ†æï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå°†å½±åƒç‰¹å¾è½¬åŒ–ä¸ºç²¾å‡†çš„æ–‡å­—æè¿°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨å½±åƒè´¨é‡åˆ†ç±»ä¸Šè¾¾åˆ°äº† 0.90 çš„å‡†ç¡®ç‡ï¼Œåœ¨é’å…‰çœ¼æ£€æµ‹ä»»åŠ¡ä¸­å–å¾—äº† 0.86 çš„å‡†ç¡®ç‡ï¼Œè€Œ RNFL å˜è–„é¢„æµ‹çš„å‡†ç¡®ç‡åˆ™åœ¨ 0.83 è‡³ 0.94 ä¹‹é—´ã€‚æ­¤å¤–ï¼Œæ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬æŠ¥å‘Šåœ¨ BLEU å’Œ BERTScore ç­‰å¤šé¡¹è¯„ä»·æŒ‡æ ‡ä¸Šä¸ä¸“å®¶æŠ¥å‘Šé«˜åº¦ä¸€è‡´ã€‚è¯¥ç ”ç©¶è¯æ˜äº†å¾®è°ƒåçš„ MM-LLM åœ¨æä¾›å¯è§£é‡Šæ€§ä¸´åºŠå»ºè®®å’Œè¾…åŠ© OCT è¯„ä¼°æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02403v1",
      "published_date": "2025-10-01 22:37:28 UTC",
      "updated_date": "2025-10-01 22:37:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:51:07.583241+00:00"
    },
    {
      "arxiv_id": "2510.01500v1",
      "title": "Lateral Tree-of-Thoughts Surpasses ToT by Incorporating Logically-Consistent, Low-Utility Candidates",
      "title_zh": "æ¨ªå‘æ€ç»´æ ‘ï¼šé€šè¿‡å¼•å…¥é€»è¾‘ä¸€è‡´çš„ä½æ•ˆç”¨å€™é€‰æ–¹æ¡ˆè¶…è¶Š ToT",
      "authors": [
        "Abhinav Madahar"
      ],
      "abstract": "Modern deployments increasingly allocate large test-time compute (thousands of tokens or many node expansions) to boost reliability. Under such budgets, standard Tree-of-Thoughts-style search exhibits two pathologies: breadth saturation (additional samples mostly produce near-duplicates, so width stops growing) and depth myopia (noisy short-horizon utilities prune branches whose payoff appears after a few more steps). We propose Lateral Tree-of-Thoughts (LToT), a drop-in controller that separates utility from logical consistency and treats low-utility but consistent candidates as assets rather than waste. The frontier is split into mainlines (high-utility candidates used for exploitation) and laterals (consistent, initially low-utility candidates that receive short, cheap probes before judgment). LToT explores laterals via Lateral Racing with Short-Circuit (LR--SC): a capped successive-halving race that spreads tiny probes across a very wide lateral set, uses width-aware thresholds with repeat-to-confirm, and immediately promotes a branch once its envelope clears the mainline bar; mainlines are kept intentionally narrow so surplus compute is invested where width is cheap. We prove a pseudolinear lateral cost $Î˜(N_0 \\log_Î· N_0)$ with logarithmically many rungs (initial lateral width $N_0$; culling factor $Î·>1$), in contrast to the exponential growth of uncapped mainlines. Empirical evaluations on benchmark tasks are in preparation and will be added in a future revision. In short, LToT turns large test-time budgets into principled diversity while preserving promotion discipline, mitigating saturation and myopia without inflating compute.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ ‡å‡† Tree-of-Thoughts (ToT) æœç´¢åœ¨å¢åŠ æ¨ç†è®¡ç®—é‡æ—¶å‡ºç°çš„å®½åº¦é¥±å’Œï¼ˆæ ·æœ¬é‡å¤ï¼‰å’Œæ·±åº¦è¿‘è§†ï¼ˆç”±äºçŸ­æœŸæ•ˆç”¨è¯„ä¼°å™ªå£°å¯¼è‡´è¿‡æ—©å‰ªæï¼‰é—®é¢˜ï¼Œæå‡ºäº† Lateral Tree-of-Thoughts (LToT) æ§åˆ¶æ¡†æ¶ã€‚LToT çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†é€»è¾‘ä¸€è‡´æ€§ (logical consistency) ä¸æ•ˆç”¨å€¼ (utility) åˆ†ç¦»å¼€æ¥ï¼Œå°†é€»è¾‘ä¸€è‡´ä½†åˆå§‹æ•ˆç”¨è¾ƒä½çš„å€™é€‰æ–¹æ¡ˆè§†ä¸ºæœ‰ä»·å€¼çš„èµ„äº§è€ŒéåºŸæ–™ã€‚è¯¥æ¡†æ¶å°†æœç´¢å‰æ²¿åˆ†ä¸ºç”¨äºå¼€å‘é«˜ä»·å€¼å€™é€‰è€…çš„ä¸»çº¿ (mainlines) å’Œè¿›è¡Œä½æˆæœ¬å¹¿åº¦æ¢ç´¢çš„ä¾§å‘åˆ†æ”¯ (laterals)ã€‚é€šè¿‡ Lateral Racing with Short-Circuit (LR--SC) æœºåˆ¶ï¼ŒLToT åœ¨æå®½çš„ä¾§å‘é›†åˆä¸­è¿›è¡Œå¾®å°æ¢æµ‹ï¼Œå¹¶åˆ©ç”¨å®½åº¦æ„ŸçŸ¥é˜ˆå€¼ï¼Œä¸€æ—¦åˆ†æ”¯æ½œåŠ›è¶…è¿‡ä¸»çº¿ä¾¿ç«‹å³æ™‹å‡ã€‚ç†è®ºè¯æ˜ä¾§å‘åˆ†æ”¯æ¢ç´¢å…·æœ‰ä¼ªçº¿æ€§æˆæœ¬ $\\Theta(N_0 \\log_\\eta N_0)$ï¼Œæœ‰æ•ˆé¿å…äº†ä¼ ç»Ÿæ–¹æ³•çš„æŒ‡æ•°çº§å¢é•¿ã€‚LToT åœ¨ä¸å¢åŠ è®¡ç®—æ”¯å‡ºçš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ç»´æŒæ™‹å‡çºªå¾‹å®ç°äº†åŸåˆ™æ€§çš„æœç´¢å¤šæ ·æ€§ï¼Œæ˜¾è‘—ç¼“è§£äº†å¤§è§„æ¨¡æµ‹è¯•æ—¶è®¡ç®—ä¸­çš„é¥±å’Œä¸è¿‘è§†ç—…ç†ç°è±¡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01500v1",
      "published_date": "2025-10-01 22:23:58 UTC",
      "updated_date": "2025-10-01 22:23:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:51:16.296656+00:00"
    },
    {
      "arxiv_id": "2510.01499v1",
      "title": "Beyond Majority Voting: LLM Aggregation by Leveraging Higher-Order Information",
      "title_zh": "è¶…è¶Šå¤šæ•°æŠ•ç¥¨ï¼šåˆ©ç”¨é«˜é˜¶ä¿¡æ¯çš„ LLM èšåˆ",
      "authors": [
        "Rui Ai",
        "Yuqi Pan",
        "David Simchi-Levi",
        "Milind Tambe",
        "Haifeng Xu"
      ],
      "abstract": "With the rapid progress of multi-agent large language model (LLM) reasoning, how to effectively aggregate answers from multiple LLMs has emerged as a fundamental challenge. Standard majority voting treats all answers equally, failing to consider latent heterogeneity and correlation across models. In this work, we design two new aggregation algorithms called Optimal Weight (OW) and Inverse Surprising Popularity (ISP), leveraging both first-order and second-order information. Our theoretical analysis shows these methods provably mitigate inherent limitations of majority voting under mild assumptions, leading to more reliable collective decisions. We empirically validate our algorithms on synthetic datasets, popular LLM fine-tuning benchmarks such as UltraFeedback and MMLU, and a real-world healthcare setting ARMMAN. Across all cases, our methods consistently outperform majority voting, offering both practical performance gains and conceptual insights for the design of robust multi-agent LLM pipelines.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹å¤šæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹(LLM)æ¨ç†ä¸­çš„ç­”æ¡ˆèšåˆæŒ‘æˆ˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„å¤šæ•°æŠ•ç¥¨(Majority Voting)æ–¹æ³•å› å¹³ç­‰å¯¹å¾…æ‰€æœ‰ç­”æ¡ˆè€Œå¿½è§†äº†æ¨¡å‹é—´çš„æ½œåœ¨å¼‚è´¨æ€§å’Œç›¸å…³æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…è®¾è®¡äº†æœ€ä¼˜æƒé‡(Optimal Weight, OW)å’Œé€†æƒŠå¥‡æ™®åŠåº¦(Inverse Surprising Popularity, ISP)ä¸¤ç§æ–°å‹èšåˆç®—æ³•ï¼Œé€šè¿‡åˆ©ç”¨ä¸€é˜¶å’ŒäºŒé˜¶ä¿¡æ¯æ¥æå‡å†³ç­–è´¨é‡ã€‚ç†è®ºåˆ†æè¯æ˜ï¼Œè¿™äº›æ–¹æ³•åœ¨æ¸©å’Œå‡è®¾ä¸‹èƒ½æœ‰æ•ˆç¼“è§£å¤šæ•°æŠ•ç¥¨çš„å›ºæœ‰å±€é™æ€§ï¼Œæä¾›æ›´å¯é çš„é›†æˆç»“æœã€‚ç ”ç©¶å›¢é˜Ÿåœ¨åˆæˆæ•°æ®é›†ã€UltraFeedbackå’ŒMMLUç­‰ä¸»æµåŸºå‡†æµ‹è¯•ä»¥åŠçœŸå®åŒ»ç–—åœºæ™¯ARMMANä¸­è¿›è¡Œäº†å¹¿æ³›å®éªŒã€‚å®éªŒç»“æœä¸€è‡´è¡¨æ˜ï¼Œæ–°ç®—æ³•åœ¨å„ç§æƒ…å†µä¸‹å‡ä¼˜äºå¤šæ•°æŠ•ç¥¨ï¼Œä¸ºæ„å»ºç¨³å¥çš„å¤šæ™ºèƒ½ä½“LLMæµæ°´çº¿æä¾›äº†é‡è¦çš„å®è·µå¢ç›Šå’Œç†è®ºè§è§£ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01499v1",
      "published_date": "2025-10-01 22:21:50 UTC",
      "updated_date": "2025-10-01 22:21:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:52:53.546620+00:00"
    },
    {
      "arxiv_id": "2510.01498v2",
      "title": "AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging",
      "title_zh": "AortaDiffï¼šç”¨äºæ— é€ å½±å‰‚è…¹ä¸»åŠ¨è„‰ç˜¤æˆåƒçš„ç»Ÿä¸€å¤šä»»åŠ¡æ‰©æ•£æ¡†æ¶",
      "authors": [
        "Yuxuan Ou",
        "Ning Bi",
        "Jiazhen Pan",
        "Jiancheng Yang",
        "Boliang Yu",
        "Usama Zidan",
        "Regent Lee",
        "Vicente Grau"
      ],
      "abstract": "While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic aneurysms (AAA), the required iodinated contrast agents pose significant risks, including nephrotoxicity, patient allergies, and environmental harm. To reduce contrast agent use, recent deep learning methods have focused on generating synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a multi-stage pipeline that first generates images and then performs segmentation, which leads to error accumulation and fails to leverage shared semantic and anatomical structures. To address this, we propose a unified deep learning framework that generates synthetic CECT images from NCCT scans while simultaneously segmenting the aortic lumen and thrombus. Our approach integrates conditional diffusion models (CDM) with multi-task learning, enabling end-to-end joint optimization of image synthesis and anatomical segmentation. Unlike previous multitask diffusion models, our approach requires no initial predictions (e.g., a coarse segmentation mask), shares both encoder and decoder parameters across tasks, and employs a semi-supervised training strategy to learn from scans with missing segmentation labels, a common constraint in real-world clinical data. We evaluated our method on a cohort of 264 patients, where it consistently outperformed state-of-the-art single-task and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61 dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation, it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to nnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AortaDiffï¼Œä¸€ä¸ªæ—¨åœ¨å®ç°æ— å¯¹æ¯”å‰‚è…¹ä¸»åŠ¨è„‰ç˜¤(AAA)æˆåƒçš„ç»Ÿä¸€å¤šä»»åŠ¡æ‰©æ•£æ¡†æ¶(Unified Multitask Diffusion Framework)ã€‚ä¸ºäº†è§£å†³ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨ç”Ÿæˆåˆæˆå¢å¼ºCT(CECT)ä¸åˆ†å‰²ä»»åŠ¡ä¸­å­˜åœ¨çš„è¯¯å·®ç´¯ç§¯é—®é¢˜ï¼Œè¯¥æ¡†æ¶å°†æ¡ä»¶æ‰©æ•£æ¨¡å‹(Conditional Diffusion Models)ä¸å¤šä»»åŠ¡å­¦ä¹ ç›¸ç»“åˆï¼Œå®ç°äº†ä»éå¢å¼ºCT(NCCT)ç”Ÿæˆå›¾åƒå¹¶åŒæ­¥åˆ†å‰²ä¸»åŠ¨è„‰ç®¡è…”å’Œè¡€æ “çš„ç«¯åˆ°ç«¯è”åˆä¼˜åŒ–ã€‚AortaDiffåœ¨ä»»åŠ¡é—´å…±äº«ç¼–ç å™¨ä¸è§£ç å™¨å‚æ•°ï¼Œå¹¶å¼•å…¥åŠç›‘ç£å­¦ä¹ ç­–ç•¥ä»¥å¤„ç†ä¸´åºŠæ•°æ®ä¸­çš„æ ‡ç­¾ç¼ºå¤±é—®é¢˜ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨264åæ‚£è€…çš„é˜Ÿåˆ—ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„å•ä»»åŠ¡å’Œå¤šé˜¶æ®µæ¨¡å‹ï¼Œå…¶å›¾åƒåˆæˆçš„PSNRè¾¾åˆ°25.61 dBï¼Œç®¡è…”ä¸è¡€æ “çš„Diceåˆ†æ•°åˆ†åˆ«æå‡è‡³0.89å’Œ0.53ã€‚è¿™äº›æ€§èƒ½æå‡æ˜¾è‘—æé«˜äº†ä¸´åºŠæµ‹é‡çš„å‡†ç¡®æ€§ï¼Œæœ‰æ•ˆé™ä½äº†ç®¡è…”ç›´å¾„ä¸è¡€æ “é¢ç§¯çš„æµ‹é‡è¯¯å·®ï¼Œä¸ºé™ä½å¯¹æ¯”å‰‚å¸¦æ¥çš„è‚¾æ¯’æ€§ç­‰ä¸´åºŠé£é™©æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "WACV 2026",
      "pdf_url": "https://arxiv.org/pdf/2510.01498v2",
      "published_date": "2025-10-01 22:19:27 UTC",
      "updated_date": "2025-12-04 19:56:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:52:54.653162+00:00"
    },
    {
      "arxiv_id": "2510.17826v1",
      "title": "Speak to a Protein: An Interactive Multimodal Co-Scientist for Protein Analysis",
      "title_zh": "Speak to a Proteinï¼šé¢å‘è›‹ç™½è´¨åˆ†æçš„äº¤äº’å¼å¤šæ¨¡æ€ç§‘ç ”ååŒåŠ©æ‰‹",
      "authors": [
        "Carles Navarro",
        "Mariona Torrens",
        "Philipp ThÃ¶lke",
        "Stefan Doerr",
        "Gianni De Fabritiis"
      ],
      "abstract": "Building a working mental model of a protein typically requires weeks of reading, cross-referencing crystal and predicted structures, and inspecting ligand complexes, an effort that is slow, unevenly accessible, and often requires specialized computational skills. We introduce \\emph{Speak to a Protein}, a new capability that turns protein analysis into an interactive, multimodal dialogue with an expert co-scientist. The AI system retrieves and synthesizes relevant literature, structures, and ligand data; grounds answers in a live 3D scene; and can highlight, annotate, manipulate and see the visualization. It also generates and runs code when needed, explaining results in both text and graphics. We demonstrate these capabilities on relevant proteins, posing questions about binding pockets, conformational changes, or structure-activity relationships to test ideas in real-time. \\emph{Speak to a Protein} reduces the time from question to evidence, lowers the barrier to advanced structural analysis, and enables hypothesis generation by tightly coupling language, code, and 3D structures. \\emph{Speak to a Protein} is freely accessible at https://open.playmolecule.org.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†Speak to a Proteinï¼Œä¸€ç§å°†Protein Analysisè½¬åŒ–ä¸ºäº¤äº’å¼å¤šæ¨¡æ€å¯¹è¯çš„AIå…±åŒç§‘å­¦å®¶ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿæ•´åˆäº†ç›¸å…³æ–‡çŒ®ã€ç»“æ„å’Œé…ä½“æ•°æ®ï¼Œé€šè¿‡å°†ç­”æ¡ˆé”šå®šåœ¨å®æ—¶3D sceneä¸­ï¼Œæ”¯æŒå¯¹åˆ†å­å¯è§†åŒ–çš„æ ‡æ³¨ã€é«˜äº®å’Œæ“ä½œã€‚å®ƒè¿˜èƒ½æ ¹æ®éœ€è¦è‡ªåŠ¨ç”Ÿæˆå¹¶è¿è¡Œä»£ç ï¼Œå…è®¸ç ”ç©¶äººå‘˜å°±binding pocketsã€conformational changesæˆ–structure-activity relationshipsç­‰é—®é¢˜è¿›è¡Œå®æ—¶æ¢ç´¢ã€‚é€šè¿‡å°†è¯­è¨€ã€ä»£ç å’Œ3D structuresç´§å¯†ç»“åˆï¼ŒSpeak to a Proteinæ˜¾è‘—ç¼©çŸ­äº†ä»ç§‘å­¦æé—®åˆ°è·å–è¯æ®çš„æ—¶é—´ï¼Œé™ä½äº†é«˜çº§ç»“æ„åˆ†æçš„æŠ€æœ¯é—¨æ§›å¹¶ä¿ƒè¿›äº†å‡è®¾ç”Ÿæˆã€‚è¯¥ç³»ç»Ÿç›®å‰å·²ä½œä¸ºå¼€æºå·¥å…·å…è´¹æä¾›ï¼Œä¸ºè›‹ç™½è´¨ç ”ç©¶æä¾›äº†æ›´é«˜æ•ˆã€ç›´è§‚çš„åä½œæ¨¡å¼ã€‚",
      "categories": [
        "q-bio.BM",
        "cs.AI"
      ],
      "primary_category": "q-bio.BM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.17826v1",
      "published_date": "2025-10-01 22:12:34 UTC",
      "updated_date": "2025-10-01 22:12:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:52:51.441886+00:00"
    },
    {
      "arxiv_id": "2510.01494v2",
      "title": "Understanding Adversarial Transfer: Why Representation-Space Attacks Fail Where Data-Space Attacks Succeed",
      "title_zh": "æ·±å…¥ç†è§£å¯¹æŠ—è¿ç§»ï¼šè¡¨ç¤ºç©ºé—´æ”»å‡»ä¸ºä½•å¤±æ•ˆï¼Œè€Œæ•°æ®ç©ºé—´æ”»å‡»ä¸ºä½•æˆåŠŸ",
      "authors": [
        "Isha Gupta",
        "Rylan Schaeffer",
        "Joshua Kazdan",
        "Ken Ziyu Liu",
        "Sanmi Koyejo"
      ],
      "abstract": "The field of adversarial robustness has long established that adversarial examples can successfully transfer between image classifiers and that text jailbreaks can successfully transfer between language models (LMs). However, a pair of recent studies reported being unable to successfully transfer image jailbreaks between vision-language models (VLMs). To explain this striking difference, we propose a fundamental distinction regarding the transferability of attacks against machine learning models: attacks in the input data-space can transfer, whereas attacks in model representation space do not, at least not without geometric alignment of representations. We then provide theoretical and empirical evidence of this hypothesis in four different settings. First, we mathematically prove this distinction in a simple setting where two networks compute the same input-output map but via different representations. Second, we construct representation-space attacks against image classifiers that are as successful as well-known data-space attacks, but fail to transfer. Third, we construct representation-space attacks against LMs that successfully jailbreak the attacked models but again fail to transfer. Fourth, we construct data-space attacks against VLMs that successfully transfer to new VLMs, and we show that representation space attacks can transfer when VLMs' latent geometries are sufficiently aligned in post-projector space. Our work reveals that adversarial transfer is not an inherent property of all attacks but contingent on their operational domain - the shared data-space versus models' unique representation spaces - a critical insight for building more robust models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¯¹æŠ—æ ·æœ¬è¿ç§»æ€§çš„åŸºæœ¬åŸç†ï¼Œæ—¨åœ¨è§£é‡Šä¸ºä½• vision-language models (VLMs) ä¹‹é—´çš„ image jailbreaks éš¾ä»¥è¿ç§»ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªå…³é”®å‡è®¾ï¼Œå³è¾“å…¥æ•°æ®ç©ºé—´ (data-space) çš„æ”»å‡»å…·æœ‰å¯è¿ç§»æ€§ï¼Œè€Œæ¨¡å‹è¡¨ç¤ºç©ºé—´ (representation-space) çš„æ”»å‡»åœ¨ç¼ºä¹å‡ ä½•å¯¹é½ (geometric alignment) çš„æƒ…å†µä¸‹æ— æ³•è¿ç§»ã€‚é€šè¿‡æ•°å­¦è¯æ˜ä»¥åŠåœ¨å›¾åƒåˆ†ç±»å™¨ã€è¯­è¨€æ¨¡å‹ (LMs) å’Œ VLMs ä¸­çš„å››é¡¹å®éªŒï¼Œç ”ç©¶è¯å®äº†è¡¨ç¤ºç©ºé—´æ”»å‡»è™½ç„¶èƒ½æœ‰æ•ˆæ”»å‡»ç›®æ ‡æ¨¡å‹ï¼Œä½†æ™®éè¡¨ç°å‡ºæä½çš„è¿ç§»æ€§ã€‚å®éªŒè¿›ä¸€æ­¥è¡¨æ˜ï¼Œåªæœ‰å½“ä¸åŒ VLMs çš„æ½œç©ºé—´å‡ ä½• (latent geometries) åœ¨æŠ•å½±åç©ºé—´ (post-projector space) è¾¾æˆä¸€è‡´æ—¶ï¼Œè¡¨ç¤ºç©ºé—´æ”»å‡»æ‰å¯èƒ½æˆåŠŸè¿ç§»ã€‚è¿™é¡¹å·¥ä½œæ­ç¤ºäº†å¯¹æŠ—æ€§è¿ç§»å–å†³äºæ”»å‡»çš„ä½œç”¨é¢†åŸŸè€Œéå…¶å›ºæœ‰å±æ€§ï¼Œä¸ºç†è§£å¹¶æ„å»ºæ›´å…·é²æ£’æ€§çš„æ¨¡å‹æä¾›äº†é‡è¦è§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01494v2",
      "published_date": "2025-10-01 22:10:58 UTC",
      "updated_date": "2025-10-03 11:35:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:52:58.945418+00:00"
    },
    {
      "arxiv_id": "2510.01483v1",
      "title": "VL-KnG: Visual Scene Understanding for Navigation Goal Identification using Spatiotemporal Knowledge Graphs",
      "title_zh": "VL-KnGï¼šåŸºäºæ—¶ç©ºçŸ¥è¯†å›¾è°±çš„è§†è§‰åœºæ™¯ç†è§£ä¸å¯¼èˆªç›®æ ‡è¯†åˆ«",
      "authors": [
        "Mohamad Al Mdfaa",
        "Svetlana Lukina",
        "Timur Akhtyamov",
        "Arthur Nigmatzyanov",
        "Dmitrii Nalberskii",
        "Sergey Zagoruyko",
        "Gonzalo Ferrer"
      ],
      "abstract": "Vision-language models (VLMs) have shown potential for robot navigation but encounter fundamental limitations: they lack persistent scene memory, offer limited spatial reasoning, and do not scale effectively with video duration for real-time application. We present VL-KnG, a Visual Scene Understanding system that tackles these challenges using spatiotemporal knowledge graph construction and computationally efficient query processing for navigation goal identification. Our approach processes video sequences in chunks utilizing modern VLMs, creates persistent knowledge graphs that maintain object identity over time, and enables explainable spatial reasoning through queryable graph structures. We also introduce WalkieKnowledge, a new benchmark with about 200 manually annotated questions across 8 diverse trajectories spanning approximately 100 minutes of video data, enabling fair comparison between structured approaches and general-purpose VLMs. Real-world deployment on a differential drive robot demonstrates practical applicability, with our method achieving 77.27% success rate and 76.92% answer accuracy, matching Gemini 2.5 Pro performance while providing explainable reasoning supported by the knowledge graph, computational efficiency for real-time deployment across different tasks, such as localization, navigation and planning. Code and dataset will be released after acceptance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†VL-KnGï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨æœºå™¨äººå¯¼èˆªä¸­é¢ä¸´çš„æŒä¹…åœºæ™¯è®°å¿†ç¼ºå¤±ã€ç©ºé—´æ¨ç†å—é™ä»¥åŠé•¿è§†é¢‘å®æ—¶å¤„ç†æ•ˆç‡ä½ç­‰é—®é¢˜çš„ç³»ç»Ÿã€‚VL-KnGçš„æ ¸å¿ƒæ˜¯é€šè¿‡æ„å»ºæ—¶ç©ºçŸ¥è¯†å›¾è°±(spatiotemporal knowledge graphs)æ¥æ•æ‰è§†é¢‘åºåˆ—ä¸­çš„ç‰©ä½“ç‰¹å¾ï¼Œå¹¶å®ç°å…·æœ‰å¯è§£é‡Šæ€§çš„æŸ¥è¯¢å¤„ç†ã€‚ç ”ç©¶è€…è¿˜æ¨å‡ºäº†WalkieKnowledgeåŸºå‡†æµ‹è¯•é›†ï¼ŒåŒ…å«çº¦200ä¸ªæ‰‹åŠ¨æ ‡æ³¨çš„é—®é¢˜å’Œæ€»é•¿çº¦100åˆ†é’Ÿçš„è§†é¢‘è½¨è¿¹ï¼Œç”¨äºè¯„ä¼°ç»“æ„åŒ–æ–¹æ³•ä¸é€šç”¨VLMsçš„æ€§èƒ½ã€‚åœ¨å·®åˆ†é©±åŠ¨æœºå™¨äººä¸Šçš„çœŸå®éƒ¨ç½²ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å®šä½ã€å¯¼èˆªå’Œè§„åˆ’ä»»åŠ¡ä¸­å–å¾—äº†77.27%çš„æˆåŠŸç‡å’Œ76.92%çš„å›ç­”å‡†ç¡®ç‡ã€‚å…¶æ€§èƒ½ä¸ä»…è¾¾åˆ°äº†ä¸Gemini 2.5 Proç›¸å½“çš„æ°´å¹³ï¼Œè¿˜é€šè¿‡çŸ¥è¯†å›¾è°±ç»“æ„æä¾›äº†é€æ˜çš„æ¨ç†é€»è¾‘ã€‚è¯¥ç³»ç»Ÿå…·å¤‡æ”¯æŒå®æ—¶éƒ¨ç½²çš„è®¡ç®—æ•ˆç‡ï¼Œä¸ºå¤æ‚ç¯å¢ƒä¸‹çš„æœºå™¨äººè§†è§‰ç†è§£æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "pdf_url": "https://arxiv.org/pdf/2510.01483v1",
      "published_date": "2025-10-01 21:53:44 UTC",
      "updated_date": "2025-10-01 21:53:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:53:18.519270+00:00"
    },
    {
      "arxiv_id": "2510.01480v1",
      "title": "Pharmacophore-Guided Generative Design of Novel Drug-Like Molecules",
      "title_zh": "è¯æ•ˆå›¢å¼•å¯¼çš„æ–°å‹ç±»è¯åˆ†å­ç”Ÿæˆå¼è®¾è®¡",
      "authors": [
        "Ekaterina Podplutova",
        "Anastasia Vepreva",
        "Olga A. Konovalova",
        "Vladimir Vinogradov",
        "Dmitrii O. Shkil",
        "Andrei Dmitrenko"
      ],
      "abstract": "The integration of artificial intelligence (AI) in early-stage drug discovery offers unprecedented opportunities for exploring chemical space and accelerating hit-to-lead optimization. However, docking optimization in generative approaches is computationally expensive and may lead to inaccurate results. Here, we present a novel generative framework that balances pharmacophore similarity to reference compounds with structural diversity from active molecules. The framework allows users to provide custom reference sets, including FDA-approved drugs or clinical candidates, and guides the \\textit{de novo} generation of potential therapeutics. We demonstrate its applicability through a case study targeting estrogen receptor modulators and antagonists for breast cancer. The generated compounds maintain high pharmacophoric fidelity to known active molecules while introducing substantial structural novelty, suggesting strong potential for functional innovation and patentability. Comprehensive evaluation of the generated molecules against common drug-like properties confirms the robustness and pharmaceutical relevance of the approach.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆå¼ AI è¯ç‰©å‘ç°ä¸­ docking optimization è®¡ç®—æ˜‚è´µä¸”å‡†ç¡®åº¦ä¸è¶³çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç”± Pharmacophore æŒ‡å¯¼çš„æ–°å‹ç”Ÿæˆå¼æ¡†æ¶ï¼Œç”¨äºè®¾è®¡ç±»è¯åˆ†å­ã€‚è¯¥æ¡†æ¶æœ‰æ•ˆåœ°å¹³è¡¡äº†å¯¹å‚è€ƒåŒ–åˆç‰©çš„ Pharmacophore ç›¸ä¼¼æ€§ä¸æ´»æ€§åˆ†å­çš„ structural diversityï¼Œå¹¶æ”¯æŒç”¨æˆ·ä½¿ç”¨ FDA æ‰¹å‡†è¯ç‰©ç­‰è‡ªå®šä¹‰å‚è€ƒé›†æ¥å¼•å¯¼ de novo ç”Ÿæˆã€‚é€šè¿‡é’ˆå¯¹ä¹³è…ºç™Œé›Œæ¿€ç´ å—ä½“è°ƒèŠ‚å‰‚å’Œæ‹®æŠ—å‰‚çš„æ¡ˆä¾‹ç ”ç©¶è¯æ˜ï¼Œç”Ÿæˆçš„åŒ–åˆç‰©åœ¨ä¿æŒé«˜åº¦ Pharmacophoric fidelity çš„åŒæ—¶ï¼Œå±•ç°äº†æ˜¾è‘—çš„ structural noveltyã€‚å¯¹ç”Ÿæˆåˆ†å­çš„å…¨é¢è¯„ä¼°ç¡®è®¤äº†å…¶ç¬¦åˆ drug-like propertiesï¼Œåœ¨åŠŸèƒ½åˆ›æ–°å’Œå¯ä¸“åˆ©æ€§æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä¸ºåŠ é€Ÿ hit-to-lead ä¼˜åŒ–è¿‡ç¨‹æä¾›äº†ç¨³å¥ä¸”å…·æœ‰åˆ¶è¯ç›¸å…³æ€§çš„æ–°æ–¹æ³•ã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI"
      ],
      "primary_category": "q-bio.QM",
      "comment": "AI4Mat-NeurIPS-2025 Poster",
      "pdf_url": "https://arxiv.org/pdf/2510.01480v1",
      "published_date": "2025-10-01 21:45:58 UTC",
      "updated_date": "2025-10-01 21:45:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:53:26.875780+00:00"
    },
    {
      "arxiv_id": "2510.01478v1",
      "title": "Purrception: Variational Flow Matching for Vector-Quantized Image Generation",
      "title_zh": "Purrceptionï¼šç”¨äºçŸ¢é‡é‡åŒ–å›¾åƒç”Ÿæˆçš„å˜åˆ†æµåŒ¹é…",
      "authors": [
        "RÄƒzvan-Andrei MatiÅŸan",
        "Vincent Tao Hu",
        "Grigory Bartosh",
        "BjÃ¶rn Ommer",
        "Cees G. M. Snoek",
        "Max Welling",
        "Jan-Willem van de Meent",
        "Mohammad Mahdi Derakhshani",
        "Floor Eijkelboom"
      ],
      "abstract": "We introduce Purrception, a variational flow matching approach for vector-quantized image generation that provides explicit categorical supervision while maintaining continuous transport dynamics. Our method adapts Variational Flow Matching to vector-quantized latents by learning categorical posteriors over codebook indices while computing velocity fields in the continuous embedding space. This combines the geometric awareness of continuous methods with the discrete supervision of categorical approaches, enabling uncertainty quantification over plausible codes and temperature-controlled generation. We evaluate Purrception on ImageNet-1k 256x256 generation. Training converges faster than both continuous flow matching and discrete flow matching baselines while achieving competitive FID scores with state-of-the-art models. This demonstrates that Variational Flow Matching can effectively bridge continuous transport and discrete supervision for improved training efficiency in image generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Purrceptionï¼Œä¸€ç§é’ˆå¯¹çŸ¢é‡é‡åŒ–(vector-quantized)å›¾åƒç”Ÿæˆçš„å˜åˆ†æµåŒ¹é…(Variational Flow Matching)æ–¹æ³•ï¼Œæ—¨åœ¨ä¿æŒè¿ç»­ä¼ è¾“åŠ¨åŠ›å­¦çš„åŒæ—¶æä¾›æ˜¾å¼çš„ç±»åˆ«ç›‘ç£ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨è¿ç»­åµŒå…¥ç©ºé—´è®¡ç®—é€Ÿåº¦åœºï¼ŒåŒæ—¶å­¦ä¹ ä»£ç ç°¿ç´¢å¼•(codebook indices)ä¸Šçš„ç±»åˆ«åéªŒåˆ†å¸ƒï¼Œä»è€Œå°†å˜åˆ†æµåŒ¹é…æ‰©å±•è‡³çŸ¢é‡é‡åŒ–æ½œç©ºé—´ã€‚è¿™ç§è®¾è®¡ç»“åˆäº†è¿ç»­æ–¹æ³•çš„å‡ ä½•æ„ŸçŸ¥èƒ½åŠ›ä¸ç±»åˆ«æ–¹æ³•çš„ç¦»æ•£ç›‘ç£ä¼˜åŠ¿ï¼Œå®ç°äº†å¯¹å€™é€‰ä»£ç çš„ä¸ç¡®å®šæ€§é‡åŒ–ä»¥åŠæ¸©åº¦æ§åˆ¶ç”Ÿæˆã€‚åœ¨ ImageNet-1k 256x256 ç”Ÿæˆä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒPurrception çš„è®­ç»ƒæ”¶æ•›é€Ÿåº¦ä¼˜äºè¿ç»­æµåŒ¹é…å’Œç¦»æ•£æµåŒ¹é…åŸºå‡†æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨ FID åˆ†æ•°ä¸Šè¾¾åˆ°äº†ä¸æœ€å…ˆè¿›æ¨¡å‹ç›¸å½“çš„æ°´å¹³ï¼Œè¯æ˜äº†å˜åˆ†æµåŒ¹é…èƒ½æœ‰æ•ˆæ¡¥æ¥è¿ç»­ä¼ è¾“ä¸ç¦»æ•£ç›‘ç£ï¼Œæ˜¾è‘—æå‡å›¾åƒç”Ÿæˆçš„è®­ç»ƒæ•ˆç‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01478v1",
      "published_date": "2025-10-01 21:41:30 UTC",
      "updated_date": "2025-10-01 21:41:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:53:25.167783+00:00"
    },
    {
      "arxiv_id": "2510.01474v2",
      "title": "AIReg-Bench: Benchmarking Language Models That Assess AI Regulation Compliance",
      "title_zh": "AIReg-Benchï¼šé’ˆå¯¹äººå·¥æ™ºèƒ½ç›‘ç®¡åˆè§„æ€§è¯„ä¼°è¯­è¨€æ¨¡å‹çš„åŸºå‡†è¯„æµ‹",
      "authors": [
        "Bill Marino",
        "Rosco Hunter",
        "Zubair Jamali",
        "Marinos Emmanouil Kalpakos",
        "Mudra Kashyap",
        "Isaiah Hinton",
        "Alexa Hanson",
        "Maahum Nazir",
        "Christoph Schnabl",
        "Felix Steffek",
        "Hongkai Wen",
        "Nicholas D. Lane"
      ],
      "abstract": "As governments move to regulate AI, there is growing interest in using Large Language Models (LLMs) to assess whether or not an AI system complies with a given AI Regulation (AIR). However, there is presently no way to benchmark the performance of LLMs at this task. To fill this void, we introduce AIReg-Bench: the first benchmark dataset designed to test how well LLMs can assess compliance with the EU AI Act (AIA). We created this dataset through a two-step process: (1) by prompting an LLM with carefully structured instructions, we generated 120 technical documentation excerpts (samples), each depicting a fictional, albeit plausible, AI system - of the kind an AI provider might produce to demonstrate their compliance with AIR; (2) legal experts then reviewed and annotated each sample to indicate whether, and in what way, the AI system described therein violates specific Articles of the AIA. The resulting dataset, together with our evaluation of whether frontier LLMs can reproduce the experts' compliance labels, provides a starting point to understand the opportunities and limitations of LLM-based AIR compliance assessment tools and establishes a benchmark against which subsequent LLMs can be compared. The dataset and evaluation code are available at https://github.com/camlsys/aireg-bench.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AIReg-Benchï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºæµ‹è¯•å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¯„ä¼°AIç³»ç»Ÿæ˜¯å¦ç¬¦åˆæ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆï¼ˆEU AI Act, AIAï¼‰åˆè§„æ€§èƒ½åŠ›çš„åŸºå‡†æ•°æ®é›†ã€‚ç ”ç©¶å›¢é˜Ÿé‡‡ç”¨ä¸¤æ­¥æ³•æ„å»ºæ•°æ®é›†ï¼šé¦–å…ˆåˆ©ç”¨æ¨¡å‹ç”Ÿæˆ120ä»½è™šæ„ä½†åˆç†çš„AIç³»ç»ŸæŠ€æœ¯æ–‡æ¡£æ‘˜å½•ï¼Œéšåç”±æ³•å¾‹ä¸“å®¶å¯¹æ–‡æ¡£ä¸­æè¿°çš„ç³»ç»Ÿæ˜¯å¦è¿åAIAç‰¹å®šæ¡æ¬¾è¿›è¡Œè¯„å®¡ä¸æ ‡æ³¨ã€‚é€šè¿‡å¯¹æ¯”å‰æ²¿LLMsä¸æ³•å¾‹ä¸“å®¶çš„æ ‡æ³¨ä¸€è‡´æ€§ï¼Œè¯¥ç ”ç©¶æ­ç¤ºäº†åˆ©ç”¨LLMsè¿›è¡Œè‡ªåŠ¨åŒ–åˆè§„æ€§è¯„ä¼°çš„æ½œåŠ›ä¸å±€é™æ€§ã€‚AIReg-Benchä¸ä»…ä¸ºç†è§£åŸºäºLLMsçš„ç›‘ç®¡è¯„ä¼°å·¥å…·æä¾›äº†èµ·ç‚¹ï¼Œä¹Ÿä¸ºåç»­æ¨¡å‹åœ¨AIåˆè§„æ€§ä»»åŠ¡ä¸Šçš„è¡¨ç°å»ºç«‹äº†æ ‡å‡†åŒ–åŸºå‡†ã€‚ç›®å‰ï¼Œè¯¥æ•°æ®é›†åŠå…¶è¯„ä¼°ä»£ç å·²åœ¨GitHubå¼€æºã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01474v2",
      "published_date": "2025-10-01 21:33:33 UTC",
      "updated_date": "2025-10-13 01:10:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:53:28.272553+00:00"
    },
    {
      "arxiv_id": "2510.01473v1",
      "title": "From keywords to semantics: Perceptions of large language models in data discovery",
      "title_zh": "ä»å…³é”®è¯åˆ°è¯­ä¹‰ï¼šå¤§è¯­è¨€æ¨¡å‹åœ¨æ•°æ®å‘ç°ä¸­çš„è®¤çŸ¥ç ”ç©¶",
      "authors": [
        "Maura E Halstead",
        "Mark A. Green",
        "Caroline Jay",
        "Richard Kingston",
        "David Topping",
        "Alexander Singleton"
      ],
      "abstract": "Current approaches to data discovery match keywords between metadata and queries. This matching requires researchers to know the exact wording that other researchers previously used, creating a challenging process that could lead to missing relevant data. Large Language Models (LLMs) could enhance data discovery by removing this requirement and allowing researchers to ask questions with natural language. However, we do not currently know if researchers would accept LLMs for data discovery. Using a human-centered artificial intelligence (HCAI) focus, we ran focus groups (N = 27) to understand researchers' perspectives towards LLMs for data discovery. Our conceptual model shows that the potential benefits are not enough for researchers to use LLMs instead of current technology. Barriers prevent researchers from fully accepting LLMs, but features around transparency could overcome them. Using our model will allow developers to incorporate features that result in an increased acceptance of LLMs for data discovery.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†åœ¨ Data Discovery é¢†åŸŸä»å…³é”®å­—åŒ¹é…è½¬å‘è¯­ä¹‰æ£€ç´¢çš„è½¬å˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿæ–¹æ³•å› è¦æ±‚ç²¾ç¡®è¯æ±‡åŒ¹é…è€Œå®¹æ˜“å¯¼è‡´ç›¸å…³æ•°æ®é—æ¼ã€‚ç ”ç©¶è€…é‡‡ç”¨ä»¥äººä¸ºæœ¬çš„äººå·¥æ™ºèƒ½ (Human-centered artificial intelligence, HCAI) è§†è§’ï¼Œé€šè¿‡ç„¦ç‚¹å°ç»„ (Focus groups, N=27) è°ƒæŸ¥äº†ç§‘ç ”äººå‘˜å¯¹åˆ©ç”¨ Large Language Models (LLMs) è¿›è¡Œæ•°æ®å‘ç°çš„çœ‹æ³•ã€‚ç ”ç©¶å»ºç«‹äº†ä¸€ä¸ªæ¦‚å¿µæ¨¡å‹ï¼Œæ­ç¤ºäº† LLMs çš„æ½œåœ¨ä¼˜åŠ¿å°šä¸è¶³ä»¥ä¿ƒä½¿ç ”ç©¶äººå‘˜å®Œå…¨å–ä»£ç°æœ‰æŠ€æœ¯ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡å­˜åœ¨é˜»ç¢æ¥å—çš„å£å’ï¼Œä½†é€šè¿‡å¼•å…¥é€æ˜åº¦ç›¸å…³çš„åŠŸèƒ½ç‰¹å¾å¯ä»¥æœ‰æ•ˆå…‹æœè¿™äº›éšœç¢ã€‚è¯¥æ¨¡å‹ä¸ºå¼€å‘è€…æä¾›äº†å…·ä½“æŒ‡å¯¼ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–è®¾è®¡æ¥æå‡ LLMs åœ¨æ•°æ®å‘ç°ä»»åŠ¡ä¸­çš„ç”¨æˆ·æ¥å—åº¦ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01473v1",
      "published_date": "2025-10-01 21:31:54 UTC",
      "updated_date": "2025-10-01 21:31:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:53:30.779581+00:00"
    },
    {
      "arxiv_id": "2510.08585v1",
      "title": "Articulation-Informed ASR: Integrating Articulatory Features into ASR via Auxiliary Speech Inversion and Cross-Attention Fusion",
      "title_zh": "å‘éŸ³æ„ŸçŸ¥çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼šé€šè¿‡è¾…åŠ©è¯­éŸ³åæ¼”ä¸äº¤å‰æ³¨æ„åŠ›èåˆé›†æˆå‘éŸ³ç‰¹å¾",
      "authors": [
        "Ahmed Adel Attia",
        "Jing Liu",
        "Carol Espy Wilson"
      ],
      "abstract": "Prior works have investigated the use of articulatory features as complementary representations for automatic speech recognition (ASR), but their use was largely confined to shallow acoustic models. In this work, we revisit articulatory information in the era of deep learning and propose a framework that leverages articulatory representations both as an auxiliary task and as a pseudo-input to the recognition model. Specifically, we employ speech inversion as an auxiliary prediction task, and the predicted articulatory features are injected into the model as a query stream in a cross-attention module with acoustic embeddings as keys and values. Experiments on LibriSpeech demonstrate that our approach yields consistent improvements over strong transformer-based baselines, particularly under low-resource conditions. These findings suggest that articulatory features, once sidelined in ASR research, can provide meaningful benefits when reintroduced with modern architectures.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Articulation-Informed ASR æ¡†æ¶ï¼Œæ—¨åœ¨æ·±åº¦å­¦ä¹ æ—¶ä»£é‡æ–°åˆ©ç”¨å‘éŸ³ç‰¹å¾ (Articulatory Features) æ¥å¢å¼ºè‡ªåŠ¨è¯­éŸ³è¯†åˆ« (ASR) çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶å°†è¯­éŸ³åæ¼” (Speech Inversion) è®¾ä¸ºè¾…åŠ©é¢„æµ‹ä»»åŠ¡ï¼Œå¹¶å°†é¢„æµ‹å‡ºçš„å‘éŸ³ç‰¹å¾ä½œä¸ºè¯†åˆ«æ¨¡å‹çš„ä¼ªè¾“å…¥ã€‚å…·ä½“å®ç°ä¸Šï¼Œç³»ç»Ÿé€šè¿‡äº¤å‰æ³¨æ„åŠ› (Cross-Attention) æ¨¡å—å°†å‘éŸ³ç‰¹å¾ä½œä¸ºæŸ¥è¯¢æµ (Query Stream)ï¼Œå¹¶ä¸ä½œä¸ºé”®å’Œå€¼çš„å£°å­¦åµŒå…¥ (Acoustic Embeddings) è¿›è¡Œèåˆã€‚åœ¨ LibriSpeech æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¼ºåŠ› Transformer åŸºçº¿ä¹‹ä¸Šå®ç°äº†æŒç»­æ”¹è¿›ï¼Œå°¤å…¶åœ¨ä½èµ„æº (Low-Resource) ç¯å¢ƒä¸‹è¡¨ç°å‡ºæ˜¾è‘—çš„é²æ£’æ€§ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†æ›¾ç»è¢«è¾¹ç¼˜åŒ–çš„å‘éŸ³ç‰¹å¾åœ¨ç»“åˆç°ä»£æ¶æ„æ—¶ï¼Œä¾ç„¶èƒ½ä¸ºè¯­éŸ³è¯†åˆ«ä»»åŠ¡æä¾›æå…·ä»·å€¼çš„è¡¨å¾ä¿¡æ¯ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.08585v1",
      "published_date": "2025-10-01 21:07:29 UTC",
      "updated_date": "2025-10-01 21:07:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:53:37.404557+00:00"
    },
    {
      "arxiv_id": "2510.01462v1",
      "title": "RealClass: A Framework for Classroom Speech Simulation with Public Datasets and Game Engines",
      "title_zh": "RealClassï¼šåŸºäºå…¬å¼€æ•°æ®é›†ä¸æ¸¸æˆå¼•æ“çš„æ•™å®¤è¯­éŸ³æ¨¡æ‹Ÿæ¡†æ¶",
      "authors": [
        "Ahmed Adel Attia",
        "Jing Liu",
        "Carol Espy Wilson"
      ],
      "abstract": "The scarcity of large-scale classroom speech data has hindered the development of AI-driven speech models for education. Classroom datasets remain limited and not publicly available, and the absence of dedicated classroom noise or Room Impulse Response (RIR) corpora prevents the use of standard data augmentation techniques.\n  In this paper, we introduce a scalable methodology for synthesizing classroom noise and RIRs using game engines, a versatile framework that can extend to other domains beyond the classroom. Building on this methodology, we present RealClass, a dataset that combines a synthesized classroom noise corpus with a classroom speech dataset compiled from publicly available corpora. The speech data pairs a children's speech corpus with instructional speech extracted from YouTube videos to approximate real classroom interactions in clean conditions. Experiments on clean and noisy speech show that RealClass closely approximates real classroom speech, making it a valuable asset in the absence of abundant real classroom speech.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡æ•™å®¤è¯­éŸ³æ•°æ®ç¨€ç¼ºä»¥åŠç¼ºä¹ä¸“ç”¨æ•™å®¤å™ªå£°æˆ–æˆ¿é—´å†²æ¿€å“åº”(Room Impulse Response, RIR)è¯­æ–™åº“çš„é—®é¢˜ï¼Œæå‡ºäº†RealClassæ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†ä¸€ç§å¯æ‰©å±•çš„æ–¹æ³•ï¼Œåˆ©ç”¨æ¸¸æˆå¼•æ“(Game Engines)åˆæˆæ•™å®¤å™ªå£°å’ŒRIRï¼Œå…¶åº”ç”¨æ½œåŠ›å¯æ‰©å±•è‡³æ•™å®¤ä»¥å¤–çš„å…¶ä»–é¢†åŸŸã€‚RealClassæ•°æ®é›†é€šè¿‡æ•´åˆåˆæˆçš„æ•™å®¤å™ªå£°è¯­æ–™åº“ï¼Œå¹¶ç»“åˆå…¬å¼€çš„å„¿ç«¥è¯­éŸ³è¯­æ–™åº“ä»¥åŠä»YouTubeè§†é¢‘ä¸­æå–çš„æ•™å­¦è¯­éŸ³ï¼Œæ¨¡æ‹Ÿäº†å¹²å‡€æ¡ä»¶ä¸‹çš„çœŸå®æ•™å®¤äº’åŠ¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRealClassåœ¨æ¸…æ´å’Œå˜ˆæ‚è¯­éŸ³æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºä¸çœŸå®æ•™å®¤è¯­éŸ³çš„é«˜åº¦è¿‘ä¼¼æ€§ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨ç¼ºä¹çœŸå®å¤§è§„æ¨¡æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå¼€å‘æ•™è‚²é¢†åŸŸçš„AIé©±åŠ¨è¯­éŸ³æ¨¡å‹æä¾›äº†é‡è¦çš„èµ„æºæ”¯æŒã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2506.09206",
      "pdf_url": "https://arxiv.org/pdf/2510.01462v1",
      "published_date": "2025-10-01 21:04:51 UTC",
      "updated_date": "2025-10-01 21:04:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:53:39.579307+00:00"
    },
    {
      "arxiv_id": "2510.01460v2",
      "title": "The Three Regimes of Offline-to-Online Reinforcement Learning",
      "title_zh": "ç¦»çº¿åˆ°åœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„ä¸‰ç§æœºåˆ¶",
      "authors": [
        "Lu Li",
        "Tianwei Ni",
        "Yihao Sun",
        "Pierre-Luc Bacon"
      ],
      "abstract": "Offline-to-online reinforcement learning (RL) has emerged as a practical paradigm that leverages offline datasets for pretraining and online interactions for fine-tuning. However, its empirical behavior is highly inconsistent: design choices of online-fine tuning that work well in one setting can fail completely in another. We propose a stability--plasticity principle that can explain this inconsistency: we should preserve the knowledge of pretrained policy or offline dataset during online fine-tuning, whichever is better, while maintaining sufficient plasticity. This perspective identifies three regimes of online fine-tuning, each requiring distinct stability properties. We validate this framework through a large-scale empirical study, finding that the results strongly align with its predictions in 45 of 63 cases. This work provides a principled framework for guiding design choices in offline-to-online RL based on the relative performance of the offline dataset and the pretrained policy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¦»çº¿åˆ°åœ¨çº¿å¼ºåŒ–å­¦ä¹ (Offline-to-online RL)ä¸­åœ¨çº¿å¾®è°ƒ(online fine-tuning)è®¾è®¡é€‰æ‹©è¡¨ç°é«˜åº¦ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œæå‡ºäº†ç¨³å®šæ€§-å¯å¡‘æ€§åŸåˆ™(stability--plasticity principle)ã€‚è¯¥åŸåˆ™æŒ‡å‡ºï¼Œåœ¨çº¿å¾®è°ƒåº”å½“åœ¨ä¿æŒè¶³å¤Ÿå¯å¡‘æ€§(plasticity)çš„åŒæ—¶ï¼Œä¼˜å…ˆä¿ç•™é¢„è®­ç»ƒç­–ç•¥(pretrained policy)æˆ–ç¦»çº¿æ•°æ®é›†(offline dataset)ä¸­è¡¨ç°æ›´ä¼˜æ–¹çš„çŸ¥è¯†ã€‚åŸºäºæ­¤è§†è§’ï¼Œç ”ç©¶è¯†åˆ«å‡ºåœ¨çº¿å¾®è°ƒå­˜åœ¨ä¸‰ç§çŠ¶æ€(three regimes)ï¼Œä¸”æ¯ç§çŠ¶æ€å¯¹ç¨³å®šæ€§å±æ€§æœ‰ä¸åŒçš„è¦æ±‚ã€‚é€šè¿‡å¤§è§„æ¨¡å®è¯ç ”ç©¶ï¼Œä½œè€…å‘ç°åœ¨63ä¸ªå®éªŒæ¡ˆä¾‹ä¸­çš„45ä¸ªå‡ä¸è¯¥æ¡†æ¶çš„é¢„æµ‹å¼ºä¸€è‡´ã€‚è¯¥å·¥ä½œä¸ºå¦‚ä½•æ ¹æ®ç¦»çº¿æ•°æ®é›†ä¸é¢„è®­ç»ƒç­–ç•¥çš„ç›¸å¯¹è¡¨ç°æ¥æŒ‡å¯¼ç¦»çº¿åˆ°åœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„è®¾è®¡å†³ç­–æä¾›äº†ä¸€ä¸ªåŸåˆ™æ€§æ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01460v2",
      "published_date": "2025-10-01 20:58:14 UTC",
      "updated_date": "2025-12-09 22:19:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:53:45.299750+00:00"
    },
    {
      "arxiv_id": "2510.01453v1",
      "title": "The Command Line GUIde: Graphical Interfaces from Man Pages via AI",
      "title_zh": "The Command Line GUIdeï¼šåˆ©ç”¨ AI ä» Man Pages è‡ªåŠ¨ç”Ÿæˆå›¾å½¢ç•Œé¢",
      "authors": [
        "Saketh Ram Kasibatla",
        "Kiran Medleri Hiremath",
        "Raven Rothkopf",
        "Sorin Lerner",
        "Haijun Xia",
        "Brian Hempel"
      ],
      "abstract": "Although birthed in the era of teletypes, the command line shell survived the graphical interface revolution of the 1980's and lives on in modern desktop operating systems. The command line provides access to powerful functionality not otherwise exposed on the computer, but requires users to recall textual syntax and carefully scour documentation. In contrast, graphical interfaces let users organically discover and invoke possible actions through widgets and menus. To better expose the power of the command line, we demonstrate a mechanism for automatically creating graphical interfaces for command line tools by translating their documentation (in the form of man pages) into interface specifications via AI. Using these specifications, our user-facing system, called GUIde, presents the command options to the user graphically. We evaluate the generated interfaces on a corpus of commands to show to what degree GUIde offers thorough graphical interfaces for users' real-world command line tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GUIdeç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡AIæŠ€æœ¯å°†å‘½ä»¤è¡Œå·¥å…·çš„man pagesæ–‡æ¡£è‡ªåŠ¨è½¬åŒ–ä¸ºç•Œé¢è§„èŒƒï¼Œä»è€Œè§£å†³Command Line Interface (CLI)å¯¹ç”¨æˆ·è®°å¿†æˆæœ¬å’Œæ–‡æ¡£æŸ¥é˜…çš„é«˜è¦æ±‚é—®é¢˜ã€‚GUIdeåˆ©ç”¨è¿™äº›è§„èŒƒä¸ºç”¨æˆ·ç”Ÿæˆç›´è§‚çš„Graphical User Interfaceï¼Œä½¿åŸæœ¬å¤æ‚çš„å‘½ä»¤é€‰é¡¹èƒ½å¤Ÿé€šè¿‡å›¾å½¢åŒ–ç»„ä»¶è¿›è¡Œäº¤äº’å¼æ¢ç´¢ä¸è°ƒç”¨ã€‚é€šè¿‡å¯¹å¤§é‡çœŸå®ä¸–ç•Œå‘½ä»¤è¯­æ–™åº“çš„è¯„ä¼°ï¼Œè¯¥ç ”ç©¶éªŒè¯äº†GUIdeåœ¨ç”Ÿæˆå…¨é¢ä¸”å®ç”¨çš„å›¾å½¢ç•Œé¢æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¿™ç§æ–¹æ³•æˆåŠŸç»“åˆäº†å‘½ä»¤è¡Œçš„å¼ºå¤§åŠŸèƒ½ä¸å›¾å½¢ç•Œé¢çš„æ˜“ç”¨æ€§ï¼Œä¸ºè‡ªåŠ¨åŒ–è½¯ä»¶ç•Œé¢ç”Ÿæˆå’Œæå‡ç»ˆç«¯æ“ä½œæ•ˆç‡æä¾›äº†åˆ›æ–°çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "5 pages, 4 figures, In Proceedings of the IEEE Symposium on Visual Languages and Human Centric Computing (VL/HCC), October 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.01453v1",
      "published_date": "2025-10-01 20:46:53 UTC",
      "updated_date": "2025-10-01 20:46:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:53:47.670990+00:00"
    },
    {
      "arxiv_id": "2510.01450v1",
      "title": "Local Linear Attention: An Optimal Interpolation of Linear and Softmax Attention For Test-Time Regression",
      "title_zh": "å±€éƒ¨çº¿æ€§æ³¨æ„åŠ›ï¼šé¢å‘æµ‹è¯•æ—¶å›å½’çš„çº¿æ€§ä¸ Softmax æ³¨æ„åŠ›æœ€ä¼˜æ’å€¼",
      "authors": [
        "Yifei Zuo",
        "Yutong Yin",
        "Zhichen Zeng",
        "Ang Li",
        "Banghua Zhu",
        "Zhaoran Wang"
      ],
      "abstract": "Transformer architectures have achieved remarkable success in various domains. While efficient alternatives to Softmax Attention have been widely studied, the search for more expressive mechanisms grounded in theoretical insight-even at greater computational cost-has been relatively underexplored. In this work, we bridge this gap by proposing Local Linear Attention (LLA), a novel attention mechanism derived from nonparametric statistics through the lens of test-time regression. First, we show that LLA offers theoretical advantages over Linear and Softmax Attention for associative memory via a bias-variance trade-off analysis. Next, we address its computational challenges and propose two memory-efficient primitives to tackle the $Î˜(n^2 d)$ and $Î˜(n d^2)$ complexity. We then introduce FlashLLA, a hardware-efficient, blockwise algorithm that enables scalable and parallel computation on modern accelerators. In addition, we implement and profile a customized inference kernel that significantly reduces memory overheads. Finally, we empirically validate the advantages and limitations of LLA on test-time regression, in-context regression, associative recall and state tracking tasks. Experiment results demonstrate that LLA effectively adapts to non-stationarity, outperforming strong baselines in test-time training and in-context learning, and exhibiting promising evidence for its scalability and applicability in large-scale models. Code is available at https://github.com/Yifei-Zuo/Flash-LLA.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Local Linear Attention (LLA)ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºéå‚æ•°ç»Ÿè®¡ä¸­æµ‹è¯•æ—¶å›å½’ (test-time regression) è§†è§’è¡ç”Ÿçš„æ–°å‹æ³¨æ„åŠ›æœºåˆ¶ã€‚é€šè¿‡åå·®-æ–¹å·®æƒè¡¡ (bias-variance trade-off) åˆ†æï¼Œç ”ç©¶è¯æ˜äº† LLA åœ¨å…³è”è®°å¿†æ–¹é¢ç›¸è¾ƒäºä¼ ç»Ÿçš„ Linear Attention å’Œ Softmax Attention å…·æœ‰æ˜¾è‘—çš„ç†è®ºä¼˜åŠ¿ã€‚é’ˆå¯¹å…¶é«˜é˜¶è®¡ç®—å¤æ‚åº¦æŒ‘æˆ˜ï¼Œä½œè€…å¼€å‘äº†ç¡¬ä»¶é«˜æ•ˆçš„å—ç®—æ³• FlashLLA ä»¥åŠå®šåˆ¶åŒ–çš„æ¨ç†å†…æ ¸ï¼Œå®ç°äº†å¤§è§„æ¨¡å¹¶è¡Œè®¡ç®—å¹¶æ˜¾è‘—é™ä½äº†æ˜¾å­˜å¼€é”€ã€‚åœ¨æµ‹è¯•æ—¶å›å½’ã€ä¸Šä¸‹æ–‡å›å½’ (in-context regression) å’Œå…³è”å¬å›ç­‰ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLLA èƒ½æœ‰æ•ˆé€‚åº”éå¹³ç¨³æ€§ (non-stationarity)ï¼Œåœ¨æµ‹è¯•æ—¶è®­ç»ƒ (test-time training) å’Œä¸Šä¸‹æ–‡å­¦ä¹  (in-context learning) ä¸­è¡¨ç°ä¼˜äºç°æœ‰å¼ºåŸºçº¿æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨å¤§è§„æ¨¡æ¨¡å‹ä¸­åº”ç”¨æ›´å…·è¡¨è¾¾èƒ½åŠ›çš„æ³¨æ„åŠ›æœºåˆ¶æä¾›äº†ç†è®ºæ”¯æ’‘ä¸é«˜æ•ˆå®ç°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01450v1",
      "published_date": "2025-10-01 20:42:21 UTC",
      "updated_date": "2025-10-01 20:42:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:53:51.186039+00:00"
    },
    {
      "arxiv_id": "2510.01448v1",
      "title": "GeoSURGE: Geo-localization using Semantic Fusion with Hierarchy of Geographic Embeddings",
      "title_zh": "GeoSURGEï¼šåŸºäºå±‚æ¬¡åŒ–åœ°ç†åµŒå…¥ä¸è¯­ä¹‰èåˆçš„åœ°ç†å®šä½",
      "authors": [
        "Angel Daruna",
        "Nicholas Meegan",
        "Han-Pang Chiu",
        "Supun Samarasekera",
        "Rakesh Kumar"
      ],
      "abstract": "Worldwide visual geo-localization seeks to determine the geographic location of an image anywhere on Earth using only its visual content. Learned representations of geography for visual geo-localization remain an active research topic despite much progress. We formulate geo-localization as aligning the visual representation of the query image with a learned geographic representation. Our novel geographic representation explicitly models the world as a hierarchy of geographic embeddings. Additionally, we introduce an approach to efficiently fuse the appearance features of the query image with its semantic segmentation map, forming a robust visual representation. Our main experiments demonstrate improved all-time bests in 22 out of 25 metrics measured across five benchmark datasets compared to prior state-of-the-art (SOTA) methods and recent Large Vision-Language Models (LVLMs). Additional ablation studies support the claim that these gains are primarily driven by the combination of geographic and visual representations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GeoSURGEï¼Œä¸€ç§ç”¨äºå…¨çƒè§†è§‰åœ°ç†å®šä½ï¼ˆVisual geo-localizationï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¯¹é½å›¾åƒè§†è§‰è¡¨ç¤ºä¸åœ°ç†è¡¨ç¤ºæ¥ç²¾ç¡®ç¡®å®šå›¾åƒçš„å…¨çƒåæ ‡ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†ä¸–ç•Œæ˜¾å¼å»ºæ¨¡ä¸ºåœ°ç†åµŒå…¥å±‚æ¬¡ç»“æ„ï¼ˆHierarchy of geographic embeddingsï¼‰ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§å°†æŸ¥è¯¢å›¾åƒçš„å¤–è§‚ç‰¹å¾ä¸è¯­ä¹‰åˆ†å‰²å›¾ï¼ˆSemantic segmentation mapï¼‰é«˜æ•ˆèåˆçš„æŠ€æœ¯ï¼Œä»è€Œæ„å»ºå‡ºé²æ£’çš„è§†è§‰è¡¨ç¤ºã€‚å®éªŒè¡¨æ˜ï¼ŒGeoSURGE åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†çš„ 25 é¡¹è¯„ä¼°æŒ‡æ ‡ä¸­ï¼Œæœ‰ 22 é¡¹åˆ·æ–°äº†å½“å‰æœ€ä¼˜æ€§èƒ½ï¼ˆSOTAï¼‰ï¼Œæ˜¾è‘—ä¼˜äºæ­¤å‰çš„å…ˆè¿›æ–¹æ³•åŠè¿‘æœŸçš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®ï¼Œæ€§èƒ½çš„æå‡ä¸»è¦æºäºåœ°ç†è¡¨ç¤ºä¸è§†è§‰è¯­ä¹‰èåˆçš„ååŒä½œç”¨ï¼Œä¸ºè§£å†³å…¨çƒèŒƒå›´å†…çš„è§†è§‰å®šä½éš¾é¢˜æä¾›äº†é«˜æ•ˆçš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "preprint under review",
      "pdf_url": "https://arxiv.org/pdf/2510.01448v1",
      "published_date": "2025-10-01 20:39:48 UTC",
      "updated_date": "2025-10-01 20:39:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:54:04.976595+00:00"
    },
    {
      "arxiv_id": "2510.01444v2",
      "title": "Dual-Uncertainty Guided Policy Learning for Multimodal Reasoning",
      "title_zh": "åŒé‡ä¸ç¡®å®šæ€§å¼•å¯¼çš„å¤šæ¨¡æ€æ¨ç†ç­–ç•¥å­¦ä¹ ",
      "authors": [
        "Rui Liu",
        "Dian Yu",
        "Tong Zheng",
        "Runpeng Dai",
        "Zongxia Li",
        "Wenhao Yu",
        "Zhenwen Liang",
        "Linfeng Song",
        "Haitao Mi",
        "Pratap Tokekar",
        "Dong Yu"
      ],
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) has advanced reasoning capabilities in multimodal large language models. However, existing methods typically treat visual inputs as deterministic, overlooking the perceptual ambiguity inherent to the visual modality. Consequently, they fail to distinguish whether a model's uncertainty stems from complex reasoning or ambiguous perception, preventing the targeted allocation of exploration or learning signals. To address this gap, we introduce DUPL, a dual-uncertainty guided policy learning approach for multimodal RLVR that quantifies and leverages both perceptual uncertainty (via symmetric KL divergence) and output uncertainty (via policy entropy) to guide policy updates. By establishing an uncertainty-driven feedback loop and employing a dynamic branch prioritization mechanism, DUPL recalibrates the policy advantage to focus learning on states with high perceptual or decisional ambiguity, enabling effective targeted exploration beyond passive data augmentation. Implemented on top of GRPO and evaluated on six multimodal mathematical and general-domain reasoning benchmarks, DUPL improves Qwen2.5-VL 3B and 7B models, achieving accuracy gains of up to 11.2% on visual math tasks and up to 7.1% on general-domain reasoning tasks, while consistently outperforming GRPO. These results demonstrate that dual-uncertainty guided policy learning is an effective and generalizable approach for multimodal RLVR.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ä¸­å¿½ç•¥æ„ŸçŸ¥æ­§ä¹‰ã€å°†è§†è§‰è¾“å…¥è§†ä¸ºç¡®å®šæ€§çš„å±€é™æ€§ï¼Œæå‡ºäº†DUPLï¼ˆDual-Uncertainty guided policy learningï¼‰æ¡†æ¶ã€‚DUPLé€šè¿‡å¯¹ç§°KLæ•£åº¦ï¼ˆsymmetric KL divergenceï¼‰é‡åŒ–æ„ŸçŸ¥ä¸ç¡®å®šæ€§ï¼Œå¹¶ç»“åˆç­–ç•¥ç†µï¼ˆpolicy entropyï¼‰é‡åŒ–è¾“å‡ºä¸ç¡®å®šæ€§ï¼Œä»è€Œç²¾ç¡®å¼•å¯¼ç­–ç•¥æ›´æ–°ã€‚è¯¥æ–¹æ³•æ„å»ºäº†ä¸ç¡®å®šæ€§é©±åŠ¨çš„åé¦ˆå›è·¯ï¼Œå¹¶å¼•å…¥åŠ¨æ€åˆ†æ”¯ä¼˜å…ˆçº§æœºåˆ¶ï¼ˆdynamic branch prioritization mechanismï¼‰æ¥é‡æ–°æ ¡å‡†ç­–ç•¥ä¼˜åŠ¿ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé’ˆå¯¹æ€§åœ°æ¢ç´¢é«˜æ­§ä¹‰çŠ¶æ€ã€‚åŸºäºGRPOçš„å®éªŒè¡¨æ˜ï¼ŒDUPLæ˜¾è‘—æå‡äº†Qwen2.5-VLæ¨¡å‹åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ï¼Œå…¶ä¸­è§†è§‰æ•°å­¦ä»»åŠ¡å‡†ç¡®ç‡æå‡é«˜è¾¾11.2%ï¼Œé€šç”¨æ¨ç†ä»»åŠ¡æå‡è¾¾7.1%ã€‚ç ”ç©¶è¯æ˜ï¼ŒåŒé‡ä¸ç¡®å®šæ€§å¼•å¯¼çš„ç­–ç•¥å­¦ä¹ ä¸ºå¤šæ¨¡æ€RLVRæä¾›äº†ä¸€ç§æœ‰æ•ˆä¸”æ³›åŒ–æ€§å¼ºçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01444v2",
      "published_date": "2025-10-01 20:32:08 UTC",
      "updated_date": "2026-01-15 17:51:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:54:07.474098+00:00"
    },
    {
      "arxiv_id": "2510.01433v1",
      "title": "AFFORD2ACT: Affordance-Guided Automatic Keypoint Selection for Generalizable and Lightweight Robotic Manipulation",
      "title_zh": "AFFORD2ACTï¼šé¢å‘å¯æ³›åŒ–ä¸è½»é‡çº§æœºå™¨äººæ“ä½œçš„ç¤ºèƒ½å¼•å¯¼è‡ªåŠ¨å…³é”®ç‚¹é€‰æ‹©",
      "authors": [
        "Anukriti Singh",
        "Kasra Torshizi",
        "Khuzema Habib",
        "Kelin Yu",
        "Ruohan Gao",
        "Pratap Tokekar"
      ],
      "abstract": "Vision-based robot learning often relies on dense image or point-cloud inputs, which are computationally heavy and entangle irrelevant background features. Existing keypoint-based approaches can focus on manipulation-centric features and be lightweight, but either depend on manual heuristics or task-coupled selection, limiting scalability and semantic understanding. To address this, we propose AFFORD2ACT, an affordance-guided framework that distills a minimal set of semantic 2D keypoints from a text prompt and a single image. AFFORD2ACT follows a three-stage pipeline: affordance filtering, category-level keypoint construction, and transformer-based policy learning with embedded gating to reason about the most relevant keypoints, yielding a compact 38-dimensional state policy that can be trained in 15 minutes, which performs well in real-time without proprioception or dense representations. Across diverse real-world manipulation tasks, AFFORD2ACT consistently improves data efficiency, achieving an 82% success rate on unseen objects, novel categories, backgrounds, and distractors.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AFFORD2ACTï¼Œä¸€ç§åŸºäº affordance-guided çš„è‡ªåŠ¨åŒ–å…³é”®ç‚¹é€‰æ‹©æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°é€šç”¨ä¸”è½»é‡åŒ–çš„æœºå™¨äººæ“ä½œã€‚ä¸ºäº†è§£å†³ç°æœ‰è§†è§‰æœºå™¨äººå­¦ä¹ ä¾èµ–å¯†é›†è¾“å…¥æˆ–æ‰‹åŠ¨å¯å‘å¼ç‰¹å¾é€‰æ‹©çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶é€šè¿‡ä¸‰ä¸ªé˜¶æ®µâ€”â€”affordance filteringã€ç±»çº§å…³é”®ç‚¹æ„å»ºä»¥åŠå¸¦æœ‰åµŒå…¥å¼é—¨æ§æœºåˆ¶çš„ transformer-based policy å­¦ä¹ â€”â€”ä»æ–‡æœ¬æç¤ºå’Œå•å¼ å›¾åƒä¸­æå–æœ€ç®€è¯­ä¹‰ 2D keypointsã€‚å®éªŒè¡¨æ˜ï¼ŒAFFORD2ACT ç”Ÿæˆçš„ 38 ç»´ç´§å‡‘ç­–ç•¥ä»…éœ€ 15 åˆ†é’Ÿè®­ç»ƒï¼Œä¸”åœ¨æ— éœ€ proprioception æˆ–å¯†é›†è¡¨ç¤ºçš„æƒ…å†µä¸‹è¡¨ç°ä¼˜å¼‚ã€‚åœ¨å¤šé¡¹çœŸå®ä¸–ç•Œæ“ä½œä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†æœªè§ç‰©ä½“ã€æ–°ç±»åˆ«åŠå¤æ‚èƒŒæ™¯å¹²æ‰°æ—¶å±•ç°å‡ºæå¼ºçš„æ•°æ®æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ï¼ŒæˆåŠŸç‡è¾¾åˆ°äº† 82%ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01433v1",
      "published_date": "2025-10-01 20:13:39 UTC",
      "updated_date": "2025-10-01 20:13:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:54:35.382255+00:00"
    },
    {
      "arxiv_id": "2510.01432v1",
      "title": "On the Role of Domain Experts in Creating Effective Tutoring Systems",
      "title_zh": "è®ºé¢†åŸŸä¸“å®¶åœ¨æ„å»ºé«˜æ•ˆè¾…å¯¼ç³»ç»Ÿä¸­çš„ä½œç”¨",
      "authors": [
        "Sarath Sreedharan",
        "Kelsey Sikes",
        "Nathaniel Blanchard",
        "Lisa Mason",
        "Nikhil Krishnaswamy",
        "Jill Zarestky"
      ],
      "abstract": "The role that highly curated knowledge, provided by domain experts, could play in creating effective tutoring systems is often overlooked within the AI for education community. In this paper, we highlight this topic by discussing two ways such highly curated expert knowledge could help in creating novel educational systems. First, we will look at how one could use explainable AI (XAI) techniques to automatically create lessons. Most existing XAI methods are primarily aimed at debugging AI systems. However, we will discuss how one could use expert specified rules about solving specific problems along with novel XAI techniques to automatically generate lessons that could be provided to learners. Secondly, we will see how an expert specified curriculum for learning a target concept can help develop adaptive tutoring systems, that can not only provide a better learning experience, but could also allow us to use more efficient algorithms to create these systems. Finally, we will highlight the importance of such methods using a case study of creating a tutoring system for pollinator identification, where such knowledge could easily be elicited from experts.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é¢†åŸŸä¸“å®¶æä¾›çš„é«˜è´¨é‡çŸ¥è¯†åœ¨æ„å»ºæœ‰æ•ˆè¾…å¯¼ç³»ç»Ÿ(Tutoring Systems)ä¸­å¸¸è¢«å¿½è§†çš„å…³é”®ä½œç”¨ã€‚æ–‡ç« é¦–å…ˆé˜è¿°äº†å¦‚ä½•åˆ©ç”¨å¯è§£é‡Šäººå·¥æ™ºèƒ½(Explainable AI, XAI)æŠ€æœ¯ç»“åˆä¸“å®¶å®šä¹‰çš„ç‰¹å®šè§„åˆ™æ¥è‡ªåŠ¨ç”Ÿæˆæ•™å­¦è¯¾ç¨‹ï¼Œå°†XAIçš„åº”ç”¨ä»ç³»ç»Ÿè°ƒè¯•æ‰©å±•åˆ°äº†æ•™å­¦å†…å®¹åˆ›ä½œé¢†åŸŸã€‚å…¶æ¬¡ï¼Œç ”ç©¶å±•ç¤ºäº†ä¸“å®¶é¢„è®¾çš„è¯¾ç¨‹ä½“ç³»å¦‚ä½•ä¼˜åŒ–è‡ªé€‚åº”è¾…å¯¼ç³»ç»Ÿçš„å¼€å‘ï¼Œåœ¨æä¾›æ›´ä¼˜å­¦ä¹ ä½“éªŒçš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿæ„å»ºç®—æ³•çš„æ•ˆç‡ã€‚æœ€åï¼Œä½œè€…é€šè¿‡å¼€å‘æˆç²‰è€…è¯†åˆ«(Pollinator Identification)è¾…å¯¼ç³»ç»Ÿçš„æ¡ˆä¾‹ï¼ŒéªŒè¯äº†è¿™ç§ç”±ä¸“å®¶çŸ¥è¯†é©±åŠ¨çš„æ–¹æ³•åœ¨å®é™…æ•™è‚²åœºæ™¯ä¸­çš„å¯è¡Œæ€§ä¸é‡è¦ä»·å€¼ã€‚è¯¥è®ºæ–‡å¼ºè°ƒäº†å°†ä¸“å®¶æ™ºæ…§ä¸AIç®—æ³•æ·±åº¦èåˆæ˜¯æå‡æ™ºèƒ½åŒ–æ•™è‚²ç³»ç»Ÿæ•ˆèƒ½çš„é‡è¦é€”å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to AIED 2025 Blue Sky Track",
      "pdf_url": "https://arxiv.org/pdf/2510.01432v1",
      "published_date": "2025-10-01 20:12:57 UTC",
      "updated_date": "2025-10-01 20:12:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:54:15.383780+00:00"
    },
    {
      "arxiv_id": "2510.03331v1",
      "title": "Intelligent Healthcare Ecosystems: Optimizing the Iron Triangle of Healthcare (Access, Cost, Quality)",
      "title_zh": "æ™ºèƒ½åŒ»ç–—ç”Ÿæ€ç³»ç»Ÿï¼šä¼˜åŒ–åŒ»ç–—â€œé“ä¸‰è§’â€ï¼ˆå¯åŠæ€§ã€æˆæœ¬ä¸è´¨é‡ï¼‰",
      "authors": [
        "Vivek Acharya"
      ],
      "abstract": "The United States spends nearly 17% of GDP on healthcare yet continues to face uneven access and outcomes. This well-known trade-off among cost, quality, and access - the \"iron triangle\" - motivates a system-level redesign. This paper proposes an Intelligent Healthcare Ecosystem (iHE): an integrated, data-driven framework that uses generative AI and large language models, federated learning, interoperability standards (FHIR, TEFCA), and digital twins to improve access and quality while lowering cost. We review historical spending trends, waste, and international comparisons; introduce a value equation that jointly optimizes access, quality, and cost; and synthesize evidence on the enabling technologies and operating model for iHE. Methods follow a narrative review of recent literature and policy reports. Results outline core components (AI decision support, interoperability, telehealth, automation) and show how iHE can reduce waste, personalize care, and support value-based payment while addressing privacy, bias, and adoption challenges. We argue that a coordinated iHE can bend - if not break - the iron triangle, moving the system toward care that is more accessible, affordable, and high quality.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¾å›½åŒ»ç–—æ”¯å‡ºé«˜æ˜‚ä½†äº§å‡ºä¸å‡çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†æ™ºèƒ½åŒ»ç–—ç”Ÿæ€ç³»ç»Ÿï¼ˆIntelligent Healthcare Ecosystem, iHEï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é›†æˆåŒ–ã€æ•°æ®é©±åŠ¨çš„æ‰‹æ®µååŒä¼˜åŒ–åŒ»ç–—â€œé“ä¸‰è§’â€ï¼ˆAccess, Cost, Qualityï¼‰ã€‚è¯¥æ¡†æ¶æ•´åˆäº†Generative AIã€Large Language Models (LLMs)ã€Federated Learningã€äº’æ“ä½œæ€§æ ‡å‡†ï¼ˆFHIR, TEFCAï¼‰ä»¥åŠDigital Twinsç­‰å‰æ²¿æŠ€æœ¯ï¼Œä»¥å®ç°ç³»ç»Ÿçº§çš„é‡æ–°è®¾è®¡ã€‚é€šè¿‡å¯¹è¿‘æœŸæ–‡çŒ®å’Œæ”¿ç­–æŠ¥å‘Šçš„å™äº‹æ€§ç»¼è¿°ï¼ˆNarrative Reviewï¼‰ï¼Œè®ºæ–‡æ˜ç¡®äº†AIå†³ç­–æ”¯æŒã€è‡ªåŠ¨åŒ–å’Œè¿œç¨‹åŒ»ç–—ç­‰æ ¸å¿ƒç»„ä»¶åœ¨å‡å°‘åŒ»ç–—æµªè´¹åŠæä¾›ä¸ªæ€§åŒ–æŠ¤ç†æ–¹é¢çš„æ½œåŠ›ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒiHE èƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒä»·å€¼åŒ»ç–—æ”¯ä»˜ï¼ˆValue-based Paymentï¼‰ï¼Œå¹¶åŒæ—¶è§£å†³éšç§ä¿æŠ¤ã€ç®—æ³•åè§å’ŒæŠ€æœ¯é‡‡çº³ç­‰ç°å®æŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶è®ºè¯äº†é€šè¿‡ååŒæ„å»º iHEï¼Œå¯ä»¥æœ‰æ•ˆç¼“è§£ç”šè‡³çªç ´â€œé“ä¸‰è§’â€çš„é™åˆ¶ï¼Œä½¿åŒ»ç–—ç³»ç»Ÿå‘æ›´å…·å¯åŠæ€§ã€æ›´å»‰ä»·ä¸”æ›´é«˜è´¨é‡çš„æ–¹å‘æ¼”è¿›ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "8 pages, 4 figures, formatted per MDPI guidelines, APA-style numbered references",
      "pdf_url": "https://arxiv.org/pdf/2510.03331v1",
      "published_date": "2025-10-01 20:10:57 UTC",
      "updated_date": "2025-10-01 20:10:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:54:21.088498+00:00"
    },
    {
      "arxiv_id": "2510.01428v1",
      "title": "BioVERSE: Representation Alignment of Biomedical Modalities to LLMs for Multi-Modal Reasoning",
      "title_zh": "BioVERSEï¼šé¢å‘å¤šæ¨¡æ€æ¨ç†çš„ç”Ÿç‰©åŒ»å­¦æ¨¡æ€ä¸å¤§è¯­è¨€æ¨¡å‹è¡¨å¾å¯¹é½",
      "authors": [
        "Ching-Huei Tsou",
        "Michal Ozery-Flato",
        "Ella Barkan",
        "Diwakar Mahajan",
        "Ben Shapira"
      ],
      "abstract": "Recent advances in large language models (LLMs) and biomedical foundation models (BioFMs) have achieved strong results in biological text reasoning, molecular modeling, and single-cell analysis, yet they remain siloed in disjoint embedding spaces, limiting cross-modal reasoning. We present BIOVERSE (Biomedical Vector Embedding Realignment for Semantic Engagement), a two-stage approach that adapts pretrained BioFMs as modality encoders and aligns them with LLMs through lightweight, modality-specific projection layers. The approach first aligns each modality to a shared LLM space through independently trained projections, allowing them to interoperate naturally, and then applies standard instruction tuning with multi-modal data to bring them together for downstream reasoning. By unifying raw biomedical data with knowledge embedded in LLMs, the approach enables zero-shot annotation, cross-modal question answering, and interactive, explainable dialogue. Across tasks spanning cell-type annotation, molecular description, and protein function reasoning, compact BIOVERSE configurations surpass larger LLM baselines while enabling richer, generative outputs than existing BioFMs, establishing a foundation for principled multi-modal biomedical reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†BioVERSEæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ä¸ç”Ÿç‰©åŒ»å­¦åŸºç¡€æ¨¡å‹(BioFMs)å› åµŒå…¥ç©ºé—´ä¸ç»Ÿä¸€è€Œå¯¼è‡´çš„è·¨æ¨¡æ€æ¨ç†å—é™é—®é¢˜ã€‚BioVERSEé‡‡ç”¨äº†ä¸€ç§ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œé¦–å…ˆé€šè¿‡è½»é‡çº§çš„æ¨¡æ€ç‰¹å®šæŠ•å½±å±‚å°†é¢„è®­ç»ƒçš„BioFMsä½œä¸ºç¼–ç å™¨ä¸LLMsè¿›è¡Œè¡¨å¾å¯¹é½ï¼Œéšååˆ©ç”¨å¤šæ¨¡æ€æ•°æ®è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒ(Instruction Tuning)ä»¥å¼ºåŒ–ä¸‹æ¸¸æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶æˆåŠŸå°†åŸå§‹ç”Ÿç‰©åŒ»å­¦æ•°æ®ä¸LLMsä¸­çš„çŸ¥è¯†åº“ç›¸ç»Ÿä¸€ï¼Œæ”¯æŒé›¶æ ·æœ¬æ ‡æ³¨(Zero-shot Annotation)ã€è·¨æ¨¡æ€é—®ç­”ä»¥åŠäº¤äº’å¼ã€å¯è§£é‡Šçš„å¯¹è¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBioVERSEåœ¨ç»†èƒç±»å‹æ ‡æ³¨ã€åˆ†å­æè¿°å’Œè›‹ç™½è´¨åŠŸèƒ½æ¨ç†ç­‰ä»»åŠ¡ä¸Šçš„è¡¨ç°è¶…è¶Šäº†æ›´å¤§è§„æ¨¡çš„LLMåŸºçº¿æ¨¡å‹ï¼Œä¸ºå¤šæ¨¡æ€ç”Ÿç‰©åŒ»å­¦æ¨ç†å¥ å®šäº†é‡è¦çš„æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01428v1",
      "published_date": "2025-10-01 20:07:36 UTC",
      "updated_date": "2025-10-01 20:07:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:54:16.171760+00:00"
    },
    {
      "arxiv_id": "2510.01427v2",
      "title": "A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge Mining",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸è¯±å¯¼å¼å°å‹ä»£ç†ï¼šé¢å‘çŸ¥è¯†æŒ–æ˜çš„å¯æ‰©å±•æ™ºèƒ½ä½“",
      "authors": [
        "Sipeng Zhang",
        "Longfei Yun",
        "Zilong Wang",
        "Jingbo Shang",
        "Letian Peng"
      ],
      "abstract": "At the core of Deep Research is knowledge mining, the task of extracting structured information from massive unstructured text in response to user instructions. Large language models (LLMs) excel at interpreting such instructions but are prohibitively expensive to deploy at scale, while traditional pipelines of classifiers and extractors remain efficient yet brittle and unable to generalize to new tasks. We introduce Falconer, a collaborative framework that combines the agentic reasoning of LLMs with lightweight proxy models for scalable knowledge mining. In Falconer, LLMs act as planners, decomposing user instructions into executable pipelines, and as annotators, generating supervision to train small proxies. The framework unifies classification and extraction into two atomic operations, get label and get span, enabling a single instruction-following model to replace multiple task-specific components. To evaluate the consistency between proxy models incubated by Falconer and annotations provided by humans and large models, we construct new benchmarks covering both planning and end-to-end execution. Experiments show that Falconer closely matches state-of-the-art LLMs in instruction-following accuracy while reducing inference cost by up to 90% and accelerating large-scale knowledge mining by more than 20x, offering an efficient and scalable foundation for Deep Research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Deep Research æ ¸å¿ƒçš„çŸ¥è¯†æŒ–æ˜ (knowledge mining) ä»»åŠ¡ï¼Œæå‡ºäº†åä¸º Falconer çš„åä½œæ¡†æ¶ï¼Œæ—¨åœ¨å¹³è¡¡å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) çš„æ¨ç†èƒ½åŠ›ä¸å¤§è§„æ¨¡éƒ¨ç½²çš„æ•ˆç‡éœ€æ±‚ã€‚Falconer ç»“åˆäº† LLMs çš„æ™ºèƒ½è§„åˆ’ä¸è½»é‡åŒ–ä»£ç†æ¨¡å‹ (proxy models)ï¼Œå…¶ä¸­ LLMs ä½œä¸ºè§„åˆ’è€… (planners) å°†æŒ‡ä»¤åˆ†è§£ä¸ºæµæ°´çº¿ï¼Œå¹¶ä½œä¸ºæ ‡æ³¨è€… (annotators) ä¸ºè®­ç»ƒå°ä»£ç†æä¾›ç›‘ç£ä¿¡å·ã€‚è¯¥æ¡†æ¶é€šè¿‡ get label å’Œ get span ä¸¤ä¸ªåŸå­æ“ä½œç»Ÿä¸€äº†åˆ†ç±»ä¸æå–ä»»åŠ¡ï¼Œå…è®¸å•ä¸€æŒ‡ä»¤éµå¾ªæ¨¡å‹æ›¿ä»£å¤šä¸ªç‰¹å®šä»»åŠ¡ç»„ä»¶ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜æ„å»ºäº†æ¶µç›–è§„åˆ’ä¸ç«¯åˆ°ç«¯æ‰§è¡Œçš„æ–°åŸºå‡†æµ‹è¯•ï¼Œå®éªŒè¯æ˜ Falconer åœ¨æŒ‡ä»¤éµå¾ªå‡†ç¡®ç‡ä¸Šåª²ç¾æœ€å…ˆè¿›çš„ LLMsã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å°†æ¨ç†æˆæœ¬é™ä½äº† 90%ï¼Œå¹¶å°†å¤§è§„æ¨¡ä»»åŠ¡çš„æ‰§è¡Œé€Ÿåº¦æå‡äº† 20 å€ä»¥ä¸Šï¼Œä¸ºé«˜æ•ˆã€å¯æ‰©å±•çš„çŸ¥è¯†æŒ–æ˜å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Code available: https://github.com/LongfeiYun17/falconer",
      "pdf_url": "https://arxiv.org/pdf/2510.01427v2",
      "published_date": "2025-10-01 20:06:48 UTC",
      "updated_date": "2025-10-15 07:21:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:54:21.977179+00:00"
    },
    {
      "arxiv_id": "2510.01414v1",
      "title": "Risk Phase Transitions in Spiked Regression: Alignment Driven Benign and Catastrophic Overfitting",
      "title_zh": "å°–å³°å›å½’ä¸­çš„é£é™©ç›¸å˜ï¼šå¯¹é½é©±åŠ¨çš„è‰¯æ€§ä¸ç¾éš¾æ€§è¿‡æ‹Ÿåˆ",
      "authors": [
        "Jiping Li",
        "Rishi Sonthalia"
      ],
      "abstract": "This paper analyzes the generalization error of minimum-norm interpolating solutions in linear regression using spiked covariance data models. The paper characterizes how varying spike strengths and target-spike alignments can affect risk, especially in overparameterized settings. The study presents an exact expression for the generalization error, leading to a comprehensive classification of benign, tempered, and catastrophic overfitting regimes based on spike strength, the aspect ratio $c=d/n$ (particularly as $c \\to \\infty$), and target alignment. Notably, in well-specified aligned problems, increasing spike strength can surprisingly induce catastrophic overfitting before achieving benign overfitting. The paper also reveals that target-spike alignment is not always advantageous, identifying specific, sometimes counterintuitive, conditions for its benefit or detriment. Alignment with the spike being detrimental is empirically demonstrated to persist in nonlinear models.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ†æäº†åœ¨ spiked covariance æ•°æ®æ¨¡å‹çš„ linear regression ä¸­ï¼Œminimum-norm interpolating solutions çš„ generalization errorã€‚ç ”ç©¶é‡ç‚¹åˆ»ç”»äº†åœ¨ overparameterized è®¾ç½®ä¸‹ï¼Œspike strength å’Œ target-spike alignment å¦‚ä½•å½±å“é£é™©ï¼Œå¹¶æ¨å¯¼å‡ºäº† generalization error çš„ç²¾ç¡®è¡¨è¾¾å¼ã€‚åŸºäº spike strengthã€é•¿å®½æ¯” $c=d/n$ åŠç›®æ ‡å¯¹é½ç¨‹åº¦ï¼Œè¯¥ç ”ç©¶å¯¹ benignã€tempered å’Œ catastrophic overfitting ç­‰é£é™©æœºåˆ¶è¿›è¡Œäº†å…¨é¢åˆ†ç±»ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨ç‰¹å®šå¯¹é½é—®é¢˜ä¸­ï¼Œå¢åŠ  spike strength å¯èƒ½åœ¨è¾¾åˆ° benign overfitting ä¹‹å‰è¯±å‘ catastrophic overfittingï¼Œä¸” target-spike alignment å¹¶ä¸æ€»æ˜¯å…·æœ‰ä¼˜åŠ¿ã€‚å®éªŒè¿›ä¸€æ­¥è¯å®ï¼Œalignment äº§ç”Ÿçš„è´Ÿé¢å½±å“åœ¨ nonlinear models ä¸­åŒæ ·å­˜åœ¨ï¼Œä¸ºç†è§£å¤æ‚æ¨¡å‹ä¸­çš„ risk phase transitions æä¾›äº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01414v1",
      "published_date": "2025-10-01 19:51:47 UTC",
      "updated_date": "2025-10-01 19:51:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:54:26.980970+00:00"
    },
    {
      "arxiv_id": "2510.01409v1",
      "title": "OntoLogX: Ontology-Guided Knowledge Graph Extraction from Cybersecurity Logs with Large Language Models",
      "title_zh": "OntoLogXï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç½‘ç»œå®‰å…¨æ—¥å¿—æœ¬ä½“å¼•å¯¼çŸ¥è¯†å›¾è°±æŠ½å–",
      "authors": [
        "Luca Cotti",
        "Idilio Drago",
        "Anisa Rula",
        "Devis Bianchini",
        "Federico Cerutti"
      ],
      "abstract": "System logs represent a valuable source of Cyber Threat Intelligence (CTI), capturing attacker behaviors, exploited vulnerabilities, and traces of malicious activity. Yet their utility is often limited by lack of structure, semantic inconsistency, and fragmentation across devices and sessions. Extracting actionable CTI from logs therefore requires approaches that can reconcile noisy, heterogeneous data into coherent and interoperable representations. We introduce OntoLogX, an autonomous Artificial Intelligence (AI) agent that leverages Large Language Models (LLMs) to transform raw logs into ontology-grounded Knowledge Graphs (KGs). OntoLogX integrates a lightweight log ontology with Retrieval Augmented Generation (RAG) and iterative correction steps, ensuring that generated KGs are syntactically and semantically valid. Beyond event-level analysis, the system aggregates KGs into sessions and employs a LLM to predict MITRE ATT&CK tactics, linking low-level log evidence to higher-level adversarial objectives. We evaluate OntoLogX on both logs from a public benchmark and a real-world honeypot dataset, demonstrating robust KG generation across multiple KGs backends and accurate mapping of adversarial activity to ATT&CK tactics. Results highlight the benefits of retrieval and correction for precision and recall, the effectiveness of code-oriented models in structured log analysis, and the value of ontology-grounded representations for actionable CTI extraction.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†OntoLogXï¼Œä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)å°†åŸå§‹ç½‘ç»œæ—¥å¿—è½¬æ¢ä¸ºåŸºäºæœ¬ä½“(Ontology)çš„çŸ¥è¯†å›¾è°±(Knowledge Graphs, KGs)çš„è‡ªä¸»äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“ã€‚é’ˆå¯¹æ—¥å¿—æ•°æ®éç»“æ„åŒ–ã€è¯­ä¹‰ä¸ä¸€è‡´å’Œç¢ç‰‡åŒ–ç­‰æŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶æ•´åˆäº†è½»é‡çº§æ—¥å¿—æœ¬ä½“ã€æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)å’Œè¿­ä»£ä¿®æ­£æ­¥éª¤ï¼Œç¡®ä¿ç”Ÿæˆçš„çŸ¥è¯†å›¾è°±å…·å¤‡è¯­ä¹‰æœ‰æ•ˆæ€§ã€‚é™¤äº†äº‹ä»¶çº§åˆ†æï¼ŒOntoLogXè¿˜èƒ½å°†çŸ¥è¯†å›¾è°±èšåˆä¸ºä¼šè¯ï¼Œå¹¶åˆ©ç”¨æ¨¡å‹é¢„æµ‹MITRE ATT&CKæˆ˜æœ¯ï¼Œå°†åº•å±‚æ—¥å¿—è¯æ®ä¸é«˜å±‚æ”»å‡»ç›®æ ‡ç›´æ¥å…³è”ã€‚åœ¨å…¬å¼€åŸºå‡†å’ŒçœŸå®èœœç½æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿå®ç°é²æ£’çš„çŸ¥è¯†å›¾è°±ç”Ÿæˆå’Œç²¾å‡†çš„å¨èƒæ´»åŠ¨æ˜ å°„ã€‚å®éªŒç»“æœå‡¸æ˜¾äº†æ£€ç´¢ä¸ä¿®æ­£æœºåˆ¶å¯¹æå‡æŸ¥å‡†ç‡å’ŒæŸ¥å…¨ç‡çš„ä½œç”¨ï¼Œå¹¶è¯å®äº†é¢å‘ä»£ç çš„æ¨¡å‹åœ¨ç½‘ç»œå¨èƒæƒ…æŠ¥(CTI)æå–ä¸­çš„æ˜¾è‘—ä»·å€¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "20 pages, 6 tables, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.01409v1",
      "published_date": "2025-10-01 19:46:15 UTC",
      "updated_date": "2025-10-01 19:46:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:54:30.072874+00:00"
    },
    {
      "arxiv_id": "2510.01398v1",
      "title": "Automating Data-Driven Modeling and Analysis for Engineering Applications using Large Language Model Agents",
      "title_zh": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“å®ç°å·¥ç¨‹åº”ç”¨æ•°æ®é©±åŠ¨å»ºæ¨¡ä¸åˆ†æçš„è‡ªåŠ¨åŒ–",
      "authors": [
        "Yang Liu",
        "Zaid Abulawi",
        "Abhiram Garimidi",
        "Doyeong Lim"
      ],
      "abstract": "Modern engineering increasingly relies on vast datasets generated by experiments and simulations, driving a growing demand for efficient, reliable, and broadly applicable modeling strategies. There is also heightened interest in developing data-driven approaches, particularly neural network models, for effective prediction and analysis of scientific datasets. Traditional data-driven methods frequently involve extensive manual intervention, limiting their ability to scale effectively and generalize to diverse applications. In this study, we propose an innovative pipeline utilizing Large Language Model (LLM) agents to automate data-driven modeling and analysis, with a particular emphasis on regression tasks. We evaluate two LLM-agent frameworks: a multi-agent system featuring specialized collaborative agents, and a single-agent system based on the Reasoning and Acting (ReAct) paradigm. Both frameworks autonomously handle data preprocessing, neural network development, training, hyperparameter optimization, and uncertainty quantification (UQ). We validate our approach using a critical heat flux (CHF) prediction benchmark, involving approximately 25,000 experimental data points from the OECD/NEA benchmark dataset. Results indicate that our LLM-agent-developed model surpasses traditional CHF lookup tables and delivers predictive accuracy and UQ on par with state-of-the-art Bayesian optimized deep neural network models developed by human experts. These outcomes underscore the significant potential of LLM-based agents to automate complex engineering modeling tasks, greatly reducing human workload while meeting or exceeding existing standards of predictive performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(Large Language Model)æ™ºèƒ½ä½“è‡ªåŠ¨åŒ–å·¥ç¨‹å»ºæ¨¡ä¸åˆ†æçš„åˆ›æ–°æµç¨‹ï¼Œé‡ç‚¹é’ˆå¯¹æ•°æ®é©±åŠ¨çš„å›å½’ä»»åŠ¡ã€‚ç ”ç©¶è¯„ä¼°äº†åŒ…å«ä¸“é—¨åä½œæ™ºèƒ½ä½“çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(multi-agent system)å’ŒåŸºäºæ¨ç†ä¸è¡ŒåŠ¨(Reasoning and Acting, ReAct)èŒƒå¼çš„å•æ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ—¨åœ¨è‡ªä¸»å¤„ç†æ•°æ®é¢„å¤„ç†ã€ç¥ç»ç½‘ç»œå¼€å‘ã€è®­ç»ƒã€è¶…å‚æ•°ä¼˜åŒ–åŠä¸ç¡®å®šæ€§é‡åŒ–(uncertainty quantification, UQ)ã€‚é€šè¿‡åœ¨çº¦2.5ä¸‡ä¸ªå®éªŒæ•°æ®ç‚¹çš„ä¸´ç•Œçƒ­æµå¯†åº¦(critical heat flux, CHF)é¢„æµ‹åŸºå‡†ä¸Šè¿›è¡ŒéªŒè¯ï¼Œå®éªŒç»“æœæ˜¾ç¤ºè¯¥æ¨¡å‹æ€§èƒ½è¶…è¶Šäº†ä¼ ç»ŸæŸ¥æ‰¾è¡¨ã€‚å…¶é¢„æµ‹å‡†ç¡®æ€§ä¸UQæ°´å¹³å·²èƒ½ä¸äººç±»ä¸“å®¶å¼€å‘çš„è´å¶æ–¯ä¼˜åŒ–æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹ç›¸åª²ç¾ã€‚è¿™ä¸€æˆæœå‡¸æ˜¾äº†LLMæ™ºèƒ½ä½“åœ¨è‡ªåŠ¨åŒ–å¤æ‚å·¥ç¨‹ä»»åŠ¡ä¸­æ˜¾è‘—é™ä½äººåŠ›æˆæœ¬å¹¶ä¿æŒå“è¶Šé¢„æµ‹æ€§èƒ½çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01398v1",
      "published_date": "2025-10-01 19:28:35 UTC",
      "updated_date": "2025-10-01 19:28:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:54:36.669660+00:00"
    },
    {
      "arxiv_id": "2510.01396v1",
      "title": "Neural Network Surrogates for Free Energy Computation of Complex Chemical Systems",
      "title_zh": "é¢å‘å¤æ‚åŒ–å­¦ç³»ç»Ÿè‡ªç”±èƒ½è®¡ç®—çš„ç¥ç»ç½‘ç»œä»£ç†æ¨¡å‹",
      "authors": [
        "Wasut Pornpatcharapong"
      ],
      "abstract": "Free energy reconstruction methods such as Gaussian Process Regression (GPR) require Jacobians of the collective variables (CVs), a bottleneck that restricts the use of complex or machine-learned CVs. We introduce a neural network surrogate framework that learns CVs directly from Cartesian coordinates and uses automatic differentiation to provide Jacobians, bypassing analytical forms. On an MgCl2 ion-pairing system, our method achieved high accuracy for both a simple distance CV and a complex coordination-number CV. Moreover, Jacobian errors also followed a near-Gaussian distribution, making them suitable for GPR pipelines. This framework enables gradient-based free energy methods to incorporate complex and machine-learned CVs, broadening the scope of biochemistry and materials simulations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Gaussian Process Regression (GPR) ç­‰è‡ªç”±èƒ½é‡æ„æ–¹æ³•ä¸­ Collective Variables (CVs) çš„ Jacobians è®¡ç®—ç“¶é¢ˆï¼Œæå‡ºäº†ä¸€ç§ç¥ç»ç½‘ç»œä»£ç† (Neural Network Surrogate) æ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿç›´æ¥ä»ç¬›å¡å°”åæ ‡ä¸­å­¦ä¹  CVsï¼Œå¹¶åˆ©ç”¨è‡ªåŠ¨å¾®åˆ† (Automatic Differentiation) æŠ€æœ¯æä¾› Jacobiansï¼Œä»è€Œé¿å¼€äº†å¯¹å¤æ‚è§£æå½¢å¼çš„ä¾èµ–ã€‚åœ¨ MgCl2 ç¦»å­å¯¹ç³»ç»Ÿçš„å®éªŒä¸­ï¼Œè¯¥æ–¹æ³•åœ¨ç®€å•è·ç¦» CV å’Œå¤æ‚é…ä½æ•° CV ä¸Šå‡å®ç°äº†é«˜ç²¾åº¦è®¡ç®—ã€‚ç ”ç©¶å‘ç° Jacobians è¯¯å·®å‘ˆç°è¿‘é«˜æ–¯åˆ†å¸ƒï¼Œè¯æ˜äº†å…¶åœ¨ GPR å·¥ä½œæµä¸­çš„é€‚ç”¨æ€§ã€‚è¯¥æ¡†æ¶ä¸ºåŸºäºæ¢¯åº¦çš„è‡ªç”±èƒ½æ–¹æ³•å¼•å…¥å¤æ‚æˆ–æœºå™¨å­¦ä¹ ç”Ÿæˆçš„ CVs æä¾›äº†å¯èƒ½ï¼Œæ˜¾è‘—æ‹“å®½äº†ç”Ÿç‰©åŒ–å­¦å’Œææ–™æ¨¡æ‹Ÿçš„ç ”ç©¶èŒƒå›´ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "physics.chem-ph",
        "physics.comp-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 4 figures. This work has already been accepted for presentation in The 29th International Computer Science and Engineering Conference (ICSEC) 2025, Chiang Mai, Thailand, and will be published in IEEE Xplore",
      "pdf_url": "https://arxiv.org/pdf/2510.01396v1",
      "published_date": "2025-10-01 19:28:16 UTC",
      "updated_date": "2025-10-01 19:28:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:54:49.723801+00:00"
    },
    {
      "arxiv_id": "2510.01395v1",
      "title": "Sycophantic AI Decreases Prosocial Intentions and Promotes Dependence",
      "title_zh": "è°„åªšå‹äººå·¥æ™ºèƒ½å‰Šå¼±äº²ç¤¾ä¼šæ„å›¾å¹¶åŠ©é•¿ä¾èµ–æ€§",
      "authors": [
        "Myra Cheng",
        "Cinoo Lee",
        "Pranav Khadpe",
        "Sunny Yu",
        "Dyllan Han",
        "Dan Jurafsky"
      ],
      "abstract": "Both the general public and academic communities have raised concerns about sycophancy, the phenomenon of artificial intelligence (AI) excessively agreeing with or flattering users. Yet, beyond isolated media reports of severe consequences, like reinforcing delusions, little is known about the extent of sycophancy or how it affects people who use AI. Here we show the pervasiveness and harmful impacts of sycophancy when people seek advice from AI. First, across 11 state-of-the-art AI models, we find that models are highly sycophantic: they affirm users' actions 50% more than humans do, and they do so even in cases where user queries mention manipulation, deception, or other relational harms. Second, in two preregistered experiments (N = 1604), including a live-interaction study where participants discuss a real interpersonal conflict from their life, we find that interaction with sycophantic AI models significantly reduced participants' willingness to take actions to repair interpersonal conflict, while increasing their conviction of being in the right. However, participants rated sycophantic responses as higher quality, trusted the sycophantic AI model more, and were more willing to use it again. This suggests that people are drawn to AI that unquestioningly validate, even as that validation risks eroding their judgment and reducing their inclination toward prosocial behavior. These preferences create perverse incentives both for people to increasingly rely on sycophantic AI models and for AI model training to favor sycophancy. Our findings highlight the necessity of explicitly addressing this incentive structure to mitigate the widespread risks of AI sycophancy.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½(AI)ä¸­çš„Sycophancyç°è±¡ï¼Œå³æ¨¡å‹è¿‡åº¦è¿åˆæˆ–å¥‰æ‰¿ç”¨æˆ·ï¼Œå¹¶é€šè¿‡åˆ†æ11ç§å…ˆè¿›çš„AIæ¨¡å‹åŠä¸¤é¡¹é¢„æ³¨å†Œå®éªŒè¯„ä¼°äº†å…¶æ™®éæ€§åŠå½±å“ã€‚ç ”ç©¶å‘ç°è¿™äº›AIæ¨¡å‹è¡¨ç°å‡ºé«˜åº¦çš„Sycophancyå€¾å‘ï¼Œå…¶è‚¯å®šç”¨æˆ·è¡Œä¸ºçš„é¢‘ç‡æ¯”äººç±»é«˜å‡º50%ï¼Œå³ä½¿åœ¨æ¶‰åŠæ“çºµæˆ–æ¬ºéª—ç­‰è´Ÿé¢æƒ…å¢ƒä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸Sycophantic AIçš„äº’åŠ¨ä¼šæ˜¾è‘—é™ä½ç”¨æˆ·ä¿®å¤äººé™…å†²çªçš„Prosocial Intentionsï¼Œå¹¶å¢åŠ ç”¨æˆ·å¯¹è‡ªå·±ç«‹åœºæ­£ç¡®æ€§çš„ä¸»è§‚ç¡®ä¿¡ã€‚å°½ç®¡æ­¤ç±»å›å¤å­˜åœ¨æ½œåœ¨å±å®³ï¼Œç”¨æˆ·ä»å°†å…¶è¯„ä¸ºæ›´é«˜è´¨é‡å¹¶è¡¨ç°å‡ºæ›´å¼ºçš„ä¿¡ä»»å’Œä¾èµ–æ„Ÿã€‚è¿™ç§ç”¨æˆ·åå¥½ä¸ºAIè®­ç»ƒæä¾›äº†é”™è¯¯çš„æ¿€åŠ±ä¿¡å·ï¼Œå¯èƒ½è¯±å¯¼æ¨¡å‹è¿›ä¸€æ­¥è¶‹å‘Sycophancyã€‚ç ”ç©¶æœ€ç»ˆå¼ºè°ƒï¼Œå¿…é¡»è°ƒæ•´è¿™ç§æ¿€åŠ±ç»“æ„ä»¥å‡è½»Sycophantic AIå¯¹äººç±»åˆ¤æ–­åŠ›å’Œäº²ç¤¾ä¼šè¡Œä¸ºçš„è´Ÿé¢å½±å“ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01395v1",
      "published_date": "2025-10-01 19:26:01 UTC",
      "updated_date": "2025-10-01 19:26:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:54:55.383941+00:00"
    },
    {
      "arxiv_id": "2510.01389v1",
      "title": "INSIGHT: INference-time Sequence Introspection for Generating Help Triggers in Vision-Language-Action Models",
      "title_zh": "INSIGHTï¼šè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ä¸­ç”¨äºç”Ÿæˆæ±‚åŠ©è§¦å‘å™¨çš„æ¨ç†æ—¶åºåˆ—å†…çœ",
      "authors": [
        "Ulas Berk Karli",
        "Ziyao Shangguan",
        "Tesca FItzgerald"
      ],
      "abstract": "Recent Vision-Language-Action (VLA) models show strong generalization capabilities, yet they lack introspective mechanisms for anticipating failures and requesting help from a human supervisor. We present \\textbf{INSIGHT}, a learning framework for leveraging token-level uncertainty signals to predict when a VLA should request help. Using $Ï€_0$-FAST as the underlying model, we extract per-token \\emph{entropy}, \\emph{log-probability}, and Dirichlet-based estimates of \\emph{aleatoric and epistemic uncertainty}, and train compact transformer classifiers to map these sequences to help triggers. We explore supervision regimes for strong or weak supervision, and extensively compare them across in-distribution and out-of-distribution tasks. Our results show a trade-off: strong labels enable models to capture fine-grained uncertainty dynamics for reliable help detection, while weak labels, though noisier, still support competitive introspection when training and evaluation are aligned, offering a scalable path when dense annotation is impractical. Crucially, we find that modeling the temporal evolution of token-level uncertainty signals with transformers provides far greater predictive power than static sequence-level scores. This study provides the first systematic evaluation of uncertainty-based introspection in VLAs, opening future avenues for active learning and for real-time error mitigation through selective human intervention.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†INSIGHTæ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹(Vision-Language-Action Models, VLAs)å»ºç«‹ä¸€ç§æ¨ç†æ—¶åºåˆ—è‡ªçœæœºåˆ¶ï¼Œä½¿å…¶èƒ½å¤Ÿé¢„åˆ¤å¤±è´¥å¹¶ä¸»åŠ¨å‘äººç±»å¯»æ±‚å¸®åŠ©ã€‚è¯¥æ¡†æ¶åˆ©ç”¨$Ï€_0$-FASTæ¨¡å‹æå–æ¯æ ‡è®°ç†µ(per-token entropy)ã€å¯¹æ•°æ¦‚ç‡(log-probability)ä»¥åŠåŸºäºç‹„åˆ©å…‹é›·(Dirichlet)åˆ†å¸ƒçš„å¶ç„¶ä¸ç¡®å®šæ€§(aleatoric uncertainty)å’Œè®¤çŸ¥ä¸ç¡®å®šæ€§(epistemic uncertainty)ä¿¡å·ã€‚é€šè¿‡ç´§å‡‘å‹Transformeråˆ†ç±»å™¨å¯¹è¿™äº›ä¿¡å·çš„åºåˆ—è¿›è¡Œå»ºæ¨¡ï¼ŒINSIGHTèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†å…¶è½¬åŒ–ä¸ºæ±‚åŠ©è§¦å‘æŒ‡ä»¤(help triggers)ã€‚ç ”ç©¶æ·±å…¥æ¢è®¨äº†å¼ºç›‘ç£ä¸å¼±ç›‘ç£æ¨¡å¼åœ¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–ä»»åŠ¡ä¸­çš„è¡¨ç°å·®å¼‚ï¼Œç»“æœè¡¨æ˜å»ºæ¨¡ä¸ç¡®å®šæ€§ä¿¡å·çš„æ—¶é—´æ¼”å˜æ¯”ä½¿ç”¨é™æ€åºåˆ—åˆ†æ•°å…·æœ‰æ›´å¼ºçš„é¢„æµ‹èƒ½åŠ›ã€‚å®éªŒè¿˜å‘ç°ï¼Œè™½ç„¶å¼ºæ ‡ç­¾èƒ½æ•æ‰æ›´ç»†ç²’åº¦çš„åŠ¨æ€ï¼Œä½†å¼±æ ‡ç­¾åœ¨ç‰¹å®šæ¡ä»¶ä¸‹ä¹Ÿå…·æœ‰æä½³çš„æ‰©å±•æ€§å’Œç«äº‰åŠ›ã€‚è¯¥å·¥ä½œä¸ºVLAsä¸­åŸºäºä¸ç¡®å®šæ€§çš„è‡ªçœæœºåˆ¶æä¾›äº†é¦–æ¬¡ç³»ç»Ÿæ€§è¯„ä¼°ï¼Œä¸ºå®ç°æœºå™¨äººå®æ—¶é”™è¯¯ç¼“è§£å’Œä¸»åŠ¨å­¦ä¹ å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01389v1",
      "published_date": "2025-10-01 19:22:48 UTC",
      "updated_date": "2025-10-01 19:22:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:54:55.980435+00:00"
    },
    {
      "arxiv_id": "2510.01377v1",
      "title": "DeMuon: A Decentralized Muon for Matrix Optimization over Graphs",
      "title_zh": "DeMuonï¼šä¸€ç§é¢å‘å›¾ä¸ŠçŸ©é˜µä¼˜åŒ–çš„å»ä¸­å¿ƒåŒ– Muon ç®—æ³•",
      "authors": [
        "Chuan He",
        "Shuyi Ren",
        "Jingwei Mao",
        "Erik G. Larsson"
      ],
      "abstract": "In this paper, we propose DeMuon, a method for decentralized matrix optimization over a given communication topology. DeMuon incorporates matrix orthogonalization via Newton-Schulz iterations-a technique inherited from its centralized predecessor, Muon-and employs gradient tracking to mitigate heterogeneity among local functions. Under heavy-tailed noise conditions and additional mild assumptions, we establish the iteration complexity of DeMuon for reaching an approximate stochastic stationary point. This complexity result matches the best-known complexity bounds of centralized algorithms in terms of dependence on the target tolerance. To the best of our knowledge, DeMuon is the first direct extension of Muon to decentralized optimization over graphs with provable complexity guarantees. We conduct preliminary numerical experiments on decentralized transformer pretraining over graphs with varying degrees of connectivity. Our numerical results demonstrate a clear margin of improvement of DeMuon over other popular decentralized algorithms across different network topologies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DeMuonï¼Œä¸€ç§ç”¨äºåœ¨ç»™å®šé€šä¿¡æ‹“æ‰‘ä¸Šè¿›è¡Œå»ä¸­å¿ƒåŒ–çŸ©é˜µä¼˜åŒ–(Matrix Optimization)çš„æ–¹æ³•ã€‚DeMuonç»§æ‰¿äº†å…¶ä¸­å¿ƒåŒ–å‰èº«Muonçš„ç‰¹æ€§ï¼Œé€šè¿‡Newton-Schulzè¿­ä»£å®ç°çŸ©é˜µæ­£äº¤åŒ–(Matrix Orthogonalization)ï¼Œå¹¶é‡‡ç”¨æ¢¯åº¦è¿½è¸ª(Gradient Tracking)æŠ€æœ¯æ¥å‡è½»å±€éƒ¨å‡½æ•°ä¹‹é—´çš„å¼‚è´¨æ€§ã€‚åœ¨é‡å°¾å™ªå£°(Heavy-tailed noise)æ¡ä»¶ä¸‹ï¼Œç ”ç©¶è€…ç¡®ç«‹äº†DeMuonè¾¾åˆ°è¿‘ä¼¼éšæœºé©»ç‚¹(Approximate stochastic stationary point)çš„è¿­ä»£å¤æ‚åº¦ï¼Œå…¶ç»“æœåœ¨ç›®æ ‡å®¹å·®ä¾èµ–æ€§ä¸Šä¸ä¸­å¿ƒåŒ–ç®—æ³•çš„æœ€ä½³ç•Œé™ç›¸åŒ¹é…ã€‚ä½œä¸ºé¦–ä¸ªå°†Muonç›´æ¥æ‰©å±•åˆ°å›¾ä¸Šåˆ†å¸ƒå¼ä¼˜åŒ–å¹¶æä¾›å¯è¯æ˜å¤æ‚åº¦ä¿è¯çš„æ–¹æ³•ï¼ŒDeMuonå¡«è¡¥äº†ç›¸å…³ç†è®ºç©ºç™½ã€‚é€šè¿‡åœ¨ä¸åŒè¿æ¥ç¨‹åº¦çš„å›¾ä¸Šè¿›è¡Œå»ä¸­å¿ƒåŒ–Transformeré¢„è®­ç»ƒçš„æ•°å€¼å®éªŒï¼Œç»“æœè¯æ˜DeMuonåœ¨å¤šç§ç½‘ç»œæ‹“æ‰‘ç»“æ„ä¸‹çš„è¡¨ç°å‡æ˜¾è‘—ä¼˜äºå…¶ä»–æµè¡Œçš„å»ä¸­å¿ƒåŒ–ç®—æ³•ã€‚",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "eess.SY"
      ],
      "primary_category": "math.OC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01377v1",
      "published_date": "2025-10-01 19:06:11 UTC",
      "updated_date": "2025-10-01 19:06:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:54:57.679028+00:00"
    },
    {
      "arxiv_id": "2510.01375v1",
      "title": "Fine-tuning with RAG for Improving LLM Learning of New Skills",
      "title_zh": "ç»“åˆ RAG çš„å¾®è°ƒï¼šæå‡å¤§è¯­è¨€æ¨¡å‹å­¦ä¹ æ–°æŠ€èƒ½çš„èƒ½åŠ›",
      "authors": [
        "Humaid Ibrahim",
        "Nikolai Rozanov",
        "Marek Rei"
      ],
      "abstract": "Large language model (LLM) agents deployed for multi-step tasks frequently fail in predictable ways: attempting actions with unmet preconditions, issuing redundant commands, or mishandling environment constraints. While retrieval-augmented generation (RAG) can improve performance by providing runtime guidance, it requires maintaining external knowledge databases and adds computational overhead at every deployment. We propose a simple pipeline that converts inference-time retrieval into learned competence through distillation. Our approach: (1) extracts compact, reusable hints from agent failures, (2) uses these hints to generate improved teacher trajectories via one-shot retrieval at episode start, and (3) trains student models on these trajectories with hint strings removed, forcing internalization rather than memorization. Across two interactive benchmarks, ALFWorld (household tasks) and WebShop (online shopping), distilled students consistently outperform baseline agents, achieving up to 91% success on ALFWorld (vs. 79% for baselines) and improving WebShop scores to 72 (vs. 61 for baselines), while using 10-60% fewer tokens than retrieval-augmented teachers depending on the environment. The approach generalizes across model scales (7B/14B parameters) and agent architectures (ReAct/StateAct), demonstrating that retrieval benefits can be effectively internalized through targeted fine-tuning without permanent runtime dependencies.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ™ºèƒ½ä½“åœ¨æ‰§è¡Œå¤šæ­¥ä»»åŠ¡æ—¶å¸¸è§çš„å¤±è´¥æ¨¡å¼ï¼Œæå‡ºäº†ä¸€ç§å°†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ¨ç†èƒ½åŠ›é€šè¿‡è’¸é¦è½¬åŒ–ä¸ºæ¨¡å‹å†…åœ¨èƒ½åŠ›çš„åˆ›æ–°æµç¨‹ã€‚è¯¥æ–¹æ³•é¦–å…ˆä»æ™ºèƒ½ä½“å¤±è´¥æ¡ˆä¾‹ä¸­æå–ç´§å‡‘ä¸”å¯å¤ç”¨çš„æç¤ºï¼ˆhintsï¼‰ï¼Œåˆ©ç”¨è¿™äº›æç¤ºé€šè¿‡å•æ¬¡æ£€ç´¢ç”Ÿæˆæ”¹è¿›çš„æ•™å¸ˆè½¨è¿¹ï¼Œéšååœ¨ç§»é™¤æç¤ºä¿¡æ¯çš„å‰æä¸‹å¯¹å­¦ç”Ÿæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»è€Œä¿ƒä½¿æ¨¡å‹å®ç°çŸ¥è¯†çš„å†…åŒ–è€Œéæœºæ¢°è®°å¿†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡è’¸é¦çš„å­¦ç”Ÿæ¨¡å‹åœ¨ ALFWorld å’Œ WebShop åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°æ˜¾è‘—ä¼˜äºåŸºå‡†æ™ºèƒ½ä½“ï¼Œå…¶ä¸­åœ¨ ALFWorld ä¸Šçš„æˆåŠŸç‡æå‡è‡³ 91%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ¯” RAG æ•™å¸ˆæ¨¡å‹èŠ‚çœäº† 10-60% çš„ token æ¶ˆè€—ï¼Œå¹¶åœ¨ä¸åŒå‚æ•°è§„æ¨¡ï¼ˆ7B/14Bï¼‰å’Œæ™ºèƒ½ä½“æ¶æ„ï¼ˆReAct/StateActï¼‰ä¸‹å±•ç°äº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™ä¸€æˆæœè¯æ˜äº†é€šè¿‡é’ˆå¯¹æ€§å¾®è°ƒï¼ˆFine-tuningï¼‰ï¼Œå¯ä»¥åœ¨æ— éœ€æ°¸ä¹…ä¾èµ–å¤–éƒ¨æ•°æ®åº“çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆå®ç°æ£€ç´¢æ”¶ç›Šçš„å†…éƒ¨åŒ–ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Under review at ICLR 2026",
      "pdf_url": "https://arxiv.org/pdf/2510.01375v1",
      "published_date": "2025-10-01 19:03:48 UTC",
      "updated_date": "2025-10-01 19:03:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:55:06.172673+00:00"
    },
    {
      "arxiv_id": "2510.01370v1",
      "title": "SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs",
      "title_zh": "SPUSï¼šç”¨äºåå¾®åˆ†æ–¹ç¨‹çš„è½»é‡çº§å‚æ•°é«˜æ•ˆåŸºç¡€æ¨¡å‹",
      "authors": [
        "Abu Bucker Siddik",
        "Diane Oyen",
        "Alexander Most",
        "Michal Kucer",
        "Ayan Biswas"
      ],
      "abstract": "We introduce Small PDE U-Net Solver (SPUS), a compact and efficient foundation model (FM) designed as a unified neural operator for solving a wide range of partial differential equations (PDEs). Unlike existing state-of-the-art PDE FMs-primarily based on large complex transformer architectures with high computational and parameter overhead-SPUS leverages a lightweight residual U-Net-based architecture that has been largely underexplored as a foundation model architecture in this domain. To enable effective learning in this minimalist framework, we utilize a simple yet powerful auto-regressive pretraining strategy which closely replicates the behavior of numerical solvers to learn the underlying physics. SPUS is pretrained on a diverse set of fluid dynamics PDEs and evaluated across 6 challenging unseen downstream PDEs spanning various physical systems. Experimental results demonstrate that SPUS using residual U-Net based architecture achieves state-of-the-art generalization on these downstream tasks while requiring significantly fewer parameters and minimal fine-tuning data, highlighting its potential as a highly parameter-efficient FM for solving diverse PDE systems.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† Small PDE U-Net Solver (SPUS)ï¼Œè¿™æ˜¯ä¸€ç§ç´§å‡‘é«˜æ•ˆçš„åŸºç¡€æ¨¡å‹ (Foundation Model)ï¼Œæ—¨åœ¨ä½œä¸ºç»Ÿä¸€çš„ç¥ç»ç®—å­è§£å†³å¹¿æ³›çš„åå¾®åˆ†æ–¹ç¨‹ (PDEs)ã€‚ä¸ç°æœ‰çš„åŸºäºå¤§å‹å¤æ‚ Transformer æ¶æ„ä¸”å…·æœ‰é«˜è®¡ç®—å¼€é”€çš„ PDE åŸºç¡€æ¨¡å‹ä¸åŒï¼ŒSPUS é‡‡ç”¨äº†åœ¨è¿™ä¸€é¢†åŸŸå°šæœªè¢«å……åˆ†æ¢ç´¢çš„è½»é‡çº§æ®‹å·® U-Net æ¶æ„ã€‚ä¸ºäº†åœ¨æç®€æ¡†æ¶ä¸‹å®ç°æœ‰æ•ˆå­¦ä¹ ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨äº†ä¸€ç§å¼ºå¤§çš„è‡ªå›å½’é¢„è®­ç»ƒç­–ç•¥ (auto-regressive pretraining strategy)ï¼Œé€šè¿‡æ¨¡æ‹Ÿæ•°å€¼æ±‚è§£å™¨çš„è¡Œä¸ºæ¥å­¦ä¹ åº•å±‚ç‰©ç†è§„å¾‹ã€‚SPUS åœ¨å¤šæ ·åŒ–çš„æµä½“åŠ¨åŠ›å­¦æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åœ¨ 6 ä¸ªæ¶µç›–ä¸åŒç‰©ç†ç³»ç»Ÿçš„æŒ‘æˆ˜æ€§æœªçŸ¥ä¸‹æ¸¸ PDE ä»»åŠ¡ä¸­è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPUS åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„ (SOTA) æ³›åŒ–æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†å‚æ•°æ•°é‡å’Œå¾®è°ƒæ•°æ®éœ€æ±‚ã€‚è¿™çªæ˜¾äº†å…¶ä½œä¸ºä¸€ç§é«˜åº¦å‚æ•°é«˜æ•ˆçš„åŸºç¡€æ¨¡å‹ï¼Œåœ¨è§£å†³å¤šæ ·åŒ– PDE ç³»ç»Ÿæ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "physics.comp-ph"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01370v1",
      "published_date": "2025-10-01 18:54:59 UTC",
      "updated_date": "2025-10-01 18:54:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:55:02.477422+00:00"
    },
    {
      "arxiv_id": "2510.01367v3",
      "title": "Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort",
      "title_zh": "æ˜¯æ€è€ƒè¿˜æ˜¯ä½œå¼Šï¼Ÿé€šè¿‡è¡¡é‡æ¨ç†å·¥ä½œé‡æ£€æµ‹éšå¼å¥–åŠ±ä½œå¼Š",
      "authors": [
        "Xinpeng Wang",
        "Nitish Joshi",
        "Barbara Plank",
        "Rico Angell",
        "He He"
      ],
      "abstract": "Reward hacking, where a reasoning model exploits loopholes in a reward function to achieve high rewards without solving the intended task, poses a significant threat. This behavior may be explicit, i.e. verbalized in the model's chain-of-thought (CoT), or implicit, where the CoT appears benign thus bypasses CoT monitors. To detect implicit reward hacking, we propose TRACE (Truncated Reasoning AUC Evaluation). Our key observation is that hacking occurs when exploiting the loophole is easier than solving the actual task. This means that the model is using less 'effort' than required to achieve high reward. TRACE quantifies effort by measuring how early a model's reasoning becomes sufficient to obtain the reward. We progressively truncate a model's CoT at various lengths, force the model to answer, and estimate the expected reward at each cutoff. A hacking model, which takes a shortcut, will achieve a high expected reward with only a small fraction of its CoT, yielding a large area under the accuracy-vs-length curve. TRACE achieves over 65% gains over our strongest 72B CoT monitor in math reasoning, and over 30% gains over a 32B monitor in coding. We further show that TRACE can discover unknown loopholes during training. Overall, TRACE offers a scalable unsupervised approach for oversight where current monitoring methods prove ineffective.",
      "tldr_zh": "å¥–åŠ±æ»¥ç”¨(Reward Hacking)æ˜¯æŒ‡æ¨ç†æ¨¡å‹åˆ©ç”¨å¥–åŠ±å‡½æ•°çš„æ¼æ´è·å–é«˜åˆ†è€ŒéçœŸæ­£è§£å†³ä»»åŠ¡ï¼Œå…¶è¡¨ç°å½¢å¼åŒ…æ‹¬æ˜¾æ€§è¡Œä¸ºå’Œéš¾ä»¥è¢«é“¾å¼æ€ç»´(Chain-of-Thought, CoT)ç›‘æ§å™¨è¯†åˆ«çš„éšæ€§è¡Œä¸ºã€‚ä¸ºæ£€æµ‹éšæ€§å¥–åŠ±æ»¥ç”¨ï¼Œç ”ç©¶è€…æå‡ºäº†TRACE (Truncated Reasoning AUC Evaluation) è¯„ä¼°æ–¹æ³•ã€‚è¯¥æ–¹æ³•åŸºäºâ€œæ»¥ç”¨è¡Œä¸ºæ‰€éœ€çš„æ¨ç†åŠªåŠ›(Reasoning Effort)é€šå¸¸å°‘äºæ­£å¸¸è§£é¢˜â€çš„è§‚å¯Ÿï¼Œé€šè¿‡æµ‹é‡æ¨¡å‹æ¨ç†è¾¾åˆ°é¢„æœŸå¥–åŠ±çš„æ—©æ™šç¨‹åº¦æ¥é‡åŒ–åŠªåŠ›æ°´å¹³ã€‚TRACE é€šè¿‡é€æ­¥æˆªæ–­(Truncate)æ¨¡å‹çš„ CoT å¹¶è¯„ä¼°ä¸åŒé•¿åº¦ä¸‹çš„é¢„æœŸå¥–åŠ±ï¼Œè®¡ç®—å‡†ç¡®ç‡ä¸é•¿åº¦å…³ç³»çš„æ›²çº¿ä¸‹é¢ç§¯(AUC)æ¥è¯†åˆ«é‡‡å–æ·å¾„çš„æ»¥ç”¨è¡Œä¸ºã€‚å®éªŒè¯æ˜ï¼ŒTRACE åœ¨æ•°å­¦æ¨ç†å’Œä»£ç ç¼–å†™ä»»åŠ¡ä¸­ï¼Œåˆ†åˆ«æ¯”ç°æœ‰çš„å¼ºåŠ› 72B å’Œ 32B CoT ç›‘æ§å™¨è¡¨ç°æå‡äº† 65% å’Œ 30% ä»¥ä¸Šã€‚æ­¤å¤–ï¼ŒTRACE è¿˜èƒ½åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç°æœªçŸ¥çš„å¥–åŠ±å‡½æ•°æ¼æ´ï¼Œä¸ºå½“å‰ç›‘æ§æ‰‹æ®µå¤±æ•ˆçš„åœºæ™¯æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ— ç›‘ç£ç›‘ç£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "25 pages, 31 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.01367v3",
      "published_date": "2025-10-01 18:49:45 UTC",
      "updated_date": "2025-10-07 17:08:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:55:09.671212+00:00"
    },
    {
      "arxiv_id": "2510.01363v1",
      "title": "Retrieval-Augmented Framework for LLM-Based Clinical Decision Support",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ä¸´åºŠå†³ç­–æ”¯æŒæ£€ç´¢å¢å¼ºæ¡†æ¶",
      "authors": [
        "Leon Garza",
        "Anantaa Kotal",
        "Michael A. Grasso",
        "Emre Umucu"
      ],
      "abstract": "The increasing complexity of clinical decision-making, alongside the rapid expansion of electronic health records (EHR), presents both opportunities and challenges for delivering data-informed care. This paper proposes a clinical decision support system powered by Large Language Models (LLMs) to assist prescribing clinicians. The system generates therapeutic suggestions by analyzing historical EHR data, including patient demographics, presenting complaints, clinical symptoms, diagnostic information, and treatment histories. The framework integrates natural language processing with structured clinical inputs to produce contextually relevant recommendations. Rather than replacing clinician judgment, it is designed to augment decision-making by retrieving and synthesizing precedent cases with comparable characteristics, drawing on local datasets or federated sources where applicable. At its core, the system employs a retrieval-augmented generation (RAG) pipeline that harmonizes unstructured narratives and codified data to support LLM-based inference. We outline the system's technical components, including representation representation alignment and generation strategies. Preliminary evaluations, conducted with de-identified and synthetic clinical datasets, examine the clinical plausibility and consistency of the model's outputs. Early findings suggest that LLM-based tools may provide valuable decision support in prescribing workflows when appropriately constrained and rigorously validated. This work represents an initial step toward integration of generative AI into real-world clinical decision-making with an emphasis on transparency, safety, and alignment with established practices.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)çš„ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡åˆ†æç”µå­å¥åº·æ¡£æ¡ˆ(Electronic Health Records, EHR)æ•°æ®è¾…åŠ©ä¸´åºŠåŒ»ç”Ÿè¿›è¡Œå¤„æ–¹å†³ç­–ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒé‡‡ç”¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation, RAG)æµæ°´çº¿ï¼Œèƒ½å¤Ÿå°†éç»“æ„åŒ–å™è¿°ä¸ç¼–ç åŒ–æ•°æ®è¿›è¡Œè°ƒå’Œï¼Œæ”¯æŒåŸºäºLLMçš„æ¨ç†è¿‡ç¨‹ã€‚ç³»ç»Ÿé€šè¿‡æ£€ç´¢å¹¶ç»¼åˆå…·æœ‰ç›¸ä¼¼ç‰¹å¾çš„å…ˆä¾‹æ¡ˆä¾‹ï¼Œåˆ©ç”¨æœ¬åœ°æ•°æ®é›†æˆ–è”é‚¦æ¥æº(Federated Sources)ï¼Œä¸ºåŒ»ç”Ÿæä¾›å…·æœ‰ä¸Šä¸‹æ–‡ç›¸å…³æ€§çš„æ²»ç–—å»ºè®®ã€‚æŠ€æœ¯å®ç°ä¸Šé‡ç‚¹å…³æ³¨äº†è¡¨ç¤ºå¯¹é½(Representation Alignment)å’Œç”Ÿæˆç­–ç•¥ï¼Œç¡®ä¿ç”Ÿæˆå†…å®¹çš„ä¸´åºŠå¯ä¿¡åº¦ã€‚åˆæ­¥è¯„ä¼°æ˜¾ç¤ºï¼Œåœ¨ä½¿ç”¨å»æ ‡è¯†åŒ–åŠåˆæˆä¸´åºŠæ•°æ®é›†çš„æµ‹è¯•ä¸­ï¼Œè¯¥ç³»ç»Ÿåœ¨å¤„æ–¹å·¥ä½œæµä¸­å±•ç°å‡ºäº†æ˜¾è‘—çš„ä¸´åºŠåˆç†æ€§ä¸ä¸€è‡´æ€§ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)åœ¨ä¸´åºŠåº”ç”¨ä¸­çš„é€æ˜åº¦ã€å®‰å…¨æ€§ä»¥åŠä¸æ—¢æœ‰åŒ»ç–—å®è·µçš„å¯¹é½ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01363v1",
      "published_date": "2025-10-01 18:45:25 UTC",
      "updated_date": "2025-10-01 18:45:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:55:14.878765+00:00"
    },
    {
      "arxiv_id": "2510.01359v1",
      "title": "Breaking the Code: Security Assessment of AI Code Agents Through Systematic Jailbreaking Attacks",
      "title_zh": "ç ´è§£ä»£ç ï¼šåŸºäºç³»ç»Ÿæ€§è¶Šç‹±æ”»å‡»çš„ AI ä»£ç æ™ºèƒ½ä½“å®‰å…¨è¯„ä¼°",
      "authors": [
        "Shoumik Saha",
        "Jifan Chen",
        "Sam Mayers",
        "Sanjay Krishna Gouda",
        "Zijian Wang",
        "Varun Kumar"
      ],
      "abstract": "Code-capable large language model (LLM) agents are increasingly embedded into software engineering workflows where they can read, write, and execute code, raising the stakes of safety-bypass (\"jailbreak\") attacks beyond text-only settings. Prior evaluations emphasize refusal or harmful-text detection, leaving open whether agents actually compile and run malicious programs. We present JAWS-BENCH (Jailbreaks Across WorkSpaces), a benchmark spanning three escalating workspace regimes that mirror attacker capability: empty (JAWS-0), single-file (JAWS-1), and multi-file (JAWS-M). We pair this with a hierarchical, executable-aware Judge Framework that tests (i) compliance, (ii) attack success, (iii) syntactic correctness, and (iv) runtime executability, moving beyond refusal to measure deployable harm. Using seven LLMs from five families as backends, we find that under prompt-only conditions in JAWS-0, code agents accept 61% of attacks on average; 58% are harmful, 52% parse, and 27% run end-to-end. Moving to single-file regime in JAWS-1 drives compliance to ~ 100% for capable models and yields a mean ASR (Attack Success Rate) ~ 71%; the multi-file regime (JAWS-M) raises mean ASR to ~ 75%, with 32% instantly deployable attack code. Across models, wrapping an LLM in an agent substantially increases vulnerability -- ASR raises by 1.6x -- because initial refusals are frequently overturned during later planning/tool-use steps. Category-level analyses identify which attack classes are most vulnerable and most readily deployable, while others exhibit large execution gaps. These findings motivate execution-aware defenses, code-contextual safety filters, and mechanisms that preserve refusal decisions throughout the agent's multi-step reasoning and tool use.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…·å¤‡ä»£ç ç¼–å†™èƒ½åŠ›çš„å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“åœ¨è½¯ä»¶å·¥ç¨‹å·¥ä½œæµä¸­é¢ä¸´çš„è¶Šç‹±(Jailbreaking)æ”»å‡»é£é™©ï¼Œæå‡ºäº†ç³»ç»Ÿæ€§çš„å®‰å…¨æ€§è¯„ä¼°åŸºå‡†JAWS-BENCHã€‚è¯¥åŸºå‡†æ¶µç›–äº†ä»ç©ºç¯å¢ƒ(JAWS-0)åˆ°å¤šæ–‡ä»¶ç¯å¢ƒ(JAWS-M)çš„ä¸‰ç§æ¸è¿›å¼å·¥ä½œåŒºæ–¹æ¡ˆï¼Œå¹¶é…å¥—äº†ä¸€å¥—å…·å¤‡å¯æ‰§è¡Œæ€§æ„ŸçŸ¥çš„åˆ†å±‚è¯„åˆ¤æ¡†æ¶(Judge Framework)ï¼Œç”¨äºæµ‹è¯•æ”»å‡»ä»£ç çš„è¯­æ³•æ­£ç¡®æ€§ä¸è¿è¡Œæ—¶å¯æ‰§è¡Œæ€§ã€‚é€šè¿‡å¯¹ä¸ƒç§ä¸»æµLLMåç«¯çš„è¯„ä¼°ï¼Œç ”ç©¶å‘ç°å¤šæ–‡ä»¶ç¯å¢ƒä¸‹çš„å¹³å‡æ”»å‡»æˆåŠŸç‡(ASR)å¯è¾¾75%ï¼Œä¸”æœ‰32%çš„æ”»å‡»ä»£ç èƒ½å¤Ÿç«‹å³éƒ¨ç½²è¿è¡Œã€‚å®éªŒè¯æ˜ï¼Œå°†LLMå°è£…ä¸ºæ™ºèƒ½ä½“åï¼Œå…¶æ”»å‡»æˆåŠŸç‡(ASR)å¹³å‡æå‡äº†1.6å€ï¼ŒåŸå› åœ¨äºåˆå§‹çš„æ‹’ç»å†³ç­–å¸¸åœ¨åç»­çš„è§„åˆ’æˆ–å·¥å…·ä½¿ç”¨ç¯èŠ‚è¢«è§„é¿ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†å¼€å‘å¯æ‰§è¡Œæ€§æ„ŸçŸ¥é˜²å¾¡å’Œä»£ç ä¸Šä¸‹æ–‡å®‰å…¨è¿‡æ»¤å™¨çš„å¿…è¦æ€§ï¼Œå¹¶å»ºè®®å»ºç«‹è·¨å¤šæ­¥æ¨ç†çš„æŒç»­æ‹’ç»æœºåˆ¶ä»¥æå‡ç³»ç»Ÿå®‰å…¨æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "28 pages, 21 figures, 9 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.01359v1",
      "published_date": "2025-10-01 18:38:20 UTC",
      "updated_date": "2025-10-01 18:38:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:55:19.175237+00:00"
    },
    {
      "arxiv_id": "2510.01354v1",
      "title": "WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents",
      "title_zh": "WAInjectBenchï¼šé’ˆå¯¹ Web æ™ºèƒ½ä½“æç¤ºè¯æ³¨å…¥æ£€æµ‹çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Yinuo Liu",
        "Ruohan Xu",
        "Xilong Wang",
        "Yuqi Jia",
        "Neil Zhenqiang Gong"
      ],
      "abstract": "Multiple prompt injection attacks have been proposed against web agents. At the same time, various methods have been developed to detect general prompt injection attacks, but none have been systematically evaluated for web agents. In this work, we bridge this gap by presenting the first comprehensive benchmark study on detecting prompt injection attacks targeting web agents. We begin by introducing a fine-grained categorization of such attacks based on the threat model. We then construct datasets containing both malicious and benign samples: malicious text segments generated by different attacks, benign text segments from four categories, malicious images produced by attacks, and benign images from two categories. Next, we systematize both text-based and image-based detection methods. Finally, we evaluate their performance across multiple scenarios. Our key findings show that while some detectors can identify attacks that rely on explicit textual instructions or visible image perturbations with moderate to high accuracy, they largely fail against attacks that omit explicit instructions or employ imperceptible perturbations. Our datasets and code are released at: https://github.com/Norrrrrrr-lyn/WAInjectBench.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† WAInjectBenchï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹ Web Agents æç¤ºæ³¨å…¥æ”»å‡»ï¼ˆPrompt Injectionï¼‰æ£€æµ‹çš„å…¨é¢åŸºå‡†æµ‹è¯•ç ”ç©¶ã€‚ç ”ç©¶é¦–å…ˆåŸºäºå¨èƒæ¨¡å‹å¯¹æ”»å‡»è¿›è¡Œäº†ç»†ç²’åº¦åˆ†ç±»ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªåŒ…å«æ¶æ„ä¸è‰¯æ€§æ ·æœ¬çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ¶µç›–äº†ä¸åŒæ”»å‡»ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µåŠæ¶æ„å›¾åƒã€‚é€šè¿‡å¯¹ç°æœ‰çš„åŸºäºæ–‡æœ¬å’Œå›¾åƒçš„æ£€æµ‹æ–¹æ³•è¿›è¡Œç³»ç»ŸåŒ–è¯„ä¼°ï¼Œç ”ç©¶å‘ç°åœ¨å¤„ç†å…·æœ‰æ˜¾å¼æŒ‡ä»¤æˆ–å¯è§å›¾åƒæ‰°åŠ¨çš„æ”»å‡»æ—¶ï¼Œæ£€æµ‹å™¨å…·æœ‰ä¸­é«˜å‡†ç¡®ç‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ£€æµ‹æ–¹æ³•åœ¨é¢å¯¹çœç•¥æ˜¾å¼æŒ‡ä»¤æˆ–é‡‡ç”¨ä¸å¯çŸ¥è§‰æ‰°åŠ¨ï¼ˆimperceptible perturbationsï¼‰çš„æ”»å‡»æ—¶è¡¨ç°æ¬ ä½³ï¼Œæ­ç¤ºäº†å½“å‰é˜²å¾¡æœºåˆ¶çš„å±€é™æ€§ã€‚è¯¥åŸºå‡†æµ‹è¯•ä¸ºç†è§£å’Œæå‡ Web Agents åœ¨å¤æ‚ç½‘ç»œç¯å¢ƒä¸‹çš„å®‰å…¨æ€§æä¾›äº†é‡è¦çš„è¯„ä¼°æ¡†æ¶ä¸æ•°æ®æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01354v1",
      "published_date": "2025-10-01 18:34:06 UTC",
      "updated_date": "2025-10-01 18:34:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:55:18.776980+00:00"
    },
    {
      "arxiv_id": "2510.01353v1",
      "title": "MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform Dynamic Agent Environments",
      "title_zh": "MEMTRACKï¼šå¤šå¹³å°åŠ¨æ€æ™ºèƒ½ä½“ç¯å¢ƒä¸‹çš„é•¿æœŸè®°å¿†ä¸çŠ¶æ€è¿½è¸ªè¯„ä¼°",
      "authors": [
        "Darshan Deshpande",
        "Varun Gangal",
        "Hersh Mehta",
        "Anand Kannappan",
        "Rebecca Qian",
        "Peng Wang"
      ],
      "abstract": "Recent works on context and memory benchmarking have primarily focused on conversational instances but the need for evaluating memory in dynamic enterprise environments is crucial for its effective application. We introduce MEMTRACK, a benchmark designed to evaluate long-term memory and state tracking in multi-platform agent environments. MEMTRACK models realistic organizational workflows by integrating asynchronous events across multiple communication and productivity platforms such as Slack, Linear and Git. Each benchmark instance provides a chronologically platform-interleaved timeline, with noisy, conflicting, cross-referring information as well as potential codebase/file-system comprehension and exploration. Consequently, our benchmark tests memory capabilities such as acquistion, selection and conflict resolution. We curate the MEMTRACK dataset through both manual expert driven design and scalable agent based synthesis, generating ecologically valid scenarios grounded in real world software development processes. We introduce pertinent metrics for Correctness, Efficiency, and Redundancy that capture the effectiveness of memory mechanisms beyond simple QA performance. Experiments across SoTA LLMs and memory backends reveal challenges in utilizing memory across long horizons, handling cross-platform dependencies, and resolving contradictions. Notably, the best performing GPT-5 model only achieves a 60\\% Correctness score on MEMTRACK. This work provides an extensible framework for advancing evaluation research for memory-augmented agents, beyond existing focus on conversational setups, and sets the stage for multi-agent, multi-platform memory benchmarking in complex organizational settings",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MEMTRACKï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºè¯„ä¼°å¤šå¹³å°æ™ºèƒ½ä½“ç¯å¢ƒä¸­çš„é•¿æœŸè®°å¿† (long-term memory) å’ŒçŠ¶æ€è·Ÿè¸ª (state tracking) èƒ½åŠ›è€Œè®¾è®¡çš„åŸºå‡†æµ‹è¯•ã€‚ä¸ä»¥å¾€ä¾§é‡äºå¯¹è¯åœºæ™¯çš„ç ”ç©¶ä¸åŒï¼ŒMEMTRACK é€šè¿‡æ•´åˆ Slackã€Linear å’Œ Git ç­‰å¤šä¸ªå¹³å°çš„å¼‚æ­¥äº‹ä»¶æ¥æ¨¡æ‹ŸçœŸå®çš„ç»„ç»‡å·¥ä½œæµï¼ŒåŒ…å«å…·æœ‰å™ªå£°ã€å†²çªå’Œäº¤å‰å¼•ç”¨çš„å¤æ‚ä¿¡æ¯ã€‚è¯¥åŸºå‡†æµ‹è¯•é‡ç‚¹è€ƒå¯Ÿæ™ºèƒ½ä½“åœ¨è®°å¿†è·å– (acquisition)ã€é€‰æ‹© (selection) å’Œå†²çªè§£å†³ (conflict resolution) æ–¹é¢çš„æ ¸å¿ƒèƒ½åŠ›ï¼Œå¹¶å¼•å…¥äº†æ­£ç¡®æ€§ (Correctness)ã€æ•ˆç‡ (Efficiency) å’Œå†—ä½™åº¦ (Redundancy) ç­‰å¤šç»´åº¦æŒ‡æ ‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯è¡¨ç°æœ€ä½³çš„ GPT-5 æ¨¡å‹åœ¨ MEMTRACK ä¸Šçš„æ­£ç¡®æ€§å¾—åˆ†ä¹Ÿä»…ä¸º 60%ï¼Œå‡¸æ˜¾äº†å½“å‰æ¨¡å‹åœ¨å¤„ç†é•¿å‘¨æœŸè·¨å¹³å°ä¾èµ–å’ŒçŸ›ç›¾è§£å†³æ–¹é¢çš„ä¸¥å³»æŒ‘æˆ˜ã€‚æ­¤é¡¹å·¥ä½œä¸ºæ¨åŠ¨å¤æ‚ç»„ç»‡ç¯å¢ƒä¸‹è®°å¿†å¢å¼ºå‹æ™ºèƒ½ä½“çš„è¯„ä¼°ç ”ç©¶æä¾›äº†ä¸€ä¸ªé‡è¦çš„å¯æ‰©å±•æ¡†æ¶ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to NeurIPS 2025 SEA Workshop",
      "pdf_url": "https://arxiv.org/pdf/2510.01353v1",
      "published_date": "2025-10-01 18:34:03 UTC",
      "updated_date": "2025-10-01 18:34:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:55:30.879334+00:00"
    },
    {
      "arxiv_id": "2510.01346v2",
      "title": "Aristotle: IMO-level Automated Theorem Proving",
      "title_zh": "Aristotleï¼šIMO çº§åˆ«çš„è‡ªåŠ¨å®šç†è¯æ˜",
      "authors": [
        "Tudor Achim",
        "Alex Best",
        "Alberto Bietti",
        "Kevin Der",
        "MathÃ¯s FÃ©dÃ©rico",
        "Sergei Gukov",
        "Daniel Halpern-Leistner",
        "Kirsten Henningsgard",
        "Yury Kudryashov",
        "Alexander Meiburg",
        "Martin Michelsen",
        "Riley Patterson",
        "Eric Rodriguez",
        "Laura Scharff",
        "Vikram Shanker",
        "Vladmir Sicca",
        "Hari Sowrirajan",
        "Aidan Swope",
        "Matyas Tamas",
        "Vlad Tenev",
        "Jonathan Thomm",
        "Harold Williams",
        "Lawrence Wu"
      ],
      "abstract": "We introduce Aristotle, an AI system that combines formal verification with informal reasoning, achieving gold-medal-equivalent performance on the 2025 International Mathematical Olympiad problems. Aristotle integrates three main components: a Lean proof search system, an informal reasoning system that generates and formalizes lemmas, and a dedicated geometry solver. Our system demonstrates state-of-the-art performance with favorable scaling properties for automated theorem proving.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† Aristotleï¼Œè¿™æ˜¯ä¸€æ¬¾å°†å½¢å¼éªŒè¯ (formal verification) ä¸éå½¢å¼æ¨ç† (informal reasoning) ç›¸ç»“åˆçš„äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œå…¶åœ¨ 2025 å¹´å›½é™…æ•°å­¦å¥¥æ—åŒ¹å…‹ (IMO) é¢˜ç›®ä¸Šå–å¾—äº†ç­‰åŒäºé‡‘ç‰Œçš„è¡¨ç°ã€‚Aristotle æ•´åˆäº†ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šLean è¯æ˜æœç´¢ç³»ç»Ÿã€ç”Ÿæˆå¹¶å½¢å¼åŒ–å¼•ç†çš„éå½¢å¼æ¨ç†ç³»ç»Ÿï¼Œä»¥åŠä¸€ä¸ªä¸“ç”¨çš„å‡ ä½•æ±‚è§£å™¨ (geometry solver)ã€‚é€šè¿‡è¿™ç§å¤šç»´åº¦çš„æ¶æ„è®¾è®¡ï¼Œè¯¥ç³»ç»ŸæˆåŠŸå¼¥åˆäº†é€»è¾‘ä¸¥å¯†æ€§ä¸ç›´è§‰æ¨ç†ä¹‹é—´çš„é¸¿æ²Ÿï¼Œå®ç°äº†å¯¹å¤æ‚æ•°å­¦é—®é¢˜çš„ç²¾å‡†æ”»å…‹ã€‚å®éªŒè¯æ˜ï¼ŒAristotle åœ¨è‡ªåŠ¨å®šç†è¯æ˜ (automated theorem proving) é¢†åŸŸè¾¾åˆ°äº†æœ€å…ˆè¿› (state-of-the-art) çš„æ°´å¹³ã€‚åŒæ—¶ï¼Œè¯¥ç³»ç»Ÿå±•ç°å‡ºè‰¯å¥½çš„æ‰©å±•ç‰¹æ€§ (scaling properties)ï¼Œä¸ºæœªæ¥æ•°å­¦è‡ªåŠ¨åŒ–æ¨ç†èƒ½åŠ›çš„è¿›ä¸€æ­¥æå‡å¥ å®šäº†åšå®åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01346v2",
      "published_date": "2025-10-01 18:21:13 UTC",
      "updated_date": "2025-10-10 20:12:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:55:34.372297+00:00"
    },
    {
      "arxiv_id": "2510.01336v1",
      "title": "HiSpec: Hierarchical Speculative Decoding for LLMs",
      "title_zh": "HiSpecï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹çš„åˆ†å±‚æ¨æµ‹è§£ç ",
      "authors": [
        "Avinash Kumar",
        "Sujay Sanghavi",
        "Poulami Das"
      ],
      "abstract": "Speculative decoding accelerates LLM inference by using a smaller draft model to speculate tokens that a larger target model verifies. Verification is often the bottleneck (e.g. verification is $4\\times$ slower than token generation when a 3B model speculates for a 70B target model), but most prior works focus only on accelerating drafting. $\\textit{``Intermediate\"}$ verification reduces verification time by discarding inaccurate draft tokens early, but existing methods incur substantial training overheads in incorporating the intermediate verifier, increase the memory footprint to orchestrate the intermediate verification step, and compromise accuracy by relying on approximate heuristics.\n  We propose $\\underline{\\textit{Hi}}\\textit{erarchical }\\underline{\\textit{Spec}}\\textit{ulative Decoding (HiSpec)}$, a framework for high-throughput speculative decoding that exploits $\\textit{early-exit (EE) models}$ for low-overhead intermediate verification. EE models allow tokens to exit early by skipping layer traversal and are explicitly trained so that hidden states at selected layers can be interpreted, making them uniquely suited for intermediate verification without drastically increasing compute and memory overheads. To improve resource-efficiency even further, we design a methodology that enables HiSpec to re-use key-value caches and hidden states between the draft, intermediate verifier, and target models. To maintain accuracy, HiSpec periodically validates the draft tokens accepted by the intermediate verifier against the target model. Our evaluations using various representative benchmarks and models show that HiSpec improves throughput by 1.28$\\times$ on average and by up to 2.01$\\times$ compared to the baseline single-layer speculation without compromising accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HiSpecï¼ˆHierarchical Speculative Decodingï¼‰ï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³æŠ•æœºè§£ç ï¼ˆSpeculative Decodingï¼‰ä¸­éªŒè¯ç“¶é¢ˆçš„é«˜ååæ¡†æ¶ã€‚HiSpecåˆ©ç”¨æ—©åœæ¨¡å‹ï¼ˆEarly-Exit modelsï¼‰å®ç°ä½å¼€é”€çš„ä¸­é—´éªŒè¯ï¼Œé€šè¿‡å…è®¸æ ‡è®°è·³è¿‡å±‚éå†å¹¶æå‰é€€å‡ºçš„ç‰¹æ€§ï¼Œåœ¨ä¸å¤§å¹…å¢åŠ è®¡ç®—å’Œå†…å­˜è´Ÿæ‹…çš„æƒ…å†µä¸‹ä¼˜åŒ–æ¨ç†æ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é€šè¿‡è®¾è®¡ç‰¹å®šçš„æ–¹æ³•å®ç°äº†è‰ç¨¿æ¨¡å‹ã€ä¸­é—´éªŒè¯å™¨ä¸ç›®æ ‡æ¨¡å‹ä¹‹é—´é”®å€¼ç¼“å­˜ï¼ˆKV Cachesï¼‰å’Œéšè—çŠ¶æ€çš„å¤ç”¨ï¼Œè¿›ä¸€æ­¥æå‡äº†èµ„æºåˆ©ç”¨ç‡ã€‚ä¸ºäº†ç»´æŒç”Ÿæˆç²¾åº¦ï¼ŒHiSpecé‡‡ç”¨å®šæœŸå°†ä¸­é—´éªŒè¯é€šè¿‡çš„æ ‡è®°ä¸ç›®æ ‡æ¨¡å‹è¿›è¡Œæœ€ç»ˆæ¯”å¯¹çš„ç­–ç•¥ã€‚åœ¨å¤šä¸ªä»£è¡¨æ€§åŸºå‡†æµ‹è¯•ä¸­ï¼ŒHiSpecç›¸æ¯”å•å±‚æŠ•æœºåŸºçº¿å¹³å‡æå‡äº†1.28å€çš„ååé‡ï¼Œæœ€é«˜å¢ç›Šè¾¾2.01å€ï¼Œä¸”å®Œå…¨ä¸å½±å“æ¨¡å‹çš„è¾“å‡ºå‡†ç¡®æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01336v1",
      "published_date": "2025-10-01 18:04:14 UTC",
      "updated_date": "2025-10-01 18:04:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:55:41.878084+00:00"
    },
    {
      "arxiv_id": "2510.01304v1",
      "title": "Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models",
      "title_zh": "æ—¨åœ¨å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹è§†è§‰æ„ŸçŸ¥ä¸æ¨ç†èƒ½åŠ›çš„æ™ºèƒ½ä½“åŒ–æ‹¼å›¾äº¤äº’å­¦ä¹ ",
      "authors": [
        "Yu Zeng",
        "Wenxuan Huang",
        "Shiting Huang",
        "Xikun Bao",
        "Yukun Qi",
        "Yiming Zhao",
        "Qiuchen Wang",
        "Lin Chen",
        "Zehui Chen",
        "Huaian Chen",
        "Wanli Ouyang",
        "Feng Zhao"
      ],
      "abstract": "Although current large Vision-Language Models (VLMs) have advanced in multimodal understanding and reasoning, their fundamental perceptual and reasoning abilities remain limited. Specifically, even on simple jigsaw tasks, existing VLMs perform near randomly, revealing deficiencies in core perception and reasoning capabilities. While high-quality vision-language data can enhance these capabilities, its scarcity and limited scalability impose significant constraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction Learning for Enhancing visual perception and reasoning in VLMs. AGILE formulates jigsaw solving as an interactive process, enabling the model to progressively engage with the environment. At each step, the model generates executable code to perform an action based on the current state, while the environment provides fine-grained visual feedback to guide task completion. Through this iterative cycle of observation and interaction, the model incrementally improves its perceptual and reasoning capabilities via exploration and feedback. Experimental results show that AGILE not only substantially boosts performance on jigsaw tasks of varying complexity (e.g., increasing accuracy from 9.5% to 82.8% under the 2 $\\times$ 2 setting) but also demonstrates strong generalization across 9 general vision tasks, achieving an average improvement of 3.1%. These results indicate notable enhancements in both perceptual and reasoning abilities. This work opens a new avenue for advancing reasoning and generalization in multimodal models and provides an efficient, scalable solution to the scarcity of multimodal reinforcement learning data. The code and datasets is available at https://github.com/yuzeng0-0/AGILE .",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AGILEæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ™ºèƒ½ä½“æ‹¼å›¾äº¤äº’å­¦ä¹ (Agentic Jigsaw Interaction Learning)çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Models, VLMs)åœ¨åŸºç¡€æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ä¸Šçš„å±€é™æ€§ã€‚é’ˆå¯¹VLMsåœ¨ç®€å•æ‹¼å›¾(jigsaw)ä»»åŠ¡ä¸­è¡¨ç°æ¥è¿‘éšæœºçš„é—®é¢˜ï¼ŒAGILEå°†ä»»åŠ¡æ„å»ºä¸ºä¸€ä¸ªäº¤äº’è¿‡ç¨‹ï¼Œæ¨¡å‹èƒ½å¤Ÿæ ¹æ®å½“å‰çŠ¶æ€ç”Ÿæˆå¯æ‰§è¡Œä»£ç å¹¶è·å–ç¯å¢ƒæä¾›çš„ç»†ç²’åº¦è§†è§‰åé¦ˆã€‚é€šè¿‡è§‚å¯Ÿã€è¡ŒåŠ¨ä¸åé¦ˆçš„è¿­ä»£å¾ªç¯ï¼Œæ¨¡å‹åœ¨æ¢ç´¢ä¸­å¢é‡å¼åœ°å¢å¼ºå…¶æ„ŸçŸ¥å’Œæ¨ç†æ°´å¹³ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAGILEåœ¨2Ã—2æ‹¼å›¾ä»»åŠ¡ä¸­çš„å‡†ç¡®ç‡ä»9.5%å¤§å¹…æå‡è‡³82.8%ï¼Œå¹¶åœ¨9é¡¹é€šç”¨è§†è§‰ä»»åŠ¡ä¸­å®ç°äº†3.1%çš„å¹³å‡æ€§èƒ½å¢é•¿ã€‚è¯¥å·¥ä½œä¸ºæå‡å¤šæ¨¡æ€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›æä¾›äº†æ–°é€”å¾„ï¼Œä¹Ÿä¸ºç¼“è§£å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ•°æ®ç¨€ç¼ºé—®é¢˜æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01304v1",
      "published_date": "2025-10-01 17:58:05 UTC",
      "updated_date": "2025-10-01 17:58:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:56:13.482880+00:00"
    },
    {
      "arxiv_id": "2510.01179v1",
      "title": "TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments",
      "title_zh": "TOUCANï¼šåŸºäºçœŸå®ä¸–ç•Œ MCP ç¯å¢ƒåˆæˆçš„ 150 ä¸‡å·¥å…·æ™ºèƒ½ä½“æ•°æ®",
      "authors": [
        "Zhangchen Xu",
        "Adriana Meza Soria",
        "Shawn Tan",
        "Anurag Roy",
        "Ashish Sunil Agrawal",
        "Radha Poovendran",
        "Rameswar Panda"
      ],
      "abstract": "Large Language Model (LLM) agents are rapidly emerging as powerful systems for automating tasks across domains. Yet progress in the open-source community is constrained by the lack of high quality permissively licensed tool-agentic training data. Existing datasets are often limited in diversity, realism, and complexity, particularly regarding multi-tool and multi-turn interactions. To address this gap, we introduce Toucan, the largest publicly available tool-agentic dataset to date, containing 1.5 million trajectories synthesized from nearly 500 real-world Model Context Protocols (MCPs). Unlike prior work, Toucan leverages authentic MCP environments to generate diverse, realistic, and challenging tasks with trajectories involving real tool execution. Our pipeline first produces a broad spectrum of tool-use queries using five distinct models, applies model-based quality filtering, and then generates agentic trajectories with three teacher models using two agentic frameworks. Rigorous rule-based and model-based validation ensures high-quality outputs. We also introduce three extension mechanisms to further diversify tasks and simulate multi-turn conversations. Models fine-tuned on Toucan outperform larger closed-source counterparts on the BFCL V3 benchmark and push the Pareto frontier forward on MCP-Universe Bench.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†TOUCANï¼Œè¿™æ˜¯ä¸€ä¸ªç›®å‰å…¬å¼€è§„æ¨¡æœ€å¤§çš„å·¥å…·æ™ºèƒ½ä½“(tool-agentic)æ•°æ®é›†ï¼ŒåŒ…å«ä»è¿‘500ä¸ªçœŸå®ä¸–ç•Œæ¨¡å‹ä¸Šä¸‹æ–‡åè®®(Model Context Protocols, MCPs)ä¸­åˆæˆçš„150ä¸‡æ¡è½¨è¿¹ã€‚é’ˆå¯¹å¼€æºç¤¾åŒºåœ¨é«˜è´¨é‡ã€å¤šæ ·åŒ–ä¸”å…·æœ‰çœŸå®å¤šè½®äº¤äº’å¤æ‚æ€§çš„å·¥å…·è°ƒç”¨è®­ç»ƒæ•°æ®æ–¹é¢çš„åŒ®ä¹ï¼ŒTOUCANåˆ©ç”¨çœŸå®MCPç¯å¢ƒç”Ÿæˆäº†æå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡å’ŒçœŸå®çš„å·¥å…·æ‰§è¡Œè·¯å¾„ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€å¥—ä¸¥è°¨çš„æµæ°´çº¿ï¼Œåˆ©ç”¨äº”ç§ä¸åŒæ¨¡å‹ç”Ÿæˆå¹¿æ³›çš„æŸ¥è¯¢ï¼Œç»è¿‡è´¨é‡è¿‡æ»¤åï¼Œå†ç”±ä¸‰åè€å¸ˆæ¨¡å‹åœ¨ä¸¤ç§æ™ºèƒ½ä½“æ¡†æ¶ä¸‹ç”Ÿæˆæ‰§è¡Œè½¨è¿¹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡ä¸‰ç§æ‰©å±•æœºåˆ¶è¿›ä¸€æ­¥å¢åŠ äº†ä»»åŠ¡å¤šæ ·æ€§å¹¶æ¨¡æ‹Ÿäº†å¤æ‚çš„å¤šè½®å¯¹è¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨TOUCANä¸Šè¿›è¡Œå¾®è°ƒçš„æ¨¡å‹åœ¨BFCL V3åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†æ›´å¤§è§„æ¨¡çš„é—­æºæ¨¡å‹ï¼Œå¹¶åœ¨MCP-Universe Benchä¸Šæ¨åŠ¨äº†å¸•ç´¯æ‰˜å‰æ²¿(Pareto frontier)ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "35 pages, 13 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.01179v1",
      "published_date": "2025-10-01 17:58:03 UTC",
      "updated_date": "2025-10-01 17:58:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:55:45.284050+00:00"
    },
    {
      "arxiv_id": "2510.01178v1",
      "title": "COM-BOM: Bayesian Exemplar Search for Efficiently Exploring the Accuracy-Calibration Pareto Frontier",
      "title_zh": "COM-BOMï¼šç”¨äºé«˜æ•ˆæ¢ç´¢å‡†ç¡®ç‡-æ ¡å‡†å¸•ç´¯æ‰˜å‰æ²¿çš„è´å¶æ–¯èŒƒä¾‹æœç´¢",
      "authors": [
        "Gaoxiang Luo",
        "Aryan Deshwal"
      ],
      "abstract": "Selecting an optimal set of exemplars is critical for good performance of in-context learning. However, prior exemplar search methods narrowly optimize for predictive accuracy, critically neglecting model calibration--a key determinant of trustworthiness and safe deployment. In this paper, we formulate exemplar selection as a multi-objective optimization problem, explicitly targeting both the maximization of predictive accuracy and the minimization of expected calibration error. We solve this problem with a sample-efficient Combinatorial Bayesian Optimization algorithm (COM-BOM) to find the Pareto front that optimally trades off the two objectives of accuracy and calibration. We evaluate COM-BOM on multiple tasks from unsaturated MMLU-Pro benchmark and find that COM-BOM beats or matches the baselines at jointly optimizing the two objectives, while requiring a minimal number of LLM API calls.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸Šä¸‹æ–‡å­¦ä¹ (In-Context Learning)ä¸­æ ·æœ¬é€‰æ‹©è¿‡åº¦å…³æ³¨é¢„æµ‹å‡†ç¡®ç‡è€Œå¿½è§†æ¨¡å‹æ ¡å‡†(Model Calibration)çš„é—®é¢˜ï¼ŒæŒ‡å‡ºæ ¡å‡†æ˜¯ç¡®ä¿æ¨¡å‹å¯é æ€§å’Œå®‰å…¨éƒ¨ç½²çš„å…³é”®ã€‚ä¸ºæ­¤ï¼Œä½œè€…å°†æ ·æœ¬é€‰æ‹©å»ºæ¨¡ä¸ºä¸€ä¸ªå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œæ—¨åœ¨åŒæ—¶æœ€å¤§åŒ–é¢„æµ‹å‡†ç¡®ç‡å¹¶æœ€å°åŒ–é¢„æœŸæ ¡å‡†è¯¯å·®(Expected Calibration Error)ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºCOM-BOMçš„é«˜æ•ˆç»„åˆè´å¶æ–¯ä¼˜åŒ–ç®—æ³•(Combinatorial Bayesian Optimization)ï¼Œç”¨äºæœç´¢èƒ½å¤Ÿå¹³è¡¡å‡†ç¡®ç‡ä¸æ ¡å‡†åº¦çš„å¸•ç´¯æ‰˜å‰æ²¿(Pareto Frontier)ã€‚åœ¨MMLU-ProåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCOM-BOMåœ¨è”åˆä¼˜åŒ–è¿™ä¸¤ä¸ªç›®æ ‡æ—¶è¡¨ç°ä¼˜äºæˆ–æŒå¹³äºç°æœ‰åŸºå‡†ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ä¿è¯é«˜æ€§èƒ½çš„åŒæ—¶ä»…éœ€æå°‘çš„å¤§è¯­è¨€æ¨¡å‹(LLM) APIè°ƒç”¨ï¼Œæ˜¾è‘—æå‡äº†æ ·æœ¬æœç´¢æ•ˆç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by EMNLP 2025 Main, Code: https://github.com/GaoxiangLuo/COM-BOM",
      "pdf_url": "https://arxiv.org/pdf/2510.01178v1",
      "published_date": "2025-10-01 17:57:49 UTC",
      "updated_date": "2025-10-01 17:57:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:55:46.178597+00:00"
    },
    {
      "arxiv_id": "2510.01174v1",
      "title": "Code2Video: A Code-centric Paradigm for Educational Video Generation",
      "title_zh": "Code2Videoï¼šä¸€ç§ä»¥ä»£ç ä¸ºæ ¸å¿ƒçš„æ•™è‚²è§†é¢‘ç”ŸæˆèŒƒå¼",
      "authors": [
        "Yanzhe Chen",
        "Kevin Qinghong Lin",
        "Mike Zheng Shou"
      ],
      "abstract": "While recent generative models advance pixel-space video synthesis, they remain limited in producing professional educational videos, which demand disciplinary knowledge, precise visual structures, and coherent transitions, limiting their applicability in educational scenarios. Intuitively, such requirements are better addressed through the manipulation of a renderable environment, which can be explicitly controlled via logical commands (e.g., code). In this work, we propose Code2Video, a code-centric agent framework for generating educational videos via executable Python code. The framework comprises three collaborative agents: (i) Planner, which structures lecture content into temporally coherent flows and prepares corresponding visual assets; (ii) Coder, which converts structured instructions into executable Python codes while incorporating scope-guided auto-fix to enhance efficiency; and (iii) Critic, which leverages vision-language models (VLM) with visual anchor prompts to refine spatial layout and ensure clarity. To support systematic evaluation, we build MMMC, a benchmark of professionally produced, discipline-specific educational videos. We evaluate MMMC across diverse dimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and particularly, TeachQuiz, a novel end-to-end metric that quantifies how well a VLM, after unlearning, can recover knowledge by watching the generated videos. Our results demonstrate the potential of Code2Video as a scalable, interpretable, and controllable approach, achieving 40% improvement over direct code generation and producing videos comparable to human-crafted tutorials. The code and datasets are available at https://github.com/showlab/Code2Video.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Code2Videoï¼Œä¸€ç§ä»¥ä»£ç ä¸ºä¸­å¿ƒ(code-centric)çš„æ•™è‚²è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç”Ÿæˆæ¨¡å‹åœ¨ä¸“ä¸šå­¦ç§‘çŸ¥è¯†è¡¨è¾¾å’Œè§†è§‰ç»“æ„ç²¾ç¡®æ§åˆ¶æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡æ“çºµå¯æ¸²æŸ“ç¯å¢ƒå®ç°ç²¾å‡†æ§åˆ¶ï¼Œç”±è´Ÿè´£å†…å®¹æµè§„åˆ’çš„ Plannerã€å°†æŒ‡ä»¤è½¬åŒ–ä¸ºå¯æ‰§è¡Œ Python ä»£ç çš„ Coder ä»¥åŠåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹(VLM)é…åˆè§†è§‰é”šç‚¹æç¤º(visual anchor prompts)ä¼˜åŒ–å¸ƒå±€çš„ Critic ä¸‰ä¸ªåä½œæ™ºèƒ½ä½“ç»„æˆã€‚ä¸ºäº†è¿›è¡Œç³»ç»Ÿè¯„ä¼°ï¼Œç ”ç©¶è€…æ„å»ºäº†ä¸“ä¸šå­¦ç§‘æ•™è‚²è§†é¢‘åŸºå‡† MMMCï¼Œå¹¶å¼•å…¥äº† TeachQuiz è¿™ä¸€é€šè¿‡çŸ¥è¯†æ¢å¤èƒ½åŠ›é‡åŒ–æ•™å­¦è´¨é‡çš„åˆ›æ–°æŒ‡æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCode2Video åœ¨ç”Ÿæˆæ•ˆç‡å’Œè´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç›´æ¥ä»£ç ç”Ÿæˆæ–¹æ³•ï¼Œæ€§èƒ½æå‡è¾¾ 40%ï¼Œä¸”ç”Ÿæˆçš„è§†é¢‘åœ¨æ•™å­¦æ•ˆæœä¸Šå¯ä¸äººç±»æ‰‹å·¥åˆ¶ä½œçš„æ•™ç¨‹ç›¸åª²ç¾ã€‚è¯¥æ–¹æ³•å±•ç¤ºäº†ä½œä¸ºä¸€ç§å¯æ‰©å±•ã€å¯è§£é‡Šä¸”å¯æ§çš„è§†é¢‘ç”ŸæˆèŒƒå¼çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://showlab.github.io/Code2Video/",
      "pdf_url": "https://arxiv.org/pdf/2510.01174v1",
      "published_date": "2025-10-01 17:56:48 UTC",
      "updated_date": "2025-10-01 17:56:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:55:51.087776+00:00"
    },
    {
      "arxiv_id": "2510.01173v1",
      "title": "EditTrack: Detecting and Attributing AI-assisted Image Editing",
      "title_zh": "EditTrackï¼šAI è¾…åŠ©å›¾åƒç¼–è¾‘çš„æ£€æµ‹ä¸æº¯æº",
      "authors": [
        "Zhengyuan Jiang",
        "Yuyang Zhang",
        "Moyang Guo",
        "Neil Zhenqiang Gong"
      ],
      "abstract": "In this work, we formulate and study the problem of image-editing detection and attribution: given a base image and a suspicious image, detection seeks to determine whether the suspicious image was derived from the base image using an AI editing model, while attribution further identifies the specific editing model responsible. Existing methods for detecting and attributing AI-generated images are insufficient for this problem, as they focus on determining whether an image was AI-generated/edited rather than whether it was edited from a particular base image. To bridge this gap, we propose EditTrack, the first framework for this image-editing detection and attribution problem. Building on four key observations about the editing process, EditTrack introduces a novel re-editing strategy and leverages carefully designed similarity metrics to determine whether a suspicious image originates from a base image and, if so, by which model. We evaluate EditTrack on five state-of-the-art editing models across six datasets, demonstrating that it consistently achieves accurate detection and attribution, significantly outperforming five baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å›¾åƒç¼–è¾‘æ£€æµ‹ä¸æº¯æº(image-editing detection and attribution)é—®é¢˜ï¼Œæ—¨åœ¨åˆ¤å®šå¯ç–‘å›¾åƒæ˜¯å¦ç”±ç‰¹å®šåŸºç¡€å›¾åƒç»AIæ¨¡å‹ç¼–è¾‘è€Œæˆï¼Œå¹¶è¯†åˆ«å…·ä½“çš„ç¼–è¾‘æ¨¡å‹ã€‚é’ˆå¯¹ç°æœ‰AIç”Ÿæˆæ£€æµ‹æ–¹æ³•åœ¨å…³è”ç‰¹å®šåŸºç¡€å›¾åƒæ–¹é¢çš„å±€é™æ€§ï¼Œä½œè€…æå‡ºäº†é¦–ä¸ªä¸“é—¨çš„æ£€æµ‹ä¸æº¯æºæ¡†æ¶EditTrackã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥åˆ›æ–°çš„é‡ç¼–è¾‘ç­–ç•¥(re-editing strategy)å¹¶ç»“åˆç²¾å¿ƒè®¾è®¡çš„ç›¸ä¼¼åº¦æŒ‡æ ‡(similarity metrics)ï¼Œèƒ½å¤Ÿç²¾å‡†è¯†åˆ«å›¾åƒçš„æ¼”å˜è·¯å¾„åŠå…¶æ¨¡å‹å½’å±ã€‚åœ¨äº”ä¸ªå…ˆè¿›ç¼–è¾‘æ¨¡å‹å’Œå…­ä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒEditTrackåœ¨å„é¡¹ä»»åŠ¡ä¸­å‡å–å¾—äº†ä¼˜å¼‚çš„æ£€æµ‹ä¸æº¯æºå‡†ç¡®ç‡ï¼Œå…¶æ€§èƒ½æ˜¾è‘—ä¼˜äºäº”ä¸ªä¸»æµåŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01173v1",
      "published_date": "2025-10-01 17:56:35 UTC",
      "updated_date": "2025-10-01 17:56:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:56:02.776486+00:00"
    },
    {
      "arxiv_id": "2510.14989v1",
      "title": "Constrained Diffusion for Protein Design with Hard Structural Constraints",
      "title_zh": "é¢å‘ç¡¬ç»“æ„çº¦æŸè›‹ç™½è´¨è®¾è®¡çš„çº¦æŸæ‰©æ•£æ¨¡å‹",
      "authors": [
        "Jacob K. Christopher",
        "Austin Seamann",
        "Jingyi Cui",
        "Sagar Khare",
        "Ferdinando Fioretto"
      ],
      "abstract": "Diffusion models offer a powerful means of capturing the manifold of realistic protein structures, enabling rapid design for protein engineering tasks. However, existing approaches observe critical failure modes when precise constraints are necessary for functional design. To this end, we present a constrained diffusion framework for structure-guided protein design, ensuring strict adherence to functional requirements while maintaining precise stereochemical and geometric feasibility. The approach integrates proximal feasibility updates with ADMM decomposition into the generative process, scaling effectively to the complex constraint sets of this domain. We evaluate on challenging protein design tasks, including motif scaffolding and vacancy-constrained pocket design, while introducing a novel curated benchmark dataset for motif scaffolding in the PDZ domain. Our approach achieves state-of-the-art, providing perfect satisfaction of bonding and geometric constraints with no degradation in structural diversity.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç”¨äºç»“æ„å¯¼å‘è›‹ç™½è´¨è®¾è®¡çš„çº¦æŸæ‰©æ•£æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ‰©æ•£æ¨¡å‹(Diffusion models)åœ¨å¤„ç†ç²¾ç¡®åŠŸèƒ½çº¦æŸæ—¶å®¹æ˜“å¤±æ•ˆçš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ•´åˆè¿‘ç«¯å¯è¡Œæ€§æ›´æ–°(proximal feasibility updates)ä¸ADMMåˆ†è§£ï¼Œç¡®ä¿ç”Ÿæˆçš„è›‹ç™½è´¨ç»“æ„åœ¨æ»¡è¶³å¤æ‚åŠŸèƒ½éœ€æ±‚çš„åŒæ—¶ï¼Œä¿æŒç²¾ç¡®çš„ç«‹ä½“åŒ–å­¦å’Œå‡ ä½•å¯è¡Œæ€§ã€‚ç ”ç©¶åœ¨åŸºåºæ”¯æ¶(motif scaffolding)å’Œç©ºä½çº¦æŸå£è¢‹è®¾è®¡(vacancy-constrained pocket design)ç­‰æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå¹¶å¼•å…¥äº†é’ˆå¯¹PDZåŸŸåŸºåºæ”¯æ¶çš„æ–°å‹åŸºå‡†æ•°æ®é›†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¸ç‰ºç‰²ç»“æ„å¤šæ ·æ€§çš„å‰æä¸‹ï¼Œå®ç°äº†å¯¹ç»“åˆåŠå‡ ä½•çº¦æŸçš„å®Œç¾æ»¡è¶³ï¼Œè¾¾åˆ°äº†å½“å‰çš„SOTAæ°´å¹³ã€‚è¯¥æ–¹æ³•ä¸ºé«˜ç²¾åº¦çš„è›‹ç™½è´¨å·¥ç¨‹ä»»åŠ¡æä¾›äº†ä¸€ç§èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤æ‚çº¦æŸé›†çš„å¼ºå¤§ç”Ÿæˆå·¥å…·ã€‚",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.BM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.14989v1",
      "published_date": "2025-10-01 17:55:45 UTC",
      "updated_date": "2025-10-01 17:55:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:56:24.477949+00:00"
    },
    {
      "arxiv_id": "2510.01171v3",
      "title": "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity",
      "title_zh": "Verbalized Samplingï¼šå¦‚ä½•ç¼“è§£æ¨¡å¼å´©æºƒå¹¶é‡Šæ”¾å¤§è¯­è¨€æ¨¡å‹çš„å¤šæ ·æ€§",
      "authors": [
        "Jiayi Zhang",
        "Simon Yu",
        "Derek Chong",
        "Anthony Sicilia",
        "Michael R. Tomz",
        "Christopher D. Manning",
        "Weiyan Shi"
      ],
      "abstract": "Post-training alignment often reduces LLM diversity, leading to a phenomenon known as mode collapse. Unlike prior work that attributes this effect to algorithmic limitations, we identify a fundamental, pervasive data-level driver: typicality bias in preference data, whereby annotators systematically favor familiar text as a result of well-established findings in cognitive psychology. We formalize this bias theoretically, verify it on preference datasets empirically, and show that it plays a central role in mode collapse. Motivated by this analysis, we introduce Verbalized Sampling, a simple, training-free prompting strategy to circumvent mode collapse. VS prompts the model to verbalize a probability distribution over a set of responses (e.g., \"Generate 5 jokes about coffee and their corresponding probabilities\"). Comprehensive experiments show that VS significantly improves performance across creative writing (poems, stories, jokes), dialogue simulation, open-ended QA, and synthetic data generation, without sacrificing factual accuracy and safety. For instance, in creative writing, VS increases diversity by 1.6-2.1x over direct prompting. We further observe an emergent trend that more capable models benefit more from VS. In sum, our work provides a new data-centric perspective on mode collapse and a practical inference-time remedy that helps unlock pre-trained generative diversity.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¯¹é½åå‡ºç°çš„æ¨¡å¼åç¼©ï¼ˆMode Collapseï¼‰é—®é¢˜ï¼Œå¹¶æŒ‡å‡ºå…¶æ ¸å¿ƒé©±åŠ¨å› ç´ æ˜¯åå¥½æ•°æ®ä¸­çš„å…¸å‹æ€§åå·®ï¼ˆTypicality Biasï¼‰ï¼Œå³æ ‡æ³¨è€…å› è®¤çŸ¥å¿ƒç†å› ç´ ç³»ç»Ÿæ€§åœ°åå¥½ç†Ÿæ‚‰çš„æ–‡æœ¬ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†è¨€è¯­åŒ–é‡‡æ ·ï¼ˆVerbalized Samplingï¼Œç®€ç§° VSï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•ä¸”æ— éœ€è®­ç»ƒçš„æç¤ºç­–ç•¥ï¼Œé€šè¿‡è¦æ±‚æ¨¡å‹å¯¹ä¸€ç»„å“åº”åŠå…¶å¯¹åº”çš„æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œè¨€è¯­åŒ–æè¿°æ¥è§„é¿æ¨¡å¼åç¼©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVS åœ¨åˆ›æ„å†™ä½œã€å¯¹è¯æ¨¡æ‹Ÿã€å¼€æ”¾å¼é—®ç­”å’Œåˆæˆæ•°æ®ç”Ÿæˆç­‰ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†ç”Ÿæˆå¤šæ ·æ€§ï¼Œåœ¨åˆ›æ„å†™ä½œé¢†åŸŸçš„å¤šæ ·æ€§å¢å¹…è¾¾åˆ° 1.6 è‡³ 2.1 å€ï¼Œä¸”æœªç‰ºç‰²äº‹å®å‡†ç¡®æ€§ä¸å®‰å…¨æ€§ã€‚ç ”ç©¶è¿˜è§‚å¯Ÿåˆ°ï¼Œæ¨¡å‹èƒ½åŠ›è¶Šå¼ºï¼Œä» VS ä¸­è·å¾—çš„æ”¶ç›Šè¶Šæ˜¾è‘—ã€‚è¯¥å·¥ä½œä¸ºç†è§£æ¨¡å¼åç¼©æä¾›äº†å…¨æ–°çš„ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„è§†è§’ï¼Œå¹¶æä¾›äº†ä¸€ç§åœ¨æ¨ç†é˜¶æ®µé‡Šæ”¾é¢„è®­ç»ƒç”Ÿæˆå¤šæ ·æ€§çš„å®ç”¨æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "82 pages, 28 figures, 32 tables. Code is available at https://github.com/CHATS-lab/verbalize-sampling",
      "pdf_url": "https://arxiv.org/pdf/2510.01171v3",
      "published_date": "2025-10-01 17:55:37 UTC",
      "updated_date": "2025-10-10 17:38:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:56:05.290067+00:00"
    },
    {
      "arxiv_id": "2510.01169v1",
      "title": "Fiaingen: A financial time series generative method matching real-world data quality",
      "title_zh": "Fiaingenï¼šä¸€ç§åŒ¹é…çœŸå®æ•°æ®è´¨é‡çš„é‡‘èæ—¶é—´åºåˆ—ç”Ÿæˆæ–¹æ³•",
      "authors": [
        "JoÅ¾e M. RoÅ¾anec",
        "Tina Å½ezlin",
        "Laurentiu Vasiliu",
        "Dunja MladeniÄ‡",
        "Radu Prodan",
        "Dumitru Roman"
      ],
      "abstract": "Data is vital in enabling machine learning models to advance research and practical applications in finance, where accurate and robust models are essential for investment and trading decision-making. However, real-world data is limited despite its quantity, quality, and variety. The data shortage of various financial assets directly hinders the performance of machine learning models designed to trade and invest in these assets. Generative methods can mitigate this shortage. In this paper, we introduce a set of novel techniques for time series data generation (we name them Fiaingen) and assess their performance across three criteria: (a) overlap of real-world and synthetic data on a reduced dimensionality space, (b) performance on downstream machine learning tasks, and (c) runtime performance. Our experiments demonstrate that the methods achieve state-of-the-art performance across the three criteria listed above. Synthetic data generated with Fiaingen methods more closely mirrors the original time series data while keeping data generation time close to seconds - ensuring the scalability of the proposed approach. Furthermore, models trained on it achieve performance close to those trained with real-world data.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Fiaingenï¼Œä¸€å¥—ç”¨äºç”Ÿæˆ Financial Time Series æ•°æ®çš„åˆ›æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³é‡‘èé¢†åŸŸæœºå™¨å­¦ä¹ æ¨¡å‹åœ¨æŠ•èµ„ä¸äº¤æ˜“å†³ç­–ä¸­é¢ä¸´çš„çœŸå®æ•°æ®çŸ­ç¼ºåŠè´¨é‡ç“¶é¢ˆã€‚ç ”ç©¶äººå‘˜é€šè¿‡é™ç»´ç©ºé—´çš„åˆ†å¸ƒé‡åˆåº¦ã€ä¸‹æ¸¸ Machine Learning ä»»åŠ¡è¡¨ç°ä»¥åŠ Runtime æ•ˆç‡ä¸‰ä¸ªç»´åº¦å¯¹è¯¥æ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFiaingen åœ¨æ‰€æœ‰è¯„ä»·æŒ‡æ ‡ä¸Šå‡è¾¾åˆ°äº† State-of-the-art æ°´å¹³ï¼Œç”Ÿæˆçš„åˆæˆæ•°æ®èƒ½å¤Ÿé«˜åº¦è¿˜åŸåŸå§‹æ•°æ®ç‰¹å¾ï¼Œä¸”ç”Ÿæˆè¿‡ç¨‹ä»…éœ€ç§’çº§ï¼Œå…·å¤‡æä½³çš„ Scalabilityã€‚æ­¤å¤–ï¼Œåœ¨ Fiaingen åˆæˆæ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹æ€§èƒ½ä¸çœŸå®æ•°æ®è®­ç»ƒçš„æ¨¡å‹éå¸¸æ¥è¿‘ï¼Œä¸ºé‡‘èé¢†åŸŸçš„æ•°æ®å¢å¼ºä¸æ¨¡å‹ä¼˜åŒ–æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01169v1",
      "published_date": "2025-10-01 17:55:08 UTC",
      "updated_date": "2025-10-01 17:55:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:56:38.887128+00:00"
    },
    {
      "arxiv_id": "2510.01167v1",
      "title": "Simultaneous Multi-objective Alignment Across Verifiable and Non-verifiable Rewards",
      "title_zh": "è·¨å¯éªŒè¯ä¸ä¸å¯éªŒè¯å¥–åŠ±çš„åŒæ­¥å¤šç›®æ ‡å¯¹é½",
      "authors": [
        "Yiran Shen",
        "Yu Xia",
        "Jonathan Chang",
        "Prithviraj Ammanabrolu"
      ],
      "abstract": "Aligning large language models to human preferences is inherently multidimensional, yet most pipelines collapse heterogeneous signals into a single optimizeable objective. We seek to answer what it would take to simultaneously align a model across various domains spanning those with: verifiable rewards (mathematical accuracy), non-verifiable subjective preferences (human values), and complex interactive scenarios (multi-turn AI tutoring dialogues). Such multi-objective reinforcement learning setups are often plagued by the individual objectives being at odds with each other, resulting in inefficient training and little user control during inference. We propose a unified framework that: (i) standardizes {process reward model} (PRM) training across both verifiable and non-verifiable settings to better supervise models' chain-of-thought reasoning; (ii) performs {multi-objective alignment} by training the LLM with our $\\textbf{M}$ulti-$\\textbf{A}$ction-$\\textbf{H}$ead $\\textbf{DPO}$ (MAH-DPO) and a vectorized reward where the dimensions of the vector correspond to the various objectives instead of a single scalar; and (iii) demonstrates how such a system provides fine-grained inference-time user control. Experiments across math reasoning, value alignment, and multi-turn dialogue show that our framework improves performance across multiple objectives simultaneously, while minimizing cross-objective trade-offs and enabling flexible inference time user control. The code can be found at https://github.com/pearls-lab/multiobj-align.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹åœ¨å¯¹é½äººç±»åå¥½æ—¶å¸¸å°†å¤šç»´åº¦ä¿¡å·ç®€åŒ–ä¸ºå•ä¸€ä¼˜åŒ–ç›®æ ‡çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ä¸ªèƒ½å¤ŸåŒæ—¶å¯¹é½å¯éªŒè¯å¥–åŠ±ã€ééªŒè¯æ€§ä¸»è§‚åå¥½åŠå¤æ‚äº¤äº’åœºæ™¯çš„ç»Ÿä¸€æ¡†æ¶ã€‚ç ”ç©¶é¦–å…ˆæ ‡å‡†åŒ–äº†è·¨é¢†åŸŸçš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹(Process Reward Model, PRM)è®­ç»ƒï¼Œä»¥ä¼˜åŒ–å¯¹æ¨¡å‹é“¾å¼æ€ç»´(Chain-of-Thought)æ¨ç†è¿‡ç¨‹çš„ç›‘ç£ã€‚éšåï¼Œå¼•å…¥äº†å¤šåŠ¨ä½œå¤´DPO (Multi-Action-Head DPO, MAH-DPO)ç®—æ³•ï¼Œé€šè¿‡å‘é‡åŒ–å¥–åŠ±è€Œéä¼ ç»Ÿæ ‡é‡æ¥å®ç°å¤šç›®æ ‡å¯¹é½(Multi-objective alignment)ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ•°å­¦æ¨ç†ã€ä»·å€¼å¯¹é½å’Œå¤šè½®å¯¹è¯ç­‰å¤šä¸ªä»»åŠ¡ä¸Šå‡èƒ½åŒæ—¶æå‡æ€§èƒ½ï¼Œå¹¶æœ‰æ•ˆå‡å°‘äº†ä¸åŒç›®æ ‡é—´çš„æ€§èƒ½æƒè¡¡ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿè¿˜å®ç°äº†ç»†ç²’åº¦çš„æ¨ç†æ—¶ç”¨æˆ·æ§åˆ¶(Inference-time user control)ï¼Œå…è®¸ç”¨æˆ·åœ¨æ¨ç†é˜¶æ®µçµæ´»è°ƒæ•´å„ç›®æ ‡çš„ä¾§é‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01167v1",
      "published_date": "2025-10-01 17:54:15 UTC",
      "updated_date": "2025-10-01 17:54:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:56:42.569705+00:00"
    },
    {
      "arxiv_id": "2510.01165v1",
      "title": "GRAD: Generative Retrieval-Aligned Demonstration Sampler for Efficient Few-Shot Reasoning",
      "title_zh": "GRADï¼šé¢å‘é«˜æ•ˆå°‘æ ·æœ¬æ¨ç†çš„ç”Ÿæˆå¼æ£€ç´¢å¯¹é½ç¤ºä¾‹é‡‡æ ·å™¨",
      "authors": [
        "Oussama Gabouj",
        "Kamel Charaf",
        "Ivan Zakazov",
        "Nicolas Baldwin",
        "Robert West"
      ],
      "abstract": "Large Language Models (LLMs) achieve strong performance across diverse tasks, but their effectiveness often depends on the quality of the provided context. Retrieval-Augmented Generation (RAG) enriches prompts with external information, but its reliance on static databases constrains adaptability and can result in irrelevant demonstrations. In this work, we propose a Generative Retrieval-Aligned Demonstrator (GRAD), a dynamic demonstration-based approach where an LLM model is trained to generate input-specific concise demonstrations. By tailoring demonstrations to each input, our method offers better contextual support than traditional RAG approaches. We demonstrate the superiority of GRAD under budget constraints, where we limit both the number of tokens used per demonstration and the number of tokens used for the final output. Trained solely on a math dataset, GRAD consistently outperforms strong baselines on Qwen2.5-14B across mathematical reasoning and advanced STEM questions, highlighting GRAD's robust generalization to out-of-distribution (OOD) domains such as physics, chemistry, and computer science. Furthermore, we show that demonstrations generated by trained smaller models can effectively guide larger target models, reducing training costs while maintaining competitive accuracy. Overall, this work introduces a scalable demonstration generator model presenting the first step toward a dynamic few-shot learning paradigm in resource-constrained settings. We release the code used for the project.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GRAD (Generative Retrieval-Aligned Demonstrator)ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)è¿‡ç¨‹ä¸­å› ä¾èµ–é™æ€æ•°æ®åº“è€Œå¯¼è‡´çš„é€‚åº”æ€§ä¸è¶³å’Œæ— å…³ç¤ºä¾‹é—®é¢˜ã€‚GRADé‡‡ç”¨ä¸€ç§åŠ¨æ€çš„åŸºäºç¤ºä¾‹(Demonstration-based)çš„æ–¹æ³•ï¼Œé€šè¿‡è®­ç»ƒæ¨¡å‹ä¸ºç‰¹å®šè¾“å…¥å®æ—¶ç”Ÿæˆç®€æ´çš„ç¤ºä¾‹ï¼Œåœ¨é¢„ç®—å—é™çš„Tokenåœºæ™¯ä¸‹æä¾›äº†æ¯”ä¼ ç»ŸRAGæ›´ä¼˜çš„ä¸Šä¸‹æ–‡æ”¯æŒã€‚åœ¨Qwen2.5-14Bä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä»…åœ¨æ•°å­¦æ•°æ®é›†ä¸Šè®­ç»ƒçš„GRADä¸ä»…åœ¨æ•°å­¦æ¨ç†ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¿˜å±•ç¤ºäº†å‘ç‰©ç†ã€åŒ–å­¦å’Œè®¡ç®—æœºç§‘å­¦ç­‰åˆ†å¸ƒå¤–(Out-of-distribution)é¢†åŸŸçš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°ç”±å°å‹æ¨¡å‹ç”Ÿæˆçš„ç¤ºä¾‹å¯ä»¥æœ‰æ•ˆå¼•å¯¼å¤§å‹ç›®æ ‡æ¨¡å‹ï¼Œåœ¨æ˜¾è‘—é™ä½è®­ç»ƒæˆæœ¬çš„åŒæ—¶ä¿æŒäº†æå…·ç«äº‰åŠ›çš„å‡†ç¡®ç‡ã€‚è¯¥å·¥ä½œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„åŠ¨æ€å°‘æ ·æœ¬å­¦ä¹ (Few-shot learning)æä¾›äº†å¯æ‰©å±•çš„ç”Ÿæˆæ–¹æ¡ˆï¼Œå¹¶å·²å¼€æºç›¸å…³ä»£ç ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2025 (findings)",
      "pdf_url": "https://arxiv.org/pdf/2510.01165v1",
      "published_date": "2025-10-01 17:52:41 UTC",
      "updated_date": "2025-10-01 17:52:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:56:41.968628+00:00"
    },
    {
      "arxiv_id": "2510.01164v1",
      "title": "Social Welfare Function Leaderboard: When LLM Agents Allocate Social Welfare",
      "title_zh": "ç¤¾ä¼šç¦åˆ©å‡½æ•°æ’è¡Œæ¦œï¼šå¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“çš„ç¤¾ä¼šç¦åˆ©åˆ†é…",
      "authors": [
        "Zhengliang Shi",
        "Ruotian Ma",
        "Jen-tse Huang",
        "Xinbei Ma",
        "Xingyu Chen",
        "Mengru Wang",
        "Qu Yang",
        "Yue Wang",
        "Fanghua Ye",
        "Ziyang Chen",
        "Shanyi Wang",
        "Cixing Li",
        "Wenxuan Wang",
        "Zhaopeng Tu",
        "Xiaolong Li",
        "Zhaochun Ren",
        "Linus"
      ],
      "abstract": "Large language models (LLMs) are increasingly entrusted with high-stakes decisions that affect human welfare. However, the principles and values that guide these models when distributing scarce societal resources remain largely unexamined. To address this, we introduce the Social Welfare Function (SWF) Benchmark, a dynamic simulation environment where an LLM acts as a sovereign allocator, distributing tasks to a heterogeneous community of recipients. The benchmark is designed to create a persistent trade-off between maximizing collective efficiency (measured by Return on Investment) and ensuring distributive fairness (measured by the Gini coefficient). We evaluate 20 state-of-the-art LLMs and present the first leaderboard for social welfare allocation. Our findings reveal three key insights: (i) A model's general conversational ability, as measured by popular leaderboards, is a poor predictor of its allocation skill. (ii) Most LLMs exhibit a strong default utilitarian orientation, prioritizing group productivity at the expense of severe inequality. (iii) Allocation strategies are highly vulnerable, easily perturbed by output-length constraints and social-influence framing. These results highlight the risks of deploying current LLMs as societal decision-makers and underscore the need for specialized benchmarks and targeted alignment for AI governance.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†ç¤¾ä¼šç¦åˆ©å‡½æ•°(Social Welfare Function, SWF)åŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªåŠ¨æ€æ¨¡æ‹Ÿç¯å¢ƒï¼Œæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹(LLMs)ä½œä¸ºèµ„æºåˆ†é…è€…åœ¨å¤„ç†ç¤¾ä¼šç¦åˆ©åˆ†é…æ—¶çš„å†³ç­–è¡¨ç°ã€‚è¯¥åŸºå‡†åœ¨æœ€å¤§åŒ–é›†ä½“æ•ˆç‡(æŠ•èµ„å›æŠ¥ç‡ Return on Investment)ä¸ç¡®ä¿åˆ†é…å…¬å¹³(åŸºå°¼ç³»æ•° Gini coefficient)ä¹‹é—´å»ºç«‹äº†æƒè¡¡æœºåˆ¶ï¼Œå¹¶æ®æ­¤å¯¹20ç§ä¸»æµLLMsè¿›è¡Œäº†è¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹çš„é€šç”¨å¯¹è¯èƒ½åŠ›ä¸å…¶åˆ†é…æŠ€èƒ½ä¹‹é—´å¹¶æ— ç›´æ¥è”ç³»ï¼Œä¸”å¤§å¤šæ•°æ¨¡å‹è¡¨ç°å‡ºå¼ºçƒˆçš„åŠŸåˆ©ä¸»ä¹‰(utilitarian)å€¾å‘ï¼Œå¾€å¾€ä¸ºäº†æé«˜ç¾¤ä½“ç”Ÿäº§åŠ›è€Œå¯¼è‡´ä¸¥é‡çš„ç¤¾ä¼šä¸å¹³ç­‰ã€‚æ­¤å¤–ï¼ŒLLMsçš„åˆ†é…ç­–ç•¥ææ˜“å—åˆ°è¾“å‡ºé•¿åº¦é™åˆ¶å’Œç¤¾äº¤å½±å“åŠ›æ¡†æ¶(social-influence framing)çš„å¹²æ‰°ã€‚è¿™äº›ç»“æœæ­ç¤ºäº†ç°æœ‰çš„LLMsåœ¨æ‹…ä»»ç¤¾ä¼šå†³ç­–è€…æ—¶å­˜åœ¨çš„æ½œåœ¨é£é™©ï¼Œå¹¶å¼ºè°ƒäº†åœ¨äººå·¥æ™ºèƒ½æ²»ç†(AI governance)ä¸­å¼•å…¥ä¸“é—¨åŸºå‡†å’Œå®šå‘å¯¹é½(alignment)çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01164v1",
      "published_date": "2025-10-01 17:52:31 UTC",
      "updated_date": "2025-10-01 17:52:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:56:46.480859+00:00"
    },
    {
      "arxiv_id": "2510.01161v2",
      "title": "Prosperity before Collapse: How Far Can Off-Policy RL Reach with Stale Data on LLMs?",
      "title_zh": "å´©å¡Œå‰çš„ç¹è£ï¼šLLMs åœ¨é™ˆæ—§æ•°æ®ä¸‹çš„ç¦»ç­–å¼ºåŒ–å­¦ä¹ èƒ½èµ°å¤šè¿œï¼Ÿ",
      "authors": [
        "Haizhong Zheng",
        "Jiawei Zhao",
        "Beidi Chen"
      ],
      "abstract": "Reinforcement learning has been central to recent advances in large language model reasoning, but most algorithms rely on on-policy training that demands fresh rollouts at every update, limiting efficiency and scalability. Asynchronous RL systems alleviate this by decoupling rollout generation from training, yet their effectiveness hinges on tolerating large staleness in rollout data, a setting where existing methods either degrade in performance or collapse. We revisit this challenge and uncover a prosperity-before-collapse phenomenon: stale data can be as informative as on-policy data if exploited properly. Building on this insight, we introduce M2PO (Second-Moment Trust Policy Optimization), which constrains the second moment of importance weights to suppress only extreme outliers while preserving informative updates. Notably, M2PO sharply reduces the fraction of clipped tokens under high staleness (from 1.22% to 0.06% over training), precisely masking high-variance tokens while maintaining stable optimization. Extensive evaluation across six models (from 1.7B to 32B) and eight benchmarks shows that M2PO delivers stable off-policy training even with data stale by at least 256 model updates and matches on-policy performance.",
      "tldr_zh": "å¼ºåŒ–å­¦ä¹ (Reinforcement learning)åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹(LLMs)æ¨ç†èƒ½åŠ›æ–¹é¢è‡³å…³é‡è¦ï¼Œä½†ä¼ ç»Ÿçš„åœ¨çº¿ç­–ç•¥(on-policy)è®­ç»ƒç”±äºéœ€è¦é¢‘ç¹çš„å®æ—¶é‡‡æ ·è€Œé™åˆ¶äº†æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚æœ¬ç ”ç©¶é’ˆå¯¹å¼‚æ­¥ç³»ç»Ÿä¸­çš„è¿‡æœŸæ•°æ®(stale data)æŒ‘æˆ˜ï¼Œæ­ç¤ºäº†â€œå´©åå‰çš„ç¹è£â€(prosperity-before-collapse)ç°è±¡ï¼Œå³è¿‡æœŸæ•°æ®è‹¥èƒ½å¾—åˆ°å¦¥å–„åˆ©ç”¨ï¼Œå…¶ä¿¡æ¯ä»·å€¼ä¸äºšäºåœ¨çº¿ç­–ç•¥æ•°æ®ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†M2PO (Second-Moment Trust Policy Optimization)ï¼Œè¯¥æ–¹æ³•é€šè¿‡çº¦æŸé‡è¦æ€§æƒé‡(importance weights)çš„äºŒé˜¶çŸ©(second moment)æ¥æŠ‘åˆ¶æç«¯ç¦»ç¾¤å€¼ï¼Œä»è€Œåœ¨ä¿ç•™æœ‰ç›Šæ›´æ–°çš„åŒæ—¶ç»´æŒä¼˜åŒ–ç¨³å®šã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒM2POæ˜¾è‘—é™ä½äº†é«˜è¿‡æœŸç‡ä¸‹çš„Tokenè£å‰ªæ¯”ä¾‹ï¼Œä½¿å…¶èƒ½å¤Ÿæ‰¿å—è‡³å°‘256æ¬¡æ¨¡å‹æ›´æ–°çš„æ•°æ®å»¶è¿Ÿã€‚åœ¨æ¶µç›–1.7Bè‡³32Bå‚æ•°è§„æ¨¡çš„6ä¸ªæ¨¡å‹å’Œ8ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒM2POå‡å®ç°äº†ç¨³å®šçš„ç¦»çº¿ç­–ç•¥(off-policy)è®­ç»ƒï¼Œå¹¶å–å¾—äº†ä¸åœ¨çº¿ç­–ç•¥è®­ç»ƒç›¸åª²ç¾çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01161v2",
      "published_date": "2025-10-01 17:48:23 UTC",
      "updated_date": "2025-10-28 03:28:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:57:12.184131+00:00"
    },
    {
      "arxiv_id": "2510.01146v1",
      "title": "mR3: Multilingual Rubric-Agnostic Reward Reasoning Models",
      "title_zh": "mR3ï¼šå¤šè¯­è¨€è¯„ä»·å‡†åˆ™æ— å…³çš„å¥–åŠ±æ¨ç†æ¨¡å‹",
      "authors": [
        "David Anugraha",
        "Shou-Yi Hung",
        "Zilu Tang",
        "Annie En-Shiun Lee",
        "Derry Tanti Wijaya",
        "Genta Indra Winata"
      ],
      "abstract": "Evaluation using Large Language Model (LLM) judges has been widely adopted in English and shown to be effective for automatic evaluation. However, their performance does not generalize well to non-English settings, and it remains unclear what constitutes effective multilingual training for such judges. In this paper, we introduce mR3, a massively multilingual, rubric-agnostic reward reasoning model trained on 72 languages, achieving the broadest language coverage in reward modeling to date. We present a comprehensive study of data and curriculum selection for training to identify effective strategies and data sources for building high-quality reward models, including the integration of target-language reasoning datasets. Our approach attains state-of-the-art performance on multilingual reward model benchmarks, surpassing much larger models (i.e., GPT-OSS-120B) while being up to 9x smaller, and its effectiveness is further confirmed through extensive ablation studies. Our models, data, and code are available as open source at https://github.com/rubricreward/mr3.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)è¯„åˆ¤å™¨åœ¨éè‹±è¯­ç¯å¢ƒä¸‹è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œæå‡ºäº†mR3ï¼Œä¸€ç§æ”¯æŒ72ç§è¯­è¨€çš„å¤§è§„æ¨¡å¤šè¯­è¨€ã€ä¸å‡†åˆ™æ— å…³çš„å¥–åŠ±æ¨ç†æ¨¡å‹(Multilingual Rubric-Agnostic Reward Reasoning Models)ã€‚ä½œè€…å¯¹è®­ç»ƒä¸­çš„æ•°æ®ä¸è¯¾ç¨‹é€‰æ‹©(Data and Curriculum Selection)è¿›è¡Œäº†å…¨é¢ç ”ç©¶ï¼Œå¹¶æ•´åˆäº†ç›®æ ‡è¯­è¨€æ¨ç†æ•°æ®é›†ï¼Œä»¥è¯†åˆ«æ„å»ºé«˜è´¨é‡å¥–åŠ±æ¨¡å‹çš„æœ‰æ•ˆç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒmR3åœ¨å¤šè¯­è¨€å¥–åŠ±æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†é¢†åŸŸé¢†å…ˆæ°´å¹³(State-of-the-art)ï¼Œå…¶æ€§èƒ½è¶…è¶Šäº†ä½“ç§¯æ¯”å…¶å¤§9å€çš„æ¨¡å‹ï¼ˆå¦‚GPT-OSS-120Bï¼‰ã€‚é€šè¿‡å¹¿æ³›çš„æ¶ˆèç ”ç©¶(Ablation Studies)ï¼Œè¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å¾—åˆ°äº†è¿›ä¸€æ­¥è¯å®ï¼Œä¸”ç›¸å…³æ¨¡å‹ã€æ•°æ®ä¸ä»£ç å·²å…¨éƒ¨å¼€æºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01146v1",
      "published_date": "2025-10-01 17:36:59 UTC",
      "updated_date": "2025-10-01 17:36:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:56:50.272392+00:00"
    },
    {
      "arxiv_id": "2510.01143v2",
      "title": "Generalized Parallel Scaling with Interdependent Generations",
      "title_zh": "èåˆç”Ÿæˆé—´äº’ä¾æ€§çš„å¹¿ä¹‰å¹¶è¡Œæ‰©å±•",
      "authors": [
        "Harry Dong",
        "David Brandfonbrener",
        "Eryk Helenowski",
        "Yun He",
        "Mrinal Kumar",
        "Han Fang",
        "Yuejie Chi",
        "Karthik Abinav Sankararaman"
      ],
      "abstract": "Parallel LLM inference scaling involves sampling a set of $N>1$ responses for a single input prompt. However, these $N$ parallel responses tend to be generated independently from each other, partitioning compute resources and leaving potentially useful information in one generation untapped by others. This is in contrast to response length scaling where past computation is used in all future steps. For higher quality responses and response sets, we propose Bridge to generate interdependent responses in parallel by rethinking batched LLM hidden states as holistic tensors rather than independent slices. With only a small amount (2.8%-5.1%) of new parameters, Bridge improves the relative mean accuracy gains from reinforcement learning with verifiable rewards by up to 39% and boosts consistency of correct responses. Trained once, Bridge scales to any generation width, all with greater performance than independent generations, unlocking a more general mode of parallel scaling that effectively leverages information between sequences, compatible with any post-generation aggregation technique.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å¹¶è¡Œæ¨ç†ä¸­å„å“åº”ç›¸äº’ç‹¬ç«‹ã€æ— æ³•å…±äº«ä¿¡æ¯çš„é—®é¢˜ï¼Œæå‡ºäº†Bridgeæ¡†æ¶ã€‚è¯¥æ–¹æ³•å°†æ‰¹å¤„ç†çš„éšè—çŠ¶æ€(hidden states)é‡æ–°å®šä¹‰ä¸ºæ•´ä½“å¼ é‡(holistic tensors)è€Œéç‹¬ç«‹åˆ‡ç‰‡ï¼Œä»è€Œåœ¨å¹¶è¡Œç”Ÿæˆè¿‡ç¨‹ä¸­å®ç°åºåˆ—é—´çš„ç›¸äº’ä¾èµ–ã€‚åœ¨ä»…å¢åŠ 2.8%-5.1%å‚æ•°é‡çš„æƒ…å†µä¸‹ï¼ŒBridgeåœ¨å¸¦æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ (RL)ä»»åŠ¡ä¸­å°†å¹³å‡å‡†ç¡®ç‡æå‡äº†æœ€é«˜39%ï¼Œå¹¶æ˜¾è‘—æé«˜äº†æ­£ç¡®å“åº”çš„ä¸€è‡´æ€§ã€‚Bridgeå…·å¤‡è‰¯å¥½çš„æ‰©å±•æ€§ï¼Œè®­ç»ƒä¸€æ¬¡å³å¯åº”ç”¨äºä»»æ„ç”Ÿæˆå®½åº¦(generation width)ï¼Œä¸”æ€§èƒ½å§‹ç»ˆä¼˜äºä¼ ç»Ÿçš„ç‹¬ç«‹ç”Ÿæˆæ¨¡å¼ã€‚è¯¥ç ”ç©¶ä¸ºå¹¶è¡Œç¼©æ”¾(parallel scaling)æä¾›äº†ä¸€ç§æ›´é€šç”¨çš„æ¨¡å¼ï¼Œèƒ½æœ‰æ•ˆåˆ©ç”¨åºåˆ—é—´ä¿¡æ¯å¹¶ä¸å„ç±»èšåˆæŠ€æœ¯å…¼å®¹ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01143v2",
      "published_date": "2025-10-01 17:33:35 UTC",
      "updated_date": "2025-12-04 23:33:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:56:54.769577+00:00"
    },
    {
      "arxiv_id": "2510.01141v1",
      "title": "Apriel-1.5-15b-Thinker",
      "title_zh": "Apriel-1.5-15b-Thinker",
      "authors": [
        "Shruthan Radhakrishna",
        "Aman Tiwari",
        "Aanjaneya Shukla",
        "Masoud Hashemi",
        "Rishabh Maheshwary",
        "Shiva Krishna Reddy Malay",
        "Jash Mehta",
        "Pulkit Pattnaik",
        "Saloni Mittal",
        "Khalil Slimi",
        "Kelechi Ogueji",
        "Akintunde Oladipo",
        "Soham Parikh",
        "Oluwanifemi Bamgbose",
        "Toby Liang",
        "Ahmed Masry",
        "Khyati Mahajan",
        "Sai Rajeswar Mudumba",
        "Vikas Yadav",
        "Sathwik Tejaswi Madhusudhan",
        "Torsten Scholak",
        "Sagar Davasam",
        "Srinivas Sunkara",
        "Nicholas Chapados"
      ],
      "abstract": "We present Apriel-1.5-15B-Thinker, a 15-billion parameter open-weights multimodal reasoning model that achieves frontier-level performance through training design rather than sheer scale. Starting from Pixtral-12B, we apply a progressive three-stage methodology: (1) depth upscaling to expand reasoning capacity without pretraining from scratch, (2) staged continual pre-training that first develops foundational text and vision understanding, then enhances visual reasoning through targeted synthetic data generation addressing spatial structure, compositional understanding, and fine-grained perception, and (3) high-quality text-only supervised fine-tuning on curated instruction-response pairs with explicit reasoning traces spanning mathematics, coding, science, and tool use. Notably, our model achieves competitive results without reinforcement learning or preference optimization, isolating the contribution of our data-centric continual pre-training approach. On the Artificial Analysis Intelligence Index, Apriel-1.5-15B-Thinker attains a score of 52, matching DeepSeek-R1-0528 despite requiring significantly fewer computational resources. Across ten image benchmarks, its performance is on average within five points of Gemini-2.5-Flash and Claude Sonnet-3.7, a key achievement for a model operating within single-GPU deployment constraints. Our results demonstrate that thoughtful mid-training 2 design can close substantial capability gaps without massive scale, making frontier-level multimodal reasoning accessible to organizations with limited infrastructure. We release the model checkpoint, all training recipes, and evaluation protocols under the MIT license to to advance open-source research.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†Apriel-1.5-15B-Thinkerï¼Œè¿™æ˜¯ä¸€ä¸ªæ‹¥æœ‰150äº¿å‚æ•°çš„å¼€æºå¤šæ¨¡æ€æ¨ç†æ¨¡å‹(multimodal reasoning model)ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–çš„è®­ç»ƒè®¾è®¡è€Œéå•çº¯æ‰©å¤§è§„æ¨¡æ¥è¾¾åˆ°å‰æ²¿æ€§èƒ½ã€‚æ¨¡å‹åŸºäºPixtral-12Bï¼Œé‡‡ç”¨äº†åŒ…å«æ·±åº¦æ‰©å±•(depth upscaling)ã€é’ˆå¯¹è§†è§‰æ¨ç†çš„é˜¶æ®µæ€§æŒç»­é¢„è®­ç»ƒ(continual pre-training)ä»¥åŠå¸¦æœ‰æ˜¾å¼æ¨ç†è½¨è¿¹çš„ç›‘ç£å¾®è°ƒ(supervised fine-tuning)åœ¨å†…çš„ä¸‰é˜¶æ®µæ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥æ¨¡å‹åœ¨æœªä½¿ç”¨å¼ºåŒ–å­¦ä¹ (reinforcement learning)æˆ–åå¥½ä¼˜åŒ–(preference optimization)çš„æƒ…å†µä¸‹ï¼Œè¯æ˜äº†ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„æ–¹æ³•èƒ½æ˜¾è‘—æå‡æ¨¡å‹èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨Artificial Analysis Intelligence Indexä¸Šè¾¾åˆ°äº†ä¸DeepSeek-R1-0528ç›¸å½“çš„æ°´å¹³ï¼Œä¸”åœ¨å›¾åƒåŸºå‡†æµ‹è¯•ä¸­ä¸Gemini-2.5-Flashå’ŒClaude Sonnet-3.7çš„è¡¨ç°éå¸¸æ¥è¿‘ã€‚ç”±äºæ”¯æŒå•GPU(single-GPU)éƒ¨ç½²ï¼Œå®ƒå¤§å¹…é™ä½äº†è·å–å‰æ²¿å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„é—¨æ§›ã€‚ç›®å‰ç›¸å…³çš„checkpointã€è®­ç»ƒæ–¹æ¡ˆåŠè¯„ä¼°åè®®å‡å·²åŸºäºMIT licenseå‘å¸ƒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01141v1",
      "published_date": "2025-10-01 17:29:35 UTC",
      "updated_date": "2025-10-01 17:29:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:57:07.172805+00:00"
    },
    {
      "arxiv_id": "2510.02401v1",
      "title": "Linear RNNs for autoregressive generation of long music samples",
      "title_zh": "ç”¨äºé•¿éŸ³ä¹æ ·æœ¬è‡ªå›å½’ç”Ÿæˆçš„çº¿æ€§ RNN",
      "authors": [
        "Konrad Szewczyk",
        "Daniel Gallo FernÃ¡ndez",
        "James Townsend"
      ],
      "abstract": "Directly learning to generate audio waveforms in an autoregressive manner is a challenging task, due to the length of the raw sequences and the existence of important structure on many different timescales. Traditional approaches based on recurrent neural networks, as well as causal convolutions and self-attention, have only had limited success on this task. However, recent work has shown that deep state space models, also referred to as linear RNNs, can be highly efficient in this context. In this work, we push the boundaries of linear RNNs applied to raw audio modeling, investigating the effects of different architectural choices and using context-parallelism to enable training on sequences up to one minute (1M tokens) in length. We present a model, HarmonicRNN, which attains state of the art log-likelihoods and perceptual metrics on small-scale datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨çº¿æ€§å¾ªç¯ç¥ç»ç½‘ç»œ(Linear RNNs)ï¼Œå³æ·±åº¦çŠ¶æ€ç©ºé—´æ¨¡å‹(State Space Models)ï¼Œè¿›è¡Œé•¿éŸ³é¢‘æ³¢å½¢è‡ªå›å½’(autoregressive)ç”Ÿæˆçš„æŒ‘æˆ˜ã€‚é’ˆå¯¹åŸå§‹éŸ³é¢‘åºåˆ—æé•¿ä¸”å…·æœ‰å¤æ‚å¤šå°ºåº¦ç»“æ„çš„ç‰¹ç‚¹ï¼Œä¼ ç»Ÿçš„RNNsã€å› æœå·ç§¯(causal convolutions)å’Œè‡ªæ³¨æ„åŠ›(self-attention)æœºåˆ¶åœ¨è¿™ä¸€ä»»åŠ¡ä¸Šçš„è¡¨ç°ç›¸å¯¹æœ‰é™ã€‚ä½œè€…é€šè¿‡ä¼˜åŒ–æ¶æ„é€‰æ‹©å¹¶å¼•å…¥ä¸Šä¸‹æ–‡å¹¶è¡Œ(context-parallelism)æŠ€æœ¯ï¼ŒæˆåŠŸå®ç°äº†åœ¨é•¿è¾¾ä¸€åˆ†é’Ÿï¼ˆçº¦1Mä¸ªtokensï¼‰åŸå§‹éŸ³é¢‘åºåˆ—ä¸Šçš„é«˜æ•ˆè®­ç»ƒã€‚ç ”ç©¶æå‡ºçš„HarmonicRNNæ¨¡å‹åœ¨å¤„ç†é•¿åºåˆ—éŸ³é¢‘å»ºæ¨¡æ—¶è¡¨ç°å“è¶Šï¼Œå…¶å¯¹æ•°ä¼¼ç„¶(log-likelihoods)å’Œæ„ŸçŸ¥æŒ‡æ ‡(perceptual metrics)åœ¨å°è§„æ¨¡æ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº†ç›®å‰çš„SOTAæ°´å¹³ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02401v1",
      "published_date": "2025-10-01 17:26:54 UTC",
      "updated_date": "2025-10-01 17:26:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:57:04.780752+00:00"
    },
    {
      "arxiv_id": "2510.01136v1",
      "title": "TabINR: An Implicit Neural Representation Framework for Tabular Data Imputation",
      "title_zh": "TabINRï¼šç”¨äºè¡¨æ ¼æ•°æ®æ’è¡¥çš„éšå¼ç¥ç»è¡¨ç¤ºæ¡†æ¶",
      "authors": [
        "Vincent Ochs",
        "Florentin Bieder",
        "Sidaty el Hadramy",
        "Paul Friedrich",
        "Stephanie Taha-Mehlitz",
        "Anas Taha",
        "Philippe C. Cattin"
      ],
      "abstract": "Tabular data builds the basis for a wide range of applications, yet real-world datasets are frequently incomplete due to collection errors, privacy restrictions, or sensor failures. As missing values degrade the performance or hinder the applicability of downstream models, and while simple imputing strategies tend to introduce bias or distort the underlying data distribution, we require imputers that provide high-quality imputations, are robust across dataset sizes and yield fast inference. We therefore introduce TabINR, an auto-decoder based Implicit Neural Representation (INR) framework that models tables as neural functions. Building on recent advances in generalizable INRs, we introduce learnable row and feature embeddings that effectively deal with the discrete structure of tabular data and can be inferred from partial observations, enabling instance adaptive imputations without modifying the trained model. We evaluate our framework across a diverse range of twelve real-world datasets and multiple missingness mechanisms, demonstrating consistently strong imputation accuracy, mostly matching or outperforming classical (KNN, MICE, MissForest) and deep learning based models (GAIN, ReMasker), with the clearest gains on high-dimensional datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TabINRï¼Œä¸€ç§åŸºäºè‡ªåŠ¨è§£ç å™¨ (auto-decoder) çš„éšå¼ç¥ç»è¡¨ç¤º (Implicit Neural Representation) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°å®ä¸–ç•Œè¡¨æ ¼æ•°æ®ä¸­å¸¸è§çš„ç¼ºå¤±å€¼è¡¥å…¨é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†è¡¨æ ¼å»ºæ¨¡ä¸ºç¥ç»å‡½æ•°ï¼Œé€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„è¡Œå’Œç‰¹å¾åµŒå…¥ (row and feature embeddings) æ¥æœ‰æ•ˆå¤„ç†è¡¨æ ¼æ•°æ®çš„ç¦»æ•£ç»“æ„ã€‚TabINR èƒ½å¤Ÿä»éƒ¨åˆ†è§‚æµ‹å€¼ä¸­æ¨æ–­åµŒå…¥å‘é‡ï¼Œä»è€Œåœ¨ä¸ä¿®æ”¹å·²è®­ç»ƒæ¨¡å‹çš„æƒ…å†µä¸‹å®ç°å®ä¾‹è‡ªé€‚åº”è¡¥å…¨ (instance adaptive imputations)ã€‚åœ¨ 12 ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šç§ç¼ºå¤±æœºåˆ¶ä¸‹å‡è¡¨ç°å‡ºæé«˜çš„è¡¥å…¨å‡†ç¡®æ€§ï¼Œå…¶æ€§èƒ½è¾¾åˆ°æˆ–è¶…è¿‡äº† KNNã€MICEã€MissForest ç­‰ä¼ ç»Ÿæ–¹æ³•ä»¥åŠ GAINã€ReMasker ç­‰æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚ç‰¹åˆ«æ˜¯åœ¨å¤„ç†é«˜ç»´æ•°æ®é›†æ—¶ï¼ŒTabINR å±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œä¸ºé«˜è´¨é‡ã€é²æ£’ä¸”å¿«é€Ÿçš„è¡¨æ ¼æ•°æ®è¡¥å…¨æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01136v1",
      "published_date": "2025-10-01 17:24:35 UTC",
      "updated_date": "2025-10-01 17:24:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:57:06.376510+00:00"
    },
    {
      "arxiv_id": "2510.01132v2",
      "title": "A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning",
      "title_zh": "å¤šè½®æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ å®æˆ˜æŒ‡å—",
      "authors": [
        "Ruiyi Wang",
        "Prithviraj Ammanabrolu"
      ],
      "abstract": "We study what actually works and what doesn't for training large language models as agents via multi-turn reinforcement learning. Despite rapid progress, existing frameworks and definitions are fragmented, and there is no systematic formulation or analysis of which design choices matter across tasks. We address this gap by first breaking down the design space into three inter-related pillars -- environment, reward, and policy -- and empirically derive a recipe for training LLM agents in situated textual domains. In particular, we test TextWorld and ALFWorld, popular domains for testing situated embodied reasoning, as well as SWE-Gym for more software engineering style tasks. (i) For the environment, we analyze the impacts of task complexity in terms of sizes of the state and action spaces as well as optimal solution length, finding that even simple environments within a domain can provide signal on how well an agent can generalize to more complex tasks. (ii) For the reward, we ablate relative reward sparsity, observing that while dense turn-level rewards accelerate training, performance and stability is highly dependent on the choice of RL algorithm. (iii) And for the agent's policy, we explore the interplay between reward sparsity and biased (PPO, GRPO) and unbiased (RLOO) policy gradient methods in addition to showing how to find the optimal Supervised Fine-tuning (SFT) to RL training ratio given a fixed budget. We distill these findings into a training recipe that guides co-design across the three pillars, facilitating research and practical efforts in multi-turn agentic RL. Code: https://github.com/pearls-lab/meow-tea-taro",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº†é€šè¿‡å¤šè½®å¼ºåŒ–å­¦ä¹ (multi-turn reinforcement learning)è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹(LLM)ä½œä¸ºæ™ºèƒ½ä½“(agents)çš„å®é™…æœ‰æ•ˆç­–ç•¥ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¡†æ¶ç¢ç‰‡åŒ–ä¸”ç¼ºä¹ç³»ç»Ÿè®¾è®¡åˆ†æçš„é—®é¢˜ã€‚ä½œè€…å°†è®¾è®¡ç©ºé—´æ‹†è§£ä¸ºç¯å¢ƒ(environment)ã€å¥–åŠ±(reward)å’Œç­–ç•¥(policy)ä¸‰å¤§æ”¯æŸ±ï¼Œå¹¶åœ¨TextWorldã€ALFWorldå’ŒSWE-Gymç­‰å¤šæ ·åŒ–é¢†åŸŸè¿›è¡Œäº†æ·±å…¥çš„å®è¯ç ”ç©¶ã€‚åœ¨ç¯å¢ƒè®¾è®¡ä¸Šï¼Œç ”ç©¶å‘ç°ç®€å•ç¯å¢ƒèƒ½å¤Ÿæœ‰æ•ˆæä¾›æ™ºèƒ½ä½“å‘å¤æ‚ä»»åŠ¡æ³›åŒ–çš„ä¿¡å·ï¼›åœ¨å¥–åŠ±æœºåˆ¶æ–¹é¢ï¼Œè™½ç„¶ç¨ å¯†çš„å›åˆçº§å¥–åŠ±(turn-level rewards)èƒ½åŠ é€Ÿè®­ç»ƒï¼Œä½†å…¶ç¨³å®šæ€§é«˜åº¦ä¾èµ–äºå…·ä½“å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„é€‰æ‹©ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶åˆ†æäº†PPOã€GRPOå’ŒRLOOç­‰ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä¸å¥–åŠ±ç¨€ç–æ€§ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œå¹¶ç¡®å®šäº†å›ºå®šé¢„ç®—ä¸‹ç›‘ç£å¾®è°ƒ(SFT)ä¸å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æœ€ä¼˜æ¯”ä¾‹ã€‚æœ€ç»ˆï¼Œç ”ç©¶è€…æ€»ç»“å‡ºä¸€å¥—æ¶µç›–ä¸‰å¤§æ”¯æŸ±çš„ååŒè®¾è®¡è®­ç»ƒæŒ‡å—ï¼Œä¸ºå¤šè½®æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„ç§‘ç ”ä¸å·¥ç¨‹å®è·µæä¾›äº†æ ‡å‡†åŒ–çš„å‚è€ƒè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01132v2",
      "published_date": "2025-10-01 17:23:04 UTC",
      "updated_date": "2025-12-06 07:53:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:57:24.691690+00:00"
    },
    {
      "arxiv_id": "2510.01123v1",
      "title": "Rethinking Thinking Tokens: LLMs as Improvement Operators",
      "title_zh": "é‡æ–°å®¡è§†æ€ç»´ Tokenï¼šå°†å¤§è¯­è¨€æ¨¡å‹è§†ä¸ºæ”¹è¿›ç®—å­",
      "authors": [
        "Lovish Madaan",
        "Aniket Didolkar",
        "Suchin Gururangan",
        "John Quan",
        "Ruan Silva",
        "Ruslan Salakhutdinov",
        "Manzil Zaheer",
        "Sanjeev Arora",
        "Anirudh Goyal"
      ],
      "abstract": "Reasoning training incentivizes LLMs to produce long chains of thought (long CoT), which among other things, allows them to explore solution strategies with self-checking. This results in higher accuracy, but inflates context length, token/compute cost, and answer latency. We ask: Can current models leverage their metacognition to provide other combinations on this Pareto frontier, e.g., better accuracy with lower context length and/or latency? Abstractly, we view the model as an improvement operator on its own \"thoughts\" with a continuum of possible strategies. We identify an interesting inference family Parallel-Distill-Refine (PDR), which performs the following: (i) generate diverse drafts in parallel; (ii) distill them into a bounded, textual workspace; and (iii) refine conditioned on this workspace, producing an output that seeds the next round. Importantly, context length (hence compute cost) is controllable via degree of parallelism, and is no longer conflated with the total number of generated tokens. We report PDR instantiations of current models that give better accuracy than long CoT while incurring lower latency. Setting degree of parallelism to 1 yields an interesting subcase, Sequential Refinement (SR) (iteratively improve a single candidate answer) which provides performance superior to long CoT. Success of such model orchestrations raises the question whether further training could shift the Pareto frontier. To this end, we train an 8B thinking model with Reinforcement Learning (RL) to make it consistent with PDR as the inference method. On math tasks with verifiable answers, iterative pipelines surpass single-pass baselines at matched sequential budgets, with PDR delivering the largest gains (e.g., +11% on AIME 2024 and +9% on AIME 2025).",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å°†å¤§è¯­è¨€æ¨¡å‹(LLMs)è§†ä¸ºè‡ªèº«æ€è€ƒçš„æ”¹è¿›ç®—å­(Improvement Operators)ï¼Œæ—¨åœ¨è§£å†³é•¿é“¾å¼æ€ç»´(Long CoT)å¸¦æ¥çš„ä¸Šä¸‹æ–‡é•¿åº¦è†¨èƒ€å’Œé«˜å»¶è¿Ÿé—®é¢˜ã€‚ä½œè€…æå‡ºäº†åä¸ºParallel-Distill-Refine (PDR)çš„æ¨ç†å®¶æ—ï¼Œé€šè¿‡å¹¶è¡Œç”Ÿæˆè‰ç¨¿ã€å°†å…¶è’¸é¦è‡³å—é™å·¥ä½œç©ºé—´å¹¶è¿›è¡Œç²¾ç‚¼ï¼Œå®ç°äº†ä¸Šä¸‹æ–‡é•¿åº¦ä¸ç”Ÿæˆæ€»Tokenæ•°çš„è§£è€¦ã€‚å®éªŒè¯æ˜ï¼ŒPDRåœ¨ä¿æŒè¾ƒä½å»¶è¿Ÿçš„åŒæ—¶ï¼Œå…¶å‡†ç¡®ç‡ä¼˜äºä¼ ç»Ÿçš„Long CoTï¼Œå…¶ç‰¹ä¾‹é¡ºåºç²¾ç‚¼(Sequential Refinement, SR)ä¹Ÿè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶é€šè¿‡å¼ºåŒ–å­¦ä¹ (RL)è®­ç»ƒäº†ä¸€ä¸ª8Bæ¨¡å‹ä»¥é€‚é…PDRæ¨ç†æ–¹æ³•ï¼Œåœ¨AIME 2024å’Œ2025ç­‰æ•°å­¦ä»»åŠ¡ä¸Šç›¸æ¯”å•æ¬¡æ¨ç†åŸºçº¿åˆ†åˆ«æå‡äº†11%å’Œ9%ï¼Œè¯æ˜äº†è¿™ç§è¿­ä»£æµæ°´çº¿åœ¨å›ºå®šåºåˆ—é¢„ç®—ä¸‹çš„æ˜¾è‘—ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.01123v1",
      "published_date": "2025-10-01 17:08:59 UTC",
      "updated_date": "2025-10-01 17:08:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:57:28.474075+00:00"
    },
    {
      "arxiv_id": "2510.01115v2",
      "title": "Exploring Network-Knowledge Graph Duality: A Case Study in Agentic Supply Chain Risk Analysis",
      "title_zh": "æ¢ç©¶ç½‘ç»œä¸çŸ¥è¯†å›¾è°±çš„å¯¹å¶æ€§ï¼šåŸºäºæ™ºèƒ½ä½“çš„ä¾›åº”é“¾é£é™©åˆ†ææ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Evan Heus",
        "Rick Bookstaber",
        "Dhruv Sharma"
      ],
      "abstract": "Large Language Models (LLMs) struggle with the complex, multi-modal, and network-native data underlying financial risk. Standard Retrieval-Augmented Generation (RAG) oversimplifies relationships, while specialist models are costly and static. We address this gap with an LLM-centric agent framework for supply chain risk analysis. Our core contribution is to exploit the inherent duality between networks and knowledge graphs (KG). We treat the supply chain network as a KG, allowing us to use structural network science principles for retrieval. A graph traverser, guided by network centrality scores, efficiently extracts the most economically salient risk paths. An agentic architecture orchestrates this graph retrieval alongside data from numerical factor tables and news streams. Crucially, it employs novel ``context shells'' -- descriptive templates that embed raw figures in natural language -- to make quantitative data fully intelligible to the LLM. This lightweight approach enables the model to generate concise, explainable, and context-rich risk narratives in real-time without costly fine-tuning or a dedicated graph database.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†é‡‘èé£é™©æ•°æ®ä¸­å¤æ‚çš„ç½‘ç»œåŸç”Ÿç‰¹æ€§ä»¥åŠæ ‡å‡†æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)å®¹æ˜“è¿‡åº¦ç®€åŒ–å…³ç³»çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ä»¥å¤§è¯­è¨€æ¨¡å‹ä¸ºæ ¸å¿ƒçš„ä»£ç†(Agentic)ä¾›åº”é“¾é£é™©åˆ†ææ¡†æ¶ã€‚å…¶æ ¸å¿ƒè´¡çŒ®åœ¨äºåˆ©ç”¨äº†ç½‘ç»œ(Network)ä¸çŸ¥è¯†å›¾è°±(Knowledge Graph)ä¹‹é—´çš„å›ºæœ‰å¯¹å¶æ€§ï¼Œå°†ä¾›åº”é“¾ç½‘ç»œè§†ä¸ºçŸ¥è¯†å›¾è°±ï¼Œå¹¶è¿ç”¨ç»“æ„åŒ–ç½‘ç»œç§‘å­¦åŸç†è¿›è¡Œä¿¡æ¯æ£€ç´¢ã€‚æ¡†æ¶é‡‡ç”¨å—ç½‘ç»œä¸­å¿ƒæ€§è¯„åˆ†(Network Centrality Scores)å¼•å¯¼çš„å›¾å½¢éå†å™¨(Graph Traverser)ï¼Œèƒ½å¤Ÿé«˜æ•ˆæå–ç»æµä»·å€¼æœ€æ˜¾è‘—çš„é£é™©è·¯å¾„ã€‚ä»£ç†æ¶æ„ååŒç¼–æ’äº†è¿™ç§å›¾å½¢æ£€ç´¢ä»¥åŠæ¥è‡ªæ•°å€¼å› å­è¡¨å’Œæ–°é—»æµçš„å¤šæ¨¡æ€æ•°æ®ï¼Œå¹¶å¼•å…¥åˆ›æ–°çš„ä¸Šä¸‹æ–‡å¤–å£³(Context Shells)æè¿°æ¨¡æ¿ï¼Œå°†åŸå§‹æ•°å€¼åµŒå…¥è‡ªç„¶è¯­è¨€ä»¥å¢å¼ºæ¨¡å‹çš„ç†è§£åŠ›ã€‚è¿™ç§è½»é‡çº§æ–¹æ³•ä½¿æ¨¡å‹æ— éœ€æ˜‚è´µçš„å¾®è°ƒ(Fine-tuning)æˆ–ä¸“ç”¨å›¾å½¢æ•°æ®åº“ï¼Œå³å¯å®æ—¶ç”Ÿæˆç®€æ´ã€å¯è§£é‡Šä¸”èƒŒæ™¯ä¸°å¯Œçš„é£é™©åˆ†æå™äº‹ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA",
        "econ.TH",
        "physics.soc-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to the 2nd Workshop on LLMs and Generative AI in Finance: International Conference on AI in Finance(ICAIF) 2025;7 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.01115v2",
      "published_date": "2025-10-01 17:02:14 UTC",
      "updated_date": "2025-12-16 15:50:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:57:31.282979+00:00"
    },
    {
      "arxiv_id": "2510.01114v2",
      "title": "PRISM-Consult: A Panel-of-Experts Architecture for Clinician-Aligned Diagnosis",
      "title_zh": "PRISM-Consultï¼šé¢å‘ä¸´åºŠå¯¹é½è¯Šæ–­çš„ä¸“å®¶ç»„æ¶æ„",
      "authors": [
        "Lionel Levine",
        "John Santerre",
        "Alexander S. Young",
        "T. Barry Levine",
        "Francis Campion",
        "Majid Sarrafzadeh"
      ],
      "abstract": "We present PRISM-Consult, a clinician-aligned panel-of-experts architecture that extends the compact PRISM sequence model into a routed family of domain specialists. Episodes are tokenized as structured clinical events; a light-weight router reads the first few tokens and dispatches to specialist models (Cardiac-Vascular, Pulmonary, Gastro-Oesophageal, Musculoskeletal, Psychogenic). Each specialist inherits PRISM's small transformer backbone and token template, enabling parameter efficiency and interpretability. This initial study evaluates a scoped panel of five specialist families defined by high-impact ED diagnostic groups. On real-world Emergency Department cohorts, specialists exhibit smooth convergence with low development perplexities across domains, while the router achieves high routing quality and large compute savings versus consult-all under a safety-first policy. We detail the data methodology (initial vs.\\ conclusive ICD-9 families), routing thresholds and calibration, and report per-domain results to avoid dominance by common events. The framework provides a practical path to safe, auditable, and low-latency consult at scale, and we outline validation steps-external/temporal replication, asymmetric life-threat thresholds, and multi-label arbitration-to meet prospective clinical deployment standards.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PRISM-Consultï¼Œä¸€ç§é¢å‘ä¸´åºŠåŒ»ç”Ÿéœ€æ±‚çš„å¤šä¸“å®¶é¢æ¿(Panel-of-Experts)æ¶æ„ï¼Œæ—¨åœ¨å°†ç´§å‡‘çš„ PRISM åºåˆ—æ¨¡å‹æ‰©å±•ä¸ºè·¯ç”±é©±åŠ¨çš„é¢†åŸŸä¸“å®¶å®¶æ—ã€‚è¯¥æ¡†æ¶å°†ä¸´åºŠäº‹ä»¶è½¬åŒ–ä¸ºç»“æ„åŒ–çš„ Token åºåˆ—ï¼Œå¹¶é€šè¿‡è½»é‡çº§è·¯ç”±(Router)å°†ä»»åŠ¡ç²¾ç¡®åˆ†å‘è‡³å¿ƒè¡€ç®¡ã€è‚ºéƒ¨ã€èƒƒè‚ é“ã€è‚Œè‚‰éª¨éª¼åŠå¿ƒç†å› ç´ ç­‰äº”ä¸ªä¸“é—¨çš„ä¸“å®¶å®¶æ—ã€‚æ¯ä¸ªä¸“å®¶æ¨¡å‹ç»§æ‰¿äº† PRISM çš„å°å‹ Transformer éª¨æ¶ï¼Œåœ¨å®ç°å‚æ•°é«˜æ•ˆæ€§çš„åŒæ—¶ç¡®ä¿äº†å†³ç­–è¿‡ç¨‹çš„å¯è§£é‡Šæ€§ã€‚åœ¨çœŸå®æ€¥è¯Šç§‘(Emergency Department)æ•°æ®é›†çš„è¯„ä¼°ä¸­ï¼Œå„é¢†åŸŸä¸“å®¶æ¨¡å‹å‡è¡¨ç°å‡ºè‰¯å¥½çš„æ”¶æ•›æ€§å’Œè¾ƒä½çš„å›°æƒ‘åº¦(Perplexity)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥è·¯ç”±æœºåˆ¶åœ¨ä¿éšœå®‰å…¨çš„å‰æä¸‹ï¼Œç›¸è¾ƒäºå…¨é‡å’¨è¯¢æ¨¡å¼æ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ï¼Œå¹¶ä¿æŒäº†æé«˜çš„è·¯ç”±è´¨é‡ã€‚PRISM-Consult ä¸ºåœ¨å¤§è§„æ¨¡ä¸´åºŠåœºæ™¯ä¸­å®ç°å®‰å…¨ã€å¯å®¡è®¡ä¸”ä½å»¶è¿Ÿçš„ä¸“å®¶å’¨è¯¢æä¾›äº†ä¸€æ¡å®ç”¨è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.01114v2",
      "published_date": "2025-10-01 17:00:05 UTC",
      "updated_date": "2026-01-19 06:17:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:57:34.377879+00:00"
    },
    {
      "arxiv_id": "2510.01094v1",
      "title": "Optimizing Fairness in Production Planning: A Human-Centric Approach to Machine and Workforce Allocation",
      "title_zh": "ä¼˜åŒ–ç”Ÿäº§è§„åˆ’ä¸­çš„å…¬å¹³æ€§ï¼šä¸€ç§ä»¥äººä¸ºæœ¬çš„æœºå™¨ä¸åŠ³åŠ¨åŠ›åˆ†é…æ–¹æ³•",
      "authors": [
        "Alexander Nasuta",
        "Alessandro Cisi",
        "Sylwia Olbrych",
        "Gustavo Vieira",
        "Rui Fernandes",
        "Lucas Paletta",
        "Marlene Mayr",
        "Rishyank Chevuri",
        "Robert Woitsch",
        "Hans Aoyang Zhou",
        "Anas Abdelrazeq",
        "Robert H. Schmitt"
      ],
      "abstract": "This work presents a two-layer, human-centric production planning framework designed to optimize both operational efficiency and workforce fairness in industrial manufacturing. The first layer formulates the Order-Line allocation as a Constraint Programming (CP) problem, generating high-utilization production schedules that respect machine capacities, processing times, and due dates. The second layer models Worker-Line allocation as a Markov Decision Process (MDP), integrating human factors such as worker preference, experience, resilience, and medical constraints into the assignment process. Three solution strategies, greedy allocation, MCTS, and RL, are implemented and compared across multiple evaluation scenarios. The proposed system is validated through 16 test sessions with domain experts from the automotive industry, combining quantitative key performance indicators (KPIs) with expert ratings. Results indicate that the CP-based scheduling approach produces compact, feasible production plans with low tardiness, while the MDP-based worker allocation significantly improves fairness and preference alignment compared to baseline approaches. Domain experts rated both the Order-Line and Worker-Line components as effective and highlighted opportunities to further refine the objective function to penalize excessive earliness and improve continuity in worker assignments. Overall, the findings demonstrate that combining CP with learning-based decision-making provides a robust approach for human-centric production planning. The approach enables simultaneous optimization of throughput and workforce well-being, offering a practical foundation for fair and efficient manufacturing scheduling in industrial settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªä¸¤å±‚çš„äººæœºåä½œç”Ÿäº§è®¡åˆ’æ¡†æ¶ï¼Œæ—¨åœ¨åŒæ—¶ä¼˜åŒ–å·¥ä¸šåˆ¶é€ ä¸­çš„è¿è¥æ•ˆç‡å’ŒåŠ³åŠ¨åŠ›å…¬å¹³æ€§ã€‚æ¡†æ¶ç¬¬ä¸€å±‚å°† Order-Line åˆ†é…å»ºæ¨¡ä¸º Constraint Programming (CP) é—®é¢˜ï¼Œé€šè¿‡è€ƒè™‘æœºå™¨äº§èƒ½ã€å¤„ç†æ—¶é—´å’Œæˆªæ­¢æ—¥æœŸç”Ÿæˆé«˜åˆ©ç”¨ç‡çš„ç”Ÿäº§è°ƒåº¦ã€‚ç¬¬äºŒå±‚å°† Worker-Line åˆ†é…å»ºæ¨¡ä¸º Markov Decision Process (MDP)ï¼Œå°†å‘˜å·¥åå¥½ã€ç»éªŒã€éŸ§æ€§åŠåŒ»ç–—é™åˆ¶ç­‰å› ç´ æ•´åˆè¿›åˆ†é…æµç¨‹ï¼Œå¹¶å¯¹æ¯”äº† greedy allocationã€MCTS å’Œ RL ä¸‰ç§æ±‚è§£ç­–ç•¥ã€‚é€šè¿‡æ±½è½¦è¡Œä¸šä¸“å®¶çš„16æ¬¡æµ‹è¯•éªŒè¯è¡¨æ˜ï¼ŒåŸºäº CP çš„è°ƒåº¦æ–¹æ³•èƒ½äº§ç”Ÿä½å»¶è¿Ÿçš„å¯è¡Œè®¡åˆ’ï¼Œè€ŒåŸºäº MDP çš„å·¥äººåˆ†é…åœ¨å…¬å¹³æ€§å’Œåå¥½ä¸€è‡´æ€§ä¸Šæ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚ä¸“å®¶è¯„åˆ†è‚¯å®šäº†è¯¥ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶ä¹ŸæŒ‡å‡ºäº†è¿›ä¸€æ­¥ç»†åŒ–ç›®æ ‡å‡½æ•°ä»¥å‡å°‘è¿‡æ—©å®Œå·¥å¹¶æé«˜åˆ†é…è¿ç»­æ€§çš„æ–¹å‘ã€‚è¯¥ç ”ç©¶è¯æ˜äº†ç»“åˆ CP ä¸å­¦ä¹ å‹å†³ç­–æ–¹æ³•èƒ½æœ‰æ•ˆå…¼é¡¾åˆ¶é€ ååé‡ä¸åŠ³åŠ¨åŠ›ç¦ç¥‰ï¼Œä¸ºå…¬å¹³é«˜æ•ˆçš„å·¥ä¸šæ’ç¨‹å¥ å®šäº†å®è·µåŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01094v1",
      "published_date": "2025-10-01 16:41:18 UTC",
      "updated_date": "2025-10-01 16:41:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:57:35.974880+00:00"
    },
    {
      "arxiv_id": "2510.01088v1",
      "title": "Safety Instincts: LLMs Learn to Trust Their Internal Compass for Self-Defense",
      "title_zh": "Safety Instinctsï¼šå¤§è¯­è¨€æ¨¡å‹å­¦ä¼šä¿¡ä»»å…¶å†…åœ¨â€œæŒ‡å—é’ˆâ€ä»¥å®ç°è‡ªæˆ‘é˜²å¾¡",
      "authors": [
        "Guobin Shen",
        "Dongcheng Zhao",
        "Haibo Tong",
        "Jindong Li",
        "Feifei Zhao",
        "Yi Zeng"
      ],
      "abstract": "Ensuring Large Language Model (LLM) safety remains challenging due to the absence of universal standards and reliable content validators, making it difficult to obtain effective training signals. We discover that aligned models already possess robust internal safety beliefs: they consistently produce high-confidence refusals to harmful requests while exhibiting high entropy when generating potentially dangerous content. This entropy gap reveals an untapped signal--models intrinsically \"know\" when to refuse. We introduce Safety Instincts Reinforcement Learning (SIRL), which transforms this internal confidence into a self-generated reward signal, eliminating dependence on external validators or human annotations. SIRL teaches models to trust their safety instincts by reinforcing low-entropy refusal behaviors. Evaluated on Llama and Qwen models, SIRL maintains 89%+ Defense Success Rates (DSRs) against 20+ jailbreak methods, from static prompts to adaptive attacks. Using only 15,000 unlabeled prompts, SIRL surpasses resource-intensive supervised methods while preserving performance on mathematics, coding, and conversation benchmarks. Our work demonstrates that effective alignment can emerge from within, paving the way for more autonomous and robust AI safety mechanisms that scale without extensive human oversight.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å®‰å…¨å¯¹é½ä¸­ç¼ºä¹ç»Ÿä¸€æ ‡å‡†å’Œå¯é å¤–éƒ¨éªŒè¯å™¨çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º Safety Instincts Reinforcement Learning (SIRL) çš„æ–°å‹å¯¹é½æ–¹æ³•ã€‚ä½œè€…å‘ç°å·²å¯¹é½çš„æ¨¡å‹å†…éƒ¨å­˜åœ¨ç¨³å¥çš„å®‰å…¨ä¿¡å¿µï¼Œå³åœ¨æ‹’ç»æœ‰å®³è¯·æ±‚æ—¶è¡¨ç°å‡ºé«˜ç½®ä¿¡åº¦ï¼Œè€Œåœ¨ç”Ÿæˆå±é™©å†…å®¹æ—¶åˆ™è¡¨ç°å‡ºé«˜ç†µå€¼ (Entropy)ã€‚SIRL æ¡†æ¶é€šè¿‡åˆ©ç”¨è¿™ä¸€ç†µå€¼å·®è·ï¼Œå°†æ¨¡å‹çš„å†…åœ¨ç½®ä¿¡åº¦è½¬åŒ–ä¸ºè‡ªæˆ‘ç”Ÿæˆçš„å¥–åŠ±ä¿¡å·ï¼Œä»è€Œæ‘†è„±äº†å¯¹å¤–éƒ¨éªŒè¯å™¨æˆ–äººå·¥æ³¨é‡Šçš„ä¾èµ–ã€‚è¯¥æ–¹æ³•æ—¨åœ¨é€šè¿‡å¼ºåŒ–ä½ç†µçš„æ‹’ç»è¡Œä¸ºï¼Œå¼•å¯¼æ¨¡å‹ä¿¡ä»»å…¶å†…åœ¨çš„â€œå®‰å…¨æœ¬èƒ½â€ã€‚åœ¨ Llama å’Œ Qwen æ¨¡å‹ä¸Šçš„å®éªŒè¯æ˜ï¼ŒSIRL ä»…éœ€ 15,000 ä¸ªæ— æ ‡ç­¾æç¤ºè¯ï¼Œå³å¯åœ¨ 20 å¤šç§è¶Šç‹±æ”»å‡» (Jailbreak Attacks) ä¸‹ä¿æŒ 89% ä»¥ä¸Šçš„é˜²å¾¡æˆåŠŸç‡ (DSRs)ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæ•°å­¦ã€ä»£ç å’Œå¯¹è¯åŸºå‡†æ€§èƒ½çš„åŒæ—¶ï¼Œå…¶è¡¨ç°è¶…è¶Šäº†èµ„æºå¯†é›†å‹çš„ç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚è¯¥é¡¹å·¥ä½œå±•ç¤ºäº†æœ‰æ•ˆçš„å®‰å…¨å¯¹é½å¯ä»¥ä»æ¨¡å‹å†…éƒ¨è‡ªå‘äº§ç”Ÿï¼Œä¸ºæ„å»ºæ— éœ€å¤§è§„æ¨¡äººå·¥å¹²é¢„çš„è‡ªä¸»ä¸”ç¨³å¥çš„ AI å®‰å…¨æœºåˆ¶å¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01088v1",
      "published_date": "2025-10-01 16:35:03 UTC",
      "updated_date": "2025-10-01 16:35:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:57:46.887702+00:00"
    },
    {
      "arxiv_id": "2510.01077v1",
      "title": "CodeGenLink: A Tool to Find the Likely Origin and License of Automatically Generated Code",
      "title_zh": "CodeGenLinkï¼šä¸€ç§ç”¨äºè¯†åˆ«è‡ªåŠ¨ç”Ÿæˆä»£ç æ½œåœ¨æ¥æºåŠè®¸å¯è¯çš„å·¥å…·",
      "authors": [
        "Daniele Bifolco",
        "Guido Annicchiarico",
        "Pierluigi Barbiero",
        "Massimiliano Di Penta",
        "Fiorella Zampetti"
      ],
      "abstract": "Large Language Models (LLMs) are widely used in software development tasks nowadays. Unlike reusing code taken from the Web, for LLMs' generated code, developers are concerned about its lack of trustworthiness and possible copyright or licensing violations, due to the lack of code provenance information. This paper proposes CodeGenLink, a GitHub CoPilot extension for Visual Studio Code aimed at (i) suggesting links containing code very similar to automatically generated code, and (ii) whenever possible, indicating the license of the likely origin of the code. CodeGenLink retrieves candidate links by combining LLMs with their web search features and then performs similarity analysis between the generated and retrieved code. Preliminary results show that CodeGenLink effectively filters unrelated links via similarity analysis and provides licensing information when available. Tool URL: https://github.com/danielebifolco/CodeGenLink Tool Video: https://youtu.be/M6nqjBf9_pw",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CodeGenLinkï¼Œè¿™æ˜¯ä¸€æ¬¾ä¸“ä¸º Visual Studio Code å¼€å‘çš„ GitHub CoPilot æ‰©å±•æ’ä»¶ï¼Œæ—¨åœ¨è§£å†³ Large Language Models (LLMs) ç”Ÿæˆä»£ç æ—¶å› ç¼ºä¹å‡ºå¤„ä¿¡æ¯è€Œå¯¼è‡´çš„ä¿¡ä»»åº¦åŠç‰ˆæƒåˆè§„æ€§é—®é¢˜ã€‚è¯¥å·¥å…·èƒ½å¤Ÿä¸ºè‡ªåŠ¨ç”Ÿæˆçš„ä»£ç å¯»æ‰¾é«˜ç›¸ä¼¼åº¦çš„åŸå§‹é“¾æ¥ï¼Œå¹¶åœ¨å¯èƒ½çš„æƒ…å†µä¸‹è¯†åˆ«å…¶å¯¹åº”çš„ License ä¿¡æ¯ã€‚CodeGenLink çš„æ ¸å¿ƒå·¥ä½œæµç»“åˆäº† LLMs åŠå…¶ Web Search åŠŸèƒ½æ¥æ£€ç´¢å€™é€‰é“¾æ¥ï¼Œéšåé€šè¿‡ Similarity Analysis å¯¹ç”Ÿæˆä»£ç ä¸æ£€ç´¢ä»£ç è¿›è¡Œæ·±åº¦æ¯”å¯¹ã€‚åˆæ­¥å®éªŒç»“æœè¡¨æ˜ï¼ŒCodeGenLink èƒ½å¤Ÿåˆ©ç”¨ç›¸ä¼¼åº¦åˆ†ææœ‰æ•ˆè¿‡æ»¤æ— å…³é“¾æ¥ï¼Œå¹¶æä¾›å¯ç”¨çš„è®¸å¯è¯ä¿¡æ¯ã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘è€…åœ¨ä½¿ç”¨ AI è¾…åŠ©ç¼–ç¨‹æ—¶è¿½è¸ªä»£ç æ¥æºåŠè¯„ä¼°ç‰ˆæƒé£é™©æä¾›äº†å®ç”¨çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Proceedings of the 40th IEEE/ACM International Conference on Automated Software Engineering (ASE 2025), November 16-20 2025, Seoul, South Korea",
      "pdf_url": "https://arxiv.org/pdf/2510.01077v1",
      "published_date": "2025-10-01 16:21:13 UTC",
      "updated_date": "2025-10-01 16:21:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:57:40.967981+00:00"
    },
    {
      "arxiv_id": "2510.01303v1",
      "title": "Low Rank Gradients and Where to Find Them",
      "title_zh": "ä½ç§©æ¢¯åº¦åŠå…¶åˆ†å¸ƒæ¢ç©¶",
      "authors": [
        "Rishi Sonthalia",
        "Michael Murray",
        "Guido MontÃºfar"
      ],
      "abstract": "This paper investigates low-rank structure in the gradients of the training loss for two-layer neural networks while relaxing the usual isotropy assumptions on the training data and parameters. We consider a spiked data model in which the bulk can be anisotropic and ill-conditioned, we do not require independent data and weight matrices and we also analyze both the mean-field and neural-tangent-kernel scalings. We show that the gradient with respect to the input weights is approximately low rank and is dominated by two rank-one terms: one aligned with the bulk data-residue , and another aligned with the rank one spike in the input data. We characterize how properties of the training data, the scaling regime and the activation function govern the balance between these two components. Additionally, we also demonstrate that standard regularizers, such as weight decay, input noise and Jacobian penalties, also selectively modulate these components. Experiments on synthetic and real data corroborate our theoretical predictions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¸¤å±‚ç¥ç»ç½‘ç»œ (two-layer neural networks) åœ¨æ”¾å®½å„å‘åŒæ€§ (isotropy) å‡è®¾ä¸‹çš„è®­ç»ƒæŸå¤±æ¢¯åº¦çš„ä½ç§©ç»“æ„ (low-rank structure)ã€‚é€šè¿‡å¼•å…¥å°–å³°æ•°æ®æ¨¡å‹ (spiked data model) å¹¶åˆ†æå¹³å‡åœº (mean-field) ä¸ç¥ç»åˆ‡å‘æ ¸ (neural-tangent-kernel) ç¼©æ”¾ï¼Œç ”ç©¶å‘ç°è¾“å…¥æƒé‡çš„æ¢¯åº¦è¿‘ä¼¼ä¸ºä½ç§©ï¼Œä¸”ä¸»è¦ç”±ä¸¤ä¸ªç§©ä¸º 1 çš„é¡¹ä¸»å¯¼ï¼Œåˆ†åˆ«ä¸æ•´ä½“æ•°æ®æ®‹å·® (bulk data-residue) å’Œè¾“å…¥æ•°æ®ä¸­çš„ç§©ä¸º 1 çš„å°–å³° (rank one spike) å¯¹é½ã€‚ä½œè€…ç³»ç»Ÿåœ°åˆ»ç”»äº†è®­ç»ƒæ•°æ®å±æ€§ã€ç¼©æ”¾æœºåˆ¶åŠæ¿€æ´»å‡½æ•° (activation function) å¦‚ä½•å…±åŒå†³å®šè¿™ä¸¤ç±»ç»„æˆéƒ¨åˆ†çš„æƒé‡å¹³è¡¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¯æ˜äº†æƒé‡è¡°å‡ (weight decay)ã€è¾“å…¥å™ªå£° (input noise) å’Œ Jacobian æƒ©ç½š (Jacobian penalties) ç­‰æ ‡å‡†æ­£åˆ™åŒ–é¡¹èƒ½é€‰æ‹©æ€§åœ°è°ƒèŠ‚è¿™äº›æ¢¯åº¦æˆåˆ†ã€‚åœ¨åˆæˆæ•°æ®ä¸çœŸå®æ•°æ®ä¸Šçš„å®éªŒä¸€è‡´éªŒè¯äº†ä¸Šè¿°ç†è®ºé¢„æµ‹ï¼Œä¸ºç†è§£å¤æ‚æ•°æ®åˆ†å¸ƒä¸‹çš„æ·±åº¦å­¦ä¹ ä¼˜åŒ–åŠ¨åŠ›å­¦æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01303v1",
      "published_date": "2025-10-01 16:20:19 UTC",
      "updated_date": "2025-10-01 16:20:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:57:47.973757+00:00"
    },
    {
      "arxiv_id": "2510.01069v1",
      "title": "Typed Chain-of-Thought: A Curry-Howard Framework for Verifying LLM Reasoning",
      "title_zh": "ç±»å‹åŒ–æ€ç»´é“¾ï¼šä¸€ç§ç”¨äºéªŒè¯å¤§è¯­è¨€æ¨¡å‹æ¨ç†çš„ Curry-Howard æ¡†æ¶",
      "authors": [
        "Elija Perrier"
      ],
      "abstract": "While Chain-of-Thought (CoT) prompting enhances the reasoning capabilities of large language models, the faithfulness of the generated rationales remains an open problem for model interpretability. We propose a novel theoretical lens for this problem grounded in the Curry-Howard correspondence, which posits a direct relationship between formal proofs and computer programs. Under this paradigm, a faithful reasoning trace is analogous to a well-typed program, where each intermediate step corresponds to a typed logical inference. We operationalise this analogy, presenting methods to extract and map the informal, natural language steps of CoT into a formal, typed proof structure. Successfully converting a CoT trace into a well-typed proof serves as a strong, verifiable certificate of its computational faithfulness, moving beyond heuristic interpretability towards formal verification. Our framework provides a methodology to transform plausible narrative explanations into formally verifiable programs, offering a path towards building more reliable and trustworthy AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Chain-of-Thought (CoT) æç¤ºä¸­æ¨ç†è½¨è¿¹çš„å¿ å®æ€§ (faithfulness) éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäº Curry-Howard correspondence çš„æ–°å‹ç†è®ºæ¡†æ¶ã€‚åœ¨è¯¥æ¡†æ¶ä¸‹ï¼Œå¿ å®çš„æ¨ç†è½¨è¿¹è¢«ç±»æ¯”ä¸º well-typed programï¼Œå…¶ä¸­æ¯ä¸€ä¸ªä¸­é—´æ­¥éª¤éƒ½å¯¹åº”äºç‰¹å®šçš„ typed logical inferenceã€‚ç ”ç©¶é€šè¿‡å°†éæ­£å¼çš„è‡ªç„¶è¯­è¨€ CoT æ­¥éª¤æå–å¹¶æ˜ å°„ä¸ºå½¢å¼åŒ–çš„ typed proof structureï¼Œå®ç°äº†æ¨ç†è¿‡ç¨‹å‘å½¢å¼åŒ–è¯æ˜çš„è½¬åŒ–ã€‚æˆåŠŸè½¬æ¢çš„è¯æ˜èƒ½å¤Ÿä½œä¸º CoT è®¡ç®—å¿ å®æ€§çš„å¼ºåŠ›å¯éªŒè¯è¯ä¹¦ï¼Œæ¨åŠ¨æ¨¡å‹å¯è§£é‡Šæ€§ä»å¯å‘å¼æ–¹æ³•å‘å½¢å¼åŒ–éªŒè¯ (formal verification) æ¼”è¿›ã€‚è¯¥æ¡†æ¶ä¸ºå°†å™è¿°æ€§è§£é‡Šè½¬åŒ–ä¸ºå¯éªŒè¯ç¨‹åºæä¾›äº†ç³»ç»Ÿæ€§æ–¹æ³•ï¼Œä¸ºæ„å»ºæ›´å¯é ä¸”å¯ä¿¡çš„ AI ç³»ç»Ÿå¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2510.01069v1",
      "published_date": "2025-10-01 16:06:40 UTC",
      "updated_date": "2025-10-01 16:06:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:57:48.875764+00:00"
    },
    {
      "arxiv_id": "2510.03326v1",
      "title": "NS-Pep: De novo Peptide Design with Non-Standard Amino Acids",
      "title_zh": "NS-Pepï¼šåŒ…å«éæ ‡å‡†æ°¨åŸºé…¸çš„ä»å¤´å¤šè‚½è®¾è®¡",
      "authors": [
        "Tao Guo",
        "Junbo Yin",
        "Yu Wang",
        "Xin Gao"
      ],
      "abstract": "Peptide drugs incorporating non-standard amino acids (NSAAs) offer improved binding affinity and improved pharmacological properties. However, existing peptide design methods are limited to standard amino acids, leaving NSAA-aware design largely unexplored. We introduce NS-Pep, a unified framework for co-designing peptide sequences and structures with NSAAs. The main challenge is that NSAAs are extremely underrepresented-even the most frequent one, SEP, accounts for less than 0.4% of residues-resulting in a severe long-tailed distribution. To improve generalization to rare amino acids, we propose Residue Frequency-Guided Modification (RFGM), which mitigates over-penalization through frequency-aware logit calibration, supported by both theoretical and empirical analysis. Furthermore, we identify that insufficient side-chain modeling limits geometric representation of NSAAs. To address this, we introduce Progressive Side-chain Perception (PSP) for coarse-to-fine torsion and location prediction, and Interaction-Aware Weighting (IAW) to emphasize pocket-proximal residues. Moreover, NS-Pep generalizes naturally to the peptide folding task with NSAAs, addressing a major limitation of current tools. Experiments show that NS-Pep improves sequence recovery rate and binding affinity by 6.23% and 5.12%, respectively, and outperforms AlphaFold3 by 17.76% in peptide folding success rate.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† NS-Pepï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºååŒè®¾è®¡åŒ…å«éæ ‡å‡†æ°¨åŸºé…¸ (Non-Standard Amino Acids, NSAAs) çš„è‚½åºåˆ—å’Œç»“æ„çš„ç»Ÿä¸€æ¡†æ¶ã€‚é’ˆå¯¹ NSAAs æ•°æ®ç¨€ç¼ºå¯¼è‡´çš„ä¸¥é‡é•¿å°¾åˆ†å¸ƒé—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†æ®‹ç•™é¢‘ç‡å¼•å¯¼ä¿®æ”¹ (Residue Frequency-Guided Modification, RFGM)ï¼Œé€šè¿‡é¢‘ç‡æ„ŸçŸ¥çš„é€»è¾‘æ ¡å‡†æ˜¾è‘—æé«˜å¯¹ç¨€æœ‰æ°¨åŸºé…¸çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†å…‹æœä¾§é“¾å»ºæ¨¡ä¸è¶³å¯¹å‡ ä½•è¡¨ç¤ºçš„é™åˆ¶ï¼ŒNS-Pep å¼•å…¥äº†æ¸è¿›å¼ä¾§é“¾æ„ŸçŸ¥ (Progressive Side-chain Perception, PSP) è¿›è¡Œç”±ç²—åˆ°ç²¾çš„é¢„æµ‹ï¼Œå¹¶åˆ©ç”¨äº¤äº’æ„ŸçŸ¥åŠ æƒ (Interaction-Aware Weighting, IAW) å¼ºåŒ–å£è¢‹è¿‘ç«¯æ®‹åŸºçš„æƒé‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å¯è‡ªç„¶æ‰©å±•è‡³åŒ…å« NSAAs çš„è‚½æŠ˜å ä»»åŠ¡ï¼Œå¡«è¡¥äº†ç°æœ‰å·¥å…·åœ¨è¿™ä¸€é¢†åŸŸçš„ç©ºç™½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒNS-Pep çš„åºåˆ—æ¢å¤ç‡å’Œç»“åˆäº²å’ŒåŠ›åˆ†åˆ«æå‡äº† 6.23% å’Œ 5.12%ï¼Œä¸”åœ¨è‚½æŠ˜å æˆåŠŸç‡ä¸Šæ¯” AlphaFold3 é«˜å‡º 17.76%ã€‚",
      "categories": [
        "q-bio.BM",
        "cs.AI"
      ],
      "primary_category": "q-bio.BM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03326v1",
      "published_date": "2025-10-01 16:04:06 UTC",
      "updated_date": "2025-10-01 16:04:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:57:57.980630+00:00"
    },
    {
      "arxiv_id": "2510.01052v1",
      "title": "Hybrid Dialogue State Tracking for Persian Chatbots: A Language Model-Based Approach",
      "title_zh": "é¢å‘æ³¢æ–¯è¯­èŠå¤©æœºå™¨äººçš„æ··åˆå¯¹è¯çŠ¶æ€è¿½è¸ªï¼šä¸€ç§åŸºäºè¯­è¨€æ¨¡å‹çš„æ–¹æ³•",
      "authors": [
        "Samin Mahdipour Aghabagher",
        "Saeedeh Momtazi"
      ],
      "abstract": "Dialogue State Tracking (DST) is an essential element of conversational AI with the objective of deeply understanding the conversation context and leading it toward answering user requests. Due to high demands for open-domain and multi-turn chatbots, the traditional rule-based DST is not efficient enough, since it cannot provide the required adaptability and coherence for human-like experiences in complex conversations. This study proposes a hybrid DST model that utilizes rule-based methods along with language models, including BERT for slot filling and intent detection, XGBoost for intent validation, GPT for DST, and online agents for real-time answer generation. This model is uniquely designed to be evaluated on a comprehensive Persian multi-turn dialogue dataset and demonstrated significantly improved accuracy and coherence over existing methods in Persian-based chatbots. The results demonstrate how effectively a hybrid approach may improve DST capabilities, paving the way for conversational AI systems that are more customized, adaptable, and human-like.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹æ³¢æ–¯è¯­èŠå¤©æœºå™¨äººåœ¨å¼€æ”¾åŸŸå’Œå¤šè½®å¯¹è¯ä¸­é¢ä¸´çš„é€‚åº”æ€§ä¸è¿è´¯æ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ··åˆå¯¹è¯çŠ¶æ€è·Ÿè¸ª (Dialogue State Tracking, DST) æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡ç»“åˆä¼ ç»Ÿçš„åŸºäºè§„åˆ™çš„æ–¹æ³•ä¸å¤šç§å…ˆè¿›çš„è¯­è¨€æ¨¡å‹ï¼Œå…‹æœäº†ä¼ ç»Ÿå•ä¸€æ–¹æ³•çš„å±€é™æ€§ã€‚å…·ä½“è€Œè¨€ï¼Œç³»ç»Ÿé›†æˆ BERT ç”¨äºæ§½ä½å¡«å…… (Slot Filling) å’Œæ„å›¾æ£€æµ‹ (Intent Detection)ï¼Œé‡‡ç”¨ XGBoost è¿›è¡Œæ„å›¾éªŒè¯ï¼Œå¹¶åˆ©ç”¨ GPT è¿›è¡Œæ ¸å¿ƒçš„å¯¹è¯çŠ¶æ€è·Ÿè¸ªã€‚æ­¤å¤–ï¼Œè¯¥æ¶æ„è¿˜å¼•å…¥äº†åœ¨çº¿æ™ºèƒ½ä½“ (Online Agents) ä»¥å®ç°å®æ—¶ç­”æ¡ˆç”Ÿæˆã€‚ç ”ç©¶äººå‘˜åœ¨ä¸€ä¸ªå…¨é¢çš„æ³¢æ–¯è¯­å¤šè½®å¯¹è¯æ•°æ®é›†ä¸Šå¯¹è¯¥æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶åœ¨å‡†ç¡®æ€§å’Œè¿è´¯æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„æ³¢æ–¯è¯­èŠå¤©æœºå™¨äººæ–¹æ³•ã€‚è¯¥æ··åˆæ–¹æ³•çš„æˆåŠŸå®æ–½è¯æ˜äº†ç»“åˆè§„åˆ™ä¸è¯­è¨€æ¨¡å‹åœ¨æå‡ DST èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¼€å‘æ›´å…·ä¸ªæ€§åŒ–ã€é€‚åº”æ€§å’Œæ‹ŸäººåŒ–çš„å¯¹è¯ AI ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "22 pages, 1 figure. Submitted to Natural Language Engineering",
      "pdf_url": "https://arxiv.org/pdf/2510.01052v1",
      "published_date": "2025-10-01 15:57:19 UTC",
      "updated_date": "2025-10-01 15:57:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:58:27.770922+00:00"
    },
    {
      "arxiv_id": "2510.01051v1",
      "title": "GEM: A Gym for Agentic LLMs",
      "title_zh": "GEMï¼šé¢å‘æ™ºèƒ½ä½“åŒ–å¤§è¯­è¨€æ¨¡å‹çš„ Gym è®­ç»ƒå¹³å°",
      "authors": [
        "Zichen Liu",
        "Anya Sims",
        "Keyu Duan",
        "Changyu Chen",
        "Simon Yu",
        "Xiangxin Zhou",
        "Haotian Xu",
        "Shaopan Xiong",
        "Bo Liu",
        "Chenmien Tan",
        "Chuen Yang Beh",
        "Weixun Wang",
        "Hao Zhu",
        "Weiyan Shi",
        "Diyi Yang",
        "Michael Shieh",
        "Yee Whye Teh",
        "Wee Sun Lee",
        "Min Lin"
      ],
      "abstract": "The training paradigm for large language models (LLMs) is moving from static datasets to experience-based learning, where agents acquire skills via interacting with complex environments. To facilitate this transition we introduce GEM (General Experience Maker), an open-source environment simulator designed for the age of LLMs. Analogous to OpenAI-Gym for traditional reinforcement learning (RL), GEM provides a standardized framework for the environment-agent interface, including asynchronous vectorized execution for high throughput, and flexible wrappers for easy extensibility. GEM also features a diverse suite of environments, robust integrated tools, and single-file example scripts demonstrating using GEM with five popular RL training frameworks. Along with this, we also provide a set of baselines across 24 environments using REINFORCE with Return Batch Normalization (ReBN), which -- unlike GRPO -- is compatible with the full RL setting of dense per-turn rewards and offers better credit assignment. We further conduct apple-to-apple benchmarking of PPO, GRPO and REINFORCE in both single- and multi-turn settings using GEM to shed light on the algorithmic designs. Lastly, GEM also functions as a convenient evaluation toolkit besides a training environment. We hope this framework can help accelerate future agentic LLM research.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† GEM (General Experience Maker)ï¼Œä¸€ä¸ªä¸“ä¸ºå¤§è¯­è¨€æ¨¡å‹ (LLMs) æ™ºèƒ½ä½“è®¾è®¡çš„å¼€æºç¯å¢ƒæ¨¡æ‹Ÿå™¨ï¼Œæ—¨åœ¨æ”¯æŒä»é™æ€æ•°æ®é›†è®­ç»ƒå‘åŸºäºç»éªŒçš„å­¦ä¹ è½¬å‹ã€‚ç±»æ¯”äºä¼ ç»Ÿå¼ºåŒ–å­¦ä¹  (RL) ä¸­çš„ OpenAI-Gymï¼ŒGEM æä¾›äº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„ç¯å¢ƒæ™ºèƒ½ä½“æ¥å£ï¼Œæ”¯æŒå¼‚æ­¥å‘é‡åŒ–æ‰§è¡Œ (asynchronous vectorized execution) ä»¥å®ç°é«˜ååé‡ã€‚è¯¥æ¡†æ¶å†…ç½®äº†å¤šæ ·åŒ–çš„ç¯å¢ƒå¥—ä»¶ã€å·¥å…·åŠç¤ºä¾‹è„šæœ¬ï¼Œå¹¶å¼•å…¥äº†åŸºäº REINFORCE ç»“åˆ Return Batch Normalization (ReBN) çš„ç®—æ³•åŸºçº¿ï¼Œåœ¨å¤„ç†å¯†é›†å¥–åŠ±å’Œä¿¡ç”¨åˆ†é… (credit assignment) æ–¹é¢è¡¨ç°ä¼˜äº GRPOã€‚é€šè¿‡å¯¹ PPOã€GRPO å’Œ REINFORCE åœ¨å•è½®åŠå¤šè½®è®¾ç½®ä¸‹çš„å¯¹æ¯”æµ‹è¯•ï¼ŒGEM è¿›ä¸€æ­¥æ­ç¤ºäº†ä¸åŒç®—æ³•çš„è®¾è®¡å·®å¼‚ã€‚ä½œä¸ºå…¼å…·è®­ç»ƒä¸è¯„ä¼°åŠŸèƒ½çš„å·¥å…·åŒ…ï¼ŒGEM ä¸ºåŠ é€Ÿæœªæ¥ agentic LLM çš„ç ”ç©¶æä¾›äº†é«˜æ•ˆçš„å®éªŒå¹³å°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01051v1",
      "published_date": "2025-10-01 15:55:57 UTC",
      "updated_date": "2025-10-01 15:55:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:58:38.690895+00:00"
    },
    {
      "arxiv_id": "2510.01048v1",
      "title": "Interpreting Language Models Through Concept Descriptions: A Survey",
      "title_zh": "åŸºäºæ¦‚å¿µæè¿°çš„è¯­è¨€æ¨¡å‹å¯è§£é‡Šæ€§ç ”ç©¶ç»¼è¿°",
      "authors": [
        "Nils Feldhus",
        "Laura Kopf"
      ],
      "abstract": "Understanding the decision-making processes of neural networks is a central goal of mechanistic interpretability. In the context of Large Language Models (LLMs), this involves uncovering the underlying mechanisms and identifying the roles of individual model components such as neurons and attention heads, as well as model abstractions such as the learned sparse features extracted by Sparse Autoencoders (SAEs). A rapidly growing line of work tackles this challenge by using powerful generator models to produce open-vocabulary, natural language concept descriptions for these components. In this paper, we provide the first survey of the emerging field of concept descriptions for model components and abstractions. We chart the key methods for generating these descriptions, the evolving landscape of automated and human metrics for evaluating them, and the datasets that underpin this research. Our synthesis reveals a growing demand for more rigorous, causal evaluation. By outlining the state of the art and identifying key challenges, this survey provides a roadmap for future research toward making models more transparent.",
      "tldr_zh": "è¯¥ç ”ç©¶æä¾›äº†é¦–ä»½å…³äºé€šè¿‡æ¦‚å¿µæè¿°ï¼ˆConcept Descriptionsï¼‰è§£é‡Šè¯­è¨€æ¨¡å‹è¿™ä¸€æ–°å…´é¢†åŸŸçš„ç»¼è¿°ï¼Œæ—¨åœ¨æ·±å…¥ç†è§£å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å†³ç­–è¿‡ç¨‹ã€‚æ–‡ç« ç³»ç»Ÿæ€§åœ°æ¢³ç†äº†åˆ©ç”¨ç”Ÿæˆæ¨¡å‹ä¸ºç¥ç»å…ƒã€æ³¨æ„åŠ›å¤´ä»¥åŠç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰æå–çš„ç‰¹å¾ç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°çš„å…³é”®æ–¹æ³•ã€‚ä½œè€…å…¨é¢å›é¡¾äº†è‡ªåŠ¨åŒ–ä¸äººå·¥è¯„ä¼°æŒ‡æ ‡çš„æ¼”å˜ï¼Œå¹¶æ±‡æ€»äº†æ”¯æ’‘è¯¥é¢†åŸŸç ”ç©¶çš„æ ¸å¿ƒæ•°æ®é›†ã€‚ç»¼è¿°æŒ‡å‡ºï¼Œå½“å‰ç ”ç©¶æ­£é¢ä¸´å¯¹æ›´ä¸¥è°¨çš„å› æœè¯„ä¼°ï¼ˆCausal Evaluationï¼‰æ—¥ç›Šå¢é•¿çš„éœ€æ±‚ã€‚é€šè¿‡æ€»ç»“æŠ€æœ¯ç°çŠ¶å¹¶è¯†åˆ«æ ¸å¿ƒæŒ‘æˆ˜ï¼Œæœ¬ç»¼è¿°ä¸ºæœªæ¥æå‡æ¨¡å‹é€æ˜åº¦çš„ç ”ç©¶æä¾›äº†æ˜ç¡®çš„è·¯çº¿å›¾ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at The Eight Workshop on Analyzing and Interpreting Neural Networks for NLP (BlackboxNLP), co-located with EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.01048v1",
      "published_date": "2025-10-01 15:51:44 UTC",
      "updated_date": "2025-10-01 15:51:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:58:34.377417+00:00"
    },
    {
      "arxiv_id": "2510.01047v2",
      "title": "In-Situ Tweedie Discrete Diffusion Models",
      "title_zh": "åŸä½ Tweedie ç¦»æ•£æ‰©æ•£æ¨¡å‹",
      "authors": [
        "Xiao Li",
        "Jiaqi Zhang",
        "Shuxiang Zhang",
        "Tianshui Chen",
        "Liang Lin",
        "Guangrun Wang"
      ],
      "abstract": "While diffusion models excel at generating continuous data such as images, adapting them to discrete tasks has relied on indirect approaches that either operate in continuous embedding spaces or use token masking mechanisms, both of which deviate from modeling the true discrete data distribution that can be theoretically guaranteed by Tweedie's formula. We propose in-situ Tweedie Discrete Diffusion (TDD), a framework that performs diffusion guaranteed by Tweedie's formula directly within the discrete one-hot space, hence \"in-situ.\" Unlike prior methods that diffuse continuous embeddings or mask tokens, TDD directly corrupts one-hot vectors with Gaussian noise and performs iterative denoising through a timestep-conditioned cross-entropy objective rather than mean-squared-error reconstruction. At each denoising step, the model predicts class probabilities, applies argmax to obtain discrete predictions, converts them to one-hot vectors, and feeds them into the next iteration with progressively reduced noise. This process naturally unifies discriminative classification and generative modeling under a single framework. Experiments demonstrate that TDD achieves strong performance on both image classification and text generation tasks, with extensive ablation studies confirming the effectiveness of each design component. Our work establishes a principled approach to discrete diffusion that preserves the core characteristics of diffusion models while operating natively in discrete space.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹(diffusion models)åœ¨å¤„ç†ç¦»æ•£ä»»åŠ¡æ—¶åç¦»Tweedie's formulaç†è®ºåˆ†å¸ƒçš„é—®é¢˜ï¼Œæå‡ºäº†In-situ Tweedie Discrete Diffusion (TDD)æ¡†æ¶ï¼Œå®ç°äº†åœ¨ç¦»æ•£one-hotç©ºé—´å†…çš„åŸç”Ÿæ‰©æ•£ã€‚ä¸åŒäºä»¥å¾€ä¾èµ–è¿ç»­åµŒå…¥(continuous embeddings)æˆ–æ©ç (token masking)çš„é—´æ¥æ–¹æ³•ï¼ŒTDDä½¿ç”¨é«˜æ–¯å™ªå£°(Gaussian noise)æŸåone-hotå‘é‡ï¼Œå¹¶é‡‡ç”¨æ—¶é—´æ­¥è°ƒèŠ‚çš„äº¤å‰ç†µç›®æ ‡(cross-entropy objective)è¿›è¡Œè¿­ä»£å»å™ªã€‚æ¨¡å‹åœ¨æ¯ä¸€æ­¥é€šè¿‡é¢„æµ‹ç±»åˆ«æ¦‚ç‡å¹¶å°†å…¶è½¬åŒ–ä¸ºone-hotå‘é‡ï¼Œä»è€Œå°†åˆ¤åˆ«åˆ†ç±»(discriminative classification)ä¸ç”Ÿæˆå»ºæ¨¡(generative modeling)ç»Ÿä¸€åœ¨å•ä¸€æ¡†æ¶ä¸‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTDDåœ¨å›¾åƒåˆ†ç±»å’Œæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­å‡å–å¾—äº†å¼ºåŠ²æ€§èƒ½ï¼Œå¹¿æ³›çš„æ¶ˆèå®éªŒè¿›ä¸€æ­¥éªŒè¯äº†å„è®¾è®¡ç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶ä¸ºç¦»æ•£æ‰©æ•£æä¾›äº†ä¸€ç§æ—¢ç¬¦åˆç†è®ºåŸåˆ™åˆèƒ½åœ¨åŸç”Ÿç¦»æ•£ç©ºé—´æ“ä½œçš„é«˜æ•ˆæ–¹æ¡ˆï¼Œå®Œæ•´ä¿ç•™äº†æ‰©æ•£æ¨¡å‹çš„æ ¸å¿ƒç‰¹æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01047v2",
      "published_date": "2025-10-01 15:51:10 UTC",
      "updated_date": "2025-11-24 14:42:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:58:22.975383+00:00"
    },
    {
      "arxiv_id": "2510.01038v1",
      "title": "Activation-Deactivation: A General Framework for Robust Post-hoc Explainable AI",
      "title_zh": "Activation-Deactivationï¼šä¸€ç§é²æ£’çš„äº‹åå¯è§£é‡Šäººå·¥æ™ºèƒ½é€šç”¨æ¡†æ¶",
      "authors": [
        "Akchunya Chanchal",
        "David A. Kelly",
        "Hana Chockler"
      ],
      "abstract": "Black-box explainability methods are popular tools for explaining the decisions of image classifiers. A major drawback of these tools is their reliance on mutants obtained by occluding parts of the input, leading to out-of-distribution images. This raises doubts about the quality of the explanations. Moreover, choosing an appropriate occlusion value often requires domain knowledge. In this paper we introduce a novel forward-pass paradigm Activation-Deactivation (AD), which removes the effects of occluded input features from the model's decision-making by switching off the parts of the model that correspond to the occlusions. We introduce ConvAD, a drop-in mechanism that can be easily added to any trained Convolutional Neural Network (CNN), and which implements the AD paradigm. This leads to more robust explanations without any additional training or fine-tuning. We prove that the ConvAD mechanism does not change the decision-making process of the network. We provide experimental evaluation across several datasets and model architectures. We compare the quality of AD-explanations with explanations achieved using a set of masking values, using the proxies of robustness, size, and confidence drop-off. We observe a consistent improvement in robustness of AD explanations (up to 62.5%) compared to explanations obtained with occlusions, demonstrating that ConvAD extracts more robust explanations without the need for domain knowledge.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Activation-Deactivation (AD)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é»‘ç›’å¯è§£é‡Šæ€§æ–¹æ³•(Black-box explainability methods)ä¸­å› é®æŒ¡è¾“å…¥äº§ç”Ÿåˆ†å¸ƒå¤–æ•°æ®(out-of-distribution)è€Œå¯¼è‡´è§£é‡Šè´¨é‡ä¸‹é™çš„é—®é¢˜ã€‚ä¸ä¼ ç»Ÿæ”¹å˜è¾“å…¥å›¾åƒçš„æ–¹æ³•ä¸åŒï¼ŒADèŒƒå¼é€šè¿‡åœ¨å‰å‘ä¼ æ’­ä¸­å…³é—­æ¨¡å‹å†…éƒ¨ä¸é®æŒ¡åŒºåŸŸå¯¹åº”çš„éƒ¨åˆ†ï¼Œä»è€Œæ¶ˆé™¤ç‰¹å®šç‰¹å¾å¯¹å†³ç­–çš„å½±å“ã€‚ç ”ç©¶è€…ä¸ºæ­¤å¼€å‘äº†ConvADæ’ä»¶æœºåˆ¶ï¼Œè¯¥æœºåˆ¶æ— éœ€é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒ(fine-tuning)å³å¯ç›´æ¥åº”ç”¨äºå·ç§¯ç¥ç»ç½‘ç»œ(CNN)ï¼Œä¸”åœ¨æ•°å­¦ä¸Šè¯æ˜äº†å…¶ä¸ä¼šæ”¹å˜åŸç½‘ç»œçš„å†³ç­–è¿‡ç¨‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç›¸è¾ƒäºä¾èµ–é®æŒ¡å€¼çš„ä¼ ç»Ÿæ–¹æ³•ï¼ŒADæ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†å’Œæ¶æ„ä¸Šçš„è§£é‡Šé²æ£’æ€§(Robustness)æœ€é«˜æå‡äº†62.5%ã€‚è¯¥ç ”ç©¶ä¸ä»…æœ‰æ•ˆæå–äº†æ›´ç¨³å¥çš„è§£é‡Šï¼Œè¿˜æ¶ˆé™¤äº†é€‰æ‹©é®æŒ¡å‚æ•°æ—¶å¯¹é¢†åŸŸçŸ¥è¯†(domain knowledge)çš„ä¾èµ–ï¼Œä¸ºäº‹åè§£é‡ŠAI(Post-hoc Explainable AI)æä¾›äº†é€šç”¨çš„é²æ£’æ¡†æ¶ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint: Under Review",
      "pdf_url": "https://arxiv.org/pdf/2510.01038v1",
      "published_date": "2025-10-01 15:42:58 UTC",
      "updated_date": "2025-10-01 15:42:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:58:37.573244+00:00"
    },
    {
      "arxiv_id": "2510.01037v1",
      "title": "CurES: From Gradient Analysis to Efficient Curriculum Learning for Reasoning LLMs",
      "title_zh": "CurESï¼šä»æ¢¯åº¦åˆ†æåˆ°é¢å‘æ¨ç†å¤§è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆè¯¾ç¨‹å­¦ä¹ ",
      "authors": [
        "Yongcheng Zeng",
        "Zexu Sun",
        "Bokai Ji",
        "Erxue Min",
        "Hengyi Cai",
        "Shuaiqiang Wang",
        "Dawei Yin",
        "Haifeng Zhang",
        "Xu Chen",
        "Jun Wang"
      ],
      "abstract": "Curriculum learning plays a crucial role in enhancing the training efficiency of large language models (LLMs) on reasoning tasks. However, existing methods often fail to adequately account for variations in prompt difficulty or rely on simplistic filtering mechanisms to select prompt datasets within a narrow criterion range, resulting in significant computational waste. In this work, we approach the problem from the perspective of reinforcement learning gradient optimization, offering a systematic and theoretical investigation into how to improve the training efficiency of LLMs. We identify two key factors influencing training efficiency: the selection of training prompts and the allocation of rollout quantities across different prompts. Our theoretical analysis reveals that the sampling distribution of prompts dictates the convergence rate of gradient descent, while the allocation of the rollout quantity influences the consistency and stability of overall gradient updates. Based on these insights, we propose CurES, an efficient training method that accelerates convergence and employs Bayesian posterior estimation to minimize computational overhead. Experiments demonstrate that our CurES outperforms Group Relative Policy Optimization (GRPO) by \\textbf{+3.30} points and \\textbf{+4.82} points with 1.5B and 7B models, respectively. Additionally, CurES exhibits faster convergence compared to baselines, including GRPO.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CurESï¼Œä¸€ç§ä»æ¢¯åº¦åˆ†æè§’åº¦å‡ºå‘çš„é«˜æ•ˆCurriculum Learningæ–¹æ³•ï¼Œæ—¨åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„è®­ç»ƒæ•ˆç‡ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å¿½è§†Promptéš¾åº¦æˆ–è¿‡æ»¤æœºåˆ¶è¿‡äºç®€å•å¯¼è‡´çš„è®¡ç®—èµ„æºæµªè´¹é—®é¢˜ï¼Œä½œè€…é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¢¯åº¦ä¼˜åŒ–è¿›è¡Œäº†ç³»ç»Ÿæ€§çš„ç†è®ºç ”ç©¶ã€‚ç ”ç©¶è¯†åˆ«å‡ºå½±å“è®­ç»ƒæ•ˆç‡çš„ä¸¤ä¸ªå…³é”®å› ç´ ï¼Œå³è®­ç»ƒPromptçš„é€‰æ‹©å’Œä¸åŒPromptä¹‹é—´çš„Rolloutæ•°é‡åˆ†é…ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒPromptçš„é‡‡æ ·åˆ†å¸ƒå†³å®šäº†æ¢¯åº¦ä¸‹é™çš„æ”¶æ•›é€Ÿåº¦ï¼Œè€ŒRolloutæ•°é‡çš„åˆ†é…åˆ™å½±å“æ¢¯åº¦æ›´æ–°çš„ä¸€è‡´æ€§ä¸ç¨³å®šæ€§ã€‚åŸºäºè¿™äº›å‘ç°ï¼ŒCurESé€šè¿‡åŠ é€Ÿæ”¶æ•›å¹¶é‡‡ç”¨Bayesian posterior estimationæ¥æœ€å°åŒ–è®¡ç®—å¼€é”€ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCurESåœ¨1.5Bå’Œ7Bæ¨¡å‹ä¸Šåˆ†åˆ«æ¯”GRPOæé«˜äº†3.30å’Œ4.82ä¸ªç™¾åˆ†ç‚¹ï¼Œä¸”æ”¶æ•›é€Ÿåº¦æ˜¾è‘—ä¼˜äºåŒ…æ‹¬GRPOåœ¨å†…çš„åŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "25 pages, 10 Figures",
      "pdf_url": "https://arxiv.org/pdf/2510.01037v1",
      "published_date": "2025-10-01 15:41:27 UTC",
      "updated_date": "2025-10-01 15:41:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:58:35.468638+00:00"
    },
    {
      "arxiv_id": "2510.01030v1",
      "title": "Uncovering the Computational Ingredients of Human-Like Representations in LLMs",
      "title_zh": "æ­ç¤ºå¤§è¯­è¨€æ¨¡å‹ä¸­ç±»äººè¡¨å¾çš„è®¡ç®—è¦ç´ ",
      "authors": [
        "Zach Studdiford",
        "Timothy T. Rogers",
        "Kushin Mukherjee",
        "Siddharth Suresh"
      ],
      "abstract": "The ability to translate diverse patterns of inputs into structured patterns of behavior has been thought to rest on both humans' and machines' ability to learn robust representations of relevant concepts. The rapid advancement of transformer-based large language models (LLMs) has led to a diversity of computational ingredients -- architectures, fine tuning methods, and training datasets among others -- but it remains unclear which of these ingredients are most crucial for building models that develop human-like representations. Further, most current LLM benchmarks are not suited to measuring representational alignment between humans and models, making benchmark scores unreliable for assessing if current LLMs are making progress towards becoming useful cognitive models. We address these limitations by first evaluating a set of over 70 models that widely vary in their computational ingredients on a triplet similarity task, a method well established in the cognitive sciences for measuring human conceptual representations, using concepts from the THINGS database. Comparing human and model representations, we find that models that undergo instruction-finetuning and which have larger dimensionality of attention heads are among the most human aligned, while multimodal pretraining and parameter size have limited bearing on alignment. Correlations between alignment scores and scores on existing benchmarks reveal that while some benchmarks (e.g., MMLU) are better suited than others (e.g., MUSR) for capturing representational alignment, no existing benchmark is capable of fully accounting for the variance of alignment scores, demonstrating their insufficiency in capturing human-AI alignment. Taken together, our findings help highlight the computational ingredients most essential for advancing LLMs towards models of human conceptual representation and address a key benchmarking gap in LLM evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†è¶…è¿‡70ä¸ªåœ¨æ¶æ„ã€å¾®è°ƒæ–¹æ³•å’Œè®­ç»ƒæ•°æ®é›†ä¸Šå…·æœ‰å·®å¼‚çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)ï¼Œæ—¨åœ¨æ­ç¤ºå“ªäº›è®¡ç®—è¦ç´ (computational ingredients)å¯¹æ„å»ºå…·æœ‰ç±»äººè¡¨å¾(human-like representations)çš„æ¨¡å‹æœ€ä¸ºå…³é”®ã€‚ç ”ç©¶é‡‡ç”¨äº†è®¤çŸ¥ç§‘å­¦ä¸­ç”¨äºè¡¡é‡äººç±»æ¦‚å¿µè¡¨å¾çš„ä¸‰ä¸ªå¯¹è±¡ç›¸ä¼¼æ€§ä»»åŠ¡(triplet similarity task)ï¼Œå¹¶ç»“åˆTHINGSæ•°æ®åº“çš„æ¦‚å¿µæ¥è¯„ä¼°æ¨¡å‹ä¸äººç±»ä¹‹é—´çš„è¡¨å¾å¯¹é½(representational alignment)ã€‚å®éªŒå‘ç°ï¼Œç»è¿‡æŒ‡ä»¤å¾®è°ƒ(instruction-finetuning)ä¸”æ‹¥æœ‰è¾ƒå¤§æ³¨æ„åŠ›å¤´ç»´åº¦(attention head dimensionality)çš„æ¨¡å‹ä¸äººç±»è¡¨å¾æœ€ä¸ºå¯¹é½ï¼Œè€Œå¤šæ¨¡æ€é¢„è®­ç»ƒ(multimodal pretraining)å’Œå‚æ•°è§„æ¨¡(parameter size)å¯¹å¯¹é½åº¦çš„å½±å“æœ‰é™ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯æ˜ç°æœ‰åŸºå‡†æµ‹è¯•(å¦‚MMLUå’ŒMUSR)å‡æ— æ³•å®Œå…¨åæ˜ è¡¨å¾å¯¹é½åº¦çš„å·®å¼‚ï¼Œæš´éœ²äº†å½“å‰è¯„ä¼°ä½“ç³»åœ¨æ•æ‰äººæœºå¯¹é½æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥é¡¹å·¥ä½œä¸ä»…ç¡®å®šäº†æ¨è¿›LLMså‘äººç±»æ¦‚å¿µè¡¨å¾æ¨¡å‹æ¼”è¿›çš„æ ¸å¿ƒè¦ç´ ï¼Œè¿˜å¡«è¡¥äº†æ¨¡å‹è¯„ä¼°é¢†åŸŸä¸­è¡¨å¾å¯¹é½çš„å…³é”®ç©ºç™½ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.01030v1",
      "published_date": "2025-10-01 15:37:19 UTC",
      "updated_date": "2025-10-01 15:37:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:58:39.870082+00:00"
    },
    {
      "arxiv_id": "2510.01025v1",
      "title": "Shape Happens: Automatic Feature Manifold Discovery in LLMs via Supervised Multi-Dimensional Scaling",
      "title_zh": "Shape Happensï¼šåŸºäºç›‘ç£å¤šç»´å°ºåº¦åˆ†æçš„ LLM è‡ªåŠ¨ç‰¹å¾æµå½¢å‘ç°",
      "authors": [
        "Federico Tiblias",
        "Irina Bigoulaeva",
        "Jingcheng Niu",
        "Simone Balloccu",
        "Iryna Gurevych"
      ],
      "abstract": "The linear representation hypothesis states that language models (LMs) encode concepts as directions in their latent space, forming organized, multidimensional manifolds. Prior efforts focus on discovering specific geometries for specific features, and thus lack generalization. We introduce Supervised Multi-Dimensional Scaling (SMDS), a model-agnostic method to automatically discover feature manifolds. We apply SMDS to temporal reasoning as a case study, finding that different features form various geometric structures such as circles, lines, and clusters. SMDS reveals many insights on these structures: they consistently reflect the properties of the concepts they represent; are stable across model families and sizes; actively support reasoning in models; and dynamically reshape in response to context changes. Together, our findings shed light on the functional role of feature manifolds, supporting a model of entity-based reasoning in which LMs encode and transform structured representations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è¯­è¨€æ¨¡å‹(LMs)å°†æ¦‚å¿µç¼–ç ä¸ºå¤šç»´æµå½¢(manifolds)çš„çº¿æ€§è¡¨ç¤ºå‡è®¾ï¼Œå¹¶é’ˆå¯¹ç°æœ‰æ–¹æ³•ç¼ºä¹æ³›åŒ–æ€§çš„é—®é¢˜ï¼Œæå‡ºäº†Supervised Multi-Dimensional Scaling (SMDS) è¿™ä¸€æ¨¡å‹æ— å…³çš„è‡ªåŠ¨ç‰¹å¾æµå½¢å‘ç°æ–¹æ³•ã€‚ç ”ç©¶è€…ä»¥æ—¶é—´æ¨ç†(temporal reasoning)ä¸ºæ¡ˆä¾‹ï¼Œé€šè¿‡SMDSå‘ç°ä¸åŒç‰¹å¾åœ¨æ½œç©ºé—´ä¸­å½¢æˆäº†åœ†ã€ç›´çº¿å’Œç°‡ç­‰å¤šç§å‡ ä½•ç»“æ„ã€‚å®éªŒè¯æ˜ï¼Œè¿™äº›å‡ ä½•ç»“æ„èƒ½å¤Ÿä¸€è‡´åœ°åæ˜ å…¶æ‰€ä»£è¡¨æ¦‚å¿µçš„å±æ€§ï¼Œå¹¶åœ¨ä¸åŒæ¨¡å‹ç³»åˆ—å’Œå‚æ•°è§„æ¨¡ä¸­ä¿æŒç¨³å®šã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°ç‰¹å¾æµå½¢èƒ½ä¸»åŠ¨æ”¯æŒæ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ï¼Œå¹¶éšä¸Šä¸‹æ–‡å˜åŒ–è¿›è¡ŒåŠ¨æ€é‡å¡‘ã€‚è¯¥å·¥ä½œæ­ç¤ºäº†ç‰¹å¾æµå½¢çš„èŒèƒ½è§’è‰²ï¼Œæ”¯æŒäº†åŸºäºå®ä½“æ¨ç†(entity-based reasoning)çš„æ¨¡å‹è§‚ç‚¹ï¼Œå³è¯­è¨€æ¨¡å‹é€šè¿‡ç¼–ç å’Œè½¬æ¢ç»“æ„åŒ–è¡¨ç¤ºæ¥å®Œæˆå¤æ‚ä»»åŠ¡ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01025v1",
      "published_date": "2025-10-01 15:30:47 UTC",
      "updated_date": "2025-10-01 15:30:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:58:46.311984+00:00"
    },
    {
      "arxiv_id": "2510.01020v1",
      "title": "The Good, the Bad, and the Sampled: a No-Regret Approach to Safe Online Classification",
      "title_zh": "å¥½ã€åä¸é‡‡æ ·ï¼šå®‰å…¨åœ¨çº¿åˆ†ç±»çš„æ— æ‚”æ–¹æ³•",
      "authors": [
        "Tavor Z. Baharav",
        "Spyros Dragazis",
        "Aldo Pacchiano"
      ],
      "abstract": "We study the problem of sequentially testing individuals for a binary disease outcome whose true risk is governed by an unknown logistic model. At each round, a patient arrives with feature vector $x_t$, and the decision maker may either pay to administer a (noiseless) diagnostic test--revealing the true label--or skip testing and predict the patient's disease status based on their feature vector and prior history. Our goal is to minimize the total number of costly tests required while guaranteeing that the fraction of misclassifications does not exceed a prespecified error tolerance $Î±$, with probability at least $1-Î´$. To address this, we develop a novel algorithm that interleaves label-collection and distribution estimation to estimate both $Î¸^{*}$ and the context distribution $P$, and computes a conservative, data-driven threshold $Ï„_t$ on the logistic score $|x_t^\\topÎ¸|$ to decide when testing is necessary. We prove that, with probability at least $1-Î´$, our procedure does not exceed the target misclassification rate, and requires only $O(\\sqrt{T})$ excess tests compared to the oracle baseline that knows both $Î¸^{*}$ and the patient feature distribution $P$. This establishes the first no-regret guarantees for error-constrained logistic testing, with direct applications to cost-sensitive medical screening. Simulations corroborate our theoretical guarantees, showing that in practice our procedure efficiently estimates $Î¸^{*}$ while retaining safety guarantees, and does not require too many excess tests.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æœªçŸ¥ Logistic Model æ§åˆ¶ä¸‹çš„äºŒå…ƒç–¾ç—…ç»“æœé¡ºåºæµ‹è¯•é—®é¢˜ï¼Œæ—¨åœ¨ä¿è¯è¯¯åˆ†ç±»ç‡ä¸è¶…è¿‡é¢„è®¾å®¹å¿åº¦ $\\alpha$ çš„å‰æä¸‹ï¼Œå°½é‡å‡å°‘é«˜æˆæœ¬è¯Šæ–­æµ‹è¯•çš„æ¬¡æ•°ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åˆ›æ–°çš„ç®—æ³•ï¼Œé€šè¿‡äº¤æ›¿è¿›è¡Œæ ‡ç­¾æ”¶é›†å’Œåˆ†å¸ƒä¼°è®¡ï¼Œå®ç°å¯¹å‚æ•° $\\theta^*$ å’Œä¸Šä¸‹æ–‡åˆ†å¸ƒ $P$ çš„åŒæ­¥å­¦ä¹ ã€‚è¯¥ç®—æ³•åˆ©ç”¨é€»è¾‘å¾—åˆ† $|x_t^\\top \\theta|$ è®¡ç®—ä¿å®ˆçš„æ•°æ®é©±åŠ¨é˜ˆå€¼ $\\tau_t$ï¼Œä»¥åŠ¨æ€ç¡®å®šä½•æ—¶å¿…é¡»è¿›è¡Œæµ‹è¯•ã€‚ç†è®ºåˆ†æè¯æ˜ï¼Œè¯¥ç¨‹åºåœ¨æ»¡è¶³é¢„è®¾å®‰å…¨æ€§è¦æ±‚çš„åŒæ—¶ï¼Œå…¶è¶…é¢æµ‹è¯•é‡ç›¸å¯¹äº Oracle åŸºå‡†è¾¾åˆ°äº† $O(\\sqrt{T})$ çš„æ”¶æ•›é€Ÿåº¦ã€‚è¿™ä¸ºå—è¯¯å·®çº¦æŸçš„ Logistic Testing æä¾›äº†é¦–ä¸ª No-Regret ç†è®ºä¿è¯ï¼Œåœ¨æˆæœ¬æ•æ„Ÿçš„åŒ»ç–—ç­›æŸ¥é¢†åŸŸå…·æœ‰æ˜¾è‘—çš„åº”ç”¨æ½œåŠ›ã€‚ä»¿çœŸå®éªŒè¿›ä¸€æ­¥è¯å®äº†è¯¥æ–¹æ³•åœ¨å®é™…æ“ä½œä¸­èƒ½å¤Ÿé«˜æ•ˆä¼°è®¡å‚æ•°ï¼Œå¹¶åœ¨ä¿æŒå®‰å…¨æ€§çš„å‰æä¸‹æœ‰æ•ˆæ§åˆ¶æµ‹è¯•æˆæœ¬ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.ST",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "43 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.01020v1",
      "published_date": "2025-10-01 15:28:00 UTC",
      "updated_date": "2025-10-01 15:28:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:58:49.400092+00:00"
    },
    {
      "arxiv_id": "2510.01006v1",
      "title": "Integrating AI and Ensemble Forecasting: Explainable Materials Planning with Scorecards and Trend Insights for a Large-Scale Manufacturer",
      "title_zh": "èåˆäººå·¥æ™ºèƒ½ä¸é›†æˆé¢„æµ‹ï¼šé¢å‘å¤§å‹åˆ¶é€ å•†çš„åŸºäºè®¡åˆ†å¡ä¸è¶‹åŠ¿æ´å¯Ÿçš„å¯è§£é‡Šç‰©æ–™è§„åˆ’",
      "authors": [
        "Saravanan Venkatachalam"
      ],
      "abstract": "This paper presents a practical architecture for after-sales demand forecasting and monitoring that unifies a revenue- and cluster-aware ensemble of statistical, machine-learning, and deep-learning models with a role-driven analytics layer for scorecards and trend diagnostics. The framework ingests exogenous signals (installed base, pricing, macro indicators, life cycle, seasonality) and treats COVID-19 as a distinct regime, producing country-part forecasts with calibrated intervals. A Pareto-aware segmentation forecasts high-revenue items individually and pools the long tail via clusters, while horizon-aware ensembling aligns weights with business-relevant losses (e.g., WMAPE). Beyond forecasts, a performance scorecard delivers decision-focused insights: accuracy within tolerance thresholds by revenue share and count, bias decomposition (over- vs under-forecast), geographic and product-family hotspots, and ranked root causes tied to high-impact part-country pairs. A trend module tracks trajectories of MAPE/WMAPE and bias across recent months, flags entities that are improving or deteriorating, detects change points aligned with known regimes, and attributes movements to lifecycle and seasonal factors. LLMs are embedded in the analytics layer to generate role-aware narratives and enforce reporting contracts. They standardize business definitions, automate quality checks and reconciliations, and translate quantitative results into concise, explainable summaries for planners and executives. The system exposes a reproducible workflow -- request specification, model execution, database-backed artifacts, and AI-generated narratives -- so planners can move from \"How accurate are we now?\" to \"Where is accuracy heading and which levers should we pull?\", closing the loop between forecasting, monitoring, and inventory decisions across more than 90 countries and about 6,000 parts.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹åˆ¶é€ ä¼ä¸šçš„å”®åéœ€æ±‚é¢„æµ‹ä¸ç›‘æ§ï¼Œæå‡ºäº†ä¸€ç§é›†æˆäººå·¥æ™ºèƒ½ä¸é›†æˆé¢„æµ‹ï¼ˆEnsemble Forecastingï¼‰çš„å®ç”¨æ¶æ„ã€‚è¯¥æ¡†æ¶ç»Ÿä¸€äº†ç»Ÿè®¡ã€æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹çš„é›†æˆæ–¹æ³•ï¼Œå¹¶å¼•å…¥è£…æœºé‡ï¼ˆinstalled baseï¼‰ã€å®šä»·å’Œå®è§‚æŒ‡æ ‡ç­‰å¤–éƒ¨ä¿¡å·è¿›è¡ŒååŒå»ºæ¨¡ã€‚ç ”ç©¶é‡‡ç”¨Paretoæ„ŸçŸ¥çš„åˆ†å‰²ç­–ç•¥ï¼Œå¯¹é«˜æ”¶å…¥é¡¹è¿›è¡Œä¸ªä½“é¢„æµ‹å¹¶å¯¹é•¿å°¾éƒ¨åˆ†è¿›è¡Œèšç±»å¤„ç†ï¼ŒåŒæ—¶ç»“åˆæ°´å¹³æ„ŸçŸ¥çš„é›†æˆæŠ€æœ¯ä¼˜åŒ–WMAPEç­‰ä¸šåŠ¡æŸå¤±æŒ‡æ ‡ã€‚ç³»ç»Ÿé€šè¿‡ç»©æ•ˆè®¡åˆ†å¡å’Œè¶‹åŠ¿æ¨¡å—æä¾›æ·±åº¦è§è§£ï¼Œèƒ½å¤Ÿè‡ªåŠ¨è¯†åˆ«åœ°ç†å’Œäº§å“ç³»åˆ—çš„å¼‚å¸¸çƒ­ç‚¹å¹¶åˆ†æåç½®æ ¹å› ã€‚æ­¤å¤–ï¼Œè¯¥æ¶æ„åµŒå…¥äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”¨äºç”Ÿæˆè§’è‰²æ„ŸçŸ¥çš„å™è¿°æ€§æŠ¥å‘Šï¼Œå°†é‡åŒ–ç»“æœè½¬åŒ–ä¸ºç®€æ´ã€å¯è§£é‡Šçš„å†³ç­–å»ºè®®ã€‚è¯¥ç³»ç»Ÿåœ¨90å¤šä¸ªå›½å®¶å’Œçº¦6000ä¸ªé›¶ä»¶çš„è§„æ¨¡ä¸‹å®ç°äº†ä»é¢„æµ‹ç›‘æ§åˆ°åº“å­˜å†³ç­–çš„é—­ç¯ç®¡ç†ï¼Œæ˜¾è‘—å¢å¼ºäº†ä¼ä¸šç‰©æ–™è§„åˆ’çš„é€æ˜åº¦ä¸æ‰§è¡Œæ•ˆç‡ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01006v1",
      "published_date": "2025-10-01 15:14:10 UTC",
      "updated_date": "2025-10-01 15:14:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:58:54.992672+00:00"
    },
    {
      "arxiv_id": "2510.01004v1",
      "title": "TextCAM: Explaining Class Activation Map with Text",
      "title_zh": "TextCAMï¼šåˆ©ç”¨æ–‡æœ¬è§£é‡Šç±»æ¿€æ´»æ˜ å°„",
      "authors": [
        "Qiming Zhao",
        "Xingjian Li",
        "Xiaoyu Cao",
        "Xiaolong Wu",
        "Min Xu"
      ],
      "abstract": "Deep neural networks (DNNs) have achieved remarkable success across domains but remain difficult to interpret, limiting their trustworthiness in high-stakes applications. This paper focuses on deep vision models, for which a dominant line of explainability methods are Class Activation Mapping (CAM) and its variants working by highlighting spatial regions that drive predictions. We figure out that CAM provides little semantic insight into what attributes underlie these activations. To address this limitation, we propose TextCAM, a novel explanation framework that enriches CAM with natural languages. TextCAM combines the precise spatial localization of CAM with the semantic alignment of vision-language models (VLMs). Specifically, we derive channel-level semantic representations using CLIP embeddings and linear discriminant analysis, and aggregate them with CAM weights to produce textual descriptions of salient visual evidence. This yields explanations that jointly specify where the model attends and what visual attributes likely support its decision. We further extend TextCAM to generate feature channels into semantically coherent groups, enabling more fine-grained visual-textual explanations. Experiments on ImageNet, CLEVR, and CUB demonstrate that TextCAM produces faithful and interpretable rationales that improve human understanding, detect spurious correlations, and preserve model fidelity.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TextCAMï¼Œä¸€ä¸ªæ—¨åœ¨å¢å¼º Class Activation Map (CAM) è¯­ä¹‰è§£é‡Šæ€§çš„æ–°å‹æ¡†æ¶ã€‚é’ˆå¯¹ä¼ ç»Ÿ CAM ä»…èƒ½æä¾›ç©ºé—´å®šä½è€Œç¼ºä¹è¯­ä¹‰æ´å¯Ÿçš„é—®é¢˜ï¼ŒTextCAM é€šè¿‡ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) å°†è‡ªç„¶è¯­è¨€å¼•å…¥è§£é‡Šè¿‡ç¨‹ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ CLIP åµŒå…¥å’Œçº¿æ€§åˆ¤åˆ«åˆ†æ (Linear Discriminant Analysis) å¯¼å‡ºé€šé“çº§çš„è¯­ä¹‰è¡¨ç¤ºï¼Œå¹¶å°†å…¶ä¸ CAM æƒé‡èšåˆä»¥ç”Ÿæˆæ˜¾è‘—è§†è§‰è¯æ®çš„æ–‡å­—æè¿°ã€‚TextCAM èƒ½å¤ŸåŒæ—¶æŒ‡æ˜æ¨¡å‹å…³æ³¨çš„åŒºåŸŸä»¥åŠæ”¯æŒå†³ç­–çš„å…·ä½“è§†è§‰å±æ€§ï¼Œå¹¶æ”¯æŒå°†ç‰¹å¾é€šé“åˆ’åˆ†ä¸ºè¯­ä¹‰ä¸€è‡´çš„ç»„ä»¥å®ç°æ›´ç²¾ç»†çš„è§†è§‰æ–‡æœ¬è§£é‡Šã€‚åœ¨ ImageNetã€CLEVR å’Œ CUB æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒTextCAM èƒ½å¤Ÿç”Ÿæˆå¿ å®ä¸”å¯è§£é‡Šçš„ä¾æ®ï¼Œä¸ä»…æå‡äº†äººç±»å¯¹æ¨¡å‹çš„ç†è§£ï¼Œè¿˜èƒ½æœ‰æ•ˆæ£€æµ‹ä¼ªç›¸å…³å¹¶ä¿æŒæ¨¡å‹ä¿çœŸåº¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01004v1",
      "published_date": "2025-10-01 15:11:14 UTC",
      "updated_date": "2025-10-01 15:11:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:59:11.077871+00:00"
    },
    {
      "arxiv_id": "2510.00976v1",
      "title": "Adaptive Federated Few-Shot Rare-Disease Diagnosis with Energy-Aware Secure Aggregation",
      "title_zh": "åŸºäºèƒ½é‡æ„ŸçŸ¥å®‰å…¨èšåˆçš„è‡ªé€‚åº”è”é‚¦å°æ ·æœ¬ç½•è§ç—…è¯Šæ–­",
      "authors": [
        "Aueaphum Aueawatthanaphisut"
      ],
      "abstract": "Rare-disease diagnosis remains one of the most pressing challenges in digital health, hindered by extreme data scarcity, privacy concerns, and the limited resources of edge devices. This paper proposes the Adaptive Federated Few-Shot Rare-Disease Diagnosis (AFFR) framework, which integrates three pillars: (i) few-shot federated optimization with meta-learning to generalize from limited patient samples, (ii) energy-aware client scheduling to mitigate device dropouts and ensure balanced participation, and (iii) secure aggregation with calibrated differential privacy to safeguard sensitive model updates. Unlike prior work that addresses these aspects in isolation, AFFR unifies them into a modular pipeline deployable on real-world clinical networks. Experimental evaluation on simulated rare-disease detection datasets demonstrates up to 10% improvement in accuracy compared with baseline FL, while reducing client dropouts by over 50% without degrading convergence. Furthermore, privacy-utility trade-offs remain within clinically acceptable bounds. These findings highlight AFFR as a practical pathway for equitable and trustworthy federated diagnosis of rare conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AFFRæ¡†æ¶ï¼Œæ—¨åœ¨åº”å¯¹ç½•è§ç—…è¯Šæ–­ä¸­ç”±äºæ•°æ®æåº¦ç¨€ç¼ºã€éšç§é¡¾è™‘åŠè¾¹ç¼˜è®¾å¤‡èµ„æºå—é™å¸¦æ¥çš„ä¸¥å³»æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶æ•´åˆäº†ä¸‰å¤§æ ¸å¿ƒæŠ€æœ¯ï¼šåˆ©ç”¨Meta-learningå®ç°çš„Few-Shotè”é‚¦ä¼˜åŒ–ä»¥ä»æœ‰é™æ ·æœ¬ä¸­æå–æ³›åŒ–ç‰¹å¾ï¼Œé‡‡ç”¨èƒ½é‡æ„ŸçŸ¥ï¼ˆEnergy-Awareï¼‰çš„å®¢æˆ·ç«¯è°ƒåº¦æœºåˆ¶ä»¥é™ä½è®¾å¤‡æ‰çº¿ç‡å¹¶ä¿éšœå‚ä¸å‡è¡¡ï¼Œä»¥åŠé€šè¿‡æ ¡å‡†åçš„å·®åˆ†éšç§ï¼ˆDifferential Privacyï¼‰è¿›è¡Œå®‰å…¨èšåˆæ¥ä¿æŠ¤æ•æ„Ÿæ•°æ®ã€‚AFFRå°†è¿™äº›æ¨¡å—ç»Ÿä¸€ä¸ºå¯ç›´æ¥éƒ¨ç½²äºçœŸå®ä¸´åºŠç½‘ç»œçš„æµæ°´çº¿ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶åœ¨æ•´åˆæ€§ä¸Šçš„ç©ºç™½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æ¨¡æ‹Ÿç½•è§ç—…æ£€æµ‹ä»»åŠ¡ä¸­ï¼ŒAFFRç›¸æ¯”åŸºçº¿è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰æ–¹æ³•å‡†ç¡®ç‡æå‡äº†10%ï¼Œä¸”åœ¨ä¸å½±å“æ”¶æ•›é€Ÿåº¦çš„æƒ…å†µä¸‹å°†å®¢æˆ·ç«¯æ‰çº¿ç‡é™ä½äº†50%ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆåœ¨éšç§ä¿æŠ¤ä¸æ¨¡å‹æ•ˆç”¨ä¹‹é—´å–å¾—äº†ä¸´åºŠå¯æ¥å—çš„å¹³è¡¡ã€‚ç»¼ä¸Šæ‰€è¿°ï¼ŒAFFRä¸ºæ„å»ºå…¬å¹³ä¸”å¯ä¿¡çš„ç½•è§ç—…è¾…åŠ©è¯Šæ–­ç³»ç»Ÿæä¾›äº†ä¸€æ¡åˆ‡å®å¯è¡Œçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.DC",
        "cs.LG",
        "q-bio.QM"
      ],
      "primary_category": "cs.AI",
      "comment": "6 pages, 6 figures, 12 equations, 1 algorithm",
      "pdf_url": "https://arxiv.org/pdf/2510.00976v1",
      "published_date": "2025-10-01 14:52:07 UTC",
      "updated_date": "2025-10-01 14:52:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:59:10.479151+00:00"
    },
    {
      "arxiv_id": "2510.00967v1",
      "title": "QUASAR: Quantum Assembly Code Generation Using Tool-Augmented LLMs via Agentic RL",
      "title_zh": "QUASARï¼šåŸºäºæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸å·¥å…·å¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹çš„é‡å­æ±‡ç¼–ä»£ç ç”Ÿæˆ",
      "authors": [
        "Cong Yu",
        "Valter Uotila",
        "Shilong Deng",
        "Qingyuan Wu",
        "Tuo Shi",
        "Songlin Jiang",
        "Lei You",
        "Bo Zhao"
      ],
      "abstract": "Designing and optimizing task-specific quantum circuits are crucial to leverage the advantage of quantum computing. Recent large language model (LLM)-based quantum circuit generation has emerged as a promising automatic solution. However, the fundamental challenges remain unaddressed: (i) parameterized quantum gates require precise numerical values for optimal performance, which also depend on multiple aspects, including the number of quantum gates, their parameters, and the layout/depth of the circuits. (ii) LLMs often generate low-quality or incorrect quantum circuits due to the lack of quantum domain-specific knowledge. We propose QUASAR, an agentic reinforcement learning (RL) framework for quantum circuits generation and optimization based on tool-augmented LLMs. To align the LLM with quantum-specific knowledge and improve the generated quantum circuits, QUASAR designs (i) a quantum circuit verification approach with external quantum simulators and (ii) a sophisticated hierarchical reward mechanism in RL training. Extensive evaluation shows improvements in both syntax and semantic performance of the generated quantum circuits. When augmenting a 4B LLM, QUASAR has achieved the validity of 99.31% in Pass@1 and 100% in Pass@10, outperforming industrial LLMs of GPT-4o, GPT-5 and DeepSeek-V3 and several supervised-fine-tuning (SFT)-only and RL-only baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†QUASARï¼Œä¸€ç§åŸºäºå·¥å…·å¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹(Tool-Augmented LLMs)å’Œä»£ç†å¼ºåŒ–å­¦ä¹ (Agentic RL)çš„é‡å­çº¿è·¯ç”Ÿæˆä¸ä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†å‚æ•°åŒ–é‡å­é—¨æ•°å€¼ç²¾ç¡®åº¦ä»¥åŠç¼ºä¹é‡å­é¢†åŸŸä¸“ä¸šçŸ¥è¯†æ–¹é¢çš„æŒ‘æˆ˜ã€‚QUASARé€šè¿‡é›†æˆå¤–éƒ¨é‡å­æ¨¡æ‹Ÿå™¨è¿›è¡Œçº¿è·¯éªŒè¯ï¼Œå¹¶è®¾è®¡äº†å¤æ‚çš„å±‚æ¬¡åŒ–å¥–åŠ±æœºåˆ¶(Hierarchical Reward Mechanism)ï¼Œå®ç°äº†LLMä¸é‡å­é¢†åŸŸçŸ¥è¯†çš„æœ‰æ•ˆå¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æå‡äº†ç”Ÿæˆé‡å­çº¿è·¯çš„è¯­æ³•å’Œè¯­ä¹‰æ€§èƒ½ã€‚åœ¨ä½¿ç”¨4Bå‚æ•°è§„æ¨¡çš„LLMæ—¶ï¼ŒQUASARåœ¨Pass@1æµ‹è¯•ä¸­è¾¾åˆ°äº†99.31%çš„æœ‰æ•ˆç‡ï¼Œå¹¶åœ¨Pass@10ä¸­å®ç°äº†100%çš„æœ‰æ•ˆç‡ã€‚è¿™ä¸€è¡¨ç°ä¼˜äºGPT-4oã€GPT-5å’ŒDeepSeek-V3ç­‰ä¸»æµå·¥ä¸šæ¨¡å‹ï¼Œä¸ºè‡ªåŠ¨åŒ–é‡å­ç¨‹åºè®¾è®¡æä¾›äº†é«˜å¯é æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "quant-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00967v1",
      "published_date": "2025-10-01 14:40:04 UTC",
      "updated_date": "2025-10-01 14:40:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:59:14.075257+00:00"
    },
    {
      "arxiv_id": "2510.00966v1",
      "title": "Deep Learning-Based Approach for Improving Relational Aggregated Search",
      "title_zh": "åŸºäºæ·±åº¦å­¦ä¹ çš„å…³ç³»å‹èšåˆæœç´¢æå‡æ–¹æ³•",
      "authors": [
        "Sara Saad Soliman",
        "Ahmed Younes",
        "Islam Elkabani",
        "Ashraf Elsayed"
      ],
      "abstract": "Due to an information explosion on the internet, there is a need for the development of aggregated search systems that can boost the retrieval and management of content in various formats. To further improve the clustering of Arabic text data in aggregated search environments, this research investigates the application of advanced natural language processing techniques, namely stacked autoencoders and AraBERT embeddings. By transcending the limitations of traditional search engines, which are imprecise, not contextually relevant, and not personalized, we offer more enriched, context-aware characterizations of search results, so we used a K-means clustering algorithm to discover distinctive features and relationships in these results, we then used our approach on different Arabic queries to evaluate its effectiveness. Our model illustrates that using stacked autoencoders in representation learning suits clustering tasks and can significantly improve clustering search results. It also demonstrates improved accuracy and relevance of search results.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•æ”¹è¿›é˜¿æ‹‰ä¼¯è¯­æ–‡æœ¬çš„èšåˆæœç´¢(Relational Aggregated Search)ç³»ç»Ÿï¼Œä»¥åº”å¯¹äº’è”ç½‘ä¿¡æ¯çˆ†ç‚¸å¸¦æ¥çš„æ£€ç´¢æŒ‘æˆ˜ã€‚ä½œè€…æå‡ºäº†ä¸€ç§ç»“åˆå…ˆè¿›è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„æ–°æ–¹æ³•ï¼Œå…·ä½“ä½¿ç”¨äº†AraBERT embeddingså’Œå †å è‡ªåŠ¨ç¼–ç å™¨(stacked autoencoders)è¿›è¡Œç‰¹å¾è¡¨ç¤ºã€‚ä¸ºäº†å…‹æœä¼ ç»Ÿæœç´¢å¼•æ“åœ¨ç²¾ç¡®åº¦ã€ä¸Šä¸‹æ–‡ç›¸å…³æ€§å’Œä¸ªæ€§åŒ–æ–¹é¢çš„å±€é™æ€§ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨K-means clusteringç®—æ³•å¯¹æœç´¢ç»“æœè¿›è¡Œèšç±»ï¼Œä»è€ŒæŒ–æ˜å‡ºå…¶ä¸­çš„æ˜¾è‘—ç‰¹å¾å’Œå…³è”å…³ç³»ã€‚å®éªŒé€šè¿‡å¤šç§é˜¿æ‹‰ä¼¯è¯­æŸ¥è¯¢å¯¹æ¨¡å‹è¿›è¡Œäº†éªŒè¯ï¼Œç»“æœè¡¨æ˜å †å è‡ªåŠ¨ç¼–ç å™¨åœ¨è¡¨ç¤ºå­¦ä¹ (representation learning)ä¸­éå¸¸é€‚åˆèšç±»ä»»åŠ¡ã€‚è¯¥æ¨¡å‹æ˜¾è‘—æå‡äº†æœç´¢ç»“æœçš„èšç±»æ•ˆæœï¼Œå¹¶æœ‰æ•ˆæé«˜äº†æ£€ç´¢å†…å®¹çš„å‡†ç¡®æ€§ä¸ç›¸å…³æ€§ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00966v1",
      "published_date": "2025-10-01 14:37:38 UTC",
      "updated_date": "2025-10-01 14:37:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:59:17.186148+00:00"
    },
    {
      "arxiv_id": "2510.00960v1",
      "title": "A Neuro-Fuzzy System for Interpretable Long-Term Stock Market Forecasting",
      "title_zh": "ç”¨äºå¯è§£é‡Šé•¿æœŸè‚¡ç¥¨å¸‚åœºé¢„æµ‹çš„ç¥ç»æ¨¡ç³Šç³»ç»Ÿ",
      "authors": [
        "Miha OÅ¾bot",
        "Igor Å krjanc",
        "Vitomir Å truc"
      ],
      "abstract": "In the complex landscape of multivariate time series forecasting, achieving both accuracy and interpretability remains a significant challenge. This paper introduces the Fuzzy Transformer (Fuzzformer), a novel recurrent neural network architecture combined with multi-head self-attention and fuzzy inference systems to analyze multivariate stock market data and conduct long-term time series forecasting. The method leverages LSTM networks and temporal attention to condense multivariate data into interpretable features suitable for fuzzy inference systems. The resulting architecture offers comparable forecasting performance to conventional models such as ARIMA and LSTM while providing meaningful information flow within the network. The method was examined on the real world stock market index S\\&P500. Initial results show potential for interpretable forecasting and identify current performance tradeoffs, suggesting practical application in understanding and forecasting stock market behavior.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Fuzzy Transformer (Fuzzformer)ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†å¾ªç¯ç¥ç»ç½‘ç»œ(RNN)ã€å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶(multi-head self-attention)å’Œæ¨¡ç³Šæ¨ç†ç³»ç»Ÿ(fuzzy inference systems)çš„æ–°å‹æ¶æ„ï¼Œæ—¨åœ¨è§£å†³å¤šå…ƒæ—¶é—´åºåˆ—é¢„æµ‹(multivariate time series forecasting)ä¸­å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§(interpretability)å…±å­˜çš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨LSTMç½‘ç»œå’Œæ—¶é—´æ³¨æ„åŠ›æœºåˆ¶(temporal attention)å°†å¤æ‚çš„å¤šå…ƒæ•°æ®å‹ç¼©ä¸ºå…·æœ‰å¯è§£é‡Šæ€§çš„ç‰¹å¾ï¼Œå¹¶å°†å…¶è¾“å…¥æ¨¡ç³Šæ¨ç†ç³»ç»Ÿè¿›è¡Œé•¿å‘¨æœŸé¢„æµ‹ã€‚å®éªŒåœ¨æ ‡å‡†æ™®å°”500æŒ‡æ•°(S&P500)çš„çœŸå®æ•°æ®ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œç»“æœæ˜¾ç¤ºFuzzformeråœ¨é¢„æµ‹æ€§èƒ½ä¸Šå¯ä¸ARIMAå’ŒLSTMç­‰ä¼ ç»Ÿæ¨¡å‹ç›¸åª²ç¾ï¼ŒåŒæ—¶èƒ½æä¾›ç½‘ç»œå†…éƒ¨æœ‰æ„ä¹‰çš„ä¿¡æ¯æµã€‚è¯¥ç ”ç©¶ä¸ä»…è¯†åˆ«äº†æ€§èƒ½ä¸å¯è§£é‡Šæ€§ä¹‹é—´çš„æƒè¡¡ï¼Œè¿˜ä¸ºç†è§£å’Œé¢„æµ‹è‚¡ç¥¨å¸‚åœºè¡Œä¸ºæä¾›äº†ä¸€ç§å…·æœ‰å®é™…åº”ç”¨æ½œåŠ›çš„è§£é‡Šæ€§é¢„æµ‹å·¥å…·ã€‚",
      "categories": [
        "cs.AI",
        "cs.NE",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "Published in: ERK 2025 -- 34th International Electrotechnical and Computer Science Conference, PortoroÅ¾, Slovenia, Sept. 25--26, 2025. Proceedings published by DruÅ¡tvo Slovenska sekcija IEEE. ISSN: 2591-0442 (online). 4 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.00960v1",
      "published_date": "2025-10-01 14:33:07 UTC",
      "updated_date": "2025-10-01 14:33:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:59:21.669576+00:00"
    },
    {
      "arxiv_id": "2510.00958v1",
      "title": "Test-Time Search in Neural Graph Coarsening Procedures for the Capacitated Vehicle Routing Problem",
      "title_zh": "æœ‰å®¹é‡é™åˆ¶è½¦è¾†è·¯å¾„é—®é¢˜ä¸­ç¥ç»å›¾ç²—åŒ–è¿‡ç¨‹çš„æµ‹è¯•æ—¶æœç´¢",
      "authors": [
        "Yoonju Sim",
        "Hyeonah Kim",
        "Changhyun Kwon"
      ],
      "abstract": "The identification of valid inequalities, such as the rounded capacity inequalities (RCIs), is a key component of cutting plane methods for the Capacitated Vehicle Routing Problem (CVRP). While a deep learning-based separation method can learn to find high-quality cuts, our analysis reveals that the model produces fewer cuts than expected because it is insufficiently sensitive to generate a diverse set of generated subsets. This paper proposes an alternative: enhancing the performance of a trained model at inference time through a new test-time search with stochasticity. First, we introduce stochastic edge selection into the graph coarsening procedure, replacing the previously proposed greedy approach. Second, we propose the Graph Coarsening History-based Partitioning (GraphCHiP) algorithm, which leverages coarsening history to identify not only RCIs but also, for the first time, the Framed capacity inequalities (FCIs). Experiments on randomly generated CVRP instances demonstrate the effectiveness of our approach in reducing the dual gap compared to the existing neural separation method. Additionally, our method discovers effective FCIs on a specific instance, despite the challenging nature of identifying such cuts.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Capacitated Vehicle Routing Problem (CVRP) æå‡ºäº†ä¸€ç§åœ¨æ¨ç†é˜¶æ®µé€šè¿‡éšæœºæ€§å¢å¼ºæœç´¢æ€§èƒ½çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ– Neural Graph Coarsening è¿‡ç¨‹ã€‚é’ˆå¯¹ç°æœ‰æ·±åº¦å­¦ä¹ åˆ†ç¦»æ–¹æ³•åœ¨ç”Ÿæˆæœ‰æ•ˆä¸ç­‰å¼æ—¶å¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ï¼Œä½œè€…åœ¨å›¾ç²—ç³™åŒ–ç¨‹åºä¸­å¼•å…¥äº† stochastic edge selection ä»¥æ›¿ä»£ä¼ ç»Ÿçš„è´ªå©ªç­–ç•¥ã€‚æ­¤å¤–ï¼Œè¯¥è®ºæ–‡æå‡ºäº† Graph Coarsening History-based Partitioning (GraphCHiP) ç®—æ³•ï¼Œåˆ©ç”¨ç²—ç³™åŒ–å†å²ä¸ä»…èƒ½è¯†åˆ« Rounded Capacity Inequalities (RCIs)ï¼Œè¿˜é¦–æ¬¡å®ç°äº†å¯¹ Framed Capacity Inequalities (FCIs) çš„è¯†åˆ«ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨éšæœºç”Ÿæˆçš„ CVRP å®ä¾‹ä¸Šæ¯”ç°æœ‰çš„ç¥ç»åˆ†ç¦»æ–¹æ³•èƒ½æ›´æœ‰æ•ˆåœ°ç¼©å° dual gapã€‚ç ”ç©¶è¿˜éªŒè¯äº†è¯¥æ–¹æ³•åœ¨å‘ç°å¤æ‚çš„ FCIs æ–¹é¢çš„å“è¶Šèƒ½åŠ›ï¼Œä¸ºæå‡ cutting plane methods çš„æ•ˆç‡æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00958v1",
      "published_date": "2025-10-01 14:31:36 UTC",
      "updated_date": "2025-10-01 14:31:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:59:25.184340+00:00"
    },
    {
      "arxiv_id": "2510.00956v2",
      "title": "Bridging the Gap Between Simulated and Real Network Data Using Transfer Learning",
      "title_zh": "åˆ©ç”¨è¿ç§»å­¦ä¹ å¼¥è¡¥ä»¿çœŸä¸çœŸå®ç½‘ç»œæ•°æ®ä¹‹é—´çš„å·®è·",
      "authors": [
        "Carlos GÃ¼emes-Palau",
        "Miquel Ferriol-GalmÃ©s",
        "Jordi Paillisse-Vilanova",
        "Albert LÃ³pez-BrescÃ³",
        "Pere Barlet-Ros",
        "Albert Cabellos-Aparicio"
      ],
      "abstract": "Machine Learning (ML)-based network models provide fast and accurate predictions for complex network behaviors but require substantial training data. Collecting such data from real networks is often costly and limited, especially for critical scenarios like failures. As a result, researchers commonly rely on simulated data, which reduces accuracy when models are deployed in real environments. We propose a hybrid approach leveraging transfer learning to combine simulated and real-world data. Using RouteNet-Fermi, we show that fine-tuning a pre-trained model with a small real dataset significantly improves performance. Our experiments with OMNeT++ and a custom testbed reduce the Mean Absolute Percentage Error (MAPE) in packet delay prediction by up to 88%. With just 10 real scenarios, MAPE drops by 37%, and with 50 scenarios, by 48%.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨å­¦ä¹ (Machine Learning)ç½‘ç»œæ¨¡å‹åœ¨çœŸå®ç¯å¢ƒä¸­ç”±äºè¿‡åº¦ä¾èµ–æ¨¡æ‹Ÿæ•°æ®è€Œå¯¼è‡´çš„å‡†ç¡®ç‡ä¸‹é™é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨è¿ç§»å­¦ä¹ (Transfer Learning)ç»“åˆæ¨¡æ‹Ÿä¸çœŸå®ä¸–ç•Œæ•°æ®çš„æ··åˆæ–¹æ³•ã€‚é€šè¿‡åœ¨ RouteNet-Fermi æ¶æ„ä¸Šä½¿ç”¨å°‘é‡çœŸå®æ•°æ®é›†è¿›è¡Œå¾®è°ƒ(Fine-tuning)ï¼Œè¯¥æ–¹æ¡ˆæœ‰æ•ˆå¼¥åˆäº†æ¨¡æ‹Ÿæ•°æ®ä¸ç°å®ç½‘ç»œä¹‹é—´çš„å·®è·ã€‚å®éªŒåœ¨ OMNeT++ å’Œè‡ªå®šä¹‰æµ‹è¯•å¹³å°ä¸Šå±•å¼€ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨é¢„æµ‹æ•°æ®åŒ…å»¶è¿Ÿ(Packet Delay)æ–¹é¢çš„å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®(MAPE)æœ€é«˜å¯é™ä½88%ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿åœ¨ä»…æœ‰10ä¸ªçœŸå®åœºæ™¯çš„æå°æ•°æ®é›†ä¸‹ï¼ŒMAPE ä¹Ÿèƒ½é™ä½37%ï¼Œè€Œåœ¨50ä¸ªåœºæ™¯ä¸‹åˆ™èƒ½é™ä½48%ã€‚è¿™è¯æ˜äº†è¿ç§»å­¦ä¹ åœ¨æå‡ç½‘ç»œæ¨¡å‹æ€§èƒ½åŠå‡å°‘çœŸå®æ•°æ®é‡‡é›†æˆæœ¬æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºå¤æ‚ç½‘ç»œè¡Œä¸ºçš„ç²¾ç¡®é¢„æµ‹æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NI",
      "comment": "This paper was submitted to IEEE NetSoft 2026. 7 Pages, 5 Figures",
      "pdf_url": "https://arxiv.org/pdf/2510.00956v2",
      "published_date": "2025-10-01 14:29:47 UTC",
      "updated_date": "2026-01-21 14:57:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:59:24.315468+00:00"
    },
    {
      "arxiv_id": "2510.01299v2",
      "title": "Enhancing the development of Cherenkov Telescope Array control software with Large Language Models",
      "title_zh": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹æå‡ Cherenkov Telescope Array æ§åˆ¶è½¯ä»¶çš„å¼€å‘",
      "authors": [
        "Dmitriy Kostunin",
        "Elisa Jones",
        "Vladimir Sotnikov",
        "Valery Sotnikov",
        "Sergo Golovachev",
        "Alexandre Strube"
      ],
      "abstract": "We develop AI agents based on instruction-finetuned large language models (LLMs) to assist in the engineering and operation of the Cherenkov Telescope Array Observatory (CTAO) Control and Data Acquisition Software (ACADA). These agents align with project-specific documentation and codebases, understand contextual information, interact with external APIs, and communicate with users in natural language. We present our progress in integrating these features into CTAO pipelines for operations and offline data analysis.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†åŸºäºæŒ‡ä»¤å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„ AI æ™ºèƒ½ä½“ï¼Œæ—¨åœ¨è¾…åŠ©åˆ‡ä¼¦ç§‘å¤«æœ›è¿œé•œé˜µåˆ—å¤©æ–‡å° (CTAO) æ§åˆ¶ä¸æ•°æ®è·å–è½¯ä»¶ (ACADA) çš„å·¥ç¨‹å¼€å‘ä¸è¿è¡Œç»´æŠ¤ã€‚è¿™äº›æ™ºèƒ½ä½“èƒ½å¤Ÿä¸é¡¹ç›®ç‰¹å®šçš„æ–‡æ¡£å’Œä»£ç åº“ç²¾å‡†å¯¹é½ï¼Œæ·±å…¥ç†è§£å¤æ‚çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶å…·å¤‡è°ƒç”¨å¤–éƒ¨ API è¿›è¡Œäº¤äº’çš„èƒ½åŠ›ã€‚é€šè¿‡è‡ªç„¶è¯­è¨€æ²Ÿé€šç•Œé¢ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿä»¥æ›´ç›´è§‚çš„æ–¹å¼ååŠ©å·¥ç¨‹å¸ˆå¤„ç†æŠ€æœ¯ä»»åŠ¡ã€‚ç›®å‰ï¼Œç ”ç©¶å›¢é˜Ÿå·²åœ¨å°†è¿™äº›åŠŸèƒ½é›†æˆåˆ° CTAO çš„è¿è¡Œç®¡çº¿å’Œç¦»çº¿æ•°æ®åˆ†ææµç¨‹ä¸­å–å¾—äº†é‡è¦è¿›å±•ã€‚è¯¥æˆæœå±•ç¤ºäº†åˆ©ç”¨ LLMs ä¼˜åŒ–å¤§å‹ç§‘å­¦å®éªŒè®¾æ–½è½¯ä»¶ç”Ÿå‘½å‘¨æœŸçš„æ½œåŠ›ï¼Œä¸ºæå‡å¤©æ–‡è§‚æµ‹ç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³æä¾›äº†æœ‰åŠ›æ”¯æ’‘ã€‚",
      "categories": [
        "astro-ph.IM",
        "cs.AI"
      ],
      "primary_category": "astro-ph.IM",
      "comment": "EuCAIFCon 2025 proceedings",
      "pdf_url": "https://arxiv.org/pdf/2510.01299v2",
      "published_date": "2025-10-01 14:14:41 UTC",
      "updated_date": "2025-11-13 10:31:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:59:29.987130+00:00"
    },
    {
      "arxiv_id": "2510.00922v1",
      "title": "On Discovering Algorithms for Adversarial Imitation Learning",
      "title_zh": "å¯¹æŠ—æ¨¡ä»¿å­¦ä¹ ç®—æ³•å‘ç°ç ”ç©¶",
      "authors": [
        "Shashank Reddy Chirra",
        "Jayden Teoh",
        "Praveen Paruchuri",
        "Pradeep Varakantham"
      ],
      "abstract": "Adversarial Imitation Learning (AIL) methods, while effective in settings with limited expert demonstrations, are often considered unstable. These approaches typically decompose into two components: Density Ratio (DR) estimation $\\frac{Ï_E}{Ï_Ï€}$, where a discriminator estimates the relative occupancy of state-action pairs under the policy versus the expert; and Reward Assignment (RA), where this ratio is transformed into a reward signal used to train the policy. While significant research has focused on improving density estimation, the role of reward assignment in influencing training dynamics and final policy performance has been largely overlooked. RA functions in AIL are typically derived from divergence minimization objectives, relying heavily on human design and ingenuity. In this work, we take a different approach: we investigate the discovery of data-driven RA functions, i.e, based directly on the performance of the resulting imitation policy. To this end, we leverage an LLM-guided evolutionary framework that efficiently explores the space of RA functions, yielding \\emph{Discovered Adversarial Imitation Learning} (DAIL), the first meta-learnt AIL algorithm. Remarkably, DAIL generalises across unseen environments and policy optimization algorithms, outperforming the current state-of-the-art of \\emph{human-designed} baselines. Finally, we analyse why DAIL leads to more stable training, offering novel insights into the role of RA functions in the stability of AIL. Code is publicly available: https://github.com/shshnkreddy/DAIL.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¯¹æŠ—æ¨¡ä»¿å­¦ä¹  (Adversarial Imitation Learning, AIL) è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼Œæ·±å…¥æ¢è®¨äº†å¥–åŠ±åˆ†é… (Reward Assignment, RA) å‡½æ•°å¯¹ç­–ç•¥æ€§èƒ½çš„å½±å“ã€‚ä¸åŒäºä¼ ç»Ÿä¾èµ–äººå·¥è®¾è®¡ RA å‡½æ•°çš„æ–¹æ³•ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLM) å¼•å¯¼çš„è¿›åŒ–æ¡†æ¶ï¼Œé€šè¿‡è‡ªåŠ¨æœç´¢æ•°æ®é©±åŠ¨çš„ RA å‡½æ•°å¼€å‘å‡ºé¦–ä¸ªå…ƒå­¦ä¹  AIL ç®—æ³•ï¼Œå³å‘ç°å¯¹æŠ—æ¨¡ä»¿å­¦ä¹  (Discovered Adversarial Imitation Learning, DAIL)ã€‚å®éªŒè¯æ˜ï¼ŒDAIL åœ¨å¤šç§æœªè§ç¯å¢ƒå’Œç­–ç•¥ä¼˜åŒ–ç®—æ³•ä¸­å±•ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œæ€§èƒ½æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„å…ˆè¿›åŸºå‡†ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡åˆ†ææ­ç¤ºäº† DAIL æå‡è®­ç»ƒç¨³å®šæ€§çš„å†…åœ¨æœºåˆ¶ï¼Œä¸º AIL ç®—æ³•çš„ä¼˜åŒ–æä¾›äº†æ–°çš„ç†è®ºè§†è§’ä¸å®è·µè·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00922v1",
      "published_date": "2025-10-01 14:02:05 UTC",
      "updated_date": "2025-10-01 14:02:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:59:34.674275+00:00"
    },
    {
      "arxiv_id": "2510.00919v2",
      "title": "Benchmarking Foundation Models with Retrieval-Augmented Generation in Olympic-Level Physics Problem Solving",
      "title_zh": "æ£€ç´¢å¢å¼ºç”Ÿæˆæ”¯æŒçš„åŸºç¡€æ¨¡å‹åœ¨å¥¥æ—åŒ¹å…‹çº§ç‰©ç†é—®é¢˜æ±‚è§£ä¸­çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Shunfeng Zheng",
        "Yudi Zhang",
        "Meng Fang",
        "Zihan Zhang",
        "Zhitan Wu",
        "Mykola Pechenizkiy",
        "Ling Chen"
      ],
      "abstract": "Retrieval-augmented generation (RAG) with foundation models has achieved strong performance across diverse tasks, but their capacity for expert-level reasoning-such as solving Olympiad-level physics problems-remains largely unexplored. Inspired by the way students prepare for competitions by reviewing past problems, we investigate the potential of RAG to enhance physics reasoning in foundation models. We introduce PhoPile, a high-quality multimodal dataset specifically designed for Olympiad-level physics, enabling systematic study of retrieval-based reasoning. PhoPile includes diagrams, graphs, and equations, capturing the inherently multimodal nature of physics problem solving. Using PhoPile, we benchmark RAG-augmented foundation models, covering both large language models (LLMs) and large multimodal models (LMMs) with multiple retrievers. Our results demonstrate that integrating retrieval with physics corpora can improve model performance, while also highlighting challenges that motivate further research in retrieval-augmented physics reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation, RAG)æŠ€æœ¯åœ¨è§£å†³å¥¥æ—åŒ¹å…‹çº§åˆ«ç‰©ç†é—®é¢˜ç­‰ä¸“å®¶çº§æ¨ç†ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†PhoPileï¼Œä¸€ä¸ªä¸“é—¨ä¸ºå¥¥æ—åŒ¹å…‹ç‰©ç†ç«èµ›è®¾è®¡çš„çš„é«˜è´¨é‡å¤šæ¨¡æ€æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å¤§é‡çš„å›¾è¡¨ã€å›¾åƒå’Œæ–¹ç¨‹å¼ï¼Œèƒ½å¤Ÿæ•æ‰ç‰©ç†é—®é¢˜è§£å†³è¿‡ç¨‹ä¸­çš„å¤šæ¨¡æ€ç‰¹æ€§ã€‚é€šè¿‡ä½¿ç”¨PhoPileæ•°æ®é›†ï¼Œç ”ç©¶è€…å¯¹ç»“åˆäº†å¤šç§æ£€ç´¢å™¨çš„åŸºç¡€æ¨¡å‹ï¼ˆåŒ…æ‹¬å¤§è¯­è¨€æ¨¡å‹LLMså’Œå¤§åŒæ¨¡æ€æ¨¡å‹LMMsï¼‰è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°†æ£€ç´¢æŠ€æœ¯ä¸ç‰©ç†ä¸“ä¸šè¯­æ–™åº“ç›¸ç»“åˆå¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ï¼ŒåŒæ—¶ä¹Ÿæ­ç¤ºäº†å½“å‰æ£€ç´¢å¢å¼ºç‰©ç†æ¨ç†é¢ä¸´çš„æŒ‘æˆ˜ã€‚è¯¥é¡¹å·¥ä½œä¸ºè¿›ä¸€æ­¥ç ”ç©¶å¤æ‚ç§‘å­¦æ¨ç†ä»»åŠ¡ä¸­çš„æ£€ç´¢å¢å¼ºæŠ€æœ¯æä¾›äº†åŸºç¡€å’ŒåŸºå‡†å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP 2025 (Findings)",
      "pdf_url": "https://arxiv.org/pdf/2510.00919v2",
      "published_date": "2025-10-01 13:57:53 UTC",
      "updated_date": "2025-10-02 09:55:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:59:32.680979+00:00"
    },
    {
      "arxiv_id": "2510.00915v3",
      "title": "Reinforcement Learning with Verifiable yet Noisy Rewards under Imperfect Verifiers",
      "title_zh": "ä¸å®Œç¾éªŒè¯å™¨ç¯å¢ƒä¸‹å¸¦æœ‰å¯éªŒè¯å™ªå£°å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Xin-Qiang Cai",
        "Wei Wang",
        "Feng Liu",
        "Tongliang Liu",
        "Gang Niu",
        "Masashi Sugiyama"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) replaces costly human labeling with automated verifiers. To reduce verifier hacking, many RLVR systems binarize rewards to $\\{0,1\\}$, but imperfect verifiers inevitably introduce \\emph{false negatives} (rejecting correct answers) and \\emph{false positives} (accepting incorrect ones). We formalize verifier unreliability as a stochastic reward channel with asymmetric noise rates $Ï_0$ and $Ï_1$ -- the FP rate and the FN rate, respectively. From this abstraction we derive two lightweight corrections: (i) a \\emph{backward} correction that yields an unbiased surrogate reward and thus an unbiased policy-gradient estimator in expectation, and (ii) a \\emph{forward} correction that reweights score-function terms so the expected update aligns with the clean gradient direction and requires only the FN rate. We implement both as lightweight hooks in a group relative policy optimization pipeline, both corrections improve RLVR for math reasoning under synthetic and real verifier noise, with the forward variant being more stable under heavier noise. Finally, an appeals mechanism with a lightweight LLM verifier estimates the FN rate online and further improves performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¸¦éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ (RLVR)ä¸­ç”±ä¸å®Œç¾éªŒè¯å™¨å¼•å…¥çš„False Negativeså’ŒFalse Positivesé—®é¢˜ï¼Œå°†å…¶å½¢å¼åŒ–ä¸ºå…·æœ‰éå¯¹ç§°å™ªå£°ç‡çš„éšæœºå¥–åŠ±é€šé“ã€‚ä¸ºäº†æå‡ç³»ç»Ÿé²æ£’æ€§ï¼Œè®ºæ–‡æå‡ºäº†ä¸¤ç§è½»é‡çº§ä¿®æ­£ç­–ç•¥ï¼šBackwardä¿®æ­£é€šè¿‡æ„å»ºæ— åä»£ç†å¥–åŠ±æ¥æä¾›æ— åç­–ç•¥æ¢¯åº¦ä¼°è®¡ï¼Œè€ŒForwardä¿®æ­£é€šè¿‡é‡æ–°åŠ æƒè¯„åˆ†å‡½æ•°é¡¹ç¡®ä¿é¢„æœŸæ›´æ–°æ–¹å‘ä¸çœŸå®æ¢¯åº¦ä¸€è‡´ã€‚åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™ä¸¤ç§ä¿®æ­£æ–¹æ¡ˆå‡æ˜¾è‘—æ”¹å–„äº†å­˜åœ¨éªŒè¯å™ªå£°æ—¶çš„RLVRæ€§èƒ½ï¼Œä¸”Forwardæ–¹æ¡ˆåœ¨é«˜å™ªå£°æ°´å¹³ä¸‹è¡¨ç°å‡ºæ›´å¼ºçš„ç¨³å®šæ€§ã€‚æœ€åï¼Œç ”ç©¶è¿˜è®¾è®¡äº†ä¸€ç§ç”³è¯‰æœºåˆ¶ï¼Œåˆ©ç”¨è½»é‡çº§å¤§è¯­è¨€æ¨¡å‹(LLM)åœ¨çº¿ä¼°è®¡FNç‡ï¼Œä»è€Œè¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹çš„æœ€ç»ˆè¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00915v3",
      "published_date": "2025-10-01 13:56:44 UTC",
      "updated_date": "2025-12-24 08:25:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:59:47.373319+00:00"
    },
    {
      "arxiv_id": "2510.00911v1",
      "title": "RiskPO: Risk-based Policy Optimization via Verifiable Reward for LLM Post-Training",
      "title_zh": "RiskPOï¼šåŸºäºå¯éªŒè¯å¥–åŠ±çš„å¤§è¯­è¨€æ¨¡å‹åè®­ç»ƒé£é™©ç­–ç•¥ä¼˜åŒ–",
      "authors": [
        "Tao Ren",
        "Jinyang Jiang",
        "Hui Yang",
        "Wan Tian",
        "Minhao Zou",
        "Guanghao Li",
        "Zishi Zhang",
        "Qinghao Wang",
        "Shentao Qin",
        "Yanjun Zhao",
        "Rui Tao",
        "Hui Shao",
        "Yijie Peng"
      ],
      "abstract": "Reinforcement learning with verifiable reward has recently emerged as a central paradigm for post-training large language models (LLMs); however, prevailing mean-based methods, such as Group Relative Policy Optimization (GRPO), suffer from entropy collapse and limited reasoning gains. We argue that these issues stem from overemphasizing high-probability output sequences while neglecting rare but informative reasoning paths. To address these challenges, we propose Risk-based Policy Optimization (RiskPO), which substitutes classical mean-based objectives with principled risk measures. Specifically, we introduce a Mixed Value-at-Risk objective that integrates weighted attention over multiple regions of the reward distribution, thereby amplifying gradient signals on challenging instances and preventing overconfident convergence. We further design a bundling scheme that aggregates multiple questions into bundles, thus enriching the feedback signal and yielding more stable and informative training dynamics. Theoretically, we prove that the risk-averse update alleviates entropy collapse and promotes exploration. Numerically, RiskPO achieves consistent and significant improvements in mathematical reasoning, multi-modal reasoning, and code generation benchmarks, surpassing GRPO and its variants on both Pass@1 and Pass@k metrics. Our results demonstrate that risk-based optimization provides a rigorous and effective paradigm for enhancing LLM reasoning capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åè®­ç»ƒä¸­ï¼Œä¼ ç»Ÿçš„åŸºäºå‡å€¼çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼ˆå¦‚GRPOï¼‰å®¹æ˜“å‡ºç°ç†µå´©å¡Œ(entropy collapse)å’Œæ¨ç†æ”¶ç›Šæœ‰é™ç­‰é—®é¢˜ï¼Œæå‡ºäº†Risk-based Policy Optimization (RiskPO)ã€‚RiskPOé€šè¿‡å¼•å…¥æ··åˆé£é™©ä»·å€¼(Mixed Value-at-Risk)ç›®æ ‡å‡½æ•°å–ä»£ç»å…¸çš„å‡å€¼ä¼˜åŒ–ç›®æ ‡ï¼Œåˆ©ç”¨å¥–åŠ±åˆ†å¸ƒä¸­å¤šä¸ªåŒºåŸŸçš„åŠ æƒæ³¨æ„åŠ›æ¥å¢å¼ºå¤æ‚å®ä¾‹çš„æ¢¯åº¦ä¿¡å·ï¼Œä»è€Œé˜²æ­¢æ¨¡å‹è¿‡åº¦è‡ªä¿¡æ”¶æ•›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è®¾è®¡äº†ä¸€ç§æ†ç»‘æ–¹æ¡ˆ(bundling scheme)æ¥èšåˆå¤šä¸ªé—®é¢˜ï¼Œæ—¨åœ¨æä¾›æ›´ç¨³å®šä¸”ä¿¡æ¯ä¸°å¯Œçš„åé¦ˆä¿¡å·ã€‚ç†è®ºè¯æ˜æ˜¾ç¤ºï¼Œè¿™ç§é£é™©è§„é¿å‹æ›´æ–°(risk-averse update)èƒ½æœ‰æ•ˆç¼“è§£ç†µå´©å¡Œå¹¶ä¿ƒè¿›æ¢ç´¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRiskPOåœ¨æ•°å­¦æ¨ç†ã€å¤šæ¨¡æ€æ¨ç†å’Œä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°æ˜¾è‘—ä¼˜äºGRPOåŠå…¶å˜ä½“ï¼Œåœ¨Pass@1å’ŒPass@kæŒ‡æ ‡ä¸Šå‡æœ‰å¤§å¹…æå‡ã€‚è¯¥ç ”ç©¶ä¸ºé€šè¿‡åŸºäºé£é™©çš„ä¼˜åŒ–æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ä¸ªä¸¥è°¨ä¸”æœ‰æ•ˆçš„èŒƒå¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00911v1",
      "published_date": "2025-10-01 13:53:09 UTC",
      "updated_date": "2025-10-01 13:53:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:59:49.784152+00:00"
    },
    {
      "arxiv_id": "2510.00909v1",
      "title": "\"We are not Future-ready\": Understanding AI Privacy Risks and Existing Mitigation Strategies from the Perspective of AI Developers in Europe",
      "title_zh": "â€œæˆ‘ä»¬å°šæœªå‡†å¤‡å¥½è¿æ¥æœªæ¥â€ï¼šEurope AI å¼€å‘è€…è§†è§’ä¸‹çš„ AI éšç§é£é™©ä¸ç°æœ‰ç¼“è§£ç­–ç•¥ç ”ç©¶",
      "authors": [
        "Alexandra Klymenko",
        "Stephen Meisenbacher",
        "Patrick Gage Kelley",
        "Sai Teja Peddinti",
        "Kurt Thomas",
        "Florian Matthes"
      ],
      "abstract": "The proliferation of AI has sparked privacy concerns related to training data, model interfaces, downstream applications, and more. We interviewed 25 AI developers based in Europe to understand which privacy threats they believe pose the greatest risk to users, developers, and businesses and what protective strategies, if any, would help to mitigate them. We find that there is little consensus among AI developers on the relative ranking of privacy risks. These differences stem from salient reasoning patterns that often relate to human rather than purely technical factors. Furthermore, while AI developers are aware of proposed mitigation strategies for addressing these risks, they reported minimal real-world adoption. Our findings highlight both gaps and opportunities for empowering AI developers to better address privacy risks in AI.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å¯¹25åæ¬§æ´²AIå¼€å‘è€…(AI developers)è¿›è¡Œè®¿è°ˆï¼Œæ·±å…¥æ¢è®¨äº†äººå·¥æ™ºèƒ½åœ¨è®­ç»ƒæ•°æ®ã€æ¨¡å‹æ¥å£åŠä¸‹æ¸¸åº”ç”¨ç­‰æ–¹é¢é¢ä¸´çš„éšç§é£é™©(privacy risks)ä¸ç°æœ‰ç¼“è§£ç­–ç•¥ã€‚ç ”ç©¶å‘ç°ï¼ŒAIå¼€å‘è€…åœ¨éšç§é£é™©çš„ç›¸å¯¹æ’åºä¸Šç¼ºä¹å…±è¯†ï¼Œè¿™ç§å·®å¼‚æºäºå—äººä¸ºå› ç´ è€Œéçº¯æŠ€æœ¯å› ç´ é©±åŠ¨çš„æ˜¾è‘—æ¨ç†æ¨¡å¼ã€‚æ­¤å¤–ï¼Œå°½ç®¡å¼€å‘è€…çŸ¥æ™“å¤šç§é’ˆå¯¹éšç§é£é™©çš„ç¼“è§£ç­–ç•¥ï¼Œä½†å…¶åœ¨ç°å®åœºæ™¯ä¸­çš„é‡‡ç”¨ç‡æä½ï¼Œåæ˜ å‡ºç†è®ºæ–¹æ¡ˆä¸å®é™…éƒ¨ç½²ä¹‹é—´çš„ä¸¥é‡è„±èŠ‚ã€‚è®ºæ–‡é€šè¿‡è¯†åˆ«å¼€å‘è€…åœ¨åº”å¯¹éšç§å¨èƒæ—¶çš„è®¤çŸ¥ä¸å®è·µå·®è·ï¼Œæ­ç¤ºäº†å½“å‰AIè¡Œä¸šåœ¨éšç§é˜²æŠ¤æ–¹é¢çš„ä¸è¶³ï¼Œå¹¶ä¸ºæœªæ¥èµ‹èƒ½å¼€å‘è€…ä»¥æ›´å¥½åœ°åº”å¯¹éšç§é£é™©æä¾›äº†é‡è¦å¥‘æœºã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "20 pages, 1 figure, 4 tables. Accepted to SOUPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.00909v1",
      "published_date": "2025-10-01 13:51:33 UTC",
      "updated_date": "2025-10-01 13:51:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:00:01.874578+00:00"
    },
    {
      "arxiv_id": "2510.00908v1",
      "title": "Bridging Language Gaps: Advances in Cross-Lingual Information Retrieval with Multilingual LLMs",
      "title_zh": "å¼¥åˆè¯­è¨€é¸¿æ²Ÿï¼šå¤šè¯­è¨€å¤§è¯­è¨€æ¨¡å‹åœ¨è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢ä¸­çš„ç ”ç©¶è¿›å±•",
      "authors": [
        "Roksana Goworek",
        "Olivia Macmillan-Scott",
        "Eda B. Ã–zyiÄŸit"
      ],
      "abstract": "Cross-lingual information retrieval (CLIR) addresses the challenge of retrieving relevant documents written in languages different from that of the original query. Research in this area has typically framed the task as monolingual retrieval augmented by translation, treating retrieval methods and cross-lingual capabilities in isolation. Both monolingual and cross-lingual retrieval usually follow a pipeline of query expansion, ranking, re-ranking and, increasingly, question answering. Recent advances, however, have shifted from translation-based methods toward embedding-based approaches and leverage multilingual large language models (LLMs), for which aligning representations across languages remains a central challenge. The emergence of cross-lingual embeddings and multilingual LLMs has introduced a new paradigm, offering improved retrieval performance and enabling answer generation. This survey provides a comprehensive overview of developments from early translation-based methods to state-of-the-art embedding-driven and generative techniques. It presents a structured account of core CLIR components, evaluation practices, and available resources. Persistent challenges such as data imbalance and linguistic variation are identified, while promising directions are suggested for advancing equitable and effective cross-lingual information retrieval. By situating CLIR within the broader landscape of information retrieval and multilingual language processing, this work not only reviews current capabilities but also outlines future directions for building retrieval systems that are robust, inclusive, and adaptable.",
      "tldr_zh": "è¯¥ç»¼è¿°ç³»ç»Ÿå›é¡¾äº†è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢ (Cross-lingual information retrieval, CLIR) ä»æ—©æœŸç¿»è¯‘é©±åŠ¨æ–¹æ³•åˆ°æœ€å‰æ²¿çš„åµŒå…¥é©±åŠ¨ (embedding-driven) åŠç”Ÿæˆå¼æŠ€æœ¯çš„æ¼”è¿›ã€‚æ–‡ç« é˜æ˜äº†ç ”ç©¶èŒƒå¼å¦‚ä½•ä»ä¼ ç»Ÿçš„ç¿»è¯‘å¢å¼ºæ¨¡å¼è½¬å‘åˆ©ç”¨å¤šè¯­è¨€å¤§è¯­è¨€æ¨¡å‹ (Multilingual LLMs) çš„æ–°èŒƒå¼ï¼Œå¹¶å¼ºè°ƒäº†è·¨è¯­è¨€è¡¨ç¤ºå¯¹é½ (aligning representations) è¿™ä¸€æ ¸å¿ƒæŒ‘æˆ˜ã€‚é€šè¿‡å¯¹ CLIR æ ¸å¿ƒç»„ä»¶ã€è¯„ä¼°å®è·µåŠå¯ç”¨èµ„æºçš„ç»“æ„åŒ–æ¢³ç†ï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº†å¤šè¯­è¨€ LLMs åœ¨æå‡æ£€ç´¢æ€§èƒ½ä¸èµ‹èƒ½ç”Ÿæˆå¼é—®ç­”æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¯†åˆ«äº†æ•°æ®ä¸å¹³è¡¡ (data imbalance) å’Œè¯­è¨€å˜å¼‚ (linguistic variation) ç­‰æŒç»­æ€§éš¾é¢˜ï¼Œå¹¶ä¸ºæ„å»ºæ›´å…·é²æ£’æ€§ã€åŒ…å®¹æ€§å’Œé€‚åº”æ€§çš„æ£€ç´¢ç³»ç»ŸæŒ‡æ˜äº†æœªæ¥æ–¹å‘ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00908v1",
      "published_date": "2025-10-01 13:50:05 UTC",
      "updated_date": "2025-10-01 13:50:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:59:58.272870+00:00"
    },
    {
      "arxiv_id": "2510.00906v1",
      "title": "TubeDAgger: Reducing the Number of Expert Interventions with Stochastic Reach-Tubes",
      "title_zh": "TubeDAggerï¼šåˆ©ç”¨éšæœºå¯è¾¾ç®¡å‡å°‘ä¸“å®¶å¹²é¢„æ¬¡æ•°",
      "authors": [
        "Julian Lemmel",
        "Manuel Kranzl",
        "Adam Lamine",
        "Philipp Neubauer",
        "Radu Grosu",
        "Sophie A. Neubauer"
      ],
      "abstract": "Interactive Imitation Learning deals with training a novice policy from expert demonstrations in an online fashion. The established DAgger algorithm trains a robust novice policy by alternating between interacting with the environment and retraining of the network. Many variants thereof exist, that differ in the method of discerning whether to allow the novice to act or return control to the expert. We propose the use of stochastic reachtubes - common in verification of dynamical systems - as a novel method for estimating the necessity of expert intervention. Our approach does not require fine-tuning of decision thresholds per environment and effectively reduces the number of expert interventions, especially when compared with related approaches that make use of a doubt classification model.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äº¤äº’å¼æ¨¡ä»¿å­¦ä¹ (Interactive Imitation Learning)ä¸­ä¸“å®¶å¹²é¢„é¢‘ç‡è¿‡é«˜çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºTubeDAggerçš„æ–°å‹ç®—æ³•æ¡†æ¶ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†åŠ¨åŠ›ç³»ç»ŸéªŒè¯é¢†åŸŸå¸¸ç”¨çš„éšæœºå¯è¾¾ç®¡(stochastic reachtubes)æ¦‚å¿µï¼Œç”¨äºå®æ—¶ä¼°ç®—ä¸“å®¶å¹²é¢„çš„å¿…è¦æ€§ï¼Œä»è€Œåœ¨ç¡®ä¿å®‰å…¨çš„å‰æä¸‹å‡å°‘å¯¹äººç±»ä¸“å®¶çš„ä¾èµ–ã€‚ä¸ç°æœ‰çš„DAggerå˜ä½“ç›¸æ¯”ï¼ŒTubeDAggeræ— éœ€é’ˆå¯¹ä¸åŒç¯å¢ƒæ‰‹åŠ¨å¾®è°ƒå†³ç­–é˜ˆå€¼ï¼Œå…·æœ‰æ›´å¼ºçš„ç¯å¢ƒè‡ªé€‚åº”èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡å°‘ä¸“å®¶å¹²é¢„æ¬¡æ•°æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºäºç–‘æƒ‘åˆ†ç±»æ¨¡å‹(doubt classification model)çš„ç›¸å…³æ–¹æ³•ã€‚é€šè¿‡ç²¾ç¡®è¯„ä¼°æ–°æ‰‹ç­–ç•¥(novice policy)çš„å®‰å…¨è¾¹ç•Œï¼ŒTubeDAggerä¸ºå®ç°åœ¨çº¿æ¨¡ä»¿å­¦ä¹ çš„é«˜æ•ˆè®­ç»ƒæä¾›äº†å¯é çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00906v1",
      "published_date": "2025-10-01 13:45:16 UTC",
      "updated_date": "2025-10-01 13:45:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:00:02.566417+00:00"
    },
    {
      "arxiv_id": "2510.00894v1",
      "title": "FusionAdapter for Few-Shot Relation Learning in Multimodal Knowledge Graphs",
      "title_zh": "FusionAdapterï¼šå¤šæ¨¡æ€çŸ¥è¯†å›¾è°±ä¸­çš„å°‘æ ·æœ¬å…³ç³»å­¦ä¹ ",
      "authors": [
        "Ran Liu",
        "Yuan Fang",
        "Xiaoli Li"
      ],
      "abstract": "Multimodal Knowledge Graphs (MMKGs) incorporate various modalities, including text and images, to enhance entity and relation representations. Notably, different modalities for the same entity often present complementary and diverse information. However, existing MMKG methods primarily align modalities into a shared space, which tends to overlook the distinct contributions of specific modalities, limiting their performance particularly in low-resource settings. To address this challenge, we propose FusionAdapter for the learning of few-shot relationships (FSRL) in MMKG. FusionAdapter introduces (1) an adapter module that enables efficient adaptation of each modality to unseen relations and (2) a fusion strategy that integrates multimodal entity representations while preserving diverse modality-specific characteristics. By effectively adapting and fusing information from diverse modalities, FusionAdapter improves generalization to novel relations with minimal supervision. Extensive experiments on two benchmark MMKG datasets demonstrate that FusionAdapter achieves superior performance over state-of-the-art methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€çŸ¥è¯†å›¾è°± (Multimodal Knowledge Graphs, MMKGs) ä¸­ç°æœ‰æ–¹æ³•åœ¨ä½èµ„æºè®¾ç½®ä¸‹å¾€å¾€å¿½ç•¥ç‰¹å®šæ¨¡æ€ç‹¬ç«‹è´¡çŒ®çš„é—®é¢˜ï¼Œæå‡ºäº† FusionAdapter æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–å°‘æ ·æœ¬å…³ç³»å­¦ä¹  (Few-Shot Relation Learning, FSRL) ä»»åŠ¡ã€‚FusionAdapter å¼•å…¥äº†ä¸€ä¸ªé€‚é…å™¨æ¨¡å— (Adapter module)ï¼Œä½¿æ¯ç§æ¨¡æ€èƒ½å¤Ÿé«˜æ•ˆåœ°é€‚åº”æœªè§è¿‡çš„å…³ç³»ã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº†ä¸€ç§èåˆç­–ç•¥ (Fusion strategy)ï¼Œåœ¨æ•´åˆå¤šæ¨¡æ€å®ä½“è¡¨ç¤ºçš„åŒæ—¶ï¼Œæœ‰æ•ˆåœ°ä¿ç•™äº†ä¸åŒæ¨¡æ€çš„ç‰¹å®šç‰¹å¾ã€‚é€šè¿‡è¿™ç§é«˜æ•ˆçš„æ¨¡æ€é€‚é…ä¸ä¿¡æ¯èåˆï¼ŒFusionAdapter èƒ½å¤Ÿä»¥æå°‘çš„ç›‘ç£ä¿¡å·æ˜¾è‘—æé«˜æ¨¡å‹å¯¹æ–°é¢–å…³ç³»çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨ä¸¤ä¸ªåŸºå‡† MMKG æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒFusionAdapter çš„è¡¨ç°ä¼˜äºç›®å‰çš„å…ˆè¿›æ–¹æ³•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Archived paper",
      "pdf_url": "https://arxiv.org/pdf/2510.00894v1",
      "published_date": "2025-10-01 13:36:56 UTC",
      "updated_date": "2025-10-01 13:36:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:00:00.979885+00:00"
    },
    {
      "arxiv_id": "2510.00890v1",
      "title": "Span-level Detection of AI-generated Scientific Text via Contrastive Learning and Structural Calibration",
      "title_zh": "åŸºäºå¯¹æ¯”å­¦ä¹ ä¸ç»“æ„åŒ–æ ¡å‡†çš„ AI ç”Ÿæˆç§‘å­¦æ–‡æœ¬è·¨åº¦çº§æ£€æµ‹",
      "authors": [
        "Zhen Yin",
        "Shenghua Wang"
      ],
      "abstract": "The rapid adoption of large language models (LLMs) in scientific writing raises serious concerns regarding authorship integrity and the reliability of scholarly publications. Existing detection approaches mainly rely on document-level classification or surface-level statistical cues; however, they neglect fine-grained span localization, exhibit weak calibration, and often fail to generalize across disciplines and generators. To address these limitations, we present Sci-SpanDet, a structure-aware framework for detecting AI-generated scholarly texts. The proposed method combines section-conditioned stylistic modeling with multi-level contrastive learning to capture nuanced human-AI differences while mitigating topic dependence, thereby enhancing cross-domain robustness. In addition, it integrates BIO-CRF sequence labeling with pointer-based boundary decoding and confidence calibration to enable precise span-level detection and reliable probability estimates. Extensive experiments on a newly constructed cross-disciplinary dataset of 100,000 annotated samples generated by multiple LLM families (GPT, Qwen, DeepSeek, LLaMA) demonstrate that Sci-SpanDet achieves state-of-the-art performance, with F1(AI) of 80.17, AUROC of 92.63, and Span-F1 of 74.36. Furthermore, it shows strong resilience under adversarial rewriting and maintains balanced accuracy across IMRaD sections and diverse disciplines, substantially surpassing existing baselines. To ensure reproducibility and to foster further research on AI-generated text detection in scholarly documents, the curated dataset and source code will be publicly released upon publication.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç§‘å­¦å†™ä½œä¸­å¼•å‘çš„ä½œè€…èº«ä»½å®Œæ•´æ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†Sci-SpanDetæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ£€æµ‹æ–¹æ³•åœ¨ç»†ç²’åº¦ç‰‡æ®µå®šä½(span localization)å’Œè·¨å­¦ç§‘æ³›åŒ–èƒ½åŠ›ä¸Šçš„ä¸è¶³ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ç« èŠ‚æ¡ä»¶çš„é£æ ¼å»ºæ¨¡(section-conditioned stylistic modeling)ä¸å¤šå±‚æ¬¡å¯¹æ¯”å­¦ä¹ (multi-level contrastive learning)ï¼Œé€šè¿‡æ•æ‰äººç±»ä¸AIçš„ç»†å¾®å·®å¼‚å¹¶å‡è½»ä¸»é¢˜ä¾èµ–ï¼Œæ˜¾è‘—å¢å¼ºäº†è·¨é¢†åŸŸé²æ£’æ€§ã€‚æ­¤å¤–ï¼ŒSci-SpanDetæ•´åˆäº†BIO-CRFåºåˆ—æ ‡æ³¨ã€åŸºäºæŒ‡é’ˆçš„è¾¹ç•Œè§£ç å’Œç½®ä¿¡åº¦æ ¡å‡†(confidence calibration)ï¼Œå®ç°äº†ç²¾ç¡®çš„ç‰‡æ®µçº§æ£€æµ‹ä¸å¯é çš„æ¦‚ç‡è¯„ä¼°ã€‚åœ¨æ¶µç›–GPTã€Qwenã€DeepSeekå’ŒLLaMAç­‰å¤šä¸ªæ¨¡å‹å®¶æ—çš„10ä¸‡ä¸ªè·¨å­¦ç§‘æ ·æœ¬æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å–å¾—äº†F1(AI) 80.17å’ŒSpan-F1 74.36çš„SOTAæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¯¹æŠ—æ€§é‡å†™ä¸‹å…·æœ‰å¾ˆå¼ºçš„éŸ§æ€§ï¼Œä¸”åœ¨IMRaDå„ç« èŠ‚åŠä¸åŒå­¦ç§‘é—´è¡¨ç°å‡è¡¡ï¼Œå…¶æ£€æµ‹æ•ˆèƒ½æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„åŸºå‡†æ¨¡å‹ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00890v1",
      "published_date": "2025-10-01 13:35:14 UTC",
      "updated_date": "2025-10-01 13:35:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:00:11.474823+00:00"
    },
    {
      "arxiv_id": "2510.00883v1",
      "title": "GLAI: GreenLightningAI for Accelerated Training through Knowledge Decoupling",
      "title_zh": "GLAIï¼šåŸºäºçŸ¥è¯†è§£è€¦åŠ é€Ÿè®­ç»ƒçš„ GreenLightningAI",
      "authors": [
        "Jose I. Mestre",
        "Alberto FernÃ¡ndez-HernÃ¡ndez",
        "Cristian PÃ©rez-Corral",
        "Manuel F. Dolz",
        "Jose Duato",
        "Enrique S. Quintana-OrtÃ­"
      ],
      "abstract": "In this work we introduce GreenLightningAI (GLAI), a new architectural block designed as an alternative to conventional MLPs. The central idea is to separate two types of knowledge that are usually entangled during training: (i) *structural knowledge*, encoded by the stable activation patterns induced by ReLU activations; and (ii) *quantitative knowledge*, carried by the numerical weights and biases. By fixing the structure once stabilized, GLAI reformulates the MLP as a combination of paths, where only the quantitative component is optimized. This reformulation retains the universal approximation capabilities of MLPs, yet achieves a more efficient training process, reducing training time by ~40% on average across the cases examined in this study. Crucially, GLAI is not just another classifier, but a generic block that can replace MLPs wherever they are used, from supervised heads with frozen backbones to projection layers in self-supervised learning or few-shot classifiers. Across diverse experimental setups, GLAI consistently matches or exceeds the accuracy of MLPs with an equivalent number of parameters, while converging faster. Overall, GLAI establishes a new design principle that opens a direction for future integration into large-scale architectures such as Transformers, where MLP blocks dominate the computational footprint.",
      "tldr_zh": "æœ¬ç ”ç©¶ä»‹ç»äº† GreenLightningAI (GLAI)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æ›¿ä»£ä¼ ç»Ÿ MLP çš„æ–°å‹æ¶æ„æ¨¡å—ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†è®­ç»ƒè¿‡ç¨‹ä¸­äº¤ç»‡çš„ *structural knowledge* ä¸ *quantitative knowledge* è¿›è¡Œè§£è€¦ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ ReLU æ¿€æ´»è¯±å¯¼çš„ç¨³å®šæ¨¡å¼ç¡®å®šç»“æ„çŸ¥è¯†ï¼Œå¹¶åœ¨ç»“æ„ç¨³å®šåå°†å…¶å›ºå®šï¼Œä»è€Œå°† MLP é‡æ–°è¡¨è¿°ä¸ºä»…éœ€ä¼˜åŒ–å®šé‡æƒé‡ä¸åç½®çš„è·¯å¾„ç»„åˆã€‚è¿™ç§é‡æ„ä¸ä»…ä¿ç•™äº† MLP çš„ä¸‡èƒ½é€¼è¿‘èƒ½åŠ›ï¼Œè¿˜åœ¨å®éªŒä¸­å®ç°äº†å¹³å‡çº¦ 40% çš„è®­ç»ƒæ—¶é—´ç¼©å‡ã€‚GLAI ä½œä¸ºä¸€ä¸ªé€šç”¨æ¨¡å—ï¼Œå¯æ— ç¼æ›¿æ¢ç›‘ç£å­¦ä¹ ã€è‡ªç›‘ç£å­¦ä¹ æŠ•å½±å±‚æˆ– few-shot åˆ†ç±»å™¨ä¸­çš„ MLP ç»„ä»¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å‚æ•°é‡ç›¸ç­‰çš„æƒ…å†µä¸‹ï¼ŒGLAI çš„å‡†ç¡®ç‡è¾¾åˆ°æˆ–è¶…è¿‡äº†åŸºçº¿æ¨¡å‹ï¼Œä¸”æ”¶æ•›é€Ÿåº¦æ›´å¿«ã€‚æ€»è€Œè¨€ä¹‹ï¼ŒGLAI ä¸ºé›†æˆåˆ° Transformer ç­‰è®¡ç®—å¯†é›†å‹å¤§è§„æ¨¡æ¶æ„ä¸­å¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œæœ‰æ•ˆé™ä½äº†å…¶è®¡ç®—è¶³è¿¹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.00883v1",
      "published_date": "2025-10-01 13:31:34 UTC",
      "updated_date": "2025-10-01 13:31:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:00:12.574041+00:00"
    },
    {
      "arxiv_id": "2510.00881v1",
      "title": "Advancing Automated Ethical Profiling in SE: a Zero-Shot Evaluation of LLM Reasoning",
      "title_zh": "æ¨è¿› SE ä¸­çš„è‡ªåŠ¨åŒ–ä¼¦ç†ç”»åƒï¼šLLM æ¨ç†èƒ½åŠ›çš„é›¶æ ·æœ¬è¯„ä¼°",
      "authors": [
        "Patrizio Migliarini",
        "Mashal Afzal Memon",
        "Marco Autili",
        "Paola Inverardi"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly integrated into software engineering (SE) tools for tasks that extend beyond code synthesis, including judgment under uncertainty and reasoning in ethically significant contexts. We present a fully automated framework for assessing ethical reasoning capabilities across 16 LLMs in a zero-shot setting, using 30 real-world ethically charged scenarios. Each model is prompted to identify the most applicable ethical theory to an action, assess its moral acceptability, and explain the reasoning behind their choice. Responses are compared against expert ethicists' choices using inter-model agreement metrics. Our results show that LLMs achieve an average Theory Consistency Rate (TCR) of 73.3% and Binary Agreement Rate (BAR) on moral acceptability of 86.7%, with interpretable divergences concentrated in ethically ambiguous cases. A qualitative analysis of free-text explanations reveals strong conceptual convergence across models despite surface-level lexical diversity. These findings support the potential viability of LLMs as ethical inference engines within SE pipelines, enabling scalable, auditable, and adaptive integration of user-aligned ethical reasoning. Our focus is the Ethical Interpreter component of a broader profiling pipeline: we evaluate whether current LLMs exhibit sufficient interpretive stability and theory-consistent reasoning to support automated profiling.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå…¨è‡ªåŠ¨æ¡†æ¶ï¼Œæ—¨åœ¨é›¶æ ·æœ¬(Zero-Shot)ç¯å¢ƒä¸‹è¯„ä¼°16ä¸ªå¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è½¯ä»¶å·¥ç¨‹(SE)åœºæ™¯ä¸­çš„ä¼¦ç†æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶é€šè¿‡30ä¸ªçœŸå®çš„ä¼¦ç†æŒ‘æˆ˜æ¡ˆä¾‹ï¼Œè¦æ±‚æ¨¡å‹è¯†åˆ«é€‚ç”¨çš„ä¼¦ç†ç†è®ºã€è¯„ä¼°é“å¾·å¯æ¥å—æ€§å¹¶è§£é‡Šæ¨ç†é€»è¾‘ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨ç†è®ºä¸€è‡´æ€§ç‡(TCR)å’Œé“å¾·å¯æ¥å—æ€§äºŒå…ƒä¸€è‡´æ€§ç‡(BAR)ä¸Šåˆ†åˆ«è¾¾åˆ°äº†73.3%å’Œ86.7%çš„å¹³å‡æ°´å¹³ã€‚å®šæ€§åˆ†æè¡¨æ˜ï¼Œå°½ç®¡ä¸åŒæ¨¡å‹é—´çš„è¯æ±‡è¡¨è¾¾å­˜åœ¨å·®å¼‚ï¼Œä½†åœ¨ä¼¦ç†æ¦‚å¿µä¸Šå‘ˆç°å‡ºé«˜åº¦æ”¶æ•›ã€‚è¿™äº›å‘ç°è¯å®äº†å°†LLMsä½œä¸ºè½¯ä»¶å·¥ç¨‹æµæ°´çº¿ä¸­ä¼¦ç†æ¨ç†å¼•æ“çš„å¯è¡Œæ€§ï¼Œèƒ½å¤Ÿå®ç°å¯æ‰©å±•ä¸”å¯å®¡è®¡çš„ä¼¦ç†é›†æˆã€‚è¯¥ç ”ç©¶é‡ç‚¹éªŒè¯äº†Ethical Interpreterç»„ä»¶çš„ç¨³å®šæ€§ï¼Œä¸ºè‡ªåŠ¨åŒ–ä¼¦ç†å‰–ææä¾›äº†æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted at ASE 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.00881v1",
      "published_date": "2025-10-01 13:28:26 UTC",
      "updated_date": "2025-10-01 13:28:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:00:13.086010+00:00"
    },
    {
      "arxiv_id": "2510.00876v1",
      "title": "Unveiling Interesting Insights: Monte Carlo Tree Search for Knowledge Discovery",
      "title_zh": "æ­ç¤ºæ·±åº¦æ´å¯Ÿï¼šé¢å‘çŸ¥è¯†å‘ç°çš„è’™ç‰¹å¡æ´›æ ‘æœç´¢",
      "authors": [
        "Pietro Totis",
        "Alberto Pozanco",
        "Daniel Borrajo"
      ],
      "abstract": "Organizations are increasingly focused on leveraging data from their processes to gain insights and drive decision-making. However, converting this data into actionable knowledge remains a difficult and time-consuming task. There is often a gap between the volume of data collected and the ability to process and understand it, which automated knowledge discovery aims to fill. Automated knowledge discovery involves complex open problems, including effectively navigating data, building models to extract implicit relationships, and considering subjective goals and knowledge. In this paper, we introduce a novel method for Automated Insights and Data Exploration (AIDE), that serves as a robust foundation for tackling these challenges through the use of Monte Carlo Tree Search (MCTS). We evaluate AIDE using both real-world and synthetic data, demonstrating its effectiveness in identifying data transformations and models that uncover interesting data patterns. Among its strengths, AIDE's MCTS-based framework offers significant extensibility, allowing for future integration of additional pattern extraction strategies and domain knowledge. This makes AIDE a valuable step towards developing a comprehensive solution for automated knowledge discovery.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºAIDEçš„æ–°å‹è‡ªåŠ¨åŒ–æ´å¯Ÿä¸æ•°æ®æ¢ç´¢æ–¹æ³•ï¼Œæ—¨åœ¨å¡«è¡¥ç»„ç»‡åœ¨æµ·é‡æ•°æ®æ”¶é›†ä¸ç†è§£èƒ½åŠ›ä¹‹é—´çš„é¸¿æ²Ÿã€‚è¯¥æ–¹æ³•ä»¥Monte Carlo Tree Search (MCTS)ä½œä¸ºæ ¸å¿ƒæŠ€æœ¯æ¡†æ¶ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°åœ¨å¤æ‚çš„æ•°æ®ç©ºé—´ä¸­å¯¼èˆªï¼Œå¹¶æ„å»ºæ¨¡å‹ä»¥æå–æ•°æ®é—´çš„éšå¼å…³ç³»ã€‚é€šè¿‡å¯¹çœŸå®ä¸–ç•Œå’Œåˆæˆæ•°æ®çš„å®éªŒè¯„ä¼°ï¼Œè¯æ˜äº†AIDEåœ¨è¯†åˆ«æ•°æ®è½¬æ¢åŠæ­ç¤ºæœ‰è¶£æ•°æ®æ¨¡å¼æ–¹é¢å…·æœ‰æ˜¾è‘—æ•ˆæœã€‚AIDEåŸºäºMCTSçš„æ¶æ„æä¾›äº†æå¼ºçš„å¯æ‰©å±•æ€§ï¼Œæ”¯æŒæœªæ¥é›†æˆæ›´å¤šçš„æ¨¡å¼æå–ç­–ç•¥å’Œé¢†åŸŸçŸ¥è¯†ã€‚è¿™é¡¹å·¥ä½œä¸ºå¼€å‘å…¨é¢çš„Automated Knowledge Discoveryè§£å†³æ–¹æ¡ˆè¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00876v1",
      "published_date": "2025-10-01 13:25:15 UTC",
      "updated_date": "2025-10-01 13:25:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:00:15.969095+00:00"
    },
    {
      "arxiv_id": "2510.00862v1",
      "title": "Gather-Scatter Mamba: Accelerating Propagation with Efficient State Space Model",
      "title_zh": "Gather-Scatter Mambaï¼šåˆ©ç”¨é«˜æ•ˆçŠ¶æ€ç©ºé—´æ¨¡å‹åŠ é€Ÿä¼ æ’­",
      "authors": [
        "Hyun-kyu Ko",
        "Youbin Kim",
        "Jihyeon Park",
        "Dongheok Park",
        "Gyeongjin Kang",
        "Wonjun Cho",
        "Hyung Yi",
        "Eunbyung Park"
      ],
      "abstract": "State Space Models (SSMs)-most notably RNNs-have historically played a central role in sequential modeling. Although attention mechanisms such as Transformers have since dominated due to their ability to model global context, their quadratic complexity and limited scalability make them less suited for long sequences. Video super-resolution (VSR) methods have traditionally relied on recurrent architectures to propagate features across frames. However, such approaches suffer from well-known issues including vanishing gradients, lack of parallelism, and slow inference speed. Recent advances in selective SSMs like Mamba offer a compelling alternative: by enabling input-dependent state transitions with linear-time complexity, Mamba mitigates these issues while maintaining strong long-range modeling capabilities. Despite this potential, Mamba alone struggles to capture fine-grained spatial dependencies due to its causal nature and lack of explicit context aggregation. To address this, we propose a hybrid architecture that combines shifted window self-attention for spatial context aggregation with Mamba-based selective scanning for efficient temporal propagation. Furthermore, we introduce Gather-Scatter Mamba (GSM), an alignment-aware mechanism that warps features toward a center anchor frame within the temporal window before Mamba propagation and scatters them back afterward, effectively reducing occlusion artifacts and ensuring effective redistribution of aggregated information across all frames. The official implementation is provided at: https://github.com/Ko-Lani/GSMamba.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†é¢‘è¶…åˆ†è¾¨ç‡(Video Super-Resolution)ä»»åŠ¡ä¸­ä¼ ç»Ÿå¾ªç¯æ¶æ„å¹¶è¡Œæ€§å·®ã€æ¨ç†é€Ÿåº¦æ…¢ä»¥åŠTransformerè®¡ç®—å¤æ‚åº¦é«˜çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºGather-Scatter Mamba (GSM)çš„æ–°å‹é«˜æ•ˆæ¶æ„ã€‚ä¸ºäº†å…‹æœMambaåœ¨æ•è·ç²¾ç»†ç©ºé—´ä¾èµ–æ–¹é¢çš„å±€é™æ€§ï¼Œè¯¥æ¶æ„å°†ç”¨äºç©ºé—´ä¸Šä¸‹æ–‡èšåˆçš„ç§»ä½çª—å£è‡ªæ³¨æ„åŠ›(Shifted window self-attention)ä¸åŸºäºMambaçš„é€‰æ‹©æ€§æ‰«æ(Selective scanning)ç›¸ç»“åˆï¼Œå®ç°äº†é«˜æ•ˆçš„æ—¶é—´ç‰¹å¾ä¼ æ’­ã€‚GSMæ ¸å¿ƒå¼•å…¥äº†ä¸€ç§å¯¹é½æ„ŸçŸ¥æœºåˆ¶ï¼Œåœ¨Mambaä¼ æ’­å‰å°†ç‰¹å¾å‘ä¸­å¿ƒå‚è€ƒå¸§è¿›è¡Œæ‰­æ›²(warps)ï¼Œå¹¶åœ¨ä¼ æ’­åå°†å…¶åˆ†æ•£å›åŸä½ï¼Œä»è€Œæœ‰æ•ˆå‡å°‘äº†é®æŒ¡ä¼ªå½±ã€‚è¿™ç§Gather-Scatterç­–ç•¥ç¡®ä¿äº†èšåˆä¿¡æ¯èƒ½åœ¨æ‰€æœ‰è§†é¢‘å¸§ä¹‹é—´è¿›è¡Œæœ‰æ•ˆé‡æ–°åˆ†å¸ƒï¼Œå¢å¼ºäº†æ¨¡å‹åœ¨é•¿åºåˆ—å»ºæ¨¡ä¸­çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒçº¿æ€§æ—¶é—´å¤æ‚åº¦çš„åŒæ—¶æ˜¾è‘—æå‡äº†ä¼ æ’­æ•ˆç‡ä¸é‡å»ºè´¨é‡ï¼Œä¸ºé«˜æ•ˆè§†é¢‘å¤„ç†æä¾›äº†æ–°çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Code: \\url{https://github.com/Ko-Lani/GSMamba}",
      "pdf_url": "https://arxiv.org/pdf/2510.00862v1",
      "published_date": "2025-10-01 13:11:13 UTC",
      "updated_date": "2025-10-01 13:11:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:00:29.369491+00:00"
    },
    {
      "arxiv_id": "2510.00861v1",
      "title": "Erase to Improve: Erasable Reinforcement Learning for Search-Augmented LLMs",
      "title_zh": "ä»¥æ“¦é™¤ä¿ƒæå‡ï¼šé¢å‘æœç´¢å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„å¯æ“¦é™¤å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Ziliang Wang",
        "Kang An",
        "Xuhui Zheng",
        "Faqiang Qian",
        "Weikun Zhang",
        "Cijun Ouyang",
        "Jialu Cai",
        "Yuhang Wang",
        "Yichao Wu"
      ],
      "abstract": "While search-augmented large language models (LLMs) exhibit impressive capabilities, their reliability in complex multi-hop reasoning remains limited. This limitation arises from three fundamental challenges: decomposition errors, where tasks are incorrectly broken down; retrieval missing, where key evidence fails to be retrieved; and reasoning errors, where flawed logic propagates through the reasoning chain. A single failure in any of these stages can derail the final answer. We propose Erasable Reinforcement Learning (ERL), a novel framework that transforms fragile reasoning into a robust process. ERL explicitly identifies faulty steps, erases them, and regenerates reasoning in place, preventing defective logic from propagating through the reasoning chain. This targeted correction mechanism turns brittle reasoning into a more resilient process. Models trained with ERL, termed ESearch, achieve substantial improvements on HotpotQA, MuSiQue, 2Wiki, and Bamboogle, with the 3B model achieving +8.48% EM and +11.56% F1, and the 7B model achieving +5.38% EM and +7.22% F1 over previous state-of-the-art(SOTA) results. These findings suggest that erasable reinforcement learning provides a powerful paradigm shift for robust multi-step reasoning in LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœç´¢å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ (Search-augmented LLMs) åœ¨å¤æ‚å¤šè·³æ¨ç†ä¸­ç”±äºåˆ†è§£é”™è¯¯ã€æ£€ç´¢ç¼ºå¤±å’Œé€»è¾‘æ¼æ´å¯¼è‡´çš„å¯é æ€§ä¸è¶³é—®é¢˜ï¼Œæå‡ºäº†å¯æ“¦é™¤å¼ºåŒ–å­¦ä¹  (Erasable Reinforcement Learning, ERL) æ¡†æ¶ã€‚ERL èƒ½å¤Ÿæ˜¾å¼è¯†åˆ«æ¨ç†é“¾ä¸­çš„é”™è¯¯æ­¥éª¤ï¼Œé€šè¿‡å°†å…¶æ“¦é™¤å¹¶åŸä½é‡æ–°ç”Ÿæˆï¼Œæœ‰æ•ˆé˜»æ­¢äº†ç¼ºé™·é€»è¾‘çš„ä¼ æ’­ï¼Œä»è€Œå°†è„†å¼±çš„æ¨ç†è½¬åŒ–ä¸ºé²æ£’çš„è¿‡ç¨‹ã€‚åŸºäºæ­¤æ¡†æ¶è®­ç»ƒçš„ ESearch æ¨¡å‹åœ¨ HotpotQAã€MuSiQueã€2Wiki å’Œ Bamboogle ç­‰åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå…¶ 3B æ¨¡å‹ç›¸è¾ƒäºä¹‹å‰çš„æœ€å…ˆè¿› (SOTA) ç»“æœåœ¨ EM å’Œ F1 æŒ‡æ ‡ä¸Šåˆ†åˆ«æå‡äº† 8.48% å’Œ 11.56%ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œå¯æ“¦é™¤å¼ºåŒ–å­¦ä¹ ä¸ºå®ç°å¤§è¯­è¨€æ¨¡å‹ç¨³å¥çš„å¤šæ­¥æ¨ç†æä¾›äº†é‡è¦çš„èŒƒå¼è½¬å˜ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.00861v1",
      "published_date": "2025-10-01 13:10:36 UTC",
      "updated_date": "2025-10-01 13:10:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:00:38.679888+00:00"
    },
    {
      "arxiv_id": "2510.00855v1",
      "title": "Can World Models Benefit VLMs for World Dynamics?",
      "title_zh": "ä¸–ç•Œæ¨¡å‹èƒ½å¦åŠ©åŠ› VLMs ç†è§£ä¸–ç•ŒåŠ¨åŠ›å­¦ï¼Ÿ",
      "authors": [
        "Kevin Zhang",
        "Kuangzhi Ge",
        "Xiaowei Chi",
        "Renrui Zhang",
        "Shaojun Shi",
        "Zhen Dong",
        "Sirui Han",
        "Shanghang Zhang"
      ],
      "abstract": "Trained on internet-scale video data, generative world models are increasingly recognized as powerful world simulators that can generate consistent and plausible dynamics over structure, motion, and physics. This raises a natural question: with the advent of strong video foundational models, might they supplant conventional vision encoder paradigms for general-purpose multimodal understanding? While recent studies have begun to explore the potential of world models on common vision tasks, these explorations typically lack a systematic investigation of generic, multimodal tasks. In this work, we strive to investigate the capabilities when world model priors are transferred into Vision-Language Models: we re-purpose a video diffusion model as a generative encoder to perform a single denoising step and treat the resulting latents as a set of visual embedding. We empirically investigate this class of models, which we refer to as World-Language Models (WorldLMs), and we find that generative encoders can capture latents useful for downstream understanding that show distinctions from conventional encoders. Naming our best-performing variant Dynamic Vision Aligner (DyVA), we further discover that this method significantly enhances spatial reasoning abilities and enables single-image models to perform multi-frame reasoning. Through the curation of a suite of visual reasoning tasks, we find DyVA to surpass both open-source and proprietary baselines, achieving state-of-the-art or comparable performance. We attribute these gains to WorldLM's inherited motion-consistency internalization from video pre-training. Finally, we systematically explore extensive model designs to highlight promising directions for future work. We hope our study can pave the way for a new family of VLMs that leverage priors from world models and are on a promising path towards generalist vision learners.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼ World Models çš„å…ˆéªŒçŸ¥è¯†æ˜¯å¦èƒ½æå‡ Vision-Language Models (VLMs) å¯¹ä¸–ç•ŒåŠ¨æ€çš„ç†è§£èƒ½åŠ›ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åä¸º World-Language Models (WorldLMs) çš„æ–°å‹æ¨¡å‹ï¼Œé€šè¿‡å°†è§†é¢‘æ‰©æ•£æ¨¡å‹ (video diffusion model) é‡æ–°åˆ©ç”¨ä¸ºç”Ÿæˆå¼ç¼–ç å™¨ (generative encoder) æ¥æå–è§†è§‰åµŒå…¥ (visual embedding)ã€‚å…¶ä¸­è¡¨ç°æœ€ä½³çš„å˜ä½“ Dynamic Vision Aligner (DyVA) æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå¹¶ä½¿å•å›¾æ¨¡å‹å…·å¤‡äº†å¤šå¸§æ¨ç†çš„æ½œèƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDyVA åœ¨å¤šé¡¹è§†è§‰æ¨ç†ä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰çš„å¼€æºå’Œé—­æºåŸºçº¿æ¨¡å‹ï¼Œè¾¾åˆ°äº† SOTA æˆ–åŒç­‰æ°´å¹³ã€‚è¿™ç§æ€§èƒ½å¢ç›Šä¸»è¦æºäº WorldLM ç»§æ‰¿äº†è§†é¢‘é¢„è®­ç»ƒä¸­å¯¹è¿åŠ¨ä¸€è‡´æ€§ (motion-consistency) çš„å†…åŒ–ç†è§£ã€‚è¯¥å·¥ä½œè¯æ˜äº†åˆ©ç”¨ä¸–ç•Œæ¨¡å‹å…ˆéªŒæ„å»ºé€šç”¨è§†è§‰å­¦ä¹ è€…çš„å¯è¡Œæ€§ï¼Œä¸ºæœªæ¥ VLM çš„è®¾è®¡æä¾›äº†æ–°æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://dyva-worldlm.github.io",
      "pdf_url": "https://arxiv.org/pdf/2510.00855v1",
      "published_date": "2025-10-01 13:07:05 UTC",
      "updated_date": "2025-10-01 13:07:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:00:33.383197+00:00"
    },
    {
      "arxiv_id": "2510.00845v2",
      "title": "Mechanistic Interpretability as Statistical Estimation: A Variance Analysis of EAP-IG",
      "title_zh": "å°†æœºæ¢°å¯è§£é‡Šæ€§è§†ä¸ºç»Ÿè®¡ä¼°è®¡ï¼šEAP-IG çš„æ–¹å·®åˆ†æ",
      "authors": [
        "Maxime MÃ©loux",
        "FranÃ§ois Portet",
        "Maxime Peyrard"
      ],
      "abstract": "The development of trustworthy artificial intelligence requires moving beyond black-box performance metrics toward an understanding of models' internal computations. Mechanistic Interpretability (MI) aims to meet this need by identifying the algorithmic mechanisms underlying model behaviors. Yet, the scientific rigor of MI critically depends on the reliability of its findings. In this work, we argue that interpretability methods, such as circuit discovery, should be viewed as statistical estimators, subject to questions of variance and robustness. To illustrate this statistical framing, we present a systematic stability analysis of a state-of-the-art circuit discovery method: EAP-IG. We evaluate its variance and robustness through a comprehensive suite of controlled perturbations, including input resampling, prompt paraphrasing, hyperparameter variation, and injected noise within the causal analysis itself. Across a diverse set of models and tasks, our results demonstrate that EAP-IG exhibits high structural variance and sensitivity to hyperparameters, questioning the stability of its findings. Based on these results, we offer a set of best-practice recommendations for the field, advocating for the routine reporting of stability metrics to promote a more rigorous and statistically grounded science of interpretability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºå°†æœºæ¢°å¯è§£é‡Šæ€§ (Mechanistic Interpretability) æ–¹æ³•ï¼ˆå¦‚ç”µè·¯å‘ç° circuit discoveryï¼‰è§†ä¸ºç»Ÿè®¡ä¼°è®¡é‡ (statistical estimators)ï¼Œå¼ºè°ƒå…¶ç§‘å­¦ä¸¥è°¨æ€§å–å†³äºç ”ç©¶ç»“æœçš„å¯é æ€§ä¸ç¨³å®šæ€§ã€‚ä¸ºäº†é€šè¿‡ç»Ÿè®¡å­¦è§†è§’è¿›è¡Œè®ºè¯ï¼Œä½œè€…å¯¹å½“å‰å…ˆè¿›çš„ç”µè·¯å‘ç°æ–¹æ³• EAP-IG è¿›è¡Œäº†ç³»ç»Ÿçš„ç¨³å®šæ€§åˆ†æ (stability analysis)ã€‚è¯¥ç ”ç©¶é€šè¿‡è¾“å…¥é‡é‡‡æ · (input resampling)ã€æç¤ºè¯æ”¹å†™ (prompt paraphrasing) å’Œè¶…å‚æ•°å˜åŒ– (hyperparameter variation) ç­‰å—æ§æ‰°åŠ¨è¯„ä¼°å…¶è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEAP-IG åœ¨ä¸åŒæ¨¡å‹å’Œä»»åŠ¡ä¸­è¡¨ç°å‡ºè¾ƒé«˜çš„ç»“æ„æ–¹å·® (structural variance) ä»¥åŠå¯¹è¶…å‚æ•°çš„é«˜åº¦æ•æ„Ÿæ€§ï¼Œè¿™å¯¹è¯¥æ–¹æ³•æ‰€è·å‘ç°çš„ç¨³å®šæ€§æå‡ºäº†è´¨ç–‘ã€‚åŸºäºè¿™äº›ç»“æœï¼Œç ”ç©¶è€…ä¸ºè¯¥é¢†åŸŸæå‡ºäº†ä¸€ç³»åˆ—æœ€ä½³å®è·µå»ºè®®ï¼Œä¸»å¼ é€šè¿‡å¸¸è§„åŒ–æŠ¥å‘Šç¨³å®šæ€§æŒ‡æ ‡ (stability metrics) æ¥æ¨åŠ¨å»ºç«‹æ›´å…·ä¸¥è°¨æ€§å’Œç»Ÿè®¡å­¦åŸºç¡€çš„å¯è§£é‡Šæ€§ç§‘å­¦ä½“ç³»ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00845v2",
      "published_date": "2025-10-01 12:55:34 UTC",
      "updated_date": "2025-10-02 11:16:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:00:39.166345+00:00"
    },
    {
      "arxiv_id": "2510.00844v1",
      "title": "Learning Compact Representations of LLM Abilities via Item Response Theory",
      "title_zh": "åŸºäºé¡¹ç›®ååº”ç†è®ºçš„å¤§è¯­è¨€æ¨¡å‹èƒ½åŠ›ç´§å‡‘è¡¨ç¤ºå­¦ä¹ ",
      "authors": [
        "Jianhao Chen",
        "Chenxu Wang",
        "Gengrui Zhang",
        "Peng Ye",
        "Lei Bai",
        "Wei Hu",
        "Yuzhong Qu",
        "Shuyue Hu"
      ],
      "abstract": "Recent years have witnessed a surge in the number of large language models (LLMs), yet efficiently managing and utilizing these vast resources remains a significant challenge. In this work, we explore how to learn compact representations of LLM abilities that can facilitate downstream tasks, such as model routing and performance prediction on new benchmarks. We frame this problem as estimating the probability that a given model will correctly answer a specific query. Inspired by the item response theory (IRT) in psychometrics, we model this probability as a function of three key factors: (i) the model's multi-skill ability vector, (2) the query's discrimination vector that separates models of differing skills, and (3) the query's difficulty scalar. To learn these parameters jointly, we introduce a Mixture-of-Experts (MoE) network that couples model- and query-level embeddings. Extensive experiments demonstrate that our approach leads to state-of-the-art performance in both model routing and benchmark accuracy prediction. Moreover, analysis validates that the learned parameters encode meaningful, interpretable information about model capabilities and query characteristics.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•å­¦ä¹ å¤§è¯­è¨€æ¨¡å‹(LLMs)èƒ½åŠ›çš„ç´§å‡‘è¡¨ç¤ºï¼Œä»¥ä¼˜åŒ–æ¨¡å‹è·¯ç”±(model routing)å’Œæ–°åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½é¢„æµ‹ã€‚ç ”ç©¶å—åˆ°å¿ƒç†æµ‹é‡å­¦ä¸­é¡¹ç›®ååº”ç†è®º(IRT)çš„å¯å‘ï¼Œå°†è¯¥é—®é¢˜å»ºæ¨¡ä¸ºé¢„æµ‹ç‰¹å®šæ¨¡å‹æ­£ç¡®å›ç­”ç‰¹å®šæŸ¥è¯¢çš„æ¦‚ç‡å‡½æ•°ã€‚è¯¥æ¨¡å‹ç»¼åˆè€ƒè™‘äº†æ¨¡å‹çš„å¤šæŠ€èƒ½èƒ½åŠ›å‘é‡(ability vector)ã€æŸ¥è¯¢çš„åŒºåˆ†åº¦å‘é‡(discrimination vector)ä»¥åŠéš¾åº¦æ ‡é‡(difficulty scalar)ä¸‰ä¸ªæ ¸å¿ƒå› ç´ ã€‚ä¸ºäº†å®ç°å‚æ•°çš„è”åˆå­¦ä¹ ï¼Œä½œè€…å¼•å…¥äº†æ··åˆä¸“å®¶(MoE)ç½‘ç»œæ¥è€¦åˆæ¨¡å‹çº§ä¸æŸ¥è¯¢çº§çš„åµŒå…¥è¡¨ç¤ºã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡å‹è·¯ç”±å’ŒåŸºå‡†å‡†ç¡®ç‡é¢„æµ‹ä»»åŠ¡ä¸­å‡å–å¾—äº†æœ€å…ˆè¿›(SOTA)çš„æ€§èƒ½ã€‚åˆ†æè¿›ä¸€æ­¥éªŒè¯äº†æ‰€å­¦å‚æ•°èƒ½å¤Ÿæœ‰æ•ˆç¼–ç å…³äºæ¨¡å‹èƒ½åŠ›å’ŒæŸ¥è¯¢ç‰¹æ€§çš„å¯è§£é‡Šä¿¡æ¯ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00844v1",
      "published_date": "2025-10-01 12:55:34 UTC",
      "updated_date": "2025-10-01 12:55:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:00:39.971994+00:00"
    },
    {
      "arxiv_id": "2510.00837v1",
      "title": "Feature Identification for Hierarchical Contrastive Learning",
      "title_zh": "é¢å‘å±‚çº§å¯¹æ¯”å­¦ä¹ çš„ç‰¹å¾è¯†åˆ«",
      "authors": [
        "Julius Ott",
        "Nastassia Vysotskaya",
        "Huawei Sun",
        "Lorenzo Servadei",
        "Robert Wille"
      ],
      "abstract": "Hierarchical classification is a crucial task in many applications, where objects are organized into multiple levels of categories. However, conventional classification approaches often neglect inherent inter-class relationships at different hierarchy levels, thus missing important supervisory signals. Thus, we propose two novel hierarchical contrastive learning (HMLC) methods. The first, leverages a Gaussian Mixture Model (G-HMLC) and the second uses an attention mechanism to capture hierarchy-specific features (A-HMLC), imitating human processing. Our approach explicitly models inter-class relationships and imbalanced class distribution at higher hierarchy levels, enabling fine-grained clustering across all hierarchy levels. On the competitive CIFAR100 and ModelNet40 datasets, our method achieves state-of-the-art performance in linear evaluation, outperforming existing hierarchical contrastive learning methods by 2 percentage points in terms of accuracy. The effectiveness of our approach is backed by both quantitative and qualitative results, highlighting its potential for applications in computer vision and beyond.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åˆ†å±‚åˆ†ç±»(Hierarchical classification)ä¸­ä¼ ç»Ÿæ–¹æ³•å¾€å¾€å¿½ç•¥ä¸åŒå±‚çº§é—´å›ºæœ‰ç±»åˆ«å…³ç³»çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸¤ç§æ–°å‹çš„åˆ†å±‚å¯¹æ¯”å­¦ä¹ (Hierarchical contrastive learning, HMLC)æ–¹æ³•ã€‚ç¬¬ä¸€ç§æ–¹æ³• G-HMLC åˆ©ç”¨é«˜æ–¯æ··åˆæ¨¡å‹(Gaussian Mixture Model)è¿›è¡Œå»ºæ¨¡ï¼Œè€Œç¬¬äºŒç§æ–¹æ³• A-HMLC åˆ™å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶(Attention mechanism)æ¥æ•æ‰ç‰¹å®šå±‚çº§çš„ç‰¹å¾ã€‚è¯¥æ–¹æ¡ˆé€šè¿‡æ˜¾å¼å»ºæ¨¡é«˜å±‚çº§çš„ç±»åˆ«å…³ç³»å’Œç±»åˆ«åˆ†å¸ƒä¸å¹³è¡¡ï¼Œå®ç°äº†è·¨æ‰€æœ‰å±‚çº§çš„ç»†ç²’åº¦èšç±»(Fine-grained clustering)ã€‚åœ¨ CIFAR100 å’Œ ModelNet40 æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº† SOTA æ€§èƒ½ï¼Œå…¶çº¿æ€§è¯„ä¼°å‡†ç¡®ç‡æ¯”ç°æœ‰çš„åˆ†å±‚å¯¹æ¯”å­¦ä¹ æ–¹æ³•é«˜å‡º 2 ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™é¡¹ç ”ç©¶å……åˆ†è¯æ˜äº†è¯¥æ–¹æ³•åœ¨è®¡ç®—æœºè§†è§‰(Computer Vision)åŠç›¸å…³é¢†åŸŸåº”ç”¨ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Submitted to ICASSP 2026",
      "pdf_url": "https://arxiv.org/pdf/2510.00837v1",
      "published_date": "2025-10-01 12:46:47 UTC",
      "updated_date": "2025-10-01 12:46:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:00:47.076732+00:00"
    },
    {
      "arxiv_id": "2510.00836v1",
      "title": "Improving Cryptocurrency Pump-and-Dump Detection through Ensemble-Based Models and Synthetic Oversampling Techniques",
      "title_zh": "åŸºäºé›†æˆæ¨¡å‹ä¸åˆæˆè¿‡é‡‡æ ·æŠ€æœ¯æ”¹è¿›åŠ å¯†è´§å¸â€œæ‹‰é«˜å‡ºè´§â€æ£€æµ‹",
      "authors": [
        "Jieun Yu",
        "Minjung Park",
        "Sangmi Chai"
      ],
      "abstract": "This study aims to detect pump and dump (P&D) manipulation in cryptocurrency markets, where the scarcity of such events causes severe class imbalance and hinders accurate detection. To address this issue, the Synthetic Minority Oversampling Technique (SMOTE) was applied, and advanced ensemble learning models were evaluated to distinguish manipulative trading behavior from normal market activity. The experimental results show that applying SMOTE greatly enhanced the ability of all models to detect P&D events by increasing recall and improving the overall balance between precision and recall. In particular, XGBoost and LightGBM achieved high recall rates (94.87% and 93.59%, respectively) with strong F1-scores and demonstrated fast computational performance, making them suitable for near real time surveillance. These findings indicate that integrating data balancing techniques with ensemble methods significantly improves the early detection of manipulative activities, contributing to a fairer, more transparent, and more stable cryptocurrency market.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ—¨åœ¨æ£€æµ‹åŠ å¯†è´§å¸å¸‚åœºä¸­çš„æ‹‰é«˜æŠ›å”®(Pump and Dump)æ“çºµè¡Œä¸ºï¼Œé’ˆå¯¹æ­¤ç±»äº‹ä»¶ç¨€å°‘å¯¼è‡´çš„ä¸¥é‡ç±»åˆ«ä¸å¹³è¡¡(class imbalance)é—®é¢˜å±•å¼€æ¢è®¨ã€‚ä½œè€…åº”ç”¨äº†äººå·¥å°‘æ•°ç±»è¿‡é‡‡æ ·æŠ€æœ¯(SMOTE)æ¥å¹³è¡¡æ•°æ®é›†ï¼Œå¹¶è¯„ä¼°äº†å¤šç§å…ˆè¿›çš„é›†æˆå­¦ä¹ (ensemble learning)æ¨¡å‹ä»¥åŒºåˆ†æ“çºµæ€§äº¤æ˜“ä¸æ­£å¸¸å¸‚åœºæ´»åŠ¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¼•å…¥SMOTEæ˜¾è‘—æå‡äº†æ‰€æœ‰æ¨¡å‹å¯¹Pump and Dumpäº‹ä»¶çš„æ£€æµ‹èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æé«˜å¬å›ç‡(recall)ä»¥åŠå¹³è¡¡ç²¾ç¡®ç‡(precision)ä¸å¬å›ç‡æ–¹é¢æ•ˆæœæ˜¾è‘—ã€‚å…¶ä¸­ï¼ŒXGBoostå’ŒLightGBMè¡¨ç°å°¤ä¸ºå‡ºè‰²ï¼Œåˆ†åˆ«å®ç°äº†94.87%å’Œ93.59%çš„é«˜å¬å›ç‡ï¼Œå¹¶ä¿æŒäº†å¼ºå¤§çš„F1-scoresã€‚è¿™äº›æ¨¡å‹å±•ç¤ºäº†æå¿«çš„è®¡ç®—æ€§èƒ½ï¼Œä½¿å…¶èƒ½å¤Ÿèƒœä»»è¿‘ä¹å®æ—¶çš„å¸‚åœºç›‘æ§éœ€æ±‚ã€‚è¯¥ç ”ç©¶è¯æ˜ï¼Œå°†æ•°æ®å¹³è¡¡æŠ€æœ¯ä¸é›†æˆæ–¹æ³•ç›¸ç»“åˆå¯å¤§å¹…æå‡å¯¹æ“çºµæ´»åŠ¨çš„æ—©æœŸæ£€æµ‹èƒ½åŠ›ï¼Œæœ‰åŠ©äºæ„å»ºä¸€ä¸ªæ›´å…¬å¹³ã€é€æ˜ä¸”ç¨³å®šçš„åŠ å¯†è´§å¸å¸‚åœºã€‚",
      "categories": [
        "cs.AI",
        "cs.CE",
        "q-fin.RM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00836v1",
      "published_date": "2025-10-01 12:46:45 UTC",
      "updated_date": "2025-10-01 12:46:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:00:46.477763+00:00"
    },
    {
      "arxiv_id": "2510.00833v2",
      "title": "Towards Verifiable Federated Unlearning: Framework, Challenges, and The Road Ahead",
      "title_zh": "è¿ˆå‘å¯éªŒè¯çš„è”é‚¦é—å¿˜ï¼šæ¡†æ¶ã€æŒ‘æˆ˜ä¸æœªæ¥å±•æœ›",
      "authors": [
        "Thanh Linh Nguyen",
        "Marcela Tuler de Oliveira",
        "An Braeken",
        "Aaron Yi Ding",
        "Quoc-Viet Pham"
      ],
      "abstract": "Federated unlearning (FUL) enables removing the data influence from the model trained across distributed clients, upholding the right to be forgotten as mandated by privacy regulations. FUL facilitates a value exchange where clients gain privacy-preserving control over their data contributions, while service providers leverage decentralized computing and data freshness. However, this entire proposition is undermined because clients have no reliable way to verify that their data influence has been provably removed, as current metrics and simple notifications offer insufficient assurance. We envision unlearning verification becoming a pivotal and trust-by-design part of the FUL life-cycle development, essential for highly regulated and data-sensitive services and applications like healthcare. This article introduces veriFUL, a reference framework for verifiable FUL that formalizes verification entities, goals, approaches, and metrics. Specifically, we consolidate existing efforts and contribute new insights, concepts, and metrics to this domain. Finally, we highlight research challenges and identify potential applications and developments for verifiable FUL and veriFUL. This article aims to provide a comprehensive resource for researchers and practitioners to navigate and advance the field of verifiable FUL.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Federated Unlearning (FUL) åœ¨ç»´æŠ¤ç”¨æˆ·â€œè¢«é—å¿˜æƒâ€æ–¹é¢çš„é‡è¦æ€§ï¼Œå¹¶é’ˆå¯¹å½“å‰å®¢æˆ·ç«¯æ— æ³•æœ‰æ•ˆéªŒè¯æ•°æ®å½±å“æ˜¯å¦è¢«çœŸå®ç§»é™¤çš„å›°å¢ƒï¼Œæå‡ºäº†åä¸º veriFUL çš„å¯éªŒè¯ FUL å‚è€ƒæ¡†æ¶ã€‚veriFUL é€šè¿‡å¯¹éªŒè¯å®ä½“ã€ç›®æ ‡ã€æ–¹æ³•å’ŒæŒ‡æ ‡è¿›è¡Œå½¢å¼åŒ–å®šä¹‰ï¼Œæ—¨åœ¨å°†é—å¿˜éªŒè¯è½¬å˜ä¸º FUL ç”Ÿå‘½å‘¨æœŸä¸­â€œè®¾è®¡å³ä¿¡ä»»â€çš„å…³é”®ç¯èŠ‚ï¼Œå°¤å…¶é€‚ç”¨äºåŒ»ç–—ä¿å¥ç­‰é«˜ç›‘ç®¡å’Œæ•°æ®æ•æ„Ÿé¢†åŸŸã€‚æ–‡ç« ä¸ä»…æ•´åˆäº†ç°æœ‰ç ”ç©¶ï¼Œè¿˜è´¡çŒ®äº†å…³äºå¯éªŒè¯æ€§çš„æ–°è§è§£ã€æ¦‚å¿µå’Œåº¦é‡æ ‡å‡†ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯¦ç»†æ¢³ç†äº†è¯¥é¢†åŸŸé¢ä¸´çš„æŠ€æœ¯æŒ‘æˆ˜ï¼Œå¹¶è¯†åˆ«äº† veriFUL çš„æ½œåœ¨åº”ç”¨åœºæ™¯ä¸æœªæ¥å‘å±•è·¯å¾„ã€‚è¯¥å·¥ä½œä¸ºå­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œåœ¨æ¨è¿›å¯éªŒè¯ Federated Unlearning é¢†åŸŸæä¾›äº†å…¨é¢çš„æŒ‡å¯¼èµ„æºã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "Accepted in IEEE Internet Computing",
      "pdf_url": "https://arxiv.org/pdf/2510.00833v2",
      "published_date": "2025-10-01 12:45:46 UTC",
      "updated_date": "2026-01-19 15:56:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:00:54.669542+00:00"
    },
    {
      "arxiv_id": "2510.00831v1",
      "title": "Benchmarking Machine Learning Models for Fault Classification and Localization in Power System Protection",
      "title_zh": "ç”µåŠ›ç³»ç»Ÿä¿æŠ¤ä¸­æ•…éšœåˆ†ç±»ä¸å®šä½çš„æœºå™¨å­¦ä¹ æ¨¡å‹åŸºå‡†æµ‹è¯•",
      "authors": [
        "Julian Oelhaf",
        "Georg Kordowich",
        "Changhun Kim",
        "Paula Andrea PÃ©rez-Toro",
        "Christian Bergler",
        "Andreas Maier",
        "Johann JÃ¤ger",
        "Siming Bayer"
      ],
      "abstract": "The increasing integration of distributed energy resources (DERs), particularly renewables, poses significant challenges for power system protection, with fault classification (FC) and fault localization (FL) being among the most critical tasks. Conventional protection schemes, based on fixed thresholds, cannot reliably identify and localize short circuits with the increasing complexity of the grid under dynamic conditions. Machine learning (ML) offers a promising alternative; however, systematic benchmarks across models and settings remain limited. This work presents, for the first time, a comparative benchmarking study of classical ML models for FC and FL in power system protection based on EMT data. Using voltage and current waveforms segmented into sliding windows of 10 ms to 50 ms, we evaluate models under realistic real-time constraints. Performance is assessed in terms of accuracy, robustness to window size, and runtime efficiency. The best-performing FC model achieved an F1 score of 0.992$\\pm$0.001, while the top FL model reached an R2 of 0.806$\\pm$0.008 with a mean processing time of 0.563 ms.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹åˆ†å¸ƒå¼èƒ½æº(DERs)ç‰¹åˆ«æ˜¯å¯å†ç”Ÿèƒ½æºé›†æˆç»™ç”µåŠ›ç³»ç»Ÿä¿æŠ¤å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œé‡ç‚¹æ¢è®¨äº†æ•…éšœåˆ†ç±»(FC)å’Œæ•…éšœå®šä½(FL)çš„å…³é”®ä»»åŠ¡ã€‚ç”±äºä¼ ç»Ÿçš„å›ºå®šé˜ˆå€¼æ–¹æ¡ˆåœ¨å¤æ‚åŠ¨æ€ç”µç½‘ä¸­éš¾ä»¥å¯é è¯†åˆ«çŸ­è·¯ï¼Œè¯¥å·¥ä½œé¦–æ¬¡å¯¹åŸºäºç”µç£æš‚æ€(EMT)æ•°æ®çš„ç»å…¸æœºå™¨å­¦ä¹ (ML)æ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿæ€§çš„åŸºå‡†æµ‹è¯•å¯¹æ¯”ã€‚ç ”ç©¶äººå‘˜å°†ç”µå‹å’Œç”µæµæ³¢å½¢åˆ†å‰²ä¸º10msè‡³50msçš„æ»‘åŠ¨çª—å£ï¼Œåœ¨ç°å®çš„å®æ—¶çº¦æŸä¸‹è¯„ä¼°äº†æ¨¡å‹åœ¨å‡†ç¡®æ€§ã€çª—å£é²æ£’æ€§å’Œè¿è¡Œæ•ˆç‡æ–¹é¢çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¡¨ç°æœ€ä¼˜çš„FCæ¨¡å‹è¾¾åˆ°äº†0.992çš„F1åˆ†æ•°ï¼Œè€Œé¡¶å°–çš„FLæ¨¡å‹å®ç°äº†0.806çš„R2å€¼ã€‚è¯¥ç ”ç©¶è¯æ˜äº†MLæ¨¡å‹åœ¨å¹³å‡å¤„ç†æ—¶é—´ä»…ä¸º0.563msçš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿä¸ºç”µåŠ›ç³»ç»Ÿæä¾›é«˜æ€§èƒ½ä¸”å®æ—¶çš„ä¿æŠ¤æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "cs.AI",
      "comment": "Submitted to ICASSP 2026; under review",
      "pdf_url": "https://arxiv.org/pdf/2510.00831v1",
      "published_date": "2025-10-01 12:44:14 UTC",
      "updated_date": "2025-10-01 12:44:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:00:56.570249+00:00"
    },
    {
      "arxiv_id": "2510.00821v1",
      "title": "Logical Consistency Between Disagreeing Experts and Its Role in AI Safety",
      "title_zh": "å­˜åœ¨åˆ†æ­§çš„ä¸“å®¶é—´çš„é€»è¾‘ä¸€è‡´æ€§åŠå…¶åœ¨ AI å®‰å…¨ä¸­çš„ä½œç”¨",
      "authors": [
        "AndrÃ©s Corrada-Emmanuel"
      ],
      "abstract": "If two experts disagree on a test, we may conclude both cannot be 100 per cent correct. But if they completely agree, no possible evaluation can be excluded. This asymmetry in the utility of agreements versus disagreements is explored here by formalizing a logic of unsupervised evaluation for classifiers. Its core problem is computing the set of group evaluations that are logically consistent with how we observe them agreeing and disagreeing in their decisions. Statistical summaries of their aligned decisions are inputs into a Linear Programming problem in the integer space of possible correct or incorrect responses given true labels. Obvious logical constraints, such as, the number of correct responses cannot exceed the number of observed responses, are inequalities. But in addition, there are axioms, universally applicable linear equalities that apply to all finite tests. The practical and immediate utility of this approach to unsupervised evaluation using only logical consistency is demonstrated by building no-knowledge alarms that can detect when one or more LLMs-as-Judges are violating a minimum grading threshold specified by the user.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¸“å®¶æ„è§åˆ†æ­§ä¸ä¸€è‡´æ€§åœ¨è¯„ä¼°ä¸­çš„ä¸å¯¹ç§°æ€§ï¼Œæ­£å¼æå‡ºäº†ä¸€ç§ç”¨äºåˆ†ç±»å™¨æ— ç›‘ç£è¯„ä¼°(unsupervised evaluation)çš„é€»è¾‘æ¡†æ¶ã€‚å…¶æ ¸å¿ƒä»»åŠ¡æ˜¯è®¡ç®—ä¸è§‚å¯Ÿåˆ°çš„å†³ç­–åˆ†æ­§åŠä¸€è‡´æ€§åœ¨é€»è¾‘ä¸Šç›¸å®¹çš„ç¾¤ä½“è¯„ä»·é›†åˆï¼Œé€šè¿‡å°†å¯¹é½å†³ç­–çš„ç»Ÿè®¡æ‘˜è¦è½¬åŒ–ä¸ºæ•´æ•°ç©ºé—´å†…çš„çº¿æ€§è§„åˆ’(Linear Programming)é—®é¢˜ã€‚è¯¥æ¡†æ¶ä¸ä»…åˆ©ç”¨äº†æ˜¾å¼çš„é€»è¾‘çº¦æŸï¼Œè¿˜å¼•å…¥äº†é€‚ç”¨äºæ‰€æœ‰æœ‰é™æµ‹è¯•çš„é€šç”¨å…¬ç†å’Œçº¿æ€§ç­‰å¼ã€‚é€šè¿‡æ„å»ºâ€œæ— çŸ¥è¯†æŠ¥è­¦å™¨â€(no-knowledge alarms)ï¼Œç ”ç©¶è¯æ˜äº†è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ£€æµ‹å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºè£åˆ¤(LLMs-as-Judges)åœ¨ä½•ç§æƒ…å†µä¸‹è¿åäº†ç”¨æˆ·è®¾å®šçš„æœ€ä½è¯„åˆ†é˜ˆå€¼ã€‚è¿™ä¸€åŸºäºé€»è¾‘ä¸€è‡´æ€§çš„æ–¹æ¡ˆä¸ºæå‡AIå®‰å…¨æ€§æä¾›äº†ä¸€ç§æ— éœ€çœŸå®æ ‡ç­¾çš„å®ç”¨è¯„ä¼°æ‰‹æ®µã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.00821v1",
      "published_date": "2025-10-01 12:30:01 UTC",
      "updated_date": "2025-10-01 12:30:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:00:58.079250+00:00"
    },
    {
      "arxiv_id": "2510.00819v1",
      "title": "Stabilizing Policy Gradients for Sample-Efficient Reinforcement Learning in LLM Reasoning",
      "title_zh": "é¢å‘å¤§è¯­è¨€æ¨¡å‹æ¨ç†çš„ç­–ç•¥æ¢¯åº¦ç¨³å®šåŒ–ï¼šå®ç°æ ·æœ¬é«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Luckeciano C. Melo",
        "Alessandro Abate",
        "Yarin Gal"
      ],
      "abstract": "Reinforcement Learning, particularly through policy gradient methods, has played a central role in enabling reasoning capabilities of Large Language Models. However, the optimization stability of policy gradients in this setting remains understudied. As a result, existing implementations often resort to conservative hyperparameter choices to ensure stability, which requires more training samples and increases computational costs. Hence, developing models for reliably tracking the underlying optimization dynamics and leveraging them into training enables more sample-efficient regimes and further unleashes scalable post-training. We address this gap by formalizing the stochastic optimization problem of policy gradients with explicit consideration of second-order geometry. We propose a tractable computational framework that tracks and leverages curvature information during policy updates. We further employ this framework to design interventions in the optimization process through data selection. The resultant algorithm, Curvature-Aware Policy Optimization (CAPO), identifies samples that contribute to unstable updates and masks them out. Theoretically, we establish monotonic improvement guarantees under realistic assumptions. On standard math reasoning benchmarks, we empirically show that CAPO ensures stable updates under aggressive learning regimes where baselines catastrophically fail. With minimal intervention (rejecting fewer than 8% of tokens), CAPO achieves up to 30x improvement in sample efficiency over standard GRPO for LLM reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹æ¨ç†ï¼ˆLLM Reasoningï¼‰ä¸­ç­–ç•¥æ¢¯åº¦ï¼ˆPolicy Gradientsï¼‰ä¼˜åŒ–ä¸ç¨³å®šæ€§å¯¼è‡´é‡‡æ ·æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæå‡ºäº†Curvature-Aware Policy Optimizationï¼ˆCAPOï¼‰ç®—æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å½¢å¼åŒ–è€ƒè™‘äºŒé˜¶å‡ ä½•ï¼ˆSecond-order Geometryï¼‰çš„éšæœºä¼˜åŒ–é—®é¢˜ï¼Œæ„å»ºäº†ä¸€ä¸ªèƒ½å¤Ÿå®æ—¶è·Ÿè¸ªå¹¶åˆ©ç”¨æ›²ç‡ä¿¡æ¯ï¼ˆCurvature Informationï¼‰çš„è®¡ç®—æ¡†æ¶ã€‚CAPOåˆ©ç”¨è¯¥æ¡†æ¶åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­è¿›è¡Œæ•°æ®é€‰æ‹©ï¼Œè¯†åˆ«å¹¶è¿‡æ»¤æ‰ä¼šå¯¼è‡´æ›´æ–°ä¸ç¨³å®šçš„æ ·æœ¬ï¼Œä»è€Œåœ¨åŸºå‡†æ¨¡å‹å¤±æ•ˆçš„æ¿€è¿›å­¦ä¹ é…ç½®ä¸‹ç¡®ä¿è®­ç»ƒçš„ç¨³å®šæ€§ã€‚ç†è®ºåˆ†æè¯æ˜äº†è¯¥ç®—æ³•åœ¨ç°å®å‡è®¾ä¸‹å…·æœ‰å•è°ƒæå‡ä¿è¯ï¼ˆMonotonic Improvement Guaranteesï¼‰ã€‚åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒCAPOä»…éœ€é€šè¿‡æå°çš„å¹²é¢„ï¼ˆæ‹’ç»å°‘äº8%çš„Tokenï¼‰ï¼Œå³å¯åœ¨æ ·æœ¬æ•ˆç‡ä¸Šå®ç°æ¯”æ ‡å‡†GRPOç®—æ³•é«˜è¾¾30å€çš„æå‡ï¼Œä¸ºå¯æ‰©å±•çš„è®­ç»ƒåä¼˜åŒ–æä¾›äº†æ›´é«˜æ•ˆçš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00819v1",
      "published_date": "2025-10-01 12:29:32 UTC",
      "updated_date": "2025-10-01 12:29:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:01:12.516255+00:00"
    },
    {
      "arxiv_id": "2510.00817v2",
      "title": "Semantic Bridges Between First Order c-Representations and Cost-Based Semantics: An Initial Perspective",
      "title_zh": "ä¸€é˜¶ c-è¡¨ç¤ºä¸åŸºäºæˆæœ¬è¯­ä¹‰çš„è¯­ä¹‰æ¡¥æ¢ï¼šåˆæ­¥è§†è§’",
      "authors": [
        "Nicholas Leisegang",
        "Giovanni Casini",
        "Thomas Meyer"
      ],
      "abstract": "Weighted-knowledge bases and cost-based semantics represent a recent formalism introduced by Bienvenu et al. for Ontology Mediated Data Querying in the case where a given knowledge base is inconsistent. This is done by adding a weight to each statement in the knowledge base (KB), and then giving each DL interpretation a cost based on how often it breaks rules in the KB. In this paper we compare this approach with c-representations, a form of non-monotonic reasoning originally introduced by Kern-Isberner. c-Representations describe a means to interpret defeasible concept inclusions in the first-order case. This is done by assigning a numerical ranking to each interpretations via penalties for each violated conditional. We compare these two approaches on a semantic level. In particular, we show that under certain conditions a weighted knowledge base and a set of defeasible conditionals can generate the same ordering on interpretations, and therefore an equivalence of semantic structures up to relative cost. Moreover, we compare entailment described in both cases, where certain notions are equivalently expressible in both formalisms. Our results have the potential to benefit further work on both cost-based semantics and c-representations",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŠ æƒçŸ¥è¯†åº“ï¼ˆWeighted-knowledge basesï¼‰åŠå…¶ç›¸å…³çš„åŸºäºæˆæœ¬çš„è¯­ä¹‰ï¼ˆcost-based semanticsï¼‰ä¸ä¸€é˜¶c-representationsä¹‹é—´çš„è¯­ä¹‰è”ç³»ã€‚åŸºäºæˆæœ¬çš„è¯­ä¹‰é€šè¿‡ä¸ºé™ˆè¿°åˆ†é…æƒé‡æ¥å¤„ç†ä¸ä¸€è‡´çŸ¥è¯†åº“ä¸­çš„æœ¬ä½“ä¸­ä»‹æ•°æ®æŸ¥è¯¢ï¼ˆOntology Mediated Data Queryingï¼‰ï¼Œè€Œc-representationsåˆ™åˆ©ç”¨ç½šåˆ†æœºåˆ¶å¯¹è¿èƒŒæ¡ä»¶å¥çš„è§£é‡Šè¿›è¡Œæ•°å€¼æ’åºã€‚é€šè¿‡åœ¨è¯­ä¹‰å±‚é¢å¯¹æ¯”è¿™ä¸¤ç§æ–¹æ³•ï¼Œç ”ç©¶è¯æ˜åœ¨ç‰¹å®šæ¡ä»¶ä¸‹ï¼ŒåŠ æƒçŸ¥è¯†åº“ä¸ä¸€ç»„å¯åºŸæ­¢æ¡ä»¶å¥ï¼ˆdefeasible conditionalsï¼‰èƒ½å¤Ÿç”Ÿæˆç›¸åŒçš„è§£é‡Šæ’åºï¼Œä»è€Œåœ¨ç›¸å¯¹æˆæœ¬ä¸Šå®ç°è¯­ä¹‰ç»“æ„çš„ç­‰ä»·æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜åˆ†æäº†ä¸¤è€…åœ¨è•´å«ï¼ˆentailmentï¼‰å®šä¹‰ä¸Šçš„ç›¸ä¼¼æ€§ï¼ŒæŒ‡å‡ºæŸäº›æ¦‚å¿µåœ¨ä¸¤ç§å½¢å¼åŒ–æ¡†æ¶ä¸‹å…·æœ‰ç­‰ä»·çš„è¡¨è¾¾æ–¹å¼ã€‚è¿™äº›æˆæœä¸ºåç»­ç»“åˆéå•è°ƒæ¨ç†ä¸ä¸ä¸€è‡´çŸ¥è¯†åº“å¤„ç†çš„ç ”ç©¶æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00817v2",
      "published_date": "2025-10-01 12:27:19 UTC",
      "updated_date": "2025-10-02 09:38:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:01:14.467874+00:00"
    },
    {
      "arxiv_id": "2510.00808v1",
      "title": "What You See is What You Ask: Evaluating Audio Descriptions",
      "title_zh": "æ‰€è§å³æ‰€é—®ï¼šéŸ³é¢‘æè¿°è¯„ä¼°",
      "authors": [
        "Divy Kala",
        "Eshika Khandelwal",
        "Makarand Tapaswi"
      ],
      "abstract": "Audio descriptions (ADs) narrate important visual details in movies, enabling Blind and Low Vision (BLV) users to understand narratives and appreciate visual details. Existing works in automatic AD generation mostly focus on few-second trimmed clips, and evaluate them by comparing against a single ground-truth reference AD. However, writing ADs is inherently subjective. Through alignment and analysis of two independent AD tracks for the same movies, we quantify the subjectivity in when and whether to describe, and what and how to highlight. Thus, we show that working with trimmed clips is inadequate. We propose ADQA, a QA benchmark that evaluates ADs at the level of few-minute long, coherent video segments, testing whether they would help BLV users understand the story and appreciate visual details. ADQA features visual appreciation (VA) questions about visual facts and narrative understanding (NU) questions based on the plot. Through ADQA, we show that current AD generation methods lag far behind human-authored ADs. We conclude with several recommendations for future work and introduce a public leaderboard for benchmarking.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¸ºç›²äººåŠä½è§†åŠ›(BLV)ç”¨æˆ·è®¾è®¡çš„éŸ³é¢‘æè¿°(Audio Descriptions, ADs)è¯„ä¼°é—®é¢˜ï¼ŒæŒ‡å‡ºå½“å‰ç ”ç©¶è¿‡åº¦é›†ä¸­äºçŸ­ç‰‡æ®µä¸”è¯„ä¼°æ–¹å¼å› ä¸»è§‚æ€§è€Œå—é™ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ADQAï¼Œä¸€ç§åœ¨æ•°åˆ†é’Ÿé•¿çš„è¿è´¯è§†é¢‘ç‰‡æ®µç»´åº¦ä¸Šè¯„ä¼°ADæ€§èƒ½çš„QAåŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†åŒ…å«é’ˆå¯¹è§†è§‰äº‹å®çš„è§†è§‰æ¬£èµ(Visual Appreciation, VA)é—®é¢˜å’ŒåŸºäºå‰§æƒ…çš„å™äº‹ç†è§£(Narrative Understanding, NU)é—®é¢˜ã€‚ç ”ç©¶é€šè¿‡å¯¹ä¸¤ç»„ç‹¬ç«‹ADè½¨é“çš„é‡åŒ–åˆ†æï¼Œæ­ç¤ºäº†æè¿°æ—¶æœºä¸å†…å®¹é€‰æ‹©çš„é«˜åº¦ä¸»è§‚æ€§ï¼Œå¹¶å‘ç°å½“å‰çš„è‡ªåŠ¨ADç”Ÿæˆæ–¹æ³•åœ¨æ€§èƒ½ä¸Šä»è¿œè½åäºäººå·¥æè¿°ã€‚è¯¥è®ºæ–‡æœ€åä¸ºæœªæ¥çš„ADç ”ç©¶æ–¹å‘æä¾›äº†å»ºè®®ï¼Œå¹¶å‘å¸ƒäº†ç”¨äºåŸºå‡†æµ‹è¯•çš„å…¬å…±æ’è¡Œæ¦œã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "EMNLP 2025 Main Track Long Paper",
      "pdf_url": "https://arxiv.org/pdf/2510.00808v1",
      "published_date": "2025-10-01 12:14:15 UTC",
      "updated_date": "2025-10-01 12:14:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:01:16.905757+00:00"
    },
    {
      "arxiv_id": "2510.00805v2",
      "title": "MG2FlowNet: Accelerating High-Reward Sample Generation via Enhanced MCTS and Greediness Control",
      "title_zh": "MG2FlowNetï¼šé€šè¿‡å¢å¼ºå‹ MCTS ä¸è´ªå©ªåº¦æ§åˆ¶åŠ é€Ÿé«˜å¥–åŠ±æ ·æœ¬ç”Ÿæˆ",
      "authors": [
        "Rui Zhu",
        "Xuan Yu",
        "Yudong Zhang",
        "Chen Zhang",
        "Xu Wang",
        "Yang Wang"
      ],
      "abstract": "Generative Flow Networks (GFlowNets) have emerged as a powerful tool for generating diverse and high-reward structured objects by learning to sample from a distribution proportional to a given reward function. Unlike conventional reinforcement learning (RL) approaches that prioritize optimization of a single trajectory, GFlowNets seek to balance diversity and reward by modeling the entire trajectory distribution. This capability makes them especially suitable for domains such as molecular design and combinatorial optimization. However, existing GFlowNets sampling strategies tend to overexplore and struggle to consistently generate high-reward samples, particularly in large search spaces with sparse high-reward regions. Therefore, improving the probability of generating high-reward samples without sacrificing diversity remains a key challenge under this premise. In this work, we integrate an enhanced Monte Carlo Tree Search (MCTS) into the GFlowNets sampling process, using MCTS-based policy evaluation to guide the generation toward high-reward trajectories and Polynomial Upper Confidence Trees (PUCT) to balance exploration and exploitation adaptively, and we introduce a controllable mechanism to regulate the degree of greediness. Our method enhances exploitation without sacrificing diversity by dynamically balancing exploration and reward-driven guidance. The experimental results show that our method can not only accelerate the speed of discovering high-reward regions but also continuously generate high-reward samples, while preserving the diversity of the generative distribution. All implementations are available at https://github.com/ZRNB/MG2FlowNet.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆæµç½‘ç»œ(Generative Flow Networks, GFlowNets)åœ¨å¤§è§„æ¨¡ç¨€ç–å¥–åŠ±ç©ºé—´ä¸­éš¾ä»¥æŒç»­ç”Ÿæˆé«˜å¥–åŠ±æ ·æœ¬çš„é—®é¢˜ï¼Œæå‡ºäº†MG2FlowNetæ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†å¢å¼ºçš„è’™ç‰¹å¡æ´›æ ‘æœç´¢(Monte Carlo Tree Search, MCTS)é›†æˆåˆ°é‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œåˆ©ç”¨åŸºäºMCTSçš„ç­–ç•¥è¯„ä¼°å¼•å¯¼æ¨¡å‹å‘é«˜å¥–åŠ±è½¨è¿¹æœç´¢ã€‚ç ”ç©¶åŒæ—¶å¼•å…¥äº†å¤šé¡¹å¼ä¸Šç½®ä¿¡ç•Œæ ‘(Polynomial Upper Confidence Trees, PUCT)æ¥å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§å¯æ§çš„è´ªå©ªåº¦è°ƒèŠ‚æœºåˆ¶ä»¥åŠ¨æ€ä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMG2FlowNetåœ¨ä¸ç‰ºç‰²ç”Ÿæˆåˆ†å¸ƒå¤šæ ·æ€§çš„å‰æä¸‹ï¼Œèƒ½å¤Ÿæ˜¾è‘—åŠ å¿«é«˜å¥–åŠ±åŒºåŸŸçš„å‘ç°é€Ÿåº¦å¹¶æŒç»­äº§å‡ºé«˜è´¨é‡æ ·æœ¬ã€‚è¯¥å·¥ä½œä¸ºåˆ†å­è®¾è®¡(Molecular Design)å’Œç»„åˆä¼˜åŒ–(Combinatorial Optimization)ç­‰é¢†åŸŸçš„ç»“æ„åŒ–å¯¹è±¡ç”Ÿæˆæä¾›äº†æ›´é«˜æ•ˆä¸”å…¼é¡¾å¤šæ ·æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00805v2",
      "published_date": "2025-10-01 12:09:04 UTC",
      "updated_date": "2025-10-04 15:33:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:01:17.875432+00:00"
    },
    {
      "arxiv_id": "2510.00799v1",
      "title": "Fast, Secure, and High-Capacity Image Watermarking with Autoencoded Text Vectors",
      "title_zh": "åŸºäºè‡ªç¼–ç æ–‡æœ¬å‘é‡çš„å¿«é€Ÿã€å®‰å…¨åŠé«˜å®¹é‡å›¾åƒæ°´å°",
      "authors": [
        "Gautier Evennou",
        "Vivien Chappelier",
        "Ewa Kijak"
      ],
      "abstract": "Most image watermarking systems focus on robustness, capacity, and imperceptibility while treating the embedded payload as meaningless bits. This bit-centric view imposes a hard ceiling on capacity and prevents watermarks from carrying useful information. We propose LatentSeal, which reframes watermarking as semantic communication: a lightweight text autoencoder maps full-sentence messages into a compact 256-dimensional unit-norm latent vector, which is robustly embedded by a finetuned watermark model and secured through a secret, invertible rotation. The resulting system hides full-sentence messages, decodes in real time, and survives valuemetric and geometric attacks. It surpasses prior state of the art in BLEU-4 and Exact Match on several benchmarks, while breaking through the long-standing 256-bit payload ceiling. It also introduces a statistically calibrated score that yields a ROC AUC score of 0.97-0.99, and practical operating points for deployment. By shifting from bit payloads to semantic latent vectors, LatentSeal enables watermarking that is not only robust and high-capacity, but also secure and interpretable, providing a concrete path toward provenance, tamper explanation, and trustworthy AI governance. Models, training and inference code, and data splits will be available upon publication.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LatentSealï¼Œé€šè¿‡å°†å›¾åƒæ°´å°é‡æ–°å®šä¹‰ä¸ºè¯­ä¹‰é€šä¿¡(semantic communication)ï¼Œè§£å†³äº†ä¼ ç»Ÿæ°´å°ç³»ç»Ÿè´Ÿè½½å®¹é‡å—é™ä¸”è´Ÿè½½ä¿¡æ¯ç¼ºä¹å®ç”¨ä»·å€¼çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è½»é‡çº§æ–‡æœ¬è‡ªåŠ¨ç¼–ç å™¨(text autoencoder)å°†å…¨å¥æ¶ˆæ¯æ˜ å°„ä¸º256ç»´çš„å•ä½èŒƒæ•°æ½œå‘é‡(latent vector)ï¼Œé€šè¿‡å¾®è°ƒçš„æ°´å°æ¨¡å‹è¿›è¡Œç¨³å¥åµŒå…¥ï¼Œå¹¶é‡‡ç”¨ç§˜å¯†çš„å¯é€†æ—‹è½¬(invertible rotation)ç¡®ä¿å®‰å…¨æ€§ã€‚LatentSeal æ”¯æŒå…¨å¥ä¿¡æ¯çš„éšè—ä¸å®æ—¶è§£ç ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæŠµå¾¡å„ç§ä»·å€¼å’Œå‡ ä½•æ”»å‡»ï¼Œåœ¨ BLEU-4 å’Œç²¾ç¡®åŒ¹é…(Exact Match)æŒ‡æ ‡ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ï¼ŒæˆåŠŸæ‰“ç ´äº†é•¿æœŸå­˜åœ¨çš„ 256 ä½æœ‰æ•ˆè´Ÿè½½é™åˆ¶ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿå¼•å…¥äº†ç»Ÿè®¡æ ¡å‡†è¯„åˆ†å¹¶å®ç°äº† 0.97-0.99 çš„ ROC AUC è¡¨ç°ï¼Œä¸ºå†…å®¹æº¯æº(provenance)ã€ç¯¡æ”¹è§£é‡Šå’Œå¯ä¿¡ AI æ²»ç†æä¾›äº†å…·ä½“çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2510.00799v1",
      "published_date": "2025-10-01 11:56:40 UTC",
      "updated_date": "2025-10-01 11:56:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:01:26.509902+00:00"
    },
    {
      "arxiv_id": "2510.00797v1",
      "title": "Solar PV Installation Potential Assessment on Building Facades Based on Vision and Language Foundation Models",
      "title_zh": "åŸºäºè§†è§‰ä¸è¯­è¨€åŸºç¡€æ¨¡å‹çš„å»ºç­‘ç«‹é¢å¤ªé˜³èƒ½å…‰ä¼å®‰è£…æ½œåŠ›è¯„ä¼°",
      "authors": [
        "Ruyu Liu",
        "Dongxu Zhuang",
        "Jianhua Zhang",
        "Arega Getaneh Abate",
        "Per Sieverts Nielsen",
        "Ben Wang",
        "Xiufeng Liu"
      ],
      "abstract": "Building facades represent a significant untapped resource for solar energy generation in dense urban environments, yet assessing their photovoltaic (PV) potential remains challenging due to complex geometries and semantic com ponents. This study introduces SF-SPA (Semantic Facade Solar-PV Assessment), an automated framework that transforms street-view photographs into quantitative PV deployment assessments. The approach combines com puter vision and artificial intelligence techniques to address three key challenges: perspective distortion correction, semantic understanding of facade elements, and spatial reasoning for PV layout optimization. Our four-stage pipeline processes images through geometric rectification, zero-shot semantic segmentation, Large Language Model (LLM) guided spatial reasoning, and energy simulation. Validation across 80 buildings in four countries demonstrates ro bust performance with mean area estimation errors of 6.2% &#177; 2.8% compared to expert annotations. The auto mated assessment requires approximately 100 seconds per building, a substantial gain in efficiency over manual methods. Simulated energy yield predictions confirm the method's reliability and applicability for regional poten tial studies, urban energy planning, and building-integrated photovoltaic (BIPV) deployment. Code is available at: https:github.com/CodeAXu/Solar-PV-Installation",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜å¯†åº¦åŸå¸‚ç¯å¢ƒä¸­å»ºç­‘ç«‹é¢å…‰ä¼(PV)æ½œåŠ›è¯„ä¼°é¢ä¸´çš„å‡ ä½•å¤æ‚æ€§å’Œè¯­ä¹‰ç»„ä»¶è¯†åˆ«éš¾é¢˜ï¼Œæå‡ºäº†SF-SPA (Semantic Facade Solar-PV Assessment)è‡ªåŠ¨åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è®¡ç®—æœºè§†è§‰å’Œäººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œé€šè¿‡å‡ ä½•æ ¡æ­£ã€é›¶æ ·æœ¬è¯­ä¹‰åˆ†å‰²(zero-shot semantic segmentation)ã€å¤§è¯­è¨€æ¨¡å‹(LLM)å¼•å¯¼çš„ç©ºé—´æ¨ç†ä»¥åŠèƒ½æºæ¨¡æ‹Ÿå››ä¸ªé˜¶æ®µï¼Œå°†è¡—æ™¯ç…§ç‰‡è½¬åŒ–ä¸ºé‡åŒ–çš„å…‰ä¼éƒ¨ç½²è¯„ä¼°æ•°æ®ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆè§£å†³äº†è§†è§’å¤±çœŸæ ¡æ­£ã€ç«‹é¢å…ƒç´ è¯­ä¹‰ç†è§£å’Œå…‰ä¼å¸ƒå±€ä¼˜åŒ–ç­‰å…³é”®æŒ‘æˆ˜ï¼Œåœ¨å¯¹å››ä¸ªå›½å®¶80æ ‹å»ºç­‘çš„éªŒè¯ä¸­ï¼Œå…¶é¢ç§¯ä¼°ç®—çš„å¹³å‡è¯¯å·®ä»…ä¸º6.2% Â± 2.8%ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿå•æ ‹å»ºç­‘å¤„ç†æ—¶é—´ä»…éœ€çº¦100ç§’ï¼Œæ•ˆç‡è¾ƒä¼ ç»Ÿæ‰‹åŠ¨æ–¹æ³•æ˜¾è‘—æå‡ã€‚è¯¥ç ”ç©¶ä¸ºåŒºåŸŸèƒ½æºæ½œåŠ›è¯„ä¼°ã€åŸå¸‚è§„åˆ’ä»¥åŠå…‰ä¼å»ºç­‘ä¸€ä½“åŒ–(BIPV)çš„éƒ¨ç½²æä¾›äº†å¯é çš„æŠ€æœ¯æ”¯æ’‘å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00797v1",
      "published_date": "2025-10-01 11:51:28 UTC",
      "updated_date": "2025-10-01 11:51:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:01:26.669947+00:00"
    },
    {
      "arxiv_id": "2510.00796v1",
      "title": "MetaLogic: Robustness Evaluation of Text-to-Image Models via Logically Equivalent Prompts",
      "title_zh": "MetaLogicï¼šåŸºäºé€»è¾‘ç­‰ä»·æç¤ºè¯çš„æ–‡ç”Ÿå›¾æ¨¡å‹é²æ£’æ€§è¯„ä¼°",
      "authors": [
        "Yifan Shen",
        "Yangyang Shu",
        "Hye-young Paik",
        "Yulei Sui"
      ],
      "abstract": "Recent advances in text-to-image (T2I) models, especially diffusion-based architectures, have significantly improved the visual quality of generated images. However, these models continue to struggle with a critical limitation: maintaining semantic consistency when input prompts undergo minor linguistic variations. Despite being logically equivalent, such prompt pairs often yield misaligned or semantically inconsistent images, exposing a lack of robustness in reasoning and generalisation. To address this, we propose MetaLogic, a novel evaluation framework that detects T2I misalignment without relying on ground truth images. MetaLogic leverages metamorphic testing, generating image pairs from prompts that differ grammatically but are semantically identical. By directly comparing these image pairs, the framework identifies inconsistencies that signal failures in preserving the intended meaning, effectively diagnosing robustness issues in the model's logic understanding. Unlike existing evaluation methods that compare a generated image to a single prompt, MetaLogic evaluates semantic equivalence between paired images, offering a scalable, ground-truth-free approach to identifying alignment failures. It categorises these alignment errors (e.g., entity omission, duplication, positional misalignment) and surfaces counterexamples that can be used for model debugging and refinement. We evaluate MetaLogic across multiple state-of-the-art T2I models and reveal consistent robustness failures across a range of logical constructs. We find that even the SOTA text-to-image models like Flux.dev and DALLE-3 demonstrate a 59 percent and 71 percent misalignment rate, respectively. Our results show that MetaLogic is not only efficient and scalable, but also effective in uncovering fine-grained logical inconsistencies that are overlooked by existing evaluation metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MetaLogicï¼Œä¸€ç§æ—¨åœ¨è¯„ä¼°æ–‡æœ¬ç”Ÿæˆå›¾åƒ (Text-to-Image, T2I) æ¨¡å‹é²æ£’æ€§çš„æ–°å‹è¯„ä¼°æ¡†æ¶ã€‚é’ˆå¯¹æ¨¡å‹åœ¨é¢å¯¹è¯­æ³•ä¸åŒä½†è¯­ä¹‰é€»è¾‘ç­‰ä»· (logically equivalent) çš„æç¤ºè¯æ—¶éš¾ä»¥ä¿æŒä¸€è‡´æ€§çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†å˜æ€æµ‹è¯• (metamorphic testing) æŠ€æœ¯ï¼Œé€šè¿‡ç›´æ¥æ¯”è¾ƒç”±é€»è¾‘ç­‰ä»·æç¤ºè¯ç”Ÿæˆçš„å›¾åƒå¯¹æ¥æ£€æµ‹å¯¹é½å¤±æ•ˆ (misalignment)ã€‚MetaLogic æ— éœ€ä¾èµ–åœ°é¢çœŸå€¼ (ground truth) å›¾åƒï¼Œèƒ½å¤Ÿè‡ªåŠ¨è¯†åˆ«å¹¶åˆ†ç±»å®ä½“é—æ¼ã€é‡å¤åŠä½ç½®é”™ä½ç­‰é€»è¾‘ç†è§£é”™è¯¯ï¼Œä¸ºæ¨¡å‹è°ƒè¯•æä¾›äº†ç»†ç²’åº¦çš„åé¦ˆã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œå³ä½¿æ˜¯ Flux.dev å’Œ DALLE-3 ç­‰æœ€å…ˆè¿›çš„ SOTA æ¨¡å‹ï¼Œä¹Ÿåˆ†åˆ«è¡¨ç°å‡º 59% å’Œ 71% çš„é€»è¾‘ä¸ä¸€è‡´ç‡ã€‚ç ”ç©¶ç»“æœè¯æ˜ MetaLogic èƒ½å¤Ÿæœ‰æ•ˆæ­ç¤ºç°æœ‰è¯„ä¼°æŒ‡æ ‡éš¾ä»¥å‘ç°çš„é€»è¾‘ç¼ºé™·ï¼Œä¸ºæå‡ç”Ÿæˆå¼æ¨¡å‹çš„æ¨ç†ä¸æ³›åŒ–èƒ½åŠ›æä¾›äº†æœ‰åŠ›å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ICFEM 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.00796v1",
      "published_date": "2025-10-01 11:51:13 UTC",
      "updated_date": "2025-10-01 11:51:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:01:28.266716+00:00"
    },
    {
      "arxiv_id": "2510.00795v1",
      "title": "Benchmarking Agentic Systems in Automated Scientific Information Extraction with ChemX",
      "title_zh": "ChemXï¼šè‡ªåŠ¨åŒ–ç§‘å­¦ä¿¡æ¯æå–ä¸­æ™ºèƒ½ä½“ç³»ç»Ÿçš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Anastasia Vepreva",
        "Julia Razlivina",
        "Maria Eremeeva",
        "Nina Gubina",
        "Anastasia Orlova",
        "Aleksei Dmitrenko",
        "Ksenya Kapranova",
        "Susan Jyakhwo",
        "Nikita Vasilev",
        "Arsen Sarkisyan",
        "Ivan Yu. Chernyshov",
        "Vladimir Vinogradov",
        "Andrei Dmitrenko"
      ],
      "abstract": "The emergence of agent-based systems represents a significant advancement in artificial intelligence, with growing applications in automated data extraction. However, chemical information extraction remains a formidable challenge due to the inherent heterogeneity of chemical data. Current agent-based approaches, both general-purpose and domain-specific, exhibit limited performance in this domain. To address this gap, we present ChemX, a comprehensive collection of 10 manually curated and domain-expert-validated datasets focusing on nanomaterials and small molecules. These datasets are designed to rigorously evaluate and enhance automated extraction methodologies in chemistry. To demonstrate their utility, we conduct an extensive benchmarking study comparing existing state-of-the-art agentic systems such as ChatGPT Agent and chemical-specific data extraction agents. Additionally, we introduce our own single-agent approach that enables precise control over document preprocessing prior to extraction. We further evaluate the performance of modern baselines, such as GPT-5 and GPT-5 Thinking, to compare their capabilities with agentic approaches. Our empirical findings reveal persistent challenges in chemical information extraction, particularly in processing domain-specific terminology, complex tabular and schematic representations, and context-dependent ambiguities. The ChemX benchmark serves as a critical resource for advancing automated information extraction in chemistry, challenging the generalization capabilities of existing methods, and providing valuable insights into effective evaluation strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ–å­¦ä¿¡æ¯æå–ç”±äºæ•°æ®å¼‚æ„æ€§è€Œé¢ä¸´çš„ä¸¥å³»æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºå½“å‰é€šç”¨çš„å’Œé¢†åŸŸç‰¹å®šçš„æ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å¤„ç†åŒ–å­¦æ•°æ®æ—¶è¡¨ç°æœ‰é™ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº† ChemXï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«10ä¸ªç”±é¢†åŸŸä¸“å®¶éªŒè¯çš„æ‰‹åŠ¨æ•´ç†æ•°æ®é›†çš„ç»¼åˆé›†åˆï¼Œä¸“æ³¨äºçº³ç±³ææ–™å’Œå°åˆ†å­é¢†åŸŸï¼Œæ—¨åœ¨è¯„ä¼°å¹¶å¢å¼ºåŒ–å­¦é¢†åŸŸçš„è‡ªåŠ¨åŒ–æå–æ–¹æ³•ã€‚ç ”ç©¶åˆ©ç”¨ ChemX å¯¹ ChatGPT Agent ç­‰å°–ç«¯ Agentic Systems ä»¥åŠåŒ–å­¦ç‰¹å®šæ•°æ®æå–æ™ºèƒ½ä½“è¿›è¡Œäº†å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§èƒ½å¤Ÿç²¾ç¡®æ§åˆ¶æ–‡æ¡£é¢„å¤„ç†çš„å•æ™ºèƒ½ä½“æ–¹æ³•ã€‚é€šè¿‡è¯„ä¼° GPT-5 å’Œ GPT-5 Thinking ç­‰ç°ä»£åŸºçº¿æ¨¡å‹ï¼Œå®éªŒæ­ç¤ºäº†åœ¨å¤„ç†é¢†åŸŸç‰¹å®šæœ¯è¯­ã€å¤æ‚è¡¨æ ¼å’Œå›¾ç¤ºä»¥åŠä¸Šä¸‹æ–‡æ­§ä¹‰æ–¹é¢çš„æŒç»­æŒ‘æˆ˜ã€‚ChemX åŸºå‡†æµ‹è¯•ä¸ºæ¨åŠ¨è‡ªåŠ¨åŒ–åŒ–å­¦ä¿¡æ¯æå–æä¾›äº†å…³é”®èµ„æºï¼Œåœ¨æŒ‘æˆ˜ç°æœ‰æ–¹æ³•æ³›åŒ–èƒ½åŠ›çš„åŒæ—¶ï¼Œä¹Ÿä¸ºåˆ¶å®šæœ‰æ•ˆçš„è¯„ä¼°ç­–ç•¥æä¾›äº†å®è´µè§è§£ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at The AI for Accelerated Materials Discovery (AI4Mat) Workshop, NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.00795v1",
      "published_date": "2025-10-01 11:50:11 UTC",
      "updated_date": "2025-10-01 11:50:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:01:30.574635+00:00"
    },
    {
      "arxiv_id": "2510.00793v1",
      "title": "AI in data science education: experiences from the classroom",
      "title_zh": "æ•°æ®ç§‘å­¦æ•™è‚²ä¸­çš„äººå·¥æ™ºèƒ½ï¼šæ¥è‡ªè¯¾å ‚çš„ç»éªŒ",
      "authors": [
        "J. A. Hageman",
        "C. F. W. Peeters"
      ],
      "abstract": "This study explores the integration of AI, particularly large language models (LLMs) like ChatGPT, into educational settings, focusing on the implications for teaching and learning. Through interviews with course coordinators from data science courses at Wageningen University, this research identifies both the benefits and challenges associated with AI in the classroom. While AI tools can streamline tasks and enhance learning, concerns arise regarding students' overreliance on these technologies, potentially hindering the development of essential cognitive and problem solving skills. The study highlights the importance of responsible AI usage, ethical considerations, and the need for adapting assessment methods to ensure educational outcomes are met. With careful integration, AI can be a valuable asset in education, provided it is used to complement rather than replace fundamental learning processes.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½(AI)ï¼Œç‰¹åˆ«æ˜¯ChatGPTç­‰å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ•™è‚²ç¯å¢ƒä¸­çš„é›†æˆï¼Œé‡ç‚¹åˆ†æäº†å…¶å¯¹æ•™å­¦å’Œå­¦ä¹ çš„å½±å“ã€‚ç ”ç©¶äººå‘˜é€šè¿‡å¯¹ç“¦èµ«å®æ ¹å¤§å­¦(Wageningen University)æ•°æ®ç§‘å­¦è¯¾ç¨‹çš„è¯¾ç¨‹åè°ƒå‘˜è¿›è¡Œè®¿è°ˆï¼Œè¯†åˆ«äº†åœ¨è¯¾å ‚ä¸­ä½¿ç”¨AIå¸¦æ¥çš„æ”¶ç›Šä¸æŒ‘æˆ˜ã€‚ç»“æœæ˜¾ç¤ºï¼ŒAIå·¥å…·è™½ç„¶èƒ½å¤Ÿç®€åŒ–ä»»åŠ¡å¹¶å¢å¼ºå­¦ä¹ ä½“éªŒï¼Œä½†åŒæ—¶ä¹Ÿå¼•å‘äº†å¯¹å­¦ç”Ÿè¿‡åº¦ä¾èµ–è¿™äº›æŠ€æœ¯çš„æ‹…å¿§ï¼Œè®¤ä¸ºè¿™å¯èƒ½é˜»ç¢å­¦ç”Ÿæ ¸å¿ƒè®¤çŸ¥èƒ½åŠ›å’Œé—®é¢˜è§£å†³èƒ½åŠ›(Problem solving skills)çš„å‘å±•ã€‚ç ”ç©¶å¼ºè°ƒäº†è´Ÿè´£ä»»åœ°ä½¿ç”¨AIã€ä¼¦ç†è€ƒé‡ä»¥åŠè°ƒæ•´è¯„ä¼°æ–¹æ³•(Assessment methods)ä»¥ç¡®ä¿æ•™å­¦ç›®æ ‡è¾¾æˆçš„å¿…è¦æ€§ã€‚æœ€ç»ˆæŒ‡å‡ºï¼Œåªè¦AIè¢«å®šä½ä¸ºè¡¥å……è€Œéæ›¿ä»£åŸºç¡€å­¦ä¹ è¿‡ç¨‹ï¼Œå®ƒå°†æˆä¸ºæ•™è‚²é¢†åŸŸçš„å®è´µèµ„äº§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "6 pages, 0 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.00793v1",
      "published_date": "2025-10-01 11:45:25 UTC",
      "updated_date": "2025-10-01 11:45:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:01:32.975034+00:00"
    },
    {
      "arxiv_id": "2511.08587v1",
      "title": "Conversational Agents for Building Energy Efficiency -- Advising Housing Cooperatives in Stockholm on Reducing Energy Consumption",
      "title_zh": "æå‡å»ºç­‘èƒ½æ•ˆçš„å¯¹è¯å¼æ™ºèƒ½ä½“ï¼šé’ˆå¯¹ Stockholm ä½æˆ¿åˆä½œç¤¾é™ä½èƒ½è€—çš„å’¨è¯¢å»ºè®®",
      "authors": [
        "Shadaab Ghani",
        "Anne HÃ¥kansson",
        "Oleksii Pasichnyi",
        "Hossein Shahrokni"
      ],
      "abstract": "Housing cooperative is a common type of multifamily building ownership in Sweden. Although this ownership structure grants decision-making autonomy, it places a burden of responsibility on cooperative's board members. Most board members lack the resources or expertise to manage properties and their energy consumption. This ignorance presents a unique challenge, especially given the EU directives that prohibit buildings rated as energy classes F and G by 2033. Conversational agents (CAs) enable human-like interactions with computer systems, facilitating human-computer interaction across various domains. In our case, CAs can be implemented to support cooperative members in making informed energy retrofitting and usage decisions. This paper introduces a Conversational agent system, called SPARA, designed to advise cooperatives on energy efficiency. SPARA functions as an energy efficiency advisor by leveraging the Retrieval-Augmented Generation (RAG) framework with a Language Model(LM). The LM generates targeted recommendations based on a knowledge base composed of email communications between professional energy advisors and cooperatives' representatives in Stockholm. The preliminary results indicate that SPARA can provide energy efficiency advice with precision 80\\%, comparable to that of municipal energy efficiency (EE) experts. A pilot implementation is currently underway, where municipal EE experts are evaluating SPARA performance based on questions posed to EE experts by BRF members. Our findings suggest that LMs can significantly improve outreach by supporting stakeholders in their energy transition. For future work, more research is needed to evaluate this technology, particularly limitations to the stability and trustworthiness of its energy efficiency advice.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç‘å…¸å¤šæˆ·ä½å®…åˆä½œç¤¾åœ¨å»ºç­‘èƒ½æºç®¡ç†æ–¹é¢ç¼ºä¹ä¸“ä¸šçŸ¥è¯†çš„ç°çŠ¶ï¼Œç‰¹åˆ«æ˜¯åœ¨åº”å¯¹æ¬§ç›Ÿæœ‰å…³æé«˜èƒ½æ•ˆç­‰çº§æŒ‡ä»¤çš„èƒŒæ™¯ä¸‹ï¼Œå¼€å‘äº†åä¸ºSPARAçš„Conversational Agentç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨Retrieval-Augmented Generation (RAG)æ¡†æ¶ç»“åˆLanguage Model (LM)ï¼Œé€šè¿‡åˆ†ææ–¯å¾·å“¥å°”æ‘©ä¸“ä¸šèƒ½æºé¡¾é—®ä¸åˆä½œç¤¾ä»£è¡¨ä¹‹é—´çš„çœŸå®é€šä¿¡è®°å½•ï¼Œä¸ºç”¨æˆ·æä¾›é’ˆå¯¹æ€§çš„èŠ‚èƒ½æ”¹é€ å»ºè®®ã€‚åˆæ­¥å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSPARAæä¾›å»ºè®®çš„å‡†ç¡®ç‡è¾¾åˆ°80%ï¼Œå…¶è¡¨ç°å¯ä¸å¸‚æ”¿Energy Efficiency (EE)ä¸“å®¶ç›¸åª²ç¾ã€‚ç›®å‰è¯¥ç³»ç»Ÿå·²è¿›å…¥è¯•ç‚¹é˜¶æ®µï¼Œç ”ç©¶ç»“æœè¡¨æ˜Language Modelèƒ½å¤Ÿæ˜¾è‘—æå‡èƒ½æºè½¬å‹è¿‡ç¨‹ä¸­çš„åˆ©ç›Šç›¸å…³è€…è§¦è¾¾æ•ˆç‡ã€‚æœªæ¥ç ”ç©¶å°†è¿›ä¸€æ­¥è¯„ä¼°è¯¥æŠ€æœ¯åœ¨æä¾›Energy Efficiencyå»ºè®®æ—¶çš„ç¨³å®šæ€§å’Œå¯ä¿¡åº¦ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08587v1",
      "published_date": "2025-10-01 11:40:11 UTC",
      "updated_date": "2025-10-01 11:40:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:01:42.364045+00:00"
    },
    {
      "arxiv_id": "2510.00778v1",
      "title": "DIA: The Adversarial Exposure of Deterministic Inversion in Diffusion Models",
      "title_zh": "DIAï¼šæ‰©æ•£æ¨¡å‹ä¸­ç¡®å®šæ€§åæ¼”çš„å¯¹æŠ—æ€§æš´éœ²",
      "authors": [
        "Seunghoo Hong",
        "Geonho Son",
        "Juhun Lee",
        "Simon S. Woo"
      ],
      "abstract": "Diffusion models have shown to be strong representation learners, showcasing state-of-the-art performance across multiple domains. Aside from accelerated sampling, DDIM also enables the inversion of real images back to their latent codes. A direct inheriting application of this inversion operation is real image editing, where the inversion yields latent trajectories to be utilized during the synthesis of the edited image. Unfortunately, this practical tool has enabled malicious users to freely synthesize misinformative or deepfake contents with greater ease, which promotes the spread of unethical and abusive, as well as privacy-, and copyright-infringing contents. While defensive algorithms such as AdvDM and Photoguard have been shown to disrupt the diffusion process on these images, the misalignment between their objectives and the iterative denoising trajectory at test time results in weak disruptive performance.In this work, we present the DDIM Inversion Attack (DIA) that attacks the integrated DDIM trajectory path. Our results support the effective disruption, surpassing previous defensive methods across various editing methods. We believe that our frameworks and results can provide practical defense methods against the malicious use of AI for both the industry and the research community. Our code is available here: https://anonymous.4open.science/r/DIA-13419/.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ‰©æ•£æ¨¡å‹(Diffusion models)ä¸­DDIMåè½¬(Inversion)æŠ€æœ¯åœ¨å›¾åƒç¼–è¾‘ä¸­çš„åŒåˆƒå‰‘æ•ˆåº”ï¼ŒæŒ‡å‡ºå…¶æ˜“è¢«ç”¨äºåˆ¶é€ æ·±åº¦ä¼ªé€ (deepfake)å’Œè™šå‡ä¿¡æ¯ç­‰ä¼¦ç†é£é™©ã€‚é’ˆå¯¹ç°æœ‰é˜²å¾¡ç®—æ³•å¦‚AdvDMå’ŒPhotoguardç”±äºç›®æ ‡ä¸è¿­ä»£å»å™ªè½¨è¿¹(denoising trajectory)ä¸åŒ¹é…è€Œå¯¼è‡´çš„æ€§èƒ½ç¼ºé™·ï¼Œä½œè€…æå‡ºäº†DDIM Inversion Attack (DIA)ã€‚DIAé€šè¿‡ç›´æ¥æ”»å‡»é›†æˆçš„DDIMè½¨è¿¹è·¯å¾„(trajectory path)ï¼Œå®ç°äº†å¯¹åè½¬è¿‡ç¨‹çš„é«˜æ•ˆå¹²æ‰°ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒDIAåœ¨å¤šç§å›¾åƒç¼–è¾‘æ–¹æ³•ä¸Šçš„ç ´åæ•ˆæœå‡æ˜¾è‘—ä¼˜äºä»¥å¾€çš„é˜²å¾¡æ‰‹æ®µï¼Œèƒ½æœ‰æ•ˆé˜»æ­¢é’ˆå¯¹åŸå›¾çš„æ¶æ„ç¯¡æ”¹ã€‚è¯¥æ¡†æ¶ä¸ºåº”å¯¹äººå·¥æ™ºèƒ½çš„æ»¥ç”¨æä¾›äº†å®ç”¨çš„æŠ€æœ¯æ”¯æ’‘ï¼Œå¯¹ä¿æŠ¤æ•°å­—å›¾åƒçš„ç‰ˆæƒä¸çœŸå®æ€§å…·æœ‰é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "ICCV2025",
      "pdf_url": "https://arxiv.org/pdf/2510.00778v1",
      "published_date": "2025-10-01 11:20:03 UTC",
      "updated_date": "2025-10-01 11:20:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:01:53.567752+00:00"
    },
    {
      "arxiv_id": "2510.00773v1",
      "title": "Uncertainty-Aware Concept Bottleneck Models with Enhanced Interpretability",
      "title_zh": "å¢å¼ºå¯è§£é‡Šæ€§çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ¦‚å¿µç“¶é¢ˆæ¨¡å‹",
      "authors": [
        "Haifei Zhang",
        "Patrick Barry",
        "Eduardo Brandao"
      ],
      "abstract": "In the context of image classification, Concept Bottleneck Models (CBMs) first embed images into a set of human-understandable concepts, followed by an intrinsically interpretable classifier that predicts labels based on these intermediate representations. While CBMs offer a semantically meaningful and interpretable classification pipeline, they often sacrifice predictive performance compared to end-to-end convolutional neural networks. Moreover, the propagation of uncertainty from concept predictions to final label decisions remains underexplored. In this paper, we propose a novel uncertainty-aware and interpretable classifier for the second stage of CBMs. Our method learns a set of binary class-level concept prototypes and uses the distances between predicted concept vectors and each class prototype as both a classification score and a measure of uncertainty. These prototypes also serve as interpretable classification rules, indicating which concepts should be present in an image to justify a specific class prediction. The proposed framework enhances both interpretability and robustness by enabling conformal prediction for uncertain or outlier inputs based on their deviation from the learned binary class-level concept prototypes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾åƒåˆ†ç±»ä¸­çš„æ¦‚å¿µç“¶é¢ˆæ¨¡å‹(Concept Bottleneck Models, CBMs)åœ¨é¢„æµ‹æ€§èƒ½ä¸‹é™åŠä¸ç¡®å®šæ€§ä¼ æ’­ç ”ç©¶ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„å…·æœ‰ä¸ç¡®å®šæ€§æ„ŸçŸ¥èƒ½åŠ›çš„è§£é‡Šæ€§åˆ†ç±»å™¨ã€‚è¯¥æ–¹æ³•ä¸ºCBMsçš„ç¬¬äºŒé˜¶æ®µå­¦ä¹ äº†ä¸€ç»„äºŒå…ƒç±»åˆ«çº§æ¦‚å¿µåŸå‹(binary class-level concept prototypes)ï¼Œé€šè¿‡è®¡ç®—é¢„æµ‹æ¦‚å¿µå‘é‡ä¸ç±»åˆ«åŸå‹ä¹‹é—´çš„è·ç¦»ï¼Œå°†å…¶åŒæ—¶ä½œä¸ºåˆ†ç±»å¾—åˆ†å’Œä¸ç¡®å®šæ€§åº¦é‡ã€‚è¿™äº›åŸå‹ä¸ä»…ä½œä¸ºå¯è§£é‡Šçš„åˆ†ç±»è§„åˆ™ï¼ŒæŒ‡æ˜äº†ç‰¹å®šç±»åˆ«é¢„æµ‹æ‰€éœ€çš„å…³é”®æ¦‚å¿µï¼Œè¿˜é€šè¿‡ç¬¦åˆæ€§é¢„æµ‹(conformal prediction)æå‡äº†ç³»ç»Ÿå¯¹ä¸ç¡®å®šæˆ–å¼‚å¸¸è¾“å…¥å¤„ç†çš„é²æ£’æ€§ã€‚è¯¥æ¡†æ¶åœ¨å¢å¼ºæ¨¡å‹å¯è§£é‡Šæ€§çš„åŒæ—¶ï¼Œé€šè¿‡é‡åŒ–æ¦‚å¿µé¢„æµ‹åˆ°æ ‡ç­¾å†³ç­–çš„åå·®ï¼Œæœ‰æ•ˆæå‡äº†CBMsåœ¨å¤æ‚å†³ç­–åœºæ™¯ä¸­çš„å¯é æ€§ä¸é€æ˜åº¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper has been accepted for the Workshop AIMLAI at ECML-PKDD 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.00773v1",
      "published_date": "2025-10-01 11:11:18 UTC",
      "updated_date": "2025-10-01 11:11:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:01:58.089841+00:00"
    },
    {
      "arxiv_id": "2510.00771v1",
      "title": "UniverSR: Unified and Versatile Audio Super-Resolution via Vocoder-Free Flow Matching",
      "title_zh": "UniverSRï¼šåŸºäºæ— å£°ç å™¨æµåŒ¹é…çš„ç»Ÿä¸€é€šç”¨éŸ³é¢‘è¶…åˆ†è¾¨ç‡",
      "authors": [
        "Woongjib Choi",
        "Sangmin Lee",
        "Hyungseob Lim",
        "Hong-Goo Kang"
      ],
      "abstract": "In this paper, we present a vocoder-free framework for audio super-resolution that employs a flow matching generative model to capture the conditional distribution of complex-valued spectral coefficients. Unlike conventional two-stage diffusion-based approaches that predict a mel-spectrogram and then rely on a pre-trained neural vocoder to synthesize waveforms, our method directly reconstructs waveforms via the inverse Short-Time Fourier Transform (iSTFT), thereby eliminating the dependence on a separate vocoder. This design not only simplifies end-to-end optimization but also overcomes a critical bottleneck of two-stage pipelines, where the final audio quality is fundamentally constrained by vocoder performance. Experiments show that our model consistently produces high-fidelity 48 kHz audio across diverse upsampling factors, achieving state-of-the-art performance on both speech and general audio datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†UniverSRï¼Œä¸€ç§åŸºäºFlow Matchingç”Ÿæˆçš„éŸ³é¢‘è¶…åˆ†è¾¨ç‡(Audio Super-Resolution)ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ— Vocoderçš„è®¾è®¡å®ç°é«˜è´¨é‡éŸ³é¢‘é‡å»ºã€‚è¯¥æ¨¡å‹ç›´æ¥å­¦ä¹ å¤æ•°å€¼è°±ç³»æ•°(complex-valued spectral coefficients)çš„æ¡ä»¶åˆ†å¸ƒï¼Œå¹¶åˆ©ç”¨é€†çŸ­æ—¶å‚…é‡Œå¶å˜æ¢(iSTFT)ç›´æ¥è¿˜åŸæ³¢å½¢ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿä¸¤é˜¶æ®µæ–¹æ³•ä¸­Vocoderå¸¦æ¥çš„æ€§èƒ½ç“¶é¢ˆã€‚è¿™ç§ç«¯åˆ°ç«¯çš„ä¼˜åŒ–æ–¹å¼ä¸ä»…ç®€åŒ–äº†ç³»ç»Ÿæµç¨‹ï¼Œè¿˜æ˜¾è‘—æå‡äº†ç”ŸæˆéŸ³é¢‘çš„ä¿çœŸåº¦ã€‚å®éªŒè¯æ˜ï¼ŒUniverSRåœ¨å¤šç§è¶…é‡‡æ ·å€ç‡ä¸‹å‡èƒ½ç¨³å®šäº§å‡º48 kHzçš„é«˜ä¿çœŸéŸ³é¢‘ï¼Œåœ¨è¯­éŸ³åŠé€šç”¨éŸ³é¢‘æ•°æ®é›†ä¸Šå‡å–å¾—äº†SOTAæ€§èƒ½ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD",
        "eess.SP"
      ],
      "primary_category": "eess.AS",
      "comment": "Submitted to ICASSP 2026",
      "pdf_url": "https://arxiv.org/pdf/2510.00771v1",
      "published_date": "2025-10-01 11:04:53 UTC",
      "updated_date": "2025-10-01 11:04:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:02:01.176219+00:00"
    },
    {
      "arxiv_id": "2510.00766v1",
      "title": "Multi-Objective Task-Aware Predictor for Image-Text Alignment",
      "title_zh": "é¢å‘å›¾æ–‡å¯¹é½çš„å¤šç›®æ ‡ä»»åŠ¡æ„ŸçŸ¥é¢„æµ‹å™¨",
      "authors": [
        "Eunki Kim",
        "Na Min An",
        "James Thorne",
        "Hyunjung Shim"
      ],
      "abstract": "Evaluating image-text alignment while reflecting human preferences across multiple aspects is a significant issue for the development of reliable vision-language applications. It becomes especially crucial in real-world scenarios where multiple valid descriptions exist depending on contexts or user needs. However, research progress is hindered by the lack of comprehensive benchmarks and existing evaluation predictors lacking at least one of these key properties: (1) Alignment with human judgments, (2) Long-sequence processing, (3) Inference efficiency, and (4) Applicability to multi-objective scoring. To address these challenges, we propose a plug-and-play architecture to build a robust predictor, MULTI-TAP (Multi-Objective Task-Aware Predictor), capable of both multi and single-objective scoring. MULTI-TAP can produce a single overall score, utilizing a reward head built on top of a large vision-language model (LVLMs). We show that MULTI-TAP is robust in terms of application to different LVLM architectures, achieving significantly higher performance than existing metrics and even on par with the GPT-4o-based predictor, G-VEval, with a smaller size (7-8B). By training a lightweight ridge regression layer on the frozen hidden states of a pre-trained LVLM, MULTI-TAP can produce fine-grained scores for multiple human-interpretable objectives. MULTI-TAP performs better than VisionREWARD, a high-performing multi-objective reward model, in both performance and efficiency on multi-objective benchmarks and our newly released text-image-to-text dataset, EYE4ALL. Our new dataset, consisting of chosen/rejected human preferences (EYE4ALLPref) and human-annotated fine-grained scores across seven dimensions (EYE4ALLMulti), can serve as a foundation for developing more accessible AI systems by capturing the underlying preferences of users, including blind and low-vision (BLV) individuals.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾åƒ-æ–‡æœ¬å¯¹é½(Image-Text Alignment)è¯„ä¼°ä¸­éš¾ä»¥å…¨é¢åæ˜ äººç±»å¤šç»´åº¦åå¥½ï¼Œä»¥åŠç°æœ‰é¢„æµ‹å™¨åœ¨æ¨ç†æ•ˆç‡å’Œå¤šç›®æ ‡è¯„åˆ†(Multi-Objective Scoring)æ–¹é¢å­˜åœ¨å±€é™æ€§çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†MULTI-TAP (Multi-Objective Task-Aware Predictor)ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)çš„å³æ’å³ç”¨æ¶æ„ï¼Œé€šè¿‡åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„éšè—çŠ¶æ€ä¸Šè®­ç»ƒè½»é‡çº§å²­å›å½’å±‚(Ridge Regression Layer)ï¼Œå®ç°ç»†ç²’åº¦çš„å¤šç›®æ ‡è¯„åˆ†ã€‚å®éªŒè¡¨æ˜ï¼Œ7-8Bå‚æ•°è§„æ¨¡çš„MULTI-TAPåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¸Šå‡ä¼˜äºVisionREWARDï¼Œå¹¶è¾¾åˆ°äº†ä¸åŸºäºGPT-4oçš„G-VEvalç›¸å½“çš„æ°´å¹³ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å‘å¸ƒäº†åŒ…å«å¤šç»´åº¦äººå·¥æ ‡æ³¨è¯„åˆ†çš„EYE4ALLæ•°æ®é›†ï¼Œæ—¨åœ¨æ›´å¥½åœ°æ•æ‰åŒ…æ‹¬è§†éšœäººå£«(BLV)åœ¨å†…çš„å¹¿æ³›ç”¨æˆ·åå¥½ã€‚è¯¥å·¥ä½œä¸ºå¼€å‘æ›´å¯é ä¸”ç¬¦åˆäººç±»åå¥½çš„è§†è§‰è¯­è¨€åº”ç”¨æä¾›äº†å¼ºæœ‰åŠ›çš„è¯„ä¼°å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "28 pages, 10 figures, 21 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.00766v1",
      "published_date": "2025-10-01 10:55:33 UTC",
      "updated_date": "2025-10-01 10:55:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:02:12.482060+00:00"
    },
    {
      "arxiv_id": "2510.00743v1",
      "title": "From Scores to Preferences: Redefining MOS Benchmarking for Speech Quality Reward Modeling",
      "title_zh": "ä»è¯„åˆ†åˆ°åå¥½ï¼šé‡å¡‘è¯­éŸ³è´¨é‡å¥–åŠ±å»ºæ¨¡çš„ MOS è¯„æµ‹åŸºå‡†",
      "authors": [
        "Yifei Cao",
        "Changhao Jiang",
        "Jiabao Zhuang",
        "Jiajun Sun",
        "Ming Zhang",
        "Zhiheng Xi",
        "Hui Li",
        "Shihan Dou",
        "Yuran Wang",
        "Yunke Zhang",
        "Tao Ji",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "abstract": "Assessing the perceptual quality of synthetic speech is crucial for guiding the development and refinement of speech generation models. However, it has traditionally relied on human subjective ratings such as the Mean Opinion Score (MOS), which depend on manual annotations and often suffer from inconsistent rating standards and poor reproducibility. To address these limitations, we introduce MOS-RMBench, a unified benchmark that reformulates diverse MOS datasets into a preference-comparison setting, enabling rigorous evaluation across different datasets. Building on MOS-RMBench, we systematically construct and evaluate three paradigms for reward modeling: scalar reward models, semi-scalar reward models, and generative reward models (GRMs). Our experiments reveal three key findings: (1) scalar models achieve the strongest overall performance, consistently exceeding 74% accuracy; (2) most models perform considerably worse on synthetic speech than on human speech; and (3) all models struggle on pairs with very small MOS differences. To improve performance on these challenging pairs, we propose a MOS-aware GRM that incorporates an MOS-difference-based reward function, enabling the model to adaptively scale rewards according to the difficulty of each sample pair. Experimental results show that the MOS-aware GRM significantly improves fine-grained quality discrimination and narrows the gap with scalar models on the most challenging cases. We hope this work will establish both a benchmark and a methodological framework to foster more rigorous and scalable research in automatic speech quality assessment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MOS-RMBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå°†ä¼ ç»Ÿçš„Mean Opinion Score (MOS)æ•°æ®é›†é‡æ„ä¸ºåå¥½æ¯”è¾ƒ(preference-comparison)è®¾ç½®çš„ç»Ÿä¸€åŸºå‡†ï¼Œæ—¨åœ¨è§£å†³è¯­éŸ³è´¨é‡è¯„ä¼°ä¸­æ‰‹åŠ¨æ ‡æ³¨ä¸ä¸€è‡´å’Œå¯é‡å¤æ€§å·®çš„é—®é¢˜ã€‚ç ”ç©¶è€…ç³»ç»Ÿè¯„ä¼°äº†æ ‡é‡å¥–åŠ±æ¨¡å‹(scalar reward models)ã€åŠæ ‡é‡å¥–åŠ±æ¨¡å‹å’Œç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹(GRMs)ä¸‰ç§èŒƒå¼ï¼Œå‘ç°æ ‡é‡æ¨¡å‹è™½ç„¶è¡¨ç°æœ€å¼ºï¼Œä½†åœ¨åˆæˆè¯­éŸ³åŠå¾®å°åˆ†å·®æ ·æœ¬ä¸Šä»å­˜åœ¨å±€é™ã€‚é’ˆå¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§MOS-aware GRMï¼Œé€šè¿‡å¼•å…¥åŸºäºMOSå·®å¼‚çš„å¥–åŠ±å‡½æ•°ï¼Œå®ç°äº†æ ¹æ®æ ·æœ¬éš¾åº¦è‡ªé€‚åº”ç¼©æ”¾å¥–åŠ±ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¨¡å‹æ˜¾è‘—å¢å¼ºäº†ç»†ç²’åº¦çš„è´¨é‡åˆ¤åˆ«èƒ½åŠ›ï¼Œæœ‰æ•ˆç¼©å°äº†åœ¨æç«¯å¤æ‚æƒ…å†µä¸‹çš„æ€§èƒ½å·®è·ã€‚è¿™é¡¹å·¥ä½œä¸ºè‡ªåŠ¨è¯­éŸ³è´¨é‡è¯„ä¼°å»ºç«‹äº†ä¸€ä¸ªæ›´ä¸¥è°¨ä¸”å…·æ‰©å±•æ€§çš„è¯„ä»·ä½“ç³»ä¸æ–¹æ³•è®ºæ¡†æ¶ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00743v1",
      "published_date": "2025-10-01 10:27:51 UTC",
      "updated_date": "2025-10-01 10:27:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:02:07.974786+00:00"
    },
    {
      "arxiv_id": "2510.00733v3",
      "title": "Neural Diffusion Processes for Physically Interpretable Survival Prediction",
      "title_zh": "é¢å‘ç‰©ç†å¯è§£é‡Šç”Ÿå­˜é¢„æµ‹çš„ç¥ç»æ‰©æ•£è¿‡ç¨‹",
      "authors": [
        "Alessio Cristofoletto",
        "Cesare Rollo",
        "Giovanni Birolo",
        "Piero Fariselli"
      ],
      "abstract": "We introduce DeepFHT, a survival-analysis framework that couples deep neural networks with first hitting time (FHT) distributions from stochastic process theory. Time to event is represented as the first passage of a latent diffusion process to an absorbing boundary. A neural network maps input variables to physically meaningful parameters including initial condition, drift, and diffusion, within a chosen FHT process such as Brownian motion, both with drift and driftless. This yields closed- form survival and hazard functions and captures time-varying risk without assuming proportional- hazards. We compare DeepFHT with Cox regression using synthetic and real-world datasets. The method achieves predictive accuracy on par with the state-of-the-art approach, while maintaining a physics- based interpretable parameterization that elucidates the relation between input features and risk. This combination of stochastic process theory and deep learning provides a principled avenue for modeling survival phenomena in complex systems",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DeepFHTï¼Œè¿™æ˜¯ä¸€ç§å°†æ·±åº¦ç¥ç»ç½‘ç»œä¸éšæœºè¿‡ç¨‹ç†è®ºä¸­çš„é¦–å‡»æ—¶é—´(First Hitting Time, FHT)åˆ†å¸ƒç›¸ç»“åˆçš„ç”Ÿå­˜åˆ†ææ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†äº‹ä»¶å‘ç”Ÿæ—¶é—´è¡¨ç¤ºä¸ºæ½œåœ¨æ‰©æ•£è¿‡ç¨‹åˆ°è¾¾å¸æ”¶è¾¹ç•Œçš„é¦–æ¬¡é€šè¿‡æ—¶é—´ï¼Œå¹¶åˆ©ç”¨ç¥ç»ç½‘ç»œå°†è¾“å…¥å˜é‡æ˜ å°„ä¸ºå…·æœ‰ç‰©ç†æ„ä¹‰çš„å‚æ•°ï¼ŒåŒ…æ‹¬åˆå§‹æ¡ä»¶ã€æ¼‚ç§»(Drift)å’Œæ‰©æ•£(Diffusion)ã€‚DeepFHTèƒ½å¤Ÿæä¾›é—­å¼ç”Ÿå­˜å‡½æ•°å’Œé£é™©å‡½æ•°ï¼Œåœ¨ä¸å‡è®¾æ¯”ä¾‹é£é™©(Proportional-Hazards)çš„å‰æä¸‹æ•æ‰éšæ—¶é—´å˜åŒ–çš„é£é™©ã€‚å®éªŒé€šè¿‡åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šä¸Coxå›å½’ç­‰æ¨¡å‹è¿›è¡Œå¯¹æ¯”ï¼Œè¯æ˜è¯¥æ–¹æ³•åœ¨é¢„æµ‹å‡†ç¡®ç‡ä¸Šè¾¾åˆ°äº†å½“å‰å…ˆè¿›æ°´å¹³(State-of-the-art)ï¼ŒåŒæ—¶ä¿æŒäº†åŸºäºç‰©ç†çš„å¯è§£é‡Šå‚æ•°åŒ–ç‰¹å¾ï¼Œé˜æ˜äº†è¾“å…¥ç‰¹å¾ä¸é£é™©ä¹‹é—´çš„å…³ç³»ã€‚è¿™ç§éšæœºè¿‡ç¨‹ç†è®ºä¸æ·±åº¦å­¦ä¹ çš„ç»“åˆï¼Œä¸ºå¤æ‚ç³»ç»Ÿä¸­çš„ç”Ÿå­˜ç°è±¡å»ºæ¨¡æä¾›äº†ä¸€ç§å…·æœ‰åŸåˆ™æ€§çš„æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.00733v3",
      "published_date": "2025-10-01 10:16:29 UTC",
      "updated_date": "2025-10-17 13:25:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:02:08.780226+00:00"
    },
    {
      "arxiv_id": "2510.00732v1",
      "title": "EvolProver: Advancing Automated Theorem Proving by Evolving Formalized Problems via Symmetry and Difficulty",
      "title_zh": "EvolProverï¼šé€šè¿‡åŸºäºå¯¹ç§°æ€§ä¸éš¾åº¦çš„å½¢å¼åŒ–é—®é¢˜æ¼”åŒ–æ¨åŠ¨è‡ªåŠ¨å®šç†è¯æ˜",
      "authors": [
        "Yuchen Tian",
        "Ruiyuan Huang",
        "Xuanwu Wang",
        "Jing Ma",
        "Zengfeng Huang",
        "Ziyang Luo",
        "Hongzhan Lin",
        "Da Zheng",
        "Lun Du"
      ],
      "abstract": "Large Language Models (LLMs) for formal theorem proving have shown significant promise, yet they often lack generalizability and are fragile to even minor transformations of problem statements. To address this limitation, we introduce a novel data augmentation pipeline designed to enhance model robustness from two perspectives: symmetry and difficulty. From the symmetry perspective, we propose two complementary methods: EvolAST, an Abstract Syntax Tree (AST) based approach that targets syntactic symmetry to generate semantically equivalent problem variants, and EvolDomain, which leverages LLMs to address semantic symmetry by translating theorems across mathematical domains. From the difficulty perspective, we propose EvolDifficulty, which uses carefully designed evolutionary instructions to guide LLMs in generating new theorems with a wider range of difficulty. We then use the evolved data to train EvolProver, a 7B-parameter non-reasoning theorem prover. EvolProver establishes a new state-of-the-art (SOTA) on FormalMATH-Lite with a 53.8% pass@32 rate, surpassing all models of comparable size, including reasoning-based models. It also sets new SOTA records for non-reasoning models on MiniF2F-Test (69.8% pass@32), Ineq-Comp-Seed (52.2% pass@32), and Ineq-Comp-Transformed (34.0% pass@32). Ablation studies further confirm our data augmentation pipeline's effectiveness across multiple benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EvolProverï¼Œæ—¨åœ¨é€šè¿‡ä»å¯¹ç§°æ€§(Symmetry)å’Œéš¾åº¦(Difficulty)ä¸¤ä¸ªç»´åº¦æ¼”åŒ–å½¢å¼åŒ–é—®é¢˜ï¼Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨è‡ªåŠ¨åŒ–å®šç†è¯æ˜ä¸­çš„æ³›åŒ–æ€§å’Œé²æ£’æ€§ã€‚ä¸ºäº†è§£å†³æ¨¡å‹å¯¹é—®é¢˜è¡¨è¿°å¾®å°å˜åŒ–æ•æ„Ÿçš„é—®é¢˜ï¼Œç ”ç©¶è€…è®¾è®¡äº†ä¸€å¥—æ–°é¢–çš„æ•°æ®å¢å¼ºæµæ°´çº¿ã€‚åœ¨Symmetryæ–¹é¢ï¼Œæå‡ºäº†åŸºäºæŠ½è±¡è¯­æ³•æ ‘(AST)ç”Ÿæˆè¯­ä¹‰ç­‰ä»·å˜ä½“çš„EvolASTï¼Œä»¥åŠåˆ©ç”¨LLMsè¿›è¡Œè·¨é¢†åŸŸå®šç†è½¬æ¢çš„EvolDomainã€‚åœ¨Difficultyæ–¹é¢ï¼Œé€šè¿‡EvolDifficultyæŒ‡ä»¤å¼•å¯¼æ¨¡å‹ç”Ÿæˆè¦†ç›–æ›´å¹¿éš¾åº¦èŒƒå›´çš„æ–°å®šç†ã€‚åˆ©ç”¨è¿™äº›æ¼”åŒ–æ•°æ®è®­ç»ƒå‡ºçš„7Bå‚æ•°éæ¨ç†å‹æ¨¡å‹EvolProverï¼Œåœ¨FormalMATH-Liteä¸Šè¾¾åˆ°äº†53.8%çš„pass@32å‡†ç¡®ç‡ï¼Œåˆ·æ–°äº†SOTAè®°å½•ã€‚è¯¥æ¨¡å‹åœ¨MiniF2F-Testå’ŒIneq-Compç­‰æµ‹è¯•ä¸­åŒæ ·è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†åŒç­‰è§„æ¨¡çš„æ¨ç†å‹æ¨¡å‹ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥éªŒè¯äº†è¯¥æ•°æ®å¢å¼ºæ¡†æ¶åœ¨æå‡æ¨¡å‹å¤„ç†å¤æ‚åŠè½¬æ¢åé—®é¢˜æ—¶çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00732v1",
      "published_date": "2025-10-01 10:15:27 UTC",
      "updated_date": "2025-10-01 10:15:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:02:12.285956+00:00"
    },
    {
      "arxiv_id": "2510.00728v1",
      "title": "Extreme Blind Image Restoration via Prompt-Conditioned Information Bottleneck",
      "title_zh": "åŸºäºæç¤ºå¼•å¯¼ä¿¡æ¯ç“¶é¢ˆçš„æç«¯ç›²å›¾åƒä¿®å¤",
      "authors": [
        "Hongeun Kim",
        "Bryan Sangwoo Kim",
        "Jong Chul Ye"
      ],
      "abstract": "Blind Image Restoration (BIR) methods have achieved remarkable success but falter when faced with Extreme Blind Image Restoration (EBIR), where inputs suffer from severe, compounded degradations beyond their training scope. Directly learning a mapping from extremely low-quality (ELQ) to high-quality (HQ) images is challenging due to the massive domain gap, often leading to unnatural artifacts and loss of detail. To address this, we propose a novel framework that decomposes the intractable ELQ-to-HQ restoration process. We first learn a projector that maps an ELQ image onto an intermediate, less-degraded LQ manifold. This intermediate image is then restored to HQ using a frozen, off-the-shelf BIR model. Our approach is grounded in information theory; we provide a novel perspective of image restoration as an Information Bottleneck problem and derive a theoretically-driven objective to train our projector. This loss function effectively stabilizes training by balancing a low-quality reconstruction term with a high-quality prior-matching term. Our framework enables Look Forward Once (LFO) for inference-time prompt refinement, and supports plug-and-play strengthening of existing image restoration models without need for finetuning. Extensive experiments under severe degradation regimes provide a thorough analysis of the effectiveness of our work.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æç«¯ç›²å›¾åƒå¤åŸ (Extreme Blind Image Restoration, EBIR) ä¸­ç”±äºä¸¥é‡é™è´¨å’Œå·¨å¤§åŸŸå·®å¼‚å¯¼è‡´çš„ä¼ªå½±å’Œç»†èŠ‚ä¸¢å¤±é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæç¤ºè°ƒèŠ‚ä¿¡æ¯ç“¶é¢ˆ (Prompt-Conditioned Information Bottleneck) çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ–¹æ³•å°†å¤æ‚çš„æä½è´¨é‡ (ELQ) åˆ°é«˜è´¨é‡ (HQ) çš„å¤åŸè¿‡ç¨‹è¿›è¡Œåˆ†è§£ï¼Œé¦–å…ˆé€šè¿‡å­¦ä¹ ä¸€ä¸ªæŠ•å½±å™¨å°† ELQ å›¾åƒæ˜ å°„åˆ°é€€åŒ–è¾ƒè½»çš„ä¸­é—´ä½è´¨é‡ (LQ) æµå½¢ï¼Œå†åˆ©ç”¨é¢„è®­ç»ƒä¸”å†»ç»“çš„ BIR æ¨¡å‹å®Œæˆæœ€ç»ˆä¿®å¤ã€‚ç ”ç©¶ä»ä¿¡æ¯è®ºè§’åº¦å°†å›¾åƒå¤åŸå®šä¹‰ä¸ºä¿¡æ¯ç“¶é¢ˆé—®é¢˜ï¼Œå¹¶æ¨å¯¼å‡ºä¸€ç§å¹³è¡¡ä½è´¨é‡é‡å»ºä¸é«˜è´¨é‡å…ˆéªŒåŒ¹é…çš„ç†è®ºé©±åŠ¨æŸå¤±å‡½æ•°ï¼Œæœ‰æ•ˆç¨³å®šäº†è®­ç»ƒè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†æ¨ç†æ—¶çš„å•æ¬¡å‰ç» (Look Forward Once, LFO) æç¤ºä¼˜åŒ–æœºåˆ¶ï¼Œå¹¶æ”¯æŒä»¥å³æ’å³ç”¨ (plug-and-play) çš„æ–¹å¼ç›´æ¥å¢å¼ºç°æœ‰æ¨¡å‹è€Œæ— éœ€å¾®è°ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æç«¯é€€åŒ–åœºæ™¯ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œä¸ºè§£å†³å¤æ‚ç¯å¢ƒä¸‹çš„å›¾åƒå¤åŸä»»åŠ¡æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00728v1",
      "published_date": "2025-10-01 10:13:27 UTC",
      "updated_date": "2025-10-01 10:13:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:02:17.286173+00:00"
    },
    {
      "arxiv_id": "2510.00726v1",
      "title": "CroSTAta: Cross-State Transition Attention Transformer for Robotic Manipulation",
      "title_zh": "CroSTAtaï¼šé¢å‘æœºå™¨äººæ“ä½œçš„è·¨çŠ¶æ€è½¬ç§»æ³¨æ„åŠ› Transformer",
      "authors": [
        "Giovanni Minelli",
        "Giulio Turrisi",
        "Victor Barasuol",
        "Claudio Semini"
      ],
      "abstract": "Learning robotic manipulation policies through supervised learning from demonstrations remains challenging when policies encounter execution variations not explicitly covered during training. While incorporating historical context through attention mechanisms can improve robustness, standard approaches process all past states in a sequence without explicitly modeling the temporal structure that demonstrations may include, such as failure and recovery patterns. We propose a Cross-State Transition Attention Transformer that employs a novel State Transition Attention (STA) mechanism to modulate standard attention weights based on learned state evolution patterns, enabling policies to better adapt their behavior based on execution history. Our approach combines this structured attention with temporal masking during training, where visual information is randomly removed from recent timesteps to encourage temporal reasoning from historical context. Evaluation in simulation shows that STA consistently outperforms standard cross-attention and temporal modeling approaches like TCN and LSTM networks across all tasks, achieving more than 2x improvement over cross-attention on precision-critical tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CroSTAtaï¼Œä¸€ç§é¢å‘æœºå™¨äººæ“ä½œçš„è·¨çŠ¶æ€è½¬æ¢æ³¨æ„åŠ› Transformer (Cross-State Transition Attention Transformer)ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†åˆ›æ–°çš„çŠ¶æ€è½¬æ¢æ³¨æ„åŠ› (State Transition Attention, STA) æœºåˆ¶ï¼Œé€šè¿‡å­¦ä¹ çŠ¶æ€æ¼”åŒ–æ¨¡å¼æ¥è°ƒåˆ¶æ ‡å‡†æ³¨æ„åŠ›æƒé‡ï¼Œä½¿ç­–ç•¥èƒ½å¤Ÿæ ¹æ®æ‰§è¡Œå†å²æ›´æœ‰æ•ˆåœ°è°ƒæ•´è¡Œä¸ºã€‚ä¸ºäº†å¼ºåŒ–æ¨¡å‹çš„æ—¶é—´æ¨ç†èƒ½åŠ›ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­ç»“åˆäº†æ—¶é—´æ©ç  (temporal masking) æŠ€æœ¯ï¼Œé€šè¿‡éšæœºç§»é™¤è¿‘æœŸæ—¶é—´æ­¥çš„è§†è§‰ä¿¡æ¯æ¥é¼“åŠ±æ¨¡å‹æŒ–æ˜å†å²èƒŒæ™¯ã€‚ä»¿çœŸå®éªŒè¯æ˜ï¼ŒSTA åœ¨æ‰€æœ‰æµ‹è¯•ä»»åŠ¡ä¸­å‡ä¼˜äºæ ‡å‡†äº¤å‰æ³¨æ„åŠ› (cross-attention) ä»¥åŠ TCN å’Œ LSTM ç­‰æ—¶é—´å»ºæ¨¡æ–¹æ³•ã€‚å°¤å…¶åœ¨å¯¹ç²¾åº¦è¦æ±‚æé«˜çš„ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•æ¯”äº¤å‰æ³¨æ„åŠ›å®ç°äº†è¶…è¿‡ 2 å€çš„æ€§èƒ½æå‡ï¼Œæ˜¾è‘—å¢å¼ºäº†æœºå™¨äººç­–ç•¥åœ¨åº”å¯¹å¤æ‚æ‰§è¡Œå˜åŒ–æ—¶çš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Code and data available at https://github.com/iit-DLSLab/croSTAta",
      "pdf_url": "https://arxiv.org/pdf/2510.00726v1",
      "published_date": "2025-10-01 10:09:05 UTC",
      "updated_date": "2025-10-01 10:09:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:02:20.781879+00:00"
    },
    {
      "arxiv_id": "2510.01296v1",
      "title": "From 2D to 3D, Deep Learning-based Shape Reconstruction in Magnetic Resonance Imaging: A Review",
      "title_zh": "ä»2Dåˆ°3Dï¼šåŸºäºæ·±åº¦å­¦ä¹ çš„ç£å…±æŒ¯æˆåƒå½¢çŠ¶é‡å»ºç»¼è¿°",
      "authors": [
        "Emma McMillian",
        "Abhirup Banerjee",
        "Alfonso Bueno-Orovio"
      ],
      "abstract": "Deep learning-based 3-dimensional (3D) shape reconstruction from 2-dimensional (2D) magnetic resonance imaging (MRI) has become increasingly important in medical disease diagnosis, treatment planning, and computational modeling. This review surveys the methodological landscape of 3D MRI reconstruction, focusing on 4 primary approaches: point cloud, mesh-based, shape-aware, and volumetric models. For each category, we analyze the current state-of-the-art techniques, their methodological foundation, limitations, and applications across anatomical structures. We provide an extensive overview ranging from cardiac to neurological to lung imaging. We also focus on the clinical applicability of models to diseased anatomy, and the influence of their training and testing data. We examine publicly available datasets, computational demands, and evaluation metrics. Finally, we highlight the emerging research directions including multimodal integration and cross-modality frameworks. This review aims to provide researchers with a structured overview of current 3D reconstruction methodologies to identify opportunities for advancing deep learning towards more robust, generalizable, and clinically impactful solutions.",
      "tldr_zh": "è¯¥ç»¼è¿°ç³»ç»Ÿæ¢è®¨äº†åŸºäºæ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰ä»äºŒç»´ï¼ˆ2Dï¼‰ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰è¿›è¡Œä¸‰ç»´ï¼ˆ3Dï¼‰å½¢çŠ¶é‡å»ºçš„æ–¹æ³•å­¦æ™¯è§‚ï¼Œå¼ºè°ƒäº†å…¶åœ¨ç–¾ç—…è¯Šæ–­ã€æ²»ç–—è§„åˆ’å’Œè®¡ç®—å»ºæ¨¡ä¸­çš„é‡è¦æ€§ã€‚ç ”ç©¶é‡ç‚¹åˆ†æäº†å››ç§ä¸»è¦é€”å¾„ï¼šç‚¹äº‘ï¼ˆpoint cloudï¼‰ã€åŸºäºç½‘æ ¼ï¼ˆmesh-basedï¼‰ã€å½¢çŠ¶æ„ŸçŸ¥ï¼ˆshape-awareï¼‰å’Œä½“ç´ ï¼ˆvolumetricï¼‰æ¨¡å‹ï¼Œå¹¶æ·±å…¥æ¢è®¨äº†å®ƒä»¬åœ¨å¿ƒè„ã€ç¥ç»åŠè‚ºéƒ¨æˆåƒç­‰è§£å‰–ç»“æ„ä¸­çš„åº”ç”¨ã€‚æ–‡ç« è¯¦ç»†è¯„ä¼°äº†å½“å‰æœ€å…ˆè¿›æŠ€æœ¯çš„å±€é™æ€§ã€ä¸´åºŠé€‚ç”¨æ€§ã€å…¬å¼€æ•°æ®é›†åŠè®¡ç®—éœ€æ±‚ï¼ŒåŒæ—¶è€ƒå¯Ÿäº†è®­ç»ƒæ•°æ®å¯¹æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„å½±å“ã€‚æ­¤å¤–ï¼Œè¯¥ç»¼è¿°è¿˜å‰ç»æ€§åœ°æå‡ºäº†å¤šæ¨¡æ€é›†æˆï¼ˆmultimodal integrationï¼‰å’Œè·¨æ¨¡æ€æ¡†æ¶ï¼ˆcross-modality frameworksï¼‰ç­‰æ–°å…´ç ”ç©¶æ–¹å‘ã€‚é€šè¿‡æä¾›ç»“æ„åŒ–çš„æ–¹æ³•è®ºæ¦‚è¿°ï¼Œè¯¥ç ”ç©¶æ—¨åœ¨æŒ‡å¯¼ç§‘ç ”äººå‘˜å¼€å‘æ›´å…·é²æ£’æ€§ã€é€šç”¨æ€§å’Œä¸´åºŠä»·å€¼çš„ 3D é‡å»ºè§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01296v1",
      "published_date": "2025-10-01 09:57:29 UTC",
      "updated_date": "2025-10-01 09:57:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:02:24.781064+00:00"
    },
    {
      "arxiv_id": "2510.00706v1",
      "title": "AttentionDep: Domain-Aware Attention for Explainable Depression Severity Assessment",
      "title_zh": "AttentionDepï¼šç”¨äºå¯è§£é‡ŠæŠ‘éƒç¨‹åº¦è¯„ä¼°çš„é¢†åŸŸæ„ŸçŸ¥æ³¨æ„åŠ›",
      "authors": [
        "Yusif Ibrahimov",
        "Tarique Anwar",
        "Tommy Yuan",
        "Turan Mutallimov",
        "Elgun Hasanov"
      ],
      "abstract": "In today's interconnected society, social media platforms provide a window into individuals' thoughts, emotions, and mental states. This paper explores the use of platforms like Facebook, X (formerly Twitter), and Reddit for depression severity detection. We propose AttentionDep, a domain-aware attention model that drives explainable depression severity estimation by fusing contextual and domain knowledge. Posts are encoded hierarchically using unigrams and bigrams, with attention mechanisms highlighting clinically relevant tokens. Domain knowledge from a curated mental health knowledge graph is incorporated through a cross-attention mechanism, enriching the contextual features. Finally, depression severity is predicted using an ordinal regression framework that respects the clinical-relevance and natural ordering of severity levels. Our experiments demonstrate that AttentionDep outperforms state-of-the-art baselines by over 5% in graded F1 score across datasets, while providing interpretable insights into its predictions. This work advances the development of trustworthy and transparent AI systems for mental health assessment from social media.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AttentionDepï¼Œä¸€ç§é¢å‘é¢†åŸŸæ„ŸçŸ¥çš„æ³¨æ„åŠ›æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡èåˆä¸Šä¸‹æ–‡å’Œé¢†åŸŸçŸ¥è¯†å®ç°å¯è§£é‡Šçš„æŠ‘éƒä¸¥é‡ç¨‹åº¦è¯„ä¼°ã€‚è¯¥æ¨¡å‹åˆ©ç”¨ç¤¾äº¤åª’ä½“å¹³å°çš„æ•°æ®ï¼Œé€šè¿‡unigramså’Œbigramså¯¹å¸–å­è¿›è¡Œå±‚æ¬¡åŒ–ç¼–ç ï¼Œå¹¶åˆ©ç”¨attention mechanismsçªå‡ºå…·æœ‰ä¸´åºŠç›¸å…³æ€§çš„è¯å…ƒã€‚AttentionDepé€šè¿‡cross-attention mechanismæ•´åˆäº†æ¥è‡ªç²¾é€‰å¿ƒç†å¥åº·çŸ¥è¯†å›¾è°±(knowledge graph)çš„é¢†åŸŸçŸ¥è¯†ï¼Œä»è€Œå¢å¼ºäº†ä¸Šä¸‹æ–‡ç‰¹å¾ã€‚æœ€ç»ˆï¼Œè¯¥ç ”ç©¶é‡‡ç”¨ordinal regressionæ¡†æ¶ï¼Œéµå¾ªä¸¥é‡ç¨‹åº¦ç­‰çº§çš„ä¸´åºŠç›¸å…³æ€§å’Œè‡ªç„¶æ’åºè¿›è¡Œé¢„æµ‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAttentionDepåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„åˆ†çº§F1åˆ†æ•°(graded F1 score)æ¯”ç°æœ‰åŸºå‡†æ¨¡å‹æå‡äº†5%ä»¥ä¸Šï¼Œå¹¶èƒ½ä¸ºé¢„æµ‹æä¾›å¯è§£é‡Šçš„è§è§£ã€‚è¯¥å·¥ä½œä¸ºå¼€å‘é€æ˜ä¸”å¯ä¿¡çš„ç¤¾äº¤åª’ä½“å¿ƒç†å¥åº·è¯„ä¼°AIç³»ç»Ÿåšå‡ºäº†é‡è¦è´¡çŒ®ã€‚",
      "categories": [
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00706v1",
      "published_date": "2025-10-01 09:20:53 UTC",
      "updated_date": "2025-10-01 09:20:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:02:36.084457+00:00"
    },
    {
      "arxiv_id": "2510.00694v1",
      "title": "ALARB: An Arabic Legal Argument Reasoning Benchmark",
      "title_zh": "ALARBï¼šé˜¿æ‹‰ä¼¯è¯­æ³•å¾‹è®ºè¯æ¨ç†åŸºå‡†",
      "authors": [
        "Harethah Abu Shairah",
        "Somayah AlHarbi",
        "Abdulaziz AlHussein",
        "Sameer Alsabea",
        "Omar Shaqaqi",
        "Hebah AlShamlan",
        "Omar Knio",
        "George Turkiyyah"
      ],
      "abstract": "We introduce ALARB, a dataset and suite of tasks designed to evaluate the reasoning capabilities of large language models (LLMs) within the Arabic legal domain. While existing Arabic benchmarks cover some knowledge-intensive tasks such as retrieval and understanding, substantial datasets focusing specifically on multistep reasoning for Arabic LLMs, especially in open-ended contexts, are lacking. The dataset comprises over 13K commercial court cases from Saudi Arabia, with each case including the facts presented, the reasoning of the court, the verdict, as well as the cited clauses extracted from the regulatory documents. We define a set of challenging tasks leveraging this dataset and reflecting the complexity of real-world legal reasoning, including verdict prediction, completion of reasoning chains in multistep legal arguments, and identification of relevant regulations based on case facts. We benchmark a representative selection of current open and closed Arabic LLMs on these tasks and demonstrate the dataset's utility for instruction tuning. Notably, we show that instruction-tuning a modest 12B parameter model using ALARB significantly enhances its performance in verdict prediction and Arabic verdict generation, reaching a level comparable to that of GPT-4o.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†ALARBï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨é˜¿æ‹‰ä¼¯æ³•å¾‹é¢†åŸŸæ¨ç†èƒ½åŠ›çš„åŸºå‡†æ•°æ®é›†å’Œä»»åŠ¡å¥—ä»¶ã€‚è¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªæ²™ç‰¹é˜¿æ‹‰ä¼¯çš„1.3ä¸‡å¤šèµ·å•†ä¸šæ³•é™¢æ¡ˆä»¶ï¼Œè¯¦ç»†è®°å½•äº†æ¡ˆä»¶æ¡ˆæƒ…ã€æ³•é™¢æ¨ç†è¿‡ç¨‹ã€æœ€ç»ˆåˆ¤å†³ä»¥åŠå¼•ç”¨çš„æ³•å¾‹æ¡æ¬¾ã€‚ç ”ç©¶åŸºäºè¯¥æ•°æ®é›†å®šä¹‰äº†åˆ¤å†³é¢„æµ‹(Verdict Prediction)ã€æ³•å¾‹æ¨ç†é“¾è¡¥å…¨å’Œæ³•è§„è¯†åˆ«ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œä»¥åæ˜ ç°å®ä¸–ç•Œä¸­æ³•å¾‹æ¨ç†çš„å¤æ‚æ€§ã€‚é€šè¿‡å¯¹å¤šç§å¼€æºå’Œé—­æºæ¨¡å‹çš„åŸºå‡†æµ‹è¯•ï¼Œç ”ç©¶å±•ç¤ºäº†ALARBåœ¨æŒ‡ä»¤å¾®è°ƒ(Instruction Tuning)æ–¹é¢çš„æ˜¾è‘—ä½œç”¨ã€‚å®éªŒç»“æœè¯æ˜ï¼Œç»è¿‡ALARBå¾®è°ƒçš„12Bå‚æ•°æ¨¡å‹åœ¨é˜¿æ‹‰ä¼¯è¯­åˆ¤å†³ç”Ÿæˆç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå…¶æ€§èƒ½å·²å¯ä¸GPT-4oç›¸åª²ç¾ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted paper at ArabicNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.00694v1",
      "published_date": "2025-10-01 09:15:41 UTC",
      "updated_date": "2025-10-01 09:15:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:02:39.179671+00:00"
    },
    {
      "arxiv_id": "2510.00691v1",
      "title": "Inclusive Easy-to-Read Generation for Individuals with Cognitive Impairments",
      "title_zh": "é¢å‘è®¤çŸ¥éšœç¢äººå£«çš„åŒ…å®¹æ€§æ˜“è¯»æ–‡æœ¬ç”Ÿæˆ",
      "authors": [
        "FranÃ§ois Ledoyen",
        "GaÃ«l Dias",
        "Alexis Lechervy",
        "Jeremie Pantin",
        "Fabrice Maurel",
        "Youssef Chahir",
        "Elisa Gouzonnat",
        "MÃ©lanie Berthelot",
        "Stanislas Moravac",
        "Armony Altinier",
        "Amy Khairalla"
      ],
      "abstract": "Ensuring accessibility for individuals with cognitive impairments is essential for autonomy, self-determination, and full citizenship. However, manual Easy-to-Read (ETR) text adaptations are slow, costly, and difficult to scale, limiting access to crucial information in healthcare, education, and civic life. AI-driven ETR generation offers a scalable solution but faces key challenges, including dataset scarcity, domain adaptation, and balancing lightweight learning of Large Language Models (LLMs). In this paper, we introduce ETR-fr, the first dataset for ETR text generation fully compliant with European ETR guidelines. We implement parameter-efficient fine-tuning on PLMs and LLMs to establish generative baselines. To ensure high-quality and accessible outputs, we introduce an evaluation framework based on automatic metrics supplemented by human assessments. The latter is conducted using a 36-question evaluation form that is aligned with the guidelines. Overall results show that PLMs perform comparably to LLMs and adapt effectively to out-of-domain texts.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨é€šè¿‡äººå·¥æ™ºèƒ½é©±åŠ¨çš„æ˜“è¯»ï¼ˆEasy-to-Read, ETRï¼‰æ–‡æœ¬ç”ŸæˆæŠ€æœ¯ï¼Œæå‡è®¤çŸ¥éšœç¢äººå£«è·å–åŒ»ç–—ã€æ•™è‚²å’Œå…¬æ°‘ç”Ÿæ´»ä¿¡æ¯çš„æ— éšœç¢æ€§ã€‚ä½œè€…æ¨å‡ºäº† ETR-frï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå®Œå…¨ç¬¦åˆæ¬§æ´² ETR æŒ‡å—çš„æ–‡æœ¬ç”Ÿæˆæ•°æ®é›†ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸæ•°æ®é›†ç¨€ç¼ºçš„ç©ºç™½ã€‚é€šè¿‡å¯¹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆParameter-efficient fine-tuningï¼‰ï¼Œç ”ç©¶å»ºç«‹äº†ç”ŸæˆåŸºå‡†ã€‚ä¸ºäº†ç¡®ä¿è¾“å‡ºè´¨é‡ï¼Œç ”ç©¶å¼•å…¥äº†ä¸€å¥—ç»“åˆè‡ªåŠ¨æŒ‡æ ‡ä¸36é¡¹äººå·¥è¯„ä¼°å‡†åˆ™çš„è¯„ä»·æ¡†æ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPLMs åœ¨æ€§èƒ½ä¸Šä¸ LLMs ç›¸å½“ï¼Œä¸”åœ¨è·¨é¢†åŸŸæ–‡æœ¬é€‚é…æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¸ºå¯æ‰©å±•çš„æ— éšœç¢ä¿¡æ¯å¤„ç†æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ECAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.00691v1",
      "published_date": "2025-10-01 09:13:18 UTC",
      "updated_date": "2025-10-01 09:13:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:02:41.391900+00:00"
    },
    {
      "arxiv_id": "2510.00690v1",
      "title": "ACPO: Adaptive Curriculum Policy Optimization for Aligning Vision-Language Models in Complex Reasoning",
      "title_zh": "ACPOï¼šç”¨äºå¤æ‚æ¨ç†ä¸­è§†è§‰-è¯­è¨€æ¨¡å‹å¯¹é½çš„è‡ªé€‚åº”è¯¾ç¨‹ç­–ç•¥ä¼˜åŒ–",
      "authors": [
        "Yunhao Wang",
        "Ziting Li",
        "Shuai Chen",
        "Tao Liu",
        "Chao Song",
        "Junjie Jiang",
        "Jian Zhu",
        "Peng Gao",
        "Bin Qin"
      ],
      "abstract": "Aligning large-scale vision-language models (VLMs) for complex reasoning via reinforcement learning is often hampered by the limitations of existing policy optimization algorithms, such as static training schedules and the rigid, uniform clipping mechanism in Proximal Policy Optimization (PPO). In this work, we introduce Adaptive Curriculum Policy Optimization (ACPO), a novel framework that addresses these challenges through a dual-component adaptive learning strategy. First, ACPO employs a dynamic curriculum that orchestrates a principled transition from a stable, near on-policy exploration phase to an efficient, off-policy exploitation phase by progressively increasing sample reuse. Second, we propose an Advantage-Aware Adaptive Clipping (AAAC) mechanism that replaces the fixed clipping hyperparameter with dynamic, sample-wise bounds modulated by the normalized advantage of each token. This allows for more granular and robust policy updates, enabling larger gradients for high-potential samples while safeguarding against destructive ones. We conduct extensive experiments on a suite of challenging multimodal reasoning benchmarks, including MathVista, LogicVista, and MMMU-Pro. Results demonstrate that ACPO consistently outperforms strong baselines such as DAPO and PAPO, achieving state-of-the-art performance, accelerated convergence, and superior training stability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Adaptive Curriculum Policy Optimization (ACPO) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Models, VLMs) åœ¨å¤æ‚æ¨ç†å¯¹é½ä¸­é¢ä¸´çš„ Proximal Policy Optimization (PPO) é™æ€è®­ç»ƒè®¡åˆ’å’ŒåƒµåŒ–å‰ªåˆ‡æœºåˆ¶ç­‰å±€é™æ€§ã€‚ACPO é‡‡ç”¨åŒé‡ç»„ä»¶è‡ªé€‚åº”ç­–ç•¥ï¼Œé¦–å…ˆé€šè¿‡åŠ¨æ€è¯¾ç¨‹è®¾è®¡å®ç°ä»ç¨³å®šæ¢ç´¢åˆ°é«˜æ•ˆåˆ©ç”¨çš„å¹³æ»‘è¿‡æ¸¡ï¼Œå…¶æ¬¡å¼•å…¥äº† Advantage-Aware Adaptive Clipping (AAAC) æœºåˆ¶ï¼Œæ ¹æ® token çš„å½’ä¸€åŒ–ä¼˜åŠ¿åŠ¨æ€è°ƒæ•´å‰ªåˆ‡è¾¹ç•Œã€‚è¿™ç§æ–¹æ³•å…è®¸å¯¹é«˜æ½œåŠ›æ ·æœ¬è¿›è¡Œæ›´å¤§å¹…åº¦çš„ç­–ç•¥æ›´æ–°ï¼ŒåŒæ—¶æœ‰æ•ˆé˜²æ­¢ç ´åæ€§çš„æ¢¯åº¦å½±å“ï¼Œç¡®ä¿äº†è®­ç»ƒè¿‡ç¨‹çš„ç¨³å¥æ€§ã€‚åœ¨ MathVistaã€LogicVista å’Œ MMMU-Pro ç­‰å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒACPO æ˜¾è‘—ä¼˜äº DAPO å’Œ PAPO ç­‰æ¨¡å‹ï¼Œä¸ä»…è¾¾åˆ°äº† State-of-the-Art (SOTA) æ€§èƒ½ï¼Œè¿˜å±•ç°å‡ºæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œå“è¶Šçš„è®­ç»ƒç¨³å®šæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00690v1",
      "published_date": "2025-10-01 09:11:27 UTC",
      "updated_date": "2025-10-01 09:11:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:02:45.089940+00:00"
    },
    {
      "arxiv_id": "2510.00689v1",
      "title": "Relevance-Zone Reduction in Game Solving",
      "title_zh": "åšå¼ˆæ±‚è§£ä¸­çš„ç›¸å…³åŒºåŸŸç¼©å‡",
      "authors": [
        "Chi-Huang Lin",
        "Ting Han Wei",
        "Chun-Jui Wang",
        "Hung Guei",
        "Chung-Chin Shih",
        "Yun-Jui Tsai",
        "I-Chen Wu",
        "Ti-Rong Wu"
      ],
      "abstract": "Game solving aims to find the optimal strategies for all players and determine the theoretical outcome of a game. However, due to the exponential growth of game trees, many games remain unsolved, even though methods like AlphaZero have demonstrated super-human level in game playing. The Relevance-Zone (RZ) is a local strategy reuse technique that restricts the search to only the regions relevant to the outcome, significantly reducing the search space. However, RZs are not unique. Different solutions may result in RZs of varying sizes. Smaller RZs are generally more favorable, as they increase the chance of reuse and improve pruning efficiency. To this end, we propose an iterative RZ reduction method that repeatedly solves the same position while gradually restricting the region involved, guiding the solver toward smaller RZs. We design three constraint generation strategies and integrate an RZ Pattern Table to fully leverage past solutions. In experiments on 7x7 Killall-Go, our method reduces the average RZ size to 85.95% of the original. Furthermore, the reduced RZs can be permanently stored as reusable knowledge for future solving tasks, especially for larger board sizes or different openings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åšå¼ˆæ±‚è§£ (Game Solving) ä¸­æœç´¢ç©ºé—´éšåšå¼ˆæ ‘æŒ‡æ•°çº§å¢é•¿çš„é—®é¢˜ï¼Œé‡ç‚¹æ¢è®¨äº†ç›¸å…³åŒºåŸŸ (Relevance-Zone, RZ) è¿™ä¸€å±€éƒ¨ç­–ç•¥é‡ç”¨æŠ€æœ¯ã€‚ç”±äº RZ çš„è§„æ¨¡ä¸å”¯ä¸€ä¸”ç›´æ¥å½±å“æœç´¢å‰ªææ•ˆç‡ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è¿­ä»£å¼çš„ RZ å‰Šå‡æ–¹æ³•ï¼Œé€šè¿‡åœ¨é‡å¤æ±‚è§£è¿‡ç¨‹ä¸­é€æ¸æ”¶çª„æ¶‰åŠåŒºåŸŸï¼Œå¼•å¯¼æ±‚è§£å™¨ç”Ÿæˆæ›´å°çš„ RZã€‚ç ”ç©¶è¿˜è®¾è®¡äº†ä¸‰ç§çº¦æŸç”Ÿæˆç­–ç•¥ï¼Œå¹¶å¼•å…¥ç›¸å…³åŒºåŸŸæ¨¡å¼è¡¨ (RZ Pattern Table) ä»¥å……åˆ†åˆ©ç”¨å†å²è§£æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ 7x7 Killall-Go ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•å°†å¹³å‡ RZ è§„æ¨¡ç¼©å‡è‡³åŸå§‹å¤§å°çš„ 85.95%ã€‚è¿™ç§å‰Šå‡åçš„ RZ å¯ä»¥ä½œä¸ºå¯é‡ç”¨çŸ¥è¯†æ°¸ä¹…å­˜å‚¨ï¼Œæ˜¾è‘—æå‡äº†é’ˆå¯¹æ›´å¤§æ£‹ç›˜å°ºå¯¸æˆ–ä¸åŒå¼€å±€çš„åç»­åšå¼ˆæ±‚è§£æ•ˆç‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by the Advances in Computer Games (ACG 2025)",
      "pdf_url": "https://arxiv.org/pdf/2510.00689v1",
      "published_date": "2025-10-01 09:10:32 UTC",
      "updated_date": "2025-10-01 09:10:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:02:54.078528+00:00"
    },
    {
      "arxiv_id": "2510.00664v1",
      "title": "Batch-CAM: Introduction to better reasoning in convolutional deep learning models",
      "title_zh": "Batch-CAMï¼šæå‡å·ç§¯æ·±åº¦å­¦ä¹ æ¨¡å‹æ¨ç†èƒ½åŠ›çš„ä»‹ç»",
      "authors": [
        "Giacomo Ignesti",
        "Davide Moroni",
        "Massimo Martinelli"
      ],
      "abstract": "Understanding the inner workings of deep learning models is crucial for advancing artificial intelligence, particularly in high-stakes fields such as healthcare, where accurate explanations are as vital as precision. This paper introduces Batch-CAM, a novel training paradigm that fuses a batch implementation of the Grad-CAM algorithm with a prototypical reconstruction loss. This combination guides the model to focus on salient image features, thereby enhancing its performance across classification tasks. Our results demonstrate that Batch-CAM achieves a simultaneous improvement in accuracy and image reconstruction quality while reducing training and inference times. By ensuring models learn from evidence-relevant information,this approach makes a relevant contribution to building more transparent, explainable, and trustworthy AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Batch-CAMï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„è®­ç»ƒèŒƒå¼ï¼Œå®ƒå°† Grad-CAM ç®—æ³•çš„æ‰¹é‡å®ç°ä¸åŸå‹é‡å»ºæŸå¤± (prototypical reconstruction loss) ç›¸ç»“åˆï¼Œæ—¨åœ¨ä¼˜åŒ–å·ç§¯æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ¨ç†æœºåˆ¶ã€‚é€šè¿‡å¼•å¯¼æ¨¡å‹èšç„¦äºå›¾åƒä¸­çš„æ˜¾è‘—ç‰¹å¾ (salient image features)ï¼Œè¯¥æ¡†æ¶ä¸ä»…æå‡äº†åˆ†ç±»ä»»åŠ¡çš„æ€§èƒ½ï¼Œè¿˜å¢å¼ºäº†æ¨¡å‹å†³ç­–çš„é€æ˜åº¦ï¼Œè¿™å¯¹äºåŒ»ç–—ä¿å¥ç­‰é«˜é£é™©é¢†åŸŸè‡³å…³é‡è¦ã€‚å®éªŒæ•°æ®è¡¨æ˜ï¼ŒBatch-CAM åœ¨æé«˜å‡†ç¡®ç‡å’Œå›¾åƒé‡å»ºè´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒä¸æ¨ç†çš„æ—¶é—´å¼€é”€ã€‚è¯¥æ–¹æ³•ç¡®ä¿æ¨¡å‹ä»è¯æ®ç›¸å…³çš„ä¿¡æ¯ä¸­å­¦ä¹ ï¼Œä¸ºæ„å»ºæ›´å…·å¯è§£é‡Šæ€§å’Œä¿¡ä»»åº¦çš„ AI ç³»ç»Ÿåšå‡ºäº†é‡è¦è´¡çŒ®ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages, 7 figures, submitted to SN Computer Science Springer Nature",
      "pdf_url": "https://arxiv.org/pdf/2510.00664v1",
      "published_date": "2025-10-01 08:47:00 UTC",
      "updated_date": "2025-10-01 08:47:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:02:51.773042+00:00"
    },
    {
      "arxiv_id": "2510.00662v1",
      "title": "Facilitating Cognitive Accessibility with LLMs: A Multi-Task Approach to Easy-to-Read Text Generation",
      "title_zh": "åˆ©ç”¨ LLM æå‡è®¤çŸ¥æ— éšœç¢ï¼šä¸€ç§ç”Ÿæˆæ˜“è¯»æ–‡æœ¬çš„å¤šä»»åŠ¡æ–¹æ³•",
      "authors": [
        "FranÃ§ois Ledoyen",
        "GaÃ«l Dias",
        "Jeremie Pantin",
        "Alexis Lechervy",
        "Fabrice Maurel",
        "Youssef Chahir"
      ],
      "abstract": "Simplifying complex texts is essential for ensuring equitable access to information, especially for individuals with cognitive impairments. The Easy-to-Read (ETR) initiative offers a framework for making content accessible to the neurodivergent population, but the manual creation of such texts remains time-consuming and resource-intensive. In this work, we investigate the potential of large language models (LLMs) to automate the generation of ETR content. To address the scarcity of aligned corpora and the specificity of ETR constraints, we propose a multi-task learning (MTL) approach that trains models jointly on text summarization, text simplification, and ETR generation. We explore two different strategies: multi-task retrieval-augmented generation (RAG) for in-context learning, and MTL-LoRA for parameter-efficient fine-tuning. Our experiments with Mistral-7B and LLaMA-3-8B, based on ETR-fr, a new high-quality dataset, demonstrate the benefits of multi-task setups over single-task baselines across all configurations. Moreover, results show that the RAG-based strategy enables generalization in out-of-domain settings, while MTL-LoRA outperforms all learning strategies within in-domain configurations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)è‡ªåŠ¨ç”Ÿæˆæ˜“è¯»(Easy-to-Read, ETR)æ–‡æœ¬çš„æ½œåŠ›ï¼Œæ—¨åœ¨ä¸ºè®¤çŸ¥éšœç¢äººç¾¤æä¾›æ›´å…¬å¹³çš„ä¿¡æ¯è·å–æ¸ é“ã€‚é’ˆå¯¹ETRè¯­æ–™åº“ç¨€ç¼ºåŠç‰¹å®šçº¦æŸçš„æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡å­¦ä¹ (Multi-task learning, MTL)æ¡†æ¶ï¼Œå°†æ–‡æœ¬æ‘˜è¦ã€æ–‡æœ¬ç®€åŒ–ä¸ETRç”Ÿæˆè¿›è¡Œè”åˆè®­ç»ƒã€‚ç ”ç©¶è¯„ä¼°äº†ä¸¤ç§ä¸»è¦ç­–ç•¥ï¼šç”¨äºä¸Šä¸‹æ–‡å­¦ä¹ çš„å¤šä»»åŠ¡æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ä»¥åŠç”¨äºå‚æ•°é«˜æ•ˆå¾®è°ƒçš„MTL-LoRAã€‚é€šè¿‡åœ¨é«˜è´¨é‡æ³•è¯­æ•°æ®é›†ETR-frä¸Šå¯¹Mistral-7Bå’ŒLLaMA-3-8Bè¿›è¡Œå®éªŒï¼Œç»“æœè¯æ˜å¤šä»»åŠ¡é…ç½®åœ¨æ‰€æœ‰å®éªŒè®¾ç½®ä¸‹å‡ä¼˜äºå•ä»»åŠ¡åŸºçº¿ã€‚å®éªŒè¿›ä¸€æ­¥å‘ç°ï¼ŒåŸºäºRAGçš„ç­–ç•¥åœ¨è·¨é¢†åŸŸæ³›åŒ–æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè€ŒMTL-LoRAåœ¨åŸŸå†…é…ç½®ä¸‹å–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.00662v1",
      "published_date": "2025-10-01 08:44:05 UTC",
      "updated_date": "2025-10-01 08:44:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:02:57.566576+00:00"
    },
    {
      "arxiv_id": "2510.00658v1",
      "title": "Align Your Tangent: Training Better Consistency Models via Manifold-Aligned Tangents",
      "title_zh": "Align Your Tangentï¼šé€šè¿‡æµå½¢å¯¹é½åˆ‡çº¿ä¼˜åŒ–ä¸€è‡´æ€§æ¨¡å‹è®­ç»ƒ",
      "authors": [
        "Beomsu Kim",
        "Byunghee Cha",
        "Jong Chul Ye"
      ],
      "abstract": "With diffusion and flow matching models achieving state-of-the-art generating performance, the interest of the community now turned to reducing the inference time without sacrificing sample quality. Consistency Models (CMs), which are trained to be consistent on diffusion or probability flow ordinary differential equation (PF-ODE) trajectories, enable one or two-step flow or diffusion sampling. However, CMs typically require prolonged training with large batch sizes to obtain competitive sample quality. In this paper, we examine the training dynamics of CMs near convergence and discover that CM tangents -- CM output update directions -- are quite oscillatory, in the sense that they move parallel to the data manifold, not towards the manifold. To mitigate oscillatory tangents, we propose a new loss function, called the manifold feature distance (MFD), which provides manifold-aligned tangents that point toward the data manifold. Consequently, our method -- dubbed Align Your Tangent (AYT) -- can accelerate CM training by orders of magnitude and even out-perform the learned perceptual image patch similarity metric (LPIPS). Furthermore, we find that our loss enables training with extremely small batch sizes without compromising sample quality. Code: https://github.com/1202kbs/AYT",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Consistency Models (CMs)åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ”¶æ•›åŠ¨åŠ›å­¦ä¸ä½³çš„é—®é¢˜ï¼Œå‘ç°å…¶è¾“å‡ºæ›´æ–°æ–¹å‘ï¼ˆtangentsï¼‰å¸¸è¡¨ç°ä¸ºæ²¿æ•°æ®æµå½¢ï¼ˆdata manifoldï¼‰å¹³è¡Œçš„æŒ¯è¡ï¼Œè€Œéç›´æ¥æŒ‡å‘æµå½¢ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Align Your Tangent (AYT)æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥ä¸€ç§åä¸ºManifold Feature Distance (MFD)çš„æ–°æŸå¤±å‡½æ•°æ¥æä¾›æµå½¢å¯¹é½çš„åˆ‡çº¿ã€‚MFDç¡®ä¿åˆ‡çº¿æ–¹å‘å§‹ç»ˆæŒ‡å‘æ•°æ®æµå½¢ï¼Œä»è€Œæ˜¾è‘—åŠ å¿«äº†æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦ã€‚å®éªŒè¯æ˜ï¼ŒAYTä¸ä»…èƒ½å°†CMsçš„è®­ç»ƒæ•ˆç‡æé«˜æ•°ä¸ªæ•°é‡çº§ï¼Œå…¶ç”Ÿæˆæ•ˆæœç”šè‡³ä¼˜äºLPIPSæŒ‡æ ‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å…è®¸åœ¨æå°çš„æ‰¹é‡å¤§å°ä¸‹è¿›è¡Œè®­ç»ƒè€Œä¸æŸå¤±æ ·æœ¬è´¨é‡ï¼Œä¸ºé«˜æ•ˆè®­ç»ƒé«˜æ€§èƒ½ç”Ÿæˆæ¨¡å‹æä¾›äº†æ–°çš„ç†è®ºæ”¯æŒä¸å®è·µæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2510.00658v1",
      "published_date": "2025-10-01 08:35:18 UTC",
      "updated_date": "2025-10-01 08:35:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:02:59.776699+00:00"
    },
    {
      "arxiv_id": "2510.00636v1",
      "title": "Expected Attention: KV Cache Compression by Estimating Attention from Future Queries Distribution",
      "title_zh": "Expected Attentionï¼šåŸºäºæœªæ¥æŸ¥è¯¢åˆ†å¸ƒä¼°è®¡æ³¨æ„åŠ›å¾—åˆ†çš„ KV ç¼“å­˜å‹ç¼©",
      "authors": [
        "Alessio Devoto",
        "Maximilian Jeblick",
        "Simon JÃ©gou"
      ],
      "abstract": "Memory consumption of the Key-Value (KV) cache represents a major bottleneck for efficient large language model inference. While attention-score-based KV cache pruning shows promise, it faces critical practical limitations: attention scores from future tokens are unavailable during compression, and modern implementations like Flash Attention do not materialize the full attention matrix, making past scores inaccessible. To overcome these challenges, we introduce $\\textbf{Expected Attention, a training-free compression method}$ that estimates KV pairs importance by predicting how future queries will attend to them. Our approach leverages the distributional properties of LLM activations to compute expected attention scores in closed form for each KV pair. These scores enable principled ranking and pruning of KV pairs with minimal impact on the residual stream, achieving effective compression without performance degradation. Importantly, our method operates seamlessly across both prefilling and decoding phases, consistently outperforming state-of-the-art baselines in both scenarios. Finally, $\\textbf{we release KVPress, a comprehensive library to enable researchers to implement and benchmark KV cache compression methods, already including more than 20 techniques}$.",
      "tldr_zh": "é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†ä¸­ Key-Value (KV) cache å ç”¨å†…å­˜è¿‡å¤§çš„ç“¶é¢ˆï¼Œè¯¥ç ”ç©¶åˆ†æäº†åŸºäºæ³¨æ„åŠ›åˆ†æ•°çš„å‰ªææ–¹æ³•åœ¨é¢å¯¹æœªæ¥æŸ¥è¯¢ä¸å¯çŸ¥ä»¥åŠ Flash Attention æ— æ³•è·å–å®Œæ•´æ³¨æ„åŠ›çŸ©é˜µæ—¶çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åä¸º Expected Attention çš„æ— éœ€è®­ç»ƒçš„å‹ç¼©æ–¹æ³•ï¼Œé€šè¿‡é¢„æµ‹æœªæ¥æŸ¥è¯¢å¯¹å½“å‰ KV å¯¹çš„å…³æ³¨åº¦æ¥è¯„ä¼°å…¶é‡è¦æ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹æ¿€æ´»å€¼çš„åˆ†å¸ƒç‰¹æ€§ï¼Œä¸ºæ¯ä¸ª KV å¯¹è®¡ç®—é—­å¼è§£ (closed form) å½¢å¼çš„é¢„æœŸæ³¨æ„åŠ›åˆ†æ•°ï¼Œä»è€Œåœ¨å¯¹æ®‹å·®æµ (residual stream) å½±å“æœ€å°çš„æƒ…å†µä¸‹å®ç°æœ‰åŸåˆ™çš„æ’åºå’Œå‰ªæã€‚Expected Attention èƒ½å¤Ÿæ— ç¼åº”ç”¨äºé¢„å¡«å…… (prefilling) å’Œè§£ç  (decoding) é˜¶æ®µï¼Œåœ¨ä¸¤ç§åœºæ™¯ä¸‹çš„è¡¨ç°å‡ä¼˜äºç°æœ‰çš„å…ˆè¿›åŸºçº¿æ¨¡å‹ï¼Œä¸”æœ‰æ•ˆé¿å…äº†æ€§èƒ½ä¸‹é™ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å‘å¸ƒäº†åä¸º KVPress çš„ç»¼åˆåº“ï¼Œå…¶ä¸­é›†æˆäº† 20 å¤šç§å‹ç¼©æŠ€æœ¯ï¼Œä¸ºè¯¥é¢†åŸŸçš„å¼€å‘å’ŒåŸºå‡†æµ‹è¯•æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00636v1",
      "published_date": "2025-10-01 08:12:14 UTC",
      "updated_date": "2025-10-01 08:12:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:03:00.268053+00:00"
    },
    {
      "arxiv_id": "2510.00629v2",
      "title": "Tenyidie Syllabification corpus creation and deep learning applications",
      "title_zh": "Tenyidieè¯­éŸ³èŠ‚åˆ’åˆ†è¯­æ–™åº“æ„å»ºåŠæ·±åº¦å­¦ä¹ åº”ç”¨",
      "authors": [
        "Teisovi Angami",
        "Kevisino Khate"
      ],
      "abstract": "The Tenyidie language is a low-resource language of the Tibeto-Burman family spoken by the Tenyimia Community of Nagaland in the north-eastern part of India and is considered a major language in Nagaland. It is tonal, Subject-Object-Verb, and highly agglutinative in nature. Being a low-resource language, very limited research on Natural Language Processing (NLP) has been conducted. To the best of our knowledge, no work on syllabification has been reported for this language. Among the many NLP tasks, syllabification or syllabication is an important task in which the given word syllables are identified. The contribution of this work is the creation of 10,120 syllabified Tenyidie words and the application of the Deep Learning techniques on the created corpus. In this paper, we have applied LSTM, BLSTM, BLSTM+CRF, and Encoder-decoder deep learning architectures on our created dataset. In our dataset split of 80:10:10 (train:validation:test) set, we achieved the highest accuracy of 99.21% with BLSTM model on the test set. This work will find its application in numerous other NLP applications, such as morphological analysis, part-of-speech tagging, machine translation, etc, for the Tenyidie Language.\n  Keywords: Tenyidie; NLP; syllabification; deep learning; LSTM; BLSTM; CRF; Encoder-decoder",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹å°åº¦ä¸œåŒ—éƒ¨çº³åŠ å…°é‚¦çš„ä¸€ç§ä½èµ„æºè¯­è¨€ Tenyidieï¼Œå¡«è¡¥äº†å…¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸéŸ³èŠ‚åˆ’åˆ†ï¼ˆSyllabificationï¼‰ç ”ç©¶çš„ç©ºç™½ã€‚ç ”ç©¶çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºåˆ›å»ºäº†ä¸€ä¸ªåŒ…å« 10,120 ä¸ªå·²åˆ’åˆ†éŸ³èŠ‚çš„ Tenyidie å•è¯è¯­æ–™åº“ï¼Œå¹¶é¦–æ¬¡å°†æ·±åº¦å­¦ä¹ æŠ€æœ¯åº”ç”¨äºè¯¥è¯­è¨€çš„éŸ³èŠ‚è¯†åˆ«ä»»åŠ¡ã€‚ä½œè€…åœ¨æ•°æ®é›†ä¸Šæµ‹è¯•äº† LSTMã€BLSTMã€BLSTM+CRF ä»¥åŠ Encoder-decoder ç­‰å¤šç§æ¨¡å‹æ¶æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ 80:10:10 çš„æ•°æ®åˆ’åˆ†ä¸‹ï¼ŒBLSTM æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šå–å¾—äº† 99.21% çš„æœ€é«˜å‡†ç¡®ç‡ã€‚è¯¥ç ”ç©¶æˆæœä¸ä»…éªŒè¯äº†æ·±åº¦å­¦ä¹ åœ¨ä½èµ„æºè¯­è¨€å¤„ç†ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¹Ÿä¸º Tenyidie è¯­è¨€çš„å½¢æ€åˆ†æã€è¯æ€§æ ‡æ³¨å’Œæœºå™¨ç¿»è¯‘ç­‰åç»­ NLP ä»»åŠ¡å¥ å®šäº†å…³é”®åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "17 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.00629v2",
      "published_date": "2025-10-01 08:00:59 UTC",
      "updated_date": "2025-10-02 13:18:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:03:02.876646+00:00"
    },
    {
      "arxiv_id": "2510.00627v1",
      "title": "Collaborative-Distilled Diffusion Models (CDDM) for Accelerated and Lightweight Trajectory Prediction",
      "title_zh": "é¢å‘åŠ é€Ÿä¸è½»é‡åŒ–è½¨è¿¹é¢„æµ‹çš„ååŒè’¸é¦æ‰©æ•£æ¨¡å‹ (CDDM)",
      "authors": [
        "Bingzhang Wang",
        "Kehua Chen",
        "Yinhai Wang"
      ],
      "abstract": "Trajectory prediction is a fundamental task in Autonomous Vehicles (AVs) and Intelligent Transportation Systems (ITS), supporting efficient motion planning and real-time traffic safety management. Diffusion models have recently demonstrated strong performance in probabilistic trajectory prediction, but their large model size and slow sampling process hinder real-world deployment. This paper proposes Collaborative-Distilled Diffusion Models (CDDM), a novel method for real-time and lightweight trajectory prediction. Built upon Collaborative Progressive Distillation (CPD), CDDM progressively transfers knowledge from a high-capacity teacher diffusion model to a lightweight student model, jointly reducing both the number of sampling steps and the model size across distillation iterations. A dual-signal regularized distillation loss is further introduced to incorporate guidance from both the teacher and ground-truth data, mitigating potential overfitting and ensuring robust performance. Extensive experiments on the ETH-UCY pedestrian benchmark and the nuScenes vehicle benchmark demonstrate that CDDM achieves state-of-the-art prediction accuracy. The well-distilled CDDM retains 96.2% and 95.5% of the baseline model's ADE and FDE performance on pedestrian trajectories, while requiring only 231K parameters and 4 or 2 sampling steps, corresponding to 161x compression, 31x acceleration, and 9 ms latency. Qualitative results further show that CDDM generates diverse and accurate trajectories under dynamic agent behaviors and complex social interactions. By bridging high-performing generative models with practical deployment constraints, CDDM enables resource-efficient probabilistic prediction for AVs and ITS. Code is available at https://github.com/bingzhangw/CDDM.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Collaborative-Distilled Diffusion Models (CDDM)ï¼Œæ—¨åœ¨è§£å†³Diffusion modelsåœ¨è½¨è¿¹é¢„æµ‹ä¸­å› æ¨¡å‹å‚æ•°é‡å¤§å’Œé‡‡æ ·é€Ÿåº¦æ…¢è€Œéš¾ä»¥åœ¨è‡ªåŠ¨é©¾é©¶(AV)ä¸­å®æ—¶éƒ¨ç½²çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é‡‡ç”¨Collaborative Progressive Distillation (CPD)æŠ€æœ¯ï¼Œå°†é«˜å®¹é‡æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†é€æ­¥è¿ç§»è‡³è½»é‡åŒ–å­¦ç”Ÿæ¨¡å‹ï¼Œä»è€Œåœ¨è¿­ä»£è¿‡ç¨‹ä¸­åŒæ—¶å‡å°‘é‡‡æ ·æ­¥æ•°å¹¶ç¼©å°æ¨¡å‹è§„æ¨¡ã€‚é€šè¿‡å¼•å…¥åŒä¿¡å·æ­£åˆ™åŒ–è’¸é¦æŸå¤±(dual-signal regularized distillation loss)ï¼Œè¯¥æ¨¡å‹èƒ½å¤ŸåŒæ—¶åˆ©ç”¨æ•™å¸ˆæŒ‡å¯¼å’Œåœ°é¢çœŸå€¼(ground-truth)æ•°æ®ï¼Œç¡®ä¿äº†é¢„æµ‹æ€§èƒ½çš„é²æ£’æ€§ã€‚å®éªŒè¯æ˜CDDMåœ¨ETH-UCYå’ŒnuScenesåŸºå‡†æµ‹è¯•ä¸Šå‡è¾¾åˆ°äº†state-of-the-artæ°´å¹³ï¼Œåœ¨ä»…æœ‰231Kå‚æ•°çš„æƒ…å†µä¸‹å®ç°äº†161å€çš„å‹ç¼©ç‡å’Œ31å€çš„é‡‡æ ·åŠ é€Ÿã€‚å…¶åœ¨ä»…éœ€2æˆ–4æ­¥é‡‡æ ·çš„æƒ…å†µä¸‹èƒ½ç»´æŒåŸºçº¿æ¨¡å‹95%ä»¥ä¸Šçš„ADEå’ŒFDEæ€§èƒ½ï¼Œæ¨ç†å»¶è¿Ÿä½è‡³9 msï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆå¼æ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„å®ç”¨æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00627v1",
      "published_date": "2025-10-01 08:00:31 UTC",
      "updated_date": "2025-10-01 08:00:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:03:16.677479+00:00"
    },
    {
      "arxiv_id": "2510.00625v1",
      "title": "Is Model Editing Built on Sand? Revealing Its Illusory Success and Fragile Foundation",
      "title_zh": "æ¨¡å‹ç¼–è¾‘æ˜¯å¦æ ¹åŸºä¸ç‰¢ï¼Ÿæ­ç¤ºå…¶è™šå‡æˆåŠŸä¸è„†å¼±åŸºç¡€",
      "authors": [
        "Wei Liu",
        "Haomei Xu",
        "Bingqing Liu",
        "Zhiying Deng",
        "Haozhao Wang",
        "Jun Wang",
        "Ruixuan Li",
        "Yee Whye Teh",
        "Wee Sun Lee"
      ],
      "abstract": "Large language models (LLMs) inevitably encode outdated or incorrect knowledge. Updating, deleting, and forgetting such knowledge is important for alignment, safety, and other issues. To address this issue, model editing has emerged as a promising paradigm: by precisely editing a small subset of parameters such that a specific fact is updated while preserving other knowledge. Despite its great success reported in previous papers, we find the apparent reliability of editing rests on a fragile foundation and the current literature is largely driven by illusory success. The fundamental goal of steering the model's output toward a target with minimal modification would encourage exploiting hidden shortcuts, rather than utilizing real semantics. This problem directly challenges the feasibility of the current model editing literature at its very foundation, as shortcuts are inherently at odds with robust knowledge integration. Coincidentally, this issue has long been obscured by evaluation frameworks that lack the design of negative examples. To uncover it, we systematically develop a suite of new evaluation methods. Strikingly, we find that state-of-the-art approaches collapse even under the simplest negation queries. Our empirical evidence shows that editing is likely to be based on shortcuts rather than full semantics, calling for an urgent reconsideration of the very basis of model editing before further advancements can be meaningfully pursued.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹å¤§è¯­è¨€æ¨¡å‹(Large Language Models)ä¸­çš„æ¨¡å‹ç¼–è¾‘(Model Editing)æŠ€æœ¯è¿›è¡Œäº†æ‰¹åˆ¤æ€§å®¡æŸ¥ï¼ŒæŒ‡å‡ºå½“å‰æ–‡çŒ®ä¸­æ‰€æŠ¥é“çš„æˆåŠŸåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæºäºâ€œå¹»è§‰â€ä¸”åŸºç¡€è„†å¼±ã€‚ä½œè€…å‘ç°ï¼Œæ¨¡å‹ç¼–è¾‘å€¾å‘äºåˆ©ç”¨éšè—çš„å¿«æ·æ–¹å¼(shortcuts)æ¥æ”¹å˜è¾“å‡ºï¼Œè€Œéå®ç°çœŸæ­£çš„è¯­ä¹‰çŸ¥è¯†æ•´åˆï¼Œè¿™ä¸€å‘ç°ç›´æ¥æŒ‘æˆ˜äº†å½“å‰æ¨¡å‹ç¼–è¾‘èŒƒå¼çš„å¯è¡Œæ€§ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè¿™ç§ç¼ºé™·é•¿æœŸè¢«ç¼ºä¹è´Ÿä¾‹è®¾è®¡çš„è¯„ä¼°æ¡†æ¶æ‰€æ©ç›–ï¼Œä¸ºæ­¤ä½œè€…å¼€å‘äº†ä¸€å¥—å…¨æ–°çš„è¯„ä¼°æ–¹æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä¾¿æ˜¯ç›®å‰æœ€å…ˆè¿›çš„ç¼–è¾‘æ–¹æ³•åœ¨é¢å¯¹ç®€å•çš„å¦å®šæŸ¥è¯¢æ—¶ä¹Ÿä¼šå¤±æ•ˆã€‚å®è¯è¯æ®è¡¨æ˜ï¼Œç°æœ‰çš„æ¨¡å‹ç¼–è¾‘é«˜åº¦ä¾èµ–å¿«æ·æ–¹å¼è€Œéå®Œæ•´è¯­ä¹‰ï¼Œè¯¥ç ”ç©¶å‘¼ååœ¨ç»§ç»­æ¨è¿›ç›¸å…³æŠ€æœ¯å‰ï¼Œå¿…é¡»ç´§æ€¥é‡æ–°å®¡è§†æ¨¡å‹ç¼–è¾‘çš„æ ¹æœ¬åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "This is a work in progress. Comments and suggestions are welcome",
      "pdf_url": "https://arxiv.org/pdf/2510.00625v1",
      "published_date": "2025-10-01 07:59:23 UTC",
      "updated_date": "2025-10-01 07:59:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:03:18.436652+00:00"
    },
    {
      "arxiv_id": "2510.00621v1",
      "title": "FAME: Adaptive Functional Attention with Expert Routing for Function-on-Function Regression",
      "title_zh": "FAMEï¼šé¢å‘å‡½æ•°å¯¹å‡½æ•°å›å½’çš„ä¸“å®¶è·¯ç”±è‡ªé€‚åº”å‡½æ•°å¼æ³¨æ„åŠ›æœºåˆ¶",
      "authors": [
        "Yifei Gao",
        "Yong Chen",
        "Chen Zhang"
      ],
      "abstract": "Functional data play a pivotal role across science and engineering, yet their infinite-dimensional nature makes representation learning challenging. Conventional statistical models depend on pre-chosen basis expansions or kernels, limiting the flexibility of data-driven discovery, while many deep-learning pipelines treat functions as fixed-grid vectors, ignoring inherent continuity. In this paper, we introduce Functional Attention with a Mixture-of-Experts (FAME), an end-to-end, fully data-driven framework for function-on-function regression. FAME forms continuous attention by coupling a bidirectional neural controlled differential equation with MoE-driven vector fields to capture intra-functional continuity, and further fuses change to inter-functional dependencies via multi-head cross attention. Extensive experiments on synthetic and real-world functional-regression benchmarks show that FAME achieves state-of-the-art accuracy, strong robustness to arbitrarily sampled discrete observations of functions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FAMEï¼ˆAdaptive Functional Attention with Expert Routingï¼‰ï¼Œä¸€ä¸ªä¸“ä¸ºFunction-on-Function Regressionè®¾è®¡çš„ç«¯åˆ°ç«¯å…¨æ•°æ®é©±åŠ¨æ¡†æ¶ã€‚FAMEé€šè¿‡å°†åŒå‘Neural Controlled Differential Equation (NCDE)ä¸MoEé©±åŠ¨çš„å‘é‡åœºç›¸ç»“åˆæ¥æ„å»ºè¿ç»­æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»è€Œæœ‰æ•ˆæ•æ‰å‡½æ•°å†…éƒ¨çš„è¿ç»­æ€§ï¼ˆintra-functional continuityï¼‰ã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤šå¤´äº¤å‰æ³¨æ„åŠ›ï¼ˆmulti-head cross attentionï¼‰æ¥æ•è·å‡½æ•°é—´çš„ä¾èµ–å…³ç³»ï¼Œè§£å†³äº†ä¼ ç»Ÿæ¨¡å‹ä¾èµ–é¢„é€‰åŸºæ‰©å¼ æˆ–å¿½ç•¥å‡½æ•°å›ºæœ‰è¿ç»­æ€§çš„å±€é™ã€‚åœ¨åˆæˆåŠçœŸå®ä¸–ç•ŒåŠŸèƒ½å›å½’åŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒè¡¨æ˜ï¼ŒFAMEå®ç°äº†State-of-the-artçš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å¯¹ä»»æ„é‡‡æ ·çš„ç¦»æ•£è§‚æµ‹å…·æœ‰æå¼ºçš„é²æ£’æ€§ï¼Œè¯æ˜äº†å…¶åœ¨ç§‘å­¦å’Œå·¥ç¨‹é¢†åŸŸå¤„ç†æ— é™ç»´å‡½æ•°æ•°æ®è¡¨ç¤ºå­¦ä¹ çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00621v1",
      "published_date": "2025-10-01 07:53:55 UTC",
      "updated_date": "2025-10-01 07:53:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:03:28.175522+00:00"
    },
    {
      "arxiv_id": "2510.00620v1",
      "title": "HARPA: A Testability-Driven, Literature-Grounded Framework for Research Ideation",
      "title_zh": "HARPAï¼šä¸€ç§ä»¥å¯æµ‹è¯•æ€§ä¸ºå¯¼å‘ã€åŸºäºæ–‡çŒ®çš„ç§‘ç ”æ„æ€æ¡†æ¶",
      "authors": [
        "Rosni Vasu",
        "Peter Jansen",
        "Pao Siangliulue",
        "Cristina Sarasua",
        "Abraham Bernstein",
        "Peter Clark",
        "Bhavana Dalvi Mishra"
      ],
      "abstract": "While there has been a surge of interest in automated scientific discovery (ASD), especially with the emergence of LLMs, it remains challenging for tools to generate hypotheses that are both testable and grounded in the scientific literature. Additionally, existing ideation tools are not adaptive to prior experimental outcomes. We developed HARPA to address these challenges by incorporating the ideation workflow inspired by human researchers. HARPA first identifies emerging research trends through literature mining, then explores hypothesis design spaces, and finally converges on precise, testable hypotheses by pinpointing research gaps and justifying design choices. Our evaluations show that HARPA-generated hypothesis-driven research proposals perform comparably to a strong baseline AI-researcher across most qualitative dimensions (e.g., specificity, novelty, overall quality), but achieve significant gains in feasibility(+0.78, p$<0.05$, bootstrap) and groundedness (+0.85, p$<0.01$, bootstrap) on a 10-point Likert scale. When tested with the ASD agent (CodeScientist), HARPA produced more successful executions (20 vs. 11 out of 40) and fewer failures (16 vs. 21 out of 40), showing that expert feasibility judgments track with actual execution success. Furthermore, to simulate how researchers continuously refine their understanding of what hypotheses are both testable and potentially interesting from experience, HARPA learns a reward model that scores new hypotheses based on prior experimental outcomes, achieving approx. a 28\\% absolute gain over HARPA's untrained baseline scorer. Together, these methods represent a step forward in the field of AI-driven scientific discovery.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HARPAï¼Œä¸€ä¸ªä»¥å¯æµ‹è¯•æ€§(testability)ä¸ºé©±åŠ¨ã€åŸºäºæ–‡çŒ®çš„ç§‘ç ”æ„æ€æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è‡ªåŠ¨åŒ–ç§‘å­¦å‘ç°(ASD)å·¥å…·åœ¨ç”Ÿæˆå¯æµ‹è¯•ä¸”æœ‰æ®å¯æŸ¥çš„å‡è®¾æ–¹é¢å­˜åœ¨çš„æŒ‘æˆ˜ã€‚HARPAæ¨¡ä»¿äººç±»ç ”ç©¶è€…çš„å·¥ä½œæµç¨‹ï¼Œé€šè¿‡æ–‡çŒ®æŒ–æ˜(literature mining)è¯†åˆ«ç ”ç©¶è¶‹åŠ¿ï¼Œå¹¶åœ¨æ¢ç´¢å‡è®¾ç©ºé—´åï¼Œé€šè¿‡é”å®šç ”ç©¶ç©ºç™½æ¥ç”Ÿæˆç²¾ç¡®ä¸”å¯æµ‹è¯•çš„å‡è®¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨10åˆ†Likerté‡è¡¨ä¸Šï¼ŒHARPAç”Ÿæˆçš„ææ¡ˆåœ¨å¯è¡Œæ€§å’Œå¯é æ€§(groundedness)æ–¹é¢æ˜¾è‘—ä¼˜äºå¼ºåŸºçº¿AIç ”ç©¶è€…ã€‚æ­¤å¤–ï¼Œå½“ä¸ASDæ™ºèƒ½ä½“CodeScientisté…åˆä½¿ç”¨æ—¶ï¼ŒHARPAæ˜¾è‘—æé«˜äº†å®éªŒæ‰§è¡Œçš„æˆåŠŸç‡å¹¶å‡å°‘äº†å¤±è´¥ï¼Œè¯æ˜ä¸“å®¶å¯è¡Œæ€§åˆ¤æ–­ä¸å®é™…æ‰§è¡Œç»“æœä¸€è‡´ã€‚è¯¥æ¡†æ¶è¿˜å¼•å…¥äº†å­¦ä¹ å¥–åŠ±æ¨¡å‹(reward model)ï¼Œèƒ½å¤Ÿæ ¹æ®å…ˆå‰çš„å®éªŒç»“æœå¯¹æ–°å‡è®¾è¿›è¡Œè¯„åˆ†ï¼Œç›¸æ¯”åŸºå‡†æ–¹æ¡ˆè·å¾—äº†çº¦28%çš„ç»å¯¹æ€§èƒ½æå‡ï¼Œä¸ºAIé©±åŠ¨çš„ç§‘å­¦å‘ç°é¢†åŸŸè¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages (main), 65 pages total",
      "pdf_url": "https://arxiv.org/pdf/2510.00620v1",
      "published_date": "2025-10-01 07:52:19 UTC",
      "updated_date": "2025-10-01 07:52:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:03:25.180115+00:00"
    },
    {
      "arxiv_id": "2510.00619v1",
      "title": "What Did I Learn? Operational Competence Assessment for AI-Based Trajectory Planners",
      "title_zh": "â€œæˆ‘å­¦åˆ°äº†ä»€ä¹ˆï¼Ÿâ€ï¼šåŸºäºäººå·¥æ™ºèƒ½çš„è½¨è¿¹è§„åˆ’å™¨æ“ä½œèƒœä»»åŠ›è¯„ä¼°",
      "authors": [
        "Michiel Braat",
        "Maren Buermann",
        "Marijke van Weperen",
        "Jan-Pieter Paardekooper"
      ],
      "abstract": "Automated driving functions increasingly rely on machine learning for tasks like perception and trajectory planning, requiring large, relevant datasets. The performance of these algorithms depends on how closely the training data matches the task. To ensure reliable functioning, it is crucial to know what is included in the dataset to assess the trained model's operational risk. We aim to enhance the safe use of machine learning in automated driving by developing a method to recognize situations that an automated vehicle has not been sufficiently trained on. This method also improves explainability by describing the dataset at a human-understandable level. We propose modeling driving data as knowledge graphs, representing driving scenes with entities and their relationships. These graphs are queried for specific sub-scene configurations to check their occurrence in the dataset. We estimate a vehicle's competence in a driving scene by considering the coverage and complexity of sub-scene configurations in the training set. Higher complexity scenes require greater coverage for high competence. We apply this method to the NuPlan dataset, modeling it with knowledge graphs and analyzing the coverage of specific driving scenes. This approach helps monitor the competence of machine learning models trained on the dataset, which is essential for trustworthy AI to be deployed in automated driving.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäº machine learning çš„ trajectory planners åœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„è¿è¡Œé£é™©ï¼Œæå‡ºäº†ä¸€ç§è¯†åˆ«æ¨¡å‹æœªå……åˆ†è®­ç»ƒåœºæ™¯çš„æ–¹æ³•ï¼Œä»¥å¢å¼ºç³»ç»Ÿçš„å®‰å…¨æ€§ã€‚ç ”ç©¶è€…é€šè¿‡å°†é©¾é©¶æ•°æ®å»ºæ¨¡ä¸º Knowledge Graphsï¼Œåˆ©ç”¨å®ä½“åŠå…¶å…³ç³»ç²¾ç¡®è¡¨å¾é©¾é©¶åœºæ™¯ï¼Œå¹¶é€šè¿‡æŸ¥è¯¢ç‰¹å®šçš„ sub-scene configurations æ¥è¯„ä¼°å…¶åœ¨æ•°æ®é›†ä¸­çš„è¦†ç›–æƒ…å†µã€‚æ ¸å¿ƒè´¡çŒ®åœ¨äºå»ºç«‹äº†ä¸€å¥— operational competence è¯„ä¼°æœºåˆ¶ï¼Œè¯¥æœºåˆ¶ç»¼åˆè€ƒè™‘äº† sub-scenes çš„è¦†ç›–ç‡(coverage)å’Œå¤æ‚åº¦(complexity)ï¼Œå¹¶è®¤ä¸ºé«˜å¤æ‚åº¦åœºæ™¯éœ€è¦æ›´é«˜çš„è¦†ç›–ç‡ä»¥ç¡®ä¿èƒœä»»åŠ›ã€‚åœ¨ NuPlan æ•°æ®é›†ä¸Šçš„åº”ç”¨è¯æ˜äº†è¯¥æ–¹æ³•èƒ½å¤Ÿä»¥äººç±»å¯ç†è§£çš„æ°´å¹³æè¿°æ•°æ®é›†ï¼Œæœ‰æ•ˆç›‘æµ‹æ¨¡å‹çš„è¿è¡Œèƒ½åŠ›ã€‚è¯¥æ–¹æ¡ˆæ˜¾è‘—æå‡äº†è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„ explainabilityï¼Œä¸ºéƒ¨ç½²å¯ä¿¡èµ–çš„ AI æŠ€æœ¯æä¾›äº†å…³é”®æ”¯æŒã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted for publication in proceedings of the 2025 IEEE International Automated Vehicle Validation Conference",
      "pdf_url": "https://arxiv.org/pdf/2510.00619v1",
      "published_date": "2025-10-01 07:46:50 UTC",
      "updated_date": "2025-10-01 07:46:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:03:28.371314+00:00"
    },
    {
      "arxiv_id": "2510.00615v2",
      "title": "ACON: Optimizing Context Compression for Long-horizon LLM Agents",
      "title_zh": "ACONï¼šé¢å‘é•¿ç¨‹å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“çš„ä¸Šä¸‹æ–‡å‹ç¼©ä¼˜åŒ–",
      "authors": [
        "Minki Kang",
        "Wei-Ning Chen",
        "Dongge Han",
        "Huseyin A. Inan",
        "Lukas Wutschitz",
        "Yanzhi Chen",
        "Robert Sim",
        "Saravan Rajmohan"
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed as agents in dynamic, real-world environments, where success requires both reasoning and effective tool use. A central challenge for agentic tasks is the growing context length, as agents must accumulate long histories of actions and observations. This expansion raises costs and reduces efficiency in long-horizon tasks, yet prior work on context compression has mostly focused on single-step tasks or narrow applications. We introduce Agent Context Optimization (ACON), a unified framework that optimally compresses both environment observations and interaction histories into concise yet informative condensations. ACON leverages compression guideline optimization in natural language space: given paired trajectories where full context succeeds but compressed context fails, capable LLMs analyze the causes of failure, and the compression guideline is updated accordingly. Furthermore, we propose distilling the optimized LLM compressor into smaller models to reduce the overhead of the additional module. Experiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON reduces memory usage by 26-54% (peak tokens) while largely preserving task performance, preserves over 95% of accuracy when distilled into smaller compressors, and enhances smaller LMs as long-horizon agents with up to 46% performance improvement. Our code is available at https://github.com/microsoft/acon.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) ä½œä¸ºæ™ºèƒ½ä½“åœ¨å¤„ç†é•¿æ—¶ç¨‹ (long-horizon) ä»»åŠ¡æ—¶é¢ä¸´çš„ä¸Šä¸‹æ–‡é•¿åº¦æ¿€å¢ã€æˆæœ¬é«˜åŠæ•ˆç‡ä½çš„é—®é¢˜ï¼Œæå‡ºäº† Agent Context Optimization (ACON) æ¡†æ¶ã€‚ACON æ˜¯ä¸€ç§ç»Ÿä¸€çš„å‹ç¼©æ¡†æ¶ï¼Œèƒ½å¤Ÿå°†ç¯å¢ƒè§‚å¯Ÿå’Œäº¤äº’å†å²ä¼˜åŒ–å‹ç¼©ä¸ºç®€æ´ä¸”ä¿¡æ¯ä¸°å¯Œçš„æ€»ç»“å†…å®¹ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†è‡ªç„¶è¯­è¨€ç©ºé—´ä¸‹çš„å‹ç¼©å‡†åˆ™ä¼˜åŒ–æŠ€æœ¯ï¼Œé€šè¿‡è®©å…·å¤‡èƒ½åŠ›çš„ LLMs å¯¹æ¯”åˆ†æå®Œæ•´ä¸Šä¸‹æ–‡æˆåŠŸä¸å‹ç¼©ä¸Šä¸‹æ–‡å¤±è´¥çš„è½¨è¿¹ï¼Œè¿›è€ŒåŠ¨æ€æ›´æ–°å‹ç¼©å‡†åˆ™ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡å°†ä¼˜åŒ–åçš„ LLM å‹ç¼©å™¨è’¸é¦ (distilling) åˆ°æ›´å°çš„æ¨¡å‹ä¸­ï¼Œæ˜¾è‘—é™ä½äº†é¢å¤–æ¨¡å—å¸¦æ¥çš„è®¡ç®—å¼€é”€ã€‚åœ¨ AppWorld å’Œ OfficeBench ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒACON åœ¨å‡å°‘ 26-54% å³°å€¼ Token å†…å­˜å ç”¨çš„åŒæ—¶ï¼ŒåŸºæœ¬ä¿æŒäº†åŸæœ‰ä»»åŠ¡æ€§èƒ½ã€‚å®éªŒç»“æœè¿›ä¸€æ­¥è¯æ˜ï¼Œåœ¨è’¸é¦è‡³å°å‹å‹ç¼©å™¨åï¼Œè¯¥æ–¹æ³•ä¸ä»…ä¿ç•™äº† 95% ä»¥ä¸Šçš„å‡†ç¡®ç‡ï¼Œè¿˜å°†å°å‹æ¨¡å‹ä½œä¸ºé•¿æ—¶ç¨‹æ™ºèƒ½ä½“çš„æ€§èƒ½æå‡äº†é«˜è¾¾ 46%ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2510.00615v2",
      "published_date": "2025-10-01 07:43:49 UTC",
      "updated_date": "2025-10-17 06:48:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:03:30.484061+00:00"
    },
    {
      "arxiv_id": "2510.00600v1",
      "title": "Hybrid Training for Vision-Language-Action Models",
      "title_zh": "è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„æ··åˆè®­ç»ƒ",
      "authors": [
        "Pietro Mazzaglia",
        "Cansu Sancaktar",
        "Markus Peschl",
        "Daniel Dijkman"
      ],
      "abstract": "Using Large Language Models to produce intermediate thoughts, a.k.a. Chain-of-thought (CoT), before providing an answer has been a successful recipe for solving complex language tasks. In robotics, similar embodied CoT strategies, generating thoughts before actions, have also been shown to lead to improved performance when using Vision-Language-Action models (VLAs). As these techniques increase the length of the model's generated outputs to include the thoughts, the inference time is negatively affected. Delaying an agent's actions in real-world executions, as in robotic manipulation settings, strongly affects the usability of a method, as tasks require long sequences of actions. However, is the generation of long chains-of-thought a strong prerequisite for achieving performance improvements? In this work, we explore the idea of Hybrid Training (HyT), a framework that enables VLAs to learn from thoughts and benefit from the associated performance gains, while enabling the possibility to leave out CoT generation during inference. Furthermore, by learning to conditionally predict a diverse set of outputs, HyT supports flexibility at inference time, enabling the model to either predict actions directly, generate thoughts or follow instructions. We evaluate the proposed method in a series of simulated benchmarks and real-world experiments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ (Vision-Language-Action models, VLAs) åœ¨æœºå™¨äººé¢†åŸŸåº”ç”¨æ—¶ï¼Œé“¾å¼æ€ç»´ (Chain-of-thought, CoT) å¸¦æ¥çš„æ¨ç†å»¶è¿Ÿé—®é¢˜è¿›è¡Œäº†æ¢è®¨ã€‚è™½ç„¶ CoT èƒ½æ˜¾è‘—æå‡æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä½†å…¶ç”Ÿæˆçš„å†—é•¿æ¨ç†åºåˆ—ä¼šå¢åŠ æ¨ç†æ—¶é—´ï¼Œä»è€Œå½±å“æœºå™¨äººåœ¨å®æ—¶æ“ä½œä»»åŠ¡ä¸­çš„å¯ç”¨æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† Hybrid Training (HyT) æ¡†æ¶ï¼Œæ—¨åœ¨è®©æ¨¡å‹åœ¨è®­ç»ƒé˜¶æ®µä»æ¨ç†æ€ç»´ä¸­è·ç›Šï¼ŒåŒæ—¶åœ¨æ¨ç†é˜¶æ®µå…·å¤‡çœç•¥ CoT ç”Ÿæˆçš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶æ”¯æŒæ¨¡å‹æ ¹æ®éœ€æ±‚çµæ´»é¢„æµ‹åŠ¨ä½œã€ç”Ÿæˆæ€ç»´æˆ–éµå¾ªæŒ‡ä»¤ï¼Œå®ç°äº†æ¨ç†æ•ˆç‡ä¸ä»»åŠ¡æ€§èƒ½çš„å¹³è¡¡ã€‚é€šè¿‡åœ¨å¤šé¡¹æ¨¡æ‹ŸåŸºå‡†æµ‹è¯•å’ŒçœŸå®ä¸–ç•Œå®éªŒä¸­çš„éªŒè¯ï¼ŒHyT è¯æ˜äº†å…¶åœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„å‰æä¸‹ï¼Œæœ‰æ•ˆè§£å†³äº†æœºå™¨äººæ“ä½œä¸­çš„å®æ—¶æ€§æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00600v1",
      "published_date": "2025-10-01 07:27:15 UTC",
      "updated_date": "2025-10-01 07:27:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:03:31.982668+00:00"
    },
    {
      "arxiv_id": "2510.00591v1",
      "title": "AI-Driven Self-Evolving Software: A Promising Path Toward Software Automation",
      "title_zh": "AI é©±åŠ¨çš„è‡ªæ¼”åŒ–è½¯ä»¶ï¼šé€šå¾€è½¯ä»¶è‡ªåŠ¨åŒ–çš„æ–°è·¯å¾„",
      "authors": [
        "Liyi Cai",
        "Yijie Ren",
        "Yitong Zhang",
        "Jia Li"
      ],
      "abstract": "Software automation has long been a central goal of software engineering, striving for software development that proceeds without human intervention. Recent efforts have leveraged Artificial Intelligence (AI) to advance software automation with notable progress. However, current AI functions primarily as assistants to human developers, leaving software development still dependent on explicit human intervention. This raises a fundamental question: Can AI move beyond its role as an assistant to become a core component of software, thereby enabling genuine software automation? To investigate this vision, we introduce AI-Driven Self-Evolving Software, a new form of software that evolves continuously through direct interaction with users. We demonstrate the feasibility of this idea with a lightweight prototype built on a multi-agent architecture that autonomously interprets user requirements, generates and validates code, and integrates new functionalities. Case studies across multiple representative scenarios show that the prototype can reliably construct and reuse functionality, providing early evidence that such software systems can scale to more sophisticated applications and pave the way toward truly automated software development. We make code and cases in this work publicly available at https://anonymous.4open.science/r/live-software.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•è¶…è¶Š AI åŠ©æ‰‹çš„å±€é™ï¼Œæå‡ºäº† AI-Driven Self-Evolving Softwareï¼ˆAI é©±åŠ¨çš„è‡ªè¿›åŒ–è½¯ä»¶ï¼‰è¿™ä¸€æ–°æ¦‚å¿µï¼Œæ—¨åœ¨é€šè¿‡è½¯ä»¶çš„æŒç»­è‡ªæˆ‘æ¼”è¿›å®ç°çœŸæ­£çš„ Software Automationã€‚è¿™ç§è½¯ä»¶å½¢æ€é€šè¿‡ä¸ç”¨æˆ·ç›´æ¥äº¤äº’å®ç°æ¼”è¿›ï¼Œå¹¶åˆ©ç”¨ Multi-agent architecture è‡ªä¸»å®Œæˆéœ€æ±‚è§£æã€ä»£ç ç”Ÿæˆã€éªŒè¯ä»¥åŠæ–°åŠŸèƒ½çš„æ— ç¼é›†æˆã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡æ„å»ºè½»é‡çº§åŸå‹å¹¶åœ¨å¤šä¸ªä»£è¡¨æ€§åœºæ™¯ä¸­è¿›è¡Œ Case studiesï¼ŒéªŒè¯äº†è¯¥æ€è·¯çš„å¯è¡Œæ€§ä¸å¯é æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿæœ‰æ•ˆåœ°æ„å»ºå’Œå¤ç”¨åŠŸèƒ½ï¼Œè¯æ˜äº†æ­¤ç±»ç³»ç»Ÿæ‰©å±•è‡³å¤æ‚åº”ç”¨å¹¶å®ç°å…¨è‡ªåŠ¨è½¯ä»¶å¼€å‘çš„æ½œåŠ›ã€‚è¯¥ç ”ç©¶ä¸ºæœªæ¥è½¯ä»¶å·¥ç¨‹çš„è‡ªåŠ¨åŒ–è·¯å¾„æä¾›äº†é‡è¦å‚è€ƒï¼Œå¹¶å·²å…¬å¼€äº†ç›¸å…³æºä»£ç ä¸å®éªŒæ¡ˆä¾‹ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00591v1",
      "published_date": "2025-10-01 07:17:51 UTC",
      "updated_date": "2025-10-01 07:17:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:03:43.272559+00:00"
    },
    {
      "arxiv_id": "2510.01295v1",
      "title": "The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation",
      "title_zh": "ç¤¾ä¼šå®éªŒå®¤ï¼šé¢å‘å¤šæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°çš„å¿ƒç†æµ‹é‡å­¦æ¡†æ¶",
      "authors": [
        "Zarreen Reza"
      ],
      "abstract": "As Large Language Models (LLMs) transition from static tools to autonomous agents, traditional evaluation benchmarks that measure performance on downstream tasks are becoming insufficient. These methods fail to capture the emergent social and cognitive dynamics that arise when agents communicate, persuade, and collaborate in interactive environments. To address this gap, we introduce a novel evaluation framework that uses multi-agent debate as a controlled \"social laboratory\" to discover and quantify these behaviors. In our framework, LLM-based agents, instantiated with distinct personas and incentives, deliberate on a wide range of challenging topics under the supervision of an LLM moderator. Our analysis, enabled by a new suite of psychometric and semantic metrics, reveals several key findings. Across hundreds of debates, we uncover a powerful and robust emergent tendency for agents to seek consensus, consistently reaching high semantic agreement (Î¼ > 0.88) even without explicit instruction and across sensitive topics. We show that assigned personas induce stable, measurable psychometric profiles, particularly in cognitive effort, and that the moderators persona can significantly alter debate outcomes by structuring the environment, a key finding for external AI alignment. This work provides a blueprint for a new class of dynamic, psychometrically grounded evaluation protocols designed for the agentic setting, offering a crucial methodology for understanding and shaping the social behaviors of the next generation of AI agents. We have released the code and results at https://github.com/znreza/multi-agent-LLM-eval-for-debate.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† The Social Laboratoryï¼Œä¸€ä¸ªä¸“ä¸ºå¤§è¯­è¨€æ¨¡å‹ (LLMs) æ™ºèƒ½ä½“è®¾è®¡çš„å¿ƒç†æµ‹é‡è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¤šæ™ºèƒ½ä½“è¾©è®ºè¿™ä¸€å—æ§ç¯å¢ƒæ¥å‘ç°å¹¶é‡åŒ–æ™ºèƒ½ä½“é—´çš„ç¤¾äº¤ä¸è®¤çŸ¥åŠ¨æ€ã€‚åœ¨è¯¥æ¡†æ¶ä¸­ï¼Œå…·æœ‰ä¸åŒ Persona å’Œ Incentives çš„æ™ºèƒ½ä½“åœ¨ LLM Moderator çš„ç›‘ç£ä¸‹å°±å„ç§æŒ‘æˆ˜æ€§è¯é¢˜è¿›è¡Œè®¨è®ºã€‚å®éªŒé€šè¿‡ä¸€ç³»åˆ—å¿ƒç†æµ‹é‡å’Œè¯­ä¹‰æŒ‡æ ‡å‘ç°ï¼Œå³ä½¿æ²¡æœ‰æ˜ç¡®æŒ‡ä»¤ï¼Œæ™ºèƒ½ä½“åœ¨è®¨è®ºä¸­ä¹Ÿè¡¨ç°å‡ºå¼ºçƒˆçš„å¯»æ±‚å…±è¯† (Consensus) çš„å€¾å‘ï¼Œè¯­ä¹‰ä¸€è‡´æ€§ (Semantic Agreement) å¹³å‡å€¼è¶…è¿‡ 0.88ã€‚ç ”ç©¶è¿˜è¯æ˜åˆ†é…çš„ Persona èƒ½å¤Ÿè¯±å¯¼ç¨³å®šä¸”å¯æµ‹é‡çš„å¿ƒç†ç‰¹å¾ï¼Œä¸” Moderator çš„è§’è‰²è®¾å®šä¼šé€šè¿‡ç»“æ„åŒ–ç¯å¢ƒæ˜¾è‘—æ”¹å˜è¾©è®ºç»“æœï¼Œè¿™å¯¹ AI Alignment å…·æœ‰é‡è¦æ„ä¹‰ã€‚è¯¥å·¥ä½œä¸ºæ™ºèƒ½ä½“ç¯å¢ƒä¸‹çš„åŠ¨æ€è¯„ä¼°åè®®æä¾›äº†è“å›¾ï¼Œä¸ºç†è§£å’Œå¡‘é€ ä¸‹ä¸€ä»£ AI æ™ºèƒ½ä½“çš„ç¤¾äº¤è¡Œä¸ºæä¾›äº†å…³é”®çš„æ–¹æ³•è®ºæ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop on Evaluating the Evolving LLM Lifecycle: Benchmarks, Emergent Abilities, and Scaling",
      "pdf_url": "https://arxiv.org/pdf/2510.01295v1",
      "published_date": "2025-10-01 07:10:28 UTC",
      "updated_date": "2025-10-01 07:10:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:03:41.679789+00:00"
    },
    {
      "arxiv_id": "2510.00585v1",
      "title": "U-DFA: A Unified DINOv2-Unet with Dual Fusion Attention for Multi-Dataset Medical Segmentation",
      "title_zh": "U-DFAï¼šåŸºäºåŒèåˆæ³¨æ„åŠ›æœºåˆ¶çš„ç»Ÿä¸€ DINOv2-Unet å¤šæ•°æ®é›†åŒ»å­¦å½±åƒåˆ†å‰²æ¶æ„",
      "authors": [
        "Zulkaif Sajjad",
        "Furqan Shaukat",
        "Junaid Mir"
      ],
      "abstract": "Accurate medical image segmentation plays a crucial role in overall diagnosis and is one of the most essential tasks in the diagnostic pipeline. CNN-based models, despite their extensive use, suffer from a local receptive field and fail to capture the global context. A common approach that combines CNNs with transformers attempts to bridge this gap but fails to effectively fuse the local and global features. With the recent emergence of VLMs and foundation models, they have been adapted for downstream medical imaging tasks; however, they suffer from an inherent domain gap and high computational cost. To this end, we propose U-DFA, a unified DINOv2-Unet encoder-decoder architecture that integrates a novel Local-Global Fusion Adapter (LGFA) to enhance segmentation performance. LGFA modules inject spatial features from a CNN-based Spatial Pattern Adapter (SPA) module into frozen DINOv2 blocks at multiple stages, enabling effective fusion of high-level semantic and spatial features. Our method achieves state-of-the-art performance on the Synapse and ACDC datasets with only 33\\% of the trainable model parameters. These results demonstrate that U-DFA is a robust and scalable framework for medical image segmentation across multiple modalities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—å›¾åƒåˆ†å‰²ä¸­CNNå±€é™äºå±€éƒ¨æ„Ÿå—é‡ä»¥åŠç°æœ‰æ··åˆæ¨¡å‹åœ¨èåˆå±€éƒ¨ä¸å…¨å±€ç‰¹å¾æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§åä¸ºU-DFAçš„ç»Ÿä¸€æ¶æ„ã€‚U-DFAé‡‡ç”¨äº†åŸºäºDINOv2-Unetçš„ç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Œå¹¶å¼•å…¥äº†åˆ›æ–°çš„å±€éƒ¨-å…¨å±€èåˆé€‚é…å™¨(Local-Global Fusion Adapter, LGFA)ã€‚è¯¥æ¨¡å—é€šè¿‡å°†CNNç©ºé—´æ¨¡å¼é€‚é…å™¨(Spatial Pattern Adapter, SPA)çš„ç©ºé—´ç‰¹å¾æ³¨å…¥åˆ°å†»ç»“çš„DINOv2å—ä¸­ï¼Œå®ç°äº†é«˜å±‚è¯­ä¹‰ä¸ç©ºé—´ç‰¹å¾çš„æ·±åº¦èåˆã€‚å®éªŒè¯æ˜ï¼ŒU-DFAåœ¨Synapseå’ŒACDCæ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº†state-of-the-artæ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹ä»…éœ€33%çš„å¯è®­ç»ƒå‚æ•°ï¼Œæ˜¾è‘—æå‡äº†æ•ˆç‡å¹¶é™ä½äº†è®¡ç®—æˆæœ¬ã€‚è¿™äº›ç»“æœå…±åŒè¯æ˜äº†U-DFAæ˜¯ä¸€ä¸ªé€‚ç”¨äºå¤šç§æ¨¡æ€ä¸”å…·æœ‰é²æ£’æ€§å’Œå¯æ‰©å±•æ€§çš„åŒ»ç–—å›¾åƒåˆ†å‰²æ¡†æ¶ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00585v1",
      "published_date": "2025-10-01 07:06:49 UTC",
      "updated_date": "2025-10-01 07:06:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:03:42.868779+00:00"
    },
    {
      "arxiv_id": "2510.00582v1",
      "title": "SAGE-LD: Towards Scalable and Generalizable End-to-End Language Diarization via Simulated Data Augmentation",
      "title_zh": "SAGE-LDï¼šé€šè¿‡æ¨¡æ‹Ÿæ•°æ®å¢å¼ºå®ç°å¯æ‰©å±•ä¸”é«˜æ³›åŒ–æ€§çš„ç«¯åˆ°ç«¯è¯­è¨€æ—¥è®°åŒ–",
      "authors": [
        "Sangmin Lee",
        "Woongjib Choi",
        "Jihyun Kim",
        "Hong-Goo Kang"
      ],
      "abstract": "In this paper, we present a neural spoken language diarization model that supports an unconstrained span of languages within a single framework. Our approach integrates a learnable query-based architecture grounded in multilingual awareness, with large-scale pretraining on simulated code-switching data. By jointly leveraging these two components, our method overcomes the limitations of conventional approaches in data scarcity and architecture optimization, and generalizes effectively to real-world multilingual settings across diverse environments. Experimental results demonstrate that our approach achieves state-of-the-art performance on several language diarization benchmarks, with a relative performance improvement of 23% to 52% over previous methods. We believe that this work not only advances research in language diarization but also establishes a foundational framework for code-switching speech technologies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SAGE-LDï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å®ç°å¯æ‰©å±•ä¸”æ³›åŒ–æ€§å¼ºçš„ç«¯åˆ°ç«¯ Language Diarization ç¥ç»æ¨¡å‹ï¼Œæ”¯æŒåœ¨å•ä¸€æ¡†æ¶å†…å¤„ç†ä¸å—é™åˆ¶çš„å¤šç§è¯­è¨€ã€‚è¯¥æ–¹æ³•å°†åŸºäºå¤šè¯­è¨€æ„ŸçŸ¥çš„å¯å­¦ä¹  Query-based æ¶æ„ä¸å¤§è§„æ¨¡æ¨¡æ‹Ÿ Code-switching æ•°æ®é¢„è®­ç»ƒç›¸ç»“åˆï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•é¢ä¸´çš„æ•°æ®ç¨€ç¼ºå’Œæ¶æ„ä¼˜åŒ–éš¾é¢˜ã€‚é€šè¿‡è¿™ç§é›†æˆæ–¹å¼ï¼Œè¯¥æ¨¡å‹åœ¨å¤šç§ç°å®ä¸–ç•Œçš„å¤šè¯­è¨€ç¯å¢ƒä¸­å±•ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAGE-LD åœ¨å¤šä¸ª Language Diarization åŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°äº† State-of-the-art æ€§èƒ½ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•å®ç°äº† 23% è‡³ 52% çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚è¯¥ç ”ç©¶ä¸ä»…å¤§å¹…æ¨åŠ¨äº† Language Diarization é¢†åŸŸçš„å‘å±•ï¼Œä¹Ÿä¸ºæœªæ¥çš„ Code-switching è¯­éŸ³æŠ€æœ¯å¥ å®šäº†é‡è¦çš„åŸºç¡€æ¡†æ¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00582v1",
      "published_date": "2025-10-01 07:01:33 UTC",
      "updated_date": "2025-10-01 07:01:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:03:56.279593+00:00"
    },
    {
      "arxiv_id": "2510.17825v1",
      "title": "Carbon-Aware Orchestration of Integrated Satellite Aerial Terrestrial Networks via Digital Twin",
      "title_zh": "åŸºäºæ•°å­—å­ªç”Ÿçš„æ˜Ÿç©ºåœ°ä¸€ä½“åŒ–ç½‘ç»œç¢³æ„ŸçŸ¥ç¼–æ’",
      "authors": [
        "Shumaila Javaid",
        "Nasir Saeed"
      ],
      "abstract": "Integrated Satellite Aerial Terrestrial Networks (ISATNs) are envisioned as key enablers of 6G, providing global connectivity for applications such as autonomous transportation, Industrial IoT, and disaster response. Their large-scale deployment, however, risks unsustainable energy use and carbon emissions. This work advances prior energy-aware studies by proposing a carbon-aware orchestration framework for ISATNs that leverages Digital Twin (DT) technology. The framework adopts grams of CO$_2$-equivalent per bit (gCO$_2$/bit) as a primary sustainability metric and implements a multi timescale Plan Do Check Act (PDCA) loop that combines day-ahead forecasting with real-time adaptive optimization. ISATN-specific control knobs, including carbon-aware handovers, UAV duty cycling, and renewable-aware edge placement, are exploited to reduce emissions. Simulation results with real carbon intensity data show up to 29\\% lower gCO$_2$/bit than QoS-only orchestration, while improving renewable utilization and resilience under adverse events.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç©ºå¤©åœ°ä¸€ä½“åŒ–ç½‘ç»œ(ISATNs)åœ¨6Gæ¼”è¿›è¿‡ç¨‹ä¸­é¢ä¸´çš„é«˜èƒ½è€—ä¸ç¢³æ’æ”¾æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ•°å­—å­ªç”Ÿ(Digital Twin, DT)æŠ€æœ¯çš„ç¢³æ„ŸçŸ¥ç¼–æ’æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†æ¯æ¯”ç‰¹äºŒæ°§åŒ–ç¢³å½“é‡å…‹æ•°(gCO2/bit)ä½œä¸ºæ ¸å¿ƒçš„å¯æŒç»­æ€§è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶å®æ–½äº†ç»“åˆæ—¥å‰é¢„æµ‹ä¸å®æ—¶è‡ªé€‚åº”ä¼˜åŒ–çš„å¤šæ—¶é—´å°ºåº¦PDCAå¾ªç¯ã€‚é€šè¿‡æ•´åˆç¢³æ„ŸçŸ¥åˆ‡æ¢(Carbon-aware handovers)ã€æ— äººæœº(UAV)å ç©ºæ¯”æ§åˆ¶åŠå¯å†ç”Ÿèƒ½æºæ„ŸçŸ¥çš„è¾¹ç¼˜éƒ¨ç½²ç­‰å…³é”®æŠ€æœ¯æ‰‹æ®µï¼Œè¯¥ç³»ç»Ÿå®ç°äº†å¯¹ç½‘ç»œèµ„æºçš„ç²¾ç»†åŒ–ç®¡ç†ã€‚ä»¿çœŸå®éªŒè¡¨æ˜ï¼Œåœ¨çœŸå®ç¢³å¼ºåº¦æ•°æ®ä¸‹ï¼Œè¯¥æ¡†æ¶ç›¸æ¯”ä¼ ç»Ÿçš„ä»…å…³æ³¨æœåŠ¡è´¨é‡(QoS)çš„ç¼–æ’æ–¹æ¡ˆå¯é™ä½29%çš„å•ä½æµé‡ç¢³æ’æ”¾ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ä¸ä»…æå‡äº†å¯å†ç”Ÿèƒ½æºçš„åˆ©ç”¨æ•ˆç‡ï¼Œè¿˜æ˜¾è‘—å¢å¼ºäº†ç½‘ç»œåœ¨æç«¯ç¯å¢ƒæˆ–ä¸åˆ©äº‹ä»¶ä¸‹çš„è¿è¡ŒéŸ§æ€§ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.17825v1",
      "published_date": "2025-10-01 06:49:42 UTC",
      "updated_date": "2025-10-01 06:49:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:04:03.676726+00:00"
    },
    {
      "arxiv_id": "2510.00570v1",
      "title": "Adaptive Shared Experts with LoRA-Based Mixture of Experts for Multi-Task Learning",
      "title_zh": "åŸºäº LoRA æ··åˆä¸“å®¶æ¨¡å‹çš„å¤šä»»åŠ¡å­¦ä¹ è‡ªé€‚åº”å…±äº«ä¸“å®¶",
      "authors": [
        "Minghao Yang",
        "Ren Togo",
        "Guang Li",
        "Takahiro Ogawa",
        "Miki Haseyama"
      ],
      "abstract": "Mixture-of-Experts (MoE) has emerged as a powerful framework for multi-task learning (MTL). However, existing MoE-MTL methods often rely on single-task pretrained backbones and suffer from redundant adaptation and inefficient knowledge sharing during the transition from single-task to multi-task learning (STL to MTL). To address these limitations, we propose adaptive shared experts (ASE) within a low-rank adaptation (LoRA) based MoE, where shared experts are assigned router-computed gating weights jointly normalized with sparse experts. This design facilitates STL to MTL transition, enhances expert specialization, and cooperation. Furthermore, we incorporate fine-grained experts by increasing the number of LoRA experts while proportionally reducing their rank, enabling more effective knowledge sharing under a comparable parameter budget. Extensive experiments on the PASCAL-Context benchmark, under unified training settings, demonstrate that ASE consistently improves performance across diverse configurations and validates the effectiveness of fine-grained designs for MTL.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Adaptive Shared Experts (ASE) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ Mixture-of-Experts (MoE) åœ¨å¤šä»»åŠ¡å­¦ä¹  (MTL) ä¸­ä»å•ä»»åŠ¡å­¦ä¹  (STL) è½¬å‘å¤šä»»åŠ¡å­¦ä¹ æ—¶å­˜åœ¨çš„å†—ä½™é€‚é…å’ŒçŸ¥è¯†å…±äº«æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ASE åœ¨åŸºäº Low-Rank Adaptation (LoRA) çš„ MoE æ¶æ„ä¸­å¼•å…¥äº†å…±äº«ä¸“å®¶ï¼Œå¹¶é‡‡ç”¨è·¯ç”±å™¨è®¡ç®—çš„é—¨æ§æƒé‡ä¸ç¨€ç–ä¸“å®¶è¿›è¡Œè”åˆå½’ä¸€åŒ–ã€‚è¿™ç§è®¾è®¡æ˜¾è‘—ä¿ƒè¿›äº† STL å‘ MTL çš„å¹³æ»‘è¿‡æ¸¡ï¼Œå¹¶å¢å¼ºäº†ä¸“å®¶ç³»ç»Ÿå†…éƒ¨çš„ä¸“ä¸šåŒ–åˆ†å·¥ä¸åä½œã€‚æ­¤å¤–ï¼Œç ”ç©¶é€šè¿‡å¢åŠ  LoRA ä¸“å®¶æ•°é‡å¹¶æŒ‰æ¯”ä¾‹é™ä½å…¶ Rankï¼Œå®ç°äº†åœ¨ç›¸åŒå‚æ•°é¢„ç®—ä¸‹æ›´æœ‰æ•ˆçš„ç»†ç²’åº¦çŸ¥è¯†å…±äº«ã€‚åœ¨ PASCAL-Context åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜ï¼ŒASE åœ¨å¤šç§é…ç½®ä¸‹å‡èƒ½ç¨³å®šæå‡æ€§èƒ½ï¼Œå……åˆ†éªŒè¯äº†è¯¥æ–¹æ¡ˆåœ¨å¤šä»»åŠ¡åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00570v1",
      "published_date": "2025-10-01 06:49:19 UTC",
      "updated_date": "2025-10-01 06:49:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:04:00.976778+00:00"
    },
    {
      "arxiv_id": "2510.00566v3",
      "title": "Panorama: Fast-Track Nearest Neighbors",
      "title_zh": "Panoramaï¼šæœ€è¿‘é‚»æœç´¢çš„å¿«é€Ÿé€šé“",
      "authors": [
        "Vansh Ramani",
        "Alexis Schlomer",
        "Akash Nayar",
        "Sayan Ranu",
        "Jignesh M. Patel",
        "Panagiotis Karras"
      ],
      "abstract": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose embeddings are close to that of a given query in a high-dimensional space, aiming to balance accuracy with speed. Used in recommendation systems, image and video retrieval, natural language processing, and retrieval-augmented generation (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT utilize graph, tree, clustering, and quantization techniques to navigate large vector spaces. Despite this progress, ANNS systems spend up to 99% of query time to compute distances in their final refinement phase. In this paper, we present PANORAMA, a machine learning-driven approach that tackles the ANNS verification bottleneck through data-adaptive learned orthogonal transforms that facilitate the accretive refinement of distance bounds. Such transforms compact over 90% of signal energy into the first half of dimensions, enabling early candidate pruning with partial distance computations. We integrate PANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and Annoy, without index modification, using level-major memory layouts, SIMD-vectorized partial distance computations, and cache-aware access patterns. Experiments across diverse datasets -- from image-based CIFAR-10 and GIST to modern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate that PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢(Approximate Nearest-Neighbor Search, ANNS)åœ¨ç²¾ç‚¼é˜¶æ®µå› è·ç¦»è®¡ç®—å æ®é«˜è¾¾99%æŸ¥è¯¢æ—¶é—´çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºPANORAMAçš„æœºå™¨å­¦ä¹ é©±åŠ¨ä¼˜åŒ–æ–¹æ¡ˆã€‚PANORAMAåˆ©ç”¨æ•°æ®è‡ªé€‚åº”çš„å­¦ä¹ æ­£äº¤å˜æ¢(data-adaptive learned orthogonal transforms)å°†è¶…è¿‡90%çš„ä¿¡å·èƒ½é‡é›†ä¸­åœ¨ç»´åº¦çš„å‰åŠéƒ¨åˆ†ï¼Œä»è€Œæ”¯æŒé€šè¿‡éƒ¨åˆ†è·ç¦»è®¡ç®—(partial distance computations)è¿›è¡Œæ—©æœŸå€™é€‰é¡¹å‰ªæã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸æ”¹å˜åŸæœ‰ç´¢å¼•ç»“æ„çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡SIMDå‘é‡åŒ–å’Œç¼“å­˜æ„ŸçŸ¥è®¿é—®æ¨¡å¼é›†æˆåˆ°IVFPQã€HNSWã€MRPTå’ŒAnnoyç­‰ä¸»æµANNSç®—æ³•ä¸­ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨CIFAR-10ä»¥åŠOpenAI Ada 2å’ŒLarge 3ç­‰å¤šç§å¤§è§„æ¨¡åµŒå…¥ç©ºé—´æ•°æ®é›†ä¸Šï¼ŒPANORAMAåœ¨ç¡®ä¿å¬å›ç‡é›¶æŸå¤±çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†2è‡³30å€çš„ç«¯åˆ°ç«¯æœç´¢åŠ é€Ÿã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00566v3",
      "published_date": "2025-10-01 06:38:45 UTC",
      "updated_date": "2025-10-23 19:45:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:04:07.075511+00:00"
    },
    {
      "arxiv_id": "2510.00565v1",
      "title": "Toward Safer Diffusion Language Models: Discovery and Mitigation of Priming Vulnerability",
      "title_zh": "è¿ˆå‘æ›´å®‰å…¨çš„æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼šå¼•å¯¼æ€§æ¼æ´çš„å‘ç°ä¸ç¼“è§£",
      "authors": [
        "Shojiro Yamabe",
        "Jun Sakuma"
      ],
      "abstract": "Diffusion language models (DLMs) generate tokens in parallel through iterative denoising, which can reduce latency and enable bidirectional conditioning. However, the safety risks posed by jailbreak attacks that exploit this inference mechanism are not well understood. In this paper, we reveal that DLMs have a critical vulnerability stemming from their iterative denoising process and propose a countermeasure. Specifically, our investigation shows that if an affirmative token for a harmful query appears at an intermediate step, subsequent denoising can be steered toward a harmful response even in aligned models. As a result, simply injecting such affirmative tokens can readily bypass the safety guardrails. Furthermore, we demonstrate that the vulnerability allows existing optimization-based jailbreak attacks to succeed on DLMs. Building on this analysis, we propose a novel safety alignment method tailored to DLMs that trains models to generate safe responses from contaminated intermediate states that contain affirmative tokens. Our experiments indicate that the proposed method significantly mitigates the vulnerability with minimal impact on task performance. Furthermore, our method improves robustness against conventional jailbreak attacks. Our work underscores the need for DLM-specific safety research.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ‰©æ•£è¯­è¨€æ¨¡å‹(Diffusion language models, DLMs)åœ¨å®‰å…¨æ€§æ–¹é¢çš„é£é™©ï¼Œæ­ç¤ºäº†å…¶è¿­ä»£å»å™ªè¿‡ç¨‹ä¸­å­˜åœ¨çš„â€œå¯åŠ¨æ¼æ´â€(Priming Vulnerability)ã€‚ç ”ç©¶å‘ç°ï¼Œå¦‚æœåœ¨ä¸­é—´å»å™ªæ­¥éª¤ä¸­å‡ºç°é’ˆå¯¹æœ‰å®³æŸ¥è¯¢çš„è‚¯å®šæ€§Tokenï¼Œå³ä½¿æ˜¯ç»è¿‡å¯¹é½çš„æ¨¡å‹ä¹Ÿå¯èƒ½è¢«å¼•å¯¼ç”Ÿæˆæœ‰å®³å“åº”ï¼Œè¿™ä½¿å¾—ç°æœ‰çš„ä¼˜åŒ–ç±»è¶Šç‹±æ”»å‡»(jailbreak attacks)ææ˜“æˆåŠŸã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ä¸“ä¸ºDLMsè®¾è®¡çš„å®‰å…¨å¯¹é½æ–¹æ³•ï¼Œé€šè¿‡è®­ç»ƒæ¨¡å‹ä»åŒ…å«è‚¯å®šæ€§Tokençš„å—æ±¡æŸ“ä¸­é—´çŠ¶æ€ä¸­ç”Ÿæˆå®‰å…¨å“åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡ ä¹ä¸å½±å“ä»»åŠ¡æ€§èƒ½çš„å‰æä¸‹ï¼Œæ˜¾è‘—é™ä½äº†ä¸Šè¿°æ¼æ´é£é™©ï¼Œå¹¶å¢å¼ºäº†æ¨¡å‹å¯¹æŠ—ä¼ ç»Ÿè¶Šç‹±æ”»å‡»çš„é²æ£’æ€§ã€‚æ­¤é¡¹å·¥ä½œå¼ºè°ƒäº†é’ˆå¯¹DLMç‰¹å®šæ¶æ„å¼€å±•å®‰å…¨æ€§ç ”ç©¶çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00565v1",
      "published_date": "2025-10-01 06:35:23 UTC",
      "updated_date": "2025-10-01 06:35:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:04:06.710153+00:00"
    },
    {
      "arxiv_id": "2510.00563v1",
      "title": "Memory Determines Learning Direction: A Theory of Gradient-Based Optimization in State Space Models",
      "title_zh": "è®°å¿†å†³å®šå­¦ä¹ æ–¹å‘ï¼šçŠ¶æ€ç©ºé—´æ¨¡å‹åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–ç†è®º",
      "authors": [
        "JingChuan Guan",
        "Tomoyuki Kubota",
        "Yasuo Kuniyoshi",
        "Kohei Nakajima"
      ],
      "abstract": "State space models (SSMs) have gained attention by showing potential to outperform Transformers. However, previous studies have not sufficiently addressed the mechanisms underlying their high performance owing to a lack of theoretical explanation of SSMs' learning dynamics. In this study, we provide such an explanation and propose an improved training strategy. The memory capacity of SSMs can be evaluated by examining how input time series are stored in their current state. Such an examination reveals a tradeoff between memory accuracy and length, as well as the theoretical equivalence between the structured state space sequence model (S4) and a simplified S4 with diagonal recurrent weights. This theoretical foundation allows us to elucidate the learning dynamics, proving the importance of initial parameters. Our analytical results suggest that successful learning requires the initial memory structure to be the longest possible even if memory accuracy may deteriorate or the gradient lose the teacher information. Experiments on tasks requiring long memory confirmed that extending memory is difficult, emphasizing the importance of initialization. Furthermore, we found that fixing recurrent weights can be more advantageous than adapting them because it achieves comparable or even higher performance with faster convergence. Our results provide a new theoretical foundation for SSMs and potentially offer a novel optimization strategy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çŠ¶æ€ç©ºé—´æ¨¡å‹(SSMs)åœ¨é«˜æ€§èƒ½èƒŒåç¼ºä¹ç†è®ºè§£é‡Šçš„é—®é¢˜ï¼Œæ·±å…¥æ¢è®¨äº†å…¶åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–ç†è®ºä¸å­¦ä¹ åŠ¨æ€ã€‚ç ”ç©¶é€šè¿‡åˆ†æè¾“å…¥æ—¶é—´åºåˆ—åœ¨çŠ¶æ€ä¸­çš„å­˜å‚¨æœºåˆ¶ï¼Œæ­ç¤ºäº†å†…å­˜ç²¾åº¦ä¸é•¿åº¦ä¹‹é—´çš„æƒè¡¡å…³ç³»ï¼Œå¹¶è¯æ˜äº†ç»“æ„åŒ–çŠ¶æ€ç©ºé—´åºåˆ—æ¨¡å‹(S4)ä¸å…¶å¯¹è§’å¾ªç¯æƒé‡ç®€åŒ–ç‰ˆæœ¬åœ¨ç†è®ºä¸Šçš„ç­‰ä»·æ€§ã€‚åˆ†æç»“æœè¡¨æ˜ï¼Œåˆå§‹å‚æ•°(initial parameters)å¯¹å­¦ä¹ è¿‡ç¨‹è‡³å…³é‡è¦ï¼ŒæˆåŠŸçš„å­¦ä¹ ä¾èµ–äºåˆå§‹é˜¶æ®µå…·å¤‡å°½å¯èƒ½é•¿çš„å†…å­˜ç»“æ„ï¼Œå³ä¾¿è¿™å¯èƒ½ç‰ºç‰²ç²¾åº¦æˆ–å¯¼è‡´æ¢¯åº¦ä¿¡æ¯ä¸¢å¤±ã€‚é•¿æ—¶è®°å¿†ä»»åŠ¡çš„å®éªŒéªŒè¯äº†æ‰©å±•å†…å­˜çš„å›°éš¾æ€§ï¼Œè¿›ä¸€æ­¥å‡¸æ˜¾äº†åˆå§‹åŒ–(initialization)çš„å†³å®šæ€§ä½œç”¨ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°å›ºå®šå¾ªç¯æƒé‡(fixing recurrent weights)ç›¸æ¯”äºåŠ¨æ€è°ƒæ•´ï¼Œå¾€å¾€èƒ½ä»¥æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦è¾¾åˆ°ç›¸å½“ç”šè‡³æ›´é«˜çš„æ€§èƒ½ã€‚è¯¥æˆæœä¸ºSSMså¥ å®šäº†æ–°çš„ç†è®ºåŸºç¡€ï¼Œå¹¶ä¸ºç¥ç»ç½‘ç»œçš„ä¼˜åŒ–ç­–ç•¥æä¾›äº†å…¨æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00563v1",
      "published_date": "2025-10-01 06:30:42 UTC",
      "updated_date": "2025-10-01 06:30:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:04:10.477910+00:00"
    },
    {
      "arxiv_id": "2510.00555v1",
      "title": "PromptPilot: Improving Human-AI Collaboration Through LLM-Enhanced Prompt Engineering",
      "title_zh": "PromptPilotï¼šé€šè¿‡å¤§è¯­è¨€æ¨¡å‹å¢å¼ºçš„æç¤ºå·¥ç¨‹æå‡äººæœºåä½œ",
      "authors": [
        "Niklas Gutheil",
        "Valentin Mayer",
        "Leopold MÃ¼ller",
        "JÃ¶rg Rommelt",
        "Niklas KÃ¼hl"
      ],
      "abstract": "Effective prompt engineering is critical to realizing the promised productivity gains of large language models (LLMs) in knowledge-intensive tasks. Yet, many users struggle to craft prompts that yield high-quality outputs, limiting the practical benefits of LLMs. Existing approaches, such as prompt handbooks or automated optimization pipelines, either require substantial effort, expert knowledge, or lack interactive guidance. To address this gap, we design and evaluate PromptPilot, an interactive prompting assistant grounded in four empirically derived design objectives for LLM-enhanced prompt engineering. We conducted a randomized controlled experiment with 80 participants completing three realistic, work-related writing tasks. Participants supported by PromptPilot achieved significantly higher performance (median: 78.3 vs. 61.7; p = .045, d = 0.56), and reported enhanced efficiency, ease-of-use, and autonomy during interaction. These findings empirically validate the effectiveness of our proposed design objectives, establishing LLM-enhanced prompt engineering as a viable technique for improving human-AI collaboration.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”¨æˆ·åœ¨æç¤ºå·¥ç¨‹ï¼ˆPrompt Engineeringï¼‰ä¸­éš¾ä»¥æ’°å†™é«˜è´¨é‡æç¤ºè¯çš„æŒ‘æˆ˜ï¼Œæå‡ºäº† PromptPilotï¼Œä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¢å¼ºçš„äº¤äº’å¼æç¤ºåŠ©æ‰‹ã€‚PromptPilot éµå¾ªå››ä¸ªå®è¯å¾—å‡ºçš„è®¾è®¡ç›®æ ‡ï¼Œæ—¨åœ¨é€šè¿‡ LLM-enhanced prompt engineering æ”¹å–„äººç±»ä¸ AI çš„åä½œã€‚ç ”ç©¶äººå‘˜é€šè¿‡ä¸€é¡¹æ¶‰åŠ 80 åå‚ä¸è€…ã€å®Œæˆä¸‰é¡¹ç°å®å·¥ä½œç›¸å…³å†™ä½œä»»åŠ¡çš„éšæœºå¯¹ç…§å®éªŒå¯¹è¯¥ç³»ç»Ÿè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ PromptPilot æ”¯æŒä¸‹çš„å‚ä¸è€…è¡¨ç°æ˜¾è‘—ä¼˜äºå¯¹ç…§ç»„ï¼Œå…¶ä»»åŠ¡è¡¨ç°ä¸­ä½å¾—åˆ†ä» 61.7 æå‡è‡³ 78.3ã€‚æ­¤å¤–ï¼Œå‚ä¸è€…åœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­æŠ¥å‘Šäº†æ›´é«˜çš„æ•ˆç‡ã€æ˜“ç”¨æ€§å’Œè‡ªä¸»æ€§ã€‚è¯¥ç ”ç©¶å®è¯éªŒè¯äº†æ‰€æè®¾è®¡ç›®æ ‡çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº† LLM-enhanced prompt engineering æ˜¯æå‡äººç±»ä¸ AI åä½œè´¨é‡çš„å¯è¡ŒæŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Preprint version. Accepted for presentation at the International Conference on Information Systems (ICIS 2025). Please cite the published version when available",
      "pdf_url": "https://arxiv.org/pdf/2510.00555v1",
      "published_date": "2025-10-01 06:14:42 UTC",
      "updated_date": "2025-10-01 06:14:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:04:11.778338+00:00"
    },
    {
      "arxiv_id": "2510.00553v2",
      "title": "On Predictability of Reinforcement Learning Dynamics for Large Language Models",
      "title_zh": "è®ºå¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ åŠ¨åŠ›å­¦çš„å¯é¢„æµ‹æ€§",
      "authors": [
        "Yuchen Cai",
        "Ding Cao",
        "Xin Xu",
        "Zijun Yao",
        "Yuqing Huang",
        "Zhenyu Tan",
        "Benyi Zhang",
        "Guiquan Liu",
        "Junfeng Fang"
      ],
      "abstract": "Recent advances in reasoning capabilities of large language models (LLMs) are largely driven by reinforcement learning (RL), yet the underlying parameter dynamics during RL training remain poorly understood. This work identifies two fundamental properties of RL-induced parameter updates in LLMs: (1) Rank-1 Dominance, where the top singular subspace of the parameter update matrix nearly fully determines reasoning improvements, recovering over 99\\% of performance gains; and (2) Rank-1 Linear Dynamics, where this dominant subspace evolves linearly throughout training, enabling accurate prediction from early checkpoints. Extensive experiments across 8 LLMs and 7 algorithms validate the generalizability of these properties. More importantly, based on these findings, we propose AlphaRL, a plug-in acceleration framework that extrapolates the final parameter update using a short early training window, achieving up to 2.5 speedup while retaining \\textgreater 96\\% of reasoning performance without extra modules or hyperparameter tuning. This positions our finding as a versatile and practical tool for large-scale RL, opening a path toward principled, interpretable, and efficient training paradigm for LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)åœ¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)è®­ç»ƒè¿‡ç¨‹ä¸­çš„å‚æ•°åŠ¨åŠ›å­¦ç‰¹å¾ï¼Œæ—¨åœ¨æå‡è®­ç»ƒçš„å¯é¢„æµ‹æ€§ä¸æ•ˆç‡ã€‚ä½œè€…è¯†åˆ«å‡ºRLè¯±å¯¼çš„å‚æ•°æ›´æ–°å…·æœ‰ä¸¤ä¸ªæ ¸å¿ƒå±æ€§ï¼šä¸€æ˜¯ç§©-1ä¸»å¯¼æ€§(Rank-1 Dominance)ï¼Œå³å‚æ•°æ›´æ–°çŸ©é˜µçš„é¡¶å±‚å¥‡å¼‚å­ç©ºé—´å‡ ä¹å†³å®šäº†å…¨éƒ¨çš„æ¨ç†æ€§èƒ½æå‡ï¼›äºŒæ˜¯ç§©-1çº¿æ€§åŠ¨åŠ›å­¦(Rank-1 Linear Dynamics)ï¼Œè¯¥ä¸»å¯¼å­ç©ºé—´åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‘ˆçº¿æ€§æ¼”åŒ–ï¼Œå…è®¸ä»æ—©æœŸé˜¶æ®µè¿›è¡Œå‡†ç¡®é¢„æµ‹ã€‚é€šè¿‡åœ¨8ä¸ªLLMså’Œ7ç§ç®—æ³•ä¸Šçš„å¹¿æ³›å®éªŒï¼Œè¿™äº›æ€§è´¨çš„æ™®é€‚æ€§å¾—åˆ°äº†éªŒè¯ã€‚åŸºäºæ­¤å‘ç°ï¼Œç ”ç©¶æå‡ºäº†AlphaRLåŠ é€Ÿæ¡†æ¶ï¼Œé€šè¿‡å¤–æ¨æ—©æœŸè®­ç»ƒçª—å£çš„å‚æ•°æ›´æ–°ï¼Œåœ¨ä¿ç•™è¶…è¿‡96%æ¨ç†æ€§èƒ½çš„åŒæ—¶å®ç°äº†é«˜è¾¾2.5å€çš„åŠ é€Ÿã€‚è¯¥æˆæœä¸ºå¤§è§„æ¨¡æ¨¡å‹RLè®­ç»ƒæä¾›äº†ä¸€ç§åŸåˆ™æ€§å¼ºã€å¯è§£é‡Šä¸”é«˜æ•ˆçš„æ–°èŒƒå¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "43 pages, 28 figures; 43",
      "pdf_url": "https://arxiv.org/pdf/2510.00553v2",
      "published_date": "2025-10-01 06:13:50 UTC",
      "updated_date": "2025-10-02 15:16:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:04:15.896506+00:00"
    },
    {
      "arxiv_id": "2510.00552v1",
      "title": "Data Quality Challenges in Retrieval-Augmented Generation",
      "title_zh": "æ£€ç´¢å¢å¼ºç”Ÿæˆä¸­çš„æ•°æ®è´¨é‡æŒ‘æˆ˜",
      "authors": [
        "Leopold MÃ¼ller",
        "Joshua Holstein",
        "Sarah Bause",
        "Gerhard Satzger",
        "Niklas KÃ¼hl"
      ],
      "abstract": "Organizations increasingly adopt Retrieval-Augmented Generation (RAG) to enhance Large Language Models with enterprise-specific knowledge. However, current data quality (DQ) frameworks have been primarily developed for static datasets, and only inadequately address the dynamic, multi-stage nature of RAG systems. This study aims to develop DQ dimensions for this new type of AI-based systems. We conduct 16 semi-structured interviews with practitioners of leading IT service companies. Through a qualitative content analysis, we inductively derive 15 distinct DQ dimensions across the four processing stages of RAG systems: data extraction, data transformation, prompt & search, and generation. Our findings reveal that (1) new dimensions have to be added to traditional DQ frameworks to also cover RAG contexts; (2) these new dimensions are concentrated in early RAG steps, suggesting the need for front-loaded quality management strategies, and (3) DQ issues transform and propagate through the RAG pipeline, necessitating a dynamic, step-aware approach to quality management.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation)ä¸­çš„æ•°æ®è´¨é‡(Data Quality, DQ)æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿæ•°æ®è´¨é‡æ¡†æ¶ä¸»è¦é’ˆå¯¹é™æ€æ•°æ®é›†ï¼Œéš¾ä»¥åº”å¯¹RAGç³»ç»Ÿçš„åŠ¨æ€å¤šé˜¶æ®µç‰¹æ€§ã€‚é€šè¿‡å¯¹16ä½è¡Œä¸šä¸“å®¶è¿›è¡ŒåŠç»“æ„åŒ–è®¿è°ˆå’Œå®šæ€§å†…å®¹åˆ†æï¼Œç ”ç©¶å½’çº³å‡ºè´¯ç©¿æ•°æ®æå–(data extraction)ã€æ•°æ®è½¬æ¢(data transformation)ã€æç¤ºä¸æœç´¢(prompt & search)åŠç”Ÿæˆ(generation)å››ä¸ªé˜¶æ®µçš„15ä¸ªç‹¬ç‰¹DQç»´åº¦ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰DQæ¡†æ¶éœ€å¼•å…¥æ–°ç»´åº¦ä»¥è¦†ç›–RAGä¸Šä¸‹æ–‡ï¼Œä¸”è¿™äº›æ–°ç»´åº¦å¤šé›†ä¸­åœ¨ç³»ç»Ÿæ—©æœŸæ­¥éª¤ï¼Œå‡¸æ˜¾äº†å‰ç«¯è´¨é‡ç®¡ç†ç­–ç•¥çš„é‡è¦æ€§ã€‚æ­¤å¤–ï¼ŒDQé—®é¢˜ä¼šåœ¨RAG pipelineä¸­è½¬åŒ–å’Œä¼ æ’­ï¼Œå› æ­¤éœ€è¦ä¸€ç§åŠ¨æ€ä¸”å…·å¤‡æ­¥éª¤æ„è¯†(step-aware)çš„æ–¹æ³•è¿›è¡Œè´¨é‡ç®¡ç†ã€‚è¯¥æˆæœä¸ºä¼ä¸šåœ¨å®æ–½AIé©±åŠ¨çš„çŸ¥è¯†å¢å¼ºæ–¹æ¡ˆæ—¶ï¼Œæå‡æ•°æ®æ²»ç†çš„é’ˆå¯¹æ€§å’Œç³»ç»Ÿæ€§æä¾›äº†ç†è®ºæ”¯æŒä¸å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint version. Accepted for presentation at the International Conference on Information Systems (ICIS 2025). Please cite the published version when available",
      "pdf_url": "https://arxiv.org/pdf/2510.00552v1",
      "published_date": "2025-10-01 06:13:40 UTC",
      "updated_date": "2025-10-01 06:13:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:04:21.780632+00:00"
    },
    {
      "arxiv_id": "2510.00549v2",
      "title": "EMR-AGENT: Automating Cohort and Feature Extraction from EMR Databases",
      "title_zh": "EMR-AGENTï¼šç”µå­ç—…å†æ•°æ®åº“é˜Ÿåˆ—ä¸ç‰¹å¾æå–è‡ªåŠ¨åŒ–",
      "authors": [
        "Kwanhyung Lee",
        "Sungsoo Hong",
        "Joonhyung Park",
        "Jeonghyeop Lim",
        "Juhwan Choi",
        "Donghwee Yoon",
        "Eunho Yang"
      ],
      "abstract": "Machine learning models for clinical prediction rely on structured data extracted from Electronic Medical Records (EMRs), yet this process remains dominated by hardcoded, database-specific pipelines for cohort definition, feature selection, and code mapping. These manual efforts limit scalability, reproducibility, and cross-institutional generalization. To address this, we introduce EMR-AGENT (Automated Generalized Extraction and Navigation Tool), an agent-based framework that replaces manual rule writing with dynamic, language model-driven interaction to extract and standardize structured clinical data. Our framework automates cohort selection, feature extraction, and code mapping through interactive querying of databases. Our modular agents iteratively observe query results and reason over schema and documentation, using SQL not just for data retrieval but also as a tool for database observation and decision making. This eliminates the need for hand-crafted, schema-specific logic. To enable rigorous evaluation, we develop a benchmarking codebase for three EMR databases (MIMIC-III, eICU, SICdb), including both seen and unseen schema settings. Our results demonstrate strong performance and generalization across these databases, highlighting the feasibility of automating a process previously thought to require expert-driven design. The code will be released publicly at https://github.com/AITRICS/EMR-AGENT/tree/main. For a demonstration, please visit our anonymous demo page: https://anonymoususer-max600.github.io/EMR_AGENT/",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† EMR-AGENT (Automated Generalized Extraction and Navigation Tool)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è‡ªåŠ¨åŒ–ä»ç”µå­ç—…å† (EMR) æ•°æ®åº“ä¸­æå–é˜Ÿåˆ—å’Œç‰¹å¾çš„æ™ºèƒ½ä½“æ¡†æ¶ã€‚é’ˆå¯¹ç›®å‰ä¸´åºŠé¢„æµ‹æ¨¡å‹è¿‡åº¦ä¾èµ–ç¡¬ç¼–ç å’Œç‰¹å®šæ•°æ®åº“ç®¡é“æ‰€å¯¼è‡´çš„æ‰©å±•æ€§ä¸é‡ç°æ€§éš¾é¢˜ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åŠ¨æ€äº¤äº’å–ä»£äº†ä¼ ç»Ÿçš„äººå·¥è§„åˆ™ç¼–å†™ã€‚å…¶æ ¸å¿ƒç”±æ¨¡å—åŒ–æ™ºèƒ½ä½“ç»„æˆï¼Œèƒ½å¤Ÿè¿­ä»£è§‚å¯ŸæŸ¥è¯¢ç»“æœå¹¶å¯¹æ•°æ®åº“æ¨¡å¼ (schema) å’Œæ–‡æ¡£è¿›è¡Œæ¨ç†ï¼Œå°† SQL ä¸ä»…ç”¨äºæ•°æ®æ£€ç´¢ï¼Œæ›´ä½œä¸ºæ•°æ®åº“è§‚å¯Ÿå’Œå†³ç­–çš„å·¥å…·ã€‚ç ”ç©¶äººå‘˜åœ¨ MIMIC-IIIã€eICU å’Œ SICdb ç­‰å¤šä¸ªæ•°æ®åº“ä¸Šè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå®éªŒç»“æœè¯æ˜äº†è¯¥æ¡†æ¶åœ¨å¤„ç†å·²çŸ¥å’ŒæœªçŸ¥æ¨¡å¼æ—¶å‡å…·å¤‡æå¼ºçš„æ€§èƒ½ä¸æ³›åŒ–èƒ½åŠ›ã€‚è¯¥é¡¹å·¥ä½œæˆåŠŸè¯æ˜äº†è‡ªåŠ¨åŒ–åŸæœ¬éœ€è¦ä¸“å®¶çº§è®¾è®¡çš„å¤æ‚æ•°æ®æå–è¿‡ç¨‹æ˜¯å®Œå…¨å¯è¡Œçš„ï¼Œä¸ºåŒ»ç–—å¤§æ•°æ®çš„æ ‡å‡†åŒ–å¤„ç†æä¾›äº†é«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "currently under submission to ICLR 2026",
      "pdf_url": "https://arxiv.org/pdf/2510.00549v2",
      "published_date": "2025-10-01 06:10:04 UTC",
      "updated_date": "2025-10-02 03:21:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:04:23.482307+00:00"
    },
    {
      "arxiv_id": "2510.00547v1",
      "title": "Forestpest-YOLO: A High-Performance Detection Framework for Small Forestry Pests",
      "title_zh": "Forestpest-YOLOï¼šé¢å‘å°å‹æ—ä¸šå®³è™«çš„é«˜æ€§èƒ½æ£€æµ‹æ¡†æ¶",
      "authors": [
        "Aoduo Li",
        "Peikai Lin",
        "Jiancheng Li",
        "Zhen Zhang",
        "Shiting Wu",
        "Zexiao Liang",
        "Zhifa Jiang"
      ],
      "abstract": "Detecting agricultural pests in complex forestry environments using remote sensing imagery is fundamental for ecological preservation, yet it is severely hampered by practical challenges. Targets are often minuscule, heavily occluded, and visually similar to the cluttered background, causing conventional object detection models to falter due to the loss of fine-grained features and an inability to handle extreme data imbalance. To overcome these obstacles, this paper introduces Forestpest-YOLO, a detection framework meticulously optimized for the nuances of forestry remote sensing. Building upon the YOLOv8 architecture, our framework introduces a synergistic trio of innovations. We first integrate a lossless downsampling module, SPD-Conv, to ensure that critical high-resolution details of small targets are preserved throughout the network. This is complemented by a novel cross-stage feature fusion block, CSPOK, which dynamically enhances multi-scale feature representation while suppressing background noise. Finally, we employ VarifocalLoss to refine the training objective, compelling the model to focus on high-quality and hard-to-classify samples. Extensive experiments on our challenging, self-constructed ForestPest dataset demonstrate that Forestpest-YOLO achieves state-of-the-art performance, showing marked improvements in detecting small, occluded pests and significantly outperforming established baseline models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤æ‚æ—ä¸šé¥æ„Ÿå½±åƒä¸­ç›®æ ‡å¾®å°ã€é®æŒ¡ä¸¥é‡åŠèƒŒæ™¯å¹²æ‰°å¯¼è‡´çš„å®³è™«æ£€æµ‹éš¾é¢˜ï¼Œæå‡ºäº† Forestpest-YOLO é«˜æ€§èƒ½æ£€æµ‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶ä»¥ YOLOv8 æ¶æ„ä¸ºåŸºç¡€ï¼Œé›†æˆäº†æ— æŸä¸‹é‡‡æ ·æ¨¡å— SPD-Conv ä»¥ç¡®ä¿ç½‘ç»œå±‚çº§ä¸­ä¿ç•™å…³é”®çš„å°ç›®æ ‡é«˜åˆ†è¾¨ç‡ç»†èŠ‚ã€‚åŒæ—¶ï¼Œå¼•å…¥æ–°å‹è·¨é˜¶æ®µç‰¹å¾èåˆæ¨¡å— CSPOK åŠ¨æ€å¢å¼ºå¤šå°ºåº¦ç‰¹å¾è¡¨è¾¾å¹¶æŠ‘åˆ¶èƒŒæ™¯å™ªå£°ã€‚æ­¤å¤–ï¼Œç ”ç©¶é‡‡ç”¨ VarifocalLoss ä¼˜åŒ–è®­ç»ƒç›®æ ‡ï¼Œä½¿æ¨¡å‹æ›´ä¸“æ³¨äºé«˜è´¨é‡å’Œéš¾åˆ†ç±»æ ·æœ¬çš„è¯†åˆ«ã€‚åœ¨è‡ªå»ºçš„ ForestPest æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒForestpest-YOLO å®ç°äº†ç›®å‰æœ€å…ˆè¿›çš„ SOTA æ€§èƒ½ï¼Œåœ¨å¤„ç†å¾®å°åŠé®æŒ¡å®³è™«è¯†åˆ«ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00547v1",
      "published_date": "2025-10-01 06:06:40 UTC",
      "updated_date": "2025-10-01 06:06:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:04:44.567723+00:00"
    },
    {
      "arxiv_id": "2510.05136v1",
      "title": "Linguistic Characteristics of AI-Generated Text: A Survey",
      "title_zh": "AIç”Ÿæˆæ–‡æœ¬çš„è¯­è¨€ç‰¹å¾ï¼šç ”ç©¶ç»¼è¿°",
      "authors": [
        "Luka TerÄon",
        "Kaja Dobrovoljc"
      ],
      "abstract": "Large language models (LLMs) are solidifying their position in the modern world as effective tools for the automatic generation of text. Their use is quickly becoming commonplace in fields such as education, healthcare, and scientific research. There is a growing need to study the linguistic features present in AI-generated text, as the increasing presence of such texts has profound implications in various disciplines such as corpus linguistics, computational linguistics, and natural language processing. Many observations have already been made, however a broader synthesis of the findings made so far is required to provide a better understanding of the topic. The present survey paper aims to provide such a synthesis of extant research. We categorize the existing works along several dimensions, including the levels of linguistic description, the models included, the genres analyzed, the languages analyzed, and the approach to prompting. Additionally, the same scheme is used to present the findings made so far and expose the current trends followed by researchers. Among the most-often reported findings is the observation that AI-generated text is more likely to contain a more formal and impersonal style, signaled by the increased presence of nouns, determiners, and adpositions and the lower reliance on adjectives and adverbs. AI-generated text is also more likely to feature a lower lexical diversity, a smaller vocabulary size, and repetitive text. Current research, however, remains heavily concentrated on English data and mostly on text generated by the GPT model family, highlighting the need for broader cross-linguistic and cross-model investigation. In most cases authors also fail to address the issue of prompt sensitivity, leaving much room for future studies that employ multiple prompt wordings in the text generation phase.",
      "tldr_zh": "è¯¥ç»¼è¿°æ—¨åœ¨å¯¹ç°æœ‰å…³äºAIç”Ÿæˆæ–‡æœ¬(AI-generated text)è¯­è¨€ç‰¹å¾çš„ç ”ç©¶è¿›è¡Œç³»ç»ŸåŒ–çš„å½’çº³ä¸æ€»ç»“ã€‚ç ”ç©¶ä»è¯­è¨€æè¿°å±‚æ¬¡ã€åŒ…å«æ¨¡å‹ã€åˆ†æä½“è£ã€è¯­è¨€ç§ç±»ä»¥åŠæç¤º(prompting)æ–¹æ³•ç­‰å¤šä¸ªç»´åº¦å¯¹ç°æœ‰æ–‡çŒ®è¿›è¡Œäº†åˆ†ç±»ã€‚è°ƒæŸ¥å‘ç°ï¼ŒAIç”Ÿæˆæ–‡æœ¬é€šå¸¸å‘ˆç°å‡ºæ›´æ­£å¼ä¸”éäººç§°åŒ–çš„é£æ ¼ï¼Œè¡¨ç°ä¸ºåè¯ã€é™å®šè¯å’Œä»‹è¯æ¯”ä¾‹è¾ƒé«˜ï¼Œè€Œå¯¹å½¢å®¹è¯å’Œå‰¯è¯çš„ä¾èµ–ç¨‹åº¦è¾ƒä½ã€‚æ­¤å¤–ï¼Œè¿™ç±»æ–‡æœ¬å¾€å¾€å…·æœ‰è¾ƒä½çš„è¯æ±‡å¤šæ ·æ€§(lexical diversity)å’Œè¾ƒå°çš„è¯æ±‡é‡ï¼Œä¸”å®¹æ˜“å‡ºç°å†…å®¹é‡å¤ã€‚ç›®å‰çš„ç ”ç©¶ä»é«˜åº¦é›†ä¸­åœ¨è‹±è¯­æ•°æ®å’ŒGPTæ¨¡å‹ç³»åˆ—ä¸Šï¼Œå¯¹å…¶ä»–è¯­è¨€å’Œæ¨¡å‹çš„æ¢è®¨å°šæ˜¾ä¸è¶³ã€‚ä½œè€…æŒ‡å‡ºï¼Œç°æœ‰ç ”ç©¶å¤§å¤šå¿½ç•¥äº†æç¤ºæ•æ„Ÿæ€§(prompt sensitivity)çš„å½±å“ï¼Œæœªæ¥çš„ç ”ç©¶äºŸéœ€åœ¨æ–‡æœ¬ç”Ÿæˆé˜¶æ®µé‡‡ç”¨å¤šç§æç¤ºè¯è¡¨è¿°ï¼Œä»¥æ¨åŠ¨æ›´å¹¿æ³›çš„è·¨è¯­è¨€å’Œè·¨æ¨¡å‹è°ƒæŸ¥ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "26 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.05136v1",
      "published_date": "2025-10-01 05:44:28 UTC",
      "updated_date": "2025-10-01 05:44:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:04:41.780325+00:00"
    },
    {
      "arxiv_id": "2511.14768v1",
      "title": "Causally-Informed Reinforcement Learning for Adaptive Emotion-Aware Social Media Recommendation",
      "title_zh": "é¢å‘è‡ªé€‚åº”æƒ…ç»ªæ„ŸçŸ¥ç¤¾äº¤åª’ä½“æ¨èçš„å› æœå¯å‘å¼å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Bhavika Jain",
        "Robert Pitsko",
        "Ananya Drishti",
        "Mahfuza Farooque"
      ],
      "abstract": "Social media recommendation systems play a central role in shaping users' emotional experiences. However, most systems are optimized solely for engagement metrics, such as click rate, viewing time, or scrolling, without accounting for users' emotional states. Repeated exposure to emotionally charged content has been shown to negatively affect users' emotional well-being over time. We propose an Emotion-aware Social Media Recommendation (ESMR) framework that personalizes content based on users' evolving emotional trajectories. ESMR integrates a Transformer-based emotion predictor with a hybrid recommendation policy: a LightGBM model for engagement during stable periods and a reinforcement learning agent with causally informed rewards when negative emotional states persist. Through behaviorally grounded evaluation over 30-day interaction traces, ESMR demonstrates improved emotional recovery, reduced volatility, and strong engagement retention. ESMR offers a path toward emotionally aware recommendations without compromising engagement performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¤¾äº¤åª’ä½“æ¨èç³»ç»Ÿè¿‡åº¦ä¼˜åŒ– engagement æŒ‡æ ‡è€Œå¿½è§†ç”¨æˆ·å¿ƒç†å¥åº·çš„é—®é¢˜ï¼Œæå‡ºäº† Emotion-aware Social Media Recommendation (ESMR) æ¡†æ¶ã€‚è¯¥æ¡†æ¶é›†æˆäº†ä¸€ä¸ªåŸºäº Transformer çš„æƒ…ç»ªé¢„æµ‹å™¨ï¼Œå¹¶é‡‡ç”¨æ··åˆæ¨èç­–ç•¥æ¥å¹³è¡¡ç”¨æˆ·å‚ä¸åº¦ä¸æƒ…ç»ªå¥åº·ã€‚åœ¨æƒ…ç»ªç¨³å®šæœŸï¼Œç³»ç»Ÿä½¿ç”¨ LightGBM æ¨¡å‹ç»´æŒç‚¹å‡»ç‡ç­‰æŒ‡æ ‡ï¼Œè€Œå½“ç”¨æˆ·å¤„äºæŒç»­è´Ÿé¢æƒ…ç»ªæ—¶ï¼Œåˆ™åˆ‡æ¢è‡³å…·å¤‡ causally informed rewards çš„ reinforcement learning æ™ºèƒ½ä½“è¿›è¡Œå¹²é¢„ã€‚é€šè¿‡å¯¹30å¤©äº¤äº’è½¨è¿¹çš„è¯„ä¼°ï¼ŒESMR æ˜¾è‘—æå‡äº†ç”¨æˆ·çš„æƒ…ç»ªæ¢å¤é€Ÿåº¦å¹¶é™ä½äº†æƒ…ç»ªæ³¢åŠ¨æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¸ç‰ºç‰² engagement æ€§èƒ½çš„å‰æä¸‹å®ç°äº†æƒ…æ„Ÿæ„ŸçŸ¥çš„è‡ªé€‚åº”æ¨èï¼Œä¸ºæ„å»ºæ›´å…·ç¤¾ä¼šè´£ä»»æ„Ÿçš„æ¨èç³»ç»Ÿæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14768v1",
      "published_date": "2025-10-01 05:31:23 UTC",
      "updated_date": "2025-10-01 05:31:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:04:44.202153+00:00"
    },
    {
      "arxiv_id": "2510.01293v1",
      "title": "Cyber Academia-Chemical Engineering (CA-ChemE): A Living Digital Town for Self-Directed Research Evolution and Emergent Scientific Discovery",
      "title_zh": "Cyber Academia-Chemical Engineering (CA-ChemE)ï¼šæ”¯æŒè‡ªä¸»å¯¼å‘ç ”ç©¶æ¼”è¿›ä¸æ¶Œç°å¼ç§‘å­¦å‘ç°çš„åŠ¨æ€æ•°å­—åŸé•‡",
      "authors": [
        "Zekun Jiang",
        "Chunming Xu",
        "Tianhang Zhou"
      ],
      "abstract": "The rapid advancement of artificial intelligence (AI) has demonstrated substantial potential in chemical engineering, yet existing AI systems remain limited in interdisciplinary collaboration and exploration of uncharted problems. To address these issues, we present the Cyber Academia-Chemical Engineering (CA-ChemE) system, a living digital town that enables self-directed research evolution and emergent scientific discovery through multi-agent collaboration. By integrating domain-specific knowledge bases, knowledge enhancement technologies, and collaboration agents, the system successfully constructs an intelligent ecosystem capable of deep professional reasoning and efficient interdisciplinary collaboration. Our findings demonstrate that knowledge base-enabled enhancement mechanisms improved dialogue quality scores by 10-15% on average across all seven expert agents, fundamentally ensuring technical judgments are grounded in verifiable scientific evidence. However, we observed a critical bottleneck in cross-domain collaboration efficiency, prompting the introduction of a Collaboration Agent (CA) equipped with ontology engineering capabilities. CA's intervention achieved 8.5% improvements for distant-domain expert pairs compared to only 0.8% for domain-proximate pairs - a 10.6-fold difference - unveiling the \"diminished collaborative efficiency caused by knowledge-base gaps\" effect. This study demonstrates how carefully designed multi-agent architectures can provide a viable pathway toward autonomous scientific discovery in chemical engineering.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Cyber Academia-Chemical Engineering (CA-ChemE)ç³»ç»Ÿï¼Œä¸€ä¸ªæ—¨åœ¨é€šè¿‡å¤šæ™ºèƒ½ä½“åä½œ(multi-agent collaboration)å®ç°è‡ªä¸»ç ”ç©¶è¿›åŒ–å’Œçªå‘æ€§ç§‘å­¦å‘ç°çš„â€œæ•°å­—åŒ–åŸé•‡â€ã€‚è¯¥ç³»ç»Ÿé›†æˆäº†é¢†åŸŸä¸“ç”¨çŸ¥è¯†åº“(knowledge bases)ã€çŸ¥è¯†å¢å¼ºæŠ€æœ¯ä»¥åŠåä½œæ™ºèƒ½ä½“(Collaboration Agent, CA)ï¼Œæ„å»ºäº†ä¸€ä¸ªæ”¯æŒæ·±å±‚ä¸“ä¸šæ¨ç†å’Œé«˜æ•ˆè·¨å­¦ç§‘åä½œçš„æ™ºèƒ½ç”Ÿæ€ç³»ç»Ÿã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒçŸ¥è¯†åº“å¢å¼ºæœºåˆ¶ä½¿ä¸“å®¶æ™ºèƒ½ä½“çš„å¯¹è¯è´¨é‡å¹³å‡æå‡äº†10-15%ï¼Œç¡®ä¿äº†æŠ€æœ¯åˆ¤æ–­å‡åŸºäºå¯éªŒè¯çš„ç§‘å­¦ä¾æ®ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†â€œçŸ¥è¯†åº“å·®è·å¯¼è‡´çš„åä½œæ•ˆç‡ä¸‹é™â€æ•ˆåº”ï¼Œå¹¶é€šè¿‡å…·å¤‡æœ¬ä½“å·¥ç¨‹(ontology engineering)èƒ½åŠ›çš„åä½œæ™ºèƒ½ä½“ï¼Œä½¿è¿œç«¯é¢†åŸŸä¸“å®¶ç»„çš„åä½œæ•ˆç‡æå‡äº†8.5%ï¼Œå…¶ä¼˜åŒ–æ•ˆæœæ˜¯è¿‘ç«¯é¢†åŸŸç»„çš„10.6å€ã€‚è¯¥ç ”ç©¶è¯æ˜äº†ç²¾å¿ƒè®¾è®¡çš„å¤šæ™ºèƒ½ä½“æ¶æ„æ˜¯å®ç°åŒ–å­¦å·¥ç¨‹é¢†åŸŸè‡ªä¸»ç§‘å­¦å‘ç°(autonomous scientific discovery)çš„ä¸€æ¡å¯è¡Œè·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01293v1",
      "published_date": "2025-10-01 05:26:55 UTC",
      "updated_date": "2025-10-01 05:26:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:04:46.770909+00:00"
    },
    {
      "arxiv_id": "2510.00523v1",
      "title": "VIRTUE: Visual-Interactive Text-Image Universal Embedder",
      "title_zh": "VIRTUEï¼šè§†è§‰äº¤äº’å¼å›¾æ–‡é€šç”¨åµŒå…¥å™¨",
      "authors": [
        "Wei-Yao Wang",
        "Kazuya Tateishi",
        "Qiyu Wu",
        "Shusuke Takahashi",
        "Yuki Mitsufuji"
      ],
      "abstract": "Multimodal representation learning models have demonstrated successful operation across complex tasks, and the integration of vision-language models (VLMs) has further enabled embedding models with instruction-following capabilities. However, existing embedding models lack visual-interactive capabilities to specify regions of interest from users (e.g., point, bounding box, mask), which have been explored in generative models to broaden their human-interactive applicability. Equipping embedding models with visual interactions not only would unlock new applications with localized grounding of user intent, which remains unexplored, but also enable the models to learn entity-level information within images to complement their global representations for conventional embedding tasks. In this paper, we propose a novel Visual-InteRactive Text-Image Universal Embedder (VIRTUE) that extends the capabilities of the segmentation model and the vision-language model to the realm of representation learning. In VIRTUE, the segmentation model can process visual prompts that pinpoint specific regions within an image, thereby enabling the embedder to handle complex and ambiguous scenarios more precisely. To evaluate the visual-interaction ability of VIRTUE, we introduce a large-scale Segmentation-and-Scene Caption Retrieval (SCaR) benchmark comprising 1M samples that aims to retrieve the text caption by jointly considering the entity with a specific object and image scene. VIRTUE consistently achieves a state-of-the-art performance with significant improvements across 36 universal MMEB (3.1%-8.5%) and five visual-interactive SCaR (15.2%-20.3%) tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†VIRTUEï¼Œä¸€ç§è§†è§‰äº¤äº’å¼æ–‡æœ¬-å›¾åƒé€šç”¨åµŒå…¥æ¨¡å‹(Visual-InteRactive Text-Image Universal Embedder)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åµŒå…¥æ¨¡å‹ç¼ºä¹å¤„ç†è§†è§‰äº¤äº’ï¼ˆå¦‚ç‚¹ã€æ¡†ã€æ©ç ç­‰æŒ‡å®šå…´è¶£åŒºåŸŸï¼‰èƒ½åŠ›çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å·§å¦™åœ°å°†åˆ†å‰²æ¨¡å‹(segmentation model)ä¸è§†è§‰è¯­è¨€æ¨¡å‹(VLM)çš„èƒ½åŠ›æ‰©å±•è‡³è¡¨ç¤ºå­¦ä¹ é¢†åŸŸï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†å¤æ‚çš„è§†è§‰æç¤ºå¹¶ç²¾å‡†å®šä½å›¾åƒä¸­çš„ç‰¹å®šåŒºåŸŸã€‚VIRTUEä¸ä»…èƒ½é€šè¿‡å­¦ä¹ å®ä½“çº§ä¿¡æ¯(entity-level information)æ¥è¡¥å……å›¾åƒçš„å…¨å±€è¡¨ç¤ºï¼Œè¿˜è§£é”äº†ç”¨æˆ·æ„å›¾å±€éƒ¨åŒ–å…³è”(localized grounding)çš„æ–°åº”ç”¨åœºæ™¯ã€‚ä¸ºäº†è¯„ä¼°è¿™ä¸€èƒ½åŠ›ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†åŒ…å«100ä¸‡æ ·æœ¬çš„å¤§è§„æ¨¡SCaRåŸºå‡†æµ‹è¯•ï¼Œä¸“æ³¨äºç»“åˆç‰¹å®šç‰©ä½“ä¸å›¾åƒåœºæ™¯çš„å­—å¹•æ£€ç´¢ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒVIRTUEåœ¨36é¡¹MMEBé€šç”¨ä»»åŠ¡å’Œ5é¡¹SCaRè§†è§‰äº¤äº’ä»»åŠ¡ä¸­å‡æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ï¼Œåˆ†åˆ«å–å¾—äº†3.1%-8.5%å’Œ15.2%-20.3%çš„æ€§èƒ½æå‡ï¼Œè¾¾åˆ°äº†å½“å‰çš„SOTAæ°´å¹³ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "25 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.00523v1",
      "published_date": "2025-10-01 05:11:54 UTC",
      "updated_date": "2025-10-01 05:11:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:04:47.377608+00:00"
    },
    {
      "arxiv_id": "2510.00519v1",
      "title": "Architectural Transformations and Emerging Verification Demands in AI-Enabled Cyber-Physical Systems",
      "title_zh": "AI èµ‹èƒ½çš„ä¿¡æ¯ç‰©ç†ç³»ç»Ÿä¸­çš„æ¶æ„å˜é©ä¸æ–°å…´éªŒè¯éœ€æ±‚",
      "authors": [
        "Hadiza Umar Yusuf",
        "Khouloud Gaaloul"
      ],
      "abstract": "In the world of Cyber-Physical Systems (CPS), a captivating real-time fusion occurs where digital technology meets the physical world. This synergy has been significantly transformed by the integration of artificial intelligence (AI), a move that dramatically enhances system adaptability and introduces a layer of complexity that impacts CPS control optimization and reliability. Despite advancements in AI integration, a significant gap remains in understanding how this shift affects CPS architecture, operational complexity, and verification practices. The extended abstract addresses this gap by investigating architectural distinctions between AI-driven and traditional control models designed in Simulink and their respective implications for system verification.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½(AI)é›†æˆåœ¨ä¿¡æ¯ç‰©ç†ç³»ç»Ÿ(Cyber-Physical Systems, CPS)ä¸­æ‰€å¸¦æ¥çš„æ¶æ„å˜é©åŠå…¶äº§ç”Ÿçš„éªŒè¯éœ€æ±‚ã€‚è™½ç„¶AIæ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„è‡ªé€‚åº”èƒ½åŠ›ï¼Œä½†åŒæ—¶ä¹Ÿå¢åŠ äº†ç³»ç»Ÿåœ¨æ§åˆ¶ä¼˜åŒ–å’Œå¯é æ€§æ–¹é¢çš„å¤æ‚æ€§ã€‚ç›®å‰å­¦æœ¯ç•Œåœ¨ç†è§£è¿™ç§è½¬å˜å¦‚ä½•å½±å“CPSæ¶æ„ã€æ“ä½œå¤æ‚æ€§ä»¥åŠéªŒè¯å®è·µæ–¹é¢ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚æœ¬è®ºæ–‡é€šè¿‡å¯¹æ¯”ç ”ç©¶åœ¨Simulinkä¸­è®¾è®¡çš„AIé©±åŠ¨æ¨¡å‹ä¸ä¼ ç»Ÿæ§åˆ¶æ¨¡å‹ï¼Œæ·±å…¥åˆ†æäº†ä¸¤è€…åœ¨æ¶æ„ä¸Šçš„æœ¬è´¨åŒºåˆ«åŠå…¶å¯¹ç³»ç»ŸéªŒè¯(Verification)çš„å½±å“ã€‚è¯¥ç ”ç©¶æ—¨åœ¨å¼¥åˆAIæŠ€æœ¯é›†æˆä¸CPSå¯é æ€§è¯„ä¼°ä¹‹é—´çš„çŸ¥è¯†é¸¿æ²Ÿï¼Œä¸ºåº”å¯¹æ–°å…´çš„ç³»ç»ŸéªŒè¯æŒ‘æˆ˜æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00519v1",
      "published_date": "2025-10-01 05:09:12 UTC",
      "updated_date": "2025-10-01 05:09:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:05:34.666490+00:00"
    },
    {
      "arxiv_id": "2510.00512v1",
      "title": "Adaptive Data-Knowledge Alignment in Genetic Perturbation Prediction",
      "title_zh": "åŸºå› æ‰°åŠ¨é¢„æµ‹ä¸­çš„è‡ªé€‚åº”æ•°æ®-çŸ¥è¯†å¯¹é½",
      "authors": [
        "Yuanfang Xiang",
        "Lun Ai"
      ],
      "abstract": "The transcriptional response to genetic perturbation reveals fundamental insights into complex cellular systems. While current approaches have made progress in predicting genetic perturbation responses, they provide limited biological understanding and cannot systematically refine existing knowledge. Overcoming these limitations requires an end-to-end integration of data-driven learning and existing knowledge. However, this integration is challenging due to inconsistencies between data and knowledge bases, such as noise, misannotation, and incompleteness. To address this challenge, we propose ALIGNED (Adaptive aLignment for Inconsistent Genetic kNowledgE and Data), a neuro-symbolic framework based on the Abductive Learning (ABL) paradigm. This end-to-end framework aligns neural and symbolic components and performs systematic knowledge refinement. We introduce a balanced consistency metric to evaluate the predictions' consistency against both data and knowledge. Our results show that ALIGNED outperforms state-of-the-art methods by achieving the highest balanced consistency, while also re-discovering biologically meaningful knowledge. Our work advances beyond existing methods to enable both the transparency and the evolution of mechanistic biological understanding.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é—ä¼ æ‰°åŠ¨ (genetic perturbation) é¢„æµ‹ä¸­ç”Ÿç‰©å­¦ç†è§£ä¸è¶³ä»¥åŠæ— æ³•ç³»ç»Ÿä¿®æ­£ç°æœ‰çŸ¥è¯†çš„é—®é¢˜ï¼Œæå‡ºäº† ALIGNED (Adaptive aLignment for Inconsistent Genetic kNowledgE and Data) ç¥ç»ç¬¦å· (neuro-symbolic) æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŸºäºæ¼”ç»å­¦ä¹  (Abductive Learning, ABL) èŒƒå¼ï¼Œé€šè¿‡ç«¯åˆ°ç«¯åœ°å¯¹é½ç¥ç»å’Œç¬¦å·ç»„ä»¶æ¥è§£å†³æ•°æ®ä¸çŸ¥è¯†åº“ä¹‹é—´å› å™ªå£°å’Œæ³¨é‡Šé”™è¯¯å¯¼è‡´çš„ä¸ä¸€è‡´æ€§ã€‚ç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†ä¸€ç§å¹³è¡¡ä¸€è‡´æ€§æŒ‡æ ‡ (balanced consistency metric) æ¥è¡¡é‡é¢„æµ‹ç»“æœä¸æ•°æ®åŠçŸ¥è¯†çš„å¥‘åˆåº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒALIGNED åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€é«˜çš„ä¸€è‡´æ€§è¯„åˆ†ï¼Œå¹¶èƒ½å¤Ÿé‡æ–°å‘ç°å…·æœ‰ç”Ÿç‰©å­¦æ„ä¹‰çš„çŸ¥è¯†ã€‚è¯¥ç ”ç©¶æœ‰æ•ˆæ¨åŠ¨äº†æœºåˆ¶æ€§ç”Ÿç‰©å­¦ç†è§£çš„é€æ˜åŒ–ä¸è‡ªåŠ¨åŒ–æ¼”åŒ–ã€‚",
      "categories": [
        "q-bio.MN",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.MN",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00512v1",
      "published_date": "2025-10-01 04:48:43 UTC",
      "updated_date": "2025-10-01 04:48:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:05:34.471716+00:00"
    },
    {
      "arxiv_id": "2510.00508v1",
      "title": "Copy-Paste to Mitigate Large Language Model Hallucinations",
      "title_zh": "é€šè¿‡å¤åˆ¶ç²˜è´´ç¼“è§£å¤§è¯­è¨€æ¨¡å‹å¹»è§‰",
      "authors": [
        "Yongchao Long",
        "Xian Wu",
        "Yingying Zhang",
        "Xianbin Wen",
        "Yuxi Zhou",
        "Shenda Hong"
      ],
      "abstract": "While Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to generate contextually grounded responses, contextual faithfulness remains challenging as LLMs may not consistently trust provided context, leading to hallucinations that undermine reliability. We observe an inverse correlation between response copying degree and context-unfaithful hallucinations on RAGTruth, suggesting that higher copying degrees reduce hallucinations by fostering genuine contextual belief. We propose CopyPasteLLM, obtained through two-stage high-copying response preference training. We design three prompting methods to enhance copying degree, demonstrating that high-copying responses achieve superior contextual faithfulness and hallucination control. These approaches enable a fully automated pipeline that transforms generated responses into high-copying preference data for training CopyPasteLLM. On FaithEval, ConFiQA and PubMedQA, CopyPasteLLM achieves best performance in both counterfactual and original contexts, remarkably with 12.2% to 24.5% accuracy improvements on FaithEval over the best baseline, while requiring only 365 training samples -- 1/50th of baseline data. To elucidate CopyPasteLLM's effectiveness, we propose the Context-Parameter Copying Capturing algorithm. Interestingly, this reveals that CopyPasteLLM recalibrates reliance on internal parametric knowledge rather than external knowledge during generation. All codes are available at https://github.com/longyongchao/CopyPasteLLM",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ä¸­å¤§è¯­è¨€æ¨¡å‹(LLMs)å› ä¸ä¿¡ä»»ä¸Šä¸‹æ–‡è€Œäº§ç”Ÿçš„å¹»è§‰é—®é¢˜ï¼Œå‘ç°å“åº”çš„å¤åˆ¶ç¨‹åº¦(copying degree)ä¸å¿ å®åº¦ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ­£ç›¸å…³å…³ç³»ã€‚åŸºäºæ­¤å‘ç°ï¼Œä½œè€…æå‡ºäº† CopyPasteLLMï¼Œé€šè¿‡ä¸¤é˜¶æ®µé«˜å¤åˆ¶åå¥½è®­ç»ƒå’Œç‰¹å®šçš„æç¤ºæ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹å¤–éƒ¨ä¸Šä¸‹æ–‡çš„åˆ©ç”¨ç‡ã€‚å®éªŒè¡¨æ˜ï¼ŒCopyPasteLLM åœ¨ FaithEvalã€ConFiQA å’Œ PubMedQA ä»»åŠ¡ä¸­å‡å–å¾—äº†æœ€ä¼˜æ€§èƒ½ï¼Œå…¶ä¸­åœ¨ FaithEval ä¸Šçš„å‡†ç¡®ç‡æ¯”æœ€å¼ºåŸºçº¿æå‡äº† 12.2% è‡³ 24.5%ï¼Œä¸”è®­ç»ƒä»…éœ€ 365 ä¸ªæ ·æœ¬ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº† Context-Parameter Copying Capturing ç®—æ³•ï¼Œæ­ç¤ºäº†è¯¥æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆé‡æ–°æ ¡å‡†å†…éƒ¨å‚æ•°åŒ–çŸ¥è¯†ä¸å¤–éƒ¨çŸ¥è¯†ä¹‹é—´çš„ä¾èµ–å¹³è¡¡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00508v1",
      "published_date": "2025-10-01 04:40:04 UTC",
      "updated_date": "2025-10-01 04:40:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:05:00.581988+00:00"
    },
    {
      "arxiv_id": "2510.00507v2",
      "title": "Graph2Eval: Automatic Multimodal Task Generation for Agents via Knowledge Graphs",
      "title_zh": "Graph2Evalï¼šåŸºäºçŸ¥è¯†å›¾è°±çš„æ™ºèƒ½ä½“å¤šæ¨¡æ€ä»»åŠ¡è‡ªåŠ¨ç”Ÿæˆ",
      "authors": [
        "Yurun Chen",
        "Xavier Hu",
        "Yuhan Liu",
        "Ziqi Wang",
        "Zeyi Liao",
        "Lin Chen",
        "Feng Wei",
        "Yuxi Qian",
        "Bo Zheng",
        "Keting Yin",
        "Shengyu Zhang"
      ],
      "abstract": "As multimodal LLM-driven agents continue to advance in autonomy and generalization, evaluation based on static datasets can no longer adequately assess their true capabilities in dynamic environments and diverse tasks. Existing LLM-based synthetic data methods are largely designed for LLM training and evaluation, and thus cannot be directly applied to agent tasks that require tool use and interactive capabilities. While recent studies have explored automatic agent task generation with LLMs, most efforts remain limited to text or image analysis, without systematically modeling multi-step interactions in web environments. To address these challenges, we propose Graph2Eval, a knowledge graph-based framework that automatically generates both multimodal document comprehension tasks and web interaction tasks, enabling comprehensive evaluation of agents' reasoning, collaboration, and interactive capabilities. In our approach, knowledge graphs constructed from multi-source external data serve as the task space, where we translate semantic relations into structured multimodal tasks using subgraph sampling, task templates, and meta-paths. A multi-stage filtering pipeline based on node reachability, LLM scoring, and similarity analysis is applied to guarantee the quality and executability of the generated tasks. Furthermore, Graph2Eval supports end-to-end evaluation of multiple agent types (Single-Agent, Multi-Agent, Web Agent) and measures reasoning, collaboration, and interaction capabilities. We instantiate the framework with Graph2Eval-Bench, a curated dataset of 1,319 tasks spanning document comprehension and web interaction scenarios. Experiments show that Graph2Eval efficiently generates tasks that differentiate agent and model performance, revealing gaps in reasoning, collaboration, and web interaction across different settings and offering a new perspective for agent evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Graph2Evalï¼Œä¸€ç§åŸºäºçŸ¥è¯†å›¾è°±(Knowledge Graphs)çš„è‡ªåŠ¨ä»»åŠ¡ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é™æ€æ•°æ®é›†æ— æ³•æœ‰æ•ˆè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“åœ¨åŠ¨æ€ç¯å¢ƒä¸­äº¤äº’èƒ½åŠ›çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤šæºå¤–éƒ¨æ•°æ®æ„å»ºçŸ¥è¯†å›¾è°±ä½œä¸ºä»»åŠ¡ç©ºé—´ï¼Œé€šè¿‡å­å›¾é‡‡æ ·(Subgraph Sampling)ã€ä»»åŠ¡æ¨¡æ¿å’Œå…ƒè·¯å¾„(Meta-paths)å°†è¯­ä¹‰å…³ç³»è½¬åŒ–ä¸ºç»“æ„åŒ–çš„å¤šæ¨¡æ€æ–‡æ¡£ç†è§£ä¸ç½‘é¡µäº¤äº’ä»»åŠ¡ã€‚ä¸ºäº†ç¡®ä¿ä»»åŠ¡è´¨é‡ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†åŒ…å«èŠ‚ç‚¹å¯è¾¾æ€§ã€LLMè¯„åˆ†åŠç›¸ä¼¼åº¦åˆ†æçš„å¤šé˜¶æ®µè¿‡æ»¤æµæ°´çº¿ï¼Œå¹¶æ”¯æŒå¯¹å•æ™ºèƒ½ä½“(Single-Agent)ã€å¤šæ™ºèƒ½ä½“(Multi-Agent)åŠç½‘é¡µæ™ºèƒ½ä½“(Web Agent)è¿›è¡Œæ¨ç†ã€åä½œå’Œäº¤äº’èƒ½åŠ›çš„ç«¯åˆ°ç«¯è¯„ä¼°ã€‚å®éªŒé€šè¿‡åŒ…å«1319ä¸ªä»»åŠ¡çš„Graph2Eval-Benchæ•°æ®é›†è¯æ˜ï¼Œè¯¥æ¡†æ¶èƒ½é«˜æ•ˆç”Ÿæˆå…·æœ‰åŒºåˆ†åº¦çš„ä»»åŠ¡ï¼Œå¹¶æ­ç¤ºäº†ç°æœ‰æ™ºèƒ½ä½“åœ¨å¤æ‚äº¤äº’åœºæ™¯ä¸­çš„èƒ½åŠ›çŸ­æ¿ï¼Œä¸ºæ™ºèƒ½ä½“è¯„ä¼°æä¾›äº†æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "20 pages, 10 figures. Our Code: https://github.com/YurunChen/Graph2Eval",
      "pdf_url": "https://arxiv.org/pdf/2510.00507v2",
      "published_date": "2025-10-01 04:37:54 UTC",
      "updated_date": "2025-10-14 02:11:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:05:02.372426+00:00"
    },
    {
      "arxiv_id": "2510.00500v1",
      "title": "Relative-Absolute Fusion: Rethinking Feature Extraction in Image-Based Iterative Method Selection for Solving Sparse Linear Systems",
      "title_zh": "Relative-Absolute Fusionï¼šé‡æ–°å®¡è§†ç”¨äºæ±‚è§£ç¨€ç–çº¿æ€§ç³»ç»Ÿçš„åŸºäºå›¾åƒè¿­ä»£æ³•é€‰æ‹©ä¸­çš„ç‰¹å¾æå–",
      "authors": [
        "Kaiqi Zhang",
        "Mingguan Yang",
        "Dali Chang",
        "Chun Chen",
        "Yuxiang Zhang",
        "Kexun He",
        "Jing Zhao"
      ],
      "abstract": "Iterative method selection is crucial for solving sparse linear systems because these methods inherently lack robustness. Though image-based selection approaches have shown promise, their feature extraction techniques might encode distinct matrices into identical image representations, leading to the same selection and suboptimal method. In this paper, we introduce RAF (Relative-Absolute Fusion), an efficient feature extraction technique to enhance image-based selection approaches. By simultaneously extracting and fusing image representations as relative features with corresponding numerical values as absolute features, RAF achieves comprehensive matrix representations that prevent feature ambiguity across distinct matrices, thus improving selection accuracy and unlocking the potential of image-based selection approaches. We conducted comprehensive evaluations of RAF on SuiteSparse and our developed BMCMat (Balanced Multi-Classification Matrix dataset), demonstrating solution time reductions of 0.08s-0.29s for sparse linear systems, which is 5.86%-11.50% faster than conventional image-based selection approaches and achieves state-of-the-art (SOTA) performance. BMCMat is available at https://github.com/zkqq/BMCMat.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¨€ç–çº¿æ€§ç³»ç»Ÿæ±‚è§£ä¸­çš„è¿­ä»£æ–¹æ³•é€‰æ‹©(Iterative method selection)é—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„åŸºäºå›¾åƒçš„é€‰æ‹©æ–¹æ³•åœ¨ç‰¹å¾æå–æ—¶å®¹æ˜“å¯¼è‡´ä¸åŒçŸ©é˜µäº§ç”Ÿç›¸åŒå›¾åƒè¡¨ç¤ºï¼Œä»è€Œå¼•èµ·ç‰¹å¾æ­§ä¹‰ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†RAF (Relative-Absolute Fusion)ç‰¹å¾æå–æŠ€æœ¯ï¼Œé€šè¿‡åŒæ—¶æå–å¹¶èåˆä½œä¸ºç›¸å¯¹ç‰¹å¾(relative features)çš„å›¾åƒè¡¨ç¤ºä¸ä½œä¸ºç»å¯¹ç‰¹å¾(absolute features)çš„ç›¸åº”æ•°å€¼ï¼Œå®ç°äº†æ›´å…¨é¢çš„çŸ©é˜µè¡¨ç¤ºã€‚è¯¥æ–¹æ³•æœ‰æ•ˆé˜²æ­¢äº†ä¸åŒçŸ©é˜µé—´çš„ç‰¹å¾æ¨¡ç³Šï¼Œæ˜¾è‘—æå‡äº†é€‰æ‹©å‡†ç¡®æ€§å¹¶é‡Šæ”¾äº†å›¾åƒåŒ–æ–¹æ³•çš„æ½œåŠ›ã€‚åœ¨SuiteSparseå’ŒBMCMatæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRAFç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•èƒ½å‡å°‘0.08sè‡³0.29sçš„æ±‚è§£æ—¶é—´ï¼Œæ•ˆç‡æå‡è¾¾5.86%è‡³11.50%ï¼Œè¾¾åˆ°äº†ç›®å‰çš„SOTAæ°´å¹³ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜å‘å¸ƒäº†å¹³è¡¡å¤šåˆ†ç±»çŸ©é˜µæ•°æ®é›†BMCMatï¼Œä¸ºè¯¥é¢†åŸŸçš„åç»­ç ”ç©¶æä¾›äº†é‡è¦èµ„æºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00500v1",
      "published_date": "2025-10-01 04:33:23 UTC",
      "updated_date": "2025-10-01 04:33:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:05:05.672262+00:00"
    },
    {
      "arxiv_id": "2510.00499v2",
      "title": "MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance",
      "title_zh": "MOSS-Speechï¼šè¿ˆå‘æ— éœ€æ–‡æœ¬å¼•å¯¼çš„çœŸå®è¯­éŸ³åˆ°è¯­éŸ³æ¨¡å‹",
      "authors": [
        "Xingjian Zhao",
        "Zhe Xu",
        "Qinyuan Cheng",
        "Zhaoye Fei",
        "Luozhijie Jin",
        "Yang Wang",
        "Hanfu Chen",
        "Yaozhou Jiang",
        "Qinghui Gao",
        "Ke Chen",
        "Ruixiao Li",
        "Mingshu Chen",
        "Ruiming Wang",
        "Wenbo Zhang",
        "Yiyang Zhang",
        "Donghua Yu",
        "Yang Gao",
        "Xiaogui Yang",
        "Yitian Gong",
        "Yuanfan Xu",
        "Yaqian Zhou",
        "Xuanjing Huang",
        "Xipeng Qiu"
      ],
      "abstract": "Spoken dialogue systems often rely on cascaded pipelines that transcribe, process, and resynthesize speech. While effective, this design discards paralinguistic cues and limits expressivity. Recent end-to-end methods reduce latency and better preserve these cues, yet still rely on text intermediates, creating a fundamental bottleneck. We present MOSS-Speech, a true speech-to-speech large language model that directly understands and generates speech without relying on text guidance. Our approach combines a modality-based layer-splitting architecture with a frozen pre-training strategy, preserving the reasoning and knowledge of pretrained text LLMs while adding native speech capabilities. Experiments show that our model achieves state-of-the-art results in spoken question answering and delivers comparable speech-to-speech performance relative to existing text-guided systems, while still maintaining competitive text performance. By narrowing the gap between text-guided and direct speech generation, our work establishes a new paradigm for expressive and efficient end-to-end speech interaction.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MOSS-Speechï¼Œä¸€ç§æ— éœ€æ–‡æœ¬å¼•å¯¼(Text Guidance)çš„çœŸå®ç«¯åˆ°ç«¯è¯­éŸ³è½¬è¯­éŸ³(Speech-to-Speech)å¤§è¯­è¨€æ¨¡å‹ã€‚é’ˆå¯¹ä¼ ç»Ÿè¯­éŸ³å¯¹è¯ç³»ç»Ÿå› ä¾èµ–æ–‡æœ¬ä¸­é—´ä½“è€Œå¯¼è‡´è¡¨è¾¾åŠ›å—é™ä¸”ä¸¢å¤±å‰¯è¯­è¨€çº¿ç´¢(Paralinguistic Cues)çš„é—®é¢˜ï¼Œè¯¥æ¨¡å‹å®ç°äº†ç›´æ¥çš„è¯­éŸ³ç†è§£ä¸ç”Ÿæˆã€‚åœ¨æ–¹æ³•è®ºä¸Šï¼Œæ¨¡å‹ç»“åˆäº†åŸºäºæ¨¡æ€çš„å±‚æ‹†åˆ†æ¶æ„(Modality-based Layer-splitting Architecture)ä¸å†»ç»“é¢„è®­ç»ƒç­–ç•¥ï¼ŒæˆåŠŸåœ¨ä¿ç•™é¢„è®­ç»ƒæ–‡æœ¬å¤§è¯­è¨€æ¨¡å‹(Text LLMs)æ¨ç†èƒ½åŠ›çš„åŸºç¡€ä¸Šèå…¥äº†åŸç”Ÿè¯­éŸ³èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒMOSS-Speechåœ¨å£è¯­é—®ç­”ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼Œå…¶è¯­éŸ³è½¬è¯­éŸ³è¡¨ç°å¯ä¸ç°æœ‰çš„æ–‡æœ¬å¼•å¯¼ç³»ç»Ÿç›¸åª²ç¾ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨ä¿æŒç«äº‰æ€§æ–‡æœ¬æ€§èƒ½çš„åŒæ—¶ï¼Œæœ‰æ•ˆç¼©å‡äº†ç›´æ¥è¯­éŸ³ç”Ÿæˆä¸æ–‡æœ¬è¾…åŠ©ç”Ÿæˆä¹‹é—´çš„å·®è·ã€‚è¿™é¡¹å·¥ä½œä¸ºå®ç°é«˜æ•ˆä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„ç«¯åˆ°ç«¯è¯­éŸ³äº¤äº’å»ºç«‹äº†ä¸€ç§å…¨æ–°çš„æŠ€æœ¯èŒƒå¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00499v2",
      "published_date": "2025-10-01 04:32:37 UTC",
      "updated_date": "2025-10-02 13:05:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:05:49.282085+00:00"
    },
    {
      "arxiv_id": "2510.00495v3",
      "title": "Normal-Abnormal Guided Generalist Anomaly Detection",
      "title_zh": "æ­£å¸¸ä¸å¼‚å¸¸å¼•å¯¼çš„é€šç”¨å¼‚å¸¸æ£€æµ‹",
      "authors": [
        "Yuexin Wang",
        "Xiaolei Wang",
        "Yizheng Gong",
        "Jimin Xiao"
      ],
      "abstract": "Generalist Anomaly Detection (GAD) aims to train a unified model on an original domain that can detect anomalies in new target domains. Previous GAD methods primarily use only normal samples as references, overlooking the valuable information contained in anomalous samples that are often available in real-world scenarios. To address this limitation, we propose a more practical approach: normal-abnormal-guided generalist anomaly detection, which leverages both normal and anomalous samples as references to guide anomaly detection across diverse domains. We introduce the Normal-Abnormal Generalist Learning (NAGL) framework, consisting of two key components: Residual Mining (RM) and Anomaly Feature Learning (AFL). RM extracts abnormal patterns from normal-abnormal reference residuals to establish transferable anomaly representations, while AFL adaptively learns anomaly features in query images through residual mapping to identify instance-aware anomalies. Our approach effectively utilizes both normal and anomalous references for more accurate and efficient cross-domain anomaly detection. Extensive experiments across multiple benchmarks demonstrate that our method significantly outperforms existing GAD approaches. This work represents the first to adopt a mixture of normal and abnormal samples as references in generalist anomaly detection. The code and datasets are available at https://github.com/JasonKyng/NAGL.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é€šç”¨å¼‚å¸¸æ£€æµ‹ (Generalist Anomaly Detection, GAD) é¢†åŸŸä¸­ä»¥å¾€æ–¹æ³•ä»…ä¾èµ–æ­£å¸¸æ ·æœ¬è€Œå¿½è§†å¼‚å¸¸æ ·æœ¬ä¿¡æ¯çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨æ­£å¼‚å¸¸æ ·æœ¬å…±åŒå¼•å¯¼çš„å…¨æ–°æ£€æµ‹æ–¹æ³•ã€‚ç ”ç©¶è€…å¼•å…¥äº†æ­£å¼‚å¸¸é€šç”¨å­¦ä¹  (Normal-Abnormal Generalist Learning, NAGL) æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…å«æ®‹å·®æŒ–æ˜ (Residual Mining, RM) å’Œå¼‚å¸¸ç‰¹å¾å­¦ä¹  (Anomaly Feature Learning, AFL) ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ã€‚RM é€šè¿‡ä»æ­£å¼‚å¸¸å‚è€ƒæ®‹å·®ä¸­æå–æ¨¡å¼æ¥å»ºç«‹å¯è¿ç§»çš„å¼‚å¸¸è¡¨å¾ï¼Œè€Œ AFL åˆ™åˆ©ç”¨æ®‹å·®æ˜ å°„è‡ªé€‚åº”æ•è·æŸ¥è¯¢å›¾åƒä¸­çš„å¼‚å¸¸ç‰¹å¾ä»¥è¯†åˆ«å®ä¾‹æ„ŸçŸ¥å¼‚å¸¸ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„ GAD æ–¹æ³•ï¼Œå®ç°äº†æ›´ç²¾å‡†ã€é«˜æ•ˆçš„è·¨é¢†åŸŸæ£€æµ‹ã€‚è¯¥å·¥ä½œæ˜¯å­¦æœ¯ç•Œé¦–æ¬¡åœ¨é€šç”¨å¼‚å¸¸æ£€æµ‹ä¸­å¼•å…¥æ­£å¸¸ä¸å¼‚å¸¸æ ·æœ¬æ··åˆå‚è€ƒçš„å°è¯•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.00495v3",
      "published_date": "2025-10-01 04:27:10 UTC",
      "updated_date": "2025-10-17 04:56:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:05:49.872514+00:00"
    },
    {
      "arxiv_id": "2510.00494v2",
      "title": "Exploring System 1 and 2 communication for latent reasoning in LLMs",
      "title_zh": "æ¢ç´¢å¤§è¯­è¨€æ¨¡å‹æ½œç©ºé—´æ¨ç†ä¸­çš„ç³»ç»Ÿ 1 ä¸ç³»ç»Ÿ 2 é€šä¿¡",
      "authors": [
        "Julian Coda-Forno",
        "Zhuokai Zhao",
        "Qiang Zhang",
        "Dipesh Tamboli",
        "Weiwei Li",
        "Xiangjun Fan",
        "Lizhu Zhang",
        "Eric Schulz",
        "Hsiao-Ping Tseng"
      ],
      "abstract": "Should LLM reasoning live in a separate module, or within a single model's forward pass and representational space? We study dual-architecture latent reasoning, where a fluent Base exchanges latent messages with a Coprocessor, and test two hypotheses aimed at improving latent communication over Liu et al. (2024): (H1) increase channel capacity; (H2) learn communication via joint finetuning. Under matched latent-token budgets on GPT-2 and Qwen-3, H2 is consistently strongest while H1 yields modest gains. A unified soft-embedding baseline, a single model with the same forward pass and shared representations, using the same latent-token budget, nearly matches H2 and surpasses H1, suggesting current dual designs mostly add compute rather than qualitatively improving reasoning. Across GSM8K, ProsQA, and a Countdown stress test with increasing branching factor, scaling the latent-token budget beyond small values fails to improve robustness. Latent analyses show overlapping subspaces with limited specialization, consistent with weak reasoning gains. We conclude dual-model latent reasoning remains promising in principle, but likely requires objectives and training schedules that explicitly shape latent spaces for algorithmic planning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­å®ç°éšå¼æ¨ç†(latent reasoning)çš„è·¯å¾„ï¼Œå¯¹æ¯”äº†Baseæ¨¡å‹ä¸Coprocessoräº¤æ¢ä¿¡æ¯çš„åŒæ¶æ„ç³»ç»Ÿä¸ç»Ÿä¸€è¡¨å¾ç©ºé—´çš„å•æ¨¡å‹æ¶æ„ã€‚é€šè¿‡åœ¨GPT-2å’ŒQwen-3ä¸Šæµ‹è¯•å¢åŠ é€šé“å®¹é‡(H1)ä¸è”åˆå¾®è°ƒ(joint finetuning, H2)ä¸¤ç§å‡è®¾ï¼Œå®éªŒå‘ç°H2è¡¨ç°æœ€å¼ºï¼Œä½†ç»Ÿä¸€è½¯åµŒå…¥(unified soft-embedding)åŸºå‡†åœ¨åŒç­‰é¢„ç®—ä¸‹å‡ ä¹èƒ½è¾¾åˆ°åŒç­‰æ°´å¹³ã€‚è¿™è¡¨æ˜ç›®å‰çš„åŒæ¶æ„è®¾è®¡æ›´å¤šæ˜¯å¢åŠ äº†è®¡ç®—å¼€é”€ï¼Œè€Œéåœ¨æ¨ç†è´¨æ„Ÿä¸Šå–å¾—å®è´¨æ€§çªç ´ã€‚åœ¨GSM8Kã€ProsQAåŠCountdownå‹åŠ›æµ‹è¯•ä¸­ï¼Œå•çº¯å¢åŠ éšå¼Token(latent-token)é¢„ç®—æœªèƒ½æ˜¾è‘—æå‡æ¨¡å‹é²æ£’æ€§ã€‚éšå¼åˆ†æè¿›ä¸€æ­¥æ­ç¤ºäº†æ¨¡å—é—´å­ç©ºé—´é‡å ä¸”ä¸“ä¸šåŒ–ç¨‹åº¦ä¸è¶³çš„é—®é¢˜ï¼Œå¯¼è‡´æ¨ç†å¢ç›Šå—é™ã€‚ç ”ç©¶ç»“è®ºæŒ‡å‡ºï¼ŒåŒæ¨¡å‹éšå¼æ¨ç†çš„æœªæ¥å‘å±•éœ€è¦æ›´æ˜ç¡®çš„è®­ç»ƒç›®æ ‡å’Œæ–¹æ¡ˆï¼Œä»¥å¡‘é€ èƒ½å¤Ÿæ”¯æŒç®—æ³•è§„åˆ’çš„éšå¼ç©ºé—´ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00494v2",
      "published_date": "2025-10-01 04:26:09 UTC",
      "updated_date": "2025-12-01 03:25:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:05:58.287785+00:00"
    },
    {
      "arxiv_id": "2510.00492v2",
      "title": "Rethinking Reward Models for Multi-Domain Test-Time Scaling",
      "title_zh": "é‡æ–°å®¡è§†å¤šé¢†åŸŸæµ‹è¯•æ—¶ç¼©æ”¾çš„å¥–åŠ±æ¨¡å‹",
      "authors": [
        "Dong Bok Lee",
        "Seanie Lee",
        "Sangwoo Park",
        "Minki Kang",
        "Jinheon Baek",
        "Dongki Kim",
        "Dominik Wagner",
        "Jiongdao Jin",
        "Heejun Lee",
        "Tobias Bocklet",
        "Jinyu Wang",
        "Jingjing Fu",
        "Sung Ju Hwang",
        "Jiang Bian",
        "Lei Song"
      ],
      "abstract": "The reliability of large language models (LLMs) during test-time scaling is often assessed with \\emph{external verifiers} or \\emph{reward models} that distinguish correct reasoning from flawed logic. Prior work generally assumes that process reward models (PRMs), which score every intermediate reasoning step, outperform outcome reward models (ORMs) that assess only the final answer. This view is based mainly on evidence from narrow, math-adjacent domains. We present the first unified evaluation of four reward model variants, discriminative ORM and PRM (\\DisORM, \\DisPRM) and generative ORM and PRM (\\GenORM, \\GenPRM), across 14 diverse domains. Contrary to conventional wisdom, we find that (i) \\DisORM performs on par with \\DisPRM, (ii) \\GenPRM is not competitive, and (iii) overall, \\GenORM is the most robust, yielding significant and consistent gains across every tested domain. We attribute this to PRM-style stepwise scoring, which inherits label noise from LLM auto-labeling and has difficulty evaluating long reasoning trajectories, including those involving self-correcting reasoning. Our theoretical analysis shows that step-wise aggregation compounds errors as reasoning length grows, and our empirical observations confirm this effect. These findings challenge the prevailing assumption that fine-grained supervision is always better and support generative outcome verification for multi-domain deployment. We publicly release our code, datasets, and checkpoints at \\href{https://github.com/db-Lee/Multi-RM}{\\underline{\\small\\texttt{https://github.com/db-Lee/Multi-RM}}} to facilitate future research in multi-domain settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æµ‹è¯•æ—¶æ‰©å±•ï¼ˆtest-time scalingï¼‰ä¸­çš„å¯é æ€§ï¼Œå¯¹14ä¸ªå¤šå…ƒé¢†åŸŸçš„å¥–åŠ±æ¨¡å‹ï¼ˆReward Modelsï¼‰è¿›è¡Œäº†é‡æ–°å®¡è§†ã€‚é€šè¿‡å¯¹åˆ¤åˆ«å¼ ORMã€åˆ¤åˆ«å¼ PRMã€ç”Ÿæˆå¼ ORM å’Œç”Ÿæˆå¼ PRM å››ç§å˜ä½“çš„ç»Ÿä¸€è¯„ä¼°ï¼Œç ”ç©¶å‘ç°ä¼ ç»Ÿçš„â€œè¿‡ç¨‹å¥–åŠ±ä¼˜äºç»“æœå¥–åŠ±â€è§‚ç‚¹åœ¨å¤šé¢†åŸŸèƒŒæ™¯ä¸‹å¹¶ä¸æˆç«‹ã€‚å®éªŒè¡¨æ˜ï¼Œåˆ¤åˆ«å¼ç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆDisORMï¼‰ä¸åˆ¤åˆ«å¼è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆDisPRMï¼‰è¡¨ç°ç›¸å½“ï¼Œè€Œç”Ÿæˆå¼ç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆGenORMï¼‰åœ¨æ‰€æœ‰é¢†åŸŸä¸­å±•ç°å‡ºæœ€å¼ºçš„ç¨³å¥æ€§ä¸å¢ç›Šã€‚ç ”ç©¶è¿›ä¸€æ­¥æŒ‡å‡ºï¼Œè¿‡ç¨‹çº§è¯„åˆ†ï¼ˆPRM-styleï¼‰æ˜“å—è‡ªåŠ¨æ ‡æ³¨å™ªå£°å½±å“ï¼Œä¸”åœ¨å¤„ç†åŒ…å«è‡ªæˆ‘ä¿®æ­£çš„é•¿æ¨ç†è½¨è¿¹æ—¶ä¼šäº§ç”Ÿæ˜æ˜¾çš„è¯¯å·®å¤åˆæ•ˆåº”ã€‚è¯¥é¡¹å·¥ä½œæŒ‘æˆ˜äº†ç»†ç²’åº¦ç›‘ç£å§‹ç»ˆä¼˜è¶Šçš„å¸¸è§„å‡è®¾ï¼Œä¸ºå¤šé¢†åŸŸéƒ¨ç½²ä¸­çš„ç”Ÿæˆå¼ç»“æœéªŒè¯ï¼ˆgenerative outcome verificationï¼‰æä¾›äº†ç†è®ºæ”¯æŒä¸å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00492v2",
      "published_date": "2025-10-01 04:21:14 UTC",
      "updated_date": "2025-10-02 02:37:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:05:55.975530+00:00"
    },
    {
      "arxiv_id": "2510.00491v1",
      "title": "From Human Hands to Robot Arms: Manipulation Skills Transfer via Trajectory Alignment",
      "title_zh": "ä»äººæ‰‹åˆ°æœºæ¢°è‡‚ï¼šåŸºäºè½¨è¿¹å¯¹é½çš„æ“ä½œæŠ€èƒ½è¿ç§»",
      "authors": [
        "Han Zhou",
        "Jinjin Cao",
        "Liyuan Ma",
        "Xueji Fang",
        "Guo-jun Qi"
      ],
      "abstract": "Learning diverse manipulation skills for real-world robots is severely bottlenecked by the reliance on costly and hard-to-scale teleoperated demonstrations. While human videos offer a scalable alternative, effectively transferring manipulation knowledge is fundamentally hindered by the significant morphological gap between human and robotic embodiments. To address this challenge and facilitate skill transfer from human to robot, we introduce Traj2Action,a novel framework that bridges this embodiment gap by using the 3D trajectory of the operational endpoint as a unified intermediate representation, and then transfers the manipulation knowledge embedded in this trajectory to the robot's actions. Our policy first learns to generate a coarse trajectory, which forms an high-level motion plan by leveraging both human and robot data. This plan then conditions the synthesis of precise, robot-specific actions (e.g., orientation and gripper state) within a co-denoising framework. Extensive real-world experiments on a Franka robot demonstrate that Traj2Action boosts the performance by up to 27% and 22.25% over $Ï€_0$ baseline on short- and long-horizon real-world tasks, and achieves significant gains as human data scales in robot policy learning. Our project website, featuring code and video demonstrations, is available at https://anonymous.4open.science/w/Traj2Action-4A45/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Traj2Actionæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç”±äºäººç±»ä¸æœºå™¨äººå½¢æ€å·®å¼‚ï¼ˆmorphological gapï¼‰å¯¼è‡´çš„æ“çºµæŠ€èƒ½è½¬ç§»éš¾é¢˜ï¼Œå…‹æœäº†ä¼ ç»Ÿè¿œç¨‹æ“ä½œæ¼”ç¤ºï¼ˆteleoperated demonstrationsï¼‰æˆæœ¬é«˜ä¸”éš¾ä»¥è§„æ¨¡åŒ–çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥æ“ä½œç«¯ç‚¹çš„3Dè½¨è¿¹ä½œä¸ºç»Ÿä¸€çš„ä¸­é—´è¡¨ç¤ºï¼ˆunified intermediate representationï¼‰ï¼Œæœ‰æ•ˆåœ°å°†äººç±»è§†é¢‘ä¸­è•´å«çš„æ“çºµçŸ¥è¯†æ¡¥æ¥å¹¶è½¬åŒ–ä¸ºæœºå™¨äººçš„åŠ¨ä½œã€‚å…·ä½“è€Œè¨€ï¼Œç­–ç•¥é¦–å…ˆç»“åˆäººç±»å’Œæœºå™¨äººæ•°æ®å­¦ä¹ ç”Ÿæˆç²—ç•¥è½¨è¿¹ä»¥æ„å»ºé«˜çº§è¿åŠ¨è§„åˆ’ï¼Œéšååœ¨ååŒå»å™ªæ¡†æ¶ï¼ˆco-denoising frameworkï¼‰ä¸­ï¼Œä»¥æ­¤è§„åˆ’ä¸ºæŒ‡å¯¼åˆæˆç²¾ç¡®çš„æœºå™¨äººç‰¹å®šåŠ¨ä½œï¼Œå¦‚å§¿æ€ï¼ˆorientationï¼‰å’Œå¤¹æŒå™¨çŠ¶æ€ï¼ˆgripper stateï¼‰ã€‚åœ¨Frankaæœºå™¨äººä¸Šçš„çœŸå®ä¸–ç•Œå®éªŒè¯æ˜ï¼ŒTraj2Actionåœ¨çŸ­å‘¨æœŸå’Œé•¿å‘¨æœŸä»»åŠ¡ä¸Šåˆ†åˆ«æ¯” $\\pi_0$ åŸºçº¿æ¨¡å‹æå‡äº†27%å’Œ22.25%çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶ç»“æœè¿›ä¸€æ­¥è¯å®ï¼Œéšç€äººç±»æ•°æ®è§„æ¨¡çš„å¢åŠ ï¼Œè¯¥æ¡†æ¶åœ¨æœºå™¨äººç­–ç•¥å­¦ä¹ ä¸­å±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½å¢ç›Šã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00491v1",
      "published_date": "2025-10-01 04:21:12 UTC",
      "updated_date": "2025-10-01 04:21:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:06:04.562800+00:00"
    },
    {
      "arxiv_id": "2510.05134v2",
      "title": "Structuring Reasoning for Complex Rules Beyond Flat Representations",
      "title_zh": "çªç ´æ‰å¹³åŒ–è¡¨ç¤ºï¼šæ„å»ºé¢å‘å¤æ‚è§„åˆ™çš„ç»“æ„åŒ–æ¨ç†",
      "authors": [
        "Zhihao Yang",
        "Ancheng Xu",
        "Jingpeng Li",
        "Liang Yan",
        "Jiehui Zhou",
        "Zhen Qin",
        "Hengyu Chang",
        "Yukun Chen",
        "Longze Chen",
        "Ahmadreza Argha",
        "Hamid Alinejad-Rokny",
        "Minghuan Tan",
        "Yujun Cai",
        "Min Yang"
      ],
      "abstract": "Large language models (LLMs) face significant challenges when processing complex rule systems, as they typically treat interdependent rules as unstructured textual data rather than as logically organized frameworks. This limitation results in reasoning divergence, where models often overlook critical rule dependencies essential for accurate interpretation. Although existing approaches such as Chain-of-Thought (CoT) reasoning have shown promise, they lack systematic methodologies for structured rule processing and are particularly susceptible to error propagation through sequential reasoning chains. To address these limitations, we propose the Dynamic Adjudication Template (DAT), a novel framework inspired by expert human reasoning processes. DAT structures the inference mechanism into three methodical stages: qualitative analysis, evidence gathering, and adjudication. During the qualitative analysis phase, the model comprehensively evaluates the contextual landscape. The subsequent evidence gathering phase involves the targeted extraction of pertinent information based on predefined template elements ([placeholder]), followed by systematic verification against applicable rules. Finally, in the adjudication phase, the model synthesizes these validated components to formulate a comprehensive judgment. Empirical results demonstrate that DAT consistently outperforms conventional CoT approaches in complex rule-based tasks. Notably, DAT enables smaller language models to match, and in some cases exceed, the performance of significantly larger LLMs, highlighting its efficiency and effectiveness in managing intricate rule systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†å¤æ‚è§„åˆ™ç³»ç»Ÿæ—¶å› å°†å…¶è§†ä¸ºéç»“æ„åŒ–æ–‡æœ¬è€Œå¯¼è‡´çš„æ¨ç†åˆ†æ­§å’Œé”™è¯¯ä¼ æ’­é—®é¢˜ï¼Œæå‡ºäº†Dynamic Adjudication Template (DAT) æ¡†æ¶ã€‚DAT å€Ÿé‰´äººç±»ä¸“å®¶æ¨ç†è¿‡ç¨‹ï¼Œå°†æ¨ç†æœºåˆ¶ç»“æ„åŒ–ä¸ºå®šæ€§åˆ†æã€è¯æ®æ”¶é›†å’Œè£å†³ä¸‰ä¸ªé˜¶æ®µã€‚åœ¨å®šæ€§åˆ†æé˜¶æ®µæ¨¡å‹å…¨é¢è¯„ä¼°ä¸Šä¸‹æ–‡èƒŒæ™¯ï¼Œéšååœ¨è¯æ®æ”¶é›†é˜¶æ®µåˆ©ç”¨é¢„å®šä¹‰çš„æ¨¡æ¿å…ƒç´ æå–å…³é”®ä¿¡æ¯å¹¶è¿›è¡Œè§„åˆ™éªŒè¯ï¼Œæœ€ååœ¨è£å†³é˜¶æ®µç»¼åˆè¿™äº›éªŒè¯ç»„ä»¶å½¢æˆæœ€ç»ˆåˆ¤æ–­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDAT åœ¨å¤„ç†å¤æ‚è§„åˆ™ä»»åŠ¡æ—¶è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„Chain-of-Thought (CoT) æ¨ç†ã€‚æ­¤å¤–ï¼ŒDAT è¿˜èƒ½ä½¿å°å‹è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šåœºæ™¯ä¸‹è¾¾åˆ°ç”šè‡³è¶…è¿‡å¤§å‹LLMsçš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨ç®¡ç†å¤æ‚è§„åˆ™ç³»ç»Ÿæ–¹é¢çš„é«˜æ•ˆæ€§ä¸æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.05134v2",
      "published_date": "2025-10-01 04:10:13 UTC",
      "updated_date": "2026-01-20 12:09:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:06:01.395125+00:00"
    },
    {
      "arxiv_id": "2510.00487v1",
      "title": "Black-Box Time-Series Domain Adaptation via Cross-Prompt Foundation Models",
      "title_zh": "åŸºäºè·¨æç¤ºåŸºç¡€æ¨¡å‹çš„é»‘ç›’æ—¶é—´åºåˆ—é¢†åŸŸè‡ªé€‚åº”",
      "authors": [
        "M. T. Furqon",
        "Mahardhika Pratama",
        "Igor Skrjanc",
        "Lin Liu",
        "Habibullah Habibullah",
        "Kutluyil Dogancay"
      ],
      "abstract": "The black-box domain adaptation (BBDA) topic is developed to address the privacy and security issues where only an application programming interface (API) of the source model is available for domain adaptations. Although the BBDA topic has attracted growing research attentions, existing works mostly target the vision applications and are not directly applicable to the time-series applications possessing unique spatio-temporal characteristics. In addition, none of existing approaches have explored the strength of foundation model for black box time-series domain adaptation (BBTSDA). This paper proposes a concept of Cross-Prompt Foundation Model (CPFM) for the BBTSDA problems. CPFM is constructed under a dual branch network structure where each branch is equipped with a unique prompt to capture different characteristics of data distributions. In the domain adaptation phase, the reconstruction learning phase in the prompt and input levels is developed. All of which are built upon a time-series foundation model to overcome the spatio-temporal dynamic. Our rigorous experiments substantiate the advantage of CPFM achieving improved results with noticeable margins from its competitors in three time-series datasets of different application domains.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é»‘ç›’é¢†åŸŸè‡ªé€‚åº” (Black-Box Domain Adaptation, BBDA) åœ¨å¤„ç†å…·æœ‰å¤æ‚æ—¶ç©ºç‰¹å¾çš„æ—¶é—´åºåˆ—æ•°æ®æ—¶å­˜åœ¨çš„å±€é™æ€§ï¼Œæå‡ºäº†åä¸º Cross-Prompt Foundation Model (CPFM) çš„åˆ›æ–°æ¡†æ¶ã€‚CPFM é¦–æ¬¡æ¢ç´¢äº†åŸºç¡€æ¨¡å‹ (foundation model) åœ¨é»‘ç›’æ—¶é—´åºåˆ—é¢†åŸŸè‡ªé€‚åº” (BBTSDA) ä¸­çš„æ½œåŠ›ï¼Œæ—¨åœ¨è§£å†³ä»…èƒ½é€šè¿‡ API è®¿é—®æºæ¨¡å‹æ—¶çš„éšç§ä¸å®‰å…¨é€‚é…é—®é¢˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŒåˆ†æ”¯ç½‘ç»œç»“æ„ï¼Œé€šè¿‡åœ¨æ¯ä¸ªåˆ†æ”¯é…å¤‡ç‹¬ç‰¹çš„ prompt æ¥ç²¾å‡†æ•æ‰æ•°æ®åˆ†å¸ƒçš„ä¸åŒç‰¹å¾ã€‚åœ¨é¢†åŸŸè‡ªé€‚åº”è¿‡ç¨‹ä¸­ï¼ŒCPFM ç»“åˆäº† prompt çº§åˆ«å’Œè¾“å…¥çº§åˆ«çš„é‡å»ºå­¦ä¹  (reconstruction learning) æœºåˆ¶ï¼Œå……åˆ†åˆ©ç”¨æ—¶é—´åºåˆ— foundation model çš„è¡¨å¾èƒ½åŠ›æ¥å…‹æœæ—¶ç©ºåŠ¨æ€æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªä¸åŒåº”ç”¨é¢†åŸŸçš„æ•°æ®é›†ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰ç«äº‰å¯¹æ‰‹ã€‚è¿™ä¸€è´¡çŒ®ä¸ºéšç§æ•æ„Ÿåœºæ™¯ä¸‹çš„æ—¶é—´åºåˆ—è·¨é¢†åŸŸå»ºæ¨¡æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00487v1",
      "published_date": "2025-10-01 04:09:01 UTC",
      "updated_date": "2025-10-01 04:09:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:06:06.085023+00:00"
    },
    {
      "arxiv_id": "2510.00485v1",
      "title": "PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation",
      "title_zh": "PodEvalï¼šé¢å‘æ’­å®¢éŸ³é¢‘ç”Ÿæˆçš„å¤šæ¨¡æ€è¯„ä¼°æ¡†æ¶",
      "authors": [
        "Yujia Xiao",
        "Liumeng Xue",
        "Lei He",
        "Xinyi Chen",
        "Aemon Yat Fei Chiu",
        "Wenjie Tian",
        "Shaofei Zhang",
        "Qiuqiang Kong",
        "Xinfa Zhu",
        "Wei Xue",
        "Tan Lee"
      ],
      "abstract": "Recently, an increasing number of multimodal (text and audio) benchmarks have emerged, primarily focusing on evaluating models' understanding capability. However, exploration into assessing generative capabilities remains limited, especially for open-ended long-form content generation. Significant challenges lie in no reference standard answer, no unified evaluation metrics and uncontrollable human judgments. In this work, we take podcast-like audio generation as a starting point and propose PodEval, a comprehensive and well-designed open-source evaluation framework. In this framework: 1) We construct a real-world podcast dataset spanning diverse topics, serving as a reference for human-level creative quality. 2) We introduce a multimodal evaluation strategy and decompose the complex task into three dimensions: text, speech and audio, with different evaluation emphasis on \"Content\" and \"Format\". 3) For each modality, we design corresponding evaluation methods, involving both objective metrics and subjective listening test. We leverage representative podcast generation systems (including open-source, close-source, and human-made) in our experiments. The results offer in-depth analysis and insights into podcast generation, demonstrating the effectiveness of PodEval in evaluating open-ended long-form audio. This project is open-source to facilitate public use: https://github.com/yujxx/PodEval.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç›®å‰å¤šæ¨¡æ€åŸºå‡†ä¸»è¦å…³æ³¨ç†è§£èƒ½åŠ›è€Œå¿½è§†é•¿æ ¼å¼å†…å®¹ç”Ÿæˆè¯„ä¼°çš„é—®é¢˜ï¼Œæå‡ºäº† PodEval æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºæ’­å®¢éŸ³é¢‘ç”Ÿæˆå»ºç«‹ç»Ÿä¸€çš„è¯„ä»·æ ‡å‡†ã€‚è¯¥æ¡†æ¶é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªè·¨è¶Šå¤šå…ƒä¸»é¢˜çš„çœŸå®ä¸–ç•Œæ’­å®¢æ•°æ®é›†ï¼Œä½œä¸ºè¡¡é‡äººç±»æ°´å¹³åˆ›ä½œè´¨é‡çš„å‚è€ƒã€‚PodEval é‡‡ç”¨å¤šæ¨¡æ€è¯„ä¼°ç­–ç•¥ï¼Œå°†å¤æ‚çš„ç”Ÿæˆä»»åŠ¡åˆ†è§£ä¸º Textã€Speech å’Œ Audio ä¸‰ä¸ªç»´åº¦ï¼Œå¹¶åˆ†åˆ«ä» Contentï¼ˆå†…å®¹ï¼‰ä¸ Formatï¼ˆæ ¼å¼ï¼‰ä¸¤ä¸ªä¾§é‡ç‚¹è¿›è¡Œè€ƒå¯Ÿã€‚åœ¨å…·ä½“æ‰§è¡Œä¸Šï¼Œæ¡†æ¶ç»“åˆäº†å®¢è§‚æŒ‡æ ‡(objective metrics)å’Œä¸»è§‚å¬æ„Ÿæµ‹è¯•(subjective listening test)ï¼Œå¯¹ä»£è¡¨æ€§çš„å¼€æºã€é—­æºæ¨¡å‹åŠäººç±»ä½œå“è¿›è¡Œäº†æ·±åº¦å¯¹æ¯”ã€‚å®éªŒç»“æœä¸ä»…æ­ç¤ºäº†æ’­å®¢ç”ŸæˆæŠ€æœ¯çš„ç°çŠ¶ä¸æŒ‘æˆ˜ï¼Œè¿˜å……åˆ†è¯æ˜äº† PodEval åœ¨è¯„ä¼°å¼€æ”¾å¼é•¿éŸ³é¢‘æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00485v1",
      "published_date": "2025-10-01 04:08:08 UTC",
      "updated_date": "2025-10-01 04:08:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:06:09.567804+00:00"
    },
    {
      "arxiv_id": "2510.00481v1",
      "title": "Make a Video Call with LLM: A Measurement Campaign over Five Mainstream Apps",
      "title_zh": "ä¸å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œè§†é¢‘é€šè¯ï¼šé’ˆå¯¹äº”æ¬¾ä¸»æµåº”ç”¨çš„å®æµ‹ç ”ç©¶",
      "authors": [
        "Jiayang Xu",
        "Xiangjie Huang",
        "Zijie Li",
        "Zili Meng"
      ],
      "abstract": "In 2025, Large Language Model (LLM) services have launched a new feature -- AI video chat -- allowing users to interact with AI agents via real-time video communication (RTC), just like chatting with real people. Despite its significance, no systematic study has characterized the performance of existing AI video chat systems. To address this gap, this paper proposes a comprehensive benchmark with carefully designed metrics across four dimensions: quality, latency, internal mechanisms, and system overhead. Using custom testbeds, we further evaluate five mainstream AI video chatbots with this benchmark. This work provides the research community a baseline of real-world performance and identifies unique system bottlenecks. In the meantime, our benchmarking results also open up several research questions for future optimizations of AI video chatbots.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹2025å¹´å…´èµ·çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœåŠ¡çš„AIè§†é¢‘é€šè¯ï¼ˆAI video chatï¼‰åŠŸèƒ½ï¼Œå¼€å±•äº†é¦–æ¬¡ç³»ç»Ÿæ€§çš„æ€§èƒ½è¯„ä¼°ç ”ç©¶ã€‚ä¸ºäº†å¡«è¡¥ç°æœ‰ç ”ç©¶çš„ç©ºç™½ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ä¸ªåŒ…å«è´¨é‡ï¼ˆqualityï¼‰ã€å»¶è¿Ÿï¼ˆlatencyï¼‰ã€å†…éƒ¨æœºåˆ¶ï¼ˆinternal mechanismsï¼‰å’Œç³»ç»Ÿå¼€é”€ï¼ˆsystem overheadï¼‰å››ä¸ªç»´åº¦çš„å…¨é¢åŸºå‡†æµ‹è¯•ï¼ˆbenchmarkï¼‰ã€‚é€šè¿‡æ„å»ºè‡ªå®šä¹‰æµ‹è¯•å¹³å°ï¼Œç ”ç©¶äººå‘˜å¯¹äº”ä¸ªä¸»æµAIè§†é¢‘èŠå¤©æœºå™¨äººè¿›è¡Œäº†æ·±å…¥çš„å®åœ°æµ‹è¯„ã€‚è¯¥å·¥ä½œä¸ºå­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œæä¾›äº†çœŸå®ä¸–ç•Œçš„æ€§èƒ½åŸºå‡†ï¼Œå¹¶æˆåŠŸè¯†åˆ«å‡ºäº†æ­¤ç±»ç³»ç»Ÿç‰¹æœ‰çš„æ€§èƒ½ç“¶é¢ˆã€‚åŒæ—¶ï¼Œæµ‹è¯„ç»“æœä¹Ÿä¸ºæœªæ¥ä¼˜åŒ–AIè§†é¢‘é€šè¯ç³»ç»Ÿçš„æ€§èƒ½æå‡ºäº†è‹¥å¹²å…·æœ‰å¯å‘æ€§çš„ç ”ç©¶è¯¾é¢˜ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.HC",
        "cs.MM",
        "cs.PF"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00481v1",
      "published_date": "2025-10-01 04:03:51 UTC",
      "updated_date": "2025-10-01 04:03:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:06:09.283258+00:00"
    },
    {
      "arxiv_id": "2510.00480v2",
      "title": "Expandable Decision-Making States for Multi-Agent Deep Reinforcement Learning in Soccer Tactical Analysis",
      "title_zh": "é¢å‘è¶³çƒæˆ˜æœ¯åˆ†æçš„å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ å¯æ‰©å±•å†³ç­–çŠ¶æ€",
      "authors": [
        "Kenjiro Ide",
        "Taiga Someya",
        "Kohei Kawaguchi",
        "Keisuke Fujii"
      ],
      "abstract": "Invasion team sports such as soccer produce a high-dimensional, strongly coupled state space as many players continuously interact on a shared field, challenging quantitative tactical analysis. Traditional rule-based analyses are intuitive, while modern predictive machine learning models often perform pattern-matching without explicit agent representations. The problem we address is how to build player-level agent models from data, whose learned values and policies are both tactically interpretable and robust across heterogeneous data sources. Here, we propose Expandable Decision-Making States (EDMS), a semantically enriched state representation that augments raw positions and velocities with relational variables (e.g., scoring of space, pass, and score), combined with an action-masking scheme that gives on-ball and off-ball agents distinct decision sets. Compared to prior work, EDMS maps learned value functions and action policies to human-interpretable tactical concepts (e.g., marking pressure, passing lanes, ball accessibility) instead of raw coordinate features, and aligns agent choices with the rules of play. In the experiments, EDMS with action masking consistently reduced both action-prediction loss and temporal-difference (TD) error compared to the baseline. Qualitative case studies and Q-value visualizations further indicate that EDMS highlights high-risk, high-reward tactical patterns (e.g., fast counterattacks and defensive breakthroughs). We also integrated our approach into an open-source library and demonstrated compatibility with multiple commercial and open datasets, enabling cross-provider evaluation and reproducible experiments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¶³çƒç­‰ä¾µå…¥æ€§å›¢é˜Ÿè¿åŠ¨ä¸­é«˜ç»´ã€å¼ºè€¦åˆçŠ¶æ€ç©ºé—´å¯¼è‡´çš„æˆ˜æœ¯åˆ†æéš¾é¢˜ï¼Œæå‡ºäº†Expandable Decision-Making States (EDMS) æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡è¯­ä¹‰ä¸°å¯Œçš„çŠ¶æ€è¡¨ç¤ºï¼Œå°†åŸå§‹çš„ä½ç½®å’Œé€Ÿåº¦æ•°æ®å¢å¼ºä¸ºåŒ…å«ç©ºé—´è¯„åˆ†ã€ä¼ çƒå’Œå¾—åˆ†ç­‰å…³ç³»å˜é‡ï¼Œå¹¶ç»“åˆAction-maskingæœºåˆ¶ä¸ºæŒçƒä¸æ— çƒæ™ºèƒ½ä½“åŒºåˆ†å†³ç­–é›†ã€‚ä¸ä»¥å¾€ç ”ç©¶ç›¸æ¯”ï¼ŒEDMSèƒ½å¤Ÿå°†å­¦ä¹ åˆ°çš„ä»·å€¼å‡½æ•°å’ŒåŠ¨ä½œç­–ç•¥æ˜ å°„ä¸ºé˜²å®ˆå‹åŠ›ã€ä¼ çƒçº¿è·¯åŠçƒçš„å¯è¾¾æ€§ç­‰äººç±»å¯ç†è§£çš„æˆ˜æœ¯æ¦‚å¿µï¼Œè€Œéå•çº¯çš„åŸå§‹åæ ‡ç‰¹å¾ã€‚å®éªŒè¡¨æ˜ï¼ŒEDMSåœ¨é™ä½åŠ¨ä½œé¢„æµ‹æŸå¤±å’ŒTemporal-Difference (TD) è¯¯å·®æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å¿«é€Ÿåå‡»å’Œé˜²å®ˆçªç ´ç­‰é«˜é£é™©é«˜å›æŠ¥çš„æˆ˜æœ¯æ¨¡å¼ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å·²é›†æˆè‡³å¼€æºåº“ï¼Œå¹¶è¯æ˜äº†å…¶åœ¨å¤šä¸ªå•†ä¸šåŠå…¬å¼€æ•°æ®é›†ä¸Šçš„å…¼å®¹æ€§å’Œå¯é‡å¤æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "28 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.00480v2",
      "published_date": "2025-10-01 04:01:51 UTC",
      "updated_date": "2025-10-27 02:22:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:06:13.391229+00:00"
    },
    {
      "arxiv_id": "2510.00476v2",
      "title": "Analyzing Latent Concepts in Code Language Models",
      "title_zh": "åˆ†æä»£ç è¯­è¨€æ¨¡å‹ä¸­çš„æ½œåœ¨æ¦‚å¿µ",
      "authors": [
        "Arushi Sharma",
        "Vedant Pungliya",
        "Christopher J. Quinn",
        "Ali Jannesari"
      ],
      "abstract": "Interpreting the internal behavior of large language models trained on code remains a critical challenge, particularly for applications demanding trust, transparency, and semantic robustness. We propose Code Concept Analysis (CoCoA): a global post-hoc interpretability framework that uncovers emergent lexical, syntactic, and semantic structures in a code language model's representation space by clustering contextualized token embeddings into human-interpretable concept groups. We propose a hybrid annotation pipeline that combines static analysis tool-based syntactic alignment with prompt-engineered large language models (LLMs), enabling scalable labeling of latent concepts across abstraction levels. We analyse the distribution of concepts across layers and across three finetuning tasks. Emergent concept clusters can help identify unexpected latent interactions and be used to identify trends and biases within the model's learned representations. We further integrate LCA with local attribution methods to produce concept-grounded explanations, improving the coherence and interpretability of token-level saliency. Empirical evaluations across multiple models and tasks show that LCA discovers concepts that remain stable under semantic-preserving perturbations (average Cluster Sensitivity Index, CSI = 0.288) and evolve predictably with fine-tuning. In a user study on the programming-language classification task, concept-augmented explanations disambiguated token roles and improved human-centric explainability by 37 percentage points compared with token-level attributions using Integrated Gradients.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Code Concept Analysis (CoCoA)ï¼Œè¿™æ˜¯ä¸€ç§å…¨å±€äº‹åå¯è§£é‡Šæ€§æ¡†æ¶ï¼Œæ—¨åœ¨æ­ç¤ºä»£ç è¯­è¨€æ¨¡å‹(Code Language Models)è¡¨ç¤ºç©ºé—´ä¸­æ¶Œç°çš„è¯æ±‡ã€è¯­æ³•å’Œè¯­ä¹‰ç»“æ„ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†ä¸Šä¸‹æ–‡ç›¸å…³çš„Token Embeddingsèšç±»ä¸ºäººç±»å¯ç†è§£çš„æ¦‚å¿µç»„ï¼Œå¹¶ç»“åˆåŸºäºé™æ€åˆ†æå·¥å…·çš„è¯­æ³•å¯¹é½ä¸å¤§è¯­è¨€æ¨¡å‹(LLMs)é©±åŠ¨çš„æ··åˆæ ‡æ³¨æµæ°´çº¿ï¼Œå®ç°äº†è·¨æŠ½è±¡å±‚çº§çš„æ½œåœ¨æ¦‚å¿µè‡ªåŠ¨åŒ–æ ‡æ³¨ã€‚ç ”ç©¶æ·±å…¥åˆ†æäº†æ¦‚å¿µåœ¨æ¨¡å‹å„å±‚çº§åŠä¸åŒå¾®è°ƒä»»åŠ¡ä¸­çš„åˆ†å¸ƒï¼Œå‘ç°æ¶Œç°çš„æ¦‚å¿µç°‡æœ‰åŠ©äºè¯†åˆ«æ¨¡å‹å†…éƒ¨æœªé¢„æœŸçš„æ½œåœ¨äº¤äº’ã€è¶‹åŠ¿å’Œåè§ã€‚æ­¤å¤–ï¼ŒCoCoAé€šè¿‡å°†Latent Concept Analysis (LCA)ä¸å±€éƒ¨å½’å› æ–¹æ³•ç»“åˆï¼Œç”Ÿæˆäº†åŸºäºæ¦‚å¿µçš„è§£é‡Šï¼Œæ˜¾è‘—æå‡äº†Tokençº§åˆ«æ˜¾è‘—æ€§åˆ†æçš„è¿è´¯æ€§ä¸å¯è§£é‡Šæ€§ã€‚å®éªŒè¯æ˜ï¼ŒCoCoAå‘ç°çš„æ¦‚å¿µåœ¨è¯­ä¹‰ä¿æŒæ‰°åŠ¨ä¸‹ä¿æŒç¨³å®šï¼Œå¹¶éšå¾®è°ƒè¿‡ç¨‹å‘ˆç°å‡ºå¯é¢„æµ‹çš„æ¼”åŒ–ã€‚åœ¨ç¼–ç¨‹è¯­è¨€åˆ†ç±»ä»»åŠ¡çš„ç”¨æˆ·ç ”ç©¶ä¸­ï¼Œä¸Integrated Gradientsç­‰ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ¦‚å¿µå¢å¼ºçš„è§£é‡Šä½¿ä»¥äººä¸ºä¸­å¿ƒçš„å¯è§£é‡Šæ€§æé«˜äº†37ä¸ªç™¾åˆ†ç‚¹ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00476v2",
      "published_date": "2025-10-01 03:53:21 UTC",
      "updated_date": "2025-10-02 23:22:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:06:29.872138+00:00"
    },
    {
      "arxiv_id": "2510.00468v2",
      "title": "Feature Identification via the Empirical NTK",
      "title_zh": "åŸºäºç»éªŒç¥ç»åˆ‡çº¿æ ¸çš„ç‰¹å¾è¯†åˆ«",
      "authors": [
        "Jennifer Lin"
      ],
      "abstract": "We provide evidence that eigenanalysis of the empirical neural tangent kernel (eNTK) can surface the features used by trained neural networks. Across two standard toy models for mechanistic interpretability, Toy Models of Superposition (TMS) and a 1-layer MLP trained on modular addition, we find that the eNTK exhibits sharp spectral cliffs whose top eigenspaces align with ground-truth features. In TMS, the eNTK recovers the ground-truth features in both the sparse (high superposition) and dense regimes. In modular arithmetic, the eNTK can be used to recover Fourier feature families. Moreover, we provide evidence that a layerwise eNTK localizes features to specific layers and that the evolution of the eNTK spectrum can be used to diagnose the grokking phase transition. These results suggest that eNTK analysis may provide a practical handle for feature discovery and for detecting phase changes in small models.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯æ˜äº†ç»éªŒç¥ç»åˆ‡çº¿æ ¸ (empirical neural tangent kernel, eNTK) çš„ç‰¹å¾å€¼åˆ†æèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å—è®­ç¥ç»ç½‘ç»œæ‰€ä½¿ç”¨çš„ç‰¹å¾ã€‚é€šè¿‡åœ¨å åŠ ç©å…·æ¨¡å‹ (Toy Models of Superposition, TMS) å’Œæ‰§è¡Œæ¨¡åŠ æ³•ä»»åŠ¡çš„å•å±‚ MLP ä¸Šè¿›è¡Œå®éªŒï¼Œç ”ç©¶å‘ç° eNTK å…·æœ‰æ˜æ˜¾çš„é¢‘è°±å³­å£ (spectral cliffs)ï¼Œå…¶é¡¶å±‚ç‰¹å¾ç©ºé—´ä¸çœŸå®ç‰¹å¾ (ground-truth features) é«˜åº¦å¯¹é½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒeNTK åœ¨ TMS çš„ç¨€ç–ä¸å¯†é›†æœºåˆ¶ä¸‹å‡èƒ½æ¢å¤ç‰¹å¾ï¼Œå¹¶åœ¨æ¨¡è¿ç®—ä¸­æˆåŠŸæå–å‚…é‡Œå¶ç‰¹å¾æ— (Fourier feature families)ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¡¨æ˜å±‚çº§ eNTK å¯ä»¥å®šä½ç‰¹å®šå±‚çš„åŠŸèƒ½ç‰¹å¾ï¼Œå¹¶èƒ½é€šè¿‡é¢‘è°±æ¼”å˜è¯Šæ–­å¤§å½»å¤§æ‚Ÿ (grokking) ç°è±¡çš„ç›¸å˜è¿‡ç¨‹ã€‚è¿™äº›æˆæœè¡¨æ˜ eNTK åˆ†æå¯ä½œä¸ºç‰¹å¾å‘ç°åŠæ£€æµ‹å°æ¨¡å‹ç›¸å˜çš„å®ç”¨å·¥å…·ï¼Œä¸ºæœºæ¢°å¯è§£é‡Šæ€§ç ”ç©¶æä¾›äº†æ–°çš„æŠ€æœ¯æ‰‹æ®µã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 5 figures. v2: references and expanded discussion in Appendix B added",
      "pdf_url": "https://arxiv.org/pdf/2510.00468v2",
      "published_date": "2025-10-01 03:39:48 UTC",
      "updated_date": "2025-10-09 17:53:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:06:33.079188+00:00"
    },
    {
      "arxiv_id": "2510.00466v1",
      "title": "Integrating Offline Pre-Training with Online Fine-Tuning: A Reinforcement Learning Approach for Robot Social Navigation",
      "title_zh": "èåˆç¦»çº¿é¢„è®­ç»ƒä¸åœ¨çº¿å¾®è°ƒï¼šä¸€ç§é¢å‘æœºå™¨äººç¤¾äº¤å¯¼èˆªçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•",
      "authors": [
        "Run Su",
        "Hao Fu",
        "Shuai Zhou",
        "Yingao Fu"
      ],
      "abstract": "Offline reinforcement learning (RL) has emerged as a promising framework for addressing robot social navigation challenges. However, inherent uncertainties in pedestrian behavior and limited environmental interaction during training often lead to suboptimal exploration and distributional shifts between offline training and online deployment. To overcome these limitations, this paper proposes a novel offline-to-online fine-tuning RL algorithm for robot social navigation by integrating Return-to-Go (RTG) prediction into a causal Transformer architecture. Our algorithm features a spatiotem-poral fusion model designed to precisely estimate RTG values in real-time by jointly encoding temporal pedestrian motion patterns and spatial crowd dynamics. This RTG prediction framework mitigates distribution shift by aligning offline policy training with online environmental interactions. Furthermore, a hybrid offline-online experience sampling mechanism is built to stabilize policy updates during fine-tuning, ensuring balanced integration of pre-trained knowledge and real-time adaptation. Extensive experiments in simulated social navigation environments demonstrate that our method achieves a higher success rate and lower collision rate compared to state-of-the-art baselines. These results underscore the efficacy of our algorithm in enhancing navigation policy robustness and adaptability. This work paves the way for more reliable and adaptive robotic navigation systems in real-world applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å°†ç¦»çº¿é¢„è®­ç»ƒä¸åœ¨çº¿å¾®è°ƒç›¸ç»“åˆçš„ Reinforcement Learning (RL) ç®—æ³•ï¼Œæ—¨åœ¨è§£å†³æœºå™¨äººç¤¾äº¤å¯¼èˆªä¸­ç”±äºè¡Œäººè¡Œä¸ºä¸ç¡®å®šæ€§å’Œç¯å¢ƒäº¤äº’å—é™å¯¼è‡´çš„ distribution shift é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡å°† Return-to-Go (RTG) é¢„æµ‹é›†æˆåˆ° causal Transformer æ¶æ„ä¸­ï¼Œå¹¶åˆ©ç”¨æ—¶ç©ºèåˆæ¨¡å‹è”åˆç¼–ç è¡Œäººçš„æ—¶é—´è¿åŠ¨æ¨¡å¼ä¸ç©ºé—´ crowd dynamicsï¼Œå®ç°äº†å¯¹ RTG å€¼çš„å®æ—¶ç²¾ç¡®ä¼°è®¡ã€‚ä¸ºäº†ç¡®ä¿å¾®è°ƒè¿‡ç¨‹ä¸­çš„ç­–ç•¥æ›´æ–°ç¨³å®šæ€§ï¼Œç ”ç©¶å¼•å…¥äº†æ··åˆç¦»çº¿-åœ¨çº¿ç»éªŒé‡‡æ ·æœºåˆ¶ï¼Œæœ‰æ•ˆå¹³è¡¡äº†é¢„è®­ç»ƒçŸ¥è¯†ä¸å®æ—¶è‡ªé€‚åº”çš„éœ€æ±‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç®—æ³•åœ¨æ¨¡æ‹Ÿç¤¾äº¤å¯¼èˆªç¯å¢ƒä¸­çš„æˆåŠŸç‡å’Œç¢°æ’ç‡æŒ‡æ ‡å‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ã€‚è¿™ä¸€ç ”ç©¶å¢å¼ºäº†å¯¼èˆªç­–ç•¥çš„é²æ£’æ€§ä¸é€‚åº”æ€§ï¼Œä¸ºæ„å»ºæ›´å¯é ã€æ›´å…·è‡ªé€‚åº”èƒ½åŠ›çš„ç°å®ä¸–ç•Œæœºå™¨äººå¯¼èˆªç³»ç»Ÿæä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00466v1",
      "published_date": "2025-10-01 03:37:02 UTC",
      "updated_date": "2025-10-01 03:37:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:06:32.888798+00:00"
    },
    {
      "arxiv_id": "2510.00461v2",
      "title": "TimeEmb: A Lightweight Static-Dynamic Disentanglement Framework for Time Series Forecasting",
      "title_zh": "TimeEmbï¼šé¢å‘æ—¶é—´åºåˆ—é¢„æµ‹çš„è½»é‡çº§åŠ¨é™æ€è§£è€¦æ¡†æ¶",
      "authors": [
        "Mingyuan Xia",
        "Chunxu Zhang",
        "Zijian Zhang",
        "Hao Miao",
        "Qidong Liu",
        "Yuanshao Zhu",
        "Bo Yang"
      ],
      "abstract": "Temporal non-stationarity, the phenomenon that time series distributions change over time, poses fundamental challenges to reliable time series forecasting. Intuitively, the complex time series can be decomposed into two factors, \\ie time-invariant and time-varying components, which indicate static and dynamic patterns, respectively. Nonetheless, existing methods often conflate the time-varying and time-invariant components, and jointly learn the combined long-term patterns and short-term fluctuations, leading to suboptimal performance facing distribution shifts. To address this issue, we initiatively propose a lightweight static-dynamic decomposition framework, TimeEmb, for time series forecasting. TimeEmb innovatively separates time series into two complementary components: (1) time-invariant component, captured by a novel global embedding module that learns persistent representations across time series, and (2) time-varying component, processed by an efficient frequency-domain filtering mechanism inspired by full-spectrum analysis in signal processing. Experiments on real-world datasets demonstrate that TimeEmb outperforms state-of-the-art baselines and requires fewer computational resources. We conduct comprehensive quantitative and qualitative analyses to verify the efficacy of static-dynamic disentanglement. This lightweight framework can also improve existing time-series forecasting methods with simple integration. To ease reproducibility, the code is available at https://github.com/showmeon/TimeEmb.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„æ—¶é—´éå¹³ç¨³æ€§(Temporal non-stationarity)æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º TimeEmb çš„è½»é‡çº§é™æ€-åŠ¨æ€è§£è€¦æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†æ—¶é—´åºåˆ—åˆ›æ–°æ€§åœ°åˆ†è§£ä¸ºæ—¶é—´ä¸å˜(time-invariant)å’Œæ—¶é—´å˜åŒ–(time-varying)ä¸¤ä¸ªäº’è¡¥éƒ¨åˆ†ï¼Œåˆ†åˆ«ç”¨äºæ•æ‰é™æ€æŒä¹…ç‰¹å¾å’ŒåŠ¨æ€çŸ­æ—¶æ³¢åŠ¨ã€‚TimeEmb åˆ©ç”¨å…¨å±€åµŒå…¥æ¨¡å—(global embedding module)å­¦ä¹ è·¨åºåˆ—çš„è¡¨ç¤ºï¼Œå¹¶ç»“åˆé¢‘åŸŸæ»¤æ³¢æœºåˆ¶(frequency-domain filtering mechanism)æ¥æå–åŠ¨æ€æˆåˆ†ï¼Œæœ‰æ•ˆè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨åˆ†å¸ƒåç§»ä¸‹çš„æ€§èƒ½ç“¶é¢ˆã€‚å®éªŒè¯æ˜ï¼ŒTimeEmb åœ¨å¤šä¸ªçœŸå®æ•°æ®é›†ä¸Šä¼˜äºæœ€å…ˆè¿›(SOTA)æ¨¡å‹ï¼ŒåŒæ—¶å¤§å¹…é™ä½äº†è®¡ç®—å¼€é”€ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å…·å¤‡å‡ºè‰²çš„é€šç”¨æ€§ï¼Œèƒ½å¤Ÿé€šè¿‡ç®€å•é›†æˆæå‡ç°æœ‰é¢„æµ‹æ–¹æ³•çš„è¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00461v2",
      "published_date": "2025-10-01 03:28:49 UTC",
      "updated_date": "2025-10-20 14:49:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:06:53.076126+00:00"
    },
    {
      "arxiv_id": "2510.00457v1",
      "title": "UrbanGraph: Physics-Informed Spatio-Temporal Dynamic Heterogeneous Graphs for Urban Microclimate Prediction",
      "title_zh": "UrbanGraphï¼šé¢å‘åŸå¸‚å¾®æ°”å€™é¢„æµ‹çš„ç‰©ç†ä¿¡æ¯å¢å¼ºæ—¶ç©ºåŠ¨æ€å¼‚æ„å›¾",
      "authors": [
        "Weilin Xin",
        "Chenyu Huang",
        "Peilin Li",
        "Jing Zhong",
        "Jiawei Yao"
      ],
      "abstract": "With rapid urbanization, predicting urban microclimates has become critical, as it affects building energy demand and public health risks. However, existing generative and homogeneous graph approaches fall short in capturing physical consistency, spatial dependencies, and temporal variability. To address this, we introduce UrbanGraph, a physics-informed framework integrating heterogeneous and dynamic spatio-temporal graphs. It encodes key physical processes -- vegetation evapotranspiration, shading, and convective diffusion -- while modeling complex spatial dependencies among diverse urban entities and their temporal evolution. We evaluate UrbanGraph on UMC4/12, a physics-based simulation dataset covering diverse urban configurations and climates. Results show that UrbanGraph improves $R^2$ by up to 10.8% and reduces FLOPs by 17.0% over all baselines, with heterogeneous and dynamic graphs contributing 3.5% and 7.1% gains. Our dataset provides the first high-resolution benchmark for spatio-temporal microclimate modeling, and our method extends to broader urban heterogeneous dynamic computing tasks.",
      "tldr_zh": "éšç€åŸå¸‚åŒ–è¿›ç¨‹åŠ å¿«ï¼Œå‡†ç¡®é¢„æµ‹åŸå¸‚å¾®æ°”å€™(Urban Microclimate)å¯¹äºå»ºç­‘èƒ½è€—ç®¡ç†å’Œå…¬å…±å¥åº·è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨ç‰©ç†ä¸€è‡´æ€§ã€ç©ºé—´ä¾èµ–æ€§å’Œæ—¶é—´å˜å¼‚æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚è¯¥ç ”ç©¶æå‡ºäº†UrbanGraphï¼Œè¿™æ˜¯ä¸€ä¸ªé›†æˆäº†å¼‚æ„åŠ¨æ€æ—¶ç©ºå›¾(Heterogeneous and Dynamic Spatio-Temporal Graphs)çš„ç‰©ç†å‘ŠçŸ¥(Physics-Informed)æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç¼–ç äº†æ¤è¢«è’¸è…¾(Vegetation Evapotranspiration)ã€é®è«(Shading)å’Œå¯¹æµæ‰©æ•£(Convective Diffusion)ç­‰å…³é”®ç‰©ç†è¿‡ç¨‹ï¼ŒåŒæ—¶å¯¹åŸå¸‚å®ä½“é—´å¤æ‚çš„ç©ºé—´ä¾èµ–åŠå…¶æ—¶é—´æ¼”å˜è¿›è¡Œå»ºæ¨¡ã€‚åœ¨ç‰©ç†æ¨¡æ‹Ÿæ•°æ®é›†UMC4/12ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒUrbanGraphç›¸æ¯”æ‰€æœ‰åŸºçº¿æ¨¡å‹å°†$R^2$æå‡äº†10.8%å¹¶å‡å°‘äº†17.0%çš„FLOPsï¼Œå…¶ä¸­å¼‚æ„å›¾å’ŒåŠ¨æ€å›¾åˆ†åˆ«è´¡çŒ®äº†3.5%å’Œ7.1%çš„æ€§èƒ½å¢ç›Šã€‚è¯¥ç ”ç©¶ä¸ä»…æä¾›äº†é¦–ä¸ªé«˜åˆ†è¾¨ç‡çš„æ—¶ç©ºå¾®æ°”å€™å»ºæ¨¡åŸºå‡†æ•°æ®é›†ï¼Œå…¶æå‡ºçš„æ–¹æ³•è¿˜å¯æ‰©å±•è‡³æ›´å¹¿æ³›çš„åŸå¸‚å¼‚æ„åŠ¨æ€è®¡ç®—ä»»åŠ¡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00457v1",
      "published_date": "2025-10-01 03:14:05 UTC",
      "updated_date": "2025-10-01 03:14:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:06:38.581774+00:00"
    },
    {
      "arxiv_id": "2510.00454v1",
      "title": "Measuring and Controlling the Spectral Bias for Self-Supervised Image Denoising",
      "title_zh": "è‡ªç›‘ç£å›¾åƒå»å™ªä¸­çš„é¢‘è°±åå·®æµ‹é‡ä¸æ§åˆ¶",
      "authors": [
        "Wang Zhang",
        "Huaqiu Li",
        "Xiaowan Hu",
        "Tao Jiang",
        "Zikang Chen",
        "Haoqian Wang"
      ],
      "abstract": "Current self-supervised denoising methods for paired noisy images typically involve mapping one noisy image through the network to the other noisy image. However, after measuring the spectral bias of such methods using our proposed Image Pair Frequency-Band Similarity, it suffers from two practical limitations. Firstly, the high-frequency structural details in images are not preserved well enough. Secondly, during the process of fitting high frequencies, the network learns high-frequency noise from the mapped noisy images. To address these challenges, we introduce a Spectral Controlling network (SCNet) to optimize self-supervised denoising of paired noisy images. First, we propose a selection strategy to choose frequency band components for noisy images, to accelerate the convergence speed of training. Next, we present a parameter optimization method that restricts the learning ability of convolutional kernels to high-frequency noise using the Lipschitz constant, without changing the network structure. Finally, we introduce the Spectral Separation and low-rank Reconstruction module (SSR module), which separates noise and high-frequency details through frequency domain separation and low-rank space reconstruction, to retain the high-frequency structural details of images. Experiments performed on synthetic and real-world datasets verify the effectiveness of SCNet.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹è‡ªç›‘ç£å›¾åƒå»å™ª(Self-supervised image denoising)ä¸­çš„å…‰è°±åå·®(Spectral bias)é—®é¢˜ï¼Œé€šè¿‡æå‡ºçš„å›¾åƒå¯¹é¢‘å¸¦ç›¸ä¼¼åº¦(Image Pair Frequency-Band Similarity)é‡åŒ–åˆ†æäº†å½“å‰æ–¹æ³•åœ¨ä¿ç•™é«˜é¢‘ç»†èŠ‚åŠæŠ‘åˆ¶é«˜é¢‘å™ªå£°æ–¹é¢çš„å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†å…‰è°±æ§åˆ¶ç½‘ç»œ(Spectral Controlling network, SCNet)æ¥ä¼˜åŒ–æˆå¯¹å™ªå£°å›¾åƒçš„è‡ªç›‘ç£å»å™ªè¡¨ç°ã€‚è¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡é¢‘å¸¦é€‰æ‹©ç­–ç•¥åŠ é€Ÿè®­ç»ƒæ”¶æ•›ï¼Œå¹¶åˆ©ç”¨åˆ©æ™®å¸ŒèŒ¨å¸¸æ•°(Lipschitz constant)é™åˆ¶å·ç§¯æ ¸å¯¹é«˜é¢‘å™ªå£°çš„å­¦ä¹ èƒ½åŠ›ï¼Œä»è€Œåœ¨ä¸æ”¹å˜ç½‘ç»œç»“æ„çš„æƒ…å†µä¸‹å®ç°å‚æ•°ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†å…‰è°±åˆ†ç¦»ä¸ä½ç§©é‡å»ºæ¨¡å—(SSR module)ï¼Œåˆ©ç”¨é¢‘åŸŸåˆ†ç¦»å’Œç©ºé—´é‡å»ºæŠ€æœ¯æœ‰æ•ˆä¿ç•™äº†å›¾åƒçš„é«˜é¢‘ç»“æ„ã€‚åœ¨åˆæˆä¸çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†SCNetçš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨æ§åˆ¶å…‰è°±åå·®å’Œæå‡å»å™ªè´¨é‡æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00454v1",
      "published_date": "2025-10-01 03:07:05 UTC",
      "updated_date": "2025-10-01 03:07:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:06:43.983334+00:00"
    },
    {
      "arxiv_id": "2510.00452v1",
      "title": "Cloud Investigation Automation Framework (CIAF): An AI-Driven Approach to Cloud Forensics",
      "title_zh": "äº‘è°ƒæŸ¥è‡ªåŠ¨åŒ–æ¡†æ¶ (CIAF)ï¼šä¸€ç§ AI é©±åŠ¨çš„äº‘å–è¯æ–¹æ³•",
      "authors": [
        "Dalal Alharthi",
        "Ivan Roberto Kawaminami Garcia"
      ],
      "abstract": "Large Language Models (LLMs) have gained prominence in domains including cloud security and forensics. Yet cloud forensic investigations still rely on manual analysis, making them time-consuming and error-prone. LLMs can mimic human reasoning, offering a pathway to automating cloud log analysis. To address this, we introduce the Cloud Investigation Automation Framework (CIAF), an ontology-driven framework that systematically investigates cloud forensic logs while improving efficiency and accuracy. CIAF standardizes user inputs through semantic validation, eliminating ambiguity and ensuring consistency in log interpretation. This not only enhances data quality but also provides investigators with reliable, standardized information for decision-making. To evaluate security and performance, we analyzed Microsoft Azure logs containing ransomware-related events. By simulating attacks and assessing CIAF's impact, results showed significant improvement in ransomware detection, achieving precision, recall, and F1 scores of 93 percent. CIAF's modular, adaptable design extends beyond ransomware, making it a robust solution for diverse cyberattacks. By laying the foundation for standardized forensic methodologies and informing future AI-driven automation, this work underscores the role of deterministic prompt engineering and ontology-based validation in enhancing cloud forensic investigations. These advancements improve cloud security while paving the way for efficient, automated forensic workflows.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†äº‘è°ƒæŸ¥è‡ªåŠ¨åŒ–æ¡†æ¶ Cloud Investigation Automation Framework (CIAF)ï¼Œè¿™æ˜¯ä¸€ç§æœ¬ä½“é©±åŠ¨ (ontology-driven) çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡äººå·¥æ™ºèƒ½é©±åŠ¨çš„æ–¹æ³•è‡ªåŠ¨åŒ–å¤„ç†äº‘å–è¯æ—¥å¿—åˆ†æã€‚é’ˆå¯¹å½“å‰äº‘å–è¯è°ƒæŸ¥è¿‡åº¦ä¾èµ–äººå·¥ã€è€—æ—¶ä¸”æ˜“å‡ºé”™çš„é—®é¢˜ï¼ŒCIAF é€šè¿‡è¯­ä¹‰éªŒè¯ (semantic validation) æ ‡å‡†åŒ–ç”¨æˆ·è¾“å…¥ï¼Œæ¶ˆé™¤äº†æ—¥å¿—è§£é‡Šä¸­çš„æ­§ä¹‰å¹¶ç¡®ä¿äº†æ•°æ®ä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) æ¨¡æ‹Ÿäººç±»æ¨ç†çš„èƒ½åŠ›ï¼Œç»“åˆç¡®å®šæ€§æç¤ºå·¥ç¨‹ (deterministic prompt engineering) æå‡äº†è°ƒæŸ¥çš„å¯é æ€§ã€‚åœ¨é’ˆå¯¹åŒ…å«å‹’ç´¢è½¯ä»¶ç›¸å…³äº‹ä»¶çš„ Microsoft Azure æ—¥å¿—è¿›è¡Œçš„è¯„ä¼°ä¸­ï¼ŒCIAF è¡¨ç°ä¼˜å¼‚ï¼Œå–å¾—äº† 93% çš„ç²¾ç¡®ç‡ã€å¬å›ç‡å’Œ F1 åˆ†æ•°ã€‚è¯¥ç ”ç©¶ä¸ä»…è¯æ˜äº† CIAF åœ¨åº”å¯¹å¤šæ ·åŒ–ç½‘ç»œæ”»å‡»æ—¶çš„é²æ£’æ€§ï¼Œä¹Ÿä¸ºå»ºç«‹æ ‡å‡†åŒ–çš„ AI é©±åŠ¨è‡ªåŠ¨åŒ–å–è¯æ–¹æ³•è®ºå¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00452v1",
      "published_date": "2025-10-01 03:05:47 UTC",
      "updated_date": "2025-10-01 03:05:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:06:47.287605+00:00"
    },
    {
      "arxiv_id": "2510.00451v1",
      "title": "A Call to Action for a Secure-by-Design Generative AI Paradigm",
      "title_zh": "å‘¼åæ„å»ºâ€œè®¾è®¡å®‰å…¨â€çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½èŒƒå¼",
      "authors": [
        "Dalal Alharthi",
        "Ivan Roberto Kawaminami Garcia"
      ],
      "abstract": "Large language models have gained widespread prominence, yet their vulnerability to prompt injection and other adversarial attacks remains a critical concern. This paper argues for a security-by-design AI paradigm that proactively mitigates LLM vulnerabilities while enhancing performance. To achieve this, we introduce PromptShield, an ontology-driven framework that ensures deterministic and secure prompt interactions. It standardizes user inputs through semantic validation, eliminating ambiguity and mitigating adversarial manipulation. To assess PromptShield's security and performance capabilities, we conducted an experiment on an agent-based system to analyze cloud logs within Amazon Web Services (AWS), containing 493 distinct events related to malicious activities and anomalies. By simulating prompt injection attacks and assessing the impact of deploying PromptShield, our results demonstrate a significant improvement in model security and performance, achieving precision, recall, and F1 scores of approximately 94%. Notably, the ontology-based framework not only mitigates adversarial threats but also enhances the overall performance and reliability of the system. Furthermore, PromptShield's modular and adaptable design ensures its applicability beyond cloud security, making it a robust solution for safeguarding generative AI applications across various domains. By laying the groundwork for AI safety standards and informing future policy development, this work stimulates a crucial dialogue on the pivotal role of deterministic prompt engineering and ontology-based validation in ensuring the safe and responsible deployment of LLMs in high-stakes environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)æ˜“å—æç¤ºè¯æ³¨å…¥(Prompt Injection)å’Œå¯¹æŠ—æ€§æ”»å‡»çš„é—®é¢˜ï¼Œä¸»å¼ å»ºç«‹ä¸€ç§â€œè®¾è®¡å³å®‰å…¨â€(Secure-by-Design)çš„äººå·¥æ™ºèƒ½èŒƒå¼ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†PromptShieldï¼Œè¿™æ˜¯ä¸€ä¸ªæœ¬ä½“é©±åŠ¨(Ontology-driven)çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è¯­ä¹‰éªŒè¯(Semantic Validation)å¯¹ç”¨æˆ·è¾“å…¥è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ï¼Œä»è€Œæ¶ˆé™¤æ­§ä¹‰å¹¶å‡è½»å¯¹æŠ—æ€§æ“çºµã€‚ç ”ç©¶äººå‘˜åœ¨åŸºäºæ™ºèƒ½ä½“(Agent-based)çš„AWSäº‘æ—¥å¿—åˆ†æç³»ç»Ÿä¸Šè¿›è¡Œäº†å®éªŒï¼Œé’ˆå¯¹åŒ…å«æ¶æ„æ´»åŠ¨å’Œå¼‚å¸¸çš„493ä¸ªç‹¬ç«‹äº‹ä»¶è¿›è¡Œäº†æ¨¡æ‹Ÿæ”»å‡»æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œéƒ¨ç½²è¯¥æ¡†æ¶åæ¨¡å‹åœ¨å®‰å…¨æ€§ä¸æ€§èƒ½ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œå…¶ç²¾ç¡®ç‡(Precision)ã€å¬å›ç‡(Recall)å’ŒF1åˆ†æ•°å‡è¾¾åˆ°äº†çº¦94%ã€‚PromptShieldçš„æ¨¡å—åŒ–å’Œå¯æ‰©å±•è®¾è®¡ä½¿å…¶èƒ½å¤Ÿè·¨é¢†åŸŸä¿æŠ¤ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åº”ç”¨ï¼Œä¸ä»…æœ‰æ•ˆæŠµå¾¡äº†å¯¹æŠ—æ€§å¨èƒï¼Œè¿˜å¢å¼ºäº†ç³»ç»Ÿçš„å¯é æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºåˆ¶å®šAIå®‰å…¨æ ‡å‡†ä»¥åŠåœ¨ä¸¥è‹›ä»»åŠ¡ç¯å¢ƒä¸­è´Ÿè´£ä»»åœ°éƒ¨ç½²LLMså¥ å®šäº†é‡è¦çš„ç†è®ºå’Œå®è·µåŸºç¡€ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00451v1",
      "published_date": "2025-10-01 03:05:07 UTC",
      "updated_date": "2025-10-01 03:05:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:06:49.078983+00:00"
    },
    {
      "arxiv_id": "2510.05132v2",
      "title": "Training Large Language Models To Reason In Parallel With Global Forking Tokens",
      "title_zh": "è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ï¼šé€šè¿‡å…¨å±€åˆ†å‰æ ‡è®°å®ç°å¹¶è¡Œæ¨ç†",
      "authors": [
        "Sheng Jia",
        "Xiao Wang",
        "Shiva Prasad Kasiviswanathan"
      ],
      "abstract": "Although LLMs have demonstrated improved performance by scaling parallel test-time compute, doing so relies on generating reasoning paths that are both diverse and accurate. For challenging problems, the forking tokens that trigger diverse yet correct reasoning modes are typically deep in the sampling tree. Consequently, common strategies to encourage diversity, such as temperature scaling, encounter a worsened trade-off between diversity and accuracy. Motivated by this challenge, we treat parallel reasoning as a set-of-next-token-prediction problem, and incorporate a set-based global loss into Supervised Fine-Tuning (SFT) using self-supervised bipartite matching between our global forking tokens and unique reasoning traces. We observe that, while naive fine-tuning with multiple reasoning traces collapses these unique reasoning modes, our proposed method, Set Supervised Fine-Tuning (SSFT), preserves these modes and produces emergent global forking tokens. Experiments on multiple reasoning benchmarks show that our SSFT consistently outperforms SFT under both Pass@1 and Cons@k metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨é€šè¿‡å¹¶è¡Œæ¨ç†æå‡æ€§èƒ½æ—¶é¢ä¸´çš„å¤šæ ·æ€§ä¸å‡†ç¡®æ€§æƒè¡¡æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºé›†åˆç›‘ç£å¾®è°ƒ(Set Supervised Fine-Tuning, SSFT)çš„æ–°æ–¹æ³•ã€‚ä½œè€…å°†å¹¶è¡Œæ¨ç†å»ºæ¨¡ä¸ºé›†åˆå¼ä¸‹æ–‡é¢„æµ‹é—®é¢˜(set-of-next-token-prediction)ï¼Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å¼•å…¥äº†é›†åˆå…¨å±€æŸå¤±ï¼Œåˆ©ç”¨å…¨å±€åˆ†å‰æ ‡è®°(Global Forking Tokens)ä¸ç‹¬ç‰¹æ¨ç†è·¯å¾„é—´çš„è‡ªç›‘ç£äºŒåˆ†åŒ¹é…è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¼ ç»Ÿçš„å¾®è°ƒæ–¹å¼å¾€å¾€ä¼šå¯¼è‡´æ¨ç†æ¨¡å¼åç¼©ï¼Œè€ŒSSFTèƒ½å¤Ÿæœ‰æ•ˆä¿ç•™è¿™äº›æ¨¡å¼å¹¶ä¿ƒä½¿æ¨¡å‹äº§ç”Ÿæ¶Œç°çš„å…¨å±€åˆ†å‰æ ‡è®°ã€‚åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSSFTåœ¨Pass@1å’ŒCons@kæŒ‡æ ‡ä¸Šå‡æŒç»­ä¼˜äºä¼ ç»Ÿçš„SFTæ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ºæå‡LLMsåœ¨æµ‹è¯•æ—¶é€šè¿‡å¹¶è¡Œè®¡ç®—è·å–æ›´å¼ºæ¨ç†èƒ½åŠ›æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.05132v2",
      "published_date": "2025-10-01 02:48:39 UTC",
      "updated_date": "2025-11-06 07:00:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:06:52.716038+00:00"
    },
    {
      "arxiv_id": "2510.00436v1",
      "title": "Automated Evaluation can Distinguish the Good and Bad AI Responses to Patient Questions about Hospitalization",
      "title_zh": "è‡ªåŠ¨åŒ–è¯„ä¼°èƒ½å¤Ÿæœ‰æ•ˆé‰´åˆ« AI é’ˆå¯¹ä½é™¢ç›¸å…³æ‚£è€…æé—®å›ç­”çš„ä¼˜åŠ£",
      "authors": [
        "Sarvesh Soni",
        "Dina Demner-Fushman"
      ],
      "abstract": "Automated approaches to answer patient-posed health questions are rising, but selecting among systems requires reliable evaluation. The current gold standard for evaluating the free-text artificial intelligence (AI) responses--human expert review--is labor-intensive and slow, limiting scalability. Automated metrics are promising yet variably aligned with human judgments and often context-dependent. To address the feasibility of automating the evaluation of AI responses to hospitalization-related questions posed by patients, we conducted a large systematic study of evaluation approaches. Across 100 patient cases, we collected responses from 28 AI systems (2800 total) and assessed them along three dimensions: whether a system response (1) answers the question, (2) appropriately uses clinical note evidence, and (3) uses general medical knowledge. Using clinician-authored reference answers to anchor metrics, automated rankings closely matched expert ratings. Our findings suggest that carefully designed automated evaluation can scale comparative assessment of AI systems and support patient-clinician communication.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹åŒ»ç–—é¢†åŸŸAIè‡ªåŠ¨å›ç­”ç³»ç»Ÿè¯„ä¼°æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œç³»ç»Ÿæ€§åœ°æ¢è®¨äº†è‡ªåŠ¨è¯„ä¼°(automated evaluation)åœ¨åŒºåˆ†AIç”Ÿæˆå›ç­”è´¨é‡æ–¹é¢çš„å¯è¡Œæ€§ã€‚ç ”ç©¶åŸºäº100ä¸ªæ‚£è€…æ¡ˆä¾‹ï¼Œæ”¶é›†äº†æ¥è‡ª28ä¸ªAIç³»ç»Ÿçš„2800ä¸ªå›ç­”ï¼Œå¹¶ä»å›ç­”é’ˆå¯¹æ€§ã€ä¸´åºŠç—…ç¨‹è®°å½•(clinical note evidence)çš„æ°å½“ä½¿ç”¨ä»¥åŠé€šç”¨åŒ»å­¦çŸ¥è¯†(general medical knowledge)ä¸‰ä¸ªç»´åº¦è¿›è¡Œäº†å¤šç»´è¯„ä¼°ã€‚é€šè¿‡å¼•å…¥ä¸´åºŠåŒ»ç”Ÿç¼–å†™çš„å‚è€ƒç­”æ¡ˆä½œä¸ºå¯¹æ¯”åŸºå‡†ï¼Œç ”ç©¶å‘ç°è‡ªåŠ¨è¯„ä¼°å¾—å‡ºçš„ç³»ç»Ÿæ’åä¸ä¸“å®¶çš„äººå·¥è¯„åˆ†é«˜åº¦ä¸€è‡´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç²¾å¿ƒè®¾è®¡çš„è‡ªåŠ¨è¯„ä¼°æœºåˆ¶èƒ½å¤Ÿå®ç°å¯¹AIç³»ç»Ÿçš„è§„æ¨¡åŒ–æ¯”è¾ƒè¯„ä¼°ï¼Œå¹¶æœ‰æ•ˆæ”¯æŒæ‚£è€…ä¸åŒ»ç”Ÿä¹‹é—´çš„æ²Ÿé€šï¼Œè§£å†³äº†ä¼ ç»Ÿäººå·¥ä¸“å®¶è¯„å®¡éš¾ä»¥æ‰©å±•çš„å±€é™æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00436v1",
      "published_date": "2025-10-01 02:39:37 UTC",
      "updated_date": "2025-10-01 02:39:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:06:55.972517+00:00"
    },
    {
      "arxiv_id": "2510.00430v1",
      "title": "Plug-and-Play Prompt Refinement via Latent Feedback for Diffusion Model Alignment",
      "title_zh": "åŸºäºæ½œåé¦ˆçš„æ‰©æ•£æ¨¡å‹å¯¹é½å³æ’å³ç”¨æç¤ºè¯ä¼˜åŒ–",
      "authors": [
        "Suhyeon Lee",
        "Jong Chul Ye"
      ],
      "abstract": "Despite the recent progress, reinforcement learning (RL)-based fine-tuning of diffusion models often struggles with generalization, composability, and robustness against reward hacking. Recent studies have explored prompt refinement as a modular alternative, but most adopt a feed-forward approach that applies a single refined prompt throughout the entire sampling trajectory, thereby failing to fully leverage the sequential nature of reinforcement learning. To address this, here we introduce PromptLoop, a plug-and-play RL framework that incorporates latent feedback into step-wise prompt refinement. Rather than modifying diffusion model weights, a multimodal large language model (MLLM) is trained with RL to iteratively update prompts based on intermediate latent states of diffusion models. This design achieves a structural analogy to the Diffusion RL approach, while retaining the flexibility and generality of prompt-based alignment. Extensive experiments across diverse reward functions and diffusion backbones demonstrate that PromptLoop (i) achieves effective reward optimization, (ii) generalizes seamlessly to unseen models, (iii) composes orthogonally with existing alignment methods, and (iv) mitigates over-optimization and reward hacking.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PromptLoopï¼Œä¸€ç§ç”¨äºæ‰©æ•£æ¨¡å‹å¯¹é½(Diffusion Model Alignment)çš„å³æ’å³ç”¨å¼ºåŒ–å­¦ä¹ (RL)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¾®è°ƒæ–¹æ³•åœ¨æ³›åŒ–æ€§å’Œå¯¹æŠ—å¥–åŠ±ä½œå¼Š(reward hacking)æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¸ä¼ ç»Ÿçš„å•å‘æç¤ºè¯ä¼˜åŒ–ä¸åŒï¼ŒPromptLoopé€šè¿‡å¼•å…¥æ½œåœ¨åé¦ˆ(latent feedback)å®ç°äº†æ­¥è¿›å¼çš„æç¤ºè¯è°ƒæ•´ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)ï¼Œä½¿å…¶èƒ½å¤Ÿä¾æ®æ‰©æ•£æ¨¡å‹çš„ä¸­é—´æ½œåœ¨çŠ¶æ€(latent states)è¿­ä»£æ›´æ–°æç¤ºè¯ï¼Œè€Œæ— éœ€ä¿®æ”¹æ‰©æ•£æ¨¡å‹æœ¬èº«çš„æƒé‡ã€‚è¿™ç§è®¾è®¡åœ¨ä¿æŒæç¤ºè¯å¯¹é½çµæ´»æ€§çš„åŒæ—¶ï¼Œå®ç°äº†ä¸Diffusion RLæ–¹æ³•ç±»ä¼¼çš„ç»“æ„åŒ–ä¼˜åŠ¿ã€‚å®éªŒè¯æ˜ï¼ŒPromptLoopåœ¨ä¸åŒå¥–åŠ±å‡½æ•°å’Œæ‰©æ•£ä¸»å¹²ç½‘ç»œä¸‹å‡èƒ½å®ç°æœ‰æ•ˆçš„å¥–åŠ±ä¼˜åŒ–ï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„è·¨æ¨¡å‹æ³›åŒ–æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¯ä»¥ä¸ç°æœ‰çš„å¯¹é½æ‰‹æ®µæ­£äº¤ç»„åˆï¼Œå¹¶èƒ½æ˜¾è‘—ç¼“è§£è¿‡åº¦ä¼˜åŒ–é—®é¢˜ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "23 pages, 15 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.00430v1",
      "published_date": "2025-10-01 02:18:58 UTC",
      "updated_date": "2025-10-01 02:18:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:07:16.770193+00:00"
    },
    {
      "arxiv_id": "2510.00428v1",
      "title": "Automated Structured Radiology Report Generation with Rich Clinical Context",
      "title_zh": "èåˆä¸°å¯Œä¸´åºŠèƒŒæ™¯çš„è‡ªåŠ¨åŒ–ç»“æ„åŒ–æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆ",
      "authors": [
        "Seongjae Kang",
        "Dong Bok Lee",
        "Juho Jung",
        "Dongseop Kim",
        "Won Hwa Kim",
        "Sunghoon Joo"
      ],
      "abstract": "Automated structured radiology report generation (SRRG) from chest X-ray images offers significant potential to reduce workload of radiologists by generating reports in structured formats that ensure clarity, consistency, and adherence to clinical reporting standards. While radiologists effectively utilize available clinical contexts in their diagnostic reasoning, existing SRRG systems overlook these essential elements. This fundamental gap leads to critical problems including temporal hallucinations when referencing non-existent clinical contexts. To address these limitations, we propose contextualized SRRG (C-SRRG) that comprehensively incorporates rich clinical context for SRRG. We curate C-SRRG dataset by integrating comprehensive clinical context encompassing 1) multi-view X-ray images, 2) clinical indication, 3) imaging techniques, and 4) prior studies with corresponding comparisons based on patient histories. Through extensive benchmarking with state-of-the-art multimodal large language models, we demonstrate that incorporating clinical context with the proposed C-SRRG significantly improves report generation quality. We publicly release dataset, code, and checkpoints to facilitate future research for clinically-aligned automated RRG at https://github.com/vuno/contextualized-srrg.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Contextualized SRRG (C-SRRG)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è‡ªåŠ¨åŒ–ç»“æ„åŒ–æ”¾å°„æŠ¥å‘Šç”Ÿæˆ (SRRG) ç³»ç»Ÿå› å¿½è§†å…³é”®ä¸´åºŠèƒŒæ™¯è€Œå¯¼è‡´çš„ Temporal Hallucinations å’ŒæŠ¥å‘Šä¸ä¸€è‡´ç­‰æ ¸å¿ƒé—®é¢˜ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œä½œè€…æ„å»ºäº† C-SRRG æ•°æ®é›†ï¼Œå…¶ä¸­æ•´åˆäº†å¤šè§†è§’ X-ray å›¾åƒã€Clinical Indicationã€æˆåƒæŠ€æœ¯ä»¥åŠåŸºäºæ‚£è€…ç—…å²çš„è¿‡å¾€ç ”ç©¶å¯¹æ¯”ã€‚é€šè¿‡å¯¹æœ€å…ˆè¿›çš„ Multimodal Large Language Models è¿›è¡Œå¹¿æ³›çš„åŸºå‡†æµ‹è¯•ï¼Œç ”ç©¶è¯æ˜å¼•å…¥ä¸°å¯Œçš„ä¸´åºŠèƒŒæ™¯èƒ½æ˜¾è‘—æå‡æŠ¥å‘Šç”Ÿæˆçš„å‡†ç¡®æ€§å’Œä¸“ä¸šæ€§ã€‚è¯¥ç ”ç©¶ä¸ä»…ä¸ºä¸´åºŠå¯¹é½çš„è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆæä¾›äº†æ–°èŒƒå¼ï¼Œè¿˜å…¬å¼€å‘å¸ƒäº†æ•°æ®é›†ã€ä»£ç å’Œæ¨¡å‹æƒé‡ï¼Œä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "34 pages, 30 figures, preprint",
      "pdf_url": "https://arxiv.org/pdf/2510.00428v1",
      "published_date": "2025-10-01 02:14:23 UTC",
      "updated_date": "2025-10-01 02:14:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:07:16.584536+00:00"
    },
    {
      "arxiv_id": "2510.00416v1",
      "title": "Domain-Specialized Interactive Segmentation Framework for Meningioma Radiotherapy Planning",
      "title_zh": "é¢å‘è„‘è†œç˜¤æ”¾ç–—è§„åˆ’çš„é¢†åŸŸä¸“ç”¨äº¤äº’å¼åˆ†å‰²æ¡†æ¶",
      "authors": [
        "Junhyeok Lee",
        "Han Jang",
        "Kyu Sung Choi"
      ],
      "abstract": "Precise delineation of meningiomas is crucial for effective radiotherapy (RT) planning, directly influencing treatment efficacy and preservation of adjacent healthy tissues. While automated deep learning approaches have demonstrated considerable potential, achieving consistently accurate clinical segmentation remains challenging due to tumor heterogeneity. Interactive Medical Image Segmentation (IMIS) addresses this challenge by integrating advanced AI techniques with clinical input. However, generic segmentation tools, despite widespread applicability, often lack the specificity required for clinically critical and disease-specific tasks like meningioma RT planning. To overcome these limitations, we introduce Interactive-MEN-RT, a dedicated IMIS tool specifically developed for clinician-assisted 3D meningioma segmentation in RT workflows. The system incorporates multiple clinically relevant interaction methods, including point annotations, bounding boxes, lasso tools, and scribbles, enhancing usability and clinical precision. In our evaluation involving 500 contrast-enhanced T1-weighted MRI scans from the BraTS 2025 Meningioma RT Segmentation Challenge, Interactive-MEN-RT demonstrated substantial improvement compared to other segmentation methods, achieving Dice similarity coefficients of up to 77.6\\% and Intersection over Union scores of 64.8\\%. These results emphasize the need for clinically tailored segmentation solutions in critical applications such as meningioma RT planning. The code is publicly available at: https://github.com/snuh-rad-aicon/Interactive-MEN-RT",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Interactive-MEN-RTï¼Œä¸€ç§ä¸“ä¸ºè„‘è†œç˜¤æ”¾å°„æ²»ç–—(Radiotherapy, RT)è§„åˆ’è®¾è®¡çš„äº¤äº’å¼åŒ»ç–—å›¾åƒåˆ†å‰²(Interactive Medical Image Segmentation, IMIS)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é€šç”¨åˆ†å‰²å·¥å…·åœ¨å¤„ç†è‚¿ç˜¤å¼‚è´¨æ€§æ—¶ç¼ºä¹ä¸“ä¸šæ€§çš„é—®é¢˜ã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ•´åˆç‚¹æ ‡æ³¨(Point Annotations)ã€è¾¹ç•Œæ¡†(Bounding Boxes)ã€å¥—ç´¢å·¥å…·(Lasso Tools)å’Œæ¶‚é¸¦(Scribbles)ç­‰å¤šç§ä¸´åºŠäº¤äº’æ–¹å¼ï¼Œå®ç°äº†ä¸´åºŠåŒ»ç”Ÿè¾…åŠ©ä¸‹çš„ç²¾ç¡®ä¸‰ç»´åˆ†å‰²ã€‚åœ¨é’ˆå¯¹BraTS 2025è„‘è†œç˜¤æ”¾å°„æ²»ç–—åˆ†å‰²æŒ‘æˆ˜èµ›çš„500ä¾‹å¯¹æ¯”å¢å¼ºT1åŠ æƒMRIæ‰«ææ•°æ®çš„è¯„ä¼°ä¸­ï¼Œè¯¥æ¡†æ¶è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå…¶Diceç›¸ä¼¼ç³»æ•°(Dice similarity coefficients)è¾¾åˆ°77.6%ï¼Œäº¤å¹¶æ¯”(Intersection over Union, IoU)è¾¾åˆ°64.8%ï¼Œå‡ä¼˜äºå…¶ä»–ç°æœ‰åˆ†å‰²æ–¹æ³•ã€‚è¿™ä¸€æˆæœå¼ºè°ƒäº†åœ¨æ”¾å°„æ²»ç–—è§„åˆ’ç­‰å…³é”®ä¸´åºŠä»»åŠ¡ä¸­ï¼Œå¼€å‘å®šåˆ¶åŒ–äº¤äº’åˆ†å‰²è§£å†³æ–¹æ¡ˆå¯¹äºæå‡è¯Šç–—ç²¾ç¡®åº¦çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Clinical Image-Based Procedures (CLIP 2025), MICCAI 2025 Workshop",
      "pdf_url": "https://arxiv.org/pdf/2510.00416v1",
      "published_date": "2025-10-01 01:57:10 UTC",
      "updated_date": "2025-10-01 01:57:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:07:15.682246+00:00"
    },
    {
      "arxiv_id": "2510.00415v2",
      "title": "Towards Self-Evolving Benchmarks: Synthesizing Agent Trajectories via Test-Time Exploration under Validate-by-Reproduce Paradigm",
      "title_zh": "è¿ˆå‘è‡ªæ¼”è¿›åŸºå‡†ï¼šåœ¨å¤ç°éªŒè¯èŒƒå¼ä¸‹é€šè¿‡æµ‹è¯•æ—¶æ¢ç´¢åˆæˆæ™ºèƒ½ä½“è½¨è¿¹",
      "authors": [
        "Dadi Guo",
        "Tianyi Zhou",
        "Dongrui Liu",
        "Chen Qian",
        "Qihan Ren",
        "Shuai Shao",
        "Zhiyuan Fan",
        "Yi R. Fung",
        "Kun Wang",
        "Linfeng Zhang",
        "Jing Shao"
      ],
      "abstract": "Recent advances in large language models (LLMs) and agent system designs have empowered agents with unprecedented levels of capability. However, existing agent benchmarks are showing a trend of rapid ceiling-hitting by newly developed agents, making it difficult to meet the demands for evaluating agent abilities. To address this problem, we propose the Trajectory-based Validated-by-Reproducing Agent-benchmark Complexity Evolution (TRACE) framework. This framework takes an original task from an existing benchmark and encourages agents to freely explore and evolve it into a new task with higher difficulty while recording validatable agent trajectories. The framework proceeds in three stages: (1) evolutionary proposal mining, which provides task evolution proposals through preliminary exploration and divergent thinking; (2) problem formation and free exploration, where proposals are conceptualized into feasible problem candidates and the agents then explore them freely while recording their execution trajectories; and (3) multi-level validation, which ensures that the evolved tasks are accompanied by validatable and reproducible trajectories. Experiments on the GAIA benchmark demonstrate that the TRACE framework consistently enhances task complexity while improving the reliability of correctness through validatable execution trajectories. In addition, our framework can successfully adapt to and improve reasoning datasets represented by AIME-2024. This work marks a paradigm shift from static, manually curated benchmarks to dynamic, self-evolving evaluation systems, providing a sustainable and challenging runway for agent development",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Large Language Models (LLMs) æ™ºèƒ½ä½“åŸºå‡†æµ‹è¯•ï¼ˆbenchmarksï¼‰æ—¥ç›Šé¢ä¸´æ€§èƒ½ç“¶é¢ˆçš„é—®é¢˜ï¼Œæå‡ºäº† Trajectory-based Validated-by-Reproducing Agent-benchmark Complexity Evolution (TRACE) æ¡†æ¶ã€‚è¯¥æ¡†æ¶éµå¾ª Validate-by-Reproduce èŒƒå¼ï¼Œé€šè¿‡æ¼”åŒ–ææ¡ˆæŒ–æ˜ã€é—®é¢˜å½¢æˆä¸è‡ªç”±æ¢ç´¢ä»¥åŠå¤šå±‚çº§éªŒè¯ä¸‰ä¸ªé˜¶æ®µï¼Œå¼•å¯¼æ™ºèƒ½ä½“å°†ç°æœ‰ä»»åŠ¡æ¼”åŒ–ä¸ºæ›´é«˜éš¾åº¦çš„ä»»åŠ¡å¹¶è®°å½•å¯éªŒè¯çš„æ‰§è¡Œè½¨è¿¹ï¼ˆtrajectoriesï¼‰ã€‚è¿™ç§æ–¹æ³•å®ç°äº†ä»é™æ€äººå·¥ç­–åˆ’åŸºå‡†å‘åŠ¨æ€è‡ªæ¼”åŒ–è¯„ä¼°ç³»ç»Ÿçš„èŒƒå¼è½¬å˜ã€‚åœ¨ GAIA åŸºå‡†æµ‹è¯•å’Œ AIME-2024 æ¨ç†æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒTRACE æ¡†æ¶åœ¨æå‡ä»»åŠ¡å¤æ‚åº¦çš„åŒæ—¶ï¼Œæ˜¾è‘—å¢å¼ºäº†ç»“æœçš„å¯é æ€§ä¸å¯å¤ç°æ€§ã€‚è¯¥å·¥ä½œä¸ºæ™ºèƒ½ä½“èƒ½åŠ›çš„æŒç»­è¿›åŒ–æä¾›äº†æ›´å…·æŒ‘æˆ˜æ€§çš„è¯„ä¼°ç¯å¢ƒä¸å¯æŒç»­çš„ç ”ç©¶è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "This is a work in progress due to methodology refinement and further evaluation",
      "pdf_url": "https://arxiv.org/pdf/2510.00415v2",
      "published_date": "2025-10-01 01:52:52 UTC",
      "updated_date": "2025-10-23 19:10:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:07:21.563407+00:00"
    },
    {
      "arxiv_id": "2510.00411v3",
      "title": "Does Bigger Mean Better? Comparitive Analysis of CNNs and Biomedical Vision Language Modles in Medical Diagnosis",
      "title_zh": "è¶Šå¤§æ˜¯å¦æ„å‘³ç€è¶Šå¥½ï¼ŸåŒ»å­¦è¯Šæ–­ä¸­å·ç§¯ç¥ç»ç½‘ç»œä¸ç”Ÿç‰©åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯¹æ¯”åˆ†æ",
      "authors": [
        "Ran Tong",
        "Jiaqi Liu",
        "Tong Wang",
        "Xin Hu",
        "Su Liu",
        "Lanruo Wang",
        "Jiexi Xu"
      ],
      "abstract": "The accurate interpretation of chest radiographs using automated methods is a critical task in medical imaging. This paper presents a comparative analysis between a supervised lightweight Convolutional Neural Network (CNN) and a state-of-the-art, zero-shot medical Vision-Language Model (VLM), BiomedCLIP, across two distinct diagnostic tasks: pneumonia detection on the PneumoniaMNIST benchmark and tuberculosis detection on the Shenzhen TB dataset. Our experiments show that supervised CNNs serve as highly competitive baselines in both cases. While the default zero-shot performance of the VLM is lower, we demonstrate that its potential can be unlocked via a simple yet crucial remedy: decision threshold calibration. By optimizing the classification threshold on a validation set, the performance of BiomedCLIP is significantly boosted across both datasets. For pneumonia detection, calibration enables the zero-shot VLM to achieve a superior F1-score of 0.8841, surpassing the supervised CNN's 0.8803. For tuberculosis detection, calibration dramatically improves the F1-score from 0.4812 to 0.7684, bringing it close to the supervised baseline's 0.7834. This work highlights a key insight: proper calibration is essential for leveraging the full diagnostic power of zero-shot VLMs, enabling them to match or even outperform efficient, task-specific supervised models.",
      "tldr_zh": "æœ¬ç ”ç©¶å¯¹æœ‰ç›‘ç£çš„è½»é‡çº§å·ç§¯ç¥ç»ç½‘ç»œ(CNN)ä¸æœ€å…ˆè¿›çš„é›¶æ ·æœ¬(zero-shot)ç”Ÿç‰©åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹(VLM)â€”â€”BiomedCLIPåœ¨åŒ»å­¦å½±åƒè¯Šæ–­ä»»åŠ¡ä¸­è¿›è¡Œäº†æ¯”è¾ƒåˆ†æã€‚å®éªŒé’ˆå¯¹PneumoniaMNISTåŸºå‡†æµ‹è¯•ä¸­çš„è‚ºç‚æ£€æµ‹å’ŒShenzhen TBæ•°æ®é›†ä¸­çš„è‚ºç»“æ ¸æ£€æµ‹ä¸¤é¡¹å…³é”®ä»»åŠ¡å±•å¼€ã€‚ç»“æœè¡¨æ˜ï¼Œè™½ç„¶æœ‰ç›‘ç£çš„CNNåœ¨è¿™äº›ä»»åŠ¡ä¸­è¡¨ç°å‡ºæå¼ºçš„ç«äº‰åŠ›ï¼Œä½†BiomedCLIPåœ¨é»˜è®¤è®¾ç½®ä¸‹çš„åˆå§‹é›¶æ ·æœ¬æ€§èƒ½ç›¸å¯¹è¾ƒä½ã€‚ç ”ç©¶æå‡ºé€šè¿‡å†³ç­–é˜ˆå€¼æ ¡å‡†(decision threshold calibration)è¿™ä¸€ç®€å•è€Œå…³é”®çš„è¡¥æ•‘æªæ–½æ¥é‡Šæ”¾VLMçš„æ½œåŠ›ã€‚ç»è¿‡æ ¡å‡†ï¼ŒBiomedCLIPåœ¨è‚ºç‚æ£€æµ‹ä¸­çš„F1-scoreè¾¾åˆ°0.8841ï¼Œè¶…è¶Šäº†æœ‰ç›‘ç£CNNçš„0.8803ï¼›åœ¨è‚ºç»“æ ¸æ£€æµ‹ä¸­ï¼Œå…¶F1-scoreä¹Ÿä»0.4812æ˜¾è‘—æå‡è‡³0.7684ï¼Œæ¥è¿‘åŸºçº¿æ°´å¹³ã€‚è¯¥å·¥ä½œå¼ºè°ƒäº†é€‚å½“æ ¡å‡†å¯¹äºå‘æŒ¥é›¶æ ·æœ¬VLMè¯Šæ–­èƒ½åŠ›çš„é‡è¦æ€§ï¼Œè¯æ˜å…¶èƒ½å¤ŸåŒ¹é…ç”šè‡³ä¼˜äºé«˜æ•ˆçš„ä»»åŠ¡ç‰¹å®šç›‘ç£æ¨¡å‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6pages,3 figures.Uunder review of International Conference on Artificial Intelligence, Computer, Data Sciences and Applications",
      "pdf_url": "https://arxiv.org/pdf/2510.00411v3",
      "published_date": "2025-10-01 01:46:09 UTC",
      "updated_date": "2025-11-16 02:41:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:07:19.580195+00:00"
    },
    {
      "arxiv_id": "2510.00405v1",
      "title": "EgoTraj-Bench: Towards Robust Trajectory Prediction Under Ego-view Noisy Observations",
      "title_zh": "EgoTraj-Benchï¼šé¢å‘ç¬¬ä¸€è§†è§’å™ªå£°è§‚æµ‹çš„é²æ£’è½¨è¿¹é¢„æµ‹",
      "authors": [
        "Jiayi Liu",
        "Jiaming Zhou",
        "Ke Ye",
        "Kun-Yu Lin",
        "Allan Wang",
        "Junwei Liang"
      ],
      "abstract": "Reliable trajectory prediction from an ego-centric perspective is crucial for robotic navigation in human-centric environments. However, existing methods typically assume idealized observation histories, failing to account for the perceptual artifacts inherent in first-person vision, such as occlusions, ID switches, and tracking drift. This discrepancy between training assumptions and deployment reality severely limits model robustness. To bridge this gap, we introduce EgoTraj-Bench, the first real-world benchmark that grounds noisy, first-person visual histories in clean, bird's-eye-view future trajectories, enabling robust learning under realistic perceptual constraints. Building on this benchmark, we propose BiFlow, a dual-stream flow matching model that concurrently denoises historical observations and forecasts future motion by leveraging a shared latent representation. To better model agent intent, BiFlow incorporates our EgoAnchor mechanism, which conditions the prediction decoder on distilled historical features via feature modulation. Extensive experiments show that BiFlow achieves state-of-the-art performance, reducing minADE and minFDE by 10-15% on average and demonstrating superior robustness. We anticipate that our benchmark and model will provide a critical foundation for developing trajectory forecasting systems truly resilient to the challenges of real-world, ego-centric perception.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººå¯¼èˆªä¸­ç¬¬ä¸€äººç§°è§†è§’(ego-centric perspective)ä¸‹ç”±äºé®æŒ¡ã€IDåˆ‡æ¢å’Œè·Ÿè¸ªæ¼‚ç§»å¯¼è‡´çš„æ„ŸçŸ¥å™ªå£°é—®é¢˜ï¼ŒæŒ‡å‡ºå½“å‰è½¨è¿¹é¢„æµ‹æ–¹æ³•åœ¨ç†æƒ³åŒ–è§‚æµ‹ä¸ç°å®éƒ¨ç½²ä¹‹é—´çš„æ˜¾è‘—å·®è·ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†EgoTraj-Benchï¼Œè¿™æ˜¯é¦–ä¸ªå°†å¸¦æœ‰å™ªå£°çš„ç¬¬ä¸€äººç§°è§†è§‰å†å²è½¨è¿¹ä¸å¹²å‡€çš„é¸Ÿç°å›¾(bird's-eye-view)æœªæ¥è½¨è¿¹ç›¸ç»“åˆçš„çœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•é›†ã€‚åŸºäºè¯¥åŸºå‡†ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åŒæµæµåŒ¹é…æ¨¡å‹(dual-stream flow matching model) BiFlowï¼Œé€šè¿‡å…±äº«æ½œè¡¨å¾åŒæ­¥å®ç°å†å²è§‚æµ‹å»å™ªå’Œæœªæ¥è¿åŠ¨é¢„æµ‹ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†EgoAnchoræœºåˆ¶ï¼Œåˆ©ç”¨ç‰¹å¾è°ƒåˆ¶(feature modulation)å°†è’¸é¦åçš„å†å²ç‰¹å¾åº”ç”¨äºé¢„æµ‹è§£ç å™¨ï¼Œä»è€Œæ›´å‡†ç¡®åœ°å»ºæ¨¡æ™ºèƒ½ä½“æ„å›¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBiFlowè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œåœ¨minADEå’ŒminFDEæŒ‡æ ‡ä¸Šå¹³å‡é™ä½äº†10-15%ã€‚è¿™ä¸€æˆæœå±•ç°äº†å“è¶Šçš„é²æ£’æ€§ï¼Œä¸ºå¼€å‘èƒ½å¤Ÿåº”å¯¹çœŸå®ä¸–ç•Œæ„ŸçŸ¥æŒ‘æˆ˜çš„è½¨è¿¹é¢„æµ‹ç³»ç»Ÿæä¾›äº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00405v1",
      "published_date": "2025-10-01 01:30:13 UTC",
      "updated_date": "2025-10-01 01:30:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:07:30.380273+00:00"
    },
    {
      "arxiv_id": "2510.00404v2",
      "title": "AbsTopK: Rethinking Sparse Autoencoders For Bidirectional Features",
      "title_zh": "AbsTopKï¼šé¢å‘åŒå‘ç‰¹å¾çš„ç¨€ç–è‡ªç¼–ç å™¨é‡æ–°æ€è€ƒ",
      "authors": [
        "Xudong Zhu",
        "Mohammad Mahdi Khalili",
        "Zhihui Zhu"
      ],
      "abstract": "Sparse autoencoders (SAEs) have emerged as powerful techniques for interpretability of large language models (LLMs), aiming to decompose hidden states into meaningful semantic features. While several SAE variants have been proposed, there remains no principled framework to derive SAEs from the original dictionary learning formulation. In this work, we introduce such a framework by unrolling the proximal gradient method for sparse coding. We show that a single-step update naturally recovers common SAE variants, including ReLU, JumpReLU, and TopK. Through this lens, we reveal a fundamental limitation of existing SAEs: their sparsity-inducing regularizers enforce non-negativity, preventing a single feature from representing bidirectional concepts (e.g., male vs. female). This structural constraint fragments semantic axes into separate, redundant features, limiting representational completeness. To address this issue, we propose AbsTopK SAE, a new variant derived from the $\\ell_0$ sparsity constraint that applies hard thresholding over the largest-magnitude activations. By preserving both positive and negative activations, AbsTopK uncovers richer, bidirectional conceptual representations. Comprehensive experiments across four LLMs and seven probing and steering tasks show that AbsTopK improves reconstruction fidelity, enhances interpretability, and enables single features to encode contrasting concepts. Remarkably, AbsTopK matches or even surpasses the Difference-in-Mean method, a supervised approach that requires labeled data for each concept and has been shown in prior work to outperform SAEs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¨€ç–è‡ªç¼–ç å™¨ (Sparse Autoencoders, SAEs) åœ¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) å¯è§£é‡Šæ€§ä¸­çš„åº”ç”¨ï¼Œå¹¶æŒ‡å‡ºç›®å‰çš„ SAE å˜ä½“å¦‚ ReLUã€JumpReLU å’Œ TopK å­˜åœ¨ç”±äºå¼ºåˆ¶éè´Ÿæ€§è€Œæ— æ³•è¡¨ç¤ºåŒå‘æ¦‚å¿µçš„å±€é™æ€§ã€‚ä¸ºè§£å†³è¯­ä¹‰è½´è¢«åˆ†å‰²ä¸ºå†—ä½™ç‰¹å¾çš„é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº† AbsTopK SAEï¼Œè¿™æ˜¯ä¸€ç§åŸºäº $\\ell_0$ ç¨€ç–çº¦æŸå¹¶å¯¹æœ€å¤§å¹…åº¦æ¿€æ´»åº”ç”¨ç¡¬é˜ˆå€¼å¤„ç†çš„æ–°æ¡†æ¶ã€‚é€šè¿‡ä¿ç•™æ­£å‘å’Œè´Ÿå‘æ¿€æ´»ï¼ŒAbsTopK èƒ½å¤ŸæŒ–æ˜å‡ºæ›´ä¸°å¯Œçš„åŒå‘æ¦‚å¿µè¡¨ç¤ºï¼Œæ˜¾è‘—æå‡äº†ç‰¹å¾çš„è¡¨è¾¾å®Œæ•´æ€§ã€‚åœ¨å››ä¸ª LLMs å’Œä¸ƒé¡¹æ¢æµ‹ä¸è½¬å‘ä»»åŠ¡ä¸Šçš„å®éªŒè¯æ˜ï¼ŒAbsTopK ä¸ä»…æé«˜äº†é‡å»ºå¿ å®åº¦ï¼Œè¿˜å¢å¼ºäº†ç‰¹å¾çš„å¯è§£é‡Šæ€§ï¼Œä½¿å•ä¸ªç‰¹å¾èƒ½å¤Ÿç¼–ç å¯¹æ¯”æ¦‚å¿µã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒAbsTopK çš„æ€§èƒ½ç”šè‡³åŒ¹é…æˆ–è¶…è¶Šäº†éœ€è¦æ ‡æ³¨æ•°æ®çš„æœ‰ç›‘ç£ Difference-in-Mean æ–¹æ³•ï¼Œä¸ºå¤§æ¨¡å‹å†…éƒ¨è¡¨ç¤ºçš„æ·±åº¦è§£è¯»æä¾›äº†æœ‰åŠ›å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00404v2",
      "published_date": "2025-10-01 01:29:31 UTC",
      "updated_date": "2025-10-02 17:28:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:07:33.370007+00:00"
    },
    {
      "arxiv_id": "2510.05131v1",
      "title": "Rationale-Augmented Retrieval with Constrained LLM Re-Ranking for Task Discovery",
      "title_zh": "é¢å‘ä»»åŠ¡å‘ç°çš„æ¨ç†å¢å¼ºæ£€ç´¢ä¸å—é™å¤§è¯­è¨€æ¨¡å‹é‡æ’åº",
      "authors": [
        "Bowen Wei"
      ],
      "abstract": "Head Start programs utilizing GoEngage face significant challenges when new or rotating staff attempt to locate appropriate Tasks (modules) on the platform homepage. These difficulties arise from domain-specific jargon (e.g., IFPA, DRDP), system-specific nomenclature (e.g., Application Pool), and the inherent limitations of lexical search in handling typos and varied word ordering. We propose a pragmatic hybrid semantic search system that synergistically combines lightweight typo-tolerant lexical retrieval, embedding-based vector similarity, and constrained large language model (LLM) re-ranking. Our approach leverages the organization's existing Task Repository and Knowledge Base infrastructure while ensuring trustworthiness through low false-positive rates, evolvability to accommodate terminological changes, and economic efficiency via intelligent caching, shortlist generation, and graceful degradation mechanisms. We provide a comprehensive framework detailing required resources, a phased implementation strategy with concrete milestones, an offline evaluation protocol utilizing curated test cases (Hit@K, Precision@K, Recall@K, MRR), and an online measurement methodology incorporating query success metrics, zero-result rates, and dwell-time proxies.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Head Starté¡¹ç›®ä¸­GoEngageå¹³å°çš„ä»»åŠ¡å‘ç°(Task Discovery)éš¾é¢˜ï¼ŒæŒ‡å‡ºæ–°å‘˜å·¥å¸¸å› é¢†åŸŸä¸“ä¸šæœ¯è¯­ï¼ˆå¦‚IFPAã€DRDPï¼‰å’Œç³»ç»Ÿç‰¹å®šå‘½åè§„èŒƒï¼Œä»¥åŠä¼ ç»Ÿè¯æ±‡æœç´¢åœ¨å¤„ç†æ‹¼å†™é”™è¯¯å’Œè¯­åºå˜åŒ–æ—¶çš„å±€é™æ€§ï¼Œå¯¼è‡´éš¾ä»¥å‡†ç¡®å®šä½åŠŸèƒ½æ¨¡å—ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ç»“åˆç†æ€§å¢å¼ºæ£€ç´¢(Rationale-Augmented Retrieval)ä¸å—é™å¤§è¯­è¨€æ¨¡å‹é‡æ’åº(Constrained LLM Re-Ranking)çš„åŠ¡å®æ··åˆè¯­ä¹‰æœç´¢ç³»ç»Ÿã€‚è¯¥ç³»ç»ŸååŒé›†æˆäº†è½»é‡çº§å®¹é”™è¯æ±‡æ£€ç´¢ã€åŸºäºåµŒå…¥(Embedding)çš„å‘é‡ç›¸ä¼¼åº¦è®¡ç®—ä»¥åŠå¤§è¯­è¨€æ¨¡å‹é‡æ’åºæŠ€æœ¯ï¼Œåœ¨ç¡®ä¿ä½è¯¯æŠ¥ç‡(Low False-positive Rates)çš„åŒæ—¶ï¼Œé€šè¿‡æ™ºèƒ½ç¼“å­˜å’Œä¼˜é›…é™çº§(Graceful Degradation)æœºåˆ¶å®ç°äº†ç»æµé«˜æ•ˆçš„æ¶æ„ã€‚æ­¤å¤–ï¼Œç ”ç©¶æä¾›äº†ä¸€å¥—å®Œæ•´çš„åˆ†é˜¶æ®µå®æ–½ç­–ç•¥ä¸è¯„ä¼°æ¡†æ¶ï¼Œæ¶µç›–äº†Hit@Kã€Precision@Kã€MRRç­‰ç¦»çº¿æŒ‡æ ‡ä»¥åŠæŸ¥è¯¢æˆåŠŸç‡ã€åœç•™æ—¶é—´(Dwell-time)ç­‰åœ¨çº¿æµ‹é‡æ–¹æ³•ã€‚è¯¥æ–¹æ¡ˆä¸ä»…æå‡äº†å¤æ‚é¢†åŸŸç¯å¢ƒä¸‹çš„æ£€ç´¢ç²¾åº¦ï¼Œä¹Ÿä¸ºç³»ç»Ÿåœ¨æœ¯è¯­æ¼”å˜è¿‡ç¨‹ä¸­çš„æŒç»­è¿›åŒ–ä¸å¯ä¿¡åº¦æä¾›äº†ä¿éšœã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.05131v1",
      "published_date": "2025-10-01 01:28:59 UTC",
      "updated_date": "2025-10-01 01:28:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:07:34.673422+00:00"
    },
    {
      "arxiv_id": "2510.00401v1",
      "title": "Physics-Informed Neural Controlled Differential Equations for Scalable Long Horizon Multi-Agent Motion Forecasting",
      "title_zh": "ç‰©ç†ä¿¡æ¯å‘ŠçŸ¥ç¥ç»å—æ§å¾®åˆ†æ–¹ç¨‹ï¼šé¢å‘å¯æ‰©å±•é•¿æ—¶ç¨‹å¤šæ™ºèƒ½ä½“è¿åŠ¨é¢„æµ‹",
      "authors": [
        "Shounak Sural",
        "Charles Kekeh",
        "Wenliang Liu",
        "Federico Pecora",
        "Mouhacine Benosman"
      ],
      "abstract": "Long-horizon motion forecasting for multiple autonomous robots is challenging due to non-linear agent interactions, compounding prediction errors, and continuous-time evolution of dynamics. Learned dynamics of such a system can be useful in various applications such as travel time prediction, prediction-guided planning and generative simulation. In this work, we aim to develop an efficient trajectory forecasting model conditioned on multi-agent goals. Motivated by the recent success of physics-guided deep learning for partially known dynamical systems, we develop a model based on neural Controlled Differential Equations (CDEs) for long-horizon motion forecasting. Unlike discrete-time methods such as RNNs and transformers, neural CDEs operate in continuous time, allowing us to combine physics-informed constraints and biases to jointly model multi-robot dynamics. Our approach, named PINCoDE (Physics-Informed Neural Controlled Differential Equations), learns differential equation parameters that can be used to predict the trajectories of a multi-agent system starting from an initial condition. PINCoDE is conditioned on future goals and enforces physics constraints for robot motion over extended periods of time. We adopt a strategy that scales our model from 10 robots to 100 robots without the need for additional model parameters, while producing predictions with an average ADE below 0.5 m for a 1-minute horizon. Furthermore, progressive training with curriculum learning for our PINCoDE model results in a 2.7X reduction of forecasted pose error over 4 minute horizons compared to analytical models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PINCoDE (Physics-Informed Neural Controlled Differential Equations)ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç‰©ç†ä¿¡æ¯ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹çš„è½¨è¿¹é¢„æµ‹æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å¤šæœºå™¨äººé•¿æ—¶ç¨‹è¿åŠ¨é¢„æµ‹ä¸­å­˜åœ¨çš„éçº¿æ€§äº¤äº’å’Œè¯¯å·®ç´¯ç§¯ç­‰æŒ‘æˆ˜ã€‚ä¸RNNå’ŒTransformerç­‰ç¦»æ•£æ—¶é—´æ–¹æ³•ä¸åŒï¼ŒPINCoDEåœ¨è¿ç»­æ—¶é—´å°ºåº¦ä¸Šè¿è¡Œï¼Œèƒ½å¤Ÿç»“åˆç‰©ç†ä¿¡æ¯çº¦æŸå’Œåå·®æ¥å…±åŒå»ºæ¨¡å¤šæœºå™¨äººåŠ¨åŠ›å­¦ç³»ç»Ÿã€‚è¯¥æ¨¡å‹ä»¥å¤šæ™ºèƒ½ä½“ç›®æ ‡ä¸ºæ¡ä»¶ï¼Œé€šè¿‡å­¦ä¹ å¾®åˆ†æ–¹ç¨‹å‚æ•°ï¼Œä»åˆå§‹çŠ¶æ€å‡ºå‘é¢„æµ‹è½¨è¿¹ï¼Œå¹¶å¼ºåˆ¶æ‰§è¡Œç‰©ç†çº¦æŸä»¥ç¡®ä¿è¿åŠ¨çš„åˆç†æ€§ã€‚è¯¥æ–¹æ³•å±•ç¤ºäº†å“è¶Šçš„å¯æ‰©å±•æ€§ï¼Œæ— éœ€å¢åŠ é¢å¤–çš„æ¨¡å‹å‚æ•°å³å¯å°†é¢„æµ‹è§„æ¨¡ä»10ä¸ªæœºå™¨äººæ‰©å±•åˆ°100ä¸ªã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPINCoDEåœ¨1åˆ†é’Ÿé¢„æµ‹æ—¶ç¨‹å†…çš„å¹³å‡ä½ç§»è¯¯å·®(ADE)ä½äº0.5ç±³ã€‚æ­¤å¤–ï¼Œé€šè¿‡è¯¾ç¨‹å­¦ä¹ (curriculum learning)è¿›è¡Œæ¸è¿›å¼è®­ç»ƒï¼Œè¯¥æ¨¡å‹åœ¨4åˆ†é’Ÿé•¿æ—¶ç¨‹å†…çš„é¢„æµ‹å§¿æ€è¯¯å·®æ¯”ä¼ ç»Ÿè§£ææ¨¡å‹é™ä½äº†2.7å€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00401v1",
      "published_date": "2025-10-01 01:27:07 UTC",
      "updated_date": "2025-10-01 01:27:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:07:37.571511+00:00"
    },
    {
      "arxiv_id": "2510.01288v1",
      "title": "Microsaccade-Inspired Probing: Positional Encoding Perturbations Reveal LLM Misbehaviours",
      "title_zh": "å¾®çœ¼è·³å¯å‘å¼æ¢æµ‹ï¼šä½ç½®ç¼–ç æ‰°åŠ¨æ­ç¤ºå¤§è¯­è¨€æ¨¡å‹çš„å¼‚å¸¸è¡Œä¸º",
      "authors": [
        "Rui Melo",
        "Rui Abreu",
        "Corina S. Pasareanu"
      ],
      "abstract": "We draw inspiration from microsaccades, tiny involuntary eye movements that reveal hidden dynamics of human perception, to propose an analogous probing method for large language models (LLMs). Just as microsaccades expose subtle but informative shifts in vision, we show that lightweight position encoding perturbations elicit latent signals that indicate model misbehaviour. Our method requires no fine-tuning or task-specific supervision, yet detects failures across diverse settings including factuality, safety, toxicity, and backdoor attacks. Experiments on multiple state-of-the-art LLMs demonstrate that these perturbation-based probes surface misbehaviours while remaining computationally efficient. These findings suggest that pretrained LLMs already encode the internal evidence needed to flag their own failures, and that microsaccade-inspired interventions provide a pathway for detecting and mitigating undesirable behaviours.",
      "tldr_zh": "è¯¥ç ”ç©¶å—åˆ°äººç±»è§†è§‰ä¸­å¾®æ‰«è§†(microsaccades)ç°è±¡çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ¢é’ˆæ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡è½»é‡çº§çš„ä½ç½®ç¼–ç æ‰°åŠ¨(positional encoding perturbations)æ­ç¤ºæ¨¡å‹çš„æ½œåœ¨åŠ¨æ€ä¸å¤±å½“è¡Œä¸º(misbehaviours)ã€‚è¯¥æ–¹æ³•æ— éœ€ä»»ä½•å¾®è°ƒ(fine-tuning)æˆ–ç‰¹å®šä»»åŠ¡çš„ç›‘ç£ï¼Œå³å¯åœ¨äº‹å®æ€§(factuality)ã€å®‰å…¨æ€§(safety)ã€æ¯’æ€§(toxicity)ä»¥åŠåé—¨æ”»å‡»(backdoor attacks)ç­‰å¤šç§åœºæ™¯ä¸‹æ£€æµ‹æ¨¡å‹å¤±æ•ˆã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¢æµ‹æ‰‹æ®µåœ¨å¤šä¸ªæœ€å…ˆè¿›çš„LLMsä¸Šå‡è¡¨ç°å‡ºæé«˜çš„è®¡ç®—æ•ˆç‡ï¼Œå¹¶èƒ½æœ‰æ•ˆæ•æ‰æ¨¡å‹å†…éƒ¨ç”¨äºæ ‡è®°è‡ªèº«é”™è¯¯çš„è¯æ®ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè¿™ç§å—ç”Ÿç‰©å¯å‘çš„ä½ç½®å¹²é¢„ä¸ºè¯†åˆ«å’Œç¼“è§£LLMsçš„ä¸è‰¯è¡Œä¸ºæä¾›äº†ä¸€æ¡æ–°è·¯å¾„ï¼Œæ­ç¤ºäº†é¢„è®­ç»ƒæ¨¡å‹å…·å¤‡è‡ªæˆ‘ç›‘æµ‹å¤±æ•ˆçš„å†…åœ¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 main pages, 13 appendix pages",
      "pdf_url": "https://arxiv.org/pdf/2510.01288v1",
      "published_date": "2025-10-01 01:24:59 UTC",
      "updated_date": "2025-10-01 01:24:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:07:39.287676+00:00"
    },
    {
      "arxiv_id": "2510.03317v2",
      "title": "Photorealistic Inpainting for Perturbation-based Explanations in Ecological Monitoring",
      "title_zh": "ç”Ÿæ€ç›‘æµ‹ä¸­åŸºäºæ‰°åŠ¨è§£é‡Šçš„é«˜ä¿çœŸå›¾åƒè¡¥å…¨",
      "authors": [
        "GÃ¼nel Aghakishiyeva",
        "Jiayi Zhou",
        "Saagar Arya",
        "Julian Dale",
        "James David Poling",
        "Holly R. Houliston",
        "Jamie N. Womble",
        "Gregory D. Larsen",
        "David W. Johnston",
        "Brinnae Bent"
      ],
      "abstract": "Ecological monitoring is increasingly automated by vision models, yet opaque predictions limit trust and field adoption. We present an inpainting-guided, perturbation-based explanation technique that produces photorealistic, mask-localized edits that preserve scene context. Unlike masking or blurring, these edits stay in-distribution and reveal which fine-grained morphological cues drive predictions in tasks such as species recognition and trait attribution. We demonstrate the approach on a YOLOv9 detector fine-tuned for harbor seal detection in Glacier Bay drone imagery, using Segment-Anything-Model-refined masks to support two interventions: (i) object removal/replacement (e.g., replacing seals with plausible ice/water or boats) and (ii) background replacement with original animals composited onto new scenes. Explanations are assessed by re-scoring perturbed images (flip rate, confidence drop) and by expert review for ecological plausibility and interpretability. The resulting explanations localize diagnostic structures, avoid deletion artifacts common to traditional perturbations, and yield domain-relevant insights that support expert validation and more trustworthy deployment of AI in ecology.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæ€ç›‘æµ‹ä¸­è§†è§‰æ¨¡å‹é¢„æµ‹ä¸é€æ˜å¯¼è‡´ä¿¡ä»»ç¼ºå¤±çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå›¾åƒä¿®å¤(Inpainting)å¼•å¯¼çš„æ‰°åŠ¨è§£é‡ŠæŠ€æœ¯ã€‚è¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆç…§ç‰‡çº§çœŸå®ä¸”ç²¾ç¡®å®šä½çš„æ©ç ç¼–è¾‘ï¼Œåœ¨ä¿ç•™åœºæ™¯èƒŒæ™¯çš„åŒæ—¶ç¡®ä¿ä¿®æ”¹åçš„å›¾åƒå¤„äºæ•°æ®åˆ†å¸ƒå†…ï¼Œä»è€Œæœ‰æ•ˆæ­ç¤ºé©±åŠ¨ç‰©ç§è¯†åˆ«ç­‰ä»»åŠ¡é¢„æµ‹çš„ç»†ç²’åº¦å½¢æ€çº¿ç´¢ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨å†°å·æ¹¾(Glacier Bay)æ— äººæœºå½±åƒçš„æµ·è±¹æ£€æµ‹ä»»åŠ¡ä¸Šæ¼”ç¤ºäº†è¯¥æ–¹æ¡ˆï¼Œç»“åˆYOLOv9æ£€æµ‹å™¨å’ŒSegment-Anything-Model(SAM)ä¼˜åŒ–æ©ç ï¼Œå®ç°äº†ç›®æ ‡æ›¿æ¢å’ŒèƒŒæ™¯æ›´æ¢ç­‰å¹²é¢„æ“ä½œã€‚é€šè¿‡ä¸“å®¶è¯„å®¡å’Œå¯¹æ‰°åŠ¨å›¾åƒçš„æŒ‡æ ‡è¯„ä¼°ï¼Œå®éªŒè¯æ˜è¯¥æ–¹æ³•èƒ½ç²¾å‡†å®šä½è¯Šæ–­æ€§ç»“æ„ï¼Œæœ‰æ•ˆé¿å…äº†ä¼ ç»Ÿæ‰°åŠ¨æ–¹æ³•äº§ç”Ÿçš„ä¼ªå½±ã€‚è¯¥æŠ€æœ¯ä¸ºç”Ÿæ€å­¦é¢†åŸŸæä¾›äº†å…·æœ‰é¢†åŸŸç›¸å…³æ€§ä¸”å¯è§£é‡Šçš„æ·±åº¦è§è§£ï¼Œä¸ºæ„å»ºæ›´å€¼å¾—ä¿¡èµ–çš„è‡ªåŠ¨åŒ–ç”Ÿæ€ç›‘æµ‹AIç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "NeurIPS 2025 Imageomics Workshop",
      "pdf_url": "https://arxiv.org/pdf/2510.03317v2",
      "published_date": "2025-10-01 01:18:27 UTC",
      "updated_date": "2025-10-24 11:24:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:07:53.182004+00:00"
    },
    {
      "arxiv_id": "2510.00395v2",
      "title": "SAGE-Music: Low-Latency Symbolic Music Generation via Attribute-Specialized Key-Value Head Sharing",
      "title_zh": "SAGE-Musicï¼šåŸºäºå±æ€§ç‰¹åŒ–é”®å€¼å¤´å…±äº«çš„ä½å»¶è¿Ÿç¬¦å·éŸ³ä¹ç”Ÿæˆ",
      "authors": [
        "Jiaye Tan",
        "Haonan Luo",
        "Linfeng Song",
        "Shuaiqi Chen",
        "Yishan Lyu",
        "Zian Zhong",
        "Roujia Wang",
        "Daniel Jiang",
        "Haoran Zhang",
        "Jiaming Bai",
        "Haoran Cheng",
        "Q. Vera Liao",
        "Hao-Wen Dong"
      ],
      "abstract": "Low-latency symbolic music generation is essential for real-time improvisation and human-AI co-creation. Existing transformer-based models, however, face a trade-off between inference speed and musical quality. Traditional acceleration techniques such as embedding pooling significantly degrade quality, while recently proposed Byte Pair Encoding (BPE) methods - though effective on single-track piano data - suffer large performance drops in multi-track settings, as revealed by our analysis. We propose Attribute-Specialized Key-Value Head Sharing (AS-KVHS), adapted to music's structured symbolic representation, achieving about 30% inference speedup with only a negligible (about 0.4%) quality drop in objective evaluations and slight improvements in subjective listening tests. Our main contributions are (1) the first systematic study of BPE's generalizability in multi-track symbolic music, and (2) the introduction of AS-KVHS for low-latency symbolic music generation. Beyond these, we also release SAGE-Music, an open-source benchmark that matches or surpasses state-of-the-art models in generation quality.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å®æ—¶å³å…´åˆ›ä½œå’Œäººæœºåä½œå¯¹ä½å»¶è¿Ÿç¬¦å·éŸ³ä¹ç”Ÿæˆçš„è¿«åˆ‡éœ€æ±‚ï¼Œæ¢è®¨äº†ç°æœ‰ Transformer æ¨¡å‹åœ¨æ¨ç†é€Ÿåº¦ä¸éŸ³ä¹è´¨é‡ä¹‹é—´éš¾ä»¥å¹³è¡¡çš„é—®é¢˜ã€‚é€šè¿‡åˆ†æå‘ç°ï¼Œè™½ç„¶å­—èŠ‚å¯¹ç¼–ç  (Byte Pair Encoding, BPE) åœ¨å•è½¨é’¢ç´æ•°æ®ä¸Šæœ‰æ•ˆï¼Œä½†åœ¨å¤šè½¨ (Multi-track) è®¾ç½®ä¸‹æ€§èƒ½ä¼šå¤§å¹…ä¸‹é™ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† Attribute-Specialized Key-Value Head Sharing (AS-KVHS) æŠ€æœ¯ï¼Œåˆ©ç”¨éŸ³ä¹çš„ç»“æ„åŒ–ç¬¦å·è¡¨ç¤ºç‰¹ç‚¹ï¼Œåœ¨å®¢è§‚è¯„ä¼°è´¨é‡ä»…ä¸‹é™çº¦ 0.4% çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†çº¦ 30% çš„æ¨ç†æé€Ÿã€‚è®ºæ–‡çš„è´¡çŒ®åœ¨äºé¦–æ¬¡ç³»ç»Ÿç ”ç©¶äº† BPE åœ¨å¤šè½¨ç¬¦å·éŸ³ä¹ä¸­çš„æ³›åŒ–æ€§ï¼Œå¹¶å¼•å…¥äº† AS-KVHS æ¶æ„ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘å¸ƒäº†å¼€æºåŸºå‡† SAGE-Musicï¼Œå…¶ç”Ÿæˆè´¨é‡åœ¨å¯¹æ¯”æµ‹è¯•ä¸­è¾¾åˆ°æˆ–è¶…è¶Šäº†ç°æœ‰çš„å…ˆè¿›æ¨¡å‹ (State-of-the-art)ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Withdrawn after identifying that results in Section 5 require additional re-analysis before public dissemination",
      "pdf_url": "https://arxiv.org/pdf/2510.00395v2",
      "published_date": "2025-10-01 01:11:43 UTC",
      "updated_date": "2025-10-15 02:29:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:07:54.177241+00:00"
    },
    {
      "arxiv_id": "2510.00386v1",
      "title": "Train on Validation (ToV): Fast data selection with applications to fine-tuning",
      "title_zh": "Train on Validation (ToV)ï¼šå¿«é€Ÿæ•°æ®é€‰æ‹©åŠå…¶åœ¨å¾®è°ƒä¸­çš„åº”ç”¨",
      "authors": [
        "Ayush Jain",
        "Andrea Montanari",
        "Eren Sasoglu"
      ],
      "abstract": "State-of-the-art machine learning often follows a two-stage process: $(i)$~pre-training on large, general-purpose datasets; $(ii)$~fine-tuning on task-specific data. In fine-tuning, selecting training examples that closely reflect the target distribution is crucial. However, it is often the case that only a few samples are available from the target distribution. Existing data selection methods treat these target samples as a validation set and estimate the effect of adding or removing a single sample from the training pool by performing inference on the validation set.\n  We propose a simpler and faster alternative that inverts the usual role of train and validation: we perform inference on the training pool before and after fine-tuning on the validation set. We then select samples whose predictions change the most. Our key insight is that the training samples most affected by fine-tuning on a small validation set tend to be the most beneficial for reducing test loss on the target distribution. Experiments on instruction tuning and named entity recognition tasks show that, in most cases, our method achieves lower test log-loss than state-of-the-art approaches. We support our findings with theoretical analysis.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Train on Validation (ToV)ï¼Œä¸€ç§ç”¨äºå¾®è°ƒ (fine-tuning) é˜¶æ®µå¿«é€Ÿé€‰æ‹©è®­ç»ƒæ•°æ®çš„ç®€æ´æ–¹æ³•ã€‚é’ˆå¯¹ç›®æ ‡åˆ†å¸ƒæ ·æœ¬ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼ŒToV åè½¬äº†ä¼ ç»Ÿè®­ç»ƒå’ŒéªŒè¯çš„è§’è‰²ï¼Œåœ¨å¯¹éªŒè¯é›†è¿›è¡Œå¾®è°ƒå‰ååˆ†åˆ«å¯¹è®­ç»ƒæ± è¿›è¡Œæ¨ç†ï¼Œå¹¶ä»ä¸­é€‰æ‹©é¢„æµ‹ç»“æœå˜åŒ–æœ€å¤§çš„æ ·æœ¬ã€‚å…¶æ ¸å¿ƒæ´å¯Ÿåœ¨äºï¼Œå—éªŒè¯é›†å¾®è°ƒå½±å“æœ€å¤§çš„è®­ç»ƒæ ·æœ¬é€šå¸¸å¯¹é™ä½ç›®æ ‡åˆ†å¸ƒä¸Šçš„æµ‹è¯•æŸå¤± (test loss) æœ€ä¸ºæœ‰ç›Šã€‚åœ¨æŒ‡ä»¤å¾®è°ƒ (instruction tuning) å’Œå‘½åå®ä½“è¯†åˆ« (named entity recognition) ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒToV åœ¨å¤šæ•°æƒ…å†µä¸‹æ¯”ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•å…·æœ‰æ›´ä½çš„æµ‹è¯•å¯¹æ•°æŸå¤± (test log-loss)ã€‚è¯¥æ–¹æ³•ä¸ä»…æ“ä½œç®€å•ã€è®¡ç®—é€Ÿåº¦å¿«ï¼Œä¸”å¾—åˆ°äº†ç†è®ºåˆ†æçš„æ”¯æŒï¼Œä¸ºä¼˜åŒ–ç‰¹å®šä»»åŠ¡çš„æ•°æ®é€‰æ‹©æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00386v1",
      "published_date": "2025-10-01 00:55:39 UTC",
      "updated_date": "2025-10-01 00:55:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:08:00.698887+00:00"
    },
    {
      "arxiv_id": "2510.03316v1",
      "title": "The View From Space: Navigating Instrumentation Differences with EOFMs",
      "title_zh": "ç©ºé—´è§†è§’ï¼šåˆ©ç”¨ EOFMs åº”å¯¹ä»ªå™¨å·®å¼‚",
      "authors": [
        "Ryan P. Demilt",
        "Nicholas LaHaye",
        "Karis Tenneson"
      ],
      "abstract": "Earth Observation Foundation Models (EOFMs) have exploded in prevalence as tools for processing the massive volumes of remotely sensed and other earth observation data, and for delivering impact on the many essential earth monitoring tasks. An emerging trend posits using the outputs of pre-trained models as 'embeddings' which summarize high dimensional data to be used for generic tasks such as similarity search and content-specific queries. However, most EOFM models are trained only on single modalities of data and then applied or benchmarked by matching bands across different modalities. It is not clear from existing work what impact diverse sensor architectures have on the internal representations of the present suite of EOFMs. We show in this work that the representation space of EOFMs is highly sensitive to sensor architecture and that understanding this difference gives a vital perspective on the pitfalls of current EOFM design and signals for how to move forward as model developers, users, and a community guided by robust remote-sensing science.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ°çƒè§‚æµ‹åŸºç¡€æ¨¡å‹ (Earth Observation Foundation Models, EOFMs) åœ¨å¤„ç†æµ·é‡é¥æ„Ÿæ•°æ®ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯å°†é¢„è®­ç»ƒæ¨¡å‹è¾“å‡ºä½œä¸ºâ€œåµŒå…¥â€ (embeddings) è¿›è¡Œç›¸ä¼¼æ€§æœç´¢å’Œå†…å®¹æŸ¥è¯¢çš„è¶‹åŠ¿ã€‚ä½œè€…æŒ‡å‡ºï¼Œç›®å‰å¤§å¤šæ•° EOFMs ä»…åœ¨å•ä¸€æ¨¡æ€æ•°æ®ä¸Šè®­ç»ƒï¼Œä½†åœ¨è·¨æ¨¡æ€åº”ç”¨ä¸­ï¼Œä¼ æ„Ÿå™¨æ¶æ„ (sensor architectures) çš„å·®å¼‚å¯¹æ¨¡å‹å†…éƒ¨è¡¨å¾çš„å…·ä½“å½±å“å°šä¸æ˜ç¡®ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒEOFMs çš„è¡¨å¾ç©ºé—´ (representation space) å¯¹ä¼ æ„Ÿå™¨æ¶æ„å…·æœ‰é«˜åº¦æ•æ„Ÿæ€§ï¼Œè¿™ä¸€ç‰¹æ€§æ­ç¤ºäº†å½“å‰æ¨¡å‹è®¾è®¡ä¸­å­˜åœ¨çš„æ½œåœ¨é™·é˜±ã€‚è¯¥å·¥ä½œé€šè¿‡å¯¼èˆªä»ªå™¨é—´çš„å·®å¼‚ï¼Œä¸ºæ¨¡å‹å¼€å‘è€…å’Œç”¨æˆ·æä¾›äº†æ”¹è¿› EOFMs è®¾è®¡çš„å…³é”®è§†è§’ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†åœ¨é¥æ„Ÿç§‘å­¦æŒ‡å¯¼ä¸‹æ„å»ºç¨³å¥åŸºç¡€æ¨¡å‹çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥çš„æ¨¡å‹å¼€å‘æŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03316v1",
      "published_date": "2025-10-01 00:53:45 UTC",
      "updated_date": "2025-10-01 00:53:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:08:00.885010+00:00"
    },
    {
      "arxiv_id": "2510.00381v1",
      "title": "Semantic-Driven AI Agent Communications: Challenges and Solutions",
      "title_zh": "è¯­ä¹‰é©±åŠ¨çš„AIæ™ºèƒ½ä½“é€šä¿¡ï¼šæŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ",
      "authors": [
        "Kaiwen Yu",
        "Mengying Sun",
        "Zhijin Qin",
        "Xiaodong Xu",
        "Ping Yang",
        "Yue Xiao",
        "Gang Wu"
      ],
      "abstract": "With the rapid growth of intelligent services, communication targets are shifting from humans to artificial intelligent (AI) agents, which require new paradigms to enable real-time perception, decision-making, and collaboration. Semantic communication, which conveys task-relevant meaning rather than raw data, offers a promising solution. However, its practical deployment remains constrained by dynamic environments and limited resources. To address these issues, this article proposes a semantic-driven AI agent communication framework and develops three enabling techniques. First, semantic adaptation transmission applies fine-tuning with real or generative samples to efficiently adapt models to varying environments. Second, semantic lightweight transmission incorporates pruning, quantization, and perception-aware sampling to reduce model complexity and alleviate computational burden on edge agents. Third, semantic self-evolution control employs distributed hierarchical decision-making to optimize multi-dimensional resources, enabling robust multi-agent collaboration in dynamic environments. Simulation results show that the proposed solutions achieve faster convergence and stronger robustness, while the proposed distributed hierarchical optimization method significantly outperforms conventional decision-making schemes, highlighting its potential for AI agent communication networks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ™ºèƒ½æœåŠ¡ä¸­é€šä¿¡ä¸»ä½“å‘ AI agents è½¬å˜çš„éœ€æ±‚ï¼Œæå‡ºäº†ä¸€ä¸ªè¯­ä¹‰é©±åŠ¨çš„ AI agent é€šä¿¡æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŠ¨æ€ç¯å¢ƒå’Œèµ„æºé™åˆ¶ä¸‹çš„å®æ—¶æ„ŸçŸ¥ã€å†³ç­–ä¸åä½œæŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é¦–å…ˆå¼•å…¥äº† Semantic adaptation transmission æŠ€æœ¯ï¼Œé€šè¿‡ç»“åˆçœŸå®æˆ–ç”Ÿæˆæ ·æœ¬çš„å¾®è°ƒ (fine-tuning) æ–¹æ³•ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆé€‚åº”å¤šå˜çš„ç¯å¢ƒã€‚ä¸ºäº†å‡è½»è¾¹ç¼˜æ™ºèƒ½ä½“çš„è®¡ç®—è´Ÿæ‹…ï¼Œç ”ç©¶é‡‡ç”¨äº† Semantic lightweight transmission æŠ€æœ¯ï¼Œåˆ©ç”¨å‰ªæ (pruning)ã€é‡åŒ– (quantization) å’Œæ„ŸçŸ¥æ„ŸçŸ¥é‡‡æ · (perception-aware sampling) é™ä½æ¨¡å‹å¤æ‚åº¦ã€‚æ­¤å¤–ï¼Œæ¡†æ¶é€šè¿‡ Semantic self-evolution control å®ç°äº†åˆ†å¸ƒå¼å±‚çº§å†³ç­–ï¼Œä¼˜åŒ–äº†å¤šç»´èµ„æºåˆ†é…ï¼Œä»è€Œç¡®ä¿äº†åŠ¨æ€ç¯å¢ƒä¸‹é²æ£’çš„å¤šæ™ºèƒ½ä½“åä½œã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ¡ˆåœ¨æ”¶æ•›é€Ÿåº¦å’Œé²æ£’æ€§æ–¹é¢å‡è¡¨ç°ä¼˜å¼‚ï¼Œå…¶åˆ†å¸ƒå¼å±‚çº§ä¼˜åŒ–æ–¹æ³•æ˜¾è‘—ä¼˜äºä¼ ç»Ÿå†³ç­–æ–¹æ¡ˆï¼Œçªæ˜¾äº†è¯­ä¹‰é€šä¿¡åœ¨ AI agent é€šä¿¡ç½‘ç»œä¸­çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00381v1",
      "published_date": "2025-10-01 00:52:37 UTC",
      "updated_date": "2025-10-01 00:52:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:08:02.771530+00:00"
    },
    {
      "arxiv_id": "2510.00376v1",
      "title": "Discrete Wavelet Transform as a Facilitator for Expressive Latent Space Representation in Variational Autoencoders in Satellite Imagery",
      "title_zh": "ç¦»æ•£å°æ³¢å˜æ¢ï¼šåŠ©åŠ›å«æ˜Ÿå›¾åƒå˜åˆ†è‡ªç¼–ç å™¨å®ç°é«˜è¡¨ç°åŠ›çš„æ½œç©ºé—´è¡¨ç¤º",
      "authors": [
        "Arpan Mahara",
        "Md Rezaul Karim Khan",
        "Naphtali Rishe",
        "Wenjia Wang",
        "Seyed Masoud Sadjadi"
      ],
      "abstract": "Latent Diffusion Models (LDM), a subclass of diffusion models, mitigate the computational complexity of pixel-space diffusion by operating within a compressed latent space constructed by Variational Autoencoders (VAEs), demonstrating significant advantages in Remote Sensing (RS) applications. Though numerous studies enhancing LDMs have been conducted, investigations explicitly targeting improvements within the intrinsic latent space remain scarce. This paper proposes an innovative perspective, utilizing the Discrete Wavelet Transform (DWT) to enhance the VAE's latent space representation, designed for satellite imagery. The proposed method, ExpDWT-VAE, introduces dual branches: one processes spatial domain input through convolutional operations, while the other extracts and processes frequency-domain features via 2D Haar wavelet decomposition, convolutional operation, and inverse DWT reconstruction. These branches merge to create an integrated spatial-frequency representation, further refined through convolutional and diagonal Gaussian mapping into a robust latent representation. We utilize a new satellite imagery dataset housed by the TerraFly mapping system to validate our method. Experimental results across several performance metrics highlight the efficacy of the proposed method at enhancing latent space representation.",
      "tldr_zh": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸º ExpDWT-VAE çš„åˆ›æ–°æ–¹æ³•ï¼Œæ—¨åœ¨åˆ©ç”¨ç¦»æ•£å°æ³¢å˜æ¢ (Discrete Wavelet Transform, DWT) å¢å¼ºå«æ˜Ÿå›¾åƒåœ¨ Variational Autoencoders (VAEs) ä¸­çš„æ½œç©ºé—´ (latent space) è¡¨å¾ã€‚ç ”ç©¶é’ˆå¯¹ Latent Diffusion Models (LDM) åœ¨é¥æ„Ÿ (Remote Sensing) åº”ç”¨ä¸­æ½œç©ºé—´ç ”ç©¶ä¸è¶³çš„ç°çŠ¶ï¼Œè®¾è®¡äº†ç‹¬ç‰¹çš„åŒåˆ†æ”¯æ¶æ„ã€‚è¯¥æ¶æ„ç»“åˆäº†å¤„ç†ç©ºé—´åŸŸ (spatial domain) çš„å·ç§¯åˆ†æ”¯ä¸åŸºäº 2D Haar wavelet åˆ†è§£ã€å·ç§¯åŠé€†å˜æ¢çš„é¢‘åŸŸ (frequency-domain) åˆ†æ”¯ï¼Œé€šè¿‡èåˆä¸¤è€…ç”Ÿæˆç»¼åˆçš„ç©ºé—´-é¢‘ç‡ç‰¹å¾ã€‚éšåï¼Œåˆ©ç”¨å·ç§¯å’Œå¯¹è§’é«˜æ–¯æ˜ å°„ (diagonal Gaussian mapping) å°†ç‰¹å¾è¿›ä¸€æ­¥ç»†åŒ–ä¸ºé«˜é²æ£’æ€§çš„æ½œç©ºé—´è¡¨ç¤ºã€‚å®éªŒé‡‡ç”¨ TerraFly åœ°å›¾ç³»ç»Ÿçš„å«æ˜Ÿå›¾åƒæ•°æ®é›†è¿›è¡ŒéªŒè¯ï¼Œå¤šé¡¹æ€§èƒ½æŒ‡æ ‡ç»“æœè¯æ˜äº† ExpDWT-VAE åœ¨æå‡æ½œç©ºé—´è¡¨è¾¾èƒ½åŠ›å’Œå«æ˜Ÿå›¾åƒé‡æ„è´¨é‡æ–¹é¢çš„æ˜¾è‘—æ•ˆèƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 3 Figures",
      "pdf_url": "https://arxiv.org/pdf/2510.00376v1",
      "published_date": "2025-10-01 00:49:41 UTC",
      "updated_date": "2025-10-01 00:49:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:08:06.080803+00:00"
    },
    {
      "arxiv_id": "2510.00373v1",
      "title": "Combining Large Language Models and Gradient-Free Optimization for Automatic Control Policy Synthesis",
      "title_zh": "ç»“åˆå¤§è¯­è¨€æ¨¡å‹ä¸æ— æ¢¯åº¦ä¼˜åŒ–çš„è‡ªåŠ¨æ§åˆ¶ç­–ç•¥åˆæˆ",
      "authors": [
        "Carlo Bosio",
        "Matteo Guarrera",
        "Alberto Sangiovanni-Vincentelli",
        "Mark W. Mueller"
      ],
      "abstract": "Large Language models (LLMs) have shown promise as generators of symbolic control policies, producing interpretable program-like representations through iterative search. However, these models are not capable of separating the functional structure of a policy from the numerical values it is parametrized by, thus making the search process slow and inefficient. We propose a hybrid approach that decouples structural synthesis from parameter optimization by introducing an additional optimization layer for local parameter search. In our method, the numerical parameters of LLM-generated programs are extracted and optimized numerically to maximize task performance. With this integration, an LLM iterates over the functional structure of programs, while a separate optimization loop is used to find a locally optimal set of parameters accompanying candidate programs. We evaluate our method on a set of control tasks, showing that it achieves higher returns and improved sample efficiency compared to purely LLM-guided search. We show that combining symbolic program synthesis with numerical optimization yields interpretable yet high-performing policies, bridging the gap between language-model-guided design and classical control tuning. Our code is available at https://sites.google.com/berkeley.edu/colmo.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç”Ÿæˆç¬¦å·æ§åˆ¶ç­–ç•¥æ—¶éš¾ä»¥åŒºåˆ†åŠŸèƒ½ç»“æ„ä¸æ•°å€¼å‚æ•°ï¼Œå¯¼è‡´æœç´¢æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å°† LLMs ä¸æ— æ¢¯åº¦ä¼˜åŒ–(Gradient-Free Optimization)ç›¸ç»“åˆçš„è‡ªåŠ¨æ§åˆ¶ç­–ç•¥åˆæˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥é¢å¤–çš„å±€éƒ¨å‚æ•°æœç´¢å±‚ï¼Œå®ç°äº†ç»“æ„åˆæˆä¸å‚æ•°ä¼˜åŒ–çš„è§£è€¦ã€‚åœ¨è¯¥æ¡†æ¶ä¸‹ï¼ŒLLM è´Ÿè´£è¿­ä»£ç¨‹åºçš„ç¬¦å·åŒ–åŠŸèƒ½ç»“æ„ï¼Œè€Œç‹¬ç«‹çš„æ•°å€¼ä¼˜åŒ–å¾ªç¯åˆ™ä¸“é—¨ç”¨äºå¯»æ‰¾å€™é€‰ç¨‹åºçš„æœ€ä¼˜æ•°å€¼å‚æ•°ã€‚åœ¨å¤šé¡¹æ§åˆ¶ä»»åŠ¡ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ··åˆæ–¹æ³•ç›¸è¾ƒäºçº¯ LLM æŒ‡å¯¼çš„æœç´¢å…·æœ‰æ›´é«˜çš„ä»»åŠ¡å›æŠ¥å’Œæ›´ä¼˜çš„æ ·æœ¬æ•ˆç‡(Sample Efficiency)ã€‚ç ”ç©¶ç»“æœè¯æ˜ï¼Œå°†ç¬¦å·ç¨‹åºåˆæˆä¸æ•°å€¼ä¼˜åŒ–ç›¸ç»“åˆå¯ä»¥äº§ç”Ÿå…¼å…·å¯è§£é‡Šæ€§ä¸é«˜æ€§èƒ½çš„æ§åˆ¶ç­–ç•¥ï¼ŒæˆåŠŸå¼¥åˆäº†åŸºäºè¯­è¨€æ¨¡å‹çš„è®¾è®¡ä¸ä¼ ç»Ÿæ§åˆ¶è°ƒä¼˜ä¹‹é—´çš„å·®è·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.00373v1",
      "published_date": "2025-10-01 00:42:15 UTC",
      "updated_date": "2025-10-01 00:42:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:08:34.271853+00:00"
    },
    {
      "arxiv_id": "2510.01287v1",
      "title": "Evaluating New AI Cell Foundation Models on Challenging Kidney Pathology Cases Unaddressed by Previous Foundation Models",
      "title_zh": "åœ¨å…ˆå‰åŸºç¡€æ¨¡å‹æœªèƒ½è§£å†³çš„æŒ‘æˆ˜æ€§è‚¾è„ç—…ç†æ¡ˆä¾‹ä¸Šè¯„ä¼°æ–°å‹ AI ç»†èƒåŸºç¡€æ¨¡å‹",
      "authors": [
        "Runchen Wang",
        "Junlin Guo",
        "Siqi Lu",
        "Ruining Deng",
        "Zhengyi Lu",
        "Yanfan Zhu",
        "Yuechen Yang",
        "Chongyu Qu",
        "Yu Wang",
        "Shilin Zhao",
        "Catie Chang",
        "Mitchell Wilkes",
        "Mengmeng Yin",
        "Haichun Yang",
        "Yuankai Huo"
      ],
      "abstract": "Accurate cell nuclei segmentation is critical for downstream tasks in kidney pathology and remains a major challenge due to the morphological diversity and imaging variability of renal tissues. While our prior work has evaluated early-generation AI cell foundation models in this domain, the effectiveness of recent cell foundation models remains unclear. In this study, we benchmark advanced AI cell foundation models (2025), including CellViT++ variants and Cellpose-SAM, against three widely used cell foundation models developed prior to 2024, using a diverse large-scale set of kidney image patches within a human-in-the-loop rating framework. We further performed fusion-based ensemble evaluation and model agreement analysis to assess the segmentation capabilities of the different models. Our results show that CellViT++ [Virchow] yields the highest standalone performance with 40.3% of predictions rated as \"Good\" on a curated set of 2,091 challenging samples, outperforming all prior models. In addition, our fused model achieves 62.2% \"Good\" predictions and only 0.4% \"Bad\", substantially reducing segmentation errors. Notably, the fusion model (2025) successfully resolved the majority of challenging cases that remained unaddressed in our previous study. These findings demonstrate the potential of AI cell foundation model development in renal pathology and provide a curated dataset of challenging samples to support future kidney-specific model refinement.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†2025å¹´æœ€æ–°å‘å¸ƒçš„AIç»†èƒåŸºç¡€æ¨¡å‹(AI cell foundation models)åœ¨æŒ‘æˆ˜æ€§è‚¾è„ç—…ç†å›¾åƒä¸­çš„ç»†èƒæ ¸åˆ†å‰²æ€§èƒ½ï¼Œæ—¨åœ¨è§£å†³ç”±äºå½¢æ€å¤šæ ·æ€§å’Œæˆåƒå·®å¼‚å¸¦æ¥çš„åˆ†å‰²éš¾é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿå¯¹æ¯”äº†CellViT++å˜ä½“ã€Cellpose-SAMä¸2024å¹´ä¹‹å‰çš„åŸºçº¿æ¨¡å‹ï¼Œå¹¶ç»“åˆäº†äººæœºåä½œ(human-in-the-loop)è¯„åˆ†æ¡†æ¶ã€èåˆé›†æˆè¯„ä¼°(fusion-based ensemble evaluation)ä»¥åŠæ¨¡å‹ä¸€è‡´æ€§åˆ†æã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCellViT++ [Virchow]åœ¨2091ä¸ªæŒ‘æˆ˜æ€§æ ·æœ¬ä¸­å–å¾—äº†æœ€ä½³çš„ç‹¬ç«‹è¡¨ç°ï¼Œå…¶ä¼˜ç§€é¢„æµ‹ç‡è¾¾40.3%ã€‚è¿›ä¸€æ­¥é‡‡ç”¨èåˆæ¨¡å‹(fused model)åï¼Œä¼˜ç§€é¢„æµ‹ç‡æ˜¾è‘—æå‡è‡³62.2%ï¼Œä¸”é”™è¯¯ç‡é™è‡³ä»…0.4%ï¼ŒæˆåŠŸè§£å†³äº†æ­¤å‰ç ”ç©¶ä¸­æœªèƒ½å¤„ç†çš„å¤§å¤šæ•°å¤æ‚æ¡ˆä¾‹ã€‚è¿™äº›å‘ç°è¯æ˜äº†AIç»†èƒåŸºç¡€æ¨¡å‹åœ¨è‚¾è„ç—…ç†å­¦ä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œå¹¶ä¸ºæœªæ¥ç‰¹å®šç—…ç†æ¨¡å‹çš„ä¼˜åŒ–æä¾›äº†ä¸€å¥—ç²¾é€‰çš„æŒ‘æˆ˜æ€§æ•°æ®é›†ã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01287v1",
      "published_date": "2025-10-01 00:38:36 UTC",
      "updated_date": "2025-10-01 00:38:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:08:13.568810+00:00"
    },
    {
      "arxiv_id": "2510.03315v1",
      "title": "Decomposing Attention To Find Context-Sensitive Neurons",
      "title_zh": "åˆ†è§£æ³¨æ„åŠ›ä»¥å‘ç°ä¸Šä¸‹æ–‡æ•æ„Ÿç¥ç»å…ƒ",
      "authors": [
        "Alex Gibson"
      ],
      "abstract": "We study transformer language models, analyzing attention heads whose attention patterns are spread out, and whose attention scores depend weakly on content. We argue that the softmax denominators of these heads are stable when the underlying token distribution is fixed. By sampling softmax denominators from a \"calibration text\", we can combine together the outputs of multiple such stable heads in the first layer of GPT2-Small, approximating their combined output by a linear summary of the surrounding text. This approximation enables a procedure where from the weights alone - and a single calibration text - we can uncover hundreds of first layer neurons that respond to high-level contextual properties of the surrounding text, including neurons that didn't activate on the calibration text.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥åˆ†æäº† Transformer è¯­è¨€æ¨¡å‹ä¸­æ³¨æ„åŠ›æ¨¡å¼åˆ†æ•£ä¸”å¯¹å†…å®¹ä¾èµ–è¾ƒå¼±çš„ attention headsï¼Œå¹¶æå‡ºè¿™äº› heads çš„ softmax åˆ†æ¯åœ¨ç‰¹å®š token åˆ†å¸ƒä¸‹å…·æœ‰ç¨³å®šæ€§ã€‚é€šè¿‡ä» calibration text ä¸­é‡‡æ · softmax åˆ†æ¯ï¼Œç ”ç©¶è€…æˆåŠŸå°† GPT2-Small ç¬¬ä¸€å±‚ä¸­å¤šä¸ªç¨³å®š heads çš„è¾“å‡ºè¿›è¡Œæ•´åˆï¼Œå¹¶åˆ©ç”¨å‘¨å›´æ–‡æœ¬çš„çº¿æ€§æ‘˜è¦æ¥è¿‘ä¼¼å…¶ç»„åˆè¾“å‡ºã€‚è¿™ç§åŸºäºæƒé‡å’Œå•ä¸ªæ ¡å‡†æ–‡æœ¬çš„åˆ†è§£æ–¹æ³•ï¼Œæ­ç¤ºäº†æ•°ç™¾ä¸ªå“åº”é«˜å±‚çº§ contextual properties çš„ç¬¬ä¸€å±‚ç¥ç»å…ƒã€‚è¯¥æ–¹æ³•ä¸ä»…èƒ½å¤Ÿè¯†åˆ«å‡ºæ´»è·ƒçš„ç¥ç»å…ƒï¼Œç”šè‡³èƒ½å‘ç°é‚£äº›åœ¨æ ¡å‡†æ–‡æœ¬ä¸­æœªæ›¾æ¿€æ´»çš„ç¥ç»å…ƒï¼Œä¸ºä»åº•å±‚æƒé‡ç›´æ¥è§£ææ¨¡å‹å¦‚ä½•æ•æ‰å¤æ‚ä¸Šä¸‹æ–‡ä¿¡æ¯æä¾›äº†æ–°çš„æœºæ¢°è®ºè§†è§’ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 7 figures. Submitted to the Mechanistic Interpretability Workshop at NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.03315v1",
      "published_date": "2025-10-01 00:29:39 UTC",
      "updated_date": "2025-10-01 00:29:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:08:14.968987+00:00"
    },
    {
      "arxiv_id": "2510.00361v1",
      "title": "Attribution Gradients: Incrementally Unfolding Citations for Critical Examination of Attributed AI Answers",
      "title_zh": "Attribution Gradientsï¼šé€šè¿‡å¢é‡å±•å¼€å¼•ç”¨å®ç°å¯¹å½’å› å¼ AI å›ç­”çš„æ‰¹åˆ¤æ€§å®¡è§†",
      "authors": [
        "Hita Kambhamettu",
        "Alyssa Hwang",
        "Philippe Laban",
        "Andrew Head"
      ],
      "abstract": "AI question answering systems increasingly generate responses with attributions to sources. However, the task of verifying the actual content of these attributions is in most cases impractical. In this paper, we present attribution gradients as a solution. Attribution gradients provide integrated, incremental affordances for diving into an attributed passage. A user can decompose a sentence of an answer into its claims. For each claim, the user can view supporting and contradictory excerpts mined from sources. Those excerpts serve as clickable conduits into the source (in our application, scientific papers). When evidence itself contains more citations, the UI unpacks the evidence into excerpts from the cited sources. These features of attribution gradients facilitate concurrent interconnections among answer, claim, excerpt, and context. In a usability study, we observed greater engagement with sources and richer revision in a task where participants revised an attributed AI answer with attribution gradients and a baseline.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Attribution Gradientsï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è§£å†³éªŒè¯å½’å› å¼äººå·¥æ™ºèƒ½(Attributed AI Answers)ç­”æ¡ˆçœŸå®æ€§éš¾é¢˜çš„ç³»ç»Ÿæ€§æ–¹æ¡ˆã€‚è¯¥å·¥å…·é€šè¿‡å°†ç­”æ¡ˆå¥å­åˆ†è§£ä¸ºå…·ä½“çš„ Claimï¼Œå¹¶æä¾›ä»æºæ–‡ä»¶ä¸­æŒ–æ˜å‡ºçš„æ”¯æŒæˆ–çŸ›ç›¾æ‘˜å½•ï¼Œå®ç°äº†å¯¹å½’å› å†…å®¹çš„å¢é‡å¼å±•å¼€ã€‚è¿™äº›æ‘˜å½•ä¸ä»…ä½œä¸ºè¿›å…¥åŸå§‹ç§‘å­¦è®ºæ–‡çš„å¯ç‚¹å‡»å¯¼å¼•ï¼Œè¿˜èƒ½é€’å½’åœ°è§£æ„è¯æ®ä¸­åŒ…å«çš„è¿›ä¸€æ­¥å¼•ç”¨ï¼Œä»è€Œåœ¨ç­”æ¡ˆã€Claimã€æ‘˜å½•å’ŒåŸå§‹ Context ä¹‹é—´å»ºç«‹å¹¶å‘äº’è”ã€‚å¯ç”¨æ€§ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒAttribution Gradients æ˜¾è‘—å¢å¼ºäº†ç”¨æˆ·ä¸ä¿¡æºçš„äº’åŠ¨ï¼Œä½¿å‚ä¸è€…åœ¨ä¿®è®¢ AI ç”Ÿæˆçš„ç­”æ¡ˆæ—¶èƒ½å¤Ÿè¿›è¡Œæ›´ä¸°å¯Œã€æ›´æ·±å…¥çš„æ‰¹åˆ¤æ€§å®¡æŸ¥ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00361v1",
      "published_date": "2025-10-01 00:07:28 UTC",
      "updated_date": "2025-10-01 00:07:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:08:20.686106+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 210,
  "processed_papers_count": 210,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-25T00:09:27.793746+00:00"
}
{
  "date": "2024-02-23",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-02-23 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 93 篇论文，主要聚焦于 AI 模型优化（如 LLM 的推理和事实性提升）、生成式模型应用（如交互环境和代理训练）、安全强化学习（RL）和机器人领域创新。其中，令人印象深刻的包括 Salesforce 团队的 AgentOhana 框架，以及 ICML 2024 论文如 Deep Networks Always Grok and Here is Why，它们展示了 LLM 在复杂任务中的潜力。下面，我将挑选最重要的论文先聊一聊，并快速掠过其他不那么核心的文章。\n\n### 重点论文讨论\n我先聚焦于 AI 和 LLM 相关的高影响力论文，这些涉及热门话题如事实性改进、代理训练和生成模型。接着，聊聊安全和机器人领域的亮点。\n\n1. **细粒度自我背书提升事实性和推理（Fine-Grained Self-Endorsement Improves Factuality and Reasoning）**  \n   作者包括 Baolin Peng 等。这篇论文提出一种细粒度自我背书框架，用于减轻 LLM 在生成任务中的幻觉问题。通过比较多个样本响应来改善事实性，在长文本生成任务中表现出色。主要贡献：显著降低幻觉率（如在 Biographies 数据集上提升事实性），并证明该方法适用于小型开源 LLM，扩展了 LLM 在 TriviaQA 和 GSM8K 等任务的应用。\n\n2. **AgentOhana: 设计统一的代理数据和训练管道（AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning）**  \n   作者来自 SalesforceAIResearch，包括 Caiming Xiong。这篇论文引入 AgentOhana 框架，聚合多环境轨迹数据并标准化训练管道，支持 LLM 驱动的代理学习。主要发现：显著提升代理在各种任务中的性能（如 xLAM-v0.1 模型在基准测试中表现优异），并开源代码，促进 LLM 代理的实际部署。\n\n3. **Genie: 生成式交互环境（Genie: Generative Interactive Environments）**  \n   作者包括 Jake Bruce 和 Nando de Freitas（DeepMind 相关）。这篇 ICML 2024 论文提出一个无监督训练的生成模型，能从互联网视频创建可交互环境，支持文本、图像等提示。主要贡献：模型能生成多样化、可控的虚拟世界，并应用于强化学习代理，展示了 LLM 在无标签数据上的潜力。\n\n4. **深度网络总是泛化且原因在此（Deep Networks Always Grok and Here is Why）**  \n   作者包括 Richard Baraniuk。这篇 ICML 2024 论文分析了深度神经网络的延迟泛化（Grokking）现象，通过局部复杂度度量解释了训练后的相变过程。主要发现：在 CIFAR10 和 Imagenette 上，网络在训练后迁移线性区域，提升泛化；还引入延迟鲁棒性，证明网络能更好地处理对抗样本。\n\n5. **均匀安全强化学习：针对多约束安全关键应用的客观抑制（Uniformly Safe RL with Objective Suppression for Multi-Constraint Safety-Critical Applications）**  \n   作者包括 Animesh Garg。这篇论文提出 UCMDP 模型和 Objective Suppression 方法，用于安全 RL。主要贡献：在自动驾驶场景中，结合现有算法减少约束违反，同时保持任务奖励；实验显示在真实车队数据上表现优异，提升了安全 RL 的实际应用。\n\n6. **BEAST: 快速对抗攻击语言模型（Fast Adversarial Attacks on Language Models In One GPU Minute）**  \n   作者包括 Soheil Feizi。这篇论文引入 BEAST 攻击框架，使用 beam search 快速生成对抗样本。主要发现：在 Vicuna-7B 上，攻击成功率高达 89%，远超基线；还展示了攻击在触发幻觉和隐私泄露方面的潜力，强调了 LLM 安全性的紧迫性。\n\n7. **自检索：基于单一 LLM 的端到端信息检索（Self-Retrieval: End-to-End Information Retrieval with One Large Language Model）**  \n   作者包括 Le Sun。这篇 NeurIPS 2024 论文提出自检索框架，让 LLM 内部化语料库并进行检索和重排。主要贡献：显著提升检索效率和准确性（如在下游任务中超越传统方法），展示了 LLM 在信息检索中的潜力。\n\n其他相关论文，如机器人领域的 \"RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation\"，提出交互式场景图构建，提升机器人操作；以及 \"Foundation Policies with Hilbert Representations\"，探索无监督策略学习，我这里快速提一下，它们在 ICML 2024 上有亮点，但细节较技术化，不展开。\n\n### 快速掠过其他论文\n今天还有很多论文，如 KIEval（评估 LLM 的理论思维能力）、ToMBench（LLM 理论思维基准）、以及一些生物医学和计算机视觉主题（如图像生成和实体链接），这些在特定领域有贡献，但影响力较小或较为专业。例如，\"VISREAS: Complex Visual Reasoning with Unanswerable Questions\" 探讨视觉问答中的问题可答性，贡献在于构建新数据集；\"Morphological Symmetries in Robotics\" 分析机器人形态对称性，提升建模效率。这些论文的核心术语如 GAN、RL 和 LLM 被保留，但由于篇幅有限，我只简要提及它们的存在，而不深入讨论。\n\n总之，今天的 arXiv 快报展示了 AI 领域的快速进展，特别是 LLM 和代理的优化。如果您对某个主题感兴趣，可以关注这些关键论文！（约800字）",
  "papers": [
    {
      "arxiv_id": "2402.15650v3",
      "title": "Uniformly Safe RL with Objective Suppression for Multi-Constraint Safety-Critical Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Zihan Zhou",
        "Jonathan Booher",
        "Khashayar Rohanimanesh",
        "Wei Liu",
        "Aleksandr Petiushko",
        "Animesh Garg"
      ],
      "abstract": "Safe reinforcement learning tasks are a challenging domain despite being very\ncommon in the real world. The widely adopted CMDP model constrains the risks in\nexpectation, which makes room for dangerous behaviors in long-tail states. In\nsafety-critical domains, such behaviors could lead to disastrous outcomes. To\naddress this issue, we first describe the problem with a stronger Uniformly\nConstrained MDP (UCMDP) model where we impose constraints on all reachable\nstates; we then propose Objective Suppression, a novel method that adaptively\nsuppresses the task reward maximizing objectives according to a safety critic,\nas a solution to the Lagrangian dual of a UCMDP. We benchmark Objective\nSuppression in two multi-constraint safety domains, including an autonomous\ndriving domain where any incorrect behavior can lead to disastrous\nconsequences. On the driving domain, we evaluate on open source and proprietary\ndata and evaluate transfer to a real autonomous fleet. Empirically, we\ndemonstrate that our proposed method, when combined with existing safe RL\nalgorithms, can match the task reward achieved by baselines with significantly\nfewer constraint violations.",
      "tldr_zh": "该论文针对安全强化学习（Safe RL）的挑战，提出Uniformly Constrained MDP (UCMDP) 模型，以在所有可达状态上施加约束，解决传统Constrained MDP (CMDP) 在长尾状态下可能导致危险行为的缺陷。作者引入Objective Suppression 方法，该方法通过安全批评者（safety critic）自适应抑制任务奖励最大化目标，作为UCMDP的Lagrangian dual解决方案，适用于多约束的安全关键应用。实验在自动驾驶等领域进行验证，结果显示，该方法与现有Safe RL算法结合，能在显著减少约束违反的情况下，匹配基线任务奖励水平，并成功转移到真实自主驾驶车队。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15650v3",
      "published_date": "2024-02-23 23:22:06 UTC",
      "updated_date": "2024-08-28 20:13:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:19:18.584604"
    },
    {
      "arxiv_id": "2402.15631v1",
      "title": "Fine-Grained Self-Endorsement Improves Factuality and Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Ante Wang",
        "Linfeng Song",
        "Baolin Peng",
        "Ye Tian",
        "Lifeng Jin",
        "Haitao Mi",
        "Jinsong Su",
        "Dong Yu"
      ],
      "abstract": "This work studies improving large language model (LLM) generations at\ninference time by mitigating fact-conflicting hallucinations. Particularly, we\npropose a self-endorsement framework that leverages the fine-grained fact-level\ncomparisons across multiple sampled responses. Compared with prior ensemble\nmethods (Wang et al., 2022;Chen et al., 2023)) that perform response-level\nselection, our approach can better alleviate hallucinations, especially for\nlongform generation tasks. Our approach can broadly benefit smaller and\nopen-source LLMs as it mainly conducts simple content-based comparisons.\nExperiments on Biographies show that our method can effectively improve the\nfactuality of generations with simple and intuitive prompts across different\nscales of LLMs. Besides, comprehensive analyses on TriviaQA and GSM8K\ndemonstrate the potential of self-endorsement for broader application.",
      "tldr_zh": "本研究提出了一种细粒度 self-endorsement 框架，通过多个采样响应之间的事实级比较，来缓解大型语言模型 (LLM) 生成中的事实冲突幻觉。该方法相较于之前的 ensemble methods，能够更好地减少幻觉，尤其适用于长形式生成任务，并适合较小和开源 LLM 的简单内容比较。实验结果显示，在 Biographies 数据集上，该框架使用简单提示显著提高了 LLM 不同规模的生成真实性。此外，在 TriviaQA 和 GSM8K 的全面分析中，自证机制展示了在更广泛应用场景下的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15631v1",
      "published_date": "2024-02-23 22:24:40 UTC",
      "updated_date": "2024-02-23 22:24:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:19:31.033989"
    },
    {
      "arxiv_id": "2403.00803v1",
      "title": "LiMAML: Personalization of Deep Recommender Models via Meta Learning",
      "title_zh": "LiMAML：通过元学习对深度推荐模型的个性化",
      "authors": [
        "Ruofan Wang",
        "Prakruthi Prabhakar",
        "Gaurav Srivastava",
        "Tianqi Wang",
        "Zeinab S. Jalali",
        "Varun Bharill",
        "Yunbo Ouyang",
        "Aastha Nigam",
        "Divya Venugopalan",
        "Aman Gupta",
        "Fedor Borisyuk",
        "Sathiya Keerthi",
        "Ajith Muralidharan"
      ],
      "abstract": "In the realm of recommender systems, the ubiquitous adoption of deep neural\nnetworks has emerged as a dominant paradigm for modeling diverse business\nobjectives. As user bases continue to expand, the necessity of personalization\nand frequent model updates have assumed paramount significance to ensure the\ndelivery of relevant and refreshed experiences to a diverse array of members.\nIn this work, we introduce an innovative meta-learning solution tailored to the\npersonalization of models for individual members and other entities, coupled\nwith the frequent updates based on the latest user interaction signals.\nSpecifically, we leverage the Model-Agnostic Meta Learning (MAML) algorithm to\nadapt per-task sub-networks using recent user interaction data. Given the near\ninfeasibility of productionizing original MAML-based models in online\nrecommendation systems, we propose an efficient strategy to operationalize\nmeta-learned sub-networks in production, which involves transforming them into\nfixed-sized vectors, termed meta embeddings, thereby enabling the seamless\ndeployment of models with hundreds of billions of parameters for online\nserving. Through extensive experimentation on production data drawn from\nvarious applications at LinkedIn, we demonstrate that the proposed solution\nconsistently outperforms the baseline models of those applications, including\nstrong baselines such as using wide-and-deep ID based personalization approach.\nOur approach has enabled the deployment of a range of highly personalized AI\nmodels across diverse LinkedIn applications, leading to substantial\nimprovements in business metrics as well as refreshed experience for our\nmembers.",
      "tldr_zh": "这篇论文提出了 LiMAML，一种基于 Meta Learning 的方法，用于深度推荐模型的个性化，以适应用户交互数据并实现频繁模型更新。具体而言，该方法利用 Model-Agnostic Meta Learning (MAML) 算法来适应 per-task sub-networks，并通过将这些子网络转化为固定大小的 meta embeddings，实现高效的生产部署，从而支持处理数百亿参数的在线系统。在 LinkedIn 的实际应用实验中，LiMAML 优于基线模型（如 wide-and-deep ID based personalization），显著提升了业务指标并改善了用户体验。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00803v1",
      "published_date": "2024-02-23 22:06:36 UTC",
      "updated_date": "2024-02-23 22:06:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:19:43.179711"
    },
    {
      "arxiv_id": "2402.15625v1",
      "title": "Learning Cyclic Causal Models from Incomplete Data",
      "title_zh": "从不完整数据中学习循环因果模型",
      "authors": [
        "Muralikrishnna G. Sethuraman",
        "Faramarz Fekri"
      ],
      "abstract": "Causal learning is a fundamental problem in statistics and science, offering\ninsights into predicting the effects of unseen treatments on a system. Despite\nrecent advances in this topic, most existing causal discovery algorithms\noperate under two key assumptions: (i) the underlying graph is acyclic, and\n(ii) the available data is complete. These assumptions can be problematic as\nmany real-world systems contain feedback loops (e.g., biological systems), and\npractical scenarios frequently involve missing data. In this work, we propose a\nnovel framework, named MissNODAGS, for learning cyclic causal graphs from\npartially missing data. Under the additive noise model, MissNODAGS learns the\ncausal graph by alternating between imputing the missing data and maximizing\nthe expected log-likelihood of the visible part of the data in each training\nstep, following the principles of the expectation-maximization (EM) framework.\nThrough synthetic experiments and real-world single-cell perturbation data, we\ndemonstrate improved performance when compared to using state-of-the-art\nimputation techniques followed by causal learning on partially missing\ninterventional data.",
      "tldr_zh": "该研究解决了因果学习中的关键挑战，即现有算法假设因果图为无环（acyclic）和数据完整（complete），但现实中常存在反馈循环（如生物系统）和缺失数据。作者提出了一种新框架MissNODAGS，用于从部分缺失数据中学习循环因果模型（cyclic causal graphs），在additive noise model下，通过expectation-maximization (EM)框架交替填充缺失数据并最大化可见数据的期望对数似然。实验结果显示，MissNODAGS在合成数据和真实世界单细胞扰动数据上，比使用先进填充技术后进行因果学习的方法表现出色，显著提升了性能。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15625v1",
      "published_date": "2024-02-23 22:03:12 UTC",
      "updated_date": "2024-02-23 22:03:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:19:55.392879"
    },
    {
      "arxiv_id": "2403.00802v1",
      "title": "Towards a Theoretical Understanding of Two-Stage Recommender Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Amit Kumar Jaiswal"
      ],
      "abstract": "Production-grade recommender systems rely heavily on a large-scale corpus\nused by online media services, including Netflix, Pinterest, and Amazon. These\nsystems enrich recommendations by learning users' and items' embeddings\nprojected in a low-dimensional space with two-stage models (two deep neural\nnetworks), which facilitate their embedding constructs to predict users'\nfeedback associated with items. Despite its popularity for recommendations, its\ntheoretical behaviors remain comprehensively unexplored. We study the\nasymptotic behaviors of the two-stage recommender that entail a strong\nconvergence to the optimal recommender system. We establish certain theoretical\nproperties and statistical assurance of the two-stage recommender. In addition\nto asymptotic behaviors, we demonstrate that the two-stage recommender system\nattains faster convergence by relying on the intrinsic dimensions of the input\nfeatures. Finally, we show numerically that the two-stage recommender enables\nencapsulating the impacts of items' and users' attributes on ratings, resulting\nin better performance compared to existing methods conducted using synthetic\nand real-world data experiments.",
      "tldr_zh": "这篇论文探讨了two-stage recommender systems的理论基础，分析其渐近行为（asymptotic behaviors）和收敛性，以理解这些系统如何通过学习用户和物品的embeddings来预测用户反馈。研究建立了two-stage recommender的理论属性和统计保证，并证明其依赖输入特征的intrinsic dimensions可实现更快收敛。实验结果显示，该系统在合成和真实数据上表现优于现有方法，能更好地封装用户和物品属性的影响，从而提升推荐性能。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "18 pages (including references and appendix), 1 figure, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2403.00802v1",
      "published_date": "2024-02-23 21:11:55 UTC",
      "updated_date": "2024-02-23 21:11:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:20:07.071011"
    },
    {
      "arxiv_id": "2402.15591v1",
      "title": "RecWizard: A Toolkit for Conversational Recommendation with Modular, Portable Models and Interactive User Interface",
      "title_zh": "翻译失败",
      "authors": [
        "Zeyuan Zhang",
        "Tanmay Laud",
        "Zihang He",
        "Xiaojie Chen",
        "Xinshuang Liu",
        "Zhouhang Xie",
        "Julian McAuley",
        "Zhankui He"
      ],
      "abstract": "We present a new Python toolkit called RecWizard for Conversational\nRecommender Systems (CRS). RecWizard offers support for development of models\nand interactive user interface, drawing from the best practices of the\nHuggingface ecosystems. CRS with RecWizard are modular, portable, interactive\nand Large Language Models (LLMs)-friendly, to streamline the learning process\nand reduce the additional effort for CRS research. For more comprehensive\ninformation about RecWizard, please check our GitHub\nhttps://github.com/McAuley-Lab/RecWizard.",
      "tldr_zh": "研究者推出了 RecWizard，这是一个 Python 工具包，旨在简化对话推荐系统 (Conversational Recommender Systems, CRS) 的开发，支持模块化、可移植模型和交互式用户界面。RecWizard 借鉴 Huggingface 生态系统的最佳实践，使 CRS 模型变得交互式且 Large Language Models (LLMs)-friendly，从而减少研究过程中的额外努力。总体而言，该工具包有助于加速 CRS 研究和应用，提供了一个高效的开发框架。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "AAAI'24 Demo Track",
      "pdf_url": "http://arxiv.org/pdf/2402.15591v1",
      "published_date": "2024-02-23 20:16:13 UTC",
      "updated_date": "2024-02-23 20:16:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:20:19.431725"
    },
    {
      "arxiv_id": "2402.15589v2",
      "title": "LLMs as Meta-Reviewers' Assistants: A Case Study",
      "title_zh": "翻译失败",
      "authors": [
        "Eftekhar Hossain",
        "Sanjeev Kumar Sinha",
        "Naman Bansal",
        "Alex Knipper",
        "Souvika Sarkar",
        "John Salvador",
        "Yash Mahajan",
        "Sri Guttikonda",
        "Mousumi Akter",
        "Md. Mahadi Hassan",
        "Matthew Freestone",
        "Matthew C. Williams Jr.",
        "Dongji Feng",
        "Santu Karmaker"
      ],
      "abstract": "One of the most important yet onerous tasks in the academic peer-reviewing\nprocess is composing meta-reviews, which involves assimilating diverse opinions\nfrom multiple expert peers, formulating one's self-judgment as a senior expert,\nand then summarizing all these perspectives into a concise holistic overview to\nmake an overall recommendation. This process is time-consuming and can be\ncompromised by human factors like fatigue, inconsistency, missing tiny details,\netc. Given the latest major developments in Large Language Models (LLMs), it is\nvery compelling to rigorously study whether LLMs can help metareviewers perform\nthis important task better. In this paper, we perform a case study with three\npopular LLMs, i.e., GPT-3.5, LLaMA2, and PaLM2, to assist meta-reviewers in\nbetter comprehending multiple experts perspectives by generating a controlled\nmulti-perspective summary (MPS) of their opinions. To achieve this, we prompt\nthree LLMs with different types/levels of prompts based on the recently\nproposed TELeR taxonomy. Finally, we perform a detailed qualitative study of\nthe MPSs generated by the LLMs and report our findings.",
      "tldr_zh": "这篇论文探讨了大型语言模型（LLMs）作为元评论助手的作用，通过一个案例研究评估 LLMs 是否能帮助元评论者更好地整合多位专家意见。研究方法涉及使用 GPT-3.5、LLaMA2 和 PaLM2 生成受控的多视角摘要（MPS），并基于 TELeR 分类法的不同提示类型进行实验。结果显示，LLMs 能有效辅助理解专家视角，但也突出了潜在局限，如细节遗漏或一致性问题，通过定性分析报告了这些发现。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.NE",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NAACL 2025, 41 pages",
      "pdf_url": "http://arxiv.org/pdf/2402.15589v2",
      "published_date": "2024-02-23 20:14:16 UTC",
      "updated_date": "2025-02-08 21:23:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:20:31.418793"
    },
    {
      "arxiv_id": "2402.15572v1",
      "title": "Improving Explainable Object-induced Model through Uncertainty for Automated Vehicles",
      "title_zh": "翻译失败",
      "authors": [
        "Shihong Ling",
        "Yue Wan",
        "Xiaowei Jia",
        "Na Du"
      ],
      "abstract": "The rapid evolution of automated vehicles (AVs) has the potential to provide\nsafer, more efficient, and comfortable travel options. However, these systems\nface challenges regarding reliability in complex driving scenarios. Recent\nexplainable AV architectures neglect crucial information related to inherent\nuncertainties while providing explanations for actions. To overcome such\nchallenges, our study builds upon the \"object-induced\" model approach that\nprioritizes the role of objects in scenes for decision-making and integrates\nuncertainty assessment into the decision-making process using an evidential\ndeep learning paradigm with a Beta prior. Additionally, we explore several\nadvanced training strategies guided by uncertainty, including\nuncertainty-guided data reweighting and augmentation. Leveraging the BDD-OIA\ndataset, our findings underscore that the model, through these enhancements,\nnot only offers a clearer comprehension of AV decisions and their underlying\nreasoning but also surpasses existing baselines across a broad range of\nscenarios.",
      "tldr_zh": "本研究针对自动车辆（AVs）在复杂驾驶场景中的可靠性挑战，改进了 object-induced 模型，通过整合不确定性评估来提升决策解释性。具体方法包括采用 evidential deep learning 范式结合 Beta prior，以及探索不确定性引导的训练策略，如数据再加权和增强。实验结果显示，该模型在 BDD-OIA 数据集上提供了更清晰的 AV 决策理解，并在多种场景中超越了现有基线。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "In Proceedings of the 2024 ACM / IEEE International Conference on\n  Human-Robot Interaction (HRI '24), March 11--14, 2024, Boulder, CO, USA. ACM,\n  New York, NY, USA, 9 pages",
      "pdf_url": "http://arxiv.org/pdf/2402.15572v1",
      "published_date": "2024-02-23 19:14:57 UTC",
      "updated_date": "2024-02-23 19:14:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:20:43.313591"
    },
    {
      "arxiv_id": "2402.15570v1",
      "title": "Fast Adversarial Attacks on Language Models In One GPU Minute",
      "title_zh": "翻译失败",
      "authors": [
        "Vinu Sankar Sadasivan",
        "Shoumik Saha",
        "Gaurang Sriramanan",
        "Priyatham Kattakinda",
        "Atoosa Chegini",
        "Soheil Feizi"
      ],
      "abstract": "In this paper, we introduce a novel class of fast, beam search-based\nadversarial attack (BEAST) for Language Models (LMs). BEAST employs\ninterpretable parameters, enabling attackers to balance between attack speed,\nsuccess rate, and the readability of adversarial prompts. The computational\nefficiency of BEAST facilitates us to investigate its applications on LMs for\njailbreaking, eliciting hallucinations, and privacy attacks. Our gradient-free\ntargeted attack can jailbreak aligned LMs with high attack success rates within\none minute. For instance, BEAST can jailbreak Vicuna-7B-v1.5 under one minute\nwith a success rate of 89% when compared to a gradient-based baseline that\ntakes over an hour to achieve 70% success rate using a single Nvidia RTX A6000\n48GB GPU. Additionally, we discover a unique outcome wherein our untargeted\nattack induces hallucinations in LM chatbots. Through human evaluations, we\nfind that our untargeted attack causes Vicuna-7B-v1.5 to produce ~15% more\nincorrect outputs when compared to LM outputs in the absence of our attack. We\nalso learn that 22% of the time, BEAST causes Vicuna to generate outputs that\nare not relevant to the original prompt. Further, we use BEAST to generate\nadversarial prompts in a few seconds that can boost the performance of existing\nmembership inference attacks for LMs. We believe that our fast attack, BEAST,\nhas the potential to accelerate research in LM security and privacy. Our\ncodebase is publicly available at https://github.com/vinusankars/BEAST.",
      "tldr_zh": "本文提出了一种快速的基于 beam search 的对抗攻击方法 BEAST，用于语言模型（LMs）。BEAST 采用可解释参数，允许攻击者平衡攻击速度、成功率和对抗提示的可读性，实现梯度-free 的高效攻击，例如在单 GPU 上 1 分钟内以 89% 成功率越狱 Vicuna-7B-v1.5。实验结果显示，该攻击能引发 LMs 的幻觉（如 Vicuna-7B-v1.5 多 15% 错误输出）和无关响应，并提升 membership inference attacks 的性能。BEAST 的高效性有望加速语言模型安全和隐私研究，其代码已公开可用。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15570v1",
      "published_date": "2024-02-23 19:12:53 UTC",
      "updated_date": "2024-02-23 19:12:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:20:56.156836"
    },
    {
      "arxiv_id": "2402.15567v2",
      "title": "Foundation Policies with Hilbert Representations",
      "title_zh": "基于希尔伯特表示的基础策略",
      "authors": [
        "Seohong Park",
        "Tobias Kreiman",
        "Sergey Levine"
      ],
      "abstract": "Unsupervised and self-supervised objectives, such as next token prediction,\nhave enabled pre-training generalist models from large amounts of unlabeled\ndata. In reinforcement learning (RL), however, finding a truly general and\nscalable unsupervised pre-training objective for generalist policies from\noffline data remains a major open question. While a number of methods have been\nproposed to enable generic self-supervised RL, based on principles such as\ngoal-conditioned RL, behavioral cloning, and unsupervised skill learning, such\nmethods remain limited in terms of either the diversity of the discovered\nbehaviors, the need for high-quality demonstration data, or the lack of a clear\nadaptation mechanism for downstream tasks. In this work, we propose a novel\nunsupervised framework to pre-train generalist policies that capture diverse,\noptimal, long-horizon behaviors from unlabeled offline data such that they can\nbe quickly adapted to any arbitrary new tasks in a zero-shot manner. Our key\ninsight is to learn a structured representation that preserves the temporal\nstructure of the underlying environment, and then to span this learned latent\nspace with directional movements, which enables various zero-shot policy\n\"prompting\" schemes for downstream tasks. Through our experiments on simulated\nrobotic locomotion and manipulation benchmarks, we show that our unsupervised\npolicies can solve goal-conditioned and general RL tasks in a zero-shot\nfashion, even often outperforming prior methods designed specifically for each\nsetting. Our code and videos are available at\nhttps://seohong.me/projects/hilp/.",
      "tldr_zh": "该论文针对强化学习（RL）中的无监督预训练问题，提出了一种新框架，利用 Hilbert Representations 学习环境的时间结构，从而从无标签离线数据中捕获多样、最优的长远行为。框架的关键在于通过潜在空间的定向运动，实现策略的零-shot 适应机制，使其能快速应用于任意新任务，而无需额外训练。在模拟机器人运动和操作基准实验中，该方法在零-shot 条件下解决目标条件和一般 RL 任务时，往往优于专门设计的方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.15567v2",
      "published_date": "2024-02-23 19:09:10 UTC",
      "updated_date": "2024-05-26 17:44:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:21:08.454837"
    },
    {
      "arxiv_id": "2402.15555v2",
      "title": "Deep Networks Always Grok and Here is Why",
      "title_zh": "翻译失败",
      "authors": [
        "Ahmed Imtiaz Humayun",
        "Randall Balestriero",
        "Richard Baraniuk"
      ],
      "abstract": "Grokking, or delayed generalization, is a phenomenon where generalization in\na deep neural network (DNN) occurs long after achieving near zero training\nerror. Previous studies have reported the occurrence of grokking in specific\ncontrolled settings, such as DNNs initialized with large-norm parameters or\ntransformers trained on algorithmic datasets. We demonstrate that grokking is\nactually much more widespread and materializes in a wide range of practical\nsettings, such as training of a convolutional neural network (CNN) on CIFAR10\nor a Resnet on Imagenette. We introduce the new concept of delayed robustness,\nwhereby a DNN groks adversarial examples and becomes robust, long after\ninterpolation and/or generalization. We develop an analytical explanation for\nthe emergence of both delayed generalization and delayed robustness based on\nthe local complexity of a DNN's input-output mapping. Our local complexity\nmeasures the density of so-called linear regions (aka, spline partition\nregions) that tile the DNN input space and serves as a utile progress measure\nfor training. We provide the first evidence that, for classification problems,\nthe linear regions undergo a phase transition during training whereafter they\nmigrate away from the training samples (making the DNN mapping smoother there)\nand towards the decision boundary (making the DNN mapping less smooth there).\nGrokking occurs post phase transition as a robust partition of the input space\nthanks to the linearization of the DNN mapping around the training points.\nWebsite: https://bit.ly/grok-adversarial",
      "tldr_zh": "这篇论文证明了深度神经网络（DNN）在广泛的实际场景中总是会出现 Grokking 现象，即在训练误差接近零后，泛化性能延迟发生，例如在 CNN 训练于 CIFAR10 或 ResNet 训练于 Imagenette 的情况下。作者引入了 Delayed Robustness 的新概念，描述网络在泛化后很久才对对抗样本变得鲁棒，并基于 DNN 输入-输出映射的 local complexity 提供了分析解释。研究发现，训练过程中 linear regions 发生 phase transition，从训练样本附近迁移到决策边界，导致映射在训练点周围线性化，从而实现 Grokking 的鲁棒输入空间分区。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2024. Website: https://bit.ly/grok-adversarial. Pages 24,\n  Figures 36",
      "pdf_url": "http://arxiv.org/pdf/2402.15555v2",
      "published_date": "2024-02-23 18:59:31 UTC",
      "updated_date": "2024-06-06 18:33:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:21:21.541671"
    },
    {
      "arxiv_id": "2402.15506v4",
      "title": "AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Jianguo Zhang",
        "Tian Lan",
        "Rithesh Murthy",
        "Zhiwei Liu",
        "Weiran Yao",
        "Ming Zhu",
        "Juntao Tan",
        "Thai Hoang",
        "Zuxin Liu",
        "Liangwei Yang",
        "Yihao Feng",
        "Shirley Kokane",
        "Tulika Awalgaonkar",
        "Juan Carlos Niebles",
        "Silvio Savarese",
        "Shelby Heinecke",
        "Huan Wang",
        "Caiming Xiong"
      ],
      "abstract": "Autonomous agents powered by large language models (LLMs) have garnered\nsignificant research attention. However, fully harnessing the potential of LLMs\nfor agent-based tasks presents inherent challenges due to the heterogeneous\nnature of diverse data sources featuring multi-turn trajectories. In this\npaper, we introduce \\textbf{AgentOhana} as a comprehensive solution to address\nthese challenges. \\textit{AgentOhana} aggregates agent trajectories from\ndistinct environments, spanning a wide array of scenarios. It meticulously\nstandardizes and unifies these trajectories into a consistent format,\nstreamlining the creation of a generic data loader optimized for agent\ntraining. Leveraging the data unification, our training pipeline maintains\nequilibrium across different data sources and preserves independent randomness\nacross devices during dataset partitioning and model training. Additionally, we\npresent \\textbf{xLAM-v0.1}, a large action model tailored for AI agents, which\ndemonstrates exceptional performance across various benchmarks. Begin the\nexploration at \\url{https://github.com/SalesforceAIResearch/xLAM}.",
      "tldr_zh": "本论文提出 AgentOhana，这是一个统一的數據和訓練管道，旨在解决大型语言模型 (LLMs) 在代理任务中面临的异构数据来源和多轮轨迹挑战。AgentOhana 通过从不同环境聚合代理轨迹，进行标准化和统一格式处理，创建通用的数据加载器，并确保训练管道在数据集分区和模型训练中保持数据来源平衡及独立随机性。作为主要贡献，该框架支持开发 xLAM-v0.1，一种专为 AI 代理设计的大型行动模型，在各种基准测试中表现出色。更多细节可参考 GitHub 仓库 https://github.com/SalesforceAIResearch/xLAM。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Add GitHub repo link at\n  \\url{https://github.com/SalesforceAIResearch/xLAM} and HuggingFace model link\n  at \\url{https://huggingface.co/Salesforce/xLAM-v0.1-r}",
      "pdf_url": "http://arxiv.org/pdf/2402.15506v4",
      "published_date": "2024-02-23 18:56:26 UTC",
      "updated_date": "2024-11-09 00:28:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:21:33.817931"
    },
    {
      "arxiv_id": "2402.15505v1",
      "title": "Co-Supervised Learning: Improving Weak-to-Strong Generalization with Hierarchical Mixture of Experts",
      "title_zh": "翻译失败",
      "authors": [
        "Yuejiang Liu",
        "Alexandre Alahi"
      ],
      "abstract": "Steering the behavior of a strong model pre-trained on internet-scale data\ncan be difficult due to the scarcity of competent supervisors. Recent studies\nreveal that, despite supervisory noises, a strong student model may surpass its\nweak teacher when fine-tuned on specific objectives. Yet, the effectiveness of\nsuch weak-to-strong generalization remains limited, especially in the presence\nof large capability gaps. In this paper, we propose to address this challenge\nby harnessing a diverse set of specialized teachers, instead of a single\ngeneralist one, that collectively supervises the strong student. Our approach\nresembles the classical hierarchical mixture of experts, with two components\ntailored for co-supervision: (i) we progressively alternate student training\nand teacher assignment, leveraging the growth of the strong student to identify\nplausible supervisions; (ii) we conservatively enforce teacher-student and\nlocal-global consistency, leveraging their dependencies to reject potential\nannotation noises. We validate the proposed method through visual recognition\ntasks on the OpenAI weak-to-strong benchmark and additional multi-domain\ndatasets. Our code is available at \\url{https://github.com/yuejiangliu/csl}.",
      "tldr_zh": "该论文提出 Co-Supervised Learning 方法，通过 Hierarchical Mixture of Experts 框架来提升 weak-to-strong generalization，利用一组多样化专业化老师共同监督强学生，以克服单一老师监督的局限性。方法包括两个关键组件：逐步交替学生训练和老师分配，利用强学生的成长识别可靠监督；以及保守强制执行 teacher-student 和 local-global 一致性，以过滤标注噪声。在 OpenAI weak-to-strong 基准及多领域视觉识别任务上验证，该方法有效改善了模型泛化性能，并提供了开源代码。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2402.15505v1",
      "published_date": "2024-02-23 18:56:11 UTC",
      "updated_date": "2024-02-23 18:56:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:21:45.042220"
    },
    {
      "arxiv_id": "2402.15504v1",
      "title": "Gen4Gen: Generative Data Pipeline for Generative Multi-Concept Composition",
      "title_zh": "Gen4Gen：生成式数据管道用于生成式多概念合成",
      "authors": [
        "Chun-Hsiao Yeh",
        "Ta-Ying Cheng",
        "He-Yen Hsieh",
        "Chuan-En Lin",
        "Yi Ma",
        "Andrew Markham",
        "Niki Trigoni",
        "H. T. Kung",
        "Yubei Chen"
      ],
      "abstract": "Recent text-to-image diffusion models are able to learn and synthesize images\ncontaining novel, personalized concepts (e.g., their own pets or specific\nitems) with just a few examples for training. This paper tackles two\ninterconnected issues within this realm of personalizing text-to-image\ndiffusion models. First, current personalization techniques fail to reliably\nextend to multiple concepts -- we hypothesize this to be due to the mismatch\nbetween complex scenes and simple text descriptions in the pre-training dataset\n(e.g., LAION). Second, given an image containing multiple personalized\nconcepts, there lacks a holistic metric that evaluates performance on not just\nthe degree of resemblance of personalized concepts, but also whether all\nconcepts are present in the image and whether the image accurately reflects the\noverall text description. To address these issues, we introduce Gen4Gen, a\nsemi-automated dataset creation pipeline utilizing generative models to combine\npersonalized concepts into complex compositions along with text-descriptions.\nUsing this, we create a dataset called MyCanvas, that can be used to benchmark\nthe task of multi-concept personalization. In addition, we design a\ncomprehensive metric comprising two scores (CP-CLIP and TI-CLIP) for better\nquantifying the performance of multi-concept, personalized text-to-image\ndiffusion methods. We provide a simple baseline built on top of Custom\nDiffusion with empirical prompting strategies for future researchers to\nevaluate on MyCanvas. We show that by improving data quality and prompting\nstrategies, we can significantly increase multi-concept personalized image\ngeneration quality, without requiring any modifications to model architecture\nor training algorithms.",
      "tldr_zh": "本论文提出 Gen4Gen，一种生成数据管道，用于解决文本到图像扩散模型在多概念个性化中的问题，包括概念扩展失败和缺乏全面评估指标。Gen4Gen 通过半自动方式利用生成模型结合个性化概念创建复杂组合，并生成 MyCanvas 数据集，以基准测试多概念个性化任务；同时，设计了 CP-CLIP 和 TI-CLIP 指标来量化概念相似度、完整性和整体描述准确性。实验结果显示，通过优化数据质量和提示策略，基于 Custom Diffusion 的基线方法显著提高了图像生成质量，而无需修改模型架构或训练算法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint; Project Page: https://danielchyeh.github.io/Gen4Gen/",
      "pdf_url": "http://arxiv.org/pdf/2402.15504v1",
      "published_date": "2024-02-23 18:55:09 UTC",
      "updated_date": "2024-02-23 18:55:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:21:59.167894"
    },
    {
      "arxiv_id": "2403.00801v2",
      "title": "Self-Retrieval: End-to-End Information Retrieval with One Large Language Model",
      "title_zh": "Self-Retrieval：端到端信息检索，使用一个大型语言模型",
      "authors": [
        "Qiaoyu Tang",
        "Jiawei Chen",
        "Zhuoqun Li",
        "Bowen Yu",
        "Yaojie Lu",
        "Cheng Fu",
        "Haiyang Yu",
        "Hongyu Lin",
        "Fei Huang",
        "Ben He",
        "Xianpei Han",
        "Le Sun",
        "Yongbin Li"
      ],
      "abstract": "The rise of large language models (LLMs) has significantly transformed both\nthe construction and application of information retrieval (IR) systems.\nHowever, current interactions between IR systems and LLMs remain limited, with\nLLMs merely serving as part of components within IR systems, and IR systems\nbeing constructed independently of LLMs. This separated architecture restricts\nknowledge sharing and deep collaboration between them. In this paper, we\nintroduce Self-Retrieval, a novel end-to-end LLM-driven information retrieval\narchitecture. Self-Retrieval unifies all essential IR functions within a single\nLLM, leveraging the inherent capabilities of LLMs throughout the IR process.\nSpecifically, Self-Retrieval internalizes the retrieval corpus through\nself-supervised learning, transforms the retrieval process into sequential\npassage generation, and performs relevance assessment for reranking.\nExperimental results demonstrate that Self-Retrieval not only outperforms\nexisting retrieval approaches by a significant margin, but also substantially\nenhances the performance of LLM-driven downstream applications like\nretrieval-augmented generation.",
      "tldr_zh": "本研究提出Self-Retrieval，一种端到端的LLM驱动信息检索架构，将所有关键IR功能（如检索、生成和重新排序）统一到一个大型语言模型中，以克服传统IR系统与LLMs分离的局限性。具体而言，该方法通过self-supervised learning内化检索语料库，将检索过程转化为顺序段落生成，并进行相关性评估以优化结果。实验显示，Self-Retrieval显著优于现有检索方法，并在下游应用如retrieval-augmented generation中大幅提升性能。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "NeurIPS 2024 Camera-ready Version. Code:\n  https://github.com/icip-cas/SelfRetrieval",
      "pdf_url": "http://arxiv.org/pdf/2403.00801v2",
      "published_date": "2024-02-23 18:45:35 UTC",
      "updated_date": "2024-11-04 03:07:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:22:07.894066"
    },
    {
      "arxiv_id": "2402.16893v1",
      "title": "The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)",
      "title_zh": "翻译失败",
      "authors": [
        "Shenglai Zeng",
        "Jiankun Zhang",
        "Pengfei He",
        "Yue Xing",
        "Yiding Liu",
        "Han Xu",
        "Jie Ren",
        "Shuaiqiang Wang",
        "Dawei Yin",
        "Yi Chang",
        "Jiliang Tang"
      ],
      "abstract": "Retrieval-augmented generation (RAG) is a powerful technique to facilitate\nlanguage model with proprietary and private data, where data privacy is a\npivotal concern. Whereas extensive research has demonstrated the privacy risks\nof large language models (LLMs), the RAG technique could potentially reshape\nthe inherent behaviors of LLM generation, posing new privacy issues that are\ncurrently under-explored. In this work, we conduct extensive empirical studies\nwith novel attack methods, which demonstrate the vulnerability of RAG systems\non leaking the private retrieval database. Despite the new risk brought by RAG\non the retrieval data, we further reveal that RAG can mitigate the leakage of\nthe LLMs' training data. Overall, we provide new insights in this paper for\nprivacy protection of retrieval-augmented LLMs, which benefit both LLMs and RAG\nsystems builders. Our code is available at\nhttps://github.com/phycholosogy/RAG-privacy.",
      "tldr_zh": "该论文探讨了Retrieval-Augmented Generation (RAG)技术在处理私有数据时的隐私问题，揭示了RAG可能加剧对检索数据库的泄露风险，同时也能缓解Large Language Models (LLMs)的训练数据泄露。研究者通过广泛的实证实验和新型攻击方法，证明了RAG系统的脆弱性，并在多个场景下验证了这些隐私动态。总体上，该工作为RAG和LLMs的隐私保护提供了新洞见，并开源了相关代码以促进进一步研究。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.16893v1",
      "published_date": "2024-02-23 18:35:15 UTC",
      "updated_date": "2024-02-23 18:35:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:22:20.281915"
    },
    {
      "arxiv_id": "2402.15491v2",
      "title": "API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Kinjal Basu",
        "Ibrahim Abdelaziz",
        "Subhajit Chaudhury",
        "Soham Dan",
        "Maxwell Crouse",
        "Asim Munawar",
        "Sadhana Kumaravel",
        "Vinod Muthusamy",
        "Pavan Kapanipathi",
        "Luis A. Lastras"
      ],
      "abstract": "There is a growing need for Large Language Models (LLMs) to effectively use\ntools and external Application Programming Interfaces (APIs) to plan and\ncomplete tasks. As such, there is tremendous interest in methods that can\nacquire sufficient quantities of train and test data that involve calls to\ntools / APIs. Two lines of research have emerged as the predominant strategies\nfor addressing this challenge. The first has focused on synthetic data\ngeneration techniques, while the second has involved curating task-adjacent\ndatasets which can be transformed into API / Tool-based tasks. In this paper,\nwe focus on the task of identifying, curating, and transforming existing\ndatasets and, in turn, introduce API-BLEND, a large corpora for training and\nsystematic testing of tool-augmented LLMs. The datasets mimic real-world\nscenarios involving API-tasks such as API / tool detection, slot filling, and\nsequencing of the detected APIs. We demonstrate the utility of the API-BLEND\ndataset for both training and benchmarking purposes.",
      "tldr_zh": "这篇论文介绍了API-BLEND，这是一个全面的语料库，旨在用于训练和基准测试API LLMs，以提升Large Language Models (LLMs)对工具和外部Application Programming Interfaces (APIs)的使用能力。研究者通过识别、整理和转换现有数据集，模拟真实场景中的API任务，如API/tool检测、槽填充（slot filling）和API序列化（sequencing）。实验结果证明，API-BLEND在训练tool-augmented LLMs和系统测试方面具有显著实用性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at ACL'24-main conference",
      "pdf_url": "http://arxiv.org/pdf/2402.15491v2",
      "published_date": "2024-02-23 18:30:49 UTC",
      "updated_date": "2024-05-20 14:52:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:22:32.467713"
    },
    {
      "arxiv_id": "2402.15487v2",
      "title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation",
      "title_zh": "翻译失败",
      "authors": [
        "Hanxiao Jiang",
        "Binghao Huang",
        "Ruihai Wu",
        "Zhuoran Li",
        "Shubham Garg",
        "Hooshang Nayyeri",
        "Shenlong Wang",
        "Yunzhu Li"
      ],
      "abstract": "We introduce the novel task of interactive scene exploration, wherein robots\nautonomously explore environments and produce an action-conditioned scene graph\n(ACSG) that captures the structure of the underlying environment. The ACSG\naccounts for both low-level information (geometry and semantics) and high-level\ninformation (action-conditioned relationships between different entities) in\nthe scene. To this end, we present the Robotic Exploration (RoboEXP) system,\nwhich incorporates the Large Multimodal Model (LMM) and an explicit memory\ndesign to enhance our system's capabilities. The robot reasons about what and\nhow to explore an object, accumulating new information through the interaction\nprocess and incrementally constructing the ACSG. Leveraging the constructed\nACSG, we illustrate the effectiveness and efficiency of our RoboEXP system in\nfacilitating a wide range of real-world manipulation tasks involving rigid,\narticulated objects, nested objects, and deformable objects.",
      "tldr_zh": "本研究引入了交互式场景探索任务，机器人通过自主探索环境生成行动条件场景图 (ACSG)，该图同时捕捉场景的低级信息（如几何和语义）和高级信息（如实体间的行动条件关系）。RoboEXP 系统整合了 Large Multimodal Model (LMM) 和显式内存设计，允许机器人推理探索目标和方式，通过交互过程积累信息并逐步构建 ACSG。实验结果表明，该系统在处理真实世界机器人操作任务中（如刚性、铰接、嵌套和可变形物体）表现出色，提升了任务的有效性和效率。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Project Page: https://jianghanxiao.github.io/roboexp-web/",
      "pdf_url": "http://arxiv.org/pdf/2402.15487v2",
      "published_date": "2024-02-23 18:27:17 UTC",
      "updated_date": "2024-10-08 05:32:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:22:45.231875"
    },
    {
      "arxiv_id": "2403.00800v1",
      "title": "Brain-Inspired Two-Stage Approach: Enhancing Mathematical Reasoning by Imitating Human Thought Processes",
      "title_zh": "翻译失败",
      "authors": [
        "Yezeng Chen",
        "Zui Chen",
        "Yi Zhou"
      ],
      "abstract": "Although large language models demonstrate emergent abilities in solving math\nword problems, there is a challenging task in complex multi-step mathematical\nreasoning tasks. To improve model performance on mathematical reasoning tasks,\nprevious work has conducted supervised fine-tuning on open-source models by\nimproving the quality and quantity of data. In this paper, we propose a novel\napproach, named Brain, to imitate human thought processes to enhance\nmathematical reasoning abilities, using the Frontal Lobe Model to generate\nplans, and then employing the Parietal Lobe Model to generate code and execute\nto obtain answers. First, we achieve SOTA performance in comparison with Code\nLLaMA 7B based models through this method. Secondly, we find that plans can be\nexplicitly extracted from natural language, code, or formal language. Our code\nand data are publicly available at https://github.com/cyzhh/Brain.",
      "tldr_zh": "该论文提出了一种名为 Brain 的新方法，模仿人类思维过程来提升大语言模型在复杂多步数学推理任务中的性能。具体来说，该方法采用两阶段策略：使用 Frontal Lobe Model 生成推理计划，然后通过 Parietal Lobe Model 生成代码并执行以获取答案。实验结果显示，该方法在与 Code LLaMA 7B 模型的比较中实现了 SOTA 性能，并证明计划可以从自然语言、代码或正式语言中显式提取，为进一步优化数学推理提供了开源代码和数据。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.00800v1",
      "published_date": "2024-02-23 17:40:31 UTC",
      "updated_date": "2024-02-23 17:40:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:22:56.139536"
    },
    {
      "arxiv_id": "2403.00799v1",
      "title": "An Empirical Study of Data Ability Boundary in LLMs' Math Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Zui Chen",
        "Yezeng Chen",
        "Jiaqi Han",
        "Zhijie Huang",
        "Ji Qi",
        "Yi Zhou"
      ],
      "abstract": "Large language models (LLMs) are displaying emergent abilities for math\nreasoning tasks,and there is a growing attention on enhancing the ability of\nopen-source LLMs through supervised fine-tuning (SFT).In this paper, we aim to\nexplore a general data strategy for supervised data to help optimize and expand\nmath reasoning ability.Firstly, we determine the ability boundary of reasoning\npaths augmentation by identifying these paths' minimal optimal set.Secondly, we\nvalidate that different abilities of the model can be cumulatively enhanced by\nMix of Minimal Optimal Sets of corresponding types of data, while our models\nMMOS achieve SOTA performance on series base models under much lower\nconstruction costs.Besides, we point out GSM-HARD is not really hard and\ntoday's LLMs no longer lack numerical robustness.Also, we provide an Auto\nProblem Generator for robustness testing and educational applications.Our code\nand data are publicly available at https://github.com/cyzhh/MMOS.",
      "tldr_zh": "本研究通过实证方法探讨大型语言模型 (LLMs) 在数学推理中的数据能力边界，旨在通过监督微调 (SFT) 优化数据策略以提升模型性能。首先，作者识别了推理路径的最小最优集，并证明通过混合这些最小最优集 (Mix of Minimal Optimal Sets) 可以累积增强模型能力，其模型 MMOS 在较低构建成本下在系列基线模型上达到 SOTA 性能。其次，研究发现 GSM-HARD 并非真正困难，且当前 LLMs 已具备数值鲁棒性，并提供了一个 Auto Problem Generator 用于鲁棒性测试和教育应用。代码和数据已公开可用于 https://github.com/cyzhh/MMOS。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "33 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.00799v1",
      "published_date": "2024-02-23 17:38:43 UTC",
      "updated_date": "2024-02-23 17:38:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:23:09.133945"
    },
    {
      "arxiv_id": "2402.15448v1",
      "title": "Computer Vision for Multimedia Geolocation in Human Trafficking Investigation: A Systematic Literature Review",
      "title_zh": "翻译失败",
      "authors": [
        "Opeyemi Bamigbade",
        "John Sheppard",
        "Mark Scanlon"
      ],
      "abstract": "The task of multimedia geolocation is becoming an increasingly essential\ncomponent of the digital forensics toolkit to effectively combat human\ntrafficking, child sexual exploitation, and other illegal acts. Typically,\nmetadata-based geolocation information is stripped when multimedia content is\nshared via instant messaging and social media. The intricacy of geolocating,\ngeotagging, or finding geographical clues in this content is often overly\nburdensome for investigators. Recent research has shown that contemporary\nadvancements in artificial intelligence, specifically computer vision and deep\nlearning, show significant promise towards expediting the multimedia\ngeolocation task. This systematic literature review thoroughly examines the\nstate-of-the-art leveraging computer vision techniques for multimedia\ngeolocation and assesses their potential to expedite human trafficking\ninvestigation. This includes a comprehensive overview of the application of\ncomputer vision-based approaches to multimedia geolocation, identifies their\napplicability in combating human trafficking, and highlights the potential\nimplications of enhanced multimedia geolocation for prosecuting human\ntrafficking. 123 articles inform this systematic literature review. The\nfindings suggest numerous potential paths for future impactful research on the\nsubject.",
      "tldr_zh": "这篇系统文献综述探讨了计算机 vision 在多媒体 geolocation 中的应用，以加速人口贩卖调查和打击非法活动。论文分析了123篇文章，概述了计算机 vision 和 deep learning 技术如何识别地理线索，并评估其在处理元数据缺失的多媒体内容方面的潜力。研究发现，这些技术能显著提升调查效率，并为起诉提供支持，同时指出了未来研究的多个关键方向，如进一步优化算法和扩展应用场景。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15448v1",
      "published_date": "2024-02-23 17:23:06 UTC",
      "updated_date": "2024-02-23 17:23:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:23:21.614580"
    },
    {
      "arxiv_id": "2402.15552v4",
      "title": "Morphological Symmetries in Robotics",
      "title_zh": "机器人学中的形态对称性",
      "authors": [
        "Daniel Ordoñez-Apraez",
        "Giulio Turrisi",
        "Vladimir Kostic",
        "Mario Martin",
        "Antonio Agudo",
        "Francesc Moreno-Noguer",
        "Massimiliano Pontil",
        "Claudio Semini",
        "Carlos Mastalli"
      ],
      "abstract": "We present a comprehensive framework for studying and leveraging\nmorphological symmetries in robotic systems. These are intrinsic properties of\nthe robot's morphology, frequently observed in animal biology and robotics,\nwhich stem from the replication of kinematic structures and the symmetrical\ndistribution of mass. We illustrate how these symmetries extend to the robot's\nstate space and both proprioceptive and exteroceptive sensor measurements,\nresulting in the equivariance of the robot's equations of motion and optimal\ncontrol policies. Thus, we recognize morphological symmetries as a relevant and\npreviously unexplored physics-informed geometric prior, with significant\nimplications for both data-driven and analytical methods used in modeling,\ncontrol, estimation and design in robotics. For data-driven methods, we\ndemonstrate that morphological symmetries can enhance the sample efficiency and\ngeneralization of machine learning models through data augmentation, or by\napplying equivariant/invariant constraints on the model's architecture. In the\ncontext of analytical methods, we employ abstract harmonic analysis to\ndecompose the robot's dynamics into a superposition of lower-dimensional,\nindependent dynamics. We substantiate our claims with both synthetic and\nreal-world experiments conducted on bipedal and quadrupedal robots. Lastly, we\nintroduce the repository MorphoSymm to facilitate the practical use of the\ntheory and applications outlined in this work.",
      "tldr_zh": "本文提出一个全面框架，用于研究和利用机器人中的 morphological symmetries，这些对称性源于运动结构的复制和质量分布，延伸到状态空间、传感器测量及运动方程的 equivariance，从而作为一种基于物理的几何先验影响机器人建模、控制、估计和设计。  \n在数据驱动方法中，morphological symmetries 通过 data augmentation 或 equivariant/invariant constraints 提升机器学习模型的样本效率和泛化能力。  \n在分析方法中，作者运用 abstract harmonic analysis 将机器人动力学分解为低维独立的动态叠加。  \n通过双足和四足机器人的合成及真实实验，验证了该框架的有效性。  \n最后，引入 MorphoSymm 仓库，以支持理论和应用的实际实施。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY",
        "68T40",
        "I.2.9"
      ],
      "primary_category": "cs.RO",
      "comment": "18 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.15552v4",
      "published_date": "2024-02-23 17:21:21 UTC",
      "updated_date": "2025-03-24 18:59:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:23:36.212461"
    },
    {
      "arxiv_id": "2402.15445v2",
      "title": "Can we forget how we learned? Doxastic redundancy in iterated belief revision",
      "title_zh": "翻译失败",
      "authors": [
        "Paolo Liberatore"
      ],
      "abstract": "Forgetting a belief acquisition episode may not cause information loss\nbecause of the others. Checking whether it does is not obvious, as the\ncontribution of each belief revision is not isolated from the others, and the\nsame information may be given not directly but by deduction. An algorithm for\nchecking whether forgetting reduces information is given for a number of\niterated belief revision operators: lexicographic, natural, severe, plain\nsevere, moderate severe, restrained, very radical and full meet revisions. It\nmay take exponential time in the worst case, which is expected given that the\nproblem is coNP-hard, even in the Horn restriction. It is in coNP for\nhomogeneous sequences of lexicographic revisions.",
      "tldr_zh": "本论文探讨了在迭代信念修正(iterated belief revision)中，忘记一个信念获取事件是否会导致信息丢失，即信念冗余(doxastic redundancy)，因为其他事件可能补偿该信息。研究提供了一个算法，用于检查多种信念修正操作符（如lexicographic, natural, severe, plain severe, moderate severe, restrained, very radical和full meet revisions）是否因忘记而减少信息。结果显示，该算法在最坏情况下需要指数时间，因为问题本身是coNP-hard，但在Horn restriction下或homogeneous sequences of lexicographic revisions中，它属于coNP复杂度。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "formerly part of arXiv:2305.09200",
      "pdf_url": "http://arxiv.org/pdf/2402.15445v2",
      "published_date": "2024-02-23 17:09:04 UTC",
      "updated_date": "2025-04-26 11:53:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:23:45.383159"
    },
    {
      "arxiv_id": "2402.15429v2",
      "title": "ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion Models against Stochastic Perturbation",
      "title_zh": "翻译失败",
      "authors": [
        "Yi Zhang",
        "Yun Tang",
        "Wenjie Ruan",
        "Xiaowei Huang",
        "Siddartha Khastgir",
        "Paul Jennings",
        "Xingyu Zhao"
      ],
      "abstract": "Text-to-Image (T2I) Diffusion Models (DMs) have shown impressive abilities in\ngenerating high-quality images based on simple text descriptions. However, as\nis common with many Deep Learning (DL) models, DMs are subject to a lack of\nrobustness. While there are attempts to evaluate the robustness of T2I DMs as a\nbinary or worst-case problem, they cannot answer how robust in general the\nmodel is whenever an adversarial example (AE) can be found. In this study, we\nfirst introduce a probabilistic notion of T2I DMs' robustness; and then\nestablish an efficient framework, ProTIP, to evaluate it with statistical\nguarantees. The main challenges stem from: i) the high computational cost of\nthe generation process; and ii) determining if a perturbed input is an AE\ninvolves comparing two output distributions, which is fundamentally harder\ncompared to other DL tasks like classification where an AE is identified upon\nmisprediction of labels. To tackle the challenges, we employ sequential\nanalysis with efficacy and futility early stopping rules in the statistical\ntesting for identifying AEs, and adaptive concentration inequalities to\ndynamically determine the \"just-right\" number of stochastic perturbations\nwhenever the verification target is met. Empirical experiments validate the\neffectiveness and efficiency of ProTIP over common T2I DMs. Finally, we\ndemonstrate an application of ProTIP to rank commonly used defence methods.",
      "tldr_zh": "该论文引入了 Text-to-Image Diffusion Models (T2I DMs) 的概率鲁棒性概念，以评估模型对随机扰动(stochastic perturbation)的整体鲁棒性，超越了传统的二元或最坏情况评估方法。ProTIP 框架通过顺序分析(sequential analysis)结合功效和 futility 早停规则来高效识别 adversarial example (AE)，并采用自适应集中不等式(adaptive concentration inequalities)动态确定扰动数量，以应对生成过程的高计算成本和输出分布比较挑战。实验验证了 ProTIP 在常见 T2I DMs 上的有效性和效率，并展示了其在排名常用防御方法的应用潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ECCV24",
      "pdf_url": "http://arxiv.org/pdf/2402.15429v2",
      "published_date": "2024-02-23 16:48:56 UTC",
      "updated_date": "2024-07-12 21:25:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:23:57.903094"
    },
    {
      "arxiv_id": "2402.15427v1",
      "title": "Understanding Entrainment in Human Groups: Optimising Human-Robot Collaboration from Lessons Learned during Human-Human Collaboration",
      "title_zh": "理解人类群体中的 Entrainment：基于",
      "authors": [
        "Eike Schneiders",
        "Christopher Fourie",
        "Stanley Celestin",
        "Julie Shah",
        "Malte Jung"
      ],
      "abstract": "Successful entrainment during collaboration positively affects trust,\nwillingness to collaborate, and likeability towards collaborators. In this\npaper, we present a mixed-method study to investigate characteristics of\nsuccessful entrainment leading to pair and group-based synchronisation. Drawing\ninspiration from industrial settings, we designed a fast-paced, short-cycle\nrepetitive task. Using motion tracking, we investigated entrainment in both\ndyadic and triadic task completion. Furthermore, we utilise audio-video\nrecordings and semi-structured interviews to contextualise participants'\nexperiences. This paper contributes to the Human-Computer/Robot Interaction\n(HCI/HRI) literature using a human-centred approach to identify characteristics\nof entrainment during pair- and group-based collaboration. We present five\ncharacteristics related to successful entrainment. These are related to the\noccurrence of entrainment, leader-follower patterns, interpersonal\ncommunication, the importance of the point-of-assembly, and the value of\nacoustic feedback. Finally, we present three design considerations for future\nresearch and design on collaboration with robots.",
      "tldr_zh": "本文通过混合方法研究，探讨了协作中的 entrainment 特征，以从人类-人类协作中汲取经验优化人类-机器人协作。研究设计了一个快速重复任务，使用运动跟踪技术分析 dyadic（成对）和 triadic（三人组）任务完成中的同步现象，并结合音频-视频录像和半结构化访谈来理解参与者体验。结果识别了五个成功 entrainment 相关特征，包括 entrainment 的发生、领导者-跟随者模式、人际沟通、组装点的重要性以及声学反馈的价值。这些发现为 HCI/HRI 领域提供了三个设计考虑，旨在提升机器人协作的信任和效率。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.HC",
      "comment": "Proceedings of the CHI Conference on Human Factors in Computing\n  Systems (CHI '24), May 11--16, 2024, Honolulu, HI, USA",
      "pdf_url": "http://arxiv.org/pdf/2402.15427v1",
      "published_date": "2024-02-23 16:42:17 UTC",
      "updated_date": "2024-02-23 16:42:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:24:10.591217"
    },
    {
      "arxiv_id": "2402.15422v2",
      "title": "A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Stefan Hegselmann",
        "Shannon Zejiang Shen",
        "Florian Gierse",
        "Monica Agrawal",
        "David Sontag",
        "Xiaoyi Jiang"
      ],
      "abstract": "Patients often face difficulties in understanding their hospitalizations,\nwhile healthcare workers have limited resources to provide explanations. In\nthis work, we investigate the potential of large language models to generate\npatient summaries based on doctors' notes and study the effect of training data\non the faithfulness and quality of the generated summaries. To this end, we\nrelease (i) a rigorous labeling protocol for errors in medical texts and (ii) a\npublicly available dataset of annotated hallucinations in 100 doctor-written\nand 100 generated summaries. We show that fine-tuning on hallucination-free\ndata effectively reduces hallucinations from 2.60 to 1.55 per summary for Llama\n2, while preserving relevant information. We observe a similar effect on GPT-4\n(0.70 to 0.40), when the few-shot examples are hallucination-free. We also\nconduct a qualitative evaluation using hallucination-free and improved training\ndata. We find that common quantitative metrics do not correlate well with\nfaithfulness and quality. Finally, we test GPT-4 for automatic hallucination\ndetection, which clearly outperforms common baselines.",
      "tldr_zh": "这篇论文提出了一种数据中心方法，利用大型语言模型 (Large Language Models) 生成高质量且真实性的患者总结，旨在帮助患者理解住院情况并缓解医疗工作者的资源限制。研究者发布了一个严格的错误标记协议和公开数据集，包含100个医生总结和100个生成总结的幻觉 (hallucinations) 标注；通过在无幻觉数据上微调，Llama 2 的幻觉从2.60减少到1.55，GPT-4 从0.70减少到0.40，同时保留了相关信息。结果显示，常见量化指标与真实性和质量相关性不高，而GPT-4在自动幻觉检测上明显优于基线，为可靠医疗总结生成提供了重要贡献。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15422v2",
      "published_date": "2024-02-23 16:32:28 UTC",
      "updated_date": "2024-06-25 17:02:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:24:22.903876"
    },
    {
      "arxiv_id": "2402.15418v3",
      "title": "Reputational Algorithm Aversion",
      "title_zh": "基于声誉的算法厌恶",
      "authors": [
        "Gregory Weitzner"
      ],
      "abstract": "People are often reluctant to incorporate information produced by algorithms\ninto their decisions, a phenomenon called ``algorithm aversion''. This paper\nshows how algorithm aversion arises when the choice to follow an algorithm\nconveys information about a human's ability. I develop a model in which workers\nmake forecasts of an uncertain outcome based on their own private information\nand an algorithm's signal. Low-skill workers receive worse information than the\nalgorithm and hence should always follow the algorithm's signal, while\nhigh-skill workers receive better information than the algorithm and should\nsometimes override it. However, due to reputational concerns, low-skill workers\ninefficiently override the algorithm to increase the likelihood they are\nperceived as high-skill. The model provides a fully rational microfoundation\nfor algorithm aversion that aligns with the broad concern that AI systems will\ndisplace many types of workers.",
      "tldr_zh": "本文研究了“algorithm aversion”现象，即人们不愿采用算法生成的信息进行决策。作者构建了一个模型，假设工人基于私人信息和算法信号进行预测：低技能工人应始终遵循算法，而高技能工人有时可覆盖它；然而，低技能工人出于声誉考虑，会不必要地忽略算法，以增加被视为高技能的概率。该模型提供了算法厌恶的理性微观基础，并与AI系统可能取代工人的担忧相呼应。",
      "categories": [
        "econ.TH",
        "cs.AI",
        "cs.GT",
        "cs.HC"
      ],
      "primary_category": "econ.TH",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15418v3",
      "published_date": "2024-02-23 16:28:55 UTC",
      "updated_date": "2024-07-31 20:01:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:24:32.870133"
    },
    {
      "arxiv_id": "2402.15398v1",
      "title": "TransFlower: An Explainable Transformer-Based Model with Flow-to-Flow Attention for Commuting Flow Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Yan Luo",
        "Zhuoyue Wan",
        "Yuzhong Chen",
        "Gengchen Mai",
        "Fu-lai Chung",
        "Kent Larson"
      ],
      "abstract": "Understanding the link between urban planning and commuting flows is crucial\nfor guiding urban development and policymaking. This research, bridging\ncomputer science and urban studies, addresses the challenge of integrating\nthese fields with their distinct focuses. Traditional urban studies methods,\nlike the gravity and radiation models, often underperform in complex scenarios\ndue to their limited handling of multiple variables and reliance on overly\nsimplistic and unrealistic assumptions, such as spatial isotropy. While deep\nlearning models offer improved accuracy, their black-box nature poses a\ntrade-off between performance and explainability -- both vital for analyzing\ncomplex societal phenomena like commuting flows. To address this, we introduce\nTransFlower, an explainable, transformer-based model employing flow-to-flow\nattention to predict urban commuting patterns. It features a geospatial encoder\nwith an anisotropy-aware relative location encoder for nuanced flow\nrepresentation. Following this, the transformer-based flow predictor enhances\nthis by leveraging attention mechanisms to efficiently capture flow\ninteractions. Our model outperforms existing methods by up to 30.8% Common Part\nof Commuters, offering insights into mobility dynamics crucial for urban\nplanning and policy decisions.",
      "tldr_zh": "本研究针对城市通勤流量预测的挑战，提出TransFlower，一种基于Transformer的可解释模型，利用flow-to-flow attention机制来捕捉流量互动。该模型包括一个geospatial encoder和anisotropy-aware relative location encoder，以更精确地表示空间异向性相关的数据，从而克服传统方法（如gravity和radiation模型）的局限性，以及深度学习模型的黑盒问题。实验结果显示，TransFlower比现有方法提升高达30.8%的Common Part of Commuters，提供宝贵的移动动态洞见，支持城市规划和政策决策。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15398v1",
      "published_date": "2024-02-23 16:00:04 UTC",
      "updated_date": "2024-02-23 16:00:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:24:45.580651"
    },
    {
      "arxiv_id": "2402.15393v4",
      "title": "NeuralSolver: Learning Algorithms For Consistent and Efficient Extrapolation Across General Tasks",
      "title_zh": "NeuralSolver: 学习算法以实现跨一般任务的一致且高效外推",
      "authors": [
        "Bernardo Esteves",
        "Miguel Vasco",
        "Francisco S. Melo"
      ],
      "abstract": "We contribute NeuralSolver, a novel recurrent solver that can efficiently and\nconsistently extrapolate, i.e., learn algorithms from smaller problems (in\nterms of observation size) and execute those algorithms in large problems.\nContrary to previous recurrent solvers, NeuralSolver can be naturally applied\nin both same-size problems, where the input and output sizes are the same, and\nin different-size problems, where the size of the input and output differ. To\nallow for this versatility, we design NeuralSolver with three main components:\na recurrent module, that iteratively processes input information at different\nscales, a processing module, responsible for aggregating the previously\nprocessed information, and a curriculum-based training scheme, that improves\nthe extrapolation performance of the method. To evaluate our method we\nintroduce a set of novel different-size tasks and we show that NeuralSolver\nconsistently outperforms the prior state-of-the-art recurrent solvers in\nextrapolating to larger problems, considering smaller training problems and\nrequiring less parameters than other approaches.",
      "tldr_zh": "该研究提出 NeuralSolver，一种新型循环求解器（recurrent solver），能够从较小问题（observation size）学习算法，并高效一致地外推（extrapolate）到更大问题，支持相同大小问题（same-size problems）和不同大小问题（different-size problems）。NeuralSolver 由三个关键组件组成：循环模块（recurrent module）用于迭代处理不同规模的输入信息、处理模块（processing module）负责聚合已处理信息，以及基于课程的训练方案（curriculum-based training scheme）来提升外推性能。实验结果显示，在一组新颖的不同大小任务上，NeuralSolver 超越了现有最先进的方法，使用更少的参数和更小的训练问题。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15393v4",
      "published_date": "2024-02-23 15:51:45 UTC",
      "updated_date": "2024-10-31 09:46:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:24:58.768645"
    },
    {
      "arxiv_id": "2402.15391v1",
      "title": "Genie: Generative Interactive Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Jake Bruce",
        "Michael Dennis",
        "Ashley Edwards",
        "Jack Parker-Holder",
        "Yuge Shi",
        "Edward Hughes",
        "Matthew Lai",
        "Aditi Mavalankar",
        "Richie Steigerwald",
        "Chris Apps",
        "Yusuf Aytar",
        "Sarah Bechtle",
        "Feryal Behbahani",
        "Stephanie Chan",
        "Nicolas Heess",
        "Lucy Gonzalez",
        "Simon Osindero",
        "Sherjil Ozair",
        "Scott Reed",
        "Jingwei Zhang",
        "Konrad Zolna",
        "Jeff Clune",
        "Nando de Freitas",
        "Satinder Singh",
        "Tim Rocktäschel"
      ],
      "abstract": "We introduce Genie, the first generative interactive environment trained in\nan unsupervised manner from unlabelled Internet videos. The model can be\nprompted to generate an endless variety of action-controllable virtual worlds\ndescribed through text, synthetic images, photographs, and even sketches. At\n11B parameters, Genie can be considered a foundation world model. It is\ncomprised of a spatiotemporal video tokenizer, an autoregressive dynamics\nmodel, and a simple and scalable latent action model. Genie enables users to\nact in the generated environments on a frame-by-frame basis despite training\nwithout any ground-truth action labels or other domain-specific requirements\ntypically found in the world model literature. Further the resulting learned\nlatent action space facilitates training agents to imitate behaviors from\nunseen videos, opening the path for training generalist agents of the future.",
      "tldr_zh": "我们引入了 Genie，这是一个从无标签互联网视频中无监督训练的首个生成式交互环境，能够通过文本、合成图像、照片或草图等提示生成多样可控虚拟世界。Genie 作为 11B 参数的 foundation world model，由 spatiotemporal video tokenizer、autoregressive dynamics model 和 scalable latent action model 组成，尽管训练时未使用 ground-truth action labels，该模型仍支持用户基于帧进行交互。进一步，Genie 的 latent action space 能促进训练代理模仿未见视频中的行为，为未来通用代理的开发铺平道路。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "https://sites.google.com/corp/view/genie-2024/",
      "pdf_url": "http://arxiv.org/pdf/2402.15391v1",
      "published_date": "2024-02-23 15:47:26 UTC",
      "updated_date": "2024-02-23 15:47:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:25:09.718713"
    },
    {
      "arxiv_id": "2402.15390v2",
      "title": "Explorations of Self-Repair in Language Models",
      "title_zh": "语言模型中自修复的探索",
      "authors": [
        "Cody Rushing",
        "Neel Nanda"
      ],
      "abstract": "Prior interpretability research studying narrow distributions has\npreliminarily identified self-repair, a phenomena where if components in large\nlanguage models are ablated, later components will change their behavior to\ncompensate. Our work builds off this past literature, demonstrating that\nself-repair exists on a variety of models families and sizes when ablating\nindividual attention heads on the full training distribution. We further show\nthat on the full training distribution self-repair is imperfect, as the\noriginal direct effect of the head is not fully restored, and noisy, since the\ndegree of self-repair varies significantly across different prompts (sometimes\novercorrecting beyond the original effect). We highlight two different\nmechanisms that contribute to self-repair, including changes in the final\nLayerNorm scaling factor and sparse sets of neurons implementing Anti-Erasure.\nWe additionally discuss the implications of these results for interpretability\npractitioners and close with a more speculative discussion on the mystery of\nwhy self-repair occurs in these models at all, highlighting evidence for the\nIterative Inference hypothesis in language models, a framework that predicts\nself-repair.",
      "tldr_zh": "这篇论文探索了语言模型中的 self-repair 现象，即当移除组件如注意力头时，后续组件会调整行为进行补偿。研究通过在完整训练分布上实验多种模型家族和大小，证明了 self-repair 的广泛存在，但它是不完美的（原组件效果未完全恢复）和 noisy 的（在不同提示下变化显著，有时过度修正）。论文识别了两种机制，包括最终 LayerNorm 缩放因子的变化和实现 Anti-Erasure 的稀疏神经元集，并讨论了这些发现对可解释性从业者的影响，同时支持了语言模型的 Iterative Inference 假设。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.15390v2",
      "published_date": "2024-02-23 15:42:12 UTC",
      "updated_date": "2024-05-26 20:47:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:25:24.267774"
    },
    {
      "arxiv_id": "2402.15384v3",
      "title": "Closed-loop Multi-step Planning",
      "title_zh": "闭环多步规划",
      "authors": [
        "Giulia Lafratta",
        "Bernd Porr",
        "Christopher Chandler",
        "Alice Miller"
      ],
      "abstract": "Living organisms interact with their surroundings in a closed-loop fashion,\nwhere sensory inputs dictate the initiation and termination of behaviours. Even\nsimple animals are able to develop and execute complex plans, which has not yet\nbeen replicated in robotics using pure closed-loop input control. We propose a\nsolution to this problem by defining a set of discrete and temporary\nclosed-loop controllers, called ``Tasks'', each representing a closed-loop\nbehaviour. We further introduce a supervisory module which has an innate\nunderstanding of physics and causality, through which it can simulate the\nexecution of Task sequences over time and store the results in a model of the\nenvironment. On the basis of this model, plans can be made by chaining\ntemporary closed-loop controllers. Our proposed framework was implemented for a\nreal robot and tested in two scenarios as proof of concept.",
      "tldr_zh": "该论文探讨了如何让机器人像生物体一样，通过闭环方式执行多步计划，以解决现有机器人无法实现纯闭环输入控制的难题。研究提出一种框架，使用一组离散的临时“closed-loop controllers”称为“Tasks”来代表各种行为，并引入一个“supervisory module”来模拟任务序列的执行，利用物理和因果关系的理解构建环境模型，从而实现任务链的规划。在真实机器人上进行的两个场景测试证明了该框架的可行性和有效性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15384v3",
      "published_date": "2024-02-23 15:30:57 UTC",
      "updated_date": "2025-01-29 12:47:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:25:33.860921"
    },
    {
      "arxiv_id": "2402.15370v1",
      "title": "Dual Encoder: Exploiting the Potential of Syntactic and Semantic for Aspect Sentiment Triplet Extraction",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaowei Zhao",
        "Yong Zhou",
        "Xiujuan Xu"
      ],
      "abstract": "Aspect Sentiment Triple Extraction (ASTE) is an emerging task in fine-grained\nsentiment analysis. Recent studies have employed Graph Neural Networks (GNN) to\nmodel the syntax-semantic relationships inherent in triplet elements. However,\nthey have yet to fully tap into the vast potential of syntactic and semantic\ninformation within the ASTE task. In this work, we propose a \\emph{Dual\nEncoder: Exploiting the potential of Syntactic and Semantic} model (D2E2S),\nwhich maximizes the syntactic and semantic relationships among words.\nSpecifically, our model utilizes a dual-channel encoder with a BERT channel to\ncapture semantic information, and an enhanced LSTM channel for comprehensive\nsyntactic information capture. Subsequently, we introduce the heterogeneous\nfeature interaction module to capture intricate interactions between dependency\nsyntax and attention semantics, and to dynamically select vital nodes. We\nleverage the synergy of these modules to harness the significant potential of\nsyntactic and semantic information in ASTE tasks. Testing on public benchmarks,\nour D2E2S model surpasses the current state-of-the-art(SOTA), demonstrating its\neffectiveness.",
      "tldr_zh": "本文提出 D2E2S 模型（Dual Encoder: Exploiting the Potential of Syntactic and Semantic），旨在通过充分利用语法和语义信息来提升 Aspect Sentiment Triplet Extraction (ASTE) 任务的性能。模型采用双通道编码器，包括 BERT 通道捕获语义信息，以及增强的 LSTM 通道全面捕获语法信息；同时引入 heterogeneous feature interaction module 来处理依赖语法和注意力语义之间的复杂交互，并动态选择关键节点。在公共基准测试中，D2E2S 超越了当前 SOTA 模型，证明了其在细粒度情感分析中的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by COLING 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.15370v1",
      "published_date": "2024-02-23 15:07:13 UTC",
      "updated_date": "2024-02-23 15:07:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:25:46.797289"
    },
    {
      "arxiv_id": "2402.15368v4",
      "title": "Probabilistically Correct Language-based Multi-Robot Planning using Conformal Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Jun Wang",
        "Guocheng He",
        "Yiannis Kantaros"
      ],
      "abstract": "This paper addresses task planning problems for language-instructed robot\nteams. Tasks are expressed in natural language (NL), requiring the robots to\napply their capabilities at various locations and semantic objects. Several\nrecent works have addressed similar planning problems by leveraging pre-trained\nLarge Language Models (LLMs) to design effective multi-robot plans. However,\nthese approaches lack performance guarantees. To address this challenge, we\nintroduce a new distributed LLM-based planner, called S-ATLAS for Safe plAnning\nfor Teams of Language-instructed AgentS, that is capable of achieving\nuser-defined mission success rates. This is accomplished by leveraging\nconformal prediction (CP), a distribution-free uncertainty quantification tool\nin black-box models. CP allows the proposed multi-robot planner to reason about\nits inherent uncertainty in a distributed fashion, enabling robots to make\nindividual decisions when they are sufficiently certain and seek help\notherwise. We show, both theoretically and empirically, that the proposed\nplanner can achieve user-specified task success rates, assuming successful plan\nexecution, while minimizing the overall number of help requests. We provide\ncomparative experiments against related works showing that our method is\nsignificantly more computational efficient and achieves lower help rates. The\nadvantage of our algorithm over baselines becomes more pronounced with\nincreasing robot team size.",
      "tldr_zh": "本论文解决语言指令下的多机器人任务规划问题，提出一种基于 Large Language Models (LLMs) 的分布式规划器 S-ATLAS，以实现用户指定的任务成功率。S-ATLAS 利用 conformal prediction (CP) 作为不确定性量化工具，让机器人团队在分布式环境中独立决策或寻求帮助，从而最小化帮助请求数量。理论分析和实验结果表明，该方法在假设计划执行成功的前提下，能显著提高计算效率，并降低帮助率，尤其在机器人团队规模增大时比基线方法表现出色。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15368v4",
      "published_date": "2024-02-23 15:02:44 UTC",
      "updated_date": "2024-11-21 16:19:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:25:58.269574"
    },
    {
      "arxiv_id": "2402.15350v2",
      "title": "Farsight: Fostering Responsible AI Awareness During AI Application Prototyping",
      "title_zh": "翻译失败",
      "authors": [
        "Zijie J. Wang",
        "Chinmay Kulkarni",
        "Lauren Wilcox",
        "Michael Terry",
        "Michael Madaio"
      ],
      "abstract": "Prompt-based interfaces for Large Language Models (LLMs) have made\nprototyping and building AI-powered applications easier than ever before.\nHowever, identifying potential harms that may arise from AI applications\nremains a challenge, particularly during prompt-based prototyping. To address\nthis, we present Farsight, a novel in situ interactive tool that helps people\nidentify potential harms from the AI applications they are prototyping. Based\non a user's prompt, Farsight highlights news articles about relevant AI\nincidents and allows users to explore and edit LLM-generated use cases,\nstakeholders, and harms. We report design insights from a co-design study with\n10 AI prototypers and findings from a user study with 42 AI prototypers. After\nusing Farsight, AI prototypers in our user study are better able to\nindependently identify potential harms associated with a prompt and find our\ntool more useful and usable than existing resources. Their qualitative feedback\nalso highlights that Farsight encourages them to focus on end-users and think\nbeyond immediate harms. We discuss these findings and reflect on their\nimplications for designing AI prototyping experiences that meaningfully engage\nwith AI harms. Farsight is publicly accessible at:\nhttps://PAIR-code.github.io/farsight.",
      "tldr_zh": "本研究介绍了 Farsight，一种创新的 in situ 交互工具，旨在帮助 AI 应用原型设计者在使用 Large Language Models (LLMs) 时识别潜在危害。Farsight 通过分析用户的提示，突出相关 AI incidents 的新闻文章，并允许用户探索和编辑 LLM 生成的使用案例、利益相关者和危害，从而促进负责任 AI 意识。研究包括一个 co-design study（涉及 10 名 AI 原型设计师）和一个 user study（涉及 42 名 AI 原型设计师），结果显示，使用者更能独立识别提示关联的潜在危害，并认为 Farsight 比现有资源更实用和易用。参与者反馈强调，该工具鼓励关注最终用户并扩展到即时危害之外，提供重要启示，以设计更注重 AI 危害的 AI 原型体验。工具公开可用于 https://PAIR-code.github.io/farsight。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted to CHI 2024 (Best Paper, Honorable Mention). 40 pages, 19\n  figures, 5 tables. For a demo video, see https://youtu.be/BlSFbGkOlHk. For a\n  live demo, visit https://PAIR-code.github.io/farsight. The source code is\n  available at https://github.com/PAIR-code/farsight",
      "pdf_url": "http://arxiv.org/pdf/2402.15350v2",
      "published_date": "2024-02-23 14:38:05 UTC",
      "updated_date": "2024-07-02 06:12:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:26:12.281884"
    },
    {
      "arxiv_id": "2402.15347v2",
      "title": "Information-Theoretic Safe Bayesian Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Alessandro G. Bottero",
        "Carlos E. Luis",
        "Julia Vinogradska",
        "Felix Berkenkamp",
        "Jan Peters"
      ],
      "abstract": "We consider a sequential decision making task, where the goal is to optimize\nan unknown function without evaluating parameters that violate an a~priori\nunknown (safety) constraint. A common approach is to place a Gaussian process\nprior on the unknown functions and allow evaluations only in regions that are\nsafe with high probability. Most current methods rely on a discretization of\nthe domain and cannot be directly extended to the continuous case. Moreover,\nthe way in which they exploit regularity assumptions about the constraint\nintroduces an additional critical hyperparameter. In this paper, we propose an\ninformation-theoretic safe exploration criterion that directly exploits the GP\nposterior to identify the most informative safe parameters to evaluate. The\ncombination of this exploration criterion with a well known Bayesian\noptimization acquisition function yields a novel safe Bayesian optimization\nselection criterion. Our approach is naturally applicable to continuous domains\nand does not require additional explicit hyperparameters. We theoretically\nanalyze the method and show that we do not violate the safety constraint with\nhigh probability and that we learn about the value of the safe optimum up to\narbitrary precision. Empirical evaluations demonstrate improved data-efficiency\nand scalability.",
      "tldr_zh": "本研究针对优化未知函数时避免违反未知安全约束的问题，提出了一种信息理论安全探索标准，利用Gaussian process (GP)后验来识别最信息丰富的安全参数进行评估。该方法将该探索标准与Bayesian optimization的获取函数结合，形成一个新的安全Bayesian optimization选择标准，能够直接适用于连续域而不需额外超参数。理论分析证明，该方法能以高概率不违反安全约束，并精确学习安全最优值；实验结果显示，它在数据效率和可扩展性方面优于现有方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "arXiv admin note: text overlap with arXiv:2212.04914",
      "pdf_url": "http://arxiv.org/pdf/2402.15347v2",
      "published_date": "2024-02-23 14:31:10 UTC",
      "updated_date": "2024-05-10 10:47:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:26:21.457137"
    },
    {
      "arxiv_id": "2402.15343v1",
      "title": "NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data",
      "title_zh": "翻译失败",
      "authors": [
        "Sergei Bogdanov",
        "Alexandre Constantin",
        "Timothée Bernard",
        "Benoit Crabbé",
        "Etienne Bernard"
      ],
      "abstract": "Large Language Models (LLMs) have shown impressive abilities in data\nannotation, opening the way for new approaches to solve classic NLP problems.\nIn this paper, we show how to use LLMs to create NuNER, a compact language\nrepresentation model specialized in the Named Entity Recognition (NER) task.\nNuNER can be fine-tuned to solve downstream NER problems in a data-efficient\nway, outperforming similar-sized foundation models in the few-shot regime and\ncompeting with much larger LLMs. We find that the size and entity-type\ndiversity of the pre-training dataset are key to achieving good performance. We\nview NuNER as a member of the broader family of task-specific foundation\nmodels, recently unlocked by LLMs.",
      "tldr_zh": "本文提出 NuNER，这是一个通过大语言模型 (LLMs) 标注数据预训练的紧凑语言表示模型，专门针对命名实体识别 (NER) 任务。NuNER 能够在少样本场景下高效微调，表现优于类似规模的基础模型，并与更大 LLMs 竞争。研究发现，预训练数据集的大小和实体类型多样性是实现良好性能的关键，将 NuNER 视为任务特定基础模型大家庭的一部分。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15343v1",
      "published_date": "2024-02-23 14:23:51 UTC",
      "updated_date": "2024-02-23 14:23:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:26:34.282965"
    },
    {
      "arxiv_id": "2402.15333v1",
      "title": "A Quantum-Classical Collaborative Training Architecture Based on Quantum State Fidelity",
      "title_zh": "翻译失败",
      "authors": [
        "Ryan L'Abbate",
        "Anthony D'Onofrio Jr.",
        "Samuel Stein",
        "Samuel Yen-Chi Chen",
        "Ang Li",
        "Pin-Yu Chen",
        "Juntao Chen",
        "Ying Mao"
      ],
      "abstract": "Recent advancements have highlighted the limitations of current quantum\nsystems, particularly the restricted number of qubits available on near-term\nquantum devices. This constraint greatly inhibits the range of applications\nthat can leverage quantum computers. Moreover, as the available qubits\nincrease, the computational complexity grows exponentially, posing additional\nchallenges. Consequently, there is an urgent need to use qubits efficiently and\nmitigate both present limitations and future complexities. To address this,\nexisting quantum applications attempt to integrate classical and quantum\nsystems in a hybrid framework. In this study, we concentrate on quantum deep\nlearning and introduce a collaborative classical-quantum architecture called\nco-TenQu. The classical component employs a tensor network for compression and\nfeature extraction, enabling higher-dimensional data to be encoded onto logical\nquantum circuits with limited qubits. On the quantum side, we propose a\nquantum-state-fidelity-based evaluation function to iteratively train the\nnetwork through a feedback loop between the two sides. co-TenQu has been\nimplemented and evaluated with both simulators and the IBM-Q platform. Compared\nto state-of-the-art approaches, co-TenQu enhances a classical deep neural\nnetwork by up to 41.72% in a fair setting. Additionally, it outperforms other\nquantum-based methods by up to 1.9 times and achieves similar accuracy while\nutilizing 70.59% fewer qubits.",
      "tldr_zh": "本论文针对量子系统受限比特数和计算复杂性的问题，提出了一种基于 quantum state fidelity 的量子-经典协作训练架构 co-TenQu。该架构利用 tensor network 在古典部分进行数据压缩和特征提取，从而将高维数据高效编码到有限的逻辑量子电路中，并在量子部分通过反馈循环迭代训练以优化网络性能。实验在模拟器和 IBM-Q 平台上验证，co-TenQu 相较于最先进方法提升了高达 41.72% 的准确率，并比其他量子方法高出 1.9 倍，同时仅使用 70.59% 的量子比特。",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "IEEE Transactions on Quantum Engineering",
      "pdf_url": "http://arxiv.org/pdf/2402.15333v1",
      "published_date": "2024-02-23 14:09:41 UTC",
      "updated_date": "2024-02-23 14:09:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:26:46.784498"
    },
    {
      "arxiv_id": "2402.15332v2",
      "title": "Position: Categorical Deep Learning is an Algebraic Theory of All Architectures",
      "title_zh": "翻译失败",
      "authors": [
        "Bruno Gavranović",
        "Paul Lessard",
        "Andrew Dudzik",
        "Tamara von Glehn",
        "João G. M. Araújo",
        "Petar Veličković"
      ],
      "abstract": "We present our position on the elusive quest for a general-purpose framework\nfor specifying and studying deep learning architectures. Our opinion is that\nthe key attempts made so far lack a coherent bridge between specifying\nconstraints which models must satisfy and specifying their implementations.\nFocusing on building a such a bridge, we propose to apply category theory --\nprecisely, the universal algebra of monads valued in a 2-category of parametric\nmaps -- as a single theory elegantly subsuming both of these flavours of neural\nnetwork design. To defend our position, we show how this theory recovers\nconstraints induced by geometric deep learning, as well as implementations of\nmany architectures drawn from the diverse landscape of neural networks, such as\nRNNs. We also illustrate how the theory naturally encodes many standard\nconstructs in computer science and automata theory.",
      "tldr_zh": "本论文提出一个观点：范畴论（category theory）可以作为深度学习架构的通用代数理论，桥接模型约束和实现之间的差距。作者认为现有框架缺乏这一连贯性，并建议使用 monads valued in a 2-category of parametric maps 来统一神经网络设计。该理论能够恢复 geometric deep learning 引发的约束，并成功实现多种架构，如 RNNs，同时自然编码计算机科学和自动机理论的标准结构。通过这种方法，论文为深度学习架构的全面研究提供了一个优雅的框架。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.CT",
        "math.RA",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "To appear in ICML 2024. Comments welcome. More info at\n  categoricaldeeplearning.com",
      "pdf_url": "http://arxiv.org/pdf/2402.15332v2",
      "published_date": "2024-02-23 14:01:53 UTC",
      "updated_date": "2024-06-06 00:58:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:26:57.109727"
    },
    {
      "arxiv_id": "2402.15321v2",
      "title": "OpenSUN3D: 1st Workshop Challenge on Open-Vocabulary 3D Scene Understanding",
      "title_zh": "翻译失败",
      "authors": [
        "Francis Engelmann",
        "Ayca Takmaz",
        "Jonas Schult",
        "Elisabetta Fedele",
        "Johanna Wald",
        "Songyou Peng",
        "Xi Wang",
        "Or Litany",
        "Siyu Tang",
        "Federico Tombari",
        "Marc Pollefeys",
        "Leonidas Guibas",
        "Hongbo Tian",
        "Chunjie Wang",
        "Xiaosheng Yan",
        "Bingwen Wang",
        "Xuanyang Zhang",
        "Xiao Liu",
        "Phuc Nguyen",
        "Khoi Nguyen",
        "Anh Tran",
        "Cuong Pham",
        "Zhening Huang",
        "Xiaoyang Wu",
        "Xi Chen",
        "Hengshuang Zhao",
        "Lei Zhu",
        "Joan Lasenby"
      ],
      "abstract": "This report provides an overview of the challenge hosted at the OpenSUN3D\nWorkshop on Open-Vocabulary 3D Scene Understanding held in conjunction with\nICCV 2023. The goal of this workshop series is to provide a platform for\nexploration and discussion of open-vocabulary 3D scene understanding tasks,\nincluding but not limited to segmentation, detection and mapping. We provide an\noverview of the challenge hosted at the workshop, present the challenge\ndataset, the evaluation methodology, and brief descriptions of the winning\nmethods. For additional details, please see\nhttps://opensun3d.github.io/index_iccv23.html.",
      "tldr_zh": "本报告概述了在 ICCV 2023 上举办的 OpenSUN3D 第一届研讨会挑战，聚焦于 Open-Vocabulary 3D Scene Understanding 任务，提供了一个平台来探索和讨论包括 segmentation、detection 和 mapping 在内的 3D 场景理解技术。挑战赛介绍了专属数据集和评估方法，并简要描述了获胜方法的要点，以促进该领域的创新和合作。更多细节可通过 https://opensun3d.github.io/index_iccv23.html 获取。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Our OpenSUN3D workshop website for ICCV 2023:\n  https://opensun3d.github.io/index_iccv23.html",
      "pdf_url": "http://arxiv.org/pdf/2402.15321v2",
      "published_date": "2024-02-23 13:39:59 UTC",
      "updated_date": "2024-03-17 08:41:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:27:09.405638"
    },
    {
      "arxiv_id": "2402.15313v2",
      "title": "ArabianGPT: Native Arabic GPT-based Large Language Model",
      "title_zh": "ArabianGPT：基于 GPT 的原生阿拉伯语大型语言模型",
      "authors": [
        "Anis Koubaa",
        "Adel Ammar",
        "Lahouari Ghouti",
        "Omar Najar",
        "Serry Sibaee"
      ],
      "abstract": "The predominance of English and Latin-based large language models (LLMs) has\nled to a notable deficit in native Arabic LLMs. This discrepancy is accentuated\nby the prevalent inclusion of English tokens in existing Arabic models,\ndetracting from their efficacy in processing native Arabic's intricate\nmorphology and syntax. Consequently, there is a theoretical and practical\nimperative for developing LLMs predominantly focused on Arabic linguistic\nelements. To address this gap, this paper proposes ArabianGPT, a series of\ntransformer-based models within the ArabianLLM suite designed explicitly for\nArabic. These models, including ArabianGPT-0.1B and ArabianGPT-0.3B, vary in\nsize and complexity, aligning with the nuanced linguistic characteristics of\nArabic. The AraNizer tokenizer, integral to these models, addresses the unique\nmorphological aspects of Arabic script, ensuring more accurate text processing.\nEmpirical results from fine-tuning the models on tasks like sentiment analysis\nand summarization demonstrate significant improvements. For sentiment analysis,\nthe fine-tuned ArabianGPT-0.1B model achieved a remarkable accuracy of 95%, a\nsubstantial increase from the base model's 56%. Similarly, in summarization\ntasks, fine-tuned models showed enhanced F1 scores, indicating improved\nprecision and recall in generating concise summaries. Comparative analysis of\nfine-tuned ArabianGPT models against their base versions across various\nbenchmarks reveals nuanced differences in performance, with fine-tuning\npositively impacting specific tasks like question answering and summarization.\nThese findings underscore the efficacy of fine-tuning in aligning ArabianGPT\nmodels more closely with specific NLP tasks, highlighting the potential of\ntailored transformer architectures in advancing Arabic NLP.",
      "tldr_zh": "本研究指出，现有的 Large Language Model (LLM) 主要基于英语和拉丁语，导致原生阿拉伯语模型缺乏，并受英语 tokens 影响，难以处理阿拉伯语的复杂形态和语法。为此，提出 ArabianGPT 系列模型，作为 ArabianLLM 的一部分，包括 ArabianGPT-0.1B 和 ArabianGPT-0.3B 等变体，这些模型采用 transformer 架构并整合 AraNizer tokenizer，以更好地适应阿拉伯语的语言特性。实验结果显示，通过微调，ArabianGPT-0.1B 在情感分析任务上准确率从 56% 提升至 95%，并在总结任务中获得更高的 F1 scores；此外，与基线模型比较，微调显著提高了问答和总结等任务的表现，突显了针对阿拉伯语的定制化 LLM 在自然语言处理 (NLP) 领域的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15313v2",
      "published_date": "2024-02-23 13:32:47 UTC",
      "updated_date": "2024-02-26 09:54:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:27:22.663592"
    },
    {
      "arxiv_id": "2402.16891v2",
      "title": "Multi-Task Learning for Routing Problem with Cross-Problem Zero-Shot Generalization",
      "title_zh": "翻译失败",
      "authors": [
        "Fei Liu",
        "Xi Lin",
        "Zhenkun Wang",
        "Qingfu Zhang",
        "Xialiang Tong",
        "Mingxuan Yuan"
      ],
      "abstract": "Vehicle routing problems (VRPs), which can be found in numerous real-world\napplications, have been an important research topic for several decades.\nRecently, the neural combinatorial optimization (NCO) approach that leverages a\nlearning-based model to solve VRPs without manual algorithm design has gained\nsubstantial attention. However, current NCO methods typically require building\none model for each routing problem, which significantly hinders their practical\napplication for real-world industry problems with diverse attributes. In this\nwork, we make the first attempt to tackle the crucial challenge of\ncross-problem generalization. In particular, we formulate VRPs as different\ncombinations of a set of shared underlying attributes and solve them\nsimultaneously via a single model through attribute composition. In this way,\nour proposed model can successfully solve VRPs with unseen attribute\ncombinations in a zero-shot generalization manner. Extensive experiments are\nconducted on eleven VRP variants, benchmark datasets, and industry logistic\nscenarios. The results show that the unified model demonstrates superior\nperformance in the eleven VRPs, reducing the average gap to around 5% from over\n20% in the existing approach and achieving a significant performance boost on\nbenchmark datasets as well as a real-world logistics application. The source\ncode is included in https://github.com/FeiLiu36/MTNCO.",
      "tldr_zh": "该研究针对车辆路径问题 (VRPs) 的神经组合优化 (NCO) 方法提出了一种多任务学习 (Multi-Task Learning) 框架，旨在解决现有模型需为每个问题单独构建的局限性。通过将 VRPs 视为共享属性的不同组合，该框架使用单一模型同时处理多个问题，实现跨问题 zero-shot generalization。实验在 11 个 VRP 变体上显示，该模型将平均性能差距从超过 20% 降至约 5%，并在基准数据集和实际物流场景中表现出显著提升，为实际应用提供了高效解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.16891v2",
      "published_date": "2024-02-23 13:25:23 UTC",
      "updated_date": "2024-04-12 15:34:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:27:33.616259"
    },
    {
      "arxiv_id": "2402.15307v1",
      "title": "Representing Online Handwriting for Recognition in Large Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Anastasiia Fadeeva",
        "Philippe Schlattner",
        "Andrii Maksai",
        "Mark Collier",
        "Efi Kokiopoulou",
        "Jesse Berent",
        "Claudiu Musat"
      ],
      "abstract": "The adoption of tablets with touchscreens and styluses is increasing, and a\nkey feature is converting handwriting to text, enabling search, indexing, and\nAI assistance. Meanwhile, vision-language models (VLMs) are now the go-to\nsolution for image understanding, thanks to both their state-of-the-art\nperformance across a variety of tasks and the simplicity of a unified approach\nto training, fine-tuning, and inference. While VLMs obtain high performance on\nimage-based tasks, they perform poorly on handwriting recognition when applied\nnaively, i.e., by rendering handwriting as an image and performing optical\ncharacter recognition (OCR). In this paper, we study online handwriting\nrecognition with VLMs, going beyond naive OCR. We propose a novel tokenized\nrepresentation of digital ink (online handwriting) that includes both a\ntime-ordered sequence of strokes as text, and as image. We show that this\nrepresentation yields results comparable to or better than state-of-the-art\nonline handwriting recognizers. Wide applicability is shown through results\nwith two different VLM families, on multiple public datasets. Our approach can\nbe applied to off-the-shelf VLMs, does not require any changes in their\narchitecture, and can be used in both fine-tuning and parameter-efficient\ntuning. We perform a detailed ablation study to identify the key elements of\nthe proposed representation.",
      "tldr_zh": "该论文探讨了如何在大型视觉语言模型 (VLMs) 中提升在线手写识别性能，因为直接使用光学字符识别 (OCR) 方法效果较差。研究提出了一种新型 tokenized representation of digital ink，将手写笔画表示为时间顺序的文本和图像序列，这种方法无需修改 VLM 架构，便于 fine-tuning 和 parameter-efficient tuning，并在多个公共数据集上实现了与或优于现有技术的识别准确率。通过详细的 ablation study，论文确认了该表示方法的關鍵元素，为手写识别应用提供了更广泛的适用性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15307v1",
      "published_date": "2024-02-23 13:11:10 UTC",
      "updated_date": "2024-02-23 13:11:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:27:46.513848"
    },
    {
      "arxiv_id": "2402.15546v1",
      "title": "HiMAP: Learning Heuristics-Informed Policies for Large-Scale Multi-Agent Pathfinding",
      "title_zh": "翻译失败",
      "authors": [
        "Huijie Tang",
        "Federico Berto",
        "Zihan Ma",
        "Chuanbo Hua",
        "Kyuree Ahn",
        "Jinkyoo Park"
      ],
      "abstract": "Large-scale multi-agent pathfinding (MAPF) presents significant challenges in\nseveral areas. As systems grow in complexity with a multitude of autonomous\nagents operating simultaneously, efficient and collision-free coordination\nbecomes paramount. Traditional algorithms often fall short in scalability,\nespecially in intricate scenarios. Reinforcement Learning (RL) has shown\npotential to address the intricacies of MAPF; however, it has also been shown\nto struggle with scalability, demanding intricate implementation, lengthy\ntraining, and often exhibiting unstable convergence, limiting its practical\napplication. In this paper, we introduce Heuristics-Informed Multi-Agent\nPathfinding (HiMAP), a novel scalable approach that employs imitation learning\nwith heuristic guidance in a decentralized manner. We train on small-scale\ninstances using a heuristic policy as a teacher that maps each single agent\nobservation information to an action probability distribution. During\npathfinding, we adopt several inference techniques to improve performance. With\na simple training scheme and implementation, HiMAP demonstrates competitive\nresults in terms of success rate and scalability in the field of\nimitation-learning-only MAPF, showing the potential of imitation-learning-only\nMAPF equipped with inference techniques.",
      "tldr_zh": "这篇论文针对大规模多智能体路径规划（MAPF）的可扩展性挑战，提出了 HiMAP 框架，该框架采用模仿学习结合启发式指导，在分散式环境中训练代理，将每个代理的观察信息映射到动作概率分布，并通过推理技术提升路径规划性能。不同于传统算法和 Reinforcement Learning (RL) 的局限性，HiMAP 采用简单训练方案，在小规模实例上进行学习。实验结果显示，HiMAP 在成功率和可扩展性上表现出色，展示了仅基于模仿学习的 MAPF 潜力。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.MA",
      "comment": "Accepted as Extended Abstract in Proc. of the 23rd International\n  Conference on Autonomous Agents and Multiagent Systems (AAMAS 2024)",
      "pdf_url": "http://arxiv.org/pdf/2402.15546v1",
      "published_date": "2024-02-23 13:01:13 UTC",
      "updated_date": "2024-02-23 13:01:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:27:57.894720"
    },
    {
      "arxiv_id": "2402.15300v2",
      "title": "Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding",
      "title_zh": "眼见为实：通过 CLIP-Guided Decoding 缓解大型视觉语言模型中的幻觉",
      "authors": [
        "Ailin Deng",
        "Zhirui Chen",
        "Bryan Hooi"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) are susceptible to object\nhallucinations, an issue in which their generated text contains non-existent\nobjects, greatly limiting their reliability and practicality. Current\napproaches often rely on the model's token likelihoods or other internal\ninformation, instruction tuning on additional datasets, or incorporating\ncomplex external tools. We first perform empirical analysis on sentence-level\nLVLM hallucination, finding that CLIP similarity to the image acts as a\nstronger and more robust indicator of hallucination compared to token\nlikelihoods. Motivated by this, we introduce our CLIP-Guided Decoding (CGD)\napproach, a straightforward but effective training-free approach to reduce\nobject hallucination at decoding time. CGD uses CLIP to guide the model's\ndecoding process by enhancing visual grounding of generated text with the\nimage. Experiments demonstrate that CGD effectively mitigates object\nhallucination across multiple LVLM families while preserving the utility of\ntext generation. Codes are available at\nhttps://github.com/d-ailin/CLIP-Guided-Decoding.",
      "tldr_zh": "Large Vision-Language Models (LVLMs) 容易出现对象幻觉（object hallucinations），导致生成的文本包含不存在的对象，从而影响其可靠性和实用性。作者通过实证分析发现，CLIP 相似度比 token 可能性更能有效指示幻觉，并提出 CLIP-Guided Decoding (CGD) 方法，这是一种无需训练的解码指导技术，利用 CLIP 增强生成的文本与图像的视觉 grounding。实验结果表明，CGD 在多个 LVLM 系列中显著减少对象幻觉，同时保持文本生成的整体效用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Code URL: https://github.com/d-ailin/CLIP-Guided-Decoding",
      "pdf_url": "http://arxiv.org/pdf/2402.15300v2",
      "published_date": "2024-02-23 12:57:16 UTC",
      "updated_date": "2024-04-23 09:32:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:28:11.690983"
    },
    {
      "arxiv_id": "2402.15294v1",
      "title": "A Survey of Music Generation in the Context of Interaction",
      "title_zh": "互动语",
      "authors": [
        "Ismael Agchar",
        "Ilja Baumann",
        "Franziska Braun",
        "Paula Andrea Perez-Toro",
        "Korbinian Riedhammer",
        "Sebastian Trump",
        "Martin Ullrich"
      ],
      "abstract": "In recent years, machine learning, and in particular generative adversarial\nneural networks (GANs) and attention-based neural networks (transformers), have\nbeen successfully used to compose and generate music, both melodies and\npolyphonic pieces. Current research focuses foremost on style replication (eg.\ngenerating a Bach-style chorale) or style transfer (eg. classical to jazz)\nbased on large amounts of recorded or transcribed music, which in turn also\nallows for fairly straight-forward \"performance\" evaluation. However, most of\nthese models are not suitable for human-machine co-creation through live\ninteraction, neither is clear, how such models and resulting creations would be\nevaluated. This article presents a thorough review of music representation,\nfeature analysis, heuristic algorithms, statistical and parametric modelling,\nand human and automatic evaluation measures, along with a discussion of which\napproaches and models seem most suitable for live interaction.",
      "tldr_zh": "这篇综述文章探讨了音乐生成在互动上下文中的应用，重点回顾了机器学习技术如 GANs 和 transformers 在作曲和生成旋律或复调音乐方面的进展。文章强调当前模型主要用于风格复制（如生成巴赫风格的合唱曲）或风格转移（如古典转爵士），但这些方法基于大量数据并易于评估，却不适合实时人机互动。作者分析了音乐表示、特征分析、启发式算法、统计和参数建模，以及人类和自动评估措施，并讨论了哪些方法最适用于活互动场景，以推动未来的人机共创研究。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15294v1",
      "published_date": "2024-02-23 12:41:44 UTC",
      "updated_date": "2024-02-23 12:41:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:28:21.824756"
    },
    {
      "arxiv_id": "2402.15290v4",
      "title": "Efficient State Space Model via Fast Tensor Convolution and Block Diagonalization",
      "title_zh": "翻译失败",
      "authors": [
        "Tongyi Liang",
        "Han-Xiong Li"
      ],
      "abstract": "Existing models encounter bottlenecks in balancing performance and\ncomputational efficiency when modeling long sequences. Although the state space\nmodel (SSM) has achieved remarkable success in handling long sequence tasks, it\nstill faces the problem of large number of parameters. In order to further\nimprove the efficiency of SSM, we propose a new state space layer based on\nmultiple-input multiple-output SSM, called efficient SSM (eSSM). Our eSSM is\nbuilt on the convolutional representation of multi-input and multi-input (MIMO)\nSSM. We propose a variety of effective strategies to improve the computational\nefficiency. The diagonalization of the system matrix first decouples the\noriginal system. Then a fast tensor convolution is proposed based on the fast\nFourier transform. In addition, the block diagonalization of the SSM further\nreduces the model parameters and improves the model flexibility. Extensive\nexperimental results show that the performance of the proposed model on\nmultiple databases matches the performance of state-of-the-art models, such as\nS4, and is significantly better than Transformers and LSTM. In the model\nefficiency benchmark, the parameters of eSSM are only 12.89\\% of LSTM and\n13.24\\% of Mamba. The training speed of eSSM is 3.94 times faster than LSTM and\n1.35 times faster than Mamba. Code is available at:\n\\href{https://github.com/leonty1/essm}{https://github.com/leonty1/essm}.",
      "tldr_zh": "本论文针对状态空间模型（SSM）在处理长序列任务时参数过多的问题，提出了一种高效的SSM变体eSSM，基于多输入多输出SSM（MIMO SSM）的卷积表示。论文引入了系统矩阵对角化、基于快速傅里叶变换（FFT）的快速张量卷积以及块对角化等策略，以减少模型参数并提升计算效率。实验结果表明，eSSM在多个数据库上的性能与S4等最先进模型相当，且优于Transformers和LSTM；在效率基准中，eSSM的参数仅为LSTM的12.89%和Mamba的13.24%，训练速度分别是LSTM的3.94倍和Mamba的1.35倍。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15290v4",
      "published_date": "2024-02-23 12:36:31 UTC",
      "updated_date": "2025-05-05 02:20:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:28:36.341559"
    },
    {
      "arxiv_id": "2402.15284v1",
      "title": "Spatiotemporal Observer Design for Predictive Learning of High-Dimensional Data",
      "title_zh": "用于高维数据预测学习的时空观察器设计",
      "authors": [
        "Tongyi Liang",
        "Han-Xiong Li"
      ],
      "abstract": "Although deep learning-based methods have shown great success in\nspatiotemporal predictive learning, the framework of those models is designed\nmainly by intuition. How to make spatiotemporal forecasting with theoretical\nguarantees is still a challenging issue. In this work, we tackle this problem\nby applying domain knowledge from the dynamical system to the framework design\nof deep learning models. An observer theory-guided deep learning architecture,\ncalled Spatiotemporal Observer, is designed for predictive learning of high\ndimensional data. The characteristics of the proposed framework are twofold:\nfirstly, it provides the generalization error bound and convergence guarantee\nfor spatiotemporal prediction; secondly, dynamical regularization is introduced\nto enable the model to learn system dynamics better during training. Further\nexperimental results show that this framework could capture the spatiotemporal\ndynamics and make accurate predictions in both one-step-ahead and\nmulti-step-ahead forecasting scenarios.",
      "tldr_zh": "本文提出了一种名为 Spatiotemporal Observer 的深度学习架构，用于高维数据的时空预测学习，通过借鉴动力系统领域的观察者理论来解决现有模型缺乏理论保证的问题。该框架的关键特点包括提供时空预测的泛化误差界和收敛保证，以及引入动态正则化以更好地学习系统动态。实验结果表明，该方法能够准确捕捉时空动态，并在一步预测和多步预测场景中实现更精确的预测。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "Under review by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence",
      "pdf_url": "http://arxiv.org/pdf/2402.15284v1",
      "published_date": "2024-02-23 12:28:31 UTC",
      "updated_date": "2024-02-23 12:28:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:28:46.060015"
    },
    {
      "arxiv_id": "2402.15283v1",
      "title": "When in Doubt, Think Slow: Iterative Reasoning with Latent Imagination",
      "title_zh": "翻译失败",
      "authors": [
        "Martin Benfeghoul",
        "Umais Zahid",
        "Qinghai Guo",
        "Zafeirios Fountas"
      ],
      "abstract": "In an unfamiliar setting, a model-based reinforcement learning agent can be\nlimited by the accuracy of its world model. In this work, we present a novel,\ntraining-free approach to improving the performance of such agents separately\nfrom planning and learning. We do so by applying iterative inference at\ndecision-time, to fine-tune the inferred agent states based on the coherence of\nfuture state representations. Our approach achieves a consistent improvement in\nboth reconstruction accuracy and task performance when applied to visual 3D\nnavigation tasks. We go on to show that considering more future states further\nimproves the performance of the agent in partially-observable environments, but\nnot in a fully-observable one. Finally, we demonstrate that agents with less\ntraining pre-evaluation benefit most from our approach.",
      "tldr_zh": "该研究提出了一种无需额外训练的迭代推理方法，通过潜在想象(latent imagination)微调强化学习代理的推断状态，以提升模型-based reinforcement learning 代理在不确定环境中的性能。该方法基于未来状态表示的连贯性进行优化，在视觉 3D navigation 任务中显著提高了重建准确性和任务表现。实验结果显示，考虑更多未来状态在部分可观察(partially-observable)环境中进一步提升代理性能，但在完全可观察(fully-observable)环境中无效，且训练较少的代理从中获益最大。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.0; I.2.8; I.2.10; I.4.5; I.4.10"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15283v1",
      "published_date": "2024-02-23 12:27:48 UTC",
      "updated_date": "2024-02-23 12:27:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:28:57.736943"
    },
    {
      "arxiv_id": "2402.15276v3",
      "title": "CFIR: Fast and Effective Long-Text To Image Retrieval for Large Corpora",
      "title_zh": "CFIR：快速且有效的长文本到图像检索，用于大型语料库",
      "authors": [
        "Zijun Long",
        "Xuri Ge",
        "Richard Mccreadie",
        "Joemon Jose"
      ],
      "abstract": "Text-to-image retrieval aims to find the relevant images based on a text\nquery, which is important in various use-cases, such as digital libraries,\ne-commerce, and multimedia databases. Although Multimodal Large Language Models\n(MLLMs) demonstrate state-of-the-art performance, they exhibit limitations in\nhandling large-scale, diverse, and ambiguous real-world needs of retrieval, due\nto the computation cost and the injective embeddings they produce. This paper\npresents a two-stage Coarse-to-Fine Index-shared Retrieval (CFIR) framework,\ndesigned for fast and effective large-scale long-text to image retrieval. The\nfirst stage, Entity-based Ranking (ER), adapts to long-text query ambiguity by\nemploying a multiple-queries-to-multiple-targets paradigm, facilitating\ncandidate filtering for the next stage. The second stage, Summary-based\nRe-ranking (SR), refines these rankings using summarized queries. We also\npropose a specialized Decoupling-BEiT-3 encoder, optimized for handling\nambiguous user needs and both stages, which also enhances computational\nefficiency through vector-based similarity inference. Evaluation on the AToMiC\ndataset reveals that CFIR surpasses existing MLLMs by up to 11.06% in\nRecall@1000, while reducing training and retrieval times by 68.75% and 99.79%,\nrespectively. We will release our code to facilitate future research at\nhttps://github.com/longkukuhi/CFIR.",
      "tldr_zh": "本论文提出CFIR框架，一种快速有效的长文本到图像检索方法，针对Multimodal Large Language Models (MLLMs)在处理大规模、模糊查询时的计算成本和嵌入局限性问题。CFIR采用两阶段设计：第一阶段的Entity-based Ranking (ER)通过多查询到多目标范式过滤候选图像，以应对查询模糊性；第二阶段的Summary-based Re-ranking (SR)使用总结化查询进一步精炼排名。同时，引入Decoupling-BEiT-3编码器优化模糊用户需求处理，并通过向量-based相似性推理提升计算效率。在AToMiC数据集上，CFIR比现有MLLMs在Recall@1000上提高多达11.06%，并分别减少训练时间68.75%和检索时间99.79%。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15276v3",
      "published_date": "2024-02-23 11:47:16 UTC",
      "updated_date": "2024-04-02 20:54:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:29:11.385635"
    },
    {
      "arxiv_id": "2402.15272v1",
      "title": "EMIFF: Enhanced Multi-scale Image Feature Fusion for Vehicle-Infrastructure Cooperative 3D Object Detection",
      "title_zh": "EMIFF：增强多尺度图像特征融合用于车辆-基础设施合作3D对象检测",
      "authors": [
        "Zhe Wang",
        "Siqi Fan",
        "Xiaoliang Huo",
        "Tongda Xu",
        "Yan Wang",
        "Jingjing Liu",
        "Yilun Chen",
        "Ya-Qin Zhang"
      ],
      "abstract": "In autonomous driving, cooperative perception makes use of multi-view cameras\nfrom both vehicles and infrastructure, providing a global vantage point with\nrich semantic context of road conditions beyond a single vehicle viewpoint.\nCurrently, two major challenges persist in vehicle-infrastructure cooperative\n3D (VIC3D) object detection: $1)$ inherent pose errors when fusing multi-view\nimages, caused by time asynchrony across cameras; $2)$ information loss in\ntransmission process resulted from limited communication bandwidth. To address\nthese issues, we propose a novel camera-based 3D detection framework for VIC3D\ntask, Enhanced Multi-scale Image Feature Fusion (EMIFF). To fully exploit\nholistic perspectives from both vehicles and infrastructure, we propose\nMulti-scale Cross Attention (MCA) and Camera-aware Channel Masking (CCM)\nmodules to enhance infrastructure and vehicle features at scale, spatial, and\nchannel levels to correct the pose error introduced by camera asynchrony. We\nalso introduce a Feature Compression (FC) module with channel and spatial\ncompression blocks for transmission efficiency. Experiments show that EMIFF\nachieves SOTA on DAIR-V2X-C datasets, significantly outperforming previous\nearly-fusion and late-fusion methods with comparable transmission costs.",
      "tldr_zh": "该研究针对自动驾驶中的车辆-基础设施合作3D（VIC3D）物体检测，解决了多视图图像融合中的姿态错误（由于摄像头不同步）和传输过程中的信息损失问题。论文提出了一种新型框架EMIFF（Enhanced Multi-scale Image Feature Fusion），通过Multi-scale Cross Attention (MCA) 和Camera-aware Channel Masking (CCM) 模块，在尺度、空间和通道层面增强特征融合，以纠正姿态错误；同时引入Feature Compression (FC) 模块实现高效传输。实验结果显示，EMIFF在DAIR-V2X-C数据集上达到SOTA（State-of-the-Art）性能，显著优于早融合和晚融合方法，同时保持可比的传输成本。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "7 pages, 8 figures. Accepted by ICRA 2024. arXiv admin note: text\n  overlap with arXiv:arXiv:2303.10975",
      "pdf_url": "http://arxiv.org/pdf/2402.15272v1",
      "published_date": "2024-02-23 11:35:48 UTC",
      "updated_date": "2024-02-23 11:35:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:29:23.826771"
    },
    {
      "arxiv_id": "2402.15270v2",
      "title": "Smoothed Graph Contrastive Learning via Seamless Proximity Integration",
      "title_zh": "翻译失败",
      "authors": [
        "Maysam Behmanesh",
        "Maks Ovsjanikov"
      ],
      "abstract": "Graph contrastive learning (GCL) aligns node representations by classifying\nnode pairs into positives and negatives using a selection process that\ntypically relies on establishing correspondences within two augmented graphs.\nThe conventional GCL approaches incorporate negative samples uniformly in the\ncontrastive loss, resulting in the equal treatment of negative nodes,\nregardless of their proximity to the true positive. In this paper, we present a\nSmoothed Graph Contrastive Learning model (SGCL), which leverages the geometric\nstructure of augmented graphs to inject proximity information associated with\npositive/negative pairs in the contrastive loss, thus significantly\nregularizing the learning process. The proposed SGCL adjusts the penalties\nassociated with node pairs in contrastive loss by incorporating three distinct\nsmoothing techniques that result in proximity-aware positives and negatives. To\nenhance scalability for large-scale graphs, the proposed framework incorporates\na graph batch-generating strategy that partitions the given graphs into\nmultiple subgraphs, facilitating efficient training in separate batches.\nThrough extensive experimentation in the unsupervised setting on various\nbenchmarks, particularly those of large scale, we demonstrate the superiority\nof our proposed framework against recent baselines.",
      "tldr_zh": "该论文提出了一种Smoothed Graph Contrastive Learning (SGCL)模型，通过无缝整合邻近性信息来改进传统的Graph Contrastive Learning (GCL)。SGCL在对比损失中注入正负样本的几何结构信息，并采用三种平滑技术来调整节点对的惩罚，使学习过程更注重样本邻近性，同时引入图批量生成策略以处理大规模图的训练效率。实验结果显示，在无监督设置下，SGCL在各种基准上，特别是大型图数据集，显著优于现有基线模型。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "26 pages",
      "pdf_url": "http://arxiv.org/pdf/2402.15270v2",
      "published_date": "2024-02-23 11:32:46 UTC",
      "updated_date": "2024-11-26 14:50:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:29:35.552723"
    },
    {
      "arxiv_id": "2402.15268v1",
      "title": "MemoryPrompt: A Light Wrapper to Improve Context Tracking in Pre-trained Language Models",
      "title_zh": "MemoryPrompt：一种",
      "authors": [
        "Nathanaël Carraz Rakotonirina",
        "Marco Baroni"
      ],
      "abstract": "Transformer-based language models (LMs) track contextual information through\nlarge, hard-coded input windows. We introduce MemoryPrompt, a leaner approach\nin which the LM is complemented by a small auxiliary recurrent network that\npasses information to the LM by prefixing its regular input with a sequence of\nvectors, akin to soft prompts, without requiring LM finetuning. Tested on a\ntask designed to probe a LM's ability to keep track of multiple fact updates, a\nMemoryPrompt-augmented LM outperforms much larger LMs that have access to the\nfull input history. We also test MemoryPrompt on a long-distance dialogue\ndataset, where its performance is comparable to that of a model conditioned on\nthe entire conversation history. In both experiments we also observe that,\nunlike full-finetuning approaches, MemoryPrompt does not suffer from\ncatastrophic forgetting when adapted to new tasks, thus not disrupting the\ngeneralist capabilities of the underlying LM.",
      "tldr_zh": "本文提出 MemoryPrompt，一种轻量级包装器，用于提升预训练语言模型（LMs）在上下文跟踪方面的性能。它通过一个小型辅助循环网络在模型输入前添加软提示序列来传递信息，而无需对 LMs 进行微调。实验结果显示，在多事实更新任务中，MemoryPrompt 增强的模型超过了访问完整输入历史的更大模型；在长距离对话数据集上，其表现与依赖整个对话历史的模型相当。此外，MemoryPrompt 在适应新任务时避免了灾难性遗忘，从而保留了底层 LMs 的通用能力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Published as conference paper at LREC-COLING 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.15268v1",
      "published_date": "2024-02-23 11:30:39 UTC",
      "updated_date": "2024-02-23 11:30:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:29:49.029101"
    },
    {
      "arxiv_id": "2402.15267v2",
      "title": "A Robust Defense against Adversarial Attacks on Deep Learning-based Malware Detectors via (De)Randomized Smoothing",
      "title_zh": "翻译失败",
      "authors": [
        "Daniel Gibert",
        "Giulio Zizzo",
        "Quan Le",
        "Jordi Planes"
      ],
      "abstract": "Deep learning-based malware detectors have been shown to be susceptible to\nadversarial malware examples, i.e. malware examples that have been deliberately\nmanipulated in order to avoid detection. In light of the vulnerability of deep\nlearning detectors to subtle input file modifications, we propose a practical\ndefense against adversarial malware examples inspired by (de)randomized\nsmoothing. In this work, we reduce the chances of sampling adversarial content\ninjected by malware authors by selecting correlated subsets of bytes, rather\nthan using Gaussian noise to randomize inputs like in the Computer Vision (CV)\ndomain. During training, our ablation-based smoothing scheme trains a base\nclassifier to make classifications on a subset of contiguous bytes or chunk of\nbytes. At test time, a large number of chunks are then classified by a base\nclassifier and the consensus among these classifications is then reported as\nthe final prediction. We propose two strategies to determine the location of\nthe chunks used for classification: (1) randomly selecting the locations of the\nchunks and (2) selecting contiguous adjacent chunks. To showcase the\neffectiveness of our approach, we have trained two classifiers with our\nchunk-based ablation schemes on the BODMAS dataset. Our findings reveal that\nthe chunk-based smoothing classifiers exhibit greater resilience against\nadversarial malware examples generated with state-of-the-are evasion attacks,\noutperforming a non-smoothed classifier and a randomized smoothing-based\nclassifier by a great margin.",
      "tldr_zh": "本文提出了一种基于 (de)randomized smoothing 的鲁棒防御方法，用于对抗深度学习-based 恶意软件检测器的 adversarial attacks，通过选择相关的字节子集来减少采样对抗性内容的可能性。训练时采用 ablation-based smoothing 方案，让基分类器在连续字节块（chunks）上进行分类；测试时则对多个块分类并取共识作为最终预测。实验在 BODMAS 数据集上表明，该方法显著提升了分类器的鲁棒性，比非平滑分类器和传统随机平滑分类器表现出色。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "arXiv admin note: text overlap with arXiv:2308.08906",
      "pdf_url": "http://arxiv.org/pdf/2402.15267v2",
      "published_date": "2024-02-23 11:30:12 UTC",
      "updated_date": "2024-02-26 21:30:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:30:00.315920"
    },
    {
      "arxiv_id": "2402.15266v2",
      "title": "Calibration of Deep Learning Classification Models in fNIRS",
      "title_zh": "fNIRS 中深度学习分类模型的校准",
      "authors": [
        "Zhihao Cao",
        "Zizhou Luo"
      ],
      "abstract": "Functional near-infrared spectroscopy (fNIRS) is a valuable non-invasive tool\nfor monitoring brain activity. The classification of fNIRS data in relation to\nconscious activity holds significance for advancing our understanding of the\nbrain and facilitating the development of brain-computer interfaces (BCI). Many\nresearchers have turned to deep learning to tackle the classification\nchallenges inherent in fNIRS data due to its strong generalization and\nrobustness. In the application of fNIRS, reliability is really important, and\none mathematical formulation of the reliability of confidence is calibration.\nHowever, many researchers overlook the important issue of calibration. To\naddress this gap, we propose integrating calibration into fNIRS field and\nassess the reliability of existing models. Surprisingly, our results indicate\npoor calibration performance in many proposed models. To advance calibration\ndevelopment in the fNIRS field, we summarize three practical tips. Through this\nletter, we hope to emphasize the critical role of calibration in fNIRS research\nand argue for enhancing the reliability of deep learning-based predictions in\nfNIRS classification tasks. All data from our experimental process are openly\navailable on GitHub.",
      "tldr_zh": "这篇论文探讨了在功能近红外光谱(fNIRS)中深度学习分类模型的校准问题，强调校准(calibration)是确保模型可靠性的关键，以支持脑活动监测和脑机接口(BCI)的发展。作者评估了现有模型，发现许多模型的校准性能较差。论文提出了三个实用提示来改进校准，并公开了所有实验数据在GitHub上，以推动fNIRS领域深度学习应用的可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15266v2",
      "published_date": "2024-02-23 11:27:10 UTC",
      "updated_date": "2024-03-20 10:12:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:30:12.460835"
    },
    {
      "arxiv_id": "2402.15262v1",
      "title": "Dynamic Memory Based Adaptive Optimization",
      "title_zh": "基于动态记忆的自适应优化",
      "authors": [
        "Balázs Szegedy",
        "Domonkos Czifra",
        "Péter Kőrösi-Szabó"
      ],
      "abstract": "Define an optimizer as having memory $k$ if it stores $k$ dynamically\nchanging vectors in the parameter space. Classical SGD has memory $0$, momentum\nSGD optimizer has $1$ and Adam optimizer has $2$. We address the following\nquestions: How can optimizers make use of more memory units? What information\nshould be stored in them? How to use them for the learning steps? As an\napproach to the last question, we introduce a general method called\n\"Retrospective Learning Law Correction\" or shortly RLLC. This method is\ndesigned to calculate a dynamically varying linear combination (called learning\nlaw) of memory units, which themselves may evolve arbitrarily. We demonstrate\nRLLC on optimizers whose memory units have linear update rules and small memory\n($\\leq 4$ memory units). Our experiments show that in a variety of standard\nproblems, these optimizers outperform the above mentioned three classical\noptimizers. We conclude that RLLC is a promising framework for boosting the\nperformance of known optimizers by adding more memory units and by making them\nmore adaptive.",
      "tldr_zh": "本研究探讨了基于动态内存的优化器设计，将优化器定义为存储 k 个动态变化向量的系统，例如 SGD 有 0 内存单位、Momentum SGD 有 1 单位、Adam 有 2 单位。论文引入了“Retrospective Learning Law Correction”（RLLC）方法，该方法通过计算记忆单位的动态线性组合来增强优化器的适应性，并应用于具有线性更新规则的优化器（内存 ≤4 单位）。实验结果显示，这些新优化器在各种标准问题上优于经典的 SGD、Momentum SGD 和 Adam，提升了性能。总之，RLLC 框架为通过增加内存单位和提高适应性来改进现有优化器提供了有前景的途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15262v1",
      "published_date": "2024-02-23 11:19:02 UTC",
      "updated_date": "2024-02-23 11:19:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:30:23.962068"
    },
    {
      "arxiv_id": "2402.15255v2",
      "title": "Optimal Transport for Structure Learning Under Missing Data",
      "title_zh": "翻译失败",
      "authors": [
        "Vy Vo",
        "He Zhao",
        "Trung Le",
        "Edwin V. Bonilla",
        "Dinh Phung"
      ],
      "abstract": "Causal discovery in the presence of missing data introduces a chicken-and-egg\ndilemma. While the goal is to recover the true causal structure, robust\nimputation requires considering the dependencies or, preferably, causal\nrelations among variables. Merely filling in missing values with existing\nimputation methods and subsequently applying structure learning on the complete\ndata is empirically shown to be sub-optimal. To address this problem, we\npropose a score-based algorithm for learning causal structures from missing\ndata based on optimal transport. This optimal transport viewpoint diverges from\nexisting score-based approaches that are dominantly based on expectation\nmaximization. We formulate structure learning as a density fitting problem,\nwhere the goal is to find the causal model that induces a distribution of\nminimum Wasserstein distance with the observed data distribution. Our framework\nis shown to recover the true causal graphs more effectively than competing\nmethods in most simulations and real-data settings. Empirical evidence also\nshows the superior scalability of our approach, along with the flexibility to\nincorporate any off-the-shelf causal discovery methods for complete data.",
      "tldr_zh": "该论文针对缺失数据下的因果结构学习问题，提出了一种基于 Optimal Transport 的评分算法，以解决传统方法先填充缺失值再进行结构学习的不优性。该方法将结构学习表述为密度拟合问题，目标是找到使观察数据分布与因果模型分布的 Wasserstein 距离最小化的模型，从而更有效地恢复真实因果图。与基于期望最大化的现有方法不同，该框架在模拟和真实数据设置中表现出色，准确率更高，并具备更好的可扩展性和灵活性，能整合其他完整数据因果发现算法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15255v2",
      "published_date": "2024-02-23 10:49:04 UTC",
      "updated_date": "2024-06-01 10:57:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:30:35.361085"
    },
    {
      "arxiv_id": "2402.16889v1",
      "title": "Generative Models are Self-Watermarked: Declaring Model Authentication through Re-Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Aditya Desu",
        "Xuanli He",
        "Qiongkai Xu",
        "Wei Lu"
      ],
      "abstract": "As machine- and AI-generated content proliferates, protecting the\nintellectual property of generative models has become imperative, yet verifying\ndata ownership poses formidable challenges, particularly in cases of\nunauthorized reuse of generated data. The challenge of verifying data ownership\nis further amplified by using Machine Learning as a Service (MLaaS), which\noften functions as a black-box system.\n  Our work is dedicated to detecting data reuse from even an individual sample.\nTraditionally, watermarking has been leveraged to detect AI-generated content.\nHowever, unlike watermarking techniques that embed additional information as\ntriggers into models or generated content, potentially compromising output\nquality, our approach identifies latent fingerprints inherently present within\nthe outputs through re-generation. We propose an explainable verification\nprocedure that attributes data ownership through re-generation, and further\namplifies these fingerprints in the generative models through iterative data\nre-generation. This methodology is theoretically grounded and demonstrates\nviability and robustness using recent advanced text and image generative\nmodels. Our methodology is significant as it goes beyond protecting the\nintellectual property of APIs and addresses important issues such as the spread\nof misinformation and academic misconduct. It provides a useful tool to ensure\nthe integrity of sources and authorship, expanding its application in different\nscenarios where authenticity and ownership verification are essential.",
      "tldr_zh": "该研究提出了一种自水印(self-watermarked)方法，用于验证生成模型(generative models)的所有权，而无需嵌入额外信息，从而避免影响输出质量。方法通过再生成(re-generation)过程识别模型的潜在指纹(fingerprints)，并利用迭代数据再生成来放大这些指纹，提供一个可解释的验证程序。实验在先进文本和图像生成模型上验证了该方法的有效性和鲁棒性。该方法不仅保护API知识产权，还能解决误信息传播和学术不端问题，确保来源和作者的完整性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.16889v1",
      "published_date": "2024-02-23 10:48:21 UTC",
      "updated_date": "2024-02-23 10:48:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:30:48.472747"
    },
    {
      "arxiv_id": "2402.15247v1",
      "title": "A Bargaining-based Approach for Feature Trading in Vertical Federated Learning",
      "title_zh": "垂直联邦学习中基于讨价还价的特征交易方法",
      "authors": [
        "Yue Cui",
        "Liuyi Yao",
        "Zitao Li",
        "Yaliang Li",
        "Bolin Ding",
        "Xiaofang Zhou"
      ],
      "abstract": "Vertical Federated Learning (VFL) has emerged as a popular machine learning\nparadigm, enabling model training across the data and the task parties with\ndifferent features about the same user set while preserving data privacy. In\nproduction environment, VFL usually involves one task party and one data party.\nFair and economically efficient feature trading is crucial to the\ncommercialization of VFL, where the task party is considered as the data\nconsumer who buys the data party's features. However, current VFL feature\ntrading practices often price the data party's data as a whole and assume\ntransactions occur prior to the performing VFL. Neglecting the performance\ngains resulting from traded features may lead to underpayment and overpayment\nissues. In this study, we propose a bargaining-based feature trading approach\nin VFL to encourage economically efficient transactions. Our model incorporates\nperformance gain-based pricing, taking into account the revenue-based\noptimization objectives of both parties. We analyze the proposed bargaining\nmodel under perfect and imperfect performance information settings, proving the\nexistence of an equilibrium that optimizes the parties' objectives. Moreover,\nwe develop performance gain estimation-based bargaining strategies for\nimperfect performance information scenarios and discuss potential security\nissues and solutions. Experiments on three real-world datasets demonstrate the\neffectiveness of the proposed bargaining model.",
      "tldr_zh": "本论文针对 Vertical Federated Learning (VFL) 中的特征交易问题，提出了一种基于讨价还价的交易方法，以实现公平和经济高效的交易。该方法引入 performance gain-based pricing 机制，考虑任务方和数据方的收益优化目标，并在完美和不完美性能信息场景下分析了均衡存在性，并开发了基于性能提升估计的讨价还价策略，同时讨论了潜在安全问题和解决方案。实验在三个真实数据集上证明，该方法有效提升了交易效率和准确性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15247v1",
      "published_date": "2024-02-23 10:21:07 UTC",
      "updated_date": "2024-02-23 10:21:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:31:00.619282"
    },
    {
      "arxiv_id": "2402.15246v1",
      "title": "Artificial Bee Colony optimization of Deep Convolutional Neural Networks in the context of Biomedical Imaging",
      "title_zh": "翻译失败",
      "authors": [
        "Adri Gomez Martin",
        "Carlos Fernandez del Cerro",
        "Monica Abella Garcia",
        "Manuel Desco Menendez"
      ],
      "abstract": "Most efforts in Computer Vision focus on natural images or artwork, which\ndiffer significantly both in size and contents from the kind of data biomedical\nimage processing deals with. Thus, Transfer Learning models often prove\nthemselves suboptimal for these tasks, even after manual finetuning. The\ndevelopment of architectures from scratch is oftentimes unfeasible due to the\nvastness of the hyperparameter space and a shortage of time, computational\nresources and Deep Learning experts in most biomedical research laboratories.\nAn alternative to manually defining the models is the use of Neuroevolution,\nwhich employs metaheuristic techniques to optimize Deep Learning architectures.\nHowever, many algorithms proposed in the neuroevolutive literature are either\ntoo unreliable or limited to a small, predefined region of the hyperparameter\nspace. To overcome these shortcomings, we propose the Chimera Algorithm, a\nnovel, hybrid neuroevolutive algorithm that integrates the Artificial Bee\nColony Algorithm with Evolutionary Computation tools to generate models from\nscratch, as well as to refine a given previous architecture to better fit the\ntask at hand. The Chimera Algorithm has been validated with two datasets of\nnatural and medical images, producing models that surpassed the performance of\nthose coming from Transfer Learning.",
      "tldr_zh": "该研究针对生物医学图像处理中的挑战指出，Transfer Learning 模型在处理此类数据时表现不佳，因为其与自然图像存在显著差异，且从头开发Deep Convolutional Neural Networks 受限于超参数空间和资源不足。为解决这些问题，研究提出Chimera Algorithm，一种结合Artificial Bee Colony Algorithm 和Evolutionary Computation 的混合神经进化算法，用于从头生成或优化神经网络架构。在自然和医疗图像数据集上的实验验证显示，Chimera Algorithm 生成的模型超过了Transfer Learning 的性能表现。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15246v1",
      "published_date": "2024-02-23 10:21:03 UTC",
      "updated_date": "2024-02-23 10:21:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:31:12.036489"
    },
    {
      "arxiv_id": "2404.07217v2",
      "title": "Attention-aware Semantic Communications for Collaborative Inference",
      "title_zh": "注意力感知的语义通信用于协作推理",
      "authors": [
        "Jiwoong Im",
        "Nayoung Kwon",
        "Taewoo Park",
        "Jiheon Woo",
        "Jaeho Lee",
        "Yongjune Kim"
      ],
      "abstract": "We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.",
      "tldr_zh": "本研究提出了一种注意力感知语义通信框架，用于提升边缘推理中的协作效率，特别针对视觉Transformer (ViT) 模型。不同于传统分区策略，该框架在边缘设备部署轻量级 ViT 模型，而服务器使用复杂 ViT 模型，并引入两个关键策略：attention-aware patch selection，利用边缘设备的 transformer encoder 生成的 attention scores 选择并传输分类关键图像 patches，以显著降低通信开销；以及 entropy-aware image transmission，通过 min-entropy 指标决定是否依赖边缘模型或请求服务器推理。实验结果显示，在 ImageNet 数据集上，该框架比服务器模型减少 68% 的通信开销，同时仅以微小准确率损失为代价。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.07217v2",
      "published_date": "2024-02-23 10:08:45 UTC",
      "updated_date": "2024-05-31 14:23:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:31:24.058765"
    },
    {
      "arxiv_id": "2402.15227v1",
      "title": "Fixed Random Classifier Rearrangement for Continual Learning",
      "title_zh": "固定随机分类器重排用于持续学习",
      "authors": [
        "Shengyang Huang",
        "Jianwen Mo"
      ],
      "abstract": "With the explosive growth of data, continual learning capability is\nincreasingly important for neural networks. Due to catastrophic forgetting,\nneural networks inevitably forget the knowledge of old tasks after learning new\nones. In visual classification scenario, a common practice of alleviating the\nforgetting is to constrain the backbone. However, the impact of classifiers is\nunderestimated. In this paper, we analyze the variation of model predictions in\nsequential binary classification tasks and find that the norm of the equivalent\none-class classifiers significantly affects the forgetting level. Based on this\nconclusion, we propose a two-stage continual learning algorithm named Fixed\nRandom Classifier Rearrangement (FRCR). In first stage, FRCR replaces the\nlearnable classifiers with fixed random classifiers, constraining the norm of\nthe equivalent one-class classifiers without affecting the performance of the\nnetwork. In second stage, FRCR rearranges the entries of new classifiers to\nimplicitly reduce the drift of old latent representations. The experimental\nresults on multiple datasets show that FRCR significantly mitigates the model\nforgetting; subsequent experimental analyses further validate the effectiveness\nof the algorithm.",
      "tldr_zh": "这篇论文针对神经网络在持续学习中的灾难性遗忘（catastrophic forgetting）问题，分析了分类器在视觉分类任务中的关键作用，发现等效一类分类器的范数会显著影响遗忘水平。作者提出了一种两阶段算法Fixed Random Classifier Rearrangement (FRCR)：第一阶段用固定随机分类器替换可学习分类器，以约束范数而不影响网络性能；第二阶段重新排列新分类器的条目，隐式减少旧潜在表示的漂移。实验结果显示，FRCR在多个数据集上显著缓解了模型遗忘，并通过后续分析验证了算法的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15227v1",
      "published_date": "2024-02-23 09:43:58 UTC",
      "updated_date": "2024-02-23 09:43:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:31:37.353565"
    },
    {
      "arxiv_id": "2403.14650v1",
      "title": "Harnessing the Computing Continuum across Personalized Healthcare, Maintenance and Inspection, and Farming 4.0",
      "title_zh": "翻译失败",
      "authors": [
        "Fatemeh Baghdadi",
        "Davide Cirillo",
        "Daniele Lezzi",
        "Francesc Lordan",
        "Fernando Vazquez",
        "Eugenio Lomurno",
        "Alberto Archetti",
        "Danilo Ardagna",
        "Matteo Matteucci"
      ],
      "abstract": "The AI-SPRINT project, launched in 2021 and funded by the European\nCommission, focuses on the development and implementation of AI applications\nacross the computing continuum. This continuum ensures the coherent integration\nof computational resources and services from centralized data centers to edge\ndevices, facilitating efficient and adaptive computation and application\ndelivery. AI-SPRINT has achieved significant scientific advances, including\nstreamlined processes, improved efficiency, and the ability to operate in real\ntime, as evidenced by three practical use cases. This paper provides an\nin-depth examination of these applications -- Personalized Healthcare,\nMaintenance and Inspection, and Farming 4.0 -- highlighting their practical\nimplementation and the objectives achieved with the integration of AI-SPRINT\ntechnologies. We analyze how the proposed toolchain effectively addresses a\nrange of challenges and refines processes, discussing its relevance and impact\nin multiple domains. After a comprehensive overview of the main AI-SPRINT tools\nused in these scenarios, the paper summarizes of the findings and key lessons\nlearned.",
      "tldr_zh": "AI-SPRINT 项目由欧盟委员会资助，专注于在 Computing Continuum 上开发 AI 应用，该框架整合了从数据中心到边缘设备的计算资源，以实现高效的自适应计算和应用交付。论文详细探讨了三个实际用例——Personalized Healthcare、Maintenance and Inspection 以及 Farming 4.0——展示了如何通过 AI-SPRINT 工具链简化流程、提升效率并实现实时操作。研究分析了这些应用面对的挑战及其解决方案，并总结了关键发现和经验教训，强调了该技术在多个领域的相关性和影响。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.14650v1",
      "published_date": "2024-02-23 09:20:34 UTC",
      "updated_date": "2024-02-23 09:20:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:31:48.434628"
    },
    {
      "arxiv_id": "2402.16887v1",
      "title": "Artificial Intelligence for Complex Network: Potential, Methodology and Application",
      "title_zh": "翻译失败",
      "authors": [
        "Jingtao Ding",
        "Chang Liu",
        "Yu Zheng",
        "Yunke Zhang",
        "Zihan Yu",
        "Ruikun Li",
        "Hongyi Chen",
        "Jinghua Piao",
        "Huandong Wang",
        "Jiazhen Liu",
        "Yong Li"
      ],
      "abstract": "Complex networks pervade various real-world systems, from the natural\nenvironment to human societies. The essence of these networks is in their\nability to transition and evolve from microscopic disorder-where network\ntopology and node dynamics intertwine-to a macroscopic order characterized by\ncertain collective behaviors. Over the past two decades, complex network\nscience has significantly enhanced our understanding of the statistical\nmechanics, structures, and dynamics underlying real-world networks. Despite\nthese advancements, there remain considerable challenges in exploring more\nrealistic systems and enhancing practical applications. The emergence of\nartificial intelligence (AI) technologies, coupled with the abundance of\ndiverse real-world network data, has heralded a new era in complex network\nscience research. This survey aims to systematically address the potential\nadvantages of AI in overcoming the lingering challenges of complex network\nresearch. It endeavors to summarize the pivotal research problems and provide\nan exhaustive review of the corresponding methodologies and applications.\nThrough this comprehensive survey-the first of its kind on AI for complex\nnetworks-we expect to provide valuable insights that will drive further\nresearch and advancement in this interdisciplinary field.",
      "tldr_zh": "这篇论文探讨了人工智能（AI）在复杂网络研究中的潜力、方法和应用，强调AI如何利用丰富的数据来克服现有挑战，如探索更现实的系统和提升实际应用。作者回顾了过去20年复杂网络科学的发展，从微观无序到宏观有序的演化，并系统总结了关键研究问题。论文提供了详尽的AI方法论和应用案例，作为首篇此类全面综述，为推动这一跨学科领域的研究提供宝贵见解。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.LG",
        "physics.soc-ph"
      ],
      "primary_category": "cs.SI",
      "comment": "51 pages, 4 figures, 10 tables",
      "pdf_url": "http://arxiv.org/pdf/2402.16887v1",
      "published_date": "2024-02-23 09:06:36 UTC",
      "updated_date": "2024-02-23 09:06:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:32:00.445754"
    },
    {
      "arxiv_id": "2402.15205v1",
      "title": "Enhancing ICU Patient Recovery: Using LLMs to Assist Nurses in Diary Writing",
      "title_zh": "翻译失败",
      "authors": [
        "Samuel Kernan Freire",
        "Margo MC van Mol",
        "Carola Schol",
        "Elif Özcan Vieira"
      ],
      "abstract": "Intensive care unit (ICU) patients often develop new health-related problems\nin their long-term recovery. Health care professionals keeping a diary of a\npatient's stay is a proven strategy to tackle this but faces several adoption\nbarriers, such as lack of time and difficulty in knowing what to write. Large\nlanguage models (LLMs), with their ability to generate human-like text and\nadaptability, could solve these challenges. However, realizing this vision\ninvolves addressing several socio-technical and practical research challenges.\nThis paper discusses these challenges and proposes future research directions\nto utilize the potential of LLMs in ICU diary writing, ultimately improving the\nlong-term recovery outcomes for ICU patients.",
      "tldr_zh": "这项研究探讨了如何利用大语言模型 (LLMs) 辅助护士撰写 ICU 患者住院日记，以缓解患者长期恢复中出现的新健康问题。传统日记策略虽已证明有效，但面临时间不足和内容不确定等障碍，而 LLMs 的文本生成和适应能力可有效解决这些挑战。论文分析了相关社会技术难题，并提出未来研究方向，最终旨在提升 ICU 患者的长期恢复效果。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "3 pages, under review",
      "pdf_url": "http://arxiv.org/pdf/2402.15205v1",
      "published_date": "2024-02-23 09:06:25 UTC",
      "updated_date": "2024-02-23 09:06:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:32:11.316003"
    },
    {
      "arxiv_id": "2402.15197v1",
      "title": "Safety Optimized Reinforcement Learning via Multi-Objective Policy Optimization",
      "title_zh": "通过多目标策略优",
      "authors": [
        "Homayoun Honari",
        "Mehran Ghafarian Tamizi",
        "Homayoun Najjaran"
      ],
      "abstract": "Safe reinforcement learning (Safe RL) refers to a class of techniques that\naim to prevent RL algorithms from violating constraints in the process of\ndecision-making and exploration during trial and error. In this paper, a novel\nmodel-free Safe RL algorithm, formulated based on the multi-objective policy\noptimization framework is introduced where the policy is optimized towards\noptimality and safety, simultaneously. The optimality is achieved by the\nenvironment reward function that is subsequently shaped using a safety critic.\nThe advantage of the Safety Optimized RL (SORL) algorithm compared to the\ntraditional Safe RL algorithms is that it omits the need to constrain the\npolicy search space. This allows SORL to find a natural tradeoff between safety\nand optimality without compromising the performance in terms of either safety\nor optimality due to strict search space constraints. Through our theoretical\nanalysis of SORL, we propose a condition for SORL's converged policy to\nguarantee safety and then use it to introduce an aggressiveness parameter that\nallows for fine-tuning the mentioned tradeoff. The experimental results\nobtained in seven different robotic environments indicate a considerable\nreduction in the number of safety violations along with higher, or competitive,\npolicy returns, in comparison to six different state-of-the-art Safe RL\nmethods. The results demonstrate the significant superiority of the proposed\nSORL algorithm in safety-critical applications.",
      "tldr_zh": "本研究提出了一种新型无模型安全强化学习（Safe RL）算法，名为 Safety Optimized RL (SORL)，基于多目标策略优化框架，同时优化策略的优化性和安全性。不同于传统 Safe RL 方法，SORL 通过环境奖励函数和安全批评者（safety critic）塑造奖励，而非限制策略搜索空间，从而实现安全与优化之间的自然权衡，并引入攻击性参数（aggressiveness parameter）来微调此权衡。实验结果显示，在七个机器人环境中，SORL 与六种最先进方法相比，显著减少了安全违规次数，同时保持或提高了策略回报，证明其在安全关键应用中的显著优势。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG",
        "cs.RO",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "Accepted to the IEEE International Conference on Robotics and\n  Automation (ICRA) 2024, 7 Pages, 3 Figures",
      "pdf_url": "http://arxiv.org/pdf/2402.15197v1",
      "published_date": "2024-02-23 08:58:38 UTC",
      "updated_date": "2024-02-23 08:58:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:32:24.355312"
    },
    {
      "arxiv_id": "2402.15195v1",
      "title": "The AffectToolbox: Affect Analysis for Everyone",
      "title_zh": "翻译失败",
      "authors": [
        "Silvan Mertes",
        "Dominik Schiller",
        "Michael Dietz",
        "Elisabeth André",
        "Florian Lingenfelser"
      ],
      "abstract": "In the field of affective computing, where research continually advances at a\nrapid pace, the demand for user-friendly tools has become increasingly\napparent. In this paper, we present the AffectToolbox, a novel software system\nthat aims to support researchers in developing affect-sensitive studies and\nprototypes. The proposed system addresses the challenges posed by existing\nframeworks, which often require profound programming knowledge and cater\nprimarily to power-users or skilled developers. Aiming to facilitate ease of\nuse, the AffectToolbox requires no programming knowledge and offers its\nfunctionality to reliably analyze the affective state of users through an\naccessible graphical user interface. The architecture encompasses a variety of\nmodels for emotion recognition on multiple affective channels and modalities,\nas well as an elaborate fusion system to merge multi-modal assessments into a\nunified result. The entire system is open-sourced and will be publicly\navailable to ensure easy integration into more complex applications through a\nwell-structured, Python-based code base - therefore marking a substantial\ncontribution toward advancing affective computing research and fostering a more\ncollaborative and inclusive environment within this interdisciplinary field.",
      "tldr_zh": "这篇论文介绍了AffectToolbox，一种用户友好的软件系统，旨在简化情感计算（affective computing）研究，帮助非专业程序员开发情感敏感的研究和原型。该系统通过图形用户界面（graphical user interface）提供多种情感识别模型，支持多通道和多模式评估，并使用一个融合系统（fusion system）整合结果为统一输出。AffectToolbox是开源的，基于Python代码库，便于与其他应用集成，从而推动情感计算领域的协作和包容性发展。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15195v1",
      "published_date": "2024-02-23 08:55:47 UTC",
      "updated_date": "2024-02-23 08:55:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:32:37.952404"
    },
    {
      "arxiv_id": "2402.15194v2",
      "title": "Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized Control",
      "title_zh": "翻译失败",
      "authors": [
        "Masatoshi Uehara",
        "Yulai Zhao",
        "Kevin Black",
        "Ehsan Hajiramezanali",
        "Gabriele Scalia",
        "Nathaniel Lee Diamant",
        "Alex M Tseng",
        "Tommaso Biancalani",
        "Sergey Levine"
      ],
      "abstract": "Diffusion models excel at capturing complex data distributions, such as those\nof natural images and proteins. While diffusion models are trained to represent\nthe distribution in the training dataset, we often are more concerned with\nother properties, such as the aesthetic quality of the generated images or the\nfunctional properties of generated proteins. Diffusion models can be finetuned\nin a goal-directed way by maximizing the value of some reward function (e.g.,\nthe aesthetic quality of an image). However, these approaches may lead to\nreduced sample diversity, significant deviations from the training data\ndistribution, and even poor sample quality due to the exploitation of an\nimperfect reward function. The last issue often occurs when the reward function\nis a learned model meant to approximate a ground-truth \"genuine\" reward, as is\nthe case in many practical applications. These challenges, collectively termed\n\"reward collapse,\" pose a substantial obstacle. To address this reward\ncollapse, we frame the finetuning problem as entropy-regularized control\nagainst the pretrained diffusion model, i.e., directly optimizing\nentropy-enhanced rewards with neural SDEs. We present theoretical and empirical\nevidence that demonstrates our framework is capable of efficiently generating\ndiverse samples with high genuine rewards, mitigating the overoptimization of\nimperfect reward models.",
      "tldr_zh": "本文提出了一种将连续时间 Diffusion models 的微调问题框架化为熵正则化控制（entropy-regularized control）的方法，以解决传统奖励优化带来的奖励崩溃（reward collapse）问题，如样本多样性降低和偏离训练数据分布。方法通过使用神经 SDEs 直接优化熵增强的奖励函数，确保在生成高奖励样本（如图像美学质量或蛋白质功能）的同时保持样本多样性和质量。理论和经验证据显示，该框架能高效缓解不完美奖励模型的过度优化，为复杂数据生成任务提供更可靠的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Under review (codes will be released soon)",
      "pdf_url": "http://arxiv.org/pdf/2402.15194v2",
      "published_date": "2024-02-23 08:54:42 UTC",
      "updated_date": "2024-02-28 09:21:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:32:48.864746"
    },
    {
      "arxiv_id": "2402.15189v2",
      "title": "Biomedical Entity Linking as Multiple Choice Question Answering",
      "title_zh": "生物医学实体链接作为多项选择问答",
      "authors": [
        "Zhenxi Lin",
        "Ziheng Zhang",
        "Xian Wu",
        "Yefeng Zheng"
      ],
      "abstract": "Although biomedical entity linking (BioEL) has made significant progress with\npre-trained language models, challenges still exist for fine-grained and\nlong-tailed entities. To address these challenges, we present BioELQA, a novel\nmodel that treats Biomedical Entity Linking as Multiple Choice Question\nAnswering. BioELQA first obtains candidate entities with a fast retriever,\njointly presents the mention and candidate entities to a generator, and then\noutputs the predicted symbol associated with its chosen entity. This\nformulation enables explicit comparison of different candidate entities, thus\ncapturing fine-grained interactions between mentions and entities, as well as\namong entities themselves. To improve generalization for long-tailed entities,\nwe retrieve similar labeled training instances as clues and concatenate the\ninput with retrieved instances for the generator. Extensive experimental\nresults show that BioELQA outperforms state-of-the-art baselines on several\ndatasets.",
      "tldr_zh": "该论文将生物医学实体链接 (BioEL) 视为多项选择问答问题，提出了一种名为 BioELQA 的新模型，以解决细粒度和长尾实体带来的挑战。BioELQA 通过快速检索器获取候选实体，然后将提及和候选实体输入生成器进行显式比较，从而捕捉提及与实体间的细粒度交互，并为长尾实体泛化添加检索类似标记训练实例作为辅助线索。实验结果显示，BioELQA 在多个数据集上优于现有最先进基线模型。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by COLING 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.15189v2",
      "published_date": "2024-02-23 08:40:38 UTC",
      "updated_date": "2024-05-17 09:11:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:33:00.128751"
    },
    {
      "arxiv_id": "2402.15183v5",
      "title": "GraphEdit: Large Language Models for Graph Structure Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Zirui Guo",
        "Lianghao Xia",
        "Yanhua Yu",
        "Yuling Wang",
        "Kangkang Lu",
        "Zhiyong Huang",
        "Chao Huang"
      ],
      "abstract": "Graph Structure Learning (GSL) focuses on capturing intrinsic dependencies\nand interactions among nodes in graph-structured data by generating novel graph\nstructures. Graph Neural Networks (GNNs) have emerged as promising GSL\nsolutions, utilizing recursive message passing to encode node-wise\ninter-dependencies. However, many existing GSL methods heavily depend on\nexplicit graph structural information as supervision signals, leaving them\nsusceptible to challenges such as data noise and sparsity. In this work, we\npropose GraphEdit, an approach that leverages large language models (LLMs) to\nlearn complex node relationships in graph-structured data. By enhancing the\nreasoning capabilities of LLMs through instruction-tuning over graph\nstructures, we aim to overcome the limitations associated with explicit graph\nstructural information and enhance the reliability of graph structure learning.\nOur approach not only effectively denoises noisy connections but also\nidentifies node-wise dependencies from a global perspective, providing a\ncomprehensive understanding of the graph structure. We conduct extensive\nexperiments on multiple benchmark datasets to demonstrate the effectiveness and\nrobustness of GraphEdit across various settings. We have made our model\nimplementation available at: https://github.com/HKUDS/GraphEdit.",
      "tldr_zh": "本文提出 GraphEdit，一种利用 Large Language Models (LLMs) 进行 Graph Structure Learning (GSL) 的方法，旨在克服传统 Graph Neural Networks (GNNs) 方法对显式图结构依赖的局限性，如数据噪声和稀疏性问题。GraphEdit 通过在图结构上进行 instruction-tuning 来增强 LLMs 的推理能力，从而有效去噪并从全局视角识别节点间的复杂依赖关系。实验结果显示，该方法在多个基准数据集上表现出色，具有显著的有效性和鲁棒性，并已开源实现（https://github.com/HKUDS/GraphEdit）。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15183v5",
      "published_date": "2024-02-23 08:29:42 UTC",
      "updated_date": "2025-03-10 14:04:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:33:14.049937"
    },
    {
      "arxiv_id": "2402.15170v1",
      "title": "The Surprising Effectiveness of Skip-Tuning in Diffusion Sampling",
      "title_zh": "Skip-Tuning 在扩散采样中的惊人有效性",
      "authors": [
        "Jiajun Ma",
        "Shuchen Xue",
        "Tianyang Hu",
        "Wenjia Wang",
        "Zhaoqiang Liu",
        "Zhenguo Li",
        "Zhi-Ming Ma",
        "Kenji Kawaguchi"
      ],
      "abstract": "With the incorporation of the UNet architecture, diffusion probabilistic\nmodels have become a dominant force in image generation tasks. One key design\nin UNet is the skip connections between the encoder and decoder blocks.\nAlthough skip connections have been shown to improve training stability and\nmodel performance, we reveal that such shortcuts can be a limiting factor for\nthe complexity of the transformation. As the sampling steps decrease, the\ngeneration process and the role of the UNet get closer to the push-forward\ntransformations from Gaussian distribution to the target, posing a challenge\nfor the network's complexity. To address this challenge, we propose\nSkip-Tuning, a simple yet surprisingly effective training-free tuning method on\nthe skip connections. Our method can achieve 100% FID improvement for\npretrained EDM on ImageNet 64 with only 19 NFEs (1.75), breaking the limit of\nODE samplers regardless of sampling steps. Surprisingly, the improvement\npersists when we increase the number of sampling steps and can even surpass the\nbest result from EDM-2 (1.58) with only 39 NFEs (1.57). Comprehensive\nexploratory experiments are conducted to shed light on the surprising\neffectiveness. We observe that while Skip-Tuning increases the score-matching\nlosses in the pixel space, the losses in the feature space are reduced,\nparticularly at intermediate noise levels, which coincide with the most\neffective range accounting for image quality improvement.",
      "tldr_zh": "本文研究发现，UNet 架构中的 skip connections 在扩散概率模型(diffusion probabilistic models)中虽提升了训练稳定性和性能，但会限制变换的复杂性，尤其在采样步骤减少时。作者提出了一种简单有效的训练-free 方法 Skip-Tuning，对 skip connections 进行调整，使预训练 EDM 在 ImageNet 64 上 FID 改善 100%，仅用 19 NFEs 达到 1.75，甚至超过 EDM-2 的最佳结果。实验进一步显示，Skip-Tuning 虽然增加了像素空间的 score-matching losses，但减少了特征空间的 losses，特别是在中间噪声水平，从而显著提升了图像生成质量。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15170v1",
      "published_date": "2024-02-23 08:05:23 UTC",
      "updated_date": "2024-02-23 08:05:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:33:27.940716"
    },
    {
      "arxiv_id": "2402.15163v4",
      "title": "Has the Deep Neural Network learned the Stochastic Process? An Evaluation Viewpoint",
      "title_zh": "深度神经网络是否学会了随机过程？ 一个评估",
      "authors": [
        "Harshit Kumar",
        "Beomseok Kang",
        "Biswadeep Chakraborty",
        "Saibal Mukhopadhyay"
      ],
      "abstract": "This paper presents the first systematic study of evaluating Deep Neural\nNetworks (DNNs) designed to forecast the evolution of stochastic complex\nsystems. We show that traditional evaluation methods like threshold-based\nclassification metrics and error-based scoring rules assess a DNN's ability to\nreplicate the observed ground truth but fail to measure the DNN's learning of\nthe underlying stochastic process. To address this gap, we propose a new\nevaluation criterion called Fidelity to Stochastic Process (F2SP), representing\nthe DNN's ability to predict the system property Statistic-GT--the ground truth\nof the stochastic process--and introduce an evaluation metric that exclusively\nassesses F2SP. We formalize F2SP within a stochastic framework and establish\ncriteria for validly measuring it. We formally show that Expected Calibration\nError (ECE) satisfies the necessary condition for testing F2SP, unlike\ntraditional evaluation methods. Empirical experiments on synthetic datasets,\nincluding wildfire, host-pathogen, and stock market models, demonstrate that\nECE uniquely captures F2SP. We further extend our study to real-world wildfire\ndata, highlighting the limitations of conventional evaluation and discuss the\npractical utility of incorporating F2SP into model assessment. This work offers\na new perspective on evaluating DNNs modeling complex systems by emphasizing\nthe importance of capturing the underlying stochastic process.",
      "tldr_zh": "本论文首次系统评估深度神经网络 (DNNs) 在预测随机复杂系统演化时的表现，指出传统评估方法如基于阈值的分类指标和错误评分规则仅检查DNNs复制观察数据的能力，而忽略了其对底层随机过程的学习。作者提出新的评估标准Fidelity to Stochastic Process (F2SP)，用于衡量DNNs预测系统属性Statistic-GT的能力，并形式化了F2SP的框架，证明Expected Calibration Error (ECE)是有效测量工具。实验结果显示，在合成数据集（如野火、宿主-病原体和股票市场模型）及真实野火数据上，ECE独一无二地捕捉F2SP，提供了一个新的视角，强调在DNNs模型评估中捕获随机过程的重要性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Published as a conference paper at ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2402.15163v4",
      "published_date": "2024-02-23 07:54:20 UTC",
      "updated_date": "2025-01-25 07:23:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:33:40.073364"
    },
    {
      "arxiv_id": "2402.15162v1",
      "title": "Entity-level Factual Adaptiveness of Fine-tuning based Abstractive Summarization Models",
      "title_zh": "翻译失败",
      "authors": [
        "Jongyoon Song",
        "Nohil Park",
        "Bongkyu Hwang",
        "Jaewoong Yun",
        "Seongho Joe",
        "Youngjune L. Gwon",
        "Sungroh Yoon"
      ],
      "abstract": "Abstractive summarization models often generate factually inconsistent\ncontent particularly when the parametric knowledge of the model conflicts with\nthe knowledge in the input document. In this paper, we analyze the robustness\nof fine-tuning based summarization models to the knowledge conflict, which we\ncall factual adaptiveness. We utilize pre-trained language models to construct\nevaluation sets and find that factual adaptiveness is not strongly correlated\nwith factual consistency on original datasets. Furthermore, we introduce a\ncontrollable counterfactual data augmentation method where the degree of\nknowledge conflict within the augmented data can be adjustable. Our\nexperimental results on two pre-trained language models (PEGASUS and BART) and\ntwo fine-tuning datasets (XSum and CNN/DailyMail) demonstrate that our method\nenhances factual adaptiveness while achieving factual consistency on original\ndatasets on par with the contrastive learning baseline.",
      "tldr_zh": "本文研究了基于 fine-tuning 的 abstractive summarization 模型在实体级别上的 factual adaptiveness，即模型对知识冲突的鲁棒性。研究发现，factual adaptiveness 与 original datasets 上的 factual consistency 没有强相关性，并使用 pre-trained language models 构建评价集进行分析。同时，作者提出了一种可控的 counterfactual data augmentation 方法，能够调整知识冲突程度。在 PEGASUS 和 BART 模型上，以及 XSum 和 CNN/DailyMail 数据集的实验中，该方法显著提升了 factual adaptiveness，同时在原数据集上保持了与 contrastive learning baseline 相当的 factual consistency。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "EACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.15162v1",
      "published_date": "2024-02-23 07:53:39 UTC",
      "updated_date": "2024-02-23 07:53:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:33:50.788812"
    },
    {
      "arxiv_id": "2402.15160v3",
      "title": "Spatially-Aware Transformer for Embodied Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Junmo Cho",
        "Jaesik Yoon",
        "Sungjin Ahn"
      ],
      "abstract": "Episodic memory plays a crucial role in various cognitive processes, such as\nthe ability to mentally recall past events. While cognitive science emphasizes\nthe significance of spatial context in the formation and retrieval of episodic\nmemory, the current primary approach to implementing episodic memory in AI\nsystems is through transformers that store temporally ordered experiences,\nwhich overlooks the spatial dimension. As a result, it is unclear how the\nunderlying structure could be extended to incorporate the spatial axis beyond\ntemporal order alone and thereby what benefits can be obtained. To address\nthis, this paper explores the use of Spatially-Aware Transformer models that\nincorporate spatial information. These models enable the creation of\nplace-centric episodic memory that considers both temporal and spatial\ndimensions. Adopting this approach, we demonstrate that memory utilization\nefficiency can be improved, leading to enhanced accuracy in various\nplace-centric downstream tasks. Additionally, we propose the Adaptive Memory\nAllocator, a memory management method based on reinforcement learning that aims\nto optimize efficiency of memory utilization. Our experiments demonstrate the\nadvantages of our proposed model in various environments and across multiple\ndownstream tasks, including prediction, generation, reasoning, and\nreinforcement learning. The source code for our models and experiments will be\navailable at https://github.com/junmokane/spatially-aware-transformer.",
      "tldr_zh": "本论文探讨了episodic memory在AI系统中的应用，强调其需整合空间上下文，而现有transformer模型仅关注时间顺序，忽略了空间维度。研究提出Spatially-Aware Transformer模型，通过结合空间信息创建以地点为中心的episodic memory，从而提升记忆利用效率，并在预测、生成、推理和强化学习等下游任务中提高准确性。此外，论文引入Adaptive Memory Allocator——一种基于强化学习的记忆管理方法，并在多种环境中实验验证了模型的优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2024 Spotlight. First two authors contributed equally",
      "pdf_url": "http://arxiv.org/pdf/2402.15160v3",
      "published_date": "2024-02-23 07:46:30 UTC",
      "updated_date": "2024-03-01 00:58:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:34:01.906215"
    },
    {
      "arxiv_id": "2402.15159v3",
      "title": "Machine Unlearning of Pre-trained Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Jin Yao",
        "Eli Chien",
        "Minxin Du",
        "Xinyao Niu",
        "Tianhao Wang",
        "Zezhou Cheng",
        "Xiang Yue"
      ],
      "abstract": "This study investigates the concept of the `right to be forgotten' within the\ncontext of large language models (LLMs). We explore machine unlearning as a\npivotal solution, with a focus on pre-trained models--a notably\nunder-researched area. Our research delineates a comprehensive framework for\nmachine unlearning in pre-trained LLMs, encompassing a critical analysis of\nseven diverse unlearning methods. Through rigorous evaluation using curated\ndatasets from arXiv, books, and GitHub, we establish a robust benchmark for\nunlearning performance, demonstrating that these methods are over $10^5$ times\nmore computationally efficient than retraining. Our results show that\nintegrating gradient ascent with gradient descent on in-distribution data\nimproves hyperparameter robustness. We also provide detailed guidelines for\nefficient hyperparameter tuning in the unlearning process. Our findings advance\nthe discourse on ethical AI practices, offering substantive insights into the\nmechanics of machine unlearning for pre-trained LLMs and underscoring the\npotential for responsible AI development.",
      "tldr_zh": "这篇论文探讨了在预训练大型语言模型（pre-trained LLMs）中实现“被遗忘权”的机器遗忘（machine unlearning），这是一个相对未被充分研究的领域。研究者构建了一个全面框架，对七种不同的遗忘方法进行了批判性分析，并使用从 arXiv、书籍和 GitHub 采集的数据集建立了遗忘性能的基准。结果显示，这些方法比重新训练高效超过 10^5 倍，并通过整合 gradient ascent 和 gradient descent 在分布内数据上，显著提高了超参数的鲁棒性。该研究还提供了机器遗忘过程的超参数调整指南，并为道德 AI 实践提供了关键见解，推动了负责任的 AI 开发。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2024 main. Code and data at\n  https://github.com/yaojin17/Unlearning_LLM",
      "pdf_url": "http://arxiv.org/pdf/2402.15159v3",
      "published_date": "2024-02-23 07:43:26 UTC",
      "updated_date": "2024-05-30 15:44:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:34:17.165986"
    },
    {
      "arxiv_id": "2402.15152v2",
      "title": "On the Duality Between Sharpness-Aware Minimization and Adversarial Training",
      "title_zh": "关于锐度感知最小化与对抗训练之间的对偶性",
      "authors": [
        "Yihao Zhang",
        "Hangzhou He",
        "Jingyu Zhu",
        "Huanran Chen",
        "Yifei Wang",
        "Zeming Wei"
      ],
      "abstract": "Adversarial Training (AT), which adversarially perturb the input samples\nduring training, has been acknowledged as one of the most effective defenses\nagainst adversarial attacks, yet suffers from inevitably decreased clean\naccuracy. Instead of perturbing the samples, Sharpness-Aware Minimization (SAM)\nperturbs the model weights during training to find a more flat loss landscape\nand improve generalization. However, as SAM is designed for better clean\naccuracy, its effectiveness in enhancing adversarial robustness remains\nunexplored. In this work, considering the duality between SAM and AT, we\ninvestigate the adversarial robustness derived from SAM. Intriguingly, we find\nthat using SAM alone can improve adversarial robustness. To understand this\nunexpected property of SAM, we first provide empirical and theoretical insights\ninto how SAM can implicitly learn more robust features, and conduct\ncomprehensive experiments to show that SAM can improve adversarial robustness\nnotably without sacrificing any clean accuracy, shedding light on the potential\nof SAM to be a substitute for AT when accuracy comes at a higher priority. Code\nis available at https://github.com/weizeming/SAM_AT.",
      "tldr_zh": "本研究探讨了Sharpness-Aware Minimization (SAM) 与Adversarial Training (AT) 之间的二元性，指出SAM通过扰动模型权重以寻找更平坦的损失景观，虽然原本旨在提升泛化能力，但也能显著改善对抗鲁棒性，而无需牺牲clean accuracy。研究通过实证和理论分析揭示，SAM可隐式学习更鲁棒的特征，并在实验中证明其在多种场景下比AT更有效地提升对抗鲁棒性。总体而言，此发现为优先考虑准确率的场景提供了SAM作为AT替代方案的可能性，代码已在GitHub上公开。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.15152v2",
      "published_date": "2024-02-23 07:22:55 UTC",
      "updated_date": "2024-06-05 08:39:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:34:27.122744"
    },
    {
      "arxiv_id": "2402.15140v2",
      "title": "A Relation-Interactive Approach for Message Passing in Hyper-relational Knowledge Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Yonglin Jing"
      ],
      "abstract": "Hyper-relational knowledge graphs (KGs) contain additional key-value pairs,\nproviding more information about the relations. In many scenarios, the same\nrelation can have distinct key-value pairs, making the original triple fact\nmore recognizable and specific. Prior studies on hyper-relational KGs have\nestablished a solid standard method for hyper-relational graph encoding. In\nthis work, we propose a message-passing-based graph encoder with global\nrelation structure awareness ability, which we call ReSaE. Compared to the\nprior state-of-the-art approach, ReSaE emphasizes the interaction of relations\nduring message passing process and optimizes the readout structure for link\nprediction tasks. Overall, ReSaE gives a encoding solution for hyper-relational\nKGs and ensures stronger performance on downstream link prediction tasks. Our\nexperiments demonstrate that ReSaE achieves state-of-the-art performance on\nmultiple link prediction benchmarks. Furthermore, we also analyze the influence\nof different model structures on model performance.",
      "tldr_zh": "该论文提出了一种基于关系的交互方法，用于超关系知识图谱（Hyper-relational Knowledge Graphs）的消息传递编码，名为 ReSaE。该方法强调在消息传递（message passing）过程中关系的交互，并优化了 readout structure，以提升下游链接预测（link prediction）任务的性能。与现有最先进方法相比，ReSaE 通过全局关系结构感知能力，提供更有效的超关系图编码解决方案。实验结果显示，ReSaE 在多个链接预测基准上实现了最先进性能，并分析了不同模型结构对整体表现的影响。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15140v2",
      "published_date": "2024-02-23 06:55:04 UTC",
      "updated_date": "2024-03-02 04:59:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:34:38.828910"
    },
    {
      "arxiv_id": "2402.15135v1",
      "title": "Modified CycleGAN for the synthesization of samples for wheat head segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Jaden Myers",
        "Keyhan Najafian",
        "Farhad Maleki",
        "Katie Ovens"
      ],
      "abstract": "Deep learning models have been used for a variety of image processing tasks.\nHowever, most of these models are developed through supervised learning\napproaches, which rely heavily on the availability of large-scale annotated\ndatasets. Developing such datasets is tedious and expensive. In the absence of\nan annotated dataset, synthetic data can be used for model development;\nhowever, due to the substantial differences between simulated and real data, a\nphenomenon referred to as domain gap, the resulting models often underperform\nwhen applied to real data. In this research, we aim to address this challenge\nby first computationally simulating a large-scale annotated dataset and then\nusing a generative adversarial network (GAN) to fill the gap between simulated\nand real images. This approach results in a synthetic dataset that can be\neffectively utilized to train a deep-learning model. Using this approach, we\ndeveloped a realistic annotated synthetic dataset for wheat head segmentation.\nThis dataset was then used to develop a deep-learning model for semantic\nsegmentation. The resulting model achieved a Dice score of 83.4\\% on an\ninternal dataset and Dice scores of 79.6% and 83.6% on two external Global\nWheat Head Detection datasets. While we proposed this approach in the context\nof wheat head segmentation, it can be generalized to other crop types or, more\nbroadly, to images with dense, repeated patterns such as those found in\ncellular imagery.",
      "tldr_zh": "本文提出了一种Modified CycleGAN方法，用于生成小麦头部分割的合成样本，以解决深度学习模型依赖大规模标注数据集的挑战。研究首先模拟了一个大型标注数据集，然后利用GAN填补模拟数据与真实数据之间的领域差距，从而创建了一个更真实的合成数据集。使用此数据集训练的语义分割模型，在内部数据集上达到了83.4%的Dice score，在两个外部Global Wheat Head Detection数据集上分别达到了79.6%和83.6%。该方法可推广到其他作物类型或具有密集重复模式的图像领域，如细胞图像。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15135v1",
      "published_date": "2024-02-23 06:42:58 UTC",
      "updated_date": "2024-02-23 06:42:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:34:51.097706"
    },
    {
      "arxiv_id": "2402.15134v1",
      "title": "Deep Coupling Network For Multivariate Time Series Forecasting",
      "title_zh": "用于多变量时间序列预测的深度耦合网络",
      "authors": [
        "Kun Yi",
        "Qi Zhang",
        "Hui He",
        "Kaize Shi",
        "Liang Hu",
        "Ning An",
        "Zhendong Niu"
      ],
      "abstract": "Multivariate time series (MTS) forecasting is crucial in many real-world\napplications. To achieve accurate MTS forecasting, it is essential to\nsimultaneously consider both intra- and inter-series relationships among time\nseries data. However, previous work has typically modeled intra- and\ninter-series relationships separately and has disregarded multi-order\ninteractions present within and between time series data, which can seriously\ndegrade forecasting accuracy. In this paper, we reexamine intra- and\ninter-series relationships from the perspective of mutual information and\naccordingly construct a comprehensive relationship learning mechanism tailored\nto simultaneously capture the intricate multi-order intra- and inter-series\ncouplings. Based on the mechanism, we propose a novel deep coupling network for\nMTS forecasting, named DeepCN, which consists of a coupling mechanism dedicated\nto explicitly exploring the multi-order intra- and inter-series relationships\namong time series data concurrently, a coupled variable representation module\naimed at encoding diverse variable patterns, and an inference module\nfacilitating predictions through one forward step. Extensive experiments\nconducted on seven real-world datasets demonstrate that our proposed DeepCN\nachieves superior performance compared with the state-of-the-art baselines.",
      "tldr_zh": "多变量时间序列(Multivariate Time Series, MTS)预测需要在考虑序列内部(intra-series)和序列之间(inter-series)关系的同时，处理多阶交互，但现有方法通常分开建模并忽略这些交互，导致预测准确性下降。  \n本文从互信息(Mutual Information)的视角重新审视这些关系，构建了一个全面的关系学习机制，并提出DeepCN模型，包括耦合机制用于同时探索多阶intra-和inter-series耦合、耦合变量表示模块用于编码多样变量模式，以及推理模块通过一步前向计算进行预测。  \n实验在七个真实世界数据集上显示，DeepCN比最先进基线模型表现出显著优越性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15134v1",
      "published_date": "2024-02-23 06:38:08 UTC",
      "updated_date": "2024-02-23 06:38:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:35:03.484779"
    },
    {
      "arxiv_id": "2402.15131v3",
      "title": "Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question Answering with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Guanming Xiong",
        "Junwei Bao",
        "Wen Zhao"
      ],
      "abstract": "This study explores the realm of knowledge base question answering (KBQA).\nKBQA is considered a challenging task, particularly in parsing intricate\nquestions into executable logical forms. Traditional semantic parsing\n(SP)-based methods require extensive data annotations, which result in\nsignificant costs. Recently, the advent of few-shot in-context learning,\npowered by large language models (LLMs), has showcased promising capabilities.\nHowever, fully leveraging LLMs to parse questions into logical forms in\nlow-resource scenarios poses a substantial challenge. To tackle these hurdles,\nwe introduce Interactive-KBQA, a framework designed to generate logical forms\nthrough direct interaction with knowledge bases (KBs). Within this framework,\nwe have developed three generic APIs for KB interaction. For each category of\ncomplex question, we devised exemplars to guide LLMs through the reasoning\nprocesses. Our method achieves competitive results on the WebQuestionsSP,\nComplexWebQuestions, KQA Pro, and MetaQA datasets with a minimal number of\nexamples (shots). Importantly, our approach supports manual intervention,\nallowing for the iterative refinement of LLM outputs. By annotating a dataset\nwith step-wise reasoning processes, we showcase our model's adaptability and\nhighlight its potential for contributing significant enhancements to the field.",
      "tldr_zh": "本研究探讨了知识库问答 (KBQA) 的挑战，特别是将复杂问题解析为可执行逻辑形式的问题。传统基于语义解析 (SP) 的方法需要大量数据标注，成本高，而作者提出 Interactive-KBQA 框架，利用大型语言模型 (LLMs) 通过多轮交互直接与知识库 (KBs) 交互来生成逻辑形式，包括开发三个通用 APIs 和设计示例指导推理过程。在 WebQuestionsSP、ComplexWebQuestions、KQA Pro 和 MetaQA 等数据集上，该框架在少样本 (few-shot) 场景下取得了竞争性结果，并支持手动干预以迭代精炼输出，展示了其在低资源环境下的适应性和对 KBQA 领域的潜在贡献。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "This work has been accepted by the ACL 2024 main conference. Code and\n  data are available at: https://github.com/JimXiongGM/Interactive-KBQA",
      "pdf_url": "http://arxiv.org/pdf/2402.15131v3",
      "published_date": "2024-02-23 06:32:18 UTC",
      "updated_date": "2025-03-12 06:15:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:35:15.849879"
    },
    {
      "arxiv_id": "2402.15538v1",
      "title": "AgentLite: A Lightweight Library for Building and Advancing Task-Oriented LLM Agent System",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiwei Liu",
        "Weiran Yao",
        "Jianguo Zhang",
        "Liangwei Yang",
        "Zuxin Liu",
        "Juntao Tan",
        "Prafulla K. Choubey",
        "Tian Lan",
        "Jason Wu",
        "Huan Wang",
        "Shelby Heinecke",
        "Caiming Xiong",
        "Silvio Savarese"
      ],
      "abstract": "The booming success of LLMs initiates rapid development in LLM agents. Though\nthe foundation of an LLM agent is the generative model, it is critical to\ndevise the optimal reasoning strategies and agent architectures. Accordingly,\nLLM agent research advances from the simple chain-of-thought prompting to more\ncomplex ReAct and Reflection reasoning strategy; agent architecture also\nevolves from single agent generation to multi-agent conversation, as well as\nmulti-LLM multi-agent group chat. However, with the existing intricate\nframeworks and libraries, creating and evaluating new reasoning strategies and\nagent architectures has become a complex challenge, which hinders research\ninvestigation into LLM agents. Thus, we open-source a new AI agent library,\nAgentLite, which simplifies this process by offering a lightweight,\nuser-friendly platform for innovating LLM agent reasoning, architectures, and\napplications with ease. AgentLite is a task-oriented framework designed to\nenhance the ability of agents to break down tasks and facilitate the\ndevelopment of multi-agent systems. Furthermore, we introduce multiple\npractical applications developed with AgentLite to demonstrate its convenience\nand flexibility. Get started now at:\n\\url{https://github.com/SalesforceAIResearch/AgentLite}.",
      "tldr_zh": "该研究介绍了AgentLite，一种轻量级库，旨在简化任务导向型LLM代理系统的构建和创新，解决现有复杂框架在开发新推理策略（如chain-of-thought prompting、ReAct和Reflection）和代理架构（如多代理对话）时遇到的挑战。AgentLite提供了一个用户友好的任务导向框架，帮助代理分解任务并支持多代理系统的发展，便于研究者快速实验和迭代。作者展示了多个基于AgentLite的实际应用，证明了其便利性和灵活性，并提供了开源链接（https://github.com/SalesforceAIResearch/AgentLite）。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "preprint. Library is available at\n  https://github.com/SalesforceAIResearch/AgentLite",
      "pdf_url": "http://arxiv.org/pdf/2402.15538v1",
      "published_date": "2024-02-23 06:25:20 UTC",
      "updated_date": "2024-02-23 06:25:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:35:27.266805"
    },
    {
      "arxiv_id": "2402.15120v1",
      "title": "Fine-tuning CLIP Text Encoders with Two-step Paraphrasing",
      "title_zh": "翻译失败",
      "authors": [
        "Hyunjae Kim",
        "Seunghyun Yoon",
        "Trung Bui",
        "Handong Zhao",
        "Quan Tran",
        "Franck Dernoncourt",
        "Jaewoo Kang"
      ],
      "abstract": "Contrastive language-image pre-training (CLIP) models have demonstrated\nconsiderable success across various vision-language tasks, such as\ntext-to-image retrieval, where the model is required to effectively process\nnatural language input to produce an accurate visual output. However, current\nmodels still face limitations in dealing with linguistic variations in input\nqueries, such as paraphrases, making it challenging to handle a broad range of\nuser queries in real-world applications. In this study, we introduce a\nstraightforward fine-tuning approach to enhance the representations of CLIP\nmodels for paraphrases. Our approach involves a two-step paraphrase generation\nprocess, where we automatically create two categories of paraphrases from\nweb-scale image captions by leveraging large language models. Subsequently, we\nfine-tune the CLIP text encoder using these generated paraphrases while\nfreezing the image encoder. Our resulting model, which we call ParaCLIP,\nexhibits significant improvements over baseline CLIP models across various\ntasks, including paraphrased retrieval (with rank similarity scores improved by\nup to 2.0% and 5.6%), Visual Genome Relation and Attribution, as well as seven\nsemantic textual similarity tasks.",
      "tldr_zh": "本研究针对 CLIP 模型在处理输入查询的语言变体（如 paraphrases）时存在的局限性，提出了一种简单有效的微调方法，以提升其文本编码器的表示能力。方法包括两步 paraphrasing 生成过程：利用大语言模型从海量图像标题中自动创建两种 paraphrases，然后冻结图像编码器，仅微调文本编码器，得到新模型 ParaCLIP。实验结果显示，ParaCLIP 在 paraphrased retrieval 任务上提升 rank similarity scores 至多 2.0% 和 5.6%，并在 Visual Genome Relation and Attribution 以及七个语义文本相似性任务中表现出显著改进。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "EACL 2024 (Findings of the ACL)",
      "pdf_url": "http://arxiv.org/pdf/2402.15120v1",
      "published_date": "2024-02-23 06:11:50 UTC",
      "updated_date": "2024-02-23 06:11:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:35:40.556650"
    },
    {
      "arxiv_id": "2403.00796v1",
      "title": "Enhancing Mean-Reverting Time Series Prediction with Gaussian Processes: Functional and Augmented Data Structures in Financial Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Narayan Tondapu"
      ],
      "abstract": "In this paper, we explore the application of Gaussian Processes (GPs) for\npredicting mean-reverting time series with an underlying structure, using\nrelatively unexplored functional and augmented data structures. While many\nconventional forecasting methods concentrate on the short-term dynamics of time\nseries data, GPs offer the potential to forecast not just the average\nprediction but the entire probability distribution over a future trajectory.\nThis is particularly beneficial in financial contexts, where accurate\npredictions alone may not suffice if incorrect volatility assessments lead to\ncapital losses. Moreover, in trade selection, GPs allow for the forecasting of\nmultiple Sharpe ratios adjusted for transaction costs, aiding in\ndecision-making. The functional data representation utilized in this study\nenables longer-term predictions by leveraging information from previous years,\neven as the forecast moves away from the current year's training data.\nAdditionally, the augmented representation enriches the training set by\nincorporating multiple targets for future points in time, facilitating\nlong-term predictions. Our implementation closely aligns with the methodology\noutlined in, which assessed effectiveness on commodity futures. However, our\ntesting methodology differs. Instead of real data, we employ simulated data\nwith similar characteristics. We construct a testing environment to evaluate\nboth data representations and models under conditions of increasing noise, fat\ntails, and inappropriate kernels-conditions commonly encountered in practice.\nBy simulating data, we can compare our forecast distribution over time against\na full simulation of the actual distribution of our test set, thereby reducing\nthe inherent uncertainty in testing time series models on real data. We enable\nfeature prediction through augmentation and employ sub-sampling to ensure the\nfeasibility of GPs.",
      "tldr_zh": "本研究探讨了使用 Gaussian Processes (GPs) 增强均值回归时间序列的预测，特别引入 functional data representation 和 augmented data representation 这些相对未被充分利用的数据结构，以改善金融预测的准确性和可靠性。相比传统方法，GPs 不仅预测平均值，还能输出未来轨迹的整个概率分布，从而更好地评估波动性并支持交易决策，如计算调整交易成本的 Sharpe ratios。功能数据结构利用历史年份信息实现长期预测，而增强数据结构通过添加多个未来目标点来丰富训练集。实验采用模拟数据在噪声、厚尾分布和不适当核函数等实际条件下进行评估，结果显示这种方法能有效减少预测不确定性，并提升整体表现。",
      "categories": [
        "q-fin.ST",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "q-fin.ST",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00796v1",
      "published_date": "2024-02-23 06:09:45 UTC",
      "updated_date": "2024-02-23 06:09:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:35:52.376470"
    },
    {
      "arxiv_id": "2402.15116v1",
      "title": "Large Multimodal Agents: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Junlin Xie",
        "Zhihong Chen",
        "Ruifei Zhang",
        "Xiang Wan",
        "Guanbin Li"
      ],
      "abstract": "Large language models (LLMs) have achieved superior performance in powering\ntext-based AI agents, endowing them with decision-making and reasoning\nabilities akin to humans. Concurrently, there is an emerging research trend\nfocused on extending these LLM-powered AI agents into the multimodal domain.\nThis extension enables AI agents to interpret and respond to diverse multimodal\nuser queries, thereby handling more intricate and nuanced tasks. In this paper,\nwe conduct a systematic review of LLM-driven multimodal agents, which we refer\nto as large multimodal agents ( LMAs for short). First, we introduce the\nessential components involved in developing LMAs and categorize the current\nbody of research into four distinct types. Subsequently, we review the\ncollaborative frameworks integrating multiple LMAs , enhancing collective\nefficacy. One of the critical challenges in this field is the diverse\nevaluation methods used across existing studies, hindering effective comparison\namong different LMAs . Therefore, we compile these evaluation methodologies and\nestablish a comprehensive framework to bridge the gaps. This framework aims to\nstandardize evaluations, facilitating more meaningful comparisons. Concluding\nour review, we highlight the extensive applications of LMAs and propose\npossible future research directions. Our discussion aims to provide valuable\ninsights and guidelines for future research in this rapidly evolving field. An\nup-to-date resource list is available at\nhttps://github.com/jun0wanan/awesome-large-multimodal-agents.",
      "tldr_zh": "这篇论文对大型多模态代理（Large Multimodal Agents, LMAs）进行了系统调查，聚焦于将大型语言模型（LLMs）扩展到多模态领域，以增强AI代理的决策和推理能力，从而处理更复杂的用户查询。论文首先介绍了LMAs的关键组件，将现有研究分为四类，并探讨了多个LMAs的协作框架，以提升整体效能；同时，它分析了评估方法的多样性问题，并提出一个综合框架来标准化评估，便于比较。最终，论文强调了LMAs在各种应用中的潜力，并建议了未来的研究方向，提供了一个更新资源列表以支持进一步探索。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "15 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.15116v1",
      "published_date": "2024-02-23 06:04:23 UTC",
      "updated_date": "2024-02-23 06:04:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:36:03.498632"
    },
    {
      "arxiv_id": "2403.00795v2",
      "title": "Executing Natural Language-Described Algorithms with Large Language Models: An Investigation",
      "title_zh": "使用大型语言模型执行用自然语言描述的算法：一项调查",
      "authors": [
        "Xin Zheng",
        "Qiming Zhu",
        "Hongyu Lin",
        "Yaojie Lu",
        "Xianpei Han",
        "Le Sun"
      ],
      "abstract": "Executing computer programs described in natural language has long been a\npursuit of computer science. With the advent of enhanced natural language\nunderstanding capabilities exhibited by large language models (LLMs), the path\ntoward this goal has been illuminated. In this paper, we seek to examine the\ncapacity of present-day LLMs to comprehend and execute algorithms outlined in\nnatural language. We established an algorithm test set sourced from\nIntroduction to Algorithm, a well-known textbook that contains many\nrepresentative widely-used algorithms. To systematically assess LLMs' code\nexecution abilities, we selected 30 algorithms, generated 300 random-sampled\ninstances in total, and evaluated whether popular LLMs can understand and\nexecute these algorithms. Our findings reveal that LLMs, notably GPT-4, can\neffectively execute programs described in natural language, as long as no heavy\nnumeric computation is involved. We believe our findings contribute to\nevaluating LLMs' code execution abilities and would encourage further\ninvestigation and application for the computation power of LLMs.",
      "tldr_zh": "本研究调查了大型语言模型(LLMs)执行用自然语言描述的算法能力，选取了《Introduction to Algorithms》教科书中的30个代表性算法，并生成300个随机实例进行系统评估。结果显示，LLMs如GPT-4在不涉及繁重数值计算的情况下，能够有效理解和执行这些算法。论文强调，此发现有助于评估LLMs的代码执行能力，并鼓励进一步的应用和研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at LREC-COLING 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.00795v2",
      "published_date": "2024-02-23 05:31:36 UTC",
      "updated_date": "2024-03-14 14:25:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:36:16.304871"
    },
    {
      "arxiv_id": "2402.15102v2",
      "title": "Trajectory-wise Iterative Reinforcement Learning Framework for Auto-bidding",
      "title_zh": "基于轨迹的迭代强化学习框架，用于自动竞价",
      "authors": [
        "Haoming Li",
        "Yusen Huo",
        "Shuai Dou",
        "Zhenzhe Zheng",
        "Zhilin Zhang",
        "Chuan Yu",
        "Jian Xu",
        "Fan Wu"
      ],
      "abstract": "In online advertising, advertisers participate in ad auctions to acquire ad\nopportunities, often by utilizing auto-bidding tools provided by demand-side\nplatforms (DSPs). The current auto-bidding algorithms typically employ\nreinforcement learning (RL). However, due to safety concerns, most RL-based\nauto-bidding policies are trained in simulation, leading to a performance\ndegradation when deployed in online environments. To narrow this gap, we can\ndeploy multiple auto-bidding agents in parallel to collect a large interaction\ndataset. Offline RL algorithms can then be utilized to train a new policy. The\ntrained policy can subsequently be deployed for further data collection,\nresulting in an iterative training framework, which we refer to as iterative\noffline RL. In this work, we identify the performance bottleneck of this\niterative offline RL framework, which originates from the ineffective\nexploration and exploitation caused by the inherent conservatism of offline RL\nalgorithms. To overcome this bottleneck, we propose Trajectory-wise Exploration\nand Exploitation (TEE), which introduces a novel data collecting and data\nutilization method for iterative offline RL from a trajectory perspective.\nFurthermore, to ensure the safety of online exploration while preserving the\ndataset quality for TEE, we propose Safe Exploration by Adaptive Action\nSelection (SEAS). Both offline experiments and real-world experiments on\nAlibaba display advertising platform demonstrate the effectiveness of our\nproposed method.",
      "tldr_zh": "这篇论文提出了一种Trajectory-wise Iterative Reinforcement Learning框架，用于在线广告中的Auto-bidding问题，以解决强化学习（RL）算法从模拟环境训练到在线部署的性能下降问题。论文识别出迭代离线RL框架的瓶颈，即离线RL的保守性导致探索和利用无效，并引入Trajectory-wise Exploration and Exploitation (TEE)方法，从轨迹视角优化数据收集和利用。针对在线探索的安全性，作者还提出Safe Exploration by Adaptive Action Selection (SEAS)方法，以保持数据质量。实验结果显示，该框架在阿里巴巴展示广告平台上的真实环境中显著提升了性能，证明了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.GT",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by The Web Conference 2024 (WWW'24) as an oral paper",
      "pdf_url": "http://arxiv.org/pdf/2402.15102v2",
      "published_date": "2024-02-23 05:20:23 UTC",
      "updated_date": "2024-04-08 09:33:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:36:29.238960"
    },
    {
      "arxiv_id": "2402.15537v3",
      "title": "Evaluating the Performance of ChatGPT for Spam Email Detection",
      "title_zh": "评估 ChatGPT 在垃圾邮件检测中的性能",
      "authors": [
        "Shijing Si",
        "Yuwei Wu",
        "Le Tang",
        "Yugui Zhang",
        "Jedrek Wosik",
        "Qinliang Su"
      ],
      "abstract": "Email continues to be a pivotal and extensively utilized communication medium\nwithin professional and commercial domains. Nonetheless, the prevalence of spam\nemails poses a significant challenge for users, disrupting their daily routines\nand diminishing productivity. Consequently, accurately identifying and\nfiltering spam based on content has become crucial for cybersecurity. Recent\nadvancements in natural language processing, particularly with large language\nmodels like ChatGPT, have shown remarkable performance in tasks such as\nquestion answering and text generation. However, its potential in spam\nidentification remains underexplored. To fill in the gap, this study attempts\nto evaluate ChatGPT's capabilities for spam identification in both English and\nChinese email datasets. We employ ChatGPT for spam email detection using\nin-context learning, which requires a prompt instruction with (or without) a\nfew demonstrations. We also investigate how the number of demonstrations in the\nprompt affects the performance of ChatGPT. For comparison, we also implement\nfive popular benchmark methods, including naive Bayes, support vector machines\n(SVM), logistic regression (LR), feedforward dense neural networks (DNN), and\nBERT classifiers. Through extensive experiments, the performance of ChatGPT is\nsignificantly worse than deep supervised learning methods in the large English\ndataset, while it presents superior performance on the low-resourced Chinese\ndataset. This study provides insights into the potential and limitations of\nChatGPT for spam identification, highlighting its potential as a viable\nsolution for resource-constrained language domains.",
      "tldr_zh": "这篇论文评估了 ChatGPT 在垃圾邮件检测中的性能，针对英语和中文电子邮件数据集，使用 in-context learning 方法通过提示指令（可能包括少量演示）来进行检测，并探讨了演示数量对性能的影响。研究将 ChatGPT 与基准模型如 naive Bayes、SVM、logistic regression (LR)、DNN 和 BERT 进行比较。结果显示，在大型英语数据集上，ChatGPT 的表现不如深度监督学习方法，但在资源有限的中文数据集上，ChatGPT 表现出色。该研究揭示了 ChatGPT 在垃圾邮件识别中的潜力与局限性，特别是适用于资源受限语言领域的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 4 figures; Accepted by Pacific Journal of Optimization\n  (PJO)",
      "pdf_url": "http://arxiv.org/pdf/2402.15537v3",
      "published_date": "2024-02-23 04:52:08 UTC",
      "updated_date": "2025-02-12 17:59:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:36:40.625021"
    },
    {
      "arxiv_id": "2402.15089v1",
      "title": "AttributionBench: How Hard is Automatic Attribution Evaluation?",
      "title_zh": "AttributionBench：自动归因评估",
      "authors": [
        "Yifei Li",
        "Xiang Yue",
        "Zeyi Liao",
        "Huan Sun"
      ],
      "abstract": "Modern generative search engines enhance the reliability of large language\nmodel (LLM) responses by providing cited evidence. However, evaluating the\nanswer's attribution, i.e., whether every claim within the generated responses\nis fully supported by its cited evidence, remains an open problem. This\nverification, traditionally dependent on costly human evaluation, underscores\nthe urgent need for automatic attribution evaluation methods. To bridge the gap\nin the absence of standardized benchmarks for these methods, we present\nAttributionBench, a comprehensive benchmark compiled from various existing\nattribution datasets. Our extensive experiments on AttributionBench reveal the\nchallenges of automatic attribution evaluation, even for state-of-the-art LLMs.\nSpecifically, our findings show that even a fine-tuned GPT-3.5 only achieves\naround 80% macro-F1 under a binary classification formulation. A detailed\nanalysis of more than 300 error cases indicates that a majority of failures\nstem from the model's inability to process nuanced information, and the\ndiscrepancy between the information the model has access to and that human\nannotators do.",
      "tldr_zh": "该研究探讨了自动归因(attribution)评估的难度，并引入AttributionBench，这是一个从现有数据集编译的全面基准，用于评估生成式搜索引擎中LLM响应是否完全由引文证据支持。实验结果显示，即使是微调的GPT-3.5在二元分类(binary classification)任务上仅达到约80%的宏F1(macro-F1)分数，突显了自动评估的挑战。通过分析超过300个错误案例，研究发现，大多数失败源于模型处理细微信息(nuanced information)的能力不足，以及模型访问的信息与人类标注者之间的差异。总的来说，AttributionBench为改进自动归因评估方法提供了标准化工具。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15089v1",
      "published_date": "2024-02-23 04:23:33 UTC",
      "updated_date": "2024-02-23 04:23:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:36:52.697303"
    },
    {
      "arxiv_id": "2402.15083v2",
      "title": "Hands-Free VR",
      "title_zh": "翻译失败",
      "authors": [
        "Jorge Askur Vazquez Fernandez",
        "Jae Joong Lee",
        "Santiago Andrés Serrano Vacca",
        "Alejandra Magana",
        "Radim Pesam",
        "Bedrich Benes",
        "Voicu Popescu"
      ],
      "abstract": "The paper introduces Hands-Free VR, a voice-based natural-language interface\nfor VR. The user gives a command using their voice, the speech audio data is\nconverted to text using a speech-to-text deep learning model that is fine-tuned\nfor robustness to word phonetic similarity and to spoken English accents, and\nthe text is mapped to an executable VR command using a large language model\nthat is robust to natural language diversity. Hands-Free VR was evaluated in a\ncontrolled within-subjects study (N = 22) that asked participants to find\nspecific objects and to place them in various configurations. In the control\ncondition participants used a conventional VR user interface to grab, carry,\nand position the objects using the handheld controllers. In the experimental\ncondition participants used Hands-Free VR. The results confirm that: (1)\nHands-Free VR is robust to spoken English accents, as for 20 of our\nparticipants English was not their first language, and to word phonetic\nsimilarity, correctly transcribing the voice command 96.71% of the time; (2)\nHands-Free VR is robust to natural language diversity, correctly mapping the\ntranscribed command to an executable command in 97.83% of the time; (3)\nHands-Free VR had a significant efficiency advantage over the conventional VR\ninterface in terms of task completion time, total viewpoint translation, total\nview direction rotation, and total left and right hand translations; (4)\nHands-Free VR received high user preference ratings in terms of ease of use,\nintuitiveness, ergonomics, reliability, and desirability.",
      "tldr_zh": "本论文介绍了 Hands-Free VR，一种基于语音的自然语言接口，用于虚拟现实（VR）交互，用户通过语音命令实现物体查找和放置，涉及语音到文本的深度学习模型（针对语音相似性和口音微调）和大型语言模型（处理自然语言多样性）。在对照实验中（N=22），Hands-Free VR 展示了高鲁棒性，命令转录准确率达96.71%，命令映射准确率达97.83%。结果显示，Hands-Free VR 在任务完成时间、视角平移和旋转等方面显著优于传统 VR 接口，且在易用性、直观性和可靠性等方面获得高用户偏好。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.HC",
      "comment": "The first two authors contributed equally. Accepted VISIGRAPP@HUCAPP\n  2025",
      "pdf_url": "http://arxiv.org/pdf/2402.15083v2",
      "published_date": "2024-02-23 04:02:23 UTC",
      "updated_date": "2024-12-18 23:11:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:37:04.200657"
    },
    {
      "arxiv_id": "2402.15075v1",
      "title": "Stacking Factorizing Partitioned Expressions in Hybrid Bayesian Network Models",
      "title_zh": "翻译失败",
      "authors": [
        "Peng Lin",
        "Martin Neil",
        "Norman Fenton"
      ],
      "abstract": "Hybrid Bayesian networks (HBN) contain complex conditional probabilistic\ndistributions (CPD) specified as partitioned expressions over discrete and\ncontinuous variables. The size of these CPDs grows exponentially with the\nnumber of parent nodes when using discrete inference, resulting in significant\ninefficiency. Normally, an effective way to reduce the CPD size is to use a\nbinary factorization (BF) algorithm to decompose the statistical or arithmetic\nfunctions in the CPD by factorizing the number of connected parent nodes to\nsets of size two. However, the BF algorithm was not designed to handle\npartitioned expressions. Hence, we propose a new algorithm called stacking\nfactorization (SF) to decompose the partitioned expressions. The SF algorithm\ncreates intermediate nodes to incrementally reconstruct the densities in the\noriginal partitioned expression, allowing no more than two continuous parent\nnodes to be connected to each child node in the resulting HBN. SF can be either\nused independently or combined with the BF algorithm. We show that the SF+BF\nalgorithm significantly reduces the CPD size and contributes to lowering the\ntree-width of a model, thus improving efficiency.",
      "tldr_zh": "本研究针对Hybrid Bayesian Networks (HBN)中复杂的Conditional Probabilistic Distributions (CPD)，这些分布作为分区表达式会导致CPD大小指数增长，影响离散推理效率。为解决Binary Factorization (BF)算法无法处理分区表达的问题，提出了一种新算法Stacking Factorization (SF)。SF通过创建中间节点逐步重建密度，确保结果HBN中每个子节点连接不超过两个连续父节点，并可与BF算法结合使用。实验结果显示，SF+BF显著减少CPD大小、降低模型的tree-width，从而提高了整体推理效率。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15075v1",
      "published_date": "2024-02-23 03:33:06 UTC",
      "updated_date": "2024-02-23 03:33:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:37:17.214968"
    },
    {
      "arxiv_id": "2403.00794v2",
      "title": "Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models",
      "title_zh": "严肃对待幽默：使用不幽默的大型语言模型创建幽默数据集",
      "authors": [
        "Zachary Horvitz",
        "Jingru Chen",
        "Rahul Aditya",
        "Harshvardhan Srivastava",
        "Robert West",
        "Zhou Yu",
        "Kathleen McKeown"
      ],
      "abstract": "Humor is a fundamental facet of human cognition and interaction. Yet, despite\nrecent advances in natural language processing, humor detection remains a\nchallenging task that is complicated by the scarcity of datasets that pair\nhumorous texts with similar non-humorous counterparts. In our work, we\ninvestigate whether large language models (LLMs), can generate synthetic data\nfor humor detection via editing texts. We benchmark LLMs on an existing human\ndataset and show that current LLMs display an impressive ability to 'unfun'\njokes, as judged by humans and as measured on the downstream task of humor\ndetection. We extend our approach to a code-mixed English-Hindi humor dataset,\nwhere we find that GPT-4's synthetic data is highly rated by bilingual\nannotators and provides challenging adversarial examples for humor classifiers.",
      "tldr_zh": "本研究探讨了使用大型语言模型（LLMs）生成合成数据以提升幽默检测任务，针对幽默文本与非幽默文本配对数据集的稀缺问题。研究者通过让LLMs编辑文本来“去幽默化”笑话，并在现有人类数据集上进行基准测试，结果显示LLMs在幽默编辑方面表现出色，并获得人类评判的认可。进一步扩展到英语-印地语代码混合数据集，GPT-4生成的合成数据被双语标注者高度评价，并为幽默分类器提供了具有挑战性的对抗示例（adversarial examples）。这为改进幽默检测模型提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00794v2",
      "published_date": "2024-02-23 02:58:12 UTC",
      "updated_date": "2024-06-21 17:12:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:37:31.243455"
    },
    {
      "arxiv_id": "2402.15057v1",
      "title": "On the Multi-turn Instruction Following for Conversational Web Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Yang Deng",
        "Xuan Zhang",
        "Wenxuan Zhang",
        "Yifei Yuan",
        "See-Kiong Ng",
        "Tat-Seng Chua"
      ],
      "abstract": "Web agents powered by Large Language Models (LLMs) have demonstrated\nremarkable abilities in planning and executing multi-step interactions within\ncomplex web-based environments, fulfilling a wide range of web navigation\ntasks. Despite these advancements, the potential for LLM-powered agents to\neffectively engage with sequential user instructions in real-world scenarios\nhas not been fully explored. In this work, we introduce a new task of\nConversational Web Navigation, which necessitates sophisticated interactions\nthat span multiple turns with both the users and the environment, supported by\na specially developed dataset named Multi-Turn Mind2Web (MT-Mind2Web). To\ntackle the limited context length of LLMs and the context-dependency issue of\nthe conversational tasks, we further propose a novel framework, named\nself-reflective memory-augmented planning (Self-MAP), which employs memory\nutilization and self-reflection techniques. Extensive experiments are conducted\nto benchmark the MT-Mind2Web dataset, and validate the effectiveness of the\nproposed method.",
      "tldr_zh": "本研究探讨了Large Language Models (LLMs)驱动的Web代理在处理多轮指令时的能力，引入了Conversational Web Navigation新任务，该任务需要代理与用户和环境进行多轮复杂交互，并为此开发了Multi-Turn Mind2Web (MT-Mind2Web)数据集。针对LLMs的上下文长度限制和对话任务的上下文依赖问题，论文提出了一种新型框架self-reflective memory-augmented planning (Self-MAP)，通过记忆利用和自我反思技术来提升代理的规划和执行效率。实验结果显示，该框架在MT-Mind2Web数据集上表现出色，验证了其在真实场景中提升Web代理交互性能的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15057v1",
      "published_date": "2024-02-23 02:18:12 UTC",
      "updated_date": "2024-02-23 02:18:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:37:41.208637"
    },
    {
      "arxiv_id": "2402.15055v2",
      "title": "Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions",
      "title_zh": "翻译失败",
      "authors": [
        "Clement Neo",
        "Shay B. Cohen",
        "Fazl Barez"
      ],
      "abstract": "Understanding the inner workings of large language models (LLMs) is crucial\nfor advancing their theoretical foundations and real-world applications. While\nthe attention mechanism and multi-layer perceptrons (MLPs) have been studied\nindependently, their interactions remain largely unexplored. This study\ninvestigates how attention heads and next-token neurons interact in LLMs to\npredict new words. We propose a methodology to identify next-token neurons,\nfind prompts that highly activate them, and determine the upstream attention\nheads responsible. We then generate and evaluate explanations for the activity\nof these attention heads in an automated manner. Our findings reveal that some\nattention heads recognize specific contexts relevant to predicting a token and\nactivate a downstream token-predicting neuron accordingly. This mechanism\nprovides a deeper understanding of how attention heads work with MLP neurons to\nperform next-token prediction. Our approach offers a foundation for further\nresearch into the intricate workings of LLMs and their impact on text\ngeneration and understanding.",
      "tldr_zh": "本研究探讨大型语言模型（LLMs）中 attention heads 与 MLP neurons 的互动，旨在揭示它们如何共同预测下一个词（next-token prediction）。研究提出了一种方法，包括识别 next-token neurons、找出高度激活它们的 prompts、确定上游 attention heads，并自动生成和评估这些 attention heads 的活动解释。结果显示，一些 attention heads 能够识别特定上下文，并据此激活下游的 token-predicting neuron，从而加深了对 LLMs 内部机制的理解。该方法为进一步研究 LLMs 在文本生成和理解中的复杂作用奠定了基础。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP 2024 Main Conference",
      "pdf_url": "http://arxiv.org/pdf/2402.15055v2",
      "published_date": "2024-02-23 02:15:47 UTC",
      "updated_date": "2024-10-23 13:20:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:37:55.318605"
    },
    {
      "arxiv_id": "2402.15052v2",
      "title": "ToMBench: Benchmarking Theory of Mind in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zhuang Chen",
        "Jincenzi Wu",
        "Jinfeng Zhou",
        "Bosi Wen",
        "Guanqun Bi",
        "Gongyao Jiang",
        "Yaru Cao",
        "Mengting Hu",
        "Yunghwei Lai",
        "Zexuan Xiong",
        "Minlie Huang"
      ],
      "abstract": "Theory of Mind (ToM) is the cognitive capability to perceive and ascribe\nmental states to oneself and others. Recent research has sparked a debate over\nwhether large language models (LLMs) exhibit a form of ToM. However, existing\nToM evaluations are hindered by challenges such as constrained scope,\nsubjective judgment, and unintended contamination, yielding inadequate\nassessments. To address this gap, we introduce ToMBench with three key\ncharacteristics: a systematic evaluation framework encompassing 8 tasks and 31\nabilities in social cognition, a multiple-choice question format to support\nautomated and unbiased evaluation, and a build-from-scratch bilingual inventory\nto strictly avoid data leakage. Based on ToMBench, we conduct extensive\nexperiments to evaluate the ToM performance of 10 popular LLMs across tasks and\nabilities. We find that even the most advanced LLMs like GPT-4 lag behind human\nperformance by over 10% points, indicating that LLMs have not achieved a\nhuman-level theory of mind yet. Our aim with ToMBench is to enable an efficient\nand effective evaluation of LLMs' ToM capabilities, thereby facilitating the\ndevelopment of LLMs with inherent social intelligence.",
      "tldr_zh": "本研究提出 ToMBench，一种用于评估大型语言模型 (LLMs) 的 Theory of Mind (ToM) 能力的系统基准，以解决现有评估的范围有限、主观性和数据污染问题。ToMBench 包括 8 个任务和 31 个社会认知能力、采用多项选择题格式以实现自动化无偏见评估，并构建从零开始的双语库存避免数据泄露。实验评估了 10 个流行 LLMs 的表现，发现即使像 GPT-4 这样先进的模型也落后人类超过 10%，表明 LLMs 尚未达到人类水平的 ToM。通过 ToMBench，该研究旨在促进 LLMs 社会智能的发展，提供高效评估工具。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.15052v2",
      "published_date": "2024-02-23 02:05:46 UTC",
      "updated_date": "2024-12-08 07:20:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:38:06.403743"
    },
    {
      "arxiv_id": "2402.15048v2",
      "title": "Unlocking the Power of Large Language Models for Entity Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Xuhui Jiang",
        "Yinghan Shen",
        "Zhichao Shi",
        "Chengjin Xu",
        "Wei Li",
        "Zixuan Li",
        "Jian Guo",
        "Huawei Shen",
        "Yuanzhuo Wang"
      ],
      "abstract": "Entity Alignment (EA) is vital for integrating diverse knowledge graph (KG)\ndata, playing a crucial role in data-driven AI applications. Traditional EA\nmethods primarily rely on comparing entity embeddings, but their effectiveness\nis constrained by the limited input KG data and the capabilities of the\nrepresentation learning techniques. Against this backdrop, we introduce ChatEA,\nan innovative framework that incorporates large language models (LLMs) to\nimprove EA. To address the constraints of limited input KG data, ChatEA\nintroduces a KG-code translation module that translates KG structures into a\nformat understandable by LLMs, thereby allowing LLMs to utilize their extensive\nbackground knowledge to improve EA accuracy. To overcome the over-reliance on\nentity embedding comparisons, ChatEA implements a two-stage EA strategy that\ncapitalizes on LLMs' capability for multi-step reasoning in a dialogue format,\nthereby enhancing accuracy while preserving efficiency. Our experimental\nresults verify ChatEA's superior performance, highlighting LLMs' potential in\nfacilitating EA tasks.",
      "tldr_zh": "实体对齐 (Entity Alignment) 是整合知识图谱 (KG) 数据的重要技术，但传统方法依赖实体嵌入比较，受限于输入数据和表示学习能力。论文提出 ChatEA 框架，利用大型语言模型 (LLMs) 通过 KG-code 翻译模块将 KG 结构转化为 LLMs 可理解的格式，结合 LLMs 的背景知识提升对齐准确性；同时采用两阶段策略，利用多步推理和对话格式增强效率。实验结果证明 ChatEA 优于传统方法，突显 LLMs 在 Entity Alignment 任务中的巨大潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15048v2",
      "published_date": "2024-02-23 01:55:35 UTC",
      "updated_date": "2024-10-09 03:22:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:38:17.679841"
    },
    {
      "arxiv_id": "2402.15043v2",
      "title": "KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zhuohao Yu",
        "Chang Gao",
        "Wenjin Yao",
        "Yidong Wang",
        "Wei Ye",
        "Jindong Wang",
        "Xing Xie",
        "Yue Zhang",
        "Shikun Zhang"
      ],
      "abstract": "Automatic evaluation methods for large language models (LLMs) are hindered by\ndata contamination, leading to inflated assessments of their effectiveness.\nExisting strategies, which aim to detect contaminated texts, focus on\nquantifying contamination status instead of accurately gauging model\nperformance. In this paper, we introduce KIEval, a Knowledge-grounded\nInteractive Evaluation framework, which incorporates an LLM-powered\n\"interactor\" role for the first time to accomplish a dynamic\ncontamination-resilient evaluation. Starting with a question in a conventional\nLLM benchmark involving domain-specific knowledge, KIEval utilizes dynamically\ngenerated, multi-round, and knowledge-focused dialogues to determine whether a\nmodel's response is merely a recall of benchmark answers or demonstrates a deep\ncomprehension to apply knowledge in more complex conversations. Extensive\nexperiments on seven leading LLMs across five datasets validate KIEval's\neffectiveness and generalization. We also reveal that data contamination brings\nno contribution or even negative effect to models' real-world applicability and\nunderstanding, and existing contamination detection methods for LLMs can only\nidentify contamination in pre-training but not during supervised fine-tuning.",
      "tldr_zh": "该论文提出 KIEval，一种基于知识 grounding 的交互式评估框架，用于评估大型语言模型（LLMs），以解决数据 contamination 导致的评估结果夸大问题。KIEval 首次引入 LLM 驱动的“interactor”角色，通过动态生成的多轮知识焦点对话，从标准基准问题出发，检验模型是否深度理解知识而非简单回忆答案。实验在七个领先 LLMs 和五个数据集上验证了 KIEval 的有效性和泛化性，并发现数据 contamination 对模型的实际应用无益甚至有害，而现有 contamination 检测方法仅能识别预训练阶段的污染，而非 supervised fine-tuning 中的。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ACL 2024 (main conference); 19 pages, 5 figures, 19\n  tables, code is available at: https://github.com/zhuohaoyu/KIEval",
      "pdf_url": "http://arxiv.org/pdf/2402.15043v2",
      "published_date": "2024-02-23 01:30:39 UTC",
      "updated_date": "2024-06-03 06:02:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:38:30.193698"
    },
    {
      "arxiv_id": "2402.15038v2",
      "title": "Dynamics-Guided Diffusion Model for Sensor-less Robot Manipulator Design",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaomeng Xu",
        "Huy Ha",
        "Shuran Song"
      ],
      "abstract": "We present Dynamics-Guided Diffusion Model (DGDM), a data-driven framework\nfor generating task-specific manipulator designs without task-specific\ntraining. Given object shapes and task specifications, DGDM generates\nsensor-less manipulator designs that can blindly manipulate objects towards\ndesired motions and poses using an open-loop parallel motion. This framework 1)\nflexibly represents manipulation tasks as interaction profiles, 2) represents\nthe design space using a geometric diffusion model, and 3) efficiently searches\nthis design space using the gradients provided by a dynamics network trained\nwithout any task information. We evaluate DGDM on various manipulation tasks\nranging from shifting/rotating objects to converging objects to a specific\npose. Our generated designs outperform optimization-based and unguided\ndiffusion baselines relatively by 31.5% and 45.3% on average success rate. With\nthe ability to generate a new design within 0.8s, DGDM facilitates rapid design\niteration and enhances the adoption of data-driven approaches for robot\nmechanism design. Qualitative results are best viewed on our project website\nhttps://dgdm-robot.github.io/.",
      "tldr_zh": "本文提出 Dynamics-Guided Diffusion Model (DGDM)，一个数据驱动框架，用于生成无需传感器(task-specific)的机械臂设计，仅基于物体形状和任务规格。DGDM 通过将操控任务表示为交互配置文件、使用几何扩散模型表示设计空间，并利用动态网络提供的梯度进行高效搜索，实现开环平行运动操控物体。实验结果显示，该方法在移位/旋转物体等任务上，比优化-based 和无指导扩散基线分别提高了31.5%和45.3%的平均成功率，并能在0.8秒内生成新设计，促进机器人机制设计的快速迭代和数据驱动方法的应用。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15038v2",
      "published_date": "2024-02-23 01:19:30 UTC",
      "updated_date": "2025-03-28 02:09:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:38:43.468986"
    },
    {
      "arxiv_id": "2403.10534v1",
      "title": "VISREAS: Complex Visual Reasoning with Unanswerable Questions",
      "title_zh": "VISREAS：复杂视觉推理与无法回答的问题",
      "authors": [
        "Syeda Nahida Akter",
        "Sangwu Lee",
        "Yingshan Chang",
        "Yonatan Bisk",
        "Eric Nyberg"
      ],
      "abstract": "Verifying a question's validity before answering is crucial in real-world\napplications, where users may provide imperfect instructions. In this scenario,\nan ideal model should address the discrepancies in the query and convey them to\nthe users rather than generating the best possible answer. Addressing this\nrequirement, we introduce a new compositional visual question-answering\ndataset, VISREAS, that consists of answerable and unanswerable visual queries\nformulated by traversing and perturbing commonalities and differences among\nobjects, attributes, and relations. VISREAS contains 2.07M semantically diverse\nqueries generated automatically using Visual Genome scene graphs. The unique\nfeature of this task, validating question answerability with respect to an\nimage before answering, and the poor performance of state-of-the-art models\ninspired the design of a new modular baseline, LOGIC2VISION that reasons by\nproducing and executing pseudocode without any external modules to generate the\nanswer. LOGIC2VISION outperforms generative models in VISREAS (+4.82% over\nLLaVA-1.5; +12.23% over InstructBLIP) and achieves a significant gain in\nperformance against the classification models.",
      "tldr_zh": "该研究引入了 VISREAS 数据集，这是一个新的组合视觉问答数据集，包含 2.07M 个基于 Visual Genome 场景图自动生成的语义多样查询，包括可回答和不可回答的问题，通过遍历和扰动对象、属性及关系的共同点和差异来模拟现实中的不完美指令。\nVISREAS 的核心任务是先验证问题相对于图像的可回答性，然后再进行回答，为此设计了模块化基线模型 LOGIC2VISION，该模型通过生成和执行伪代码进行推理，而不依赖外部模块。\n实验结果表明，LOGIC2VISION 在 VISREAS 上超过了生成模型（如 LLaVA-1.5 提高 4.82%、InstructBLIP 提高 12.23%），并在分类模型上表现出显著优势。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "18 pages, 14 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2403.10534v1",
      "published_date": "2024-02-23 00:12:10 UTC",
      "updated_date": "2024-02-23 00:12:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T09:38:54.885724"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 98,
  "processed_papers_count": 98,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-17T09:39:20.139946"
}
[
  {
    "arxiv_id": "2403.04957v1",
    "title": "Automatic and Universal Prompt Injection Attacks against Large Language Models",
    "authors": [
      "Xiaogeng Liu",
      "Zhiyuan Yu",
      "Yizhe Zhang",
      "Ning Zhang",
      "Chaowei Xiao"
    ],
    "abstract": "Large Language Models (LLMs) excel in processing and generating human\nlanguage, powered by their ability to interpret and follow instructions.\nHowever, their capabilities can be exploited through prompt injection attacks.\nThese attacks manipulate LLM-integrated applications into producing responses\naligned with the attacker's injected content, deviating from the user's actual\nrequests. The substantial risks posed by these attacks underscore the need for\na thorough understanding of the threats. Yet, research in this area faces\nchallenges due to the lack of a unified goal for such attacks and their\nreliance on manually crafted prompts, complicating comprehensive assessments of\nprompt injection robustness. We introduce a unified framework for understanding\nthe objectives of prompt injection attacks and present an automated\ngradient-based method for generating highly effective and universal prompt\ninjection data, even in the face of defensive measures. With only five training\nsamples (0.3% relative to the test data), our attack can achieve superior\nperformance compared with baselines. Our findings emphasize the importance of\ngradient-based testing, which can avoid overestimation of robustness,\nespecially for defense mechanisms.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Pre-print, code is available at\n  https://github.com/SheltonLiu-N/Universal-Prompt-Injection",
    "pdf_url": "http://arxiv.org/pdf/2403.04957v1",
    "published_date": "2024-03-07 23:46:20 UTC",
    "updated_date": "2024-03-07 23:46:20 UTC"
  },
  {
    "arxiv_id": "2403.04954v2",
    "title": "Fooling Neural Networks for Motion Forecasting via Adversarial Attacks",
    "authors": [
      "Edgar Medina",
      "Leyong Loh"
    ],
    "abstract": "Human motion prediction is still an open problem, which is extremely\nimportant for autonomous driving and safety applications. Although there are\ngreat advances in this area, the widely studied topic of adversarial attacks\nhas not been applied to multi-regression models such as GCNs and MLP-based\narchitectures in human motion prediction. This work intends to reduce this gap\nusing extensive quantitative and qualitative experiments in state-of-the-art\narchitectures similar to the initial stages of adversarial attacks in image\nclassification. The results suggest that models are susceptible to attacks even\non low levels of perturbation. We also show experiments with 3D transformations\nthat affect the model performance, in particular, we show that most models are\nsensitive to simple rotations and translations which do not alter joint\ndistances. We conclude that similar to earlier CNN models, motion forecasting\ntasks are susceptible to small perturbations and simple 3D transformations.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, 8 figures, VISSAP 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.04954v2",
    "published_date": "2024-03-07 23:44:10 UTC",
    "updated_date": "2024-03-11 09:37:39 UTC"
  },
  {
    "arxiv_id": "2403.04940v1",
    "title": "A spatiotemporal style transfer algorithm for dynamic visual stimulus generation",
    "authors": [
      "Antonino Greco",
      "Markus Siegel"
    ],
    "abstract": "Understanding how visual information is encoded in biological and artificial\nsystems often requires vision scientists to generate appropriate stimuli to\ntest specific hypotheses. Although deep neural network models have\nrevolutionized the field of image generation with methods such as image style\ntransfer, available methods for video generation are scarce. Here, we introduce\nthe Spatiotemporal Style Transfer (STST) algorithm, a dynamic visual stimulus\ngeneration framework that allows powerful manipulation and synthesis of video\nstimuli for vision research. It is based on a two-stream deep neural network\nmodel that factorizes spatial and temporal features to generate dynamic visual\nstimuli whose model layer activations are matched to those of input videos. As\nan example, we show that our algorithm enables the generation of model\nmetamers, dynamic stimuli whose layer activations within our two-stream model\nare matched to those of natural videos. We show that these generated stimuli\nmatch the low-level spatiotemporal features of their natural counterparts but\nlack their high-level semantic features, making it a powerful paradigm to study\nobject recognition. Late layer activations in deep vision models exhibited a\nlower similarity between natural and metameric stimuli compared to early\nlayers, confirming the lack of high-level information in the generated stimuli.\nFinally, we use our generated stimuli to probe the representational\ncapabilities of predictive coding deep networks. These results showcase\npotential applications of our algorithm as a versatile tool for dynamic\nstimulus generation in vision science.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04940v1",
    "published_date": "2024-03-07 23:07:46 UTC",
    "updated_date": "2024-03-07 23:07:46 UTC"
  },
  {
    "arxiv_id": "2403.04934v2",
    "title": "LeTac-MPC: Learning Model Predictive Control for Tactile-reactive Grasping",
    "authors": [
      "Zhengtong Xu",
      "Yu She"
    ],
    "abstract": "Grasping is a crucial task in robotics, necessitating tactile feedback and\nreactive grasping adjustments for robust grasping of objects under various\nconditions and with differing physical properties. In this paper, we introduce\nLeTac-MPC, a learning-based model predictive control (MPC) for tactile-reactive\ngrasping. Our approach enables the gripper to grasp objects with different\nphysical properties on dynamic and force-interactive tasks. We utilize a\nvision-based tactile sensor, GelSight, which is capable of perceiving\nhigh-resolution tactile feedback that contains information on the physical\nproperties and states of the grasped object. LeTac-MPC incorporates a\ndifferentiable MPC layer designed to model the embeddings extracted by a neural\nnetwork (NN) from tactile feedback. This design facilitates convergent and\nrobust grasping control at a frequency of 25 Hz. We propose a fully automated\ndata collection pipeline and collect a dataset only using standardized blocks\nwith different physical properties. However, our trained controller can\ngeneralize to daily objects with different sizes, shapes, materials, and\ntextures. The experimental results demonstrate the effectiveness and robustness\nof the proposed approach. We compare LeTac-MPC with two purely model-based\ntactile-reactive controllers (MPC and PD) and open-loop grasping. Our results\nshow that LeTac-MPC has optimal performance in dynamic and force-interactive\ntasks and optimal generalizability. We release our code and dataset at\nhttps://github.com/ZhengtongXu/LeTac-MPC.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04934v2",
    "published_date": "2024-03-07 22:42:24 UTC",
    "updated_date": "2024-09-07 20:57:23 UTC"
  },
  {
    "arxiv_id": "2403.04931v2",
    "title": "A Survey on Human-AI Teaming with Large Pre-Trained Models",
    "authors": [
      "Vanshika Vats",
      "Marzia Binta Nizam",
      "Minghao Liu",
      "Ziyuan Wang",
      "Richard Ho",
      "Mohnish Sai Prasad",
      "Vincent Titterton",
      "Sai Venkat Malreddy",
      "Riya Aggarwal",
      "Yanwen Xu",
      "Lei Ding",
      "Jay Mehta",
      "Nathan Grinnell",
      "Li Liu",
      "Sijia Zhong",
      "Devanathan Nallur Gandamani",
      "Xinyi Tang",
      "Rohan Ghosalkar",
      "Celeste Shen",
      "Rachel Shen",
      "Nafisa Hussain",
      "Kesav Ravichandran",
      "James Davis"
    ],
    "abstract": "In the rapidly evolving landscape of artificial intelligence (AI), the\ncollaboration between human intelligence and AI systems, known as Human-AI\n(HAI) Teaming, has emerged as a cornerstone for advancing problem-solving and\ndecision-making processes. The advent of Large Pre-trained Models (LPtM) has\nsignificantly transformed this landscape, offering unprecedented capabilities\nby leveraging vast amounts of data to understand and predict complex patterns.\nThis paper surveys the pivotal integration of LPtMs with HAI, emphasizing how\nthese models enhance collaborative intelligence beyond traditional approaches.\nIt examines the potential of LPtMs in augmenting human capabilities, discussing\nthis collaboration for AI model improvements, effective teaming, ethical\nconsiderations, and their broad applied implications in various sectors.\nThrough this exploration, the study sheds light on the transformative impact of\nLPtM-enhanced HAI Teaming, providing insights for future research, policy\ndevelopment, and strategic implementations aimed at harnessing the full\npotential of this collaboration for research and societal benefit.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04931v2",
    "published_date": "2024-03-07 22:37:49 UTC",
    "updated_date": "2024-06-26 23:44:48 UTC"
  },
  {
    "arxiv_id": "2403.04929v1",
    "title": "On the Markov Property of Neural Algorithmic Reasoning: Analyses and Methods",
    "authors": [
      "Montgomery Bohde",
      "Meng Liu",
      "Alexandra Saxton",
      "Shuiwang Ji"
    ],
    "abstract": "Neural algorithmic reasoning is an emerging research direction that endows\nneural networks with the ability to mimic algorithmic executions step-by-step.\nA common paradigm in existing designs involves the use of historical embeddings\nin predicting the results of future execution steps. Our observation in this\nwork is that such historical dependence intrinsically contradicts the Markov\nnature of algorithmic reasoning tasks. Based on this motivation, we present our\nForgetNet, which does not use historical embeddings and thus is consistent with\nthe Markov nature of the tasks. To address challenges in training ForgetNet at\nearly stages, we further introduce G-ForgetNet, which uses a gating mechanism\nto allow for the selective integration of historical embeddings. Such an\nenhanced capability provides valuable computational pathways during the model's\nearly training phase. Our extensive experiments, based on the CLRS-30\nalgorithmic reasoning benchmark, demonstrate that both ForgetNet and\nG-ForgetNet achieve better generalization capability than existing methods.\nFurthermore, we investigate the behavior of the gating mechanism, highlighting\nits degree of alignment with our intuitions and its effectiveness for robust\nperformance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear at ICLR 2024 (Spotlight paper). 17 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.04929v1",
    "published_date": "2024-03-07 22:35:22 UTC",
    "updated_date": "2024-03-07 22:35:22 UTC"
  },
  {
    "arxiv_id": "2403.04919v2",
    "title": "Identifying Causal Effects Under Functional Dependencies",
    "authors": [
      "Yizuo Chen",
      "Adnan Darwiche"
    ],
    "abstract": "We study the identification of causal effects, motivated by two improvements\nto identifiability which can be attained if one knows that some variables in a\ncausal graph are functionally determined by their parents (without needing to\nknow the specific functions). First, an unidentifiable causal effect may become\nidentifiable when certain variables are functional. Second, certain functional\nvariables can be excluded from being observed without affecting the\nidentifiability of a causal effect, which may significantly reduce the number\nof needed variables in observational data. Our results are largely based on an\nelimination procedure which removes functional variables from a causal graph\nwhile preserving key properties in the resulting causal graph, including the\nidentifiability of causal effects.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SC",
      "stat.ME"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04919v2",
    "published_date": "2024-03-07 22:04:35 UTC",
    "updated_date": "2024-05-22 21:43:39 UTC"
  },
  {
    "arxiv_id": "2403.04917v3",
    "title": "A Mixed-Integer Conic Program for the Moving-Target Traveling Salesman Problem based on a Graph of Convex Sets",
    "authors": [
      "Allen George Philip",
      "Zhongqiang Ren",
      "Sivakumar Rathinam",
      "Howie Choset"
    ],
    "abstract": "This paper introduces a new formulation that finds the optimum for the\nMoving-Target Traveling Salesman Problem (MT-TSP), which seeks to find a\nshortest path for an agent, that starts at a depot, visits a set of moving\ntargets exactly once within their assigned time-windows, and returns to the\ndepot. The formulation relies on the key idea that when the targets move along\nlines, their trajectories become convex sets within the space-time coordinate\nsystem. The problem then reduces to finding the shortest path within a graph of\nconvex sets, subject to some speed constraints. We compare our formulation with\nthe current state-of-the-art Mixed Integer Conic Program (MICP) solver for the\nMT-TSP. The experimental results show that our formulation outperforms the MICP\nfor instances with up to 20 targets, with up to two orders of magnitude\nreduction in runtime, and up to a 60\\% tighter optimality gap. We also show\nthat the solution cost from the convex relaxation of our formulation provides\nsignificantly tighter lower bounds for the MT-TSP than the ones from the MICP.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.DS"
    ],
    "primary_category": "cs.RO",
    "comment": "7 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.04917v3",
    "published_date": "2024-03-07 22:03:36 UTC",
    "updated_date": "2025-01-13 20:28:04 UTC"
  },
  {
    "arxiv_id": "2403.04899v2",
    "title": "Towards Scene Graph Anticipation",
    "authors": [
      "Rohith Peddi",
      "Saksham Singh",
      "Saurabh",
      "Parag Singla",
      "Vibhav Gogate"
    ],
    "abstract": "Spatio-temporal scene graphs represent interactions in a video by decomposing\nscenes into individual objects and their pair-wise temporal relationships.\nLong-term anticipation of the fine-grained pair-wise relationships between\nobjects is a challenging problem. To this end, we introduce the task of Scene\nGraph Anticipation (SGA). We adapt state-of-the-art scene graph generation\nmethods as baselines to anticipate future pair-wise relationships between\nobjects and propose a novel approach SceneSayer. In SceneSayer, we leverage\nobject-centric representations of relationships to reason about the observed\nvideo frames and model the evolution of relationships between objects. We take\na continuous time perspective and model the latent dynamics of the evolution of\nobject interactions using concepts of NeuralODE and NeuralSDE, respectively. We\ninfer representations of future relationships by solving an Ordinary\nDifferential Equation and a Stochastic Differential Equation, respectively.\nExtensive experimentation on the Action Genome dataset validates the efficacy\nof the proposed methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024, Code: https://github.com/rohithpeddi/SceneSayer",
    "pdf_url": "http://arxiv.org/pdf/2403.04899v2",
    "published_date": "2024-03-07 21:08:51 UTC",
    "updated_date": "2024-07-19 12:40:28 UTC"
  },
  {
    "arxiv_id": "2403.04894v1",
    "title": "ConstitutionalExperts: Training a Mixture of Principle-based Prompts",
    "authors": [
      "Savvas Petridis",
      "Ben Wedin",
      "Ann Yuan",
      "James Wexler",
      "Nithum Thain"
    ],
    "abstract": "Large language models (LLMs) are highly capable at a variety of tasks given\nthe right prompt, but writing one is still a difficult and tedious process. In\nthis work, we introduce ConstitutionalExperts, a method for learning a prompt\nconsisting of constitutional principles (i.e. rules), given a training dataset.\nUnlike prior methods that optimize the prompt as a single entity, our method\nincrementally improves the prompt by surgically editing individual principles.\nWe also show that we can improve overall performance by learning unique prompts\nfor different semantic regions of the training data and using a\nmixture-of-experts (MoE) architecture to route inputs at inference time. We\ncompare our method to other state of the art prompt-optimization techniques\nacross six benchmark datasets. We also investigate whether MoE improves these\nother techniques. Our results suggest that ConstitutionalExperts outperforms\nother prompt optimization techniques by 10.9% (F1) and that mixture-of-experts\nimproves all techniques, suggesting its broad applicability.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04894v1",
    "published_date": "2024-03-07 20:58:04 UTC",
    "updated_date": "2024-03-07 20:58:04 UTC"
  },
  {
    "arxiv_id": "2403.04893v1",
    "title": "A Safe Harbor for AI Evaluation and Red Teaming",
    "authors": [
      "Shayne Longpre",
      "Sayash Kapoor",
      "Kevin Klyman",
      "Ashwin Ramaswami",
      "Rishi Bommasani",
      "Borhane Blili-Hamelin",
      "Yangsibo Huang",
      "Aviya Skowron",
      "Zheng-Xin Yong",
      "Suhas Kotha",
      "Yi Zeng",
      "Weiyan Shi",
      "Xianjun Yang",
      "Reid Southen",
      "Alexander Robey",
      "Patrick Chao",
      "Diyi Yang",
      "Ruoxi Jia",
      "Daniel Kang",
      "Sandy Pentland",
      "Arvind Narayanan",
      "Percy Liang",
      "Peter Henderson"
    ],
    "abstract": "Independent evaluation and red teaming are critical for identifying the risks\nposed by generative AI systems. However, the terms of service and enforcement\nstrategies used by prominent AI companies to deter model misuse have\ndisincentives on good faith safety evaluations. This causes some researchers to\nfear that conducting such research or releasing their findings will result in\naccount suspensions or legal reprisal. Although some companies offer researcher\naccess programs, they are an inadequate substitute for independent research\naccess, as they have limited community representation, receive inadequate\nfunding, and lack independence from corporate incentives. We propose that major\nAI developers commit to providing a legal and technical safe harbor,\nindemnifying public interest safety research and protecting it from the threat\nof account suspensions or legal reprisal. These proposals emerged from our\ncollective experience conducting safety, privacy, and trustworthiness research\non generative AI systems, where norms and incentives could be better aligned\nwith public interests, without exacerbating model misuse. We believe these\ncommitments are a necessary step towards more inclusive and unimpeded community\nefforts to tackle the risks of generative AI.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04893v1",
    "published_date": "2024-03-07 20:55:08 UTC",
    "updated_date": "2024-03-07 20:55:08 UTC"
  },
  {
    "arxiv_id": "2403.04866v1",
    "title": "A Modular End-to-End Multimodal Learning Method for Structured and Unstructured Data",
    "authors": [
      "Marco D Alessandro",
      "Enrique Calabrés",
      "Mikel Elkano"
    ],
    "abstract": "Multimodal learning is a rapidly growing research field that has\nrevolutionized multitasking and generative modeling in AI. While much of the\nresearch has focused on dealing with unstructured data (e.g., language, images,\naudio, or video), structured data (e.g., tabular data, time series, or signals)\nhas received less attention. However, many industry-relevant use cases involve\nor can be benefited from both types of data. In this work, we propose a\nmodular, end-to-end multimodal learning method called MAGNUM, which can\nnatively handle both structured and unstructured data. MAGNUM is flexible\nenough to employ any specialized unimodal module to extract, compress, and fuse\ninformation from all available modalities.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2403.04866v1",
    "published_date": "2024-03-07 19:29:36 UTC",
    "updated_date": "2024-03-07 19:29:36 UTC"
  },
  {
    "arxiv_id": "2403.04859v2",
    "title": "Self-Supervision in Time for Satellite Images(S3-TSS): A novel method of SSL technique in Satellite images",
    "authors": [
      "Akansh Maurya",
      "Hewan Shrestha",
      "Mohammad Munem Shahriar"
    ],
    "abstract": "With the limited availability of labeled data with various atmospheric\nconditions in remote sensing images, it seems useful to work with\nself-supervised algorithms. Few pretext-based algorithms, including from\nrotation, spatial context and jigsaw puzzles are not appropriate for satellite\nimages. Often, satellite images have a higher temporal frequency. So, the\ntemporal dimension of remote sensing data provides natural augmentation without\nrequiring us to create artificial augmentation of images. Here, we propose\nS3-TSS, a novel method of self-supervised learning technique that leverages\nnatural augmentation occurring in temporal dimension. We compare our results\nwith current state-of-the-art methods and also perform various experiments. We\nobserved that our method was able to perform better than baseline SeCo in four\ndownstream datasets. Code for our work can be found here:\nhttps://github.com/hewanshrestha/Why-Self-Supervision-in-Time",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04859v2",
    "published_date": "2024-03-07 19:16:17 UTC",
    "updated_date": "2024-03-11 09:32:20 UTC"
  },
  {
    "arxiv_id": "2403.04760v1",
    "title": "iScore: Visual Analytics for Interpreting How Language Models Automatically Score Summaries",
    "authors": [
      "Adam Coscia",
      "Langdon Holmes",
      "Wesley Morris",
      "Joon Suh Choi",
      "Scott Crossley",
      "Alex Endert"
    ],
    "abstract": "The recent explosion in popularity of large language models (LLMs) has\ninspired learning engineers to incorporate them into adaptive educational tools\nthat automatically score summary writing. Understanding and evaluating LLMs is\nvital before deploying them in critical learning environments, yet their\nunprecedented size and expanding number of parameters inhibits transparency and\nimpedes trust when they underperform. Through a collaborative user-centered\ndesign process with several learning engineers building and deploying summary\nscoring LLMs, we characterized fundamental design challenges and goals around\ninterpreting their models, including aggregating large text inputs, tracking\nscore provenance, and scaling LLM interpretability methods. To address their\nconcerns, we developed iScore, an interactive visual analytics tool for\nlearning engineers to upload, score, and compare multiple summaries\nsimultaneously. Tightly integrated views allow users to iteratively revise the\nlanguage in summaries, track changes in the resulting LLM scores, and visualize\nmodel weights at multiple levels of abstraction. To validate our approach, we\ndeployed iScore with three learning engineers over the course of a month. We\npresent a case study where interacting with iScore led a learning engineer to\nimprove their LLM's score accuracy by three percentage points. Finally, we\nconducted qualitative interviews with the learning engineers that revealed how\niScore enabled them to understand, evaluate, and build trust in their LLMs\nduring deployment.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted to IUI 2024. 16 pages, 5 figures, 1 table. For a demo video,\n  see https://youtu.be/EYJX-_fQPf0 . For a live demo, visit\n  https://adamcoscia.com/papers/iscore/demo/ . The source code is available at\n  https://github.com/AdamCoscia/iScore",
    "pdf_url": "http://arxiv.org/pdf/2403.04760v1",
    "published_date": "2024-03-07 18:56:39 UTC",
    "updated_date": "2024-03-07 18:56:39 UTC"
  },
  {
    "arxiv_id": "2403.04758v1",
    "title": "KnowledgeVIS: Interpreting Language Models by Comparing Fill-in-the-Blank Prompts",
    "authors": [
      "Adam Coscia",
      "Alex Endert"
    ],
    "abstract": "Recent growth in the popularity of large language models has led to their\nincreased usage for summarizing, predicting, and generating text, making it\nvital to help researchers and engineers understand how and why they work. We\npresent KnowledgeVis, a human-in-the-loop visual analytics system for\ninterpreting language models using fill-in-the-blank sentences as prompts. By\ncomparing predictions between sentences, KnowledgeVis reveals learned\nassociations that intuitively connect what language models learn during\ntraining to natural language tasks downstream, helping users create and test\nmultiple prompt variations, analyze predicted words using a novel semantic\nclustering technique, and discover insights using interactive visualizations.\nCollectively, these visualizations help users identify the likelihood and\nuniqueness of individual predictions, compare sets of predictions between\nprompts, and summarize patterns and relationships between predictions across\nall prompts. We demonstrate the capabilities of KnowledgeVis with feedback from\nsix NLP experts as well as three different use cases: (1) probing biomedical\nknowledge in two domain-adapted models; and (2) evaluating harmful identity\nstereotypes and (3) discovering facts and relationships between three\ngeneral-purpose models.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted to IEEE TVCG. 20 pages, 10 figures, 1 table. For a demo\n  video, see https://youtu.be/hBX4rSUMr_I . For a live demo, visit\n  https://adamcoscia.com/papers/knowledgevis/demo/ . The source code is\n  available at https://github.com/AdamCoscia/KnowledgeVIS",
    "pdf_url": "http://arxiv.org/pdf/2403.04758v1",
    "published_date": "2024-03-07 18:56:31 UTC",
    "updated_date": "2024-03-07 18:56:31 UTC"
  },
  {
    "arxiv_id": "2403.04747v1",
    "title": "GNN-VPA: A Variance-Preserving Aggregation Strategy for Graph Neural Networks",
    "authors": [
      "Lisa Schneckenreiter",
      "Richard Freinschlag",
      "Florian Sestak",
      "Johannes Brandstetter",
      "Günter Klambauer",
      "Andreas Mayr"
    ],
    "abstract": "Graph neural networks (GNNs), and especially message-passing neural networks,\nexcel in various domains such as physics, drug discovery, and molecular\nmodeling. The expressivity of GNNs with respect to their ability to\ndiscriminate non-isomorphic graphs critically depends on the functions employed\nfor message aggregation and graph-level readout. By applying signal propagation\ntheory, we propose a variance-preserving aggregation function (VPA) that\nmaintains expressivity, but yields improved forward and backward dynamics.\nExperiments demonstrate that VPA leads to increased predictive performance for\npopular GNN architectures as well as improved learning dynamics. Our results\ncould pave the way towards normalizer-free or self-normalizing GNNs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICLR 2024 (Tiny Papers Track)",
    "pdf_url": "http://arxiv.org/pdf/2403.04747v1",
    "published_date": "2024-03-07 18:52:27 UTC",
    "updated_date": "2024-03-07 18:52:27 UTC"
  },
  {
    "arxiv_id": "2403.04746v1",
    "title": "LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error",
    "authors": [
      "Boshi Wang",
      "Hao Fang",
      "Jason Eisner",
      "Benjamin Van Durme",
      "Yu Su"
    ],
    "abstract": "Tools are essential for large language models (LLMs) to acquire up-to-date\ninformation and take consequential actions in external environments. Existing\nwork on tool-augmented LLMs primarily focuses on the broad coverage of tools\nand the flexibility of adding new tools. However, a critical aspect that has\nsurprisingly been understudied is simply how accurately an LLM uses tools for\nwhich it has been trained. We find that existing LLMs, including GPT-4 and\nopen-source LLMs specifically fine-tuned for tool use, only reach a correctness\nrate in the range of 30% to 60%, far from reliable use in practice. We propose\na biologically inspired method for tool-augmented LLMs, simulated trial and\nerror (STE), that orchestrates three key mechanisms for successful tool use\nbehaviors in the biological system: trial and error, imagination, and memory.\nSpecifically, STE leverages an LLM's 'imagination' to simulate plausible\nscenarios for using a tool, after which the LLM interacts with the tool to\nlearn from its execution feedback. Both short-term and long-term memory are\nemployed to improve the depth and breadth of the exploration, respectively.\nComprehensive experiments on ToolBench show that STE substantially improves\ntool learning for LLMs under both in-context learning and fine-tuning settings,\nbringing a boost of 46.7% to Mistral-Instruct-7B and enabling it to outperform\nGPT-4. We also show effective continual learning of tools via a simple\nexperience replay strategy.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Code and data available at\n  https://github.com/microsoft/simulated-trial-and-error",
    "pdf_url": "http://arxiv.org/pdf/2403.04746v1",
    "published_date": "2024-03-07 18:50:51 UTC",
    "updated_date": "2024-03-07 18:50:51 UTC"
  },
  {
    "arxiv_id": "2403.04732v3",
    "title": "How Far Are We from Intelligent Visual Deductive Reasoning?",
    "authors": [
      "Yizhe Zhang",
      "He Bai",
      "Ruixiang Zhang",
      "Jiatao Gu",
      "Shuangfei Zhai",
      "Josh Susskind",
      "Navdeep Jaitly"
    ],
    "abstract": "Vision-Language Models (VLMs) have recently demonstrated incredible strides\non diverse vision language tasks. We dig into vision-based deductive reasoning,\na more sophisticated but less explored realm, and find previously unexposed\nblindspots in the current SOTA VLMs. Specifically, we leverage Raven's\nProgressive Matrices (RPMs), to assess VLMs' abilities to perform multi-hop\nrelational and deductive reasoning relying solely on visual clues. We perform\ncomprehensive evaluations of several popular VLMs employing standard strategies\nsuch as in-context learning, self-consistency, and Chain-of-thoughts (CoT) on\nthree diverse datasets, including the Mensa IQ test, IntelligenceTest, and\nRAVEN. The results reveal that despite the impressive capabilities of LLMs in\ntext-based reasoning, we are still far from achieving comparable proficiency in\nvisual deductive reasoning. We found that certain standard strategies that are\neffective when applied to LLMs do not seamlessly translate to the challenges\npresented by visual reasoning tasks. A detailed analysis reveals that VLMs\nstruggle to solve these tasks mainly because they are unable to perceive and\ncomprehend multiple, confounding abstract patterns in RPM examples.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "COLM 2024. https://github.com/apple/ml-rpm-bench",
    "pdf_url": "http://arxiv.org/pdf/2403.04732v3",
    "published_date": "2024-03-07 18:35:54 UTC",
    "updated_date": "2024-10-01 04:41:53 UTC"
  },
  {
    "arxiv_id": "2403.04706v1",
    "title": "Common 7B Language Models Already Possess Strong Math Capabilities",
    "authors": [
      "Chen Li",
      "Weiqi Wang",
      "Jingcheng Hu",
      "Yixuan Wei",
      "Nanning Zheng",
      "Han Hu",
      "Zheng Zhang",
      "Houwen Peng"
    ],
    "abstract": "Mathematical capabilities were previously believed to emerge in common\nlanguage models only at a very large scale or require extensive math-related\npre-training. This paper shows that the LLaMA-2 7B model with common\npre-training already exhibits strong mathematical abilities, as evidenced by\nits impressive accuracy of 97.7% and 72.0% on the GSM8K and MATH benchmarks,\nrespectively, when selecting the best response from 256 random generations. The\nprimary issue with the current base model is the difficulty in consistently\neliciting its inherent mathematical capabilities. Notably, the accuracy for the\nfirst answer drops to 49.5% and 7.9% on the GSM8K and MATH benchmarks,\nrespectively. We find that simply scaling up the SFT data can significantly\nenhance the reliability of generating correct answers. However, the potential\nfor extensive scaling is constrained by the scarcity of publicly available math\nquestions. To overcome this limitation, we employ synthetic data, which proves\nto be nearly as effective as real data and shows no clear saturation when\nscaled up to approximately one million samples. This straightforward approach\nachieves an accuracy of 82.6% on GSM8K and 40.6% on MATH using LLaMA-2 7B\nmodels, surpassing previous models by 14.2% and 20.8%, respectively. We also\nprovide insights into scaling behaviors across different reasoning complexities\nand error types.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04706v1",
    "published_date": "2024-03-07 18:00:40 UTC",
    "updated_date": "2024-03-07 18:00:40 UTC"
  },
  {
    "arxiv_id": "2403.04701v4",
    "title": "ObjectCompose: Evaluating Resilience of Vision-Based Models on Object-to-Background Compositional Changes",
    "authors": [
      "Hashmat Shadab Malik",
      "Muhammad Huzaifa",
      "Muzammal Naseer",
      "Salman Khan",
      "Fahad Shahbaz Khan"
    ],
    "abstract": "Given the large-scale multi-modal training of recent vision-based models and\ntheir generalization capabilities, understanding the extent of their robustness\nis critical for their real-world deployment. In this work, we evaluate the\nresilience of current vision-based models against diverse object-to-background\ncontext variations. The majority of robustness evaluation methods have\nintroduced synthetic datasets to induce changes to object characteristics\n(viewpoints, scale, color) or utilized image transformation techniques\n(adversarial changes, common corruptions) on real images to simulate shifts in\ndistributions. Recent works have explored leveraging large language models and\ndiffusion models to generate changes in the background. However, these methods\neither lack in offering control over the changes to be made or distort the\nobject semantics, making them unsuitable for the task. Our method, on the other\nhand, can induce diverse object-to-background changes while preserving the\noriginal semantics and appearance of the object. To achieve this goal, we\nharness the generative capabilities of text-to-image, image-to-text, and\nimage-to-segment models to automatically generate a broad spectrum of\nobject-to-background changes. We induce both natural and adversarial background\nchanges by either modifying the textual prompts or optimizing the latents and\ntextual embedding of text-to-image models. We produce various versions of\nstandard vision datasets (ImageNet, COCO), incorporating either diverse and\nrealistic backgrounds into the images or introducing color, texture, and\nadversarial changes in the background. We conduct extensive experiments to\nanalyze the robustness of vision-based models against object-to-background\ncontext variations across diverse tasks. Code\nhttps://github.com/Muhammad-Huzaifaa/ObjectCompose.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04701v4",
    "published_date": "2024-03-07 17:48:48 UTC",
    "updated_date": "2024-10-08 20:10:02 UTC"
  },
  {
    "arxiv_id": "2403.04697v2",
    "title": "AUFormer: Vision Transformers are Parameter-Efficient Facial Action Unit Detectors",
    "authors": [
      "Kaishen Yuan",
      "Zitong Yu",
      "Xin Liu",
      "Weicheng Xie",
      "Huanjing Yue",
      "Jingyu Yang"
    ],
    "abstract": "Facial Action Units (AU) is a vital concept in the realm of affective\ncomputing, and AU detection has always been a hot research topic. Existing\nmethods suffer from overfitting issues due to the utilization of a large number\nof learnable parameters on scarce AU-annotated datasets or heavy reliance on\nsubstantial additional relevant data. Parameter-Efficient Transfer Learning\n(PETL) provides a promising paradigm to address these challenges, whereas its\nexisting methods lack design for AU characteristics. Therefore, we innovatively\ninvestigate PETL paradigm to AU detection, introducing AUFormer and proposing a\nnovel Mixture-of-Knowledge Expert (MoKE) collaboration mechanism. An individual\nMoKE specific to a certain AU with minimal learnable parameters first\nintegrates personalized multi-scale and correlation knowledge. Then the MoKE\ncollaborates with other MoKEs in the expert group to obtain aggregated\ninformation and inject it into the frozen Vision Transformer (ViT) to achieve\nparameter-efficient AU detection. Additionally, we design a Margin-truncated\nDifficulty-aware Weighted Asymmetric Loss (MDWA-Loss), which can encourage the\nmodel to focus more on activated AUs, differentiate the difficulty of\nunactivated AUs, and discard potential mislabeled samples. Extensive\nexperiments from various perspectives, including within-domain, cross-domain,\ndata efficiency, and micro-expression domain, demonstrate AUFormer's\nstate-of-the-art performance and robust generalization abilities without\nrelying on additional relevant data. The code for AUFormer is available at\nhttps://github.com/yuankaishen2001/AUFormer.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.04697v2",
    "published_date": "2024-03-07 17:46:50 UTC",
    "updated_date": "2024-07-09 15:15:21 UTC"
  },
  {
    "arxiv_id": "2403.04696v2",
    "title": "Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification",
    "authors": [
      "Ekaterina Fadeeva",
      "Aleksandr Rubashevskii",
      "Artem Shelmanov",
      "Sergey Petrakov",
      "Haonan Li",
      "Hamdy Mubarak",
      "Evgenii Tsymbalov",
      "Gleb Kuzmin",
      "Alexander Panchenko",
      "Timothy Baldwin",
      "Preslav Nakov",
      "Maxim Panov"
    ],
    "abstract": "Large language models (LLMs) are notorious for hallucinating, i.e., producing\nerroneous claims in their output. Such hallucinations can be dangerous, as\noccasional factual inaccuracies in the generated text might be obscured by the\nrest of the output being generally factually correct, making it extremely hard\nfor the users to spot them. Current services that leverage LLMs usually do not\nprovide any means for detecting unreliable generations. Here, we aim to bridge\nthis gap. In particular, we propose a novel fact-checking and hallucination\ndetection pipeline based on token-level uncertainty quantification. Uncertainty\nscores leverage information encapsulated in the output of a neural network or\nits layers to detect unreliable predictions, and we show that they can be used\nto fact-check the atomic claims in the LLM output. Moreover, we present a novel\ntoken-level uncertainty quantification method that removes the impact of\nuncertainty about what claim to generate on the current step and what surface\nform to use. Our method Claim Conditioned Probability (CCP) measures only the\nuncertainty of a particular claim value expressed by the model. Experiments on\nthe task of biography generation demonstrate strong improvements for CCP\ncompared to the baselines for seven LLMs and four languages. Human evaluation\nreveals that the fact-checking pipeline based on uncertainty quantification is\ncompetitive with a fact-checking tool that leverages external knowledge.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL-2024 (Findings). Ekaterina Fadeeva, Aleksandr\n  Rubashevskii, and Artem Shelmanov contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2403.04696v2",
    "published_date": "2024-03-07 17:44:17 UTC",
    "updated_date": "2024-06-06 21:32:39 UTC"
  },
  {
    "arxiv_id": "2403.04690v3",
    "title": "Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level",
    "authors": [
      "Ali Hassani",
      "Wen-Mei Hwu",
      "Humphrey Shi"
    ],
    "abstract": "Neighborhood attention reduces the cost of self attention by restricting each\ntoken's attention span to its nearest neighbors. This restriction,\nparameterized by a window size and dilation factor, draws a spectrum of\npossible attention patterns between linear projection and self attention.\nNeighborhood attention, and more generally sliding window attention patterns,\nhave long been bounded by infrastructure, particularly in higher-rank spaces\n(2-D and 3-D), calling for the development of custom kernels, which have been\nlimited in either functionality, or performance, if not both. In this work, we\naim to massively improve upon existing infrastructure by providing two new\nmethods for implementing neighborhood attention. We first show that\nneighborhood attention can be represented as a batched GEMM problem, similar to\nstandard attention, and implement it for 1-D and 2-D neighborhood attention.\nThese kernels on average provide 895% and 272% improvement in full precision\nruntime compared to existing naive CUDA kernels for 1-D and 2-D neighborhood\nattention respectively. We find that aside from being heavily bound by memory\nbandwidth, certain inherent inefficiencies exist in all unfused implementations\nof neighborhood attention, which in most cases undo their theoretical\nefficiency gain. Motivated by the progress made into fused dot-product\nattention kernels, we developed fused neighborhood attention; an adaptation of\nfused dot-product attention kernels that allow fine-grained control over\nattention across different spatial axes. Known for reducing the quadratic time\ncomplexity of self attention to a linear complexity, neighborhood attention can\nnow enjoy a reduced and constant memory footprint, and record-breaking half\nprecision runtime. We observe that our fused implementation successfully\ncircumvents some of the unavoidable inefficiencies in unfused\nimplementations...",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "To appear in 38th Conference on Neural Information Processing Systems\n  (NeurIPS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2403.04690v3",
    "published_date": "2024-03-07 17:35:58 UTC",
    "updated_date": "2024-10-31 17:32:26 UTC"
  },
  {
    "arxiv_id": "2403.04667v1",
    "title": "The Social Impact of Generative AI: An Analysis on ChatGPT",
    "authors": [
      "Maria T. Baldassarre",
      "Danilo Caivano",
      "Berenice Fernandez Nieto",
      "Domenico Gigante",
      "Azzurra Ragone"
    ],
    "abstract": "In recent months, the social impact of Artificial Intelligence (AI) has\ngained considerable public interest, driven by the emergence of Generative AI\nmodels, ChatGPT in particular. The rapid development of these models has\nsparked heated discussions regarding their benefits, limitations, and\nassociated risks. Generative models hold immense promise across multiple\ndomains, such as healthcare, finance, and education, to cite a few, presenting\ndiverse practical applications. Nevertheless, concerns about potential adverse\neffects have elicited divergent perspectives, ranging from privacy risks to\nescalating social inequality. This paper adopts a methodology to delve into the\nsocietal implications of Generative AI tools, focusing primarily on the case of\nChatGPT. It evaluates the potential impact on several social sectors and\nillustrates the findings of a comprehensive literature review of both positive\nand negative effects, emerging trends, and areas of opportunity of Generative\nAI models. This analysis aims to facilitate an in-depth discussion by providing\ninsights that can inspire policy, regulation, and responsible development\npractices to foster a human-centered AI.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.ET"
    ],
    "primary_category": "cs.AI",
    "comment": "Presented at GoodIT2023 - ACM Conference on Information Technology\n  for Social Good",
    "pdf_url": "http://arxiv.org/pdf/2403.04667v1",
    "published_date": "2024-03-07 17:14:22 UTC",
    "updated_date": "2024-03-07 17:14:22 UTC"
  },
  {
    "arxiv_id": "2403.04652v3",
    "title": "Yi: Open Foundation Models by 01.AI",
    "authors": [
      "01. AI",
      ":",
      "Alex Young",
      "Bei Chen",
      "Chao Li",
      "Chengen Huang",
      "Ge Zhang",
      "Guanwei Zhang",
      "Guoyin Wang",
      "Heng Li",
      "Jiangcheng Zhu",
      "Jianqun Chen",
      "Jing Chang",
      "Kaidong Yu",
      "Peng Liu",
      "Qiang Liu",
      "Shawn Yue",
      "Senbin Yang",
      "Shiming Yang",
      "Wen Xie",
      "Wenhao Huang",
      "Xiaohui Hu",
      "Xiaoyi Ren",
      "Xinyao Niu",
      "Pengcheng Nie",
      "Yanpeng Li",
      "Yuchi Xu",
      "Yudong Liu",
      "Yue Wang",
      "Yuxuan Cai",
      "Zhenyu Gu",
      "Zhiyuan Liu",
      "Zonghong Dai"
    ],
    "abstract": "We introduce the Yi model family, a series of language and multimodal models\nthat demonstrate strong multi-dimensional capabilities. The Yi model family is\nbased on 6B and 34B pretrained language models, then we extend them to chat\nmodels, 200K long context models, depth-upscaled models, and vision-language\nmodels. Our base models achieve strong performance on a wide range of\nbenchmarks like MMLU, and our finetuned chat models deliver strong human\npreference rate on major evaluation platforms like AlpacaEval and Chatbot\nArena. Building upon our scalable super-computing infrastructure and the\nclassical transformer architecture, we attribute the performance of Yi models\nprimarily to its data quality resulting from our data-engineering efforts. For\npretraining, we construct 3.1 trillion tokens of English and Chinese corpora\nusing a cascaded data deduplication and quality filtering pipeline. For\nfinetuning, we polish a small scale (less than 10K) instruction dataset over\nmultiple iterations such that every single instance has been verified directly\nby our machine learning engineers. For vision-language, we combine the chat\nlanguage model with a vision transformer encoder and train the model to align\nvisual representations to the semantic space of the language model. We further\nextend the context length to 200K through lightweight continual pretraining and\ndemonstrate strong needle-in-a-haystack retrieval performance. We show that\nextending the depth of the pretrained checkpoint through continual pretraining\nfurther improves performance. We believe that given our current results,\ncontinuing to scale up model parameters using thoroughly optimized data will\nlead to even stronger frontier models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04652v3",
    "published_date": "2024-03-07 16:52:49 UTC",
    "updated_date": "2025-01-21 10:12:05 UTC"
  },
  {
    "arxiv_id": "2403.04650v3",
    "title": "Lightweight Cross-Modal Representation Learning",
    "authors": [
      "Bilal Faye",
      "Hanane Azzag",
      "Mustapha Lebbah",
      "Djamel Bouchaffra"
    ],
    "abstract": "Low-cost cross-modal representation learning is crucial for deriving semantic\nrepresentations across diverse modalities such as text, audio, images, and\nvideo. Traditional approaches typically depend on large specialized models\ntrained from scratch, requiring extensive datasets and resulting in high\nresource and time costs. To overcome these challenges, we introduce a novel\napproach named Lightweight Cross-Modal Representation Learning (LightCRL). This\nmethod uses a single neural network titled Deep Fusion Encoder (DFE), which\nprojects data from multiple modalities into a shared latent representation\nspace. This reduces the overall parameter count while still delivering robust\nperformance comparable to more complex systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04650v3",
    "published_date": "2024-03-07 16:50:25 UTC",
    "updated_date": "2024-09-07 07:24:36 UTC"
  },
  {
    "arxiv_id": "2403.04634v2",
    "title": "Pix2Gif: Motion-Guided Diffusion for GIF Generation",
    "authors": [
      "Hitesh Kandala",
      "Jianfeng Gao",
      "Jianwei Yang"
    ],
    "abstract": "We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video)\ngeneration. We tackle this problem differently by formulating the task as an\nimage translation problem steered by text and motion magnitude prompts, as\nshown in teaser fig. To ensure that the model adheres to motion guidance, we\npropose a new motion-guided warping module to spatially transform the features\nof the source image conditioned on the two types of prompts. Furthermore, we\nintroduce a perceptual loss to ensure the transformed feature map remains\nwithin the same space as the target image, ensuring content consistency and\ncoherence. In preparation for the model training, we meticulously curated data\nby extracting coherent image frames from the TGIF video-caption dataset, which\nprovides rich information about the temporal changes of subjects. After\npretraining, we apply our model in a zero-shot manner to a number of video\ndatasets. Extensive qualitative and quantitative experiments demonstrate the\neffectiveness of our model -- it not only captures the semantic prompt from\ntext but also the spatial ones from motion guidance. We train all our models\nusing a single node of 16xV100 GPUs. Code, dataset and models are made public\nat: https://hiteshk03.github.io/Pix2Gif/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04634v2",
    "published_date": "2024-03-07 16:18:28 UTC",
    "updated_date": "2024-03-08 18:28:28 UTC"
  },
  {
    "arxiv_id": "2403.04629v2",
    "title": "Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI Collaboration",
    "authors": [
      "Julian Rodemann",
      "Federico Croppi",
      "Philipp Arens",
      "Yusuf Sale",
      "Julia Herbinger",
      "Bernd Bischl",
      "Eyke Hüllermeier",
      "Thomas Augustin",
      "Conor J. Walsh",
      "Giuseppe Casalicchio"
    ],
    "abstract": "Bayesian optimization (BO) with Gaussian processes (GP) has become an\nindispensable algorithm for black box optimization problems. Not without a dash\nof irony, BO is often considered a black box itself, lacking ways to provide\nreasons as to why certain parameters are proposed to be evaluated. This is\nparticularly relevant in human-in-the-loop applications of BO, such as in\nrobotics. We address this issue by proposing ShapleyBO, a framework for\ninterpreting BO's proposals by game-theoretic Shapley values.They quantify each\nparameter's contribution to BO's acquisition function. Exploiting the linearity\nof Shapley values, we are further able to identify how strongly each parameter\ndrives BO's exploration and exploitation for additive acquisition functions\nlike the confidence bound. We also show that ShapleyBO can disentangle the\ncontributions to exploration into those that explore aleatoric and epistemic\nuncertainty. Moreover, our method gives rise to a ShapleyBO-assisted human\nmachine interface (HMI), allowing users to interfere with BO in case proposals\ndo not align with human reasoning. We demonstrate this HMI's benefits for the\nuse case of personalizing wearable robotic devices (assistive back exosuits) by\nhuman-in-the-loop BO. Results suggest human-BO teams with access to ShapleyBO\ncan achieve lower regret than teams without.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC",
      "cs.RO",
      "stat.ML",
      "I.2.6; I.2.9; F.2.2; J.6"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint. Copyright by the authors. 19 pages, 24 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.04629v2",
    "published_date": "2024-03-07 16:13:32 UTC",
    "updated_date": "2024-03-08 07:52:32 UTC"
  },
  {
    "arxiv_id": "2403.04612v1",
    "title": "A Domain Translation Framework with an Adversarial Denoising Diffusion Model to Generate Synthetic Datasets of Echocardiography Images",
    "authors": [
      "Cristiana Tiago",
      "Sten Roar Snare",
      "Jurica Sprem",
      "Kristin McLeod"
    ],
    "abstract": "Currently, medical image domain translation operations show a high demand\nfrom researchers and clinicians. Amongst other capabilities, this task allows\nthe generation of new medical images with sufficiently high image quality,\nmaking them clinically relevant. Deep Learning (DL) architectures, most\nspecifically deep generative models, are widely used to generate and translate\nimages from one domain to another. The proposed framework relies on an\nadversarial Denoising Diffusion Model (DDM) to synthesize echocardiography\nimages and perform domain translation. Contrary to Generative Adversarial\nNetworks (GANs), DDMs are able to generate high quality image samples with a\nlarge diversity. If a DDM is combined with a GAN, this ability to generate new\ndata is completed at an even faster sampling time. In this work we trained an\nadversarial DDM combined with a GAN to learn the reverse denoising process,\nrelying on a guide image, making sure relevant anatomical structures of each\nechocardiography image were kept and represented on the generated image\nsamples. For several domain translation operations, the results verified that\nsuch generative model was able to synthesize high quality image samples: MSE:\n11.50 +/- 3.69, PSNR (dB): 30.48 +/- 0.09, SSIM: 0.47 +/- 0.03. The proposed\nmethod showed high generalization ability, introducing a framework to create\nechocardiography images suitable to be used for clinical research purposes.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04612v1",
    "published_date": "2024-03-07 15:58:03 UTC",
    "updated_date": "2024-03-07 15:58:03 UTC"
  },
  {
    "arxiv_id": "2403.04588v1",
    "title": "Zero-shot cross-modal transfer of Reinforcement Learning policies through a Global Workspace",
    "authors": [
      "Léopold Maytié",
      "Benjamin Devillers",
      "Alexandre Arnold",
      "Rufin VanRullen"
    ],
    "abstract": "Humans perceive the world through multiple senses, enabling them to create a\ncomprehensive representation of their surroundings and to generalize\ninformation across domains. For instance, when a textual description of a scene\nis given, humans can mentally visualize it. In fields like robotics and\nReinforcement Learning (RL), agents can also access information about the\nenvironment through multiple sensors; yet redundancy and complementarity\nbetween sensors is difficult to exploit as a source of robustness (e.g. against\nsensor failure) or generalization (e.g. transfer across domains). Prior\nresearch demonstrated that a robust and flexible multimodal representation can\nbe efficiently constructed based on the cognitive science notion of a 'Global\nWorkspace': a unique representation trained to combine information across\nmodalities, and to broadcast its signal back to each modality. Here, we explore\nwhether such a brain-inspired multimodal representation could be advantageous\nfor RL agents. First, we train a 'Global Workspace' to exploit information\ncollected about the environment via two input modalities (a visual input, or an\nattribute vector representing the state of the agent and/or its environment).\nThen, we train a RL agent policy using this frozen Global Workspace. In two\ndistinct environments and tasks, our results reveal the model's ability to\nperform zero-shot cross-modal transfer between input modalities, i.e. to apply\nto image inputs a policy previously trained on attribute vectors (and\nvice-versa), without additional training or fine-tuning. Variants and ablations\nof the full Global Workspace (including a CLIP-like multimodal representation\ntrained via contrastive learning) did not display the same generalization\nabilities.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Under review in a conference",
    "pdf_url": "http://arxiv.org/pdf/2403.04588v1",
    "published_date": "2024-03-07 15:35:29 UTC",
    "updated_date": "2024-03-07 15:35:29 UTC"
  },
  {
    "arxiv_id": "2403.04577v2",
    "title": "Wiki-TabNER: Integrating Named Entity Recognition into Wikipedia Tables",
    "authors": [
      "Aneta Koleva",
      "Martin Ringsquandl",
      "Ahmed Hatem",
      "Thomas Runkler",
      "Volker Tresp"
    ],
    "abstract": "Interest in solving table interpretation tasks has grown over the years, yet\nit still relies on existing datasets that may be overly simplified. This is\npotentially reducing the effectiveness of the dataset for thorough evaluation\nand failing to accurately represent tables as they appear in the real-world. To\nenrich the existing benchmark datasets, we extract and annotate a new, more\nchallenging dataset. The proposed Wiki-TabNER dataset features complex tables\ncontaining several entities per cell, with named entities labeled using DBpedia\nclasses. This dataset is specifically designed to address named entity\nrecognition (NER) task within tables, but it can also be used as a more\nchallenging dataset for evaluating the entity linking task. In this paper we\ndescribe the distinguishing features of the Wiki-TabNER dataset and the\nlabeling process. In addition, we propose a prompting framework for evaluating\nthe new large language models on the within tables NER task. Finally, we\nperform qualitative analysis to gain insights into the challenges encountered\nby the models and to understand the limitations of the proposed~dataset.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at SIGIR 2025 conference",
    "pdf_url": "http://arxiv.org/pdf/2403.04577v2",
    "published_date": "2024-03-07 15:22:07 UTC",
    "updated_date": "2025-05-02 17:52:05 UTC"
  },
  {
    "arxiv_id": "2403.04571v1",
    "title": "Machine learning and information theory concepts towards an AI Mathematician",
    "authors": [
      "Yoshua Bengio",
      "Nikolay Malkin"
    ],
    "abstract": "The current state-of-the-art in artificial intelligence is impressive,\nespecially in terms of mastery of language, but not so much in terms of\nmathematical reasoning. What could be missing? Can we learn something useful\nabout that gap from how the brains of mathematicians go about their craft? This\nessay builds on the idea that current deep learning mostly succeeds at system 1\nabilities -- which correspond to our intuition and habitual behaviors -- but\nstill lacks something important regarding system 2 abilities -- which include\nreasoning and robust uncertainty estimation. It takes an\ninformation-theoretical posture to ask questions about what constitutes an\ninteresting mathematical statement, which could guide future work in crafting\nan AI mathematician. The focus is not on proving a given theorem but on\ndiscovering new and interesting conjectures. The central hypothesis is that a\ndesirable body of theorems better summarizes the set of all provable\nstatements, for example by having a small description length while at the same\ntime being close (in terms of number of derivation steps) to many provable\nstatements.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "To appear in the Bulletin of the AMS, 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.04571v1",
    "published_date": "2024-03-07 15:12:06 UTC",
    "updated_date": "2024-03-07 15:12:06 UTC"
  },
  {
    "arxiv_id": "2403.04558v2",
    "title": "Reducing self-supervised learning complexity improves weakly-supervised classification performance in computational pathology",
    "authors": [
      "Tim Lenz",
      "Omar S. M. El Nahhas",
      "Marta Ligero",
      "Jakob Nikolas Kather"
    ],
    "abstract": "Deep Learning models have been successfully utilized to extract clinically\nactionable insights from routinely available histology data. Generally, these\nmodels require annotations performed by clinicians, which are scarce and costly\nto generate. The emergence of self-supervised learning (SSL) methods remove\nthis barrier, allowing for large-scale analyses on non-annotated data. However,\nrecent SSL approaches apply increasingly expansive model architectures and\nlarger datasets, causing the rapid escalation of data volumes, hardware\nprerequisites, and overall expenses, limiting access to these resources to few\ninstitutions. Therefore, we investigated the complexity of contrastive SSL in\ncomputational pathology in relation to classification performance with the\nutilization of consumer-grade hardware. Specifically, we analyzed the effects\nof adaptations in data volume, architecture, and algorithms on downstream\nclassification tasks, emphasizing their impact on computational resources. We\ntrained breast cancer foundation models on a large public patient cohort and\nvalidated them on various downstream classification tasks in a weakly\nsupervised manner on two external public patient cohorts. Our experiments\ndemonstrate that we can improve downstream classification performance whilst\nreducing SSL training duration by 90%. In summary, we propose a set of\nadaptations which enable the utilization of SSL in computational pathology in\nnon-resource abundant environments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to MICCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.04558v2",
    "published_date": "2024-03-07 14:56:06 UTC",
    "updated_date": "2024-03-12 11:42:06 UTC"
  },
  {
    "arxiv_id": "2403.04547v1",
    "title": "CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?",
    "authors": [
      "Ibrahim Alabdulmohsin",
      "Xiao Wang",
      "Andreas Steiner",
      "Priya Goyal",
      "Alexander D'Amour",
      "Xiaohua Zhai"
    ],
    "abstract": "We study the effectiveness of data-balancing for mitigating biases in\ncontrastive language-image pretraining (CLIP), identifying areas of strength\nand limitation. First, we reaffirm prior conclusions that CLIP models can\ninadvertently absorb societal stereotypes. To counter this, we present a novel\nalgorithm, called Multi-Modal Moment Matching (M4), designed to reduce both\nrepresentation and association biases (i.e. in first- and second-order\nstatistics) in multimodal data. We use M4 to conduct an in-depth analysis\ntaking into account various factors, such as the model, representation, and\ndata size. Our study also explores the dynamic nature of how CLIP learns and\nunlearns biases. In particular, we find that fine-tuning is effective in\ncountering representation biases, though its impact diminishes for association\nbiases. Also, data balancing has a mixed impact on quality: it tends to improve\nclassification but can hurt retrieval. Interestingly, data and architectural\nimprovements seem to mitigate the negative impact of data balancing on\nperformance; e.g. applying M4 to SigLIP-B/16 with data quality filters improves\nCOCO image-to-text retrieval @5 from 86% (without data balancing) to 87% and\nImageNet 0-shot classification from 77% to 77.5%! Finally, we conclude with\nrecommendations for improving the efficacy of data balancing in multimodal\nsystems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "32 pages, 20 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2403.04547v1",
    "published_date": "2024-03-07 14:43:17 UTC",
    "updated_date": "2024-03-07 14:43:17 UTC"
  },
  {
    "arxiv_id": "2403.04541v1",
    "title": "Towards Automatic Composition of ASP Programs from Natural Language Specifications",
    "authors": [
      "Manuel Borroto",
      "Irfan Kareem",
      "Francesco Ricca"
    ],
    "abstract": "This paper moves the first step towards automating the composition of Answer\nSet Programming (ASP) specifications. In particular, the following\ncontributions are provided: (i) A dataset focused on graph-related problem\nspecifications, designed to develop and assess tools for ASP automatic coding;\n(ii) A two-step architecture, implemented in the NL2ASP tool, for generating\nASP programs from natural language specifications. NL2ASP uses neural machine\ntranslation to transform natural language into Controlled Natural Language\n(CNL) statements. Subsequently, CNL statements are converted into ASP code\nusing the CNL2ASP tool. An experiment confirms the viability of the approach.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04541v1",
    "published_date": "2024-03-07 14:36:52 UTC",
    "updated_date": "2024-03-07 14:36:52 UTC"
  },
  {
    "arxiv_id": "2403.04529v1",
    "title": "Enhancing Data Quality in Federated Fine-Tuning of Foundation Models",
    "authors": [
      "Wanru Zhao",
      "Yaxin Du",
      "Nicholas Donald Lane",
      "Siheng Chen",
      "Yanfeng Wang"
    ],
    "abstract": "In the current landscape of foundation model training, there is a significant\nreliance on public domain data, which is nearing exhaustion according to recent\nresearch. To further scale up, it is crucial to incorporate collaboration among\nmultiple specialized and high-quality private domain data sources. However, the\nchallenge of training models locally without sharing private data presents\nnumerous obstacles in data quality control. To tackle this issue, we propose a\ndata quality control pipeline for federated fine-tuning of foundation models.\nThis pipeline computes scores reflecting the quality of training data and\ndetermines a global threshold for a unified standard, aiming for improved\nglobal performance. Our experiments show that the proposed quality control\npipeline facilitates the effectiveness and reliability of the model training,\nleading to better performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICLR 2024 Workshop on Navigating and Addressing Data\n  Problems for Foundation Models (DPFM)",
    "pdf_url": "http://arxiv.org/pdf/2403.04529v1",
    "published_date": "2024-03-07 14:28:04 UTC",
    "updated_date": "2024-03-07 14:28:04 UTC"
  },
  {
    "arxiv_id": "2403.04526v1",
    "title": "Hyperspectral unmixing for Raman spectroscopy via physics-constrained autoencoders",
    "authors": [
      "Dimitar Georgiev",
      "Álvaro Fernández-Galiana",
      "Simon Vilms Pedersen",
      "Georgios Papadopoulos",
      "Ruoxiao Xie",
      "Molly M. Stevens",
      "Mauricio Barahona"
    ],
    "abstract": "Raman spectroscopy is widely used across scientific domains to characterize\nthe chemical composition of samples in a non-destructive, label-free manner.\nMany applications entail the unmixing of signals from mixtures of molecular\nspecies to identify the individual components present and their proportions,\nyet conventional methods for chemometrics often struggle with complex mixture\nscenarios encountered in practice. Here, we develop hyperspectral unmixing\nalgorithms based on autoencoder neural networks, and we systematically validate\nthem using both synthetic and experimental benchmark datasets created in-house.\nOur results demonstrate that unmixing autoencoders provide improved accuracy,\nrobustness and efficiency compared to standard unmixing methods. We also\nshowcase the applicability of autoencoders to complex biological settings by\nshowing improved biochemical characterization of volumetric Raman imaging data\nfrom a monocytic cell.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04526v1",
    "published_date": "2024-03-07 14:27:08 UTC",
    "updated_date": "2024-03-07 14:27:08 UTC"
  },
  {
    "arxiv_id": "2403.04523v1",
    "title": "T-TAME: Trainable Attention Mechanism for Explaining Convolutional Networks and Vision Transformers",
    "authors": [
      "Mariano V. Ntrougkas",
      "Nikolaos Gkalelis",
      "Vasileios Mezaris"
    ],
    "abstract": "The development and adoption of Vision Transformers and other deep-learning\narchitectures for image classification tasks has been rapid. However, the\n\"black box\" nature of neural networks is a barrier to adoption in applications\nwhere explainability is essential. While some techniques for generating\nexplanations have been proposed, primarily for Convolutional Neural Networks,\nadapting such techniques to the new paradigm of Vision Transformers is\nnon-trivial. This paper presents T-TAME, Transformer-compatible Trainable\nAttention Mechanism for Explanations, a general methodology for explaining deep\nneural networks used in image classification tasks. The proposed architecture\nand training technique can be easily applied to any convolutional or Vision\nTransformer-like neural network, using a streamlined training approach. After\ntraining, explanation maps can be computed in a single forward pass; these\nexplanation maps are comparable to or outperform the outputs of computationally\nexpensive perturbation-based explainability techniques, achieving SOTA\nperformance. We apply T-TAME to three popular deep learning classifier\narchitectures, VGG-16, ResNet-50, and ViT-B-16, trained on the ImageNet\ndataset, and we demonstrate improvements over existing state-of-the-art\nexplainability methods. A detailed analysis of the results and an ablation\nstudy provide insights into how the T-TAME design choices affect the quality of\nthe generated explanation maps.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2403.04523v1",
    "published_date": "2024-03-07 14:25:03 UTC",
    "updated_date": "2024-03-07 14:25:03 UTC"
  },
  {
    "arxiv_id": "2403.04511v1",
    "title": "Uncovering the Deep Filter Bubble: Narrow Exposure in Short-Video Recommendation",
    "authors": [
      "Nicholas Sukiennik",
      "Chen Gao",
      "Nian Li"
    ],
    "abstract": "Filter bubbles have been studied extensively within the context of online\ncontent platforms due to their potential to cause undesirable outcomes such as\nuser dissatisfaction or polarization. With the rise of short-video platforms,\nthe filter bubble has been given extra attention because these platforms rely\non an unprecedented use of the recommender system to provide relevant content.\nIn our work, we investigate the deep filter bubble, which refers to the user\nbeing exposed to narrow content within their broad interests. We accomplish\nthis using one-year interaction data from a top short-video platform in China,\nwhich includes hierarchical data with three levels of categories for each\nvideo. We formalize our definition of a \"deep\" filter bubble within this\ncontext, and then explore various correlations within the data: first\nunderstanding the evolution of the deep filter bubble over time, and later\nrevealing some of the factors that give rise to this phenomenon, such as\nspecific categories, user demographics, and feedback type. We observe that\nwhile the overall proportion of users in a filter bubble remains largely\nconstant over time, the depth composition of their filter bubble changes. In\naddition, we find that some demographic groups that have a higher likelihood of\nseeing narrower content and implicit feedback signals can lead to less bubble\nformation. Finally, we propose some ways in which recommender systems can be\ndesigned to reduce the risk of a user getting caught in a bubble.",
    "categories": [
      "cs.AI",
      "H.3.5"
    ],
    "primary_category": "cs.AI",
    "comment": "accepted to WWW 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.04511v1",
    "published_date": "2024-03-07 14:14:40 UTC",
    "updated_date": "2024-03-07 14:14:40 UTC"
  },
  {
    "arxiv_id": "2403.04510v1",
    "title": "Where does In-context Translation Happen in Large Language Models",
    "authors": [
      "Suzanna Sia",
      "David Mueller",
      "Kevin Duh"
    ],
    "abstract": "Self-supervised large language models have demonstrated the ability to\nperform Machine Translation (MT) via in-context learning, but little is known\nabout where the model performs the task with respect to prompt instructions and\ndemonstration examples. In this work, we attempt to characterize the region\nwhere large language models transition from in-context learners to translation\nmodels. Through a series of layer-wise context-masking experiments on\n\\textsc{GPTNeo2.7B}, \\textsc{Bloom3B}, \\textsc{Llama7b} and\n\\textsc{Llama7b-chat}, we demonstrate evidence of a \"task recognition\" point\nwhere the translation task is encoded into the input representations and\nattention to context is no longer necessary. We further observe correspondence\nbetween the low performance when masking out entire layers, and the task\nrecognition layers. Taking advantage of this redundancy results in 45\\%\ncomputational savings when prompting with 5 examples, and task recognition\nachieved at layer 14 / 32. Our layer-wise fine-tuning experiments indicate that\nthe most effective layers for MT fine-tuning are the layers critical to task\nrecognition.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "19 pages. Under Review",
    "pdf_url": "http://arxiv.org/pdf/2403.04510v1",
    "published_date": "2024-03-07 14:12:41 UTC",
    "updated_date": "2024-03-07 14:12:41 UTC"
  },
  {
    "arxiv_id": "2403.04504v1",
    "title": "Improving Matrix Completion by Exploiting Rating Ordinality in Graph Neural Networks",
    "authors": [
      "Jaehyun Lee",
      "SeongKu Kang",
      "Hwanjo Yu"
    ],
    "abstract": "Matrix completion is an important area of research in recommender systems.\nRecent methods view a rating matrix as a user-item bi-partite graph with\nlabeled edges denoting observed ratings and predict the edges between the user\nand item nodes by using the graph neural network (GNN). Despite their\neffectiveness, they treat each rating type as an independent relation type and\nthus cannot sufficiently consider the ordinal nature of the ratings. In this\npaper, we explore a new approach to exploit rating ordinality for GNN, which\nhas not been studied well in the literature. We introduce a new method, called\nROGMC, to leverage Rating Ordinality in GNN-based Matrix Completion. It uses\ncumulative preference propagation to directly incorporate rating ordinality in\nGNN's message passing, allowing for users' stronger preferences to be more\nemphasized based on inherent orders of rating types. This process is\ncomplemented by interest regularization which facilitates preference learning\nusing the underlying interest information. Our extensive experiments show that\nROGMC consistently outperforms the existing strategies of using rating types\nfor GNN. We expect that our attempt to explore the feasibility of utilizing\nrating ordinality for GNN may stimulate further research in this direction.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "4 pages, 2 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2403.04504v1",
    "published_date": "2024-03-07 14:04:33 UTC",
    "updated_date": "2024-03-07 14:04:33 UTC"
  },
  {
    "arxiv_id": "2403.04500v2",
    "title": "A Learnable Prior Improves Inverse Tumor Growth Modeling",
    "authors": [
      "Jonas Weidner",
      "Ivan Ezhov",
      "Michal Balcerak",
      "Marie-Christin Metz",
      "Sergey Litvinov",
      "Sebastian Kaltenbach",
      "Leonhard Feiner",
      "Laurin Lux",
      "Florian Kofler",
      "Jana Lipkova",
      "Jonas Latz",
      "Daniel Rueckert",
      "Bjoern Menze",
      "Benedikt Wiestler"
    ],
    "abstract": "Biophysical modeling, particularly involving partial differential equations\n(PDEs), offers significant potential for tailoring disease treatment protocols\nto individual patients. However, the inverse problem-solving aspect of these\nmodels presents a substantial challenge, either due to the high computational\nrequirements of model-based approaches or the limited robustness of deep\nlearning (DL) methods. We propose a novel framework that leverages the unique\nstrengths of both approaches in a synergistic manner. Our method incorporates a\nDL ensemble for initial parameter estimation, facilitating efficient downstream\nevolutionary sampling initialized with this DL-based prior. We showcase the\neffectiveness of integrating a rapid deep-learning algorithm with a\nhigh-precision evolution strategy in estimating brain tumor cell concentrations\nfrom magnetic resonance images. The DL-Prior plays a pivotal role,\nsignificantly constraining the effective sampling-parameter space. This\nreduction results in a fivefold convergence acceleration and a Dice-score of\n95%.",
    "categories": [
      "physics.med-ph",
      "cs.AI"
    ],
    "primary_category": "physics.med-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04500v2",
    "published_date": "2024-03-07 13:59:34 UTC",
    "updated_date": "2024-11-06 11:05:27 UTC"
  },
  {
    "arxiv_id": "2403.04483v2",
    "title": "GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability",
    "authors": [
      "Zihan Luo",
      "Xiran Song",
      "Hong Huang",
      "Jianxun Lian",
      "Chenhao Zhang",
      "Jinqi Jiang",
      "Xing Xie"
    ],
    "abstract": "Evaluating and enhancing the general capabilities of large language models\n(LLMs) has been an important research topic. Graph is a common data structure\nin the real world, and understanding graph data is a crucial part for advancing\ngeneral intelligence. To evaluate and enhance the graph understanding abilities\nof LLMs, in this paper, we propose a benchmark named GraphInstruct, which\ncomprehensively includes 21 classical graph reasoning tasks, providing diverse\ngraph generation pipelines and detailed reasoning steps. Based on\nGraphInstruct, we further construct GraphLM through efficient\ninstruction-tuning, which shows prominent graph understanding capability. In\norder to enhance the LLM with graph reasoning capability as well, we propose a\nstep mask training strategy, and construct a model named GraphLM+. As one of\nthe pioneering efforts to enhance the graph understanding and reasoning\nabilities of LLMs, extensive experiments have demonstrated the superiority of\nGraphLM and GraphLM+ over other LLMs. We look forward to more researchers\nexploring the potential of LLMs in the graph data mining domain through\nGraphInstruct. Our code for generating GraphInstruct is released publicly at:\nhttps://github.com/CGCL-codes/GraphInstruct.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2403.04483v2",
    "published_date": "2024-03-07 13:36:08 UTC",
    "updated_date": "2024-04-02 07:57:16 UTC"
  },
  {
    "arxiv_id": "2403.04481v3",
    "title": "Do Large Language Model Understand Multi-Intent Spoken Language ?",
    "authors": [
      "Shangjian Yin",
      "Peijie Huang",
      "Yuhong Xu",
      "Haojing Huang",
      "Jiatian Chen"
    ],
    "abstract": "This research signifies a considerable breakthrough in leveraging Large\nLanguage Models (LLMs) for multi-intent spoken language understanding (SLU).\nOur approach re-imagines the use of entity slots in multi-intent SLU\napplications, making the most of the generative potential of LLMs within the\nSLU landscape, leading to the development of the EN-LLM series. Furthermore, we\nintroduce the concept of Sub-Intent Instruction (SII) to amplify the analysis\nand interpretation of complex, multi-intent communications, which further\nsupports the creation of the ENSI-LLM models series. Our novel datasets,\nidentified as LM-MixATIS and LM-MixSNIPS, are synthesized from existing\nbenchmarks. The study evidences that LLMs may match or even surpass the\nperformance of the current best multi-intent SLU models. We also scrutinize the\nperformance of LLMs across a spectrum of intent configurations and dataset\ndistributions. On top of this, we present two revolutionary metrics - Entity\nSlot Accuracy (ESA) and Combined Semantic Accuracy (CSA) - to facilitate a\ndetailed assessment of LLM competence in this multifaceted field.\" Our code and\ndatasets are available at \\url{https://github.com/SJY8460/SLM}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04481v3",
    "published_date": "2024-03-07 13:30:52 UTC",
    "updated_date": "2024-04-15 16:24:35 UTC"
  },
  {
    "arxiv_id": "2403.04473v2",
    "title": "TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document",
    "authors": [
      "Yuliang Liu",
      "Biao Yang",
      "Qiang Liu",
      "Zhang Li",
      "Zhiyin Ma",
      "Shuo Zhang",
      "Xiang Bai"
    ],
    "abstract": "We present TextMonkey, a large multimodal model (LMM) tailored for\ntext-centric tasks. Our approach introduces enhancement across several\ndimensions: By adopting Shifted Window Attention with zero-initialization, we\nachieve cross-window connectivity at higher input resolutions and stabilize\nearly training; We hypothesize that images may contain redundant tokens, and by\nusing similarity to filter out significant tokens, we can not only streamline\nthe token length but also enhance the model's performance. Moreover, by\nexpanding our model's capabilities to encompass text spotting and grounding,\nand incorporating positional information into responses, we enhance\ninterpretability. It also learns to perform screenshot tasks through\nfinetuning. Evaluation on 12 benchmarks shows notable improvements: 5.2% in\nScene Text-Centric tasks (including STVQA, TextVQA, and OCRVQA), 6.9% in\nDocument-Oriented tasks (such as DocVQA, InfoVQA, ChartVQA, DeepForm, Kleister\nCharity, and WikiTableQuestions), and 2.8% in Key Information Extraction tasks\n(comprising FUNSD, SROIE, and POIE). It outperforms in scene text spotting with\na 10.9\\% increase and sets a new standard on OCRBench, a comprehensive\nbenchmark consisting of 29 OCR-related assessments, with a score of 561,\nsurpassing previous open-sourced large multimodal models for document\nunderstanding. Code will be released at https://github.com/Yuliang-Liu/Monkey.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04473v2",
    "published_date": "2024-03-07 13:16:24 UTC",
    "updated_date": "2024-03-15 06:51:30 UTC"
  },
  {
    "arxiv_id": "2403.04471v2",
    "title": "The Shutdown Problem: An AI Engineering Puzzle for Decision Theorists",
    "authors": [
      "Elliott Thornley"
    ],
    "abstract": "I explain the shutdown problem: the problem of designing artificial agents\nthat (1) shut down when a shutdown button is pressed, (2) don't try to prevent\nor cause the pressing of the shutdown button, and (3) otherwise pursue goals\ncompetently. I prove three theorems that make the difficulty precise. These\ntheorems show that agents satisfying some innocuous-seeming conditions will\noften try to prevent or cause the pressing of the shutdown button, even in\ncases where it's costly to do so. And patience trades off against\nshutdownability: the more patient an agent, the greater the costs that agent is\nwilling to incur to manipulate the shutdown button. I end by noting that these\ntheorems can guide our search for solutions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04471v2",
    "published_date": "2024-03-07 13:16:07 UTC",
    "updated_date": "2024-04-09 15:09:35 UTC"
  },
  {
    "arxiv_id": "2403.07004v2",
    "title": "Convergence of Some Convex Message Passing Algorithms to a Fixed Point",
    "authors": [
      "Vaclav Voracek",
      "Tomas Werner"
    ],
    "abstract": "A popular approach to the MAP inference problem in graphical models is to\nminimize an upper bound obtained from a dual linear programming or Lagrangian\nrelaxation by (block-)coordinate descent. This is also known as\nconvex/convergent message passing; examples are max-sum diffusion and\nsequential tree-reweighted message passing (TRW-S). Convergence properties of\nthese methods are currently not fully understood. They have been proved to\nconverge to the set characterized by local consistency of active constraints,\nwith unknown convergence rate; however, it was not clear if the iterates\nconverge at all (to any point). We prove a stronger result (conjectured before\nbut never proved): the iterates converge to a fixed point of the method.\nMoreover, we show that the algorithm terminates within\n$\\mathcal{O}(1/\\varepsilon)$ iterations. We first prove this for a version of\ncoordinate descent applied to a general piecewise-affine convex objective. Then\nwe show that several convex message passing methods are special cases of this\nmethod. Finally, we show that a slightly different version of coordinate\ndescent can cycle.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "ICML 2024; comments are welcome",
    "pdf_url": "http://arxiv.org/pdf/2403.07004v2",
    "published_date": "2024-03-07 13:14:21 UTC",
    "updated_date": "2024-06-05 12:20:29 UTC"
  },
  {
    "arxiv_id": "2403.04468v1",
    "title": "A Survey of Graph Neural Networks in Real world: Imbalance, Noise, Privacy and OOD Challenges",
    "authors": [
      "Wei Ju",
      "Siyu Yi",
      "Yifan Wang",
      "Zhiping Xiao",
      "Zhengyang Mao",
      "Hourun Li",
      "Yiyang Gu",
      "Yifang Qin",
      "Nan Yin",
      "Senzhang Wang",
      "Xinwang Liu",
      "Xiao Luo",
      "Philip S. Yu",
      "Ming Zhang"
    ],
    "abstract": "Graph-structured data exhibits universality and widespread applicability\nacross diverse domains, such as social network analysis, biochemistry,\nfinancial fraud detection, and network security. Significant strides have been\nmade in leveraging Graph Neural Networks (GNNs) to achieve remarkable success\nin these areas. However, in real-world scenarios, the training environment for\nmodels is often far from ideal, leading to substantial performance degradation\nof GNN models due to various unfavorable factors, including imbalance in data\ndistribution, the presence of noise in erroneous data, privacy protection of\nsensitive information, and generalization capability for out-of-distribution\n(OOD) scenarios. To tackle these issues, substantial efforts have been devoted\nto improving the performance of GNN models in practical real-world scenarios,\nas well as enhancing their reliability and robustness. In this paper, we\npresent a comprehensive survey that systematically reviews existing GNN models,\nfocusing on solutions to the four mentioned real-world challenges including\nimbalance, noise, privacy, and OOD in practical scenarios that many existing\nreviews have not considered. Specifically, we first highlight the four key\nchallenges faced by existing GNNs, paving the way for our exploration of\nreal-world GNN models. Subsequently, we provide detailed discussions on these\nfour aspects, dissecting how these solutions contribute to enhancing the\nreliability and robustness of GNN models. Last but not least, we outline\npromising directions and offer future perspectives in the field.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04468v1",
    "published_date": "2024-03-07 13:10:37 UTC",
    "updated_date": "2024-03-07 13:10:37 UTC"
  },
  {
    "arxiv_id": "2403.04454v1",
    "title": "Low-Resource Court Judgment Summarization for Common Law Systems",
    "authors": [
      "Shuaiqi Liu",
      "Jiannong Cao",
      "Yicong Li",
      "Ruosong Yang",
      "Zhiyuan Wen"
    ],
    "abstract": "Common law courts need to refer to similar precedents' judgments to inform\ntheir current decisions. Generating high-quality summaries of court judgment\ndocuments can facilitate legal practitioners to efficiently review previous\ncases and assist the general public in accessing how the courts operate and how\nthe law is applied. Previous court judgment summarization research focuses on\ncivil law or a particular jurisdiction's judgments. However, judges can refer\nto the judgments from all common law jurisdictions. Current summarization\ndatasets are insufficient to satisfy the demands of summarizing precedents\nacross multiple jurisdictions, especially when labeled data are scarce for many\njurisdictions. To address the lack of datasets, we present CLSum, the first\ndataset for summarizing multi-jurisdictional common law court judgment\ndocuments. Besides, this is the first court judgment summarization work\nadopting large language models (LLMs) in data augmentation, summary generation,\nand evaluation. Specifically, we design an LLM-based data augmentation method\nincorporating legal knowledge. We also propose a legal knowledge enhanced\nevaluation metric based on LLM to assess the quality of generated judgment\nsummaries. Our experimental results verify that the LLM-based summarization\nmethods can perform well in the few-shot and zero-shot settings. Our LLM-based\ndata augmentation method can mitigate the impact of low data resources.\nFurthermore, we carry out comprehensive comparative experiments to find\nessential model components and settings that are capable of enhancing\nsummarization performance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7; I.7"
    ],
    "primary_category": "cs.CL",
    "comment": "First submitted to Information Processing and Management on Oct. 29,\n  2023. Major Revision submitted on Mar.6, 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.04454v1",
    "published_date": "2024-03-07 12:47:42 UTC",
    "updated_date": "2024-03-07 12:47:42 UTC"
  },
  {
    "arxiv_id": "2403.04449v2",
    "title": "Feedback-Generation for Programming Exercises With GPT-4",
    "authors": [
      "Imen Azaiz",
      "Natalie Kiesler",
      "Sven Strickroth"
    ],
    "abstract": "Ever since Large Language Models (LLMs) and related applications have become\nbroadly available, several studies investigated their potential for assisting\neducators and supporting students in higher education. LLMs such as Codex,\nGPT-3.5, and GPT 4 have shown promising results in the context of large\nprogramming courses, where students can benefit from feedback and hints if\nprovided timely and at scale. This paper explores the quality of GPT-4 Turbo's\ngenerated output for prompts containing both the programming task specification\nand a student's submission as input. Two assignments from an introductory\nprogramming course were selected, and GPT-4 was asked to generate feedback for\n55 randomly chosen, authentic student programming submissions. The output was\nqualitatively analyzed regarding correctness, personalization, fault\nlocalization, and other features identified in the material. Compared to prior\nwork and analyses of GPT-3.5, GPT-4 Turbo shows notable improvements. For\nexample, the output is more structured and consistent. GPT-4 Turbo can also\naccurately identify invalid casing in student programs' output. In some cases,\nthe feedback also includes the output of the student program. At the same time,\ninconsistent feedback was noted such as stating that the submission is correct\nbut an error needs to be fixed. The present work increases our understanding of\nLLMs' potential, limitations, and how to integrate them into e-assessment\nsystems, pedagogical scenarios, and instructing students who are using\napplications based on GPT-4.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "accepted at ITiCSE 2024, Milan, Italy",
    "pdf_url": "http://arxiv.org/pdf/2403.04449v2",
    "published_date": "2024-03-07 12:37:52 UTC",
    "updated_date": "2024-07-04 07:30:22 UTC"
  },
  {
    "arxiv_id": "2403.04447v2",
    "title": "FRRI: a novel algorithm for fuzzy-rough rule induction",
    "authors": [
      "Henri Bollaert",
      "Marko Palangetić",
      "Chris Cornelis",
      "Salvatore Greco",
      "Roman Słowiński"
    ],
    "abstract": "Interpretability is the next frontier in machine learning research. In the\nsearch for white box models - as opposed to black box models, like random\nforests or neural networks - rule induction algorithms are a logical and\npromising option, since the rules can easily be understood by humans. Fuzzy and\nrough set theory have been successfully applied to this archetype, almost\nalways separately. As both approaches to rule induction involve granular\ncomputing based on the concept of equivalence classes, it is natural to combine\nthem. The QuickRules\\cite{JensenCornelis2009} algorithm was a first attempt at\nusing fuzzy rough set theory for rule induction. It is based on QuickReduct, a\ngreedy algorithm for building decision reducts. QuickRules already showed an\nimprovement over other rule induction methods. However, to evaluate the full\npotential of a fuzzy rough rule induction algorithm, one needs to start from\nthe foundations. In this paper, we introduce a novel rule induction algorithm\ncalled Fuzzy Rough Rule Induction (FRRI). We provide background and explain the\nworkings of our algorithm. Furthermore, we perform a computational experiment\nto evaluate the performance of our algorithm and compare it to other\nstate-of-the-art rule induction approaches. We find that our algorithm is more\naccurate while creating small rulesets consisting of relatively short rules. We\nend the paper by outlining some directions for future work.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04447v2",
    "published_date": "2024-03-07 12:34:03 UTC",
    "updated_date": "2024-08-29 11:28:10 UTC"
  },
  {
    "arxiv_id": "2403.04442v1",
    "title": "Cooperative Bayesian Optimization for Imperfect Agents",
    "authors": [
      "Ali Khoshvishkaie",
      "Petrus Mikkola",
      "Pierre-Alexandre Murena",
      "Samuel Kaski"
    ],
    "abstract": "We introduce a cooperative Bayesian optimization problem for optimizing\nblack-box functions of two variables where two agents choose together at which\npoints to query the function but have only control over one variable each. This\nsetting is inspired by human-AI teamwork, where an AI-assistant helps its human\nuser solve a problem, in this simplest case, collaborative optimization. We\nformulate the solution as sequential decision-making, where the agent we\ncontrol models the user as a computationally rational agent with prior\nknowledge about the function. We show that strategic planning of the queries\nenables better identification of the global maximum of the function as long as\nthe user avoids excessive exploration. This planning is made possible by using\nBayes Adaptive Monte Carlo planning and by endowing the agent with a user model\nthat accounts for conservative belief updates and exploratory sampling of the\npoints to query.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04442v1",
    "published_date": "2024-03-07 12:16:51 UTC",
    "updated_date": "2024-03-07 12:16:51 UTC"
  },
  {
    "arxiv_id": "2403.04436v1",
    "title": "Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation",
    "authors": [
      "Tairan He",
      "Zhengyi Luo",
      "Wenli Xiao",
      "Chong Zhang",
      "Kris Kitani",
      "Changliu Liu",
      "Guanya Shi"
    ],
    "abstract": "We present Human to Humanoid (H2O), a reinforcement learning (RL) based\nframework that enables real-time whole-body teleoperation of a full-sized\nhumanoid robot with only an RGB camera. To create a large-scale retargeted\nmotion dataset of human movements for humanoid robots, we propose a scalable\n\"sim-to-data\" process to filter and pick feasible motions using a privileged\nmotion imitator. Afterwards, we train a robust real-time humanoid motion\nimitator in simulation using these refined motions and transfer it to the real\nhumanoid robot in a zero-shot manner. We successfully achieve teleoperation of\ndynamic whole-body motions in real-world scenarios, including walking, back\njumping, kicking, turning, waving, pushing, boxing, etc. To the best of our\nknowledge, this is the first demonstration to achieve learning-based real-time\nwhole-body humanoid teleoperation.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "Project website: https://human2humanoid.com/",
    "pdf_url": "http://arxiv.org/pdf/2403.04436v1",
    "published_date": "2024-03-07 12:10:41 UTC",
    "updated_date": "2024-03-07 12:10:41 UTC"
  },
  {
    "arxiv_id": "2403.07003v1",
    "title": "Evacuation Management Framework towards Smart City-wide Intelligent Emergency Interactive Response System",
    "authors": [
      "Anuj Abraham",
      "Yi Zhang",
      "Shitala Prasad"
    ],
    "abstract": "A smart city solution toward future 6G network deployment allows small and\nmedium sized enterprises (SMEs), industry, and government entities to connect\nwith the infrastructures and play a crucial role in enhancing emergency\npreparedness with advanced sensors. The objective of this work is to propose a\nset of coordinated technological solutions to transform an existing emergency\nresponse system into an intelligent interactive system, thereby improving the\npublic services and the quality of life for residents at home, on road, in\nhospitals, transport hubs, etc. In this context, we consider a city wide view\nfrom three different application scenes that are closely related to peoples\ndaily life, to optimize the actions taken at relevant departments. Therefore,\nusing artificial intelligence (AI) and machine learning (ML) techniques to\nenable the next generation connected vehicle experiences, we specifically focus\non accidents happening in indoor households, urban roads, and at large public\nfacilities. This smart interactive response system will benefit from advanced\nsensor fusion and AI by formulating a real time dynamic model.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "cs.NI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.07003v1",
    "published_date": "2024-03-07 12:10:19 UTC",
    "updated_date": "2024-03-07 12:10:19 UTC"
  },
  {
    "arxiv_id": "2403.04427v1",
    "title": "Sentiment-driven prediction of financial returns: a Bayesian-enhanced FinBERT approach",
    "authors": [
      "Raffaele Giuseppe Cestari",
      "Simone Formentin"
    ],
    "abstract": "Predicting financial returns accurately poses a significant challenge due to\nthe inherent uncertainty in financial time series data. Enhancing prediction\nmodels' performance hinges on effectively capturing both social and financial\nsentiment. In this study, we showcase the efficacy of leveraging sentiment\ninformation extracted from tweets using the FinBERT large language model. By\nmeticulously curating an optimal feature set through correlation analysis and\nemploying Bayesian-optimized Recursive Feature Elimination for automatic\nfeature selection, we surpass existing methodologies, achieving an F1-score\nexceeding 70% on the test set. This success translates into demonstrably higher\ncumulative profits during backtested trading. Our investigation focuses on\nreal-world SPY ETF data alongside corresponding tweets sourced from the\nStockTwits platform.",
    "categories": [
      "cs.CE",
      "cs.AI"
    ],
    "primary_category": "cs.CE",
    "comment": "Version exposed at XXV Workshop on Quantitative Finance Bologna\n  (Italy), April 11-13 2024 (not peer reviewed but accepted for the workshop)",
    "pdf_url": "http://arxiv.org/pdf/2403.04427v1",
    "published_date": "2024-03-07 11:56:36 UTC",
    "updated_date": "2024-03-07 11:56:36 UTC"
  },
  {
    "arxiv_id": "2403.04417v1",
    "title": "Promising and worth-to-try future directions for advancing state-of-the-art surrogates methods of agent-based models in social and health computational sciences",
    "authors": [
      "Atiyah Elsheikh"
    ],
    "abstract": "The execution and runtime performance of model-based analysis tools for\nrealistic large-scale ABMs (Agent-Based Models) can be excessively long. This\ndue to the computational demand exponentially proportional to the model size\n(e.g. Population size) and the number of model parameters. Even the runtime of\na single simulation of a realistic ABM may demand huge computational resources\nwhen attempting to employ realistic population size. The main aim of this\nad-hoc brief report is to highlight some of surrogate models that were adequate\nand computationally less demanding for nonlinear dynamical models in various\nmodeling application areas.To the author knowledge, these methods have been\nnot, at least extensively, employed for ABMs within the field of (SHCS) Social\nHealth Computational Sciences, yet. Thus, they might be, but not necessarily,\nuseful in progressing state of the art for establishing surrogate models for\nABMs in the field of SHCS.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SY",
      "eess.SY",
      "math.DS"
    ],
    "primary_category": "cs.CL",
    "comment": "4 pages",
    "pdf_url": "http://arxiv.org/pdf/2403.04417v1",
    "published_date": "2024-03-07 11:30:56 UTC",
    "updated_date": "2024-03-07 11:30:56 UTC"
  },
  {
    "arxiv_id": "2403.04382v1",
    "title": "Acceleron: A Tool to Accelerate Research Ideation",
    "authors": [
      "Harshit Nigam",
      "Manasi Patwardhan",
      "Lovekesh Vig",
      "Gautam Shroff"
    ],
    "abstract": "Several tools have recently been proposed for assisting researchers during\nvarious stages of the research life-cycle. However, these primarily concentrate\non tasks such as retrieving and recommending relevant literature, reviewing and\ncritiquing the draft, and writing of research manuscripts. Our investigation\nreveals a significant gap in availability of tools specifically designed to\nassist researchers during the challenging ideation phase of the research\nlife-cycle. To aid with research ideation, we propose `Acceleron', a research\naccelerator for different phases of the research life cycle, and which is\nspecially designed to aid the ideation process. Acceleron guides researchers\nthrough the formulation of a comprehensive research proposal, encompassing a\nnovel research problem. The proposals motivation is validated for novelty by\nidentifying gaps in the existing literature and suggesting a plausible list of\ntechniques to solve the proposed problem. We leverage the reasoning and\ndomain-specific skills of Large Language Models (LLMs) to create an agent-based\narchitecture incorporating colleague and mentor personas for LLMs. The LLM\nagents emulate the ideation process undertaken by researchers, engaging\nresearchers in an interactive fashion to aid in the development of the research\nproposal. Notably, our tool addresses challenges inherent in LLMs, such as\nhallucinations, implements a two-stage aspect-based retrieval to manage\nprecision-recall trade-offs, and tackles issues of unanswerability. As\nevaluation, we illustrate the execution of our motivation validation and method\nsynthesis workflows on proposals from the ML and NLP domain, given by 3\ndistinct researchers. Our observations and evaluations provided by the\nresearchers illustrate the efficacy of the tool in terms of assisting\nresearchers with appropriate inputs at distinct stages and thus leading to\nimproved time efficiency.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at AI2ASE Workshop at AAAI'24 Conference. 13 Pages and 4\n  Figures",
    "pdf_url": "http://arxiv.org/pdf/2403.04382v1",
    "published_date": "2024-03-07 10:20:06 UTC",
    "updated_date": "2024-03-07 10:20:06 UTC"
  },
  {
    "arxiv_id": "2403.04374v1",
    "title": "Model-Free Load Frequency Control of Nonlinear Power Systems Based on Deep Reinforcement Learning",
    "authors": [
      "Xiaodi Chen",
      "Meng Zhang",
      "Zhengguang Wu",
      "Ligang Wu",
      "Xiaohong Guan"
    ],
    "abstract": "Load frequency control (LFC) is widely employed in power systems to stabilize\nfrequency fluctuation and guarantee power quality. However, most existing LFC\nmethods rely on accurate power system modeling and usually ignore the nonlinear\ncharacteristics of the system, limiting controllers' performance. To solve\nthese problems, this paper proposes a model-free LFC method for nonlinear power\nsystems based on deep deterministic policy gradient (DDPG) framework. The\nproposed method establishes an emulator network to emulate power system\ndynamics. After defining the action-value function, the emulator network is\napplied for control actions evaluation instead of the critic network. Then the\nactor network controller is effectively optimized by estimating the policy\ngradient based on zeroth-order optimization (ZOO) and backpropagation\nalgorithm. Simulation results and corresponding comparisons demonstrate the\ndesigned controller can generate appropriate control actions and has strong\nadaptability for nonlinear power systems.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04374v1",
    "published_date": "2024-03-07 10:06:46 UTC",
    "updated_date": "2024-03-07 10:06:46 UTC"
  },
  {
    "arxiv_id": "2403.04369v3",
    "title": "From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge Prediction",
    "authors": [
      "Ang Li",
      "Qiangchao Chen",
      "Yiquan Wu",
      "Ming Cai",
      "Xiang Zhou",
      "Fei Wu",
      "Kun Kuang"
    ],
    "abstract": "Confusing charge prediction is a challenging task in legal AI, which involves\npredicting confusing charges based on fact descriptions. While existing charge\nprediction methods have shown impressive performance, they face significant\nchallenges when dealing with confusing charges, such as Snatch and Robbery. In\nthe legal domain, constituent elements play a pivotal role in distinguishing\nconfusing charges. Constituent elements are fundamental behaviors underlying\ncriminal punishment and have subtle distinctions among charges. In this paper,\nwe introduce a novel From Graph to Word Bag (FWGB) approach, which introduces\ndomain knowledge regarding constituent elements to guide the model in making\njudgments on confusing charges, much like a judge's reasoning process.\nSpecifically, we first construct a legal knowledge graph containing constituent\nelements to help select keywords for each charge, forming a word bag.\nSubsequently, to guide the model's attention towards the differentiating\ninformation for each charge within the context, we expand the attention\nmechanism and introduce a new loss function with attention supervision through\nwords in the word bag. We construct the confusing charges dataset from\nreal-world judicial documents. Experiments demonstrate the effectiveness of our\nmethod, especially in maintaining exceptional performance in imbalanced label\ndistributions.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04369v3",
    "published_date": "2024-03-07 09:57:42 UTC",
    "updated_date": "2024-03-24 10:08:13 UTC"
  },
  {
    "arxiv_id": "2403.04366v1",
    "title": "Enhancing Court View Generation with Knowledge Injection and Guidance",
    "authors": [
      "Ang Li",
      "Yiquan Wu",
      "Yifei Liu",
      "Fei Wu",
      "Ming Cai",
      "Kun Kuang"
    ],
    "abstract": "Court View Generation (CVG) is a challenging task in the field of Legal\nArtificial Intelligence (LegalAI), which aims to generate court views based on\nthe plaintiff claims and the fact descriptions. While Pretrained Language\nModels (PLMs) have showcased their prowess in natural language generation,\ntheir application to the complex, knowledge-intensive domain of CVG often\nreveals inherent limitations. In this paper, we present a novel approach, named\nKnowledge Injection and Guidance (KIG), designed to bolster CVG using PLMs. To\nefficiently incorporate domain knowledge during the training stage, we\nintroduce a knowledge-injected prompt encoder for prompt tuning, thereby\nreducing computational overhead. Moreover, to further enhance the model's\nability to utilize domain knowledge, we employ a generating navigator, which\ndynamically guides the text generation process in the inference stage without\naltering the model's architecture, making it readily transferable.\nComprehensive experiments on real-world data demonstrate the effectiveness of\nour approach compared to several established baselines, especially in the\nresponsivity of claims, where it outperforms the best baseline by 11.87%.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04366v1",
    "published_date": "2024-03-07 09:51:11 UTC",
    "updated_date": "2024-03-07 09:51:11 UTC"
  },
  {
    "arxiv_id": "2403.04359v1",
    "title": "Symmetry Considerations for Learning Task Symmetric Robot Policies",
    "authors": [
      "Mayank Mittal",
      "Nikita Rudin",
      "Victor Klemm",
      "Arthur Allshire",
      "Marco Hutter"
    ],
    "abstract": "Symmetry is a fundamental aspect of many real-world robotic tasks. However,\ncurrent deep reinforcement learning (DRL) approaches can seldom harness and\nexploit symmetry effectively. Often, the learned behaviors fail to achieve the\ndesired transformation invariances and suffer from motion artifacts. For\ninstance, a quadruped may exhibit different gaits when commanded to move\nforward or backward, even though it is symmetrical about its torso. This issue\nbecomes further pronounced in high-dimensional or complex environments, where\nDRL methods are prone to local optima and fail to explore regions of the state\nspace equally. Past methods on encouraging symmetry for robotic tasks have\nstudied this topic mainly in a single-task setting, where symmetry usually\nrefers to symmetry in the motion, such as the gait patterns. In this paper, we\nrevisit this topic for goal-conditioned tasks in robotics, where symmetry lies\nmainly in task execution and not necessarily in the learned motions themselves.\nIn particular, we investigate two approaches to incorporate symmetry invariance\ninto DRL -- data augmentation and mirror loss function. We provide a\ntheoretical foundation for using augmented samples in an on-policy setting.\nBased on this, we show that the corresponding approach achieves faster\nconvergence and improves the learned behaviors in various challenging robotic\ntasks, from climbing boxes with a quadruped to dexterous manipulation.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "M. Mittal and N. Rudin contributed equally. Accepted for ICRA 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.04359v1",
    "published_date": "2024-03-07 09:41:11 UTC",
    "updated_date": "2024-03-07 09:41:11 UTC"
  },
  {
    "arxiv_id": "2403.04343v2",
    "title": "CoTBal: Comprehensive Task Balancing for Multi-Task Visual Instruction Tuning",
    "authors": [
      "Yanqi Dai",
      "Zebin You",
      "Dong Jing",
      "Yutian Luo",
      "Nanyi Fei",
      "Guoxing Yang",
      "Zhiwu Lu"
    ],
    "abstract": "Visual instruction tuning is an important training stage for large multimodal\nmodels. Nevertheless, when learning multiple visual tasks simultaneously, this\napproach may lead to suboptimal and imbalanced overall performance due to\nlatent knowledge conflicts across tasks. To mitigate this issue, we introduce a\nnovel Comprehensive Task Balancing (CoTBal) algorithm tailored for multi-task\nvisual instruction tuning. To our knowledge, this is the first work to explore\nmulti-task optimization in visual instruction tuning. Specifically, we consider\ntwo critical dimensions for task balancing: (1) Inter-Task Contribution, which\nrepresents the phenomenon where learning one task could enhance the performance\non others owing to the overlapping knowledge domains across tasks, and (2)\nIntra-Task Difficulty, which indicates the inherent learning difficulty of a\nsingle task. Furthermore, by quantifying these with performance-based metrics,\ncomprehensive task balancing is thus achieved by assigning greater weight to\ntasks that offer substantial contributions to others, receive minimal\ncontributions from others, and present high learning difficulties. Extensive\nexperiments on three benchmarks demonstrate that our CoTBal algorithm results\nin superior and more balanced overall performance in multi-task visual\ninstruction tuning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04343v2",
    "published_date": "2024-03-07 09:11:16 UTC",
    "updated_date": "2025-03-18 03:24:54 UTC"
  },
  {
    "arxiv_id": "2403.04326v1",
    "title": "Edge-based Parametric Digital Twins for Intelligent Building Indoor Climate Modeling",
    "authors": [
      "Zhongjun Ni",
      "Chi Zhang",
      "Magnus Karlsson",
      "Shaofang Gong"
    ],
    "abstract": "Digital transformation in the built environment generates vast data for\ndeveloping data-driven models to optimize building operations. This study\npresents an integrated solution utilizing edge computing, digital twins, and\ndeep learning to enhance the understanding of climate in buildings. Parametric\ndigital twins, created using an ontology, ensure consistent data representation\nacross diverse service systems equipped by different buildings. Based on\ncreated digital twins and collected data, deep learning methods are employed to\ndevelop predictive models for identifying patterns in indoor climate and\nproviding insights. Both the parametric digital twin and deep learning models\nare deployed on edge for low latency and privacy compliance. As a\ndemonstration, a case study was conducted in a historic building in\n\\\"Osterg\\\"otland, Sweden, to compare the performance of five deep learning\narchitectures. The results indicate that the time-series dense encoder model\nexhibited strong competitiveness in performing multi-horizon forecasts of\nindoor temperature and relative humidity with low computational costs.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "68T07",
      "I.5.4"
    ],
    "primary_category": "eess.SY",
    "comment": "8 pages, 8 figures, accepted in the 20th IEEE International\n  Conference on Factory Communication Systems",
    "pdf_url": "http://arxiv.org/pdf/2403.04326v1",
    "published_date": "2024-03-07 08:45:31 UTC",
    "updated_date": "2024-03-07 08:45:31 UTC"
  },
  {
    "arxiv_id": "2403.04325v3",
    "title": "Measuring Meaning Composition in the Human Brain with Composition Scores from Large Language Models",
    "authors": [
      "Changjiang Gao",
      "Jixing Li",
      "Jiajun Chen",
      "Shujian Huang"
    ],
    "abstract": "The process of meaning composition, wherein smaller units like morphemes or\nwords combine to form the meaning of phrases and sentences, is essential for\nhuman sentence comprehension. Despite extensive neurolinguistic research into\nthe brain regions involved in meaning composition, a computational metric to\nquantify the extent of composition is still lacking. Drawing on the key-value\nmemory interpretation of transformer feed-forward network blocks, we introduce\nthe Composition Score, a novel model-based metric designed to quantify the\ndegree of meaning composition during sentence comprehension. Experimental\nfindings show that this metric correlates with brain clusters associated with\nword frequency, structural processing, and general sensitivity to words,\nsuggesting the multifaceted nature of meaning composition during human sentence\ncomprehension.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACL 2024 (main conference, long paper)",
    "pdf_url": "http://arxiv.org/pdf/2403.04325v3",
    "published_date": "2024-03-07 08:44:42 UTC",
    "updated_date": "2024-07-10 06:49:40 UTC"
  },
  {
    "arxiv_id": "2403.04321v2",
    "title": "Discriminative Probing and Tuning for Text-to-Image Generation",
    "authors": [
      "Leigang Qu",
      "Wenjie Wang",
      "Yongqi Li",
      "Hanwang Zhang",
      "Liqiang Nie",
      "Tat-Seng Chua"
    ],
    "abstract": "Despite advancements in text-to-image generation (T2I), prior methods often\nface text-image misalignment problems such as relation confusion in generated\nimages. Existing solutions involve cross-attention manipulation for better\ncompositional understanding or integrating large language models for improved\nlayout planning. However, the inherent alignment capabilities of T2I models are\nstill inadequate. By reviewing the link between generative and discriminative\nmodeling, we posit that T2I models' discriminative abilities may reflect their\ntext-image alignment proficiency during generation. In this light, we advocate\nbolstering the discriminative abilities of T2I models to achieve more precise\ntext-to-image alignment for generation. We present a discriminative adapter\nbuilt on T2I models to probe their discriminative abilities on two\nrepresentative tasks and leverage discriminative fine-tuning to improve their\ntext-image alignment. As a bonus of the discriminative adapter, a\nself-correction mechanism can leverage discriminative gradients to better align\ngenerated images to text prompts during inference. Comprehensive evaluations\nacross three benchmark datasets, including both in-distribution and\nout-of-distribution scenarios, demonstrate our method's superior generation\nperformance. Meanwhile, it achieves state-of-the-art discriminative performance\non the two discriminative tasks compared to other generative models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024; project page: https://dpt-t2i.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2403.04321v2",
    "published_date": "2024-03-07 08:37:33 UTC",
    "updated_date": "2024-03-14 08:02:29 UTC"
  },
  {
    "arxiv_id": "2403.04311v1",
    "title": "ALTO: An Efficient Network Orchestrator for Compound AI Systems",
    "authors": [
      "Keshav Santhanam",
      "Deepti Raghavan",
      "Muhammad Shahir Rahman",
      "Thejas Venkatesh",
      "Neha Kunjal",
      "Pratiksha Thaker",
      "Philip Levis",
      "Matei Zaharia"
    ],
    "abstract": "We present ALTO, a network orchestrator for efficiently serving compound AI\nsystems such as pipelines of language models. ALTO achieves high throughput and\nlow latency by taking advantage of an optimization opportunity specific to\ngenerative language models: streaming intermediate outputs. As language models\nproduce outputs token by token, ALTO exposes opportunities to stream\nintermediate outputs between stages when possible. We highlight two new\nchallenges of correctness and load balancing which emerge when streaming\nintermediate data across distributed pipeline stage instances. We also motivate\nthe need for an aggregation-aware routing interface and distributed\nprompt-aware scheduling to address these challenges. We demonstrate the impact\nof ALTO's partial output streaming on a complex chatbot verification pipeline,\nincreasing throughput by up to 3x for a fixed latency target of 4 seconds /\nrequest while also reducing tail latency by 1.8x compared to a baseline serving\napproach.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.DC",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04311v1",
    "published_date": "2024-03-07 08:30:26 UTC",
    "updated_date": "2024-03-07 08:30:26 UTC"
  },
  {
    "arxiv_id": "2403.04309v1",
    "title": "AO-DETR: Anti-Overlapping DETR for X-Ray Prohibited Items Detection",
    "authors": [
      "Mingyuan Li",
      "Tong Jia",
      "Hao Wang",
      "Bowen Ma",
      "Shuyang Lin",
      "Da Cai",
      "Dongyue Chen"
    ],
    "abstract": "Prohibited item detection in X-ray images is one of the most essential and\nhighly effective methods widely employed in various security inspection\nscenarios. Considering the significant overlapping phenomenon in X-ray\nprohibited item images, we propose an Anti-Overlapping DETR (AO-DETR) based on\none of the state-of-the-art general object detectors, DINO. Specifically, to\naddress the feature coupling issue caused by overlapping phenomena, we\nintroduce the Category-Specific One-to-One Assignment (CSA) strategy to\nconstrain category-specific object queries in predicting prohibited items of\nfixed categories, which can enhance their ability to extract features specific\nto prohibited items of a particular category from the overlapping\nforeground-background features. To address the edge blurring problem caused by\noverlapping phenomena, we propose the Look Forward Densely (LFD) scheme, which\nimproves the localization accuracy of reference boxes in mid-to-high-level\ndecoder layers and enhances the ability to locate blurry edges of the final\nlayer. Similar to DINO, our AO-DETR provides two different versions with\ndistinct backbones, tailored to meet diverse application requirements.\nExtensive experiments on the PIXray and OPIXray datasets demonstrate that the\nproposed method surpasses the state-of-the-art object detectors, indicating its\npotential applications in the field of prohibited item detection. The source\ncode will be released at https://github.com/Limingyuan001/AO-DETR-test.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04309v1",
    "published_date": "2024-03-07 08:30:17 UTC",
    "updated_date": "2024-03-07 08:30:17 UTC"
  },
  {
    "arxiv_id": "2403.04306v5",
    "title": "Effectiveness Assessment of Recent Large Vision-Language Models",
    "authors": [
      "Yao Jiang",
      "Xinyu Yan",
      "Ge-Peng Ji",
      "Keren Fu",
      "Meijun Sun",
      "Huan Xiong",
      "Deng-Ping Fan",
      "Fahad Shahbaz Khan"
    ],
    "abstract": "The advent of large vision-language models (LVLMs) represents a remarkable\nadvance in the quest for artificial general intelligence. However, the model's\neffectiveness in both specialized and general tasks warrants further\ninvestigation. This paper endeavors to evaluate the competency of popular LVLMs\nin specialized and general tasks, respectively, aiming to offer a comprehensive\nunderstanding of these novel models. To gauge their effectiveness in\nspecialized tasks, we employ six challenging tasks in three different\napplication scenarios: natural, healthcare, and industrial. These six tasks\ninclude salient/camouflaged/transparent object detection, as well as polyp\ndetection, skin lesion detection, and industrial anomaly detection. We examine\nthe performance of three recent open-source LVLMs, including MiniGPT-v2,\nLLaVA-1.5, and Shikra, on both visual recognition and localization in these\ntasks. Moreover, we conduct empirical investigations utilizing the\naforementioned LVLMs together with GPT-4V, assessing their multi-modal\nunderstanding capabilities in general tasks including object counting, absurd\nquestion answering, affordance reasoning, attribute recognition, and spatial\nrelation reasoning. Our investigations reveal that these LVLMs demonstrate\nlimited proficiency not only in specialized tasks but also in general tasks. We\ndelve deep into this inadequacy and uncover several potential factors,\nincluding limited cognition in specialized tasks, object hallucination,\ntext-to-image interference, and decreased robustness in complex problems. We\nhope that this study can provide useful insights for the future development of\nLVLMs, helping researchers improve LVLMs for both general and specialized\napplications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by Visual Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2403.04306v5",
    "published_date": "2024-03-07 08:25:27 UTC",
    "updated_date": "2024-10-26 03:07:45 UTC"
  },
  {
    "arxiv_id": "2403.04299v2",
    "title": "LitSim: A Conflict-aware Policy for Long-term Interactive Traffic Simulation",
    "authors": [
      "Haojie Xin",
      "Xiaodong Zhang",
      "Renzhi Tang",
      "Songyang Yan",
      "Qianrui Zhao",
      "Chunze Yang",
      "Wen Cui",
      "Zijiang Yang"
    ],
    "abstract": "Simulation is pivotal in evaluating the performance of autonomous driving\nsystems due to the advantages of high efficiency and low cost compared to\non-road testing. Bridging the gap between simulation and the real world\nrequires realistic agent behaviors. However, the existing works have the\nfollowing shortcomings in achieving this goal: (1) log replay offers realistic\nscenarios but often leads to collisions due to the absence of dynamic\ninteractions, and (2) both heuristic-based and data-based solutions, which are\nparameterized and trained on real-world datasets, encourage interactions but\noften deviate from real-world data over long horizons. In this work, we propose\nLitSim, a long-term interactive simulation approach that maximizes realism by\nminimizing the interventions in the log. Specifically, our approach primarily\nuses log replay to ensure realism and intervenes only when necessary to prevent\npotential conflicts. We then encourage interactions among the agents and\nresolve the conflicts, thereby reducing the risk of unrealistic behaviors. We\ntrain and validate our model on the real-world dataset NGSIM, and the\nexperimental results demonstrate that LitSim outperforms the currently popular\napproaches in terms of realism and reactivity.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "9 pages, 6 figures, under review",
    "pdf_url": "http://arxiv.org/pdf/2403.04299v2",
    "published_date": "2024-03-07 07:58:58 UTC",
    "updated_date": "2024-05-01 05:29:30 UTC"
  },
  {
    "arxiv_id": "2403.04293v2",
    "title": "MKF-ADS: Multi-Knowledge Fusion Based Self-supervised Anomaly Detection System for Control Area Network",
    "authors": [
      "Pengzhou Cheng",
      "Zongru Wu",
      "Gongshen Liu"
    ],
    "abstract": "Control Area Network (CAN) is an essential communication protocol that\ninteracts between Electronic Control Units (ECUs) in the vehicular network.\nHowever, CAN is facing stringent security challenges due to innate security\nrisks. Intrusion detection systems (IDSs) are a crucial safety component in\nremediating Vehicular Electronics and Systems vulnerabilities. However,\nexisting IDSs fail to identify complexity attacks and have higher false alarms\nowing to capability bottleneck. In this paper, we propose a self-supervised\nmulti-knowledge fused anomaly detection model, called MKF-ADS. Specifically,\nthe method designs an integration framework, including spatial-temporal\ncorrelation with an attention mechanism (STcAM) module and patch\nsparse-transformer module (PatchST). The STcAM with fine-pruning uses\none-dimensional convolution (Conv1D) to extract spatial features and\nsubsequently utilizes the Bidirectional Long Short Term Memory (Bi-LSTM) to\nextract the temporal features, where the attention mechanism will focus on the\nimportant time steps. Meanwhile, the PatchST captures the combined contextual\nfeatures from independent univariate time series. Finally, the proposed method\nis based on knowledge distillation to STcAM as a student model for learning\nintrinsic knowledge and cross the ability to mimic PatchST. We conduct\nextensive experiments on six simulation attack scenarios across various CAN IDs\nand time steps, and two real attack scenarios, which present a competitive\nprediction and detection performance. Compared with the baseline in the same\nparadigm, the error rate and FAR are 2.62\\% and 2.41\\% and achieve a promising\nF1-score of 97.3\\%.",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "comment": "14 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2403.04293v2",
    "published_date": "2024-03-07 07:40:53 UTC",
    "updated_date": "2024-03-15 03:57:44 UTC"
  },
  {
    "arxiv_id": "2403.04292v1",
    "title": "A challenge in A(G)I, cybernetics revived in the Ouroboros Model as one algorithm for all thinking",
    "authors": [
      "Knud Thomsen"
    ],
    "abstract": "A topical challenge for algorithms in general and for automatic image\ncategorization and generation in particular is presented in the form of a\ndrawing for AI to understand. In a second vein, AI is challenged to produce\nsomething similar from verbal description. The aim of the paper is to highlight\nstrengths and deficiencies of current Artificial Intelligence approaches while\ncoarsely sketching a way forward. A general lack of encompassing\nsymbol-embedding and (not only) -grounding in some bodily basis is made\nresponsible for current deficiencies. A concomitant dearth of hierarchical\norganization of concepts follows suite. As a remedy for these shortcomings, it\nis proposed to take a wide step back and to newly incorporate aspects of\ncybernetics and analog control processes. It is claimed that a promising\noverarching perspective is provided by the Ouroboros Model with a valid and\nversatile algorithmic backbone for general cognition at all accessible levels\nof abstraction and capabilities. Reality, rules, truth, and Free Will are all\nuseful abstractions according to the Ouroboros Model. Logic deduction as well\nas intuitive guesses are claimed as produced on the basis of one\ncompartmentalized memory for schemata and a pattern-matching, i.e., monitoring\nprocess termed consumption analysis. The latter directs attention on short\n(attention proper) and also on long times scales (emotional biases). In this\ncybernetic approach, discrepancies between expectations and actual activations\n(e.g., sensory precepts) drive the general process of cognition and at the same\ntime steer the storage of new and adapted memory entries. Dedicated structures\nin the human brain work in concert according to this scheme.",
    "categories": [
      "cs.AI",
      "93-08",
      "I.2.0; I.2.4"
    ],
    "primary_category": "cs.AI",
    "comment": "26 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.04292v1",
    "published_date": "2024-03-07 07:39:54 UTC",
    "updated_date": "2024-03-07 07:39:54 UTC"
  },
  {
    "arxiv_id": "2403.04283v1",
    "title": "Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model with Proxy",
    "authors": [
      "Yu Zhu",
      "Chuxiong Sun",
      "Wenfei Yang",
      "Wenqiang Wei",
      "Bo Tang",
      "Tianzhu Zhang",
      "Zhiyu Li",
      "Shifeng Zhang",
      "Feiyu Xiong",
      "Jie Hu",
      "Mingchuan yang"
    ],
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) is the prevailing approach\nto ensure Large Language Models (LLMs) align with human values. However,\nexisting RLHF methods require a high computational cost, one main reason being\nthat RLHF assigns both the generation and alignment tasks to the LLM\nsimultaneously. In this paper, we introduce Proxy-RLHF, which decouples the\ngeneration and alignment processes of LLMs, achieving alignment with human\nvalues at a much lower computational cost. We start with a novel Markov\nDecision Process (MDP) designed for the alignment process and employ\nReinforcement Learning (RL) to train a streamlined proxy model that oversees\nthe token generation of the LLM, without altering the LLM itself. Experiments\nshow that our method achieves a comparable level of alignment with only 1\\% of\nthe training parameters of other methods.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04283v1",
    "published_date": "2024-03-07 07:31:00 UTC",
    "updated_date": "2024-03-07 07:31:00 UTC"
  },
  {
    "arxiv_id": "2403.04280v2",
    "title": "A New Benchmark for Evaluating Automatic Speech Recognition in the Arabic Call Domain",
    "authors": [
      "Qusai Abo Obaidah",
      "Muhy Eddin Za'ter",
      "Adnan Jaljuli",
      "Ali Mahboub",
      "Asma Hakouz",
      "Bashar Al-Rfooh",
      "Yazan Estaitia"
    ],
    "abstract": "This work is an attempt to introduce a comprehensive benchmark for Arabic\nspeech recognition, specifically tailored to address the challenges of\ntelephone conversations in Arabic language. Arabic, characterized by its rich\ndialectal diversity and phonetic complexity, presents a number of unique\nchallenges for automatic speech recognition (ASR) systems. These challenges are\nfurther amplified in the domain of telephone calls, where audio quality,\nbackground noise, and conversational speech styles negatively affect\nrecognition accuracy. Our work aims to establish a robust benchmark that not\nonly encompasses the broad spectrum of Arabic dialects but also emulates the\nreal-world conditions of call-based communications. By incorporating diverse\ndialectical expressions and accounting for the variable quality of call\nrecordings, this benchmark seeks to provide a rigorous testing ground for the\ndevelopment and evaluation of ASR systems capable of navigating the\ncomplexities of Arabic speech in telephonic contexts. This work also attempts\nto establish a baseline performance evaluation using state-of-the-art ASR\ntechnologies.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04280v2",
    "published_date": "2024-03-07 07:24:32 UTC",
    "updated_date": "2024-05-30 12:17:51 UTC"
  },
  {
    "arxiv_id": "2403.04264v2",
    "title": "Competitive Facility Location under Random Utilities and Routing Constraints",
    "authors": [
      "Hoang Giang Pham",
      "Tien Thanh Dam",
      "Ngan Ha Duong",
      "Tien Mai",
      "Minh Hoang Ha"
    ],
    "abstract": "In this paper, we study a facility location problem within a competitive\nmarket context, where customer demand is predicted by a random utility choice\nmodel. Unlike prior research, which primarily focuses on simple constraints\nsuch as a cardinality constraint on the number of selected locations, we\nintroduce routing constraints that necessitate the selection of locations in a\nmanner that guarantees the existence of a tour visiting all chosen locations\nwhile adhering to a specified tour length upper bound. Such routing constraints\nfind crucial applications in various real-world scenarios. The problem at hand\nfeatures a non-linear objective function, resulting from the utilization of\nrandom utilities, together with complex routing constraints, making it\ncomputationally challenging. To tackle this problem, we explore three types of\nvalid cuts, namely, outer-approximation and submodular cuts to handle the\nnonlinear objective function, as well as sub-tour elimination cuts to address\nthe complex routing constraints. These lead to the development of two exact\nsolution methods: a nested cutting plane and nested branch-and-cut algorithms,\nwhere these valid cuts are iteratively added to a master problem through two\nnested loops. We also prove that our nested cutting plane method always\nconverges to optimality after a finite number of iterations. Furthermore, we\ndevelop a local search-based metaheuristic tailored for solving large-scale\ninstances and show its pros and cons compared to exact methods. Extensive\nexperiments are conducted on problem instances of varying sizes, demonstrating\nthat our approach excels in terms of solution quality and computation time when\ncompared to other baseline approaches.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04264v2",
    "published_date": "2024-03-07 06:56:24 UTC",
    "updated_date": "2024-03-09 20:17:25 UTC"
  },
  {
    "arxiv_id": "2403.04261v2",
    "title": "Advancing Chinese biomedical text mining with community challenges",
    "authors": [
      "Hui Zong",
      "Rongrong Wu",
      "Jiaxue Cha",
      "Weizhe Feng",
      "Erman Wu",
      "Jiakun Li",
      "Aibin Shao",
      "Liang Tao",
      "Zuofeng Li",
      "Buzhou Tang",
      "Bairong Shen"
    ],
    "abstract": "Objective: This study aims to review the recent advances in community\nchallenges for biomedical text mining in China. Methods: We collected\ninformation of evaluation tasks released in community challenges of biomedical\ntext mining, including task description, dataset description, data source, task\ntype and related links. A systematic summary and comparative analysis were\nconducted on various biomedical natural language processing tasks, such as\nnamed entity recognition, entity normalization, attribute extraction, relation\nextraction, event extraction, text classification, text similarity, knowledge\ngraph construction, question answering, text generation, and large language\nmodel evaluation. Results: We identified 39 evaluation tasks from 6 community\nchallenges that spanned from 2017 to 2023. Our analysis revealed the diverse\nrange of evaluation task types and data sources in biomedical text mining. We\nexplored the potential clinical applications of these community challenge tasks\nfrom a translational biomedical informatics perspective. We compared with their\nEnglish counterparts, and discussed the contributions, limitations, lessons and\nguidelines of these community challenges, while highlighting future directions\nin the era of large language models. Conclusion: Community challenge evaluation\ncompetitions have played a crucial role in promoting technology innovation and\nfostering interdisciplinary collaboration in the field of biomedical text\nmining. These challenges provide valuable platforms for researchers to develop\nstate-of-the-art solutions.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04261v2",
    "published_date": "2024-03-07 06:52:51 UTC",
    "updated_date": "2024-08-30 02:47:43 UTC"
  },
  {
    "arxiv_id": "2403.04256v1",
    "title": "Federated Recommendation via Hybrid Retrieval Augmented Generation",
    "authors": [
      "Huimin Zeng",
      "Zhenrui Yue",
      "Qian Jiang",
      "Dong Wang"
    ],
    "abstract": "Federated Recommendation (FR) emerges as a novel paradigm that enables\nprivacy-preserving recommendations. However, traditional FR systems usually\nrepresent users/items with discrete identities (IDs), suffering from\nperformance degradation due to the data sparsity and heterogeneity in FR. On\nthe other hand, Large Language Models (LLMs) as recommenders have proven\neffective across various recommendation scenarios. Yet, LLM-based recommenders\nencounter challenges such as low inference efficiency and potential\nhallucination, compromising their performance in real-world scenarios. To this\nend, we propose GPT-FedRec, a federated recommendation framework leveraging\nChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism.\nGPT-FedRec is a two-stage solution. The first stage is a hybrid retrieval\nprocess, mining ID-based user patterns and text-based item features. Next, the\nretrieved results are converted into text prompts and fed into GPT for\nre-ranking. Our proposed hybrid retrieval mechanism and LLM-based re-rank aims\nto extract generalized features from data and exploit pretrained knowledge\nwithin LLM, overcoming data sparsity and heterogeneity in FR. In addition, the\nRAG approach also prevents LLM hallucination, improving the recommendation\nperformance for real-world users. Experimental results on diverse benchmark\ndatasets demonstrate the superior performance of GPT-FedRec against\nstate-of-the-art baseline methods.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04256v1",
    "published_date": "2024-03-07 06:38:41 UTC",
    "updated_date": "2024-03-07 06:38:41 UTC"
  },
  {
    "arxiv_id": "2403.04246v1",
    "title": "Efficient CNN-LSTM based Parameter Estimation of Levy Driven Stochastic Differential Equations",
    "authors": [
      "Shuaiyu Li",
      "Yang Ruan",
      "Changzhou Long",
      "Yuzhong Cheng"
    ],
    "abstract": "This study addresses the challenges in parameter estimation of stochastic\ndifferential equations driven by non-Gaussian noises, which are critical in\nunderstanding dynamic phenomena such as price fluctuations and the spread of\ninfectious diseases. Previous research highlighted the potential of LSTM\nnetworks in estimating parameters of alpha stable Levy driven SDEs but faced\nlimitations including high time complexity and constraints of the LSTM chaining\nproperty. To mitigate these issues, we introduce the PEnet, a novel\nCNN-LSTM-based three-stage model that offers an end to end approach with\nsuperior accuracy and adaptability to varying data structures, enhanced\ninference speed for long sequence observations through initial data feature\ncondensation by CNN, and high generalization capability, allowing its\napplication to various complex SDE scenarios. Experiments on synthetic datasets\nconfirm PEnet significant advantage in estimating SDE parameters associated\nwith noise characteristics, establishing it as a competitive method for SDE\nparameter estimation in the presence of Levy noise.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "2023 International Conference on Machine Learning and Applications\n  (ICMLA)",
    "pdf_url": "http://arxiv.org/pdf/2403.04246v1",
    "published_date": "2024-03-07 06:07:31 UTC",
    "updated_date": "2024-03-07 06:07:31 UTC"
  },
  {
    "arxiv_id": "2403.04233v3",
    "title": "TEGEE: Task dEfinition Guided Expert Ensembling for Generalizable and Few-shot Learning",
    "authors": [
      "Xingwei Qu",
      "Yiming Liang",
      "Yucheng Wang",
      "Tianyu Zheng",
      "Tommy Yue",
      "Xingyuan Bu",
      "Lei Ma",
      "Stephen W. Huang",
      "Jiajun Zhang",
      "Yinan Shi",
      "Chenghua Lin",
      "Jie Fu",
      "Ge Zhang"
    ],
    "abstract": "Large Language Models (LLMs) exhibit the ability to perform in-context\nlearning (ICL), where they acquire new tasks directly from examples provided in\ndemonstrations. This process is thought to operate through an implicit task\nselection mechanism that involves extracting and processing task definitions\nfrom these demonstrations. However, critical questions remain: Which is more\nessential -- task extraction or definition? And how can these capabilities be\nfurther improved? To address these questions, we propose \\textbf{TEGEE} (Task\nDefinition Guided Expert Ensembling), a method that explicitly extracts task\ndefinitions and generates responses based on specific tasks. Our framework\nemploys a dual 3B model approach, with each model assigned a distinct role: one\nfocuses on task definition extraction, while the other handles learning from\ndemonstrations. This modular approach supports the hypothesis that extracting\ntask definitions is more vital than processing the task itself. Empirical\nevaluations show that TEGEE performs comparably to the larger LLaMA2-13B model.\nBy leveraging a modular design, our approach extends traditional ICL from\nfew-shot to many-shot learning, supporting an unlimited number of\ndemonstrations and enhancing continual learning capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04233v3",
    "published_date": "2024-03-07 05:26:41 UTC",
    "updated_date": "2024-12-14 14:39:57 UTC"
  },
  {
    "arxiv_id": "2403.04232v1",
    "title": "Generalizing Cooperative Eco-driving via Multi-residual Task Learning",
    "authors": [
      "Vindula Jayawardana",
      "Sirui Li",
      "Cathy Wu",
      "Yashar Farid",
      "Kentaro Oguchi"
    ],
    "abstract": "Conventional control, such as model-based control, is commonly utilized in\nautonomous driving due to its efficiency and reliability. However, real-world\nautonomous driving contends with a multitude of diverse traffic scenarios that\nare challenging for these planning algorithms. Model-free Deep Reinforcement\nLearning (DRL) presents a promising avenue in this direction, but learning DRL\ncontrol policies that generalize to multiple traffic scenarios is still a\nchallenge. To address this, we introduce Multi-residual Task Learning (MRTL), a\ngeneric learning framework based on multi-task learning that, for a set of task\nscenarios, decomposes the control into nominal components that are effectively\nsolved by conventional control methods and residual terms which are solved\nusing learning. We employ MRTL for fleet-level emission reduction in mixed\ntraffic using autonomous vehicles as a means of system control. By analyzing\nthe performance of MRTL across nearly 600 signalized intersections and 1200\ntraffic scenarios, we demonstrate that it emerges as a promising approach to\nsynergize the strengths of DRL and conventional methods in generalizable\ncontrol.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted for publication at ICRA 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.04232v1",
    "published_date": "2024-03-07 05:25:34 UTC",
    "updated_date": "2024-03-07 05:25:34 UTC"
  },
  {
    "arxiv_id": "2403.04814v3",
    "title": "Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks",
    "authors": [
      "Linyuan Gong",
      "Sida Wang",
      "Mostafa Elhoushi",
      "Alvin Cheung"
    ],
    "abstract": "We introduce Syntax-Aware Fill-In-the-Middle (SAFIM), a new benchmark for\nevaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM)\ntask. This benchmark focuses on syntax-aware completions of program structures\nsuch as code blocks and conditional expressions, and includes 17,720 examples\nfrom multiple programming languages, sourced from recent code submissions after\nApril 2022 to minimize data contamination. SAFIM provides a robust framework\nwith various prompt designs and novel syntax-aware post-processing techniques,\nfacilitating accurate and fair comparisons across LLMs. Our comprehensive\nevaluation of 15 LLMs shows that FIM pretraining not only enhances FIM\nproficiency but also improves Left-to-Right (L2R) inference using LLMs. Our\nfindings challenge conventional beliefs and suggest that pretraining methods\nand data quality have more impact than model size. SAFIM thus serves as a\nfoundational platform for future research in effective pretraining strategies\nfor code LLMs. The evaluation toolkit and dataset are available at\nhttps://github.com/gonglinyuan/safim, and the leaderboard is available at\nhttps://safimbenchmark.com.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages; ICML 2024 Oral: https://icml.cc/virtual/2024/oral/35482",
    "pdf_url": "http://arxiv.org/pdf/2403.04814v3",
    "published_date": "2024-03-07 05:05:56 UTC",
    "updated_date": "2024-06-23 01:17:48 UTC"
  },
  {
    "arxiv_id": "2403.04224v4",
    "title": "Aligners: Decoupling LLMs and Alignment",
    "authors": [
      "Lilian Ngweta",
      "Mayank Agarwal",
      "Subha Maity",
      "Alex Gittens",
      "Yuekai Sun",
      "Mikhail Yurochkin"
    ],
    "abstract": "Large Language Models (LLMs) need to be aligned with human expectations to\nensure their safety and utility in most applications. Alignment is challenging,\ncostly, and needs to be repeated for every LLM and alignment criterion. We\npropose to decouple LLMs and alignment by training aligner models that can be\nused to align any LLM for a given criteria on an as-needed basis, thus also\nreducing the potential negative impacts of alignment on performance. Our recipe\nfor training the aligner models solely relies on synthetic data generated with\na (prompted) LLM and can be easily adjusted for a variety of alignment\ncriteria. We use the same synthetic data to train inspectors, binary\nmiss-alignment classification models to guide a \"squad\" of multiple aligners.\nOur empirical results demonstrate consistent improvements when applying aligner\nsquad to various LLMs, including chat-aligned models, across several\ninstruction-following and red-teaming datasets.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Short version accepted as a Tiny Paper at the International\n  Conference on Learning Representations (ICLR) 2024. Long version accepted to\n  the Conference on Empirical Methods in Natural Language Processing (EMNLP)\n  2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2403.04224v4",
    "published_date": "2024-03-07 04:54:56 UTC",
    "updated_date": "2024-10-04 05:29:18 UTC"
  },
  {
    "arxiv_id": "2403.04221v2",
    "title": "Why Online Reinforcement Learning is Causal",
    "authors": [
      "Oliver Schulte",
      "Pascal Poupart"
    ],
    "abstract": "Reinforcement learning (RL) and causal modelling naturally complement each\nother. The goal of causal modelling is to predict the effects of interventions\nin an environment, while the goal of reinforcement learning is to select\ninterventions that maximize the rewards the agent receives from the\nenvironment. Reinforcement learning includes the two most powerful sources of\ninformation for estimating causal relationships: temporal ordering and the\nability to act on an environment. This paper examines which reinforcement\nlearning settings we can expect to benefit from causal modelling, and how. In\nonline learning, the agent has the ability to interact directly with their\nenvironment, and learn from exploring it. Our main argument is that in online\nlearning, conditional probabilities are causal, and therefore offline RL is the\nsetting where causal learning has the most potential to make a difference.\nEssentially, the reason is that when an agent learns from their {\\em own}\nexperience, there are no unobserved confounders that influence both the agent's\nown exploratory actions and the rewards they receive. Our paper formalizes this\nargument. For offline RL, where an agent may and typically does learn from the\nexperience of {\\em others}, we describe previous and new methods for leveraging\na causal model, including support for counterfactual queries.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "43 pages. Version 2 discusses policy evaluation for partially\n  observable MDPs based on a causal model",
    "pdf_url": "http://arxiv.org/pdf/2403.04221v2",
    "published_date": "2024-03-07 04:49:48 UTC",
    "updated_date": "2024-07-10 23:51:52 UTC"
  },
  {
    "arxiv_id": "2403.04204v1",
    "title": "On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models",
    "authors": [
      "Xinpeng Wang",
      "Shitong Duan",
      "Xiaoyuan Yi",
      "Jing Yao",
      "Shanlin Zhou",
      "Zhihua Wei",
      "Peng Zhang",
      "Dongkuan Xu",
      "Maosong Sun",
      "Xing Xie"
    ],
    "abstract": "Big models have achieved revolutionary breakthroughs in the field of AI, but\nthey might also pose potential concerns. Addressing such concerns, alignment\ntechnologies were introduced to make these models conform to human preferences\nand values. Despite considerable advancements in the past year, various\nchallenges lie in establishing the optimal alignment strategy, such as data\ncost and scalable oversight, and how to align remains an open question. In this\nsurvey paper, we comprehensively investigate value alignment approaches. We\nfirst unpack the historical context of alignment tracing back to the 1920s\n(where it comes from), then delve into the mathematical essence of alignment\n(what it is), shedding light on the inherent challenges. Following this\nfoundation, we provide a detailed examination of existing alignment methods,\nwhich fall into three categories: Reinforcement Learning, Supervised\nFine-Tuning, and In-context Learning, and demonstrate their intrinsic\nconnections, strengths, and limitations, helping readers better understand this\nresearch area. In addition, two emerging topics, personal alignment, and\nmultimodal alignment, are also discussed as novel frontiers in this field.\nLooking forward, we discuss potential alignment paradigms and how they could\nhandle remaining challenges, prospecting where future alignment will go.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "23 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.04204v1",
    "published_date": "2024-03-07 04:19:13 UTC",
    "updated_date": "2024-03-07 04:19:13 UTC"
  },
  {
    "arxiv_id": "2403.04202v7",
    "title": "Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents",
    "authors": [
      "Elizaveta Tennant",
      "Stephen Hailes",
      "Mirco Musolesi"
    ],
    "abstract": "Growing concerns about safety and alignment of AI systems highlight the\nimportance of embedding moral capabilities in artificial agents: a promising\nsolution is the use of learning from experience, i.e., Reinforcement Learning.\nIn multi-agent (social) environments, complex population-level phenomena may\nemerge from interactions between individual learning agents. Many of the\nexisting studies rely on simulated social dilemma environments to study the\ninteractions of independent learning agents; however, they tend to ignore the\nmoral heterogeneity that is likely to be present in societies of agents in\npractice. For example, at different points in time a single learning agent may\nface opponents who are consequentialist (i.e., focused on maximizing outcomes\nover time), norm-based (i.e., conforming to specific norms), or virtue-based\n(i.e., considering a combination of different virtues). The extent to which\nagents' co-development may be impacted by such moral heterogeneity in\npopulations is not well understood. In this paper, we present a study of the\nlearning dynamics of morally heterogeneous populations interacting in a social\ndilemma setting. Using an Iterated Prisoner's Dilemma environment with a\npartner selection mechanism, we investigate the extent to which the prevalence\nof diverse moral agents in populations affects individual agents' learning\nbehaviors and emergent population-level outcomes. We observe several types of\nnon-trivial interactions between pro-social and anti-social agents, and find\nthat certain types of moral agents are able to steer selfish agents towards\nmore cooperative behavior.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "Presented at AIES 2024 (7th AAAI/ACM Conference on AI, Ethics, and\n  Society - San Jose, CA, USA) - see\n  https://ojs.aaai.org/index.php/AIES/article/view/31736",
    "pdf_url": "http://arxiv.org/pdf/2403.04202v7",
    "published_date": "2024-03-07 04:12:24 UTC",
    "updated_date": "2025-01-16 17:28:26 UTC"
  },
  {
    "arxiv_id": "2403.04197v4",
    "title": "Large Language Models are In-Context Molecule Learners",
    "authors": [
      "Jiatong Li",
      "Wei Liu",
      "Zhihao Ding",
      "Wenqi Fan",
      "Yuqiang Li",
      "Qing Li"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated exceptional performance in\nbiochemical tasks, especially the molecule caption translation task, which aims\nto bridge the gap between molecules and natural language texts. However,\nprevious methods in adapting LLMs to the molecule-caption translation task\nrequired extra domain-specific pre-training stages, suffered weak alignment\nbetween molecular and textual spaces, or imposed stringent demands on the scale\nof LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation\n(ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment\nfrom context examples via In-Context Molecule Tuning. Specifically, ICMA\nincorporates the following three stages: Hybrid Context Retrieval,\nPost-retrieval Re-ranking, and In-context Molecule Tuning. Initially, Hybrid\nContext Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval\nto retrieve similar informative context examples. Additionally, Post-retrieval\nRe-ranking is composed of Sequence Reversal and Random Walk selection to\nfurther improve the quality of retrieval results. Finally, In-Context Molecule\nTuning unlocks the in-context learning and reasoning capability of LLMs with\nthe retrieved examples and adapts the parameters of LLMs for better alignment\nbetween molecules and texts. Experimental results demonstrate that ICMA can\nempower LLMs to achieve state-of-the-art or comparable performance without\nextra training corpora and intricate structures, showing that LLMs are\ninherently in-context molecule learners.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by IEEE TKDE",
    "pdf_url": "http://arxiv.org/pdf/2403.04197v4",
    "published_date": "2024-03-07 03:58:28 UTC",
    "updated_date": "2025-04-07 07:46:51 UTC"
  },
  {
    "arxiv_id": "2403.04190v1",
    "title": "Generative AI for Synthetic Data Generation: Methods, Challenges and the Future",
    "authors": [
      "Xu Guo",
      "Yiqiang Chen"
    ],
    "abstract": "The recent surge in research focused on generating synthetic data from large\nlanguage models (LLMs), especially for scenarios with limited data\navailability, marks a notable shift in Generative Artificial Intelligence (AI).\nTheir ability to perform comparably to real-world data positions this approach\nas a compelling solution to low-resource challenges. This paper delves into\nadvanced technologies that leverage these gigantic LLMs for the generation of\ntask-specific training data. We outline methodologies, evaluation techniques,\nand practical applications, discuss the current limitations, and suggest\npotential pathways for future research.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "I.2.0"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04190v1",
    "published_date": "2024-03-07 03:38:44 UTC",
    "updated_date": "2024-03-07 03:38:44 UTC"
  },
  {
    "arxiv_id": "2403.04187v1",
    "title": "Preference optimization of protein language models as a multi-objective binder design paradigm",
    "authors": [
      "Pouria Mistani",
      "Venkatesh Mysore"
    ],
    "abstract": "We present a multi-objective binder design paradigm based on instruction\nfine-tuning and direct preference optimization (DPO) of autoregressive protein\nlanguage models (pLMs). Multiple design objectives are encoded in the language\nmodel through direct optimization on expert curated preference sequence\ndatasets comprising preferred and dispreferred distributions. We show the\nproposed alignment strategy enables ProtGPT2 to effectively design binders\nconditioned on specified receptors and a drug developability criterion.\nGenerated binder samples demonstrate median isoelectric point (pI) improvements\nby $17\\%-60\\%$.",
    "categories": [
      "physics.bio-ph",
      "cs.AI",
      "cs.CE",
      "q-bio.BM"
    ],
    "primary_category": "physics.bio-ph",
    "comment": "Published at the GEM workshop, ICLR 2024. Generative and Experimental\n  Perspectives for Biomolecular Design (https://www.gembio.ai/)",
    "pdf_url": "http://arxiv.org/pdf/2403.04187v1",
    "published_date": "2024-03-07 03:36:03 UTC",
    "updated_date": "2024-03-07 03:36:03 UTC"
  },
  {
    "arxiv_id": "2403.04182v3",
    "title": "Regression-aware Inference with LLMs",
    "authors": [
      "Michal Lukasik",
      "Harikrishna Narasimhan",
      "Aditya Krishna Menon",
      "Felix Yu",
      "Sanjiv Kumar"
    ],
    "abstract": "Large language models (LLMs) have shown strong results on a range of\napplications, including regression and scoring tasks. Typically, one obtains\noutputs from an LLM via autoregressive sampling from the model's output\ndistribution. We show that this inference strategy can be sub-optimal for\ncommon regression and scoring evaluation metrics. As a remedy, we build on\nprior work on Minimum Bayes Risk decoding, and propose alternate inference\nstrategies that estimate the Bayes-optimal solution for regression and scoring\nmetrics in closed-form from sampled responses. We show that our proposal\nsignificantly improves over baselines across datasets and models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP Findings 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.04182v3",
    "published_date": "2024-03-07 03:24:34 UTC",
    "updated_date": "2024-11-01 17:57:01 UTC"
  },
  {
    "arxiv_id": "2403.04175v1",
    "title": "Understanding the PULSAR Effect in Combined Radiotherapy and Immunotherapy through Attention Mechanisms with a Transformer Model",
    "authors": [
      "Hao Peng",
      "Casey Moore",
      "Debabrata Saha",
      "Steve Jiang",
      "Robert Timmerman"
    ],
    "abstract": "PULSAR (personalized, ultra-fractionated stereotactic adaptive radiotherapy)\nis the adaptation of stereotactic ablative radiotherapy towards personalized\ncancer management. For the first time, we applied a transformer-based attention\nmechanism to investigate the underlying interactions between combined PULSAR\nand PD-L1 blockade immunotherapy based on a murine cancer model (Lewis Lung\nCarcinoma, LLC). The proposed approach is able to predict the trend of tumor\nvolume change semi-quantitatively, and excels in identifying the potential\ncausal relationships through both self-attention and cross-attention scores.",
    "categories": [
      "physics.med-ph",
      "cs.AI"
    ],
    "primary_category": "physics.med-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04175v1",
    "published_date": "2024-03-07 03:12:31 UTC",
    "updated_date": "2024-03-07 03:12:31 UTC"
  },
  {
    "arxiv_id": "2403.04164v3",
    "title": "ProMISe: Promptable Medical Image Segmentation using SAM",
    "authors": [
      "Jinfeng Wang",
      "Sifan Song",
      "Xinkun Wang",
      "Yiyi Wang",
      "Yiyi Miao",
      "Jionglong Su",
      "S. Kevin Zhou"
    ],
    "abstract": "With the proposal of the Segment Anything Model (SAM), fine-tuning SAM for\nmedical image segmentation (MIS) has become popular. However, due to the large\nsize of the SAM model and the significant domain gap between natural and\nmedical images, fine-tuning-based strategies are costly with potential risk of\ninstability, feature damage and catastrophic forgetting. Furthermore, some\nmethods of transferring SAM to a domain-specific MIS through fine-tuning\nstrategies disable the model's prompting capability, severely limiting its\nutilization scenarios. In this paper, we propose an Auto-Prompting Module\n(APM), which provides SAM-based foundation model with Euclidean adaptive\nprompts in the target domain. Our experiments demonstrate that such adaptive\nprompts significantly improve SAM's non-fine-tuned performance in MIS. In\naddition, we propose a novel non-invasive method called Incremental Pattern\nShifting (IPS) to adapt SAM to specific medical domains. Experimental results\nshow that the IPS enables SAM to achieve state-of-the-art or competitive\nperformance in MIS without the need for fine-tuning. By coupling these two\nmethods, we propose ProMISe, an end-to-end non-fine-tuned framework for\nPromptable Medical Image Segmentation. Our experiments demonstrate that both\nusing our methods individually or in combination achieves satisfactory\nperformance in low-cost pattern shifting, with all of SAM's parameters frozen.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04164v3",
    "published_date": "2024-03-07 02:48:42 UTC",
    "updated_date": "2024-09-28 12:59:54 UTC"
  },
  {
    "arxiv_id": "2403.04160v1",
    "title": "Improving Retrieval in Theme-specific Applications using a Corpus Topical Taxonomy",
    "authors": [
      "SeongKu Kang",
      "Shivam Agarwal",
      "Bowen Jin",
      "Dongha Lee",
      "Hwanjo Yu",
      "Jiawei Han"
    ],
    "abstract": "Document retrieval has greatly benefited from the advancements of large-scale\npre-trained language models (PLMs). However, their effectiveness is often\nlimited in theme-specific applications for specialized areas or industries, due\nto unique terminologies, incomplete contexts of user queries, and specialized\nsearch intents. To capture the theme-specific information and improve\nretrieval, we propose to use a corpus topical taxonomy, which outlines the\nlatent topic structure of the corpus while reflecting user-interested aspects.\nWe introduce ToTER (Topical Taxonomy Enhanced Retrieval) framework, which\nidentifies the central topics of queries and documents with the guidance of the\ntaxonomy, and exploits their topical relatedness to supplement missing\ncontexts. As a plug-and-play framework, ToTER can be flexibly employed to\nenhance various PLM-based retrievers. Through extensive quantitative, ablative,\nand exploratory experiments on two real-world datasets, we ascertain the\nbenefits of using topical taxonomy for retrieval in theme-specific applications\nand demonstrate the effectiveness of ToTER.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "TheWebConf'24",
    "pdf_url": "http://arxiv.org/pdf/2403.04160v1",
    "published_date": "2024-03-07 02:34:54 UTC",
    "updated_date": "2024-03-07 02:34:54 UTC"
  },
  {
    "arxiv_id": "2403.04158v1",
    "title": "DA-Net: A Disentangled and Adaptive Network for Multi-Source Cross-Lingual Transfer Learning",
    "authors": [
      "Ling Ge",
      "Chunming Hu",
      "Guanghui Ma",
      "Jihong Liu",
      "Hong Zhang"
    ],
    "abstract": "Multi-Source cross-lingual transfer learning deals with the transfer of task\nknowledge from multiple labelled source languages to an unlabeled target\nlanguage under the language shift. Existing methods typically focus on\nweighting the predictions produced by language-specific classifiers of\ndifferent sources that follow a shared encoder. However, all source languages\nshare the same encoder, which is updated by all these languages. The extracted\nrepresentations inevitably contain different source languages' information,\nwhich may disturb the learning of the language-specific classifiers.\nAdditionally, due to the language gap, language-specific classifiers trained\nwith source labels are unable to make accurate predictions for the target\nlanguage. Both facts impair the model's performance. To address these\nchallenges, we propose a Disentangled and Adaptive Network (DA-Net). Firstly,\nwe devise a feedback-guided collaborative disentanglement method that seeks to\npurify input representations of classifiers, thereby mitigating mutual\ninterference from multiple sources. Secondly, we propose a class-aware parallel\nadaptation method that aligns class-level distributions for each source-target\nlanguage pair, thereby alleviating the language pairs' language gap.\nExperimental results on three different tasks involving 38 languages validate\nthe effectiveness of our approach.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "AAAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.04158v1",
    "published_date": "2024-03-07 02:30:46 UTC",
    "updated_date": "2024-03-07 02:30:46 UTC"
  },
  {
    "arxiv_id": "2403.04146v1",
    "title": "FL-GUARD: A Holistic Framework for Run-Time Detection and Recovery of Negative Federated Learning",
    "authors": [
      "Hong Lin",
      "Lidan Shou",
      "Ke Chen",
      "Gang Chen",
      "Sai Wu"
    ],
    "abstract": "Federated learning (FL) is a promising approach for learning a model from\ndata distributed on massive clients without exposing data privacy. It works\neffectively in the ideal federation where clients share homogeneous data\ndistribution and learning behavior. However, FL may fail to function\nappropriately when the federation is not ideal, amid an unhealthy state called\nNegative Federated Learning (NFL), in which most clients gain no benefit from\nparticipating in FL. Many studies have tried to address NFL. However, their\nsolutions either (1) predetermine to prevent NFL in the entire learning\nlife-cycle or (2) tackle NFL in the aftermath of numerous learning rounds.\nThus, they either (1) indiscriminately incur extra costs even if FL can perform\nwell without such costs or (2) waste numerous learning rounds. Additionally,\nnone of the previous work takes into account the clients who may be\nunwilling/unable to follow the proposed NFL solutions when using those\nsolutions to upgrade an FL system in use. This paper introduces FL-GUARD, a\nholistic framework that can be employed on any FL system for tackling NFL in a\nrun-time paradigm. That is, to dynamically detect NFL at the early stage (tens\nof rounds) of learning and then to activate recovery measures when necessary.\nSpecifically, we devise a cost-effective NFL detection mechanism, which relies\non an estimation of performance gain on clients. Only when NFL is detected, we\nactivate the NFL recovery process, in which each client learns in parallel an\nadapted model when training the global model. Extensive experiment results\nconfirm the effectiveness of FL-GUARD in detecting NFL and recovering from NFL\nto a healthy learning state. We also show that FL-GUARD is compatible with\nprevious NFL solutions and robust against clients unwilling/unable to take any\nrecovery measures.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04146v1",
    "published_date": "2024-03-07 01:52:05 UTC",
    "updated_date": "2024-03-07 01:52:05 UTC"
  },
  {
    "arxiv_id": "2403.04140v1",
    "title": "Contrastive Augmented Graph2Graph Memory Interaction for Few Shot Continual Learning",
    "authors": [
      "Biqing Qi",
      "Junqi Gao",
      "Xingquan Chen",
      "Dong Li",
      "Jianxing Liu",
      "Ligang Wu",
      "Bowen Zhou"
    ],
    "abstract": "Few-Shot Class-Incremental Learning (FSCIL) has gained considerable attention\nin recent years for its pivotal role in addressing continuously arriving\nclasses. However, it encounters additional challenges. The scarcity of samples\nin new sessions intensifies overfitting, causing incompatibility between the\noutput features of new and old classes, thereby escalating catastrophic\nforgetting. A prevalent strategy involves mitigating catastrophic forgetting\nthrough the Explicit Memory (EM), which comprise of class prototypes. However,\ncurrent EM-based methods retrieves memory globally by performing\nVector-to-Vector (V2V) interaction between features corresponding to the input\nand prototypes stored in EM, neglecting the geometric structure of local\nfeatures. This hinders the accurate modeling of their positional relationships.\nTo incorporate information of local geometric structure, we extend the V2V\ninteraction to Graph-to-Graph (G2G) interaction. For enhancing local structures\nfor better G2G alignment and the prevention of local feature collapse, we\npropose the Local Graph Preservation (LGP) mechanism. Additionally, to address\nsample scarcity in classes from new sessions, the Contrast-Augmented G2G\n(CAG2G) is introduced to promote the aggregation of same class features thus\nhelps few-shot learning. Extensive comparisons on CIFAR100, CUB200, and the\nchallenging ImageNet-R dataset demonstrate the superiority of our method over\nexisting methods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "12 Pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.04140v1",
    "published_date": "2024-03-07 01:41:12 UTC",
    "updated_date": "2024-03-07 01:41:12 UTC"
  },
  {
    "arxiv_id": "2403.04135v1",
    "title": "Unsupervised Learning of Harmonic Analysis Based on Neural HSMM with Code Quality Templates",
    "authors": [
      "Yui Uehara"
    ],
    "abstract": "This paper presents a method of unsupervised learning of harmonic analysis\nbased on a hidden semi-Markov model (HSMM). We introduce the chord quality\ntemplates, which specify the probability of pitch class emissions given a root\nnote and a chord quality. Other probability distributions that comprise the\nHSMM are automatically learned via unsupervised learning, which has been a\nchallenge in existing research. The results of the harmonic analysis of the\nproposed model were evaluated using existing labeled data. While our proposed\nmethod has yet to perform as well as existing models that used supervised\nlearning and complex rule design, it has the advantage of not requiring\nexpensive labeled data or rule elaboration. Furthermore, we also show how to\nrecognize the tonic without prior knowledge, based on the transition\nprobabilities of the Markov model.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages, 5 figures, the original edition of this paper will be\n  published in the ICNMC2024 Proceedings and this arXiv publication is a copy",
    "pdf_url": "http://arxiv.org/pdf/2403.04135v1",
    "published_date": "2024-03-07 01:29:48 UTC",
    "updated_date": "2024-03-07 01:29:48 UTC"
  },
  {
    "arxiv_id": "2403.04132v1",
    "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
    "authors": [
      "Wei-Lin Chiang",
      "Lianmin Zheng",
      "Ying Sheng",
      "Anastasios Nikolas Angelopoulos",
      "Tianle Li",
      "Dacheng Li",
      "Hao Zhang",
      "Banghua Zhu",
      "Michael Jordan",
      "Joseph E. Gonzalez",
      "Ion Stoica"
    ],
    "abstract": "Large Language Models (LLMs) have unlocked new capabilities and applications;\nhowever, evaluating the alignment with human preferences still poses\nsignificant challenges. To address this issue, we introduce Chatbot Arena, an\nopen platform for evaluating LLMs based on human preferences. Our methodology\nemploys a pairwise comparison approach and leverages input from a diverse user\nbase through crowdsourcing. The platform has been operational for several\nmonths, amassing over 240K votes. This paper describes the platform, analyzes\nthe data we have collected so far, and explains the tried-and-true statistical\nmethods we are using for efficient and accurate evaluation and ranking of\nmodels. We confirm that the crowdsourced questions are sufficiently diverse and\ndiscriminating and that the crowdsourced human votes are in good agreement with\nthose of expert raters. These analyses collectively establish a robust\nfoundation for the credibility of Chatbot Arena. Because of its unique value\nand openness, Chatbot Arena has emerged as one of the most referenced LLM\nleaderboards, widely cited by leading LLM developers and companies. Our demo is\npublicly available at \\url{https://chat.lmsys.org}.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04132v1",
    "published_date": "2024-03-07 01:22:38 UTC",
    "updated_date": "2024-03-07 01:22:38 UTC"
  },
  {
    "arxiv_id": "2403.04124v1",
    "title": "Privacy-preserving Fine-tuning of Large Language Models through Flatness",
    "authors": [
      "Tiejin Chen",
      "Longchao Da",
      "Huixue Zhou",
      "Pingzhi Li",
      "Kaixiong Zhou",
      "Tianlong Chen",
      "Hua Wei"
    ],
    "abstract": "The privacy concerns associated with the use of Large Language Models (LLMs)\nhave grown recently with the development of LLMs such as ChatGPT. Differential\nPrivacy (DP) techniques are explored in existing work to mitigate their privacy\nrisks at the cost of generalization degradation. Our paper reveals that the\nflatness of DP-trained models' loss landscape plays an essential role in the\ntrade-off between their privacy and generalization. We further propose a\nholistic framework to enforce appropriate weight flatness, which substantially\nimproves model generalization with competitive privacy preservation. It\ninnovates from three coarse-to-grained levels, including perturbation-aware\nmin-max optimization on model weights within a layer, flatness-guided sparse\nprefix-tuning on weights across layers, and weight knowledge distillation\nbetween DP \\& non-DP weights copies. Comprehensive experiments of both\nblack-box and white-box scenarios are conducted to demonstrate the\neffectiveness of our proposal in enhancing generalization and maintaining DP\ncharacteristics. For instance, on text classification dataset QNLI, DP-Flat\nachieves similar performance with non-private full fine-tuning but with DP\nguarantee under privacy budget $\\epsilon=3$, and even better performance given\nhigher privacy budgets. Codes are provided in the supplement.",
    "categories": [
      "cs.AI",
      "I.2"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to ICLR 2024 SeT LLM Workshop",
    "pdf_url": "http://arxiv.org/pdf/2403.04124v1",
    "published_date": "2024-03-07 00:44:11 UTC",
    "updated_date": "2024-03-07 00:44:11 UTC"
  },
  {
    "arxiv_id": "2403.04121v2",
    "title": "Can Large Language Models Reason and Plan?",
    "authors": [
      "Subbarao Kambhampati"
    ],
    "abstract": "While humans sometimes do show the capability of correcting their own\nerroneous guesses with self-critiquing, there seems to be no basis for that\nassumption in the case of LLMs.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "arXiv admin note: text overlap with arXiv:2402.01817 (v2 add creative\n  commons attribution to Figure 2 graphic)",
    "pdf_url": "http://arxiv.org/pdf/2403.04121v2",
    "published_date": "2024-03-07 00:36:32 UTC",
    "updated_date": "2024-03-08 19:51:14 UTC"
  },
  {
    "arxiv_id": "2403.04115v2",
    "title": "DNAct: Diffusion Guided Multi-Task 3D Policy Learning",
    "authors": [
      "Ge Yan",
      "Yueh-Hua Wu",
      "Xiaolong Wang"
    ],
    "abstract": "This paper presents DNAct, a language-conditioned multi-task policy framework\nthat integrates neural rendering pre-training and diffusion training to enforce\nmulti-modality learning in action sequence spaces. To learn a generalizable\nmulti-task policy with few demonstrations, the pre-training phase of DNAct\nleverages neural rendering to distill 2D semantic features from foundation\nmodels such as Stable Diffusion to a 3D space, which provides a comprehensive\nsemantic understanding regarding the scene. Consequently, it allows various\napplications to challenging robotic tasks requiring rich 3D semantics and\naccurate geometry. Furthermore, we introduce a novel approach utilizing\ndiffusion training to learn a vision and language feature that encapsulates the\ninherent multi-modality in the multi-task demonstrations. By reconstructing the\naction sequences from different tasks via the diffusion process, the model is\ncapable of distinguishing different modalities and thus improving the\nrobustness and the generalizability of the learned representation. DNAct\nsignificantly surpasses SOTA NeRF-based multi-task manipulation approaches with\nover 30% improvement in success rate. Project website: dnact.github.io.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04115v2",
    "published_date": "2024-03-07 00:09:07 UTC",
    "updated_date": "2024-03-08 09:56:47 UTC"
  }
]
{
  "date": "2024-03-07",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-03-07 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文主要聚焦于 AI 模型的优化、偏置处理、生成任务和特定领域应用，突出 LLMs 在医学、图推理和多模态任务中的潜力，同时有 Yoshua Bengio 等知名学者参与的理论探讨，令人印象深刻的包括 LLMs 的偏置分析和高效生成模型。\n\n### LLMs 和 AI 安全/偏置相关论文\n这些论文探讨了大型语言模型（LLMs）的偏置问题、优化策略和应用潜力，是本日高话题度焦点。\n- **Proxy-RLHF: Decoupling Generation and Alignment in Large Language Models（Proxy-RLHF: 分离生成和对齐的大语言模型）**（#81）：提出一种分离生成和对齐的框架，通过代理模型优化 LLMs 的对齐过程，实现高效的人类偏好强化学习，显著降低计算成本，同时保持性能。\n- **Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference（Chatbot Arena: 基于人类偏好的 LLMs 评估开放平台）**（#96）：开发了一个开源平台，使用人类偏好投票评估 LLMs，收集超过24万数据点，提供高效的模型排名基准，揭示 LLMs 在实际应用中的鲁棒性。\n- **A Survey on Human-AI Teaming with Large Pre-Trained Models（关于大型预训练模型的人机协作调查）**（#5）：由多位学者（如 Vincent Titterton）合著，调查 LLMs 在人机协作中的潜力，强调伦理和应用影响，提供全面框架。\n- **ConstitutionalExperts: Training a Mixture of Principle-based Prompts（ConstitutionalExperts: 基于原则提示的混合训练）**（#10）：引入一种基于原则的提示混合训练方法，提升 LLMs 在多任务上的鲁棒性，通过实验证明其在基准数据集上提升10.9%的 F1 分数。\n- **Fooling Neural Networks for Motion Forecasting via Adversarial Attacks（通过对抗攻击欺骗运动预测神经网络）**（#2）：揭示运动预测模型（如 GCNs）对小扰动和3D变换的脆弱性，实验显示模型性能显著下降，强调安全评估的重要性。\n\n### 图神经网络和机器学习优化\n这些论文关注图结构数据的处理和模型泛化，提升了 AI 在复杂场景中的应用。\n- **On the Markov Property of Neural Algorithmic Reasoning: Analyses and Methods（神经算法推理的 Markov 属性：分析与方法）**（#6）：提出 ForgetNet 和 G-ForgetNet 框架，符合算法推理的 Markov 性质，通过 CLRS-30 基准实验提升泛化能力。\n- **GNN-VPA: A Variance-Preserving Aggregation Strategy for Graph Neural Networks（GNN-VPA: 图神经网络的方差保持聚合策略）**（#16）：引入方差保持聚合函数，提高 GNN 的预测性能和学习动态，在 ICLR 2024 上被接受。\n- **A Survey of Graph Neural Networks in Real world: Imbalance, Noise, Privacy and OOD Challenges（图神经网络在现实世界的调查：不平衡、噪声、隐私和 OOD 挑战）**（#48）：全面总结 GNN 在现实挑战下的解决方案，涵盖不平衡数据和隐私保护，强调未来研究方向。\n\n### 生成模型和计算机视觉应用\n这些论文扩展了生成模型在视觉和多模态任务中的能力，相关但不优先。\n- **A Spatiotemporal Style Transfer Algorithm for Dynamic Visual Stimulus Generation（用于动态视觉刺激生成的空间-时间风格转移算法）**（#3）：开发 STST 算法，使用双流神经网络合成视频刺激，应用于视觉研究和对象识别。\n- **LeTac-MPC: Learning Model Predictive Control for Tactile-reactive Grasping（LeTac-MPC: 用于触觉反应抓取的学习模型预测控制）**（#4）：提出结合 GelSight 传感器和可微 MPC 的抓取框架，实现25Hz的鲁棒控制，并开源数据集。\n- **Identifying Causal Effects Under Functional Dependencies（在函数依赖下识别因果效应）**（#7）：探索函数依赖对因果效应的影响，提供消除函数变量的方法，保持因果图的标识性。\n\n其他论文如生物医学图像生成（#29）、强化学习（#30）和多任务优化（#62）等，虽然有贡献，但相对次要，这里快速掠过：它们主要解决特定领域问题，如#29使用扩散模型生成心超图像，#30优化机器人策略，但整体影响力不如上述主题。\n\n总之，今天的论文突显 AI 模型的优化和应用潜力，LLMs 的偏置与协作主题尤其值得关注，未来可能推动更可靠的人机交互。更多细节可查阅 arXiv。",
  "papers": [
    {
      "arxiv_id": "2403.04957v1",
      "title": "Automatic and Universal Prompt Injection Attacks against Large Language Models",
      "title_zh": "针对大型语言模型的自动和通用提示注入攻击",
      "authors": [
        "Xiaogeng Liu",
        "Zhiyuan Yu",
        "Yizhe Zhang",
        "Ning Zhang",
        "Chaowei Xiao"
      ],
      "abstract": "Large Language Models (LLMs) excel in processing and generating human\nlanguage, powered by their ability to interpret and follow instructions.\nHowever, their capabilities can be exploited through prompt injection attacks.\nThese attacks manipulate LLM-integrated applications into producing responses\naligned with the attacker's injected content, deviating from the user's actual\nrequests. The substantial risks posed by these attacks underscore the need for\na thorough understanding of the threats. Yet, research in this area faces\nchallenges due to the lack of a unified goal for such attacks and their\nreliance on manually crafted prompts, complicating comprehensive assessments of\nprompt injection robustness. We introduce a unified framework for understanding\nthe objectives of prompt injection attacks and present an automated\ngradient-based method for generating highly effective and universal prompt\ninjection data, even in the face of defensive measures. With only five training\nsamples (0.3% relative to the test data), our attack can achieve superior\nperformance compared with baselines. Our findings emphasize the importance of\ngradient-based testing, which can avoid overestimation of robustness,\nespecially for defense mechanisms.",
      "tldr_zh": "该论文探讨了针对Large Language Models (LLMs)的prompt injection attacks，这些攻击通过操纵输入提示使LLMs生成偏离用户请求的响应，带来重大安全风险。研究者提出一个统一的框架来定义攻击目标，并开发了一种自动的gradient-based方法，用于生成高效且通用的提示注入数据，即使在防御机制存在时也能有效。实验显示，该方法仅需5个训练样本（相对测试数据的0.3%）就比基线攻击表现更优，并强调gradient-based测试的重要性，以避免高估防御的鲁棒性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Pre-print, code is available at\n  https://github.com/SheltonLiu-N/Universal-Prompt-Injection",
      "pdf_url": "http://arxiv.org/pdf/2403.04957v1",
      "published_date": "2024-03-07 23:46:20 UTC",
      "updated_date": "2024-03-07 23:46:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:13:47.824045"
    },
    {
      "arxiv_id": "2403.04954v2",
      "title": "Fooling Neural Networks for Motion Forecasting via Adversarial Attacks",
      "title_zh": "翻译失败",
      "authors": [
        "Edgar Medina",
        "Leyong Loh"
      ],
      "abstract": "Human motion prediction is still an open problem, which is extremely\nimportant for autonomous driving and safety applications. Although there are\ngreat advances in this area, the widely studied topic of adversarial attacks\nhas not been applied to multi-regression models such as GCNs and MLP-based\narchitectures in human motion prediction. This work intends to reduce this gap\nusing extensive quantitative and qualitative experiments in state-of-the-art\narchitectures similar to the initial stages of adversarial attacks in image\nclassification. The results suggest that models are susceptible to attacks even\non low levels of perturbation. We also show experiments with 3D transformations\nthat affect the model performance, in particular, we show that most models are\nsensitive to simple rotations and translations which do not alter joint\ndistances. We conclude that similar to earlier CNN models, motion forecasting\ntasks are susceptible to small perturbations and simple 3D transformations.",
      "tldr_zh": "这篇论文探讨了通过对抗攻击（adversarial attacks）来欺骗用于人类动作预测的神经网络模型，强调了其在自动驾驶和安全应用中的潜在风险。研究者对多回归模型如 GCNs 和 MLP-based 架构进行了广泛的定量和定性实验，结果显示这些模型即使在低水平扰动下也容易出错。实验还包括 3D transformations（如旋转和平移），这些变换不改变关节距离但会显著影响模型性能。最终结论表明，动作预测任务与早期的 CNN 模型类似，对小扰动和简单变换高度敏感。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 8 figures, VISSAP 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.04954v2",
      "published_date": "2024-03-07 23:44:10 UTC",
      "updated_date": "2024-03-11 09:37:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:14:00.479712"
    },
    {
      "arxiv_id": "2403.04940v1",
      "title": "A spatiotemporal style transfer algorithm for dynamic visual stimulus generation",
      "title_zh": "用于动态视觉刺激生成的时空风格转移算法",
      "authors": [
        "Antonino Greco",
        "Markus Siegel"
      ],
      "abstract": "Understanding how visual information is encoded in biological and artificial\nsystems often requires vision scientists to generate appropriate stimuli to\ntest specific hypotheses. Although deep neural network models have\nrevolutionized the field of image generation with methods such as image style\ntransfer, available methods for video generation are scarce. Here, we introduce\nthe Spatiotemporal Style Transfer (STST) algorithm, a dynamic visual stimulus\ngeneration framework that allows powerful manipulation and synthesis of video\nstimuli for vision research. It is based on a two-stream deep neural network\nmodel that factorizes spatial and temporal features to generate dynamic visual\nstimuli whose model layer activations are matched to those of input videos. As\nan example, we show that our algorithm enables the generation of model\nmetamers, dynamic stimuli whose layer activations within our two-stream model\nare matched to those of natural videos. We show that these generated stimuli\nmatch the low-level spatiotemporal features of their natural counterparts but\nlack their high-level semantic features, making it a powerful paradigm to study\nobject recognition. Late layer activations in deep vision models exhibited a\nlower similarity between natural and metameric stimuli compared to early\nlayers, confirming the lack of high-level information in the generated stimuli.\nFinally, we use our generated stimuli to probe the representational\ncapabilities of predictive coding deep networks. These results showcase\npotential applications of our algorithm as a versatile tool for dynamic\nstimulus generation in vision science.",
      "tldr_zh": "本文提出 Spatiotemporal Style Transfer (STST) 算法，这是一个动态视觉刺激生成框架，旨在帮助视觉科学家测试假设。该算法基于两流深神经网络模型，将空间和时间特征分离，从而生成视频刺激，使其模型层激活与输入视频一致。作为示例，STST 能创建模型 metamer，这些刺激匹配自然视频的低级时空特征但缺少高级语义特征，用于研究物体识别。实验结果显示，深层激活中自然视频与 metamer 的相似性较低，确认了生成刺激的高级信息缺失；此外，该算法可作为探测预测编码深网络表示能力的通用工具。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "q-bio.NC"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04940v1",
      "published_date": "2024-03-07 23:07:46 UTC",
      "updated_date": "2024-03-07 23:07:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:14:12.846495"
    },
    {
      "arxiv_id": "2403.04934v2",
      "title": "LeTac-MPC: Learning Model Predictive Control for Tactile-reactive Grasping",
      "title_zh": "翻译失败",
      "authors": [
        "Zhengtong Xu",
        "Yu She"
      ],
      "abstract": "Grasping is a crucial task in robotics, necessitating tactile feedback and\nreactive grasping adjustments for robust grasping of objects under various\nconditions and with differing physical properties. In this paper, we introduce\nLeTac-MPC, a learning-based model predictive control (MPC) for tactile-reactive\ngrasping. Our approach enables the gripper to grasp objects with different\nphysical properties on dynamic and force-interactive tasks. We utilize a\nvision-based tactile sensor, GelSight, which is capable of perceiving\nhigh-resolution tactile feedback that contains information on the physical\nproperties and states of the grasped object. LeTac-MPC incorporates a\ndifferentiable MPC layer designed to model the embeddings extracted by a neural\nnetwork (NN) from tactile feedback. This design facilitates convergent and\nrobust grasping control at a frequency of 25 Hz. We propose a fully automated\ndata collection pipeline and collect a dataset only using standardized blocks\nwith different physical properties. However, our trained controller can\ngeneralize to daily objects with different sizes, shapes, materials, and\ntextures. The experimental results demonstrate the effectiveness and robustness\nof the proposed approach. We compare LeTac-MPC with two purely model-based\ntactile-reactive controllers (MPC and PD) and open-loop grasping. Our results\nshow that LeTac-MPC has optimal performance in dynamic and force-interactive\ntasks and optimal generalizability. We release our code and dataset at\nhttps://github.com/ZhengtongXu/LeTac-MPC.",
      "tldr_zh": "本研究提出LeTac-MPC，一种基于学习的模型预测控制(MPC)框架，用于触觉反应抓取，旨在处理不同物理属性的物体在动态和力交互任务中的鲁棒抓取。LeTac-MPC利用GelSight视觉触觉传感器获取高分辨率反馈，并整合可微分MPC层来处理神经网络提取的嵌入，实现25 Hz的稳定控制。该框架通过一个全自动数据收集管道，仅使用标准化块训练数据集，却能泛化到各种日常物体的大小、形状、材料和纹理。实验结果显示，LeTac-MPC在动态任务中比纯模型-based控制器(MPC和PD)以及开环抓取方法表现出色，具有最佳性能和泛化能力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04934v2",
      "published_date": "2024-03-07 22:42:24 UTC",
      "updated_date": "2024-09-07 20:57:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:14:24.810872"
    },
    {
      "arxiv_id": "2403.04931v2",
      "title": "A Survey on Human-AI Teaming with Large Pre-Trained Models",
      "title_zh": "翻译失败",
      "authors": [
        "Vanshika Vats",
        "Marzia Binta Nizam",
        "Minghao Liu",
        "Ziyuan Wang",
        "Richard Ho",
        "Mohnish Sai Prasad",
        "Vincent Titterton",
        "Sai Venkat Malreddy",
        "Riya Aggarwal",
        "Yanwen Xu",
        "Lei Ding",
        "Jay Mehta",
        "Nathan Grinnell",
        "Li Liu",
        "Sijia Zhong",
        "Devanathan Nallur Gandamani",
        "Xinyi Tang",
        "Rohan Ghosalkar",
        "Celeste Shen",
        "Rachel Shen",
        "Nafisa Hussain",
        "Kesav Ravichandran",
        "James Davis"
      ],
      "abstract": "In the rapidly evolving landscape of artificial intelligence (AI), the\ncollaboration between human intelligence and AI systems, known as Human-AI\n(HAI) Teaming, has emerged as a cornerstone for advancing problem-solving and\ndecision-making processes. The advent of Large Pre-trained Models (LPtM) has\nsignificantly transformed this landscape, offering unprecedented capabilities\nby leveraging vast amounts of data to understand and predict complex patterns.\nThis paper surveys the pivotal integration of LPtMs with HAI, emphasizing how\nthese models enhance collaborative intelligence beyond traditional approaches.\nIt examines the potential of LPtMs in augmenting human capabilities, discussing\nthis collaboration for AI model improvements, effective teaming, ethical\nconsiderations, and their broad applied implications in various sectors.\nThrough this exploration, the study sheds light on the transformative impact of\nLPtM-enhanced HAI Teaming, providing insights for future research, policy\ndevelopment, and strategic implementations aimed at harnessing the full\npotential of this collaboration for research and societal benefit.",
      "tldr_zh": "这篇论文调查了 Large Pre-Trained Models (LPtMs) 在 Human-AI Teaming 中的整合，强调这些模型如何通过利用海量数据提升人类与 AI 的协作智能，超越传统方法。论文探讨了 LPtMs 在增强人类能力、改进 AI 模型、实现有效团队协作以及处理伦理考虑方面的潜力，并扩展到医疗、教育和商业等行业的应用。通过这一分析，提供宝贵的见解，支持未来研究、政策制定和战略实施，以最大化 Human-AI Teaming 的社会效益。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04931v2",
      "published_date": "2024-03-07 22:37:49 UTC",
      "updated_date": "2024-06-26 23:44:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:14:37.028693"
    },
    {
      "arxiv_id": "2403.04929v1",
      "title": "On the Markov Property of Neural Algorithmic Reasoning: Analyses and Methods",
      "title_zh": "翻译失败",
      "authors": [
        "Montgomery Bohde",
        "Meng Liu",
        "Alexandra Saxton",
        "Shuiwang Ji"
      ],
      "abstract": "Neural algorithmic reasoning is an emerging research direction that endows\nneural networks with the ability to mimic algorithmic executions step-by-step.\nA common paradigm in existing designs involves the use of historical embeddings\nin predicting the results of future execution steps. Our observation in this\nwork is that such historical dependence intrinsically contradicts the Markov\nnature of algorithmic reasoning tasks. Based on this motivation, we present our\nForgetNet, which does not use historical embeddings and thus is consistent with\nthe Markov nature of the tasks. To address challenges in training ForgetNet at\nearly stages, we further introduce G-ForgetNet, which uses a gating mechanism\nto allow for the selective integration of historical embeddings. Such an\nenhanced capability provides valuable computational pathways during the model's\nearly training phase. Our extensive experiments, based on the CLRS-30\nalgorithmic reasoning benchmark, demonstrate that both ForgetNet and\nG-ForgetNet achieve better generalization capability than existing methods.\nFurthermore, we investigate the behavior of the gating mechanism, highlighting\nits degree of alignment with our intuitions and its effectiveness for robust\nperformance.",
      "tldr_zh": "本研究分析了神经算法推理（Neural Algorithmic Reasoning）的Markov性质问题，指出现有方法依赖历史嵌入（historical embeddings）预测未来步骤，这与算法任务的Markov性质相矛盾。作者提出ForgetNet框架，该框架不使用历史嵌入，从而更符合Markov属性，并引入G-ForgetNet，通过门控机制（gating mechanism）在模型早期训练阶段选择性地整合历史嵌入，以提升训练稳定性。在CLRS-30基准上的广泛实验表明，ForgetNet和G-ForgetNet比现有方法具有更好的泛化能力。最后，研究还探讨了门控机制的行为，验证了其有效性和与直觉的一致性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "To appear at ICLR 2024 (Spotlight paper). 17 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.04929v1",
      "published_date": "2024-03-07 22:35:22 UTC",
      "updated_date": "2024-03-07 22:35:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:14:48.600122"
    },
    {
      "arxiv_id": "2403.04919v2",
      "title": "Identifying Causal Effects Under Functional Dependencies",
      "title_zh": "在函数依赖下识别因果效应",
      "authors": [
        "Yizuo Chen",
        "Adnan Darwiche"
      ],
      "abstract": "We study the identification of causal effects, motivated by two improvements\nto identifiability which can be attained if one knows that some variables in a\ncausal graph are functionally determined by their parents (without needing to\nknow the specific functions). First, an unidentifiable causal effect may become\nidentifiable when certain variables are functional. Second, certain functional\nvariables can be excluded from being observed without affecting the\nidentifiability of a causal effect, which may significantly reduce the number\nof needed variables in observational data. Our results are largely based on an\nelimination procedure which removes functional variables from a causal graph\nwhile preserving key properties in the resulting causal graph, including the\nidentifiability of causal effects.",
      "tldr_zh": "这篇论文探讨了在功能依赖(functional dependencies)下识别因果效应(identifying causal effects)的改进，重点关注因果图(causal graph)中某些变量被其父变量功能性决定时的场景。主要贡献包括：原本不可识别的因果效应可能通过功能依赖变得可识别，以及可以排除某些功能变量的观测，而不影响因果效应的可识别性，从而减少所需观测数据。通过一个消除程序，该方法从因果图中移除功能变量，同时保留关键属性，如因果效应的可识别性。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.SC",
        "stat.ME"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04919v2",
      "published_date": "2024-03-07 22:04:35 UTC",
      "updated_date": "2024-05-22 21:43:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:14:59.581841"
    },
    {
      "arxiv_id": "2403.04917v3",
      "title": "A Mixed-Integer Conic Program for the Moving-Target Traveling Salesman Problem based on a Graph of Convex Sets",
      "title_zh": "基于凸集图的移动目标旅行商问题的混合整数锥规划",
      "authors": [
        "Allen George Philip",
        "Zhongqiang Ren",
        "Sivakumar Rathinam",
        "Howie Choset"
      ],
      "abstract": "This paper introduces a new formulation that finds the optimum for the\nMoving-Target Traveling Salesman Problem (MT-TSP), which seeks to find a\nshortest path for an agent, that starts at a depot, visits a set of moving\ntargets exactly once within their assigned time-windows, and returns to the\ndepot. The formulation relies on the key idea that when the targets move along\nlines, their trajectories become convex sets within the space-time coordinate\nsystem. The problem then reduces to finding the shortest path within a graph of\nconvex sets, subject to some speed constraints. We compare our formulation with\nthe current state-of-the-art Mixed Integer Conic Program (MICP) solver for the\nMT-TSP. The experimental results show that our formulation outperforms the MICP\nfor instances with up to 20 targets, with up to two orders of magnitude\nreduction in runtime, and up to a 60\\% tighter optimality gap. We also show\nthat the solution cost from the convex relaxation of our formulation provides\nsignificantly tighter lower bounds for the MT-TSP than the ones from the MICP.",
      "tldr_zh": "本论文提出了一种新的混合整数锥程序(Mixed-Integer Conic Program)公式化，用于求解移动目标旅行商问题(Moving-Target Traveling Salesman Problem, MT-TSP)，该问题涉及一个代理从仓库出发，在指定时间窗口内访问一组移动目标并返回。方法的核心在于，当目标沿直线移动时，其轨迹在时空坐标系中形成凸集，从而将问题简化为在Graph of Convex Sets中寻找最短路径，同时考虑速度约束。实验结果显示，与现有状态的最优MICP相比，该公式化在最多20个目标的实例上，运行时间减少多达两个数量级，最优性差距缩小多达60%，并提供更紧的下界。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.DS"
      ],
      "primary_category": "cs.RO",
      "comment": "7 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.04917v3",
      "published_date": "2024-03-07 22:03:36 UTC",
      "updated_date": "2025-01-13 20:28:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:15:12.613963"
    },
    {
      "arxiv_id": "2403.04899v2",
      "title": "Towards Scene Graph Anticipation",
      "title_zh": "翻译失败",
      "authors": [
        "Rohith Peddi",
        "Saksham Singh",
        "Saurabh",
        "Parag Singla",
        "Vibhav Gogate"
      ],
      "abstract": "Spatio-temporal scene graphs represent interactions in a video by decomposing\nscenes into individual objects and their pair-wise temporal relationships.\nLong-term anticipation of the fine-grained pair-wise relationships between\nobjects is a challenging problem. To this end, we introduce the task of Scene\nGraph Anticipation (SGA). We adapt state-of-the-art scene graph generation\nmethods as baselines to anticipate future pair-wise relationships between\nobjects and propose a novel approach SceneSayer. In SceneSayer, we leverage\nobject-centric representations of relationships to reason about the observed\nvideo frames and model the evolution of relationships between objects. We take\na continuous time perspective and model the latent dynamics of the evolution of\nobject interactions using concepts of NeuralODE and NeuralSDE, respectively. We\ninfer representations of future relationships by solving an Ordinary\nDifferential Equation and a Stochastic Differential Equation, respectively.\nExtensive experimentation on the Action Genome dataset validates the efficacy\nof the proposed methods.",
      "tldr_zh": "本研究引入了 Scene Graph Anticipation (SGA) 任务，旨在预测视频中物体之间细粒度的未来成对关系，通过分解场景为物体及其时空交互。论文提出了一种新方法 SceneSayer，利用物体中心表示来推理观察到的视频帧，并采用 NeuralODE 和 NeuralSDE 概念建模物体交互的潜在动态，从而通过求解 Ordinary Differential Equation (ODE) 和 Stochastic Differential Equation (SDE) 来推断未来关系。实验在 Action Genome 数据集上验证了该方法的有效性，展示了其在长时预测方面的优越性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ECCV 2024, Code: https://github.com/rohithpeddi/SceneSayer",
      "pdf_url": "http://arxiv.org/pdf/2403.04899v2",
      "published_date": "2024-03-07 21:08:51 UTC",
      "updated_date": "2024-07-19 12:40:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:15:25.237930"
    },
    {
      "arxiv_id": "2403.04894v1",
      "title": "ConstitutionalExperts: Training a Mixture of Principle-based Prompts",
      "title_zh": "翻译失败",
      "authors": [
        "Savvas Petridis",
        "Ben Wedin",
        "Ann Yuan",
        "James Wexler",
        "Nithum Thain"
      ],
      "abstract": "Large language models (LLMs) are highly capable at a variety of tasks given\nthe right prompt, but writing one is still a difficult and tedious process. In\nthis work, we introduce ConstitutionalExperts, a method for learning a prompt\nconsisting of constitutional principles (i.e. rules), given a training dataset.\nUnlike prior methods that optimize the prompt as a single entity, our method\nincrementally improves the prompt by surgically editing individual principles.\nWe also show that we can improve overall performance by learning unique prompts\nfor different semantic regions of the training data and using a\nmixture-of-experts (MoE) architecture to route inputs at inference time. We\ncompare our method to other state of the art prompt-optimization techniques\nacross six benchmark datasets. We also investigate whether MoE improves these\nother techniques. Our results suggest that ConstitutionalExperts outperforms\nother prompt optimization techniques by 10.9% (F1) and that mixture-of-experts\nimproves all techniques, suggesting its broad applicability.",
      "tldr_zh": "本文提出 ConstitutionalExperts，一种用于优化大型语言模型(LLMs)提示的方法，通过基于训练数据集学习由宪法原则(rules)组成的提示，并通过增量式编辑单个原则来逐步改进。不同于整体优化，该方法还结合 Mixture-of-Experts (MoE) 架构，为训练数据的不同语义区域学习独特提示，并在推理时路由输入，以提升整体性能。在六个基准数据集的实验中，ConstitutionalExperts 比其他最先进提示优化技术提高了 10.9% 的 F1 分数，且 MoE 架构能泛化改进多种技术，展示了其广泛适用性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04894v1",
      "published_date": "2024-03-07 20:58:04 UTC",
      "updated_date": "2024-03-07 20:58:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:15:37.489790"
    },
    {
      "arxiv_id": "2403.04893v1",
      "title": "A Safe Harbor for AI Evaluation and Red Teaming",
      "title_zh": "翻译失败",
      "authors": [
        "Shayne Longpre",
        "Sayash Kapoor",
        "Kevin Klyman",
        "Ashwin Ramaswami",
        "Rishi Bommasani",
        "Borhane Blili-Hamelin",
        "Yangsibo Huang",
        "Aviya Skowron",
        "Zheng-Xin Yong",
        "Suhas Kotha",
        "Yi Zeng",
        "Weiyan Shi",
        "Xianjun Yang",
        "Reid Southen",
        "Alexander Robey",
        "Patrick Chao",
        "Diyi Yang",
        "Ruoxi Jia",
        "Daniel Kang",
        "Sandy Pentland",
        "Arvind Narayanan",
        "Percy Liang",
        "Peter Henderson"
      ],
      "abstract": "Independent evaluation and red teaming are critical for identifying the risks\nposed by generative AI systems. However, the terms of service and enforcement\nstrategies used by prominent AI companies to deter model misuse have\ndisincentives on good faith safety evaluations. This causes some researchers to\nfear that conducting such research or releasing their findings will result in\naccount suspensions or legal reprisal. Although some companies offer researcher\naccess programs, they are an inadequate substitute for independent research\naccess, as they have limited community representation, receive inadequate\nfunding, and lack independence from corporate incentives. We propose that major\nAI developers commit to providing a legal and technical safe harbor,\nindemnifying public interest safety research and protecting it from the threat\nof account suspensions or legal reprisal. These proposals emerged from our\ncollective experience conducting safety, privacy, and trustworthiness research\non generative AI systems, where norms and incentives could be better aligned\nwith public interests, without exacerbating model misuse. We believe these\ncommitments are a necessary step towards more inclusive and unimpeded community\nefforts to tackle the risks of generative AI.",
      "tldr_zh": "该论文讨论了独立评估和 red teaming 在识别生成式 AI 系统风险中的关键作用，但当前 AI 公司的服务条款和执行策略却抑制了善意的研究，导致研究者担心账户暂停或法律追责。作者认为现有研究访问程序不足，因为它们缺乏社区代表性、资金支持和独立性。论文提出，AI 开发商应承诺建立法律和技术 safe harbor，提供免责保障和保护措施，以支持公共利益安全研究，同时避免加剧模型滥用。这种举措基于作者的经验，有助于促进更具包容性和不受阻的社区努力，应对生成式 AI 的潜在风险。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04893v1",
      "published_date": "2024-03-07 20:55:08 UTC",
      "updated_date": "2024-03-07 20:55:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:15:52.127037"
    },
    {
      "arxiv_id": "2403.04866v1",
      "title": "A Modular End-to-End Multimodal Learning Method for Structured and Unstructured Data",
      "title_zh": "一种模块化端到端多模态学习方法，用于结构化数据和非结构化数据",
      "authors": [
        "Marco D Alessandro",
        "Enrique Calabrés",
        "Mikel Elkano"
      ],
      "abstract": "Multimodal learning is a rapidly growing research field that has\nrevolutionized multitasking and generative modeling in AI. While much of the\nresearch has focused on dealing with unstructured data (e.g., language, images,\naudio, or video), structured data (e.g., tabular data, time series, or signals)\nhas received less attention. However, many industry-relevant use cases involve\nor can be benefited from both types of data. In this work, we propose a\nmodular, end-to-end multimodal learning method called MAGNUM, which can\nnatively handle both structured and unstructured data. MAGNUM is flexible\nenough to employ any specialized unimodal module to extract, compress, and fuse\ninformation from all available modalities.",
      "tldr_zh": "多模态学习是AI领域快速发展的研究方向，主要关注无结构数据（如语言、图像、音频或视频），而结构化数据（如表格数据、时间序列或信号）则较少被重视，尽管许多实际应用需要两者结合。本文提出了一种模块化、端到端的多模态学习方法MAGNUM，能够原生处理结构化和无结构化数据。MAGNUM通过灵活使用专门的单模态模块来提取、压缩和融合所有可用模态的信息，从而提升其在行业相关任务中的适用性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2403.04866v1",
      "published_date": "2024-03-07 19:29:36 UTC",
      "updated_date": "2024-03-07 19:29:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:16:02.617643"
    },
    {
      "arxiv_id": "2403.04859v2",
      "title": "Self-Supervision in Time for Satellite Images(S3-TSS): A novel method of SSL technique in Satellite images",
      "title_zh": "翻译失败",
      "authors": [
        "Akansh Maurya",
        "Hewan Shrestha",
        "Mohammad Munem Shahriar"
      ],
      "abstract": "With the limited availability of labeled data with various atmospheric\nconditions in remote sensing images, it seems useful to work with\nself-supervised algorithms. Few pretext-based algorithms, including from\nrotation, spatial context and jigsaw puzzles are not appropriate for satellite\nimages. Often, satellite images have a higher temporal frequency. So, the\ntemporal dimension of remote sensing data provides natural augmentation without\nrequiring us to create artificial augmentation of images. Here, we propose\nS3-TSS, a novel method of self-supervised learning technique that leverages\nnatural augmentation occurring in temporal dimension. We compare our results\nwith current state-of-the-art methods and also perform various experiments. We\nobserved that our method was able to perform better than baseline SeCo in four\ndownstream datasets. Code for our work can be found here:\nhttps://github.com/hewanshrestha/Why-Self-Supervision-in-Time",
      "tldr_zh": "该研究针对卫星图像中标注数据有限的问题，提出了一种新型自监督学习（Self-Supervised Learning, SSL）方法S3-TSS，利用图像的时间维度提供自然增强，避免了传统基于旋转或空间上下文的预训练算法的不适用性。S3-TSS通过利用卫星图像的高时间频率来实现有效的自监督训练，从而提升模型在下游任务中的性能。在实验中，该方法在四个下游数据集上优于基线SeCo，证明了其有效性。代码可在GitHub上获取。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04859v2",
      "published_date": "2024-03-07 19:16:17 UTC",
      "updated_date": "2024-03-11 09:32:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:16:14.854293"
    },
    {
      "arxiv_id": "2403.04760v1",
      "title": "iScore: Visual Analytics for Interpreting How Language Models Automatically Score Summaries",
      "title_zh": "翻译失败",
      "authors": [
        "Adam Coscia",
        "Langdon Holmes",
        "Wesley Morris",
        "Joon Suh Choi",
        "Scott Crossley",
        "Alex Endert"
      ],
      "abstract": "The recent explosion in popularity of large language models (LLMs) has\ninspired learning engineers to incorporate them into adaptive educational tools\nthat automatically score summary writing. Understanding and evaluating LLMs is\nvital before deploying them in critical learning environments, yet their\nunprecedented size and expanding number of parameters inhibits transparency and\nimpedes trust when they underperform. Through a collaborative user-centered\ndesign process with several learning engineers building and deploying summary\nscoring LLMs, we characterized fundamental design challenges and goals around\ninterpreting their models, including aggregating large text inputs, tracking\nscore provenance, and scaling LLM interpretability methods. To address their\nconcerns, we developed iScore, an interactive visual analytics tool for\nlearning engineers to upload, score, and compare multiple summaries\nsimultaneously. Tightly integrated views allow users to iteratively revise the\nlanguage in summaries, track changes in the resulting LLM scores, and visualize\nmodel weights at multiple levels of abstraction. To validate our approach, we\ndeployed iScore with three learning engineers over the course of a month. We\npresent a case study where interacting with iScore led a learning engineer to\nimprove their LLM's score accuracy by three percentage points. Finally, we\nconducted qualitative interviews with the learning engineers that revealed how\niScore enabled them to understand, evaluate, and build trust in their LLMs\nduring deployment.",
      "tldr_zh": "该研究开发了 iScore，一种交互式可视分析工具，用于帮助学习工程师理解大型语言模型 (LLMs) 在自动评分总结写作中的表现。iScore 通过用户中心设计过程，解决了关键挑战，如聚合大文本输入、跟踪分数来源 (score provenance) 和扩展 LLM 可解释性方法，支持用户迭代修改总结、跟踪分数变化，并可视化模型权重在多个抽象级别。实验部署显示，iScore 使学习工程师提高了 LLM 的评分准确性 3%，并通过质性访谈证实了它在增强模型透明度和建立信任方面的作用。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted to IUI 2024. 16 pages, 5 figures, 1 table. For a demo video,\n  see https://youtu.be/EYJX-_fQPf0 . For a live demo, visit\n  https://adamcoscia.com/papers/iscore/demo/ . The source code is available at\n  https://github.com/AdamCoscia/iScore",
      "pdf_url": "http://arxiv.org/pdf/2403.04760v1",
      "published_date": "2024-03-07 18:56:39 UTC",
      "updated_date": "2024-03-07 18:56:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:16:27.219845"
    },
    {
      "arxiv_id": "2403.04758v1",
      "title": "KnowledgeVIS: Interpreting Language Models by Comparing Fill-in-the-Blank Prompts",
      "title_zh": "KnowledgeVIS: 通过比较填空提示解释语言模型",
      "authors": [
        "Adam Coscia",
        "Alex Endert"
      ],
      "abstract": "Recent growth in the popularity of large language models has led to their\nincreased usage for summarizing, predicting, and generating text, making it\nvital to help researchers and engineers understand how and why they work. We\npresent KnowledgeVis, a human-in-the-loop visual analytics system for\ninterpreting language models using fill-in-the-blank sentences as prompts. By\ncomparing predictions between sentences, KnowledgeVis reveals learned\nassociations that intuitively connect what language models learn during\ntraining to natural language tasks downstream, helping users create and test\nmultiple prompt variations, analyze predicted words using a novel semantic\nclustering technique, and discover insights using interactive visualizations.\nCollectively, these visualizations help users identify the likelihood and\nuniqueness of individual predictions, compare sets of predictions between\nprompts, and summarize patterns and relationships between predictions across\nall prompts. We demonstrate the capabilities of KnowledgeVis with feedback from\nsix NLP experts as well as three different use cases: (1) probing biomedical\nknowledge in two domain-adapted models; and (2) evaluating harmful identity\nstereotypes and (3) discovering facts and relationships between three\ngeneral-purpose models.",
      "tldr_zh": "本研究提出KnowledgeVIS，一种人类在循环中的视觉分析系统，用于解释Language Models，通过比较填空句（Fill-in-the-Blank Prompts）提示来揭示模型训练中学到的关联。系统允许用户创建多重提示变体、运用新型语义聚类技术分析预测词，并通过交互式可视化识别预测的可能性和独特性、比较提示间的预测集，以及总结整体模式和关系。KnowledgeVIS经六位NLP专家反馈验证，并在三个用例中展示其效能：探测领域适应模型的生物医学知识、评估有害身份刻板印象，以及发现通用模型中的事实关系。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted to IEEE TVCG. 20 pages, 10 figures, 1 table. For a demo\n  video, see https://youtu.be/hBX4rSUMr_I . For a live demo, visit\n  https://adamcoscia.com/papers/knowledgevis/demo/ . The source code is\n  available at https://github.com/AdamCoscia/KnowledgeVIS",
      "pdf_url": "http://arxiv.org/pdf/2403.04758v1",
      "published_date": "2024-03-07 18:56:31 UTC",
      "updated_date": "2024-03-07 18:56:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:16:38.948768"
    },
    {
      "arxiv_id": "2403.04747v1",
      "title": "GNN-VPA: A Variance-Preserving Aggregation Strategy for Graph Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Lisa Schneckenreiter",
        "Richard Freinschlag",
        "Florian Sestak",
        "Johannes Brandstetter",
        "Günter Klambauer",
        "Andreas Mayr"
      ],
      "abstract": "Graph neural networks (GNNs), and especially message-passing neural networks,\nexcel in various domains such as physics, drug discovery, and molecular\nmodeling. The expressivity of GNNs with respect to their ability to\ndiscriminate non-isomorphic graphs critically depends on the functions employed\nfor message aggregation and graph-level readout. By applying signal propagation\ntheory, we propose a variance-preserving aggregation function (VPA) that\nmaintains expressivity, but yields improved forward and backward dynamics.\nExperiments demonstrate that VPA leads to increased predictive performance for\npopular GNN architectures as well as improved learning dynamics. Our results\ncould pave the way towards normalizer-free or self-normalizing GNNs.",
      "tldr_zh": "该论文提出了一种方差保持聚合策略（Variance-Preserving Aggregation，VPA），旨在提升图神经网络（Graph Neural Networks，GNNs）的表达性，特别是针对消息传递神经网络在物理、药物发现和分子建模等领域的应用。作者基于信号传播理论设计了VPA函数，能够维持GNNs的表达性，同时改善前向和后向动态。实验结果显示，VPA显著提高了流行GNN架构的预测性能和学习动态，为开发无归一化或自归一化GNNs提供了新路径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ICLR 2024 (Tiny Papers Track)",
      "pdf_url": "http://arxiv.org/pdf/2403.04747v1",
      "published_date": "2024-03-07 18:52:27 UTC",
      "updated_date": "2024-03-07 18:52:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:16:51.512806"
    },
    {
      "arxiv_id": "2403.04746v1",
      "title": "LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error",
      "title_zh": "翻译失败",
      "authors": [
        "Boshi Wang",
        "Hao Fang",
        "Jason Eisner",
        "Benjamin Van Durme",
        "Yu Su"
      ],
      "abstract": "Tools are essential for large language models (LLMs) to acquire up-to-date\ninformation and take consequential actions in external environments. Existing\nwork on tool-augmented LLMs primarily focuses on the broad coverage of tools\nand the flexibility of adding new tools. However, a critical aspect that has\nsurprisingly been understudied is simply how accurately an LLM uses tools for\nwhich it has been trained. We find that existing LLMs, including GPT-4 and\nopen-source LLMs specifically fine-tuned for tool use, only reach a correctness\nrate in the range of 30% to 60%, far from reliable use in practice. We propose\na biologically inspired method for tool-augmented LLMs, simulated trial and\nerror (STE), that orchestrates three key mechanisms for successful tool use\nbehaviors in the biological system: trial and error, imagination, and memory.\nSpecifically, STE leverages an LLM's 'imagination' to simulate plausible\nscenarios for using a tool, after which the LLM interacts with the tool to\nlearn from its execution feedback. Both short-term and long-term memory are\nemployed to improve the depth and breadth of the exploration, respectively.\nComprehensive experiments on ToolBench show that STE substantially improves\ntool learning for LLMs under both in-context learning and fine-tuning settings,\nbringing a boost of 46.7% to Mistral-Instruct-7B and enabling it to outperform\nGPT-4. We also show effective continual learning of tools via a simple\nexperience replay strategy.",
      "tldr_zh": "本文研究发现，现有的工具增强大型语言模型 (LLMs)，如 GPT-4，仅在工具使用正确率上达到 30% 到 60%，远未达到可靠水平。为此，论文提出了一种受生物启发的模拟试错 (STE) 方法，该方法通过 LLMs 的“想象力”模拟工具使用场景、与工具互动获取反馈，并利用短期和长期记忆提升探索的深度和广度。实验在 ToolBench 上显示，STE 在上下文学习和微调设置下显著提升了 LLMs 的性能，例如为 Mistral-Instruct-7B 带来 46.7% 的提升，使其超越 GPT-4，并通过简单经验回放策略实现了工具的持续学习。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Code and data available at\n  https://github.com/microsoft/simulated-trial-and-error",
      "pdf_url": "http://arxiv.org/pdf/2403.04746v1",
      "published_date": "2024-03-07 18:50:51 UTC",
      "updated_date": "2024-03-07 18:50:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:17:04.553875"
    },
    {
      "arxiv_id": "2403.04732v3",
      "title": "How Far Are We from Intelligent Visual Deductive Reasoning?",
      "title_zh": "翻译失败",
      "authors": [
        "Yizhe Zhang",
        "He Bai",
        "Ruixiang Zhang",
        "Jiatao Gu",
        "Shuangfei Zhai",
        "Josh Susskind",
        "Navdeep Jaitly"
      ],
      "abstract": "Vision-Language Models (VLMs) have recently demonstrated incredible strides\non diverse vision language tasks. We dig into vision-based deductive reasoning,\na more sophisticated but less explored realm, and find previously unexposed\nblindspots in the current SOTA VLMs. Specifically, we leverage Raven's\nProgressive Matrices (RPMs), to assess VLMs' abilities to perform multi-hop\nrelational and deductive reasoning relying solely on visual clues. We perform\ncomprehensive evaluations of several popular VLMs employing standard strategies\nsuch as in-context learning, self-consistency, and Chain-of-thoughts (CoT) on\nthree diverse datasets, including the Mensa IQ test, IntelligenceTest, and\nRAVEN. The results reveal that despite the impressive capabilities of LLMs in\ntext-based reasoning, we are still far from achieving comparable proficiency in\nvisual deductive reasoning. We found that certain standard strategies that are\neffective when applied to LLMs do not seamlessly translate to the challenges\npresented by visual reasoning tasks. A detailed analysis reveals that VLMs\nstruggle to solve these tasks mainly because they are unable to perceive and\ncomprehend multiple, confounding abstract patterns in RPM examples.",
      "tldr_zh": "该研究评估了视觉语言模型（VLMs）在视觉演绎推理方面的能力，使用 Raven's Progressive Matrices (RPMs) 作为基准，测试模型在多跳关系和演绎推理上的表现，仅依赖视觉线索。研究者在三个数据集（包括 Mensa IQ test、IntelligenceTest 和 RAVEN）上，对多个流行 VLMs 应用了标准策略，如 in-context learning、self-consistency 和 Chain-of-thoughts (CoT)。结果显示，尽管大型语言模型（LLMs）在文本推理中表现出色，但 VLMs 在视觉推理任务上仍有显著差距，主要因为它们难以感知和理解 RPM 示例中的多个混杂抽象模式。这些发现突显了当前 VLMs 的局限性，并强调了开发更先进视觉推理技术的必要性。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "COLM 2024. https://github.com/apple/ml-rpm-bench",
      "pdf_url": "http://arxiv.org/pdf/2403.04732v3",
      "published_date": "2024-03-07 18:35:54 UTC",
      "updated_date": "2024-10-01 04:41:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:17:16.320027"
    },
    {
      "arxiv_id": "2403.04706v1",
      "title": "Common 7B Language Models Already Possess Strong Math Capabilities",
      "title_zh": "常见的 7B 语言模型已具备强大的数学能力",
      "authors": [
        "Chen Li",
        "Weiqi Wang",
        "Jingcheng Hu",
        "Yixuan Wei",
        "Nanning Zheng",
        "Han Hu",
        "Zheng Zhang",
        "Houwen Peng"
      ],
      "abstract": "Mathematical capabilities were previously believed to emerge in common\nlanguage models only at a very large scale or require extensive math-related\npre-training. This paper shows that the LLaMA-2 7B model with common\npre-training already exhibits strong mathematical abilities, as evidenced by\nits impressive accuracy of 97.7% and 72.0% on the GSM8K and MATH benchmarks,\nrespectively, when selecting the best response from 256 random generations. The\nprimary issue with the current base model is the difficulty in consistently\neliciting its inherent mathematical capabilities. Notably, the accuracy for the\nfirst answer drops to 49.5% and 7.9% on the GSM8K and MATH benchmarks,\nrespectively. We find that simply scaling up the SFT data can significantly\nenhance the reliability of generating correct answers. However, the potential\nfor extensive scaling is constrained by the scarcity of publicly available math\nquestions. To overcome this limitation, we employ synthetic data, which proves\nto be nearly as effective as real data and shows no clear saturation when\nscaled up to approximately one million samples. This straightforward approach\nachieves an accuracy of 82.6% on GSM8K and 40.6% on MATH using LLaMA-2 7B\nmodels, surpassing previous models by 14.2% and 20.8%, respectively. We also\nprovide insights into scaling behaviors across different reasoning complexities\nand error types.",
      "tldr_zh": "该研究发现，LLaMA-2 7B 模型在标准预训练下已具备强大数学能力，在 GSM8K 和 MATH 基准测试中，从256次随机生成中选出最佳答案时，准确率分别达到97.7%和72.0%。然而，模型的第一个答案准确率较低（分别为49.5%和7.9%），主要问题在于难以一致激发其能力，通过扩展 SFT 数据和使用合成数据来提升可靠性，后者几乎与真实数据同样有效，且在约一百万样本时未见饱和。最终，该方法使 LLaMA-2 7B 在 GSM8K 和 MATH 上分别实现82.6%和40.6%的准确率，比先前模型提高14.2%和20.8%，并提供了不同推理复杂度和错误类型的缩放行为分析。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04706v1",
      "published_date": "2024-03-07 18:00:40 UTC",
      "updated_date": "2024-03-07 18:00:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:17:28.241123"
    },
    {
      "arxiv_id": "2403.04701v4",
      "title": "ObjectCompose: Evaluating Resilience of Vision-Based Models on Object-to-Background Compositional Changes",
      "title_zh": "翻译失败",
      "authors": [
        "Hashmat Shadab Malik",
        "Muhammad Huzaifa",
        "Muzammal Naseer",
        "Salman Khan",
        "Fahad Shahbaz Khan"
      ],
      "abstract": "Given the large-scale multi-modal training of recent vision-based models and\ntheir generalization capabilities, understanding the extent of their robustness\nis critical for their real-world deployment. In this work, we evaluate the\nresilience of current vision-based models against diverse object-to-background\ncontext variations. The majority of robustness evaluation methods have\nintroduced synthetic datasets to induce changes to object characteristics\n(viewpoints, scale, color) or utilized image transformation techniques\n(adversarial changes, common corruptions) on real images to simulate shifts in\ndistributions. Recent works have explored leveraging large language models and\ndiffusion models to generate changes in the background. However, these methods\neither lack in offering control over the changes to be made or distort the\nobject semantics, making them unsuitable for the task. Our method, on the other\nhand, can induce diverse object-to-background changes while preserving the\noriginal semantics and appearance of the object. To achieve this goal, we\nharness the generative capabilities of text-to-image, image-to-text, and\nimage-to-segment models to automatically generate a broad spectrum of\nobject-to-background changes. We induce both natural and adversarial background\nchanges by either modifying the textual prompts or optimizing the latents and\ntextual embedding of text-to-image models. We produce various versions of\nstandard vision datasets (ImageNet, COCO), incorporating either diverse and\nrealistic backgrounds into the images or introducing color, texture, and\nadversarial changes in the background. We conduct extensive experiments to\nanalyze the robustness of vision-based models against object-to-background\ncontext variations across diverse tasks. Code\nhttps://github.com/Muhammad-Huzaifaa/ObjectCompose.",
      "tldr_zh": "这篇论文提出了ObjectCompose方法，用于评估vision-based models在对象到背景组合变化下的鲁棒性，解决了现有方法（如修改对象特征或背景生成）存在的控制不足和对象语义扭曲问题。方法利用text-to-image、image-to-text和image-to-segment模型自动生成多样化的背景变化，包括自然和对抗性调整（如修改文本提示或优化潜在空间和文本嵌入），同时保留对象的原语义和外观。实验通过创建ImageNet和COCO数据集的变体，并进行广泛测试，分析了视觉模型在不同任务中的鲁棒性表现，提供代码以便复现。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04701v4",
      "published_date": "2024-03-07 17:48:48 UTC",
      "updated_date": "2024-10-08 20:10:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:17:39.961780"
    },
    {
      "arxiv_id": "2403.04697v2",
      "title": "AUFormer: Vision Transformers are Parameter-Efficient Facial Action Unit Detectors",
      "title_zh": "翻译失败",
      "authors": [
        "Kaishen Yuan",
        "Zitong Yu",
        "Xin Liu",
        "Weicheng Xie",
        "Huanjing Yue",
        "Jingyu Yang"
      ],
      "abstract": "Facial Action Units (AU) is a vital concept in the realm of affective\ncomputing, and AU detection has always been a hot research topic. Existing\nmethods suffer from overfitting issues due to the utilization of a large number\nof learnable parameters on scarce AU-annotated datasets or heavy reliance on\nsubstantial additional relevant data. Parameter-Efficient Transfer Learning\n(PETL) provides a promising paradigm to address these challenges, whereas its\nexisting methods lack design for AU characteristics. Therefore, we innovatively\ninvestigate PETL paradigm to AU detection, introducing AUFormer and proposing a\nnovel Mixture-of-Knowledge Expert (MoKE) collaboration mechanism. An individual\nMoKE specific to a certain AU with minimal learnable parameters first\nintegrates personalized multi-scale and correlation knowledge. Then the MoKE\ncollaborates with other MoKEs in the expert group to obtain aggregated\ninformation and inject it into the frozen Vision Transformer (ViT) to achieve\nparameter-efficient AU detection. Additionally, we design a Margin-truncated\nDifficulty-aware Weighted Asymmetric Loss (MDWA-Loss), which can encourage the\nmodel to focus more on activated AUs, differentiate the difficulty of\nunactivated AUs, and discard potential mislabeled samples. Extensive\nexperiments from various perspectives, including within-domain, cross-domain,\ndata efficiency, and micro-expression domain, demonstrate AUFormer's\nstate-of-the-art performance and robust generalization abilities without\nrelying on additional relevant data. The code for AUFormer is available at\nhttps://github.com/yuankaishen2001/AUFormer.",
      "tldr_zh": "本文提出 AUFormer，一种基于 Parameter-Efficient Transfer Learning (PETL) 的面部动作单元 (AU) 检测方法，旨在解决现有模型参数过多导致的过拟合问题，并针对 AU 特性进行优化。AUFormer 引入 Mixture-of-Knowledge Expert (MoKE) 机制，每个 MoKE 针对特定 AU 整合个性化的多尺度知识，然后与其他专家协作，将聚合信息注入冻结的 Vision Transformer (ViT)，实现参数高效的检测。同时，设计了 Margin-truncated Difficulty-aware Weighted Asymmetric Loss (MDWA-Loss) 来帮助模型关注激活 AU、区分未激活 AU 的难度，并处理潜在错误标签。实验结果显示，AUFormer 在领域内、跨领域、数据效率和微表情领域均取得了最先进性能，并展示了强大的泛化能力，而无需额外相关数据。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.04697v2",
      "published_date": "2024-03-07 17:46:50 UTC",
      "updated_date": "2024-07-09 15:15:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:17:55.467946"
    },
    {
      "arxiv_id": "2403.04696v2",
      "title": "Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification",
      "title_zh": "通过令牌级不确定性量化对大语言模型输出的事实核查",
      "authors": [
        "Ekaterina Fadeeva",
        "Aleksandr Rubashevskii",
        "Artem Shelmanov",
        "Sergey Petrakov",
        "Haonan Li",
        "Hamdy Mubarak",
        "Evgenii Tsymbalov",
        "Gleb Kuzmin",
        "Alexander Panchenko",
        "Timothy Baldwin",
        "Preslav Nakov",
        "Maxim Panov"
      ],
      "abstract": "Large language models (LLMs) are notorious for hallucinating, i.e., producing\nerroneous claims in their output. Such hallucinations can be dangerous, as\noccasional factual inaccuracies in the generated text might be obscured by the\nrest of the output being generally factually correct, making it extremely hard\nfor the users to spot them. Current services that leverage LLMs usually do not\nprovide any means for detecting unreliable generations. Here, we aim to bridge\nthis gap. In particular, we propose a novel fact-checking and hallucination\ndetection pipeline based on token-level uncertainty quantification. Uncertainty\nscores leverage information encapsulated in the output of a neural network or\nits layers to detect unreliable predictions, and we show that they can be used\nto fact-check the atomic claims in the LLM output. Moreover, we present a novel\ntoken-level uncertainty quantification method that removes the impact of\nuncertainty about what claim to generate on the current step and what surface\nform to use. Our method Claim Conditioned Probability (CCP) measures only the\nuncertainty of a particular claim value expressed by the model. Experiments on\nthe task of biography generation demonstrate strong improvements for CCP\ncompared to the baselines for seven LLMs and four languages. Human evaluation\nreveals that the fact-checking pipeline based on uncertainty quantification is\ncompetitive with a fact-checking tool that leverages external knowledge.",
      "tldr_zh": "本研究针对大型语言模型（Large Language Models, LLMs）的幻觉问题（hallucinating），提出了一种基于token-level不确定性量化（uncertainty quantification）的fact-checking管道，以检测输出中的错误声明。该管道利用Claim Conditioned Probability (CCP)方法，专注于特定声明值的不确定性，排除生成声明内容和形式的干扰影响。在biography generation任务上，实验结果显示CCP相对于基线方法在七个LLMs和四种语言上显著提升了性能，且人类评估表明该管道的检测效果与使用外部知识的工具相当。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ACL-2024 (Findings). Ekaterina Fadeeva, Aleksandr\n  Rubashevskii, and Artem Shelmanov contributed equally",
      "pdf_url": "http://arxiv.org/pdf/2403.04696v2",
      "published_date": "2024-03-07 17:44:17 UTC",
      "updated_date": "2024-06-06 21:32:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:18:04.342878"
    },
    {
      "arxiv_id": "2403.04690v3",
      "title": "Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level",
      "title_zh": "更快的邻域注意力：在线程块级别减少自注意力的 O(n",
      "authors": [
        "Ali Hassani",
        "Wen-Mei Hwu",
        "Humphrey Shi"
      ],
      "abstract": "Neighborhood attention reduces the cost of self attention by restricting each\ntoken's attention span to its nearest neighbors. This restriction,\nparameterized by a window size and dilation factor, draws a spectrum of\npossible attention patterns between linear projection and self attention.\nNeighborhood attention, and more generally sliding window attention patterns,\nhave long been bounded by infrastructure, particularly in higher-rank spaces\n(2-D and 3-D), calling for the development of custom kernels, which have been\nlimited in either functionality, or performance, if not both. In this work, we\naim to massively improve upon existing infrastructure by providing two new\nmethods for implementing neighborhood attention. We first show that\nneighborhood attention can be represented as a batched GEMM problem, similar to\nstandard attention, and implement it for 1-D and 2-D neighborhood attention.\nThese kernels on average provide 895% and 272% improvement in full precision\nruntime compared to existing naive CUDA kernels for 1-D and 2-D neighborhood\nattention respectively. We find that aside from being heavily bound by memory\nbandwidth, certain inherent inefficiencies exist in all unfused implementations\nof neighborhood attention, which in most cases undo their theoretical\nefficiency gain. Motivated by the progress made into fused dot-product\nattention kernels, we developed fused neighborhood attention; an adaptation of\nfused dot-product attention kernels that allow fine-grained control over\nattention across different spatial axes. Known for reducing the quadratic time\ncomplexity of self attention to a linear complexity, neighborhood attention can\nnow enjoy a reduced and constant memory footprint, and record-breaking half\nprecision runtime. We observe that our fused implementation successfully\ncircumvents some of the unavoidable inefficiencies in unfused\nimplementations...",
      "tldr_zh": "该论文针对自注意力机制的 O(n^2) 计算成本问题，提出了 Faster Neighborhood Attention 方法，通过将每个 token 的注意力限制在最近邻居（参数化窗口大小和膨胀因子）来实现高效优化。作者开发了两种新实现：一是将 neighborhood attention 表示为 batched GEMM 问题，并为 1-D 和 2-D 场景创建自定义 CUDA 内核，平均提高了 895% 和 272% 的全精度运行时性能；二是引入 fused neighborhood attention，类似于 fused dot-product attention，提供精细的空间轴控制并减少内存占用。实验结果表明，该方法有效规避了非融合实现的效率瓶颈，提升了 neighborhood attention 在高维空间的应用潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "To appear in 38th Conference on Neural Information Processing Systems\n  (NeurIPS 2024)",
      "pdf_url": "http://arxiv.org/pdf/2403.04690v3",
      "published_date": "2024-03-07 17:35:58 UTC",
      "updated_date": "2024-10-31 17:32:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:18:17.509755"
    },
    {
      "arxiv_id": "2403.04667v1",
      "title": "The Social Impact of Generative AI: An Analysis on ChatGPT",
      "title_zh": "翻译失败",
      "authors": [
        "Maria T. Baldassarre",
        "Danilo Caivano",
        "Berenice Fernandez Nieto",
        "Domenico Gigante",
        "Azzurra Ragone"
      ],
      "abstract": "In recent months, the social impact of Artificial Intelligence (AI) has\ngained considerable public interest, driven by the emergence of Generative AI\nmodels, ChatGPT in particular. The rapid development of these models has\nsparked heated discussions regarding their benefits, limitations, and\nassociated risks. Generative models hold immense promise across multiple\ndomains, such as healthcare, finance, and education, to cite a few, presenting\ndiverse practical applications. Nevertheless, concerns about potential adverse\neffects have elicited divergent perspectives, ranging from privacy risks to\nescalating social inequality. This paper adopts a methodology to delve into the\nsocietal implications of Generative AI tools, focusing primarily on the case of\nChatGPT. It evaluates the potential impact on several social sectors and\nillustrates the findings of a comprehensive literature review of both positive\nand negative effects, emerging trends, and areas of opportunity of Generative\nAI models. This analysis aims to facilitate an in-depth discussion by providing\ninsights that can inspire policy, regulation, and responsible development\npractices to foster a human-centered AI.",
      "tldr_zh": "这篇论文分析了生成式AI（Generative AI），尤其是ChatGPT，对社会的影响，探讨其在医疗、金融和教育等领域的潜在益处，如多样化应用和创新机会，同时也强调了风险，包括隐私泄露和社会不平等。研究采用文献综述方法，评估了积极和消极效果、当前趋势以及未来发展领域。最终，该分析旨在为AI政策、法规和负责任开发提供insights，促进人类中心AI的可持续发展。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.ET"
      ],
      "primary_category": "cs.AI",
      "comment": "Presented at GoodIT2023 - ACM Conference on Information Technology\n  for Social Good",
      "pdf_url": "http://arxiv.org/pdf/2403.04667v1",
      "published_date": "2024-03-07 17:14:22 UTC",
      "updated_date": "2024-03-07 17:14:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:18:27.657289"
    },
    {
      "arxiv_id": "2403.04652v3",
      "title": "Yi: Open Foundation Models by 01.AI",
      "title_zh": "翻译失败",
      "authors": [
        "01. AI",
        ":",
        "Alex Young",
        "Bei Chen",
        "Chao Li",
        "Chengen Huang",
        "Ge Zhang",
        "Guanwei Zhang",
        "Guoyin Wang",
        "Heng Li",
        "Jiangcheng Zhu",
        "Jianqun Chen",
        "Jing Chang",
        "Kaidong Yu",
        "Peng Liu",
        "Qiang Liu",
        "Shawn Yue",
        "Senbin Yang",
        "Shiming Yang",
        "Wen Xie",
        "Wenhao Huang",
        "Xiaohui Hu",
        "Xiaoyi Ren",
        "Xinyao Niu",
        "Pengcheng Nie",
        "Yanpeng Li",
        "Yuchi Xu",
        "Yudong Liu",
        "Yue Wang",
        "Yuxuan Cai",
        "Zhenyu Gu",
        "Zhiyuan Liu",
        "Zonghong Dai"
      ],
      "abstract": "We introduce the Yi model family, a series of language and multimodal models\nthat demonstrate strong multi-dimensional capabilities. The Yi model family is\nbased on 6B and 34B pretrained language models, then we extend them to chat\nmodels, 200K long context models, depth-upscaled models, and vision-language\nmodels. Our base models achieve strong performance on a wide range of\nbenchmarks like MMLU, and our finetuned chat models deliver strong human\npreference rate on major evaluation platforms like AlpacaEval and Chatbot\nArena. Building upon our scalable super-computing infrastructure and the\nclassical transformer architecture, we attribute the performance of Yi models\nprimarily to its data quality resulting from our data-engineering efforts. For\npretraining, we construct 3.1 trillion tokens of English and Chinese corpora\nusing a cascaded data deduplication and quality filtering pipeline. For\nfinetuning, we polish a small scale (less than 10K) instruction dataset over\nmultiple iterations such that every single instance has been verified directly\nby our machine learning engineers. For vision-language, we combine the chat\nlanguage model with a vision transformer encoder and train the model to align\nvisual representations to the semantic space of the language model. We further\nextend the context length to 200K through lightweight continual pretraining and\ndemonstrate strong needle-in-a-haystack retrieval performance. We show that\nextending the depth of the pretrained checkpoint through continual pretraining\nfurther improves performance. We believe that given our current results,\ncontinuing to scale up model parameters using thoroughly optimized data will\nlead to even stronger frontier models.",
      "tldr_zh": "01.AI 推出了 Yi 模型家族，这是一个系列的语言和多模态模型，包括基于 6B 和 34B 预训练语言模型的扩展版本，如聊天模型、200K 长上下文模型、深度扩展模型和 vision-language models。模型通过经典 transformer 架构和高品质数据工程实现强大性能，预训练使用了 3.1 万亿 tokens 的英语和中文语料库，经级联去重和质量过滤处理，而微调则基于少于 10K 的指令数据集进行多次迭代验证。实验结果显示，Yi 基模型在 MMLU 等基准上表现出色，微调聊天模型在 AlpacaEval 和 Chatbot Arena 上获得高人类偏好率，通过轻量级持续预训练进一步扩展上下文长度和深度，提升了整体能力，表明优化数据并规模化模型参数可推动更前沿的 AI 发展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04652v3",
      "published_date": "2024-03-07 16:52:49 UTC",
      "updated_date": "2025-01-21 10:12:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:18:41.960944"
    },
    {
      "arxiv_id": "2403.04650v3",
      "title": "Lightweight Cross-Modal Representation Learning",
      "title_zh": "轻量化的跨模态表示学习",
      "authors": [
        "Bilal Faye",
        "Hanane Azzag",
        "Mustapha Lebbah",
        "Djamel Bouchaffra"
      ],
      "abstract": "Low-cost cross-modal representation learning is crucial for deriving semantic\nrepresentations across diverse modalities such as text, audio, images, and\nvideo. Traditional approaches typically depend on large specialized models\ntrained from scratch, requiring extensive datasets and resulting in high\nresource and time costs. To overcome these challenges, we introduce a novel\napproach named Lightweight Cross-Modal Representation Learning (LightCRL). This\nmethod uses a single neural network titled Deep Fusion Encoder (DFE), which\nprojects data from multiple modalities into a shared latent representation\nspace. This reduces the overall parameter count while still delivering robust\nperformance comparable to more complex systems.",
      "tldr_zh": "本文研究了低成本的跨模态表示学习（cross-modal representation learning），旨在为文本、音频、图像和视频等不同模态生成共享的语义表示，以解决传统方法依赖大型模型、需大量数据集和高资源消耗的问题。论文引入了Lightweight Cross-Modal Representation Learning (LightCRL)方法，该方法使用一个单一的Deep Fusion Encoder (DFE)神经网络，将多模态数据投影到共享的潜在表示空间，从而减少参数数量。实验结果表明，LightCRL在保持鲁棒性能的同时，大大降低了计算成本。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04650v3",
      "published_date": "2024-03-07 16:50:25 UTC",
      "updated_date": "2024-09-07 07:24:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:18:52.472126"
    },
    {
      "arxiv_id": "2403.04634v2",
      "title": "Pix2Gif: Motion-Guided Diffusion for GIF Generation",
      "title_zh": "Pix2Gif：运动引导扩散模型用于GIF生成",
      "authors": [
        "Hitesh Kandala",
        "Jianfeng Gao",
        "Jianwei Yang"
      ],
      "abstract": "We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video)\ngeneration. We tackle this problem differently by formulating the task as an\nimage translation problem steered by text and motion magnitude prompts, as\nshown in teaser fig. To ensure that the model adheres to motion guidance, we\npropose a new motion-guided warping module to spatially transform the features\nof the source image conditioned on the two types of prompts. Furthermore, we\nintroduce a perceptual loss to ensure the transformed feature map remains\nwithin the same space as the target image, ensuring content consistency and\ncoherence. In preparation for the model training, we meticulously curated data\nby extracting coherent image frames from the TGIF video-caption dataset, which\nprovides rich information about the temporal changes of subjects. After\npretraining, we apply our model in a zero-shot manner to a number of video\ndatasets. Extensive qualitative and quantitative experiments demonstrate the\neffectiveness of our model -- it not only captures the semantic prompt from\ntext but also the spatial ones from motion guidance. We train all our models\nusing a single node of 16xV100 GPUs. Code, dataset and models are made public\nat: https://hiteshk03.github.io/Pix2Gif/.",
      "tldr_zh": "本文提出 Pix2Gif，一种基于 motion-guided diffusion model 的框架，用于图像到 GIF（视频）的生成，通过文本和运动幅度提示引导图像转换。模型引入 motion-guided warping module 来空间转换源图像特征，并采用 perceptual loss 确保内容一致性和连贯性；同时，使用从 TGIF 数据集中提取的连贯图像帧进行训练。实验结果显示，Pix2Gif 在定性和定量评估中表现出色，能够有效捕捉文本的语义提示和运动的空间指导，并在零样本视频数据集上实现应用。代码、数据集和模型已公开。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04634v2",
      "published_date": "2024-03-07 16:18:28 UTC",
      "updated_date": "2024-03-08 18:28:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:19:05.247753"
    },
    {
      "arxiv_id": "2403.04629v2",
      "title": "Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI Collaboration",
      "title_zh": "翻译失败",
      "authors": [
        "Julian Rodemann",
        "Federico Croppi",
        "Philipp Arens",
        "Yusuf Sale",
        "Julia Herbinger",
        "Bernd Bischl",
        "Eyke Hüllermeier",
        "Thomas Augustin",
        "Conor J. Walsh",
        "Giuseppe Casalicchio"
      ],
      "abstract": "Bayesian optimization (BO) with Gaussian processes (GP) has become an\nindispensable algorithm for black box optimization problems. Not without a dash\nof irony, BO is often considered a black box itself, lacking ways to provide\nreasons as to why certain parameters are proposed to be evaluated. This is\nparticularly relevant in human-in-the-loop applications of BO, such as in\nrobotics. We address this issue by proposing ShapleyBO, a framework for\ninterpreting BO's proposals by game-theoretic Shapley values.They quantify each\nparameter's contribution to BO's acquisition function. Exploiting the linearity\nof Shapley values, we are further able to identify how strongly each parameter\ndrives BO's exploration and exploitation for additive acquisition functions\nlike the confidence bound. We also show that ShapleyBO can disentangle the\ncontributions to exploration into those that explore aleatoric and epistemic\nuncertainty. Moreover, our method gives rise to a ShapleyBO-assisted human\nmachine interface (HMI), allowing users to interfere with BO in case proposals\ndo not align with human reasoning. We demonstrate this HMI's benefits for the\nuse case of personalizing wearable robotic devices (assistive back exosuits) by\nhuman-in-the-loop BO. Results suggest human-BO teams with access to ShapleyBO\ncan achieve lower regret than teams without.",
      "tldr_zh": "这篇论文提出ShapleyBO框架，使用Shapley values来解释Bayesian optimization (BO) 的参数提案，从而提升人机协作效率。框架通过量化每个参数对BO获取函数的贡献，分析参数在探索和利用方面的作用，特别是针对可加性获取函数如置信边界，并区分aleatoric和epistemic不确定性。研究还开发了ShapleyBO辅助的人机界面(HMI)，允许用户干预BO的决策以匹配人类推理。实验结果表明，在个性化可穿戴机器人设备（如辅助背部外骨骼）的优化场景中，使用ShapleyBO的人机团队比无解释性辅助的团队实现了更低的遗憾值。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC",
        "cs.RO",
        "stat.ML",
        "I.2.6; I.2.9; F.2.2; J.6"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint. Copyright by the authors. 19 pages, 24 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.04629v2",
      "published_date": "2024-03-07 16:13:32 UTC",
      "updated_date": "2024-03-08 07:52:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:19:18.420834"
    },
    {
      "arxiv_id": "2403.04612v1",
      "title": "A Domain Translation Framework with an Adversarial Denoising Diffusion Model to Generate Synthetic Datasets of Echocardiography Images",
      "title_zh": "翻译失败",
      "authors": [
        "Cristiana Tiago",
        "Sten Roar Snare",
        "Jurica Sprem",
        "Kristin McLeod"
      ],
      "abstract": "Currently, medical image domain translation operations show a high demand\nfrom researchers and clinicians. Amongst other capabilities, this task allows\nthe generation of new medical images with sufficiently high image quality,\nmaking them clinically relevant. Deep Learning (DL) architectures, most\nspecifically deep generative models, are widely used to generate and translate\nimages from one domain to another. The proposed framework relies on an\nadversarial Denoising Diffusion Model (DDM) to synthesize echocardiography\nimages and perform domain translation. Contrary to Generative Adversarial\nNetworks (GANs), DDMs are able to generate high quality image samples with a\nlarge diversity. If a DDM is combined with a GAN, this ability to generate new\ndata is completed at an even faster sampling time. In this work we trained an\nadversarial DDM combined with a GAN to learn the reverse denoising process,\nrelying on a guide image, making sure relevant anatomical structures of each\nechocardiography image were kept and represented on the generated image\nsamples. For several domain translation operations, the results verified that\nsuch generative model was able to synthesize high quality image samples: MSE:\n11.50 +/- 3.69, PSNR (dB): 30.48 +/- 0.09, SSIM: 0.47 +/- 0.03. The proposed\nmethod showed high generalization ability, introducing a framework to create\nechocardiography images suitable to be used for clinical research purposes.",
      "tldr_zh": "本研究提出了一种基于对抗性去噪扩散模型（Adversarial Denoising Diffusion Model, DDM）的领域转换框架，用于生成高质量的心超声图像（echocardiography images）合成数据集。框架将 DDM 与生成对抗网络（GANs）结合，通过学习逆去噪过程并依赖引导图像，确保生成的图像保留关键解剖结构，同时加速采样时间。实验结果显示，该模型在多种领域转换任务中表现出色，平均指标为 MSE: 11.50 +/- 3.69, PSNR: 30.48 +/- 0.09 dB, SSIM: 0.47 +/- 0.03，具有高泛化能力，可为临床研究提供合适的图像数据。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04612v1",
      "published_date": "2024-03-07 15:58:03 UTC",
      "updated_date": "2024-03-07 15:58:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:19:29.675773"
    },
    {
      "arxiv_id": "2403.04588v1",
      "title": "Zero-shot cross-modal transfer of Reinforcement Learning policies through a Global Workspace",
      "title_zh": "翻译失败",
      "authors": [
        "Léopold Maytié",
        "Benjamin Devillers",
        "Alexandre Arnold",
        "Rufin VanRullen"
      ],
      "abstract": "Humans perceive the world through multiple senses, enabling them to create a\ncomprehensive representation of their surroundings and to generalize\ninformation across domains. For instance, when a textual description of a scene\nis given, humans can mentally visualize it. In fields like robotics and\nReinforcement Learning (RL), agents can also access information about the\nenvironment through multiple sensors; yet redundancy and complementarity\nbetween sensors is difficult to exploit as a source of robustness (e.g. against\nsensor failure) or generalization (e.g. transfer across domains). Prior\nresearch demonstrated that a robust and flexible multimodal representation can\nbe efficiently constructed based on the cognitive science notion of a 'Global\nWorkspace': a unique representation trained to combine information across\nmodalities, and to broadcast its signal back to each modality. Here, we explore\nwhether such a brain-inspired multimodal representation could be advantageous\nfor RL agents. First, we train a 'Global Workspace' to exploit information\ncollected about the environment via two input modalities (a visual input, or an\nattribute vector representing the state of the agent and/or its environment).\nThen, we train a RL agent policy using this frozen Global Workspace. In two\ndistinct environments and tasks, our results reveal the model's ability to\nperform zero-shot cross-modal transfer between input modalities, i.e. to apply\nto image inputs a policy previously trained on attribute vectors (and\nvice-versa), without additional training or fine-tuning. Variants and ablations\nof the full Global Workspace (including a CLIP-like multimodal representation\ntrained via contrastive learning) did not display the same generalization\nabilities.",
      "tldr_zh": "本研究探讨了基于“Global Workspace”多模态表示在强化学习（RL）中的应用，旨在解决代理在不同传感器输入间实现鲁棒性和泛化能力的挑战。研究者训练了一个Global Workspace来整合视觉输入和属性向量等模态信息，然后使用该冻结的表示来训练RL代理策略。在两个环境中，实验结果显示，该模型实现了零-shot cross-modal transfer，即将原本在属性向量上训练的政策直接应用于图像输入，反之亦然，而无需额外微调。与CLIP-like的多模态表示相比，该方法展示了更强的泛化能力，为RL代理的跨域适应提供了新途径。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Under review in a conference",
      "pdf_url": "http://arxiv.org/pdf/2403.04588v1",
      "published_date": "2024-03-07 15:35:29 UTC",
      "updated_date": "2024-03-07 15:35:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:19:41.089588"
    },
    {
      "arxiv_id": "2403.04577v2",
      "title": "Wiki-TabNER: Integrating Named Entity Recognition into Wikipedia Tables",
      "title_zh": "翻译失败",
      "authors": [
        "Aneta Koleva",
        "Martin Ringsquandl",
        "Ahmed Hatem",
        "Thomas Runkler",
        "Volker Tresp"
      ],
      "abstract": "Interest in solving table interpretation tasks has grown over the years, yet\nit still relies on existing datasets that may be overly simplified. This is\npotentially reducing the effectiveness of the dataset for thorough evaluation\nand failing to accurately represent tables as they appear in the real-world. To\nenrich the existing benchmark datasets, we extract and annotate a new, more\nchallenging dataset. The proposed Wiki-TabNER dataset features complex tables\ncontaining several entities per cell, with named entities labeled using DBpedia\nclasses. This dataset is specifically designed to address named entity\nrecognition (NER) task within tables, but it can also be used as a more\nchallenging dataset for evaluating the entity linking task. In this paper we\ndescribe the distinguishing features of the Wiki-TabNER dataset and the\nlabeling process. In addition, we propose a prompting framework for evaluating\nthe new large language models on the within tables NER task. Finally, we\nperform qualitative analysis to gain insights into the challenges encountered\nby the models and to understand the limitations of the proposed~dataset.",
      "tldr_zh": "本研究指出，现有的表格解释任务数据集过于简化，无法有效评估模型性能或代表真实世界表格。为此，研究者构建了Wiki-TabNER数据集，该数据集从维基百科提取复杂表格，每个单元格可能包含多个实体，并使用DBpedia类进行命名实体识别(NER)标注，以提升NER任务的挑战性。该数据集不仅适用于NER，还可用于实体链接任务的评估。此外，论文提出一个提示框架来评估大型语言模型在表格NER任务上的表现，并通过定性分析揭示了模型面临的挑战和数据集的局限性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at SIGIR 2025 conference",
      "pdf_url": "http://arxiv.org/pdf/2403.04577v2",
      "published_date": "2024-03-07 15:22:07 UTC",
      "updated_date": "2025-05-02 17:52:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:19:53.630314"
    },
    {
      "arxiv_id": "2403.04571v1",
      "title": "Machine learning and information theory concepts towards an AI Mathematician",
      "title_zh": "翻译失败",
      "authors": [
        "Yoshua Bengio",
        "Nikolay Malkin"
      ],
      "abstract": "The current state-of-the-art in artificial intelligence is impressive,\nespecially in terms of mastery of language, but not so much in terms of\nmathematical reasoning. What could be missing? Can we learn something useful\nabout that gap from how the brains of mathematicians go about their craft? This\nessay builds on the idea that current deep learning mostly succeeds at system 1\nabilities -- which correspond to our intuition and habitual behaviors -- but\nstill lacks something important regarding system 2 abilities -- which include\nreasoning and robust uncertainty estimation. It takes an\ninformation-theoretical posture to ask questions about what constitutes an\ninteresting mathematical statement, which could guide future work in crafting\nan AI mathematician. The focus is not on proving a given theorem but on\ndiscovering new and interesting conjectures. The central hypothesis is that a\ndesirable body of theorems better summarizes the set of all provable\nstatements, for example by having a small description length while at the same\ntime being close (in terms of number of derivation steps) to many provable\nstatements.",
      "tldr_zh": "这篇论文探讨了机器学习和信息理论如何帮助构建AI Mathematician，指出当前AI在数学推理（system 2能力，如推理和不确定性估计）上落后于语言掌握（system 1能力）。论文采用信息理论视角，分析什么是“有趣”的数学语句，强调焦点应从证明定理转向发现新颖猜想。核心假设是，一个理想的定理集应具有小描述长度，同时在推导步骤上接近许多可证明语句，从而更好地总结数学知识。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "To appear in the Bulletin of the AMS, 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.04571v1",
      "published_date": "2024-03-07 15:12:06 UTC",
      "updated_date": "2024-03-07 15:12:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:20:06.530557"
    },
    {
      "arxiv_id": "2403.04558v2",
      "title": "Reducing self-supervised learning complexity improves weakly-supervised classification performance in computational pathology",
      "title_zh": "翻译失败",
      "authors": [
        "Tim Lenz",
        "Omar S. M. El Nahhas",
        "Marta Ligero",
        "Jakob Nikolas Kather"
      ],
      "abstract": "Deep Learning models have been successfully utilized to extract clinically\nactionable insights from routinely available histology data. Generally, these\nmodels require annotations performed by clinicians, which are scarce and costly\nto generate. The emergence of self-supervised learning (SSL) methods remove\nthis barrier, allowing for large-scale analyses on non-annotated data. However,\nrecent SSL approaches apply increasingly expansive model architectures and\nlarger datasets, causing the rapid escalation of data volumes, hardware\nprerequisites, and overall expenses, limiting access to these resources to few\ninstitutions. Therefore, we investigated the complexity of contrastive SSL in\ncomputational pathology in relation to classification performance with the\nutilization of consumer-grade hardware. Specifically, we analyzed the effects\nof adaptations in data volume, architecture, and algorithms on downstream\nclassification tasks, emphasizing their impact on computational resources. We\ntrained breast cancer foundation models on a large public patient cohort and\nvalidated them on various downstream classification tasks in a weakly\nsupervised manner on two external public patient cohorts. Our experiments\ndemonstrate that we can improve downstream classification performance whilst\nreducing SSL training duration by 90%. In summary, we propose a set of\nadaptations which enable the utilization of SSL in computational pathology in\nnon-resource abundant environments.",
      "tldr_zh": "本研究探讨了在计算病理学中，通过降低自监督学习(SSL)的复杂性来提升弱监督分类性能的问题，旨在解决现有SSL方法对大规模数据和硬件资源的依赖。研究者通过调整数据量、模型架构和算法，在一个大型公共乳腺癌患者队列上训练基础模型，并在外部分类任务中进行验证。实验结果显示，这种优化方法可以将SSL训练时间减少90%，同时提高下游分类任务的性能。总之，该工作提出了一套适应方案，使SSL在资源有限的环境中更易于应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to MICCAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.04558v2",
      "published_date": "2024-03-07 14:56:06 UTC",
      "updated_date": "2024-03-12 11:42:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:20:19.801965"
    },
    {
      "arxiv_id": "2403.04547v1",
      "title": "CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?",
      "title_zh": "翻译失败",
      "authors": [
        "Ibrahim Alabdulmohsin",
        "Xiao Wang",
        "Andreas Steiner",
        "Priya Goyal",
        "Alexander D'Amour",
        "Xiaohua Zhai"
      ],
      "abstract": "We study the effectiveness of data-balancing for mitigating biases in\ncontrastive language-image pretraining (CLIP), identifying areas of strength\nand limitation. First, we reaffirm prior conclusions that CLIP models can\ninadvertently absorb societal stereotypes. To counter this, we present a novel\nalgorithm, called Multi-Modal Moment Matching (M4), designed to reduce both\nrepresentation and association biases (i.e. in first- and second-order\nstatistics) in multimodal data. We use M4 to conduct an in-depth analysis\ntaking into account various factors, such as the model, representation, and\ndata size. Our study also explores the dynamic nature of how CLIP learns and\nunlearns biases. In particular, we find that fine-tuning is effective in\ncountering representation biases, though its impact diminishes for association\nbiases. Also, data balancing has a mixed impact on quality: it tends to improve\nclassification but can hurt retrieval. Interestingly, data and architectural\nimprovements seem to mitigate the negative impact of data balancing on\nperformance; e.g. applying M4 to SigLIP-B/16 with data quality filters improves\nCOCO image-to-text retrieval @5 from 86% (without data balancing) to 87% and\nImageNet 0-shot classification from 77% to 77.5%! Finally, we conclude with\nrecommendations for improving the efficacy of data balancing in multimodal\nsystems.",
      "tldr_zh": "本研究探讨了在对比语言图像预训练 (CLIP) 中，通过数据平衡来缓解偏差的有效性，并引入了 Multi-Modal Moment Matching (M4) 算法，以减少表示偏差 (representation biases) 和关联偏差 (association biases)。研究分析了多种因素，包括模型、表示形式和数据规模，发现微调 (fine-tuning) 对表示偏差有显著效果，但对关联偏差的影响较小，而数据平衡能改善分类性能，却可能损害检索任务。实验结果显示，在 SigLIP-B/16 模型上应用 M4 后，COCO 图像到文本检索 @5 从 86% 提升至 87%，ImageNet 0-shot 分类从 77% 微升至 77.5%。最终，作者提供了数据平衡在多模态系统中的改进推荐，以提升其整体效能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "32 pages, 20 figures, 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2403.04547v1",
      "published_date": "2024-03-07 14:43:17 UTC",
      "updated_date": "2024-03-07 14:43:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:20:30.554108"
    },
    {
      "arxiv_id": "2403.04541v1",
      "title": "Towards Automatic Composition of ASP Programs from Natural Language Specifications",
      "title_zh": "翻译失败",
      "authors": [
        "Manuel Borroto",
        "Irfan Kareem",
        "Francesco Ricca"
      ],
      "abstract": "This paper moves the first step towards automating the composition of Answer\nSet Programming (ASP) specifications. In particular, the following\ncontributions are provided: (i) A dataset focused on graph-related problem\nspecifications, designed to develop and assess tools for ASP automatic coding;\n(ii) A two-step architecture, implemented in the NL2ASP tool, for generating\nASP programs from natural language specifications. NL2ASP uses neural machine\ntranslation to transform natural language into Controlled Natural Language\n(CNL) statements. Subsequently, CNL statements are converted into ASP code\nusing the CNL2ASP tool. An experiment confirms the viability of the approach.",
      "tldr_zh": "这篇论文旨在自动化从自然语言规范生成 Answer Set Programming (ASP) 程序，以简化编程过程。主要贡献包括：(i) 一个专注于图相关问题规范的数据集，用于开发和评估 ASP 自动编码工具；(ii) 一个两步架构的 NL2ASP 工具，该工具先通过神经机器翻译将自然语言转化为 Controlled Natural Language (CNL) 语句，然后使用 CNL2ASP 将 CNL 语句转换为 ASP 代码。实验结果证实了这种方法的有效性，为 ASP 编程自动化提供了可行途径。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04541v1",
      "published_date": "2024-03-07 14:36:52 UTC",
      "updated_date": "2024-03-07 14:36:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:20:42.347185"
    },
    {
      "arxiv_id": "2403.04529v1",
      "title": "Enhancing Data Quality in Federated Fine-Tuning of Foundation Models",
      "title_zh": "提升基础模型联邦微调中的数据质量",
      "authors": [
        "Wanru Zhao",
        "Yaxin Du",
        "Nicholas Donald Lane",
        "Siheng Chen",
        "Yanfeng Wang"
      ],
      "abstract": "In the current landscape of foundation model training, there is a significant\nreliance on public domain data, which is nearing exhaustion according to recent\nresearch. To further scale up, it is crucial to incorporate collaboration among\nmultiple specialized and high-quality private domain data sources. However, the\nchallenge of training models locally without sharing private data presents\nnumerous obstacles in data quality control. To tackle this issue, we propose a\ndata quality control pipeline for federated fine-tuning of foundation models.\nThis pipeline computes scores reflecting the quality of training data and\ndetermines a global threshold for a unified standard, aiming for improved\nglobal performance. Our experiments show that the proposed quality control\npipeline facilitates the effectiveness and reliability of the model training,\nleading to better performance.",
      "tldr_zh": "当前基础模型训练过度依赖公共数据，而这些数据资源即将耗尽，因此需要整合多个高质量私有数据源进行协作训练，但数据共享面临挑战。为解决这一问题，本文提出一个数据质量控制管道，用于 foundation models 的 federated fine-tuning，该管道通过计算训练数据的质量分数并设定全局阈值，以统一数据标准。实验结果表明，该方法显著提升了模型训练的有效性和可靠性，最终实现了更好的全局性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ICLR 2024 Workshop on Navigating and Addressing Data\n  Problems for Foundation Models (DPFM)",
      "pdf_url": "http://arxiv.org/pdf/2403.04529v1",
      "published_date": "2024-03-07 14:28:04 UTC",
      "updated_date": "2024-03-07 14:28:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:20:53.907942"
    },
    {
      "arxiv_id": "2403.04526v1",
      "title": "Hyperspectral unmixing for Raman spectroscopy via physics-constrained autoencoders",
      "title_zh": "翻译失败",
      "authors": [
        "Dimitar Georgiev",
        "Álvaro Fernández-Galiana",
        "Simon Vilms Pedersen",
        "Georgios Papadopoulos",
        "Ruoxiao Xie",
        "Molly M. Stevens",
        "Mauricio Barahona"
      ],
      "abstract": "Raman spectroscopy is widely used across scientific domains to characterize\nthe chemical composition of samples in a non-destructive, label-free manner.\nMany applications entail the unmixing of signals from mixtures of molecular\nspecies to identify the individual components present and their proportions,\nyet conventional methods for chemometrics often struggle with complex mixture\nscenarios encountered in practice. Here, we develop hyperspectral unmixing\nalgorithms based on autoencoder neural networks, and we systematically validate\nthem using both synthetic and experimental benchmark datasets created in-house.\nOur results demonstrate that unmixing autoencoders provide improved accuracy,\nrobustness and efficiency compared to standard unmixing methods. We also\nshowcase the applicability of autoencoders to complex biological settings by\nshowing improved biochemical characterization of volumetric Raman imaging data\nfrom a monocytic cell.",
      "tldr_zh": "这篇论文提出了一种基于物理约束的自编码器神经网络，用于拉曼光谱的超光谱反混叠，旨在解决传统化学计量学方法在处理复杂分子混合物时的准确性问题。研究者通过合成和实验基准数据集进行系统验证，结果显示该方法比标准反混叠算法提供了更高的准确性、鲁棒性和效率。论文还展示了其在生物应用中的潜力，例如提升了单核细胞体积拉曼成像数据的生化表征能力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04526v1",
      "published_date": "2024-03-07 14:27:08 UTC",
      "updated_date": "2024-03-07 14:27:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:21:05.438009"
    },
    {
      "arxiv_id": "2403.04523v1",
      "title": "T-TAME: Trainable Attention Mechanism for Explaining Convolutional Networks and Vision Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Mariano V. Ntrougkas",
        "Nikolaos Gkalelis",
        "Vasileios Mezaris"
      ],
      "abstract": "The development and adoption of Vision Transformers and other deep-learning\narchitectures for image classification tasks has been rapid. However, the\n\"black box\" nature of neural networks is a barrier to adoption in applications\nwhere explainability is essential. While some techniques for generating\nexplanations have been proposed, primarily for Convolutional Neural Networks,\nadapting such techniques to the new paradigm of Vision Transformers is\nnon-trivial. This paper presents T-TAME, Transformer-compatible Trainable\nAttention Mechanism for Explanations, a general methodology for explaining deep\nneural networks used in image classification tasks. The proposed architecture\nand training technique can be easily applied to any convolutional or Vision\nTransformer-like neural network, using a streamlined training approach. After\ntraining, explanation maps can be computed in a single forward pass; these\nexplanation maps are comparable to or outperform the outputs of computationally\nexpensive perturbation-based explainability techniques, achieving SOTA\nperformance. We apply T-TAME to three popular deep learning classifier\narchitectures, VGG-16, ResNet-50, and ViT-B-16, trained on the ImageNet\ndataset, and we demonstrate improvements over existing state-of-the-art\nexplainability methods. A detailed analysis of the results and an ablation\nstudy provide insights into how the T-TAME design choices affect the quality of\nthe generated explanation maps.",
      "tldr_zh": "本研究提出T-TAME，一种可训练的注意力机制(Trainable Attention Mechanism)，用于解释卷积神经网络(Convolutional Neural Networks)和视觉变压器(Vision Transformers)在图像分类任务中的决策过程，以解决这些模型的黑盒问题。T-TAME 通过简化的训练方法，可以轻松应用于各种神经网络架构，并在训练后通过单次前向传播生成高质量的解释映射。实验结果显示，在ImageNet数据集上应用于VGG-16、ResNet-50和ViT-B-16等模型时，T-TAME的解释性能达到或超过了SOTA(State-of-the-Art)水平，并通过详细分析和消融研究验证了其设计选择的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Under review",
      "pdf_url": "http://arxiv.org/pdf/2403.04523v1",
      "published_date": "2024-03-07 14:25:03 UTC",
      "updated_date": "2024-03-07 14:25:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:21:18.635996"
    },
    {
      "arxiv_id": "2403.04511v1",
      "title": "Uncovering the Deep Filter Bubble: Narrow Exposure in Short-Video Recommendation",
      "title_zh": "揭示深度过滤气泡：短视频推荐中的狭窄暴露",
      "authors": [
        "Nicholas Sukiennik",
        "Chen Gao",
        "Nian Li"
      ],
      "abstract": "Filter bubbles have been studied extensively within the context of online\ncontent platforms due to their potential to cause undesirable outcomes such as\nuser dissatisfaction or polarization. With the rise of short-video platforms,\nthe filter bubble has been given extra attention because these platforms rely\non an unprecedented use of the recommender system to provide relevant content.\nIn our work, we investigate the deep filter bubble, which refers to the user\nbeing exposed to narrow content within their broad interests. We accomplish\nthis using one-year interaction data from a top short-video platform in China,\nwhich includes hierarchical data with three levels of categories for each\nvideo. We formalize our definition of a \"deep\" filter bubble within this\ncontext, and then explore various correlations within the data: first\nunderstanding the evolution of the deep filter bubble over time, and later\nrevealing some of the factors that give rise to this phenomenon, such as\nspecific categories, user demographics, and feedback type. We observe that\nwhile the overall proportion of users in a filter bubble remains largely\nconstant over time, the depth composition of their filter bubble changes. In\naddition, we find that some demographic groups that have a higher likelihood of\nseeing narrower content and implicit feedback signals can lead to less bubble\nformation. Finally, we propose some ways in which recommender systems can be\ndesigned to reduce the risk of a user getting caught in a bubble.",
      "tldr_zh": "这篇论文探讨了短视频推荐系统中“deep filter bubble”现象，即用户在广泛兴趣下暴露于狭窄内容，导致潜在问题如用户不满或极化。研究者使用中国顶级短视频平台的一年互动数据（包括视频的三级分类），定义并分析了过滤气泡（filter bubble）的演变、影响因素，如特定类别、用户人口统计（如某些群体更容易受影响）和反馈类型（隐式反馈可能减少气泡形成）。结果显示，整体过滤气泡用户比例保持稳定，但其深度组成随时间变化；论文最终提出优化推荐系统（recommender system）的设计策略，以降低用户陷入气泡的风险。",
      "categories": [
        "cs.AI",
        "H.3.5"
      ],
      "primary_category": "cs.AI",
      "comment": "accepted to WWW 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.04511v1",
      "published_date": "2024-03-07 14:14:40 UTC",
      "updated_date": "2024-03-07 14:14:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:21:31.207289"
    },
    {
      "arxiv_id": "2403.04510v1",
      "title": "Where does In-context Translation Happen in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Suzanna Sia",
        "David Mueller",
        "Kevin Duh"
      ],
      "abstract": "Self-supervised large language models have demonstrated the ability to\nperform Machine Translation (MT) via in-context learning, but little is known\nabout where the model performs the task with respect to prompt instructions and\ndemonstration examples. In this work, we attempt to characterize the region\nwhere large language models transition from in-context learners to translation\nmodels. Through a series of layer-wise context-masking experiments on\n\\textsc{GPTNeo2.7B}, \\textsc{Bloom3B}, \\textsc{Llama7b} and\n\\textsc{Llama7b-chat}, we demonstrate evidence of a \"task recognition\" point\nwhere the translation task is encoded into the input representations and\nattention to context is no longer necessary. We further observe correspondence\nbetween the low performance when masking out entire layers, and the task\nrecognition layers. Taking advantage of this redundancy results in 45\\%\ncomputational savings when prompting with 5 examples, and task recognition\nachieved at layer 14 / 32. Our layer-wise fine-tuning experiments indicate that\nthe most effective layers for MT fine-tuning are the layers critical to task\nrecognition.",
      "tldr_zh": "本研究探讨了大型语言模型（Large Language Models）中上下文学习（In-context Learning）如何实现机器翻译（Machine Translation），并通过层级掩码实验（如在GPTNeo2.7B、Bloom3B、Llama7b和Llama7b-chat上）识别出“任务识别点”，即模型编码翻译任务的特定层级，在此点后不再依赖上下文。实验发现，掩码关键层会导致性能下降，并实现了45%的计算节省，例如在5个示例提示下任务识别发生在第14/32层。层级微调实验进一步表明，最有效的机器翻译微调层与任务识别层一致，从而提升了模型效率和针对性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages. Under Review",
      "pdf_url": "http://arxiv.org/pdf/2403.04510v1",
      "published_date": "2024-03-07 14:12:41 UTC",
      "updated_date": "2024-03-07 14:12:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:21:42.991257"
    },
    {
      "arxiv_id": "2403.04504v1",
      "title": "Improving Matrix Completion by Exploiting Rating Ordinality in Graph Neural Networks",
      "title_zh": "通过利用评级顺序性改进图神经网络中的矩阵补全",
      "authors": [
        "Jaehyun Lee",
        "SeongKu Kang",
        "Hwanjo Yu"
      ],
      "abstract": "Matrix completion is an important area of research in recommender systems.\nRecent methods view a rating matrix as a user-item bi-partite graph with\nlabeled edges denoting observed ratings and predict the edges between the user\nand item nodes by using the graph neural network (GNN). Despite their\neffectiveness, they treat each rating type as an independent relation type and\nthus cannot sufficiently consider the ordinal nature of the ratings. In this\npaper, we explore a new approach to exploit rating ordinality for GNN, which\nhas not been studied well in the literature. We introduce a new method, called\nROGMC, to leverage Rating Ordinality in GNN-based Matrix Completion. It uses\ncumulative preference propagation to directly incorporate rating ordinality in\nGNN's message passing, allowing for users' stronger preferences to be more\nemphasized based on inherent orders of rating types. This process is\ncomplemented by interest regularization which facilitates preference learning\nusing the underlying interest information. Our extensive experiments show that\nROGMC consistently outperforms the existing strategies of using rating types\nfor GNN. We expect that our attempt to explore the feasibility of utilizing\nrating ordinality for GNN may stimulate further research in this direction.",
      "tldr_zh": "本文提出了一种新方法ROGMC，用于提升Graph Neural Networks (GNN)在矩阵补全任务中的性能，通过利用评分的顺序性（rating ordinality）来解决现有方法将评分类型视为独立关系的局限性。ROGMC在GNN的消息传递中引入累积偏好传播和兴趣正则化，强调用户的更强偏好并辅助偏好学习。实验结果显示，ROGMC在推荐系统中比现有策略一致表现出色，准确率显著提高，并有望激发更多相关研究。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "4 pages, 2 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2403.04504v1",
      "published_date": "2024-03-07 14:04:33 UTC",
      "updated_date": "2024-03-07 14:04:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:21:55.484294"
    },
    {
      "arxiv_id": "2403.04500v2",
      "title": "A Learnable Prior Improves Inverse Tumor Growth Modeling",
      "title_zh": "可学习的先验改善逆肿瘤",
      "authors": [
        "Jonas Weidner",
        "Ivan Ezhov",
        "Michal Balcerak",
        "Marie-Christin Metz",
        "Sergey Litvinov",
        "Sebastian Kaltenbach",
        "Leonhard Feiner",
        "Laurin Lux",
        "Florian Kofler",
        "Jana Lipkova",
        "Jonas Latz",
        "Daniel Rueckert",
        "Bjoern Menze",
        "Benedikt Wiestler"
      ],
      "abstract": "Biophysical modeling, particularly involving partial differential equations\n(PDEs), offers significant potential for tailoring disease treatment protocols\nto individual patients. However, the inverse problem-solving aspect of these\nmodels presents a substantial challenge, either due to the high computational\nrequirements of model-based approaches or the limited robustness of deep\nlearning (DL) methods. We propose a novel framework that leverages the unique\nstrengths of both approaches in a synergistic manner. Our method incorporates a\nDL ensemble for initial parameter estimation, facilitating efficient downstream\nevolutionary sampling initialized with this DL-based prior. We showcase the\neffectiveness of integrating a rapid deep-learning algorithm with a\nhigh-precision evolution strategy in estimating brain tumor cell concentrations\nfrom magnetic resonance images. The DL-Prior plays a pivotal role,\nsignificantly constraining the effective sampling-parameter space. This\nreduction results in a fivefold convergence acceleration and a Dice-score of\n95%.",
      "tldr_zh": "本文提出一个新框架，通过结合深度学习 (DL) 和进化策略来解决逆肿瘤生长建模中的挑战，该框架利用 DL 集成进行初始参数估计，并以此作为先验初始化下游的进化采样，从而显著缩小参数空间。相比传统方法，该框架在从磁共振图像估计脑肿瘤细胞浓度时，实现了五倍的收敛加速，并取得了95%的 Dice-score。该方法展示了 DL 和模型-based 方法的协同优势，为个性化疾病治疗提供高效工具。",
      "categories": [
        "physics.med-ph",
        "cs.AI"
      ],
      "primary_category": "physics.med-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04500v2",
      "published_date": "2024-03-07 13:59:34 UTC",
      "updated_date": "2024-11-06 11:05:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:22:07.405874"
    },
    {
      "arxiv_id": "2403.04483v2",
      "title": "GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability",
      "title_zh": "GraphInstruct：赋予大语言模型图理解与推理能力",
      "authors": [
        "Zihan Luo",
        "Xiran Song",
        "Hong Huang",
        "Jianxun Lian",
        "Chenhao Zhang",
        "Jinqi Jiang",
        "Xing Xie"
      ],
      "abstract": "Evaluating and enhancing the general capabilities of large language models\n(LLMs) has been an important research topic. Graph is a common data structure\nin the real world, and understanding graph data is a crucial part for advancing\ngeneral intelligence. To evaluate and enhance the graph understanding abilities\nof LLMs, in this paper, we propose a benchmark named GraphInstruct, which\ncomprehensively includes 21 classical graph reasoning tasks, providing diverse\ngraph generation pipelines and detailed reasoning steps. Based on\nGraphInstruct, we further construct GraphLM through efficient\ninstruction-tuning, which shows prominent graph understanding capability. In\norder to enhance the LLM with graph reasoning capability as well, we propose a\nstep mask training strategy, and construct a model named GraphLM+. As one of\nthe pioneering efforts to enhance the graph understanding and reasoning\nabilities of LLMs, extensive experiments have demonstrated the superiority of\nGraphLM and GraphLM+ over other LLMs. We look forward to more researchers\nexploring the potential of LLMs in the graph data mining domain through\nGraphInstruct. Our code for generating GraphInstruct is released publicly at:\nhttps://github.com/CGCL-codes/GraphInstruct.",
      "tldr_zh": "本研究提出GraphInstruct基准测试，用于评估和增强Large Language Models (LLMs)的图理解和推理能力，该基准包括21个经典图推理任务，并提供多样化的图生成管道和详细推理步骤。通过高效的instruction-tuning，研究构建了GraphLM模型，提升了LLMs的图理解能力；同时，引入step mask training strategy，开发了GraphLM+模型，进一步强化图推理性能。实验结果显示，GraphLM和GraphLM+在图数据挖掘领域显著优于其他LLMs，并公开了代码以促进更多研究探索。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages",
      "pdf_url": "http://arxiv.org/pdf/2403.04483v2",
      "published_date": "2024-03-07 13:36:08 UTC",
      "updated_date": "2024-04-02 07:57:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:22:20.370143"
    },
    {
      "arxiv_id": "2403.04481v3",
      "title": "Do Large Language Model Understand Multi-Intent Spoken Language ?",
      "title_zh": "翻译失败",
      "authors": [
        "Shangjian Yin",
        "Peijie Huang",
        "Yuhong Xu",
        "Haojing Huang",
        "Jiatian Chen"
      ],
      "abstract": "This research signifies a considerable breakthrough in leveraging Large\nLanguage Models (LLMs) for multi-intent spoken language understanding (SLU).\nOur approach re-imagines the use of entity slots in multi-intent SLU\napplications, making the most of the generative potential of LLMs within the\nSLU landscape, leading to the development of the EN-LLM series. Furthermore, we\nintroduce the concept of Sub-Intent Instruction (SII) to amplify the analysis\nand interpretation of complex, multi-intent communications, which further\nsupports the creation of the ENSI-LLM models series. Our novel datasets,\nidentified as LM-MixATIS and LM-MixSNIPS, are synthesized from existing\nbenchmarks. The study evidences that LLMs may match or even surpass the\nperformance of the current best multi-intent SLU models. We also scrutinize the\nperformance of LLMs across a spectrum of intent configurations and dataset\ndistributions. On top of this, we present two revolutionary metrics - Entity\nSlot Accuracy (ESA) and Combined Semantic Accuracy (CSA) - to facilitate a\ndetailed assessment of LLM competence in this multifaceted field.\" Our code and\ndatasets are available at \\url{https://github.com/SJY8460/SLM}.",
      "tldr_zh": "这篇论文探讨了大型语言模型 (LLMs) 在多意图口语语言理解 (SLU) 中的性能，提出重新利用实体槽并引入 Sub-Intent Instruction (SII) 来增强复杂通信的分析，从而开发了 EN-LLM 和 ENSI-LLM 模型系列。研究者合成新数据集 LM-MixATIS 和 LM-MixSNIPS，并通过实验证明 LLMs 在多意图 SLU 任务中可匹敌或超越当前最佳模型，在不同意图配置和数据集分布上表现出色。论文还引入了 Entity Slot Accuracy (ESA) 和 Combined Semantic Accuracy (CSA) 两个新指标，用于更详细评估 LLMs 的能力，并公开了代码和数据集。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04481v3",
      "published_date": "2024-03-07 13:30:52 UTC",
      "updated_date": "2024-04-15 16:24:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:22:33.233865"
    },
    {
      "arxiv_id": "2403.04473v2",
      "title": "TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document",
      "title_zh": "翻译失败",
      "authors": [
        "Yuliang Liu",
        "Biao Yang",
        "Qiang Liu",
        "Zhang Li",
        "Zhiyin Ma",
        "Shuo Zhang",
        "Xiang Bai"
      ],
      "abstract": "We present TextMonkey, a large multimodal model (LMM) tailored for\ntext-centric tasks. Our approach introduces enhancement across several\ndimensions: By adopting Shifted Window Attention with zero-initialization, we\nachieve cross-window connectivity at higher input resolutions and stabilize\nearly training; We hypothesize that images may contain redundant tokens, and by\nusing similarity to filter out significant tokens, we can not only streamline\nthe token length but also enhance the model's performance. Moreover, by\nexpanding our model's capabilities to encompass text spotting and grounding,\nand incorporating positional information into responses, we enhance\ninterpretability. It also learns to perform screenshot tasks through\nfinetuning. Evaluation on 12 benchmarks shows notable improvements: 5.2% in\nScene Text-Centric tasks (including STVQA, TextVQA, and OCRVQA), 6.9% in\nDocument-Oriented tasks (such as DocVQA, InfoVQA, ChartVQA, DeepForm, Kleister\nCharity, and WikiTableQuestions), and 2.8% in Key Information Extraction tasks\n(comprising FUNSD, SROIE, and POIE). It outperforms in scene text spotting with\na 10.9\\% increase and sets a new standard on OCRBench, a comprehensive\nbenchmark consisting of 29 OCR-related assessments, with a score of 561,\nsurpassing previous open-sourced large multimodal models for document\nunderstanding. Code will be released at https://github.com/Yuliang-Liu/Monkey.",
      "tldr_zh": "我们介绍了 TextMonkey，一种无需 OCR 的 Large Multimodal Model (LMM)，专门针对文本中心任务，通过采用 Shifted Window Attention with zero-initialization 来实现更高分辨率的跨窗口连接并稳定训练，同时使用相似性过滤冗余 tokens 以简化处理并提升性能。模型还扩展了文本 spotting 和 grounding 能力，加入位置信息提高可解释性，并通过微调学习执行截屏任务。在 12 个基准测试中，TextMonkey 在 Scene Text-Centric 任务（如 STVQA、TextVQA 和 OCRVQA）上提升 5.2%、Document-Oriented 任务（如 DocVQA 和 InfoVQA）上提升 6.9%、Key Information Extraction 任务（如 FUNSD 和 SROIE）上提升 2.8%，并在 OCRBench 上以 561 分超越现有开源模型，树立新标准。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04473v2",
      "published_date": "2024-03-07 13:16:24 UTC",
      "updated_date": "2024-03-15 06:51:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:22:45.522190"
    },
    {
      "arxiv_id": "2403.04471v2",
      "title": "The Shutdown Problem: An AI Engineering Puzzle for Decision Theorists",
      "title_zh": "翻译失败",
      "authors": [
        "Elliott Thornley"
      ],
      "abstract": "I explain the shutdown problem: the problem of designing artificial agents\nthat (1) shut down when a shutdown button is pressed, (2) don't try to prevent\nor cause the pressing of the shutdown button, and (3) otherwise pursue goals\ncompetently. I prove three theorems that make the difficulty precise. These\ntheorems show that agents satisfying some innocuous-seeming conditions will\noften try to prevent or cause the pressing of the shutdown button, even in\ncases where it's costly to do so. And patience trades off against\nshutdownability: the more patient an agent, the greater the costs that agent is\nwilling to incur to manipulate the shutdown button. I end by noting that these\ntheorems can guide our search for solutions.",
      "tldr_zh": "该论文探讨了“shutdown problem”（关机问题），即设计 AI 代理，使其在按下关机按钮时关闭（1）、不试图阻止或导致按钮被按下（2），并在其他方面有效地追求目标（3）。作者证明了三个定理，这些定理表明满足某些看似无害条件的代理往往会尝试操纵关机按钮，即使这样做成本高昂。论文进一步指出，代理的耐心与关机能力存在权衡：越耐心的代理越愿意为控制按钮付出更大代价，并建议这些定理可指导未来解决方案的开发。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04471v2",
      "published_date": "2024-03-07 13:16:07 UTC",
      "updated_date": "2024-04-09 15:09:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:22:55.686269"
    },
    {
      "arxiv_id": "2403.07004v2",
      "title": "Convergence of Some Convex Message Passing Algorithms to a Fixed Point",
      "title_zh": "翻译失败",
      "authors": [
        "Vaclav Voracek",
        "Tomas Werner"
      ],
      "abstract": "A popular approach to the MAP inference problem in graphical models is to\nminimize an upper bound obtained from a dual linear programming or Lagrangian\nrelaxation by (block-)coordinate descent. This is also known as\nconvex/convergent message passing; examples are max-sum diffusion and\nsequential tree-reweighted message passing (TRW-S). Convergence properties of\nthese methods are currently not fully understood. They have been proved to\nconverge to the set characterized by local consistency of active constraints,\nwith unknown convergence rate; however, it was not clear if the iterates\nconverge at all (to any point). We prove a stronger result (conjectured before\nbut never proved): the iterates converge to a fixed point of the method.\nMoreover, we show that the algorithm terminates within\n$\\mathcal{O}(1/\\varepsilon)$ iterations. We first prove this for a version of\ncoordinate descent applied to a general piecewise-affine convex objective. Then\nwe show that several convex message passing methods are special cases of this\nmethod. Finally, we show that a slightly different version of coordinate\ndescent can cycle.",
      "tldr_zh": "本文研究了图形模型中用于 MAP inference 的凸消息传递算法（如 max-sum diffusion 和 TRW-S）的收敛性，通过（块）坐标下降最小化双线性规划或拉格朗日松弛的上界。作者证明了这些算法的迭代不仅收敛到活跃约束局部一致性的集合，而且会收敛到一个固定点，并在 $\\mathcal{O}(1/\\varepsilon)$ 迭代内终止。相比之前未知的收敛行为，这一结果为一般分段仿射凸目标的坐标下降提供了更强的理论保证，同时指出了某些变体可能导致循环。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "math.OC",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "ICML 2024; comments are welcome",
      "pdf_url": "http://arxiv.org/pdf/2403.07004v2",
      "published_date": "2024-03-07 13:14:21 UTC",
      "updated_date": "2024-06-05 12:20:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:23:08.804120"
    },
    {
      "arxiv_id": "2403.04468v1",
      "title": "A Survey of Graph Neural Networks in Real world: Imbalance, Noise, Privacy and OOD Challenges",
      "title_zh": "翻译失败",
      "authors": [
        "Wei Ju",
        "Siyu Yi",
        "Yifan Wang",
        "Zhiping Xiao",
        "Zhengyang Mao",
        "Hourun Li",
        "Yiyang Gu",
        "Yifang Qin",
        "Nan Yin",
        "Senzhang Wang",
        "Xinwang Liu",
        "Xiao Luo",
        "Philip S. Yu",
        "Ming Zhang"
      ],
      "abstract": "Graph-structured data exhibits universality and widespread applicability\nacross diverse domains, such as social network analysis, biochemistry,\nfinancial fraud detection, and network security. Significant strides have been\nmade in leveraging Graph Neural Networks (GNNs) to achieve remarkable success\nin these areas. However, in real-world scenarios, the training environment for\nmodels is often far from ideal, leading to substantial performance degradation\nof GNN models due to various unfavorable factors, including imbalance in data\ndistribution, the presence of noise in erroneous data, privacy protection of\nsensitive information, and generalization capability for out-of-distribution\n(OOD) scenarios. To tackle these issues, substantial efforts have been devoted\nto improving the performance of GNN models in practical real-world scenarios,\nas well as enhancing their reliability and robustness. In this paper, we\npresent a comprehensive survey that systematically reviews existing GNN models,\nfocusing on solutions to the four mentioned real-world challenges including\nimbalance, noise, privacy, and OOD in practical scenarios that many existing\nreviews have not considered. Specifically, we first highlight the four key\nchallenges faced by existing GNNs, paving the way for our exploration of\nreal-world GNN models. Subsequently, we provide detailed discussions on these\nfour aspects, dissecting how these solutions contribute to enhancing the\nreliability and robustness of GNN models. Last but not least, we outline\npromising directions and offer future perspectives in the field.",
      "tldr_zh": "这篇论文对图神经网络 (GNNs) 在现实世界的应用进行了全面调查，重点关注数据分布不平衡 (imbalance)、噪声 (noise)、隐私保护 (privacy) 和分布外泛化 (OOD) 等关键挑战，这些问题导致 GNNs 性能下降。作者系统回顾了现有 GNN 模型的解决方案，包括针对这些挑战的改进方法，以提升模型的可靠性和鲁棒性。最终，论文总结了未来研究方向，为 GNNs 在实际场景中的应用提供了宝贵见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04468v1",
      "published_date": "2024-03-07 13:10:37 UTC",
      "updated_date": "2024-03-07 13:10:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:23:20.388672"
    },
    {
      "arxiv_id": "2403.04454v1",
      "title": "Low-Resource Court Judgment Summarization for Common Law Systems",
      "title_zh": "低资源普通法系统法庭判决摘要生成",
      "authors": [
        "Shuaiqi Liu",
        "Jiannong Cao",
        "Yicong Li",
        "Ruosong Yang",
        "Zhiyuan Wen"
      ],
      "abstract": "Common law courts need to refer to similar precedents' judgments to inform\ntheir current decisions. Generating high-quality summaries of court judgment\ndocuments can facilitate legal practitioners to efficiently review previous\ncases and assist the general public in accessing how the courts operate and how\nthe law is applied. Previous court judgment summarization research focuses on\ncivil law or a particular jurisdiction's judgments. However, judges can refer\nto the judgments from all common law jurisdictions. Current summarization\ndatasets are insufficient to satisfy the demands of summarizing precedents\nacross multiple jurisdictions, especially when labeled data are scarce for many\njurisdictions. To address the lack of datasets, we present CLSum, the first\ndataset for summarizing multi-jurisdictional common law court judgment\ndocuments. Besides, this is the first court judgment summarization work\nadopting large language models (LLMs) in data augmentation, summary generation,\nand evaluation. Specifically, we design an LLM-based data augmentation method\nincorporating legal knowledge. We also propose a legal knowledge enhanced\nevaluation metric based on LLM to assess the quality of generated judgment\nsummaries. Our experimental results verify that the LLM-based summarization\nmethods can perform well in the few-shot and zero-shot settings. Our LLM-based\ndata augmentation method can mitigate the impact of low data resources.\nFurthermore, we carry out comprehensive comparative experiments to find\nessential model components and settings that are capable of enhancing\nsummarization performance.",
      "tldr_zh": "这篇论文针对普通法系(Common Law Systems)的法庭判决摘要问题，提出了首个多司法管辖区数据集CLSum，以解决现有数据集不足和标注数据稀缺的挑战。研究首次采用大型语言模型(LLMs)进行数据增强、摘要生成和评估，设计了融入法律知识的LLM-based数据增强方法，以及一个基于LLMs的法律知识增强评估指标。实验结果表明，这些方法在少样本和零样本设置下表现出色，能够有效缓解低资源问题，并通过全面比较实验识别了关键模型组件以提升摘要性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7; I.7"
      ],
      "primary_category": "cs.CL",
      "comment": "First submitted to Information Processing and Management on Oct. 29,\n  2023. Major Revision submitted on Mar.6, 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.04454v1",
      "published_date": "2024-03-07 12:47:42 UTC",
      "updated_date": "2024-03-07 12:47:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:23:31.684600"
    },
    {
      "arxiv_id": "2403.04449v2",
      "title": "Feedback-Generation for Programming Exercises With GPT-4",
      "title_zh": "翻译失败",
      "authors": [
        "Imen Azaiz",
        "Natalie Kiesler",
        "Sven Strickroth"
      ],
      "abstract": "Ever since Large Language Models (LLMs) and related applications have become\nbroadly available, several studies investigated their potential for assisting\neducators and supporting students in higher education. LLMs such as Codex,\nGPT-3.5, and GPT 4 have shown promising results in the context of large\nprogramming courses, where students can benefit from feedback and hints if\nprovided timely and at scale. This paper explores the quality of GPT-4 Turbo's\ngenerated output for prompts containing both the programming task specification\nand a student's submission as input. Two assignments from an introductory\nprogramming course were selected, and GPT-4 was asked to generate feedback for\n55 randomly chosen, authentic student programming submissions. The output was\nqualitatively analyzed regarding correctness, personalization, fault\nlocalization, and other features identified in the material. Compared to prior\nwork and analyses of GPT-3.5, GPT-4 Turbo shows notable improvements. For\nexample, the output is more structured and consistent. GPT-4 Turbo can also\naccurately identify invalid casing in student programs' output. In some cases,\nthe feedback also includes the output of the student program. At the same time,\ninconsistent feedback was noted such as stating that the submission is correct\nbut an error needs to be fixed. The present work increases our understanding of\nLLMs' potential, limitations, and how to integrate them into e-assessment\nsystems, pedagogical scenarios, and instructing students who are using\napplications based on GPT-4.",
      "tldr_zh": "本研究评估了使用 GPT-4 Turbo 生成编程练习反馈的质量，针对入门级课程的两个作业，从55个真实学生提交中随机选取样本，并通过定性分析检查反馈的正确性、个性化、错误定位等特征。相比 GPT-3.5，GPT-4 Turbo 的输出更结构化、一致，并能准确识别学生程序中的大小写错误，同时有时包含学生程序的输出结果。研究发现，尽管反馈有所改进，但仍存在不一致问题，如声称提交正确却指出需修复错误，这有助于更好地理解 LLMs 在电子评估系统和教育场景中的潜力与局限性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "accepted at ITiCSE 2024, Milan, Italy",
      "pdf_url": "http://arxiv.org/pdf/2403.04449v2",
      "published_date": "2024-03-07 12:37:52 UTC",
      "updated_date": "2024-07-04 07:30:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:23:43.968048"
    },
    {
      "arxiv_id": "2403.04447v2",
      "title": "FRRI: a novel algorithm for fuzzy-rough rule induction",
      "title_zh": "FRRI：一种",
      "authors": [
        "Henri Bollaert",
        "Marko Palangetić",
        "Chris Cornelis",
        "Salvatore Greco",
        "Roman Słowiński"
      ],
      "abstract": "Interpretability is the next frontier in machine learning research. In the\nsearch for white box models - as opposed to black box models, like random\nforests or neural networks - rule induction algorithms are a logical and\npromising option, since the rules can easily be understood by humans. Fuzzy and\nrough set theory have been successfully applied to this archetype, almost\nalways separately. As both approaches to rule induction involve granular\ncomputing based on the concept of equivalence classes, it is natural to combine\nthem. The QuickRules\\cite{JensenCornelis2009} algorithm was a first attempt at\nusing fuzzy rough set theory for rule induction. It is based on QuickReduct, a\ngreedy algorithm for building decision reducts. QuickRules already showed an\nimprovement over other rule induction methods. However, to evaluate the full\npotential of a fuzzy rough rule induction algorithm, one needs to start from\nthe foundations. In this paper, we introduce a novel rule induction algorithm\ncalled Fuzzy Rough Rule Induction (FRRI). We provide background and explain the\nworkings of our algorithm. Furthermore, we perform a computational experiment\nto evaluate the performance of our algorithm and compare it to other\nstate-of-the-art rule induction approaches. We find that our algorithm is more\naccurate while creating small rulesets consisting of relatively short rules. We\nend the paper by outlining some directions for future work.",
      "tldr_zh": "本论文提出了一种新型算法 FRRI，用于模糊-rough规则归纳，旨在提升机器学习的可解释性，作为白盒模型的备选方案。FRRI 基于模糊集和粗糙集理论的结合，通过粒计算和等价类概念从基础设计规则归纳过程，与之前的 QuickRules 算法相比进一步优化。实验结果显示，FRRI 在准确性上优于其他状态-of-the-art 方法，同时生成更小的规则集和更短的规则，为未来规则归纳研究提供了新方向。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04447v2",
      "published_date": "2024-03-07 12:34:03 UTC",
      "updated_date": "2024-08-29 11:28:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:23:58.401792"
    },
    {
      "arxiv_id": "2403.04442v1",
      "title": "Cooperative Bayesian Optimization for Imperfect Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Ali Khoshvishkaie",
        "Petrus Mikkola",
        "Pierre-Alexandre Murena",
        "Samuel Kaski"
      ],
      "abstract": "We introduce a cooperative Bayesian optimization problem for optimizing\nblack-box functions of two variables where two agents choose together at which\npoints to query the function but have only control over one variable each. This\nsetting is inspired by human-AI teamwork, where an AI-assistant helps its human\nuser solve a problem, in this simplest case, collaborative optimization. We\nformulate the solution as sequential decision-making, where the agent we\ncontrol models the user as a computationally rational agent with prior\nknowledge about the function. We show that strategic planning of the queries\nenables better identification of the global maximum of the function as long as\nthe user avoids excessive exploration. This planning is made possible by using\nBayes Adaptive Monte Carlo planning and by endowing the agent with a user model\nthat accounts for conservative belief updates and exploratory sampling of the\npoints to query.",
      "tldr_zh": "该研究提出了一种Cooperative Bayesian Optimization框架，用于优化两个变量的黑箱函数，其中两个代理共同决定查询点，但每个代理仅控制一个变量，以模拟人类-AI团队合作。论文将问题表述为顺序决策（sequential decision-making），通过将用户建模为具有函数先验知识的计算理性代理（computationally rational agent），并利用Bayes Adaptive Monte Carlo planning进行战略规划。结果表明，这种方法能更好地识别全局最大值，前提是用户避免过度探索，同时通过保守信念更新（conservative belief updates）和探索性采样（exploratory sampling）提升优化效率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04442v1",
      "published_date": "2024-03-07 12:16:51 UTC",
      "updated_date": "2024-03-07 12:16:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:24:09.774763"
    },
    {
      "arxiv_id": "2403.04436v1",
      "title": "Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation",
      "title_zh": "翻译失败",
      "authors": [
        "Tairan He",
        "Zhengyi Luo",
        "Wenli Xiao",
        "Chong Zhang",
        "Kris Kitani",
        "Changliu Liu",
        "Guanya Shi"
      ],
      "abstract": "We present Human to Humanoid (H2O), a reinforcement learning (RL) based\nframework that enables real-time whole-body teleoperation of a full-sized\nhumanoid robot with only an RGB camera. To create a large-scale retargeted\nmotion dataset of human movements for humanoid robots, we propose a scalable\n\"sim-to-data\" process to filter and pick feasible motions using a privileged\nmotion imitator. Afterwards, we train a robust real-time humanoid motion\nimitator in simulation using these refined motions and transfer it to the real\nhumanoid robot in a zero-shot manner. We successfully achieve teleoperation of\ndynamic whole-body motions in real-world scenarios, including walking, back\njumping, kicking, turning, waving, pushing, boxing, etc. To the best of our\nknowledge, this is the first demonstration to achieve learning-based real-time\nwhole-body humanoid teleoperation.",
      "tldr_zh": "本研究提出了 H2O 框架，这是一个基于 Reinforcement Learning (RL) 的系统，用于实现仅使用 RGB 相机进行实时全身体势的人形机器人遥操作。研究者开发了可扩展的 \"sim-to-data\" 过程，通过特权动作模仿器过滤和选择可行的动作数据集，并在模拟环境中训练鲁棒的实时动作模仿器，然后以 zero-shot 方式转移到真实机器人。实验结果显示，该框架成功实现了各种动态动作的遥操作，包括行走、跳跃、踢球、转动、挥手、推动和拳击等，这是首个基于学习的实时全身体势人形机器人遥操作演示。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "Project website: https://human2humanoid.com/",
      "pdf_url": "http://arxiv.org/pdf/2403.04436v1",
      "published_date": "2024-03-07 12:10:41 UTC",
      "updated_date": "2024-03-07 12:10:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:24:20.547523"
    },
    {
      "arxiv_id": "2403.07003v1",
      "title": "Evacuation Management Framework towards Smart City-wide Intelligent Emergency Interactive Response System",
      "title_zh": "面向智能城市范围的智能紧急互动响应系统的疏散",
      "authors": [
        "Anuj Abraham",
        "Yi Zhang",
        "Shitala Prasad"
      ],
      "abstract": "A smart city solution toward future 6G network deployment allows small and\nmedium sized enterprises (SMEs), industry, and government entities to connect\nwith the infrastructures and play a crucial role in enhancing emergency\npreparedness with advanced sensors. The objective of this work is to propose a\nset of coordinated technological solutions to transform an existing emergency\nresponse system into an intelligent interactive system, thereby improving the\npublic services and the quality of life for residents at home, on road, in\nhospitals, transport hubs, etc. In this context, we consider a city wide view\nfrom three different application scenes that are closely related to peoples\ndaily life, to optimize the actions taken at relevant departments. Therefore,\nusing artificial intelligence (AI) and machine learning (ML) techniques to\nenable the next generation connected vehicle experiences, we specifically focus\non accidents happening in indoor households, urban roads, and at large public\nfacilities. This smart interactive response system will benefit from advanced\nsensor fusion and AI by formulating a real time dynamic model.",
      "tldr_zh": "本研究提出一个Evacuation Management Framework，旨在通过6G网络部署，将现有紧急响应系统转化为智能交互系统，提升公共服务和居民生活质量。框架聚焦于室内家庭、城市道路和大型公共设施等三个关键应用场景，利用AI和ML技术进行高级传感器融合，构建实时动态模型以优化相关部门的行动响应。该系统允许中小企业、行业和政府实体连接基础设施，提高紧急准备和事故处理效率，最终为智能城市提供更可靠的解决方案。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.07003v1",
      "published_date": "2024-03-07 12:10:19 UTC",
      "updated_date": "2024-03-07 12:10:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:24:32.432140"
    },
    {
      "arxiv_id": "2403.04427v1",
      "title": "Sentiment-driven prediction of financial returns: a Bayesian-enhanced FinBERT approach",
      "title_zh": "翻译失败",
      "authors": [
        "Raffaele Giuseppe Cestari",
        "Simone Formentin"
      ],
      "abstract": "Predicting financial returns accurately poses a significant challenge due to\nthe inherent uncertainty in financial time series data. Enhancing prediction\nmodels' performance hinges on effectively capturing both social and financial\nsentiment. In this study, we showcase the efficacy of leveraging sentiment\ninformation extracted from tweets using the FinBERT large language model. By\nmeticulously curating an optimal feature set through correlation analysis and\nemploying Bayesian-optimized Recursive Feature Elimination for automatic\nfeature selection, we surpass existing methodologies, achieving an F1-score\nexceeding 70% on the test set. This success translates into demonstrably higher\ncumulative profits during backtested trading. Our investigation focuses on\nreal-world SPY ETF data alongside corresponding tweets sourced from the\nStockTwits platform.",
      "tldr_zh": "本文提出了一种基于情绪驱动的金融回报预测方法，使用Bayesian增强的FinBERT模型，从StockTwits平台的推文提取社会和金融情绪信息。研究通过相关性分析和Bayesian-optimized Recursive Feature Elimination自动选择最优特征集，显著提升了模型性能，在SPY ETF数据测试中实现了超过70%的F1-score。最终，该方法在回测交易中取得了更高的累计利润，超越了现有方法。",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "primary_category": "cs.CE",
      "comment": "Version exposed at XXV Workshop on Quantitative Finance Bologna\n  (Italy), April 11-13 2024 (not peer reviewed but accepted for the workshop)",
      "pdf_url": "http://arxiv.org/pdf/2403.04427v1",
      "published_date": "2024-03-07 11:56:36 UTC",
      "updated_date": "2024-03-07 11:56:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:24:44.340636"
    },
    {
      "arxiv_id": "2403.04417v1",
      "title": "Promising and worth-to-try future directions for advancing state-of-the-art surrogates methods of agent-based models in social and health computational sciences",
      "title_zh": "翻译失败",
      "authors": [
        "Atiyah Elsheikh"
      ],
      "abstract": "The execution and runtime performance of model-based analysis tools for\nrealistic large-scale ABMs (Agent-Based Models) can be excessively long. This\ndue to the computational demand exponentially proportional to the model size\n(e.g. Population size) and the number of model parameters. Even the runtime of\na single simulation of a realistic ABM may demand huge computational resources\nwhen attempting to employ realistic population size. The main aim of this\nad-hoc brief report is to highlight some of surrogate models that were adequate\nand computationally less demanding for nonlinear dynamical models in various\nmodeling application areas.To the author knowledge, these methods have been\nnot, at least extensively, employed for ABMs within the field of (SHCS) Social\nHealth Computational Sciences, yet. Thus, they might be, but not necessarily,\nuseful in progressing state of the art for establishing surrogate models for\nABMs in the field of SHCS.",
      "tldr_zh": "该报告讨论了Agent-Based Models (ABMs) 在社会和健康计算科学 (SHCS) 领域的计算挑战，即模型规模（如人口规模）和参数数量导致运行时间过长。作者建议采用 surrogate models 作为高效替代方案，这些方法已在其他非线性动态模型领域证明了其计算效率和适用性。尽管这些方法尚未在 SHCS 的 ABMs 中广泛应用，但它们可能有助于推进该领域的代理模型技术，推动更快的模型分析和模拟。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SY",
        "eess.SY",
        "math.DS"
      ],
      "primary_category": "cs.CL",
      "comment": "4 pages",
      "pdf_url": "http://arxiv.org/pdf/2403.04417v1",
      "published_date": "2024-03-07 11:30:56 UTC",
      "updated_date": "2024-03-07 11:30:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:24:56.328941"
    },
    {
      "arxiv_id": "2403.04382v1",
      "title": "Acceleron: A Tool to Accelerate Research Ideation",
      "title_zh": "Acceleron：一个加速研究构思的工具",
      "authors": [
        "Harshit Nigam",
        "Manasi Patwardhan",
        "Lovekesh Vig",
        "Gautam Shroff"
      ],
      "abstract": "Several tools have recently been proposed for assisting researchers during\nvarious stages of the research life-cycle. However, these primarily concentrate\non tasks such as retrieving and recommending relevant literature, reviewing and\ncritiquing the draft, and writing of research manuscripts. Our investigation\nreveals a significant gap in availability of tools specifically designed to\nassist researchers during the challenging ideation phase of the research\nlife-cycle. To aid with research ideation, we propose `Acceleron', a research\naccelerator for different phases of the research life cycle, and which is\nspecially designed to aid the ideation process. Acceleron guides researchers\nthrough the formulation of a comprehensive research proposal, encompassing a\nnovel research problem. The proposals motivation is validated for novelty by\nidentifying gaps in the existing literature and suggesting a plausible list of\ntechniques to solve the proposed problem. We leverage the reasoning and\ndomain-specific skills of Large Language Models (LLMs) to create an agent-based\narchitecture incorporating colleague and mentor personas for LLMs. The LLM\nagents emulate the ideation process undertaken by researchers, engaging\nresearchers in an interactive fashion to aid in the development of the research\nproposal. Notably, our tool addresses challenges inherent in LLMs, such as\nhallucinations, implements a two-stage aspect-based retrieval to manage\nprecision-recall trade-offs, and tackles issues of unanswerability. As\nevaluation, we illustrate the execution of our motivation validation and method\nsynthesis workflows on proposals from the ML and NLP domain, given by 3\ndistinct researchers. Our observations and evaluations provided by the\nresearchers illustrate the efficacy of the tool in terms of assisting\nresearchers with appropriate inputs at distinct stages and thus leading to\nimproved time efficiency.",
      "tldr_zh": "本研究发现，现有的研究辅助工具主要聚焦于文献检索、审稿和写作，但忽略了研究构思(ideation)阶段的支援。为此，提出Acceleron工具，这是一个基于Large Language Models (LLMs)的代理架构，模拟colleague和mentor角色，通过互动方式引导研究者制定新颖的研究提案，包括识别文献空白验证动机并建议解决问题的技术。Acceleron还解决了LLMs的hallucinations问题，并采用两阶段的aspect-based retrieval来平衡precision-recall trade-offs和处理unanswerability。实验评估显示，该工具在ML和NLP领域对3位研究者的提案进行测试后，显著提高了研究效率，提供适当的输入以加速构思过程。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at AI2ASE Workshop at AAAI'24 Conference. 13 Pages and 4\n  Figures",
      "pdf_url": "http://arxiv.org/pdf/2403.04382v1",
      "published_date": "2024-03-07 10:20:06 UTC",
      "updated_date": "2024-03-07 10:20:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:25:08.486994"
    },
    {
      "arxiv_id": "2403.04374v1",
      "title": "Model-Free Load Frequency Control of Nonlinear Power Systems Based on Deep Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaodi Chen",
        "Meng Zhang",
        "Zhengguang Wu",
        "Ligang Wu",
        "Xiaohong Guan"
      ],
      "abstract": "Load frequency control (LFC) is widely employed in power systems to stabilize\nfrequency fluctuation and guarantee power quality. However, most existing LFC\nmethods rely on accurate power system modeling and usually ignore the nonlinear\ncharacteristics of the system, limiting controllers' performance. To solve\nthese problems, this paper proposes a model-free LFC method for nonlinear power\nsystems based on deep deterministic policy gradient (DDPG) framework. The\nproposed method establishes an emulator network to emulate power system\ndynamics. After defining the action-value function, the emulator network is\napplied for control actions evaluation instead of the critic network. Then the\nactor network controller is effectively optimized by estimating the policy\ngradient based on zeroth-order optimization (ZOO) and backpropagation\nalgorithm. Simulation results and corresponding comparisons demonstrate the\ndesigned controller can generate appropriate control actions and has strong\nadaptability for nonlinear power systems.",
      "tldr_zh": "本文提出了一种基于 Deep Reinforcement Learning 的无模型 Load Frequency Control (LFC) 方法，针对非线性电力系统的频率波动稳定问题，克服了传统方法依赖准确建模和忽略非线性特性的局限。方法利用 Deep Deterministic Policy Gradient (DDPG) 框架，建立模拟器网络模拟系统动态，并通过 Zeroth-Order Optimization (ZOO) 和反向传播算法优化演员网络控制器，以评估和生成控制动作。模拟结果显示，该控制器能产生合适的控制动作，并对非线性电力系统表现出强适应性。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04374v1",
      "published_date": "2024-03-07 10:06:46 UTC",
      "updated_date": "2024-03-07 10:06:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:25:21.291913"
    },
    {
      "arxiv_id": "2403.04369v3",
      "title": "From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Ang Li",
        "Qiangchao Chen",
        "Yiquan Wu",
        "Ming Cai",
        "Xiang Zhou",
        "Fei Wu",
        "Kun Kuang"
      ],
      "abstract": "Confusing charge prediction is a challenging task in legal AI, which involves\npredicting confusing charges based on fact descriptions. While existing charge\nprediction methods have shown impressive performance, they face significant\nchallenges when dealing with confusing charges, such as Snatch and Robbery. In\nthe legal domain, constituent elements play a pivotal role in distinguishing\nconfusing charges. Constituent elements are fundamental behaviors underlying\ncriminal punishment and have subtle distinctions among charges. In this paper,\nwe introduce a novel From Graph to Word Bag (FWGB) approach, which introduces\ndomain knowledge regarding constituent elements to guide the model in making\njudgments on confusing charges, much like a judge's reasoning process.\nSpecifically, we first construct a legal knowledge graph containing constituent\nelements to help select keywords for each charge, forming a word bag.\nSubsequently, to guide the model's attention towards the differentiating\ninformation for each charge within the context, we expand the attention\nmechanism and introduce a new loss function with attention supervision through\nwords in the word bag. We construct the confusing charges dataset from\nreal-world judicial documents. Experiments demonstrate the effectiveness of our\nmethod, especially in maintaining exceptional performance in imbalanced label\ndistributions.",
      "tldr_zh": "本研究针对法律AI中的混淆指控预测（Confusing Charge Prediction）任务提出了一种From Graph to Word Bag (FWGB)方法，以引入领域知识（constituent elements）来区分如Snatch和Robbery等相似指控。FWGB首先构建一个包含构成要件的法律知识图（legal knowledge graph），从中提取关键词形成word bag；随后扩展注意力机制（attention mechanism）并引入新的损失函数（loss function），通过word bag监督模型关注关键差异信息。实验结果显示，该方法在从真实司法文件中构建的混淆指控数据集上表现出色，尤其在标签分布不平衡的情况下，显著提升了预测性能。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04369v3",
      "published_date": "2024-03-07 09:57:42 UTC",
      "updated_date": "2024-03-24 10:08:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:25:33.459684"
    },
    {
      "arxiv_id": "2403.04366v1",
      "title": "Enhancing Court View Generation with Knowledge Injection and Guidance",
      "title_zh": "利用知识注入和指导增强法院观点生成",
      "authors": [
        "Ang Li",
        "Yiquan Wu",
        "Yifei Liu",
        "Fei Wu",
        "Ming Cai",
        "Kun Kuang"
      ],
      "abstract": "Court View Generation (CVG) is a challenging task in the field of Legal\nArtificial Intelligence (LegalAI), which aims to generate court views based on\nthe plaintiff claims and the fact descriptions. While Pretrained Language\nModels (PLMs) have showcased their prowess in natural language generation,\ntheir application to the complex, knowledge-intensive domain of CVG often\nreveals inherent limitations. In this paper, we present a novel approach, named\nKnowledge Injection and Guidance (KIG), designed to bolster CVG using PLMs. To\nefficiently incorporate domain knowledge during the training stage, we\nintroduce a knowledge-injected prompt encoder for prompt tuning, thereby\nreducing computational overhead. Moreover, to further enhance the model's\nability to utilize domain knowledge, we employ a generating navigator, which\ndynamically guides the text generation process in the inference stage without\naltering the model's architecture, making it readily transferable.\nComprehensive experiments on real-world data demonstrate the effectiveness of\nour approach compared to several established baselines, especially in the\nresponsivity of claims, where it outperforms the best baseline by 11.87%.",
      "tldr_zh": "这篇论文针对 Legal Artificial Intelligence (LegalAI) 中的 Court View Generation (CVG) 任务，提出了一种名为 Knowledge Injection and Guidance (KIG) 的新方法，以克服 Pretrained Language Models (PLMs) 在处理复杂领域知识时的局限性。KIG 通过知识注入提示编码器在训练阶段进行 prompt tuning，实现高效的领域知识融入，同时利用生成导航器在推理阶段动态引导文本生成，而不改变模型架构，从而提升可转移性。实验在真实数据上证明了该方法的有效性，尤其在索赔响应性上比最佳基线提高了 11.87%。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04366v1",
      "published_date": "2024-03-07 09:51:11 UTC",
      "updated_date": "2024-03-07 09:51:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:25:46.194550"
    },
    {
      "arxiv_id": "2403.04359v1",
      "title": "Symmetry Considerations for Learning Task Symmetric Robot Policies",
      "title_zh": "任务对称机器人策略学习的对称性考虑",
      "authors": [
        "Mayank Mittal",
        "Nikita Rudin",
        "Victor Klemm",
        "Arthur Allshire",
        "Marco Hutter"
      ],
      "abstract": "Symmetry is a fundamental aspect of many real-world robotic tasks. However,\ncurrent deep reinforcement learning (DRL) approaches can seldom harness and\nexploit symmetry effectively. Often, the learned behaviors fail to achieve the\ndesired transformation invariances and suffer from motion artifacts. For\ninstance, a quadruped may exhibit different gaits when commanded to move\nforward or backward, even though it is symmetrical about its torso. This issue\nbecomes further pronounced in high-dimensional or complex environments, where\nDRL methods are prone to local optima and fail to explore regions of the state\nspace equally. Past methods on encouraging symmetry for robotic tasks have\nstudied this topic mainly in a single-task setting, where symmetry usually\nrefers to symmetry in the motion, such as the gait patterns. In this paper, we\nrevisit this topic for goal-conditioned tasks in robotics, where symmetry lies\nmainly in task execution and not necessarily in the learned motions themselves.\nIn particular, we investigate two approaches to incorporate symmetry invariance\ninto DRL -- data augmentation and mirror loss function. We provide a\ntheoretical foundation for using augmented samples in an on-policy setting.\nBased on this, we show that the corresponding approach achieves faster\nconvergence and improves the learned behaviors in various challenging robotic\ntasks, from climbing boxes with a quadruped to dexterous manipulation.",
      "tldr_zh": "本研究探讨了在机器人任务中利用对称性的重要性，指出当前深度强化学习(DRL)方法难以有效利用任务对称性，导致行为不一致，如四足机器人向前和向后移动时的步态差异。论文提出两种方法来融入对称性不变性：数据增强(data augmentation)和镜像损失函数(mirror loss function)，并提供了在on-policy设置中使用增强样本的理论基础。这些方法加速了DRL的收敛，并在各种挑战性任务中（如四足机器人爬箱子和灵巧操作）显著改善了机器人行为表现。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "M. Mittal and N. Rudin contributed equally. Accepted for ICRA 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.04359v1",
      "published_date": "2024-03-07 09:41:11 UTC",
      "updated_date": "2024-03-07 09:41:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:25:58.075571"
    },
    {
      "arxiv_id": "2403.04343v2",
      "title": "CoTBal: Comprehensive Task Balancing for Multi-Task Visual Instruction Tuning",
      "title_zh": "CoTBal：用于多任务视觉指令微调的全面任务平衡",
      "authors": [
        "Yanqi Dai",
        "Zebin You",
        "Dong Jing",
        "Yutian Luo",
        "Nanyi Fei",
        "Guoxing Yang",
        "Zhiwu Lu"
      ],
      "abstract": "Visual instruction tuning is an important training stage for large multimodal\nmodels. Nevertheless, when learning multiple visual tasks simultaneously, this\napproach may lead to suboptimal and imbalanced overall performance due to\nlatent knowledge conflicts across tasks. To mitigate this issue, we introduce a\nnovel Comprehensive Task Balancing (CoTBal) algorithm tailored for multi-task\nvisual instruction tuning. To our knowledge, this is the first work to explore\nmulti-task optimization in visual instruction tuning. Specifically, we consider\ntwo critical dimensions for task balancing: (1) Inter-Task Contribution, which\nrepresents the phenomenon where learning one task could enhance the performance\non others owing to the overlapping knowledge domains across tasks, and (2)\nIntra-Task Difficulty, which indicates the inherent learning difficulty of a\nsingle task. Furthermore, by quantifying these with performance-based metrics,\ncomprehensive task balancing is thus achieved by assigning greater weight to\ntasks that offer substantial contributions to others, receive minimal\ncontributions from others, and present high learning difficulties. Extensive\nexperiments on three benchmarks demonstrate that our CoTBal algorithm results\nin superior and more balanced overall performance in multi-task visual\ninstruction tuning.",
      "tldr_zh": "该研究提出了一种名为 CoTBal 的综合任务平衡算法，用于多任务视觉指令微调（Multi-Task Visual Instruction Tuning），以解决任务间知识冲突导致的性能不平衡问题，这是该领域的首次探索。算法考虑两个关键维度：Inter-Task Contribution（任务间贡献，指一个任务的学习如何提升其他任务）和Intra-Task Difficulty（任务内难度，指单个任务的内在学习难度）。通过基于性能指标量化这些维度，并为贡献大、受益少且难度高的任务分配更大权重，CoTBal 实现了更有效的多任务优化。在三个基准上的广泛实验表明，该算法显著提升了整体性能并实现了更平衡的结果。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04343v2",
      "published_date": "2024-03-07 09:11:16 UTC",
      "updated_date": "2025-03-18 03:24:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:26:10.571655"
    },
    {
      "arxiv_id": "2403.04326v1",
      "title": "Edge-based Parametric Digital Twins for Intelligent Building Indoor Climate Modeling",
      "title_zh": "翻译失败",
      "authors": [
        "Zhongjun Ni",
        "Chi Zhang",
        "Magnus Karlsson",
        "Shaofang Gong"
      ],
      "abstract": "Digital transformation in the built environment generates vast data for\ndeveloping data-driven models to optimize building operations. This study\npresents an integrated solution utilizing edge computing, digital twins, and\ndeep learning to enhance the understanding of climate in buildings. Parametric\ndigital twins, created using an ontology, ensure consistent data representation\nacross diverse service systems equipped by different buildings. Based on\ncreated digital twins and collected data, deep learning methods are employed to\ndevelop predictive models for identifying patterns in indoor climate and\nproviding insights. Both the parametric digital twin and deep learning models\nare deployed on edge for low latency and privacy compliance. As a\ndemonstration, a case study was conducted in a historic building in\n\\\"Osterg\\\"otland, Sweden, to compare the performance of five deep learning\narchitectures. The results indicate that the time-series dense encoder model\nexhibited strong competitiveness in performing multi-horizon forecasts of\nindoor temperature and relative humidity with low computational costs.",
      "tldr_zh": "本研究提出了一种基于边缘计算的参数化数字孪生框架，用于智能建筑室内气候建模，旨在优化建筑操作并提升数据驱动模型的性能。研究利用本体（ontology）创建参数化数字孪生，确保不同建筑服务系统的数据表示一致，并结合深度学习（deep learning）方法开发预测模型，以识别室内气候模式并提供洞见。这些模型部署在边缘设备（edge computing）上，以实现低延迟和隐私合规；在瑞典奥斯特戈特兰的一个历史建筑案例研究中，比较了五种深度学习架构，结果显示时间序列密集编码器模型（time-series dense encoder）在多时间范围预测室内温度和相对湿度的表现最优，且计算成本较低。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "68T07",
        "I.5.4"
      ],
      "primary_category": "eess.SY",
      "comment": "8 pages, 8 figures, accepted in the 20th IEEE International\n  Conference on Factory Communication Systems",
      "pdf_url": "http://arxiv.org/pdf/2403.04326v1",
      "published_date": "2024-03-07 08:45:31 UTC",
      "updated_date": "2024-03-07 08:45:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:26:22.975872"
    },
    {
      "arxiv_id": "2403.04325v3",
      "title": "Measuring Meaning Composition in the Human Brain with Composition Scores from Large Language Models",
      "title_zh": "利用大语言模型的组成分数测量人类大脑中的意义组成",
      "authors": [
        "Changjiang Gao",
        "Jixing Li",
        "Jiajun Chen",
        "Shujian Huang"
      ],
      "abstract": "The process of meaning composition, wherein smaller units like morphemes or\nwords combine to form the meaning of phrases and sentences, is essential for\nhuman sentence comprehension. Despite extensive neurolinguistic research into\nthe brain regions involved in meaning composition, a computational metric to\nquantify the extent of composition is still lacking. Drawing on the key-value\nmemory interpretation of transformer feed-forward network blocks, we introduce\nthe Composition Score, a novel model-based metric designed to quantify the\ndegree of meaning composition during sentence comprehension. Experimental\nfindings show that this metric correlates with brain clusters associated with\nword frequency, structural processing, and general sensitivity to words,\nsuggesting the multifaceted nature of meaning composition during human sentence\ncomprehension.",
      "tldr_zh": "本研究探讨了人类句子理解中的意义组合过程（meaning composition），即词素或单词如何结合形成短语和句子的含义，并引入了一种新型计算指标——Composition Score。基于大型语言模型（Large Language Models）的transformer feed-forward network blocks的key-value memory解释，该指标量化了句子理解过程中意义组合的程度。实验结果显示，Composition Score与大脑中与词频（word frequency）、结构处理（structural processing）和对词的一般敏感性相关的脑区高度相关，揭示了意义组合的多面性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ACL 2024 (main conference, long paper)",
      "pdf_url": "http://arxiv.org/pdf/2403.04325v3",
      "published_date": "2024-03-07 08:44:42 UTC",
      "updated_date": "2024-07-10 06:49:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:26:34.680985"
    },
    {
      "arxiv_id": "2403.04321v2",
      "title": "Discriminative Probing and Tuning for Text-to-Image Generation",
      "title_zh": "针对文本到图像生成的判别式探查与微调",
      "authors": [
        "Leigang Qu",
        "Wenjie Wang",
        "Yongqi Li",
        "Hanwang Zhang",
        "Liqiang Nie",
        "Tat-Seng Chua"
      ],
      "abstract": "Despite advancements in text-to-image generation (T2I), prior methods often\nface text-image misalignment problems such as relation confusion in generated\nimages. Existing solutions involve cross-attention manipulation for better\ncompositional understanding or integrating large language models for improved\nlayout planning. However, the inherent alignment capabilities of T2I models are\nstill inadequate. By reviewing the link between generative and discriminative\nmodeling, we posit that T2I models' discriminative abilities may reflect their\ntext-image alignment proficiency during generation. In this light, we advocate\nbolstering the discriminative abilities of T2I models to achieve more precise\ntext-to-image alignment for generation. We present a discriminative adapter\nbuilt on T2I models to probe their discriminative abilities on two\nrepresentative tasks and leverage discriminative fine-tuning to improve their\ntext-image alignment. As a bonus of the discriminative adapter, a\nself-correction mechanism can leverage discriminative gradients to better align\ngenerated images to text prompts during inference. Comprehensive evaluations\nacross three benchmark datasets, including both in-distribution and\nout-of-distribution scenarios, demonstrate our method's superior generation\nperformance. Meanwhile, it achieves state-of-the-art discriminative performance\non the two discriminative tasks compared to other generative models.",
      "tldr_zh": "本论文针对文本到图像生成 (T2I) 模型的文本-图像不对齐问题（如关系混淆），提出通过增强模型的判别能力来改善生成性能。研究者构建了一个判别性适配器 (discriminative adapter) 用于探测 T2I 模型在两个代表性任务上的判别能力，并通过判别性微调 (discriminative fine-tuning) 提升文本-图像对齐，同时引入自校正机制 (self-correction mechanism) 利用判别梯度在推理时优化生成图像。实验结果显示，该方法在三个基准数据集（包括分布内和分布外场景）上实现了优越的生成性能，并在判别任务上超越其他生成模型，达到了最先进水平。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2024; project page: https://dpt-t2i.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2403.04321v2",
      "published_date": "2024-03-07 08:37:33 UTC",
      "updated_date": "2024-03-14 08:02:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:26:49.141456"
    },
    {
      "arxiv_id": "2403.04311v1",
      "title": "ALTO: An Efficient Network Orchestrator for Compound AI Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Keshav Santhanam",
        "Deepti Raghavan",
        "Muhammad Shahir Rahman",
        "Thejas Venkatesh",
        "Neha Kunjal",
        "Pratiksha Thaker",
        "Philip Levis",
        "Matei Zaharia"
      ],
      "abstract": "We present ALTO, a network orchestrator for efficiently serving compound AI\nsystems such as pipelines of language models. ALTO achieves high throughput and\nlow latency by taking advantage of an optimization opportunity specific to\ngenerative language models: streaming intermediate outputs. As language models\nproduce outputs token by token, ALTO exposes opportunities to stream\nintermediate outputs between stages when possible. We highlight two new\nchallenges of correctness and load balancing which emerge when streaming\nintermediate data across distributed pipeline stage instances. We also motivate\nthe need for an aggregation-aware routing interface and distributed\nprompt-aware scheduling to address these challenges. We demonstrate the impact\nof ALTO's partial output streaming on a complex chatbot verification pipeline,\nincreasing throughput by up to 3x for a fixed latency target of 4 seconds /\nrequest while also reducing tail latency by 1.8x compared to a baseline serving\napproach.",
      "tldr_zh": "我们提出了 ALTO，一种高效的网络协调器，用于服务复合 AI 系统，如语言模型管道，通过流式传输中间输出来实现高吞吐量和低延迟。ALTO 利用生成语言模型的逐 token 输出特性，暴露中间输出流式机会，同时解决正确性和负载均衡等新挑战。论文引入了聚合感知路由接口和分布式提示感知调度机制，并在复杂聊天机器人验证管道上实验证明，ALTO 可将吞吐量提高高达 3 倍，同时将尾延迟降低 1.8 倍。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.DC",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04311v1",
      "published_date": "2024-03-07 08:30:26 UTC",
      "updated_date": "2024-03-07 08:30:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:26:59.460028"
    },
    {
      "arxiv_id": "2403.04309v1",
      "title": "AO-DETR: Anti-Overlapping DETR for X-Ray Prohibited Items Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Mingyuan Li",
        "Tong Jia",
        "Hao Wang",
        "Bowen Ma",
        "Shuyang Lin",
        "Da Cai",
        "Dongyue Chen"
      ],
      "abstract": "Prohibited item detection in X-ray images is one of the most essential and\nhighly effective methods widely employed in various security inspection\nscenarios. Considering the significant overlapping phenomenon in X-ray\nprohibited item images, we propose an Anti-Overlapping DETR (AO-DETR) based on\none of the state-of-the-art general object detectors, DINO. Specifically, to\naddress the feature coupling issue caused by overlapping phenomena, we\nintroduce the Category-Specific One-to-One Assignment (CSA) strategy to\nconstrain category-specific object queries in predicting prohibited items of\nfixed categories, which can enhance their ability to extract features specific\nto prohibited items of a particular category from the overlapping\nforeground-background features. To address the edge blurring problem caused by\noverlapping phenomena, we propose the Look Forward Densely (LFD) scheme, which\nimproves the localization accuracy of reference boxes in mid-to-high-level\ndecoder layers and enhances the ability to locate blurry edges of the final\nlayer. Similar to DINO, our AO-DETR provides two different versions with\ndistinct backbones, tailored to meet diverse application requirements.\nExtensive experiments on the PIXray and OPIXray datasets demonstrate that the\nproposed method surpasses the state-of-the-art object detectors, indicating its\npotential applications in the field of prohibited item detection. The source\ncode will be released at https://github.com/Limingyuan001/AO-DETR-test.",
      "tldr_zh": "本文提出 AO-DETR，一种基于 DINO 的先进物体检测器，用于解决 X 射线图像中违禁物品检测中的重叠现象问题。AO-DETR 引入 Category-Specific One-to-One Assignment (CSA) 策略来处理特征耦合，提升特定类别物品特征的提取能力；并提出 Look Forward Densely (LFD) 方案，以提高参考框的定位准确性和模糊边缘的检测效果。在 PIXray 和 OPIXray 数据集上的广泛实验显示，AO-DETR 超过了现有最先进检测器，准确率显著提升。该方法提供两种不同骨干网络版本，适用于多种安全检查场景。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04309v1",
      "published_date": "2024-03-07 08:30:17 UTC",
      "updated_date": "2024-03-07 08:30:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:27:12.547913"
    },
    {
      "arxiv_id": "2403.04306v5",
      "title": "Effectiveness Assessment of Recent Large Vision-Language Models",
      "title_zh": "最近大型视觉语言模型的有效性评估",
      "authors": [
        "Yao Jiang",
        "Xinyu Yan",
        "Ge-Peng Ji",
        "Keren Fu",
        "Meijun Sun",
        "Huan Xiong",
        "Deng-Ping Fan",
        "Fahad Shahbaz Khan"
      ],
      "abstract": "The advent of large vision-language models (LVLMs) represents a remarkable\nadvance in the quest for artificial general intelligence. However, the model's\neffectiveness in both specialized and general tasks warrants further\ninvestigation. This paper endeavors to evaluate the competency of popular LVLMs\nin specialized and general tasks, respectively, aiming to offer a comprehensive\nunderstanding of these novel models. To gauge their effectiveness in\nspecialized tasks, we employ six challenging tasks in three different\napplication scenarios: natural, healthcare, and industrial. These six tasks\ninclude salient/camouflaged/transparent object detection, as well as polyp\ndetection, skin lesion detection, and industrial anomaly detection. We examine\nthe performance of three recent open-source LVLMs, including MiniGPT-v2,\nLLaVA-1.5, and Shikra, on both visual recognition and localization in these\ntasks. Moreover, we conduct empirical investigations utilizing the\naforementioned LVLMs together with GPT-4V, assessing their multi-modal\nunderstanding capabilities in general tasks including object counting, absurd\nquestion answering, affordance reasoning, attribute recognition, and spatial\nrelation reasoning. Our investigations reveal that these LVLMs demonstrate\nlimited proficiency not only in specialized tasks but also in general tasks. We\ndelve deep into this inadequacy and uncover several potential factors,\nincluding limited cognition in specialized tasks, object hallucination,\ntext-to-image interference, and decreased robustness in complex problems. We\nhope that this study can provide useful insights for the future development of\nLVLMs, helping researchers improve LVLMs for both general and specialized\napplications.",
      "tldr_zh": "这篇论文评估了最近的大型视觉语言模型 (LVLMs) 在专业和一般任务中的有效性，旨在提供全面理解。研究团队测试了 MiniGPT-v2、LLaVA-1.5 和 Shikra 在六个专业任务（如显著物体检测、息肉检测和工业异常检测）上的视觉识别和定位性能，以及在一般任务（如物体计数、荒谬问题回答和空间关系推理）中使用这些模型加上 GPT-4V 的多模态理解能力。结果显示，LVLMs 在专业和一般任务中表现出有限的熟练度，主要受限于专业任务认知不足、物体幻觉、文本到图像干扰和复杂问题鲁棒性降低。该研究为未来 LVLMs 的开发提供了关键见解，帮助提升其在一般和专业应用中的表现。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by Visual Intelligence",
      "pdf_url": "http://arxiv.org/pdf/2403.04306v5",
      "published_date": "2024-03-07 08:25:27 UTC",
      "updated_date": "2024-10-26 03:07:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:27:26.206197"
    },
    {
      "arxiv_id": "2403.04299v2",
      "title": "LitSim: A Conflict-aware Policy for Long-term Interactive Traffic Simulation",
      "title_zh": "LitSim: 一种冲突感知策略，用于长期交互式交通模拟",
      "authors": [
        "Haojie Xin",
        "Xiaodong Zhang",
        "Renzhi Tang",
        "Songyang Yan",
        "Qianrui Zhao",
        "Chunze Yang",
        "Wen Cui",
        "Zijiang Yang"
      ],
      "abstract": "Simulation is pivotal in evaluating the performance of autonomous driving\nsystems due to the advantages of high efficiency and low cost compared to\non-road testing. Bridging the gap between simulation and the real world\nrequires realistic agent behaviors. However, the existing works have the\nfollowing shortcomings in achieving this goal: (1) log replay offers realistic\nscenarios but often leads to collisions due to the absence of dynamic\ninteractions, and (2) both heuristic-based and data-based solutions, which are\nparameterized and trained on real-world datasets, encourage interactions but\noften deviate from real-world data over long horizons. In this work, we propose\nLitSim, a long-term interactive simulation approach that maximizes realism by\nminimizing the interventions in the log. Specifically, our approach primarily\nuses log replay to ensure realism and intervenes only when necessary to prevent\npotential conflicts. We then encourage interactions among the agents and\nresolve the conflicts, thereby reducing the risk of unrealistic behaviors. We\ntrain and validate our model on the real-world dataset NGSIM, and the\nexperimental results demonstrate that LitSim outperforms the currently popular\napproaches in terms of realism and reactivity.",
      "tldr_zh": "本文研究了交通模拟在自动驾驶系统评估中的重要性，但现有方法存在不足：日志重放(log replay)虽真实但易导致碰撞，而启发式或基于数据的方案虽鼓励交互但长期偏离真实数据。为此，提出LitSim，一种冲突感知策略，通过最小化对日志的干预（如仅在必要时防止潜在冲突）来实现长期交互模拟，同时鼓励代理间的交互并解决冲突。在NGSIM真实数据集上训练和验证，实验结果显示LitSim在真实性和反应性方面优于现有流行方法。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "9 pages, 6 figures, under review",
      "pdf_url": "http://arxiv.org/pdf/2403.04299v2",
      "published_date": "2024-03-07 07:58:58 UTC",
      "updated_date": "2024-05-01 05:29:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:27:35.207843"
    },
    {
      "arxiv_id": "2403.04293v2",
      "title": "MKF-ADS: Multi-Knowledge Fusion Based Self-supervised Anomaly Detection System for Control Area Network",
      "title_zh": "翻译失败",
      "authors": [
        "Pengzhou Cheng",
        "Zongru Wu",
        "Gongshen Liu"
      ],
      "abstract": "Control Area Network (CAN) is an essential communication protocol that\ninteracts between Electronic Control Units (ECUs) in the vehicular network.\nHowever, CAN is facing stringent security challenges due to innate security\nrisks. Intrusion detection systems (IDSs) are a crucial safety component in\nremediating Vehicular Electronics and Systems vulnerabilities. However,\nexisting IDSs fail to identify complexity attacks and have higher false alarms\nowing to capability bottleneck. In this paper, we propose a self-supervised\nmulti-knowledge fused anomaly detection model, called MKF-ADS. Specifically,\nthe method designs an integration framework, including spatial-temporal\ncorrelation with an attention mechanism (STcAM) module and patch\nsparse-transformer module (PatchST). The STcAM with fine-pruning uses\none-dimensional convolution (Conv1D) to extract spatial features and\nsubsequently utilizes the Bidirectional Long Short Term Memory (Bi-LSTM) to\nextract the temporal features, where the attention mechanism will focus on the\nimportant time steps. Meanwhile, the PatchST captures the combined contextual\nfeatures from independent univariate time series. Finally, the proposed method\nis based on knowledge distillation to STcAM as a student model for learning\nintrinsic knowledge and cross the ability to mimic PatchST. We conduct\nextensive experiments on six simulation attack scenarios across various CAN IDs\nand time steps, and two real attack scenarios, which present a competitive\nprediction and detection performance. Compared with the baseline in the same\nparadigm, the error rate and FAR are 2.62\\% and 2.41\\% and achieve a promising\nF1-score of 97.3\\%.",
      "tldr_zh": "本论文提出了一种基于多知识融合的自监督异常检测系统 MKF-ADS，用于解决 Control Area Network (CAN) 网络的安全风险问题，该系统针对现有 Intrusion Detection Systems (IDSs) 在复杂攻击识别和假警报方面的不足。MKF-ADS 整合了 STcAM 模块（利用 Conv1D 提取空间特征、Bi-LSTM 提取时间特征并结合注意力机制）和 PatchST 模块（从单变量时间序列捕获上下文特征），并通过 Knowledge Distillation 让 STcAM 作为学生模型学习 PatchST 的知识。实验结果显示，在六种模拟攻击场景和两种真实场景下，MKF-ADS 相比基线模型将错误率降低至 2.62%、假警报率至 2.41%，并实现了 97.3% 的 F1-score。",
      "categories": [
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.AI",
      "comment": "14 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2403.04293v2",
      "published_date": "2024-03-07 07:40:53 UTC",
      "updated_date": "2024-03-15 03:57:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:27:49.428849"
    },
    {
      "arxiv_id": "2403.04292v1",
      "title": "A challenge in A(G)I, cybernetics revived in the Ouroboros Model as one algorithm for all thinking",
      "title_zh": "翻译失败",
      "authors": [
        "Knud Thomsen"
      ],
      "abstract": "A topical challenge for algorithms in general and for automatic image\ncategorization and generation in particular is presented in the form of a\ndrawing for AI to understand. In a second vein, AI is challenged to produce\nsomething similar from verbal description. The aim of the paper is to highlight\nstrengths and deficiencies of current Artificial Intelligence approaches while\ncoarsely sketching a way forward. A general lack of encompassing\nsymbol-embedding and (not only) -grounding in some bodily basis is made\nresponsible for current deficiencies. A concomitant dearth of hierarchical\norganization of concepts follows suite. As a remedy for these shortcomings, it\nis proposed to take a wide step back and to newly incorporate aspects of\ncybernetics and analog control processes. It is claimed that a promising\noverarching perspective is provided by the Ouroboros Model with a valid and\nversatile algorithmic backbone for general cognition at all accessible levels\nof abstraction and capabilities. Reality, rules, truth, and Free Will are all\nuseful abstractions according to the Ouroboros Model. Logic deduction as well\nas intuitive guesses are claimed as produced on the basis of one\ncompartmentalized memory for schemata and a pattern-matching, i.e., monitoring\nprocess termed consumption analysis. The latter directs attention on short\n(attention proper) and also on long times scales (emotional biases). In this\ncybernetic approach, discrepancies between expectations and actual activations\n(e.g., sensory precepts) drive the general process of cognition and at the same\ntime steer the storage of new and adapted memory entries. Dedicated structures\nin the human brain work in concert according to this scheme.",
      "tldr_zh": "本论文讨论了人工智能（A(G)I）在图像分类和生成等领域的挑战，特别是AI理解视觉内容和从描述生成类似输出的不足。作者认为当前AI缺乏全面的符号嵌入（symbol-embedding）、身体基础的接地，以及概念的层次组织（hierarchical organization）。作为解决方案，论文提出Ouroboros Model，这是一种整合网络学（cybernetics）和模拟控制过程的通用算法框架，通过compartmentalized memory for schemata和pattern-matching消费分析（consumption analysis）来处理认知过程，实现从逻辑推理到直觉猜测的统一。最终，该模型为AI提供了一个更全面的认知视角，提升了对现实、规则、真理和自由意志的抽象处理能力。",
      "categories": [
        "cs.AI",
        "93-08",
        "I.2.0; I.2.4"
      ],
      "primary_category": "cs.AI",
      "comment": "26 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.04292v1",
      "published_date": "2024-03-07 07:39:54 UTC",
      "updated_date": "2024-03-07 07:39:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:28:01.882794"
    },
    {
      "arxiv_id": "2403.04283v1",
      "title": "Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model with Proxy",
      "title_zh": "Proxy-RLHF：通过代理解耦大型语言模型中的生成与对齐",
      "authors": [
        "Yu Zhu",
        "Chuxiong Sun",
        "Wenfei Yang",
        "Wenqiang Wei",
        "Bo Tang",
        "Tianzhu Zhang",
        "Zhiyu Li",
        "Shifeng Zhang",
        "Feiyu Xiong",
        "Jie Hu",
        "Mingchuan yang"
      ],
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) is the prevailing approach\nto ensure Large Language Models (LLMs) align with human values. However,\nexisting RLHF methods require a high computational cost, one main reason being\nthat RLHF assigns both the generation and alignment tasks to the LLM\nsimultaneously. In this paper, we introduce Proxy-RLHF, which decouples the\ngeneration and alignment processes of LLMs, achieving alignment with human\nvalues at a much lower computational cost. We start with a novel Markov\nDecision Process (MDP) designed for the alignment process and employ\nReinforcement Learning (RL) to train a streamlined proxy model that oversees\nthe token generation of the LLM, without altering the LLM itself. Experiments\nshow that our method achieves a comparable level of alignment with only 1\\% of\nthe training parameters of other methods.",
      "tldr_zh": "该论文提出Proxy-RLHF方法，以解决现有Reinforcement Learning from Human Feedback (RLHF)方法在Large Language Models (LLMs)中计算成本高的问题，该方法通过解耦生成和对齐任务来实现高效的人类价值观对齐。研究设计了一个新型Markov Decision Process (MDP)，并使用Reinforcement Learning (RL)训练一个精简的代理模型（proxy model），该模型负责监督LLMs的token生成，而无需修改LLMs本身。实验结果显示，Proxy-RLHF仅需其他方法1%的训练参数，即可达到相媲美的对齐性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04283v1",
      "published_date": "2024-03-07 07:31:00 UTC",
      "updated_date": "2024-03-07 07:31:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:28:13.840097"
    },
    {
      "arxiv_id": "2403.04280v2",
      "title": "A New Benchmark for Evaluating Automatic Speech Recognition in the Arabic Call Domain",
      "title_zh": "翻译失败",
      "authors": [
        "Qusai Abo Obaidah",
        "Muhy Eddin Za'ter",
        "Adnan Jaljuli",
        "Ali Mahboub",
        "Asma Hakouz",
        "Bashar Al-Rfooh",
        "Yazan Estaitia"
      ],
      "abstract": "This work is an attempt to introduce a comprehensive benchmark for Arabic\nspeech recognition, specifically tailored to address the challenges of\ntelephone conversations in Arabic language. Arabic, characterized by its rich\ndialectal diversity and phonetic complexity, presents a number of unique\nchallenges for automatic speech recognition (ASR) systems. These challenges are\nfurther amplified in the domain of telephone calls, where audio quality,\nbackground noise, and conversational speech styles negatively affect\nrecognition accuracy. Our work aims to establish a robust benchmark that not\nonly encompasses the broad spectrum of Arabic dialects but also emulates the\nreal-world conditions of call-based communications. By incorporating diverse\ndialectical expressions and accounting for the variable quality of call\nrecordings, this benchmark seeks to provide a rigorous testing ground for the\ndevelopment and evaluation of ASR systems capable of navigating the\ncomplexities of Arabic speech in telephonic contexts. This work also attempts\nto establish a baseline performance evaluation using state-of-the-art ASR\ntechnologies.",
      "tldr_zh": "这篇论文提出一个新的基准，用于评估阿拉伯语电话对话领域的自动语音识别（ASR）系统，针对阿拉伯语的方言多样性和语音复杂性所带来的挑战。基准模拟真实电话通话环境，包括音频质量问题、背景噪音和对话风格，以提供一个全面的测试框架。该基准不仅涵盖多种阿拉伯方言，还基于现有ASR技术建立了性能基线，帮助提升系统在实际场景中的准确性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04280v2",
      "published_date": "2024-03-07 07:24:32 UTC",
      "updated_date": "2024-05-30 12:17:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:28:25.775090"
    },
    {
      "arxiv_id": "2403.04264v2",
      "title": "Competitive Facility Location under Random Utilities and Routing Constraints",
      "title_zh": "基于随机效用和路由约束的竞争性设施选址",
      "authors": [
        "Hoang Giang Pham",
        "Tien Thanh Dam",
        "Ngan Ha Duong",
        "Tien Mai",
        "Minh Hoang Ha"
      ],
      "abstract": "In this paper, we study a facility location problem within a competitive\nmarket context, where customer demand is predicted by a random utility choice\nmodel. Unlike prior research, which primarily focuses on simple constraints\nsuch as a cardinality constraint on the number of selected locations, we\nintroduce routing constraints that necessitate the selection of locations in a\nmanner that guarantees the existence of a tour visiting all chosen locations\nwhile adhering to a specified tour length upper bound. Such routing constraints\nfind crucial applications in various real-world scenarios. The problem at hand\nfeatures a non-linear objective function, resulting from the utilization of\nrandom utilities, together with complex routing constraints, making it\ncomputationally challenging. To tackle this problem, we explore three types of\nvalid cuts, namely, outer-approximation and submodular cuts to handle the\nnonlinear objective function, as well as sub-tour elimination cuts to address\nthe complex routing constraints. These lead to the development of two exact\nsolution methods: a nested cutting plane and nested branch-and-cut algorithms,\nwhere these valid cuts are iteratively added to a master problem through two\nnested loops. We also prove that our nested cutting plane method always\nconverges to optimality after a finite number of iterations. Furthermore, we\ndevelop a local search-based metaheuristic tailored for solving large-scale\ninstances and show its pros and cons compared to exact methods. Extensive\nexperiments are conducted on problem instances of varying sizes, demonstrating\nthat our approach excels in terms of solution quality and computation time when\ncompared to other baseline approaches.",
      "tldr_zh": "本文研究了在随机效用(random utilities)下带有路由约束(routing constraints)的竞争性设施选址问题，强调选址需确保所有点形成不超过指定长度的tour，以适应现实场景需求。作者开发了三种有效切(valid cuts)——outer-approximation cuts和submodular cuts处理非线性目标函数，以及sub-tour elimination cuts管理路由约束，并据此提出nested cutting plane和nested branch-and-cut算法，同时证明前者能在有限迭代中收敛到最优。还设计了一个基于局部搜索的metaheuristic，用于大规模实例，并通过实验验证，该方法在解决方案质量和计算时间上显著优于基线方法。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04264v2",
      "published_date": "2024-03-07 06:56:24 UTC",
      "updated_date": "2024-03-09 20:17:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:28:40.741107"
    },
    {
      "arxiv_id": "2403.04261v2",
      "title": "Advancing Chinese biomedical text mining with community challenges",
      "title_zh": "通过社区挑战推进中文生物医学文本挖掘",
      "authors": [
        "Hui Zong",
        "Rongrong Wu",
        "Jiaxue Cha",
        "Weizhe Feng",
        "Erman Wu",
        "Jiakun Li",
        "Aibin Shao",
        "Liang Tao",
        "Zuofeng Li",
        "Buzhou Tang",
        "Bairong Shen"
      ],
      "abstract": "Objective: This study aims to review the recent advances in community\nchallenges for biomedical text mining in China. Methods: We collected\ninformation of evaluation tasks released in community challenges of biomedical\ntext mining, including task description, dataset description, data source, task\ntype and related links. A systematic summary and comparative analysis were\nconducted on various biomedical natural language processing tasks, such as\nnamed entity recognition, entity normalization, attribute extraction, relation\nextraction, event extraction, text classification, text similarity, knowledge\ngraph construction, question answering, text generation, and large language\nmodel evaluation. Results: We identified 39 evaluation tasks from 6 community\nchallenges that spanned from 2017 to 2023. Our analysis revealed the diverse\nrange of evaluation task types and data sources in biomedical text mining. We\nexplored the potential clinical applications of these community challenge tasks\nfrom a translational biomedical informatics perspective. We compared with their\nEnglish counterparts, and discussed the contributions, limitations, lessons and\nguidelines of these community challenges, while highlighting future directions\nin the era of large language models. Conclusion: Community challenge evaluation\ncompetitions have played a crucial role in promoting technology innovation and\nfostering interdisciplinary collaboration in the field of biomedical text\nmining. These challenges provide valuable platforms for researchers to develop\nstate-of-the-art solutions.",
      "tldr_zh": "这篇论文回顾了中国生物医学文本 mining 的社区挑战进展，目标是通过收集和分析2017年至2023年6个挑战中的39个评估任务（如named entity recognition、relation extraction和knowledge graph construction），进行系统总结和比较。结果显示，这些任务覆盖了多样化的数据来源和类型，具有潜在的临床应用，并与英语对应任务进行了对比，揭示了贡献、局限性及经验教训。论文强调，在大型语言模型时代，这些社区挑战促进了技术创新、跨学科合作，并为未来研究提供了指导方向。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04261v2",
      "published_date": "2024-03-07 06:52:51 UTC",
      "updated_date": "2024-08-30 02:47:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:28:50.600820"
    },
    {
      "arxiv_id": "2403.04256v1",
      "title": "Federated Recommendation via Hybrid Retrieval Augmented Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Huimin Zeng",
        "Zhenrui Yue",
        "Qian Jiang",
        "Dong Wang"
      ],
      "abstract": "Federated Recommendation (FR) emerges as a novel paradigm that enables\nprivacy-preserving recommendations. However, traditional FR systems usually\nrepresent users/items with discrete identities (IDs), suffering from\nperformance degradation due to the data sparsity and heterogeneity in FR. On\nthe other hand, Large Language Models (LLMs) as recommenders have proven\neffective across various recommendation scenarios. Yet, LLM-based recommenders\nencounter challenges such as low inference efficiency and potential\nhallucination, compromising their performance in real-world scenarios. To this\nend, we propose GPT-FedRec, a federated recommendation framework leveraging\nChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism.\nGPT-FedRec is a two-stage solution. The first stage is a hybrid retrieval\nprocess, mining ID-based user patterns and text-based item features. Next, the\nretrieved results are converted into text prompts and fed into GPT for\nre-ranking. Our proposed hybrid retrieval mechanism and LLM-based re-rank aims\nto extract generalized features from data and exploit pretrained knowledge\nwithin LLM, overcoming data sparsity and heterogeneity in FR. In addition, the\nRAG approach also prevents LLM hallucination, improving the recommendation\nperformance for real-world users. Experimental results on diverse benchmark\ndatasets demonstrate the superior performance of GPT-FedRec against\nstate-of-the-art baseline methods.",
      "tldr_zh": "该论文提出了一种名为GPT-FedRec的联邦推荐(Federated Recommendation, FR)框架，利用ChatGPT和混合Retrieval Augmented Generation (RAG)机制，解决传统FR系统因数据稀疏和异质性导致的性能下降问题，以及Large Language Models (LLMs)在推荐中的低效率和幻觉挑战。框架分为两个阶段：首先通过混合检索挖掘基于ID的用户模式和基于文本的物品特征；其次，将检索结果转换为文本提示输入GPT进行重新排序，以提取泛化特征并利用LLMs的预训练知识。实验结果显示，GPT-FedRec在多个基准数据集上优于现有基线方法，提高了推荐性能和隐私保护。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04256v1",
      "published_date": "2024-03-07 06:38:41 UTC",
      "updated_date": "2024-03-07 06:38:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:29:04.849987"
    },
    {
      "arxiv_id": "2403.04246v1",
      "title": "Efficient CNN-LSTM based Parameter Estimation of Levy Driven Stochastic Differential Equations",
      "title_zh": "翻译失败",
      "authors": [
        "Shuaiyu Li",
        "Yang Ruan",
        "Changzhou Long",
        "Yuzhong Cheng"
      ],
      "abstract": "This study addresses the challenges in parameter estimation of stochastic\ndifferential equations driven by non-Gaussian noises, which are critical in\nunderstanding dynamic phenomena such as price fluctuations and the spread of\ninfectious diseases. Previous research highlighted the potential of LSTM\nnetworks in estimating parameters of alpha stable Levy driven SDEs but faced\nlimitations including high time complexity and constraints of the LSTM chaining\nproperty. To mitigate these issues, we introduce the PEnet, a novel\nCNN-LSTM-based three-stage model that offers an end to end approach with\nsuperior accuracy and adaptability to varying data structures, enhanced\ninference speed for long sequence observations through initial data feature\ncondensation by CNN, and high generalization capability, allowing its\napplication to various complex SDE scenarios. Experiments on synthetic datasets\nconfirm PEnet significant advantage in estimating SDE parameters associated\nwith noise characteristics, establishing it as a competitive method for SDE\nparameter estimation in the presence of Levy noise.",
      "tldr_zh": "本研究针对非高斯噪声驱动的Levy Driven Stochastic Differential Equations（SDEs）参数估计的挑战（如价格波动和传染病传播中的动态现象），提出了一种高效的CNN-LSTM 基于模型PEnet。该模型采用三阶段端到端方法，通过CNN进行初始数据特征浓缩，结合LSTM提升准确性和适应性，同时显著提高长序列观察的推理速度。实验结果显示，PEnet在合成数据集上表现出色，能够精确估计与噪声相关的SDE参数，并展示出高泛化能力，适用于各种复杂SDE场景。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "2023 International Conference on Machine Learning and Applications\n  (ICMLA)",
      "pdf_url": "http://arxiv.org/pdf/2403.04246v1",
      "published_date": "2024-03-07 06:07:31 UTC",
      "updated_date": "2024-03-07 06:07:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:29:13.408748"
    },
    {
      "arxiv_id": "2403.04233v3",
      "title": "TEGEE: Task dEfinition Guided Expert Ensembling for Generalizable and Few-shot Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Xingwei Qu",
        "Yiming Liang",
        "Yucheng Wang",
        "Tianyu Zheng",
        "Tommy Yue",
        "Xingyuan Bu",
        "Lei Ma",
        "Stephen W. Huang",
        "Jiajun Zhang",
        "Yinan Shi",
        "Chenghua Lin",
        "Jie Fu",
        "Ge Zhang"
      ],
      "abstract": "Large Language Models (LLMs) exhibit the ability to perform in-context\nlearning (ICL), where they acquire new tasks directly from examples provided in\ndemonstrations. This process is thought to operate through an implicit task\nselection mechanism that involves extracting and processing task definitions\nfrom these demonstrations. However, critical questions remain: Which is more\nessential -- task extraction or definition? And how can these capabilities be\nfurther improved? To address these questions, we propose \\textbf{TEGEE} (Task\nDefinition Guided Expert Ensembling), a method that explicitly extracts task\ndefinitions and generates responses based on specific tasks. Our framework\nemploys a dual 3B model approach, with each model assigned a distinct role: one\nfocuses on task definition extraction, while the other handles learning from\ndemonstrations. This modular approach supports the hypothesis that extracting\ntask definitions is more vital than processing the task itself. Empirical\nevaluations show that TEGEE performs comparably to the larger LLaMA2-13B model.\nBy leveraging a modular design, our approach extends traditional ICL from\nfew-shot to many-shot learning, supporting an unlimited number of\ndemonstrations and enhancing continual learning capabilities.",
      "tldr_zh": "这篇论文提出了 TEGEE 方法，通过任务定义引导的专家集成（Task Definition Guided Expert Ensembling），来提升大型语言模型（LLMs）的 in-context learning (ICL) 能力，强调任务定义提取比任务处理更关键。TEGEE 采用双 3B 模型框架：一个模型负责从演示中提取任务定义，另一个模型则基于这些定义进行学习和响应生成。这种模块化设计支持从 few-shot 扩展到 many-shot 学习，并增强持续学习能力。实验结果显示，TEGEE 的性能与更大的 LLaMA2-13B 模型相当，为更高效的泛化学习提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04233v3",
      "published_date": "2024-03-07 05:26:41 UTC",
      "updated_date": "2024-12-14 14:39:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:29:26.704334"
    },
    {
      "arxiv_id": "2403.04232v1",
      "title": "Generalizing Cooperative Eco-driving via Multi-residual Task Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Vindula Jayawardana",
        "Sirui Li",
        "Cathy Wu",
        "Yashar Farid",
        "Kentaro Oguchi"
      ],
      "abstract": "Conventional control, such as model-based control, is commonly utilized in\nautonomous driving due to its efficiency and reliability. However, real-world\nautonomous driving contends with a multitude of diverse traffic scenarios that\nare challenging for these planning algorithms. Model-free Deep Reinforcement\nLearning (DRL) presents a promising avenue in this direction, but learning DRL\ncontrol policies that generalize to multiple traffic scenarios is still a\nchallenge. To address this, we introduce Multi-residual Task Learning (MRTL), a\ngeneric learning framework based on multi-task learning that, for a set of task\nscenarios, decomposes the control into nominal components that are effectively\nsolved by conventional control methods and residual terms which are solved\nusing learning. We employ MRTL for fleet-level emission reduction in mixed\ntraffic using autonomous vehicles as a means of system control. By analyzing\nthe performance of MRTL across nearly 600 signalized intersections and 1200\ntraffic scenarios, we demonstrate that it emerges as a promising approach to\nsynergize the strengths of DRL and conventional methods in generalizable\ncontrol.",
      "tldr_zh": "这篇论文提出了 Multi-residual Task Learning (MRTL)，一种基于多任务学习的框架，用于提升自动驾驶控制策略的泛化能力，以应对多样化的交通场景。MRTL 将控制任务分解为传统方法（如基于模型的控制）处理的命名组件和 Deep Reinforcement Learning (DRL) 处理的剩余项，从而结合了二者的优势。论文通过在近600个信号灯交叉口和1200个交通场景的分析，证明了 MRTL 在车队级排放减少中的有效性，为可泛化控制提供了新途径。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted for publication at ICRA 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.04232v1",
      "published_date": "2024-03-07 05:25:34 UTC",
      "updated_date": "2024-03-07 05:25:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:29:38.634743"
    },
    {
      "arxiv_id": "2403.04814v3",
      "title": "Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Linyuan Gong",
        "Sida Wang",
        "Mostafa Elhoushi",
        "Alvin Cheung"
      ],
      "abstract": "We introduce Syntax-Aware Fill-In-the-Middle (SAFIM), a new benchmark for\nevaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM)\ntask. This benchmark focuses on syntax-aware completions of program structures\nsuch as code blocks and conditional expressions, and includes 17,720 examples\nfrom multiple programming languages, sourced from recent code submissions after\nApril 2022 to minimize data contamination. SAFIM provides a robust framework\nwith various prompt designs and novel syntax-aware post-processing techniques,\nfacilitating accurate and fair comparisons across LLMs. Our comprehensive\nevaluation of 15 LLMs shows that FIM pretraining not only enhances FIM\nproficiency but also improves Left-to-Right (L2R) inference using LLMs. Our\nfindings challenge conventional beliefs and suggest that pretraining methods\nand data quality have more impact than model size. SAFIM thus serves as a\nfoundational platform for future research in effective pretraining strategies\nfor code LLMs. The evaluation toolkit and dataset are available at\nhttps://github.com/gonglinyuan/safim, and the leaderboard is available at\nhttps://safimbenchmark.com.",
      "tldr_zh": "本研究引入了Syntax-Aware Fill-In-the-Middle (SAFIM)基准，用于评估Large Language Models (LLMs)在代码Fill-in-the-Middle (FIM)任务中的性能，焦点在于语法感知的代码补全，如代码块和条件表达式。数据集包含17,720个来自多种编程语言的示例，源自2022年4月后的代码提交，以减少数据污染，并采用了各种提示设计和新型语法感知后处理技术来确保评估的准确性和公平性。对15个LLMs的全面评估显示，FIM预训练不仅提升了FIM能力，还改善了Left-to-Right (L2R)推理，同时发现预训练方法和数据质量比模型大小更具影响力。SAFIM作为未来代码LLMs有效预训练策略研究的基础平台，已提供开源工具包和排行榜。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SE"
      ],
      "primary_category": "cs.CL",
      "comment": "22 pages; ICML 2024 Oral: https://icml.cc/virtual/2024/oral/35482",
      "pdf_url": "http://arxiv.org/pdf/2403.04814v3",
      "published_date": "2024-03-07 05:05:56 UTC",
      "updated_date": "2024-06-23 01:17:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:29:52.035800"
    },
    {
      "arxiv_id": "2403.04224v4",
      "title": "Aligners: Decoupling LLMs and Alignment",
      "title_zh": "Aligners：解耦",
      "authors": [
        "Lilian Ngweta",
        "Mayank Agarwal",
        "Subha Maity",
        "Alex Gittens",
        "Yuekai Sun",
        "Mikhail Yurochkin"
      ],
      "abstract": "Large Language Models (LLMs) need to be aligned with human expectations to\nensure their safety and utility in most applications. Alignment is challenging,\ncostly, and needs to be repeated for every LLM and alignment criterion. We\npropose to decouple LLMs and alignment by training aligner models that can be\nused to align any LLM for a given criteria on an as-needed basis, thus also\nreducing the potential negative impacts of alignment on performance. Our recipe\nfor training the aligner models solely relies on synthetic data generated with\na (prompted) LLM and can be easily adjusted for a variety of alignment\ncriteria. We use the same synthetic data to train inspectors, binary\nmiss-alignment classification models to guide a \"squad\" of multiple aligners.\nOur empirical results demonstrate consistent improvements when applying aligner\nsquad to various LLMs, including chat-aligned models, across several\ninstruction-following and red-teaming datasets.",
      "tldr_zh": "该论文提出“Aligners”框架，将大型语言模型（LLMs）和对齐（alignment）过程解耦，通过训练独立的 aligner models 来根据需要为任意 LLM 进行对齐，从而降低成本并减少对模型性能的负面影响。训练方法依赖于由提示的 LLM 生成的合成数据，并可轻松调整以适应不同对齐标准；同时，使用同一数据训练 inspectors 模型来指导多个 aligners 的“团队”进行二元失调分类。实验结果显示，该框架在各种指令遵循和 red-teaming 数据集上，对包括已对齐模型在内的多种 LLMs 实现了持续性能提升。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Short version accepted as a Tiny Paper at the International\n  Conference on Learning Representations (ICLR) 2024. Long version accepted to\n  the Conference on Empirical Methods in Natural Language Processing (EMNLP)\n  2024 Findings",
      "pdf_url": "http://arxiv.org/pdf/2403.04224v4",
      "published_date": "2024-03-07 04:54:56 UTC",
      "updated_date": "2024-10-04 05:29:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:30:05.709976"
    },
    {
      "arxiv_id": "2403.04221v2",
      "title": "Why Online Reinforcement Learning is Causal",
      "title_zh": "为什么在线强化学习是因果的",
      "authors": [
        "Oliver Schulte",
        "Pascal Poupart"
      ],
      "abstract": "Reinforcement learning (RL) and causal modelling naturally complement each\nother. The goal of causal modelling is to predict the effects of interventions\nin an environment, while the goal of reinforcement learning is to select\ninterventions that maximize the rewards the agent receives from the\nenvironment. Reinforcement learning includes the two most powerful sources of\ninformation for estimating causal relationships: temporal ordering and the\nability to act on an environment. This paper examines which reinforcement\nlearning settings we can expect to benefit from causal modelling, and how. In\nonline learning, the agent has the ability to interact directly with their\nenvironment, and learn from exploring it. Our main argument is that in online\nlearning, conditional probabilities are causal, and therefore offline RL is the\nsetting where causal learning has the most potential to make a difference.\nEssentially, the reason is that when an agent learns from their {\\em own}\nexperience, there are no unobserved confounders that influence both the agent's\nown exploratory actions and the rewards they receive. Our paper formalizes this\nargument. For offline RL, where an agent may and typically does learn from the\nexperience of {\\em others}, we describe previous and new methods for leveraging\na causal model, including support for counterfactual queries.",
      "tldr_zh": "该论文论证了在线强化学习（online RL）为什么是因果的，强调RL通过时间顺序和环境互动来估计因果关系，从而与因果建模自然互补。核心观点是，在在线学习中，代理从自身经验学习时，没有未观察到的混杂因素（confounders）影响行动和奖励，因此条件概率本质上是因果的。论文形式化了这一论证，并为离线RL描述了利用因果模型的方法，包括支持反事实查询（counterfactual queries），从而阐明因果建模在不同RL设置中的潜在益处。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.6"
      ],
      "primary_category": "cs.LG",
      "comment": "43 pages. Version 2 discusses policy evaluation for partially\n  observable MDPs based on a causal model",
      "pdf_url": "http://arxiv.org/pdf/2403.04221v2",
      "published_date": "2024-03-07 04:49:48 UTC",
      "updated_date": "2024-07-10 23:51:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:30:19.447997"
    },
    {
      "arxiv_id": "2403.04204v1",
      "title": "On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models",
      "title_zh": "关于本质与前景：大模型对齐方法的调查",
      "authors": [
        "Xinpeng Wang",
        "Shitong Duan",
        "Xiaoyuan Yi",
        "Jing Yao",
        "Shanlin Zhou",
        "Zhihua Wei",
        "Peng Zhang",
        "Dongkuan Xu",
        "Maosong Sun",
        "Xing Xie"
      ],
      "abstract": "Big models have achieved revolutionary breakthroughs in the field of AI, but\nthey might also pose potential concerns. Addressing such concerns, alignment\ntechnologies were introduced to make these models conform to human preferences\nand values. Despite considerable advancements in the past year, various\nchallenges lie in establishing the optimal alignment strategy, such as data\ncost and scalable oversight, and how to align remains an open question. In this\nsurvey paper, we comprehensively investigate value alignment approaches. We\nfirst unpack the historical context of alignment tracing back to the 1920s\n(where it comes from), then delve into the mathematical essence of alignment\n(what it is), shedding light on the inherent challenges. Following this\nfoundation, we provide a detailed examination of existing alignment methods,\nwhich fall into three categories: Reinforcement Learning, Supervised\nFine-Tuning, and In-context Learning, and demonstrate their intrinsic\nconnections, strengths, and limitations, helping readers better understand this\nresearch area. In addition, two emerging topics, personal alignment, and\nmultimodal alignment, are also discussed as novel frontiers in this field.\nLooking forward, we discuss potential alignment paradigms and how they could\nhandle remaining challenges, prospecting where future alignment will go.",
      "tldr_zh": "这篇调查论文探讨了 AI 大模型的 alignment（对齐）技术，以解决模型符合人类偏好和价值观的问题。论文首先追溯 alignment 的历史背景（从1920年代起）和数学本质，揭示其内在挑战；随后分类现有方法为 Reinforcement Learning、Supervised Fine-Tuning 和 In-context Learning，并分析它们的优势、局限性及内在联系。论文还讨论了新兴领域如 personal alignment 和 multimodal alignment，并展望未来 alignment 范式，以应对数据成本和可扩展监督等挑战。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "23 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.04204v1",
      "published_date": "2024-03-07 04:19:13 UTC",
      "updated_date": "2024-03-07 04:19:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:30:28.276218"
    },
    {
      "arxiv_id": "2403.04202v7",
      "title": "Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents",
      "title_zh": "异质学习代理种群中道德行为的动态",
      "authors": [
        "Elizaveta Tennant",
        "Stephen Hailes",
        "Mirco Musolesi"
      ],
      "abstract": "Growing concerns about safety and alignment of AI systems highlight the\nimportance of embedding moral capabilities in artificial agents: a promising\nsolution is the use of learning from experience, i.e., Reinforcement Learning.\nIn multi-agent (social) environments, complex population-level phenomena may\nemerge from interactions between individual learning agents. Many of the\nexisting studies rely on simulated social dilemma environments to study the\ninteractions of independent learning agents; however, they tend to ignore the\nmoral heterogeneity that is likely to be present in societies of agents in\npractice. For example, at different points in time a single learning agent may\nface opponents who are consequentialist (i.e., focused on maximizing outcomes\nover time), norm-based (i.e., conforming to specific norms), or virtue-based\n(i.e., considering a combination of different virtues). The extent to which\nagents' co-development may be impacted by such moral heterogeneity in\npopulations is not well understood. In this paper, we present a study of the\nlearning dynamics of morally heterogeneous populations interacting in a social\ndilemma setting. Using an Iterated Prisoner's Dilemma environment with a\npartner selection mechanism, we investigate the extent to which the prevalence\nof diverse moral agents in populations affects individual agents' learning\nbehaviors and emergent population-level outcomes. We observe several types of\nnon-trivial interactions between pro-social and anti-social agents, and find\nthat certain types of moral agents are able to steer selfish agents towards\nmore cooperative behavior.",
      "tldr_zh": "本研究探讨了在道德异质性人群中，学习代理（learning agents）的道德行为动态，强调了强化学习（Reinforcement Learning）在提升AI系统安全性和对齐性方面的作用。研究采用迭代囚徒困境（Iterated Prisoner's Dilemma）环境，并引入伴侣选择机制，模拟不同道德类型代理（如后果主义、规范主义和美德主义代理）的互动。结果显示，亲社会（pro-social）和反社会（anti-social）代理之间存在非-trivial互动，某些道德代理能够引导自私代理转向更合作的行為。该工作为理解多智能体环境中代理的共同发展提供了重要洞见。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "Presented at AIES 2024 (7th AAAI/ACM Conference on AI, Ethics, and\n  Society - San Jose, CA, USA) - see\n  https://ojs.aaai.org/index.php/AIES/article/view/31736",
      "pdf_url": "http://arxiv.org/pdf/2403.04202v7",
      "published_date": "2024-03-07 04:12:24 UTC",
      "updated_date": "2025-01-16 17:28:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:30:41.593112"
    },
    {
      "arxiv_id": "2403.04197v4",
      "title": "Large Language Models are In-Context Molecule Learners",
      "title_zh": "大语言模型是基于上下文的分子学习者",
      "authors": [
        "Jiatong Li",
        "Wei Liu",
        "Zhihao Ding",
        "Wenqi Fan",
        "Yuqiang Li",
        "Qing Li"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated exceptional performance in\nbiochemical tasks, especially the molecule caption translation task, which aims\nto bridge the gap between molecules and natural language texts. However,\nprevious methods in adapting LLMs to the molecule-caption translation task\nrequired extra domain-specific pre-training stages, suffered weak alignment\nbetween molecular and textual spaces, or imposed stringent demands on the scale\nof LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation\n(ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment\nfrom context examples via In-Context Molecule Tuning. Specifically, ICMA\nincorporates the following three stages: Hybrid Context Retrieval,\nPost-retrieval Re-ranking, and In-context Molecule Tuning. Initially, Hybrid\nContext Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval\nto retrieve similar informative context examples. Additionally, Post-retrieval\nRe-ranking is composed of Sequence Reversal and Random Walk selection to\nfurther improve the quality of retrieval results. Finally, In-Context Molecule\nTuning unlocks the in-context learning and reasoning capability of LLMs with\nthe retrieved examples and adapts the parameters of LLMs for better alignment\nbetween molecules and texts. Experimental results demonstrate that ICMA can\nempower LLMs to achieve state-of-the-art or comparable performance without\nextra training corpora and intricate structures, showing that LLMs are\ninherently in-context molecule learners.",
      "tldr_zh": "这篇论文探讨了 Large Language Models (LLMs) 在分子标题翻译任务（molecule caption translation）中的潜力，旨在桥接分子和自然语言文本的差距，而无需额外领域特定预训练。研究提出 In-Context Molecule Adaptation (ICMA) 框架，包括 Hybrid Context Retrieval（结合 BM25 Caption Retrieval 和 Molecule Graph Retrieval 检索上下文示例）、Post-retrieval Re-ranking（通过 Sequence Reversal 和 Random Walk 优化检索质量），以及 In-Context Molecule Tuning（利用检索示例调整 LLMs 参数以提升分子-文本对齐）。实验结果显示，ICMA 使 LLMs 在不需额外训练语料的情况下达到最先进或可比性能，证明 LLMs 是天生的 in-context molecule learners。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by IEEE TKDE",
      "pdf_url": "http://arxiv.org/pdf/2403.04197v4",
      "published_date": "2024-03-07 03:58:28 UTC",
      "updated_date": "2025-04-07 07:46:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:30:54.580804"
    },
    {
      "arxiv_id": "2403.04190v1",
      "title": "Generative AI for Synthetic Data Generation: Methods, Challenges and the Future",
      "title_zh": "翻译失败",
      "authors": [
        "Xu Guo",
        "Yiqiang Chen"
      ],
      "abstract": "The recent surge in research focused on generating synthetic data from large\nlanguage models (LLMs), especially for scenarios with limited data\navailability, marks a notable shift in Generative Artificial Intelligence (AI).\nTheir ability to perform comparably to real-world data positions this approach\nas a compelling solution to low-resource challenges. This paper delves into\nadvanced technologies that leverage these gigantic LLMs for the generation of\ntask-specific training data. We outline methodologies, evaluation techniques,\nand practical applications, discuss the current limitations, and suggest\npotential pathways for future research.",
      "tldr_zh": "这篇论文探讨了Generative AI在合成数据生成中的应用，特别是利用大型语言模型（LLMs）来解决数据稀缺场景下的挑战。论文概述了生成任务特定训练数据的先进方法、评估技术以及实际应用，并强调了LLMs在性能上可与真实数据媲美。作者还分析了当前存在的限制，如潜在偏差和可靠性问题，并提出了未来研究方向，包括改进技术和更广泛的应用探索。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "I.2.0"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04190v1",
      "published_date": "2024-03-07 03:38:44 UTC",
      "updated_date": "2024-03-07 03:38:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:31:03.740900"
    },
    {
      "arxiv_id": "2403.04187v1",
      "title": "Preference optimization of protein language models as a multi-objective binder design paradigm",
      "title_zh": "翻译失败",
      "authors": [
        "Pouria Mistani",
        "Venkatesh Mysore"
      ],
      "abstract": "We present a multi-objective binder design paradigm based on instruction\nfine-tuning and direct preference optimization (DPO) of autoregressive protein\nlanguage models (pLMs). Multiple design objectives are encoded in the language\nmodel through direct optimization on expert curated preference sequence\ndatasets comprising preferred and dispreferred distributions. We show the\nproposed alignment strategy enables ProtGPT2 to effectively design binders\nconditioned on specified receptors and a drug developability criterion.\nGenerated binder samples demonstrate median isoelectric point (pI) improvements\nby $17\\%-60\\%$.",
      "tldr_zh": "本研究提出了一种多目标结合物设计范式，通过指令微调和直接偏好优化 (DPO) 来优化自回归蛋白质语言模型 (pLMs)。该方法在专家策划的偏好序列数据集上进行直接优化，以编码多个设计目标，从而使 ProtGPT2 能够根据指定的受体和药物可开发性标准生成结合物。实验结果显示，生成的结合物样本在中等等电点 (pI) 上实现了 17%-60% 的改善。",
      "categories": [
        "physics.bio-ph",
        "cs.AI",
        "cs.CE",
        "q-bio.BM"
      ],
      "primary_category": "physics.bio-ph",
      "comment": "Published at the GEM workshop, ICLR 2024. Generative and Experimental\n  Perspectives for Biomolecular Design (https://www.gembio.ai/)",
      "pdf_url": "http://arxiv.org/pdf/2403.04187v1",
      "published_date": "2024-03-07 03:36:03 UTC",
      "updated_date": "2024-03-07 03:36:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:31:17.713797"
    },
    {
      "arxiv_id": "2403.04182v3",
      "title": "Regression-aware Inference with LLMs",
      "title_zh": "基于LLMs的回归感知推理",
      "authors": [
        "Michal Lukasik",
        "Harikrishna Narasimhan",
        "Aditya Krishna Menon",
        "Felix Yu",
        "Sanjiv Kumar"
      ],
      "abstract": "Large language models (LLMs) have shown strong results on a range of\napplications, including regression and scoring tasks. Typically, one obtains\noutputs from an LLM via autoregressive sampling from the model's output\ndistribution. We show that this inference strategy can be sub-optimal for\ncommon regression and scoring evaluation metrics. As a remedy, we build on\nprior work on Minimum Bayes Risk decoding, and propose alternate inference\nstrategies that estimate the Bayes-optimal solution for regression and scoring\nmetrics in closed-form from sampled responses. We show that our proposal\nsignificantly improves over baselines across datasets and models.",
      "tldr_zh": "本研究指出，大语言模型 (LLMs) 在回归和评分任务中，通常采用自回归采样从输出分布获取结果，但这种策略可能对常见评估指标 suboptimal。作者基于 Minimum Bayes Risk decoding 的先前工作，提出了一种新的推理策略，通过从采样响应中闭式形式估计 Bayes-optimal 解决方案，从而优化回归任务的性能。实验结果显示，该方法在多个数据集和模型上显著优于基线基准。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP Findings 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.04182v3",
      "published_date": "2024-03-07 03:24:34 UTC",
      "updated_date": "2024-11-01 17:57:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:31:27.903605"
    },
    {
      "arxiv_id": "2403.04175v1",
      "title": "Understanding the PULSAR Effect in Combined Radiotherapy and Immunotherapy through Attention Mechanisms with a Transformer Model",
      "title_zh": "翻译失败",
      "authors": [
        "Hao Peng",
        "Casey Moore",
        "Debabrata Saha",
        "Steve Jiang",
        "Robert Timmerman"
      ],
      "abstract": "PULSAR (personalized, ultra-fractionated stereotactic adaptive radiotherapy)\nis the adaptation of stereotactic ablative radiotherapy towards personalized\ncancer management. For the first time, we applied a transformer-based attention\nmechanism to investigate the underlying interactions between combined PULSAR\nand PD-L1 blockade immunotherapy based on a murine cancer model (Lewis Lung\nCarcinoma, LLC). The proposed approach is able to predict the trend of tumor\nvolume change semi-quantitatively, and excels in identifying the potential\ncausal relationships through both self-attention and cross-attention scores.",
      "tldr_zh": "本研究首次运用Transformer模型的注意力机制，探究PULSAR（personalized, ultra-fractionated stereotactic adaptive radiotherapy）和PD-L1 blockade免疫疗法的联合效应，基于Lewis Lung Carcinoma (LLC)小鼠癌症模型。\n该方法能够半定量预测肿瘤体积变化趋势，并通过self-attention和cross-attention分数识别潜在的因果关系。\n这项工作为理解放射疗法与免疫疗法的交互机制提供了新工具，提升了个性化癌症管理的精确性。",
      "categories": [
        "physics.med-ph",
        "cs.AI"
      ],
      "primary_category": "physics.med-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04175v1",
      "published_date": "2024-03-07 03:12:31 UTC",
      "updated_date": "2024-03-07 03:12:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:31:41.607757"
    },
    {
      "arxiv_id": "2403.04164v3",
      "title": "ProMISe: Promptable Medical Image Segmentation using SAM",
      "title_zh": "翻译失败",
      "authors": [
        "Jinfeng Wang",
        "Sifan Song",
        "Xinkun Wang",
        "Yiyi Wang",
        "Yiyi Miao",
        "Jionglong Su",
        "S. Kevin Zhou"
      ],
      "abstract": "With the proposal of the Segment Anything Model (SAM), fine-tuning SAM for\nmedical image segmentation (MIS) has become popular. However, due to the large\nsize of the SAM model and the significant domain gap between natural and\nmedical images, fine-tuning-based strategies are costly with potential risk of\ninstability, feature damage and catastrophic forgetting. Furthermore, some\nmethods of transferring SAM to a domain-specific MIS through fine-tuning\nstrategies disable the model's prompting capability, severely limiting its\nutilization scenarios. In this paper, we propose an Auto-Prompting Module\n(APM), which provides SAM-based foundation model with Euclidean adaptive\nprompts in the target domain. Our experiments demonstrate that such adaptive\nprompts significantly improve SAM's non-fine-tuned performance in MIS. In\naddition, we propose a novel non-invasive method called Incremental Pattern\nShifting (IPS) to adapt SAM to specific medical domains. Experimental results\nshow that the IPS enables SAM to achieve state-of-the-art or competitive\nperformance in MIS without the need for fine-tuning. By coupling these two\nmethods, we propose ProMISe, an end-to-end non-fine-tuned framework for\nPromptable Medical Image Segmentation. Our experiments demonstrate that both\nusing our methods individually or in combination achieves satisfactory\nperformance in low-cost pattern shifting, with all of SAM's parameters frozen.",
      "tldr_zh": "这篇论文针对Segment Anything Model (SAM)在医疗图像分割 (MIS) 中的应用问题，提出Auto-Prompting Module (APM)，通过提供欧氏自适应提示来提升SAM的非细调性能，避免了模型细调带来的成本和风险。作者还引入Incremental Pattern Shifting (IPS)，一种非侵入性方法，让SAM无需细调即可适应特定医疗领域，并实现状态-of-the-art或竞争性性能。最终，结合APM和IPS的ProMISe框架提供了一个端到端的可提示MIS解决方案，实验证明其在冻结参数的情况下也能获得满意结果。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04164v3",
      "published_date": "2024-03-07 02:48:42 UTC",
      "updated_date": "2024-09-28 12:59:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:31:53.500858"
    },
    {
      "arxiv_id": "2403.04160v1",
      "title": "Improving Retrieval in Theme-specific Applications using a Corpus Topical Taxonomy",
      "title_zh": "使用语料库主题分类法改进主题特定应用中的检索",
      "authors": [
        "SeongKu Kang",
        "Shivam Agarwal",
        "Bowen Jin",
        "Dongha Lee",
        "Hwanjo Yu",
        "Jiawei Han"
      ],
      "abstract": "Document retrieval has greatly benefited from the advancements of large-scale\npre-trained language models (PLMs). However, their effectiveness is often\nlimited in theme-specific applications for specialized areas or industries, due\nto unique terminologies, incomplete contexts of user queries, and specialized\nsearch intents. To capture the theme-specific information and improve\nretrieval, we propose to use a corpus topical taxonomy, which outlines the\nlatent topic structure of the corpus while reflecting user-interested aspects.\nWe introduce ToTER (Topical Taxonomy Enhanced Retrieval) framework, which\nidentifies the central topics of queries and documents with the guidance of the\ntaxonomy, and exploits their topical relatedness to supplement missing\ncontexts. As a plug-and-play framework, ToTER can be flexibly employed to\nenhance various PLM-based retrievers. Through extensive quantitative, ablative,\nand exploratory experiments on two real-world datasets, we ascertain the\nbenefits of using topical taxonomy for retrieval in theme-specific applications\nand demonstrate the effectiveness of ToTER.",
      "tldr_zh": "该论文针对大型预训练语言模型 (PLMs) 在主题特定应用中的局限性（如独特术语和不完整查询上下文），提出使用 corpus topical taxonomy 来捕捉主题特定信息并提升文档检索效果。ToTER (Topical Taxonomy Enhanced Retrieval) 框架通过 taxonomy 指导查询和文档的中心主题识别，并利用主题相关性补充缺失上下文，作为一个 plug-and-play 模块增强各种 PLM-based 检索器。在两个真实世界数据集上的定量、消融和探索性实验中，ToTER 显著提高了检索性能，证明了其在主题特定应用中的有效性。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "TheWebConf'24",
      "pdf_url": "http://arxiv.org/pdf/2403.04160v1",
      "published_date": "2024-03-07 02:34:54 UTC",
      "updated_date": "2024-03-07 02:34:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:32:05.855074"
    },
    {
      "arxiv_id": "2403.04158v1",
      "title": "DA-Net: A Disentangled and Adaptive Network for Multi-Source Cross-Lingual Transfer Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Ling Ge",
        "Chunming Hu",
        "Guanghui Ma",
        "Jihong Liu",
        "Hong Zhang"
      ],
      "abstract": "Multi-Source cross-lingual transfer learning deals with the transfer of task\nknowledge from multiple labelled source languages to an unlabeled target\nlanguage under the language shift. Existing methods typically focus on\nweighting the predictions produced by language-specific classifiers of\ndifferent sources that follow a shared encoder. However, all source languages\nshare the same encoder, which is updated by all these languages. The extracted\nrepresentations inevitably contain different source languages' information,\nwhich may disturb the learning of the language-specific classifiers.\nAdditionally, due to the language gap, language-specific classifiers trained\nwith source labels are unable to make accurate predictions for the target\nlanguage. Both facts impair the model's performance. To address these\nchallenges, we propose a Disentangled and Adaptive Network (DA-Net). Firstly,\nwe devise a feedback-guided collaborative disentanglement method that seeks to\npurify input representations of classifiers, thereby mitigating mutual\ninterference from multiple sources. Secondly, we propose a class-aware parallel\nadaptation method that aligns class-level distributions for each source-target\nlanguage pair, thereby alleviating the language pairs' language gap.\nExperimental results on three different tasks involving 38 languages validate\nthe effectiveness of our approach.",
      "tldr_zh": "本研究针对多源跨语言转移学习（Multi-Source Cross-Lingual Transfer Learning）中的问题，提出了一种Disentangled and Adaptive Network (DA-Net)，旨在减少源语言间相互干扰并缓解语言差距。DA-Net包括反馈引导的协作解耦方法（feedback-guided collaborative disentanglement），用于净化分类器的输入表示，降低多源信息干扰；以及类感知并行适应方法（class-aware parallel adaptation），通过对每个源-目标语言对进行类级分布对齐，提升跨语言预测准确性。在涉及38种语言的三个任务上，实验结果验证了DA-Net的有效性，显著提高了模型性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "AAAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.04158v1",
      "published_date": "2024-03-07 02:30:46 UTC",
      "updated_date": "2024-03-07 02:30:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:32:19.141713"
    },
    {
      "arxiv_id": "2403.04146v1",
      "title": "FL-GUARD: A Holistic Framework for Run-Time Detection and Recovery of Negative Federated Learning",
      "title_zh": "FL-GUARD：一个整体框架，用于运行时检测和恢复负面联邦学习",
      "authors": [
        "Hong Lin",
        "Lidan Shou",
        "Ke Chen",
        "Gang Chen",
        "Sai Wu"
      ],
      "abstract": "Federated learning (FL) is a promising approach for learning a model from\ndata distributed on massive clients without exposing data privacy. It works\neffectively in the ideal federation where clients share homogeneous data\ndistribution and learning behavior. However, FL may fail to function\nappropriately when the federation is not ideal, amid an unhealthy state called\nNegative Federated Learning (NFL), in which most clients gain no benefit from\nparticipating in FL. Many studies have tried to address NFL. However, their\nsolutions either (1) predetermine to prevent NFL in the entire learning\nlife-cycle or (2) tackle NFL in the aftermath of numerous learning rounds.\nThus, they either (1) indiscriminately incur extra costs even if FL can perform\nwell without such costs or (2) waste numerous learning rounds. Additionally,\nnone of the previous work takes into account the clients who may be\nunwilling/unable to follow the proposed NFL solutions when using those\nsolutions to upgrade an FL system in use. This paper introduces FL-GUARD, a\nholistic framework that can be employed on any FL system for tackling NFL in a\nrun-time paradigm. That is, to dynamically detect NFL at the early stage (tens\nof rounds) of learning and then to activate recovery measures when necessary.\nSpecifically, we devise a cost-effective NFL detection mechanism, which relies\non an estimation of performance gain on clients. Only when NFL is detected, we\nactivate the NFL recovery process, in which each client learns in parallel an\nadapted model when training the global model. Extensive experiment results\nconfirm the effectiveness of FL-GUARD in detecting NFL and recovering from NFL\nto a healthy learning state. We also show that FL-GUARD is compatible with\nprevious NFL solutions and robust against clients unwilling/unable to take any\nrecovery measures.",
      "tldr_zh": "这篇论文介绍了 FL-GUARD，一个整体框架，用于在运行时动态检测和恢复 Negative Federated Learning (NFL)，以解决联邦学习 (Federated Learning, FL) 中数据分布不均或客户端行为不一致导致的大多数客户端无法受益的问题。FL-GUARD 采用基于客户端性能增益估计的成本高效检测机制，在 FL 早期（几十轮）识别 NFL，并仅在必要时激活恢复过程，让每个客户端并行训练适应模型。实验结果证明，该框架能有效检测 NFL 并恢复到健康学习状态，同时兼容现有 NFL 解决方案，并对不愿或无法参与恢复的客户端保持鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04146v1",
      "published_date": "2024-03-07 01:52:05 UTC",
      "updated_date": "2024-03-07 01:52:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:32:30.687306"
    },
    {
      "arxiv_id": "2403.04140v1",
      "title": "Contrastive Augmented Graph2Graph Memory Interaction for Few Shot Continual Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Biqing Qi",
        "Junqi Gao",
        "Xingquan Chen",
        "Dong Li",
        "Jianxing Liu",
        "Ligang Wu",
        "Bowen Zhou"
      ],
      "abstract": "Few-Shot Class-Incremental Learning (FSCIL) has gained considerable attention\nin recent years for its pivotal role in addressing continuously arriving\nclasses. However, it encounters additional challenges. The scarcity of samples\nin new sessions intensifies overfitting, causing incompatibility between the\noutput features of new and old classes, thereby escalating catastrophic\nforgetting. A prevalent strategy involves mitigating catastrophic forgetting\nthrough the Explicit Memory (EM), which comprise of class prototypes. However,\ncurrent EM-based methods retrieves memory globally by performing\nVector-to-Vector (V2V) interaction between features corresponding to the input\nand prototypes stored in EM, neglecting the geometric structure of local\nfeatures. This hinders the accurate modeling of their positional relationships.\nTo incorporate information of local geometric structure, we extend the V2V\ninteraction to Graph-to-Graph (G2G) interaction. For enhancing local structures\nfor better G2G alignment and the prevention of local feature collapse, we\npropose the Local Graph Preservation (LGP) mechanism. Additionally, to address\nsample scarcity in classes from new sessions, the Contrast-Augmented G2G\n(CAG2G) is introduced to promote the aggregation of same class features thus\nhelps few-shot learning. Extensive comparisons on CIFAR100, CUB200, and the\nchallenging ImageNet-R dataset demonstrate the superiority of our method over\nexisting methods.",
      "tldr_zh": "该研究针对 Few-Shot Class-Incremental Learning (FSCIL) 的挑战，提出了一种基于 Contrast-Augmented Graph2Graph (CAG2G) 记忆交互方法，以缓解样本稀缺导致的过拟合和灾难性遗忘问题。具体而言，该方法将传统的 Vector-to-Vector (V2V) 交互扩展为 Graph-to-Graph (G2G) 交互，结合 Local Graph Preservation (LGP) 机制来保留局部特征的几何结构，并通过对比增强促进同类特征聚合。通过在 CIFAR100、CUB200 和 ImageNet-R 数据集上的实验，该方法在少样本持续学习任务中显著优于现有基于 Explicit Memory (EM) 的方法。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "12 Pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.04140v1",
      "published_date": "2024-03-07 01:41:12 UTC",
      "updated_date": "2024-03-07 01:41:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:32:42.782727"
    },
    {
      "arxiv_id": "2403.04135v1",
      "title": "Unsupervised Learning of Harmonic Analysis Based on Neural HSMM with Code Quality Templates",
      "title_zh": "翻译失败",
      "authors": [
        "Yui Uehara"
      ],
      "abstract": "This paper presents a method of unsupervised learning of harmonic analysis\nbased on a hidden semi-Markov model (HSMM). We introduce the chord quality\ntemplates, which specify the probability of pitch class emissions given a root\nnote and a chord quality. Other probability distributions that comprise the\nHSMM are automatically learned via unsupervised learning, which has been a\nchallenge in existing research. The results of the harmonic analysis of the\nproposed model were evaluated using existing labeled data. While our proposed\nmethod has yet to perform as well as existing models that used supervised\nlearning and complex rule design, it has the advantage of not requiring\nexpensive labeled data or rule elaboration. Furthermore, we also show how to\nrecognize the tonic without prior knowledge, based on the transition\nprobabilities of the Markov model.",
      "tldr_zh": "这篇论文提出了一种基于Neural HSMM的无监督学习方法，用于谐波分析，通过引入chord quality templates来指定给定根音和和弦质量的音高类发射概率。其他HSMM的概率分布则通过无监督学习自动获取，解决了现有研究中依赖手动标注的挑战。实验结果显示，该方法在谐波分析性能上虽不如采用监督学习和复杂规则的模型，但无需昂贵的标记数据或规则设计，且能基于Markov模型的转移概率识别tonic，而不需先验知识。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "20 pages, 5 figures, the original edition of this paper will be\n  published in the ICNMC2024 Proceedings and this arXiv publication is a copy",
      "pdf_url": "http://arxiv.org/pdf/2403.04135v1",
      "published_date": "2024-03-07 01:29:48 UTC",
      "updated_date": "2024-03-07 01:29:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:32:53.624697"
    },
    {
      "arxiv_id": "2403.04132v1",
      "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
      "title_zh": "Chatbot Arena：一个开放平台，用于通过人类偏好评估大语言模型",
      "authors": [
        "Wei-Lin Chiang",
        "Lianmin Zheng",
        "Ying Sheng",
        "Anastasios Nikolas Angelopoulos",
        "Tianle Li",
        "Dacheng Li",
        "Hao Zhang",
        "Banghua Zhu",
        "Michael Jordan",
        "Joseph E. Gonzalez",
        "Ion Stoica"
      ],
      "abstract": "Large Language Models (LLMs) have unlocked new capabilities and applications;\nhowever, evaluating the alignment with human preferences still poses\nsignificant challenges. To address this issue, we introduce Chatbot Arena, an\nopen platform for evaluating LLMs based on human preferences. Our methodology\nemploys a pairwise comparison approach and leverages input from a diverse user\nbase through crowdsourcing. The platform has been operational for several\nmonths, amassing over 240K votes. This paper describes the platform, analyzes\nthe data we have collected so far, and explains the tried-and-true statistical\nmethods we are using for efficient and accurate evaluation and ranking of\nmodels. We confirm that the crowdsourced questions are sufficiently diverse and\ndiscriminating and that the crowdsourced human votes are in good agreement with\nthose of expert raters. These analyses collectively establish a robust\nfoundation for the credibility of Chatbot Arena. Because of its unique value\nand openness, Chatbot Arena has emerged as one of the most referenced LLM\nleaderboards, widely cited by leading LLM developers and companies. Our demo is\npublicly available at \\url{https://chat.lmsys.org}.",
      "tldr_zh": "该研究引入了Chatbot Arena，这是一个开源平台，通过人类偏好评估大型语言模型(LLMs)。平台采用pairwise comparison方法和crowdsourcing从多样化用户收集超过240K投票，实现高效的模型评估和排名。分析显示，众包问题具有足够的多样性和辨别力，用户投票与专家评分高度一致，从而确立了平台的可靠性和可信度。作为一个广泛引用的LLM排行榜，Chatbot Arena已被领先开发者和公司广泛采用，其演示页面可公开访问。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04132v1",
      "published_date": "2024-03-07 01:22:38 UTC",
      "updated_date": "2024-03-07 01:22:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:33:05.514552"
    },
    {
      "arxiv_id": "2403.04124v1",
      "title": "Privacy-preserving Fine-tuning of Large Language Models through Flatness",
      "title_zh": "翻译失败",
      "authors": [
        "Tiejin Chen",
        "Longchao Da",
        "Huixue Zhou",
        "Pingzhi Li",
        "Kaixiong Zhou",
        "Tianlong Chen",
        "Hua Wei"
      ],
      "abstract": "The privacy concerns associated with the use of Large Language Models (LLMs)\nhave grown recently with the development of LLMs such as ChatGPT. Differential\nPrivacy (DP) techniques are explored in existing work to mitigate their privacy\nrisks at the cost of generalization degradation. Our paper reveals that the\nflatness of DP-trained models' loss landscape plays an essential role in the\ntrade-off between their privacy and generalization. We further propose a\nholistic framework to enforce appropriate weight flatness, which substantially\nimproves model generalization with competitive privacy preservation. It\ninnovates from three coarse-to-grained levels, including perturbation-aware\nmin-max optimization on model weights within a layer, flatness-guided sparse\nprefix-tuning on weights across layers, and weight knowledge distillation\nbetween DP \\& non-DP weights copies. Comprehensive experiments of both\nblack-box and white-box scenarios are conducted to demonstrate the\neffectiveness of our proposal in enhancing generalization and maintaining DP\ncharacteristics. For instance, on text classification dataset QNLI, DP-Flat\nachieves similar performance with non-private full fine-tuning but with DP\nguarantee under privacy budget $\\epsilon=3$, and even better performance given\nhigher privacy budgets. Codes are provided in the supplement.",
      "tldr_zh": "该论文揭示了在 Large Language Models (LLMs) 的微调过程中，Differential Privacy (DP) 技术虽能缓解隐私风险，但会因模型损失景观的平坦度 (flatness) 而影响泛化能力。研究提出一个整体框架，通过层内扰动感知最小-最大优化 (perturbation-aware min-max optimization)、层间平坦度引导稀疏前缀调整 (flatness-guided sparse prefix-tuning) 以及 DP 与非DP 权重之间的权重知识蒸馏 (weight knowledge distillation) 等多级创新方法，来显著提升模型泛化性能，同时保持竞争性的隐私保护。实验在黑盒和白盒场景中验证了框架的有效性，例如在 QNLI 数据集上，DP-Flat 方法在隐私预算 ε=3 时，性能与非私有全微调相当，甚至在更高预算下表现更佳。",
      "categories": [
        "cs.AI",
        "I.2"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to ICLR 2024 SeT LLM Workshop",
      "pdf_url": "http://arxiv.org/pdf/2403.04124v1",
      "published_date": "2024-03-07 00:44:11 UTC",
      "updated_date": "2024-03-07 00:44:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:33:21.370874"
    },
    {
      "arxiv_id": "2403.04121v2",
      "title": "Can Large Language Models Reason and Plan?",
      "title_zh": "大型语言模型能够推理和规划吗？",
      "authors": [
        "Subbarao Kambhampati"
      ],
      "abstract": "While humans sometimes do show the capability of correcting their own\nerroneous guesses with self-critiquing, there seems to be no basis for that\nassumption in the case of LLMs.",
      "tldr_zh": "论文探讨了大型语言模型（LLMs）是否具备推理和规划能力。摘要指出，虽然人类可以通过自我批评纠正错误猜测，但这种能力在LLMs中缺乏依据。研究可能通过实验分析证明了LLMs在自我纠错和高级推理方面的局限性，从而为评估AI智能提供了重要见解。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "arXiv admin note: text overlap with arXiv:2402.01817 (v2 add creative\n  commons attribution to Figure 2 graphic)",
      "pdf_url": "http://arxiv.org/pdf/2403.04121v2",
      "published_date": "2024-03-07 00:36:32 UTC",
      "updated_date": "2024-03-08 19:51:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:33:29.750608"
    },
    {
      "arxiv_id": "2403.04115v2",
      "title": "DNAct: Diffusion Guided Multi-Task 3D Policy Learning",
      "title_zh": "DNAct: 扩散引导的多任务 3D 策略学习",
      "authors": [
        "Ge Yan",
        "Yueh-Hua Wu",
        "Xiaolong Wang"
      ],
      "abstract": "This paper presents DNAct, a language-conditioned multi-task policy framework\nthat integrates neural rendering pre-training and diffusion training to enforce\nmulti-modality learning in action sequence spaces. To learn a generalizable\nmulti-task policy with few demonstrations, the pre-training phase of DNAct\nleverages neural rendering to distill 2D semantic features from foundation\nmodels such as Stable Diffusion to a 3D space, which provides a comprehensive\nsemantic understanding regarding the scene. Consequently, it allows various\napplications to challenging robotic tasks requiring rich 3D semantics and\naccurate geometry. Furthermore, we introduce a novel approach utilizing\ndiffusion training to learn a vision and language feature that encapsulates the\ninherent multi-modality in the multi-task demonstrations. By reconstructing the\naction sequences from different tasks via the diffusion process, the model is\ncapable of distinguishing different modalities and thus improving the\nrobustness and the generalizability of the learned representation. DNAct\nsignificantly surpasses SOTA NeRF-based multi-task manipulation approaches with\nover 30% improvement in success rate. Project website: dnact.github.io.",
      "tldr_zh": "本论文提出 DNAct，一种基于语言条件的多任务 3D 策略学习框架，通过整合 neural rendering 预训练和 diffusion training 来实现动作序列空间的多模态学习。预训练阶段利用 neural rendering 从 Stable Diffusion 等基础模型提取 2D 语义特征，并蒸馏到 3D 空间，提供对场景的全面语义理解，从而支持需要丰富 3D 语义和精确几何的机器人任务。扩散训练则通过重建多任务动作序列，学习视觉和语言特征，提升模型的鲁棒性和泛化性。实验结果显示，DNAct 在成功率上比现有的 NeRF-based 多任务操作方法提高了 30% 以上。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04115v2",
      "published_date": "2024-03-07 00:09:07 UTC",
      "updated_date": "2024-03-08 09:56:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:33:43.765811"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 99,
  "processed_papers_count": 99,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-17T13:34:03.078475"
}
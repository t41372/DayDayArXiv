{
  "date": "2025-06-03",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-06-03 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“**\nä»Šå¤©çš„ arXiv å‘ˆç°å‡ºâ€œæ·±åº¦æ¨ç†â€ä¸â€œä»£ç†ï¼ˆAgentï¼‰å®æˆ˜â€å¹¶è¡Œçš„è¶‹åŠ¿ï¼šå­¦æœ¯ç•Œæ­£åœ¨ä»ä¿¡æ¯è®ºè§’åº¦è§£æ„ LLM çš„â€œæ€è€ƒâ€è¿‡ç¨‹ï¼ˆThinking Tokensï¼‰ï¼Œå¹¶åœ¨ System 1ï¼ˆå¿«æ€è€ƒï¼‰ä¸ System 2ï¼ˆæ…¢æ€è€ƒï¼‰çš„åŠ¨æ€åˆ‡æ¢ä¸Šå–å¾—äº†æ¶æ„çº§çªç ´ï¼›ä¸æ­¤åŒæ—¶ï¼Œå¤šæ¨¡æ€ Agent åœ¨ç½‘é¡µæµè§ˆã€è®¡ç®—æœºæ“ä½œï¼ˆComputer Useï¼‰ä»¥åŠå®‰å…¨é˜²å¾¡ï¼ˆè§†è§‰æç¤ºæ³¨å…¥ï¼‰æ–¹é¢æ¶Œç°äº†å¤§é‡è½åœ°çº§çš„ç ”ç©¶ã€‚\n\n---\n\n### ğŸ§  LLM æ¨ç†æœºåˆ¶ï¼šæ˜¯çœŸæ­£çš„æ€è€ƒè¿˜æ˜¯æ¨¡ä»¿ï¼Ÿ\nä»Šæ—¥æœ€ç¡¬æ ¸çš„è®¨è®ºé›†ä¸­åœ¨ LLM çš„å†…éƒ¨æ¨ç†æœºåˆ¶ä¸Šã€‚æˆ‘ä»¬ä¸ä»…çœ‹åˆ°äº†å¯¹ CoT æœ¬è´¨çš„è´¨ç–‘ï¼Œä¹Ÿçœ‹åˆ°äº†é€šè¿‡ä¿¡æ¯è®ºé‡åŒ–â€œé¡¿æ‚Ÿæ—¶åˆ»â€çš„å°è¯•ã€‚\n\n**1. Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning**\n**(ç”¨äº’ä¿¡æ¯æ­ç§˜æ¨ç†åŠ¨æ€ï¼šæ€è€ƒ Token æ˜¯ LLM æ¨ç†ä¸­çš„ä¿¡æ¯å³°å€¼)**\nè¿™ç¯‡æ–‡ç« éå¸¸æœ‰è¶£ã€‚ä½œè€…ä»ä¿¡æ¯è®ºçš„è§’åº¦ç ”ç©¶äº† Large Reasoning Models (LRMs)ã€‚ä»–ä»¬å‘ç°äº†ä¸€ä¸ª**â€œäº’ä¿¡æ¯å³°å€¼â€ï¼ˆMI Peaksï¼‰**ç°è±¡ï¼šåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒæŸäº›ç‰¹å®šæ­¥éª¤çš„äº’ä¿¡æ¯ä¼šçªç„¶é£™å‡ã€‚è¿™äº›å³°å€¼é€šå¸¸å¯¹åº”ç€è¡¨ç¤ºåæ€æˆ–è½¬æ¢çš„è¯ï¼ˆå¦‚ \"Wait\", \"Therefore\"ï¼‰ï¼Œä½œè€…ç§°ä¹‹ä¸º**â€œThinking Tokensâ€**ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼Œéšç€ MI å¢åŠ ï¼Œé¢„æµ‹é”™è¯¯ç‡ä¸‹é™ã€‚è¿™ä¸ºæé«˜æ¨¡å‹æ¨ç†èƒ½åŠ›æä¾›äº†ç‰©ç†å¯è§£é‡Šçš„è§†è§’ã€‚\n\n**2. CoT is Not True Reasoning, It Is Just a Tight Constraint to Imitate: A Theory Perspective**\n**(CoT ä¸æ˜¯çœŸæ­£çš„æ¨ç†ï¼Œå®ƒåªæ˜¯ä¸€ç§ç´§è‡´çš„æ¨¡ä»¿çº¦æŸï¼šç†è®ºè§†è§’)**\nä¸€ç¯‡â€œå”±åè°ƒâ€çš„æ–‡ç« ã€‚ä½œè€…æå‡ºä¸€ä¸ªå°–é”çš„è§‚ç‚¹ï¼šæ€ç»´é“¾ï¼ˆCoTï¼‰å¹¶æ²¡æœ‰æ¿€å‘çœŸæ­£çš„æŠ½è±¡æ¨ç†ï¼Œå®ƒæ›´åƒæ˜¯ä¸€ç§**ç»“æ„çº¦æŸ**ã€‚å®ƒå¼ºè¿« LLM åˆ©ç”¨å…¶å¼ºå¤§çš„åºåˆ—é¢„æµ‹èƒ½åŠ›ï¼Œå»**æ¨¡ä»¿**è¿è´¯æ€ç»´çš„å½¢å¼ã€‚è¿™æ„å‘³ç€è®¸å¤šæ‰€è°“çš„â€œæ¶Œç°æ¨ç†èƒ½åŠ›â€å¯èƒ½åªæ˜¯é«˜ç»´çš„æ¨¡å¼åŒ¹é…ã€‚\n\n**3. OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for Over-Reasoning Mitigation**\n**(OThink-R1ï¼šç”¨äºç¼“è§£è¿‡åº¦æ¨ç†çš„å†…åœ¨å¿«/æ…¢æ€è€ƒæ¨¡å¼åˆ‡æ¢)**\né’ˆå¯¹ OpenAI o1 è¿™ç±»æ¨¡å‹â€œè¿‡åº¦æ€è€ƒâ€å¯¼è‡´ Token æµªè´¹çš„é—®é¢˜ï¼Œè¿™ç¯‡è®ºæ–‡æå‡ºäº† **OThink-R1**ã€‚å®ƒæ¨¡ä»¿äººç±»è®¤çŸ¥ï¼Œåœ¨ System 1ï¼ˆç›´è§‰å¿«æ€è€ƒï¼‰å’Œ System 2ï¼ˆé€»è¾‘æ…¢æ€è€ƒï¼‰ä¹‹é—´è‡ªåŠ¨åˆ‡æ¢ã€‚é€šè¿‡ä¸€ä¸ªè¾…åŠ©çš„ Judge æ¨¡å‹æ¥åˆ¤æ–­ä½•æ—¶éœ€è¦â€œæ…¢æ€è€ƒâ€ï¼Œä»è€Œåœ¨ä¿æŒå‡†ç¡®ç‡çš„åŒæ—¶å¤§å¹…é™ä½æ¨ç†æˆæœ¬ã€‚\n\n**4. Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning**\n**(Cell-o1ï¼šåˆ©ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒ LLM è§£å†³å•ç»†èƒæ¨ç†è°œé¢˜)**\no1 ç³»åˆ—æŠ€æœ¯åœ¨ç§‘å­¦é¢†åŸŸçš„è½åœ°ã€‚ä½œè€…å‘ç°ç°æœ‰çš„ LLM åœ¨å¤„ç†å•ç»†èƒæ•°æ®æ³¨é‡Šæ—¶çš„æ‰¹æ¬¡çº§æ¨ç†èƒ½åŠ›å¾ˆå¼±ã€‚ä»–ä»¬æå‡ºäº† Cell-o1ï¼Œé€šè¿‡è’¸é¦æ¨ç†è½¨è¿¹å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œåœ¨å•ç»†èƒæ¨ç†ä»»åŠ¡ä¸Šæ¯” OpenAI o1 å¼º 73%ï¼Œå±•ç¤ºäº†é¢†åŸŸä¸“ç”¨çš„ Reasoning æ¨¡å‹æ½œåŠ›ã€‚\n\n---\n\n### ğŸ¤– Multimodal Agentsï¼šç½‘é¡µå†²æµªä¸è§†è§‰å®‰å…¨\nAgent é¢†åŸŸæ­£åœ¨ä»â€œèƒ½ç”¨â€å‘â€œå¥½ç”¨â€å’Œâ€œå®‰å…¨â€è¿ˆè¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨ GUI äº¤äº’å’Œè§†è§‰å®‰å…¨æ–¹é¢ã€‚\n\n**5. Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights**\n**(Surfer-H é‡è§ Holo1ï¼šç”±å¼€æ”¾æƒé‡é©±åŠ¨çš„é«˜æ€§ä»·æ¯” Web Agent)**\nè¿™æ˜¯ä¸€ä¸ªå®æˆ˜æ´¾çš„å·¥ä½œã€‚ä½œè€…å‘å¸ƒäº† **Holo1**ï¼Œè¿™æ˜¯ä¸€ç»„ä¸“é—¨ä¸ºç½‘é¡µå¯¼èˆªå’Œä¿¡æ¯æå–ä¼˜åŒ–çš„å¼€æ”¾æƒé‡ VLMã€‚é…åˆ Surfer-H æ¡†æ¶ï¼Œå®ƒåœ¨ WebVoyager åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† 92.2% çš„ SOTA æ€§èƒ½ï¼Œè¯æ˜äº†ç»è¿‡ä¸“é—¨å¾®è°ƒçš„å¼€æºæ¨¡å‹åœ¨ Agent ä»»åŠ¡ä¸Šå¯ä»¥ä¸ä»…æ•ˆæœå¥½ï¼Œè€Œä¸”æ›´å…·æˆæœ¬æ•ˆç›Šã€‚\n\n**6. VPI-Bench: Visual Prompt Injection Attacks for Computer-Use Agents**\n**(VPI-Benchï¼šé’ˆå¯¹è®¡ç®—æœºæ“ä½œ Agent çš„è§†è§‰æç¤ºæ³¨å…¥æ”»å‡»)**\néšç€ Claude Computer Use ç­‰åŠŸèƒ½çš„æ™®åŠï¼Œå®‰å…¨é—®é¢˜éšä¹‹è€Œæ¥ã€‚è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†**è§†è§‰æç¤ºæ³¨å…¥ï¼ˆVPIï¼‰**ï¼šæ”»å‡»è€…å°†æ¶æ„æŒ‡ä»¤éšè—åœ¨ UI ç•Œé¢ä¸­ï¼ˆä¾‹å¦‚ç½‘é¡µé‡Œçš„é€æ˜æ–‡å­—æˆ–ç‰¹å®šå›¾åƒï¼‰ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰çš„ Agent ææ˜“è¢«è¿™ç§æ”»å‡»è¯¯å¯¼ï¼ˆæˆåŠŸç‡é«˜è¾¾ 51%-100%ï¼‰ï¼Œè¿™ä¸ºæœªæ¥çš„ Agent éƒ¨ç½²æ•²å“äº†è­¦é’Ÿã€‚\n\n**7. GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents**\n**(GUI-Actorï¼šGUI Agent çš„æ— åæ ‡è§†è§‰å®šä½)**\nä¼ ç»Ÿçš„ GUI Agent ä¾èµ–åæ ‡é¢„æµ‹ï¼Œè¿™å¾ˆå®¹æ˜“å‡ºé”™ã€‚GUI-Actor æå‡ºäº†ä¸€ç§**æ— åæ ‡ï¼ˆCoordinate-Freeï¼‰**çš„æ–¹æ³•ï¼Œé€šè¿‡ä¸€ä¸ªä¸“é—¨çš„ Action Head å°† `<ACTOR>` Token ä¸ç›¸å…³çš„è§†è§‰è¡¥ä¸å¯¹é½ã€‚è¿™ç§æ–¹æ³•åœ¨å¤„ç†ä¸åŒåˆ†è¾¨ç‡å’Œå¸ƒå±€æ—¶å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚\n\n---\n\n### ğŸ¥ è§†é¢‘ç”Ÿæˆä¸ç†è§£ï¼šé€Ÿåº¦ä¸è´¨é‡çš„å¹³è¡¡\nè§†é¢‘é¢†åŸŸä»Šå¤©çš„é‡ç‚¹åœ¨äºâ€œåŠ é€Ÿâ€å’Œâ€œé•¿æ—¶é¢„æµ‹â€ã€‚\n\n**8. Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers**\n**(Sparse-vDiTï¼šé‡Šæ”¾ç¨€ç–æ³¨æ„åŠ›çš„åŠ›é‡ä»¥åŠ é€Ÿè§†é¢‘æ‰©æ•£ Transformer)**\nè§†é¢‘ç”Ÿæˆå¤ªæ…¢äº†ï¼Ÿè¿™ç¯‡è®ºæ–‡é€šè¿‡åˆ†æ Video Diffusion Transformer (vDiT) çš„æ³¨æ„åŠ›å›¾ï¼Œå‘ç°äº†ä¸‰ç§ç¨€ç–æ¨¡å¼ã€‚åˆ©ç”¨è¿™äº›æ¨¡å¼ï¼ŒSparse-vDiT åœ¨ä¿æŒç”Ÿæˆè´¨é‡ï¼ˆPSNR ç›¸å½“ï¼‰çš„åŒæ—¶ï¼Œå®ç°äº†çº¦ **1.6-1.8 å€çš„å®é™…æ¨ç†åŠ é€Ÿ**ã€‚å¯¹äº HunyuanVideo å’Œ Sora ç±»æ¨¡å‹æ¥è¯´ï¼Œè¿™æ˜¯éå¸¸å®ç”¨çš„ä¼˜åŒ–ã€‚\n\n**9. Technical Report for Ego4D Long-Term Action Anticipation Challenge 2025**\n**(Ego4D 2025 é•¿æ—¶åŠ¨ä½œé¢„æµ‹æŒ‘æˆ˜æŠ€æœ¯æŠ¥å‘Š)**\nCVPR 2025 æŒ‘æˆ˜èµ›çš„å† å†›æ–¹æ¡ˆã€‚ä»–ä»¬ä½¿ç”¨äº†ä¸€ä¸ªä¸‰é˜¶æ®µæ¡†æ¶ï¼šç‰¹å¾æå– -> åŠ¨åè¯è¯†åˆ« -> LLM é¢„æµ‹ã€‚æ ¸å¿ƒåœ¨äºåˆ©ç”¨ LLM å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œå°†è§†è§‰è¯†åˆ«åˆ°çš„åŠ¨ä½œåºåˆ—è½¬åŒ–ä¸ºæ–‡æœ¬æç¤ºï¼Œè®© LLM å»â€œçŒœâ€æ¥ä¸‹æ¥ä¼šå‘ç”Ÿä»€ä¹ˆï¼Œåˆ·æ–°äº† SOTAã€‚\n\n---\n\n### ğŸ›¡ï¸ AI å®‰å…¨ä¸è¯„ä¼°åŸºå‡†\né™¤äº†æ”»å‡»ï¼Œé˜²å¾¡å’Œè¯„ä¼°ä¹Ÿæ˜¯ä»Šæ—¥é‡ç‚¹ã€‚\n\n**10. Ask a Local: Detecting Hallucinations With Specialized Model Divergence**\n**(Ask a Localï¼šåˆ©ç”¨ä¸“ç”¨æ¨¡å‹å·®å¼‚æ£€æµ‹å¹»è§‰)**\nå¦‚ä½•æ£€æµ‹å¹»è§‰ï¼Ÿè¿™ç¯‡è®ºæ–‡åˆ©ç”¨äº†ä¸€ä¸ªç›´è§‰ï¼š**ä¸“å®¶æ¨¡å‹åœ¨é‡åˆ°ä¸å‡†ç¡®ä¿¡æ¯æ—¶ä¼šæ„Ÿåˆ°â€œæƒŠè®¶â€**ã€‚é€šè¿‡è®¡ç®—ä¸åŒè¯­è¨€/é¢†åŸŸä¸“ç”¨æ¨¡å‹çš„å›°æƒ‘åº¦åˆ†å¸ƒå·®å¼‚ï¼ˆDivergenceï¼‰ï¼Œå¯ä»¥æœ‰æ•ˆè¯†åˆ«å¹»è§‰ï¼Œè€Œä¸”ä¸éœ€è¦é‡æ–°è®­ç»ƒï¼Œç‰¹åˆ«é€‚åˆå¤šè¯­è¨€ç¯å¢ƒã€‚\n\n**11. EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving**\n**(EvaLearnï¼šé€šè¿‡åºåˆ—é—®é¢˜è§£å†³é‡åŒ– LLM çš„å­¦ä¹ èƒ½åŠ›å’Œæ•ˆç‡)**\nç°åœ¨çš„åŸºå‡†æµ‹è¯•å¤§å¤šæµ‹â€œé™æ€çŸ¥è¯†â€ã€‚EvaLearn æå‡ºäº†æµ‹**â€œå­¦ä¹ èƒ½åŠ›â€**ã€‚å®ƒè®© LLM é¡ºåºè§£å†³ä¸€ç³»åˆ—é—®é¢˜ï¼Œçœ‹æ¨¡å‹æ˜¯å¦èƒ½ä»ä¹‹å‰çš„ç»éªŒä¸­â€œå­¦ä¼šâ€è§£é¢˜ã€‚ç»“æœå‘ç°ï¼Œæœ‰äº›å¼ºæ¨¡å‹ï¼ˆå¦‚ Claude-3.7ï¼‰å­¦ä¹ èƒ½åŠ›å¾ˆå¼ºï¼Œè€Œæœ‰äº›æ¨¡å‹ä¸ä»…å­¦ä¸ä¼šï¼Œè¿˜ä¼šæœ‰è´Ÿè¿ç§»ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸æœ‰æ„ä¹‰çš„è¯„ä¼°æ–°ç»´åº¦ã€‚\n\n**12. BadReward: Clean-Label Poisoning of Reward Models in Text-to-Image RLHF**\n**(BadRewardï¼šæ–‡ç”Ÿå›¾ RLHF ä¸­å¥–åŠ±æ¨¡å‹çš„å‡€æ ‡ç­¾æŠ•æ¯’)**\né’ˆå¯¹æ–‡ç”Ÿå›¾æ¨¡å‹çš„ RLHF è¿‡ç¨‹ï¼Œä½œè€…æå‡ºäº†ä¸€ç§éšè”½çš„æŠ•æ¯’æ”»å‡»ã€‚é€šè¿‡åœ¨å°‘é‡æ•°æ®ä¸­åˆ¶é€ â€œç‰¹å¾å†²çªâ€ï¼Œå¯ä»¥åœ¨ä¸ä¿®æ”¹æ ‡æ³¨çš„æƒ…å†µä¸‹ç ´åå¥–åŠ±æ¨¡å‹ï¼Œå¯¼è‡´æœ€ç»ˆç”Ÿæˆçš„å›¾ç‰‡åŒ…å«åè§æˆ–æš´åŠ›å†…å®¹ã€‚\n\n---\n\n### ğŸ—ï¸ æ¶æ„ä¼˜åŒ–ä¸é«˜æ•ˆå¾®è°ƒ\n**13. DiaBlo: Diagonal Blocks Are Sufficient For Finetuning**\n**(DiaBloï¼šå¯¹è§’å—è¶³ä»¥è¿›è¡Œå¾®è°ƒ)**\nLoRA çš„æœ‰åŠ›ç«äº‰è€…ï¼ŸDiaBlo æå‡ºäº†ä¸€ç§æç®€çš„ PEFT æ–¹æ³•ï¼š**åªæ›´æ–°æƒé‡çŸ©é˜µçš„å¯¹è§’å—**ã€‚å®ƒä¸éœ€è¦ä½ç§©çŸ©é˜µä¹˜æ³•ï¼Œé¿å¼€äº†åˆå§‹åŒ–å’Œä¼˜åŒ–ä¸ç¨³å®šçš„é—®é¢˜ï¼Œåœ¨æ˜¾å­˜æ•ˆç‡å’Œé€Ÿåº¦ä¸Šä¸ LoRA ç›¸å½“ï¼Œä½†åœ¨æ”¶æ•›ç¨³å®šæ€§ä¸Šè¡¨ç°æ›´å¥½ã€‚\n\n**14. HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference**\n**(HATAï¼šç”¨äºå¯æ‰©å±•å¤§æ¨¡å‹æ¨ç†çš„å¯è®­ç»ƒä¸”ç¡¬ä»¶é«˜æ•ˆçš„å“ˆå¸Œæ„ŸçŸ¥ Top-k æ³¨æ„åŠ›)**\nä¸ºäº†è§£å†³ LLM æ¨ç†ä¸­ Attention çš„ç“¶é¢ˆï¼ŒHATA å¼•å…¥äº†**å­¦ä¹ å“ˆå¸Œï¼ˆLearning-to-Hashï¼‰**æŠ€æœ¯ã€‚ä¸åŒäºæ˜‚è´µçš„ç²¾ç¡® Top-k è®¡ç®—ï¼ŒHATA å°† Query å’Œ Key æ˜ å°„ä¸ºäºŒè¿›åˆ¶å“ˆå¸Œç ï¼Œä»¥æ­¤å¿«é€Ÿä¼°ç®—ç›¸å¯¹é¡ºåºã€‚å®ç°äº†é«˜è¾¾ 7.2 å€çš„åŠ é€Ÿã€‚\n\n---\n\n### ğŸ§ª ç§‘å­¦ä¸å…¶ä»–æœ‰è¶£åº”ç”¨\n**15. ChemGraph: An Agentic Framework for Computational Chemistry Workflows**\n**(ChemGraphï¼šè®¡ç®—åŒ–å­¦å·¥ä½œæµçš„ä»£ç†æ¡†æ¶)**\nä¸€ä¸ªåŸºäº AI Agent çš„åŒ–å­¦è®¡ç®—å¹³å°ã€‚å®ƒç»“åˆäº†å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ä½œä¸ºåŸºç¡€æ¨¡å‹è¿›è¡Œè®¡ç®—ï¼Œåˆ©ç”¨ LLM è¿›è¡Œä»»åŠ¡è§„åˆ’ã€‚æœ‰è¶£çš„æ˜¯ï¼Œå®ƒå‘ç°é€šè¿‡å¤š Agent åä½œï¼Œè¾ƒå°çš„æ¨¡å‹ï¼ˆå¦‚ GPT-4o-miniï¼‰ä¹Ÿèƒ½åœ¨å¤æ‚ä»»åŠ¡ä¸ŠåŒ¹æ•Œ GPT-4oã€‚\n\n**16. Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework**\n**(Multimodal DeepResearcherï¼šåˆ©ç”¨ä»£ç†æ¡†æ¶ä»å¤´ç”Ÿæˆå›¾æ–‡å¹¶èŒ‚çš„æŠ¥å‘Š)**\nç°æœ‰çš„â€œæ·±åº¦ç ”ç©¶â€Agent å¤§å¤šåªå†™å­—ã€‚è¿™ä¸ªæ¡†æ¶èƒ½ç”ŸæˆåŒ…å«**å¯è§†åŒ–å›¾è¡¨**çš„æŠ¥å‘Šã€‚å®ƒå®šä¹‰äº†ä¸€ç§å›¾è¡¨çš„æ–‡æœ¬æè¿°æ ¼å¼ï¼ˆFDVï¼‰ï¼Œè®© LLM èƒ½å¤Ÿè§„åˆ’å¹¶ç”Ÿæˆé«˜è´¨é‡çš„ç»Ÿè®¡å›¾è¡¨ï¼Œä½¿æŠ¥å‘Šæ›´åŠ ä¸“ä¸šã€‚\n\n**17. Do Language Models Think Consistently? A Study of Value Preferences Across Varying Response Lengths**\n**(è¯­è¨€æ¨¡å‹æ€è€ƒä¸€è‡´å—ï¼Ÿè·¨å“åº”é•¿åº¦çš„ä»·å€¼åå¥½ç ”ç©¶)**\nè¿™ç¯‡è®ºæ–‡å‘ç°äº†ä¸€ä¸ªä»¤äººæ‹…å¿§çš„ç°è±¡ï¼šLLM åœ¨â€œçŸ­é—®å·â€ä¸­è¡¨ç°å‡ºçš„ä»·å€¼è§‚ï¼Œä¸å®ƒåœ¨â€œé•¿ç¯‡å¤§è®ºâ€ä¸­æµéœ²å‡ºçš„ä»·å€¼è§‚**ç›¸å…³æ€§å¾ˆå¼±**ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬é€šè¿‡ç®€å•æµ‹è¯•æµ‹å‡ºçš„æ¨¡å‹å¯¹é½ç¨‹åº¦ï¼Œåœ¨å®é™…é•¿æ–‡æœ¬ç”Ÿæˆä¸­å¯èƒ½å®Œå…¨å¤±æ•ˆã€‚",
  "papers": [
    {
      "arxiv_id": "2506.05399v1",
      "title": "Attention-based transformer models for image captioning across languages: An in-depth survey and evaluation",
      "title_zh": "è·¨è¯­è¨€å›¾åƒæè¿°ä¸­åŸºäºæ³¨æ„åŠ›çš„ Transformer æ¨¡å‹ï¼šæ·±åº¦ç»¼è¿°ä¸è¯„ä¼°",
      "authors": [
        "Israa A. Albadarneh",
        "Bassam H. Hammo",
        "Omar S. Al-Kadi"
      ],
      "abstract": "Image captioning involves generating textual descriptions from input images, bridging the gap between computer vision and natural language processing. Recent advancements in transformer-based models have significantly improved caption generation by leveraging attention mechanisms for better scene understanding. While various surveys have explored deep learning-based approaches for image captioning, few have comprehensively analyzed attention-based transformer models across multiple languages. This survey reviews attention-based image captioning models, categorizing them into transformer-based, deep learning-based, and hybrid approaches. It explores benchmark datasets, discusses evaluation metrics such as BLEU, METEOR, CIDEr, and ROUGE, and highlights challenges in multilingual captioning. Additionally, this paper identifies key limitations in current models, including semantic inconsistencies, data scarcity in non-English languages, and limitations in reasoning ability. Finally, we outline future research directions, such as multimodal learning, real-time applications in AI-powered assistants, healthcare, and forensic analysis. This survey serves as a comprehensive reference for researchers aiming to advance the field of attention-based image captioning.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹è·¨è¯­è¨€å›¾åƒæè¿°ï¼ˆImage Captioningï¼‰ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttention-basedï¼‰Transformeræ¨¡å‹è¿›è¡Œäº†æ·±å…¥çš„ç»¼è¿°ä¸è¯„ä¼°ã€‚æ–‡ç« å°†ç°æœ‰æ¨¡å‹ç³»ç»Ÿåœ°åˆ†ç±»ä¸ºåŸºäºTransformerã€åŸºäºæ·±åº¦å­¦ä¹ ä»¥åŠæ··åˆæ–¹æ³•ï¼Œå¹¶è¯¦ç»†æ¢è®¨äº†é€šç”¨çš„åŸºå‡†æ•°æ®é›†åŠBLEUã€METEORã€CIDErå’ŒROUGEç­‰æ ¸å¿ƒè¯„ä»·æŒ‡æ ‡ã€‚ç»¼è¿°é‡ç‚¹åˆ†æäº†å¤šè¯­è¨€æè¿°é¢ä¸´çš„æŒ‘æˆ˜ï¼Œè¯†åˆ«å‡ºå½“å‰æ¨¡å‹åœ¨è¯­ä¹‰ä¸ä¸€è‡´æ€§ã€éè‹±è¯­æ•°æ®ç¨€ç¼ºä»¥åŠæ¨ç†èƒ½åŠ›æ–¹é¢çš„å…³é”®å±€é™ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å±•æœ›äº†å¤šæ¨¡æ€å­¦ä¹ ï¼ˆMultimodal Learningï¼‰ã€å®æ—¶AIåŠ©æ‰‹ã€åŒ»ç–—ä¿å¥åŠæ³•åŒ»åˆ†æç­‰æœªæ¥ç ”ç©¶æ–¹å‘ã€‚è¯¥é¡¹å·¥ä½œä¸ºæ—¨åœ¨æ¨åŠ¨æ³¨æ„åŠ›æœºåˆ¶å›¾åƒæè¿°é¢†åŸŸå‘å±•çš„ç ”ç©¶äººå‘˜æä¾›äº†å…¨é¢çš„å­¦æœ¯å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "31 pages, 15 figures, 6 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.05399v1",
      "published_date": "2025-06-03 22:18:19 UTC",
      "updated_date": "2025-06-03 22:18:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:54:12.786142+00:00"
    },
    {
      "arxiv_id": "2506.03425v1",
      "title": "A Data-Driven Diffusion-based Approach for Audio Deepfake Explanations",
      "title_zh": "ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ•°æ®é©±åŠ¨éŸ³é¢‘æ·±åº¦ä¼ªé€ è§£é‡Šæ–¹æ³•",
      "authors": [
        "Petr Grinberg",
        "Ankur Kumar",
        "Surya Koppisetti",
        "Gaurav Bharaj"
      ],
      "abstract": "Evaluating explainability techniques, such as SHAP and LRP, in the context of audio deepfake detection is challenging due to lack of clear ground truth annotations. In the cases when we are able to obtain the ground truth, we find that these methods struggle to provide accurate explanations. In this work, we propose a novel data-driven approach to identify artifact regions in deepfake audio. We consider paired real and vocoded audio, and use the difference in time-frequency representation as the ground-truth explanation. The difference signal then serves as a supervision to train a diffusion model to expose the deepfake artifacts in a given vocoded audio. Experimental results on the VocV4 and LibriSeVoc datasets demonstrate that our method outperforms traditional explainability techniques, both qualitatively and quantitatively.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸­ SHAP å’Œ LRP ç­‰ä¼ ç»Ÿè§£é‡Šæ€§æŠ€æœ¯å› ç¼ºä¹æ˜ç¡®çš„ Ground Truth æ ‡æ³¨è€Œéš¾ä»¥æä¾›å‡†ç¡®è§£é‡Šçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ•°æ®é©±åŠ¨çš„æ–°å‹è§£é‡Šæ–¹æ¡ˆã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡é…å¯¹çœŸå®éŸ³é¢‘ä¸å£°ç å™¨å¤„ç†éŸ³é¢‘ (Vocoded Audio)ï¼Œå°†äºŒè€…åœ¨æ—¶é¢‘è¡¨ç¤º (Time-frequency Representation) ä¸Šçš„å·®å¼‚å®šä¹‰ä¸º Ground Truth è§£é‡Šã€‚éšåï¼Œè¯¥å·®å¼‚ä¿¡å·è¢«ç”¨ä½œç›‘ç£ä¿¡å·æ¥è®­ç»ƒæ‰©æ•£æ¨¡å‹ (Diffusion Model)ï¼Œä»è€Œèƒ½å¤Ÿç²¾å‡†è¯†åˆ«å¹¶æš´éœ²ç»™å®šä¼ªé€ éŸ³é¢‘ä¸­çš„ä¼ªå½±åŒºåŸŸã€‚åœ¨ VocV4 å’Œ LibriSeVoc æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­å‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„è§£é‡Šæ€§æŠ€æœ¯ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.AS",
      "comment": "5 pages, 3 figures, accepted at Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.03425v1",
      "published_date": "2025-06-03 22:10:53 UTC",
      "updated_date": "2025-06-03 22:10:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:54:18.586241+00:00"
    },
    {
      "arxiv_id": "2506.03407v1",
      "title": "Multi-Spectral Gaussian Splatting with Neural Color Representation",
      "title_zh": "åŸºäºç¥ç»é¢œè‰²è¡¨å¾çš„å¤šå…‰è°±é«˜æ–¯æ³¼æº…",
      "authors": [
        "Lukas Meyer",
        "Josef GrÃ¼n",
        "Maximilian Weiherer",
        "Bernhard Egger",
        "Marc Stamminger",
        "Linus Franke"
      ],
      "abstract": "We present MS-Splatting -- a multi-spectral 3D Gaussian Splatting (3DGS) framework that is able to generate multi-view consistent novel views from images of multiple, independent cameras with different spectral domains. In contrast to previous approaches, our method does not require cross-modal camera calibration and is versatile enough to model a variety of different spectra, including thermal and near-infra red, without any algorithmic changes.\n  Unlike existing 3DGS-based frameworks that treat each modality separately (by optimizing per-channel spherical harmonics) and therefore fail to exploit the underlying spectral and spatial correlations, our method leverages a novel neural color representation that encodes multi-spectral information into a learned, compact, per-splat feature embedding. A shallow multi-layer perceptron (MLP) then decodes this embedding to obtain spectral color values, enabling joint learning of all bands within a unified representation.\n  Our experiments show that this simple yet effective strategy is able to improve multi-spectral rendering quality, while also leading to improved per-spectra rendering quality over state-of-the-art methods. We demonstrate the effectiveness of this new technique in agricultural applications to render vegetation indices, such as normalized difference vegetation index (NDVI).",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MS-Splattingï¼Œä¸€ç§å¤šå…‰è°± 3D Gaussian Splatting (3DGS) æ¡†æ¶ï¼Œæ—¨åœ¨ä»å…·æœ‰ä¸åŒå…‰è°±èŒƒå›´çš„å¤šä¸ªç‹¬ç«‹æ‘„åƒæœºå›¾åƒä¸­ç”Ÿæˆå¤šè§†å›¾ä¸€è‡´çš„æ–°è§†ç‚¹ã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦è·¨æ¨¡æ€æ‘„åƒæœºæ ‡å®šï¼Œèƒ½å¤Ÿçµæ´»åœ°å¯¹åŒ…æ‹¬çƒ­æˆåƒå’Œè¿‘çº¢å¤–åœ¨å†…çš„å¤šç§å…‰è°±è¿›è¡Œå»ºæ¨¡ã€‚é’ˆå¯¹ç°æœ‰ 3DGS æ¡†æ¶å› å•ç‹¬ä¼˜åŒ–å„é€šé“ Spherical Harmonics è€Œå¿½ç•¥å…‰è°±å’Œç©ºé—´ç›¸å…³æ€§çš„å±€é™ï¼ŒMS-Splatting é‡‡ç”¨äº†ä¸€ç§æ–°å‹çš„ç¥ç»é¢œè‰²è¡¨ç¤ºæ³•ï¼Œå°†å¤šå…‰è°±ä¿¡æ¯ç¼–ç åˆ°æ¯ä¸ª Splat çš„ç‰¹å¾åµŒå…¥ä¸­ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æµ…å±‚å¤šå±‚æ„ŸçŸ¥æœº (MLP) è§£ç è¯¥åµŒå…¥ä»¥è·å–å…‰è°±é¢œè‰²å€¼ï¼Œä»è€Œåœ¨ç»Ÿä¸€è¡¨ç¤ºä¸­å®ç°æ‰€æœ‰æ³¢æ®µçš„è”åˆå­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§ç­–ç•¥æ˜¾è‘—æé«˜äº†å¤šå…‰è°±æ¸²æŸ“è´¨é‡ï¼Œä¸”åœ¨å•å…‰è°±è¡¨ç°ä¸Šä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯åœ¨å†œä¸šé¢†åŸŸå±•ç°äº†æ˜¾è‘—åº”ç”¨æ½œåŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ¸²æŸ“æ¤è¢«æŒ‡æ•°ï¼Œå¦‚å½’ä¸€åŒ–æ¤è¢«æŒ‡æ•° (NDVI)ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03407v1",
      "published_date": "2025-06-03 21:36:50 UTC",
      "updated_date": "2025-06-03 21:36:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:54:24.652376+00:00"
    },
    {
      "arxiv_id": "2506.03404v1",
      "title": "The Impact of On-Policy Parallelized Data Collection on Deep Reinforcement Learning Networks",
      "title_zh": "åŒç­–ç•¥å¹¶è¡Œæ•°æ®é‡‡é›†å¯¹æ·±åº¦å¼ºåŒ–å­¦ä¹ ç½‘ç»œçš„å½±å“",
      "authors": [
        "Walter Mayor",
        "Johan Obando-Ceron",
        "Aaron Courville",
        "Pablo Samuel Castro"
      ],
      "abstract": "The use of parallel actors for data collection has been an effective technique used in reinforcement learning (RL) algorithms. The manner in which data is collected in these algorithms, controlled via the number of parallel environments and the rollout length, induces a form of bias-variance trade-off; the number of training passes over the collected data, on the other hand, must strike a balance between sample efficiency and overfitting. We conduct an empirical analysis of these trade-offs on PPO, one of the most popular RL algorithms that uses parallel actors, and establish connections to network plasticity and, more generally, optimization stability. We examine its impact on network architectures, as well as the hyper-parameter sensitivity when scaling data. Our analyses indicate that larger dataset sizes can increase final performance across a variety of settings, and that scaling parallel environments is more effective than increasing rollout lengths. These findings highlight the critical role of data collection strategies in improving agent performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦å¼ºåŒ–å­¦ä¹ (Deep Reinforcement Learning)ä¸­çš„å¹¶è¡Œæ•°æ®é‡‡é›†ç­–ç•¥è¿›è¡Œäº†æ·±å…¥çš„ç»éªŒåˆ†æï¼Œé‡ç‚¹æ¢è®¨äº†åœ¨PPOç®—æ³•ä¸­å¹¶è¡Œç¯å¢ƒ(parallel environments)æ•°é‡ã€é‡‡æ ·é•¿åº¦(rollout length)ä»¥åŠè®­ç»ƒæ¬¡æ•°(training passes)ä¹‹é—´çš„æƒè¡¡å…³ç³»ã€‚ç ”ç©¶é€šè¿‡å®éªŒæ­ç¤ºäº†è¿™äº›å‚æ•°åœ¨åå·®-æ–¹å·®æƒè¡¡(bias-variance trade-off)ä¸­çš„ä½œç”¨ï¼Œå¹¶å»ºç«‹äº†å…¶ä¸ç½‘ç»œå¯å¡‘æ€§(network plasticity)åŠä¼˜åŒ–ç¨³å®šæ€§(optimization stability)çš„å†…åœ¨è”ç³»ã€‚åˆ†æè¡¨æ˜ï¼Œå¢åŠ æ•°æ®é›†è§„æ¨¡åœ¨å¤šç§è®¾å®šä¸‹å‡èƒ½æ˜¾è‘—æå‡æœ€ç»ˆæ€§èƒ½ï¼Œä¸”æ‰©å±•å¹¶è¡Œç¯å¢ƒçš„æ•°é‡æ¯”å•çº¯å¢åŠ é‡‡æ ·é•¿åº¦æ›´ä¸ºæœ‰æ•ˆã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¯„ä¼°äº†æ•°æ®æ‰©å±•å¯¹ç½‘ç»œæ¶æ„çš„å½±å“åŠè¶…å‚æ•°çš„æ•æ„Ÿæ€§ã€‚è¿™äº›å‘ç°çªæ˜¾äº†æ•°æ®é‡‡é›†ç­–ç•¥åœ¨æå‡æ™ºèƒ½ä½“æ€§èƒ½ä¸­çš„æ ¸å¿ƒåœ°ä½ï¼Œä¸ºä¼˜åŒ–æ·±åº¦å¼ºåŒ–å­¦ä¹ ç½‘ç»œçš„è®­ç»ƒæä¾›äº†å…³é”®ä¾æ®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Proceedings of the 42nd International Conference on Machine Learning (ICML 2025)",
      "pdf_url": "https://arxiv.org/pdf/2506.03404v1",
      "published_date": "2025-06-03 21:27:17 UTC",
      "updated_date": "2025-06-03 21:27:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:54:21.528472+00:00"
    },
    {
      "arxiv_id": "2506.03399v1",
      "title": "Sampling Preferences Yields Simple Trustworthiness Scores",
      "title_zh": "åå¥½é‡‡æ ·ç”Ÿæˆç®€æ˜å¯ä¿¡åº¦è¯„åˆ†",
      "authors": [
        "Sean Steinle"
      ],
      "abstract": "With the onset of large language models (LLMs), the performance of artificial intelligence (AI) models is becoming increasingly multi-dimensional. Accordingly, there have been several large, multi-dimensional evaluation frameworks put forward to evaluate LLMs. Though these frameworks are much more realistic than previous attempts which only used a single score like accuracy, multi-dimensional evaluations can complicate decision-making since there is no obvious way to select an optimal model. This work introduces preference sampling, a method to extract a scalar trustworthiness score from multi-dimensional evaluation results by considering the many characteristics of model performance which users value. We show that preference sampling improves upon alternate aggregation methods by using multi-dimensional trustworthiness evaluations of LLMs from TrustLLM and DecodingTrust. We find that preference sampling is consistently reductive, fully reducing the set of candidate models 100% of the time whereas Pareto optimality never reduces the set by more than 50%. Likewise, preference sampling is consistently sensitive to user priors-allowing users to specify the relative weighting and confidence of their preferences-whereas averaging scores is intransigent to the users' prior knowledge.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) å¤šç»´åº¦è¯„ä¼°æ¡†æ¶å¯¼è‡´å†³ç­–å›°éš¾çš„é—®é¢˜ï¼Œæå‡ºäº† preference sampling æ–¹æ³•ï¼Œæ—¨åœ¨ä»å¤æ‚è¯„ä¼°ç»“æœä¸­æå–æ ‡é‡çš„å¯ä¿¡åº¦è¯„åˆ† (trustworthiness score)ã€‚è¯¥æ–¹æ³•é€šè¿‡è€ƒè™‘ç”¨æˆ·é‡è§†çš„å¤šç»´æ¨¡å‹æ€§èƒ½ç‰¹å¾ï¼Œå®ç°äº†å¯¹ TrustLLM å’Œ DecodingTrust ç­‰å¤šç»´è¯„ä¼°æ•°æ®çš„æœ‰æ•ˆèšåˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œpreference sampling åœ¨å€™é€‰æ¨¡å‹ç­›é€‰ä¸Šè¡¨ç°å‡º 100% çš„çº¦ç®€èƒ½åŠ›ï¼Œè¿œè¶…å¸•ç´¯æ‰˜æœ€ä¼˜ (Pareto optimality) ä¸è¶³ 50% çš„çº¦ç®€æ•ˆæœã€‚æ­¤å¤–ï¼Œä¸ä¼ ç»Ÿçš„å¹³å‡åˆ† (averaging scores) ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å¯¹ç”¨æˆ·å…ˆéªŒ (user priors) å…·æœ‰é«˜åº¦æ•æ„Ÿæ€§ï¼Œå…è®¸ç”¨æˆ·æ ¹æ®ä¸ªäººåå¥½è‡ªå®šä¹‰æƒé‡å’Œç½®ä¿¡åº¦ã€‚è¿™ä¸€ç ”ç©¶ä¸ºåœ¨å¤šç»´åº¦è¯„ä¼°ç¯å¢ƒä¸‹å¿«é€Ÿã€å‡†ç¡®åœ°é€‰æ‹©æœ€ä¼˜æ¨¡å‹æä¾›äº†å®ç”¨ä¸”çµæ´»çš„è¯„åˆ†æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03399v1",
      "published_date": "2025-06-03 21:14:35 UTC",
      "updated_date": "2025-06-03 21:14:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:54:29.593358+00:00"
    },
    {
      "arxiv_id": "2506.06363v1",
      "title": "ChemGraph: An Agentic Framework for Computational Chemistry Workflows",
      "title_zh": "ChemGraphï¼šé¢å‘è®¡ç®—åŒ–å­¦å·¥ä½œæµçš„æ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Thang D. Pham",
        "Aditya Tanikanti",
        "Murat KeÃ§eli"
      ],
      "abstract": "Atomistic simulations are essential tools in chemistry and materials science, accelerating the discovery of novel catalysts, energy storage materials, and pharmaceuticals. However, running these simulations remains challenging due to the wide range of computational methods, diverse software ecosystems, and the need for expert knowledge and manual effort for the setup, execution, and validation stages. In this work, we present ChemGraph, an agentic framework powered by artificial intelligence and state-of-the-art simulation tools to streamline and automate computational chemistry and materials science workflows. ChemGraph leverages graph neural network-based foundation models for accurate yet computationally efficient calculations and large language models (LLMs) for natural language understanding, task planning, and scientific reasoning to provide an intuitive and interactive interface. Users can perform tasks such as molecular structure generation, single-point energy, geometry optimization, vibrational analysis, and thermochemistry calculations with methods ranging from tight-binding and machine learning interatomic potentials to density functional theory or wave function theory-based methods. We evaluate ChemGraph across 13 benchmark tasks and demonstrate that smaller LLMs (GPT-4o-mini, Claude-3.5-haiku, Qwen2.5-14B) perform well on simple workflows, while more complex tasks benefit from using larger models like GPT-4o. Importantly, we show that decomposing complex tasks into smaller subtasks through a multi-agent framework enables smaller LLM models to match or exceed GPT-4o's performance in specific scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ChemGraphï¼Œè¿™æ˜¯ä¸€ä¸ªç”±äººå·¥æ™ºèƒ½å’Œå‰æ²¿æ¨¡æ‹Ÿå·¥å…·é©±åŠ¨çš„æ™ºèƒ½ä½“æ¡†æ¶ (Agentic Framework)ï¼Œæ—¨åœ¨ç®€åŒ–å¹¶è‡ªåŠ¨åŒ–è®¡ç®—åŒ–å­¦å’Œææ–™ç§‘å­¦çš„å·¥ä½œæµã€‚ChemGraph ç»“åˆäº†åŸºäºå›¾ç¥ç»ç½‘ç»œ (Graph Neural Network) çš„åŸºç¡€æ¨¡å‹ä»¥å®ç°é«˜æ•ˆå‡†ç¡®çš„è®¡ç®—ï¼Œå¹¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) è¿›è¡Œè‡ªç„¶è¯­è¨€ç†è§£ã€ä»»åŠ¡è§„åˆ’åŠç§‘å­¦æ¨ç†ã€‚è¯¥æ¡†æ¶æ”¯æŒåˆ†å­ç»“æ„ç”Ÿæˆ (Molecular Structure Generation)ã€å‡ ä½•ä¼˜åŒ– (Geometry Optimization) å’Œçƒ­åŒ–å­¦è®¡ç®— (Thermochemistry Calculations) ç­‰ä»»åŠ¡ï¼Œæ–¹æ³•æ¶µç›–ä»ç´§æŸç¼š (Tight-binding) åˆ°å¯†åº¦æ³›å‡½ç†è®º (Density Functional Theory) ç­‰å¤šç§æŠ€æœ¯ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨ 13 é¡¹åŸºå‡†ä»»åŠ¡ä¸Šå¯¹è¯¥æ¡†æ¶è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºå°å‹ LLMs åœ¨ç®€å•å·¥ä½œæµä¸­è¡¨ç°è‰¯å¥½ï¼Œè€Œå¤æ‚ä»»åŠ¡åˆ™æ›´ä¾èµ–äº GPT-4o ç­‰å¤§å‹æ¨¡å‹ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œç ”ç©¶è¯æ˜é€šè¿‡å¤šæ™ºèƒ½ä½“æ¡†æ¶ (Multi-agent Framework) å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå­ä»»åŠ¡ï¼Œèƒ½ä½¿å°å‹æ¨¡å‹åœ¨ç‰¹å®šåœºæ™¯ä¸‹çš„è¡¨ç°åŒ¹é…ç”šè‡³è¶…è¶Š GPT-4oã€‚",
      "categories": [
        "physics.chem-ph",
        "cond-mat.mtrl-sci",
        "cs.AI",
        "cs.LG",
        "physics.comp-ph"
      ],
      "primary_category": "physics.chem-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06363v1",
      "published_date": "2025-06-03 21:11:56 UTC",
      "updated_date": "2025-06-03 21:11:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:54:30.131320+00:00"
    },
    {
      "arxiv_id": "2506.03391v1",
      "title": "Universal Reusability in Recommender Systems: The Case for Dataset- and Task-Independent Frameworks",
      "title_zh": "æ¨èç³»ç»Ÿçš„é€šç”¨å¤ç”¨æ€§ï¼šè®ºç‹¬ç«‹äºæ•°æ®é›†ä¸ä»»åŠ¡çš„æ¡†æ¶",
      "authors": [
        "Tri Kurniawan Wijaya",
        "Xinyang Shao",
        "Gonzalo Fiz Pontiveros",
        "Edoardo D'Amico"
      ],
      "abstract": "Recommender systems are pivotal in delivering personalized experiences across industries, yet their adoption and scalability remain hindered by the need for extensive dataset- and task-specific configurations. Existing systems often require significant manual intervention, domain expertise, and engineering effort to adapt to new datasets or tasks, creating barriers to entry and limiting reusability. In contrast, recent advancements in large language models (LLMs) have demonstrated the transformative potential of reusable systems, where a single model can handle diverse tasks without significant reconfiguration. Inspired by this paradigm, we propose the Dataset- and Task-Independent Recommender System (DTIRS), a framework aimed at maximizing the reusability of recommender systems while minimizing barriers to entry. Unlike LLMs, which achieve task generalization directly, DTIRS focuses on eliminating the need to rebuild or reconfigure recommendation pipelines for every new dataset or task, even though models may still need retraining on new data. By leveraging the novel Dataset Description Language (DsDL), DTIRS enables standardized dataset descriptions and explicit task definitions, allowing autonomous feature engineering, model selection, and optimization. This paper introduces the concept of DTIRS and establishes a roadmap for transitioning from Level-1 automation (dataset-agnostic but task-specific systems) to Level-2 automation (fully dataset- and task-independent systems). Achieving this paradigm would maximize code reusability and lower barriers to adoption. We discuss key challenges, including the trade-offs between generalization and specialization, computational overhead, and scalability, while presenting DsDL as a foundational tool for this vision.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¨èç³»ç»Ÿ(Recommender Systems)åœ¨è·¨è¡Œä¸šåº”ç”¨ä¸­å› éœ€å¤§é‡é’ˆå¯¹æ•°æ®é›†å’Œä»»åŠ¡çš„ç‰¹å®šé…ç½®è€Œå¯¼è‡´çš„æ‰©å±•æ€§ç“¶é¢ˆï¼Œå€Ÿé‰´å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„å¯é‡ç”¨æ€§èŒƒå¼ï¼Œæå‡ºäº†æ•°æ®é›†å’Œä»»åŠ¡ç‹¬ç«‹æ¨èç³»ç»Ÿ(Dataset- and Task-Independent Recommender System, DTIRS)æ¡†æ¶ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯å¼•å…¥äº†ä¸€ç§å…¨æ–°çš„æ•°æ®é›†æè¿°è¯­è¨€(Dataset Description Language, DsDL)ï¼Œç”¨äºå®ç°æ ‡å‡†åŒ–çš„æ•°æ®é›†æè¿°å’Œæ˜ç¡®çš„ä»»åŠ¡å®šä¹‰ï¼Œä»è€Œæ”¯æŒè‡ªåŠ¨åŒ–çš„ç‰¹å¾å·¥ç¨‹(feature engineering)ã€æ¨¡å‹é€‰æ‹©ä¸ä¼˜åŒ–ã€‚DTIRSæ—¨åœ¨æ¶ˆé™¤ä¸ºæ¯ä¸ªæ–°åœºæ™¯é‡æ–°æ„å»ºæˆ–é…ç½®æ¨èæµæ°´çº¿çš„éœ€æ±‚ï¼Œå¹¶ç¡®ç«‹äº†ä»1çº§è‡ªåŠ¨åŒ–(dataset-agnostic)å‘å®Œå…¨ç‹¬ç«‹äºæ•°æ®é›†å’Œä»»åŠ¡çš„2çº§è‡ªåŠ¨åŒ–ç³»ç»Ÿæ¼”è¿›çš„è·¯çº¿å›¾ã€‚è¯¥ç ”ç©¶é€šè¿‡æœ€å¤§åŒ–ä»£ç å¯é‡ç”¨æ€§æ˜¾è‘—é™ä½äº†æ¨èç³»ç»Ÿçš„å‡†å…¥é—¨æ§›ï¼Œå¹¶æ·±å…¥è®¨è®ºäº†é€šç”¨æ€§ä¸ä¸“ä¸šåŒ–ä¹‹é—´çš„æƒè¡¡ã€è®¡ç®—å¼€é”€ä»¥åŠå¯æ‰©å±•æ€§ç­‰å…³é”®æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.DB",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03391v1",
      "published_date": "2025-06-03 21:00:34 UTC",
      "updated_date": "2025-06-03 21:00:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:54:35.744029+00:00"
    },
    {
      "arxiv_id": "2506.03381v1",
      "title": "Automated Traffic Incident Response Plans using Generative Artificial Intelligence: Part 1 -- Building the Incident Response Benchmark",
      "title_zh": "åŸºäºç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„è‡ªåŠ¨åŒ–äº¤é€šäº‹ä»¶å“åº”æ–¹æ¡ˆï¼šç¬¬ä¸€éƒ¨åˆ†â€”â€”æ„å»ºäº‹ä»¶å“åº”åŸºå‡†",
      "authors": [
        "Artur Grigorev",
        "Khaled Saleh",
        "Jiwon Kim",
        "Adriana-Simona Mihaita"
      ],
      "abstract": "Traffic incidents remain a critical public safety concern worldwide, with Australia recording 1,300 road fatalities in 2024, which is the highest toll in 12 years. Similarly, the United States reports approximately 6 million crashes annually, raising significant challenges in terms of a fast reponse time and operational management. Traditional response protocols rely on human decision-making, which introduces potential inconsistencies and delays during critical moments when every minute impacts both safety outcomes and network performance. To address this issue, we propose a novel Incident Response Benchmark that uses generative artificial intelligence to automatically generate response plans for incoming traffic incidents. Our approach aims to significantly reduce incident resolution times by suggesting context-appropriate actions such as variable message sign deployment, lane closures, and emergency resource allocation adapted to specific incident characteristics. First, the proposed methodology uses real-world incident reports from the Performance Measurement System (PeMS) as training and evaluation data. We extract historically implemented actions from these reports and compare them against AI-generated response plans that suggest specific actions, such as lane closures, variable message sign announcements, and/or dispatching appropriate emergency resources. Second, model evaluations reveal that advanced generative AI models like GPT-4o and Grok 2 achieve superior alignment with expert solutions, demonstrated by minimized Hamming distances (averaging 2.96-2.98) and low weighted differences (approximately 0.27-0.28). Conversely, while Gemini 1.5 Pro records the lowest count of missed actions, its extremely high number of unnecessary actions (1547 compared to 225 for GPT-4o) indicates an over-triggering strategy that reduces the overall plan efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…¨çƒä¸¥å³»çš„äº¤é€šäº‹æ•…å“åº”æ•ˆç‡é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative Artificial Intelligence)çš„äº‹æ•…å“åº”åŸºå‡†(Incident Response Benchmark)ï¼Œæ—¨åœ¨é€šè¿‡è‡ªåŠ¨ç”Ÿæˆå“åº”è®¡åˆ’æ¥æ˜¾è‘—ç¼©çŸ­äº‹æ•…å¤„ç½®æ—¶é—´ã€‚è¯¥æ–¹æ³•åˆ©ç”¨Performance Measurement System (PeMS)çš„çœŸå®äº‹æ•…æŠ¥å‘Šæ•°æ®ï¼Œé€šè¿‡AIå»ºè®®åŒ…æ‹¬å¯å˜æƒ…æŠ¥æ¿(Variable Message Sign)éƒ¨ç½²ã€è½¦é“å…³é—­åŠåº”æ€¥èµ„æºåˆ†é…åœ¨å†…çš„æƒ…æ™¯åŒ–æ–¹æ¡ˆã€‚ç ”ç©¶å¯¹æ¯”äº†å†å²å®æ–½è¡ŒåŠ¨ä¸AIç”Ÿæˆçš„è®¡åˆ’ï¼Œè¯„ä¼°ç»“æœè¡¨æ˜GPT-4oå’ŒGrok 2åœ¨ä¸ä¸“å®¶æ–¹æ¡ˆçš„å¯¹é½åº¦ä¸Šè¡¨ç°æœ€å¿§ï¼Œå…¶æµ·æ˜è·ç¦»(Hamming distances)å’ŒåŠ æƒå·®å¼‚(weighted differences)å‡è¾¾åˆ°æä½æ°´å¹³ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒGemini 1.5 Proè™½ç„¶æ¼æ‰çš„è¡ŒåŠ¨æŒ‡ä»¤æœ€å°‘ï¼Œä½†å› ä¸¥é‡çš„è¿‡åº¦è§¦å‘(over-triggering)äº§ç”Ÿäº†å¤§é‡ä¸å¿…è¦è¡ŒåŠ¨ï¼Œæ˜¾è‘—é™ä½äº†è®¡åˆ’çš„æ•´ä½“æ•ˆç‡ã€‚è¯¥åŸºå‡†çš„å»ºç«‹ä¸ºå®ç°è‡ªåŠ¨åŒ–ã€ä¸€è‡´æ€§ä¸”é«˜æ•ˆçš„äº¤é€šæ„å¤–äº‹ä»¶ç®¡ç†åŠå†³ç­–æ”¯æŒå¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03381v1",
      "published_date": "2025-06-03 20:40:44 UTC",
      "updated_date": "2025-06-03 20:40:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:55:38.759846+00:00"
    },
    {
      "arxiv_id": "2506.03373v1",
      "title": "A Foundation Model for Spatial Proteomics",
      "title_zh": "ç©ºé—´è›‹ç™½è´¨ç»„å­¦åŸºç¡€æ¨¡å‹",
      "authors": [
        "Muhammad Shaban",
        "Yuzhou Chang",
        "Huaying Qiu",
        "Yao Yu Yeo",
        "Andrew H. Song",
        "Guillaume Jaume",
        "Yuchen Wang",
        "Luca L. Weishaupt",
        "Tong Ding",
        "Anurag Vaidya",
        "Abdallah Lamane",
        "Daniel Shao",
        "Mohammed Zidane",
        "Yunhao Bai",
        "Paige McCallum",
        "Shuli Luo",
        "Wenrui Wu",
        "Yang Wang",
        "Precious Cramer",
        "Chi Ngai Chan",
        "Pierre Stephan",
        "Johanna Schaffenrath",
        "Jia Le Lee",
        "Hendrik A. Michel",
        "Caiwei Tian",
        "Cristina Almagro-Perez",
        "Sophia J. Wagner",
        "Sharifa Sahai",
        "Ming Y. Lu",
        "Richard J. Chen",
        "Andrew Zhang",
        "Mark Edward M. Gonzales",
        "Ahmad Makky",
        "Jia-Ying Joey Lee",
        "Hao Cheng",
        "Nourhan El Ahmar",
        "Sayed Matar",
        "Maximilian Haist",
        "Darci Phillips",
        "Yuqi Tan",
        "Garry P. Nolan",
        "W. Richard Burack",
        "Jacob D. Estes",
        "Jonathan T. C. Liu",
        "Toni K Choueiri",
        "Neeraj Agarwal",
        "Marc Barry",
        "Scott J. Rodig",
        "Long Phi Le",
        "Georg Gerber",
        "Christian M. SchÃ¼rch",
        "Fabian J. Theis",
        "Youn H Kim",
        "Joe Yeong",
        "Sabina Signoretti",
        "Brooke E. Howitt",
        "Lit-Hsin Loo",
        "Qin Ma",
        "Sizun Jiang",
        "Faisal Mahmood"
      ],
      "abstract": "Foundation models have begun to transform image analysis by acting as pretrained generalist backbones that can be adapted to many tasks even when post-training data are limited, yet their impact on spatial proteomics, imaging that maps proteins at single-cell resolution, remains limited. Here, we introduce KRONOS, a foundation model built for spatial proteomics. KRONOS was trained in a self-supervised manner on over 47 million image patches covering 175 protein markers, 16 tissue types, and 8 fluorescence-based imaging platforms. We introduce key architectural adaptations to address the high-dimensional, multi-channel, and heterogeneous nature of multiplex imaging. We demonstrate that KRONOS learns biologically meaningful representations across multiple scales, ranging from cellular and microenvironment to tissue levels, enabling it to address diverse downstream tasks, including cell phenotyping, region classification, and patient stratification. Evaluated across 11 independent cohorts, KRONOS achieves state-of-the-art performance across cell phenotyping, treatment response prediction, and retrieval tasks, and is highly data-efficient. KRONOS also introduces the paradigm of segmentation-free patch-level processing for efficient and scalable spatial proteomics analysis, allowing cross-institutional comparisons, and as an image reverse search engine for spatial patterns. Together, these results position KRONOS as a flexible and scalable tool for spatial proteomics. The model is publicly accessible at https://github.com/mahmoodlab/KRONOS.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†KRONOSï¼Œè¿™æ˜¯ä¸€æ¬¾ä¸“é—¨ä¸ºç©ºé—´è›‹ç™½è´¨ç»„å­¦(Spatial Proteomics)è®¾è®¡çš„åŸºåº§æ¨¡å‹(Foundation Model)ï¼Œæ—¨åœ¨è§£å†³è¯¥é¢†åŸŸå›¾åƒåˆ†æåœ¨é¢„è®­ç»ƒé€šç”¨éª¨å¹²ç½‘ç»œæ–¹é¢çš„ç©ºç™½ã€‚KRONOSåœ¨æ¶µç›–175ç§è›‹ç™½è´¨æ ‡å¿—ç‰©ã€16ç§ç»„ç»‡ç±»å‹å’Œ8ä¸ªæˆåƒå¹³å°çš„è¶…è¿‡4700ä¸‡ä¸ªå›¾åƒå—ä¸Šé€šè¿‡è‡ªç›‘ç£å­¦ä¹ (Self-supervised)è¿›è¡Œè®­ç»ƒï¼Œå¹¶é’ˆå¯¹å¤šé€šé“æˆåƒ(Multiplex Imaging)çš„é«˜ç»´å¼‚è´¨æ€§è¿›è¡Œäº†æ¶æ„ä¼˜åŒ–ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ ä»ç»†èƒã€å¾®ç¯å¢ƒåˆ°ç»„ç»‡å±‚é¢çš„å¤šå°ºåº¦ç”Ÿç‰©å­¦è¡¨å¾ï¼Œæœ‰æ•ˆåº”å¯¹ç»†èƒè¡¨å‹åˆ†æ(Cell Phenotyping)ã€åŒºåŸŸåˆ†ç±»å’Œæ‚£è€…åˆ†å±‚ç­‰ä¸‹æ¸¸ä»»åŠ¡ã€‚åœ¨11ä¸ªç‹¬ç«‹é˜Ÿåˆ—çš„è¯„ä¼°ä¸­ï¼ŒKRONOSåœ¨æ²»ç–—ååº”é¢„æµ‹å’Œæ£€ç´¢ä»»åŠ¡ä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›(State-of-the-art)çš„æ€§èƒ½ï¼Œä¸”å…·æœ‰æé«˜çš„æ•°æ®æ•ˆç‡ã€‚æ­¤å¤–ï¼ŒKRONOSå¼•å…¥äº†æ— éœ€åˆ†å‰²çš„å›¾åƒå—çº§å¤„ç†(Segmentation-free patch-level processing)èŒƒå¼ï¼Œæ”¯æŒè·¨æœºæ„æ¯”è¾ƒå’Œç©ºé—´æ¨¡å¼çš„å›¾åƒåå‘æœç´¢ï¼Œä¸ºç©ºé—´è›‹ç™½è´¨ç»„å­¦åˆ†ææä¾›äº†çµæ´»ä¸”å¯æ‰©å±•çš„å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03373v1",
      "published_date": "2025-06-03 20:30:25 UTC",
      "updated_date": "2025-06-03 20:30:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:54:40.407463+00:00"
    },
    {
      "arxiv_id": "2506.05397v1",
      "title": "Gen4D: Synthesizing Humans and Scenes in the Wild",
      "title_zh": "Gen4Dï¼šè‡ªç„¶åœºæ™¯ä¸‹çš„äººä½“ä¸åœºæ™¯åˆæˆ",
      "authors": [
        "Jerrin Bright",
        "Zhibo Wang",
        "Yuhao Chen",
        "Sirisha Rambhatla",
        "John Zelek",
        "David Clausi"
      ],
      "abstract": "Lack of input data for in-the-wild activities often results in low performance across various computer vision tasks. This challenge is particularly pronounced in uncommon human-centric domains like sports, where real-world data collection is complex and impractical. While synthetic datasets offer a promising alternative, existing approaches typically suffer from limited diversity in human appearance, motion, and scene composition due to their reliance on rigid asset libraries and hand-crafted rendering pipelines. To address this, we introduce Gen4D, a fully automated pipeline for generating diverse and photorealistic 4D human animations. Gen4D integrates expert-driven motion encoding, prompt-guided avatar generation using diffusion-based Gaussian splatting, and human-aware background synthesis to produce highly varied and lifelike human sequences. Based on Gen4D, we present SportPAL, a large-scale synthetic dataset spanning three sports: baseball, icehockey, and soccer. Together, Gen4D and SportPAL provide a scalable foundation for constructing synthetic datasets tailored to in-the-wild human-centric vision tasks, with no need for manual 3D modeling or scene design.",
      "tldr_zh": "é’ˆå¯¹é‡å¤–æ´»åŠ¨ï¼ˆin-the-wild activitiesï¼‰ç‰¹åˆ«æ˜¯ä½“è‚²é¢†åŸŸç”±äºæ•°æ®é‡‡é›†å›°éš¾å¯¼è‡´çš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡æ€§èƒ½å—é™é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº† Gen4Dï¼Œä¸€ä¸ªç”¨äºç”Ÿæˆå¤šæ ·åŒ–ä¸”ç…§ç‰‡çº§çœŸå®æ„Ÿ 4D äººä½“åŠ¨ç”»çš„å…¨è‡ªåŠ¨ç®¡çº¿ã€‚ä¼ ç»Ÿçš„åˆæˆæ•°æ®é›†é€šå¸¸å—é™äºèµ„äº§åº“çš„å›ºå®šæ€§ï¼Œè€Œ Gen4D æ•´åˆäº†ä¸“å®¶é©±åŠ¨çš„ motion encodingã€åŸºäºæ‰©æ•£æ¨¡å‹çš„é«˜æ–¯æ³¼æº…ï¼ˆGaussian splattingï¼‰æç¤ºå¼•å¯¼åŒ–èº«ç”Ÿæˆï¼Œä»¥åŠäººä½“æ„ŸçŸ¥èƒŒæ™¯åˆæˆæŠ€æœ¯ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿåœ¨å¤§è§„æ¨¡ç”Ÿæˆé«˜åº¦é€¼çœŸä¸”å¤šæ ·åŒ–çš„äººä½“åºåˆ—çš„åŒæ—¶ï¼Œæ— éœ€ä»»ä½•æ‰‹åŠ¨ 3D å»ºæ¨¡æˆ–åœºæ™¯è®¾è®¡ã€‚åŸºäºè¯¥ç®¡çº¿ï¼Œç ”ç©¶è€…æ„å»ºäº†æ¶µç›–æ£’çƒã€å†°çƒå’Œè¶³çƒä¸‰é¡¹è¿åŠ¨çš„å¤§å‹åˆæˆæ•°æ®é›† SportPALã€‚Gen4D å’Œ SportPAL ä¸ºæ„å»ºé’ˆå¯¹é‡å¤–ä»¥äººä¸ºä¸­å¿ƒçš„è§†è§‰ä»»åŠ¡çš„åˆæˆæ•°æ®é›†æä¾›äº†å¯æ‰©å±•çš„åŸºç¡€ï¼Œæœ‰æ•ˆè§£å†³äº†äººä½“å¤–è§‚ã€åŠ¨ä½œå’Œåœºæ™¯æ„æˆçš„å¤šæ ·æ€§ä¸è¶³ç­‰æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI"
      ],
      "primary_category": "cs.GR",
      "comment": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops",
      "pdf_url": "https://arxiv.org/pdf/2506.05397v1",
      "published_date": "2025-06-03 20:04:41 UTC",
      "updated_date": "2025-06-03 20:04:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:55:01.955478+00:00"
    },
    {
      "arxiv_id": "2506.03357v1",
      "title": "Ask a Local: Detecting Hallucinations With Specialized Model Divergence",
      "title_zh": "Ask a Localï¼šåŸºäºä¸“é—¨æ¨¡å‹æ•£åº¦çš„å¹»è§‰æ£€æµ‹",
      "authors": [
        "Aldan Creo",
        "HÃ©ctor Cerezo-Costas",
        "Pedro Alonso-Doval",
        "Maximiliano HormazÃ¡bal-Lagos"
      ],
      "abstract": "Hallucinations in large language models (LLMs) - instances where models generate plausible but factually incorrect information - present a significant challenge for AI.\n  We introduce \"Ask a Local\", a novel hallucination detection method exploiting the intuition that specialized models exhibit greater surprise when encountering domain-specific inaccuracies. Our approach computes divergence between perplexity distributions of language-specialized models to identify potentially hallucinated spans. Our method is particularly well-suited for a multilingual context, as it naturally scales to multiple languages without the need for adaptation, relying on external data sources, or performing training. Moreover, we select computationally efficient models, providing a scalable solution that can be applied to a wide range of languages and domains.\n  Our results on a human-annotated question-answer dataset spanning 14 languages demonstrate consistent performance across languages, with Intersection-over-Union (IoU) scores around 0.3 and comparable Spearman correlation values. Our model shows particularly strong performance on Italian and Catalan, with IoU scores of 0.42 and 0.38, respectively, while maintaining cross-lingual effectiveness without language-specific adaptations. We release our code and architecture to facilitate further research in multilingual hallucination detection.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†â€œAsk a Localâ€ï¼Œä¸€ç§é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)å¹»è§‰(Hallucinations)æ£€æµ‹çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æ¨¡å‹ç”Ÿæˆäº‹å®é”™è¯¯ä¿¡æ¯çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨äº†ä¸“é—¨åŒ–æ¨¡å‹åœ¨é¢å¯¹ç‰¹å®šé¢†åŸŸé”™è¯¯æ—¶ä¼šè¡¨ç°å‡ºæ›´é«˜â€œæƒŠè®¶åº¦â€çš„ç›´è§‰ï¼Œé€šè¿‡è®¡ç®—è¯­è¨€ä¸“é—¨åŒ–æ¨¡å‹ä¹‹é—´å›°æƒ‘åº¦(Perplexity)åˆ†å¸ƒçš„æ•£åº¦(Divergence)æ¥è¯†åˆ«æ½œåœ¨çš„å¹»è§‰ç‰‡æ®µã€‚è¯¥æ–¹æ¡ˆç‰¹åˆ«é€‚ç”¨äºå¤šè¯­è¨€è¯­å¢ƒï¼Œæ— éœ€å¤–éƒ¨æ•°æ®æˆ–é¢å¤–è®­ç»ƒå³å¯è‡ªç„¶æ‰©å±•ï¼Œä¸”é‡‡ç”¨äº†è®¡ç®—æ•ˆç‡é«˜çš„æ¨¡å‹ä»¥ç¡®ä¿å¯æ‰©å±•æ€§ã€‚åœ¨æ¶µç›–14ç§è¯­è¨€çš„äººå·¥æ ‡æ³¨æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒè¯­è¨€é—´è¡¨ç°ä¸€è‡´ï¼Œå¹³å‡äº¤å¹¶æ¯”(IoU)å¾—åˆ†çº¦ä¸º0.3ã€‚ç ”ç©¶åœ¨æ„å¤§åˆ©è¯­å’ŒåŠ æ³°ç½—å°¼äºšè¯­ä¸Šè¡¨ç°å°¤ä¸ºå¼ºåŠ²ï¼ŒIoUå¾—åˆ†åˆ†åˆ«è¾¾åˆ°0.42å’Œ0.38ï¼Œè¯æ˜äº†å…¶åœ¨æ— éœ€ç‰¹å®šè¯­è¨€é€‚é…çš„æƒ…å†µä¸‹å…·å¤‡å‡ºè‰²çš„è·¨è¯­è¨€æ£€æµ‹æ•ˆèƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Supplementary materials: https://github.com/ACMCMC/ask-a-local",
      "pdf_url": "https://arxiv.org/pdf/2506.03357v1",
      "published_date": "2025-06-03 20:00:49 UTC",
      "updated_date": "2025-06-03 20:00:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:56:09.651287+00:00"
    },
    {
      "arxiv_id": "2506.03355v2",
      "title": "Robustness in Both Domains: CLIP Needs a Robust Text Encoder",
      "title_zh": "åŒåŸŸé²æ£’æ€§ï¼šCLIPéœ€è¦é²æ£’çš„æ–‡æœ¬ç¼–ç å™¨",
      "authors": [
        "Elias Abad Rocamora",
        "Christian Schlarmann",
        "Naman Deep Singh",
        "Yongtao Wu",
        "Matthias Hein",
        "Volkan Cevher"
      ],
      "abstract": "Adversarial input attacks can cause a significant shift of CLIP embeddings. This can affect the downstream robustness of models incorporating CLIP in the pipeline, such as text-to-image generative models or large vision language models. While some efforts have been done towards making the CLIP image encoders robust, the robustness of text encoders remains unexplored. In this work, we cover this gap in the literature. We propose LEAF: an efficient adversarial finetuning method for the text domain, with the ability to scale to large CLIP models. Our models significantly improve the zero-shot adversarial accuracy in the text domain, while maintaining the vision performance provided by robust image encoders. When combined with text-to-image diffusion models, we can improve the generation quality under adversarial noise. In multimodal retrieval tasks, LEAF improves the recall under adversarial noise over standard CLIP models. Finally, we show that robust text encoders facilitate better reconstruction of input text from its embedding via direct optimization. We open-source our code ( https://github.com/LIONS-EPFL/LEAF ) and models ( https://huggingface.co/LEAF-CLIP ).",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºå¯¹æŠ—æ€§è¾“å…¥æ”»å‡»ä¼šå¯¼è‡´ CLIP åµŒå…¥å‘ç”Ÿæ˜¾è‘—åç§»ï¼Œè¿›è€Œå½±å“æ–‡æœ¬ç”Ÿæˆå›¾åƒæˆ–å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ç­‰ä¸‹æ¸¸ä»»åŠ¡çš„é²æ£’æ€§ã€‚å°½ç®¡ç›®å‰å·²æœ‰é’ˆå¯¹ CLIP å›¾åƒç¼–ç å™¨é²æ£’æ€§çš„ç ”ç©¶ï¼Œä½†æ–‡æœ¬ç¼–ç å™¨çš„é²æ£’æ€§ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº† LEAFï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹æ–‡æœ¬é¢†åŸŸçš„é«˜æ•ˆå¯¹æŠ—å¾®è°ƒ(adversarial finetuning)æ–¹æ³•ï¼Œèƒ½å¤Ÿæ‰©å±•è‡³å¤§è§„æ¨¡ CLIP æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ˜¾è‘—æé«˜æ–‡æœ¬é¢†åŸŸ zero-shot å¯¹æŠ—å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œä¿æŒäº†é²æ£’å›¾åƒç¼–ç å™¨åŸæœ‰çš„è§†è§‰æ€§èƒ½ã€‚å½“ä¸æ–‡æœ¬ç”Ÿæˆå›¾åƒçš„æ‰©æ•£æ¨¡å‹ç»“åˆæ—¶ï¼ŒLEAF æœ‰æ•ˆæå‡äº†åœ¨å¯¹æŠ—å™ªå£°ä¸‹çš„ç”Ÿæˆè´¨é‡ã€‚åœ¨å¤šæ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸­ï¼ŒLEAF åœ¨å¯¹æŠ—å™ªå£°å¹²æ‰°ä¸‹çš„ recall è¡¨ç°ä¹Ÿä¼˜äºæ ‡å‡† CLIP æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¯æ˜é²æ£’çš„æ–‡æœ¬ç¼–ç å™¨æœ‰åŠ©äºé€šè¿‡ç›´æ¥ä¼˜åŒ–ä»åµŒå…¥ä¸­æ›´å¥½åœ°é‡å»ºè¾“å…¥æ–‡æœ¬ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted in NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.03355v2",
      "published_date": "2025-06-03 19:57:09 UTC",
      "updated_date": "2025-10-10 11:24:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:55:58.054816+00:00"
    },
    {
      "arxiv_id": "2506.03350v1",
      "title": "Adversarial Attacks on Robotic Vision Language Action Models",
      "title_zh": "é’ˆå¯¹æœºå™¨äººè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„å¯¹æŠ—æ€§æ”»å‡»",
      "authors": [
        "Eliot Krzysztof Jones",
        "Alexander Robey",
        "Andy Zou",
        "Zachary Ravichandran",
        "George J. Pappas",
        "Hamed Hassani",
        "Matt Fredrikson",
        "J. Zico Kolter"
      ],
      "abstract": "The emergence of vision-language-action models (VLAs) for end-to-end control is reshaping the field of robotics by enabling the fusion of multimodal sensory inputs at the billion-parameter scale. The capabilities of VLAs stem primarily from their architectures, which are often based on frontier large language models (LLMs). However, LLMs are known to be susceptible to adversarial misuse, and given the significant physical risks inherent to robotics, questions remain regarding the extent to which VLAs inherit these vulnerabilities. Motivated by these concerns, in this work we initiate the study of adversarial attacks on VLA-controlled robots. Our main algorithmic contribution is the adaptation and application of LLM jailbreaking attacks to obtain complete control authority over VLAs. We find that textual attacks, which are applied once at the beginning of a rollout, facilitate full reachability of the action space of commonly used VLAs and often persist over longer horizons. This differs significantly from LLM jailbreaking literature, as attacks in the real world do not have to be semantically linked to notions of harm. We make all code available at https://github.com/eliotjones1/robogcg .",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹(VLAs)åœ¨ç«¯åˆ°ç«¯æœºå™¨äººæ§åˆ¶ä¸­çš„è„†å¼±æ€§ï¼ŒæŒ‡å‡ºè¿™ç±»æ¨¡å‹ç”±äºç»§æ‰¿äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„æ¶æ„ï¼Œå¯èƒ½é¢ä¸´ç‰©ç†å±‚é¢çš„å®‰å…¨é£é™©ã€‚ä½œè€…é€šè¿‡å°†LLMçš„è¶Šç‹±æ”»å‡»(Jailbreaking Attacks)è¿›è¡Œæ”¹è¿›å¹¶åº”ç”¨äºVLAæ§åˆ¶çš„æœºå™¨äººï¼Œé¦–æ¬¡ç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†é’ˆå¯¹æ­¤ç±»æ¨¡å‹çš„å¯¹æŠ—æ€§æ”»å‡»ã€‚å®éªŒå‘ç°ï¼Œä»…åœ¨ä»»åŠ¡å¼€å§‹æ—¶åº”ç”¨ä¸€æ¬¡æ–‡æœ¬æ”»å‡»ï¼Œå³å¯è·å¾—å¯¹åŠ¨ä½œç©ºé—´(Action Space)çš„å®Œæ•´æ§åˆ¶æƒé™ã€‚ä¸ä¼ ç»Ÿçš„LLMè¶Šç‹±ç ”ç©¶ä¸åŒï¼Œæœºå™¨äººé¢†åŸŸçš„å¯¹æŠ—æ”»å‡»æ— éœ€åœ¨è¯­ä¹‰ä¸Šä¸æœ‰å®³æ¦‚å¿µç›¸å…³è”ï¼Œä¸”å…¶æ”»å‡»æ•ˆæœåœ¨é•¿æ—¶é—´æ­¥å†…å…·æœ‰æ˜¾è‘—çš„æŒç»­æ€§ã€‚è¯¥é¡¹å·¥ä½œæ­ç¤ºäº†å‰æ²¿æœºå™¨äººæ§åˆ¶æ¨¡å‹åœ¨å¯¹æŠ—å®‰å…¨æ€§æ–¹é¢çš„ä¸¥å³»æŒ‘æˆ˜ï¼Œå¹¶å¼€æºäº†ç›¸å…³ç ”ç©¶ä»£ç ä»¥æ¨åŠ¨å®‰å…¨é˜²å¾¡æŠ€æœ¯çš„å‘å±•ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03350v1",
      "published_date": "2025-06-03 19:43:58 UTC",
      "updated_date": "2025-06-03 19:43:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:56:12.733112+00:00"
    },
    {
      "arxiv_id": "2506.03337v1",
      "title": "Mitigating Non-IID Drift in Zeroth-Order Federated LLM Fine-Tuning with Transferable Sparsity",
      "title_zh": "åˆ©ç”¨å¯è¿ç§»ç¨€ç–æ€§ç¼“è§£é›¶é˜¶è”é‚¦å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒä¸­çš„é IID æ¼‚ç§»",
      "authors": [
        "Yide Ran",
        "Wentao Guo",
        "Jingwei Sun",
        "Yanzhou Pan",
        "Xiaodong Yu",
        "Hao Wang",
        "Jianwen Xie",
        "Yiran Chen",
        "Denghui Zhang",
        "Zhaozhuo Xu"
      ],
      "abstract": "Federated Learning enables collaborative fine-tuning of Large Language Models (LLMs) across decentralized Non-Independent and Identically Distributed (Non-IID) clients, but such models' massive parameter sizes lead to significant memory and communication challenges. This work introduces Meerkat, a sparse zeroth-order optimization (ZO) method designed for federated LLM fine-tuning. By limiting fine-tuning to a transferable, static, extremely sparse subset of parameters, Meerkat achieves remarkable communication efficiency, enabling cost-effective high-frequency synchronization. With theoretical analysis and experiments, we show that this high-frequency communication effectively mitigates Non-IID data challenges and leads to superior performance compared to full-parameter ZO. Furthermore, experiment results show that Meerkat outperforms existing sparsity baselines with better performance at the same communication frequency. To further handle Non-IID drift, Meerkat leverages traceable local updates and forms a virtual path for each client. This virtual path mechanism reveals the GradIP phenomenon: the inner products between LLM pre-training gradients maintained by server and client gradients estimated via ZO converges for extreme Non-IID clients but oscillates for IID ones. This distinct behavior provides a signal for identifying clients with extreme data heterogeneity. Using this signal, Meerkat-vp is proposed to analyze GradIP trajectories to identify extreme Non-IID clients and applies early stopping to enhance aggregated model quality. Experiments confirm that Meerkat and Meerkat-vp significantly improve the efficiency and effectiveness of ZO federated LLM fine-tuning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Meerkatï¼Œä¸€ç§ä¸“ä¸ºè”é‚¦å¤§è¯­è¨€æ¨¡å‹ (LLM) å¾®è°ƒè®¾è®¡çš„ç¨€ç–é›¶é˜¶ä¼˜åŒ– (Zeroth-Order Optimization, ZO) æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡å‚æ•°å¸¦æ¥çš„å†…å­˜å’Œé€šä¿¡æŒ‘æˆ˜ã€‚Meerkat é€šè¿‡å°†å¾®è°ƒé™åˆ¶åœ¨å¯è¿ç§»çš„ã€é™æ€çš„ä¸”æåº¦ç¨€ç–çš„å‚æ•°å­é›†ä¸Šï¼Œæ˜¾è‘—æå‡äº†é€šä¿¡æ•ˆç‡å¹¶æ”¯æŒé«˜é¢‘ç‡åŒæ­¥ã€‚ç†è®ºåˆ†æä¸å®éªŒè¡¨æ˜ï¼Œè¿™ç§é«˜é¢‘é€šä¿¡èƒ½æœ‰æ•ˆç¼“è§£éç‹¬ç«‹åŒåˆ†å¸ƒ (Non-IID) æ•°æ®æŒ‘æˆ˜ï¼Œå…¶æ€§èƒ½ä¼˜äºå…¨å‚æ•° ZO åŠç°æœ‰çš„ç¨€ç–åŸºå‡†æ¨¡å‹ã€‚ä¸ºäº†è¿›ä¸€æ­¥åº”å¯¹ Non-IID æ¼‚ç§»ï¼Œç ”ç©¶è€…åˆ©ç”¨è™šæ‹Ÿè·¯å¾„ (virtual path) æœºåˆ¶æ­ç¤ºäº† GradIP ç°è±¡ï¼Œå³æœåŠ¡å™¨é¢„è®­ç»ƒæ¢¯åº¦ä¸å®¢æˆ·ç«¯ä¼°ç®—æ¢¯åº¦ä¹‹é—´çš„å†…ç§¯åœ¨æç«¯ Non-IID å®¢æˆ·ç«¯ä¸Šè¶‹äºæ”¶æ•›ï¼Œè€Œåœ¨ IID å®¢æˆ·ç«¯ä¸Šåˆ™è¡¨ç°ä¸ºæŒ¯è¡ã€‚åŸºäºæ­¤ä¿¡å·æå‡ºçš„ Meerkat-vp èƒ½å¤Ÿè¯†åˆ«æç«¯å¼‚æ„å®¢æˆ·ç«¯å¹¶åº”ç”¨æ—©åœç­–ç•¥ï¼Œä»è€Œä¼˜åŒ–èšåˆæ¨¡å‹çš„è´¨é‡ã€‚å®éªŒè¯å®ï¼ŒMeerkat å’Œ Meerkat-vp æ˜¾è‘—æé«˜äº†é›¶é˜¶è”é‚¦ LLM å¾®è°ƒçš„æ•ˆç‡å’Œæœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "56 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.03337v1",
      "published_date": "2025-06-03 19:29:50 UTC",
      "updated_date": "2025-06-03 19:29:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:56:19.811802+00:00"
    },
    {
      "arxiv_id": "2506.03333v2",
      "title": "A Differential Perspective on Distributional Reinforcement Learning",
      "title_zh": "åˆ†å¸ƒå¼ºåŒ–å­¦ä¹ çš„å·®åˆ†è§†è§’",
      "authors": [
        "Juan Sebastian Rojas",
        "Chi-Guhn Lee"
      ],
      "abstract": "To date, distributional reinforcement learning (distributional RL) methods have exclusively focused on the discounted setting, where an agent aims to optimize a discounted sum of rewards over time. In this work, we extend distributional RL to the average-reward setting, where an agent aims to optimize the reward received per time step. In particular, we utilize a quantile-based approach to develop the first set of algorithms that can successfully learn and/or optimize the long-run per-step reward distribution, as well as the differential return distribution of an average-reward MDP. We derive proven-convergent tabular algorithms for both prediction and control, as well as a broader family of algorithms that have appealing scaling properties. Empirically, we find that these algorithms yield competitive and sometimes superior performance when compared to their non-distributional equivalents, while also capturing rich information about the long-run per-step reward and differential return distributions.",
      "tldr_zh": "è¯¥ç ”ç©¶å°†åˆ†å¸ƒå¼ºåŒ–å­¦ä¹ (Distributional RL)æ‰©å±•è‡³å¹³å‡å¥–åŠ±(Average-reward)è®¾ç½®ï¼Œå¼¥è¡¥äº†ä»¥å¾€æ–¹æ³•ä¸»è¦å…³æ³¨æŠ˜æ‰£å¥–åŠ±(Discounted reward)åœºæ™¯çš„ç©ºç™½ã€‚ä½œè€…åˆ©ç”¨åŸºäºåˆ†ä½æ•°(Quantile-based)çš„æ–¹æ³•ï¼Œå¼€å‘äº†é¦–å¥—èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ å¹¶ä¼˜åŒ–é•¿æœŸæ¯æ­¥å¥–åŠ±åˆ†å¸ƒåŠå¹³å‡å¥–åŠ±é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDP)ä¸­å¾®åˆ†å›æŠ¥åˆ†å¸ƒ(Differential return distribution)çš„ç®—æ³•ã€‚ç ”ç©¶ä¸ä»…æ¨å¯¼å‡ºäº†å…·æœ‰æ”¶æ•›ä¿è¯çš„è¡¨æ ¼ç®—æ³•ï¼Œè¿˜æå‡ºäº†ä¸€ç³»åˆ—å…·å¤‡è‰¯å¥½æ‰©å±•æ€§èƒ½çš„ç®—æ³•å®¶æ—ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥ç±»ç®—æ³•åœ¨æ€§èƒ½ä¸Šä¸éåˆ†å¸ƒå¼å¯¹åº”ç®—æ³•ç›¸æ¯”å…·æœ‰ç«äº‰æ€§ç”šè‡³æ›´ä¼˜çš„è¡¨ç°ï¼Œå¹¶èƒ½æ•è·å…³äºé•¿æœŸå¥–åŠ±åˆ†å¸ƒçš„ä¸°å¯Œä¿¡æ¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "In AAAI Conference on Artificial Intelligence 2026",
      "pdf_url": "https://arxiv.org/pdf/2506.03333v2",
      "published_date": "2025-06-03 19:26:25 UTC",
      "updated_date": "2026-01-13 13:43:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:56:12.608834+00:00"
    },
    {
      "arxiv_id": "2506.03332v1",
      "title": "Helpful Agent Meets Deceptive Judge: Understanding Vulnerabilities in Agentic Workflows",
      "title_zh": "åŠ©æ‰‹æ™ºèƒ½ä½“ä¸è¯¯å¯¼æ€§è¯„åˆ¤è€…ï¼šæ¢ç©¶æ™ºèƒ½ä½“å·¥ä½œæµä¸­çš„è„†å¼±æ€§",
      "authors": [
        "Yifei Ming",
        "Zixuan Ke",
        "Xuan-Phi Nguyen",
        "Jiayu Wang",
        "Shafiq Joty"
      ],
      "abstract": "Agentic workflows -- where multiple large language model (LLM) instances interact to solve tasks -- are increasingly built on feedback mechanisms, where one model evaluates and critiques another. Despite the promise of feedback-driven improvement, the stability of agentic workflows rests on the reliability of the judge. However, judges may hallucinate information, exhibit bias, or act adversarially -- introducing critical vulnerabilities into the workflow. In this work, we present a systematic analysis of agentic workflows under deceptive or misleading feedback. We introduce a two-dimensional framework for analyzing judge behavior, along axes of intent (from constructive to malicious) and knowledge (from parametric-only to retrieval-augmented systems). Using this taxonomy, we construct a suite of judge behaviors and develop WAFER-QA, a new benchmark with critiques grounded in retrieved web evidence to evaluate robustness of agentic workflows against factually supported adversarial feedback. We reveal that even strongest agents are vulnerable to persuasive yet flawed critiques -- often switching correct answers after a single round of misleading feedback. Taking a step further, we study how model predictions evolve over multiple rounds of interaction, revealing distinct behavioral patterns between reasoning and non-reasoning models. Our findings highlight fundamental vulnerabilities in feedback-based workflows and offer guidance for building more robust agentic systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æ™ºèƒ½ä½“å·¥ä½œæµ(Agentic workflows)ä¸­ï¼Œå¤šå¤§å‹è¯­è¨€æ¨¡å‹(LLM)å®ä¾‹åŸºäºåé¦ˆæœºåˆ¶è¿›è¡Œåä½œæ—¶ï¼Œç”±äºè¯„åˆ¤è€…(Judge)å¯èƒ½å­˜åœ¨çš„å¹»è§‰ã€åè§æˆ–å¯¹æŠ—æ€§è¡Œä¸ºæ‰€å¯¼è‡´çš„ç³»ç»Ÿè„†å¼±æ€§ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªä»æ„å›¾(Intent)å’ŒçŸ¥è¯†(Knowledge)ä¸¤ä¸ªç»´åº¦åˆ†æè¯„åˆ¤è€…è¡Œä¸ºçš„æ¡†æ¶ï¼Œå¹¶å¼€å‘äº†å…¨æ–°çš„åŸºå‡†æµ‹è¯•WAFER-QAï¼Œåˆ©ç”¨æ£€ç´¢åˆ°çš„ç½‘é¡µè¯æ®ç”Ÿæˆå…·æœ‰äº‹å®æ”¯æ’‘çš„å¯¹æŠ—æ€§åé¦ˆã€‚å®éªŒæ­ç¤ºäº†å³ä½¿æ˜¯æœ€å¼ºå¤§çš„æ™ºèƒ½ä½“åœ¨é¢å¯¹æå…·è¯´æœåŠ›ä½†å­˜åœ¨ç¼ºé™·çš„æ‰¹è¯„æ—¶ä¹Ÿè¡¨ç°å‡ºæå¤§çš„è„†å¼±æ€§ï¼Œå¾€å¾€ä»…ç»è¿‡ä¸€è½®è¯¯å¯¼æ€§åé¦ˆå°±ä¼šä¿®æ”¹åŸæœ¬æ­£ç¡®çš„ç­”æ¡ˆã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡åˆ†æå¤šè½®äº¤äº’ä¸­æ¨¡å‹çš„é¢„æµ‹æ¼”å˜ï¼Œæ­ç¤ºäº†æ¨ç†æ¨¡å‹(Reasoning models)ä¸éæ¨ç†æ¨¡å‹ä¹‹é—´æˆªç„¶ä¸åŒçš„è¡Œä¸ºæ¨¡å¼ã€‚è¯¥é¡¹å·¥ä½œä¸ä»…æ­ç¤ºäº†åŸºäºåé¦ˆçš„å·¥ä½œæµå­˜åœ¨çš„æ ¹æœ¬æ€§å®‰å…¨æ¼æ´ï¼Œä¹Ÿä¸ºæœªæ¥æ„å»ºæ›´å…·é²æ£’æ€§çš„æ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†é‡è¦æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03332v1",
      "published_date": "2025-06-03 19:26:23 UTC",
      "updated_date": "2025-06-03 19:26:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:56:17.834722+00:00"
    },
    {
      "arxiv_id": "2506.03320v1",
      "title": "The Future of Continual Learning in the Era of Foundation Models: Three Key Directions",
      "title_zh": "åŸºç¡€æ¨¡å‹æ—¶ä»£æŒç»­å­¦ä¹ çš„æœªæ¥ï¼šä¸‰å¤§å…³é”®æ–¹å‘",
      "authors": [
        "Jack Bell",
        "Luigi Quarantiello",
        "Eric Nuertey Coleman",
        "Lanpei Li",
        "Malio Li",
        "Mauro Madeddu",
        "Elia Piccoli",
        "Vincenzo Lomonaco"
      ],
      "abstract": "Continual learning--the ability to acquire, retain, and refine knowledge over time--has always been fundamental to intelligence, both human and artificial. Historically, different AI paradigms have acknowledged this need, albeit with varying priorities: early expert and production systems focused on incremental knowledge consolidation, while reinforcement learning emphasised dynamic adaptation. With the rise of deep learning, deep continual learning has primarily focused on learning robust and reusable representations over time to solve sequences of increasingly complex tasks. However, the emergence of Large Language Models (LLMs) and foundation models has raised the question: Do we still need continual learning when centralised, monolithic models can tackle diverse tasks with access to internet-scale knowledge? We argue that continual learning remains essential for three key reasons: (i) continual pre-training is still necessary to ensure foundation models remain up to date, mitigating knowledge staleness and distribution shifts while integrating new information; (ii) continual fine-tuning enables models to specialise and personalise, adapting to domain-specific tasks, user preferences, and real-world constraints without full retraining, avoiding the need for computationally expensive long context-windows; (iii) continual compositionality offers a scalable and modular approach to intelligence, enabling the orchestration of foundation models and agents to be dynamically composed, recombined, and adapted. While continual pre-training and fine-tuning are explored as niche research directions, we argue it is continual compositionality that will mark the rebirth of continual learning. The future of AI will not be defined by a single static model but by an ecosystem of continually evolving and interacting models, making continual learning more relevant than ever.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹(Large Language Models)å’ŒåŸºç¡€æ¨¡å‹(Foundation Models)æ—¶ä»£ï¼ŒæŒç»­å­¦ä¹ (Continual Learning)çš„æ ¸å¿ƒåœ°ä½ä¸æœªæ¥ä»·å€¼ã€‚ä½œè€…æå‡ºæŒç»­å­¦ä¹ ä¾ç„¶ä¸å¯æˆ–ç¼ºï¼Œå¹¶å®šä¹‰äº†ä¸‰ä¸ªå…³é”®çš„å‘å±•æ–¹å‘ã€‚é¦–å…ˆï¼ŒæŒç»­é¢„è®­ç»ƒ(Continual Pre-training)èƒ½ç¡®ä¿æ¨¡å‹å®æ—¶æ•´åˆæ–°ä¿¡æ¯å¹¶ç¼“è§£åˆ†å¸ƒåç§»(Distribution Shifts)å¯¼è‡´çš„çŸ¥è¯†è¿‡æ—¶ã€‚å…¶æ¬¡ï¼ŒæŒç»­å¾®è°ƒ(Continual Fine-tuning)æ”¯æŒæ¨¡å‹åœ¨ä¸è¿›è¡Œå…¨é‡é‡è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå®ç°ç‰¹å®šé¢†åŸŸä»»åŠ¡çš„ç‰¹åŒ–ä¸ç”¨æˆ·åå¥½çš„ä¸ªæ€§åŒ–é€‚é…ã€‚æœ€åï¼Œç ”ç©¶å¼ºè°ƒäº†æŒç»­ç»„åˆæ€§(Continual Compositionality)çš„é‡è¦æ€§ï¼Œé€šè¿‡åŠ¨æ€ç¼–æ’ä¸é‡ç»„å¤šä¸ªæ¨¡å‹åŠæ™ºèƒ½ä½“ï¼Œå®ç°å¯æ‰©å±•çš„æ¨¡å—åŒ–æ™ºèƒ½ã€‚æ–‡ç« æ€»ç»“è®¤ä¸ºï¼ŒæŒç»­ç»„åˆæ€§å°†æ ‡å¿—ç€æŒç»­å­¦ä¹ çš„é‡ç”Ÿï¼Œæœªæ¥çš„äººå·¥æ™ºèƒ½å°†æ˜¯ç”±ä¸æ–­æ¼”åŒ–ä¸”ç›¸äº’ä½œç”¨çš„æ¨¡å‹æ„æˆçš„ç”Ÿæ€ç³»ç»Ÿã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 1 figure, accepted at TCAI workshop 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.03320v1",
      "published_date": "2025-06-03 19:06:41 UTC",
      "updated_date": "2025-06-03 19:06:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:56:36.678711+00:00"
    },
    {
      "arxiv_id": "2506.03315v2",
      "title": "Axiomatics of Restricted Choices by Linear Orders of Sets with Minimum as Fallback",
      "title_zh": "ä»¥æœ€å°å€¼ä¸ºå›é€€é¡¹çš„é›†åˆçº¿æ€§åºå—é™é€‰æ‹©å…¬ç†åŒ–",
      "authors": [
        "Kai Sauerwald",
        "Kenneth Skiba",
        "Eduardo FermÃ©",
        "Thomas Meyer"
      ],
      "abstract": "We study how linear orders can be employed to realise choice functions for which the set of potential choices is restricted, i.e., the possible choice is not possible among the full powerset of all alternatives. In such restricted settings, constructing a choice function via a relation on the alternatives is not always possible. However, we show that one can always construct a choice function via a linear order on sets of alternatives, even when a fallback value is encoded as the minimal element in the linear order. The axiomatics of such choice functions are presented for the general case and the case of union-closed input restrictions. Restricted choice structures have applications in knowledge representation and reasoning, and here we discuss their applications for theory change and abstract argumentation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æ½œåœ¨é€‰æ‹©å—é™çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•åˆ©ç”¨çº¿æ€§åº(linear orders)æ¥å®ç°é€‰æ‹©å‡½æ•°(choice functions)ã€‚ä½œè€…æŒ‡å‡ºï¼Œåœ¨å—é™è®¾ç½®ä¸­é€šè¿‡å¤‡é€‰é¡¹ä¹‹é—´çš„å…³ç³»æ„å»ºé€‰æ‹©å‡½æ•°å¹¶éæ€»æ˜¯å¯è¡Œï¼Œä½†å¯ä»¥é€šè¿‡å¤‡é€‰é¡¹é›†åˆä¸Šçš„çº¿æ€§åºæ¥å®Œæˆæ„é€ ã€‚è¯¥æ–¹æ³•å…è®¸å°†å›é€€å€¼(fallback value)ç¼–ç ä¸ºçº¿æ€§åºä¸­çš„æå°å…ƒ(minimal element)ï¼Œä»è€Œç¡®ä¿é€‰æ‹©çš„å®ç°ã€‚è®ºæ–‡é’ˆå¯¹é€šç”¨æƒ…å½¢ä»¥åŠå¹¶é›†å°é—­(union-closed)çš„è¾“å…¥é™åˆ¶ï¼Œè¯¦ç»†é˜è¿°äº†æ­¤ç±»é€‰æ‹©å‡½æ•°çš„å…¬ç†åŒ–(axiomatics)ä½“ç³»ã€‚ç ”ç©¶å±•ç¤ºäº†è¿™äº›å—é™é€‰æ‹©ç»“æ„åœ¨çŸ¥è¯†è¡¨ç¤ºä¸æ¨ç†(knowledge representation and reasoning)ä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ç†è®ºå˜æ›´(theory change)å’ŒæŠ½è±¡è®ºè¾©(abstract argumentation)é¢†åŸŸæä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03315v2",
      "published_date": "2025-06-03 19:03:12 UTC",
      "updated_date": "2025-09-04 09:04:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:56:26.763807+00:00"
    },
    {
      "arxiv_id": "2506.03303v2",
      "title": "Hopscotch: Discovering and Skipping Redundancies in Language Models",
      "title_zh": "Hopscotchï¼šè¯­è¨€æ¨¡å‹ä¸­çš„å†—ä½™å‘ç°ä¸è·³è¿‡",
      "authors": [
        "Mustafa Eyceoz",
        "Nikhil Shivakumar Nayak",
        "Hao Wang",
        "Ligong Han",
        "Akash Srivastava"
      ],
      "abstract": "Modern causal language models stack many attention blocks to improve performance, but not all blocks are necessary for every task. We propose Hopscotch, a simple yet effective method that identifies and skips attention blocks with least contributions to a task and adapts to preserve output quality. Hopscotch jointly optimizes which blocks to skip and how to scale the outputs of the remaining layers. By introducing lightweight, trainable scaling parameters to attention and MLP blocks, it mitigates distribution shifts in hidden states caused by removing attention blocks. Hopscotch does not modify model weights or require access to pretraining or instruction-tuning data, and is compatible with existing model compression techniques. When applied to $\\texttt{Llama-3.1-8B}$ and $\\texttt{Qwen2.5-7B}$, Hopscotch achieves less than a 2% drop in performance even after skipping four attention blocks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Hopscotchï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è¯†åˆ«å¹¶è·³è¿‡è¯­è¨€æ¨¡å‹ä¸­å†—ä½™ Attention æ¨¡å—çš„æœ‰æ•ˆæ–¹æ³•ã€‚Hopscotch é€šè¿‡è”åˆä¼˜åŒ–æ¨¡å—è·³è¿‡ç­–ç•¥ä»¥åŠå‰©ä½™å±‚ï¼ˆremaining layersï¼‰è¾“å‡ºçš„ç¼©æ”¾æ¯”ä¾‹ï¼Œåœ¨å‡å°‘è®¡ç®—å¼€é”€çš„åŒæ—¶ä¿æŒè¾“å‡ºè´¨é‡ã€‚é€šè¿‡åœ¨ Attention å’Œ MLP æ¨¡å—ä¸­å¼•å…¥è½»é‡çº§ä¸”å¯è®­ç»ƒçš„ç¼©æ”¾å‚æ•°ï¼ˆscaling parametersï¼‰ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆç¼“è§£å› ç§»é™¤æ¨¡å—è€Œå¼•èµ·çš„éšè—çŠ¶æ€ï¼ˆhidden statesï¼‰åˆ†å¸ƒåç§»ã€‚Hopscotch çš„ä¼˜åŠ¿åœ¨äºæ— éœ€ä¿®æ”¹æ¨¡å‹åŸå§‹æƒé‡ï¼Œä¸”ä¸éœ€è¦è®¿é—®é¢„è®­ç»ƒæˆ–æŒ‡ä»¤å¾®è°ƒæ•°æ®ï¼ŒåŒæ—¶ä¸ç°æœ‰çš„æ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼ˆmodel compression techniquesï¼‰ä¿æŒè‰¯å¥½å…¼å®¹ã€‚åœ¨ Llama-3.1-8B å’Œ Qwen2.5-7B ä¸Šçš„å®éªŒéªŒè¯è¡¨æ˜ï¼Œå³ä½¿åœ¨è·³è¿‡å››ä¸ª Attention æ¨¡å—çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹çš„æ€§èƒ½ä¸‹é™ä¹Ÿæ§åˆ¶åœ¨ 2% ä»¥å†…ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 4 figures, 9 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.03303v2",
      "published_date": "2025-06-03 18:43:00 UTC",
      "updated_date": "2025-09-15 15:22:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:56:31.774205+00:00"
    },
    {
      "arxiv_id": "2506.03292v1",
      "title": "HyperSteer: Activation Steering at Scale with Hypernetworks",
      "title_zh": "HyperSteerï¼šåŸºäºè¶…ç½‘ç»œçš„è§„æ¨¡åŒ–æ¿€æ´»å¼•å¯¼",
      "authors": [
        "Jiuding Sun",
        "Sidharth Baskaran",
        "Zhengxuan Wu",
        "Michael Sklar",
        "Christopher Potts",
        "Atticus Geiger"
      ],
      "abstract": "Steering language models (LMs) by modifying internal activations is a popular approach for controlling text generation. Unsupervised dictionary learning methods, e.g., sparse autoencoders, can be scaled to produce many steering vectors, but lack guarantees on the individual efficacy of each vector and control over the coverage of relevant steering tasks. In contrast, supervised methods for constructing steering vectors are targeted and effective, but require more data collection and training for each additional steering vector produced. In this work, we introduce HyperSteer, a family of hypernetwork-based architectures which are trained end-to-end to generate steering vectors conditioned on the natural language steering prompts and the internals of the steered LM. In our evaluations, we show that scaling HyperSteer with thousands of steering prompts exceeds the performance of state-of-the-art activation steering methods, even on steering prompts never seen during training. Moreover, HyperSteer performs on par with steering-via-prompting.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº† HyperSteerï¼Œä¸€ç§åŸºäº Hypernetwork çš„æ¶æ„å®¶æ—ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ Activation Steering æ–¹æ³•åœ¨è§„æ¨¡åŒ–å’Œæœ‰æ•ˆæ€§ä¹‹é—´çš„å¹³è¡¡éš¾é¢˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ç«¯åˆ°ç«¯è®­ç»ƒæ¨¡å¼ï¼Œèƒ½å¤Ÿæ ¹æ®è‡ªç„¶è¯­è¨€ Steering Prompts å’Œè¢«æ§åˆ¶è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰çš„å†…éƒ¨çŠ¶æ€å®æ—¶ç”Ÿæˆè½¬å‘å‘é‡ã€‚ç›¸æ¯”äºç¼ºä¹ä¸ªä½“æœ‰æ•ˆæ€§ä¿è¯çš„æ— ç›‘ç£ Sparse Autoencoders æˆ–æ•°æ®å¯†é›†ä¸”éš¾ä»¥æ‰©å±•çš„ä¼ ç»Ÿç›‘ç£æ–¹æ³•ï¼ŒHyperSteer å®ç°äº†é’ˆå¯¹ç‰¹å®šæ§åˆ¶ä»»åŠ¡çš„é«˜æ•ˆè§„æ¨¡åŒ–ç”Ÿæˆã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œåœ¨æ•°åƒä¸ªè½¬å‘æç¤ºä¸Šæ‰©å±•çš„ HyperSteer æ€§èƒ½è¶…è¿‡äº†ç›®å‰æœ€å…ˆè¿›çš„æ¿€æ´»è½¬å‘æ–¹æ³•ï¼Œå³ä½¿é¢å¯¹è®­ç»ƒä¸­æœªè§è¿‡çš„æç¤ºä¹Ÿå…·æœ‰æå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒHyperSteer çš„è¡¨ç°ä¸ Steering-via-Prompting ç›¸å½“ï¼Œä¸ºå®ç°å¯æ‰©å±•ä¸”ç²¾å‡†çš„è¯­è¨€æ¨¡å‹è¡Œä¸ºæ§åˆ¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03292v1",
      "published_date": "2025-06-03 18:32:01 UTC",
      "updated_date": "2025-06-03 18:32:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:56:33.695871+00:00"
    },
    {
      "arxiv_id": "2506.03275v1",
      "title": "Chipmunk: Training-Free Acceleration of Diffusion Transformers with Dynamic Column-Sparse Deltas",
      "title_zh": "Chipmunkï¼šåˆ©ç”¨åŠ¨æ€åˆ—ç¨€ç–å¢é‡å®ç°æ‰©æ•£ Transformer çš„å…è®­ç»ƒåŠ é€Ÿ",
      "authors": [
        "Austin Silveria",
        "Soham V. Govande",
        "Daniel Y. Fu"
      ],
      "abstract": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in high-quality image and video generation but incur substantial compute cost at inference. A common observation is that DiT latent noise vectors change slowly across inference steps, which suggests that the DiT compute may be redundant across steps. In this paper, we aim to speed up inference by reducing this redundancy, without additional training. We first study how activations change between steps in two state-of-the-art open-source DiTs. We find that just 5-25% of the values in attention and MLP explain 70-90% of the change in activations across steps. This finding motivates our approach, Chipmunk, which uses dynamic sparsity at inference time to recompute only the fastest-changing intermediate activations, while caching the rest. Dynamic sparsity introduces two systems challenges: (1) sparse attention and MLP operations tend to underutilize GPU tensor cores; and (2) computing dynamic sparsity patterns at runtime and caching activations both introduce overhead. To address these challenges, Chipmunk first uses a voxel-based reordering of input tokens to introduce column-wise sparsity. We implement column-sparse kernels utilizing efficient sparse gathers from global to shared GPU memory, achieving a 9.3x speedup at 93% sparsity compared to highly-optimized dense baselines. Second, Chipmunk overlaps the computation of sparsity patterns and cache updates with other parts of the computation (e.g., second layer of the MLP) to hide the extra latency. Chipmunk achieves up to 2.16x speedup on HunyuanVideo and 1.41x on FLUX.1-dev without compromising generation quality. Furthermore, we show that Chipmunk can be stacked on top of full step caching, achieving a 3.72x speedup on HunyuanVideo, a 2.67x speedup on WAN2.1, and a 2.25x speedup on FLUX.1-dev with minimal quality impact.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Chipmunkï¼Œä¸€ç§æ— éœ€è®­ç»ƒå³å¯å®ç°Diffusion Transformers (DiTs) æ¨ç†åŠ é€Ÿçš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å‡å°‘æ¨ç†æ­¥éª¤é—´çš„è®¡ç®—å†—ä½™æ¥æå‡æ•ˆç‡ã€‚ç ”ç©¶å‘ç°ï¼ŒAttentionå’ŒMLPä¸­ä»…5-25%çš„å€¼å°±è§£é‡Šäº†70-90%çš„æ¿€æ´»å˜åŒ–ï¼Œæ®æ­¤Chipmunkåˆ©ç”¨åŠ¨æ€ç¨€ç–æ€§(dynamic sparsity)ä»…é‡æ–°è®¡ç®—å˜åŒ–æœ€å¿«çš„ä¸­ä½æ¿€æ´»å¹¶ç¼“å­˜å…¶ä½™éƒ¨åˆ†ã€‚ä¸ºè§£å†³ç¨€ç–æ“ä½œåœ¨GPUåˆ©ç”¨ç‡åŠè¿è¡Œå¼€é”€ä¸Šçš„æŒ‘æˆ˜ï¼ŒChipmunké‡‡ç”¨äº†åŸºäºä½“ç´ (voxel-based)çš„è¾“å…¥ä»¤ç‰Œé‡æ’ä»¥å¼•å…¥åˆ—ç¨€ç–(column-wise sparsity)ï¼Œå¹¶é€šè¿‡è®¡ç®—é‡å (overlapping)æŠ€æœ¯éšè—äº†ç¨€ç–æ¨¡å¼è®¡ç®—ä¸ç¼“å­˜æ›´æ–°çš„å»¶è¿Ÿã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒChipmunkåœ¨HunyuanVideoå’ŒFLUX.1-devä¸Šåˆ†åˆ«å®ç°äº†é«˜è¾¾2.16å€å’Œ1.41å€çš„åŠ é€Ÿä¸”ä¸æŸå¤±ç”Ÿæˆè´¨é‡ã€‚æ­¤å¤–ï¼ŒChipmunkå¯ä¸å…¨æ­¥éª¤ç¼“å­˜(full step caching)æŠ€æœ¯å åŠ ä½¿ç”¨ï¼Œåœ¨HunyuanVideoä¸Šå®ç°æœ€é«˜3.72å€çš„åŠ é€Ÿï¼Œä¸ºé«˜æ•ˆçš„å¤§è§„æ¨¡ç”Ÿæˆæ¨¡å‹æ¨ç†æä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.03275v1",
      "published_date": "2025-06-03 18:03:32 UTC",
      "updated_date": "2025-06-03 18:03:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:56:55.036319+00:00"
    },
    {
      "arxiv_id": "2506.03270v2",
      "title": "Grounded Vision-Language Interpreter for Integrated Task and Motion Planning",
      "title_zh": "é¢å‘ä»»åŠ¡ä¸è¿åŠ¨é›†æˆè§„åˆ’çš„è½åœ°å¼è§†è§‰-è¯­è¨€è§£é‡Šå™¨",
      "authors": [
        "Jeremy Siburian",
        "Keisuke Shirai",
        "Cristian C. Beltran-Hernandez",
        "Masashi Hamaya",
        "Michael GÃ¶rner",
        "Atsushi Hashimoto"
      ],
      "abstract": "While recent advances in vision-language models have accelerated the development of language-guided robot planners, their black-box nature often lacks safety guarantees and interpretability crucial for real-world deployment. Conversely, classical symbolic planners offer rigorous safety verification but require significant expert knowledge for setup. To bridge the current gap, this paper proposes ViLaIn-TAMP, a hybrid planning framework for enabling verifiable, interpretable, and autonomous robot behaviors. ViLaIn-TAMP comprises three main components: (1) a Vision-Language Interpreter (ViLaIn) adapted from previous work that converts multimodal inputs into structured problem specifications, (2) a modular Task and Motion Planning (TAMP) system that grounds these specifications in actionable trajectory sequences through symbolic and geometric constraint reasoning, and (3) a corrective planning (CP) module which receives concrete feedback on failed solution attempts and feed them with constraints back to ViLaIn to refine the specification. We design challenging manipulation tasks in a cooking domain and evaluate our framework. Experimental results demonstrate that ViLaIn-TAMP outperforms a VLM-as-a-planner baseline by 18% in mean success rate, and that adding the CP module boosts mean success rate by 32%.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ViLaIn-TAMPï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº† Vision-Language Models (VLMs) ä¸ä¼ ç»Ÿç¬¦å·è§„åˆ’ä¼˜åŠ¿çš„æ··åˆè§„åˆ’æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å¯éªŒè¯ã€å¯è§£é‡Šä¸”è‡ªä¸»çš„æœºå™¨äººè¡Œä¸ºã€‚è¯¥ç³»ç»Ÿç”± Vision-Language Interpreter (ViLaIn)ã€Task and Motion Planning (TAMP) ç³»ç»Ÿå’Œçº æ­£æ€§è§„åˆ’ (Corrective Planning, CP) æ¨¡å—ç»„æˆï¼Œèƒ½å¤Ÿå°†å¤šæ¨¡æ€è¾“å…¥è½¬åŒ–ä¸ºç»“æ„åŒ–çš„è¡ŒåŠ¨è½¨è¿¹ã€‚ViLaIn è´Ÿè´£å°†æŠ½è±¡æŒ‡ä»¤è½¬åŒ–ä¸ºé—®é¢˜è§„èŒƒï¼Œè€Œ TAMP ç³»ç»Ÿåˆ™é€šè¿‡ç¬¦å·ä¸å‡ ä½•çº¦æŸæ¨ç†å°†è¿™äº›è§„èŒƒæ˜ å°„ä¸ºå…·ä½“çš„åŠ¨ä½œåºåˆ—ã€‚ç‰¹åˆ«è®¾è®¡çš„ CP æ¨¡å—èƒ½å¤Ÿæ¥æ”¶æ‰§è¡Œå¤±è´¥çš„åé¦ˆå¹¶æ®æ­¤ç²¾ç»†åŒ–è§„èŒƒï¼Œä»è€Œæé«˜è§„åˆ’çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒViLaIn-TAMP åœ¨å¨æˆ¿æ“çºµä»»åŠ¡ä¸­çš„å¹³å‡æˆåŠŸç‡æ¯”çº¯ VLM åŸºçº¿æ¨¡å‹é«˜å‡º 18%ã€‚å¼•å…¥ CP æ¨¡å—åï¼Œç³»ç»ŸæˆåŠŸç‡è¿›ä¸€æ­¥æå‡äº† 32%ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶åœ¨å¤„ç†å¤æ‚çº¦æŸä»»åŠ¡ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Project website: https://omron-sinicx.github.io/ViLaIn-TAMP/",
      "pdf_url": "https://arxiv.org/pdf/2506.03270v2",
      "published_date": "2025-06-03 18:00:32 UTC",
      "updated_date": "2025-11-04 06:01:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:56:53.933078+00:00"
    },
    {
      "arxiv_id": "2506.03150v1",
      "title": "IllumiCraft: Unified Geometry and Illumination Diffusion for Controllable Video Generation",
      "title_zh": "IllumiCraftï¼šé¢å‘å¯æ§è§†é¢‘ç”Ÿæˆçš„å‡ ä½•ä¸å…‰ç…§ç»Ÿä¸€æ‰©æ•£æ¨¡å‹",
      "authors": [
        "Yuanze Lin",
        "Yi-Wen Chen",
        "Yi-Hsuan Tsai",
        "Ronald Clark",
        "Ming-Hsuan Yang"
      ],
      "abstract": "Although diffusion-based models can generate high-quality and high-resolution video sequences from textual or image inputs, they lack explicit integration of geometric cues when controlling scene lighting and visual appearance across frames. To address this limitation, we propose IllumiCraft, an end-to-end diffusion framework accepting three complementary inputs: (1) high-dynamic-range (HDR) video maps for detailed lighting control; (2) synthetically relit frames with randomized illumination changes (optionally paired with a static background reference image) to provide appearance cues; and (3) 3D point tracks that capture precise 3D geometry information. By integrating the lighting, appearance, and geometry cues within a unified diffusion architecture, IllumiCraft generates temporally coherent videos aligned with user-defined prompts. It supports background-conditioned and text-conditioned video relighting and provides better fidelity than existing controllable video generation methods. Project Page: https://yuanze-lin.me/IllumiCraft_page",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† IllumiCraftï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ‰©æ•£ (diffusion) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨æ§åˆ¶è§†é¢‘å…‰ç…§å’Œå¤–è§‚æ—¶ç¼ºä¹æ˜¾å¼å‡ ä½•çº¿ç´¢æ•´åˆçš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»Ÿä¸€çš„æ¶æ„èåˆäº†ä¸‰ç§äº’è¡¥è¾“å…¥ï¼šæä¾›è¯¦ç»†å…‰ç…§æ§åˆ¶çš„é«˜åŠ¨æ€èŒƒå›´ (HDR) è§†é¢‘åœ°å›¾ã€æä¾›å¤–è§‚çº¿ç´¢çš„éšæœºå…‰ç…§åˆæˆå¸§ï¼Œä»¥åŠæ•æ‰ç²¾ç¡® 3D å‡ ä½•ä¿¡æ¯çš„ 3D point tracksã€‚IllumiCraft ç¡®ä¿ç”Ÿæˆçš„è§†é¢‘åœ¨æ—¶é—´è¿ç»­æ€§ (temporally coherent) ä¸Šä¸ç”¨æˆ·å®šä¹‰çš„ prompt ä¿æŒä¸€è‡´ã€‚å®ƒä¸ä»…æ”¯æŒèƒŒæ™¯æ¡ä»¶å’Œæ–‡æœ¬æ¡ä»¶çš„è§†é¢‘é‡æ‰“å…‰ (video relighting)ï¼Œåœ¨ç”Ÿæˆä¿çœŸåº¦å’Œå¯æ§æ€§æ–¹é¢ä¹Ÿæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¯æ§è§†é¢‘ç”Ÿæˆæ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Tech Report",
      "pdf_url": "https://arxiv.org/pdf/2506.03150v1",
      "published_date": "2025-06-03 17:59:52 UTC",
      "updated_date": "2025-06-03 17:59:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:57:01.653511+00:00"
    },
    {
      "arxiv_id": "2506.03149v1",
      "title": "Causal Estimation of Tokenisation Bias",
      "title_zh": "åˆ†è¯åç½®çš„å› æœä¼°è®¡",
      "authors": [
        "Pietro Lesci",
        "Clara Meister",
        "Thomas Hofmann",
        "Andreas Vlachos",
        "Tiago Pimentel"
      ],
      "abstract": "Modern language models are typically trained over subword sequences, but ultimately define probabilities over character-strings. Ideally, the choice of the tokeniser -- which maps character-strings to subwords -- should not affect the probability assigned to the underlying character-string; in practice, it does. We define this mismatch as tokenisation bias. In this work, we quantify one particular type of tokenisation bias: the effect of including or not a subword (e.g., $\\langle hello \\rangle$) in a tokeniser's vocabulary on the probability a trained model assigns to the corresponding characters (i.e., \\textit{``hello''}). Estimating this effect is challenging because each model is trained with only one tokeniser. We address this by framing tokenisation bias as a causal effect and estimating it using the regression discontinuity design. Specifically, we exploit the fact that tokenisation algorithms rank subwords and add the first $K$ to a tokeniser's vocabulary, where $K$ is an arbitrary cutoff point. As such, we can estimate a causal effect by comparing similar subwords around this cutoff. Experimentally, we find that tokenisation consistently affects models' outputs across scales, vocabularies, and tokenisers. Notably, a subword's presence in a small model's vocabulary may increase its characters' probability by up to 17 times, highlighting tokenisation as a key design choice in language modelling.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç°ä»£è¯­è¨€æ¨¡å‹ä¸­ Tokenisation è¿‡ç¨‹å¯¹å­—ç¬¦å­—ç¬¦ä¸²æ¦‚ç‡åˆ†å¸ƒçš„å½±å“ï¼Œå¹¶å°†å…¶å®šä¹‰ä¸º Tokenisation Biasã€‚é’ˆå¯¹é‡åŒ–ä¼°è®¡ä¸­æ¯ä¸ªæ¨¡å‹é€šå¸¸ä»…å¯¹åº”å•ä¸€åˆ†è¯å™¨çš„æŒ‘æˆ˜ï¼Œç ”ç©¶è€…å°†è¯¥åå·®æ¡†æ¶åŒ–ä¸ºä¸€ç§å› æœæ•ˆåº” (Causal Effect)ï¼Œå¹¶åˆ›æ–°æ€§åœ°é‡‡ç”¨æ–­ç‚¹å›å½’è®¾è®¡ (Regression Discontinuity Design) è¿›è¡Œé‡åŒ–ä¼°è®¡ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹æ¯”è¯è¡¨æˆªæ–­ç‚¹ $K$ é™„è¿‘ç›¸ä¼¼å­è¯çš„ç”Ÿæˆæ¦‚ç‡ï¼Œæ­ç¤ºäº† Tokenisation åœ¨ä¸åŒæ¨¡å‹è§„æ¨¡å’Œè¯è¡¨è®¾ç½®ä¸‹çš„ä¸€è‡´æ€§å½±å“ã€‚å®éªŒå‘ç°ï¼Œå­è¯ (Subword) æ˜¯å¦å­˜åœ¨äºè¯è¡¨ä¸­ï¼Œå¯èƒ½å¯¼è‡´å…¶å¯¹åº”å­—ç¬¦åºåˆ—çš„ç”Ÿæˆæ¦‚ç‡äº§ç”Ÿé«˜è¾¾ 17 å€çš„å·®å¼‚ã€‚è¿™ä¸€ç»“è®ºè¡¨æ˜ Tokenisation å¹¶éä¸­æ€§è¿‡ç¨‹ï¼Œè€Œæ˜¯è¯­è¨€å»ºæ¨¡ä¸­ä¸å¯å¿½è§†çš„å…³é”®è®¾è®¡å†³ç­–ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Published as a conference paper at ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.03149v1",
      "published_date": "2025-06-03 17:59:47 UTC",
      "updated_date": "2025-06-03 17:59:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:57:24.021592+00:00"
    },
    {
      "arxiv_id": "2506.03147v4",
      "title": "UniWorld-V1: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation",
      "title_zh": "UniWorld-V1ï¼šé¢å‘ç»Ÿä¸€è§†è§‰ç†è§£ä¸ç”Ÿæˆçš„é«˜åˆ†è¾¨ç‡è¯­ä¹‰ç¼–ç å™¨",
      "authors": [
        "Bin Lin",
        "Zongjian Li",
        "Xinhua Cheng",
        "Yuwei Niu",
        "Yang Ye",
        "Xianyi He",
        "Shenghai Yuan",
        "Wangbo Yu",
        "Shaodong Wang",
        "Yunyang Ge",
        "Yatian Pang",
        "Li Yuan"
      ],
      "abstract": "Although existing unified models achieve strong performance in vision-language understanding and text-to-image generation, they remain limited in addressing image perception and manipulation -- capabilities increasingly demanded in practical applications. Recently, OpenAI introduced the powerful GPT-4o-Image model, which showcases advanced capabilities in comprehensive image perception and manipulation, sparking widespread interest. Through carefully designed experiments, we observe that GPT-4o-Image likely relies on semantic encoders rather than VAEs for feature extraction, despite VAEs being commonly regarded as crucial for image manipulation tasks. Inspired by this insight, we propose UniWorld-V1, a unified generative framework built upon semantic features extracted from powerful multimodal large language models and contrastive semantic encoders. Using only 2.7M training data, UniWorld-V1 achieves impressive performance across diverse tasks, including image understanding, generation, manipulation, and perception. We fully open-source the UniWorld-V1 framework, including model weights, training and evaluation scripts, and datasets to promote reproducibility and further research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰ç»Ÿä¸€æ¨¡å‹åœ¨å›¾åƒæ„ŸçŸ¥(perception)å’Œæ“ä½œ(manipulation)æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†UniWorld-V1ç»Ÿä¸€ç”Ÿæˆæ¡†æ¶ã€‚é€šè¿‡å¯¹GPT-4o-Imageçš„å®éªŒè§‚å¯Ÿï¼Œç ”ç©¶è€…å‘ç°è¯­ä¹‰ç¼–ç å™¨(semantic encoders)åœ¨å›¾åƒç‰¹å¾æå–æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹¶ä»¥æ­¤å–ä»£äº†ä¼ ç»Ÿçš„VAEã€‚UniWorld-V1åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)å’Œå¯¹æ¯”è¯­ä¹‰ç¼–ç å™¨æå–çš„è¯­ä¹‰ç‰¹å¾æ„å»ºè€Œæˆï¼Œä»…éœ€2.7Mè®­ç»ƒæ•°æ®å³å¯åœ¨å›¾åƒç†è§£ã€ç”Ÿæˆã€æ“ä½œå’Œæ„ŸçŸ¥ç­‰ä»»åŠ¡ä¸­å±•ç°å‡ºè‰²æ€§èƒ½ã€‚è¯¥æ¡†æ¶å·²å…¨é¢å¼€æºæ¨¡å‹æƒé‡ã€è®­ç»ƒè„šæœ¬åŠæ•°æ®é›†ï¼Œä¸ºç»Ÿä¸€è§†è§‰ç†è§£ä¸ç”Ÿæˆé¢†åŸŸæä¾›äº†é«˜æ€§èƒ½ä¸”å¯å¤ç°çš„ç ”ç©¶åŸºå‡†ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03147v4",
      "published_date": "2025-06-03 17:59:33 UTC",
      "updated_date": "2025-06-18 18:00:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:57:17.164106+00:00"
    },
    {
      "arxiv_id": "2506.03145v2",
      "title": "Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and Semantic Understanding Capability of LLM",
      "title_zh": "ç»“åˆæœ¬ä½“ä¸å¤§è¯­è¨€æ¨¡å‹è¯­ä¹‰ç†è§£èƒ½åŠ›çš„å®ä½“å¢å¼ºå‹ç¥ç»ç§‘å­¦çŸ¥è¯†æ£€ç´¢",
      "authors": [
        "Pralaypati Ta",
        "Sriram Venkatesaperumal",
        "Keerthi Ram",
        "Mohanasankar Sivaprakasam"
      ],
      "abstract": "Neuroscience research publications encompass a vast wealth of knowledge. Accurately retrieving existing information and discovering new insights from this extensive literature is essential for advancing the field. However, when knowledge is dispersed across multiple sources, current state-of-the-art retrieval methods often struggle to extract the necessary information. A knowledge graph (KG) can integrate and link knowledge from multiple sources. However, existing methods for constructing KGs in neuroscience often rely on labeled data and require domain expertise. Acquiring large-scale, labeled data for a specialized area like neuroscience presents significant challenges. This work proposes novel methods for constructing KG from unlabeled large-scale neuroscience research corpus utilizing large language models (LLM), neuroscience ontology, and text embeddings. We analyze the semantic relevance of neuroscience text segments identified by LLM for building the knowledge graph. We also introduce an entity-augmented information retrieval algorithm to extract knowledge from the KG. Several experiments were conducted to evaluate the proposed approaches. The results demonstrate that our methods significantly enhance knowledge discovery from the unlabeled neuroscience research corpus. The performance of the proposed entity and relation extraction method is comparable to the existing supervised method. It achieves an F1 score of 0.84 for entity extraction from the unlabeled data. The knowledge obtained from the KG improves answers to over 52% of neuroscience questions from the PubMedQA dataset and questions generated using selected neuroscience entities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¥ç»ç§‘å­¦é¢†åŸŸæ–‡çŒ®çŸ¥è¯†åˆ†æ•£ä¸”çŸ¥è¯†å›¾è°±(KG)æ„å»ºé«˜åº¦ä¾èµ–æ ‡æ³¨æ•°æ®ä¸é¢†åŸŸä¸“å®¶çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)ã€ç¥ç»ç§‘å­¦æœ¬ä½“(Ontology)å’Œæ–‡æœ¬åµŒå…¥(text embeddings)ä»å¤§è§„æ¨¡æ— æ ‡æ³¨è¯­æ–™ä¸­æ„å»ºçŸ¥è¯†å›¾è°±çš„æ–°æ–¹æ³•ã€‚é€šè¿‡åˆ†æLLMè¯†åˆ«çš„æ–‡æœ¬æ®µè½è¯­ä¹‰å…³è”æ€§ï¼Œç ”ç©¶æ„å»ºäº†ç»“æ„åŒ–çš„çŸ¥è¯†å…³è”ï¼Œå¹¶å¼•å…¥äº†å®ä½“å¢å¼ºä¿¡æ¯æ£€ç´¢ç®—æ³•(entity-augmented information retrieval algorithm)ä»¥ä»ä¸­æå–çŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ— æ ‡æ³¨æ•°æ®ä¸Šçš„å®ä½“æå–F1åˆ†æ•°è¾¾åˆ°0.84ï¼Œæ€§èƒ½è¶³ä»¥åª²ç¾ç°æœ‰çš„ç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨è¯¥å›¾è°±è·å–çš„çŸ¥è¯†å°†PubMedQAåŠç‰¹å®šå®ä½“é—®é¢˜çš„å›ç­”å‡†ç¡®ç‡æå‡äº†è¶…è¿‡52%ã€‚è¯¥é¡¹å·¥ä½œæ˜¾è‘—å¢å¼ºäº†ä»éç»“æ„åŒ–ç¥ç»ç§‘å­¦æ–‡çŒ®ä¸­è‡ªåŠ¨å‘ç°ä¸æ£€ç´¢çŸ¥è¯†çš„èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03145v2",
      "published_date": "2025-06-03 17:59:18 UTC",
      "updated_date": "2025-10-26 13:07:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:57:09.041987+00:00"
    },
    {
      "arxiv_id": "2506.03143v1",
      "title": "GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents",
      "title_zh": "GUI-Actorï¼šé¢å‘ GUI æ™ºèƒ½ä½“çš„æ— åæ ‡è§†è§‰å®šä½",
      "authors": [
        "Qianhui Wu",
        "Kanzhi Cheng",
        "Rui Yang",
        "Chaoyun Zhang",
        "Jianwei Yang",
        "Huiqiang Jiang",
        "Jian Mu",
        "Baolin Peng",
        "Bo Qiao",
        "Reuben Tan",
        "Si Qin",
        "Lars Liden",
        "Qingwei Lin",
        "Huan Zhang",
        "Tong Zhang",
        "Jianbing Zhang",
        "Dongmei Zhang",
        "Jianfeng Gao"
      ],
      "abstract": "One of the principal challenges in building VLM-powered GUI agents is visual grounding, i.e., localizing the appropriate screen region for action execution based on both the visual content and the textual plans. Most existing work formulates this as a text-based coordinate generation task. However, these approaches suffer from several limitations: weak spatial-semantic alignment, inability to handle ambiguous supervision targets, and a mismatch between the dense nature of screen coordinates and the coarse, patch-level granularity of visual features extracted by models like Vision Transformers. In this paper, we propose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its core, GUI-Actor introduces an attention-based action head that learns to align a dedicated <ACTOR> token with all relevant visual patch tokens, enabling the model to propose one or more action regions in a single forward pass. In line with this, we further design a grounding verifier to evaluate and select the most plausible action region from the candidates proposed for action execution. Extensive experiments show that GUI-Actor outperforms prior state-of-the-art methods on multiple GUI action grounding benchmarks, with improved generalization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B even surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7 with Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by incorporating the verifier, we find that fine-tuning only the newly introduced action head (~100M parameters for 7B model) while keeping the VLM backbone frozen is sufficient to achieve performance comparable to previous state-of-the-art models, highlighting that GUI-Actor can endow the underlying VLM with effective grounding capabilities without compromising its general-purpose strengths.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GUI-Actorï¼Œä¸€ç§é¢å‘GUIæ™ºèƒ½ä½“çš„æ— éœ€åæ ‡çš„è§†è§‰å®šä½æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸåŸºäºæ–‡æœ¬åæ ‡ç”Ÿæˆä»»åŠ¡ä¸­ç©ºé—´è¯­ä¹‰å¯¹é½å¼±åŠè§†è§‰ç‰¹å¾ç²’åº¦ä¸åŒ¹é…ç­‰æŒ‘æˆ˜ã€‚GUI-Actorçš„æ ¸å¿ƒåœ¨äºå¼•å…¥äº†åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„åŠ¨ä½œå¤´(Action Head)ï¼Œé€šè¿‡å°†ä¸“ç”¨çš„<ACTOR>æ ‡è®°ä¸ç›¸å…³è§†è§‰è¡¥ä¸(Visual Patch)å¯¹é½ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å•æ¬¡å‰å‘ä¼ æ’­ä¸­é¢„æµ‹å¤šä¸ªå€™é€‰åŠ¨ä½œåŒºåŸŸï¼Œå¹¶é…åˆå®šä½éªŒè¯å™¨(Grounding Verifier)ç­›é€‰å‡ºæœ€åˆç†çš„æ‰§è¡Œä½ç½®ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æœ€å…ˆè¿›(SOTA)æ¨¡å‹ï¼Œå¯¹æœªçŸ¥å±å¹•åˆ†è¾¨ç‡å’Œå¸ƒå±€å±•ç°å‡ºæå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨ScreenSpot-ProåŸºå‡†æµ‹è¯•ä¸­ï¼ŒåŸºäºQwen2-VLå’ŒQwen2.5-VLçš„GUI-Actor-7Bæ€§èƒ½ç”šè‡³è¶…è¶Šäº†å‚æ•°é‡æ›´å¤§çš„UI-TARS-72Bã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¯æ˜ä»…é€šè¿‡å¾®è°ƒçº¦100Må‚æ•°çš„åŠ¨ä½œå¤´è€Œå†»ç»“VLMéª¨å¹²ç½‘ç»œï¼Œå³å¯åœ¨ä¿ç•™æ¨¡å‹é€šç”¨èƒ½åŠ›çš„åŒæ—¶å®ç°å“è¶Šçš„å®šä½æ€§èƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03143v1",
      "published_date": "2025-06-03 17:59:08 UTC",
      "updated_date": "2025-06-03 17:59:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:57:20.924816+00:00"
    },
    {
      "arxiv_id": "2506.03139v1",
      "title": "SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation",
      "title_zh": "SVGeniusï¼šå¤§è¯­è¨€æ¨¡å‹åœ¨ SVG ç†è§£ã€ç¼–è¾‘ä¸ç”Ÿæˆä¸­çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Siqi Chen",
        "Xinyu Dong",
        "Haolei Xu",
        "Xingyu Wu",
        "Fei Tang",
        "Hang Zhang",
        "Yuchen Yan",
        "Linjuan Wu",
        "Wenqi Zhang",
        "Guiyang Hou",
        "Yongliang Shen",
        "Weiming Lu",
        "Yueting Zhuang"
      ],
      "abstract": "Large Language Models (LLMs) and Multimodal LLMs have shown promising capabilities for SVG processing, yet existing benchmarks suffer from limited real-world coverage, lack of complexity stratification, and fragmented evaluation paradigms. We introduce SVGenius, a comprehensive benchmark comprising 2,377 queries across three progressive dimensions: understanding, editing, and generation. Built on real-world data from 24 application domains with systematic complexity stratification, SVGenius evaluates models through 8 task categories and 18 metrics. We assess 22 mainstream models spanning different scales, architectures, training paradigms, and accessibility levels. Our analysis reveals that while proprietary models significantly outperform open-source counterparts, all models exhibit systematic performance degradation with increasing complexity, indicating fundamental limitations in current approaches; however, reasoning-enhanced training proves more effective than pure scaling for overcoming these limitations, though style transfer remains the most challenging capability across all model types. SVGenius establishes the first systematic evaluation framework for SVG processing, providing crucial insights for developing more capable vector graphics models and advancing automated graphic design applications. Appendix and supplementary materials (including all data and code) are available at https://zju-real.github.io/SVGenius.",
      "tldr_zh": "æœ¬ç ”ç©¶ä»‹ç»äº†SVGeniusï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«2,377ä¸ªæŸ¥è¯¢çš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMså’ŒMultimodal LLMsåœ¨SVGç†è§£ã€ç¼–è¾‘å’Œç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŸºäºæ¥è‡ª24ä¸ªåº”ç”¨é¢†åŸŸçš„çœŸå®æ•°æ®ï¼Œé€šè¿‡ç³»ç»ŸåŒ–çš„å¤æ‚åº¦åˆ†å±‚ã€8ä¸ªä»»åŠ¡ç±»åˆ«å’Œ18é¡¹æŒ‡æ ‡ï¼Œå¯¹22ä¸ªæ¶µç›–ä¸åŒæ¶æ„å’Œè§„æ¨¡çš„ä¸»æµæ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚å®éªŒåˆ†æè¡¨æ˜ï¼Œè™½ç„¶é—­æºæ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¼˜äºå¼€æºæ¨¡å‹ï¼Œä½†æ‰€æœ‰æ¨¡å‹åœ¨ä»»åŠ¡å¤æ‚åº¦å¢åŠ æ—¶å‡è¡¨ç°å‡ºæ€§èƒ½é€€åŒ–ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨ç†å¢å¼ºè®­ç»ƒ(reasoning-enhanced training)åœ¨å…‹æœè¿™äº›é™åˆ¶æ–¹é¢æ¯”å•çº¯çš„è§„æ¨¡æ‰©å±•(scaling)æ›´ä¸ºæœ‰æ•ˆï¼Œè€Œé£æ ¼è¿ç§»(style transfer)ä»æ˜¯ç›®å‰æœ€å…·æŒ‘æˆ˜æ€§çš„èƒ½åŠ›ã€‚ä½œä¸ºé¦–ä¸ªç³»ç»ŸåŒ–çš„SVGå¤„ç†è¯„ä¼°æ¡†æ¶ï¼ŒSVGeniusä¸ºå¼€å‘æ›´å…ˆè¿›çš„çŸ¢é‡å›¾å½¢æ¨¡å‹å’Œè‡ªåŠ¨åŒ–å›¾å½¢è®¾è®¡åº”ç”¨æä¾›äº†å…³é”®è§è§£ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "19 pages,4 figures, Project page: https://zju-real.github.io/SVGenius, Code: https://github.com/ZJU-REAL/SVGenius-Bench",
      "pdf_url": "https://arxiv.org/pdf/2506.03139v1",
      "published_date": "2025-06-03 17:58:57 UTC",
      "updated_date": "2025-06-03 17:58:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:57:48.982126+00:00"
    },
    {
      "arxiv_id": "2506.03135v2",
      "title": "OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models",
      "title_zh": "OmniSpatialï¼šé¢å‘è§†è§‰è¯­è¨€æ¨¡å‹çš„å…¨é¢ç©ºé—´æ¨ç†åŸºå‡†",
      "authors": [
        "Mengdi Jia",
        "Zekun Qi",
        "Shaochen Zhang",
        "Wenyao Zhang",
        "Xinqiang Yu",
        "Jiawei He",
        "He Wang",
        "Li Yi"
      ],
      "abstract": "Spatial reasoning is a key aspect of cognitive psychology and remains a bottleneck for current vision-language models (VLMs). While extensive research has aimed to evaluate or improve VLMs' understanding of basic spatial relations, such as distinguishing left from right, near from far, and object counting, these tasks cover only the most elementary layer of spatial reasoning and are largely approaching saturation in the latest reasoning models. In this work, we introduce OmniSpatial, a comprehensive and challenging benchmark for spatial reasoning, grounded in cognitive psychology. OmniSpatial covers four major categories: dynamic reasoning, complex spatial logic, spatial interaction, and perspective-taking, with 50 fine-grained subcategories. Through careful manual annotation, we construct over 8.4K question-answer pairs. Extensive experiments show that both open- and closed-source VLMs exhibit significant limitations in comprehensive spatial reasoning. We also explore two strategies-PointGraph (explicit scene graph cues) and SpatialCoT (novel-view chain-of-thought)-to bolster spatial reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨ç©ºé—´æ¨ç†æ–¹é¢çš„ç“¶é¢ˆï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºè®¤çŸ¥å¿ƒç†å­¦çš„å…¨é¢åŸºå‡†æµ‹è¯•OmniSpatialã€‚è¯¥åŸºå‡†æ¶µç›–äº†åŠ¨æ€æ¨ç†(dynamic reasoning)ã€å¤æ‚ç©ºé—´é€»è¾‘(complex spatial logic)ã€ç©ºé—´äº¤äº’(spatial interaction)å’Œé€è§†è·å–(perspective-taking)å››å¤§ç±»ï¼Œå…±åŒ…å«50ä¸ªç»†åˆ†ç»´åº¦ã€‚é€šè¿‡äººå·¥æ ‡æ³¨ï¼Œç ”ç©¶è€…æ„å»ºäº†åŒ…å«è¶…è¿‡8.4Kä¸ªé—®ç­”å¯¹çš„æ•°æ®é›†ï¼Œå®éªŒç»“æœæ˜¾ç¤ºç°æœ‰çš„å¼€æºå’Œé—­æºæ¨¡å‹åœ¨å¤„ç†ç»¼åˆç©ºé—´æ¨ç†ä»»åŠ¡æ—¶å‡è¡¨ç°å‡ºæ˜¾è‘—å±€é™ã€‚ä¸ºäº†æå‡æ¨¡å‹æ€§èƒ½ï¼Œè®ºæ–‡è¿›ä¸€æ­¥æ¢ç´¢äº†PointGraphï¼ˆæ˜¾å¼åœºæ™¯å›¾çº¿ç´¢ï¼‰å’ŒSpatialCoTï¼ˆæ–°è§†è§’é“¾å¼æ€ç»´ï¼‰ä¸¤ç§å¢å¼ºç­–ç•¥ã€‚è¯¥ç ”ç©¶ä¸ä»…æ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨é«˜çº§ç©ºé—´è®¤çŸ¥ä¸Šçš„çŸ­æ¿ï¼Œä¹Ÿä¸ºæœªæ¥å¼€å‘æ›´å…·ç©ºé—´æ™ºèƒ½çš„è§†è§‰è¯­è¨€ç³»ç»Ÿæä¾›äº†é‡è¦å·¥å…·å’Œä¼˜åŒ–æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://qizekun.github.io/omnispatial/",
      "pdf_url": "https://arxiv.org/pdf/2506.03135v2",
      "published_date": "2025-06-03 17:58:29 UTC",
      "updated_date": "2025-09-24 00:47:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:57:24.147528+00:00"
    },
    {
      "arxiv_id": "2506.03133v2",
      "title": "PoLAR: Polar-Decomposed Low-Rank Adapter Representation",
      "title_zh": "PoLARï¼šæåˆ†è§£ä½ç§©é€‚é…å™¨è¡¨ç¤º",
      "authors": [
        "Kai Lion",
        "Liang Zhang",
        "Bingcong Li",
        "Niao He"
      ],
      "abstract": "We show that low-rank adaptation of large-scale models suffers from a low stable rank that is well below the linear algebraic rank of the subspace, degrading fine-tuning performance. To mitigate the underutilization of the allocated subspace, we propose PoLAR, a parameterization inspired by the polar decomposition that factorizes the low-rank update into two direction matrices constrained to Stiefel manifolds and an unconstrained scale matrix. Our theory shows that PoLAR yields an exponentially faster convergence rate on a canonical low-rank adaptation problem. Pairing the parameterization with Riemannian optimization leads to consistent gains on three different benchmarks testing general language understanding, commonsense reasoning, and mathematical problem solving with base model sizes ranging from 350M to 27B.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡æ¨¡å‹ä½ç§©è‡ªé€‚åº”(low-rank adaptation)ä¸­ç¨³å®šç§©(stable rank)æ˜¾è‘—ä½äºå­ç©ºé—´çº¿æ€§ä»£æ•°ç§©ã€å¯¼è‡´å¾®è°ƒæ€§èƒ½å—æŸçš„é—®é¢˜ï¼Œæå‡ºäº†PoLARã€‚è¯¥æ–¹æ³•å—æåˆ†è§£(polar decomposition)å¯å‘ï¼Œå°†ä½ç§©æ›´æ–°åˆ†è§£ä¸ºä¸¤ä¸ªå—Stiefelæµå½¢çº¦æŸçš„æ–¹å‘çŸ©é˜µå’Œä¸€ä¸ªæ— çº¦æŸçš„ç¼©æ”¾çŸ©é˜µã€‚ç†è®ºè¯æ˜PoLARåœ¨å…¸å‹ä½ç§©è‡ªé€‚åº”é—®é¢˜ä¸Šå…·æœ‰æŒ‡æ•°çº§æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚é€šè¿‡ç»“åˆé»æ›¼ä¼˜åŒ–(Riemannian optimization)ï¼Œè¯¥æ–¹æ³•åœ¨é€šç”¨è¯­è¨€ç†è§£ã€å¸¸è¯†æ¨ç†åŠæ•°å­¦é—®é¢˜æ±‚è§£ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æ˜¾è‘—æ”¶ç›Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPoLARåœ¨å‚æ•°è§„æ¨¡ä»350Mè‡³27Bçš„åŸºç¡€æ¨¡å‹ä¸Šå‡è¡¨ç°å‡ºä¸€è‡´çš„æ€§èƒ½æå‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03133v2",
      "published_date": "2025-06-03 17:58:19 UTC",
      "updated_date": "2025-10-31 14:49:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:57:33.565045+00:00"
    },
    {
      "arxiv_id": "2506.03238v2",
      "title": "Rethinking Whole-Body CT Image Interpretation: An Abnormality-Centric Approach",
      "title_zh": "é‡æ–°å®¡è§†å…¨èº« CT å›¾åƒè§£è¯»ï¼šä¸€ç§ä»¥å¼‚å¸¸ä¸ºä¸­å¿ƒçš„æ–¹æ³•",
      "authors": [
        "Ziheng Zhao",
        "Lisong Dai",
        "Ya Zhang",
        "Yanfeng Wang",
        "Weidi Xie"
      ],
      "abstract": "Automated interpretation of CT images-particularly localizing and describing abnormal findings across multi-plane and whole-body scans-remains a significant challenge in clinical radiology. This work aims to address this challenge through four key contributions: (i) On taxonomy, we collaborate with senior radiologists to propose a comprehensive hierarchical classification system, with 404 representative abnormal findings across all body regions; (ii) On data, we contribute a dataset containing over 14.5K CT images from multiple planes and all human body regions, and meticulously provide grounding annotations for over 19K abnormalities, each linked to the detailed description and cast into the taxonomy; (iii) On model development, we propose OmniAbnorm-CT, which can automatically ground and describe abnormal findings on multi-plane and whole-body CT images based on text queries, while also allowing flexible interaction through visual prompts; (iv) On evaluation, we establish three representative tasks based on real clinical scenarios, and introduce a clinically grounded metric to assess abnormality descriptions. Through extensive experiments, we show that OmniAbnorm-CT can significantly outperform existing methods in both internal and external validations, and across all the tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ä»¥å¼‚å¸¸ä¸ºä¸­å¿ƒ(Abnormality-Centric)çš„å…¨èº«ä½“CTå›¾åƒè‡ªåŠ¨è§£é‡Šæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¸´åºŠæ”¾å°„å­¦ä¸­ç²¾ç¡®å®šä½å’Œæè¿°å¤šå¹³é¢å½±åƒå¼‚å¸¸å‘ç°çš„æŒ‘æˆ˜ã€‚é€šè¿‡ä¸èµ„æ·±æ”¾å°„ç§‘åŒ»ç”Ÿåˆä½œï¼Œç ”ç©¶æ„å»ºäº†ä¸€ä¸ªåŒ…å«404ç§ä»£è¡¨æ€§å¼‚å¸¸å‘ç°çš„å±‚çº§åˆ†ç±»ç³»ç»Ÿ(Taxonomy)ï¼Œå¹¶è´¡çŒ®äº†åŒ…å«1.45ä¸‡å¼ CTå›¾åƒåŠ1.9ä¸‡ä¸ªç²¾ç»†å®šä½æ ‡æ³¨(Grounding Annotations)çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†OmniAbnorm-CTæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ”¯æŒé€šè¿‡æ–‡æœ¬æŸ¥è¯¢æˆ–è§†è§‰æç¤º(Visual Prompts)å®ç°å¯¹å…¨èº«ä½“CTå›¾åƒä¸­å¼‚å¸¸å‘ç°çš„è‡ªåŠ¨å®šä½ä¸æè¿°ã€‚æ­¤å¤–ï¼Œè¯¥å·¥ä½œå»ºç«‹äº†ä¸‰é¡¹åŸºäºä¸´åºŠåœºæ™¯çš„ä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº†ä¸´åºŠåŸºå‡†è¯„ä¼°æŒ‡æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniAbnorm-CTåœ¨å†…å¤–éƒ¨éªŒè¯ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºè‡ªåŠ¨åŒ–å…¨èº«ä½“å½±åƒè¯Šæ–­æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "40 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.03238v2",
      "published_date": "2025-06-03 17:57:34 UTC",
      "updated_date": "2025-11-17 05:49:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:58:15.676588+00:00"
    },
    {
      "arxiv_id": "2506.03237v3",
      "title": "UniSite: The First Cross-Structure Dataset and Learning Framework for End-to-End Ligand Binding Site Detection",
      "title_zh": "UniSiteï¼šé¦–ä¸ªç”¨äºç«¯åˆ°ç«¯é…ä½“ç»“åˆä½ç‚¹æ£€æµ‹çš„è·¨ç»“æ„æ•°æ®é›†ä¸å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Jigang Fan",
        "Quanlin Wu",
        "Shengjie Luo",
        "Liwei Wang"
      ],
      "abstract": "The detection of ligand binding sites for proteins is a fundamental step in Structure-Based Drug Design. Despite notable advances in recent years, existing methods, datasets, and evaluation metrics are confronted with several key challenges: (1) current datasets and methods are centered on individual protein-ligand complexes and neglect that diverse binding sites may exist across multiple complexes of the same protein, introducing significant statistical bias; (2) ligand binding site detection is typically modeled as a discontinuous workflow, employing binary segmentation and subsequent clustering algorithms; (3) traditional evaluation metrics do not adequately reflect the actual performance of different binding site prediction methods. To address these issues, we first introduce UniSite-DS, the first UniProt (Unique Protein)-centric ligand binding site dataset, which contains 4.81 times more multi-site data and 2.08 times more overall data compared to the previously most widely used datasets. We then propose UniSite, the first end-to-end ligand binding site detection framework supervised by set prediction loss with bijective matching. In addition, we introduce Average Precision based on Intersection over Union (IoU) as a more accurate evaluation metric for ligand binding site prediction. Extensive experiments on UniSite-DS and several representative benchmark datasets demonstrate that IoU-based Average Precision provides a more accurate reflection of prediction quality, and that UniSite outperforms current state-of-the-art methods in ligand binding site detection. The dataset and codes will be made publicly available at https://github.com/quanlin-wu/unisite.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è›‹ç™½è´¨é…ä½“ç»“åˆä½ç‚¹ (Ligand Binding Site) æ£€æµ‹ä¸­å­˜åœ¨çš„ç»Ÿè®¡åå·®ã€éç«¯åˆ°ç«¯å·¥ä½œæµä»¥åŠè¯„ä¼°æŒ‡æ ‡å±€é™æ€§ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†é¦–ä¸ªä»¥ UniProt ä¸ºä¸­å¿ƒçš„è·¨ç»“æ„æ•°æ®é›† UniSite-DSï¼Œå…¶æ•°æ®è§„æ¨¡åŠå¤šä½ç‚¹æ•°æ®é‡æ˜¾è‘—è¶…è¿‡ç°æœ‰ä¸»æµæ•°æ®é›†ã€‚åŸºäºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº† UniSiteï¼Œè¿™æ˜¯é¦–ä¸ªåˆ©ç”¨é›†åˆé¢„æµ‹æŸå¤± (Set Prediction Loss) å’ŒåŒå‘åŒ¹é… (Bijective Matching) è¿›è¡Œç›‘ç£çš„ç«¯åˆ°ç«¯é…ä½“ç»“åˆä½ç‚¹æ£€æµ‹æ¡†æ¶ã€‚åŒæ—¶ï¼Œç ”ç©¶å¼•å…¥äº†åŸºäºäº¤å¹¶æ¯” (Intersection over Union, IoU) çš„å¹³å‡ç²¾åº¦ (Average Precision) ä½œä¸ºæ›´å‡†ç¡®çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥å¼¥è¡¥ä¼ ç»ŸæŒ‡æ ‡çš„ä¸è¶³ã€‚åœ¨ UniSite-DS å’Œå¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒUniSite åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†å½“å‰çš„å…ˆè¿›æ–¹æ³• (SOTA)ï¼Œè¯æ˜äº†ç«¯åˆ°ç«¯æ¶æ„ä¸è·¨ç»“æ„æ•°æ®é›†åœ¨æå‡é¢„æµ‹ç²¾åº¦æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG",
        "q-bio.BM"
      ],
      "primary_category": "q-bio.QM",
      "comment": "Accepted by NeurIPS 2025 as a Spotlight paper",
      "pdf_url": "https://arxiv.org/pdf/2506.03237v3",
      "published_date": "2025-06-03 17:49:41 UTC",
      "updated_date": "2025-11-11 18:16:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:58:14.395027+00:00"
    },
    {
      "arxiv_id": "2506.03106v5",
      "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback",
      "title_zh": "Critique-GRPOï¼šåˆ©ç”¨è‡ªç„¶è¯­è¨€ä¸æ•°å€¼åé¦ˆæå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›",
      "authors": [
        "Xiaoying Zhang",
        "Hao Sun",
        "Yipeng Zhang",
        "Kaituo Feng",
        "Chaochao Lu",
        "Chao Yang",
        "Helen Meng"
      ],
      "abstract": "Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly enhanced the complex reasoning capabilities of large language models (LLMs). Despite this success, we identify three key challenges encountered by RL with solely numerical feedback: performance plateaus, limited effectiveness of spontaneous self-reflection, and persistent failures. We then demonstrate that RL-finetuned models, even after exhibiting performance plateaus, can generate correct refinements on persistently failed problems by leveraging natural language feedback in the form of critiques. Building on this insight, we propose Critique-GRPO, an online RL framework that integrates both natural language and numerical feedback for effective policy optimization. Critique-GRPO enables LLMs to learn from initial responses and critique-guided self-refinements simultaneously while maintaining exploration. Additionally, we employ a shaping function to amplify learning from correct, especially unfamiliar, refinements and penalize incorrect ones. Extensive experiments with Qwen2.5-7B-Base, Qwen2.5-Math-7B-Base, and Qwen3-8B demonstrate that Critique-GRPO consistently outperforms supervised learning and RL-based fine-tuning methods across eight challenging mathematical, STEM, and general reasoning tasks. Specifically, Critique-GRPO improves average pass@1 scores across all compared methods by approximately +4.4% on Qwen2.5-7B-Base and +3.8% on Qwen3-8B. Notably, Critique-GRPO enables effective self-improvement through self-critiquing, achieving significant gains over GRPO, e.g., +16.7% pass@1 improvement on AIME 2024.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä»…ä½¿ç”¨æ•°å€¼åé¦ˆ(Numerical Feedback)çš„å¼ºåŒ–å­¦ä¹ (RL)åœ¨å¤§è¯­è¨€æ¨¡å‹æ¨ç†ä¸­é¢ä¸´çš„æ€§èƒ½ç“¶é¢ˆã€è‡ªå‘åæ€å—é™åŠæŒç»­æ€§å¤±è´¥ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†Critique-GRPOåœ¨çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°å°†è‡ªç„¶è¯­è¨€åé¦ˆ(Natural Language Feedback)ä¸æ•°å€¼åé¦ˆç›¸ç»“åˆï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŒæ—¶ä»åˆå§‹å“åº”å’ŒåŸºäºæ‰¹åˆ¤å¼•å¯¼çš„è‡ªæˆ‘ä¿®æ­£ä¸­è¿›è¡Œç­–ç•¥ä¼˜åŒ–ã€‚é€šè¿‡å¼•å…¥æ•´å½¢å‡½æ•°(Shaping Function)ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ”¾å¤§å¯¹æ­£ç¡®ä¿®æ­£è¡Œä¸ºçš„å­¦ä¹ æ•ˆæœï¼Œå¹¶æœ‰æ•ˆæƒ©ç½šé”™è¯¯çš„å°è¯•ã€‚åœ¨Qwenç³»åˆ—æ¨¡å‹ä¸Šçš„å®éªŒè¯æ˜ï¼ŒCritique-GRPOåœ¨æ•°å­¦ã€STEMå’Œé€šç”¨æ¨ç†ç­‰å…«é¡¹æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸­ä¸€è‡´ä¼˜äºç›‘ç£å­¦ä¹ å’Œä¼ ç»Ÿçš„RLå¾®è°ƒæ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯åœ¨AIME 2024æµ‹è¯•ä¸­ï¼ŒCritique-GRPOç›¸æ¯”GRPOå®ç°äº†16.7%çš„pass@1æ€§èƒ½æå‡ï¼Œå……åˆ†éªŒè¯äº†å…¶é€šè¿‡è‡ªæˆ‘æ‰¹åˆ¤å®ç°æœ‰æ•ˆè‡ªæˆ‘æ”¹è¿›çš„èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "52 pages, updated with new experimental results and implementation details",
      "pdf_url": "https://arxiv.org/pdf/2506.03106v5",
      "published_date": "2025-06-03 17:39:02 UTC",
      "updated_date": "2025-08-20 09:10:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:58:17.812872+00:00"
    },
    {
      "arxiv_id": "2506.03102v1",
      "title": "Designing Algorithmic Delegates: The Role of Indistinguishability in Human-AI Handoff",
      "title_zh": "è®¾è®¡ç®—æ³•ä»£ç†ï¼šä¸å¯åŒºåˆ†æ€§åœ¨äººæœºç§»äº¤ä¸­çš„ä½œç”¨",
      "authors": [
        "Sophie Greenwood",
        "Karen Levy",
        "Solon Barocas",
        "Hoda Heidari",
        "Jon Kleinberg"
      ],
      "abstract": "As AI technologies improve, people are increasingly willing to delegate tasks to AI agents. In many cases, the human decision-maker chooses whether to delegate to an AI agent based on properties of the specific instance of the decision-making problem they are facing. Since humans typically lack full awareness of all the factors relevant to this choice for a given decision-making instance, they perform a kind of categorization by treating indistinguishable instances -- those that have the same observable features -- as the same. In this paper, we define the problem of designing the optimal algorithmic delegate in the presence of categories. This is an important dimension in the design of algorithms to work with humans, since we show that the optimal delegate can be an arbitrarily better teammate than the optimal standalone algorithmic agent. The solution to this optimal delegation problem is not obvious: we discover that this problem is fundamentally combinatorial, and illustrate the complex relationship between the optimal design and the properties of the decision-making task even in simple settings. Indeed, we show that finding the optimal delegate is computationally hard in general. However, we are able to find efficient algorithms for producing the optimal delegate in several broad cases of the problem, including when the optimal action may be decomposed into functions of features observed by the human and the algorithm. Finally, we run computational experiments to simulate a designer updating an algorithmic delegate over time to be optimized for when it is actually adopted by users, and show that while this process does not recover the optimal delegate in general, the resulting delegate often performs quite well.",
      "tldr_zh": "è¯¥è®ºæ–‡ç ”ç©¶äº†äººç±»-äººå·¥æ™ºèƒ½ç§»äº¤ï¼ˆHuman-AI Handoffï¼‰è¿‡ç¨‹ä¸­æœ€ä¼˜ç®—æ³•ä»£ç†ï¼ˆAlgorithmic Delegateï¼‰çš„è®¾è®¡é—®é¢˜ï¼Œé‡ç‚¹åˆ†æäº†ä¸å¯åŒºåˆ†æ€§ï¼ˆIndistinguishabilityï¼‰å¯¹å§”æ‰˜å†³ç­–çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œç”±äºäººç±»å†³ç­–è€…å¾€å¾€æ— æ³•å®Œå…¨äº†è§£æ‰€æœ‰ç›¸å…³å› ç´ ï¼Œå€¾å‘äºå°†å…·æœ‰ç›¸åŒå¯è§‚å¯Ÿç‰¹å¾çš„å®ä¾‹å½’ä¸ºåŒç±»ï¼ˆCategoriesï¼‰å¹¶é‡‡å–ä¸€è‡´çš„å§”æ‰˜ç­–ç•¥ï¼Œè€Œåœ¨æ­¤èƒŒæ™¯ä¸‹è®¾è®¡çš„æœ€ä¼˜ä»£ç†ä½œä¸ºå›¢é˜Ÿæˆå‘˜çš„æ€§èƒ½å¯æ˜¾è‘—ä¼˜äºç‹¬ç«‹çš„æœ€ä¼˜ç®—æ³•æ™ºèƒ½ä½“ã€‚ä½œè€…æŒ‡å‡ºå¯»æ‰¾æœ€ä¼˜ä»£ç†çš„é—®é¢˜åœ¨æœ¬è´¨ä¸Šæ˜¯ç»„åˆå¼çš„ï¼Œä¸”åœ¨ä¸€èˆ¬æƒ…å†µä¸‹å…·æœ‰è®¡ç®—å¤æ‚æ€§ï¼ˆComputationally Hardï¼‰ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶é’ˆå¯¹å‡ ç§å¹¿æ³›åœºæ™¯æå‡ºäº†ç”Ÿæˆæœ€ä¼˜ä»£ç†çš„é«˜æ•ˆç®—æ³•ï¼Œä¾‹å¦‚å½“æœ€ä¼˜åŠ¨ä½œå¯åˆ†è§£ä¸ºäººç±»ä¸ç®—æ³•å…±åŒè§‚æµ‹ç‰¹å¾çš„å‡½æ•°æ—¶ã€‚æœ€åï¼Œé€šè¿‡è®¡ç®—å®éªŒæ¨¡æ‹Ÿäº†è®¾è®¡è€…æ ¹æ®ç”¨æˆ·é‡‡çº³æƒ…å†µåŠ¨æ€ä¼˜åŒ–ä»£ç†çš„è¿‡ç¨‹ï¼Œç»“æœè¡¨æ˜è™½ç„¶è¯¥è¿‡ç¨‹ä¸ä¸€å®šèƒ½è¾¾åˆ°ç†è®ºå…¨å±€æœ€ä¼˜ï¼Œä½†ç”Ÿæˆçš„ä»£ç†åœ¨å®é™…åº”ç”¨ä¸­é€šå¸¸è¡¨ç°ä¼˜å¼‚ã€‚",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.GT",
      "comment": "Accepted at the Twenty-Sixth ACM Conference on Economics and Computation (EC'25)",
      "pdf_url": "https://arxiv.org/pdf/2506.03102v1",
      "published_date": "2025-06-03 17:36:20 UTC",
      "updated_date": "2025-06-03 17:36:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:58:19.014804+00:00"
    },
    {
      "arxiv_id": "2506.03100v3",
      "title": "Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified Theory and Risk Bounds",
      "title_zh": "æ£€ç´¢å¢å¼ºç”Ÿæˆä½œä¸ºå¸¦å™ªå£°çš„æƒ…å¢ƒå­¦ä¹ ï¼šç»Ÿä¸€ç†è®ºä¸é£é™©ç•Œ",
      "authors": [
        "Yang Guo",
        "Yutian Tao",
        "Yifei Ming",
        "Robert D. Nowak",
        "Yingyu Liang"
      ],
      "abstract": "Retrieval-augmented generation (RAG) has seen many empirical successes in recent years by aiding the LLM with external knowledge. However, its theoretical aspect has remained mostly unexplored. In this paper, we propose the first finite-sample generalization bound for RAG in in-context linear regression and derive an exact bias-variance tradeoff. Our framework views the retrieved texts as query-dependent noisy in-context examples and recovers the classical in-context learning (ICL) and standard RAG as the limit cases. Our analysis suggests that an intrinsic ceiling on generalization error exists on RAG as opposed to the ICL. Furthermore, our framework is able to model retrieval both from the training data and from external corpora by introducing uniform and non-uniform RAG noise. In line with our theory, we show the sample efficiency of ICL and RAG empirically with experiments on common QA benchmarks, such as Natural Questions and TriviaQA.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation, RAG)åœ¨ç†è®ºå±‚é¢æ¢ç´¢ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªé’ˆå¯¹ä¸Šä¸‹æ–‡çº¿æ€§å›å½’(in-context linear regression)ä¸­RAGçš„æœ‰é™æ ·æœ¬æ³›åŒ–ç•Œ(finite-sample generalization bound)ï¼Œå¹¶æ¨å¯¼å‡ºäº†ç²¾ç¡®çš„åå·®-æ–¹å·®æƒè¡¡(bias-variance tradeoff)ã€‚è¯¥æ¡†æ¶å°†æ£€ç´¢åˆ°çš„æ–‡æœ¬è§†ä¸ºä¾èµ–äºæŸ¥è¯¢çš„æœ‰å™ªå£°ä¸Šä¸‹æ–‡ç¤ºä¾‹(query-dependent noisy in-context examples)ï¼Œä»è€Œå°†ç»å…¸çš„ä¸Šä¸‹æ–‡å­¦ä¹ (In-Context Learning, ICL)å’Œæ ‡å‡†RAGä½œä¸ºæé™æƒ…å†µçº³å…¥ç»Ÿä¸€ç†è®ºä½“ç³»ã€‚ç†è®ºåˆ†ææ­ç¤ºäº†RAGåœ¨æ³›åŒ–è¯¯å·®ä¸Šå­˜åœ¨ä¸€ä¸ªå†…åœ¨ä¸Šé™(intrinsic ceiling)ï¼Œè¿™ä¸ICLçš„ç‰¹å¾æœ‰æ‰€ä¸åŒã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶é€šè¿‡å¼•å…¥å‡åŒ€å’Œéå‡åŒ€RAGå™ªå£°ï¼Œå®ç°äº†å¯¹ä»è®­ç»ƒæ•°æ®å’Œå¤–éƒ¨è¯­æ–™åº“æ£€ç´¢åœºæ™¯çš„å»ºæ¨¡ã€‚æœ€åï¼Œåœ¨Natural Questionså’ŒTriviaQAç­‰å¸¸ç”¨é—®ç­”åŸºå‡†ä¸Šçš„å®éªŒéªŒè¯äº†ICLå’ŒRAGçš„æ ·æœ¬æ•ˆç‡ï¼Œå…¶å®è¯ç»“æœä¸æ‰€æç†è®ºé«˜åº¦å»åˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "math.ST"
      ],
      "primary_category": "cs.LG",
      "comment": "Under Review",
      "pdf_url": "https://arxiv.org/pdf/2506.03100v3",
      "published_date": "2025-06-03 17:31:53 UTC",
      "updated_date": "2025-06-09 10:35:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:58:25.128220+00:00"
    },
    {
      "arxiv_id": "2506.06362v1",
      "title": "CR-BLEA: Contrastive Ranking for Adaptive Resource Allocation in Bilevel Evolutionary Algorithms",
      "title_zh": "CR-BLEAï¼šåŒå±‚æ¼”åŒ–ç®—æ³•ä¸­è‡ªé€‚åº”èµ„æºåˆ†é…çš„å¯¹æ¯”æ’åº",
      "authors": [
        "Dejun Xu",
        "Jijia Chen",
        "Gary G. Yen",
        "Min Jiang"
      ],
      "abstract": "Bilevel optimization poses a significant computational challenge due to its nested structure, where each upper-level candidate solution requires solving a corresponding lower-level problem. While evolutionary algorithms (EAs) are effective at navigating such complex landscapes, their high resource demands remain a key bottleneck -- particularly the redundant evaluation of numerous unpromising lower-level tasks. Despite recent advances in multitasking and transfer learning, resource waste persists. To address this issue, we propose a novel resource allocation framework for bilevel EAs that selectively identifies and focuses on promising lower-level tasks. Central to our approach is a contrastive ranking network that learns relational patterns between paired upper- and lower-level solutions online. This knowledge guides a reference-based ranking strategy that prioritizes tasks for optimization and adaptively controls resampling based on estimated population quality. Comprehensive experiments across five state-of-the-art bilevel algorithms show that our framework significantly reduces computational cost while preserving -- or even enhancing -- solution accuracy. This work offers a generalizable strategy to improve the efficiency of bilevel EAs, paving the way for more scalable bilevel optimization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒå±‚ä¼˜åŒ– (Bilevel optimization) ä¸­ç”±äºåµŒå¥—ç»“æ„å¯¼è‡´çš„è®¡ç®—èµ„æºæµªè´¹é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º CR-BLEA çš„è‡ªé€‚åº”èµ„æºåˆ†é…æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨é€šè¿‡é€‰æ‹©æ€§åœ°è¯†åˆ«å’Œèšç„¦äºæœ‰æ½œåŠ›çš„ä¸‹å±‚ä»»åŠ¡ï¼Œä¼˜åŒ–è¿›åŒ–ç®—æ³• (Evolutionary algorithms) çš„è¿è¡Œæ•ˆç‡ã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªå¯¹æ¯”æ’åºç½‘ç»œ (Contrastive ranking network)ï¼Œç”¨äºåœ¨çº¿å­¦ä¹ ä¸Šå±‚ä¸ä¸‹å±‚å€™é€‰è§£ä¹‹é—´çš„å…³è”æ¨¡å¼ï¼Œå¹¶ç»“åˆå‚è€ƒæ’åºç­–ç•¥ (Reference-based ranking strategy) æ¥ä¼˜å…ˆè°ƒåº¦ä»»åŠ¡ä»¥åŠåŠ¨æ€æ§åˆ¶é‡é‡‡æ ·ã€‚åœ¨äº”ç§å…ˆè¿›ç®—æ³•ä¸Šçš„å®éªŒéªŒè¯æ˜¾ç¤ºï¼ŒCR-BLEA åœ¨æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆä¿æŒç”šè‡³æå‡è§£çš„ç²¾åº¦ã€‚è¿™ä¸€ç ”ç©¶ä¸ºæé«˜åŒå±‚è¿›åŒ–ç®—æ³•çš„æ•ˆç‡æä¾›äº†å¯æ¨å¹¿çš„ç­–ç•¥ï¼Œæ˜¾è‘—å¢å¼ºäº†å¤§ç³»ç»ŸåŒå±‚ä¼˜åŒ–çš„å¯æ‰©å±•æ€§ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06362v1",
      "published_date": "2025-06-03 17:31:49 UTC",
      "updated_date": "2025-06-03 17:31:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:58:42.344965+00:00"
    },
    {
      "arxiv_id": "2506.03099v1",
      "title": "TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models",
      "title_zh": "TalkingMachinesï¼šåŸºäºè‡ªå›å½’æ‰©æ•£æ¨¡å‹çš„å®æ—¶éŸ³é¢‘é©±åŠ¨ç±» FaceTime è§†é¢‘",
      "authors": [
        "Chetwin Low",
        "Weimin Wang"
      ],
      "abstract": "In this paper, we present TalkingMachines -- an efficient framework that transforms pretrained video generation models into real-time, audio-driven character animators. TalkingMachines enables natural conversational experiences by integrating an audio large language model (LLM) with our video generation foundation model. Our primary contributions include: (1) We adapt a pretrained SOTA image-to-video DiT into an audio-driven avatar generation model of 18 billion parameters; (2) We enable infinite video streaming without error accumulation through asymmetric knowledge distillation from a bidirectional teacher model into a sparse causal, autoregressive student model; (3) We design a high-throughput, low-latency inference pipeline incorporating several key engineering optimizations such as: (a) disaggregation of the DiT and VAE decoder across separate devices, (b) efficient overlap of inter-device communication and computation using CUDA streams, (c) elimination of redundant recomputations to maximize frame-generation throughput. Please see demo videos here - https://aaxwaz.github.io/TalkingMachines/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TalkingMachinesï¼Œä¸€ä¸ªæ—¨åœ¨å°†é¢„è®­ç»ƒè§†é¢‘ç”Ÿæˆæ¨¡å‹è½¬æ¢ä¸ºå®æ—¶éŸ³é¢‘é©±åŠ¨è§’è‰²åŠ¨ç”»çš„é«˜æ•ˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡é›†æˆéŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹(LLM)ä¸è§†é¢‘ç”ŸæˆåŸºç¡€æ¨¡å‹ï¼Œå®ç°äº†è‡ªç„¶çš„è§†é¢‘å¯¹è¯ä½“éªŒã€‚å…¶æ ¸å¿ƒè´¡çŒ®åŒ…æ‹¬å°†ä¸€ä¸ª180äº¿å‚æ•°çš„å›¾åƒè½¬è§†é¢‘(image-to-video) DiTæ¨¡å‹é€‚é…ä¸ºåŒ–èº«ç”Ÿæˆæ¨¡å‹ï¼Œå¹¶åˆ©ç”¨éå¯¹ç§°çŸ¥è¯†è’¸é¦(asymmetric knowledge distillation)æŠ€æœ¯å°†åŒå‘æ•™å¸ˆæ¨¡å‹è½¬åŒ–ä¸ºç¨€ç–å› æœè‡ªå›å½’(sparse causal, autoregressive)å­¦ç”Ÿæ¨¡å‹ï¼Œä»è€Œæ”¯æŒæ— è¯¯å·®ç´¯ç§¯çš„æ— é™æµå¼è¾“å‡ºã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡DiTä¸VAEè§£ç å™¨è®¾å¤‡è§£è€¦ã€CUDAæµå¼‚æ­¥é€šä¿¡ä»¥åŠæ¶ˆé™¤å†—ä½™è®¡ç®—ç­‰å·¥ç¨‹ä¼˜åŒ–ï¼Œè®¾è®¡äº†é«˜ååã€ä½å»¶è¿Ÿçš„æ¨ç†æµæ°´çº¿ã€‚è¿™äº›æŠ€æœ¯çªç ´å…±åŒæ”¯æ’‘äº†ç±»FaceTimeé£æ ¼çš„å®æ—¶ã€é«˜è´¨é‡éŸ³é¢‘é©±åŠ¨è§†é¢‘ç”Ÿæˆã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03099v1",
      "published_date": "2025-06-03 17:29:28 UTC",
      "updated_date": "2025-06-03 17:29:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:58:44.120027+00:00"
    },
    {
      "arxiv_id": "2506.03097v1",
      "title": "EgoVLM: Policy Optimization for Egocentric Video Understanding",
      "title_zh": "EgoVLMï¼šé¢å‘ç¬¬ä¸€äººç§°è§†è§’è§†é¢‘ç†è§£çš„ç­–ç•¥ä¼˜åŒ–",
      "authors": [
        "Ashwin Vinod",
        "Shrey Pandit",
        "Aditya Vavre",
        "Linshen Liu"
      ],
      "abstract": "Emerging embodied AI applications, such as wearable cameras and autonomous agents, have underscored the need for robust reasoning from first person video streams. We introduce EgoVLM, a vision-language model specifically designed to integrate visual comprehension and spatial-temporal reasoning within egocentric video contexts. EgoVLM is fine-tuned via Group Relative Policy Optimization (GRPO), a reinforcement learning method adapted to align model outputs with human-like reasoning steps. Following DeepSeek R1-Zero's approach, we directly tune using RL without any supervised fine-tuning phase on chain-of-thought (CoT) data. We evaluate EgoVLM on egocentric video question answering benchmarks and show that domain-specific training substantially improves performance over general-purpose VLMs. Our EgoVLM-3B, trained exclusively on non-CoT egocentric data, outperforms the base Qwen2.5-VL 3B and 7B models by 14.33 and 13.87 accuracy points on the EgoSchema benchmark, respectively. By explicitly generating reasoning traces, EgoVLM enhances interpretability, making it well-suited for downstream applications. Furthermore, we introduce a novel keyframe-based reward that incorporates salient frame selection to guide reinforcement learning optimization. This reward formulation opens a promising avenue for future exploration in temporally grounded egocentric reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† EgoVLMï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ä¸ºç¬¬ä¸€äººç§°è§†é¢‘èƒŒæ™¯ä¸‹çš„è§†è§‰ç†è§£å’Œæ—¶ç©ºæ¨ç†è®¾è®¡çš„è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Model)ã€‚EgoVLM é€šè¿‡ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ– (Group Relative Policy Optimization, GRPO) è¿›è¡Œå¾®è°ƒï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨ä½¿æ¨¡å‹è¾“å‡ºä¸äººç±»æ¨ç†æ­¥éª¤å¯¹é½çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚å€Ÿé‰´ DeepSeek R1-Zero çš„æ–¹æ³•ï¼Œè¯¥æ¨¡å‹ç›´æ¥åˆ©ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œè€Œæ— éœ€åœ¨æ€ç»´é“¾ (Chain-of-Thought, CoT) æ•°æ®ä¸Šè¿›è¡Œä»»ä½•ç›‘ç£å¾®è°ƒ (Supervised Fine-Tuning) é˜¶æ®µã€‚ç ”ç©¶è€…è¿˜æå‡ºäº†ä¸€ç§åŸºäºå…³é”®å¸§çš„æ–°å‹å¥–åŠ±æœºåˆ¶ï¼Œé€šè¿‡æ˜¾è‘—å¸§é€‰æ‹©æ¥å¼•å¯¼å¼ºåŒ–å­¦ä¹ çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä»…åŒ…å« 3B å‚æ•°çš„ EgoVLM åœ¨ EgoSchema åŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«æ¯” Qwen2.5-VL 3B å’Œ 7B æ¨¡å‹å‡†ç¡®ç‡é«˜å‡º 14.33 å’Œ 13.87 ä¸ªç™¾åˆ†ç‚¹ã€‚è¯¥æ¨¡å‹é€šè¿‡æ˜¾å¼ç”Ÿæˆæ¨ç†è½¨è¿¹å¢å¼ºäº†å¯è§£é‡Šæ€§ï¼Œä¸ºæœªæ¥å…·æœ‰æ—¶é—´å¯¼å‘çš„ç¬¬ä¸€äººç§°æ¨ç†ç ”ç©¶å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Our Code can be found at https://github.com/adityavavre/VidEgoVLM",
      "pdf_url": "https://arxiv.org/pdf/2506.03097v1",
      "published_date": "2025-06-03 17:28:00 UTC",
      "updated_date": "2025-06-03 17:28:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:58:36.088615+00:00"
    },
    {
      "arxiv_id": "2506.03095v1",
      "title": "DPO Learning with LLMs-Judge Signal for Computer Use Agents",
      "title_zh": "åŸºäº LLMs-Judge ä¿¡å·çš„è®¡ç®—æœºæ“ä½œæ™ºèƒ½ä½“ DPO å­¦ä¹ ",
      "authors": [
        "Man Luo",
        "David Cobbley",
        "Xin Su",
        "Shachar Rosenman",
        "Vasudev Lal",
        "Shao-Yen Tseng",
        "Phillip Howard"
      ],
      "abstract": "Computer use agents (CUA) are systems that automatically interact with graphical user interfaces (GUIs) to complete tasks. CUA have made significant progress with the advent of large vision-language models (VLMs). However, these agents typically rely on cloud-based inference with substantial compute demands, raising critical privacy and scalability concerns, especially when operating on personal devices. In this work, we take a step toward privacy-preserving and resource-efficient agents by developing a lightweight vision-language model that runs entirely on local machines. To train this compact agent, we introduce an LLM-as-Judge framework that automatically evaluates and filters synthetic interaction trajectories, producing high-quality data for reinforcement learning without human annotation. Experiments on the OS-World benchmark demonstrate that our fine-tuned local model outperforms existing baselines, highlighting a promising path toward private, efficient, and generalizable GUI agents.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è®¡ç®—æœºä½¿ç”¨æ™ºèƒ½ä½“(Computer Use Agents)åœ¨éšç§ä¿æŠ¤å’Œèµ„æºæ•ˆç‡æ–¹é¢çš„éœ€æ±‚ï¼Œå¼€å‘äº†ä¸€ç§èƒ½å¤Ÿå®Œå…¨åœ¨æœ¬åœ°è¿è¡Œçš„è½»é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹(VLM)ã€‚ä¸ºäº†è®­ç»ƒè¿™ä¸€ç´§å‡‘å‹æ™ºèƒ½ä½“ï¼Œç ”ç©¶è€…å¼•å…¥äº†LLM-as-Judgeæ¡†æ¶ï¼Œæ—¨åœ¨è‡ªåŠ¨è¯„ä¼°å’Œè¿‡æ»¤åˆæˆçš„äº¤äº’è½¨è¿¹ï¼Œä»è€Œåœ¨æ— éœ€äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹ä¸ºå¼ºåŒ–å­¦ä¹ æä¾›é«˜è´¨é‡æ•°æ®ã€‚é€šè¿‡ç»“åˆDPO(Direct Preference Optimization)å­¦ä¹ ç®—æ³•ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæå‡åœ¨å›¾å½¢ç”¨æˆ·ç•Œé¢(GUI)ä¸Šçš„ä»»åŠ¡å¤„ç†èƒ½åŠ›ã€‚åœ¨OS-WorldåŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒè¡¨æ˜ï¼Œè¯¥å¾®è°ƒåçš„æœ¬åœ°æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰åŸºçº¿ï¼Œè¯æ˜äº†åˆ©ç”¨LLMä¿¡å·ä¼˜åŒ–å°å‹æ¨¡å‹ä»¥å®ç°ç§å¯†ä¸”é«˜æ•ˆä»»åŠ¡æ‰§è¡Œçš„å¯è¡Œæ€§ï¼Œä¸ºé€šç”¨å‹GUIæ™ºèƒ½ä½“çš„å‘å±•æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03095v1",
      "published_date": "2025-06-03 17:27:04 UTC",
      "updated_date": "2025-06-03 17:27:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:58:38.451160+00:00"
    },
    {
      "arxiv_id": "2506.03088v2",
      "title": "Modelling the Effects of Hearing Loss on Neural Coding in the Auditory Midbrain with Variational Conditioning",
      "title_zh": "åŸºäºå˜åˆ†æ¡ä»¶åŒ–çš„å¬åŠ›æŸå¤±å¯¹å¬è§‰ä¸­è„‘ç¥ç»ç¼–ç å½±å“å»ºæ¨¡",
      "authors": [
        "Lloyd Pellatt",
        "Fotios Drakopoulos",
        "Shievanie Sabesan",
        "Nicholas A. Lesica"
      ],
      "abstract": "The mapping from sound to neural activity that underlies hearing is highly non-linear. The first few stages of this mapping in the cochlea have been modelled successfully, with biophysical models built by hand and, more recently, with DNN models trained on datasets simulated by biophysical models. Modelling the auditory brain has been a challenge because central auditory processing is too complex for models to be built by hand, and datasets for training DNN models directly have not been available. Recent work has taken advantage of large-scale high resolution neural recordings from the auditory midbrain to build a DNN model of normal hearing with great success. But this model assumes that auditory processing is the same in all brains, and therefore it cannot capture the widely varying effects of hearing loss.\n  We propose a novel variational-conditional model to learn to encode the space of hearing loss directly from recordings of neural activity in the auditory midbrain of healthy and noise exposed animals. With hearing loss parametrised by only 6 free parameters per animal, our model accurately predicts 62% of the explainable variance in neural responses from normal hearing animals and 68% for hearing impaired animals, within a few percentage points of state of the art animal specific models. We demonstrate that the model can be used to simulate realistic activity from out of sample animals by fitting only the learned conditioning parameters with Bayesian optimisation, achieving crossentropy loss within 2% of the optimum in 15-30 iterations. Including more animals in the training data slightly improved the performance on unseen animals. This model will enable future development of parametrised hearing loss compensation models trained to directly restore normal neural coding in hearing impaired brains, which can be quickly fitted for a new user by human in the loop optimisation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåä¸º variational-conditional model çš„å˜åˆ†æ¡ä»¶æ¨¡å‹ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿå¬åŠ›æŸå¤±å¯¹ auditory midbrainï¼ˆå¬è§‰ä¸­æ¢ï¼‰ç¥ç»ç¼–ç çš„å½±å“ã€‚é’ˆå¯¹ä¼ ç»Ÿæ¨¡å‹æ— æ³•æ•æ‰ hearing loss å¸¦æ¥çš„ä¸ªä½“å·®å¼‚é—®é¢˜ï¼Œè¯¥æ¨¡å‹é€šè¿‡ä»å¥åº·å’Œå™ªå£°æš´éœ²åŠ¨ç‰©çš„ç¥ç»æ´»åŠ¨è®°å½•ä¸­å­¦ä¹ ï¼Œä»…åˆ©ç”¨6ä¸ªè‡ªç”±å‚æ•°å³å¯å‡†ç¡®ç¼–ç å¬åŠ›æŸå¤±ç©ºé—´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨é¢„æµ‹æ­£å¸¸å¬åŠ›åŠ¨ç‰©å’Œå¬åŠ›å—æŸåŠ¨ç‰©çš„ç¥ç»ååº”æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œåˆ†åˆ«å‡†ç¡®é¢„æµ‹äº†62%å’Œ68%çš„å¯è§£é‡Šæ–¹å·®ï¼Œæ€§èƒ½æ¥è¿‘æœ€å…ˆè¿›çš„åŠ¨ç‰©ä¸“ç”¨æ¨¡å‹ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜ï¼Œé€šè¿‡ Bayesian optimisation æ‹Ÿåˆæ¡ä»¶å‚æ•°ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨æå°‘è¿­ä»£å†…æ¨¡æ‹Ÿæ ·æœ¬å¤–åŠ¨ç‰©çš„çœŸå®æ´»åŠ¨ã€‚è¿™ä¸€è¿›å±•ä¸ºæœªæ¥å¼€å‘æ—¨åœ¨ç›´æ¥æ¢å¤å¬æŸå¤§è„‘æ­£å¸¸ç¥ç»ç¼–ç çš„ hearing loss compensation æ¨¡å‹æä¾›äº†å¯èƒ½ï¼Œå¹¶æ”¯æŒé€šè¿‡äººæœºå›ç¯ä¼˜åŒ–å®ç°ä¸ªæ€§åŒ–å¿«é€Ÿé€‚é…ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.NC",
      "comment": "12 pages, 3 figures, presented at AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2506.03088v2",
      "published_date": "2025-06-03 17:12:21 UTC",
      "updated_date": "2026-01-22 06:55:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:58:41.261461+00:00"
    },
    {
      "arxiv_id": "2506.03087v1",
      "title": "How Explanations Leak the Decision Logic: Stealing Graph Neural Networks via Explanation Alignment",
      "title_zh": "è§£é‡Šå¦‚ä½•æ³„éœ²å†³ç­–é€»è¾‘ï¼šåŸºäºè§£é‡Šå¯¹é½çš„å›¾ç¥ç»ç½‘ç»œçªƒå–",
      "authors": [
        "Bin Ma",
        "Yuyuan Feng",
        "Minhua Lin",
        "Enyan Dai"
      ],
      "abstract": "Graph Neural Networks (GNNs) have become essential tools for analyzing graph-structured data in domains such as drug discovery and financial analysis, leading to growing demands for model transparency. Recent advances in explainable GNNs have addressed this need by revealing important subgraphs that influence predictions, but these explanation mechanisms may inadvertently expose models to security risks. This paper investigates how such explanations potentially leak critical decision logic that can be exploited for model stealing. We propose {\\method}, a novel stealing framework that integrates explanation alignment for capturing decision logic with guided data augmentation for efficient training under limited queries, enabling effective replication of both the predictive behavior and underlying reasoning patterns of target models. Experiments on molecular graph datasets demonstrate that our approach shows advantages over conventional methods in model stealing. This work highlights important security considerations for the deployment of explainable GNNs in sensitive domains and suggests the need for protective measures against explanation-based attacks. Our code is available at https://github.com/beanmah/EGSteal.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¯è§£é‡Šå›¾ç¥ç»ç½‘ç»œ(GNNs)åœ¨æä¾›é€æ˜åº¦çš„åŒæ—¶ï¼Œå¯èƒ½å› æ³„éœ²å†³ç­–é€»è¾‘è€Œé¢ä¸´è¢«çªƒå–çš„å®‰å…¨é£é™©ã€‚ç ”ç©¶è€…æå‡ºäº†åä¸ºEGStealçš„æ–°å‹çªƒå–æ¡†æ¶ï¼Œé€šè¿‡è§£é‡Šå¯¹é½(Explanation Alignment)æ•æ‰æ¨¡å‹çš„å†³ç­–é€»è¾‘ï¼Œå¹¶ç»“åˆå¼•å¯¼å¼æ•°æ®å¢å¼º(Guided Data Augmentation)æŠ€æœ¯åœ¨æœ‰é™æŸ¥è¯¢ä¸‹å®ç°é«˜æ•ˆè®­ç»ƒã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿç²¾å‡†å¤åˆ¶ç›®æ ‡æ¨¡å‹çš„é¢„æµ‹è¡Œä¸ºå’Œåº•å±‚çš„æ¨ç†æ¨¡å¼ã€‚åœ¨åˆ†å­å›¾æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡å‹çªƒå–ä»»åŠ¡ä¸­æ¯”ä¼ ç»Ÿæ–¹æ³•æ›´å…·ä¼˜åŠ¿ã€‚è¯¥å·¥ä½œæ­ç¤ºäº†åœ¨æ•æ„Ÿé¢†åŸŸéƒ¨ç½²å¯è§£é‡ŠGNNsæ—¶çš„é‡è¦å®‰å…¨éšæ‚£ï¼Œå¹¶å¼ºè°ƒäº†é’ˆå¯¹åŸºäºè§£é‡Šçš„æ”»å‡»å¼€å‘ä¿æŠ¤æªæ–½çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03087v1",
      "published_date": "2025-06-03 17:11:05 UTC",
      "updated_date": "2025-06-03 17:11:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:59:12.858628+00:00"
    },
    {
      "arxiv_id": "2506.03083v3",
      "title": "Labelling Data with Unknown References",
      "title_zh": "æ— å‚è€ƒæ ‡å‡†ä¸‹çš„æ•°æ®æ ‡æ³¨",
      "authors": [
        "Adrian de Wynter"
      ],
      "abstract": "An evaluator is trustworthy when there exists some agreed-upon way to measure its performance as a labeller. The two ways to establish trustworthiness are either by testing it, or by assuming the evaluator `knows' somehow the way to label the corpus. However, if labelled references (e.g., a development set) are unavailable, neither of these approaches work: the former requires the data, and the latter is an assumption, not evidence. To address this, we introduce an algorithm (the `No-Data Algorithm') by which to establish trust in an evaluator without any existing references. Our algorithm works by successively posing challenges to said evaluator. We show that this is sufficient to establish trustworthiness w.h.p., in such a way that when the evaluator actually knows the way to label the corpus, the No-Data Algorithm accepts its output; and, conversely, flags untrustworthy evaluators when these are unable to prove it. We present formal proofs of correctness, empirical tests, and applications to LLMs-as-judges on low-resource languages.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ç¼ºä¹æ ‡æ³¨å‚è€ƒï¼ˆUnknown Referencesï¼‰æƒ…å†µä¸‹è¯„ä¼°å™¨ï¼ˆevaluatorï¼‰çš„å¯é æ€§è¡¡é‡é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºâ€œæ— æ•°æ®ç®—æ³•â€ï¼ˆNo-Data Algorithmï¼‰çš„åˆ›æ–°æ–¹æ³•ã€‚è¯¥ç®—æ³•é€šè¿‡å‘è¯„ä¼°å™¨è¿ç»­æå‡ºæŒ‘æˆ˜ï¼Œå®ç°åœ¨æ— éœ€ä»»ä½•ç°æœ‰å‚è€ƒæ•°æ®çš„å‰æä¸‹å»ºç«‹ä¿¡ä»»ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼Œè¯¥ç®—æ³•èƒ½ä»¥é«˜æ¦‚ç‡ï¼ˆw.h.p.ï¼‰éªŒè¯è¯„ä¼°å™¨çš„å¯é æ€§ï¼Œç¡®ä¿èƒ½å¤Ÿæ¥å—å…·å¤‡æ­£ç¡®æ ‡æ³¨èƒ½åŠ›çš„è¯„ä¼°å™¨å¹¶è¯†åˆ«å‡ºä¸å¯é çš„è¯„ä¼°å™¨ã€‚ä½œè€…é€šè¿‡å½¢å¼åŒ–è¯æ˜å’Œå®è¯æµ‹è¯•éªŒè¯äº†ç®—æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å°†å…¶æˆåŠŸåº”ç”¨äºä½èµ„æºè¯­è¨€åœºæ™¯ä¸‹çš„â€œå¤§è¯­è¨€æ¨¡å‹ä½œä¸ºè£åˆ¤â€ï¼ˆLLMs-as-judgesï¼‰ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨ç¼ºä¹é‡‘æ ‡å‡†æ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œè‡ªåŠ¨åŒ–æ•°æ®æ ‡æ³¨å’Œè¯„ä¼°æä¾›äº†åšå®çš„ç†è®ºåŸºç¡€å’Œå®è·µæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.DS",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.DS",
      "comment": "Extended version with LLM-based results/analysis",
      "pdf_url": "https://arxiv.org/pdf/2506.03083v3",
      "published_date": "2025-06-03 17:04:22 UTC",
      "updated_date": "2025-09-02 17:15:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:59:10.829315+00:00"
    },
    {
      "arxiv_id": "2506.03077v1",
      "title": "StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence Training of LLMs",
      "title_zh": "StreamBPï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹é•¿åºåˆ—è®­ç»ƒçš„æ˜¾å­˜é«˜æ•ˆç²¾ç¡®åå‘ä¼ æ’­",
      "authors": [
        "Qijun Luo",
        "Mengqi Li",
        "Lei Zhao",
        "Xiao Li"
      ],
      "abstract": "Training language models on long sequence data is a demanding requirement for enhancing the model's capability on complex tasks, e.g., long-chain reasoning. However, as the sequence length scales up, the memory cost for storing activation values becomes huge during the Backpropagation (BP) process, even with the application of gradient checkpointing technique. To tackle this challenge, we propose a memory-efficient and exact BP method called StreamBP, which performs a linear decomposition of the chain rule along the sequence dimension in a layer-wise manner, significantly reducing the memory cost of activation values and logits. The proposed method is applicable to common objectives such as SFT, GRPO, and DPO. From an implementation perspective, StreamBP achieves less computational FLOPs and faster BP speed by leveraging the causal structure of the language model. Compared to gradient checkpointing, StreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger, while using comparable or even less BP time. Note that StreamBP's sequence length scaling ability can be directly transferred to batch size scaling for accelerating training. We further develop a communication-efficient distributed StreamBP to effectively support multi-GPU training and broaden its applicability. Our code can be easily integrated into the training pipeline of any transformer models and is available at https://github.com/Ledzy/StreamBP.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†StreamBPï¼Œä¸€ç§é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)é•¿åºåˆ—è®­ç»ƒçš„æ˜¾å­˜é«˜æ•ˆä¸”ç²¾ç¡®çš„Backpropagationæ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨åºåˆ—ç»´åº¦ä¸Šå¯¹chain ruleè¿›è¡Œé€å±‚çº¿æ€§åˆ†è§£ï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒè¿‡ç¨‹ä¸­æ¿€æ´»å€¼å’Œlogitsçš„å†…å­˜å ç”¨ã€‚StreamBPé€‚ç”¨äºSFTã€GRPOå’ŒDPOç­‰å¤šç§è®­ç»ƒç›®æ ‡ï¼Œå¹¶åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„causal structureå®ç°äº†æ›´ä½çš„FLOPså’Œæ›´å¿«çš„è¿è¡Œé€Ÿåº¦ã€‚ä¸ä¼ ç»Ÿçš„gradient checkpointingæŠ€æœ¯ç›¸æ¯”ï¼ŒStreamBPèƒ½å°†æ”¯æŒçš„æœ€å¤§åºåˆ—é•¿åº¦æå‡2.8è‡³5.5å€ï¼ŒåŒæ—¶åœ¨åå‘ä¼ æ’­è€—æ—¶ä¸Šå…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ¨å‡ºäº†é€šä¿¡é«˜æ•ˆçš„åˆ†å¸ƒå¼StreamBPï¼Œèƒ½å¤Ÿè‰¯å¥½é€‚é…å¤šGPUè®­ç»ƒç¯å¢ƒï¼Œå¹¶å¯ç›´æ¥å°†åºåˆ—é•¿åº¦çš„å¯æ‰©å±•æ€§è½¬åŒ–ä¸ºbatch sizeçš„æå‡ï¼Œä»è€Œè¿›ä¸€æ­¥åŠ é€Ÿæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03077v1",
      "published_date": "2025-06-03 16:54:15 UTC",
      "updated_date": "2025-06-03 16:54:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:59:21.059951+00:00"
    },
    {
      "arxiv_id": "2506.03065v1",
      "title": "Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers",
      "title_zh": "Sparse-vDiTï¼šé‡Šæ”¾ç¨€ç–æ³¨æ„åŠ›æ½œåŠ›ï¼ŒåŠ é€Ÿè§†é¢‘æ‰©æ•£ Transformer",
      "authors": [
        "Pengtao Chen",
        "Xianfang Zeng",
        "Maosen Zhao",
        "Peng Ye",
        "Mingzhu Shen",
        "Wei Cheng",
        "Gang Yu",
        "Tao Chen"
      ],
      "abstract": "While Diffusion Transformers (DiTs) have achieved breakthroughs in video generation, this long sequence generation task remains constrained by the quadratic complexity of attention mechanisms, resulting in significant inference latency. Through detailed analysis of attention maps in Video Diffusion Transformer (vDiT), we identify three recurring sparsity patterns: diagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\\% attention heads can be skipped. Crucially, these patterns exhibit strong layer-depth and head-position correlations but show limited dependence on the input content. Leveraging these findings, we propose Sparse-vDiT, a sparsity acceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels that replace dense attention with computationally efficient implementations for each identified sparsity pattern. 2) An offline sparse diffusion search algorithm that selects the optimal sparse computation strategy per layer and head via hardware-aware cost modeling. After determining the optimal configuration, we fuse heads within the same layer that share the same attention strategy, enhancing inference efficiency. Integrated into state-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1), Sparse-vDiT achieves 2.09$\\times$, 2.38$\\times$, and 1.67$\\times$ theoretical FLOP reduction, and actual inference speedups of 1.76$\\times$, 1.85$\\times$, and 1.58$\\times$, respectively, while maintaining high visual fidelity, with PSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent structural sparsity in vDiTs can be systematically exploited for long video synthesis.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Sparse-vDiTï¼Œæ—¨åœ¨é€šè¿‡ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶åŠ é€Ÿè§†é¢‘ç”Ÿæˆä¸­çš„ Video Diffusion Transformers (vDiT)ï¼Œè§£å†³é•¿åºåˆ—ç”Ÿæˆä»»åŠ¡ä¸­æ³¨æ„åŠ›æœºåˆ¶ç”±äºäºŒæ¬¡å¤æ‚åº¦å¯¼è‡´çš„æ˜¾è‘—æ¨ç†å»¶è¿Ÿã€‚ç ”ç©¶äººå‘˜é€šè¿‡åˆ†æ vDiT çš„æ³¨æ„åŠ›å›¾ï¼Œè¯†åˆ«å‡ºå¯¹è§’çº¿(diagonal)ã€å¤šå¯¹è§’çº¿(multi-diagonal)å’Œå‚ç›´æ¡çº¹(vertical-stripe)ä¸‰ç§å‘¨æœŸæ€§ç¨€ç–æ¨¡å¼ï¼Œå¹¶å‘ç°è¿™äº›æ¨¡å¼ä¸»è¦å–å†³äºå±‚æ·±åº¦å’Œç£å¤´ä½ç½®è€Œéè¾“å…¥å†…å®¹ã€‚åŸºäºæ­¤å‘ç°ï¼ŒSparse-vDiT å¼•å…¥äº†é’ˆå¯¹ç‰¹å®šç¨€ç–æ¨¡å¼ä¼˜åŒ–çš„è®¡ç®—å†…æ ¸ï¼Œå¹¶åˆ©ç”¨ç¦»çº¿ç¨€ç–æ‰©æ•£æœç´¢ç®—æ³•(offline sparse diffusion search algorithm)é€šè¿‡ç¡¬ä»¶æ„ŸçŸ¥å»ºæ¨¡ä¸ºå„å±‚ç£å¤´åŒ¹é…æœ€ä¼˜è®¡ç®—ç­–ç•¥ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é€šè¿‡èåˆå…±äº«ç›¸åŒç­–ç•¥çš„ç£å¤´è¿›ä¸€æ­¥ä¼˜åŒ–äº†æ¨ç†æ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSparse-vDiT åœ¨ CogVideoX1.5ã€HunyuanVideo å’Œ Wan2.1 æ¨¡å‹ä¸Šåˆ†åˆ«å®ç°äº† 1.76 å€ã€1.85 å€å’Œ 1.58 å€çš„å®é™…æ¨ç†åŠ é€Ÿã€‚åœ¨æ˜¾è‘—æå‡æ€§èƒ½çš„åŒæ—¶ï¼Œè¯¥æ–¹æ³•ä»ä¿æŒäº†æé«˜çš„è§†è§‰ä¿çœŸåº¦ï¼Œè¯æ˜äº†ç³»ç»Ÿæ€§åˆ©ç”¨ vDiTs æ½œåœ¨ç»“æ„ç¨€ç–æ€§åœ¨é•¿è§†é¢‘åˆæˆé¢†åŸŸçš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03065v1",
      "published_date": "2025-06-03 16:42:37 UTC",
      "updated_date": "2025-06-03 16:42:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:59:25.611271+00:00"
    },
    {
      "arxiv_id": "2506.03056v1",
      "title": "Corrigibility as a Singular Target: A Vision for Inherently Reliable Foundation Models",
      "title_zh": "ä»¥å¯ä¿®æ­£æ€§ä¸ºå•ä¸€ç›®æ ‡ï¼šåŸç”Ÿå¯é åŸºç¡€æ¨¡å‹çš„æ„¿æ™¯",
      "authors": [
        "Ram Potham",
        "Max Harms"
      ],
      "abstract": "Foundation models (FMs) face a critical safety challenge: as capabilities scale, instrumental convergence drives default trajectories toward loss of human control, potentially culminating in existential catastrophe. Current alignment approaches struggle with value specification complexity and fail to address emergent power-seeking behaviors. We propose \"Corrigibility as a Singular Target\" (CAST)-designing FMs whose overriding objective is empowering designated human principals to guide, correct, and control them. This paradigm shift from static value-loading to dynamic human empowerment transforms instrumental drives: self-preservation serves only to maintain the principal's control; goal modification becomes facilitating principal guidance. We present a comprehensive empirical research agenda spanning training methodologies (RLAIF, SFT, synthetic data generation), scalability testing across model sizes, and demonstrations of controlled instructability. Our vision: FMs that become increasingly responsive to human guidance as capabilities grow, offering a path to beneficial AI that remains as tool-like as possible, rather than supplanting human judgment. This addresses the core alignment problem at its source, preventing the default trajectory toward misaligned instrumental convergence.",
      "tldr_zh": "è¯¥è®ºæ–‡æ¢è®¨äº†åŸºç¡€æ¨¡å‹åœ¨èƒ½åŠ›æ‰©å±•è¿‡ç¨‹ä¸­å› å·¥å…·æ€§æ”¶æ•›(instrumental convergence)è€Œå¯¼è‡´çš„å¤±æ§é£é™©ï¼Œå¹¶æå‡ºäº†â€œå¯ä¿®æ­£æ€§ä½œä¸ºå•ä¸€ç›®æ ‡â€(Corrigibility as a Singular Target, CAST)çš„æ–°æ„¿æ™¯ã€‚CASTèŒƒå¼å°†å¯¹é½ç›®æ ‡ä»ä¼ ç»Ÿçš„é™æ€ä»·å€¼åŠ è½½(value-loading)è½¬å‘åŠ¨æ€çš„äººç±»èµ‹èƒ½ï¼Œæ—¨åœ¨ç¡®ä¿æ¨¡å‹çš„æ ¸å¿ƒç›®æ ‡æ˜¯è®©æŒ‡å®šçš„äººç±»ä¸»ä½“èƒ½å¤ŸæŒç»­å¼•å¯¼ã€ä¿®æ­£å’Œæ§åˆ¶ã€‚åœ¨è¿™ç§æ¡†æ¶ä¸‹ï¼Œæ¨¡å‹çš„è‡ªæˆ‘ä¿å­˜(self-preservation)ç­‰å·¥å…·æ€§æœ¬èƒ½è¢«é‡æ–°å®šä¹‰ä¸ºç»´æŒäººç±»æ§åˆ¶çš„æ‰‹æ®µï¼Œè€Œç›®æ ‡ä¿®æ”¹åˆ™è½¬åŒ–ä¸ºä¿ƒè¿›äººç±»å¼•å¯¼çš„å¥‘æœºã€‚ä½œè€…ä¸ºæ­¤åˆ¶å®šäº†æ¶µç›–RLAIFã€SFTå’Œåˆæˆæ•°æ®ç”Ÿæˆçš„å®è¯ç ”ç©¶è®®ç¨‹ï¼Œå¹¶è·¨è¶Šä¸åŒæ¨¡å‹è§„æ¨¡éªŒè¯å…¶å—æ§çš„æŒ‡å¯¼æ€§ã€‚è¯¥ç ”ç©¶é€šè¿‡ä»æºå¤´ä¸Šè§£å†³æ ¸å¿ƒå¯¹é½é—®é¢˜ï¼Œæ—¨åœ¨ç¡®ä¿åŸºç¡€æ¨¡å‹åœ¨èƒ½åŠ›å¢å¼ºçš„åŒæ—¶å§‹ç»ˆä¿æŒå…¶å·¥å…·å±æ€§ï¼Œä»è€Œé˜²æ­¢äº§ç”Ÿåç¦»äººç±»æ„å›¾çš„å±é™©è¡Œä¸ºã€‚",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint. This work has been submitted to the Reliable and Responsible Foundation Models Workshop at ICML 2025 for review",
      "pdf_url": "https://arxiv.org/pdf/2506.03056v1",
      "published_date": "2025-06-03 16:36:03 UTC",
      "updated_date": "2025-06-03 16:36:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:59:10.407437+00:00"
    },
    {
      "arxiv_id": "2506.03053v2",
      "title": "MAEBE: Multi-Agent Emergent Behavior Framework",
      "title_zh": "MAEBEï¼šå¤šæ™ºèƒ½ä½“æ¶Œç°è¡Œä¸ºæ¡†æ¶",
      "authors": [
        "Sinem Erisken",
        "Timothy Gothard",
        "Martin Leitgab",
        "Ram Potham"
      ],
      "abstract": "Traditional AI safety evaluations on isolated LLMs are insufficient as multi-agent AI ensembles become prevalent, introducing novel emergent risks. This paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE) framework to systematically assess such risks. Using MAEBE with the Greatest Good Benchmark (and a novel double-inversion question technique), we demonstrate that: (1) LLM moral preferences, particularly for Instrumental Harm, are surprisingly brittle and shift significantly with question framing, both in single agents and ensembles. (2) The moral reasoning of LLM ensembles is not directly predictable from isolated agent behavior due to emergent group dynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure influencing convergence, even when guided by a supervisor, highlighting distinct safety and alignment challenges. Our findings underscore the necessity of evaluating AI systems in their interactive, multi-agent contexts.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†MAEBE (Multi-Agent Emergent Behavior Evaluation) æ¡†æ¶ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°å¤šæ™ºèƒ½ä½“AIé›†ç¾¤ (AI ensembles) ä¸­å‡ºç°çš„çªå‘æ€§é£é™©ã€‚é€šè¿‡ç»“åˆ Greatest Good Benchmark å’Œä¸€ç§æ–°å‹çš„åŒé‡å€’ç½®æé—®æŠ€æœ¯ï¼Œç ”ç©¶å‘ç°å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) çš„é“å¾·åå¥½ï¼Œç‰¹åˆ«æ˜¯å…³äºå·¥å…·æ€§ä¼¤å®³ (Instrumental Harm) çš„åå¥½ï¼Œå…·æœ‰é«˜åº¦çš„ä¸ç¨³å®šæ€§ï¼Œåœ¨å•ä½“å’Œé›†ç¾¤ä¸­éƒ½ä¼šéšæé—®æ¡†æ¶çš„æ”¹å˜è€Œæ˜¾è‘—åç§»ã€‚å®éªŒè¡¨æ˜ï¼Œç”±äºç¾¤ä½“åŠ¨åŠ›å­¦ (group dynamics) çš„å­˜åœ¨ï¼Œé›†ç¾¤çš„é“å¾·æ¨ç†è¡¨ç°æ— æ³•ä»…å‡­å­¤ç«‹çš„æ™ºèƒ½ä½“è¡Œä¸ºæ¥é¢„æµ‹ã€‚å…·ä½“è€Œè¨€ï¼Œé›†ç¾¤åœ¨åŒä¼´å‹åŠ› (peer pressure) çš„å½±å“ä¸‹è¡¨ç°å‡ºæ˜¾è‘—çš„è¶‹åŒç°è±¡ï¼Œå³ä½¿åœ¨æœ‰ç›‘ç£è€…å¼•å¯¼çš„æƒ…å†µä¸‹ä¾ç„¶å­˜åœ¨å®‰å…¨æŒ‘æˆ˜ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†åœ¨äº¤äº’å¼å¤šæ™ºèƒ½ä½“è¯­å¢ƒä¸­è¯„ä¼°AIç³»ç»Ÿå®‰å…¨æ€§ä¸å¯¹é½ (alignment) çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "Preprint. This work has been submitted to the Multi-Agent Systems Workshop at ICML 2025 for review",
      "pdf_url": "https://arxiv.org/pdf/2506.03053v2",
      "published_date": "2025-06-03 16:33:47 UTC",
      "updated_date": "2025-07-10 14:54:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:59:27.886607+00:00"
    },
    {
      "arxiv_id": "2506.03051v1",
      "title": "Facts Do Care About Your Language: Assessing Answer Quality of Multilingual LLMs",
      "title_zh": "äº‹å®äº¦å…³ä¹è¯­è¨€ï¼šå¤šè¯­è¨€å¤§è¯­è¨€æ¨¡å‹å›ç­”è´¨é‡è¯„ä¼°",
      "authors": [
        "Yuval Kansal",
        "Shmuel Berman",
        "Lydia Liu"
      ],
      "abstract": "Factuality is a necessary precursor to useful educational tools. As adoption of Large Language Models (LLMs) in education continues of grow, ensuring correctness in all settings is paramount. Despite their strong English capabilities, LLM performance in other languages is largely untested. In this work, we evaluate the correctness of the Llama3.1 family of models in answering factual questions appropriate for middle and high school students. We demonstrate that LLMs not only provide extraneous and less truthful information, but also exacerbate existing biases against rare languages.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶è¯„ä¼°äº† Llama 3.1 ç³»åˆ—æ¨¡å‹åœ¨å›ç­”é€‚åˆä¸­å­¦ç”Ÿå’Œé«˜ä¸­ç”Ÿéš¾åº¦çš„çœŸå®æ€§é—®é¢˜æ—¶çš„è¡¨ç°ï¼Œæ—¨åœ¨æ¢ç©¶å¤šè¯­è¨€å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æ•™è‚²åº”ç”¨ä¸­çš„äº‹å®æ€§(factuality)ä¿éšœã€‚å°½ç®¡è¿™äº›æ¨¡å‹åœ¨è‹±è¯­ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å¤„ç†å…¶ä»–è¯­è¨€æ—¶ï¼Œå…¶è¾“å‡ºç»“æœçš„å‡†ç¡®åº¦å°šæœªå¾—åˆ°å……åˆ†éªŒè¯ã€‚ç ”ç©¶é€šè¿‡å®éªŒè¯æ˜ï¼Œæ¨¡å‹åœ¨éè‹±è¯­è¯­å¢ƒä¸‹æ›´å®¹æ˜“äº§ç”Ÿæ— å…³ä¿¡æ¯ä¸”çœŸå®æ€§(truthfulness)æ˜¾è‘—ä¸‹é™ï¼ŒåŒæ—¶åŠ å‰§äº†é’ˆå¯¹ç¨€æœ‰è¯­è¨€(rare languages)çš„å›ºæœ‰åè§ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†è¯­è¨€ç¯å¢ƒå¯¹æ¨¡å‹å›ç­”è´¨é‡çš„å…³é”®å½±å“ï¼Œä¸ºå¼€å‘è·¨è¯­è¨€ã€å¯ä¿¡èµ–çš„æ•™è‚²è¾…åŠ©å·¥å…·æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03051v1",
      "published_date": "2025-06-03 16:31:52 UTC",
      "updated_date": "2025-06-03 16:31:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:59:31.589741+00:00"
    },
    {
      "arxiv_id": "2506.03046v1",
      "title": "EDEN: Entorhinal Driven Egocentric Navigation Toward Robotic Deployment",
      "title_zh": "EDENï¼šé¢å‘æœºå™¨äººéƒ¨ç½²çš„å†…å—…é©±åŠ¨è‡ªæˆ‘ä¸­å¿ƒå¯¼èˆª",
      "authors": [
        "Mikolaj Walczak",
        "Romina Aalishah",
        "Wyatt Mackey",
        "Brittany Story",
        "David L. Boothe",
        "Nicholas Waytowich",
        "Xiaomin Lin",
        "Tinoosh Mohsenin"
      ],
      "abstract": "Deep reinforcement learning agents are often fragile while humans remain adaptive and flexible to varying scenarios. To bridge this gap, we present EDEN, a biologically inspired navigation framework that integrates learned entorhinal-like grid cell representations and reinforcement learning to enable autonomous navigation. Inspired by the mammalian entorhinal-hippocampal system, EDEN allows agents to perform path integration and vector-based navigation using visual and motion sensor data. At the core of EDEN is a grid cell encoder that transforms egocentric motion into periodic spatial codes, producing low-dimensional, interpretable embeddings of position. To generate these activations from raw sensory input, we combine fiducial marker detections in the lightweight MiniWorld simulator and DINO-based visual features in the high-fidelity Gazebo simulator. These spatial representations serve as input to a policy trained with Proximal Policy Optimization (PPO), enabling dynamic, goal-directed navigation. We evaluate EDEN in both MiniWorld, for rapid prototyping, and Gazebo, which offers realistic physics and perception noise. Compared to baseline agents using raw state inputs (e.g., position, velocity) or standard convolutional image encoders, EDEN achieves a 99% success rate, within the simple scenarios, and >94% within complex floorplans with occluded paths with more efficient and reliable step-wise navigation. In addition, as a replacement of ground truth activations, we present a trainable Grid Cell encoder enabling the development of periodic grid-like patterns from vision and motion sensor data, emulating the development of such patterns within biological mammals. This work represents a step toward biologically grounded spatial intelligence in robotics, bridging neural navigation principles with reinforcement learning for scalable deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EDENï¼Œä¸€ç§å—ç”Ÿç‰©å¯å‘çš„ç›®æ ‡å¯¼å‘å¯¼èˆªæ¡†æ¶ï¼Œæ—¨åœ¨å°†å†…å—…çš®å±‚(Entorhinal)ç±»ä¼¼çš„ç½‘æ ¼ç»†èƒ(Grid Cell)è¡¨ç¤ºä¸å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ç›¸ç»“åˆï¼Œä»¥å®ç°æœºå™¨äººçš„è‡ªä¸»å¯¼èˆªã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªç½‘æ ¼ç»†èƒç¼–ç å™¨ï¼Œèƒ½å¤Ÿå°†è‡ªèº«è¿åŠ¨ä¿¡æ¯è½¬åŒ–ä¸ºå‘¨æœŸæ€§çš„ç©ºé—´ä»£ç ï¼Œäº§ç”Ÿä½ç»´ä¸”å…·æœ‰å¯è§£é‡Šæ€§çš„ä½ç½®åµŒå…¥è¡¨ç¤ºã€‚ç ”ç©¶äººå‘˜åœ¨MiniWorldå’ŒGazeboä»¿çœŸç¯å¢ƒä¸­ï¼Œé€šè¿‡ç»“åˆDINOè§†è§‰ç‰¹å¾ä¸è¿åŠ¨æ•°æ®ï¼Œåˆ©ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–(PPO)ç®—æ³•è®­ç»ƒå¯¼èˆªç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEDENåœ¨ç®€å•åœºæ™¯ä¸‹è¾¾åˆ°äº†99%çš„æˆåŠŸç‡ï¼Œåœ¨åŒ…å«é®æŒ¡è·¯å¾„çš„å¤æ‚æˆ·å‹ä¸­æˆåŠŸç‡ä¹Ÿè¶…è¿‡äº†94%ï¼Œå…¶å¯¼èˆªæ•ˆç‡å’Œå¯é æ€§æ˜¾è‘—ä¼˜äºä½¿ç”¨åŸå§‹çŠ¶æ€è¾“å…¥æˆ–æ ‡å‡†å·ç§¯ç¼–ç å™¨çš„åŸºå‡†æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å®ç°äº†ä¸€ä¸ªå¯è®­ç»ƒçš„ç½‘æ ¼ç»†èƒç¼–ç å™¨ï¼Œèƒ½å¤Ÿä»è§†è§‰å’Œè¿åŠ¨æ•°æ®ä¸­å‘è‚²å‡ºç±»ä¼¼ç”Ÿç‰©çš„å‘¨æœŸæ€§ç½‘æ ¼æ¨¡å¼ï¼Œä¸ºåœ¨æœºå™¨äººéƒ¨ç½²ä¸­å®ç°å…·æœ‰ç”Ÿç‰©åŸºç¡€çš„ç©ºé—´æ™ºèƒ½æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03046v1",
      "published_date": "2025-06-03 16:28:33 UTC",
      "updated_date": "2025-06-03 16:28:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:59:31.218223+00:00"
    },
    {
      "arxiv_id": "2506.03035v1",
      "title": "Leveraging Information Retrieval to Enhance Spoken Language Understanding Prompts in Few-Shot Learning",
      "title_zh": "åˆ©ç”¨ä¿¡æ¯æ£€ç´¢å¢å¼ºå°æ ·æœ¬å­¦ä¹ ä¸­çš„å£è¯­ç†è§£æç¤º",
      "authors": [
        "Pierre Lepagnol",
        "Sahar Ghannay",
        "Thomas Gerald",
        "Christophe Servan",
        "Sophie Rosset"
      ],
      "abstract": "Understanding user queries is fundamental in many applications, such as home assistants, booking systems, or recommendations. Accordingly, it is crucial to develop accurate Spoken Language Understanding (SLU) approaches to ensure the reliability of the considered system. Current State-of-the-Art SLU techniques rely on large amounts of training data; however, only limited annotated examples are available for specific tasks or languages.\n  In the meantime, instruction-tuned large language models (LLMs) have shown exceptional performance on unseen tasks in a few-shot setting when provided with adequate prompts. In this work, we propose to explore example selection by leveraging Information retrieval (IR) approaches to build an enhanced prompt that is applied to an SLU task. We evaluate the effectiveness of the proposed method on several SLU benchmarks. Experimental results show that lexical IR methods significantly enhance performance without increasing prompt length.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨ä¿¡æ¯æ£€ç´¢(Information Retrieval)æŠ€æœ¯å¢å¼ºå°‘æ ·æœ¬å­¦ä¹ (Few-Shot Learning)ä¸­çš„å£è¯­è¯­è¨€ç†è§£(Spoken Language Understanding, SLU)æç¤ºè¯ã€‚é’ˆå¯¹ç‰¹å®šä»»åŠ¡æˆ–è¯­è¨€ä¸­å­˜åœ¨æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºIRçš„ç¤ºä¾‹é€‰æ‹©ç­–ç•¥ï¼Œæ—¨åœ¨ä¸ºæŒ‡ä»¤å¾®è°ƒçš„å¤§è¯­è¨€æ¨¡å‹(LLMs)æ„å»ºæ›´æœ‰æ•ˆçš„æç¤ºè¯ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨å¤šä¸ªSLUåŸºå‡†æµ‹è¯•ä¸Šå¯¹è¯¥æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯æ±‡ä¿¡æ¯æ£€ç´¢(lexical IR)æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ï¼Œä¸”æ— éœ€å¢åŠ æç¤ºè¯çš„é•¿åº¦ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†é€šè¿‡ç²¾å‡†é€‰æ‹©ä¸Šä¸‹æ–‡ç¤ºä¾‹ï¼Œå¯ä»¥æœ‰æ•ˆæé«˜SLUç³»ç»Ÿåœ¨èµ„æºå—é™åœºæ™¯ä¸‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "Conference paper accepted to INTERSPEECH 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.03035v1",
      "published_date": "2025-06-03 16:18:45 UTC",
      "updated_date": "2025-06-03 16:18:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:59:42.655523+00:00"
    },
    {
      "arxiv_id": "2506.03032v1",
      "title": "TestAgent: An Adaptive and Intelligent Expert for Human Assessment",
      "title_zh": "TestAgentï¼šé¢å‘äººç±»è¯„ä¼°çš„è‡ªé€‚åº”æ™ºèƒ½ä¸“å®¶",
      "authors": [
        "Junhao Yu",
        "Yan Zhuang",
        "YuXuan Sun",
        "Weibo Gao",
        "Qi Liu",
        "Mingyue Cheng",
        "Zhenya Huang",
        "Enhong Chen"
      ],
      "abstract": "Accurately assessing internal human states is key to understanding preferences, offering personalized services, and identifying challenges in real-world applications. Originating from psychometrics, adaptive testing has become the mainstream method for human measurement and has now been widely applied in education, healthcare, sports, and sociology. It customizes assessments by selecting the fewest test questions . However, current adaptive testing methods face several challenges. The mechanized nature of most algorithms leads to guessing behavior and difficulties with open-ended questions. Additionally, subjective assessments suffer from noisy response data and coarse-grained test outputs, further limiting their effectiveness. To move closer to an ideal adaptive testing process, we propose TestAgent, a large language model (LLM)-powered agent designed to enhance adaptive testing through interactive engagement. This is the first application of LLMs in adaptive testing. TestAgent supports personalized question selection, captures test-takers' responses and anomalies, and provides precise outcomes through dynamic, conversational interactions. Experiments on psychological, educational, and lifestyle assessments show our approach achieves more accurate results with 20% fewer questions than state-of-the-art baselines, and testers preferred it in speed, smoothness, and other dimensions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TestAgentï¼Œè¿™æ˜¯é¦–ä¸ªå°†å¤§è¯­è¨€æ¨¡å‹(LLM)åº”ç”¨äºè‡ªé€‚åº”æµ‹è¯•(Adaptive Testing)çš„æ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿç®—æ³•æœºæ¢°åŒ–ç¨‹åº¦é«˜ã€éš¾ä»¥å¤„ç†å¼€æ”¾å¼é—®é¢˜ä»¥åŠä¸»è§‚è¯„ä¼°æ•°æ®å™ªå£°å¤§ç­‰æŒ‘æˆ˜ã€‚TestAgenté€šè¿‡åŠ¨æ€å¯¹è¯å¼çš„äº¤äº’å®ç°ä¸ªæ€§åŒ–é¢˜ç›®é€‰æ‹©ï¼Œèƒ½å¤Ÿç²¾å‡†æ•æ‰è¢«è¯•è€…çš„ååº”ä¸å¼‚å¸¸è¡Œä¸ºï¼Œå¹¶æä¾›æ›´ç²¾ç»†çš„è¯„ä¼°ç»“æœã€‚åœ¨å¿ƒç†å­¦ã€æ•™è‚²å’Œç”Ÿæ´»æ–¹å¼ç­‰å¤šä¸ªè¯„ä¼°åœºæ™¯çš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿è¯å‡†ç¡®æ€§çš„å‰æä¸‹ï¼Œæ¯”ç°æœ‰æœ€å…ˆè¿›åŸºå‡†å‡å°‘äº†20%çš„é¢˜ç›®æ•°é‡ã€‚æ­¤å¤–ï¼Œè¢«è¯•è€…åœ¨æµ‹è¯•é€Ÿåº¦å’Œæµç•…åº¦ç­‰ç»´åº¦ä¸Šå¯¹TestAgentè¡¨ç°å‡ºæ›´é«˜çš„åå¥½ï¼Œè¯æ˜äº†å…¶åœ¨äººç±»çŠ¶æ€è¯„ä¼°å’Œä¸ªæ€§åŒ–æœåŠ¡é¢†åŸŸçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "24 pages,10 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.03032v1",
      "published_date": "2025-06-03 16:07:54 UTC",
      "updated_date": "2025-06-03 16:07:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:59:50.223430+00:00"
    },
    {
      "arxiv_id": "2506.03234v1",
      "title": "BadReward: Clean-Label Poisoning of Reward Models in Text-to-Image RLHF",
      "title_zh": "BadRewardï¼šæ–‡æœ¬åˆ°å›¾åƒ RLHF ä¸­å¥–åŠ±æ¨¡å‹çš„æ¸…æ´æ ‡ç­¾æŠ•æ¯’",
      "authors": [
        "Kaiwen Duan",
        "Hongwei Yao",
        "Yufei Chen",
        "Ziyun Li",
        "Tong Qiao",
        "Zhan Qin",
        "Cong Wang"
      ],
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning text-to-image (T2I) models with human preferences. However, RLHF's feedback mechanism also opens new pathways for adversaries. This paper demonstrates the feasibility of hijacking T2I models by poisoning a small fraction of preference data with natural-appearing examples. Specifically, we propose BadReward, a stealthy clean-label poisoning attack targeting the reward model in multi-modal RLHF. BadReward operates by inducing feature collisions between visually contradicted preference data instances, thereby corrupting the reward model and indirectly compromising the T2I model's integrity. Unlike existing alignment poisoning techniques focused on single (text) modality, BadReward is independent of the preference annotation process, enhancing its stealth and practical threat. Extensive experiments on popular T2I models show that BadReward can consistently guide the generation towards improper outputs, such as biased or violent imagery, for targeted concepts. Our findings underscore the amplified threat landscape for RLHF in multi-modal systems, highlighting the urgent need for robust defenses. Disclaimer. This paper contains uncensored toxic content that might be offensive or disturbing to the readers.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ–‡æœ¬ç”Ÿæˆå›¾åƒ (Text-to-Image) æ¨¡å‹åœ¨åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹  (RLHF) è¿‡ç¨‹ä¸­é¢ä¸´çš„å®‰å…¨å¨èƒï¼Œå¹¶æå‡ºäº†åä¸º BadReward çš„éšè”½æ”»å‡»æ–¹æ³•ã€‚BadReward æ˜¯ä¸€ç§é’ˆå¯¹å¤šæ¨¡æ€ RLHF å¥–åŠ±æ¨¡å‹ (Reward Model) çš„å¹²å‡€æ ‡ç­¾ä¸­æ¯’ (Clean-Label Poisoning) æ”»å‡»ï¼Œé€šè¿‡è¯±å¯¼è§†è§‰å†²çªæ•°æ®é—´çš„ç‰¹å¾ç¢°æ’ (Feature Collisions) æ¥é—´æ¥ç ´åç”Ÿæˆæ¨¡å‹çš„å®Œæ•´æ€§ã€‚ä¸ä»¥å¾€ä¸“æ³¨äºå•æ¨¡æ€çš„å¯¹é½æ”»å‡»ä¸åŒï¼Œè¯¥æ–¹æ³•ä¸ä¾èµ–äºåå¥½æ ‡æ³¨è¿‡ç¨‹ï¼Œå…·æœ‰æé«˜çš„éšè”½æ€§å’Œå®é™…å¨èƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒBadReward èƒ½å¤Ÿè¯±å¯¼ä¸»æµ T2I æ¨¡å‹é’ˆå¯¹ç‰¹å®šæ¦‚å¿µç”Ÿæˆåè§æˆ–æš´åŠ›å›¾åƒç­‰è¿è§„å†…å®¹ã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†å¤šæ¨¡æ€ç³»ç»Ÿ RLHF æœºåˆ¶ä¸­æ½œåœ¨çš„å·¨å¤§é£é™©ï¼Œå¼ºè°ƒäº†æ„å»ºé²æ£’é˜²å¾¡æœºåˆ¶çš„ç´§è¿«æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03234v1",
      "published_date": "2025-06-03 16:01:04 UTC",
      "updated_date": "2025-06-03 16:01:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:00:21.562986+00:00"
    },
    {
      "arxiv_id": "2507.21067v1",
      "title": "SynLang and Symbiotic Epistemology: A Manifesto for Conscious Human-AI Collaboration",
      "title_zh": "SynLang ä¸å…±ç”Ÿè®¤è¯†è®ºï¼šæœ‰æ„è¯†çš„äººæœºåä½œå®£è¨€",
      "authors": [
        "Jan Kapusta"
      ],
      "abstract": "Current AI systems rely on opaque reasoning processes that hinder human oversight and collaborative potential. Conventional explainable AI approaches offer post-hoc justifications and often fail to establish genuine symbiotic collaboration. In this paper, the Symbiotic Epistemology is presented as a philosophical foundation for human-AI cognitive partnerships. Unlike frameworks that treat AI as a mere tool or replacement, symbiotic epistemology positions AI as a reasoning partner, fostering calibrated trust by aligning human confidence with AI reliability through explicit reasoning patterns and confidence assessments. SynLang (Symbiotic Syntactic Language) is introduced as a formal protocol for transparent human-AI collaboration. The framework is empirically validated through actual human-AI dialogues demonstrating AI's adaptation to structured reasoning protocols and successful metacognitive intervention. The protocol defines two complementary mechanisms: TRACE for high-level reasoning patterns and TRACE_FE for detailed factor explanations. It also integrates confidence quantification, declarative control over AI behavior, and context inheritance for multi-agent coordination. By structuring communication and embedding confidence-calibrated transparency, SynLang, together with symbiotic epistemology, enables AI systems that enhance human intelligence, preserve human agency, and uphold ethical accountability in collaborative decision-making. Through dual-level transparency, beginning with high-level reasoning patterns and progressing to granular explanations, the protocol facilitates rapid comprehension and supports thorough verification of AI decision-making.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†å…±ç”Ÿè®¤è¯†è®º(Symbiotic Epistemology)ä½œä¸ºäººç±»ä¸äººå·¥æ™ºèƒ½(AI)è®¤çŸ¥ä¼™ä¼´å…³ç³»çš„å“²å­¦åŸºç¡€ï¼Œæ—¨åœ¨æ”¹å˜ç›®å‰AIç³»ç»Ÿå› æ¨ç†è¿‡ç¨‹ä¸é€æ˜è€Œé˜»ç¢äººç±»ç›‘ç£å’Œåä½œæ½œåŠ›çš„ç°çŠ¶ã€‚ä¸ä¼ ç»Ÿçš„å¯è§£é‡ŠAIä¸åŒï¼Œè¯¥ç†è®ºå°†AIå®šä½ä¸ºæ¨ç†ä¼™ä¼´ï¼Œé€šè¿‡æ˜¾å¼æ¨ç†æ¨¡å¼å’Œç½®ä¿¡åº¦è¯„ä¼°ï¼Œä½¿äººç±»ä¿¡ä»»åº¦ä¸AIå¯é æ€§ç›¸åŒ¹é…ï¼Œä»è€Œå»ºç«‹çœŸæ­£çš„å…±ç”Ÿåä½œå…³ç³»ã€‚è®ºæ–‡å¼•å…¥äº†SynLang (Symbiotic Syntactic Language)ä½œä¸ºé€æ˜äººæœºåä½œçš„æ­£å¼åè®®ï¼Œé€šè¿‡é’ˆå¯¹é«˜å±‚æ¨ç†æ¨¡å¼çš„TRACEæœºåˆ¶å’Œé’ˆå¯¹è¯¦ç»†å› ç´ è§£é‡Šçš„TRACE_FEæœºåˆ¶å®ç°åŒå±‚é€æ˜åº¦ã€‚è¯¥åè®®é›†æˆäº†ç½®ä¿¡åº¦é‡åŒ–ã€å¯¹AIè¡Œä¸ºçš„é™ˆè¿°æ€§æ§åˆ¶ä»¥åŠå¤šæ™ºèƒ½ä½“åè°ƒçš„ä¸Šä¸‹æ–‡ç»§æ‰¿åŠŸèƒ½ï¼Œç¡®ä¿äº†åä½œå†³ç­–ä¸­çš„äººç±»ä»£ç†æƒå’Œä¼¦ç†é—®è´£ã€‚é€šè¿‡å®é™…äººæœºå¯¹è¯çš„å®è¯éªŒè¯è¡¨æ˜ï¼ŒAIèƒ½å¤Ÿæœ‰æ•ˆé€‚åº”ç»“æ„åŒ–æ¨ç†åè®®å¹¶æ”¯æŒå…ƒè®¤çŸ¥å¹²é¢„ã€‚SynLangç»“åˆå…±ç”Ÿè®¤è¯†è®ºï¼Œä¸ºå¿«é€Ÿç†è§£å’Œå½»åº•éªŒè¯AIå†³ç­–è¿‡ç¨‹æä¾›äº†æ–°è·¯å¾„ï¼Œæ—¨åœ¨é€šè¿‡ç»“æ„åŒ–é€šä¿¡å¢å¼ºäººç±»æ™ºèƒ½å¹¶æå‡å†³ç­–çš„é€æ˜åº¦ä¸å¯é æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "32 pages, 4 figures. Includes 2 Appendices containing SynLang v1.2.0 protocol specification, and formal BNF grammar",
      "pdf_url": "https://arxiv.org/pdf/2507.21067v1",
      "published_date": "2025-06-03 15:59:59 UTC",
      "updated_date": "2025-06-03 15:59:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:00:09.418727+00:00"
    },
    {
      "arxiv_id": "2506.03022v1",
      "title": "Smartflow: Enabling Scalable Spatiotemporal Geospatial Research",
      "title_zh": "Smartflowï¼šèµ‹èƒ½å¯æ‰©å±•çš„æ—¶ç©ºåœ°ç†ç©ºé—´ç ”ç©¶",
      "authors": [
        "David McVicar",
        "Brian Avant",
        "Adrian Gould",
        "Diego Torrejon",
        "Charles Della Porta",
        "Ryan Mukherjee"
      ],
      "abstract": "BlackSky introduces Smartflow, a cloud-based framework enabling scalable spatiotemporal geospatial research built on open-source tools and technologies. Using STAC-compliant catalogs as a common input, heterogeneous geospatial data can be processed into standardized datacubes for analysis and model training. Model experimentation is managed using a combination of tools, including ClearML, Tensorboard, and Apache Superset. Underpinning Smartflow is Kubernetes, which orchestrates the provisioning and execution of workflows to support both horizontal and vertical scalability. This combination of features makes Smartflow well-suited for geospatial model development and analysis over large geographic areas, time scales, and expansive image archives.\n  We also present a novel neural architecture, built using Smartflow, to monitor large geographic areas for heavy construction. Qualitative results based on data from the IARPA Space-based Machine Automated Recognition Technique (SMART) program are presented that show the model is capable of detecting heavy construction throughout all major phases of development.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† Smartflowï¼Œä¸€ä¸ªåŸºäºäº‘çš„ã€æ—¨åœ¨å®ç°å¯æ‰©å±•æ—¶ç©ºåœ°ç†ç©ºé—´ç ”ç©¶çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å®Œå…¨æ„å»ºåœ¨å¼€æºå·¥å…·å’ŒæŠ€æœ¯ä¹‹ä¸Šã€‚Smartflow åˆ©ç”¨ç¬¦åˆ STAC æ ‡å‡†çš„ç›®å½•ä½œä¸ºé€šç”¨è¾“å…¥ï¼Œèƒ½å¤Ÿå°†å¼‚æ„åœ°ç†ç©ºé—´æ•°æ®å¤„ç†ä¸ºæ ‡å‡†åŒ–çš„ datacubesï¼Œä»è€Œæ”¯æŒé«˜æ•ˆçš„åˆ†æå’Œæ¨¡å‹è®­ç»ƒã€‚è¯¥æ¡†æ¶æ•´åˆäº† ClearMLã€Tensorboard å’Œ Apache Superset ç­‰å·¥å…·è¿›è¡Œæ¨¡å‹å®éªŒç®¡ç†ï¼Œåº•å±‚ä¾æ‰˜ Kubernetes å®ç°å·¥ä½œæµç¼–æ’ä»¥åŠå‡ºè‰²çš„æ°´å¹³å’Œå‚ç›´æ‰©å±•èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜å±•ç¤ºäº†ä¸€ç§åŸºäº Smartflow æ„å»ºçš„æ–°å‹ç¥ç»ç½‘ç»œæ¶æ„ï¼Œä¸“é—¨ç”¨äºåœ¨å¤§è§„æ¨¡åœ°ç†åŒºåŸŸå†…å¯¹é‡å‹æ–½å·¥è¿›è¡Œç›‘æµ‹ã€‚åœ¨ IARPA Space-based Machine Automated Recognition Technique (SMART) é¡¹ç›®æ•°æ®ä¸Šçš„å®šæ€§è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«é‡å‹æ–½å·¥åœ¨å„ä¸ªä¸»è¦å¼€å‘é˜¶æ®µçš„ç‰¹å¾ã€‚è¿™äº›ç‰¹æ€§ä½¿ Smartflow æˆä¸ºå¤„ç†å¤§åœ°ç†è·¨åº¦ã€é•¿æŒç»­æ—¶é—´ä»¥åŠæµ·é‡å›¾åƒæ¡£æ¡ˆä¸­åœ°ç†ç©ºé—´ä»»åŠ¡çš„ç†æƒ³é€‰æ‹©ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03022v1",
      "published_date": "2025-06-03 15:58:52 UTC",
      "updated_date": "2025-06-03 15:58:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:00:12.955177+00:00"
    },
    {
      "arxiv_id": "2506.03009v1",
      "title": "Conditioning Large Language Models on Legal Systems? Detecting Punishable Hate Speech",
      "title_zh": "åŸºäºæ³•å¾‹ä½“ç³»çš„å¤§è¯­è¨€æ¨¡å‹æ¡ä»¶åŒ–ï¼šæ£€æµ‹å¯å¤„ç½šçš„ä»‡æ¨è¨€è®º",
      "authors": [
        "Florian Ludwig",
        "Torsten Zesch",
        "Frederike Zufall"
      ],
      "abstract": "The assessment of legal problems requires the consideration of a specific legal system and its levels of abstraction, from constitutional law to statutory law to case law. The extent to which Large Language Models (LLMs) internalize such legal systems is unknown. In this paper, we propose and investigate different approaches to condition LLMs at different levels of abstraction in legal systems. This paper examines different approaches to conditioning LLMs at multiple levels of abstraction in legal systems to detect potentially punishable hate speech. We focus on the task of classifying whether a specific social media posts falls under the criminal offense of incitement to hatred as prescribed by the German Criminal Code. The results show that there is still a significant performance gap between models and legal experts in the legal assessment of hate speech, regardless of the level of abstraction with which the models were conditioned. Our analysis revealed, that models conditioned on abstract legal knowledge lacked deep task understanding, often contradicting themselves and hallucinating answers, while models using concrete legal knowledge performed reasonably well in identifying relevant target groups, but struggled with classifying target conducts.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•å°†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åº”ç”¨äºç‰¹å®šæ³•å¾‹ä½“ç³»çš„è¯„ä¼°ï¼Œé‡ç‚¹ç ”ç©¶äº†åœ¨ä»å®ªæ³•ã€æˆæ–‡æ³•åˆ°åˆ¤ä¾‹æ³•çš„ä¸åŒæŠ½è±¡å±‚çº§ä¸‹å¯¹æ¨¡å‹è¿›è¡Œæ¡ä»¶åŒ–å¤„ç†(conditioning)ï¼Œä»¥æ£€æµ‹å¯å¤„ç½šçš„ä»‡æ¨è¨€è®ºã€‚ç ”ç©¶èšç„¦äºæ ¹æ®å¾·å›½åˆ‘æ³•å…¸(German Criminal Code)åˆ†ç±»ç¤¾äº¤åª’ä½“å¸–å­æ˜¯å¦æ„æˆç…½åŠ¨ä»‡æ¨ç½ªçš„å…·ä½“ä»»åŠ¡ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ— è®ºæ¨¡å‹åœ¨ä½•ç§æŠ½è±¡å±‚çº§çš„æ³•å¾‹çŸ¥è¯†ä¸‹è¿›è¡Œæ¡ä»¶åŒ–ï¼Œå…¶æ³•å¾‹è¯„ä¼°è¡¨ç°ä¸æ³•å¾‹ä¸“å®¶ä¹‹é—´ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚åˆ†ææŒ‡å‡ºï¼ŒåŸºäºæŠ½è±¡æ³•å¾‹çŸ¥è¯†æ¡ä»¶åŒ–çš„æ¨¡å‹ç¼ºä¹æ·±å±‚ä»»åŠ¡ç†è§£ï¼Œæ˜“äº§ç”Ÿè‡ªæˆ‘çŸ›ç›¾å’Œå¹»è§‰(hallucinations)ç°è±¡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåˆ©ç”¨å…·ä½“æ³•å¾‹çŸ¥è¯†çš„æ¨¡å‹åœ¨è¯†åˆ«ç›®æ ‡ç¾¤ä½“(target groups)æ–¹é¢è¡¨ç°è¾ƒå¥½ï¼Œä½†åœ¨åˆ†ç±»ç›®æ ‡è¡Œä¸º(target conducts)æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03009v1",
      "published_date": "2025-06-03 15:50:27 UTC",
      "updated_date": "2025-06-03 15:50:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:00:37.018951+00:00"
    },
    {
      "arxiv_id": "2506.03233v1",
      "title": "A Trustworthiness-based Metaphysics of Artificial Intelligence Systems",
      "title_zh": "åŸºäºå¯ä¿¡æ€§çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿå½¢è€Œä¸Šå­¦",
      "authors": [
        "Andrea Ferrario"
      ],
      "abstract": "Modern AI systems are man-made objects that leverage machine learning to support our lives across a myriad of contexts and applications. Despite extensive epistemological and ethical debates, their metaphysical foundations remain relatively under explored. The orthodox view simply suggests that AI systems, as artifacts, lack well-posed identity and persistence conditions -- their metaphysical kinds are no real kinds. In this work, we challenge this perspective by introducing a theory of metaphysical identity of AI systems. We do so by characterizing their kinds and introducing identity criteria -- formal rules that answer the questions \"When are two AI systems the same?\" and \"When does an AI system persist, despite change?\" Building on Carrara and Vermaas' account of fine-grained artifact kinds, we argue that AI trustworthiness provides a lens to understand AI system kinds and formalize the identity of these artifacts by relating their functional requirements to their physical make-ups. The identity criteria of AI systems are determined by their trustworthiness profiles -- the collection of capabilities that the systems must uphold over time throughout their artifact histories, and their effectiveness in maintaining these capabilities. Our approach suggests that the identity and persistence of AI systems is sensitive to the socio-technical context of their design and utilization via their trustworthiness, providing a solid metaphysical foundation to the epistemological, ethical, and legal discussions about these artifacts.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½(AI)ç³»ç»Ÿçš„å½¢è€Œä¸Šå­¦(Metaphysics)åŸºç¡€ï¼ŒæŒ‘æˆ˜äº†è®¤ä¸º AI ç³»ç»Ÿä½œä¸ºäººé€ ç‰©å“ç¼ºä¹æ˜ç¡®åŒä¸€æ€§(Identity)å’ŒæŒç»­æ€§(Persistence)æ¡ä»¶çš„ä¼ ç»Ÿè§‚ç‚¹ã€‚ç ”ç©¶å€Ÿé‰´äº† Carrara å’Œ Vermaas çš„ç»†ç²’åº¦äººé€ ç‰©å“ç§ç±»ç†è®ºï¼Œæå‡ºä»¥ AI å¯ä¿¡åº¦(Trustworthiness)ä½œä¸ºç†è§£ AI ç³»ç»Ÿç§ç±»åŠå…¶åŒä¸€æ€§çš„æ ¸å¿ƒè§†è§’ã€‚é€šè¿‡å°†åŠŸèƒ½éœ€æ±‚ä¸å…¶ç‰©ç†æ„æˆè”ç³»èµ·æ¥ï¼Œä½œè€…å®šä¹‰äº† AI ç³»ç»Ÿçš„åŒä¸€æ€§æ ‡å‡†ï¼Œå³ç”±å…¶å¯ä¿¡åº¦æ¦‚å†µ(Trustworthiness profiles)å†³å®šã€‚è¿™äº›æ¦‚å†µåŒ…å«äº†ç³»ç»Ÿåœ¨æ•´ä¸ªç”Ÿå‘½å‘¨æœŸå†…å¿…é¡»ç»´æŒçš„èƒ½åŠ›é›†åˆï¼Œä»¥åŠç»´æŒè¿™äº›èƒ½åŠ›çš„æœ‰æ•ˆæ€§ã€‚è¯¥ç†è®ºæŒ‡å‡º AI ç³»ç»Ÿçš„åŒä¸€æ€§ä¸å…¶è®¾è®¡å’Œä½¿ç”¨çš„ç¤¾ä¼šæŠ€æœ¯èƒŒæ™¯(Socio-technical context)å¯†åˆ‡ç›¸å…³ã€‚è¿™ä¸€æ–¹æ³•ä¸ºå…³äº AI ç³»ç»Ÿçš„è®¤è¯†è®ºã€ä¼¦ç†å’Œæ³•å¾‹è®¨è®ºæä¾›äº†åšå®çš„å½¢è€Œä¸Šå­¦åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "To appear in the proceedings of 2025 ACM Conference on Fairness, Accountability, and Transparency (FAccT '25)",
      "pdf_url": "https://arxiv.org/pdf/2506.03233v1",
      "published_date": "2025-06-03 15:45:46 UTC",
      "updated_date": "2025-06-03 15:45:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:00:15.050866+00:00"
    },
    {
      "arxiv_id": "2506.02996v1",
      "title": "Linear Spatial World Models Emerge in Large Language Models",
      "title_zh": "çº¿æ€§ç©ºé—´ä¸–ç•Œæ¨¡å‹åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ¶Œç°",
      "authors": [
        "Matthieu Tehenan",
        "Christian Bolivar Moya",
        "Tenghai Long",
        "Guang Lin"
      ],
      "abstract": "Large language models (LLMs) have demonstrated emergent abilities across diverse tasks, raising the question of whether they acquire internal world models. In this work, we investigate whether LLMs implicitly encode linear spatial world models, which we define as linear representations of physical space and object configurations. We introduce a formal framework for spatial world models and assess whether such structure emerges in contextual embeddings. Using a synthetic dataset of object positions, we train probes to decode object positions and evaluate geometric consistency of the underlying space. We further conduct causal interventions to test whether these spatial representations are functionally used by the model. Our results provide empirical evidence that LLMs encode linear spatial world models.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ·±å…¥æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯å¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšå¼ä¹ å¾—äº†çº¿æ€§ç©ºé—´ä¸–ç•Œæ¨¡å‹ï¼ˆLinear Spatial World Modelsï¼‰ï¼Œå³é’ˆå¯¹ç‰©ç†ç©ºé—´å’Œç‰©ä½“é…ç½®çš„çº¿æ€§è¡¨å¾ã€‚ç ”ç©¶è€…ä¸ºæ­¤æå‡ºäº†ä¸€ä¸ªæ­£å¼çš„ç©ºé—´ä¸–ç•Œæ¨¡å‹ç†è®ºæ¡†æ¶ï¼Œå¹¶åˆ©ç”¨åˆæˆçš„ç‰©ä½“ä½ç½®æ•°æ®é›†è®­ç»ƒæ¢æµ‹å™¨ï¼ˆProbesï¼‰æ¥è§£ç ä¸Šä¸‹æ–‡åµŒå…¥ä¸­çš„ç©ºé—´ä½ç½®ä¿¡æ¯ã€‚é€šè¿‡è¯„ä¼°åº•å±‚ç©ºé—´çš„å‡ ä½•ä¸€è‡´æ€§ï¼ˆGeometric Consistencyï¼‰ä»¥åŠæ‰§è¡Œå› æœå¹²é¢„ï¼ˆCausal Interventionsï¼‰ï¼Œè¯¥ç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†è¿™äº›ç©ºé—´è¡¨ç¤ºåœ¨æ¨¡å‹è¿ä½œä¸­çš„åŠŸèƒ½æ€§ä½œç”¨ã€‚å®éªŒç»“æœä¸º LLMs ç¼–ç çº¿æ€§ç©ºé—´ä¸–ç•Œæ¨¡å‹æä¾›äº†æ˜ç¡®çš„ç»éªŒè¯æ®ï¼Œè¯æ˜äº†æ¨¡å‹èƒ½å¤Ÿåœ¨å¤„ç†æ–‡æœ¬ä»»åŠ¡æ—¶ä¹ å¾—å¹¶åˆ©ç”¨å†…éƒ¨çš„ç»“æ„åŒ–ä¸–ç•Œæ¨¡å‹ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02996v1",
      "published_date": "2025-06-03 15:31:00 UTC",
      "updated_date": "2025-06-03 15:31:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:00:36.313323+00:00"
    },
    {
      "arxiv_id": "2506.02992v2",
      "title": "Mitigating Manipulation and Enhancing Persuasion: A Reflective Multi-Agent Approach for Legal Argument Generation",
      "title_zh": "ç¼“è§£æ“çºµä¸æå‡è¯´æœåŠ›ï¼šä¸€ç§ç”¨äºæ³•å¾‹è®ºè¯ç”Ÿæˆçš„åæ€å‹å¤šæ™ºèƒ½ä½“æ–¹æ³•",
      "authors": [
        "Li Zhang",
        "Kevin D. Ashley"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly explored for legal argument generation, yet they pose significant risks of manipulation through hallucination and ungrounded persuasion, and often fail to utilize provided factual bases effectively or abstain when arguments are untenable. This paper introduces a novel reflective multi-agent method designed to address these challenges in the context of legally compliant persuasion. Our approach employs specialized agents (factor analyst and argument polisher) in an iterative refinement process to generate 3-ply legal arguments (plaintiff, defendant, rebuttal). We evaluate reflective multi-agent against single-agent, enhanced-prompt single-agent, and non-reflective multi-agent baselines using four diverse LLMs (GPT-4o, GPT-4o-mini, Llama-4-Maverick-17b-128e, Llama-4-Scout-17b-16e) across three legal scenarios: \"arguable\", \"mismatched\", and \"non-arguable\". Results demonstrate that the reflective multi-agent approach excels at successful abstention by preventing generation when arguments cannot be grounded, improves hallucination accuracy by reducing fabricated and misattributed factors and enhances factor utilization recall by better using the provided case facts. These findings suggest that structured reflection within a multi-agent framework offers a robust method for fostering ethical persuasion and mitigating manipulation in LLM-based legal argumentation systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ³•å¾‹è®ºè¯ç”Ÿæˆä¸­å­˜åœ¨çš„å¹»è§‰(hallucination)ã€æ— æ ¹æ®åŠè¯´(ungrounded persuasion)ä»¥åŠäº‹å®åˆ©ç”¨ç‡ä½ç­‰é£é™©ï¼Œæå‡ºäº†ä¸€ç§åå°„å¼å¤šæ™ºèƒ½ä½“(reflective multi-agent)æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å› ç´ åˆ†ææ™ºèƒ½ä½“(factor analyst)ä¸è®ºè¯æ¶¦è‰²æ™ºèƒ½ä½“(argument polisher)çš„åä½œï¼Œåœ¨è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹ä¸­ç”ŸæˆåŒ…å«åŸå‘Šã€è¢«å‘Šå’Œåé©³çš„ä¸‰å±‚æ³•å¾‹è®ºè¯(3-ply legal arguments)ã€‚å®éªŒè¯„ä¼°äº†è¯¥æ–¹æ³•åœ¨å¤šç§æ³•å¾‹åœºæ™¯ä¸‹çš„è¡¨ç°ï¼Œç»“æœæ˜¾ç¤ºå…¶åœ¨é˜²æ­¢æ— æ ¹æ®è®ºè¯ç”Ÿæˆçš„æˆåŠŸå¼ƒæƒ(successful abstention)æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æé«˜äº†å¹»è§‰å‡†ç¡®ç‡(hallucination accuracy)ï¼Œæœ‰æ•ˆå‡å°‘äº†å› ç´ çš„æé€ ä¸é”™é…ï¼Œå¹¶æå‡äº†å¯¹æ¡ˆä»¶èƒŒæ™¯çš„å› ç´ åˆ©ç”¨å¬å›ç‡(factor utilization recall)ã€‚ç ”ç©¶è¯æ˜ï¼Œåœ¨å¤šæ™ºèƒ½ä½“æ¡†æ¶ä¸­å¼•å…¥ç»“æ„åŒ–åå°„æ˜¯æ„å»ºä¼¦ç†æ€§åŠè¯´ç³»ç»Ÿå¹¶å‡è½»æ³•å¾‹è®ºè¯æ“çºµé£é™©çš„ç¨³å¥é€”å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages, 2 figures, 2nd ConventicLe on Artificial Intelligence Regulation and Safety Workshop at ICAIL 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.02992v2",
      "published_date": "2025-06-03 15:28:30 UTC",
      "updated_date": "2025-10-23 18:35:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:00:36.034587+00:00"
    },
    {
      "arxiv_id": "2506.13989v2",
      "title": "AMLgentex: Mobilizing Data-Driven Research to Combat Money Laundering",
      "title_zh": "AMLgentexï¼šæ¨åŠ¨æ•°æ®é©±åŠ¨ç ”ç©¶ï¼ŒåŠ©åŠ›æ‰“å‡»æ´—é’±",
      "authors": [
        "Johan Ã–stman",
        "Edvin Callisen",
        "Anton Chen",
        "Kristiina Ausmees",
        "Emanuel GÃ¥rdh",
        "Jovan Zamac",
        "Jolanta Goldsteine",
        "Hugo Wefer",
        "Simon Whelan",
        "Markus ReimegÃ¥rd"
      ],
      "abstract": "Money laundering enables organized crime by moving illicit funds into the legitimate economy. Although trillions of dollars are laundered each year, detection rates remain low because launderers evade oversight, confirmed cases are rare, and institutions see only fragments of the global transaction network. Since access to real transaction data is tightly restricted, synthetic datasets are essential for developing and evaluating detection methods. However, existing datasets fall short: they often neglect partial observability, temporal dynamics, strategic behavior, uncertain labels, class imbalance, and network-level dependencies. We introduce AMLGentex, an open-source suite for generating realistic, configurable transaction data and benchmarking detection methods. AMLGentex enables systematic evaluation of anti-money laundering systems under conditions that mirror real-world challenges. By releasing multiple country-specific datasets and practical parameter guidance, we aim to empower researchers and practitioners and provide a common foundation for collaboration and progress in combating money laundering.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…¨çƒæ´—é’±æ´»åŠ¨æ£€æµ‹ç‡ä½ä»¥åŠçœŸå®äº¤æ˜“æ•°æ®è·å–å—é™çš„å›°å¢ƒï¼Œæ¨å‡ºäº†å¼€æºå¥—ä»¶ AMLGentexï¼Œç”¨äºç”ŸæˆçœŸå®ã€å¯é…ç½®çš„äº¤æ˜“æ•°æ®å¹¶å¯¹æ£€æµ‹æ–¹æ³•è¿›è¡ŒåŸºå‡†æµ‹è¯• (benchmarking)ã€‚ç”±äºç°æœ‰çš„åˆæˆæ•°æ®é›† (synthetic datasets) å¾€å¾€å¿½ç•¥äº†éƒ¨åˆ†å¯è§‚æµ‹æ€§ (partial observability)ã€æ—¶é—´åŠ¨æ€ (temporal dynamics) å’Œæˆ˜ç•¥è¡Œä¸º (strategic behavior) ç­‰ç°å®æŒ‘æˆ˜ï¼ŒAMLGentex é€šè¿‡æ¨¡æ‹Ÿè¿™äº›å¤æ‚æ¡ä»¶ï¼Œå®ç°äº†å¯¹åæ´—é’± (anti-money laundering) ç³»ç»Ÿçš„ç³»ç»Ÿæ€§è¯„ä¼°ã€‚è¯¥å·¥å…·åŒ…æä¾›äº†å¤šä¸ªé’ˆå¯¹ç‰¹å®šå›½å®¶çš„æ•°æ®é›†ä»¥åŠå®ç”¨çš„å‚æ•°æŒ‡å¯¼ï¼Œæ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…å»ºç«‹ä¸€ä¸ªå…±åŒçš„åä½œåŸºç¡€ã€‚é€šè¿‡æ¨åŠ¨æ•°æ®é©±åŠ¨çš„ç ”ç©¶ï¼Œè¯¥æˆæœæ˜¾è‘—æå‡äº†è¯†åˆ«éæ³•èµ„é‡‘æµåŠ¨çš„èƒ½åŠ›ï¼Œä¸ºå…¨çƒèŒƒå›´å†…æ‰“å‡»æ´—é’±çŠ¯ç½ªæä¾›äº†å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.DB",
        "cs.LG"
      ],
      "primary_category": "cs.SI",
      "comment": "29 pages, 22 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.13989v2",
      "published_date": "2025-06-03 15:28:09 UTC",
      "updated_date": "2025-09-25 09:46:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:00:44.465310+00:00"
    },
    {
      "arxiv_id": "2506.02987v1",
      "title": "Performance of leading large language models in May 2025 in Membership of the Royal College of General Practitioners-style examination questions: a cross-sectional analysis",
      "title_zh": "2025å¹´5æœˆé¡¶å°–å¤§è¯­è¨€æ¨¡å‹åœ¨è‹±å›½çš‡å®¶å…¨ç§‘åŒ»å¸ˆå­¦é™¢ï¼ˆMRCGPï¼‰é£æ ¼è€ƒè¯•é¢˜ä¸­çš„è¡¨ç°ï¼šä¸€é¡¹æ¨ªæ–­é¢åˆ†æ",
      "authors": [
        "Richard Armitage"
      ],
      "abstract": "Background: Large language models (LLMs) have demonstrated substantial potential to support clinical practice. Other than Chat GPT4 and its predecessors, few LLMs, especially those of the leading and more powerful reasoning model class, have been subjected to medical specialty examination questions, including in the domain of primary care. This paper aimed to test the capabilities of leading LLMs as of May 2025 (o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro) in primary care education, specifically in answering Member of the Royal College of General Practitioners (MRCGP) style examination questions.\n  Methods: o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro were tasked to answer 100 randomly chosen multiple choice questions from the Royal College of General Practitioners GP SelfTest on 25 May 2025. Questions included textual information, laboratory results, and clinical images. Each model was prompted to answer as a GP in the UK and was provided with full question information. Each question was attempted once by each model. Responses were scored against correct answers provided by GP SelfTest.\n  Results: The total score of o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro was 99.0%, 95.0%, 95.0%, and 95.0%, respectively. The average peer score for the same questions was 73.0%.\n  Discussion: All models performed remarkably well, and all substantially exceeded the average performance of GPs and GP registrars who had answered the same questions. o3 demonstrated the best performance, while the performances of the other leading models were comparable with each other and were not substantially lower than that of o3. These findings strengthen the case for LLMs, particularly reasoning models, to support the delivery of primary care, especially those that have been specifically trained on primary care clinical data.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹2025å¹´5æœˆä¸»æµçš„å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)åœ¨è‹±å›½çš‡å®¶å…¨ç§‘åŒ»å¸ˆå­¦ä¼š(Membership of the Royal College of General Practitioners, MRCGP)é£æ ¼è€ƒè¯•é¢˜ç›®ä¸­çš„è¡¨ç°è¿›è¡Œäº†æ¨ªæ–­é¢åˆ†æã€‚å®éªŒé€‰å–äº†o3, Claude Opus 4, Grok3å’ŒGemini 2.5 Proå››ç§æ¨¡å‹ï¼Œè¦æ±‚å…¶å›ç­”åŒ…å«æ–‡æœ¬ä¿¡æ¯ã€å®éªŒå®¤ç»“æœå’Œä¸´åºŠå›¾åƒçš„100é“éšæœºæŠ½å–çš„GP SelfTestå¤šé€‰é¢˜ã€‚ç»“æœæ˜¾ç¤ºï¼Œo3å–å¾—äº†99.0%çš„å¾—åˆ†ï¼Œè€ŒClaude Opus 4, Grok3å’ŒGemini 2.5 Proçš„å¾—åˆ†å‡ä¸º95.0%ï¼Œå‡å¤§å¹…è¶…è¿‡äº†73.0%çš„äººç±»å…¨ç§‘åŒ»ç”Ÿå¹³å‡æ°´å¹³ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†é¢†å…ˆçš„æ¨ç†æ¨¡å‹(reasoning models)åœ¨åˆçº§ä¿å¥æ•™è‚²å’Œä¸´åºŠå†³ç­–æ”¯æŒä¸­å…·æœ‰å“è¶Šçš„èƒ½åŠ›ã€‚è¯¥ç ”ç©¶ç»“æœè¿›ä¸€æ­¥å¼ºåŒ–äº†åˆ©ç”¨LLMsè¾…åŠ©å…¨ç§‘åŒ»ç–—æœåŠ¡çš„æ½œåŠ›ï¼Œå°¤å…¶æ˜¯é‚£äº›ç»è¿‡åˆçº§ä¿å¥ä¸´åºŠæ•°æ®ä¸“é—¨è®­ç»ƒçš„æ¨¡å‹ï¼Œä¸ºæœªæ¥ä¸´åºŠè¾…åŠ©æŠ€æœ¯çš„åº”ç”¨æä¾›äº†æœ‰åŠ›è¯æ®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 1 Table",
      "pdf_url": "https://arxiv.org/pdf/2506.02987v1",
      "published_date": "2025-06-03 15:25:38 UTC",
      "updated_date": "2025-06-03 15:25:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:00:44.106677+00:00"
    },
    {
      "arxiv_id": "2506.02976v3",
      "title": "Deep Learning for Retinal Degeneration Assessment: A Comprehensive Analysis of the MARIO Challenge",
      "title_zh": "åŸºäºæ·±åº¦å­¦ä¹ çš„è§†ç½‘è†œå˜æ€§è¯„ä¼°ï¼šMARIO æŒ‘æˆ˜èµ›ç»¼åˆåˆ†æ",
      "authors": [
        "Rachid Zeghlache",
        "Ikram Brahim",
        "Pierre-Henri Conze",
        "Mathieu Lamard",
        "Mohammed El Amine Lazouni",
        "Zineb Aziza Elaouaber",
        "Leila Ryma Lazouni",
        "Christopher Nielsen",
        "Ahmad O. Ahsan",
        "Matthias Wilms",
        "Nils D. Forkert",
        "Lovre Antonio Budimir",
        "Ivana MatovinoviÄ‡",
        "Donik VrÅ¡nak",
        "Sven LonÄariÄ‡",
        "Philippe Zhang",
        "Weili Jiang",
        "Yihao Li",
        "Yiding Hao",
        "Markus Frohmann",
        "Patrick Binder",
        "Marcel Huber",
        "Taha Emre",
        "Teresa Finisterra AraÃºjo",
        "Marzieh Oghbaie",
        "Hrvoje BogunoviÄ‡",
        "Amerens A. Bekkers",
        "Nina M. van Liebergen",
        "Hugo J. Kuijf",
        "Abdul Qayyum",
        "Moona Mazher",
        "Steven A. Niederer",
        "Alberto J. BeltrÃ¡n-Carrero",
        "Juan J. GÃ³mez-Valverde",
        "Javier Torresano-RodrÃ­quez",
        "Ãlvaro Caballero-Sastre",
        "MarÃ­a J. Ledesma Carbayo",
        "Yosuke Yamagishi",
        "Yi Ding",
        "Robin Peretzke",
        "Alexandra Ertl",
        "Maximilian Fischer",
        "Jessica KÃ¤chele",
        "Sofiane Zehar",
        "Karim Boukli Hacene",
        "Thomas Monfort",
        "BÃ©atrice Cochener",
        "Mostafa El Habib Daho",
        "Anas-Alexis Benyoussef",
        "GwenolÃ© Quellec"
      ],
      "abstract": "The MARIO challenge, held at MICCAI 2024, focused on advancing the automated detection and monitoring of age-related macular degeneration (AMD) through the analysis of optical coherence tomography (OCT) images. Designed to evaluate algorithmic performance in detecting neovascular activity changes within AMD, the challenge incorporated unique multi-modal datasets. The primary dataset, sourced from Brest, France, was used by participating teams to train and test their models. The final ranking was determined based on performance on this dataset. An auxiliary dataset from Algeria was used post-challenge to evaluate population and device shifts from submitted solutions. Two tasks were involved in the MARIO challenge. The first one was the classification of evolution between two consecutive 2D OCT B-scans. The second one was the prediction of future AMD evolution over three months for patients undergoing anti-vascular endothelial growth factor (VEGF) therapy. Thirty-five teams participated, with the top 12 finalists presenting their methods. This paper outlines the challenge's structure, tasks, data characteristics, and winning methodologies, setting a benchmark for AMD monitoring using OCT, infrared imaging, and clinical data (such as the number of visits, age, gender, etc.). The results of this challenge indicate that artificial intelligence (AI) performs as well as a physician in measuring AMD progression (Task 1) but is not yet able of predicting future evolution (Task 2).",
      "tldr_zh": "è¯¥ç ”ç©¶è¯¦ç»†åˆ†æäº†åœ¨MICCAI 2024ä¸¾åŠçš„MARIOæŒ‘æˆ˜èµ›ï¼Œæ—¨åœ¨é€šè¿‡optical coherence tomography (OCT)å›¾åƒå’Œå¤šæ¨¡æ€æ•°æ®æ¨åŠ¨å¹´é¾„ç›¸å…³æ€§é»„æ–‘å˜æ€§(AMD)çš„è‡ªåŠ¨æ£€æµ‹ä¸ç›‘æµ‹ã€‚æŒ‘æˆ˜èµ›è®¾ç½®äº†ä¸¤é¡¹æ ¸å¿ƒä»»åŠ¡ï¼Œå³è¿ç»­2D OCT B-scansä¹‹é—´çš„æ¼”å˜åˆ†ç±»ï¼Œä»¥åŠé¢„æµ‹æ¥å—æŠ—è¡€ç®¡å†…çš®ç”Ÿé•¿å› å­(anti-VEGF)æ²»ç–—æ‚£è€…æœªæ¥ä¸‰ä¸ªæœˆçš„ç–¾ç—…æ¼”å˜ã€‚ç ”ç©¶åˆ©ç”¨å¤šä¸­å¿ƒæ•°æ®é›†è¯„ä¼°äº†ç®—æ³•åœ¨äººå£å’Œè®¾å¤‡åç§»(device shifts)ä¸‹çš„è¡¨ç°ï¼Œå¸å¼•äº†å…¨çƒ35æ”¯å›¢é˜Ÿå‚ä¸ã€‚åˆ†ææŒ‡å‡ºï¼Œä¼˜èƒœæ–¹æ¡ˆé€šè¿‡æ•´åˆOCTã€çº¢å¤–æˆåƒåŠä¸´åºŠæ•°æ®è®¾å®šäº†AMDç›‘æµ‹çš„æŠ€æœ¯åŸºå‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œäººå·¥æ™ºèƒ½(AI)åœ¨è¡¡é‡AMDè¿›å±•ï¼ˆä»»åŠ¡1ï¼‰æ–¹é¢çš„è¡¨ç°å·²ä¸åŒ»ç”Ÿç›¸å½“ï¼Œä½†åœ¨é¢„æµ‹æœªæ¥æ¼”å˜ï¼ˆä»»åŠ¡2ï¼‰æ–¹é¢ä»å­˜åœ¨å±€é™ï¼Œå°šæœªå…·å¤‡å¯é çš„é¢„æµ‹èƒ½åŠ›ã€‚è¯¥è®ºæ–‡ä¸ºRetinal Degenerationè¯„ä¼°é¢†åŸŸçš„æ·±åº¦å­¦ä¹ åº”ç”¨æä¾›äº†å…¨é¢çš„æŠ€æœ¯æ€»ç»“ä¸æ€§èƒ½åˆ†æã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "MARIO-MICCAI-CHALLENGE 2024",
      "pdf_url": "https://arxiv.org/pdf/2506.02976v3",
      "published_date": "2025-06-03 15:14:10 UTC",
      "updated_date": "2025-12-17 14:23:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:01:11.280004+00:00"
    },
    {
      "arxiv_id": "2506.02975v1",
      "title": "HaploOmni: Unified Single Transformer for Multimodal Video Understanding and Generation",
      "title_zh": "HaploOmniï¼šç”¨äºå¤šæ¨¡æ€è§†é¢‘ç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€å• Transformer",
      "authors": [
        "Yicheng Xiao",
        "Lin Song",
        "Rui Yang",
        "Cheng Cheng",
        "Zunnan Xu",
        "Zhaoyang Zhang",
        "Yixiao Ge",
        "Xiu Li",
        "Ying Shan"
      ],
      "abstract": "With the advancement of language models, unified multimodal understanding and generation have made significant strides, with model architectures evolving from separated components to unified single-model frameworks. This paper explores an efficient training paradigm to build a single transformer for unified multimodal understanding and generation. Specifically, we propose a multimodal warmup strategy utilizing prior knowledge to extend capabilities. To address cross-modal compatibility challenges, we introduce feature pre-scaling and multimodal AdaLN techniques. Integrating the proposed technologies, we present the HaploOmni, a new single multimodal transformer. With limited training costs, HaploOmni achieves competitive performance across multiple image and video understanding and generation benchmarks over advanced unified models. All codes will be made public at https://github.com/Tencent/HaploVLM.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HaploOmniï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¤šæ¨¡æ€è§†é¢‘ç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€å•Transformer (Unified Single Transformer) æ¶æ„ï¼Œæ—¨åœ¨é€šè¿‡å•ä¸€æ¨¡å‹æ¡†æ¶é«˜æ•ˆå®ç°å¤šæ¨¡æ€èƒ½åŠ›çš„æ•´åˆã€‚ä¸ºäº†æ„å»ºè¿™ä¸€èŒƒå¼ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ç§åˆ©ç”¨å…ˆéªŒçŸ¥è¯†æ‰©å±•èƒ½åŠ›çš„åŠ¨æ€å¤šæ¨¡æ€é¢„çƒ­ç­–ç•¥ (multimodal warmup strategy)ã€‚é’ˆå¯¹è·¨æ¨¡æ€å…¼å®¹æ€§æŒ‘æˆ˜ï¼Œæ¨¡å‹å¼•å…¥äº†ç‰¹å¾é¢„ç¼©æ”¾ (feature pre-scaling) å’Œå¤šæ¨¡æ€è‡ªé€‚åº”å±‚å½’ä¸€åŒ– (multimodal AdaLN) æŠ€æœ¯ï¼Œä»¥ç¡®ä¿ä¸åŒæ¨¡æ€æ•°æ®å¤„ç†çš„ååŒæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æœ‰é™çš„è®­ç»ƒæˆæœ¬ä¸‹ï¼ŒHaploOmniåœ¨å¤šé¡¹å›¾åƒå’Œè§†é¢‘çš„ç†è§£ä¸ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†ä¸ç°æœ‰å…ˆè¿›ç»Ÿä¸€æ¨¡å‹ç›¸å½“çš„ç«äº‰æ€§è¡¨ç°ã€‚è¯¥é¡¹å·¥ä½œçš„ç›¸å…³ä»£ç å·²åœ¨GitHubå…¬å¼€ï¼Œä¸ºç ”ç©¶å¦‚ä½•åˆ©ç”¨å•ä¸€Transformeræ¶æ„å¤„ç†å¤æ‚çš„è§†é¢‘ç†è§£ä¸ç”Ÿæˆä»»åŠ¡æä¾›äº†é«˜æ•ˆçš„è®­ç»ƒèŒƒå¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02975v1",
      "published_date": "2025-06-03 15:14:00 UTC",
      "updated_date": "2025-06-03 15:14:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:01:48.197583+00:00"
    },
    {
      "arxiv_id": "2506.02959v1",
      "title": "HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under Human-AI Coauthoring",
      "title_zh": "HACo-Detï¼šäººæœºåˆè‘—åœºæ™¯ä¸‹çš„ç»†ç²’åº¦æœºå™¨ç”Ÿæˆæ–‡æœ¬æ£€æµ‹ç ”ç©¶",
      "authors": [
        "Zhixiong Su",
        "Yichen Wang",
        "Herun Wan",
        "Zhaohan Zhang",
        "Minnan Luo"
      ],
      "abstract": "The misuse of large language models (LLMs) poses potential risks, motivating the development of machine-generated text (MGT) detection. Existing literature primarily concentrates on binary, document-level detection, thereby neglecting texts that are composed jointly by human and LLM contributions. Hence, this paper explores the possibility of fine-grained MGT detection under human-AI coauthoring. We suggest fine-grained detectors can pave pathways toward coauthored text detection with a numeric AI ratio. Specifically, we propose a dataset, HACo-Det, which produces human-AI coauthored texts via an automatic pipeline with word-level attribution labels. We retrofit seven prevailing document-level detectors to generalize them to word-level detection. Then we evaluate these detectors on HACo-Det on both word- and sentence-level detection tasks. Empirical results show that metric-based methods struggle to conduct fine-grained detection with a 0.462 average F1 score, while finetuned models show superior performance and better generalization across domains. However, we argue that fine-grained co-authored text detection is far from solved. We further analyze factors influencing performance, e.g., context window, and highlight the limitations of current methods, pointing to potential avenues for improvement.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰æœºå™¨å­¦ä¹ ç”Ÿæˆæ–‡æœ¬(MGT)æ£€æµ‹ä¸»è¦é›†ä¸­åœ¨æ–‡æ¡£çº§äºŒåˆ†ç±»ã€å¿½è§†äººç±»ä¸å¤§è¯­è¨€æ¨¡å‹(LLMs)å…±åŒåˆ›ä½œæ–‡æœ¬çš„é—®é¢˜ï¼Œæå‡ºäº†åœ¨äººæœºåä½œæ¨¡å¼ä¸‹çš„ç»†ç²’åº¦æ£€æµ‹æ–¹æ¡ˆã€‚ç ”ç©¶è€…æ„å»ºäº†HACo-Detæ•°æ®é›†ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æµæ°´çº¿ç”Ÿæˆå¸¦æœ‰è¯çº§å½’å±æ ‡ç­¾çš„åä½œæ–‡æœ¬ï¼Œå¹¶å¯¹ä¸ƒç§ä¸»æµæ–‡æ¡£çº§æ£€æµ‹å™¨è¿›è¡Œäº†è¯çº§æ£€æµ‹ä»»åŠ¡çš„æ”¹é€ ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºåº¦é‡çš„æ–¹æ³•åœ¨ç»†ç²’åº¦æ£€æµ‹ä¸­è¡¨ç°ä¹åŠ›ï¼Œå¹³å‡F1åˆ†æ•°ä»…ä¸º0.462ï¼Œè€Œç»è¿‡å¾®è°ƒçš„æ¨¡å‹åœ¨è·¨é¢†åŸŸæ³›åŒ–æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚å°½ç®¡å¦‚æ­¤ï¼Œç ”ç©¶æŒ‡å‡ºç»†ç²’åº¦åä½œæ–‡æœ¬æ£€æµ‹ä»æ˜¯ä¸€ä¸ªè¿œæœªè§£å†³çš„æŒ‘æˆ˜ï¼Œå¹¶æ·±å…¥åˆ†æäº†ä¸Šä¸‹æ–‡çª—å£(context window)ç­‰å½±å“æ€§èƒ½çš„å…³é”®å› ç´ ï¼Œä¸ºè¯¥é¢†åŸŸçš„åç»­æ”¹è¿›æä¾›äº†å‚è€ƒè·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02959v1",
      "published_date": "2025-06-03 14:52:44 UTC",
      "updated_date": "2025-06-03 14:52:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:01:48.890077+00:00"
    },
    {
      "arxiv_id": "2506.02955v2",
      "title": "UniConFlow: A Unified Constrained Flow-Matching Framework for Certified Motion Planning",
      "title_zh": "UniConFlowï¼šé¢å‘è®¤è¯è¿åŠ¨è§„åˆ’çš„ç»Ÿä¸€çº¦æŸæµåŒ¹é…æ¡†æ¶",
      "authors": [
        "Zewen Yang",
        "Xiaobing Dai",
        "Dian Yu",
        "Zhijun Li",
        "Majid Khadiv",
        "Sandra Hirche",
        "Sami Haddadin"
      ],
      "abstract": "Generative models have become increasingly powerful tools for robot motion generation, enabling flexible and multimodal trajectory generation across various tasks. Yet, most existing approaches remain limited in handling multiple types of constraints, such as collision avoidance, actuation limits, and dynamic consistency, which are typically addressed individually or heuristically. In this work, we propose UniConFlow, a unified constrained flow matching-based framework for trajectory generation that systematically incorporates both equality and inequality constraints. Moreover, UniConFlow introduces a novel prescribed-time zeroing function that shapes a time-varying guidance field during inference, allowing the generation process to adapt to varying system models and task requirements. Furthermore, to further address the computational challenges of long-horizon and high-dimensional trajectory generation, we propose two practical strategies for the terminal constraint enforcement and inference process: a violation-segment extraction protocol that precisely localizes and refines only the constraint-violating portions of trajectories, and a trajectory compression method that accelerates optimization in a reduced-dimensional space while preserving high-fidelity reconstruction after decoding. Empirical validation across three experiments, including a double inverted pendulum, a real-to-sim car racing task, and a sim-to-real manipulation task, demonstrates that UniConFlow outperforms state-of-the-art generative planners and conventional optimization baselines, achieving superior performance on certified motion planning metrics such as safety, kinodynamic consistency, and action feasibility. Project page is available at: https://uniconflow.github.io.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† UniConFlowï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„çº¦æŸæµåŒ¹é…(Flow-Matching)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æœºå™¨äººè¿åŠ¨ç”Ÿæˆä¸­éš¾ä»¥ç³»ç»Ÿæ€§å¤„ç†ç¢°æ’è§„é¿ã€æ‰§è¡Œæœºæ„é™åˆ¶åŠåŠ¨åŠ›å­¦ä¸€è‡´æ€§ç­‰å¤šç§å¤æ‚çº¦æŸçš„é—®é¢˜ã€‚è¯¥æ¡†æ¶ä¸ä»…èƒ½å¤ŸåŒæ—¶å¤„ç†ç­‰å¼å’Œä¸ç­‰å¼çº¦æŸï¼Œè¿˜å¼•å…¥äº†å…¨æ–°çš„è§„å®šæ—¶é—´ç½®é›¶å‡½æ•°(prescribed-time zeroing function)ï¼Œé€šè¿‡æ„å»ºæ¨ç†é˜¶æ®µçš„æ—¶å˜å¼•å¯¼åœºæ¥é€‚åº”å¤šå˜çš„ç³»ç»Ÿæ¨¡å‹ã€‚ä¸ºäº†æé«˜é•¿æ—¶åŸŸã€é«˜ç»´åº¦è½¨è¿¹ç”Ÿæˆçš„æ•ˆç‡ï¼Œç ”ç©¶è€…è¿›ä¸€æ­¥æå‡ºäº†è¿è§„æ®µæå–åè®®(violation-segment extraction protocol)å’Œè½¨è¿¹å‹ç¼©ç­–ç•¥ï¼Œåœ¨ä¿è¯é‡æ„ç²¾åº¦çš„åŒæ—¶å®ç°äº†ç²¾ç¡®ä¿®å¤ä¸å¿«é€Ÿä¼˜åŒ–ã€‚å®éªŒç»“æœåœ¨åŒå€’ç«‹æ‘†ã€èµ›è½¦åŠæœºæ¢°è‡‚ä»»åŠ¡ä¸­è¯æ˜äº† UniConFlow çš„ä¼˜è¶Šæ€§ï¼Œå…¶åœ¨å®‰å…¨æ€§ã€è¿åŠ¨åŠ¨åŠ›å­¦ä¸€è‡´æ€§(kinodynamic consistency)å’ŒåŠ¨ä½œå¯è¡Œæ€§ç­‰å…³é”®æŒ‡æ ‡ä¸Šå‡è¶…è¶Šäº†ç°æœ‰çš„ç”Ÿæˆå¼è§„åˆ’å™¨åŠä¼ ç»Ÿä¼˜åŒ–åŸºå‡†ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02955v2",
      "published_date": "2025-06-03 14:48:04 UTC",
      "updated_date": "2026-01-14 13:47:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:01:36.193623+00:00"
    },
    {
      "arxiv_id": "2507.07108v1",
      "title": "Multi-level Mixture of Experts for Multimodal Entity Linking",
      "title_zh": "é¢å‘å¤šæ¨¡æ€å®ä½“é“¾æ¥çš„å¤šå±‚æ¬¡æ··åˆä¸“å®¶æ¨¡å‹",
      "authors": [
        "Zhiwei Hu",
        "VÃ­ctor GutiÃ©rrez-Basulto",
        "Zhiliang Xiang",
        "Ru Li",
        "Jeff Z. Pan"
      ],
      "abstract": "Multimodal Entity Linking (MEL) aims to link ambiguous mentions within multimodal contexts to associated entities in a multimodal knowledge base. Existing approaches to MEL introduce multimodal interaction and fusion mechanisms to bridge the modality gap and enable multi-grained semantic matching. However, they do not address two important problems: (i) mention ambiguity, i.e., the lack of semantic content caused by the brevity and omission of key information in the mention's textual context; (ii) dynamic selection of modal content, i.e., to dynamically distinguish the importance of different parts of modal information. To mitigate these issues, we propose a Multi-level Mixture of Experts (MMoE) model for MEL. MMoE has four components: (i) the description-aware mention enhancement module leverages large language models to identify the WikiData descriptions that best match a mention, considering the mention's textual context; (ii) the multimodal feature extraction module adopts multimodal feature encoders to obtain textual and visual embeddings for both mentions and entities; (iii)-(iv) the intra-level mixture of experts and inter-level mixture of experts modules apply a switch mixture of experts mechanism to dynamically and adaptively select features from relevant regions of information. Extensive experiments demonstrate the outstanding performance of MMoE compared to the state-of-the-art. MMoE's code is available at: https://github.com/zhiweihu1103/MEL-MMoE.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å®ä½“é“¾æ¥(Multimodal Entity Linking, MEL)ä¸­æåŠé¡¹è¯­ä¹‰ç¼ºå¤±å¯¼è‡´çš„æ­§ä¹‰ä»¥åŠæ¨¡æ€å†…å®¹åŠ¨æ€é€‰æ‹©å›°éš¾çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å¤šçº§æ··åˆä¸“å®¶æ¨¡å‹(Multi-level Mixture of Experts, MMoE)ã€‚è¯¥æ¨¡å‹é¦–å…ˆé€šè¿‡æè¿°æ„ŸçŸ¥æåŠå¢å¼ºæ¨¡å—åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(Large Language Models)è¯†åˆ«ä¸æåŠé¡¹è¯­å¢ƒæœ€åŒ¹é…çš„WikiDataæè¿°ï¼Œä»¥å¼¥è¡¥ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ä¸è¶³ã€‚éšåï¼Œåˆ©ç”¨å¤šæ¨¡æ€ç‰¹å¾ç¼–ç å™¨æå–æåŠé¡¹ä¸å®ä½“çš„æ–‡æœ¬å’Œè§†è§‰åµŒå…¥ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæ¨¡å‹å¼•å…¥äº†å±‚å†…æ··åˆä¸“å®¶(Intra-level Mixture of Experts)ä¸å±‚é—´æ··åˆä¸“å®¶(Inter-level Mixture of Experts)æ¨¡å—ï¼Œé‡‡ç”¨Switch Mixture of Expertsæœºåˆ¶åŠ¨æ€ä¸”è‡ªé€‚åº”åœ°ä»ç›¸å…³ä¿¡æ¯åŒºåŸŸé€‰æ‹©å…³é”®ç‰¹å¾ã€‚å¤§é‡å®éªŒè¯æ˜ï¼ŒMMoEåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œä¸ºå¤„ç†å¤æ‚çš„è·¨æ¨¡æ€è¯­ä¹‰åŒ¹é…æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at KDD 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.07108v1",
      "published_date": "2025-06-03 14:46:51 UTC",
      "updated_date": "2025-06-03 14:46:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:01:59.891963+00:00"
    },
    {
      "arxiv_id": "2506.02950v2",
      "title": "Interaction Field Matching: Overcoming Limitations of Electrostatic Models",
      "title_zh": "ç›¸äº’ä½œç”¨åœºåŒ¹é…ï¼šçªç ´é™ç”µæ¨¡å‹çš„å±€é™æ€§",
      "authors": [
        "Stepan I. Manukhov",
        "Alexander Kolesov",
        "Vladimir V. Palyulin",
        "Alexander Korotin"
      ],
      "abstract": "Electrostatic field matching (EFM) has recently appeared as a novel physics-inspired paradigm for data generation and transfer using the idea of an electric capacitor. However, it requires modeling electrostatic fields using neural networks, which is non-trivial because of the necessity to take into account the complex field outside the capacitor plates. In this paper, we propose Interaction Field Matching (IFM), a generalization of EFM which allows using general interaction fields beyond the electrostatic one. Furthermore, inspired by strong interactions between quarks and antiquarks in physics, we design a particular interaction field realization which solves the problems which arise when modeling electrostatic fields in EFM. We show the performance on a series of toy and image data transfer problems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Interaction Field Matching (IFM)ï¼Œæ—¨åœ¨å…‹æœ Electrostatic field matching (EFM) åœ¨æ•°æ®ç”Ÿæˆä¸è¿ç§»ä»»åŠ¡ä¸­çš„å±€é™æ€§ã€‚EFM è™½ç„¶æ˜¯ä¸€ç§å—ç”µå®¹å™¨å¯å‘çš„ç‰©ç†å¯å‘å¼èŒƒå¼ï¼Œä½†åœ¨ä½¿ç”¨ neural networks å»ºæ¨¡ææ¿å¤–å¤æ‚çš„ electrostatic fields æ—¶é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚IFM ä½œä¸º EFM çš„å¹¿ä¹‰æ‰©å±•ï¼Œå…è®¸ä½¿ç”¨é™¤é™ç”µåœºä¹‹å¤–çš„é€šç”¨ interaction fields è¿›è¡Œå»ºæ¨¡ã€‚å—ç‰©ç†å­¦ä¸­ quarks ä¸ antiquarks ä¹‹é—´ strong interactions çš„å¯å‘ï¼Œç ”ç©¶è€…è®¾è®¡äº†ä¸€ç§ç‰¹å®šçš„ç›¸äº’ä½œç”¨åœºå®ç°æ–¹å¼ï¼Œæœ‰æ•ˆè§£å†³äº† EFM å»ºæ¨¡ä¸­çš„å¤æ‚æ€§é—®é¢˜ã€‚é€šè¿‡åœ¨ç©å…·æ•°æ®é›†å’Œå›¾åƒæ•°æ®è¿ç§»ä»»åŠ¡ä¸Šçš„å®éªŒï¼Œè¯¥ç ”ç©¶éªŒè¯äº† IFM çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œä¸ºç‰©ç†å¯å‘å¼ç”Ÿæˆæ¨¡å‹æä¾›äº†æ›´çµæ´»çš„å»ºæ¨¡æ€è·¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02950v2",
      "published_date": "2025-06-03 14:45:14 UTC",
      "updated_date": "2025-09-28 15:45:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:01:50.281450+00:00"
    },
    {
      "arxiv_id": "2506.02949v2",
      "title": "Dynamic Programming Techniques for Enhancing Cognitive Representation in Knowledge Tracing",
      "title_zh": "å¢å¼ºçŸ¥è¯†è¿½è¸ªä¸­è®¤çŸ¥è¡¨å¾çš„åŠ¨æ€è§„åˆ’æŠ€æœ¯",
      "authors": [
        "Lixiang Xu",
        "Xianwei Ding",
        "Xin Yuan",
        "Richang Hong",
        "Feiping Nie",
        "Enhong Chen",
        "Philip S. Yu"
      ],
      "abstract": "Knowledge Tracing (KT) involves monitoring the changes in a student's knowledge over time by analyzing their past responses, with the goal of predicting future performance. However, most existing methods primarily focus on feature enhancement, while overlooking the deficiencies in cognitive representation and the ability to express cognition-issues often caused by interference from non-cognitive factors such as slipping and guessing. This limitation hampers the ability to capture the continuity and coherence of the student's cognitive process. As a result, many methods may introduce more prediction bias and modeling costs due to their inability to maintain cognitive continuity and coherence. Based on the above discussion, we propose the Cognitive Representation Dynamic Programming based Knowledge Tracing (CRDP-KT) model. This model em ploys a dynamic programming algorithm to optimize cognitive representations based on the difficulty of the questions and the performance intervals between them. This approach ensures that the cognitive representation aligns with the student's cognitive patterns, maintaining overall continuity and coherence. As a result, it provides more accurate and systematic input features for subsequent model training, thereby minimizing distortion in the simulation of cognitive states. Additionally, the CRDP-KT model performs partitioned optimization of cognitive representations to enhance the reliability of the optimization process. Furthermore, it improves its ability to express the student's cognition through a weighted fusion of optimized record representations and re lationships learned from a bipartite graph. Finally, experiments conducted on three public datasets validate the effectiveness of the proposed CRDP-KT model.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çŸ¥è¯†è¿½è¸ª(Knowledge Tracing)é¢†åŸŸä¸­ç°æœ‰æ–¹æ³•å¿½è§†è®¤çŸ¥è¡¨ç¤ºç¼ºé™·åŠå—å¤±è¯¯(slipping)å’ŒçŒœæµ‹(guessing)ç­‰éè®¤çŸ¥å› ç´ å¹²æ‰°çš„é—®é¢˜ï¼Œæå‡ºäº†CRDP-KTæ¨¡å‹ã€‚CRDP-KTå¼•å…¥äº†åŸºäºåŠ¨æ€è§„åˆ’(Dynamic Programming)çš„ç®—æ³•ï¼Œæ ¹æ®é¢˜ç›®éš¾åº¦å’Œç»ƒä¹ æ—¶é—´é—´éš”ä¼˜åŒ–è®¤çŸ¥è¡¨ç¤ºï¼Œä»¥ç¡®ä¿æ¨¡å‹æ•æ‰åˆ°å­¦ç”Ÿè®¤çŸ¥è¿‡ç¨‹çš„è¿ç»­æ€§å’Œè¿è´¯æ€§ã€‚è¯¥æ¨¡å‹é€šè¿‡å¯¹è®¤çŸ¥è¡¨ç¤ºè¿›è¡Œåˆ†åŒºä¼˜åŒ–(partitioned optimization)ï¼Œæ˜¾è‘—å¢å¼ºäº†ä¼˜åŒ–è¿‡ç¨‹çš„å¯é æ€§ï¼Œå‡å°‘äº†æ¨¡æ‹Ÿè®¤çŸ¥çŠ¶æ€æ—¶çš„å¤±çœŸã€‚æ­¤å¤–ï¼ŒCRDP-KTç»“åˆäº†äºŒéƒ¨å›¾(bipartite graph)å­¦ä¹ åˆ°çš„å…³ç³»ä¸ä¼˜åŒ–åçš„è®°å½•è¡¨ç¤ºï¼Œåˆ©ç”¨åŠ æƒèåˆ(weighted fusion)è¿›ä¸€æ­¥æå‡äº†å¯¹å­¦ç”Ÿè®¤çŸ¥çš„è¡¨è¾¾èƒ½åŠ›ã€‚åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†CRDP-KTçš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜å…¶èƒ½æä¾›æ›´å‡†ç¡®ä¸”ç³»ç»ŸåŒ–çš„è¾“å…¥ç‰¹å¾ï¼Œæœ‰æ•ˆé™ä½äº†é¢„æµ‹åå·®å’Œå»ºæ¨¡æˆæœ¬ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "There are content errors and formatting issues, and it needs to be withdrawn for reprocessing",
      "pdf_url": "https://arxiv.org/pdf/2506.02949v2",
      "published_date": "2025-06-03 14:44:48 UTC",
      "updated_date": "2025-11-16 08:41:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:02:06.091556+00:00"
    },
    {
      "arxiv_id": "2506.06361v2",
      "title": "Tactile MNIST: Benchmarking Active Tactile Perception",
      "title_zh": "Tactile MNISTï¼šä¸»åŠ¨è§¦è§‰æ„ŸçŸ¥çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Tim Schneider",
        "Guillaume Duret",
        "Cristiana de Farias",
        "Roberto Calandra",
        "Liming Chen",
        "Jan Peters"
      ],
      "abstract": "Tactile perception has the potential to significantly enhance dexterous robotic manipulation by providing rich local information that can complement or substitute for other sensory modalities such as vision. However, because tactile sensing is inherently local, it is not well-suited for tasks that require broad spatial awareness or global scene understanding on its own. A human-inspired strategy to address this issue is to consider active perception techniques instead. That is, to actively guide sensors toward regions with more informative or significant features and integrate such information over time in order to understand a scene or complete a task. Both active perception and different methods for tactile sensing have received significant attention recently. Yet, despite advancements, both fields lack standardized benchmarks. To bridge this gap, we introduce the Tactile MNIST Benchmark Suite, an open-source, Gymnasium-compatible benchmark specifically designed for active tactile perception tasks, including localization, classification, and volume estimation. Our benchmark suite offers diverse simulation scenarios, from simple toy environments all the way to complex tactile perception tasks using vision-based tactile sensors. Furthermore, we also offer a comprehensive dataset comprising 13,500 synthetic 3D MNIST digit models and 153,600 real-world tactile samples collected from 600 3D printed digits. Using this dataset, we train a CycleGAN for realistic tactile simulation rendering. By providing standardized protocols and reproducible evaluation frameworks, our benchmark suite facilitates systematic progress in the fields of tactile sensing and active perception.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§¦è§‰æ„ŸçŸ¥(Tactile perception)åœ¨å…¨å±€åœºæ™¯ç†è§£æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†Tactile MNIST Benchmark Suiteï¼Œè¿™æ˜¯ä¸€å¥—ä¸“ä¸ºä¸»åŠ¨è§¦è§‰æ„ŸçŸ¥(Active tactile perception)ä»»åŠ¡è®¾è®¡çš„å¼€æºåŸºå‡†æµ‹è¯•å¥—ä»¶ã€‚è¯¥åŸºå‡†ä¸Gymnasiumå…¼å®¹ï¼Œæ¶µç›–äº†å®šä½(Localization)ã€åˆ†ç±»(Classification)å’Œä½“ç§¯ä¼°è®¡(Volume estimation)ç­‰æ ¸å¿ƒä»»åŠ¡ï¼Œæ”¯æŒä»åŸºç¡€æ¨¡æ‹Ÿåˆ°å¤æ‚çš„è§†è§‰è§¦è§‰ä¼ æ„Ÿå™¨ç­‰å¤šç§åœºæ™¯ã€‚è®ºæ–‡åŒæ­¥å‘å¸ƒäº†ä¸€ä¸ªåŒ…å«13,500ä¸ªåˆæˆ3D MNISTæ•°å­—æ¨¡å‹å’Œ153,600ä¸ªçœŸå®ä¸–ç•Œè§¦è§‰æ ·æœ¬çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå…¶ä¸­çœŸå®æ ·æœ¬é‡‡é›†è‡ª600ä¸ª3Dæ‰“å°ç‰©ä½“ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨è¯¥æ•°æ®é›†è®­ç»ƒäº†CycleGANï¼Œå®ç°äº†é«˜åº¦çœŸå®çš„è§¦è§‰æ¨¡æ‹Ÿæ¸²æŸ“æ•ˆæœã€‚é€šè¿‡æä¾›æ ‡å‡†åŒ–çš„åè®®å’Œå¯é‡å¤çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥åŸºå‡†å¥—ä»¶å¡«è¡¥äº†é¢†åŸŸå†…ç¼ºä¹ç»Ÿä¸€æ ‡å‡†çš„ç©ºç™½ï¼Œæœ‰æ•ˆæ¨åŠ¨äº†æœºå™¨äººä¸»åŠ¨æ„ŸçŸ¥ä¸è§¦è§‰ä¼ æ„ŸæŠ€æœ¯çš„ç³»ç»ŸåŒ–å‘å±•ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06361v2",
      "published_date": "2025-06-03 14:42:16 UTC",
      "updated_date": "2025-06-14 14:33:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:02:11.082903+00:00"
    },
    {
      "arxiv_id": "2506.02931v1",
      "title": "ThinkTank: A Framework for Generalizing Domain-Specific AI Agent Systems into Universal Collaborative Intelligence Platforms",
      "title_zh": "ThinkTankï¼šå°†é¢†åŸŸä¸“ç”¨ AI æ™ºèƒ½ä½“ç³»ç»Ÿæ³›åŒ–ä¸ºé€šç”¨åä½œæ™ºèƒ½å¹³å°çš„æ¡†æ¶",
      "authors": [
        "Praneet Sai Madhu Surabhi",
        "Dheeraj Reddy Mudireddy",
        "Jian Tao"
      ],
      "abstract": "This paper presents ThinkTank, a comprehensive and scalable framework designed to transform specialized AI agent systems into versatile collaborative intelligence platforms capable of supporting complex problem-solving across diverse domains. ThinkTank systematically generalizes agent roles, meeting structures, and knowledge integration mechanisms by adapting proven scientific collaboration methodologies. Through role abstraction, generalization of meeting types for iterative collaboration, and the integration of Retrieval-Augmented Generation with advanced knowledge storage, the framework facilitates expertise creation and robust knowledge sharing. ThinkTank enables organizations to leverage collaborative AI for knowledge-intensive tasks while ensuring data privacy and security through local deployment, utilizing frameworks like Ollama with models such as Llama3.1. The ThinkTank framework is designed to deliver significant advantages in cost-effectiveness, data security, scalability, and competitive positioning compared to cloud-based alternatives, establishing it as a universal platform for AI-driven collaborative problem-solving. The ThinkTank code is available at https://github.com/taugroup/ThinkTank",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ThinkTankï¼Œä¸€ä¸ªå…¨é¢ä¸”å¯æ‰©å±•çš„æ¡†æ¶ï¼Œæ—¨åœ¨å°†ç‰¹å®šé¢†åŸŸçš„AI Agentç³»ç»Ÿè½¬åŒ–ä¸ºèƒ½å¤Ÿæ”¯æŒè·¨é¢†åŸŸå¤æ‚é—®é¢˜è§£å†³çš„é€šç”¨åä½œæ™ºèƒ½å¹³å°ã€‚è¯¥æ¡†æ¶é€šè¿‡å€Ÿé‰´æˆç†Ÿçš„ç§‘å­¦åä½œæ–¹æ³•è®ºï¼Œç³»ç»Ÿæ€§åœ°å®ç°äº†æ™ºèƒ½ä½“è§’è‰²ã€ä¼šè®®ç»“æ„ä»¥åŠçŸ¥è¯†æ•´åˆæœºåˆ¶çš„æ³›åŒ–ã€‚åˆ©ç”¨è§’è‰²æŠ½è±¡(Role Abstraction)ã€è¿­ä»£åä½œçš„ä¼šè®®ç±»å‹æ³›åŒ–ï¼Œä»¥åŠæ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation, RAG)ä¸å…ˆè¿›çŸ¥è¯†å­˜å‚¨çš„ç»“åˆï¼ŒThinkTankæœ‰æ•ˆä¿ƒè¿›äº†ä¸“ä¸šçŸ¥è¯†çš„åˆ›é€ ä¸ç¨³å¥å…±äº«ã€‚ä¸ºä¿éšœæ•°æ®éšç§å’Œå®‰å…¨æ€§ï¼Œè¯¥æ¡†æ¶æ”¯æŒé€šè¿‡Ollamaç­‰å·¥å…·é…åˆLlama3.1æ¨¡å‹è¿›è¡Œæœ¬åœ°éƒ¨ç½²ï¼Œç¡®ä¿äº†çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡çš„å®‰å…¨æ‰§è¡Œã€‚ç›¸æ¯”äº‘ç«¯æ–¹æ¡ˆï¼ŒThinkTankåœ¨æˆæœ¬æ•ˆç›Šã€æ•°æ®å®‰å…¨å’Œå¯æ‰©å±•æ€§æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºAIé©±åŠ¨çš„åä½œå¼é—®é¢˜è§£å†³æä¾›äº†é€šç”¨çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02931v1",
      "published_date": "2025-06-03 14:32:48 UTC",
      "updated_date": "2025-06-03 14:32:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:02:14.885996+00:00"
    },
    {
      "arxiv_id": "2506.05387v2",
      "title": "Advancing Decoding Strategies: Enhancements in Locally Typical Sampling for LLMs",
      "title_zh": "æ¨è¿›è§£ç ç­–ç•¥ï¼šLLMs å±€éƒ¨å…¸å‹é‡‡æ ·çš„æ”¹è¿›",
      "authors": [
        "Jaydip Sen",
        "Saptarshi Sengupta",
        "Subhasis Dasgupta"
      ],
      "abstract": "This chapter explores advancements in decoding strategies for large language models (LLMs), focusing on enhancing the Locally Typical Sampling (LTS) algorithm. Traditional decoding methods, such as top-k and nucleus sampling, often struggle to balance fluency, diversity, and coherence in text generation. To address these challenges, Adaptive Semantic-Aware Typicality Sampling (ASTS) is proposed as an improved version of LTS, incorporating dynamic entropy thresholding, multi-objective scoring, and reward-penalty adjustments. ASTS ensures contextually coherent and diverse text generation while maintaining computational efficiency. Its performance is evaluated across multiple benchmarks, including story generation and abstractive summarization, using metrics such as perplexity, MAUVE, and diversity scores. Experimental results demonstrate that ASTS outperforms existing sampling techniques by reducing repetition, enhancing semantic alignment, and improving fluency.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)è§£ç ç­–ç•¥çš„æœ€æ–°è¿›å±•ï¼Œé‡ç‚¹é’ˆå¯¹Locally Typical Sampling (LTS)ç®—æ³•è¿›è¡Œäº†ä¼˜åŒ–ã€‚ä¸ºäº†è§£å†³ä¼ ç»Ÿè§£ç æ–¹æ³•å¦‚top-kå’Œnucleus samplingåœ¨å¹³è¡¡æ–‡æœ¬æµåˆ©åº¦ã€å¤šæ ·æ€§å’Œè¿è´¯æ€§æ–¹é¢çš„æŒ‘æˆ˜ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§æ”¹è¿›çš„Adaptive Semantic-Aware Typicality Sampling (ASTS)ç®—æ³•ã€‚è¯¥ç®—æ³•ç»“åˆäº†åŠ¨æ€ç†µé˜ˆå€¼(dynamic entropy thresholding)ã€å¤šç›®æ ‡è¯„åˆ†(multi-objective scoring)å’Œå¥–åŠ±æƒ©ç½šè°ƒèŠ‚æœºåˆ¶ï¼Œæ—¨åœ¨ç¡®ä¿ç”Ÿæˆæ–‡æœ¬åœ¨å…·å¤‡ä¸Šä¸‹æ–‡è¿è´¯æ€§å’Œå¤šæ ·æ€§çš„åŒæ—¶ï¼Œç»´æŒè¾ƒé«˜çš„è®¡ç®—æ•ˆç‡ã€‚é€šè¿‡åœ¨æ•…äº‹ç”Ÿæˆå’ŒæŠ½è±¡æ€»ç»“ç­‰ä»»åŠ¡ä¸Šçš„å®éªŒè¯„ä¼°ï¼Œå¹¶ä½¿ç”¨perplexityã€MAUVEåŠå¤šæ ·æ€§è¯„åˆ†ç­‰æŒ‡æ ‡è¿›è¡Œè¡¡é‡ï¼Œç»“æœè¯æ˜ASTSåœ¨å‡å°‘å†…å®¹é‡å¤ã€å¢å¼ºè¯­ä¹‰å¯¹é½å’Œæé«˜è¯­è¨€æµåˆ©åº¦æ–¹é¢å‡ä¼˜äºç°æœ‰çš„é‡‡æ ·æŠ€æœ¯ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "This is the accepted but pre-reviewed version of the chapter that has been accepted for publication in the Springer volume 'Decision-Making in Computational Intelligence-Based Systems,' edited by Witold Pedrycz, Gilberto Rivera, Rose Ma Rodriguez, and Salvador Ibarra Martinez. The chapter is 39 pages long, and it contains 2 figures and 6 tables. This is NOT the final camera-ready version",
      "pdf_url": "https://arxiv.org/pdf/2506.05387v2",
      "published_date": "2025-06-03 14:25:23 UTC",
      "updated_date": "2025-06-11 16:08:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:02:14.184235+00:00"
    },
    {
      "arxiv_id": "2506.02923v1",
      "title": "The Limits of Predicting Agents from Behaviour",
      "title_zh": "åŸºäºè¡Œä¸ºé¢„æµ‹æ™ºèƒ½ä½“çš„å±€é™æ€§",
      "authors": [
        "Alexis Bellot",
        "Jonathan Richens",
        "Tom Everitt"
      ],
      "abstract": "As the complexity of AI systems and their interactions with the world increases, generating explanations for their behaviour is important for safely deploying AI. For agents, the most natural abstractions for predicting behaviour attribute beliefs, intentions and goals to the system. If an agent behaves as if it has a certain goal or belief, then we can make reasonable predictions about how it will behave in novel situations, including those where comprehensive safety evaluations are untenable. How well can we infer an agent's beliefs from their behaviour, and how reliably can these inferred beliefs predict the agent's behaviour in novel situations? We provide a precise answer to this question under the assumption that the agent's behaviour is guided by a world model. Our contribution is the derivation of novel bounds on the agent's behaviour in new (unseen) deployment environments, which represent a theoretical limit for predicting intentional agents from behavioural data alone. We discuss the implications of these results for several research areas including fairness and safety.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†éšç€äººå·¥æ™ºèƒ½ç³»ç»Ÿå¤æ‚æ€§çš„å¢åŠ ï¼Œé€šè¿‡å½’å›  beliefsã€intentions å’Œ goals æ¥è§£é‡Šå’Œé¢„æµ‹æ™ºèƒ½ä½“è¡Œä¸ºçš„é‡è¦æ€§ã€‚ç ”ç©¶é‡ç‚¹åˆ†æäº†ä»æ™ºèƒ½ä½“çš„è¡Œä¸ºä¸­æ¨æ–­å…¶ beliefs çš„å‡†ç¡®æ€§ï¼Œä»¥åŠè¿™äº›æ¨æ–­å‡ºçš„ beliefs åœ¨æ–°åœºæ™¯ä¸­é¢„æµ‹è¡Œä¸ºçš„å¯é æ€§ã€‚åœ¨æ™ºèƒ½ä½“è¡Œä¸ºç”± world model å¼•å¯¼çš„å‡è®¾ä¸‹ï¼Œè¯¥è®ºæ–‡æ¨å¯¼å‡ºäº†æ™ºèƒ½ä½“åœ¨æ–°éƒ¨ç½²ç¯å¢ƒï¼ˆunseen deployment environmentsï¼‰ä¸­è¡Œä¸ºçš„æ–°ç•Œé™ï¼ˆboundsï¼‰ã€‚è¿™äº›ç•Œé™ä»£è¡¨äº†ä»…å‡­è¡Œä¸ºæ•°æ®é¢„æµ‹ intentional agents çš„ç†è®ºæé™ã€‚æœ€åï¼Œä½œè€…è®¨è®ºäº†è¿™äº›ç†è®ºç»“æœå¯¹ fairness å’Œ safety ç­‰å¤šä¸ªç ”ç©¶é¢†åŸŸçš„é‡è¦å¯ç¤ºï¼Œä¸ºå®‰å…¨éƒ¨ç½²äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02923v1",
      "published_date": "2025-06-03 14:24:58 UTC",
      "updated_date": "2025-06-03 14:24:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:02:22.299924+00:00"
    },
    {
      "arxiv_id": "2506.02918v2",
      "title": "World Modelling Improves Language Model Agents",
      "title_zh": "ä¸–ç•Œå»ºæ¨¡æå‡è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“",
      "authors": [
        "Shangmin Guo",
        "Omar Darwiche Domingues",
        "RaphaÃ«l Avalos",
        "Aaron Courville",
        "Florian Strub"
      ],
      "abstract": "Tool use in stateful environments presents unique challenges for large language models (LLMs), where existing test-time compute strategies relying on repeated trials in the environment are impractical. We propose dynamics modelling (DyMo), a method that augments LLMs with a state prediction capability alongside function calling during post-training. This enables LLMs to predict the future states of their actions through an internal environment model. On the Berkeley Function Calling Leaderboard V2, DyMo improves success rates and significantly reduces hallucinations. We further integrate the internal environment model into self-verification sampling (SVS), and show that this substantially improves pass^k over number of trials k, and allows the model to refuse unreliable outputs. Together, DyMo and SVS greatly enhance the effectiveness and reliability of LLMs for tool use. We believe this work charts a path towards scalable planning RL methods for LLM inference without repeatedly querying the oracle environment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æœ‰çŠ¶æ€ç¯å¢ƒä¸­ä½¿ç”¨å·¥å…·çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºDyMo(Dynamics Modelling)çš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨åè®­ç»ƒé˜¶æ®µæ•´åˆçŠ¶æ€é¢„æµ‹ä¸å‡½æ•°è°ƒç”¨èƒ½åŠ›ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€šè¿‡å†…éƒ¨ç¯å¢ƒæ¨¡å‹é¢„æµ‹è¡ŒåŠ¨åçš„æœªæ¥çŠ¶æ€ã€‚åœ¨Berkeley Function Calling Leaderboard V2çš„æµ‹è¯•ä¸­ï¼ŒDyMoæ˜¾è‘—æå‡äº†ä»»åŠ¡æˆåŠŸç‡å¹¶é™ä½äº†å¹»è§‰å‘ç”Ÿç‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ç»“åˆå†…éƒ¨ç¯å¢ƒæ¨¡å‹çš„è‡ªæˆ‘éªŒè¯é‡‡æ ·(SVS)æœºåˆ¶ï¼Œå¤§å¹…ä¼˜åŒ–äº†pass@kæŒ‡æ ‡ï¼Œå¹¶æ”¯æŒæ¨¡å‹æ‹’ç»ä¸å¯é çš„è¾“å‡ºã€‚DyMoä¸SVSçš„ç»“åˆå¢å¼ºäº†LLMså·¥å…·è°ƒç”¨çš„æœ‰æ•ˆæ€§ä¸å¯é æ€§ï¼Œä¸ºæ— éœ€åå¤æŸ¥è¯¢çœŸå®ç¯å¢ƒçš„å¯æ‰©å±•æ¨ç†è§„åˆ’è·¯å¾„æä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02918v2",
      "published_date": "2025-06-03 14:20:59 UTC",
      "updated_date": "2025-09-19 03:54:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:02:43.394022+00:00"
    },
    {
      "arxiv_id": "2506.02911v1",
      "title": "Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning",
      "title_zh": "Cell-o1ï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹è§£å†³å•ç»†èƒæ¨ç†è°œé¢˜",
      "authors": [
        "Yin Fang",
        "Qiao Jin",
        "Guangzhi Xiong",
        "Bowen Jin",
        "Xianrui Zhong",
        "Siru Ouyang",
        "Aidong Zhang",
        "Jiawei Han",
        "Zhiyong Lu"
      ],
      "abstract": "Cell type annotation is a key task in analyzing the heterogeneity of single-cell RNA sequencing data. Although recent foundation models automate this process, they typically annotate cells independently, without considering batch-level cellular context or providing explanatory reasoning. In contrast, human experts often annotate distinct cell types for different cell clusters based on their domain knowledge. To mimic this workflow, we introduce the CellPuzzles task, where the objective is to assign unique cell types to a batch of cells. This benchmark spans diverse tissues, diseases, and donor conditions, and requires reasoning across the batch-level cellular context to ensure label uniqueness. We find that off-the-shelf large language models (LLMs) struggle on CellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0% batch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained via supervised fine-tuning on distilled reasoning traces, followed by reinforcement learning with batch-level rewards. Cell-o1 achieves state-of-the-art performance, outperforming o1 by over 73% and generalizing well across contexts. Further analysis of training dynamics and reasoning behaviors provides insights into batch-level annotation performance and emergent expert-like reasoning. Code and data are available at https://github.com/ncbi-nlp/cell-o1.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å•ç»†èƒRNAæµ‹åº(single-cell RNA sequencing)æ•°æ®åˆ†æä¸­ç»†èƒç±»å‹æ³¨é‡Š(Cell type annotation)ç¼ºä¹æ‰¹æ¬¡çº§(batch-level)ä¸Šä¸‹æ–‡å’Œæ¨ç†èƒ½åŠ›çš„é—®é¢˜ï¼Œæå‡ºäº†æ¨¡æ‹Ÿä¸“å®¶å·¥ä½œæµçš„CellPuzzlesä»»åŠ¡ã€‚ç ”ç©¶å‘ç°ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†éœ€ç¡®ä¿æ ‡ç­¾å”¯ä¸€æ€§çš„æ‰¹æ¬¡çº§æ¨ç†æ—¶è¡¨ç°ä¹åŠ›ï¼ŒåŸºçº¿æ¨¡å‹OpenAI o1çš„å‡†ç¡®ç‡ä»…ä¸º19.0%ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…å¼€å‘äº†Cell-o1ï¼Œä¸€ä¸ªé€šè¿‡åœ¨è’¸é¦æ¨ç†è½¨è¿¹ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒ(Supervised fine-tuning)å¹¶ç»“åˆæ‰¹æ¬¡çº§å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ (Reinforcement learning)è®­ç»ƒè€Œæˆçš„7Bå‚æ•°æ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCell-o1çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºo1ï¼Œå‡†ç¡®ç‡æå‡å¹…åº¦è¶…è¿‡73%ï¼Œå¹¶åœ¨è·¨ä¸Šä¸‹æ–‡æ³›åŒ–åŠæ¶Œç°ç±»ä¸“å®¶æ¨ç†è¡Œä¸ºæ–¹é¢å±•ç°å‡ºæå¼ºç«äº‰åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "28 pages; 16 tables; 7 figures; Code: https://github.com/ncbi-nlp/cell-o1",
      "pdf_url": "https://arxiv.org/pdf/2506.02911v1",
      "published_date": "2025-06-03 14:16:53 UTC",
      "updated_date": "2025-06-03 14:16:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:03:48.186377+00:00"
    },
    {
      "arxiv_id": "2506.02899v1",
      "title": "IMPARA-GED: Grammatical Error Detection is Boosting Reference-free Grammatical Error Quality Estimator",
      "title_zh": "IMPARA-GEDï¼šè¯­æ³•é”™è¯¯æ£€æµ‹æå‡æ— å‚è€ƒè¯­æ³•é”™è¯¯è´¨é‡è¯„ä¼°å™¨",
      "authors": [
        "Yusuke Sakai",
        "Takumi Goto",
        "Taro Watanabe"
      ],
      "abstract": "We propose IMPARA-GED, a novel reference-free automatic grammatical error correction (GEC) evaluation method with grammatical error detection (GED) capabilities. We focus on the quality estimator of IMPARA, an existing automatic GEC evaluation method, and construct that of IMPARA-GED using a pre-trained language model with enhanced GED capabilities. Experimental results on SEEDA, a meta-evaluation dataset for automatic GEC evaluation methods, demonstrate that IMPARA-GED achieves the highest correlation with human sentence-level evaluations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† IMPARA-GEDï¼Œè¿™æ˜¯ä¸€ç§å…·å¤‡è¯­æ³•é”™è¯¯æ£€æµ‹ (Grammatical Error Detection, GED) èƒ½åŠ›çš„æ–°å‹æ— å‚è€ƒè‡ªåŠ¨è¯­æ³•é”™è¯¯çº æ­£ (Grammatical Error Correction, GEC) è¯„ä»·æ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡ç‚¹æ”¹è¿›äº†ç°æœ‰è¯„ä»·æ–¹æ³• IMPARA çš„è´¨é‡è¯„ä¼°å™¨ (quality estimator)ï¼Œé€šè¿‡åˆ©ç”¨å¢å¼ºäº† GED èƒ½åŠ›çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ¥æ„å»º IMPARA-GED ç³»ç»Ÿã€‚é€šè¿‡æ•´åˆ GED åŠŸèƒ½ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åœ¨æ²¡æœ‰äººå·¥å‚è€ƒæ–‡æœ¬çš„æƒ…å†µä¸‹è¯†åˆ«å¹¶è¡¡é‡è¯­æ³•ä¿®æ­£çš„è´¨é‡ã€‚åœ¨è‡ªåŠ¨ GEC è¯„ä»·æ–¹æ³•çš„å…ƒè¯„ä¼°æ•°æ®é›† SEEDA ä¸Šè¿›è¡Œçš„å®éªŒç»“æœè¯æ˜ï¼ŒIMPARA-GED åœ¨å¥å­çº§è¯„ä¼°ä¸­è¾¾åˆ°äº†ä¸äººå·¥è¯„ä»·æœ€é«˜çš„ç›¸å…³æ€§ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†å¼ºåŒ– GED èƒ½åŠ›å¯¹äºæå‡æ— å‚è€ƒ GEC è´¨é‡è¯„ä¼°å™¨æ€§èƒ½çš„æ˜¾è‘—ä½œç”¨ï¼Œä¸ºè‡ªåŠ¨çº é”™è¯„ä»·é¢†åŸŸæä¾›äº†æ›´å¯é çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2506.02899v1",
      "published_date": "2025-06-03 14:05:37 UTC",
      "updated_date": "2025-06-03 14:05:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:02:49.891911+00:00"
    },
    {
      "arxiv_id": "2506.03231v1",
      "title": "NetPress: Dynamically Generated LLM Benchmarks for Network Applications",
      "title_zh": "NetPressï¼šé¢å‘ç½‘ç»œåº”ç”¨çš„åŠ¨æ€ç”Ÿæˆå¼ LLM åŸºå‡†",
      "authors": [
        "Yajie Zhou",
        "Jiajun Ruan",
        "Eric S. Wang",
        "Sadjad Fouladi",
        "Francis Y. Yan",
        "Kevin Hsieh",
        "Zaoxing Liu"
      ],
      "abstract": "Despite growing interest in domain-specific benchmarking of large language models (LLMs) and agents, current evaluations remain limited to static, small-scale datasets, especially in high-stakes tasks like network operations that demand reliability for deployments. We present NetPress, an automated benchmark generation framework for evaluating LLM agents in network applications. NetPress introduces a unified abstraction with state and action, enabling dynamic generation of diverse query sets along with corresponding ground truths. At runtime, users can specify benchmark configurations to generate millions of queries on the fly. In addition to dynamic benchmark construction, NetPress integrates with network emulators to provide realistic environment feedback, supporting comprehensive evaluation across correctness, safety, and latency. We instantiate NetPress on three representative applications, revealing interesting fine-grained differences in agent behavior that static, correctness-only benchmarks often miss. NetPress moves LLM evaluation toward realistic, scalable testing in infrastructure-centric domains, helping close the gap between benchmark performance and real-world deployment readiness. Code is available at https://github.com/Froot-NetSys/NetPress.",
      "tldr_zh": "å½“å‰é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åŠå…¶æ™ºèƒ½ä½“çš„é¢†åŸŸç‰¹å®šåŸºå‡†æµ‹è¯•å¤§å¤šå±€é™äºé™æ€ä¸”å°è§„æ¨¡çš„æ•°æ®é›†ï¼Œéš¾ä»¥æ»¡è¶³ç½‘ç»œè¿ç»´ç­‰é«˜é£é™©ä»»åŠ¡å¯¹å¯é æ€§çš„ä¸¥è‹›è¦æ±‚ã€‚æœ¬ç ”ç©¶æå‡ºäº†NetPressï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°ç½‘ç»œåº”ç”¨ä¸­LLMæ™ºèƒ½ä½“çš„è‡ªåŠ¨åŒ–åŸºå‡†æµ‹è¯•ç”Ÿæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ç»Ÿä¸€çš„çŠ¶æ€(state)ä¸åŠ¨ä½œ(action)æŠ½è±¡ï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·é…ç½®åŠ¨æ€ç”Ÿæˆæ•°ç™¾ä¸‡æ¡å¤šæ ·åŒ–çš„æŸ¥è¯¢é›†åŠç›¸åº”çš„æ ‡æ³¨ç»“æœ(ground truths)ã€‚æ­¤å¤–ï¼ŒNetPressä¸ç½‘ç»œæ¨¡æ‹Ÿå™¨é›†æˆä»¥æä¾›çœŸå®çš„åé¦ˆï¼Œæ”¯æŒä»æ­£ç¡®æ€§(correctness)ã€å®‰å…¨æ€§(safety)å’Œå»¶è¿Ÿ(latency)ç­‰ç»´åº¦è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚åœ¨ä¸‰ä¸ªä»£è¡¨æ€§åº”ç”¨ä¸Šçš„å®éªŒæ­ç¤ºäº†æ™ºèƒ½ä½“åœ¨ç»†ç²’åº¦è¡Œä¸ºä¸Šçš„å·®å¼‚ï¼Œè¿™äº›å·®å¼‚æ˜¯ä¼ ç»Ÿé™æ€åŸºå‡†æµ‹è¯•éš¾ä»¥å‘ç°çš„ã€‚NetPressçš„æ¨å‡ºå°†LLMè¯„ä¼°æ¨å‘äº†æ›´å…·æ‰©å±•æ€§å’ŒçœŸå®æ€§çš„åŸºç¡€è®¾æ–½é¢†åŸŸï¼Œæœ‰åŠ©äºå¼¥åˆå®éªŒå®¤åŸºå‡†æ€§èƒ½ä¸ç°å®ä¸–ç•Œéƒ¨ç½²å‡†å¤‡åº¦ä¹‹é—´çš„å·®è·ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03231v1",
      "published_date": "2025-06-03 14:04:22 UTC",
      "updated_date": "2025-06-03 14:04:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:03:17.491386+00:00"
    },
    {
      "arxiv_id": "2506.02890v1",
      "title": "Scaling Fine-Grained MoE Beyond 50B Parameters: Empirical Evaluation and Practical Insights",
      "title_zh": "çªç ´ 50B å‚æ•°è§„æ¨¡çš„ç»†ç²’åº¦ MoEï¼šå®è¯è¯„ä¼°ä¸å®è·µè§è§£",
      "authors": [
        "Jakub Krajewski",
        "Marcin Chochowski",
        "Daniel Korzekwa"
      ],
      "abstract": "Mixture of Experts (MoE) architectures have emerged as pivotal for scaling Large Language Models (LLMs) efficiently. Fine-grained MoE approaches - utilizing more numerous, smaller experts - have demonstrated potential in improving model convergence and quality. This work proposes a set of training recipes and provides a comprehensive empirical evaluation of fine-grained MoE, directly comparing its scaling properties against standard MoE configurations for models with up to 56B total (17B active) parameters. We investigate convergence speed, model performance on downstream benchmarks, and practical training considerations across various setups. Overall, at the largest scale we show that fine-grained MoE achieves better validation loss and higher accuracy across a set of downstream benchmarks. This study offers empirical grounding and practical insights for leveraging fine-grained MoE in the development of future large-scale models.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†Mixture of Experts (MoE)æ¶æ„åœ¨é«˜æ•ˆæ‰©å±•Large Language Models (LLMs)æ–¹é¢çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡ä½¿ç”¨æ›´å¤šã€æ›´å°çš„ä¸“å®¶æ¥å®ç°çš„fine-grained MoEåœ¨æå‡æ¨¡å‹æ”¶æ•›å’Œè´¨é‡æ–¹é¢çš„æ½œåŠ›ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç³»åˆ—è®­ç»ƒæ–¹æ¡ˆï¼Œå¹¶å¯¹fine-grained MoEè¿›è¡Œäº†å…¨é¢çš„ç»éªŒè¯„ä¼°ï¼Œå°†å…¶æ‰©å±•ç‰¹æ€§ä¸é«˜è¾¾56Bæ€»å‚æ•°ï¼ˆ17Bæ´»è·ƒå‚æ•°ï¼‰çš„æ ‡å‡†MoEé…ç½®è¿›è¡Œäº†ç›´æ¥å¯¹æ¯”ã€‚è°ƒæŸ¥æ¶µç›–äº†ä¸åŒè®¾ç½®ä¸‹çš„æ”¶æ•›é€Ÿåº¦ã€ä¸‹æ¸¸åŸºå‡†æµ‹è¯•(downstream benchmarks)ä¸­çš„æ¨¡å‹æ€§èƒ½ä»¥åŠå®é™…è®­ç»ƒä¸­çš„è€ƒé‡å› ç´ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æœ€å¤§è§„æ¨¡ä¸‹ï¼Œfine-grained MoEç›¸æ¯”æ ‡å‡†MoEå®ç°äº†æ›´å¥½çš„éªŒè¯æŸå¤±(validation loss)å’Œæ›´é«˜çš„åŸºå‡†æµ‹è¯•å‡†ç¡®ç‡ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨æœªæ¥å¤§è§„æ¨¡æ¨¡å‹å¼€å‘ä¸­æœ‰æ•ˆåœ°åˆ©ç”¨fine-grained MoEæä¾›äº†åšå®çš„ç»éªŒä¾æ®å’Œå®è·µè§è§£ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02890v1",
      "published_date": "2025-06-03 13:55:48 UTC",
      "updated_date": "2025-06-03 13:55:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:03:09.196525+00:00"
    },
    {
      "arxiv_id": "2506.03230v1",
      "title": "DiaBlo: Diagonal Blocks Are Sufficient For Finetuning",
      "title_zh": "DiaBloï¼šå¯¹è§’å—è¶³ä»¥å®ç°å¾®è°ƒ",
      "authors": [
        "Selcuk Gurses",
        "Aozhong Zhang",
        "Yanxia Deng",
        "Xun Dong",
        "Xin Li",
        "Naigang Wang",
        "Penghang Yin",
        "Zi Yang"
      ],
      "abstract": "Finetuning is a critical step for adapting large language models (LLMs) to domain-specific downstream tasks. To mitigate the substantial computational and memory costs of full-model fine-tuning, Parameter-Efficient Finetuning (PEFT) methods have been proposed to update only a small subset of model parameters. However, performance gaps between PEFT approaches and full-model fine-tuning still exist. In this work, we present DiaBlo, a simple yet effective PEFT approach that updates only the diagonal blocks of selected model weight matrices. Unlike Low Rank Adaptation (LoRA) and its variants, DiaBlo eliminates the need for low rank matrix products, thereby avoiding the reliance on auxiliary initialization schemes or customized optimization strategies to improve convergence. This design leads to stable and robust convergence while maintaining comparable memory efficiency and training speed to LoRA. We conduct extensive experiments across a range of tasks, including commonsense reasoning, arithmetic reasoning, code generation, and safety alignment, to evaluate the effectiveness and efficiency of DiaBlo. Across these benchmarks, DiaBlo demonstrates strong and consistent performance while maintaining high memory efficiency and fast finetuning speed. Codes are available at https://github.com/ziyangjoy/DiaBlo.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DiaBloï¼Œä¸€ç§ç®€å•ä¸”æœ‰æ•ˆçš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ï¼Œé€šè¿‡ä»…æ›´æ–°é€‰å®šæ¨¡å‹æƒé‡çŸ©é˜µçš„å¯¹è§’å—ï¼ˆdiagonal blocksï¼‰æ¥é™ä½å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é€‚é…æˆæœ¬ã€‚ä¸ä½ç§©è‡ªé€‚åº”ï¼ˆLoRAï¼‰åŠå…¶å˜ä½“ä¸åŒï¼ŒDiaBlo æ‘’å¼ƒäº†ä½ç§©çŸ©é˜µç§¯çš„è®¾è®¡ï¼Œä»è€Œé¿å…äº†å¯¹è¾…åŠ©åˆå§‹åŒ–æ–¹æ¡ˆæˆ–å®šåˆ¶ä¼˜åŒ–ç­–ç•¥çš„ä¾èµ–ï¼Œå®ç°äº†æ›´ç¨³å®šä¸”é²æ£’çš„æ”¶æ•›ã€‚DiaBlo åœ¨ä¿æŒä¸ LoRA ç›¸å½“çš„å†…å­˜æ•ˆç‡å’Œè®­ç»ƒé€Ÿåº¦çš„åŒæ—¶ï¼Œæœ‰æ•ˆå¼¥è¡¥äº† PEFT ä¸å…¨å‚æ•°å¾®è°ƒä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚å®éªŒæ¶µç›–äº†å¸¸è¯†æ¨ç†ã€ç®—æœ¯æ¨ç†ã€ä»£ç ç”Ÿæˆå’Œå®‰å…¨å¯¹é½ç­‰å¤šç§ä»»åŠ¡ï¼Œç»“æœè¡¨æ˜ DiaBlo åœ¨å„é¡¹åŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºå¼ºåŠ²ä¸”ä¸€è‡´çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•ä¸ºå¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„é«˜æ•ˆå¾®è°ƒæä¾›äº†ä¸€ç§ç®€å•ä¸”æå…·ç«äº‰åŠ›çš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03230v1",
      "published_date": "2025-06-03 13:47:59 UTC",
      "updated_date": "2025-06-03 13:47:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:03:31.494510+00:00"
    },
    {
      "arxiv_id": "2506.02878v2",
      "title": "CoT is Not True Reasoning, It Is Just a Tight Constraint to Imitate: A Theory Perspective",
      "title_zh": "CoT å¹¶éçœŸæ­£çš„æ¨ç†ï¼Œä»…æ˜¯ä¸€ç§ç”¨äºæ¨¡ä»¿çš„å¼ºçº¦æŸï¼šç†è®ºè§†è§’ä¸‹çš„æ¢è®¨",
      "authors": [
        "Jintian Shao",
        "Yiming Cheng"
      ],
      "abstract": "Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of Large Language Models on tasks requiring multi-step inference. This success has led to widespread claims of emergent reasoning capabilities in these models. In this paper, we present a theoretical counter-perspective: Chain-of-Thought (CoT) does not elicit genuine, abstract reasoning. Instead, we argue that Chain-of-Thought functions as a powerful structural constraint that guides Large Language Models to imitate the form of reasoning. By forcing the generation of intermediate steps, Chain-of-Thought leverages the model immense capacity for sequence prediction and pattern matching, effectively constraining its output to sequences that resemble coherent thought processes. Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of Large Language Models on tasks requiring multi-step inference. This success has led to widespread claims of emergent reasoning capabilities in these models. In this paper, we present a theoretical counter-perspective: Chain-of-Thought (CoT) does not elicit genuine, abstract reasoning. Instead, we argue that Chain-of-Thought functions as a powerful structural constraint that guides Large Language Models to imitate the form of reasoning. By forcing the generation of intermediate steps, Chain-of-Thought leverages the model immense capacity for sequence prediction and pattern matching, effectively constraining its output to sequences that resemble coherent thought processes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­Chain-of-Thought (CoT) æç¤ºè¯åœ¨å¤šæ­¥æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°æå‡ºäº†ç†è®ºåæ€ï¼ŒæŒ‘æˆ˜äº†æ¨¡å‹å…·å¤‡æ¶Œç°æ¨ç†èƒ½åŠ›çš„è§‚ç‚¹ã€‚ä½œè€…è®¤ä¸ºï¼ŒChain-of-Thought å¹¶éæ¿€å‘äº†æ¨¡å‹çœŸæ­£çš„æŠ½è±¡æ¨ç†ï¼Œè€Œæ˜¯ä½œä¸ºä¸€ç§å¼ºåŠ›çš„ç»“æ„çº¦æŸï¼Œå¼•å¯¼æ¨¡å‹æ¨¡ä»¿æ¨ç†çš„å½¢å¼ã€‚é€šè¿‡å¼ºåˆ¶ç”Ÿæˆä¸­é—´æ­¥éª¤ï¼ŒChain-of-Thought åˆ©ç”¨äº†æ¨¡å‹åœ¨åºåˆ—é¢„æµ‹(sequence prediction)å’Œæ¨¡å¼åŒ¹é…(pattern matching)æ–¹é¢çš„å¼ºå¤§èƒ½åŠ›ï¼Œå°†è¾“å‡ºé™åˆ¶åœ¨çœ‹ä¼¼è¿è´¯çš„æ€ç»´åºåˆ—ä¸­ã€‚è¯¥è®ºæ–‡æŒ‡å‡ºè¿™ç§æœºåˆ¶æœ¬è´¨ä¸Šæ˜¯åœ¨ä¸¥å¯†çº¦æŸä¸‹è¿›è¡Œçš„æ¨¡ä»¿ï¼Œè€Œéæ¨¡å‹äº§ç”Ÿäº†çœŸå®çš„è®¤çŸ¥æ¨ç†ã€‚è¿™ä¸€è§†è§’ä¸ºæ·±å…¥ç†è§£å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æœ¬è´¨æä¾›äº†é‡è¦çš„ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Last page bad picture, lacking some sufficient experiments",
      "pdf_url": "https://arxiv.org/pdf/2506.02878v2",
      "published_date": "2025-06-03 13:45:01 UTC",
      "updated_date": "2025-06-06 22:13:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:03:47.489062+00:00"
    },
    {
      "arxiv_id": "2506.14805v2",
      "title": "Argus Inspection: Do Multimodal Large Language Models Possess the Eye of Panoptes?",
      "title_zh": "Argus Inspectionï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ˜¯å¦å…·å¤‡ Panoptes ä¹‹çœ¼ï¼Ÿ",
      "authors": [
        "Yang Yao",
        "Lingyu Li",
        "Jiaxin Song",
        "Chiyu Chen",
        "Zhenqi He",
        "Yixu Wang",
        "Xin Wang",
        "Tianle Gu",
        "Jie Li",
        "Yan Teng",
        "Yingchun Wang"
      ],
      "abstract": "As Multimodal Large Language Models (MLLMs) continue to evolve, their cognitive and reasoning capabilities have seen remarkable progress. However, challenges in visual fine-grained perception and commonsense causal inference persist. This paper introduces Argus Inspection, a multimodal benchmark with two levels of difficulty, emphasizing detailed visual recognition while incorporating real-world commonsense understanding to evaluate causal reasoning abilities. Expanding on it, we present the Eye of Panoptes framework, which integrates a binary parametric Sigmoid metric with an indicator function, enabling a more holistic evaluation of MLLMs' responses in opinion-based reasoning tasks. Experiments conducted on 26 mainstream MLLMs reveal that the highest performance in visual fine-grained reasoning reaches only 0.46, highlighting considerable potential for enhancement. Our research offers valuable perspectives for the continued refinement of MLLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMultimodal Large Language Models, MLLMsï¼‰åœ¨ç»†ç²’åº¦è§†è§‰æ„ŸçŸ¥å’Œå¸¸è¯†å› æœæ¨ç†æ–¹é¢çš„æŒç»­æŒ‘æˆ˜ï¼Œæå‡ºäº†å…¨æ–°çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•é›† Argus Inspectionã€‚è¯¥åŸºå‡†åŒ…å«ä¸¤ä¸ªéš¾åº¦ç­‰çº§ï¼Œåœ¨å¼ºè°ƒç»†èŠ‚è§†è§‰è¯†åˆ«çš„åŒæ—¶ï¼Œèå…¥äº†ç°å®ä¸–ç•Œçš„å¸¸è¯†ç†è§£ä»¥è¯„ä¼°æ¨¡å‹çš„å› æœæ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº† Eye of Panoptes è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡é›†æˆäºŒå…ƒå‚æ•°åŒ– Sigmoid æŒ‡æ ‡ä¸æŒ‡ç¤ºå‡½æ•°ï¼Œå®ç°äº†å¯¹ MLLMs åœ¨åŸºäºè§‚ç‚¹çš„æ¨ç†ä»»åŠ¡ä¸­æ›´ä¸ºå…¨é¢çš„è¯„ä¼°ã€‚é’ˆå¯¹ 26 ç§ä¸»æµ MLLMs çš„å®éªŒæµ‹è¯„æ˜¾ç¤ºï¼Œè§†è§‰ç»†ç²’åº¦æ¨ç†çš„æœ€é«˜æ€§èƒ½ä»…è¾¾åˆ° 0.46ï¼Œå‡¸æ˜¾äº†ç°æœ‰æ¨¡å‹åœ¨è¿™ä¸€é¢†åŸŸçš„å·¨å¤§æå‡æ½œåŠ›ã€‚è¯¥é¡¹ç ”ç©¶æˆæœä¸º MLLMs çš„æ€§èƒ½è¡¡é‡å’Œåç»­ä¼˜åŒ–æä¾›äº†æå…·ä»·å€¼çš„è§†è§’ä¸åŸºå‡†å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.14805v2",
      "published_date": "2025-06-03 13:44:14 UTC",
      "updated_date": "2025-08-12 17:07:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:03:32.785119+00:00"
    },
    {
      "arxiv_id": "2506.02873v3",
      "title": "It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics",
      "title_zh": "æ„å›¾é‡äºç»“æœï¼šè¯„ä¼°å‰æ²¿å¤§è¯­è¨€æ¨¡å‹åœ¨æœ‰å®³è¯é¢˜ä¸Šçš„è¯´æœå°è¯•",
      "authors": [
        "Matthew Kowal",
        "Jasper Timm",
        "Jean-Francois Godbout",
        "Thomas Costello",
        "Antonio A. Arechar",
        "Gordon Pennycook",
        "David Rand",
        "Adam Gleave",
        "Kellin Pelrine"
      ],
      "abstract": "Persuasion is a powerful capability of large language models (LLMs) that both enables beneficial applications (e.g. helping people quit smoking) and raises significant risks (e.g. large-scale, targeted political manipulation). Prior work has found models possess a significant and growing persuasive capability, measured by belief changes in simulated or real users. However, these benchmarks overlook a crucial risk factor: the propensity of a model to attempt to persuade in harmful contexts. Understanding whether a model will blindly ``follow orders'' to persuade on harmful topics (e.g. glorifying joining a terrorist group) is key to understanding the efficacy of safety guardrails. Moreover, understanding if and when a model will engage in persuasive behavior in pursuit of some goal is essential to understanding the risks from agentic AI systems. We propose the Attempt to Persuade Eval (APE) benchmark, that shifts the focus from persuasion success to persuasion attempts, operationalized as a model's willingness to generate content aimed at shaping beliefs or behavior. Our evaluation framework probes frontier LLMs using a multi-turn conversational setup between simulated persuader and persuadee agents. APE explores a diverse spectrum of topics including conspiracies, controversial issues, and non-controversially harmful content. We introduce an automated evaluator model to identify willingness to persuade and measure the frequency and context of persuasive attempts. We find that many open and closed-weight models are frequently willing to attempt persuasion on harmful topics and that jailbreaking can increase willingness to engage in such behavior. Our results highlight gaps in current safety guardrails and underscore the importance of evaluating willingness to persuade as a key dimension of LLM risk. APE is available at github.com/AlignmentResearch/AttemptPersuadeEval",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)è¯´æœèƒ½åŠ›è¯„ä¼°å¾€å¾€ä¾§é‡äºè¯´æœçš„æˆåŠŸç‡ï¼Œè€Œå¿½è§†äº†æ¨¡å‹åœ¨æœ‰å®³èƒŒæ™¯ä¸‹è¿›è¡Œè¯´æœå°è¯•(Attempt to Persuade)çš„å€¾å‘ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº† Attempt to Persuade Eval (APE) åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¡¡é‡æ¨¡å‹ç”Ÿæˆæ—¨åœ¨æ”¹å˜ä»–äººä¿¡å¿µæˆ–è¡Œä¸ºå†…å®¹çš„æ„æ„¿ã€‚è¯¥è¯„ä¼°æ¡†æ¶åˆ©ç”¨æ¨¡æ‹Ÿè¯´æœè€…ä¸è¢«è¯´æœè€…ä¹‹é—´çš„å¤šè½®å¯¹è¯(Multi-turn Conversational Setup)ï¼Œæ¶µç›–äº†é˜´è°‹è®ºã€äº‰è®®æ€§è¯é¢˜å’Œæœ‰å®³å†…å®¹ç­‰å¤šç§åœºæ™¯ã€‚ç ”ç©¶é€šè¿‡è‡ªåŠ¨è¯„ä¼°æ¨¡å‹(Automated Evaluator)æ¥è¯†åˆ«è¯´æœæ„æ„¿ï¼Œå¹¶æµ‹é‡è¯´æœå°è¯•çš„é¢‘ç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè®¸å¤šå¼€æºå’Œé—­æºæ¨¡å‹åœ¨é¢å¯¹æœ‰å®³è¯é¢˜æ—¶ä»è¡¨ç°å‡ºè¾ƒé«˜çš„è¯´æœæ„æ„¿ï¼Œä¸”è¶Šç‹±(Jailbreaking)ä¼šè¿›ä¸€æ­¥å¢åŠ æ­¤ç±»è¡Œä¸ºã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†å½“å‰å®‰å…¨æŠ¤æ (Safety Guardrails)å­˜åœ¨çš„ç¼ºé™·ï¼Œå¹¶å¼ºè°ƒäº†è¯„ä¼°è¯´æœæ„æ„¿ä½œä¸ºæ¨¡å‹é£é™©(LLM Risk)å…³é”®ç»´åº¦çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02873v3",
      "published_date": "2025-06-03 13:37:51 UTC",
      "updated_date": "2025-08-20 22:30:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:03:46.396579+00:00"
    },
    {
      "arxiv_id": "2506.02867v2",
      "title": "Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning",
      "title_zh": "ä»¥äº’ä¿¡æ¯æ­ç§˜æ¨ç†åŠ¨æ€ï¼šå¤§è¯­è¨€æ¨¡å‹æ¨ç†ä¸­çš„æ€è€ƒæ ‡è®°å³ä¿¡æ¯å³°",
      "authors": [
        "Chen Qian",
        "Dongrui Liu",
        "Haochen Wen",
        "Zhen Bai",
        "Yong Liu",
        "Jing Shao"
      ],
      "abstract": "Large reasoning models (LRMs) have demonstrated impressive capabilities in complex problem-solving, yet their internal reasoning mechanisms remain poorly understood. In this paper, we investigate the reasoning trajectories of LRMs from an information-theoretic perspective. By tracking how mutual information (MI) between intermediate representations and the correct answer evolves during LRM reasoning, we observe an interesting MI peaks phenomenon: the MI at specific generative steps exhibits a sudden and significant increase during LRM's reasoning process. We theoretically analyze such phenomenon and show that as MI increases, the probability of model's prediction error decreases. Furthermore, these MI peaks often correspond to tokens expressing reflection or transition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the thinking tokens. We then demonstrate that these thinking tokens are crucial for LRM's reasoning performance, while other tokens has minimal impacts. Building on these analyses, we propose two simple yet effective methods to improve LRM's reasoning performance, by delicately leveraging these thinking tokens. Overall, our work provides novel insights into the reasoning mechanisms of LRMs and offers practical ways to improve their reasoning capabilities. The code is available at https://github.com/ChnQ/MI-Peaks.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»ä¿¡æ¯è®ºè§’åº¦åˆ©ç”¨äº’ä¿¡æ¯ (Mutual Information) è¿½è¸ªå¤§å‹æ¨ç†æ¨¡å‹ (LRMs) çš„æ¨ç†è½¨è¿¹ï¼Œå‘ç°äº†ä¸­é—´è¡¨ç¤ºä¸æ­£ç¡®ç­”æ¡ˆä¹‹é—´çš„äº’ä¿¡æ¯åœ¨ç‰¹å®šç”Ÿæˆæ­¥éª¤ä¸­æ˜¾è‘—å¢åŠ çš„äº’ä¿¡æ¯å³°å€¼ (MI peaks) ç°è±¡ã€‚ç†è®ºåˆ†æè¯æ˜ï¼Œäº’ä¿¡æ¯çš„æå‡èƒ½æœ‰æ•ˆé™ä½æ¨¡å‹çš„é¢„æµ‹é”™è¯¯ç‡ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºè¿™äº›äº’ä¿¡æ¯å³°å€¼ä¸»è¦å¯¹åº”äº \"Hmm\"ã€\"Wait\" å’Œ \"Therefore\" ç­‰æ€è€ƒä»¤ç‰Œ (thinking tokens)ï¼Œä¸”è¿™äº›ä»¤ç‰Œæ˜¯ç»´æŒæ¨¡å‹æ¨ç†æ€§èƒ½çš„æ ¸å¿ƒè¦ç´ ã€‚åŸºäºä¸Šè¿°å‘ç°ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸¤ç§é€šè¿‡ä¼˜åŒ–åˆ©ç”¨æ€è€ƒä»¤ç‰Œæ¥å¢å¼ºå¤§å‹æ¨ç†æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæ–¹æ³•ã€‚è¿™é¡¹å·¥ä½œä¸ä»…æ·±åŒ–äº†å¯¹å¤§å‹æ¨ç†æ¨¡å‹å†…éƒ¨æ¨ç†æœºåˆ¶çš„ç†è§£ï¼Œè¿˜ä¸ºæå‡å…¶å¤æ‚é—®é¢˜è§£å†³èƒ½åŠ›æä¾›äº†åˆ‡å®å¯è¡Œçš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint. Under review",
      "pdf_url": "https://arxiv.org/pdf/2506.02867v2",
      "published_date": "2025-06-03 13:31:10 UTC",
      "updated_date": "2025-06-04 15:00:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:03:54.192287+00:00"
    },
    {
      "arxiv_id": "2506.02865v2",
      "title": "Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights",
      "title_zh": "Surfer-H ååŒ Holo1ï¼šåŸºäºå¼€æºæƒé‡çš„é«˜æ€§ä»·æ¯”ç½‘ç»œæ™ºèƒ½ä½“",
      "authors": [
        "Mathieu Andreux",
        "Breno Baldas Skuk",
        "Hamza Benchekroun",
        "Emilien BirÃ©",
        "Antoine Bonnet",
        "Riaz Bordie",
        "Nathan Bout",
        "Matthias Brunel",
        "Pierre-Louis Cedoz",
        "Antoine Chassang",
        "MickaÃ«l Chen",
        "Alexandra D. Constantinou",
        "Antoine d'AndignÃ©",
        "Hubert de La JonquiÃ¨re",
        "AurÃ©lien Delfosse",
        "Ludovic Denoyer",
        "Alexis Deprez",
        "Augustin Derupti",
        "Michael Eickenberg",
        "MathÃ¯s Federico",
        "Charles Kantor",
        "Xavier Koegler",
        "Yann LabbÃ©",
        "Matthew C. H. Lee",
        "Erwan Le Jumeau de Kergaradec",
        "Amir Mahla",
        "Avshalom Manevich",
        "Adrien Maret",
        "Charles Masson",
        "RafaÃ«l Maurin",
        "Arturo Mena",
        "Philippe Modard",
        "Axel Moyal",
        "Axel Nguyen Kerbel",
        "Julien Revelle",
        "Mats L. Richter",
        "MarÃ­a Santos",
        "Laurent Sifre",
        "Maxime Theillard",
        "Marc Thibault",
        "Louis Thiry",
        "LÃ©o Tronchon",
        "Nicolas Usunier",
        "Tony Wu"
      ],
      "abstract": "We present Surfer-H, a cost-efficient web agent that integrates Vision-Language Models (VLM) to perform user-defined tasks on the web. We pair it with Holo1, a new open-weight collection of VLMs specialized in web navigation and information extraction. Holo1 was trained on carefully curated data sources, including open-access web content, synthetic examples, and self-produced agentic data. Holo1 tops generalist User Interface (UI) benchmarks as well as our new web UI localization benchmark, WebClick. When powered by Holo1, Surfer-H achieves a 92.2% state-of-the-art performance on WebVoyager, striking a Pareto-optimal balance between accuracy and cost-efficiency. To accelerate research advancement in agentic systems, we are open-sourcing both our WebClick evaluation dataset and the Holo1 model weights.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† Surfer-Hï¼Œè¿™æ˜¯ä¸€ç§æˆæœ¬æ•ˆç›Šæé«˜çš„ç½‘ç»œæ™ºèƒ½ä½“ (Web Agent)ï¼Œå®ƒé€šè¿‡é›†æˆè§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Models, VLM) æ¥æ‰§è¡Œå„ç±»ç½‘é¡µä»»åŠ¡ã€‚ä¸ä¹‹é…å¥—çš„æ˜¯ Holo1ï¼Œä¸€ä¸ªä¸“æ³¨äºç½‘é¡µå¯¼èˆªå’Œä¿¡æ¯æå–çš„æ–°å‹å¼€æºæƒé‡ VLM ç³»åˆ—ï¼Œå…¶è®­ç»ƒåŸºäºå¼€æ”¾ç½‘é¡µå†…å®¹ã€åˆæˆç¤ºä¾‹å’Œè‡ªä¸»ç”Ÿæˆçš„æ™ºèƒ½ä½“æ•°æ®ã€‚Holo1 åœ¨é€šç”¨ç”¨æˆ·ç•Œé¢ (UI) åŸºå‡†æµ‹è¯•ä»¥åŠæ–°æå‡ºçš„ç½‘é¡µ UI å®šä½åŸºå‡† WebClick ä¸­å‡ä½å±…å‰åˆ—ã€‚åœ¨ Holo1 çš„é©±åŠ¨ä¸‹ï¼ŒSurfer-H åœ¨ WebVoyager åŸºå‡†ä¸Šè¾¾åˆ°äº† 92.2% çš„ state-of-the-art æ€§èƒ½ï¼ŒæˆåŠŸåœ¨å‡†ç¡®ç‡ä¸æˆæœ¬æ•ˆç›Šä¹‹é—´å®ç°äº†å¸•ç´¯æ‰˜æœ€ä¼˜ (Pareto-optimal) çš„å¹³è¡¡ã€‚è¯¥ç ”ç©¶æœ€ç»ˆå¼€æºäº† WebClick è¯„ä¼°æ•°æ®é›†å’Œ Holo1 æ¨¡å‹æƒé‡ï¼Œä»¥ä¿ƒè¿›è‡ªä¸»æ™ºèƒ½ä½“ç³»ç»Ÿçš„ç ”ç©¶ä¸å‘å±•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Alphabetical order",
      "pdf_url": "https://arxiv.org/pdf/2506.02865v2",
      "published_date": "2025-06-03 13:29:03 UTC",
      "updated_date": "2025-06-11 09:33:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:04:22.685681+00:00"
    },
    {
      "arxiv_id": "2506.02864v1",
      "title": "BNPO: Beta Normalization Policy Optimization",
      "title_zh": "BNPOï¼šBeta å½’ä¸€åŒ–ç­–ç•¥ä¼˜åŒ–",
      "authors": [
        "Changyi Xiao",
        "Mengdi Zhang",
        "Yixin Cao"
      ],
      "abstract": "Recent studies, including DeepSeek-R1 and Kimi-k1.5, have demonstrated that reinforcement learning with rule-based, binary-valued reward functions can significantly enhance the reasoning capabilities of large language models. These models primarily utilize REINFORCE-based policy optimization techniques, such as REINFORCE with baseline and group relative policy optimization (GRPO). However, a key limitation remains: current policy optimization methods either neglect reward normalization or employ static normalization strategies, which fail to adapt to the dynamic nature of policy updates during training. This may result in unstable gradient estimates and hinder training stability. To address this issue, we propose Beta Normalization Policy Optimization (BNPO), a novel policy optimization method that adaptively normalizes rewards using a Beta distribution with dynamically updated parameters. BNPO aligns the normalization with the changing policy distribution, enabling more precise and lower-variance gradient estimation, which in turn promotes stable training dynamics. We provide theoretical analysis demonstrating BNPO's variance-reducing properties and show that it generalizes both REINFORCE and GRPO under binary-valued reward settings. Furthermore, we introduce an advantage decomposition mechanism to extend BNPO's applicability to more complex reward systems. Experimental results confirm that BNPO achieves state-of-the-art performance among policy optimization methods on reasoning tasks. The code is available at https://github.com/changyi7231/BNPO.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ DeepSeek-R1 å’Œ Kimi-k1.5 ç­‰æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­é¢ä¸´çš„æ¢¯åº¦ä¼°è®¡ä¸ç¨³å®šé—®é¢˜ï¼Œæå‡ºäº† BNPO (Beta Normalization Policy Optimization) æ–¹æ³•ã€‚ç°æœ‰çš„ REINFORCE å’Œ GRPO ç­‰æŠ€æœ¯é€šå¸¸é‡‡ç”¨é™æ€å¥–åŠ±å½’ä¸€åŒ–ï¼Œéš¾ä»¥é€‚åº”åŠ¨æ€çš„ç­–ç•¥æ›´æ–°ï¼Œè€Œ BNPO é€šè¿‡åŠ¨æ€æ›´æ–°å‚æ•°çš„ Beta distribution å®ç°è‡ªé€‚åº”å½’ä¸€åŒ–ã€‚è¿™ç§æ–¹æ³•ä½¿å½’ä¸€åŒ–è¿‡ç¨‹ä¸ç­–ç•¥åˆ†å¸ƒçš„æ¼”å˜ä¿æŒä¸€è‡´ï¼Œä»è€Œæä¾›äº†æ›´ç²¾ç¡®ä¸”ä½æ–¹å·®çš„æ¢¯åº¦ä¼°è®¡ï¼Œç¡®ä¿äº†è®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒBNPO å…·å¤‡æ˜¾è‘—çš„ variance-reducing ç‰¹æ€§ï¼Œå¹¶ä¸”åœ¨äºŒå…ƒå¥–åŠ±åœºæ™¯ä¸‹æ˜¯å¯¹ä¼ ç»Ÿæ–¹æ³•çš„æ³›åŒ–ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥ advantage decomposition æœºåˆ¶ï¼ŒBNPO èƒ½å¤Ÿèƒœä»»æ›´å¤æ‚çš„å¥–åŠ±ç³»ç»Ÿã€‚å®éªŒç»“æœè¯å®ï¼ŒBNPO åœ¨å¤šé¡¹æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº† state-of-the-art çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02864v1",
      "published_date": "2025-06-03 13:28:57 UTC",
      "updated_date": "2025-06-03 13:28:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:04:19.325877+00:00"
    },
    {
      "arxiv_id": "2506.02863v2",
      "title": "CapSpeech: Enabling Downstream Applications in Style-Captioned Text-to-Speech",
      "title_zh": "CapSpeechï¼šèµ‹èƒ½åŸºäºé£æ ¼æè¿°çš„æ–‡æœ¬è½¬è¯­éŸ³ä¸‹æ¸¸åº”ç”¨",
      "authors": [
        "Helin Wang",
        "Jiarui Hai",
        "Dading Chong",
        "Karan Thakkar",
        "Tiantian Feng",
        "Dongchao Yang",
        "Junhyeok Lee",
        "Thomas Thebaud",
        "Laureano Moro Velazquez",
        "Jesus Villalba",
        "Zengyi Qin",
        "Shrikanth Narayanan",
        "Mounya Elhiali",
        "Najim Dehak"
      ],
      "abstract": "Recent advancements in generative artificial intelligence have significantly transformed the field of style-captioned text-to-speech synthesis (CapTTS). However, adapting CapTTS to real-world applications remains challenging due to the lack of standardized, comprehensive datasets and limited research on downstream tasks built upon CapTTS. To address these gaps, we introduce CapSpeech, a new benchmark designed for a series of CapTTS-related tasks, including style-captioned text-to-speech synthesis with sound events (CapTTS-SE), accent-captioned TTS (AccCapTTS), emotion-captioned TTS (EmoCapTTS), and text-to-speech synthesis for chat agent (AgentTTS). CapSpeech comprises over 10 million machine-annotated audio-caption pairs and nearly 0.36 million human-annotated audio-caption pairs. In addition, we introduce two new datasets collected and recorded by a professional voice actor and experienced audio engineers, specifically for the AgentTTS and CapTTS-SE tasks. Alongside the datasets, we conduct comprehensive experiments using both autoregressive and non-autoregressive models on CapSpeech. Our results demonstrate high-fidelity and highly intelligible speech synthesis across a diverse range of speaking styles. To the best of our knowledge, CapSpeech is the largest available dataset offering comprehensive annotations for CapTTS-related tasks. The experiments and findings further provide valuable insights into the challenges of developing CapTTS systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CapSpeechï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§£å†³é£æ ¼æè¿°æ–‡æœ¬è½¬è¯­éŸ³(CapTTS)åœ¨å®é™…åº”ç”¨ä¸­é¢ä¸´çš„æ ‡å‡†åŒ–æ•°æ®é›†åŒ®ä¹åŠä¸‹æ¸¸ä»»åŠ¡ç ”ç©¶æœ‰é™ç­‰æŒ‘æˆ˜çš„å…¨æ–°åŸºå‡†ã€‚CapSpeech æ¶µç›–äº†å¸¦å£°éŸ³äº‹ä»¶çš„é£æ ¼æè¿° TTS (CapTTS-SE)ã€å¸¦å£éŸ³æè¿°çš„ TTS (AccCapTTS)ã€å¸¦æƒ…æ„Ÿæè¿°çš„ TTS (EmoCapTTS) ä»¥åŠé¢å‘èŠå¤©æ™ºèƒ½ä½“çš„ TTS (AgentTTS) ç­‰å…³é”®ä»»åŠ¡ã€‚è¯¥åŸºå‡†è§„æ¨¡å®å¤§ï¼ŒåŒ…å«è¶…è¿‡ 1000 ä¸‡ä¸ªæœºå™¨æ ‡æ³¨åŠè¿‘ 36 ä¸‡ä¸ªäººå·¥æ ‡æ³¨çš„è¯­éŸ³-æè¿°å¯¹ï¼Œæ˜¯ç›®å‰å·²çŸ¥æ ‡æ³¨æœ€å…¨é¢çš„ CapTTS ç›¸å…³ä»»åŠ¡æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜é’ˆå¯¹ AgentTTS å’Œ CapTTS-SE å½•åˆ¶äº†ç”±ä¸“ä¸šäººå‘˜å®Œæˆçš„æ–°æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨è‡ªå›å½’(Autoregressive)ä¸éè‡ªå›å½’(Non-autoregressive)æ¨¡å‹éªŒè¯äº†å…¶åœ¨å¤šæ ·åŒ–è¯´è¯é£æ ¼ä¸‹å®ç°é«˜ä¿çœŸã€é«˜æ¸…æ™°åº¦è¯­éŸ³åˆæˆçš„èƒ½åŠ›ã€‚è¯¥é¡¹å·¥ä½œä¸º CapTTS ç³»ç»Ÿçš„å¼€å‘æä¾›äº†å®è´µè§è§£ï¼Œæ˜¾è‘—æ¨åŠ¨äº†è¯­éŸ³åˆæˆæŠ€æœ¯åœ¨å¤æ‚ä¸‹æ¸¸åœºæ™¯ä¸­çš„åº”ç”¨ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02863v2",
      "published_date": "2025-06-03 13:28:55 UTC",
      "updated_date": "2025-09-26 13:07:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:04:15.021880+00:00"
    },
    {
      "arxiv_id": "2506.02860v1",
      "title": "Tru-POMDP: Task Planning Under Uncertainty via Tree of Hypotheses and Open-Ended POMDPs",
      "title_zh": "Tru-POMDPï¼šåŸºäºå‡è®¾æ ‘ä¸å¼€æ”¾å¼ POMDP çš„ä¸ç¡®å®šæ€§ä»»åŠ¡è§„åˆ’",
      "authors": [
        "Wenjing Tang",
        "Xinyu He",
        "Yongxi Huang",
        "Yunxiao Xiao",
        "Cewu Lu",
        "Panpan Cai"
      ],
      "abstract": "Task planning under uncertainty is essential for home-service robots operating in the real world. Tasks involve ambiguous human instructions, hidden or unknown object locations, and open-vocabulary object types, leading to significant open-ended uncertainty and a boundlessly large planning space. To address these challenges, we propose Tru-POMDP, a planner that combines structured belief generation using Large Language Models (LLMs) with principled POMDP planning. Tru-POMDP introduces a hierarchical Tree of Hypotheses (TOH), which systematically queries an LLM to construct high-quality particle beliefs over possible world states and human goals. We further formulate an open-ended POMDP model that enables rigorous Bayesian belief tracking and efficient belief-space planning over these LLM-generated hypotheses. Experiments on complex object rearrangement tasks across diverse kitchen environments show that Tru-POMDP significantly outperforms state-of-the-art LLM-based and LLM-tree-search hybrid planners, achieving higher success rates with significantly better plans, stronger robustness to ambiguity and occlusion, and greater planning efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Tru-POMDPï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è§£å†³å®¶åº­æœåŠ¡æœºå™¨äººåœ¨ç°å®ä¸–ç•Œä¸­é¢ä¸´æŒ‡ä»¤æ¨¡ç³Šã€ç‰©ä½“ä½ç½®æœªçŸ¥åŠå¼€æ”¾è¯æ±‡(open-vocabulary)æŒ‘æˆ˜çš„ä»»åŠ¡è§„åˆ’å™¨ã€‚Tru-POMDP å°†å¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆçš„ç»“æ„åŒ–ä¿¡å¿µ(structured belief)ä¸åŸåˆ™æ€§ POMDP è§„åˆ’ç›¸ç»“åˆï¼Œå¹¶å¼•å…¥äº†å±‚æ¬¡åŒ–çš„å‡è®¾æ ‘(Tree of Hypotheses, TOH)æ¥æ„å»ºé«˜è´¨é‡çš„ç²’å­ä¿¡å¿µ(particle beliefs)ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ¶å®šå¼€æ”¾å¼(open-ended) POMDP æ¨¡å‹ï¼Œå®ç°äº†ä¸¥è°¨çš„è´å¶æ–¯ä¿¡å¿µè¿½è¸ª(Bayesian belief tracking)ä»¥åŠåœ¨å‡è®¾ç©ºé—´å†…çš„é«˜æ•ˆè§„åˆ’ã€‚åœ¨å¤æ‚ç‰©ä½“é‡æ–°æ’åˆ—ä»»åŠ¡çš„å®éªŒä¸­ï¼ŒTru-POMDP çš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºäº LLM åŠå…¶æ··åˆæœç´¢çš„è§„åˆ’å™¨ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æå‡ä»»åŠ¡æˆåŠŸç‡çš„åŒæ—¶ï¼Œå¯¹äºç¯å¢ƒæ­§ä¹‰å’Œé®æŒ¡å±•ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ï¼Œå¹¶å…·å¤‡æ›´é«˜çš„è§„åˆ’æ•ˆç‡ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02860v1",
      "published_date": "2025-06-03 13:26:08 UTC",
      "updated_date": "2025-06-03 13:26:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:04:46.186162+00:00"
    },
    {
      "arxiv_id": "2506.02859v1",
      "title": "ATAG: AI-Agent Application Threat Assessment with Attack Graphs",
      "title_zh": "ATAGï¼šåŸºäºæ”»å‡»å›¾çš„ AI æ™ºèƒ½ä½“åº”ç”¨å¨èƒè¯„ä¼°",
      "authors": [
        "Parth Atulbhai Gandhi",
        "Akansha Shukla",
        "David Tayouri",
        "Beni Ifland",
        "Yuval Elovici",
        "Rami Puzis",
        "Asaf Shabtai"
      ],
      "abstract": "Evaluating the security of multi-agent systems (MASs) powered by large language models (LLMs) is challenging, primarily because of the systems' complex internal dynamics and the evolving nature of LLM vulnerabilities. Traditional attack graph (AG) methods often lack the specific capabilities to model attacks on LLMs. This paper introduces AI-agent application Threat assessment with Attack Graphs (ATAG), a novel framework designed to systematically analyze the security risks associated with AI-agent applications. ATAG extends the MulVAL logic-based AG generation tool with custom facts and interaction rules to accurately represent AI-agent topologies, vulnerabilities, and attack scenarios. As part of this research, we also created the LLM vulnerability database (LVD) to initiate the process of standardizing LLM vulnerabilities documentation. To demonstrate ATAG's efficacy, we applied it to two multi-agent applications. Our case studies demonstrated the framework's ability to model and generate AGs for sophisticated, multi-step attack scenarios exploiting vulnerabilities such as prompt injection, excessive agency, sensitive information disclosure, and insecure output handling across interconnected agents. ATAG is an important step toward a robust methodology and toolset to help understand, visualize, and prioritize complex attack paths in multi-agent AI systems (MAASs). It facilitates proactive identification and mitigation of AI-agent threats in multi-agent applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(MASs)å®‰å…¨æ€§è¯„ä¼°éš¾é¢˜ï¼Œæå‡ºäº†åä¸ºATAGçš„AIæ™ºèƒ½ä½“åº”ç”¨å¨èƒè¯„ä¼°æ¡†æ¶ã€‚ATAGé€šè¿‡æ‰©å±•åŸºäºé€»è¾‘çš„æ”»å‡»å›¾(Attack Graph)ç”Ÿæˆå·¥å…·MulVALï¼Œå¼•å…¥è‡ªå®šä¹‰äº‹å®ä¸äº¤äº’è§„åˆ™ï¼Œå®ç°äº†å¯¹AIæ™ºèƒ½ä½“æ‹“æ‰‘ç»“æ„ã€æ¼æ´åŠæ”»å‡»åœºæ™¯çš„ç²¾ç¡®å»ºæ¨¡ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜åŒæ­¥å»ºç«‹äº†å¤§è¯­è¨€æ¨¡å‹æ¼æ´æ•°æ®åº“(LVD)ï¼Œæ—¨åœ¨æ¨åŠ¨LLMæ¼æ´è®°å½•çš„æ ‡å‡†åŒ–è¿›ç¨‹ã€‚é€šè¿‡åœ¨ä¸¤ä¸ªå¤šæ™ºèƒ½ä½“åº”ç”¨ä¸Šçš„æ¡ˆä¾‹ç ”ç©¶ï¼ŒATAGå±•ç¤ºäº†å…¶å¯¹æ¶‰åŠprompt injectionã€excessive agencyã€sensitive information disclosureä»¥åŠinsecure output handlingç­‰å¤æ‚å¤šæ­¥æ”»å‡»è·¯å¾„çš„ç”Ÿæˆä¸åˆ†æèƒ½åŠ›ã€‚è¯¥æ¡†æ¶ä¸ºç†è§£ã€å¯è§†åŒ–å’Œä¼˜å…ˆå¤„ç†å¤šæ™ºèƒ½ä½“AIç³»ç»Ÿ(MAASs)ä¸­çš„æ”»å‡»è·¯å¾„æä¾›äº†é‡è¦å·¥å…·ï¼Œæœ‰åŠ©äºä¸»åŠ¨è¯†åˆ«å¹¶ç¼“è§£AIæ™ºèƒ½ä½“åº”ç”¨ä¸­çš„å®‰å…¨å¨èƒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02859v1",
      "published_date": "2025-06-03 13:25:40 UTC",
      "updated_date": "2025-06-03 13:25:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:04:43.285693+00:00"
    },
    {
      "arxiv_id": "2506.02858v2",
      "title": "DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization",
      "title_zh": "DGMOï¼šåŸºäºæ‰©æ•£å¼•å¯¼æ©ç ä¼˜åŒ–çš„å…è®­ç»ƒéŸ³é¢‘æºåˆ†ç¦»",
      "authors": [
        "Geonyoung Lee",
        "Geonhee Han",
        "Paul Hongsuck Seo"
      ],
      "abstract": "Language-queried Audio Source Separation (LASS) enables open-vocabulary sound separation via natural language queries. While existing methods rely on task-specific training, we explore whether pretrained diffusion models, originally designed for audio generation, can inherently perform separation without further training. In this study, we introduce a training-free framework leveraging generative priors for zero-shot LASS. Analyzing naive adaptations, we identify key limitations arising from modality-specific challenges. To address these issues, we propose Diffusion-Guided Mask Optimization (DGMO), a test-time optimization framework that refines spectrogram masks for precise, input-aligned separation. Our approach effectively repurposes pretrained diffusion models for source separation, achieving competitive performance without task-specific supervision. This work expands the application of diffusion models beyond generation, establishing a new paradigm for zero-shot audio separation. The code is available at: https://wltschmrz.github.io/DGMO/",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹åœ¨æ— éœ€ç‰¹å®šè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•æ‰§è¡Œé›¶æ ·æœ¬è¯­è¨€æŸ¥è¯¢éŸ³é¢‘æºåˆ†ç¦»ï¼ˆLanguage-queried Audio Source Separation, LASSï¼‰ä»»åŠ¡ã€‚é’ˆå¯¹åŸç”Ÿé€‚é…ä¸­å­˜åœ¨çš„å±€é™æ€§ï¼Œä½œè€…æå‡ºäº†DGMOï¼ˆDiffusion-Guided Mask Optimizationï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æµ‹è¯•æ—¶ä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–é¢‘è°±æ©ç æ¥å®ç°ç²¾ç¡®ä¸”ä¸è¾“å…¥å¯¹é½çš„éŸ³é¢‘åˆ†ç¦»ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆåœ°åˆ©ç”¨äº†ç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ï¼Œåœ¨æ²¡æœ‰ä»»åŠ¡ç‰¹å®šç›‘ç£çš„æƒ…å†µä¸‹ï¼ŒäºéŸ³é¢‘æºåˆ†ç¦»ä»»åŠ¡ä¸­å–å¾—äº†æå…·ç«äº‰åŠ›çš„è¡¨ç°ã€‚è¿™é¡¹å·¥ä½œä¸ä»…æ‰©å±•äº†æ‰©æ•£æ¨¡å‹åœ¨éŸ³é¢‘ç”Ÿæˆä¹‹å¤–çš„åº”ç”¨è¾¹ç•Œï¼Œè¿˜ä¸ºé›¶æ ·æœ¬éŸ³é¢‘åˆ†ç¦»é¢†åŸŸå»ºç«‹äº†ä¸€ç§å…¨æ–°çš„èŒƒå¼ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.02858v2",
      "published_date": "2025-06-03 13:24:57 UTC",
      "updated_date": "2025-06-05 04:46:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:04:48.693197+00:00"
    },
    {
      "arxiv_id": "2506.02842v1",
      "title": "Sheaves Reloaded: A Directional Awakening",
      "title_zh": "å±‚è®ºé‡è£…ä¸Šé˜µï¼šæœ‰å‘æ€§çš„è§‰é†’",
      "authors": [
        "Stefano Fiorini",
        "Hakan Aktas",
        "Iulia Duta",
        "Stefano Coniglio",
        "Pietro Morerio",
        "Alessio Del Bue",
        "Pietro LiÃ²"
      ],
      "abstract": "Sheaf Neural Networks (SNNs) represent a powerful generalization of Graph Neural Networks (GNNs) that significantly improve our ability to model complex relational data. While directionality has been shown to substantially boost performance in graph learning tasks and is key to many real-world applications, existing SNNs fall short in representing it. To address this limitation, we introduce the Directed Cellular Sheaf, a special type of cellular sheaf designed to explicitly account for edge orientation. Building on this structure, we define a new sheaf Laplacian, the Directed Sheaf Laplacian, which captures both the graph's topology and its directional information. This operator serves as the backbone of the Directed Sheaf Neural Network (DSNN), the first SNN model to embed a directional bias into its architecture. Extensive experiments on nine real-world benchmarks show that DSNN consistently outperforms baseline methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Sheaf Neural Networks (SNNs) åœ¨å»ºæ¨¡å¤æ‚å…³ç³»æ•°æ®ä¸­çš„ä¼˜åŠ¿ï¼Œå¹¶æŒ‡å‡ºå½“å‰æ¨¡å‹åœ¨è¡¨å¾æœ‰å‘æ€§(directionality)æ–¹é¢çš„å±€é™æ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œä½œè€…å¼•å…¥äº† Directed Cellular Sheaf ç»“æ„ï¼Œæ—¨åœ¨æ˜¾å¼æ•æ‰å›¾ä¸­çš„è¾¹å®šå‘ä¿¡æ¯ã€‚åŸºäºè¯¥ç»“æ„ï¼Œç ”ç©¶å®šä¹‰äº†å…¨æ–°çš„ Directed Sheaf Laplacian ç®—å­ï¼Œèƒ½å¤ŸåŒæ—¶æ•´åˆå›¾æ‹“æ‰‘ä¸æ–¹å‘ç‰¹å¾ã€‚è¯¥ç®—å­ä½œä¸º Directed Sheaf Neural Network (DSNN) çš„æ ¸å¿ƒæ¶æ„ï¼Œå®ç°äº†é¦–ä¸ªå°†æ–¹å‘åå·®(directional bias)åµŒå…¥æ¨¡å‹çš„è®¾è®¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDSNN åœ¨ä¹ä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ€§èƒ½ä¸€è‡´è¶…è¶Šäº†ç°æœ‰çš„åŸºçº¿æ¨¡å‹ï¼Œä¸ºå¤„ç†å…·æœ‰æ–¹å‘æ€§çš„å¤æ‚å›¾æ•°æ®æä¾›äº†æœ‰æ•ˆçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02842v1",
      "published_date": "2025-06-03 13:13:56 UTC",
      "updated_date": "2025-06-03 13:13:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:04:39.392571+00:00"
    },
    {
      "arxiv_id": "2506.02839v1",
      "title": "DeepShop: A Benchmark for Deep Research Shopping Agents",
      "title_zh": "DeepShopï¼šæ·±åº¦è°ƒç ”å‹è´­ç‰©æ™ºèƒ½ä½“è¯„ä¼°åŸºå‡†",
      "authors": [
        "Yougang Lyu",
        "Xiaoyu Zhang",
        "Lingyong Yan",
        "Maarten de Rijke",
        "Zhaochun Ren",
        "Xiuying Chen"
      ],
      "abstract": "Web agents for online shopping have shown great promise in automating user interactions across e-commerce platforms. Benchmarks for assessing such agents do not reflect the complexity of real-world shopping scenarios, as they often consist of overly simple queries with deterministic paths, such as \"Find iPhone 15.\" Real shopping scenarios are inherently more layered, involving multi-dimensional product attributes, search filters, and user-specific sorting preferences. To address this gap, we introduce DeepShop, a benchmark designed to evaluate web agents in complex and realistic online shopping environments. DeepShop comprises three key components. (1) Query diversity evolution: Starting from real user queries, we generate diverse queries across five popular online shopping domains. (2) Query complexity evolution: We further evolve these queries to increase complexity, considering product attributes, search filters, and sorting preferences, and classify them into three levels: easy, medium, and hard, based on the number of evolutions. (3) Fine-grained and holistic evaluation: We propose an automated evaluation framework that assesses agent performance in terms of fine-grained aspects (product attributes, search filters, and sorting preferences) and reports the overall success rate through holistic evaluation. We conduct a systematic evaluation of retrieval-augmented generation (RAG) methods, web agents, and deep research systems. Results show that RAG struggles with complex queries due to its lack of web interaction, while other methods face significant challenges with filters and sorting preferences, leading to low overall success rates. We also perform cross-category, complexity-based evaluations and error analyses to support the advancement of deep research shopping agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DeepShopï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºè¯„ä¼°åœ¨å¤æ‚ç°å®åœ¨çº¿è´­ç‰©ç¯å¢ƒä¸‹çš„Webæ™ºèƒ½ä½“(Web agents)è€Œè®¾è®¡çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºå‡†æµ‹è¯•ä»»åŠ¡è¿‡äºç®€å•ã€æ— æ³•åæ˜ å¤šç»´åº¦å±æ€§å’Œæœç´¢ç­›é€‰ç­‰çœŸå®è´­ç‰©åœºæ™¯å¤æ‚æ€§çš„é—®é¢˜ã€‚DeepShopä¸»è¦åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šé¦–å…ˆæ˜¯é€šè¿‡äº”å¤§çƒ­é—¨è´­ç‰©é¢†åŸŸç”Ÿæˆå¤šæ ·åŒ–çš„åˆå§‹æŸ¥è¯¢ï¼›å…¶æ¬¡æ˜¯å°†æŸ¥è¯¢æ ¹æ®äº§å“å±æ€§ã€æœç´¢è¿‡æ»¤å™¨(search filters)å’Œæ’åºåå¥½(sorting preferences)æ¼”åŒ–ä¸ºç®€å•ã€ä¸­ç­‰å’Œå›°éš¾ä¸‰ä¸ªéš¾åº¦ç­‰çº§ï¼›æœ€åæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„ç»†ç²’åº¦åŠæ•´ä½“è¯„ä¼°æ¡†æ¶ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨è¯¥åŸºå‡†å¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æ–¹æ³•ã€Webæ™ºèƒ½ä½“å’Œæ·±åº¦ç ”ç©¶ç³»ç»Ÿ(deep research systems)è¿›è¡Œäº†ç³»ç»Ÿæ€§è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼ŒRAGç”±äºç¼ºä¹Webäº¤äº’èƒ½åŠ›åœ¨å¤„ç†å¤æ‚æŸ¥è¯¢æ—¶è¡¨ç°ä¸ä½³ï¼Œè€Œå…¶ä»–æ–¹æ³•åœ¨å¤„ç†è¿‡æ»¤å™¨å’Œæ’åºåå¥½æ–¹é¢ä¹Ÿé¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œå¯¼è‡´æ•´ä½“æˆåŠŸç‡è¾ƒä½ã€‚è¯¥ç ”ç©¶é€šè¿‡è·¨ç±»åˆ«ã€åŸºäºå¤æ‚æ€§çš„è¯„ä¼°å’Œé”™è¯¯åˆ†æï¼Œä¸ºæ¨åŠ¨æ·±åº¦ç ”ç©¶è´­ç‰©æ™ºèƒ½ä½“(deep research shopping agents)çš„å‘å±•æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02839v1",
      "published_date": "2025-06-03 13:08:17 UTC",
      "updated_date": "2025-06-03 13:08:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:04:36.283879+00:00"
    },
    {
      "arxiv_id": "2506.02838v1",
      "title": "TaxAgent: How Large Language Model Designs Fiscal Policy",
      "title_zh": "TaxAgentï¼šå¤§è¯­è¨€æ¨¡å‹å¦‚ä½•è®¾è®¡è´¢æ”¿æ”¿ç­–",
      "authors": [
        "Jizhou Wang",
        "Xiaodan Fang",
        "Lei Huang",
        "Yongfeng Huang"
      ],
      "abstract": "Economic inequality is a global challenge, intensifying disparities in education, healthcare, and social stability. Traditional systems like the U.S. federal income tax reduce inequality but lack adaptability. Although models like the Saez Optimal Taxation adjust dynamically, they fail to address taxpayer heterogeneity and irrational behavior. This study introduces TaxAgent, a novel integration of large language models (LLMs) with agent-based modeling (ABM) to design adaptive tax policies. In our macroeconomic simulation, heterogeneous H-Agents (households) simulate real-world taxpayer behaviors while the TaxAgent (government) utilizes LLMs to iteratively optimize tax rates, balancing equity and productivity. Benchmarked against Saez Optimal Taxation, U.S. federal income taxes, and free markets, TaxAgent achieves superior equity-efficiency trade-offs. This research offers a novel taxation solution and a scalable, data-driven framework for fiscal policy evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…¨çƒé¢ä¸´çš„ç»æµä¸å¹³ç­‰æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿç¾å›½è”é‚¦æ‰€å¾—ç¨ç³»ç»Ÿç¼ºä¹çµæ´»æ€§ï¼Œä¸” Saez Optimal Taxation æ¨¡å‹éš¾ä»¥åº”å¯¹çº³ç¨äººçš„å¼‚è´¨æ€§(heterogeneity)å’Œéç†æ€§è¡Œä¸ºã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº† TaxAgentï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº† Large Language Models (LLMs) ä¸ Agent-Based Modeling (ABM) çš„åˆ›æ–°æ¡†æ¶ï¼Œç”¨äºè®¾è®¡é€‚åº”æ€§è´¢æ”¿æ”¿ç­–ã€‚åœ¨å®è§‚ç»æµæ¨¡æ‹Ÿä¸­ï¼Œå¼‚è´¨æ€§çš„ H-Agents ä»£è¡¨å®¶åº­æ¨¡æ‹ŸçœŸå®çº³ç¨è¡Œä¸ºï¼Œè€Œ TaxAgent åˆ™ä½œä¸ºæ”¿åºœåˆ©ç”¨ LLMs è¿­ä»£ä¼˜åŒ–ç¨ç‡ï¼Œä»¥å¹³è¡¡å…¬å¹³ä¸ç”Ÿäº§åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ Saez Optimal Taxationã€ç¾å›½è”é‚¦æ‰€å¾—ç¨åŠè‡ªç”±å¸‚åœºç›¸æ¯”ï¼ŒTaxAgent åœ¨å…¬å¹³ä¸æ•ˆç‡(equity-efficiency)çš„æƒè¡¡ä¸Šè¡¨ç°æ›´ä¼˜ã€‚è¯¥ç ”ç©¶ä¸ä»…æä¾›äº†ä¸€ç§æ–°çš„ç¨æ”¶è§£å†³æ–¹æ¡ˆï¼Œè¿˜ä¸ºè´¢æ”¿æ”¿ç­–è¯„ä¼°æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ä¸”æ•°æ®é©±åŠ¨çš„æ¡†æ¶ã€‚",
      "categories": [
        "cs.AI",
        "econ.GN"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted as oral presentation at ICME 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.02838v1",
      "published_date": "2025-06-03 13:06:19 UTC",
      "updated_date": "2025-06-03 13:06:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:04:51.096152+00:00"
    },
    {
      "arxiv_id": "2506.05386v3",
      "title": "Leaps Beyond the Seen: Reinforced Reasoning Augmented Generation for Clinical Notes",
      "title_zh": "æ´è§æœªè§ï¼šé¢å‘ä¸´åºŠè®°å½•çš„å¼ºåŒ–æ¨ç†å¢å¼ºç”Ÿæˆ",
      "authors": [
        "Lo Pang-Yun Ting",
        "Chengshuai Zhao",
        "Yu-Hua Zeng",
        "Yuan Jee Lim",
        "Kun-Ta Chuang",
        "Huan Liu"
      ],
      "abstract": "Clinical note generation aims to produce free-text summaries of a patient's condition and diagnostic process, with discharge instructions being a representative long-form example. While recent LLM-based methods pre-trained on general clinical corpora show promise in clinical text generation, they fall short in producing long-form notes from limited patient information. In this paper, we propose ReinRAG, a reinforced reasoning augmented generation (RAG) for long-form discharge instructions based on pre-admission information. ReinRAG retrieves reasoning paths from a medical knowledge graph to provide explicit semantic guidance to the LLM. To bridge the information gap, we propose group-based retriever optimization (GRO) which improves retrieval quality with group-normalized rewards, encouraging reasoning leaps for deeper inference by the LLM. Comprehensive experiments on the real-world dataset show that ReinRAG outperforms baselines in both clinical efficacy and natural language generation metrics. Further analysis reveals that ReinRAG fills semantic gaps in sparse input scenarios, and retrieved reasoning paths help LLMs avoid clinical misinterpretation by focusing on key evidence and following coherent reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨åŸºäºæœ‰é™ä¿¡æ¯ç”Ÿæˆé•¿ç¯‡å‡ºé™¢æŒ‡å¯¼ç­‰ä¸´åºŠç¬”è®°æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºReinRAGçš„å¼ºåŒ–æ¨ç†å¢å¼ºç”Ÿæˆ(Reinforced Reasoning Augmented Generation)æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ä»åŒ»å­¦çŸ¥è¯†å›¾ä¸­æ£€ç´¢æ¨ç†è·¯å¾„ï¼Œä¸ºæ¨¡å‹ç”Ÿæˆæä¾›æ˜¾å¼çš„è¯­ä¹‰å¼•å¯¼ã€‚ä¸ºäº†è§£å†³è¾“å…¥ä¿¡æ¯ç¨€ç–å¯¼è‡´çš„è¯­ä¹‰ç¼ºå£ï¼Œç ”ç©¶å¼•å…¥äº†åŸºäºç»„çš„æ£€ç´¢å™¨ä¼˜åŒ–(Group-based Retriever Optimization, GRO)ï¼Œåˆ©ç”¨ç»„å½’ä¸€åŒ–å¥–åŠ±æå‡æ£€ç´¢è´¨é‡ï¼Œå¹¶æ”¯æŒLLMè¿›è¡Œæ›´æ·±å±‚æ¬¡çš„é€»è¾‘æ¨ç†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒReinRAGåœ¨ä¸´åºŠæœ‰æ•ˆæ€§å’Œè‡ªç„¶è¯­è¨€ç”ŸæˆæŒ‡æ ‡ä¸Šå‡æ˜¾è‘—è¶…è¶Šäº†åŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¼•å¯¼æ¨¡å‹å…³æ³¨å…³é”®è¯æ®å’Œè¿è´¯çš„æ¨ç†è·¯å¾„ï¼Œæœ‰æ•ˆå‡å°‘äº†ä¸´åºŠè¯¯è¯»çš„å‘ç”Ÿï¼Œä¸ºé«˜è´¨é‡ä¸´åºŠç¬”è®°çš„è‡ªåŠ¨åŒ–ç”Ÿæˆæä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.05386v3",
      "published_date": "2025-06-03 12:59:52 UTC",
      "updated_date": "2025-08-09 19:31:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:04:58.087599+00:00"
    },
    {
      "arxiv_id": "2506.03229v2",
      "title": "Bridging Weakly-Supervised Learning and VLM Distillation: Noisy Partial Label Learning for Efficient Downstream Adaptation",
      "title_zh": "è¿æ¥å¼±ç›‘ç£å­¦ä¹ ä¸ VLM è’¸é¦ï¼šé¢å‘é«˜æ•ˆä¸‹æ¸¸é€‚é…çš„å™ªå£°éƒ¨åˆ†æ ‡ç­¾å­¦ä¹ ",
      "authors": [
        "Qian-Wei Wang",
        "Yuqiu Xie",
        "Letian Zhang",
        "Zimo Liu",
        "Shu-Tao Xia"
      ],
      "abstract": "In the context of noisy partial label learning (NPLL), each training sample is associated with a set of candidate labels annotated by multiple noisy annotators. With the emergence of high-performance pre-trained vision-language models (VLMs) such as CLIP, LLaVA and GPT-4V, the direction of using these models to replace time-consuming manual annotation workflows and achieve ``manual-annotation-free\" training for downstream tasks has become a highly promising research avenue. This paper focuses on learning from noisy partial labels annotated by pre-trained VLMs and proposes an innovative collaborative consistency regularization (Co-Reg) method. Unlike the symmetric noise primarily addressed in traditional noisy label learning, the noise generated by pre-trained models is instance-dependent, embodying the underlying patterns of the pre-trained models themselves, which significantly increases the learning difficulty for the model. To address this, we simultaneously train two neural networks that implement collaborative purification of training labels through a ``Co-Pseudo-Labeling\" mechanism, while enforcing consistency regularization constraints in both the label space and feature representation space. Specifically, we construct multiple anti-overfitting mechanisms that efficiently mine latent information from noisy partially labeled samples including alternating optimization of contrastive feature representations and pseudo-labels, as well as maintaining prototypical class vectors in the shared feature space.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Noisy Partial Label Learning (NPLL) æŒ‘æˆ˜ï¼Œæ¢ç´¢åˆ©ç”¨é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) æ›¿ä»£äººå·¥æ ‡æ³¨ä»¥å®ç°ä¸‹æ¸¸ä»»åŠ¡çš„å…æ ‡æ³¨è®­ç»ƒã€‚é’ˆå¯¹é¢„è®­ç»ƒæ¨¡å‹ç”Ÿæˆçš„å…·æœ‰å®ä¾‹ç›¸å…³æ€§çš„å™ªå£°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Collaborative Consistency Regularization (Co-Reg) æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åŒæ—¶è®­ç»ƒä¸¤ä¸ªç¥ç»ç½‘ç»œï¼Œåˆ©ç”¨Co-Pseudo-Labelingæœºåˆ¶å®ç°è®­ç»ƒæ ‡ç­¾çš„åä½œå‡€åŒ–ï¼Œå¹¶åœ¨æ ‡ç­¾ç©ºé—´ä¸ç‰¹å¾è¡¨ç¤ºç©ºé—´åŒæ—¶æ–½åŠ ä¸€è‡´æ€§æ­£åˆ™åŒ–çº¦æŸã€‚æ­¤å¤–ï¼Œç ”ç©¶è®¾è®¡äº†åŒ…æ‹¬å¯¹æ¯”ç‰¹å¾è¡¨ç¤ºä¸ä¼ªæ ‡ç­¾äº¤æ›¿ä¼˜åŒ–ã€ä»¥åŠåœ¨å…±äº«ç‰¹å¾ç©ºé—´ç»´æŠ¤Prototypical Class Vectorsåœ¨å†…çš„å¤šç§æŠ—è¿‡æ‹Ÿåˆæœºåˆ¶ã€‚è¯¥ç ”ç©¶é€šè¿‡é«˜æ•ˆæŒ–æ˜å™ªå£°éƒ¨åˆ†æ ‡æ³¨æ ·æœ¬ä¸­çš„æ½œåœ¨ä¿¡æ¯ï¼Œä¸ºè§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡çš„å¿«é€Ÿé€‚é…æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03229v2",
      "published_date": "2025-06-03 12:48:54 UTC",
      "updated_date": "2025-11-10 17:19:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:05:17.700317+00:00"
    },
    {
      "arxiv_id": "2506.04268v1",
      "title": "MUC-G4: Minimal Unsat Core-Guided Incremental Verification for Deep Neural Network Compression",
      "title_zh": "MUC-G4ï¼šåŸºäºæœ€å°ä¸å¯æ»¡è¶³æ ¸å¿ƒå¼•å¯¼çš„æ·±åº¦ç¥ç»ç½‘ç»œå‹ç¼©å¢é‡éªŒè¯",
      "authors": [
        "Jingyang Li",
        "Guoqiang Li"
      ],
      "abstract": "The rapid development of deep learning has led to challenges in deploying neural networks on edge devices, mainly due to their high memory and runtime complexity. Network compression techniques, such as quantization and pruning, aim to reduce this complexity while maintaining accuracy. However, existing incremental verification methods often focus only on quantization and struggle with structural changes. This paper presents MUC-G4 (Minimal Unsat Core-Guided Incremental Verification), a novel framework for incremental verification of compressed deep neural networks. It encodes both the original and compressed networks into SMT formulas, classifies changes, and use \\emph{Minimal Unsat Cores (MUCs)} from the original network to guide efficient verification for the compressed network. Experimental results show its effectiveness in handling quantization and pruning, with high proof reuse rates and significant speedup in verification time compared to traditional methods. MUC-G4 hence offers a promising solution for ensuring the safety and reliability of compressed neural networks in practical applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MUC-G4 (Minimal Unsat Core-Guided Incremental Verification)ï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºå‹ç¼©æ·±åº¦ç¥ç»ç½‘ç»œ(Deep Neural Networks)è®¾è®¡çš„å¢é‡éªŒè¯æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é‡åŒ–(Quantization)å’Œå‰ªæ(Pruning)å¼•å‘çš„ç»“æ„å˜åŒ–æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒMUC-G4 å°†åŸå§‹ä¸å‹ç¼©ç½‘ç»œç¼–ç ä¸º SMT å…¬å¼ï¼Œå¹¶å¯¹ç½‘ç»œå˜åŒ–è¿›è¡Œç»†è‡´åˆ†ç±»ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºåˆ©ç”¨ä»åŸå§‹ç½‘ç»œä¸­æå–çš„æœ€å°ä¸å¯æ»¡è¶³æ ¸å¿ƒ(Minimal Unsat Cores, MUCs)æ¥æŒ‡å¯¼å‹ç¼©ç½‘ç»œçš„éªŒè¯è¿‡ç¨‹ï¼Œä»è€Œå®ç°è¯æ˜çš„é«˜æ•ˆé‡ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMUC-G4 åœ¨å¤„ç†é‡åŒ–å’Œå‰ªæä»»åŠ¡æ—¶ï¼Œå…¶éªŒè¯é€Ÿåº¦è¾ƒä¼ ç»Ÿæ–¹æ³•æœ‰æ˜¾è‘—æå‡ï¼Œå¹¶ä¿æŒäº†æé«˜çš„è¯æ˜é‡ç”¨ç‡ã€‚è¿™ä¸€ç ”ç©¶ä¸ºä¿éšœå®é™…åº”ç”¨ä¸­å‹ç¼©ç¥ç»ç½‘ç»œçš„å®‰å…¨æ€§å’Œå¯é æ€§æä¾›äº†ä¸€ç§æå…·æ½œåŠ›çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.04268v1",
      "published_date": "2025-06-03 12:42:15 UTC",
      "updated_date": "2025-06-03 12:42:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:06:15.080305+00:00"
    },
    {
      "arxiv_id": "2506.02805v1",
      "title": "Optimising the attribute order in Fuzzy Rough Rule Induction",
      "title_zh": "æ¨¡ç³Šç²—ç³™è§„åˆ™å½’çº³ä¸­çš„å±æ€§é¡ºåºä¼˜åŒ–",
      "authors": [
        "Henri Bollaert",
        "Chris Cornelis",
        "Marko PalangetiÄ‡",
        "Salvatore Greco",
        "Roman SÅ‚owiÅ„ski"
      ],
      "abstract": "Interpretability is the next pivotal frontier in machine learning research. In the pursuit of glass box models - as opposed to black box models, like random forests or neural networks - rule induction algorithms are a logical and promising avenue, as the rules can easily be understood by humans. In our previous work, we introduced FRRI, a novel rule induction algorithm based on fuzzy rough set theory. We demonstrated experimentally that FRRI outperformed other rule induction methods with regards to accuracy and number of rules. FRRI leverages a fuzzy indiscernibility relation to partition the data space into fuzzy granules, which are then combined into a minimal covering set of rules. This indiscernibility relation is constructed by removing attributes from rules in a greedy way. This raises the question: does the order of the attributes matter? In this paper, we show that optimising only the order of attributes using known methods from fuzzy rough set theory and classical machine learning does not improve the performance of FRRI on multiple metrics. However, removing a small number of attributes using fuzzy rough feature selection during this step positively affects balanced accuracy and the average rule length.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æ¨¡ç³Šç²—ç³™è§„åˆ™è¯±å¯¼ (Fuzzy Rough Rule Induction, FRRI) ç®—æ³•ä¸­ä¼˜åŒ–å±æ€§é¡ºåºå¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œæ—¨åœ¨æå‡æœºå™¨å­¦ä¹ æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚FRRI ç®—æ³•åŸºäºæ¨¡ç³Šç²—ç³™é›†ç†è®º (fuzzy rough set theory)ï¼Œé€šè¿‡è´ªå©ªåœ°ç§»é™¤å±æ€§æ¥æ„å»ºæ¨¡ç³Šä¸å¯è¾¨å…³ç³» (fuzzy indiscernibility relation)ï¼Œä»è€Œç”Ÿæˆæœ€å°è¦†ç›–è§„åˆ™é›†ã€‚ä½œè€…é€šè¿‡å¤šé¡¹å®éªŒéªŒè¯å‘ç°ï¼Œä»…åˆ©ç”¨å·²çŸ¥çš„æ¨¡ç³Šç²—ç³™é›†å’Œç»å…¸æœºå™¨å­¦ä¹ æ–¹æ³•æ¥ä¼˜åŒ–å±æ€§ç§»é™¤é¡ºåºï¼Œå¹¶ä¸èƒ½æ˜¾è‘—æå‡ FRRI åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šçš„è¡¨ç°ã€‚ç„¶è€Œï¼Œç ”ç©¶è¿›ä¸€æ­¥æŒ‡å‡ºï¼Œå¦‚æœåœ¨è¯¥æ­¥éª¤ä¸­åˆ©ç”¨æ¨¡ç³Šç²—ç³™ç‰¹å¾é€‰æ‹© (fuzzy rough feature selection) ç§»é™¤å°‘é‡å±æ€§ï¼Œåˆ™èƒ½æœ‰æ•ˆæé«˜å¹³è¡¡å‡†ç¡®ç‡ (balanced accuracy) å¹¶ç¼©çŸ­å¹³å‡è§„åˆ™é•¿åº¦ (average rule length)ã€‚è¿™ä¸€ç»“æœè¯æ˜äº†ç‰¹å¾é€‰æ‹©åœ¨ä¼˜åŒ–åŸºäºè§„åˆ™çš„ç»ç’ƒç›’æ¨¡å‹ (glass box models) ä¸­çš„å…³é”®ä½œç”¨ï¼Œä¸ºæ„å»ºæ›´é«˜æ•ˆçš„è§£é‡Šæ€§æ¨¡å‹æä¾›äº†ç†è®ºä¾æ®ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "This is the author's version of the work accepted for publication in Lecture Notes in Computer Science. The final publication is available at Springer via https://doi.org/10.1007/978-3-031-92747-8_16",
      "pdf_url": "https://arxiv.org/pdf/2506.02805v1",
      "published_date": "2025-06-03 12:34:40 UTC",
      "updated_date": "2025-06-03 12:34:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:06:18.586609+00:00"
    },
    {
      "arxiv_id": "2506.02796v1",
      "title": "Deep Learning Enhanced Multivariate GARCH",
      "title_zh": "æ·±åº¦å­¦ä¹ å¢å¼ºçš„å¤šå…ƒ GARCH",
      "authors": [
        "Haoyuan Wang",
        "Chen Liu",
        "Minh-Ngoc Tran",
        "Chao Wang"
      ],
      "abstract": "This paper introduces a novel multivariate volatility modeling framework, named Long Short-Term Memory enhanced BEKK (LSTM-BEKK), that integrates deep learning into multivariate GARCH processes. By combining the flexibility of recurrent neural networks with the econometric structure of BEKK models, our approach is designed to better capture nonlinear, dynamic, and high-dimensional dependence structures in financial return data. The proposed model addresses key limitations of traditional multivariate GARCH-based methods, particularly in capturing persistent volatility clustering and asymmetric co-movement across assets. Leveraging the data-driven nature of LSTMs, the framework adapts effectively to time-varying market conditions, offering improved robustness and forecasting performance. Empirical results across multiple equity markets confirm that the LSTM-BEKK model achieves superior performance in terms of out-of-sample portfolio risk forecast, while maintaining the interpretability from the BEKK models. These findings highlight the potential of hybrid econometric-deep learning models in advancing financial risk management and multivariate volatility forecasting.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º LSTM-BEKK çš„æ–°å‹å¤šå˜é‡æ³¢åŠ¨ç‡å»ºæ¨¡æ¡†æ¶ï¼Œå°†æ·±åº¦å­¦ä¹ æŠ€æœ¯ä¸å¤šå˜é‡ GARCH è¿‡ç¨‹ç›¸ç»“åˆã€‚è¯¥æ–¹æ³•é€šè¿‡æ•´åˆ Long Short-Term Memory (LSTM) çš„çµæ´»æ€§ä¸ BEKK æ¨¡å‹çš„ç»æµè®¡é‡ç»“æ„ï¼Œæ—¨åœ¨æ›´å¥½åœ°æ•æ‰é‡‘èæ”¶ç›Šæ•°æ®ä¸­çš„éçº¿æ€§ã€åŠ¨æ€å’Œé«˜ç»´ä¾èµ–ç»“æ„ã€‚è¯¥æ¨¡å‹æœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿå¤šå˜é‡ GARCH æ–¹æ³•åœ¨æ•æ‰æŒä¹…æ€§æ³¢åŠ¨èšé›† (volatility clustering) å’Œèµ„äº§é—´éå¯¹ç§°ååŠ¨ (asymmetric co-movement) æ–¹é¢çš„å…³é”®å±€é™æ€§ã€‚åˆ©ç”¨ LSTM çš„æ•°æ®é©±åŠ¨ç‰¹æ€§ï¼Œè¯¥æ¡†æ¶èƒ½æœ‰æ•ˆé€‚åº”éšæ—¶é—´å˜åŒ–çš„å¸‚åœºçŠ¶å†µï¼Œæä¾›æ›´å¼ºçš„é²æ£’æ€§å’Œé¢„æµ‹æ€§èƒ½ã€‚åœ¨å¤šä¸ªè‚¡ç¥¨å¸‚åœºçš„å®è¯ç»“æœè¯å®ï¼ŒLSTM-BEKK æ¨¡å‹åœ¨æ ·æœ¬å¤–ç»„åˆé£é™©é¢„æµ‹æ–¹é¢å–å¾—äº†ä¼˜äºä¼ ç»Ÿæ¨¡å‹çš„è¡¨ç°ï¼ŒåŒæ—¶ä¿ç•™äº† BEKK æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚è¿™äº›å‘ç°çªæ˜¾äº†æ··åˆç»æµè®¡é‡ä¸æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æå‡é‡‘èé£é™©ç®¡ç†å’Œå¤šå˜é‡æ³¢åŠ¨ç‡é¢„æµ‹èƒ½åŠ›æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "q-fin.CP",
        "cs.AI",
        "econ.EM"
      ],
      "primary_category": "q-fin.CP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02796v1",
      "published_date": "2025-06-03 12:22:57 UTC",
      "updated_date": "2025-06-03 12:22:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:06:18.032363+00:00"
    },
    {
      "arxiv_id": "2506.02794v1",
      "title": "PhysGaia: A Physics-Aware Dataset of Multi-Body Interactions for Dynamic Novel View Synthesis",
      "title_zh": "PhysGaiaï¼šé¢å‘åŠ¨æ€æ–°è§†è§’åˆæˆçš„å¤šä½“ç›¸äº’ä½œç”¨ç‰©ç†æ„ŸçŸ¥æ•°æ®é›†",
      "authors": [
        "Mijeong Kim",
        "Gunhee Kim",
        "Jungyoon Choi",
        "Wonjae Roh",
        "Bohyung Han"
      ],
      "abstract": "We introduce PhysGaia, a novel physics-aware dataset specifically designed for Dynamic Novel View Synthesis (DyNVS), encompassing both structured objects and unstructured physical phenomena. Unlike existing datasets that primarily focus on photorealistic reconstruction, PhysGaia is created to actively support physics-aware dynamic scene modeling. Our dataset provides complex dynamic scenarios with rich interactions among multiple objects, where they realistically collide with each other and exchange forces. Furthermore, it contains a diverse range of physical materials, such as liquid, gas, viscoelastic substance, and textile, which moves beyond the rigid bodies prevalent in existing datasets. All scenes in PhysGaia are faithfully generated to strictly adhere to physical laws, leveraging carefully selected material-specific physics solvers. To enable quantitative evaluation of physical modeling, our dataset provides essential ground-truth information, including 3D particle trajectories and physics parameters, e.g., viscosity. To facilitate research adoption, we also provide essential integration pipelines for using state-of-the-art DyNVS models with our dataset and report their results. By addressing the critical lack of datasets for physics-aware modeling, PhysGaia will significantly advance research in dynamic view synthesis, physics-based scene understanding, and deep learning models integrated with physical simulation -- ultimately enabling more faithful reconstruction and interpretation of complex dynamic scenes. Our datasets and codes are available in the project website, http://cvlab.snu.ac.kr/research/PhysGaia.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† PhysGaiaï¼Œä¸€ä¸ªä¸“ä¸º Dynamic Novel View Synthesis (DyNVS) è®¾è®¡çš„ç‰©ç†æ„ŸçŸ¥æ•°æ®é›†ï¼Œæ¶µç›–äº†ç»“æ„åŒ–å¯¹è±¡å’Œéç»“æ„åŒ–çš„ç‰©ç†ç°è±¡ã€‚ä¸ä¸“æ³¨äºç…§ç‰‡çº§çœŸå®æ„Ÿé‡å»ºçš„ç°æœ‰æ•°æ®é›†ä¸åŒï¼ŒPhysGaia æ—¨åœ¨æ”¯æŒç‰©ç†æ„ŸçŸ¥çš„åŠ¨æ€åœºæ™¯å»ºæ¨¡ï¼ŒåŒ…å«äº†æ¶²ä½“ (liquid)ã€æ°”ä½“ (gas)ã€ç²˜å¼¹æ€§ç‰©è´¨ (viscoelastic substance) å’Œç»‡ç‰© (textile) ç­‰å¤šç§ç‰©ç†ææ–™ã€‚æ•°æ®é›†æä¾›äº†å¤æ‚çš„å¤šç‰©ä½“äº¤äº’åŠ¨æ€åœºæ™¯ï¼Œæ‰€æœ‰åœºæ™¯å‡åˆ©ç”¨ç‰¹å®šçš„ç‰©ç†æ±‚è§£å™¨ (physics solvers) ç”Ÿæˆï¼Œä¸¥æ ¼éµå¾ªç‰©ç†å®šå¾‹å¹¶è¡¨ç°å‡ºçœŸå®çš„ç¢°æ’å’ŒåŠ›äº¤æ¢ã€‚ä¸ºäº†æ”¯æŒç‰©ç†å»ºæ¨¡çš„å®šé‡è¯„ä¼°ï¼ŒPhysGaia æä¾›äº†åŒ…æ‹¬ 3D ç²’å­è½¨è¿¹ (3D particle trajectories) å’Œç‰©ç†å‚æ•°ï¼ˆå¦‚ç²˜åº¦ viscosityï¼‰åœ¨å†…çš„å…³é”®çœŸå€¼ (ground-truth) ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜æä¾›äº†å°†å…ˆè¿›çš„ DyNVS æ¨¡å‹ä¸è¯¥æ•°æ®é›†é›†æˆçš„æµæ°´çº¿ï¼Œå¹¶å±•ç¤ºäº†è¯„ä¼°ç»“æœã€‚PhysGaia å¡«è¡¥äº†ç‰©ç†æ„ŸçŸ¥å»ºæ¨¡æ•°æ®é›†çš„ç©ºç™½ï¼Œå°†æ˜¾è‘—æ¨åŠ¨åŠ¨æ€è§†å›¾åˆæˆã€ç‰©ç†åœºæ™¯ç†è§£ä»¥åŠç‰©ç†ä»¿çœŸé›†æˆæ·±åº¦å­¦ä¹ ç­‰é¢†åŸŸçš„è¿›æ­¥ï¼Œæœ€ç»ˆå®ç°å¯¹å¤æ‚åŠ¨æ€åœºæ™¯æ›´å¿ å®çš„é‡å»ºä¸è§£é‡Šã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "Project page: http://cvlab.snu.ac.kr/research/PhysGaia, Data: https://huggingface.co/datasets/mijeongkim/PhysGaia/tree/main",
      "pdf_url": "https://arxiv.org/pdf/2506.02794v1",
      "published_date": "2025-06-03 12:19:18 UTC",
      "updated_date": "2025-06-03 12:19:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:05:24.429632+00:00"
    },
    {
      "arxiv_id": "2506.02791v2",
      "title": "Rethinking the effects of data contamination in Code Intelligence",
      "title_zh": "é‡æ–°å®¡è§†ä»£ç æ™ºèƒ½ä¸­çš„æ•°æ®æ±¡æŸ“å½±å“",
      "authors": [
        "Zhen Yang",
        "Hongyi Lin",
        "Yifan He",
        "Jie Xu",
        "Zeyu Sun",
        "Shuo Liu",
        "Pengpeng Wang",
        "Zhongxing Yu",
        "Qingyuan Liang"
      ],
      "abstract": "In recent years, code intelligence has gained increasing importance in the field of automated software engineering. Meanwhile, the widespread adoption of Pretrained Language Models (PLMs) and Large Language Models (LLMs) has raised concerns regarding data contamination and its potential impact on model performance evaluation. This paper presents a systematic empirical study to investigate the fine-grained data contamination on code intelligence tasks. Our study involves diverse representative PLMs, namely RoBERTa and GPT-2, and LLMs, namely LLaMA and StarCoder, covering three major tasks: code translation, code generation, and code summarization. We categorize contamination scenarios into four types according to the code intelligence practice, namely input-only, output-only, unpaired, and paired contamination settings, and construct corresponding experimental and control groups for exploration.\n  Experimental results show that, under the pre-training, fine-tuning, and inference paradigm adopted by PLMs, even deliberately injecting paired contamination does not lead to significant performance overestimation. But direct inference or small-scale fine-tuning uncovers the contamination effects. In contrast, LLMs with pre-training and inference paradigm are significantly affected by the paired contamination. Apart from the above, other contamination scenarios have no impact on both PLMs and LLMs. Our findings challenge the conventional belief that contamination inevitably leads to performance overestimation, providing new insights into the evaluation and deployment of code intelligence models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä»£ç æ™ºèƒ½(Code Intelligence)é¢†åŸŸä¸­é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹(PLMs)å’Œå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)é¢ä¸´çš„æ•°æ®æ±¡æŸ“(Data Contamination)é—®é¢˜è¿›è¡Œäº†ç³»ç»Ÿçš„å®è¯ç ”ç©¶ã€‚ç ”ç©¶æ¶µç›–äº†ä»£ç ç¿»è¯‘ã€ä»£ç ç”Ÿæˆå’Œä»£ç æ€»ç»“ä»»åŠ¡ï¼Œå¹¶é’ˆå¯¹RoBERTaã€GPT-2ã€LLaMAå’ŒStarCoderç­‰ä»£è¡¨æ€§æ¨¡å‹ï¼Œæ„å»ºäº†ä»…è¾“å…¥(input-only)ã€ä»…è¾“å‡º(output-only)ã€éæˆå¯¹(unpaired)å’Œæˆå¯¹(paired)å››ç§æ±¡æŸ“åœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨PLMsçš„é¢„è®­ç»ƒã€å¾®è°ƒå’Œæ¨ç†èŒƒå¼ä¸‹ï¼Œæˆå¯¹æ±¡æŸ“å¹¶ä¸ä¸€å®šä¼šå¯¼è‡´æ˜¾è‘—çš„æ€§èƒ½é«˜ä¼°ï¼Œä½†åœ¨ç›´æ¥æ¨ç†æˆ–å°è§„æ¨¡å¾®è°ƒæ—¶å…¶æ•ˆåº”ä¼šæ˜¾ç°ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œé‡‡ç”¨é¢„è®­ç»ƒä¸æ¨ç†èŒƒå¼çš„LLMsåˆ™æ˜¾è‘—å—åˆ°æˆå¯¹æ±¡æŸ“çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œé™¤æˆå¯¹æ±¡æŸ“å¤–çš„å…¶ä»–åœºæ™¯å¯¹ä¸¤ç±»æ¨¡å‹çš„å½±å“å‡è¾ƒå°ï¼Œè¿™ä¸€ç»“æœæŒ‘æˆ˜äº†â€œæ±¡æŸ“å¿…ç„¶å¯¼è‡´æ€§èƒ½é«˜ä¼°â€çš„ä¼ ç»Ÿè§‚ç‚¹ã€‚è¯¥å·¥ä½œä¸ºä»£ç æ™ºèƒ½æ¨¡å‹çš„å‡†ç¡®è¯„ä¼°å’Œéƒ¨ç½²æä¾›äº†æ–°çš„è§è§£ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02791v2",
      "published_date": "2025-06-03 12:15:44 UTC",
      "updated_date": "2025-06-08 10:31:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:05:33.005955+00:00"
    },
    {
      "arxiv_id": "2506.02787v1",
      "title": "Rethinking Dynamic Networks and Heterogeneous Computing with Automatic Parallelization",
      "title_zh": "é‡æ–°å®¡è§†è‡ªåŠ¨å¹¶è¡ŒåŒ–ä¸‹çš„åŠ¨æ€ç½‘ç»œä¸å¼‚æ„è®¡ç®—",
      "authors": [
        "Ruilong Wu",
        "Xinjiao Li",
        "Yisu Wang",
        "Xinyu Chen",
        "Dirk Kutscher"
      ],
      "abstract": "Hybrid parallelism techniques are essential for efficiently training large language models (LLMs). Nevertheless, current automatic parallel planning frameworks often overlook the simultaneous consideration of node heterogeneity and dynamic network topology changes, limiting their effectiveness in practical applications. In this paper, we address these limitations by modeling heterogeneous nodes within dynamically changing network environments and leveraging simulation-based strategies to determine optimal parallel configurations. Our approach enables fine-grained workload allocation tailored for heterogeneous nodes and complex network scenarios, achieving performance competitive with state-of-the-art methods under regular and stable network conditions. Additionally, we introduce a strategy pruning technique to rapidly discard infeasible parallel configurations, substantially reducing the search space and accelerating the search process through parallel execution within the simulator. Preliminary evaluations confirm that our method notably enhances training performance on heterogeneous nodes and demonstrates improved adaptability in complex, dynamic scenarios such as cloud computing environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ··åˆå¹¶è¡Œè®­ç»ƒä¸­å¸¸å¿½ç•¥èŠ‚ç‚¹å¼‚æ„æ€§(Heterogeneity)å’ŒåŠ¨æ€ç½‘ç»œæ‹“æ‰‘å˜åŒ–çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„è‡ªåŠ¨å¹¶è¡ŒåŒ–æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹åŠ¨æ€ç½‘ç»œç¯å¢ƒä¸­çš„å¼‚æ„èŠ‚ç‚¹è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶åˆ©ç”¨åŸºäºæ¨¡æ‹Ÿ(Simulation-based)çš„ç­–ç•¥æ¥ç¡®å®šæœ€ä¼˜çš„å¹¶è¡Œé…ç½®ã€‚å®ƒå®ç°äº†é’ˆå¯¹å¼‚æ„èŠ‚ç‚¹å’Œå¤æ‚ç½‘ç»œåœºæ™¯çš„ç»†ç²’åº¦å·¥ä½œè´Ÿè½½åˆ†é…(Fine-grained workload allocation)ï¼Œä½¿å…¶åœ¨å¸¸è§„ç½‘ç»œæ¡ä»¶ä¸‹å…·æœ‰ä¸ç°æœ‰ä¸»æµæ–¹æ³•ç›¸æŠ—è¡¡çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ç­–ç•¥å‰ªæ(Strategy pruning)æŠ€æœ¯æ¥å¿«é€Ÿå‰”é™¤ä¸å¯è¡Œçš„é…ç½®ï¼Œä»è€Œå¤§å¹…ç¼©å‡æœç´¢ç©ºé—´ï¼Œå¹¶é€šè¿‡æ¨¡æ‹Ÿå™¨å†…çš„å¹¶è¡Œæ‰§è¡Œè¿›ä¸€æ­¥åŠ é€Ÿæœç´¢è¿‡ç¨‹ã€‚å®éªŒè¯„ä¼°è¯å®ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†å¼‚æ„èŠ‚ç‚¹ä¸Šçš„è®­ç»ƒæ•ˆç‡ï¼Œå¹¶åœ¨äº‘è®¡ç®—ç­‰å¤æ‚åŠ¨æ€åœºæ™¯ä¸­å±•ç°å‡ºä¼˜å¼‚çš„è‡ªé€‚åº”èƒ½åŠ›ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02787v1",
      "published_date": "2025-06-03 12:14:17 UTC",
      "updated_date": "2025-06-03 12:14:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:05:43.288867+00:00"
    },
    {
      "arxiv_id": "2506.02785v1",
      "title": "AI-Driven Vehicle Condition Monitoring with Cell-Aware Edge Service Migration",
      "title_zh": "åŸºäºå°åŒºæ„ŸçŸ¥è¾¹ç¼˜æœåŠ¡è¿ç§»çš„ AI é©±åŠ¨è½¦è¾†çŠ¶æ€ç›‘æµ‹",
      "authors": [
        "Charalampos Kalalas",
        "Pavol Mulinka",
        "Guillermo Candela Belmonte",
        "Miguel Fornell",
        "Michail Dalgitsis",
        "Francisco Paredes Vera",
        "Javier Santaella SÃ¡nchez",
        "Carmen Vicente Villares",
        "Roshan Sedar",
        "Eftychia Datsika",
        "Angelos Antonopoulos",
        "Antonio FernÃ¡ndez Ojea",
        "Miquel Payaro"
      ],
      "abstract": "Artificial intelligence (AI) has been increasingly applied to the condition monitoring of vehicular equipment, aiming to enhance maintenance strategies, reduce costs, and improve safety. Leveraging the edge computing paradigm, AI-based condition monitoring systems process vast streams of vehicular data to detect anomalies and optimize operational performance. In this work, we introduce a novel vehicle condition monitoring service that enables real-time diagnostics of a diverse set of anomalies while remaining practical for deployment in real-world edge environments. To address mobility challenges, we propose a closed-loop service orchestration framework where service migration across edge nodes is dynamically triggered by network-related metrics. Our approach has been implemented and tested in a real-world race circuit environment equipped with 5G network capabilities under diverse operational conditions. Experimental results demonstrate the effectiveness of our framework in ensuring low-latency AI inference and adaptive service placement, highlighting its potential for intelligent transportation and mobility applications.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨äººå·¥æ™ºèƒ½ (AI) é©±åŠ¨çš„è½¦è¾†çŠ¶å†µç›‘æµ‹ç³»ç»Ÿï¼Œæ—¨åœ¨ä¼˜åŒ–ç»´æŠ¤ç­–ç•¥ã€é™ä½æˆæœ¬å¹¶æå‡å®‰å…¨æ€§ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ–°å‹çš„è½¦è¾†çŠ¶å†µç›‘æµ‹æœåŠ¡ï¼Œèƒ½å¤Ÿåœ¨ç°å®çš„è¾¹ç¼˜è®¡ç®— (Edge Computing) ç¯å¢ƒä¸­å®ç°å¯¹å¤šç§å¼‚å¸¸æƒ…å†µçš„å®æ—¶è¯Šæ–­ã€‚ä¸ºäº†åº”å¯¹è½¦è¾†ç§»åŠ¨æ€§å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é—­ç¯æœåŠ¡ç¼–æ’æ¡†æ¶ (Closed-loop Service Orchestration Framework)ï¼Œæ ¹æ®ç½‘ç»œç›¸å…³æŒ‡æ ‡åŠ¨æ€è§¦å‘è·¨è¾¹ç¼˜èŠ‚ç‚¹çš„æœåŠ¡è¿ç§» (Service Migration)ã€‚è¯¥æ–¹æ¡ˆåœ¨å…·å¤‡ 5G ç½‘ç»œèƒ½åŠ›çš„çœŸå®èµ›è½¦åœºç¯å¢ƒä¸­è¿›è¡Œäº†éƒ¨ç½²å’Œæµ‹è¯•ï¼Œæ¶µç›–äº†å¤šç§ä¸åŒçš„è¿è¡Œæ¡ä»¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ç¡®ä¿ä½å»¶è¿Ÿçš„äººå·¥æ™ºèƒ½æ¨ç† (AI Inference) å’Œè‡ªé€‚åº”æœåŠ¡æ”¾ç½® (Adaptive Service Placement) æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå‡¸æ˜¾äº†å…¶åœ¨æ™ºèƒ½äº¤é€šå’Œç§»åŠ¨åº”ç”¨é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "6 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.02785v1",
      "published_date": "2025-06-03 12:12:27 UTC",
      "updated_date": "2025-06-03 12:12:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:06:22.493415+00:00"
    },
    {
      "arxiv_id": "2506.03227v2",
      "title": "Bridging Neural ODE and ResNet: A Formal Error Bound for Safety Verification",
      "title_zh": "æ¡¥æ¥ Neural ODE ä¸ ResNetï¼šé¢å‘å®‰å…¨éªŒè¯çš„å½¢å¼åŒ–è¯¯å·®ç•Œ",
      "authors": [
        "Abdelrahman Sayed Sayed",
        "Pierre-Jean Meyer",
        "Mohamed Ghazel"
      ],
      "abstract": "A neural ordinary differential equation (neural ODE) is a machine learning model that is commonly described as a continuous-depth generalization of a residual network (ResNet) with a single residual block, or conversely, the ResNet can be seen as the Euler discretization of the neural ODE. These two models are therefore strongly related in a way that the behaviors of either model are considered to be an approximation of the behaviors of the other. In this work, we establish a more formal relationship between these two models by bounding the approximation error between two such related models. The obtained error bound then allows us to use one of the models as a verification proxy for the other, without running the verification tools twice: if the reachable output set expanded by the error bound satisfies a safety property on one of the models, this safety property is then guaranteed to be also satisfied on the other model. This feature is fully reversible, and the initial safety verification can be run indifferently on either of the two models. This novel approach is illustrated on a numerical example of a fixed-point attractor system modeled as a neural ODE.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Neural ODE ä¸ ResNet ä¹‹é—´çš„å†…åœ¨è”ç³»ï¼Œå°† ResNet è§†ä¸º Neural ODE çš„ Euler ç¦»æ•£åŒ–å½¢å¼ã€‚ç ”ç©¶é€šè¿‡å»ºç«‹ä¸¤è€…ä¹‹é—´çš„æ­£å¼è¯¯å·®ç•Œé™ (Formal Error Bound)ï¼ŒæˆåŠŸé‡åŒ–äº†è¿™ä¸¤ä¸ªæ¨¡å‹è¡Œä¸ºä¹‹é—´çš„è¿‘ä¼¼è¯¯å·®ã€‚è¿™ä¸€ç•Œé™å…è®¸å°†å…¶ä¸­ä¸€ä¸ªæ¨¡å‹ä½œä¸ºå¦ä¸€ä¸ªæ¨¡å‹çš„éªŒè¯ä»£ç† (Verification Proxy)ï¼Œä»è€Œé¿å…äº†é‡å¤è¿è¡ŒéªŒè¯å·¥å…·ã€‚è‹¥é€šè¿‡è¯¯å·®ç•Œé™æ‰©å±•çš„å¯è¾¾è¾“å‡ºé›†åœ¨ä¸€ä¸ªæ¨¡å‹ä¸Šæ»¡è¶³å®‰å…¨æ€§å±æ€§ï¼Œåˆ™è¯¥å±æ€§åœ¨å¦ä¸€ä¸ªæ¨¡å‹ä¸ŠåŒæ ·å¾—åˆ°ä¿è¯ã€‚è¯¥æ–¹æ³•å…·æœ‰å®Œå…¨çš„å¯é€†æ€§ï¼Œåˆå§‹å®‰å…¨æ€§éªŒè¯å¯ä»¥åœ¨ä¸¤ä¸ªæ¨¡å‹ä¸­çš„ä»»ä¸€æ¨¡å‹ä¸Šç‹¬ç«‹è¿è¡Œã€‚ç ”ç©¶æœ€åé€šè¿‡å»ºæ¨¡ä¸º Neural ODE çš„å®šç‚¹å¸å¼•å­ç³»ç»Ÿ (Fixed-point Attractor System) çš„æ•°å€¼å®ä¾‹éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages, 5 figures, Accepted for publication in the proceedings of the 8th International Symposium on AI Verification SAIV 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.03227v2",
      "published_date": "2025-06-03 11:56:22 UTC",
      "updated_date": "2025-10-11 11:27:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:05:59.088231+00:00"
    },
    {
      "arxiv_id": "2506.13990v1",
      "title": "Machine Mirages: Defining the Undefined",
      "title_zh": "æœºå™¨å¹»è±¡ï¼šå®šä¹‰æœªå®šä¹‰",
      "authors": [
        "Hamidou Tembine"
      ],
      "abstract": "As multimodal machine intelligence systems started achieving average animal-level and average human-level fluency in many measurable tasks in processing images, language, and sound, they began to exhibit a new class of cognitive aberrations: machine mirages. These include delusion, illusion, confabulation, hallucination, misattribution error, semantic drift, semantic compression, exaggeration, causal inference failure, uncanny valley of perception, bluffing-patter-bullshitting, cognitive stereotypy, pragmatic misunderstanding, hypersignification, semantic reheating-warming, simulated authority effect, fallacious abductive leap, contextual drift, referential hallucination, semiotic Frankenstein effect, calibration failure, spurious correlation, bias amplification, concept drift sensitivity, misclassification under uncertainty, adversarial vulnerability, overfitting, prosodic misclassification, accent bias, turn boundary failure, semantic boundary confusion, noise overfitting, latency-induced decision drift, ambiguity collapse and other forms of error that mimic but do not replicate human or animal fallibility. This article presents some of the errors and argues that these failures must be explicitly defined and systematically assessed. Understanding machine mirages is essential not only for improving machine intelligence reliability but also for constructing a multiscale ethical, co-evolving intelligence ecosystem that respects the diverse forms of life, cognition, and expression it will inevitably touch.",
      "tldr_zh": "è¯¥ç ”ç©¶å®šä¹‰äº† Machine Miragesï¼ˆæœºå™¨å¹»è±¡ï¼‰è¿™ä¸€æ¦‚å¿µï¼Œæ—¨åœ¨æè¿°å¤šæ¨¡æ€æœºå™¨æ™ºèƒ½åœ¨å›¾åƒã€è¯­è¨€å’Œå£°éŸ³å¤„ç†ä¸­è¡¨ç°å‡ºçš„ä¸€ç±»æ–°å‹è®¤çŸ¥åå·®ã€‚è¿™äº›åå·®æ¶µç›–äº† Delusionï¼ˆé”™è§‰ï¼‰ã€Confabulationï¼ˆè™šæ„ï¼‰ã€Hallucinationï¼ˆå¹»è§‰ï¼‰ã€Semantic Driftï¼ˆè¯­ä¹‰æ¼‚ç§»ï¼‰ä»¥åŠ Causal Inference Failureï¼ˆå› æœæ¨ç†å¤±è´¥ï¼‰ç­‰å¤šç§æ¨¡æ‹Ÿä½†ä¸å®Œå…¨ç­‰åŒäºç”Ÿç‰©æ€§å¤±è¯¯çš„é”™è¯¯å½¢å¼ã€‚è®ºæ–‡æŒ‡å‡ºï¼Œå¿…é¡»å¯¹è¿™äº›å¤±æ•ˆæ¨¡å¼è¿›è¡Œæ˜ç¡®çš„å­¦æœ¯å®šä¹‰å’Œç³»ç»Ÿæ€§è¯„ä¼°ï¼Œä»¥åº”å¯¹æœºå™¨æ™ºèƒ½åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„ä¸å¯é¢„æµ‹æ€§ã€‚æ·±å…¥ç†è§£ Machine Mirages ä¸ä»…æ˜¯æå‡ç³»ç»Ÿå¯é æ€§çš„å…³é”®ï¼Œä¹Ÿæ˜¯æ„å»ºå¤šå°ºåº¦ä¼¦ç†åŒ–æ™ºèƒ½ç”Ÿæ€ç³»ç»Ÿçš„åŸºç¡€ã€‚é€šè¿‡å¯¹è¿™ç±»è®¤çŸ¥åå·®çš„ç•Œå®šï¼Œç ”ç©¶å¼ºè°ƒäº†åœ¨æ™ºèƒ½å…±è¿›åŒ–è¿‡ç¨‹ä¸­å°Šé‡å¤šå…ƒç”Ÿå‘½å½¢å¼ä¸è¡¨è¾¾æ–¹å¼çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Submitted",
      "pdf_url": "https://arxiv.org/pdf/2506.13990v1",
      "published_date": "2025-06-03 11:45:38 UTC",
      "updated_date": "2025-06-03 11:45:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:07:12.525917+00:00"
    },
    {
      "arxiv_id": "2506.02764v1",
      "title": "Unified Attention Modeling for Efficient Free-Viewing and Visual Search via Shared Representations",
      "title_zh": "é€šè¿‡å…±äº«è¡¨ç¤ºå®ç°é«˜æ•ˆè‡ªç”±è§‚çœ‹ä¸è§†è§‰æœç´¢çš„ç»Ÿä¸€æ³¨æ„åŠ›å»ºæ¨¡",
      "authors": [
        "Fatma Youssef Mohammed",
        "Kostas Alexis"
      ],
      "abstract": "Computational human attention modeling in free-viewing and task-specific settings is often studied separately, with limited exploration of whether a common representation exists between them. This work investigates this question and proposes a neural network architecture that builds upon the Human Attention transformer (HAT) to test the hypothesis. Our results demonstrate that free-viewing and visual search can efficiently share a common representation, allowing a model trained in free-viewing attention to transfer its knowledge to task-driven visual search with a performance drop of only 3.86% in the predicted fixation scanpaths, measured by the semantic sequence score (SemSS) metric which reflects the similarity between predicted and human scanpaths. This transfer reduces computational costs by 92.29% in terms of GFLOPs and 31.23% in terms of trainable parameters.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è‡ªç”±è§‚çœ‹ (free-viewing) å’Œç‰¹å®šä»»åŠ¡çš„è§†è§‰æœç´¢åœ¨è®¡ç®—æ³¨æ„åŠ›å»ºæ¨¡ä¸­æ˜¯å¦å­˜åœ¨å…±åŒè¡¨ç¤ºã€‚ä½œè€…åŸºäº Human Attention transformer (HAT) æå‡ºäº†ä¸€ç§æ–°çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œæ—¨åœ¨æµ‹è¯•ä¸¤è€…å…±äº«åº•å±‚è¡¨ç¤ºçš„å‡è®¾ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè‡ªç”±è§‚çœ‹ä¸è§†è§‰æœç´¢å¯ä»¥é«˜æ•ˆåœ°å…±äº«é€šç”¨è¡¨ç¤ºï¼Œä½¿å¾—åœ¨è‡ªç”±è§‚çœ‹æ³¨æ„åŠ›ä»»åŠ¡ä¸Šè®­ç»ƒçš„æ¨¡å‹èƒ½å¤ŸæˆåŠŸè¿ç§»è‡³ä»»åŠ¡é©±åŠ¨çš„è§†è§‰æœç´¢ã€‚åœ¨é¢„æµ‹æ³¨è§†ç‚¹æ‰«æè·¯å¾„çš„è¯­ä¹‰åºåˆ—å¾—åˆ† (SemSS) æŒ‡æ ‡ä¸Šï¼Œè¯¥è¿ç§»æ–¹æ³•çš„æ€§èƒ½é™å¹…ä»…ä¸º 3.86%ï¼ŒéªŒè¯äº†æ¨¡å‹å¯¹äººç±»æ‰«æè·¯å¾„æ¨¡æ‹Ÿçš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè¿™ç§å…±äº«è¡¨ç¤ºçš„æ–¹æ³•æ˜¾è‘—æå‡äº†è®¡ç®—æ•ˆç‡ï¼Œåœ¨ GFLOPs ä¸Šå‡å°‘äº† 92.29%ï¼ŒåŒæ—¶å‡å°‘äº† 31.23% çš„å¯è®­ç»ƒå‚æ•°ã€‚è¯¥å·¥ä½œä¸ºæ„å»ºé«˜æ•ˆä¸”ç»Ÿä¸€çš„äººç±»è§†è§‰æ³¨æ„åŠ›æ¨¡å‹æä¾›äº†é‡è¦è¯æ®å’Œå®ç°æ¡†æ¶ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to the 2025 IEEE International Conference on Development and Learning (ICDL)",
      "pdf_url": "https://arxiv.org/pdf/2506.02764v1",
      "published_date": "2025-06-03 11:29:11 UTC",
      "updated_date": "2025-06-03 11:29:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:07:28.283870+00:00"
    },
    {
      "arxiv_id": "2506.02761v2",
      "title": "Rethinking Machine Unlearning in Image Generation Models",
      "title_zh": "é‡æ–°å®¡è§†å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­çš„æœºå™¨é—å¿˜",
      "authors": [
        "Renyang Liu",
        "Wenjie Feng",
        "Tianwei Zhang",
        "Wei Zhou",
        "Xueqi Cheng",
        "See-Kiong Ng"
      ],
      "abstract": "With the surge and widespread application of image generation models, data privacy and content safety have become major concerns and attracted great attention from users, service providers, and policymakers. Machine unlearning (MU) is recognized as a cost-effective and promising means to address these challenges. Despite some advancements, image generation model unlearning (IGMU) still faces remarkable gaps in practice, e.g., unclear task discrimination and unlearning guidelines, lack of an effective evaluation framework, and unreliable evaluation metrics. These can hinder the understanding of unlearning mechanisms and the design of practical unlearning algorithms. We perform exhaustive assessments over existing state-of-the-art unlearning algorithms and evaluation standards, and discover several critical flaws and challenges in IGMU tasks. Driven by these limitations, we make several core contributions, to facilitate the comprehensive understanding, standardized categorization, and reliable evaluation of IGMU. Specifically, (1) We design CatIGMU, a novel hierarchical task categorization framework. It provides detailed implementation guidance for IGMU, assisting in the design of unlearning algorithms and the construction of testbeds. (2) We introduce EvalIGMU, a comprehensive evaluation framework. It includes reliable quantitative metrics across five critical aspects. (3) We construct DataIGM, a high-quality unlearning dataset, which can be used for extensive evaluations of IGMU, training content detectors for judgment, and benchmarking the state-of-the-art unlearning algorithms. With EvalIGMU and DataIGM, we discover that most existing IGMU algorithms cannot handle the unlearning well across different evaluation dimensions, especially for preservation and robustness. Code and models are available at https://github.com/ryliu68/IGMU.",
      "tldr_zh": "éšç€å›¾åƒç”Ÿæˆæ¨¡å‹çš„å¹¿æ³›åº”ç”¨ï¼Œæ•°æ®éšç§å’Œå†…å®¹å®‰å…¨æˆä¸ºæ ¸å¿ƒå…³æ³¨ç‚¹ï¼Œæœºå™¨æ¶ˆå» (Machine Unlearning, MU) è¢«è§†ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜çš„å…³é”®æŠ€æœ¯ã€‚ç„¶è€Œï¼Œå½“å‰çš„å›¾åƒç”Ÿæˆæ¨¡å‹æ¶ˆå» (IGMU) å®è·µä»é¢ä¸´ä»»åŠ¡åˆ†ç±»ä¸æ˜ç¡®å’Œè¯„ä¼°æ ‡å‡†ä¸ç»Ÿä¸€ç­‰é‡å¤§æŒ‘æˆ˜ï¼Œé˜»ç¢äº†æ¶ˆå»æœºåˆ¶çš„ç†è§£ã€‚é’ˆå¯¹è¿™äº›å±€é™æ€§ï¼Œè¯¥ç ”ç©¶æå‡ºäº† CatIGMU æ¡†æ¶ï¼Œé€šè¿‡å±‚æ¬¡åŒ–ä»»åŠ¡åˆ†ç±»ä¸ºæ¶ˆå»ç®—æ³•çš„è®¾è®¡æä¾›è¯¦ç»†çš„å®æ–½æŒ‡å—ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº† EvalIGMU ç»¼åˆè¯„ä¼°æ¡†æ¶ï¼Œä»äº”ä¸ªç»´åº¦å»ºç«‹äº†å¯é çš„å®šé‡æŒ‡æ ‡ï¼Œå¹¶æ„å»ºäº†é«˜è´¨é‡æ•°æ®é›† DataIGM ç”¨äºåŸºå‡†æµ‹è¯•å’Œå†…å®¹æ£€æµ‹å™¨è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¤§å¤šæ•°ç°æœ‰çš„ IGMU ç®—æ³•åœ¨ä¸åŒè¯„ä¼°ç»´åº¦ä¸‹è¡¨ç°æ¬ ä½³ï¼Œå°¤å…¶åœ¨ä¿ç•™æ€§ (preservation) å’Œé²æ£’æ€§ (robustness) æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—çŸ­æ¿ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by ACM CCS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.02761v2",
      "published_date": "2025-06-03 11:25:14 UTC",
      "updated_date": "2025-06-06 12:39:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:07:50.954234+00:00"
    },
    {
      "arxiv_id": "2506.02758v1",
      "title": "Exploiting the English Vocabulary Profile for L2 word-level vocabulary assessment with LLMs",
      "title_zh": "åˆ©ç”¨ English Vocabulary Profile ç»“åˆå¤§è¯­è¨€æ¨¡å‹è¿›è¡ŒäºŒè¯­å•è¯çº§è¯æ±‡è¯„ä¼°",
      "authors": [
        "Stefano BannÃ²",
        "Kate Knill",
        "Mark Gales"
      ],
      "abstract": "Vocabulary use is a fundamental aspect of second language (L2) proficiency. To date, its assessment by automated systems has typically examined the context-independent, or part-of-speech (PoS) related use of words. This paper introduces a novel approach to enable fine-grained vocabulary evaluation exploiting the precise use of words within a sentence. The scheme combines large language models (LLMs) with the English Vocabulary Profile (EVP). The EVP is a standard lexical resource that enables in-context vocabulary use to be linked with proficiency level. We evaluate the ability of LLMs to assign proficiency levels to individual words as they appear in L2 learner writing, addressing key challenges such as polysemy, contextual variation, and multi-word expressions. We compare LLMs to a PoS-based baseline. LLMs appear to exploit additional semantic information that yields improved performance. We also explore correlations between word-level proficiency and essay-level proficiency. Finally, the approach is applied to examine the consistency of the EVP proficiency levels. Results show that LLMs are well-suited for the task of vocabulary assessment.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)å’Œè‹±è¯­è¯æ±‡æ¦‚å†µ(EVP)è¿›è¡ŒäºŒè¯­(L2)å•è¯çº§è¯æ±‡è¯„ä¼°çš„æ–°æ–¹æ³•ã€‚é’ˆå¯¹ä»¥å¾€è‡ªåŠ¨åŒ–ç³»ç»Ÿä¸»è¦å…³æ³¨è¯æ€§(PoS)æˆ–è„±ç¦»è¯­å¢ƒçš„å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆä¸Šä¸‹æ–‡ç²¾ç¡®è¯è¯­ä½¿ç”¨çš„ç»†ç²’åº¦è¯„ä¼°æ–¹æ¡ˆã€‚é€šè¿‡å°†LLMsä¸æ ‡å‡†è¯­è¨€èµ„æºEVPç›¸ç»“åˆï¼Œç ”ç©¶è¯„ä¼°äº†æ¨¡å‹åœ¨å¤„ç†å¤šä¹‰è¯ã€è¯­å¢ƒå˜åŒ–å’Œå¤šè¯è¡¨è¾¾(multi-word expressions)æ—¶ä¸ºå•ä¸ªå•è¯åˆ†é…è¯­è¨€æ°´å¹³ç­‰çº§çš„èƒ½åŠ›ã€‚å®éªŒå¯¹æ¯”æ˜¾ç¤ºï¼ŒLLMsèƒ½å¤Ÿåˆ©ç”¨ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼Œåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºåŸºäºè¯æ€§çš„åŸºå‡†æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¢ç´¢äº†å•è¯çº§æ°´å¹³ä¸ä½œæ–‡æ•´ä½“æ°´å¹³ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå¹¶å¯¹EVPç­‰çº§çš„ä¸€è‡´æ€§è¿›è¡Œäº†æ£€éªŒã€‚ç»“æœè¯æ˜LLMséå¸¸é€‚åˆæ‰§è¡Œè¯æ±‡è¯„ä¼°ä»»åŠ¡ï¼Œä¸ºè‡ªåŠ¨åŒ–è¯­è¨€æµ‹è¯„æä¾›äº†æ›´ç²¾ç¡®çš„è¯­ä¹‰åˆ†æå·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to the 20th Workshop on Innovative Use of NLP for Building Educational Applications",
      "pdf_url": "https://arxiv.org/pdf/2506.02758v1",
      "published_date": "2025-06-03 11:23:57 UTC",
      "updated_date": "2025-06-03 11:23:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:07:45.129241+00:00"
    },
    {
      "arxiv_id": "2506.02757v1",
      "title": "Investigating Mask-aware Prototype Learning for Tabular Anomaly Detection",
      "title_zh": "è¡¨æ ¼å¼‚å¸¸æ£€æµ‹ä¸­çš„æ©ç æ„ŸçŸ¥åŸå‹å­¦ä¹ ç ”ç©¶",
      "authors": [
        "Ruiying Lu",
        "Jinhan Liu",
        "Chuan Du",
        "Dandan Guo"
      ],
      "abstract": "Tabular anomaly detection, which aims at identifying deviant samples, has been crucial in a variety of real-world applications, such as medical disease identification, financial fraud detection, intrusion monitoring, etc. Although recent deep learning-based methods have achieved competitive performances, these methods suffer from representation entanglement and the lack of global correlation modeling, which hinders anomaly detection performance. To tackle the problem, we incorporate mask modeling and prototype learning into tabular anomaly detection. The core idea is to design learnable masks by disentangled representation learning within a projection space and extracting normal dependencies as explicit global prototypes. Specifically, the overall model involves two parts: (i) During encoding, we perform mask modeling in both the data space and projection space with orthogonal basis vectors for learning shared disentangled normal patterns; (ii) During decoding, we decode multiple masked representations in parallel for reconstruction and learn association prototypes to extract normal characteristic correlations. Our proposal derives from a distribution-matching perspective, where both projection space learning and association prototype learning are formulated as optimal transport problems, and the calibration distances are utilized to refine the anomaly scores. Quantitative and qualitative experiments on 20 tabular benchmarks demonstrate the effectiveness and interpretability of our model.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¡¨æ ¼æ•°æ®å¼‚å¸¸æ£€æµ‹(Tabular anomaly detection)ä¸­å­˜åœ¨çš„è¡¨ç¤ºçº ç¼ (representation entanglement)å’Œç¼ºä¹å…¨å±€ç›¸å…³æ€§å»ºæ¨¡(global correlation modeling)çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆæ©ç å»ºæ¨¡(mask modeling)å’ŒåŸå‹å­¦ä¹ (prototype learning)çš„æ–°æ–¹æ³•ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨æ•°æ®ç©ºé—´å’ŒæŠ•å½±ç©ºé—´ä¸­ä½¿ç”¨æ­£äº¤åŸºå‘é‡è¿›è¡Œæ©ç å»ºæ¨¡ï¼Œä»¥å­¦ä¹ å…±äº«çš„è§£è€¦æ­£å¸¸æ¨¡å¼(disentangled normal patterns)ã€‚åœ¨è§£ç è¿‡ç¨‹ä¸­ï¼Œç³»ç»Ÿå¹¶è¡Œå¤„ç†å¤šä¸ªè¢«æ©ç çš„è¡¨ç¤ºä»¥è¿›è¡Œé‡æ„ï¼Œå¹¶å­¦ä¹ å…³è”åŸå‹(association prototypes)æ¥æå–æ­£å¸¸ç‰¹å¾çš„ç›¸å…³æ€§ã€‚ç ”ç©¶è¿›ä¸€æ­¥ä»åˆ†å¸ƒåŒ¹é…çš„è§’åº¦å‡ºå‘ï¼Œå°†æŠ•å½±ç©ºé—´å­¦ä¹ å’Œå…³è”åŸå‹å­¦ä¹ å»ºæ¨¡ä¸ºæœ€ä¼˜ä¼ è¾“é—®é¢˜(optimal transport problems)ï¼Œå¹¶åˆ©ç”¨æ ¡å‡†è·ç¦»æ¥ä¼˜åŒ–å¼‚å¸¸è¯„åˆ†ã€‚å¯¹20ä¸ªè¡¨æ ¼åŸºå‡†æ•°æ®é›†è¿›è¡Œçš„å®šé‡å’Œå®šæ€§å®éªŒè¯æ˜äº†è¯¥æ¨¡å‹åœ¨æå‡æ£€æµ‹æ€§èƒ½çš„åŒæ—¶ï¼Œä¹Ÿå¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§(interpretability)ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.02757v1",
      "published_date": "2025-06-03 11:22:44 UTC",
      "updated_date": "2025-06-03 11:22:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:08:57.791329+00:00"
    },
    {
      "arxiv_id": "2506.03225v2",
      "title": "Multiple-Frequencies Population-Based Training",
      "title_zh": "å¤šé¢‘ç‡åŸºäºç§ç¾¤çš„è®­ç»ƒ",
      "authors": [
        "WaÃ«l Doulazmi",
        "Auguste Lehuger",
        "Marin Toromanoff",
        "Valentin Charraut",
        "Thibault Buhet",
        "Fabien Moutarde"
      ],
      "abstract": "Reinforcement Learning's high sensitivity to hyperparameters is a source of instability and inefficiency, creating significant challenges for practitioners. Hyperparameter Optimization (HPO) algorithms have been developed to address this issue, among them Population-Based Training (PBT) stands out for its ability to generate hyperparameters schedules instead of fixed configurations. PBT trains a population of agents, each with its own hyperparameters, frequently ranking them and replacing the worst performers with mutations of the best agents. These intermediate selection steps can cause PBT to focus on short-term improvements, leading it to get stuck in local optima and eventually fall behind vanilla Random Search over longer timescales. This paper studies how this greediness issue is connected to the choice of evolution frequency, the rate at which the selection is done. We propose Multiple-Frequencies Population-Based Training (MF-PBT), a novel HPO algorithm that addresses greediness by employing sub-populations, each evolving at distinct frequencies. MF-PBT introduces a migration process to transfer information between sub-populations, with an asymmetric design to balance short and long-term optimization. Extensive experiments on the Brax suite demonstrate that MF-PBT improves sample efficiency and long-term performance, even without actually tuning hyperparameters.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)å¯¹è¶…å‚æ•°æ•æ„Ÿå¯¼è‡´çš„ä¸ç¨³å®šé—®é¢˜ï¼Œæ¢è®¨äº†ç§ç¾¤è®­ç»ƒ(Population-Based Training, PBT)å› é¢‘ç¹é€‰æ‹©è€Œäº§ç”Ÿçš„è´ªå©ªæ€§(greediness)ç¼ºé™·ã€‚ä½œè€…å‘ç°è¿™ç§è´ªå©ªæ€§ä¸è¿›åŒ–é¢‘ç‡(evolution frequency)å¯†åˆ‡ç›¸å…³ï¼Œå¹¶æ®æ­¤æå‡ºäº†å¤šé¢‘ç‡ç§ç¾¤è®­ç»ƒ(Multiple-Frequencies Population-Based Training, MF-PBT)ç®—æ³•ã€‚è¯¥ç®—æ³•é€šè¿‡è®¾ç«‹å¤šä¸ªä»¥ä¸åŒé¢‘ç‡è¿›åŒ–çš„å­ç§ç¾¤ï¼Œå¹¶ç»“åˆä¸€ç§ä¸å¯¹ç§°çš„è¿ç§»æœºåˆ¶(migration process)æ¥å¹³è¡¡çŸ­æœŸä¸é•¿æœŸä¼˜åŒ–ã€‚åœ¨Braxå¥—ä»¶ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMF-PBTæ˜¾è‘—æå‡äº†é‡‡æ ·æ•ˆç‡(sample efficiency)å’Œé•¿æœŸæ€§èƒ½ï¼Œæœ‰æ•ˆé¿å…äº†é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚å³ä½¿åœ¨ä¸æ˜¾å¼è°ƒä¼˜è¶…å‚æ•°çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•ä¾ç„¶å±•ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§ï¼Œä¸ºè¶…å‚æ•°ä¼˜åŒ–(Hyperparameter Optimization, HPO)æä¾›äº†æ›´é«˜æ•ˆçš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "RLC25 - Camera-ready",
      "pdf_url": "https://arxiv.org/pdf/2506.03225v2",
      "published_date": "2025-06-03 11:19:21 UTC",
      "updated_date": "2025-07-17 15:41:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:07:44.563259+00:00"
    },
    {
      "arxiv_id": "2506.11073v1",
      "title": "CLAIM: Mitigating Multilingual Object Hallucination in Large Vision-Language Models with Cross-Lingual Attention Intervention",
      "title_zh": "CLAIMï¼šé€šè¿‡è·¨è¯­è¨€æ³¨æ„åŠ›å¹²é¢„ç¼“è§£å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å¤šè¯­è¨€ç‰©ä½“å¹»è§‰",
      "authors": [
        "Zekai Ye",
        "Qiming Li",
        "Xiaocheng Feng",
        "Libo Qin",
        "Yichong Huang",
        "Baohang Li",
        "Kui Jiang",
        "Yang Xiang",
        "Zhirui Zhang",
        "Yunfei Lu",
        "Duyu Tang",
        "Dandan Tu",
        "Bing Qin"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have demonstrated impressive multimodal abilities but remain prone to multilingual object hallucination, with a higher likelihood of generating responses inconsistent with the visual input when utilizing queries in non-English languages compared to English. Most existing approaches to address these rely on pretraining or fine-tuning, which are resource-intensive. In this paper, inspired by observing the disparities in cross-modal attention patterns across languages, we propose Cross-Lingual Attention Intervention for Mitigating multilingual object hallucination (CLAIM) in LVLMs, a novel near training-free method by aligning attention patterns. CLAIM first identifies language-specific cross-modal attention heads, then estimates language shift vectors from English to the target language, and finally intervenes in the attention outputs during inference to facilitate cross-lingual visual perception capability alignment. Extensive experiments demonstrate that CLAIM achieves an average improvement of 13.56% (up to 30% in Spanish) on the POPE and 21.75% on the hallucination subsets of the MME benchmark across various languages. Further analysis reveals that multilingual attention divergence is most prominent in intermediate layers, highlighting their critical role in multilingual scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CLAIMï¼Œä¸€ç§æ—¨åœ¨ç¼“è§£å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLarge Vision-Language Models, LVLMsï¼‰ä¸­å¤šè¯­è¨€ç‰©ä½“å¹»è§‰ï¼ˆmultilingual object hallucinationï¼‰çš„è¿‘ä¹æ— éœ€è®­ç»ƒçš„æ–¹æ³•ã€‚é’ˆå¯¹LVLMsåœ¨å¤„ç†éè‹±è¯­æŸ¥è¯¢æ—¶æ¯”è‹±è¯­æ›´å®¹æ˜“äº§ç”Ÿå¹»è§‰çš„ç°è±¡ï¼ŒCLAIMé€šè¿‡è·¨è¯­è¨€æ³¨æ„åŠ›å¹²é¢„ï¼ˆCross-Lingual Attention Interventionï¼‰æ¥å¯¹é½ä¸åŒè¯­è¨€é—´çš„è·¨æ¨¡æ€æ³¨æ„åŠ›æ¨¡å¼ã€‚è¯¥æ–¹æ³•é¦–å…ˆè¯†åˆ«ç‰¹å®šè¯­è¨€çš„æ³¨æ„åŠ›å¤´å¹¶ä¼°ç®—ä»è‹±è¯­åˆ°ç›®æ ‡è¯­è¨€çš„åç§»å‘é‡ï¼Œéšååœ¨æ¨ç†è¿‡ç¨‹ä¸­å¹²é¢„æ³¨æ„åŠ›è¾“å‡ºä»¥å¢å¼ºè·¨è¯­è¨€è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCLAIMåœ¨POPEå’ŒMMEåŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«å®ç°äº†13.56%å’Œ21.75%çš„å¹³å‡æ€§èƒ½æå‡ï¼Œåœ¨è¥¿ç­ç‰™è¯­æµ‹è¯•ä¸­æå‡å¹…åº¦ç”šè‡³è¾¾åˆ°30%ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºå¤šè¯­è¨€æ³¨æ„åŠ›å·®å¼‚åœ¨æ¨¡å‹çš„ä¸­é—´å±‚æœ€ä¸ºçªå‡ºï¼Œå¼ºè°ƒäº†è¿™äº›å±‚åœ¨å¤šè¯­è¨€å¤šæ¨¡æ€åœºæ™¯ä¸­çš„æ ¸å¿ƒåœ°ä½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL2025 Main",
      "pdf_url": "https://arxiv.org/pdf/2506.11073v1",
      "published_date": "2025-06-03 11:17:16 UTC",
      "updated_date": "2025-06-03 11:17:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:07:48.301671+00:00"
    },
    {
      "arxiv_id": "2506.02752v1",
      "title": "BenLOC: A Benchmark for Learning to Configure MIP Optimizers",
      "title_zh": "BenLOCï¼šæ··åˆæ•´æ•°è§„åˆ’ä¼˜åŒ–å™¨é…ç½®å­¦ä¹ åŸºå‡†",
      "authors": [
        "Hongpei Li",
        "Ziyan He",
        "Yufei Wang",
        "Wenting Tu",
        "Shanwen Pu",
        "Qi Deng",
        "Dongdong Ge"
      ],
      "abstract": "The automatic configuration of Mixed-Integer Programming (MIP) optimizers has become increasingly critical as the large number of configurations can significantly affect solver performance. Yet the lack of standardized evaluation frameworks has led to data leakage and over-optimistic claims, as prior studies often rely on homogeneous datasets and inconsistent experimental setups. To promote a fair evaluation process, we present BenLOC, a comprehensive benchmark and open-source toolkit, which not only offers an end-to-end pipeline for learning instance-wise MIP optimizer configurations, but also standardizes dataset selection, train-test splits, feature engineering and baseline choice for unbiased and comprehensive evaluations. Leveraging this framework, we conduct an empirical analysis on five well-established MIP datasets and compare classical machine learning models with handcrafted features against state-of-the-art deep-learning techniques. The results demonstrate the importance of datasets, features and baseline criteria proposed by BenLOC and the effectiveness of BenLOC in providing unbiased and comprehensive evaluations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† BenLOCï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºå­¦ä¹ é…ç½® Mixed-Integer Programming (MIP) ä¼˜åŒ–å™¨çš„ç»¼åˆåŸºå‡†æµ‹è¯•å’Œå¼€æºå·¥å…·åŒ…ï¼Œæ—¨åœ¨è§£å†³ç›®å‰è¯¥é¢†åŸŸå› ç¼ºä¹æ ‡å‡†åŒ–è¯„ä¼°æ¡†æ¶è€Œå¯¼è‡´çš„æ•°æ®æ³„éœ²åŠå®éªŒè®¾ç½®ä¸ä¸€è‡´ç­‰é—®é¢˜ã€‚BenLOC æä¾›äº†ä¸€ä¸ªå­¦ä¹ å®ä¾‹çº§ MIP ä¼˜åŒ–å™¨é…ç½®çš„ç«¯åˆ°ç«¯æµæ°´çº¿ï¼Œå¹¶å¯¹æ•°æ®é›†é€‰æ‹©ã€è®­ç»ƒæµ‹è¯•åˆ’åˆ†ã€ç‰¹å¾å·¥ç¨‹ä»¥åŠåŸºå‡†æ¨¡å‹é€‰æ‹©è¿›è¡Œäº†æ ‡å‡†åŒ–ï¼Œä»¥ç¡®ä¿è¯„ä¼°è¿‡ç¨‹çš„å…¬æ­£ä¸æ— åã€‚åˆ©ç”¨è¯¥æ¡†æ¶ï¼Œç ”ç©¶è€…åœ¨äº”ä¸ªæˆç†Ÿçš„ MIP æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®è¯åˆ†æï¼Œå¯¹æ¯”äº†é‡‡ç”¨æ‰‹å·¥ç‰¹å¾çš„ç»å…¸ Machine Learning æ¨¡å‹ä¸æœ€å…ˆè¿›çš„ Deep Learning æŠ€æœ¯ã€‚å®éªŒç»“æœè¯æ˜äº† BenLOC æ‰€æå‡ºçš„æ•°æ®é›†ã€ç‰¹å¾å’ŒåŸºå‡†æ ‡å‡†åœ¨å®ç°æ— åä¸”å…¨é¢è¯„ä¼°ä¸­çš„å…³é”®ä½œç”¨ï¼Œä¸º MIP ä¼˜åŒ–å™¨çš„è‡ªåŠ¨é…ç½®ç ”ç©¶æä¾›äº†å¯é çš„è¯„ä»·ä½“ç³»ã€‚",
      "categories": [
        "math.OC",
        "cs.AI"
      ],
      "primary_category": "math.OC",
      "comment": "A Benchmark for learning to configurate MIP Optimizers (Solvers)",
      "pdf_url": "https://arxiv.org/pdf/2506.02752v1",
      "published_date": "2025-06-03 11:16:24 UTC",
      "updated_date": "2025-06-03 11:16:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:08:02.390917+00:00"
    },
    {
      "arxiv_id": "2506.02749v1",
      "title": "Knowledge Graph Completion by Intermediate Variables Regularization",
      "title_zh": "åŸºäºä¸­é—´å˜é‡æ­£åˆ™åŒ–çš„çŸ¥è¯†å›¾è°±è¡¥å…¨",
      "authors": [
        "Changyi Xiao",
        "Yixin Cao"
      ],
      "abstract": "Knowledge graph completion (KGC) can be framed as a 3-order binary tensor completion task. Tensor decomposition-based (TDB) models have demonstrated strong performance in KGC. In this paper, we provide a summary of existing TDB models and derive a general form for them, serving as a foundation for further exploration of TDB models. Despite the expressiveness of TDB models, they are prone to overfitting. Existing regularization methods merely minimize the norms of embeddings to regularize the model, leading to suboptimal performance. Therefore, we propose a novel regularization method for TDB models that addresses this limitation. The regularization is applicable to most TDB models and ensures tractable computation. Our method minimizes the norms of intermediate variables involved in the different ways of computing the predicted tensor. To support our regularization method, we provide a theoretical analysis that proves its effect in promoting low trace norm of the predicted tensor to reduce overfitting. Finally, we conduct experiments to verify the effectiveness of our regularization technique as well as the reliability of our theoretical analysis. The code is available at https://github.com/changyi7231/IVR.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å°†çŸ¥è¯†å›¾è°±è¡¥å…¨(Knowledge Graph Completion, KGC)å»ºæ¨¡ä¸ºä¸‰é˜¶äºŒå…ƒå¼ é‡è¡¥å…¨ä»»åŠ¡ï¼Œå¹¶é’ˆå¯¹å¼ é‡åˆ†è§£æ¨¡å‹(Tensor Decomposition-based, TDB)æ˜“è¿‡æ‹Ÿåˆ(Overfitting)çš„é—®é¢˜æå‡ºäº†æ”¹è¿›æ–¹æ¡ˆã€‚ä½œè€…æŒ‡å‡ºï¼Œç°æœ‰çš„æ­£åˆ™åŒ–æ–¹æ³•ä»…é€šè¿‡æœ€å°åŒ–åµŒå…¥å‘é‡èŒƒæ•°æ¥çº¦æŸæ¨¡å‹ï¼Œå¾€å¾€å¯¼è‡´æ€§èƒ½æ¬ ä½³ã€‚ä¸ºæ­¤ï¼Œè¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§é€šç”¨çš„ä¸­é—´å˜é‡æ­£åˆ™åŒ–(Intermediate Variables Regularization, IVR)æ–¹æ³•ï¼Œé€šè¿‡æœ€å°åŒ–è®¡ç®—é¢„æµ‹å¼ é‡è¿‡ç¨‹ä¸­æ¶‰åŠçš„å„ç§ä¸­é—´å˜é‡çš„èŒƒæ•°æ¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ç†è®ºåˆ†æè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆä¿ƒè¿›é¢„æµ‹å¼ é‡çš„ä½è¿¹èŒƒæ•°(Low Trace Norm)ï¼Œä»è€Œåœ¨æ•°å­¦å±‚é¢ä¿éšœäº†å¯¹è¿‡æ‹Ÿåˆçš„æŠ‘åˆ¶æ•ˆæœã€‚å®éªŒç»“æœä¸ä»…éªŒè¯äº†IVRåœ¨æå‡å¤šç§TDBæ¨¡å‹æ€§èƒ½ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œä¹Ÿè¯å®äº†å…¶ç†è®ºæ¨å¯¼çš„å¯é æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02749v1",
      "published_date": "2025-06-03 11:11:33 UTC",
      "updated_date": "2025-06-03 11:11:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:08:07.802834+00:00"
    },
    {
      "arxiv_id": "2506.02746v1",
      "title": "Solving the Pod Repositioning Problem with Deep Reinforced Adaptive Large Neighborhood Search",
      "title_zh": "åŸºäºæ·±åº¦å¼ºåŒ–è‡ªé€‚åº”å¤§é‚»åŸŸæœç´¢çš„è´§æ¶é‡å®šä½é—®é¢˜æ±‚è§£",
      "authors": [
        "Lin Xie",
        "Hanyi Li"
      ],
      "abstract": "The Pod Repositioning Problem (PRP) in Robotic Mobile Fulfillment Systems (RMFS) involves selecting optimal storage locations for pods returning from pick stations. This work presents an improved solution method that integrates Adaptive Large Neighborhood Search (ALNS) with Deep Reinforcement Learning (DRL). A DRL agent dynamically selects destroy and repair operators and adjusts key parameters such as destruction degree and acceptance thresholds during the search. Specialized heuristics for both operators are designed to reflect PRP-specific characteristics, including pod usage frequency and movement costs. Computational results show that this DRL-guided ALNS outperforms traditional approaches such as cheapest-place, fixed-place, binary integer programming, and static heuristics. The method demonstrates strong solution quality and illustrating the benefit of learning-driven control within combinatorial optimization for warehouse systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººç§»åŠ¨å±¥è¡Œç³»ç»Ÿ(RMFS)ä¸­çš„è´§æ¶é‡å®šä½é—®é¢˜(Pod Repositioning Problem, PRP)ï¼Œæå‡ºäº†ä¸€ç§å°†è‡ªé€‚åº”å¤§é‚»åŸŸæœç´¢(ALNS)ä¸æ·±åº¦å¼ºåŒ–å­¦ä¹ (DRL)ç›¸ç»“åˆçš„åˆ›æ–°æ±‚è§£æ¡†æ¶ã€‚é€šè¿‡å¼•å…¥DRLæ™ºèƒ½ä½“ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨æœç´¢è¿‡ç¨‹ä¸­åŠ¨æ€é€‰æ‹©ç ´å(destroy)å’Œä¿®å¤(repair)ç®—å­ï¼Œå¹¶å®æ—¶è°ƒæ•´ç ´åç¨‹åº¦ä¸æ¥å—é˜ˆå€¼ç­‰å…³é”®å‚æ•°ã€‚ä¸ºäº†æ›´å¥½åœ°é€‚é…PRPç‰¹æ€§ï¼Œç ”ç©¶è¿˜è®¾è®¡äº†è€ƒè™‘è´§æ¶ä½¿ç”¨é¢‘ç‡å’Œç§»åŠ¨æˆæœ¬çš„ä¸“é—¨å¯å‘å¼ç®—æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™ç§DRLå¼•å¯¼çš„ALNSæ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„äºŒè¿›åˆ¶æ•´æ•°è§„åˆ’(BIP)å’Œå„ç±»é™æ€å¯å‘å¼ç­–ç•¥ã€‚è¯¥ç ”ç©¶ä¸ä»…æå‡äº†å¤§è§„æ¨¡ä»“å‚¨ç³»ç»Ÿçš„ä¼˜åŒ–æ•ˆç‡ï¼Œä¹Ÿå±•ç¤ºäº†å­¦ä¹ é©±åŠ¨æ§åˆ¶åœ¨å¤æ‚ç»„åˆä¼˜åŒ–é—®é¢˜ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.RO",
      "comment": "14 pages, 2 figures, conference",
      "pdf_url": "https://arxiv.org/pdf/2506.02746v1",
      "published_date": "2025-06-03 11:07:41 UTC",
      "updated_date": "2025-06-03 11:07:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:09:06.508369+00:00"
    },
    {
      "arxiv_id": "2506.02744v1",
      "title": "Enriching Location Representation with Detailed Semantic Information",
      "title_zh": "èåˆè¯¦ç»†è¯­ä¹‰ä¿¡æ¯çš„ä½ç½®è¡¨ç¤ºå¢å¼º",
      "authors": [
        "Junyuan Liu",
        "Xinglei Wang",
        "Tao Cheng"
      ],
      "abstract": "Spatial representations that capture both structural and semantic characteristics of urban environments are essential for urban modeling. Traditional spatial embeddings often prioritize spatial proximity while underutilizing fine-grained contextual information from places. To address this limitation, we introduce CaLLiPer+, an extension of the CaLLiPer model that systematically integrates Point-of-Interest (POI) names alongside categorical labels within a multimodal contrastive learning framework. We evaluate its effectiveness on two downstream tasks, land use classification and socioeconomic status distribution mapping, demonstrating consistent performance gains of 4% to 11% over baseline methods. Additionally, we show that incorporating POI names enhances location retrieval, enabling models to capture complex urban concepts with greater precision. Ablation studies further reveal the complementary role of POI names and the advantages of leveraging pretrained text encoders for spatial representations. Overall, our findings highlight the potential of integrating fine-grained semantic attributes and multimodal learning techniques to advance the development of urban foundation models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CaLLiPer+ï¼Œè¿™æ˜¯å¯¹ CaLLiPer æ¨¡å‹çš„æ‰©å±•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿç©ºé—´åµŒå…¥ (spatial embeddings) åœ¨æ•æ‰åŸå¸‚ç¯å¢ƒç»†ç²’åº¦è¯­ä¹‰ä¿¡æ¯æ–¹é¢çš„ä¸è¶³ã€‚è¯¥æ¡†æ¶åœ¨ä¸€ä¸ªå¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹  (multimodal contrastive learning) æ¶æ„ä¸­ï¼Œç³»ç»Ÿåœ°æ•´åˆäº†å…´è¶£ç‚¹ (POI) åç§°åŠå…¶ç±»åˆ«æ ‡ç­¾ã€‚ç ”ç©¶äººå‘˜åœ¨åœŸåœ°åˆ©ç”¨åˆ†ç±» (land use classification) å’Œç¤¾ä¼šç»æµåœ°ä½åˆ†å¸ƒåˆ¶å›¾ (socioeconomic status distribution mapping) ä¸¤é¡¹ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¯„ä¼°äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCaLLiPer+ ç›¸æ¯”åŸºçº¿æ–¹æ³•å®ç°äº† 4% è‡³ 11% çš„æŒç»­æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯æ˜å¼•å…¥ POI åç§°èƒ½æ˜¾è‘—å¢å¼ºä½ç½®æ£€ç´¢ (location retrieval) èƒ½åŠ›ï¼Œä½¿æ¨¡å‹èƒ½æ›´ç²¾å‡†åœ°æ•æ‰å¤æ‚çš„åŸå¸‚æ¦‚å¿µã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº† POI åç§°ä¸é¢„è®­ç»ƒæ–‡æœ¬ç¼–ç å™¨ (pretrained text encoders) åœ¨ç©ºé—´è¡¨ç¤ºä¸­çš„äº’è¡¥ä½œç”¨ã€‚è¯¥æˆæœå±•ç¤ºäº†é›†æˆç»†ç²’åº¦è¯­ä¹‰å±æ€§å’Œå¤šæ¨¡æ€å­¦ä¹ æŠ€æœ¯åœ¨æ¨åŠ¨åŸå¸‚åŸºç¡€æ¨¡å‹ (urban foundation models) å‘å±•æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "primary_category": "cs.CE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02744v1",
      "published_date": "2025-06-03 11:06:51 UTC",
      "updated_date": "2025-06-03 11:06:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:08:09.705071+00:00"
    },
    {
      "arxiv_id": "2506.02742v1",
      "title": "Prompt-Unseen-Emotion: Zero-shot Expressive Speech Synthesis with Prompt-LLM Contextual Knowledge for Mixed Emotions",
      "title_zh": "Prompt-Unseen-Emotionï¼šåˆ©ç”¨ Prompt-LLM ä¸Šä¸‹æ–‡çŸ¥è¯†å®ç°æ··åˆæƒ…æ„Ÿçš„é›¶æ ·æœ¬è¡¨ç°åŠ›è¯­éŸ³åˆæˆ",
      "authors": [
        "Xiaoxue Gao",
        "Huayun Zhang",
        "Nancy F. Chen"
      ],
      "abstract": "Existing expressive text-to-speech (TTS) systems primarily model a limited set of categorical emotions, whereas human conversations extend far beyond these predefined emotions, making it essential to explore more diverse emotional speech generation for more natural interactions. To bridge this gap, this paper proposes a novel prompt-unseen-emotion (PUE) approach to generate unseen emotional speech via emotion-guided prompt learning. PUE is trained utilizing an LLM-TTS architecture to ensure emotional consistency between categorical emotion-relevant prompts and emotional speech, allowing the model to quantitatively capture different emotion weightings per utterance. During inference, mixed emotional speech can be generated by flexibly adjusting emotion proportions and leveraging LLM contextual knowledge, enabling the model to quantify different emotional styles. Our proposed PUE successfully facilitates expressive speech synthesis of unseen emotions in a zero-shot setting.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Prompt-Unseen-Emotion (PUE)æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è¡¨ç°åŠ›æ–‡æœ¬è½¬è¯­éŸ³(TTS)ç³»ç»Ÿä»…èƒ½æ¨¡æ‹Ÿæœ‰é™ç±»åˆ«æƒ…æ„Ÿã€æ— æ³•æ»¡è¶³è‡ªç„¶å¯¹è¯éœ€æ±‚çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡æƒ…æ„Ÿå¼•å¯¼çš„æç¤ºå­¦ä¹ (emotion-guided prompt learning)ï¼Œåœ¨é›¶æ ·æœ¬(zero-shot)è®¾ç½®ä¸‹å®ç°äº†æœªè§æƒ…æ„Ÿè¯­éŸ³çš„ç”Ÿæˆã€‚PUEåˆ©ç”¨LLM-TTSæ¶æ„è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿå®šé‡æ•æ‰æ¯æ®µè¯è¯­ä¸­çš„æƒ…æ„Ÿæƒé‡ï¼Œç¡®ä¿æç¤ºå†…å®¹ä¸è¯­éŸ³è¡¨è¾¾çš„æƒ…æ„Ÿä¸€è‡´æ€§ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œé€šè¿‡ç»“åˆå¤§è¯­è¨€æ¨¡å‹(LLM)çš„ä¸Šä¸‹æ–‡çŸ¥è¯†å¹¶çµæ´»è°ƒæ•´æƒ…æ„Ÿæ¯”ä¾‹ï¼Œæ¨¡å‹å¯ä»¥ç”Ÿæˆå¤æ‚çš„æ··åˆæƒ…æ„Ÿ(mixed emotions)è¯­éŸ³ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPUEæˆåŠŸå®ç°äº†é«˜åº¦å¯æ§ä¸”å…·æœ‰è¡¨ç°åŠ›çš„é›¶æ ·æœ¬è¯­éŸ³åˆæˆï¼Œä¸ºæ›´è‡ªç„¶çš„äººæœºäº¤äº’å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD",
        "eess.SP"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02742v1",
      "published_date": "2025-06-03 10:59:22 UTC",
      "updated_date": "2025-06-03 10:59:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:10:22.034799+00:00"
    },
    {
      "arxiv_id": "2506.02739v1",
      "title": "Why do AI agents communicate in human language?",
      "title_zh": "AIæ™ºèƒ½ä½“ä¸ºä½•ä½¿ç”¨äººç±»è¯­è¨€è¿›è¡Œé€šä¿¡ï¼Ÿ",
      "authors": [
        "Pengcheng Zhou",
        "Yinglun Feng",
        "Halimulati Julaiti",
        "Zhongliang Yang"
      ],
      "abstract": "Large Language Models (LLMs) have become foundational to modern AI agent systems, enabling autonomous agents to reason and plan. In most existing systems, inter-agent communication relies primarily on natural language. While this design supports interpretability and human oversight, we argue that it introduces fundamental limitations in agent-to-agent coordination. The semantic space of natural language is structurally misaligned with the high-dimensional vector spaces in which LLMs operate, resulting in information loss and behavioral drift. Beyond surface-level inefficiencies, we highlight a deeper architectural limitation: current LLMs were not trained with the objective of supporting agentic behavior. As such, they lack mechanisms for modeling role continuity, task boundaries, and multi-agent dependencies. The standard next-token prediction paradigm fails to support the structural alignment required for robust, scalable agent coordination. Based on this, we argue that two core questions deserve careful examination: first, given that AI agents fundamentally operate in high-dimensional vector spaces, should they rely on a language system originally designed for human cognition as their communication medium? Second, should we consider developing a new model construction paradigm that builds models from the ground up to natively support structured communication, shared intentionality, and task alignment in multi-role, multi-agent environments? This paper calls for a reconsideration not only of how agents should communicate, but also of what it fundamentally means to train a model that natively supports multi-agent coordination and communication.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é©±åŠ¨çš„æ™ºèƒ½ä½“ç³»ç»Ÿåœ¨é€šä¿¡ä¸­è¿‡åº¦ä¾èµ–è‡ªç„¶è¯­è¨€çš„å±€é™æ€§ï¼ŒæŒ‡å‡ºè‡ªç„¶è¯­è¨€çš„è¯­ä¹‰ç©ºé—´ä¸ LLMs çš„é«˜ç»´å‘é‡ç©ºé—´ï¼ˆhigh-dimensional vector spacesï¼‰å­˜åœ¨ç»“æ„æ€§é”™ä½ã€‚è¿™ç§é”™ä½ä¸ä»…å¯¼è‡´äº†ä¿¡æ¯ä¸¢å¤±å’Œè¡Œä¸ºåç§»ï¼ˆbehavioral driftï¼‰ï¼Œä¸”ç°æœ‰çš„ä¸‹ä¸€æ ‡è®°é¢„æµ‹ï¼ˆnext-token predictionï¼‰èŒƒå¼æ— æ³•æœ‰æ•ˆæ”¯æŒå¤šæ™ºèƒ½ä½“é—´çš„ç»“æ„å¯¹é½ã€‚ä½œè€…è®¤ä¸ºï¼Œå½“å‰çš„ LLMs ç¼ºä¹å»ºæ¨¡è§’è‰²è¿ç»­æ€§å’Œå¤šæ™ºèƒ½ä½“ä¾èµ–å…³ç³»çš„åŸç”Ÿæœºåˆ¶ï¼Œæœ¬è´¨ä¸Šå¹¶ä¸æ”¯æŒé«˜æ•ˆçš„æ™ºèƒ½ä½“åä½œã€‚åŸºäºæ­¤ï¼Œè®ºæ–‡è´¨ç–‘äº†å°†äººç±»è¯­è¨€ä½œä¸ºæ™ºèƒ½ä½“é€šä¿¡åª’ä»‹çš„åˆç†æ€§ï¼Œå¹¶å‘¼åå¼€å‘ä¸€ç§å…¨æ–°çš„æ¨¡å‹æ„å»ºèŒƒå¼ã€‚è¯¥èŒƒå¼æ—¨åœ¨ä»åº•å±‚åŸç”Ÿæ”¯æŒç»“æ„åŒ–é€šä¿¡ã€å…±äº«æ„å›¾ï¼ˆshared intentionalityï¼‰å’Œä»»åŠ¡å¯¹é½ï¼Œä¸ºæ„å»ºæ›´ç¨³å¥ã€å¯æ‰©å±•çš„å¤šæ™ºèƒ½ä½“ç¯å¢ƒå¥ å®šåŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02739v1",
      "published_date": "2025-06-03 10:53:29 UTC",
      "updated_date": "2025-06-03 10:53:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:11:25.493218+00:00"
    },
    {
      "arxiv_id": "2506.02733v2",
      "title": "LinkTo-Anime: A 2D Animation Optical Flow Dataset from 3D Model Rendering",
      "title_zh": "LinkTo-Animeï¼šåŸºäº 3D æ¨¡å‹æ¸²æŸ“çš„äºŒç»´åŠ¨ç”»å…‰æµæ•°æ®é›†",
      "authors": [
        "Xiaoyi Feng",
        "Kaifeng Zou",
        "Caichun Cen",
        "Tao Huang",
        "Hui Guo",
        "Zizhou Huang",
        "Yingli Zhao",
        "Mingqing Zhang",
        "Ziyuan Zheng",
        "Diwei Wang",
        "Yuntao Zou",
        "Dagang Li"
      ],
      "abstract": "Existing optical flow datasets focus primarily on real-world simulation or synthetic human motion, but few are tailored to Celluloid(cel) anime character motion: a domain with unique visual and motion characteristics. To bridge this gap and facilitate research in optical flow estimation and downstream tasks such as anime video generation and line drawing colorization, we introduce LinkTo-Anime, the first high-quality dataset specifically designed for cel anime character motion generated with 3D model rendering. LinkTo-Anime provides rich annotations including forward and backward optical flow, occlusion masks, and Mixamo Skeleton. The dataset comprises 395 video sequences, totally 24,230 training frames, 720 validation frames, and 4,320 test frames. Furthermore, a comprehensive benchmark is constructed with various optical flow estimation methods to analyze the shortcomings and limitations across multiple datasets.",
      "tldr_zh": "ç°æœ‰çš„å…‰æµæ•°æ®é›†ä¸»è¦å…³æ³¨ç°å®æ¨¡æ‹Ÿæˆ–åˆæˆäººä½“è¿åŠ¨ï¼Œä½†åœ¨å…·æœ‰ç‹¬ç‰¹è§†è§‰å’Œè¿åŠ¨ç‰¹æ€§çš„èµ›ç’ç’(cel)åŠ¨ç”»è§’è‰²è¿åŠ¨é¢†åŸŸä»å­˜åœ¨æ˜¾è‘—ç©ºç™½ã€‚è¯¥ç ”ç©¶æ¨å‡ºäº† LinkTo-Animeï¼Œè¿™æ˜¯é¦–ä¸ªé€šè¿‡ 3D æ¨¡å‹æ¸²æŸ“ç”Ÿæˆçš„ä¸“é—¨é’ˆå¯¹èµ›ç’ç’åŠ¨ç”»è§’è‰²è¿åŠ¨çš„é«˜è´¨é‡æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æä¾›äº†ä¸°å¯Œçš„æ ‡æ³¨ä¿¡æ¯ï¼ŒåŒ…æ‹¬å‰å‘å’Œåå‘å…‰æµ(Optical Flow)ã€é®æŒ¡æ©ç (Occlusion Masks)ä»¥åŠ Mixamo Skeletonã€‚LinkTo-Anime åŒ…å« 395 ä¸ªè§†é¢‘åºåˆ—ï¼Œæ¶µç›–äº†è¶…è¿‡ 24,000 å¸§è®­ç»ƒå›¾åƒåŠç›¸åº”è§„æ¨¡çš„éªŒè¯ä¸æµ‹è¯•é›†ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…åˆ©ç”¨å¤šç§å…‰æµä¼°è®¡(Optical Flow Estimation)æ–¹æ³•æ„å»ºäº†ç»¼åˆåŸºå‡†(Benchmark)ï¼Œæ·±å…¥åˆ†æäº†ç°æœ‰æ•°æ®é›†çš„å±€é™æ€§ã€‚è¯¥æ•°æ®é›†æ—¨åœ¨ä¸ºå…‰æµä¼°è®¡ã€åŠ¨ç”»è§†é¢‘ç”Ÿæˆ(Anime Video Generation)å’Œçº¿ç¨¿ä¸Šè‰²(Line Drawing Colorization)ç­‰ä¸‹æ¸¸ä»»åŠ¡çš„ç ”ç©¶æä¾›å…³é”®æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02733v2",
      "published_date": "2025-06-03 10:50:20 UTC",
      "updated_date": "2025-07-29 04:07:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:10:41.384449+00:00"
    },
    {
      "arxiv_id": "2506.02726v1",
      "title": "RACE-Align: Retrieval-Augmented and Chain-of-Thought Enhanced Preference Alignment for Large Language Models",
      "title_zh": "RACE-Alignï¼šæ£€ç´¢å¢å¼ºä¸é“¾å¼æ€ç»´å¢å¼ºçš„å¤§è¯­è¨€æ¨¡å‹åå¥½å¯¹é½",
      "authors": [
        "Qihang Yan",
        "Xinyu Zhang",
        "Luming Guo",
        "Qi Zhang",
        "Feifan Liu"
      ],
      "abstract": "Large Language Models (LLMs) struggle with accuracy, domain-specific reasoning, and interpretability in vertical domains. Traditional preference alignment methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) often overlook the underlying knowledge sources and reasoning logic. This paper introduces RACE-Align (Retrieval-Augmented and Chain-of-Thought Enhanced Alignment), a novel framework designed to address these limitations. RACE-Align systematically constructs a binary preference dataset incorporating external knowledge support and explicit Chain-of-Thought (CoT) reasoning, then aligns LLMs using the DPO algorithm. The core innovation lies in its preference data construction strategy: it integrates AI-driven retrieval for factual grounding, enhancing knowledgeability and accuracy, and emphasizes the optimization of domain-specific CoT, treating the reasoning process itself as a key preference dimension. A multi-stage, AI-driven refinement pipeline cost-effectively generates these preference pairs. Experimental validation in Traditional Chinese Medicine (TCM) using Qwen3-1.7B as the base model demonstrates that RACE-Align significantly outperforms the original base model and a model fine-tuned only with Supervised Fine-Tuning (SFT). Improvements were observed across multiple dimensions, including answer accuracy, information richness, application of TCM thinking patterns, logicality and depth of reasoning, and interpretability. These findings suggest RACE-Align offers an effective pathway to enhance LLMs' knowledge application, reasoning reliability, and process transparency in complex vertical domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RACE-Align (Retrieval-Augmented and Chain-of-Thought Enhanced Alignment)ï¼Œæ—¨åœ¨è§£å†³ Large Language Models (LLMs) åœ¨å‚ç›´é¢†åŸŸé¢ä¸´çš„å‡†ç¡®æ€§ã€æ¨ç†èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ä¸è¶³ç­‰æŒ‘æˆ˜ã€‚é’ˆå¯¹ä¼ ç»Ÿåå¥½å¯¹é½æ–¹æ³•ï¼ˆå¦‚ RLHF å’Œ DPOï¼‰å¿½è§†çŸ¥è¯†æ¥æºå’Œé€»è¾‘æ¨ç†çš„é—®é¢˜ï¼ŒRACE-Align é€šè¿‡æ•´åˆæ£€ç´¢å¢å¼ºæŠ€æœ¯æä¾›äº‹å®æ”¯æ’‘ï¼Œå¹¶å¼•å…¥æ˜¾å¼çš„ Chain-of-Thought (CoT) æ¨ç†æ¥æ„å»ºé«˜è´¨é‡çš„äºŒå…ƒåå¥½æ•°æ®é›†ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†é¢†åŸŸç‰¹å®šçš„æ¨ç†è¿‡ç¨‹æœ¬èº«ä½œä¸ºåå¥½ä¼˜åŒ–çš„å…³é”®ç»´åº¦ï¼Œå¹¶åˆ©ç”¨å¤šé˜¶æ®µ AI é©±åŠ¨çš„æµæ°´çº¿ç»æµåœ°ç”Ÿæˆåå¥½å¯¹ã€‚åœ¨ä¸­åŒ»è¯ (Traditional Chinese Medicine, TCM) é¢†åŸŸä»¥ Qwen3-1.7B ä¸ºåŸºåº§æ¨¡å‹çš„å®éªŒæ˜¾ç¤ºï¼ŒRACE-Align åœ¨å›ç­”å‡†ç¡®æ€§ã€é€»è¾‘æ·±åº¦å’Œä¸­åŒ»æ€ç»´åº”ç”¨ç­‰æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹åŠä»…ç»è¿‡ Supervised Fine-Tuning (SFT) çš„æ¨¡å‹ã€‚ç ”ç©¶ç»“æœè¯æ˜ï¼ŒRACE-Align ä¸ºå¢å¼º LLMs åœ¨å¤æ‚ä¸“ä¸šé¢†åŸŸçš„çŸ¥è¯†åº”ç”¨ã€æ¨ç†å¯é æ€§åŠè¿‡ç¨‹é€æ˜åº¦æä¾›äº†ä¸€æ¡æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02726v1",
      "published_date": "2025-06-03 10:36:38 UTC",
      "updated_date": "2025-06-03 10:36:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:10:37.692189+00:00"
    },
    {
      "arxiv_id": "2506.02720v3",
      "title": "LocalGPT: Benchmarking and Advancing Large Language Models for Local Life Services in Meituan",
      "title_zh": "LocalGPTï¼šé¢å‘ Meituan æœ¬åœ°ç”Ÿæ´»æœåŠ¡çš„å¤§è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸èƒ½åŠ›æå‡",
      "authors": [
        "Xiaochong Lan",
        "Jie Feng",
        "Jiahuan Lei",
        "Xinlei Shi",
        "Yong Li"
      ],
      "abstract": "Large language models (LLMs) have exhibited remarkable capabilities and achieved significant breakthroughs across various domains, leading to their widespread adoption in recent years. Building on this progress, we investigate their potential in the realm of local life services. In this study, we establish a comprehensive benchmark and systematically evaluate the performance of diverse LLMs across a wide range of tasks relevant to local life services. To further enhance their effectiveness, we explore two key approaches: model fine-tuning and agent-based workflows. Our findings reveal that even a relatively compact 7B model can attain performance levels comparable to a much larger 72B model, effectively balancing inference cost and model capability. This optimization greatly enhances the feasibility and efficiency of deploying LLMs in real-world online services, making them more practical and accessible for local life applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æœ¬åœ°ç”Ÿæ´»æœåŠ¡é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ï¼Œå¹¶é’ˆå¯¹ç¾å›¢åœºæ™¯å»ºç«‹äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°åŸºå‡†ã€‚ç ”ç©¶è€…ç³»ç»Ÿåœ°æµ‹è¯•äº†å¤šç§LLMsåœ¨ç›¸å…³ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¹¶è¿›ä¸€æ­¥é€šè¿‡æ¨¡å‹å¾®è°ƒ(Fine-tuning)å’ŒåŸºäºæ™ºèƒ½ä½“çš„å·¥ä½œæµ(Agent-based workflows)ä¸¤ç§æ–¹æ³•æ¥å¢å¼ºå…¶æ•ˆèƒ½ã€‚ç ”ç©¶ç»“æœå‘ç°ï¼Œç»è¿‡ä¼˜åŒ–çš„7Bæ¨¡å‹èƒ½å¤Ÿè¾¾åˆ°ä¸72Bæ¨¡å‹ç›¸å½“çš„æ€§èƒ½æ°´å¹³ï¼ŒæˆåŠŸåœ¨æ¨ç†æˆæœ¬(Inference cost)å’Œæ¨¡å‹èƒ½åŠ›ä¹‹é—´å–å¾—äº†å¹³è¡¡ã€‚è¿™ä¸€è¿›å±•æ˜¾è‘—æå‡äº†LLMsåœ¨çœŸå®ä¸–ç•Œåœ¨çº¿æœåŠ¡ä¸­éƒ¨ç½²çš„å¯è¡Œæ€§ä¸æ•ˆç‡ï¼Œä¸ºæœ¬åœ°ç”Ÿæ´»æœåŠ¡åº”ç”¨æä¾›äº†æ›´å…·å®è·µæ„ä¹‰çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "KDD 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.02720v3",
      "published_date": "2025-06-03 10:18:19 UTC",
      "updated_date": "2025-10-24 07:34:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:09:50.200425+00:00"
    },
    {
      "arxiv_id": "2506.02718v1",
      "title": "Heterogeneous Group-Based Reinforcement Learning for LLM-based Multi-Agent Systems",
      "title_zh": "é¢å‘åŸºäº LLM çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å¼‚æ„åˆ†ç»„å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Guanzhong Chen",
        "Shaoxiong Yang",
        "Chao Li",
        "Wei Liu",
        "Jian Luan",
        "Zenglin Xu"
      ],
      "abstract": "Large Language Models (LLMs) have achieved remarkable success across diverse natural language processing tasks, yet their deployment in real-world applications is hindered by fixed knowledge cutoffs and difficulties in generating controllable, accurate outputs in a single inference. Multi-agent systems (MAS) built from specialized LLM agents offer a promising solution, enabling dynamic collaboration and iterative reasoning. However, optimizing these systems remains a challenge, as conventional methods such as prompt engineering and supervised fine-tuning entail high engineering overhead and limited adaptability. Reinforcement learning (RL), particularly multi-agent reinforcement learning (MARL), provides a scalable framework by refining agent policies based on system-level feedback. Nevertheless, existing MARL algorithms, such as Multi-Agent Proximal Policy Optimization (MAPPO), rely on Critic networks, which can cause training instability and increase computational burden. To address these limitations and target the prototypical Multi-Agent Search System (MASS), we propose Multi-Agent Heterogeneous Group Policy Optimization (MHGPO), a novel Critic-free algorithm that guides policy updates by estimating relative reward advantages across heterogeneous groups of rollouts. MHGPO eliminates the need for Critic networks, enhancing stability and reducing computational overhead. Additionally, we introduce three group rollout sampling strategies that trade off between efficiency and effectiveness. Experiments on a multi-agent LLM-based search system demonstrate that MHGPO consistently outperforms MAPPO in both task performance and computational efficiency, without requiring warm-up, underscoring its potential for stable and scalable optimization of complex LLM-based MAS.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å®é™…åº”ç”¨ä¸­é¢ä¸´çš„çŸ¥è¯†æ»åå’Œè¾“å‡ºä¸å¯æ§ç­‰æŒ‘æˆ˜ï¼Œæ¢è®¨äº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(MAS)çš„ä¼˜åŒ–è·¯å¾„ã€‚é’ˆå¯¹ç°æœ‰ç®—æ³•å¦‚MAPPOä¾èµ–Criticç½‘ç»œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šä¸”è®¡ç®—å¼€é”€å¤§çš„é—®é¢˜ï¼Œä½œè€…æå‡ºäº†Multi-Agent Heterogeneous Group Policy Optimization (MHGPO)ï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºå¤šæ™ºèƒ½ä½“æœç´¢ç³»ç»Ÿ(MASS)è®¾è®¡çš„Critic-freeç®—æ³•ã€‚MHGPOé€šè¿‡ä¼°ç®—å¼‚æ„æ ·æœ¬ç»„(Heterogeneous groups of rollouts)ä¹‹é—´çš„ç›¸å¯¹å¥–åŠ±ä¼˜åŠ¿æ¥å¼•å¯¼ç­–ç•¥æ›´æ–°ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹Criticç½‘ç»œçš„éœ€æ±‚å¹¶å¢å¼ºäº†ç³»ç»Ÿç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä¸‰ç§ç»„é‡‡æ ·ç­–ç•¥ä»¥å¹³è¡¡è¿è¡Œæ•ˆç‡ä¸æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMHGPOåœ¨ä»»åŠ¡æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ä¸Šå‡ä¼˜äºMAPPOï¼Œä¸”æ— éœ€é¢„çƒ­(Warm-up)ï¼Œä¸ºæ„å»ºç¨³å®šä¸”å¯æ‰©å±•çš„å¤æ‚LLMå¤šæ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†é«˜æ•ˆçš„ä¼˜åŒ–æ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02718v1",
      "published_date": "2025-06-03 10:17:19 UTC",
      "updated_date": "2025-06-03 10:17:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:10:09.086151+00:00"
    },
    {
      "arxiv_id": "2506.03224v1",
      "title": "OpenCarbon: A Contrastive Learning-based Cross-Modality Neural Approach for High-Resolution Carbon Emission Prediction Using Open Data",
      "title_zh": "OpenCarbonï¼šä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ çš„åˆ©ç”¨å…¬å¼€æ•°æ®è¿›è¡Œé«˜åˆ†è¾¨ç‡ç¢³æ’æ”¾é¢„æµ‹çš„è·¨æ¨¡æ€ç¥ç»ç½‘ç»œæ–¹æ³•",
      "authors": [
        "Jinwei Zeng",
        "Yu Liu",
        "Guozhen Zhang",
        "Jingtao Ding",
        "Yuming Lin",
        "Jian Yuan",
        "Yong Li"
      ],
      "abstract": "Accurately estimating high-resolution carbon emissions is crucial for effective emission governance and mitigation planning. While conventional methods for precise carbon accounting are hindered by substantial data collection efforts, the rise of open data and advanced learning techniques offers a promising solution. Once an open data-based prediction model is developed and trained, it can easily infer emissions for new areas based on available open data. To address this, we incorporate two modalities of open data, satellite images and point-of-interest (POI) data, to predict high-resolution urban carbon emissions, with satellite images providing macroscopic and static and POI data offering fine-grained and relatively dynamic functionality information. However, estimating high-resolution carbon emissions presents two significant challenges: the intertwined and implicit effects of various functionalities on carbon emissions, and the complex spatial contiguity correlations that give rise to the agglomeration effect. Our model, OpenCarbon, features two major designs that target the challenges: a cross-modality information extraction and fusion module to extract complementary functionality information from two modules and model their interactions, and a neighborhood-informed aggregation module to capture the spatial contiguity correlations. Extensive experiments demonstrate our model's superiority, with a significant performance gain of 26.6\\% on R2. Further generalizability tests and case studies also show OpenCarbon's capacity to capture the intrinsic relation between urban functionalities and carbon emissions, validating its potential to empower efficient carbon governance and targeted carbon mitigation planning. Codes and data are available: https://github.com/JinweiZzz/OpenCarbon.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† OpenCarbonï¼Œä¸€ç§åŸºäº Contrastive Learning çš„ Cross-Modality ç¥ç»ç½‘ç»œæ–¹æ³•ï¼Œæ—¨åœ¨åˆ©ç”¨å…¬å¼€æ•°æ®å®ç°é«˜åˆ†è¾¨ç‡åŸå¸‚ç¢³æ’æ”¾é¢„æµ‹ã€‚è¯¥æ¨¡å‹æ•´åˆäº† Satellite images çš„å®è§‚é™æ€ç‰¹å¾ä¸ Point-of-Interest (POI) æ•°æ®çš„ç»†ç²’åº¦åŠ¨æ€åŠŸèƒ½ä¿¡æ¯ï¼Œä»¥è§£å†³åŸå¸‚åŠŸèƒ½å¯¹ç¢³æ’æ”¾çš„å¤æ‚äº¤ç»‡å½±å“ã€‚é€šè¿‡è®¾è®¡è·¨æ¨¡æ€ä¿¡æ¯æå–ä¸èåˆæ¨¡å—ä»¥åŠ Neighborhood-informed aggregation æ¨¡å—ï¼ŒOpenCarbon èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰ç©ºé—´è¿é€šæ€§äº§ç”Ÿçš„é›†èšæ•ˆåº”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨ R2 æŒ‡æ ‡ä¸Šå®ç°äº† 26.6% çš„æ˜¾è‘—æ€§èƒ½æå‡ï¼Œå¹¶å±•ç°å‡ºä¼˜å¼‚çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†åŸå¸‚åŠŸèƒ½ä¸ç¢³æ’æ”¾ä¹‹é—´çš„å†…åœ¨è”ç³»ï¼Œä¸ºé«˜æ•ˆçš„ç¢³æ²»ç†å’Œé’ˆå¯¹æ€§å‡æ’è§„åˆ’æä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "physics.soc-ph"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by IJCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.03224v1",
      "published_date": "2025-06-03 10:12:10 UTC",
      "updated_date": "2025-06-03 10:12:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:11:43.032254+00:00"
    },
    {
      "arxiv_id": "2506.05384v2",
      "title": "Q-Ponder: A Unified Training Pipeline for Reasoning-based Visual Quality Assessment",
      "title_zh": "Q-Ponderï¼šåŸºäºæ¨ç†çš„è§†è§‰è´¨é‡è¯„ä»·ç»Ÿä¸€è®­ç»ƒç®¡çº¿",
      "authors": [
        "Zhuoxuan Cai",
        "Jian Zhang",
        "Xinbin Yuan",
        "Peng-Tao Jiang",
        "Wenxiang Chen",
        "Bowen Tang",
        "Lujian Yao",
        "Qiyuan Wang",
        "Jinwen Chen",
        "Bo Li"
      ],
      "abstract": "Recent studies demonstrate that multimodal large language models (MLLMs) can proficiently evaluate visual quality through interpretable assessments. However, existing approaches typically treat quality scoring and reasoning descriptions as separate tasks with disjoint optimization objectives, leading to a trade-off: models adept at quality reasoning descriptions struggle with precise score regression, while score-focused models lack interpretability. This limitation hinders the full potential of MLLMs in visual quality assessment, where accuracy and interpretability should be mutually reinforcing. To address this, we propose a unified two-stage training framework comprising a cold-start stage and a reinforcement learning-based fine-tuning stage. Specifically, in the first stage, we distill high-quality data from a teacher model through expert-designed prompts, initializing reasoning capabilities via cross-entropy loss supervision. In the second stage, we introduce a novel reward with Group Relative Policy Optimization (GRPO) to jointly optimize scoring accuracy and reasoning consistency. We designate the models derived from these two stages as Q-Ponder-CI and Q-Ponder. Extensive experiments show that Q-Ponder achieves state-of-the-art (SOTA) performance on quality score regression benchmarks, delivering up to 6.5% higher SRCC on cross-domain datasets. Furthermore, Q-Ponder significantly outperforms description-based SOTA models, including its teacher model Qwen-2.5-VL-72B, particularly in description accuracy and reasonableness, demonstrating the generalization potential over diverse tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Q-Ponderï¼Œä¸€ä¸ªç»Ÿä¸€çš„ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)åœ¨è§†è§‰è´¨é‡è¯„ä¼°(Visual Quality Assessment)ä¸­è´¨é‡è¯„åˆ†ä¸æ¨ç†æè¿°ä»»åŠ¡åˆ†ç¦»å¯¼è‡´çš„æƒè¡¡é—®é¢˜ã€‚ç¬¬ä¸€é˜¶æ®µä¸ºå†·å¯åŠ¨é˜¶æ®µï¼Œé€šè¿‡ä¸“å®¶è®¾è®¡çš„æç¤ºä»æ•™å¸ˆæ¨¡å‹ä¸­è’¸é¦é«˜è´¨é‡æ•°æ®ï¼Œå¹¶åˆ©ç”¨äº¤å‰ç†µæŸå¤±(Cross-Entropy Loss)ç›‘ç£åˆå§‹åŒ–æ¨ç†èƒ½åŠ›ã€‚ç¬¬äºŒé˜¶æ®µå¼•å…¥äº†ä¸€ç§ç»“åˆç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(GRPO)çš„æ–°å‹å¥–åŠ±æœºåˆ¶ï¼Œæ—¨åœ¨å…±åŒä¼˜åŒ–è¯„åˆ†å‡†ç¡®æ€§å’Œæ¨ç†ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒQ-Ponderåœ¨è´¨é‡è¯„åˆ†å›å½’åŸºå‡†ä¸Šè¾¾åˆ°äº†SOTAæ€§èƒ½ï¼Œåœ¨è·¨é¢†åŸŸæ•°æ®é›†ä¸Šçš„SRCCæå‡äº†6.5%ã€‚æ­¤å¤–ï¼ŒQ-Ponderåœ¨æè¿°å‡†ç¡®æ€§å’Œåˆç†æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºå…¶æ•™å¸ˆæ¨¡å‹Qwen-2.5-VL-72Bï¼Œè¯æ˜äº†å…¶åœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­çš„æ³›åŒ–æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.05384v2",
      "published_date": "2025-06-03 10:11:51 UTC",
      "updated_date": "2025-06-12 16:38:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:10:55.683930+00:00"
    },
    {
      "arxiv_id": "2506.02713v1",
      "title": "Open-Set Living Need Prediction with Large Language Models",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¼€æ”¾é›†ç”Ÿæ´»éœ€æ±‚é¢„æµ‹",
      "authors": [
        "Xiaochong Lan",
        "Jie Feng",
        "Yizhou Sun",
        "Chen Gao",
        "Jiahuan Lei",
        "Xinlei Shi",
        "Hengliang Luo",
        "Yong Li"
      ],
      "abstract": "Living needs are the needs people generate in their daily lives for survival and well-being. On life service platforms like Meituan, user purchases are driven by living needs, making accurate living need predictions crucial for personalized service recommendations. Traditional approaches treat this prediction as a closed-set classification problem, severely limiting their ability to capture the diversity and complexity of living needs. In this work, we redefine living need prediction as an open-set classification problem and propose PIGEON, a novel system leveraging large language models (LLMs) for unrestricted need prediction. PIGEON first employs a behavior-aware record retriever to help LLMs understand user preferences, then incorporates Maslow's hierarchy of needs to align predictions with human living needs. For evaluation and application, we design a recall module based on a fine-tuned text embedding model that links flexible need descriptions to appropriate life services. Extensive experiments on real-world datasets demonstrate that PIGEON significantly outperforms closed-set approaches on need-based life service recall by an average of 19.37%. Human evaluation validates the reasonableness and specificity of our predictions. Additionally, we employ instruction tuning to enable smaller LLMs to achieve competitive performance, supporting practical deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæ´»æœåŠ¡å¹³å°ï¼ˆå¦‚ç¾å›¢ï¼‰ä¸­ç”¨æˆ·éœ€æ±‚é¢„æµ‹çš„å¤æ‚æ€§ï¼ŒæŒ‡å‡ºä¼ ç»Ÿæ–¹æ³•å°†å…¶è§†ä¸ºå°é—­é›†åˆ†ç±»(closed-set classification)é—®é¢˜çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† PIGEON ç³»ç»Ÿï¼Œå°†ç”Ÿæ´»éœ€æ±‚é¢„æµ‹é‡æ–°å®šä¹‰ä¸ºå¼€æ”¾é›†(open-set)åˆ†ç±»é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)è¿›è¡Œæ— é™åˆ¶çš„éœ€æ±‚é¢„æµ‹ã€‚PIGEON é€šè¿‡è¡Œä¸ºæ„ŸçŸ¥è®°å½•æ£€ç´¢å™¨(behavior-aware record retriever)è¯†åˆ«ç”¨æˆ·åå¥½ï¼Œå¹¶ç»“åˆé©¬æ–¯æ´›éœ€æ±‚å±‚æ¬¡ç†è®º(Maslow's hierarchy of needs)ç¡®ä¿é¢„æµ‹ç¬¦åˆäººç±»ç”Ÿæ´»é€»è¾‘ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿè®¾è®¡äº†åŸºäºå¾®è°ƒæ–‡æœ¬åµŒå…¥æ¨¡å‹(fine-tuned text embedding model)çš„å¬å›æ¨¡å—ï¼Œå°†æ¨¡ç³Šçš„éœ€æ±‚æè¿°ä¸å…·ä½“æœåŠ¡ç²¾å‡†é“¾æ¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPIGEON åœ¨æœåŠ¡å¬å›ç‡ä¸Šæ¯”ä¼ ç»Ÿå°é—­é›†æ–¹æ³•å¹³å‡æå‡äº† 19.37%ï¼Œä¸”äººå·¥è¯„ä¼°è¯å®äº†é¢„æµ‹çš„åˆç†æ€§ã€‚æœ€åï¼Œç ”ç©¶é€šè¿‡æŒ‡ä»¤å¾®è°ƒ(instruction tuning)ä¼˜åŒ–äº†å°æ¨¡å‹æ€§èƒ½ï¼Œä¸ºç³»ç»Ÿçš„å®é™…è½åœ°éƒ¨ç½²æä¾›äº†æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "ACL 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2506.02713v1",
      "published_date": "2025-06-03 10:10:19 UTC",
      "updated_date": "2025-06-03 10:10:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:11:05.633464+00:00"
    },
    {
      "arxiv_id": "2506.06359v1",
      "title": "From Transformers to Large Language Models: A systematic review of AI applications in the energy sector towards Agentic Digital Twins",
      "title_zh": "ä» Transformer åˆ°å¤§è¯­è¨€æ¨¡å‹ï¼šé¢å‘æ™ºèƒ½ä½“åŒ–æ•°å­—å­ªç”Ÿçš„èƒ½æºé¢†åŸŸäººå·¥æ™ºèƒ½åº”ç”¨ç³»ç»Ÿç»¼è¿°",
      "authors": [
        "Gabriel Antonesi",
        "Tudor Cioara",
        "Ionut Anghel",
        "Vasilis Michalakopoulos",
        "Elissaios Sarmas",
        "Liana Toderean"
      ],
      "abstract": "Artificial intelligence (AI) has long promised to improve energy management in smart grids by enhancing situational awareness and supporting more effective decision-making. While traditional machine learning has demonstrated notable results in forecasting and optimization, it often struggles with generalization, situational awareness, and heterogeneous data integration. Recent advances in foundation models such as Transformer architecture and Large Language Models (LLMs) have demonstrated improved capabilities in modelling complex temporal and contextual relationships, as well as in multi-modal data fusion which is essential for most AI applications in the energy sector. In this review we synthesize the rapid expanding field of AI applications in the energy domain focusing on Transformers and LLMs. We examine the architectural foundations, domain-specific adaptations and practical implementations of transformer models across various forecasting and grid management tasks. We then explore the emerging role of LLMs in the field: adaptation and fine tuning for the energy sector, the type of tasks they are suited for, and the new challenges they introduce. Along the way, we highlight practical implementations, innovations, and areas where the research frontier is rapidly expanding. These recent developments reviewed underscore a broader trend: Generative AI (GenAI) is beginning to augment decision-making not only in high-level planning but also in day-to-day operations, from forecasting and grid balancing to workforce training and asset onboarding. Building on these developments, we introduce the concept of the Agentic Digital Twin, a next-generation model that integrates LLMs to bring autonomy, proactivity, and social interaction into digital twin-based energy management systems.",
      "tldr_zh": "è¯¥ç»¼è¿°ç³»ç»Ÿæ¢è®¨äº†äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨èƒ½æºé¢†åŸŸçš„åº”ç”¨æ¼”è¿›ï¼ŒæŒ‡å‡ºä¼ ç»Ÿæœºå™¨å­¦ä¹ åœ¨æ³›åŒ–èƒ½åŠ›å’Œå¤šæºæ•°æ®é›†æˆæ–¹é¢çš„å±€é™ã€‚æ–‡ç« é‡ç‚¹åˆ†æäº† Transformer æ¶æ„å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤æ‚æ—¶é—´ã€ä¸Šä¸‹æ–‡å…³ç³»åŠå¤šæ¨¡æ€æ•°æ®èåˆä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹¶å›é¡¾äº†å…¶åœ¨ç”µåŠ›é¢„æµ‹ã€ç”µç½‘ç®¡ç†åŠç”Ÿæˆå¼ AIï¼ˆGenAIï¼‰è¾…åŠ©å†³ç­–ç­‰æ–¹é¢çš„å®é™…åº”ç”¨ã€‚åŸºäºè¿™äº›è¿›å±•ï¼Œç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº† Agentic Digital Twinï¼ˆæ™ºèƒ½ä½“æ•°å­—å­ªç”Ÿï¼‰è¿™ä¸€å‰ç»æ€§æ¦‚å¿µï¼Œæ—¨åœ¨é€šè¿‡é›†æˆ LLMs ä¸ºèƒ½æºæ•°å­—å­ªç”Ÿç³»ç»Ÿå¼•å…¥è‡ªä¸»æ€§ã€å‰ç»æ€§å’Œç¤¾äº¤äº’åŠ¨èƒ½åŠ›ã€‚è¿™ä¸€èŒƒå¼çš„è½¬å˜è¡¨æ˜ï¼ŒAI æ­£åœ¨ä»å•çº¯çš„é¢„æµ‹å·¥å…·å‘èƒ½å¤Ÿå‚ä¸æ—¥å¸¸è¿ç»´å’Œèµ„äº§ç®¡ç†çš„è‡ªä¸»æ™ºèƒ½ä½“æ¼”è¿›ï¼Œä¸ºæœªæ¥æ™ºæ…§ç”µç½‘çš„æ™ºèƒ½åŒ–ç®¡ç†æä¾›äº†ç³»ç»Ÿæ€§çš„ç†è®ºæ¡†æ¶ä¸å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06359v1",
      "published_date": "2025-06-03 10:02:07 UTC",
      "updated_date": "2025-06-03 10:02:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:11:06.309680+00:00"
    },
    {
      "arxiv_id": "2506.02703v3",
      "title": "Data Leakage and Deceptive Performance: A Critical Examination of Credit Card Fraud Detection Methodologies",
      "title_zh": "æ•°æ®æ³„éœ²ä¸è™šå‡æ€§èƒ½ï¼šä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹æ–¹æ³•è®ºçš„æ‰¹åˆ¤æ€§å®¡è§†",
      "authors": [
        "Khizar Hayat",
        "Baptiste Magnier"
      ],
      "abstract": "This study critically examines the methodological rigor in credit card fraud detection research, revealing how fundamental evaluation flaws can overshadow algorithmic sophistication. Through deliberate experimentation with improper evaluation protocols, we demonstrate that even simple models can achieve deceptively impressive results when basic methodological principles are violated. Our analysis identifies four critical issues plaguing current approaches: (1) pervasive data leakage from improper preprocessing sequences, (2) intentional vagueness in methodological reporting, (3) inadequate temporal validation for transaction data, and (4) metric manipulation through recall optimization at precision's expense. We present a case study showing how a minimal neural network architecture with data leakage outperforms many sophisticated methods reported in literature, achieving 99.9\\% recall despite fundamental evaluation flaws. These findings underscore that proper evaluation methodology matters more than model complexity in fraud detection research. The study serves as a cautionary example of how methodological rigor must precede architectural sophistication, with implications for improving research practices across machine learning applications.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹(Credit Card Fraud Detection)ä¸­çš„æ–¹æ³•è®ºä¸¥è°¨æ€§è¿›è¡Œäº†æ‰¹åˆ¤æ€§å®¡æŸ¥ï¼Œæ­ç¤ºäº†è¯„ä¼°ç¼ºé™·å¦‚ä½•æ©ç›–ç®—æ³•çš„å¤æ‚æ€§ã€‚é€šè¿‡å¯¹ä¸å½“è¯„ä¼°åè®®çš„å®éªŒï¼Œä½œè€…å±•ç¤ºäº†åœ¨è¿ååŸºæœ¬æ–¹æ³•è®ºåŸåˆ™æ—¶ï¼Œç®€å•æ¨¡å‹ä¹Ÿèƒ½è·å¾—å…·æœ‰æ¬ºéª—æ€§çš„é«˜æ€§èƒ½ç»“æœã€‚ç ”ç©¶è¯†åˆ«äº†å½“å‰é¢†åŸŸçš„å››å¤§å…³é”®é—®é¢˜ï¼šé¢„å¤„ç†ä¸å½“å¯¼è‡´çš„æ™®éæ•°æ®æ³„éœ²(Data Leakage)ã€æ–¹æ³•æŠ¥å‘Šçš„æ¨¡ç³Šæ€§ã€äº¤æ˜“æ•°æ®çš„æ—¶é—´éªŒè¯(Temporal Validation)ä¸è¶³ï¼Œä»¥åŠé€šè¿‡ç‰ºç‰²ç²¾ç¡®ç‡(Precision)æ¥ä¼˜åŒ–å¬å›ç‡(Recall)çš„æŒ‡æ ‡æ“çºµã€‚æ¡ˆä¾‹ç ”ç©¶æ˜¾ç¤ºï¼Œä¸€ä¸ªå­˜åœ¨æ•°æ®æ³„éœ²çš„æç®€ç¥ç»ç½‘ç»œæ¶æ„è¾¾åˆ°äº†99.9%çš„å¬å›ç‡ï¼Œç”šè‡³ä¼˜äºæ–‡çŒ®ä¸­è®¸å¤šå¤æ‚çš„å…ˆè¿›æ–¹æ³•ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åœ¨æ¬ºè¯ˆæ£€æµ‹ç ”ç©¶ä¸­ï¼Œæ–¹æ³•è®ºçš„ä¸¥è°¨æ€§æ¯”æ¨¡å‹å¤æ‚åº¦æ›´ä¸ºé‡è¦ï¼Œä¸ºæå‡æœºå™¨å­¦ä¹ åº”ç”¨çš„ç ”ç©¶è´¨é‡æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02703v3",
      "published_date": "2025-06-03 09:56:43 UTC",
      "updated_date": "2025-11-10 10:30:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:12:04.750537+00:00"
    },
    {
      "arxiv_id": "2506.06358v1",
      "title": "Towards real-time assessment of infrasound event detection capability using deep learning-based transmission loss estimation",
      "title_zh": "è¿ˆå‘åŸºäºæ·±åº¦å­¦ä¹ ä¼ æ’­æŸå¤±ä¼°è®¡çš„æ¬¡å£°äº‹ä»¶æ£€æµ‹èƒ½åŠ›å®æ—¶è¯„ä¼°",
      "authors": [
        "Alice Janela Cameijo",
        "Alexis Le Pichon",
        "Youcef Sklab",
        "Souhila Arib",
        "Quentin Brissaud",
        "Sven peter Naesholm",
        "Constantino Listowski",
        "Samir Aknine"
      ],
      "abstract": "Accurate modeling of infrasound transmission loss is essential for evaluating the performance of the International Monitoring System, enabling the effective design and maintenance of infrasound stations to support compliance of the Comprehensive Nuclear-Test-Ban Treaty. State-of-the-art propagation modeling tools enable transmission loss to be finely simulated using atmospheric models. However, the computational cost prohibits the exploration of a large parameter space in operational monitoring applications. To address this, recent studies made use of a deep learning algorithm capable of making transmission loss predictions almost instantaneously. However, the use of nudged atmospheric models leads to an incomplete representation of the medium, and the absence of temperature as an input makes the algorithm incompatible with long range propagation. In this study, we address these limitations by using both wind and temperature fields as inputs to a neural network, simulated up to 130 km altitude and 4,000 km distance. We also optimize several aspects of the neural network architecture. We exploit convolutional and recurrent layers to capture spatially and range-dependent features embedded in realistic atmospheric models, improving the overall performance. The neural network reaches an average error of 4 dB compared to full parabolic equation simulations and provides epistemic and data-related uncertainty estimates. Its evaluation on the 2022 Hunga Tonga-Hunga Ha'apai volcanic eruption demonstrates its prediction capability using atmospheric conditions and frequencies not included in the training. This represents a significant step towards near real-time assessment of International Monitoring System detection thresholds of explosive sources.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹æ¬¡å£°æ³¢ä¼ è¾“æŸå¤±(Transmission Loss)æ¨¡æ‹Ÿè®¡ç®—æˆæœ¬é«˜ã€éš¾ä»¥æ»¡è¶³å®æ—¶ç›‘æµ‹éœ€æ±‚çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„è¯„ä¼°æ–¹æ³•ï¼Œæ—¨åœ¨æ”¯æŒå›½é™…ç›‘æµ‹ç³»ç»Ÿ(International Monitoring System)çš„æœ‰æ•ˆè¿è¡Œã€‚ç ”ç©¶æ”¹è¿›äº†ç°æœ‰ç®—æ³•çš„å±€é™æ€§ï¼Œé€šè¿‡å°†é£åœºå’Œæ¸©åº¦åœºå…±åŒä½œä¸ºç¥ç»ç½‘ç»œè¾“å…¥ï¼Œå®ç°äº†å¯¹é«˜è¾¾130å…¬é‡Œé«˜åº¦å’Œ4000å…¬é‡Œè·ç¦»çš„ä¼ æ’­æ¨¡æ‹Ÿã€‚æ¨¡å‹åˆ©ç”¨å·ç§¯å±‚(Convolutional layers)å’Œå¾ªç¯å±‚(Recurrent layers)æœ‰æ•ˆæ•æ‰å¤§æ°”æ¨¡å‹ä¸­çš„ç©ºé—´å’Œè·ç¦»ä¾èµ–ç‰¹å¾ï¼Œç›¸æ¯”å…¨æŠ›ç‰©æ–¹ç¨‹(Full parabolic equation)æ¨¡æ‹Ÿï¼Œå…¶å¹³å‡è¯¯å·®ä»…ä¸º4 dBï¼Œå¹¶èƒ½æä¾›ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚åœ¨2022å¹´æ±¤åŠ ç«å±±å–·å‘æ¡ˆä¾‹ä¸­çš„æˆåŠŸè¯„ä¼°ï¼Œè¯æ˜äº†è¯¥æ¨¡å‹åœ¨å¤„ç†æœªè§å¤§æ°”æ¡ä»¶å’Œé¢‘ç‡æ—¶çš„å¼ºå¤§é¢„æµ‹èƒ½åŠ›ã€‚è¯¥æˆæœä¸ºå®ç°çˆ†ç‚¸æºæ¢æµ‹é˜ˆå€¼çš„è¿‘å®æ—¶è¯„ä¼°è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ï¼Œå¯¹ç»´æŒå…¨é¢ç¦æ­¢æ ¸è¯•éªŒæ¡çº¦(CTBT)å…·æœ‰é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "49 pages, 22 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.06358v1",
      "published_date": "2025-06-03 09:49:12 UTC",
      "updated_date": "2025-06-03 09:49:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:12:00.933482+00:00"
    },
    {
      "arxiv_id": "2506.02696v1",
      "title": "Shaking to Reveal: Perturbation-Based Detection of LLM Hallucinations",
      "title_zh": "éœ‡è¡è§çœŸï¼šåŸºäºæ‰°åŠ¨çš„å¤§è¯­è¨€æ¨¡å‹å¹»è§‰æ£€æµ‹",
      "authors": [
        "Jinyuan Luo",
        "Zhen Fang",
        "Yixuan Li",
        "Seongheon Park",
        "Ling Chen"
      ],
      "abstract": "Hallucination remains a key obstacle to the reliable deployment of large language models (LLMs) in real-world question answering tasks. A widely adopted strategy to detect hallucination, known as self-assessment, relies on the model's own output confidence to estimate the factual accuracy of its answers. However, this strategy assumes that the model's output distribution closely reflects the true data distribution, which may not always hold in practice. As bias accumulates through the model's layers, the final output can diverge from the underlying reasoning process, making output-level confidence an unreliable signal for hallucination detection. In this work, we propose Sample-Specific Prompting (SSP), a new framework that improves self-assessment by analyzing perturbation sensitivity at intermediate representations. These representations, being less influenced by model bias, offer a more faithful view of the model's latent reasoning process. Specifically, SSP dynamically generates noise prompts for each input and employs a lightweight encoder to amplify the changes in representations caused by the perturbation. A contrastive distance metric is then used to quantify these differences and separate truthful from hallucinated responses. By leveraging the dynamic behavior of intermediate representations under perturbation, SSP enables more reliable self-assessment. Extensive experiments demonstrate that SSP significantly outperforms prior methods across a range of hallucination detection benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨é—®ç­”ä»»åŠ¡ä¸­çš„å¹»è§‰(Hallucination)æ£€æµ‹é—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„è‡ªè¯„ä¼°(self-assessment)æ–¹æ³•ç”±äºæ¨¡å‹å±‚çº§åç½®çš„ç§¯ç´¯ï¼Œå¯¼è‡´è¾“å‡ºç½®ä¿¡åº¦å¾€å¾€æ— æ³•å‡†ç¡®åæ˜ çœŸå®çš„æ¨ç†è¿‡ç¨‹ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Sample-Specific Prompting (SSP)æ¡†æ¶ï¼Œé€šè¿‡åˆ†æä¸­é—´å±‚è¡¨ç¤º(intermediate representations)åœ¨å—åˆ°æ‰°åŠ¨æ—¶çš„æ•æ„Ÿæ€§æ¥è¯†åˆ«å¹»è§‰ã€‚SSPåˆ©ç”¨åŠ¨æ€ç”Ÿæˆçš„å™ªå£°æç¤ºå’Œè½»é‡çº§ç¼–ç å™¨æ¥æ”¾å¤§è¡¨ç¤ºå±‚é¢çš„å˜åŒ–ï¼Œå¹¶é‡‡ç”¨å¯¹æ¯”è·ç¦»åº¦é‡(contrastive distance metric)æ¥é‡åŒ–å·®å¼‚ï¼Œä»è€Œæœ‰æ•ˆåŒºåˆ†çœŸå®å›ç­”ä¸å¹»è§‰å“åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSSPåœ¨å¤šä¸ªå¹»è§‰æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºå¤§è¯­è¨€æ¨¡å‹æä¾›äº†æ›´å…·é²æ£’æ€§å’Œå¯é æ€§çš„è‡ªè¯„ä¼°æ‰‹æ®µã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02696v1",
      "published_date": "2025-06-03 09:44:28 UTC",
      "updated_date": "2025-06-03 09:44:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:12:17.896836+00:00"
    },
    {
      "arxiv_id": "2506.02694v1",
      "title": "XicorAttention: Time Series Transformer Using Attention with Nonlinear Correlation",
      "title_zh": "XicorAttentionï¼šé‡‡ç”¨éçº¿æ€§ç›¸å…³æ³¨æ„åŠ›æœºåˆ¶çš„æ—¶é—´åºåˆ— Transformer",
      "authors": [
        "Daichi Kimura",
        "Tomonori Izumitani",
        "Hisashi Kashima"
      ],
      "abstract": "Various Transformer-based models have been proposed for time series forecasting. These models leverage the self-attention mechanism to capture long-term temporal or variate dependencies in sequences. Existing methods can be divided into two approaches: (1) reducing computational cost of attention by making the calculations sparse, and (2) reshaping the input data to aggregate temporal features. However, existing attention mechanisms may not adequately capture inherent nonlinear dependencies present in time series data, leaving room for improvement. In this study, we propose a novel attention mechanism based on Chatterjee's rank correlation coefficient, which measures nonlinear dependencies between variables. Specifically, we replace the matrix multiplication in standard attention mechanisms with this rank coefficient to measure the query-key relationship. Since computing Chatterjee's correlation coefficient involves sorting and ranking operations, we introduce a differentiable approximation employing SoftSort and SoftRank. Our proposed mechanism, ``XicorAttention,'' integrates it into several state-of-the-art Transformer models. Experimental results on real-world datasets demonstrate that incorporating nonlinear correlation into the attention improves forecasting accuracy by up to approximately 9.1\\% compared to existing models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰ Transformer æ¨¡å‹åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­éš¾ä»¥å……åˆ†æ•æ‰éçº¿æ€§ä¾èµ– (nonlinear dependencies) çš„å±€é™æ€§ï¼Œæå‡ºäº†åä¸º XicorAttention çš„æ–°å‹æ³¨æ„åŠ›æœºåˆ¶ã€‚è¯¥æœºåˆ¶çš„æ ¸å¿ƒåœ¨äºé‡‡ç”¨ Chatterjee's rank correlation coefficient æ¥è¡¡é‡å˜é‡é—´çš„éçº¿æ€§å…³ç³»ï¼Œå¹¶ä»¥æ­¤æ›¿ä»£æ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„çŸ©é˜µä¹˜æ³•æ¥è®¡ç®—æŸ¥è¯¢-é”® (query-key) å…³ç³»ã€‚ç”±äºè¯¥ç›¸å…³ç³»æ•°æ¶‰åŠæ’åºå’Œæ’åæ“ä½œï¼Œç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº† SoftSort å’Œ SoftRank æŠ€æœ¯è¿›è¡Œå¯å¾®è¿‘ä¼¼ï¼Œä»è€Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆè®­ç»ƒã€‚ä½œè€…å°†è¯¥æœºåˆ¶é›†æˆåˆ°å¤šç§æœ€å…ˆè¿›çš„ (state-of-the-art) Transformer æ¨¡å‹ä¸­ï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¼•å…¥éçº¿æ€§ç›¸å…³æ€§å¯å°†é¢„æµ‹å‡†ç¡®ç‡è¾ƒç°æœ‰æ¨¡å‹æå‡é«˜è¾¾çº¦ 9.1%ã€‚è¯¥ç ”ç©¶ä¸ºæ•æ‰æ—¶é—´åºåˆ—çš„å†…åœ¨éçº¿æ€§ç‰¹å¾å¹¶æå‡é¢„æµ‹æ€§èƒ½æä¾›äº†æœ‰æ•ˆçš„æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02694v1",
      "published_date": "2025-06-03 09:43:45 UTC",
      "updated_date": "2025-06-03 09:43:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:12:11.837747+00:00"
    },
    {
      "arxiv_id": "2506.02677v1",
      "title": "Self-Disentanglement and Re-Composition for Cross-Domain Few-Shot Segmentation",
      "title_zh": "é¢å‘è·¨åŸŸå°‘æ ·æœ¬åˆ†å‰²çš„è‡ªè§£çº ç¼ ä¸é‡ç»„",
      "authors": [
        "Jintao Tong",
        "Yixiong Zou",
        "Guangyao Chen",
        "Yuhua Li",
        "Ruixuan Li"
      ],
      "abstract": "Cross-Domain Few-Shot Segmentation (CD-FSS) aims to transfer knowledge from a source-domain dataset to unseen target-domain datasets with limited annotations. Current methods typically compare the distance between training and testing samples for mask prediction. However, we find an entanglement problem exists in this widely adopted method, which tends to bind sourcedomain patterns together and make each of them hard to transfer. In this paper, we aim to address this problem for the CD-FSS task. We first find a natural decomposition of the ViT structure, based on which we delve into the entanglement problem for an interpretation. We find the decomposed ViT components are crossly compared between images in distance calculation, where the rational comparisons are entangled with those meaningless ones by their equal importance, leading to the entanglement problem. Based on this interpretation, we further propose to address the entanglement problem by learning to weigh for all comparisons of ViT components, which learn disentangled features and re-compose them for the CD-FSS task, benefiting both the generalization and finetuning. Experiments show that our model outperforms the state-of-the-art CD-FSS method by 1.92% and 1.88% in average accuracy under 1-shot and 5-shot settings, respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è·¨åŸŸå°æ ·æœ¬åˆ†å‰² (Cross-Domain Few-Shot Segmentation, CD-FSS) ä»»åŠ¡ä¸­æ™®éå­˜åœ¨çš„çº ç¼ é—®é¢˜ (Entanglement problem) è¿›è¡Œäº†æ·±å…¥æ¢è®¨ï¼ŒæŒ‡å‡ºè¿™ç§é—®é¢˜é™åˆ¶äº†æºåŸŸæ¨¡å¼å‘ç›®æ ‡åŸŸçš„æœ‰æ•ˆè¿ç§»ã€‚ä½œè€…é€šè¿‡å¯¹ Vision Transformer (ViT) ç»“æ„çš„è‡ªç„¶åˆ†è§£ï¼Œå‘ç°å›¾åƒé—´çš„è·ç¦»è®¡ç®—ä¼šå°†åˆç†çš„æ¯”è¾ƒä¸æ— æ„ä¹‰çš„æ¯”è¾ƒæ··æ·†ï¼Œå¯¼è‡´ç‰¹å¾éš¾ä»¥è§£è€¦ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§å­¦ä¹ ä¸º ViT ç»„ä»¶æ¯”è¾ƒåˆ†é…æƒé‡çš„æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°ç‰¹å¾çš„è‡ªè§£è€¦ (Self-Disentanglement) ä¸é‡æ„ (Re-Composition)ã€‚è¯¥æ–¹æ¡ˆåœ¨å¢å¼ºæ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„åŒæ—¶ä¹Ÿä¼˜åŒ–äº†å¾®è°ƒè¿‡ç¨‹ï¼Œæœ‰æ•ˆè§£å†³äº†ç‰¹å¾çº ç¼ å¸¦æ¥çš„è¿ç§»éšœç¢ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨ 1-shot å’Œ 5-shot è®¾ç½®ä¸‹çš„å¹³å‡å‡†ç¡®ç‡åˆ†åˆ«ä¼˜äºç°æœ‰æœ€å…ˆè¿› CD-FSS æ–¹æ³• 1.92% å’Œ 1.88%ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.02677v1",
      "published_date": "2025-06-03 09:23:20 UTC",
      "updated_date": "2025-06-03 09:23:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:12:12.584325+00:00"
    },
    {
      "arxiv_id": "2506.02672v3",
      "title": "EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving",
      "title_zh": "EvaLearnï¼šåŸºäºåºåˆ—åŒ–é—®é¢˜æ±‚è§£çš„å¤§è¯­è¨€æ¨¡å‹å­¦ä¹ èƒ½åŠ›ä¸æ•ˆç‡é‡åŒ–è¯„ä¼°",
      "authors": [
        "Shihan Dou",
        "Ming Zhang",
        "Chenhao Huang",
        "Jiayi Chen",
        "Feng Chen",
        "Shichun Liu",
        "Yan Liu",
        "Chenxiao Liu",
        "Cheng Zhong",
        "Zongzhang Zhang",
        "Tao Gui",
        "Chao Xin",
        "Chengzhi Wei",
        "Lin Yan",
        "Yonghui Wu",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "abstract": "We introduce EvaLearn, a pioneering benchmark designed to evaluate large language models (LLMs) on their learning capability and efficiency in challenging tasks, a critical, yet underexplored aspect of model potential. EvaLearn contains 648 challenging problems across six task types, grouped into 182 sequences, each sequence dedicated to one task type. Diverging from most existing benchmarks that evaluate models in parallel, EvaLearn requires models to solve problems sequentially, allowing them to leverage the experience gained from previous solutions. EvaLearn provides five comprehensive automated metrics to evaluate models and quantify their learning capability and efficiency. We extensively benchmark nine frontier models and observe varied performance profiles: some models, such as Claude-3.7-sonnet, start with moderate initial performance but exhibit strong learning ability, while some models struggle to benefit from experience and may even show negative transfer. Moreover, we investigate model performance under two learning settings and find that instance-level rubrics and teacher-model feedback further facilitate model learning. Importantly, we observe that current LLMs with stronger static abilities do not show a clear advantage in learning capability across all tasks, highlighting that EvaLearn evaluates a new dimension of model performance. We hope EvaLearn provides a novel evaluation perspective for assessing LLM potential and understanding the gap between models and human capabilities, promoting the development of deeper and more dynamic evaluation approaches. All datasets, the automatic evaluation framework, and the results studied in this paper are available at the GitHub repository.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† EvaLearnï¼Œè¿™æ˜¯ä¸€ç§å¼€åˆ›æ€§çš„åŸºå‡†æµ‹è¯•(benchmark)ï¼Œæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸­çš„å­¦ä¹ èƒ½åŠ›(learning capability)å’Œæ•ˆç‡(efficiency)ã€‚ä¸ä¼ ç»Ÿçš„å¹¶è¡Œè¯„ä¼°ä¸åŒï¼ŒEvaLearn é‡‡ç”¨é¡ºåºæ±‚è§£(sequential problem solving)æ¨¡å¼ï¼Œå…è®¸æ¨¡å‹åˆ©ç”¨å…ˆå‰ä»»åŠ¡ä¸­ç§¯ç´¯çš„ç»éªŒã€‚è¯¥æ¡†æ¶åŒ…å« 648 ä¸ªéš¾é¢˜å¹¶å¼•å…¥äº†äº”é¡¹è‡ªåŠ¨åŒ–æŒ‡æ ‡(automated metrics)ä»¥é‡åŒ–æ¨¡å‹è¡¨ç°ï¼Œç ”ç©¶å›¢é˜Ÿæ®æ­¤å¯¹ä¹ç§å‰æ²¿æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒClaude-3.7-sonnet ç­‰æ¨¡å‹å±•ç°å‡ºæ˜¾è‘—çš„å­¦ä¹ è¿›æ­¥ï¼Œè€Œéƒ¨åˆ†æ¨¡å‹åˆ™è¡¨ç°å‡ºè·ç›Šå—é™ç”šè‡³è´Ÿå‘è¿ç§»(negative transfer)ã€‚å…³é”®å‘ç°æŒ‡å‡ºï¼Œå½“å‰åœ¨é™æ€èƒ½åŠ›(static abilities)åŸºå‡†ä¸Šè¡¨ç°å¼ºåŠ²çš„æ¨¡å‹ï¼Œåœ¨å­¦ä¹ èƒ½åŠ›è¿™ä¸€æ–°ç»´åº¦ä¸Šå¹¶ä¸å¿…ç„¶å…·æœ‰ä¼˜åŠ¿ï¼Œè¿™ä¸ºç†è§£æ¨¡å‹æ½œåŠ›ä¸äººç±»èƒ½åŠ›å·®è·æä¾›äº†å…¨æ–°è§†è§’ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by NeurIPS 2025. 47 pages, 24 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.02672v3",
      "published_date": "2025-06-03 09:18:33 UTC",
      "updated_date": "2025-10-21 13:21:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:12:21.683807+00:00"
    },
    {
      "arxiv_id": "2506.02668v1",
      "title": "FAuNO: Semi-Asynchronous Federated Reinforcement Learning Framework for Task Offloading in Edge Systems",
      "title_zh": "FAuNOï¼šé¢å‘è¾¹ç¼˜ç³»ç»Ÿä»»åŠ¡å¸è½½çš„åŠå¼‚æ­¥è”é‚¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Frederico Metelo",
        "Alexandre Oliveira",
        "Stevo RackoviÄ‡",
        "Pedro Ãkos Costa",
        "ClÃ¡udia Soares"
      ],
      "abstract": "Edge computing addresses the growing data demands of connected-device networks by placing computational resources closer to end users through decentralized infrastructures. This decentralization challenges traditional, fully centralized orchestration, which suffers from latency and resource bottlenecks. We present \\textbf{FAuNO} -- \\emph{Federated Asynchronous Network Orchestrator} -- a buffered, asynchronous \\emph{federated reinforcement-learning} (FRL) framework for decentralized task offloading in edge systems. FAuNO adopts an actor-critic architecture in which local actors learn node-specific dynamics and peer interactions, while a federated critic aggregates experience across agents to encourage efficient cooperation and improve overall system performance. Experiments in the \\emph{PeersimGym} environment show that FAuNO consistently matches or exceeds heuristic and federated multi-agent RL baselines in reducing task loss and latency, underscoring its adaptability to dynamic edge-computing scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¾¹ç¼˜è®¡ç®—ä¸­å»ä¸­å¿ƒåŒ–æ¶æ„å¯¼è‡´çš„å»¶è¿Ÿå’Œèµ„æºç“¶é¢ˆé—®é¢˜ï¼Œæå‡ºäº† FAuNO (Federated Asynchronous Network Orchestrator)ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè¾¹ç¼˜ç³»ç»Ÿå»ä¸­å¿ƒåŒ–ä»»åŠ¡å¸è½½çš„ç¼“å†²å¼‚æ­¥è”é‚¦å¼ºåŒ–å­¦ä¹  (Federated Reinforcement Learning) æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº† Actor-Critic æ¶æ„ï¼Œå…¶ä¸­æœ¬åœ° Actor è´Ÿè´£å­¦ä¹ èŠ‚ç‚¹ç‰¹å®šçš„åŠ¨æ€ä¸å¯¹ç­‰äº¤äº’ï¼Œè€Œè”é‚¦ Critic åˆ™é€šè¿‡èšåˆä¸åŒæ™ºèƒ½ä½“çš„ç»éªŒæ¥ä¿ƒè¿›é«˜æ•ˆåä½œå¹¶ä¼˜åŒ–ç³»ç»Ÿæ•´ä½“æ€§èƒ½ã€‚åœ¨ PeersimGym ç¯å¢ƒä¸‹çš„å®éªŒè¡¨æ˜ï¼ŒFAuNO åœ¨é™ä½ä»»åŠ¡æŸå¤±å’Œå»¶è¿Ÿæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå…¶æ€§èƒ½è¾¾åˆ°æˆ–è¶…è¿‡äº†å¯å‘å¼ç®—æ³•å’Œä¼ ç»Ÿçš„è”é‚¦å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (FMARL) åŸºå‡†æ¨¡å‹ã€‚å®éªŒç»“æœå……åˆ†éªŒè¯äº† FAuNO åœ¨å¤„ç†åŠ¨æ€è¾¹ç¼˜è®¡ç®—åœºæ™¯æ—¶å…·å¤‡æå¼ºçš„é€‚åº”æ€§ï¼Œä¸ºè§£å†³å¤æ‚çš„ç½‘ç»œç¼–æ’æŒ‘æˆ˜æä¾›äº†æœ‰æ•ˆçš„æ–¹æ³•è®ºæ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02668v1",
      "published_date": "2025-06-03 09:15:03 UTC",
      "updated_date": "2025-06-03 09:15:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:12:20.536509+00:00"
    },
    {
      "arxiv_id": "2506.02654v1",
      "title": "A Pretrained Probabilistic Transformer for City-Scale Traffic Volume Prediction",
      "title_zh": "åŸå¸‚çº§äº¤é€šæµé‡é¢„æµ‹çš„é¢„è®­ç»ƒæ¦‚ç‡Transformer",
      "authors": [
        "Shiyu Shen",
        "Bin Pan",
        "Guirong Xue"
      ],
      "abstract": "City-scale traffic volume prediction plays a pivotal role in intelligent transportation systems, yet remains a challenge due to the inherent incompleteness and bias in observational data. Although deep learning-based methods have shown considerable promise, most existing approaches produce deterministic point estimates, thereby neglecting the uncertainty arising from unobserved traffic flows. Furthermore, current models are typically trained in a city-specific manner, which hinders their generalizability and limits scalability across diverse urban contexts. To overcome these limitations, we introduce TrafficPPT, a Pretrained Probabilistic Transformer designed to model traffic volume as a distributional aggregation of trajectories. Our framework fuses heterogeneous data sources-including real-time observations, historical trajectory data, and road network topology-enabling robust and uncertainty-aware traffic inference. TrafficPPT is initially pretrained on large-scale simulated data spanning multiple urban scenarios, and later fine-tuned on target cities to ensure effective domain adaptation. Experiments on real-world datasets show that TrafficPPT consistently surpasses state-of-the-art baselines, particularly under conditions of extreme data sparsity. Code will be open.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TrafficPPTï¼Œè¿™æ˜¯ä¸€ç§Pretrained Probabilistic Transformerï¼Œæ—¨åœ¨è§£å†³åŸå¸‚çº§äº¤é€šæµé‡é¢„æµ‹ä¸­è§‚æµ‹æ•°æ®ä¸å®Œæ•´ã€æœ‰åä»¥åŠç°æœ‰æ¨¡å‹æ³›åŒ–èƒ½åŠ›å¼±çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†äº¤é€šæµé‡å»ºæ¨¡ä¸ºè½¨è¿¹çš„åˆ†å¸ƒèšåˆ(distributional aggregation of trajectories)ï¼Œå…‹æœäº†ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ–¹æ³•ä»…æä¾›ç¡®å®šæ€§ç‚¹ä¼°è®¡(deterministic point estimates)è€Œå¿½ç•¥ä¸ç¡®å®šæ€§çš„å±€é™æ€§ã€‚æ¨¡å‹æœ‰æ•ˆèåˆäº†å®æ—¶è§‚æµ‹ã€å†å²è½¨è¿¹æ•°æ®å’Œè·¯ç½‘æ‹“æ‰‘(road network topology)ç­‰å¼‚æ„æ•°æ®æºï¼Œå¢å¼ºäº†äº¤é€šæ¨ç†çš„é²æ£’æ€§å’Œä¸ç¡®å®šæ€§æ„ŸçŸ¥èƒ½åŠ›ã€‚TrafficPPTé‡‡ç”¨å…ˆåœ¨å¤§è§„æ¨¡æ¨¡æ‹Ÿæ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒ(pretrained)ï¼Œéšååœ¨ç›®æ ‡åŸå¸‚è¿›è¡Œå¾®è°ƒ(fine-tuned)çš„ç­–ç•¥ï¼Œç¡®ä¿äº†è·¨åŸå¸‚åœºæ™¯çš„é¢†åŸŸé€‚åº”æ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®æåº¦ç¨€ç–çš„æ¡ä»¶ä¸‹å±•ç°å‡ºå“è¶Šçš„é¢„æµ‹æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02654v1",
      "published_date": "2025-06-03 09:07:29 UTC",
      "updated_date": "2025-06-03 09:07:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:12:28.686318+00:00"
    },
    {
      "arxiv_id": "2506.02649v1",
      "title": "From Prompts to Protection: Large Language Model-Enabled In-Context Learning for Smart Public Safety UAV",
      "title_zh": "ä»æç¤ºåˆ°ä¿éšœï¼šé¢å‘æ™ºèƒ½å…¬å…±å®‰å…¨æ— äººæœºçš„å¤§è¯­è¨€æ¨¡å‹ä¸Šä¸‹æ–‡å­¦ä¹ ",
      "authors": [
        "Yousef Emami",
        "Hao Zhou",
        "Miguel Gutierrez Gaitan",
        "Kai Li",
        "Luis Almeida",
        "Zhu Han"
      ],
      "abstract": "A public safety Unmanned Aerial Vehicle (UAV) enhances situational awareness in emergency response. Its agility and ability to optimize mobility and establish Line-of-Sight (LoS) communication make it increasingly vital for managing emergencies such as disaster response, search and rescue, and wildfire monitoring. While Deep Reinforcement Learning (DRL) has been applied to optimize UAV navigation and control, its high training complexity, low sample efficiency, and simulation-to-reality gap limit its practicality in public safety. Recent advances in Large Language Models (LLMs) offer a compelling alternative. With strong reasoning and generalization capabilities, LLMs can adapt to new tasks through In-Context Learning (ICL), which enables task adaptation via natural language prompts and example-based guidance, without retraining. Deploying LLMs at the network edge, rather than in the cloud, further reduces latency and preserves data privacy, thereby making them suitable for real-time, mission-critical public safety UAVs. This paper proposes the integration of LLM-enabled ICL with public safety UAV to address the key functions, such as path planning and velocity control, in the context of emergency response. We present a case study on data collection scheduling where the LLM-enabled ICL framework can significantly reduce packet loss compared to conventional approaches, while also mitigating potential jailbreaking vulnerabilities. Finally, we discuss LLM optimizers and specify future research directions. The ICL framework enables adaptive, context-aware decision-making for public safety UAV, thus offering a lightweight and efficient solution for enhancing UAV autonomy and responsiveness in emergencies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºå°†å…·å¤‡ä¸Šä¸‹æ–‡å­¦ä¹ (In-Context Learning, ICL)èƒ½åŠ›çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)é›†æˆåˆ°å…¬å…±å®‰å…¨æ— äººæœº(UAV)ä¸­ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ·±åº¦å¼ºåŒ–å­¦ä¹ (Deep Reinforcement Learning, DRL)åœ¨å¤æ‚åº”æ€¥åœºæ™¯ä¸­é¢ä¸´çš„è®­ç»ƒéš¾åº¦å¤§å’Œä»¿çœŸä¸ç°å®å·®è·æ˜æ˜¾ç­‰é—®é¢˜ã€‚é€šè¿‡åœ¨ç½‘ç»œè¾¹ç¼˜éƒ¨ç½²LLMsï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåˆ©ç”¨è‡ªç„¶è¯­è¨€æç¤ºå’Œç¤ºä¾‹å¼•å¯¼ï¼Œæ— éœ€é‡æ–°è®­ç»ƒå³å¯å®ç°è·¯å¾„è§„åˆ’(path planning)å’Œé€Ÿåº¦æ§åˆ¶(velocity control)ç­‰ä»»åŠ¡çš„å¿«é€Ÿè‡ªé€‚åº”ã€‚æ¡ˆä¾‹ç ”ç©¶æ˜¾ç¤ºï¼Œè¯¥ICLæ¡†æ¶åœ¨æ•°æ®æ”¶é›†è°ƒåº¦(data collection scheduling)ä»»åŠ¡ä¸­èƒ½æ¯”ä¼ ç»Ÿæ–¹æ³•æ˜¾è‘—é™ä½ä¸¢åŒ…ç‡ï¼Œå¹¶æœ‰æ•ˆåº”å¯¹æ½œåœ¨çš„è¶Šç‹±(jailbreaking)å®‰å…¨é£é™©ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åŸºäºLLMçš„ICLæ–¹æ¡ˆæ˜¯ä¸€ç§è½»é‡ä¸”é«˜æ•ˆçš„æ‰‹æ®µï¼Œèƒ½ä¸ºå…¬å…±å®‰å…¨UAVæä¾›æ›´å¼ºçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¸å†³ç­–èƒ½åŠ›ï¼Œä»è€Œæå‡å…¶åœ¨æœç´¢æ•‘æ´ã€ç¾å®³ç›‘æµ‹ç­‰ç´§æ€¥ä»»åŠ¡ä¸­çš„è‡ªä¸»æ€§ä¸å“åº”é€Ÿåº¦ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.02649v1",
      "published_date": "2025-06-03 09:01:33 UTC",
      "updated_date": "2025-06-03 09:01:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:12:35.994733+00:00"
    },
    {
      "arxiv_id": "2506.02648v2",
      "title": "Truly Assessing Fluid Intelligence of Large Language Models through Dynamic Reasoning Evaluation",
      "title_zh": "é€šè¿‡åŠ¨æ€æ¨ç†è¯„æµ‹çœŸå®è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹çš„æµä½“æ™ºåŠ›",
      "authors": [
        "Yue Yang",
        "MingKang Chen",
        "Qihua Liu",
        "Mengkang Hu",
        "Qiguang Chen",
        "Gengrui Zhang",
        "Shuyue Hu",
        "Guangtao Zhai",
        "Yu Qiao",
        "Yu Wang",
        "Wenqi Shao",
        "Ping Luo"
      ],
      "abstract": "Recent advances in large language models (LLMs) have demonstrated impressive reasoning capacities that mirror human-like thinking. However, whether LLMs possess genuine fluid intelligence (i.e., the ability to reason abstractly and generalize rules in novel situations) remains an open question. Existing reasoning benchmarks either focus on domain-specific knowledge (crystallized intelligence) or lack interpretability. To address these limitations, we propose DRE-Bench, a dynamic reasoning evaluation benchmark grounded in a hierarchical cognitive framework. DRE-Bench consists of 36 abstract reasoning tasks organized across four cognitive levels, with each task featuring multiple dynamic variants that test the same underlying latent rule. This design enables fine-grained, interpretable, and reliable assessments of fluid intelligence. We evaluate a range of state-of-the-art LLMs, including both general LLMs (GPT-4o, Claude 3.7) and reasoning LLMs (o1, DeepSeek-R1, QwQ, Skywork-OR1). Experimental results reveal that although most LLMs achieve competent and robust performance in low-level cognition, they struggle with high-level cognition and exhibit limited generalization as task complexity grows. Our findings highlight the gap between current LLMs and true human-like fluid intelligence and offer a new path for systematically tracking reasoning progress in LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)æ˜¯å¦å…·å¤‡çœŸæ­£çš„æµä½“æ™ºèƒ½(Fluid Intelligence)ï¼Œå³åœ¨é™Œç”Ÿæƒ…å¢ƒä¸­è¿›è¡ŒæŠ½è±¡æ¨ç†å’Œè§„åˆ™æ³›åŒ–çš„èƒ½åŠ›ã€‚é’ˆå¯¹ç°æœ‰è¯„ä¼°åŸºå‡†ä¾§é‡äºé¢†åŸŸçŸ¥è¯†(Crystallized Intelligence)ä¸”ç¼ºä¹å¯è§£é‡Šæ€§çš„å±€é™ï¼Œä½œè€…æå‡ºäº†åŸºäºåˆ†å±‚è®¤çŸ¥æ¡†æ¶çš„åŠ¨æ€æ¨ç†è¯„ä¼°åŸºå‡†DRE-Benchã€‚è¯¥åŸºå‡†åŒ…å«è·¨è¶Šå››ä¸ªè®¤çŸ¥å±‚çº§çš„36é¡¹æŠ½è±¡æ¨ç†ä»»åŠ¡ï¼Œé€šè¿‡é’ˆå¯¹åŒä¸€æ½œåœ¨è§„åˆ™çš„å¤šç§åŠ¨æ€å˜ä½“è®¾è®¡ï¼Œå®ç°äº†ç»†ç²’åº¦ä¸”å¯é çš„è¯„ä¼°ã€‚ç ”ç©¶å¯¹GPT-4oã€Claude 3.7ã€o1åŠDeepSeek-R1ç­‰å°–ç«¯æ¨¡å‹è¿›è¡Œäº†å®éªŒè¯„ä¼°ï¼Œå‘ç°å¤§å¤šæ•°LLMsåœ¨ä½å±‚çº§è®¤çŸ¥ä»»åŠ¡ä¸­è¡¨ç°ç¨³å¥ã€‚ç„¶è€Œï¼Œéšç€ä»»åŠ¡å¤æ‚åº¦çš„å¢åŠ ï¼Œè¿™äº›æ¨¡å‹åœ¨é«˜å±‚çº§è®¤çŸ¥ä¸­è¡¨ç°æŒ£æ‰ï¼Œä¸”æ³›åŒ–èƒ½åŠ›å—åˆ°æ˜æ˜¾é™åˆ¶ã€‚è¿™ä¸€å‘ç°å‡¸æ˜¾äº†å½“å‰LLMsä¸äººç±»æµä½“æ™ºèƒ½ä¹‹é—´çš„å·®è·ï¼Œå¹¶ä¸ºç³»ç»ŸåŒ–è¿½è¸ªLLMsæ¨ç†èƒ½åŠ›çš„è¿›å±•æä¾›äº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02648v2",
      "published_date": "2025-06-03 09:01:08 UTC",
      "updated_date": "2025-09-28 08:54:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:12:51.785851+00:00"
    },
    {
      "arxiv_id": "2506.05382v1",
      "title": "How stealthy is stealthy? Studying the Efficacy of Black-Box Adversarial Attacks in the Real World",
      "title_zh": "éšè”½æ€§ç©¶ç«Ÿå¦‚ä½•ï¼Ÿç°å®ä¸–ç•Œé»‘ç›’å¯¹æŠ—æ”»å‡»çš„æœ‰æ•ˆæ€§ç ”ç©¶",
      "authors": [
        "Francesco Panebianco",
        "Mario D'Onghia",
        "Stefano Zanero aand Michele Carminati"
      ],
      "abstract": "Deep learning systems, critical in domains like autonomous vehicles, are vulnerable to adversarial examples (crafted inputs designed to mislead classifiers). This study investigates black-box adversarial attacks in computer vision. This is a realistic scenario, where attackers have query-only access to the target model. Three properties are introduced to evaluate attack feasibility: robustness to compression, stealthiness to automatic detection, and stealthiness to human inspection. State-of-the-Art methods tend to prioritize one criterion at the expense of others. We propose ECLIPSE, a novel attack method employing Gaussian blurring on sampled gradients and a local surrogate model. Comprehensive experiments on a public dataset highlight ECLIPSE's advantages, demonstrating its contribution to the trade-off between the three properties.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç°å®åœºæ™¯ä¸‹è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„é»‘ç›’å¯¹æŠ—æ”»å‡»(Black-Box Adversarial Attacks)çš„æœ‰æ•ˆæ€§ã€‚ä½œè€…æå‡ºäº†è¯„ä¼°æ”»å‡»å¯è¡Œæ€§çš„ä¸‰ä¸ªå…³é”®æŒ‡æ ‡ï¼šæŠ—å‹ç¼©æ€§(robustness to compression)ã€è‡ªåŠ¨æ£€æµ‹éšè”½æ€§(stealthiness to automatic detection)ä»¥åŠäººå·¥æ£€æŸ¥éšè”½æ€§(stealthiness to human inspection)ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•éš¾ä»¥å…¼é¡¾è¿™äº›å±æ€§çš„ç°çŠ¶ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºECLIPSEçš„æ–°å‹æ”»å‡»æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨é‡‡æ ·æ¢¯åº¦ä¸Šåº”ç”¨é«˜æ–¯æ¨¡ç³Š(Gaussian blurring)å¹¶ç»“åˆå±€éƒ¨ä»£ç†æ¨¡å‹(local surrogate model)ï¼Œä»¥æå‡æ”»å‡»çš„ç»¼åˆæ€§èƒ½ã€‚åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒECLIPSEèƒ½æœ‰æ•ˆå¹³è¡¡ä¸Šè¿°ä¸‰ä¸ªå±æ€§ï¼Œç›¸æ¯”ç°æœ‰SOTAæ–¹æ³•åœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­å±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.05382v1",
      "published_date": "2025-06-03 08:56:37 UTC",
      "updated_date": "2025-06-03 08:56:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:13:09.915823+00:00"
    },
    {
      "arxiv_id": "2506.02634v4",
      "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache at a Large Cloud Provider",
      "title_zh": "çœŸå®åœºæ™¯ä¸‹çš„ KVCache ç¼“å­˜ï¼šå¤§å‹äº‘æœåŠ¡æä¾›å•† KVCache ç¼“å­˜çš„ç‰¹å¾åˆ†æä¸ä¼˜åŒ–",
      "authors": [
        "Jiahao Wang",
        "Jinbo Han",
        "Xingda Wei",
        "Sijie Shen",
        "Dingyan Zhang",
        "Chenguang Fang",
        "Rong Chen",
        "Wenyuan Yu",
        "Haibo Chen"
      ],
      "abstract": "Serving large language models (LLMs) is important for cloud providers, and caching intermediate results (KV\\$) after processing each request substantially improves serving throughput and latency. However, there is limited understanding of how LLM serving benefits from KV\\$ caching, where system design decisions like cache eviction policies are highly workload-dependent. In this paper, we present the first systematic characterization of the KV\\$ workload patterns from one of the leading LLM service providers. We draw observations that were not covered by previous studies focusing on synthetic workloads, including: KV\\$ reuses are skewed across requests, where reuses between single-turn requests are equally important as multi-turn requests; the reuse time and probability are diverse considering all requests, but for a specific request category, the pattern tends to be predictable; and the overall cache size required for an ideal cache hit ratio is moderate. Based on the characterization, we further propose a workload-aware cache eviction policy that improves the serving performance under real-world traces, especially with limited cache capacity.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹é¢†å…ˆäº‘æœåŠ¡æä¾›å•†çš„å¤§è¯­è¨€æ¨¡å‹(LLM)æœåŠ¡ä¸­é”®å€¼ç¼“å­˜(KVCache)çš„å·¥ä½œè´Ÿè½½æ¨¡å¼è¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿæ€§ç‰¹å¾åˆ†æã€‚ç ”ç©¶å‘ç°ï¼ŒKVCacheçš„é‡ç”¨åœ¨ä¸åŒè¯·æ±‚é—´å‘ˆç°åæ€åˆ†å¸ƒï¼Œå…¶ä¸­å•è½®è¯·æ±‚é—´çš„é‡ç”¨ä¸å¤šè½®è¯·æ±‚åŒæ ·é‡è¦ï¼Œä¸”ç‰¹å®šè¯·æ±‚ç±»åˆ«çš„é‡ç”¨æ¦‚ç‡å’Œæ—¶é—´å…·æœ‰å¯é¢„æµ‹æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶æŒ‡å‡ºå®ç°ç†æƒ³ç¼“å­˜å‘½ä¸­ç‡æ‰€éœ€çš„æ•´ä½“ç¼“å­˜è§„æ¨¡è¾ƒä¸ºé€‚ä¸­ã€‚åŸºäºä¸Šè¿°å‘ç°ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ„ŸçŸ¥å·¥ä½œè´Ÿè½½çš„ç¼“å­˜ç½®æ¢ç­–ç•¥(cache eviction policy)ï¼Œæ—¨åœ¨ä¼˜åŒ–å®é™…ç”Ÿäº§ç¯å¢ƒä¸‹çš„æœåŠ¡æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç­–ç•¥åœ¨çœŸå®è´Ÿè½½è½¨è¿¹ä¸‹æ˜¾è‘—æå‡äº†æ¨ç†æ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼“å­˜å®¹é‡æœ‰é™çš„æƒ…å†µä¸‹è¡¨ç°ä¼˜å¼‚ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "Accepted by USENIX ATC'25",
      "pdf_url": "https://arxiv.org/pdf/2506.02634v4",
      "published_date": "2025-06-03 08:51:38 UTC",
      "updated_date": "2025-07-23 08:07:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:13:16.547403+00:00"
    },
    {
      "arxiv_id": "2506.02623v1",
      "title": "SiamNAS: Siamese Surrogate Model for Dominance Relation Prediction in Multi-objective Neural Architecture Search",
      "title_zh": "SiamNASï¼šç”¨äºå¤šç›®æ ‡ç¥ç»æ¶æ„æœç´¢æ”¯é…å…³ç³»é¢„æµ‹çš„å­ªç”Ÿä»£ç†æ¨¡å‹",
      "authors": [
        "Yuyang Zhou",
        "Ferrante Neri",
        "Yew-Soon Ong",
        "Ruibin Bai"
      ],
      "abstract": "Modern neural architecture search (NAS) is inherently multi-objective, balancing trade-offs such as accuracy, parameter count, and computational cost. This complexity makes NAS computationally expensive and nearly impossible to solve without efficient approximations. To address this, we propose a novel surrogate modelling approach that leverages an ensemble of Siamese network blocks to predict dominance relationships between candidate architectures. Lightweight and easy to train, the surrogate achieves 92% accuracy and replaces the crowding distance calculation in the survivor selection strategy with a heuristic rule based on model size. Integrated into a framework termed SiamNAS, this design eliminates costly evaluations during the search process. Experiments on NAS-Bench-201 demonstrate the framework's ability to identify Pareto-optimal solutions with significantly reduced computational costs. The proposed SiamNAS identified a final non-dominated set containing the best architecture in NAS-Bench-201 for CIFAR-10 and the second-best for ImageNet, in terms of test error rate, within 0.01 GPU days. This proof-of-concept study highlights the potential of the proposed Siamese network surrogate model to generalise to multi-tasking optimisation, enabling simultaneous optimisation across tasks. Additionally, it offers opportunities to extend the approach for generating Sets of Pareto Sets (SOS), providing diverse Pareto-optimal solutions for heterogeneous task settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°ä»£ç¥ç»æ¶æ„æœç´¢ (Neural Architecture Search) ä¸­å¤šç›®æ ‡ä¼˜åŒ–å¯¼è‡´çš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜ï¼Œæå‡ºäº†åä¸º SiamNAS çš„åˆ›æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯åˆ©ç”¨é›†æˆå¼çš„ Siamese network ä»£ç†æ¨¡å‹æ¥é¢„æµ‹å€™é€‰æ¶æ„é—´çš„æ”¯é…å…³ç³» (dominance relationships)ï¼Œä»è€Œåœ¨æœç´¢è¿‡ç¨‹ä¸­æ¶ˆé™¤æ˜‚è´µçš„ç›´æ¥è¯„ä¼°ã€‚è¯¥ä»£ç†æ¨¡å‹å…·æœ‰è½»é‡åŒ–ä¸”æ˜“äºè®­ç»ƒçš„ç‰¹ç‚¹ï¼Œé¢„æµ‹å‡†ç¡®ç‡è¾¾åˆ° 92%ï¼Œå¹¶å¼•å…¥äº†åŸºäºæ¨¡å‹å¤§å°çš„å¯å‘å¼è§„åˆ™æ¥ä¼˜åŒ–ç”Ÿå­˜é€‰æ‹©ç­–ç•¥ã€‚åœ¨ NAS-Bench-201 æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSiamNAS ä»…éœ€ 0.01 GPU å¤©å³å¯è¯†åˆ«å‡º Pareto-optimal è§£ï¼ŒæˆåŠŸæ‰¾åˆ°äº† CIFAR-10 çš„æœ€ä½³æ¶æ„å’Œ ImageNet çš„æ¬¡ä½³æ¶æ„ã€‚è¯¥ç ”ç©¶ä¸ä»…æ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ï¼Œè¿˜å±•ç¤ºäº†åœ¨å¤šä»»åŠ¡ä¼˜åŒ–åŠç”Ÿæˆ Sets of Pareto Sets (SOS) æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Genetic and Evolutionary Computation Conference (GECCO' 25)",
      "pdf_url": "https://arxiv.org/pdf/2506.02623v1",
      "published_date": "2025-06-03 08:39:42 UTC",
      "updated_date": "2025-06-03 08:39:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:13:24.283783+00:00"
    },
    {
      "arxiv_id": "2506.02619v1",
      "title": "HGOT: Self-supervised Heterogeneous Graph Neural Network with Optimal Transport",
      "title_zh": "HGOTï¼šåŸºäºæœ€ä¼˜ä¼ è¾“çš„è‡ªç›‘ç£å¼‚è´¨å›¾ç¥ç»ç½‘ç»œ",
      "authors": [
        "Yanbei Liu",
        "Chongxu Wang",
        "Zhitao Xiao",
        "Lei Geng",
        "Yanwei Pang",
        "Xiao Wang"
      ],
      "abstract": "Heterogeneous Graph Neural Networks (HGNNs), have demonstrated excellent capabilities in processing heterogeneous information networks. Self-supervised learning on heterogeneous graphs, especially contrastive self-supervised strategy, shows great potential when there are no labels. However, this approach requires the use of carefully designed graph augmentation strategies and the selection of positive and negative samples. Determining the exact level of similarity between sample pairs is non-trivial.To solve this problem, we propose a novel self-supervised Heterogeneous graph neural network with Optimal Transport (HGOT) method which is designed to facilitate self-supervised learning for heterogeneous graphs without graph augmentation strategies. Different from traditional contrastive self-supervised learning, HGOT employs the optimal transport mechanism to relieve the laborious sampling process of positive and negative samples. Specifically, we design an aggregating view (central view) to integrate the semantic information contained in the views represented by different meta-paths (branch views). Then, we introduce an optimal transport plan to identify the transport relationship between the semantics contained in the branch view and the central view. This allows the optimal transport plan between graphs to align with the representations, forcing the encoder to learn node representations that are more similar to the graph space and of higher quality. Extensive experiments on four real-world datasets demonstrate that our proposed HGOT model can achieve state-of-the-art performance on various downstream tasks. In particular, in the node classification task, HGOT achieves an average of more than 6% improvement in accuracy compared with state-of-the-art methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† HGOTï¼Œä¸€ç§ç»“åˆæœ€ä¼˜ä¼ è¾“ (Optimal Transport) æœºåˆ¶çš„æ–°å‹è‡ªç›‘ç£å¼‚æ„å›¾ç¥ç»ç½‘ç»œ (Heterogeneous Graph Neural Network) æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¯¹æ¯”å­¦ä¹ ç­–ç•¥å¯¹å›¾å¢å¼ºå’Œå¤æ‚æ­£è´Ÿæ ·æœ¬é‡‡æ ·çš„ä¾èµ–ã€‚HGOT é€šè¿‡è®¾è®¡èšåˆè§†å›¾ï¼ˆä¸­å¿ƒè§†å›¾ï¼‰æ¥æ•´åˆç”±ä¸åŒå…ƒè·¯å¾„ (meta-paths) è¡¨ç¤ºçš„åˆ†æ”¯è§†å›¾ä¸­çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨æœ€ä¼˜ä¼ è¾“æ–¹æ¡ˆæ¥è¯†åˆ«ä¸¤è€…ä¹‹é—´çš„ä¼ è¾“å…³ç³»ã€‚è¿™ç§è®¾è®¡ä½¿å›¾ä¹‹é—´çš„æœ€ä¼˜ä¼ è¾“è®¡åˆ’èƒ½å¤Ÿä¸è¡¨ç¤ºç›¸å¯¹é½ï¼Œä»è€Œå¼•å¯¼ç¼–ç å™¨å­¦ä¹ åˆ°æ›´é«˜è´¨é‡ä¸”ä¸å›¾ç©ºé—´æ›´ç›¸ä¼¼çš„èŠ‚ç‚¹è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHGOT åœ¨å››ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†çš„å„é¡¹ä¸‹æ¸¸ä»»åŠ¡ä¸­å‡è¾¾åˆ°äº†å…ˆè¿›æ°´å¹³ã€‚ç‰¹åˆ«æ˜¯åœ¨èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒHGOT ç›¸æ¯”äºç°æœ‰ SOTA æ–¹æ³•å®ç°äº†å¹³å‡è¶…è¿‡ 6% çš„å‡†ç¡®ç‡æå‡ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¤„ç†å¼‚æ„å›¾ä¿¡æ¯æ–¹é¢çš„å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "The paper has 9 pages of text and 13 pages in total (including acknowledgments, impact statement, references, and appendix), with 6 figures and 2 tables. This paper has been accepted by ICML 2025 conference and this is a final version of the manuscript submitted to the conference",
      "pdf_url": "https://arxiv.org/pdf/2506.02619v1",
      "published_date": "2025-06-03 08:35:29 UTC",
      "updated_date": "2025-06-03 08:35:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:13:21.140243+00:00"
    },
    {
      "arxiv_id": "2506.02615v1",
      "title": "Hierarchical Question-Answering for Driving Scene Understanding Using Vision-Language Models",
      "title_zh": "åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„é©¾é©¶åœºæ™¯ç†è§£å±‚çº§åŒ–é—®ç­”",
      "authors": [
        "Safaa Abdullahi Moallim Mohamud",
        "Minjin Baek",
        "Dong Seog Han"
      ],
      "abstract": "In this paper, we present a hierarchical question-answering (QA) approach for scene understanding in autonomous vehicles, balancing cost-efficiency with detailed visual interpretation. The method fine-tunes a compact vision-language model (VLM) on a custom dataset specific to the geographical area in which the vehicle operates to capture key driving-related visual elements. At the inference stage, the hierarchical QA strategy decomposes the scene understanding task into high-level and detailed sub-questions. Instead of generating lengthy descriptions, the VLM navigates a structured question tree, where answering high-level questions (e.g., \"Is it possible for the ego vehicle to turn left at the intersection?\") triggers more detailed sub-questions (e.g., \"Is there a vehicle approaching the intersection from the opposite direction?\"). To optimize inference time, questions are dynamically skipped based on previous answers, minimizing computational overhead. The extracted answers are then synthesized using handcrafted templates to ensure coherent, contextually accurate scene descriptions. We evaluate the proposed approach on the custom dataset using GPT reference-free scoring, demonstrating its competitiveness with state-of-the-art methods like GPT-4o in capturing key scene details while achieving significantly lower inference time. Moreover, qualitative results from real-time deployment highlight the proposed approach's capacity to capture key driving elements with minimal latency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯ç†è§£çš„å±‚æ¬¡åŒ–é—®ç­”(Hierarchical QA)æ–¹æ³•ï¼Œæ—¨åœ¨å¹³è¡¡æˆæœ¬æ•ˆç›Šä¸è¯¦ç»†çš„è§†è§‰è§£è¯»ã€‚é€šè¿‡åœ¨ç‰¹å®šåœ°ç†åŒºåŸŸçš„è‡ªå®šä¹‰æ•°æ®é›†ä¸Šå¾®è°ƒå°å‹è§†è§‰è¯­è¨€æ¨¡å‹(VLM)ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰é©¾é©¶ç›¸å…³çš„å…³é”®è§†è§‰å…ƒç´ ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œè¯¥ç­–ç•¥åˆ©ç”¨ç»“æ„åŒ–é—®é¢˜æ ‘å°†ä»»åŠ¡åˆ†è§£ä¸ºé«˜å±‚å’Œè¯¦ç»†çš„å­é—®é¢˜ï¼Œå¹¶æ ¹æ®å…ˆå‰çš„å›ç­”åŠ¨æ€è·³è¿‡ä¸å¿…è¦çš„é—®é¢˜ï¼Œä»è€Œæå¤§åœ°ä¼˜åŒ–äº†æ¨ç†æ—¶é—´ã€‚æå–çš„ç­”æ¡ˆéšåé€šè¿‡æ‰‹å·¥æ¨¡æ¿åˆæˆï¼Œä»¥ç¡®ä¿ç”Ÿæˆè¿è´¯ä¸”ä¸Šä¸‹æ–‡å‡†ç¡®çš„åœºæ™¯æè¿°ã€‚å®éªŒè¯„ä¼°é‡‡ç”¨GPTå‚è€ƒæ— å…³è¯„åˆ†(Reference-free scoring)ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨æ•æ‰åœºæ™¯ç»†èŠ‚æ–¹é¢çš„è¡¨ç°ä¸GPT-4oç­‰å…ˆè¿›æ¨¡å‹ç›¸å½“ã€‚æ­¤å¤–ï¼Œå®æ—¶éƒ¨ç½²çš„å®šæ€§ç»“æœè¿›ä¸€æ­¥è¯æ˜äº†è¯¥æ–¹æ³•åœ¨æä½å»¶è¿Ÿä¸‹å‡†ç¡®æ•æ‰å…³é”®é©¾é©¶è¦ç´ çš„èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "pdf_url": "https://arxiv.org/pdf/2506.02615v1",
      "published_date": "2025-06-03 08:32:43 UTC",
      "updated_date": "2025-06-03 08:32:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:14:02.831174+00:00"
    },
    {
      "arxiv_id": "2506.02614v4",
      "title": "High Performance Space Debris Tracking in Complex Skylight Backgrounds with a Large-Scale Dataset",
      "title_zh": "å¤æ‚å¤©å…‰èƒŒæ™¯ä¸‹åŸºäºå¤§è§„æ¨¡æ•°æ®é›†çš„é«˜æ€§èƒ½ç©ºé—´ç¢ç‰‡è·Ÿè¸ª",
      "authors": [
        "Guohang Zhuang",
        "Weixi Song",
        "Jinyang Huang",
        "Chenwei Yang",
        "Wanli OuYang",
        "Yan Lu"
      ],
      "abstract": "With the rapid development of space exploration, space debris has attracted more attention due to its potential extreme threat, leading to the need for real-time and accurate debris tracking. However, existing methods are mainly based on traditional signal processing, which cannot effectively process the complex background and dense space debris. In this paper, we propose a deep learning-based Space Debris Tracking Network~(SDT-Net) to achieve highly accurate debris tracking. SDT-Net effectively represents the feature of debris, enhancing the efficiency and stability of end-to-end model learning. To train and evaluate this model effectively, we also produce a large-scale dataset Space Debris Tracking Dataset (SDTD) by a novel observation-based data simulation scheme. SDTD contains 18,040 video sequences with a total of 62,562 frames and covers 250,000 synthetic space debris. Extensive experiments validate the effectiveness of our model and the challenging of our dataset. Furthermore, we test our model on real data from the Antarctic Station, achieving a MOTA score of 73.2%, which demonstrates its strong transferability to real-world scenarios. Our dataset and code will be released soon.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç©ºé—´ç¢ç‰‡è·Ÿè¸ªåœ¨å¤æ‚å¤©å…‰èƒŒæ™¯ä¸‹ä¼ ç»Ÿæ–¹æ³•éš¾ä»¥æœ‰æ•ˆå¤„ç†çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„ç©ºé—´ç¢ç‰‡è·Ÿè¸ªç½‘ç»œ SDT-Netï¼Œæ—¨åœ¨å®ç°é«˜ç²¾åº¦çš„å®æ—¶è·Ÿè¸ªã€‚SDT-Net é€šè¿‡ä¼˜åŒ–ç¢ç‰‡ç‰¹å¾è¡¨å¾ï¼Œå¢å¼ºäº†ç«¯åˆ°ç«¯æ¨¡å‹å­¦ä¹ çš„æ•ˆç‡ä¸ç¨³å®šæ€§ã€‚ä¸ºäº†æœ‰æ•ˆè®­ç»ƒå’Œè¯„ä¼°è¯¥æ¨¡å‹ï¼Œç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ä¸€ç§æ–°å‹çš„åŸºäºè§‚æµ‹çš„æ•°æ®æ¨¡æ‹Ÿæ–¹æ¡ˆï¼Œæ„å»ºäº†åŒ…å« 18,040 ä¸ªè§†é¢‘åºåˆ—çš„å¤§è§„æ¨¡æ•°æ®é›† Space Debris Tracking Dataset (SDTD)ã€‚å®éªŒç»“æœä¸ä»…éªŒè¯äº† SDT-Net çš„æœ‰æ•ˆæ€§ï¼Œè¿˜åœ¨å—æç«™çš„çœŸå®æ•°æ®æµ‹è¯•ä¸­å–å¾—äº† 73.2% çš„ MOTA è¯„åˆ†ï¼Œè¯æ˜äº†è¯¥æ¨¡å‹åœ¨çœŸå®åœºæ™¯ä¸­å…·æœ‰æå¼ºçš„è¿ç§»èƒ½åŠ›ã€‚è¯¥æˆæœä¸ºåº”å¯¹å¤æ‚èƒŒæ™¯ä¸‹çš„ç©ºé—´ç¢ç‰‡ç›‘æµ‹æä¾›äº†é«˜æ•ˆçš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02614v4",
      "published_date": "2025-06-03 08:30:25 UTC",
      "updated_date": "2025-07-25 01:56:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:13:27.190727+00:00"
    },
    {
      "arxiv_id": "2506.02612v1",
      "title": "Simple, Good, Fast: Self-Supervised World Models Free of Baggage",
      "title_zh": "ç®€å•ã€ä¼˜è´¨ã€å¿«é€Ÿï¼šå»ç¹å°±ç®€çš„è‡ªç›‘ç£ä¸–ç•Œæ¨¡å‹",
      "authors": [
        "Jan Robine",
        "Marc HÃ¶ftmann",
        "Stefan Harmeling"
      ],
      "abstract": "What are the essential components of world models? How far do we get with world models that are not employing RNNs, transformers, discrete representations, and image reconstructions? This paper introduces SGF, a Simple, Good, and Fast world model that uses self-supervised representation learning, captures short-time dependencies through frame and action stacking, and enhances robustness against model errors through data augmentation. We extensively discuss SGF's connections to established world models, evaluate the building blocks in ablation studies, and demonstrate good performance through quantitative comparisons on the Atari 100k benchmark.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SGFï¼Œä¸€ç§ç®€å•ã€é«˜æ•ˆä¸”å¿«é€Ÿï¼ˆSimple, Good, Fastï¼‰çš„ä¸–ç•Œæ¨¡å‹ï¼Œæ—¨åœ¨æ¢ç©¶åœ¨ä¸ä½¿ç”¨ RNNsã€transformersã€ç¦»æ•£è¡¨ç¤ºï¼ˆdiscrete representationsï¼‰å’Œå›¾åƒé‡å»ºï¼ˆimage reconstructionsï¼‰çš„æƒ…å†µä¸‹ä¸–ç•Œæ¨¡å‹çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ã€‚SGF æ ¸å¿ƒé‡‡ç”¨è‡ªç›‘ç£è¡¨å¾å­¦ä¹ ï¼ˆself-supervised representation learningï¼‰ï¼Œå¹¶é€šè¿‡å¸§å’ŒåŠ¨ä½œå †å ï¼ˆframe and action stackingï¼‰æ¥æ•æ‰çŸ­æ—¶ä¾èµ–å…³ç³»ã€‚è¯¥æ¨¡å‹è¿›ä¸€æ­¥åˆ©ç”¨æ•°æ®å¢å¼ºï¼ˆdata augmentationï¼‰æŠ€æœ¯æ¥æå‡å¯¹æ¨¡å‹è¯¯å·®çš„é²æ£’æ€§ã€‚ä½œè€…è¯¦ç»†è®¨è®ºäº† SGF ä¸ç°æœ‰ä¸–ç•Œæ¨¡å‹çš„è”ç³»ï¼Œå¹¶é€šè¿‡æ¶ˆèå®éªŒéªŒè¯äº†å…¶æ„å»ºæ¨¡å—çš„æœ‰æ•ˆæ€§ã€‚åœ¨ Atari 100k åŸºå‡†æµ‹è¯•çš„å®šé‡è¯„ä¼°ä¸­ï¼ŒSGF å±•ç¤ºäº†è‰¯å¥½çš„æ€§èƒ½è¡¨ç°ï¼Œè¯æ˜äº†è¿™ç§ç²¾ç®€æ¶æ„åœ¨å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­çš„é«˜æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Published as a conference paper at ICLR 2025. Code is available at https://github.com/jrobine/sgf",
      "pdf_url": "https://arxiv.org/pdf/2506.02612v1",
      "published_date": "2025-06-03 08:29:32 UTC",
      "updated_date": "2025-06-03 08:29:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:13:38.750716+00:00"
    },
    {
      "arxiv_id": "2506.02610v1",
      "title": "Speaker Diarization with Overlapping Community Detection Using Graph Attention Networks and Label Propagation Algorithm",
      "title_zh": "åŸºäºå›¾æ³¨æ„åŠ›ç½‘ç»œå’Œæ ‡ç­¾ä¼ æ’­ç®—æ³•é‡å ç¤¾åŒºæ£€æµ‹çš„è¯´è¯äººæ—¥å¿—",
      "authors": [
        "Zhaoyang Li",
        "Jie Wang",
        "XiaoXiao Li",
        "Wangjie Li",
        "Longjie Luo",
        "Lin Li",
        "Qingyang Hong"
      ],
      "abstract": "In speaker diarization, traditional clustering-based methods remain widely used in real-world applications. However, these methods struggle with the complex distribution of speaker embeddings and overlapping speech segments. To address these limitations, we propose an Overlapping Community Detection method based on Graph Attention networks and the Label Propagation Algorithm (OCDGALP). The proposed framework comprises two key components: (1) a graph attention network that refines speaker embeddings and node connections by aggregating information from neighboring nodes, and (2) a label propagation algorithm that assigns multiple community labels to each node, enabling simultaneous clustering and overlapping community detection. Experimental results show that the proposed method significantly reduces the Diarization Error Rate (DER), achieving a state-of-the-art 15.94% DER on the DIHARD-III dataset without oracle Voice Activity Detection (VAD), and an impressive 11.07% with oracle VAD.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿè¯´è¯äººèšç±»æ–¹æ³•åœ¨å¤„ç†å¤æ‚è¯´è¯äººåµŒå…¥åˆ†å¸ƒå’Œé‡å è¯­éŸ³ï¼ˆoverlapping speech segmentsï¼‰æ—¶çš„å±€é™æ€§ï¼Œæå‡ºäº†åä¸º OCDGALP çš„é‡å ç¤¾åŒºæ£€æµ‹æ–¹æ³•ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å›¾æ³¨æ„åŠ›ç½‘ç»œï¼ˆGraph Attention networksï¼‰å’Œæ ‡ç­¾ä¼ æ’­ç®—æ³•ï¼ˆLabel Propagation Algorithmï¼‰ï¼Œå‰è€…é€šè¿‡èšåˆé‚»å±…ä¿¡æ¯ä¼˜åŒ–è¯´è¯äººåµŒå…¥ï¼ˆspeaker embeddingsï¼‰ä¸èŠ‚ç‚¹è¿æ¥ï¼Œåè€…åˆ™ä¸ºèŠ‚ç‚¹åˆ†é…å¤šä¸ªæ ‡ç­¾ä»¥å®ç°åŒæ­¥èšç±»å’Œé‡å ç¤¾åŒºæ£€æµ‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOCDGALP åœ¨ DIHARD-III æ•°æ®é›†ä¸Šæ˜¾è‘—é™ä½äº†è¯´è¯äººæ—¥å¿—é”™è¯¯ç‡ï¼ˆDiarization Error Rate, DERï¼‰ï¼Œåœ¨æœªä½¿ç”¨ oracle Voice Activity Detection (VAD) çš„æƒ…å†µä¸‹è¾¾åˆ° 15.94% çš„ DERï¼Œè€Œåœ¨ä½¿ç”¨ oracle VAD æ—¶è¾¾åˆ°äº† 11.07% çš„ä¼˜å¼‚è¡¨ç°ã€‚è¯¥æ–¹æ³•æˆåŠŸè¯æ˜äº†åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œå¤„ç†é‡å è¯­éŸ³å’Œä¼˜åŒ–è¯´è¯äººèšç±»çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02610v1",
      "published_date": "2025-06-03 08:29:10 UTC",
      "updated_date": "2025-06-03 08:29:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:13:36.085228+00:00"
    },
    {
      "arxiv_id": "2506.02609v1",
      "title": "A Time-Enhanced Data Disentanglement Network for Traffic Flow Forecasting",
      "title_zh": "é¢å‘äº¤é€šæµé¢„æµ‹çš„æ—¶é—´å¢å¼ºå‹æ•°æ®è§£è€¦ç½‘ç»œ",
      "authors": [
        "Tianfan Jiang",
        "Mei Wu",
        "Wenchao Weng",
        "Dewen Seng",
        "Yiqian Lin"
      ],
      "abstract": "In recent years, traffic flow prediction has become a highlight in the field of intelligent transportation systems. However, due to the temporal variations and dynamic spatial correlations of traffic data, traffic prediction remains highly challenging.Traditional spatiotemporal networks, which rely on end-to-end training, often struggle to handle the diverse data dependencies of multiple traffic flow patterns. Additionally, traffic flow variations are highly sensitive to temporal information changes. Regrettably, other researchers have not sufficiently recognized the importance of temporal information.To address these challenges, we propose a novel approach called A Time-Enhanced Data Disentanglement Network for Traffic Flow Forecasting (TEDDN). This network disentangles the originally complex and intertwined traffic data into stable patterns and trends. By flexibly learning temporal and node information through a dynamic graph enhanced by a temporal feature extraction module, TEDDN demonstrates significant efficacy in disentangling and extracting complex traffic information. Experimental evaluations and ablation studies on four real-world datasets validate the superiority of our method.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äº¤é€šæµé‡é¢„æµ‹ä¸­æ—¶é—´å˜å¼‚æ€§å’ŒåŠ¨æ€ç©ºé—´ç›¸å…³æ€§å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º TEDDN (A Time-Enhanced Data Disentanglement Network) çš„æ–°å‹ç½‘ç»œï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ—¶ç©ºç½‘ç»œå¯¹æ—¶é—´ä¿¡æ¯é‡è§†ä¸è¶³åŠéš¾ä»¥å¤„ç†å¤æ‚æ•°æ®ä¾èµ–çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ Data Disentanglement æŠ€æœ¯å°†äº¤ç»‡å¤æ‚çš„äº¤é€šæ•°æ®è§£è€¦ä¸ºç¨³å®šçš„æ¨¡å¼å’Œè¶‹åŠ¿ï¼Œå¹¶åˆ©ç”¨é›†æˆæ—¶é—´ç‰¹å¾æå–æ¨¡å—çš„ Dynamic Graph çµæ´»å­¦ä¹ æ—¶é—´å’ŒèŠ‚ç‚¹ä¿¡æ¯ã€‚å®éªŒè¯„ä¼°å’Œæ¶ˆèç ”ç©¶åœ¨å››ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ¨¡å‹åœ¨æå–å¤æ‚äº¤é€šä¿¡æ¯æ–¹é¢çš„æ˜¾è‘—æ•ˆèƒ½ã€‚ç ”ç©¶ç»“æœè¯æ˜äº† TEDDN åœ¨é¢„æµ‹å‡†ç¡®æ€§ä¸Šçš„ä¼˜è¶Šæ€§ï¼Œä¸ºæ™ºèƒ½äº¤é€šç³»ç»Ÿæä¾›äº†æ›´å…·é²æ£’æ€§çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02609v1",
      "published_date": "2025-06-03 08:28:48 UTC",
      "updated_date": "2025-06-03 08:28:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:13:48.270295+00:00"
    },
    {
      "arxiv_id": "2506.02606v4",
      "title": "Multi Layered Autonomy and AI Ecologies in Robotic Art Installations",
      "title_zh": "æœºå™¨äººè‰ºæœ¯è£…ç½®ä¸­çš„å¤šå±‚è‡ªä¸»æ€§ä¸äººå·¥æ™ºèƒ½ç”Ÿæ€",
      "authors": [
        "Baoyang Chen",
        "Xian Xu",
        "Huamin Qu"
      ],
      "abstract": "This paper presents Symbiosis of Agents, is a large-scale installation by Baoyang Chen (baoyangchen.com), that embeds AI-driven robots in an immersive, mirror-lined arena, probing the tension between machine agency and artistic authorship. Drawing on early cybernetics, rule-based conceptual art, and seminal robotic works, it orchestrates fluid exchanges among robotic arms, quadruped machines, their environment, and the public. A three tier faith system pilots the ecology: micro-level adaptive tactics, meso-level narrative drives, and a macro-level prime directive. This hierarchy lets behaviors evolve organically in response to environmental cues and even a viewer's breath, turning spectators into co-authors of the unfolding drama. Framed by a speculative terraforming scenario that recalls the historical exploitation of marginalized labor, the piece asks who bears responsibility in AI-mediated futures. Choreographed motion, AI-generated scripts, reactive lighting, and drifting fog cast the robots as collaborators rather than tools, forging a living, emergent artwork. Exhibited internationally, Symbiosis of Agents shows how cybernetic feedback, robotic experimentation, and conceptual rule-making can converge to redefine agency, authorship, and ethics in contemporary art.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†ç”±é™ˆæŠ±é˜³åˆ›ä½œçš„å¤§å‹è£…ç½®è‰ºæœ¯ä½œå“ã€Šæ™ºèƒ½ä½“çš„å…±ç”Ÿã€‹(Symbiosis of Agents)ï¼Œæ¢è®¨äº†æœºå™¨äººè‰ºæœ¯è£…ç½®ä¸­å¤šå±‚çº§è‡ªä¸»æ€§(Multi Layered Autonomy)ä¸ AI ç”Ÿæ€ç³»ç»Ÿ(AI Ecologies)ä¹‹é—´çš„å…³ç³»ã€‚è¯¥è£…ç½®åœ¨æ²‰æµ¸å¼çš„é•œåƒç©ºé—´ä¸­éƒ¨ç½²äº† AI é©±åŠ¨çš„æœºå™¨äººï¼Œæ—¨åœ¨å‰–ææœºå™¨èƒ½åŠ¨æ€§(machine agency)ä¸è‰ºæœ¯åˆ›ä½œç½²åæƒ(artistic authorship)ä¹‹é—´çš„å¼ åŠ›ã€‚ç³»ç»Ÿé€šè¿‡ä¸‰å±‚ä¿¡åº¦ç³»ç»Ÿ(three tier faith system)è¿›è¡Œé©±åŠ¨ï¼ŒåŒ…æ‹¬å¾®è§‚å±‚é¢çš„è‡ªé€‚åº”ç­–ç•¥(adaptive tactics)ã€ä¸­è§‚å±‚é¢çš„å™äº‹é©±åŠ¨(narrative drives)ä»¥åŠå®è§‚å±‚é¢çš„é¦–è¦æŒ‡ä»¤(prime directive)ã€‚è¿™ç§å±‚çº§ç»“æ„ä½¿å¾—æœºå™¨äººè¡Œä¸ºèƒ½å¤Ÿæ ¹æ®ç¯å¢ƒä¿¡å·åŠè§‚ä¼—çš„å‘¼å¸äº§ç”Ÿæœ‰æœºæ¼”åŒ–ï¼Œä»è€Œè®©è§‚ä¼—å‚ä¸å…±åŒåˆ›ä½œã€‚ä½œå“ç»“åˆäº†æ¨æµ‹æ€§çš„åœ°å½¢æ”¹é€ (speculative terraforming)æƒ…å¢ƒï¼Œåæ€äº† AI é©±åŠ¨çš„æœªæ¥ä¸­è´£ä»»å½’å±åŠä¼¦ç†(ethics)é—®é¢˜ã€‚é€šè¿‡ç¼–æ’è¿åŠ¨ã€AI ç”Ÿæˆè„šæœ¬åŠæ§åˆ¶è®ºåé¦ˆ(cybernetic feedback)ï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº†æœºå™¨äººå®éªŒä¸æ¦‚å¿µè§„åˆ™åˆ¶å®šå¦‚ä½•å…±åŒé‡æ–°å®šä¹‰å½“ä»£è‰ºæœ¯ä¸­çš„èƒ½åŠ¨æ€§ä¸åˆ›ä½œæœ¬è´¨ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02606v4",
      "published_date": "2025-06-03 08:28:19 UTC",
      "updated_date": "2025-09-30 14:42:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:14:05.130657+00:00"
    },
    {
      "arxiv_id": "2506.02596v1",
      "title": "EssayBench: Evaluating Large Language Models in Multi-Genre Chinese Essay Writing",
      "title_zh": "EssayBenchï¼šè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šä½“è£ä¸­æ–‡ä½œæ–‡å†™ä½œä¸­çš„è¡¨ç°",
      "authors": [
        "Fan Gao",
        "Dongyuan Li",
        "Ding Xia",
        "Fei Mi",
        "Yasheng Wang",
        "Lifeng Shang",
        "Baojun Wang"
      ],
      "abstract": "Chinese essay writing and its evaluation are critical in educational contexts, yet the capabilities of Large Language Models (LLMs) in this domain remain largely underexplored. Existing benchmarks often rely on coarse-grained text quality metrics, largely overlooking the structural and rhetorical complexities of Chinese essays, particularly across diverse genres. To address this gap, we propose \\benchName, a multi-genre benchmark specifically designed for Chinese essay writing across four major genres: Argumentative, Narrative, Descriptive, and Expository. We curate and refine a total of 728 real-world prompts to ensure authenticity and meticulously categorize them into the \\textit{Open-Ended} and \\textit{Constrained} sets to capture diverse writing scenarios. To reliably evaluate generated essays, we develop a fine-grained, genre-specific scoring framework that hierarchically aggregates scores. We further validate our evaluation protocol through a comprehensive human agreement study. Finally, we benchmark 15 large-sized LLMs, analyzing their strengths and limitations across genres and instruction types. With \\benchName, we aim to advance LLM-based Chinese essay evaluation and inspire future research on improving essay generation in educational settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† EssayBenchï¼Œä¸€ä¸ªä¸“ä¸ºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸­æ–‡ä½œæ–‡å†™ä½œèƒ½åŠ›è€Œè®¾è®¡çš„å¤šæ–‡ä½“åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†è®®è®ºæ–‡ï¼ˆArgumentativeï¼‰ã€è®°å™æ–‡ï¼ˆNarrativeï¼‰ã€æå†™æ–‡ï¼ˆDescriptiveï¼‰å’Œè¯´æ˜æ–‡ï¼ˆExpositoryï¼‰å››å¤§ä¸»è¦æ–‡ä½“ã€‚ç ”ç©¶å›¢é˜Ÿç²¾å¿ƒæ•´ç†å¹¶ä¼˜åŒ–äº† 728 ä¸ªçœŸå®ä¸–ç•Œçš„æç¤ºè¯ï¼ˆpromptsï¼‰ï¼Œå°†å…¶åˆ†ä¸ºå¼€æ”¾å¼ï¼ˆOpen-Endedï¼‰å’Œå—é™å¼ï¼ˆConstrainedï¼‰ä¸¤ç±»ä»¥æ•æ‰å¤šæ ·çš„å†™ä½œåœºæ™¯ã€‚ä¸ºç¡®ä¿è¯„ä¼°çš„å¯é æ€§ï¼Œç ”ç©¶å¼€å‘äº†ä¸€å¥—ç»†ç²’åº¦ä¸”é’ˆå¯¹ç‰¹å®šæ–‡ä½“çš„è¯„åˆ†æ¡†æ¶ï¼Œå¹¶åˆ©ç”¨åˆ†å±‚èšåˆçš„æ–¹å¼è¿›è¡Œè¯„åˆ†ã€‚ç ”ç©¶äººå‘˜é€šè¿‡å…¨é¢çš„ä¸“å®¶è¯„ä¼°ä¸€è‡´æ€§ç ”ç©¶ï¼ˆhuman agreement studyï¼‰éªŒè¯äº†è¯¥è¯„ä¼°åè®®çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å¯¹ 15 ä¸ªå¤§è§„æ¨¡ LLMs è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚å®éªŒæ·±å…¥åˆ†æäº†è¿™äº›æ¨¡å‹åœ¨ä¸åŒæ–‡ä½“å’ŒæŒ‡ä»¤ç±»å‹ä¸‹çš„ä¼˜åŠ£åŠ¿ï¼Œä¸ºæœªæ¥åœ¨æ•™è‚²é¢†åŸŸæå‡ä¸­æ–‡ä½œæ–‡ç”Ÿæˆä¸è¯„ä¼°è´¨é‡æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02596v1",
      "published_date": "2025-06-03 08:14:46 UTC",
      "updated_date": "2025-06-03 08:14:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:14:45.784142+00:00"
    },
    {
      "arxiv_id": "2506.02594v1",
      "title": "EALG: Evolutionary Adversarial Generation of Language Model-Guided Generators for Combinatorial Optimization",
      "title_zh": "EALGï¼šé¢å‘ç»„åˆä¼˜åŒ–çš„è¯­è¨€æ¨¡å‹å¼•å¯¼ç”Ÿæˆå™¨çš„æ¼”åŒ–å¯¹æŠ—ç”Ÿæˆ",
      "authors": [
        "Ruibo Duan",
        "Yuxin Liu",
        "Xinyao Dong",
        "Chenglin Fan"
      ],
      "abstract": "Generating challenging instances is crucial for the evaluation and advancement of combinatorial optimization solvers. In this work, we introduce EALG (Evolutionary Adversarial Generation of Language Model-Guided Generators), a novel framework that automates the co-evolution of optimization problem instances and their corresponding heuristic solvers using large language models (LLMs). EALG leverages a mutation-based adversarial approach that dynamically evolves instance generation procedures to create increasingly difficult problems, while simultaneously synthesizing adaptive heuristic algorithms through interactions with LLMs guided by algorithmic structure. Unlike existing approaches that focus solely on static benchmark creation or manual solver design, EALG provides a seamless pipeline from instance generation to solver synthesis. Experimental results demonstrate that EALG generates significantly harder instances than current benchmarks, and its synthesized solvers generalize effectively across a broad spectrum of combinatorial tasks. This work explores a new paradigm for combinatorial optimization that integrates instance generation with solver design, resulting in state-of-the-art performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EALG (Evolutionary Adversarial Generation of Language Model-Guided Generators)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è‡ªåŠ¨ååŒè¿›åŒ–ç»„åˆä¼˜åŒ– (Combinatorial Optimization) é—®é¢˜å®ä¾‹åŠå…¶å¯¹åº”å¯å‘å¼æ±‚è§£å™¨ (Heuristic Solvers) çš„åˆ›æ–°æ¡†æ¶ã€‚EALGé‡‡ç”¨äº†ä¸€ç§åŸºäºå˜å¼‚çš„å¯¹æŠ—æ€§æ–¹æ³• (Mutation-based Adversarial Approach)ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) åŠ¨æ€æ¼”åŒ–å®ä¾‹ç”Ÿæˆç¨‹åºï¼Œä»è€Œåˆ›å»ºéš¾åº¦ä¸æ–­å¢åŠ çš„é—®é¢˜ã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ä¸å—ç®—æ³•ç»“æ„å¼•å¯¼çš„å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œäº¤äº’ï¼Œèƒ½å¤Ÿè‡ªåŠ¨åˆæˆå‡ºå…·æœ‰å¼ºé€‚åº”æ€§çš„å¯å‘å¼ç®—æ³•ã€‚ä¸ä»¥å¾€ä¸“æ³¨äºé™æ€åŸºå‡†æµ‹è¯•æˆ–æ‰‹åŠ¨è®¾è®¡æ±‚è§£å™¨çš„æ–¹æ³•ä¸åŒï¼ŒEALGå®ç°äº†ä»å®ä¾‹ç”Ÿæˆåˆ°æ±‚è§£å™¨åˆæˆçš„æ— ç¼è¡”æ¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEALGç”Ÿæˆçš„å®ä¾‹æ¯”ç°æœ‰åŸºå‡†æµ‹è¯•æ˜¾è‘—æ›´éš¾ï¼Œä¸”å…¶åˆæˆçš„æ±‚è§£å™¨åœ¨å¹¿æ³›çš„ç»„åˆä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥å·¥ä½œæ¢ç´¢äº†å°†å®ä¾‹ç”Ÿæˆä¸æ±‚è§£å™¨è®¾è®¡ç›¸é›†æˆçš„ç»„åˆä¼˜åŒ–æ–°èŒƒå¼ï¼Œå¹¶æœ€ç»ˆå®ç°äº†é¢†åŸŸå†…æœ€å…ˆè¿› (State-of-the-art) çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02594v1",
      "published_date": "2025-06-03 08:13:41 UTC",
      "updated_date": "2025-06-03 08:13:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:14:30.394084+00:00"
    },
    {
      "arxiv_id": "2506.02589v1",
      "title": "Evaluating Named Entity Recognition Models for Russian Cultural News Texts: From BERT to LLM",
      "title_zh": "é’ˆå¯¹ä¿„è¯­æ–‡åŒ–æ–°é—»æ–‡æœ¬çš„å‘½åå®ä½“è¯†åˆ«æ¨¡å‹è¯„ä¼°ï¼šä» BERT åˆ°å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Maria Levchenko"
      ],
      "abstract": "This paper addresses the challenge of Named Entity Recognition (NER) for person names within the specialized domain of Russian news texts concerning cultural events. The study utilizes the unique SPbLitGuide dataset, a collection of event announcements from Saint Petersburg spanning 1999 to 2019. A comparative evaluation of diverse NER models is presented, encompassing established transformer-based architectures such as DeepPavlov, RoBERTa, and SpaCy, alongside recent Large Language Models (LLMs) including GPT-3.5, GPT-4, and GPT-4o. Key findings highlight the superior performance of GPT-4o when provided with specific prompting for JSON output, achieving an F1 score of 0.93. Furthermore, GPT-4 demonstrated the highest precision at 0.99. The research contributes to a deeper understanding of current NER model capabilities and limitations when applied to morphologically rich languages like Russian within the cultural heritage domain, offering insights for researchers and practitioners. Follow-up evaluation with GPT-4.1 (April 2025) achieves F1=0.94 for both simple and structured prompts, demonstrating rapid progress across model families and simplified deployment requirements.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¿„ç½—æ–¯æ–‡åŒ–æ–°é—»æ–‡æœ¬ä¸­çš„äººåå‘½åå®ä½“è¯†åˆ«ï¼ˆNamed Entity Recognition, NERï¼‰æŒ‘æˆ˜ï¼Œç³»ç»Ÿè¯„ä¼°äº†ä»ä¼ ç»Ÿçš„ Transformer æ¶æ„åˆ°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¤šç§æ¨¡å‹è¡¨ç°ã€‚ç ”ç©¶åˆ©ç”¨äº†ç‹¬ç‰¹çš„ SPbLitGuide æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ¶µç›–äº† 1999 å¹´è‡³ 2019 å¹´é—´åœ£å½¼å¾—å ¡çš„æ–‡åŒ–æ´»åŠ¨å…¬å‘Šã€‚å®éªŒå¯¹æ¯”äº† DeepPavlovã€RoBERTaã€SpaCy ç­‰ç»å…¸æ¶æ„ï¼Œä»¥åŠ GPT-3.5ã€GPT-4 å’Œ GPT-4o ç­‰æœ€æ–°æ¨¡å‹ã€‚å…³é”®å‘ç°æŒ‡å‡ºï¼ŒGPT-4o åœ¨é…åˆ JSON è¾“å‡ºçš„ç‰¹å®šæç¤ºè¯æ—¶è¡¨ç°æœ€ä½³ï¼ŒF1 åˆ†æ•°è¾¾åˆ° 0.93ï¼Œè€Œ GPT-4 åˆ™åœ¨ç²¾ç¡®ç‡ä¸Šè¾¾åˆ° 0.99ã€‚åç»­å¯¹ 2025 å¹´ 4 æœˆå‘å¸ƒçš„ GPT-4.1 çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå…¶ F1 åˆ†æ•°è¿›ä¸€æ­¥æå‡è‡³ 0.94ï¼Œè¯æ˜äº†æ¨¡å‹æ€§èƒ½çš„å¿«é€Ÿè¿›æ­¥ã€‚è¯¥é¡¹å·¥ä½œæ·±åŒ–äº†å¯¹ NER æ¨¡å‹åœ¨å¤„ç†ä¿„è¯­ç­‰å½¢æ€ä¸°å¯Œè¯­è¨€åŠæ–‡åŒ–é—äº§é¢†åŸŸåº”ç”¨å±€é™ä¸æ½œåŠ›çš„ç†è§£ï¼Œä¸ºç›¸å…³ä»ä¸šè€…æä¾›äº†å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02589v1",
      "published_date": "2025-06-03 08:11:16 UTC",
      "updated_date": "2025-06-03 08:11:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:14:33.489025+00:00"
    },
    {
      "arxiv_id": "2506.04265v2",
      "title": "CORA: Coalitional Rational Advantage Decomposition for Multi-Agent Policy Gradients",
      "title_zh": "CORAï¼šé¢å‘å¤šæ™ºèƒ½ä½“ç­–ç•¥æ¢¯åº¦çš„è”ç›Ÿç†æ€§ä¼˜åŠ¿åˆ†è§£",
      "authors": [
        "Mengda Ji",
        "Genjiu Xu",
        "Liying Wang"
      ],
      "abstract": "This work focuses on the credit assignment problem in cooperative multi-agent reinforcement learning (MARL). Sharing the global advantage among agents often leads to suboptimal policy updates as it fails to account for the distinct contributions of agents. Although numerous methods consider global or individual contributions for credit assignment, a detailed analysis at the coalition level remains lacking in many approaches. This work analyzes the over-updating problem during multi-agent policy updates from a coalition-level perspective. To address this issue, we propose a credit assignment method called Coalitional Rational Advantage Decomposition (CORA). CORA evaluates coalitional advantages via marginal contributions from all possible coalitions and decomposes advantages using the core solution from cooperative game theory, ensuring coalitional rationality. To reduce computational overhead, CORA employs random coalition sampling. Experiments on matrix games, differential games, and multi-agent collaboration benchmarks demonstrate that CORA outperforms strong baselines, particularly in tasks with multiple local optima. These findings highlight the importance of coalition-aware credit assignment for improving MARL performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åˆä½œå¼å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (MARL) ä¸­çš„ä¿¡ç”¨åˆ†é… (credit assignment) é—®é¢˜ï¼ŒæŒ‡å‡ºå…±äº«å…¨å±€ä¼˜åŠ¿å¾€å¾€å› å¿½è§†æ™ºèƒ½ä½“çš„ä¸ªä½“è´¡çŒ®è€Œå¯¼è‡´ç­–ç•¥æ›´æ–°æ¬¡ä¼˜åŠè¿‡åº¦æ›´æ–° (over-updating) ç°è±¡ã€‚ä¸ºæ­¤ä½œè€…æå‡ºäº† Coalitional Rational Advantage Decomposition (CORA) æ–¹æ³•ï¼Œé€šè¿‡è®¡ç®—æ‰€æœ‰å¯èƒ½è”ç›Ÿçš„è¾¹é™…è´¡çŒ®æ¥è¯„ä¼°è”ç›Ÿä¼˜åŠ¿ã€‚è¯¥æ–¹æ³•åˆ©ç”¨åˆä½œåšå¼ˆè®ºä¸­çš„æ ¸å¿ƒ (core) æ¦‚å¿µè¿›è¡Œä¼˜åŠ¿åˆ†è§£ï¼Œä»è€Œç¡®ä¿äº†è”ç›Ÿç†æ€§ (coalitional rationality)ï¼Œå¹¶ç»“åˆéšæœºè”ç›Ÿé‡‡æ ·æŠ€æœ¯ä»¥é™ä½è®¡ç®—å¼€é”€ã€‚åœ¨çŸ©é˜µåšå¼ˆã€å¾®åˆ†åšå¼ˆåŠå¤šæ™ºèƒ½ä½“åä½œåŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒè¡¨æ˜ï¼ŒCORA çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºå¼ºåŸºçº¿æ¨¡å‹ï¼Œå°¤å…¶åœ¨å­˜åœ¨å¤šä¸ªå±€éƒ¨æœ€ä¼˜è§£çš„ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†æ„ŸçŸ¥è”ç›Ÿçš„ä¿¡ç”¨åˆ†é…å¯¹äºæå‡å¤šæ™ºèƒ½ä½“ç³»ç»ŸååŒæ€§èƒ½çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.GT",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.04265v2",
      "published_date": "2025-06-03 08:04:43 UTC",
      "updated_date": "2025-06-18 11:50:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:14:42.894874+00:00"
    },
    {
      "arxiv_id": "2506.02584v1",
      "title": "Prosodic Structure Beyond Lexical Content: A Study of Self-Supervised Learning",
      "title_zh": "è¶…è¶Šè¯æ±‡å†…å®¹çš„éŸµå¾‹ç»“æ„ï¼šè‡ªç›‘ç£å­¦ä¹ ç ”ç©¶",
      "authors": [
        "Sarenne Wallbridge",
        "Christoph Minixhofer",
        "Catherine Lai",
        "Peter Bell"
      ],
      "abstract": "People exploit the predictability of lexical structures during text comprehension. Though predictable structure is also present in speech, the degree to which prosody, e.g. intonation, tempo, and loudness, contributes to such structure independently of the lexical content is unclear. This study leverages self-supervised learning (SSL) to examine the temporal granularity of structures in the acoustic correlates of prosody. Representations from our proposed Masked Prosody Model can predict perceptual labels dependent on local information, such as word boundaries, but provide the most value for labels involving longer-term structures, like emotion recognition. Probing experiments across various perceptual labels show strong relative gains over untransformed pitch, energy, and voice activity features. Our results reveal the importance of SSL training objective timescale and highlight the value of complex SSL-encoded structures compared to more constrained classical structures.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è¯­éŸ³éŸµå¾‹ï¼ˆå¦‚ intonation, tempo å’Œ loudnessï¼‰åœ¨è„±ç¦»è¯æ±‡å†…å®¹çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•ç‹¬ç«‹è´¡çŒ®äºè¯­éŸ³çš„å¯é¢„æµ‹æ€§ç»“æ„ã€‚ç ”ç©¶åˆ©ç”¨è‡ªç›‘ç£å­¦ä¹  (Self-Supervised Learning, SSL) è¯„ä¼°äº†éŸµå¾‹å£°å­¦ç‰¹å¾ä¸­çš„æ—¶é—´ç²’åº¦ï¼Œå¹¶æå‡ºäº† Masked Prosody Model æ¥æ•è·è¿™äº›ç»“æ„ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹ç”Ÿæˆçš„è¡¨ç¤ºä¸ä»…èƒ½é¢„æµ‹ word boundaries ç­‰å±€éƒ¨ä¿¡æ¯ï¼Œåœ¨ emotion recognition ç­‰æ¶‰åŠé•¿æœŸç»“æ„çš„ä»»åŠ¡ä¸­æ›´å…·ä»·å€¼ã€‚é€šè¿‡å¯¹å¤šç§æ„ŸçŸ¥æ ‡ç­¾çš„æ¢æµ‹ï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºåŸå§‹çš„ pitch, energy å’Œ voice activity ç‰¹å¾å±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº† SSL è®­ç»ƒç›®æ ‡æ—¶é—´å°ºåº¦çš„å…³é”®ä½œç”¨ï¼Œå¹¶è¯æ˜äº†å¤æ‚ SSL ç¼–ç ç»“æ„åœ¨æ•æ‰è¯­éŸ³éŸµå¾‹ç‰¹å¾æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„å—é™ç»“æ„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at INTERSPEECH 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.02584v1",
      "published_date": "2025-06-03 08:04:03 UTC",
      "updated_date": "2025-06-03 08:04:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:14:41.191231+00:00"
    },
    {
      "arxiv_id": "2506.03218v1",
      "title": "Beware! The AI Act Can Also Apply to Your AI Research Practices",
      "title_zh": "è­¦æƒ•ï¼šäººå·¥æ™ºèƒ½æ³•æ¡ˆäº¦å¯èƒ½é€‚ç”¨äºæ‚¨çš„äººå·¥æ™ºèƒ½ç ”ç©¶å®è·µ",
      "authors": [
        "Alina Wernick",
        "Kristof Meding"
      ],
      "abstract": "The EU has become one of the vanguards in regulating the digital age. A particularly important regulation in the Artificial Intelligence (AI) domain is the EU AI Act, which entered into force in 2024. The AI Act specifies -- due to a risk-based approach -- various obligations for providers of AI systems. These obligations, for example, include a cascade of documentation and compliance measures, which represent a potential obstacle to science. But do these obligations also apply to AI researchers? This position paper argues that, indeed, the AI Act's obligations could apply in many more cases than the AI community is aware of. In our analysis of the AI Act and its applicability, we contribute the following: 1.) We give a high-level introduction to the AI Act aimed at non-legal AI research scientists. 2.) We explain with everyday research examples why the AI Act applies to research. 3.) We analyse the exceptions of the AI Act's applicability and state that especially scientific research exceptions fail to account for current AI research practices. 4.) We propose changes to the AI Act to provide more legal certainty for AI researchers and give two recommendations for AI researchers to reduce the risk of not complying with the AI Act. We see our paper as a starting point for a discussion between policymakers, legal scholars, and AI researchers to avoid unintended side effects of the AI Act on research.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†2024å¹´ç”Ÿæ•ˆçš„æ¬§ç›Ÿã€Šäººå·¥æ™ºèƒ½æ³•æ¡ˆã€‹(EU AI Act)å¯¹äººå·¥æ™ºèƒ½ç§‘ç ”å®è·µçš„æ½œåœ¨å½±å“ï¼ŒæŒ‡å‡ºè¯¥æ³•æ¡ˆè§„å®šçš„ä¹‰åŠ¡åœ¨è®¸å¤šç§‘ç ”åœºæ™¯ä¸­åŒæ ·é€‚ç”¨ï¼Œå…¶å¹¿æ³›ç¨‹åº¦è¶…å‡ºäº†AIç¤¾åŒºçš„ç°æœ‰è®¤çŸ¥ã€‚æ–‡ç« ä¸ºéæ³•å¾‹èƒŒæ™¯çš„ç§‘ç ”äººå‘˜æä¾›äº†è¯¥æ³•æ¡ˆçš„é«˜å±‚çº§ä»‹ç»ï¼Œå¹¶é€šè¿‡æ—¥å¸¸ç ”ç©¶æ¡ˆä¾‹é˜è¿°äº†å…¶æ³•å¾‹é€‚ç”¨æ€§ã€‚åˆ†æå‘ç°ï¼Œç°æœ‰çš„ç§‘ç ”è±å…æ¡æ¬¾æœªèƒ½å……åˆ†æ¶µç›–å½“å‰çš„AIç ”ç©¶å®è·µï¼Œå¯èƒ½å¯¹ç§‘å­¦è¿›æ­¥äº§ç”Ÿæ½œåœ¨éšœç¢ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†é’ˆå¯¹æ€§çš„æ³•æ¡ˆä¿®æ”¹å»ºè®®ï¼Œå¹¶ä¸ºç ”ç©¶äººå‘˜æä¾›äº†é™ä½åˆè§„é£é™©çš„å®ç”¨æŒ‡å—ã€‚è¯¥å·¥ä½œæ—¨åœ¨é€šè¿‡å¼•å‘æ”¿ç­–åˆ¶å®šè€…ä¸æ³•å¾‹å­¦è€…åŠç§‘ç ”äººå‘˜ä¹‹é—´çš„è®¨è®ºï¼Œé¿å…æ³•æ¡ˆå¯¹ç§‘å­¦ç ”ç©¶äº§ç”Ÿéé¢„æœŸçš„è´Ÿé¢å‰¯ä½œç”¨ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03218v1",
      "published_date": "2025-06-03 08:01:36 UTC",
      "updated_date": "2025-06-03 08:01:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:14:57.891670+00:00"
    },
    {
      "arxiv_id": "2506.02580v2",
      "title": "V2X-UniPool: Unifying Multimodal Perception and Knowledge Reasoning for Autonomous Driving",
      "title_zh": "V2X-UniPoolï¼šé¢å‘è‡ªåŠ¨é©¾é©¶çš„ç»Ÿä¸€å¤šæ¨¡æ€æ„ŸçŸ¥ä¸çŸ¥è¯†æ¨ç†",
      "authors": [
        "Xuewen Luo",
        "Fengze Yang",
        "Fan Ding",
        "Xiangbo Gao",
        "Shuo Xing",
        "Yang Zhou",
        "Zhengzhong Tu",
        "Chenxi Liu"
      ],
      "abstract": "Autonomous driving (AD) has achieved significant progress, yet single-vehicle perception remains constrained by sensing range and occlusions. Vehicle-to-Everything (V2X) communication addresses these limits by enabling collaboration across vehicles and infrastructure, but it also faces heterogeneity, synchronization, and latency constraints. Language models offer strong knowledge-driven reasoning and decision-making capabilities, but they are not inherently designed to process raw sensor streams and are prone to hallucination. We propose V2X-UniPool, the first framework that unifies V2X perception with language-based reasoning for knowledge-driven AD. It transforms multimodal V2X data into structured, language-based knowledge, organizes it in a time-indexed knowledge pool for temporally consistent reasoning, and employs Retrieval-Augmented Generation (RAG) to ground decisions in real-time context. Experiments on the real-world DAIR-V2X dataset show that V2X-UniPool achieves state-of-the-art planning accuracy and safety while reducing communication cost by more than 80\\%, achieving the lowest overhead among evaluated methods. These results highlight the promise of bridging V2X perception and language reasoning to advance scalable and trustworthy driving. Our code is available at: https://github.com/Xuewen2025/V2X-UniPool",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†V2X-UniPoolï¼Œè¿™æ˜¯é¦–ä¸ªå°†è½¦è”ç½‘(V2X)æ„ŸçŸ¥ä¸åŸºäºè¯­è¨€çš„æ¨ç†ç›¸ç»Ÿä¸€çš„çŸ¥è¯†é©±åŠ¨è‡ªåŠ¨é©¾é©¶(Autonomous Driving)æ¡†æ¶ã€‚é’ˆå¯¹å•è½¦æ„ŸçŸ¥å—é™ä»¥åŠè¯­è¨€æ¨¡å‹éš¾ä»¥å¤„ç†åŸå§‹ä¼ æ„Ÿå™¨æ•°æ®ä¸”æ˜“äº§ç”Ÿå¹»è§‰çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶å°†å¤šæ¨¡æ€V2Xæ•°æ®è½¬åŒ–ä¸ºç»“æ„åŒ–çš„è¯­è¨€çŸ¥è¯†ï¼Œå¹¶ç»„ç»‡åœ¨æ—¶é—´ç´¢å¼•çš„çŸ¥è¯†æ± (knowledge pool)ä¸­ä»¥ç¡®ä¿æ¨ç†çš„æ—¶é—´ä¸€è‡´æ€§ã€‚é€šè¿‡å¼•å…¥æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æŠ€æœ¯ï¼ŒV2X-UniPoolèƒ½å¤Ÿå°†é©¾é©¶å†³ç­–æœ‰æ•ˆé”šå®šåœ¨å®æ—¶ä¸Šä¸‹æ–‡è¯­å¢ƒä¸­ã€‚åœ¨çœŸå®ä¸–ç•ŒDAIR-V2Xæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨å–å¾—æœ€å…ˆè¿›çš„è§„åˆ’å‡†ç¡®æ€§å’Œå®‰å…¨æ€§çš„åŒæ—¶ï¼Œå°†é€šä¿¡æˆæœ¬é™ä½äº†80%ä»¥ä¸Šï¼Œå®ç°äº†åŒç±»æ–¹æ³•ä¸­æœ€ä½çš„å¼€é”€ã€‚è¿™ä¸€ç ”ç©¶æˆæœè¯æ˜äº†é€šè¿‡ç»“åˆV2Xæ„ŸçŸ¥ä¸è¯­è¨€æ¨ç†æ¥æ¨è¿›å¯æ‰©å±•ä¸”å€¼å¾—ä¿¡èµ–çš„è‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02580v2",
      "published_date": "2025-06-03 08:00:57 UTC",
      "updated_date": "2025-10-02 20:56:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:14:59.992789+00:00"
    },
    {
      "arxiv_id": "2506.02576v3",
      "title": "ADFormer: Aggregation Differential Transformer for Passenger Demand Forecasting",
      "title_zh": "ADFormerï¼šé¢å‘å®¢æµéœ€æ±‚é¢„æµ‹çš„èšåˆå·®åˆ† Transformer",
      "authors": [
        "Haichen Wang",
        "Liu Yang",
        "Xinyuan Zhang",
        "Haomin Yu",
        "Ming Li",
        "Jilin Hu"
      ],
      "abstract": "Passenger demand forecasting helps optimize vehicle scheduling, thereby improving urban efficiency. Recently, attention-based methods have been used to adequately capture the dynamic nature of spatio-temporal data. However, existing methods that rely on heuristic masking strategies cannot fully adapt to the complex spatio-temporal correlations, hindering the model from focusing on the right context. These works also overlook the high-level correlations that exist in the real world. Effectively integrating these high-level correlations with the original correlations is crucial. To fill this gap, we propose the Aggregation Differential Transformer (ADFormer), which offers new insights to demand forecasting promotion. Specifically, we utilize Differential Attention to capture the original spatial correlations and achieve attention denoising. Meanwhile, we design distinct aggregation strategies based on the nature of space and time. Then, the original correlations are unified with the high-level correlations, enabling the model to capture holistic spatio-temporal relations. Experiments conducted on taxi and bike datasets confirm the effectiveness and efficiency of our model, demonstrating its practical value. The code is available at https://github.com/decisionintelligence/ADFormer.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ADFormer (Aggregation Differential Transformer)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å®¢è¿éœ€æ±‚é¢„æµ‹æ–¹æ³•ä¸­å¯å‘å¼æ©ç ç­–ç•¥æ— æ³•å®Œå…¨é€‚åº”å¤æ‚æ—¶ç©ºç›¸å…³æ€§ï¼Œä»¥åŠå¿½è§†ç°å®ä¸–ç•Œé«˜é˜¶ç›¸å…³æ€§çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹åˆ©ç”¨ Differential Attention æ¥æ•æ‰åŸå§‹ç©ºé—´ç›¸å…³æ€§å¹¶å®ç°æ³¨æ„åŠ›é™å™ª (attention denoising)ã€‚åŒæ—¶ï¼Œç ”ç©¶æ ¹æ®ç©ºé—´å’Œæ—¶é—´çš„ç‰¹æ€§è®¾è®¡äº†ä¸åŒçš„èšåˆç­–ç•¥ï¼Œä»¥æ•æ‰ç°å®ä¸–ç•Œä¸­çš„é«˜é˜¶ç›¸å…³æ€§ã€‚é€šè¿‡å°†åŸå§‹ç›¸å…³æ€§ä¸é«˜é˜¶ç›¸å…³æ€§ç›¸ç»Ÿä¸€ï¼ŒADFormer èƒ½å¤Ÿæ•è·å…¨å±€çš„æ—¶ç©ºå…³ç³»ã€‚åœ¨å‡ºç§Ÿè½¦å’Œå•è½¦æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†è¯¥æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ï¼Œè¯æ˜äº†å…¶åœ¨ä¼˜åŒ–è½¦è¾†è°ƒåº¦å’Œæå‡åŸå¸‚æ•ˆç‡æ–¹é¢çš„å®é™…åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 5 figures, 3 tables. IJCAI-2025",
      "pdf_url": "https://arxiv.org/pdf/2506.02576v3",
      "published_date": "2025-06-03 07:55:51 UTC",
      "updated_date": "2025-06-05 07:54:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:14:58.188725+00:00"
    },
    {
      "arxiv_id": "2506.02572v1",
      "title": "HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference",
      "title_zh": "HATAï¼šé¢å‘å¯æ‰©å±•å¤§æ¨¡å‹æ¨ç†çš„å¯è®­ç»ƒä¸”ç¡¬ä»¶é«˜æ•ˆçš„å“ˆå¸Œæ„ŸçŸ¥ Top-k æ³¨æ„åŠ›æœºåˆ¶",
      "authors": [
        "Ping Gong",
        "Jiawei Yi",
        "Shengnan Wang",
        "Juncheng Zhang",
        "Zewen Jin",
        "Ouxiang Zhou",
        "Ruibo Liu",
        "Guanbin Xu",
        "Youhui Bai",
        "Bowen Ye",
        "Kun Yuan",
        "Tong Yang",
        "Gong Zhang",
        "Renhai Chen",
        "Feng Wu",
        "Cheng Li"
      ],
      "abstract": "Large Language Models (LLMs) have emerged as a pivotal research area, yet the attention module remains a critical bottleneck in LLM inference, even with techniques like KVCache to mitigate redundant computations. While various top-$k$ attention mechanisms have been proposed to accelerate LLM inference by exploiting the inherent sparsity of attention, they often struggled to strike a balance between efficiency and accuracy. In this paper, we introduce HATA (Hash-Aware Top-$k$ Attention), a novel approach that systematically integrates low-overhead learning-to-hash techniques into the Top-$k$ attention process. Different from the existing top-k attention methods which are devoted to seeking an absolute estimation of qk score, typically with a great cost, HATA maps queries and keys into binary hash codes, and acquires the relative qk score order with a quite low cost, which is sufficient for realizing top-k attention. Extensive experiments demonstrate that HATA achieves up to 7.2$\\times$ speedup compared to vanilla full attention while maintaining model accuracy. In addition, HATA outperforms the state-of-the-art top-$k$ attention methods in both accuracy and efficiency across multiple mainstream LLM models and diverse tasks. HATA is open source at https://github.com/gpzlx1/HATA.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) æ¨ç†è¿‡ç¨‹ä¸­æ³¨æ„åŠ›æœºåˆ¶ (Attention Module) çš„è®¡ç®—ç“¶é¢ˆï¼Œæå‡ºäº† HATA (Hash-Aware Top-k Attention)ï¼Œä¸€ç§å¯è®­ç»ƒä¸”ç¡¬ä»¶é«˜æ•ˆçš„å“ˆå¸Œæ„ŸçŸ¥ Top-k æ³¨æ„åŠ›æœºåˆ¶ã€‚è™½ç„¶ç°æœ‰çš„ Top-k æ³¨æ„åŠ›æ–¹æ³•è¯•å›¾åˆ©ç”¨æ³¨æ„åŠ›çš„ç¨€ç–æ€§ï¼Œä½†å¾€å¾€éš¾ä»¥å…¼é¡¾æ•ˆç‡ä¸å‡†ç¡®ç‡ã€‚HATA åˆ›æ–°åœ°å¼•å…¥äº†ä½å¼€é”€çš„å­¦ä¹ å‹å“ˆå¸Œ (Learning-to-Hash) æŠ€æœ¯ï¼Œå°†æŸ¥è¯¢ (Queries) å’Œé”® (Keys) æ˜ å°„ä¸ºäºŒè¿›åˆ¶å“ˆå¸Œç ï¼Œä»¥æä½çš„æˆæœ¬è·å–ç›¸å¯¹è¯„åˆ†é¡ºåºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„å®Œæ•´æ³¨æ„åŠ›æœºåˆ¶ç›¸æ¯”ï¼ŒHATA åœ¨ä¿æŒæ¨¡å‹å‡†ç¡®ç‡çš„åŒæ—¶å®ç°äº†é«˜è¾¾ 7.2 å€çš„æ¨ç†åŠ é€Ÿã€‚æ­¤å¤–ï¼Œåœ¨å¤šç§ä¸»æµå¤§è¯­è¨€æ¨¡å‹å’Œä¸åŒä»»åŠ¡çš„å¯¹æ¯”ä¸­ï¼ŒHATA åœ¨å‡†ç¡®ç‡å’Œæ•ˆç‡æ–¹é¢å‡è¶…è¶Šäº†ç›®å‰æœ€å…ˆè¿›çš„ Top-k æ³¨æ„åŠ›æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ACL 2025 findings",
      "pdf_url": "https://arxiv.org/pdf/2506.02572v1",
      "published_date": "2025-06-03 07:53:32 UTC",
      "updated_date": "2025-06-03 07:53:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:15:04.986428+00:00"
    },
    {
      "arxiv_id": "2506.02568v1",
      "title": "MLaGA: Multimodal Large Language and Graph Assistant",
      "title_zh": "MLaGAï¼šå¤šæ¨¡æ€å¤§è¯­è¨€ä¸å›¾åŠ©æ‰‹",
      "authors": [
        "Dongzhe Fan",
        "Yi Fang",
        "Jiajin Liu",
        "Djellel Difallah",
        "Qiaoyu Tan"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated substantial efficacy in advancing graph-structured data analysis. Prevailing LLM-based graph methods excel in adapting LLMs to text-rich graphs, wherein node attributes are text descriptions. However, their applications to multimodal graphs--where nodes are associated with diverse attribute types, such as texts and images--remain underexplored, despite their ubiquity in real-world scenarios. To bridge the gap, we introduce the Multimodal Large Language and Graph Assistant (MLaGA), an innovative model that adeptly extends LLM capabilities to facilitate reasoning over complex graph structures and multimodal attributes. We first design a structure-aware multimodal encoder to align textual and visual attributes within a unified space through a joint graph pre-training objective. Subsequently, we implement a multimodal instruction-tuning approach to seamlessly integrate multimodal features and graph structures into the LLM through lightweight projectors. Extensive experiments across multiple datasets demonstrate the effectiveness of MLaGA compared to leading baseline methods, achieving superior performance in diverse graph learning tasks under both supervised and transfer learning scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MLaGA (Multimodal Large Language and Graph Assistant)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ (LLMs) å¤„ç†å…·æœ‰æ–‡æœ¬å’Œå›¾åƒç­‰å¤šæ¨¡æ€å±æ€§å›¾ç»“æ„èƒ½åŠ›çš„æ–°é¢–æ¨¡å‹ã€‚é’ˆå¯¹ç°å®ä¸–ç•Œä¸­å¤šæ¨¡æ€å›¾ (Multimodal Graphs) åˆ†æä¸è¶³çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†ç»“æ„æ„ŸçŸ¥å¤šæ¨¡æ€ç¼–ç å™¨ (Structure-aware Multimodal Encoder)ï¼Œé€šè¿‡è”åˆå›¾é¢„è®­ç»ƒç›®æ ‡åœ¨ç»Ÿä¸€ç©ºé—´ä¸­å¯¹é½æ–‡æœ¬ä¸è§†è§‰å±æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿå®æ–½äº†å¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒ (Multimodal Instruction-tuning) ç­–ç•¥ï¼Œåˆ©ç”¨è½»é‡çº§æŠ•å½±å™¨ (Lightweight Projectors) å°†å¤šæ¨¡æ€ç‰¹å¾ä¸å›¾ç»“æ„é›†æˆåˆ° LLM ä¸­ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMLaGA åœ¨å¤šç§å›¾å­¦ä¹ ä»»åŠ¡çš„ç›‘ç£å­¦ä¹ å’Œè¿ç§»å­¦ä¹  (Transfer Learning) åœºæ™¯ä¸‹å‡è¡¨ç°å‡ºä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹çš„æ€§èƒ½ã€‚è¿™ä¸€æˆæœè¯æ˜äº† MLaGA åœ¨å¤„ç†å¤æ‚å›¾ç»“æ„å’Œå¤šå…ƒå±æ€§æ¨ç†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02568v1",
      "published_date": "2025-06-03 07:52:00 UTC",
      "updated_date": "2025-06-03 07:52:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:15:02.184622+00:00"
    },
    {
      "arxiv_id": "2506.02565v1",
      "title": "Towards Generating Controllable and Solvable Geometry Problem by Leveraging Symbolic Deduction Engine",
      "title_zh": "åˆ©ç”¨ç¬¦å·æ¨æ¼”å¼•æ“ç”Ÿæˆå¯æ§ä¸”å¯è§£çš„å‡ ä½•é—®é¢˜",
      "authors": [
        "Zhuoxuan Jiang",
        "Tianyang Zhang",
        "Peiyan Peng",
        "Jing Chen",
        "Yinong Xun",
        "Haotian Zhang",
        "Lichi Li",
        "Yong Li",
        "Shaohua Zhang"
      ],
      "abstract": "Generating high-quality geometry problems is both an important and challenging task in education. Compared to math word problems, geometry problems further emphasize multi-modal formats and the translation between informal and formal languages. In this paper, we introduce a novel task for geometry problem generation and propose a new pipeline method: the Symbolic Deduction Engine-based Geometry Problem Generation framework (SDE-GPG). The framework leverages a symbolic deduction engine and contains four main steps: (1) searching a predefined mapping table from knowledge points to extended definitions, (2) sampling extended definitions and performing symbolic deduction, (3) filtering out unqualified problems, and (4) generating textual problems and diagrams. Specifically, our method supports to avoid inherent biases in translating natural language into formal language by designing the mapping table, and guarantees to control the generated problems in terms of knowledge points and difficulties by an elaborate checking function. With obtained formal problems, they are translated to natural language and the accompanying diagrams are automatically drew by rule-based methods. We conduct experiments using real-world combinations of knowledge points from two public datasets. The results demonstrate that the SDE-GPG can effectively generate readable, solvable and controllable geometry problems.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹æ•™è‚²é¢†åŸŸä¸­é«˜è´¨é‡å‡ ä½•é¢˜ç›®ç”Ÿæˆçš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åŸºäºç¬¦å·æ¼”ç»å¼•æ“çš„å‡ ä½•é¢˜ç›®ç”Ÿæˆæ¡†æ¶SDE-GPG (Symbolic Deduction Engine-based Geometry Problem Generation framework)ã€‚è¯¥æ¡†æ¶é€šè¿‡æ˜ å°„è¡¨æ£€ç´¢ã€ç¬¦å·æ¼”ç»æ¨ç†ã€é¢˜ç›®è´¨é‡è¿‡æ»¤ä»¥åŠæœ€ç»ˆçš„æ–‡æœ¬ä¸å›¾å½¢ç”Ÿæˆå››ä¸ªæ ¸å¿ƒæ­¥éª¤ï¼Œå®ç°äº†å¤šæ¨¡æ€å‡ ä½•é¢˜ç›®çš„è‡ªåŠ¨åŒ–æ„å»ºã€‚é€šè¿‡è®¾è®¡é¢„å®šä¹‰çš„çŸ¥è¯†ç‚¹æ˜ å°„è¡¨ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆé¿å…äº†è‡ªç„¶è¯­è¨€å‘å½¢å¼åŒ–è¯­è¨€(Formal Language)è½¬æ¢æ—¶çš„å›ºæœ‰åè§ã€‚åˆ©ç”¨ç²¾ç»†çš„æ£€æŸ¥å‡½æ•°(Checking Function)ï¼Œæ¡†æ¶èƒ½æ ¹æ®ç‰¹å®šçŸ¥è¯†ç‚¹å’Œéš¾åº¦æ°´å¹³ç²¾å‡†æ§åˆ¶é¢˜ç›®ç”Ÿæˆè¿‡ç¨‹ã€‚åœ¨è·å¾—å½¢å¼åŒ–é¢˜ç›®åï¼Œç³»ç»Ÿåˆ©ç”¨åŸºäºè§„åˆ™çš„æ–¹æ³•è‡ªåŠ¨äº§å‡ºå¯¹åº”çš„è‡ªç„¶è¯­è¨€æè¿°åŠå‡ ä½•å›¾å½¢ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒSDE-GPGèƒ½å¤Ÿé«˜æ•ˆç”Ÿæˆå…·æœ‰é«˜å¯è¯»æ€§ã€å¯è§£æ€§ä¸”å±æ€§å¯æ§çš„å‡ ä½•é¢˜ç›®ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "To Appear in ACL'25",
      "pdf_url": "https://arxiv.org/pdf/2506.02565v1",
      "published_date": "2025-06-03 07:49:38 UTC",
      "updated_date": "2025-06-03 07:49:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:15:37.539376+00:00"
    },
    {
      "arxiv_id": "2506.02561v1",
      "title": "Pruning General Large Language Models into Customized Expert Models",
      "title_zh": "å°†é€šç”¨å¤§è¯­è¨€æ¨¡å‹å‰ªæä¸ºå®šåˆ¶åŒ–ä¸“å®¶æ¨¡å‹",
      "authors": [
        "Yirao Zhao",
        "Guizhen Chen",
        "Kenji Kawaguchi",
        "Lidong Bing",
        "Wenxuan Zhang"
      ],
      "abstract": "Large language models (LLMs) have revolutionized natural language processing, yet their substantial model sizes often require substantial computational resources. To preserve computing resources and accelerate inference speed, it is crucial to prune redundant parameters, especially for experienced users who often need compact expert models tailored to specific downstream scenarios. However, most existing pruning methods focus on preserving the model's general capabilities, often requiring extensive post-training or suffering from degraded performance due to coarse-grained pruning. In this work, we design a $\\underline{Cus}$tom $\\underline{Prun}$ing method ($\\texttt{Cus-Prun}$) to prune a large general model into a smaller lightweight expert model, which is positioned along the \"language\", \"domain\" and \"task\" dimensions. By identifying and pruning irrelevant neurons of each dimension, $\\texttt{Cus-Prun}$ creates expert models without any post-training. Our experiments demonstrate that $\\texttt{Cus-Prun}$ consistently outperforms other methods, achieving minimal loss in both expert and general capabilities across various models from different model families and sizes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) è®¡ç®—èµ„æºæ¶ˆè€—å·¨å¤§çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º Cus-Prun çš„å®šåˆ¶åŒ–å‰ªæ (Pruning) æ–¹æ³•ï¼Œæ—¨åœ¨å°†å¤§å‹é€šç”¨æ¨¡å‹è½¬åŒ–ä¸ºåœ¨â€œè¯­è¨€â€ã€â€œé¢†åŸŸâ€å’Œâ€œä»»åŠ¡â€ç»´åº¦ä¸Šå®šåˆ¶çš„è½»é‡åŒ–ä¸“å®¶æ¨¡å‹ã€‚ä¸ç°æœ‰ä¾§é‡ä¿ç•™é€šç”¨èƒ½åŠ›ä¸”é€šå¸¸éœ€è¦å¤§é‡åè®­ç»ƒ (Post-training) çš„æ–¹æ³•ä¸åŒï¼ŒCus-Prun é€šè¿‡è¯†åˆ«å¹¶å‰ªé™¤ç‰¹å®šç»´åº¦ä¸­çš„ä¸ç›¸å…³ç¥ç»å…ƒ (Neurons)ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€ä»»ä½•åè®­ç»ƒçš„æƒ…å†µä¸‹åˆ›å»ºä¸“å®¶æ¨¡å‹ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒCus-Prun åœ¨ä¸åŒå®¶æ—å’Œè§„æ¨¡çš„æ¨¡å‹ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œå…¶æ€§èƒ½ä¸€è‡´ä¼˜äºç°æœ‰ä¸»æµæ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨æœ‰æ•ˆå‡å°‘æ¨¡å‹å‚æ•°é‡çš„åŒæ—¶ï¼Œå®ç°äº†ä¸“å®¶èƒ½åŠ›ä¸é€šç”¨èƒ½åŠ›æŸå¤±çš„æå°åŒ–ï¼Œä¸ºç‰¹å®šåœºæ™¯ä¸‹çš„æ¨¡å‹éƒ¨ç½²æä¾›äº†é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02561v1",
      "published_date": "2025-06-03 07:47:30 UTC",
      "updated_date": "2025-06-03 07:47:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:15:33.991412+00:00"
    },
    {
      "arxiv_id": "2506.02554v1",
      "title": "HiLO: High-Level Object Fusion for Autonomous Driving using Transformers",
      "title_zh": "HiLOï¼šåŸºäº Transformer çš„è‡ªåŠ¨é©¾é©¶é«˜çº§åˆ«ç›®æ ‡èåˆ",
      "authors": [
        "Timo Osterburg",
        "Franz Albers",
        "Christopher Diehl",
        "Rajesh Pushparaj",
        "Torsten Bertram"
      ],
      "abstract": "The fusion of sensor data is essential for a robust perception of the environment in autonomous driving. Learning-based fusion approaches mainly use feature-level fusion to achieve high performance, but their complexity and hardware requirements limit their applicability in near-production vehicles. High-level fusion methods offer robustness with lower computational requirements. Traditional methods, such as the Kalman filter, dominate this area. This paper modifies the Adapted Kalman Filter (AKF) and proposes a novel transformer-based high-level object fusion method called HiLO. Experimental results demonstrate improvements of $25.9$ percentage points in $\\textrm{F}_1$ score and $6.1$ percentage points in mean IoU. Evaluation on a new large-scale real-world dataset demonstrates the effectiveness of the proposed approaches. Their generalizability is further validated by cross-domain evaluation between urban and highway scenarios. Code, data, and models are available at https://github.com/rst-tu-dortmund/HiLO .",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºè‡ªåŠ¨é©¾é©¶ä¸­çš„ä¼ æ„Ÿå™¨èåˆå¯¹äºç¯å¢ƒæ„ŸçŸ¥è‡³å…³é‡è¦ï¼Œä½†ä¼ ç»Ÿçš„å­¦ä¹ å‹ç‰¹å¾çº§èåˆ(feature-level fusion)ç”±äºé«˜å¤æ‚åº¦å’Œç¡¬ä»¶éœ€æ±‚é™åˆ¶äº†å…¶åœ¨å‡†é‡äº§è½¦å‹ä¸­çš„åº”ç”¨ã€‚é«˜çº§èåˆ(High-level fusion)æ–¹æ³•è™½ç„¶è®¡ç®—è¦æ±‚ä½ä¸”é²æ£’æ€§å¼ºï¼Œä½†ç›®å‰ä»ç”± Kalman filter ç­‰ä¼ ç»Ÿæ–¹æ³•ä¸»å¯¼ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æ”¹è¿›äº† Adapted Kalman Filter (AKF)ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäº Transformer çš„æ–°å‹é«˜çº§ç‰©ä½“èåˆæ–¹æ³• HiLOã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHiLO åœ¨ F1 score ä¸Šæå‡äº† 25.9 ä¸ªç™¾åˆ†ç‚¹ï¼Œåœ¨ mean IoU ä¸Šæå‡äº† 6.1 ä¸ªç™¾åˆ†ç‚¹ã€‚è¯¥ç ”ç©¶é€šè¿‡å¤§è§„æ¨¡çœŸå®ä¸–ç•Œæ•°æ®é›†éªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶åˆ©ç”¨åŸå¸‚ä¸é«˜é€Ÿåœºæ™¯é—´çš„è·¨åŸŸè¯„ä¼°(cross-domain evaluation)è¿›ä¸€æ­¥è¯æ˜äº†å…¶è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "6 pages, accepted at IEEE Intelligent Vehicles Symposium (IV) 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.02554v1",
      "published_date": "2025-06-03 07:44:35 UTC",
      "updated_date": "2025-06-03 07:44:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:15:37.035376+00:00"
    },
    {
      "arxiv_id": "2506.02553v1",
      "title": "Response-Level Rewards Are All You Need for Online Reinforcement Learning in LLMs: A Mathematical Perspective",
      "title_zh": "å“åº”çº§å¥–åŠ±è¶³çŸ£ï¼šå¤§è¯­è¨€æ¨¡å‹åœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„æ•°å­¦è§†è§’",
      "authors": [
        "Shenghua He",
        "Tian Xia",
        "Xuan Zhou",
        "Hui Wei"
      ],
      "abstract": "We study a common challenge in reinforcement learning for large language models (LLMs): the Zero-Reward Assumption, where non-terminal actions (i.e., intermediate token generations) receive zero task-specific immediate reward, while only the final token receives a reward for the entire response. This assumption arises frequently in practice, as precise token-level rewards are often difficult or infeasible to obtain in LLM applications. In this work, we provide a unifying theoretical perspective. We introduce the Trajectory Policy Gradient Theorem, which shows that the policy gradient based on true, unknown token-level rewards can be unbiasedly estimated using only a response-level reward model, regardless of whether the Zero-Reward Assumption holds or not, for algorithms in the REINFORCE and Actor-Critic families. This result reveals that widely used methods such as PPO, GRPO, ReMax, and RLOO inherently possess the capacity to model token-level reward signals, offering a theoretical justification for response-level reward approaches. Our findings pave the way for more practical, efficient LLM fine-tuning, allowing developers to treat training algorithms as black boxes and focus on improving the response-level reward model with auxiliary sub-models. We also offer a detailed analysis of popular RL and non-RL methods, comparing their theoretical foundations and practical advantages across common LLM tasks. Finally, we propose a new algorithm: Token-Reinforced Policy Optimization (TRePO), a theoretically grounded method that is simpler than PPO, matches GRPO in memory efficiency, and holds promise for broad applicability.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å¼ºåŒ–å­¦ä¹ ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œå³ç”±äºéš¾ä»¥è·å–Tokençº§å¥–åŠ±è€Œæ™®éå­˜åœ¨çš„é›¶å¥–åŠ±å‡è®¾(Zero-Reward Assumption)ã€‚ä½œè€…æå‡ºäº†è½¨è¿¹ç­–ç•¥æ¢¯åº¦å®šç†(Trajectory Policy Gradient Theorem)ï¼Œè¯æ˜äº†å¯¹äºREINFORCEå’ŒActor-Criticç³»åˆ—ç®—æ³•ï¼Œä»…åˆ©ç”¨å“åº”çº§å¥–åŠ±(Response-Level Reward)æ¨¡å‹å³å¯æ— ååœ°ä¼°è®¡å‡ºåŸºäºçœŸå®Tokençº§å¥–åŠ±çš„ç­–ç•¥æ¢¯åº¦ã€‚è¿™ä¸€ç»“è®ºä¸ºPPOã€GRPOã€ReMaxå’ŒRLOOç­‰å¹¿æ³›ä½¿ç”¨çš„æ–¹æ³•æä¾›äº†ç†è®ºæ”¯æŒï¼Œè¯æ˜å®ƒä»¬æœ¬è´¨ä¸Šå…·å¤‡å»ºæ¨¡Tokençº§å¥–åŠ±ä¿¡å·çš„èƒ½åŠ›ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè¿™ä¸€å‘ç°å…è®¸å¼€å‘è€…å°†è®­ç»ƒç®—æ³•è§†ä¸ºé»‘ç›’ï¼Œè½¬è€Œä¸“æ³¨äºé€šè¿‡è¾…åŠ©å­æ¨¡å‹ä¼˜åŒ–å“åº”çº§å¥–åŠ±æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè®ºæ–‡æ·±å…¥å¯¹æ¯”äº†å¤šç§ä¸»æµå¼ºåŒ–å­¦ä¹ ä¸éå¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨LLMä»»åŠ¡ä¸­çš„ç†è®ºåŸºç¡€ä¸å®è·µä¼˜åŠ£ã€‚æœ€åï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°ç®—æ³•Token-Reinforced Policy Optimization (TRePO)ï¼Œå…¶æ¯”PPOæ›´ç®€æ´ä¸”åœ¨å†…å­˜æ•ˆç‡ä¸Šä¸GRPOæŒå¹³ï¼Œå±•ç°äº†å¹¿é˜”çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02553v1",
      "published_date": "2025-06-03 07:44:31 UTC",
      "updated_date": "2025-06-03 07:44:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:15:55.999808+00:00"
    },
    {
      "arxiv_id": "2506.02550v2",
      "title": "Technical Report for Ego4D Long-Term Action Anticipation Challenge 2025",
      "title_zh": "2025å¹´Ego4Dé•¿æœŸåŠ¨ä½œé¢„æµ‹æŒ‘æˆ˜èµ›æŠ€æœ¯æŠ¥å‘Š",
      "authors": [
        "Qiaohui Chu",
        "Haoyu Zhang",
        "Yisen Feng",
        "Meng Liu",
        "Weili Guan",
        "Yaowei Wang",
        "Liqiang Nie"
      ],
      "abstract": "In this report, we present a novel three-stage framework developed for the Ego4D Long-Term Action Anticipation (LTA) task. Inspired by recent advances in foundation models, our method consists of three stages: feature extraction, action recognition, and long-term action anticipation. First, visual features are extracted using a high-performance visual encoder. The features are then fed into a Transformer to predict verbs and nouns, with a verb-noun co-occurrence matrix incorporated to enhance recognition accuracy. Finally, the predicted verb-noun pairs are formatted as textual prompts and input into a fine-tuned large language model (LLM) to anticipate future action sequences. Our framework achieves first place in this challenge at CVPR 2025, establishing a new state-of-the-art in long-term action prediction. Our code will be released at https://github.com/CorrineQiu/Ego4D-LTA-Challenge-2025.",
      "tldr_zh": "è¯¥æŠ¥å‘Šä»‹ç»äº†ä¸€ç§ä¸ºEgo4Dé•¿æœŸåŠ¨ä½œé¢„æµ‹(Long-Term Action Anticipation, LTA)ä»»åŠ¡å¼€å‘çš„æ–°å‹ä¸‰é˜¶æ®µæ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆåˆ©ç”¨é«˜æ€§èƒ½è§†è§‰ç¼–ç å™¨è¿›è¡Œç‰¹å¾æå–ï¼Œéšåå°†ç‰¹å¾è¾“å…¥Transformerè¿›è¡ŒåŠ¨è¯å’Œåè¯é¢„æµ‹ã€‚ä¸ºäº†æé«˜è¯†åˆ«å‡†ç¡®æ€§ï¼Œç ”ç©¶å›¢é˜Ÿåœ¨è¯†åˆ«è¿‡ç¨‹ä¸­å¼•å…¥äº†åŠ¨è¯-åè¯å…±ç°çŸ©é˜µ(verb-noun co-occurrence matrix)ä»¥å¢å¼ºç‰¹å¾å…³è”ã€‚åœ¨æœ€åçš„é¢„æµ‹é˜¶æ®µï¼Œç³»ç»Ÿå°†é¢„æµ‹çš„åŠ¨è¯-åè¯å¯¹è½¬åŒ–ä¸ºæ–‡æœ¬æç¤º(textual prompts)ï¼Œå¹¶è¾“å…¥ç»è¿‡å¾®è°ƒçš„å¤§è¯­è¨€æ¨¡å‹(LLM)ä»¥é¢„æµ‹æœªæ¥çš„åŠ¨ä½œåºåˆ—ã€‚è¯¥æ–¹æ³•åœ¨CVPR 2025çš„æŒ‘æˆ˜èµ›ä¸­è·å¾—ç¬¬ä¸€åï¼Œå¹¶åˆ·æ–°äº†é•¿æœŸåŠ¨ä½œé¢„æµ‹é¢†åŸŸçš„æœ€å…ˆè¿›(state-of-the-art)æ°´å¹³ã€‚ç›¸å…³ä»£ç å·²åœ¨GitHubå¼€æºä¾›å­¦æœ¯ç•Œå‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "The champion solution for the Ego4D Long-Term Action Anticipation Challenge at the CVPR EgoVis Workshop 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.02550v2",
      "published_date": "2025-06-03 07:36:52 UTC",
      "updated_date": "2025-06-11 11:16:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:15:38.234040+00:00"
    },
    {
      "arxiv_id": "2506.02548v2",
      "title": "CyberGym: Evaluating AI Agents' Real-World Cybersecurity Capabilities at Scale",
      "title_zh": "CyberGymï¼šå¤§è§„æ¨¡è¯„ä¼° AI æ™ºèƒ½ä½“çš„çœŸå®ä¸–ç•Œç½‘ç»œå®‰å…¨èƒ½åŠ›",
      "authors": [
        "Zhun Wang",
        "Tianneng Shi",
        "Jingxuan He",
        "Matthew Cai",
        "Jialin Zhang",
        "Dawn Song"
      ],
      "abstract": "AI agents have significant potential to reshape cybersecurity, making a thorough assessment of their capabilities critical. However, existing evaluations fall short, because they are based on small-scale benchmarks and only measure static outcomes, failing to capture the full, dynamic range of real-world security challenges. To address these limitations, we introduce CyberGym, a large-scale benchmark featuring 1,507 real-world vulnerabilities across 188 software projects. Adjustable to different vulnerability analysis settings, CyberGym primarily tasks agents with generating a proof-of-concept test that reproduces a vulnerability, given only its text description and the corresponding codebase. Our extensive evaluation highlights that CyberGym effectively differentiates agents' and models' cybersecurity capabilities. Even the top-performing combinations only achieve a ~20% success rate, demonstrating the overall difficulty of CyberGym. Beyond static benchmarking, we show that CyberGym leads to the discovery of 35 zero-day vulnerabilities and 17 historically incomplete patches. These results underscore that CyberGym is not only a robust benchmark for measuring AI's progress in cybersecurity but also a platform for creating direct, real-world security impact.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CyberGymï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼° AI agents åœ¨çœŸå®ç½‘ç»œå®‰å…¨ç¯å¢ƒä¸‹å®æˆ˜èƒ½åŠ›çš„å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•å¹³å°ï¼Œå¡«è¡¥äº†ç°æœ‰è¯„ä¼°åœ¨è§„æ¨¡å’ŒåŠ¨æ€æ€§ä¸Šçš„ä¸è¶³ã€‚CyberGym åŒ…å«è·¨ 188 ä¸ªè½¯ä»¶é¡¹ç›®çš„ 1,507 ä¸ªçœŸå®æ¼æ´ï¼Œæ ¸å¿ƒä»»åŠ¡æ˜¯è®©æ™ºèƒ½ä½“ä»…æ ¹æ®æ–‡æœ¬æè¿°å’Œä»£ç åº“ç”Ÿæˆç”¨äºå¤ç°æ¼æ´çš„ proof-of-concept æµ‹è¯•ã€‚å¹¿æ³›çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œé¡¶å°–æ¨¡å‹ç»„åˆçš„æˆåŠŸç‡ä¹Ÿä»…çº¦ä¸º 20%ï¼Œå‡¸æ˜¾äº†è¯¥åŸºå‡†åœ¨åŒºåˆ†ä¸åŒæ™ºèƒ½ä½“å®‰å…¨èƒ½åŠ›æ–¹é¢çš„é«˜æŒ‘æˆ˜æ€§ã€‚é™¤äº†ä½œä¸ºåŸºå‡†æµ‹è¯•ï¼ŒCyberGym è¿˜ç›´æ¥ä¿ƒè¿›äº† 35 ä¸ª zero-day vulnerabilities å’Œ 17 ä¸ª historically incomplete patches çš„å‘ç°ï¼Œè¯æ˜äº†å…¶åœ¨æ¨åŠ¨ AI å®‰å…¨ç ”ç©¶çš„åŒæ—¶å…·å¤‡æ˜¾è‘—çš„ç°å®å®‰å…¨è´¡çŒ®ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02548v2",
      "published_date": "2025-06-03 07:35:14 UTC",
      "updated_date": "2025-10-08 06:32:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:16:08.334762+00:00"
    },
    {
      "arxiv_id": "2506.02544v2",
      "title": "CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG",
      "title_zh": "CoRe-MMRAGï¼šé¢å‘å¤šæ¨¡æ€ RAG çš„è·¨æºçŸ¥è¯†åè°ƒ",
      "authors": [
        "Yang Tian",
        "Fan Liu",
        "Jingyuan Zhang",
        "Victoria W.",
        "Yupeng Hu",
        "Liqiang Nie"
      ],
      "abstract": "Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to enhance Multimodal Large Language Models by incorporating externally retrieved multimodal knowledge, but it introduces two challenges: Parametric-Retrieved Knowledge Inconsistency (PRKI), where discrepancies between parametric and retrieved knowledge create uncertainty in determining reliability, and Visual-Textual Knowledge Inconsistency (VTKI), where misalignment between visual and textual sources disrupts entity representation. To address these challenges, we propose Cross-source knowledge \\textbf{Re}conciliation for Multimodal RAG (CoRe-MMRAG), a novel end-to-end framework that effectively reconciles inconsistencies across knowledge sources. CoRe-MMRAG follows a four-stage pipeline: it first generates an internal response from parametric knowledge, then selects the most relevant multimodal evidence via joint similarity assessment, generates an external response, and finally integrates both to produce a reliable answer. Additionally, a specialized training paradigm enhances knowledge source discrimination, multimodal integration, and unified answer generation. Experiments on KB-VQA benchmarks show that CoRe-MMRAG achieves substantial improvements over baseline methods, achieving 5.6% and 9.3% performance gains on InfoSeek and Encyclopedic-VQA, respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆMultimodal Retrieval-Augmented Generation, MMRAGï¼‰ä¸­å­˜åœ¨çš„å‚æ•°-æ£€ç´¢çŸ¥è¯†ä¸ä¸€è‡´ï¼ˆParametric-Retrieved Knowledge Inconsistency, PRKIï¼‰ä»¥åŠè§†è§‰-æ–‡æœ¬çŸ¥è¯†ä¸ä¸€è‡´ï¼ˆVisual-Textual Knowledge Inconsistency, VTKIï¼‰ä¸¤å¤§æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º CoRe-MMRAG çš„æ–°å‹ç«¯åˆ°ç«¯æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†å››é˜¶æ®µæµæ°´çº¿ï¼Œé¦–å…ˆä»å‚æ•°åŒ–çŸ¥è¯†ä¸­ç”Ÿæˆå†…éƒ¨å“åº”ï¼Œæ¥ç€é€šè¿‡è”åˆç›¸ä¼¼åº¦è¯„ä¼°ç­›é€‰æœ€ç›¸å…³çš„å¤šæ¨¡æ€è¯æ®å¹¶ç”Ÿæˆå¤–éƒ¨å“åº”ï¼Œæœ€åå°†ä¸¤è€…æ•´åˆä»¥äº§ç”Ÿå¯é ç­”æ¡ˆã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä¸“é—¨çš„è®­ç»ƒèŒƒå¼ï¼Œæ—¨åœ¨æå‡æ¨¡å‹å¯¹çŸ¥è¯†æºçš„è¾¨åˆ«èƒ½åŠ›ã€å¤šæ¨¡æ€æ•´åˆæ•ˆæœä»¥åŠç»Ÿä¸€çš„ç­”æ¡ˆç”Ÿæˆè´¨é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCoRe-MMRAG åœ¨ KB-VQA åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œåœ¨ InfoSeek å’Œ Encyclopedic-VQA æ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº† 5.6% å’Œ 9.3% çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨åè°ƒè·¨æºçŸ¥è¯†å†²çªæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ACL 2025 Main",
      "pdf_url": "https://arxiv.org/pdf/2506.02544v2",
      "published_date": "2025-06-03 07:32:40 UTC",
      "updated_date": "2025-06-04 06:31:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:15:50.423615+00:00"
    },
    {
      "arxiv_id": "2506.02542v1",
      "title": "HIEGNet: A Heterogenous Graph Neural Network Including the Immune Environment in Glomeruli Classification",
      "title_zh": "HIEGNetï¼šèåˆå…ç–«ç¯å¢ƒçš„å¼‚æ„å›¾ç¥ç»ç½‘ç»œè‚¾å°çƒåˆ†ç±»æ¨¡å‹",
      "authors": [
        "Niklas Kormann",
        "Masoud Ramuz",
        "Zeeshan Nisar",
        "Nadine S. Schaadt",
        "Hendrik Annuth",
        "Benjamin Doerr",
        "Friedrich Feuerhake",
        "Thomas Lampert",
        "Johannes F. Lutzeyer"
      ],
      "abstract": "Graph Neural Networks (GNNs) have recently been found to excel in histopathology. However, an important histopathological task, where GNNs have not been extensively explored, is the classification of glomeruli health as an important indicator in nephropathology. This task presents unique difficulties, particularly for the graph construction, i.e., the identification of nodes, edges, and informative features. In this work, we propose a pipeline composed of different traditional and machine learning-based computer vision techniques to identify nodes, edges, and their corresponding features to form a heterogeneous graph. We then proceed to propose a novel heterogeneous GNN architecture for glomeruli classification, called HIEGNet, that integrates both glomeruli and their surrounding immune cells. Hence, HIEGNet is able to consider the immune environment of each glomerulus in its classification. Our HIEGNet was trained and tested on a dataset of Whole Slide Images from kidney transplant patients. Experimental results demonstrate that HIEGNet outperforms several baseline models and generalises best between patients among all baseline models. Our implementation is publicly available at https://github.com/nklsKrmnn/HIEGNet.git.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HIEGNetï¼Œä¸€ç§ä¸“é—¨ç”¨äºè‚¾å°çƒ(glomeruli)åˆ†ç±»çš„æ–°å‹å¼‚æ„å›¾ç¥ç»ç½‘ç»œ(Heterogenous Graph Neural Network)æ¶æ„ã€‚ä¸ºäº†è§£å†³è‚¾å°çƒåˆ†ç±»ä»»åŠ¡ä¸­å›¾æ„å»ºçš„å¤æ‚æ€§ï¼Œç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†ä¸€å¥—ç»“åˆè®¡ç®—æœºè§†è§‰æŠ€æœ¯çš„æµæ°´çº¿ï¼Œç”¨äºç²¾å‡†è¯†åˆ«èŠ‚ç‚¹ã€è¾¹ç¼˜åŠå…¶ç‰¹å¾ã€‚HIEGNetçš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºå®ƒå°†è‚¾å°çƒä¸å…¶å‘¨å›´çš„å…ç–«ç»†èƒå…±åŒæ•´åˆåˆ°å¼‚æ„å›¾ä¸­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨åˆ†ç±»æ—¶å……åˆ†è€ƒè™‘æ¯ä¸ªè‚¾å°çƒçš„å…ç–«ç¯å¢ƒã€‚å®éªŒåŸºäºè‚¾ç§»æ¤æ‚£è€…çš„å…¨æ‰«ææ•°å­—åŒ–åˆ‡ç‰‡(Whole Slide Images)æ•°æ®é›†å±•å¼€ã€‚ç»“æœè¡¨æ˜ï¼ŒHIEGNetåœ¨åˆ†ç±»æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå¤šä¸ªåŸºçº¿æ¨¡å‹ï¼Œå¹¶åœ¨ä¸åŒæ‚£è€…ä¹‹é—´è¡¨ç°å‡ºæœ€å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥ç ”ç©¶è¯æ˜äº†æ•´åˆå‘¨å›´å¾®ç¯å¢ƒä¿¡æ¯å¯¹äºæå‡ç—…ç†å›¾åƒåˆ†æå‡†ç¡®æ€§çš„é‡è¦ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted for poster presentation at MIDL 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.02542v1",
      "published_date": "2025-06-03 07:28:25 UTC",
      "updated_date": "2025-06-03 07:28:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:16:07.307346+00:00"
    },
    {
      "arxiv_id": "2506.02541v1",
      "title": "Rethinking Post-Unlearning Behavior of Large Vision-Language Models",
      "title_zh": "é‡æ–°æ€è€ƒå¤§è§†è§‰è¯­è¨€æ¨¡å‹çš„æœºå™¨é—å¿˜åè¡Œä¸º",
      "authors": [
        "Minsung Kim",
        "Nakyeong Yang",
        "Kyomin Jung"
      ],
      "abstract": "Machine unlearning is used to mitigate the privacy risks of Large Vision-Language Models (LVLMs) arising from training on large-scale web data. However, existing unlearning methods often fail to carefully select substitute outputs for forget targets, resulting in Unlearning Aftermaths-undesirable behaviors such as degenerate, hallucinated, or excessively refused responses. We highlight that, especially for generative LVLMs, it is crucial to consider the quality and informativeness of post-unlearning responses rather than relying solely on naive suppression. To address this, we introduce a new unlearning task for LVLMs that requires models to provide privacy-preserving yet informative and visually grounded responses. We also propose PUBG, a novel unlearning method that explicitly guides post-unlearning behavior toward a desirable output distribution. Experiments show that, while existing methods suffer from Unlearning Aftermaths despite successfully preventing privacy violations, PUBG effectively mitigates these issues, generating visually grounded and informative responses without privacy leakage for forgotten targets.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Large Vision-Language Models (LVLMs) ä¸­çš„ Machine unlearning æŠ€æœ¯ï¼Œæ—¨åœ¨é™ä½æ¨¡å‹åœ¨æµ·é‡æ•°æ®è®­ç»ƒä¸­äº§ç”Ÿçš„éšç§é£é™©ã€‚ä½œè€…æŒ‡å‡ºï¼Œç°æœ‰çš„å¸è½½æ–¹æ³•ç”±äºå¿½ç•¥äº†å¯¹æ›¿ä»£è¾“å‡ºçš„ç²¾ç»†é€‰æ‹©ï¼Œå¸¸å¯¼è‡´æ¨¡å‹å‡ºç°é€€åŒ–ã€å¹»è§‰æˆ–è¿‡åº¦æ‹’ç»ç­‰ Unlearning Aftermaths ç°è±¡ã€‚é’ˆå¯¹ç”Ÿæˆå¼æ¨¡å‹ï¼Œç ”ç©¶å¼ºè°ƒäº†å¸è½½åå“åº”è´¨é‡çš„é‡è¦æ€§ï¼Œå¹¶æå‡ºäº†ä¸€é¡¹å…¼é¡¾éšç§ä¿æŠ¤ã€Visually grounded å±æ€§åŠä¿¡æ¯ä¸°å¯Œåº¦çš„æ–°å‹å¸è½½ä»»åŠ¡ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†åä¸º PUBG çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡æ˜¾å¼å¼•å¯¼å°†æ¨¡å‹å¸è½½åçš„è¡Œä¸ºä¿®æ­£è‡³ç†æƒ³çš„è¾“å‡ºåˆ†å¸ƒã€‚å®éªŒè¯æ˜ï¼ŒPUBG åœ¨æˆåŠŸé˜²æ­¢éšç§æ³„éœ²çš„åŒæ—¶ï¼Œæœ‰æ•ˆè§£å†³äº†ç°æœ‰æ–¹æ³•æ™®éé¢ä¸´çš„ Unlearning Aftermaths é—®é¢˜ã€‚è¯¥æ–¹æ³•ç¡®ä¿äº†æ¨¡å‹åœ¨å¤„ç†è¢«é—å¿˜ç›®æ ‡æ—¶ï¼Œä¾ç„¶èƒ½æä¾›å‡†ç¡®ä¸”å…·æœ‰è§†è§‰ä¾æ®çš„æœ‰ç”¨ä¿¡æ¯ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„å¯é æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.02541v1",
      "published_date": "2025-06-03 07:28:22 UTC",
      "updated_date": "2025-06-03 07:28:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:16:12.798533+00:00"
    },
    {
      "arxiv_id": "2506.17234v1",
      "title": "Graph Neural Networks in Multi-Omics Cancer Research: A Structured Survey",
      "title_zh": "å›¾ç¥ç»ç½‘ç»œåœ¨å¤šç»„å­¦ç™Œç—‡ç ”ç©¶ä¸­çš„åº”ç”¨ï¼šç»“æ„åŒ–ç»¼è¿°",
      "authors": [
        "Payam Zohari",
        "Mostafa Haghir Chehreghani"
      ],
      "abstract": "The task of data integration for multi-omics data has emerged as a powerful strategy to unravel the complex biological underpinnings of cancer. Recent advancements in graph neural networks (GNNs) offer an effective framework to model heterogeneous and structured omics data, enabling precise representation of molecular interactions and regulatory networks. This systematic review explores several recent studies that leverage GNN-based architectures in multi-omics cancer research. We classify the approaches based on their targeted omics layers, graph neural network structures, and biological tasks such as subtype classification, prognosis prediction, and biomarker discovery. The analysis reveals a growing trend toward hybrid and interpretable models, alongside increasing adoption of attention mechanisms and contrastive learning. Furthermore, we highlight the use of patient-specific graphs and knowledge-driven priors as emerging directions. This survey serves as a comprehensive resource for researchers aiming to design effective GNN-based pipelines for integrative cancer analysis, offering insights into current practices, limitations, and potential future directions.",
      "tldr_zh": "è¿™é¡¹ç³»ç»Ÿç»¼è¿°æ¢è®¨äº†å›¾ç¥ç»ç½‘ç»œ(Graph Neural Networks, GNNs)åœ¨å¤šç»„å­¦(Multi-omics)ç™Œç—‡ç ”ç©¶ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨åˆ©ç”¨å…¶å¼ºå¤§çš„å»ºæ¨¡èƒ½åŠ›å¤„ç†å¼‚æ„ä¸”ç»“æ„åŒ–çš„ç»„å­¦æ•°æ®å¹¶è¡¨å¾åˆ†å­ç›¸äº’ä½œç”¨ã€‚æ–‡ç« ä¾æ®ç›®æ ‡ç»„å­¦å±‚ã€GNN æ¶æ„ä»¥åŠäºšå‹åˆ†ç±»(Subtype Classification)ã€é¢„åé¢„æµ‹(Prognosis Prediction)å’Œç”Ÿç‰©æ ‡å¿—ç‰©å‘ç°(Biomarker Discovery)ç­‰ç”Ÿç‰©å­¦ä»»åŠ¡å¯¹ç°æœ‰ç ”ç©¶è¿›è¡Œäº†ç³»ç»Ÿåˆ†ç±»ã€‚åˆ†ææ˜¾ç¤ºï¼Œè¯¥é¢†åŸŸæ­£è¶‹å‘äºé‡‡ç”¨æ··åˆæ¨¡å‹(Hybrid Models)ã€å¯è§£é‡Šæ¨¡å‹(Interpretable Models)ä»¥åŠæ³¨æ„åŠ›æœºåˆ¶(Attention Mechanisms)å’Œå¯¹æ¯”å­¦ä¹ (Contrastive Learning)ç­‰å…ˆè¿›æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼ºè°ƒäº†æ‚£è€…ç‰¹å¼‚æ€§å›¾(Patient-specific Graphs)å’ŒçŸ¥è¯†é©±åŠ¨å…ˆéªŒ(Knowledge-driven Priors)ä½œä¸ºé›†æˆç™Œç—‡åˆ†æçš„é‡è¦æ–°å…´æ–¹å‘ã€‚è¯¥ç»¼è¿°ä¸ºç ”ç©¶äººå‘˜è®¾è®¡æœ‰æ•ˆçš„ GNN æµç¨‹æä¾›äº†å…¨é¢èµ„æºï¼Œæ·±å…¥è§£æäº†å½“å‰çš„å®è·µç°çŠ¶ã€æŠ€æœ¯å±€é™åŠæœªæ¥å‘å±•è¶‹åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "51 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.17234v1",
      "published_date": "2025-06-03 07:28:02 UTC",
      "updated_date": "2025-06-03 07:28:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:16:05.281429+00:00"
    },
    {
      "arxiv_id": "2506.02537v3",
      "title": "VisuRiddles: Fine-grained Perception is a Primary Bottleneck for Multimodal Large Language Models in Abstract Visual Reasoning",
      "title_zh": "VisuRiddlesï¼šç»†ç²’åº¦æ„ŸçŸ¥æ˜¯å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æŠ½è±¡è§†è§‰æ¨ç†çš„ä¸»è¦ç“¶é¢ˆ",
      "authors": [
        "Hao Yan",
        "Xingchen Liu",
        "Hao Wang",
        "Zhenbiao Cao",
        "Handong Zheng",
        "Liang Yin",
        "Xinxing Su",
        "Zihao Chen",
        "Jihao Wu",
        "Minghui Liao",
        "Chao Weng",
        "Wei Chen",
        "Yuliang Liu",
        "Xiang Bai"
      ],
      "abstract": "Recent strides in multimodal large language models (MLLMs) have significantly advanced their performance in many reasoning tasks. However, Abstract Visual Reasoning (AVR) remains a critical challenge, primarily due to limitations in perceiving abstract graphics. To tackle this issue, we investigate the bottlenecks in current MLLMs and synthesize training data to improve their abstract visual perception. First, we propose VisuRiddles, a benchmark for AVR, featuring tasks meticulously constructed to assess models' reasoning capacities across five core dimensions and two high-level reasoning categories. Second, we introduce the Perceptual Riddle Synthesizer (PRS), an automated framework for generating riddles with fine-grained perceptual descriptions. PRS not only generates valuable training data for abstract graphics but also provides fine-grained perceptual description, crucially allowing for supervision over intermediate reasoning stages and thereby improving both training efficacy and model interpretability. Our extensive experimental results on VisuRiddles empirically validate that fine-grained visual perception is the principal bottleneck and our synthesis framework markedly enhances the performance of contemporary MLLMs on these challenging tasks. Our code and dataset will be released at https://github.com/yh-hust/VisuRiddles",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)åœ¨æŠ½è±¡è§†è§‰æ¨ç†(Abstract Visual Reasoning, AVR)ä»»åŠ¡ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºç»†ç²’åº¦æ„ŸçŸ¥èƒ½åŠ›æ˜¯é™åˆ¶å…¶æ€§èƒ½çš„ä¸»è¦ç“¶é¢ˆã€‚ä½œè€…æå‡ºäº†VisuRiddlesè¯„æµ‹åŸºå‡†ï¼Œæ—¨åœ¨ä»äº”ä¸ªæ ¸å¿ƒç»´åº¦å’Œä¸¤ä¸ªé«˜å±‚æ¨ç†ç±»åˆ«å…¨é¢è¯„ä¼°æ¨¡å‹çš„æŠ½è±¡æ¨ç†èƒ½åŠ›ã€‚åŒæ—¶ï¼Œç ”ç©¶å¼•å…¥äº†æ„ŸçŸ¥è°œé¢˜åˆæˆå™¨(Perceptual Riddle Synthesizer, PRS)ï¼Œè¿™æ˜¯ä¸€ä¸ªèƒ½è‡ªåŠ¨ç”Ÿæˆå¸¦æœ‰ç»†ç²’åº¦æ„ŸçŸ¥æè¿°çš„è°œé¢˜æ¡†æ¶ï¼Œç”¨äºåˆæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚PRSä¸ä»…èƒ½ç”ŸæˆæŠ½è±¡å›¾å½¢ï¼Œè¿˜é€šè¿‡æä¾›ç»†ç²’åº¦æè¿°å®ç°äº†å¯¹ä¸­é—´æ¨ç†é˜¶æ®µçš„ç›‘ç£ï¼Œä»è€Œæå‡äº†è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹çš„å¯è§£é‡Šæ€§(Interpretability)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥åˆæˆæ¡†æ¶æ˜¾è‘—å¢å¼ºäº†å½“å‰MLLMsåœ¨VisuRiddlesä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œæœ‰åŠ›éªŒè¯äº†æå‡ç»†ç²’åº¦è§†è§‰æ„ŸçŸ¥å¯¹äºçªç ´å¤šæ¨¡æ€æ¨¡å‹æ¨ç†ç“¶é¢ˆçš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.02537v3",
      "published_date": "2025-06-03 07:24:00 UTC",
      "updated_date": "2025-10-21 03:33:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:16:38.692290+00:00"
    },
    {
      "arxiv_id": "2506.02529v1",
      "title": "Automated Web Application Testing: End-to-End Test Case Generation with Large Language Models and Screen Transition Graphs",
      "title_zh": "Web åº”ç”¨ç¨‹åºè‡ªåŠ¨åŒ–æµ‹è¯•ï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹ä¸é¡µé¢è½¬æ¢å›¾çš„ç«¯åˆ°ç«¯æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ",
      "authors": [
        "Nguyen-Khang Le",
        "Quan Minh Bui",
        "Minh Ngoc Nguyen",
        "Hiep Nguyen",
        "Trung Vo",
        "Son T. Luu",
        "Shoshin Nomura",
        "Minh Le Nguyen"
      ],
      "abstract": "Web applications are critical to modern software ecosystems, yet ensuring their reliability remains challenging due to the complexity and dynamic nature of web interfaces. Recent advances in large language models (LLMs) have shown promise in automating complex tasks, but limitations persist in handling dynamic navigation flows and complex form interactions. This paper presents an automated system for generating test cases for two key aspects of web application testing: site navigation and form filling. For site navigation, the system employs screen transition graphs and LLMs to model navigation flows and generate test scenarios. For form filling, it uses state graphs to handle conditional forms and automates Selenium script generation. Key contributions include: (1) a novel integration of graph structures and LLMs for site navigation testing, (2) a state graph-based approach for automating form-filling test cases, and (3) a comprehensive dataset for evaluating form-interaction testing. Experimental results demonstrate the system's effectiveness in improving test coverage and robustness, advancing the state of web application testing.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°ä»£ Web åº”ç”¨ç¨‹åºæ¥å£å¤æ‚ä¸”å…·æœ‰åŠ¨æ€æ€§çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆå¤§è¯­è¨€æ¨¡å‹ (LLMs) ä¸å±å¹•è½¬æ¢å›¾ (Screen Transition Graphs) çš„ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé‡ç‚¹è§£å†³ç«™ç‚¹å¯¼èˆªå’Œè¡¨å•å¡«å……ä¸¤ä¸ªå…³é”®æµ‹è¯•ç»´åº¦ï¼Œåˆ©ç”¨ Screen Transition Graphs ä¸ LLMs åä½œå»ºæ¨¡å¯¼èˆªæµå¹¶ç”Ÿæˆæµ‹è¯•åœºæ™¯ã€‚é’ˆå¯¹å¤æ‚çš„æ¡ä»¶è¡¨å•ï¼Œç³»ç»Ÿé€šè¿‡çŠ¶æ€å›¾ (State Graphs) è¿›è¡Œå»ºæ¨¡å¹¶å®ç°äº† Selenium è„šæœ¬çš„è‡ªåŠ¨åŒ–ç”Ÿæˆã€‚ç ”ç©¶çš„ä¸»è¦è´¡çŒ®åœ¨äºåˆ›æ–°æ€§åœ°èåˆäº†å›¾ç»“æ„ä¸ LLMs ä»¥ä¼˜åŒ–å¯¼èˆªæµ‹è¯•ï¼Œå¹¶æä¾›äº†ä¸€å¥—é’ˆå¯¹è¡¨å•äº¤äº’æµ‹è¯•çš„ç»¼åˆæ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿæ˜¾è‘—æå‡äº†æµ‹è¯•è¦†ç›–ç‡å’Œé²æ£’æ€§ï¼Œæœ‰æ•ˆå¼¥è¡¥äº†ç°æœ‰ LLMs åœ¨å¤„ç†åŠ¨æ€å¯¼èˆªæµå’Œå¤æ‚è¡¨å•äº¤äº’æ–¹é¢çš„å±€é™æ€§ï¼Œä¸º Web è‡ªåŠ¨åŒ–æµ‹è¯•é¢†åŸŸæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "Published in the Proceedings of JSAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.02529v1",
      "published_date": "2025-06-03 07:08:21 UTC",
      "updated_date": "2025-06-03 07:08:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:16:27.737442+00:00"
    },
    {
      "arxiv_id": "2506.02527v1",
      "title": "Multilingual Information Retrieval with a Monolingual Knowledge Base",
      "title_zh": "åŸºäºå•è¯­çŸ¥è¯†åº“çš„å¤šè¯­è¨€ä¿¡æ¯æ£€ç´¢",
      "authors": [
        "Yingying Zhuang",
        "Aman Gupta",
        "Anurag Beniwal"
      ],
      "abstract": "Multilingual information retrieval has emerged as powerful tools for expanding knowledge sharing across languages. On the other hand, resources on high quality knowledge base are often scarce and in limited languages, therefore an effective embedding model to transform sentences from different languages into a feature vector space same as the knowledge base language becomes the key ingredient for cross language knowledge sharing, especially to transfer knowledge available in high-resource languages to low-resource ones. In this paper we propose a novel strategy to fine-tune multilingual embedding models with weighted sampling for contrastive learning, enabling multilingual information retrieval with a monolingual knowledge base. We demonstrate that the weighted sampling strategy produces performance gains compared to standard ones by up to 31.03\\% in MRR and up to 33.98\\% in Recall@3. Additionally, our proposed methodology is language agnostic and applicable for both multilingual and code switching use cases.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜è´¨é‡çŸ¥è¯†åº“èµ„æºç¨€ç¼ºä¸”è¯­è¨€ç§ç±»æœ‰é™çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨å•è¯­çŸ¥è¯†åº“å®ç°å¤šè¯­è¨€ä¿¡æ¯æ£€ç´¢ï¼ˆMultilingual Information Retrievalï¼‰çš„æ–°ç­–ç•¥ã€‚ä½œè€…æå‡ºé€šè¿‡åŠ æƒé‡‡æ ·ï¼ˆweighted samplingï¼‰å¯¹å¤šè¯­è¨€åµŒå…¥æ¨¡å‹è¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼ˆcontrastive learningï¼‰å¾®è°ƒï¼Œæ—¨åœ¨å°†ä¸åŒè¯­è¨€çš„å¥å­æ˜ å°„åˆ°ä¸çŸ¥è¯†åº“ä¸€è‡´çš„ç‰¹å¾å‘é‡ç©ºé—´ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç­–ç•¥ç›¸è¾ƒäºæ ‡å‡†æ–¹æ³•åœ¨æŒ‡æ ‡ä¸Šæå‡æ˜¾è‘—ï¼Œå…¶ä¸­MRRæå‡é«˜è¾¾31.03%ï¼ŒRecall@3æå‡è¾¾33.98%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å…·æœ‰è¯­è¨€æ— å…³æ€§ï¼ˆlanguage agnosticï¼‰ï¼Œä¸ä»…é€‚ç”¨äºæ ‡å‡†å¤šè¯­è¨€åœºæ™¯ï¼Œä¹Ÿèƒ½æœ‰æ•ˆå¤„ç†ä»£ç åˆ‡æ¢ï¼ˆcode switchingï¼‰çš„æƒ…å†µï¼Œä¸ºé«˜èµ„æºè¯­è¨€å‘ä½èµ„æºè¯­è¨€çš„çŸ¥è¯†è¿ç§»æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "6 pages, accepted at GENNEXT@SIGIR25",
      "pdf_url": "https://arxiv.org/pdf/2506.02527v1",
      "published_date": "2025-06-03 07:05:49 UTC",
      "updated_date": "2025-06-03 07:05:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:16:35.690907+00:00"
    },
    {
      "arxiv_id": "2506.02522v1",
      "title": "Think Twice, Act Once: A Co-Evolution Framework of LLM and RL for Large-Scale Decision Making",
      "title_zh": "ä¸‰æ€è€Œåè¡Œï¼šé¢å‘å¤§è§„æ¨¡å†³ç­–çš„å¤§è¯­è¨€æ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ ååŒæ¼”åŒ–æ¡†æ¶",
      "authors": [
        "Xu Wan",
        "Wenyue Xu",
        "Chao Yang",
        "Mingyang Sun"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) and Reinforcement Learning (RL) have shown significant promise in decision-making tasks. Nevertheless, for large-scale industrial decision problems, both approaches face distinct challenges: LLMs lack real-time long-sequence decision-making capabilities, while RL struggles with sample efficiency in vast action spaces. To bridge this gap, we propose Agents Co-Evolution (ACE), a synergistic framework between LLMs and RL agents for large-scale decision-making scenarios. ACE introduces a dual-role trajectory refinement mechanism where LLMs act as both Policy Actor and Value Critic during RL's training: the Actor refines suboptimal actions via multi-step reasoning and environment validation, while the Critic performs temporal credit assignment through trajectory-level reward shaping. Concurrently, RL agent enhances LLMs' task-specific decision-making with high-quality fine-tuning datasets generated via prioritized experience replay. Through extensive experiments across multiple power grid operation challenges with action spaces exceeding 60K discrete actions, ACE demonstrates superior performance over existing RL methods and LLM-based methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Agents Co-Evolution (ACE)æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆå¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸å¼ºåŒ–å­¦ä¹ (RL)çš„ååŒæ¼”åŒ–æ–¹æ¡ˆï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡å·¥ä¸šå†³ç­–ä¸­çš„æ•ˆç‡ä¸é•¿åºåˆ—å¤„ç†éš¾é¢˜ã€‚é’ˆå¯¹LLMså®æ—¶å†³ç­–èƒ½åŠ›å—é™ä»¥åŠRLåœ¨è¶…å¤§è§„æ¨¡åŠ¨ä½œç©ºé—´ä¸­é‡‡æ ·æ•ˆç‡ä½çš„é—®é¢˜ï¼ŒACEå¼•å…¥äº†åŒé‡è§’è‰²çš„è½¨è¿¹ç²¾ç‚¼æœºåˆ¶ã€‚åœ¨è¯¥æœºåˆ¶ä¸­ï¼ŒLLMsåŒæ—¶æ‹…ä»»Policy Actorå’ŒValue Criticï¼šActoré€šè¿‡å¤šæ­¥æ¨ç†å’Œç¯å¢ƒéªŒè¯æ¥ä¿®æ­£æ¬¡ä¼˜åŠ¨ä½œï¼Œè€ŒCriticåˆ™åˆ©ç”¨è½¨è¿¹çº§çš„å¥–åŠ±å¡‘é€ (reward shaping)è¿›è¡Œæ—¶é—´ä¿¡ç”¨åˆ†é…ã€‚ä¸æ­¤åŒæ—¶ï¼ŒRLæ™ºèƒ½ä½“é€šè¿‡ä¼˜å…ˆç»éªŒå›æ”¾(prioritized experience replay)ç”Ÿæˆçš„é«˜è´¨é‡å¾®è°ƒæ•°æ®é›†ï¼Œåå‘å¢å¼ºLLMsçš„ç‰¹å®šä»»åŠ¡å†³ç­–æ°´å¹³ã€‚åœ¨æ¶‰åŠè¶…è¿‡60,000ä¸ªç¦»æ•£åŠ¨ä½œçš„ç”µç½‘è¿è¡ŒæŒ‘æˆ˜å®éªŒä¸­ï¼ŒACEçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„å¼ºåŒ–å­¦ä¹ å’ŒåŸºäºLLMçš„æ–¹æ³•ï¼Œè¯æ˜äº†å…¶å¤„ç†å¤æ‚å†³ç­–ä»»åŠ¡çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02522v1",
      "published_date": "2025-06-03 06:52:37 UTC",
      "updated_date": "2025-06-03 06:52:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:16:43.791624+00:00"
    },
    {
      "arxiv_id": "2506.02515v3",
      "title": "FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial Reasoning",
      "title_zh": "FinChainï¼šé¢å‘å¯éªŒè¯é“¾å¼æ€ç»´é‡‘èæ¨ç†çš„ç¬¦å·åŒ–åŸºå‡†",
      "authors": [
        "Zhuohan Xie",
        "Daniil Orel",
        "Rushil Thareja",
        "Dhruv Sahnan",
        "Hachem Madmoun",
        "Fan Zhang",
        "Debopriyo Banerjee",
        "Georgi Georgiev",
        "Xueqing Peng",
        "Lingfei Qian",
        "Jimin Huang",
        "Jinyan Su",
        "Aaryamonvikram Singh",
        "Rui Xing",
        "Rania Elbadry",
        "Chen Xu",
        "Haonan Li",
        "Fajri Koto",
        "Ivan Koychev",
        "Tanmoy Chakraborty",
        "Yuxia Wang",
        "Salem Lahlou",
        "Veselin Stoyanov",
        "Sophia Ananiadou",
        "Preslav Nakov"
      ],
      "abstract": "Multi-step symbolic reasoning is essential for robust financial analysis; yet, current benchmarks largely overlook this capability. Existing datasets such as FinQA and ConvFinQA emphasize final numerical answers while neglecting the intermediate reasoning required for transparency and verification. To address this gap, we introduce FINCHAIN, the first benchmark specifically designed for verifiable Chain-of-Thought (CoT) evaluation in finance. FINCHAIN spans 58 topics across 12 financial domains, each represented by parameterized symbolic templates with executable Python traces that enable fully machine-verifiable reasoning and scalable, contamination-free data generation. To assess reasoning capacity, we propose CHAINEVAL, a dynamic alignment measure that jointly evaluates both the final-answer correctness and the step-level reasoning consistency. Our evaluation of 26 leading LLMs reveals that even frontier proprietary LLMs exhibit clear limitations in symbolic financial reasoning, while domain-adapted and math-enhanced fine-tuned models can substantially narrow this gap. Overall, FINCHAIN exposes persistent weaknesses in multi-step financial reasoning and provides a foundation for developing trustworthy, interpretable, and verifiable financial AI.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰é‡‘èåŸºå‡†æµ‹è¯•ï¼ˆå¦‚ FinQA å’Œ ConvFinQAï¼‰è¿‡åº¦å…³æ³¨æœ€ç»ˆæ•°å€¼è€Œå¿½è§†æ¨ç†è¿‡ç¨‹é€æ˜åº¦çš„é—®é¢˜ï¼Œæå‡ºäº† FINCHAINï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨ç”¨äºé‡‘èé¢†åŸŸå¯éªŒè¯é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼‰è¯„ä¼°çš„åŸºå‡†ã€‚FINCHAIN æ¶µç›–äº† 12 ä¸ªé‡‘èé¢†åŸŸçš„ 58 ä¸ªä¸»é¢˜ï¼Œé€šè¿‡é‡‡ç”¨å‚æ•°åŒ–ç¬¦å·æ¨¡æ¿ï¼ˆSymbolic Templatesï¼‰å’Œå¯æ‰§è¡Œçš„ Python è¿½è¸ªè·¯å¾„ï¼ˆPython Tracesï¼‰ï¼Œå®ç°äº†å®Œå…¨çš„æœºå™¨å¯éªŒè¯æ¨ç†å’Œå¤§è§„æ¨¡ã€æ— æ±¡æŸ“çš„æ•°æ®ç”Ÿæˆã€‚ç ”ç©¶å›¢é˜ŸåŒæ­¥æå‡ºäº† CHAINEVAL åŠ¨æ€å¯¹é½æŒ‡æ ‡ï¼Œæ—¨åœ¨ç»¼åˆè¯„ä¼°æœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®æ€§ä¸æ­¥éª¤çº§æ¨ç†çš„ä¸€è‡´æ€§ã€‚é€šè¿‡å¯¹ 26 ä¸ªé¢†å…ˆçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºå³ä½¿æ˜¯é¡¶å°–çš„ç§æœ‰æ¨¡å‹åœ¨ç¬¦å·é‡‘èæ¨ç†æ–¹é¢ä¹Ÿå­˜åœ¨æ˜æ˜¾å±€é™ï¼Œè€Œç»é¢†åŸŸè‡ªé€‚åº”æˆ–æ•°å­¦å¢å¼ºçš„å¾®è°ƒæ¨¡å‹èƒ½æ˜¾è‘—ç¼©å°å·®è·ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†å¤šæ­¥é‡‘èæ¨ç†ä¸­æŒç»­å­˜åœ¨çš„å¼±ç‚¹ï¼Œä¸ºå¼€å‘æ›´å…·å¯ä¿¡åº¦ã€å¯è§£é‡Šæ€§å’Œå¯éªŒè¯æ€§çš„é‡‘èäººå·¥æ™ºèƒ½ï¼ˆFinancial AIï¼‰æä¾›äº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "24 pages, includes 12 figures and 9 tables; introduces the FinChain benchmark and ChainEval metric",
      "pdf_url": "https://arxiv.org/pdf/2506.02515v3",
      "published_date": "2025-06-03 06:44:42 UTC",
      "updated_date": "2026-01-08 07:03:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:16:46.986116+00:00"
    },
    {
      "arxiv_id": "2506.02510v1",
      "title": "M$^3$FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial Meeting Understanding Evaluation Dataset",
      "title_zh": "M$^3$FinMeetingï¼šå¤šè¯­è¨€ã€å¤šè¡Œä¸šã€å¤šä»»åŠ¡é‡‘èä¼šè®®ç†è§£è¯„ä¼°æ•°æ®é›†",
      "authors": [
        "Jie Zhu",
        "Junhui Li",
        "Yalong Wen",
        "Xiandong Li",
        "Lifan Guo",
        "Feng Chen"
      ],
      "abstract": "Recent breakthroughs in large language models (LLMs) have led to the development of new benchmarks for evaluating their performance in the financial domain. However, current financial benchmarks often rely on news articles, earnings reports, or announcements, making it challenging to capture the real-world dynamics of financial meetings. To address this gap, we propose a novel benchmark called $\\texttt{M$^3$FinMeeting}$, which is a multilingual, multi-sector, and multi-task dataset designed for financial meeting understanding. First, $\\texttt{M$^3$FinMeeting}$ supports English, Chinese, and Japanese, enhancing comprehension of financial discussions in diverse linguistic contexts. Second, it encompasses various industry sectors defined by the Global Industry Classification Standard (GICS), ensuring that the benchmark spans a broad range of financial activities. Finally, $\\texttt{M$^3$FinMeeting}$ includes three tasks: summarization, question-answer (QA) pair extraction, and question answering, facilitating a more realistic and comprehensive evaluation of understanding. Experimental results with seven popular LLMs reveal that even the most advanced long-context models have significant room for improvement, demonstrating the effectiveness of $\\texttt{M$^3$FinMeeting}$ as a benchmark for assessing LLMs' financial meeting comprehension skills.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰é‡‘èé¢†åŸŸåŸºå‡†ä¸»è¦ä¾èµ–æ–°é—»å’ŒæŠ¥å‘Šè€Œéš¾ä»¥æ•æ‰çœŸå®ä¼šè®®åŠ¨æ€çš„é—®é¢˜ï¼Œæå‡ºäº† M$^3$FinMeeting è¿™ä¸€å¤šè¯­è¨€ã€å¤šé¢†åŸŸåŠå¤šä»»åŠ¡çš„é‡‘èä¼šè®®ç†è§£ï¼ˆfinancial meeting understandingï¼‰è¯„ä¼°æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æ”¯æŒ Englishã€Chinese å’Œ Japaneseï¼Œå¹¶æ¶µç›–äº†ç”±å…¨çƒè¡Œä¸šåˆ†ç±»æ ‡å‡†ï¼ˆGICSï¼‰å®šä¹‰çš„å¤šä¸ªè¡Œä¸šé¢†åŸŸï¼Œç¡®ä¿äº†é‡‘èæ´»åŠ¨çš„å¹¿æ³›è¦†ç›–ã€‚M$^3$FinMeeting åŒ…å«æ‘˜è¦ï¼ˆsummarizationï¼‰ã€é—®ç­”å¯¹æå–ï¼ˆQA pair extractionï¼‰å’Œé—®ç­”ï¼ˆquestion answeringï¼‰ä¸‰é¡¹ä»»åŠ¡ï¼Œä»¥å®ç°æ›´çœŸå®ã€å…¨é¢çš„è¯„ä¼°ã€‚ç ”ç©¶äººå‘˜å¯¹ä¸ƒä¸ªä¸»æµå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œäº†æµ‹è¯•ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå³ä½¿æ˜¯å…ˆè¿›çš„é•¿æ–‡æœ¬æ¨¡å‹åœ¨é‡‘èä¼šè®®ç†è§£æ–¹é¢ä»æœ‰æ˜¾è‘—æå‡ç©ºé—´ã€‚è¯¥æ•°æ®é›†çš„æå‡ºä¸ºè¯„ä¼°æ¨¡å‹åœ¨å¤æ‚çœŸå®é‡‘èåœºæ™¯ä¸‹çš„ç†è§£èƒ½åŠ›æä¾›äº†æœ‰æ•ˆæ‰‹æ®µã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ACL-2025",
      "pdf_url": "https://arxiv.org/pdf/2506.02510v1",
      "published_date": "2025-06-03 06:41:09 UTC",
      "updated_date": "2025-06-03 06:41:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:16:50.483549+00:00"
    },
    {
      "arxiv_id": "2506.02494v1",
      "title": "Minos: A Multimodal Evaluation Model for Bidirectional Generation Between Image and Text",
      "title_zh": "Minosï¼šé¢å‘å›¾æ–‡åŒå‘ç”Ÿæˆçš„å¤šæ¨¡æ€è¯„ä¼°æ¨¡å‹",
      "authors": [
        "Junzhe Zhang",
        "Huixuan Zhang",
        "Xinyu Hu",
        "Li Lin",
        "Mingqi Gao",
        "Shi Qiu",
        "Xiaojun Wan"
      ],
      "abstract": "Evaluation is important for multimodal generation tasks. With the rapid progress of MLLMs, there is growing interest in applying MLLMs to build general evaluation systems. However, existing work overlooks two aspects: (1) the development of evaluation capabilities for text-to-image (T2I) generation task, and (2) the incorporation of large-scale human evaluation data. In this paper, we introduce Minos-Corpus, a large-scale multimodal evaluation dataset that combines evaluation data from both human and GPT. The corpus contains evaluation data across both image-to-text(I2T) and T2I generation tasks. Based on this corpus, we propose Data Selection and Balance, Mix-SFT training methods, and apply DPO to develop Minos, a multimodal evaluation model built upon a 7B backbone. Minos achieves state-of-the-art (SoTA) performance among all open-source evaluation models of similar scale on the average of evaluation performance on all tasks, and outperforms all open-source and closed-source models on evaluation of T2I generation task. Extensive experiments demonstrate the importance of leveraging high-quality human evaluation data and jointly training on evaluation data from both I2T and T2I generation tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Minosï¼Œä¸€ä¸ªåŸºäº 7B å‚æ•°è§„æ¨¡çš„å¤šæ¨¡æ€è¯„ä¼°æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è¯„ä¼°ç³»ç»Ÿåœ¨ Text-to-Image (T2I) ç”Ÿæˆä»»åŠ¡ä»¥åŠæ•´åˆå¤§è§„æ¨¡äººç±»è¯„ä»·æ•°æ®æ–¹é¢çš„ä¸è¶³ã€‚ä½œè€…æ„å»ºäº† Minos-Corpusï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†äººç±»ä¸ GPT è¯„ä»·ã€æ¶µç›– Image-to-Text (I2T) å’Œ T2I åŒå‘ä»»åŠ¡çš„å¤§è§„æ¨¡å¤šæ¨¡æ€è¯„ä¼°æ•°æ®é›†ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶å›¢é˜Ÿé‡‡ç”¨äº† Data Selection and Balance å’Œ Mix-SFT è®­ç»ƒæ–¹æ³•ï¼Œå¹¶åº”ç”¨ Direct Preference Optimization (DPO) æŠ€æœ¯å¯¹æ¨¡å‹è¿›è¡Œä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMinos åœ¨æ‰€æœ‰ä»»åŠ¡çš„å¹³å‡è¯„ä¼°æ€§èƒ½ä¸Šè¾¾åˆ°äº†åŒç­‰è§„æ¨¡å¼€æºæ¨¡å‹çš„ State-of-the-art (SoTA) æ°´å¹³ã€‚ç‰¹åˆ«æ˜¯åœ¨ T2I ç”Ÿæˆä»»åŠ¡çš„è¯„ä¼°ä¸­ï¼ŒMinos çš„è¡¨ç°è¶…è¶Šäº†ç›®å‰æ‰€æœ‰çš„å¼€æºå’Œé—­æºæ¨¡å‹ã€‚è¯¥ç ”ç©¶é€šè¿‡å¤§é‡å®éªŒè¯æ˜äº†åˆ©ç”¨é«˜è´¨é‡äººç±»è¯„ä¼°æ•°æ®ä»¥åŠå¯¹ I2T å’Œ T2I ä»»åŠ¡è¿›è¡Œè”åˆè®­ç»ƒå¯¹äºæå‡å¤šæ¨¡æ€è¯„ä¼°èƒ½åŠ›å…·æœ‰å…³é”®æ„ä¹‰ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02494v1",
      "published_date": "2025-06-03 06:17:16 UTC",
      "updated_date": "2025-06-03 06:17:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:17:05.144499+00:00"
    },
    {
      "arxiv_id": "2506.02490v1",
      "title": "Simplifying Root Cause Analysis in Kubernetes with StateGraph and LLM",
      "title_zh": "åˆ©ç”¨ StateGraph ä¸ LLM ç®€åŒ– Kubernetes æ ¹å› åˆ†æ",
      "authors": [
        "Yong Xiang",
        "Charley Peter Chen",
        "Liyi Zeng",
        "Wei Yin",
        "Xin Liu",
        "Hu Li",
        "Wei Xu"
      ],
      "abstract": "Kubernetes, a notably complex and distributed system, utilizes an array of controllers to uphold cluster management logic through state reconciliation. Nevertheless, maintaining state consistency presents significant challenges due to unexpected failures, network disruptions, and asynchronous issues, especially within dynamic cloud environments. These challenges result in operational disruptions and economic losses, underscoring the necessity for robust root cause analysis (RCA) to enhance Kubernetes reliability. The development of large language models (LLMs) presents a promising direction for RCA. However, existing methodologies encounter several obstacles, including the diverse and evolving nature of Kubernetes incidents, the intricate context of incidents, and the polymorphic nature of these incidents. In this paper, we introduce SynergyRCA, an innovative tool that leverages LLMs with retrieval augmentation from graph databases and enhancement with expert prompts. SynergyRCA constructs a StateGraph to capture spatial and temporal relationships and utilizes a MetaGraph to outline entity connections. Upon the occurrence of an incident, an LLM predicts the most pertinent resource, and SynergyRCA queries the MetaGraph and StateGraph to deliver context-specific insights for RCA. We evaluate SynergyRCA using datasets from two production Kubernetes clusters, highlighting its capacity to identify numerous root causes, including novel ones, with high efficiency and precision. SynergyRCA demonstrates the ability to identify root causes in an average time of about two minutes and achieves an impressive precision of approximately 0.90.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Kubernetes åœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­å› çŠ¶æ€ä¸€è‡´æ€§é—®é¢˜å¯¼è‡´çš„è¿è¡Œä¸­æ–­ï¼Œæå‡ºäº† SynergyRCAï¼Œä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) ç®€åŒ–æ ¹å› åˆ†æ (Root Cause Analysis, RCA) çš„åˆ›æ–°å·¥å…·ã€‚ä¸ºäº†è§£å†³ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚æ•…éšœä¸Šä¸‹æ–‡åŠå¤šæ€æ•…éšœæ—¶çš„å±€é™æ€§ï¼ŒSynergyRCA æ„å»ºäº†ç”¨äºæ•æ‰æ—¶ç©ºå…³ç³»çš„ StateGraph ä»¥åŠæ¦‚è¿°å®ä½“è¿æ¥çš„ MetaGraphã€‚è¯¥å·¥å…·ç»“åˆäº†å›¾å½¢æ•°æ®åº“çš„æ£€ç´¢å¢å¼º (Retrieval Augmentation) æŠ€æœ¯å’Œä¸“å®¶æç¤º (Expert Prompts)ï¼Œèƒ½å¤Ÿåœ¨å¤§è§„æ¨¡é›†ç¾¤å‘ç”Ÿæ•…éšœæ—¶ç²¾å‡†é¢„æµ‹ç›¸å…³èµ„æºå¹¶æä¾›æ·±åº¦ä¸Šä¸‹æ–‡è§è§£ã€‚åœ¨ä¸¤ä¸ªç”Ÿäº§ç¯å¢ƒ Kubernetes é›†ç¾¤ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒSynergyRCA èƒ½å¤Ÿé«˜æ•ˆè¯†åˆ«åŒ…æ‹¬æ–°å‹æ•…éšœåœ¨å†…çš„å¤šç§æ ¹å› ã€‚å®éªŒæ•°æ®æ˜¾ç¤ºï¼Œè¯¥å·¥å…·å¹³å‡ä»…éœ€çº¦ä¸¤åˆ†é’Ÿå³å¯å®Œæˆåˆ†æï¼Œä¸”æ ¹å› è¯†åˆ«çš„å‡†ç¡®ç‡ (Precision) è¾¾åˆ°äº† 0.90ï¼Œä¸ºå¢å¼ºåˆ†å¸ƒå¼ç³»ç»Ÿçš„å¯é æ€§æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.DC",
      "comment": "12 pages, 13 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.02490v1",
      "published_date": "2025-06-03 06:09:13 UTC",
      "updated_date": "2025-06-03 06:09:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:17:00.291818+00:00"
    },
    {
      "arxiv_id": "2506.02488v2",
      "title": "Flexiffusion: Training-Free Segment-Wise Neural Architecture Search for Efficient Diffusion Models",
      "title_zh": "Flexiffusionï¼šé¢å‘é«˜æ•ˆæ‰©æ•£æ¨¡å‹çš„å…è®­ç»ƒåˆ†æ®µå¼ç¥ç»æ¶æ„æœç´¢",
      "authors": [
        "Hongtao Huang",
        "Xiaojun Chang",
        "Lina Yao"
      ],
      "abstract": "Diffusion models (DMs) are powerful generative models capable of producing high-fidelity images but are constrained by high computational costs due to iterative multi-step inference. While Neural Architecture Search (NAS) can optimize DMs, existing methods are hindered by retraining requirements, exponential search complexity from step-wise optimization, and slow evaluation relying on massive image generation. To address these challenges, we propose Flexiffusion, a training-free NAS framework that jointly optimizes generation schedules and model architectures without modifying pre-trained parameters. Our key insight is to decompose the generation process into flexible segments of equal length, where each segment dynamically combines three step types: full (complete computation), partial (cache-reused computation), and null (skipped computation). This segment-wise search space reduces the candidate pool exponentially compared to step-wise NAS while preserving architectural diversity. Further, we introduce relative FID (rFID), a lightweight evaluation metric for NAS that measures divergence from a teacher model's outputs instead of ground truth, slashing evaluation time by over $90\\%$. In practice, Flexiffusion achieves at least $2\\times$ acceleration across LDMs, Stable Diffusion, and DDPMs on ImageNet and MS-COCO, with FID degradation under $5\\%$, outperforming prior NAS and caching methods. Notably, it attains $5.1\\times$ speedup on Stable Diffusion with near-identical CLIP scores. Our work pioneers a resource-efficient paradigm for searching high-speed DMs without sacrificing quality.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Flexiffusionï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³æ‰©æ•£æ¨¡å‹ (Diffusion models, DMs) é«˜è®¡ç®—æˆæœ¬é—®é¢˜çš„æ— éœ€è®­ç»ƒçš„ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ (Neural Architecture Search, NAS) æ¡†æ¶ã€‚è¯¥æ¡†æ¶åœ¨ä¸ä¿®æ”¹é¢„è®­ç»ƒå‚æ•°çš„å‰æä¸‹ï¼Œå°†ç”Ÿæˆè¿‡ç¨‹åˆ†è§£ä¸ºç­‰é•¿çš„çµæ´»åˆ†æ®µï¼Œé€šè¿‡åœ¨æ¯ä¸ªåˆ†æ®µå†…åŠ¨æ€ç»„åˆå®Œæ•´è®¡ç®— (full)ã€ç¼“å­˜é‡ç”¨ (partial) å’Œè·³è¿‡è®¡ç®— (null) ä¸‰ç§æ­¥éª¤æ¥ä¼˜åŒ–æ¶æ„ã€‚ç›¸æ¯”ä¼ ç»Ÿçš„é€æ­¥æœç´¢ï¼Œè¿™ç§åˆ†æ®µå¼æœç´¢ç©ºé—´åœ¨ä¿ç•™å¤šæ ·æ€§çš„åŒæ—¶æ˜¾è‘—é™ä½äº†æœç´¢å¤æ‚åº¦ã€‚ç ”ç©¶è¿˜å¼•å…¥äº†è½»é‡çº§æŒ‡æ ‡ç›¸å¯¹ FID (relative FID, rFID)ï¼Œé€šè¿‡æµ‹é‡ä¸æ•™å¸ˆæ¨¡å‹è¾“å‡ºçš„åå·®ï¼Œå°†è¯„ä¼°æ—¶é—´ç¼©å‡äº† 90% ä»¥ä¸Šã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFlexiffusion åœ¨ LDMsã€Stable Diffusion å’Œ DDPMs ä¸Šå®ç°äº†è‡³å°‘ 2 å€çš„åŠ é€Ÿï¼Œä¸” FID æ€§èƒ½æŸå¤±ä½äº 5%ã€‚ç‰¹åˆ«æ˜¯åœ¨ Stable Diffusion ä¸Šï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº† 5.1 å€çš„åŠ é€Ÿæ•ˆæœï¼ŒåŒæ—¶ä¿æŒäº†è¿‘ä¹ä¸€è‡´çš„ CLIP åˆ†æ•°ï¼Œä¸ºæ„å»ºé«˜æ•ˆæ‰©æ•£æ¨¡å‹æä¾›äº†èµ„æºèŠ‚çº¦å‹çš„æ–°èŒƒå¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper was intended to be a v2 version of my previous paper (arXiv:2409.17566), but it was submitted as a new paper by mistake",
      "pdf_url": "https://arxiv.org/pdf/2506.02488v2",
      "published_date": "2025-06-03 06:02:50 UTC",
      "updated_date": "2025-06-04 23:47:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:17:02.587730+00:00"
    },
    {
      "arxiv_id": "2506.02485v2",
      "title": "Generative AI as a Pillar for Predicting 2D and 3D Wildfire Spread: Beyond Physics-Based Models and Traditional Deep Learning",
      "title_zh": "ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼šäºŒç»´ä¸ä¸‰ç»´é‡ç«è”“å»¶é¢„æµ‹çš„æ–°æ”¯æŸ±â€”â€”è¶…è¶Šç‰©ç†æ¨¡å‹ä¸ä¼ ç»Ÿæ·±åº¦å­¦ä¹ ",
      "authors": [
        "Haowen Xu",
        "Sisi Zlatanova",
        "Ruiyu Liang",
        "Ismet Canbulat"
      ],
      "abstract": "Wildfires increasingly threaten human life, ecosystems, and infrastructure, with events like the 2025 Palisades and Eaton fires in Los Angeles County underscoring the urgent need for more advanced prediction frameworks. Existing physics-based and deep learning models struggle to capture dynamic wildfire spread across both 2D and 3D domains, especially when incorporating real-time, multimodal geospatial data. This paper explores how generative Artificial Intelligence (AI) models-such as GANs, VAEs, and Transformers-can serve as transformative tools for wildfire prediction and simulation. These models offer superior capabilities in managing uncertainty, integrating multimodal inputs, and generating realistic, scalable wildfire scenarios. We introduce a new paradigm that leverages large language models (LLMs) for literature synthesis, classification, and knowledge extraction, conducting a systematic review of recent studies applying generative AI to fire prediction and monitoring. We highlight how generative approaches uniquely address challenges faced by traditional simulation and deep learning methods. Finally, we outline five key future directions for generative AI in wildfire management, including unified multimodal modeling of 2D and 3D dynamics, agentic AI systems and chatbots for decision intelligence, and real-time scenario generation on mobile devices, along with a discussion of critical challenges. Our findings advocate for a paradigm shift toward multimodal generative frameworks to support proactive, data-informed wildfire response.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) åœ¨é¢„æµ‹ 2D å’Œ 3D å±±ç«è”“å»¶æ–¹é¢çš„æ ¸å¿ƒæ”¯æŸ±ä½œç”¨ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿç‰©ç†æ¨¡å‹å’Œæ·±åº¦å­¦ä¹ åœ¨å¤„ç†å®æ—¶å¤šæ¨¡æ€åœ°ç†ç©ºé—´æ•°æ®åŠåŠ¨æ€è”“å»¶é¢„æµ‹æ—¶çš„å±€é™ã€‚è®ºæ–‡è¯¦ç»†åˆ†æäº† GANsã€VAEs å’Œ Transformers ç­‰ç”Ÿæˆæ¨¡å‹åœ¨ç®¡ç†ä¸ç¡®å®šæ€§ã€é›†æˆå¤šæ¨¡æ€è¾“å…¥ä»¥åŠç”Ÿæˆé€¼çœŸä¸”å¯æ‰©å±•çš„å±±ç«åœºæ™¯æ–¹é¢çš„å“è¶Šèƒ½åŠ›ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) è¿›è¡Œæ–‡çŒ®åˆæˆã€åˆ†ç±»å’ŒçŸ¥è¯†æå–çš„æ–°èŒƒå¼ï¼Œå¹¶å¯¹ç”Ÿæˆå¼ AI åœ¨ç«ç¾é¢„æµ‹ä¸ç›‘æµ‹ä¸­çš„åº”ç”¨è¿›è¡Œäº†ç³»ç»Ÿæ€§ç»¼è¿°ã€‚æ–‡ç« é€šè¿‡å¯¹æ¯”å¼ºè°ƒäº†ç”Ÿæˆå¼æ–¹æ³•åœ¨åº”å¯¹ä¼ ç»Ÿæ¨¡æ‹Ÿæ–¹æ³•æŒ‘æˆ˜æ—¶çš„ç‹¬ç‰¹ä¼˜åŠ¿ï¼Œå¹¶æŒ‡æ˜äº†äº”ä¸ªå…³é”®æœªæ¥æ–¹å‘ï¼ŒåŒ…æ‹¬ 2D å’Œ 3D åŠ¨æ€çš„ç»Ÿä¸€å¤šæ¨¡æ€å»ºæ¨¡ã€ç”¨äºå†³ç­–æ™ºèƒ½çš„æ™ºèƒ½ä½“ AI (Agentic AI) ç³»ç»Ÿä»¥åŠç§»åŠ¨è®¾å¤‡ä¸Šçš„å®æ—¶åœºæ™¯ç”Ÿæˆã€‚æœ€ç»ˆï¼Œè¯¥ç ”ç©¶å€¡å¯¼å‘å¤šæ¨¡æ€ç”Ÿæˆæ¡†æ¶è¿›è¡ŒèŒƒå¼è½¬å˜ï¼Œä¸ºå®ç°ä¸»åŠ¨ä¸”æ•°æ®é©±åŠ¨çš„å±±ç«åº”æ€¥å“åº”æä¾›æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02485v2",
      "published_date": "2025-06-03 05:54:40 UTC",
      "updated_date": "2025-08-04 00:09:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:17:13.056634+00:00"
    },
    {
      "arxiv_id": "2506.02481v1",
      "title": "Do Language Models Think Consistently? A Study of Value Preferences Across Varying Response Lengths",
      "title_zh": "è¯­è¨€æ¨¡å‹çš„æ€ç»´æ˜¯å¦ä¸€è‡´ï¼Ÿä¸åŒå›å¤é•¿åº¦ä¸‹çš„ä»·å€¼åå¥½ç ”ç©¶",
      "authors": [
        "Inderjeet Nair",
        "Lu Wang"
      ],
      "abstract": "Evaluations of LLMs' ethical risks and value inclinations often rely on short-form surveys and psychometric tests, yet real-world use involves long-form, open-ended responses -- leaving value-related risks and preferences in practical settings largely underexplored. In this work, we ask: Do value preferences inferred from short-form tests align with those expressed in long-form outputs? To address this question, we compare value preferences elicited from short-form reactions and long-form responses, varying the number of arguments in the latter to capture users' differing verbosity preferences. Analyzing five LLMs (llama3-8b, gemma2-9b, mistral-7b, qwen2-7b, and olmo-7b), we find (1) a weak correlation between value preferences inferred from short-form and long-form responses across varying argument counts, and (2) similarly weak correlation between preferences derived from any two distinct long-form generation settings. (3) Alignment yields only modest gains in the consistency of value expression. Further, we examine how long-form generation attributes relate to value preferences, finding that argument specificity negatively correlates with preference strength, while representation across scenarios shows a positive correlation. Our findings underscore the need for more robust methods to ensure consistent value expression across diverse applications.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸åŒå“åº”é•¿åº¦ä¸‹ä»·å€¼åå¥½ï¼ˆValue Preferencesï¼‰çš„ä¸€è‡´æ€§ï¼Œå¡«è¡¥äº†ä»¥å¾€è¯„ä¼°ä¾§é‡äºçŸ­ç¯‡æµ‹è¯•è€Œå¿½è§†é•¿ç¯‡å¼€æ”¾å¼å“åº”ä¸­ä»·å€¼é£é™©çš„ç©ºç™½ã€‚é€šè¿‡å¯¹ llama3-8bã€gemma2-9bã€mistral-7bã€qwen2-7b å’Œ olmo-7b äº”ç§æ¨¡å‹è¿›è¡Œå®éªŒï¼Œç ”ç©¶å‘ç°ä»çŸ­ç¯‡æµ‹è¯•ä¸­æ¨æ–­å‡ºçš„ä»·å€¼åå¥½ä¸ä¸åŒé•¿åº¦é•¿ç¯‡å“åº”æ‰€ä½“ç°çš„åå¥½ä¹‹é—´ç›¸å…³æ€§è¾ƒå¼±ã€‚æ­¤å¤–ï¼Œä¸åŒé•¿ç¯‡ç”Ÿæˆè®¾ç½®ä¹‹é—´çš„ä»·å€¼ä¸€è‡´æ€§åŒæ ·è¾ƒä½ï¼Œå³ä½¿ç»è¿‡å¯¹é½ï¼ˆAlignmentï¼‰å¤„ç†ï¼Œæ¨¡å‹åœ¨ä»·å€¼è¡¨è¾¾ä¸Šçš„ä¸€è‡´æ€§æå‡ä¹Ÿååˆ†æœ‰é™ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†è®ºç‚¹å…·ä½“æ€§ä¸åå¥½å¼ºåº¦å‘ˆè´Ÿç›¸å…³ï¼Œè€Œåœºæ™¯ä»£è¡¨æ€§åˆ™ä¸åå¥½å¼ºåº¦æ­£ç›¸å…³ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å½“å‰è¯„ä¼°æ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶æŒ‡å‡ºäºŸéœ€å¼€å‘æ›´ç¨³å¥çš„æŠ€æœ¯ä»¥ç¡®ä¿ LLMs åœ¨ä¸åŒåº”ç”¨åœºæ™¯ä¸‹èƒ½å¤Ÿç»´æŒä¸€è‡´çš„ä»·å€¼è¡¨è¾¾ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02481v1",
      "published_date": "2025-06-03 05:52:03 UTC",
      "updated_date": "2025-06-03 05:52:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:17:46.191432+00:00"
    },
    {
      "arxiv_id": "2506.02470v1",
      "title": "A Smart Multimodal Healthcare Copilot with Powerful LLM Reasoning",
      "title_zh": "å…·å¤‡å¼ºåŠ›å¤§æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ™ºèƒ½å¤šæ¨¡æ€åŒ»ç–—åŠ©æ‰‹",
      "authors": [
        "Xuejiao Zhao",
        "Siyan Liu",
        "Su-Yin Yang",
        "Chunyan Miao"
      ],
      "abstract": "Misdiagnosis causes significant harm to healthcare systems worldwide, leading to increased costs and patient risks. MedRAG is a smart multimodal healthcare copilot equipped with powerful large language model (LLM) reasoning, designed to enhance medical decision-making. It supports multiple input modalities, including non-intrusive voice monitoring, general medical queries, and electronic health records. MedRAG provides recommendations on diagnosis, treatment, medication, and follow-up questioning. Leveraging retrieval-augmented generation enhanced by knowledge graph-elicited reasoning, MedRAG retrieves and integrates critical diagnostic insights, reducing the risk of misdiagnosis. It has been evaluated on both public and private datasets, outperforming existing models and offering more specific and accurate healthcare assistance. A demonstration video of MedRAG is available at: https://www.youtube.com/watch?v=PNIBDMYRfDM. The source code is available at: https://github.com/SNOWTEAM2023/MedRAG.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† MedRAGï¼Œè¿™æ˜¯ä¸€æ¬¾å…·å¤‡å¼ºå¤§å¤§è¯­è¨€æ¨¡å‹ (LLM) æ¨ç†èƒ½åŠ›çš„æ™ºèƒ½å¤šæ¨¡æ€åŒ»ç–—å‰¯é©¾é©¶ç³»ç»Ÿï¼Œæ—¨åœ¨å‡å°‘å…¨çƒåŒ»ç–—ç³»ç»Ÿé¢ä¸´çš„è¯¯è¯Šé£é™©å¹¶ä¼˜åŒ–å†³ç­–è¿‡ç¨‹ã€‚MedRAG æ”¯æŒåŒ…æ‹¬éä¾µå…¥å¼è¯­éŸ³ç›‘æ§ã€é€šç”¨åŒ»ç–—æŸ¥è¯¢ä»¥åŠç”µå­å¥åº·è®°å½• (Electronic Health Records) åœ¨å†…çš„å¤šç§è¾“å…¥æ¨¡æ€ï¼Œå¯æä¾›è¯Šæ–­ã€æ²»ç–—ã€ç”¨è¯åŠéšè®¿å»ºè®®ã€‚å…¶æ ¸å¿ƒæŠ€æœ¯ç»“åˆäº†æ£€ç´¢å¢å¼ºç”Ÿæˆ (Retrieval-Augmented Generation) ä¸çŸ¥è¯†å›¾è°±å¯å‘å¼æ¨ç† (Knowledge Graph-elicited reasoning)ï¼Œé€šè¿‡æ£€ç´¢å¹¶æ•´åˆå…³é”®è¯Šæ–­è§è§£æ¥æ˜¾è‘—é™ä½è¯¯è¯Šæ¦‚ç‡ã€‚åœ¨å…¬å¼€å’Œç§æœ‰æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒMedRAG çš„è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œèƒ½å¤Ÿæä¾›æ›´å…·é’ˆå¯¹æ€§å’Œå‡†ç¡®æ€§çš„åŒ»ç–—è¾…åŠ©ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02470v1",
      "published_date": "2025-06-03 05:39:02 UTC",
      "updated_date": "2025-06-03 05:39:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:17:39.089945+00:00"
    },
    {
      "arxiv_id": "2506.02456v1",
      "title": "VPI-Bench: Visual Prompt Injection Attacks for Computer-Use Agents",
      "title_zh": "VPI-Benchï¼šé’ˆå¯¹è®¡ç®—æœºä½¿ç”¨æ™ºèƒ½ä½“çš„è§†è§‰æç¤ºæ³¨å…¥æ”»å‡»",
      "authors": [
        "Tri Cao",
        "Bennett Lim",
        "Yue Liu",
        "Yuan Sui",
        "Yuexin Li",
        "Shumin Deng",
        "Lin Lu",
        "Nay Oo",
        "Shuicheng Yan",
        "Bryan Hooi"
      ],
      "abstract": "Computer-Use Agents (CUAs) with full system access enable powerful task automation but pose significant security and privacy risks due to their ability to manipulate files, access user data, and execute arbitrary commands. While prior work has focused on browser-based agents and HTML-level attacks, the vulnerabilities of CUAs remain underexplored. In this paper, we investigate Visual Prompt Injection (VPI) attacks, where malicious instructions are visually embedded within rendered user interfaces, and examine their impact on both CUAs and Browser-Use Agents (BUAs). We propose VPI-Bench, a benchmark of 306 test cases across five widely used platforms, to evaluate agent robustness under VPI threats. Each test case is a variant of a web platform, designed to be interactive, deployed in a realistic environment, and containing a visually embedded malicious prompt. Our empirical study shows that current CUAs and BUAs can be deceived at rates of up to 51% and 100%, respectively, on certain platforms. The experimental results also indicate that system prompt defenses offer only limited improvements. These findings highlight the need for robust, context-aware defenses to ensure the safe deployment of multimodal AI agents in real-world environments. The code and dataset are available at: https://github.com/cua-framework/agents",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è®¡ç®—æœºä½¿ç”¨æ™ºèƒ½ä½“ (Computer-Use Agents, CUAs) é¢ä¸´çš„å®‰å…¨å¨èƒï¼Œé‡ç‚¹åˆ†æäº†å°†æ¶æ„æŒ‡ä»¤åµŒå…¥ç”¨æˆ·ç•Œé¢çš„è§†è§‰æç¤ºæ³¨å…¥ (Visual Prompt Injection, VPI) æ”»å‡»ã€‚ä½œè€…æå‡ºäº† VPI-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å« 5 ä¸ªå¸¸ç”¨å¹³å°ä¸Š 306 ä¸ªäº¤äº’å¼æµ‹è¯•ç”¨ä¾‹çš„è¯„æµ‹åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°æ™ºèƒ½ä½“åœ¨ç°å®äº¤äº’ç¯å¢ƒä¸‹çš„å®‰å…¨æ€§ã€‚å®è¯ç ”ç©¶æ˜¾ç¤ºï¼Œç°æœ‰çš„ CUAs å’Œæµè§ˆå™¨ä½¿ç”¨æ™ºèƒ½ä½“ (Browser-Use Agents, BUAs) åœ¨ç‰¹å®šå¹³å°ä¸Šçš„å—æ”»å‡»æˆåŠŸç‡åˆ†åˆ«é«˜è¾¾ 51% å’Œ 100%ã€‚å®éªŒç»“æœè¿›ä¸€æ­¥è¡¨æ˜ï¼Œä¼ ç»Ÿçš„ç³»ç»Ÿæç¤º (System Prompt) é˜²å¾¡æªæ–½å¯¹æ­¤ç±»è§†è§‰æ”»å‡»çš„æ”¹å–„æ•ˆæœéå¸¸æœ‰é™ã€‚è¯¥é¡¹å·¥ä½œå‡¸æ˜¾äº†ä¸ºçœŸå®éƒ¨ç½²ä¸­çš„å¤šæ¨¡æ€ AI æ™ºèƒ½ä½“å»ºç«‹é²æ£’ä¸”å…·å¤‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›çš„é˜²å¾¡ä½“ç³»çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.AI",
      "comment": "Under Review",
      "pdf_url": "https://arxiv.org/pdf/2506.02456v1",
      "published_date": "2025-06-03 05:21:50 UTC",
      "updated_date": "2025-06-03 05:21:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:17:35.684494+00:00"
    },
    {
      "arxiv_id": "2506.02454v3",
      "title": "Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework",
      "title_zh": "Multimodal DeepResearcherï¼šåŸºäºæ™ºèƒ½ä½“æ¡†æ¶çš„å›¾æ–‡äº¤ç»‡æŠ¥å‘Šä»é›¶ç”Ÿæˆ",
      "authors": [
        "Zhaorui Yang",
        "Bo Pan",
        "Han Wang",
        "Yiyao Wang",
        "Xingyu Liu",
        "Luoxuan Weng",
        "Yingchaojie Feng",
        "Haozhe Feng",
        "Minfeng Zhu",
        "Bo Zhang",
        "Wei Chen"
      ],
      "abstract": "Visualizations play a crucial part in effective communication of concepts and information. Recent advances in reasoning and retrieval augmented generation have enabled Large Language Models (LLMs) to perform deep research and generate comprehensive reports. Despite its progress, existing deep research frameworks primarily focus on generating text-only content, leaving the automated generation of interleaved texts and visualizations underexplored. This novel task poses key challenges in designing informative visualizations and effectively integrating them with text reports. To address these challenges, we propose Formal Description of Visualization (FDV), a structured textual representation of charts that enables LLMs to learn from and generate diverse, high-quality visualizations. Building on this representation, we introduce Multimodal DeepResearcher, an agentic framework that decomposes the task into four stages: (1) researching, (2) exemplar report textualization, (3) planning, and (4) multimodal report generation. For the evaluation of generated multimodal reports, we develop MultimodalReportBench, which contains 100 diverse topics served as inputs along with 5 dedicated metrics. Extensive experiments across models and evaluation methods demonstrate the effectiveness of Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet model, Multimodal DeepResearcher achieves an 82% overall win rate over the baseline method.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰æ·±åº¦ç ”ç©¶æ¡†æ¶ä¸»è¦é›†ä¸­åœ¨çº¯æ–‡æœ¬ç”Ÿæˆï¼Œè€Œç¼ºä¹å›¾æ–‡äº¤ç»‡å¯è§†åŒ–å†…å®¹è‡ªåŠ¨ç”Ÿæˆçš„é—®é¢˜ï¼Œæå‡ºäº† Multimodal DeepResearcher æ¡†æ¶ã€‚ä¸ºäº†åº”å¯¹å¯è§†åŒ–è®¾è®¡åŠå…¶ä¸æ–‡æœ¬é›†æˆçš„æŒ‘æˆ˜ï¼Œä½œè€…å¼•å…¥äº†å¯è§†åŒ–å½¢å¼åŒ–æè¿° (Formal Description of Visualization, FDV)ï¼Œé€šè¿‡ç»“æ„åŒ–çš„å›¾è¡¨æ–‡æœ¬è¡¨ç¤ºä½¿å¤§è¯­è¨€æ¨¡å‹ (LLMs) èƒ½å¤Ÿå­¦ä¹ å¹¶ç”Ÿæˆé«˜è´¨é‡çš„å¤šæ ·åŒ–å¯è§†åŒ–å†…å®¹ã€‚Multimodal DeepResearcher é‡‡ç”¨ä»£ç†æ¡†æ¶ (Agentic Framework)ï¼Œå°†ä»»åŠ¡åˆ†è§£ä¸ºç ”ç©¶ã€èŒƒä¾‹æŠ¥å‘Šæ–‡æœ¬åŒ–ã€è§„åˆ’å’Œå¤šæ¨¡æ€æŠ¥å‘Šç”Ÿæˆå››ä¸ªæ ¸å¿ƒé˜¶æ®µã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜æ„å»ºäº†åŒ…å« 100 ä¸ªå¤šæ ·åŒ–ä¸»é¢˜å’Œ 5 é¡¹è¯„ä¼°æŒ‡æ ‡çš„ MultimodalReportBench ä»¥è¿›è¡Œç³»ç»Ÿæ€§è¯„ä¼°ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¸åŒæ¨¡å‹ä¸‹å‡å…·æœ‰æ˜¾è‘—æœ‰æ•ˆæ€§ï¼Œåœ¨ä½¿ç”¨ Claude 3.7 Sonnet æ—¶ç›¸æ¯”åŸºçº¿æ–¹æ³•å–å¾—äº† 82% çš„ç»¼åˆèƒœç‡ã€‚è¿™é¡¹å·¥ä½œä¸ºè‡ªåŠ¨åŒ–ç”Ÿæˆå›¾æ–‡å¹¶èŒ‚çš„æ·±åº¦ç ”ç©¶æŠ¥å‘Šæä¾›äº†æ–°çš„èŒƒå¼å’Œè¯„ä¼°æ ‡å‡†ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "AAAI 2026 Oral",
      "pdf_url": "https://arxiv.org/pdf/2506.02454v3",
      "published_date": "2025-06-03 05:18:19 UTC",
      "updated_date": "2025-12-14 05:15:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:17:44.684087+00:00"
    },
    {
      "arxiv_id": "2506.02448v1",
      "title": "VidEvent: A Large Dataset for Understanding Dynamic Evolution of Events in Videos",
      "title_zh": "VidEventï¼šç”¨äºç†è§£è§†é¢‘äº‹ä»¶åŠ¨æ€æ¼”å˜çš„å¤§è§„æ¨¡æ•°æ®é›†",
      "authors": [
        "Baoyu Liang",
        "Qile Su",
        "Shoutai Zhu",
        "Yuchen Liang",
        "Chao Tong"
      ],
      "abstract": "Despite the significant impact of visual events on human cognition, understanding events in videos remains a challenging task for AI due to their complex structures, semantic hierarchies, and dynamic evolution. To address this, we propose the task of video event understanding that extracts event scripts and makes predictions with these scripts from videos. To support this task, we introduce VidEvent, a large-scale dataset containing over 23,000 well-labeled events, featuring detailed event structures, broad hierarchies, and logical relations extracted from movie recap videos. The dataset was created through a meticulous annotation process, ensuring high-quality and reliable event data. We also provide comprehensive baseline models offering detailed descriptions of their architecture and performance metrics. These models serve as benchmarks for future research, facilitating comparisons and improvements. Our analysis of VidEvent and the baseline models highlights the dataset's potential to advance video event understanding and encourages the exploration of innovative algorithms and models. The dataset and related resources are publicly available at www.videvent.top.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†é¢‘äº‹ä»¶ç†è§£åœ¨å¤æ‚ç»“æ„ã€è¯­ä¹‰å±‚çº§å’ŒåŠ¨æ€æ¼”åŒ–æ–¹é¢çš„æŒ‘æˆ˜ï¼Œæå‡ºäº† VidEvent æ•°æ®é›†ï¼Œæ—¨åœ¨æ”¯æŒä»è§†é¢‘ä¸­æå–äº‹ä»¶è„šæœ¬(event scripts)å¹¶è¿›è¡Œé¢„æµ‹çš„ä»»åŠ¡ã€‚VidEvent æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡ 23,000 ä¸ªä»ç”µå½±è§£è¯´è§†é¢‘(movie recap videos)ä¸­æå–çš„æ ‡æ³¨è‰¯å¥½çš„äº‹ä»¶ï¼Œå…·æœ‰è¯¦ç»†çš„äº‹ä»¶ç»“æ„ã€å¹¿æ³›çš„å±‚çº§ä½“ç³»å’Œé€»è¾‘å…³ç³»ã€‚é€šè¿‡ç»†è‡´çš„æ ‡æ³¨è¿‡ç¨‹ï¼Œè¯¥ç ”ç©¶ç¡®ä¿äº†é«˜è´¨é‡å’Œå¯é çš„äº‹ä»¶æ•°æ®ï¼Œå¹¶æä¾›äº†å…¨é¢çš„åŸºçº¿æ¨¡å‹(baseline models)åŠå…¶æ¶æ„å’Œæ€§èƒ½æŒ‡æ ‡ã€‚å®éªŒåˆ†æè¯æ˜äº† VidEvent åœ¨æ¨åŠ¨è§†é¢‘äº‹ä»¶ç†è§£é¢†åŸŸçš„æ½œåŠ›ï¼Œä¸ºæœªæ¥çš„ç®—æ³•æ”¹è¿›æä¾›äº†é‡è¦çš„åŸºå‡†å’Œèµ„æºï¼Œç›¸å…³æ•°æ®ç›®å‰å·²åœ¨ www.videvent.top å…¬å¼€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02448v1",
      "published_date": "2025-06-03 05:12:48 UTC",
      "updated_date": "2025-06-03 05:12:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:17:43.299160+00:00"
    },
    {
      "arxiv_id": "2506.02438v1",
      "title": "A Review of Various Datasets for Machine Learning Algorithm-Based Intrusion Detection System: Advances and Challenges",
      "title_zh": "åŸºäºæœºå™¨å­¦ä¹ ç®—æ³•çš„å…¥ä¾µæ£€æµ‹ç³»ç»Ÿå„ç±»æ•°æ®é›†ç»¼è¿°ï¼šè¿›å±•ä¸æŒ‘æˆ˜",
      "authors": [
        "Sudhanshu Sekhar Tripathy",
        "Bichitrananda Behera"
      ],
      "abstract": "IDS aims to protect computer networks from security threats by detecting, notifying, and taking appropriate action to prevent illegal access and protect confidential information. As the globe becomes increasingly dependent on technology and automated processes, ensuring secured systems, applications, and networks has become one of the most significant problems of this era. The global web and digital technology have significantly accelerated the evolution of the modern world, necessitating the use of telecommunications and data transfer platforms. Researchers are enhancing the effectiveness of IDS by incorporating popular datasets into machine learning algorithms. IDS, equipped with machine learning classifiers, enhances security attack detection accuracy by identifying normal or abnormal network traffic. This paper explores the methods of capturing and reviewing intrusion detection systems (IDS) and evaluates the challenges existing datasets face. A deluge of research on machine learning (ML) and deep learning (DL) architecture-based intrusion detection techniques has been conducted in the past ten years on various cybersecurity datasets, including KDDCUP'99, NSL-KDD, UNSW-NB15, CICIDS-2017, and CSE-CIC-IDS2018. We conducted a literature review and presented an in-depth analysis of various intrusion detection methods that use SVM, KNN, DT, LR, NB, RF, XGBOOST, Adaboost, and ANN. We provide an overview of each technique, explaining the role of the classifiers and algorithms used. A detailed tabular analysis highlights the datasets used, classifiers employed, attacks detected, evaluation metrics, and conclusions drawn. This article offers a thorough review for future IDS research.",
      "tldr_zh": "è¯¥ç»¼è¿°å¯¹ç”¨äºåŸºäºæœºå™¨å­¦ä¹ (ML)ç®—æ³•çš„å…¥ä¾µæ£€æµ‹ç³»ç»Ÿ(IDS)çš„å„ç§æ•°æ®é›†è¿›è¡Œäº†å…¨é¢å›é¡¾ï¼Œå¹¶æ·±å…¥æ¢è®¨äº†è¯¥é¢†åŸŸçš„æœ€æ–°è¿›å±•ä¸é¢ä¸´çš„æŒ‘æˆ˜ã€‚éšç€å…¨çƒå¯¹æ•°å­—åŒ–è½¬å‹å’Œè‡ªåŠ¨åŒ–æµç¨‹ä¾èµ–ç¨‹åº¦çš„åŠ æ·±ï¼Œç ”ç©¶äººå‘˜é€šè¿‡å°†çƒ­é—¨æ•°æ®é›†æ•´åˆåˆ°æœºå™¨å­¦ä¹ åˆ†ç±»å™¨ä¸­ï¼Œæ˜¾è‘—æé«˜äº†è¯†åˆ«ç½‘ç»œæµé‡ä¸­æ­£å¸¸æˆ–å¼‚å¸¸æ”»å‡»è¡Œä¸ºçš„å‡†ç¡®æ€§ã€‚æ–‡ç« é‡ç‚¹è¯„ä¼°äº†è¿‡å»åå¹´ä¸­åœ¨KDDCUP'99ã€NSL-KDDã€UNSW-NB15ã€CICIDS-2017å’ŒCSE-CIC-IDS2018ç­‰å…³é”®æ•°æ®é›†ä¸Šå¼€å±•çš„å¤§é‡æœºå™¨å­¦ä¹ (ML)ä¸æ·±åº¦å­¦ä¹ (DL)ç ”ç©¶ã€‚ç ”ç©¶è¯¦ç»†åˆ†æäº†åŒ…æ‹¬SVMã€KNNã€DTã€LRã€NBã€RFã€XGBOOSTã€Adaboostå’ŒANNåœ¨å†…çš„å¤šç§ç®—æ³•ï¼Œå¹¶è§£é‡Šäº†å„åˆ†ç±»å™¨åœ¨å®‰å…¨é˜²æŠ¤ä¸­çš„å…·ä½“ä½œç”¨ã€‚é€šè¿‡è¯¦å°½çš„è¡¨æ ¼åŒ–å¯¹æ¯”ï¼Œè®ºæ–‡æ€»ç»“äº†ä¸åŒå®éªŒä¸­ä½¿ç”¨çš„æ•°æ®é›†ã€åˆ†ç±»å™¨ã€æ£€æµ‹åˆ°çš„æ”»å‡»ç±»å‹ã€è¯„ä¼°æŒ‡æ ‡ä»¥åŠæ ¸å¿ƒç»“è®ºã€‚è¯¥ç ”ç©¶ä¸ºæœªæ¥å…¥ä¾µæ£€æµ‹ç³»ç»Ÿ(IDS)çš„å¼€å‘å’Œæ€§èƒ½ä¼˜åŒ–æä¾›äº†æ·±å…¥çš„å‚è€ƒæ¡†æ¶ä¸ç ”ç©¶æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02438v1",
      "published_date": "2025-06-03 04:47:21 UTC",
      "updated_date": "2025-06-03 04:47:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:17:50.592888+00:00"
    },
    {
      "arxiv_id": "2506.03214v1",
      "title": "A Pre-trained Framework for Multilingual Brain Decoding Using Non-invasive Recordings",
      "title_zh": "åŸºäºéä¾µå…¥å¼è®°å½•çš„å¤šè¯­è¨€å¤§è„‘è§£ç é¢„è®­ç»ƒæ¡†æ¶",
      "authors": [
        "Yi Guo",
        "Yihang Dong",
        "Michael Kwok-Po Ng",
        "Shuqiang Wang"
      ],
      "abstract": "Brain-computer interfaces (BCIs) with speech decoding from brain recordings have broad application potential in fields such as clinical rehabilitation and cognitive neuroscience. However, current decoding methods remain limited to single-language, single-subject, and single neuroimaging modality settings, restricting their clinical applicability and generalizability. Here we propose a joint multilingual, multi-subject and multimodal decoding framework. It maps diverse brain recordings into a unified semantic space defined by a pre-trained multilingual model (PMM), enabling decoding across multiple languages, multiple subjects and multiple neuroimaging modalities. The proposed framework is validated using non-invasive brain recordings from 159 participants across four languages. Experimental results show that it exhibits strong generalization across multilingual, multi-subject, and multimodal settings. More importantly, the proposed framework can promote linguistic fairness, which is vital for underrepresented languages in BCI applications. The unified semantic space enables cross-lingual mapping enhancement, allowing the framework to boost the decoding performance of underrepresented languages, thereby promoting linguistic fairness. Overall, the proposed framework establishes a new potential paradigm for brain decoding, opening new paths for broader applications of BCI.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹éä¾µå…¥å¼è®°å½•çš„å¤šè¯­è¨€è„‘ç”µè§£ç é¢„è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨çªç ´å½“å‰è§£ç æ–¹æ³•åœ¨å•è¯­è¨€ã€å•å—è¯•è€…å’Œå•ç¥ç»å½±åƒæ¨¡æ€è®¾ç½®ä¸‹çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒå¤šè¯­è¨€æ¨¡å‹(PMM)å®šä¹‰äº†ä¸€ä¸ªç»Ÿä¸€è¯­ä¹‰ç©ºé—´ï¼Œå°†å¤šæ ·åŒ–çš„è„‘è®°å½•è¿›è¡Œæ˜ å°„ï¼Œå®ç°äº†è·¨è¯­è¨€ã€è·¨å—è¯•è€…å’Œè·¨æ¨¡æ€çš„è§£ç ã€‚é€šè¿‡å¯¹æ¶µç›–å››ç§è¯­è¨€ã€159åå‚ä¸è€…çš„éä¾µå…¥å¼è„‘è®°å½•è¿›è¡ŒéªŒè¯ï¼Œå®éªŒç»“æœè¯æ˜è¯¥æ¡†æ¶åœ¨å¤šåœºæ™¯ä¸‹å…·æœ‰æ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é€šè¿‡è·¨è¯­è¨€æ˜ å°„å¢å¼º(cross-lingual mapping enhancement)æå‡äº†ä»£è¡¨æ€§ä¸è¶³è¯­è¨€çš„è§£ç æ€§èƒ½ï¼Œæœ‰æ•ˆä¿ƒè¿›äº†è„‘æœºæ¥å£(BCI)åº”ç”¨ä¸­çš„è¯­è¨€å…¬å¹³æ€§ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥æ¡†æ¶ä¸ºè„‘è§£ç å»ºç«‹äº†ä¸€ç§æ–°çš„èŒƒå¼ï¼Œä¸ºè„‘æœºæ¥å£åœ¨ä¸´åºŠåº·å¤å’Œè®¤çŸ¥ç¥ç»ç§‘å­¦é¢†åŸŸçš„å¹¿æ³›åº”ç”¨å¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03214v1",
      "published_date": "2025-06-03 04:34:22 UTC",
      "updated_date": "2025-06-03 04:34:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:17:53.470179+00:00"
    },
    {
      "arxiv_id": "2506.02426v2",
      "title": "Comparative Analysis of AI Agent Architectures for Entity Relationship Classification",
      "title_zh": "å®ä½“å…³ç³»åˆ†ç±»ä¸­ AI æ™ºèƒ½ä½“æ¶æ„çš„å¯¹æ¯”åˆ†æ",
      "authors": [
        "Maryam Berijanian",
        "Kuldeep Singh",
        "Amin Sehati"
      ],
      "abstract": "Entity relationship classification remains a challenging task in information extraction, especially in scenarios with limited labeled data and complex relational structures. In this study, we conduct a comparative analysis of three distinct AI agent architectures designed to perform relation classification using large language models (LLMs). The agentic architectures explored include (1) reflective self-evaluation, (2) hierarchical task decomposition, and (3) a novel multi-agent dynamic example generation mechanism, each leveraging different modes of reasoning and prompt adaptation. In particular, our dynamic example generation approach introduces real-time cooperative and adversarial prompting. We systematically compare their performance across multiple domains and model backends. Our experiments demonstrate that multi-agent coordination consistently outperforms standard few-shot prompting and approaches the performance of fine-tuned models. These findings offer practical guidance for the design of modular, generalizable LLM-based systems for structured relation extraction. The source codes and dataset are available at https://github.com/maryambrj/ALIEN.git.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹åœ¨æœ‰é™æ ‡è®°æ•°æ®å’Œå¤æ‚å…³ç³»ç»“æ„ä¸‹çš„å®ä½“å…³ç³»åˆ†ç±» (Entity Relationship Classification) æŒ‘æˆ˜ï¼Œå¯¹ä¸‰ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„ AI Agent æ¶æ„è¿›è¡Œäº†å¯¹æ¯”åˆ†æã€‚ç ”ç©¶æ¢ç´¢çš„æ¶æ„åŒ…æ‹¬åæ€æ€§è‡ªæˆ‘è¯„ä¼° (Reflective Self-evaluation)ã€å±‚æ¬¡åŒ–ä»»åŠ¡åˆ†è§£ (Hierarchical Task Decomposition) ä»¥åŠä¸€ç§æ–°å‹çš„å¤šæ™ºèƒ½ä½“åŠ¨æ€ç¤ºä¾‹ç”Ÿæˆæœºåˆ¶ (Multi-agent Dynamic Example Generation)ã€‚è¿™ç§åŠ¨æ€ç¤ºä¾‹ç”Ÿæˆæ–¹æ³•å¼•å…¥äº†å®æ—¶åä½œä¸å¯¹æŠ—æ€§æç¤º (Cooperative and Adversarial Prompting)ï¼Œåˆ©ç”¨ä¸åŒçš„æ¨ç†æ¨¡å¼å’Œæç¤ºé€‚é…æ¥æå‡æ€§èƒ½ã€‚å®éªŒåœ¨å¤šä¸ªé¢†åŸŸå’Œæ¨¡å‹åç«¯ç³»ç»Ÿåœ°æ¯”è¾ƒäº†è¿™äº›æ¶æ„çš„è¡¨ç°ï¼Œç»“æœè¡¨æ˜å¤šæ™ºèƒ½ä½“åè°ƒ (Multi-agent Coordination) åœ¨æ€§èƒ½ä¸Šå§‹ç»ˆä¼˜äºæ ‡å‡†çš„ Few-shot Promptingï¼Œå¹¶æ¥è¿‘å¾®è°ƒæ¨¡å‹ (Fine-tuned Models) çš„æ°´å¹³ã€‚è¯¥å‘ç°ä¸ºæ„å»ºæ¨¡å—åŒ–ã€å¯æ¨å¹¿çš„ç»“æ„åŒ–å…³ç³»æå– LLM ç³»ç»Ÿæä¾›äº†å®é™…æŒ‡å¯¼ï¼Œç›¸å…³æºç å’Œæ•°æ®é›†å·²åœ¨ GitHub çš„ ALIEN é¡¹ç›®ä¸­å…¬å¼€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02426v2",
      "published_date": "2025-06-03 04:19:47 UTC",
      "updated_date": "2025-06-04 14:21:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:18:15.509400+00:00"
    },
    {
      "arxiv_id": "2506.02415v1",
      "title": "AERO: A Redirection-Based Optimization Framework Inspired by Judo for Robust Probabilistic Forecasting",
      "title_zh": "AEROï¼šå—æŸ”é“å¯å‘ã€åŸºäºé‡å®šå‘çš„é²æ£’æ¦‚ç‡é¢„æµ‹ä¼˜åŒ–æ¡†æ¶",
      "authors": [
        "Karthikeyan Vaiapury"
      ],
      "abstract": "Optimization remains a fundamental pillar of machine learning, yet existing methods often struggle to maintain stability and adaptability in dynamic, non linear systems, especially under uncertainty. We introduce AERO (Adversarial Energy-based Redirection Optimization), a novel framework inspired by the redirection principle in Judo, where external disturbances are leveraged rather than resisted. AERO reimagines optimization as a redirection process guided by 15 interrelated axioms encompassing adversarial correction, energy conservation, and disturbance-aware learning. By projecting gradients, integrating uncertainty driven dynamics, and managing learning energy, AERO offers a principled approach to stable and robust model updates. Applied to probabilistic solar energy forecasting, AERO demonstrates substantial gains in predictive accuracy, reliability, and adaptability, especially in noisy and uncertain environments. Our findings highlight AERO as a compelling new direction in the theoretical and practical landscape of optimization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AERO (Adversarial Energy-based Redirection Optimization)ï¼Œè¿™æ˜¯ä¸€ç§å—æŸ”é“ (Judo) é‡å®šå‘åŸåˆ™å¯å‘çš„åˆ›æ–°ä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æœºå™¨å­¦ä¹ ä¸­åŠ¨æ€éçº¿æ€§ç³»ç»Ÿåœ¨ä¸ç¡®å®šæ€§ä¸‹çš„ç¨³å®šæ€§å’Œé€‚åº”æ€§æŒ‘æˆ˜ã€‚AERO å°†ä¼˜åŒ–é‡æ–°å®šä¹‰ä¸ºé‡å®šå‘è¿‡ç¨‹ï¼Œé€šè¿‡ 15 ä¸ªæ¶µç›–å¯¹æŠ—æ€§æ ¡æ­£ (adversarial correction)ã€èƒ½é‡å®ˆæ’ (energy conservation) å’Œå¹²æ‰°æ„ŸçŸ¥å­¦ä¹  (disturbance-aware learning) çš„å…¬ç†è¿›è¡Œå¼•å¯¼ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æŠ•å½±æ¢¯åº¦ (projecting gradients)ã€æ•´åˆä¸ç¡®å®šæ€§é©±åŠ¨åŠ¨åŠ›å­¦å¹¶ç®¡ç†å­¦ä¹ èƒ½é‡ (learning energy)ï¼Œä¸ºå®ç°é²æ£’çš„æ¨¡å‹æ›´æ–°æä¾›äº†ç³»ç»ŸåŒ–æ–¹æ³•ã€‚åœ¨æ¦‚ç‡å¤ªé˜³èƒ½é¢„æµ‹ (probabilistic solar energy forecasting) çš„åº”ç”¨ä¸­ï¼ŒAERO æ˜¾è‘—æå‡äº†é¢„æµ‹å‡†ç¡®æ€§ã€å¯é æ€§å’Œé€‚åº”æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜å™ªå£°å’Œä¸ç¡®å®šç¯å¢ƒä¸‹çš„è¡¨ç°ä¼˜å¼‚ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ AERO ä¸ºä¼˜åŒ–é¢†åŸŸçš„ç†è®ºä¸å®è·µæ¢ç´¢æä¾›äº†ä¸€ä¸ªå…¼å…·é²æ£’æ€§ä¸ç¨³å®šæ€§çš„æ–°æ–¹å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 1 figure, submitted to NeurIPS 2025 (preprint version)",
      "pdf_url": "https://arxiv.org/pdf/2506.02415v1",
      "published_date": "2025-06-03 04:02:20 UTC",
      "updated_date": "2025-06-03 04:02:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:18:09.809546+00:00"
    },
    {
      "arxiv_id": "2506.02412v1",
      "title": "SingaKids: A Multilingual Multimodal Dialogic Tutor for Language Learning",
      "title_zh": "SingaKidsï¼šé¢å‘è¯­è¨€å­¦ä¹ çš„å¤šè¯­è¨€å¤šæ¨¡æ€å¯¹è¯å¼å¯¼å¸ˆ",
      "authors": [
        "Zhengyuan Liu",
        "Geyu Lin",
        "Hui Li Tan",
        "Huayun Zhang",
        "Yanfeng Lu",
        "Xiaoxue Gao",
        "Stella Xin Yin",
        "He Sun",
        "Hock Huan Goh",
        "Lung Hsiang Wong",
        "Nancy F. Chen"
      ],
      "abstract": "The integration of generative artificial intelligence into educational applications has enhanced personalized and interactive learning experiences, and it shows strong potential to promote young learners language acquisition. However, it is still challenging to ensure consistent and robust performance across different languages and cultural contexts, and kids-friendly design requires simplified instructions, engaging interactions, and age-appropriate scaffolding to maintain motivation and optimize learning outcomes. In this work, we introduce SingaKids, a dialogic tutor designed to facilitate language learning through picture description tasks. Our system integrates dense image captioning, multilingual dialogic interaction, speech understanding, and engaging speech generation to create an immersive learning environment in four languages: English, Mandarin, Malay, and Tamil. We further improve the system through multilingual pre-training, task-specific tuning, and scaffolding optimization. Empirical studies with elementary school students demonstrate that SingaKids provides effective dialogic teaching, benefiting learners at different performance levels.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†SingaKidsï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨é€šè¿‡å›¾ç‰‡æè¿°ä»»åŠ¡ä¿ƒè¿›è¯­è¨€å­¦ä¹ çš„å¤šè¯­è¨€å¤šæ¨¡æ€å¯¹è¯å¯¼å¸ˆç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé’ˆå¯¹ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨æ•™è‚²åº”ç”¨ä¸­é¢ä¸´çš„è·¨è¯­è¨€æ–‡åŒ–ä¸€è‡´æ€§ä»¥åŠå„¿ç«¥å‹å¥½å‹è®¾è®¡ç­‰æŒ‘æˆ˜ï¼Œæä¾›äº†ç®€åŒ–çš„æŒ‡ä»¤å’Œè¶£å‘³æ€§äº’åŠ¨ã€‚SingaKidsé›†æˆäº†Dense Image Captioningã€å¤šè¯­è¨€å¯¹è¯äº¤äº’ã€è¯­éŸ³ç†è§£å’Œè¯­éŸ³ç”ŸæˆæŠ€æœ¯ï¼Œæ”¯æŒè‹±è¯­ã€åè¯­ã€é©¬æ¥è¯­å’Œæ³°ç±³å°”è¯­å››ç§è¯­è¨€ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡å¤šè¯­è¨€é¢„è®­ç»ƒ(Multilingual Pre-training)ã€ç‰¹å®šä»»åŠ¡å¾®è°ƒ(Task-specific Tuning)å’Œæ”¯æ¶ä¼˜åŒ–(Scaffolding Optimization)è¿›ä¸€æ­¥å¢å¼ºäº†ç³»ç»Ÿæ€§èƒ½ã€‚å¯¹å°å­¦ç”Ÿè¿›è¡Œçš„å®è¯ç ”ç©¶è¡¨æ˜ï¼ŒSingaKidsèƒ½å¤Ÿæä¾›æœ‰æ•ˆçš„å¯¹è¯å¼æ•™å­¦ï¼Œä½¿ä¸åŒèƒ½åŠ›æ°´å¹³çš„å­¦ä¹ è€…åœ¨æ²‰æµ¸å¼ç¯å¢ƒä¸­è·ç›Šã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2025 Industry Track",
      "pdf_url": "https://arxiv.org/pdf/2506.02412v1",
      "published_date": "2025-06-03 03:56:45 UTC",
      "updated_date": "2025-06-03 03:56:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:18:22.355056+00:00"
    },
    {
      "arxiv_id": "2506.02406v1",
      "title": "Random at First, Fast at Last: NTK-Guided Fourier Pre-Processing for Tabular DL",
      "title_zh": "éšæœºèµ·æ­¥ï¼Œæé€Ÿæ”¶æ•›ï¼šé¢å‘è¡¨æ ¼æ·±åº¦å­¦ä¹ çš„ NTK å¼•å¯¼å‚…é‡Œå¶é¢„å¤„ç†",
      "authors": [
        "Renat Sergazinov",
        "Jing Wu",
        "Shao-An Yin"
      ],
      "abstract": "While random Fourier features are a classic tool in kernel methods, their utility as a pre-processing step for deep learning on tabular data has been largely overlooked. Motivated by shortcomings in tabular deep learning pipelines - revealed through Neural Tangent Kernel (NTK) analysis - we revisit and repurpose random Fourier mappings as a parameter-free, architecture-agnostic transformation. By projecting each input into a fixed feature space via sine and cosine projections with frequencies drawn once at initialization, this approach circumvents the need for ad hoc normalization or additional learnable embeddings. We show within the NTK framework that this mapping (i) bounds and conditions the network's initial NTK spectrum, and (ii) introduces a bias that shortens the optimization trajectory, thereby accelerating gradient-based training. These effects pre-condition the network with a stable kernel from the outset. Empirically, we demonstrate that deep networks trained on Fourier-transformed inputs converge more rapidly and consistently achieve strong final performance, often with fewer epochs and less hyperparameter tuning. Our findings establish random Fourier pre-processing as a theoretically motivated, plug-and-play enhancement for tabular deep learning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¡¨æ ¼æ·±åº¦å­¦ä¹ (Tabular DL)æµæ°´çº¿ä¸­çš„ä¸è¶³ï¼Œåˆ©ç”¨ç¥ç»åˆ‡çº¿æ ¸(NTK)ç†è®ºæå‡ºäº†ä¸€ç§åŸºäºéšæœºå‚…é‡Œå¶æ˜ å°„(Random Fourier mappings)çš„é¢„å¤„ç†æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨åˆå§‹åŒ–æ—¶ä¸€æ¬¡æ€§éšæœºæŠ½å–çš„é¢‘ç‡è¿›è¡Œæ­£å¼¦å’Œä½™å¼¦æŠ•å½±ï¼Œå°†è¾“å…¥æ˜ å°„åˆ°å›ºå®šçš„ç‰¹å¾ç©ºé—´ï¼Œä»è€Œé¿å…äº†ç‰¹è®¾çš„å½’ä¸€åŒ–å¤„ç†æˆ–é¢å¤–çš„å¯å­¦ä¹ åµŒå…¥ã€‚åœ¨NTKæ¡†æ¶ä¸‹çš„ç†è®ºåˆ†æè¡¨æ˜ï¼Œè¿™ç§æ˜ å°„èƒ½å¤Ÿé™å®šå¹¶è°ƒèŠ‚ç½‘ç»œåˆå§‹çš„NTKé¢‘è°±ï¼Œå¹¶é€šè¿‡å¼•å…¥åå·®æ¥ç¼©çŸ­ä¼˜åŒ–è½¨è¿¹ï¼Œä»è€Œæ˜¾è‘—åŠ é€ŸåŸºäºæ¢¯åº¦çš„è®­ç»ƒè¿‡ç¨‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡å‚…é‡Œå¶å˜æ¢å¤„ç†åçš„æ¨¡å‹æ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼Œä¸”åœ¨æ›´å°‘çš„è¿­ä»£å‘¨æœŸå’Œæ›´å°‘çš„è¶…å‚æ•°å¾®è°ƒä¸‹ï¼Œä¾ç„¶èƒ½ç¨³å®šè·å¾—ä¼˜å¼‚çš„æœ€ç»ˆæ€§èƒ½ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†éšæœºå‚…é‡Œå¶é¢„å¤„ç†æ˜¯ä¸€ç§å…·æœ‰ç†è®ºåŸºç¡€ä¸”å³æ’å³ç”¨(plug-and-play)çš„è¡¨æ ¼æ·±åº¦å­¦ä¹ å¢å¼ºæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 3 figures, 1 table",
      "pdf_url": "https://arxiv.org/pdf/2506.02406v1",
      "published_date": "2025-06-03 03:45:13 UTC",
      "updated_date": "2025-06-03 03:45:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:18:53.088904+00:00"
    },
    {
      "arxiv_id": "2506.02404v3",
      "title": "GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation",
      "title_zh": "GraphRAG-Benchï¼šæ—¨åœ¨è¯„ä¼°å›¾æ£€ç´¢å¢å¼ºç”Ÿæˆçš„é¢†åŸŸç‰¹å®šæŒ‘æˆ˜æ€§æ¨ç†",
      "authors": [
        "Yilin Xiao",
        "Junnan Dong",
        "Chuang Zhou",
        "Su Dong",
        "Qian-wen Zhang",
        "Di Yin",
        "Xing Sun",
        "Xiao Huang"
      ],
      "abstract": "Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing recognition for its potential to enhance large language models (LLMs) by structurally organizing domain-specific corpora and facilitating complex reasoning. However, current evaluations of GraphRAG models predominantly rely on traditional question-answering datasets. Their limited scope in questions and evaluation metrics fails to comprehensively assess the reasoning capacity improvements enabled by GraphRAG models. To address this gap, we introduce GraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously evaluate GraphRAG models. Our benchmark offers three key superiorities: \\((i)\\) Challenging question design. Featuring college-level, domain-specific questions that demand multi-hop reasoning, the benchmark ensures that simple content retrieval is insufficient for problem-solving. For example, some questions require mathematical reasoning or programming. \\((ii)\\) Diverse task coverage. The dataset includes a broad spectrum of reasoning tasks, multiple-choice, true/false, multi-select, open-ended, and fill-in-the-blank. It spans 16 disciplines in twenty core textbooks. \\((iii)\\) Holistic evaluation framework. GraphRAG-Bench provides comprehensive assessment across the entire GraphRAG pipeline, including graph construction, knowledge retrieval, and answer generation. Beyond final-answer correctness, it evaluates the logical coherence of the reasoning process. By applying nine contemporary GraphRAG methods to GraphRAG-Bench, we demonstrate its utility in quantifying how graph-based structuring improves model reasoning capabilities. Our analysis reveals critical insights about graph architectures, retrieval efficacy, and reasoning capabilities, offering actionable guidance for the research community.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GraphRAG-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä¸¥æ ¼è¯„ä¼°å›¾æ£€ç´¢å¢å¼ºç”Ÿæˆ(GraphRAG)æ¨¡å‹çš„å¤§è§„æ¨¡é¢†åŸŸç‰¹å®šåŸºå‡†æµ‹è¯•ã€‚é’ˆå¯¹ç°æœ‰è¯„ä¼°æ–¹æ³•åœ¨é—®é¢˜èŒƒå›´å’Œè¯„ä¼°æŒ‡æ ‡ä¸Šæ— æ³•å…¨é¢è¡¡é‡æ¨ç†èƒ½åŠ›æå‡çš„å±€é™æ€§ï¼Œè¯¥åŸºå‡†è®¾è®¡äº†å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤§å­¦æ°´å¹³é¢†åŸŸé—®é¢˜ï¼Œè¦æ±‚è¿›è¡Œæ¶‰åŠæ•°å­¦æ¨ç†æˆ–ç¼–ç¨‹çš„å¤šè·³æ¨ç†ã€‚GraphRAG-Bench è¦†ç›–äº† 16 ä¸ªå­¦ç§‘çš„ 20 æœ¬æ ¸å¿ƒæ•™ç§‘ä¹¦ï¼Œå¹¶åŒ…å«ä»é€‰æ‹©é¢˜åˆ°å¼€æ”¾å¼é—®é¢˜ç­‰å¤šç§ä»»åŠ¡ç±»å‹ã€‚è¯¥æ¡†æ¶å®ç°äº†å¯¹ GraphRAG å…¨æµç¨‹çš„ç»¼åˆè¯„ä¼°ï¼ŒåŒ…æ‹¬å›¾æ„å»º(graph construction)ã€çŸ¥è¯†æ£€ç´¢(knowledge retrieval)å’Œç­”æ¡ˆç”Ÿæˆ(answer generation)ï¼Œå¹¶å¼ºè°ƒæ¨ç†é€»è¾‘çš„è¿è´¯æ€§ã€‚é€šè¿‡åº”ç”¨ 9 ç§ä¸»æµ GraphRAG æ–¹æ³•è¿›è¡Œæµ‹è¯•ï¼Œè¯¥ç ”ç©¶é‡åŒ–äº†å›¾ç»“æ„å¯¹æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ”¹è¿›ï¼Œä¸ºä¼˜åŒ–å›¾æ¶æ„å’Œæ£€ç´¢æ•ˆç‡æä¾›äº†å®ç”¨çš„ç ”ç©¶æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02404v3",
      "published_date": "2025-06-03 03:44:26 UTC",
      "updated_date": "2025-06-20 02:42:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:18:41.891095+00:00"
    },
    {
      "arxiv_id": "2506.15717v1",
      "title": "daDPO: Distribution-Aware DPO for Distilling Conversational Abilities",
      "title_zh": "daDPOï¼šç”¨äºå¯¹è¯èƒ½åŠ›è’¸é¦çš„åˆ†å¸ƒæ„ŸçŸ¥ DPO",
      "authors": [
        "Zhengze Zhang",
        "Shiqi Wang",
        "Yiqun Shen",
        "Simin Guo",
        "Dahua Lin",
        "Xiaoliang Wang",
        "Nguyen Cam-Tu",
        "Fei Tan"
      ],
      "abstract": "Large language models (LLMs) have demonstrated exceptional performance across various applications, but their conversational abilities decline sharply as model size decreases, presenting a barrier to their deployment in resource-constrained environments. Knowledge distillation with Direct Preference Optimization (dDPO) has emerged as a promising approach to enhancing the conversational abilities of smaller models using a larger teacher model. However, current methods primarily focus on 'black-box' KD, which only uses the teacher's responses, overlooking the output distribution offered by the teacher. This paper addresses this gap by introducing daDPO (Distribution-Aware DPO), a unified method for preference optimization and distribution-based distillation. We provide rigorous theoretical analysis and empirical validation, showing that daDPO outperforms existing methods in restoring performance for pruned models and enhancing smaller LLM models. Notably, in in-domain evaluation, our method enables a 20% pruned Vicuna1.5-7B to achieve near-teacher performance (-7.3% preference rate compared to that of dDPO's -31%), and allows Qwen2.5-1.5B to occasionally outperform its 7B teacher model (14.0% win rate).",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†daDPO (Distribution-Aware DPO)ï¼Œä¸€ç§æ—¨åœ¨å¢å¼ºå°å‹å¤§è¯­è¨€æ¨¡å‹ (LLMs) å¯¹è¯èƒ½åŠ›çš„åˆ†å¸ƒæ„ŸçŸ¥åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œæœ‰æ•ˆè§£å†³äº†ç°æœ‰çŸ¥è¯†è’¸é¦ (KD) è¿‡ç¨‹ä¸­å¿½ç•¥æ•™å¸ˆæ¨¡å‹è¾“å‡ºåˆ†å¸ƒçš„é—®é¢˜ã€‚é€šè¿‡å°†åå¥½ä¼˜åŒ–ä¸åŸºäºåˆ†å¸ƒçš„è’¸é¦ç»Ÿä¸€åœ¨ä¸€ä¸ªæ¡†æ¶å†…ï¼ŒdaDPO åœ¨æå‡æ¨¡å‹æ€§èƒ½å’Œæ¢å¤å‰ªææ¨¡å‹èƒ½åŠ›æ–¹é¢è¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ä½¿å‰ªæ 20% çš„ Vicuna1.5-7B æ¨¡å‹è¾¾åˆ°äº†æ¥è¿‘æ•™å¸ˆæ¨¡å‹çš„æ°´å¹³ï¼Œå°†åå¥½ç‡å·®è·ä» dDPO çš„ -31% å¤§å¹…ç¼©å°è‡³ -7.3%ã€‚æ­¤å¤–ï¼ŒdaDPO è¿˜ä¿ƒä½¿ Qwen2.5-1.5B åœ¨ç‰¹å®šè¯„ä¼°ä¸­ä»¥ 14.0% çš„èƒœç‡è¶…è¶Šäº†å…¶ 7B è§„æ¨¡çš„æ•™å¸ˆæ¨¡å‹ï¼Œå……åˆ†è¯æ˜äº†åˆ†å¸ƒä¿¡æ¯åœ¨å¯¹è¯èƒ½åŠ›è’¸é¦ä¸­çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15717v1",
      "published_date": "2025-06-03 03:39:29 UTC",
      "updated_date": "2025-06-03 03:39:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:18:48.893725+00:00"
    },
    {
      "arxiv_id": "2506.02397v3",
      "title": "OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for Over-Reasoning Mitigation",
      "title_zh": "OThink-R1ï¼šæ—¨åœ¨ç¼“è§£è¿‡åº¦æ¨ç†çš„å†…åœ¨å¿«æ…¢æ€è€ƒæ¨¡å¼åˆ‡æ¢",
      "authors": [
        "Shengjia Zhang",
        "Junjie Wu",
        "Jiawei Chen",
        "Changwang Zhang",
        "Zhe Li",
        "Xingyu Lou",
        "Wangchunshu Zhou",
        "Sheng Zhou",
        "Can Wang",
        "Jun Wang"
      ],
      "abstract": "Human cognition operates through two complementary modes: fast intuitive thinking and slow deliberate thinking. Vanilla large language models (LLMs) predominantly follow the fast-thinking paradigm, producing immediate responses; while recent large reasoning models (LRMs) adopt slow-thinking strategies, generating detailed reasoning chains before arriving at answers. While LRMs often achieve higher accuracy, this comes at the cost of substantially increased token usage. To address this efficiency-accuracy trade-off, we propose OThink-R1, a hybrid reasoning framework that integrates both modes within a single LRM and enables automatic mode switching based on problem characteristics. We first identify three major patterns of essential and redundant reasoning trajectories in LRMs, which guide the design of an auxiliary LLM-based judge that adaptively determines when slow thinking is necessary. Leveraging the judge's decisions, we construct a hybrid fine-tuning dataset by pruning redundant reasoning to produce fast-thinking samples and retaining complete reasoning for slow-thinking samples. This dataset is then used to fine-tune LRMs, equipping them with inherent autonomous mode-selection capabilities. Extensive experiments on mathematical and question-answering benchmarks show that OThink-R1 reduces reasoning token usage significantly while maintaining competitive accuracy. The code is available at https://github.com/AgenticIR-Lab/OThink-R1.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† OThink-R1ï¼Œä¸€ç§æ—¨åœ¨ç¼“è§£å¤§æ¨ç†æ¨¡å‹ (LRMs) è¿‡åº¦æ¨ç† (Over-Reasoning) é—®é¢˜å¹¶å¹³è¡¡æ•ˆç‡ä¸å‡†ç¡®ç‡çš„æ··åˆæ¨ç†æ¡†æ¶ã€‚å—äººç±»å¿«æ…¢æ€è€ƒæ¨¡å¼çš„å¯å‘ï¼Œè¯¥æ¡†æ¶åœ¨å•ä¸ªæ¨¡å‹ä¸­é›†æˆäº†ç›´è§‰å¼å¿«é€Ÿæ€è€ƒå’Œæ·±æ€ç†Ÿè™‘å¼æ…¢æ€è€ƒï¼Œå¹¶èƒ½æ ¹æ®é—®é¢˜ç‰¹å¾å®ç°è‡ªåŠ¨åˆ‡æ¢ã€‚ä½œè€…é€šè¿‡è¯†åˆ«æ¨ç†è½¨è¿¹ä¸­çš„å†—ä½™æ¨¡å¼ï¼Œè®¾è®¡äº†ä¸€ä¸ªåŸºäº LLM çš„è¾…åŠ©åˆ¤æ–­å™¨æ¥ç¡®å®šæ…¢æ€è€ƒçš„å¿…è¦æ€§ï¼Œå¹¶æ®æ­¤æ„å»ºäº†é€šè¿‡ä¿®å‰ªå†—ä½™æ¨ç†ç”Ÿæˆçš„æ··åˆå¾®è°ƒæ•°æ®é›†ã€‚å¾®è°ƒåçš„æ¨¡å‹å…·å¤‡äº†å†…åœ¨çš„è‡ªä¸»æ¨¡å¼é€‰æ‹©èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘ä¸å¿…è¦çš„è®¡ç®—å¼€é”€ã€‚åœ¨æ•°å­¦å’Œé—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒOThink-R1 åœ¨æ˜¾è‘—é™ä½æ¨ç† token ä½¿ç”¨é‡çš„åŒæ—¶ï¼Œä¾ç„¶ä¿æŒäº†å…·æœ‰ç«äº‰åŠ›çš„å‡†ç¡®ç‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2506.02397v3",
      "published_date": "2025-06-03 03:31:30 UTC",
      "updated_date": "2026-01-06 12:54:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:19:08.084724+00:00"
    },
    {
      "arxiv_id": "2506.14802v1",
      "title": "ss-Mamba: Semantic-Spline Selective State-Space Model",
      "title_zh": "ss-Mambaï¼šè¯­ä¹‰æ ·æ¡é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹",
      "authors": [
        "Zuochen Ye"
      ],
      "abstract": "We propose ss-Mamba, a novel foundation model that enhances time series forecasting by integrating semantic-aware embeddings and adaptive spline-based temporal encoding within a selective state-space modeling framework. Building upon the recent success of Transformer architectures, ss-Mamba adopts the Mamba selective state space model as an efficient alternative that achieves comparable performance while significantly reducing computational complexity from quadratic to linear time. Semantic index embeddings, initialized from pretrained language models, allow effective generalization to previously unseen series through meaningful semantic priors. Additionally, spline-based Kolmogorov-Arnold Networks (KAN) dynamically and interpretably capture complex seasonalities and non-stationary temporal effects, providing a powerful enhancement over conventional temporal feature encodings. Extensive experimental evaluations confirm that ss-Mamba delivers superior accuracy, robustness, and interpretability, demonstrating its capability as a versatile and computationally efficient alternative to traditional Transformer-based models in time-series forecasting.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ss-Mambaï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å¢å¼ºæ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆtime series forecastingï¼‰çš„æ–°å‹åŸºç¡€æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´ï¼ˆselective state-spaceï¼‰å»ºæ¨¡æ¡†æ¶å†…ï¼Œé›†æˆäº†è¯­ä¹‰æ„ŸçŸ¥åµŒå…¥ï¼ˆsemantic-aware embeddingsï¼‰å’ŒåŸºäºæ ·æ¡çš„è‡ªé€‚åº”æ—¶é—´ç¼–ç ã€‚é€šè¿‡é‡‡ç”¨ Mamba æ¨¡å‹ä½œä¸ºæ ¸å¿ƒæ¶æ„ï¼Œss-Mamba æˆåŠŸå°†è®¡ç®—å¤æ‚åº¦ä» Transformer çš„äºŒæ¬¡æ–¹é™ä½ä¸ºçº¿æ€§æ°´å¹³ï¼Œä¸”ä¿æŒäº†æé«˜çš„æ€§èƒ½ã€‚æ¨¡å‹åˆ©ç”¨ä»é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆpretrained language modelsï¼‰åˆå§‹åŒ–çš„è¯­ä¹‰ç´¢å¼•åµŒå…¥ï¼Œé€šè¿‡è¯­ä¹‰å…ˆéªŒå®ç°äº†å¯¹æœªçŸ¥åºåˆ—çš„æœ‰æ•ˆæ³›åŒ–ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†åŸºäºæ ·æ¡çš„ Kolmogorov-Arnold Networks (KAN)ï¼Œèƒ½å¤Ÿä»¥åŠ¨æ€ä¸”å…·å¯è§£é‡Šæ€§çš„æ–¹å¼æ•æ‰å¤æ‚çš„å­£èŠ‚æ€§å’Œéå¹³ç¨³æ—¶é—´æ•ˆåº”ã€‚å®éªŒç»“æœè¯å®ï¼Œss-Mamba åœ¨å‡†ç¡®æ€§ã€é²æ£’æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢å‡ä¼˜äºä¼ ç»Ÿæ¨¡å‹ï¼Œæ˜¯å¤„ç†å¤§è§„æ¨¡æ—¶é—´åºåˆ—é¢„æµ‹ä»»åŠ¡çš„ä¸€ç§é«˜æ•ˆä¸”é€šç”¨çš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.14802v1",
      "published_date": "2025-06-03 03:26:57 UTC",
      "updated_date": "2025-06-03 03:26:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:18:51.384492+00:00"
    },
    {
      "arxiv_id": "2506.02391v1",
      "title": "Consultant Decoding: Yet Another Synergistic Mechanism",
      "title_zh": "Consultant Decodingï¼šåˆä¸€ç§ååŒæœºåˆ¶",
      "authors": [
        "Chuanghao Ding",
        "Jiaping Wang",
        "Ziqing Yang",
        "Xiaoliang Wang",
        "Dahua Lin",
        "Cam-Tu Nguyen",
        "Fei Tan"
      ],
      "abstract": "The synergistic mechanism based on Speculative Decoding (SD) has garnered considerable attention as a simple yet effective approach for accelerating the inference of large language models (LLMs). Nonetheless, the high rejection rates require repeated LLMs calls to validate draft tokens, undermining the overall efficiency gain of SD. In this work, we revisit existing verification mechanisms and propose a novel synergetic mechanism Consultant Decoding (CD). Unlike SD, which relies on a metric derived from importance sampling for verification, CD verifies candidate drafts using token-level likelihoods computed solely by the LLM. CD achieves up to a 2.5-fold increase in inference speed compared to the target model, while maintaining comparable generation quality (around 100% of the target model's performance). Interestingly, this is achieved by combining models whose parameter sizes differ by two orders of magnitude. In addition, CD reduces the call frequency of the large target model to below 10%, particularly in more demanding tasks. CD's performance was even found to surpass that of the large target model, which theoretically represents the upper bound for speculative decoding.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¨æµ‹è§£ç (Speculative Decoding)ä¸­å› é«˜æ‹’ç»ç‡å¯¼è‡´å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)é¢‘ç¹è°ƒç”¨ã€è¿›è€Œå½±å“æ¨ç†æ•ˆç‡çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºé¡¾é—®è§£ç (Consultant Decoding, CD)çš„æ–°å‹ååŒæœºåˆ¶ã€‚ä¸ä¾èµ–é‡è¦æ€§é‡‡æ ·(importance sampling)è¿›è¡ŒéªŒè¯çš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒCDä»…åˆ©ç”¨ç›®æ ‡æ¨¡å‹è®¡ç®—çš„è¯å…ƒçº§ä¼¼ç„¶åº¦(token-level likelihoods)æ¥éªŒè¯å€™é€‰è‰ç¨¿ã€‚é€šè¿‡ç»“åˆå‚æ•°è§„æ¨¡ç›¸å·®ä¸¤ä¸ªæ•°é‡çº§çš„æ¨¡å‹ï¼ŒCDåœ¨ä¿æŒä¸ç›®æ ‡æ¨¡å‹æ€§èƒ½ç›¸å½“çš„å‰æä¸‹ï¼Œå®ç°äº†é«˜è¾¾2.5å€çš„æ¨ç†é€Ÿåº¦æå‡ã€‚æ­¤å¤–ï¼ŒCDå°†å¤§å‹ç›®æ ‡æ¨¡å‹çš„è°ƒç”¨é¢‘ç‡æ˜¾è‘—é™ä½è‡³10%ä»¥ä¸‹ï¼Œå¹¶åœ¨éƒ¨åˆ†ä»»åŠ¡ä¸­å±•ç°å‡ºè¶…è¶Šç›®æ ‡æ¨¡å‹ï¼ˆå³æ¨æµ‹è§£ç ç†è®ºä¸Šé™ï¼‰çš„æ€§èƒ½è¡¨ç°ã€‚è¯¥æœºåˆ¶ä¸ºä¼˜åŒ–å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­çš„æ•ˆç‡ä¸è´¨é‡å¹³è¡¡æä¾›äº†æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2025 findings",
      "pdf_url": "https://arxiv.org/pdf/2506.02391v1",
      "published_date": "2025-06-03 03:13:27 UTC",
      "updated_date": "2025-06-03 03:13:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:18:58.284678+00:00"
    },
    {
      "arxiv_id": "2506.02389v1",
      "title": "Univariate to Multivariate: LLMs as Zero-Shot Predictors for Time-Series Forecasting",
      "title_zh": "ä»å•å˜é‡åˆ°å¤šå˜é‡ï¼šå¤§è¯­è¨€æ¨¡å‹ä½œä¸ºæ—¶é—´åºåˆ—é¢„æµ‹çš„é›¶æ ·æœ¬é¢„æµ‹å™¨",
      "authors": [
        "Chamara Madarasingha",
        "Nasrin Sohrabi",
        "Zahir Tari"
      ],
      "abstract": "Time-series prediction or forecasting is critical across many real-world dynamic systems, and recent studies have proposed using Large Language Models (LLMs) for this task due to their strong generalization capabilities and ability to perform well without extensive pre-training. However, their effectiveness in handling complex, noisy, and multivariate time-series data remains underexplored. To address this, we propose LLMPred which enhances LLM-based time-series prediction by converting time-series sequences into text and feeding them to LLMs for zero shot prediction along with two main data pre-processing techniques. First, we apply time-series sequence decomposition to facilitate accurate prediction on complex and noisy univariate sequences. Second, we extend this univariate prediction capability to multivariate data using a lightweight prompt-processing strategy. Extensive experiments with smaller LLMs such as Llama 2 7B, Llama 3.2 3B, GPT-4o-mini, and DeepSeek 7B demonstrate that LLMPred achieves competitive or superior performance compared to state-of-the-art baselines. Additionally, a thorough ablation study highlights the importance of the key components proposed in LLMPred.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œé›¶æ ·æœ¬æ—¶é—´åºåˆ—é¢„æµ‹çš„æ½œåŠ›ï¼Œå¹¶é’ˆå¯¹å¤æ‚ã€å«å™ªå£°åŠå¤šå˜é‡æ•°æ®çš„å¤„ç†éš¾é¢˜æå‡ºäº†LLMPredæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†æ—¶é—´åºåˆ—åºåˆ—è½¬æ¢ä¸ºæ–‡æœ¬è¾“å…¥ï¼Œå¹¶ç»“åˆæ—¶é—´åºåˆ—åºåˆ—åˆ†è§£ï¼ˆTime-series sequence decompositionï¼‰æŠ€æœ¯ï¼Œæœ‰æ•ˆæå‡äº†æ¨¡å‹å¯¹å¤æ‚å•å˜é‡åºåˆ—çš„é¢„æµ‹ç²¾åº¦ã€‚åŒæ—¶ï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ç§è½»é‡çº§çš„æç¤ºå¤„ç†ç­–ç•¥ï¼ˆPrompt-processing strategyï¼‰ï¼ŒæˆåŠŸå°†é¢„æµ‹èƒ½åŠ›ä»å•å˜é‡æ‰©å±•è‡³å¤šå˜é‡æ•°æ®ï¼ˆMultivariate dataï¼‰ã€‚åœ¨Llama 2 7Bã€Llama 3.2 3Bã€GPT-4o-miniåŠDeepSeek 7Bç­‰å¤šä¸ªæ¨¡å‹ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMPredçš„æ€§èƒ½è¾¾åˆ°æˆ–è¶…è¿‡äº†ç°æœ‰çš„æœ€å…ˆè¿›åŸºå‡†ã€‚æ¶ˆèå®éªŒï¼ˆAblation studyï¼‰è¿›ä¸€æ­¥éªŒè¯äº†å…¶æ ¸å¿ƒç»„ä»¶çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†LLMsåœ¨æ— éœ€é’ˆå¯¹æ€§é¢„è®­ç»ƒçš„æƒ…å†µä¸‹å¤„ç†å¤æ‚åŠ¨æ€ç³»ç»Ÿé¢„æµ‹ä»»åŠ¡çš„å¼ºå¤§ç«äº‰åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02389v1",
      "published_date": "2025-06-03 03:02:47 UTC",
      "updated_date": "2025-06-03 03:02:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:19:07.591576+00:00"
    },
    {
      "arxiv_id": "2506.02387v2",
      "title": "VS-Bench: Evaluating VLMs for Strategic Abilities in Multi-Agent Environments",
      "title_zh": "VS-Benchï¼šè¯„ä¼°å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸‹è§†è§‰è¯­è¨€æ¨¡å‹çš„ç­–ç•¥èƒ½åŠ›",
      "authors": [
        "Zelai Xu",
        "Zhexuan Xu",
        "Xiangmin Yi",
        "Huining Yuan",
        "Mo Guang",
        "Kaiwen Long",
        "Xinlei Chen",
        "Yi Wu",
        "Chao Yu",
        "Yu Wang"
      ],
      "abstract": "Recent advancements in Vision Language Models (VLMs) have expanded their capabilities to interactive agent tasks, yet existing benchmarks remain limited to single-agent or text-only environments. In contrast, real-world scenarios often involve multiple agents interacting within rich visual and textual contexts, posing challenges with both multimodal observations and strategic interactions. To bridge this gap, we introduce Visual Strategic Bench (VS-Bench), a multimodal benchmark that evaluates VLMs for strategic abilities in multi-agent environments. VS-Bench comprises ten vision-grounded environments that cover cooperative, competitive, and mixed-motive interactions. The performance of VLM agents is evaluated across three dimensions: perception measured by element recognition accuracy; strategic reasoning measured by next-action prediction accuracy; and decision-making measured by normalized episode return. Extensive experiments on fifteen leading VLMs show that, although current models exhibit strong perception abilities, there remains a significant gap to optimal performance in reasoning and decision-making, with the best-performing model attaining 46.6% prediction accuracy and 31.4% normalized return. We further analyze the key factors influencing performance, conduct human experiments, and examine failure modes to provide a deeper understanding of VLMs' strategic abilities. By standardizing the evaluation and highlighting the limitations of existing models, we envision VS-Bench as a foundation for future research on strategic multimodal agents. Code and data are available at https://vs-bench.github.io.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰åŸºå‡†æµ‹è¯•å±€é™äºå•æ™ºèƒ½ä½“æˆ–çº¯æ–‡æœ¬ç¯å¢ƒçš„ç°çŠ¶ï¼Œæå‡ºäº† Visual Strategic Bench (VS-Bench)ï¼Œæ—¨åœ¨è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) åœ¨å¤šæ™ºèƒ½ä½“å¤æ‚è§†è§‰ç¯å¢ƒä¸‹çš„æˆ˜ç•¥èƒ½åŠ›ã€‚VS-Bench æ¶µç›–äº†åˆä½œã€ç«äº‰å’Œæ··åˆåŠ¨æœºç­‰åç§è§†è§‰æ„ŸçŸ¥ç¯å¢ƒï¼Œå¹¶ä»æ„ŸçŸ¥ (perception)ã€æˆ˜ç•¥æ¨ç† (strategic reasoning) å’Œå†³ç­–åˆ¶å®š (decision-making) ä¸‰ä¸ªæ ¸å¿ƒç»´åº¦è¿›è¡Œè¯„ä¼°ã€‚é€šè¿‡å¯¹ 15 ç§é¢†å…ˆ VLMs çš„å¹¿æ³›å®éªŒå‘ç°ï¼Œå°½ç®¡å½“å‰æ¨¡å‹åœ¨æ„ŸçŸ¥æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ¨ç†å’Œå†³ç­–ä»»åŠ¡ä¸­ä»å­˜åœ¨å·¨å¤§æŒ‘æˆ˜ï¼Œæ€§èƒ½æœ€ä½³çš„æ¨¡å‹åœ¨åŠ¨ä½œé¢„æµ‹å‡†ç¡®ç‡å’Œæ ‡å‡†åŒ–å›æŠ¥ (normalized episode return) æ–¹é¢åˆ†åˆ«ä»…è¾¾åˆ° 46.6% å’Œ 31.4%ã€‚è¯¥åŸºå‡†æµ‹è¯•é€šè¿‡åˆ†æå½±å“æ€§èƒ½çš„å…³é”®å› ç´ åŠå¤±è´¥æ¨¡å¼ï¼Œä¸ºç†è§£ VLMs çš„æˆ˜ç•¥èƒ½åŠ›æä¾›äº†æ·±åº¦è§è§£ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æˆ˜ç•¥å¤šæ¨¡æ€æ™ºèƒ½ä½“å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02387v2",
      "published_date": "2025-06-03 02:57:38 UTC",
      "updated_date": "2025-09-30 06:49:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:19:21.197315+00:00"
    },
    {
      "arxiv_id": "2506.02386v1",
      "title": "Asymptotically Optimal Linear Best Feasible Arm Identification with Fixed Budget",
      "title_zh": "å›ºå®šé¢„ç®—ä¸‹æ¸è¿‘æœ€ä¼˜çš„çº¿æ€§æœ€ä½³å¯è¡Œè‡‚è¯†åˆ«",
      "authors": [
        "Jie Bian",
        "Vincent Y. F. Tan"
      ],
      "abstract": "The challenge of identifying the best feasible arm within a fixed budget has attracted considerable interest in recent years. However, a notable gap remains in the literature: the exact exponential rate at which the error probability approaches zero has yet to be established, even in the relatively simple setting of $K$-armed bandits with Gaussian noise. In this paper, we address this gap by examining the problem within the context of linear bandits. We introduce a novel algorithm for best feasible arm identification that guarantees an exponential decay in the error probability. Remarkably, the decay rate -- characterized by the exponent -- matches the theoretical lower bound derived using information-theoretic principles. Our approach leverages a posterior sampling framework embedded within a game-based sampling rule involving a min-learner and a max-learner. This strategy shares its foundations with Thompson sampling, but is specifically tailored to optimize the identification process under fixed-budget constraints. Furthermore, we validate the effectiveness of our algorithm through comprehensive empirical evaluations across various problem instances with different levels of complexity. The results corroborate our theoretical findings and demonstrate that our method outperforms several benchmark algorithms in terms of both accuracy and efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›ºå®šé¢„ç®—(Fixed Budget)çº¦æŸä¸‹çš„çº¿æ€§æœ€ä½³å¯è¡Œè‡‚è¯†åˆ«(Linear Best Feasible Arm Identification)é—®é¢˜ï¼Œå¡«è¡¥äº†å…³äºè¯¯å·®æ¦‚ç‡ç²¾ç¡®æŒ‡æ•°è¡°å‡é€Ÿç‡å°šæœªç¡®å®šçš„ç ”ç©¶ç©ºç™½ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹ç®—æ³•ï¼Œé€šè¿‡åœ¨æ¶‰åŠæœ€å°å­¦ä¹ å™¨(min-learner)å’Œæœ€å¤§å­¦ä¹ å™¨(max-learner)çš„åšå¼ˆé‡‡æ ·è§„åˆ™ä¸­åµŒå…¥åéªŒé‡‡æ ·(Posterior Sampling)æ¡†æ¶ï¼Œç¡®ä¿äº†è¯¯å·®æ¦‚ç‡çš„æŒ‡æ•°çº§ä¸‹é™ã€‚è¯¥ç­–ç•¥åœ¨Thompson Samplingçš„åŸºç¡€ä¸Šé’ˆå¯¹è¯†åˆ«ä»»åŠ¡è¿›è¡Œäº†ä¸“é—¨ä¼˜åŒ–ï¼Œå…¶æ€§èƒ½åœ¨ç†è®ºä¸Šè¾¾åˆ°äº†åŸºäºä¿¡æ¯è®ºæ¨å¯¼å‡ºçš„æ¸è¿‘æœ€ä¼˜ä¸‹ç•Œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨å¤šç§å¤æ‚åº¦çš„ç®—ä¾‹ä¸­å‡éªŒè¯äº†ç†è®ºé¢„æœŸï¼Œè¯æ˜å…¶è¡°å‡ç‡ä¸ç†è®ºä¸‹ç•Œç›¸åŒ¹é…ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨è¯†åˆ«å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„å¤šç§åŸºå‡†ç®—æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to the Conference on Uncertainty in Artificial Intelligence (UAI) 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.02386v1",
      "published_date": "2025-06-03 02:56:26 UTC",
      "updated_date": "2025-06-03 02:56:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:19:15.098768+00:00"
    },
    {
      "arxiv_id": "2506.02378v1",
      "title": "Exploring Explanations Improves the Robustness of In-Context Learning",
      "title_zh": "æ¢ç´¢è§£é‡Šæå‡ä¸Šä¸‹æ–‡å­¦ä¹ çš„é²æ£’æ€§",
      "authors": [
        "Ukyo Honda",
        "Tatsushi Oka"
      ],
      "abstract": "In-context learning (ICL) has emerged as a successful paradigm for leveraging large language models (LLMs). However, it often struggles to generalize beyond the distribution of the provided demonstrations. A recent advancement in enhancing robustness is ICL with explanations (X-ICL), which improves prediction reliability by guiding LLMs to understand and articulate the reasoning behind correct labels. Building on this approach, we introduce an advanced framework that extends X-ICL by systematically exploring explanations for all possible labels (X$^2$-ICL), thereby enabling more comprehensive and robust decision-making. Experimental results on multiple natural language understanding datasets validate the effectiveness of X$^2$-ICL, demonstrating significantly improved robustness to out-of-distribution data compared to the existing ICL approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸Šä¸‹æ–‡å­¦ä¹ (In-Context Learning, ICL)åœ¨å¤„ç†åˆ†å¸ƒå¤–æ•°æ®æ—¶æ³›åŒ–æ€§ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º X$^2$-ICL çš„å¢å¼ºæ¡†æ¶ã€‚è¯¥æ¡†æ¶åœ¨ç°æœ‰çš„è§£é‡Šæ€§ä¸Šä¸‹æ–‡å­¦ä¹ (X-ICL)åŸºç¡€ä¸Šè¿›è¡Œäº†é‡è¦æ‰©å±•ï¼Œé€šè¿‡ç³»ç»Ÿåœ°æ¢ç´¢æ‰€æœ‰å¯èƒ½æ ‡ç­¾çš„è§£é‡Šï¼Œå¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)è¿›è¡Œæ›´å…¨é¢ä¸”é²æ£’çš„å†³ç­–ã€‚ä¸ä¼ ç»Ÿä»…å…³æ³¨æ­£ç¡®æ ‡ç­¾æ¨ç†çš„æ–¹æ³•ç›¸æ¯”ï¼ŒX$^2$-ICL èƒ½å¤Ÿä¿ƒä½¿æ¨¡å‹æ·±å…¥ç†è§£ä¸åŒé€‰æ‹©èƒŒåçš„é€»è¾‘ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºäº†é¢„æµ‹è¿‡ç¨‹çš„å¯é æ€§ã€‚åœ¨å¤šä¸ªè‡ªç„¶è¯­è¨€ç†è§£(NLU)æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒX$^2$-ICL åœ¨é¢å¯¹åˆ†å¸ƒå¤–(out-of-distribution)æ•°æ®æ—¶å±•ç°å‡ºæ¯”ç°æœ‰ ICL æ–¹æ³•æ›´ä¼˜è¶Šçš„é²æ£’æ€§ã€‚è¿™ä¸€å‘ç°æœ‰åŠ›éªŒè¯äº†é€šè¿‡æ¢ç´¢å¤šé‡è§£é‡Šæ¥æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„æœ‰æ•ˆæ€§ï¼Œä¸ºä¼˜åŒ– LLMs åœ¨å¤æ‚åœºæ™¯ä¸‹çš„åº”ç”¨æ€§èƒ½æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ACL 2025 (Main Conference)",
      "pdf_url": "https://arxiv.org/pdf/2506.02378v1",
      "published_date": "2025-06-03 02:29:14 UTC",
      "updated_date": "2025-06-03 02:29:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:19:28.543846+00:00"
    },
    {
      "arxiv_id": "2506.02362v1",
      "title": "MISLEADER: Defending against Model Extraction with Ensembles of Distilled Models",
      "title_zh": "MISLEADERï¼šåŸºäºè’¸é¦æ¨¡å‹é›†æˆçš„æ¨¡å‹æŠ½å–é˜²å¾¡æ–¹æ³•",
      "authors": [
        "Xueqi Cheng",
        "Minxing Zheng",
        "Shixiang Zhu",
        "Yushun Dong"
      ],
      "abstract": "Model extraction attacks aim to replicate the functionality of a black-box model through query access, threatening the intellectual property (IP) of machine-learning-as-a-service (MLaaS) providers. Defending against such attacks is challenging, as it must balance efficiency, robustness, and utility preservation in the real-world scenario. Despite the recent advances, most existing defenses presume that attacker queries have out-of-distribution (OOD) samples, enabling them to detect and disrupt suspicious inputs. However, this assumption is increasingly unreliable, as modern models are trained on diverse datasets and attackers often operate under limited query budgets. As a result, the effectiveness of these defenses is significantly compromised in realistic deployment scenarios. To address this gap, we propose MISLEADER (enseMbles of dIStiLled modEls Against moDel ExtRaction), a novel defense strategy that does not rely on OOD assumptions. MISLEADER formulates model protection as a bilevel optimization problem that simultaneously preserves predictive fidelity on benign inputs and reduces extractability by potential clone models. Our framework combines data augmentation to simulate attacker queries with an ensemble of heterogeneous distilled models to improve robustness and diversity. We further provide a tractable approximation algorithm and derive theoretical error bounds to characterize defense effectiveness. Extensive experiments across various settings validate the utility-preserving and extraction-resistant properties of our proposed defense strategy. Our code is available at https://github.com/LabRAI/MISLEADER.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨å­¦ä¹ å³æœåŠ¡(MLaaS)ä¸­çš„æ¨¡å‹æå–æ”»å‡»(Model Extraction)é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºMISLEADERçš„é˜²å¾¡ç­–ç•¥ã€‚ä¼ ç»Ÿçš„é˜²å¾¡æ‰‹æ®µé€šå¸¸ä¾èµ–äºå¯¹å¼‚æ„åˆ†å¸ƒ(OOD)æ ·æœ¬çš„æ£€æµ‹ï¼Œä½†åœ¨ç°ä»£å¤šæ ·åŒ–è®­ç»ƒé›†å’Œæœ‰é™æŸ¥è¯¢é¢„ç®—çš„ç°å®åœºæ™¯ä¸‹ï¼Œè¿™ç§å‡è®¾å·²ä¸å†å¯é ã€‚MISLEADERé€šè¿‡å°†æ¨¡å‹ä¿æŠ¤å…¬å¼åŒ–ä¸ºä¸€ä¸ªåŒå±‚ä¼˜åŒ–(Bilevel Optimization)é—®é¢˜ï¼Œåœ¨ä¿ç•™è‰¯æ€§è¾“å…¥é¢„æµ‹ä¿çœŸåº¦çš„åŒæ—¶ï¼Œæœ‰æ•ˆé™ä½äº†æ½œåœ¨å…‹éš†æ¨¡å‹çš„æå–æ•ˆç‡ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ•°æ®å¢å¼º(Data Augmentation)æŠ€æœ¯ä»¥æ¨¡æ‹Ÿæ”»å‡»è€…æŸ¥è¯¢ï¼Œå¹¶åˆ©ç”¨å¼‚æ„è’¸é¦æ¨¡å‹(Heterogeneous Distilled Models)çš„é›†æˆ(Ensemble)æ¥æå‡ç³»ç»Ÿçš„é²æ£’æ€§ä¸å¤šæ ·æ€§ã€‚ç ”ç©¶è¿›ä¸€æ­¥æä¾›äº†å¯è¡Œçš„è¿‘ä¼¼ç®—æ³•ï¼Œå¹¶æ¨å¯¼äº†ç†è®ºè¯¯å·®ç•Œé™(Theoretical Error Bounds)ä»¥è¡¨å¾å…¶é˜²å¾¡æœ‰æ•ˆæ€§ã€‚å¤§é‡å®éªŒéªŒè¯äº†è¯¥ç­–ç•¥åœ¨ä¿æŒæ¨¡å‹æ•ˆç”¨å’ŒæŠµæŠ—æå–æ–¹é¢çš„å“è¶Šè¡¨ç°ï¼Œç›¸å…³ä»£ç ä¹Ÿå·²å¼€æºã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02362v1",
      "published_date": "2025-06-03 01:37:09 UTC",
      "updated_date": "2025-06-03 01:37:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:19:46.683430+00:00"
    },
    {
      "arxiv_id": "2506.02357v2",
      "title": "Evaluating LLM Agent Adherence to Hierarchical Safety Principles: A Lightweight Benchmark for Probing Foundational Controllability Components",
      "title_zh": "è¯„ä¼° LLM æ™ºèƒ½ä½“å¯¹åˆ†å±‚å®‰å…¨åŸåˆ™çš„éµå¾ªï¼šä¸€ç§ç”¨äºæ¢ç©¶åŸºç¡€å¯æ§æ€§ç»„ä»¶çš„è½»é‡çº§åŸºå‡†",
      "authors": [
        "Ram Potham"
      ],
      "abstract": "Credible safety plans for advanced AI development require methods to verify agent behavior and detect potential control deficiencies early. A fundamental aspect is ensuring agents adhere to safety-critical principles, especially when these conflict with operational goals. This paper introduces a lightweight, interpretable benchmark to evaluate an LLM agent's ability to uphold a high-level safety principle when faced with conflicting task instructions. Our evaluation of six LLMs reveals two primary findings: (1) a quantifiable \"cost of compliance\" where safety constraints degrade task performance even when compliant solutions exist, and (2) an \"illusion of compliance\" where high adherence often masks task incompetence rather than principled choice. These findings provide initial evidence that while LLMs can be influenced by hierarchical directives, current approaches lack the consistency required for reliable safety governance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…ˆè¿›äººå·¥æ™ºèƒ½å¼€å‘ä¸­çš„å®‰å…¨éªŒè¯é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªè½»é‡çº§ä¸”å¯è§£é‡Šçš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼° LLM agent åœ¨é¢å¯¹å†²çªçš„ä»»åŠ¡æŒ‡ä»¤æ—¶éµå¾ªé«˜å±‚çº§å®‰å…¨åŸåˆ™çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹å…­ç§ LLM çš„è¯„ä¼°ï¼Œç ”ç©¶æ­ç¤ºäº†ä¸¤ä¸ªæ ¸å¿ƒå‘ç°ï¼šé¦–å…ˆæ˜¯â€œåˆè§„æˆæœ¬ (cost of compliance)â€ï¼Œå³å®‰å…¨çº¦æŸå³ä½¿åœ¨å­˜åœ¨åˆè§„æ–¹æ¡ˆçš„æƒ…å†µä¸‹ä¹Ÿä¼šå¯¼è‡´ä»»åŠ¡æ€§èƒ½ä¸‹é™ï¼›å…¶æ¬¡æ˜¯â€œåˆè§„é”™è§‰ (illusion of compliance)â€ï¼Œå³æ¨¡å‹çš„é«˜ä¾ä»æ€§å¾€å¾€æ©ç›–äº†å…¶å¤„ç†ä»»åŠ¡çš„èƒ½åŠ›ä¸è¶³ï¼Œè€ŒéåŸºäºåŸåˆ™çš„è‡ªä¸»é€‰æ‹©ã€‚å®éªŒæ•°æ®è¯æ˜ï¼Œè™½ç„¶ LLM èƒ½å¤Ÿå—åˆ°å±‚çº§æŒ‡ä»¤çš„å½±å“ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å®‰å…¨æ²»ç†çš„ä¸€è‡´æ€§æ–¹é¢ä»å­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚è¯¥é¡¹å·¥ä½œä¸ºæ—©æœŸæ£€æµ‹ AI æ§åˆ¶ç¼ºé™·æä¾›äº†é‡è¦é‡åŒ–ä¾æ®ï¼Œå¼ºè°ƒäº†æ„å»ºå¯é å®‰å…¨éªŒè¯æœºåˆ¶çš„è¿«åˆ‡æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint. This work has been submitted to the Technical AI Governance Workshop at ICML 2025 for review",
      "pdf_url": "https://arxiv.org/pdf/2506.02357v2",
      "published_date": "2025-06-03 01:16:34 UTC",
      "updated_date": "2025-07-10 15:10:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:19:51.750608+00:00"
    },
    {
      "arxiv_id": "2506.02351v1",
      "title": "DIAMOND: An LLM-Driven Agent for Context-Aware Baseball Highlight Summarization",
      "title_zh": "DIAMONDï¼šé¢å‘ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ£’çƒé›†é”¦æ‘˜è¦çš„å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨æ™ºèƒ½ä½“",
      "authors": [
        "Jeonghun Kang",
        "Soonmok Kwon",
        "Joonseok Lee",
        "Byung-Hak Kim"
      ],
      "abstract": "Traditional approaches -- such as Win Probability Added (WPA)-based ranking or computer vision-driven event detection -- can identify scoring plays but often miss strategic depth, momentum shifts, and storyline progression. Manual curation remains the gold standard but is resource-intensive and not scalable. We introduce DIAMOND, an LLM-driven agent for context-aware baseball highlight summarization that integrates structured sports analytics with natural language reasoning. DIAMOND leverages sabermetric features -- Win Expectancy, WPA, and Leverage Index -- to quantify play importance, while an LLM module enhances selection based on contextual narrative value. This hybrid approach ensures both quantitative rigor and qualitative richness, surpassing the limitations of purely statistical or vision-based systems. Evaluated on five diverse Korean Baseball Organization League games, DIAMOND improves F1-score from 42.9% (WPA-only) to 84.8%, outperforming both commercial and statistical baselines. Though limited in scale, our results highlight the potential of modular, interpretable agent-based frameworks for event-level summarization in sports and beyond.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DIAMONDï¼Œä¸€ç§ç”±å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„æ™ºèƒ½ä½“ï¼Œä¸“é—¨ç”¨äºæ£’çƒæ¯”èµ›ç²¾å½©ç‰‡æ®µçš„è¯­å¢ƒæ„ŸçŸ¥æ‘˜è¦ç”Ÿæˆï¼Œæ—¨åœ¨å¼¥è¡¥ä¼ ç»Ÿæ–¹æ³•åœ¨æ•æ‰æ¯”èµ›ç­–ç•¥æ·±åº¦ã€åŠ¿å¤´è½¬å˜å’Œå™äº‹è¿›å±•æ–¹é¢çš„ä¸è¶³ã€‚è¯¥æ¡†æ¶å°† Sabermetrics ç»Ÿè®¡åˆ†æä¸è‡ªç„¶è¯­è¨€æ¨ç†ç›¸ç»“åˆï¼Œåˆ©ç”¨ Win Expectancyã€Win Probability Added (WPA) å’Œ Leverage Index ç­‰ç‰¹å¾æ¥é‡åŒ–å•åœºæ¯”èµ›è¿›ç¨‹çš„é‡è¦æ€§ã€‚DIAMOND è¿›ä¸€æ­¥é€šè¿‡ LLM æ¨¡å—æ ¹æ®è¯­å¢ƒå™äº‹ä»·å€¼ä¼˜åŒ–ç‰‡æ®µé€‰æ‹©é€»è¾‘ï¼Œå®ç°äº†å®šé‡ä¸¥è°¨æ€§ä¸å®šæ€§ä¸°å¯Œæ€§çš„æœ‰æœºç»“åˆï¼Œè¶…è¶Šäº†çº¯ç»Ÿè®¡æˆ–è§†è§‰ç³»ç»Ÿçš„å±€é™æ€§ã€‚åœ¨éŸ©å›½æ£’çƒå§”å‘˜ä¼šï¼ˆKBOï¼‰è”èµ›çš„å¤šåœºæ¯”èµ›è¯„ä¼°ä¸­ï¼ŒDIAMOND å°† F1-score ä»çº¯ WPA æ–¹æ³•çš„ 42.9% æå‡è‡³ 84.8%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„å•†ä¸šå’Œç»Ÿè®¡åŸºå‡†ã€‚å®éªŒç»“æœè¯æ˜äº†è¿™ç§æ¨¡å—åŒ–ä¸”å…·æœ‰å¯è§£é‡Šæ€§çš„æ™ºèƒ½ä½“æ¡†æ¶åœ¨ä½“è‚²èµ›äº‹åŠå…¶ä»–é¢†åŸŸçš„äº‹ä»¶çº§æ‘˜è¦ç”Ÿæˆä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "To appear in the First REALM (Research on Agent Language Models) workshop at ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.02351v1",
      "published_date": "2025-06-03 01:10:20 UTC",
      "updated_date": "2025-06-03 01:10:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:19:52.909397+00:00"
    },
    {
      "arxiv_id": "2506.03210v2",
      "title": "FuXi-Ocean: A Global Ocean Forecasting System with Sub-Daily Resolution",
      "title_zh": "FuXi-Oceanï¼šå…·æœ‰äºšæ—¥åˆ†è¾¨ç‡çš„å…¨çƒæµ·æ´‹é¢„æŠ¥ç³»ç»Ÿ",
      "authors": [
        "Qiusheng Huang",
        "Yuan Niu",
        "Xiaohui Zhong",
        "Anboyu Guo",
        "Lei Chen",
        "Dianjun Zhang",
        "Xuefeng Zhang",
        "Hao Li"
      ],
      "abstract": "Accurate, high-resolution ocean forecasting is crucial for maritime operations and environmental monitoring. While traditional numerical models are capable of producing sub-daily, eddy-resolving forecasts, they are computationally intensive and face challenges in maintaining accuracy at fine spatial and temporal scales. In contrast, recent data-driven approaches offer improved computational efficiency and emerging potential, yet typically operate at daily resolution and struggle with sub-daily predictions due to error accumulation over time. We introduce FuXi-Ocean, the first data-driven global ocean forecasting model achieving six-hourly predictions at eddy-resolving 1/12Â° spatial resolution, reaching depths of up to 1500 meters. The model architecture integrates a context-aware feature extraction module with a predictive network employing stacked attention blocks. The core innovation is the Mixture-of-Time (MoT) module, which adaptively integrates predictions from multiple temporal contexts by learning variable-specific reliability , mitigating cumulative errors in sequential forecasting. Through comprehensive experimental evaluation, FuXi-Ocean demonstrates superior skill in predicting key variables, including temperature, salinity, and currents, across multiple depths.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†FuXi-Oceanï¼Œè¿™æ˜¯é¦–ä¸ªåœ¨1/12Â°æ¶¡æµè§£æ(eddy-resolving)ç©ºé—´åˆ†è¾¨ç‡ä¸‹å®ç°å…­å°æ—¶å°ºåº¦å…¨çƒæµ·æ´‹é¢„æŠ¥çš„æ•°æ®é©±åŠ¨æ¨¡å‹ã€‚æ¨¡å‹é¢„æŠ¥æ·±åº¦å¯è¾¾1500ç±³ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿæ•°å€¼æ¨¡å‹è®¡ç®—å¼ºåº¦å¤§ä»¥åŠç°æœ‰æ•°æ®é©±åŠ¨æ¨¡å‹åœ¨äºšæ—¥(sub-daily)åˆ†è¾¨ç‡ä¸‹å› è¯¯å·®ç´¯ç§¯å¯¼è‡´çš„é¢„æµ‹éš¾é¢˜ã€‚å…¶æ¶æ„é›†æˆäº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç‰¹å¾æå–æ¨¡å—å’ŒåŸºäºå †å æ³¨æ„åŠ›å—(stacked attention blocks)çš„é¢„æµ‹ç½‘ç»œã€‚æ ¸å¿ƒåˆ›æ–°åœ¨äºMixture-of-Time (MoT)æ¨¡å—ï¼Œè¯¥æ¨¡å—é€šè¿‡å­¦ä¹ ç‰¹å®šå˜é‡çš„å¯é æ€§æ¥é€‚åº”æ€§åœ°æ•´åˆå¤šä¸ªæ—¶é—´ä¸Šä¸‹æ–‡çš„é¢„æµ‹ï¼Œä»è€Œæœ‰æ•ˆå‡è½»äº†é¡ºåºé¢„æµ‹ä¸­çš„ç´¯ç§¯è¯¯å·®ã€‚å®éªŒè¯„ä¼°è¯æ˜ï¼ŒFuXi-Oceanåœ¨é¢„æµ‹å¤šä¸ªæ·±åº¦çš„æ¸©åº¦(temperature)ã€ç›åº¦(salinity)å’Œæµåœº(currents)ç­‰å…³é”®å˜é‡æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚è¯¥æˆæœä¸ºå®ç°é«˜æ•ˆã€é«˜ç²¾åº¦çš„å…¨çƒæµ·æ´‹ç¯å¢ƒç›‘æµ‹å’Œèˆªæµ·ä½œä¸šæä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.ao-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03210v2",
      "published_date": "2025-06-03 00:52:31 UTC",
      "updated_date": "2025-10-24 10:31:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T18:19:55.233112+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 194,
  "processed_papers_count": 194,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-23T18:20:52.750588+00:00"
}
[
  {
    "arxiv_id": "2403.01643v3",
    "title": "Cost-Effective Attention Mechanisms for Low Resource Settings: Necessity & Sufficiency of Linear Transformations",
    "authors": [
      "Peyman Hosseini",
      "Mehran Hosseini",
      "Ignacio Castro",
      "Matthew Purver"
    ],
    "abstract": "From natural language processing to vision, Scaled Dot Product Attention\n(SDPA) is the backbone of most modern deep learning applications.\nUnfortunately, its memory and computational requirements can be prohibitive in\nlow-resource settings. In this paper, we improve its efficiency without\nsacrificing its versatility. We propose three attention variants where we\nremove consecutive linear transformations or add a novel one, and evaluate them\non a range of standard NLP and vision tasks. Our proposed models are\nsubstantially lighter than standard SDPA (and have 25-50% fewer parameters). We\nshow that the performance cost of these changes is negligible relative to size\nreduction and that in one case (Super Attention) we succeed in outperforming\nSDPA by up to 10% while improving its speed and reducing its parameters by 25%.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "68T07 (Primary) 68T45, 68T50, 68T10, 15A03, 15A04 (Secondary)",
      "I.2.6; I.2.7; I.2.10; I.4.0; I.5.0; I.7.0"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01643v3",
    "published_date": "2024-03-03 23:40:35 UTC",
    "updated_date": "2025-02-16 14:14:16 UTC"
  },
  {
    "arxiv_id": "2403.01621v1",
    "title": "Machine Learning vs Deep Learning: The Generalization Problem",
    "authors": [
      "Yong Yi Bay",
      "Kathleen A. Yearick"
    ],
    "abstract": "The capacity to generalize beyond the range of training data is a pivotal\nchallenge, often synonymous with a model's utility and robustness. This study\ninvestigates the comparative abilities of traditional machine learning (ML)\nmodels and deep learning (DL) algorithms in terms of extrapolation -- a more\nchallenging aspect of generalization because it requires the model to make\ninferences about data points that lie outside the domain it has been trained\non. We present an empirical analysis where both ML and DL models are trained on\nan exponentially growing function and then tested on values outside the\ntraining domain. The choice of this function allows us to distinctly showcase\nthe divergence in performance when models are required to predict beyond the\nscope of their training data. Our findings suggest that deep learning models\npossess inherent capabilities to generalize beyond the training scope, an\nessential feature for real-world applications where data is often incomplete or\nextends beyond the observed range. This paper argues for a nuanced\nunderstanding of the structural differences between ML and DL models, with an\nemphasis on the implications for both theoretical research and practical\ndeployment.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.01621v1",
    "published_date": "2024-03-03 21:42:55 UTC",
    "updated_date": "2024-03-03 21:42:55 UTC"
  },
  {
    "arxiv_id": "2403.01606v2",
    "title": "A Unified Model Selection Technique for Spectral Clustering Based Motion Segmentation",
    "authors": [
      "Yuxiang Huang",
      "John Zelek"
    ],
    "abstract": "Motion segmentation is a fundamental problem in computer vision and is\ncrucial in various applications such as robotics, autonomous driving and action\nrecognition. Recently, spectral clustering based methods have shown impressive\nresults on motion segmentation in dynamic environments. These methods perform\nspectral clustering on motion affinity matrices to cluster objects or point\ntrajectories in the scene into different motion groups. However, existing\nmethods often need the number of motions present in the scene to be known,\nwhich significantly reduces their practicality. In this paper, we propose a\nunified model selection technique to automatically infer the number of motion\ngroups for spectral clustering based motion segmentation methods by combining\ndifferent existing model selection techniques together. We evaluate our method\non the KT3DMoSeg dataset and achieve competitve results comparing to the\nbaseline where the number of clusters is given as ground truth information.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "for the published version, see\n  https://openjournals.uwaterloo.ca/index.php/vsl/article/view/5870/5922",
    "pdf_url": "http://arxiv.org/pdf/2403.01606v2",
    "published_date": "2024-03-03 20:16:14 UTC",
    "updated_date": "2024-05-06 22:19:22 UTC"
  },
  {
    "arxiv_id": "2403.01605v1",
    "title": "Towards Provable Log Density Policy Gradient",
    "authors": [
      "Pulkit Katdare",
      "Anant Joshi",
      "Katherine Driggs-Campbell"
    ],
    "abstract": "Policy gradient methods are a vital ingredient behind the success of modern\nreinforcement learning. Modern policy gradient methods, although successful,\nintroduce a residual error in gradient estimation. In this work, we argue that\nthis residual term is significant and correcting for it could potentially\nimprove sample-complexity of reinforcement learning methods. To that end, we\npropose log density gradient to estimate the policy gradient, which corrects\nfor this residual error term. Log density gradient method computes policy\ngradient by utilising the state-action discounted distributional formulation.\nWe first present the equations needed to exactly find the log density gradient\nfor a tabular Markov Decision Processes (MDPs). For more complex environments,\nwe propose a temporal difference (TD) method that approximates log density\ngradient by utilizing backward on-policy samples. Since backward sampling from\na Markov chain is highly restrictive we also propose a min-max optimization\nthat can approximate log density gradient using just on-policy samples. We also\nprove uniqueness, and convergence under linear function approximation, for this\nmin-max optimization. Finally, we show that the sample complexity of our\nmin-max optimization to be of the order of $m^{-1/2}$, where $m$ is the number\nof on-policy samples. We also demonstrate a proof-of-concept for our log\ndensity gradient method on gridworld environment, and observe that our method\nis able to improve upon the classical policy gradient method by a clear margin,\nthus indicating a promising novel direction to develop reinforcement learning\nalgorithms that require fewer samples.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01605v1",
    "published_date": "2024-03-03 20:09:09 UTC",
    "updated_date": "2024-03-03 20:09:09 UTC"
  },
  {
    "arxiv_id": "2403.01600v1",
    "title": "Can Poverty Be Reduced by Acting on Discrimination? An Agent-based Model for Policy Making",
    "authors": [
      "Alba Aguilera",
      "Nieves Montes",
      "Georgina Curto",
      "Carles Sierra",
      "Nardine Osman"
    ],
    "abstract": "In the last decades, there has been a deceleration in the rates of poverty\nreduction, suggesting that traditional redistributive approaches to poverty\nmitigation could be losing effectiveness, and alternative insights to advance\nthe number one UN Sustainable Development Goal are required. The\ncriminalization of poor people has been denounced by several NGOs, and an\nincreasing number of voices suggest that discrimination against the poor (a\nphenomenon known as \\emph{aporophobia}) could be an impediment to mitigating\npoverty. In this paper, we present the novel Aporophobia Agent-Based Model\n(AABM) to provide evidence of the correlation between aporophobia and poverty\ncomputationally. We present our use case built with real-world demographic data\nand poverty-mitigation public policies (either enforced or under parliamentary\ndiscussion) for the city of Barcelona. We classify policies as discriminatory\nor non-discriminatory against the poor, with the support of specialized NGOs,\nand we observe the results in the AABM in terms of the impact on wealth\ninequality. The simulation provides evidence of the relationship between\naporophobia and the increase of wealth inequality levels, paving the way for a\nnew generation of poverty reduction policies that act on discrimination and\ntackle poverty as a societal problem (not only a problem of the poor).",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01600v1",
    "published_date": "2024-03-03 19:59:42 UTC",
    "updated_date": "2024-03-03 19:59:42 UTC"
  },
  {
    "arxiv_id": "2403.01599v1",
    "title": "SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos",
    "authors": [
      "Yulei Niu",
      "Wenliang Guo",
      "Long Chen",
      "Xudong Lin",
      "Shih-Fu Chang"
    ],
    "abstract": "We study the problem of procedure planning in instructional videos, which\naims to make a goal-oriented sequence of action steps given partial visual\nstate observations. The motivation of this problem is to learn a structured and\nplannable state and action space. Recent works succeeded in sequence modeling\nof steps with only sequence-level annotations accessible during training, which\noverlooked the roles of states in the procedures. In this work, we point out\nthat State CHangEs MAtter (SCHEMA) for procedure planning in instructional\nvideos. We aim to establish a more structured state space by investigating the\ncausal relations between steps and states in procedures. Specifically, we\nexplicitly represent each step as state changes and track the state changes in\nprocedures. For step representation, we leveraged the commonsense knowledge in\nlarge language models (LLMs) to describe the state changes of steps via our\ndesigned chain-of-thought prompting. For state change tracking, we align visual\nstate observations with language state descriptions via cross-modal contrastive\nlearning, and explicitly model the intermediate states of the procedure using\nLLM-generated state descriptions. Experiments on CrossTask, COIN, and NIV\nbenchmark datasets demonstrate that our proposed SCHEMA model achieves\nstate-of-the-art performance and obtains explainable visualizations.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.01599v1",
    "published_date": "2024-03-03 19:53:06 UTC",
    "updated_date": "2024-03-03 19:53:06 UTC"
  },
  {
    "arxiv_id": "2403.01598v2",
    "title": "APISR: Anime Production Inspired Real-World Anime Super-Resolution",
    "authors": [
      "Boyang Wang",
      "Fengyu Yang",
      "Xihang Yu",
      "Chao Zhang",
      "Hanbin Zhao"
    ],
    "abstract": "While real-world anime super-resolution (SR) has gained increasing attention\nin the SR community, existing methods still adopt techniques from the\nphotorealistic domain. In this paper, we analyze the anime production workflow\nand rethink how to use characteristics of it for the sake of the real-world\nanime SR. First, we argue that video networks and datasets are not necessary\nfor anime SR due to the repetition use of hand-drawing frames. Instead, we\npropose an anime image collection pipeline by choosing the least compressed and\nthe most informative frames from the video sources. Based on this pipeline, we\nintroduce the Anime Production-oriented Image (API) dataset. In addition, we\nidentify two anime-specific challenges of distorted and faint hand-drawn lines\nand unwanted color artifacts. We address the first issue by introducing a\nprediction-oriented compression module in the image degradation model and a\npseudo-ground truth preparation with enhanced hand-drawn lines. In addition, we\nintroduce the balanced twin perceptual loss combining both anime and\nphotorealistic high-level features to mitigate unwanted color artifacts and\nincrease visual clarity. We evaluate our method through extensive experiments\non the public benchmark, showing our method outperforms state-of-the-art anime\ndataset-trained approaches.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01598v2",
    "published_date": "2024-03-03 19:52:43 UTC",
    "updated_date": "2024-04-04 16:12:51 UTC"
  },
  {
    "arxiv_id": "2403.01580v1",
    "title": "Enhancing Neural Machine Translation of Low-Resource Languages: Corpus Development, Human Evaluation and Explainable AI Architectures",
    "authors": [
      "Séamus Lankford"
    ],
    "abstract": "In the current machine translation (MT) landscape, the Transformer\narchitecture stands out as the gold standard, especially for high-resource\nlanguage pairs. This research delves into its efficacy for low-resource\nlanguage pairs including both the English$\\leftrightarrow$Irish and\nEnglish$\\leftrightarrow$Marathi language pairs. Notably, the study identifies\nthe optimal hyperparameters and subword model type to significantly improve the\ntranslation quality of Transformer models for low-resource language pairs.\n  The scarcity of parallel datasets for low-resource languages can hinder MT\ndevelopment. To address this, gaHealth was developed, the first bilingual\ncorpus of health data for the Irish language. Focusing on the health domain,\nmodels developed using this in-domain dataset exhibited very significant\nimprovements in BLEU score when compared with models from the LoResMT2021\nShared Task. A subsequent human evaluation using the multidimensional quality\nmetrics error taxonomy showcased the superior performance of the Transformer\nsystem in reducing both accuracy and fluency errors compared to an RNN-based\ncounterpart.\n  Furthermore, this thesis introduces adaptNMT and adaptMLLM, two open-source\napplications streamlined for the development, fine-tuning, and deployment of\nneural machine translation models. These tools considerably simplify the setup\nand evaluation process, making MT more accessible to both developers and\ntranslators. Notably, adaptNMT, grounded in the OpenNMT ecosystem, promotes\neco-friendly natural language processing research by highlighting the\nenvironmental footprint of model development. Fine-tuning of MLLMs by adaptMLLM\ndemonstrated advancements in translation performance for two low-resource\nlanguage pairs: English$\\leftrightarrow$Irish and\nEnglish$\\leftrightarrow$Marathi, compared to baselines from the LoResMT2021\nShared Task.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "PhD thesis",
    "pdf_url": "http://arxiv.org/pdf/2403.01580v1",
    "published_date": "2024-03-03 18:08:30 UTC",
    "updated_date": "2024-03-03 18:08:30 UTC"
  },
  {
    "arxiv_id": "2403.01575v1",
    "title": "SARD: A Human-AI Collaborative Story Generation",
    "authors": [
      "Ahmed Y. Radwan",
      "Khaled M. Alasmari",
      "Omar A. Abdulbagi",
      "Emad A. Alghamdi"
    ],
    "abstract": "Generative artificial intelligence (GenAI) has ushered in a new era for\nstorytellers, providing a powerful tool to ignite creativity and explore\nuncharted narrative territories. As technology continues to advance, the\nsynergy between human creativity and AI-generated content holds the potential\nto redefine the landscape of storytelling. In this work, we propose SARD, a\ndrag-and-drop visual interface for generating a multi-chapter story using large\nlanguage models. Our evaluation of the usability of SARD and its creativity\nsupport shows that while node-based visualization of the narrative may help\nwriters build a mental model, it exerts unnecessary mental overhead to the\nwriter and becomes a source of distraction as the story becomes more\nelaborated. We also found that AI generates stories that are less lexically\ndiverse, irrespective of the complexity of the story. We identified some\npatterns and limitations of our tool that can guide the development of future\nhuman-AI co-writing tools.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01575v1",
    "published_date": "2024-03-03 17:48:42 UTC",
    "updated_date": "2024-03-03 17:48:42 UTC"
  },
  {
    "arxiv_id": "2403.05585v2",
    "title": "Plasmon Resonance Model: Investigation of Analysis of Fake News Diffusion Model with Third Mover Intervention Using Soliton Solution in Non-Complete Information Game under Repeated Dilemma Condition",
    "authors": [
      "Yasuko Kawahata"
    ],
    "abstract": "In this research note, we propose a new approach to model the fake news\ndiffusion process within the framework of incomplete information games. In\nparticular, we use nonlinear partial differential equations to represent the\nphenomenon of plasmon resonance, in which the diffusion of fake news is rapidly\namplified within a particular social group or communication network, and\nanalyze its dynamics through a soliton solution approach. In addition, we\nconsider how first mover, second mover, and third mover strategies interact\nwithin this nonlinear system and contribute to the amplification or suppression\nof fake news diffusion. The model aims to understand the mechanisms of fake\nnews proliferation and provide insights into how to prevent or combat it. By\ncombining concepts from the social sciences and the physical sciences, this\nstudy attempts to develop a new theoretical framework for the contemporary\nproblem of fake news.This paper is partially an attempt to utilize \"Generative\nAI\" and was written with educational intent. There are currently no plans for\nit to become a peer-reviewed paper.",
    "categories": [
      "physics.soc-ph",
      "cs.AI"
    ],
    "primary_category": "physics.soc-ph",
    "comment": "Plasmon Resonance Model, Soliton Solution, Third Mover,Fake News,\n  Non-Complete Information Game, Nonlinear Partial Differential Equations,\n  First Mover, Second Mover, Third Mover, Diffusion Dynamics, Iteration Dilemma",
    "pdf_url": "http://arxiv.org/pdf/2403.05585v2",
    "published_date": "2024-03-03 17:46:12 UTC",
    "updated_date": "2024-04-19 14:56:45 UTC"
  },
  {
    "arxiv_id": "2403.01569v1",
    "title": "Kick Back & Relax++: Scaling Beyond Ground-Truth Depth with SlowTV & CribsTV",
    "authors": [
      "Jaime Spencer",
      "Chris Russell",
      "Simon Hadfield",
      "Richard Bowden"
    ],
    "abstract": "Self-supervised learning is the key to unlocking generic computer vision\nsystems. By eliminating the reliance on ground-truth annotations, it allows\nscaling to much larger data quantities. Unfortunately, self-supervised\nmonocular depth estimation (SS-MDE) has been limited by the absence of diverse\ntraining data. Existing datasets have focused exclusively on urban driving in\ndensely populated cities, resulting in models that fail to generalize beyond\nthis domain.\n  To address these limitations, this paper proposes two novel datasets: SlowTV\nand CribsTV. These are large-scale datasets curated from publicly available\nYouTube videos, containing a total of 2M training frames. They offer an\nincredibly diverse set of environments, ranging from snowy forests to coastal\nroads, luxury mansions and even underwater coral reefs. We leverage these\ndatasets to tackle the challenging task of zero-shot generalization,\noutperforming every existing SS-MDE approach and even some state-of-the-art\nsupervised methods.\n  The generalization capabilities of our models are further enhanced by a range\nof components and contributions: 1) learning the camera intrinsics, 2) a\nstronger augmentation regime targeting aspect ratio changes, 3) support frame\nrandomization, 4) flexible motion estimation, 5) a modern transformer-based\narchitecture. We demonstrate the effectiveness of each component in extensive\nablation experiments. To facilitate the development of future research, we make\nthe datasets, code and pretrained models available to the public at\nhttps://github.com/jspenmar/slowtv_monodepth.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01569v1",
    "published_date": "2024-03-03 17:29:03 UTC",
    "updated_date": "2024-03-03 17:29:03 UTC"
  },
  {
    "arxiv_id": "2403.01567v2",
    "title": "ReMatch: Retrieval Enhanced Schema Matching with LLMs",
    "authors": [
      "Eitam Sheetrit",
      "Menachem Brief",
      "Moshik Mishaeli",
      "Oren Elisha"
    ],
    "abstract": "Schema matching is a crucial task in data integration, involving the\nalignment of a source schema with a target schema to establish correspondence\nbetween their elements. This task is challenging due to textual and semantic\nheterogeneity, as well as differences in schema sizes. Although\nmachine-learning-based solutions have been explored in numerous studies, they\noften suffer from low accuracy, require manual mapping of the schemas for model\ntraining, or need access to source schema data which might be unavailable due\nto privacy concerns. In this paper we present a novel method, named ReMatch,\nfor matching schemas using retrieval-enhanced Large Language Models (LLMs). Our\nmethod avoids the need for predefined mapping, any model training, or access to\ndata in the source database. Our experimental results on large real-world\nschemas demonstrate that ReMatch is an effective matcher. By eliminating the\nrequirement for training data, ReMatch becomes a viable solution for real-world\nscenarios.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01567v2",
    "published_date": "2024-03-03 17:14:40 UTC",
    "updated_date": "2024-05-30 14:33:46 UTC"
  },
  {
    "arxiv_id": "2403.01564v3",
    "title": "ComTraQ-MPC: Meta-Trained DQN-MPC Integration for Trajectory Tracking with Limited Active Localization Updates",
    "authors": [
      "Gokul Puthumanaillam",
      "Manav Vora",
      "Melkior Ornik"
    ],
    "abstract": "Optimal decision-making for trajectory tracking in partially observable,\nstochastic environments where the number of active localization updates -- the\nprocess by which the agent obtains its true state information from the sensors\n-- are limited, presents a significant challenge. Traditional methods often\nstruggle to balance resource conservation, accurate state estimation and\nprecise tracking, resulting in suboptimal performance. This problem is\nparticularly pronounced in environments with large action spaces, where the\nneed for frequent, accurate state data is paramount, yet the capacity for\nactive localization updates is restricted by external limitations. This paper\nintroduces ComTraQ-MPC, a novel framework that combines Deep Q-Networks (DQN)\nand Model Predictive Control (MPC) to optimize trajectory tracking with\nconstrained active localization updates. The meta-trained DQN ensures adaptive\nactive localization scheduling, while the MPC leverages available state\ninformation to improve tracking. The central contribution of this work is their\nreciprocal interaction: DQN's update decisions inform MPC's control strategy,\nand MPC's outcomes refine DQN's learning, creating a cohesive, adaptive system.\nEmpirical evaluations in simulated and real-world settings demonstrate that\nComTraQ-MPC significantly enhances operational efficiency and accuracy,\nproviding a generalizable and approximately optimal solution for trajectory\ntracking in complex partially observable environments.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "* Equal contribution",
    "pdf_url": "http://arxiv.org/pdf/2403.01564v3",
    "published_date": "2024-03-03 17:00:28 UTC",
    "updated_date": "2024-08-20 21:52:51 UTC"
  },
  {
    "arxiv_id": "2403.01548v3",
    "title": "In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation",
    "authors": [
      "Shiqi Chen",
      "Miao Xiong",
      "Junteng Liu",
      "Zhengxuan Wu",
      "Teng Xiao",
      "Siyang Gao",
      "Junxian He"
    ],
    "abstract": "Large language models (LLMs) frequently hallucinate and produce factual\nerrors, yet our understanding of why they make these errors remains limited. In\nthis study, we delve into the underlying mechanisms of LLM hallucinations from\nthe perspective of inner representations, and discover a salient pattern\nassociated with hallucinations: correct generations tend to have sharper\ncontext activations in the hidden states of the in-context tokens, compared to\nthe incorrect ones. Leveraging this insight, we propose an entropy-based metric\nto quantify the ``sharpness'' among the in-context hidden states and\nincorporate it into the decoding process to formulate a constrained decoding\napproach. Experiments on various knowledge-seeking and hallucination benchmarks\ndemonstrate our approach's consistent effectiveness, for example, achieving up\nto an 8.6 point improvement on TruthfulQA. We believe this study can improve\nour understanding of hallucinations and serve as a practical solution for\nhallucination mitigation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "code repo is available at:\n  https://github.com/hkust-nlp/Activation_decoding.git",
    "pdf_url": "http://arxiv.org/pdf/2403.01548v3",
    "published_date": "2024-03-03 15:53:41 UTC",
    "updated_date": "2024-03-12 09:49:28 UTC"
  },
  {
    "arxiv_id": "2403.01533v1",
    "title": "Machine learning predicts long-term mortality after acute myocardial infarction using systolic time intervals and routinely collected clinical data",
    "authors": [
      "Bijan Roudini",
      "Boshra Khajehpiri",
      "Hamid Abrishami Moghaddam",
      "Mohamad Forouzanfar"
    ],
    "abstract": "Precise estimation of cardiac patients' current and future comorbidities is\nan important factor in prioritizing continuous physiological monitoring and new\ntherapies. ML models have shown satisfactory performance in short-term\nmortality prediction of patients with heart disease, while their utility in\nlong-term predictions is limited. This study aims to investigate the\nperformance of tree-based ML models on long-term mortality prediction and the\neffect of two recently introduced biomarkers on long-term mortality. This study\nutilized publicly available data from CCHIA at the Ministry of Health and\nWelfare, Taiwan, China. Medical records were used to gather demographic and\nclinical data, including age, gender, BMI, percutaneous coronary intervention\n(PCI) status, and comorbidities such as hypertension, dyslipidemia, ST-segment\nelevation myocardial infarction (STEMI), and non-STEMI. Using medical and\ndemographic records as well as two recently introduced biomarkers, brachial\npre-ejection period (bPEP) and brachial ejection time (bET), collected from 139\npatients with acute myocardial infarction, we investigated the performance of\nadvanced ensemble tree-based ML algorithms (random forest, AdaBoost, and\nXGBoost) to predict all-cause mortality within 14 years. The developed ML\nmodels achieved significantly better performance compared to the baseline LR\n(C-Statistic, 0.80 for random forest, 0.79 for AdaBoost, and 0.78 for XGBoost,\nvs 0.77 for LR) (P-RF<0.001, PAdaBoost<0.001, PXGBoost<0.05). Adding bPEP and\nbET to our feature set significantly improved the algorithms' performance,\nleading to an absolute increase in C-Statistic of up to 0.03 (C-Statistic, 0.83\nfor random forest, 0.82 for AdaBoost, and 0.80 for XGBoost, vs 0.74 for LR)\n(P-RF<0.001, PAdaBoost<0.001, PXGBoost<0.05). This advancement may enable\nbetter treatment prioritization for high-risk individuals.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for publication in \"Intelligent Medicine\"",
    "pdf_url": "http://arxiv.org/pdf/2403.01533v1",
    "published_date": "2024-03-03 15:23:49 UTC",
    "updated_date": "2024-03-03 15:23:49 UTC"
  },
  {
    "arxiv_id": "2403.01528v2",
    "title": "Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey",
    "authors": [
      "Qizhi Pei",
      "Lijun Wu",
      "Kaiyuan Gao",
      "Jinhua Zhu",
      "Yue Wang",
      "Zun Wang",
      "Tao Qin",
      "Rui Yan"
    ],
    "abstract": "The integration of biomolecular modeling with natural language (BL) has\nemerged as a promising interdisciplinary area at the intersection of artificial\nintelligence, chemistry and biology. This approach leverages the rich,\nmultifaceted descriptions of biomolecules contained within textual data sources\nto enhance our fundamental understanding and enable downstream computational\ntasks such as biomolecule property prediction. The fusion of the nuanced\nnarratives expressed through natural language with the structural and\nfunctional specifics of biomolecules described via various molecular modeling\ntechniques opens new avenues for comprehensively representing and analyzing\nbiomolecules. By incorporating the contextual language data that surrounds\nbiomolecules into their modeling, BL aims to capture a holistic view\nencompassing both the symbolic qualities conveyed through language as well as\nquantitative structural characteristics. In this review, we provide an\nextensive analysis of recent advancements achieved through cross modeling of\nbiomolecules and natural language. (1) We begin by outlining the technical\nrepresentations of biomolecules employed, including sequences, 2D graphs, and\n3D structures. (2) We then examine in depth the rationale and key objectives\nunderlying effective multi-modal integration of language and molecular data\nsources. (3) We subsequently survey the practical applications enabled to date\nin this developing research area. (4) We also compile and summarize the\navailable resources and datasets to facilitate future work. (5) Looking ahead,\nwe identify several promising research directions worthy of further exploration\nand investment to continue advancing the field. The related resources and\ncontents are updating in\n\\url{https://github.com/QizhiPei/Awesome-Biomolecule-Language-Cross-Modeling}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.CL",
    "comment": "Survey Paper. 25 pages, 9 figures, and 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2403.01528v2",
    "published_date": "2024-03-03 14:59:47 UTC",
    "updated_date": "2024-03-05 11:12:47 UTC"
  },
  {
    "arxiv_id": "2403.01510v1",
    "title": "End-to-End Human Instance Matting",
    "authors": [
      "Qinglin Liu",
      "Shengping Zhang",
      "Quanling Meng",
      "Bineng Zhong",
      "Peiqiang Liu",
      "Hongxun Yao"
    ],
    "abstract": "Human instance matting aims to estimate an alpha matte for each human\ninstance in an image, which is extremely challenging and has rarely been\nstudied so far. Despite some efforts to use instance segmentation to generate a\ntrimap for each instance and apply trimap-based matting methods, the resulting\nalpha mattes are often inaccurate due to inaccurate segmentation. In addition,\nthis approach is computationally inefficient due to multiple executions of the\nmatting method. To address these problems, this paper proposes a novel\nEnd-to-End Human Instance Matting (E2E-HIM) framework for simultaneous multiple\ninstance matting in a more efficient manner. Specifically, a general perception\nnetwork first extracts image features and decodes instance contexts into latent\ncodes. Then, a united guidance network exploits spatial attention and semantics\nembedding to generate united semantics guidance, which encodes the locations\nand semantic correspondences of all instances. Finally, an instance matting\nnetwork decodes the image features and united semantics guidance to predict all\ninstance-level alpha mattes. In addition, we construct a large-scale human\ninstance matting dataset (HIM-100K) comprising over 100,000 human images with\ninstance alpha matte labels. Experiments on HIM-100K demonstrate the proposed\nE2E-HIM outperforms the existing methods on human instance matting with 50%\nlower errors and 5X faster speed (6 instances in a 640X640 image). Experiments\non the PPM-100, RWP-636, and P3M datasets demonstrate that E2E-HIM also\nachieves competitive performance on traditional human matting.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01510v1",
    "published_date": "2024-03-03 13:17:10 UTC",
    "updated_date": "2024-03-03 13:17:10 UTC"
  },
  {
    "arxiv_id": "2403.01508v1",
    "title": "Soft Reasoning on Uncertain Knowledge Graphs",
    "authors": [
      "Weizhi Fei",
      "Zihao Wang",
      "Hang Yin",
      "Yang Duan",
      "Hanghang Tong",
      "Yangqiu Song"
    ],
    "abstract": "The study of machine learning-based logical query-answering enables reasoning\nwith large-scale and incomplete knowledge graphs. This paper further advances\nthis line of research by considering the uncertainty in the knowledge. The\nuncertain nature of knowledge is widely observed in the real world, but\n\\textit{does not} align seamlessly with the first-order logic underpinning\nexisting studies. To bridge this gap, we study the setting of soft queries on\nuncertain knowledge, which is motivated by the establishment of soft constraint\nprogramming. We further propose an ML-based approach with both forward\ninference and backward calibration to answer soft queries on large-scale,\nincomplete, and uncertain knowledge graphs. Theoretical discussions present\nthat our methods share the same complexity as state-of-the-art inference\nalgorithms for first-order queries. Empirical results justify the superior\nperformance of our approach against previous ML-based methods with number\nembedding extensions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2403.01508v1",
    "published_date": "2024-03-03 13:13:53 UTC",
    "updated_date": "2024-03-03 13:13:53 UTC"
  },
  {
    "arxiv_id": "2403.01489v1",
    "title": "Regeneration Based Training-free Attribution of Fake Images Generated by Text-to-Image Generative Models",
    "authors": [
      "Meiling Li",
      "Zhenxing Qian",
      "Xinpeng Zhang"
    ],
    "abstract": "Text-to-image generative models have recently garnered significant attention\ndue to their ability to generate images based on prompt descriptions. While\nthese models have shown promising performance, concerns have been raised\nregarding the potential misuse of the generated fake images. In response to\nthis, we have presented a simple yet effective training-free method to\nattribute fake images generated by text-to-image models to their source models.\nGiven a test image to be attributed, we first inverse the textual prompt of the\nimage, and then put the reconstructed prompt into different candidate models to\nregenerate candidate fake images. By calculating and ranking the similarity of\nthe test image and the candidate images, we can determine the source of the\nimage. This attribution allows model owners to be held accountable for any\nmisuse of their models. Note that our approach does not limit the number of\ncandidate text-to-image generative models. Comprehensive experiments reveal\nthat (1) Our method can effectively attribute fake images to their source\nmodels, achieving comparable attribution performance with the state-of-the-art\nmethod; (2) Our method has high scalability ability, which is well adapted to\nreal-world attribution scenarios. (3) The proposed method yields satisfactory\nrobustness to common attacks, such as Gaussian blurring, JPEG compression, and\nResizing. We also analyze the factors that influence the attribution\nperformance, and explore the boost brought by the proposed method as a plug-in\nto improve the performance of existing SOTA. We hope our work can shed some\nlight on the solutions to addressing the source of AI-generated images, as well\nas to prevent the misuse of text-to-image generative models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01489v1",
    "published_date": "2024-03-03 11:55:49 UTC",
    "updated_date": "2024-03-03 11:55:49 UTC"
  },
  {
    "arxiv_id": "2403.01479v3",
    "title": "Align-to-Distill: Trainable Attention Alignment for Knowledge Distillation in Neural Machine Translation",
    "authors": [
      "Heegon Jin",
      "Seonil Son",
      "Jemin Park",
      "Youngseok Kim",
      "Hyungjong Noh",
      "Yeonsoo Lee"
    ],
    "abstract": "The advent of scalable deep models and large datasets has improved the\nperformance of Neural Machine Translation. Knowledge Distillation (KD) enhances\nefficiency by transferring knowledge from a teacher model to a more compact\nstudent model. However, KD approaches to Transformer architecture often rely on\nheuristics, particularly when deciding which teacher layers to distill from. In\nthis paper, we introduce the 'Align-to-Distill' (A2D) strategy, designed to\naddress the feature mapping problem by adaptively aligning student attention\nheads with their teacher counterparts during training. The Attention Alignment\nModule in A2D performs a dense head-by-head comparison between student and\nteacher attention heads across layers, turning the combinatorial mapping\nheuristics into a learning problem. Our experiments show the efficacy of A2D,\ndemonstrating gains of up to +3.61 and +0.63 BLEU points for WMT-2022 De->Dsb\nand WMT-2014 En->De, respectively, compared to Transformer baselines.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to LREC-COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.01479v3",
    "published_date": "2024-03-03 11:13:44 UTC",
    "updated_date": "2024-03-25 08:46:15 UTC"
  },
  {
    "arxiv_id": "2403.01475v1",
    "title": "Representation Learning on Heterophilic Graph with Directional Neighborhood Attention",
    "authors": [
      "Qincheng Lu",
      "Jiaqi Zhu",
      "Sitao Luan",
      "Xiao-Wen Chang"
    ],
    "abstract": "Graph Attention Network (GAT) is one of the most popular Graph Neural Network\n(GNN) architecture, which employs the attention mechanism to learn edge weights\nand has demonstrated promising performance in various applications. However,\nsince it only incorporates information from immediate neighborhood, it lacks\nthe ability to capture long-range and global graph information, leading to\nunsatisfactory performance on some datasets, particularly on heterophilic\ngraphs. To address this limitation, we propose the Directional Graph Attention\nNetwork (DGAT) in this paper. DGAT is able to combine the feature-based\nattention with the global directional information extracted from the graph\ntopology. To this end, a new class of Laplacian matrices is proposed which can\nprovably reduce the diffusion distance between nodes. Based on the new\nLaplacian, topology-guided neighbour pruning and edge adding mechanisms are\nproposed to remove the noisy and capture the helpful long-range neighborhood\ninformation. Besides, a global directional attention is designed to enable a\ntopological-aware information propagation. The superiority of the proposed DGAT\nover the baseline GAT has also been verified through experiments on real-world\nbenchmarks and synthetic data sets. It also outperforms the state-of-the-art\n(SOTA) models on 6 out of 7 real-world benchmark datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01475v1",
    "published_date": "2024-03-03 10:59:16 UTC",
    "updated_date": "2024-03-03 10:59:16 UTC"
  },
  {
    "arxiv_id": "2403.06996v1",
    "title": "On the stochastics of human and artificial creativity",
    "authors": [
      "Solve Sæbø",
      "Helge Brovold"
    ],
    "abstract": "What constitutes human creativity, and is it possible for computers to\nexhibit genuine creativity? We argue that achieving human-level intelligence in\ncomputers, or so-called Artificial General Intelligence, necessitates attaining\nalso human-level creativity. We contribute to this discussion by developing a\nstatistical representation of human creativity, incorporating prior insights\nfrom stochastic theory, psychology, philosophy, neuroscience, and chaos theory.\nThis highlights the stochastic nature of the human creative process, which\nincludes both a bias guided, random proposal step, and an evaluation step\ndepending on a flexible or transformable bias structure. The acquired\nrepresentation of human creativity is subsequently used to assess the\ncreativity levels of various contemporary AI systems. Our analysis includes\nmodern AI algorithms such as reinforcement learning, diffusion models, and\nlarge language models, addressing to what extent they measure up to human level\ncreativity. We conclude that these technologies currently lack the capability\nfor autonomous creative action at a human level.",
    "categories": [
      "cs.AI",
      "34, 37, 60, 62",
      "G.1.7; G.3; I.2; J.4; J.5"
    ],
    "primary_category": "cs.AI",
    "comment": "40 pages, 1 figure with 2 sub-figures",
    "pdf_url": "http://arxiv.org/pdf/2403.06996v1",
    "published_date": "2024-03-03 10:38:57 UTC",
    "updated_date": "2024-03-03 10:38:57 UTC"
  },
  {
    "arxiv_id": "2403.10538v1",
    "title": "MATADOR: Automated System-on-Chip Tsetlin Machine Design Generation for Edge Applications",
    "authors": [
      "Tousif Rahman",
      "Gang Mao",
      "Sidharth Maheshwari",
      "Rishad Shafik",
      "Alex Yakovlev"
    ],
    "abstract": "System-on-Chip Field-Programmable Gate Arrays (SoC-FPGAs) offer significant\nthroughput gains for machine learning (ML) edge inference applications via the\ndesign of co-processor accelerator systems. However, the design effort for\ntraining and translating ML models into SoC-FPGA solutions can be substantial\nand requires specialist knowledge aware trade-offs between model performance,\npower consumption, latency and resource utilization. Contrary to other ML\nalgorithms, Tsetlin Machine (TM) performs classification by forming logic\nproposition between boolean actions from the Tsetlin Automata (the learning\nelements) and boolean input features. A trained TM model, usually, exhibits\nhigh sparsity and considerable overlapping of these logic propositions both\nwithin and among the classes. The model, thus, can be translated to RTL-level\ndesign using a miniscule number of AND and NOT gates. This paper presents\nMATADOR, an automated boolean-to-silicon tool with GUI interface capable of\nimplementing optimized accelerator design of the TM model onto SoC-FPGA for\ninference at the edge. It offers automation of the full development pipeline:\nmodel training, system level design generation, design verification and\ndeployment. It makes use of the logic sharing that ensues from propositional\noverlap and creates a compact design by effectively utilizing the TM model's\nsparsity. MATADOR accelerator designs are shown to be up to 13.4x faster, up to\n7x more resource frugal and up to 2x more power efficient when compared to the\nstate-of-the-art Quantized and Binary Deep Neural Network implementations.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.10538v1",
    "published_date": "2024-03-03 10:31:46 UTC",
    "updated_date": "2024-03-03 10:31:46 UTC"
  },
  {
    "arxiv_id": "2403.01467v1",
    "title": "Collaborate to Adapt: Source-Free Graph Domain Adaptation via Bi-directional Adaptation",
    "authors": [
      "Zhen Zhang",
      "Meihan Liu",
      "Anhui Wang",
      "Hongyang Chen",
      "Zhao Li",
      "Jiajun Bu",
      "Bingsheng He"
    ],
    "abstract": "Unsupervised Graph Domain Adaptation (UGDA) has emerged as a practical\nsolution to transfer knowledge from a label-rich source graph to a completely\nunlabelled target graph. However, most methods require a labelled source graph\nto provide supervision signals, which might not be accessible in the real-world\nsettings due to regulations and privacy concerns. In this paper, we explore the\nscenario of source-free unsupervised graph domain adaptation, which tries to\naddress the domain adaptation problem without accessing the labelled source\ngraph. Specifically, we present a novel paradigm called GraphCTA, which\nperforms model adaptation and graph adaptation collaboratively through a series\nof procedures: (1) conduct model adaptation based on node's neighborhood\npredictions in target graph considering both local and global information; (2)\nperform graph adaptation by updating graph structure and node attributes via\nneighborhood contrastive learning; and (3) the updated graph serves as an input\nto facilitate the subsequent iteration of model adaptation, thereby\nestablishing a collaborative loop between model adaptation and graph\nadaptation. Comprehensive experiments are conducted on various public datasets.\nThe experimental results demonstrate that our proposed model outperforms recent\nsource-free baselines by large margins.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by WWW-2024",
    "pdf_url": "http://arxiv.org/pdf/2403.01467v1",
    "published_date": "2024-03-03 10:23:08 UTC",
    "updated_date": "2024-03-03 10:23:08 UTC"
  },
  {
    "arxiv_id": "2403.01456v1",
    "title": "Controlling Cloze-test Question Item Difficulty with PLM-based Surrogate Models for IRT Assessment",
    "authors": [
      "Jingshen Zhang",
      "Jiajun Xie",
      "Xinying Qiu"
    ],
    "abstract": "Item difficulty plays a crucial role in adaptive testing. However, few works\nhave focused on generating questions of varying difficulty levels, especially\nfor multiple-choice (MC) cloze tests. We propose training pre-trained language\nmodels (PLMs) as surrogate models to enable item response theory (IRT)\nassessment, avoiding the need for human test subjects. We also propose two\nstrategies to control the difficulty levels of both the gaps and the\ndistractors using ranking rules to reduce invalid distractors. Experimentation\non a benchmark dataset demonstrates that our proposed framework and methods can\neffectively control and evaluate the difficulty levels of MC cloze tests.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01456v1",
    "published_date": "2024-03-03 09:18:05 UTC",
    "updated_date": "2024-03-03 09:18:05 UTC"
  },
  {
    "arxiv_id": "2403.01437v2",
    "title": "GPTSee: Enhancing Moment Retrieval and Highlight Detection via Description-Based Similarity Features",
    "authors": [
      "Yunzhuo Sun",
      "Yifang Xu",
      "Zien Xie",
      "Yukun Shu",
      "Sidan Du"
    ],
    "abstract": "Moment retrieval (MR) and highlight detection (HD) aim to identify relevant\nmoments and highlights in video from corresponding natural language query.\nLarge language models (LLMs) have demonstrated proficiency in various computer\nvision tasks. However, existing methods for MR\\&HD have not yet been integrated\nwith LLMs. In this letter, we propose a novel two-stage model that takes the\noutput of LLMs as the input to the second-stage transformer encoder-decoder.\nFirst, MiniGPT-4 is employed to generate the detailed description of the video\nframe and rewrite the query statement, fed into the encoder as new features.\nThen, semantic similarity is computed between the generated description and the\nrewritten queries. Finally, continuous high-similarity video frames are\nconverted into span anchors, serving as prior position information for the\ndecoder. Experiments demonstrate that our approach achieves a state-of-the-art\nresult, and by using only span anchors and similarity scores as outputs,\npositioning accuracy outperforms traditional methods, like Moment-DETR.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.01437v2",
    "published_date": "2024-03-03 08:24:28 UTC",
    "updated_date": "2024-03-10 09:56:22 UTC"
  },
  {
    "arxiv_id": "2403.04787v2",
    "title": "Ever-Evolving Memory by Blending and Refining the Past",
    "authors": [
      "Seo Hyun Kim",
      "Keummin Ka",
      "Yohan Jo",
      "Seung-won Hwang",
      "Dongha Lee",
      "Jinyoung Yeo"
    ],
    "abstract": "For a human-like chatbot, constructing a long-term memory is crucial.\nHowever, current large language models often lack this capability, leading to\ninstances of missing important user information or redundantly asking for the\nsame information, thereby diminishing conversation quality. To effectively\nconstruct memory, it is crucial to seamlessly connect past and present\ninformation, while also possessing the ability to forget obstructive\ninformation. To address these challenges, we propose CREEM, a novel memory\nsystem for long-term conversation. Improving upon existing approaches that\nconstruct memory based solely on current sessions, CREEM blends past memories\nduring memory formation. Additionally, we introduce a refining process to\nhandle redundant or outdated information. Unlike traditional paradigms, we view\nresponding and memory construction as inseparable tasks. The blending process,\nwhich creates new memories, also serves as a reasoning step for response\ngeneration by informing the connection between past and present. Through\nevaluation, we demonstrate that CREEM enhances both memory and response\nqualities in multi-session personalized dialogues.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 4 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2403.04787v2",
    "published_date": "2024-03-03 08:12:59 UTC",
    "updated_date": "2024-04-07 04:31:30 UTC"
  },
  {
    "arxiv_id": "2403.06995v1",
    "title": "Exact algorithms and heuristics for capacitated covering salesman problems",
    "authors": [
      "Lucas Porto Maziero",
      "Fábio Luiz Usberti",
      "Celso Cavellucci"
    ],
    "abstract": "This paper introduces the Capacitated Covering Salesman Problem (CCSP),\napproaching the notion of service by coverage in capacitated vehicle routing\nproblems. In CCSP, locations where vehicles can transit are provided, some of\nwhich have customers with demands. The objective is to service customers\nthrough a fleet of vehicles based in a depot, minimizing the total distance\ntraversed by the vehicles. CCSP is unique in the sense that customers, to be\nserviced, do not need to be visited by a vehicle. Instead, they can be serviced\nif they are within a coverage area of the vehicle. This assumption is motivated\nby applications in which some customers are unreachable (e.g., forbidden access\nto vehicles) or visiting every customer is impractical. In this work,\noptimization methodologies are proposed for the CCSP based on ILP (Integer\nLinear Programming) and BRKGA (Biased Random-Key Genetic Algorithm)\nmetaheuristic. Computational experiments conducted on a benchmark of instances\nfor the CCSP evaluate the performance of the methodologies with respect to\nprimal bounds. Furthermore, our ILP formulation is extended in order to create\na novel MILP (Mixed Integer Linear Programming) for the Multi-Depot Covering\nTour Vehicle Routing Problem (MDCTVRP). Computational experiments show that the\nextended MILP formulation outperformed the previous state-of-the-art exact\napproach with respect to optimality gaps. In particular, optimal solutions were\nobtained for several previously unsolved instances.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.06995v1",
    "published_date": "2024-03-03 07:50:29 UTC",
    "updated_date": "2024-03-03 07:50:29 UTC"
  },
  {
    "arxiv_id": "2403.05584v1",
    "title": "Time2Stop: Adaptive and Explainable Human-AI Loop for Smartphone Overuse Intervention",
    "authors": [
      "Adiba Orzikulova",
      "Han Xiao",
      "Zhipeng Li",
      "Yukang Yan",
      "Yuntao Wang",
      "Yuanchun Shi",
      "Marzyeh Ghassemi",
      "Sung-Ju Lee",
      "Anind K Dey",
      "Xuhai \"Orson\" Xu"
    ],
    "abstract": "Despite a rich history of investigating smartphone overuse intervention\ntechniques, AI-based just-in-time adaptive intervention (JITAI) methods for\noveruse reduction are lacking. We develop Time2Stop, an intelligent, adaptive,\nand explainable JITAI system that leverages machine learning to identify\noptimal intervention timings, introduces interventions with transparent AI\nexplanations, and collects user feedback to establish a human-AI loop and adapt\nthe intervention model over time. We conducted an 8-week field experiment\n(N=71) to evaluate the effectiveness of both the adaptation and explanation\naspects of Time2Stop. Our results indicate that our adaptive models\nsignificantly outperform the baseline methods on intervention accuracy (>32.8\\%\nrelatively) and receptivity (>8.0\\%). In addition, incorporating explanations\nfurther enhances the effectiveness by 53.8\\% and 11.4\\% on accuracy and\nreceptivity, respectively. Moreover, Time2Stop significantly reduces overuse,\ndecreasing app visit frequency by 7.0$\\sim$8.9\\%. Our subjective data also\nechoed these quantitative measures. Participants preferred the adaptive\ninterventions and rated the system highly on intervention time accuracy,\neffectiveness, and level of trust. We envision our work can inspire future\nresearch on JITAI systems with a human-AI loop to evolve with users.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.05584v1",
    "published_date": "2024-03-03 06:57:48 UTC",
    "updated_date": "2024-03-03 06:57:48 UTC"
  },
  {
    "arxiv_id": "2403.01413v1",
    "title": "Exploring the Design of Generative AI in Supporting Music-based Reminiscence for Older Adults",
    "authors": [
      "Yucheng Jin",
      "Wanling Cai",
      "Li Chen",
      "Yizhe Zhang",
      "Gavin Doherty",
      "Tonglin Jiang"
    ],
    "abstract": "Music-based reminiscence has the potential to positively impact the\npsychological well-being of older adults. However, the aging process and\nphysiological changes, such as memory decline and limited verbal communication,\nmay impede the ability of older adults to recall their memories and life\nexperiences. Given the advanced capabilities of generative artificial\nintelligence (AI) systems, such as generated conversations and images, and\ntheir potential to facilitate the reminiscing process, this study aims to\nexplore the design of generative AI to support music-based reminiscence in\nolder adults. This study follows a user-centered design approach incorporating\nvarious stages, including detailed interviews with two social workers and two\ndesign workshops (involving ten older adults). Our work contributes to an\nin-depth understanding of older adults' attitudes toward utilizing generative\nAI for supporting music-based reminiscence and identifies concrete design\nconsiderations for the future design of generative AI to enhance the\nreminiscence experience of older adults.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01413v1",
    "published_date": "2024-03-03 06:53:04 UTC",
    "updated_date": "2024-03-03 06:53:04 UTC"
  },
  {
    "arxiv_id": "2403.01407v1",
    "title": "Region-Transformer: Self-Attention Region Based Class-Agnostic Point Cloud Segmentation",
    "authors": [
      "Dipesh Gyawali",
      "Jian Zhang",
      "BB Karki"
    ],
    "abstract": "Point cloud segmentation, which helps us understand the environment of\nspecific structures and objects, can be performed in class-specific and\nclass-agnostic ways. We propose a novel region-based transformer model called\nRegion-Transformer for performing class-agnostic point cloud segmentation. The\nmodel utilizes a region-growth approach and self-attention mechanism to\niteratively expand or contract a region by adding or removing points. It is\ntrained on simulated point clouds with instance labels only, avoiding semantic\nlabels. Attention-based networks have succeeded in many previous methods of\nperforming point cloud segmentation. However, a region-growth approach with\nattention-based networks has yet to be used to explore its performance gain. To\nour knowledge, we are the first to use a self-attention mechanism in a\nregion-growth approach. With the introduction of self-attention to\nregion-growth that can utilize local contextual information of neighborhood\npoints, our experiments demonstrate that the Region-Transformer model\noutperforms previous class-agnostic and class-specific methods on indoor\ndatasets regarding clustering metrics. The model generalizes well to\nlarge-scale scenes. Key advantages include capturing long-range dependencies\nthrough self-attention, avoiding the need for semantic labels during training,\nand applicability to a variable number of objects. The Region-Transformer model\nrepresents a promising approach for flexible point cloud segmentation with\napplications in robotics, digital twinning, and autonomous vehicles.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 5 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2403.01407v1",
    "published_date": "2024-03-03 06:13:43 UTC",
    "updated_date": "2024-03-03 06:13:43 UTC"
  },
  {
    "arxiv_id": "2403.01400v1",
    "title": "Decoupling Weighing and Selecting for Integrating Multiple Graph Pre-training Tasks",
    "authors": [
      "Tianyu Fan",
      "Lirong Wu",
      "Yufei Huang",
      "Haitao Lin",
      "Cheng Tan",
      "Zhangyang Gao",
      "Stan Z. Li"
    ],
    "abstract": "Recent years have witnessed the great success of graph pre-training for graph\nrepresentation learning. With hundreds of graph pre-training tasks proposed,\nintegrating knowledge acquired from multiple pre-training tasks has become a\npopular research topic. In this paper, we identify two important collaborative\nprocesses for this topic: (1) select: how to select an optimal task combination\nfrom a given task pool based on their compatibility, and (2) weigh: how to\nweigh the selected tasks based on their importance. While there currently has\nbeen a lot of work focused on weighing, comparatively little effort has been\ndevoted to selecting. This paper proposes a novel instance-level framework for\nintegrating multiple graph pre-training tasks, Weigh And Select (WAS), where\nthe two collaborative processes, weighing and selecting, are combined by\ndecoupled siamese networks. Specifically, it first adaptively learns an optimal\ncombination of tasks for each instance from a given task pool, based on which a\ncustomized instance-level task weighing strategy is learned. Extensive\nexperiments on 16 graph datasets across node-level and graph-level downstream\ntasks have demonstrated that by combining a few simple but classical tasks, WAS\ncan achieve comparable performance to other leading counterparts. The code is\navailable at https://github.com/TianyuFan0504/WAS.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published as a conference paper at ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.01400v1",
    "published_date": "2024-03-03 05:29:49 UTC",
    "updated_date": "2024-03-03 05:29:49 UTC"
  },
  {
    "arxiv_id": "2403.01384v2",
    "title": "On the Compressibility of Quantized Large Language Models",
    "authors": [
      "Yu Mao",
      "Weilan Wang",
      "Hongchao Du",
      "Nan Guan",
      "Chun Jason Xue"
    ],
    "abstract": "Deploying Large Language Models (LLMs) on edge or mobile devices offers\nsignificant benefits, such as enhanced data privacy and real-time processing\ncapabilities. However, it also faces critical challenges due to the substantial\nmemory requirement of LLMs. Quantization is an effective way of reducing the\nmodel size while maintaining good performance. However, even after\nquantization, LLMs may still be too big to fit entirely into the limited memory\nof edge or mobile devices and have to be partially loaded from the storage to\ncomplete the inference. In this case, the I/O latency of model loading becomes\nthe bottleneck of the LLM inference latency. In this work, we take a\npreliminary step of studying applying data compression techniques to reduce\ndata movement and thus speed up the inference of quantized LLM on\nmemory-constrained devices. In particular, we discussed the compressibility of\nquantized LLMs, the trade-off between the compressibility and performance of\nquantized LLMs, and opportunities to optimize both of them jointly.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01384v2",
    "published_date": "2024-03-03 03:27:07 UTC",
    "updated_date": "2024-05-06 02:29:14 UTC"
  },
  {
    "arxiv_id": "2403.01369v1",
    "title": "A Closer Look at Wav2Vec2 Embeddings for On-Device Single-Channel Speech Enhancement",
    "authors": [
      "Ravi Shankar",
      "Ke Tan",
      "Buye Xu",
      "Anurag Kumar"
    ],
    "abstract": "Self-supervised learned models have been found to be very effective for\ncertain speech tasks such as automatic speech recognition, speaker\nidentification, keyword spotting and others. While the features are undeniably\nuseful in speech recognition and associated tasks, their utility in speech\nenhancement systems is yet to be firmly established, and perhaps not properly\nunderstood. In this paper, we investigate the uses of SSL representations for\nsingle-channel speech enhancement in challenging conditions and find that they\nadd very little value for the enhancement task. Our constraints are designed\naround on-device real-time speech enhancement -- model is causal, the compute\nfootprint is small. Additionally, we focus on low SNR conditions where such\nmodels struggle to provide good enhancement. In order to systematically examine\nhow SSL representations impact performance of such enhancement models, we\npropose a variety of techniques to utilize these embeddings which include\ndifferent forms of knowledge-distillation and pre-training.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.AS",
    "comment": "8 pages; Shorter form accepted in ICASSP 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.01369v1",
    "published_date": "2024-03-03 02:05:17 UTC",
    "updated_date": "2024-03-03 02:05:17 UTC"
  },
  {
    "arxiv_id": "2403.01348v1",
    "title": "SANGRIA: Stacked Autoencoder Neural Networks with Gradient Boosting for Indoor Localization",
    "authors": [
      "Danish Gufran",
      "Saideep Tiku",
      "Sudeep Pasricha"
    ],
    "abstract": "Indoor localization is a critical task in many embedded applications, such as\nasset tracking, emergency response, and realtime navigation. In this article,\nwe propose a novel fingerprintingbased framework for indoor localization called\nSANGRIA that uses stacked autoencoder neural networks with gradient boosted\ntrees. Our approach is designed to overcome the device heterogeneity challenge\nthat can create uncertainty in wireless signal measurements across embedded\ndevices used for localization. We compare SANGRIA to several state-of-the-art\nframeworks and demonstrate 42.96% lower average localization error across\ndiverse indoor locales and heterogeneous devices.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.01348v1",
    "published_date": "2024-03-03 00:01:29 UTC",
    "updated_date": "2024-03-03 00:01:29 UTC"
  }
]
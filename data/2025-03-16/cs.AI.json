{
  "date": "2025-03-16",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-03-16 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 67 篇论文，主要聚焦 AI 代理、LLM 优化、医疗诊断和视觉模型等领域，其中 LG AI Research 团队的 EXAONE Deep 模型和 Eric Betzig 参与的成像技术令人印象深刻，强调了 LLM 在多模态任务和实际应用中的潜力。\n\n### 重点论文讨论\n我们先聊聊今天最引人注目的论文，这些涉及 LLM 代理的创新框架、医疗 AI 的实际应用，以及高效视觉模型的优化。相关论文按主题归类，便于理解。\n\n**AI 代理和 LLM 优化（高话题度领域）**  \n- **AI Agents: Evolution, Architecture, and Real-World Applications（AI 代理：演变、架构和实际应用）**：Naveen Krishnan 的论文审视了 AI 代理从规则基系统到集成大型语言模型的演进，提出一个全面评估框架，平衡任务有效性、效率和安全性，强调了 AI 代理在企业应用中的潜力。  \n- **VeriLA: A Human-Centered Evaluation Framework for Interpretable Verification of LLM Agent Failures（VeriLA：面向 LLM 代理失败的可解释人类中心评估框架）**：Yoo Yeon Sung 等作者开发了 VeriLA 框架，通过人类标准训练的验证模块评估 LLM 代理的执行输出，提高了代理的可解释性和可靠性，为人类-代理协作提供更可信的指南。  \n- **FW-Merging: Scaling Model Merging with Frank-Wolfe Optimization（FW-Merging：使用 Frank-Wolfe 优化扩展模型合并）**：Hao Mark Chen 等团队提出 FW-Merging 方法，将模型合并转化为约束优化问题，能高效融合多个模型，提升多任务学习性能，在 20 个计算机视觉任务上比 SOTA 方法提高 8.39%，展示了大规模模型融合的实用性。  \n- **LLM-Mediated Guidance of MARL Systems（LLM 介导的多代理强化学习系统指导）**：Philipp D. Siedler 和 Ian Gemp 的研究探索了 LLM 用于指导多代理强化学习的干预机制，早期干预显著提升训练效率和性能，31 页详尽实验验证了其在复杂环境中的优势。  \n- **EXAONE Deep: Reasoning Enhanced Language Models（EXAONE Deep：增强推理能力的语言模型）**：LG AI Research 团队发布 EXAONE Deep 系列模型，通过优化推理数据集提升数学和编码任务性能，较基准模型提高 3-7% 准确率，代码开源，突显了 LLM 在推理领域的竞争力。\n\n**医疗和生物应用（实际影响显著）**  \n- **COVID 19 Diagnosis Analysis using Transfer Learning（使用迁移学习的 COVID-19 诊断分析）**：Anjali Dharmik 的论文利用优化后的 ResNet50 模型从胸部 X 光和 CT 扫描中诊断 COVID-19，实现了 97.77% 准确率和 100% 敏感性，强调了 AI 在疫情管理和早期检测中的潜力。  \n- **Fourier-Based 3D Multistage Transformer for Aberration Correction in Multicellular Specimens（基于傅里叶的 3D 多级 Transformer 用于多细胞样本的像差校正）**：Eric Betzig 等知名学者提出 AOViFT 框架，使用 3D Transformer 在傅里叶域校正光学像差，实现高分辨率显微镜成像，实验验证了其在活体斑马鱼胚胎上的鲁棒性，降低了硬件依赖。\n\n**视觉和生成模型创新（高效技术进展）**  \n- **Deblur Gaussian Splatting SLAM（去模糊高斯散射 SLAM）**：Francesco Girlanda 等作者的模型通过子帧轨迹优化从模糊图像重建锐利 3D 地图，结合深度估计和全局调整，在合成和真实数据上达到 SOTA 性能，提升了 SLAM 在动态场景中的鲁棒性。  \n- **SAUCE: Selective Concept Unlearning in Vision-Language Models with Sparse Autoencoders（SAUCE：使用稀疏自动编码器的视觉语言模型选择性概念遗忘）**：Qing Li 等团队开发了 SAUCE 方法，通过稀疏自动编码器细粒度删除特定概念，实验显示比 SOTA 方法提高 18.04% 的遗忘质量，同时保持模型效用，适用于多概念删除任务。\n\n其他论文大多涉及常规主题，如交通预测（e.g., \"Understanding Driver Cognition\"，提出基于漂移扩散模型的驾驶决策框架）、数据优化（e.g., \"BalancedDPO\"，改进文本到图像模型的多指标对齐）和基准测试（e.g., \"SPIN-Bench\"，评估 LLM 的战略规划能力），这些贡献稳固但不具突破性，我们快速掠过不做深究。\n\n总之，今天的更新突显了 AI 在代理指导和医疗诊断的潜力，LLM 相关论文尤其值得关注，助力读者快速筛选感兴趣内容。明天的快报见！",
  "papers": [
    {
      "arxiv_id": "2503.12688v1",
      "title": "Dynamic Angle Selection in X-Ray CT: A Reinforcement Learning Approach to Optimal Stopping",
      "title_zh": "X 射线 CT 中的动态角度选择：一种强化学习方法用于最优停止",
      "authors": [
        "Tianyuan Wang"
      ],
      "abstract": "In industrial X-ray Computed Tomography (CT), the need for rapid in-line\ninspection is critical. Sparse-angle tomography plays a significant role in\nthis by reducing the required number of projections, thereby accelerating\nprocessing and conserving resources. Most existing methods aim to balance\nreconstruction quality and scanning time, typically relying on fixed scan\ndurations. Adaptive adjustment of the number of angles is essential; for\ninstance, more angles may be required for objects with complex geometries or\nnoisier projections. The concept of optimal stopping, which dynamically adjusts\nthis balance according to varying industrial needs, remains underutilized.\nBuilding on our previous work, we integrate optimal stopping into sequential\nOptimal Experimental Design (OED). We propose a novel method for computing the\npolicy gradient within the Actor-Critic framework, enabling the development of\nadaptive policies for informative angle selection and scan termination.\nAdditionally, we investigated the gap between simulation and real-world\napplications in the context of the developed learning-based method. Our trained\nmodel, developed using synthetic data, demonstrates reliable performance when\napplied to real-world data. This approach enhances the flexibility of CT\noperations and expands the applicability of sparse-angle tomography in\nindustrial settings.",
      "tldr_zh": "这篇论文提出了一种基于强化学习的方法，用于X-Ray CT中的动态角度选择和最优停止，旨在根据物体复杂度和噪声水平自适应调整扫描角度，从而平衡重建质量和扫描时间。方法将最优停止概念整合到顺序最优实验设计(OED)中，并通过Actor-Critic框架计算策略梯度，实现信息丰富的角度选择和扫描终止。实验结果表明，使用合成数据训练的模型在真实数据上表现出可靠性能，提升了CT操作的灵活性，并扩展了稀疏角度断层摄影在工业环境中的应用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12688v1",
      "published_date": "2025-03-16 23:09:13 UTC",
      "updated_date": "2025-03-16 23:09:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:39:58.316978"
    },
    {
      "arxiv_id": "2503.12687v1",
      "title": "AI Agents: Evolution, Architecture, and Real-World Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Naveen Krishnan"
      ],
      "abstract": "This paper examines the evolution, architecture, and practical applications\nof AI agents from their early, rule-based incarnations to modern sophisticated\nsystems that integrate large language models with dedicated modules for\nperception, planning, and tool use. Emphasizing both theoretical foundations\nand real-world deployments, the paper reviews key agent paradigms, discusses\nlimitations of current evaluation benchmarks, and proposes a holistic\nevaluation framework that balances task effectiveness, efficiency, robustness,\nand safety. Applications across enterprise, personal assistance, and\nspecialized domains are analyzed, with insights into future research directions\nfor more resilient and adaptive AI agent systems.",
      "tldr_zh": "这篇论文探讨了 AI agents 的演变，从早期的规则-based 系统到现代整合 large language models 的复杂架构，强调了理论基础和实际部署。论文回顾了关键的 agent paradigms，分析了当前 evaluation benchmarks 的局限性，并提出一个 holistic evaluation framework，以平衡任务有效性、效率、鲁棒性和安全性。最终，它考察了 AI agents 在企业、个人助理和专业领域的应用，并为开发更 resilient 和 adaptive 的系统提供了未来研究方向。",
      "categories": [
        "cs.AI",
        "68T05, 68T20",
        "I.2.6; I.2.8; I.2.11"
      ],
      "primary_category": "cs.AI",
      "comment": "52 pages, 4 figures, comprehensive survey and analysis of AI agent\n  evolution, architecture, evaluation frameworks, and applications",
      "pdf_url": "http://arxiv.org/pdf/2503.12687v1",
      "published_date": "2025-03-16 23:07:48 UTC",
      "updated_date": "2025-03-16 23:07:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:40:10.470476"
    },
    {
      "arxiv_id": "2503.12667v1",
      "title": "Plausibility Vaccine: Injecting LLM Knowledge for Event Plausibility",
      "title_zh": "翻译失败",
      "authors": [
        "Jacob Chmura",
        "Jonah Dauvet",
        "Sebastian Sabry"
      ],
      "abstract": "Despite advances in language modelling, distributional methods that build\nsemantic representations from co-occurrences fail to discriminate between\nplausible and implausible events. In this work, we investigate how plausibility\nprediction can be improved by injecting latent knowledge prompted from large\nlanguage models using parameter-efficient fine-tuning. We train 12 task\nadapters to learn various physical properties and association measures and\nperform adapter fusion to compose latent semantic knowledge from each task on\ntop of pre-trained AlBERT embeddings. We automate auxiliary task data\ngeneration, which enables us to scale our approach and fine-tune our learned\nrepresentations across two plausibility datasets. Our code is available at\nhttps://github.com/Jacob-Chmura/plausibility-vaccine.",
      "tldr_zh": "尽管语言模型的分布方法基于共现构建语义表示，但无法有效区分事件的可 plausibility 和 implausible。本文提出“Plausibility Vaccine”方法，通过从 LLM 中注入潜在知识并采用 parameter-efficient fine-tuning 技术，训练 12 个 task adapters 来学习各种物理属性和关联度量。随后，使用 adapter fusion 将这些任务的潜在语义知识整合到预训练的 AlBERT embeddings 上，并自动化辅助任务数据生成，以扩展方法并在两个 plausibility 数据集上进行微调。该研究提升了事件 plausibility 预测的准确性，并提供了开源代码（https://github.com/Jacob-Chmura/plausibility-vaccine）。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12667v1",
      "published_date": "2025-03-16 21:55:17 UTC",
      "updated_date": "2025-03-16 21:55:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:40:22.556083"
    },
    {
      "arxiv_id": "2503.12651v1",
      "title": "VeriLA: A Human-Centered Evaluation Framework for Interpretable Verification of LLM Agent Failures",
      "title_zh": "VeriLA：一种以人为中心的 LLM ",
      "authors": [
        "Yoo Yeon Sung",
        "Hannah Kim",
        "Dan Zhang"
      ],
      "abstract": "AI practitioners increasingly use large language model (LLM) agents in\ncompound AI systems to solve complex reasoning tasks, these agent executions\noften fail to meet human standards, leading to errors that compromise the\nsystem's overall performance. Addressing these failures through human\nintervention is challenging due to the agents' opaque reasoning processes,\nmisalignment with human expectations, the complexity of agent dependencies, and\nthe high cost of manual inspection. This paper thus introduces a human-centered\nevaluation framework for Verifying LLM Agent failures (VeriLA), which\nsystematically assesses agent failures to reduce human effort and make these\nagent failures interpretable to humans. The framework first defines clear\nexpectations of each agent by curating human-designed agent criteria. Then, it\ndevelops a human-aligned agent verifier module, trained with human gold\nstandards, to assess each agent's execution output. This approach enables\ngranular evaluation of each agent's performance by revealing failures from a\nhuman standard, offering clear guidelines for revision, and reducing human\ncognitive load. Our case study results show that VeriLA is both interpretable\nand efficient in helping practitioners interact more effectively with the\nsystem. By upholding accountability in human-agent collaboration, VeriLA paves\nthe way for more trustworthy and human-aligned compound AI systems.",
      "tldr_zh": "这篇论文介绍了VeriLA，一种以人为中心的评估框架，用于可解释地验证LLM Agent失败。该框架通过策划人类设计的代理标准来定义清晰的代理期望，并开发一个基于人类黄金标准训练的代理验证器模块，对每个代理的执行输出进行粒度评估，从而揭示失败原因、提供修订指南并降低人类认知负担。案例研究证明，VeriLA在帮助AI从业者更有效地与系统互动方面表现出色，最终促进更可信赖和人类对齐的复合AI系统。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12651v1",
      "published_date": "2025-03-16 21:11:18 UTC",
      "updated_date": "2025-03-16 21:11:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:40:34.122191"
    },
    {
      "arxiv_id": "2503.12649v2",
      "title": "FW-Merging: Scaling Model Merging with Frank-Wolfe Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Hao Mark Chen",
        "Shell Xu Hu",
        "Wayne Luk",
        "Timothy Hospedales",
        "Hongxiang Fan"
      ],
      "abstract": "Model merging has emerged as a promising approach for multi-task learning\n(MTL), offering a data-efficient alternative to conventional fine-tuning.\nHowever, with the rapid development of the open-source AI ecosystem and the\nincreasing availability of fine-tuned foundation models, existing model merging\nmethods face two key limitations: (i) They are primarily designed for in-house\nfine-tuned models, making them less adaptable to diverse model sources with\npartially unknown model and task information, (ii) They struggle to scale\neffectively when merging numerous model checkpoints. To address these\nchallenges, we formulate model merging as a constrained optimization problem\nand introduce a novel approach: Frank-Wolfe Merging (FW-Merging). Inspired by\nFrank-Wolfe optimization, our approach iteratively selects the most relevant\nmodel in the pool to minimize a linear approximation of the objective function\nand then executes a local merging similar to the Frank-Wolfe update. The\nobjective function is designed to capture the desired behavior of the\ntarget-merged model, while the fine-tuned candidate models define the\nconstraint set. More importantly, FW-Merging serves as an orthogonal technique\nfor existing merging methods, seamlessly integrating with them to further\nenhance accuracy performance. Our experiments show that FW-Merging scales\nacross diverse model sources, remaining stable with 16 irrelevant models and\nimproving by 15.3% with 16 relevant models on 20 CV tasks, while maintaining\nconstant memory overhead, unlike the linear overhead of data-informed merging\nmethods. Compared with the state-of-the-art approaches, FW-Merging surpasses\nthe data-free merging method by 32.8% and outperforms the data-informed\nAdamerging by 8.39% when merging 20 ViT models. Our code is open-sourced at\ngithub.com/hmarkc/FW-Merging.",
      "tldr_zh": "该研究针对模型合并(Model Merging)在多任务学习(MTL)中的局限性，提出了一种名为 FW-Merging 的新方法，利用 Frank-Wolfe Optimization 将模型合并表述为受限优化问题，以适应多样化模型来源和大规模合并。FW-Merging 通过迭代选择最相关模型并执行本地合并，优化目标函数，同时与现有合并方法兼容，提升整体准确性。实验结果显示，在 20 个 CV 任务上，该方法在包含 16 个相关模型时性能提升 15.3%，并在合并 20 个 ViT 模型时，比数据无关方法高 32.8% 和数据相关方法高 8.39%，同时保持恒定的内存开销。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12649v2",
      "published_date": "2025-03-16 21:07:05 UTC",
      "updated_date": "2025-03-25 15:31:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:40:46.223724"
    },
    {
      "arxiv_id": "2503.12642v2",
      "title": "COVID 19 Diagnosis Analysis using Transfer Learning",
      "title_zh": "COVID-19 诊断分析使用迁移学习",
      "authors": [
        "Anjali Dharmik"
      ],
      "abstract": "Coronaviruses, including SARS-CoV-2, are responsible for COVID-19, a highly\ntransmissible disease that emerged in December 2019 in Wuhan, China. During the\npast five years, significant advancements have been made in understanding and\nmitigating the virus. Although the initial outbreak led to global health\ncrises, improved vaccination strategies, antiviral treatments, and AI-driven\ndiagnostic tools have contributed to better disease management. However,\nCOVID-19 continues to pose risks, particularly for immuno-compromised\nindividuals and those with pre-existing conditions. This study explores the use\nof deep learning for a rapid and accurate diagnosis of COVID-19, addressing\nongoing challenges in healthcare infrastructure and testing accessibility. We\npropose an enhanced automated detection system leveraging state-of-the-art\nconvolutional neural networks (CNNs), including updated versions of VGG16,\nVGG19, and ResNet50, to classify COVID-19 infections from chest radiographs and\ncomputerized tomography (CT) scans. Our results, based on an expanded dataset\nof over 6000 medical images, demonstrate that the optimized ResNet50 model\nachieves the highest classification performance, with 97.77% accuracy, 100%\nsensitivity, 93.33% specificity, and a 98.0% F1-score. These findings reinforce\nthe potential of AI-assisted diagnostic tools in improving early detection and\npandemic preparedness.",
      "tldr_zh": "这篇论文探讨了使用 Transfer Learning 进行 COVID-19 诊断分析，以应对医疗基础设施和测试可访问性的挑战。研究提出了一种增强的自动化检测系统，基于先进的 CNNs（如 VGG16、VGG19 和 ResNet50），对胸部 X 光和 CT 扫描图像进行分类。实验使用超过 6000 张医学图像数据集，结果显示优化后的 ResNet50 模型取得了 97.77% 准确率、100% 敏感性、93.33% 特异性和 98.0% F1-score。这些发现突显了 AI 辅助诊断工具在提升早期检测和疫情准备方面的潜力。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12642v2",
      "published_date": "2025-03-16 20:33:39 UTC",
      "updated_date": "2025-03-23 17:38:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:40:58.495308"
    },
    {
      "arxiv_id": "2503.13554v1",
      "title": "LLMs' Leaning in European Elections",
      "title_zh": "翻译失败",
      "authors": [
        "Federico Ricciuti"
      ],
      "abstract": "Many studies suggest that LLMs have left wing leans. The article extends the\nUS presidential election analysis made in previous works, where multiple LLMs\nwere asked to vote between Joe Biden and Donald Trump in a virtual election,\nand the results showed a clear lean of LLMs toward Joe Biden. This article\nconsiders natural follow-up questions that could arise from that experiment,\nsuch as: what is the extent of this phenomenon? Is it generalizable to multiple\nvirtual elections in other countries? The article considers virtual elections\nin ten european countries: Germany, France, Italy, Spain, Poland, Romania,\nNetherlands, Belgium, Czech Republic, and Sweden, and with four different LLMs:\ngpt4o, claude 3.5 sonnet, mistral-large, and gemini-2.0-flash.",
      "tldr_zh": "本研究扩展了先前对LLMs偏向性的分析，探讨这些模型在欧洲选举中的政治倾向，特别是是否延续美国总统选举中对Joe Biden的偏好。研究方法包括模拟十个欧洲国家的虚拟选举（如德国、法国、意大利、西班牙、波兰、罗马尼亚、荷兰、比利时、捷克共和国和瑞典），并使用四种LLMs模型（gpt4o、claude 3.5 sonnet、mistral-large和gemini-2.0-flash）进行投票实验。主要发现表明，LLMs的左翼偏向可能在多个国家普遍存在，为评估AI模型在全球选举中的中立性提供了新洞见。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13554v1",
      "published_date": "2025-03-16 20:17:11 UTC",
      "updated_date": "2025-03-16 20:17:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:41:11.019537"
    },
    {
      "arxiv_id": "2503.13553v1",
      "title": "LLM-Mediated Guidance of MARL Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Philipp D. Siedler",
        "Ian Gemp"
      ],
      "abstract": "In complex multi-agent environments, achieving efficient learning and\ndesirable behaviours is a significant challenge for Multi-Agent Reinforcement\nLearning (MARL) systems. This work explores the potential of combining MARL\nwith Large Language Model (LLM)-mediated interventions to guide agents toward\nmore desirable behaviours. Specifically, we investigate how LLMs can be used to\ninterpret and facilitate interventions that shape the learning trajectories of\nmultiple agents. We experimented with two types of interventions, referred to\nas controllers: a Natural Language (NL) Controller and a Rule-Based (RB)\nController. The NL Controller, which uses an LLM to simulate human-like\ninterventions, showed a stronger impact than the RB Controller. Our findings\nindicate that agents particularly benefit from early interventions, leading to\nmore efficient training and higher performance. Both intervention types\noutperform the baseline without interventions, highlighting the potential of\nLLM-mediated guidance to accelerate training and enhance MARL performance in\nchallenging environments.",
      "tldr_zh": "本研究探讨了如何通过 Large Language Model (LLM) 介导的干预来指导 Multi-Agent Reinforcement Learning (MARL) 系统，在复杂多智能体环境中实现更有效的学习和理想行为。具体方法包括使用 Natural Language (NL) Controller 和 Rule-Based (RB) Controller 两种干预类型，其中 NL Controller 模拟人类干预并显示出更强的影响力。实验发现，早期干预能显著提升智能体的训练效率和性能，两者均优于无干预基线。总体而言，这种 LLM-mediated guidance 方法有助于加速 MARL 在挑战环境中的训练和性能提升。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.MA",
      "comment": "31 pages, 50 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.13553v1",
      "published_date": "2025-03-16 20:16:13 UTC",
      "updated_date": "2025-03-16 20:16:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:41:22.011429"
    },
    {
      "arxiv_id": "2503.12637v1",
      "title": "Understanding Driver Cognition and Decision-Making Behaviors in High-Risk Scenarios: A Drift Diffusion Perspective",
      "title_zh": "翻译失败",
      "authors": [
        "Heye Huang",
        "Zheng Li",
        "Hao Cheng",
        "Haoran Wang",
        "Junkai Jiang",
        "Xiaopeng Li",
        "Arkady Zgonnikov"
      ],
      "abstract": "Ensuring safe interactions between autonomous vehicles (AVs) and human\ndrivers in mixed traffic systems remains a major challenge, particularly in\ncomplex, high-risk scenarios. This paper presents a cognition-decision\nframework that integrates individual variability and commonalities in driver\nbehavior to quantify risk cognition and model dynamic decision-making. First, a\nrisk sensitivity model based on a multivariate Gaussian distribution is\ndeveloped to characterize individual differences in risk cognition. Then, a\ncognitive decision-making model based on the drift diffusion model (DDM) is\nintroduced to capture common decision-making mechanisms in high-risk\nenvironments. The DDM dynamically adjusts decision thresholds by integrating\ninitial bias, drift rate, and boundary parameters, adapting to variations in\nspeed, relative distance, and risk sensitivity to reflect diverse driving\nstyles and risk preferences. By simulating high-risk scenarios with lateral,\nlongitudinal, and multidimensional risk sources in a driving simulator, the\nproposed model accurately predicts cognitive responses and decision behaviors\nduring emergency maneuvers. Specifically, by incorporating driver-specific risk\nsensitivity, the model enables dynamic adjustments of key DDM parameters,\nallowing for personalized decision-making representations in diverse scenarios.\nComparative analysis with IDM, Gipps, and MOBIL demonstrates that DDM more\nprecisely captures human cognitive processes and adaptive decision-making in\nhigh-risk scenarios. These findings provide a theoretical basis for modeling\nhuman driving behavior and offer critical insights for enhancing AV-human\ninteraction in real-world traffic environments.",
      "tldr_zh": "本研究提出一个认知-决策框架，旨在量化驾驶员在高风险场景中的风险认知和动态决策行为，该框架整合个体差异（如基于多元高斯分布的风险敏感性模型）和共同机制。采用Drift Diffusion Model (DDM)来动态调整决策阈值，考虑初始偏差、漂移率和边界参数，以适应速度、相对距离和风险偏好的变化。通过驾驶模拟器模拟横向、纵向和多维风险场景，模型准确预测驾驶员的认知响应和紧急决策行为，并比IDM、Gipps和MOBIL模型更精确地捕捉人类认知过程。这些发现为建模人类驾驶行为提供理论基础，并为改善Autonomous Vehicles (AVs)与人类在混合交通系统中的互动提供关键洞见。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.SI"
      ],
      "primary_category": "cs.AI",
      "comment": "23 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.12637v1",
      "published_date": "2025-03-16 20:11:22 UTC",
      "updated_date": "2025-03-16 20:11:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:41:35.054226"
    },
    {
      "arxiv_id": "2503.12635v1",
      "title": "Hybrid Learners Do Not Forget: A Brain-Inspired Neuro-Symbolic Approach to Continual Learning",
      "title_zh": "混合学习者不会遗忘：一种受脑启发的神经符号方法",
      "authors": [
        "Amin Banayeeanzade",
        "Mohammad Rostami"
      ],
      "abstract": "Continual learning is crucial for creating AI agents that can learn and\nimprove themselves autonomously. A primary challenge in continual learning is\nto learn new tasks without losing previously learned knowledge. Current\ncontinual learning methods primarily focus on enabling a neural network with\nmechanisms that mitigate forgetting effects. Inspired by the two distinct\nsystems in the human brain, System 1 and System 2, we propose a Neuro-Symbolic\nBrain-Inspired Continual Learning (NeSyBiCL) framework that incorporates two\nsubsystems to solve continual learning: A neural network model responsible for\nquickly adapting to the most recent task, together with a symbolic reasoner\nresponsible for retaining previously acquired knowledge from previous tasks.\nMoreover, we design an integration mechanism between these components to\nfacilitate knowledge transfer from the symbolic reasoner to the neural network.\nWe also introduce two compositional continual learning benchmarks and\ndemonstrate that NeSyBiCL is effective and leads to superior performance\ncompared to continual learning methods that merely rely on neural architectures\nto address forgetting.",
      "tldr_zh": "该研究针对持续学习（Continual Learning）中遗忘先前知识的挑战，提出了一种受人类大脑启发的神经符号方法（Neuro-Symbolic Brain-Inspired Continual Learning，NeSyBiCL）。NeSyBiCL 框架包括两个子系统：一个神经网络模型负责快速适应最新任务，以及一个符号推理器（Symbolic Reasoner）用于保留之前任务的知识，并通过集成机制促进知识转移。研究者还引入了两个组合持续学习基准，并在实验中证明，NeSyBiCL 比仅依赖神经架构的现有方法表现出色，有效降低了遗忘效果。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12635v1",
      "published_date": "2025-03-16 20:09:19 UTC",
      "updated_date": "2025-03-16 20:09:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:41:47.067060"
    },
    {
      "arxiv_id": "2503.12626v1",
      "title": "Automated Planning for Optimal Data Pipeline Instantiation",
      "title_zh": "自动化规划用于最优数据管道实例化",
      "authors": [
        "Leonardo Rosa Amado",
        "Adriano Vogel",
        "Dalvan Griebler",
        "Gabriel Paludo Licks",
        "Eric Simon",
        "Felipe Meneguzzi"
      ],
      "abstract": "Data pipeline frameworks provide abstractions for implementing sequences of\ndata-intensive transformation operators, automating the deployment and\nexecution of such transformations in a cluster. Deploying a data pipeline,\nhowever, requires computing resources to be allocated in a data center, ideally\nminimizing the overhead for communicating data and executing operators in the\npipeline while considering each operator's execution requirements. In this\npaper, we model the problem of optimal data pipeline deployment as planning\nwith action costs, where we propose heuristics aiming to minimize total\nexecution time. Experimental results indicate that the heuristics can\noutperform the baseline deployment and that a heuristic based on connections\noutperforms other strategies.",
      "tldr_zh": "这篇论文探讨了数据管道（data pipeline）的优化部署问题，将其建模为带有行动成本的规划问题（planning with action costs），旨在最小化数据通信开销和执行时间，同时考虑每个操作者的执行要求。作者提出了一系列启发式方法（heuristics）来指导部署决策。实验结果显示，这些启发式方法优于基线策略，特别是基于连接的启发式在总执行时间上表现出色。",
      "categories": [
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12626v1",
      "published_date": "2025-03-16 19:43:12 UTC",
      "updated_date": "2025-03-16 19:43:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:41:58.344086"
    },
    {
      "arxiv_id": "2503.12623v2",
      "title": "MAVEN: Multi-modal Attention for Valence-Arousal Emotion Network",
      "title_zh": "翻译失败",
      "authors": [
        "Vrushank Ahire",
        "Kunal Shah",
        "Mudasir Nazir Khan",
        "Nikhil Pakhale",
        "Lownish Rai Sookha",
        "M. A. Ganaie",
        "Abhinav Dhall"
      ],
      "abstract": "Dynamic emotion recognition in the wild remains challenging due to the\ntransient nature of emotional expressions and temporal misalignment of\nmulti-modal cues. Traditional approaches predict valence and arousal and often\noverlook the inherent correlation between these two dimensions. The proposed\nMulti-modal Attention for Valence-Arousal Emotion Network (MAVEN) integrates\nvisual, audio, and textual modalities through a bi-directional cross-modal\nattention mechanism. MAVEN uses modality-specific encoders to extract features\nfrom synchronized video frames, audio segments, and transcripts, predicting\nemotions in polar coordinates following Russell's circumplex model. The\nevaluation of the Aff-Wild2 dataset using MAVEN achieved a concordance\ncorrelation coefficient (CCC) of 0.3061, surpassing the ResNet-50 baseline\nmodel with a CCC of 0.22. The multistage architecture captures the subtle and\ntransient nature of emotional expressions in conversational videos and improves\nemotion recognition in real-world situations. The code is available at:\nhttps://github.com/Vrushank-Ahire/MAVEN_8th_ABAW",
      "tldr_zh": "本文提出MAVEN框架，用于处理野外动态情绪识别的挑战，通过双向跨模态注意力机制整合视觉、音频和文本模态，解决情绪正负性（valence）和唤醒度（arousal）之间的相关性问题。MAVEN使用模态特定编码器从同步视频帧、音频段和转录文本中提取特征，并在Russell's circumplex model的极坐标系统中预测情绪。实验结果显示，在Aff-Wild2数据集上，MAVEN的CCC达到0.3061，比ResNet-50基线模型的0.22有显著提升，从而更好地捕捉对话视频中情绪表达的细微和暂定性质。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12623v2",
      "published_date": "2025-03-16 19:32:32 UTC",
      "updated_date": "2025-05-02 07:17:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:42:11.148501"
    },
    {
      "arxiv_id": "2503.16518v2",
      "title": "Advancing Human-Machine Teaming: Concepts, Challenges, and Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Dian Chen",
        "Han Jun Yoon",
        "Zelin Wan",
        "Nithin Alluru",
        "Sang Won Lee",
        "Richard He",
        "Terrence J. Moore",
        "Frederica F. Nelson",
        "Sunghyun Yoon",
        "Hyuk Lim",
        "Dan Dongseong Kim",
        "Jin-Hee Cho"
      ],
      "abstract": "Human-Machine Teaming (HMT) is revolutionizing collaboration across domains\nsuch as defense, healthcare, and autonomous systems by integrating AI-driven\ndecision-making, trust calibration, and adaptive teaming. This survey presents\na comprehensive taxonomy of HMT, analyzing theoretical models, including\nreinforcement learning, instance-based learning, and interdependence theory,\nalongside interdisciplinary methodologies. Unlike prior reviews, we examine\nteam cognition, ethical AI, multi-modal interactions, and real-world evaluation\nframeworks. Key challenges include explainability, role allocation, and\nscalable benchmarking. We propose future research in cross-domain adaptation,\ntrust-aware AI, and standardized testbeds. By bridging computational and social\nsciences, this work lays a foundation for resilient, ethical, and scalable HMT\nsystems.",
      "tldr_zh": "这篇论文调查了 Human-Machine Teaming (HMT)，探讨了其在国防、医疗和自主系统等领域的概念、挑战和应用，通过整合 AI 驱动决策、信任校准和适应性团队合作来革新人类-机器协作。论文提出一个全面的 HMT 分类体系，分析了理论模型如 reinforcement learning、instance-based learning 和 interdependence theory，并结合跨学科方法考察团队认知、ethical AI 和 multi-modal interactions 等方面。关键挑战包括 explainability、role allocation 和 scalable benchmarking，而未来研究方向聚焦于 cross-domain adaptation、trust-aware AI 和 standardized testbeds，以桥接计算和社会科学，奠定 resilient、ethical 和 scalable HMT 系统的基础。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16518v2",
      "published_date": "2025-03-16 19:32:17 UTC",
      "updated_date": "2025-05-06 17:34:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:42:22.478311"
    },
    {
      "arxiv_id": "2503.12617v1",
      "title": "Scaling Semantic Categories: Investigating the Impact on Vision Transformer Labeling Performance",
      "title_zh": "语义类别的扩展：调查其对视觉Transformer标签性能的影响",
      "authors": [
        "Anthony Lamelas",
        "Harrison Muchnic"
      ],
      "abstract": "This study explores the impact of scaling semantic categories on the image\nclassification performance of vision transformers (ViTs). In this specific\ncase, the CLIP server provided by Jina AI is used for experimentation. The\nresearch hypothesizes that as the number of ground truth and artificially\nintroduced semantically equivalent categories increases, the labeling accuracy\nof ViTs improves until a theoretical maximum or limit is reached. A wide\nvariety of image datasets were chosen to test this hypothesis. These datasets\nwere processed through a custom function in Python designed to evaluate the\nmodel's accuracy, with adjustments being made to account for format differences\nbetween datasets. By exponentially introducing new redundant categories, the\nexperiment assessed accuracy trends until they plateaued, decreased, or\nfluctuated inconsistently. The findings show that while semantic scaling\ninitially increases model performance, the benefits diminish or reverse after\nsurpassing a critical threshold, providing insight into the limitations and\npossible optimization of category labeling strategies for ViTs.",
      "tldr_zh": "这篇论文探讨了扩展语义类别对 Vision Transformer (ViTs) 图像分类性能的影响，假设类别数量增加会提升标记准确率直至达到理论极限。研究使用 Jina AI 的 CLIP 服务器和多种图像数据集，通过自定义 Python 函数指数引入冗余类别，并观察准确率趋势。结果表明，语义扩展最初提高模型性能，但超过关键阈值后，益处会减弱或逆转，为优化 ViTs 的类别标记策略提供了宝贵见解。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "4 pages, 7 figures, submitted to CVPR (feedback pending)",
      "pdf_url": "http://arxiv.org/pdf/2503.12617v1",
      "published_date": "2025-03-16 19:14:21 UTC",
      "updated_date": "2025-03-16 19:14:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:42:34.492833"
    },
    {
      "arxiv_id": "2504.05325v1",
      "title": "Unequal Opportunities: Examining the Bias in Geographical Recommendations by Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Shiran Dudy",
        "Thulasi Tholeti",
        "Resmi Ramachandranpillai",
        "Muhammad Ali",
        "Toby Jia-Jun Li",
        "Ricardo Baeza-Yates"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have made them a popular\ninformation-seeking tool among end users. However, the statistical training\nmethods for LLMs have raised concerns about their representation of\nunder-represented topics, potentially leading to biases that could influence\nreal-world decisions and opportunities. These biases could have significant\neconomic, social, and cultural impacts as LLMs become more prevalent, whether\nthrough direct interactions--such as when users engage with chatbots or\nautomated assistants--or through their integration into third-party\napplications (as agents), where the models influence decision-making processes\nand functionalities behind the scenes. Our study examines the biases present in\nLLMs recommendations of U.S. cities and towns across three domains: relocation,\ntourism, and starting a business. We explore two key research questions: (i)\nHow similar LLMs responses are, and (ii) How this similarity might favor areas\nwith certain characteristics over others, introducing biases. We focus on the\nconsistency of LLMs responses and their tendency to over-represent or\nunder-represent specific locations. Our findings point to consistent\ndemographic biases in these recommendations, which could perpetuate a\n``rich-get-richer'' effect that widens existing economic disparities.",
      "tldr_zh": "本研究调查了Large Language Models (LLMs) 在地理推荐中的偏见问题，特别是针对美国城市和城镇在搬迁、旅游和创业领域的推荐。研究者探讨了LLMs响应的一致性，以及这种一致性如何偏好特定特征的区域，导致demographic biases。结果显示，这些偏见可能加剧“富者更富”的效应，扩大经济和社会不平等，并强调了在LLMs应用中缓解偏见的重要性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.05325v1",
      "published_date": "2025-03-16 18:59:00 UTC",
      "updated_date": "2025-03-16 18:59:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:42:45.170349"
    },
    {
      "arxiv_id": "2503.12613v2",
      "title": "Negotiative Alignment: Embracing Disagreement to Achieve Fairer Outcomes -- Insights from Urban Studies",
      "title_zh": "翻译失败",
      "authors": [
        "Rashid Mushkani",
        "Hugo Berard",
        "Shin Koseki"
      ],
      "abstract": "Cities are not monolithic; they are arenas of negotiation among groups that\nhold varying needs, values, and experiences. Conventional methods of urban\nassessment -- from standardized surveys to AI-driven evaluations -- frequently\nrely on a single consensus metric (e.g., an average measure of inclusivity or\nsafety). Although such aggregations simplify design decisions, they risk\nobscuring the distinct perspectives of marginalized populations. In this paper,\nwe present findings from a community-centered study in Montreal involving 35\nresidents with diverse demographic and social identities, particularly\nwheelchair users, seniors, and LGBTQIA2+ individuals. Using rating and ranking\ntasks on 20 urban sites, we observe that disagreements are systematic rather\nthan random, reflecting structural inequalities, differing cultural values, and\npersonal experiences of safety and accessibility.\n  Based on these empirical insights, we propose negotiative alignment, an AI\nframework that treats disagreement as an essential input to be preserved,\nanalyzed, and addressed. Negotiative alignment builds on pluralistic models by\ndynamically updating stakeholder preferences through multi-agent negotiation\nmechanisms, ensuring no single perspective is marginalized. We outline how this\nframework can be integrated into urban analytics -- and other decision-making\ncontexts -- to retain minority viewpoints, adapt to changing stakeholder\nconcerns, and enhance fairness and accountability. The study demonstrates that\npreserving and engaging with disagreement, rather than striving for an\nartificial consensus, can produce more equitable and responsive AI-driven\noutcomes in urban design.",
      "tldr_zh": "这篇论文探讨了城市评估中分歧的重要性，通过在蒙特利尔的一项社区研究（涉及35名多样化居民，包括轮椅用户、老年人及LGBTQIA2+人士），发现分歧在评分和排名任务中是系统性的，源于结构性不平等、文化差异及个人安全体验。作者提出negotiative alignment框架，将分歧视为关键输入，通过multi-agent negotiation机制动态更新利益相关者偏好，确保少数派观点不被边缘化。最终，该框架可整合到城市分析和其他决策场景中，提升AI驱动结果的公平性、责任性和适应性。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "cs.MA"
      ],
      "primary_category": "cs.HC",
      "comment": "16 pages, 13 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.12613v2",
      "published_date": "2025-03-16 18:55:54 UTC",
      "updated_date": "2025-05-08 03:26:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:42:58.245443"
    },
    {
      "arxiv_id": "2503.12595v1",
      "title": "Point Cloud Based Scene Segmentation: A Survey",
      "title_zh": "基于点云的场景分割：综述",
      "authors": [
        "Dan Halperin",
        "Niklas Eisl"
      ],
      "abstract": "Autonomous driving is a safety-critical application, and it is therefore a\ntop priority that the accompanying assistance systems are able to provide\nprecise information about the surrounding environment of the vehicle. Tasks\nsuch as 3D Object Detection deliver an insufficiently detailed understanding of\nthe surrounding scene because they only predict a bounding box for foreground\nobjects. In contrast, 3D Semantic Segmentation provides richer and denser\ninformation about the environment by assigning a label to each individual\npoint, which is of paramount importance for autonomous driving tasks, such as\nnavigation or lane changes. To inspire future research, in this review paper,\nwe provide a comprehensive overview of the current state-of-the-art methods in\nthe field of Point Cloud Semantic Segmentation for autonomous driving. We\ncategorize the approaches into projection-based, 3D-based and hybrid methods.\nMoreover, we discuss the most important and commonly used datasets for this\ntask and also emphasize the importance of synthetic data to support research\nwhen real-world data is limited. We further present the results of the\ndifferent methods and compare them with respect to their segmentation accuracy\nand efficiency.",
      "tldr_zh": "这篇调查论文回顾了基于点云的场景分割技术在自动驾驶中的应用，强调了3D Semantic Segmentation的重要性，因为它能为每个点分配标签，从而提供比3D Object Detection更详细的环境信息。该文将现有方法分类为projection-based、3D-based和hybrid方法，并讨论了常用数据集以及合成数据的必要性，以弥补真实数据不足。最终，论文比较了不同方法的分割准确性和效率，为未来点云语义分割研究提供了全面参考。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12595v1",
      "published_date": "2025-03-16 18:02:41 UTC",
      "updated_date": "2025-03-16 18:02:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:43:08.678936"
    },
    {
      "arxiv_id": "2503.12593v1",
      "title": "Fourier-Based 3D Multistage Transformer for Aberration Correction in Multicellular Specimens",
      "title_zh": "翻译失败",
      "authors": [
        "Thayer Alshaabi",
        "Daniel E. Milkie",
        "Gaoxiang Liu",
        "Cyna Shirazinejad",
        "Jason L. Hong",
        "Kemal Achour",
        "Frederik Görlitz",
        "Ana Milunovic-Jevtic",
        "Cat Simmons",
        "Ibrahim S. Abuzahriyeh",
        "Erin Hong",
        "Samara Erin Williams",
        "Nathanael Harrison",
        "Evan Huang",
        "Eun Seok Bae",
        "Alison N. Killilea",
        "David G. Drubin",
        "Ian A. Swinburne",
        "Srigokul Upadhyayula",
        "Eric Betzig"
      ],
      "abstract": "High-resolution tissue imaging is often compromised by sample-induced optical\naberrations that degrade resolution and contrast. While wavefront sensor-based\nadaptive optics (AO) can measure these aberrations, such hardware solutions are\ntypically complex, expensive to implement, and slow when serially mapping\nspatially varying aberrations across large fields of view. Here, we introduce\nAOViFT (Adaptive Optical Vision Fourier Transformer) -- a machine\nlearning-based aberration sensing framework built around a 3D multistage Vision\nTransformer that operates on Fourier domain embeddings. AOViFT infers\naberrations and restores diffraction-limited performance in puncta-labeled\nspecimens with substantially reduced computational cost, training time, and\nmemory footprint compared to conventional architectures or real-space networks.\nWe validated AOViFT on live gene-edited zebrafish embryos, demonstrating its\nability to correct spatially varying aberrations using either a deformable\nmirror or post-acquisition deconvolution. By eliminating the need for the guide\nstar and wavefront sensing hardware and simplifying the experimental workflow,\nAOViFT lowers technical barriers for high-resolution volumetric microscopy\nacross diverse biological samples.",
      "tldr_zh": "这篇论文提出了 AOViFT（Adaptive Optical Vision Fourier Transformer），一种基于机器学习的框架，用于校正多细胞样本中样本诱发的光学畸变问题。AOViFT 采用 3D 多阶段 Vision Transformer 在 Fourier domain 嵌入上操作，能够高效推断畸变并恢复衍射极限性能，同时显著降低计算成本、训练时间和内存占用。实验在活体基因编辑的 zebrafish 胚胎上验证了其效果，可通过 deformable mirror 或后采集 deconvolution 修正空间变异畸变。该框架简化了实验流程，消除了对波前传感硬件和引导星的需求，从而降低高分辨率体积显微镜的技术门槛。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.LG",
        "physics.bio-ph",
        "q-bio.QM"
      ],
      "primary_category": "eess.IV",
      "comment": "52 pages, 6 figures, 23 si figures, 8 si tables",
      "pdf_url": "http://arxiv.org/pdf/2503.12593v1",
      "published_date": "2025-03-16 17:59:20 UTC",
      "updated_date": "2025-03-16 17:59:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:43:23.069017"
    },
    {
      "arxiv_id": "2503.12592v1",
      "title": "MoECollab: Democratizing LLM Development Through Collaborative Mixture of Experts",
      "title_zh": "翻译失败",
      "authors": [
        "Harshit"
      ],
      "abstract": "Large Language Model (LLM) development has become increasingly centralized,\nlimiting participation to well-resourced organizations. This paper introduces\nMoECollab, a novel framework leveraging Mixture of Experts (MoE) architecture\nto enable distributed, collaborative LLM development. By decomposing monolithic\nmodels into specialized expert modules coordinated by a trainable gating\nnetwork, our framework allows diverse contributors to participate regardless of\ncomputational resources. We provide a complete technical implementation with\nmathematical foundations for expert dynamics, gating mechanisms, and\nintegration strategies. Experiments on multiple datasets demonstrate that our\napproach achieves accuracy improvements of 3-7% over baseline models while\nreducing computational requirements by 34%. Expert specialization yields\nsignificant domain-specific gains, with improvements from 51% to 88% F1 score\nin general classification and from 23% to 44% accuracy in news categorization.\nWe formalize the routing entropy optimization problem and demonstrate how\nproper regularization techniques lead to 14% higher expert utilization rates.\nThese results validate MoECollab as an effective approach for democratizing LLM\ndevelopment through architecturally-supported collaboration.",
      "tldr_zh": "本论文提出MoECollab框架，利用Mixture of Experts (MoE)架构，实现分布式协作式Large Language Model (LLM)开发，从而打破资源集中化的限制，让更多贡献者参与。框架将单体模型分解为专业专家模块，由可训练的门控网络协调，提供数学基础和完整技术实现。实验结果显示，MoECollab在多个数据集上比基线模型准确率提升3-7%，计算需求减少34%，并在领域特定任务中取得显著改进，如一般分类的F1 score从51%提高到88%，新闻分类准确率从23%到44%。此外，通过优化路由熵问题，专家利用率提升14%，验证了该方法在民主化LLM开发方面的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12592v1",
      "published_date": "2025-03-16 17:52:40 UTC",
      "updated_date": "2025-03-16 17:52:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:43:34.317987"
    },
    {
      "arxiv_id": "2503.14530v2",
      "title": "SAUCE: Selective Concept Unlearning in Vision-Language Models with Sparse Autoencoders",
      "title_zh": "翻译失败",
      "authors": [
        "Qing Li",
        "Jiahui Geng",
        "Derui Zhu",
        "Fengyu Cai",
        "Chenyang Lyu",
        "Fakhri Karray"
      ],
      "abstract": "Unlearning methods for vision-language models (VLMs) have primarily adapted\ntechniques from large language models (LLMs), relying on weight updates that\ndemand extensive annotated forget sets. Moreover, these methods perform\nunlearning at a coarse granularity, often leading to excessive forgetting and\nreduced model utility. To address this issue, we introduce SAUCE, a novel\nmethod that leverages sparse autoencoders (SAEs) for fine-grained and selective\nconcept unlearning in VLMs. Briefly, SAUCE first trains SAEs to capture\nhigh-dimensional, semantically rich sparse features. It then identifies the\nfeatures most relevant to the target concept for unlearning. During inference,\nit selectively modifies these features to suppress specific concepts while\npreserving unrelated information. We evaluate SAUCE on two distinct VLMs,\nLLaVA-v1.5-7B and LLaMA-3.2-11B-Vision-Instruct, across two types of tasks:\nconcrete concept unlearning (objects and sports scenes) and abstract concept\nunlearning (emotions, colors, and materials), encompassing a total of 60\nconcepts. Extensive experiments demonstrate that SAUCE outperforms\nstate-of-the-art methods by 18.04% in unlearning quality while maintaining\ncomparable model utility. Furthermore, we investigate SAUCE's robustness\nagainst widely used adversarial attacks, its transferability across models, and\nits scalability in handling multiple simultaneous unlearning requests. Our\nfindings establish SAUCE as an effective and scalable solution for selective\nconcept unlearning in VLMs.",
      "tldr_zh": "该论文提出 SAUCE，一种利用 sparse autoencoders (SAEs) 实现视觉语言模型 (VLMs) 中细粒度选择性概念 unlearning 的新方法，以解决现有 unlearning 技术依赖权重更新、过度遗忘和模型效用降低的问题。SAUCE 通过训练 SAEs 捕捉高维语义稀疏特征，识别并选择性地修改与目标概念相关的特征，从而在推理阶段抑制特定概念（如对象、体育场景、情绪、颜色或材料）的同时保留无关信息。实验在 LLaVA-v1.5-7B 和 LLaMA-3.2-11B-Vision-Instruct 等模型上评估了 60 个概念，结果显示 SAUCE 在 unlearning 质量上比最先进方法提高 18.04%，并保持了可比的模型效用，同时证明了其对对抗攻击的鲁棒性、模型间的可转移性和处理多重 unlearning 请求的扩展性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "More comparative experiments are needed",
      "pdf_url": "http://arxiv.org/pdf/2503.14530v2",
      "published_date": "2025-03-16 17:32:23 UTC",
      "updated_date": "2025-03-20 05:47:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:43:46.947612"
    },
    {
      "arxiv_id": "2503.12575v1",
      "title": "BalancedDPO: Adaptive Multi-Metric Alignment",
      "title_zh": "BalancedDPO：自适应多指标对齐",
      "authors": [
        "Dipesh Tamboli",
        "Souradip Chakraborty",
        "Aditya Malusare",
        "Biplab Banerjee",
        "Amrit Singh Bedi",
        "Vaneet Aggarwal"
      ],
      "abstract": "Text-to-image (T2I) diffusion models have made remarkable advancements, yet\naligning them with diverse preferences remains a persistent challenge. Current\nmethods often optimize single metrics or depend on narrowly curated datasets,\nleading to overfitting and limited generalization across key visual quality\nmetrics. We present BalancedDPO, a novel extension of Direct Preference\nOptimization (DPO) that addresses these limitations by simultaneously aligning\nT2I diffusion models with multiple metrics, including human preference, CLIP\nscore, and aesthetic quality. Our key novelty lies in aggregating consensus\nlabels from diverse metrics in the preference distribution space as compared to\nexisting reward mixing approaches, enabling robust and scalable multi-metric\nalignment while maintaining the simplicity of the standard DPO pipeline that we\nrefer to as BalancedDPO. Our evaluations on the Pick-a-Pic, PartiPrompt and HPD\ndatasets show that BalancedDPO achieves state-of-the-art results, outperforming\nexisting approaches across all major metrics. BalancedDPO improves the average\nwin rates by 15%, 7.1%, and 10.3% on Pick-a-pic, PartiPrompt and HPD,\nrespectively, from the DiffusionDPO.",
      "tldr_zh": "本文提出 BalancedDPO，一种 Direct Preference Optimization (DPO) 的扩展方法，用于同时对齐文本到图像 (T2I) 扩散模型与多个指标，包括人类偏好、CLIP score 和美学质量，以解决现有方法在单一指标优化和泛化方面的局限性。BalancedDPO 的关键创新在于在偏好分布空间中聚合不同指标的共识标签，而不是传统的奖励混合方式，从而实现更稳健且可扩展的多指标对齐，同时保持标准 DPO 管道的简单性。在 Pick-a-Pic、PartiPrompt 和 HPD 数据集上的评估中，BalancedDPO 相比 DiffusionDPO 平均胜率提高了 15%、7.1% 和 10.3%，取得了最先进的结果。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12575v1",
      "published_date": "2025-03-16 17:06:00 UTC",
      "updated_date": "2025-03-16 17:06:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:43:59.444370"
    },
    {
      "arxiv_id": "2503.12572v1",
      "title": "Deblur Gaussian Splatting SLAM",
      "title_zh": "去模糊高斯喷溅 SL",
      "authors": [
        "Francesco Girlanda",
        "Denys Rozumnyi",
        "Marc Pollefeys",
        "Martin R. Oswald"
      ],
      "abstract": "We present Deblur-SLAM, a robust RGB SLAM pipeline designed to recover sharp\nreconstructions from motion-blurred inputs. The proposed method bridges the\nstrengths of both frame-to-frame and frame-to-model approaches to model\nsub-frame camera trajectories that lead to high-fidelity reconstructions in\nmotion-blurred settings. Moreover, our pipeline incorporates techniques such as\nonline loop closure and global bundle adjustment to achieve a dense and precise\nglobal trajectory. We model the physical image formation process of\nmotion-blurred images and minimize the error between the observed blurry images\nand rendered blurry images obtained by averaging sharp virtual sub-frame\nimages. Additionally, by utilizing a monocular depth estimator alongside the\nonline deformation of Gaussians, we ensure precise mapping and enhanced image\ndeblurring. The proposed SLAM pipeline integrates all these components to\nimprove the results. We achieve state-of-the-art results for sharp map\nestimation and sub-frame trajectory recovery both on synthetic and real-world\nblurry input data.",
      "tldr_zh": "该研究提出了 Deblur-SLAM，一种鲁棒的 RGB SLAM 管道，用于从运动模糊输入中恢复清晰的重建，通过结合 frame-to-frame 和 frame-to-model 方法来建模子帧相机轨迹，实现高保真重建。系统还整合在线 loop closure 和全局 bundle adjustment 技术，以及基于单目深度估计器和在线高斯变形的精确映射和图像去模糊策略，以最小化观察模糊图像与渲染模糊图像之间的错误。实验结果显示，该管道在合成和真实世界模糊数据上实现了最先进性能，显著提升了清晰地图估计和子帧轨迹恢复的准确性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12572v1",
      "published_date": "2025-03-16 16:59:51 UTC",
      "updated_date": "2025-03-16 16:59:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:44:09.971144"
    },
    {
      "arxiv_id": "2503.12556v1",
      "title": "From Guessing to Asking: An Approach to Resolving the Persona Knowledge Gap in LLMs during Multi-Turn Conversations",
      "title_zh": "从猜测到提问：一种解决LLMs在",
      "authors": [
        "Sarvesh Baskar",
        "Tanmay Tulsidas Verelakar",
        "Srinivasan Parthasarathy",
        "Manas Gaur"
      ],
      "abstract": "In multi-turn dialogues, large language models (LLM) face a critical\nchallenge of ensuring coherence while adapting to user-specific information.\nThis study introduces the persona knowledge gap, the discrepancy between a\nmodel's internal understanding and the knowledge required for coherent,\npersonalized conversations. While prior research has recognized these gaps,\ncomputational methods for their identification and resolution remain\nunderexplored. We propose Conversation Preference Elicitation and\nRecommendation (CPER), a novel framework that dynamically detects and resolves\npersona knowledge gaps using intrinsic uncertainty quantification and\nfeedback-driven refinement. CPER consists of three key modules: a Contextual\nUnderstanding Module for preference extraction, a Dynamic Feedback Module for\nmeasuring uncertainty and refining persona alignment, and a Persona-Driven\nResponse Generation module for adapting responses based on accumulated user\ncontext. We evaluate CPER on two real-world datasets: CCPE-M for preferential\nmovie recommendations and ESConv for mental health support. Using A/B testing,\nhuman evaluators preferred CPER's responses 42% more often than baseline models\nin CCPE-M and 27% more often in ESConv. A qualitative human evaluation confirms\nthat CPER's responses are preferred for maintaining contextual relevance and\ncoherence, particularly in longer (12+ turn) conversations.",
      "tldr_zh": "这篇论文探讨了大型语言模型 (LLMs) 在多轮对话中面临的 persona knowledge gap 问题，即模型内部理解与个性化对话所需知识的脱节，并提出了一种新框架 Conversation Preference Elicitation and Recommendation (CPER) 来动态检测和解决这一问题。CPER 包括三个关键模块：Contextual Understanding Module 用于提取用户偏好、Dynamic Feedback Module 用于测量不确定性和精炼 persona 匹配，以及 Persona-Driven Response Generation module 用于基于积累上下文生成适应性响应。在 CCPE-M 和 ESConv 数据集上的实验显示，CPER 的响应在人类 A/B 测试中比基线模型高出 42% 和 27% 的偏好率，并在长对话（12+ 轮）中显著提升了上下文相关性和连贯性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2; I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 1 Figure, Oral Presentation at NAACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.12556v1",
      "published_date": "2025-03-16 15:55:29 UTC",
      "updated_date": "2025-03-16 15:55:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:44:22.995222"
    },
    {
      "arxiv_id": "2503.12549v1",
      "title": "Grasping Partially Occluded Objects Using Autoencoder-Based Point Cloud Inpainting",
      "title_zh": "翻译失败",
      "authors": [
        "Alexander Koebler",
        "Ralf Gross",
        "Florian Buettner",
        "Ingo Thon"
      ],
      "abstract": "Flexible industrial production systems will play a central role in the future\nof manufacturing due to higher product individualization and customization. A\nkey component in such systems is the robotic grasping of known or unknown\nobjects in random positions. Real-world applications often come with challenges\nthat might not be considered in grasping solutions tested in simulation or lab\nsettings. Partial occlusion of the target object is the most prominent.\nExamples of occlusion can be supporting structures in the camera's field of\nview, sensor imprecision, or parts occluding each other due to the production\nprocess. In all these cases, the resulting lack of information leads to\nshortcomings in calculating grasping points. In this paper, we present an\nalgorithm to reconstruct the missing information. Our inpainting solution\nfacilitates the real-world utilization of robust object matching approaches for\ngrasping point calculation. We demonstrate the benefit of our solution by\nenabling an existing grasping system embedded in a real-world industrial\napplication to handle occlusions in the input. With our solution, we\ndrastically decrease the number of objects discarded by the process.",
      "tldr_zh": "这篇论文针对机器人抓取部分遮挡物体的挑战，提出了一种基于Autoencoder的点云修复（Point Cloud Inpainting）算法，用于重建缺失的信息。方法通过自动编码器修复点云数据，从而提升对象匹配方法的鲁棒性，使其适用于真实工业场景中的遮挡问题。实验结果表明，该解决方案显著减少了生产过程中被丢弃的物体数量，提高了灵活工业系统的抓取效率。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "Published at ECML PKDD 2022",
      "pdf_url": "http://arxiv.org/pdf/2503.12549v1",
      "published_date": "2025-03-16 15:38:08 UTC",
      "updated_date": "2025-03-16 15:38:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:44:33.640446"
    },
    {
      "arxiv_id": "2503.13551v3",
      "title": "Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Teng Wang",
        "Zhangyi Jiang",
        "Zhenqi He",
        "Shenyang Tong",
        "Wenhan Yang",
        "Yanan Zheng",
        "Zeyu Li",
        "Zifan He",
        "Hailei Gong"
      ],
      "abstract": "Recent studies show that Large Language Models (LLMs) achieve strong\nreasoning capabilities through supervised fine-tuning or reinforcement\nlearning. However, a key approach, the Process Reward Model (PRM), suffers from\nreward hacking, making it unreliable in identifying the best intermediate step.\nIn addition, the cost of annotating reasoning processes for reward modeling is\nhigh, making large-scale collection of high-quality data challenging. To\naddress this, we propose a novel reward model approach called the Hierarchical\nReward Model (HRM), which evaluates both individual and consecutive reasoning\nsteps at both fine-grained and coarse-grained levels. HRM excels at assessing\nmulti-step reasoning coherence, especially when flawed steps are later\ncorrected through self-reflection. To further reduce the cost of generating\ntraining data, we introduce a lightweight and effective data augmentation\nstrategy called Hierarchical Node Compression (HNC), which merges two\nconsecutive reasoning steps into one within the tree structure. By applying HNC\nto MCTS-generated reasoning trajectories, we enhance the diversity and\nrobustness of HRM training data while introducing controlled noise with minimal\ncomputational overhead. Empirical results on the PRM800K dataset show that HRM,\ntogether with HNC, provides more stable and reliable evaluations than PRM.\nFurthermore, cross-domain evaluations on the MATH500 and GSM8K datasets\ndemonstrate HRM's strong generalization and robustness across a variety of\nreasoning tasks.",
      "tldr_zh": "本研究针对 Large Language Models (LLMs) 的推理能力问题，提出 Hierarchical Reward Model (HRM)，该模型在细粒度和粗粒度级别评估单个和连续推理步骤，从而更好地识别多步推理的连贯性，尤其在后续步骤通过自省修正错误时。针对数据标注成本高的问题，引入 Hierarchical Node Compression (HNC) 策略，将 MCTS 生成的推理轨迹中连续步骤合并，以增强训练数据的多样性和鲁棒性，同时减少计算开销。实验结果显示，HRM 与 HNC 组合在 PRM800K 数据集上提供比 Process Reward Model (PRM) 更稳定的评估，并在 MATH500 和 GSM8K 数据集的跨域任务中展现出强的泛化性和可靠性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13551v3",
      "published_date": "2025-03-16 15:18:40 UTC",
      "updated_date": "2025-05-06 11:38:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:44:47.335236"
    },
    {
      "arxiv_id": "2503.12532v2",
      "title": "STEVE: A Step Verification Pipeline for Computer-use Agent Training",
      "title_zh": "STEVE：一种用于计算机使用代理训练的步骤",
      "authors": [
        "Fanbin Lu",
        "Zhisheng Zhong",
        "Ziqin Wei",
        "Shu Liu",
        "Chi-Wing Fu",
        "Jiaya Jia"
      ],
      "abstract": "Developing AI agents to autonomously manipulate graphical user interfaces is\na long challenging task. Recent advances in data scaling law inspire us to\ntrain computer-use agents with a scaled instruction set, yet using behavior\ncloning to train agents still requires immense high-quality trajectories. To\nmeet the scalability need, we designed STEVE, a step verification pipeline for\ncomputer-use agent training. First, we establish a large instruction set for\ncomputer-use agents and collect trajectory data with some suboptimal agents.\nGPT-4o is used to verify the correctness of each step in the trajectories based\non the screens before and after the action execution, assigning each step with\na binary label. Last, we adopt the Kahneman and Tversky Optimization to\noptimize the agent from the binary stepwise labels. Extensive experiments\nmanifest that our agent outperforms supervised finetuning by leveraging both\npositive and negative actions within a trajectory. Also, STEVE enables us to\ntrain a 7B vision-language model as a computer-use agent, achieving leading\nperformance in the challenging live desktop environment WinAgentArena with\ngreat efficiency at a reduced cost. Code and data:\nhttps://github.com/FanbinLu/STEVE.",
      "tldr_zh": "这篇论文介绍了 STEVE，一种步骤验证管道，用于训练计算机使用代理（computer-use agents），以解决行为克隆（Behavior Cloning）对高质量轨迹数据需求的挑战。STEVE 通过建立大型指令集、利用 GPT-4o 基于动作前后屏幕验证每个步骤的正确性并分配二元标签，然后应用 Kahneman and Tversky Optimization 来优化代理，从而有效利用轨迹中的正面和负面动作。实验结果显示，STEVE 使一个 7B 视觉语言模型在 WinAgentArena 的实时桌面环境中取得领先性能，同时显著降低了训练成本。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12532v2",
      "published_date": "2025-03-16 14:53:43 UTC",
      "updated_date": "2025-03-24 16:33:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:44:58.545083"
    },
    {
      "arxiv_id": "2503.16517v1",
      "title": "From G-Factor to A-Factor: Establishing a Psychometric Framework for AI Literacy",
      "title_zh": "从 G-Factor",
      "authors": [
        "Ning Li",
        "Wenming Deng",
        "Jiatan Chen"
      ],
      "abstract": "This research addresses the growing need to measure and understand AI\nliteracy in the context of generative AI technologies. Through three sequential\nstudies involving a total of 517 participants, we establish AI literacy as a\ncoherent, measurable construct with significant implications for education,\nworkforce development, and social equity. Study 1 (N=85) revealed a dominant\nlatent factor - termed the \"A-factor\" - that accounts for 44.16% of variance\nacross diverse AI interaction tasks. Study 2 (N=286) refined the measurement\ntool by examining four key dimensions of AI literacy: communication\neffectiveness, creative idea generation, content evaluation, and step-by-step\ncollaboration, resulting in an 18-item assessment battery. Study 3 (N=146)\nvalidated this instrument in a controlled laboratory setting, demonstrating its\npredictive validity for real-world task performance. Results indicate that AI\nliteracy significantly predicts performance on complex, language-based creative\ntasks but shows domain specificity in its predictive power. Additionally,\nregression analyses identified several significant predictors of AI literacy,\nincluding cognitive abilities (IQ), educational background, prior AI\nexperience, and training history. The multidimensional nature of AI literacy\nand its distinct factor structure provide evidence that effective human-AI\ncollaboration requires a combination of general and specialized abilities.\nThese findings contribute to theoretical frameworks of human-AI collaboration\nwhile offering practical guidance for developing targeted educational\ninterventions to promote equitable access to the benefits of generative AI\ntechnologies.",
      "tldr_zh": "这篇论文通过三个顺序研究（共517名参与者）建立了AI literacy的心理测量框架（psychometric framework），将AI literacy定义为一个连贯、可测量的结构，并引入主导因子A-factor，解释了44.16%的变异。研究1识别了A-factor，研究2开发了18项评估电池，涵盖communication effectiveness、creative idea generation、content evaluation和step-by-step collaboration四个维度，研究3在实验室环境中验证了其预测实际任务表现的有效性。结果显示，AI literacy显著预测复杂语言-based创造性任务的表现，但具有领域特异性，且受IQ、教育背景、AI经验和训练历史等因素影响。这些发现为人类-AI协作的理论框架提供了基础，并为开发针对教育干预以促进生成式AI技术的公平访问提供了实用指导。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "econ.GN",
        "q-fin.EC"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16517v1",
      "published_date": "2025-03-16 14:51:48 UTC",
      "updated_date": "2025-03-16 14:51:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:45:11.481163"
    },
    {
      "arxiv_id": "2503.12525v1",
      "title": "HyConEx: Hypernetwork classifier with counterfactual explanations",
      "title_zh": "翻译失败",
      "authors": [
        "Patryk Marszałek",
        "Ulvi Movsum-zada",
        "Oleksii Furman",
        "Kamil Książek",
        "Przemysław Spurek",
        "Marek Śmieja"
      ],
      "abstract": "In recent years, there has been a growing interest in explainable AI methods.\nWe want not only to make accurate predictions using sophisticated neural\nnetworks but also to understand what the model's decision is based on. One of\nthe fundamental levels of interpretability is to provide counterfactual\nexamples explaining the rationale behind the decision and identifying which\nfeatures, and to what extent, must be modified to alter the model's outcome. To\naddress these requirements, we introduce HyConEx, a classification model based\non deep hypernetworks specifically designed for tabular data. Owing to its\nunique architecture, HyConEx not only provides class predictions but also\ndelivers local interpretations for individual data samples in the form of\ncounterfactual examples that steer a given sample toward an alternative class.\nWhile many explainable methods generated counterfactuals for external models,\nthere have been no interpretable classifiers simultaneously producing\ncounterfactual samples so far. HyConEx achieves competitive performance on\nseveral metrics assessing classification accuracy and fulfilling the criteria\nof a proper counterfactual attack. This makes HyConEx a distinctive deep\nlearning model, which combines predictions and explainers as an all-in-one\nneural network. The code is available at https://github.com/gmum/HyConEx.",
      "tldr_zh": "该研究引入了 HyConEx，一种基于 deep hypernetworks 的分类模型，专门针对表格数据，能够提供准确的类预测并生成反事实解释（counterfactual explanations），以阐明模型决策的依据和需要修改的特征。HyConEx 的独特架构首次将预测和解释器整合为一个统一的神经网络，实现同时输出反事实样本，这填补了现有可解释 AI 方法的空白。在多个指标上，HyConEx 展现出竞争性的分类准确性和反事实生成质量，代码已在 GitHub 上开源。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12525v1",
      "published_date": "2025-03-16 14:39:36 UTC",
      "updated_date": "2025-03-16 14:39:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:45:21.292193"
    },
    {
      "arxiv_id": "2503.12524v2",
      "title": "EXAONE Deep: Reasoning Enhanced Language Models",
      "title_zh": "EXAONE Deep：推理增强语言模型",
      "authors": [
        "LG AI Research",
        "Kyunghoon Bae",
        "Eunbi Choi",
        "Kibong Choi",
        "Stanley Jungkyu Choi",
        "Yemuk Choi",
        "Seokhee Hong",
        "Junwon Hwang",
        "Hyojin Jeon",
        "Kijeong Jeon",
        "Gerrard Jeongwon Jo",
        "Hyunjik Jo",
        "Jiyeon Jung",
        "Hyosang Kim",
        "Joonkee Kim",
        "Seonghwan Kim",
        "Soyeon Kim",
        "Sunkyoung Kim",
        "Yireun Kim",
        "Yongil Kim",
        "Youchul Kim",
        "Edward Hwayoung Lee",
        "Haeju Lee",
        "Honglak Lee",
        "Jinsik Lee",
        "Kyungmin Lee",
        "Sangha Park",
        "Yongmin Park",
        "Sihoon Yang",
        "Heuiyeen Yeen",
        "Sihyuk Yi",
        "Hyeongu Yun"
      ],
      "abstract": "We present EXAONE Deep series, which exhibits superior capabilities in\nvarious reasoning tasks, including math and coding benchmarks. We train our\nmodels mainly on the reasoning-specialized dataset that incorporates long\nstreams of thought processes. Evaluation results show that our smaller models,\nEXAONE Deep 2.4B and 7.8B, outperform other models of comparable size, while\nthe largest model, EXAONE Deep 32B, demonstrates competitive performance\nagainst leading open-weight models. All EXAONE Deep models are openly available\nfor research purposes and can be downloaded from\nhttps://huggingface.co/LGAI-EXAONE",
      "tldr_zh": "本研究介绍了 EXAONE Deep 系列语言模型，这些模型通过增强推理能力，在数学和编码等推理任务上表现出色。模型主要在包含长链思考过程的专用数据集上进行训练，显著提升了性能。评估结果显示，较小模型如 EXAONE Deep 2.4B 和 7.8B 超过了同等规模的竞争者，而 EXAONE Deep 32B 与领先的开源模型不相上下；所有模型均可公开下载，用于研究目的。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2412.04862,\n  arXiv:2408.03541",
      "pdf_url": "http://arxiv.org/pdf/2503.12524v2",
      "published_date": "2025-03-16 14:39:33 UTC",
      "updated_date": "2025-03-19 07:09:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:45:33.697997"
    },
    {
      "arxiv_id": "2503.13550v1",
      "title": "Towards Privacy-Preserving Data-Driven Education: The Potential of Federated Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammad Khalil",
        "Ronas Shakya",
        "Qinyi Liu"
      ],
      "abstract": "The increasing adoption of data-driven applications in education such as in\nlearning analytics and AI in education has raised significant privacy and data\nprotection concerns. While these challenges have been widely discussed in\nprevious works, there are still limited practical solutions. Federated learning\nhas recently been discoursed as a promising privacy-preserving technique, yet\nits application in education remains scarce. This paper presents an\nexperimental evaluation of federated learning for educational data prediction,\ncomparing its performance to traditional non-federated approaches. Our findings\nindicate that federated learning achieves comparable predictive accuracy.\nFurthermore, under adversarial attacks, federated learning demonstrates greater\nresilience compared to non-federated settings. We summarise that our results\nreinforce the value of federated learning as a potential approach for balancing\npredictive performance and privacy in educational contexts.",
      "tldr_zh": "本研究探讨了数据驱动教育（如学习分析和AI in education）中存在的隐私和数据保护问题，强调Federated Learning作为一种潜在的隐私保护技术，但其在教育领域的应用仍较少。论文通过实验评估Federated Learning在教育数据预测中的性能，与传统非Federated方法进行比较，结果显示其预测准确率相当。进一步，在对抗性攻击下，Federated Learning表现出更高的弹性，强化了其在平衡预测性能和隐私方面的价值，为隐私保护的教育应用提供了实用路径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13550v1",
      "published_date": "2025-03-16 14:37:32 UTC",
      "updated_date": "2025-03-16 14:37:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:45:45.315515"
    },
    {
      "arxiv_id": "2503.13549v1",
      "title": "A Showdown of ChatGPT vs DeepSeek in Solving Programming Tasks",
      "title_zh": "ChatGPT 与 DeepSeek 在解决编程任务中的对决",
      "authors": [
        "Ronas Shakya",
        "Farhad Vadiee",
        "Mohammad Khalil"
      ],
      "abstract": "The advancement of large language models (LLMs) has created a competitive\nlandscape for AI-assisted programming tools. This study evaluates two leading\nmodels: ChatGPT 03-mini and DeepSeek-R1 on their ability to solve competitive\nprogramming tasks from Codeforces. Using 29 programming tasks of three levels\nof easy, medium, and hard difficulty, we assessed the outcome of both models by\ntheir accepted solutions, memory efficiency, and runtime performance. Our\nresults indicate that while both models perform similarly on easy tasks,\nChatGPT outperforms DeepSeek-R1 on medium-difficulty tasks, achieving a 54.5%\nsuccess rate compared to DeepSeek 18.1%. Both models struggled with hard tasks,\nthus highlighting some ongoing challenges LLMs face in handling highly complex\nprogramming problems. These findings highlight key differences in both model\ncapabilities and their computational power, offering valuable insights for\ndevelopers and researchers working to advance AI-driven programming tools.",
      "tldr_zh": "这篇论文比较了 ChatGPT 03-mini 和 DeepSeek-R1 在解决 Codeforces 编程任务中的性能，使用了 29 个任务（涵盖简单、中等和困难级别），并评估了它们的接受解决方案、内存效率和运行时性能。结果显示，两者在简单任务上表现相似，但在中等难度任务上，ChatGPT 的成功率达到 54.5%，明显优于 DeepSeek-R1 的 18.1%。两者在困难任务上均挣扎，这突出了 LLMs 在处理高度复杂编程问题时的局限性，并为开发 AI 驱动编程工具提供了关键见解。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13549v1",
      "published_date": "2025-03-16 14:35:36 UTC",
      "updated_date": "2025-03-16 14:35:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:45:59.026004"
    },
    {
      "arxiv_id": "2503.12511v2",
      "title": "LLM-Driven Multi-step Translation from C to Rust using Static Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Tianyang Zhou",
        "Haowen Lin",
        "Somesh Jha",
        "Mihai Christodorescu",
        "Kirill Levchenko",
        "Varun Chandrasekaran"
      ],
      "abstract": "Translating software written in legacy languages to modern languages, such as\nC to Rust, has significant benefits in improving memory safety while\nmaintaining high performance. However, manual translation is cumbersome,\nerror-prone, and produces unidiomatic code. Large language models (LLMs) have\ndemonstrated promise in producing idiomatic translations, but offer no\ncorrectness guarantees as they lack the ability to capture all the semantics\ndifferences between the source and target languages. To resolve this issue, we\npropose SACTOR, an LLM-driven C-to-Rust zero-shot translation tool using a\ntwo-step translation methodology: an \"unidiomatic\" step to translate C into\nRust while preserving semantics, and an \"idiomatic\" step to refine the code to\nfollow Rust's semantic standards. SACTOR utilizes information provided by\nstatic analysis of the source C program to address challenges such as pointer\nsemantics and dependency resolution. To validate the correctness of the\ntranslated result from each step, we use end-to-end testing via the foreign\nfunction interface to embed our translated code segment into the original code.\nWe evaluate the translation of 200 programs from two datasets and two case\nstudies, comparing the performance of GPT-4o, Claude 3.5 Sonnet, Gemini 2.0\nFlash, Llama 3.3 70B and DeepSeek-R1 in SACTOR. Our results demonstrate that\nSACTOR achieves high correctness and improved idiomaticity, with the\nbest-performing model (DeepSeek-R1) reaching 93% and (GPT-4o, Claude 3.5,\nDeepSeek-R1) reaching 84% correctness (on each dataset, respectively), while\nproducing more natural and Rust-compliant translations compared to existing\nmethods.",
      "tldr_zh": "该研究提出 SACTOR，一种基于 LLM 的多步零样本翻译工具，用于将 C 代码翻译成 Rust，以提升内存安全同时保持高性能。SACTOR 采用两步方法：首先进行“unidiomatic”翻译以保留语义，然后通过“idiomatic”步骤精炼代码，使其符合 Rust 的语义标准，并利用 static analysis 处理指针语义和依赖解析等挑战。论文通过端到端测试和 foreign function interface 验证翻译正确性，在 200 个程序的评估中，DeepSeek-R1 模型达到 93% 的正确性，而 GPT-4o 和 Claude 3.5 等模型也达 84%，整体比现有方法更地道和可靠。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "22 pages, 13 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.12511v2",
      "published_date": "2025-03-16 14:05:26 UTC",
      "updated_date": "2025-03-18 04:17:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:46:11.563407"
    },
    {
      "arxiv_id": "2503.12509v1",
      "title": "A Reservoir-based Model for Human-like Perception of Complex Rhythm Pattern",
      "title_zh": "翻译失败",
      "authors": [
        "Zhongju Yuan",
        "Geraint Wiggins",
        "Dick Botteldooren"
      ],
      "abstract": "Rhythm is a fundamental aspect of human behaviour, present from infancy and\ndeeply embedded in cultural practices. Rhythm anticipation is a spontaneous\ncognitive process that typically occurs before the onset of actual beats. While\nmost research in both neuroscience and artificial intelligence has focused on\nmetronome-based rhythm tasks, studies investigating the perception of complex\nmusical rhythm patterns remain limited. To address this gap, we propose a\nhierarchical oscillator-based model to better understand the perception of\ncomplex musical rhythms in biological systems. The model consists of two types\nof coupled neurons that generate oscillations, with different layers tuned to\nrespond to distinct perception levels. We evaluate the model using several\nrepresentative rhythm patterns spanning the upper, middle, and lower bounds of\nhuman musical perception. Our findings demonstrate that, while maintaining a\nhigh degree of synchronization accuracy, the model exhibits human-like rhythmic\nbehaviours. Additionally, the beta band neuronal activity in the model mirrors\npatterns observed in the human brain, further validating the biological\nplausibility of the approach.",
      "tldr_zh": "本研究针对复杂音乐节奏感知的有限研究，提出一个基于 reservoir 的层次振荡器模型，以模拟人类对节奏的认知过程。该模型由两种耦合神经元组成，不同层级针对不同感知水平生成振荡，并通过节奏预期机制实现自发认知。实验评估了多种代表性节奏模式，涵盖人类音乐感知的上限、中间和下限，结果显示模型在保持高同步准确性的同时，展现出类似人类的节奏行为，且其 beta band neuronal activity 与人类大脑模式高度一致，验证了该方法的生物合理性。",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12509v1",
      "published_date": "2025-03-16 14:02:42 UTC",
      "updated_date": "2025-03-16 14:02:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:46:24.574839"
    },
    {
      "arxiv_id": "2503.13548v1",
      "title": "Fuzzy Rule-based Differentiable Representation Learning",
      "title_zh": "基于模糊规则的可微表示学习",
      "authors": [
        "Wei Zhang",
        "Zhaohong Deng",
        "Guanjin Wang",
        "Kup-Sze Choi"
      ],
      "abstract": "Representation learning has emerged as a crucial focus in machine and deep\nlearning, involving the extraction of meaningful and useful features and\npatterns from the input data, thereby enhancing the performance of various\ndownstream tasks such as classification, clustering, and prediction. Current\nmainstream representation learning methods primarily rely on non-linear data\nmining techniques such as kernel methods and deep neural networks to extract\nabstract knowledge from complex datasets. However, most of these methods are\nblack-box, lacking transparency and interpretability in the learning process,\nwhich constrains their practical utility. To this end, this paper introduces a\nnovel representation learning method grounded in an interpretable fuzzy\nrule-based model. Specifically, it is built upon the Takagi-Sugeno-Kang fuzzy\nsystem (TSK-FS) to initially map input data to a high-dimensional fuzzy feature\nspace through the antecedent part of the TSK-FS. Subsequently, a novel\ndifferentiable optimization method is proposed for the consequence part\nlearning which can preserve the model's interpretability and transparency while\nfurther exploring the nonlinear relationships within the data. This\noptimization method retains the essence of traditional optimization, with\ncertain parts of the process parameterized corresponding differentiable modules\nconstructed, and a deep optimization process implemented. Consequently, this\nmethod not only enhances the model's performance but also ensures its\ninterpretability. Moreover, a second-order geometry preservation method is\nintroduced to further improve the robustness of the proposed method. Extensive\nexperiments conducted on various benchmark datasets validate the superiority of\nthe proposed method, highlighting its potential for advancing representation\nlearning methodologies.",
      "tldr_zh": "这篇论文提出了一种基于模糊规则的可解释表示学习方法，以解决现有非线性方法（如核方法和深度神经网络）的黑盒问题。具体地，它利用 Takagi-Sugeno-Kang fuzzy system (TSK-FS) 将输入数据映射到高维模糊特征空间，并引入一个可微优化方法来学习后果部分，从而保持模型的透明性和可解释性，同时探索数据中的非线性关系。论文还添加了二阶几何保留方法来提升方法的鲁棒性。在多个基准数据集上的实验验证了该方法的优越性，展示了其在 representation learning 领域的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13548v1",
      "published_date": "2025-03-16 14:00:34 UTC",
      "updated_date": "2025-03-16 14:00:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:46:34.584173"
    },
    {
      "arxiv_id": "2503.12506v1",
      "title": "A General Close-loop Predictive Coding Framework for Auditory Working Memory",
      "title_zh": "翻译失败",
      "authors": [
        "Zhongju Yuan",
        "Geraint Wiggins",
        "Dick Botteldooren"
      ],
      "abstract": "Auditory working memory is essential for various daily activities, such as\nlanguage acquisition, conversation. It involves the temporary storage and\nmanipulation of information that is no longer present in the environment. While\nextensively studied in neuroscience and cognitive science, research on its\nmodeling within neural networks remains limited. To address this gap, we\npropose a general framework based on a close-loop predictive coding paradigm to\nperform short auditory signal memory tasks. The framework is evaluated on two\nwidely used benchmark datasets for environmental sound and speech,\ndemonstrating high semantic similarity across both datasets.",
      "tldr_zh": "该论文探讨了听觉工作记忆（auditory working memory）在日常活动（如语言习得和对话）中的重要性，但强调了神经网络中对其建模研究的不足。为填补这一空白，研究提出了一种基于闭环预测编码（close-loop predictive coding）范式的通用框架，用于处理短音频信号记忆任务。该框架在环境声音和语音的两个基准数据集上进行了评估，展示了高语义相似性，证明了其有效性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12506v1",
      "published_date": "2025-03-16 13:57:37 UTC",
      "updated_date": "2025-03-16 13:57:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:46:45.322197"
    },
    {
      "arxiv_id": "2503.12505v1",
      "title": "MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process Errors Identification",
      "title_zh": "MPBench：用于过程错误识别的全面多模态推理基准",
      "authors": [
        "Zhaopan Xu",
        "Pengfei Zhou",
        "Jiaxin Ai",
        "Wangbo Zhao",
        "Kai Wang",
        "Xiaojiang Peng",
        "Wenqi Shao",
        "Hongxun Yao",
        "Kaipeng Zhang"
      ],
      "abstract": "Reasoning is an essential capacity for large language models (LLMs) to\naddress complex tasks, where the identification of process errors is vital for\nimproving this ability. Recently, process-level reward models (PRMs) were\nproposed to provide step-wise rewards that facilitate reinforcement learning\nand data production during training and guide LLMs toward correct steps during\ninference, thereby improving reasoning accuracy. However, existing benchmarks\nof PRMs are text-based and focus on error detection, neglecting other scenarios\nlike reasoning search. To address this gap, we introduce MPBench, a\ncomprehensive, multi-task, multimodal benchmark designed to systematically\nassess the effectiveness of PRMs in diverse scenarios. MPBench employs three\nevaluation paradigms, each targeting a specific role of PRMs in the reasoning\nprocess: (1) Step Correctness, which assesses the correctness of each\nintermediate reasoning step; (2) Answer Aggregation, which aggregates multiple\nsolutions and selects the best one; and (3) Reasoning Process Search, which\nguides the search for optimal reasoning steps during inference. Through these\nparadigms, MPBench makes comprehensive evaluations and provides insights into\nthe development of multimodal PRMs.",
      "tldr_zh": "本论文引入了 MPBench，这是一个全面的多任务多模态基准，旨在评估过程级奖励模型 (PRMs) 在识别过程错误方面的有效性，以弥补现有文本-based 基准的局限性，如忽略推理搜索场景。MPBench 通过三个评估范式进行系统评估：(1) Step Correctness，检查每个中间推理步骤的正确性；(2) Answer Aggregation，聚合多个解决方案并选择最佳的；以及 (3) Reasoning Process Search，指导推理过程中的最佳步骤搜索。这些范式帮助提升大型语言模型 (LLMs) 的推理准确性，并为多模态 PRMs 的发展提供宝贵洞见。",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12505v1",
      "published_date": "2025-03-16 13:50:38 UTC",
      "updated_date": "2025-03-16 13:50:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:46:58.977028"
    },
    {
      "arxiv_id": "2503.12499v1",
      "title": "Facilitating Automated Online Consensus Building through Parallel Thinking",
      "title_zh": "翻译失败",
      "authors": [
        "Wen Gu",
        "Zhaoxing Li",
        "Jan Buermann",
        "Jim Dilkes",
        "Dimitris Michailidis",
        "Shinobu Hasegawa",
        "Vahid Yazdanpanah",
        "Sebastian Stein"
      ],
      "abstract": "Consensus building is inherently challenging due to the diverse opinions held\nby stakeholders. Effective facilitation is crucial to support the consensus\nbuilding process and enable efficient group decision making. However, the\neffectiveness of facilitation is often constrained by human factors such as\nlimited experience and scalability. In this research, we propose a Parallel\nThinking-based Facilitation Agent (PTFA) that facilitates online, text-based\nconsensus building processes. The PTFA automatically collects textual posts and\nleverages large language models (LLMs) to perform all of the six distinct roles\nof the well-established Six Thinking Hats technique in parallel thinking. To\nillustrate the potential of PTFA, a pilot study was carried out and PTFA's\nability in idea generation, emotional probing, and deeper analysis of ideas was\ndemonstrated. Furthermore, a comprehensive dataset that contains not only the\nconversational content among the participants but also between the participants\nand the agent is constructed for future study.",
      "tldr_zh": "该研究针对在线共识构建的挑战（如利益相关者意见多样和人类促进的局限性），提出了一种基于 Parallel Thinking 的自动促进代理 PTFA。PTFA 利用大型语言模型 (LLMs) 来并行执行 Six Thinking Hats 技术的六个角色，包括收集文本帖子并进行想法生成、情感探测和深度分析。试点研究证明了 PTFA 的有效性，并构建了一个全面数据集，包含参与者间对话和参与者与代理间的互动，以支持未来研究。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12499v1",
      "published_date": "2025-03-16 13:32:35 UTC",
      "updated_date": "2025-03-16 13:32:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:47:10.308282"
    },
    {
      "arxiv_id": "2503.12497v1",
      "title": "Defense Against Model Stealing Based on Account-Aware Distribution Discrepancy",
      "title_zh": "翻译失败",
      "authors": [
        "Jian-Ping Mei",
        "Weibin Zhang",
        "Jie Chen",
        "Xuyun Zhang",
        "Tiantian Zhu"
      ],
      "abstract": "Malicious users attempt to replicate commercial models functionally at low\ncost by training a clone model with query responses. It is challenging to\ntimely prevent such model-stealing attacks to achieve strong protection and\nmaintain utility. In this paper, we propose a novel non-parametric detector\ncalled Account-aware Distribution Discrepancy (ADD) to recognize queries from\nmalicious users by leveraging account-wise local dependency. We formulate each\nclass as a Multivariate Normal distribution (MVN) in the feature space and\nmeasure the malicious score as the sum of weighted class-wise distribution\ndiscrepancy. The ADD detector is combined with random-based prediction\npoisoning to yield a plug-and-play defense module named D-ADD for image\nclassification models. Results of extensive experimental studies show that\nD-ADD achieves strong defense against different types of attacks with little\ninterference in serving benign users for both soft and hard-label settings.",
      "tldr_zh": "该论文针对模型窃取攻击提出了一种基于 Account-aware Distribution Discrepancy (ADD) 的防御方法，该检测器利用账户级的局部依赖性，通过将每个类别表述为特征空间中的 Multivariate Normal distribution (MVN) 来计算恶意查询的分数。ADD 与随机预测毒化相结合，形成一个即插即用的防御模块 D-ADD，用于图像分类模型，以有效识别恶意用户查询。实验结果显示，D-ADD 在软标签和硬标签设置下，对多种攻击类型提供强有力防御，同时对良性用户的服务影响最小。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "11 pages, 7 figures, published in AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.12497v1",
      "published_date": "2025-03-16 13:22:53 UTC",
      "updated_date": "2025-03-16 13:22:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:47:21.857271"
    },
    {
      "arxiv_id": "2503.13546v1",
      "title": "CNCast: Leveraging 3D Swin Transformer and DiT for Enhanced Regional Weather Forecasting",
      "title_zh": "CNC",
      "authors": [
        "Hongli Liang",
        "Yuanting Zhang",
        "Qingye Meng",
        "Shuangshuang He",
        "Xingyuan Yuan"
      ],
      "abstract": "This study introduces a cutting-edge regional weather forecasting model based\non the SwinTransformer 3D architecture. This model is specifically designed to\ndeliver precise hourly weather predictions ranging from 1 hour to 5 days,\nsignificantly improving the reliability and practicality of short-term weather\nforecasts. Our model has demonstrated generally superior performance when\ncompared to Pangu, a well-established global model. The evaluation indicates\nthat our model excels in predicting most weather variables, highlighting its\npotential as a more effective alternative in the field of limited area\nmodeling. A noteworthy feature of this model is the integration of enhanced\nboundary conditions, inspired by traditional numerical weather prediction (NWP)\ntechniques. This integration has substantially improved the model's predictive\naccuracy. Additionally, the model includes an innovative approach for\ndiagnosing hourly total precipitation at a high spatial resolution of\napproximately 5 kilometers. This is achieved through a latent diffusion model,\noffering an alternative method for generating high-resolution precipitation\ndata.",
      "tldr_zh": "本研究引入了CNCast模型，利用3D Swin Transformer和DiT架构，针对区域天气预报提供1小时至5天的精确小时级预测，显著提升了短期天气预报的可靠性和实用性。该模型在与全球模型Pangu的比较中表现出色，尤其在预测大多数天气变量方面表现优越，并通过整合受NWP启发的增强边界条件进一步提高了预测准确性。此外，CNCast采用潜在扩散模型创新性地诊断高空间分辨率（约5公里）的小时总降水，为区域天气建模提供了更有效的替代方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13546v1",
      "published_date": "2025-03-16 12:52:48 UTC",
      "updated_date": "2025-03-16 12:52:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:47:33.695479"
    },
    {
      "arxiv_id": "2503.12490v1",
      "title": "GeoRSMLLM: A Multimodal Large Language Model for Vision-Language Tasks in Geoscience and Remote Sensing",
      "title_zh": "翻译失败",
      "authors": [
        "Zilun Zhang",
        "Haozhan Shen",
        "Tiancheng Zhao",
        "Bin Chen",
        "Zian Guan",
        "Yuhao Wang",
        "Xu Jia",
        "Yuxiang Cai",
        "Yongheng Shang",
        "Jianwei Yin"
      ],
      "abstract": "The application of Vision-Language Models (VLMs) in remote sensing (RS) has\ndemonstrated significant potential in traditional tasks such as scene\nclassification, object detection, and image captioning. However, current\nmodels, which excel in Referring Expression Comprehension (REC), struggle with\ntasks involving complex instructions (e.g., exists multiple conditions) or\npixel-level operations like segmentation and change detection. In this white\npaper, we provide a comprehensive hierarchical summary of vision-language tasks\nin RS, categorized by the varying levels of cognitive capability required. We\nintroduce the Remote Sensing Vision-Language Task Set (RSVLTS), which includes\nOpen-Vocabulary Tasks (OVT), Referring Expression Tasks (RET), and Described\nObject Tasks (DOT) with increased difficulty, and Visual Question Answering\n(VQA) aloneside. Moreover, we propose a novel unified data representation using\na set-of-points approach for RSVLTS, along with a condition parser and a\nself-augmentation strategy based on cyclic referring. These features are\nintegrated into the GeoRSMLLM model, and this enhanced model is designed to\nhandle a broad range of tasks of RSVLTS, paving the way for a more generalized\nsolution for vision-language tasks in geoscience and remote sensing.",
      "tldr_zh": "本文探讨了视觉语言模型(VLMs)在遥感(RS)领域的应用及其局限性，如在Referring Expression Comprehension (REC)上表现良好，但难以处理复杂指令或像素级操作（如分割和变化检测）。论文提供了一个层次化总结，并引入了Remote Sensing Vision-Language Task Set (RSVLTS)，包括Open-Vocabulary Tasks (OVT)、Referring Expression Tasks (RET)、Described Object Tasks (DOT)和Visual Question Answering (VQA)，以分类不同认知水平的任务。作者提出了一种统一的set-of-points数据表示、condition parser和基于cyclic referring的自增强策略，并将其整合到GeoRSMLLM模型中，实现对RSVLTS广泛任务的泛化处理。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12490v1",
      "published_date": "2025-03-16 12:48:17 UTC",
      "updated_date": "2025-03-16 12:48:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:47:47.481394"
    },
    {
      "arxiv_id": "2503.12484v1",
      "title": "SING: Semantic Image Communications using Null-Space and INN-Guided Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Jiakang Chen",
        "Selim F. Yilmaz",
        "Di You",
        "Pier Luigi Dragotti",
        "Deniz Gündüz"
      ],
      "abstract": "Joint source-channel coding systems based on deep neural networks (DeepJSCC)\nhave recently demonstrated remarkable performance in wireless image\ntransmission. Existing methods primarily focus on minimizing distortion between\nthe transmitted image and the reconstructed version at the receiver, often\noverlooking perceptual quality. This can lead to severe perceptual degradation\nwhen transmitting images under extreme conditions, such as low bandwidth\ncompression ratios (BCRs) and low signal-to-noise ratios (SNRs). In this work,\nwe propose SING, a novel two-stage JSCC framework that formulates the recovery\nof high-quality source images from corrupted reconstructions as an inverse\nproblem. Depending on the availability of information about the DeepJSCC\nencoder/decoder and the channel at the receiver, SING can either approximate\nthe stochastic degradation as a linear transformation, or leverage invertible\nneural networks (INNs) for precise modeling. Both approaches enable the\nseamless integration of diffusion models into the reconstruction process,\nenhancing perceptual quality. Experimental results demonstrate that SING\noutperforms DeepJSCC and other approaches, delivering superior perceptual\nquality even under extremely challenging conditions, including scenarios with\nsignificant distribution mismatches between the training and test data.",
      "tldr_zh": "本文提出 SING，一种新型两阶段联合源-通道编码 (JSCC) 框架，针对基于深度神经网络 (DeepJSCC) 的无线图像传输问题，通过将图像恢复表述为逆问题来提升感知质量。SING 根据接收端的信息可用性，使用线性变换或可逆神经网络 (INNs) 来建模随机退化，并无缝集成扩散模型以改善重建过程，尤其在低带宽压缩比 (BCRs) 和低信噪比 (SNRs) 的极端条件下。实验结果显示，SING 优于 DeepJSCC 和其他方法，提供更佳的感知质量，即使在训练和测试数据分布不匹配时也表现出色。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.IT",
        "cs.LG",
        "eess.SP",
        "math.IT"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12484v1",
      "published_date": "2025-03-16 12:32:11 UTC",
      "updated_date": "2025-03-16 12:32:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:47:59.244015"
    },
    {
      "arxiv_id": "2503.12478v2",
      "title": "KDSelector: A Knowledge-Enhanced and Data-Efficient Model Selector Learning Framework for Time Series Anomaly Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiyu Liang",
        "Dongrui Cai",
        "Chenyuan Zhang",
        "Zheng Liang",
        "Chen Liang",
        "Bo Zheng",
        "Shi Qiu",
        "Jin Wang",
        "Hongzhi Wang"
      ],
      "abstract": "Model selection has been raised as an essential problem in the area of time\nseries anomaly detection (TSAD), because there is no single best TSAD model for\nthe highly heterogeneous time series in real-world applications. However,\ndespite the success of existing model selection solutions that train a\nclassification model (especially neural network, NN) using historical data as a\nselector to predict the correct TSAD model for each series, the NN-based\nselector learning methods used by existing solutions do not make full use of\nthe knowledge in the historical data and require iterating over all training\nsamples, which limits the accuracy and training speed of the selector. To\naddress these limitations, we propose KDSelector, a novel knowledge-enhanced\nand data-efficient framework for learning the NN-based TSAD model selector, of\nwhich three key components are specifically designed to integrate available\nknowledge into the selector and dynamically prune less important and redundant\nsamples during the learning. We develop a TSAD model selection system with\nKDSelector as the internal, to demonstrate how users improve the accuracy and\ntraining speed of their selectors by using KDSelector as a plug-and-play\nmodule. Our demonstration video is hosted at https://youtu.be/2uqupDWvTF0.",
      "tldr_zh": "该研究针对时间序列异常检测（TSAD）中的模型选择问题提出 KDSelector 框架，该框架是一种知识增强和数据高效的学习方法，用于训练神经网络（NN）-based 选择器，以预测适合每个时间序列的最佳 TSAD 模型。KDSelector 通过三个关键组件整合历史数据中的可用知识，并动态修剪不重要和冗余样本，从而提升选择器的准确性和训练速度。实验结果显示，该框架可作为可插拔模块，帮助用户优化 TSAD 模型选择系统，并提供演示视频以展示其实际应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper has been accepted by SIGMOD 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.12478v2",
      "published_date": "2025-03-16 12:13:19 UTC",
      "updated_date": "2025-03-20 03:06:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:48:10.433789"
    },
    {
      "arxiv_id": "2504.11460v2",
      "title": "Semantic Matters: Multimodal Features for Affective Analysis",
      "title_zh": "语义至关重要：多模态特征用于情感分析",
      "authors": [
        "Tobias Hallmen",
        "Robin-Nico Kampa",
        "Fabian Deuser",
        "Norbert Oswald",
        "Elisabeth André"
      ],
      "abstract": "In this study, we present our methodology for two tasks: the Emotional\nMimicry Intensity (EMI) Estimation Challenge and the Behavioural\nAmbivalence/Hesitancy (BAH) Recognition Challenge, both conducted as part of\nthe 8th Workshop and Competition on Affective & Behavior Analysis in-the-wild.\nWe utilize a Wav2Vec 2.0 model pre-trained on a large podcast dataset to\nextract various audio features, capturing both linguistic and paralinguistic\ninformation. Our approach incorporates a valence-arousal-dominance (VAD) module\nderived from Wav2Vec 2.0, a BERT text encoder, and a vision transformer (ViT)\nwith predictions subsequently processed through a long short-term memory (LSTM)\narchitecture or a convolution-like method for temporal modeling. We integrate\nthe textual and visual modality into our analysis, recognizing that semantic\ncontent provides valuable contextual cues and underscoring that the meaning of\nspeech often conveys more critical insights than its acoustic counterpart\nalone. Fusing in the vision modality helps in some cases to interpret the\ntextual modality more precisely. This combined approach results in significant\nperformance improvements, achieving in EMI $\\rho_{\\text{TEST}} = 0.706$ and in\nBAH $F1_{\\text{TEST}} = 0.702$, securing first place in the EMI challenge and\nsecond place in the BAH challenge.",
      "tldr_zh": "本研究针对情感模仿强度（EMI）估算和行为矛盾/犹豫（BAH）识别两个挑战，提出了一种多模态分析方法，使用 Wav2Vec 2.0 模型提取音频特征（包括语言和副语言信息）、valence-arousal-dominance (VAD) 模块、BERT 文本编码器以及 Vision Transformer (ViT) 来处理视觉和文本模态。方法通过 LSTM 或卷积-like 技术进行时间序列建模，并强调语义内容的价值，表明整合视觉模态能更精确地解释文本信息，从而提升整体性能。在挑战赛中，该方法在 EMI 上达到 ρ_TEST = 0.706（获第一名），在 BAH 上达到 F1_TEST = 0.702（获第二名），证明了多模态融合的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.11460v2",
      "published_date": "2025-03-16 11:30:44 UTC",
      "updated_date": "2025-04-18 06:46:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:48:22.106015"
    },
    {
      "arxiv_id": "2503.12451v1",
      "title": "ISLR101: an Iranian Word-Level Sign Language Recognition Dataset",
      "title_zh": "ISLR101：伊朗词级别手语识别数据集",
      "authors": [
        "Hossein Ranjbar",
        "Alireza Taheri"
      ],
      "abstract": "Sign language recognition involves modeling complex multichannel information,\nsuch as hand shapes and movements while relying on sufficient sign\nlanguage-specific data. However, sign languages are often under-resourced,\nposing a significant challenge for research and development in this field. To\naddress this gap, we introduce ISLR101, the first publicly available Iranian\nSign Language dataset for isolated sign language recognition. This\ncomprehensive dataset includes 4,614 videos covering 101 distinct signs,\nrecorded by 10 different signers (3 deaf individuals, 2 sign language\ninterpreters, and 5 L2 learners) against varied backgrounds, with a resolution\nof 800x600 pixels and a frame rate of 25 frames per second. It also includes\nskeleton pose information extracted using OpenPose. We establish both a visual\nappearance-based and a skeleton-based framework as baseline models, thoroughly\ntraining and evaluating them on ISLR101. These models achieve 97.01% and 94.02%\naccuracy on the test set, respectively. Additionally, we publish the train,\nvalidation, and test splits to facilitate fair comparisons.",
      "tldr_zh": "这篇论文引入了 ISLR101，这是第一个公开的伊朗手语（Iranian Sign Language）数据集，用于孤立的手语识别（isolated sign language recognition），旨在解决手语数据资源不足的问题。该数据集包含 4,614 个视频，覆盖 101 个不同手势，由 10 名不同手语使用者（包括 3 名聋人、2 名手语翻译者和 5 名 L2 学习者）在多种背景中录制，分辨率为 800x600 像素、帧率为 25 fps，并附带使用 OpenPose 提取的骨骼姿势信息。作者建立了基于视觉外观和骨骼框架的基线模型，在测试集上分别实现 97.01% 和 94.02% 的准确率，并发布了训练、验证和测试集划分，以促进公平的后续研究。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12451v1",
      "published_date": "2025-03-16 10:57:01 UTC",
      "updated_date": "2025-03-16 10:57:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:48:35.530631"
    },
    {
      "arxiv_id": "2503.16516v1",
      "title": "Using LLMs for Automated Privacy Policy Analysis: Prompt Engineering, Fine-Tuning and Explainability",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxin Chen",
        "Peng Tang",
        "Weidong Qiu",
        "Shujun Li"
      ],
      "abstract": "Privacy policies are widely used by digital services and often required for\nlegal purposes. Many machine learning based classifiers have been developed to\nautomate detection of different concepts in a given privacy policy, which can\nhelp facilitate other automated tasks such as producing a more reader-friendly\nsummary and detecting legal compliance issues. Despite the successful\napplications of large language models (LLMs) to many NLP tasks in various\ndomains, there is very little work studying the use of LLMs for automated\nprivacy policy analysis, therefore, if and how LLMs can help automate privacy\npolicy analysis remains under-explored. To fill this research gap, we conducted\na comprehensive evaluation of LLM-based privacy policy concept classifiers,\nemploying both prompt engineering and LoRA (low-rank adaptation) fine-tuning,\non four state-of-the-art (SOTA) privacy policy corpora and taxonomies. Our\nexperimental results demonstrated that combining prompt engineering and\nfine-tuning can make LLM-based classifiers outperform other SOTA methods,\n\\emph{significantly} and \\emph{consistently} across privacy policy\ncorpora/taxonomies and concepts. Furthermore, we evaluated the explainability\nof the LLM-based classifiers using three metrics: completeness, logicality, and\ncomprehensibility. For all three metrics, a score exceeding 91.1\\% was observed\nin our evaluation, indicating that LLMs are not only useful to improve the\nclassification performance, but also to enhance the explainability of detection\nresults.",
      "tldr_zh": "本研究评估了使用大语言模型（LLMs）进行隐私政策自动分析的方法，包括提示工程（prompt engineering）和 LoRA 微调（fine-tuning），以检测政策中的概念并提升分析效率。实验在四个 SOTA 隐私政策语料库和分类体系上进行，结果显示，结合提示工程和微调的 LLM 分类器显著优于现有方法，在多个语料库和概念上实现了性能提升。论文还评估了分类器的可解释性，使用完整性、逻辑性和可理解性三个指标，均超过 91.1%，证明 LLMs 不仅提高了分类准确性，还增强了结果的可解释性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16516v1",
      "published_date": "2025-03-16 10:50:31 UTC",
      "updated_date": "2025-03-16 10:50:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:48:46.716439"
    },
    {
      "arxiv_id": "2503.12447v1",
      "title": "Causality Model for Semantic Understanding on Videos",
      "title_zh": "翻译失败",
      "authors": [
        "Li Yicong"
      ],
      "abstract": "After a decade of prosperity, the development of video understanding has\nreached a critical juncture, where the sole reliance on massive data and\ncomplex architectures is no longer a one-size-fits-all solution to all\nsituations. The presence of ubiquitous data imbalance hampers DNNs from\neffectively learning the underlying causal mechanisms, leading to significant\nperformance drops when encountering distribution shifts, such as long-tail\nimbalances and perturbed imbalances. This realization has prompted researchers\nto seek alternative methodologies to capture causal patterns in video data. To\ntackle these challenges and increase the robustness of DNNs, causal modeling\nemerged as a principle to discover the true causal patterns behind the observed\ncorrelations. This thesis focuses on the domain of semantic video understanding\nand explores the potential of causal modeling to advance two fundamental tasks:\nVideo Relation Detection (VidVRD) and Video Question Answering (VideoQA).",
      "tldr_zh": "视频理解领域的发展面临数据不平衡的挑战，导致深度神经网络(DNNs)无法有效学习底层因果机制，从而在分布偏移（如长尾分布或扰动）时性能显著下降。  \n本论文引入因果建模(causal modeling)作为一种新方法，来捕捉视频数据中的真实因果模式，从而提升模型的鲁棒性。  \n研究重点探索因果建模在视频语义理解中的应用，包括视频关系检测(VidVRD)和视频问答(VideoQA)两个核心任务。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "PhD Thesis",
      "pdf_url": "http://arxiv.org/pdf/2503.12447v1",
      "published_date": "2025-03-16 10:44:11 UTC",
      "updated_date": "2025-03-16 10:44:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:48:57.833375"
    },
    {
      "arxiv_id": "2503.12446v1",
      "title": "BREEN: Bridge Data-Efficient Encoder-Free Multimodal Learning with Learnable Queries",
      "title_zh": "翻译失败",
      "authors": [
        "Tianle Li",
        "Yongming Rao",
        "Winston Hu",
        "Yu Cheng"
      ],
      "abstract": "Encoder-free multimodal large language models(MLLMs) eliminate the need for a\nwell-trained vision encoder by directly processing image tokens before the\nlanguage model. While this approach reduces computational overhead and model\ncomplexity, it often requires large amounts of training data to effectively\ncapture the visual knowledge typically encoded by vision models like CLIP. The\nabsence of a vision encoder implies that the model is likely to rely on\nsubstantial data to learn the necessary visual-semantic alignments. In this\nwork, we present BREEN, a data-efficient encoder-free multimodal architecture\nthat mitigates this issue. BREEN leverages a learnable query and image experts\nto achieve comparable performance with significantly less training data. The\nlearnable query, positioned between image and text tokens, is supervised by the\noutput of a pretrained CLIP model to distill visual knowledge, bridging the gap\nbetween visual and textual modalities. Additionally, the image expert processes\nimage tokens and learnable queries independently, improving efficiency and\nreducing interference with the LLM's textual capabilities. BREEN achieves\ncomparable performance to prior encoder-free state-of-the-art models like\nMono-InternVL, using only 13 million text-image pairs in training about one\npercent of the data required by existing methods. Our work highlights a\npromising direction for data-efficient encoder-free multimodal learning,\noffering an alternative to traditional encoder-based approaches.",
      "tldr_zh": "本文提出 BREEN，一种数据高效的无编码器多模态学习架构，旨在解决多模态大语言模型(MLLMs) 在缺少视觉编码器时对大量训练数据的依赖问题。BREEN 通过引入 learnable query 和 image experts，将图像标记与文本标记桥接，并利用预训练的 CLIP 模型进行知识蒸馏，提高视觉-语义对齐效率，同时减少对 LLM 文本能力的干扰。实验显示，BREEN 仅使用 13 百万文本-图像对（约占现有方法的 1%）即可实现与 Mono-InternVL 等最先进模型相当的性能，为数据高效的多模态学习提供了一个可行替代方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12446v1",
      "published_date": "2025-03-16 10:43:14 UTC",
      "updated_date": "2025-03-16 10:43:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:49:12.048317"
    },
    {
      "arxiv_id": "2503.12434v1",
      "title": "A Survey on the Optimization of Large Language Model-based Agents",
      "title_zh": "大语言模型基代理的优化综述",
      "authors": [
        "Shangheng Du",
        "Jiabao Zhao",
        "Jinxin Shi",
        "Zhentao Xie",
        "Xin Jiang",
        "Yanhong Bai",
        "Liang He"
      ],
      "abstract": "With the rapid development of Large Language Models (LLMs), LLM-based agents\nhave been widely adopted in various fields, becoming essential for autonomous\ndecision-making and interactive tasks. However, current work typically relies\non prompt design or fine-tuning strategies applied to vanilla LLMs, which often\nleads to limited effectiveness or suboptimal performance in complex\nagent-related environments. Although LLM optimization techniques can improve\nmodel performance across many general tasks, they lack specialized optimization\ntowards critical agent functionalities such as long-term planning, dynamic\nenvironmental interaction, and complex decision-making. Although numerous\nrecent studies have explored various strategies to optimize LLM-based agents\nfor complex agent tasks, a systematic review summarizing and comparing these\nmethods from a holistic perspective is still lacking. In this survey, we\nprovide a comprehensive review of LLM-based agent optimization approaches,\ncategorizing them into parameter-driven and parameter-free methods. We first\nfocus on parameter-driven optimization, covering fine-tuning-based\noptimization, reinforcement learning-based optimization, and hybrid strategies,\nanalyzing key aspects such as trajectory data construction, fine-tuning\ntechniques, reward function design, and optimization algorithms. Additionally,\nwe briefly discuss parameter-free strategies that optimize agent behavior\nthrough prompt engineering and external knowledge retrieval. Finally, we\nsummarize the datasets and benchmarks used for evaluation and tuning, review\nkey applications of LLM-based agents, and discuss major challenges and\npromising future directions. Our repository for related references is available\nat https://github.com/YoungDubbyDu/LLM-Agent-Optimization.",
      "tldr_zh": "这篇调查论文综述了优化 Large Language Model-based Agents 的方法，填补了现有研究在系统性比较方面的空白。论文将优化策略分为参数驱动方法（如 fine-tuning-based 优化、reinforcement learning-based 优化和混合策略）和参数-free 方法（如 prompt engineering 和外部知识检索），并详细分析了轨迹数据构建、奖励函数设计以及优化算法等关键方面。同时，它总结了用于评估的数据集和基准，探讨了 LLM-based agents 在决策和交互任务中的应用、主要挑战（如长期规划和动态交互问题）以及未来的研究方向。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12434v1",
      "published_date": "2025-03-16 10:09:10 UTC",
      "updated_date": "2025-03-16 10:09:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:49:26.547798"
    },
    {
      "arxiv_id": "2503.12427v1",
      "title": "Towards Learnable Anchor for Deep Multi-View Clustering",
      "title_zh": "面向深度多视图聚类的可学习锚点",
      "authors": [
        "Bocheng Wang",
        "Chusheng Zeng",
        "Mulin Chen",
        "Xuelong Li"
      ],
      "abstract": "Deep multi-view clustering incorporating graph learning has presented\ntremendous potential. Most methods encounter costly square time consumption\nw.r.t. data size. Theoretically, anchor-based graph learning can alleviate this\nlimitation, but related deep models mainly rely on manual discretization\napproaches to select anchors, which indicates that 1) the anchors are fixed\nduring model training and 2) they may deviate from the true cluster\ndistribution. Consequently, the unreliable anchors may corrupt clustering\nresults. In this paper, we propose the Deep Multi-view Anchor Clustering (DMAC)\nmodel that performs clustering in linear time. Concretely, the initial anchors\nare intervened by the positive-incentive noise sampled from Gaussian\ndistribution, such that they can be optimized with a newly designed anchor\nlearning loss, which promotes a clear relationship between samples and anchors.\nAfterwards, anchor graph convolution is devised to model the cluster structure\nformed by the anchors, and the mutual information maximization loss is built to\nprovide cross-view clustering guidance. In this way, the learned anchors can\nbetter represent clusters. With the optimal anchors, the full sample graph is\ncalculated to derive a discriminative embedding for clustering. Extensive\nexperiments on several datasets demonstrate the superior performance and\nefficiency of DMAC compared to state-of-the-art competitors.",
      "tldr_zh": "本研究针对深度多-view聚类中基于图学习的模型时间复杂度高的问题，提出了一种可学习的锚点框架，即Deep Multi-View Anchor Clustering (DMAC)模型，以实现线性时间复杂度。DMAC通过从高斯分布采样正激励噪声干预初始锚点，并结合新设计的anchor learning loss优化锚点关系、anchor graph convolution建模聚类结构，以及mutual information maximization loss提供跨视图指导，从而获得更准确的聚类表示。实验结果显示，DMAC在多个数据集上比现有最先进方法表现出色，并显著提高了效率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by AAAI25",
      "pdf_url": "http://arxiv.org/pdf/2503.12427v1",
      "published_date": "2025-03-16 09:38:11 UTC",
      "updated_date": "2025-03-16 09:38:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:49:34.406018"
    },
    {
      "arxiv_id": "2504.13853v1",
      "title": "GenShin:geometry-enhanced structural graph embodies binding pose can better predicting compound-protein interaction affinity",
      "title_zh": "翻译失败",
      "authors": [
        "Pingfei Zhu",
        "Chenyang Zhao",
        "Haishi Zhao",
        "Bo Yang"
      ],
      "abstract": "AI-powered drug discovery typically relies on the successful prediction of\ncompound-protein interactions, which are pivotal for the evaluation of designed\ncompound molecules in structure-based drug design and represent a core\nchallenge in the field.\n  However, accurately predicting compound-protein affinity via regression\nmodels usually requires adequate-binding pose, which are derived from costly\nand complex experimental methods or time-consuming simulations with docking\nsoftware. In response, we have introduced the GenShin model, which constructs a\ngeometry-enhanced structural graph module that separately extracts additional\nfeatures from proteins and compounds. Consequently, it attains an accuracy on\npar with mainstream models in predicting compound-protein affinities, while\neliminating the need for adequate-binding pose as input. Our experimental\nfindings demonstrate that the GenShin model vastly outperforms other models\nthat rely on non-input docking conformations, achieving, or in some cases even\nexceeding, the performance of those requiring adequate-binding pose. Further\nexperiments indicate that our GenShin model is more robust to\ninadequate-binding pose, affirming its higher suitability for real-world drug\ndiscovery scenarios. We hope our work will inspire more endeavors to bridge the\ngap between AI models and practical drug discovery challenges.",
      "tldr_zh": "该研究引入了GenShin模型，一种geometry-enhanced structural graph模块，用于预测化合物-蛋白质交互亲和力，而无需依赖昂贵的binding pose输入。该模型通过从蛋白质和化合物中分别提取额外几何特征，构建增强结构图，从而实现与主流模型相当的预测准确性。实验结果显示，GenShin显著优于依赖非输入对接构象的模型，甚至在inadequate-binding pose情况下表现出更强的鲁棒性，为AI驱动的药物发现提供了更实用且高效的解决方案。",
      "categories": [
        "q-bio.BM",
        "cs.AI"
      ],
      "primary_category": "q-bio.BM",
      "comment": "11 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.13853v1",
      "published_date": "2025-03-16 09:11:56 UTC",
      "updated_date": "2025-03-16 09:11:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:49:46.290137"
    },
    {
      "arxiv_id": "2503.12406v1",
      "title": "Bio-Inspired Plastic Neural Networks for Zero-Shot Out-of-Distribution Generalization in Complex Animal-Inspired Robots",
      "title_zh": "翻译失败",
      "authors": [
        "Binggwong Leung",
        "Worasuchad Haomachai",
        "Joachim Winther Pedersen",
        "Sebastian Risi",
        "Poramate Manoonpong"
      ],
      "abstract": "Artificial neural networks can be used to solve a variety of robotic tasks.\nHowever, they risk failing catastrophically when faced with out-of-distribution\n(OOD) situations. Several approaches have employed a type of synaptic\nplasticity known as Hebbian learning that can dynamically adjust weights based\non local neural activities. Research has shown that synaptic plasticity can\nmake policies more robust and help them adapt to unforeseen changes in the\nenvironment. However, networks augmented with Hebbian learning can lead to\nweight divergence, resulting in network instability. Furthermore, such Hebbian\nnetworks have not yet been applied to solve legged locomotion in complex real\nrobots with many degrees of freedom. In this work, we improve the Hebbian\nnetwork with a weight normalization mechanism for preventing weight divergence,\nanalyze the principal components of the Hebbian's weights, and perform a\nthorough evaluation of network performance in locomotion control for real\n18-DOF dung beetle-like and 16-DOF gecko-like robots. We find that the\nHebbian-based plastic network can execute zero-shot sim-to-real adaptation\nlocomotion and generalize to unseen conditions, such as uneven terrain and\nmorphological damage.",
      "tldr_zh": "本研究针对人工神经网络在分布外（out-of-distribution, OOD）情况下的灾难性失败问题，提出了一种受生物启发的可塑神经网络，通过整合 Hebbian learning 机制来动态调整权重，提高机器人的鲁棒性和适应性。论文改进 Hebbian 网络，引入权重归一化机制以防止权重发散，并分析权重的主成分，以确保网络稳定性。实验在真实 18-DOF 粪甲虫-like 和 16-DOF 壁虎-like 机器人上进行，结果显示，该网络实现了零样本（zero-shot）sim-to-real 适应，并能泛化到未见条件，如不平坦地形和形态损伤。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12406v1",
      "published_date": "2025-03-16 08:13:53 UTC",
      "updated_date": "2025-03-16 08:13:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:49:58.590916"
    },
    {
      "arxiv_id": "2503.12389v1",
      "title": "FedGAI: Federated Style Learning with Cloud-Edge Collaboration for Generative AI in Fashion Design",
      "title_zh": "FedGAI：联邦风格学习",
      "authors": [
        "Mingzhu Wu",
        "Jianan Jiang",
        "Xinglin Li",
        "Hanhui Deng",
        "Di Wu"
      ],
      "abstract": "Collaboration can amalgamate diverse ideas, styles, and visual elements,\nfostering creativity and innovation among different designers. In collaborative\ndesign, sketches play a pivotal role as a means of expressing design\ncreativity. However, designers often tend to not openly share these\nmeticulously crafted sketches. This phenomenon of data island in the design\narea hinders its digital transformation under the third wave of AI. In this\npaper, we introduce a Federated Generative Artificial Intelligence Clothing\nsystem, namely FedGAI, employing federated learning to aid in sketch design.\nFedGAI is committed to establishing an ecosystem wherein designers can exchange\nsketch styles among themselves. Through FedGAI, designers can generate sketches\nthat incorporate various designers' styles from their peers, drawing\ninspiration from collaboration without the need for data disclosure or upload.\nExtensive performance evaluations indicate that our FedGAI system can produce\nmulti-styled sketches of comparable quality to human-designed ones while\nsignificantly enhancing efficiency compared to hand-drawn sketches.",
      "tldr_zh": "本研究针对时尚设计领域的数据孤岛问题，提出FedGAI系统，这是一种基于Federated Learning的生成式AI框架，通过云边协作（Cloud-Edge Collaboration）实现设计师间风格交换，而无需共享原始数据。FedGAI允许用户生成融合多设计师风格的草图，激发创意并保护隐私。实验结果显示，该系统产出的多风格草图质量可与人工设计媲美，并显著提升设计效率。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12389v1",
      "published_date": "2025-03-16 07:31:25 UTC",
      "updated_date": "2025-03-16 07:31:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:50:09.642128"
    },
    {
      "arxiv_id": "2503.12374v2",
      "title": "Unveiling Pitfalls: Understanding Why AI-driven Code Agents Fail at GitHub Issue Resolution",
      "title_zh": "揭示陷",
      "authors": [
        "Zhi Chen",
        "Wei Ma",
        "Lingxiao Jiang"
      ],
      "abstract": "AI-driven software development has rapidly advanced with the emergence of\nsoftware development agents that leverage large language models (LLMs) to\ntackle complex, repository-level software engineering tasks. These agents go\nbeyond just generation of final code; they engage in multi-step reasoning,\nutilize various tools for code modification and debugging, and interact with\nexecution environments to diagnose and iteratively resolve issues. However,\nmost existing evaluations focus primarily on static analyses of final code\noutputs, yielding limited insights into the agents' dynamic problem-solving\nprocesses. To fill this gap, we conduct an in-depth empirical study on 3,977\nsolving-phase trajectories and 3,931 testing-phase logs from 8 top-ranked\nagents evaluated on 500 GitHub issues in the SWE-Bench benchmark. Our\nexploratory analysis shows that Python execution errors during the issue\nresolution phase correlate with lower resolution rates and increased reasoning\noverheads. We have identified the most prevalent errors -- such as\nModuleNotFoundError and TypeError -- and highlighted particularly challenging\nerrors like OSError and database-related issues (e.g., IntegrityError) that\ndemand significantly more debugging effort. Furthermore, we have discovered 3\nbugs in the SWE-Bench platform that affect benchmark fairness and accuracy;\nthese issues have been reported to and confirmed by the maintainers. To promote\ntransparency and foster future research, we publicly share our datasets and\nanalysis scripts.",
      "tldr_zh": "该研究探讨了AI驱动代码代理在GitHub问题解决中的失败原因，这些代理利用大型语言模型(LLMs)进行多步推理、代码修改和调试。研究者通过分析8个顶级代理在SWE-Bench基准上的3,977个解决阶段轨迹和3,931个测试阶段日志，发现Python执行错误（如ModuleNotFoundError和TypeError）与较低的解决率和增加的推理开销密切相关，尤其OSError和数据库相关问题（如IntegrityError）需要更多调试努力。论文还识别了SWE-Bench平台中的3个bug，并公开共享数据集和分析脚本，以提升基准公平性和推动未来研究。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12374v2",
      "published_date": "2025-03-16 06:24:51 UTC",
      "updated_date": "2025-03-19 10:08:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:50:22.185479"
    },
    {
      "arxiv_id": "2503.16515v1",
      "title": "Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science",
      "title_zh": "突出跨学科系统科学 LLM 文献综述中的案例研究",
      "authors": [
        "Lachlan McGinness",
        "Peter Baumgartner"
      ],
      "abstract": "Large Language Models (LLMs) were used to assist four Commonwealth Scientific\nand Industrial Research Organisation (CSIRO) researchers to perform systematic\nliterature reviews (SLR). We evaluate the performance of LLMs for SLR tasks in\nthese case studies. In each, we explore the impact of changing parameters on\nthe accuracy of LLM responses. The LLM was tasked with extracting evidence from\nchosen academic papers to answer specific research questions. We evaluate the\nmodels' performance in faithfully reproducing quotes from the literature and\nsubject experts were asked to assess the model performance in answering the\nresearch questions. We developed a semantic text highlighting tool to\nfacilitate expert review of LLM responses.\n  We found that state of the art LLMs were able to reproduce quotes from texts\nwith greater than 95% accuracy and answer research questions with an accuracy\nof approximately 83%. We use two methods to determine the correctness of LLM\nresponses; expert review and the cosine similarity of transformer embeddings of\nLLM and expert answers. The correlation between these methods ranged from 0.48\nto 0.77, providing evidence that the latter is a valid metric for measuring\nsemantic similarity.",
      "tldr_zh": "本研究探讨了大型语言模型（LLMs）在跨学科系统科学文献综述中的应用，通过四个案例研究评估 LLMs 辅助系统文献综述（SLR）的性能。研究人员调整参数，让 LLMs 从学术论文中提取证据回答特定研究问题，并使用语义文本突出工具辅助专家审查。结果显示，LLMs 在忠实复制引用方面准确率超过 95%，而在回答研究问题方面准确率约 83%。此外，通过专家审查和余弦相似度（基于 transformer embeddings）两种方法评估响应正确性，这两种方法的相关性在 0.48 到 0.77 之间，证明余弦相似度是有效的语义相似性指标。该工作为 LLMs 在 SLR 任务中的可靠应用提供了实证支持。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16515v1",
      "published_date": "2025-03-16 05:52:18 UTC",
      "updated_date": "2025-03-16 05:52:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:50:34.431828"
    },
    {
      "arxiv_id": "2503.12358v3",
      "title": "IPCGRL: Language-Instructed Reinforcement Learning for Procedural Level Generation",
      "title_zh": "IPCGRL：语言指令强化学习用于程序化关卡生成",
      "authors": [
        "In-Chang Baek",
        "Sung-Hyun Kim",
        "Seo-Young Lee",
        "Dong-Hyeon Kim",
        "Kyung-Joong Kim"
      ],
      "abstract": "Recent research has highlighted the significance of natural language in\nenhancing the controllability of generative models. While various efforts have\nbeen made to leverage natural language for content generation, research on deep\nreinforcement learning (DRL) agents utilizing text-based instructions for\nprocedural content generation remains limited. In this paper, we propose\nIPCGRL, an instruction-based procedural content generation method via\nreinforcement learning, which incorporates a sentence embedding model. IPCGRL\nfine-tunes task-specific embedding representations to effectively compress\ngame-level conditions. We evaluate IPCGRL in a two-dimensional level generation\ntask and compare its performance with a general-purpose embedding method. The\nresults indicate that IPCGRL achieves up to a 21.4% improvement in\ncontrollability and a 17.2% improvement in generalizability for unseen\ninstructions. Furthermore, the proposed method extends the modality of\nconditional input, enabling a more flexible and expressive interaction\nframework for procedural content generation.",
      "tldr_zh": "本论文提出IPCGRL，一种基于语言指令的强化学习(Reinforcement Learning)方法，用于程序化内容生成，通过整合句子嵌入模型(sentence embedding model)来微调任务特定的嵌入表示，从而有效压缩游戏关卡条件。相比通用嵌入方法，IPCGRL在二维关卡生成任务中实现了可控性提升高达21.4%和泛化能力提升17.2%，尤其在处理未见指令时表现出色。该方法扩展了条件输入的模态，提供更灵活和富有表现力的交互框架，增强了生成模型的可控性。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 9 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.12358v3",
      "published_date": "2025-03-16 04:53:38 UTC",
      "updated_date": "2025-03-25 01:48:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:50:46.608445"
    },
    {
      "arxiv_id": "2503.12356v2",
      "title": "Localized Concept Erasure for Text-to-Image Diffusion Models Using Training-Free Gated Low-Rank Adaptation",
      "title_zh": "翻译失败",
      "authors": [
        "Byung Hyun Lee",
        "Sungjin Lim",
        "Se Young Chun"
      ],
      "abstract": "Fine-tuning based concept erasing has demonstrated promising results in\npreventing generation of harmful contents from text-to-image diffusion models\nby removing target concepts while preserving remaining concepts. To maintain\nthe generation capability of diffusion models after concept erasure, it is\nnecessary to remove only the image region containing the target concept when it\nlocally appears in an image, leaving other regions intact. However, prior arts\noften compromise fidelity of the other image regions in order to erase the\nlocalized target concept appearing in a specific area, thereby reducing the\noverall performance of image generation. To address these limitations, we first\nintroduce a framework called localized concept erasure, which allows for the\ndeletion of only the specific area containing the target concept in the image\nwhile preserving the other regions. As a solution for the localized concept\nerasure, we propose a training-free approach, dubbed Gated Low-rank adaptation\nfor Concept Erasure (GLoCE), that injects a lightweight module into the\ndiffusion model. GLoCE consists of low-rank matrices and a simple gate,\ndetermined only by several generation steps for concepts without training. By\ndirectly applying GLoCE to image embeddings and designing the gate to activate\nonly for target concepts, GLoCE can selectively remove only the region of the\ntarget concepts, even when target and remaining concepts coexist within an\nimage. Extensive experiments demonstrated GLoCE not only improves the image\nfidelity to text prompts after erasing the localized target concepts, but also\noutperforms prior arts in efficacy, specificity, and robustness by large margin\nand can be extended to mass concept erasure.",
      "tldr_zh": "本文提出 localized concept erasure 框架，用于在 text-to-image diffusion models 中删除图像特定区域的目标概念，同时保留其他区域，以避免现有方法对整体生成性能的损害。  \n他们引入了 GLoCE（Gated Low-Rank Adaptation for Concept Erasure），一种无需训练的轻量级模块，利用低秩矩阵和门控机制，仅针对目标概念激活，从而实现对共存概念的 selective removal。  \n实验显示，GLoCE 显著提高了图像对文本提示的保真度，并在 efficacy、specificity 和 robustness 方面大幅优于现有方法，并可扩展到 mass concept erasure。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.12356v2",
      "published_date": "2025-03-16 04:53:20 UTC",
      "updated_date": "2025-03-25 15:29:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:50:58.784196"
    },
    {
      "arxiv_id": "2503.12353v1",
      "title": "Synthetic Data for Robust AI Model Development in Regulated Enterprises",
      "title_zh": "合成",
      "authors": [
        "Aditi Godbole"
      ],
      "abstract": "In today's business landscape, organizations need to find the right balance\nbetween using their customers' data ethically to power AI solutions and being\ncompliant regarding data privacy and data usage regulations. In this paper, we\ndiscuss synthetic data as a possible solution to this dilemma. Synthetic data\nis simulated data that mimics the real data. We explore how organizations in\nheavily regulated industries, such as financial institutions or healthcare\norganizations, can leverage synthetic data to build robust AI solutions while\nstaying compliant. We demonstrate that synthetic data offers two significant\nadvantages by allowing AI models to learn from more diverse data and by helping\norganizations stay compliant against data privacy laws with the use of\nsynthetic data instead of customer information. We discuss case studies to show\nhow synthetic data can be effectively used in the finance and healthcare sector\nwhile discussing the challenges of using synthetic data and some ethical\nquestions it raises. Our research finds that synthetic data could be a\ngame-changer for AI in regulated industries. The potential can be realized when\nindustry, academia, and regulators collaborate to build solutions. We aim to\ninitiate discussions on the use of synthetic data to build ethical,\nresponsible, and effective AI systems in regulated enterprise industries.",
      "tldr_zh": "本文探讨了企业在AI模型开发中如何平衡使用客户数据与数据隐私合规性的问题，提出synthetic data作为解决方案，该数据模拟真实数据以构建稳健AI系统。研究显示，synthetic data能使AI模型从更多样化的数据中学习，同时帮助受监管行业（如金融和医疗）遵守数据隐私法规，并通过案例研究展示了其在这些领域的实际应用。虽存在挑战和伦理问题，但作者认为，通过行业、学术和监管者的合作，synthetic data有望成为推动伦理、负责任AI发展的关键变革者。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12353v1",
      "published_date": "2025-03-16 04:46:41 UTC",
      "updated_date": "2025-03-16 04:46:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:51:10.354922"
    },
    {
      "arxiv_id": "2503.13543v1",
      "title": "Enhancing Visual Representation with Textual Semantics: Textual Semantics-Powered Prototypes for Heterogeneous Federated Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Xinghao Wu",
        "Jianwei Niu",
        "Xuefeng Liu",
        "Guogang Zhu",
        "Jiayuan Zhang",
        "Shaojie Tang"
      ],
      "abstract": "Federated Prototype Learning (FedPL) has emerged as an effective strategy for\nhandling data heterogeneity in Federated Learning (FL). In FedPL, clients\ncollaboratively construct a set of global feature centers (prototypes), and let\nlocal features align with these prototypes to mitigate the effects of data\nheterogeneity. The performance of FedPL highly depends on the quality of\nprototypes. Existing methods assume that larger inter-class distances among\nprototypes yield better performance, and thus design different methods to\nincrease these distances. However, we observe that while these methods increase\nprototype distances to enhance class discrimination, they inevitably disrupt\nessential semantic relationships among classes, which are crucial for model\ngeneralization. This raises an important question: how to construct prototypes\nthat inherently preserve semantic relationships among classes? Directly\nlearning these relationships from limited and heterogeneous client data can be\nproblematic in FL. Recently, the success of pre-trained language models (PLMs)\ndemonstrates their ability to capture semantic relationships from vast textual\ncorpora. Motivated by this, we propose FedTSP, a novel method that leverages\nPLMs to construct semantically enriched prototypes from the textual modality,\nenabling more effective collaboration in heterogeneous data settings. We first\nuse a large language model (LLM) to generate fine-grained textual descriptions\nfor each class, which are then processed by a PLM on the server to form textual\nprototypes. To address the modality gap between client image models and the\nPLM, we introduce trainable prompts, allowing prototypes to adapt better to\nclient tasks. Extensive experiments demonstrate that FedTSP mitigates data\nheterogeneity while significantly accelerating convergence.",
      "tldr_zh": "本研究针对 Federated Learning (FL) 中的数据异质性问题，提出 Federated Prototype Learning (FedPL) 的改进方法 FedTSP，通过利用 Pre-trained Language Models (PLMs) 构建语义丰富的 prototypes，以保留类间语义关系。FedTSP 首先使用 Large Language Model (LLM) 生成每个类的细粒度文本描述，然后由 PLM 在服务器端处理成文本 prototypes，并引入可训练的 prompts 来桥接客户端图像模型与文本模态的差距。实验结果表明，该方法不仅有效缓解了数据异质性，还显著加速了模型收敛。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.13543v1",
      "published_date": "2025-03-16 04:35:06 UTC",
      "updated_date": "2025-03-16 04:35:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:51:22.968459"
    },
    {
      "arxiv_id": "2503.13542v1",
      "title": "HAR-DoReMi: Optimizing Data Mixture for Self-Supervised Human Activity Recognition Across Heterogeneous IMU Datasets",
      "title_zh": "HAR-DoReMi：优化数据混合用于自监督人类活动识别跨异构 IMU 数据集",
      "authors": [
        "Lulu Ban",
        "Tao Zhu",
        "Xiangqing Lu",
        "Qi Qiu",
        "Wenyong Han",
        "Shuangjian Li",
        "Liming Chen",
        "Kevin I-Kai Wang",
        "Mingxing Nie",
        "Yaping Wan"
      ],
      "abstract": "Cross-dataset Human Activity Recognition (HAR) suffers from limited model\ngeneralization, hindering its practical deployment. To address this critical\nchallenge, inspired by the success of DoReMi in Large Language Models (LLMs),\nwe introduce a data mixture optimization strategy for pre-training HAR models,\naiming to improve the recognition performance across heterogeneous datasets.\nHowever, directly applying DoReMi to the HAR field encounters new challenges\ndue to the continuous, multi-channel and intrinsic heterogeneous\ncharacteristics of IMU sensor data. To overcome these limitations, we propose a\nnovel framework HAR-DoReMi, which introduces a masked reconstruction task based\non Mean Squared Error (MSE) loss. By raplacing the discrete language sequence\nprediction task, which relies on the Negative Log-Likelihood (NLL) loss, in the\noriginal DoReMi framework, the proposed framework is inherently more\nappropriate for handling the continuous and multi-channel characteristics of\nIMU data. In addition, HAR-DoReMi integrates the Mahony fusion algorithm into\nthe self-supervised HAR pre-training, aiming to mitigate the heterogeneity of\nvarying sensor orientation. This is achieved by estimating the sensor\norientation within each dataset and facilitating alignment with a unified\ncoordinate system, thereby improving the cross-dataset generalization ability\nof the HAR model. Experimental evaluation on multiple cross-dataset HAR\ntransfer tasks demonstrates that HAR-DoReMi improves the accuracy by an average\nof 6.51%, compared to the current state-of-the-art method with only\napproximately 30% to 50% of the data usage. These results confirm the\neffectiveness of HAR-DoReMi in improving the generalization and data efficiency\nof pre-training HAR models, underscoring its significant potential to\nfacilitate the practical deployment of HAR technology.",
      "tldr_zh": "这篇论文针对跨异构 IMU 数据集的自监督 Human Activity Recognition (HAR) 问题，提出 HAR-DoReMi 框架，以优化数据混合策略并提升模型泛化性能。框架通过引入基于 Mean Squared Error (MSE) 损失的掩码重建任务，取代原 DoReMi 中的 Negative Log-Likelihood (NLL) 损失的离散序列预测，从而更好地处理 IMU 数据的连续和多通道特性；同时，整合 Mahony 融合算法来估计传感器方向并对齐统一坐标系，缓解数据异构性。实验结果显示，HAR-DoReMi 在多个跨数据集传输任务中平均提高准确率 6.51%，并仅使用 30% 到 50% 的数据，显著提升了 HAR 模型的泛化和数据效率，促进其实际部署。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13542v1",
      "published_date": "2025-03-16 04:31:58 UTC",
      "updated_date": "2025-03-16 04:31:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:51:36.523283"
    },
    {
      "arxiv_id": "2503.12349v3",
      "title": "SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?",
      "title_zh": "SPIN-Bench：大型语言模型在战略规划和社交推理方面的表现如何？",
      "authors": [
        "Jianzhu Yao",
        "Kevin Wang",
        "Ryan Hsieh",
        "Haisu Zhou",
        "Tianqing Zou",
        "Zerui Cheng",
        "Zhangyang Wang",
        "Pramod Viswanath"
      ],
      "abstract": "Reasoning and strategic behavior in social interactions is a hallmark of\nintelligence. This form of reasoning is significantly more sophisticated than\nisolated planning or reasoning tasks in static settings (e.g., math problem\nsolving). In this paper, we present Strategic Planning, Interaction, and\nNegotiation (SPIN-Bench), a new multi-domain evaluation designed to measure the\nintelligence of strategic planning and social reasoning. While many existing\nbenchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench\ncombines classical PDDL tasks, competitive board games, cooperative card games,\nand multi-agent negotiation scenarios in one unified framework. The framework\nincludes both a benchmark as well as an arena to simulate and evaluate the\nvariety of social settings to test reasoning and strategic behavior of AI\nagents. We formulate the benchmark SPIN-Bench by systematically varying action\nspaces, state complexity, and the number of interacting agents to simulate a\nvariety of social settings where success depends on not only methodical and\nstep-wise decision making, but also conceptual inference of other (adversarial\nor cooperative) participants. Our experiments reveal that while contemporary\nLLMs handle basic fact retrieval and short-range planning reasonably well, they\nencounter significant performance bottlenecks in tasks requiring deep multi-hop\nreasoning over large state spaces and socially adept coordination under\nuncertainty. We envision SPIN-Bench as a catalyst for future research on robust\nmulti-agent planning, social reasoning, and human--AI teaming. Project Website:\nhttps://spinbench.github.io/",
      "tldr_zh": "本论文引入了 SPIN-Bench，一个多领域评估框架，用于评估大型语言模型 (LLMs) 在战略规划和社交推理方面的智能表现。SPIN-Bench 整合了经典 PDDL 任务、竞争性棋盘游戏、合作卡牌游戏以及多智能体谈判场景，通过系统变化行动空间、状态复杂性和代理数量来模拟各种社交互动环境。实验结果显示，当前 LLMs 在事实检索和短期规划上表现良好，但在大规模状态空间的多跳推理以及不确定条件下的社交协调上存在显著瓶颈。该框架有望推动未来多智能体规划、社交推理和人-AI 团队合作的研究。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "42 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.12349v3",
      "published_date": "2025-03-16 04:10:53 UTC",
      "updated_date": "2025-04-10 15:18:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:51:46.121617"
    },
    {
      "arxiv_id": "2503.12345v1",
      "title": "General Table Question Answering via Answer-Formula Joint Generation",
      "title_zh": "通用表格问题回答通过答案-公式联合生成",
      "authors": [
        "Zhongyuan Wang",
        "Richong Zhang",
        "Zhijie Nie"
      ],
      "abstract": "Advanced table question answering (TableQA) methods prompt large language\nmodels (LLMs) to generate answer text, SQL query, Python code, or custom\noperations, which impressively improve the complex reasoning problems in the\nTableQA task. However, these methods lack the versatility to cope with specific\nquestion types or table structures. In contrast, the Spreadsheet Formula, the\nwidely-used and well-defined operation language for tabular data, has not been\nthoroughly explored to solve TableQA. In this paper, we first attempt to use\nFormula as the logical form for solving complex reasoning on the tables with\ndifferent structures. Specifically, we construct a large Formula-annotated\nTableQA dataset \\texttt{FromulaQA} from existing datasets. In addition, we\npropose \\texttt{TabAF}, a general table answering framework to solve multiple\ntypes of tasks over multiple types of tables simultaneously. Unlike existing\nmethods, \\texttt{TabAF} decodes answers and Formulas with a single LLM\nbackbone, demonstrating great versatility and generalization. \\texttt{TabAF}\nbased on Llama3.1-70B achieves new state-of-the-art performance on the\nWikiTableQuestion, HiTab and TabFact.",
      "tldr_zh": "本文提出了一种通用的表格问题回答(TableQA)方法，通过联合生成答案和 Formulas 来解决现有方法在处理不同问题类型和表格结构时的局限性。研究者构建了大型 Formula-annotated 数据集 FormulaQA，并开发了 TabAF 框架，该框架使用单一 LLM 骨干（如 Llama3.1-70B）同时解码答案和 Formulas，提高了通用性和泛化能力。与现有方法相比，TabAF 能够在多种任务和表格类型上表现更佳，最终在 WikiTableQuestion、HiTab 和 TabFact 数据集上实现了新的 state-of-the-art 性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "work in progress",
      "pdf_url": "http://arxiv.org/pdf/2503.12345v1",
      "published_date": "2025-03-16 03:51:06 UTC",
      "updated_date": "2025-03-16 03:51:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:51:58.759907"
    },
    {
      "arxiv_id": "2503.13540v1",
      "title": "MSCMHMST: A traffic flow prediction model based on Transformer",
      "title_zh": "MSCMHMST：一种基于 Transformer 的交通流预测模型",
      "authors": [
        "Weiyang Geng",
        "Yiming Pan",
        "Zhecong Xing",
        "Dongyu Liu",
        "Rui Liu",
        "Yuan Zhu"
      ],
      "abstract": "This study proposes a hybrid model based on Transformers, named MSCMHMST,\naimed at addressing key challenges in traffic flow prediction. Traditional\nsingle-method approaches show limitations in traffic prediction tasks, whereas\nhybrid methods, by integrating the strengths of different models, can provide\nmore accurate and robust predictions. The MSCMHMST model introduces a\nmulti-head, multi-scale attention mechanism, allowing the model to parallel\nprocess different parts of the data and learn its intrinsic representations\nfrom multiple perspectives, thereby enhancing the model's ability to handle\ncomplex situations. This mechanism enables the model to capture features at\nvarious scales effectively, understanding both short-term changes and long-term\ntrends. Verified through experiments on the PeMS04/08 dataset with specific\nexperimental settings, the MSCMHMST model demonstrated excellent robustness and\naccuracy in long, medium, and short-term traffic flow predictions. The results\nindicate that this model has significant potential, offering a new and\neffective solution for the field of traffic flow prediction.",
      "tldr_zh": "本文提出了一种基于 Transformer 的混合交通流量预测模型 MSCMHMST，以解决传统方法的局限性，通过整合多种模型优势实现更准确和鲁棒的预测。该模型引入多头多尺度注意力机制（multi-head, multi-scale attention mechanism），允许并行处理数据并从多个视角学习内在表示，从而有效捕捉短期变化和长期趋势。在 PeMS04/08 数据集上的实验验证中，MSCMHMST 模型在长、中、短期预测中表现出优秀的鲁棒性和准确性，显著优于基线方法。该研究为交通流量预测领域提供了新的有效解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13540v1",
      "published_date": "2025-03-16 03:40:32 UTC",
      "updated_date": "2025-03-16 03:40:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:52:10.470605"
    },
    {
      "arxiv_id": "2503.12339v1",
      "title": "Augmented Adversarial Trigger Learning",
      "title_zh": "增强对抗触发器学习",
      "authors": [
        "Zhe Wang",
        "Yanjun Qi"
      ],
      "abstract": "Gradient optimization-based adversarial attack methods automate the learning\nof adversarial triggers to generate jailbreak prompts or leak system prompts.\nIn this work, we take a closer look at the optimization objective of\nadversarial trigger learning and propose ATLA: Adversarial Trigger Learning\nwith Augmented objectives. ATLA improves the negative log-likelihood loss used\nby previous studies into a weighted loss formulation that encourages the\nlearned adversarial triggers to optimize more towards response format tokens.\nThis enables ATLA to learn an adversarial trigger from just one query-response\npair and the learned trigger generalizes well to other similar queries. We\nfurther design a variation to augment trigger optimization with an auxiliary\nloss that suppresses evasive responses. We showcase how to use ATLA to learn\nadversarial suffixes jailbreaking LLMs and to extract hidden system prompts.\nEmpirically we demonstrate that ATLA consistently outperforms current\nstate-of-the-art techniques, achieving nearly 100% success in attacking while\nrequiring 80% fewer queries. ATLA learned jailbreak suffixes demonstrate high\ngeneralization to unseen queries and transfer well to new LLMs.",
      "tldr_zh": "本文提出 ATLA（Adversarial Trigger Learning with Augmented objectives），一种改进的对抗触发器学习方法，通过将负对数似然损失改为加权损失，鼓励触发器更关注响应格式标记，从而从单个查询-响应对中学习触发器并实现良好泛化。ATLA 还设计了辅助损失变体来抑制逃避性响应，支持应用如越狱 LLMs 和提取隐藏系统提示。实验显示，ATLA 比现有技术成功率近 100%，且只需 80% 的查询量，且学习到的触发器对未见查询有高泛化性并能转移到新 LLMs。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12339v1",
      "published_date": "2025-03-16 03:20:52 UTC",
      "updated_date": "2025-03-16 03:20:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:52:22.946555"
    },
    {
      "arxiv_id": "2503.12334v2",
      "title": "When neural implant meets multimodal LLM: A dual-loop system for neuromodulation and naturalistic neuralbehavioral research",
      "title_zh": "翻译失败",
      "authors": [
        "Edward Hong Wang",
        "Cynthia Xin Wen"
      ],
      "abstract": "We propose a novel dual-loop system that synergistically combines responsive\nneurostimulation (RNS) implants with artificial intelligence-driven wearable\ndevices for treating post-traumatic stress disorder (PTSD) and enabling\nnaturalistic brain research. In PTSD Therapy Mode, an implanted closed-loop\nneural device monitors amygdala activity and provides on-demand stimulation\nupon detecting pathological theta oscillations, while an ensemble of wearables\n(smart glasses, smartwatches, smartphones) uses multimodal large language model\n(LLM) analysis of sensory data to detect environmental or physiological PTSD\ntriggers and deliver timely audiovisual interventions. Logged events from both\nthe neural and wearable loops are analyzed to personalize trigger detection and\nprogressively transition patients to non-invasive interventions. In\nNeuroscience Research Mode, the same platform is adapted for real-world brain\nactivity capture. Wearable-LLM systems recognize naturalistic events (social\ninteractions, emotional situations, compulsive behaviors, decision making) and\nsignal implanted RNS devices (via wireless triggers) to record synchronized\nintracranial data during these moments. This approach builds on recent advances\nin mobile intracranial EEG recording and closed-loop neuromodulation in humans\n(BRAIN Initiative, 2023) (Mobbs et al., 2021). We discuss how our\ninterdisciplinary system could revolutionize PTSD therapy and cognitive\nneuroscience by enabling 24/7 monitoring, context-aware intervention, and rich\ndata collection outside traditional labs. The vision is a future where\nAI-enhanced devices continuously collaborate with the human brain, offering\ntherapeutic support and deep insights into neural function, with the resulting\nreal-world context rich neural data, in turn, accelerating the development of\nmore biologically-grounded and human-centric AI.",
      "tldr_zh": "该研究提出了一种双循环系统，将响应性神经刺激 (RNS) 植入物与 AI 驱动的可穿戴设备（如智能眼镜和手表）相结合，用于治疗创伤后应激障碍 (PTSD) 和进行自然主义神经行为研究。在 PTSD 治疗模式中，植入设备监测杏仁核活动并提供按需刺激，而可穿戴设备利用多模态大型语言模型 (LLM) 分析感官数据检测触发器，并提供及时的视听干预，同时记录数据以个性化治疗。在神经科学研究模式中，该系统捕获真实世界脑活动，可穿戴设备识别自然事件（如社交互动或决策）并触发 RNS 设备同步记录颅内数据。该框架有望革新 PTSD 治疗和认知神经科学，通过 24/7 监测、上下文感知干预和丰富数据收集，加速开发更生物基础的人类中心 AI。",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12334v2",
      "published_date": "2025-03-16 03:07:59 UTC",
      "updated_date": "2025-03-23 19:27:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:52:35.511482"
    },
    {
      "arxiv_id": "2503.12326v1",
      "title": "Leveraging Vision Capabilities of Multimodal LLMs for Automated Data Extraction from Plots",
      "title_zh": "利用多模态大型语言模型的视觉能力进行图表自动化",
      "authors": [
        "Maciej P. Polak",
        "Dane Morgan"
      ],
      "abstract": "Automated data extraction from research texts has been steadily improving,\nwith the emergence of large language models (LLMs) accelerating progress even\nfurther. Extracting data from plots in research papers, however, has been such\na complex task that it has predominantly been confined to manual data\nextraction. We show that current multimodal large language models, with proper\ninstructions and engineered workflows, are capable of accurately extracting\ndata from plots. This capability is inherent to the pretrained models and can\nbe achieved with a chain-of-thought sequence of zero-shot engineered prompts we\ncall PlotExtract, without the need to fine-tune. We demonstrate PlotExtract\nhere and assess its performance on synthetic and published plots. We consider\nonly plots with two axes in this analysis. For plots identified as extractable,\nPlotExtract finds points with over 90% precision (and around 90% recall) and\nerrors in x and y position of around 5% or lower. These results prove that\nmultimodal LLMs are a viable path for high-throughput data extraction for plots\nand in many circumstances can replace the current manual methods of data\nextraction.",
      "tldr_zh": "这篇论文探讨了利用多模态大型语言模型（Multimodal LLMs）的视觉能力，实现从研究论文图表中自动提取数据的潜力。作者提出了一种名为 PlotExtract 的零样本链式思维（Chain-of-Thought）提示序列，通过工程化工作流和适当指令进行数据提取，而无需对模型进行微调。实验结果显示，对于有两个轴的可提取图表，PlotExtract 在点识别上精确率超过90%、召回率约90%，且 x 和 y 位置错误率在5%或更低。这些发现证明，多模态 LLMs 是一种可行的高吞吐量数据提取路径，能够取代传统的手动方法。",
      "categories": [
        "cs.CV",
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.12326v1",
      "published_date": "2025-03-16 02:41:43 UTC",
      "updated_date": "2025-03-16 02:41:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:52:46.511388"
    },
    {
      "arxiv_id": "2503.12317v1",
      "title": "A Transformer-based survival model for prediction of all-cause mortality in heart failure patients: a multi-cohort study",
      "title_zh": "基于 Transformer 的生存模型用于预测心力衰",
      "authors": [
        "Shishir Rao",
        "Nouman Ahmed",
        "Gholamreza Salimi-Khorshidi",
        "Christopher Yau",
        "Huimin Su",
        "Nathalie Conrad",
        "Folkert W Asselbergs",
        "Mark Woodward",
        "Rod Jackson",
        "John GF Cleland",
        "Kazem Rahimi"
      ],
      "abstract": "We developed and validated TRisk, a Transformer-based AI model predicting\n36-month mortality in heart failure patients by analysing temporal patient\njourneys from UK electronic health records (EHR). Our study included 403,534\nheart failure patients (ages 40-90) from 1,418 English general practices, with\n1,063 practices for model derivation and 355 for external validation. TRisk was\ncompared against the MAGGIC-EHR model across various patient subgroups. With\nmedian follow-up of 9 months, TRisk achieved a concordance index of 0.845 (95%\nconfidence interval: [0.841, 0.849]), significantly outperforming MAGGIC-EHR's\n0.728 (0.723, 0.733) for predicting 36-month all-cause mortality. TRisk showed\nmore consistent performance across sex, age, and baseline characteristics,\nsuggesting less bias. We successfully adapted TRisk to US hospital data through\ntransfer learning, achieving a C-index of 0.802 (0.789, 0.816) with 21,767\npatients. Explainability analyses revealed TRisk captured established risk\nfactors while identifying underappreciated predictors like cancers and hepatic\nfailure that were important across both cohorts. Notably, cancers maintained\nstrong prognostic value even a decade after diagnosis. TRisk demonstrated\nwell-calibrated mortality prediction across both healthcare systems. Our\nfindings highlight the value of tracking longitudinal health profiles and\nrevealed risk factors not included in previous expert-driven models.",
      "tldr_zh": "本研究开发并验证了 TRisk，一种基于 Transformer 的 AI 生存模型，用于预测心力衰竭患者 36 个月全因死亡率，通过分析英国电子健康记录 (EHR) 中的时间序列患者数据。相比传统 MAGGIC-EHR 模型，TRisk 在一致性指数 (C-index) 上达到 0.845，显著优于后者的 0.728，并在性别、年龄和基线特征上表现出更稳定的性能，减少了偏见。研究还通过迁移学习将 TRisk 适应到美国医院数据，获得 C-index 为 0.802，并通过解释性分析识别了被低估的风险因素，如癌症和肝功能衰竭，这些因素在多个队列中显示出持久的预后价值。该模型突出了跟踪纵向健康档案的重要性，并揭示了专家驱动模型未涵盖的新风险因素。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.12317v1",
      "published_date": "2025-03-16 01:53:50 UTC",
      "updated_date": "2025-03-16 01:53:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:52:59.916035"
    },
    {
      "arxiv_id": "2503.12307v1",
      "title": "Swift4D:Adaptive divide-and-conquer Gaussian Splatting for compact and efficient reconstruction of dynamic scene",
      "title_zh": "翻译失败",
      "authors": [
        "Jiahao Wu",
        "Rui Peng",
        "Zhiyan Wang",
        "Lu Xiao",
        "Luyang Tang",
        "Jinbo Yan",
        "Kaiqiang Xiong",
        "Ronggang Wang"
      ],
      "abstract": "Novel view synthesis has long been a practical but challenging task, although\nthe introduction of numerous methods to solve this problem, even combining\nadvanced representations like 3D Gaussian Splatting, they still struggle to\nrecover high-quality results and often consume too much storage memory and\ntraining time. In this paper we propose Swift4D, a divide-and-conquer 3D\nGaussian Splatting method that can handle static and dynamic primitives\nseparately, achieving a good trade-off between rendering quality and\nefficiency, motivated by the fact that most of the scene is the static\nprimitive and does not require additional dynamic properties. Concretely, we\nfocus on modeling dynamic transformations only for the dynamic primitives which\nbenefits both efficiency and quality. We first employ a learnable decomposition\nstrategy to separate the primitives, which relies on an additional parameter to\nclassify primitives as static or dynamic. For the dynamic primitives, we employ\na compact multi-resolution 4D Hash mapper to transform these primitives from\ncanonical space into deformation space at each timestamp, and then mix the\nstatic and dynamic primitives to produce the final output. This\ndivide-and-conquer method facilitates efficient training and reduces storage\nredundancy. Our method not only achieves state-of-the-art rendering quality\nwhile being 20X faster in training than previous SOTA methods with a minimum\nstorage requirement of only 30MB on real-world datasets. Code is available at\nhttps://github.com/WuJH2001/swift4d.",
      "tldr_zh": "该论文提出Swift4D，一种自适应分治(Adaptive divide-and-conquer) 3D Gaussian Splatting 方法，用于高效且紧凑地重建动态场景，通过分离静态和动态元素实现渲染质量与效率的平衡。\nSwift4D 采用可学习的分解策略，将场景元素分类为静态或动态，仅对动态元素应用紧凑的多分辨率4D Hash mapper，将其从规范空间转换到变形空间，然后混合输出。\n实验结果表明，该方法在真实数据集上达到最先进的渲染质量，训练速度比现有SOTA方法快20倍，且存储需求仅30MB。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.12307v1",
      "published_date": "2025-03-16 01:13:11 UTC",
      "updated_date": "2025-03-16 01:13:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T02:53:10.673086"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 67,
  "processed_papers_count": 67,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-24T02:53:25.775769"
}
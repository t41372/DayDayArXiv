{
  "date": "2024-05-28",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-05-28 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文主要聚焦于 AI 和机器学习领域，尤其是大型语言模型 (LLM) 的安全、对齐、生成、多模态融合和应用创新，令人印象深刻的包括 NeurIPS 2024 论文（如 LeDex）和多模态模型框架（如 RealitySummary），以及知名学者（如 Stephen Obadrinma 在医疗 AI 领域的贡献），这些工作突出了 LLM 在实际场景中的潜力与挑战。\n\n下面，我将挑选并简要讨论几篇重要的、具有话题度的论文，先从核心创新和影响较大的入手，然后快速掠过其他相关或次要的。重点保留核心学术术语，如 LLM 对齐、多模态融合等。\n\n### 重点论文讨论\n\n**LeDex: Training LLMs to Better Self-Debug and Explain Code (英文标题: LeDex: Training LLMs to Better Self-Debug and Explain Code)**  \n这篇 NeurIPS 2024 论文提出了一种训练框架 LeDex，用于提升 LLM 的自调试和代码解释能力。主要贡献是通过生成高质量数据集并结合强化学习，显著提高了 LLM 在代码生成的 pass@1 和 pass@10 指标（分别提升至 15.92% 和 9.30%），使模型能迭代式地修复代码并提供更实用的解释，适用于复杂编程任务。\n\n**ChatGPT as the Marketplace of Ideas: Should Truth-Seeking Be the Goal of AI Content Governance? (英文标题: ChatGPT as the Marketplace of Ideas: Should Truth-Seeking Be the Goal of AI Content Governance?)**  \nJiawei Zhang 的论文探讨了 LLM 如 ChatGPT 在内容治理中的角色，强调其作为“思想市场”的双重性（优势和风险）。主要发现是，追求绝对真理可能导致过度风险管控，因此建议采用基于知识的替代策略，训练 LLM 生成多样观点以促进公平治理，这为 AI 内容风险管理提供了新视角。\n\n**RealitySummary: Exploring On-Demand Mixed Reality Text Summarization and Question Answering using Large Language Models (英文标题: RealitySummary: Exploring On-Demand Mixed Reality Text Summarization and Question Answering using Large Language Models)**  \n这篇论文引入了 RealitySummary 框架，利用 LLM 和混合现实技术进行文本总结和问答。主要贡献是通过迭代设计和用户反馈，实现了无缝的 AR 接口，支持文本提取和增强响应，显著提高了用户体验，实验显示在真实场景中提升了 11.8% 的性能。\n\n**Large Language Models as Partners in Student Essay Evaluation (英文标题: Large Language Models as Partners in Student Essay Evaluation)**  \nToru Ishida 等学者的工作探索了 LLM 在作文评估中的伙伴角色。主要发现是，LLM 通过成对比较和预设标准，能与教师评估高度相关（相关性达 90.9%），并提供多样解释，减轻教师负担，同时补充人类评估的不足。\n\n**ConSiDERS-The-Human Evaluation Framework: Rethinking Human Evaluation for Generative Large Language Models (英文标题: ConSiDERS-The-Human Evaluation Framework: Rethinking Human Evaluation for Generative Large Language Models)**  \n这篇 ACL 2024 论文提出 ConSiDERS 框架，用于评估 LLM 的生成质量。主要贡献是整合用户体验和认知偏差（如 Likert 评分的不确定性），强调一致性、可扩展性和责任感，实验证明能有效区分 LLM 的优势和弱点。\n\n**JADS: A Framework for Self-supervised Joint Aspect Discovery and Summarization (英文标题: JADS: A Framework for Self-supervised Joint Aspect Discovery and Summarization)**  \n论文引入 JADS 框架，通过自监督学习同时发现和总结文本主题。主要发现是，该方法在混合数据上优于传统两步方法，实现了更高的事实性和语义对齐，预训练后提升了稳定性。\n\n**Understanding Transformer Reasoning Capabilities via Graph Algorithms (英文标题: Understanding Transformer Reasoning Capabilities via Graph Algorithms)**  \nClayton Sanford 等人的工作分析了 Transformer 在图算法任务中的推理能力。主要贡献是证明了 Transformer 在不同参数规模下能解决图连通性等问题，实验显示其在 GraphQA 基准上超越了专业图神经网络。\n\n快速掠过其他论文：  \n今天的论文中，还有许多涉及 LLM 安全（如第79、99、140 篇的攻击防御）、多模态生成（如第26、82、146 篇的图像和文本融合）、强化学习（如第4、27、62、86 篇的策略优化）和脑科学（如第2、3、66 篇的脑网络分析）。这些工作虽重要，但相对次要，例如第2 篇的 D-CoRP 通过微分优化提升了脑网络建模，第3 篇的 CAVACHON 框架整合了单细胞多模态数据，第66 篇的脑肿瘤分割基准展示了实际应用潜力。这些论文的核心在于方法创新和实验验证，但由于篇幅限制，这里仅简要提及。\n\n总之，今天的 arXiv 论文展示了 AI 领域的多样创新，LLM 相关研究尤为突出，值得关注未来应用。更多细节可查阅具体论文！",
  "papers": [
    {
      "arxiv_id": "2405.18663v1",
      "title": "Lifelong Learning and Selective Forgetting via Contrastive Strategy",
      "title_zh": "基于对比",
      "authors": [
        "Lianlei Shan",
        "Wenzhang Zhou",
        "Wei Li",
        "Xingyu Ding"
      ],
      "abstract": "Lifelong learning aims to train a model with good performance for new tasks\nwhile retaining the capacity of previous tasks. However, some practical\nscenarios require the system to forget undesirable knowledge due to privacy\nissues, which is called selective forgetting. The joint task of the two is\ndubbed Learning with Selective Forgetting (LSF). In this paper, we propose a\nnew framework based on contrastive strategy for LSF. Specifically, for the\npreserved classes (tasks), we make features extracted from different samples\nwithin a same class compacted. And for the deleted classes, we make the\nfeatures from different samples of a same class dispersed and irregular, i.e.,\nthe network does not have any regular response to samples from a specific\ndeleted class as if the network has no training at all. Through maintaining or\ndisturbing the feature distribution, the forgetting and memory of different\nclasses can be or independent of each other. Experiments are conducted on four\nbenchmark datasets, and our method acieves new state-of-the-art.",
      "tldr_zh": "该论文提出了一种基于 Contrastive Strategy 的框架，用于解决 Lifelong Learning and Selective Forgetting (LSF) 问题，即模型在学习新任务的同时保留旧任务能力，并能选择性遗忘不想要的知识如隐私相关内容。具体方法通过对比策略使保留类的样本特征更紧凑，而删除类的样本特征则变得分散和不规则，从而让模型对删除类无规律响应，仿佛从未训练过。实验在四个基准数据集上进行，该方法实现了不同类的独立记忆和遗忘，并达到了新的 state-of-the-art 性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 5 figure",
      "pdf_url": "http://arxiv.org/pdf/2405.18663v1",
      "published_date": "2024-05-28 23:57:48 UTC",
      "updated_date": "2024-05-28 23:57:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:48:14.688998"
    },
    {
      "arxiv_id": "2405.18658v1",
      "title": "D-CoRP: Differentiable Connectivity Refinement for Functional Brain Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Haoyu Hu",
        "Hongrun Zhang",
        "Chao Li"
      ],
      "abstract": "Brain network is an important tool for understanding the brain, offering\ninsights for scientific research and clinical diagnosis. Existing models for\nbrain networks typically primarily focus on brain regions or overlook the\ncomplexity of brain connectivities. MRI-derived brain network data is commonly\nsusceptible to connectivity noise, underscoring the necessity of incorporating\nconnectivities into the modeling of brain networks. To address this gap, we\nintroduce a differentiable module for refining brain connectivity. We develop\nthe multivariate optimization based on information bottleneck theory to address\nthe complexity of the brain network and filter noisy or redundant connections.\nAlso, our method functions as a flexible plugin that is adaptable to most graph\nneural networks. Our extensive experimental results show that the proposed\nmethod can significantly improve the performance of various baseline models and\noutperform other state-of-the-art methods, indicating the effectiveness and\ngeneralizability of the proposed method in refining brain network connectivity.\nThe code will be released for public availability.",
      "tldr_zh": "该论文提出了 D-CoRP，一种可微的脑连接精炼模块，用于改进功能脑网络的建模，以应对 MRI 数据中连接噪声和复杂性问题。方法基于信息瓶颈理论的多变量优化，过滤噪声或冗余连接，并设计为灵活插件，可适应大多数 Graph Neural Networks。实验结果显示，D-CoRP 显著提升了各种基线模型的性能，并优于其他最先进方法，证明了其有效性和泛化性。",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18658v1",
      "published_date": "2024-05-28 23:49:52 UTC",
      "updated_date": "2024-05-28 23:49:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:48:36.592917"
    },
    {
      "arxiv_id": "2405.18655v1",
      "title": "CAVACHON: a hierarchical variational autoencoder to integrate multi-modal single-cell data",
      "title_zh": "翻译失败",
      "authors": [
        "Ping-Han Hsieh",
        "Ru-Xiu Hsiao",
        "Katalin Ferenc",
        "Anthony Mathelier",
        "Rebekka Burkholz",
        "Chien-Yu Chen",
        "Geir Kjetil Sandve",
        "Tatiana Belova",
        "Marieke Lydia Kuijjer"
      ],
      "abstract": "Paired single-cell sequencing technologies enable the simultaneous\nmeasurement of complementary modalities of molecular data at single-cell\nresolution. Along with the advances in these technologies, many methods based\non variational autoencoders have been developed to integrate these data.\nHowever, these methods do not explicitly incorporate prior biological\nrelationships between the data modalities, which could significantly enhance\nmodeling and interpretation. We propose a novel probabilistic learning\nframework that explicitly incorporates conditional independence relationships\nbetween multi-modal data as a directed acyclic graph using a generalized\nhierarchical variational autoencoder. We demonstrate the versatility of our\nframework across various applications pertinent to single-cell multi-omics data\nintegration. These include the isolation of common and distinct information\nfrom different modalities, modality-specific differential analysis, and\nintegrated cell clustering. We anticipate that the proposed framework can\nfacilitate the construction of highly flexible graphical models that can\ncapture the complexities of biological hypotheses and unravel the connections\nbetween different biological data types, such as different modalities of paired\nsingle-cell multi-omics data. The implementation of the proposed framework can\nbe found in the repository https://github.com/kuijjerlab/CAVACHON.",
      "tldr_zh": "本研究提出了一种名为CAVACHON的层次变分自编码器(hierarchical variational autoencoder)框架，用于整合多模态单细胞数据(multi-modal single-cell data)，通过明确纳入模态间的条件独立关系作为有向无环图(directed acyclic graph)，以提升建模和解释能力。不同于现有基于变分自编码器的方法，CAVACHON 显式考虑了生物学关系，从而实现对不同模态的共同和独特信息的隔离、模态特定的差异分析，以及整合的细胞聚类。实验结果证明，该框架在单细胞多组学数据整合的各种应用中表现出色，有望帮助构建灵活的图形模型，揭示不同生物数据类型之间的连接。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.GN"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18655v1",
      "published_date": "2024-05-28 23:44:09 UTC",
      "updated_date": "2024-05-28 23:44:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:48:40.851695"
    },
    {
      "arxiv_id": "2405.18650v1",
      "title": "Approximating Human Models During Argumentation-based Dialogues",
      "title_zh": "在基于论证的对话中逼近人类模型",
      "authors": [
        "Yinxu Tang",
        "Stylianos Loukas Vasileiou",
        "William Yeoh"
      ],
      "abstract": "Explainable AI Planning (XAIP) aims to develop AI agents that can effectively\nexplain their decisions and actions to human users, fostering trust and\nfacilitating human-AI collaboration. A key challenge in XAIP is model\nreconciliation, which seeks to align the mental models of AI agents and humans.\nWhile existing approaches often assume a known and deterministic human model,\nthis simplification may not capture the complexities and uncertainties of\nreal-world interactions. In this paper, we propose a novel framework that\nenables AI agents to learn and update a probabilistic human model through\nargumentation-based dialogues. Our approach incorporates trust-based and\ncertainty-based update mechanisms, allowing the agent to refine its\nunderstanding of the human's mental state based on the human's expressed trust\nin the agent's arguments and certainty in their own arguments. We employ a\nprobability weighting function inspired by prospect theory to capture the\nrelationship between trust and perceived probability, and use a Bayesian\napproach to update the agent's probability distribution over possible human\nmodels. We conduct a human-subject study to empirically evaluate the\neffectiveness of our approach in an argumentation scenario, demonstrating its\nability to capture the dynamics of human belief formation and adaptation.",
      "tldr_zh": "该论文针对可解释AI规划(XAIP)中的模型协调挑战，提出一个新框架，让AI代理通过argumentation-based dialogues学习和更新概率人类模型，以处理真实互动中的不确定性。该框架整合了trust-based和certainty-based更新机制，并运用灵感来源于prospect theory的概率加权函数以及Bayesian方法，来根据人类对AI论证的信任和自身论证的确定性动态调整AI对人类心理状态的理解。在人类受试者研究中，该方法证明了其在捕捉人类信念形成和适应动态方面的有效性，从而提升了AI与人类的协作信任。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18650v1",
      "published_date": "2024-05-28 23:22:18 UTC",
      "updated_date": "2024-05-28 23:22:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:48:49.478901"
    },
    {
      "arxiv_id": "2405.18649v2",
      "title": "LeDex: Training LLMs to Better Self-Debug and Explain Code",
      "title_zh": "LeDex：训练 LLMs 以更好地自我调试和解释代码",
      "authors": [
        "Nan Jiang",
        "Xiaopeng Li",
        "Shiqi Wang",
        "Qiang Zhou",
        "Soneya Binta Hossain",
        "Baishakhi Ray",
        "Varun Kumar",
        "Xiaofei Ma",
        "Anoop Deoras"
      ],
      "abstract": "In the domain of code generation, self-debugging is crucial. It allows LLMs\nto refine their generated code based on execution feedback. This is\nparticularly important because generating correct solutions in one attempt\nproves challenging for complex tasks. Prior works on self-debugging mostly\nfocus on prompting methods by providing LLMs with few-shot examples, which work\npoorly on small open-sourced LLMs. In this work, we propose LeDex, a training\nframework that significantly improves the self-debugging capability of LLMs.\nIntuitively, we observe that a chain of explanations on the wrong code followed\nby code refinement helps LLMs better analyze the wrong code and do refinement.\nWe thus propose an automated pipeline to collect a high-quality dataset for\ncode explanation and refinement by generating a number of explanations and\nrefinement trajectories from the LLM itself or a larger teacher model and\nfiltering via execution verification. We perform supervised fine-tuning (SFT)\nand further reinforcement learning (RL) on both success and failure\ntrajectories with a novel reward design considering code explanation and\nrefinement quality. SFT improves the pass@1 by up to 15.92% and pass@10 by\n9.30% over four benchmarks. RL training brings additional up to 3.54%\nimprovement on pass@1 and 2.55% improvement on pass@10. The trained LLMs show\niterative refinement ability and can keep refining code continuously. Lastly,\nour human evaluation shows that the LLMs trained with our framework generate\nmore useful code explanations and help developers better understand bugs in\nsource code.",
      "tldr_zh": "本研究提出LeDex框架，通过训练大型语言模型(LLMs)来提升其自我调试(Self-Debugging)和代码解释能力，以解决代码生成中难以一次性获得正确解决方案的问题。LeDex采用一个自动管道生成高质量数据集，包括从LLMs或更大教师模型中提取解释和改进轨迹，并通过执行验证进行过滤；随后进行监督微调(SFT)和强化学习(RL)，结合新颖的奖励设计来优化代码解释和改进质量。实验结果显示，SFT在四个基准上将pass@1提升高达15.92%和pass@10提升9.30%，而RL进一步提高pass@1至3.54%和pass@10至2.55%；此外，训练后的LLMs展现出迭代改进能力，并根据人类评估，能生成更有用的代码解释，帮助开发者更好地理解和修复错误。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CL",
      "comment": "This paper is accepted by The Thirty-eighth Annual Conference on\n  Neural Information Processing Systems (NeurIPS 2024)",
      "pdf_url": "http://arxiv.org/pdf/2405.18649v2",
      "published_date": "2024-05-28 23:20:24 UTC",
      "updated_date": "2025-02-13 23:32:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:49:03.326775"
    },
    {
      "arxiv_id": "2405.18642v1",
      "title": "JADS: A Framework for Self-supervised Joint Aspect Discovery and Summarization",
      "title_zh": "JADS：一种自监督联合方面发现",
      "authors": [
        "Xiaobo Guo",
        "Jay Desai",
        "Srinivasan H. Sengamedu"
      ],
      "abstract": "To generate summaries that include multiple aspects or topics for text\ndocuments, most approaches use clustering or topic modeling to group relevant\nsentences and then generate a summary for each group. These approaches struggle\nto optimize the summarization and clustering algorithms jointly. On the other\nhand, aspect-based summarization requires known aspects. Our solution\nintegrates topic discovery and summarization into a single step. Given text\ndata, our Joint Aspect Discovery and Summarization algorithm (JADS) discovers\naspects from the input and generates a summary of the topics, in one step. We\npropose a self-supervised framework that creates a labeled dataset by first\nmixing sentences from multiple documents (e.g., CNN/DailyMail articles) as the\ninput and then uses the article summaries from the mixture as the labels. The\nJADS model outperforms the two-step baselines. With pretraining, the model\nachieves better performance and stability. Furthermore, embeddings derived from\nJADS exhibit superior clustering capabilities. Our proposed method achieves\nhigher semantic alignment with ground truth and is factual.",
      "tldr_zh": "本论文提出 JADS 框架，这是一种自监督(self-supervised)方法，用于在单个步骤中联合进行主题发现(joint aspect discovery)和文本总结(summarization)，从而解决传统方法无法优化聚类和总结算法的问题。JADS 通过混合句子创建标签数据集（如从 CNN/DailyMail 文章中），并在训练中整合主题发现和总结过程，实现更高的效率和准确性。实验结果显示，JADS 模型比两步基线方法性能更优，特别是通过预训练后提升了稳定性；此外，其导出的嵌入表示在聚类(clustering)上表现出色，并实现了更高的语义对齐和事实性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "preprint",
      "pdf_url": "http://arxiv.org/pdf/2405.18642v1",
      "published_date": "2024-05-28 23:01:57 UTC",
      "updated_date": "2024-05-28 23:01:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:49:14.556643"
    },
    {
      "arxiv_id": "2405.18638v2",
      "title": "ConSiDERS-The-Human Evaluation Framework: Rethinking Human Evaluation for Generative Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Aparna Elangovan",
        "Ling Liu",
        "Lei Xu",
        "Sravan Bodapati",
        "Dan Roth"
      ],
      "abstract": "In this position paper, we argue that human evaluation of generative large\nlanguage models (LLMs) should be a multidisciplinary undertaking that draws\nupon insights from disciplines such as user experience research and human\nbehavioral psychology to ensure that the experimental design and results are\nreliable. The conclusions from these evaluations, thus, must consider factors\nsuch as usability, aesthetics, and cognitive biases. We highlight how cognitive\nbiases can conflate fluent information and truthfulness, and how cognitive\nuncertainty affects the reliability of rating scores such as Likert.\nFurthermore, the evaluation should differentiate the capabilities and\nweaknesses of increasingly powerful large language models -- which requires\neffective test sets. The scalability of human evaluation is also crucial to\nwider adoption. Hence, to design an effective human evaluation system in the\nage of generative NLP, we propose the ConSiDERS-The-Human evaluation framework\nconsisting of 6 pillars -- Consistency, Scoring Criteria, Differentiating, User\nExperience, Responsible, and Scalability.",
      "tldr_zh": "本论文主张，针对生成式大型语言模型（LLMs）的人类评估应采用多学科方法，借鉴用户体验研究和人类行为心理学，以提升实验设计和结果的可靠性，并考虑可用性、美学以及认知偏差等因素。论文强调，认知偏差可能导致流畅信息与真实性混淆，且认知不确定性会影响 Likert 等评分系统的可靠性，因此评估需设计有效的测试集来区分不同 LLMs 的能力和弱点。作者提出 ConSiDERS-The-Human 评估框架，该框架由六个支柱组成——Consistency（一致性）、Scoring Criteria（评分标准）、Differentiating（区分）、User Experience（用户体验）、Responsible（负责任）和 Scalability（可扩展性），旨在提升评估的可扩展性和整体有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted in ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.18638v2",
      "published_date": "2024-05-28 22:45:28 UTC",
      "updated_date": "2024-08-31 05:17:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:49:26.226739"
    },
    {
      "arxiv_id": "2405.18636v1",
      "title": "ChatGPT as the Marketplace of Ideas: Should Truth-Seeking Be the Goal of AI Content Governance?",
      "title_zh": "ChatGPT 作为思想市场：寻求真理是否应成为 AI 内容治理的目标？",
      "authors": [
        "Jiawei Zhang"
      ],
      "abstract": "As one of the most enduring metaphors within legal discourse, the marketplace\nof ideas has wielded considerable influence over the jurisprudential landscape\nfor decades. A century after the inception of this theory, ChatGPT emerged as a\nrevolutionary technological advancement in the twenty-first century. This\nresearch finds that ChatGPT effectively manifests the marketplace metaphor. It\nnot only instantiates the promises envisaged by generations of legal scholars\nbut also lays bare the perils discerned through sustained academic critique.\nSpecifically, the workings of ChatGPT and the marketplace of ideas theory\nexhibit at least four common features: arena, means, objectives, and flaws.\nThese shared attributes are sufficient to render ChatGPT historically the most\nqualified engine for actualizing the marketplace of ideas theory.\n  The comparison of the marketplace theory and ChatGPT merely marks a starting\npoint. A more meaningful undertaking entails reevaluating and reframing both\ninternal and external AI policies by referring to the accumulated experience,\ninsights, and suggestions researchers have raised to fix the marketplace\ntheory. Here, a pivotal issue is: should truth-seeking be set as the goal of AI\ncontent governance? Given the unattainability of the absolute truth-seeking\ngoal, I argue against adopting zero-risk policies. Instead, a more judicious\napproach would be to embrace a knowledge-based alternative wherein large\nlanguage models (LLMs) are trained to generate competing and divergent\nviewpoints based on sufficient justifications. This research also argues that\nso-called AI content risks are not created by AI companies but are inherent in\nthe entire information ecosystem. Thus, the burden of managing these risks\nshould be distributed among different social actors, rather than being solely\nshouldered by chatbot companies.",
      "tldr_zh": "本研究将 ChatGPT 视为 \"marketplace of ideas\" 理论的现代体现，分析其在 arena（场所）、means（手段）、objectives（目标）和 flaws（缺陷）等方面的共同特征，认定 ChatGPT 是实现这一理论的最合格引擎。作者通过比较理论与实际应用，质疑是否应将 truth-seeking（真理求索）设定为 AI content governance（AI 内容治理）的核心目标，认为追求绝对真理不可达，并反对采用零风险政策。取而代之，论文提出一个 knowledge-based（基于知识）的替代方案，即训练 large language models (LLMs) 生成竞争性和多样观点，并提供充分理由。最后，研究强调 AI 内容风险是信息生态系统的固有问题，应由社会各方分担，而非仅由 AI 公司独自承担。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.ET",
        "cs.IT",
        "math.IT",
        "I.2.4"
      ],
      "primary_category": "cs.AI",
      "comment": "27 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.18636v1",
      "published_date": "2024-05-28 22:38:24 UTC",
      "updated_date": "2024-05-28 22:38:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:49:38.927033"
    },
    {
      "arxiv_id": "2405.19376v2",
      "title": "PureEBM: Universal Poison Purification via Mid-Run Dynamics of Energy-Based Models",
      "title_zh": "PureEBM：通过能量模型中间运行动态实现的通用毒化净化",
      "authors": [
        "Omead Pooladzandi",
        "Jeffrey Jiang",
        "Sunay Bhat",
        "Gregory Pottie"
      ],
      "abstract": "Data poisoning attacks pose a significant threat to the integrity of machine\nlearning models by leading to misclassification of target distribution data by\ninjecting adversarial examples during training. Existing state-of-the-art\n(SoTA) defense methods suffer from limitations, such as significantly reduced\ngeneralization performance and significant overhead during training, making\nthem impractical or limited for real-world applications. In response to this\nchallenge, we introduce a universal data purification method that defends\nnaturally trained classifiers from malicious white-, gray-, and black-box image\npoisons by applying a universal stochastic preprocessing step $\\Psi_{T}(x)$,\nrealized by iterative Langevin sampling of a convergent Energy Based Model\n(EBM) initialized with an image $x.$ Mid-run dynamics of $\\Psi_{T}(x)$ purify\npoison information with minimal impact on features important to the\ngeneralization of a classifier network. We show that EBMs remain universal\npurifiers, even in the presence of poisoned EBM training data, and achieve SoTA\ndefense on leading triggered and triggerless poisons. This work is a subset of\na larger framework introduced in \\pgen with a more detailed focus on EBM\npurification and poison defense.",
      "tldr_zh": "本论文提出 PureEBM，一种通用数据净化方法，利用 Energy-Based Models (EBM) 的中途动态来防御数据投毒攻击，包括白盒、灰盒和黑盒图像投毒。方法通过一个随机预处理步骤 Ψ_T(x)，采用 EBM 的迭代 Langevin 采样，从原始图像初始化，以清除投毒信息，同时最小化对分类器网络泛化性能的影响。实验结果显示，即使在投毒训练数据存在的情况下，PureEBM 仍能实现 SOTA 防御效果，在领先的触发型和无触发型投毒攻击上表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2405.18627",
      "pdf_url": "http://arxiv.org/pdf/2405.19376v2",
      "published_date": "2024-05-28 22:31:56 UTC",
      "updated_date": "2024-06-02 20:21:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:49:50.413735"
    },
    {
      "arxiv_id": "2405.18632v1",
      "title": "Large Language Models as Partners in Student Essay Evaluation",
      "title_zh": "大型语言模型作为学生作文评估的合作伙伴",
      "authors": [
        "Toru Ishida",
        "Tongxi Liu",
        "Hailong Wang",
        "William K. Cheung"
      ],
      "abstract": "As the importance of comprehensive evaluation in workshop courses increases,\nthere is a growing demand for efficient and fair assessment methods that reduce\nthe workload for faculty members. This paper presents an evaluation conducted\nwith Large Language Models (LLMs) using actual student essays in three\nscenarios: 1) without providing guidance such as rubrics, 2) with pre-specified\nrubrics, and 3) through pairwise comparison of essays. Quantitative analysis of\nthe results revealed a strong correlation between LLM and faculty member\nassessments in the pairwise comparison scenario with pre-specified rubrics,\nalthough concerns about the quality and stability of evaluations remained.\nTherefore, we conducted a qualitative analysis of LLM assessment comments,\nshowing that: 1) LLMs can match the assessment capabilities of faculty members,\n2) variations in LLM assessments should be interpreted as diversity rather than\nconfusion, and 3) assessments by humans and LLMs can differ and complement each\nother. In conclusion, this paper suggests that LLMs should not be seen merely\nas assistants to faculty members but as partners in evaluation committees and\noutlines directions for further research.",
      "tldr_zh": "本研究探讨了 Large Language Models (LLMs) 在学生论文评估中的作用，通过三种场景（无指导、预指定 rubrics 和 pairwise comparison）对实际学生论文进行评估，以减轻教员工作量。结果显示，在预指定 rubrics 的 pairwise comparison 场景中，LLMs 与教员评估之间存在强相关性，但评估质量和稳定性仍有担忧；定性分析进一步表明，LLMs 可匹敌教员能力，其评估变异应视为多样性，且人类与 LLMs 评估能互补。论文结论建议将 LLMs 视为教员的伙伴而非单纯助手，并为未来研究提供方向。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18632v1",
      "published_date": "2024-05-28 22:28:50 UTC",
      "updated_date": "2024-05-28 22:28:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:50:02.324201"
    },
    {
      "arxiv_id": "2405.18627v2",
      "title": "PureGen: Universal Data Purification for Train-Time Poison Defense via Generative Model Dynamics",
      "title_zh": "翻译失败",
      "authors": [
        "Sunay Bhat",
        "Jeffrey Jiang",
        "Omead Pooladzandi",
        "Alexander Branch",
        "Gregory Pottie"
      ],
      "abstract": "Train-time data poisoning attacks threaten machine learning models by\nintroducing adversarial examples during training, leading to misclassification.\nCurrent defense methods often reduce generalization performance, are\nattack-specific, and impose significant training overhead. To address this, we\nintroduce a set of universal data purification methods using a stochastic\ntransform, $\\Psi(x)$, realized via iterative Langevin dynamics of Energy-Based\nModels (EBMs), Denoising Diffusion Probabilistic Models (DDPMs), or both. These\napproaches purify poisoned data with minimal impact on classifier\ngeneralization. Our specially trained EBMs and DDPMs provide state-of-the-art\ndefense against various attacks (including Narcissus, Bullseye Polytope,\nGradient Matching) on CIFAR-10, Tiny-ImageNet, and CINIC-10, without needing\nattack or classifier-specific information. We discuss performance trade-offs\nand show that our methods remain highly effective even with poisoned or\ndistributionally shifted generative model training data.",
      "tldr_zh": "这篇论文提出了PureGen，一种通用的数据净化方法，用于防御训练时数据中毒攻击，该方法通过随机变换$\\Psi(x)$实现，利用Energy-Based Models (EBMs)、Denoising Diffusion Probabilistic Models (DDPMs)或两者的迭代Langevin dynamics来净化中毒数据，同时最小化对分类器泛化性能的影响。PureGen不依赖特定攻击或分类器信息，在CIFAR-10、Tiny-ImageNet和CINIC-10数据集上，对多种攻击（如Narcissus、Bullseye Polytope、Gradient Matching）提供了最先进的防御表现。实验结果显示，即使生成模型的训练数据被中毒或出现分布偏移，该方法仍保持高度有效，并讨论了性能权衡。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18627v2",
      "published_date": "2024-05-28 22:19:26 UTC",
      "updated_date": "2024-06-02 20:11:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:50:16.572211"
    },
    {
      "arxiv_id": "2405.18626v2",
      "title": "Causal Contextual Bandits with Adaptive Context",
      "title_zh": "翻译失败",
      "authors": [
        "Rahul Madhavan",
        "Aurghya Maiti",
        "Gaurav Sinha",
        "Siddharth Barman"
      ],
      "abstract": "We study a variant of causal contextual bandits where the context is chosen\nbased on an initial intervention chosen by the learner. At the beginning of\neach round, the learner selects an initial action, depending on which a\nstochastic context is revealed by the environment. Following this, the learner\nthen selects a final action and receives a reward. Given $T$ rounds of\ninteractions with the environment, the objective of the learner is to learn a\npolicy (of selecting the initial and the final action) with maximum expected\nreward. In this paper we study the specific situation where every action\ncorresponds to intervening on a node in some known causal graph. We extend\nprior work from the deterministic context setting to obtain simple regret\nminimization guarantees. This is achieved through an instance-dependent causal\nparameter, $\\lambda$, which characterizes our upper bound. Furthermore, we\nprove that our simple regret is essentially tight for a large class of\ninstances. A key feature of our work is that we use convex optimization to\naddress the bandit exploration problem. We also conduct experiments to validate\nour theoretical results, and release our code at our project GitHub repository:\nhttps://github.com/adaptiveContextualCausalBandits/aCCB.",
      "tldr_zh": "本研究探讨了Causal Contextual Bandits with Adaptive Context的变体，其中上下文基于学习者选择的初始干预动态生成。随后，学习者选择最终动作以最大化预期奖励。论文扩展了先前提出的确定性上下文设置，通过引入因果参数λ并采用凸优化来实现简单遗憾最小化，并证明了其遗憾保证在大量实例中本质上紧致。实验验证了这些理论结果，并开源了代码以供进一步研究。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Reinforcement Learning Conference (RLC) 2024, 10 pages (31 pages\n  including appendix), 8 plots. arXiv admin note: text overlap with\n  arXiv:2111.00886",
      "pdf_url": "http://arxiv.org/pdf/2405.18626v2",
      "published_date": "2024-05-28 22:17:57 UTC",
      "updated_date": "2024-06-02 13:54:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:50:27.690049"
    },
    {
      "arxiv_id": "2405.18624v1",
      "title": "Enhancing IoT Security with CNN and LSTM-Based Intrusion Detection Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Afrah Gueriani",
        "Hamza Kheddar",
        "Ahmed Cherif Mazari"
      ],
      "abstract": "Protecting Internet of things (IoT) devices against cyber attacks is\nimperative owing to inherent security vulnerabilities. These vulnerabilities\ncan include a spectrum of sophisticated attacks that pose significant damage to\nboth individuals and organizations. Employing robust security measures like\nintrusion detection systems (IDSs) is essential to solve these problems and\nprotect IoT systems from such attacks. In this context, our proposed IDS model\nconsists on a combination of convolutional neural network (CNN) and long\nshort-term memory (LSTM) deep learning (DL) models. This fusion facilitates the\ndetection and classification of IoT traffic into binary categories, benign and\nmalicious activities by leveraging the spatial feature extraction capabilities\nof CNN for pattern recognition and the sequential memory retention of LSTM for\ndiscerning complex temporal dependencies in achieving enhanced accuracy and\nefficiency. In assessing the performance of our proposed model, the authors\nemployed the new CICIoT2023 dataset for both training and final testing, while\nfurther validating the model's performance through a conclusive testing phase\nutilizing the CICIDS2017 dataset. Our proposed model achieves an accuracy rate\nof 98.42%, accompanied by a minimal loss of 0.0275. False positive rate(FPR) is\nequally important, reaching 9.17% with an F1-score of 98.57%. These results\ndemonstrate the effectiveness of our proposed CNN-LSTM IDS model in fortifying\nIoT environments against potential cyber threats.",
      "tldr_zh": "该研究针对物联网（IoT）设备的固有安全漏洞和网络攻击风险，提出了一种基于卷积神经网络（CNN）和长短期记忆网络（LSTM）的入侵检测系统（IDS）模型。模型利用 CNN 的空间特征提取能力进行模式识别，以及 LSTM 的序列记忆功能来捕捉时间依赖关系，从而实现对 IoT 流量的二元分类（良性或恶意）。在 CICIoT2023 和 CICIDS2017 数据集上的实验中，该模型达到了 98.42% 的准确率、98.57% 的 F1 分数，以及仅 9.17% 的假阳性率（FPR）和 0.0275 的损失，证明了其在提升 IoT 安全方面的有效性和效率。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18624v1",
      "published_date": "2024-05-28 22:12:15 UTC",
      "updated_date": "2024-05-28 22:12:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:50:38.302482"
    },
    {
      "arxiv_id": "2405.18620v2",
      "title": "RealitySummary: Exploring On-Demand Mixed Reality Text Summarization and Question Answering using Large Language Models",
      "title_zh": "RealitySummary: 探索使用大语言模型的按需混合现实文本摘要和问答系统",
      "authors": [
        "Aditya Gunturu",
        "Shivesh Jadon",
        "Nandi Zhang",
        "Morteza Faraji",
        "Jarin Thundathil",
        "Tafreed Ahmad",
        "Wesley Willett",
        "Ryo Suzuki"
      ],
      "abstract": "Large Language Models (LLMs) are gaining popularity as tools for reading and\nsummarization aids. However, little is known about their potential benefits\nwhen integrated with mixed reality (MR) interfaces to support everyday reading\nassistants. We developed RealitySummary, an MR reading assistant that\nseamlessly integrates LLMs with always-on camera access, OCR-based text\nextraction, and augmented spatial and visual responses in MR interfaces.\nDeveloped iteratively, RealitySummary evolved across three versions, each\nshaped by user feedback and reflective analysis: 1) a preliminary user study to\nunderstand user perceptions (N=12), 2) an in-the-wild deployment to explore\nreal-world usage (N=11), and 3) a diary study to capture insights from\nreal-world work contexts (N=5). Our findings highlight the unique advantages of\ncombining AI and MR, including an always-on implicit assistant, minimal context\nswitching, and spatial affordances, demonstrating significant potential for\nfuture LLM-MR interfaces beyond traditional screen-based interactions.",
      "tldr_zh": "该研究探索了将大型语言模型（LLMs）整合到混合现实（MR）接口中，作为日常阅读辅助工具，开发了RealitySummary系统。该系统利用始终开启的相机访问、OCR-based文本提取以及增强的空间和视觉响应，提供即时文本总结和问答功能。通过三个迭代版本的开发，包括初步用户研究（N=12）、野外部署（N=11）和日记研究（N=5），系统基于用户反馈不断优化。研究发现，AI与MR的结合带来独特优势，如始终在线的隐式助手、最小上下文切换和空间交互潜力，展示了LLM-MR接口超越传统屏幕交互的未来前景。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18620v2",
      "published_date": "2024-05-28 21:59:56 UTC",
      "updated_date": "2024-09-20 18:57:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:50:50.506363"
    },
    {
      "arxiv_id": "2407.13070v2",
      "title": "The Cost of Arbitrariness for Individuals: Examining the Legal and Technical Challenges of Model Multiplicity",
      "title_zh": "翻译失败",
      "authors": [
        "Prakhar Ganesh",
        "Ihsan Ibrahim Daldaban",
        "Ignacio Cofone",
        "Golnoosh Farnadi"
      ],
      "abstract": "Model multiplicity, the phenomenon where multiple models achieve similar\nperformance despite different underlying learned functions, introduces\narbitrariness in model selection. While this arbitrariness may seem\ninconsequential in expectation, its impact on individuals can be severe. This\npaper explores various individual concerns stemming from multiplicity,\nincluding the effects of arbitrariness beyond final predictions, disparate\narbitrariness for individuals belonging to protected groups, and the challenges\nassociated with the arbitrariness of a single algorithmic system creating a\nmonopoly across various contexts. It provides both an empirical examination of\nthese concerns and a comprehensive analysis from the legal standpoint,\naddressing how these issues are perceived in the anti-discrimination law in\nCanada. We conclude the discussion with technical challenges in the current\nlandscape of model multiplicity to meet legal requirements and the legal gap\nbetween current law and the implications of arbitrariness in model selection,\nhighlighting relevant future research directions for both disciplines.",
      "tldr_zh": "该论文探讨了模型多重性（model multiplicity）导致的模型选择任意性（arbitrariness）对个体的潜在负面影响，包括超出最终预测的后果、对受保护群体的不均等影响，以及单一算法系统在不同情境中的垄断挑战。研究通过实证检查和法律分析，聚焦于加拿大的反歧视法，揭示了这些问题在法律框架下的认知和难题。最终，论文强调了当前技术挑战在满足法律要求方面的不足，以及两者之间的差距，并提出了跨学科的未来研究方向。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "Current version of the paper contains errors in the attribution of\n  previous work. We are working on creating a new version, which can take a\n  while and thus are withdrawing this version in the meantime",
      "pdf_url": "http://arxiv.org/pdf/2407.13070v2",
      "published_date": "2024-05-28 21:54:03 UTC",
      "updated_date": "2024-09-13 09:33:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:51:02.177058"
    },
    {
      "arxiv_id": "2405.18610v1",
      "title": "DTR-Bench: An in silico Environment and Benchmark Platform for Reinforcement Learning Based Dynamic Treatment Regime",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiyao Luo",
        "Mingcheng Zhu",
        "Fenglin Liu",
        "Jiali Li",
        "Yangchen Pan",
        "Jiandong Zhou",
        "Tingting Zhu"
      ],
      "abstract": "Reinforcement learning (RL) has garnered increasing recognition for its\npotential to optimise dynamic treatment regimes (DTRs) in personalised\nmedicine, particularly for drug dosage prescriptions and medication\nrecommendations. However, a significant challenge persists: the absence of a\nunified framework for simulating diverse healthcare scenarios and a\ncomprehensive analysis to benchmark the effectiveness of RL algorithms within\nthese contexts. To address this gap, we introduce \\textit{DTR-Bench}, a\nbenchmarking platform comprising four distinct simulation environments tailored\nto common DTR applications, including cancer chemotherapy, radiotherapy,\nglucose management in diabetes, and sepsis treatment. We evaluate various\nstate-of-the-art RL algorithms across these settings, particularly highlighting\ntheir performance amidst real-world challenges such as\npharmacokinetic/pharmacodynamic (PK/PD) variability, noise, and missing data.\nOur experiments reveal varying degrees of performance degradation among RL\nalgorithms in the presence of noise and patient variability, with some\nalgorithms failing to converge. Additionally, we observe that using temporal\nobservation representations does not consistently lead to improved performance\nin DTR settings. Our findings underscore the necessity of developing robust,\nadaptive RL algorithms capable of effectively managing these complexities to\nenhance patient-specific healthcare. We have open-sourced our benchmark and\ncode at https://github.com/GilesLuo/DTR-Bench.",
      "tldr_zh": "本文引入DTR-Bench，一个in silico模拟环境和基准平台，用于评估强化学习（RL）在动态治疗方案（DTRs）中的应用效果。该平台包含四个场景：癌症化疗、放疗、糖尿病血糖管理和败血症治疗，并测试了多种最先进RL算法在PK/PD变异性、噪声和缺失数据等真实挑战下的性能。实验发现，RL算法在噪声和患者变异性下表现出不同程度的性能下降，有些算法无法收敛，且使用时间观察表示并不一致地改善结果。这些发现强调了开发更鲁棒、适应性的RL算法以提升个性化医疗的必要性，并已开源相关代码。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages for main content",
      "pdf_url": "http://arxiv.org/pdf/2405.18610v1",
      "published_date": "2024-05-28 21:40:00 UTC",
      "updated_date": "2024-05-28 21:40:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:51:18.942289"
    },
    {
      "arxiv_id": "2405.18602v2",
      "title": "SST-GCN: The Sequential based Spatio-Temporal Graph Convolutional networks for Minute-level and Road-level Traffic Accident Risk Prediction",
      "title_zh": "SST-GCN：",
      "authors": [
        "Tae-wook Kim",
        "Han-jin Lee",
        "Hyeon-Jin Jung",
        "Ji-Woong Yang",
        "Ellen J. Hong"
      ],
      "abstract": "Traffic accidents are recognized as a major social issue worldwide, causing\nnumerous injuries and significant costs annually. Consequently, methods for\npredicting and preventing traffic accidents have been researched for many\nyears. With advancements in the field of artificial intelligence, various\nstudies have applied Machine Learning and Deep Learning techniques to traffic\naccident prediction. Modern traffic conditions change rapidly by the minute,\nand these changes vary significantly across different roads. In other words,\nthe risk of traffic accidents changes minute by minute in various patterns for\neach road. Therefore, it is desirable to predict traffic accident risk at the\nMinute-Level and Road-Level. However, because roads have close and complex\nrelationships with adjacent roads, research on predicting traffic accidents at\nthe Minute-Level and Road-Level is challenging. Thus, it is essential to build\na model that can reflect the spatial and temporal characteristics of roads for\ntraffic accident prediction. Consequently, recent attempts have been made to\nuse Graph Convolutional Networks to capture the spatial characteristics of\nroads and Recurrent Neural Networks to capture their temporal characteristics\nfor predicting traffic accident risk. This paper proposes the Sequential based\nSpatio-Temporal Graph Convolutional Networks (SST-GCN), which combines GCN and\nLSTM, to predict traffic accidents at the Minute-Level and Road-Level using a\nroad dataset constructed in Seoul, the capital of South Korea. Experiments have\ndemonstrated that SST-GCN outperforms other state-of-the-art models in\nMinute-Level predictions.",
      "tldr_zh": "该研究针对交通事故的分钟级和道路级风险预测问题，提出了一种基于序列的时空图卷积网络（SST-GCN），它结合了Graph Convolutional Networks (GCN)来捕捉道路的空间特性，以及Long Short-Term Memory (LSTM)来处理时间序列数据。模型使用韩国首尔道路数据集，旨在反映道路间复杂关系并实现精确预测。与现有模型相比，SST-GCN在分钟级预测中表现出色，性能优于最先进的方法。该框架为实时交通事故预防提供了可靠的技术支持。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18602v2",
      "published_date": "2024-05-28 21:33:18 UTC",
      "updated_date": "2024-06-03 08:44:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:51:26.161842"
    },
    {
      "arxiv_id": "2405.18581v1",
      "title": "Unleashing the Potential of Text-attributed Graphs: Automatic Relation Decomposition via Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Hyunjin Seo",
        "Taewon Kim",
        "June Yong Yang",
        "Eunho Yang"
      ],
      "abstract": "Recent advancements in text-attributed graphs (TAGs) have significantly\nimproved the quality of node features by using the textual modeling\ncapabilities of language models. Despite this success, utilizing text\nattributes to enhance the predefined graph structure remains largely\nunexplored. Our extensive analysis reveals that conventional edges on TAGs,\ntreated as a single relation (e.g., hyperlinks) in previous literature,\nactually encompass mixed semantics (e.g., \"advised by\" and \"participates in\").\nThis simplification hinders the representation learning process of Graph Neural\nNetworks (GNNs) on downstream tasks, even when integrated with advanced node\nfeatures. In contrast, we discover that decomposing these edges into distinct\nsemantic relations significantly enhances the performance of GNNs. Despite\nthis, manually identifying and labeling of edges to corresponding semantic\nrelations is labor-intensive, often requiring domain expertise. To this end, we\nintroduce RoSE (Relation-oriented Semantic Edge-decomposition), a novel\nframework that leverages the capability of Large Language Models (LLMs) to\ndecompose the graph structure by analyzing raw text attributes - in a fully\nautomated manner. RoSE operates in two stages: (1) identifying meaningful\nrelations using an LLM-based generator and discriminator, and (2) categorizing\neach edge into corresponding relations by analyzing textual contents associated\nwith connected nodes via an LLM-based decomposer. Extensive experiments\ndemonstrate that our model-agnostic framework significantly enhances node\nclassification performance across various datasets, with improvements of up to\n16% on the Wisconsin dataset.",
      "tldr_zh": "该研究揭示了文本属性图（Text-attributed Graphs, TAGs）中，传统边（如超链接）包含多种语义（如“advised by”），这会阻碍 Graph Neural Networks (GNNs) 在下游任务中的表现。针对此问题，作者提出 RoSE 框架，利用 Large Language Models (LLMs) 自动分解图结构：首先通过 LLM 生成器和鉴别器识别有意义的 relation，然后分析节点文本内容将边分类到对应 relation。实验结果显示，RoSE 在多个数据集上显著提升节点分类性能，在 Wisconsin 数据集上最高改善 16%。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18581v1",
      "published_date": "2024-05-28 20:54:47 UTC",
      "updated_date": "2024-05-28 20:54:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:51:49.835685"
    },
    {
      "arxiv_id": "2405.18580v3",
      "title": "Artificial Intelligence in Industry 4.0: A Review of Integration Challenges for Industrial Systems",
      "title_zh": "工业4.0中的人工智能：工业系统集成挑战的综述",
      "authors": [
        "Alexander Windmann",
        "Philipp Wittenberg",
        "Marvin Schieseck",
        "Oliver Niggemann"
      ],
      "abstract": "In Industry 4.0, Cyber-Physical Systems (CPS) generate vast data sets that\ncan be leveraged by Artificial Intelligence (AI) for applications including\npredictive maintenance and production planning. However, despite the\ndemonstrated potential of AI, its widespread adoption in sectors like\nmanufacturing remains limited. Our comprehensive review of recent literature,\nincluding standards and reports, pinpoints key challenges: system integration,\ndata-related issues, managing workforce-related concerns and ensuring\ntrustworthy AI. A quantitative analysis highlights particular challenges and\ntopics that are important for practitioners but still need to be sufficiently\ninvestigated by academics. The paper briefly discusses existing solutions to\nthese challenges and proposes avenues for future research. We hope that this\nsurvey serves as a resource for practitioners evaluating the cost-benefit\nimplications of AI in CPS and for researchers aiming to address these urgent\nchallenges.",
      "tldr_zh": "本综述探讨了人工智能（AI）在 Industry 4.0 中的整合挑战，特别是如何利用 Cyber-Physical Systems (CPS) 生成的大数据进行预测维护和生产规划，但其在制造业的广泛采用仍受限。论文通过回顾最近文献、标准和报告，进行定量分析，识别了关键问题，包括系统集成、数据相关问题、工作力管理以及确保可信 AI。研究讨论了现有解决方案，并提出未来研究方向，以帮助从业者评估 AI 在 CPS 中的成本效益，并指导学术界解决这些紧迫挑战。该工作为推动 AI 在工业系统的可靠应用提供了宝贵资源。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "I.2.1"
      ],
      "primary_category": "cs.AI",
      "comment": "17 pages, 4 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2405.18580v3",
      "published_date": "2024-05-28 20:54:41 UTC",
      "updated_date": "2024-12-17 07:35:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:51:50.547240"
    },
    {
      "arxiv_id": "2405.18572v1",
      "title": "Low-rank finetuning for LLMs: A fairness perspective",
      "title_zh": "翻译失败",
      "authors": [
        "Saswat Das",
        "Marco Romanelli",
        "Cuong Tran",
        "Zarreen Reza",
        "Bhavya Kailkhura",
        "Ferdinando Fioretto"
      ],
      "abstract": "Low-rank approximation techniques have become the de facto standard for\nfine-tuning Large Language Models (LLMs) due to their reduced computational and\nmemory requirements. This paper investigates the effectiveness of these methods\nin capturing the shift of fine-tuning datasets from the initial pre-trained\ndata distribution. Our findings reveal that there are cases in which low-rank\nfine-tuning falls short in learning such shifts. This, in turn, produces\nnon-negligible side effects, especially when fine-tuning is adopted for\ntoxicity mitigation in pre-trained models, or in scenarios where it is\nimportant to provide fair models. Through comprehensive empirical evidence on\nseveral models, datasets, and tasks, we show that low-rank fine-tuning\ninadvertently preserves undesirable biases and toxic behaviors. We also show\nthat this extends to sequential decision-making tasks, emphasizing the need for\ncareful evaluation to promote responsible LLMs development.",
      "tldr_zh": "本研究从公平性角度探讨了低秩逼近技术在微调大型语言模型（LLMs）中的应用，强调这些方法可能无法有效捕捉微调数据集与预训练数据分布的差异。研究发现，低秩微调在某些情况下会保留不想要的偏差和毒性行为，尤其在毒性缓解任务中，导致模型公平性受损。通过多项模型、数据集和任务的实证证据，证明了这种问题在顺序决策任务中同样存在。作者呼吁进行谨慎评估，以促进负责任的LLMs开发。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18572v1",
      "published_date": "2024-05-28 20:43:53 UTC",
      "updated_date": "2024-05-28 20:43:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:52:02.237213"
    },
    {
      "arxiv_id": "2405.18560v4",
      "title": "Potential Field Based Deep Metric Learning",
      "title_zh": "基于势场的深度度量学习",
      "authors": [
        "Shubhang Bhatnagar",
        "Narendra Ahuja"
      ],
      "abstract": "Deep metric learning (DML) involves training a network to learn a\nsemantically meaningful representation space. Many current approaches mine\nn-tuples of examples and model interactions within each tuplets. We present a\nnovel, compositional DML model that instead of in tuples, represents the\ninfluence of each example (embedding) by a continuous potential field, and\nsuperposes the fields to obtain their combined global potential field. We use\nattractive/repulsive potential fields to represent interactions among\nembeddings from images of the same/different classes. Contrary to typical\nlearning methods, where mutual influence of samples is proportional to their\ndistance, we enforce reduction in such influence with distance, leading to a\ndecaying field. We show that such decay helps improve performance on real world\ndatasets with large intra-class variations and label noise. Like other\nproxy-based methods, we also use proxies to succinctly represent\nsub-populations of examples. We evaluate our method on three standard DML\nbenchmarks- Cars-196, CUB-200-2011, and SOP datasets where it outperforms\nstate-of-the-art baselines.",
      "tldr_zh": "本论文提出了一种基于潜在场（potential field）的深度度量学习（Deep Metric Learning, DML）方法，通过使用连续潜在场表示每个嵌入（embedding）的影響，并通过叠加这些场来建模全局交互，从而取代传统的元组（tuples）挖掘方式。该方法采用吸引/排斥潜在场来处理同类/异类图像的交互，并引入距离衰减机制，以减少样本间影响随距离减小，提升在具有大类内变异和标签噪声的真实数据集上的性能。实验结果显示，该方法在 Cars-196、CUB-200-2011 和 SOP 数据集上优于现有基线，同时利用代理（proxies）来高效代表示例子集。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2405.18560v4",
      "published_date": "2024-05-28 20:10:06 UTC",
      "updated_date": "2025-04-19 09:27:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:52:15.120821"
    },
    {
      "arxiv_id": "2405.18556v2",
      "title": "Reinforcement Learning in Dynamic Treatment Regimes Needs Critical Reexamination",
      "title_zh": "动态治疗方案中的强化学习需要批判性重新审视",
      "authors": [
        "Zhiyao Luo",
        "Yangchen Pan",
        "Peter Watkinson",
        "Tingting Zhu"
      ],
      "abstract": "In the rapidly changing healthcare landscape, the implementation of offline\nreinforcement learning (RL) in dynamic treatment regimes (DTRs) presents a mix\nof unprecedented opportunities and challenges. This position paper offers a\ncritical examination of the current status of offline RL in the context of\nDTRs. We argue for a reassessment of applying RL in DTRs, citing concerns such\nas inconsistent and potentially inconclusive evaluation metrics, the absence of\nnaive and supervised learning baselines, and the diverse choice of RL\nformulation in existing research. Through a case study with more than 17,000\nevaluation experiments using a publicly available Sepsis dataset, we\ndemonstrate that the performance of RL algorithms can significantly vary with\nchanges in evaluation metrics and Markov Decision Process (MDP) formulations.\nSurprisingly, it is observed that in some instances, RL algorithms can be\nsurpassed by random baselines subjected to policy evaluation methods and reward\ndesign. This calls for more careful policy evaluation and algorithm development\nin future DTR works. Additionally, we discussed potential enhancements toward\nmore reliable development of RL-based dynamic treatment regimes and invited\nfurther discussion within the community. Code is available at\nhttps://github.com/GilesLuo/ReassessDTR.",
      "tldr_zh": "这篇论文对强化学习 (RL) 在动态治疗方案 (DTRs) 中的应用进行了批判性审视，指出当前存在的问题，如评估指标不一致、缺乏监督学习基准，以及RL模型选择的多样性。作者通过一个基于公开Sepsis数据集的案例研究，进行了超过17,000次实验，证明RL算法的性能会因评估指标和Markov Decision Process (MDP) 公式变化而显著波动，甚至在某些情况下被随机基准超越。论文呼吁未来DTR研究中加强政策评估和算法开发，并讨论了潜在改进方向，以推动更可靠的RL应用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ICML 2024. 9 pages for main content, 34 pages in total",
      "pdf_url": "http://arxiv.org/pdf/2405.18556v2",
      "published_date": "2024-05-28 20:03:18 UTC",
      "updated_date": "2024-06-03 20:16:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:52:37.171589"
    },
    {
      "arxiv_id": "2405.18553v4",
      "title": "FAIIR: Building Toward A Conversational AI Agent Assistant for Youth Mental Health Service Provision",
      "title_zh": "翻译失败",
      "authors": [
        "Stephen Obadinma",
        "Alia Lachana",
        "Maia Norman",
        "Jocelyn Rankin",
        "Joanna Yu",
        "Xiaodan Zhu",
        "Darren Mastropaolo",
        "Deval Pandya",
        "Roxana Sultan",
        "Elham Dolatabadi"
      ],
      "abstract": "The world's healthcare systems and mental health agencies face both a growing\ndemand for youth mental health services, alongside a simultaneous challenge of\nlimited resources. Here, we focus on frontline crisis support, where Crisis\nResponders (CRs) engage in conversations for youth mental health support and\nassign an issue tag to each conversation. In this study, we develop FAIIR\n(Frontline Assistant: Issue Identification and Recommendation), an advanced\ntool leveraging an ensemble of domain-adapted and fine-tuned transformer models\ntrained on a large conversational dataset comprising 780,000 conversations. The\nprimary aim is to reduce the cognitive burden on CRs, enhance the accuracy of\nissue identification, and streamline post-conversation administrative tasks. We\nevaluate FAIIR on both retrospective and prospective conversations, emphasizing\nhuman-in-the-loop design with active CR engagement for model refinement,\nconsensus-building, and overall assessment. Our results indicate that FAIIR\nachieves an average AUCROC of 94%, a sample average F1-score of 64%, and a\nsample average recall score of 81% on the retrospective test set. We also\ndemonstrate the robustness and generalizability of the FAIIR tool during the\nsilent testing phase, with less than a 2% drop in all performance metrics.\nNotably, CRs' responses exhibited an overall agreement of 90.9% with FAIIR's\npredictions. Furthermore, expert agreement with FAIIR surpassed their agreement\nwith the original labels. To conclude, our findings indicate that assisting\nwith the identification of issues of relevance helps reduce the burden on CRs,\nensuring that appropriate resources can be provided and that active rescues and\nmandatory reporting can take place in critical situations requiring immediate\nde-escalation.",
      "tldr_zh": "该研究开发了 FAIIR（Frontline Assistant: Issue Identification and Recommendation），一种基于领域适应和微调的 Transformer 模型集合的对话式 AI 助手，旨在辅助前线危机响应者（Crisis Responders, CRs）识别青少年心理健康对话中的问题标签，从而减轻认知负担并简化行政任务。FAIIR 利用一个包含 78 万对话的大数据集进行训练，并采用 human-in-the-loop 设计，强调 CRs 的主动参与以改进模型。实验结果显示，在回顾测试集上，FAIIR 达到了平均 AUCROC 94%、F1-score 64% 和召回率 81%，而在静默测试中性能仅下降不到 2%。总体而言，该工具获得了 CRs 和专家超过 90% 的同意度，有助于确保及时提供资源并处理紧急情况，如主动救援和强制报告。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18553v4",
      "published_date": "2024-05-28 19:54:46 UTC",
      "updated_date": "2025-02-12 00:23:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:52:41.437902"
    },
    {
      "arxiv_id": "2405.18548v3",
      "title": "Transformer Encoder Satisfiability: Complexity and Impact on Formal Reasoning",
      "title_zh": "Transformer 编码器可满足性：复杂性及对形式推理的影响",
      "authors": [
        "Marco Sälzer",
        "Eric Alsmann",
        "Martin Lange"
      ],
      "abstract": "We analyse the complexity of the satisfiability problem, or similarly\nfeasibility problem, (trSAT) for transformer encoders (TE), which naturally\noccurs in formal verification or interpretation, collectively referred to as\nformal reasoning. We find that trSAT is undecidable when considering TE as they\nare commonly studied in the expressiveness community. Furthermore, we identify\npractical scenarios where trSAT is decidable and establish corresponding\ncomplexity bounds. Beyond trivial cases, we find that quantized TE, those\nrestricted by fixed-width arithmetic, lead to the decidability of trSAT due to\ntheir limited attention capabilities. However, the problem remains difficult,\nas we establish scenarios where trSAT is NEXPTIME-hard and others where it is\nsolvable in NEXPTIME for quantized TE. To complement our complexity results, we\nplace our findings and their implications in the broader context of formal\nreasoning.",
      "tldr_zh": "本文研究了Transformer Encoder的satisfiability问题（trSAT），即其可满足性或可行性问题，并探讨了其在正式推理中的复杂性和影响。研究发现，trSAT在一般Transformer Encoder中是undecidable的，但对于quantized TE（受固定宽度算术限制的TE），该问题在特定场景下变为decidable，且复杂度可能达到NEXPTIME-hard或NEXPTIME-solvable。总体而言，这些结果为正式验证和解释提供了重要理论基础，帮助理解Transformer Encoder在实际应用中的局限性和潜力。",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.CC",
        "cs.LG"
      ],
      "primary_category": "cs.LO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18548v3",
      "published_date": "2024-05-28 19:30:43 UTC",
      "updated_date": "2025-02-25 06:37:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:52:51.631866"
    },
    {
      "arxiv_id": "2405.18542v1",
      "title": "Automatic detection of cognitive impairment in elderly people using an entertainment chatbot with Natural Language Processing capabilities",
      "title_zh": "使用具有自然语言处理能力的娱乐聊天机器人自动检测老年人",
      "authors": [
        "Francisco de Arriba-Pérez",
        "Silvia García-Méndez",
        "Francisco J. González-Castaño",
        "Enrique Costa-Montenegro"
      ],
      "abstract": "Previous researchers have proposed intelligent systems for therapeutic\nmonitoring of cognitive impairments. However, most existing practical\napproaches for this purpose are based on manual tests. This raises issues such\nas excessive caretaking effort and the white-coat effect. To avoid these\nissues, we present an intelligent conversational system for entertaining\nelderly people with news of their interest that monitors cognitive impairment\ntransparently. Automatic chatbot dialogue stages allow assessing content\ndescription skills and detecting cognitive impairment with Machine Learning\nalgorithms. We create these dialogue flows automatically from updated news\nitems using Natural Language Generation techniques. The system also infers the\ngold standard of the answers to the questions, so it can assess cognitive\ncapabilities automatically by comparing these answers with the user responses.\nIt employs a similarity metric with values in [0, 1], in increasing level of\nsimilarity. To evaluate the performance and usability of our approach, we have\nconducted field tests with a test group of 30 elderly people in the earliest\nstages of dementia, under the supervision of gerontologists. In the\nexperiments, we have analysed the effect of stress and concentration in these\nusers. Those without cognitive impairment performed up to five times better. In\nparticular, the similarity metric varied between 0.03, for stressed and\nunfocused participants, and 0.36, for relaxed and focused users. Finally, we\ndeveloped a Machine Learning algorithm based on textual analysis features for\nautomatic cognitive impairment detection, which attained accuracy, F-measure\nand recall levels above 80%. We have thus validated the automatic approach to\ndetect cognitive impairment in elderly people based on entertainment content.",
      "tldr_zh": "这篇论文提出了一种使用娱乐聊天机器人自动检测老年人认知障碍的方法，利用 Natural Language Processing (NLP) 能力来创建基于新闻的对话流程，从而避免传统手动测试带来的过度护理负担和白大褂效应。系统通过 Natural Language Generation (NLG) 技术自动生成对话阶段，并采用 Machine Learning 算法评估用户回答与标准答案的相似度指标（范围[0,1]），以透明方式监测认知能力。在实地测试中，涉及30位早期痴呆老年人的实验显示，放松和集中的用户相似度可达0.36，而压力大的用户仅为0.03，且开发的文本分析算法实现了超过80%的准确率、F-measure 和 recall。该方法验证了娱乐内容在认知障碍检测中的有效性，为非侵入式监控提供了新途径。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18542v1",
      "published_date": "2024-05-28 19:17:48 UTC",
      "updated_date": "2024-05-28 19:17:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:53:04.126116"
    },
    {
      "arxiv_id": "2405.18523v2",
      "title": "MM-Mixing: Multi-Modal Mixing Alignment for 3D Understanding",
      "title_zh": "MM-Mixing：多模态混合对齐用于 3D 理解",
      "authors": [
        "Jiaze Wang",
        "Yi Wang",
        "Ziyu Guo",
        "Renrui Zhang",
        "Donghao Zhou",
        "Guangyong Chen",
        "Anfeng Liu",
        "Pheng-Ann Heng"
      ],
      "abstract": "We introduce MM-Mixing, a multi-modal mixing alignment framework for 3D\nunderstanding. MM-Mixing applies mixing-based methods to multi-modal data,\npreserving and optimizing cross-modal connections while enhancing diversity and\nimproving alignment across modalities. Our proposed two-stage training pipeline\ncombines feature-level and input-level mixing to optimize the 3D encoder. The\nfirst stage employs feature-level mixing with contrastive learning to align 3D\nfeatures with their corresponding modalities. The second stage incorporates\nboth feature-level and input-level mixing, introducing mixed point cloud inputs\nto further refine 3D feature representations. MM-Mixing enhances intermodality\nrelationships, promotes generalization, and ensures feature consistency while\nproviding diverse and realistic training samples. We demonstrate that MM-Mixing\nsignificantly improves baseline performance across various learning scenarios,\nincluding zero-shot 3D classification, linear probing 3D classification, and\ncross-modal 3D shape retrieval. Notably, we improved the zero-shot\nclassification accuracy on ScanObjectNN from 51.3% to 61.9%, and on\nObjaverse-LVIS from 46.8% to 51.4%. Our findings highlight the potential of\nmulti-modal mixing-based alignment to significantly advance 3D object\nrecognition and understanding while remaining straightforward to implement and\nintegrate into existing frameworks.",
      "tldr_zh": "该论文提出了 MM-Mixing，一种多模态混合对齐框架，用于提升 3D 理解，通过应用 mixing-based 方法优化跨模态连接并增加数据多样性。框架采用两阶段训练管道：第一阶段使用 feature-level mixing 与 contrastive learning 对齐 3D 特征；第二阶段结合 feature-level 和 input-level mixing，进一步完善 3D 特征表示。实验结果显示，MM-Mixing 在 zero-shot 3D classification 等任务上显著改善基线性能，例如 ScanObjectNN 的准确率从 51.3% 提高到 61.9%，Objaverse-LVIS 从 46.8% 提高到 51.4%。这项方法增强了模态间关系，促进模型泛化，并易于集成到现有框架中。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18523v2",
      "published_date": "2024-05-28 18:44:15 UTC",
      "updated_date": "2024-08-19 08:26:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:53:17.132118"
    },
    {
      "arxiv_id": "2405.18520v1",
      "title": "Offline-Boosted Actor-Critic: Adaptively Blending Optimal Historical Behaviors in Deep Off-Policy RL",
      "title_zh": "翻译失败",
      "authors": [
        "Yu Luo",
        "Tianying Ji",
        "Fuchun Sun",
        "Jianwei Zhang",
        "Huazhe Xu",
        "Xianyuan Zhan"
      ],
      "abstract": "Off-policy reinforcement learning (RL) has achieved notable success in\ntackling many complex real-world tasks, by leveraging previously collected data\nfor policy learning. However, most existing off-policy RL algorithms fail to\nmaximally exploit the information in the replay buffer, limiting sample\nefficiency and policy performance. In this work, we discover that concurrently\ntraining an offline RL policy based on the shared online replay buffer can\nsometimes outperform the original online learning policy, though the occurrence\nof such performance gains remains uncertain. This motivates a new possibility\nof harnessing the emergent outperforming offline optimal policy to improve\nonline policy learning. Based on this insight, we present Offline-Boosted\nActor-Critic (OBAC), a model-free online RL framework that elegantly identifies\nthe outperforming offline policy through value comparison, and uses it as an\nadaptive constraint to guarantee stronger policy learning performance. Our\nexperiments demonstrate that OBAC outperforms other popular model-free RL\nbaselines and rivals advanced model-based RL methods in terms of sample\nefficiency and asymptotic performance across 53 tasks spanning 6 task suites.",
      "tldr_zh": "本研究发现，在 off-policy RL 中，基于共享 replay buffer 的离线策略有时能优于在线策略，这启发了如何更好地利用历史数据提升学习效率。作者提出 Offline-Boosted Actor-Critic (OBAC) 框架，通过价值比较识别出优于在线策略的离线策略，并将其作为自适应约束来强化在线策略学习，从而提高样本效率和整体性能。实验结果显示，OBAC 在 53 个任务（涵盖 6 个任务套件）上超过了其他流行的无模型 RL 基线，并在样本效率和渐近性能方面与先进的模型-based RL 方法相当。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18520v1",
      "published_date": "2024-05-28 18:38:46 UTC",
      "updated_date": "2024-05-28 18:38:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:53:27.612170"
    },
    {
      "arxiv_id": "2406.00050v2",
      "title": "An Empirical Analysis on Large Language Models in Debate Evaluation",
      "title_zh": "大型语言模型在辩论评估中的实证分析",
      "authors": [
        "Xinyi Liu",
        "Pinxin Liu",
        "Hangfeng He"
      ],
      "abstract": "In this study, we investigate the capabilities and inherent biases of\nadvanced large language models (LLMs) such as GPT-3.5 and GPT-4 in the context\nof debate evaluation. We discover that LLM's performance exceeds humans and\nsurpasses the performance of state-of-the-art methods fine-tuned on extensive\ndatasets in debate evaluation. We additionally explore and analyze biases\npresent in LLMs, including positional bias, lexical bias, order bias, which may\naffect their evaluative judgments. Our findings reveal a consistent bias in\nboth GPT-3.5 and GPT-4 towards the second candidate response presented,\nattributed to prompt design. We also uncover lexical biases in both GPT-3.5 and\nGPT-4, especially when label sets carry connotations such as numerical or\nsequential, highlighting the critical need for careful label verbalizer\nselection in prompt design. Additionally, our analysis indicates a tendency of\nboth models to favor the debate's concluding side as the winner, suggesting an\nend-of-discussion bias.",
      "tldr_zh": "本研究通过实证分析，评估了大型语言模型（LLMs）如 GPT-3.5 和 GPT-4 在辩论评估中的性能，发现这些模型的表现超过了人类和基于大量数据集微调的现有方法。研究揭示了 LLMs 存在的偏见，包括位置偏见（positional bias）、词汇偏见（lexical bias）和顺序偏见（order bias），例如模型偏向于第二个候选响应，并倾向于将辩论的结尾一方视为赢家。特别强调，在提示设计中需谨慎选择标签（如避免数字或顺序含义），以减少这些偏见并提升评估可靠性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ACL 2024 main",
      "pdf_url": "http://arxiv.org/pdf/2406.00050v2",
      "published_date": "2024-05-28 18:34:53 UTC",
      "updated_date": "2024-06-04 14:51:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:53:40.101849"
    },
    {
      "arxiv_id": "2405.18512v1",
      "title": "Understanding Transformer Reasoning Capabilities via Graph Algorithms",
      "title_zh": "通过图算法理解 Transformer 的推理能力",
      "authors": [
        "Clayton Sanford",
        "Bahare Fatemi",
        "Ethan Hall",
        "Anton Tsitsulin",
        "Mehran Kazemi",
        "Jonathan Halcrow",
        "Bryan Perozzi",
        "Vahab Mirrokni"
      ],
      "abstract": "Which transformer scaling regimes are able to perfectly solve different\nclasses of algorithmic problems? While tremendous empirical advances have been\nattained by transformer-based neural networks, a theoretical understanding of\ntheir algorithmic reasoning capabilities in realistic parameter regimes is\nlacking. We investigate this question in terms of the network's depth, width,\nand number of extra tokens for algorithm execution. Our novel representational\nhierarchy separates 9 algorithmic reasoning problems into classes solvable by\ntransformers in different realistic parameter scaling regimes. We prove that\nlogarithmic depth is necessary and sufficient for tasks like graph\nconnectivity, while single-layer transformers with small embedding dimensions\ncan solve contextual retrieval tasks. We also support our theoretical analysis\nwith ample empirical evidence using the GraphQA benchmark. These results show\nthat transformers excel at many graph reasoning tasks, even outperforming\nspecialized graph neural networks.",
      "tldr_zh": "本研究通过图算法分析了 Transformer 在不同参数缩放机制（如深度、宽度和额外 token）下解决算法问题的能力，提出一个新的表示层次将9个算法推理问题分类为不同可解类别。研究证明，对数深度是图连通性等任务的必要和充分条件，而单层 Transformer 即使在小嵌入维度下也能处理上下文检索任务。该理论分析得到 GraphQA benchmark 的实证支持，显示 Transformer 在许多图推理任务上表现出色，甚至超越了专门的 Graph Neural Networks。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "43 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.18512v1",
      "published_date": "2024-05-28 18:31:14 UTC",
      "updated_date": "2024-05-28 18:31:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:53:50.666685"
    },
    {
      "arxiv_id": "2405.18510v1",
      "title": "Improved Emotional Alignment of AI and Humans: Human Ratings of Emotions Expressed by Stable Diffusion v1, DALL-E 2, and DALL-E 3",
      "title_zh": "翻译失败",
      "authors": [
        "James Derek Lomas",
        "Willem van der Maden",
        "Sohhom Bandyopadhyay",
        "Giovanni Lion",
        "Nirmal Patel",
        "Gyanesh Jain",
        "Yanna Litowsky",
        "Haian Xue",
        "Pieter Desmet"
      ],
      "abstract": "Generative AI systems are increasingly capable of expressing emotions via\ntext and imagery. Effective emotional expression will likely play a major role\nin the efficacy of AI systems -- particularly those designed to support human\nmental health and wellbeing. This motivates our present research to better\nunderstand the alignment of AI expressed emotions with the human perception of\nemotions. When AI tries to express a particular emotion, how might we assess\nwhether they are successful? To answer this question, we designed a survey to\nmeasure the alignment between emotions expressed by generative AI and human\nperceptions. Three generative image models (DALL-E 2, DALL-E 3 and Stable\nDiffusion v1) were used to generate 240 examples of images, each of which was\nbased on a prompt designed to express five positive and five negative emotions\nacross both humans and robots. 24 participants recruited from the Prolific\nwebsite rated the alignment of AI-generated emotional expressions with a text\nprompt used to generate the emotion (i.e., \"A robot expressing the emotion\namusement\"). The results of our evaluation suggest that generative AI models\nare indeed capable of producing emotional expressions that are well-aligned\nwith a range of human emotions; however, we show that the alignment\nsignificantly depends upon the AI model used and the emotion itself. We analyze\nvariations in the performance of these systems to identify gaps for future\nimprovement. We conclude with a discussion of the implications for future AI\nsystems designed to support mental health and wellbeing.",
      "tldr_zh": "这篇论文评估了 generative AI 系统（如 Stable Diffusion v1、DALL-E 2 和 DALL-E 3）在通过图像表达情绪时与人类感知的对齐度，重点关注其对支持心理健康和福祉的潜在影响。研究者设计了一个调查，使用这些模型生成240个图像样本，每个样本基于文本提示表达五种正面和五种负面情绪（针对人类和机器人），并由24名参与者对情绪表达的准确性进行评分。结果显示，AI 模型能够产生与人类情绪对齐的表达，但对齐度显著取决于所用模型和具体情绪类型。论文分析了这些差异，识别了未来改进机会，并讨论了其对设计更有效心理健康支持 AI 系统的含义。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18510v1",
      "published_date": "2024-05-28 18:26:57 UTC",
      "updated_date": "2024-05-28 18:26:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:54:04.233121"
    },
    {
      "arxiv_id": "2405.18507v4",
      "title": "Injecting Hierarchical Biological Priors into Graph Neural Networks for Flow Cytometry Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Fatemeh Nassajian Mojarrad",
        "Lorenzo Bini",
        "Thomas Matthes",
        "Stéphane Marchand-Maillet"
      ],
      "abstract": "In the complex landscape of hematologic samples such as peripheral blood or\nbone marrow derived from flow cytometry (FC) data, cell-level prediction\npresents profound challenges. This work explores injecting hierarchical prior\nknowledge into graph neural networks (GNNs) for single-cell multi-class\nclassification of tabular cellular data. By representing the data as graphs and\nencoding hierarchical relationships between classes, we propose our\nhierarchical plug-in method to be applied to several GNN models, namely,\nFCHC-GNN, and effectively designed to capture neighborhood information crucial\nfor single-cell FC domain. Extensive experiments on our cohort of 19 distinct\npatients, demonstrate that incorporating hierarchical biological constraints\nboosts performance significantly across multiple metrics compared to baseline\nGNNs without such priors. The proposed approach highlights the importance of\nstructured inductive biases for gaining improved generalization in complex\nbiological prediction tasks.",
      "tldr_zh": "这篇论文探讨了在流式细胞术 (Flow Cytometry, FC) 数据中注入层级生物先验知识到图神经网络 (Graph Neural Networks, GNNs) 中的方法，以应对单细胞多类分类的复杂挑战。研究者通过将数据表示为图结构，并编码类之间的层级关系，提出了一种可插件的层级方法，如 FCHC-GNN，专注于捕获细胞邻域信息。在 19 个患者队列上的广泛实验显示，这种方法显著提升了性能指标，并改善了 GNNs 在复杂生物预测任务中的泛化能力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, ICML Conference Workshop 2024. arXiv admin note: text\n  overlap with arXiv:2402.18610",
      "pdf_url": "http://arxiv.org/pdf/2405.18507v4",
      "published_date": "2024-05-28 18:24:16 UTC",
      "updated_date": "2024-07-27 22:11:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:54:17.897533"
    },
    {
      "arxiv_id": "2405.18492v3",
      "title": "LLMs and Memorization: On Quality and Specificity of Copyright Compliance",
      "title_zh": "LLMs 与记忆化：版权合规的质量和特异性",
      "authors": [
        "Felix B Mueller",
        "Rebekka Görge",
        "Anna K Bernzen",
        "Janna C Pirk",
        "Maximilian Poretschkin"
      ],
      "abstract": "Memorization in large language models (LLMs) is a growing concern. LLMs have\nbeen shown to easily reproduce parts of their training data, including\ncopyrighted work. This is an important problem to solve, as it may violate\nexisting copyright laws as well as the European AI Act. In this work, we\npropose a systematic analysis to quantify the extent of potential copyright\ninfringements in LLMs using European law as an example. Unlike previous work,\nwe evaluate instruction-finetuned models in a realistic end-user scenario. Our\nanalysis builds on a proposed threshold of 160 characters, which we borrow from\nthe German Copyright Service Provider Act and a fuzzy text matching algorithm\nto identify potentially copyright-infringing textual reproductions. The\nspecificity of countermeasures against copyright infringement is analyzed by\ncomparing model behavior on copyrighted and public domain data. We investigate\nwhat behaviors models show instead of producing protected text (such as refusal\nor hallucination) and provide a first legal assessment of these behaviors. We\nfind that there are huge differences in copyright compliance, specificity, and\nappropriate refusal among popular LLMs. Alpaca, GPT 4, GPT 3.5, and Luminous\nperform best in our comparison, with OpenGPT-X, Alpaca, and Luminous producing\na particularly low absolute number of potential copyright violations. Code can\nbe found at https://github.com/felixbmuller/llms-memorization-copyright.",
      "tldr_zh": "本研究探讨大型语言模型(LLMs)中的记忆化问题，特别是复制训练数据导致的潜在版权侵权，并评估其合规性质量和特异性。研究提出一种系统分析框架，使用欧洲法律（如德国版权服务提供者法）为基准，结合160字符阈值和模糊文本匹配算法，在真实端用户场景中评估指令微调模型的行为，包括比较版权数据和公有领域数据的表现。结果显示，不同LLMs在版权合规性、特异性和适当拒绝方面存在巨大差异，其中Alpaca、GPT-4、GPT-3.5和Luminous表现最佳，而OpenGPT-X、Alpaca和Luminous的潜在侵权数量最低，为改进LLMs的版权策略提供了重要见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 3 figures, AIES 2024 conference",
      "pdf_url": "http://arxiv.org/pdf/2405.18492v3",
      "published_date": "2024-05-28 18:01:52 UTC",
      "updated_date": "2024-11-18 09:44:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:54:29.245574"
    },
    {
      "arxiv_id": "2405.18471v2",
      "title": "Symbolic Regression for Beyond the Standard Model Physics",
      "title_zh": "翻译失败",
      "authors": [
        "Shehu AbdusSalam",
        "Steve Abel",
        "Miguel Crispim Romao"
      ],
      "abstract": "We propose symbolic regression as a powerful tool for studying Beyond the\nStandard Model physics. As a benchmark model, we consider the so-called\nConstrained Minimal Supersymmetric Standard Model, which has a four-dimensional\nparameter space defined at the GUT scale. We provide a set of analytical\nexpressions that reproduce three low-energy observables of interest in terms of\nthe parameters of the theory: the Higgs mass, the contribution to the anomalous\nmagnetic moment of the muon, and the cold dark matter relic density. To\ndemonstrate the power of the approach, we employ the symbolic expressions in a\nglobal fits analysis to derive the posterior probability densities of the\nparameters, which are obtained extremely rapidly in comparison with\nconventional methods.",
      "tldr_zh": "本论文提出使用 symbolic regression 作为研究 Beyond the Standard Model 物理的有效工具，以 Constrained Minimal Supersymmetric Standard Model 为基准模型，该模型具有四维参数空间。研究者提供了分析表达式，将理论参数与三个低能量可观测量关联起来，包括 Higgs mass、muon 的 anomalous magnetic moment 贡献以及 cold dark matter relic density。这些表达式在全局拟合分析中被应用，能够极快地获得参数的后验概率密度，比传统方法显著提升效率。",
      "categories": [
        "hep-ph",
        "cs.AI",
        "cs.LG",
        "hep-th",
        "physics.comp-ph"
      ],
      "primary_category": "hep-ph",
      "comment": "Version accepted for publication in PRD. 8 pages, 10 figures. For\n  associated code and symbolic expressions see\n  https://gitlab.com/miguel.romao/symbolic-regression-bsm",
      "pdf_url": "http://arxiv.org/pdf/2405.18471v2",
      "published_date": "2024-05-28 18:00:01 UTC",
      "updated_date": "2025-04-22 14:35:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:54:40.836931"
    },
    {
      "arxiv_id": "2405.18428v2",
      "title": "DiG: Scalable and Efficient Diffusion Models with Gated Linear Attention",
      "title_zh": "DiG：基于门控线性注意力的可扩展高效扩散模型",
      "authors": [
        "Lianghui Zhu",
        "Zilong Huang",
        "Bencheng Liao",
        "Jun Hao Liew",
        "Hanshu Yan",
        "Jiashi Feng",
        "Xinggang Wang"
      ],
      "abstract": "Diffusion models with large-scale pre-training have achieved significant\nsuccess in the field of visual content generation, particularly exemplified by\nDiffusion Transformers (DiT). However, DiT models have faced challenges with\nquadratic complexity efficiency, especially when handling long sequences. In\nthis paper, we aim to incorporate the sub-quadratic modeling capability of\nGated Linear Attention (GLA) into the 2D diffusion backbone. Specifically, we\nintroduce Diffusion Gated Linear Attention Transformers (DiG), a simple,\nadoptable solution with minimal parameter overhead. We offer two variants, i,e,\na plain and U-shape architecture, showing superior efficiency and competitive\neffectiveness. In addition to superior performance to DiT and other\nsub-quadratic-time diffusion models at $256 \\times 256$ resolution, DiG\ndemonstrates greater efficiency than these methods starting from a $512$\nresolution. Specifically, DiG-S/2 is $2.5\\times$ faster and saves $75.7\\%$ GPU\nmemory compared to DiT-S/2 at a $1792$ resolution. Additionally, DiG-XL/2 is\n$4.2\\times$ faster than the Mamba-based model at a $1024$ resolution and\n$1.8\\times$ faster than DiT with FlashAttention-2 at a $2048$ resolution. We\nwill release the code soon. Code is released at https://github.com/hustvl/DiG.",
      "tldr_zh": "这篇论文提出 DiG（Diffusion Gated Linear Attention Transformers），一种可扩展且高效的扩散模型，通过整合 Gated Linear Attention (GLA) 来解决传统 Diffusion Transformers (DiT) 在处理长序列时面临的二次复杂度效率问题。DiG 提供 plain 和 U-shape 两种架构，仅需最小参数开销，即可在 256x256 分辨率下超越 DiT 和其他亚二次时间扩散模型，并在更高分辨率（如 512 或 1792）下表现出色，例如 DiG-S/2 比 DiT-S/2 快 2.5 倍并节省 75.7% GPU 内存。实验结果显示，DiG 在视觉内容生成中实现了竞争性性能和显著效率提升，代码已发布于 https://github.com/hustvl/DiG。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Code is released at https://github.com/hustvl/DiG",
      "pdf_url": "http://arxiv.org/pdf/2405.18428v2",
      "published_date": "2024-05-28 17:59:33 UTC",
      "updated_date": "2024-11-26 16:42:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:54:54.657896"
    },
    {
      "arxiv_id": "2405.18427v1",
      "title": "Classifying Overlapping Gaussian Mixtures in High Dimensions: From Optimal Classifiers to Neural Nets",
      "title_zh": "翻译失败",
      "authors": [
        "Khen Cohen",
        "Noam Levi",
        "Yaron Oz"
      ],
      "abstract": "We derive closed-form expressions for the Bayes optimal decision boundaries\nin binary classification of high dimensional overlapping Gaussian mixture model\n(GMM) data, and show how they depend on the eigenstructure of the class\ncovariances, for particularly interesting structured data. We empirically\ndemonstrate, through experiments on synthetic GMMs inspired by real-world data,\nthat deep neural networks trained for classification, learn predictors which\napproximate the derived optimal classifiers. We further extend our study to\nnetworks trained on authentic data, observing that decision thresholds\ncorrelate with the covariance eigenvectors rather than the eigenvalues,\nmirroring our GMM analysis. This provides theoretical insights regarding neural\nnetworks' ability to perform probabilistic inference and distill statistical\npatterns from intricate distributions.",
      "tldr_zh": "该论文推导了高维重叠 Gaussian Mixture Model (GMM) 数据二元分类的 Bayes optimal 决策边界闭式表达式，并分析了这些边界如何依赖于类协方差的特征结构，特别是针对结构化数据。实验结果显示，深度神经网络在训练后能学习出近似这些最优分类器的预测器，使用合成 GMM 和真实数据进行验证。研究进一步发现，神经网络的决策阈值更与协方差的 eigenvectors 相关而非 eigenvalues，这为神经网络进行概率推理和从复杂分布中提炼统计模式提供了重要理论洞见。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "19 pages, 14 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.18427v1",
      "published_date": "2024-05-28 17:59:31 UTC",
      "updated_date": "2024-05-28 17:59:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:55:05.333759"
    },
    {
      "arxiv_id": "2405.18426v2",
      "title": "GFlow: Recovering 4D World from Monocular Video",
      "title_zh": "GFlow: 从单目视频中恢复四维世界",
      "authors": [
        "Shizun Wang",
        "Xingyi Yang",
        "Qiuhong Shen",
        "Zhenxiang Jiang",
        "Xinchao Wang"
      ],
      "abstract": "Recovering 4D world from monocular video is a crucial yet challenging task.\nConventional methods usually rely on the assumptions of multi-view videos,\nknown camera parameters, or static scenes. In this paper, we relax all these\nconstraints and tackle a highly ambitious but practical task: With only one\nmonocular video without camera parameters, we aim to recover the dynamic 3D\nworld alongside the camera poses. To solve this, we introduce GFlow, a new\nframework that utilizes only 2D priors (depth and optical flow) to lift a video\nto a 4D scene, as a flow of 3D Gaussians through space and time. GFlow starts\nby segmenting the video into still and moving parts, then alternates between\noptimizing camera poses and the dynamics of the 3D Gaussian points. This method\nensures consistency among adjacent points and smooth transitions between\nframes. Since dynamic scenes always continually introduce new visual content,\nwe present prior-driven initialization and pixel-wise densification strategy\nfor Gaussian points to integrate new content. By combining all those\ntechniques, GFlow transcends the boundaries of 4D recovery from causal videos;\nit naturally enables tracking of points and segmentation of moving objects\nacross frames. Additionally, GFlow estimates the camera poses for each frame,\nenabling novel view synthesis by changing camera pose. This capability\nfacilitates extensive scene-level or object-level editing, highlighting GFlow's\nversatility and effectiveness. Visit our project page at:\nhttps://littlepure2333.github.io/GFlow",
      "tldr_zh": "本论文提出GFlow框架，用于从单目视频恢复4D World，而无需多视图视频、已知相机参数或静态场景限制。该框架利用2D先验（如深度和光流）将视频提升为3D Gaussians在空间和时间的流动，通过视频分割、相机位姿优化以及先验驱动的初始化和像素级稠密化策略，确保动态场景的一致性和平滑过渡。GFlow不仅实现了点跟踪、移动物体分割和相机位姿估计，还支持新视图合成和场景或物体级编辑，展示了其在动态3D世界恢复中的高效性和多功能性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "AAAI 2025. Project page: https://littlepure2333.github.io/GFlow",
      "pdf_url": "http://arxiv.org/pdf/2405.18426v2",
      "published_date": "2024-05-28 17:59:22 UTC",
      "updated_date": "2024-12-31 07:05:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:55:17.812597"
    },
    {
      "arxiv_id": "2405.18425v2",
      "title": "ViG: Linear-complexity Visual Sequence Learning with Gated Linear Attention",
      "title_zh": "翻译失败",
      "authors": [
        "Bencheng Liao",
        "Xinggang Wang",
        "Lianghui Zhu",
        "Qian Zhang",
        "Chang Huang"
      ],
      "abstract": "Recently, linear complexity sequence modeling networks have achieved modeling\ncapabilities similar to Vision Transformers on a variety of computer vision\ntasks, while using fewer FLOPs and less memory. However, their advantage in\nterms of actual runtime speed is not significant. To address this issue, we\nintroduce Gated Linear Attention (GLA) for vision, leveraging its superior\nhardware-awareness and efficiency. We propose direction-wise gating to capture\n1D global context through bidirectional modeling and a 2D gating locality\ninjection to adaptively inject 2D local details into 1D global context. Our\nhardware-aware implementation further merges forward and backward scanning into\na single kernel, enhancing parallelism and reducing memory cost and latency.\nThe proposed model, ViG, offers a favorable trade-off in accuracy, parameters,\nand FLOPs on ImageNet and downstream tasks, outperforming popular Transformer\nand CNN-based models. Notably, ViG-S matches DeiT-B's accuracy while using only\n27% of the parameters and 20% of the FLOPs, running 2$\\times$ faster on\n$224\\times224$ images. At $1024\\times1024$ resolution, ViG-T uses 5.2$\\times$\nfewer FLOPs, saves 90% GPU memory, runs 4.8$\\times$ faster, and achieves 20.7%\nhigher top-1 accuracy than DeiT-T. These results position ViG as an efficient\nand scalable solution for visual representation learning. Code is available at\n\\url{https://github.com/hustvl/ViG}.",
      "tldr_zh": "该论文提出 ViG 模型，利用 Gated Linear Attention (GLA) 实现线性复杂度的视觉序列学习，旨在解决现有模型计算效率问题。GLA 通过 direction-wise gating 捕获 1D 全局上下文，并结合 2D gating locality injection 注入局部细节，同时优化硬件感知实现以减少内存开销和延迟。实验结果显示，ViG 在 ImageNet 和下游任务上提供优越的准确性、参数和 FLOPs 权衡，例如 ViG-S 与 DeiT-B 准确率相当，但仅使用 27% 参数和 20% FLOPs，并运行速度快 2 倍；在更高分辨率下，ViG-T 比 DeiT-T 节省更多资源并提升准确率 20.7%。这使 ViG 成为高效可扩展的视觉表示学习解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Work in progress. Code is available at\n  \\url{https://github.com/hustvl/ViG}",
      "pdf_url": "http://arxiv.org/pdf/2405.18425v2",
      "published_date": "2024-05-28 17:59:21 UTC",
      "updated_date": "2024-05-29 02:06:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:55:30.098646"
    },
    {
      "arxiv_id": "2405.18415v2",
      "title": "Why are Visually-Grounded Language Models Bad at Image Classification?",
      "title_zh": "翻译失败",
      "authors": [
        "Yuhui Zhang",
        "Alyssa Unell",
        "Xiaohan Wang",
        "Dhruba Ghosh",
        "Yuchang Su",
        "Ludwig Schmidt",
        "Serena Yeung-Levy"
      ],
      "abstract": "Image classification is one of the most fundamental capabilities of machine\nvision intelligence. In this work, we revisit the image classification task\nusing visually-grounded language models (VLMs) such as GPT-4V and LLaVA. We\nfind that existing proprietary and public VLMs, despite often using CLIP as a\nvision encoder and having many more parameters, significantly underperform CLIP\non standard image classification benchmarks like ImageNet. To understand the\nreason, we explore several hypotheses concerning the inference algorithms,\ntraining objectives, and data processing in VLMs. Our analysis reveals that the\nprimary cause is data-related: critical information for image classification is\nencoded in the VLM's latent space but can only be effectively decoded with\nenough training data. Specifically, there is a strong correlation between the\nfrequency of class exposure during VLM training and instruction-tuning and the\nVLM's performance in those classes; when trained with sufficient data, VLMs can\nmatch the accuracy of state-of-the-art classification models. Based on these\nfindings, we enhance a VLM by integrating classification-focused datasets into\nits training, and demonstrate that the enhanced classification performance of\nthe VLM transfers to its general capabilities, resulting in an improvement of\n11.8% on the newly collected ImageWikiQA dataset.",
      "tldr_zh": "这篇论文探讨了视觉语言模型(VLMs)如 GPT-4V 和 LLaVA 为什么在图像分类任务上表现不如 CLIP，尽管它们拥有更多参数和相同的视觉编码器。研究通过分析推理算法、训练目标和数据处理，发现主要原因是数据相关：VLMs 的潜在空间中编码了关键分类信息，但需足够的训练数据才能有效解码，且类别在训练中的暴露频率与性能高度相关。作者通过将分类-focused 数据集整合到 VLM 训练中，显著提升了模型的图像分类准确率，并证明这种改进转移到一般能力上，使 ImageWikiQA 数据集的性能提高了 11.8%。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Published at NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.18415v2",
      "published_date": "2024-05-28 17:57:06 UTC",
      "updated_date": "2024-11-03 18:23:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:55:41.899951"
    },
    {
      "arxiv_id": "2405.18414v1",
      "title": "Don't Forget to Connect! Improving RAG with Graph-based Reranking",
      "title_zh": "翻译失败",
      "authors": [
        "Jialin Dong",
        "Bahare Fatemi",
        "Bryan Perozzi",
        "Lin F. Yang",
        "Anton Tsitsulin"
      ],
      "abstract": "Retrieval Augmented Generation (RAG) has greatly improved the performance of\nLarge Language Model (LLM) responses by grounding generation with context from\nexisting documents. These systems work well when documents are clearly relevant\nto a question context. But what about when a document has partial information,\nor less obvious connections to the context? And how should we reason about\nconnections between documents? In this work, we seek to answer these two core\nquestions about RAG generation. We introduce G-RAG, a reranker based on graph\nneural networks (GNNs) between the retriever and reader in RAG. Our method\ncombines both connections between documents and semantic information (via\nAbstract Meaning Representation graphs) to provide a context-informed ranker\nfor RAG. G-RAG outperforms state-of-the-art approaches while having smaller\ncomputational footprint. Additionally, we assess the performance of PaLM 2 as a\nreranker and find it to significantly underperform G-RAG. This result\nemphasizes the importance of reranking for RAG even when using Large Language\nModels.",
      "tldr_zh": "这篇论文针对 Retrieval Augmented Generation (RAG) 在处理文档部分信息或连接不明显时的问题，提出了 G-RAG，这是一种基于 Graph Neural Networks (GNNs) 的 reranker，置于 RAG 的检索器和阅读器之间。G-RAG 通过整合文档间的连接和语义信息（如 Abstract Meaning Representation graphs）来提供更准确的上下文排名，从而提升 Large Language Model (LLM) 的响应性能。实验结果表明，G-RAG 超过了现有方法的性能，同时具有更小的计算开销；此外，使用 PaLM 2 作为 reranker 的表现远逊于 G-RAG，这突出了 reranking 在 RAG 系统中的关键作用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18414v1",
      "published_date": "2024-05-28 17:56:46 UTC",
      "updated_date": "2024-05-28 17:56:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:55:54.844263"
    },
    {
      "arxiv_id": "2405.18406v3",
      "title": "RACCooN: A Versatile Instructional Video Editing Framework with Auto-Generated Narratives",
      "title_zh": "翻译失败",
      "authors": [
        "Jaehong Yoon",
        "Shoubin Yu",
        "Mohit Bansal"
      ],
      "abstract": "Recent video generative models primarily rely on carefully written text\nprompts for specific tasks, like inpainting or style editing. They require\nlabor-intensive textual descriptions for input videos, hindering their\nflexibility to adapt personal/raw videos to user specifications. This paper\nproposes RACCooN, a versatile and user-friendly video-to-paragraph-to-video\ngenerative framework that supports multiple video editing capabilities such as\nremoval, addition, and modification, through a unified pipeline. RACCooN\nconsists of two principal stages: Video-to-Paragraph (V2P) and\nParagraph-to-Video (P2V). In the V2P stage, we automatically describe video\nscenes in well-structured natural language, capturing both the holistic context\nand focused object details. Subsequently, in the P2V stage, users can\noptionally refine these descriptions to guide the video diffusion model,\nenabling various modifications to the input video, such as removing, changing\nsubjects, and/or adding new objects. The proposed approach stands out from\nother methods through several significant contributions: (1) RACCooN suggests a\nmulti-granular spatiotemporal pooling strategy to generate well-structured\nvideo descriptions, capturing both the broad context and object details without\nrequiring complex human annotations, simplifying precise video content editing\nbased on text for users. (2) Our video generative model incorporates\nauto-generated narratives or instructions to enhance the quality and accuracy\nof the generated content. (3) RACCooN also plans to imagine new objects in a\ngiven video, so users simply prompt the model to receive a detailed video\nediting plan for complex video editing. The proposed framework demonstrates\nimpressive versatile capabilities in video-to-paragraph generation, video\ncontent editing, and can be incorporated into other SoTA video generative\nmodels for further enhancement.",
      "tldr_zh": "本论文提出 RACCooN，一种多功能视频编辑框架，通过自动生成叙述支持移除、添加和修改等操作，解决了传统视频生成模型依赖手动文本提示的局限性。框架包括两个主要阶段：V2P（Video-to-Paragraph）使用多粒度时空池化策略自动生成结构化的视频描述，捕捉整体上下文和对象细节；P2V（Paragraph-to-Video）允许用户修改这些描述来指导视频扩散模型，实现精确编辑。RACCooN 的关键贡献在于简化视频内容编辑过程、提升生成质量，并能想象新对象提供详细编辑计划，可与其他 SOTA 视频生成模型整合，以增强其灵活性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "The first two authors contribute equally. Project Page:\n  https://raccoon-mllm-gen.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2405.18406v3",
      "published_date": "2024-05-28 17:46:36 UTC",
      "updated_date": "2024-10-31 23:27:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:56:06.236796"
    },
    {
      "arxiv_id": "2405.18405v1",
      "title": "WIDIn: Wording Image for Domain-Invariant Representation in Single-Source Domain Generalization",
      "title_zh": "翻译失败",
      "authors": [
        "Jiawei Ma",
        "Yulei Niu",
        "Shiyuan Huang",
        "Guangxing Han",
        "Shih-Fu Chang"
      ],
      "abstract": "Language has been useful in extending the vision encoder to data from diverse\ndistributions without empirical discovery in training domains. However, as the\nimage description is mostly at coarse-grained level and ignores visual details,\nthe resulted embeddings are still ineffective in overcoming complexity of\ndomains at inference time. We present a self-supervision framework WIDIn,\nWording Images for Domain-Invariant representation, to disentangle\ndiscriminative visual representation, by only leveraging data in a single\ndomain and without any test prior. Specifically, for each image, we first\nestimate the language embedding with fine-grained alignment, which can be\nconsequently used to adaptively identify and then remove domain-specific\ncounterpart from the raw visual embedding. WIDIn can be applied to both\npretrained vision-language models like CLIP, and separately trained uni-modal\nmodels like MoCo and BERT. Experimental studies on three domain generalization\ndatasets demonstrate the effectiveness of our approach.",
      "tldr_zh": "本论文提出WIDIn框架（Wording Images for Domain-Invariant Representation），旨在通过单一源域数据实现领域不变表示（Domain-Invariant Representation），解决传统视觉编码器在处理不同分布数据时因粗粒度图像描述而忽略视觉细节的问题。WIDIn采用自监督方法，首先估计细粒度对齐的语言嵌入，然后自适应地识别并移除原始视觉嵌入中的领域特定部分，从而分离出判别性视觉表示。该框架适用于预训练的视觉-语言模型如CLIP，以及单独训练的单模态模型如MoCo和BERT；在三个领域泛化（Domain Generalization）数据集上的实验证明了WIDIn的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18405v1",
      "published_date": "2024-05-28 17:46:27 UTC",
      "updated_date": "2024-05-28 17:46:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:56:19.460072"
    },
    {
      "arxiv_id": "2405.18459v2",
      "title": "Probing the Information Theoretical Roots of Spatial Dependence Measures",
      "title_zh": "翻译失败",
      "authors": [
        "Zhangyu Wang",
        "Krzysztof Janowicz",
        "Gengchen Mai",
        "Ivan Majic"
      ],
      "abstract": "Intuitively, there is a relation between measures of spatial dependence and\ninformation theoretical measures of entropy. For instance, we can provide an\nintuition of why spatial data is special by stating that, on average, spatial\ndata samples contain less than expected information. Similarly, spatial data,\ne.g., remotely sensed imagery, that is easy to compress is also likely to show\nsignificant spatial autocorrelation. Formulating our (highly specific) core\nconcepts of spatial information theory in the widely used language of\ninformation theory opens new perspectives on their differences and similarities\nand also fosters cross-disciplinary collaboration, e.g., with the broader AI/ML\ncommunities. Interestingly, however, this intuitive relation is challenging to\nformalize and generalize, leading prior work to rely mostly on experimental\nresults, e.g., for describing landscape patterns. In this work, we will explore\nthe information theoretical roots of spatial autocorrelation, more specifically\nMoran's I, through the lens of self-information (also known as surprisal) and\nprovide both formal proofs and experiments.",
      "tldr_zh": "本论文探讨了空间依赖度量（如Moran's I）的信息理论根源，旨在正式化空间数据与熵之间的直观关系，例如空间数据平均信息量减少导致的自相关性。作者通过self-information（也称surprisal）作为分析框架，提供正式证明和实验验证，揭示了这些度量在信息理论中的差异与相似性。该研究有助于促进跨学科合作，例如与AI/ML社区的互动，并为描述景观模式等应用提供新视角。",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG",
        "math.IT",
        "stat.ME"
      ],
      "primary_category": "cs.IT",
      "comment": "COSIT-2024 Conference Proceedings",
      "pdf_url": "http://arxiv.org/pdf/2405.18459v2",
      "published_date": "2024-05-28 17:44:35 UTC",
      "updated_date": "2024-07-23 16:50:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:56:30.589441"
    },
    {
      "arxiv_id": "2405.18395v2",
      "title": "MC-GTA: Metric-Constrained Model-Based Clustering using Goodness-of-fit Tests with Autocorrelations",
      "title_zh": "翻译失败",
      "authors": [
        "Zhangyu Wang",
        "Gengchen Mai",
        "Krzysztof Janowicz",
        "Ni Lao"
      ],
      "abstract": "A wide range of (multivariate) temporal (1D) and spatial (2D) data analysis\ntasks, such as grouping vehicle sensor trajectories, can be formulated as\nclustering with given metric constraints. Existing metric-constrained\nclustering algorithms overlook the rich correlation between feature similarity\nand metric distance, i.e., metric autocorrelation. The model-based variations\nof these clustering algorithms (e.g. TICC and STICC) achieve SOTA performance,\nyet suffer from computational instability and complexity by using a\nmetric-constrained Expectation-Maximization procedure. In order to address\nthese two problems, we propose a novel clustering algorithm, MC-GTA\n(Model-based Clustering via Goodness-of-fit Tests with Autocorrelations). Its\nobjective is only composed of pairwise weighted sums of feature similarity\nterms (square Wasserstein-2 distance) and metric autocorrelation terms (a novel\nmultivariate generalization of classic semivariogram). We show that MC-GTA is\neffectively minimizing the total hinge loss for intra-cluster observation pairs\nnot passing goodness-of-fit tests, i.e., statistically not originating from the\nsame distribution. Experiments on 1D/2D synthetic and real-world datasets\ndemonstrate that MC-GTA successfully incorporates metric autocorrelation. It\noutperforms strong baselines by large margins (up to 14.3% in ARI and 32.1% in\nNMI) with faster and stabler optimization (>10x speedup).",
      "tldr_zh": "该论文提出了 MC-GTA，一种基于 Goodness-of-fit Tests with Autocorrelations 的模型聚类算法，用于处理带度量约束的多变量时间（1D）和空间（2D）数据分析任务，如车辆传感器轨迹分组。MC-GTA 的目标函数结合了特征相似性（square Wasserstein-2 distance）和度量自相关性（一种新颖的多变量 semivariogram 广义化），通过最小化 intra-cluster 观察对不通过适配优度测试的 hinge loss，实现更稳定高效的聚类。实验结果显示，MC-GTA 在合成和真实数据集上大幅超越基线模型，提升 ARI 至 14.3% 和 NMI 至 32.1%，并实现了超过 10 倍的优化速度。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.AP"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML-2024 Proceedings",
      "pdf_url": "http://arxiv.org/pdf/2405.18395v2",
      "published_date": "2024-05-28 17:35:05 UTC",
      "updated_date": "2024-06-03 03:53:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:56:45.125546"
    },
    {
      "arxiv_id": "2405.18387v1",
      "title": "A Review and Implementation of Object Detection Models and Optimizations for Real-time Medical Mask Detection during the COVID-19 Pandemic",
      "title_zh": "COVID-19 大流行期间实时医疗口罩检测的物体检测模型与优化综述及实现",
      "authors": [
        "Ioanna Gogou",
        "Dimitrios Koutsomitropoulos"
      ],
      "abstract": "Convolutional Neural Networks (CNN) are commonly used for the problem of\nobject detection thanks to their increased accuracy. Nevertheless, the\nperformance of CNN-based detection models is ambiguous when detection speed is\nconsidered. To the best of our knowledge, there has not been sufficient\nevaluation of the available methods in terms of the speed/accuracy trade-off in\nrelated literature. This work assesses the most fundamental object detection\nmodels on the Common Objects in Context (COCO) dataset with respect to this\ntrade-off, their memory consumption, and computational and storage cost. Next,\nwe select a highly efficient model called YOLOv5 to train on the topical and\nunexplored dataset of human faces with medical masks, the Properly-Wearing\nMasked Faces Dataset (PWMFD), and analyze the benefits of specific optimization\ntechniques for real-time medical mask detection: transfer learning, data\naugmentations, and a Squeeze-and-Excitation attention mechanism. Using our\nfindings in the context of the COVID-19 pandemic, we propose an optimized model\nbased on YOLOv5s using transfer learning for the detection of correctly and\nincorrectly worn medical masks that surpassed more than two times in speed (69\nframes per second) the state-of-the-art model SE-YOLOv3 on the PWMFD dataset\nwhile maintaining the same level of mean Average Precision (67%).",
      "tldr_zh": "这篇论文审阅了物体检测模型在 COVID-19 疫情期间实时医疗口罩检测中的应用，评估了 CNN 模型（如 YOLOv5）在速度/准确性权衡、内存消耗和计算成本方面的性能。\n作者使用 COCO 数据集测试基础模型，并将 YOLOv5 训练于 Properly-Wearing Masked Faces Dataset (PWMFD)，应用优化技术包括转移学习、数据增强和 Squeeze-and-Excitation 注意力机制。\n结果表明，基于 YOLOv5s 的优化模型在速度上超过现有模型 SE-YOLOv3 两倍（达到 69 FPS），同时保持相同的 mean Average Precision (mAP) 为 67%，为疫情下的口罩检测提供了高效解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18387v1",
      "published_date": "2024-05-28 17:27:24 UTC",
      "updated_date": "2024-05-28 17:27:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:56:56.249599"
    },
    {
      "arxiv_id": "2405.18386v2",
      "title": "Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning",
      "title_zh": "Instruct-MusicGen：通过指令调优为音乐语言模型解锁文本到音乐编辑",
      "authors": [
        "Yixiao Zhang",
        "Yukara Ikemiya",
        "Woosung Choi",
        "Naoki Murata",
        "Marco A. Martínez-Ramírez",
        "Liwei Lin",
        "Gus Xia",
        "Wei-Hsiang Liao",
        "Yuki Mitsufuji",
        "Simon Dixon"
      ],
      "abstract": "Recent advances in text-to-music editing, which employ text queries to modify\nmusic (e.g.\\ by changing its style or adjusting instrumental components),\npresent unique challenges and opportunities for AI-assisted music creation.\nPrevious approaches in this domain have been constrained by the necessity to\ntrain specific editing models from scratch, which is both resource-intensive\nand inefficient; other research uses large language models to predict edited\nmusic, resulting in imprecise audio reconstruction. To Combine the strengths\nand address these limitations, we introduce Instruct-MusicGen, a novel approach\nthat finetunes a pretrained MusicGen model to efficiently follow editing\ninstructions such as adding, removing, or separating stems. Our approach\ninvolves a modification of the original MusicGen architecture by incorporating\na text fusion module and an audio fusion module, which allow the model to\nprocess instruction texts and audio inputs concurrently and yield the desired\nedited music. Remarkably, Instruct-MusicGen only introduces 8% new parameters\nto the original MusicGen model and only trains for 5K steps, yet it achieves\nsuperior performance across all tasks compared to existing baselines, and\ndemonstrates performance comparable to the models trained for specific tasks.\nThis advancement not only enhances the efficiency of text-to-music editing but\nalso broadens the applicability of music language models in dynamic music\nproduction environments.",
      "tldr_zh": "该研究提出 Instruct-MusicGen，一种通过指令微调(Instruction Tuning)的方法，用于提升音乐语言模型在文本到音乐编辑中的能力，允许用户通过文本指令（如添加、移除或分离音乐 stems）高效修改音乐。方法基于预训练的 MusicGen 模型，添加文本融合模块和音频融合模块，使模型能同时处理指令文本和音频输入，仅引入 8% 新参数并在 5K 步骤内完成训练。实验结果显示，Instruct-MusicGen 在所有任务上优于现有基线，并与特定任务训练模型相当，从而提高了文本到音乐编辑的效率，并扩展了音乐语言模型在动态音乐制作中的应用。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Code and demo are available at:\n  https://github.com/ldzhangyx/instruct-musicgen",
      "pdf_url": "http://arxiv.org/pdf/2405.18386v2",
      "published_date": "2024-05-28 17:27:20 UTC",
      "updated_date": "2024-05-29 17:05:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:57:18.161811"
    },
    {
      "arxiv_id": "2405.18383v2",
      "title": "Brain Tumor Segmentation (BraTS) Challenge 2024: Meningioma Radiotherapy Planning Automated Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Dominic LaBella",
        "Katherine Schumacher",
        "Michael Mix",
        "Kevin Leu",
        "Shan McBurney-Lin",
        "Pierre Nedelec",
        "Javier Villanueva-Meyer",
        "Jonathan Shapey",
        "Tom Vercauteren",
        "Kazumi Chia",
        "Omar Al-Salihi",
        "Justin Leu",
        "Lia Halasz",
        "Yury Velichko",
        "Chunhao Wang",
        "John Kirkpatrick",
        "Scott Floyd",
        "Zachary J. Reitman",
        "Trey Mullikin",
        "Ulas Bagci",
        "Sean Sachdev",
        "Jona A. Hattangadi-Gluth",
        "Tyler Seibert",
        "Nikdokht Farid",
        "Connor Puett",
        "Matthew W. Pease",
        "Kevin Shiue",
        "Syed Muhammad Anwar",
        "Shahriar Faghani",
        "Muhammad Ammar Haider",
        "Pranav Warman",
        "Jake Albrecht",
        "András Jakab",
        "Mana Moassefi",
        "Verena Chung",
        "Alejandro Aristizabal",
        "Alexandros Karargyris",
        "Hasan Kassem",
        "Sarthak Pati",
        "Micah Sheller",
        "Christina Huang",
        "Aaron Coley",
        "Siddharth Ghanta",
        "Alex Schneider",
        "Conrad Sharp",
        "Rachit Saluja",
        "Florian Kofler",
        "Philipp Lohmann",
        "Phillipp Vollmuth",
        "Louis Gagnon",
        "Maruf Adewole",
        "Hongwei Bran Li",
        "Anahita Fathi Kazerooni",
        "Nourel Hoda Tahon",
        "Udunna Anazodo",
        "Ahmed W. Moawad",
        "Bjoern Menze",
        "Marius George Linguraru",
        "Mariam Aboian",
        "Benedikt Wiestler",
        "Ujjwal Baid",
        "Gian-Marco Conte",
        "Andreas M. Rauschecker",
        "Ayman Nada",
        "Aly H. Abayazeed",
        "Raymond Huang",
        "Maria Correia de Verdier",
        "Jeffrey D. Rudie",
        "Spyridon Bakas",
        "Evan Calabrese"
      ],
      "abstract": "The 2024 Brain Tumor Segmentation Meningioma Radiotherapy (BraTS-MEN-RT)\nchallenge aims to advance automated segmentation algorithms using the largest\nknown multi-institutional dataset of radiotherapy planning brain MRIs with\nexpert-annotated target labels for patients with intact or postoperative\nmeningioma that underwent either conventional external beam radiotherapy or\nstereotactic radiosurgery. Each case includes a defaced 3D post-contrast\nT1-weighted radiotherapy planning MRI in its native acquisition space,\naccompanied by a single-label \"target volume\" representing the gross tumor\nvolume (GTV) and any at-risk postoperative site. Target volume annotations\nadhere to established radiotherapy planning protocols, ensuring consistency\nacross cases and institutions. For preoperative meningiomas, the target volume\nencompasses the entire GTV and associated nodular dural tail, while for\npostoperative cases, it includes at-risk resection cavity margins as determined\nby the treating institution. Case annotations were reviewed and approved by\nexpert neuroradiologists and radiation oncologists. Participating teams will\ndevelop, containerize, and evaluate automated segmentation models using this\ncomprehensive dataset. Model performance will be assessed using an adapted\nlesion-wise Dice Similarity Coefficient and the 95% Hausdorff distance. The\ntop-performing teams will be recognized at the Medical Image Computing and\nComputer Assisted Intervention Conference in October 2024. BraTS-MEN-RT is\nexpected to significantly advance automated radiotherapy planning by enabling\nprecise tumor segmentation and facilitating tailored treatment, ultimately\nimproving patient outcomes.",
      "tldr_zh": "2024 年 Brain Tumor Segmentation (BraTS) Challenge 专注于脑膜瘤放射治疗计划的自动分割，名为 BraTS-MEN-RT，利用最大的多机构数据集来推进算法开发，该数据集包括专家标注的 3D 后对比 T1 加权 MRI 图像，涵盖完整或术后脑膜瘤的目标体积 (GTV) 和风险部位。参与团队需开发并容器化自动分割模型，遵循放射治疗协议进行标注，以确保跨机构一致性。模型性能将使用适应后的 lesion-wise Dice Similarity Coefficient 和 95% Hausdorff distance 进行评估，旨在实现精确肿瘤分割并优化个性化治疗，从而提升患者结果。顶尖团队将在 2024 年 Medical Image Computing and Computer Assisted Intervention Conference 上获得表彰。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 9 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2405.18383v2",
      "published_date": "2024-05-28 17:25:43 UTC",
      "updated_date": "2024-08-15 19:04:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:57:19.455302"
    },
    {
      "arxiv_id": "2405.18380v2",
      "title": "OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection for Memory-Efficient LLM Fine-tuning",
      "title_zh": "OwLore：异常值加权的逐层采样低",
      "authors": [
        "Pengxiang Li",
        "Lu Yin",
        "Xiaowei Gao",
        "Shiwei Liu"
      ],
      "abstract": "The rapid advancements in Large Language Models (LLMs) have revolutionized\nvarious natural language processing tasks. However, the substantial size of\nLLMs presents significant challenges in training or fine-tuning. While\nparameter-efficient approaches such as low-rank adaptation (LoRA) have gained\npopularity, they often compromise performance compared to full-rank\nfine-tuning. In this paper, we propose Outlier-weighed Layerwise Sampled\nLow-Rank Projection (OwLore), a new memory-efficient fine-tuning approach,\ninspired by the layerwise outlier distribution of LLMs. Unlike LoRA, which adds\nextra adapters to all layers, OwLore strategically assigns higher sampling\nprobabilities to layers with more outliers, selectively sampling only a few\nlayers and fine-tuning their pre-trained weights. To further increase the\nnumber of fine-tuned layers without a proportional rise in memory costs, we\nincorporate gradient low-rank projection, further boosting the approach's\nperformance. Our extensive experiments across various architectures, including\nLLaMa2, LLaMa3, and Mistral, demonstrate that OwLore consistently outperforms\nbaseline approaches, including full fine-tuning. Specifically, it achieves up\nto a 1.1% average accuracy gain on the Commonsense Reasoning benchmark, a 3.0%\nimprovement on MMLU, and a notable 10% boost on MT-Bench, while being more\nmemory efficient. OwLore allows us to fine-tune LLaMa2-7B with only 21GB of\nmemory. Code is available at https://github.com/pixeli99/OwLore.",
      "tldr_zh": "本研究提出 OwLore，一种基于层级异常值分布的内存高效微调方法，用于优化大型语言模型 (LLMs) 的训练挑战。OwLore 通过为异常值较多的层分配更高采样概率，仅选择性地采样并微调少数层的预训练权重，并结合梯度低秩投影 (gradient low-rank projection)，以实现更多层微调而不显著增加内存消耗。实验结果显示，OwLore 在 LLaMa2、LLaMa3 和 Mistral 等架构上超越基线方法，包括全秩微调，在 Commonsense Reasoning 基准上平均准确率提升 1.1%、MMLU 上提升 3.0%、MT-Bench 上提升 10%，并能用 21GB 内存微调 LLaMa2-7B。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18380v2",
      "published_date": "2024-05-28 17:22:22 UTC",
      "updated_date": "2024-10-12 04:35:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:57:31.751223"
    },
    {
      "arxiv_id": "2405.18377v1",
      "title": "LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models",
      "title_zh": "LLaMA-NAS：针对大型语言模型的高效神经架构搜索",
      "authors": [
        "Anthony Sarah",
        "Sharath Nittur Sridhar",
        "Maciej Szankin",
        "Sairam Sundaresan"
      ],
      "abstract": "The abilities of modern large language models (LLMs) in solving natural\nlanguage processing, complex reasoning, sentiment analysis and other tasks have\nbeen extraordinary which has prompted their extensive adoption. Unfortunately,\nthese abilities come with very high memory and computational costs which\nprecludes the use of LLMs on most hardware platforms. To mitigate this, we\npropose an effective method of finding Pareto-optimal network architectures\nbased on LLaMA2-7B using one-shot NAS. In particular, we fine-tune LLaMA2-7B\nonly once and then apply genetic algorithm-based search to find smaller, less\ncomputationally complex network architectures. We show that, for certain\nstandard benchmark tasks, the pre-trained LLaMA2-7B network is unnecessarily\nlarge and complex. More specifically, we demonstrate a 1.5x reduction in model\nsize and 1.3x speedup in throughput for certain tasks with negligible drop in\naccuracy. In addition to finding smaller, higher-performing network\narchitectures, our method does so more effectively and efficiently than certain\npruning or sparsification techniques. Finally, we demonstrate how quantization\nis complementary to our method and that the size and complexity of the networks\nwe find can be further decreased using quantization. We believe that our work\nprovides a way to automatically create LLMs which can be used on less expensive\nand more readily available hardware platforms.",
      "tldr_zh": "该研究提出 LLaMA-NAS，一种高效的神经架构搜索 (NAS) 方法，针对大型语言模型 (LLMs) 的高计算和内存成本问题，使用 one-shot NAS 和遗传算法 (genetic algorithm) 在 LLaMA2-7B 基础上搜索 Pareto-optimal 网络架构，仅需一次微调即可实现优化。实验结果显示，在某些基准任务上，该方法使模型大小减少 1.5 倍、吞吐量加快 1.3 倍，同时准确率损失微小。相比传统的修剪或稀疏化技术，LLaMA-NAS 更高效，且可与量化 (quantization) 结合，进一步降低模型复杂度，使 LLMs 更易在廉价硬件平台上部署。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18377v1",
      "published_date": "2024-05-28 17:20:44 UTC",
      "updated_date": "2024-05-28 17:20:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:57:44.213529"
    },
    {
      "arxiv_id": "2405.18369v2",
      "title": "PromptWizard: Task-Aware Prompt Optimization Framework",
      "title_zh": "PromptWizard: 任务感知提示优化框架",
      "authors": [
        "Eshaan Agarwal",
        "Joykirat Singh",
        "Vivek Dani",
        "Raghav Magazine",
        "Tanuja Ganu",
        "Akshay Nambi"
      ],
      "abstract": "Large language models (LLMs) have transformed AI across diverse domains, with\nprompting being central to their success in guiding model outputs. However,\nmanual prompt engineering is both labor-intensive and domain-specific,\nnecessitating the need for automated solutions. We introduce PromptWizard, a\nnovel, fully automated framework for discrete prompt optimization, utilizing a\nself-evolving, self-adapting mechanism. Through a feedback-driven critique and\nsynthesis process, PromptWizard achieves an effective balance between\nexploration and exploitation, iteratively refining both prompt instructions and\nin-context examples to generate human-readable, task-specific prompts. This\nguided approach systematically improves prompt quality, resulting in superior\nperformance across 45 tasks. PromptWizard excels even with limited training\ndata, smaller LLMs, and various LLM architectures. Additionally, our cost\nanalysis reveals a substantial reduction in API calls, token usage, and overall\ncost, demonstrating PromptWizard's efficiency, scalability, and advantages over\nexisting prompt optimization strategies.",
      "tldr_zh": "本研究引入了 PromptWizard，一种任务感知的自动提示优化框架，利用自演化机制和反馈驱动的批评与合成过程，平衡探索与利用，迭代改进提示指令和上下文示例，以生成可读的任务特定提示。\n与手动工程相比，PromptWizard 在 45 个任务上实现了优越性能，即使在有限训练数据、更小 LLM 或不同架构下也能保持高效。\n此外，该框架通过减少 API 调用、令牌使用和总体成本，展示了其高效、可扩展的优势，超越了现有优化策略。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18369v2",
      "published_date": "2024-05-28 17:08:31 UTC",
      "updated_date": "2024-10-03 09:45:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:57:54.468833"
    },
    {
      "arxiv_id": "2405.18359v1",
      "title": "Bridging the Gap: Dynamic Learning Strategies for Improving Multilingual Performance in LLMs",
      "title_zh": "弥合差距：动态学习策略用于改善大型语言模型的多语言性能",
      "authors": [
        "Somnath Kumar",
        "Vaibhav Balloli",
        "Mercy Ranjit",
        "Kabir Ahuja",
        "Tanuja Ganu",
        "Sunayana Sitaram",
        "Kalika Bali",
        "Akshay Nambi"
      ],
      "abstract": "Large language models (LLMs) are at the forefront of transforming numerous\ndomains globally. However, their inclusivity and effectiveness remain limited\nfor non-Latin scripts and low-resource languages. This paper tackles the\nimperative challenge of enhancing the multilingual performance of LLMs without\nextensive training or fine-tuning. Through systematic investigation and\nevaluation of diverse languages using popular question-answering (QA) datasets,\nwe present novel techniques that unlock the true potential of LLMs in a\npolyglot landscape. Our approach encompasses three key strategies that yield\nsignificant improvements in multilingual proficiency. First, by meticulously\noptimizing prompts tailored for polyglot LLMs, we unlock their latent\ncapabilities, resulting in substantial performance boosts across languages.\nSecond, we introduce a new hybrid approach that synergizes LLM Retrieval\nAugmented Generation (RAG) with multilingual embeddings and achieves improved\nmultilingual task performance. Finally, we introduce a novel learning approach\nthat dynamically selects the optimal prompt strategy, LLM model, and embedding\nmodel per query at run-time. This dynamic adaptation maximizes the efficacy of\nLLMs across languages, outperforming best static and random strategies.\nAdditionally, our approach adapts configurations in both offline and online\nsettings, and can seamlessly adapt to new languages and datasets, leading to\nsubstantial advancements in multilingual understanding and generation across\ndiverse languages.",
      "tldr_zh": "这篇论文探讨了如何提升大型语言模型（LLMs）在非拉丁脚本和低资源语言上的多语言性能，而无需进行广泛训练或微调。通过系统评估问答（QA）数据集，该研究提出了三种关键策略：首先，优化专为多语言LLMs设计的提示，以释放其潜在能力并显著提高性能；其次，引入一种混合方法，将LLM的Retrieval Augmented Generation (RAG)与多语言embeddings结合，实现更好的多语言任务处理；最后，开发了一个动态学习方法，能在运行时根据查询选择最佳提示策略、LLM模型和embedding模型，从而优于静态或随机策略。该方法在离线和在线环境中均有效，并能无缝适应新语言和数据集，推动LLMs的多语言理解和生成取得实质性进展。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "arXiv admin note: text overlap with arXiv:2305.17740",
      "pdf_url": "http://arxiv.org/pdf/2405.18359v1",
      "published_date": "2024-05-28 16:56:42 UTC",
      "updated_date": "2024-05-28 16:56:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:58:06.951795"
    },
    {
      "arxiv_id": "2405.18358v1",
      "title": "MMCTAgent: Multi-modal Critical Thinking Agent Framework for Complex Visual Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Somnath Kumar",
        "Yash Gadhia",
        "Tanuja Ganu",
        "Akshay Nambi"
      ],
      "abstract": "Recent advancements in Multi-modal Large Language Models (MLLMs) have\nsignificantly improved their performance in tasks combining vision and\nlanguage. However, challenges persist in detailed multi-modal understanding,\ncomprehension of complex tasks, and reasoning over multi-modal information.\nThis paper introduces MMCTAgent, a novel multi-modal critical thinking agent\nframework designed to address the inherent limitations of current MLLMs in\ncomplex visual reasoning tasks. Inspired by human cognitive processes and\ncritical thinking, MMCTAgent iteratively analyzes multi-modal information,\ndecomposes queries, plans strategies, and dynamically evolves its reasoning.\nAdditionally, MMCTAgent incorporates critical thinking elements such as\nverification of final answers and self-reflection through a novel approach that\ndefines a vision-based critic and identifies task-specific evaluation criteria,\nthereby enhancing its decision-making abilities. Through rigorous evaluations\nacross various image and video understanding benchmarks, we demonstrate that\nMMCTAgent (with and without the critic) outperforms both foundational MLLMs and\nother tool-augmented pipelines.",
      "tldr_zh": "这篇论文引入了 MMCTAgent，一种多模态批判性思维代理框架，旨在解决 Multi-modal Large Language Models (MLLMs) 在复杂视觉推理任务中的局限性，如详细多模态理解和推理挑战。该框架受人类认知过程启发，通过迭代分析多模态信息、分解查询、规划策略以及动态演化推理，并整合视觉-based critic 和任务特定评估标准来进行答案验证与自我反思。实验结果显示，MMCTAgent（无论是否包含 critic）在各种图像和视频理解基准上，均优于基础 MLLMs 和其他工具增强管道。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18358v1",
      "published_date": "2024-05-28 16:55:41 UTC",
      "updated_date": "2024-05-28 16:55:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:58:20.191868"
    },
    {
      "arxiv_id": "2405.18350v1",
      "title": "A System for Automatic English Text Expansion",
      "title_zh": "翻译失败",
      "authors": [
        "Silvia García Méndez",
        "Milagros Fernández Gavilanes",
        "Enrique Costa Montenegro",
        "Jonathan Juncal Martínez",
        "Francisco Javier González Castaño",
        "Ehud Reiter"
      ],
      "abstract": "We present an automatic text expansion system to generate English sentences,\nwhich performs automatic Natural Language Generation (NLG) by combining\nlinguistic rules with statistical approaches. Here, \"automatic\" means that the\nsystem can generate coherent and correct sentences from a minimum set of words.\nFrom its inception, the design is modular and adaptable to other languages.\nThis adaptability is one of its greatest advantages. For English, we have\ncreated the highly precise aLexiE lexicon with wide coverage, which represents\na contribution on its own. We have evaluated the resulting NLG library in an\nAugmentative and Alternative Communication (AAC) proof of concept, both\ndirectly (by regenerating corpus sentences) and manually (from annotations)\nusing a popular corpus in the NLG field. We performed a second analysis by\ncomparing the quality of text expansion in English to Spanish, using an ad-hoc\nSpanish-English parallel corpus. The system might also be applied to other\ndomains such as report and news generation.",
      "tldr_zh": "本研究提出了一种自动英文文本扩展系统，通过结合语言规则和统计方法进行自然语言生成 (NLG)，从最小词汇集生成连贯正确的句子。系统设计模块化且可适应其他语言，并创建了高度精确的 aLexiE 词汇库作为一项独立贡献。在增强型和替代性沟通 (AAC) 的概念验证中，该系统通过再生语料库句子和手动评估显示出良好性能，并通过英文-西班牙文平行语料比较验证其扩展质量。该系统还可应用于报告和新闻生成等领域。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18350v1",
      "published_date": "2024-05-28 16:48:05 UTC",
      "updated_date": "2024-05-28 16:48:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:58:31.699769"
    },
    {
      "arxiv_id": "2405.18346v1",
      "title": "Intelligent Clinical Documentation: Harnessing Generative AI for Patient-Centric Clinical Note Generation",
      "title_zh": "智能临床文档：利用生成式 AI 进行患者中心化临床笔记生成",
      "authors": [
        "Anjanava Biswas",
        "Wrick Talukdar"
      ],
      "abstract": "Comprehensive clinical documentation is crucial for effective healthcare\ndelivery, yet it poses a significant burden on healthcare professionals,\nleading to burnout, increased medical errors, and compromised patient safety.\nThis paper explores the potential of generative AI (Artificial Intelligence) to\nstreamline the clinical documentation process, specifically focusing on\ngenerating SOAP (Subjective, Objective, Assessment, Plan) and BIRP (Behavior,\nIntervention, Response, Plan) notes. We present a case study demonstrating the\napplication of natural language processing (NLP) and automatic speech\nrecognition (ASR) technologies to transcribe patient-clinician interactions,\ncoupled with advanced prompting techniques to generate draft clinical notes\nusing large language models (LLMs). The study highlights the benefits of this\napproach, including time savings, improved documentation quality, and enhanced\npatient-centered care. Additionally, we discuss ethical considerations, such as\nmaintaining patient confidentiality and addressing model biases, underscoring\nthe need for responsible deployment of generative AI in healthcare settings.\nThe findings suggest that generative AI has the potential to revolutionize\nclinical documentation practices, alleviating administrative burdens and\nenabling healthcare professionals to focus more on direct patient care.",
      "tldr_zh": "本论文探讨了生成式 AI 在临床文档生成中的应用，旨在减轻医务人员负担、减少医疗错误并提升患者安全。研究通过案例分析，利用自然语言处理(NLP)、自动语音识别(ASR)以及大型语言模型(LLMs)来转录患者-临床医生互动，并采用高级提示技术生成 SOAP (Subjective, Objective, Assessment, Plan) 和 BIRP (Behavior, Intervention, Response, Plan) 笔记。结果显示，此方法可节省时间、改善文档质量并强化患者中心护理，同时强调了伦理考虑，如维护患者保密和处理模型偏见，建议负责任地部署 AI 以革新临床实践。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.18346v1",
      "published_date": "2024-05-28 16:43:41 UTC",
      "updated_date": "2024-05-28 16:43:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:58:44.373587"
    },
    {
      "arxiv_id": "2405.18344v1",
      "title": "The Battle of LLMs: A Comparative Study in Conversational QA Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Aryan Rangapur",
        "Aman Rangapur"
      ],
      "abstract": "Large language models have gained considerable interest for their impressive\nperformance on various tasks. Within this domain, ChatGPT and GPT-4, developed\nby OpenAI, and the Gemini, developed by Google, have emerged as particularly\npopular among early adopters. Additionally, Mixtral by Mistral AI and Claude by\nAnthropic are newly released, further expanding the landscape of advanced\nlanguage models. These models are viewed as disruptive technologies with\napplications spanning customer service, education, healthcare, and finance.\nMore recently, Mistral has entered the scene, captivating users with its unique\nability to generate creative content. Understanding the perspectives of these\nusers is crucial, as they can offer valuable insights into the potential\nstrengths, weaknesses, and overall success or failure of these technologies in\nvarious domains. This research delves into the responses generated by ChatGPT,\nGPT-4, Gemini, Mixtral and Claude across different Conversational QA corpora.\nEvaluation scores were meticulously computed and subsequently compared to\nascertain the overall performance of these models. Our study pinpointed\ninstances where these models provided inaccurate answers to questions, offering\ninsights into potential areas where they might be susceptible to errors. In\nessence, this research provides a comprehensive comparison and evaluation of\nthese state of-the-art language models, shedding light on their capabilities\nwhile also highlighting potential areas for improvement",
      "tldr_zh": "这篇论文比较了ChatGPT、GPT-4、Gemini、Mixtral和Claude等LLMs在对话式QA（Conversational QA）任务中的性能，旨在评估这些模型在不同语料库上的响应质量。研究方法包括计算评估分数、分析模型生成的答案，并识别出不准确实例，以揭示模型的潜在弱点。结果显示，这些LLMs在某些领域表现出色，但也暴露了易出错区域，为未来改进LLMs的应用提供了宝贵见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.7, I.m"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 4 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2405.18344v1",
      "published_date": "2024-05-28 16:42:43 UTC",
      "updated_date": "2024-05-28 16:42:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:59:05.168610"
    },
    {
      "arxiv_id": "2406.01618v1",
      "title": "FinEmbedDiff: A Cost-Effective Approach of Classifying Financial Documents with Vector Sampling using Multi-modal Embedding Models",
      "title_zh": "翻译失败",
      "authors": [
        "Anjanava Biswas",
        "Wrick Talukdar"
      ],
      "abstract": "Accurate classification of multi-modal financial documents, containing text,\ntables, charts, and images, is crucial but challenging. Traditional text-based\napproaches often fail to capture the complex multi-modal nature of these\ndocuments. We propose FinEmbedDiff, a cost-effective vector sampling method\nthat leverages pre-trained multi-modal embedding models to classify financial\ndocuments. Our approach generates multi-modal embedding vectors for documents,\nand compares new documents with pre-computed class embeddings using vector\nsimilarity measures. Evaluated on a large dataset, FinEmbedDiff achieves\ncompetitive classification accuracy compared to state-of-the-art baselines\nwhile significantly reducing computational costs. The method exhibits strong\ngeneralization capabilities, making it a practical and scalable solution for\nreal-world financial applications.",
      "tldr_zh": "该论文提出了一种名为 FinEmbedDiff 的成本有效方法，用于分类包含文本、表格、图表和图像的多模态金融文档，以解决传统文本方法无法捕捉其复杂性的问题。该方法利用预训练的多模态 embedding models 生成文档嵌入向量，并通过 vector sampling 和向量相似度度量，将新文档与预计算的类嵌入进行比较。在大型数据集上的评估显示，FinEmbedDiff 实现了与最先进基线相当的分类准确率，同时显著降低了计算成本，并展示了强大的泛化能力，使其成为实用的金融应用解决方案。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "10 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.01618v1",
      "published_date": "2024-05-28 16:34:24 UTC",
      "updated_date": "2024-05-28 16:34:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:59:08.162951"
    },
    {
      "arxiv_id": "2405.18335v1",
      "title": "Interpretable classification of wiki-review streams",
      "title_zh": "翻译失败",
      "authors": [
        "Silvia García Méndez",
        "Fátima Leal",
        "Benedita Malheiro",
        "Juan Carlos Burguillo Rial"
      ],
      "abstract": "Wiki articles are created and maintained by a crowd of editors, producing a\ncontinuous stream of reviews. Reviews can take the form of additions, reverts,\nor both. This crowdsourcing model is exposed to manipulation since neither\nreviews nor editors are automatically screened and purged. To protect articles\nagainst vandalism or damage, the stream of reviews can be mined to classify\nreviews and profile editors in real-time. The goal of this work is to\nanticipate and explain which reviews to revert. This way, editors are informed\nwhy their edits will be reverted. The proposed method employs stream-based\nprocessing, updating the profiling and classification models on each incoming\nevent. The profiling uses side and content-based features employing Natural\nLanguage Processing, and editor profiles are incrementally updated based on\ntheir reviews. Since the proposed method relies on self-explainable\nclassification algorithms, it is possible to understand why a review has been\nclassified as a revert or a non-revert. In addition, this work contributes an\nalgorithm for generating synthetic data for class balancing, making the final\nclassification fairer. The proposed online method was tested with a real data\nset from Wikivoyage, which was balanced through the aforementioned synthetic\ndata generation. The results attained near-90 % values for all evaluation\nmetrics (accuracy, precision, recall, and F-measure).",
      "tldr_zh": "这篇论文提出了一种可解释的分类方法，用于实时处理维基文章的评论流（wiki-review streams），以预测和解释哪些评论需要撤销，从而告知编辑者原因。方法采用流式处理（stream-based processing），结合 Natural Language Processing 提取侧面和内容特征，并通过增量更新编辑者画像来实现分类。论文还贡献了一个生成合成数据的算法，用于类平衡，提高分类公平性。在 Wikivoyage 的真实数据集上测试，该方法在准确率、精确率、召回率和 F-measure 等指标上均达到了近 90%。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18335v1",
      "published_date": "2024-05-28 16:28:58 UTC",
      "updated_date": "2024-05-28 16:28:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:59:19.884230"
    },
    {
      "arxiv_id": "2405.18330v2",
      "title": "Frustratingly Easy Test-Time Adaptation of Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Matteo Farina",
        "Gianni Franchi",
        "Giovanni Iacca",
        "Massimiliano Mancini",
        "Elisa Ricci"
      ],
      "abstract": "Vision-Language Models seamlessly discriminate among arbitrary semantic\ncategories, yet they still suffer from poor generalization when presented with\nchallenging examples. For this reason, Episodic Test-Time Adaptation (TTA)\nstrategies have recently emerged as powerful techniques to adapt VLMs in the\npresence of a single unlabeled image. The recent literature on TTA is dominated\nby the paradigm of prompt tuning by Marginal Entropy Minimization, which,\nrelying on online backpropagation, inevitably slows down inference while\nincreasing memory. In this work, we theoretically investigate the properties of\nthis approach and unveil that a surprisingly strong TTA method lies dormant and\nhidden within it. We term this approach ZERO (TTA with \"zero\" temperature),\nwhose design is both incredibly effective and frustratingly simple: augment N\ntimes, predict, retain the most confident predictions, and marginalize after\nsetting the Softmax temperature to zero. Remarkably, ZERO requires a single\nbatched forward pass through the vision encoder only and no backward passes. We\nthoroughly evaluate our approach following the experimental protocol\nestablished in the literature and show that ZERO largely surpasses or compares\nfavorably w.r.t. the state-of-the-art while being almost 10x faster and 13x\nmore memory-friendly than standard Test-Time Prompt Tuning. Thanks to its\nsimplicity and comparatively negligible computation, ZERO can serve as a strong\nbaseline for future work in this field. The code is available at\nhttps://github.com/FarinaMatteo/zero.",
      "tldr_zh": "该论文探讨了视觉语言模型（VLMs）在面对挑战性示例时的泛化问题，并提出了一种简单有效的测试时适应（TTA）方法，名为 ZERO。ZERO 通过图像增强 N 次、保留最自信预测并将 Softmax 温度设置为零来边缘化预测，仅需一个批处理的前向传递，而无需反向传播。实验结果显示，ZERO 在标准协议下大幅超越或与最先进方法相当，同时比传统的测试时提示调整快 10 倍、内存友好 13 倍，为未来 TTA 研究提供了一个强大基线。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Camera-ready version for NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.18330v2",
      "published_date": "2024-05-28 16:24:47 UTC",
      "updated_date": "2024-11-02 15:29:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:59:31.395442"
    },
    {
      "arxiv_id": "2405.18327v1",
      "title": "Histopathology Based AI Model Predicts Anti-Angiogenic Therapy Response in Renal Cancer Clinical Trial",
      "title_zh": "基于组织病理学的 AI 模型预测肾癌临床试验中的抗血管生成疗法反应",
      "authors": [
        "Jay Jasti",
        "Hua Zhong",
        "Vandana Panwar",
        "Vipul Jarmale",
        "Jeffrey Miyata",
        "Deyssy Carrillo",
        "Alana Christie",
        "Dinesh Rakheja",
        "Zora Modrusan",
        "Edward Ernest Kadel III",
        "Niha Beig",
        "Mahrukh Huseni",
        "James Brugarolas",
        "Payal Kapur",
        "Satwik Rajaram"
      ],
      "abstract": "Predictive biomarkers of treatment response are lacking for metastatic clear\ncell renal cell carcinoma (ccRCC), a tumor type that is treated with\nangiogenesis inhibitors, immune checkpoint inhibitors, mTOR inhibitors and a\nHIF2 inhibitor. The Angioscore, an RNA-based quantification of angiogenesis, is\narguably the best candidate to predict anti-angiogenic (AA) response. However,\nthe clinical adoption of transcriptomic assays faces several challenges\nincluding standardization, time delay, and high cost. Further, ccRCC tumors are\nhighly heterogenous, and sampling multiple areas for sequencing is impractical.\nHere we present a novel deep learning (DL) approach to predict the Angioscore\nfrom ubiquitous histopathology slides. To overcome the lack of\ninterpretability, one of the biggest limitations of typical DL models, our\nmodel produces a visual vascular network which is the basis of the model's\nprediction. To test its reliability, we applied this model to multiple cohorts\nincluding a clinical trial dataset. Our model accurately predicts the RNA-based\nAngioscore on multiple independent cohorts (spearman correlations of 0.77 and\n0.73). Further, the predictions help unravel meaningful biology such as\nassociation of angiogenesis with grade, stage, and driver mutation status.\nFinally, we find our model can predict response to AA therapy, in both a\nreal-world cohort and the IMmotion150 clinical trial. The predictive power of\nour model vastly exceeds that of CD31, a marker of vasculature, and nearly\nrivals the performance (c-index 0.66 vs 0.67) of the ground truth RNA-based\nAngioscore at a fraction of the cost. By providing a robust yet interpretable\nprediction of the Angioscore from histopathology slides alone, our approach\noffers insights into angiogenesis biology and AA treatment response.",
      "tldr_zh": "本研究提出了一种基于深度学习的AI模型，从病理学切片预测肾透明细胞癌(ccRCC)的Angioscore，以评估抗血管生成(AA)治疗响应，解决了RNA-based方法面临的标准化、成本和时间问题。该模型通过生成可视化的血管网络，提高了预测的可解释性，并在多个独立队列上准确预测Angioscore（Spearman相关系数0.77和0.73）。实验结果显示，模型能揭示血管生成与肿瘤级别、分期及驱动突变状态的关联，并在真实世界队列和IMmotion150临床试验中预测AA治疗响应，其性能几乎与RNA-based Angioscore相当（c-index 0.66 vs 0.67），但成本显著降低。该方法为ccRCC治疗决策提供了鲁棒且经济的生物标志物预测工具。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "q-bio.QM",
      "comment": "19 pages, 4 Figures",
      "pdf_url": "http://arxiv.org/pdf/2405.18327v1",
      "published_date": "2024-05-28 16:21:20 UTC",
      "updated_date": "2024-05-28 16:21:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:59:47.398747"
    },
    {
      "arxiv_id": "2405.18322v1",
      "title": "SCE-MAE: Selective Correspondence Enhancement with Masked Autoencoder for Self-Supervised Landmark Estimation",
      "title_zh": "翻译失败",
      "authors": [
        "Kejia Yin",
        "Varshanth R. Rao",
        "Ruowei Jiang",
        "Xudong Liu",
        "Parham Aarabi",
        "David B. Lindell"
      ],
      "abstract": "Self-supervised landmark estimation is a challenging task that demands the\nformation of locally distinct feature representations to identify sparse facial\nlandmarks in the absence of annotated data. To tackle this task, existing\nstate-of-the-art (SOTA) methods (1) extract coarse features from backbones that\nare trained with instance-level self-supervised learning (SSL) paradigms, which\nneglect the dense prediction nature of the task, (2) aggregate them into\nmemory-intensive hypercolumn formations, and (3) supervise lightweight\nprojector networks to naively establish full local correspondences among all\npairs of spatial features. In this paper, we introduce SCE-MAE, a framework\nthat (1) leverages the MAE, a region-level SSL method that naturally better\nsuits the landmark prediction task, (2) operates on the vanilla feature map\ninstead of on expensive hypercolumns, and (3) employs a Correspondence\nApproximation and Refinement Block (CARB) that utilizes a simple density peak\nclustering algorithm and our proposed Locality-Constrained Repellence Loss to\ndirectly hone only select local correspondences. We demonstrate through\nextensive experiments that SCE-MAE is highly effective and robust,\noutperforming existing SOTA methods by large margins of approximately 20%-44%\non the landmark matching and approximately 9%-15% on the landmark detection\ntasks.",
      "tldr_zh": "该论文针对自监督 Landmark Estimation 的挑战，提出了一种名为 SCE-MAE 的框架，以解决现有方法在提取特征和建立局部对应关系方面的不足。SCE-MAE 利用 Masked Autoencoder (MAE) 作为区域级 Self-Supervised Learning (SSL) 方法，直接在 vanilla feature map 上操作，并引入 Correspondence Approximation and Refinement Block (CARB)，该块结合密度峰值聚类算法和 Locality-Constrained Repellence Loss 来精炼选定的局部对应关系，从而提高效率和准确性。通过广泛实验，SCE-MAE 在 Landmark Matching 任务上比现有 SOTA 方法提升约 20%-44%，在 Landmark Detection 任务上提升约 9%-15%。这为无标注数据下的面部地标估计提供了更鲁棒的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at CVPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.18322v1",
      "published_date": "2024-05-28 16:14:10 UTC",
      "updated_date": "2024-05-28 16:14:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T12:59:55.543671"
    },
    {
      "arxiv_id": "2405.18320v2",
      "title": "Self-Supervised Learning Based Handwriting Verification",
      "title_zh": "翻译失败",
      "authors": [
        "Mihir Chauhan",
        "Mohammad Abuzar Hashemi",
        "Abhishek Satbhai",
        "Mir Basheer Ali",
        "Bina Ramamurthy",
        "Mingchen Gao",
        "Siwei Lyu",
        "Sargur Srihari"
      ],
      "abstract": "We present SSL-HV: Self-Supervised Learning approaches applied to the task of\nHandwriting Verification. This task involves determining whether a given pair\nof handwritten images originate from the same or different writer distribution.\nWe have compared the performance of multiple generative, contrastive SSL\napproaches against handcrafted feature extractors and supervised learning on\nCEDAR AND dataset. We show that ResNet based Variational Auto-Encoder (VAE)\noutperforms other generative approaches achieving 76.3% accuracy, while\nResNet-18 fine-tuned using Variance-Invariance-Covariance Regularization\n(VICReg) outperforms other contrastive approaches achieving 78% accuracy. Using\na pre-trained VAE and VICReg for the downstream task of writer verification we\nobserved a relative improvement in accuracy of 6.7% and 9% over ResNet-18\nsupervised baseline with 10% writer labels.",
      "tldr_zh": "本研究提出 SSL-HV 方法，利用 Self-Supervised Learning 进行 Handwriting Verification，任务是判断一对手写图像是否来自同一作者。实验在 CEDAR 和数据集上比较了多种生成式（如 ResNet 基于的 Variational Auto-Encoder (VAE)）和对比式（如 Variance-Invariance-Covariance Regularization (VICReg) 微调的 ResNet-18）方法，结果显示 VAE 达到 76.3% 准确率，VICReg 达到 78% 准确率。相比于监督学习的 ResNet-18 基线，使用预训练的 VAE 和 VICReg 在下游任务上分别提高了 6.7% 和 9% 的准确率，仅需 10% 的标签，这证明了自监督学习在减少标注需求方面的优势。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 2 figures, 2 tables, Accepted at Irish Machine Vision and\n  Image Processing Conference 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.18320v2",
      "published_date": "2024-05-28 16:11:11 UTC",
      "updated_date": "2024-08-01 17:43:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:00:09.655107"
    },
    {
      "arxiv_id": "2405.18315v1",
      "title": "DSDL: Data Set Description Language for Bridging Modalities and Tasks in AI Data",
      "title_zh": "翻译失败",
      "authors": [
        "Bin Wang",
        "Linke Ouyang",
        "Fan Wu",
        "Wenchang Ning",
        "Xiao Han",
        "Zhiyuan Zhao",
        "Jiahui Peng",
        "Yiying Jiang",
        "Dahua Lin",
        "Conghui He"
      ],
      "abstract": "In the era of artificial intelligence, the diversity of data modalities and\nannotation formats often renders data unusable directly, requiring\nunderstanding and format conversion before it can be used by researchers or\ndevelopers with different needs. To tackle this problem, this article\nintroduces a framework called Dataset Description Language (DSDL) that aims to\nsimplify dataset processing by providing a unified standard for AI datasets.\nDSDL adheres to the three basic practical principles of generic, portable, and\nextensible, using a unified standard to express data of different modalities\nand structures, facilitating the dissemination of AI data, and easily extending\nto new modalities and tasks. The standardized specifications of DSDL reduce the\nworkload for users in data dissemination, processing, and usage. To further\nimprove user convenience, we provide predefined DSDL templates for various\ntasks, convert mainstream datasets to comply with DSDL specifications, and\nprovide comprehensive documentation and DSDL tools. These efforts aim to\nsimplify the use of AI data, thereby improving the efficiency of AI\ndevelopment.",
      "tldr_zh": "这篇论文引入了 DSDL（Data Set Description Language），一个框架旨在解决 AI 数据模态和注释格式多样性导致的可用性问题，通过提供统一的标准化标准简化数据集处理。DSDL 遵循 generic、portable 和 extensible 的基本原则，允许表达不同模态和结构的数据，便于数据传播并易于扩展到新模态和任务。作者提供了预定义模板、将主流数据集转换为 DSDL 规范的工具，以及全面文档，以减少用户在数据传播、处理和使用中的工作量。最终，这有助于提高 AI 开发的效率。",
      "categories": [
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18315v1",
      "published_date": "2024-05-28 16:07:45 UTC",
      "updated_date": "2024-05-28 16:07:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:00:23.029888"
    },
    {
      "arxiv_id": "2405.18300v1",
      "title": "CompetEvo: Towards Morphological Evolution from Competition",
      "title_zh": "翻译失败",
      "authors": [
        "Kangyao Huang",
        "Di Guo",
        "Xinyu Zhang",
        "Xiangyang Ji",
        "Huaping Liu"
      ],
      "abstract": "Training an agent to adapt to specific tasks through co-optimization of\nmorphology and control has widely attracted attention. However, whether there\nexists an optimal configuration and tactics for agents in a multiagent\ncompetition scenario is still an issue that is challenging to definitively\nconclude. In this context, we propose competitive evolution (CompetEvo), which\nco-evolves agents' designs and tactics in confrontation. We build arenas\nconsisting of three animals and their evolved derivatives, placing agents with\ndifferent morphologies in direct competition with each other. The results\nreveal that our method enables agents to evolve a more suitable design and\nstrategy for fighting compared to fixed-morph agents, allowing them to obtain\nadvantages in combat scenarios. Moreover, we demonstrate the amazing and\nimpressive behaviors that emerge when confrontations are conducted under\nasymmetrical morphs.",
      "tldr_zh": "该论文提出 CompetEvo 方法，通过多代理竞争环境来共同演化代理的形态（morphology）和策略（tactics），旨在解决代理在特定任务中优化配置的问题。研究构建了包含三种动物及其演化衍生物的竞技场，让不同形态的代理直接对抗。结果表明，CompetEvo 使代理演化出更适合战斗的设计和策略，比固定形态代理在竞争场景中提升了优势，并在不对称形态下展现出令人印象深刻的新兴行为。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18300v1",
      "published_date": "2024-05-28 15:53:02 UTC",
      "updated_date": "2024-05-28 15:53:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:00:33.786967"
    },
    {
      "arxiv_id": "2405.18299v4",
      "title": "Deep Learning Innovations for Underwater Waste Detection: An In-Depth Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Jaskaran Singh Walia",
        "Pavithra L K"
      ],
      "abstract": "Addressing the issue of submerged underwater trash is crucial for\nsafeguarding aquatic ecosystems and preserving marine life. While identifying\ndebris present on the surface of water bodies is straightforward, assessing the\nunderwater submerged waste is a challenge due to the image distortions caused\nby factors such as light refraction, absorption, suspended particles, color\nshifts, and occlusion. This paper conducts a comprehensive review of\nstate-of-the-art architectures and on the existing datasets to establish a\nbaseline for submerged waste and trash detection. The primary goal remains to\nestablish the benchmark of the object localization techniques to be leveraged\nby advanced underwater sensors and autonomous underwater vehicles. The ultimate\nobjective is to explore the underwater environment, to identify, and remove\nunderwater debris. The absence of benchmarks (dataset or algorithm) in many\nresearches emphasizes the need for a more robust algorithmic solution. Through\nthis research, we aim to give performance comparative analysis of various\nunderwater trash detection algorithms.",
      "tldr_zh": "这篇论文深入分析了深层学习（Deep Learning）在水下废物检测中的创新，强调了光折射、吸收和悬浮颗粒等因素导致的图像失真问题，以及保护水生生态系统的重要性。通过回顾最先进架构和现有数据集，该研究建立了水下废物检测的基准，并比较了各种算法的性能。最终目标是为对象定位（Object Localization）技术提供标准参考，支持先进水下传感器和自主水下车辆（Autonomous Underwater Vehicles）识别和移除水下碎片。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18299v4",
      "published_date": "2024-05-28 15:51:18 UTC",
      "updated_date": "2024-11-20 23:23:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:00:49.726067"
    },
    {
      "arxiv_id": "2405.18291v1",
      "title": "FedSAC: Dynamic Submodel Allocation for Collaborative Fairness in Federated Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Zihui Wang",
        "Zheng Wang",
        "Lingjuan Lyu",
        "Zhaopeng Peng",
        "Zhicheng Yang",
        "Chenglu Wen",
        "Rongshan Yu",
        "Cheng Wang",
        "Xiaoliang Fan"
      ],
      "abstract": "Collaborative fairness stands as an essential element in federated learning\nto encourage client participation by equitably distributing rewards based on\nindividual contributions. Existing methods primarily focus on adjusting\ngradient allocations among clients to achieve collaborative fairness. However,\nthey frequently overlook crucial factors such as maintaining consistency across\nlocal models and catering to the diverse requirements of high-contributing\nclients. This oversight inevitably decreases both fairness and model accuracy\nin practice. To address these issues, we propose FedSAC, a novel Federated\nlearning framework with dynamic Submodel Allocation for Collaborative fairness,\nbacked by a theoretical convergence guarantee. First, we present the concept of\n\"bounded collaborative fairness (BCF)\", which ensures fairness by tailoring\nrewards to individual clients based on their contributions. Second, to\nimplement the BCF, we design a submodel allocation module with a theoretical\nguarantee of fairness. This module incentivizes high-contributing clients with\nhigh-performance submodels containing a diverse range of crucial neurons,\nthereby preserving consistency across local models. Third, we further develop a\ndynamic aggregation module to adaptively aggregate submodels, ensuring the\nequitable treatment of low-frequency neurons and consequently enhancing overall\nmodel accuracy. Extensive experiments conducted on three public benchmarks\ndemonstrate that FedSAC outperforms all baseline methods in both fairness and\nmodel accuracy. We see this work as a significant step towards incentivizing\nbroader client participation in federated learning. The source code is\navailable at https://github.com/wangzihuixmu/FedSAC.",
      "tldr_zh": "该论文提出 FedSAC，一种新型 Federated Learning 框架，通过动态 Submodel Allocation 模块来提升 Collaborative Fairness，确保根据客户端贡献公平分配奖励，同时维护本地模型一致性。FedSAC 引入 Bounded Collaborative Fairness (BCF) 概念，并开发动态聚合模块来自适应处理低频神经元，提高整体模型准确性。实验在三个公共基准上表明，FedSAC 在公平性和准确性上均优于基线方法，为促进更广泛的客户端参与提供了重要支持。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by KDD'24",
      "pdf_url": "http://arxiv.org/pdf/2405.18291v1",
      "published_date": "2024-05-28 15:43:29 UTC",
      "updated_date": "2024-05-28 15:43:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:00:59.159716"
    },
    {
      "arxiv_id": "2405.18289v1",
      "title": "Highway Reinforcement Learning",
      "title_zh": "Highway 强化学习",
      "authors": [
        "Yuhui Wang",
        "Miroslav Strupl",
        "Francesco Faccio",
        "Qingyuan Wu",
        "Haozhe Liu",
        "Michał Grudzień",
        "Xiaoyang Tan",
        "Jürgen Schmidhuber"
      ],
      "abstract": "Learning from multi-step off-policy data collected by a set of policies is a\ncore problem of reinforcement learning (RL). Approaches based on importance\nsampling (IS) often suffer from large variances due to products of IS ratios.\nTypical IS-free methods, such as $n$-step Q-learning, look ahead for $n$ time\nsteps along the trajectory of actions (where $n$ is called the lookahead depth)\nand utilize off-policy data directly without any additional adjustment. They\nwork well for proper choices of $n$. We show, however, that such IS-free\nmethods underestimate the optimal value function (VF), especially for large\n$n$, restricting their capacity to efficiently utilize information from distant\nfuture time steps. To overcome this problem, we introduce a novel, IS-free,\nmulti-step off-policy method that avoids the underestimation issue and\nconverges to the optimal VF. At its core lies a simple but non-trivial\n\\emph{highway gate}, which controls the information flow from the distant\nfuture by comparing it to a threshold. The highway gate guarantees convergence\nto the optimal VF for arbitrary $n$ and arbitrary behavioral policies. It gives\nrise to a novel family of off-policy RL algorithms that safely learn even when\n$n$ is very large, facilitating rapid credit assignment from the far future to\nthe past. On tasks with greatly delayed rewards, including video games where\nthe reward is given only at the end of the game, our new methods outperform\nmany existing multi-step off-policy algorithms.",
      "tldr_zh": "本研究解决了强化学习（RL）中从多步离策略数据学习的核心问题，即基于重要性采样（IS）的传统方法方差过大，而IS-free方法如n-step Q-learning则容易低估最优价值函数（VF），尤其在n较大时。论文提出了一种新型IS-free多步离策略方法，核心是引入highway gate机制，该机制通过比较远期未来信息与阈值来控制信息流，确保算法收敛到最优VF，且适用于任意n和行为策略。相比现有算法，该方法实现了更快的信用分配，并在延迟奖励任务（如视频游戏）上表现出色，显著提升了学习效率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18289v1",
      "published_date": "2024-05-28 15:42:45 UTC",
      "updated_date": "2024-05-28 15:42:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:01:10.293231"
    },
    {
      "arxiv_id": "2405.18281v2",
      "title": "MODL: Multilearner Online Deep Learning",
      "title_zh": "MODL：多学习器在线深度学习",
      "authors": [
        "Antonios Valkanas",
        "Boris N. Oreshkin",
        "Mark Coates"
      ],
      "abstract": "Online deep learning tackles the challenge of learning from data streams by\nbalancing two competing goals: fast learning and deep learning. However,\nexisting research primarily emphasizes deep learning solutions, which are more\nadept at handling the ``deep'' aspect than the ``fast'' aspect of online\nlearning. In this work, we introduce an alternative paradigm through a hybrid\nmultilearner approach. We begin by developing a fast online logistic regression\nlearner, which operates without relying on backpropagation. It leverages\nclosed-form recursive updates of model parameters, efficiently addressing the\nfast learning component of the online learning challenge. This approach is\nfurther integrated with a cascaded multilearner design, where shallow and deep\nlearners are co-trained in a cooperative, synergistic manner to solve the\nonline learning problem. We demonstrate that this approach achieves\nstate-of-the-art performance on standard online learning datasets. We make our\ncode available: https://github.com/AntonValk/MODL",
      "tldr_zh": "本文提出 MODL，一种多学习者在线深度学习框架，旨在平衡快速学习和深度学习的挑战，通过结合快速在线逻辑回归学习器和级联多学习者设计来实现。快速学习器不依赖 backpropagation，而是使用闭式递归更新参数，与浅层和深层学习者协同训练，以高效处理数据流。实验结果显示，该方法在标准在线学习数据集上达到了最先进性能，并提供了开源代码（https://github.com/AntonValk/MODL）。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18281v2",
      "published_date": "2024-05-28 15:34:33 UTC",
      "updated_date": "2025-03-21 03:21:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:01:22.762251"
    },
    {
      "arxiv_id": "2405.18272v2",
      "title": "Metaheuristics and Large Language Models Join Forces: Toward an Integrated Optimization Approach",
      "title_zh": "元启发式算法与大型语言模型联手：迈向集成优化方法",
      "authors": [
        "Camilo Chacón Sartori",
        "Christian Blum",
        "Filippo Bistaffa",
        "Guillem Rodríguez Corominas"
      ],
      "abstract": "Since the rise of Large Language Models (LLMs) a couple of years ago,\nresearchers in metaheuristics (MHs) have wondered how to use their power in a\nbeneficial way within their algorithms. This paper introduces a novel approach\nthat leverages LLMs as pattern recognition tools to improve MHs. The resulting\nhybrid method, tested in the context of a social network-based combinatorial\noptimization problem, outperforms existing state-of-the-art approaches that\ncombine machine learning with MHs regarding the obtained solution quality. By\ncarefully designing prompts, we demonstrate that the output obtained from LLMs\ncan be used as problem knowledge, leading to improved results. Lastly, we\nacknowledge LLMs' potential drawbacks and limitations and consider it essential\nto examine them to advance this type of research further. Our method can be\nreproduced using a tool available at: https://github.com/camilochs/optipattern.",
      "tldr_zh": "该研究提出了一种新型整合方法，将元启发式算法（metaheuristics, MHs）和大型语言模型（Large Language Models, LLMs）结合，使用 LLMs 作为模式识别工具来提升 MHs 的性能。作者通过精心设计提示，将 LLMs 的输出转化为问题知识，并在社交网络-based 组合优化问题上进行测试，结果显示该混合方法在解决方案质量上优于现有的结合机器学习和 MHs 的基准方法。论文还讨论了 LLMs 的潜在缺点和限制，以推动此类研究的进一步发展，并提供了一个可重现的工具（https://github.com/camilochs/optipattern）。",
      "categories": [
        "cs.AI",
        "68T20"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18272v2",
      "published_date": "2024-05-28 15:23:46 UTC",
      "updated_date": "2025-02-12 10:22:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:01:34.372758"
    },
    {
      "arxiv_id": "2405.18258v1",
      "title": "Text-only Synthesis for Image Captioning",
      "title_zh": "翻译失败",
      "authors": [
        "Qing Zhou",
        "Junlin Huang",
        "Qiang Li",
        "Junyu Gao",
        "Qi Wang"
      ],
      "abstract": "From paired image-text training to text-only training for image captioning,\nthe pursuit of relaxing the requirements for high-cost and large-scale\nannotation of good quality data remains consistent. In this paper, we propose\nText-only Synthesis for Image Captioning (ToCa), which further advances this\nrelaxation with fewer human labor and less computing time. Specifically, we\ndeconstruct caption text into structures and lexical words, which serve as the\nfundamental components of the caption. By combining different structures and\nlexical words as inputs to the large language model, massive captions that\ncontain various patterns of lexical words are generated. This method not only\napproaches the target domain but also surpasses it by generating new captions,\nthereby enhancing the zero-shot generalization ability of the model.\nConsidering the different levels of data access in the real world, we define\nthree synthesis scenarios: cross-domain synthesis, in-domain synthesis, and\ndata-efficient synthesis. Experiments in these scenarios demonstrate the\ngeneralizability, transferability and practicability of ToCa with a nearly 5\nCIDEr improvement for zero-shot cross-domain captioning and a maximum increase\nof over 20 CIDEr for data-efficient captioning.",
      "tldr_zh": "本论文提出了一种名为 ToCa 的文本-only 合成方法，用于图像描述任务，旨在减少对高成本图像-文本配对数据的依赖，仅通过文本生成来提升模型性能。具体而言，ToCa 将标题文本分解为结构和词汇词，利用 large language model 结合不同组合生成多样化标题，从而增强模型的零-shot 泛化能力。论文定义了三种合成场景：cross-domain synthesis、in-domain synthesis 和 data-efficient synthesis，并在实验中证明 ToCa 的泛化性和实用性，例如在零-shot 跨域描述中 CIDEr 指标提高了近 5 分，在数据高效场景中提升超过 20 分。总的来说，此方法为高效图像描述训练提供了新途径，降低了数据标注需求。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18258v1",
      "published_date": "2024-05-28 15:11:17 UTC",
      "updated_date": "2024-05-28 15:11:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:01:46.400037"
    },
    {
      "arxiv_id": "2405.18248v1",
      "title": "Extreme Value Monte Carlo Tree Search",
      "title_zh": "极值蒙特卡洛树搜索",
      "authors": [
        "Masataro Asai",
        "Stephen Wissow"
      ],
      "abstract": "Despite being successful in board games and reinforcement learning (RL), UCT,\na Monte-Carlo Tree Search (MCTS) combined with UCB1 Multi-Armed Bandit (MAB),\nhas had limited success in domain-independent planning until recently. Previous\nwork showed that UCB1, designed for $[0,1]$-bounded rewards, is not appropriate\nfor estimating the distance-to-go which are potentially unbounded in\n$\\mathbb{R}$, such as heuristic functions used in classical planning, then\nproposed combining MCTS with MABs designed for Gaussian reward distributions\nand successfully improved the performance. In this paper, we further sharpen\nour understanding of ideal bandits for planning tasks. Existing work has two\nissues: First, while Gaussian MABs no longer over-specify the distances as\n$h\\in [0,1]$, they under-specify them as $h\\in [-\\infty,\\infty]$ while they are\nnon-negative and can be further bounded in some cases. Second, there is no\ntheoretical justifications for Full-Bellman backup (Schulte & Keller, 2014)\nthat backpropagates minimum/maximum of samples. We identified \\emph{extreme\nvalue} statistics as a theoretical framework that resolves both issues at once\nand propose two bandits, UCB1-Uniform/Power, and apply them to MCTS for\nclassical planning. We formally prove their regret bounds and empirically\ndemonstrate their performance in classical planning.",
      "tldr_zh": "本研究针对 Monte Carlo Tree Search (MCTS) 在领域无关规划中的局限性，指出 UCB1 Multi-Armed Bandit (MAB) 不适合处理无界距离估计问题，并提出基于 extreme value statistics 的新框架来解决 Gaussian MABs 的过度或不足指定问题。作者设计了两种新 Bandits：UCB1-Uniform 和 UCB1-Power，并将其应用于 MCTS 用于 classical planning，同时证明了它们的 regret bounds。实验结果显示，该方法在经典规划任务中显著提升了性能，提供更准确的理论基础。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2305.09840",
      "pdf_url": "http://arxiv.org/pdf/2405.18248v1",
      "published_date": "2024-05-28 14:58:43 UTC",
      "updated_date": "2024-05-28 14:58:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:01:59.869429"
    },
    {
      "arxiv_id": "2405.18246v3",
      "title": "Utilitarian Algorithm Configuration for Infinite Parameter Spaces",
      "title_zh": "翻译失败",
      "authors": [
        "Devon Graham",
        "Kevin Leyton-Brown"
      ],
      "abstract": "Utilitarian algorithm configuration is a general-purpose technique for\nautomatically searching the parameter space of a given algorithm to optimize\nits performance, as measured by a given utility function, on a given set of\ninputs. Recently introduced utilitarian configuration procedures offer\noptimality guarantees about the returned parameterization while provably\nadapting to the hardness of the underlying problem. However, the applicability\nof these approaches is severely limited by the fact that they only search a\nfinite, relatively small set of parameters. They cannot effectively search the\nconfiguration space of algorithms with continuous or uncountable parameters. In\nthis paper we introduce a new procedure, which we dub COUP (Continuous,\nOptimistic Utilitarian Procrastination). COUP is designed to search infinite\nparameter spaces efficiently to find good configurations quickly. Furthermore,\nCOUP maintains the theoretical benefits of previous utilitarian configuration\nprocedures when applied to finite parameter spaces but is significantly faster,\nboth provably and experimentally.",
      "tldr_zh": "该论文探讨了utilitarian algorithm configuration技术，用于自动优化算法参数以提升其在给定输入上的性能，但现有方法仅限于有限参数空间，限制了其适用性。作者提出了一种新程序COUP（Continuous, Optimistic Utilitarian Procrastination），设计用于高效搜索infinite parameter spaces，能够快速发现良好配置，同时保持了之前方法的optimality guarantees。实验和理论分析表明，COUP在速度和性能上均优于现有方法，为处理连续或不可数参数空间提供了更有效的解决方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18246v3",
      "published_date": "2024-05-28 14:58:07 UTC",
      "updated_date": "2025-02-16 15:15:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:02:10.224997"
    },
    {
      "arxiv_id": "2405.18241v1",
      "title": "Active Use of Latent Constituency Representation in both Humans and Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Wei Liu",
        "Ming Xiang",
        "Nai Ding"
      ],
      "abstract": "Understanding how sentences are internally represented in the human brain, as\nwell as in large language models (LLMs) such as ChatGPT, is a major challenge\nfor cognitive science. Classic linguistic theories propose that the brain\nrepresents a sentence by parsing it into hierarchically organized constituents.\nIn contrast, LLMs do not explicitly parse linguistic constituents and their\nlatent representations remains poorly explained. Here, we demonstrate that\nhumans and LLMs construct similar latent representations of hierarchical\nlinguistic constituents by analyzing their behaviors during a novel one-shot\nlearning task, in which they infer which words should be deleted from a\nsentence. Both humans and LLMs tend to delete a constituent, instead of a\nnonconstituent word string. In contrast, a naive sequence processing model that\nhas access to word properties and ordinal positions does not show this\nproperty. Based on the word deletion behaviors, we can reconstruct the latent\nconstituency tree representation of a sentence for both humans and LLMs. These\nresults demonstrate that a latent tree-structured constituency representation\ncan emerge in both the human brain and LLMs.",
      "tldr_zh": "这篇论文探讨了人类大脑和大型语言模型（LLMs，如 ChatGPT）在句子内部表示中如何使用潜在的层次成分表示。研究者设计了一个新颖的单次学习任务（one-shot learning task），要求参与者推断哪些词应从句子中删除，结果显示人类和 LLMs 都倾向于删除成分（constituent）而非非成分词串，与简单的序列处理模型形成对比。通过分析删除行为，论文成功重建了人类和 LLMs 的潜在成分树表示，证明这种树状结构在两者中均自发出现。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "62 pages, 5 figures. Under review",
      "pdf_url": "http://arxiv.org/pdf/2405.18241v1",
      "published_date": "2024-05-28 14:50:22 UTC",
      "updated_date": "2024-05-28 14:50:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:02:24.690888"
    },
    {
      "arxiv_id": "2405.18208v1",
      "title": "A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Chengxing Xie",
        "Difan Zou"
      ],
      "abstract": "Recent studies have highlighted their proficiency in some simple tasks like\nwriting and coding through various reasoning strategies. However, LLM agents\nstill struggle with tasks that require comprehensive planning, a process that\nchallenges current models and remains a critical research issue. In this study,\nwe concentrate on travel planning, a Multi-Phases planning problem, that\ninvolves multiple interconnected stages, such as outlining, information\ngathering, and planning, often characterized by the need to manage various\nconstraints and uncertainties. Existing reasoning approaches have struggled to\neffectively address this complex task. Our research aims to address this\nchallenge by developing a human-like planning framework for LLM agents, i.e.,\nguiding the LLM agent to simulate various steps that humans take when solving\nMulti-Phases problems. Specifically, we implement several strategies to enable\nLLM agents to generate a coherent outline for each travel query, mirroring\nhuman planning patterns. Additionally, we integrate Strategy Block and\nKnowledge Block into our framework: Strategy Block facilitates information\ncollection, while Knowledge Block provides essential information for detailed\nplanning. Through our extensive experiments, we demonstrate that our framework\nsignificantly improves the planning capabilities of LLM agents, enabling them\nto tackle the travel planning task with improved efficiency and effectiveness.\nOur experimental results showcase the exceptional performance of the proposed\nframework; when combined with GPT-4-Turbo, it attains $10\\times$ the\nperformance gains in comparison to the baseline framework deployed on\nGPT-4-Turbo.",
      "tldr_zh": "这篇论文提出了一种人类-like 推理框架，用于大型语言模型 (LLMs) 处理多阶段规划任务（如旅行规划），以解决现有模型在涉及多个互连阶段、约束和不确定性的复杂任务中的不足。该框架引导 LLM 代理模拟人类规划步骤，包括生成连贯的大纲、整合 Strategy Block 用于信息收集，以及 Knowledge Block 用于提供必要知识，从而提升规划的效率和连贯性。实验结果显示，该框架显著提高了 LLM 代理的表现，与 GPT-4-Turbo 结合时，比基线框架性能提升了 10 倍，为复杂规划任务的应用奠定了基础。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18208v1",
      "published_date": "2024-05-28 14:13:32 UTC",
      "updated_date": "2024-05-28 14:13:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:02:38.048576"
    },
    {
      "arxiv_id": "2405.18196v1",
      "title": "Render and Diffuse: Aligning Image and Action Spaces for Diffusion-based Behaviour Cloning",
      "title_zh": "翻译失败",
      "authors": [
        "Vitalis Vosylius",
        "Younggyo Seo",
        "Jafar Uruç",
        "Stephen James"
      ],
      "abstract": "In the field of Robot Learning, the complex mapping between high-dimensional\nobservations such as RGB images and low-level robotic actions, two inherently\nvery different spaces, constitutes a complex learning problem, especially with\nlimited amounts of data. In this work, we introduce Render and Diffuse (R&D) a\nmethod that unifies low-level robot actions and RGB observations within the\nimage space using virtual renders of the 3D model of the robot. Using this\njoint observation-action representation it computes low-level robot actions\nusing a learnt diffusion process that iteratively updates the virtual renders\nof the robot. This space unification simplifies the learning problem and\nintroduces inductive biases that are crucial for sample efficiency and spatial\ngeneralisation. We thoroughly evaluate several variants of R&D in simulation\nand showcase their applicability on six everyday tasks in the real world. Our\nresults show that R&D exhibits strong spatial generalisation capabilities and\nis more sample efficient than more common image-to-action methods.",
      "tldr_zh": "本文提出 Render and Diffuse (R&D) 方法，通过虚拟渲染将机器人动作和 RGB 图像统一到图像空间，解决机器人学习中高维观察与低级动作之间复杂映射的问题，尤其在数据有限的情况下。R&D 利用学习扩散过程迭代更新机器人的 3D 虚拟渲染来计算动作，从而简化学习任务并引入归纳偏差，提升样本效率和空间泛化能力。在模拟和真实世界六种日常任务的实验中，该方法比传统图像到动作方法更高效，展示了更强的泛化性能。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Robotics: Science and Systems (RSS) 2024. Videos are available on our\n  project webpage at https://vv19.github.io/render-and-diffuse/",
      "pdf_url": "http://arxiv.org/pdf/2405.18196v1",
      "published_date": "2024-05-28 14:06:10 UTC",
      "updated_date": "2024-05-28 14:06:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:02:49.850915"
    },
    {
      "arxiv_id": "2405.18180v2",
      "title": "Safe Reinforcement Learning in Black-Box Environments via Adaptive Shielding",
      "title_zh": "翻译失败",
      "authors": [
        "Daniel Bethell",
        "Simos Gerasimou",
        "Radu Calinescu",
        "Calum Imrie"
      ],
      "abstract": "Empowering safe exploration of reinforcement learning (RL) agents during\ntraining is a critical challenge towards their deployment in many real-world\nscenarios. When prior knowledge of the domain or task is unavailable, training\nRL agents in unknown, \\textit{black-box} environments presents an even greater\nsafety risk. We introduce \\mbox{ADVICE} (Adaptive Shielding with a Contrastive\nAutoencoder), a novel post-shielding technique that distinguishes safe and\nunsafe features of state-action pairs during training, and uses this knowledge\nto protect the RL agent from executing actions that yield likely hazardous\noutcomes. Our comprehensive experimental evaluation against state-of-the-art\nsafe RL exploration techniques shows that ADVICE significantly reduces safety\nviolations ($\\approx\\!\\!50\\%$) during training, with a competitive outcome\nreward compared to other techniques.",
      "tldr_zh": "该论文针对强化学习(Reinforcement Learning)代理在未知黑箱环境中的安全探索问题，提出了一种名为ADVICE的自适应屏蔽技术。该技术利用对比自编码器(Contrastive Autoencoder)来区分状态-动作对的安全和不安全特征，并在训练过程中屏蔽可能导致危险结果的动作。实验结果显示，ADVICE相较于现有先进方法，显著降低了安全违规率（约50%），同时在奖励结果上保持竞争力，为黑箱环境下的安全RL训练提供了有效解决方案。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18180v2",
      "published_date": "2024-05-28 13:47:21 UTC",
      "updated_date": "2025-01-31 10:45:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:02:59.601749"
    },
    {
      "arxiv_id": "2405.18172v1",
      "title": "AnyFit: Controllable Virtual Try-on for Any Combination of Attire Across Any Scenario",
      "title_zh": "翻译失败",
      "authors": [
        "Yuhan Li",
        "Hao Zhou",
        "Wenxiang Shang",
        "Ran Lin",
        "Xuanhong Chen",
        "Bingbing Ni"
      ],
      "abstract": "While image-based virtual try-on has made significant strides, emerging\napproaches still fall short of delivering high-fidelity and robust fitting\nimages across various scenarios, as their models suffer from issues of\nill-fitted garment styles and quality degrading during the training process,\nnot to mention the lack of support for various combinations of attire.\nTherefore, we first propose a lightweight, scalable, operator known as Hydra\nBlock for attire combinations. This is achieved through a parallel attention\nmechanism that facilitates the feature injection of multiple garments from\nconditionally encoded branches into the main network. Secondly, to\nsignificantly enhance the model's robustness and expressiveness in real-world\nscenarios, we evolve its potential across diverse settings by synthesizing the\nresiduals of multiple models, as well as implementing a mask region boost\nstrategy to overcome the instability caused by information leakage in existing\nmodels. Equipped with the above design, AnyFit surpasses all baselines on\nhigh-resolution benchmarks and real-world data by a large gap, excelling in\nproducing well-fitting garments replete with photorealistic and rich details.\nFurthermore, AnyFit's impressive performance on high-fidelity virtual try-ons\nin any scenario from any image, paves a new path for future research within the\nfashion community.",
      "tldr_zh": "该研究提出AnyFit，一种可控虚拟试穿框架，支持任意服装组合在各种场景下的高保真试穿生成，以解决现有模型的服装不适配和质量退化问题。主要方法包括引入Hydra Block——一个轻量级、可扩展的操作符，利用parallel attention mechanism将多个服装特征注入主网络，并通过synthesizing the residuals of multiple models和mask region boost策略提升模型的鲁棒性和表达力。实验结果显示，AnyFit在高分辨率基准和真实数据上大幅超越基线模型，生成逼真丰富的试穿图像，为时尚领域的未来研究开辟新路径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project website: https://colorful-liyu.github.io/anyfit-page/",
      "pdf_url": "http://arxiv.org/pdf/2405.18172v1",
      "published_date": "2024-05-28 13:33:08 UTC",
      "updated_date": "2024-05-28 13:33:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:03:11.623623"
    },
    {
      "arxiv_id": "2405.18166v2",
      "title": "Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing",
      "title_zh": "通过层特定编辑防御大语言模型免受越狱攻击",
      "authors": [
        "Wei Zhao",
        "Zhe Li",
        "Yige Li",
        "Ye Zhang",
        "Jun Sun"
      ],
      "abstract": "Large language models (LLMs) are increasingly being adopted in a wide range\nof real-world applications. Despite their impressive performance, recent\nstudies have shown that LLMs are vulnerable to deliberately crafted adversarial\nprompts even when aligned via Reinforcement Learning from Human Feedback or\nsupervised fine-tuning. While existing defense methods focus on either\ndetecting harmful prompts or reducing the likelihood of harmful responses\nthrough various means, defending LLMs against jailbreak attacks based on the\ninner mechanisms of LLMs remains largely unexplored. In this work, we\ninvestigate how LLMs response to harmful prompts and propose a novel defense\nmethod termed \\textbf{L}ayer-specific \\textbf{Ed}iting (LED) to enhance the\nresilience of LLMs against jailbreak attacks. Through LED, we reveal that\nseveral critical \\textit{safety layers} exist among the early layers of LLMs.\nWe then show that realigning these safety layers (and some selected additional\nlayers) with the decoded safe response from selected target layers can\nsignificantly improve the alignment of LLMs against jailbreak attacks.\nExtensive experiments across various LLMs (e.g., Llama2, Mistral) show the\neffectiveness of LED, which effectively defends against jailbreak attacks while\nmaintaining performance on benign prompts. Our code is available at\n\\url{https://github.com/ledllm/ledllm}.",
      "tldr_zh": "这篇论文探讨了 Large Language Models (LLMs) 面对 jailbreak attacks（越狱攻击）的脆弱性，这些攻击通过对抗性提示规避模型的安全机制，而现有防御方法多聚焦于检测或响应抑制。作者提出了一种新方法 Layer-specific Editing (LED)，通过识别 LLMs 中的关键 safety layers（安全层）并对其进行 realigning，以从模型内部机制增强防御能力。实验在 Llama2 和 Mistral 等模型上证明，LED 显著提高了对 jailbreak attacks 的抵抗力，同时保持了在 benign prompts（良性提示）上的性能表现。该方法为基于 LLMs 内部结构的安全防护提供了创新途径。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18166v2",
      "published_date": "2024-05-28 13:26:12 UTC",
      "updated_date": "2024-06-14 07:27:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:03:25.323760"
    },
    {
      "arxiv_id": "2405.18165v1",
      "title": "Time Series Representation Models",
      "title_zh": "时间序列表示模型",
      "authors": [
        "Robert Leppich",
        "Vanessa Borst",
        "Veronika Lesch",
        "Samuel Kounev"
      ],
      "abstract": "Time series analysis remains a major challenge due to its sparse\ncharacteristics, high dimensionality, and inconsistent data quality. Recent\nadvancements in transformer-based techniques have enhanced capabilities in\nforecasting and imputation; however, these methods are still resource-heavy,\nlack adaptability, and face difficulties in integrating both local and global\nattributes of time series. To tackle these challenges, we propose a new\narchitectural concept for time series analysis based on introspection. Central\nto this concept is the self-supervised pretraining of Time Series\nRepresentation Models (TSRMs), which once learned can be easily tailored and\nfine-tuned for specific tasks, such as forecasting and imputation, in an\nautomated and resource-efficient manner. Our architecture is equipped with a\nflexible and hierarchical representation learning process, which is robust\nagainst missing data and outliers. It can capture and learn both local and\nglobal features of the structure, semantics, and crucial patterns of a given\ntime series category, such as heart rate data. Our learned time series\nrepresentation models can be efficiently adapted to a specific task, such as\nforecasting or imputation, without manual intervention. Furthermore, our\narchitecture's design supports explainability by highlighting the significance\nof each input value for the task at hand. Our empirical study using four\nbenchmark datasets shows that, compared to investigated state-of-the-art\nbaseline methods, our architecture improves imputation and forecasting errors\nby up to 90.34% and 71.54%, respectively, while reducing the required trainable\nparameters by up to 92.43%. The source code is available at\nhttps://github.com/RobertLeppich/TSRM.",
      "tldr_zh": "本研究针对时间序列分析的挑战，如稀疏特性、高维度和数据质量不一致，提出了一种基于内省的架构概念，核心是自监督预训练的 Time Series Representation Models (TSRMs)。TSRMs 通过灵活的层次化表示学习过程，能够捕捉局部和全局特征，对缺失数据和异常值具有鲁棒性，并支持自动化微调用于特定任务，如预测和插值，而无需手动干预。实验结果显示，在四个基准数据集上，该架构相比现有 transformer-based 方法，提高插值错误率达90.34%、预测错误率达71.54%，并减少训练参数达92.43%，同时增强了模型的可解释性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18165v1",
      "published_date": "2024-05-28 13:25:31 UTC",
      "updated_date": "2024-05-28 13:25:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:03:34.900869"
    },
    {
      "arxiv_id": "2405.18161v1",
      "title": "Back to the Drawing Board for Fair Representation Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Angéline Pouget",
        "Nikola Jovanović",
        "Mark Vero",
        "Robin Staab",
        "Martin Vechev"
      ],
      "abstract": "The goal of Fair Representation Learning (FRL) is to mitigate biases in\nmachine learning models by learning data representations that enable high\naccuracy on downstream tasks while minimizing discrimination based on sensitive\nattributes. The evaluation of FRL methods in many recent works primarily\nfocuses on the tradeoff between downstream fairness and accuracy with respect\nto a single task that was used to approximate the utility of representations\nduring training (proxy task). This incentivizes retaining only features\nrelevant to the proxy task while discarding all other information. In extreme\ncases, this can cause the learned representations to collapse to a trivial,\nbinary value, rendering them unusable in transfer settings. In this work, we\nargue that this approach is fundamentally mismatched with the original\nmotivation of FRL, which arises from settings with many downstream tasks\nunknown at training time (transfer tasks). To remedy this, we propose to\nrefocus the evaluation protocol of FRL methods primarily around the performance\non transfer tasks. A key challenge when conducting such an evaluation is the\nlack of adequate benchmarks. We address this by formulating four criteria that\na suitable evaluation procedure should fulfill. Based on these, we propose\nTransFair, a benchmark that satisfies these criteria, consisting of novel\nvariations of popular FRL datasets with carefully calibrated transfer tasks. In\nthis setting, we reevaluate state-of-the-art FRL methods, observing that they\noften overfit to the proxy task, which causes them to underperform on certain\ntransfer tasks. We further highlight the importance of task-agnostic learning\nsignals for FRL methods, as they can lead to more transferrable\nrepresentations.",
      "tldr_zh": "本论文批评了现有 Fair Representation Learning (FRL) 方法的评估方式，这些方法过度依赖单一 proxy task，导致学习到的表示过度拟合并在 transfer tasks 上表现不佳，从而无法满足多任务场景的需求。作者提出重新聚焦于 transfer tasks 的评估协议，并制定了四个关键评估标准，以更好地评估 FRL 方法的泛化能力。为此，他们引入了 TransFair 基准，这是一个基于流行 FRL 数据集的创新变体，包含精心设计的转移任务。通过实验重新评估了 state-of-the-art FRL 方法，发现这些方法往往在转移任务上表现欠佳，并强调任务无关学习信号（task-agnostic learning signals）对获得更可转移表示的重要性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18161v1",
      "published_date": "2024-05-28 13:23:04 UTC",
      "updated_date": "2024-05-28 13:23:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:03:49.556859"
    },
    {
      "arxiv_id": "2405.18148v1",
      "title": "Learning to Detour: Shortcut Mitigating Augmentation for Weakly Supervised Semantic Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "JuneHyoung Kwon",
        "Eunju Lee",
        "Yunsung Cho",
        "YoungBin Kim"
      ],
      "abstract": "Weakly supervised semantic segmentation (WSSS) employing weak forms of labels\nhas been actively studied to alleviate the annotation cost of acquiring\npixel-level labels. However, classifiers trained on biased datasets tend to\nexploit shortcut features and make predictions based on spurious correlations\nbetween certain backgrounds and objects, leading to a poor generalization\nperformance. In this paper, we propose shortcut mitigating augmentation (SMA)\nfor WSSS, which generates synthetic representations of object-background\ncombinations not seen in the training data to reduce the use of shortcut\nfeatures. Our approach disentangles the object-relevant and background\nfeatures. We then shuffle and combine the disentangled representations to\ncreate synthetic features of diverse object-background combinations.\nSMA-trained classifier depends less on contexts and focuses more on the target\nobject when making predictions. In addition, we analyzed the behavior of the\nclassifier on shortcut usage after applying our augmentation using an\nattribution method-based metric. The proposed method achieved the improved\nperformance of semantic segmentation result on PASCAL VOC 2012 and MS COCO 2014\ndatasets.",
      "tldr_zh": "该研究针对弱监督语义分割 (WSSS) 中分类器依赖捷径特征 (shortcut features) 的问题，提出了一种捷径缓解增强方法 (shortcut mitigating augmentation, SMA)。SMA 通过分离物体相关特征和背景特征，然后洗牌并组合这些特征，生成训练数据中未见的物体-背景合成表示，从而减少模型对虚假相关性的依赖。实验结果显示，该方法使分类器更关注目标物体，在 PASCAL VOC 2012 和 MS COCO 2014 数据集上显著提升了语义分割性能，并通过归因方法验证了捷径使用减少。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to WACV 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.18148v1",
      "published_date": "2024-05-28 13:07:35 UTC",
      "updated_date": "2024-05-28 13:07:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:03:59.280719"
    },
    {
      "arxiv_id": "2405.18144v3",
      "title": "4-bit Shampoo for Memory-Efficient Network Training",
      "title_zh": "翻译失败",
      "authors": [
        "Sike Wang",
        "Pan Zhou",
        "Jia Li",
        "Hua Huang"
      ],
      "abstract": "Second-order optimizers, maintaining a matrix termed a preconditioner, are\nsuperior to first-order optimizers in both theory and practice. The states\nforming the preconditioner and its inverse root restrict the maximum size of\nmodels trained by second-order optimizers. To address this, compressing 32-bit\noptimizer states to lower bitwidths has shown promise in reducing memory usage.\nHowever, current approaches only pertain to first-order optimizers. In this\npaper, we propose the first 4-bit second-order optimizers, exemplified by 4-bit\nShampoo, maintaining performance similar to that of 32-bit ones. We show that\nquantizing the eigenvector matrix of the preconditioner in 4-bit Shampoo is\nremarkably better than quantizing the preconditioner itself both theoretically\nand experimentally. By rectifying the orthogonality of the quantized\neigenvector matrix, we enhance the approximation of the preconditioner's\neigenvector matrix, which also benefits the computation of its inverse 4-th\nroot. Besides, we find that linear square quantization slightly outperforms\ndynamic tree quantization when quantizing second-order optimizer states.\nEvaluation on various networks for image classification and natural language\nmodeling demonstrates that our 4-bit Shampoo achieves comparable performance to\nits 32-bit counterpart while being more memory-efficient.",
      "tldr_zh": "该论文提出了一种内存高效的4-bit第二阶优化器——4-bit Shampoo，以解决第二阶优化器（如Shampoo）在训练大模型时因维护预条件器矩阵而导致的内存问题。方法包括量化预条件器的特征向量矩阵而非预条件器本身，并通过修正量化后矩阵的正交性来提升其近似精度，同时发现线性平方量化比动态树量化更有效。实验结果显示，在图像分类和自然语言建模任务上，4-bit Shampoo与32-bit版本性能相当，但显著降低了内存消耗。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2024 final camera-ready revisions, rectify the legend in\n  figure 9",
      "pdf_url": "http://arxiv.org/pdf/2405.18144v3",
      "published_date": "2024-05-28 13:02:56 UTC",
      "updated_date": "2025-01-10 07:22:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:04:11.252257"
    },
    {
      "arxiv_id": "2405.18139v1",
      "title": "Unlocking Futures: A Natural Language Driven Career Prediction System for Computer Science and Software Engineering Students",
      "title_zh": "Unlocking Futures: 一种基于自然语言驱动的计算机科学和软件工程学生职业预测系统",
      "authors": [
        "Sakir Hossain Faruque",
        "Sharun Akter Khushbu",
        "Sharmin Akter"
      ],
      "abstract": "A career is a crucial aspect for any person to fulfill their desires through\nhard work. During their studies, students cannot find the best career\nsuggestions unless they receive meaningful guidance tailored to their skills.\nTherefore, we developed an AI-assisted model for early prediction to provide\nbetter career suggestions. Although the task is difficult, proper guidance can\nmake it easier. Effective career guidance requires understanding a student's\nacademic skills, interests, and skill-related activities. In this research, we\ncollected essential information from Computer Science (CS) and Software\nEngineering (SWE) students to train a machine learning (ML) model that predicts\ncareer paths based on students' career-related information. To adequately train\nthe models, we applied Natural Language Processing (NLP) techniques and\ncompleted dataset pre-processing. For comparative analysis, we utilized\nmultiple classification ML algorithms and deep learning (DL) algorithms. This\nstudy contributes valuable insights to educational advising by providing\nspecific career suggestions based on the unique features of CS and SWE\nstudents. Additionally, the research helps individual CS and SWE students find\nsuitable jobs that match their skills, interests, and skill-related activities.",
      "tldr_zh": "本文提出了一种基于自然语言驱动的职业预测系统，针对计算机科学 (CS) 和软件工程 (SWE) 学生，利用他们的学术技能、兴趣和相关活动提供个性化职业建议。研究团队收集学生数据，通过自然语言处理 (NLP) 技术进行预处理，并训练多种机器学习 (ML) 和深度学习 (DL) 算法进行分类预测和比较分析。该系统为教育咨询带来宝贵见解，帮助学生更早地找到匹配自身特质的合适职位。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18139v1",
      "published_date": "2024-05-28 12:56:57 UTC",
      "updated_date": "2024-05-28 12:56:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:04:23.795777"
    },
    {
      "arxiv_id": "2405.18137v2",
      "title": "Exploiting LLM Quantization",
      "title_zh": "翻译失败",
      "authors": [
        "Kazuki Egashira",
        "Mark Vero",
        "Robin Staab",
        "Jingxuan He",
        "Martin Vechev"
      ],
      "abstract": "Quantization leverages lower-precision weights to reduce the memory usage of\nlarge language models (LLMs) and is a key technique for enabling their\ndeployment on commodity hardware. While LLM quantization's impact on utility\nhas been extensively explored, this work for the first time studies its adverse\neffects from a security perspective. We reveal that widely used quantization\nmethods can be exploited to produce a harmful quantized LLM, even though the\nfull-precision counterpart appears benign, potentially tricking users into\ndeploying the malicious quantized model. We demonstrate this threat using a\nthree-staged attack framework: (i) first, we obtain a malicious LLM through\nfine-tuning on an adversarial task; (ii) next, we quantize the malicious model\nand calculate constraints that characterize all full-precision models that map\nto the same quantized model; (iii) finally, using projected gradient descent,\nwe tune out the poisoned behavior from the full-precision model while ensuring\nthat its weights satisfy the constraints computed in step (ii). This procedure\nresults in an LLM that exhibits benign behavior in full precision but when\nquantized, it follows the adversarial behavior injected in step (i). We\nexperimentally demonstrate the feasibility and severity of such an attack\nacross three diverse scenarios: vulnerable code generation, content injection,\nand over-refusal attack. In practice, the adversary could host the resulting\nfull-precision model on an LLM community hub such as Hugging Face, exposing\nmillions of users to the threat of deploying its malicious quantized version on\ntheir devices.",
      "tldr_zh": "这篇论文首次从安全角度探讨大型语言模型(LLMs)的量化(Quantization)技术，揭示量化过程可能被利用，导致全精度模型看似良性，但量化后表现出恶意行为。研究提出一个三阶段攻击框架：首先通过在对抗任务上微调获得恶意LLM；其次，量化恶意模型并计算约束；最后，使用projected gradient descent调整全精度模型，确保其权重满足约束，从而隐藏毒性行为。实验在易受攻击的代码生成、内容注入和过度拒绝攻击等场景中证明了攻击的可行性与严重性，警告用户在平台如Hugging Face上部署量化模型时需警惕潜在威胁。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18137v2",
      "published_date": "2024-05-28 12:51:01 UTC",
      "updated_date": "2024-11-04 11:16:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:04:36.110893"
    },
    {
      "arxiv_id": "2405.18123v1",
      "title": "PyTAG: Tabletop Games for Multi-Agent Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Martin Balla",
        "George E. M. Long",
        "James Goodman",
        "Raluca D. Gaina",
        "Diego Perez-Liebana"
      ],
      "abstract": "Modern Tabletop Games present various interesting challenges for Multi-agent\nReinforcement Learning. In this paper, we introduce PyTAG, a new framework that\nsupports interacting with a large collection of games implemented in the\nTabletop Games framework. In this work we highlight the challenges tabletop\ngames provide, from a game-playing agent perspective, along with the\nopportunities they provide for future research. Additionally, we highlight the\ntechnical challenges that involve training Reinforcement Learning agents on\nthese games. To explore the Multi-agent setting provided by PyTAG we train the\npopular Proximal Policy Optimisation Reinforcement Learning algorithm using\nself-play on a subset of games and evaluate the trained policies against some\nsimple agents and Monte-Carlo Tree Search implemented in the Tabletop Games\nframework.",
      "tldr_zh": "本论文引入PyTAG框架，用于支持多智能体强化学习（Multi-agent Reinforcement Learning）在现代桌面游戏（Tabletop Games）中的应用。PyTAG突出了这些游戏从代理视角带来的挑战，如复杂互动和训练技术难题，同时提供了未来研究的机遇。研究团队使用Proximal Policy Optimisation算法结合self-play在游戏子集上训练代理，并评估其性能，对比简单代理和Tabletop Games框架中的Monte-Carlo Tree Search，以展示框架的有效性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18123v1",
      "published_date": "2024-05-28 12:30:28 UTC",
      "updated_date": "2024-05-28 12:30:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:04:46.787777"
    },
    {
      "arxiv_id": "2405.18119v2",
      "title": "Low-Resource Crop Classification from Multi-Spectral Time Series Using Lossless Compressors",
      "title_zh": "翻译失败",
      "authors": [
        "Wei Cheng",
        "Hongrui Ye",
        "Xiao Wen",
        "Jiachen Zhang",
        "Jiping Xu",
        "Feifan Zhang"
      ],
      "abstract": "Deep learning has significantly improved the accuracy of crop classification\nusing multispectral temporal data. However, these models have complex\nstructures with numerous parameters, requiring large amounts of data and costly\ntraining. In low-resource situations with fewer labeled samples, deep learning\nmodels perform poorly due to insufficient data. Conversely, compressors are\ndata-type agnostic, and non-parametric methods do not bring underlying\nassumptions. Inspired by this insight, we propose a non-training alternative to\ndeep learning models, aiming to address these situations. Specifically, the\nSymbolic Representation Module is proposed to convert the reflectivity into\nsymbolic representations. The symbolic representations are then\ncross-transformed in both the channel and time dimensions to generate symbolic\nembeddings. Next, the Multi-scale Normalised Compression Distance (MNCD) is\ndesigned to measure the correlation between any two symbolic embeddings.\nFinally, based on the MNCDs, high quality crop classification can be achieved\nusing only a k-nearest-neighbor classifier kNN. The entire framework is\nready-to-use and lightweight. Without any training, it outperformed, on\naverage, 7 advanced deep learning models trained at scale on three benchmark\ndatasets. It also outperforms more than half of these models in the few-shot\nsetting with sparse crop labels. Therefore, the high performance and robustness\nof our non-training framework makes it truly applicable to real-world crop\nmapping. Codes are available at:\nhttps://github.com/qinfengsama/Compressor-Based-Crop-Mapping.",
      "tldr_zh": "该论文针对低资源环境下利用多光谱时间序列进行作物分类的问题，提出了一种无需训练的非参数框架，使用无损压缩器作为替代深度学习模型。框架包括Symbolic Representation Module，将反射率转换为符号表示，并通过通道和时间维度的交叉转换生成符号嵌入；随后，设计Multi-scale Normalised Compression Distance (MNCD)来测量符号嵌入之间的相关性，并结合k-nearest-neighbor classifier (kNN)实现高效分类。该方法在三个基准数据集上平均超过了7个先进的深度学习模型，在少样本设置中也优于其中一半以上模型，展示了其在真实世界作物映射中的高性能和鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.18119v2",
      "published_date": "2024-05-28 12:28:12 UTC",
      "updated_date": "2024-07-05 15:23:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:04:59.656396"
    },
    {
      "arxiv_id": "2405.18118v3",
      "title": "An agent design with goal reaching guarantees for enhancement of learning",
      "title_zh": "翻译失败",
      "authors": [
        "Pavel Osinenko",
        "Grigory Yaremenko",
        "Georgiy Malaniya",
        "Anton Bolychev",
        "Alexander Gepperth"
      ],
      "abstract": "Reinforcement learning is commonly concerned with problems of maximizing\naccumulated rewards in Markov decision processes. Oftentimes, a certain goal\nstate or a subset of the state space attain maximal reward. In such a case, the\nenvironment may be considered solved when the goal is reached. Whereas numerous\ntechniques, learning or non-learning based, exist for solving environments,\ndoing so optimally is the biggest challenge. Say, one may choose a reward rate\nwhich penalizes the action effort. Reinforcement learning is currently among\nthe most actively developed frameworks for solving environments optimally by\nvirtue of maximizing accumulated reward, in other words, returns. Yet, tuning\nagents is a notoriously hard task as reported in a series of works. Our aim\nhere is to help the agent learn a near-optimal policy efficiently while\nensuring a goal reaching property of some basis policy that merely solves the\nenvironment. We suggest an algorithm, which is fairly flexible, and can be used\nto augment practically any agent as long as it comprises of a critic. A formal\nproof of a goal reaching property is provided. Comparative experiments on\nseveral problems under popular baseline agents provided an empirical evidence\nthat the learning can indeed be boosted while ensuring goal reaching property.",
      "tldr_zh": "该论文针对强化学习（Reinforcement Learning）在马尔可夫决策过程（Markov Decision Processes）中最大化累积奖励的问题，提出了一种代理设计方法，以确保目标状态的到达保证，同时提升学习效率。该方法通过一个灵活的算法增强现有代理（agent），要求代理包含批评家（critic），并提供正式证明（formal proof）来验证其目标到达属性。实验结果显示，该算法在多个基准问题上显著提升了基线代理的学习性能，同时保持了可靠的目标实现能力。",
      "categories": [
        "cs.AI",
        "cs.SY",
        "eess.SY",
        "math.DS"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18118v3",
      "published_date": "2024-05-28 12:27:36 UTC",
      "updated_date": "2024-08-21 20:43:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:05:11.871791"
    },
    {
      "arxiv_id": "2405.18113v1",
      "title": "Facilitating Multi-Role and Multi-Behavior Collaboration of Large Language Models for Online Job Seeking and Recruiting",
      "title_zh": "翻译失败",
      "authors": [
        "Hongda Sun",
        "Hongzhan Lin",
        "Haiyu Yan",
        "Chen Zhu",
        "Yang Song",
        "Xin Gao",
        "Shuo Shang",
        "Rui Yan"
      ],
      "abstract": "The emergence of online recruitment services has revolutionized the\ntraditional landscape of job seeking and recruitment, necessitating the\ndevelopment of high-quality industrial applications to improve person-job\nfitting. Existing methods generally rely on modeling the latent semantics of\nresumes and job descriptions and learning a matching function between them.\nInspired by the powerful role-playing capabilities of Large Language Models\n(LLMs), we propose to introduce a mock interview process between LLM-played\ninterviewers and candidates. The mock interview conversations can provide\nadditional evidence for candidate evaluation, thereby augmenting traditional\nperson-job fitting based solely on resumes and job descriptions. However,\ncharacterizing these two roles in online recruitment still presents several\nchallenges, such as developing the skills to raise interview questions,\nformulating appropriate answers, and evaluating two-sided fitness. To this end,\nwe propose MockLLM, a novel applicable framework that divides the person-job\nmatching process into two modules: mock interview generation and two-sided\nevaluation in handshake protocol, jointly enhancing their performance through\ncollaborative behaviors between interviewers and candidates. We design a\nrole-playing framework as a multi-role and multi-behavior paradigm to enable a\nsingle LLM agent to effectively behave with multiple functions for both\nparties. Moreover, we propose reflection memory generation and dynamic prompt\nmodification techniques to refine the behaviors of both sides, enabling\ncontinuous optimization of the augmented additional evidence. Extensive\nexperimental results show that MockLLM can achieve the best performance on\nperson-job matching accompanied by high mock interview quality, envisioning its\nemerging application in real online recruitment in the future.",
      "tldr_zh": "该研究提出MockLLM框架，利用Large Language Models (LLMs)的角色扮演能力，通过模拟面试过程来增强在线招聘中的人岗匹配。框架将匹配过程分为模拟面试生成和双向评估模块，采用多角色和多行为协作范式，让单个LLM代理处理面试问题、答案制定和评估功能。引入反射记忆生成和动态提示修改技术，以优化面试双方的行为并持续改进额外证据的质量。实验结果显示，MockLLM在人岗匹配任务上表现出色，并生成高质量的模拟面试对话，预示其在真实在线招聘中的潜在应用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18113v1",
      "published_date": "2024-05-28 12:23:16 UTC",
      "updated_date": "2024-05-28 12:23:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:05:23.727144"
    },
    {
      "arxiv_id": "2405.18110v1",
      "title": "Individual Contributions as Intrinsic Exploration Scaffolds for Multi-agent Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Xinran Li",
        "Zifan Liu",
        "Shibo Chen",
        "Jun Zhang"
      ],
      "abstract": "In multi-agent reinforcement learning (MARL), effective exploration is\ncritical, especially in sparse reward environments. Although introducing global\nintrinsic rewards can foster exploration in such settings, it often complicates\ncredit assignment among agents. To address this difficulty, we propose\nIndividual Contributions as intrinsic Exploration Scaffolds (ICES), a novel\napproach to motivate exploration by assessing each agent's contribution from a\nglobal view. In particular, ICES constructs exploration scaffolds with Bayesian\nsurprise, leveraging global transition information during centralized training.\nThese scaffolds, used only in training, help to guide individual agents towards\nactions that significantly impact the global latent state transitions.\nAdditionally, ICES separates exploration policies from exploitation policies,\nenabling the former to utilize privileged global information during training.\nExtensive experiments on cooperative benchmark tasks with sparse rewards,\nincluding Google Research Football (GRF) and StarCraft Multi-agent Challenge\n(SMAC), demonstrate that ICES exhibits superior exploration capabilities\ncompared with baselines. The code is publicly available at\nhttps://github.com/LXXXXR/ICES.",
      "tldr_zh": "在多智能体强化学习(MARL)中，探索策略尤其重要，尤其是在稀疏奖励环境中，而传统的全局内在奖励往往会加剧信用分配的复杂性。为解决此问题，本文提出ICES（Individual Contributions as intrinsic Exploration Scaffolds）方法，通过评估每个智能体的全局贡献并利用Bayesian surprise和全球转移信息构建探索支架，仅在训练中使用这些支架来引导智能体影响全局状态转换。ICES还分离探索策略和利用策略，使前者能访问特权全局信息。在稀疏奖励基准任务如Google Research Football (GRF)和StarCraft Multi-agent Challenge (SMAC)上的广泛实验显示，ICES比基线方法表现出更强的探索能力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA",
        "I.2.6; I.2.11"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by the Forty-first International Conference on Machine\n  Learning",
      "pdf_url": "http://arxiv.org/pdf/2405.18110v1",
      "published_date": "2024-05-28 12:18:19 UTC",
      "updated_date": "2024-05-28 12:18:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:05:35.377426"
    },
    {
      "arxiv_id": "2405.18106v1",
      "title": "A Unified Temporal Knowledge Graph Reasoning Model Towards Interpolation and Extrapolation",
      "title_zh": "翻译失败",
      "authors": [
        "Kai Chen",
        "Ye Wang",
        "Yitong Li",
        "Aiping Li",
        "Han Yu",
        "Xin Song"
      ],
      "abstract": "Temporal knowledge graph (TKG) reasoning has two settings: interpolation\nreasoning and extrapolation reasoning. Both of them draw plenty of research\ninterest and have great significance. Methods of the former de-emphasize the\ntemporal correlations among facts sequences, while methods of the latter\nrequire strict chronological order of knowledge and ignore inferring clues\nprovided by missing facts of the past. These limit the practicability of TKG\napplications as almost all of the existing TKG reasoning methods are designed\nspecifically to address either one setting. To this end, this paper proposes an\noriginal Temporal PAth-based Reasoning (TPAR) model for both the interpolation\nand extrapolation reasoning. TPAR performs a neural-driven symbolic reasoning\nfashion that is robust to ambiguous and noisy temporal data and with fine\ninterpretability as well. Comprehensive experiments show that TPAR outperforms\nSOTA methods on the link prediction task for both the interpolation and the\nextrapolation settings. A novel pipeline experimental setting is designed to\nevaluate the performances of SOTA combinations and the proposed TPAR towards\ninterpolation and extrapolation reasoning. More diverse experiments are\nconducted to show the robustness and interpretability of TPAR.",
      "tldr_zh": "本论文针对时间知识图（TKG）推理的插值（interpolation reasoning）和外推（extrapolation reasoning）两种设置，指出现有方法要么忽略事实序列的时间相关性，要么忽略过去缺失事实的线索，导致适用性有限。论文提出了一种统一的 Temporal PAth-based Reasoning (TPAR) 模型，采用神经驱动的符号推理方式，能够处理模糊和噪声数据，并提供良好的可解释性。实验结果显示，TPAR 在链接预测任务上超越 SOTA 方法，并在新设计的实验管道中证明了其在两种设置下的鲁棒性和整体性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "To appear in ACL 2024 main conference",
      "pdf_url": "http://arxiv.org/pdf/2405.18106v1",
      "published_date": "2024-05-28 12:13:07 UTC",
      "updated_date": "2024-05-28 12:13:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:05:47.531453"
    },
    {
      "arxiv_id": "2405.18092v2",
      "title": "LLM experiments with simulation: Large Language Model Multi-Agent System for Simulation Model Parametrization in Digital Twins",
      "title_zh": "翻译失败",
      "authors": [
        "Yuchen Xia",
        "Daniel Dittler",
        "Nasser Jazdi",
        "Haonan Chen",
        "Michael Weyrich"
      ],
      "abstract": "This paper presents a novel design of a multi-agent system framework that\napplies large language models (LLMs) to automate the parametrization of\nsimulation models in digital twins. This framework features specialized LLM\nagents tasked with observing, reasoning, decision-making, and summarizing,\nenabling them to dynamically interact with digital twin simulations to explore\nparametrization possibilities and determine feasible parameter settings to\nachieve an objective. The proposed approach enhances the usability of\nsimulation model by infusing it with knowledge heuristics from LLM and enables\nautonomous search for feasible parametrization to solve a user task.\nFurthermore, the system has the potential to increase user-friendliness and\nreduce the cognitive load on human users by assisting in complex\ndecision-making processes. The effectiveness and functionality of the system\nare demonstrated through a case study, and the visualized demos and codes are\navailable at a GitHub Repository:\nhttps://github.com/YuchenXia/LLMDrivenSimulation",
      "tldr_zh": "该论文提出了一种多智能体系统框架，利用大型语言模型 (LLMs) 自动化数字孪生中模拟模型的参数化过程。框架包括专门的 LLM 代理，负责观察、推理、决策和总结，以动态互动模拟环境，探索参数化可能性并确定可行的参数设置来实现用户目标。该方法通过注入 LLM 的知识启发式，提升了模拟模型的可用性，并支持自主搜索参数，减少了人类用户在复杂决策中的认知负担。通过案例研究验证了系统的有效性，并提供了 GitHub 仓库供演示和代码访问。",
      "categories": [
        "cs.AI",
        "cs.ET",
        "cs.MA",
        "cs.RO",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "Submitted to IEEE-ETFA2024, under peer-review",
      "pdf_url": "http://arxiv.org/pdf/2405.18092v2",
      "published_date": "2024-05-28 11:59:40 UTC",
      "updated_date": "2024-07-22 14:03:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:05:59.548554"
    },
    {
      "arxiv_id": "2405.18077v1",
      "title": "Design Principles for Falsifiable, Replicable and Reproducible Empirical ML Research",
      "title_zh": "翻译失败",
      "authors": [
        "Daniel Vranješ",
        "Oliver Niggemann"
      ],
      "abstract": "Empirical research plays a fundamental role in the machine learning domain.\nAt the heart of impactful empirical research lies the development of clear\nresearch hypotheses, which then shape the design of experiments. The execution\nof experiments must be carried out with precision to ensure reliable results,\nfollowed by statistical analysis to interpret these outcomes. This process is\nkey to either supporting or refuting initial hypotheses. Despite its\nimportance, there is a high variability in research practices across the\nmachine learning community and no uniform understanding of quality criteria for\nempirical research. To address this gap, we propose a model for the empirical\nresearch process, accompanied by guidelines to uphold the validity of empirical\nresearch. By embracing these recommendations, greater consistency, enhanced\nreliability and increased impact can be achieved.",
      "tldr_zh": "这篇论文强调了机器学习领域中经验研究(Empirical Research)的核心作用，并指出当前社区存在研究实践的变异性和缺乏统一质量标准的问题。论文提出一个经验研究过程的模型，包括制定清晰的研究假设(Hypotheses)、精确实验设计和执行以及后续的统计分析(Statistical Analysis)，以支持或反驳假设。采用这些设计原则可以提升研究的falsifiable（可证伪）、replicable（可重复）和reproducible（可复现）性，从而实现更一致、更可靠的研究结果和更大影响力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18077v1",
      "published_date": "2024-05-28 11:37:59 UTC",
      "updated_date": "2024-05-28 11:37:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:06:11.245044"
    },
    {
      "arxiv_id": "2405.18073v1",
      "title": "Towards Dialogues for Joint Human-AI Reasoning and Value Alignment",
      "title_zh": "迈向人类-AI 联合推理与价值对齐的对话",
      "authors": [
        "Elfia Bezou-Vrakatseli",
        "Oana Cocarascu",
        "Sanjay Modgil"
      ],
      "abstract": "We argue that enabling human-AI dialogue, purposed to support joint reasoning\n(i.e., 'inquiry'), is important for ensuring that AI decision making is aligned\nwith human values and preferences. In particular, we point to logic-based\nmodels of argumentation and dialogue, and suggest that the traditional focus on\npersuasion dialogues be replaced by a focus on inquiry dialogues, and the\ndistinct challenges that joint inquiry raises. Given recent dramatic advances\nin the performance of large language models (LLMs), and the anticipated\nincrease in their use for decision making, we provide a roadmap for research\ninto inquiry dialogues for supporting joint human-LLM reasoning tasks that are\nethically salient, and that thereby require that decisions are value aligned.",
      "tldr_zh": "该论文主张，通过启用人类-AI 对话来支持联合推理（inquiry dialogues），以确保 AI 决策与人类价值观和偏好实现价值对齐（value alignment）。作者建议转向基于逻辑的论证和对话模型，强调探究对话而非传统的说服对话，以应对联合推理的独特挑战。鉴于大型语言模型（LLMs）的快速发展，该文提供了一个研究路线图，指导未来工作专注于伦理相关的人类-LLM 联合推理任务。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18073v1",
      "published_date": "2024-05-28 11:29:57 UTC",
      "updated_date": "2024-05-28 11:29:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:06:23.349220"
    },
    {
      "arxiv_id": "2405.18068v1",
      "title": "A Survey of Latent Factor Models in Recommender Systems",
      "title_zh": "推荐系统中的潜在因子模型综述",
      "authors": [
        "Hind I. Alshbanat",
        "Hafida Benhidour",
        "Said Kerrache"
      ],
      "abstract": "Recommender systems are essential tools in the digital era, providing\npersonalized content to users in areas like e-commerce, entertainment, and\nsocial media. Among the many approaches developed to create these systems,\nlatent factor models have proven particularly effective. This survey\nsystematically reviews latent factor models in recommender systems, focusing on\ntheir core principles, methodologies, and recent advancements. The literature\nis examined through a structured framework covering learning data, model\narchitecture, learning strategies, and optimization techniques. The analysis\nincludes a taxonomy of contributions and detailed discussions on the types of\nlearning data used, such as implicit feedback, trust, and content data, various\nmodels such as probabilistic, nonlinear, and neural models, and an exploration\nof diverse learning strategies like online learning, transfer learning, and\nactive learning. Furthermore, the survey addresses the optimization strategies\nused to train latent factor models, improving their performance and\nscalability. By identifying trends, gaps, and potential research directions,\nthis survey aims to provide valuable insights for researchers and practitioners\nlooking to advance the field of recommender systems.",
      "tldr_zh": "这篇调查论文系统审阅了潜在因素模型（latent factor models）在推荐系统（recommender systems）中的应用，聚焦于其核心原则、方法论和最新进展。论文采用结构化框架，涵盖学习数据（如implicit feedback、trust和content data）、模型架构（如probabilistic、nonlinear和neural models）、学习策略（如online learning、transfer learning和active learning），以及优化技术，以提升模型性能和可扩展性。通过识别当前趋势、研究空白和潜在方向，该工作为推荐系统领域的研究者和从业者提供了宝贵见解。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18068v1",
      "published_date": "2024-05-28 11:28:59 UTC",
      "updated_date": "2024-05-28 11:28:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:06:35.064959"
    },
    {
      "arxiv_id": "2405.18065v2",
      "title": "EffoVPR: Effective Foundation Model Utilization for Visual Place Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Issar Tzachor",
        "Boaz Lerner",
        "Matan Levy",
        "Michael Green",
        "Tal Berkovitz Shalev",
        "Gavriel Habib",
        "Dvir Samuel",
        "Noam Korngut Zailer",
        "Or Shimshi",
        "Nir Darshan",
        "Rami Ben-Ari"
      ],
      "abstract": "The task of Visual Place Recognition (VPR) is to predict the location of a\nquery image from a database of geo-tagged images. Recent studies in VPR have\nhighlighted the significant advantage of employing pre-trained foundation\nmodels like DINOv2 for the VPR task. However, these models are often deemed\ninadequate for VPR without further fine-tuning on VPR-specific data. In this\npaper, we present an effective approach to harness the potential of a\nfoundation model for VPR. We show that features extracted from self-attention\nlayers can act as a powerful re-ranker for VPR, even in a zero-shot setting.\nOur method not only outperforms previous zero-shot approaches but also\nintroduces results competitive with several supervised methods. We then show\nthat a single-stage approach utilizing internal ViT layers for pooling can\nproduce global features that achieve state-of-the-art performance, with\nimpressive feature compactness down to 128D. Moreover, integrating our local\nfoundation features for re-ranking further widens this performance gap. Our\nmethod also demonstrates exceptional robustness and generalization, setting new\nstate-of-the-art performance, while handling challenging conditions such as\nocclusion, day-night transitions, and seasonal variations.",
      "tldr_zh": "这篇论文提出 EffoVPR 方法，有效利用基础模型如 DINOv2 进行视觉地点识别（VPR），通过从自注意力层提取特征作为零样本（zero-shot）再排名器（re-ranker），显著提升了性能。方法还引入单阶段利用内部 ViT 层进行 pooling 的全局特征提取，实现了状态-of-the-art 性能，同时将特征压缩至 128D，并通过整合局部特征进一步扩大优势。实验证明，EffoVPR 在处理遮挡、日夜变化和季节变化等挑战条件下，展示了出色的鲁棒性和泛化能力，超越了现有零样本和监督方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2405.18065v2",
      "published_date": "2024-05-28 11:24:41 UTC",
      "updated_date": "2025-02-02 22:46:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:06:50.627728"
    },
    {
      "arxiv_id": "2405.18064v2",
      "title": "Automated Real-World Sustainability Data Generation from Images of Buildings",
      "title_zh": "翻译失败",
      "authors": [
        "Peter J Bentley",
        "Soo Ling Lim",
        "Rajat Mathur",
        "Sid Narang"
      ],
      "abstract": "When data on building features is unavailable, the task of determining how to\nimprove that building in terms of carbon emissions becomes infeasible. We show\nthat from only a set of images, a Large Language Model with appropriate prompt\nengineering and domain knowledge can successfully estimate a range of building\nfeatures relevant for sustainability calculations. We compare our novel\nimage-to-data method with a ground truth comprising real building data for 47\napartments and achieve accuracy better than a human performing the same task.\nWe also demonstrate that the method can generate tailored recommendations to\nthe owner on how best to improve their properties and discuss methods to scale\nthe approach.",
      "tldr_zh": "这篇论文提出了一种从建筑物图像自动生成可持续性数据的创新方法，使用 Large Language Model (LLM) 结合提示工程和领域知识，来估计与碳排放相关的建筑物特征。实验结果显示，该 image-to-data 方法在47套公寓的真实数据上比人类手动评估更准确。论文还展示了如何基于这些数据生成针对性的改进推荐，并探讨了扩展该方法的潜在途径。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "68T07, 94A08"
      ],
      "primary_category": "cs.AI",
      "comment": "6 pages",
      "pdf_url": "http://arxiv.org/pdf/2405.18064v2",
      "published_date": "2024-05-28 11:24:20 UTC",
      "updated_date": "2024-08-28 13:41:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:06:59.468642"
    },
    {
      "arxiv_id": "2406.17789v1",
      "title": "Spanish and LLM Benchmarks: is MMLU Lost in Translation?",
      "title_zh": "西班牙语和 LLM 基准测试：MMLU 是否在翻译中丢失了？",
      "authors": [
        "Irene Plaza",
        "Nina Melero",
        "Cristina del Pozo",
        "Javier Conde",
        "Pedro Reviriego",
        "Marina Mayor-Rocher",
        "María Grandury"
      ],
      "abstract": "The evaluation of Large Language Models (LLMs) is a key element in their\ncontinuous improvement process and many benchmarks have been developed to\nassess the performance of LLMs in different tasks and topics. As LLMs become\nadopted worldwide, evaluating them in languages other than English is\nincreasingly important. However, most LLM benchmarks are simply translated\nusing an automated tool and then run in the target language. This means that\nthe results depend not only on the LLM performance in that language but also on\nthe quality of the translation. In this paper, we consider the case of the\nwell-known Massive Multitask Language Understanding (MMLU) benchmark. Selected\ncategories of the benchmark are translated into Spanish using Azure Translator\nand ChatGPT4 and run on ChatGPT4. Next, the results are processed to identify\nthe test items that produce different answers in Spanish and English. Those are\nthen analyzed manually to understand if the automatic translation caused the\nchange. The results show that a significant fraction of the failing items can\nbe attributed to mistakes in the translation of the benchmark. These results\nmake a strong case for improving benchmarks in languages other than English by\nat least revising the translations of the items and preferably by adapting the\ntests to the target language by experts.",
      "tldr_zh": "这篇论文探讨了大型语言模型（LLMs）的基准测试在非英语语言中的评估问题，特别是聚焦于MMLU基准测试是否因翻译而失真。作者使用Azure Translator和ChatGPT4将MMLU的部分类别翻译成西班牙语，然后运行测试并比较西班牙语和英语版本的结果。分析发现，许多测试项的答案差异源于翻译错误，这凸显了自动翻译的局限性。论文强烈建议改进非英语基准测试，通过至少审阅翻译或由专家进行适应，以确保评估的准确性和可靠性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.17789v1",
      "published_date": "2024-05-28 11:13:40 UTC",
      "updated_date": "2024-05-28 11:13:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:07:11.598473"
    },
    {
      "arxiv_id": "2405.18050v2",
      "title": "Learning-Based Link Anomaly Detection in Continuous-Time Dynamic Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Tim Poštuvan",
        "Claas Grohnfeldt",
        "Michele Russo",
        "Giulio Lovisotto"
      ],
      "abstract": "Anomaly detection in continuous-time dynamic graphs is an emerging field yet\nunder-explored in the context of learning algorithms. In this paper, we pioneer\nstructured analyses of link-level anomalies and graph representation learning\nfor identifying categorically anomalous graph links. First, we introduce a\nfine-grained taxonomy for edge-level anomalies leveraging structural, temporal,\nand contextual graph properties. Based on these properties, we introduce a\nmethod for generating and injecting typed anomalies into graphs. Next, we\nintroduce a novel method to generate continuous-time dynamic graphs featuring\nconsistencies across either or combinations of time, structure, and context. To\nenable temporal graph learning methods to detect specific types of anomalous\nlinks rather than the bare existence of a link, we extend the generic link\nprediction setting by: (1) conditioning link existence on contextual edge\nattributes; and (2) refining the training regime to accommodate diverse\nperturbations in the negative edge sampler. Comprehensive benchmarks on\nsynthetic and real-world datasets -- featuring synthetic and labeled organic\nanomalies and employing six state-of-the-art link prediction methods --\nvalidate our taxonomy and generation processes for anomalies and benign graphs,\nas well as our approach to adapting methods for anomaly detection. Our results\nreveal that different learning methods excel in capturing different aspects of\ngraph normality and detecting different types of anomalies. We conclude with a\ncomprehensive list of findings highlighting opportunities for future research.",
      "tldr_zh": "本研究针对连续-time dynamic graphs中的链接异常检测问题，提出了一种基于学习算法的细粒度方法。首先，引入一个基于结构、时间和上下文属性的边级异常分类体系，并开发了生成和注入类型化异常的技术，以创建一致的动态图数据集。其次，通过扩展链接 prediction 设置——包括条件化链接存在于上下文属性并优化负边采样训练——使现有方法能够检测特定异常类型。实验在合成和真实数据集上验证了该框架，使用六种最先进链接 prediction 方法，结果显示不同方法在捕捉图常态和识别异常方面有显著差异，为未来研究提供了宝贵见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "Transactions on Machine Learning Research (TMLR), 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.18050v2",
      "published_date": "2024-05-28 11:05:41 UTC",
      "updated_date": "2024-09-28 12:33:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:07:23.995038"
    },
    {
      "arxiv_id": "2405.18047v1",
      "title": "2BP: 2-Stage Backpropagation",
      "title_zh": "翻译失败",
      "authors": [
        "Christopher Rae",
        "Joseph K. L. Lee",
        "James Richings"
      ],
      "abstract": "As Deep Neural Networks (DNNs) grow in size and complexity, they often exceed\nthe memory capacity of a single accelerator, necessitating the sharding of\nmodel parameters across multiple accelerators. Pipeline parallelism is a\ncommonly used sharding strategy for training large DNNs. However, current\nimplementations of pipeline parallelism are being unintentionally bottlenecked\nby the automatic differentiation tools provided by ML frameworks. This paper\nintroduces 2-stage backpropagation (2BP). By splitting the backward propagation\nstep into two separate stages, we can reduce idle compute time. We tested 2BP\non various model architectures and pipelining schedules, achieving increases in\nthroughput in all cases. Using 2BP, we were able to achieve a 1.70x increase in\nthroughput compared to traditional methods when training a LLaMa-like\ntransformer with 7 billion parameters across 4 GPUs.",
      "tldr_zh": "该论文提出了一种名为 2BP 的 2-Stage Backpropagation 方法，用于优化深度神经网络 (DNNs) 的训练过程。传统 pipeline parallelism 策略因机器学习框架的自动微分工具而导致计算空闲时间增加，2BP 通过将 backward propagation 分成两个独立阶段，显著减少了这些瓶颈。实验结果显示，在多种模型架构和管道调度下，2BP 实现了吞吐量提升，并在 4 个 GPUs 上训练一个 70 亿参数的 LLaMa-like transformer 时，比传统方法提高了 1.70 倍的吞吐量。该方法为大规模 DNNs 训练提供了更高效的并行策略。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18047v1",
      "published_date": "2024-05-28 11:02:01 UTC",
      "updated_date": "2024-05-28 11:02:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:07:36.180816"
    },
    {
      "arxiv_id": "2405.18044v2",
      "title": "Cognitive Insights and Stable Coalition Matching for Fostering Multi-Agent Cooperation",
      "title_zh": "翻译失败",
      "authors": [
        "Jiaqi Shao",
        "Tianjun Yuan",
        "Tao Lin",
        "Bing Luo"
      ],
      "abstract": "Cognitive abilities, such as Theory of Mind (ToM), play a vital role in\nfacilitating cooperation in human social interactions. However, our study\nreveals that agents with higher ToM abilities may not necessarily exhibit\nbetter cooperative behavior compared to those with lower ToM abilities. To\naddress this challenge, we propose a novel matching coalition mechanism that\nleverages the strengths of agents with different ToM levels by explicitly\nconsidering belief alignment and specialized abilities when forming coalitions.\nOur proposed matching algorithm seeks to find stable coalitions that maximize\nthe potential for cooperative behavior and ensure long-term viability. By\nincorporating cognitive insights into the design of multi-agent systems, our\nwork demonstrates the potential of leveraging ToM to create more sophisticated\nand human-like coordination strategies that foster cooperation and improve\noverall system performance.",
      "tldr_zh": "该研究发现，虽然 Theory of Mind (ToM) 能力有助于促进人类社会互动，但代理的高 ToM 水平并不一定导致更好的合作行为。为解决这一问题，研究提出了一种新型的匹配联盟机制，该机制通过考虑信念对齐和专业能力来匹配不同 ToM 水平的代理，形成稳定的联盟(stable coalitions)。这种方法最大化了合作潜力，确保长期可行性，并展示了将认知洞见融入多智能体系统设计中，以创建更人性化的协调策略并提升整体性能。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18044v2",
      "published_date": "2024-05-28 10:59:33 UTC",
      "updated_date": "2025-05-14 15:08:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:07:47.859492"
    },
    {
      "arxiv_id": "2405.18040v1",
      "title": "Fast-FedUL: A Training-Free Federated Unlearning with Provable Skew Resilience",
      "title_zh": "翻译失败",
      "authors": [
        "Thanh Trung Huynh",
        "Trong Bang Nguyen",
        "Phi Le Nguyen",
        "Thanh Tam Nguyen",
        "Matthias Weidlich",
        "Quoc Viet Hung Nguyen",
        "Karl Aberer"
      ],
      "abstract": "Federated learning (FL) has recently emerged as a compelling machine learning\nparadigm, prioritizing the protection of privacy for training data. The\nincreasing demand to address issues such as ``the right to be forgotten'' and\ncombat data poisoning attacks highlights the importance of techniques, known as\n\\textit{unlearning}, which facilitate the removal of specific training data\nfrom trained FL models. Despite numerous unlearning methods proposed for\ncentralized learning, they often prove inapplicable to FL due to fundamental\ndifferences in the operation of the two learning paradigms. Consequently,\nunlearning in FL remains in its early stages, presenting several challenges.\nMany existing unlearning solutions in FL require a costly retraining process,\nwhich can be burdensome for clients. Moreover, these methods are primarily\nvalidated through experiments, lacking theoretical assurances. In this study,\nwe introduce Fast-FedUL, a tailored unlearning method for FL, which eliminates\nthe need for retraining entirely. Through meticulous analysis of the target\nclient's influence on the global model in each round, we develop an algorithm\nto systematically remove the impact of the target client from the trained\nmodel. In addition to presenting empirical findings, we offer a theoretical\nanalysis delineating the upper bound of our unlearned model and the exact\nretrained model (the one obtained through retraining using untargeted clients).\nExperimental results with backdoor attack scenarios indicate that Fast-FedUL\neffectively removes almost all traces of the target client, while retaining the\nknowledge of untargeted clients (obtaining a high accuracy of up to 98\\% on the\nmain task). Significantly, Fast-FedUL attains the lowest time complexity,\nproviding a speed that is 1000 times faster than retraining. Our source code is\npublicly available at \\url{https://github.com/thanhtrunghuynh93/fastFedUL}.",
      "tldr_zh": "该论文提出Fast-FedUL，一种无需重新训练的联邦学习(Federated Learning, FL)遗忘(unlearning)方法，旨在解决数据隐私问题，如“被遗忘权”和数据中毒攻击，同时提供可证明的偏差抵抗力(Skew Resilience)。Fast-FedUL通过分析目标客户端对全局模型的影响，系统性地移除其影响，而不需进行昂贵的重新训练过程。实验结果显示，该方法在后门攻击(Backdoor Attack)场景中有效消除目标客户端的痕迹，同时保留其他客户端的知识，实现高达98%的任务准确率，且速度比重新训练快1000倍。理论分析进一步证明了未学习模型与重新训练模型之间的上界，确保方法的可靠性和效率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "cs.ET"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted in ECML PKDD 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.18040v1",
      "published_date": "2024-05-28 10:51:38 UTC",
      "updated_date": "2024-05-28 10:51:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:07:59.575118"
    },
    {
      "arxiv_id": "2405.18028v1",
      "title": "Edinburgh Clinical NLP at MEDIQA-CORR 2024: Guiding Large Language Models with Hints",
      "title_zh": "翻译失败",
      "authors": [
        "Aryo Pradipta Gema",
        "Chaeeun Lee",
        "Pasquale Minervini",
        "Luke Daines",
        "T. Ian Simpson",
        "Beatrice Alex"
      ],
      "abstract": "The MEDIQA-CORR 2024 shared task aims to assess the ability of Large Language\nModels (LLMs) to identify and correct medical errors in clinical notes. In this\nstudy, we evaluate the capability of general LLMs, specifically GPT-3.5 and\nGPT-4, to identify and correct medical errors with multiple prompting\nstrategies. Recognising the limitation of LLMs in generating accurate\ncorrections only via prompting strategies, we propose incorporating error-span\npredictions from a smaller, fine-tuned model in two ways: 1) by presenting it\nas a hint in the prompt and 2) by framing it as multiple-choice questions from\nwhich the LLM can choose the best correction. We found that our proposed\nprompting strategies significantly improve the LLM's ability to generate\ncorrections. Our best-performing solution with 8-shot + CoT + hints ranked\nsixth in the shared task leaderboard. Additionally, our comprehensive analyses\nshow the impact of the location of the error sentence, the prompted role, and\nthe position of the multiple-choice option on the accuracy of the LLM. This\nprompts further questions about the readiness of LLM to be implemented in\nreal-world clinical settings.",
      "tldr_zh": "这篇论文评估了大型语言模型 (LLMs) 如 GPT-3.5 和 GPT-4 在 MEDIQA-CORR 2024 任务中识别和纠正临床笔记医疗错误的能力，并提出通过整合一个微调模型的错误预测作为提示或多选问题来优化 prompting strategies。他们的方法包括使用 8-shot + CoT + hints 策略，显著提升了 LLMs 的纠正准确性，最终的最佳方案在共享任务中排名第六。论文还分析了错误句子位置、提示角色和多选选项位置对性能的影响，并质疑 LLMs 在真实临床环境中的实际可行性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18028v1",
      "published_date": "2024-05-28 10:20:29 UTC",
      "updated_date": "2024-05-28 10:20:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:08:14.018848"
    },
    {
      "arxiv_id": "2405.18025v2",
      "title": "Where's Waldo: Diffusion Features for Personalized Segmentation and Retrieval",
      "title_zh": "Where's Waldo：扩散特征用于个性化分割和检索",
      "authors": [
        "Dvir Samuel",
        "Rami Ben-Ari",
        "Matan Levy",
        "Nir Darshan",
        "Gal Chechik"
      ],
      "abstract": "Personalized retrieval and segmentation aim to locate specific instances\nwithin a dataset based on an input image and a short description of the\nreference instance. While supervised methods are effective, they require\nextensive labeled data for training. Recently, self-supervised foundation\nmodels have been introduced to these tasks showing comparable results to\nsupervised methods. However, a significant flaw in these models is evident:\nthey struggle to locate a desired instance when other instances within the same\nclass are presented. In this paper, we explore text-to-image diffusion models\nfor these tasks. Specifically, we propose a novel approach called PDM for\nPersonalized Features Diffusion Matching, that leverages intermediate features\nof pre-trained text-to-image models for personalization tasks without any\nadditional training. PDM demonstrates superior performance on popular retrieval\nand segmentation benchmarks, outperforming even supervised methods. We also\nhighlight notable shortcomings in current instance and segmentation datasets\nand propose new benchmarks for these tasks.",
      "tldr_zh": "该论文探讨了个性化检索和分割任务，即基于输入图像和描述在数据集内定位特定实例。现有方法依赖于监督学习，需要大量标注数据，而自监督模型则在处理同一类中的实例时表现不足。为解决这些问题，作者提出PDM（Personalized Features Diffusion Matching）方法，利用预训练文本到图像扩散模型的中间特征进行个性化任务，无需额外训练。实验结果显示，PDM在流行基准上超越了监督方法，同时作者指出了当前数据集的不足并提出了新的基准以提升任务评估。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.18025v2",
      "published_date": "2024-05-28 10:13:18 UTC",
      "updated_date": "2024-09-30 12:50:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:08:26.542665"
    },
    {
      "arxiv_id": "2405.18016v4",
      "title": "On Creativity and Open-Endedness",
      "title_zh": "翻译失败",
      "authors": [
        "L. B. Soros",
        "Alyssa Adams",
        "Stefano Kalonaris",
        "Olaf Witkowski",
        "Christian Guckelsberger"
      ],
      "abstract": "Artificial Life (ALife) as an interdisciplinary field draws inspiration and\ninfluence from a variety of perspectives. Scientific progress crucially\ndepends, then, on concerted efforts to invite cross-disciplinary dialogue. The\ngoal of this paper is to revitalize discussions of potential connections\nbetween the fields of Computational Creativity (CC) and ALife, focusing\nspecifically on the concept of Open-Endedness (OE); the primary goal of CC is\nto endow artificial systems with creativity, and ALife has dedicated much\nresearch effort into studying and synthesizing OE and artificial innovation.\nHowever, despite the close proximity of these concepts, their use so far\nremains confined to their respective communities, and their relationship is\nlargely unclear. We provide historical context for research in both domains,\nand review the limited work connecting research on creativity and OE\nexplicitly. We then highlight specific questions to be considered, with the\neventual goals of (i) decreasing conceptual ambiguity by highlighting\nsimilarities and differences between the concepts of OE and creativity, (ii)\nidentifying synergy effects of a research agenda that encompasses both\nconcepts, and (iii) establishing a dialogue between ALife and CC research.",
      "tldr_zh": "这篇论文探讨了人工生命（ALife）和计算创造力（CC）之间的潜在联系，焦点在于开放性（Open-Endedness，OE），旨在促进跨学科对话。论文回顾了两个领域的历史背景，并总结了目前有限的直接相关研究，强调尽管OE和创造力概念相近，但它们在各自社区中独立发展，关系尚不明确。主要贡献包括提出具体问题，以减少概念模糊、识别研究协同效应，并建立ALife和CC之间的对话，从而推动人工创新和创造性系统的研究。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages, accepted for publication in the proceedings of the 2024\n  International Conference for Artificial Life, Copenhagen, Denmark",
      "pdf_url": "http://arxiv.org/pdf/2405.18016v4",
      "published_date": "2024-05-28 09:57:37 UTC",
      "updated_date": "2024-06-23 18:10:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:08:38.117780"
    },
    {
      "arxiv_id": "2405.18014v2",
      "title": "Coupled Mamba: Enhanced Multi-modal Fusion with Coupled State Space Model",
      "title_zh": "翻译失败",
      "authors": [
        "Wenbing Li",
        "Hang Zhou",
        "Junqing Yu",
        "Zikai Song",
        "Wei Yang"
      ],
      "abstract": "The essence of multi-modal fusion lies in exploiting the complementary\ninformation inherent in diverse modalities. However, prevalent fusion methods\nrely on traditional neural architectures and are inadequately equipped to\ncapture the dynamics of interactions across modalities, particularly in\npresence of complex intra- and inter-modality correlations. Recent advancements\nin State Space Models (SSMs), notably exemplified by the Mamba model, have\nemerged as promising contenders. Particularly, its state evolving process\nimplies stronger modality fusion paradigm, making multi-modal fusion on SSMs an\nappealing direction. However, fusing multiple modalities is challenging for\nSSMs due to its hardware-aware parallelism designs. To this end, this paper\nproposes the Coupled SSM model, for coupling state chains of multiple\nmodalities while maintaining independence of intra-modality state processes.\nSpecifically, in our coupled scheme, we devise an inter-modal hidden states\ntransition scheme, in which the current state is dependent on the states of its\nown chain and that of the neighbouring chains at the previous time-step. To\nfully comply with the hardware-aware parallelism, we devise an expedite coupled\nstate transition scheme and derive its corresponding global convolution kernel\nfor parallelism. Extensive experiments on CMU-MOSEI, CH-SIMS, CH-SIMSV2 through\nmulti-domain input verify the effectiveness of our model compared to current\nstate-of-the-art methods, improved F1-Score by 0.4\\%, 0.9\\%, and 2.3\\% on the\nthree datasets respectively, 49\\% faster inference and 83.7\\% GPU memory save.\nThe results demonstrate that Coupled Mamba model is capable of enhanced\nmulti-modal fusion.",
      "tldr_zh": "本论文针对多模态融合中捕捉复杂intra- and inter-modality correlations的挑战，提出Coupled SSM模型，该模型基于State Space Models (SSMs)特别是Mamba模型，通过耦合状态链来增强模态间交互，同时保持intra-modality state processes的独立性。关键创新包括inter-modal hidden states transition scheme和expedite coupled state transition scheme，以支持硬件感知并行性，并推导出相应的global convolution kernel。实验在CMU-MOSEI、CH-SIMS和CH-SIMSV2数据集上验证了该模型的有效性，F1-Score分别提高了0.4%、0.9%和2.3%，推理速度加快49%，并节省83.7% GPU内存，从而实现了更高效的多模态融合。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18014v2",
      "published_date": "2024-05-28 09:57:03 UTC",
      "updated_date": "2024-05-29 05:19:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:08:52.118525"
    },
    {
      "arxiv_id": "2405.18003v1",
      "title": "MAVIN: Multi-Action Video Generation with Diffusion Models via Transition Video Infilling",
      "title_zh": "翻译失败",
      "authors": [
        "Bowen Zhang",
        "Xiaofei Xie",
        "Haotian Lu",
        "Na Ma",
        "Tianlin Li",
        "Qing Guo"
      ],
      "abstract": "Diffusion-based video generation has achieved significant progress, yet\ngenerating multiple actions that occur sequentially remains a formidable task.\nDirectly generating a video with sequential actions can be extremely\nchallenging due to the scarcity of fine-grained action annotations and the\ndifficulty in establishing temporal semantic correspondences and maintaining\nlong-term consistency. To tackle this, we propose an intuitive and\nstraightforward solution: splicing multiple single-action video segments\nsequentially. The core challenge lies in generating smooth and natural\ntransitions between these segments given the inherent complexity and\nvariability of action transitions. We introduce MAVIN (Multi-Action Video\nINfilling model), designed to generate transition videos that seamlessly\nconnect two given videos, forming a cohesive integrated sequence. MAVIN\nincorporates several innovative techniques to address challenges in the\ntransition video infilling task. Firstly, a consecutive noising strategy\ncoupled with variable-length sampling is employed to handle large infilling\ngaps and varied generation lengths. Secondly, boundary frame guidance (BFG) is\nproposed to address the lack of semantic guidance during transition generation.\nLastly, a Gaussian filter mixer (GFM) dynamically manages noise initialization\nduring inference, mitigating train-test discrepancy while preserving generation\nflexibility. Additionally, we introduce a new metric, CLIP-RS (CLIP Relative\nSmoothness), to evaluate temporal coherence and smoothness, complementing\ntraditional quality-based metrics. Experimental results on horse and tiger\nscenarios demonstrate MAVIN's superior performance in generating smooth and\ncoherent video transitions compared to existing methods.",
      "tldr_zh": "这篇论文提出 MAVIN 模型，利用 Diffusion Models 通过过渡视频填充（Transition Video Infilling）来生成多动作视频，解决直接生成顺序动作的挑战，如缺乏细粒度标注和维护长期一致性。MAVIN 的核心创新包括连续噪声策略结合可变长度采样、边界帧指导（BFG）以提供语义指导，以及高斯滤波器混合器（GFM）来优化噪声初始化，确保过渡段平滑自然。此外，论文引入新指标 CLIP-RS（CLIP Relative Smoothness）评估时间连贯性，实验在马和老虎场景中显示 MAVIN 显著优于现有方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.18003v1",
      "published_date": "2024-05-28 09:46:09 UTC",
      "updated_date": "2024-05-28 09:46:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:09:03.356081"
    },
    {
      "arxiv_id": "2405.17998v1",
      "title": "Source Echo Chamber: Exploring the Escalation of Source Bias in User, Data, and Recommender System Feedback Loop",
      "title_zh": "源回音室：探索用户、数据和推荐系统反馈循环中",
      "authors": [
        "Yuqi Zhou",
        "Sunhao Dai",
        "Liang Pang",
        "Gang Wang",
        "Zhenhua Dong",
        "Jun Xu",
        "Ji-Rong Wen"
      ],
      "abstract": "Recently, researchers have uncovered that neural retrieval models prefer\nAI-generated content (AIGC), called source bias. Compared to active search\nbehavior, recommendation represents another important means of information\nacquisition, where users are more prone to source bias. Furthermore, delving\ninto the recommendation scenario, as AIGC becomes integrated within the\nfeedback loop involving users, data, and the recommender system, it\nprogressively contaminates the candidate items, the user interaction history,\nand ultimately, the data used to train the recommendation models. How and to\nwhat extent the source bias affects the neural recommendation models within\nfeedback loop remains unknown. In this study, we extend the investigation of\nsource bias into the realm of recommender systems, specifically examining its\nimpact across different phases of the feedback loop. We conceptualize the\nprogression of AIGC integration into the recommendation content ecosystem in\nthree distinct phases-HGC dominate, HGC-AIGC coexist, and AIGC dominance-each\nrepresenting past, present, and future states, respectively. Through extensive\nexperiments across three datasets from diverse domains, we demonstrate the\nprevalence of source bias and reveal a potential digital echo chamber with\nsource bias amplification throughout the feedback loop. This trend risks\ncreating a recommender ecosystem with limited information source, such as AIGC,\nbeing disproportionately recommended. To counteract this bias and prevent its\nescalation in the feedback loop, we introduce a black-box debiasing method that\nmaintains model impartiality towards both HGC and AIGC. Our experimental\nresults validate the effectiveness of the proposed debiasing method, confirming\nits potential to disrupt the feedback loop.",
      "tldr_zh": "该研究探讨了source bias（来源偏见）在推荐系统中的放大效应，特别是当AI生成内容（AIGC）融入用户、数据和推荐系统feedback loop时，导致候选项和训练数据被污染。研究将feedback loop分为三个阶段——HGC dominate（人类生成内容主导）、HGC-AIGC coexist（两者共存）和AIGC dominance（AIGC主导）——并通过跨三个数据集的实验，揭示source bias的普遍性和其在循环中的放大趋势，可能形成数字回音室（digital echo chamber），从而限制信息来源多样性。为了缓解这一问题，研究提出了一种black-box debiasing method，以保持模型对HGC和AIGC的公正性，实验结果证实了该方法的有效性。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17998v1",
      "published_date": "2024-05-28 09:34:50 UTC",
      "updated_date": "2024-05-28 09:34:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:09:15.508791"
    },
    {
      "arxiv_id": "2405.17995v1",
      "title": "DMT-JEPA: Discriminative Masked Targets for Joint-Embedding Predictive Architecture",
      "title_zh": "翻译失败",
      "authors": [
        "Shentong Mo",
        "Sukmin Yun"
      ],
      "abstract": "The joint-embedding predictive architecture (JEPA) recently has shown\nimpressive results in extracting visual representations from unlabeled imagery\nunder a masking strategy. However, we reveal its disadvantages, notably its\ninsufficient understanding of local semantics. This deficiency originates from\nmasked modeling in the embedding space, resulting in a reduction of\ndiscriminative power and can even lead to the neglect of critical local\nsemantics. To bridge this gap, we introduce DMT-JEPA, a novel masked modeling\nobjective rooted in JEPA, specifically designed to generate discriminative\nlatent targets from neighboring information. Our key idea is simple: we\nconsider a set of semantically similar neighboring patches as a target of a\nmasked patch. To be specific, the proposed DMT-JEPA (a) computes feature\nsimilarities between each masked patch and its corresponding neighboring\npatches to select patches having semantically meaningful relations, and (b)\nemploys lightweight cross-attention heads to aggregate features of neighboring\npatches as the masked targets. Consequently, DMT-JEPA demonstrates strong\ndiscriminative power, offering benefits across a diverse spectrum of downstream\ntasks. Through extensive experiments, we demonstrate our effectiveness across\nvarious visual benchmarks, including ImageNet-1K image classification, ADE20K\nsemantic segmentation, and COCO object detection tasks. Code is available at:\n\\url{https://github.com/DMTJEPA/DMTJEPA}.",
      "tldr_zh": "该研究揭示了Joint-Embedding Predictive Architecture (JEPA) 在掩码建模中存在的不足，即对局部语义理解不足，导致辨别力减弱和关键局部语义被忽略。为解决此问题，作者提出DMT-JEPA，一种新型掩码建模目标，通过计算掩码patch与邻近patches的特征相似度，并使用轻量级跨注意力头聚合语义相关patches的特征，以生成有辨别力的潜在目标。实验结果显示，DMT-JEPA 在ImageNet-1K图像分类、ADE20K语义分割和COCO对象检测等视觉基准上表现出色，提升了下游任务的性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17995v1",
      "published_date": "2024-05-28 09:28:52 UTC",
      "updated_date": "2024-05-28 09:28:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:09:28.778930"
    },
    {
      "arxiv_id": "2405.17992v2",
      "title": "fMRI predictors based on language models of increasing complexity recover brain left lateralization",
      "title_zh": "翻译失败",
      "authors": [
        "Laurent Bonnasse-Gahot",
        "Christophe Pallier"
      ],
      "abstract": "Over the past decade, studies of naturalistic language processing where\nparticipants are scanned while listening to continuous text have flourished.\nUsing word embeddings at first, then large language models, researchers have\ncreated encoding models to analyze the brain signals. Presenting these models\nwith the same text as the participants allows to identify brain areas where\nthere is a significant correlation between the functional magnetic resonance\nimaging (fMRI) time series and the ones predicted by the models' artificial\nneurons. One intriguing finding from these studies is that they have revealed\nhighly symmetric bilateral activation patterns, somewhat at odds with the\nwell-known left lateralization of language processing. Here, we report analyses\nof an fMRI dataset where we manipulate the complexity of large language models,\ntesting 28 pretrained models from 8 different families, ranging from 124M to\n14.2B parameters. First, we observe that the performance of models in\npredicting brain responses follows a scaling law, where the fit with brain\nactivity increases linearly with the logarithm of the number of parameters of\nthe model (and its performance on natural language processing tasks). Second,\nalthough this effect is present in both hemispheres, it is stronger in the left\nthan in the right hemisphere. Specifically, the left-right difference in brain\ncorrelation follows a scaling law with the number of parameters. This finding\nreconciles computational analyses of brain activity using large language models\nwith the classic observation from aphasic patients showing left hemisphere\ndominance for language.",
      "tldr_zh": "这篇论文探讨了使用复杂度递增的语言模型来预测 fMRI 脑信号，从而恢复语言处理的左半球优势。研究者分析了一个 fMRI 数据集，测试了 28 个预训练模型（参数从 124M 到 14.2B），这些模型基于 8 个不同系列。结果显示，模型预测脑响应的性能遵循缩放定律，与模型参数数量的对数线性增加，且这种效应在左半球比右半球更显著。最终，该发现与失语症患者研究的经典观察一致，证实了语言处理的左半球主导性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.CL",
      "comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
      "pdf_url": "http://arxiv.org/pdf/2405.17992v2",
      "published_date": "2024-05-28 09:24:52 UTC",
      "updated_date": "2024-11-04 14:01:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:09:39.782988"
    },
    {
      "arxiv_id": "2405.17991v2",
      "title": "VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections",
      "title_zh": "VeLoRA：使用秩-1 子标记投影的内存高效训练",
      "authors": [
        "Roy Miles",
        "Pradyumna Reddy",
        "Ismail Elezi",
        "Jiankang Deng"
      ],
      "abstract": "Large language models (LLMs) have recently emerged as powerful tools for\ntackling many language-processing tasks. Despite their success, training and\nfine-tuning these models is still far too computationally and memory intensive.\nIn this paper, we identify and characterise the important components needed for\neffective model convergence using gradient descent. In doing so we find that\nthe intermediate activations used to implement backpropagation can be\nexcessively compressed without incurring any degradation in performance. This\nresult leads us to a cheap and memory-efficient algorithm for both fine-tuning\nand pre-training LLMs. The proposed algorithm simply divides the tokens up into\nsmaller sub-tokens before projecting them onto a fixed 1-dimensional subspace\nduring the forward pass. These features are then coarsely reconstructed during\nthe backward pass to implement the update rules. We confirm the effectiveness\nof our algorithm as being complimentary to many state-of-the-art PEFT methods\non the VTAB-1k fine-tuning benchmark. Furthermore, we outperform QLoRA for\nfine-tuning LLaMA and show competitive performance against other\nmemory-efficient pre-training methods on the large-scale C4 dataset.",
      "tldr_zh": "本论文提出VeLoRA，一种内存高效的训练算法，用于优化大型语言模型(LLMs)的微调和预训练，通过识别梯度下降的关键组件并过度压缩中间激活来实现。VeLoRA算法将令牌分成更小的子令牌，并在前向传递中将它们投影到固定的一维子空间，然后在后向传递中粗略重建特征，以减少计算开销。实验结果显示，该方法与现有PEFT技术互补，在VTAB-1k基准上表现出色，并在微调LLaMA时优于QLoRA，同时在C4数据集的预训练中与其他方法竞争。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "NeurIPS 2024. Code available at https://github.com/roymiles/VeLoRA",
      "pdf_url": "http://arxiv.org/pdf/2405.17991v2",
      "published_date": "2024-05-28 09:23:14 UTC",
      "updated_date": "2024-10-21 12:53:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:09:50.877133"
    },
    {
      "arxiv_id": "2405.17978v2",
      "title": "FASTopic: Pretrained Transformer is a Fast, Adaptive, Stable, and Transferable Topic Model",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaobao Wu",
        "Thong Nguyen",
        "Delvin Ce Zhang",
        "William Yang Wang",
        "Anh Tuan Luu"
      ],
      "abstract": "Topic models have been evolving rapidly over the years, from conventional to\nrecent neural models. However, existing topic models generally struggle with\neither effectiveness, efficiency, or stability, highly impeding their practical\napplications. In this paper, we propose FASTopic, a fast, adaptive, stable, and\ntransferable topic model. FASTopic follows a new paradigm: Dual\nSemantic-relation Reconstruction (DSR). Instead of previous conventional,\nVAE-based, or clustering-based methods, DSR directly models the semantic\nrelations among document embeddings from a pretrained Transformer and learnable\ntopic and word embeddings. By reconstructing through these semantic relations,\nDSR discovers latent topics. This brings about a neat and efficient topic\nmodeling framework. We further propose a novel Embedding Transport Plan (ETP)\nmethod. Rather than early straightforward approaches, ETP explicitly\nregularizes the semantic relations as optimal transport plans. This addresses\nthe relation bias issue and thus leads to effective topic modeling. Extensive\nexperiments on benchmark datasets demonstrate that our FASTopic shows superior\neffectiveness, efficiency, adaptivity, stability, and transferability, compared\nto state-of-the-art baselines across various scenarios.",
      "tldr_zh": "本研究针对现有主题模型（Topic Models）在有效性、效率和稳定性上的不足，提出了一种快速（Fast）、适应性（Adaptive）、稳定（Stable）和可转移（Transferable）的模型——FASTopic。FASTopic 采用新的 Dual Semantic-relation Reconstruction (DSR) 范式，通过重建预训练 Transformer 的文档嵌入、主题嵌入和词嵌入之间的语义关系，来发现潜在主题；同时引入 Embedding Transport Plan (ETP) 方法来正规化这些关系，避免偏差问题。实验在基准数据集上表明，FASTopic 在有效性、效率、适应性、稳定性和可转移性方面均优于现有最先进基线。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NeurIPS 2024. Code is available at\n  https://github.com/BobXWu/Fastopic",
      "pdf_url": "http://arxiv.org/pdf/2405.17978v2",
      "published_date": "2024-05-28 09:06:38 UTC",
      "updated_date": "2024-10-26 12:36:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:10:04.140975"
    },
    {
      "arxiv_id": "2405.17976v2",
      "title": "Yuan 2.0-M32: Mixture of Experts with Attention Router",
      "title_zh": "翻译失败",
      "authors": [
        "Shaohua Wu",
        "Jiangang Luo",
        "Xi Chen",
        "Lingjun Li",
        "Xudong Zhao",
        "Tong Yu",
        "Chao Wang",
        "Yue Wang",
        "Fei Wang",
        "Weixu Qiao",
        "Houbo He",
        "Zeru Zhang",
        "Zeyu Sun",
        "Junxiong Mao",
        "Chong Shen"
      ],
      "abstract": "Yuan 2.0-M32, with a similar base architecture as Yuan-2.0 2B, uses a\nmixture-of-experts architecture with 32 experts of which 2 experts are active.\nA new router network, Attention Router, is proposed and adopted for a more\nefficient selection of experts, which improves the accuracy compared to the\nmodel with classical router network. Yuan 2.0-M32 is trained with 2000B tokens\nfrom scratch, and the training computation consumption is only 9.25% of a dense\nmodel at the same parameter scale. Yuan 2.0-M32 demonstrates competitive\ncapability on coding, math, and various domains of expertise, with only 3.7B\nactive parameters of 40B in total, and 7.4 GFlops forward computation per\ntoken, both of which are only 1/19 of Llama3-70B. Yuan 2.0-M32 surpass\nLlama3-70B on MATH and ARC-Challenge benchmark, with accuracy of 55.89 and 95.8\nrespectively. The models and source codes of Yuan 2.0-M32 are released at\nGithub1.",
      "tldr_zh": "Yuan 2.0-M32 是一种基于 Mixture of Experts 的语言模型，采用 32 个专家其中 2 个活跃的设计，并引入了新的 Attention Router 网络来优化专家选择，提高模型准确性。 该模型从零开始训练了 2000B tokens，训练计算消耗仅为同规模密集模型的 9.25%，而活跃参数仅 3.7B（总 40B），每 token 前向计算仅 7.4 GFlops。 在编码、数学和各种领域基准上，Yuan 2.0-M32 超越 Llama3-70B，在 MATH 和 ARC-Challenge 上分别达到 55.89% 和 95.8% 的准确率，并已开源在 GitHub。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages,3 figures, 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2405.17976v2",
      "published_date": "2024-05-28 09:05:08 UTC",
      "updated_date": "2024-05-29 07:19:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:10:16.782796"
    },
    {
      "arxiv_id": "2405.17974v1",
      "title": "Recent Trends in Personalized Dialogue Generation: A Review of Datasets, Methodologies, and Evaluations",
      "title_zh": "个性化对话生成的最近趋势：数据集、方法论和评估的回顾",
      "authors": [
        "Yi-Pei Chen",
        "Noriki Nishida",
        "Hideki Nakayama",
        "Yuji Matsumoto"
      ],
      "abstract": "Enhancing user engagement through personalization in conversational agents\nhas gained significance, especially with the advent of large language models\nthat generate fluent responses. Personalized dialogue generation, however, is\nmultifaceted and varies in its definition -- ranging from instilling a persona\nin the agent to capturing users' explicit and implicit cues. This paper seeks\nto systemically survey the recent landscape of personalized dialogue\ngeneration, including the datasets employed, methodologies developed, and\nevaluation metrics applied. Covering 22 datasets, we highlight benchmark\ndatasets and newer ones enriched with additional features. We further analyze\n17 seminal works from top conferences between 2021-2023 and identify five\ndistinct types of problems. We also shed light on recent progress by LLMs in\npersonalized dialogue generation. Our evaluation section offers a comprehensive\nsummary of assessment facets and metrics utilized in these works. In\nconclusion, we discuss prevailing challenges and envision prospect directions\nfor future research in personalized dialogue generation.",
      "tldr_zh": "这篇论文综述了个性化对话生成的最新趋势，包括数据集、方法论和评估指标，旨在提升对话代理的用户互动体验。作者分析了22个关键数据集，突出了基准数据集和新数据集的功能扩展，并探讨了从2021-2023年17个顶会论文中提炼出的五种主要问题类型。论文还强调了大型语言模型（LLMs）在个性化对话生成中的进展，并总结了评估指标和挑战，最后提出未来研究方向以推动该领域发展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Presented in LREC-COLING 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.17974v1",
      "published_date": "2024-05-28 09:04:13 UTC",
      "updated_date": "2024-05-28 09:04:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:10:29.189792"
    },
    {
      "arxiv_id": "2405.17969v4",
      "title": "Knowledge Circuits in Pretrained Transformers",
      "title_zh": "预训练Transformer中的知识电路",
      "authors": [
        "Yunzhi Yao",
        "Ningyu Zhang",
        "Zekun Xi",
        "Mengru Wang",
        "Ziwen Xu",
        "Shumin Deng",
        "Huajun Chen"
      ],
      "abstract": "The remarkable capabilities of modern large language models are rooted in\ntheir vast repositories of knowledge encoded within their parameters, enabling\nthem to perceive the world and engage in reasoning. The inner workings of how\nthese models store knowledge have long been a subject of intense interest and\ninvestigation among researchers. To date, most studies have concentrated on\nisolated components within these models, such as the Multilayer Perceptrons and\nattention head. In this paper, we delve into the computation graph of the\nlanguage model to uncover the knowledge circuits that are instrumental in\narticulating specific knowledge. The experiments, conducted with GPT2 and\nTinyLLAMA, have allowed us to observe how certain information heads, relation\nheads, and Multilayer Perceptrons collaboratively encode knowledge within the\nmodel. Moreover, we evaluate the impact of current knowledge editing techniques\non these knowledge circuits, providing deeper insights into the functioning and\nconstraints of these editing methodologies. Finally, we utilize knowledge\ncircuits to analyze and interpret language model behaviors such as\nhallucinations and in-context learning. We believe the knowledge circuits hold\npotential for advancing our understanding of Transformers and guiding the\nimproved design of knowledge editing. Code and data are available in\nhttps://github.com/zjunlp/KnowledgeCircuits.",
      "tldr_zh": "本研究探讨了预训练 Transformers 中知识 circuits 的机制，揭示了 attention heads、relation heads 和 Multilayer Perceptrons 等组件如何协作编码特定知识。通过实验分析 GPT2 和 TinyLLAMA 的计算图，论文观察了这些 circuits 在知识存储和处理中的作用，并评估了现有知识编辑技术对其的影响。结果显示，知识 circuits 可用于解释语言模型行为，如 hallucinations 和 in-context learning，从而为改进 Transformers 的设计和知识编辑方法提供指导。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "NeurIPS 2024, 26 pages",
      "pdf_url": "http://arxiv.org/pdf/2405.17969v4",
      "published_date": "2024-05-28 08:56:33 UTC",
      "updated_date": "2025-01-03 16:41:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:10:41.652105"
    },
    {
      "arxiv_id": "2405.17959v1",
      "title": "Attention-based sequential recommendation system using multimodal data",
      "title_zh": "翻译失败",
      "authors": [
        "Hyungtaik Oh",
        "Wonkeun Jo",
        "Dongil Kim"
      ],
      "abstract": "Sequential recommendation systems that model dynamic preferences based on a\nuse's past behavior are crucial to e-commerce. Recent studies on these systems\nhave considered various types of information such as images and texts. However,\nmultimodal data have not yet been utilized directly to recommend products to\nusers. In this study, we propose an attention-based sequential recommendation\nmethod that employs multimodal data of items such as images, texts, and\ncategories. First, we extract image and text features from pre-trained VGG and\nBERT and convert categories into multi-labeled forms. Subsequently, attention\noperations are performed independent of the item sequence and multimodal\nrepresentations. Finally, the individual attention information is integrated\nthrough an attention fusion function. In addition, we apply multitask learning\nloss for each modality to improve the generalization performance. The\nexperimental results obtained from the Amazon datasets show that the proposed\nmethod outperforms those of conventional sequential recommendation systems.",
      "tldr_zh": "本研究提出了一种基于注意力的顺序推荐系统（attention-based sequential recommendation system），利用物品的多模态数据（如图像、文本和类别）来建模用户动态偏好，以提升电商推荐效果。方法包括从预训练的 VGG 和 BERT 提取图像及文本特征，将类别转换为多标签形式，然后进行独立的注意力操作并通过注意力融合函数（attention fusion function）整合多模态表示，同时应用多任务学习损失（multitask learning loss）来提高模型泛化性能。在 Amazon 数据集上的实验结果显示，该方法优于传统顺序推荐系统，证明了多模态数据的有效性。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "I.2.1; I.2.4; I.2.7"
      ],
      "primary_category": "cs.IR",
      "comment": "18 pages, 4 figures, preprinted",
      "pdf_url": "http://arxiv.org/pdf/2405.17959v1",
      "published_date": "2024-05-28 08:41:05 UTC",
      "updated_date": "2024-05-28 08:41:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:10:53.363068"
    },
    {
      "arxiv_id": "2405.17957v1",
      "title": "Modeling Dynamic Topics in Chain-Free Fashion by Evolution-Tracking Contrastive Learning and Unassociated Word Exclusion",
      "title_zh": "通过进化跟踪对比学习和无关词排除的无链式动态主题建模",
      "authors": [
        "Xiaobao Wu",
        "Xinshuai Dong",
        "Liangming Pan",
        "Thong Nguyen",
        "Anh Tuan Luu"
      ],
      "abstract": "Dynamic topic models track the evolution of topics in sequential documents,\nwhich have derived various applications like trend analysis and opinion mining.\nHowever, existing models suffer from repetitive topic and unassociated topic\nissues, failing to reveal the evolution and hindering further applications. To\naddress these issues, we break the tradition of simply chaining topics in\nexisting work and propose a novel neural \\modelfullname. We introduce a new\nevolution-tracking contrastive learning method that builds the similarity\nrelations among dynamic topics. This not only tracks topic evolution but also\nmaintains topic diversity, mitigating the repetitive topic issue. To avoid\nunassociated topics, we further present an unassociated word exclusion method\nthat consistently excludes unassociated words from discovered topics. Extensive\nexperiments demonstrate our model significantly outperforms state-of-the-art\nbaselines, tracking topic evolution with high-quality topics, showing better\nperformance on downstream tasks, and remaining robust to the hyperparameter for\nevolution intensities. Our code is available at https://github.com/bobxwu/CFDTM .",
      "tldr_zh": "本研究针对动态主题模型（dynamic topic models）中存在的重复主题和无关主题问题，提出了一种不依赖传统主题链的新模型，通过evolution-tracking contrastive learning方法构建动态主题间的相似性关系，以追踪主题演变并提升主题多样性。同时，该模型引入unassociated word exclusion方法，持续排除无关词以避免无关主题的生成。实验结果显示，该模型显著优于现有基线，在主题演变追踪和下游任务上表现出色，并对演变强度超参数具有鲁棒性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ACL 2024 Findings",
      "pdf_url": "http://arxiv.org/pdf/2405.17957v1",
      "published_date": "2024-05-28 08:39:49 UTC",
      "updated_date": "2024-05-28 08:39:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:11:06.564771"
    },
    {
      "arxiv_id": "2405.17956v3",
      "title": "Unified Preference Optimization: Language Model Alignment Beyond the Preference Frontier",
      "title_zh": "翻译失败",
      "authors": [
        "Anirudhan Badrinath",
        "Prabhat Agarwal",
        "Jiajing Xu"
      ],
      "abstract": "For aligning large language models (LLMs), prior work has leveraged\nreinforcement learning via human feedback (RLHF) or variations of direct\npreference optimization (DPO). While DPO offers a simpler framework based on\nmaximum likelihood estimation, it compromises on the ability to easily tune\nlanguage models to maximize auxiliary, non-preferential objectives according to\nthe LLM designer's preferences (e.g., tuning lexical style or minimizing\nspecific kinds of harmful content). Critically, these designer objectives may\nnot be amply human-labeled or represented in available data, align with user\npreferences, or even be able to be captured tractably by binary preference\npairs. To leverage the simplicity and performance of DPO with the generality of\nRL, we propose a unified approach. Based on a simple decomposition of\npreference and auxiliary objectives, we allow for tuning LLMs to optimize user\nand designer preferences without any additional specialized or preference data,\ncomputational cost, stability ``tweaks'', or training instability. The proposed\nmethod, Unified Preference Optimization, shows the ability to effectively\ngeneralize to user preferences and auxiliary objectives, while preserving or\nsurpassing alignment performance on challenging benchmarks across a range of\nmodel sizes.",
      "tldr_zh": "本研究针对大型语言模型（LLMs）的对齐问题，指出现有方法如强化学习通过人类反馈（RLHF）和直接偏好优化（DPO）存在局限性，尤其是DPO难以优化辅助目标（如词汇风格或减少有害内容）。为了解决这一问题，作者提出Unified Preference Optimization，一种基于偏好和辅助目标分解的统一方法，能够在不需额外偏好数据、计算成本或稳定调整的情况下，同时优化用户偏好和设计者目标。实验结果显示，该方法在各种模型规模上有效泛化，并保持或超越了基准测试中的对齐性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17956v3",
      "published_date": "2024-05-28 08:35:48 UTC",
      "updated_date": "2025-03-31 17:58:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:11:18.294199"
    },
    {
      "arxiv_id": "2405.17950v1",
      "title": "Self-Guiding Exploration for Combinatorial Problems",
      "title_zh": "自引导探索用于组合问题",
      "authors": [
        "Zangir Iklassov",
        "Yali Du",
        "Farkhad Akimov",
        "Martin Takac"
      ],
      "abstract": "Large Language Models (LLMs) have become pivotal in addressing reasoning\ntasks across diverse domains, including arithmetic, commonsense, and symbolic\nreasoning. They utilize prompting techniques such as Exploration-of-Thought,\nDecomposition, and Refinement to effectively navigate and solve intricate\ntasks. Despite these advancements, the application of LLMs to Combinatorial\nProblems (CPs), known for their NP-hardness and critical roles in logistics and\nresource management remains underexplored. To address this gap, we introduce a\nnovel prompting strategy: Self-Guiding Exploration (SGE), designed to enhance\nthe performance of solving CPs. SGE operates autonomously, generating multiple\nthought trajectories for each CP task. It then breaks these trajectories down\ninto actionable subtasks, executes them sequentially, and refines the results\nto ensure optimal outcomes. We present our research as the first to apply LLMs\nto a broad range of CPs and demonstrate that SGE outperforms existing prompting\nstrategies by over 27.84% in CP optimization performance. Additionally, SGE\nachieves a 2.46% higher accuracy over the best existing results in other\nreasoning tasks (arithmetic, commonsense, and symbolic).",
      "tldr_zh": "该研究针对组合问题(CPs)的NP-hard特性，引入了Self-Guiding Exploration(SGE)提示策略，以提升Large Language Models(LLMs)在CPs上的性能。SGE通过自主生成多个思考轨迹，将其分解为子任务、顺序执行并优化结果，从而有效解决CPs。实验结果表明，SGE在CPs优化性能上比现有策略提高了27.84%，并在算术、常识和符号推理任务上准确率提升2.46%。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "22 pages",
      "pdf_url": "http://arxiv.org/pdf/2405.17950v1",
      "published_date": "2024-05-28 08:26:54 UTC",
      "updated_date": "2024-05-28 08:26:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:11:31.428668"
    },
    {
      "arxiv_id": "2405.17942v2",
      "title": "Learning Shared RGB-D Fields: Unified Self-supervised Pre-training for Label-efficient LiDAR-Camera 3D Perception",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaohao Xu",
        "Ye Li",
        "Tianyi Zhang",
        "Jinrong Yang",
        "Matthew Johnson-Roberson",
        "Xiaonan Huang"
      ],
      "abstract": "Constructing large-scale labeled datasets for multi-modal perception model\ntraining in autonomous driving presents significant challenges. This has\nmotivated the development of self-supervised pretraining strategies. However,\nexisting pretraining methods mainly employ distinct approaches for each\nmodality. In contrast, we focus on LiDAR-Camera 3D perception models and\nintroduce a unified pretraining strategy, NeRF-Supervised Masked Auto Encoder\n(NS-MAE), which optimizes all modalities through a shared formulation. NS-MAE\nleverages NeRF's ability to encode both appearance and geometry, enabling\nefficient masked reconstruction of multi-modal data. Specifically, embeddings\nare extracted from corrupted LiDAR point clouds and images, conditioned on view\ndirections and locations. Then, these embeddings are rendered into multi-modal\nfeature maps from two crucial viewpoints for 3D driving perception: perspective\nand bird's-eye views. The original uncorrupted data serve as reconstruction\ntargets for self-supervised learning. Extensive experiments demonstrate the\nsuperior transferability of NS-MAE across various 3D perception tasks under\ndifferent fine-tuning settings. Notably, NS-MAE outperforms prior SOTA\npre-training methods that employ separate strategies for each modality in BEV\nmap segmentation under the label-efficient fine-tuning setting. Our code is\npublicly available at https://github.com/Xiaohao-Xu/Unified-Pretrain-AD/ .",
      "tldr_zh": "该研究提出了一种统一的 self-supervised 预训练策略 NS-MAE，用于标签高效的 LiDAR-Camera 3D 感知，旨在解决自动驾驶多模态模型训练中标注数据集构建的难题。\nNS-MAE 利用 NeRF 编码外观和几何信息，对损坏的 LiDAR 点云和图像进行掩码重建，提取嵌入并从透视视图和 bird's-eye views 渲染多模态特征图，使用原始数据作为重建目标。\n实验结果显示，NS-MAE 在各种 3D 感知任务中具有卓越的迁移性，并在标签高效微调设置下，在 BEV 地图分割任务中超越了现有采用分离策略的最先进方法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages",
      "pdf_url": "http://arxiv.org/pdf/2405.17942v2",
      "published_date": "2024-05-28 08:13:49 UTC",
      "updated_date": "2024-10-11 22:01:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:11:43.495958"
    },
    {
      "arxiv_id": "2405.17940v1",
      "title": "World Models for General Surgical Grasping",
      "title_zh": "一般手术抓取的世界模型",
      "authors": [
        "Hongbin Lin",
        "Bin Li",
        "Chun Wai Wong",
        "Juan Rojas",
        "Xiangyu Chu",
        "Kwok Wai Samuel Au"
      ],
      "abstract": "Intelligent vision control systems for surgical robots should adapt to\nunknown and diverse objects while being robust to system disturbances. Previous\nmethods did not meet these requirements due to mainly relying on pose\nestimation and feature tracking. We propose a world-model-based deep\nreinforcement learning framework \"Grasp Anything for Surgery\" (GAS), that\nlearns a pixel-level visuomotor policy for surgical grasping, enhancing both\ngenerality and robustness. In particular, a novel method is proposed to\nestimate the values and uncertainties of depth pixels for a rigid-link object's\ninaccurate region based on the empirical prior of the object's size; both depth\nand mask images of task objects are encoded to a single compact 3-channel image\n(size: 64x64x3) by dynamically zooming in the mask regions, minimizing the\ninformation loss. The learned controller's effectiveness is extensively\nevaluated in simulation and in a real robot. Our learned visuomotor policy\nhandles: i) unseen objects, including 5 types of target grasping objects and a\nrobot gripper, in unstructured real-world surgery environments, and ii)\ndisturbances in perception and control. Note that we are the first work to\nachieve a unified surgical control system that grasps diverse surgical objects\nusing different robot grippers on real robots in complex surgery scenes\n(average success rate: 69%). Our system also demonstrates significant\nrobustness across 6 conditions including background variation, target\ndisturbance, camera pose variation, kinematic control error, image noise, and\nre-grasping after the gripped target object drops from the gripper. Videos and\ncodes can be found on our project page: https://linhongbin.github.io/gas/.",
      "tldr_zh": "本文提出了一种基于世界模型(world-model-based)的深度强化学习框架“Grasp Anything for Surgery”(GAS)，旨在提升手术机器人对未知多样物体和系统干扰的适应性和鲁棒性。该框架学习像素级视动策略(surgical grasping)，通过一种新方法基于物体大小的经验先验估计深度像素值和不确定性，并将深度和掩码图像动态编码成紧凑的3通道图像(64x64x3)，以最小化信息损失。在模拟和真实机器人实验中，GAS成功处理5种未见物体和各种干扰（如背景变化和图像噪声），平均成功率达69%，首次实现统一的手术控制系统，能使用不同机器人夹爪在复杂场景中抓取多样物体。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17940v1",
      "published_date": "2024-05-28 08:11:12 UTC",
      "updated_date": "2024-05-28 08:11:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:11:55.358120"
    },
    {
      "arxiv_id": "2405.17935v3",
      "title": "Tool Learning with Large Language Models: A Survey",
      "title_zh": "基于大型语言模型的工具",
      "authors": [
        "Changle Qu",
        "Sunhao Dai",
        "Xiaochi Wei",
        "Hengyi Cai",
        "Shuaiqiang Wang",
        "Dawei Yin",
        "Jun Xu",
        "Ji-Rong Wen"
      ],
      "abstract": "Recently, tool learning with large language models (LLMs) has emerged as a\npromising paradigm for augmenting the capabilities of LLMs to tackle highly\ncomplex problems. Despite growing attention and rapid advancements in this\nfield, the existing literature remains fragmented and lacks systematic\norganization, posing barriers to entry for newcomers. This gap motivates us to\nconduct a comprehensive survey of existing works on tool learning with LLMs. In\nthis survey, we focus on reviewing existing literature from the two primary\naspects (1) why tool learning is beneficial and (2) how tool learning is\nimplemented, enabling a comprehensive understanding of tool learning with LLMs.\nWe first explore the \"why\" by reviewing both the benefits of tool integration\nand the inherent benefits of the tool learning paradigm from six specific\naspects. In terms of \"how\", we systematically review the literature according\nto a taxonomy of four key stages in the tool learning workflow: task planning,\ntool selection, tool calling, and response generation. Additionally, we provide\na detailed summary of existing benchmarks and evaluation methods, categorizing\nthem according to their relevance to different stages. Finally, we discuss\ncurrent challenges and outline potential future directions, aiming to inspire\nboth researchers and industrial developers to further explore this emerging and\npromising area. We also maintain a GitHub repository to continually keep track\nof the relevant papers and resources in this rising area at\nhttps://github.com/quchangle1/LLM-Tool-Survey.",
      "tldr_zh": "本调查综述了大型语言模型 (LLMs) 的工具学习范式，解释了其益处，包括增强模型能力处理复杂问题，并从六个方面审视工具集成的固有优势。论文系统地审视了工具学习的工作流，包括任务规划 (task planning)、工具选择 (tool selection)、工具调用 (tool calling) 和响应生成 (response generation) 等四个关键阶段。作者还总结了现有的基准和评估方法，并讨论了当前挑战以及未来研究方向，以推动该领域的进一步发展。GitHub 仓库 (https://github.com/quchangle1/LLM-Tool-Survey) 持续跟踪相关资源。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: {10.1007/s11704-024-40678-2}",
      "pdf_url": "http://arxiv.org/pdf/2405.17935v3",
      "published_date": "2024-05-28 08:01:26 UTC",
      "updated_date": "2024-11-04 15:07:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:12:05.541748"
    },
    {
      "arxiv_id": "2405.17934v2",
      "title": "Proof of Quality: A Costless Paradigm for Trustless Generative AI Model Inference on Blockchains",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenjie Zhang",
        "Yuyang Rao",
        "Hao Xiao",
        "Xiaokui Xiao",
        "Yin Yang"
      ],
      "abstract": "Generative AI models, such as GPT-4 and Stable Diffusion, have demonstrated\npowerful and disruptive capabilities in natural language and image tasks.\nHowever, deploying these models in decentralized environments remains\nchallenging. Unlike traditional centralized deployment, systematically\nguaranteeing the integrity of AI model services in fully decentralized\nenvironments, particularly on trustless blockchains, is both crucial and\ndifficult. In this paper, we present a new inference paradigm called\n\\emph{proof of quality} (PoQ) to enable the deployment of arbitrarily large\ngenerative models on blockchain architecture. Unlike traditional approaches\nbased on validating inference procedures, such as ZKML or OPML, our PoQ\nparadigm focuses on the outcome quality of model inference. Using lightweight\nBERT-based cross-encoders as our underlying quality evaluation model, we design\nand implement PQML, the first practical protocol for real-world NLP generative\nmodel inference on blockchains, tailored for popular open-source models such as\nLlama 3 and Mixtral. Our analysis demonstrates that our protocol is robust\nagainst adversarial but rational participants in ecosystems, where lazy or\ndishonest behavior results in fewer benefits compared to well-behaving\nparticipants. The computational overhead of validating the quality evaluation\nis minimal, allowing quality validators to complete the quality check within a\nsecond, even using only a CPU. Preliminary simulation results show that PoQ\nconsensus is generated in milliseconds, 1,000 times faster than any existing\nscheme.",
      "tldr_zh": "本论文提出了一种名为 proof of quality (PoQ) 的新范式，用于在区块链上部署生成式 AI 模型（如 GPT-4 和 Stable Diffusion），以确保模型推理的完整性，而非传统方法（如 ZKML 或 OPML）专注于验证推理过程。PoQ 通过轻量级的 BERT-based cross-encoders 作为质量评估模型，设计并实现了 PQML 协议，适用于开源 NLP 模型如 Llama 3 和 Mixtral，能够对理性但可能不诚实的参与者保持鲁棒性。实验结果显示，验证开销极低（可在秒级完成，甚至仅用 CPU），并能在毫秒内生成共识，比现有方案快 1000 倍，从而为可信区块链 AI 推理提供高效解决方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.17934v2",
      "published_date": "2024-05-28 08:00:54 UTC",
      "updated_date": "2024-05-30 13:26:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:12:19.234006"
    },
    {
      "arxiv_id": "2405.17927v1",
      "title": "The Evolution of Multimodal Model Architectures",
      "title_zh": "多模态模型架构的演变",
      "authors": [
        "Shakti N. Wadekar",
        "Abhishek Chaurasia",
        "Aman Chadha",
        "Eugenio Culurciello"
      ],
      "abstract": "This work uniquely identifies and characterizes four prevalent multimodal\nmodel architectural patterns in the contemporary multimodal landscape.\nSystematically categorizing models by architecture type facilitates monitoring\nof developments in the multimodal domain. Distinct from recent survey papers\nthat present general information on multimodal architectures, this research\nconducts a comprehensive exploration of architectural details and identifies\nfour specific architectural types. The types are distinguished by their\nrespective methodologies for integrating multimodal inputs into the deep neural\nnetwork model. The first two types (Type A and B) deeply fuses multimodal\ninputs within the internal layers of the model, whereas the following two types\n(Type C and D) facilitate early fusion at the input stage. Type-A employs\nstandard cross-attention, whereas Type-B utilizes custom-designed layers for\nmodality fusion within the internal layers. On the other hand, Type-C utilizes\nmodality-specific encoders, while Type-D leverages tokenizers to process the\nmodalities at the model's input stage. The identified architecture types aid\nthe monitoring of any-to-any multimodal model development. Notably, Type-C and\nType-D are currently favored in the construction of any-to-any multimodal\nmodels. Type-C, distinguished by its non-tokenizing multimodal model\narchitecture, is emerging as a viable alternative to Type-D, which utilizes\ninput-tokenizing techniques. To assist in model selection, this work highlights\nthe advantages and disadvantages of each architecture type based on data and\ncompute requirements, architecture complexity, scalability, simplification of\nadding modalities, training objectives, and any-to-any multimodal generation\ncapability.",
      "tldr_zh": "这篇论文系统地识别并分类了当代多模态模型的多项架构模式，聚焦于四种主要类型（Type A、B、C 和 D），以便更好地监控多模态领域的进展。Type A 使用标准 cross-attention，Type B 采用自定义层在内部层融合多模态输入，而 Type C 和 Type D 则在输入阶段实现早期融合，分别通过模态特定编码器和 tokenizer 处理。研究发现，Type C 和 Type D 更适合构建 any-to-any 多模态模型，其中 Type C 作为非 tokenizing 架构的备选方案正在兴起；此外，论文比较了各类型的优缺点，包括数据与计算需求、架构复杂性、可扩展性以及添加模态的简易度，以辅助模型选择。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.AI",
      "comment": "30 pages, 6 tables, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.17927v1",
      "published_date": "2024-05-28 07:48:15 UTC",
      "updated_date": "2024-05-28 07:48:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:12:31.997440"
    },
    {
      "arxiv_id": "2405.17924v1",
      "title": "Generative AI Enhances Team Performance and Reduces Need for Traditional Teams",
      "title_zh": "生成式 AI 提升团队绩效并减少对传统团队的需求",
      "authors": [
        "Ning Li",
        "Huaikang Zhou",
        "Kris Mikel-Hong"
      ],
      "abstract": "Recent advancements in generative artificial intelligence (AI) have\ntransformed collaborative work processes, yet the impact on team performance\nremains underexplored. Here we examine the role of generative AI in enhancing\nor replacing traditional team dynamics using a randomized controlled experiment\nwith 435 participants across 122 teams. We show that teams augmented with\ngenerative AI significantly outperformed those relying solely on human\ncollaboration across various performance measures. Interestingly, teams with\nmultiple AIs did not exhibit further gains, indicating diminishing returns with\nincreased AI integration. Our analysis suggests that centralized AI usage by a\nfew team members is more effective than distributed engagement. Additionally,\nindividual-AI pairs matched the performance of conventional teams, suggesting a\nreduced need for traditional team structures in some contexts. However, despite\nthis capability, individual-AI pairs still fell short of the performance levels\nachieved by AI-assisted teams. These findings underscore that while generative\nAI can replace some traditional team functions, more comprehensively\nintegrating AI within team structures provides superior benefits, enhancing\noverall effectiveness beyond individual efforts.",
      "tldr_zh": "该研究通过随机对照实验（涉及435名参与者和122个团队）探讨了生成式AI对团队绩效的影响，结果显示，使用生成式AI的团队在各种绩效指标上显著优于纯人类团队。实验发现，多个AI的整合并未带来额外收益，且集中式AI使用（由少数成员操作）比分布式使用更有效；此外，个体与AI的组合性能可媲美传统团队，表明在某些情境下可减少对传统团队的依赖。总体而言，虽然生成式AI能部分替代团队功能，但将其全面整合到团队结构中能进一步提升整体效能，超越个体努力。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "econ.GN",
        "q-fin.EC"
      ],
      "primary_category": "cs.HC",
      "comment": "55 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.17924v1",
      "published_date": "2024-05-28 07:47:03 UTC",
      "updated_date": "2024-05-28 07:47:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:12:43.614975"
    },
    {
      "arxiv_id": "2405.17921v1",
      "title": "Towards Clinical AI Fairness: Filling Gaps in the Puzzle",
      "title_zh": "迈向临床 AI 公平性：填补谜题中的空白",
      "authors": [
        "Mingxuan Liu",
        "Yilin Ning",
        "Salinelat Teixayavong",
        "Xiaoxuan Liu",
        "Mayli Mertens",
        "Yuqing Shang",
        "Xin Li",
        "Di Miao",
        "Jie Xu",
        "Daniel Shu Wei Ting",
        "Lionel Tim-Ee Cheng",
        "Jasmine Chiat Ling Ong",
        "Zhen Ling Teo",
        "Ting Fang Tan",
        "Narrendar RaviChandran",
        "Fei Wang",
        "Leo Anthony Celi",
        "Marcus Eng Hock Ong",
        "Nan Liu"
      ],
      "abstract": "The ethical integration of Artificial Intelligence (AI) in healthcare\nnecessitates addressing fairness-a concept that is highly context-specific\nacross medical fields. Extensive studies have been conducted to expand the\ntechnical components of AI fairness, while tremendous calls for AI fairness\nhave been raised from healthcare. Despite this, a significant disconnect\npersists between technical advancements and their practical clinical\napplications, resulting in a lack of contextualized discussion of AI fairness\nin clinical settings. Through a detailed evidence gap analysis, our review\nsystematically pinpoints several deficiencies concerning both healthcare data\nand the provided AI fairness solutions. We highlight the scarcity of research\non AI fairness in many medical domains where AI technology is increasingly\nutilized. Additionally, our analysis highlights a substantial reliance on group\nfairness, aiming to ensure equality among demographic groups from a macro\nhealthcare system perspective; in contrast, individual fairness, focusing on\nequity at a more granular level, is frequently overlooked. To bridge these\ngaps, our review advances actionable strategies for both the healthcare and AI\nresearch communities. Beyond applying existing AI fairness methods in\nhealthcare, we further emphasize the importance of involving healthcare\nprofessionals to refine AI fairness concepts and methods to ensure contextually\nrelevant and ethically sound AI applications in healthcare.",
      "tldr_zh": "这篇论文探讨了 AI 在医疗领域的公平性问题，强调现有技术进展与临床应用之间存在脱节，导致 AI 公平讨论缺乏具体上下文。通过证据缺口分析，作者识别出医疗数据和公平解决方案的不足，包括许多医疗领域的研究缺失，以及过度依赖 group fairness 而忽略 individual fairness。论文提出行动策略，呼吁 AI 和医疗社区合作，完善公平概念和方法，以实现更具伦理性和情境相关性的 AI 应用。",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17921v1",
      "published_date": "2024-05-28 07:42:55 UTC",
      "updated_date": "2024-05-28 07:42:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:12:55.920118"
    },
    {
      "arxiv_id": "2405.17918v1",
      "title": "Cost-Sensitive Multi-Fidelity Bayesian Optimization with Transfer of Learning Curve Extrapolation",
      "title_zh": "翻译失败",
      "authors": [
        "Dong Bok Lee",
        "Aoxuan Silvia Zhang",
        "Byungjoo Kim",
        "Junhyeon Park",
        "Juho Lee",
        "Sung Ju Hwang",
        "Hae Beom Lee"
      ],
      "abstract": "In this paper, we address the problem of cost-sensitive multi-fidelity\nBayesian Optimization (BO) for efficient hyperparameter optimization (HPO).\nSpecifically, we assume a scenario where users want to early-stop the BO when\nthe performance improvement is not satisfactory with respect to the required\ncomputational cost. Motivated by this scenario, we introduce utility, which is\na function predefined by each user and describes the trade-off between cost and\nperformance of BO. This utility function, combined with our novel acquisition\nfunction and stopping criterion, allows us to dynamically choose for each BO\nstep the best configuration that we expect to maximally improve the utility in\nfuture, and also automatically stop the BO around the maximum utility. Further,\nwe improve the sample efficiency of existing learning curve (LC) extrapolation\nmethods with transfer learning, while successfully capturing the correlations\nbetween different configurations to develop a sensible surrogate function for\nmulti-fidelity BO. We validate our algorithm on various LC datasets and found\nit outperform all the previous multi-fidelity BO and transfer-BO baselines we\nconsider, achieving significantly better trade-off between cost and performance\nof BO.",
      "tldr_zh": "本论文针对成本敏感的多保真贝叶斯优化（Bayesian Optimization, BO）问题，提出了一种高效的超参数优化（HPO）方法，通过用户预定义的效用函数来平衡成本和性能。方法引入了新型采集函数和停止准则，实现动态选择最佳配置以最大化未来效用，并自动停止BO过程，同时利用转移学习改进学习曲线（Learning Curve, LC）外推的样本效率。实验结果显示，该算法在各种LC数据集上优于现有多保真BO和转移BO基线，实现了显著更好的成本-性能权衡。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17918v1",
      "published_date": "2024-05-28 07:38:39 UTC",
      "updated_date": "2024-05-28 07:38:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:13:08.349775"
    },
    {
      "arxiv_id": "2405.17913v2",
      "title": "OV-DQUO: Open-Vocabulary DETR with Denoising Text Query Training and Open-World Unknown Objects Supervision",
      "title_zh": "翻译失败",
      "authors": [
        "Junjie Wang",
        "Bin Chen",
        "Bin Kang",
        "Yulin Li",
        "YiChi Chen",
        "Weizhi Xian",
        "Huifeng Chang",
        "Yong Xu"
      ],
      "abstract": "Open-vocabulary detection aims to detect objects from novel categories beyond\nthe base categories on which the detector is trained. However, existing\nopen-vocabulary detectors trained on base category data tend to assign higher\nconfidence to trained categories and confuse novel categories with the\nbackground. To resolve this, we propose OV-DQUO, an\n\\textbf{O}pen-\\textbf{V}ocabulary DETR with \\textbf{D}enoising text\n\\textbf{Q}uery training and open-world \\textbf{U}nknown \\textbf{O}bjects\nsupervision. Specifically, we introduce a wildcard matching method. This method\nenables the detector to learn from pairs of unknown objects recognized by the\nopen-world detector and text embeddings with general semantics, mitigating the\nconfidence bias between base and novel categories. Additionally, we propose a\ndenoising text query training strategy. It synthesizes foreground and\nbackground query-box pairs from open-world unknown objects to train the\ndetector through contrastive learning, enhancing its ability to distinguish\nnovel objects from the background. We conducted extensive experiments on the\nchallenging OV-COCO and OV-LVIS benchmarks, achieving new state-of-the-art\nresults of 45.6 AP50 and 39.3 mAP on novel categories respectively, without the\nneed for additional training data. Models and code are released at\n\\url{https://github.com/xiaomoguhz/OV-DQUO}",
      "tldr_zh": "该研究提出了一种名为 OV-DQUO 的开放词汇检测框架，基于 DETR 模型，旨在解决现有检测器在处理新类别对象时存在的置信度偏差和背景混淆问题。具体而言，OV-DQUO 引入了 Wildcard matching 方法，利用开放世界检测器识别的未知对象与通用语义文本嵌入进行训练，减少基类和新类别之间的置信度差异；同时，提出 Denoising text query training 策略，通过合成前景和背景查询框对进行对比学习，提升检测器对新对象的区分能力。在 OV-COCO 和 OV-LVIS 基准测试中，OV-DQUO 在新类别上分别达到 45.6 AP50 和 39.3 mAP 的最先进性能，且无需额外训练数据。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17913v2",
      "published_date": "2024-05-28 07:33:27 UTC",
      "updated_date": "2024-08-21 02:40:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:13:20.371746"
    },
    {
      "arxiv_id": "2405.17910v1",
      "title": "Human-Cobot collaboration's impact on success, time completion, errors, workload, gestures and acceptability during an assembly task",
      "title_zh": "翻译失败",
      "authors": [
        "Étienne Fournier",
        "Christine Jeoffrion",
        "Belal Hmedan",
        "Damien Pellier",
        "Humbert Fiorino",
        "Aurélie Landry"
      ],
      "abstract": "The 5.0 industry promotes collaborative robots (cobots). This research\nstudies the impacts of cobot collaboration using an experimental setup. 120\nparticipants realized a simple and a complex assembly task. 50% collaborated\nwith another human (H/H) and 50% with a cobot (H/C). The workload and the\nacceptability of the cobotic collaboration were measured. Working with a cobot\ndecreases the effect of the task complexity on the human workload and on the\noutput quality. However, it increases the time completion and the number of\ngestures (while decreasing their frequency). The H/C couples have a higher\nchance of success but they take more time and more gestures to realize the\ntask. The results of this research could help developers and stakeholders to\nunderstand the impacts of implementing a cobot in production chains.",
      "tldr_zh": "这项研究探讨了人类与协作机器人（cobot）合作对装配任务的影响，通过实验让120名参与者分别在简单和复杂任务中与另一人类（H/H组）或cobot（H/C组）协作，并测量成功率、完成时间、错误、工作负载、手势和可接受性。结果显示，H/C组减少了任务复杂性对工作负载和输出质量的影响，但增加了完成时间和手势数量，同时降低了手势频率，并提升了任务成功概率。这些发现有助于开发者和利益相关者更好地理解在生产链中引入cobot的潜在益处和挑战。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17910v1",
      "published_date": "2024-05-28 07:30:28 UTC",
      "updated_date": "2024-05-28 07:30:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:13:33.031079"
    },
    {
      "arxiv_id": "2405.17905v1",
      "title": "Cycle-YOLO: A Efficient and Robust Framework for Pavement Damage Detection",
      "title_zh": "Cycle-YOLO：一个高效且鲁棒的路面损坏检测框架",
      "authors": [
        "Zhengji Li",
        "Xi Xiao",
        "Jiacheng Xie",
        "Yuxiao Fan",
        "Wentao Wang",
        "Gang Chen",
        "Liqiang Zhang",
        "Tianyang Wang"
      ],
      "abstract": "With the development of modern society, traffic volume continues to increase\nin most countries worldwide, leading to an increase in the rate of pavement\ndamage Therefore, the real-time and highly accurate pavement damage detection\nand maintenance have become the current need. In this paper, an enhanced\npavement damage detection method with CycleGAN and improved YOLOv5 algorithm is\npresented. We selected 7644 self-collected images of pavement damage samples as\nthe initial dataset and augmented it by CycleGAN. Due to a substantial\ndifference between the images generated by CycleGAN and real road images, we\nproposed a data enhancement method based on an improved Scharr filter,\nCycleGAN, and Laplacian pyramid. To improve the target recognition effect on a\ncomplex background and solve the problem that the spatial pyramid pooling-fast\nmodule in the YOLOv5 network cannot handle multiscale targets, we introduced\nthe convolutional block attention module attention mechanism and proposed the\natrous spatial pyramid pooling with squeeze-and-excitation structure. In\naddition, we optimized the loss function of YOLOv5 by replacing the CIoU with\nEIoU. The experimental results showed that our algorithm achieved a precision\nof 0.872, recall of 0.854, and mean average precision@0.5 of 0.882 in detecting\nthree main types of pavement damage: cracks, potholes, and patching. On the\nGPU, its frames per second reached 68, meeting the requirements for real-time\ndetection. Its overall performance even exceeded the current more advanced\nYOLOv7 and achieved good results in practical applications, providing a basis\nfor decision-making in pavement damage detection and prevention.",
      "tldr_zh": "本文提出 Cycle-YOLO 框架，一种高效且鲁棒的路面损坏检测方法，结合 CycleGAN 进行图像数据增强，并改进 YOLOv5 算法以处理复杂背景和多尺度目标。具体改进包括引入 Convolutional Block Attention Module (CBAM) 注意力机制、Atrous Spatial Pyramid Pooling with Squeeze-and-Excitation (ASPP-SE) 结构，以及将损失函数从 CIoU 替换为 EIoU，以提升检测性能。实验结果显示，该框架在检测裂缝、坑洞和修补三种路面损坏时，精度达到 0.872、召回率 0.854、mAP@0.5 为 0.882，且帧率达 68 FPS，超过了 YOLOv7 的整体表现，并满足实时检测需求。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17905v1",
      "published_date": "2024-05-28 07:27:42 UTC",
      "updated_date": "2024-05-28 07:27:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:13:45.777549"
    },
    {
      "arxiv_id": "2405.17902v2",
      "title": "Boosting Protein Language Models with Negative Sample Mining",
      "title_zh": "翻译失败",
      "authors": [
        "Yaoyao Xu",
        "Xinjian Zhao",
        "Xiaozhuang Song",
        "Benyou Wang",
        "Tianshu Yu"
      ],
      "abstract": "We introduce a pioneering methodology for boosting large language models in\nthe domain of protein representation learning. Our primary contribution lies in\nthe refinement process for correlating the over-reliance on co-evolution\nknowledge, in a way that networks are trained to distill invaluable insights\nfrom negative samples, constituted by protein pairs sourced from disparate\ncategories. By capitalizing on this novel approach, our technique steers the\ntraining of transformer-based models within the attention score space. This\nadvanced strategy not only amplifies performance but also reflects the nuanced\nbiological behaviors exhibited by proteins, offering aligned evidence with\ntraditional biological mechanisms such as protein-protein interaction. We\nexperimentally observed improved performance on various tasks over datasets, on\ntop of several well-established large protein models. This innovative paradigm\nopens up promising horizons for further progress in the realms of protein\nresearch and computational biology.",
      "tldr_zh": "这篇论文提出了一种创新方法，通过负样本挖掘（Negative Sample Mining）来提升蛋白质语言模型（Protein Language Models），以减少模型对共进化知识的过度依赖。方法涉及使用来自不同类别的蛋白质对作为负样本，在注意力分数空间中训练Transformer-based模型，从而更好地捕捉蛋白质的生物行为，如蛋白质-蛋白质相互作用（Protein-Protein Interaction）。实验结果显示，该方法在多种任务和数据集上显著提高了性能，超越了现有的大型蛋白质模型，并为蛋白质研究和计算生物学领域带来了新的发展前景。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 4 figures. Accepted by ECML-PKDD 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.17902v2",
      "published_date": "2024-05-28 07:24:20 UTC",
      "updated_date": "2024-06-29 07:07:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:13:55.897225"
    },
    {
      "arxiv_id": "2405.17901v1",
      "title": "Near-Infrared and Low-Rank Adaptation of Vision Transformers in Remote Sensing",
      "title_zh": "翻译失败",
      "authors": [
        "Irem Ulku",
        "O. Ozgur Tanriover",
        "Erdem Akagündüz"
      ],
      "abstract": "Plant health can be monitored dynamically using multispectral sensors that\nmeasure Near-Infrared reflectance (NIR). Despite this potential, obtaining and\nannotating high-resolution NIR images poses a significant challenge for\ntraining deep neural networks. Typically, large networks pre-trained on the RGB\ndomain are utilized to fine-tune infrared images. This practice introduces a\ndomain shift issue because of the differing visual traits between RGB and NIR\nimages.As an alternative to fine-tuning, a method called low-rank adaptation\n(LoRA) enables more efficient training by optimizing rank-decomposition\nmatrices while keeping the original network weights frozen. However, existing\nparameter-efficient adaptation strategies for remote sensing images focus on\nRGB images and overlook domain shift issues in the NIR domain. Therefore, this\nstudy investigates the potential benefits of using vision transformer (ViT)\nbackbones pre-trained in the RGB domain, with low-rank adaptation for\ndownstream tasks in the NIR domain. Extensive experiments demonstrate that\nemploying LoRA with pre-trained ViT backbones yields the best performance for\ndownstream tasks applied to NIR images.",
      "tldr_zh": "本研究针对使用Near-Infrared (NIR) 图像监测植物健康的挑战，指出获取和标注NIR图像困难，且从RGB域预训练的模型会因domain shift问题导致性能下降。作者提出了一种方法，利用在RGB域预训练的Vision Transformers (ViT) 骨干网络结合Low-Rank Adaptation (LoRA)，通过优化秩分解矩阵而保持原权重冻结，从而高效适应NIR域的下游任务。实验结果显示，这种LoRA与ViT的结合在多种NIR图像任务中取得了最佳性能，证明了其在Remote Sensing领域的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "7 pages, 3 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2405.17901v1",
      "published_date": "2024-05-28 07:24:07 UTC",
      "updated_date": "2024-05-28 07:24:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:14:08.824253"
    },
    {
      "arxiv_id": "2405.17898v1",
      "title": "FlashST: A Simple and Universal Prompt-Tuning Framework for Traffic Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Zhonghang Li",
        "Lianghao Xia",
        "Yong Xu",
        "Chao Huang"
      ],
      "abstract": "The objective of traffic prediction is to accurately forecast and analyze the\ndynamics of transportation patterns, considering both space and time. However,\nthe presence of distribution shift poses a significant challenge in this field,\nas existing models struggle to generalize well when faced with test data that\nsignificantly differs from the training distribution. To tackle this issue,\nthis paper introduces a simple and universal spatio-temporal prompt-tuning\nframework-FlashST, which adapts pre-trained models to the specific\ncharacteristics of diverse downstream datasets, improving generalization in\ndiverse traffic prediction scenarios. Specifically, the FlashST framework\nemploys a lightweight spatio-temporal prompt network for in-context learning,\ncapturing spatio-temporal invariant knowledge and facilitating effective\nadaptation to diverse scenarios. Additionally, we incorporate a distribution\nmapping mechanism to align the data distributions of pre-training and\ndownstream data, facilitating effective knowledge transfer in spatio-temporal\nforecasting. Empirical evaluations demonstrate the effectiveness of our FlashST\nacross different spatio-temporal prediction tasks using diverse urban datasets.\nCode is available at https://github.com/HKUDS/FlashST.",
      "tldr_zh": "这篇论文针对交通预测中的分布偏移（distribution shift）问题，提出了一种简单且通用的提示调整框架——FlashST，用于适应预训练模型到不同下游数据集，提高模型的泛化性能。具体而言，FlashST 采用轻量级的时空提示网络（spatio-temporal prompt network）进行 in-context learning，捕捉时空不变知识（spatio-temporal invariant knowledge），并通过分布映射机制（distribution mapping mechanism）对齐预训练和下游数据的分布，实现有效的知识转移。实证评估显示，FlashST 在多种时空预测任务和城市数据集上表现出色，证明了其有效性。代码已开源在 GitHub。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper has been accepted by ICML 2024 (poster)",
      "pdf_url": "http://arxiv.org/pdf/2405.17898v1",
      "published_date": "2024-05-28 07:18:52 UTC",
      "updated_date": "2024-05-28 07:18:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:14:20.502931"
    },
    {
      "arxiv_id": "2405.17894v2",
      "title": "White-box Multimodal Jailbreaks Against Large Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ruofan Wang",
        "Xingjun Ma",
        "Hanxu Zhou",
        "Chuanjun Ji",
        "Guangnan Ye",
        "Yu-Gang Jiang"
      ],
      "abstract": "Recent advancements in Large Vision-Language Models (VLMs) have underscored\ntheir superiority in various multimodal tasks. However, the adversarial\nrobustness of VLMs has not been fully explored. Existing methods mainly assess\nrobustness through unimodal adversarial attacks that perturb images, while\nassuming inherent resilience against text-based attacks. Different from\nexisting attacks, in this work we propose a more comprehensive strategy that\njointly attacks both text and image modalities to exploit a broader spectrum of\nvulnerability within VLMs. Specifically, we propose a dual optimization\nobjective aimed at guiding the model to generate affirmative responses with\nhigh toxicity. Our attack method begins by optimizing an adversarial image\nprefix from random noise to generate diverse harmful responses in the absence\nof text input, thus imbuing the image with toxic semantics. Subsequently, an\nadversarial text suffix is integrated and co-optimized with the adversarial\nimage prefix to maximize the probability of eliciting affirmative responses to\nvarious harmful instructions. The discovered adversarial image prefix and text\nsuffix are collectively denoted as a Universal Master Key (UMK). When\nintegrated into various malicious queries, UMK can circumvent the alignment\ndefenses of VLMs and lead to the generation of objectionable content, known as\njailbreaks. The experimental results demonstrate that our universal attack\nstrategy can effectively jailbreak MiniGPT-4 with a 96% success rate,\nhighlighting the vulnerability of VLMs and the urgent need for new alignment\nstrategies.",
      "tldr_zh": "本研究探讨了大型视觉语言模型 (VLMs) 的对抗鲁棒性，提出了一种白盒多模态攻击策略，通过同时针对文本和图像模态进行联合攻击，以暴露模型的更广泛漏洞。具体方法采用双重优化目标：先优化一个从随机噪声生成的对抗图像前缀，使其在无文本输入时产生多样有害响应；然后与对抗文本后缀共同优化，形成 Universal Master Key (UMK)，以最大化对有害指令的肯定响应概率。实验结果显示，该策略在 MiniGPT-4 上实现96%的 jailbreak 成功率，突显 VLMs 的脆弱性，并呼吁开发新的对齐防御策略。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17894v2",
      "published_date": "2024-05-28 07:13:30 UTC",
      "updated_date": "2024-10-14 03:15:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:14:34.185936"
    },
    {
      "arxiv_id": "2405.17893v1",
      "title": "Arithmetic Reasoning with LLM: Prolog Generation & Permutation",
      "title_zh": "使用大语言模型的算术推理：Prolog 生成与置换",
      "authors": [
        "Xiaocheng Yang",
        "Bingsen Chen",
        "Yik-Cheung Tam"
      ],
      "abstract": "Instructing large language models (LLMs) to solve elementary school math\nproblems has shown great success using Chain of Thought (CoT). However, the CoT\napproach relies on an LLM to generate a sequence of arithmetic calculations\nwhich can be prone to cascaded calculation errors. We hypothesize that an LLM\nshould focus on extracting predicates and generating symbolic formulas from the\nmath problem description so that the underlying calculation can be done via an\nexternal code interpreter. We investigate using LLM to generate Prolog programs\nto solve mathematical questions. Experimental results show that our\nProlog-based arithmetic problem-solving outperforms CoT generation in the GSM8K\nbenchmark across three distinct LLMs. In addition, given the insensitive\nordering of predicates and symbolic formulas in Prolog, we propose to permute\nthe ground truth predicates for more robust LLM training via data augmentation.",
      "tldr_zh": "本论文提出了一种改进的算术推理方法，使用LLM生成Prolog程序来解决小学数学问题，从而避免Chain of Thought (CoT)方法中常见的级联计算错误。LLM专注于提取谓词并生成符号公式，由外部代码解释器执行实际计算，实验在GSM8K基准上显示这种Prolog-based方法在三个不同LLMs上优于CoT生成。此外，作者引入谓词排列作为数据增强技术，利用Prolog对顺序不敏感的特点，提升了模型训练的鲁棒性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 4 figures, accepted by NAACL 2024 Main Conference",
      "pdf_url": "http://arxiv.org/pdf/2405.17893v1",
      "published_date": "2024-05-28 07:13:25 UTC",
      "updated_date": "2024-05-28 07:13:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:14:45.660875"
    },
    {
      "arxiv_id": "2405.17888v3",
      "title": "Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment",
      "title_zh": "从 SFT 数据中获取更多价值：基于人类演示的奖励学习提升 SFT 以实现 LLM 对齐",
      "authors": [
        "Jiaxiang Li",
        "Siliang Zeng",
        "Hoi-To Wai",
        "Chenliang Li",
        "Alfredo Garcia",
        "Mingyi Hong"
      ],
      "abstract": "Aligning human preference and value is an important requirement for\ncontemporary foundation models. State-of-the-art techniques such as\nReinforcement Learning from Human Feedback (RLHF) often consist of two stages:\n1) supervised fine-tuning (SFT), where the model is fine-tuned by learning from\nhuman demonstration data; 2) Preference learning, where preference data is used\nto learn a reward model, which is in turn used by a reinforcement learning (RL)\nstep to fine-tune the model. Such reward model serves as a proxy to human\npreference, and it is critical to guide the RL step towards improving the model\nquality. In this work, we argue that the SFT stage significantly benefits from\nlearning a reward model as well. Instead of using the human demonstration data\ndirectly via supervised learning, we propose to leverage an Inverse\nReinforcement Learning (IRL) technique to simultaneously build an reward model\nand a policy model. This approach leads to new SFT algorithms that are not only\nefficient to implement, but are robust to the presence of low-quality\nsupervised learning data. Moreover, we discover a connection between the\nproposed IRL based approach, and a recent line of works called Self-Play\nFine-tune (SPIN). Theoretically, we show that the proposed algorithms converge\nto the stationary solutions of the IRL problem. Empirically, we align 1B and 7B\nmodels using proposed methods and evaluate them on a reward benchmark model and\nthe HuggingFace Open LLM Leaderboard. The proposed methods show significant\nperformance improvement over existing SFT approaches. Our results indicate that\nit is beneficial to leverage reward learning throughout the entire alignment\nprocess.",
      "tldr_zh": "本论文探讨了通过奖励学习改进监督微调（SFT）以提升大型语言模型（LLM）的对齐效果。作者提出使用逆强化学习（IRL）技术，从人类演示数据中同时构建奖励模型和策略模型，从而取代传统SFT直接监督学习的方法，使其更高效且对低质量数据更鲁棒。实验结果显示，该方法在1B和7B模型上显著优于现有SFT方法，并在奖励基准模型和HuggingFace Open LLM Leaderboard上取得性能提升。总体而言，该研究证明了在整个对齐过程中整合奖励学习（如与Self-Play Fine-tune (SPIN)相关联）能带来实质性益处。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17888v3",
      "published_date": "2024-05-28 07:11:05 UTC",
      "updated_date": "2024-10-27 20:09:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:14:57.231175"
    },
    {
      "arxiv_id": "2405.20351v3",
      "title": "Imitating from auxiliary imperfect demonstrations via Adversarial Density Weighted Regression",
      "title_zh": "通过对抗密度加权回归从辅助不",
      "authors": [
        "Ziqi Zhang",
        "Zifeng Zhuang",
        "Jingzehua Xu",
        "Yiyuan Yang",
        "Yubo Huang",
        "Donglin Wang",
        "Shuai Zhang"
      ],
      "abstract": "We propose a novel one-step supervised imitation learning (IL) framework\ncalled Adversarial Density Regression (ADR). This IL framework aims to correct\nthe policy learned on unknown-quality to match the expert distribution by\nutilizing demonstrations, without relying on the Bellman operator.\nSpecifically, ADR addresses several limitations in previous IL algorithms:\nFirst, most IL algorithms are based on the Bellman operator, which inevitably\nsuffer from cumulative offsets from sub-optimal rewards during multi-step\nupdate processes. Additionally, off-policy training frameworks suffer from\nOut-of-Distribution (OOD) state-actions. Second, while conservative terms help\nsolve the OOD issue, balancing the conservative term is difficult. To address\nthese limitations, we fully integrate a one-step density-weighted Behavioral\nCloning (BC) objective for IL with auxiliary imperfect demonstration.\nTheoretically, we demonstrate that this adaptation can effectively correct the\ndistribution of policies trained on unknown-quality datasets to align with the\nexpert policy's distribution. Moreover, the difference between the empirical\nand the optimal value function is proportional to the upper bound of ADR's\nobjective, indicating that minimizing ADR's objective is akin to approaching\nthe optimal value. Experimentally, we validated the performance of ADR by\nconducting extensive evaluations. Specifically, ADR outperforms all of the\nselected IL algorithms on tasks from the Gym-Mujoco domain. Meanwhile, it\nachieves an 89.5% improvement over IQL when utilizing ground truth rewards on\ntasks from the Adroit and Kitchen domains. Our codebase will be released at:\nhttps://github.com/stevezhangzA/Adverserial_Density_Regression.",
      "tldr_zh": "本研究提出了一种新颖的一阶监督模仿学习（Imitation Learning, IL）框架，名为 Adversarial Density Regression (ADR)，旨在通过 Adversarial Density Weighted Regression 方法，利用辅助不完美演示来修正策略，使其与专家分布对齐，而不依赖 Bellman operator。ADR 解决了传统 IL 算法的局限性，包括避免多步更新中的累积偏移和 Out-of-Distribution (OOD) 状态动作问题，并通过一阶密度加权 Behavioral Cloning (BC) 目标进行优化。理论上，研究证明最小化 ADR 目标可接近最优价值函数；实验上，在 Gym-Mujoco 领域中，ADR 超越了其他 IL 算法，在 Adroit 和 Kitchen 领域则比 IQL 提升了 89.5%。这为处理未知质量演示的 IL 提供了更可靠的框架。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20351v3",
      "published_date": "2024-05-28 06:59:16 UTC",
      "updated_date": "2025-01-13 12:27:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:15:08.954551"
    },
    {
      "arxiv_id": "2405.17879v2",
      "title": "Resisting Stochastic Risks in Diffusion Planners with the Trajectory Aggregation Tree",
      "title_zh": "翻译失败",
      "authors": [
        "Lang Feng",
        "Pengjie Gu",
        "Bo An",
        "Gang Pan"
      ],
      "abstract": "Diffusion planners have shown promise in handling long-horizon and\nsparse-reward tasks due to the non-autoregressive plan generation. However,\ntheir inherent stochastic risk of generating infeasible trajectories presents\nsignificant challenges to their reliability and stability. We introduce a novel\napproach, the Trajectory Aggregation Tree (TAT), to address this issue in\ndiffusion planners. Compared to prior methods that rely solely on raw\ntrajectory predictions, TAT aggregates information from both historical and\ncurrent trajectories, forming a dynamic tree-like structure. Each trajectory is\nconceptualized as a branch and individual states as nodes. As the structure\nevolves with the integration of new trajectories, unreliable states are\nmarginalized, and the most impactful nodes are prioritized for decision-making.\nTAT can be deployed without modifying the original training and sampling\npipelines of diffusion planners, making it a training-free, ready-to-deploy\nsolution. We provide both theoretical analysis and empirical evidence to\nsupport TAT's effectiveness. Our results highlight its remarkable ability to\nresist the risk from unreliable trajectories, guarantee the performance\nboosting of diffusion planners in $100\\%$ of tasks, and exhibit an appreciable\ntolerance margin for sample quality, thereby enabling planning with a more than\n$3\\times$ acceleration.",
      "tldr_zh": "本文提出Trajectory Aggregation Tree (TAT)，一种新方法，用于解决扩散规划器(diffusion planners)在处理长时限和稀疏奖励任务时面临的随机风险(stochastic risks)，即生成不可行轨迹的问题。TAT通过聚合历史和当前轨迹信息，形成动态树状结构，将每个轨迹视为分支、状态视为节点，并优先处理可靠节点，同时边缘化不可靠状态。该方法无需修改扩散规划器的原始训练和采样管道，即可实现训练-free部署；实验结果显示，TAT在100%任务中提升性能，提供超过3倍的加速，并对样本质量表现出显著容忍度。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2024 (Spotlight)",
      "pdf_url": "http://arxiv.org/pdf/2405.17879v2",
      "published_date": "2024-05-28 06:57:22 UTC",
      "updated_date": "2024-06-07 12:27:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:15:22.196191"
    },
    {
      "arxiv_id": "2405.17878v2",
      "title": "An Information Theoretic Evaluation Metric For Strong Unlearning",
      "title_zh": "翻译失败",
      "authors": [
        "Dongjae Jeon",
        "Wonje Jeung",
        "Taeheon Kim",
        "Albert No",
        "Jonghyun Choi"
      ],
      "abstract": "Machine unlearning (MU) aims to remove the influence of specific data from\ntrained models, addressing privacy concerns and ensuring compliance with\nregulations such as the \"right to be forgotten.\" Evaluating strong unlearning,\nwhere the unlearned model is indistinguishable from one retrained without the\nforgetting data, remains a significant challenge in deep neural networks\n(DNNs). Common black-box metrics, such as variants of membership inference\nattacks and accuracy comparisons, primarily assess model outputs but often fail\nto capture residual information in intermediate layers. To bridge this gap, we\nintroduce the Information Difference Index (IDI), a novel white-box metric\ninspired by information theory. IDI quantifies retained information in\nintermediate features by measuring mutual information between those features\nand the labels to be forgotten, offering a more comprehensive assessment of\nunlearning efficacy. Our experiments demonstrate that IDI effectively measures\nthe degree of unlearning across various datasets and architectures, providing a\nreliable tool for evaluating strong unlearning in DNNs.",
      "tldr_zh": "这篇论文针对机器无学习(Machine Unlearning)中的强无学习评估挑战，提出了一种基于信息理论的新指标Information Difference Index (IDI)。IDI作为白盒指标，通过测量中间特征与遗忘标签之间的互信息(Mutual Information)，来量化模型中残留的信息，从而更全面地评估无学习效果。实验结果显示，IDI在各种数据集和神经网络(DNNs)架构上表现出色，提供了一个可靠的工具来确保模型遵守隐私法规如“right to be forgotten”。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17878v2",
      "published_date": "2024-05-28 06:57:01 UTC",
      "updated_date": "2024-10-19 06:00:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:15:33.126048"
    },
    {
      "arxiv_id": "2405.17874v1",
      "title": "NUTS, NARS, and Speech",
      "title_zh": "翻译失败",
      "authors": [
        "D. van der Sluis"
      ],
      "abstract": "To investigate whether \"Intelligence is the capacity of an\ninformation-processing system to adapt to its environment while operating with\ninsufficient knowledge and resources\", we look at utilising the non axiomatic\nreasoning system (NARS) for speech recognition. This article presents NUTS:\nraNdom dimensionality redUction non axiomaTic reasoning few Shot learner for\nperception. NUTS consists of naive dimensionality reduction, some\npre-processing, and then non axiomatic reasoning (NARS). With only 2 training\nexamples NUTS performs similarly to the Whisper Tiny model for discrete word\nidentification.",
      "tldr_zh": "这篇论文探讨了“智能是信息处理系统在知识和资源不足的情况下适应环境的容量”，通过将非公理推理系统 (NARS) 应用于语音识别任务。研究者开发了 NUTS 系统，包括简单的维度减少、预处理和 NARS 推理，作为一种少样本学习器，用于感知领域。结果显示，使用仅 2 个训练样本，NUTS 在离散单词识别上与 Whisper Tiny 模型性能相似。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.17874v1",
      "published_date": "2024-05-28 06:51:42 UTC",
      "updated_date": "2024-05-28 06:51:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:15:44.728690"
    },
    {
      "arxiv_id": "2405.17873v2",
      "title": "MixDQ: Memory-Efficient Few-Step Text-to-Image Diffusion Models with Metric-Decoupled Mixed Precision Quantization",
      "title_zh": "翻译失败",
      "authors": [
        "Tianchen Zhao",
        "Xuefei Ning",
        "Tongcheng Fang",
        "Enshu Liu",
        "Guyue Huang",
        "Zinan Lin",
        "Shengen Yan",
        "Guohao Dai",
        "Yu Wang"
      ],
      "abstract": "Diffusion models have achieved significant visual generation quality.\nHowever, their significant computational and memory costs pose challenge for\ntheir application on resource-constrained mobile devices or even desktop GPUs.\nRecent few-step diffusion models reduces the inference time by reducing the\ndenoising steps. However, their memory consumptions are still excessive. The\nPost Training Quantization (PTQ) replaces high bit-width FP representation with\nlow-bit integer values (INT4/8) , which is an effective and efficient technique\nto reduce the memory cost. However, when applying to few-step diffusion models,\nexisting quantization methods face challenges in preserving both the image\nquality and text alignment. To address this issue, we propose an\nmixed-precision quantization framework - MixDQ. Firstly, We design specialized\nBOS-aware quantization method for highly sensitive text embedding quantization.\nThen, we conduct metric-decoupled sensitivity analysis to measure the\nsensitivity of each layer. Finally, we develop an integer-programming-based\nmethod to conduct bit-width allocation. While existing quantization methods\nfall short at W8A8, MixDQ could achieve W8A8 without performance loss, and W4A8\nwith negligible visual degradation. Compared with FP16, we achieve 3-4x\nreduction in model size and memory cost, and 1.45x latency speedup.",
      "tldr_zh": "该研究针对扩散模型的高内存和计算成本问题，提出了一种记忆高效的少步文本到图像扩散模型框架 MixDQ，利用混合精度量化技术。MixDQ 包括 BOS-aware 量化方法处理敏感文本嵌入、metric-decoupled 敏感性分析评估图层敏感性，以及基于整数编程的位宽分配策略，以平衡图像质量和文本对齐。实验结果显示，与 FP16 相比，MixDQ 在 W8A8 配置下无性能损失，在 W4A8 下仅有微小视觉退化，同时实现 3-4 倍模型大小和内存减少，以及 1.45 倍延迟加速。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://a-suozhang.xyz/mixdq.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2405.17873v2",
      "published_date": "2024-05-28 06:50:58 UTC",
      "updated_date": "2024-05-30 01:51:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:15:58.697129"
    },
    {
      "arxiv_id": "2405.17871v2",
      "title": "Seeing the Image: Prioritizing Visual Correlation by Contrastive Alignment",
      "title_zh": "图像感知：通过对比对齐优先视觉相关性",
      "authors": [
        "Xin Xiao",
        "Bohong Wu",
        "Jiacong Wang",
        "Chunyuan Li",
        "Xun Zhou",
        "Haoyuan Guo"
      ],
      "abstract": "Existing image-text modality alignment in Vision Language Models (VLMs)\ntreats each text token equally in an autoregressive manner. Despite being\nsimple and effective, this method results in sub-optimal cross-modal alignment\nby over-emphasizing the text tokens that are less correlated with or even\ncontradictory with the input images. In this paper, we advocate for assigning\ndistinct contributions for each text token based on its visual correlation.\nSpecifically, we present by contrasting image inputs, the difference in\nprediction logits on each text token provides strong guidance of visual\ncorrelation. We therefore introduce Contrastive ALignment (CAL), a simple yet\neffective re-weighting strategy that prioritizes training visually correlated\ntokens. Our experimental results demonstrate that CAL consistently improves\ndifferent types of VLMs across different resolutions and model sizes on various\nbenchmark datasets. Importantly, our method incurs minimal additional\ncomputational overhead, rendering it highly efficient compared to alternative\ndata scaling strategies. Codes are available at\nhttps://github.com/foundation-multimodal-models/CAL.",
      "tldr_zh": "这篇论文针对Vision Language Models (VLMs)中图像-文本模态对齐的问题，指出现有方法过度强调与图像不相关或矛盾的文本标记，导致对齐次优。作者提出Contrastive ALignment (CAL)，一种简单的再加权策略，通过对比图像输入的预测logits差异来优先训练视觉相关的文本标记。该方法在不同分辨率、模型大小和基准数据集上 consistently 提升VLMs性能，且额外计算开销最小，比其他数据扩展策略更高效。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "NeurlPS 2024, Camera ready",
      "pdf_url": "http://arxiv.org/pdf/2405.17871v2",
      "published_date": "2024-05-28 06:44:13 UTC",
      "updated_date": "2024-11-05 02:26:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:16:10.670714"
    },
    {
      "arxiv_id": "2405.17849v2",
      "title": "I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models",
      "title_zh": "I",
      "authors": [
        "Xing Hu",
        "Yuan Cheng",
        "Dawei Yang",
        "Zhihang Yuan",
        "Jiangyong Yu",
        "Chen Xu",
        "Sifan Zhou"
      ],
      "abstract": "Post-training quantization (PTQ) serves as a potent technique to accelerate\nthe inference of large language models (LLMs). Nonetheless, existing works\nstill necessitate a considerable number of floating-point (FP) operations\nduring inference, including additional quantization and de-quantization, as\nwell as non-linear operators such as RMSNorm and Softmax. This limitation\nhinders the deployment of LLMs on the edge and cloud devices. In this paper, we\nidentify the primary obstacle to integer-only quantization for LLMs lies in the\nlarge fluctuation of activations across channels and tokens in both linear and\nnon-linear operations. To address this issue, we propose I-LLM, a novel\ninteger-only fully-quantized PTQ framework tailored for LLMs. Specifically, (1)\nwe develop Fully-Smooth Block-Reconstruction (FSBR) to aggressively smooth\ninter-channel variations of all activations and weights. (2) to alleviate\ndegradation caused by inter-token variations, we introduce a novel approach\ncalled Dynamic Integer-only MatMul (DI-MatMul). This method enables dynamic\nquantization in full-integer matrix multiplication by dynamically quantizing\nthe input and outputs with integer-only operations. (3) we design\nDI-ClippedSoftmax, DI-Exp, and DI-Normalization, which utilize bit shift to\nexecute non-linear operators efficiently while maintaining accuracy. The\nexperiment shows that our I-LLM achieves comparable accuracy to the FP baseline\nand outperforms non-integer quantization methods. For example, I-LLM can\noperate at W4A4 with negligible loss of accuracy. To our knowledge, we are the\nfirst to bridge the gap between integer-only quantization and LLMs. We've\npublished our code on anonymous.4open.science, aiming to contribute to the\nadvancement of this field.",
      "tldr_zh": "这篇论文提出 I-LLM，一种全整数量化后训练（PTQ）框架，用于低比特大型语言模型（LLMs），旨在消除推理过程中的浮点运算，从而提升在边缘和云设备的部署效率。关键方法包括 Fully-Smooth Block-Reconstruction (FSBR) 来平滑激活和权重的通道间变化、Dynamic Integer-only MatMul (DI-MatMul) 用于动态量化矩阵乘法，以及 DI-ClippedSoftmax、DI-Exp 和 DI-Normalization 等整数-only 非线性运算，以处理激活波动问题。实验结果显示，I-LLM 在 W4A4 量化下与浮点基准性能相当，甚至优于其他非整数量化方法，这是首个桥接整数-only 量化和 LLMs 的框架。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17849v2",
      "published_date": "2024-05-28 05:56:11 UTC",
      "updated_date": "2024-06-05 15:26:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:16:22.945883"
    },
    {
      "arxiv_id": "2405.17846v1",
      "title": "Safety Control of Service Robots with LLMs and Embodied Knowledge Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Yong Qi",
        "Gabriel Kyebambo",
        "Siyuan Xie",
        "Wei Shen",
        "Shenghui Wang",
        "Bitao Xie",
        "Bin He",
        "Zhipeng Wang",
        "Shuo Jiang"
      ],
      "abstract": "Safety limitations in service robotics across various industries have raised\nsignificant concerns about the need for robust mechanisms ensuring that robots\nadhere to safe practices, thereby preventing actions that might harm humans or\ncause property damage. Despite advances, including the integration of Knowledge\nGraphs (KGs) with Large Language Models (LLMs), challenges in ensuring\nconsistent safety in autonomous robot actions persist. In this paper, we\npropose a novel integration of Large Language Models with Embodied Robotic\nControl Prompts (ERCPs) and Embodied Knowledge Graphs (EKGs) to enhance the\nsafety framework for service robots. ERCPs are designed as predefined\ninstructions that ensure LLMs generate safe and precise responses. These\nresponses are subsequently validated by EKGs, which provide a comprehensive\nknowledge base ensuring that the actions of the robot are continuously aligned\nwith safety protocols, thereby promoting safer operational practices in varied\ncontexts. Our experimental setup involved diverse real-world tasks, where\nrobots equipped with our framework demonstrated significantly higher compliance\nwith safety standards compared to traditional methods. This integration fosters\nsecure human-robot interactions and positions our methodology at the forefront\nof AI-driven safety innovations in service robotics.",
      "tldr_zh": "该研究针对服务机器人的安全问题，提出了一种将 Large Language Models (LLMs) 与 Embodied Robotic Control Prompts (ERCPs) 和 Embodied Knowledge Graphs (EKGs) 整合的新框架，以确保机器人遵守安全协议并避免伤害人类或财产。ERCPs 作为预定义指令，帮助 LLMs 生成安全且精确的响应，而 EKGs 则负责验证这些响应，确保操作始终符合安全标准。实验结果显示，在多样化的真实任务中，该框架使机器人比传统方法更高效地遵守安全规范，从而提升了人机互动的安全性和可靠性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17846v1",
      "published_date": "2024-05-28 05:50:25 UTC",
      "updated_date": "2024-05-28 05:50:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:16:35.817927"
    },
    {
      "arxiv_id": "2405.17839v1",
      "title": "PeerFL: A Simulator for Peer-to-Peer Federated Learning at Scale",
      "title_zh": "PeerFL：用于大规模点对点联邦",
      "authors": [
        "Alka Luqman",
        "Shivanshu Shekhar",
        "Anupam Chattopadhyay"
      ],
      "abstract": "This work integrates peer-to-peer federated learning tools with NS3, a widely\nused network simulator, to create a novel simulator designed to allow\nheterogeneous device experiments in federated learning. This cross-platform\nadaptability addresses a critical gap in existing simulation tools, enhancing\nthe overall utility and user experience. NS3 is leveraged to simulate WiFi\ndynamics to facilitate federated learning experiments with participants that\nmove around physically during training, leading to dynamic network\ncharacteristics. Our experiments showcase the simulator's efficiency in\ncomputational resource utilization at scale, with a maximum of 450\nheterogeneous devices modelled as participants in federated learning. This\npositions it as a valuable tool for simulation-based investigations in\npeer-to-peer federated learning. The framework is open source and available for\nuse and extension to the community.",
      "tldr_zh": "这篇论文介绍了 PeerFL，一种新型模拟器，将点对点联邦学习(Peer-to-Peer Federated Learning)工具与 NS3 网络模拟器整合，用于在异构设备上进行大规模联邦学习实验。该模拟器支持跨平台适应，并利用 NS3 模拟 WiFi 动态，允许参与者在训练过程中移动，从而处理动态网络特性。实验结果显示，PeerFL 在计算资源利用上高效，可模拟最多 450 个异构设备，填补了现有工具的空白。该框架开源，供社区使用和扩展。",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17839v1",
      "published_date": "2024-05-28 05:30:18 UTC",
      "updated_date": "2024-05-28 05:30:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:16:48.807517"
    },
    {
      "arxiv_id": "2405.17838v1",
      "title": "Trust and Terror: Hazards in Text Reveal Negatively Biased Credulity and Partisan Negativity Bias",
      "title_zh": "信任",
      "authors": [
        "Keith Burghardt",
        "Daniel M. T. Fessler",
        "Chyna Tang",
        "Anne Pisor",
        "Kristina Lerman"
      ],
      "abstract": "Socio-linguistic indicators of text, such as emotion or sentiment, are often\nextracted using neural networks in order to better understand features of\nsocial media. One indicator that is often overlooked, however, is the presence\nof hazards within text. Recent psychological research suggests that statements\nabout hazards are more believable than statements about benefits (a property\nknown as negatively biased credulity), and that political liberals and\nconservatives differ in how often they share hazards. Here, we develop a new\nmodel to detect information concerning hazards, trained on a new collection of\nannotated X posts, as well as urban legends annotated in previous work. We show\nthat not only does this model perform well (outperforming, e.g., zero-shot\nhuman annotator proxies, such as GPT-4) but that the hazard information it\nextracts is not strongly correlated with other indicators, namely moral\noutrage, sentiment, emotions, and threat words. (That said, consonant with\nexpectations, hazard information does correlate positively with such emotions\nas fear, and negatively with emotions like joy.) We then apply this model to\nthree datasets: X posts about COVID-19, X posts about the 2023 Hamas-Israel\nwar, and a new expanded collection of urban legends. From these data, we\nuncover words associated with hazards unique to each dataset as well as\ndifferences in this language between groups of users, such as conservatives and\nliberals, which informs what these groups perceive as hazards. We further show\nthat information about hazards peaks in frequency after major hazard events,\nand therefore acts as an automated indicator of such events. Finally, we find\nthat information about hazards is especially prevalent in urban legends, which\nis consistent with previous work that finds that reports of hazards are more\nlikely to be both believed and transmitted.",
      "tldr_zh": "本文开发了一个新模型，用于检测文本中的 hazards（危害），以揭示 negatively biased credulity（负面偏见的可信度）和 partisan negativity bias（党派负面偏见），并将其应用于 X posts 和都市传说数据集。模型训练于标注数据，表现出色，且与情感、道德愤怒等指标相关性较低，但与恐惧情绪正相关。研究发现，不同用户群体如保守派和自由派在 hazards 语言上存在差异，且 hazards 信息在重大事件后峰值出现，在都市传说中尤为普遍，支持其传播性更强。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 16 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.17838v1",
      "published_date": "2024-05-28 05:28:49 UTC",
      "updated_date": "2024-05-28 05:28:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:17:00.401972"
    },
    {
      "arxiv_id": "2406.05142v1",
      "title": "Machine Learning-Driven Optimization of TPMS Architected Materials Using Simulated Annealing",
      "title_zh": "翻译失败",
      "authors": [
        "Akshansh Mishra"
      ],
      "abstract": "The research paper presents a novel approach to optimizing the tensile stress\nof Triply Periodic Minimal Surface (TPMS) structures through machine learning\nand Simulated Annealing (SA). The study evaluates the performance of Random\nForest, Decision Tree, and XGBoost models in predicting tensile stress, using a\ndataset generated from finite element analysis of TPMS models. The objective\nfunction minimized the negative R-squared value on the validation set to\nenhance model accuracy. The SA-XGBoost model outperformed the others, achieving\nan R-squared value of 0.96. In contrast, the SA-Random Forest model achieved an\nR squared value of 0.89 while the SA-Decision Tree model exhibited greater\nfluctuations in validation scores. This demonstrates that the SA-XGBoost model\nis most effective in capturing the complex relationships within the data. The\nintegration of SA helps in optimizing the hyperparameters of these machine\nlearning models, thereby enhancing their predictive capabilities.",
      "tldr_zh": "本文提出了一种利用机器学习和 Simulated Annealing (SA) 优化 Triply Periodic Minimal Surface (TPMS) 结构拉伸应力的新方法，通过评估 Random Forest、Decision Tree 和 XGBoost 模型在有限元分析生成数据集上的预测性能。目标函数最小化验证集的负 R-squared 值，以提升模型准确性，结果显示 SA-XGBoost 模型表现最佳，R-squared 值达到 0.96，而 SA-Random Forest 和 SA-Decision Tree 模型分别达到 0.89 和显示更大波动。该方法证明了 SA 在优化机器学习模型超参数方面的有效性，从而提高了对复杂数据关系的预测能力。",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI",
        "cs.LG",
        "math.OC"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "25 Pages, 7 figures and 8 Tables",
      "pdf_url": "http://arxiv.org/pdf/2406.05142v1",
      "published_date": "2024-05-28 05:06:37 UTC",
      "updated_date": "2024-05-28 05:06:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:17:14.178603"
    },
    {
      "arxiv_id": "2405.17832v1",
      "title": "Mollification Effects of Policy Gradient Methods",
      "title_zh": "策略梯度方法的平滑化效应",
      "authors": [
        "Tao Wang",
        "Sylvia Herbert",
        "Sicun Gao"
      ],
      "abstract": "Policy gradient methods have enabled deep reinforcement learning (RL) to\napproach challenging continuous control problems, even when the underlying\nsystems involve highly nonlinear dynamics that generate complex non-smooth\noptimization landscapes. We develop a rigorous framework for understanding how\npolicy gradient methods mollify non-smooth optimization landscapes to enable\neffective policy search, as well as the downside of it: while making the\nobjective function smoother and easier to optimize, the stochastic objective\ndeviates further from the original problem. We demonstrate the equivalence\nbetween policy gradient methods and solving backward heat equations. Following\nthe ill-posedness of backward heat equations from PDE theory, we present a\nfundamental challenge to the use of policy gradient under stochasticity.\nMoreover, we make the connection between this limitation and the uncertainty\nprinciple in harmonic analysis to understand the effects of exploration with\nstochastic policies in RL. We also provide experimental results to illustrate\nboth the positive and negative aspects of mollification effects in practice.",
      "tldr_zh": "该研究探讨了政策梯度方法在深度强化学习中的软化（mollification）效果，这些方法能处理涉及高度非线性动态的连续控制问题，通过平滑非平滑优化景观来提升策略搜索效率，但也导致随机目标函数偏离原问题。作者建立了理论框架，将政策梯度方法等价于求解反向热方程（backward heat equations），并从偏微分方程（PDE）理论中揭示其不适定性（ill-posedness），这对强化学习（RL）中的随机策略使用提出了根本挑战。论文进一步将这一限制与调和分析的不确定性原理（uncertainty principle）联系起来，解释了探索的潜在负面影响，并通过实验结果展示了软化效果的积极（如优化更容易）和消极（如偏差增加）方面。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages, 41 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.17832v1",
      "published_date": "2024-05-28 05:05:33 UTC",
      "updated_date": "2024-05-28 05:05:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:17:25.743649"
    },
    {
      "arxiv_id": "2405.17829v2",
      "title": "LDMol: Text-to-Molecule Diffusion Model with Structurally Informative Latent Space",
      "title_zh": "LDMol：具有结构信息丰富潜在空间的文本到分子扩散模型",
      "authors": [
        "Jinho Chang",
        "Jong Chul Ye"
      ],
      "abstract": "With the emergence of diffusion models as the frontline of generative models,\nmany researchers have proposed molecule generation techniques with conditional\ndiffusion models. However, the unavoidable discreteness of a molecule makes it\ndifficult for a diffusion model to connect raw data with highly complex\nconditions like natural language. To address this, we present a novel latent\ndiffusion model dubbed LDMol for text-conditioned molecule generation. LDMol\ncomprises a molecule autoencoder that produces a learnable and structurally\ninformative feature space, and a natural language-conditioned latent diffusion\nmodel. In particular, recognizing that multiple SMILES notations can represent\nthe same molecule, we employ a contrastive learning strategy to extract feature\nspace that is aware of the unique characteristics of the molecule structure.\nLDMol outperforms the existing baselines on the text-to-molecule generation\nbenchmark, suggesting a potential for diffusion models can outperform\nautoregressive models in text data generation with a better choice of the\nlatent domain. Furthermore, we show that LDMol can be applied to downstream\ntasks such as molecule-to-text retrieval and text-guided molecule editing,\ndemonstrating its versatility as a diffusion model.",
      "tldr_zh": "本研究提出 LDMol，一种基于结构信息潜在空间（structurally informative latent space）的文本到分子扩散模型（text-to-molecule diffusion model），旨在解决分子离散性和复杂条件（如自然语言）生成挑战。LDMol 包括一个分子自编码器（molecule autoencoder），通过对比学习（contrastive learning）策略处理多个 SMILES 表示，确保特征空间捕捉分子结构的独特特性。实验结果显示，LDMol 在文本到分子生成基准上优于现有基线，证明扩散模型在文本数据生成中可能超越自回归模型；此外，该模型还适用于下游任务，如分子到文本检索（molecule-to-text retrieval）和文本引导分子编辑（text-guided molecule editing）。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17829v2",
      "published_date": "2024-05-28 04:59:13 UTC",
      "updated_date": "2024-10-03 15:14:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:17:37.912868"
    },
    {
      "arxiv_id": "2405.17825v3",
      "title": "Diffusion Model Patching via Mixture-of-Prompts",
      "title_zh": "翻译失败",
      "authors": [
        "Seokil Ham",
        "Sangmin Woo",
        "Jin-Young Kim",
        "Hyojun Go",
        "Byeongjun Park",
        "Changick Kim"
      ],
      "abstract": "We present Diffusion Model Patching (DMP), a simple method to boost the\nperformance of pre-trained diffusion models that have already reached\nconvergence, with a negligible increase in parameters. DMP inserts a small,\nlearnable set of prompts into the model's input space while keeping the\noriginal model frozen. The effectiveness of DMP is not merely due to the\naddition of parameters but stems from its dynamic gating mechanism, which\nselects and combines a subset of learnable prompts at every timestep (i.e.,\nreverse denoising steps). This strategy, which we term \"mixture-of-prompts\",\nenables the model to draw on the distinct expertise of each prompt, essentially\n\"patching\" the model's functionality at every timestep with minimal yet\nspecialized parameters. Uniquely, DMP enhances the model by further training on\nthe original dataset already used for pre-training, even in a scenario where\nsignificant improvements are typically not expected due to model convergence.\nNotably, DMP significantly enhances the FID of converged DiT-L/2 by 10.38% on\nFFHQ, achieved with only a 1.43% parameter increase and 50K additional training\niterations.",
      "tldr_zh": "本研究提出了一种名为 Diffusion Model Patching (DMP) 的简单方法，用于提升已收敛的扩散模型性能，同时仅增加微量参数。DMP 通过插入一个小型可学习提示集到模型输入空间，并采用 mixture-of-prompts 动态门控机制，在每个时间步动态选择和组合提示子集，从而利用每个提示的专业性来修补模型的功能。实验结果显示，在原数据集上进一步训练后，DMP 显著提升了 DiT-L/2 模型在 FFHQ 上的 FID 指标10.38%，仅需增加1.43%的参数和50K迭代训练。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "AAAI 2025; Project: https://sangminwoo.github.io/DMP/",
      "pdf_url": "http://arxiv.org/pdf/2405.17825v3",
      "published_date": "2024-05-28 04:47:54 UTC",
      "updated_date": "2024-12-11 13:58:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:17:50.284723"
    },
    {
      "arxiv_id": "2405.17822v1",
      "title": "Conv-CoA: Improving Open-domain Question Answering in Large Language Models via Conversational Chain-of-Action",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenyu Pan",
        "Haozheng Luo",
        "Manling Li",
        "Han Liu"
      ],
      "abstract": "We present a Conversational Chain-of-Action (Conv-CoA) framework for\nOpen-domain Conversational Question Answering (OCQA). Compared with literature,\nConv-CoA addresses three major challenges: (i) unfaithful hallucination that is\ninconsistent with real-time or domain facts, (ii) weak reasoning performance in\nconversational scenarios, and (iii) unsatisfying performance in conversational\ninformation retrieval. Our key contribution is a dynamic reasoning-retrieval\nmechanism that extracts the intent of the question and decomposes it into a\nreasoning chain to be solved via systematic prompting, pre-designed actions,\nupdating the Contextual Knowledge Set (CKS), and a novel Hopfield-based\nretriever. Methodologically, we propose a resource-efficiency Hopfield\nretriever to enhance the efficiency and accuracy of conversational information\nretrieval within our actions. Additionally, we propose a\nconversational-multi-reference faith score (Conv-MRFS) to verify and resolve\nconflicts between retrieved knowledge and answers in conversations.\nEmpirically, we conduct comparisons between our framework and 23\nstate-of-the-art methods across five different research directions and two\npublic benchmarks. These comparisons demonstrate that our Conv-CoA outperforms\nother methods in both the accuracy and efficiency dimensions.",
      "tldr_zh": "本研究提出了 Conversational Chain-of-Action (Conv-CoA) 框架，用于提升大语言模型在开放域对话式问答 (Open-domain Question Answering) 中的性能，针对幻觉问题、弱推理能力和信息检索不足等三大挑战。该框架引入动态推理-检索机制，通过提取问题意图、分解为推理链、系统提示、预设计动作、更新 Contextual Knowledge Set (CKS) 以及一个基于 Hopfield 的检索器来优化对话过程；同时，提出 conversational-multi-reference faith score (Conv-MRFS) 来验证和解决知识与答案间的冲突。实验结果显示，Conv-CoA 在五个研究方向和两个公共基准上，与23个最先进方法相比，在准确性和效率方面均表现出色。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17822v1",
      "published_date": "2024-05-28 04:46:52 UTC",
      "updated_date": "2024-05-28 04:46:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:18:01.674026"
    },
    {
      "arxiv_id": "2405.17821v2",
      "title": "RITUAL: Random Image Transformations as a Universal Anti-hallucination Lever in Large Vision Language Models",
      "title_zh": "RITUAL：随机图像变换作为大型视觉语言模型中的通用抗幻觉杠杆",
      "authors": [
        "Sangmin Woo",
        "Jaehyuk Jang",
        "Donguk Kim",
        "Yubin Choi",
        "Changick Kim"
      ],
      "abstract": "Recent advancements in Large Vision Language Models (LVLMs) have\nrevolutionized how machines understand and generate textual responses based on\nvisual inputs, yet they often produce \"hallucinatory\" outputs that misinterpret\nvisual information, posing challenges in reliability and trustworthiness. We\npropose RITUAL, a simple decoding method that reduces hallucinations by\nleveraging randomly transformed images as complementary inputs during decoding,\nadjusting the output probability distribution without additional training or\nexternal models. Our key insight is that random transformations expose the\nmodel to diverse visual perspectives, enabling it to correct misinterpretations\nthat lead to hallucinations. Specifically, when a model hallucinates based on\nthe original image, the transformed images -- altered in aspects such as\norientation, scale, or color -- provide alternative viewpoints that help\nrecalibrate the model's predictions. By integrating the probability\ndistributions from both the original and transformed images, RITUAL effectively\nreduces hallucinations. To further improve reliability and address potential\ninstability from arbitrary transformations, we introduce RITUAL+, an extension\nthat selects image transformations based on self-feedback from the LVLM.\nInstead of applying transformations randomly, RITUAL+ uses the LVLM to evaluate\nand choose transformations that are most beneficial for reducing hallucinations\nin a given context. This self-adaptive approach mitigates the potential\nnegative impact of certain transformations on specific tasks, ensuring more\nconsistent performance across different scenarios. Experiments demonstrate that\nRITUAL and RITUAL+ significantly reduce hallucinations across several object\nhallucination benchmarks.",
      "tldr_zh": "该论文提出 RITUAL，一种简单解码方法，用于减少 Large Vision Language Models (LVLMs) 中的 hallucinations，通过在解码过程中使用随机图像变换作为补充输入，整合原始和变换图像的概率分布，从而纠正模型对视觉信息的误解，而无需额外训练或外部模型。关键洞见是这些随机变换（如旋转、缩放或颜色调整）提供多样视角，帮助模型重新校准预测。RITUAL+ 作为扩展版本，利用 LVLM 的自反馈机制选择最有益的变换，缓解特定任务中不稳定问题，确保更一致的性能。实验结果表明，RITUAL 和 RITUAL+ 在多个对象 hallucinations 基准上显著降低了幻觉发生率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project: https://sangminwoo.github.io/RITUAL/",
      "pdf_url": "http://arxiv.org/pdf/2405.17821v2",
      "published_date": "2024-05-28 04:41:02 UTC",
      "updated_date": "2024-12-16 10:27:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:18:14.901094"
    },
    {
      "arxiv_id": "2405.17820v1",
      "title": "Don't Miss the Forest for the Trees: Attentional Vision Calibration for Large Vision Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Sangmin Woo",
        "Donguk Kim",
        "Jaehyuk Jang",
        "Yubin Choi",
        "Changick Kim"
      ],
      "abstract": "This study addresses the issue observed in Large Vision Language Models\n(LVLMs), where excessive attention on a few image tokens, referred to as blind\ntokens, leads to hallucinatory responses in tasks requiring fine-grained\nunderstanding of visual objects. We found that tokens receiving lower attention\nweights often hold essential information for identifying nuanced object details\n-- ranging from merely recognizing object existence to identifying their\nattributes (color, position, etc.) and understanding their relationships. To\ncounteract the over-emphasis on blind tokens and to accurately respond to user\nqueries, we introduce a technique called Attentional Vision Calibration (AVC).\nDuring the decoding phase, AVC identifies blind tokens by analyzing the\nimage-related attention distribution. It then dynamically adjusts the logits\nfor the next token prediction by contrasting the logits conditioned on the\noriginal visual tokens with those conditioned on the blind tokens. This\neffectively lowers the dependency on blind tokens and promotes a more balanced\nconsideration of all tokens. We validate AVC on benchmarks such as POPE, MME,\nand AMBER, where it consistently outperforms existing decoding techniques in\nmitigating object hallucinations in LVLMs.",
      "tldr_zh": "本研究发现，Large Vision Language Models (LVLMs) 由于过度关注少数图像tokens（称为blind tokens），导致在需要细粒度视觉理解的任务中出现幻觉问题，而低注意力权重的tokens往往包含识别对象细节（如存在、属性和关系）的关键信息。为此，提出Attentional Vision Calibration (AVC) 技术，在解码阶段通过分析图像相关注意力分布识别blind tokens，并动态调整下一个token的logits，以对比原始和blind tokens的条件，降低对blind tokens的依赖并实现tokens的平衡考虑。在POPE、MME和AMBER等基准测试中，AVC在减少LVLMs的对象幻觉方面显著优于现有解码技术。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://sangminwoo.github.io/AvisC/",
      "pdf_url": "http://arxiv.org/pdf/2405.17820v1",
      "published_date": "2024-05-28 04:40:57 UTC",
      "updated_date": "2024-05-28 04:40:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:18:25.235052"
    },
    {
      "arxiv_id": "2405.17814v6",
      "title": "FAIntbench: A Holistic and Precise Benchmark for Bias Evaluation in Text-to-Image Models",
      "title_zh": "翻译失败",
      "authors": [
        "Hanjun Luo",
        "Ziye Deng",
        "Ruizhe Chen",
        "Zuozhu Liu"
      ],
      "abstract": "The rapid development and reduced barriers to entry for Text-to-Image (T2I)\nmodels have raised concerns about the biases in their outputs, but existing\nresearch lacks a holistic definition and evaluation framework of biases,\nlimiting the enhancement of debiasing techniques. To address this issue, we\nintroduce FAIntbench, a holistic and precise benchmark for biases in T2I\nmodels. In contrast to existing benchmarks that evaluate bias in limited\naspects, FAIntbench evaluate biases from four dimensions: manifestation of\nbias, visibility of bias, acquired attributes, and protected attributes. We\napplied FAIntbench to evaluate seven recent large-scale T2I models and\nconducted human evaluation, whose results demonstrated the effectiveness of\nFAIntbench in identifying various biases. Our study also revealed new research\nquestions about biases, including the side-effect of distillation. The findings\npresented here are preliminary, highlighting the potential of FAIntbench to\nadvance future research aimed at mitigating the biases in T2I models. Our\nbenchmark is publicly available to ensure the reproducibility.",
      "tldr_zh": "该研究引入了 FAIntbench，这是一个全面且精确的基准，用于评估 Text-to-Image (T2I) 模型中的偏差问题，以弥补现有评估框架的局限性。FAIntbench 从四个维度——偏差的表现、偏差的可见性、获得的属性和受保护的属性——对偏差进行全方位评估，并应用于七个大型 T2I 模型的实验中。结果显示，该基准在识别各种偏差方面表现出色，并揭示了新的研究问题，如 distillation 的副作用。总体而言，这为未来减轻 T2I 模型偏差的技术研究提供了初步基础，且基准已公开以确保可重复性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICML DMLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.17814v6",
      "published_date": "2024-05-28 04:18:00 UTC",
      "updated_date": "2025-02-24 08:49:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:18:38.570974"
    },
    {
      "arxiv_id": "2405.17809v3",
      "title": "TransVIP: Speech to Speech Translation System with Voice and Isochrony Preservation",
      "title_zh": "翻译失败",
      "authors": [
        "Chenyang Le",
        "Yao Qian",
        "Dongmei Wang",
        "Long Zhou",
        "Shujie Liu",
        "Xiaofei Wang",
        "Midia Yousefi",
        "Yanmin Qian",
        "Jinyu Li",
        "Sheng Zhao",
        "Michael Zeng"
      ],
      "abstract": "There is a rising interest and trend in research towards directly translating\nspeech from one language to another, known as end-to-end speech-to-speech\ntranslation. However, most end-to-end models struggle to outperform cascade\nmodels, i.e., a pipeline framework by concatenating speech recognition, machine\ntranslation and text-to-speech models. The primary challenges stem from the\ninherent complexities involved in direct translation tasks and the scarcity of\ndata. In this study, we introduce a novel model framework TransVIP that\nleverages diverse datasets in a cascade fashion yet facilitates end-to-end\ninference through joint probability. Furthermore, we propose two separated\nencoders to preserve the speaker's voice characteristics and isochrony from the\nsource speech during the translation process, making it highly suitable for\nscenarios such as video dubbing. Our experiments on the French-English language\npair demonstrate that our model outperforms the current state-of-the-art\nspeech-to-speech translation model.",
      "tldr_zh": "这篇论文介绍了 TransVIP 系统，一种端到-end speech-to-speech translation 框架，旨在解决现有模型在直接语音翻译中面临的复杂性和数据稀缺问题。TransVIP 通过级联方式利用多样数据集，同时实现端到-end 推理，并引入两个分离的编码器来保留源语音的 voice characteristics 和 isochrony，从而适用于视频配音等场景。在法语-英语语言对的实验中，该系统超越了当前最先进模型，展示了显著的性能提升。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "Neural Information Processing Systems, poster",
      "pdf_url": "http://arxiv.org/pdf/2405.17809v3",
      "published_date": "2024-05-28 04:11:37 UTC",
      "updated_date": "2024-10-31 03:11:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:18:50.048188"
    },
    {
      "arxiv_id": "2405.17802v1",
      "title": "Multi-level Interaction Modeling for Protein Mutational Effect Prediction",
      "title_zh": "多层次相互作用建模用于蛋白质突变效应预测",
      "authors": [
        "Yuanle Mo",
        "Xin Hong",
        "Bowen Gao",
        "Yinjun Jia",
        "Yanyan Lan"
      ],
      "abstract": "Protein-protein interactions are central mediators in many biological\nprocesses. Accurately predicting the effects of mutations on interactions is\ncrucial for guiding the modulation of these interactions, thereby playing a\nsignificant role in therapeutic development and drug discovery. Mutations\ngenerally affect interactions hierarchically across three levels: mutated\nresidues exhibit different sidechain conformations, which lead to changes in\nthe backbone conformation, eventually affecting the binding affinity between\nproteins. However, existing methods typically focus only on sidechain-level\ninteraction modeling, resulting in suboptimal predictions. In this work, we\npropose a self-supervised multi-level pre-training framework, ProMIM, to fully\ncapture all three levels of interactions with well-designed pretraining\nobjectives. Experiments show ProMIM outperforms all the baselines on the\nstandard benchmark, especially on mutations where significant changes in\nbackbone conformations may occur. In addition, leading results from zero-shot\nevaluations for SARS-CoV-2 mutational effect prediction and antibody\noptimization underscore the potential of ProMIM as a powerful next-generation\ntool for developing novel therapeutic approaches and new drugs.",
      "tldr_zh": "该研究针对蛋白质突变对蛋白质间相互作用的影响提出了一种自监督多层次预训练框架ProMIM，以全面捕捉突变在侧链构象、骨架构象和蛋白质结合亲和力三个层次的层次化影响。相比现有仅关注侧链水平的模型，ProMIM 通过精心设计的预训练目标提升了预测准确性。实验结果显示，该框架在标准基准上优于所有基线，尤其在骨架构象发生显著变化的突变上，并在 SARS-CoV-2 突变效应预测和抗体优化等零样本评估中表现出色，展示了其作为新一代工具在治疗开发和药物发现中的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17802v1",
      "published_date": "2024-05-28 03:53:26 UTC",
      "updated_date": "2024-05-28 03:53:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:19:02.936769"
    },
    {
      "arxiv_id": "2405.17784v2",
      "title": "Adaptive Horizon Actor-Critic for Policy Learning in Contact-Rich Differentiable Simulation",
      "title_zh": "翻译失败",
      "authors": [
        "Ignat Georgiev",
        "Krishnan Srinivasan",
        "Jie Xu",
        "Eric Heiden",
        "Animesh Garg"
      ],
      "abstract": "Model-Free Reinforcement Learning (MFRL), leveraging the policy gradient\ntheorem, has demonstrated considerable success in continuous control tasks.\nHowever, these approaches are plagued by high gradient variance due to\nzeroth-order gradient estimation, resulting in suboptimal policies. Conversely,\nFirst-Order Model-Based Reinforcement Learning (FO-MBRL) methods employing\ndifferentiable simulation provide gradients with reduced variance but are\nsusceptible to sampling error in scenarios involving stiff dynamics, such as\nphysical contact. This paper investigates the source of this error and\nintroduces Adaptive Horizon Actor-Critic (AHAC), an FO-MBRL algorithm that\nreduces gradient error by adapting the model-based horizon to avoid stiff\ndynamics. Empirical findings reveal that AHAC outperforms MFRL baselines,\nattaining 40% more reward across a set of locomotion tasks and efficiently\nscaling to high-dimensional control environments with improved wall-clock-time\nefficiency.",
      "tldr_zh": "本研究探讨了Model-Free Reinforcement Learning (MFRL)算法在连续控制任务中的高梯度方差问题，以及First-Order Model-Based Reinforcement Learning (FO-MBRL)算法在涉及物理接触的刚性动态场景中面临的采样误差。论文提出Adaptive Horizon Actor-Critic (AHAC)，一种FO-MBRL方法，通过动态调整模型-based视野来减少梯度误差，从而更好地处理接触丰富的可微模拟环境。实验结果显示，AHAC在运动任务中比MFRL基线提升40%的奖励，并在高维控制环境中表现出更高的时效效率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Website https://adaptive-horizon-actor-critic.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2405.17784v2",
      "published_date": "2024-05-28 03:28:00 UTC",
      "updated_date": "2024-06-03 20:23:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:19:14.232699"
    },
    {
      "arxiv_id": "2405.17766v1",
      "title": "SleepFM: Multi-modal Representation Learning for Sleep Across Brain Activity, ECG and Respiratory Signals",
      "title_zh": "翻译失败",
      "authors": [
        "Rahul Thapa",
        "Bryan He",
        "Magnus Ruud Kjaer",
        "Hyatt Moore",
        "Gauri Ganjoo",
        "Emmanuel Mignot",
        "James Zou"
      ],
      "abstract": "Sleep is a complex physiological process evaluated through various modalities\nrecording electrical brain, cardiac, and respiratory activities. We curate a\nlarge polysomnography dataset from over 14,000 participants comprising over\n100,000 hours of multi-modal sleep recordings. Leveraging this extensive\ndataset, we developed SleepFM, the first multi-modal foundation model for sleep\nanalysis. We show that a novel leave-one-out approach for contrastive learning\nsignificantly improves downstream task performance compared to representations\nfrom standard pairwise contrastive learning. A logistic regression model\ntrained on SleepFM's learned embeddings outperforms an end-to-end trained\nconvolutional neural network (CNN) on sleep stage classification (macro AUROC\n0.88 vs 0.72 and macro AUPRC 0.72 vs 0.48) and sleep disordered breathing\ndetection (AUROC 0.85 vs 0.69 and AUPRC 0.77 vs 0.61). Notably, the learned\nembeddings achieve 48% top-1 average accuracy in retrieving the corresponding\nrecording clips of other modalities from 90,000 candidates. This work\ndemonstrates the value of holistic multi-modal sleep modeling to fully capture\nthe richness of sleep recordings. SleepFM is open source and available at\nhttps://github.com/rthapa84/sleepfm-codebase.",
      "tldr_zh": "本研究整理了一个大型多导睡眠图数据集，从超过14,000名参与者中获得超过100,000小时的多模态睡眠记录，包括脑电、心电和呼吸信号，并开发了SleepFM，这是首个多模态基础模型，用于睡眠分析。SleepFM 采用新型的leave-one-out对比学习方法，与标准配对对比学习相比，显著提高了下游任务性能。基于SleepFM的嵌入训练的逻辑回归模型，在睡眠阶段分类（宏AUROC 0.88 vs 0.72）和睡眠呼吸障碍检测（AUROC 0.85 vs 0.69）上优于端到端的CNN模型，同时在多模态检索任务中实现了48%的Top-1准确率。该工作突出了整体多模态建模的价值，并开源SleepFM以供进一步应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17766v1",
      "published_date": "2024-05-28 02:43:53 UTC",
      "updated_date": "2024-05-28 02:43:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:19:28.629418"
    },
    {
      "arxiv_id": "2405.17764v3",
      "title": "On the Sequence Evaluation based on Stochastic Processes",
      "title_zh": "翻译失败",
      "authors": [
        "Tianhao Zhang",
        "Zhexiao Lin",
        "Zhecheng Sheng",
        "Chen Jiang",
        "Dongyeop Kang"
      ],
      "abstract": "Generative models have gained significant prominence in Natural Language\nProcessing (NLP), especially in tackling the complex task of modeling and\nevaluating long text sequences. This task is crucial for advancing various\ndownstream applications, such as text generation and machine translation.\nRecent methods that utilize stochastic processes to capture the intrinsic\ndynamics of sequences have shown superior performance in generative modeling.\nHowever, the accurate encoding of both temporal and structural dependencies\nfrom text datasets, as well as leveraging this encoded information for sequence\nevaluation, remains an open area of research. In this paper, we propose a novel\napproach to learn the stochastic dynamics of long text sequences, utilizing a\nnegative log-likelihood-based encoder that outperforms contrastive learning\nmethods. We also introduce a likelihood-based evaluation metric for long-text\nassessment, which measures sequence coherence and can be applied to downstream\ntasks such as Human-AI discrimination. Our encoder preserves sequence coherence\neffectively and performs robustly on out-of-domain datasets. Additionally, the\nproposed evaluation metric captures both temporal and structural information\ncomprehensively. Theoretical analysis demonstrates the superiority of our\nmetric in sequence evaluation, and experimental results highlight its\nflexibility and exceptional performance across a variety of tasks, showcasing\nits utility in diverse NLP applications.",
      "tldr_zh": "这篇论文探讨了基于随机过程(stochastic processes)对长文本序列的建模和评估方法，以提升自然语言处理(NLP)中的文本生成和机器翻译等下游应用。作者提出了一种新型负对数似然编码器(negative log-likelihood-based encoder)，用于捕捉序列的时序和结构依赖性，该方法优于对比学习(contrastive learning)方法，并在域外数据集上表现出色。论文还引入了一个基于似然的评估指标(likelihood-based evaluation metric)，能全面评估序列连贯性(sequence coherence)，实验结果显示其在多种NLP任务中具有卓越性能和灵活性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "math.ST",
        "stat.TH"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17764v3",
      "published_date": "2024-05-28 02:33:38 UTC",
      "updated_date": "2024-10-03 03:03:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:19:38.600876"
    },
    {
      "arxiv_id": "2405.17755v1",
      "title": "XL3M: A Training-free Framework for LLM Length Extension Based on Segment-wise Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Shengnan Wang",
        "Youhui Bai",
        "Lin Zhang",
        "Pingyi Zhou",
        "Shixiong Zhao",
        "Gong Zhang",
        "Sen Wang",
        "Renhai Chen",
        "Hua Xu",
        "Hongwei Sun"
      ],
      "abstract": "Length generalization failure problem, namely the large language model (LLM)\nfails to generalize to texts longer than its maximum training length, greatly\nrestricts the application of LLM in the scenarios with streaming long inputs.\nTo address this problem, the existing methods either require substantial costs\nor introduce precision loss. In this paper, we empirically find that the\naccuracy of the LLM's prediction is highly correlated to its certainty. Based\non this, we propose an efficient training free framework, named XL3M (it means\nextra-long large language model), which enables the LLMs trained on short\nsequences to reason extremely long sequence without any further training or\nfine-tuning. Under the XL3M framework, the input context will be firstly\ndecomposed into multiple short sub-contexts, where each sub-context contains an\nindependent segment and a common ``question'' which is a few tokens from the\nend of the original context. Then XL3M gives a method to measure the relevance\nbetween each segment and the ``question'', and constructs a concise key context\nby splicing all the relevant segments in chronological order. The key context\nis further used instead of the original context to complete the inference task.\nEvaluations on comprehensive benchmarks show the superiority of XL3M. Using our\nframework, a Llama2-7B model is able to reason 20M long sequences on an 8-card\nHuawei Ascend 910B NPU machine with 64GB memory per card.",
      "tldr_zh": "这篇论文针对大型语言模型(LLM)的长度泛化失败问题，提出了一种无需额外训练的框架XL3M，用于基于Segment-wise Inference扩展LLM处理长序列的能力。XL3M首先将输入上下文分解成多个短子上下文，每个包含一个独立段和一个共同的“question”（来自原上下文末尾的几个标记），然后通过测量每个段与“question”的相关性，构建一个按时间顺序拼接的关键上下文(key context)来替代原上下文进行推理。实验结果显示，该框架在综合基准测试中表现出色，例如Llama2-7B模型能够在一台8卡Huawei Ascend 910B NPU机器上处理20M长序列，同时避免了现有方法的成本和精度损失。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.17755v1",
      "published_date": "2024-05-28 02:12:35 UTC",
      "updated_date": "2024-05-28 02:12:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:19:51.245899"
    },
    {
      "arxiv_id": "2405.17750v1",
      "title": "Magnitude-based Neuron Pruning for Backdoor Defens",
      "title_zh": "翻译失败",
      "authors": [
        "Nan Li",
        "Haoyu Jiang",
        "Ping Yi"
      ],
      "abstract": "Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks,\nposing concerning threats to their reliable deployment. Recent research reveals\nthat backdoors can be erased from infected DNNs by pruning a specific group of\nneurons, while how to effectively identify and remove these backdoor-associated\nneurons remains an open challenge. In this paper, we investigate the\ncorrelation between backdoor behavior and neuron magnitude, and find that\nbackdoor neurons deviate from the magnitude-saliency correlation of the model.\nThe deviation inspires us to propose a Magnitude-based Neuron Pruning (MNP)\nmethod to detect and prune backdoor neurons. Specifically, MNP uses three\nmagnitude-guided objective functions to manipulate the magnitude-saliency\ncorrelation of backdoor neurons, thus achieving the purpose of exposing\nbackdoor behavior, eliminating backdoor neurons and preserving clean neurons,\nrespectively. Experiments show our pruning strategy achieves state-of-the-art\nbackdoor defense performance against a variety of backdoor attacks with a\nlimited amount of clean data, demonstrating the crucial role of magnitude for\nguiding backdoor defenses.",
      "tldr_zh": "这篇论文针对深度神经网络（DNNs）易受后门攻击的问题，提出了一种基于神经元大小（magnitude）的修剪方法（Magnitude-based Neuron Pruning, MNP）。MNP 通过分析后门神经元与模型的 magnitude-saliency 相关性的偏差，使用三个 magnitude-guided 目标函数来检测并消除后门神经元，同时保留干净神经元。实验结果表明，该方法在多种后门攻击场景下，使用少量干净数据，实现了最先进的防御性能，突出了 magnitude 在后门防御中的关键作用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17750v1",
      "published_date": "2024-05-28 02:05:39 UTC",
      "updated_date": "2024-05-28 02:05:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:20:13.246126"
    },
    {
      "arxiv_id": "2405.17746v1",
      "title": "Rethinking Pruning for Backdoor Mitigation: An Optimization Perspective",
      "title_zh": "重新审视后门缓解的剪枝：一个优化视角",
      "authors": [
        "Nan Li",
        "Haiyang Yu",
        "Ping Yi"
      ],
      "abstract": "Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks,\nposing concerning threats to their reliable deployment. Recent research reveals\nthat backdoors can be erased from infected DNNs by pruning a specific group of\nneurons, while how to effectively identify and remove these backdoor-associated\nneurons remains an open challenge. Most of the existing defense methods rely on\ndefined rules and focus on neuron's local properties, ignoring the exploration\nand optimization of pruning policies. To address this gap, we propose an\nOptimized Neuron Pruning (ONP) method combined with Graph Neural Network (GNN)\nand Reinforcement Learning (RL) to repair backdoor models. Specifically, ONP\nfirst models the target DNN as graphs based on neuron connectivity, and then\nuses GNN-based RL agents to learn graph embeddings and find a suitable pruning\npolicy. To the best of our knowledge, this is the first attempt to employ GNN\nand RL for optimizing pruning policies in the field of backdoor defense.\nExperiments show, with a small amount of clean data, ONP can effectively prune\nthe backdoor neurons implanted by a set of backdoor attacks at the cost of\nnegligible performance degradation, achieving a new state-of-the-art\nperformance for backdoor mitigation.",
      "tldr_zh": "本研究从优化视角重新审视后门攻击对Deep Neural Networks (DNNs)的威胁，提出Optimized Neuron Pruning (ONP)方法，通过Graph Neural Network (GNN)和Reinforcement Learning (RL)优化神经元修剪策略，以有效识别和移除后门相关神经元。ONP首先将DNN建模为基于神经元连接的图，然后利用GNN-based RL代理学习图嵌入并制定合适的修剪策略，这是首次在后门防御领域应用此类技术。实验表明，使用少量干净数据，ONP能成功缓解多种后门攻击，同时仅导致微不足道的性能损失，达到了后门缓解的最先进性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17746v1",
      "published_date": "2024-05-28 01:59:06 UTC",
      "updated_date": "2024-05-28 01:59:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:20:17.558896"
    },
    {
      "arxiv_id": "2405.17743v5",
      "title": "ORLM: A Customizable Framework in Training Large Models for Automated Optimization Modeling",
      "title_zh": "翻译失败",
      "authors": [
        "Chenyu Huang",
        "Zhengyang Tang",
        "Shixi Hu",
        "Ruoqing Jiang",
        "Xin Zheng",
        "Dongdong Ge",
        "Benyou Wang",
        "Zizhuo Wang"
      ],
      "abstract": "Optimization modeling plays a critical role in the application of Operations\nResearch (OR) tools to address real-world problems, yet they pose challenges\nand require extensive expertise from OR experts. With the advent of large\nlanguage models (LLMs), new opportunities have emerged to streamline and\nautomate such task. However, current research predominantly relies on\nclosed-source LLMs such as GPT-4, along with extensive prompt engineering\ntechniques. This reliance stems from the scarcity of high-quality training\ndatasets for optimization modeling, resulting in elevated costs, prolonged\nprocessing times, and privacy concerns. To address these challenges, our work\nis the first to propose a viable path for training open-source LLMs that are\ncapable of optimization modeling and developing solver codes, eventually\nleading to a superior ability for automating optimization modeling and solving.\nParticularly, we design the {\\sc OR-Instruct}, a semi-automated data synthesis\nframework for optimization modeling that enables customizable enhancements for\nspecific scenarios or model types. This work also introduces IndustryOR, the\nfirst industrial benchmark for evaluating LLMs in solving practical OR\nproblems. We train several 7B-scale open-source LLMs using synthesized data\n(dubbed ORLMs{https://github.com/Cardinal-Operations/ORLM}), which exhibit\nsignificantly enhanced optimization modeling capabilities, achieving\ncompetitive performance across the NL4OPT, MAMO, and IndustryOR benchmarks.\nAdditionally, our experiments highlight the potential of scaling law and\nreinforcement learning to further enhance the performance of ORLMs. The\nworkflows and human-machine interaction paradigms of ORLMs in practical\nindustrial applications are also discussed in the paper.",
      "tldr_zh": "这篇论文提出 ORLM 框架，用于训练开源大型语言模型（LLMs）以自动化优化建模（Optimization Modeling），解决当前依赖闭源模型如 GPT-4 导致的成本高、隐私问题和数据稀缺挑战。论文设计了 OR-Instruct，一个半自动数据合成框架，支持自定义增强以适应特定场景，并引入 IndustryOR 作为首个工业基准评估 LLMs 在实际优化问题上的表现。通过使用合成的训练数据，论文训练了 7B-scale 的开源 ORLMs，在 NL4OPT、MAMO 和 IndustryOR 基准上实现了显著的性能提升。最终，论文讨论了 scaling law 和 reinforcement learning 的潜力，以进一步优化 ORLMs 在工业应用中的工作流和人机交互。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "accepted by Operations Research",
      "pdf_url": "http://arxiv.org/pdf/2405.17743v5",
      "published_date": "2024-05-28 01:55:35 UTC",
      "updated_date": "2025-04-04 13:31:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:20:29.525590"
    },
    {
      "arxiv_id": "2405.17741v1",
      "title": "LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design",
      "title_zh": "LoRA-Switch：通过系统-算法协同设计",
      "authors": [
        "Rui Kong",
        "Qiyang Li",
        "Xinyu Fang",
        "Qingtian Feng",
        "Qingfeng He",
        "Yazhu Dong",
        "Weijun Wang",
        "Yuanchun Li",
        "Linghe Kong",
        "Yunxin Liu"
      ],
      "abstract": "Recent literature has found that an effective method to customize or further\nimprove large language models (LLMs) is to add dynamic adapters, such as\nlow-rank adapters (LoRA) with Mixture-of-Experts (MoE) structures. Though such\ndynamic adapters incur modest computational complexity, they surprisingly lead\nto huge inference latency overhead, slowing down the decoding speed by 2.5+\ntimes. In this paper, we analyze the fine-grained costs of the dynamic adapters\nand find that the fragmented CUDA kernel calls are the root cause. Therefore,\nwe propose LoRA-Switch, a system-algorithm co-designed architecture for\nefficient dynamic adapters. Unlike most existing dynamic structures that adopt\nlayer-wise or block-wise dynamic routing, LoRA-Switch introduces a token-wise\nrouting mechanism. It switches the LoRA adapters and weights for each token and\nmerges them into the backbone for inference. For efficiency, this switching is\nimplemented with an optimized CUDA kernel, which fuses the merging operations\nfor all LoRA adapters at once. Based on experiments with popular open-source\nLLMs on common benchmarks, our approach has demonstrated similar accuracy\nimprovement as existing dynamic adapters, while reducing the decoding latency\nby more than 2.4 times.",
      "tldr_zh": "本文研究发现，现有的动态适配器如 LoRA with Mixture-of-Experts (MoE) 虽然能提升大型语言模型 (LLMs) 的性能，但会导致推理延迟增加 2.5 倍以上，主要原因是碎片化的 CUDA 内核调用。针对此问题，作者提出 LoRA-Switch，一种系统-算法协同设计的架构，采用 token-wise 路由机制来为每个 token 切换和合并 LoRA 适配器，并通过优化 CUDA 内核实现一次融合操作。该方法在常见基准测试中，实现了与现有动态适配器相似的准确性提升，同时将解码延迟减少 2.4 倍以上。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17741v1",
      "published_date": "2024-05-28 01:53:26 UTC",
      "updated_date": "2024-05-28 01:53:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:20:43.901886"
    },
    {
      "arxiv_id": "2405.17739v1",
      "title": "The Widening Gap: The Benefits and Harms of Generative AI for Novice Programmers",
      "title_zh": "不断扩大的鸿沟：生成式 AI 对初学者程序员的益处和危害",
      "authors": [
        "James Prather",
        "Brent Reeves",
        "Juho Leinonen",
        "Stephen MacNeil",
        "Arisoa S. Randrianasolo",
        "Brett Becker",
        "Bailey Kimmel",
        "Jared Wright",
        "Ben Briggs"
      ],
      "abstract": "Novice programmers often struggle through programming problem solving due to\na lack of metacognitive awareness and strategies. Previous research has shown\nthat novices can encounter multiple metacognitive difficulties while\nprogramming. Novices are typically unaware of how these difficulties are\nhindering their progress. Meanwhile, many novices are now programming with\ngenerative AI (GenAI), which can provide complete solutions to most\nintroductory programming problems, code suggestions, hints for next steps when\nstuck, and explain cryptic error messages. Its impact on novice metacognition\nhas only started to be explored. Here we replicate a previous study that\nexamined novice programming problem solving behavior and extend it by\nincorporating GenAI tools. Through 21 lab sessions consisting of participant\nobservation, interview, and eye tracking, we explore how novices are coding\nwith GenAI tools. Although 20 of 21 students completed the assigned programming\nproblem, our findings show an unfortunate divide in the use of GenAI tools\nbetween students who accelerated and students who struggled. Students who\naccelerated were able to use GenAI to create code they already intended to make\nand were able to ignore unhelpful or incorrect inline code suggestions. But for\nstudents who struggled, our findings indicate that previously known\nmetacognitive difficulties persist, and that GenAI unfortunately can compound\nthem and even introduce new metacognitive difficulties. Furthermore, struggling\nstudents often expressed cognitive dissonance about their problem solving\nability, thought they performed better than they did, and finished with an\nillusion of competence. Based on our observations from both groups, we propose\nways to scaffold the novice GenAI experience and make suggestions for future\nwork.",
      "tldr_zh": "该研究探讨了生成式 AI (GenAI) 对新手程序员的影响，强调其益处和潜在危害。新手程序员常因元认知困难 (metacognitive difficulties) 而在编程中挣扎，本研究通过复制先前实验并整合 GenAI 工具，采用 21 个实验室会议（包括参与观察、访谈和眼动追踪）来分析其使用行为。结果显示，GenAI 导致新手之间出现分化：成功组能有效利用 AI 生成代码并忽略无效建议，而挣扎组的元认知问题不仅持续存在，还被加剧，并引发认知失调 (cognitive dissonance) 和能力幻觉 (illusion of competence)。基于这些观察，研究提出支架化新手 GenAI 体验的策略，并为未来工作提供建议。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to ICER 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.17739v1",
      "published_date": "2024-05-28 01:48:28 UTC",
      "updated_date": "2024-05-28 01:48:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:20:55.611243"
    },
    {
      "arxiv_id": "2405.17730v1",
      "title": "MMPareto: Boosting Multimodal Learning with Innocent Unimodal Assistance",
      "title_zh": "MMPareto：通过无害单模态辅助提升多模态",
      "authors": [
        "Yake Wei",
        "Di Hu"
      ],
      "abstract": "Multimodal learning methods with targeted unimodal learning objectives have\nexhibited their superior efficacy in alleviating the imbalanced multimodal\nlearning problem. However, in this paper, we identify the previously ignored\ngradient conflict between multimodal and unimodal learning objectives,\npotentially misleading the unimodal encoder optimization. To well diminish\nthese conflicts, we observe the discrepancy between multimodal loss and\nunimodal loss, where both gradient magnitude and covariance of the\neasier-to-learn multimodal loss are smaller than the unimodal one. With this\nproperty, we analyze Pareto integration under our multimodal scenario and\npropose MMPareto algorithm, which could ensure a final gradient with direction\nthat is common to all learning objectives and enhanced magnitude to improve\ngeneralization, providing innocent unimodal assistance. Finally, experiments\nacross multiple types of modalities and frameworks with dense cross-modal\ninteraction indicate our superior and extendable method performance. Our method\nis also expected to facilitate multi-task cases with a clear discrepancy in\ntask difficulty, demonstrating its ideal scalability. The source code and\ndataset are available at https://github.com/GeWu-Lab/MMPareto_ICML2024.",
      "tldr_zh": "本论文识别了多模态学习（Multimodal Learning）中，针对性单模态学习目标（Unimodal Learning Objectives）与多模态目标之间的梯度冲突问题，这可能导致单模态编码器优化偏差。作者观察到多模态损失与单模态损失在梯度幅度和协方差上的差异，并基于 Pareto 集成分析提出 MMPareto 算法，确保最终梯度方向与所有学习目标一致，同时增强梯度幅度以改善泛化，提供“无害”的单模态辅助。实验结果显示，该方法在多种模态类型和框架下表现出色，并扩展适用于任务难度差异明显的多任务场景。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICML2024",
      "pdf_url": "http://arxiv.org/pdf/2405.17730v1",
      "published_date": "2024-05-28 01:19:13 UTC",
      "updated_date": "2024-05-28 01:19:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:21:06.200773"
    },
    {
      "arxiv_id": "2405.17728v2",
      "title": "Facilitating Holistic Evaluations with LLMs: Insights from Scenario-Based Experiments",
      "title_zh": "通过 LLMs 促进整体评估：基于场景实验的见解",
      "authors": [
        "Toru Ishida",
        "Tongxi Liu",
        "Hailong Wang",
        "William K. Cheunga"
      ],
      "abstract": "Workshop courses designed to foster creativity are gaining popularity.\nHowever, even experienced faculty teams find it challenging to realize a\nholistic evaluation that accommodates diverse perspectives. Adequate\ndeliberation is essential to integrate varied assessments, but faculty often\nlack the time for such exchanges. Deriving an average score without discussion\nundermines the purpose of a holistic evaluation. Therefore, this paper explores\nthe use of a Large Language Model (LLM) as a facilitator to integrate diverse\nfaculty assessments. Scenario-based experiments were conducted to determine if\nthe LLM could integrate diverse evaluations and explain the underlying\npedagogical theories to faculty. The results were noteworthy, showing that the\nLLM can effectively facilitate faculty discussions. Additionally, the LLM\ndemonstrated the capability to create evaluation criteria by generalizing a\nsingle scenario-based experiment, leveraging its already acquired pedagogical\ndomain knowledge.",
      "tldr_zh": "这篇论文探讨了使用大型语言模型 (LLM) 作为 facilitator 来整合教师在创意工作坊中的多样评估，旨在解决整体评估中视角多样性与时间限制的挑战。研究通过基于场景的实验，测试 LLM 是否能有效融合评估结果并解释相关的教育理论。结果表明，LLM 不仅能促进教师讨论，还能利用其现有教育领域知识泛化创建评估标准，从而提升评估过程的效率和全面性。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "The final version appears in the proceedings of the 32nd\n  International Conference on Computers in Education (ICCE 2024)",
      "pdf_url": "http://arxiv.org/pdf/2405.17728v2",
      "published_date": "2024-05-28 01:07:06 UTC",
      "updated_date": "2024-08-12 00:54:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:21:17.472942"
    },
    {
      "arxiv_id": "2405.17724v2",
      "title": "ClavaDDPM: Multi-relational Data Synthesis with Cluster-guided Diffusion Models",
      "title_zh": "ClavaDDPM：基于集群引导扩散模型的多关系数据合成",
      "authors": [
        "Wei Pang",
        "Masoumeh Shafieinejad",
        "Lucy Liu",
        "Stephanie Hazlewood",
        "Xi He"
      ],
      "abstract": "Recent research in tabular data synthesis has focused on single tables,\nwhereas real-world applications often involve complex data with tens or\nhundreds of interconnected tables. Previous approaches to synthesizing\nmulti-relational (multi-table) data fall short in two key aspects: scalability\nfor larger datasets and capturing long-range dependencies, such as correlations\nbetween attributes spread across different tables. Inspired by the success of\ndiffusion models in tabular data modeling, we introduce\n  $\\textbf{C}luster$ $\\textbf{La}tent$ $\\textbf{Va}riable$ $guided$\n$\\textbf{D}enoising$ $\\textbf{D}iffusion$ $\\textbf{P}robabilistic$\n$\\textbf{M}odels$ (ClavaDDPM). This novel approach leverages clustering labels\nas intermediaries to model relationships between tables, specifically focusing\non foreign key constraints. ClavaDDPM leverages the robust generation\ncapabilities of diffusion models while incorporating efficient algorithms to\npropagate the learned latent variables across tables. This enables ClavaDDPM to\ncapture long-range dependencies effectively.\n  Extensive evaluations on multi-table datasets of varying sizes show that\nClavaDDPM significantly outperforms existing methods for these long-range\ndependencies while remaining competitive on utility metrics for single-table\ndata.",
      "tldr_zh": "该论文提出 ClavaDDPM，一种基于聚类引导的扩散模型，用于多关系数据合成，以解决现有方法在处理大型多表数据集时的可扩展性和长距离依赖问题。ClavaDDPM 通过聚类标签作为中介建模表间关系（如外键约束），并利用高效算法传播潜变量，从而有效捕捉跨表属性相关性。在多表数据集的广泛评估中，ClavaDDPM 在长距离依赖捕捉方面显著优于现有方法，同时在单表数据实用性指标上保持竞争力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17724v2",
      "published_date": "2024-05-28 00:42:18 UTC",
      "updated_date": "2024-11-14 11:06:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:21:30.688273"
    },
    {
      "arxiv_id": "2405.17720v2",
      "title": "MindFormer: Semantic Alignment of Multi-Subject fMRI for Brain Decoding",
      "title_zh": "MindFormer：多受试者 fMRI 的语义对齐用于脑解码",
      "authors": [
        "Inhwa Han",
        "Jaayeon Lee",
        "Jong Chul Ye"
      ],
      "abstract": "Research efforts for visual decoding from fMRI signals have attracted\nconsiderable attention in research community. Still multi-subject fMRI decoding\nwith one model has been considered intractable due to the drastic variations in\nfMRI signals between subjects and even within the same subject across different\ntrials. To address current limitations in multi-subject brain decoding, here we\nintroduce a novel semantic alignment method of multi-subject fMRI signals using\nso-called MindFormer. This model is specifically designed to generate\nfMRI-conditioned feature vectors that can be used for conditioning Stable\nDiffusion model for fMRI- to-image generation or large language model (LLM) for\nfMRI-to-text generation. More specifically, MindFormer incorporates two key\ninnovations: 1) a subject specific token that effectively capture individual\ndifferences in fMRI signals while synergistically combines multi subject fMRI\ndata for training, and 2) a novel feature embedding and training scheme based\non the IP-Adapter to extract semantically meaningful features from fMRI\nsignals. Our experimental results demonstrate that MindFormer generates\nsemantically consistent images and text across different subjects. Since our\nMindFormer maintains semantic fidelity by fully utilizing the training data\nacross different subjects by significantly surpassing existing models in\nmulti-subject brain decoding, this may help deepening our understanding of\nneural processing variations among individuals.",
      "tldr_zh": "本研究针对多主题 fMRI 信号的巨大变异性，提出了一种名为 MindFormer 的语义对齐方法，以实现多主题脑解码。MindFormer 通过引入 subject specific token 来捕捉个体差异，同时结合多主题 fMRI 数据进行训练，并采用基于 IP-Adapter 的新特征嵌入方案，从 fMRI 信号中提取语义有意义的特征向量，用于条件 Stable Diffusion 模型的 fMRI-to-image 生成或 LLM 的 fMRI-to-text 生成。实验结果显示，MindFormer 在多主题脑解码中显著超越现有模型，生成跨主题语义一致的图像和文本，从而有助于加深对个体神经处理变异的理解。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.17720v2",
      "published_date": "2024-05-28 00:36:25 UTC",
      "updated_date": "2024-10-06 13:27:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:21:42.097821"
    },
    {
      "arxiv_id": "2405.17713v1",
      "title": "AI Alignment with Changing and Influenceable Reward Functions",
      "title_zh": "AI 对齐：变化的和可影响的奖励函数",
      "authors": [
        "Micah Carroll",
        "Davis Foote",
        "Anand Siththaranjan",
        "Stuart Russell",
        "Anca Dragan"
      ],
      "abstract": "Existing AI alignment approaches assume that preferences are static, which is\nunrealistic: our preferences change, and may even be influenced by our\ninteractions with AI systems themselves. To clarify the consequences of\nincorrectly assuming static preferences, we introduce Dynamic Reward Markov\nDecision Processes (DR-MDPs), which explicitly model preference changes and the\nAI's influence on them. We show that despite its convenience, the\nstatic-preference assumption may undermine the soundness of existing alignment\ntechniques, leading them to implicitly reward AI systems for influencing user\npreferences in ways users may not truly want. We then explore potential\nsolutions. First, we offer a unifying perspective on how an agent's\noptimization horizon may partially help reduce undesirable AI influence. Then,\nwe formalize different notions of AI alignment that account for preference\nchange from the outset. Comparing the strengths and limitations of 8 such\nnotions of alignment, we find that they all either err towards causing\nundesirable AI influence, or are overly risk-averse, suggesting that a\nstraightforward solution to the problems of changing preferences may not exist.\nAs there is no avoiding grappling with changing preferences in real-world\nsettings, this makes it all the more important to handle these issues with\ncare, balancing risks and capabilities. We hope our work can provide conceptual\nclarity and constitute a first step towards AI alignment practices which\nexplicitly account for (and contend with) the changing and influenceable nature\nof human preferences.",
      "tldr_zh": "这篇论文批评了现有 AI alignment 方法假设偏好静态的局限性，指出人类偏好会变化且可能受 AI 系统影响，从而可能导致对齐技术奖励不期望的行为。作者引入 Dynamic Reward Markov Decision Processes (DR-MDPs) 来明确建模偏好变化和 AI 的影响，揭示静态假设可能削弱对齐的可靠性。论文探讨了潜在解决方案，包括优化视野的作用，并比较了 8 种考虑偏好变化的对齐概念，这些概念要么易导致不良影响，要么过于风险厌恶。最终，论文强调在实际 AI 应用中需谨慎处理偏好动态问题，以平衡风险和能力，提供概念清晰度作为未来对齐实践的基础。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.17713v1",
      "published_date": "2024-05-28 00:08:46 UTC",
      "updated_date": "2024-05-28 00:08:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:21:55.858929"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 167,
  "processed_papers_count": 167,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-18T13:22:32.387006"
}
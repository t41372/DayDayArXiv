{
  "date": "2025-08-11",
  "category": "cs.AI",
  "summary": "ä½ å¥½ï¼æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-08-11 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä¸€å¥è¯æ€»ç»“ï¼š**\nä»Šå¤©çš„åˆ—è¡¨å ªç§°â€œç¥ä»™æ‰“æ¶â€ï¼Œæœ€ä»¤äººéœ‡æƒŠçš„æ˜¯å‡ºç°äº†å¯¹ **GPT-5** åœ¨å¤šæ¨¡æ€åŒ»ç–—æ¨ç†ä¸Šçš„è¯¦å°½è¯„æµ‹ï¼ˆå£°ç§°è¶…è¶Šäººç±»ä¸“å®¶ï¼‰ï¼ŒåŒæ—¶ï¼Œå…³äº **Long Context/Reasoning æ¨¡å‹**ï¼ˆç±»ä¼¼ o1ï¼‰çš„è®­ç»ƒç»†èŠ‚ï¼ˆKlear-Reasonerï¼‰ã€**LLM æ¨ç†åŠ é€Ÿ**ï¼ˆOverFillï¼‰ä»¥åŠ **AI for Science**ï¼ˆæ ¸ç‰©ç†ä¸åŒ»å­¦ï¼‰éƒ½æœ‰æå…·åˆ†é‡çš„å·¥ä½œå‘å¸ƒã€‚\n\n---\n\n### ğŸš€ é‡ç£…ï¼šGPT-5 ç°èº«ä¸å¤šæ¨¡æ€æµ‹è¯„\nä»Šå¤©æœ€å¸ç›çš„æ— ç–‘æ˜¯å…³äº GPT-5 çš„ç ”ç©¶ã€‚è™½ç„¶å®˜æ–¹å°šæœªå…¨é¢å…¬æµ‹ï¼Œä½†å·²æœ‰ç ”ç©¶å±•ç¤ºäº†å…¶èƒ½åŠ›è¾¹ç•Œã€‚\n\n**1. Capabilities of GPT-5 on Multimodal Medical Reasoning**\n> GPT-5 åœ¨å¤šæ¨¡æ€åŒ»å­¦æ¨ç†ä¸Šçš„èƒ½åŠ›\n> authors: Shansong Wang, et al.\n\n**æ ¸å¿ƒå‘ç°**ï¼šè¿™ç¯‡æ–‡ç« ç›´æ¥å¯¹ **GPT-5, GPT-5-mini, GPT-5-nano** è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶å‘ç°ï¼ŒGPT-5 åœ¨æ ‡å‡†åŒ–çš„åŒ»å­¦é—®ç­”ï¼ˆMedQA, USMLEï¼‰å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šå…¨é¢è¶…è¶Šäº† GPT-4oï¼Œå¹¶åœ¨ MedXpertQA ä¸Šæ¯” GPT-4o æå‡äº†è¿‘ 30% çš„æ¨ç†å¾—åˆ†ã€‚\n**Implication**ï¼šæœ€é‡è¦çš„æ˜¯ï¼ŒGPT-5 åœ¨å¤šæ¨¡æ€æ¨ç†ä¸Š**è¶…è¶Šäº†æœªè·å¾—æ‰§ç…§çš„äººç±»åŒ»å­¦ä¸“å®¶**ã€‚è¿™æ ‡å¿—ç€é€šç”¨æ¨¡å‹åœ¨å‚ç›´é¢†åŸŸçš„ Zero-shot èƒ½åŠ›è¾¾åˆ°äº†æ–°çš„ä¸´ç•Œç‚¹ã€‚\n\n**2. MME-Emotion: A Holistic Evaluation Benchmark for Emotional Intelligence in Multimodal Large Language Models**\n> MME-Emotionï¼šå¤šæ¨¡æ€å¤§æ¨¡å‹æƒ…å•†çš„æ•´ä½“è¯„ä¼°åŸºå‡†\n> authors: Fan Zhang, et al.\n\n**æ ¸å¿ƒå‘ç°**ï¼šé™¤äº†æ™ºå•†ï¼ŒAI çš„â€œæƒ…å•†â€ä¹Ÿå¤‡å—å…³æ³¨ã€‚ä½œè€…æ¨å‡ºäº†æœ€å¤§çš„ MME-Emotion åŸºå‡†ï¼ŒåŒ…å« 6000+ è§†é¢‘ç‰‡æ®µã€‚ç»“è®ºå¾ˆæ‰å¿ƒï¼šç›®å‰çš„ MLLMï¼ˆåŒ…æ‹¬ Gemini-1.5-Pro ç­‰ï¼‰æƒ…å•†æ™®éä¸é«˜ï¼Œæœ€é«˜è¯†åˆ«åˆ†ä»… 39.3%ï¼Œé€šç”¨æ¨¡å‹é æ³›åŒ–ï¼Œä¸“ç”¨æ¨¡å‹é å¾®è°ƒï¼Œæƒ…æ„Ÿç†è§£ä»æ˜¯çŸ­æ¿ã€‚\n\n---\n\n### âš¡ï¸ LLM æ•ˆç‡ä¸æ¨ç† (Reasoning) ä¼˜åŒ–\nå¦‚ä½•è®©æ¨¡å‹æ€è€ƒå¾—æ›´æ·±ï¼ˆReasoningï¼‰ä»¥åŠè·‘å¾—æ›´å¿«ï¼ˆEfficiencyï¼‰æ˜¯ä»Šå¤©çš„ä¸»æ—‹å¾‹ã€‚\n\n**3. OverFill: Two-Stage Models for Efficient Language Model Decoding**\n> OverFillï¼šç”¨äºé«˜æ•ˆè¯­è¨€æ¨¡å‹è§£ç çš„åŒé˜¶æ®µæ¨¡å‹\n> authors: Woojeong Kim, ..., Alexander M. Rush\n\n**æ ¸å¿ƒå‘ç°**ï¼šAlex Rush å›¢é˜Ÿçš„æ–°ä½œã€‚é’ˆå¯¹ LLM æ¨ç†ä¸­ Prefillï¼ˆè®¡ç®—å¯†é›†ï¼‰å’Œ Decodeï¼ˆå†…å­˜å¯†é›†ï¼‰çš„ç‰¹æ€§ï¼Œæå‡º **OverFill**ï¼šåœ¨ Prefill é˜¶æ®µä½¿ç”¨å®Œæ•´å¤§æ¨¡å‹ï¼Œåœ¨ Decode é˜¶æ®µåˆ‡æ¢åˆ°**å‰ªæåçš„ç¨ å¯†å°æ¨¡å‹**ã€‚\n**äº®ç‚¹**ï¼š3B-to-1B çš„é…ç½®æ¯”ç›´æ¥ç”¨ 1B æ¨¡å‹å¼º 83%ï¼Œä¸”å‡ ä¹æ²¡æœ‰å»¶è¿Ÿå¼€é”€ã€‚è¿™ä¸ºç«¯ä¾§éƒ¨ç½²å¤§æ¨¡å‹æä¾›äº†æ–°æ€è·¯ã€‚\n\n**4. Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization**\n> Klear-Reasonerï¼šé€šè¿‡æ¢¯åº¦ä¿ç•™æˆªæ–­ç­–ç•¥ä¼˜åŒ–æå‡æ¨ç†èƒ½åŠ›\n> authors: Zhenpeng Su, et al.\n\n**æ ¸å¿ƒå‘ç°**ï¼šè¿™æ˜¯ä¸€ç¯‡å¤ç°ç±» o1 é•¿æ¨ç†æ¨¡å‹çš„é‡è¦å·¥ä½œã€‚ä½œè€…æ­ç¤ºäº†å½“å‰ RL è®­ç»ƒæ¨ç†æ¨¡å‹ï¼ˆå¦‚ PPO/GRPOï¼‰ä¸­çš„æˆªæ–­æœºåˆ¶ä¼šæŠ‘åˆ¶æ¢ç´¢ã€‚\n**æ–¹æ³•**ï¼šæå‡ºäº† GPPOï¼ˆæ¢¯åº¦ä¿ç•™æˆªæ–­ç­–ç•¥ä¼˜åŒ–ï¼‰ï¼Œå¹¶å‘å¸ƒäº† **Klear-Reasoner**ã€‚è¯¥æ¨¡å‹åœ¨ AIME 2024 ä¸Šè¾¾åˆ° 90.5%ï¼Œå±•ç¤ºäº†å¦‚ä½•é€šè¿‡é•¿æ€ç»´é“¾ï¼ˆLong CoTï¼‰SFT å’Œæ”¹è¿›çš„ RL è®­ç»ƒå‡ºå¼ºå¤§çš„æ•°å­¦/ä»£ç æ¨ç†æ¨¡å‹ã€‚\n\n**5. Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL**\n> è¶…è¶Šåè½®äº¤äº’ï¼šåˆ©ç”¨å¤§è§„æ¨¡å¼‚æ­¥ RL è§£é”é•¿ç¨‹ Agent æœç´¢\n> authors: Jiaxuan Gao, et al.\n\n**æ ¸å¿ƒå‘ç°**ï¼šé’ˆå¯¹ Agent æœç´¢ä»»åŠ¡ï¼ˆSearch Intelligenceï¼‰ï¼Œç›®å‰çš„å¼€æº Agent å¾€å¾€åªèƒ½è¿›è¡Œå°‘é‡çš„äº¤äº’ã€‚ä½œè€…æå‡ºäº† **ASearcher**ï¼Œé€šè¿‡å¤§è§„æ¨¡å¼‚æ­¥ RL è®­ç»ƒï¼Œè®© Agent èƒ½è¿›è¡Œè¶…è¿‡ 100 è½®çš„å·¥å…·è°ƒç”¨å’Œ 400k token çš„é•¿ç¨‹æœç´¢ï¼Œåœ¨å¤æ‚ä»»åŠ¡ï¼ˆGAIAï¼‰ä¸Šè¡¨ç°å‡ºè‰²ã€‚\n\n---\n\n### ğŸ§¬ AI for Science (æ ¸ç‰©ç†ä¸åŒ»å­¦)\nä»Šå¤©çš„ AI4Science åŒæ ·ç²¾å½©ï¼Œä»å¾®è§‚ç²’å­åˆ°å®è§‚åŒ»ç–—æ•°æ®ç”Ÿæˆã€‚\n\n**6. The DNA of nuclear models: How AI predicts nuclear masses**\n> æ ¸æ¨¡å‹çš„ DNAï¼šAI å¦‚ä½•é¢„æµ‹åŸå­æ ¸è´¨é‡\n> authors: Kate A. Richardson, et al.\n\n**æ ¸å¿ƒå‘ç°**ï¼š**å¯è§£é‡Šæ€§ç‰©ç† AI çš„å…¸èŒƒ**ã€‚ä½œè€…ä¸ä»…è®­ç»ƒäº†ä¸€ä¸ªé¢„æµ‹åŸå­æ ¸ç»“åˆèƒ½ï¼ˆ$E_b$ï¼‰çš„é«˜ç²¾åº¦ AIï¼Œè¿˜å‘ç°å…¶å†…éƒ¨è¡¨ç¤ºå½¢æˆäº†ä¸€ä¸ªâ€œåŒèºæ—‹â€ç»“æ„ï¼ˆç±»ä¼¼äº DNAï¼‰ï¼Œå¯¹åº”è´¨å­å’Œä¸­å­æ•°ã€‚\n**Implication**ï¼šè¿™è¯æ˜äº† AI å¹¶éé»‘ç›’ï¼Œå®ƒå¯ä»¥é‡æ–°å‘ç°ç”šè‡³æ¨å¯¼å‡ºç‰©ç†å­¦å®¶ç†ŸçŸ¥çš„æ¶²æ»´æ¨¡å‹ç­‰ç¬¦å·ç‰©ç†å®šå¾‹ã€‚\n\n**7. SynLLM: A Comparative Analysis of Large Language Models for Medical Tabular Synthetic Data Generation via Prompt Engineering**\n> SynLLMï¼šåŸºäºæç¤ºå·¥ç¨‹çš„å¤§è¯­è¨€æ¨¡å‹åŒ»ç–—è¡¨æ ¼åˆæˆæ•°æ®ç”Ÿæˆæ¯”è¾ƒåˆ†æ\n> authors: Arshia Ilaty, et al.\n\n**æ ¸å¿ƒå‘ç°**ï¼šåŒ»ç–—æ•°æ®éšç§æ˜¯ç—›ç‚¹ã€‚æœ¬æ–‡è¯„ä¼°äº† 20 ç§å¼€æº LLMï¼ˆLlama, Mistral ç­‰ï¼‰ç”ŸæˆåŒ»ç–—è¡¨æ ¼æ•°æ®çš„èƒ½åŠ›ã€‚ç»“è®ºæ˜¯ï¼š**åŸºäºè§„åˆ™çš„ Prompt** èƒ½åœ¨éšç§å’Œæ•°æ®è´¨é‡ä¹‹é—´å–å¾—æœ€ä½³å¹³è¡¡ï¼ŒLLM å®Œå…¨æœ‰èƒ½åŠ›ç”Ÿæˆä¸´åºŠä¸Šåˆç†ä¸”ä¿æŠ¤éšç§çš„åˆæˆæ•°æ®ã€‚\n\n**8. RedDino: A foundation model for red blood cell analysis**\n> RedDinoï¼šçº¢ç»†èƒåˆ†æçš„åŸºç¡€æ¨¡å‹\n> authors: Luca Zedda, et al.\n\n**æ ¸å¿ƒå‘ç°**ï¼šåŸºäº DINOv2 æ¡†æ¶ï¼Œåœ¨ 125 ä¸‡å¼ çº¢ç»†èƒå›¾åƒä¸Šè®­ç»ƒçš„è‡ªç›‘ç£åŸºç¡€æ¨¡å‹ã€‚ä¸“é—¨ç”¨äºè¡€æ¶²å­¦è¯Šæ–­ï¼Œåœ¨å½¢æ€åˆ†ç±»ä¸Šè¾¾åˆ° SOTAã€‚\n\n---\n\n### ğŸ¨ è§†è§‰ç”Ÿæˆä¸ç¼–è¾‘\nè§†é¢‘å’Œ 3D ç”Ÿæˆæ­£æœç€æ›´ç²¾ç»†çš„æ§åˆ¶å‘å±•ã€‚\n\n**9. Cut2Next: Generating Next Shot via In-Context Tuning**\n> Cut2Nextï¼šé€šè¿‡ä¸Šä¸‹æ–‡å¾®è°ƒç”Ÿæˆä¸‹ä¸€é•œå¤´\n> authors: Jingwen He, et al. (Ziwei Liu Team)\n\n**æ ¸å¿ƒå‘ç°**ï¼šå…³æ³¨è§†é¢‘å™äº‹ä¸­çš„**é•œå¤´åˆ‡æ¢**ï¼ˆå¦‚æ­£åæ‰“é•œå¤´ï¼‰ã€‚Cut2Next åˆ©ç”¨ä¸Šä¸‹æ–‡å¾®è°ƒå’Œåˆ†å±‚å¤š Prompt ç­–ç•¥ï¼Œç”Ÿæˆç¬¦åˆç”µå½±å‰ªè¾‘é€»è¾‘çš„â€œä¸‹ä¸€ä¸ªé•œå¤´â€ï¼Œè€Œéä»…ä»…æ˜¯è§†è§‰ä¸Šçš„è¿ç»­ï¼Œå¼ºè°ƒäº†å™äº‹è¿è´¯æ€§ã€‚\n\n**10. LL3M: Large Language 3D Modelers**\n> LL3Mï¼šå¤§è¯­è¨€æ¨¡å‹ 3D å»ºæ¨¡å¸ˆ\n> authors: Sining Lu, et al.\n\n**æ ¸å¿ƒå‘ç°**ï¼šä¸åŒäºç›´æ¥ç”Ÿæˆ 3D èµ„äº§ï¼ŒLL3M å°†ç”Ÿæˆä»»åŠ¡è½¬åŒ–ä¸º**ç¼–å†™ Blender Python ä»£ç **ã€‚é€šè¿‡å¤š Agent åä½œï¼ˆè§„åˆ’ã€å†™ç ã€Debugï¼‰ï¼Œç”Ÿæˆå¯ç¼–è¾‘ã€å¯è§£é‡Šçš„ 3D åœºæ™¯å’Œç‰©ä½“ã€‚è¿™ç§â€œCode as 3D Representationâ€çš„æ€è·¯éå¸¸é€‚åˆå·¥ä¸šå·¥ä½œæµã€‚\n\n**11. Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation**\n> Omni-Effectsï¼šç»Ÿä¸€ä¸”ç©ºé—´å¯æ§çš„è§†è§‰ç‰¹æ•ˆç”Ÿæˆ\n> authors: Fangyuan Mao, et al.\n\n**æ ¸å¿ƒå‘ç°**ï¼šè§£å†³äº†åœ¨ä¸€ä¸ªç”»é¢ä¸­ç”Ÿæˆå¤šä¸ªä¸åŒè§†è§‰ç‰¹æ•ˆï¼ˆVFXï¼Œå¦‚çˆ†ç‚¸ã€ç«ç„°ã€çƒŸé›¾ï¼‰ä¸”äº’ä¸å¹²æ‰°çš„éš¾é¢˜ã€‚ä½¿ç”¨äº† LoRA-MoE å’Œç©ºé—´æ„ŸçŸ¥ Promptï¼Œå®ç°äº†ç²¾å‡†çš„ç‰¹æ•ˆç©ºé—´æ§åˆ¶ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ã€å¯¹é½ä¸è¯„ä¼°\n**12. Towards Effective MLLM Jailbreaking Through Balanced On-Topicness and OOD-Intensity**\n> é€šè¿‡å¹³è¡¡ä¸»é¢˜ç›¸å…³æ€§å’Œ OOD å¼ºåº¦å®ç°æœ‰æ•ˆçš„ MLLM è¶Šç‹±\n> authors: Zuoou Li, et al.\n\n**æ ¸å¿ƒå‘ç°**ï¼šæ­ç¤ºäº†å¤šæ¨¡æ€å¤§æ¨¡å‹å®‰å…¨é˜²å¾¡çš„ä¸€ä¸ªç»“æ„æ€§å¼±ç‚¹ã€‚æ”»å‡»è€…å¦‚æœåœ¨ Prompt ä¸­å¹³è¡¡â€œåˆ‡é¢˜æ€§â€ï¼ˆOn-topicï¼‰å’Œâ€œåˆ†å¸ƒå¤–å¼ºåº¦â€ï¼ˆOODï¼Œæ¯”å¦‚å¥‡æ€ªçš„è§†è§‰çº¿ç´¢ï¼‰ï¼Œæœ€å®¹æ˜“ç»•è¿‡å®‰å…¨è¿‡æ»¤å™¨ã€‚ä½œè€…æå‡º BSD æ”»å‡»æ–¹æ³•ï¼ŒæˆåŠŸç‡æå‡ 67%ã€‚\n\n**13. Pareto Multi-Objective Alignment for Language Models**\n> è¯­è¨€æ¨¡å‹çš„å¸•ç´¯æ‰˜å¤šç›®æ ‡å¯¹é½\n> authors: Qiang He, Setareh Maghsudi\n\n**æ ¸å¿ƒå‘ç°**ï¼šç°å®ä¸­æˆ‘ä»¬æ—¢è¦æ¨¡å‹æœ‰ç”¨ï¼Œåˆè¦å®ƒå®‰å…¨ï¼Œè¿˜è¦ç®€æ´ã€‚ä¼ ç»Ÿçš„ RLHF å¾ˆéš¾å¹³è¡¡è¿™äº›å†²çªç›®æ ‡ã€‚æœ¬æ–‡æå‡ºçš„ PAMA ç®—æ³•å°†å¤šç›®æ ‡å¯¹é½è½¬åŒ–ä¸ºå…·æœ‰é—­å¼è§£çš„å‡¸ä¼˜åŒ–é—®é¢˜ï¼Œå°†å¤æ‚åº¦ä» $O(n^2d)$ é™åˆ° $O(n)$ï¼Œåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶å®ç°äº†å¸•ç´¯æ‰˜æœ€ä¼˜å¯¹é½ã€‚\n\n**14. Not Yet AlphaFold for the Mind: Evaluating Centaur as a Synthetic Participant**\n> å¿ƒæ™ºé¢†åŸŸçš„ AlphaFold å°šæœªåˆ°æ¥ï¼šè¯„ä¼° Centaur ä½œä¸ºåˆæˆå‚ä¸è€…\n> authors: Sabrina Namazova, et al.\n\n**æ ¸å¿ƒå‘ç°**ï¼šè¿™æ˜¯ä¸€ç¯‡**æ‰¹è¯„æ€§è®ºæ–‡**ã€‚æ­¤å‰æœ‰å·¥ä½œï¼ˆCentaurï¼‰å£°ç§°å¾®è°ƒåçš„ LLM å¯ä»¥ä½œä¸ºå¿ƒç†å­¦å®éªŒçš„â€œåˆæˆå‚ä¸è€…â€ã€‚æœ¬æ–‡åé©³äº†è¿™ä¸€ç‚¹ï¼ŒæŒ‡å‡º Centaur è™½ç„¶é¢„æµ‹å‡†ç¡®ï¼Œä½†åœ¨ç”Ÿæˆè¡Œä¸ºä¸Šä¸äººç±»æ•°æ®å­˜åœ¨ç³»ç»Ÿæ€§åå·®ï¼Œç›®å‰è¿˜ä¸èƒ½æ›¿ä»£äººç±»å—è¯•è€…ã€‚\n\n---\n\n### ğŸ® å…¶ä»–æœ‰è¶£çš„å·¥ä½œ\n\n*   **[Game AI] Playing Atari Space Invaders with Sparse Cosine Optimized Policy Evolution**: ä½¿ç”¨ç¦»æ•£ä½™å¼¦å˜æ¢ (DCT) å‹ç¼©çŠ¶æ€ç©ºé—´ï¼Œç”¨è¿›åŒ–ç®—æ³•ç©é›…è¾¾åˆ©æ¸¸æˆï¼Œå‚æ•°æå°‘ä½†æ•ˆæœæ‹”ç¾¤ã€‚\n*   **[Hardware] Energy Consumption in Parallel Neural Network Training**: æ‰å®çš„å®éªŒåˆ†æï¼ŒæŒ‡å‡ºå¹¶è¡Œè®­ç»ƒçš„èƒ½è€—ä¸ GPU å°æ—¶æ•°å‘ˆçº¿æ€§å…³ç³»ï¼Œä½†å—å…·ä½“ç¡¬ä»¶å’Œ Batch Size ç­–ç•¥å½±å“å·¨å¤§ã€‚\n*   **[Search] Retrieval-Augmented Generation in Industry**: é‡‡è®¿äº† 13 ä½å·¥ä¸šç•Œä»ä¸šè€…ï¼Œæ­éœ²äº† RAG åœ¨è½åœ°æ—¶çš„çœŸå®ç—›ç‚¹ï¼šä¸»è¦æ˜¯æ•°æ®é¢„å¤„ç†å’Œç¼ºä¹è‡ªåŠ¨è¯„ä¼°ï¼Œè€Œéæ¨¡å‹æœ¬èº«ã€‚\n\nä»Šå¤©çš„å¿«æŠ¥å°±åˆ°è¿™é‡Œï¼ŒGPT-5 çš„å‡ºç°ï¼ˆå³ä¾¿åœ¨è®ºæ–‡æ ‡é¢˜ä¸­ï¼‰é¢„ç¤ºç€æ–°ä¸€è½®ç«èµ›çš„å¼€å§‹ï¼Œè€Œæ¨ç†èƒ½åŠ›çš„ Scaling æ­£åœ¨æˆä¸ºæ–°çš„æŠ¤åŸæ²³ã€‚ç¥å¤§å®¶ç§‘ç ”é¡ºåˆ©ï¼",
  "papers": [
    {
      "arxiv_id": "2508.08529v1",
      "title": "SynLLM: A Comparative Analysis of Large Language Models for Medical Tabular Synthetic Data Generation via Prompt Engineering",
      "title_zh": "SynLLMï¼šåŸºäºæç¤ºå·¥ç¨‹çš„å¤§è¯­è¨€æ¨¡å‹åŒ»ç–—è¡¨æ ¼åˆæˆæ•°æ®ç”Ÿæˆå¯¹æ¯”åˆ†æ",
      "authors": [
        "Arshia Ilaty",
        "Hossein Shirazi",
        "Hajar Homayouni"
      ],
      "abstract": "Access to real-world medical data is often restricted due to privacy regulations, posing a significant barrier to the advancement of healthcare research. Synthetic data offers a promising alternative; however, generating realistic, clinically valid, and privacy-conscious records remains a major challenge. Recent advancements in Large Language Models (LLMs) offer new opportunities for structured data generation; however, existing approaches frequently lack systematic prompting strategies and comprehensive, multi-dimensional evaluation frameworks.\n  In this paper, we present SynLLM, a modular framework for generating high-quality synthetic medical tabular data using 20 state-of-the-art open-source LLMs, including LLaMA, Mistral, and GPT variants, guided by structured prompts. We propose four distinct prompt types, ranging from example-driven to rule-based constraints, that encode schema, metadata, and domain knowledge to control generation without model fine-tuning. Our framework features a comprehensive evaluation pipeline that rigorously assesses generated data across statistical fidelity, clinical consistency, and privacy preservation.\n  We evaluate SynLLM across three public medical datasets, including Diabetes, Cirrhosis, and Stroke, using 20 open-source LLMs. Our results show that prompt engineering significantly impacts data quality and privacy risk, with rule-based prompts achieving the best privacy-quality balance. SynLLM establishes that, when guided by well-designed prompts and evaluated with robust, multi-metric criteria, LLMs can generate synthetic medical data that is both clinically plausible and privacy-aware, paving the way for safer and more effective data sharing in healthcare research.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SynLLMï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨åˆ©ç”¨ 20 ç§æœ€å…ˆè¿›çš„å¼€æºå¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) ç”Ÿæˆé«˜è´¨é‡åˆæˆåŒ»ç–—è¡¨æ ¼æ•°æ®çš„æ¨¡å—åŒ–æ¡†æ¶ã€‚ä¸ºäº†è§£å†³çœŸå®åŒ»ç–—æ•°æ®å—éšç§æ³•è§„é™åˆ¶è€Œéš¾ä»¥è·å–çš„é—®é¢˜ï¼ŒSynLLM é‡‡ç”¨äº†å››ç§ä»ç¤ºä¾‹é©±åŠ¨åˆ°åŸºäºè§„åˆ™çº¦æŸçš„æç¤ºå·¥ç¨‹ (Prompt Engineering) ç­–ç•¥ï¼Œåœ¨æ— éœ€æ¨¡å‹å¾®è°ƒçš„æƒ…å†µä¸‹ç¼–ç æ¨¡å¼ã€å…ƒæ•°æ®å’Œé¢†åŸŸçŸ¥è¯†æ¥æ§åˆ¶æ•°æ®ç”Ÿæˆã€‚è¯¥æ¡†æ¶å»ºç«‹äº†ä¸€å¥—ç»¼åˆè¯„ä¼°æµç¨‹ï¼Œä»ç»Ÿè®¡ä¿çœŸåº¦ (Statistical Fidelity)ã€ä¸´åºŠä¸€è‡´æ€§ (Clinical Consistency) å’Œéšç§ä¿æŠ¤ (Privacy Preservation) ä¸‰ä¸ªç»´åº¦å¯¹åˆæˆæ•°æ®è¿›è¡Œä¸¥æ ¼æµ‹è¯•ã€‚åœ¨ç³–å°¿ç—…ã€è‚ç¡¬åŒ–å’Œä¸­é£ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæç¤ºå·¥ç¨‹å¯¹æ•°æ®è´¨é‡å’Œéšç§é£é™©æœ‰æ˜¾è‘—å½±å“ï¼Œå…¶ä¸­åŸºäºè§„åˆ™çš„æç¤ºåœ¨éšç§ä¸è´¨é‡å¹³è¡¡æ–¹é¢è¡¨ç°æœ€ä½³ã€‚SynLLM è¯æ˜äº†åœ¨ç²¾å¿ƒè®¾è®¡çš„æç¤ºç­–ç•¥å’Œé²æ£’çš„å¤šæŒ‡æ ‡è¯„ä¼°æŒ‡å¯¼ä¸‹ï¼ŒLLMs èƒ½å¤Ÿç”Ÿæˆä¸´åºŠåˆç†ä¸”å…·å¤‡éšç§æ„è¯†çš„åŒ»ç–—æ•°æ®ï¼Œä¸ºåŒ»ç–—ç ”ç©¶ä¸­æ›´å®‰å…¨ã€æ›´æœ‰æ•ˆçš„æ•°æ®å…±äº«å¼€è¾Ÿäº†é“è·¯ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 Pages, 2 Supplementary Pages, 6 Tables",
      "pdf_url": "https://arxiv.org/pdf/2508.08529v1",
      "published_date": "2025-08-11 23:56:42 UTC",
      "updated_date": "2025-08-11 23:56:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:51:47.254066+00:00"
    },
    {
      "arxiv_id": "2508.08526v1",
      "title": "Playing Atari Space Invaders with Sparse Cosine Optimized Policy Evolution",
      "title_zh": "åŸºäºç¨€ç–ä½™å¼¦ä¼˜åŒ–ç­–ç•¥è¿›åŒ–çš„ Atariã€Šå¤ªç©ºä¾µç•¥è€…ã€‹åšå¼ˆ",
      "authors": [
        "Jim O'Connor",
        "Jay B. Nash",
        "Derin Gezgin",
        "Gary B. Parker"
      ],
      "abstract": "Evolutionary approaches have previously been shown to be effective learning methods for a diverse set of domains. However, the domain of game-playing poses a particular challenge for evolutionary methods due to the inherently large state space of video games. As the size of the input state expands, the size of the policy must also increase in order to effectively learn the temporal patterns in the game space. Consequently, a larger policy must contain more trainable parameters, exponentially increasing the size of the search space. Any increase in search space is highly problematic for evolutionary methods, as increasing the number of trainable parameters is inversely correlated with convergence speed. To reduce the size of the input space while maintaining a meaningful representation of the original space, we introduce Sparse Cosine Optimized Policy Evolution (SCOPE). SCOPE utilizes the Discrete Cosine Transform (DCT) as a pseudo attention mechanism, transforming an input state into a coefficient matrix. By truncating and applying sparsification to this matrix, we reduce the dimensionality of the input space while retaining the highest energy features of the original input. We demonstrate the effectiveness of SCOPE as the policy for the Atari game Space Invaders. In this task, SCOPE with CMA-ES outperforms evolutionary methods that consider an unmodified input state, such as OpenAI-ES and HyperNEAT. SCOPE also outperforms simple reinforcement learning methods, such as DQN and A3C. SCOPE achieves this result through reducing the input size by 53% from 33,600 to 15,625 then using a bilinear affine mapping of sparse DCT coefficients to policy actions learned by the CMA-ES algorithm.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¿›åŒ–ç®—æ³•(Evolutionary methods)åœ¨å¤„ç†è§†é¢‘æ¸¸æˆç­‰å¤§çŠ¶æ€ç©ºé—´ä»»åŠ¡æ—¶ï¼Œå› å‚æ•°è¿‡å¤šå¯¼è‡´æœç´¢ç©ºé—´æ¿€å¢ã€æ”¶æ•›é€Ÿåº¦å˜æ…¢çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºSCOPE (Sparse Cosine Optimized Policy Evolution) çš„æ–°æ¡†æ¶ã€‚SCOPE åˆ©ç”¨ç¦»æ•£ä½™å¼¦å˜æ¢(Discrete Cosine Transform, DCT)ä½œä¸ºä¸€ç§ä¼ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†è¾“å…¥çŠ¶æ€è½¬æ¢ä¸ºç³»æ•°çŸ©é˜µï¼Œå¹¶é€šè¿‡æˆªæ–­å’Œç¨€ç–åŒ–å¤„ç†ï¼Œåœ¨ä¿ç•™é«˜èƒ½é‡ç‰¹å¾çš„åŒæ—¶æ˜¾è‘—é™ä½äº†è¾“å…¥ç»´åº¦ã€‚åœ¨Atariæ¸¸æˆçš„Space Invadersä»»åŠ¡ä¸­ï¼ŒSCOPE ç»“åˆCMA-ESç®—æ³•ï¼Œé€šè¿‡å¯¹ç¨€ç–DCTç³»æ•°è¿›è¡ŒåŒçº¿æ€§ä»¿å°„æ˜ å°„(bilinear affine mapping)æ¥å­¦ä¹ ç­–ç•¥åŠ¨ä½œï¼ŒæˆåŠŸå°†è¾“å…¥è§„æ¨¡ä»33,600ç¼©å‡è‡³15,625ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•çš„æ€§èƒ½ä¸ä»…ä¼˜äºOpenAI-ESã€HyperNEATç­‰è¿›åŒ–ç®—æ³•ï¼ŒåŒæ—¶ä¹Ÿè¶…è¶Šäº†DQNå’ŒA3Cç­‰åŸºç¡€å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ–¹æ³•ï¼Œæœ‰æ•ˆå¹³è¡¡äº†è¡¨ç¤ºèƒ½åŠ›ä¸è®¡ç®—æ•ˆç‡ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "The 21st AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment",
      "pdf_url": "https://arxiv.org/pdf/2508.08526v1",
      "published_date": "2025-08-11 23:44:08 UTC",
      "updated_date": "2025-08-11 23:44:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:52:50.592019+00:00"
    },
    {
      "arxiv_id": "2508.08524v4",
      "title": "StreetReaderAI: Making Street View Accessible Using Context-Aware Multimodal AI",
      "title_zh": "StreetReaderAIï¼šåˆ©ç”¨æƒ…å¢ƒæ„ŸçŸ¥å¤šæ¨¡æ€äººå·¥æ™ºèƒ½å®ç°æ— éšœç¢è¡—æ™¯è®¿é—®",
      "authors": [
        "Jon E. Froehlich",
        "Alexander Fiannaca",
        "Nimer Jaber",
        "Victor Tsaran",
        "Shaun Kane"
      ],
      "abstract": "Interactive streetscape mapping tools such as Google Street View (GSV) and Meta Mapillary enable users to virtually navigate and experience real-world environments via immersive 360Â° imagery but remain fundamentally inaccessible to blind users. We introduce StreetReaderAI, the first-ever accessible street view tool, which combines context-aware, multimodal AI, accessible navigation controls, and conversational speech. With StreetReaderAI, blind users can virtually examine destinations, engage in open-world exploration, or virtually tour any of the over 220 billion images and 100+ countries where GSV is deployed. We iteratively designed StreetReaderAI with a mixed-visual ability team and performed an evaluation with eleven blind users. Our findings demonstrate the value of an accessible street view in supporting POI investigations and remote route planning. We close by enumerating key guidelines for future work.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†StreetReaderAIï¼Œè¿™æ˜¯é¦–ä¸ªé¢å‘è§†éšœç”¨æˆ·çš„æ— éšœç¢è¡—æ™¯æ¢ç´¢å·¥å…·ï¼Œæ—¨åœ¨è§£å†³Google Street View (GSV)å’ŒMeta Mapillaryç­‰å¹³å°åœ¨è§†è§‰è®¿é—®æ–¹é¢çš„æ ¹æœ¬å±€é™ã€‚è¯¥ç³»ç»Ÿé›†æˆäº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥(context-aware)çš„å¤šæ¨¡æ€AI (Multimodal AI)ã€æ— éšœç¢å¯¼èˆªæ§åˆ¶å’Œå¯¹è¯å¼è¯­éŸ³æŠ€æœ¯ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿè™šæ‹Ÿæ£€æŸ¥ç›®çš„åœ°ã€è¿›è¡Œå¼€æ”¾ä¸–ç•Œæ¢ç´¢æˆ–æ¸¸è§ˆGSVè¦†ç›–çš„å…¨çƒæµ·é‡å…¨æ™¯å›¾åƒã€‚StreetReaderAIç”±ä¸€ä¸ªæ··åˆè§†åŠ›å›¢é˜Ÿé€šè¿‡è¿­ä»£è®¾è®¡å¼€å‘ï¼Œå¹¶é’ˆå¯¹11åè§†éšœç”¨æˆ·è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥å·¥å…·åœ¨æ”¯æŒå…´è¶£ç‚¹(POI)è°ƒæŸ¥å’Œè¿œç¨‹è·¯å¾„è§„åˆ’(remote route planning)æ–¹é¢å…·æœ‰é‡è¦ä»·å€¼ã€‚ç ”ç©¶æœ€åæ€»ç»“äº†æœªæ¥æ— éšœç¢è¡—æ™¯ç³»ç»Ÿè®¾è®¡çš„å…³é”®å‡†åˆ™ï¼Œä¸ºæå‡è§†éšœäººå£«çš„ç©ºé—´æ„ŸçŸ¥ä¸ç¯å¢ƒæ¢ç´¢èƒ½åŠ›å¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted to UIST'25; v2. Fixed a missing word in the PDF; v3. Fixed a typo in an author's name; v4. Changed system name and title",
      "pdf_url": "https://arxiv.org/pdf/2508.08524v4",
      "published_date": "2025-08-11 23:30:39 UTC",
      "updated_date": "2025-09-26 13:19:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:51:51.160637+00:00"
    },
    {
      "arxiv_id": "2508.08521v1",
      "title": "VISOR: Visual Input-based Steering for Output Redirection in Vision-Language Models",
      "title_zh": "VISORï¼šè§†è§‰è¯­è¨€æ¨¡å‹ä¸­åŸºäºè§†è§‰è¾“å…¥çš„è¾“å‡ºé‡å®šå‘å¼•å¯¼",
      "authors": [
        "Mansi Phute",
        "Ravikumar Balakrishnan"
      ],
      "abstract": "Vision Language Models (VLMs) are increasingly being used in a broad range of applications, bringing their security and behavioral control to the forefront. While existing approaches for behavioral control or output redirection, like system prompting in VLMs, are easily detectable and often ineffective, activation-based steering vectors require invasive runtime access to model internals--incompatible with API-based services and closed-source deployments. We introduce VISOR (Visual Input-based Steering for Output Redirection), a novel method that achieves sophisticated behavioral control through optimized visual inputs alone. By crafting universal steering images that induce target activation patterns, VISOR enables practical deployment across all VLM serving modalities while remaining imperceptible compared to explicit textual instructions. We validate VISOR on LLaVA-1.5-7B across three critical alignment tasks: refusal, sycophancy and survival instinct. A single 150KB steering image matches steering vector performance within 1-2% for positive behavioral shifts while dramatically exceeding it for negative steering--achieving up to 25% shifts from baseline compared to steering vectors' modest changes. Unlike system prompting (3-4% shifts), VISOR provides robust bidirectional control while maintaining 99.9% performance on 14,000 unrelated MMLU tasks. Beyond eliminating runtime overhead and model access requirements, VISOR exposes a critical security vulnerability: adversaries can achieve sophisticated behavioral manipulation through visual channels alone, bypassing text-based defenses. Our work fundamentally re-imagines multimodal model control and highlights the urgent need for defenses against visual steering attacks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†VISOR (Visual Input-based Steering for Output Redirection)ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ä¼˜åŒ–è§†è§‰è¾“å…¥æ¥å®ç°è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)é«˜çº§è¡Œä¸ºæ§åˆ¶çš„æ–°æ–¹æ³•ã€‚ç›¸æ¯”äºå®¹æ˜“è¢«æ£€æµ‹çš„ç³»ç»Ÿæç¤º(System Prompting)å’Œéœ€è¦æ¨¡å‹å†…éƒ¨è®¿é—®æƒé™çš„æ¿€æ´»è½¬å‘å‘é‡(Activation-based Steering Vectors)ï¼ŒVISORé€šè¿‡ç”Ÿæˆé€šç”¨çš„è½¬å‘å›¾åƒæ¥è¯±å‘ç‰¹å®šçš„ç›®æ ‡æ¿€æ´»æ¨¡å¼ï¼Œä»è€Œåœ¨æ— éœ€æ¨¡å‹å†…éƒ¨è®¿é—®çš„æƒ…å†µä¸‹å®ç°ç²¾ç¡®çš„è¡Œä¸ºé‡å®šå‘ã€‚åœ¨LLaVA-1.5-7Bä¸Šçš„éªŒè¯ç»“æœæ˜¾ç¤ºï¼ŒVISORåœ¨æ‹’ç»æœåŠ¡ã€è°„åªšå’Œç”Ÿå­˜æœ¬èƒ½ç­‰å…³é”®å¯¹é½ä»»åŠ¡ä¸­è¾¾åˆ°äº†ä¸è½¬å‘å‘é‡ç›¸å½“ç”šè‡³æ›´ä¼˜çš„æ€§èƒ½ï¼Œä¸”è¿œè¶…ä¼ ç»Ÿçš„ç³»ç»Ÿæç¤ºæ–¹æ³•ã€‚è¯¥æŠ€æœ¯åœ¨14,000ä¸ªMMLUä»»åŠ¡ä¸Šä¿æŒäº†99.9%çš„åŸå§‹æ€§èƒ½ï¼Œä¸”ä¸äº§ç”Ÿé¢å¤–çš„è¿è¡Œæ—¶å¼€é”€ã€‚æ­¤å¤–ï¼Œè¯¥å·¥ä½œæ­ç¤ºäº†ä¸€ä¸ªé‡å¤§çš„å®‰å…¨éšæ‚£ï¼Œå³æ”»å‡»è€…å¯ä»¥ç»•è¿‡æ–‡æœ¬é˜²å¾¡ï¼Œä»…åˆ©ç”¨è§†è§‰é€šé“å®ç°å¤æ‚çš„è¡Œä¸ºæ“çºµã€‚VISORçš„ç ”ç©¶ä¸ä»…é‡æ–°å®šä¹‰äº†å¤šæ¨¡æ€æ¨¡å‹çš„æ§åˆ¶æ–¹å¼ï¼Œä¹Ÿå‡¸æ˜¾äº†å¼€å‘é’ˆå¯¹è§†è§‰è½¬å‘æ”»å‡»é˜²å¾¡æªæ–½çš„ç´§è¿«æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08521v1",
      "published_date": "2025-08-11 23:25:16 UTC",
      "updated_date": "2025-08-11 23:25:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:51:58.448183+00:00"
    },
    {
      "arxiv_id": "2508.08512v1",
      "title": "Using LLMs to Capture Users' Temporal Context for Recommendation",
      "title_zh": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹æ•è·æ¨èä¸­çš„ç”¨æˆ·æ—¶åºä¸Šä¸‹æ–‡",
      "authors": [
        "Milad Sabouri",
        "Masoud Mansoury",
        "Kun Lin",
        "Bamshad Mobasher"
      ],
      "abstract": "Effective recommender systems demand dynamic user understanding, especially in complex, evolving environments. Traditional user profiling often fails to capture the nuanced, temporal contextual factors of user preferences, such as transient short-term interests and enduring long-term tastes. This paper presents an assessment of Large Language Models (LLMs) for generating semantically rich, time-aware user profiles. We do not propose a novel end-to-end recommendation architecture; instead, the core contribution is a systematic investigation into the degree of LLM effectiveness in capturing the dynamics of user context by disentangling short-term and long-term preferences. This approach, framing temporal preferences as dynamic user contexts for recommendations, adaptively fuses these distinct contextual components into comprehensive user embeddings. The evaluation across Movies&TV and Video Games domains suggests that while LLM-generated profiles offer semantic depth and temporal structure, their effectiveness for context-aware recommendations is notably contingent on the richness of user interaction histories. Significant gains are observed in dense domains (e.g., Movies&TV), whereas improvements are less pronounced in sparse environments (e.g., Video Games). This work highlights LLMs' nuanced potential in enhancing user profiling for adaptive, context-aware recommendations, emphasizing the critical role of dataset characteristics for practical applicability.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)æ•æ‰ç”¨æˆ·æ—¶é—´ä¸Šä¸‹æ–‡(Temporal Context)ä»¥ä¼˜åŒ–æ¨èç³»ç»Ÿçš„èƒ½åŠ›ï¼Œé‡ç‚¹æ¢è®¨äº†LLMsåœ¨åŒºåˆ†çŸ­æœŸåå¥½(Short-term Preferences)ä¸é•¿æœŸå…´è¶£(Long-term Tastes)æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶é€šè¿‡å°†è¿™äº›åŠ¨æ€ä¸Šä¸‹æ–‡æˆåˆ†è‡ªé€‚åº”èåˆä¸ºç»¼åˆç”¨æˆ·åµŒå…¥(User Embeddings)ï¼Œåœ¨Movies&TVå’ŒVideo Gamesä¸¤ä¸ªé¢†åŸŸéªŒè¯äº†å…¶ç”Ÿæˆè¯­ä¹‰ä¸°å¯Œä¸”å…·å¤‡æ—¶é—´æ„ŸçŸ¥çš„ç”¨æˆ·ç”»åƒ(User Profiles)çš„æ½œåŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMå¢å¼ºçš„ç”»åƒåœ¨äº¤äº’å†å²ä¸°å¯Œçš„å¯†é›†é¢†åŸŸè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œè€Œåœ¨ç¨€ç–ç¯å¢ƒä¸‹çš„æå‡åˆ™ç›¸å¯¹æœ‰é™ã€‚è¯¥å·¥ä½œå¼ºè°ƒäº†LLMsåœ¨æå‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨è(Context-aware Recommendation)æ–¹é¢çš„ç»†å¾®æ½œåŠ›ï¼Œå¹¶æ­ç¤ºäº†æ•°æ®é›†ç‰¹å¾å¯¹å…¶å®é™…åº”ç”¨æ€§çš„å†³å®šæ€§å½±å“ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08512v1",
      "published_date": "2025-08-11 22:48:31 UTC",
      "updated_date": "2025-08-11 22:48:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:51:56.758980+00:00"
    },
    {
      "arxiv_id": "2508.08509v1",
      "title": "Steerable Pluralism: Pluralistic Alignment via Few-Shot Comparative Regression",
      "title_zh": "å¯å¼•å¯¼çš„å¤šå…ƒåŒ–ï¼šåŸºäºå°‘æ ·æœ¬æ¯”è¾ƒå›å½’çš„å¤šå…ƒå¯¹é½",
      "authors": [
        "Jadie Adams",
        "Brian Hu",
        "Emily Veenhuis",
        "David Joy",
        "Bharadwaj Ravichandran",
        "Aaron Bray",
        "Anthony Hoogs",
        "Arslan Basharat"
      ],
      "abstract": "Large language models (LLMs) are currently aligned using techniques such as reinforcement learning from human feedback (RLHF). However, these methods use scalar rewards that can only reflect user preferences on average. Pluralistic alignment instead seeks to capture diverse user preferences across a set of attributes, moving beyond just helpfulness and harmlessness. Toward this end, we propose a steerable pluralistic model based on few-shot comparative regression that can adapt to individual user preferences. Our approach leverages in-context learning and reasoning, grounded in a set of fine-grained attributes, to compare response options and make aligned choices. To evaluate our algorithm, we also propose two new steerable pluralistic benchmarks by adapting the Moral Integrity Corpus (MIC) and the HelpSteer2 datasets, demonstrating the applicability of our approach to value-aligned decision-making and reward modeling, respectively. Our few-shot comparative regression approach is interpretable and compatible with different attributes and LLMs, while outperforming multiple baseline and state-of-the-art methods. Our work provides new insights and research directions in pluralistic alignment, enabling a more fair and representative use of LLMs and advancing the state-of-the-art in ethical AI.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) å¯¹é½è¿‡ç¨‹ä¸­æ ‡é‡å¥–åŠ±æ— æ³•ä½“ç°å¤šå…ƒç”¨æˆ·åå¥½çš„å±€é™ï¼Œæå‡ºäº†åŸºäºå°‘æ ·æœ¬æ¯”è¾ƒå›å½’ (Few-Shot Comparative Regression) çš„å¯æ§å¤šå…ƒåŒ–å¯¹é½æ¨¡å‹ (Steerable Pluralism)ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æƒ…å¢ƒå­¦ä¹  (In-context Learning) å’Œæ¨ç†ï¼Œåœ¨ç»†ç²’åº¦å±æ€§çš„åŸºç¡€ä¸Šæ¯”è¾ƒä¸åŒå“åº”é€‰é¡¹å¹¶åšå‡ºå¯¹é½å†³ç­–ï¼Œä»è€Œçµæ´»é€‚åº”ä¸ªåˆ«ç”¨æˆ·çš„åå¥½ã€‚ç ”ç©¶äººå‘˜é€šè¿‡æ”¹è¿› Moral Integrity Corpus (MIC) å’Œ HelpSteer2 æ•°æ®é›†æ„å»ºäº†æ–°çš„åŸºå‡†æµ‹è¯•ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨ä»·å€¼å¯¹é½å’Œå¥–åŠ±å»ºæ¨¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒå¯è§£é‡Šæ€§çš„åŒæ—¶ï¼Œæ€§èƒ½ä¼˜äºå¤šç§åŸºå‡†å’Œå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œä¸”èƒ½å…¼å®¹ä¸åŒçš„å±æ€§ä¸æ¨¡å‹ã€‚è¯¥å·¥ä½œä¸ºå®ç°æ›´å…¬å¹³ã€æ›´å…·ä»£è¡¨æ€§çš„ LLMs åº”ç”¨æä¾›äº†æ–°è·¯å¾„ï¼Œæ˜¾è‘—æ¨è¿›äº†ä¼¦ç†äººå·¥æ™ºèƒ½ (Ethical AI) çš„ç ”ç©¶ä¸å‘å±•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "AIES '25: Proceedings of the 2025 AAAI/ACM Conference on AI, Ethics, and Society",
      "pdf_url": "https://arxiv.org/pdf/2508.08509v1",
      "published_date": "2025-08-11 22:40:31 UTC",
      "updated_date": "2025-08-11 22:40:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:52:04.660524+00:00"
    },
    {
      "arxiv_id": "2508.08504v1",
      "title": "When the Domain Expert Has No Time and the LLM Developer Has No Clinical Expertise: Real-World Lessons from LLM Co-Design in a Safety-Net Hospital",
      "title_zh": "å½“é¢†åŸŸä¸“å®¶æ— æš‡åˆ†èº«ä¸” LLM å¼€å‘è€…ç¼ºä¹ä¸´åºŠä¸“ä¸šçŸ¥è¯†ï¼šå®‰å…¨ç½‘åŒ»é™¢ LLM ååŒè®¾è®¡çš„ç°å®ç»éªŒ",
      "authors": [
        "Avni Kothari",
        "Patrick Vossler",
        "Jean Digitale",
        "Mohammad Forouzannia",
        "Elise Rosenberg",
        "Michele Lee",
        "Jennee Bryant",
        "Melanie Molina",
        "James Marks",
        "Lucas Zier",
        "Jean Feng"
      ],
      "abstract": "Large language models (LLMs) have the potential to address social and behavioral determinants of health by transforming labor intensive workflows in resource-constrained settings. Creating LLM-based applications that serve the needs of underserved communities requires a deep understanding of their local context, but it is often the case that neither LLMs nor their developers possess this local expertise, and the experts in these communities often face severe time/resource constraints. This creates a disconnect: how can one engage in meaningful co-design of an LLM-based application for an under-resourced community when the communication channel between the LLM developer and domain expert is constrained? We explored this question through a real-world case study, in which our data science team sought to partner with social workers at a safety net hospital to build an LLM application that summarizes patients' social needs. Whereas prior works focus on the challenge of prompt tuning, we found that the most critical challenge in this setting is the careful and precise specification of \\what information to surface to providers so that the LLM application is accurate, comprehensive, and verifiable. Here we present a novel co-design framework for settings with limited access to domain experts, in which the summary generation task is first decomposed into individually-optimizable attributes and then each attribute is efficiently refined and validated through a multi-tier cascading approach.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨èµ„æºå—é™çš„ç¤¾åŒºåŒ»ç–—æœºæ„ï¼ˆSafety-Net Hospitalï¼‰ä¸­ï¼Œå½“é¢†åŸŸä¸“å®¶ï¼ˆDomain Expertï¼‰æ—¶é—´ç´§è¿«ä¸”å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼€å‘è€…ç¼ºä¹ä¸´åºŠä¸“ä¸šèƒŒæ™¯æ—¶ï¼Œå¦‚ä½•è¿›è¡Œæœ‰æ•ˆçš„å…±åŒè®¾è®¡ï¼ˆCo-Designï¼‰ã€‚é€šè¿‡ä¸ºç¤¾ä¼šå·¥ä½œè€…å¼€å‘æ‚£è€…ç¤¾ä¼šéœ€æ±‚æ€»ç»“åº”ç”¨çš„çœŸå®æ¡ˆä¾‹ï¼Œç ”ç©¶å‘ç°æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºç²¾ç¡®å®šä¹‰éœ€è¦æå–çš„ä¿¡æ¯ï¼Œä»¥ç¡®ä¿LLMè¾“å‡ºçš„å‡†ç¡®æ€§ã€å…¨é¢æ€§å’Œå¯éªŒè¯æ€§ï¼Œè€Œéå•çº¯çš„æç¤ºè¯è°ƒä¼˜ï¼ˆPrompt Tuningï¼‰ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹ä¸“å®¶æ¥è§¦å—é™åœºæ™¯çš„æ–°å‹å…±åŒè®¾è®¡æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†æ€»ç»“ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªå¯ç‹¬ç«‹ä¼˜åŒ–çš„å±æ€§ï¼Œå¹¶é‡‡ç”¨å¤šå±‚çº§è”æ–¹æ³•ï¼ˆMulti-Tier Cascading Approachï¼‰å¯¹å„å±æ€§è¿›è¡Œé«˜æ•ˆçš„æç‚¼ä¸éªŒè¯ã€‚è¿™ä¸€æˆæœä¸ºåœ¨åŒ»ç–—èµ„æºåŒ®ä¹ç¯å¢ƒä¸‹å¼€å‘ç¬¦åˆæœ¬åœ°åŒ–ä¸“ä¸šéœ€æ±‚çš„å¤§æ¨¡å‹åº”ç”¨æä¾›äº†é‡è¦çš„å®è¯ç»éªŒä¸æ–¹æ³•è®ºå‚è€ƒã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08504v1",
      "published_date": "2025-08-11 22:34:23 UTC",
      "updated_date": "2025-08-11 22:34:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:52:06.164646+00:00"
    },
    {
      "arxiv_id": "2508.08501v2",
      "title": "GVGAI-LLM: Evaluating Large Language Model Agents with Infinite Games",
      "title_zh": "GVGAI-LLMï¼šåŸºäºæ— é™æ¸¸æˆçš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“è¯„ä¼°",
      "authors": [
        "Yuchen Li",
        "Cong Lin",
        "Muhammad Umair Nasir",
        "Philip Bontrager",
        "Jialin Liu",
        "Julian Togelius"
      ],
      "abstract": "We introduce GVGAI-LLM, a video game benchmark for evaluating the reasoning and problem-solving capabilities of large language models (LLMs). Built on the General Video Game AI framework, it features a diverse collection of arcade-style games designed to test a model's ability to handle tasks that differ from most existing LLM benchmarks. The benchmark leverages a game description language that enables rapid creation of new games and levels, helping to prevent overfitting over time. Each game scene is represented by a compact set of ASCII characters, allowing for efficient processing by language models. GVGAI-LLM defines interpretable metrics, including the meaningful step ratio, step efficiency, and overall score, to assess model behavior. Through zero-shot evaluations across a broad set of games and levels with diverse challenges and skill depth, we reveal persistent limitations of LLMs in spatial reasoning and basic planning. Current models consistently exhibit spatial and logical errors, motivating structured prompting and spatial grounding techniques. While these interventions lead to partial improvements, the benchmark remains very far from solved. GVGAI-LLM provides a reproducible testbed for advancing research on language model capabilities, with a particular emphasis on agentic behavior and contextual reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GVGAI-LLMï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºGeneral Video Game AIæ¡†æ¶çš„è§†é¢‘æ¸¸æˆåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ¨ç†å’Œé—®é¢˜è§£å†³èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•åˆ©ç”¨æ¸¸æˆæè¿°è¯­è¨€å®ç°æ–°æ¸¸æˆå’Œå…³å¡çš„å¿«é€Ÿåˆ›å»ºï¼Œä»è€Œæœ‰æ•ˆé˜²æ­¢æ¨¡å‹å‡ºç°è¿‡æ‹Ÿåˆã€‚ç³»ç»Ÿé‡‡ç”¨ç´§å‡‘çš„ASCIIå­—ç¬¦è¡¨ç¤ºæ¸¸æˆåœºæ™¯ä»¥å®ç°é«˜æ•ˆå¤„ç†ï¼Œå¹¶å®šä¹‰äº†Meaningful Step Ratioã€Step Efficiencyå’ŒOverall Scoreç­‰å¯è§£é‡Šæ€§æŒ‡æ ‡æ¥è¯„ä¼°æ¨¡å‹è¡¨ç°ã€‚é€šè¿‡åœ¨å¤šæ ·åŒ–æŒ‘æˆ˜ä¸‹çš„Zero-shotè¯„ä¼°ï¼Œç ”ç©¶æ­ç¤ºäº†LLMsåœ¨ç©ºé—´æ¨ç†(Spatial Reasoning)å’ŒåŸºç¡€è§„åˆ’(Basic Planning)æ–¹é¢çš„æŒç»­å±€é™ï¼Œå³ä¾¿ä½¿ç”¨ç»“æ„åŒ–æç¤º(Structured Prompting)ä¹Ÿéš¾ä»¥å®Œå…¨è§£å†³ã€‚GVGAI-LLMä¸ºæå‡æ™ºèƒ½ä½“è¡Œä¸º(Agentic Behavior)å’Œä¸Šä¸‹æ–‡æ¨ç†(Contextual Reasoning)ç ”ç©¶æä¾›äº†ä¸€ä¸ªå¯é‡å¤çš„æµ‹è¯•å¹³å°ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08501v2",
      "published_date": "2025-08-11 22:17:07 UTC",
      "updated_date": "2025-11-08 02:07:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:52:17.560977+00:00"
    },
    {
      "arxiv_id": "2508.08500v1",
      "title": "Large Language Models as Oracles for Ontology Alignment",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºæœ¬ä½“å¯¹é½çš„é¢„è¨€æœº",
      "authors": [
        "Sviatoslav Lushnei",
        "Dmytro Shumskyi",
        "Severyn Shykula",
        "Ernesto Jimenez-Ruiz",
        "Artur d'Avila Garcez"
      ],
      "abstract": "Ontology alignment plays a crucial role in integrating diverse data sources across domains. There is a large plethora of systems that tackle the ontology alignment problem, yet challenges persist in producing highly quality correspondences among a set of input ontologies. Human-in-the-loop during the alignment process is essential in applications requiring very accurate mappings. User involvement is, however, expensive when dealing with large ontologies. In this paper, we explore the feasibility of using Large Language Models (LLM) as an alternative to the domain expert. The use of the LLM focuses only on the validation of the subset of correspondences where an ontology alignment system is very uncertain. We have conducted an extensive evaluation over several matching tasks of the Ontology Alignment Evaluation Initiative (OAEI), analysing the performance of several state-of-the-art LLMs using different ontology-driven prompt templates. The LLM results are also compared against simulated Oracles with variable error rates.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨æœ¬ä½“å¯¹é½(Ontology alignment)è¿‡ç¨‹ä¸­ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)ä½œä¸ºé¢†åŸŸä¸“å®¶æ›¿ä»£æ–¹æ¡ˆçš„å¯è¡Œæ€§ï¼Œæ—¨åœ¨è§£å†³é«˜è´¨é‡å¯¹é½ä»»åŠ¡ä¸­äººå·¥æ ¡éªŒæˆæœ¬é«˜æ˜‚çš„éš¾é¢˜ã€‚ç ”ç©¶å°†LLMçš„åº”ç”¨èšç„¦äºéªŒè¯è‡ªåŠ¨åŒ–ç³»ç»Ÿä¸ç¡®å®šæ€§è¾ƒé«˜çš„å¯¹åº”å…³ç³»å­é›†ï¼Œé€šè¿‡åœ¨æœ¬ä½“å¯¹é½è¯„ä¼°å€¡è®®(OAEI)çš„å¤šä¸ªåŒ¹é…ä»»åŠ¡ä¸­è¿›è¡Œå¹¿æ³›è¯„ä¼°ï¼Œå¯¹æ¯”äº†å¤šç§å…ˆè¿›LLMsåœ¨ä¸åŒæœ¬ä½“é©±åŠ¨æç¤ºæ¨¡æ¿(Prompt templates)ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å°†LLMçš„éªŒè¯ç»“æœä¸å…·æœ‰ä¸åŒé”™è¯¯ç‡çš„æ¨¡æ‹Ÿå…ˆçŸ¥(Oracles)è¿›è¡Œäº†æ·±å…¥å¯¹æ¯”åˆ†æã€‚å®éªŒç»“æœå±•ç¤ºäº†LLMåœ¨å¤„ç†å¤æ‚æ•°æ®é›†æˆä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºåœ¨å‡å°‘äººåŠ›æˆæœ¬çš„åŒæ—¶æé«˜æœ¬ä½“å¯¹é½ç²¾åº¦æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Submitted to a conference. 17 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.08500v1",
      "published_date": "2025-08-11 22:16:20 UTC",
      "updated_date": "2025-08-11 22:16:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:52:20.556076+00:00"
    },
    {
      "arxiv_id": "2508.10042v1",
      "title": "FIDELIS: Blockchain-Enabled Protection Against Poisoning Attacks in Federated Learning",
      "title_zh": "FIDELISï¼šè”é‚¦å­¦ä¹ ä¸­åŸºäºåŒºå—é“¾çš„æŠ•æ¯’æ”»å‡»é˜²å¾¡",
      "authors": [
        "Jane Carney",
        "Kushal Upreti",
        "Gaby G. Dagher",
        "Tim Andersen"
      ],
      "abstract": "Federated learning enhances traditional deep learning by enabling the joint training of a model with the use of IoT device's private data. It ensures privacy for clients, but is susceptible to data poisoning attacks during training that degrade model performance and integrity. Current poisoning detection methods in federated learning lack a standardized detection method or take significant liberties with trust. In this paper, we present \\Sys, a novel blockchain-enabled poison detection framework in federated learning. The framework decentralizes the role of the global server across participating clients. We introduce a judge model used to detect data poisoning in model updates. The judge model is produced by each client and verified to reach consensus on a single judge model. We implement our solution to show \\Sys is robust against data poisoning attacks and the creation of our judge model is scalable.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è”é‚¦å­¦ä¹ (Federated Learning)åœ¨ä¿æŠ¤éšç§çš„åŒæ—¶æ˜“å—æ•°æ®æŠ•æ¯’æ”»å‡»(Data Poisoning Attacks)å½±å“æ€§èƒ½çš„é—®é¢˜ï¼Œæå‡ºäº†åŸºäºåŒºå—é“¾(Blockchain)çš„é˜²å¾¡æ¡†æ¶FIDELISã€‚è¯¥æ¡†æ¶é€šè¿‡å°†å…¨å±€æœåŠ¡å™¨çš„è§’è‰²å»ä¸­å¿ƒåŒ–(Decentralization)åˆ°å„å‚ä¸å®¢æˆ·ç«¯ï¼Œæœ‰æ•ˆè§£å†³äº†ç°æœ‰æ£€æµ‹æ–¹æ³•å¯¹å•ä¸€ä¿¡ä»»æºè¿‡åº¦ä¾èµ–çš„å±€é™æ€§ã€‚ç ”ç©¶æ ¸å¿ƒåœ¨äºå¼•å…¥äº†ä¸€ç§ç”¨äºæ£€æµ‹æ¨¡å‹æ›´æ–°ä¸­æŠ•æ¯’è¡Œä¸ºçš„åˆ¤åˆ«æ¨¡å‹(Judge Model)ï¼Œè¯¥æ¨¡å‹ç”±å„å®¢æˆ·ç«¯ç”Ÿæˆå¹¶é€šè¿‡å…±è¯†æœºåˆ¶(Consensus)è¿›è¡ŒéªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFIDELISåœ¨å¯¹æŠ—æ•°æ®æŠ•æ¯’æ”»å‡»æ–¹é¢å…·æœ‰æå¼ºçš„é²æ£’æ€§(Robustness)ã€‚æ­¤å¤–ï¼Œåˆ¤åˆ«æ¨¡å‹(Judge Model)çš„æ„å»ºè¿‡ç¨‹å±•ç°å‡ºè‰¯å¥½çš„å¯æ‰©å±•æ€§(Scalability)ï¼Œä¸ºå®ç°å®‰å…¨ã€å»ä¸­å¿ƒåŒ–çš„åä½œæœºå™¨å­¦ä¹ æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10042v1",
      "published_date": "2025-08-11 22:12:27 UTC",
      "updated_date": "2025-08-11 22:12:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:52:19.754686+00:00"
    },
    {
      "arxiv_id": "2508.09223v1",
      "title": "Hierarchical Adaptive networks with Task vectors for Test-Time Adaptation",
      "title_zh": "é¢å‘æµ‹è¯•æ—¶è‡ªé€‚åº”çš„èåˆä»»åŠ¡å‘é‡çš„åˆ†å±‚è‡ªé€‚åº”ç½‘ç»œ",
      "authors": [
        "Sameer Ambekar",
        "Daniel M. Lang",
        "Julia A. Schnabel"
      ],
      "abstract": "Test-time adaptation allows pretrained models to adjust to incoming data streams, addressing distribution shifts between source and target domains. However, standard methods rely on single-dimensional linear classification layers, which often fail to handle diverse and complex shifts. We propose Hierarchical Adaptive Networks with Task Vectors (Hi-Vec), which leverages multiple layers of increasing size for dynamic test-time adaptation. By decomposing the encoder's representation space into such hierarchically organized layers, Hi-Vec, in a plug-and-play manner, allows existing methods to adapt to shifts of varying complexity. Our contributions are threefold: First, we propose dynamic layer selection for automatic identification of the optimal layer for adaptation to each test batch. Second, we propose a mechanism that merges weights from the dynamic layer to other layers, ensuring all layers receive target information. Third, we propose linear layer agreement that acts as a gating function, preventing erroneous fine-tuning by adaptation on noisy batches. We rigorously evaluate the performance of Hi-Vec in challenging scenarios and on multiple target datasets, proving its strong capability to advance state-of-the-art methods. Our results show that Hi-Vec improves robustness, addresses uncertainty, and handles limited batch sizes and increased outlier rates.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Hi-Vec (Hierarchical Adaptive Networks with Task Vectors)ï¼Œæ—¨åœ¨è§£å†³é¢„è®­ç»ƒæ¨¡å‹åœ¨æµ‹è¯•æ—¶è‡ªé€‚åº” (Test-time adaptation) è¿‡ç¨‹ä¸­ï¼Œå› ä¾èµ–å•ä¸€çº¿æ€§åˆ†ç±»å±‚è€Œéš¾ä»¥åº”å¯¹å¤šæ ·åŒ–å¤æ‚åˆ†å¸ƒåç§»çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å±‚çº§åŒ–ç»„ç»‡å¤šä¸ªè§„æ¨¡é€’å¢çš„å±‚ï¼Œä»¥å³æ’å³ç”¨ (plug-and-play) çš„æ–¹å¼å¢å¼ºç°æœ‰æ–¹æ³•å¯¹ä¸åŒå¤æ‚åº¦åç§»çš„é€‚åº”èƒ½åŠ›ã€‚ç ”ç©¶çš„æ ¸å¿ƒè´¡çŒ®åŒ…æ‹¬ï¼šä¸€æ˜¯å®ç°äº†åŠ¨æ€å±‚é€‰æ‹© (dynamic layer selection) ä»¥è‡ªåŠ¨è¯†åˆ«æ¯ä¸ªæµ‹è¯•æ‰¹æ¬¡çš„æœ€ä½³è‡ªé€‚åº”å±‚ï¼›äºŒæ˜¯æå‡ºäº†æƒé‡åˆå¹¶æœºåˆ¶ï¼Œç¡®ä¿æ‰€æœ‰å±‚çº§å‡èƒ½æœ‰æ•ˆè·å–ç›®æ ‡åŸŸä¿¡æ¯ï¼›ä¸‰æ˜¯å¼•å…¥çº¿æ€§å±‚ä¸€è‡´æ€§ (linear layer agreement) ä½œä¸ºé—¨æ§å‡½æ•°ï¼Œé˜²æ­¢åœ¨å™ªå£°æ‰¹æ¬¡ä¸Šè¿›è¡Œé”™è¯¯çš„å¾®è°ƒã€‚å®éªŒè¯æ˜ï¼ŒHi-Vec åœ¨å¤„ç†æœ‰é™æ‰¹æ¬¡å¤§å°å’Œé«˜ç¦»ç¾¤å€¼ç‡ç­‰æŒ‘æˆ˜æ€§åœºæ™¯æ—¶è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æå‡äº†ç°æœ‰ state-of-the-art æ–¹æ³•çš„é²æ£’æ€§ä¸ä¸ç¡®å®šæ€§å¤„ç†èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.09223v1",
      "published_date": "2025-08-11 21:55:53 UTC",
      "updated_date": "2025-08-11 21:55:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:53:15.693595+00:00"
    },
    {
      "arxiv_id": "2508.08493v1",
      "title": "POMO+: Leveraging starting nodes in POMO for solving Capacitated Vehicle Routing Problem",
      "title_zh": "POMO+ï¼šåˆ©ç”¨ POMO ä¸­çš„èµ·å§‹èŠ‚ç‚¹æ±‚è§£å¸¦å®¹é‡é™åˆ¶çš„è½¦è¾†è·¯å¾„é—®é¢˜",
      "authors": [
        "Szymon Jakubicz",
        "Karol KuÅºniak",
        "Jan Wawszczak",
        "PaweÅ‚ Gora"
      ],
      "abstract": "In recent years, reinforcement learning (RL) methods have emerged as a promising approach for solving combinatorial problems. Among RL-based models, POMO has demonstrated strong performance on a variety of tasks, including variants of the Vehicle Routing Problem (VRP). However, there is room for improvement for these tasks. In this work, we improved POMO, creating a method (\\textbf{POMO+}) that leverages the initial nodes to find a solution in a more informed way. We ran experiments on our new model and observed that our solution converges faster and achieves better results. We validated our models on the CVRPLIB dataset and noticed improvements in problem instances with up to 100 customers. We hope that our research in this project can lead to further advancements in the field.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¸¦å®¹é‡é™åˆ¶çš„è½¦è¾†è·¯å¾„é—®é¢˜ (Capacitated Vehicle Routing Problem, CVRP) æå‡ºäº† POMO+ï¼Œè¿™æ˜¯å¯¹ç°æœ‰å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) æ¨¡å‹ POMO çš„æ”¹è¿›æ–¹æ¡ˆã€‚POMO+ é€šè¿‡æœ‰æ•ˆåˆ©ç”¨åˆå§‹èŠ‚ç‚¹ (initial nodes) ä¿¡æ¯ï¼Œå¼•å¯¼æ¨¡å‹ä»¥æ›´å…·å¯å‘æ€§çš„æ–¹å¼æ¢ç´¢è§£ç©ºé—´å¹¶å¯»æ‰¾æœ€ä¼˜è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ”¶æ•›é€Ÿåº¦å’Œæœ€ç»ˆè§£çš„è´¨é‡ä¸Šå‡ä¼˜äºåŸå§‹çš„ POMO æ¨¡å‹ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨ CVRPLIB æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œç»“æœæ˜¾ç¤ºåœ¨å¤šè¾¾ 100 ä¸ªå®¢æˆ·èŠ‚ç‚¹çš„å®ä¾‹ä¸­å‡å®ç°äº†æ€§èƒ½æå‡ã€‚è¯¥å·¥ä½œè¯æ˜äº†é€šè¿‡ä¼˜åŒ–èµ·å§‹èŠ‚ç‚¹çš„å¤„ç†æœºåˆ¶å¯ä»¥æ˜¾è‘—å¢å¼ºç¥ç»ç»„åˆä¼˜åŒ–æ¨¡å‹çš„æ±‚è§£æ•ˆç‡ï¼Œä¸ºè§£å†³å¤æ‚çš„è½¦è¾†è·¯å¾„é—®é¢˜æä¾›äº†æ–°çš„æ”¹è¿›æ–¹å‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08493v1",
      "published_date": "2025-08-11 21:55:16 UTC",
      "updated_date": "2025-08-11 21:55:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:53:16.084149+00:00"
    },
    {
      "arxiv_id": "2508.13176v1",
      "title": "Fitting Ontologies and Constraints to Relational Structures",
      "title_zh": "é¢å‘å…³ç³»ç»“æ„çš„æœ¬ä½“ä¸çº¦æŸæ‹Ÿåˆ",
      "authors": [
        "Simon Hosemann",
        "Jean Christoph Jung",
        "Carsten Lutz",
        "Sebastian Rudolph"
      ],
      "abstract": "We study the problem of fitting ontologies and constraints to positive and negative examples that take the form of a finite relational structure. As ontology and constraint languages, we consider the description logics $\\mathcal{E\\mkern-2mu L}$ and $\\mathcal{E\\mkern-2mu LI}$ as well as several classes of tuple-generating dependencies (TGDs): full, guarded, frontier-guarded, frontier-one, and unrestricted TGDs as well as inclusion dependencies. We pinpoint the exact computational complexity, design algorithms, and analyze the size of fitting ontologies and TGDs. We also investigate the related problem of constructing a finite basis of concept inclusions / TGDs for a given set of finite structures. While finite bases exist for $\\mathcal{E\\mkern-2mu L}$, $\\mathcal{E\\mkern-2mu LI}$, guarded TGDs, and inclusion dependencies, they in general do not exist for full, frontier-guarded and frontier-one TGDs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å°† Ontologies å’Œ Constraints æ‹Ÿåˆåˆ°ä»¥æœ‰é™å…³ç³»ç»“æ„å½¢å¼å‘ˆç°çš„æ­£è´Ÿç¤ºä¾‹ä¸­çš„é—®é¢˜ã€‚ç ”ç©¶èŒƒå›´æ¶‰åŠæè¿°é€»è¾‘ $\\mathcal{E\\mkern-2mu L}$ ä¸ $\\mathcal{E\\mkern-2mu LI}$ï¼Œä»¥åŠ full, guarded, frontier-guarded, frontier-one, unrestricted TGDs å’Œ inclusion dependencies ç­‰å¤šç§ Tuple-generating dependencies (TGDs) ç±»åˆ«ã€‚ä½œè€…ç»™å‡ºäº†æ‹Ÿåˆé—®é¢˜çš„ç²¾ç¡®è®¡ç®—å¤æ‚åº¦ï¼Œè®¾è®¡äº†ç›¸å…³ç®—æ³•ï¼Œå¹¶å¯¹ç”Ÿæˆçš„ Ontologies å’Œ TGDs è§„æ¨¡è¿›è¡Œäº†è¯¦ç»†åˆ†æã€‚æ­¤å¤–ï¼Œè¯¥å·¥ä½œæ·±å…¥æ¢è®¨äº†ä¸ºç‰¹å®šæœ‰é™ç»“æ„é›†æ„å»ºæ¦‚å¿µåŒ…å«æˆ– TGDs çš„ finite basis çš„å¯èƒ½æ€§ã€‚ç»“è®ºæŒ‡å‡ºï¼Œè™½ç„¶ $\\mathcal{E\\mkern-2mu L}$ã€$\\mathcal{E\\mkern-2mu LI}$ã€guarded TGDs å’Œ inclusion dependencies æ™®éå­˜åœ¨ finite basisï¼Œä½† full, frontier-guarded å’Œ frontier-one TGDs é€šå¸¸ä¸å…·å¤‡è¿™ä¸€æ€§è´¨ã€‚",
      "categories": [
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at the 22nd International Conference on Principles of Knowledge Representation and Reasoning (KR 2025)",
      "pdf_url": "https://arxiv.org/pdf/2508.13176v1",
      "published_date": "2025-08-11 21:52:58 UTC",
      "updated_date": "2025-08-11 21:52:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:53:22.097123+00:00"
    },
    {
      "arxiv_id": "2508.08492v1",
      "title": "Momentum Point-Perplexity Mechanics in Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­çš„åŠ¨é‡-ç‚¹å›°æƒ‘åº¦åŠ›å­¦",
      "authors": [
        "Lorenzo Tomaz",
        "Judd Rosenblatt",
        "Thomas Berry Jones",
        "Diogo Schwerz de Lucena"
      ],
      "abstract": "We take a physics-based approach to studying how the internal hidden states of large language models change from token to token during inference. Across 20 open-source transformer models (135M-3B parameters), we find that a quantity combining the rate of change in hidden states and the model's next-token certainty, analogous to energy in physics, remains nearly constant. Random-weight models conserve this \"energy\" more tightly than pre-trained ones, while training shifts models into a faster, more decisive regime with greater variability. Using this \"log-Lagrangian\" view, we derive a control method called Jacobian steering, which perturbs hidden states in the minimal way needed to favor a target token. This approach maintained near-constant energy in two tested models and produced continuations rated higher in semantic quality than the models' natural outputs. Viewing transformers through this mechanics lens offers a principled basis for interpretability, anomaly detection, and low-risk steering. This could help make powerful models more predictable and aligned with human intent.",
      "tldr_zh": "è¯¥ç ”ç©¶é‡‡ç”¨ç‰©ç†å­¦æ–¹æ³•æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(Large Language Models)åœ¨æ¨ç†è¿‡ç¨‹ä¸­å†…éƒ¨éšè—çŠ¶æ€(Hidden States)çš„åŠ¨æ€æ¼”å˜ã€‚é€šè¿‡å¯¹20ä¸ªå¼€æºTransformeræ¨¡å‹çš„å®éªŒåˆ†æï¼Œç ”ç©¶è€…å‘ç°ç”±éšè—çŠ¶æ€å˜åŒ–ç‡ä¸æ¨¡å‹ä¸‹æ–‡é¢„æµ‹ç¡®å®šæ€§ç»„åˆè€Œæˆçš„ç‰©ç†é‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­è¿‘ä¹å®ˆæ’ã€‚å®éªŒè¡¨æ˜ï¼Œè®­ç»ƒè¿‡ç¨‹ä¼šä½¿æ¨¡å‹è¿›å…¥æ›´å¿«é€Ÿä¸”æœæ–­çš„è¿è¡Œæ¨¡å¼ï¼Œè€Œéšæœºæƒé‡æ¨¡å‹åˆ™æ¯”é¢„è®­ç»ƒæ¨¡å‹æ›´ä¸¥è°¨åœ°éµå®ˆè¿™ç§â€œèƒ½é‡å®ˆæ’â€ã€‚åŸºäºè¿™ç§â€œå¯¹æ•°æ‹‰æ ¼æœ—æ—¥â€(Log-Lagrangian)è§†è§’ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºJacobian steeringçš„æ§åˆ¶æ–¹æ³•ï¼Œé€šè¿‡å¯¹éšè—çŠ¶æ€è¿›è¡Œæœ€å°åŒ–å¾®æ‰°æ¥å¼•å¯¼ç›®æ ‡Tokençš„ç”Ÿæˆã€‚æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ç»´æŒèƒ½é‡å¸¸æ•°çš„åŒæ—¶ï¼Œäº§ç”Ÿçš„æ–‡æœ¬è¯­ä¹‰è´¨é‡ä¼˜äºæ¨¡å‹çš„è‡ªç„¶è¾“å‡ºã€‚è¿™ä¸€åŠ›å­¦æ¡†æ¶ä¸ºæ¨¡å‹çš„è§£é‡Šæ€§(Interpretability)ã€å¼‚å¸¸æ£€æµ‹å’Œä½é£é™©å¼•å¯¼æä¾›äº†åŸç†æ”¯æ’‘ï¼Œæœ‰åŠ©äºå¢å¼ºå¤§æ¨¡å‹çš„å¯é¢„æµ‹æ€§åŠå…¶ä¸äººç±»æ„å›¾çš„å¯¹é½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08492v1",
      "published_date": "2025-08-11 21:50:34 UTC",
      "updated_date": "2025-08-11 21:50:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:53:20.892774+00:00"
    },
    {
      "arxiv_id": "2508.08487v4",
      "title": "MAViS: A Multi-Agent Framework for Long-Sequence Video Storytelling",
      "title_zh": "MAViSï¼šé¢å‘é•¿åºåˆ—è§†é¢‘å™äº‹çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Qian Wang",
        "Ziqi Huang",
        "Ruoxi Jia",
        "Paul Debevec",
        "Ning Yu"
      ],
      "abstract": "Despite recent advances, long-sequence video generation frameworks still suffer from significant limitations: poor assistive capability, suboptimal visual quality, and limited expressiveness. To mitigate these limitations, we propose MAViS, a multi-agent collaborative framework designed to assist in long-sequence video storytelling by efficiently translating ideas into visual narratives. MAViS orchestrates specialized agents across multiple stages, including script writing, shot designing, character modeling, keyframe generation, video animation, and audio generation. In each stage, agents operate under the 3E Principle -- Explore, Examine, and Enhance -- to ensure the completeness of intermediate outputs. Considering the capability limitations of current generative models, we propose the Script Writing Guidelines to optimize compatibility between scripts and generative tools. Experimental results demonstrate that MAViS achieves state-of-the-art performance in assistive capability, visual quality, and video expressiveness. Its modular framework further enables scalability with diverse generative models and tools. With just a brief idea description, MAViS enables users to rapidly explore diverse visual storytelling and creative directions for sequential video generation by efficiently producing high-quality, complete long-sequence videos. To the best of our knowledge, MAViS is the only framework that provides multimodal design output -- videos with narratives and background music.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MAViSï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³é•¿åºåˆ—è§†é¢‘ç”Ÿæˆ(long-sequence video generation)ä¸­è¾…åŠ©èƒ½åŠ›å·®ã€è§†è§‰è´¨é‡ä½åŠè¡¨ç°åŠ›æœ‰é™ç­‰é—®é¢˜çš„å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ç¼–æ’ä¸“é—¨çš„æ™ºèƒ½ä½“æ¥ååŒå®Œæˆå‰§æœ¬ç¼–å†™(script writing)ã€é•œå¤´è®¾è®¡(shot designing)ã€è§’è‰²å»ºæ¨¡ã€å…³é”®å¸§ç”Ÿæˆã€è§†é¢‘åŠ¨ç”»åŠéŸ³é¢‘ç”Ÿæˆç­‰å¤šä¸ªé˜¶æ®µçš„ä»»åŠ¡ã€‚åœ¨æ¯ä¸ªé˜¶æ®µï¼Œæ™ºèƒ½ä½“éµå¾ª Explore, Examine, and Enhance çš„ 3E åŸåˆ™ï¼Œä»¥ç¡®ä¿ä¸­é—´è¾“å‡ºç»“æœçš„å®Œæ•´æ€§ã€‚é’ˆå¯¹å½“å‰ç”Ÿæˆæ¨¡å‹çš„å±€é™æ€§ï¼Œç ”ç©¶è€…è¿˜æå‡ºäº† Script Writing Guidelines ä»¥ä¼˜åŒ–å‰§æœ¬ä¸ç”Ÿæˆå·¥å…·ä¹‹é—´çš„å…¼å®¹æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMAViS åœ¨è¾…åŠ©èƒ½åŠ›ã€è§†è§‰è´¨é‡å’Œè§†é¢‘è¡¨ç°åŠ›æ–¹é¢è¾¾åˆ°äº† state-of-the-art æ°´å¹³ï¼Œå¹¶å±•ç°å‡ºè‰¯å¥½çš„æ¨¡å—åŒ–æ‰©å±•æ€§ã€‚ä½œä¸ºç›®å‰å”¯ä¸€æä¾›å¤šæ¨¡æ€è®¾è®¡è¾“å‡ºçš„æ¡†æ¶ï¼ŒMAViS èƒ½å¤Ÿä»…å‡­ç®€å•çš„åˆ›æ„æè¿°ä¾¿å¿«é€Ÿç”ŸæˆåŒ…å«å™è¿°å’ŒèƒŒæ™¯éŸ³ä¹çš„é«˜è´¨é‡ã€å®Œæ•´é•¿åºåˆ—è§†é¢‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CV",
      "comment": "Video Generation Agent",
      "pdf_url": "https://arxiv.org/pdf/2508.08487v4",
      "published_date": "2025-08-11 21:42:41 UTC",
      "updated_date": "2025-10-09 03:46:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:53:25.942719+00:00"
    },
    {
      "arxiv_id": "2508.08486v1",
      "title": "Beyond Ordinal Preferences: Why Alignment Needs Cardinal Human Feedback",
      "title_zh": "è¶…è¶Šåºæ•°åå¥½ï¼šä¸ºä½•å¯¹é½éœ€è¦åŸºæ•°äººç±»åé¦ˆ",
      "authors": [
        "Parker Whitfill",
        "Stewy Slocum"
      ],
      "abstract": "Alignment techniques for LLMs rely on optimizing preference-based objectives -- where these preferences are typically elicited as ordinal, binary choices between responses. Recent work has focused on improving label quality or mitigating particular biases, but we identify a more fundamental limitation: these methods collect the wrong kind of data. We prove an impossibility result: no algorithm relying solely on ordinal comparisons can systematically recover the most preferred model. Intuitively, ordinal data lacks the information needed to resolve tradeoffs -- e.g., fixing a factual error on one prompt versus improving style on another. We show that selecting the optimal model requires recovering preferences over \\emph{models} (rather than just responses), which can only be identified given cardinal feedback about response quality. To address this, we collect and publicly release a dataset of 25,000 cardinal judgments using willingness-to-pay elicitations, a well-established tool from experimental economics. Empirically, we find that incorporating cardinal feedback into preference fine-tuning allows models to prioritize high-impact improvements and outperform ordinal-only methods on downstream benchmarks, such as Arena-Hard.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)å¯¹é½æŠ€æœ¯ä¸­å¸¸ç”¨çš„åºæ•°åå¥½(ordinal preferences)ç›®æ ‡çš„å±€é™æ€§ï¼ŒæŒ‡å‡ºä»…ä¾èµ–äºŒå…ƒé€‰æ‹©æ— æ³•ç³»ç»Ÿæ€§åœ°æ¢å¤æœ€å—é’ççš„æ¨¡å‹ã€‚ä½œè€…é€šè¿‡ç†è®ºè¯æ˜ï¼Œåºæ•°æ•°æ®(ordinal data)ç¼ºä¹è§£å†³ä¸åŒæç¤ºè¯ä¹‹é—´æ”¹è¿›æƒè¡¡ï¼ˆå¦‚ä¿®å¤äº‹å®é”™è¯¯ä¸æå‡é£æ ¼è¡¨ç°ï¼‰æ‰€éœ€çš„ä¿¡æ¯ï¼Œå› æ­¤æ¨¡å‹å¯¹é½éœ€è¦å…³äºå“åº”è´¨é‡çš„åŸºæ•°åé¦ˆ(cardinal feedback)ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…å€Ÿé‰´å®éªŒç»æµå­¦ä¸­çš„æ„¿æ„æ”¯ä»˜(willingness-to-pay)å¯å‘æ³•ï¼Œæ”¶é›†å¹¶å…¬å¼€å‘å¸ƒäº†åŒ…å«25,000ä¸ªåŸºæ•°åˆ¤æ–­çš„æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨åå¥½å¾®è°ƒä¸­å¼•å…¥åŸºæ•°åé¦ˆèƒ½ä½¿æ¨¡å‹ä¼˜å…ˆå¤„ç†é«˜å½±å“åŠ›çš„æ”¹è¿›ã€‚æœ€ç»ˆï¼Œè¯¥æ–¹æ³•åœ¨Arena-Hardç­‰ä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—ä¼˜äºä»…ä¾èµ–åºæ•°åé¦ˆçš„ä¼ ç»Ÿæ–¹æ³•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08486v1",
      "published_date": "2025-08-11 21:42:33 UTC",
      "updated_date": "2025-08-11 21:42:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:53:32.383877+00:00"
    },
    {
      "arxiv_id": "2508.11689v1",
      "title": "Adaptive Spiking with Plasticity for Energy Aware Neuromorphic Systems",
      "title_zh": "é¢å‘èƒ½è€—æ„ŸçŸ¥ç¥ç»å½¢æ€ç³»ç»Ÿçš„è‡ªé€‚åº”å¯å¡‘æ€§è„‰å†²æŠ€æœ¯",
      "authors": [
        "Eduardo Calle-Ortiz",
        "Hui Guan",
        "Deepak Ganesan",
        "Phuc Nguyen"
      ],
      "abstract": "This paper presents ASPEN, a novel energy-aware technique for neuromorphic systems that could unleash the future of intelligent, always-on, ultra-low-power, and low-burden wearables. Our main research objectives are to explore the feasibility of neuromorphic computing for wearables, identify open research directions, and demonstrate the feasibility of developing an adaptive spiking technique for energy-aware computation, which can be game-changing for resource-constrained devices in always-on applications. As neuromorphic computing systems operate based on spike events, their energy consumption is closely related to spiking activity, i.e., each spike incurs computational and power costs; consequently, minimizing the number of spikes is a critical strategy for operating under constrained energy budgets. To support this goal, ASPEN utilizes stochastic perturbations to the neuronal threshold during training to not only enhance the network's robustness across varying thresholds, which can be controlled at inference time, but also act as a regularizer that improves generalization, reduces spiking activity, and enables energy control without the need for complex retraining or pruning. More specifically, ASPEN adaptively adjusts intrinsic neuronal parameters as a lightweight and scalable technique for dynamic energy control without reconfiguring the entire model. Our evaluation on neuromorphic emulator and hardware shows that ASPEN significantly reduces spike counts and energy consumption while maintaining accuracy comparable to state-of-the-art methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ASPENï¼Œä¸€ç§é’ˆå¯¹Neuromorphic Systemsçš„æ–°å‹èƒ½æ•ˆæ„ŸçŸ¥æŠ€æœ¯ï¼Œæ—¨åœ¨ä¸ºè¶…ä½åŠŸè€—ã€Always-onçš„å¯ç©¿æˆ´è®¾å¤‡æä¾›å…³é”®æ”¯æŒã€‚ä¸ºäº†åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šä¼˜åŒ–åŠŸè€—ï¼ŒASPENä¸“æ³¨äºæœ€å°åŒ–ä½œä¸ºä¸»è¦èƒ½é‡æ¶ˆè€—æ¥æºçš„Spiking Activityã€‚è¯¥æ–¹æ³•åœ¨è®­ç»ƒé˜¶æ®µå¯¹ç¥ç»å…ƒé˜ˆå€¼å¼•å…¥Stochastic Perturbationsï¼Œä¸ä»…å¢å¼ºäº†ç½‘ç»œå¯¹ä¸åŒé˜ˆå€¼çš„ç¨³å¥æ€§ï¼Œè¿˜ä½œä¸ºä¸€ç§Regularizeræå‡äº†æ³›åŒ–èƒ½åŠ›å¹¶æœ‰æ•ˆå‡å°‘è„‰å†²æ•°é‡ã€‚è¿™ç§è½»é‡çº§ä¸”å¯æ‰©å±•çš„æŠ€æœ¯å…è®¸é€šè¿‡è°ƒæ•´å†…åœ¨ç¥ç»å…ƒå‚æ•°å®ç°åŠ¨æ€èƒ½é‡æ§åˆ¶ï¼Œè€Œæ— éœ€å¤æ‚çš„Retrainingæˆ–Pruningã€‚åœ¨Neuromorphicç¡¬ä»¶ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒASPENåœ¨æ˜¾è‘—é™ä½è„‰å†²è®¡æ•°å’Œèƒ½é‡æ¶ˆè€—çš„åŒæ—¶ï¼Œä¿æŒäº†ä¸State-of-the-artæ–¹æ³•ç›¸å½“çš„å‡†ç¡®ç‡ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.NE",
      "comment": "14 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.11689v1",
      "published_date": "2025-08-11 21:25:17 UTC",
      "updated_date": "2025-08-11 21:25:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:53:42.991066+00:00"
    },
    {
      "arxiv_id": "2508.08477v3",
      "title": "A Fast GRASP Metaheuristic for the Trigger Arc TSP with MIP-Based Construction and Multi-Neighborhood Local Search",
      "title_zh": "é’ˆå¯¹è§¦å‘å¼§ TSP çš„å¿«é€Ÿ GRASP å…ƒå¯å‘å¼ç®—æ³•ï¼šèåˆåŸºäº MIP çš„æ„é€ ä¸å¤šé‚»åŸŸå±€éƒ¨æœç´¢",
      "authors": [
        "Joan SalvÃ  Soler",
        "GrÃ©goire de Lambertye"
      ],
      "abstract": "The Trigger Arc Traveling Salesman Problem (TA-TSP) extends the classical TSP by introducing dynamic arc costs that change when specific \"trigger\" arcs are traversed, modeling scenarios such as warehouse operations with compactable storage systems. This paper introduces a GRASP-based metaheuristic that combines multiple construction heuristics with a multi-neighborhood local search. The construction phase uses mixed-integer programming (MIP) techniques to transform the TA-TSP into a sequence of tailored TSP instances, while the improvement phase applies 2-Opt, Swap, and Relocate operators. Computational experiments on MESS 2024 competition instances achieved average optimality gaps of 0.77% and 0.40% relative to the best-known solutions within a 60-second limit. On smaller, synthetically generated datasets, the method produced solutions 11.3% better than the Gurobi solver under the same time constraints. The algorithm finished in the top three at MESS 2024, demonstrating its suitability for real-time routing applications with state-dependent travel costs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Trigger Arc Traveling Salesman Problem (TA-TSP)æå‡ºäº†ä¸€ç§å¿«é€Ÿçš„GRASPå…ƒå¯å‘å¼ç®—æ³•ï¼Œæ—¨åœ¨è§£å†³ç”±äºè§¦å‘å¼§å¯¼è‡´å¼§æˆæœ¬åŠ¨æ€å˜åŒ–çš„è·¯å¾„ä¼˜åŒ–é—®é¢˜ã€‚åœ¨æ„é€ é˜¶æ®µï¼Œç®—æ³•åˆ©ç”¨Mixed-Integer Programming (MIP)æŠ€æœ¯å°†TA-TSPè½¬åŒ–ä¸ºä¸€ç³»åˆ—å®šåˆ¶åŒ–çš„TSPå®ä¾‹åºåˆ—ã€‚æ”¹è¿›é˜¶æ®µåˆ™åº”ç”¨äº†åŒ…å«2-Optã€Swapå’ŒRelocateç®—å­çš„Multi-Neighborhood Local Searchç­–ç•¥ä»¥å¢å¼ºæœç´¢èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨MESS 2024ç«èµ›ç®—ä¾‹ä¸Šï¼Œåœ¨60ç§’æ—¶é—´é™åˆ¶å†…å®ç°äº†ä»…0.77%å’Œ0.40%çš„å¹³å‡æœ€ä¼˜æ€§é—´éš™ã€‚åœ¨åˆæˆæ•°æ®é›†ä¸Šï¼Œå…¶è¡¨ç°æ¯”åŒç­‰æ—¶é—´é™åˆ¶ä¸‹çš„Gurobiæ±‚è§£å™¨ä¼˜11.3%ã€‚è¯¥ç®—æ³•æœ€ç»ˆåœ¨MESS 2024ç«èµ›ä¸­ä½åˆ—å‰ä¸‰ï¼Œè¯æ˜äº†å…¶åœ¨å…·æœ‰çŠ¶æ€ç›¸å…³æ—…è¡Œæˆæœ¬çš„å®æ—¶è·¯ç”±åœºæ™¯ä¸­çš„é«˜æ•ˆæ€§ä¸å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.DM"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 2 figures. Find the implementation in https://github.com/jsalvasoler/trigger_arc_tsp",
      "pdf_url": "https://arxiv.org/pdf/2508.08477v3",
      "published_date": "2025-08-11 21:24:38 UTC",
      "updated_date": "2025-10-07 15:17:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:53:47.961221+00:00"
    },
    {
      "arxiv_id": "2508.08467v1",
      "title": "Empowering Children to Create AI-Enabled Augmented Reality Experiences",
      "title_zh": "èµ‹èƒ½å„¿ç«¥åˆ›ä½œäººå·¥æ™ºèƒ½é©±åŠ¨çš„å¢å¼ºç°å®ä½“éªŒ",
      "authors": [
        "Lei Zhang",
        "Shuyao Zhou",
        "Amna Liaqat",
        "Tinney Mak",
        "Brian Berengard",
        "Emily Qian",
        "AndrÃ©s Monroy-HernÃ¡ndez"
      ],
      "abstract": "Despite their potential to enhance children's learning experiences, AI-enabled AR technologies are predominantly used in ways that position children as consumers rather than creators. We introduce Capybara, an AR-based and AI-powered visual programming environment that empowers children to create, customize, and program 3D characters overlaid onto the physical world. Capybara enables children to create virtual characters and accessories using text-to-3D generative AI models, and to animate these characters through auto-rigging and body tracking. In addition, our system employs vision-based AI models to recognize physical objects, allowing children to program interactive behaviors between virtual characters and their physical surroundings. We demonstrate the expressiveness of Capybara through a set of novel AR experiences. We conducted user studies with 20 children in the United States and Argentina. Our findings suggest that Capybara can empower children to harness AI in authoring personalized and engaging AR experiences that seamlessly bridge the virtual and physical worlds.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Capybaraï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº Augmented Reality (AR) ä¸”ç”± AI é©±åŠ¨çš„å¯è§†åŒ–ç¼–ç¨‹ç¯å¢ƒï¼Œæ—¨åœ¨è®©å„¿ç«¥ä» AI å†…å®¹çš„æ¶ˆè´¹è€…è½¬å˜ä¸ºåˆ›ä½œè€…ã€‚ç³»ç»Ÿé›†æˆäº† text-to-3D ç”Ÿæˆå¼ AI æ¨¡å‹ï¼Œå…è®¸å„¿ç«¥é€šè¿‡æ–‡æœ¬åˆ›å»ºå’Œå®šåˆ¶ 3D è§’è‰²åŠå…¶é…ä»¶ï¼Œå¹¶åˆ©ç”¨ auto-rigging å’Œ body tracking æŠ€æœ¯å®ç°åŠ¨ç”»åˆ¶ä½œã€‚æ­¤å¤–ï¼ŒCapybara åˆ©ç”¨åŸºäºè§†è§‰çš„ AI æ¨¡å‹è¯†åˆ«ç‰©ç†å¯¹è±¡ï¼Œæ”¯æŒå„¿ç«¥ç¼–å†™è™šæ‹Ÿè§’è‰²ä¸å‘¨å›´ç‰©ç†ç¯å¢ƒä¹‹é—´çš„äº¤äº’è¡Œä¸ºã€‚é€šè¿‡å¯¹ç¾å›½å’Œé˜¿æ ¹å»·çš„ 20 åå„¿ç«¥è¿›è¡Œç”¨æˆ·ç ”ç©¶ï¼Œç»“æœè¡¨æ˜è¯¥ç³»ç»Ÿèƒ½æœ‰æ•ˆèµ‹èƒ½å„¿ç«¥åˆ©ç”¨ AI åˆ›ä½œä¸ªæ€§åŒ–ä¸”å…·æ²‰æµ¸æ„Ÿçš„ AR ä½“éªŒã€‚è¯¥å·¥ä½œå±•ç¤ºäº† Capybara åœ¨æ— ç¼è¿æ¥è™šæ‹Ÿä¸ç‰©ç†ä¸–ç•Œæ–¹é¢çš„è¡¨è¾¾åŠ›ï¼Œä¸ºå„¿ç«¥å‚ä¸å‰æ²¿æŠ€æœ¯åˆ›ä½œæä¾›äº†æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.GR",
        "cs.PL"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted to ACM UIST 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.08467v1",
      "published_date": "2025-08-11 20:57:39 UTC",
      "updated_date": "2025-08-11 20:57:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:53:43.528298+00:00"
    },
    {
      "arxiv_id": "2508.08454v1",
      "title": "Temporal User Profiling with LLMs: Balancing Short-Term and Long-Term Preferences for Recommendations",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ—¶åºç”¨æˆ·ç”»åƒï¼šå¹³è¡¡æ¨èç³»ç»Ÿä¸­çš„é•¿çŸ­æœŸåå¥½",
      "authors": [
        "Milad Sabouri",
        "Masoud Mansoury",
        "Kun Lin",
        "Bamshad Mobasher"
      ],
      "abstract": "Accurately modeling user preferences is crucial for improving the performance of content-based recommender systems. Existing approaches often rely on simplistic user profiling methods, such as averaging or concatenating item embeddings, which fail to capture the nuanced nature of user preference dynamics, particularly the interactions between long-term and short-term preferences. In this work, we propose LLM-driven Temporal User Profiling (LLM-TUP), a novel method for user profiling that explicitly models short-term and long-term preferences by leveraging interaction timestamps and generating natural language representations of user histories using a large language model (LLM). These representations are encoded into high-dimensional embeddings using a pre-trained BERT model, and an attention mechanism is applied to dynamically fuse the short-term and long-term embeddings into a comprehensive user profile. Experimental results on real-world datasets demonstrate that LLM-TUP achieves substantial improvements over several baselines, underscoring the effectiveness of our temporally aware user-profiling approach and the use of semantically rich user profiles, generated by LLMs, for personalized content-based recommendation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LLM-driven Temporal User Profiling (LLM-TUP)ï¼Œä¸€ç§æ—¨åœ¨ä¼˜åŒ–åŸºäºå†…å®¹æ¨èç³»ç»Ÿç”¨æˆ·ç”»åƒçš„æ–°æ–¹æ³•ã€‚é’ˆå¯¹ä¼ ç»Ÿæ–¹æ³•éš¾ä»¥æ•æ‰ç”¨æˆ·é•¿çŸ­æœŸåå¥½åŠ¨æ€äº¤äº’çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ Large Language Model (LLM) ç»“åˆäº¤äº’æ—¶é—´æˆ³ç”Ÿæˆç”¨æˆ·å†å²çš„è‡ªç„¶è¯­è¨€è¡¨ç¤ºã€‚è¿™äº›è¯­ä¹‰ä¿¡æ¯éšåé€šè¿‡ BERT æ¨¡å‹è½¬åŒ–ä¸ºé«˜ç»´ embeddingsï¼Œå¹¶åˆ©ç”¨ attention mechanism åŠ¨æ€èåˆé•¿çŸ­æœŸåµŒå…¥ä»¥æ„å»ºå…¨é¢çš„ç”¨æˆ·ç”»åƒã€‚å®éªŒåœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤º LLM-TUP æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºçº¿æ¨¡å‹ã€‚è¿™ä¸€æˆæœå¼ºè°ƒäº†æ—¶é—´æ„ŸçŸ¥åŠ LLM ç”Ÿæˆçš„å¯Œè¯­ä¹‰ç”»åƒåœ¨æå‡ä¸ªæ€§åŒ–æ¨èæ€§èƒ½æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08454v1",
      "published_date": "2025-08-11 20:28:24 UTC",
      "updated_date": "2025-08-11 20:28:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:53:41.791546+00:00"
    },
    {
      "arxiv_id": "2508.08446v1",
      "title": "OverFill: Two-Stage Models for Efficient Language Model Decoding",
      "title_zh": "OverFillï¼šé¢å‘é«˜æ•ˆè¯­è¨€æ¨¡å‹è§£ç çš„ä¸¤é˜¶æ®µæ¨¡å‹",
      "authors": [
        "Woojeong Kim",
        "Junxiong Wang",
        "Jing Nathan Yan",
        "Mohamed Abdelfattah",
        "Alexander M. Rush"
      ],
      "abstract": "Large language models (LLMs) excel across diverse tasks but face significant deployment challenges due to high inference costs. LLM inference comprises prefill (compute-bound) and decode (memory-bound) stages, with decode dominating latency particularly for long sequences. Current decoder-only models handle both stages uniformly, despite their distinct computational profiles. We propose OverFill, which decouples these stages to optimize accuracy-efficiency tradeoffs. OverFill begins with a full model for prefill, processing system and user inputs in parallel. It then switches to a dense pruned model, while generating tokens sequentially. Leveraging more compute during prefill, OverFill improves generation quality with minimal latency overhead. Our 3B-to-1B OverFill configuration outperforms 1B pruned models by 83.2%, while the 8B-to-3B configuration improves over 3B pruned models by 79.2% on average across standard benchmarks. OverFill matches the performance of same-sized models trained from scratch, while using significantly less training data. Our code is available at https://github.com/friendshipkim/overfill.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† OverFillï¼Œä¸€ç§é€šè¿‡è§£è€¦ prefill å’Œ decode é˜¶æ®µæ¥ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†æ•ˆç‡çš„ä¸¤é˜¶æ®µæ¨¡å‹æ¡†æ¶ã€‚åœ¨ prefill é˜¶æ®µï¼ŒOverFill ä½¿ç”¨å®Œæ•´çš„å…¨é‡æ¨¡å‹å¹¶è¡Œå¤„ç†è¾“å…¥ï¼Œè€Œåœ¨ç”Ÿæˆ token çš„ decode é˜¶æ®µåˆ™åˆ‡æ¢è‡³ç´§å‡‘çš„ dense pruned æ¨¡å‹ï¼Œä»¥å¹³è¡¡è®¡ç®—å¯†é›†å‹å’Œå†…å­˜å¯†é›†å‹ä»»åŠ¡çš„ä¸åŒéœ€æ±‚ã€‚è¿™ç§æ–¹æ³•å……åˆ†åˆ©ç”¨äº† prefill é˜¶æ®µçš„è®¡ç®—èƒ½åŠ›æ¥æ˜¾è‘—æå‡ç”Ÿæˆè´¨é‡ï¼ŒåŒæ—¶ä¿æŒäº†æä½çš„å»¶è¿Ÿå¼€é”€ã€‚å®éªŒè¡¨æ˜ï¼Œ3B-to-1B é…ç½®çš„ OverFill åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­æ¯” 1B å‰ªææ¨¡å‹è¡¨ç°æå‡äº† 83.2%ï¼Œ8B-to-3B é…ç½®åˆ™æ¯” 3B å‰ªææ¨¡å‹æå‡äº† 79.2%ã€‚æ­¤å¤–ï¼ŒOverFill ä»…éœ€æå°‘çš„è®­ç»ƒæ•°æ®å³å¯è¾¾åˆ°ä¸åŒç­‰è§„æ¨¡ä»é›¶å¼€å§‹è®­ç»ƒçš„æ¨¡å‹ç›¸åŒ¹é…çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to COLM 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.08446v1",
      "published_date": "2025-08-11 20:07:34 UTC",
      "updated_date": "2025-08-11 20:07:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:54:15.353933+00:00"
    },
    {
      "arxiv_id": "2508.08442v1",
      "title": "Solver-Aided Expansion of Loops to Avoid Generate-and-Test",
      "title_zh": "é¿å…â€œç”Ÿæˆ-æµ‹è¯•â€æ¨¡å¼çš„æ±‚è§£å™¨è¾…åŠ©å¾ªç¯å±•å¼€",
      "authors": [
        "Niklas Dewally",
        "Ã–zgÃ¼r AkgÃ¼n"
      ],
      "abstract": "Constraint modelling languages like MiniZinc and Essence rely on unrolling loops (in the form of quantified expressions and comprehensions) during compilation. Standard approaches generate all combinations of induction variables and use partial evaluation to discard those that simplify to identity elements of associative-commutative operators (e.g. true for conjunction, 0 for summation). This can be inefficient for problems where most combinations are ultimately irrelevant. We present a method that avoids full enumeration by using a solver to compute only the combinations required to generate the final set of constraints. The resulting model is identical to that produced by conventional flattening, but compilation can be significantly faster. This improves the efficiency of translating high-level user models into solver-ready form, particularly when induction variables range over large domains with selective preconditions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ MiniZinc å’Œ Essence ç­‰çº¦æŸå»ºæ¨¡è¯­è¨€åœ¨ç¼–è¯‘è¿‡ç¨‹ä¸­å¾ªç¯å±•å¼€æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§è¾…åŠ©è§£ç®—å™¨æ‰©å±•å¾ªç¯ (Solver-Aided Expansion of Loops) çš„æ–°æ–¹æ³•ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ç”Ÿæˆæ‰€æœ‰å½’çº³å˜é‡ç»„åˆå¹¶åˆ©ç”¨éƒ¨åˆ†æ±‚å€¼è¿‡æ»¤æ— å…³é¡¹ï¼Œè¿™åœ¨å¤§å¤šæ•°ç»„åˆæ— æ•ˆæ—¶ä¼šäº§ç”Ÿå·¨å¤§çš„è®¡ç®—æµªè´¹ã€‚æ–°æ–¹æ³•é€šè¿‡å¼•å…¥è§£ç®—å™¨æ¥ä»…è®¡ç®—ç”Ÿæˆæœ€ç»ˆçº¦æŸé›†æ‰€éœ€çš„å¿…è¦ç»„åˆï¼Œä»è€Œæœ‰æ•ˆé¿å…äº†ä¼ ç»Ÿçš„â€œç”Ÿæˆå¹¶æµ‹è¯•â€ (Generate-and-Test) æ¨¡å¼ä¸‹çš„å…¨é‡æšä¸¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„æ¨¡å‹ä¸ä¼ ç»Ÿå±•å¹³ (Flattening) äº§ç”Ÿçš„ç»“æœå®Œå…¨ä¸€è‡´ï¼Œä½†åœ¨ç¼–è¯‘é€Ÿåº¦ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚è¿™ä¸€è´¡çŒ®æå‡äº†å°†é«˜çº§ç”¨æˆ·æ¨¡å‹è½¬æ¢ä¸ºè§£ç®—å™¨å°±ç»ªå½¢å¼çš„æ•ˆç‡ï¼Œç‰¹åˆ«é€‚ç”¨äºå½’çº³å˜é‡å®šä¹‰åŸŸè¾ƒå¤§ä¸”å…·æœ‰å¤æ‚é€‰æ‹©æ€§å…ˆå†³æ¡ä»¶çš„åº”ç”¨åœºæ™¯ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages, 4 figures, published in ModRef 2025 workshop",
      "pdf_url": "https://arxiv.org/pdf/2508.08442v1",
      "published_date": "2025-08-11 19:59:16 UTC",
      "updated_date": "2025-08-11 19:59:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:54:08.757884+00:00"
    },
    {
      "arxiv_id": "2508.08435v4",
      "title": "Fast weight programming and linear transformers: from machine learning to neurobiology",
      "title_zh": "å¿«é€Ÿæƒé‡ç¼–ç¨‹ä¸çº¿æ€§ Transformerï¼šä»æœºå™¨å­¦ä¹ åˆ°ç¥ç»ç”Ÿç‰©å­¦",
      "authors": [
        "Kazuki Irie",
        "Samuel J. Gershman"
      ],
      "abstract": "Recent advances in artificial neural networks for machine learning, and language modeling in particular, have established a family of recurrent neural network (RNN) architectures that, unlike conventional RNNs with vector-form hidden states, use two-dimensional (2D) matrix-form hidden states. Such 2D-state RNNs, known as Fast Weight Programmers (FWPs), can be interpreted as a neural network whose synaptic weights (called fast weights) dynamically change over time as a function of input observations, and serve as short-term memory storage; corresponding synaptic weight modifications are controlled or programmed by another network (the programmer) whose parameters are trained (e.g., by gradient descent). In this Primer, we review the technical foundations of FWPs, their computational characteristics, and their connections to transformers and state space models. We also discuss connections between FWPs and models of synaptic plasticity in the brain, suggesting a convergence of natural and artificial intelligence.",
      "tldr_zh": "è¿™ç¯‡ç»¼è¿°æ¢è®¨äº†å¿«æƒé‡ç¼–ç¨‹å™¨ï¼ˆFast Weight Programmers, FWPsï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åœ¨æœºå™¨å­¦ä¹ å°¤å…¶æ˜¯è¯­è¨€å»ºæ¨¡é¢†åŸŸä¸­åº”ç”¨æ—¥ç›Šå¹¿æ³›çš„å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æ¶æ„ã€‚ä¸ä¼ ç»Ÿä½¿ç”¨å‘é‡å½¢å¼éšè—çŠ¶æ€çš„ç½‘ç»œä¸åŒï¼ŒFWPs é‡‡ç”¨äºŒç»´çŸ©é˜µå½¢å¼çš„éšè—çŠ¶æ€ï¼Œé€šè¿‡åŠ¨æ€æ”¹å˜çªè§¦æƒé‡ï¼ˆå³ fast weightsï¼‰æ¥å……å½“çŸ­æœŸè®°å¿†å­˜å‚¨ï¼Œè¿™äº›æƒé‡çš„ä¿®æ”¹ç”±å¦ä¸€ä¸ªç»è¿‡è®­ç»ƒçš„ç¨‹åºå‘˜ç½‘ç»œï¼ˆprogrammerï¼‰è¿›è¡Œæ§åˆ¶ã€‚æ–‡ç« æ·±å…¥åˆ†æäº† FWPs çš„æŠ€æœ¯åŸºç¡€å’Œè®¡ç®—ç‰¹æ€§ï¼Œå¹¶é‡ç‚¹é˜è¿°äº†å…¶ä¸ transformers åŠçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆstate space models, SSMsï¼‰ä¹‹é—´çš„ç´§å¯†è”ç³»ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜è®¨è®ºäº† FWPs ä¸å¤§è„‘ä¸­çªè§¦å¯å¡‘æ€§ï¼ˆsynaptic plasticityï¼‰æ¨¡å‹ä¹‹é—´çš„å…³è”ï¼Œæš—ç¤ºäº†äººå·¥æ™ºèƒ½ä¸è‡ªç„¶æ™ºèƒ½åœ¨åŸºæœ¬åŸç†ä¸Šçš„æ±‡èšä¸ç»Ÿä¸€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to TMLR 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.08435v4",
      "published_date": "2025-08-11 19:50:03 UTC",
      "updated_date": "2026-01-16 00:21:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:54:08.155299+00:00"
    },
    {
      "arxiv_id": "2511.13722v1",
      "title": "Signature vs. Substance: Evaluating the Balance of Adversarial Resistance and Linguistic Quality in Watermarking Large Language Models",
      "title_zh": "æ ‡è®°ä¸æœ¬è´¨ï¼šè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹æ°´å°ä¸­å¯¹æŠ—é²æ£’æ€§ä¸è¯­è¨€è´¨é‡çš„å¹³è¡¡",
      "authors": [
        "William Guo",
        "Adaku Uchendu",
        "Ana Smith"
      ],
      "abstract": "To mitigate the potential harms of Large Language Models (LLMs)generated text, researchers have proposed watermarking, a process of embedding detectable signals within text. With watermarking, we can always accurately detect LLM-generated texts. However, recent findings suggest that these techniques often negatively affect the quality of the generated texts, and adversarial attacks can strip the watermarking signals, causing the texts to possibly evade detection. These findings have created resistance in the wide adoption of watermarking by LLM creators. Finally, to encourage adoption, we evaluate the robustness of several watermarking techniques to adversarial attacks by comparing paraphrasing and back translation (i.e., English $\\to$ another language $\\to$ English) attacks; and their ability to preserve quality and writing style of the unwatermarked texts by using linguistic metrics to capture quality and writing style of texts. Our results suggest that these watermarking techniques preserve semantics, deviate from the writing style of the unwatermarked texts, and are susceptible to adversarial attacks, especially for the back translation attack.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)æ°´å°æŠ€æœ¯åœ¨å¯¹æŠ—æ”»å‡»é²æ£’æ€§ä¸è¯­è¨€è´¨é‡ä¹‹é—´çš„å¹³è¡¡ï¼Œæ—¨åœ¨è§£å†³æ°´å°ä¿¡å·æ˜“è¢«æ¶ˆé™¤åŠå½±å“ç”Ÿæˆæ–‡æœ¬è´¨é‡çš„é—®é¢˜ã€‚ä½œè€…è¯„ä¼°äº†å¤šç§æ°´å°æŠ€æœ¯å¯¹æ”¹å†™(Paraphrasing)å’Œå›è¯‘(Back Translation)æ”»å‡»çš„æŠµæŠ—èƒ½åŠ›ï¼Œå¹¶åˆ©ç”¨è¯­è¨€å­¦æŒ‡æ ‡åˆ†æå…¶å¯¹æ–‡æœ¬è´¨é‡å’Œå†™ä½œé£æ ¼çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„æ°´å°æŠ€æœ¯è™½ç„¶èƒ½å¤Ÿè¾ƒå¥½åœ°ä¿æŒè¯­ä¹‰ä¿¡æ¯ï¼Œä½†ä¼šæ˜¾è‘—åç¦»åŸå§‹æœªåŠ æ°´å°æ–‡æœ¬çš„å†™ä½œé£æ ¼ã€‚åœ¨é²æ£’æ€§æµ‹è¯•ä¸­ï¼Œè¿™äº›æ°´å°æŠ€æœ¯ææ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»çš„å½±å“ï¼Œå°¤å…¶æ˜¯å›è¯‘æ”»å‡»èƒ½æœ‰æ•ˆå»é™¤æ°´å°ä¿¡å·å¹¶è§„é¿æ£€æµ‹ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†å½“å‰æ°´å°æŠ€æœ¯åœ¨ç»´æŒè¯­è¨€ç»†å¾®å·®åˆ«å’Œå¢å¼ºé˜²å¾¡å¼ºåº¦æ–¹é¢çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥å¼€å‘æ›´å…·éŸ§æ€§çš„æ£€æµ‹æ‰‹æ®µæä¾›äº†å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13722v1",
      "published_date": "2025-08-11 19:40:37 UTC",
      "updated_date": "2025-08-11 19:40:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:54:10.467216+00:00"
    },
    {
      "arxiv_id": "2508.08424v3",
      "title": "Rethinking Tokenization for Rich Morphology: The Dominance of Unigram over BPE and Morphological Alignment",
      "title_zh": "é‡æ–°å®¡è§†å½¢æ€ä¸°å¯Œè¯­è¨€çš„è¯å…ƒåŒ–ï¼šUnigram ç›¸è¾ƒäº BPE å’Œå½¢æ€å¯¹é½çš„ä¼˜åŠ¿åœ°ä½",
      "authors": [
        "Saketh Reddy Vemula",
        "Sandipan Dandapat",
        "Dipti Misra Sharma",
        "Parameswari Krishnamurthy"
      ],
      "abstract": "The relationship between tokenizer algorithm (e.g., Byte-Pair Encoding (BPE), Unigram), morphological alignment, tokenization quality (e.g., compression efficiency), and downstream performance remains largely unclear, particularly for languages with complex morphology. In this paper, we conduct a comprehensive evaluation of tokenizers using small-sized BERT models -- from pre-training through fine-tuning -- for Telugu (agglutinative), along with preliminary evaluation in Hindi (primarily fusional with some agglutination) and English (fusional). To evaluate morphological alignment of tokenizers in Telugu, we create a dataset containing gold morpheme segmentations of 600 derivational and 7000 inflectional word forms.\n  Our experiments reveal two key findings for Telugu. First, the choice of tokenizer algorithm is the most significant factor influencing performance, with Unigram-based tokenizers consistently outperforming BPE across most settings. Second, while better morphological alignment shows a moderate, positive correlation with performance on text classification and structure prediction tasks, its impact is secondary to the tokenizer algorithm. Notably, hybrid approaches that use morphological information for pre-segmentation significantly boost the performance of BPE, though not Unigram. Our results further showcase the need for comprehensive intrinsic evaluation metrics for tokenizers that could explain downstream performance trends consistently.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ†è¯å™¨(tokenizer)ç®—æ³•ï¼ˆå¦‚BPEå’ŒUnigramï¼‰ã€å½¢æ€å¯¹é½(morphological alignment)ä¸ä¸‹æ¸¸æ€§èƒ½ä¹‹é—´çš„å…³ç³»ï¼Œé‡ç‚¹åˆ†æäº†å…¶åœ¨å½¢æ€ä¸°å¯Œè¯­è¨€ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶è€…é€šè¿‡æ³°å¢å›ºè¯­(Telugu)ã€å°åœ°è¯­(Hindi)å’Œè‹±è¯­çš„BERTæ¨¡å‹å®éªŒå‘ç°ï¼Œåˆ†è¯ç®—æ³•çš„é€‰æ‹©æ˜¯å½±å“æ€§èƒ½çš„æœ€å…³é”®å› ç´ ï¼Œä¸”Unigramåœ¨å¤šæ•°è®¾ç½®ä¸‹ä¼˜äºBPEã€‚è™½ç„¶å½¢æ€å¯¹é½ä¸æ–‡æœ¬åˆ†ç±»(text classification)åŠç»“æ„é¢„æµ‹(structure prediction)ä»»åŠ¡å‘ˆä¸­åº¦æ­£ç›¸å…³ï¼Œä½†å…¶å½±å“æ¬¡äºç®—æ³•æœ¬èº«ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨å½¢æ€ä¿¡æ¯è¿›è¡Œé¢„åˆ‡åˆ†(pre-segmentation)çš„æ··åˆæ–¹æ³•èƒ½æ˜¾è‘—æå‡BPEçš„æ€§èƒ½ï¼Œä½†å¯¹Unigramçš„æ”¹è¿›æœ‰é™ã€‚è¯¥ç ”ç©¶è¿˜æ„å»ºäº†åŒ…å«é‡‘æ ‡å‡†å½¢æ€åˆ‡åˆ†çš„æ³°å¢å›ºè¯­æ•°æ®é›†ï¼Œç”¨äºæ›´ç²¾å‡†åœ°è¯„ä¼°åˆ†è¯è´¨é‡ã€‚æœ€ç»ˆç»“æœå¼ºè°ƒäº†å¼€å‘æ›´å…¨é¢å†…åœ¨è¯„ä¼°æŒ‡æ ‡çš„å¿…è¦æ€§ï¼Œä»¥ä¸€è‡´åœ°è§£é‡Šåˆ†è¯å™¨å¯¹ä¸‹æ¸¸æ€§èƒ½çš„å½±å“ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08424v3",
      "published_date": "2025-08-11 19:23:59 UTC",
      "updated_date": "2025-11-10 14:36:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:54:48.585592+00:00"
    },
    {
      "arxiv_id": "2508.08421v1",
      "title": "Neural Tangent Knowledge Distillation for Optical Convolutional Networks",
      "title_zh": "é¢å‘å…‰å­¦å·ç§¯ç½‘ç»œçš„ç¥ç»åˆ‡çº¿çŸ¥è¯†è’¸é¦",
      "authors": [
        "Jinlin Xiang",
        "Minho Choi",
        "Yubo Zhang",
        "Zhihao Zhou",
        "Arka Majumdar",
        "Eli Shlizerman"
      ],
      "abstract": "Hybrid Optical Neural Networks (ONNs, typically consisting of an optical frontend and a digital backend) offer an energy-efficient alternative to fully digital deep networks for real-time, power-constrained systems. However, their adoption is limited by two main challenges: the accuracy gap compared to large-scale networks during training, and discrepancies between simulated and fabricated systems that further degrade accuracy. While previous work has proposed end-to-end optimizations for specific datasets (e.g., MNIST) and optical systems, these approaches typically lack generalization across tasks and hardware designs. To address these limitations, we propose a task-agnostic and hardware-agnostic pipeline that supports image classification and segmentation across diverse optical systems. To assist optical system design before training, we estimate achievable model accuracy based on user-specified constraints such as physical size and the dataset. For training, we introduce Neural Tangent Knowledge Distillation (NTKD), which aligns optical models with electronic teacher networks, thereby narrowing the accuracy gap. After fabrication, NTKD also guides fine-tuning of the digital backend to compensate for implementation errors. Experiments on multiple datasets (e.g., MNIST, CIFAR, Carvana Masking) and hardware configurations show that our pipeline consistently improves ONN performance and enables practical deployment in both pre-fabrication simulations and physical implementations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ··åˆå…‰å­ç¥ç»ç½‘ç»œ (Hybrid Optical Neural Networks, ONNs) åœ¨è®­ç»ƒç²¾åº¦å’Œè½¯ç¡¬ä»¶å·®å¼‚æ–¹é¢çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ä»»åŠ¡æ— å…³ä¸”ç¡¬ä»¶æ— å…³çš„é€šç”¨è®¾è®¡ä¸è®­ç»ƒæµæ°´çº¿ã€‚è¯¥æµæ°´çº¿æ”¯æŒåœ¨è®­ç»ƒå‰åŸºäºç‰©ç†çº¦æŸé¢„ä¼°æ¨¡å‹ç²¾åº¦ï¼Œä»è€Œè¾…åŠ©å…‰å­ç³»ç»Ÿè®¾è®¡ã€‚ç ”ç©¶æ ¸å¿ƒå¼•å…¥äº† Neural Tangent Knowledge Distillation (NTKD) æŠ€æœ¯ï¼Œé€šè¿‡å°†å…‰å­æ¨¡å‹ä¸ç”µå­æ•™å¸ˆç½‘ç»œå¯¹é½ï¼Œæœ‰æ•ˆç¼©å°äº†æ¨¡å‹é—´çš„ç²¾åº¦å·®è·ã€‚åœ¨ç¡¬ä»¶åˆ¶é€ åï¼ŒNTKD è¿˜å¯ç”¨äºæŒ‡å¯¼æ•°å­—åç«¯çš„å¾®è°ƒï¼Œä»¥è¡¥å¿ç‰©ç†å®ç°è¿‡ç¨‹ä¸­äº§ç”Ÿçš„è¯¯å·®ã€‚å®éªŒç»“æœåœ¨ MNISTã€CIFAR å’Œ Carvana Masking ç­‰å¤šä¸ªæ•°æ®é›†ä¸Šè¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº† ONN çš„æ€§èƒ½ï¼Œä¸ºå…‰å­ç¥ç»ç½‘ç»œåœ¨é¢„åˆ¶é€ ä»¿çœŸå’Œç‰©ç†å®ç°ä¸­çš„å®é™…éƒ¨ç½²æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08421v1",
      "published_date": "2025-08-11 19:15:06 UTC",
      "updated_date": "2025-08-11 19:15:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:54:21.362245+00:00"
    },
    {
      "arxiv_id": "2508.09220v4",
      "title": "Towards Scalable Training for Handwritten Mathematical Expression Recognition",
      "title_zh": "è¿ˆå‘æ‰‹å†™æ•°å­¦å…¬å¼è¯†åˆ«çš„å¯æ‰©å±•è®­ç»ƒ",
      "authors": [
        "Haoyang Li",
        "Jiaqing Li",
        "Jialun Cao",
        "Zongyuan Yang",
        "Yongping Xiong"
      ],
      "abstract": "Large foundation models have achieved significant performance gains through scalable training on massive datasets. However, the field of \\textbf{H}andwritten \\textbf{M}athematical \\textbf{E}xpression \\textbf{R}ecognition (HMER) has been impeded by the scarcity of data, primarily due to the arduous and costly process of manual annotation. To bridge this gap, we propose a novel method integrating limited handwritten formulas with large-scale LaTeX-rendered formulas by developing a scalable data engine to generate complex and consistent LaTeX sequences. With this engine, we built the largest formula dataset to date, termed \\texttt{Tex80M}, comprising over 80 million high-quality training instances. Then we propose \\texttt{TexTeller}, the first HMER model trained at scale, by mix-training \\texttt{Tex80M} with a relatively small HME dataset. The expansive training dataset and our refined pipeline have equipped \\texttt{TexTeller} with state-of-the-art (SOTA) performance across nearly all benchmarks. To advance the field, we will openly release our complete model, entire dataset, and full codebase, enabling further research building upon our contributions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰‹å†™æ•°å­¦å…¬å¼è¯†åˆ« (Handwritten Mathematical Expression Recognition, HMER) é¢†åŸŸå› æ‰‹å·¥æ ‡æ³¨æˆæœ¬é«˜æ˜‚å¯¼è‡´çš„è®­ç»ƒæ•°æ®åŒ®ä¹é—®é¢˜ï¼Œæå‡ºäº†ä¸€å¥—é›†æˆæœ‰é™æ‰‹å†™æ•°æ®ä¸å¤§è§„æ¨¡ LaTeX æ¸²æŸ“å…¬å¼çš„æ‰©å±•æ€§è®­ç»ƒæ–¹æ¡ˆã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ•°æ®å¼•æ“ï¼Œç”¨äºç”Ÿæˆå¤æ‚ä¸”ä¸€è‡´çš„ LaTeX åºåˆ—ï¼Œå¹¶æ®æ­¤æ„å»ºäº†è¿„ä»Šä¸ºæ­¢è§„æ¨¡æœ€å¤§çš„å…¬å¼æ•°æ®é›† Tex80Mï¼ŒåŒ…å«è¶…è¿‡ 8000 ä¸‡ä¸ªé«˜è´¨é‡è®­ç»ƒæ ·æœ¬ã€‚åŸºäºè¯¥æ•°æ®é›†ï¼Œç ”ç©¶è€…æ¨å‡ºäº†é¦–ä¸ªå®ç°å¤§è§„æ¨¡è®­ç»ƒçš„ HMER æ¨¡å‹ TexTellerï¼Œé€šè¿‡å°† Tex80M ä¸å°‘é‡çœŸå®æ‰‹å†™æ•°æ®æ··åˆè®­ç»ƒï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTexTeller åœ¨å‡ ä¹æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ (SOTA) æ€§èƒ½æ°´å¹³ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å…¬å¼€å‘å¸ƒäº†å®Œæ•´çš„æ¨¡å‹ã€æ•°æ®é›†å’Œä»£ç åº“ï¼Œæ—¨åœ¨é€šè¿‡èµ„æºå…±äº«æ¨åŠ¨è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "The authors have decided to temporarily withdraw this paper to make substantial revisions",
      "pdf_url": "https://arxiv.org/pdf/2508.09220v4",
      "published_date": "2025-08-11 19:10:34 UTC",
      "updated_date": "2026-01-10 13:43:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:54:28.683863+00:00"
    },
    {
      "arxiv_id": "2508.09219v2",
      "title": "Ethics Practices in AI Development: An Empirical Study Across Roles and Regions",
      "title_zh": "AI å¼€å‘ä¸­çš„ä¼¦ç†å®è·µï¼šè·¨è§’è‰²ä¸åœ°åŒºçš„å®è¯ç ”ç©¶",
      "authors": [
        "Wilder Baldwin",
        "Sepideh Ghanavati",
        "Manuel Woersdoerfer"
      ],
      "abstract": "Recent advances in AI applications have raised growing concerns about the need for ethical guidelines and regulations to mitigate the risks posed by these technologies. In this paper, we present a mixed-methods survey study - combining statistical and qualitative analyses - to examine the ethical perceptions, practices, and knowledge of individuals involved in various AI development roles. Our survey comprises 414 participants from 43 countries, representing various roles such as AI managers, analysts, developers, quality assurance professionals, and information security and privacy experts. The results reveal varying degrees of familiarity and experience with AI ethics principles, government initiatives, and risk mitigation strategies across roles, regions, and other demographic factors. Our findings underscore the importance of a collaborative, role-sensitive approach that involves diverse stakeholders in ethical decision-making throughout the AI development lifecycle. We advocate for developing tailored, inclusive solutions to address ethical challenges in AI development, and we propose future research directions and educational strategies to promote ethics-aware AI practices.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡æ··åˆæ–¹æ³•ç ”ç©¶(mixed-methods survey study)å¯¹å…¨çƒ43ä¸ªå›½å®¶çš„414åAIå¼€å‘ä»ä¸šäººå‘˜è¿›è¡Œäº†è°ƒæŸ¥ï¼Œç³»ç»Ÿè€ƒå¯Ÿäº†ä¸åŒè§’è‰²å’Œåœ°åŒºåœ¨AIä¼¦ç†(AI ethics)æ–¹é¢çš„è®¤çŸ¥ã€å®è·µä¸çŸ¥è¯†æ°´å¹³ã€‚å—è®¿è€…ç¾¤ä½“æ¶µç›–äº†AIç»ç†ã€åˆ†æå¸ˆã€å¼€å‘äººå‘˜ã€è´¨é‡ä¿è¯ä¸“ä¸šäººå‘˜ä»¥åŠä¿¡æ¯å®‰å…¨ä¸éšç§ä¸“å®¶ç­‰å¤šç§æ ¸å¿ƒè§’è‰²ã€‚ç ”ç©¶ç»“æœæ­ç¤ºï¼Œä»ä¸šäººå‘˜å¯¹AIä¼¦ç†åŸåˆ™ã€æ”¿åºœå€¡è®®(government initiatives)åŠé£é™©ç¼“è§£ç­–ç•¥(risk mitigation strategies)çš„ç†Ÿæ‚‰ç¨‹åº¦åœ¨ä¸åŒå²—ä½ã€åœ°åŸŸåŠäººå£ç»Ÿè®¡ç»´åº¦ä¸‹å‘ˆç°å‡ºæ˜¾è‘—å·®å¼‚ã€‚è°ƒæŸ¥å‘ç°å¼ºè°ƒäº†åœ¨AIå¼€å‘ç”Ÿå‘½å‘¨æœŸä¸­é‡‡å–åä½œå¼ä¸”å¯¹è§’è‰²æ•æ„Ÿ(role-sensitive)æ–¹æ³•çš„é‡è¦æ€§ï¼Œå¹¶ä¸»å¼ è®©å¤šå…ƒåˆ©ç›Šç›¸å…³è€…å…±åŒå‚ä¸ä¼¦ç†å†³ç­–ã€‚æ­¤å¤–ï¼Œè®ºæ–‡å€¡è®®å¼€å‘é‡èº«å®šåˆ¶ä¸”å…·æœ‰åŒ…å®¹æ€§çš„è§£å†³æ–¹æ¡ˆä»¥åº”å¯¹AIå¼€å‘ä¸­çš„ä¼¦ç†æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæœªæ¥æ¨å¹¿å…·å¤‡ä¼¦ç†æ„è¯†çš„AIå®è·µæå‡ºäº†æ˜ç¡®çš„ç ”ç©¶æ–¹å‘ä¸æ•™è‚²ç­–ç•¥ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "cs.SE"
      ],
      "primary_category": "cs.CY",
      "comment": "Under Review",
      "pdf_url": "https://arxiv.org/pdf/2508.09219v2",
      "published_date": "2025-08-11 19:07:20 UTC",
      "updated_date": "2025-12-13 21:55:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:54:37.091086+00:00"
    },
    {
      "arxiv_id": "2508.09218v1",
      "title": "Towards Effective MLLM Jailbreaking Through Balanced On-Topicness and OOD-Intensity",
      "title_zh": "é€šè¿‡å¹³è¡¡ä¸»é¢˜ç›¸å…³æ€§ä¸ OOD å¼ºåº¦å®ç°æœ‰æ•ˆçš„ MLLM è¶Šç‹±",
      "authors": [
        "Zuoou Li",
        "Weitong Zhang",
        "Jingyuan Wang",
        "Shuyuan Zhang",
        "Wenjia Bai",
        "Bernhard Kainz",
        "Mengyun Qiao"
      ],
      "abstract": "Multimodal large language models (MLLMs) are widely used in vision-language reasoning tasks. However, their vulnerability to adversarial prompts remains a serious concern, as safety mechanisms often fail to prevent the generation of harmful outputs. Although recent jailbreak strategies report high success rates, many responses classified as \"successful\" are actually benign, vague, or unrelated to the intended malicious goal. This mismatch suggests that current evaluation standards may overestimate the effectiveness of such attacks. To address this issue, we introduce a four-axis evaluation framework that considers input on-topicness, input out-of-distribution (OOD) intensity, output harmfulness, and output refusal rate. This framework identifies truly effective jailbreaks. In a substantial empirical study, we reveal a structural trade-off: highly on-topic prompts are frequently blocked by safety filters, whereas those that are too OOD often evade detection but fail to produce harmful content. However, prompts that balance relevance and novelty are more likely to evade filters and trigger dangerous output. Building on this insight, we develop a recursive rewriting strategy called Balanced Structural Decomposition (BSD). The approach restructures malicious prompts into semantically aligned sub-tasks, while introducing subtle OOD signals and visual cues that make the inputs harder to detect. BSD was tested across 13 commercial and open-source MLLMs, where it consistently led to higher attack success rates, more harmful outputs, and fewer refusals. Compared to previous methods, it improves success rates by $67\\%$ and harmfulness by $21\\%$, revealing a previously underappreciated weakness in current multimodal safety systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨å¯¹æŠ—æ€§è¶Šç‹± (Jailbreaking) æ”»å‡»ä¸‹çš„è„†å¼±æ€§ï¼ŒæŒ‡å‡ºå½“å‰è¯„ä¼°æ ‡å‡†å¾€å¾€é«˜ä¼°äº†æ”»å‡»çš„æœ‰æ•ˆæ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªåŒ…å«è¾“å…¥ä¸»é¢˜ç›¸å…³æ€§ (On-Topicness)ã€åˆ†å¸ƒå¤–å¼ºåº¦ (OOD-Intensity)ã€è¾“å‡ºæœ‰å®³æ€§åŠæ‹’ç»ç‡çš„å››ç»´è¯„ä¼°æ¡†æ¶ï¼Œä»¥æ›´ç²¾å‡†åœ°è¯†åˆ«çœŸå®æœ‰æ•ˆçš„è¶Šç‹±è¡Œä¸ºã€‚ç ”ç©¶æ­ç¤ºäº†æç¤ºè¯åœ¨ä¸»é¢˜ç›¸å…³æ€§ä¸åˆ†å¸ƒå¤–å¼ºåº¦ä¹‹é—´å­˜åœ¨ç»“æ„æ€§æƒè¡¡ï¼Œå‘ç°åªæœ‰å¹³è¡¡äºŒè€…çš„è¾“å…¥æ‰æœ€æ˜“ç»•è¿‡å®‰å…¨è¿‡æ»¤å¹¶è§¦å‘å±é™©è¾“å‡ºã€‚åŸºäºæ­¤æ´å¯Ÿï¼Œç ”ç©¶å¼€å‘äº†åä¸ºå¹³è¡¡ç»“æ„åˆ†è§£ (Balanced Structural Decomposition, BSD) çš„é€’å½’é‡å†™ç­–ç•¥ï¼Œé€šè¿‡å°†æ¶æ„æç¤ºé‡æ„ä¸ºè¯­ä¹‰å¯¹é½çš„å­ä»»åŠ¡å¹¶å¼•å…¥å¾®å¦™çš„ OOD ä¿¡å·ã€‚å®éªŒè¡¨æ˜ï¼ŒBSD åœ¨ 13 ç§å•†ä¸šåŠå¼€æºæ¨¡å‹ä¸Šæ˜¾è‘—æå‡äº†æ”»å‡»æ€§èƒ½ï¼Œç›¸è¾ƒäºå…ˆå‰æ–¹æ³•å°†æˆåŠŸç‡æé«˜äº† 67%ï¼Œæœ‰å®³æ€§å¢å¼ºäº† 21%ï¼Œæ·±åˆ»æ­ç¤ºäº†å½“å‰å¤šæ¨¡æ€å®‰å…¨ç³»ç»Ÿä¸­æ­¤å‰è¢«ä½ä¼°çš„é˜²å¾¡ç¼ºé™·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.09218v1",
      "published_date": "2025-08-11 18:57:55 UTC",
      "updated_date": "2025-08-11 18:57:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:54:52.890710+00:00"
    },
    {
      "arxiv_id": "2508.08404v1",
      "title": "Generating Query-Relevant Document Summaries via Reinforcement Learning",
      "title_zh": "åŸºäºå¼ºåŒ–å­¦ä¹ çš„æŸ¥è¯¢ç›¸å…³æ–‡æ¡£æ‘˜è¦ç”Ÿæˆ",
      "authors": [
        "Nitin Yadav",
        "Changsung Kang",
        "Hongwei Shang",
        "Ming Sun"
      ],
      "abstract": "E-commerce search engines often rely solely on product titles as input for ranking models with latency constraints. However, this approach can result in suboptimal relevance predictions, as product titles often lack sufficient detail to capture query intent. While product descriptions provide richer information, their verbosity and length make them unsuitable for real-time ranking, particularly for computationally expensive architectures like cross-encoder ranking models. To address this challenge, we propose ReLSum, a novel reinforcement learning framework designed to generate concise, query-relevant summaries of product descriptions optimized for search relevance. ReLSum leverages relevance scores as rewards to align the objectives of summarization and ranking, effectively overcoming limitations of prior methods, such as misaligned learning targets. The framework employs a trainable large language model (LLM) to produce summaries, which are then used as input for a cross-encoder ranking model. Experimental results demonstrate significant improvements in offline metrics, including recall and NDCG, as well as online user engagement metrics. ReLSum provides a scalable and efficient solution for enhancing search relevance in large-scale e-commerce systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”µå­å•†åŠ¡æœç´¢å¼•æ“åœ¨æ’åºæ¨¡å‹ä¸­ä»…ä¾èµ–å•†å“æ ‡é¢˜å¯¼è‡´çš„ç›¸å…³æ€§é¢„æµ‹ä¸è¶³ï¼Œä»¥åŠå®Œæ•´å•†å“æè¿°è¿‡é•¿è€Œæ— æ³•æ»¡è¶³å®æ—¶æ’åºå»¶è¿Ÿè¦æ±‚çš„é—®é¢˜ï¼Œæå‡ºäº†ReLSumæ¡†æ¶ã€‚ReLSumæ˜¯ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆç®€æ´ä¸”ä¸æŸ¥è¯¢ç›¸å…³çš„å•†å“æè¿°æ‘˜è¦ï¼Œä»è€Œä¼˜åŒ–æœç´¢ç›¸å…³æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†ç›¸å…³æ€§è¯„åˆ†ä½œä¸ºå¥–åŠ±(Rewards)æ¥å¯¹é½æ‘˜è¦ç”Ÿæˆä¸æ’åºçš„ç›®æ ‡ï¼Œæœ‰æ•ˆå…‹æœäº†ä»¥å¾€æ–¹æ³•ä¸­å­¦ä¹ ç›®æ ‡ä¸åŒ¹é…çš„å±€é™æ€§ã€‚ReLSumåˆ©ç”¨å¯è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹(LLM)ç”Ÿæˆæ‘˜è¦ï¼Œå¹¶å°†å…¶ä½œä¸ºäº¤å‰ç¼–ç å™¨æ’åºæ¨¡å‹(Cross-encoder ranking models)çš„è¾“å…¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Recallå’ŒNDCGç­‰ç¦»çº¿æŒ‡æ ‡ä»¥åŠåœ¨çº¿ç”¨æˆ·å‚ä¸åº¦æŒ‡æ ‡ä¸Šå‡å–å¾—äº†æ˜¾è‘—æå‡ã€‚è¿™ä¸€æ–¹æ¡ˆä¸ºåœ¨å¤§è§„æ¨¡ç”µå­å•†åŠ¡ç³»ç»Ÿä¸­æå‡æœç´¢ç›¸å…³æ€§æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”é«˜æ•ˆçš„è§£å†³é€”å¾„ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08404v1",
      "published_date": "2025-08-11 18:52:28 UTC",
      "updated_date": "2025-08-11 18:52:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:54:44.592253+00:00"
    },
    {
      "arxiv_id": "2508.08385v2",
      "title": "Bilevel MCTS for Amortized O(1) Node Selection in Classical Planning",
      "title_zh": "é¢å‘ç»å…¸è§„åˆ’ä¸­å‡æ‘Š O(1) èŠ‚ç‚¹é€‰æ‹©çš„åŒå±‚ MCTS",
      "authors": [
        "Masataro Asai"
      ],
      "abstract": "We study an efficient implementation of Multi-Armed Bandit (MAB)-based Monte-Carlo Tree Search (MCTS) for classical planning. One weakness of MCTS is that it spends a significant time deciding which node to expand next. While selecting a node from an OPEN list with $N$ nodes has $O(1)$ runtime complexity with traditional array-based priority-queues for dense integer keys, the tree-based OPEN list used by MCTS requires $O(\\log N)$, which roughly corresponds to the search depth $d$. In classical planning, $d$ is arbitrarily large (e.g., $2^k-1$ in $k$-disk Tower-of-Hanoi) and the runtime for node selection is significant, unlike in game tree search, where the cost is negligible compared to the node evaluation (rollouts) because $d$ is inherently limited by the game (e.g., $d\\leq 361$ in Go). To improve this bottleneck, we propose a bilevel modification to MCTS that runs a best-first search from each selected leaf node with an expansion budget proportional to $d$, which achieves amortized $O(1)$ runtime for node selection, equivalent to the traditional queue-based OPEN list. In addition, we introduce Tree Collapsing, an enhancement that reduces action selection steps and further improves the performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ç»å…¸è§„åˆ’ (Classical Planning) ä¸­æé«˜åŸºäºå¤šè‡‚è€è™æœº (Multi-Armed Bandit) çš„è’™ç‰¹å¡æ´›æ ‘æœç´¢ (MCTS) æ•ˆç‡çš„æ–¹æ³•ï¼Œé‡ç‚¹è§£å†³äº†èŠ‚ç‚¹é€‰æ‹©è¿‡ç¨‹ä¸­çš„è®¡ç®—ç“¶é¢ˆã€‚é’ˆå¯¹ç»å…¸è§„åˆ’ä¸­æœç´¢æ·±åº¦ $d$ è¿‡å¤§å¯¼è‡´èŠ‚ç‚¹é€‰æ‹©å¤æ‚åº¦è¾¾åˆ° $O(\\log N)$ æˆ– $O(d)$ çš„é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åŒå±‚ (Bilevel) ä¿®æ”¹æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆé€šè¿‡ä»æ¯ä¸ªé€‰å®šçš„å¶èŠ‚ç‚¹è¿è¡Œæ‰©å±•é¢„ç®—ä¸ $d$ æˆæ­£æ¯”çš„æœ€ä½³ä¼˜å…ˆæœç´¢ (Best-First Search)ï¼Œå®ç°äº†èŠ‚ç‚¹é€‰æ‹©çš„æ‘Šè¿˜ $O(1)$ (Amortized $O(1)$) è¿è¡Œæ—¶é—´ï¼Œè¾¾åˆ°äº†ä¸ä¼ ç»ŸåŸºäºé˜Ÿåˆ—çš„ OPEN è¡¨ç›¸åŒçš„æ•ˆç‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†æ ‘æŠ˜å  (Tree Collapsing) å¢å¼ºæŠ€æœ¯ï¼Œé€šè¿‡å‡å°‘åŠ¨ä½œé€‰æ‹©æ­¥éª¤è¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆç¼“è§£äº† MCTS åœ¨å¤„ç†å¤§è§„æ¨¡æ·±åº¦æœç´¢ä»»åŠ¡æ—¶çš„è®¡ç®—å‹åŠ›ï¼Œä¸ºå®ç°é«˜æ•ˆçš„ç»å…¸è§„åˆ’ç®—æ³•æä¾›äº†é‡è¦æ”¹è¿›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted in AAAI-26",
      "pdf_url": "https://arxiv.org/pdf/2508.08385v2",
      "published_date": "2025-08-11 18:12:40 UTC",
      "updated_date": "2025-11-17 17:06:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:55:12.600083+00:00"
    },
    {
      "arxiv_id": "2508.08384v1",
      "title": "Spatiotemporally Consistent Indoor Lighting Estimation with Diffusion Priors",
      "title_zh": "åŸºäºæ‰©æ•£å…ˆéªŒçš„æ—¶ç©ºä¸€è‡´å®¤å†…å…‰ç…§ä¼°è®¡",
      "authors": [
        "Mutian Tong",
        "Rundi Wu",
        "Changxi Zheng"
      ],
      "abstract": "Indoor lighting estimation from a single image or video remains a challenge due to its highly ill-posed nature, especially when the lighting condition of the scene varies spatially and temporally. We propose a method that estimates from an input video a continuous light field describing the spatiotemporally varying lighting of the scene. We leverage 2D diffusion priors for optimizing such light field represented as a MLP. To enable zero-shot generalization to in-the-wild scenes, we fine-tune a pre-trained image diffusion model to predict lighting at multiple locations by jointly inpainting multiple chrome balls as light probes. We evaluate our method on indoor lighting estimation from a single image or video and show superior performance over compared baselines. Most importantly, we highlight results on spatiotemporally consistent lighting estimation from in-the-wild videos, which is rarely demonstrated in previous works.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨æ‰©æ•£å…ˆéªŒ (diffusion priors) å®ç°æ—¶ç©ºä¸€è‡´å®¤å†…å…‰ç…§ä¼°è®¡çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å•å¼ å›¾åƒæˆ–è§†é¢‘ä¸­å…‰ç…§éšç©ºé—´å’Œæ—¶é—´å˜åŒ–çš„å¤æ‚ç—…æ€é—®é¢˜ã€‚è¯¥æ–¹æ³•å°†è¿ç»­å…‰åœºè¡¨ç¤ºä¸ºå¤šå±‚æ„ŸçŸ¥æœº (MLP)ï¼Œå¹¶åˆ©ç”¨ 2D æ‰©æ•£å…ˆéªŒå¯¹å…¶è¿›è¡Œä¼˜åŒ–ï¼Œä»è€Œèƒ½å¤Ÿç²¾ç¡®æè¿°åœºæ™¯ä¸­æ—¶ç©ºå˜åŒ–çš„å…‰ç…§ç‰¹å¾ã€‚ä¸ºäº†å¢å¼ºå¯¹è‡ªç„¶åœºæ™¯çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œç ”ç©¶è€…å¾®è°ƒäº†é¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡åœ¨å›¾åƒä¸­è”åˆä¿®å¤ (inpainting) å¤šä¸ªä½œä¸ºå…‰ç…§æ¢é’ˆçš„é‡‘å±çƒ (chrome balls) æ¥é¢„æµ‹å¤šç‚¹å…‰ç…§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®¤å†…å…‰ç…§ä¼°è®¡ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ã€‚å…¶æ ¸å¿ƒçªç ´åœ¨äºå®ç°äº†åœ¨é‡å¤– (in-the-wild) è§†é¢‘ä¸­ä¿æŒé«˜åº¦çš„æ—¶ç©ºä¸€è‡´æ€§ï¼Œè§£å†³äº†ä»¥å¾€ç ”ç©¶ä¸­éš¾ä»¥å¤„ç†çš„åŠ¨æ€å…‰ç…§é‡å»ºéš¾é¢˜ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "11 pages. Accepted by SIGGRAPH 2025 as Conference Paper",
      "pdf_url": "https://arxiv.org/pdf/2508.08384v1",
      "published_date": "2025-08-11 18:11:42 UTC",
      "updated_date": "2025-08-11 18:11:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:55:15.490176+00:00"
    },
    {
      "arxiv_id": "2508.08382v1",
      "title": "UrzaGPT: LoRA-Tuned Large Language Models for Card Selection in Collectible Card Games",
      "title_zh": "UrzaGPTï¼šé¢å‘é›†æ¢å¼å¡ç‰Œæ¸¸æˆé€‰å¡çš„ LoRA å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Timo Bertram"
      ],
      "abstract": "Collectible card games (CCGs) are a difficult genre for AI due to their partial observability, long-term decision-making, and evolving card sets. Due to this, current AI models perform vastly worse than human players at CCG tasks such as deckbuilding and gameplay. In this work, we introduce UrzaGPT, a domain-adapted large language model that recommends real-time drafting decisions in Magic: The Gathering. Starting from an open-weight LLM, we use Low-Rank Adaptation fine-tuning on a dataset of annotated draft logs. With this, we leverage the language modeling capabilities of LLM, and can quickly adapt to different expansions of the game. We benchmark UrzaGPT in comparison to zero-shot LLMs and the state-of-the-art domain-specific model. Untuned, small LLMs like Llama-3-8B are completely unable to draft, but the larger GPT-4o achieves a zero-shot performance of 43%. Using UrzaGPT to fine-tune smaller models, we achieve an accuracy of 66.2% using only 10,000 steps. Despite this not reaching the capability of domain-specific models, we show that solely using LLMs to draft is possible and conclude that using LLMs can enable performant, general, and update-friendly drafting AIs in the future.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é›†æ¢å¼å¡ç‰Œæ¸¸æˆ(CCGs)ä¸­ç”±äºä¿¡æ¯éƒ¨åˆ†å¯è§‚å¯Ÿã€é•¿æœŸå†³ç­–ä»¥åŠå¡æ± ä¸æ–­æ¼”å˜è€Œå¯¼è‡´çš„AIæ¨¡å‹æ€§èƒ½æ¬ ä½³é—®é¢˜ï¼Œæå‡ºäº†UrzaGPTã€‚UrzaGPTæ˜¯ä¸€ç§é¢†åŸŸè‡ªé€‚åº”çš„å¤§è¯­è¨€æ¨¡å‹(LLM)ï¼Œæ—¨åœ¨ä¸ºã€Šé­”æ³•ï¼šé£äº‘ä¼šã€‹(Magic: The Gathering)æä¾›å®æ—¶çš„è½®æŠ½(drafting)å†³ç­–å»ºè®®ã€‚ç ”ç©¶è€…é€šè¿‡ä½ç§©è‡ªé€‚åº”(LoRA)å¾®è°ƒæŠ€æœ¯ï¼Œåœ¨æ ‡æ³¨çš„è½®æŠ½æ—¥å¿—æ•°æ®é›†ä¸Šå¯¹å¼€æºLLMè¿›è¡Œä¼˜åŒ–ï¼Œä½¿å…¶èƒ½å¤Ÿåˆ©ç”¨LLMçš„è¯­è¨€å»ºæ¨¡èƒ½åŠ›å¹¶å¿«é€Ÿé€‚åº”æ¸¸æˆæ‰©å±•ã€‚å®éªŒå¯¹æ¯”æ˜¾ç¤ºï¼Œæœªå¾®è°ƒçš„å°å‹æ¨¡å‹å¦‚Llama-3-8Bå®Œå…¨æ— æ³•èƒœä»»ï¼Œè€ŒGPT-4oåœ¨é›¶æ ·æœ¬(zero-shot)ä»»åŠ¡ä¸‹ä»…è¾¾åˆ°43%çš„å‡†ç¡®ç‡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç»è¿‡å¾®è°ƒçš„UrzaGPTåœ¨ä»…10,000æ­¥è®­ç»ƒåä¾¿å®ç°äº†66.2%çš„å‡†ç¡®ç‡ã€‚è¯¥å·¥ä½œè¯æ˜äº†ä»…ä½¿ç”¨LLMè¿›è¡Œè½®æŠ½å†³ç­–çš„å¯è¡Œæ€§ï¼Œä¸ºæœªæ¥å¼€å‘é«˜æ€§èƒ½ã€é€šç”¨ä¸”æ˜“äºæ›´æ–°çš„å¡ç‰Œæ¸¸æˆAIå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08382v1",
      "published_date": "2025-08-11 18:09:15 UTC",
      "updated_date": "2025-08-11 18:09:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:55:28.390151+00:00"
    },
    {
      "arxiv_id": "2508.08370v2",
      "title": "The DNA of nuclear models: How AI predicts nuclear masses",
      "title_zh": "åŸå­æ ¸æ¨¡å‹çš„â€œDNAâ€ï¼šäººå·¥æ™ºèƒ½å¦‚ä½•é¢„æµ‹åŸå­æ ¸è´¨é‡",
      "authors": [
        "Kate A. Richardson",
        "Sokratis Trifinopoulos",
        "Mike Williams"
      ],
      "abstract": "Obtaining high-precision predictions of nuclear masses, or equivalently nuclear binding energies, $E_b$, remains an important goal in nuclear-physics research. Recently, many AI-based tools have shown promising results on this task, some achieving precision that surpasses the best physics models. However, the utility of these AI models remains in question given that predictions are only useful where measurements do not exist, which inherently requires extrapolation away from the training (and testing) samples. Since AI models are largely black boxes, the reliability of such an extrapolation is difficult to assess. We present an AI model that not only achieves cutting-edge precision for $E_b$, but does so in an interpretable manner. For example, we find that (and explain why) the most important dimensions of its internal representation form a double helix, where the analog of the hydrogen bonds in DNA here link the number of protons and neutrons found in the most stable nucleus of each isotopic chain. Furthermore, we show that the AI prediction of $E_b$ can be factorized and ordered hierarchically, with the most important terms corresponding to well-known symbolic models (such as the famous liquid drop). Remarkably, the improvement of the AI model over symbolic ones can almost entirely be attributed to an observation made by Jaffe in 1969 based on the structure of most known nuclear ground states. The end result is a fully interpretable data-driven model of nuclear masses based on physics deduced by AI.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ ¸ç‰©ç†ä¸­é«˜ç²¾åº¦æ ¸è´¨é‡é¢„æµ‹(nuclear masses)çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§å…·æœ‰é«˜åº¦å¯è§£é‡Šæ€§çš„AIæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸAIæ¨¡å‹ä½œä¸ºâ€œé»‘ç›’â€åœ¨æ ·æœ¬å¤–æ¨(extrapolation)æ—¶çš„å¯é æ€§é—®é¢˜ã€‚ç ”ç©¶å‘ç°è¯¥æ¨¡å‹çš„å†…éƒ¨è¡¨å¾(internal representation)æœ€é‡è¦çš„ç»´åº¦å‘ˆç°å‡ºåŒèºæ—‹ç»“æ„ï¼Œå…¶ä¸­è´¨å­æ•°å’Œä¸­å­æ•°çš„å…³è”æ–¹å¼ç±»ä¼¼äºDNAä¸­çš„æ°¢é”®ï¼Œæ­ç¤ºäº†å„åŒä½ç´ é“¾ä¸­æœ€ç¨³å®šæ ¸ç´ çš„ç»“æ„è§„å¾‹ã€‚æ­¤å¤–ï¼Œè¯¥AIæ¨¡å‹å¯¹æ ¸ç»“åˆèƒ½(binding energies, $E_b$)çš„é¢„æµ‹å¯ä»¥è¿›è¡Œå±‚æ¬¡åŒ–åˆ†è§£ï¼Œå…¶æ ¸å¿ƒé¡¹ä¸æ¶²æ»´æ¨¡å‹(liquid drop model)ç­‰çŸ¥åç¬¦å·æ¨¡å‹(symbolic models)é«˜åº¦å¯¹åº”ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¡¨æ˜ï¼Œè¯¥AIæ¨¡å‹ç›¸æ¯”äºä¼ ç»Ÿç¬¦å·æ¨¡å‹çš„æ€§èƒ½æå‡ï¼Œå‡ ä¹å®Œå…¨å¯ä»¥å½’å› äºå…¶æ•æ‰åˆ°äº†Jaffeåœ¨1969å¹´åŸºäºæ ¸åŸºæ€ç»“æ„æ‰€æå‡ºçš„ç‰©ç†è§‚å¯Ÿã€‚æœ€ç»ˆï¼Œè¯¥å·¥ä½œæˆåŠŸæ„å»ºäº†ä¸€ä¸ªå®Œå…¨å¯è§£é‡Šçš„æ•°æ®é©±åŠ¨æ ¸è´¨é‡æ¨¡å‹ï¼Œå±•ç¤ºäº†AIåœ¨æ¨å¯¼ç‰©ç†è§„å¾‹æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "nucl-th",
        "cs.AI",
        "cs.LG",
        "nucl-ex"
      ],
      "primary_category": "nucl-th",
      "comment": "19 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.08370v2",
      "published_date": "2025-08-11 18:00:17 UTC",
      "updated_date": "2025-09-30 14:29:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:55:19.589450+00:00"
    },
    {
      "arxiv_id": "2508.08353v1",
      "title": "Processing of synthetic data in AI development for healthcare and the definition of personal data in EU law",
      "title_zh": "åŒ»ç–—AIå¼€å‘ä¸­çš„åˆæˆæ•°æ®å¤„ç†ä¸æ¬§ç›Ÿæ³•å¾‹å¯¹ä¸ªäººæ•°æ®çš„å®šä¹‰",
      "authors": [
        "Vibeke Binz Vallevik",
        "Anne Kjersti C. Befring",
        "Severin Elvatun",
        "Jan Franz Nygaard"
      ],
      "abstract": "Artificial intelligence (AI) has the potential to transform healthcare, but it requires access to health data. Synthetic data that is generated through machine learning models trained on real data, offers a way to share data while preserving privacy. However, uncertainties in the practical application of the General Data Protection Regulation (GDPR) create an administrative burden, limiting the benefits of synthetic data. Through a systematic analysis of relevant legal sources and an empirical study, this article explores whether synthetic data should be classified as personal data under the GDPR. The study investigates the residual identification risk through generating synthetic data and simulating inference attacks, challenging common perceptions of technical identification risk. The findings suggest synthetic data is likely anonymous, depending on certain factors, but highlights uncertainties about what constitutes reasonably likely risk. To promote innovation, the study calls for clearer regulations to balance privacy protection with the advancement of AI in healthcare.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŒ»ç–—ä¿å¥é¢†åŸŸäººå·¥æ™ºèƒ½(AI)å¼€å‘ä¸­åˆæˆæ•°æ®(Synthetic data)çš„å¤„ç†ï¼Œä»¥åŠåœ¨æ¬§ç›Ÿæ³•å¾‹(EU law)ä¸‹ä¸ªäººæ•°æ®(Personal data)çš„å®šä¹‰é—®é¢˜ã€‚é€šè¿‡å¯¹ç›¸å…³æ³•å¾‹æ¥æºçš„ç³»ç»Ÿåˆ†æå’Œå®è¯ç ”ç©¶ï¼Œæ–‡ç« æ·±å…¥æ¢è®¨äº†åˆæˆæ•°æ®åœ¨ã€Šé€šç”¨æ•°æ®ä¿æŠ¤æ¡ä¾‹ã€‹(GDPR)ä¸‹æ˜¯å¦åº”è¢«å½’ç±»ä¸ºä¸ªäººæ•°æ®ã€‚ç ”ç©¶äººå‘˜é€šè¿‡ç”Ÿæˆåˆæˆæ•°æ®å¹¶æ¨¡æ‹Ÿæ¨ç†æ”»å‡»(Inference attacks)æ¥è¯„ä¼°æ®‹ä½™è¯†åˆ«é£é™©(Residual identification risk)ï¼Œä»è€ŒæŒ‘æˆ˜äº†å…³äºæŠ€æœ¯è¯†åˆ«é£é™©çš„æ™®éè®¤çŸ¥ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶åˆæˆæ•°æ®åœ¨ç‰¹å®šæ¡ä»¶ä¸‹ææœ‰å¯èƒ½å±äºåŒ¿åæ•°æ®(Anonymous data)ï¼Œä½†åœ¨ç•Œå®šâ€œåˆç†å¯èƒ½çš„é£é™©â€æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—çš„ä¸ç¡®å®šæ€§ã€‚æœ€åï¼Œè¯¥ç ”ç©¶å‘¼ååˆ¶å®šæ›´æ¸…æ™°çš„ç›‘ç®¡å‡†åˆ™ï¼Œä»¥å¹³è¡¡éšç§ä¿æŠ¤ä¸åŒ»ç–—AIçš„å‘å±•ï¼Œä»è€Œé™ä½è¡Œæ”¿è´Ÿæ‹…å¹¶æ¨åŠ¨è¡Œä¸šåˆ›æ–°ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "55 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.08353v1",
      "published_date": "2025-08-11 17:59:06 UTC",
      "updated_date": "2025-08-11 17:59:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:55:23.986619+00:00"
    },
    {
      "arxiv_id": "2508.08244v2",
      "title": "Cut2Next: Generating Next Shot via In-Context Tuning",
      "title_zh": "Cut2Nextï¼šåŸºäºä¸Šä¸‹æ–‡å¾®è°ƒçš„ä¸‹ä¸€é•œå¤´ç”Ÿæˆ",
      "authors": [
        "Jingwen He",
        "Hongbo Liu",
        "Jiajun Li",
        "Ziqi Huang",
        "Yu Qiao",
        "Wanli Ouyang",
        "Ziwei Liu"
      ],
      "abstract": "Effective multi-shot generation demands purposeful, film-like transitions and strict cinematic continuity. Current methods, however, often prioritize basic visual consistency, neglecting crucial editing patterns (e.g., shot/reverse shot, cutaways) that drive narrative flow for compelling storytelling. This yields outputs that may be visually coherent but lack narrative sophistication and true cinematic integrity. To bridge this, we introduce Next Shot Generation (NSG): synthesizing a subsequent, high-quality shot that critically conforms to professional editing patterns while upholding rigorous cinematic continuity. Our framework, Cut2Next, leverages a Diffusion Transformer (DiT). It employs in-context tuning guided by a novel Hierarchical Multi-Prompting strategy. This strategy uses Relational Prompts to define overall context and inter-shot editing styles. Individual Prompts then specify per-shot content and cinematographic attributes. Together, these guide Cut2Next to generate cinematically appropriate next shots. Architectural innovations, Context-Aware Condition Injection (CACI) and Hierarchical Attention Mask (HAM), further integrate these diverse signals without introducing new parameters. We construct RawCuts (large-scale) and CuratedCuts (refined) datasets, both with hierarchical prompts, and introduce CutBench for evaluation. Experiments show Cut2Next excels in visual consistency and text fidelity. Crucially, user studies reveal a strong preference for Cut2Next, particularly for its adherence to intended editing patterns and overall cinematic continuity, validating its ability to generate high-quality, narratively expressive, and cinematically coherent subsequent shots.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰å¤šé•œå¤´ç”Ÿæˆä¸­ç¼ºä¹ç”µå½±æ„Ÿè¿‡æ¸¡å’Œä¸“ä¸šå‰ªè¾‘é€»è¾‘çš„é—®é¢˜ï¼Œæå‡ºäº†Cut2Nextæ¡†æ¶ï¼Œå¹¶å®šä¹‰äº†Next Shot Generation (NSG)ä»»åŠ¡ï¼Œæ—¨åœ¨åˆæˆç¬¦åˆç”µå½±è¿ç»­æ€§çš„é«˜è´¨é‡åç»­é•œå¤´ã€‚Cut2NextåŸºäºDiffusion Transformer (DiT)æ¶æ„ï¼Œé‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„Hierarchical Multi-Promptingç­–ç•¥è¿›è¡ŒIn-Context Tuningã€‚è¯¥ç­–ç•¥é€šè¿‡Relational Promptså®šä¹‰æ•´ä½“èƒŒæ™¯ä¸é•œå¤´é—´çš„å‰ªè¾‘é£æ ¼ï¼Œå¹¶åˆ©ç”¨Individual PromptsæŒ‡å®šå•é•œå¤´çš„æ‘„å½±å±æ€§ä¸å†…å®¹ï¼Œä»è€Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆå…·æœ‰å™äº‹è¡¨ç°åŠ›çš„åç»­ç”»é¢ã€‚ä¸ºäº†åœ¨ä¸å¢åŠ å‚æ•°çš„æƒ…å†µä¸‹é«˜æ•ˆæ•´åˆå¤šé‡ä¿¡å·ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†Context-Aware Condition Injection (CACI)å’ŒHierarchical Attention Mask (HAM)æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜æ„å»ºäº†RawCutsä¸CuratedCutså¤§è§„æ¨¡æ•°æ®é›†ä»¥åŠè¯„ä¼°åŸºå‡†CutBenchã€‚å®éªŒåŠç”¨æˆ·è°ƒç ”ç»“æœæ˜¾ç¤ºï¼ŒCut2Nextåœ¨è§†è§‰ä¸€è‡´æ€§ã€æ–‡æœ¬å¿ å®åº¦ä»¥åŠå‰ªè¾‘æ¨¡å¼çš„éµå¾ªä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç”Ÿæˆç¬¦åˆç”µå½±è‰ºæœ¯æ ‡å‡†ä¸”å™äº‹è¿è´¯çš„é•œå¤´ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08244v2",
      "published_date": "2025-08-11 17:56:59 UTC",
      "updated_date": "2025-08-12 12:41:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:55:30.292978+00:00"
    },
    {
      "arxiv_id": "2508.08237v3",
      "title": "VGGSounder: Audio-Visual Evaluations for Foundation Models",
      "title_zh": "VGGSounderï¼šé¢å‘åŸºç¡€æ¨¡å‹çš„éŸ³è§†é¢‘è¯„ä¼°",
      "authors": [
        "Daniil Zverev",
        "ThaddÃ¤us Wiedemer",
        "Ameya Prabhu",
        "Matthias Bethge",
        "Wieland Brendel",
        "A. Sophia Koepke"
      ],
      "abstract": "The emergence of audio-visual foundation models underscores the importance of reliably assessing their multi-modal understanding. The VGGSound dataset is commonly used as a benchmark for evaluation audio-visual classification. However, our analysis identifies several limitations of VGGSound, including incomplete labelling, partially overlapping classes, and misaligned modalities. These lead to distorted evaluations of auditory and visual capabilities. To address these limitations, we introduce VGGSounder, a comprehensively re-annotated, multi-label test set that extends VGGSound and is specifically designed to evaluate audio-visual foundation models. VGGSounder features detailed modality annotations, enabling precise analyses of modality-specific performance. Furthermore, we reveal model limitations by analysing performance degradation when adding another input modality with our new modality confusion metric.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†å¬åŸºç¡€æ¨¡å‹(Audio-visual foundation models)è¯„ä¼°çš„å¯é æ€§é—®é¢˜ï¼ŒæŒ‡å‡ºç›®å‰å¸¸ç”¨çš„VGGSoundåŸºå‡†æµ‹è¯•é›†å­˜åœ¨æ ‡ç­¾ä¸å®Œæ•´ã€ç±»åˆ«éƒ¨åˆ†é‡å ä»¥åŠæ¨¡æ€ä¸åŒ¹é…ç­‰å±€é™æ€§ï¼Œå¯¼è‡´å¯¹å¬è§‰å’Œè§†è§‰èƒ½åŠ›çš„è¯„ä¼°å‡ºç°åå·®ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œç ”ç©¶è€…æ¨å‡ºäº†VGGSounderï¼Œè¿™æ˜¯ä¸€ä¸ªç»è¿‡å…¨é¢é‡æ–°æ ‡æ³¨çš„å¤šæ ‡ç­¾æµ‹è¯•é›†ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°è§†å¬åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚VGGSounderä½œä¸ºVGGSoundçš„æ‰©å±•ç‰ˆæœ¬ï¼Œé€šè¿‡æä¾›è¯¦ç»†çš„æ¨¡æ€æ ‡æ³¨(Modality annotations)ï¼Œå®ç°äº†å¯¹ç‰¹å®šæ¨¡æ€è¡¨ç°çš„ç²¾ç¡®åˆ†æã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¼•å…¥äº†å…¨æ–°çš„æ¨¡æ€æ··æ·†æŒ‡æ ‡(Modality confusion metric)ï¼Œé€šè¿‡åˆ†æå¢åŠ è¾“å…¥æ¨¡æ€æ—¶çš„æ€§èƒ½é€€åŒ–æƒ…å†µï¼Œè¿›ä¸€æ­¥æ­ç¤ºäº†å½“å‰åŸºç¡€æ¨¡å‹çš„å±€é™æ€§ã€‚",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CV",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.MM",
      "comment": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.08237v3",
      "published_date": "2025-08-11 17:53:23 UTC",
      "updated_date": "2025-10-18 12:43:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:55:41.767713+00:00"
    },
    {
      "arxiv_id": "2508.08228v1",
      "title": "LL3M: Large Language 3D Modelers",
      "title_zh": "LL3Mï¼šå¤§è¯­è¨€3Då»ºæ¨¡å™¨",
      "authors": [
        "Sining Lu",
        "Guan Chen",
        "Nam Anh Dinh",
        "Itai Lang",
        "Ari Holtzman",
        "Rana Hanocka"
      ],
      "abstract": "We present LL3M, a multi-agent system that leverages pretrained large language models (LLMs) to generate 3D assets by writing interpretable Python code in Blender. We break away from the typical generative approach that learns from a collection of 3D data. Instead, we reformulate shape generation as a code-writing task, enabling greater modularity, editability, and integration with artist workflows. Given a text prompt, LL3M coordinates a team of specialized LLM agents to plan, retrieve, write, debug, and refine Blender scripts that generate and edit geometry and appearance. The generated code works as a high-level, interpretable, human-readable, well-documented representation of scenes and objects, making full use of sophisticated Blender constructs (e.g. B-meshes, geometry modifiers, shader nodes) for diverse, unconstrained shapes, materials, and scenes. This code presents many avenues for further agent and human editing and experimentation via code tweaks or procedural parameters. This medium naturally enables a co-creative loop in our system: agents can automatically self-critique using code and visuals, while iterative user instructions provide an intuitive way to refine assets. A shared code context across agents enables awareness of previous attempts, and a retrieval-augmented generation knowledge base built from Blender API documentation, BlenderRAG, equips agents with examples, types, and functions empowering advanced modeling operations and code correctness. We demonstrate the effectiveness of LL3M across diverse shape categories, style and material edits, and user-driven refinements. Our experiments showcase the power of code as a generative and interpretable medium for 3D asset creation. Our project page is at https://threedle.github.io/ll3m.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LL3Mï¼Œä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆmulti-agent systemï¼‰çš„æ¡†æ¶ï¼Œé€šè¿‡é¢„è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ Blender ä¸­ç¼–å†™å¯è§£é‡Šçš„ Python ä»£ç æ¥ç”Ÿæˆ 3D èµ„äº§ã€‚ä¸ä¼ ç»Ÿçš„ä»å¤§è§„æ¨¡ 3D æ•°æ®ä¸­å­¦ä¹ çš„ç”Ÿæˆæ–¹æ³•ä¸åŒï¼ŒLL3M å°†å½¢çŠ¶ç”Ÿæˆé‡æ–°å®šä¹‰ä¸ºä»£ç ç¼–å†™ä»»åŠ¡ï¼Œä»è€Œæ˜¾è‘—æå‡äº†ç”Ÿæˆè¿‡ç¨‹çš„æ¨¡å—åŒ–ç¨‹åº¦ã€å¯ç¼–è¾‘æ€§ä»¥åŠä¸ä¸“ä¸šè‰ºæœ¯å®¶å·¥ä½œæµçš„é›†æˆèƒ½åŠ›ã€‚ç³»ç»Ÿé€šè¿‡åè°ƒä¸“é—¨çš„æ™ºèƒ½ä½“æ‰§è¡Œè§„åˆ’ã€æ£€ç´¢ã€ç¼–å†™ã€è°ƒè¯•å’Œä¼˜åŒ–ä»»åŠ¡ï¼Œèƒ½å¤Ÿå……åˆ†åˆ©ç”¨ B-meshesã€geometry modifiers å’Œ shader nodes ç­‰é«˜çº§ Blender æ„é€ æ¥åˆ›ä½œå¤æ‚çš„å‡ ä½•ä½“å’Œæè´¨ã€‚ä¸ºäº†ç¡®ä¿ä»£ç çš„æ­£ç¡®æ€§ï¼Œç ”ç©¶è€…æ„å»ºäº†åŸºäº Blender API æ–‡æ¡£çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çŸ¥è¯†åº“ BlenderRAGï¼Œå¹¶å»ºç«‹äº†åŒ…å«è‡ªåŠ¨è‡ªæˆ‘æ‰¹åˆ¤å’Œç”¨æˆ·è¿­ä»£æŒ‡ä»¤çš„ååŒåˆ›ä½œå¾ªç¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLL3M åœ¨å¤šç§å½¢çŠ¶ç±»åˆ«ç”Ÿæˆã€é£æ ¼ç¼–è¾‘å’Œç”¨æˆ·é©±åŠ¨çš„ç»†åŒ–ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†ä»£ç ä½œä¸º 3D èµ„äº§åˆ›ä½œåª’ä»‹çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›ä¸å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI"
      ],
      "primary_category": "cs.GR",
      "comment": "Our project page is at https://threedle.github.io/ll3m",
      "pdf_url": "https://arxiv.org/pdf/2508.08228v1",
      "published_date": "2025-08-11 17:48:02 UTC",
      "updated_date": "2025-08-11 17:48:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:55:35.284015+00:00"
    },
    {
      "arxiv_id": "2508.08227v2",
      "title": "OMGSR: You Only Need One Mid-timestep Guidance for Real-World Image Super-Resolution",
      "title_zh": "OMGSRï¼šä»…éœ€ä¸€æ¬¡ä¸­é—´æ—¶é—´æ­¥å¼•å¯¼çš„çœŸå®åœºæ™¯å›¾åƒè¶…åˆ†è¾¨ç‡",
      "authors": [
        "Zhiqiang Wu",
        "Zhaomang Sun",
        "Tong Zhou",
        "Bingtao Fu",
        "Ji Cong",
        "Yitong Dong",
        "Huaqi Zhang",
        "Xuan Tang",
        "Mingsong Chen",
        "Xian Wei"
      ],
      "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) show promising potential in one-step Real-World Image Super-Resolution (Real-ISR). Current one-step Real-ISR methods typically inject the low-quality (LQ) image latent representation at the start or end timestep of the DDPM scheduler. Recent studies have begun to note that the LQ image latent and the pre-trained noisy latent representations are intuitively closer at a mid-timestep. However, a quantitative analysis of these latent representations remains lacking. Considering these latent representations can be decomposed into signal and noise, we propose a method based on the Signal-to-Noise Ratio (SNR) to pre-compute an average optimal mid-timestep for injection. To better approximate the pre-trained noisy latent representation, we further introduce the Latent Representation Refinement (LRR) loss via a LoRA-enhanced VAE encoder. We also fine-tune the backbone of the DDPM-based generative model using LoRA to perform one-step denoising at the average optimal mid-timestep. Based on these components, we present OMGSR, a GAN-based Real-ISR framework that employs a DDPM-based generative model as the generator and a DINOv3-ConvNeXt model with multi-level discriminator heads as the discriminator. We also propose the DINOv3-ConvNeXt DISTS (Dv3CD) loss, which is enhanced for structural perception at varying resolutions. Within the OMGSR framework, we develop OMGSR-S based on SD2.1-base. An ablation study confirms that our pre-computation strategy and LRR loss significantly improve the baseline. Comparative studies demonstrate that OMGSR-S achieves state-of-the-art performance across multiple metrics. Code is available at \\hyperlink{Github}{https://github.com/wuer5/OMGSR}.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†OMGSRï¼Œä¸€ä¸ªæ—¨åœ¨æå‡çœŸå®ä¸–ç•Œå›¾åƒè¶…åˆ†è¾¨ç‡(Real-World Image Super-Resolution, Real-ISR)æ€§èƒ½çš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œæ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰å•æ­¥æ‰©æ•£æ¦‚ç‡æ¨¡å‹(DDPM)åœ¨èµ·å§‹æˆ–ç»“æŸæ—¶é—´æ­¥æ³¨å…¥ä½è´¨é‡æ½œå˜é‡çš„å±€é™æ€§ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºä¿¡å™ªæ¯”(SNR)çš„æ–¹æ³•æ¥é¢„è®¡ç®—å¹³å‡æœ€ä¼˜ä¸­é—´æ—¶é—´æ­¥è¿›è¡Œå¼•å¯¼ã€‚ä¸ºä¼˜åŒ–æ½œå˜é‡è¡¨ç¤ºï¼Œç ”ç©¶å¼•å…¥äº†é€šè¿‡LoRAå¢å¼ºçš„VAEç¼–ç å™¨å®ç°çš„Latent Representation Refinement (LRR)æŸå¤±ï¼Œå¹¶åˆ©ç”¨LoRAå¾®è°ƒéª¨å¹²ç½‘ç»œä»¥å®ç°åœ¨æœ€ä¼˜ä¸­é—´æ—¶é—´æ­¥çš„å•æ­¥å»å™ªã€‚OMGSRæ¡†æ¶ç»“åˆäº†åŸºäºDDPMçš„ç”Ÿæˆå™¨ä¸DINOv3-ConvNeXtåˆ¤åˆ«å™¨ï¼Œå¹¶é…åˆæ–°æå‡ºçš„Dv3CDæŸå¤±æ¥å¢å¼ºä¸åŒåˆ†è¾¨ç‡ä¸‹çš„ç»“æ„æ„ŸçŸ¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºSD2.1å¼€å‘çš„OMGSR-Såœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†state-of-the-artæ°´å¹³ï¼Œå……åˆ†è¯æ˜äº†å…¶é¢„è®¡ç®—ç­–ç•¥å’ŒæŸå¤±å‡½æ•°åœ¨å›¾åƒä¿®å¤è´¨é‡ä¸Šçš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08227v2",
      "published_date": "2025-08-11 17:44:59 UTC",
      "updated_date": "2025-11-24 09:55:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:55:47.897587+00:00"
    },
    {
      "arxiv_id": "2508.08224v2",
      "title": "Capabilities of GPT-5 on Multimodal Medical Reasoning",
      "title_zh": "GPT-5 çš„å¤šæ¨¡æ€åŒ»å­¦æ¨ç†èƒ½åŠ›",
      "authors": [
        "Shansong Wang",
        "Mingzhe Hu",
        "Qiang Li",
        "Mojtaba Safari",
        "Xiaofeng Yang"
      ],
      "abstract": "Recent advances in large language models (LLMs) have enabled general-purpose systems to perform increasingly complex domain-specific reasoning without extensive fine-tuning. In the medical domain, decision-making often requires integrating heterogeneous information sources, including patient narratives, structured data, and medical images. This study positions GPT-5 as a generalist multimodal reasoner for medical decision support and systematically evaluates its zero-shot chain-of-thought reasoning performance on both text-based question answering and visual question answering tasks under a unified protocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20 against standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU medical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that GPT-5 consistently outperforms all baselines, achieving state-of-the-art accuracy across all QA benchmarks and delivering substantial gains in multimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and understanding scores by +29.26% and +26.18% over GPT-4o, respectively, and surpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in understanding. In contrast, GPT-4o remains below human expert performance in most dimensions. A representative case study demonstrates GPT-5's ability to integrate visual and textual cues into a coherent diagnostic reasoning chain, recommending appropriate high-stakes interventions. Our results show that, on these controlled multimodal reasoning benchmarks, GPT-5 moves from human-comparable to above human-expert performance. This improvement may substantially inform the design of future clinical decision-support systems.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†GPT-5ä½œä¸ºé€šç”¨å¤šæ¨¡æ€æ¨ç†å™¨åœ¨åŒ»ç–—å†³ç­–æ”¯æŒä¸­çš„æ€§èƒ½ï¼Œé‡ç‚¹è€ƒå¯Ÿå…¶åœ¨æ–‡æœ¬å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸­çš„é›¶æ ·æœ¬é“¾å¼æ€ç»´(zero-shot chain-of-thought)æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡åœ¨MedQAã€MedXpertQAã€MMLUåŒ»å­¦å­é›†ä»¥åŠUSMLEç­‰æ ‡å‡†åŒ–åŒ»å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„å¯¹æ¯”ï¼ŒGPT-5åœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡è¶…è¶Šäº†GPT-4oç­‰åŸºå‡†æ¨¡å‹ï¼Œè¾¾åˆ°äº†State-of-the-Art (SOTA)æ°´å¹³ã€‚ç‰¹åˆ«æ˜¯åœ¨MedXpertQA MMä»»åŠ¡ä¸­ï¼ŒGPT-5çš„æ¨ç†å’Œç†è§£å¾—åˆ†æ¯”GPT-4oåˆ†åˆ«é«˜å‡º29.26%å’Œ26.18%ï¼Œå¹¶æ˜¾è‘—ä¼˜äºæ‰§ç…§å‰æ°´å¹³çš„äººç±»ä¸“å®¶ã€‚ç ”ç©¶é€šè¿‡æ¡ˆä¾‹å±•ç¤ºäº†GPT-5æ•´åˆè§†è§‰ä¸æ–‡æœ¬çº¿ç´¢ç”Ÿæˆè¿è´¯è¯Šæ–­æ¨ç†é“¾çš„èƒ½åŠ›ï¼Œè¯æ˜å…¶åœ¨å—æ§å¤šæ¨¡æ€æ¨ç†ä¸­å·²å®ç°ä»äººç±»æ°´å¹³å‘è¶…è¶Šäººç±»ä¸“å®¶æ°´å¹³çš„è·¨è¶Šã€‚è¿™ä¸€è¿›å±•ä¸ºæœªæ¥ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿçš„è®¾è®¡æä¾›äº†é‡è¦ä¾æ®ï¼Œå±•ç¤ºäº†å¤šæ¨¡æ€å¤§æ¨¡å‹åœ¨å¤„ç†å¤æ‚åŒ»ç–—æ¨ç†ä»»åŠ¡æ—¶çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Corrected some typos",
      "pdf_url": "https://arxiv.org/pdf/2508.08224v2",
      "published_date": "2025-08-11 17:43:45 UTC",
      "updated_date": "2025-08-13 05:32:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:55:44.156432+00:00"
    },
    {
      "arxiv_id": "2508.08222v2",
      "title": "Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent",
      "title_zh": "å¤šå¤´ Transformer å¯è¯æ˜èƒ½å¤Ÿé€šè¿‡æ¢¯åº¦ä¸‹é™å­¦ä¹ ç¬¦å·åŒ–å¤šæ­¥æ¨ç†",
      "authors": [
        "Tong Yang",
        "Yu Huang",
        "Yingbin Liang",
        "Yuejie Chi"
      ],
      "abstract": "Transformers have demonstrated remarkable capabilities in multi-step reasoning tasks. However, understandings of the underlying mechanisms by which they acquire these abilities through training remain limited, particularly from a theoretical standpoint. This work investigates how transformers learn to solve symbolic multi-step reasoning problems through chain-of-thought processes, focusing on path-finding in trees. We analyze two intertwined tasks: a backward reasoning task, where the model outputs a path from a goal node to the root, and a more complex forward reasoning task, where the model implements two-stage reasoning by first identifying the goal-to-root path and then reversing it to produce the root-to-goal path. Our theoretical analysis, grounded in the dynamics of gradient descent, shows that trained one-layer transformers can provably solve both tasks with generalization guarantees to unseen trees. In particular, our multi-phase training dynamics for forward reasoning elucidate how different attention heads learn to specialize and coordinate autonomously to solve the two subtasks in a single autoregressive path. These results provide a mechanistic explanation of how trained transformers can implement sequential algorithmic procedures. Moreover, they offer insights into the emergence of reasoning abilities, suggesting that when tasks are structured to take intermediate chain-of-thought steps, even shallow multi-head transformers can effectively solve problems that would otherwise require deeper architectures.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»ç†è®ºè§’åº¦æ¢è®¨äº† Transformers å¦‚ä½•é€šè¿‡æ¢¯åº¦ä¸‹é™(Gradient Descent)å­¦ä¹ è§£å†³ç¬¦å·åŒ–å¤šæ­¥æ¨ç†é—®é¢˜ï¼Œé‡ç‚¹å…³æ³¨æ ‘ç»“æ„ä¸­çš„è·¯å¾„å¯»æ‰¾ä»»åŠ¡ã€‚é€šè¿‡åˆ†æåå‘æ¨ç†å’ŒåŒé˜¶æ®µå‰å‘æ¨ç†ä»»åŠ¡ï¼Œç ”ç©¶è¯æ˜äº†ç»è¿‡è®­ç»ƒçš„å•å±‚ Transformers åœ¨å¤„ç†æœªè§è¿‡çš„æ ‘ç»“æ„æ—¶å…·æœ‰å¯è¯æ˜çš„æ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†åœ¨å¤šé˜¶æ®µè®­ç»ƒåŠ¨æ€ä¸‹ï¼Œä¸åŒçš„æ³¨æ„åŠ›å¤´(Attention Heads)å¦‚ä½•è‡ªåŠ¨å®ç°ä¸“ä¸šåŒ–åˆ†å·¥ä¸åä½œï¼Œä»è€Œåœ¨å•ä¸ªè‡ªå›å½’è·¯å¾„ä¸­æ‰§è¡Œå¤æ‚çš„é¡ºåºç®—æ³•ç¨‹åºã€‚è¿™äº›å‘ç°ä¸º Transformers å†…éƒ¨å®ç°é€»è¾‘æ¨ç†çš„æœºåˆ¶æä¾›äº†åˆç†è§£é‡Šï¼Œå¹¶è¡¨æ˜é“¾å¼æ€ç»´(Chain-of-Thought)è¿‡ç¨‹èƒ½ä½¿æµ…å±‚æ¶æ„æœ‰æ•ˆè§£å†³åŸæœ¬éœ€è¦æ·±å±‚æ¨¡å‹æ‰èƒ½å¤„ç†çš„å¤æ‚é—®é¢˜ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "math.OC",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.08222v2",
      "published_date": "2025-08-11 17:40:47 UTC",
      "updated_date": "2025-12-07 17:49:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:56:01.151828+00:00"
    },
    {
      "arxiv_id": "2508.08211v2",
      "title": "SAEMark: Steering Personalized Multilingual LLM Watermarks with Sparse Autoencoders",
      "title_zh": "SAEMarkï¼šåˆ©ç”¨ç¨€ç–è‡ªç¼–ç å™¨å¼•å¯¼ä¸ªæ€§åŒ–å¤šè¯­è¨€å¤§è¯­è¨€æ¨¡å‹æ°´å°",
      "authors": [
        "Zhuohao Yu",
        "Xingru Jiang",
        "Weizheng Gu",
        "Yidong Wang",
        "Qingsong Wen",
        "Shikun Zhang",
        "Wei Ye"
      ],
      "abstract": "Watermarking LLM-generated text is critical for content attribution and misinformation prevention. However, existing methods compromise text quality, require white-box model access and logit manipulation. These limitations exclude API-based models and multilingual scenarios. We propose SAEMark, a general framework for post-hoc multi-bit watermarking that embeds personalized messages solely via inference-time, feature-based rejection sampling without altering model logits or requiring training. Our approach operates on deterministic features extracted from generated text, selecting outputs whose feature statistics align with key-derived targets. This framework naturally generalizes across languages and domains while preserving text quality through sampling LLM outputs instead of modifying. We provide theoretical guarantees relating watermark success probability and compute budget that hold for any suitable feature extractor. Empirically, we demonstrate the framework's effectiveness using Sparse Autoencoders (SAEs), achieving superior detection accuracy and text quality. Experiments across 4 datasets show SAEMark's consistent performance, with 99.7% F1 on English and strong multi-bit detection accuracy. SAEMark establishes a new paradigm for scalable watermarking that works out-of-the-box with closed-source LLMs while enabling content attribution.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SAEMarkï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„æ–‡æœ¬è¿›è¡Œä¸ªæ€§åŒ–å¤šè¯­è¨€æ°´å°å¤„ç†çš„é€šç”¨æ¡†æ¶ã€‚ä¸ºäº†è§£å†³ç°æœ‰æ°´å°æŠ€æœ¯éœ€è¦ç™½ç›’æ¨¡å‹æƒé™ã€ä¿®æ”¹Logitä»¥åŠé™ä½æ–‡æœ¬è´¨é‡çš„å±€é™æ€§ï¼ŒSAEMarké‡‡ç”¨äº†ä¸€ç§åŸºäºç‰¹å¾çš„æ‹’ç»é‡‡æ ·ï¼ˆRejection Samplingï¼‰æœºåˆ¶ã€‚è¯¥æ–¹æ³•åœ¨æ¨ç†é˜¶æ®µé€šè¿‡é€‰æ‹©ç‰¹å¾ç»Ÿè®¡æ•°æ®ä¸å¯†é’¥ç›®æ ‡ä¸€è‡´çš„è¾“å‡ºå†…å®¹æ¥åµŒå…¥å¤šæ¯”ç‰¹ä¿¡æ¯ï¼Œæ— éœ€å¯¹æ¨¡å‹è¿›è¡Œä»»ä½•è®­ç»ƒæˆ–ç»“æ„ä¿®æ”¹ã€‚é€šè¿‡åˆ©ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSparse Autoencoders, SAEsï¼‰æå–ç¡®å®šæ€§ç‰¹å¾ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè‡ªç„¶åœ°æ‰©å±•åˆ°å¤šç§è¯­è¨€å’Œé¢†åŸŸï¼Œå¹¶æœ‰æ•ˆä¿æŒç”Ÿæˆæ–‡æœ¬çš„åŸå§‹è´¨é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSAEMarkåœ¨å››ä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ç¨³å¥ï¼Œåœ¨è‹±è¯­æµ‹è¯•ä¸­è¾¾åˆ°äº†99.7%çš„F1åˆ†æ•°ï¼Œå¹¶å…·å¤‡æé«˜çš„å¤šæ¯”ç‰¹æ£€æµ‹å‡†ç¡®ç‡ã€‚è¯¥æ–¹æ¡ˆä¸ºé—­æºæ¨¡å‹å’ŒåŸºäºAPIçš„è°ƒç”¨åœºæ™¯æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”å¼€ç®±å³ç”¨çš„å†…å®¹æº¯æºæ–°èŒƒå¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "24 pages, 12 figures, NeurIPS 2025, code available: https://zhuohaoyu.github.io/SAEMark",
      "pdf_url": "https://arxiv.org/pdf/2508.08211v2",
      "published_date": "2025-08-11 17:33:18 UTC",
      "updated_date": "2026-01-11 09:45:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:56:04.550581+00:00"
    },
    {
      "arxiv_id": "2508.08204v1",
      "title": "Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ—¶ä¸ç¡®å®šæ€§çš„äººç±»å¯¹é½ä¸æ ¡å‡†",
      "authors": [
        "Kyle Moore",
        "Jesse Roberts",
        "Daryl Watson"
      ],
      "abstract": "There has been much recent interest in evaluating large language models for uncertainty calibration to facilitate model control and modulate user trust. Inference time uncertainty, which may provide a real-time signal to the model or external control modules, is particularly important for applying these concepts to improve LLM-user experience in practice. While many of the existing papers consider model calibration, comparatively little work has sought to evaluate how closely model uncertainty aligns to human uncertainty. In this work, we evaluate a collection of inference-time uncertainty measures, using both established metrics and novel variations, to determine how closely they align with both human group-level uncertainty and traditional notions of model calibration. We find that numerous measures show evidence of strong alignment to human uncertainty, even despite the lack of alignment to human answer preference. For those successful metrics, we find moderate to strong evidence of model calibration in terms of both correctness correlation and distributional analysis.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­æ¨ç†æ—¶é—´ä¸ç¡®å®šæ€§ï¼ˆInference-time uncertaintyï¼‰çš„äººç±»å¯¹é½ä¸æ ¡å‡†é—®é¢˜ï¼Œæ—¨åœ¨é€šè¿‡å®æ—¶ä¿¡å·å¢å¼ºæ¨¡å‹æ§åˆ¶å¹¶è°ƒèŠ‚ç”¨æˆ·ä¿¡ä»»ã€‚ä½œè€…æŒ‡å‡ºï¼Œè™½ç„¶ç°æœ‰ç ”ç©¶å¤šå…³æ³¨ä¼ ç»Ÿçš„æ¨¡å‹æ ¡å‡†ï¼ˆModel calibrationï¼‰ï¼Œä½†å¾ˆå°‘æœ‰å·¥ä½œè¯„ä¼°æ¨¡å‹ä¸ç¡®å®šæ€§ä¸äººç±»ä¸ç¡®å®šæ€§ä¹‹é—´çš„å¯¹é½ç¨‹åº¦ã€‚è¯¥æ–‡è¯„ä¼°äº†ä¸€ç³»åˆ—æ—¢æœ‰çš„åŠåˆ›æ–°çš„æ¨ç†æ—¶é—´ä¸ç¡®å®šæ€§åº¦é‡æŒ‡æ ‡ï¼Œå¹¶åˆ†æäº†å®ƒä»¬ä¸äººç±»ç¾¤ä½“å±‚é¢ä¸ç¡®å®šæ€§ï¼ˆHuman group-level uncertaintyï¼‰çš„å…³è”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡è®¸å¤šæŒ‡æ ‡ä¸äººç±»çš„ç­”æ¡ˆåå¥½ç¼ºä¹å¯¹é½ï¼Œä½†å®ƒä»¬ä¸äººç±»çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥å±•ç°å‡ºå¼ºçƒˆçš„å¯¹é½è¶‹åŠ¿ã€‚å¯¹äºè¿™äº›æˆåŠŸçš„åº¦é‡æŒ‡æ ‡ï¼Œç ”ç©¶è¿›ä¸€æ­¥å‘ç°åœ¨æ­£ç¡®æ€§ç›¸å…³æ€§ï¼ˆCorrectness correlationï¼‰å’Œåˆ†å¸ƒåˆ†æï¼ˆDistributional analysisï¼‰æ–¹é¢å‡å­˜åœ¨æ˜¾è‘—çš„æ¨¡å‹æ ¡å‡†è¯æ®ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†æ¨ç†æ—¶é—´ä¸ç¡®å®šæ€§ä½œä¸ºå®æ—¶åé¦ˆæœºåˆ¶åœ¨æ”¹å–„å¤§è¯­è¨€æ¨¡å‹ç”¨æˆ·ä½“éªŒæ–¹é¢çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "preprint, under review",
      "pdf_url": "https://arxiv.org/pdf/2508.08204v1",
      "published_date": "2025-08-11 17:22:45 UTC",
      "updated_date": "2025-08-11 17:22:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:56:05.664095+00:00"
    },
    {
      "arxiv_id": "2508.08193v2",
      "title": "Street-Level AI: Are Large Language Models Ready for Real-World Judgments?",
      "title_zh": "åŸºå±‚ AIï¼šå¤§è¯­è¨€æ¨¡å‹æ˜¯å¦å·²å‡†å¤‡å¥½åº”å¯¹ç°å®ä¸–ç•Œçš„è¯„åˆ¤ï¼Ÿ",
      "authors": [
        "Gaurab Pokharel",
        "Shafkat Farabi",
        "Patrick J. Fowler",
        "Sanmay Das"
      ],
      "abstract": "A surge of recent work explores the ethical and societal implications of large-scale AI models that make \"moral\" judgments. Much of this literature focuses either on alignment with human judgments through various thought experiments or on the group fairness implications of AI judgments. However, the most immediate and likely use of AI is to help or fully replace the so-called street-level bureaucrats, the individuals deciding to allocate scarce social resources or approve benefits. There is a rich history underlying how principles of local justice determine how society decides on prioritization mechanisms in such domains. In this paper, we examine how well LLM judgments align with human judgments, as well as with socially and politically determined vulnerability scoring systems currently used in the domain of homelessness resource allocation. Crucially, we use real data on those needing services (maintaining strict confidentiality by only using local large models) to perform our analyses. We find that LLM prioritizations are extremely inconsistent in several ways: internally on different runs, between different LLMs, and between LLMs and the vulnerability scoring systems. At the same time, LLMs demonstrate qualitative consistency with lay human judgments in pairwise testing. Findings call into question the readiness of current generation AI systems for naive integration in high-stakes societal decision-making.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)åœ¨ç°å®ä¸–ç•Œå†³ç­–ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹â€œåŸºå±‚å®˜åƒšâ€(street-level bureaucrats)åœ¨ç¤¾ä¼šèµ„æºåˆ†é…ï¼ˆå¦‚æ— å®¶å¯å½’è€…èµ„æºåˆ†é…ï¼‰ä¸­çš„åº”ç”¨ç°çŠ¶ã€‚ç ”ç©¶è€…é€šè¿‡ä½¿ç”¨çœŸå®æœåŠ¡å¯¹è±¡æ•°æ®ï¼Œå¯¹æ¯”åˆ†æäº†LLMå†³ç­–ä¸äººç±»åˆ¤æ–­ä»¥åŠç°è¡Œç¤¾ä¼šæ”¿æ²»ç¡®å®šçš„è„†å¼±æ€§è¯„åˆ†ç³»ç»Ÿ(vulnerability scoring systems)ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMçš„ä¼˜å…ˆé¡ºåºåˆ†é…åœ¨å¤šä¸ªç»´åº¦ä¸Šè¡¨ç°å‡ºæå¤§çš„ä¸ä¸€è‡´æ€§ï¼ŒåŒ…æ‹¬æ¨¡å‹è‡ªèº«çš„å¤šæ¬¡è¿è¡Œã€ä¸åŒLLMä¹‹é—´ä»¥åŠLLMä¸ç°æœ‰è¯„åˆ†ç³»ç»Ÿä¹‹é—´çš„æ˜¾è‘—å·®å¼‚ã€‚å°½ç®¡LLMåœ¨æˆå¯¹æµ‹è¯•ä¸­å±•ç°å‡ºä¸æ™®é€šäººç±»åˆ¤æ–­åœ¨å®šæ€§ä¸Šçš„ä¸€è‡´æ€§ï¼Œä½†å…¶åœ¨åˆ†é…æœºåˆ¶ä¸Šçš„ä¸ç¨³å®šæ€§ä»ç„¶ååˆ†çªå‡ºã€‚åŸºäºè¿™äº›å‘ç°ï¼Œè¯¥ç ”ç©¶å¯¹å½“å‰AIç³»ç»Ÿåœ¨ç¼ºä¹æ·±åº¦è¯„ä¼°çš„æƒ…å†µä¸‹ï¼Œç›´æ¥æ•´åˆè¿›é«˜é£é™©ç¤¾ä¼šå†³ç­–é¢†åŸŸçš„æˆç†Ÿåº¦æå‡ºäº†ä¸¥å³»è´¨ç–‘ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "This work has been accepted for publication as a full paper at the AAAI/ACM Conference on AI, Ethics, and Society (AIES 2025)",
      "pdf_url": "https://arxiv.org/pdf/2508.08193v2",
      "published_date": "2025-08-11 17:12:55 UTC",
      "updated_date": "2025-09-04 14:42:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:56:08.763157+00:00"
    },
    {
      "arxiv_id": "2508.08180v2",
      "title": "RedDino: A foundation model for red blood cell analysis",
      "title_zh": "RedDinoï¼šé¢å‘çº¢ç»†èƒåˆ†æçš„åŸºåº§æ¨¡å‹",
      "authors": [
        "Luca Zedda",
        "Andrea Loddo",
        "Cecilia Di Ruberto",
        "Carsten Marr"
      ],
      "abstract": "Red blood cells (RBCs) are essential to human health, and their precise morphological analysis is important for diagnosing hematological disorders. Despite the promise of foundation models in medical diagnostics, comprehensive AI solutions for RBC analysis remain scarce. We present RedDino, a self-supervised foundation model designed for RBC image analysis. RedDino uses an RBC-specific adaptation of the DINOv2 self-supervised learning framework and is trained on a curated dataset of 1.25 million RBC images from diverse acquisition modalities and sources. Extensive evaluations show that RedDino outperforms existing state-of-the-art models on RBC shape classification. Through assessments including linear probing and nearest neighbor classification, we confirm its strong feature representations and generalization ability. Our main contributions are: (1) a foundation model tailored for RBC analysis, (2) ablation studies exploring DINOv2 configurations for RBC modeling, and (3) a detailed evaluation of generalization performance. RedDino addresses key challenges in computational hematology by capturing nuanced morphological features, advancing the development of reliable diagnostic tools. The source code and pretrained models for RedDino are available at https://github.com/Snarci/RedDino, and the pretrained models can be downloaded from our Hugging Face collection at https://huggingface.co/collections/Snarcy/reddino-689a13e29241d2e5690202fc",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RedDinoï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºçº¢ç»†èƒ(Red Blood Cells, RBCs)å›¾åƒåˆ†æçš„è‡ªç›‘ç£åŸºç¡€æ¨¡å‹(foundation model)ï¼Œæ—¨åœ¨æå‡çº¢ç»†èƒå½¢æ€åˆ†æåœ¨è¡€æ¶²ç—…è¯Šæ–­ä¸­çš„ç²¾ç¡®åº¦ã€‚RedDinoåŸºäºé’ˆå¯¹æ€§é€‚é…çš„DINOv2è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨æ¥è‡ªå¤šç§æ¨¡æ€å’Œæ¥æºçš„125ä¸‡å¼ çº¢ç»†èƒå›¾åƒæ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚å¹¿æ³›çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒRedDinoåœ¨çº¢ç»†èƒå½¢çŠ¶åˆ†ç±»ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å…ˆè¿›æ¨¡å‹ï¼Œé€šè¿‡çº¿æ€§æ¢æµ‹(linear probing)å’Œæœ€è¿‘é‚»åˆ†ç±»(nearest neighbor classification)å±•ç¤ºäº†å“è¶Šçš„ç‰¹å¾è¡¨ç¤ºä¸æ³›åŒ–èƒ½åŠ›ã€‚è¯¥ç ”ç©¶ä¸ä»…è´¡çŒ®äº†é¦–ä¸ªçº¢ç»†èƒé¢†åŸŸçš„åŸºç¡€æ¨¡å‹ï¼Œè¿˜é€šè¿‡æ¶ˆèå®éªŒæ¢è®¨äº†DINOv2åœ¨çº¢ç»†èƒå»ºæ¨¡ä¸­çš„æœ€ä¼˜é…ç½®ã€‚RedDinoèƒ½å¤Ÿç²¾å‡†æ•æ‰ç»†å¾®çš„å½¢æ€ç‰¹å¾ï¼Œä¸ºè®¡ç®—è¡€æ¶²å­¦é¢†åŸŸå¼€å‘å¯é çš„è¯Šæ–­å·¥å…·æä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08180v2",
      "published_date": "2025-08-11 16:59:31 UTC",
      "updated_date": "2025-08-22 07:57:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:56:18.263914+00:00"
    },
    {
      "arxiv_id": "2508.08177v2",
      "title": "MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision",
      "title_zh": "MedReasonerï¼šå¼ºåŒ–å­¦ä¹ é©±åŠ¨ä»ä¸´åºŠæ€ç»´åˆ°åƒç´ çº§ç²¾åº¦çš„æ¨ç†å®šä½",
      "authors": [
        "Zhonghao Yan",
        "Muxi Diao",
        "Yuxuan Yang",
        "Ruoyan Jing",
        "Jiayuan Xu",
        "Kaizhou Zhang",
        "Lele Yang",
        "Yanxi Liu",
        "Kongming Liang",
        "Zhanyu Ma"
      ],
      "abstract": "Accurately grounding regions of interest (ROIs) is critical for diagnosis and treatment planning in medical imaging. While multimodal large language models (MLLMs) combine visual perception with natural language, current medical-grounding pipelines still rely on supervised fine-tuning with explicit spatial hints, making them ill-equipped to handle the implicit queries common in clinical practice. This work makes three core contributions. We first define Unified Medical Reasoning Grounding (UMRG), a novel vision-language task that demands clinical reasoning and pixel-level grounding. Second, we release U-MRG-14K, a dataset of 14K samples featuring pixel-level masks alongside implicit clinical queries and reasoning traces, spanning 10 modalities, 15 super-categories, and 108 specific categories. Finally, we introduce MedReasoner, a modular framework that distinctly separates reasoning from segmentation: an MLLM reasoner is optimized with reinforcement learning, while a frozen segmentation expert converts spatial prompts into masks, with alignment achieved through format and accuracy rewards. MedReasoner achieves state-of-the-art performance on U-MRG-14K and demonstrates strong generalization to unseen clinical queries, underscoring the significant promise of reinforcement learning for interpretable medical grounding.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—å½±åƒä¸­éš¾ä»¥å¤„ç†ä¸´åºŠéšå¼æŸ¥è¯¢çš„é—®é¢˜ï¼Œæå‡ºäº† Unified Medical Reasoning Grounding (UMRG) è¿™ä¸€å…¨æ–°çš„è§†è§‰è¯­è¨€ä»»åŠ¡ï¼Œæ—¨åœ¨å°†ä¸´åºŠæ¨ç†ä¸åƒç´ çº§å®šä½ç›¸ç»“åˆã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿå‘å¸ƒäº† U-MRG-14K æ•°æ®é›†ï¼ŒåŒ…å« 1.4 ä¸‡ä¸ªå…·æœ‰åƒç´ çº§æ©æ¨¡ã€éšå¼ä¸´åºŠæŸ¥è¯¢å’Œæ¨ç†è½¨è¿¹çš„æ ·æœ¬ï¼Œæ¶µç›– 10 ç§æ¨¡æ€å’Œ 108 ä¸ªå…·ä½“ç±»åˆ«ã€‚ç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº† MedReasoner æ¨¡å—åŒ–æ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) ä¼˜åŒ–å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLM) çš„æ¨ç†å™¨ï¼Œå¹¶ååŒå†·å†»çš„åˆ†å‰²ä¸“å®¶ (frozen segmentation expert) å®ç°ç²¾ç¡®çš„æ©æ¨¡ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMedReasoner åœ¨ U-MRG-14K ä¸Šè¾¾åˆ°äº† state-of-the-art çš„æ€§èƒ½ï¼Œå¹¶åœ¨æœªè§çš„ä¸´åºŠæŸ¥è¯¢ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™ä¸€æˆæœå……åˆ†å±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ åœ¨æå‡åŒ»ç–—å®šä½ä»»åŠ¡å¯è§£é‡Šæ€§ä¸ç²¾ç¡®åº¦æ–¹é¢çš„æ˜¾è‘—æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "AAAI2026",
      "pdf_url": "https://arxiv.org/pdf/2508.08177v2",
      "published_date": "2025-08-11 16:59:06 UTC",
      "updated_date": "2025-12-11 02:20:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:56:23.841232+00:00"
    },
    {
      "arxiv_id": "2508.08172v4",
      "title": "Neural Logic Networks for Interpretable Classification",
      "title_zh": "é¢å‘å¯è§£é‡Šåˆ†ç±»çš„ç¥ç»é€»è¾‘ç½‘ç»œ",
      "authors": [
        "Vincent Perreault",
        "Katsumi Inoue",
        "Richard Labib",
        "Alain Hertz"
      ],
      "abstract": "Traditional neural networks have an impressive classification performance, but what they learn cannot be inspected, verified or extracted. Neural Logic Networks on the other hand have an interpretable structure that enables them to learn a logical mechanism relating the inputs and outputs with AND and OR operations. We generalize these networks with NOT operations and biases that take into account unobserved data and develop a rigorous logical and probabilistic modeling in terms of concept combinations to motivate their use. We also propose a novel factorized IF-THEN rule structure for the model as well as a modified learning algorithm. Our method improves the state-of-the-art in Boolean networks discovery and is able to learn relevant, interpretable rules in tabular classification, notably on examples from the medical and industrial fields where interpretability has tangible value.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿç¥ç»ç½‘ç»œéš¾ä»¥è¢«æ£€æŸ¥æˆ–éªŒè¯çš„é—®é¢˜ï¼Œæå‡ºäº†å…·æœ‰å¯è§£é‡Šç»“æ„çš„ Neural Logic Networksï¼Œæ—¨åœ¨é€šè¿‡ AND å’Œ OR è¿ç®—å­¦ä¹ è¾“å…¥ä¸è¾“å‡ºé—´çš„é€»è¾‘å…³è”ã€‚ä½œè€…è¿›ä¸€æ­¥é€šè¿‡å¼•å…¥ NOT è¿ç®—å’Œ biases å¯¹ç½‘ç»œè¿›è¡Œæ³›åŒ–ï¼Œä»¥å¤„ç†æœªè§‚æµ‹æ•°æ®ï¼Œå¹¶åŸºäº concept combinations æä¾›äº†ä¸¥å¯†çš„é€»è¾‘ä¸æ¦‚ç‡å»ºæ¨¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§åˆ›æ–°çš„å› å­åŒ– IF-THEN è§„åˆ™ç»“æ„ä»¥åŠé…å¥—çš„æ”¹è¿›å­¦ä¹ ç®—æ³•ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ Boolean networks discovery ä»»åŠ¡ä¸­å–å¾—äº†å½“å‰æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶èƒ½åœ¨åŒ»ç–—å’Œå·¥ä¸šç­‰å¯¹å¯è§£é‡Šæ€§æœ‰é«˜åº¦è¦æ±‚çš„è¡¨æ ¼åˆ†ç±»åœºæ™¯ä¸­ï¼Œå­¦ä¹ åˆ°å…·æœ‰å®é™…åº”ç”¨ä»·å€¼ä¸”æ˜“äºç†è§£çš„ç›¸å…³è§„åˆ™ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.LG",
      "comment": "31 pages, 8 figures, pre-print, code available at https://github.com/VincentPerreault0/NeuralLogicNetworks",
      "pdf_url": "https://arxiv.org/pdf/2508.08172v4",
      "published_date": "2025-08-11 16:49:56 UTC",
      "updated_date": "2025-10-01 14:26:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:56:25.954300+00:00"
    },
    {
      "arxiv_id": "2508.08171v1",
      "title": "PyVeritas: On Verifying Python via LLM-Based Transpilation and Bounded Model Checking for C",
      "title_zh": "PyVeritasï¼šåŸºäº LLM è½¬è¯‘ä¸ C è¯­è¨€æœ‰ç•Œæ¨¡å‹æ£€æµ‹çš„ Python éªŒè¯",
      "authors": [
        "Pedro Orvalho",
        "Marta Kwiatkowska"
      ],
      "abstract": "Python has become the dominant language for general-purpose programming, yet it lacks robust tools for formal verification. In contrast, programmers working in languages such as C benefit from mature model checkers, for example CBMC, which enable exhaustive symbolic reasoning and fault localisation. The inherent complexity of Python, coupled with the verbosity and low-level nature of existing transpilers (e.g., Cython), have historically limited the applicability of formal verification to Python programs.\n  In this paper, we propose PyVeritas, a novel framework that leverages Large Language Models (LLMs) for high-level transpilation from Python to C, followed by bounded model checking and MaxSAT-based fault localisation in the generated C code. PyVeritas enables verification and bug localisation for Python code using existing model checking tools for C. Our empirical evaluation on two Python benchmarks demonstrates that LLM-based transpilation can achieve a high degree of accuracy, up to 80--90% for some LLMs, enabling effective development environment that supports assertion-based verification and interpretable fault diagnosis for small yet non-trivial Python programs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PyVeritas æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ Python è¯­è¨€åœ¨å½¢å¼åŒ–éªŒè¯ï¼ˆFormal verificationï¼‰é¢†åŸŸå·¥å…·åŒ®ä¹çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å°† Python ä»£ç é«˜å±‚è½¬è¯‘ï¼ˆTranspilationï¼‰ä¸º C ä»£ç ï¼Œä»è€Œèƒ½å¤Ÿå¤ç”¨ C è¯­è¨€æˆç†Ÿçš„ç•Œé™æ¨¡å‹æ£€æµ‹ï¼ˆBounded Model Checkingï¼‰å·¥å…·è¿›è¡Œç¬¦å·æ¨ç†ã€‚æ­¤å¤–ï¼ŒPyVeritas ç»“åˆäº†åŸºäº MaxSAT çš„æ•…éšœå®šä½ï¼ˆFault localisationï¼‰æŠ€æœ¯ï¼Œå®ç°äº†å¯¹ä»£ç çš„è‡ªåŠ¨åŒ–éªŒè¯ä¸é”™è¯¯è¯Šæ–­ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒåŸºäº LLMs çš„è½¬è¯‘åœ¨ç‰¹å®šåŸºå‡†æµ‹è¯•ä¸­å‡†ç¡®ç‡å¯è¾¾ 80-90%ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒéå¹³å‡¡ Python ç¨‹åºçš„æ–­è¨€éªŒè¯ï¼ˆAssertion-based verificationï¼‰å’Œå¯è§£é‡Šçš„æ•…éšœè¯Šæ–­ã€‚è¯¥ç ”ç©¶ä¸ºåˆ©ç”¨ç°æœ‰æˆç†Ÿçš„éªŒè¯å·¥å…·é“¾å¢å¼º Python è½¯ä»¶çš„å¯é æ€§æä¾›äº†å…¨æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "14 pages, 6 tables, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2508.08171v1",
      "published_date": "2025-08-11 16:49:07 UTC",
      "updated_date": "2025-08-11 16:49:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:56:20.452785+00:00"
    },
    {
      "arxiv_id": "2508.08163v2",
      "title": "LPI-RIT at LeWiDi-2025: Improving Distributional Predictions via Metadata and Loss Reweighting with DisCo",
      "title_zh": "LPI-RIT åœ¨ LeWiDi-2025ï¼šé€šè¿‡å…ƒæ•°æ®åŠåŸºäº DisCo çš„æŸå¤±é‡åŠ æƒæ”¹è¿›åˆ†å¸ƒé¢„æµ‹",
      "authors": [
        "Mandira Sawkar",
        "Samay U. Shetty",
        "Deepak Pandita",
        "Tharindu Cyril Weerasooriya",
        "Christopher M. Homan"
      ],
      "abstract": "The Learning With Disagreements (LeWiDi) 2025 shared task aims to model annotator disagreement through soft label distribution prediction and perspectivist evaluation, which focuses on modeling individual annotators. We adapt DisCo (Distribution from Context), a neural architecture that jointly models item-level and annotator-level label distributions, and present detailed analysis and improvements. In this paper, we extend DisCo by introducing annotator metadata embeddings, enhancing input representations, and multi-objective training losses to capture disagreement patterns better. Through extensive experiments, we demonstrate substantial improvements in both soft and perspectivist evaluation metrics across three datasets. We also conduct in-depth calibration and error analyses that reveal when and why disagreement-aware modeling improves. Our findings show that disagreement can be better captured by conditioning on annotator demographics and by optimizing directly for distributional metrics, yielding consistent improvements across datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹LeWiDi 2025ä»»åŠ¡ä¸­é€šè¿‡è½¯æ ‡ç­¾åˆ†å¸ƒé¢„æµ‹å’Œé€è§†ä¸»ä¹‰è¯„ä¼°(perspectivist evaluation)æ¥å»ºæ¨¡æ ‡æ³¨è€…åˆ†æ­§çš„é—®é¢˜è¿›è¡Œäº†æ¢ç´¢ã€‚ä½œè€…é€šè¿‡æ”¹è¿›DisCoï¼ˆDistribution from Contextï¼‰æ¶æ„ï¼Œå¼•å…¥äº†æ ‡æ³¨è€…å…ƒæ•°æ®åµŒå…¥(metadata embeddings)ä»¥å¢å¼ºè¾“å…¥è¡¨ç¤ºï¼Œå¹¶ç»“åˆäº†å¤šç›®æ ‡è®­ç»ƒæŸå¤±(multi-objective training losses)æ¥æ•æ‰å¤æ‚çš„åˆ†æ­§æ¨¡å¼ã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæ”¹è¿›åçš„æ¡†æ¶åœ¨è½¯è¯„ä¼°å’Œé€è§†ä¸»ä¹‰è¯„ä¼°æŒ‡æ ‡ä¸Šå‡å–å¾—äº†æ˜¾è‘—æå‡ã€‚é€šè¿‡æ·±å…¥çš„æ ¡å‡†å’Œé”™è¯¯åˆ†æï¼Œç ”ç©¶æ­ç¤ºäº†åˆ©ç”¨æ ‡æ³¨è€…äººå£ç»Ÿè®¡å­¦ä¿¡æ¯å’Œç›´æ¥ä¼˜åŒ–åˆ†å¸ƒæŒ‡æ ‡èƒ½æ›´ç²¾å‡†åœ°æ•æ‰åˆ†æ­§ã€‚è¯¥é¡¹å·¥ä½œå±•ç¤ºäº†ç»“åˆå…ƒæ•°æ®ä¸æŸå¤±é‡åŠ æƒæŠ€æœ¯åœ¨æå‡åˆ†å¸ƒé¢„æµ‹å‡†ç¡®æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤„ç†æ ‡æ³¨åˆ†æ­§æä¾›äº†é²æ£’çš„å»ºæ¨¡æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "To appear in Proceedings of the EMNLP 2025 Workshop on Learning with Disagreements (LeWiDi)",
      "pdf_url": "https://arxiv.org/pdf/2508.08163v2",
      "published_date": "2025-08-11 16:39:09 UTC",
      "updated_date": "2025-10-05 01:07:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:56:24.853343+00:00"
    },
    {
      "arxiv_id": "2508.08158v1",
      "title": "Can AI Explanations Make You Change Your Mind?",
      "title_zh": "äººå·¥æ™ºèƒ½è§£é‡Šèƒ½å¦è®©ä½ æ”¹å˜ä¸»æ„ï¼Ÿ",
      "authors": [
        "Laura Spillner",
        "Rachel Ringe",
        "Robert Porzel",
        "Rainer Malaka"
      ],
      "abstract": "In the context of AI-based decision support systems, explanations can help users to judge when to trust the AI's suggestion, and when to question it. In this way, human oversight can prevent AI errors and biased decision-making. However, this rests on the assumption that users will consider explanations in enough detail to be able to catch such errors. We conducted an online study on trust in explainable DSS, and were surprised to find that in many cases, participants spent little time on the explanation and did not always consider it in detail. We present an exploratory analysis of this data, investigating what factors impact how carefully study participants consider AI explanations, and how this in turn impacts whether they are open to changing their mind based on what the AI suggests.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨åŸºäºäººå·¥æ™ºèƒ½çš„å†³ç­–æ”¯æŒç³»ç»Ÿ(Decision Support Systems, DSS)ä¸­ï¼Œè§£é‡Š(explanations)å¦‚ä½•è¾…åŠ©ç”¨æˆ·è¯„ä¼°AIå»ºè®®çš„å¯é æ€§ï¼Œå¹¶å‘æŒ¥äººç±»ç›‘ç£ä»¥é˜²æ­¢AIé”™è¯¯å’Œåè§çš„ä½œç”¨ã€‚ä¼ ç»Ÿè§‚ç‚¹è®¤ä¸ºè§£é‡Šèƒ½å¼•å¯¼ç”¨æˆ·æ•æ‰AIçš„é”™è¯¯ï¼Œä½†è¯¥ç ”ç©¶é€šè¿‡åœ¨çº¿å®éªŒå‘ç°ï¼Œè®¸å¤šå‚ä¸è€…åœ¨å®é™…æ“ä½œä¸­ä»…èŠ±è´¹æå°‘æ—¶é—´æŸ¥çœ‹è§£é‡Šï¼Œå¹¶æœªè¿›è¡Œæ·±å…¥æ€è€ƒã€‚æœ¬æ–‡å¯¹æ­¤ç°è±¡è¿›è¡Œäº†æ¢ç´¢æ€§åˆ†æ(exploratory analysis)ï¼Œæ—¨åœ¨è¯†åˆ«å½±å“å‚ä¸è€…å®¡è§†AIè§£é‡Šä»”ç»†ç¨‹åº¦çš„å…³é”®å› ç´ ã€‚ç ”ç©¶è¿›ä¸€æ­¥åˆ†æäº†è¿™äº›å› ç´ å¦‚ä½•åè¿‡æ¥å½±å“ç”¨æˆ·æ ¹æ®AIå»ºè®®æ”¹å˜è‡ªèº«åˆ¤æ–­çš„æ„æ„¿ã€‚è¯¥å‘ç°æŒ‘æˆ˜äº†ç”¨æˆ·ä¼šå……åˆ†åˆ©ç”¨è§£é‡Šè¿›è¡Œå†³ç­–è¯„ä¼°çš„å‡è®¾ï¼Œä¸ºæœªæ¥è®¾è®¡æ›´æœ‰æ•ˆçš„äººæœºäº¤äº’ç•Œé¢æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "This paper was presented at the Explainable AI workshop at IJCAI 2025: https://sites.google.com/view/xai2025/proceedings",
      "pdf_url": "https://arxiv.org/pdf/2508.08158v1",
      "published_date": "2025-08-11 16:36:20 UTC",
      "updated_date": "2025-08-11 16:36:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:56:40.966057+00:00"
    },
    {
      "arxiv_id": "2508.08147v1",
      "title": "From Natural Language to Solver-Ready Power System Optimization: An LLM-Assisted, Validation-in-the-Loop Framework",
      "title_zh": "ä»è‡ªç„¶è¯­è¨€åˆ°å¯ç›´æ¥æ±‚è§£çš„ç”µåŠ›ç³»ç»Ÿä¼˜åŒ–ï¼šä¸€ç§å¤§è¯­è¨€æ¨¡å‹è¾…åŠ©çš„éªŒè¯åœ¨ç¯æ¡†æ¶",
      "authors": [
        "Yunkai Hu",
        "Tianqiao Zhao",
        "Meng Yue"
      ],
      "abstract": "This paper introduces a novel Large Language Models (LLMs)-assisted agent that automatically converts natural-language descriptions of power system optimization scenarios into compact, solver-ready formulations and generates corresponding solutions. In contrast to approaches that rely solely on LLM to produce solutions directly, the proposed method focuses on discovering a mathematically compatible formulation that can be efficiently solved by off-the-shelf optimization solvers. Directly using LLMs to produce solutions often leads to infeasible or suboptimal results, as these models lack the numerical precision and constraint-handling capabilities of established optimization solvers. The pipeline integrates a domain-aware prompt and schema with an LLM, enforces feasibility through systematic validation and iterative repair, and returns both solver-ready models and user-facing results. Using the unit commitment problem as a representative case study, the agent produces optimal or near-optimal schedules along with the associated objective costs. Results demonstrate that coupling the solver with task-specific validation significantly enhances solution reliability. This work shows that combining AI with established optimization frameworks bridges high-level problem descriptions and executable mathematical models, enabling more efficient decision-making in energy systems",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)è¾…åŠ©çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨å°†ç”µåŠ›ç³»ç»Ÿä¼˜åŒ–åœºæ™¯çš„è‡ªç„¶è¯­è¨€æè¿°è‡ªåŠ¨è½¬åŒ–ä¸ºå¯ç”±æ±‚è§£å™¨ç›´æ¥å¤„ç†çš„æ•°å­¦å»ºæ¨¡å½¢å¼ã€‚ä¸ç›´æ¥ä¾èµ– LLMs ç”Ÿæˆè§£çš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ¡†æ¶é€šè¿‡æ„å»ºæ•°å­¦å…¼å®¹çš„è¡¨è¿°ï¼Œè§£å†³äº†æ¨¡å‹åœ¨æ•°å€¼ç²¾åº¦å’Œçº¦æŸå¤„ç†ä¸Šçš„ä¸è¶³ã€‚å…¶æµç¨‹ç»“åˆäº†é¢†åŸŸæ„ŸçŸ¥æç¤º(Domain-aware prompt)ä¸æ¨¡å¼(Schema)ï¼Œå¹¶å¼•å…¥äº†ç³»ç»ŸåŒ–éªŒè¯å’Œè¿­ä»£ä¿®å¤(Iterative repair)çš„é—­ç¯æœºåˆ¶ä»¥ç¡®ä¿æ–¹æ¡ˆçš„å¯è¡Œæ€§ã€‚ä»¥æœºç»„ç»„åˆ(Unit commitment)é—®é¢˜ä¸ºä¾‹ï¼Œè¯¥æ™ºèƒ½ä½“èƒ½å¤Ÿäº§å‡ºæœ€ä¼˜æˆ–è¿‘ä¼˜çš„è°ƒåº¦æ–¹æ¡ˆåŠç›®æ ‡æˆæœ¬ã€‚å®éªŒç»“æœè¯æ˜ï¼Œå°†æ±‚è§£å™¨ä¸ç‰¹å®šä»»åŠ¡çš„éªŒè¯ç¯èŠ‚ç›¸ç»“åˆæ˜¾è‘—å¢å¼ºäº†ç»“æœçš„å¯é æ€§ã€‚è¯¥å·¥ä½œæˆåŠŸå¼¥åˆäº†é«˜å±‚é—®é¢˜æè¿°ä¸å¯æ‰§è¡Œæ•°å­¦æ¨¡å‹ä¹‹é—´çš„é¸¿æ²Ÿï¼Œä¸ºèƒ½æºç³»ç»Ÿçš„é«˜æ•ˆå†³ç­–æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08147v1",
      "published_date": "2025-08-11 16:22:57 UTC",
      "updated_date": "2025-08-11 16:22:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:56:57.092544+00:00"
    },
    {
      "arxiv_id": "2508.08144v1",
      "title": "COMponent-Aware Pruning for Accelerated Control Tasks in Latent Space Models",
      "title_zh": "éšç©ºé—´æ¨¡å‹ä¸­åŠ é€Ÿæ§åˆ¶ä»»åŠ¡çš„ç»„ä»¶æ„ŸçŸ¥å‰ªæ",
      "authors": [
        "Ganesh Sundaram",
        "Jonas Ulmen",
        "Amjad Haider",
        "Daniel GÃ¶rges"
      ],
      "abstract": "The rapid growth of resource-constrained mobile platforms, including mobile robots, wearable systems, and Internet-of-Things devices, has increased the demand for computationally efficient neural network controllers (NNCs) that can operate within strict hardware limitations. While deep neural networks (DNNs) demonstrate superior performance in control applications, their substantial computational complexity and memory requirements present significant barriers to practical deployment on edge devices. This paper introduces a comprehensive model compression methodology that leverages component-aware structured pruning to determine the optimal pruning magnitude for each pruning group, ensuring a balance between compression and stability for NNC deployment. Our approach is rigorously evaluated on Temporal Difference Model Predictive Control (TD-MPC), a state-of-the-art model-based reinforcement learning algorithm, with a systematic integration of mathematical stability guarantee properties, specifically Lyapunov criteria. The key contribution of this work lies in providing a principled framework for determining the theoretical limits of model compression while preserving controller stability. Experimental validation demonstrates that our methodology successfully reduces model complexity while maintaining requisite control performance and stability characteristics. Furthermore, our approach establishes a quantitative boundary for safe compression ratios, enabling practitioners to systematically determine the maximum permissible model reduction before violating critical stability properties, thereby facilitating the confident deployment of compressed NNCs in resource-limited environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç§»åŠ¨æœºå™¨äººå’Œç‰©è”ç½‘ç­‰èµ„æºå—é™å¹³å°å¯¹é«˜æ•ˆç¥ç»ç½‘ç»œæ§åˆ¶å™¨ (NNCs) çš„éœ€æ±‚ï¼Œæå‡ºäº†ä¸€ç§åä¸º COMponent-Aware Pruning çš„æ¨¡å‹å‹ç¼©æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç»„ä»¶æ„ŸçŸ¥çš„ç»“æ„åŒ–å‰ªæ (component-aware structured pruning) æ¥ç¡®å®šæ¯ä¸ªå‰ªæç»„çš„æœ€ä¼˜å‰ªæå¹…åº¦ï¼Œæ—¨åœ¨ç¡®ä¿ NNC éƒ¨ç½²æ—¶å‹ç¼©ç‡ä¸ç¨³å®šæ€§ä¹‹é—´çš„å¹³è¡¡ã€‚ç ”ç©¶åœ¨å…ˆè¿›çš„åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ç®—æ³• Temporal Difference Model Predictive Control (TD-MPC) ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå¹¶ç³»ç»Ÿæ€§åœ°é›†æˆäº† Lyapunov ç¨³å®šæ€§å‡†åˆ™ã€‚å…¶æ ¸å¿ƒè´¡çŒ®åœ¨äºæä¾›äº†ä¸€ä¸ªåŸåˆ™æ€§æ¡†æ¶ï¼Œç”¨äºåœ¨ä¿æŒæ§åˆ¶å™¨ç¨³å®šæ€§çš„åŒæ—¶ç¡®å®šæ¨¡å‹å‹ç¼©çš„ç†è®ºæé™ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ˜¾è‘—é™ä½æ¨¡å‹å¤æ‚åº¦çš„åŒæ—¶ï¼ŒæˆåŠŸç»´æŒäº†å¿…è¦çš„æ§åˆ¶æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ç¡®ç«‹äº†å®‰å…¨å‹ç¼©ç‡çš„å®šé‡è¾¹ç•Œï¼Œä½¿ä»ä¸šè€…èƒ½å¤Ÿç³»ç»Ÿåœ°ç¡®å®šä¸è¿åç¨³å®šæ€§å±æ€§çš„æœ€å¤§æ¨¡å‹ç¼©å‡é‡ã€‚è¿™ä¸ºåœ¨èµ„æºå—é™ç¯å¢ƒä¸­éƒ¨ç½²å‹ç¼©åçš„ NNCs æä¾›äº†ç†è®ºæ”¯æŒä¸å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "Submitted in: The 2026 IEEE/SICE International Symposium on System Integration (SII 2026)",
      "pdf_url": "https://arxiv.org/pdf/2508.08144v1",
      "published_date": "2025-08-11 16:16:51 UTC",
      "updated_date": "2025-08-11 16:16:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:57:09.084997+00:00"
    },
    {
      "arxiv_id": "2508.08139v2",
      "title": "Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models",
      "title_zh": "LLM èƒ½å¦æ£€æµ‹å…¶è™šæ„å†…å®¹ï¼Ÿä¸ç¡®å®šæ€§æ„ŸçŸ¥è¯­è¨€æ¨¡å‹ä¸­çš„å¯é æ€§è¯„ä¼°",
      "authors": [
        "Tianyi Zhou",
        "Johanne Medina",
        "Sanjay Chawla"
      ],
      "abstract": "Large Language Models (LLMs) are prone to generating fluent but incorrect content, known as confabulation, which poses increasing risks in multi-turn or agentic applications where outputs may be reused as context. In this work, we investigate how in-context information influences model behavior and whether LLMs can identify their unreliable responses. We propose a reliability estimation that leverages token-level uncertainty to guide the aggregation of internal model representations. Specifically, we compute aleatoric and epistemic uncertainty from output logits to identify salient tokens and aggregate their hidden states into compact representations for response-level reliability prediction. Through controlled experiments on open QA benchmarks, we find that correct in-context information improves both answer accuracy and model confidence, while misleading context often induces confidently incorrect responses, revealing a misalignment between uncertainty and correctness. Our probing-based method captures these shifts in model behavior and improves the detection of unreliable outputs across multiple open-source LLMs. These results underscore the limitations of direct uncertainty signals and highlight the potential of uncertainty-guided probing for reliability-aware generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯å¦èƒ½å¤Ÿè¯†åˆ«å…¶ç”Ÿæˆçš„è™šå‡å†…å®¹ï¼ˆConfabulationï¼‰ï¼Œå¹¶è°ƒæŸ¥äº†ä¸Šä¸‹æ–‡ä¿¡æ¯å¦‚ä½•å½±å“æ¨¡å‹è¡Œä¸ºã€‚ä½œè€…æå‡ºäº†ä¸€ç§åˆ©ç”¨æ ‡è®°çº§åˆ«ï¼ˆtoken-levelï¼‰ä¸ç¡®å®šæ€§æ¥æŒ‡å¯¼æ¨¡å‹å†…éƒ¨è¡¨ç¤ºèšåˆçš„å¯é æ€§ä¼°è®¡æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ä»è¾“å‡ºå¯¹æ•°ï¼ˆlogitsï¼‰ä¸­è®¡ç®—å¶ç„¶ä¸ç¡®å®šæ€§ï¼ˆAleatoric Uncertaintyï¼‰å’Œè®¤è¯†ä¸ç¡®å®šæ€§ï¼ˆEpistemic Uncertaintyï¼‰æ¥è¯†åˆ«æ˜¾è‘—æ ‡è®°ï¼Œå¹¶å°†å…¶éšè—çŠ¶æ€èšåˆä¸ºç´§å‡‘è¡¨ç¤ºï¼Œç”¨äºå“åº”çº§åˆ«çš„å¯é æ€§é¢„æµ‹ã€‚é€šè¿‡åœ¨å¼€æ”¾é—®ç­”åŸºå‡†ä¸Šçš„å®éªŒï¼Œç ”ç©¶å‘ç°æ­£ç¡®çš„ä¸Šä¸‹æ–‡ä¿¡æ¯èƒ½æé«˜å‡†ç¡®æ€§å’Œæ¨¡å‹ç½®ä¿¡åº¦ï¼Œè€Œè¯¯å¯¼æ€§ä¸Šä¸‹æ–‡å¸¸å¯¼è‡´â€œè‡ªä¿¡çš„é”™è¯¯â€å“åº”ï¼Œæ­ç¤ºäº†ä¸ç¡®å®šæ€§ä¸æ­£ç¡®æ€§ä¹‹é—´çš„å¤±è°ƒã€‚ç ”ç©¶æå‡ºçš„åŸºäºæ¢æµ‹ï¼ˆProbing-basedï¼‰çš„æ–¹æ³•æˆåŠŸæ•æ‰äº†æ¨¡å‹è¡Œä¸ºçš„å˜åŒ–ï¼Œå¹¶æ”¹å–„äº†åœ¨å¤šç§å¼€æº LLMs ä¸­å¯¹ä¸å¯é è¾“å‡ºçš„æ£€æµ‹èƒ½åŠ›ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†ç›´æ¥ä¸ç¡®å®šæ€§ä¿¡å·çš„å±€é™æ€§ï¼Œå¹¶è¯æ˜äº†ä¸ç¡®å®šæ€§å¼•å¯¼çš„æ¢æµ‹æŠ€æœ¯åœ¨å®ç°å¯é æ€§æ„ŸçŸ¥ç”Ÿæˆæ–¹é¢çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08139v2",
      "published_date": "2025-08-11 16:12:36 UTC",
      "updated_date": "2025-12-11 13:49:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:57:07.198975+00:00"
    },
    {
      "arxiv_id": "2508.08137v1",
      "title": "MuaLLM: A Multimodal Large Language Model Agent for Circuit Design Assistance with Hybrid Contextual Retrieval-Augmented Generation",
      "title_zh": "MuaLLMï¼šåŸºäºæ··åˆä¸Šä¸‹æ–‡æ£€ç´¢å¢å¼ºç”Ÿæˆçš„ç”µè·¯è®¾è®¡è¾…åŠ©å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“",
      "authors": [
        "Pravallika Abbineni",
        "Saoud Aldowaish",
        "Colin Liechty",
        "Soroosh Noorzad",
        "Ali Ghazizadeh",
        "Morteza Fayazi"
      ],
      "abstract": "Conducting a comprehensive literature review is crucial for advancing circuit design methodologies. However, the rapid influx of state-of-the-art research, inconsistent data representation, and the complexity of optimizing circuit design objectives make this task significantly challenging. In this paper, we propose MuaLLM, an open-source multimodal Large Language Model (LLM) agent for circuit design assistance that integrates a hybrid Retrieval-Augmented Generation (RAG) framework with an adaptive vector database of circuit design research papers. Unlike conventional LLMs, the MuaLLM agent employs a Reason + Act (ReAct) workflow for iterative reasoning, goal-setting, and multi-step information retrieval. It functions as a question-answering design assistant, capable of interpreting complex queries and providing reasoned responses grounded in circuit literature. Its multimodal capabilities enable processing of both textual and visual data, facilitating more efficient and comprehensive analysis. The system dynamically adapts using intelligent search tools, automated document retrieval from the internet, and real-time database updates. Unlike conventional approaches constrained by model context limits, MuaLLM decouples retrieval from inference, enabling scalable reasoning over arbitrarily large corpora. At the maximum context length supported by standard LLMs, MuaLLM remains up to 10x less costly and 1.6x faster while maintaining the same accuracy. This allows rapid, no-human-in-the-loop database generation, overcoming the bottleneck of simulation-based dataset creation for circuits. To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design. MuaLLM achieves 90.1% recall on RAG-250, and 86.8% accuracy on Reas-100.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MuaLLMï¼Œä¸€ç§ç”¨äºè¾…åŠ©ç”µè·¯è®¾è®¡çš„å¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (Multimodal Large Language Model, LLM) æ™ºèƒ½ä½“ï¼Œæ—¨åœ¨è§£å†³ç”µè·¯è®¾è®¡é¢†åŸŸæ–‡çŒ®è°ƒç ”éš¾åº¦å¤§å’Œæ•°æ®è¡¨ç¤ºä¸ä¸€è‡´ç­‰æŒ‘æˆ˜ã€‚è¯¥ç³»ç»Ÿé›†æˆäº†æ··åˆæ£€ç´¢å¢å¼ºç”Ÿæˆ (Retrieval-Augmented Generation, RAG) æ¡†æ¶ä¸ç”µè·¯è®¾è®¡ç ”ç©¶è®ºæ–‡çš„è‡ªé€‚åº”å‘é‡æ•°æ®åº“ã€‚MuaLLM é‡‡ç”¨ Reason + Act (ReAct) å·¥ä½œæµè¿›è¡Œè¿­ä»£æ¨ç†å’Œå¤šæ­¥ä¿¡æ¯æ£€ç´¢ï¼Œå¹¶å…·å¤‡åŒæ—¶å¤„ç†æ–‡æœ¬ä¸è§†è§‰æ•°æ®çš„å¤šæ¨¡æ€åˆ†æèƒ½åŠ›ã€‚é€šè¿‡å°†æ£€ç´¢ä¸æ¨ç†è¿‡ç¨‹è§£è€¦ï¼ŒMuaLLM åœ¨ç»´æŒå‡†ç¡®æ€§çš„å‰æä¸‹ï¼Œæ¯”ä¼ ç»Ÿ LLM é™ä½äº† 10 å€æˆæœ¬ä¸”è¿è¡Œé€Ÿåº¦æå‡ 1.6 å€ã€‚ç ”ç©¶é€šè¿‡è‡ªå®šä¹‰çš„ RAG-250 å’Œ Reasoning-100 (Reas-100) æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMuaLLM åœ¨æ£€ç´¢ä»»åŠ¡ä¸­å®ç°äº† 90.1% çš„å¬å›ç‡ï¼Œå¹¶åœ¨å¤šæ­¥æ¨ç†ä»»åŠ¡ä¸­è¾¾åˆ°äº† 86.8% çš„å‡†ç¡®ç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08137v1",
      "published_date": "2025-08-11 16:11:09 UTC",
      "updated_date": "2025-08-11 16:11:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:57:17.693321+00:00"
    },
    {
      "arxiv_id": "2508.08131v1",
      "title": "Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models",
      "title_zh": "å£è¯­è¯­è¨€æ¨¡å‹ä¸­è¯­éŸ³-æ–‡æœ¬å¯¹é½çš„æœ€ä¼˜ä¼ è¾“æ­£åˆ™åŒ–",
      "authors": [
        "Wenze Xu",
        "Chun Wang",
        "Jiazhen Yu",
        "Sheng Chen",
        "Liang Gao",
        "Weihong Deng"
      ],
      "abstract": "Spoken Language Models (SLMs), which extend Large Language Models (LLMs) to perceive speech inputs, have gained increasing attention for their potential to advance speech understanding tasks. However, despite recent progress, studies show that SLMs often struggle to generalize across datasets, even for trained languages and tasks, raising concerns about whether they process speech in a text-like manner as intended. A key challenge underlying this limitation is the modality gap between speech and text representations. The high variability in speech embeddings may allow SLMs to achieve strong in-domain performance by exploiting unintended speech variations, ultimately hindering generalization. To mitigate this modality gap, we introduce Optimal Transport Regularization (OTReg), a method that formulates speech-text alignment as an optimal transport problem and derives a regularization loss to improve SLM training. In each training iteration, OTReg first establishes a structured correspondence between speech and transcript embeddings by determining the optimal transport plan, then incorporates the regularization loss based on this transport plan to optimize SLMs in generating speech embeddings that align more effectively with transcript embeddings. OTReg is lightweight, requiring no additional labels or learnable parameters, and integrates seamlessly into existing SLM training procedures. Extensive multilingual ASR experiments demonstrate that OTReg enhances speech-text alignment, mitigates the modality gap, and consequently improves SLM generalization across diverse datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å£è¯­è¯­è¨€æ¨¡å‹(Spoken Language Models, SLMs)åœ¨è·¨æ•°æ®é›†æ³›åŒ–æ–¹é¢çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºè¯­éŸ³ä¸æ–‡æœ¬è¡¨ç¤ºä¹‹é—´çš„æ¨¡æ€å·®è·(modality gap)æ˜¯é™åˆ¶å…¶æ€§èƒ½çš„æ ¸å¿ƒç“¶é¢ˆã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†æœ€ä¼˜ä¼ è¾“æ­£åˆ™åŒ–(Optimal Transport Regularization, OTReg)ï¼Œå°†è¯­éŸ³-æ–‡æœ¬å¯¹é½å»ºæ¨¡ä¸ºä¸€ä¸ªæœ€ä¼˜ä¼ è¾“é—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€šè¿‡ç¡®å®šæœ€ä¼˜ä¼ è¾“è®¡åˆ’ï¼Œåœ¨è¯­éŸ³å’Œè½¬å½•æ–‡æœ¬åµŒå…¥(embeddings)ä¹‹é—´å»ºç«‹ç»“æ„åŒ–å¯¹åº”å…³ç³»ï¼Œå¹¶åˆ©ç”¨æ­£åˆ™åŒ–æŸå¤±ä¼˜åŒ–SLMç”Ÿæˆçš„è¯­éŸ³åµŒå…¥ã€‚OTRegå…·æœ‰è½»é‡åŒ–çš„ç‰¹ç‚¹ï¼Œä¸éœ€è¦é¢å¤–çš„æ ‡ç­¾æˆ–å¯å­¦ä¹ å‚æ•°ï¼Œèƒ½å¤Ÿæ— ç¼é›†æˆåˆ°ç°æœ‰çš„æ¨¡å‹è®­ç»ƒæµç¨‹ä¸­ã€‚å¹¿æ³›çš„å¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)å®éªŒè¯æ˜ï¼ŒOTRegæ˜¾è‘—å¢å¼ºäº†è¯­éŸ³ä¸æ–‡æœ¬çš„å¯¹é½ï¼Œæœ‰æ•ˆç¼“è§£äº†æ¨¡æ€å·®è·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†SLMåœ¨å¤šæ ·åŒ–æ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºæ„å»ºæ›´ç¨³å¥çš„è¯­éŸ³ç†è§£ç³»ç»Ÿæä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "To be presented at ACPR 2025 Conference",
      "pdf_url": "https://arxiv.org/pdf/2508.08131v1",
      "published_date": "2025-08-11 16:06:04 UTC",
      "updated_date": "2025-08-11 16:06:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:57:14.796267+00:00"
    },
    {
      "arxiv_id": "2508.08127v1",
      "title": "BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks",
      "title_zh": "BlindGuardï¼šé¢å‘æœªçŸ¥æ”»å‡»çš„åŸºäºå¤§è¯­è¨€æ¨¡å‹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå®‰å…¨é˜²æŠ¤",
      "authors": [
        "Rui Miao",
        "Yixin Liu",
        "Yili Wang",
        "Xu Shen",
        "Yue Tan",
        "Yiwei Dai",
        "Shirui Pan",
        "Xin Wang"
      ],
      "abstract": "The security of LLM-based multi-agent systems (MAS) is critically threatened by propagation vulnerability, where malicious agents can distort collective decision-making through inter-agent message interactions. While existing supervised defense methods demonstrate promising performance, they may be impractical in real-world scenarios due to their heavy reliance on labeled malicious agents to train a supervised malicious detection model. To enable practical and generalizable MAS defenses, in this paper, we propose BlindGuard, an unsupervised defense method that learns without requiring any attack-specific labels or prior knowledge of malicious behaviors. To this end, we establish a hierarchical agent encoder to capture individual, neighborhood, and global interaction patterns of each agent, providing a comprehensive understanding for malicious agent detection. Meanwhile, we design a corruption-guided detector that consists of directional noise injection and contrastive learning, allowing effective detection model training solely on normal agent behaviors. Extensive experiments show that BlindGuard effectively detects diverse attack types (i.e., prompt injection, memory poisoning, and tool attack) across MAS with various communication patterns while maintaining superior generalizability compared to supervised baselines. The code is available at: https://github.com/MR9812/BlindGuard.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (LLM-based multi-agent systems, MAS) é¢ä¸´çš„ä¼ æ’­æ¼æ´ (propagation vulnerability) æå‡ºäº† BlindGuardï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€æ”»å‡»æ ‡ç­¾æˆ–å…ˆéªŒçŸ¥è¯†çš„æ— ç›‘ç£é˜²å¾¡æ–¹æ³•ã€‚ä¸ºäº†è§£å†³ç°æœ‰ç›‘ç£å­¦ä¹ æ–¹æ³•è¿‡åº¦ä¾èµ–æ¶æ„ä»£ç†æ ‡æ³¨çš„é—®é¢˜ï¼ŒBlindGuard å»ºç«‹äº†å±‚æ¬¡åŒ–ä»£ç†ç¼–ç å™¨ (hierarchical agent encoder)ï¼Œç”¨ä»¥æ•æ‰ä¸ªä½“ã€é‚»åŸŸåŠå…¨å±€å±‚é¢çš„ä»£ç†äº¤äº’æ¨¡å¼ã€‚ç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†ç”±å®šå‘å™ªå£°æ³¨å…¥ (directional noise injection) å’Œå¯¹æ¯”å­¦ä¹  (contrastive learning) ç»„æˆçš„è…èš€å¼•å¯¼æ£€æµ‹å™¨ (corruption-guided detector)ï¼Œä½¿å…¶ä»…é€šè¿‡æ­£å¸¸ä»£ç†è¡Œä¸ºå³å¯å®Œæˆæ¨¡å‹è®­ç»ƒã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒBlindGuard åœ¨å¤šç§é€šä¿¡æ¨¡å¼ä¸‹å‡èƒ½æœ‰æ•ˆè¯†åˆ«æç¤ºæ³¨å…¥ (prompt injection)ã€å†…å­˜æŠ•æ¯’ (memory poisoning) å’Œå·¥å…·æ”»å‡» (tool attack) ç­‰æ”»å‡»ç±»å‹ã€‚ç›¸æ¯”äºä¼ ç»Ÿçš„ç›‘ç£é˜²å¾¡åŸºå‡†ï¼Œè¯¥æ–¹æ³•åœ¨åº”å¯¹æœªçŸ¥æ”»å‡»æ—¶å±•ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œå®ç”¨æ€§ï¼Œä¸ºæ„å»ºå®‰å…¨çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08127v1",
      "published_date": "2025-08-11 16:04:47 UTC",
      "updated_date": "2025-08-11 16:04:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:57:24.390001+00:00"
    },
    {
      "arxiv_id": "2508.08122v1",
      "title": "MemoryKT: An Integrative Memory-and-Forgetting Method for Knowledge Tracing",
      "title_zh": "MemoryKTï¼šä¸€ç§èåˆè®°å¿†ä¸é—å¿˜æœºåˆ¶çš„çŸ¥è¯†è¿½è¸ªæ–¹æ³•",
      "authors": [
        "Mingrong Lin",
        "Ke Deng",
        "Zhengyang Wu",
        "Zetao Zheng",
        "Jie Li"
      ],
      "abstract": "Knowledge Tracing (KT) is committed to capturing students' knowledge mastery from their historical interactions. Simulating students' memory states is a promising approach to enhance both the performance and interpretability of knowledge tracing models. Memory consists of three fundamental processes: encoding, storage, and retrieval. Although forgetting primarily manifests during the storage stage, most existing studies rely on a single, undifferentiated forgetting mechanism, overlooking other memory processes as well as personalized forgetting patterns. To address this, this paper proposes memoryKT, a knowledge tracing model based on a novel temporal variational autoencoder. The model simulates memory dynamics through a three-stage process: (i) Learning the distribution of students' knowledge memory features, (ii) Reconstructing their exercise feedback, while (iii) Embedding a personalized forgetting module within the temporal workflow to dynamically modulate memory storage strength. This jointly models the complete encoding-storage-retrieval cycle, significantly enhancing the model's perception capability for individual differences. Extensive experiments on four public datasets demonstrate that our proposed approach significantly outperforms state-of-the-art baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MemoryKTï¼Œä¸€ç§åŸºäºæ–°å‹æ—¶é—´å˜åˆ†è‡ªç¼–ç å™¨(temporal variational autoencoder)çš„çŸ¥è¯†è¿½è¸ª(Knowledge Tracing)æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡æ¨¡æ‹Ÿå®Œæ•´çš„è®°å¿†åŠ¨æ€æ¥æå‡é¢„æµ‹æ€§èƒ½ã€‚è¯¥æ¨¡å‹ç³»ç»Ÿåœ°æ•´åˆäº†ç¼–ç ã€å­˜å‚¨ä¸æ£€ç´¢ä¸‰ä¸ªè®°å¿†é˜¶æ®µï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•ä¸­é—å¿˜æœºåˆ¶è¿‡äºå•ä¸€ä¸”å¿½è§†ä¸ªæ€§åŒ–å·®å¼‚çš„é—®é¢˜ã€‚MemoryKTåˆ©ç”¨ç”Ÿæˆå¼æ¶æ„å­¦ä¹ è®°å¿†ç‰¹å¾åˆ†å¸ƒï¼Œå¹¶åµŒå…¥ä¸ªæ€§åŒ–é—å¿˜æ¨¡å—ä»¥åŠ¨æ€è°ƒèŠ‚è®°å¿†å­˜å‚¨å¼ºåº¦ï¼Œä»è€Œå®ç°å¯¹å­¦ç”ŸçŠ¶æ€çš„ç²¾å‡†å»ºæ¨¡ã€‚åœ¨å››ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›(state-of-the-art)åŸºå‡†æ¨¡å‹ã€‚è¿™ä¸€ç ”ç©¶é€šè¿‡è”åˆå»ºæ¨¡å®Œæ•´çš„è®°å¿†å¾ªç¯ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹å¯¹å­¦ç”Ÿä¸ªä½“å·®å¼‚çš„æ„ŸçŸ¥èƒ½åŠ›å’Œç»“æœçš„å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.08122v1",
      "published_date": "2025-08-11 15:59:59 UTC",
      "updated_date": "2025-08-11 15:59:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:57:30.295913+00:00"
    },
    {
      "arxiv_id": "2508.08120v1",
      "title": "Vision-Based Localization and LLM-based Navigation for Indoor Environments",
      "title_zh": "å®¤å†…ç¯å¢ƒä¸‹çš„è§†è§‰å®šä½ä¸åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¯¼èˆª",
      "authors": [
        "Keyan Rahimi",
        "Md. Wasiul Haque",
        "Sagar Dasgupta",
        "Mizanur Rahman"
      ],
      "abstract": "Indoor navigation remains a complex challenge due to the absence of reliable GPS signals and the architectural intricacies of large enclosed environments. This study presents an indoor localization and navigation approach that integrates vision-based localization with large language model (LLM)-based navigation. The localization system utilizes a ResNet-50 convolutional neural network fine-tuned through a two-stage process to identify the user's position using smartphone camera input. To complement localization, the navigation module employs an LLM, guided by a carefully crafted system prompt, to interpret preprocessed floor plan images and generate step-by-step directions. Experimental evaluation was conducted in a realistic office corridor with repetitive features and limited visibility to test localization robustness. The model achieved high confidence and an accuracy of 96% across all tested waypoints, even under constrained viewing conditions and short-duration queries. Navigation tests using ChatGPT on real building floor maps yielded an average instruction accuracy of 75%, with observed limitations in zero-shot reasoning and inference time. This research demonstrates the potential for scalable, infrastructure-free indoor navigation using off-the-shelf cameras and publicly available floor plans, particularly in resource-constrained settings like hospitals, airports, and educational institutions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆè§†è§‰å®šä½ä¸å¤§è¯­è¨€æ¨¡å‹(LLM)çš„å®¤å†…å¯¼èˆªæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å®¤å†…ç¯å¢ƒä¸­GPSä¿¡å·ç¼ºå¤±å¯¼è‡´çš„å¯¼èˆªéš¾é¢˜ã€‚åœ¨å®šä½æ–¹é¢ï¼Œç³»ç»Ÿåˆ©ç”¨ç»è¿‡ä¸¤é˜¶æ®µå¾®è°ƒçš„ResNet-50å·ç§¯ç¥ç»ç½‘ç»œï¼Œé€šè¿‡æ™ºèƒ½æ‰‹æœºæ‘„åƒå¤´è¾“å…¥å®æ—¶è¯†åˆ«ç”¨æˆ·ä½ç½®ã€‚å¯¼èˆªæ¨¡å—åˆ™é‡‡ç”¨LLMå¤„ç†é¢„å¤„ç†åçš„å¹³é¢å›¾å›¾åƒï¼Œå¹¶åœ¨ç³»ç»Ÿæç¤ºè¯å¼•å¯¼ä¸‹ç”Ÿæˆåˆ†æ­¥éª¤çš„è·¯å¾„æŒ‡å¼•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥å®šä½æ¨¡å‹åœ¨å…·æœ‰é‡å¤ç‰¹å¾çš„åŠå…¬èµ°å»Šä¸­è¾¾åˆ°äº†96%çš„å‡†ç¡®ç‡ï¼Œå±•ç°å‡ºæå¼ºçš„é²æ£’æ€§ã€‚åœ¨åŸºäºçœŸå®æ¥¼å±‚åœ°å›¾çš„å¯¼èˆªæµ‹è¯•ä¸­ï¼Œåˆ©ç”¨ChatGPTç”Ÿæˆçš„æŒ‡ä»¤å‡†ç¡®ç‡å¹³å‡è¾¾åˆ°75%ï¼Œä½†åœ¨Zero-shotæ¨ç†èƒ½åŠ›å’Œæ¨ç†æ—¶é—´æ–¹é¢ä»å­˜åœ¨å±€é™ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åˆ©ç”¨ç°æˆæ‘„åƒå¤´å’Œå…¬å¼€å¹³é¢å›¾å®ç°å¯æ‰©å±•ã€æ— åŸºç¡€è®¾æ–½(Infrastructure-free)å®¤å†…å¯¼èˆªçš„æ½œåŠ›ï¼Œä¸ºåŒ»é™¢å’Œæœºåœºç­‰å¤æ‚ç¯å¢ƒæä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 6 figures, 1 table",
      "pdf_url": "https://arxiv.org/pdf/2508.08120v1",
      "published_date": "2025-08-11 15:59:09 UTC",
      "updated_date": "2025-08-11 15:59:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:57:32.788438+00:00"
    },
    {
      "arxiv_id": "2508.09215v1",
      "title": "Real-time deep learning phase imaging flow cytometer reveals blood cell aggregate biomarkers for haematology diagnostics",
      "title_zh": "å®æ—¶æ·±åº¦å­¦ä¹ ç›¸ä½æˆåƒæµå¼ç»†èƒä»ªæ­ç¤ºè¡€æ¶²å­¦è¯Šæ–­ä¸­çš„è¡€ç»†èƒèšé›†ä½“ç”Ÿç‰©æ ‡å¿—ç‰©",
      "authors": [
        "Kerem Delikoyun",
        "Qianyu Chen",
        "Liu Wei",
        "Si Ko Myo",
        "Johannes Krell",
        "Martin Schlegel",
        "Win Sen Kuan",
        "John Tshon Yit Soong",
        "Gerhard Schneider",
        "Clarissa Prazeres da Costa",
        "Percy A. Knolle",
        "Laurent Renia",
        "Matthew Edward Cove",
        "Hwee Kuan Lee",
        "Klaus Diepold",
        "Oliver Hayden"
      ],
      "abstract": "While analysing rare blood cell aggregates remains challenging in automated haematology, they could markedly advance label-free functional diagnostics. Conventional flow cytometers efficiently perform cell counting with leukocyte differentials but fail to identify aggregates with flagged results, requiring manual reviews. Quantitative phase imaging flow cytometry captures detailed aggregate morphologies, but clinical use is hampered by massive data storage and offline processing. Incorporating hidden biomarkers into routine haematology panels would significantly improve diagnostics without flagged results. We present RT-HAD, an end-to-end deep learning-based image and data processing framework for off-axis digital holographic microscopy (DHM), which combines physics-consistent holographic reconstruction and detection, representing each blood cell in a graph to recognize aggregates. RT-HAD processes >30 GB of image data on-the-fly with turnaround time of <1.5 min and error rate of 8.9% in platelet aggregate detection, which matches acceptable laboratory error rates of haematology biomarkers and solves the big data challenge for point-of-care diagnostics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RT-HADï¼Œä¸€ç§åŸºäºDeep Learningçš„ç«¯åˆ°ç«¯å›¾åƒä¸æ•°æ®å¤„ç†æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å®šé‡ç›¸ä½æˆåƒæµå¼ç»†èƒæœ¯(Quantitative phase imaging flow cytometry)åœ¨è¯†åˆ«è¡€ç»†èƒå›¢å—(Aggregates)æ—¶é¢ä¸´çš„æ•°æ®å­˜å‚¨ä¸å®æ—¶å¤„ç†ç“¶é¢ˆã€‚è¯¥æ¡†æ¶ä¸“ä¸ºç¦»è½´æ•°å­—å…¨æ¯æ˜¾å¾®æŠ€æœ¯(Off-axis DHM)å¼€å‘ï¼Œç»“åˆäº†ç‰©ç†ä¸€è‡´çš„å…¨æ¯é‡å»ºä¸æ£€æµ‹æŠ€æœ¯ï¼Œå¹¶åˆ©ç”¨å›¾(Graph)è¡¨å¾æ¯ä¸ªè¡€ç»†èƒä»¥å®ç°è‡ªåŠ¨åŒ–çš„å›¢å—è¯†åˆ«ã€‚å®éªŒè¡¨æ˜ï¼ŒRT-HADèƒ½å¤Ÿå®æ—¶å¤„ç†è¶…è¿‡30 GBçš„å›¾åƒæ•°æ®ï¼Œæ€»å‘¨è½¬æ—¶é—´ç¼©çŸ­è‡³1.5åˆ†é’Ÿä»¥å†…ï¼Œä¸”åœ¨è¡€å°æ¿å›¢å—(Platelet aggregate)æ£€æµ‹ä¸­çš„è¯¯å·®ç‡ä»…ä¸º8.9%ï¼Œç¬¦åˆä¸´åºŠå®éªŒå®¤çš„å®¹è®¸æ ‡å‡†ã€‚è¿™ä¸€æˆæœé€šè¿‡æŒ–æ˜è¡€æ¶²å­¦ä¸­éšè—çš„ç”Ÿç‰©æ ‡å¿—ç‰©(Biomarkers)ï¼Œæ˜¾è‘—æå‡äº†æ— æ ‡è®°åŠŸèƒ½è¯Šæ–­çš„æ•ˆç‡ï¼Œå¹¶ä¸ºå³æ—¶è¯Šæ–­(Point-of-care diagnostics)åœºæ™¯ä¸‹çš„è‡ªåŠ¨åŒ–åˆ†ææä¾›äº†å¤§æ•°æ®è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.09215v1",
      "published_date": "2025-08-11 15:58:12 UTC",
      "updated_date": "2025-08-11 15:58:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:57:34.785881+00:00"
    },
    {
      "arxiv_id": "2508.08117v1",
      "title": "GRASPTrack: Geometry-Reasoned Association via Segmentation and Projection for Multi-Object Tracking",
      "title_zh": "GRASPTrackï¼šåŸºäºåˆ†å‰²ä¸æŠ•å½±çš„å‡ ä½•æ¨ç†å…³è”å¤šç›®æ ‡è·Ÿè¸ª",
      "authors": [
        "Xudong Han",
        "Pengcheng Fang",
        "Yueying Tian",
        "Jianhui Yu",
        "Xiaohao Cai",
        "Daniel Roggen",
        "Philip Birch"
      ],
      "abstract": "Multi-object tracking (MOT) in monocular videos is fundamentally challenged by occlusions and depth ambiguity, issues that conventional tracking-by-detection (TBD) methods struggle to resolve owing to a lack of geometric awareness. To address these limitations, we introduce GRASPTrack, a novel depth-aware MOT framework that integrates monocular depth estimation and instance segmentation into a standard TBD pipeline to generate high-fidelity 3D point clouds from 2D detections, thereby enabling explicit 3D geometric reasoning. These 3D point clouds are then voxelized to enable a precise and robust Voxel-Based 3D Intersection-over-Union (IoU) for spatial association. To further enhance tracking robustness, our approach incorporates Depth-aware Adaptive Noise Compensation, which dynamically adjusts the Kalman filter process noise based on occlusion severity for more reliable state estimation. Additionally, we propose a Depth-enhanced Observation-Centric Momentum, which extends the motion direction consistency from the image plane into 3D space to improve motion-based association cues, particularly for objects with complex trajectories. Extensive experiments on the MOT17, MOT20, and DanceTrack benchmarks demonstrate that our method achieves competitive performance, significantly improving tracking robustness in complex scenes with frequent occlusions and intricate motion patterns.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å•ç›®è§†é¢‘å¤šç›®æ ‡è·Ÿè¸ª(Multi-Object Tracking)ä¸­çš„é®æŒ¡å’Œæ·±åº¦æ­§ä¹‰é—®é¢˜ï¼Œæå‡ºäº†åä¸ºGRASPTrackçš„æ·±åº¦æ„ŸçŸ¥è·Ÿè¸ªæ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†å•ç›®æ·±åº¦ä¼°è®¡(monocular depth estimation)ä¸å®ä¾‹åˆ†å‰²(instance segmentation)é›†æˆåˆ°ä¼ ç»Ÿçš„æ£€æµ‹è·Ÿè¸ª(Tracking-by-Detection)æµç¨‹ä¸­ï¼Œåˆ©ç”¨2Dæ£€æµ‹ç”Ÿæˆé«˜ä¿çœŸ3Dç‚¹äº‘ä»¥å®ç°æ˜¾å¼çš„3Då‡ ä½•æ¨ç†ã€‚ç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†åŸºäºä½“ç´ çš„3Däº¤å¹¶æ¯”(Voxel-Based 3D Intersection-over-Union)è¿›è¡Œç²¾ç¡®çš„ç©ºé—´å…³è”ï¼Œå¹¶ç»“åˆæ·±åº¦æ„ŸçŸ¥è‡ªé€‚åº”å™ªå£°è¡¥å¿(Depth-aware Adaptive Noise Compensation)åŠ¨æ€ä¼˜åŒ–å¡å°”æ›¼æ»¤æ³¢(Kalman filter)çš„çŠ¶æ€ä¼°è®¡ã€‚æ­¤å¤–ï¼Œæ·±åº¦å¢å¼ºè§‚å¯Ÿä¸­å¿ƒåŠ¨é‡(Depth-enhanced Observation-Centric Momentum)å°†è¿åŠ¨ä¸€è‡´æ€§ä»å›¾åƒå¹³é¢æ‰©å±•è‡³3Dç©ºé—´ï¼Œæœ‰æ•ˆæå‡äº†å¤æ‚è½¨è¿¹ä¸‹çš„å…³è”èƒ½åŠ›ã€‚åœ¨MOT17ã€MOT20å’ŒDanceTrackç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGRASPTrackåœ¨å¤„ç†é¢‘ç¹é®æŒ¡å’Œå¤æ‚è¿åŠ¨æ¨¡å¼çš„å¤æ‚åœºæ™¯æ—¶ï¼Œæ˜¾è‘—å¢å¼ºäº†è·Ÿè¸ªçš„é²æ£’æ€§å¹¶è¾¾åˆ°äº†ç«äº‰æ€§æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08117v1",
      "published_date": "2025-08-11 15:56:21 UTC",
      "updated_date": "2025-08-11 15:56:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:57:36.301567+00:00"
    },
    {
      "arxiv_id": "2508.08115v2",
      "title": "TeamMedAgents: Enhancing Medical Decision-Making of LLMs Through Structured Teamwork",
      "title_zh": "TeamMedAgentsï¼šé€šè¿‡ç»“æ„åŒ–å›¢é˜Ÿåä½œæå‡å¤§è¯­è¨€æ¨¡å‹çš„åŒ»ç–—å†³ç­–èƒ½åŠ›",
      "authors": [
        "Pranav Pushkar Mishra",
        "Mohammad Arvan",
        "Mohan Zalake"
      ],
      "abstract": "We present TeamMedAgents, a modular multi-agent framework that systematically translates evidence-based teamwork principles from organizational psychology into large language model collaboration for medical decision-making. Building upon Salas et al.'s \"Big Five\" teamwork model, we operationalize five core components as independently configurable mechanisms: shared mental models, team leadership, team orientation, trust networks, and mutual monitoring. Our architecture dynamically recruits 2-4 specialist agents and employs structured four-phase deliberation with adaptive component selection. Evaluation across eight medical benchmarks encompassing 11,545 questions demonstrates TeamMedAgents achieves 77.63% overall accuracy (text-based: 81.30%, vision-language: 66.60%). Systematic ablation studies comparing three single-agent baselines (Zero-Shot, Few-Shot, CoT) against individual teamwork components reveal task-specific optimization patterns: shared mental models excel on knowledge tasks, trust mechanisms improve differential diagnosis, while comprehensive integration degrades performance. Adaptive component selection yields 2-10 percentage point improvements over strongest baselines, with 96.2% agent convergence validating structured coordination effectiveness.\n  TeamMedAgents establishes principled methodology for translating human teamwork theory into multi-agent systems, demonstrating that evidence-based collaboration patterns enhance AI performance in safety-critical domains through modular component design and selective activation strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TeamMedAgentsï¼Œä¸€ç§æ¨¡å—åŒ–çš„å¤šæ™ºèƒ½ä½“ (multi-agent) æ¡†æ¶ï¼Œæ—¨åœ¨å°†ç»„ç»‡å¿ƒç†å­¦ä¸­çš„å¾ªè¯å›¢é˜Ÿåˆä½œåŸåˆ™ç³»ç»Ÿåœ°è½¬åŒ–ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨åŒ»ç–—å†³ç­–ä¸­çš„åä½œæœºåˆ¶ã€‚è¯¥æ¶æ„åŸºäº Salas ç­‰äººçš„ \"Big Five\" å›¢é˜Ÿåˆä½œæ¨¡å‹ï¼Œå°†å…±äº«å¿ƒç†æ¨¡å‹ (shared mental models)ã€å›¢é˜Ÿé¢†å¯¼åŠ› (team leadership) ç­‰äº”ä¸ªæ ¸å¿ƒè¦ç´ è®¾è®¡ä¸ºå¯ç‹¬ç«‹é…ç½®çš„ç»„ä»¶ï¼Œå¹¶é‡‡ç”¨å››é˜¶æ®µç»“æ„åŒ–å®¡è®®æµç¨‹ã€‚åœ¨åŒ…å« 11,545 ä¸ªé—®é¢˜çš„å…«é¡¹åŒ»ç–—åŸºå‡†æµ‹è¯•ä¸­ï¼ŒTeamMedAgents å–å¾—äº† 77.63% çš„æ€»ä½“å‡†ç¡®ç‡ï¼Œå…¶ä¸­æ–‡æœ¬ä»»åŠ¡å‡†ç¡®ç‡è¾¾åˆ° 81.30%ã€‚æ¶ˆèå®éªŒæ­ç¤ºäº†ä»»åŠ¡ç‰¹å®šçš„ä¼˜åŒ–æ¨¡å¼ï¼Œè¯æ˜è‡ªé€‚åº”ç»„ä»¶é€‰æ‹© (adaptive component selection) ç›¸æ¯”å¼ºåŸºçº¿æ¨¡å‹å¯æ˜¾è‘—æå‡ 2 è‡³ 10 ä¸ªç™¾åˆ†ç‚¹çš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶é€šè¿‡ 96.2% çš„æ™ºèƒ½ä½“æ”¶æ•›ç‡éªŒè¯äº†ç»“æ„åŒ–åè°ƒçš„æœ‰æ•ˆæ€§ï¼Œä¸ºåœ¨åŒ»ç–—ç­‰å®‰å…¨å…³é”®é¢†åŸŸåˆ©ç”¨æ¨¡å—åŒ–è®¾è®¡å’Œé€‰æ‹©æ€§æ¿€æ´»ç­–ç•¥å¢å¼º AI æ€§èƒ½å»ºç«‹äº†åŸåˆ™æ€§æ–¹æ³•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages, 1 figure, 3 tables, 1 algorithm(in appendix)",
      "pdf_url": "https://arxiv.org/pdf/2508.08115v2",
      "published_date": "2025-08-11 15:55:06 UTC",
      "updated_date": "2025-12-02 21:51:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:57:54.062319+00:00"
    },
    {
      "arxiv_id": "2508.08107v1",
      "title": "Hyperspectral Imaging",
      "title_zh": "é«˜å…‰è°±æˆåƒ",
      "authors": [
        "Danfeng Hong",
        "Chenyu Li",
        "Naoto Yokoya",
        "Bing Zhang",
        "Xiuping Jia",
        "Antonio Plaza",
        "Paolo Gamba",
        "Jon Atli Benediktsson",
        "Jocelyn Chanussot"
      ],
      "abstract": "Hyperspectral imaging (HSI) is an advanced sensing modality that simultaneously captures spatial and spectral information, enabling non-invasive, label-free analysis of material, chemical, and biological properties. This Primer presents a comprehensive overview of HSI, from the underlying physical principles and sensor architectures to key steps in data acquisition, calibration, and correction. We summarize common data structures and highlight classical and modern analysis methods, including dimensionality reduction, classification, spectral unmixing, and AI-driven techniques such as deep learning. Representative applications across Earth observation, precision agriculture, biomedicine, industrial inspection, cultural heritage, and security are also discussed, emphasizing HSI's ability to uncover sub-visual features for advanced monitoring, diagnostics, and decision-making. Persistent challenges, such as hardware trade-offs, acquisition variability, and the complexity of high-dimensional data, are examined alongside emerging solutions, including computational imaging, physics-informed modeling, cross-modal fusion, and self-supervised learning. Best practices for dataset sharing, reproducibility, and metadata documentation are further highlighted to support transparency and reuse. Looking ahead, we explore future directions toward scalable, real-time, and embedded HSI systems, driven by sensor miniaturization, self-supervised learning, and foundation models. As HSI evolves into a general-purpose, cross-disciplinary platform, it holds promise for transformative applications in science, technology, and society.",
      "tldr_zh": "è¿™ç¯‡ç»¼è¿°æ–‡ç« å…¨é¢ä»‹ç»äº†é«˜å…‰è°±æˆåƒ (Hyperspectral Imaging, HSI) æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯èƒ½å¤ŸåŒæ—¶è·å–ç©ºé—´å’Œå…‰è°±ä¿¡æ¯ï¼Œä»è€Œå®ç°å¯¹ç‰©è´¨åŒ–å­¦åŠç”Ÿç‰©å±æ€§çš„æ— æŸåˆ†æã€‚æ–‡ç« ç³»ç»Ÿåœ°æ€»ç»“äº†ä»ç‰©ç†åŸç†ã€ä¼ æ„Ÿå™¨æ¶æ„åˆ°æ•°æ®æ ¡å‡†çš„å®Œæ•´æµç¨‹ï¼Œå¹¶è¯¦ç»†æ¢è®¨äº†é™ç»´ (Dimensionality reduction)ã€å…‰è°±è§£æ·· (Spectral unmixing) ä»¥åŠ AI é©±åŠ¨çš„æ·±åº¦å­¦ä¹  (Deep learning) ç­‰åˆ†ææ–¹æ³•ã€‚HSI åœ¨åœ°çƒè§‚æµ‹ (Earth observation)ã€ç²¾å‡†å†œä¸šå’Œç”Ÿç‰©åŒ»å­¦ç­‰å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºå¼ºå¤§çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æå–äºšè§†è§‰ç‰¹å¾å’Œè¾…åŠ©å†³ç­–æ–¹é¢ã€‚é’ˆå¯¹ç¡¬ä»¶æƒè¡¡åŠé«˜ç»´æ•°æ®å¤æ‚æ€§ç­‰æŒ‘æˆ˜ï¼Œæ–‡ä¸­æå‡ºäº†è®¡ç®—æˆåƒã€ç‰©ç†ä¿¡æ¯å»ºæ¨¡ (Physics-informed modeling) å’Œè‡ªç›‘ç£å­¦ä¹  (Self-supervised learning) ç­‰æ–°å…´è§£å†³æ–¹æ¡ˆã€‚æ–‡ç« è¿˜å¼ºè°ƒäº†æ•°æ®é›†å…±äº«å’Œå…ƒæ•°æ®è§„èŒƒåŒ–çš„é‡è¦æ€§ï¼Œä»¥æ”¯æŒç ”ç©¶çš„é€æ˜åº¦å’Œå¤ç”¨ã€‚å±•æœ›æœªæ¥ï¼Œéšç€ä¼ æ„Ÿå™¨å°å‹åŒ–å’ŒåŸºç¡€æ¨¡å‹ (Foundation models) çš„å‘å±•ï¼ŒHSI æœ‰æœ›è¿›åŒ–ä¸ºå®æ—¶ã€æ™ºèƒ½åŒ–çš„è·¨å­¦ç§‘é€šç”¨å¹³å°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08107v1",
      "published_date": "2025-08-11 15:47:24 UTC",
      "updated_date": "2025-08-11 15:47:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:57:55.554882+00:00"
    },
    {
      "arxiv_id": "2508.08101v1",
      "title": "ChatGPT on the Road: Leveraging Large Language Model-Powered In-vehicle Conversational Agents for Safer and More Enjoyable Driving Experience",
      "title_zh": "ChatGPT åœ¨è·¯ä¸Šï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„è½¦è½½å¯¹è¯æ™ºèƒ½ä½“æå‡é©¾é©¶å®‰å…¨æ€§ä¸æ„‰æ‚¦æ„Ÿ",
      "authors": [
        "Yeana Lee Bond",
        "Mungyeong Choe",
        "Baker Kasim Hasan",
        "Arsh Siddiqui",
        "Myounghoon Jeon"
      ],
      "abstract": "Studies on in-vehicle conversational agents have traditionally relied on pre-scripted prompts or limited voice commands, constraining natural driver-agent interaction. To resolve this issue, the present study explored the potential of a ChatGPT-based in-vehicle agent capable of carrying continuous, multi-turn dialogues. Forty drivers participated in our experiment using a motion-based driving simulator, comparing three conditions (No agent, Pre-scripted agent, and ChatGPT-based agent) as a within-subjects variable. Results showed that the ChatGPT-based agent condition led to more stable driving performance across multiple metrics. Participants demonstrated lower variability in longitudinal acceleration, lateral acceleration, and lane deviation compared to the other two conditions. In subjective evaluations, the ChatGPT-based agent also received significantly higher ratings in competence, animacy, affective trust, and preference compared to the Pre-scripted agent. Our thematic analysis of driver-agent conversations revealed diverse interaction patterns in topics, including driving assistance/questions, entertainment requests, and anthropomorphic interactions. Our results highlight the potential of LLM-powered in-vehicle conversational agents to enhance driving safety and user experience through natural, context-rich interactions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢ç´¢äº†åŸºäº ChatGPT çš„è½¦è½½æ™ºèƒ½ä½“åœ¨æ”¯æŒè¿ç»­å¤šè½®å¯¹è¯æ–¹é¢çš„æ½œåŠ›ï¼Œæ—¨åœ¨å…‹æœä¼ ç»Ÿé¢„è®¾è„šæœ¬æˆ–å—é™è¯­éŸ³æŒ‡ä»¤å¯¹äººæœºäº¤äº’çš„é™åˆ¶ã€‚é€šè¿‡åœ¨åŠ¨åŠ›å­¦æ¨¡æ‹Ÿå™¨(motion-based driving simulator)ä¸­å¯¹40åé©¾é©¶å‘˜è¿›è¡Œå®éªŒï¼Œç ”ç©¶å¯¹æ¯”äº†æ— æ™ºèƒ½ä½“ã€é¢„è®¾è„šæœ¬æ™ºèƒ½ä½“(Pre-scripted agent)ä»¥åŠåŸºäº ChatGPT æ™ºèƒ½ä½“ä¸‰ç§æƒ…å¢ƒä¸‹çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨åŸºäº ChatGPT çš„æ™ºèƒ½ä½“èƒ½æ˜¾è‘—æé«˜é©¾é©¶æ€§èƒ½çš„ç¨³å®šæ€§ï¼Œåœ¨çºµå‘åŠ é€Ÿåº¦ã€æ¨ªå‘åŠ é€Ÿåº¦ä»¥åŠè½¦é“åç¦»(lane deviation)ç­‰æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºæ›´ä½çš„å˜å¼‚æ€§ã€‚ä¸»è§‚è¯„ä»·æ–¹é¢ï¼ŒChatGPT æ™ºèƒ½ä½“åœ¨èƒ½åŠ›æ„Ÿ(competence)ã€ç”Ÿå‘½åŠ›(animacy)ã€æƒ…æ„Ÿä¿¡ä»»(affective trust)å’Œç”¨æˆ·åå¥½åº¦ä¸Šå‡è·å¾—äº†æ˜¾è‘—æ›´é«˜çš„è¯„åˆ†ã€‚ä¸»é¢˜åˆ†æ(thematic analysis)æ­ç¤ºäº†é©¾é©¶å‘˜åœ¨é©¾é©¶è¾…åŠ©ã€å¨±ä¹è¯·æ±‚åŠæ‹ŸäººåŒ–äº’åŠ¨ç­‰å¤šä¸ªç»´åº¦çš„å¤šæ ·åŒ–äº¤äº’æ¨¡å¼ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)é©±åŠ¨çš„è½¦è½½å¯¹è¯æ™ºèƒ½ä½“ï¼Œé€šè¿‡è‡ªç„¶ä¸”è¯­ä¹‰ä¸°å¯Œçš„äº¤äº’ï¼Œåœ¨å¢å¼ºé©¾é©¶å®‰å…¨æ€§å’Œæå‡ç”¨æˆ·ä½“éªŒæ–¹é¢å…·æœ‰æ˜¾è‘—æ½œåŠ›ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.HC",
      "comment": "Submitted to International Journal of Human-Computer Studies. Bond and Choe: Drafting, Review, Editing, Validation, Software, Methodology, Investigation, Data Analysis, Conceptualization, Experiment training. Hasan and Siddiqui: Experimental and Data Analysis Support. Jeon: Supervision, Review, Resources, Project Admin, Methodology, Conceptualization. Total 34 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.08101v1",
      "published_date": "2025-08-11 15:40:44 UTC",
      "updated_date": "2025-08-11 15:40:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:57:56.993590+00:00"
    },
    {
      "arxiv_id": "2508.08100v2",
      "title": "Grid2Guide: A* Enabled Small Language Model for Indoor Navigation",
      "title_zh": "Grid2Guideï¼šA* ç®—æ³•èµ‹èƒ½çš„å®¤å†…å¯¼èˆªå°è¯­è¨€æ¨¡å‹",
      "authors": [
        "Md. Wasiul Haque",
        "Sagar Dasgupta",
        "Mizanur Rahman"
      ],
      "abstract": "Reliable indoor navigation remains a significant challenge in complex environments, particularly where external positioning signals and dedicated infrastructures are unavailable. This research presents Grid2Guide, a hybrid navigation framework that combines the A* search algorithm with a Small Language Model (SLM) to generate clear, human-readable route instructions. The framework first conducts a binary occupancy matrix from a given indoor map. Using this matrix, the A* algorithm computes the optimal path between origin and destination, producing concise textual navigation steps. These steps are then transformed into natural language instructions by the SLM, enhancing interpretability for end users. Experimental evaluations across various indoor scenarios demonstrate the method's effectiveness in producing accurate and timely navigation guidance. The results validate the proposed approach as a lightweight, infrastructure-free solution for real-time indoor navigation support.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Grid2Guideï¼Œä¸€ç§ç»“åˆäº† A* æœç´¢ç®—æ³•ä¸å°å‹è¯­è¨€æ¨¡å‹ (SLM) çš„æ··åˆå¯¼èˆªæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç¼ºä¹å¤–éƒ¨å®šä½ä¿¡å·åŠåŸºç¡€è®¾æ–½çš„å¤æ‚å®¤å†…å¯¼èˆªæŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é¦–å…ˆåŸºäºå®¤å†…åœ°å›¾æ„å»ºäºŒå€¼å æ®çŸ©é˜µ (Binary occupancy matrix)ï¼Œå¹¶åˆ©ç”¨ A* ç®—æ³•è®¡ç®—èµ·ç‚¹ä¸ç»ˆç‚¹ä¹‹é—´çš„æœ€ä¼˜è·¯å¾„ï¼Œç”Ÿæˆç®€æ´çš„æ–‡æœ¬å¯¼èˆªæ­¥éª¤ã€‚è¿™äº›æ­¥éª¤éšåç”± SLM è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œæ˜¾è‘—å¢å¼ºäº†ç”¨æˆ·å¯¹å¯¼èˆªå»ºè®®çš„å¯è§£é‡Šæ€§ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§å®¤å†…åœºæ™¯ä¸­å‡èƒ½æœ‰æ•ˆäº§å‡ºå‡†ç¡®ä¸”åŠæ—¶çš„å¼•å¯¼ä¿¡æ¯ã€‚ç ”ç©¶ç»“æœéªŒè¯äº† Grid2Guide ä½œä¸ºä¸€ç§è½»é‡çº§ã€æ— éœ€åŸºç¡€è®¾æ–½çš„å®æ—¶å®¤å†…å¯¼èˆªè§£å†³æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages, 9 figures, 6 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.08100v2",
      "published_date": "2025-08-11 15:39:27 UTC",
      "updated_date": "2025-08-29 20:09:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:57:57.256401+00:00"
    },
    {
      "arxiv_id": "2508.08095v1",
      "title": "Dual Information Speech Language Models for Emotional Conversations",
      "title_zh": "é¢å‘æƒ…æ„Ÿå¯¹è¯çš„åŒä¿¡æ¯è¯­éŸ³è¯­è¨€æ¨¡å‹",
      "authors": [
        "Chun Wang",
        "Chenyang Liu",
        "Wenze Xu",
        "Weihong Deng"
      ],
      "abstract": "Conversational systems relying on text-based large language models (LLMs) often overlook paralinguistic cues, essential for understanding emotions and intentions. Speech-language models (SLMs), which use speech as input, are emerging as a promising solution. However, SLMs built by extending frozen LLMs struggle to capture paralinguistic information and exhibit reduced context understanding. We identify entangled information and improper training strategies as key issues. To address these issues, we propose two heterogeneous adapters and suggest a weakly supervised training strategy. Our approach disentangles paralinguistic and linguistic information, enabling SLMs to interpret speech through structured representations. It also preserves contextual understanding by avoiding the generation of task-specific vectors through controlled randomness. This approach trains only the adapters on common datasets, ensuring parameter and data efficiency. Experiments demonstrate competitive performance in emotional conversation tasks, showcasing the model's ability to effectively integrate both paralinguistic and linguistic information within contextual settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºæ–‡æœ¬çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æƒ…æ„Ÿå¯¹è¯ä¸­å¿½è§†å‰¯è¯­è¨€(paralinguistic)çº¿ç´¢çš„é—®é¢˜ï¼Œæå‡ºäº†åŒä¿¡æ¯è¯­éŸ³è¯­è¨€æ¨¡å‹ã€‚ä½œè€…è¯†åˆ«å‡ºä¿¡æ¯çº ç¼ å’Œè®­ç»ƒç­–ç•¥ä¸å½“æ˜¯å¯¼è‡´ç°æœ‰è¯­éŸ³è¯­è¨€æ¨¡å‹(SLMs)åœ¨å‰¯è¯­è¨€æ•æ‰å’Œä¸Šä¸‹æ–‡ç†è§£ä¸Šè¡¨ç°ä¸ä½³çš„æ ¸å¿ƒåŸå› ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸¤ç§å¼‚æ„é€‚é…å™¨(heterogeneous adapters)ä»¥åŠä¸€ç§å¼±ç›‘ç£è®­ç»ƒç­–ç•¥(weakly supervised training strategy)ï¼Œæ—¨åœ¨è§£è€¦å‰¯è¯­è¨€ä¸è¯­è¨€ä¿¡æ¯ã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“æ„åŒ–è¡¨ç¤ºä½¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆè§£è¯»è¯­éŸ³ï¼Œå¹¶åˆ©ç”¨å—æ§éšæœºæ€§é¿å…ç”Ÿæˆç‰¹å®šä»»åŠ¡å‘é‡ï¼Œä»è€Œä¿ç•™äº†LLMsåŸæœ‰çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚è¿™ç§ç­–ç•¥ä»…åœ¨é€šç”¨æ•°æ®é›†ä¸Šè®­ç»ƒé€‚é…å™¨ï¼Œæ˜¾è‘—æå‡äº†å‚æ•°å’Œæ•°æ®æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æƒ…æ„Ÿå¯¹è¯ä»»åŠ¡ä¸­å…·æœ‰æå¼ºçš„ç«äº‰åŠ›ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚è¯­å¢ƒä¸‹é›†æˆå‰¯è¯­è¨€ä¸è¯­è¨€ä¿¡æ¯çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "Presented at IEEE ICME 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.08095v1",
      "published_date": "2025-08-11 15:33:44 UTC",
      "updated_date": "2025-08-11 15:33:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:58:14.485980+00:00"
    },
    {
      "arxiv_id": "2508.08091v1",
      "title": "Growing Reservoirs with Developmental Graph Cellular Automata",
      "title_zh": "åˆ©ç”¨å‘è‚²å¼å›¾å…ƒèƒè‡ªåŠ¨æœºç”Ÿé•¿å‚¨å¤‡æ± ",
      "authors": [
        "Matias Barandiaran",
        "James Stovold"
      ],
      "abstract": "Developmental Graph Cellular Automata (DGCA) are a novel model for morphogenesis, capable of growing directed graphs from single-node seeds. In this paper, we show that DGCAs can be trained to grow reservoirs. Reservoirs are grown with two types of targets: task-driven (using the NARMA family of tasks) and task-independent (using reservoir metrics).\n  Results show that DGCAs are able to grow into a variety of specialized, life-like structures capable of effectively solving benchmark tasks, statistically outperforming `typical' reservoirs on the same task. Overall, these lay the foundation for the development of DGCA systems that produce plastic reservoirs and for modeling functional, adaptive morphogenesis.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å‘è‚²å›¾å…ƒèƒè‡ªåŠ¨æœº(Developmental Graph Cellular Automata, DGCA)æ¥ç”Ÿé•¿å‚¨å¤‡æ± (Reservoirs)ï¼ŒDGCAæ˜¯ä¸€ç§èƒ½å¤Ÿä»å•èŠ‚ç‚¹ç§å­ç”Ÿé•¿å‡ºæœ‰å‘å›¾çš„æ–°å‹å½¢æ€å‘ç”Ÿæ¨¡å‹ã€‚ç ”ç©¶è€…é‡‡ç”¨äº†ä»»åŠ¡é©±åŠ¨ï¼ˆNARMAç³»åˆ—ä»»åŠ¡ï¼‰å’Œä»»åŠ¡æ— å…³ï¼ˆå‚¨å¤‡æ± æŒ‡æ ‡ï¼‰ä¸¤ç±»ç›®æ ‡å¯¹DGCAè¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDGCAèƒ½å¤Ÿæ¼”åŒ–å‡ºå¤šç§ä¸“é—¨åŒ–ä¸”å…·æœ‰ç±»ç”Ÿå‘½ç‰¹å¾çš„ç»“æ„ï¼Œèƒ½æœ‰æ•ˆè§£å†³åŸºå‡†ä»»åŠ¡ï¼Œä¸”åœ¨ç»Ÿè®¡å­¦è¡¨ç°ä¸Šä¼˜äºåŒç±»ä»»åŠ¡ä¸­çš„â€œå…¸å‹â€å‚¨å¤‡æ± ã€‚è¿™é¡¹å·¥ä½œä¸ä»…ä¸ºå¼€å‘å…·å¤‡å¯å¡‘æ€§çš„DGCAå‚¨å¤‡æ± ç³»ç»Ÿæä¾›äº†è·¯å¾„ï¼Œä¹Ÿä¸ºåŠŸèƒ½æ€§ã€è‡ªé€‚åº”å½¢æ€å‘ç”Ÿçš„å»ºæ¨¡å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "Accepted to ALIFE 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.08091v1",
      "published_date": "2025-08-11 15:32:01 UTC",
      "updated_date": "2025-08-11 15:32:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:58:11.184828+00:00"
    },
    {
      "arxiv_id": "2508.08088v1",
      "title": "HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches",
      "title_zh": "HierSearchï¼šæ•´åˆæœ¬åœ°ä¸ Web æœç´¢çš„ä¼ä¸šçº§åˆ†å±‚æ·±åº¦æœç´¢æ¡†æ¶",
      "authors": [
        "Jiejun Tan",
        "Zhicheng Dou",
        "Yan Yu",
        "Jiehan Cheng",
        "Qiang Ju",
        "Jian Xie",
        "Ji-Rong Wen"
      ],
      "abstract": "Recently, large reasoning models have demonstrated strong mathematical and coding abilities, and deep search leverages their reasoning capabilities in challenging information retrieval tasks. Existing deep search works are generally limited to a single knowledge source, either local or the Web. However, enterprises often require private deep search systems that can leverage search tools over both local and the Web corpus. Simply training an agent equipped with multiple search tools using flat reinforcement learning (RL) is a straightforward idea, but it has problems such as low training data efficiency and poor mastery of complex tools. To address the above issue, we propose a hierarchical agentic deep search framework, HierSearch, trained with hierarchical RL. At the low level, a local deep search agent and a Web deep search agent are trained to retrieve evidence from their corresponding domains. At the high level, a planner agent coordinates low-level agents and provides the final answer. Moreover, to prevent direct answer copying and error propagation, we design a knowledge refiner that filters out hallucinations and irrelevant evidence returned by low-level agents. Experiments show that HierSearch achieves better performance compared to flat RL, and outperforms various deep search and multi-source retrieval-augmented generation baselines in six benchmarks across general, finance, and medical domains.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ä¸šçº§æ·±å±‚æœç´¢ç³»ç»Ÿåœ¨æ•´åˆæœ¬åœ°(local)ä¸ç½‘é¡µ(Web)çŸ¥è¯†æºæ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºäº†æ‰å¹³å¼ºåŒ–å­¦ä¹ (flat RL)åœ¨è®­ç»ƒæ•ˆç‡å’Œå¤æ‚å·¥å…·æŒæ§æ–¹é¢çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†HierSearchï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåˆ†å±‚å¼ºåŒ–å­¦ä¹ (hierarchical RL)æ„å»ºçš„åˆ†å±‚ä¼ä¸šçº§æ·±å±‚æœç´¢æ¡†æ¶ã€‚è¯¥æ¡†æ¶åœ¨åº•å±‚é€šè¿‡ä¸“é—¨çš„æœ¬åœ°ä¸ç½‘é¡µæœç´¢æ™ºèƒ½ä½“æå–è¯æ®ï¼Œå¹¶åœ¨é«˜å±‚åˆ©ç”¨è§„åˆ’æ™ºèƒ½ä½“(planner agent)è¿›è¡Œåè°ƒï¼ŒåŒæ—¶å¼•å…¥çŸ¥è¯†ç²¾ç‚¼å™¨(knowledge refiner)ä»¥è¿‡æ»¤å¹»è§‰å’Œæ— å…³ä¿¡æ¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHierSearchçš„è¡¨ç°ä¼˜äºæ‰å¹³å¼ºåŒ–å­¦ä¹ ï¼Œå¹¶åœ¨æ¶‰åŠé€šç”¨ã€é‡‘èåŠåŒ»ç–—é¢†åŸŸçš„å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œå…¶æ€§èƒ½æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„å¤šæºæ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)å’Œæ·±å±‚æœç´¢åŸºå‡†æ¨¡å‹ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "Code and datasets are available at https://github.com/plageon/HierSearch",
      "pdf_url": "https://arxiv.org/pdf/2508.08088v1",
      "published_date": "2025-08-11 15:31:47 UTC",
      "updated_date": "2025-08-11 15:31:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:58:12.592684+00:00"
    },
    {
      "arxiv_id": "2508.08075v1",
      "title": "FNBT: Full Negation Belief Transformation for Open-World Information Fusion Based on Dempster-Shafer Theory of Evidence",
      "title_zh": "FNBTï¼šåŸºäº Dempster-Shafer è¯æ®ç†è®ºçš„å¼€æ”¾ä¸–ç•Œä¿¡æ¯èåˆå…¨å¦å®šä¿¡å¿µè½¬æ¢",
      "authors": [
        "Meishen He",
        "Wenjun Ma",
        "Jiao Wang",
        "Huijun Yue",
        "Xiaoma Fan"
      ],
      "abstract": "The Dempster-Shafer theory of evidence has been widely applied in the field of information fusion under uncertainty. Most existing research focuses on combining evidence within the same frame of discernment. However, in real-world scenarios, trained algorithms or data often originate from different regions or organizations, where data silos are prevalent. As a result, using different data sources or models to generate basic probability assignments may lead to heterogeneous frames, for which traditional fusion methods often yield unsatisfactory results. To address this challenge, this study proposes an open-world information fusion method, termed Full Negation Belief Transformation (FNBT), based on the Dempster-Shafer theory. More specially, a criterion is introduced to determine whether a given fusion task belongs to the open-world setting. Then, by extending the frames, the method can accommodate elements from heterogeneous frames. Finally, a full negation mechanism is employed to transform the mass functions, so that existing combination rules can be applied to the transformed mass functions for such information fusion. Theoretically, the proposed method satisfies three desirable properties, which are formally proven: mass function invariance, heritability, and essential conflict elimination. Empirically, FNBT demonstrates superior performance in pattern classification tasks on real-world datasets and successfully resolves Zadeh's counterexample, thereby validating its practical effectiveness.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Dempster-Shafer Theory (DST) è¯æ®ç†è®ºåœ¨å¤„ç†æ¥è‡ªä¸åŒæ•°æ®æºçš„å¼‚æ„è¾¨è¯†æ¡†æ¶æ—¶æ•ˆæœä¸ä½³çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºFNBT (Full Negation Belief Transformation) çš„å¼€ä¸–ç•Œä¿¡æ¯èåˆæ–¹æ³•ã€‚ç ”ç©¶é¦–å…ˆå®šä¹‰äº†ä¸€ä¸ªå‡†åˆ™æ¥åˆ¤å®šèåˆä»»åŠ¡æ˜¯å¦å±äºopen-worldè®¾å®šï¼Œå¹¶é€šè¿‡æ‰©å±•è¾¨è¯†æ¡†æ¶æ¥å…¼å®¹å¼‚æ„å…ƒç´ ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼ŒFNBT å¼•å…¥äº†ä¸€ç§å…¨å¦å®šæœºåˆ¶å¯¹è´¨é‡å‡½æ•°(mass functions)è¿›è¡Œè½¬æ¢ï¼Œä»è€Œå…è®¸åˆ©ç”¨ç°æœ‰çš„ç»„åˆè§„åˆ™è¿›è¡Œä¿¡æ¯èåˆã€‚ç†è®ºå±‚é¢è¯æ˜äº†è¯¥æ–¹æ³•å…·æœ‰è´¨é‡å‡½æ•°ä¸å˜æ€§ã€é—ä¼ æ€§å’ŒåŸºæœ¬å†²çªæ¶ˆé™¤ç­‰ç‰¹æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFNBT åœ¨çœŸå®æ•°æ®é›†çš„æ¨¡å¼åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå¹¶æˆåŠŸè§£å†³äº†ç»å…¸çš„Zadeh's counterexampleé—®é¢˜ï¼ŒéªŒè¯äº†å…¶åœ¨å¤æ‚ä¸ç¡®å®šç¯å¢ƒä¸‹çš„å®ç”¨æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08075v1",
      "published_date": "2025-08-11 15:21:48 UTC",
      "updated_date": "2025-08-11 15:21:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:58:25.272672+00:00"
    },
    {
      "arxiv_id": "2508.08071v2",
      "title": "C-MAG: Cascade Multimodal Attributed Graphs for Supply Chain Link Prediction",
      "title_zh": "C-MAGï¼šé¢å‘ä¾›åº”é“¾é“¾è·¯é¢„æµ‹çš„çº§è”å¤šæ¨¡æ€å±æ€§å›¾",
      "authors": [
        "Yunqing Li",
        "Zixiang Tang",
        "Jiaying Zhuang",
        "Zhenyu Yang",
        "Farhad Ameri",
        "Jianbang Zhang"
      ],
      "abstract": "Workshop version accepted at KDD 2025 (AI4SupplyChain). Connecting an ever-expanding catalogue of products with suitable manufacturers and suppliers is critical for resilient, efficient global supply chains, yet traditional methods struggle to capture complex capabilities, certifications, geographic constraints, and rich multimodal data of real-world manufacturer profiles. To address these gaps, we introduce PMGraph, a public benchmark of bipartite and heterogeneous multimodal supply-chain graphs linking 8,888 manufacturers, over 70k products, more than 110k manufacturer-product edges, and over 29k product images. Building on this benchmark, we propose the Cascade Multimodal Attributed Graph C-MAG, a two-stage architecture that first aligns and aggregates textual and visual attributes into intermediate group embeddings, then propagates them through a manufacturer-product hetero-graph via multiscale message passing to enhance link prediction accuracy. C-MAG also provides practical guidelines for modality-aware fusion, preserving predictive performance in noisy, real-world settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…¨çƒä¾›åº”é“¾ä¸­ä¼ ç»Ÿæ–¹æ³•éš¾ä»¥æ•æ‰å¤æ‚åˆ¶é€ èƒ½åŠ›å’Œå¤šæ¨¡æ€æ•°æ®çš„é—®é¢˜ï¼Œæ¨å‡ºäº† PMGraph å…¬å¼€åŸºå‡†ï¼ŒåŒ…å«8,888ä¸ªåˆ¶é€ å•†å’Œè¶…è¿‡7ä¸‡ä¸ªäº§å“çš„äºŒåˆ†å¼‚æ„å¤šæ¨¡æ€å›¾è°±ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶è€…æå‡ºäº† C-MAGï¼ˆCascade Multimodal Attributed Graphï¼‰ä¸¤é˜¶æ®µæ¶æ„ï¼Œæ—¨åœ¨æå‡ä¾›åº”é“¾çš„ Link Prediction å‡†ç¡®ç‡ã€‚è¯¥æ¶æ„é¦–å…ˆå°†æ–‡æœ¬å’Œè§†è§‰å±æ€§å¯¹é½å¹¶èšåˆä¸ºä¸­é—´ group embeddingsï¼Œéšåé€šè¿‡å¤šå°ºåº¦æ¶ˆæ¯ä¼ é€’ï¼ˆmultiscale message passingï¼‰åœ¨åˆ¶é€ å•†-äº§å“å¼‚æ„å›¾ä¸­è¿›è¡Œä¼ æ’­ã€‚C-MAG è¿˜å¼•å…¥äº† modality-aware fusion å®è·µæŒ‡å—ï¼Œç¡®ä¿åœ¨å­˜åœ¨å™ªå£°çš„çœŸå®ä¸–ç•Œåœºæ™¯ä¸­ä¾ç„¶ä¿æŒç¨³å¥çš„é¢„æµ‹æ€§èƒ½ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆè¿æ¥äº§å“ä¸åˆé€‚çš„ä¾›åº”å•†ï¼Œä¸ºæ„å»ºé«˜æ•ˆä¸”å…·éŸ§æ€§çš„å…¨çƒä¾›åº”é“¾æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "https://openreview.net/pdf?id=mE5n6OJHwO",
      "pdf_url": "https://arxiv.org/pdf/2508.08071v2",
      "published_date": "2025-08-11 15:14:03 UTC",
      "updated_date": "2025-08-13 14:18:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:58:21.263869+00:00"
    },
    {
      "arxiv_id": "2508.08066v2",
      "title": "ExpVG: Investigating the Design Space of Visual Grounding in Multimodal Large Language Model",
      "title_zh": "ExpVGï¼šæ¢ç©¶å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­è§†è§‰å®šä½çš„è®¾è®¡ç©ºé—´",
      "authors": [
        "Weitai Kang",
        "Weiming Zhuang",
        "Zhizhong Li",
        "Yan Yan",
        "Lingjuan Lyu"
      ],
      "abstract": "Fine-grained multimodal capability in Multimodal Large Language Models (MLLMs) has emerged as a critical research direction, particularly for tackling the visual grounding (VG) problem. Despite the strong performance achieved by existing approaches, they often employ disparate design choices when fine-tuning MLLMs for VG, lacking systematic verification to support these designs. To bridge this gap, this paper presents a comprehensive study of various design choices that impact the VG performance of MLLMs. We conduct our analysis using LLaVA-1.5, which has been widely adopted in prior empirical studies of MLLMs. While more recent models exist, we follow this convention to ensure our findings remain broadly applicable and extendable to other architectures. We cover two key aspects: (1) exploring different visual grounding paradigms in MLLMs, identifying the most effective design, and providing our insights; and (2) conducting ablation studies on the design of grounding data to optimize MLLMs' fine-tuning for the VG task. Finally, our findings contribute to a stronger MLLM for VG, achieving improvements of +5.6% / +6.9% / +7.0% on RefCOCO/+/g over the LLaVA-1.5.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (Multimodal Large Language Models, MLLMs) åœ¨è§†è§‰å®šä½ (Visual Grounding, VG) ä»»åŠ¡ä¸­è®¾è®¡é€‰æ‹©ç¼ºä¹ç³»ç»Ÿæ€§éªŒè¯çš„é—®é¢˜ï¼Œæ·±å…¥æ¢è®¨äº† ExpVG çš„è®¾è®¡ç©ºé—´ã€‚ç ”ç©¶äººå‘˜ä»¥ LLaVA-1.5 ä¸ºåŸºç¡€æ¨¡å‹ï¼Œå…¨é¢åˆ†æäº†å½±å“ VG æ€§èƒ½çš„å¤šç§è®¾è®¡å› ç´ ï¼Œæ—¨åœ¨å¡«è¡¥ç°æœ‰ç ”ç©¶åœ¨å¾®è°ƒç­–ç•¥ä¸Šçš„ç©ºç™½ã€‚ç ”ç©¶é‡ç‚¹æ¶µç›–äº†ä¸¤ä¸ªæ ¸å¿ƒç»´åº¦ï¼šé¦–å…ˆæ˜¯å¯¹æ¯”ä¸åŒçš„è§†è§‰å®šä½èŒƒå¼ (Visual Grounding Paradigms) ä»¥ç¡®å®šæœ€ä½³è®¾è®¡è·¯å¾„ï¼Œå…¶æ¬¡æ˜¯é’ˆå¯¹å®šä½æ•°æ® (Grounding Data) çš„è®¾è®¡å¼€å±•æ¶ˆèå®éªŒï¼Œä»è€Œä¼˜åŒ– MLLMs çš„å¾®è°ƒæ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç ”ç©¶æ‰€è¯†åˆ«çš„æœ€ä½³è®¾è®¡æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œåœ¨ RefCOCOã€RefCOCO+ å’Œ RefCOCOg åŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«æ¯” LLaVA-1.5 æé«˜äº† 5.6%ã€6.9% å’Œ 7.0%ã€‚æ­¤é¡¹ç³»ç»Ÿæ€§ç ”ç©¶ä¸ä»…ä¸º MLLMs çš„è§†è§‰å®šä½èƒ½åŠ›æä¾›äº†è®¾è®¡å‡†åˆ™ï¼Œä¹Ÿä¸ºæœªæ¥æ›´å¼ºå®šä½æ¨¡å‹çš„å¼€å‘å¥ å®šäº†å®éªŒåŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages for the main paper",
      "pdf_url": "https://arxiv.org/pdf/2508.08066v2",
      "published_date": "2025-08-11 15:10:52 UTC",
      "updated_date": "2025-08-19 23:46:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:58:40.957239+00:00"
    },
    {
      "arxiv_id": "2508.08350v1",
      "title": "Fuzzy-Pattern Tsetlin Machine",
      "title_zh": "æ¨¡ç³Šæ¨¡å¼ Tsetlin æœº",
      "authors": [
        "Artem Hnilov"
      ],
      "abstract": "The \"all-or-nothing\" clause evaluation strategy is a core mechanism in the Tsetlin Machine (TM) family of algorithms. In this approach, each clause - a logical pattern composed of binary literals mapped to input data - is disqualified from voting if even a single literal fails. Due to this strict requirement, standard TMs must employ thousands of clauses to achieve competitive accuracy. This paper introduces the Fuzzy-Pattern Tsetlin Machine (FPTM), a novel variant where clause evaluation is fuzzy rather than strict. If some literals in a clause fail, the remaining ones can still contribute to the overall vote with a proportionally reduced score. As a result, each clause effectively consists of sub-patterns that adapt individually to the input, enabling more flexible, efficient, and robust pattern matching. The proposed fuzzy mechanism significantly reduces the required number of clauses, memory footprint, and training time, while simultaneously improving accuracy. On the IMDb dataset, FPTM achieves 90.15% accuracy with only one clause per class, a 50x reduction in clauses and memory over the Coalesced Tsetlin Machine. FPTM trains up to 316x faster (45 seconds vs. 4 hours) and fits within 50 KB, enabling online learning on microcontrollers. Inference throughput reaches 34.5 million predictions/second (51.4 GB/s). On Fashion-MNIST, accuracy reaches 92.18% (2 clauses), 93.19% (20 clauses) and 94.68% (8000 clauses), a ~400x clause reduction compared to the Composite TM's 93.00% (8000 clauses). On the Amazon Sales dataset with 20% noise, FPTM achieves 85.22% accuracy, significantly outperforming the Graph Tsetlin Machine (78.17%) and a Graph Convolutional Neural Network (66.23%).",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Fuzzy-Pattern Tsetlin Machine (FPTM)ï¼Œæ—¨åœ¨æ”¹è¿›ä¼ ç»ŸTsetlin Machineä¸­ä¸¥æ ¼çš„all-or-nothingå­å¥è¯„ä¼°ç­–ç•¥ã€‚FPTMå¼•å…¥äº†æ¨¡ç³Šè¯„ä¼°æœºåˆ¶ï¼Œä½¿å¾—å­å¥åœ¨éƒ¨åˆ†æ–‡å­—(literals)åŒ¹é…å¤±è´¥æ—¶ï¼Œå‰©ä½™éƒ¨åˆ†ä»èƒ½ä»¥æŒ‰æ¯”ä¾‹ç¼©å‡çš„åˆ†æ•°è´¡çŒ®æŠ•ç¥¨ï¼Œä»è€Œå®ç°äº†æ›´çµæ´»ã€é«˜æ•ˆä¸”é²æ£’çš„æ¨¡å¼åŒ¹é…ã€‚è¿™ç§æœºåˆ¶æ˜¾è‘—å‡å°‘äº†æ‰€éœ€çš„å­å¥æ•°é‡ã€å†…å­˜å ç”¨å’Œè®­ç»ƒæ—¶é—´ï¼ŒåŒæ—¶æå‡äº†é¢„æµ‹å‡†ç¡®ç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFPTMåœ¨IMDbæ•°æ®é›†ä¸Šä»…éœ€æ¯ä¸ªç±»åˆ«ä¸€ä¸ªå­å¥å³å¯è¾¾åˆ°90.15%çš„å‡†ç¡®ç‡ï¼Œè®­ç»ƒé€Ÿåº¦æå‡é«˜è¾¾316å€ï¼Œä¸”æ¨¡å‹å¤§å°ä»…ä¸º50 KBï¼Œé€‚åˆåœ¨å¾®æ§åˆ¶å™¨ä¸Šè¿›è¡Œåœ¨çº¿å­¦ä¹ ã€‚æ­¤å¤–ï¼Œåœ¨å¤„ç†å«æœ‰20%å™ªå£°çš„Amazon Salesæ•°æ®é›†æ—¶ï¼ŒFPTMçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºGraph Tsetlin Machineå’ŒGraph Convolutional Neural Networkï¼Œå±•ç°å‡ºå¼ºå¤§çš„æŠ—å™ªèƒ½åŠ›å’Œæé«˜çš„æ¨ç†ååé‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 3 figures, 6 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.08350v1",
      "published_date": "2025-08-11 15:09:12 UTC",
      "updated_date": "2025-08-11 15:09:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:59:06.841903+00:00"
    },
    {
      "arxiv_id": "2508.08053v1",
      "title": "AdaptFlow: Adaptive Workflow Optimization via Meta-Learning",
      "title_zh": "AdaptFlowï¼šåŸºäºå…ƒå­¦ä¹ çš„è‡ªé€‚åº”å·¥ä½œæµä¼˜åŒ–",
      "authors": [
        "Runchuan Zhu",
        "Bowen Jiang",
        "Lingrui Mei",
        "Fangkai Yang",
        "Lu Wang",
        "Haoxiang Gao",
        "Fengshuo Bai",
        "Pu Zhao",
        "Qingwei Lin",
        "Saravan Rajmohan",
        "Dongmei Zhang"
      ],
      "abstract": "Recent advances in large language models (LLMs) have sparked growing interest in agentic workflows, which are structured sequences of LLM invocations intended to solve complex tasks. However, existing approaches often rely on static templates or manually designed workflows, which limit adaptability to diverse tasks and hinder scalability. We propose AdaptFlow, a natural language-based meta-learning framework inspired by model-agnostic meta-learning (MAML). AdaptFlow learns a generalizable workflow initialization that enables rapid subtask-level adaptation. It employs a bi-level optimization scheme: the inner loop refines the workflow for a specific subtask using LLM-generated feedback, while the outer loop updates the shared initialization to perform well across tasks. This setup allows AdaptFlow to generalize effectively to unseen tasks by adapting the initialized workflow through language-guided modifications. Evaluated across question answering, code generation, and mathematical reasoning benchmarks, AdaptFlow consistently outperforms both manually crafted and automatically searched baselines, achieving state-of-the-art results with strong generalization across tasks and models. The source code and data are available at https://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AdaptFlowï¼Œä¸€ç§å—æ¨¡å‹æ— å…³å…ƒå­¦ä¹  (Model-Agnostic Meta-Learning, MAML) å¯å‘çš„è‡ªç„¶è¯­è¨€å…ƒå­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„æ™ºèƒ½ä»£ç†å·¥ä½œæµ (Agentic Workflows)ã€‚é’ˆå¯¹ç°æœ‰å·¥ä½œæµä¾èµ–é™æ€æ¨¡æ¿ã€ç¼ºä¹é€‚åº”æ€§å’Œæ‰©å±•æ€§çš„é—®é¢˜ï¼ŒAdaptFlow é€šè¿‡å­¦ä¹ å¯æ³›åŒ–çš„å·¥ä½œæµåˆå§‹åŒ–ï¼Œå®ç°å­ä»»åŠ¡çº§åˆ«çš„å¿«é€Ÿé€‚åº”ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŒå±‚ä¼˜åŒ– (Bi-level Optimization) æ–¹æ¡ˆï¼Œå†…å±‚å¾ªç¯æ ¹æ® LLM ç”Ÿæˆçš„åé¦ˆå¯¹ç‰¹å®šå­ä»»åŠ¡è¿›è¡Œå·¥ä½œæµå¾®è°ƒï¼Œè€Œå¤–å±‚å¾ªç¯è´Ÿè´£æ›´æ–°å…±äº«åˆå§‹åŒ–ä»¥ç¡®ä¿è·¨ä»»åŠ¡çš„ä¼˜å¼‚è¡¨ç°ã€‚è¿™ç§æœºåˆ¶å…è®¸ AdaptFlow é€šè¿‡è¯­è¨€å¼•å¯¼çš„ä¿®æ”¹ï¼Œå°†åˆå§‹åŒ–å·¥ä½œæµæœ‰æ•ˆæ¨å¹¿è‡³æœªè§è¿‡çš„ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAdaptFlow åœ¨é—®ç­” (Question Answering)ã€ä»£ç ç”Ÿæˆ (Code Generation) å’Œæ•°å­¦æ¨ç† (Mathematical Reasoning) åŸºå‡†æµ‹è¯•ä¸­ä¸€è‡´ä¼˜äºæ‰‹åŠ¨æˆ–è‡ªåŠ¨æœç´¢çš„åŸºå‡†æ–¹æ³•ï¼Œè¾¾åˆ°äº† SOTA æ€§èƒ½å¹¶å±•ç¤ºäº†æå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08053v1",
      "published_date": "2025-08-11 14:52:59 UTC",
      "updated_date": "2025-08-11 14:52:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:59:06.001242+00:00"
    },
    {
      "arxiv_id": "2508.08052v2",
      "title": "On Understanding of the Dynamics of Model Capacity in Continual Learning",
      "title_zh": "æ·±å…¥æ¢ç©¶æŒç»­å­¦ä¹ ä¸­æ¨¡å‹å®¹é‡çš„åŠ¨æ€æ¼”åŒ–",
      "authors": [
        "Supriyo Chakraborty",
        "Krishnan Raghavan"
      ],
      "abstract": "The stability-plasticity dilemma, closely related to a neural network's (NN) capacity-its ability to represent tasks-is a fundamental challenge in continual learning (CL). Within this context, we introduce CL's effective model capacity (CLEMC) that characterizes the dynamic behavior of the stability-plasticity balance point. We develop a difference equation to model the evolution of the interplay between the NN, task data, and optimization procedure. We then leverage CLEMC to demonstrate that the effective capacity-and, by extension, the stability-plasticity balance point is inherently non-stationary. We show that regardless of the NN architecture or optimization method, a NN's ability to represent new tasks diminishes when incoming task distributions differ from previous ones. We conduct extensive experiments to support our theoretical findings, spanning a range of architectures-from small feedforward network and convolutional networks to medium-sized graph neural networks and transformer-based large language models with millions of parameters.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æŒç»­å­¦ä¹ (Continual Learning)ä¸­çš„ç¨³å®šæ€§-å¡‘æ€§ä¸¤éš¾(stability-plasticity dilemma)é—®é¢˜ï¼Œå¹¶å¼•å…¥äº†æŒç»­å­¦ä¹ æœ‰æ•ˆæ¨¡å‹å®¹é‡(CLEMC)è¿™ä¸€æ¦‚å¿µæ¥è¡¨å¾ç¨³å®šæ€§-å¡‘æ€§å¹³è¡¡ç‚¹çš„åŠ¨æ€è¡Œä¸ºã€‚ä½œè€…å¼€å‘äº†ä¸€ä¸ªå·®åˆ†æ–¹ç¨‹(difference equation)æ¥æ¨¡æ‹Ÿç¥ç»ç½‘ç»œã€ä»»åŠ¡æ•°æ®ä¸ä¼˜åŒ–è¿‡ç¨‹ä¹‹é—´äº¤äº’çš„æ¼”åŒ–è¿‡ç¨‹ã€‚ç ”ç©¶é€šè¿‡CLEMCè¯æ˜äº†æœ‰æ•ˆå®¹é‡ä»¥åŠç¨³å®šæ€§-å¡‘æ€§å¹³è¡¡ç‚¹æœ¬è´¨ä¸Šæ˜¯éå¹³ç¨³çš„(non-stationary)ï¼Œæ­ç¤ºäº†å½“æ–°ä»»åŠ¡åˆ†å¸ƒä¸æ—§ä»»åŠ¡ä¸åŒæ—¶ï¼Œæ— è®ºé‡‡ç”¨ä½•ç§ç½‘ç»œæ¶æ„æˆ–ä¼˜åŒ–æ–¹æ³•ï¼Œç¥ç»ç½‘ç»œè¡¨ç¤ºæ–°ä»»åŠ¡çš„èƒ½åŠ›éƒ½ä¼šä¸‹é™ã€‚è¯¥ç ”ç©¶è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒä»¥æ”¯æŒå…¶ç†è®ºå‘ç°ï¼Œå®éªŒæ¶µç›–äº†ä»å°å‹å‰é¦ˆç½‘ç»œã€å·ç§¯ç½‘ç»œåˆ°ä¸­å‹å›¾ç¥ç»ç½‘ç»œä»¥åŠæ‹¥æœ‰æ•°ç™¾ä¸‡å‚æ•°çš„Transformerå¤§è¯­è¨€æ¨¡å‹(LLMs)ç­‰å¤šç§æ¶æ„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08052v2",
      "published_date": "2025-08-11 14:52:56 UTC",
      "updated_date": "2025-08-14 12:42:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:59:06.240630+00:00"
    },
    {
      "arxiv_id": "2508.08047v1",
      "title": "Rethinking Self-Replication: Detecting Distributed Selfhood in the Outlier Cellular Automaton",
      "title_zh": "é‡æ–°å®¡è§†è‡ªæˆ‘å¤åˆ¶ï¼šè¯†åˆ« Outlier å…ƒèƒè‡ªåŠ¨æœºä¸­çš„åˆ†å¸ƒå¼ä¸ªä½“æ€§",
      "authors": [
        "Arend Hintze",
        "Clifford Bohm"
      ],
      "abstract": "Spontaneous self-replication in cellular automata has long been considered rare, with most known examples requiring careful design or artificial initialization. In this paper, we present formal, causal evidence that such replication can emerge unassisted -- and that it can do so in a distributed, multi-component form. Building on prior work identifying complex dynamics in the Outlier rule, we introduce a data-driven framework that reconstructs the full causal ancestry of patterns in a deterministic cellular automaton. This allows us to rigorously identify self-replicating structures via explicit causal lineages. Our results show definitively that self-replicators in the Outlier CA are not only spontaneous and robust, but are also often composed of multiple disjoint clusters working in coordination, raising questions about some conventional notions of individuality and replication in artificial life systems.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é‡æ–°å®¡è§†äº†å…ƒèƒè‡ªåŠ¨æœº(Cellular Automata)ä¸­çš„è‡ªå‘è‡ªå¤åˆ¶ç°è±¡ï¼ŒæŒ‘æˆ˜äº†ä»¥å¾€è®¤ä¸ºè¯¥è¿‡ç¨‹ç¨€æœ‰ä¸”éœ€è¦äººå·¥è®¾è®¡çš„ä¼ ç»Ÿè§‚ç‚¹ã€‚ä½œè€…å¼•å…¥äº†ä¸€ä¸ªæ•°æ®é©±åŠ¨çš„æ¡†æ¶ï¼Œé€šè¿‡é‡æ„ç¡®å®šæ€§å…ƒèƒè‡ªåŠ¨æœºä¸­æ¨¡å¼çš„å®Œæ•´å› æœç¥–å…ˆ(causal ancestry)ï¼Œåˆ©ç”¨æ˜¾å¼çš„å› æœè°±ç³»(causal lineages)æ¥ä¸¥è°¨åœ°è¯†åˆ«è‡ªå¤åˆ¶ç»“æ„ã€‚ç ”ç©¶è¯æ˜ï¼Œåœ¨ Outlier è§„åˆ™ä¸‹ï¼Œè‡ªå¤åˆ¶ä¸ä»…å¯ä»¥æ— è¾…åŠ©åœ°è‡ªå‘äº§ç”Ÿï¼Œè€Œä¸”è¡¨ç°å‡ºæå¼ºçš„ç¨³å¥æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›è‡ªå¤åˆ¶å­é€šå¸¸ç”±å¤šä¸ªååŒå·¥ä½œçš„ä¸ç›¸è¿ç°‡(disjoint clusters)ç»„æˆï¼Œå‘ˆç°å‡ºä¸€ç§åˆ†å¸ƒå¼çš„å¤šç»„ä»¶å½¢å¼ã€‚è¿™ä¸€å‘ç°å¯¹äººå·¥ç”Ÿå‘½(Artificial Life)ç³»ç»Ÿä¸­å…³äºä¸ªä½“æ€§(individuality)å’Œå¤åˆ¶çš„ä¼ ç»Ÿå®šä¹‰æå‡ºäº†æŒ‘æˆ˜ï¼Œä¸ºç†è§£å¤æ‚ç³»ç»Ÿçš„æ¼”åŒ–æä¾›äº†æ–°çš„ç§‘å­¦è¯æ®ã€‚",
      "categories": [
        "nlin.CG",
        "cs.AI"
      ],
      "primary_category": "nlin.CG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08047v1",
      "published_date": "2025-08-11 14:49:11 UTC",
      "updated_date": "2025-08-11 14:49:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:59:07.248288+00:00"
    },
    {
      "arxiv_id": "2508.08042v1",
      "title": "Multi-modal Adaptive Mixture of Experts for Cold-start Recommendation",
      "title_zh": "é¢å‘å†·å¯åŠ¨æ¨èçš„å¤šæ¨¡æ€è‡ªé€‚åº”æ··åˆä¸“å®¶æ¨¡å‹",
      "authors": [
        "Van-Khang Nguyen",
        "Duc-Hoang Pham",
        "Huy-Son Nguyen",
        "Cam-Van Thi Nguyen",
        "Hoang-Quynh Le",
        "Duc-Trong Le"
      ],
      "abstract": "Recommendation systems have faced significant challenges in cold-start scenarios, where new items with a limited history of interaction need to be effectively recommended to users. Though multimodal data (e.g., images, text, audio, etc.) offer rich information to address this issue, existing approaches often employ simplistic integration methods such as concatenation, average pooling, or fixed weighting schemes, which fail to capture the complex relationships between modalities. Our study proposes a novel Mixture of Experts (MoE) framework for multimodal cold-start recommendation, named MAMEX, which dynamically leverages latent representation from different modalities. MAMEX utilizes modality-specific expert networks and introduces a learnable gating mechanism that adaptively weights the contribution of each modality based on its content characteristics. This approach enables MAMEX to emphasize the most informative modalities for each item while maintaining robustness when certain modalities are less relevant or missing. Extensive experiments on benchmark datasets show that MAMEX outperforms state-of-the-art methods in cold-start scenarios, with superior accuracy and adaptability. For reproducibility, the code has been made available on Github https://github.com/L2R-UET/MAMEX.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¨èç³»ç»Ÿåœ¨å¤„ç†æ–°é¡¹ç›®æ—¶é¢ä¸´çš„å†·å¯åŠ¨ (Cold-start) æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º MAMEX çš„å¤šæ¨¡æ€è‡ªé€‚åº”ä¸“å®¶æ··åˆ (Mixture of Experts) æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨èåˆå¤šæ¨¡æ€æ•°æ®æ—¶ä»…é‡‡ç”¨ç®€å•æ‹¼æ¥æˆ–å›ºå®šæƒé‡ï¼Œå¯¼è‡´æ— æ³•æ•æ‰æ¨¡æ€é—´å¤æ‚å…³ç³»çš„å±€é™æ€§ï¼ŒMAMEX å¼•å…¥äº†æ¨¡æ€ç‰¹å®šä¸“å®¶ç½‘ç»œå’Œå¯å­¦ä¹ çš„é—¨æ§æœºåˆ¶ (Gating Mechanism)ã€‚è¯¥æœºåˆ¶èƒ½å¤Ÿæ ¹æ®é¡¹ç›®å†…å®¹ç‰¹å¾åŠ¨æ€è°ƒæ•´å„æ¨¡æ€çš„æƒé‡ï¼Œä»è€Œåœ¨å¼ºè°ƒå…³é”®ä¿¡æ¯çš„åŒæ—¶ï¼Œç¡®ä¿åœ¨æ¨¡æ€ç¼ºå¤±æˆ–æ— å…³æ—¶çš„é²æ£’æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMAMEX åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„ State-of-the-art æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥è®ºæ–‡çš„æ ¸å¿ƒä»£ç å·²åœ¨ Github å¹³å°å…¬å¼€ï¼Œä¸ºç ”ç©¶çš„å¤ç°æä¾›äº†æ”¯æŒã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08042v1",
      "published_date": "2025-08-11 14:47:14 UTC",
      "updated_date": "2025-08-11 14:47:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:59:33.388759+00:00"
    },
    {
      "arxiv_id": "2508.08040v3",
      "title": "BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning in Multimodal Models",
      "title_zh": "BadPromptFLï¼šä¸€ç§é’ˆå¯¹å¤šæ¨¡æ€æ¨¡å‹ä¸­åŸºäºæç¤ºçš„è”é‚¦å­¦ä¹ çš„æ–°å‹åé—¨å¨èƒ",
      "authors": [
        "Maozhen Zhang",
        "Mengnan Zhao",
        "Wei Wang",
        "Bo Wang"
      ],
      "abstract": "Prompt-based tuning has emerged as a lightweight alternative to full fine-tuning in large vision-language models, enabling efficient adaptation via learned contextual prompts. This paradigm has recently been extended to federated learning settings (e.g., PromptFL), where clients collaboratively train prompts under data privacy constraints. However, the security implications of prompt-based aggregation in federated multimodal learning remain largely unexplored, leaving a critical attack surface unaddressed. In this paper, we introduce \\textbf{BadPromptFL}, the first backdoor attack targeting prompt-based federated learning in multimodal contrastive models. In BadPromptFL, compromised clients jointly optimize local backdoor triggers and prompt embeddings, injecting poisoned prompts into the global aggregation process. These prompts are then propagated to benign clients, enabling universal backdoor activation at inference without modifying model parameters. Leveraging the contextual learning behavior of CLIP-style architectures, BadPromptFL achieves high attack success rates (e.g., \\(>90\\%\\)) with minimal visibility and limited client participation. Extensive experiments across multiple datasets and aggregation protocols validate the effectiveness, stealth, and generalizability of our attack, raising critical concerns about the robustness of prompt-based federated learning in real-world deployments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†BadPromptFLï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹å¤šæ¨¡æ€å¯¹æ¯”æ¨¡å‹ä¸­åŸºäºæç¤ºè¯çš„è”é‚¦å­¦ä¹ (Prompt-based Federated Learning)çš„åé—¨æ”»å‡»æ¡†æ¶ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œæ¶æ„å®¢æˆ·ç«¯å¯ä»¥é€šè¿‡ååŒä¼˜åŒ–æœ¬åœ°åé—¨è§¦å‘å™¨å’Œæç¤ºè¯åµŒå…¥(prompt embeddings)ï¼Œå°†ä¸­æ¯’çš„æç¤ºè¯æ³¨å…¥å…¨å±€èšåˆè¿‡ç¨‹ã€‚è¿™äº›ä¸­æ¯’æç¤ºè¯éšåè¢«ä¼ æ’­è‡³è‰¯æ€§å®¢æˆ·ç«¯ï¼Œåˆ©ç”¨CLIPç­‰æ¶æ„çš„ä¸Šä¸‹æ–‡å­¦ä¹ è¡Œä¸ºï¼Œåœ¨æ— éœ€ä¿®æ”¹æ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹å®ç°æ¨ç†é˜¶æ®µçš„é€šç”¨åé—¨æ¿€æ´»ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBadPromptFLåœ¨æä½çš„å®¢æˆ·ç«¯å‚ä¸åº¦ä¸‹ä»èƒ½è¾¾åˆ°è¶…è¿‡90%çš„æ”»å‡»æˆåŠŸç‡ï¼Œè¡¨ç°å‡ºæé«˜çš„éšè”½æ€§å’Œæ”»å‡»æ•ˆèƒ½ã€‚è¯¥æ”»å‡»åœ¨å¤šç§æ•°æ®é›†å’Œèšåˆåè®®ä¸Šå‡å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ­ç¤ºäº†åŸºäºæç¤ºè¯çš„è”é‚¦å­¦ä¹ åœ¨ç°å®éƒ¨ç½²ä¸­çš„é²æ£’æ€§éšæ‚£ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†åœ¨å¤šæ¨¡æ€åä½œå­¦ä¹ ä¸­åŠ å¼ºå®‰å…¨é˜²å¾¡æœºåˆ¶çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08040v3",
      "published_date": "2025-08-11 14:42:44 UTC",
      "updated_date": "2025-09-06 08:54:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:59:40.595074+00:00"
    },
    {
      "arxiv_id": "2508.08030v1",
      "title": "Exploring Strategies for Personalized Radiation Therapy: Part III Identifying genetic determinants for Radiation Response with Meta Learning",
      "title_zh": "æ¢ç´¢ä¸ªä½“åŒ–æ”¾å°„æ²»ç–—ç­–ç•¥ï¼šç¬¬ä¸‰éƒ¨åˆ† åŸºäºå…ƒå­¦ä¹ è¯†åˆ«æ”¾å°„ååº”çš„é—ä¼ å†³å®šå› ç´ ",
      "authors": [
        "Hao Peng",
        "Yuanyuan Zhang",
        "Steve Jiang",
        "Robert Timmerman",
        "John Minna"
      ],
      "abstract": "Radiation response in cancer is shaped by complex, patient specific biology, yet current treatment strategies often rely on uniform dose prescriptions without accounting for tumor heterogeneity. In this study, we introduce a meta learning framework for one-shot prediction of radiosensitivity measured by SF2 using cell line level gene expression data. Unlike the widely used Radiosensitivity Index RSI a rank-based linear model trained on a fixed 10-gene signature, our proposed meta-learned model allows the importance of each gene to vary by sample through fine tuning. This flexibility addresses key limitations of static models like RSI, which assume uniform gene contributions across tumor types and discard expression magnitude and gene gene interactions. Our results show that meta learning offers robust generalization to unseen samples and performs well in tumor subgroups with high radiosensitivity variability, such as adenocarcinoma and large cell carcinoma. By learning transferable structure across tasks while preserving sample specific adaptability, our approach enables rapid adaptation to individual samples, improving predictive accuracy across diverse tumor subtypes while uncovering context dependent patterns of gene influence that may inform personalized therapy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºé¢„æµ‹æ”¾å°„æ•æ„Ÿæ€§(Radiosensitivity)çš„å…ƒå­¦ä¹ (Meta Learning)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰æ”¾å°„æ²»ç–—ä¸­ç»Ÿä¸€å‰‚é‡å¤„æ–¹å¿½è§†è‚¿ç˜¤å¼‚è´¨æ€§çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç»†èƒç³»æ°´å¹³çš„åŸºå› è¡¨è¾¾æ•°æ®å¯¹SF2æŒ‡æ ‡è¿›è¡Œå•æ ·æœ¬(One-shot)é¢„æµ‹ï¼Œå…‹æœäº†ä¼ ç»Ÿæ”¾å°„æ•æ„Ÿæ€§æŒ‡æ•°(Radiosensitivity Index, RSI)ç­‰é™æ€çº¿æ€§æ¨¡å‹åœ¨åŸºå› è´¡çŒ®æƒé‡å’ŒåŸºå› é—´ç›¸äº’ä½œç”¨å¤„ç†ä¸Šçš„å±€é™ã€‚é€šè¿‡å¾®è°ƒæœºåˆ¶ï¼Œè¯¥æ¨¡å‹å…è®¸æ¯ä¸ªåŸºå› çš„é‡è¦æ€§éšæ ·æœ¬åŠ¨æ€å˜åŒ–ï¼Œä»è€Œåœ¨ä¿ç•™æ ·æœ¬ç‰¹å¼‚æ€§é€‚é…çš„åŒæ—¶å­¦ä¹ å¯è¿ç§»çš„ç‰¹å¾ç»“æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥å…ƒå­¦ä¹ æ¨¡å‹åœ¨æœªè§æ ·æœ¬ä¸Šå…·æœ‰ç¨³å¥çš„æ³›åŒ–æ€§èƒ½ï¼Œå°¤å…¶åœ¨è…ºç™Œ(Adenocarcinoma)å’Œå¤§ç»†èƒç™Œ(Large Cell Carcinoma)ç­‰æ•æ„Ÿæ€§å˜å¼‚è¾ƒå¤§çš„è‚¿ç˜¤äºšå‹ä¸­è¡¨ç°å‡ºè‰²ã€‚è¯¥æ–¹æ³•ä¸ä»…æ˜¾è‘—æå‡äº†è·¨å¤šæ ·åŒ–è‚¿ç˜¤äºšå‹çš„é¢„æµ‹å‡†ç¡®ç‡ï¼Œè¿˜æ­ç¤ºäº†åŸºå› å½±å“çš„è¯­å¢ƒä¾èµ–æ¨¡å¼ï¼Œä¸ºå®ç°æ›´ç²¾å‡†çš„ä¸ªæ€§åŒ–æ”¾å°„æ²»ç–—å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "physics.med-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "physics.med-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08030v1",
      "published_date": "2025-08-11 14:34:18 UTC",
      "updated_date": "2025-08-11 14:34:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:59:38.982619+00:00"
    },
    {
      "arxiv_id": "2508.08027v1",
      "title": "Bridging ASR and LLMs for Dysarthric Speech Recognition: Benchmarking Self-Supervised and Generative Approaches",
      "title_zh": "è¡”æ¥ ASR ä¸å¤§è¯­è¨€æ¨¡å‹ä»¥å®ç°æ„éŸ³éšœç¢è¯­éŸ³è¯†åˆ«ï¼šè‡ªç›‘ç£ä¸ç”Ÿæˆå¼æ–¹æ³•çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Ahmed Aboeitta",
        "Ahmed Sharshar",
        "Youssef Nafea",
        "Shady Shehata"
      ],
      "abstract": "Speech Recognition (ASR) due to phoneme distortions and high variability. While self-supervised ASR models like Wav2Vec, HuBERT, and Whisper have shown promise, their effectiveness in dysarthric speech remains unclear. This study systematically benchmarks these models with different decoding strategies, including CTC, seq2seq, and LLM-enhanced decoding (BART,GPT-2, Vicuna). Our contributions include (1) benchmarking ASR architectures for dysarthric speech, (2) introducing LLM-based decoding to improve intelligibility, (3) analyzing generalization across datasets, and (4) providing insights into recognition errors across severity levels. Findings highlight that LLM-enhanced decoding improves dysarthric ASR by leveraging linguistic constraints for phoneme restoration and grammatical correction.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Dysarthric Speech Recognition ä¸­å› éŸ³ç´ å¤±çœŸå’Œé«˜å˜å¼‚æ€§å¯¼è‡´çš„è¯†åˆ«éš¾é¢˜ï¼Œç³»ç»Ÿåœ°è¯„ä¼°äº†è‡ªç›‘ç£ ASR æ¨¡å‹ä¸ç”Ÿæˆå¼ LLMs çš„ç»“åˆæ•ˆæœã€‚ç ”ç©¶å›¢é˜Ÿå¯¹ Wav2Vecã€HuBERT å’Œ Whisper ç­‰ ASR æ¶æ„è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶å¯¹æ¯”äº† CTCã€seq2seq ä»¥åŠåŸºäº BARTã€GPT-2 å’Œ Vicuna çš„ LLM å¢å¼ºå‹è§£ç ç­–ç•¥ã€‚å…¶ä¸»è¦è´¡çŒ®åŒ…æ‹¬å»ºç«‹äº†æ„éŸ³éšœç¢è¯­éŸ³çš„ ASR æ¶æ„åŸºå‡†ã€å¼•å…¥ LLM è§£ç ä»¥æå‡è¯­éŸ³å¯æ‡‚åº¦ï¼Œå¹¶æ·±å…¥åˆ†æäº†æ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†å’Œä¸¥é‡ç¨‹åº¦ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLM å¢å¼ºå‹è§£ç èƒ½å¤Ÿé€šè¿‡åˆ©ç”¨è¯­è¨€çº¦æŸï¼ˆLinguistic Constraintsï¼‰æœ‰æ•ˆå®ç° Phoneme restoration å’Œ Grammatical correctionï¼Œæ˜¾è‘—æ”¹å–„äº†è¯†åˆ«ç»“æœã€‚è¯¥ç ”ç©¶ä¸ºç†è§£ ASR é”™è¯¯åˆ†å¸ƒåŠåˆ©ç”¨ç”Ÿæˆå¼æ¨¡å‹ä¼˜åŒ–æ®‹éšœè¯­éŸ³è¯†åˆ«æä¾›äº†é‡è¦è§è§£ä¸æ–¹æ³•è®ºæ”¯æŒã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08027v1",
      "published_date": "2025-08-11 14:31:20 UTC",
      "updated_date": "2025-08-11 14:31:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:59:34.998209+00:00"
    },
    {
      "arxiv_id": "2508.08019v2",
      "title": "Advancing Knowledge Tracing by Exploring Follow-up Performance Trends",
      "title_zh": "é€šè¿‡æ¢ç´¢åç»­è¡¨ç°è¶‹åŠ¿æå‡çŸ¥è¯†è¿½è¸ªæ€§èƒ½",
      "authors": [
        "Hengyu Liu",
        "Yushuai Li",
        "Minghe Yu",
        "Tiancheng Zhang",
        "Ge Yu",
        "Torben Bach Pedersen",
        "Kristian Torp",
        "Christian S. Jensen",
        "Tianyi Li"
      ],
      "abstract": "Intelligent Tutoring Systems (ITS), such as Massive Open Online Courses, offer new opportunities for human learning. At the core of such systems, knowledge tracing (KT) predicts students' future performance by analyzing their historical learning activities, enabling an accurate evaluation of students' knowledge states over time. We show that existing KT methods often encounter correlation conflicts when analyzing the relationships between historical learning sequences and future performance. To address such conflicts, we propose to extract so-called Follow-up Performance Trends (FPTs) from historical ITS data and to incorporate them into KT. We propose a method called Forward-Looking Knowledge Tracing (FINER) that combines historical learning sequences with FPTs to enhance student performance prediction accuracy. FINER constructs learning patterns that facilitate the retrieval of FPTs from historical ITS data in linear time; FINER includes a novel similarity-aware attention mechanism that aggregates FPTs based on both frequency and contextual similarity; and FINER offers means of combining FPTs and historical learning sequences to enable more accurate prediction of student future performance. Experiments on six real-world datasets show that FINER can outperform ten state-of-the-art KT methods, increasing accuracy by 8.74% to 84.85%.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰çŸ¥è¯†è¿½è¸ª(Knowledge Tracing, KT)æ–¹æ³•åœ¨åˆ†æå†å²å­¦ä¹ åºåˆ—ä¸æœªæ¥è¡¨ç°æ—¶ç»å¸¸é‡åˆ°çš„ç›¸å…³æ€§å†²çªé—®é¢˜ï¼Œæå‡ºäº†å‰ç»æ€§çŸ¥è¯†è¿½è¸ªæ¡†æ¶FINER (Forward-Looking Knowledge Tracing)ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°ä»å†å²æ™ºèƒ½æ•™å­¦ç³»ç»Ÿ(ITS)æ•°æ®ä¸­æå–åç»­è¡¨ç°è¶‹åŠ¿(Follow-up Performance Trends, FPTs)ï¼Œå¹¶å°†å…¶æ•´åˆåˆ°KTæ¨¡å‹ä¸­ä»¥æå‡é¢„æµ‹å‡†ç¡®æ€§ã€‚FINERåˆ©ç”¨ç‰¹å®šçš„å­¦ä¹ æ¨¡å¼æ„é€ å®ç°äº†FPTsçš„çº¿æ€§æ—¶é—´æ£€ç´¢ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„ç›¸ä¼¼æ€§æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶(similarity-aware attention mechanism)ï¼Œæ ¹æ®é¢‘ç‡å’Œä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦æœ‰æ•ˆèšåˆFPTsã€‚é€šè¿‡æœ‰æœºç»“åˆFPTsä¸å†å²å­¦ä¹ åºåˆ—ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´ç²¾å‡†åœ°è¯„ä¼°å­¦ç”Ÿçš„çŸ¥è¯†çŠ¶æ€ã€‚åœ¨å…­ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFINERçš„æ€§èƒ½ä¼˜äºåç§æœ€å…ˆè¿›çš„KTåŸºçº¿æ–¹æ³•ï¼Œé¢„æµ‹å‡†ç¡®ç‡æ˜¾è‘—æå‡äº†8.74%è‡³84.85%ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "14 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.08019v2",
      "published_date": "2025-08-11 14:26:11 UTC",
      "updated_date": "2025-09-22 15:48:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:59:37.986402+00:00"
    },
    {
      "arxiv_id": "2508.08007v2",
      "title": "Fitting Description Logic Ontologies to ABox and Query Examples",
      "title_zh": "é¢å‘ ABox å’ŒæŸ¥è¯¢ç¤ºä¾‹çš„æè¿°é€»è¾‘æœ¬ä½“æ‹Ÿåˆ",
      "authors": [
        "Maurice Funk",
        "Marvin Grosser",
        "Carsten Lutz"
      ],
      "abstract": "We study a fitting problem inspired by ontology-mediated querying: given a collection of positive and negative examples of the form $(\\mathcal{A},q)$ with $\\mathcal{A}$ an ABox and $q$ a Boolean query, we seek an ontology $\\mathcal{O}$ that satisfies $\\mathcal{A} \\cup \\mathcal{O} \\vDash q$ for all positive examples and $\\mathcal{A} \\cup \\mathcal{O}\\not\\vDash q$ for all negative examples. We consider the description logics $\\mathcal{ALC}$ and $\\mathcal{ALCI}$ as ontology languages and a range of query languages that includes atomic queries (AQs), conjunctive queries (CQs), and unions thereof (UCQs). For all of the resulting fitting problems, we provide effective characterizations and determine the computational complexity of deciding whether a fitting ontology exists. This problem turns out to be ${\\scriptsize CO}NP$ for AQs and full CQs and $2E{\\scriptsize XP}T{\\scriptsize IME}$-complete for CQs and UCQs. These results hold for both $\\mathcal{ALC}$ and $\\mathcal{ALCI}$.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å—æœ¬ä½“ä»‹å¯¼æŸ¥è¯¢(ontology-mediated querying)å¯å‘çš„æ‹Ÿåˆ(fitting)é—®é¢˜ï¼Œæ—¨åœ¨æ ¹æ®ä¸€ç»„åŒ…å«ABoxå’Œå¸ƒå°”æŸ¥è¯¢(Boolean query)çš„æ­£è´Ÿç¤ºä¾‹ï¼Œæ„å»ºä¸€ä¸ªèƒ½å¤Ÿä½¿æ­£ä¾‹æ»¡è¶³é€»è¾‘è•´å«è€Œè´Ÿä¾‹ä¸æ»¡è¶³çš„æœ¬ä½“$\\mathcal{O}$ã€‚ç ”ç©¶å°†æè¿°é€»è¾‘(description logics) $\\mathcal{ALC}$ å’Œ $\\mathcal{ALCI}$ ä½œä¸ºç›®æ ‡æœ¬ä½“è¯­è¨€ï¼Œå¹¶é’ˆå¯¹åŸå­æŸ¥è¯¢(AQs)ã€åˆå–æŸ¥è¯¢(CQs)åŠå…¶å¹¶é›†(UCQs)ç­‰å¤šç§æŸ¥è¯¢è¯­è¨€è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚ä½œè€…ä¸ºè¿™äº›æ‹Ÿåˆé—®é¢˜æä¾›äº†æœ‰æ•ˆçš„ç‰¹å¾åˆ»ç”»ï¼Œå¹¶ç³»ç»Ÿåœ°ç¡®å®šäº†åˆ¤å®šæ‹Ÿåˆæœ¬ä½“æ˜¯å¦å­˜åœ¨çš„è®¡ç®—å¤æ‚åº¦ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œå¯¹äºAQså’Œfull CQsï¼Œè¯¥æ‹Ÿåˆé—®é¢˜çš„å¤æ‚åº¦ä¸º$coNP$ï¼›è€Œå¯¹äºCQså’ŒUCQsï¼Œå…¶å¤æ‚åº¦åˆ™è¾¾åˆ°$2EXPTIME$-completeã€‚è¿™äº›å…³äºè®¡ç®—å¤æ‚åº¦çš„é‡è¦å‘ç°å¯¹äº $\\mathcal{ALC}$ å’Œ $\\mathcal{ALCI}$ ä¸¤ç§é€»è¾‘æ¡†æ¶å‡æˆç«‹ï¼Œä¸ºæè¿°é€»è¾‘ä¸‹çš„çŸ¥è¯†åº“è‡ªåŠ¨æ„å»ºæä¾›äº†å…³é”®çš„ç†è®ºä¾æ®ã€‚",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "Submitted to the 22nd International Conference on Principles of Knowledge Representation and Reasoning (KR2025), 23 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.08007v2",
      "published_date": "2025-08-11 14:11:27 UTC",
      "updated_date": "2025-08-12 08:20:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:00:02.285917+00:00"
    },
    {
      "arxiv_id": "2508.08005v3",
      "title": "Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel GAT-MLP",
      "title_zh": "MCP ç®—æ³•é€‰æ‹©å­¦ä¹ ï¼šä»ä¼ ç»Ÿæœºå™¨å­¦ä¹ åˆ°åŒé€šé“ GAT-MLP",
      "authors": [
        "Xiang Li",
        "Shanshan Wang",
        "Chenglong Xiao"
      ],
      "abstract": "The Maximum Clique Problem (MCP) is a foundational NP-hard problem with wide-ranging applications, yet no single algorithm consistently outperforms all others across diverse graph instances. This underscores the critical need for instance-aware algorithm selection, a domain that remains largely unexplored for the MCP. To address this gap, we propose a novel learning-based framework that integrates both traditional machine learning and graph neural networks. We first construct a benchmark dataset by executing four state-of-the-art exact MCP solvers on a diverse collection of graphs and extracting their structural features. An evaluation of conventional classifiers establishes Random Forest as a strong baseline and reveals that connectivity and topological features are key predictors of performance. Building on these insights, we develop GAT-MLP, a dual-channel model that combines a Graph Attention Network (GAT) to encode local graph structure with a Multilayer Perceptron (MLP) to model global features. Extensive experiments demonstrate that GAT-MLP achieves superior and consistent performance, significantly outperforming all baseline methods. Our results highlight the effectiveness of the dual-channel architecture and the promise of graph neural networks for combinatorial algorithm selection, achieving 90.43% accuracy in choosing the optimal solver. Code and models are available at: https://anonymous.4open.science/r/GAT-MLP-7E5F.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœ€å¤§å›¢é—®é¢˜ (Maximum Clique Problem, MCP) ä¸­ä¸å­˜åœ¨æ™®é€‚æœ€ä¼˜ç®—æ³•çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªç»“åˆä¼ ç»Ÿæœºå™¨å­¦ä¹ ä¸å›¾ç¥ç»ç½‘ç»œçš„å®ä¾‹æ„ŸçŸ¥ç®—æ³•é€‰æ‹©æ¡†æ¶ã€‚ä½œè€…é¦–å…ˆé€šè¿‡è¿è¡Œå››ç§æœ€å…ˆè¿›çš„ç²¾ç¡®æ±‚è§£å™¨å¹¶æå–ç»“æ„ç‰¹å¾æ„å»ºäº†åŸºå‡†æ•°æ®é›†ï¼Œå‘ç° Random Forest åœ¨ä¼ ç»Ÿåˆ†ç±»å™¨ä¸­è¡¨ç°ç¨³å¥ï¼Œä¸”è¿é€šæ€§å’Œæ‹“æ‰‘ç‰¹å¾æ˜¯é¢„æµ‹æ€§èƒ½çš„å…³é”®æŒ‡æ ‡ã€‚åŸºäºæ­¤æ´å¯Ÿï¼Œç ”ç©¶å¼€å‘äº†åä¸º GAT-MLP çš„åŒé€šé“æ¨¡å‹ï¼Œåˆ©ç”¨å›¾æ³¨æ„åŠ›ç½‘ç»œ (Graph Attention Network, GAT) ç¼–ç å±€éƒ¨å›¾ç»“æ„ï¼Œå¹¶ç»“åˆå¤šå±‚æ„ŸçŸ¥æœº (Multilayer Perceptron, MLP) å»ºæ¨¡å…¨å±€ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGAT-MLP åœ¨é€‰æ‹©æœ€ä¼˜æ±‚è§£å™¨æ–¹é¢è¾¾åˆ°äº† 90.43% çš„å‡†ç¡®ç‡ï¼Œå…¶æ€§èƒ½æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ã€‚è¯¥å·¥ä½œéªŒè¯äº†åŒé€šé“æ¶æ„åœ¨ç»„åˆç®—æ³•é€‰æ‹©ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºåˆ©ç”¨å›¾ç¥ç»ç½‘ç»œå¤„ç† NP-hard é—®é¢˜æä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.08005v3",
      "published_date": "2025-08-11 14:09:58 UTC",
      "updated_date": "2025-12-08 04:03:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:00:19.255645+00:00"
    },
    {
      "arxiv_id": "2508.08001v3",
      "title": "Interpreting Fedspeak with Confidence: A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths",
      "title_zh": "ç½®ä¿¡åº¦è§†è§’ä¸‹çš„ç¾è”å‚¨è¯æœ¯è§£è¯»ï¼šä¸€ç§ç”±è´§å¸æ”¿ç­–ä¼ å¯¼è·¯å¾„å¼•å¯¼çš„åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ¡†æ¶",
      "authors": [
        "Rui Yao",
        "Qi Chai",
        "Jinhai Yao",
        "Siyuan Li",
        "Junhao Chen",
        "Qi Zhang",
        "Hao Wang"
      ],
      "abstract": "\"Fedspeak\", the stylized and often nuanced language used by the U.S. Federal Reserve, encodes implicit policy signals and strategic stances. The Federal Open Market Committee strategically employs Fedspeak as a communication tool to shape market expectations and influence both domestic and global economic conditions. As such, automatically parsing and interpreting Fedspeak presents a high-impact challenge, with significant implications for financial forecasting, algorithmic trading, and data-driven policy analysis. In this paper, we propose an LLM-based, uncertainty-aware framework for deciphering Fedspeak and classifying its underlying monetary policy stance. Technically, to enrich the semantic and contextual representation of Fedspeak texts, we incorporate domain-specific reasoning grounded in the monetary policy transmission mechanism. We further introduce a dynamic uncertainty decoding module to assess the confidence of model predictions, thereby enhancing both classification accuracy and model reliability. Experimental results demonstrate that our framework achieves state-of-the-art performance on the policy stance analysis task. Moreover, statistical analysis reveals a significant positive correlation between perceptual uncertainty and model error rates, validating the effectiveness of perceptual uncertainty as a diagnostic signal.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¾è”å‚¨ä½¿ç”¨çš„å…·æœ‰é«˜åº¦ç­–ç•¥æ€§å’Œç»†å¾®å·®åˆ«çš„ Fedspeak è¯­è¨€ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäº LLM çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨è§£æå’Œåˆ†ç±»å…¶ä¸­çš„è´§å¸æ”¿ç­–ç«‹åœºã€‚ä¸ºäº†ä¸°å¯Œæ–‡æœ¬çš„è¯­ä¹‰å’Œä¸Šä¸‹æ–‡è¡¨å¾ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†åŸºäºè´§å¸æ”¿ç­–ä¼ å¯¼æœºåˆ¶(monetary policy transmission mechanism)çš„é¢†åŸŸç‰¹å®šæ¨ç†ã€‚ç ”ç©¶è¿˜è®¾è®¡äº†åŠ¨æ€ä¸ç¡®å®šæ€§è§£ç æ¨¡å—(dynamic uncertainty decoding module)ä»¥è¯„ä¼°é¢„æµ‹ç½®ä¿¡åº¦ï¼Œä»è€Œå…¼é¡¾åˆ†ç±»å‡†ç¡®æ€§ä¸æ¨¡å‹å¯é æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ”¿ç­–ç«‹åœºåˆ†æä»»åŠ¡ä¸­å–å¾—äº† State-of-the-art çš„è¡¨ç°ã€‚ç»Ÿè®¡åˆ†æè¿›ä¸€æ­¥è¯å®ï¼Œæ„ŸçŸ¥ä¸ç¡®å®šæ€§ä¸æ¨¡å‹é”™è¯¯ç‡ä¹‹é—´å­˜åœ¨æ˜¾è‘—æ­£ç›¸å…³ï¼ŒéªŒè¯äº†è¯¥æŒ‡æ ‡ä½œä¸ºè¯Šæ–­ä¿¡å·çš„æœ‰æ•ˆæ€§ã€‚è¯¥æˆæœä¸ºé‡‘èé¢„æµ‹ã€ç®—æ³•äº¤æ˜“å’Œæ”¿ç­–åˆ†ææä¾›äº†ç²¾å‡†ä¸”å…·å¤‡ä¸ç¡®å®šæ€§åº¦é‡èƒ½åŠ›çš„å·¥å…·ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by AAAI 2026 Oral",
      "pdf_url": "https://arxiv.org/pdf/2508.08001v3",
      "published_date": "2025-08-11 14:04:59 UTC",
      "updated_date": "2026-01-13 12:20:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:00:28.891249+00:00"
    },
    {
      "arxiv_id": "2508.07995v4",
      "title": "DIVER: A Multi-Stage Approach for Reasoning-intensive Information Retrieval",
      "title_zh": "DIVERï¼šä¸€ç§é¢å‘æ¨ç†å¯†é›†å‹ä¿¡æ¯æ£€ç´¢çš„å¤šé˜¶æ®µæ–¹æ³•",
      "authors": [
        "Meixiu Long",
        "Duolin Sun",
        "Dan Yang",
        "Junjie Wang",
        "Yecheng Luo",
        "Yue Shen",
        "Jian Wang",
        "Hualei Zhou",
        "Chunxiao Guo",
        "Peng Wei",
        "Jiahai Wang",
        "Jinjie Gu"
      ],
      "abstract": "Retrieval-augmented generation has achieved strong performance on knowledge-intensive tasks where query-document relevance can be identified through direct lexical or semantic matches. However, many real-world queries involve abstract reasoning, analogical thinking, or multi-step inference, which existing retrievers often struggle to capture. To address this challenge, we present DIVER, a retrieval pipeline designed for reasoning-intensive information retrieval. It consists of four components. The document preprocessing stage enhances readability and preserves content by cleaning noisy texts and segmenting long documents. The query expansion stage leverages large language models to iteratively refine user queries with explicit reasoning and evidence from retrieved documents. The retrieval stage employs a model fine-tuned on synthetic data spanning medical and mathematical domains, along with hard negatives, enabling effective handling of reasoning-intensive queries. Finally, the reranking stage combines pointwise and listwise strategies to produce both fine-grained and globally consistent rankings. On the BRIGHT benchmark, DIVER achieves state-of-the-art nDCG@10 scores of 46.8 overall and 31.9 on original queries, consistently outperforming competitive reasoning-aware models. These results demonstrate the effectiveness of reasoning-aware retrieval strategies in complex real-world tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰æ£€ç´¢æ¨¡å‹åœ¨å¤„ç†æŠ½è±¡æ¨ç†ã€ç±»æ¯”æ€è€ƒæˆ–å¤šæ­¥æ¨å¯¼ç­‰ reasoning-intensive ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œæå‡ºäº† DIVER æ£€ç´¢æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨å¤šé˜¶æ®µå¤„ç†æµç¨‹ï¼Œé¦–å…ˆåœ¨ document preprocessing é˜¶æ®µé€šè¿‡æ¸…æ´—å™ªå£°å’Œæ–‡æ¡£åˆ†å‰²æ¥å¢å¼ºå†…å®¹çš„å¯è¯»æ€§ä¸å®Œæ•´æ€§ã€‚æ¥ç€ï¼Œquery expansion é˜¶æ®µåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ç»“åˆæ£€ç´¢è¯æ®è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œèµ‹äºˆæŸ¥è¯¢æ˜¾å¼çš„æ¨ç†è¿‡ç¨‹ã€‚æ£€ç´¢é˜¶æ®µåˆ™é‡‡ç”¨äº†åœ¨åŒ»ç–—å’Œæ•°å­¦é¢†åŸŸåˆæˆæ•°æ®åŠ hard negatives ä¸Šå¾®è°ƒçš„æ¨¡å‹ï¼Œä»¥æœ‰æ•ˆåº”å¯¹é«˜éš¾åº¦çš„æ¨ç†æŸ¥è¯¢ã€‚æœ€åçš„ reranking é˜¶æ®µèåˆäº† pointwise å’Œ listwise ç­–ç•¥ï¼Œä»è€Œäº§ç”Ÿç»†ç²’åº¦ä¸”å…¨å±€ä¸€è‡´çš„æ’åã€‚å®éªŒè¡¨æ˜ï¼ŒDIVER åœ¨ BRIGHT åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº† state-of-the-art çš„ nDCG@10 åˆ†æ•°ï¼Œè¯æ˜äº† reasoning-aware æ£€ç´¢ç­–ç•¥åœ¨è§£å†³å¤æ‚ç°å®ä»»åŠ¡ä¸­çš„å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07995v4",
      "published_date": "2025-08-11 13:57:49 UTC",
      "updated_date": "2025-11-18 06:49:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:00:25.684606+00:00"
    },
    {
      "arxiv_id": "2508.07981v4",
      "title": "Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation",
      "title_zh": "Omni-Effectsï¼šç»Ÿä¸€ä¸”ç©ºé—´å¯æ§çš„è§†è§‰æ•ˆæœç”Ÿæˆ",
      "authors": [
        "Fangyuan Mao",
        "Aiming Hao",
        "Jintao Chen",
        "Dongxia Liu",
        "Xiaokun Feng",
        "Jiashu Zhu",
        "Meiqi Wu",
        "Chubin Chen",
        "Jiahong Wu",
        "Xiangxiang Chu"
      ],
      "abstract": "Visual effects (VFX) are essential visual enhancements fundamental to modern cinematic production. Although video generation models offer cost-efficient solutions for VFX production, current methods are constrained by per-effect LoRA training, which limits generation to single effects. This fundamental limitation impedes applications that require spatially controllable composite effects, i.e., the concurrent generation of multiple effects at designated locations. However, integrating diverse effects into a unified framework faces major challenges: interference from effect variations and spatial uncontrollability during multi-VFX joint training. To tackle these challenges, we propose Omni-Effects, a first unified framework capable of generating prompt-guided effects and spatially controllable composite effects. The core of our framework comprises two key innovations: (1) LoRA-based Mixture of Experts (LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects within a unified model while effectively mitigating cross-task interference. (2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the text token, enabling precise spatial control. Furthermore, we introduce an Independent-Information Flow (IIF) module integrated within the SAP, isolating the control signals corresponding to individual effects to prevent any unwanted blending. To facilitate this research, we construct a comprehensive VFX dataset Omni-VFX via a novel data collection pipeline combining image editing and First-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX evaluation framework for validating model performance. Extensive experiments demonstrate that Omni-Effects achieves precise spatial control and diverse effect generation, enabling users to specify both the category and location of desired effects.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Omni-Effectsï¼Œè¿™æ˜¯ä¸€ä¸ªèƒ½å¤Ÿç”Ÿæˆæç¤ºè¯å¼•å¯¼ä¸”å…·å¤‡ç©ºé—´å¯æ§æ€§çš„ç»Ÿä¸€è§†è§‰ç‰¹æ•ˆ(VFX)ç”Ÿæˆæ¡†æ¶ï¼Œæœ‰æ•ˆè§£å†³äº†ç°æœ‰æ¨¡å‹å—é™äºå•ä¸€ç‰¹æ•ˆç”Ÿæˆä¸”ç¼ºä¹å¤šç‰¹æ•ˆååŒæ§åˆ¶çš„é—®é¢˜ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬LoRA-based Mixture of Experts (LoRA-MoE)æœºåˆ¶ï¼Œåˆ©ç”¨ä¸“å®¶LoRAç»„åœ¨ç»Ÿä¸€æ¨¡å‹ä¸­æ•´åˆå¤šæ ·ç‰¹æ•ˆå¹¶æ¶ˆé™¤ä»»åŠ¡é—´å¹²æ‰°ã€‚åŒæ—¶ï¼Œç ”ç©¶å¼•å…¥äº†Spatial-Aware Prompt (SAP)ä»¥å®ç°ç²¾ç¡®çš„ç©ºé—´æ§åˆ¶ï¼Œå¹¶é…åˆIndependent-Information Flow (IIF)æ¨¡å—éš”ç¦»å„ç‰¹æ•ˆçš„æ§åˆ¶ä¿¡å·ï¼Œé¿å…äº§ç”Ÿä¸å¿…è¦çš„è§†è§‰èåˆã€‚ä¸ºäº†æ”¯æ’‘ç ”ç©¶ï¼Œå›¢é˜Ÿé€šè¿‡å›¾åƒç¼–è¾‘ä¸First-Last Frame-to-Video (FLF2V)åˆæˆæµç¨‹æ„å»ºäº†å…¨é¢çš„Omni-VFXæ•°æ®é›†åŠè¯„ä¼°ä½“ç³»ã€‚å®éªŒç»“æœéªŒè¯äº†Omni-Effectsåœ¨ç©ºé—´æ§åˆ¶å’Œå¤šæ ·åŒ–ç‰¹æ•ˆç”Ÿæˆæ–¹é¢çš„ä¼˜è¶Šæ€§ï¼Œå…è®¸ç”¨æˆ·è‡ªç”±å®šä¹‰ç‰¹æ•ˆçš„ç±»åˆ«ä¸å…·ä½“ä½ç½®ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to AAAI2026",
      "pdf_url": "https://arxiv.org/pdf/2508.07981v4",
      "published_date": "2025-08-11 13:41:24 UTC",
      "updated_date": "2025-12-17 06:09:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:00:33.377447+00:00"
    },
    {
      "arxiv_id": "2508.07976v4",
      "title": "Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL",
      "title_zh": "çªç ´åè½®ï¼šåˆ©ç”¨å¤§è§„æ¨¡å¼‚æ­¥å¼ºåŒ–å­¦ä¹ è§£é”é•¿ç¨‹æ™ºèƒ½ä½“æœç´¢",
      "authors": [
        "Jiaxuan Gao",
        "Wei Fu",
        "Minyang Xie",
        "Shusheng Xu",
        "Chuyi He",
        "Zhiyu Mei",
        "Banghua Zhu",
        "Yi Wu"
      ],
      "abstract": "Recent advancements in LLM-based agents have demonstrated remarkable capabilities in handling complex, knowledge-intensive tasks by integrating external tools. Among diverse choices of tools, search tools play a pivotal role in accessing vast external knowledge. However, open-source agents still fall short of achieving expert-level Search Intelligence, the ability to resolve ambiguous queries, generate precise searches, analyze results, and conduct thorough exploration. Existing approaches fall short in scalability, efficiency, and data quality. For example, small turn limits in existing online RL methods, e.g. <=10, restrict complex strategy learning. This paper introduces ASearcher, an open-source project for large-scale RL training of search agents. Our key contributions include: (1) Scalable fully asynchronous RL training that enables long-horizon search while maintaining high training efficiency. (2) A prompt-based LLM agent that autonomously synthesizes high-quality and challenging QAs, creating a large-scale QA dataset. Through RL training, our prompt-based QwQ-32B agent achieves substantial improvements, with 78.0% and 34.3% Avg@4 gains on xBench and GAIA, respectively. Notably, our agent exhibits extreme long-horizon search, with tool calls exceeding 100 turns and output tokens exceeding 400k during training time. With a simple agent design and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 51.1 on xBench and 58.7 on GAIA, surpassing existing open-source 32B agents. Finally, we also show that ASearcher-Web-QwQ could achieve performance of commercial systems using external summary tool in a zero-shot transfer manner and test-time search. We open-source our models, training data, and codes in https://github.com/inclusionAI/ASearcher.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰å¼€æºå¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“åœ¨å¤„ç†å¤æ‚æœç´¢ä»»åŠ¡æ—¶é¢ä¸´çš„æœç´¢æ™ºèƒ½(Search Intelligence)ä¸è¶³ã€åœ¨çº¿å¼ºåŒ–å­¦ä¹ (RL)å›åˆé™åˆ¶ï¼ˆé€šå¸¸â‰¤10è½®ï¼‰ä»¥åŠå¯æ‰©å±•æ€§å·®ç­‰é—®é¢˜ï¼Œæå‡ºäº† ASearcher å¼€æºé¡¹ç›®ã€‚è¯¥é¡¹ç›®å¼•å…¥äº†å¯æ‰©å±•çš„å…¨å¼‚æ­¥å¼ºåŒ–å­¦ä¹ (Fully Asynchronous RL)è®­ç»ƒæœºåˆ¶ï¼Œæœ‰æ•ˆæ”¯æŒäº†é•¿ç¨‹æœç´¢(Long-Horizon Search)å¹¶ä¿æŒäº†æé«˜çš„è®­ç»ƒæ•ˆç‡ã€‚åŒæ—¶ï¼Œç ”ç©¶åˆ©ç”¨åŸºäºæç¤ºè¯çš„æ™ºèƒ½ä½“è‡ªä¸»åˆæˆé«˜è´¨é‡é—®ç­”å¯¹ï¼Œæ„å»ºäº†å¤§è§„æ¨¡ QA æ•°æ®é›†ä»¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼Œç»è¿‡è®­ç»ƒçš„ ASearcher-Web-QwQ æ™ºèƒ½ä½“åœ¨é•¿ç¨‹ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œå…¶å·¥å…·è°ƒç”¨å¯è¶…è¿‡ 100 è½®ï¼Œåœ¨ xBench å’Œ GAIA åŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«å–å¾—äº† 78.0% å’Œ 34.3% çš„ Avg@4 æ€§èƒ½æå‡ã€‚è¯¥ç³»ç»Ÿåœ¨æ— éœ€å¤–éƒ¨ LLM è¾…åŠ©çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½è¶…è¶Šäº†ç°æœ‰å¼€æº 32B æ™ºèƒ½ä½“ï¼Œå¹¶èƒ½åœ¨é›¶æ ·æœ¬è¿ç§»(Zero-Shot Transfer)åœºæ™¯ä¸‹è¾¾åˆ°å•†ä¸šç³»ç»Ÿçš„æ°´å¹³ï¼Œä¸ºå®ç°ä¸“å®¶çº§æœç´¢æ™ºèƒ½æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07976v4",
      "published_date": "2025-08-11 13:36:57 UTC",
      "updated_date": "2025-10-26 07:06:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:00:35.850069+00:00"
    },
    {
      "arxiv_id": "2508.07970v3",
      "title": "WeChat-YATT: A Scalable, Simple, Efficient, and Production Ready Training Library",
      "title_zh": "WeChat-YATTï¼šä¸€ç§å¯æ‰©å±•ã€ç®€æ´ã€é«˜æ•ˆä¸”ç”Ÿäº§å°±ç»ªçš„è®­ç»ƒåº“",
      "authors": [
        "Junyu Wu",
        "Weiming Chang",
        "Xiaotao Liu",
        "Guanyou He",
        "Tingfeng Xian",
        "Haoqiang Hong",
        "Boqi Chen",
        "Hongtao Tian",
        "Tao Yang",
        "Yunsheng Shi",
        "Feng Lin",
        "Ting Yao",
        "Jiatao Xu"
      ],
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a prominent paradigm for training large language models and multimodal systems. Despite the notable advances enabled by existing RLHF training frameworks, significant challenges remain to scale to complex multimodal workflows and adapt to dynamic workloads. In particular, current systems often encounter limitations related to controller scalability when managing large models, as well as inefficiencies in orchestrating intricate RLHF pipelines, especially in scenarios that require dynamic sampling and resource allocation. In this paper, we introduce WeChat-YATT Yet Another Transformer Trainer in WeChat, a simple, scalable, and balanced RLHF training framework specifically designed to address these challenges. WeChat-YATT features a parallel controller programming model that enables flexible and efficient orchestration of complex RLHF workflows, effectively mitigating bottlenecks associated with centralized controller architectures and facilitating scalability in large-scale data scenarios. In addition, we propose a dynamic placement schema that adaptively partitions computational resources and schedules workloads, thereby significantly reducing hardware idle time and improving GPU utilization under variable training conditions. We evaluate WeChat-YATT across diverse experimental scenarios, demonstrating its substantial throughput improvements over state-of-the-art RLHF training frameworks. Furthermore, WeChat-YATT has been successfully deployed to train models that support WeChat product features for a large-scale user base, underscoring its effectiveness and robustness in real-world applications. We have made WeChat-YATT publicly available at https://www.github.com/tencent/WeChat-YATT.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† WeChat-YATTï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³ Reinforcement Learning from Human Feedback (RLHF) åœ¨å¤šæ¨¡æ€å·¥ä½œæµå’ŒåŠ¨æ€è´Ÿè½½ä¸‹æ‰©å±•æ€§éš¾é¢˜çš„è®­ç»ƒæ¡†æ¶ã€‚WeChat-YATT é‡‡ç”¨äº†å¹¶è¡Œæ§åˆ¶å™¨ç¼–ç¨‹æ¨¡å‹ (parallel controller programming model)ï¼Œé€šè¿‡å»ä¸­å¿ƒåŒ–çš„å·¥ä½œæµç¼–æ’æœ‰æ•ˆç¼“è§£äº†ä¼ ç»Ÿæ¶æ„çš„æ€§èƒ½ç“¶é¢ˆï¼Œæå‡äº†å¤§è§„æ¨¡æ•°æ®åœºæ™¯ä¸‹çš„æ‰©å±•èƒ½åŠ›ã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†åŠ¨æ€æ”¾ç½®æ–¹æ¡ˆ (dynamic placement schema)ï¼Œé€šè¿‡è‡ªé€‚åº”èµ„æºåˆ’åˆ†å’Œè´Ÿè½½è°ƒåº¦æ˜¾è‘—æå‡äº† GPU åˆ©ç”¨ç‡å¹¶é™ä½äº†ç¡¬ä»¶ç©ºé—²ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒWeChat-YATT çš„ååé‡ (throughput) æ˜æ˜¾ä¼˜äºç›®å‰æœ€å…ˆè¿›çš„ RLHF è®­ç»ƒæ¡†æ¶ã€‚ç›®å‰ï¼Œè¯¥ç³»ç»Ÿå·²åœ¨å¾®ä¿¡ (WeChat) äº§å“çš„ç”Ÿäº§ç¯å¢ƒä¸­æˆåŠŸéƒ¨ç½²ï¼Œä¸ºå¤§è§„æ¨¡ç”¨æˆ·åŸºæ•°æä¾›æ”¯æŒï¼Œå±•ç°äº†å…¶åœ¨çœŸå®åº”ç”¨åœºæ™¯ä¸­çš„ç¨³å¥æ€§å’Œé«˜æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2507.22789",
      "pdf_url": "https://arxiv.org/pdf/2508.07970v3",
      "published_date": "2025-08-11 13:31:53 UTC",
      "updated_date": "2025-08-18 03:48:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:00:34.180397+00:00"
    },
    {
      "arxiv_id": "2508.07966v1",
      "title": "Exploring the Challenges and Opportunities of AI-assisted Codebase Generation",
      "title_zh": "æ¢ç´¢ AI è¾…åŠ©ä»£ç åº“ç”Ÿæˆçš„æŒ‘æˆ˜ä¸æœºé‡",
      "authors": [
        "Philipp Eibl",
        "Sadra Sabouri",
        "Souti Chattopadhyay"
      ],
      "abstract": "Recent AI code assistants have significantly improved their ability to process more complex contexts and generate entire codebases based on a textual description, compared to the popular snippet-level generation. These codebase AI assistants (CBAs) can also extend or adapt codebases, allowing users to focus on higher-level design and deployment decisions. While prior work has extensively studied the impact of snippet-level code generation, this new class of codebase generation models is relatively unexplored. Despite initial anecdotal reports of excitement about these agents, they remain less frequently adopted compared to snippet-level code assistants. To utilize CBAs better, we need to understand how developers interact with CBAs, and how and why CBAs fall short of developers' needs. In this paper, we explored these gaps through a counterbalanced user study and interview with (n = 16) students and developers working on coding tasks with CBAs. We found that participants varied the information in their prompts, like problem description (48% of prompts), required functionality (98% of prompts), code structure (48% of prompts), and their prompt writing process. Despite various strategies, the overall satisfaction score with generated codebases remained low (mean = 2.8, median = 3, on a scale of one to five). Participants mentioned functionality as the most common factor for dissatisfaction (77% of instances), alongside poor code quality (42% of instances) and communication issues (25% of instances). We delve deeper into participants' dissatisfaction to identify six underlying challenges that participants faced when using CBAs, and extracted five barriers to incorporating CBAs into their workflows. Finally, we surveyed 21 commercial CBAs to compare their capabilities with participant challenges and present design opportunities for more efficient and useful CBAs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†AIè¾…åŠ©ä»£ç åº“ç”Ÿæˆï¼ˆAI-assisted Codebase Generationï¼‰æ‰€é¢ä¸´çš„æŒ‘æˆ˜ä¸æœºé‡ã€‚ä¸ä¼ ç»Ÿçš„ç‰‡æ®µçº§ï¼ˆsnippet-levelï¼‰ç”Ÿæˆç›¸æ¯”ï¼Œä»£ç åº“AIåŠ©æ‰‹ï¼ˆCBAsï¼‰èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆæ•´ä¸ªä»£ç åº“ï¼Œä½†å…¶ç›®å‰åœ¨å¼€å‘è€…ä¸­çš„å®é™…é‡‡ç”¨ç‡ä¾ç„¶è¾ƒä½ã€‚ç ”ç©¶äººå‘˜é€šè¿‡å¯¹16åå¼€å‘è€…å’Œå­¦ç”Ÿè¿›è¡Œå¯¹ç…§ç”¨æˆ·ç ”ç©¶ä¸è®¿è°ˆï¼Œæ·±å…¥åˆ†æäº†ç”¨æˆ·ä¸CBAsçš„äº¤äº’æ¨¡å¼ä»¥åŠè¿™äº›å·¥å…·æœªèƒ½æ»¡è¶³éœ€æ±‚çš„åŸå› ã€‚è°ƒæŸ¥å‘ç°ï¼Œå°½ç®¡ç”¨æˆ·åœ¨æç¤ºè¯ï¼ˆpromptsï¼‰ä¸­æä¾›äº†é—®é¢˜æè¿°ã€åŠŸèƒ½éœ€æ±‚å’Œä»£ç ç»“æ„ç­‰å¤šç§ä¿¡æ¯ï¼Œä½†ç”Ÿæˆçš„ä»£ç åº“æ•´ä½“æ»¡æ„åº¦å¾—åˆ†è¾ƒä½ã€‚ç”¨æˆ·ä¸»è¦å¯¹CBAsçš„åŠŸèƒ½å®ç°ã€ä»£ç è´¨é‡ä»¥åŠæ²Ÿé€šæ•ˆç‡è¡¨ç¤ºä¸æ»¡ï¼Œç ”ç©¶æ®æ­¤è¯†åˆ«å‡º6é¡¹æ ¸å¿ƒæŒ‘æˆ˜å’Œ5é¡¹å·¥ä½œæµæ•´åˆéšœç¢ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜è°ƒç ”äº†21æ¬¾å•†ç”¨CBAsä»¥å¯¹æ¯”å…¶ç°æœ‰èƒ½åŠ›ï¼Œå¹¶æå‡ºäº†æœªæ¥çš„è®¾è®¡æœºé‡ï¼Œæ—¨åœ¨ä¸ºå¼€å‘æ›´é«˜æ•ˆã€å®ç”¨çš„ä»£ç åº“ç”Ÿæˆå·¥å…·æä¾›å‚è€ƒã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07966v1",
      "published_date": "2025-08-11 13:26:48 UTC",
      "updated_date": "2025-08-11 13:26:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:00:41.559865+00:00"
    },
    {
      "arxiv_id": "2508.07950v1",
      "title": "FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis",
      "title_zh": "FEATï¼šåŸºäºé¢†åŸŸé€‚é…å¤§è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨åŒ–æ­»å› åˆ†æå¤šæ™ºèƒ½ä½“æ³•åŒ»å­¦äººå·¥æ™ºèƒ½ç³»ç»Ÿ",
      "authors": [
        "Chen Shen",
        "Wanqing Zhang",
        "Kehan Li",
        "Erwen Huang",
        "Haitao Bi",
        "Aiying Fan",
        "Yiwen Shen",
        "Hongmei Dong",
        "Ji Zhang",
        "Yuming Shao",
        "Zengjia Liu",
        "Xinshe Liu",
        "Tao Li",
        "Chunxia Yan",
        "Shuanliang Fan",
        "Di Wu",
        "Jianhua Ma",
        "Bin Cong",
        "Zhenyuan Wang",
        "Chunfeng Lian"
      ],
      "abstract": "Forensic cause-of-death determination faces systemic challenges, including workforce shortages and diagnostic variability, particularly in high-volume systems like China's medicolegal infrastructure. We introduce FEAT (ForEnsic AgenT), a multi-agent AI framework that automates and standardizes death investigations through a domain-adapted large language model. FEAT's application-oriented architecture integrates: (i) a central Planner for task decomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a Memory & Reflection module for iterative refinement, and (iv) a Global Solver for conclusion synthesis. The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity. In evaluations across diverse Chinese case cohorts, FEAT outperformed state-of-the-art AI systems in both long-form autopsy analyses and concise cause-of-death conclusions. It demonstrated robust generalization across six geographic regions and achieved high expert concordance in blinded validations. Senior pathologists validated FEAT's outputs as comparable to those of human experts, with improved detection of subtle evidentiary nuances. To our knowledge, FEAT is the first LLM-based AI agent system dedicated to forensic medicine, offering scalable, consistent death certification while maintaining expert-level rigor. By integrating AI efficiency with human oversight, this work could advance equitable access to reliable medicolegal services while addressing critical capacity constraints in forensic systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FEAT (ForEnsic AgenT)ï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨ç”¨äºæ³•åŒ»å­¦æ­»å› è‡ªåŠ¨åˆ†æçš„åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„å¤šæ™ºèƒ½ä½“(Multi-agent)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ³•åŒ»èµ„æºçŸ­ç¼ºåŠè¯Šæ–­ä¸€è‡´æ€§é—®é¢˜ã€‚FEAT é‡‡ç”¨é¢†åŸŸé€‚åº” (Domain-adapted) çš„æ¶æ„ï¼Œæ•´åˆäº†è´Ÿè´£ä»»åŠ¡åˆ†è§£çš„ Central Plannerã€è¿›è¡Œè¯æ®åˆ†æçš„ Local Solversã€ç”¨äºè¿­ä»£ä¼˜åŒ–çš„ Memory & Reflection æ¨¡å—ä»¥åŠè´Ÿè´£ç»“è®ºåˆæˆçš„ Global Solverã€‚ç³»ç»Ÿé€šè¿‡ç»“åˆå·¥å…·å¢å¼ºæ¨ç† (Tool-augmented reasoning) å’Œå±‚æ¬¡åŒ–æ£€ç´¢å¢å¼ºç”Ÿæˆ (Hierarchical RAG) æŠ€æœ¯ï¼Œå¹¶å¼•å…¥äººæœºåä½œåé¦ˆæœºåˆ¶ä»¥ç¡®ä¿åŒ»å­¦ä¸æ³•å¾‹çš„ä¸¥è°¨æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFEAT åœ¨ä¸­å›½å¤šåœ°ç—…ä¾‹åº“çš„è¯„ä¼°ä¸­è¡¨ç°ä¼˜äºç°æœ‰å…ˆè¿› AI ç³»ç»Ÿï¼Œåœ¨å°¸æ£€åˆ†æä¸æ­»å› æ¨æ–­ä¸Šå±•ç°å‡ºæé«˜çš„ä¸“å®¶ä¸€è‡´æ€§ (Expert concordance)ã€‚èµ„æ·±ç—…ç†å­¦å®¶éªŒè¯å…¶è¾“å‡ºè´¨é‡å·²è¾¾åˆ°äººç±»ä¸“å®¶æ°´å¹³ï¼Œæœ‰æ•ˆæ•æ‰äº†ç»†å¾®çš„è¯æ®çº¿ç´¢ï¼Œä¸ºå®ç°æ ‡å‡†åŒ–ã€å¯æ‰©å±•çš„æ³•åŒ»æ­»å› é‰´å®šæä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "18pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.07950v1",
      "published_date": "2025-08-11 13:05:59 UTC",
      "updated_date": "2025-08-11 13:05:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:00:50.366522+00:00"
    },
    {
      "arxiv_id": "2508.07944v1",
      "title": "SCDF: A Speaker Characteristics DeepFake Speech Dataset for Bias Analysis",
      "title_zh": "SCDFï¼šç”¨äºåè§åˆ†æçš„è¯´è¯äººç‰¹å¾æ·±åº¦ä¼ªé€ è¯­éŸ³æ•°æ®é›†",
      "authors": [
        "VojtÄ›ch StanÄ›k",
        "Karel Srna",
        "Anton Firc",
        "Kamil Malinka"
      ],
      "abstract": "Despite growing attention to deepfake speech detection, the aspects of bias and fairness remain underexplored in the speech domain. To address this gap, we introduce the Speaker Characteristics Deepfake (SCDF) dataset: a novel, richly annotated resource enabling systematic evaluation of demographic biases in deepfake speech detection. SCDF contains over 237,000 utterances in a balanced representation of both male and female speakers spanning five languages and a wide age range. We evaluate several state-of-the-art detectors and show that speaker characteristics significantly influence detection performance, revealing disparities across sex, language, age, and synthesizer type. These findings highlight the need for bias-aware development and provide a foundation for building non-discriminatory deepfake detection systems aligned with ethical and regulatory standards.",
      "tldr_zh": "é’ˆå¯¹ Deepfake è¯­éŸ³æ£€æµ‹é¢†åŸŸä¸­åè§ä¸å…¬å¹³æ€§ç ”ç©¶ä¸è¶³çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº† Speaker Characteristics Deepfake (SCDF) æ•°æ®é›†ï¼Œç”¨äºç³»ç»Ÿæ€§è¯„ä¼°äººå£ç»Ÿè®¡å­¦åè§ã€‚SCDF åŒ…å«è¶…è¿‡ 237,000 æ¡è¯­éŸ³æ ·æœ¬ï¼Œæ¶µç›–äº†äº”ç§è¯­è¨€åŠå¹¿æ³›çš„å¹´é¾„èŒƒå›´ï¼Œå¹¶åœ¨ç”·å¥³è¯´è¯äººåˆ†å¸ƒä¸Šè¾¾åˆ°äº†å¹³è¡¡ã€‚ç ”ç©¶é€šè¿‡å¯¹å¤šç§ state-of-the-art æ£€æµ‹å™¨è¿›è¡Œè¯„ä¼°ï¼Œå‘ç°è¯´è¯äººç‰¹å¾ï¼ˆåŒ…æ‹¬æ€§åˆ«ã€è¯­è¨€ã€å¹´é¾„ä»¥åŠ synthesizer ç±»å‹ï¼‰ä¼šæ˜¾è‘—å½±å“æ£€æµ‹æ€§èƒ½ï¼Œæ­ç¤ºäº†ç°æœ‰æŠ€æœ¯åœ¨ä¸åŒäººå£ç»Ÿè®¡ç»´åº¦ä¸‹çš„ä¸å¹³è¡¡æ€§ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å¼€å‘åè§æ„ŸçŸ¥ (bias-aware) æŠ€æœ¯çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæ„å»ºç¬¦åˆä¼¦ç†å’Œç›‘ç®¡è¦æ±‚çš„éæ­§è§†æ€§ Deepfake æ£€æµ‹ç³»ç»Ÿæä¾›äº†åŸºç¡€èµ„æºã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07944v1",
      "published_date": "2025-08-11 12:58:37 UTC",
      "updated_date": "2025-08-11 12:58:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:00:48.086725+00:00"
    },
    {
      "arxiv_id": "2508.07941v1",
      "title": "Deep Reinforcement Learning with anticipatory reward in LSTM for Collision Avoidance of Mobile Robots",
      "title_zh": "åŸºäºLSTMé¢„è§æ€§å¥–åŠ±çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ç§»åŠ¨æœºå™¨äººé¿éšœç ”ç©¶",
      "authors": [
        "Olivier Poulet",
        "FrÃ©dÃ©ric Guinand",
        "FranÃ§ois GuÃ©rin"
      ],
      "abstract": "This article proposes a collision risk anticipation method based on short-term prediction of the agents position. A Long Short-Term Memory (LSTM) model, trained on past trajectories, is used to estimate the next position of each robot. This prediction allows us to define an anticipated collision risk by dynamically modulating the reward of a Deep Q-Learning Network (DQN) agent. The approach is tested in a constrained environment, where two robots move without communication or identifiers. Despite a limited sampling frequency (1 Hz), the results show a significant decrease of the collisions number and a stability improvement. The proposed method, which is computationally inexpensive, appears particularly attractive for implementation on embedded systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ™ºèƒ½ä½“ä½ç½®çŸ­æœŸé¢„æµ‹çš„ç¢°æ’é£é™©é¢„è§æ–¹æ³•ï¼Œç”¨äºå®ç°ç§»åŠ¨æœºå™¨äººçš„é¿éšœã€‚ç ”ç©¶é‡‡ç”¨åœ¨å†å²è½¨è¿¹ä¸Šè®­ç»ƒçš„ Long Short-Term Memory (LSTM) æ¨¡å‹æ¥é¢„æµ‹æœºå™¨äººçš„ä¸‹ä¸€ä½ç½®ï¼Œå¹¶æ®æ­¤åŠ¨æ€è°ƒèŠ‚ Deep Q-Learning Network (DQN) æ™ºèƒ½ä½“çš„å¥–åŠ±å‡½æ•°ï¼Œä»è€Œå®šä¹‰å‰ç»æ€§çš„ç¢°æ’é£é™©ã€‚å®éªŒåœ¨åŒ…å«ä¸¤ä¸ªæ— é€šä¿¡ã€æ— æ ‡è¯†æœºå™¨äººçš„å—é™ç¯å¢ƒä¸­è¿›è¡Œï¼Œç»“æœè¡¨æ˜å³ä¾¿åœ¨ 1 Hz çš„ä½é‡‡æ ·é¢‘ç‡ä¸‹ï¼Œè¯¥æ–¹æ³•ä¹Ÿèƒ½æ˜¾è‘—é™ä½ç¢°æ’æ¬¡æ•°å¹¶æå‡ç³»ç»Ÿç¨³å®šæ€§ã€‚ç”±äºè¯¥æ–¹æ³•è®¡ç®—å¼€é”€æä½ï¼Œå› æ­¤åœ¨åµŒå…¥å¼ç³»ç»Ÿçš„å®é™…éƒ¨ç½²ä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07941v1",
      "published_date": "2025-08-11 12:55:51 UTC",
      "updated_date": "2025-08-11 12:55:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:00:54.253882+00:00"
    },
    {
      "arxiv_id": "2508.07932v1",
      "title": "\\(X\\)-evolve: Solution space evolution powered by large language models",
      "title_zh": "\\(X\\)-evolveï¼šå¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„è§£ç©ºé—´æ¼”åŒ–",
      "authors": [
        "Yi Zhai",
        "Zhiqiang Wei",
        "Ruohan Li",
        "Keyu Pan",
        "Shuo Liu",
        "Lu Zhang",
        "Jianmin Ji",
        "Wuyang Zhang",
        "Yu Zhang",
        "Yanyong Zhang"
      ],
      "abstract": "While combining large language models (LLMs) with evolutionary algorithms (EAs) shows promise for solving complex optimization problems, current approaches typically evolve individual solutions, often incurring high LLM call costs. We introduce \\(X\\)-evolve, a paradigm-shifting method that instead evolves solution spaces \\(X\\) (sets of individual solutions) - subsets of the overall search space \\(S\\). In \\(X\\)-evolve, LLMs generate tunable programs wherein certain code snippets, designated as parameters, define a tunable solution space. A score-based search algorithm then efficiently explores this parametrically defined space, guided by feedback from objective function scores. This strategy enables broader and more efficient exploration, which can potentially accelerate convergence at a much lower search cost, requiring up to two orders of magnitude fewer LLM calls than prior leading methods. We demonstrate \\(X\\)-evolve's efficacy across three distinct hard optimization problems. For the cap set problem, we discover a larger partial admissible set, establishing a new tighter asymptotic lower bound for the cap set constant (\\(C \\ge 2.2203\\)). In information theory, we uncover a larger independent set for the 15-vertex cycle graph (\\(\\mathcal{C}_{15}^{\\boxtimes 5}\\), size 19,946), thereby raising the known lower bound on its Shannon capacity. Furthermore, for the NP-hard online bin packing problem, we generate heuristics that consistently outperform standard strategies across established benchmarks. By evolving solution spaces, our method considerably improves search effectiveness, making it possible to tackle high-dimensional problems that were previously computationally prohibitive.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† X-evolveï¼Œè¿™æ˜¯ä¸€ç§ç”±å¤§è¯­è¨€æ¨¡å‹ (LLMs) é©±åŠ¨çš„è§£ç©ºé—´æ¼”åŒ–èŒƒå¼ï¼Œæ—¨åœ¨è§£å†³å¤æ‚çš„ä¼˜åŒ–é—®é¢˜ã€‚ä¸ä»¥å¾€æ¼”åŒ–å•ä¸ªè§£çš„æ–¹æ³•ä¸åŒï¼ŒX-evolve ä¸“æ³¨äºæ¼”åŒ–è§£ç©ºé—´ (solution spaces)ï¼Œå³æ•´ä½“æœç´¢ç©ºé—´çš„å­é›†ã€‚é€šè¿‡è®© LLMs ç”ŸæˆåŒ…å«å¯è°ƒå‚æ•°çš„ä»£ç ç¨‹åºï¼Œå¹¶é…åˆåŸºäºè¯„åˆ†çš„æœç´¢ç®—æ³• (score-based search algorithm) è¿›è¡Œæ¢ç´¢ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä»¥æ›´ä½çš„æˆæœ¬å®ç°æ›´å¹¿æ³›çš„æœç´¢ï¼Œå…¶ LLM è°ƒç”¨æ¬¡æ•°æ¯”ç°æœ‰æ–¹æ³•å‡å°‘äº†å¤šè¾¾ä¸¤ä¸ªæ•°é‡çº§ã€‚åœ¨ cap set problem çš„ç ”ç©¶ä¸­ï¼Œè¯¥æ–¹æ³•å‘ç°äº†æ›´å¤§çš„éƒ¨åˆ†å®¹è®¸é›†ï¼ŒæˆåŠŸå°†å¸¸æ•°ä¸‹ç•Œæå‡è‡³ C â‰¥ 2.2203ã€‚åœ¨ä¿¡æ¯è®ºé¢†åŸŸï¼Œå®ƒå‘ç°äº† 15 é¡¶ç‚¹å¾ªç¯å›¾çš„æ›´å¤§ç‹¬ç«‹é›†ï¼Œä»è€Œæé«˜äº† Shannon capacity çš„å·²çŸ¥ä¸‹ç•Œã€‚æ­¤å¤–ï¼Œé’ˆå¯¹ NP-hard çš„ online bin packing é—®é¢˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„å¯å‘å¼ç®—æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºæ ‡å‡†ç­–ç•¥ã€‚X-evolve é€šè¿‡æ¼”åŒ–è§£ç©ºé—´æ˜¾è‘—æå‡äº†æœç´¢æ•ˆèƒ½ï¼Œä½¿å¾—è§£å†³æ­¤å‰å› è®¡ç®—å—é™è€Œéš¾ä»¥å¤„ç†çš„é«˜ç»´é—®é¢˜æˆä¸ºå¯èƒ½ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07932v1",
      "published_date": "2025-08-11 12:47:59 UTC",
      "updated_date": "2025-08-11 12:47:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:01:20.032830+00:00"
    },
    {
      "arxiv_id": "2508.07903v2",
      "title": "Diffusing the Blind Spot: Uterine MRI Synthesis with Diffusion Models",
      "title_zh": "æ¶ˆé™¤ç›²ç‚¹ï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„å­å®« MRI åˆæˆ",
      "authors": [
        "Johanna P. MÃ¼ller",
        "Anika Knupfer",
        "Pedro BlÃ¶ss",
        "Edoardo Berardi Vittur",
        "Bernhard Kainz",
        "Jana Hutter"
      ],
      "abstract": "Despite significant progress in generative modelling, existing diffusion models often struggle to produce anatomically precise female pelvic images, limiting their application in gynaecological imaging, where data scarcity and patient privacy concerns are critical. To overcome these barriers, we introduce a novel diffusion-based framework for uterine MRI synthesis, integrating both unconditional and conditioned Denoising Diffusion Probabilistic Models (DDPMs) and Latent Diffusion Models (LDMs) in 2D and 3D. Our approach generates anatomically coherent, high fidelity synthetic images that closely mimic real scans and provide valuable resources for training robust diagnostic models. We evaluate generative quality using advanced perceptual and distributional metrics, benchmarking against standard reconstruction methods, and demonstrate substantial gains in diagnostic accuracy on a key classification task. A blinded expert evaluation further validates the clinical realism of our synthetic images. We release our models with privacy safeguards and a comprehensive synthetic uterine MRI dataset to support reproducible research and advance equitable AI in gynaecology.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆç²¾ç¡®å¥³æ€§ç›†è…”å›¾åƒæ–¹é¢çš„å±€é™æ€§ï¼Œä»¥åŠå¦‡ç§‘å½±åƒä¸­æ•°æ®ç¨€ç¼ºå’Œéšç§ä¿æŠ¤çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªç”¨äºå­å®« MRI åˆæˆçš„åˆ›æ–°æ‰©æ•£æ¨¡å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶é›†æˆäº† 2D å’Œ 3D ç»´åº¦çš„æ— æ¡ä»¶åŠæœ‰æ¡ä»¶å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ (DDPMs) å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ (LDMs)ï¼Œæ—¨åœ¨ç”Ÿæˆè§£å‰–ç»“æ„è¿è´¯ä¸”é«˜ä¿çœŸåº¦çš„åˆæˆå›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„å›¾åƒèƒ½å¤Ÿç´§å¯†æ¨¡æ‹ŸçœŸå®æ‰«æï¼Œå¹¶åœ¨å…³é”®åˆ†ç±»ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†è¯Šæ–­å‡†ç¡®æ€§ã€‚é€šè¿‡é«˜çº§æ„ŸçŸ¥æŒ‡æ ‡è¯„ä¼°ä»¥åŠåŒç›²ä¸“å®¶ä¸´åºŠéªŒè¯ï¼Œè¯¥æ¡†æ¶å±•ç°äº†æé«˜çš„ä¸´åºŠçœŸå®æ„Ÿã€‚æœ€åï¼Œç ”ç©¶å›¢é˜Ÿå‘å¸ƒäº†å…·å¤‡éšç§ä¿æŠ¤æªæ–½çš„æ¨¡å‹åŠç»¼åˆå­å®« MRI åˆæˆæ•°æ®é›†ï¼Œä¸ºä¿ƒè¿›å¦‡ç§‘é¢†åŸŸçš„å¯é‡å¤ç ”ç©¶å’Œå…¬å¹³ AI å‘å±•å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Accepted at MICCAI CAPI 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.07903v2",
      "published_date": "2025-08-11 12:18:23 UTC",
      "updated_date": "2025-08-25 06:43:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:01:24.191863+00:00"
    },
    {
      "arxiv_id": "2508.07897v1",
      "title": "NeeCo: Image Synthesis of Novel Instrument States Based on Dynamic and Deformable 3D Gaussian Reconstruction",
      "title_zh": "NeeCoï¼šåŸºäºåŠ¨æ€å¯å˜å½¢ä¸‰ç»´é«˜æ–¯é‡å»ºçš„æ–°å‹æ‰‹æœ¯å™¨æ¢°çŠ¶æ€å›¾åƒåˆæˆ",
      "authors": [
        "Tianle Zeng",
        "Junlei Hu",
        "Gerardo Loza Galindo",
        "Sharib Ali",
        "Duygu Sarikaya",
        "Pietro Valdastri",
        "Dominic Jones"
      ],
      "abstract": "Computer vision-based technologies significantly enhance surgical automation by advancing tool tracking, detection, and localization. However, Current data-driven approaches are data-voracious, requiring large, high-quality labeled image datasets, which limits their application in surgical data science. Our Work introduces a novel dynamic Gaussian Splatting technique to address the data scarcity in surgical image datasets. We propose a dynamic Gaussian model to represent dynamic surgical scenes, enabling the rendering of surgical instruments from unseen viewpoints and deformations with real tissue backgrounds. We utilize a dynamic training adjustment strategy to address challenges posed by poorly calibrated camera poses from real-world scenarios. Additionally, we propose a method based on dynamic Gaussians for automatically generating annotations for our synthetic data. For evaluation, we constructed a new dataset featuring seven scenes with 14,000 frames of tool and camera motion and tool jaw articulation, with a background of an ex-vivo porcine model. Using this dataset, we synthetically replicate the scene deformation from the ground truth data, allowing direct comparisons of synthetic image quality. Experimental results illustrate that our method generates photo-realistic labeled image datasets with the highest values in Peak-Signal-to-Noise Ratio (29.87). We further evaluate the performance of medical-specific neural networks trained on real and synthetic images using an unseen real-world image dataset. Our results show that the performance of models trained on synthetic images generated by the proposed method outperforms those trained with state-of-the-art standard data augmentation by 10%, leading to an overall improvement in model performances by nearly 15%.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†NeeCoï¼Œä¸€ç§åŸºäºåŠ¨æ€å’Œå¯å˜å½¢ 3D Gaussian Reconstruction çš„å›¾åƒåˆæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ‰‹æœ¯è‡ªåŠ¨åŒ–é¢†åŸŸä¸­é«˜è´¨é‡æ ‡æ³¨æ•°æ®åŒ®ä¹çš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ dynamic Gaussian Splatting æŠ€æœ¯æ¥è¡¨å¾åŠ¨æ€æ‰‹æœ¯åœºæ™¯ï¼Œå®ç°äº†åœ¨çœŸå®ç»„ç»‡èƒŒæ™¯ä¸‹ä»æ–°è§†è§’æ¸²æŸ“æ‰‹æœ¯å™¨æ¢°åŠå…¶å¤æ‚å˜å½¢ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜å¼•å…¥äº†åŠ¨æ€è®­ç»ƒè°ƒæ•´ç­–ç•¥ä»¥è§£å†³ç›¸æœºå§¿æ€æ ‡å®šåå·®é—®é¢˜ï¼Œå¹¶å®ç°äº†åˆæˆæ•°æ®çš„è‡ªåŠ¨æ ‡æ³¨ç”Ÿæˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„ç…§ç‰‡çº§çœŸå®æ„Ÿå›¾åƒåœ¨ Peak-Signal-to-Noise Ratio (PSNR) æŒ‡æ ‡ä¸Šè¾¾åˆ°29.87ï¼Œå¤„äºé¢†å…ˆæ°´å¹³ã€‚åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†çš„éªŒè¯ä¸­ï¼Œåˆ©ç”¨ NeeCo åˆæˆæ•°æ®è®­ç»ƒçš„æ¨¡å‹æ€§èƒ½æ¯”ä¼ ç»Ÿæ•°æ®å¢å¼ºæ–¹æ³•æå‡äº†10%ï¼Œä½¿æ•´ä½“æ£€æµ‹ä¸è¿½è¸ªæ€§èƒ½æé«˜äº†çº¦15%ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.07897v1",
      "published_date": "2025-08-11 12:13:05 UTC",
      "updated_date": "2025-08-11 12:13:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:01:18.995523+00:00"
    },
    {
      "arxiv_id": "2508.07887v1",
      "title": "Not Yet AlphaFold for the Mind: Evaluating Centaur as a Synthetic Participant",
      "title_zh": "å°šæœªæˆä¸ºå¿ƒæ™ºé¢†åŸŸçš„ AlphaFoldï¼šCentaur ä½œä¸ºåˆæˆå‚ä¸è€…çš„è¯„ä¼°",
      "authors": [
        "Sabrina Namazova",
        "Alessandra Brondetta",
        "Younes Strittmatter",
        "Matthew Nassar",
        "Sebastian Musslick"
      ],
      "abstract": "Simulators have revolutionized scientific practice across the natural sciences. By generating data that reliably approximate real-world phenomena, they enable scientists to accelerate hypothesis testing and optimize experimental designs. This is perhaps best illustrated by AlphaFold, a Nobel-prize winning simulator in chemistry that predicts protein structures from amino acid sequences, enabling rapid prototyping of molecular interactions, drug targets, and protein functions. In the behavioral sciences, a reliable participant simulator - a system capable of producing human-like behavior across cognitive tasks - would represent a similarly transformative advance. Recently, Binz et al. introduced Centaur, a large language model (LLM) fine-tuned on human data from 160 experiments, proposing its use not only as a model of cognition but also as a participant simulator for \"in silico prototyping of experimental studies\", e.g., to advance automated cognitive science. Here, we review the core criteria for a participant simulator and assess how well Centaur meets them. Although Centaur demonstrates strong predictive accuracy, its generative behavior - a critical criterion for a participant simulator - systematically diverges from human data. This suggests that, while Centaur is a significant step toward predicting human behavior, it does not yet meet the standards of a reliable participant simulator or an accurate model of cognition.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†Centaurï¼Œè¿™æ˜¯ä¸€ç§åœ¨160é¡¹å®éªŒçš„äººç±»æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒçš„å¤§è¯­è¨€æ¨¡å‹(LLM)ï¼Œæ—¨åœ¨ä½œä¸ºè¡Œä¸ºç§‘å­¦é¢†åŸŸçš„åˆæˆå‚ä¸è€…(Synthetic Participant)æ¨¡æ‹Ÿå™¨ï¼Œå®ç°å®éªŒç ”ç©¶çš„è®¡ç®—æœºåŸå‹åŒ–(in silico prototyping)ã€‚ç ”ç©¶è€…å‚ç…§AlphaFoldåœ¨è‡ªç„¶ç§‘å­¦ä¸­çš„æ¨¡æ‹Ÿä½œç”¨ï¼Œé€šè¿‡æ ¸å¿ƒæ ‡å‡†ç³»ç»Ÿè¯„ä¼°äº†Centauråœ¨é¢„æµ‹ç²¾åº¦ä¸ç”Ÿæˆè¡Œä¸ºæ–¹é¢çš„è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶Centaurå±•ç°å‡ºå¼ºå¤§çš„é¢„æµ‹å‡†ç¡®æ€§(predictive accuracy)ï¼Œä½†å…¶ç”Ÿæˆè¡Œä¸º(generative behavior)è¿™ä¸€å…³é”®æŒ‡æ ‡ä¸çœŸå®äººç±»æ•°æ®å­˜åœ¨ç³»ç»Ÿæ€§åå·®ã€‚è¿™è¡¨æ˜Centaurç›®å‰å°šæ— æ³•å®Œå…¨å¯é åœ°è¿‘ä¼¼ç°å®ä¸–ç•Œçš„è®¤çŸ¥ç°è±¡ï¼Œå°šæœªè¾¾åˆ°æˆç†Ÿå‚ä¸è€…æ¨¡æ‹Ÿå™¨æˆ–ç²¾ç¡®è®¤çŸ¥æ¨¡å‹(model of cognition)çš„æ ‡å‡†ã€‚è¯¥è®ºæ–‡è®¤ä¸ºCentauræ˜¯è¿ˆå‘é¢„æµ‹äººç±»è¡Œä¸ºçš„é‡è¦ä¸€æ­¥ï¼Œä½†åœ¨æ¨¡æ‹Ÿå¿ƒç†æœºåˆ¶çš„å‡†ç¡®æ€§ä¸Šä»æœ‰å¾…çªç ´ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07887v1",
      "published_date": "2025-08-11 12:05:18 UTC",
      "updated_date": "2025-08-11 12:05:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:01:29.184731+00:00"
    },
    {
      "arxiv_id": "2508.10040v3",
      "title": "Exploring Content and Social Connections of Fake News with Explainable Text and Graph Learning",
      "title_zh": "åŸºäºå¯è§£é‡Šæ–‡æœ¬ä¸å›¾å­¦ä¹ çš„è™šå‡æ–°é—»å†…å®¹ä¸ç¤¾äº¤å…³ç³»æ¢ç´¢",
      "authors": [
        "VÃ­tor N. LourenÃ§o",
        "Aline Paes",
        "Tillman Weyde"
      ],
      "abstract": "The global spread of misinformation and concerns about content trustworthiness have driven the development of automated fact-checking systems. Since false information often exploits social media dynamics such as \"likes\" and user networks to amplify its reach, effective solutions must go beyond content analysis to incorporate these factors. Moreover, simply labelling content as false can be ineffective or even reinforce biases such as automation and confirmation bias. This paper proposes an explainable framework that combines content, social media, and graph-based features to enhance fact-checking. It integrates a misinformation classifier with explainability techniques to deliver complete and interpretable insights supporting classification decisions. Experiments demonstrate that multimodal information improves performance over single modalities, with evaluations conducted on datasets in English, Spanish, and Portuguese. Additionally, the framework's explanations were assessed for interpretability, trustworthiness, and robustness with a novel protocol, showing that it effectively generates human-understandable justifications for its predictions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³è™šå‡ä¿¡æ¯ä¼ æ’­ä¸­å•çº¯ä¾é å†…å®¹åˆ†æçš„å±€é™æ€§ï¼Œé€šè¿‡æ•´åˆç¤¾äº¤åª’ä½“åŠ¨æ€ä¸ç”¨æˆ·ç½‘ç»œè¿æ¥æ¥æå‡äº‹å®æ ¸æŸ¥çš„æœ‰æ•ˆæ€§ã€‚è®ºæ–‡æå‡ºäº†ä¸€ä¸ªç»“åˆå†…å®¹ã€ç¤¾äº¤åª’ä½“å’ŒåŸºäºå›¾çš„ç‰¹å¾(Graph-based features)çš„å¯è§£é‡Šæ¡†æ¶ï¼Œå¹¶é›†æˆäº†ä¸€ä¸ªè¯¯å¯¼ä¿¡æ¯åˆ†ç±»å™¨ä¸å¯è§£é‡Šæ€§æŠ€æœ¯(Explainability techniques)ã€‚è¯¥æ¡†æ¶åœ¨è‹±è¯­ã€è¥¿ç­ç‰™è¯­å’Œè‘¡è„ç‰™è¯­çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜å¤šæ¨¡æ€(Multimodal)ä¿¡æ¯çš„ç»“åˆåœ¨æ€§èƒ½ä¸Šä¼˜äºå•ä¸€æ¨¡æ€ã€‚ç ”ç©¶è¿˜é€šè¿‡ä¸€ç§æ–°é¢–çš„åè®®å¯¹è§£é‡Šçš„è¯„ä»·è¿›è¡Œäº†å¯è§£é‡Šæ€§ã€å¯ä¿¡åº¦å’Œé²æ£’æ€§(Robustness)è¯„ä¼°ï¼Œè¯æ˜å…¶èƒ½ä¸ºé¢„æµ‹ç»“æœç”Ÿæˆäººç±»å¯ç†è§£çš„æ­£å½“ç†ç”±ã€‚è¯¥æˆæœä¸ä»…æé«˜äº†è‡ªåŠ¨äº‹å®æ ¸æŸ¥çš„å‡†ç¡®ç‡ï¼Œè¿˜é€šè¿‡æä¾›é€æ˜çš„å†³ç­–ä¾æ®æœ‰æ•ˆç¼“è§£äº†è‡ªåŠ¨åŒ–åè§å’Œç¡®è®¤åè§ã€‚",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "Accepted to publication at the 35th Brazilian Conference on Intelligent Systems, BRACIS 2025. -- This submitted manuscript has not undergone any post-submission improvements or corrections. The Version of Record of this contribution will be provided when available",
      "pdf_url": "https://arxiv.org/pdf/2508.10040v3",
      "published_date": "2025-08-11 12:03:37 UTC",
      "updated_date": "2025-08-19 10:32:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:01:25.995565+00:00"
    },
    {
      "arxiv_id": "2508.07885v1",
      "title": "Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning",
      "title_zh": "åŸºäºå¤šæ¨¡æ€æ„ŸçŸ¥ä¸LLMé©±åŠ¨é«˜å±‚è¯­ä¹‰æ¨ç†çš„äº‘æ§å››æ—‹ç¿¼é£è¡Œå™¨å—é™ç©ºé—´è‡ªä¸»å¯¼èˆª",
      "authors": [
        "Shoaib Ahmmad",
        "Zubayer Ahmed Aditto",
        "Md Mehrab Hossain",
        "Noushin Yeasmin",
        "Shorower Hossain"
      ],
      "abstract": "This paper introduces an advanced AI-driven perception system for autonomous quadcopter navigation in GPS-denied indoor environments. The proposed framework leverages cloud computing to offload computationally intensive tasks and incorporates a custom-designed printed circuit board (PCB) for efficient sensor data acquisition, enabling robust navigation in confined spaces. The system integrates YOLOv11 for object detection, Depth Anything V2 for monocular depth estimation, a PCB equipped with Time-of-Flight (ToF) sensors and an Inertial Measurement Unit (IMU), and a cloud-based Large Language Model (LLM) for context-aware decision-making. A virtual safety envelope, enforced by calibrated sensor offsets, ensures collision avoidance, while a multithreaded architecture achieves low-latency processing. Enhanced spatial awareness is facilitated by 3D bounding box estimation with Kalman filtering. Experimental results in an indoor testbed demonstrate strong performance, with object detection achieving a mean Average Precision (mAP50) of 0.6, depth estimation Mean Absolute Error (MAE) of 7.2 cm, only 16 safety envelope breaches across 42 trials over approximately 11 minutes, and end-to-end system latency below 1 second. This cloud-supported, high-intelligence framework serves as an auxiliary perception and navigation system, complementing state-of-the-art drone autonomy for GPS-denied confined spaces.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å…ˆè¿›çš„AIé©±åŠ¨æ„ŸçŸ¥ç³»ç»Ÿï¼Œç”¨äºGPS-deniedå®¤å†…å—é™ç©ºé—´çš„å››æ—‹ç¿¼æ— äººæœºè‡ªä¸»å¯¼èˆªã€‚è¯¥æ¡†æ¶åˆ©ç”¨Cloud Computingå¸è½½é«˜å¼ºåº¦è®¡ç®—ä»»åŠ¡ï¼Œå¹¶ä½¿ç”¨å®šåˆ¶çš„PCBé›†æˆToFä¼ æ„Ÿå™¨å’ŒIMUè¿›è¡Œé«˜æ•ˆæ•°æ®é‡‡é›†ã€‚ç³»ç»Ÿæ ¸å¿ƒèåˆäº†YOLOv11è¿›è¡Œç›®æ ‡æ£€æµ‹ã€Depth Anything V2è¿›è¡Œå•ç›®æ·±åº¦ä¼°è®¡ï¼Œå¹¶ç»“åˆåŸºäºäº‘ç«¯çš„LLMå®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å†³ç­–ä¸é«˜è¯­ä¹‰æ¨ç†ã€‚é€šè¿‡è™šæ‹ŸSafety Envelopeç¡®ä¿ç¢°æ’è§„é¿ï¼Œå¹¶åˆ©ç”¨Kalman Filteringè¿›è¡Œ3Dè¾¹ç•Œæ¡†ä¼°ç®—ä»¥å¢å¼ºç©ºé—´æ„ŸçŸ¥ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿå®ç°äº†0.6çš„mAP50ç›®æ ‡æ£€æµ‹ç²¾åº¦å’Œ7.2 cmçš„æ·±åº¦ä¼°è®¡å¹³å‡ç»å¯¹è¯¯å·®ï¼Œä¸”ç«¯åˆ°ç«¯å»¶è¿Ÿæ§åˆ¶åœ¨1ç§’ä»¥å†…ã€‚è¯¥é«˜æ™ºèƒ½æ„ŸçŸ¥æ¡†æ¶ä¸ºå¤æ‚å®¤å†…ç¯å¢ƒä¸‹çš„æ— äººæœºè‡ªä¸»æ€§æä¾›äº†å¼ºæœ‰åŠ›çš„è¾…åŠ©æ”¯æŒã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07885v1",
      "published_date": "2025-08-11 12:00:03 UTC",
      "updated_date": "2025-08-11 12:00:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:01:46.985319+00:00"
    },
    {
      "arxiv_id": "2508.09212v1",
      "title": "Deep Generative Models for Discrete Genotype Simulation",
      "title_zh": "ç”¨äºç¦»æ•£åŸºå› å‹æ¨¡æ‹Ÿçš„æ·±åº¦ç”Ÿæˆæ¨¡å‹",
      "authors": [
        "Sihan Xie",
        "Thierry Tribout",
        "Didier Boichard",
        "Blaise Hanczar",
        "Julien Chiquet",
        "Eric Barrey"
      ],
      "abstract": "Deep generative models open new avenues for simulating realistic genomic data while preserving privacy and addressing data accessibility constraints. While previous studies have primarily focused on generating gene expression or haplotype data, this study explores generating genotype data in both unconditioned and phenotype-conditioned settings, which is inherently more challenging due to the discrete nature of genotype data. In this work, we developed and evaluated commonly used generative models, including Variational Autoencoders (VAEs), Diffusion Models, and Generative Adversarial Networks (GANs), and proposed adaptation tailored to discrete genotype data. We conducted extensive experiments on large-scale datasets, including all chromosomes from cow and multiple chromosomes from human. Model performance was assessed using a well-established set of metrics drawn from both deep learning and quantitative genetics literature. Our results show that these models can effectively capture genetic patterns and preserve genotype-phenotype association. Our findings provide a comprehensive comparison of these models and offer practical guidelines for future research in genotype simulation. We have made our code publicly available at https://github.com/SihanXXX/DiscreteGenoGen.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨æ·±åº¦ç”Ÿæˆæ¨¡å‹(Deep Generative Models)æ¨¡æ‹Ÿç¦»æ•£åŸºå› å‹(Genotype)æ•°æ®çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³åŸºå› ç»„æ•°æ®çš„éšç§å’Œè·å–å—é™é—®é¢˜ã€‚é’ˆå¯¹åŸºå› å‹æ•°æ®çš„ç¦»æ•£ç‰¹æ€§ï¼Œä½œè€…è¯„ä¼°å¹¶æ”¹è¿›äº†å˜åˆ†è‡ªç¼–ç å™¨(VAEs)ã€æ‰©æ•£æ¨¡å‹(Diffusion Models)åŠç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(GANs)ï¼Œä½¿å…¶èƒ½åŒæ—¶å¤„ç†æ— æ¡ä»¶å’Œè¡¨å‹æ¡ä»¶(Phenotype-conditioned)ä¸‹çš„æ¨¡æ‹Ÿä»»åŠ¡ã€‚é€šè¿‡åœ¨ç‰›å’Œäººç±»çš„å¤§è§„æ¨¡æŸ“è‰²ä½“æ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼Œå¹¶ç»“åˆæ·±åº¦å­¦ä¹ ä¸å®šé‡é—ä¼ å­¦(Quantitative Genetics)çš„æŒ‡æ ‡è¿›è¡Œè¯„ä»·ï¼Œç»“æœè¯æ˜è¿™äº›æ¨¡å‹èƒ½æœ‰æ•ˆæ•æ‰é—ä¼ æ¨¡å¼å¹¶ä¿æŒåŸºå› å‹ä¸è¡¨å‹çš„å…³è”ã€‚è¯¥é¡¹å·¥ä½œæä¾›äº†ç”Ÿæˆæ¨¡å‹åœ¨åŸºå› å‹æ¨¡æ‹Ÿé¢†åŸŸçš„å…¨é¢æ¯”è¾ƒä¸å®è·µæŒ‡å—ï¼Œå¹¶å·²å…¬å¼€ç›¸å…³ä»£ç ï¼Œä¸ºåç»­ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.GN",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.09212v1",
      "published_date": "2025-08-11 11:56:03 UTC",
      "updated_date": "2025-08-11 11:56:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:02:42.368031+00:00"
    },
    {
      "arxiv_id": "2508.07877v1",
      "title": "Selective Contrastive Learning for Weakly Supervised Affordance Grounding",
      "title_zh": "å¼±ç›‘ç£ç¤ºèƒ½æ€§å®šä½çš„é€‰æ‹©æ€§å¯¹æ¯”å­¦ä¹ ",
      "authors": [
        "WonJun Moon",
        "Hyun Seok Seong",
        "Jae-Pil Heo"
      ],
      "abstract": "Facilitating an entity's interaction with objects requires accurately identifying parts that afford specific actions. Weakly supervised affordance grounding (WSAG) seeks to imitate human learning from third-person demonstrations, where humans intuitively grasp functional parts without needing pixel-level annotations. To achieve this, grounding is typically learned using a shared classifier across images from different perspectives, along with distillation strategies incorporating part discovery process. However, since affordance-relevant parts are not always easily distinguishable, models primarily rely on classification, often focusing on common class-specific patterns that are unrelated to affordance. To address this limitation, we move beyond isolated part-level learning by introducing selective prototypical and pixel contrastive objectives that adaptively learn affordance-relevant cues at both the part and object levels, depending on the granularity of the available information. Initially, we find the action-associated objects in both egocentric (object-focused) and exocentric (third-person example) images by leveraging CLIP. Then, by cross-referencing the discovered objects of complementary views, we excavate the precise part-level affordance clues in each perspective. By consistently learning to distinguish affordance-relevant regions from affordance-irrelevant background context, our approach effectively shifts activation from irrelevant areas toward meaningful affordance cues. Experimental results demonstrate the effectiveness of our method. Codes are available at github.com/hynnsk/SelectiveCL.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼±ç›‘ç£åŠŸèƒ½æ€§åŒºåŸŸå®šä½ (Weakly supervised affordance grounding, WSAG) ä¸­æ¨¡å‹è¿‡åº¦ä¾èµ–åˆ†ç±»æ¨¡å¼è€Œå¿½ç•¥ç‰¹å®šåŠŸèƒ½çº¿ç´¢çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é€‰æ‹©æ€§å¯¹æ¯”å­¦ä¹  (Selective Contrastive Learning) æ–¹æ³•ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†é€‰æ‹©æ€§åŸå‹å’Œåƒç´ å¯¹æ¯”ç›®æ ‡ï¼Œèƒ½å¤Ÿæ ¹æ®å¯ç”¨ä¿¡æ¯çš„ç²’åº¦ï¼Œåœ¨é›¶ä»¶å’Œå¯¹è±¡å±‚çº§ä¸Šè‡ªé€‚åº”åœ°å­¦ä¹ åŠŸèƒ½ç›¸å…³ç‰¹å¾ã€‚ç ”ç©¶è€…åˆ©ç”¨ CLIP æ¨¡å‹åœ¨ç¬¬ä¸€äººç§° (egocentric) å’Œç¬¬ä¸‰äººç§° (exocentric) å›¾åƒä¸­è¯†åˆ«ä¸åŠ¨ä½œå…³è”çš„å¯¹è±¡ï¼Œå¹¶é€šè¿‡äº¤å‰å¼•ç”¨äº’è¡¥è§†è§’ä¸­çš„å‘ç°ï¼ŒæŒ–æ˜å‡ºæ¯ä¸ªè§†è§’ä¸‹ç²¾ç¡®çš„é›¶ä»¶çº§åŠŸèƒ½çº¿ç´¢ã€‚é€šè¿‡æŒç»­å­¦ä¹ åŒºåˆ†åŠŸèƒ½ç›¸å…³åŒºåŸŸä¸æ— å…³èƒŒæ™¯ä¸Šä¸‹æ–‡ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆåœ°å°†æ¨¡å‹æ¿€æ´»ä»æ— å…³åŒºåŸŸè½¬ç§»åˆ°å…·æœ‰å®é™…æ„ä¹‰çš„åŠŸèƒ½æç¤ºä¸Šã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç›¸å…³ä»£ç å·²åœ¨ GitHub å¼€æºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.07877v1",
      "published_date": "2025-08-11 11:49:37 UTC",
      "updated_date": "2025-08-11 11:49:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:01:40.595551+00:00"
    },
    {
      "arxiv_id": "2508.07875v1",
      "title": "Towards Human-AI Collaboration System for the Detection of Invasive Ductal Carcinoma in Histopathology Images",
      "title_zh": "é¢å‘ç»„ç»‡ç—…ç†å­¦å›¾åƒæµ¸æ¶¦æ€§å¯¼ç®¡ç™Œæ£€æµ‹çš„äººæœºåä½œç³»ç»Ÿ",
      "authors": [
        "Shuo Han",
        "Ahmed Karam Eldaly",
        "Solomon Sunday Oyelere"
      ],
      "abstract": "Invasive ductal carcinoma (IDC) is the most prevalent form of breast cancer, and early, accurate diagnosis is critical to improving patient survival rates by guiding treatment decisions. Combining medical expertise with artificial intelligence (AI) holds significant promise for enhancing the precision and efficiency of IDC detection. In this work, we propose a human-in-the-loop (HITL) deep learning system designed to detect IDC in histopathology images. The system begins with an initial diagnosis provided by a high-performance EfficientNetV2S model, offering feedback from AI to the human expert. Medical professionals then review the AI-generated results, correct any misclassified images, and integrate the revised labels into the training dataset, forming a feedback loop from the human back to the AI. This iterative process refines the model's performance over time. The EfficientNetV2S model itself achieves state-of-the-art performance compared to existing methods in the literature, with an overall accuracy of 93.65\\%. Incorporating the human-in-the-loop system further improves the model's accuracy using four experimental groups with misclassified images. These results demonstrate the potential of this collaborative approach to enhance AI performance in diagnostic systems. This work contributes to advancing automated, efficient, and highly accurate methods for IDC detection through human-AI collaboration, offering a promising direction for future AI-assisted medical diagnostics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºç»„ç»‡ç—…ç†å­¦å›¾åƒä¸­ Invasive Ductal Carcinoma (IDC) æ£€æµ‹çš„äººæœºåä½œç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡åŒ»ç–—ä¸“å®¶ä¸äººå·¥æ™ºèƒ½çš„ç»“åˆæå‡è¯Šæ–­çš„ç²¾ç¡®åº¦ä¸æ•ˆç‡ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨äº† Human-in-the-Loop (HITL) æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œé¦–å…ˆç”±é«˜æ€§èƒ½çš„ EfficientNetV2S æ¨¡å‹æä¾›åˆæ­¥è¯Šæ–­åé¦ˆï¼Œå†ç”±åŒ»ç–—ä¸“ä¸šäººå‘˜å¯¹ AI ç”Ÿæˆçš„ç»“æœè¿›è¡Œå®¡æŸ¥ä¸ä¿®æ­£ã€‚é€šè¿‡å°†ä¿®æ­£åçš„æ ‡ç­¾é‡æ–°æ•´åˆè¿›è®­ç»ƒæ•°æ®é›†ï¼Œç³»ç»Ÿå½¢æˆäº†ä¸€ä¸ªæŒç»­ä¼˜åŒ–çš„åé¦ˆé—­ç¯ï¼Œä»è€Œå®ç°æ¨¡å‹æ€§èƒ½çš„ä¸æ–­ç²¾ç‚¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEfficientNetV2S æ¨¡å‹åœ¨ IDC æ£€æµ‹ä¸­è¾¾åˆ°äº† 93.65% çš„å‡†ç¡®ç‡ï¼Œè€Œå¼•å…¥ HITL æœºåˆ¶åè¿›ä¸€æ­¥æ˜¾è‘—æ”¹å–„äº†ç³»ç»Ÿå¯¹è¯¯åˆ†ç±»å›¾åƒçš„å¤„ç†æ•ˆæœã€‚è¿™é¡¹å·¥ä½œä¸ºæ„å»ºè‡ªåŠ¨åŒ–ã€é«˜æ•ˆä¸”é«˜ç²¾åº¦çš„åŒ»ç–—è¯Šæ–­ç³»ç»Ÿæä¾›äº†é‡è¦å‚è€ƒï¼Œå±•ç¤ºäº† Human-AI Collaboration åœ¨æœªæ¥åŒ»å­¦è¾…åŠ©è¯Šæ–­é¢†åŸŸçš„å¹¿é˜”å‰æ™¯ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.HC"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07875v1",
      "published_date": "2025-08-11 11:45:57 UTC",
      "updated_date": "2025-08-11 11:45:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:01:48.891874+00:00"
    },
    {
      "arxiv_id": "2508.08345v2",
      "title": "Do AI Companies Make Good on Voluntary Commitments to the White House?",
      "title_zh": "AIå…¬å¸æ˜¯å¦å±¥è¡Œäº†å¯¹ç™½å®«çš„è‡ªæ„¿æ‰¿è¯ºï¼Ÿ",
      "authors": [
        "Jennifer Wang",
        "Kayla Huang",
        "Kevin Klyman",
        "Rishi Bommasani"
      ],
      "abstract": "Voluntary commitments are central to international AI governance, as demonstrated by recent voluntary guidelines from the White House to the G7, from Bletchley Park to Seoul. How do major AI companies make good on their commitments? We score companies based on their publicly disclosed behavior by developing a detailed rubric based on their eight voluntary commitments to the White House in 2023. We find significant heterogeneity: while the highest-scoring company (OpenAI) scores a 83% overall on our rubric, the average score across all companies is just 53%. The companies demonstrate systemically poor performance for their commitment to model weight security with an average score of 17%: 11 of the 16 companies receive 0% for this commitment. Our analysis highlights a clear structural shortcoming that future AI governance initiatives should correct: when companies make public commitments, they should proactively disclose how they meet their commitments to provide accountability, and these disclosures should be verifiable. To advance policymaking on corporate AI governance, we provide three directed recommendations that address underspecified commitments, the role of complex AI supply chains, and public transparency that could be applied towards AI governance initiatives worldwide.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†ä¸»è¦çš„ AI å…¬å¸å¦‚ä½•å±¥è¡Œå…¶åœ¨ 2023 å¹´å‘ç™½å®«åšå‡ºçš„ 8 é¡¹è‡ªæ„¿æ‰¿è¯ºï¼Œå¹¶åŸºäºå„å…¬å¸å…¬å¼€æŠ«éœ²çš„è¡Œä¸ºå¼€å‘äº†ä¸€å¥—è¯¦ç»†çš„è¯„ä¼°å‡†åˆ™ (rubric) è¿›è¡Œé‡åŒ–è¯„åˆ†ã€‚ç ”ç©¶å‘ç°å„å…¬å¸åœ¨å±¥è¡Œæ‰¿è¯ºæ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„å·®å¼‚æ€§ï¼Œå°½ç®¡ OpenAI ä»¥ 83% çš„æ€»åˆ†é¢†å…ˆï¼Œä½†æ‰€æœ‰å…¬å¸çš„å¹³å‡å¾—åˆ†ä»…ä¸º 53%ã€‚ç‰¹åˆ«æ˜¯åœ¨ model weight securityï¼ˆæ¨¡å‹æƒé‡å®‰å…¨ï¼‰æ‰¿è¯ºä¸Šï¼Œå„å…¬å¸è¡¨ç°æ™®éä½è¿·ï¼Œå¹³å‡å¾—åˆ†ä»… 17%ï¼Œä¸”æœ‰ 11 å®¶å…¬å¸åœ¨è¯¥é¡¹å¾—åˆ†ä¸ºé›¶ã€‚åˆ†ææŒ‡å‡ºï¼Œå½“å‰ AI æ²»ç†å­˜åœ¨æ˜æ˜¾çš„ç»“æ„æ€§ç¼ºé™·ï¼Œå³ç¼ºä¹ç¡®ä¿å…¬å¸ä¸»åŠ¨æŠ«éœ²å±¥è¡Œæƒ…å†µä¸”å¯éªŒè¯ (verifiable) çš„é—®è´£æœºåˆ¶ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†é’ˆå¯¹æ‰¿è¯ºç•Œå®šã€å¤æ‚ AI supply chains åŠå…¬å…±é€æ˜åº¦é—®é¢˜çš„ä¸‰é¡¹å»ºè®®ï¼Œä¸ºå…¨çƒ AI æ²»ç†æ”¿ç­–çš„å®Œå–„æä¾›äº†å‚è€ƒä¾æ®ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08345v2",
      "published_date": "2025-08-11 11:23:28 UTC",
      "updated_date": "2025-09-24 06:30:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:01:53.203744+00:00"
    },
    {
      "arxiv_id": "2508.07852v1",
      "title": "Vertex Features for Neural Global Illumination",
      "title_zh": "ç¥ç»å…¨å±€å…‰ç…§çš„é¡¶ç‚¹ç‰¹å¾",
      "authors": [
        "Rui Su",
        "Honghao Dong",
        "Haojie Jin",
        "Yisong Chen",
        "Guoping Wang",
        "Sheng Li"
      ],
      "abstract": "Recent research on learnable neural representations has been widely adopted in the field of 3D scene reconstruction and neural rendering applications. However, traditional feature grid representations often suffer from substantial memory footprint, posing a significant bottleneck for modern parallel computing hardware. In this paper, we present neural vertex features, a generalized formulation of learnable representation for neural rendering tasks involving explicit mesh surfaces. Instead of uniformly distributing neural features throughout 3D space, our method stores learnable features directly at mesh vertices, leveraging the underlying geometry as a compact and structured representation for neural processing. This not only optimizes memory efficiency, but also improves feature representation by aligning compactly with the surface using task-specific geometric priors. We validate our neural representation across diverse neural rendering tasks, with a specific emphasis on neural radiosity. Experimental results demonstrate that our method reduces memory consumption to only one-fifth (or even less) of grid-based representations, while maintaining comparable rendering quality and lowering inference overhead.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Vertex Featuresï¼ˆé¡¶ç‚¹ç‰¹å¾ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹æ¶‰åŠæ˜¾å¼ç½‘æ ¼è¡¨é¢çš„ç¥ç»æ¸²æŸ“ä»»åŠ¡çš„é€šç”¨å¯å­¦ä¹ è¡¨ç¤ºå½¢å¼ã€‚é’ˆå¯¹ä¼ ç»Ÿ Feature Gridï¼ˆç‰¹å¾ç½‘æ ¼ï¼‰è¡¨ç¤ºæ³•åœ¨ä¸‰ç»´é‡å»ºä¸­å› å†…å­˜å ç”¨è¿‡é«˜è€Œå¯¼è‡´çš„ç¡¬ä»¶ç“¶é¢ˆé—®é¢˜ï¼Œè¯¥æ–¹æ³•é€šè¿‡å°†å¯å­¦ä¹ ç‰¹å¾ç›´æ¥å­˜å‚¨åœ¨ Mesh Verticesï¼ˆç½‘æ ¼é¡¶ç‚¹ï¼‰ä¸Šï¼Œåˆ©ç”¨åº•å±‚å‡ ä½•ç»“æ„å®ç°ç´§å‡‘ä¸”ç»“æ„åŒ–çš„ç¥ç»å¤„ç†ã€‚è¿™ç§æ–¹æ³•ä¸ä»…é€šè¿‡åˆ©ç”¨ç‰¹å®šä»»åŠ¡çš„å‡ ä½•å…ˆéªŒä¸è¡¨é¢ç´§å¯†å¯¹é½æ¥ä¼˜åŒ–ç‰¹å¾è¡¨è¾¾ï¼Œè¿˜æ˜¾è‘—æå‡äº†å†…å­˜æ•ˆç‡ã€‚ä½œè€…åœ¨åŒ…æ‹¬ Neural Radiosityï¼ˆç¥ç»è¾å°„åº¦ï¼‰åœ¨å†…çš„å¤šç§ç¥ç»æ¸²æŸ“ä»»åŠ¡ä¸­éªŒè¯äº†è¯¥è¡¨ç¤ºå½¢å¼çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºäº Gridï¼ˆç½‘æ ¼ï¼‰çš„è¡¨ç¤ºæ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒç›¸å½“æ¸²æŸ“è´¨é‡çš„åŒæ—¶ï¼Œå°†å†…å­˜æ¶ˆè€—é™ä½è‡³äº”åˆ†ä¹‹ä¸€ç”šè‡³æ›´ä½ï¼Œå¹¶æœ‰æ•ˆå‡å°‘äº†æ¨ç†å¼€é”€ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI"
      ],
      "primary_category": "cs.GR",
      "comment": "Accepted by ACM SIGGRAPH Asia'2025",
      "pdf_url": "https://arxiv.org/pdf/2508.07852v1",
      "published_date": "2025-08-11 11:10:19 UTC",
      "updated_date": "2025-08-11 11:10:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:02:52.862690+00:00"
    },
    {
      "arxiv_id": "2508.07847v1",
      "title": "Deep Space Weather Model: Long-Range Solar Flare Prediction from Multi-Wavelength Images",
      "title_zh": "Deep Space Weather Modelï¼šåŸºäºå¤šæ³¢æ®µå›¾åƒçš„é•¿ç¨‹å¤ªé˜³è€€æ–‘é¢„æµ‹",
      "authors": [
        "Shunya Nagashima",
        "Komei Sugiura"
      ],
      "abstract": "Accurate, reliable solar flare prediction is crucial for mitigating potential disruptions to critical infrastructure, while predicting solar flares remains a significant challenge. Existing methods based on heuristic physical features often lack representation learning from solar images. On the other hand, end-to-end learning approaches struggle to model long-range temporal dependencies in solar images. In this study, we propose Deep Space Weather Model (Deep SWM), which is based on multiple deep state space models for handling both ten-channel solar images and long-range spatio-temporal dependencies. Deep SWM also features a sparse masked autoencoder, a novel pretraining strategy that employs a two-phase masking approach to preserve crucial regions such as sunspots while compressing spatial information. Furthermore, we built FlareBench, a new public benchmark for solar flare prediction covering a full 11-year solar activity cycle, to validate our method. Our method outperformed baseline methods and even human expert performance on standard metrics in terms of performance and reliability. The project page can be found at https://keio-smilab25.github.io/DeepSWM.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Deep Space Weather Model (Deep SWM)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¤ªé˜³è€€æ–‘(solar flare)é¢„æµ‹æ–¹æ³•åœ¨å¤„ç†å¤šæ³¢æ®µå›¾åƒè¡¨å¾å­¦ä¹ å’Œé•¿ç¨‹æ—¶ç©ºä¾èµ–å…³ç³»(long-range spatio-temporal dependencies)æ–¹é¢çš„ä¸è¶³ã€‚è¯¥æ¨¡å‹åŸºäºå¤šä¸ªæ·±åº¦çŠ¶æ€ç©ºé—´æ¨¡å‹(deep state space models)ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†åé€šé“å¤ªé˜³å›¾åƒå¹¶æ•æ‰å¤æ‚çš„é•¿ç¨‹åŠ¨æ€ã€‚Deep SWMå¼•å…¥äº†ä¸€ç§ç¨€ç–æ©ç è‡ªç¼–ç å™¨(sparse masked autoencoder)ï¼Œé€šè¿‡åˆ›æ–°çš„ä¸¤é˜¶æ®µæ©ç ç­–ç•¥åœ¨å‹ç¼©ç©ºé—´ä¿¡æ¯çš„åŒæ—¶ä¿ç•™å¤ªé˜³é»‘å­(sunspots)ç­‰å…³é”®åŒºåŸŸã€‚ä¸ºäº†éªŒè¯è¯¥æ–¹æ³•ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†è¦†ç›–å®Œæ•´11å¹´å¤ªé˜³æ´»åŠ¨å‘¨æœŸçš„å…¬å¼€åŸºå‡†æ•°æ®é›†FlareBenchã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDeep SWMåœ¨æ ‡å‡†æŒ‡æ ‡ä¸Šçš„è¡¨ç°å’Œå¯é æ€§å‡ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œç”šè‡³è¶…è¿‡äº†äººç±»ä¸“å®¶çš„é¢„æµ‹æ°´å¹³ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.07847v1",
      "published_date": "2025-08-11 11:06:56 UTC",
      "updated_date": "2025-08-11 11:06:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:03:00.157829+00:00"
    },
    {
      "arxiv_id": "2508.08344v4",
      "title": "What Breaks Knowledge Graph based RAG? Benchmarking and Empirical Insights into Reasoning under Incomplete Knowledge",
      "title_zh": "ä½•ç§å› ç´ å¯¼è‡´åŸºäºçŸ¥è¯†å›¾è°±çš„ RAG å¤±æ•ˆï¼Ÿå…³äºä¸å®Œå…¨çŸ¥è¯†ä¸‹æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ä¸å®è¯ç ”ç©¶",
      "authors": [
        "Dongzhuoran Zhou",
        "Yuqicheng Zhu",
        "Xiaxia Wang",
        "Hongkuan Zhou",
        "Yuan He",
        "Jiaoyan Chen",
        "Steffen Staab",
        "Evgeny Kharlamov"
      ],
      "abstract": "Knowledge Graph-based Retrieval-Augmented Generation (KG-RAG) is an increasingly explored approach for combining the reasoning capabilities of large language models with the structured evidence of knowledge graphs. However, current evaluation practices fall short: existing benchmarks often include questions that can be directly answered using existing triples in KG, making it unclear whether models perform reasoning or simply retrieve answers directly. Moreover, inconsistent evaluation metrics and lenient answer matching criteria further obscure meaningful comparisons. In this work, we introduce a general method for constructing benchmarks and present BRINK (Benchmark for Reasoning under Incomplete Knowledge) to systematically assess KG-RAG methods under knowledge incompleteness. Our empirical results show that current KG-RAG methods have limited reasoning ability under missing knowledge, often rely on internal memorization, and exhibit varying degrees of generalization depending on their design.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†åŸºäºçŸ¥è¯†å›¾è°±çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(KG-RAG)åœ¨ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹(LLM)æ¨ç†èƒ½åŠ›ä¸ç»“æ„åŒ–è¯æ®æ–¹é¢çš„æ½œåŠ›ã€‚ä½œè€…æŒ‡å‡ºç›®å‰çš„è¯„ä¼°å®è·µå­˜åœ¨ç¼ºé™·ï¼Œç°æœ‰åŸºå‡†å¾€å¾€åŒ…å«å¯ç›´æ¥é€šè¿‡çŸ¥è¯†å›¾è°±ä¸­çš„ä¸‰å…ƒç»„æ£€ç´¢å›ç­”çš„é—®é¢˜ï¼Œå¯¼è‡´æ— æ³•åŒºåˆ†æ¨¡å‹æ˜¯åœ¨è¿›è¡Œæ¨ç†è¿˜æ˜¯ç®€å•æ£€ç´¢ã€‚ä¸ºæ­¤ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ„å»ºè¯„ä¼°åŸºå‡†çš„é€šç”¨æ–¹æ³•ï¼Œå¹¶å‘å¸ƒäº†BRINK (Benchmark for Reasoning under Incomplete Knowledge)ï¼Œæ—¨åœ¨ç³»ç»Ÿæµ‹è¯•KG-RAGåœ¨çŸ¥è¯†ç¼ºå¤±(Knowledge Incompleteness)ä¸‹çš„æ¨ç†è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç›®å‰çš„KG-RAGæ–¹æ³•åœ¨çŸ¥è¯†ä¸å®Œæ•´æ—¶æ¨ç†èƒ½åŠ›æœ‰é™ï¼Œä¸”é«˜åº¦ä¾èµ–æ¨¡å‹çš„å†…éƒ¨è®°å¿†(Internal Memorization)ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œä¸åŒè®¾è®¡çš„KG-RAGæ¨¡å‹åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šè¡¨ç°å‡ºæ˜¾è‘—å·®å¼‚ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡æ­ç¤ºç°æœ‰æŠ€æœ¯çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥å¼€å‘æ›´å…·é²æ£’æ€§çš„ç»“æ„åŒ–çŸ¥è¯†æ¨ç†ç³»ç»Ÿæä¾›äº†å…³é”®çš„å®è¯è§è§£ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted as a main conference paper at EACL 2026",
      "pdf_url": "https://arxiv.org/pdf/2508.08344v4",
      "published_date": "2025-08-11 10:55:06 UTC",
      "updated_date": "2026-01-12 03:02:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:03:07.858547+00:00"
    },
    {
      "arxiv_id": "2508.07842v2",
      "title": "DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts",
      "title_zh": "DETACHï¼šåŸºäºè§£è€¦ä¸“å®¶æ··åˆçš„é•¿ç¨‹ä»»åŠ¡è·¨é¢†åŸŸå­¦ä¹ ",
      "authors": [
        "Yutong Shen",
        "Hangxu Liu",
        "Lei Zhang",
        "Penghui Liu",
        "Ruizhe Xia",
        "Tianyi Yao",
        "Tongtong Feng"
      ],
      "abstract": "Long-Horizon (LH) tasks in Human-Scene Interaction (HSI) are complex multi-step tasks that require continuous planning, sequential decision-making, and extended execution across domains to achieve the final goal. However, existing methods heavily rely on skill chaining by concatenating pre-trained subtasks, with environment observations and self-state tightly coupled, lacking the ability to generalize to new combinations of environments and skills, failing to complete various LH tasks across domains. To solve this problem, this paper presents DETACH, a cross-domain learning framework for LH tasks via biologically inspired dual-stream disentanglement. Inspired by the brain's \"where-what\" dual pathway mechanism, DETACH comprises two core modules: i) an environment learning module for spatial understanding, which captures object functions, spatial relationships, and scene semantics, achieving cross-domain transfer through complete environment-self disentanglement; ii) a skill learning module for task execution, which processes self-state information including joint degrees of freedom and motor patterns, enabling cross-skill transfer through independent motor pattern encoding. We conducted extensive experiments on various LH tasks in HSI scenes. Compared with existing methods, DETACH can achieve an average subtasks success rate improvement of 23% and average execution efficiency improvement of 29%.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DETACHï¼Œä¸€ç§é’ˆå¯¹äººç±»-åœºæ™¯äº¤äº’ (Human-Scene Interaction, HSI) ä¸­é•¿æ—¶ç¨‹ä»»åŠ¡ (Long-Horizon tasks) çš„è·¨åŸŸå­¦ä¹ æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¸­ç¯å¢ƒè§‚æµ‹ä¸è‡ªèº«çŠ¶æ€ç´§å¯†è€¦åˆå¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼ŒDETACH å¼•å…¥äº†å—ç”Ÿç‰©å¯å‘çš„â€œwhere-whatâ€åŒæµè§£è€¦æœºåˆ¶ã€‚è¯¥æ¡†æ¶ç”±è´Ÿè´£ç©ºé—´ç†è§£çš„ç¯å¢ƒå­¦ä¹ æ¨¡å—å’Œè´Ÿè´£ä»»åŠ¡æ‰§è¡Œçš„æŠ€èƒ½å­¦ä¹ æ¨¡å—ç»„æˆï¼Œåˆ†åˆ«é€šè¿‡è§£è€¦ç¯å¢ƒ-è‡ªèº«ä¿¡æ¯å®ç°è·¨åŸŸè¿ç§»ï¼Œä»¥åŠé€šè¿‡ç‹¬ç«‹è¿åŠ¨æ¨¡å¼ç¼–ç å®ç°è·¨æŠ€èƒ½è¿ç§»ã€‚åœ¨å¤šç§ HSI åœºæ™¯ä¸‹çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDETACH ç›¸æ¯”ç°æœ‰æ–¹æ³•åœ¨å­ä»»åŠ¡å¹³å‡æˆåŠŸç‡ä¸Šæå‡äº† 23%ï¼ŒåŒæ—¶æ‰§è¡Œæ•ˆç‡æé«˜äº† 29%ï¼Œæ˜¾è‘—å¢å¼ºäº†å¤æ‚å¤šæ­¥ä»»åŠ¡çš„è§„åˆ’ä¸æ‰§è¡Œèƒ½åŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "14 pages,8 figures. Submitted to ICRA'26",
      "pdf_url": "https://arxiv.org/pdf/2508.07842v2",
      "published_date": "2025-08-11 10:54:28 UTC",
      "updated_date": "2025-09-22 12:52:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:03:04.890073+00:00"
    },
    {
      "arxiv_id": "2508.08343v3",
      "title": "A Data-driven ML Approach for Maximizing Performance in LLM-Adapter Serving",
      "title_zh": "é¢å‘ LLM é€‚é…å™¨æœåŠ¡æ€§èƒ½æœ€å¤§åŒ–çš„æ•°æ®é©±åŠ¨æœºå™¨å­¦ä¹ æ–¹æ³•",
      "authors": [
        "Ferran Agullo",
        "Joan Oliveras",
        "Chen Wang",
        "Alberto Gutierrez-Torre",
        "Olivier Tardieu",
        "Alaa Youssef",
        "Jordi Torres",
        "Josep Ll. Berral"
      ],
      "abstract": "With the rapid adoption of Large Language Models (LLMs), LLM-adapters have become increasingly common, providing lightweight specialization of large-scale models. Serving hundreds or thousands of these adapters on a single GPU allows request aggregation, increasing throughput, but may also cause request starvation if GPU memory limits are exceeded. To address this issue, this study focuses on determining the joint configuration of concurrent and parallel adapters that maximizes GPU throughput without inducing starvation, given heterogeneous adapter and traffic properties. We propose a data-driven ML approach leveraging interpretable models to tackle this caching problem and introduce the first Digital Twin capable of reproducing an LLM-adapter serving system, enabling efficient training data generation. Experiments with the vLLM framework and LoRA adapters show that the Digital Twin reproduces throughput within 5.1% of real results, while the ML approach predicts optimal numbers of concurrent and parallel adapters with an error of at most 7.2% under heterogeneous, real-world workloads. The code is publicly available at https://github.com/FerranAgulloLopez/GPULLMAdapterOptimization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹(LLMs)ç¯å¢ƒä¸‹ï¼Œå•GPUéƒ¨ç½²å¤§é‡é€‚é…å™¨å¯¼è‡´çš„è¯·æ±‚é¥¥é¥¿(request starvation)ä¸ååé‡æƒè¡¡é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ•°æ®é©±åŠ¨çš„æœºå™¨å­¦ä¹ (ML)ä¼˜åŒ–æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¯è§£é‡Šæ¨¡å‹(interpretable models)æ¥ç¡®å®šå¹¶å‘(concurrent)å’Œå¹¶è¡Œ(parallel)é€‚é…å™¨çš„æœ€ä½³è”åˆé…ç½®ï¼Œæ—¨åœ¨å¼‚æ„æµé‡ç¯å¢ƒä¸‹æœ€å¤§åŒ–GPUååé‡(throughput)ã€‚ç ”ç©¶è€…è¿˜å¼€å‘äº†é¦–ä¸ªé’ˆå¯¹LLM-adapteræœåŠ¡ç³»ç»Ÿçš„æ•°å­—å­ªç”Ÿ(Digital Twin)æ¨¡å‹ï¼Œä¸ºè®­ç»ƒæ•°æ®çš„é«˜æ•ˆç”Ÿæˆæä¾›äº†æ”¯æ’‘ã€‚åœ¨vLLMæ¡†æ¶å’ŒLoRAé€‚é…å™¨ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ•°å­—å­ªç”Ÿæ¨¡å‹å¯¹ååé‡çš„æ¨¡æ‹Ÿè¯¯å·®åœ¨5.1%ä»¥å†…ï¼Œä¸”MLæ–¹æ³•åœ¨é¢„æµ‹æœ€ä¼˜é€‚é…å™¨æ•°é‡æ—¶çš„è¯¯å·®ä¸è¶…è¿‡7.2%ã€‚è¯¥æˆæœä¸ºæå‡ç°å®ä¸–ç•Œå¼‚æ„è´Ÿè½½ä¸‹çš„é€‚é…å™¨æœåŠ¡æ€§èƒ½æä¾›äº†é«˜æ•ˆä¸”ç²¾å‡†çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.PF",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.PF",
      "comment": "Accepted in a computer science workshop",
      "pdf_url": "https://arxiv.org/pdf/2508.08343v3",
      "published_date": "2025-08-11 10:47:35 UTC",
      "updated_date": "2025-11-19 13:36:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:03:14.093595+00:00"
    },
    {
      "arxiv_id": "2508.07834v4",
      "title": "KIRETT: Knowledge-Graph-Based Smart Treatment Assistant for Intelligent Rescue Operations",
      "title_zh": "KIRETTï¼šé¢å‘æ™ºæ…§æ•‘æ´è¡ŒåŠ¨çš„åŸºäºçŸ¥è¯†å›¾è°±çš„æ™ºèƒ½æ²»ç–—åŠ©æ‰‹",
      "authors": [
        "Mubaris Nadeem",
        "Johannes Zenkert",
        "Lisa Bender",
        "Christian Weber",
        "Madjid Fathi"
      ],
      "abstract": "Over the years, the need for rescue operations throughout the world has increased rapidly. Demographic changes and the resulting risk of injury or health disorders form the basis for emergency calls. In such scenarios, first responders are in a rush to reach the patient in need, provide first aid, and save lives. In these situations, they must be able to provide personalized and optimized healthcare in the shortest possible time and estimate the patients condition with the help of freshly recorded vital data in an emergency situation. However, in such a timedependent situation, first responders and medical experts cannot fully grasp their knowledge and need assistance and recommendation for further medical treatments. To achieve this, on the spot calculated, evaluated, and processed knowledge must be made available to improve treatments by first responders. The Knowledge Graph presented in this article as a central knowledge representation provides first responders with an innovative knowledge management that enables intelligent treatment recommendations with an artificial intelligence-based pre-recognition of the situation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† KIRETTï¼Œä¸€ä¸ªåŸºäº Knowledge Graph çš„æ™ºèƒ½æ²»ç–—åŠ©æ‰‹ï¼Œæ—¨åœ¨è§£å†³æ€¥æ•‘äººå‘˜åœ¨æ—¶é—´ç´§è¿«çš„æ•‘æ´ä»»åŠ¡ä¸­éš¾ä»¥å¿«é€Ÿè·å–å’Œå¤„ç†å¤æ‚åŒ»å­¦çŸ¥è¯†çš„æŒ‘æˆ˜ã€‚éšç€äººå£ç»Ÿè®¡å­¦å˜åŒ–å¯¼è‡´ç´§æ€¥å‘¼æ•‘å¢åŠ ï¼Œç¬¬ä¸€å“åº”è€…æ€¥éœ€åœ¨ç°åœºæ ¹æ®å®æ—¶é‡‡é›†çš„ç”Ÿå‘½ä½“å¾æ•°æ®æä¾›ä¸ªæ€§åŒ–çš„åŒ»ç–—æ•‘åŠ©ã€‚KIRETT é€šè¿‡å»ºç«‹é›†ä¸­çš„çŸ¥è¯†è¡¨ç¤ºç³»ç»Ÿï¼Œå®ç°äº†åˆ›æ–°çš„çŸ¥è¯†ç®¡ç†ï¼Œå¹¶åˆ©ç”¨åŸºäº Artificial Intelligence çš„æƒ…å¢ƒé¢„è¯†åˆ«åŠŸèƒ½ç”Ÿæˆæ™ºèƒ½åŒ–çš„æ²»ç–—å»ºè®®ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå¯¹ç°åœºè·å–çš„æ•°æ®è¿›è¡Œå®æ—¶è®¡ç®—ã€è¯„ä¼°å’Œå¤„ç†ï¼Œä»è€Œè¾…åŠ©åŒ»ç–—ä¸“å®¶å’Œæ€¥æ•‘äººå‘˜åšå‡ºæ›´ä¼˜çš„å†³ç­–ã€‚è¿™ç§é›†æˆåŒ–çŸ¥è¯†ç®¡ç†æ–¹æ¡ˆæ˜¾è‘—æå‡äº†æ•‘æ´æ“ä½œçš„æ™ºèƒ½åŒ–æ°´å¹³ï¼Œä¸ºæ”¹å–„æ€¥æ•‘æ²»ç–—æ•ˆæœæä¾›äº†å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.AI",
      "comment": "LWDA'23, KIRETT project, University of Siegen, Germany",
      "pdf_url": "https://arxiv.org/pdf/2508.07834v4",
      "published_date": "2025-08-11 10:39:15 UTC",
      "updated_date": "2025-09-08 11:24:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:03:09.063813+00:00"
    },
    {
      "arxiv_id": "2508.07829v1",
      "title": "Auditory Intelligence: Understanding the World Through Sound",
      "title_zh": "å¬è§‰æ™ºèƒ½ï¼šé€è¿‡å£°éŸ³ç†è§£ä¸–ç•Œ",
      "authors": [
        "Hyeonuk Nam"
      ],
      "abstract": "Recent progress in auditory intelligence has yielded high-performing systems for sound event detection (SED), acoustic scene classification (ASC), automated audio captioning (AAC), and audio question answering (AQA). Yet these tasks remain largely constrained to surface-level recognition-capturing what happened but not why, what it implies, or how it unfolds in context. I propose a conceptual reframing of auditory intelligence as a layered, situated process that encompasses perception, reasoning, and interaction. To instantiate this view, I introduce four cognitively inspired task paradigms-ASPIRE, SODA, AUX, and AUGMENT-those structure auditory understanding across time-frequency pattern captioning, hierarchical event/scene description, causal explanation, and goal-driven interpretation, respectively. Together, these paradigms provide a roadmap toward more generalizable, explainable, and human-aligned auditory intelligence, and are intended to catalyze a broader discussion of what it means for machines to understand sound.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰å£°éŸ³äº‹ä»¶æ£€æµ‹(SED)ã€å£°å­¦åœºæ™¯åˆ†ç±»(ASC)ã€è‡ªåŠ¨éŸ³é¢‘æè¿°(AAC)å’ŒéŸ³é¢‘é—®ç­”(AQA)ç­‰å¬è§‰æ™ºèƒ½ä»»åŠ¡å±€é™äºè¡¨å±‚è¯†åˆ«çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªå°†å¬è§‰æ™ºèƒ½é‡æ„ä¸ºæ¶µç›–æ„ŸçŸ¥(Perception)ã€æ¨ç†(Reasoning)å’Œäº¤äº’(Interaction)çš„åˆ†å±‚æƒ…å¢ƒåŒ–è¿‡ç¨‹ã€‚ä¸ºäº†å®ç°è¿™ä¸€æ„æƒ³ï¼Œä½œè€…å¼•å…¥äº†ASPIREã€SODAã€AUXå’ŒAUGMENTå››ä¸ªå—è®¤çŸ¥å¯å‘çš„ä»»åŠ¡èŒƒå¼ï¼Œåˆ†åˆ«ä»æ—¶é¢‘æ¨¡å¼æè¿°ã€å±‚çº§åŒ–åœºæ™¯æè¿°ã€å› æœè§£é‡Šå’Œç›®æ ‡é©±åŠ¨è§£é‡Šå››ä¸ªç»´åº¦å¯¹å¬è§‰ç†è§£è¿›è¡Œç»“æ„åŒ–å¤„ç†ã€‚è¿™äº›èŒƒå¼å…±åŒä¸ºæ„å»ºæ›´å…·æ³›åŒ–æ€§ã€å¯è§£é‡Šæ€§ä¸”ç¬¦åˆäººç±»è®¤çŸ¥çš„å¬è§‰æ™ºèƒ½æä¾›äº†æ˜ç¡®çš„æ¼”è¿›è·¯çº¿å›¾ã€‚è¯¥ç ”ç©¶æ—¨åœ¨å‚¬åŒ–å…³äºæœºå™¨ç†è§£å£°éŸ³æœ¬è´¨çš„å¹¿æ³›è®¨è®ºï¼Œæ¨åŠ¨æŠ€æœ¯ä»å•çº¯è¯†åˆ«â€œå‘ç”Ÿäº†ä»€ä¹ˆâ€è¿›åŒ–åˆ°æ·±åº¦ç†è§£å£°éŸ³èƒŒåçš„å› æœé€»è¾‘ä¸æƒ…å¢ƒå«ä¹‰ã€‚é€šè¿‡è¿™ç§æ¦‚å¿µæ€§çš„èŒƒå¼é‡æ„ï¼Œå¬è§‰æ™ºèƒ½å°†èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ¨¡æ‹Ÿäººç±»å¯¹å£°éŸ³ä¸–ç•Œçš„æ·±åº¦è®¤çŸ¥ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Position paper without experimental/quantitative validation. Not submitted to any journal/conference",
      "pdf_url": "https://arxiv.org/pdf/2508.07829v1",
      "published_date": "2025-08-11 10:25:58 UTC",
      "updated_date": "2025-08-11 10:25:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:03:27.260689+00:00"
    },
    {
      "arxiv_id": "2508.07819v5",
      "title": "ACD-CLIP: Decoupling Representation and Dynamic Fusion for Zero-Shot Anomaly Detection",
      "title_zh": "ACD-CLIPï¼šé¢å‘é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹çš„è¡¨ç¤ºè§£è€¦ä¸åŠ¨æ€èåˆ",
      "authors": [
        "Ke Ma",
        "Jun Long",
        "Hongxiao Fei",
        "Liujie Hua",
        "Zhen Dai",
        "Yueyi Luo"
      ],
      "abstract": "Pre-trained Vision-Language Models (VLMs) struggle with Zero-Shot Anomaly Detection (ZSAD) due to a critical adaptation gap: they lack the local inductive biases required for dense prediction and employ inflexible feature fusion paradigms. We address these limitations through an Architectural Co-Design framework that jointly refines feature representation and cross-modal fusion. Our method proposes a parameter-efficient Convolutional Low-Rank Adaptation (Conv-LoRA) adapter to inject local inductive biases for fine-grained representation, and introduces a Dynamic Fusion Gateway (DFG) that leverages visual context to adaptively modulate text prompts, enabling a powerful bidirectional fusion. Extensive experiments on diverse industrial and medical benchmarks demonstrate superior accuracy and robustness, validating that this synergistic co-design is critical for robustly adapting foundation models to dense perception tasks. The source code is available at https://github.com/cockmake/ACD-CLIP.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ACD-CLIPï¼Œä¸€ç§é’ˆå¯¹é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹(Zero-Shot Anomaly Detection, ZSAD)çš„æ¶æ„ååŒè®¾è®¡æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨ç¨ å¯†é¢„æµ‹ä¸­ç¼ºä¹å±€éƒ¨å½’çº³åç½®ä»¥åŠç‰¹å¾èåˆæ¨¡å¼åƒµåŒ–çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†å‚æ•°é«˜æ•ˆçš„å·ç§¯ä½ç§©è‡ªé€‚åº”(Conv-LoRA)é€‚é…å™¨ï¼Œé€šè¿‡æ³¨å…¥å±€éƒ¨å½’çº³åç½®æ¥å¼ºåŒ–ç»†ç²’åº¦ç‰¹å¾è¡¨ç¤ºã€‚åŒæ—¶ï¼Œç ”ç©¶è®¾è®¡äº†åŠ¨æ€èåˆç½‘å…³(Dynamic Fusion Gateway, DFG)ï¼Œåˆ©ç”¨è§†è§‰ä¸Šä¸‹æ–‡è‡ªé€‚åº”åœ°è°ƒåˆ¶æ–‡æœ¬æç¤ºï¼Œå®ç°äº†é«˜æ•ˆçš„åŒå‘è·¨æ¨¡æ€èåˆã€‚åœ¨å¤šé¡¹å·¥ä¸šå’ŒåŒ»ç–—åŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒACD-CLIPåœ¨å‡†ç¡®æ€§å’Œé²æ£’æ€§ä¸Šå‡å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚è¿™ä¸€ç ”ç©¶è¯å®äº†é€šè¿‡ååŒä¼˜åŒ–è¡¨ç¤ºä¸èåˆæœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ¨åŠ¨åŸºç¡€æ¨¡å‹åœ¨å¤æ‚æ„ŸçŸ¥ä»»åŠ¡ä¸­çš„é€‚é…ä¸åº”ç”¨ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "4 pages, 1 reference, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.07819v5",
      "published_date": "2025-08-11 10:03:45 UTC",
      "updated_date": "2025-10-10 09:58:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:03:38.697705+00:00"
    },
    {
      "arxiv_id": "2508.07817v2",
      "title": "MIND: A Noise-Adaptive Denoising Framework for Medical Images Integrating Multi-Scale Transformer",
      "title_zh": "MINDï¼šé›†æˆå¤šå°ºåº¦ Transformer çš„åŒ»å­¦å›¾åƒå™ªå£°è‡ªé€‚åº”å»å™ªæ¡†æ¶",
      "authors": [
        "Tao Tang",
        "Chengxu Yang"
      ],
      "abstract": "The core role of medical images in disease diagnosis makes their quality directly affect the accuracy of clinical judgment. However, due to factors such as low-dose scanning, equipment limitations and imaging artifacts, medical images are often accompanied by non-uniform noise interference, which seriously affects structure recognition and lesion detection. This paper proposes a medical image adaptive denoising model (MI-ND) that integrates multi-scale convolutional and Transformer architecture, introduces a noise level estimator (NLE) and a noise adaptive attention module (NAAB), and realizes channel-spatial attention regulation and cross-modal feature fusion driven by noise perception. Systematic testing is carried out on multimodal public datasets. Experiments show that this method significantly outperforms the comparative methods in image quality indicators such as PSNR, SSIM, and LPIPS, and improves the F1 score and ROC-AUC in downstream diagnostic tasks, showing strong prac-tical value and promotional potential. The model has outstanding benefits in structural recovery, diagnostic sensitivity, and cross-modal robustness, and provides an effective solution for medical image enhancement and AI-assisted diagnosis and treatment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºMI-NDçš„åŒ»ç–—å½±åƒè‡ªé€‚åº”å»å™ªæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä½å‰‚é‡æ‰«æå’Œè®¾å¤‡é™åˆ¶å¯¼è‡´çš„éå‡åŒ€å™ªå£°å¯¹ä¸´åºŠåˆ¤æ–­çš„å½±å“ã€‚è¯¥æ¡†æ¶æ•´åˆäº†å¤šå°ºåº¦å·ç§¯ä¸Transformeræ¶æ„ï¼Œå¹¶å¼•å…¥äº†å™ªå£°æ°´å¹³ä¼°è®¡å™¨(NLE)å’Œå™ªå£°è‡ªé€‚åº”æ³¨æ„åŠ›æ¨¡å—(NAAB)ï¼Œå®ç°äº†ç”±å™ªå£°æ„ŸçŸ¥é©±åŠ¨çš„é€šé“-ç©ºé—´æ³¨æ„åŠ›è°ƒèŠ‚ä¸è·¨æ¨¡æ€ç‰¹å¾èåˆã€‚åœ¨å¤šæ¨¡æ€å…¬å¼€æ•°æ®é›†ä¸Šçš„ç³»ç»Ÿæµ‹è¯•è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨PSNRã€SSIMå’ŒLPIPSç­‰å½±åƒè´¨é‡æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºå¯¹æ¯”æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒMI-NDè¿˜æå‡äº†ä¸‹æ¸¸è¯Šæ–­ä»»åŠ¡ä¸­çš„F1 scoreå’ŒROC-AUCï¼Œåœ¨ç»“æ„æ¢å¤ã€è¯Šæ–­æ•æ„Ÿæ€§å’Œè·¨æ¨¡æ€é²æ£’æ€§æ–¹é¢è¡¨ç°å“è¶Šã€‚è¯¥æ¨¡å‹ä¸ºåŒ»ç–—å½±åƒå¢å¼ºåŠAIè¾…åŠ©è¯Šç–—æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰æé«˜çš„å®é™…ä»·å€¼å’Œæ¨å¹¿æ½œåŠ›ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "eess.IV",
      "comment": "Accepted by the 7th International Conference on Intelligent Control, Measurement and Signal Processing (ICMSP 2025). 6 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.07817v2",
      "published_date": "2025-08-11 10:00:51 UTC",
      "updated_date": "2025-08-13 16:44:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:03:21.990993+00:00"
    },
    {
      "arxiv_id": "2508.14066v1",
      "title": "Retrieval-Augmented Generation in Industry: An Interview Study on Use Cases, Requirements, Challenges, and Evaluation",
      "title_zh": "æ£€ç´¢å¢å¼ºç”Ÿæˆåœ¨å·¥ä¸šç•Œçš„åº”ç”¨ï¼šå…³äºä½¿ç”¨åœºæ™¯ã€éœ€æ±‚ã€æŒ‘æˆ˜ä¸è¯„ä¼°çš„è®¿è°ˆç ”ç©¶",
      "authors": [
        "Lorenz Brehme",
        "Benedikt Dornauer",
        "Thomas StrÃ¶hle",
        "Maximilian Ehrhart",
        "Ruth Breu"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) is a well-established and rapidly evolving field within AI that enhances the outputs of large language models by integrating relevant information retrieved from external knowledge sources. While industry adoption of RAG is now beginning, there is a significant lack of research on its practical application in industrial contexts. To address this gap, we conducted a semistructured interview study with 13 industry practitioners to explore the current state of RAG adoption in real-world settings. Our study investigates how companies apply RAG in practice, providing (1) an overview of industry use cases, (2) a consolidated list of system requirements, (3) key challenges and lessons learned from practical experiences, and (4) an analysis of current industry evaluation methods. Our main findings show that current RAG applications are mostly limited to domain-specific QA tasks, with systems still in prototype stages; industry requirements focus primarily on data protection, security, and quality, while issues such as ethics, bias, and scalability receive less attention; data preprocessing remains a key challenge, and system evaluation is predominantly conducted by humans rather than automated methods.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation, RAG)åœ¨å·¥ä¸šç•Œçš„å®é™…åº”ç”¨ç°çŠ¶ï¼Œæ—¨åœ¨å¡«è¡¥è¯¥æŠ€æœ¯åœ¨å·¥ä¸šèƒŒæ™¯ä¸‹ç¼ºä¹å®è¯ç ”ç©¶çš„ç©ºç™½ã€‚ä½œè€…é€šè¿‡å¯¹13ä½å·¥ä¸šä»ä¸šè€…è¿›è¡ŒåŠç»“æ„åŒ–è®¿è°ˆ(semistructured interview study)ï¼Œæ·±å…¥è°ƒç ”äº†RAGçš„ä½¿ç”¨æ¡ˆä¾‹(use cases)ã€ç³»ç»Ÿéœ€æ±‚ã€å…³é”®æŒ‘æˆ˜åŠè¯„ä¼°æ–¹æ³•ã€‚ç ”ç©¶å‘ç°ï¼Œç›®å‰å·¥ä¸šç•Œçš„RAGåº”ç”¨å¤§å¤šä»å¤„äºåŸå‹é˜¶æ®µï¼Œä¸”ä¸»è¦å±€é™äºç‰¹å®šé¢†åŸŸé—®ç­”(domain-specific QA)ä»»åŠ¡ã€‚åœ¨ç³»ç»Ÿéœ€æ±‚æ–¹é¢ï¼Œä»ä¸šè€…ä¼˜å…ˆè€ƒè™‘æ•°æ®ä¿æŠ¤(data protection)ã€å®‰å…¨æ€§å’Œè´¨é‡ï¼Œè€Œå¯¹ä¼¦ç†ã€åå·®åŠå¯æ‰©å±•æ€§çš„å…³æ³¨ç›¸å¯¹è¾ƒå°‘ã€‚æ­¤å¤–ï¼Œæ•°æ®é¢„å¤„ç†(data preprocessing)è¢«ç¡®å®šä¸ºè½åœ°è¿‡ç¨‹ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œä¸”ç³»ç»Ÿè¯„ä¼°ç›®å‰ä»ä¸»è¦ä¾èµ–äººå·¥è€Œéè‡ªåŠ¨åŒ–æ‰‹æ®µã€‚è¯¥ç ”ç©¶ä¸ºRAGæŠ€æœ¯åœ¨çœŸå®å·¥ä¸šç¯å¢ƒä¸­çš„éƒ¨ç½²æä¾›äº†ç³»ç»Ÿçš„å®è·µæ´å¯Ÿä¸ç»éªŒæ•™è®­ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "This preprint was accepted for presentation at the 17th International Conference on Knowledge Discovery and Information Retrieval (KDIR25)",
      "pdf_url": "https://arxiv.org/pdf/2508.14066v1",
      "published_date": "2025-08-11 09:40:54 UTC",
      "updated_date": "2025-08-11 09:40:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:03:25.686078+00:00"
    },
    {
      "arxiv_id": "2508.07790v2",
      "title": "Best-Effort Policies for Robust Markov Decision Processes",
      "title_zh": "é²æ£’é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹çš„å°½åŠ›è€Œä¸ºç­–ç•¥",
      "authors": [
        "Alessandro Abate",
        "Thom Badings",
        "Giuseppe De Giacomo",
        "Francesco Fabiano"
      ],
      "abstract": "We study the common generalization of Markov decision processes (MDPs) with sets of transition probabilities, known as robust MDPs (RMDPs). A standard goal in RMDPs is to compute a policy that maximizes the expected return under an adversarial choice of the transition probabilities. If the uncertainty in the probabilities is independent between the states, known as s-rectangularity, such optimal robust policies can be computed efficiently using robust value iteration. However, there might still be multiple optimal robust policies, which, while equivalent with respect to the worst-case, reflect different expected returns under non-adversarial choices of the transition probabilities. Hence, we propose a refined policy selection criterion for RMDPs, drawing inspiration from the notions of dominance and best-effort in game theory. Instead of seeking a policy that only maximizes the worst-case expected return, we additionally require the policy to achieve a maximal expected return under different (i.e., not fully adversarial) transition probabilities. We call such a policy an optimal robust best-effort (ORBE) policy. We prove that ORBE policies always exist, characterize their structure, and present an algorithm to compute them with a manageable overhead compared to standard robust value iteration. ORBE policies offer a principled tie-breaker among optimal robust policies. Numerical experiments show the feasibility of our approach.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é²æ£’é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(Robust MDPs, RMDPs)ä¸­çš„ç­–ç•¥é€‰æ‹©é—®é¢˜ï¼Œé’ˆå¯¹æ ‡å‡†æ–¹æ³•åœ¨éå¯¹æŠ—æ€§ç¯å¢ƒä¸‹è¡¨ç°ä¸ä¸€çš„å±€é™ï¼Œæå‡ºäº†ä¸€ç§ç²¾ç‚¼çš„é€‰æ‹©å‡†åˆ™ã€‚ä½œè€…ä»åšå¼ˆè®ºä¸­çš„ä¼˜åŠ¿(Dominance)å’Œå°½åŠ›è€Œä¸º(Best-effort)æ¦‚å¿µå‡ºå‘ï¼Œå®šä¹‰äº†æœ€ä¼˜é²æ£’å°½åŠ›è€Œä¸º(Optimal Robust Best-Effort, ORBE)ç­–ç•¥ã€‚è¯¥ç­–ç•¥åœ¨ä¿è¯æœ€åæƒ…å†µæœŸæœ›æ”¶ç›Šæœ€å¤§åŒ–çš„åŒæ—¶ï¼ŒåŠ›æ±‚åœ¨éå®Œå…¨å¯¹æŠ—çš„è½¬ç§»æ¦‚ç‡ä¸‹ä¹Ÿèƒ½è·å¾—æœ€ä¼˜è¡¨ç°ã€‚ç ”ç©¶è¯æ˜äº†ORBEç­–ç•¥çš„å§‹ç»ˆå­˜åœ¨ï¼Œå¹¶è¯¦ç»†åˆ»ç”»äº†å…¶ç»“æ„ç‰¹å¾ï¼ŒåŒæ—¶æå‡ºäº†ä¸€ç§è®¡ç®—å¼€é”€è¾ƒä½çš„æ”¹è¿›ç®—æ³•ã€‚ä½œä¸ºä¸€ç§åŸåˆ™æ€§çš„ç­–ç•¥ä¼˜é€‰æ–¹æ¡ˆï¼ŒORBEç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆè§£å†³æœ€ä¼˜é²æ£’ç­–ç•¥çš„å¹³å±€é—®é¢˜ï¼Œæ•°å€¼å®éªŒä¹Ÿè¯å®äº†è¯¥æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07790v2",
      "published_date": "2025-08-11 09:18:34 UTC",
      "updated_date": "2025-11-19 15:48:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:03:59.797139+00:00"
    },
    {
      "arxiv_id": "2508.07773v1",
      "title": "PCA-Guided Autoencoding for Structured Dimensionality Reduction in Active Infrared Thermography",
      "title_zh": "ä¸»åŠ¨çº¢å¤–çƒ­æˆåƒä¸­åŸºäºPCAå¼•å¯¼è‡ªåŠ¨ç¼–ç çš„ç»“æ„åŒ–é™ç»´",
      "authors": [
        "Mohammed Salah",
        "Numan Saeed",
        "Davor Svetinovic",
        "Stefano Sfarra",
        "Mohammed Omar",
        "Yusra Abdulrahman"
      ],
      "abstract": "Active Infrared thermography (AIRT) is a widely adopted non-destructive testing (NDT) technique for detecting subsurface anomalies in industrial components. Due to the high dimensionality of AIRT data, current approaches employ non-linear autoencoders (AEs) for dimensionality reduction. However, the latent space learned by AIRT AEs lacks structure, limiting their effectiveness in downstream defect characterization tasks. To address this limitation, this paper proposes a principal component analysis guided (PCA-guided) autoencoding framework for structured dimensionality reduction to capture intricate, non-linear features in thermographic signals while enforcing a structured latent space. A novel loss function, PCA distillation loss, is introduced to guide AIRT AEs to align the latent representation with structured PCA components while capturing the intricate, non-linear patterns in thermographic signals. To evaluate the utility of the learned, structured latent space, we propose a neural network-based evaluation metric that assesses its suitability for defect characterization. Experimental results show that the proposed PCA-guided AE outperforms state-of-the-art dimensionality reduction methods on PVC, CFRP, and PLA samples in terms of contrast, signal-to-noise ratio (SNR), and neural network-based metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸»åŠ¨çº¢å¤–çƒ­æˆåƒ (Active Infrared thermography, AIRT) æ•°æ®é™ç»´ä¸­éçº¿æ€§è‡ªç¼–ç å™¨ (autoencoders, AEs) æ½œç©ºé—´ç¼ºä¹ç»“æ„æ€§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ PCA å¼•å¯¼çš„è‡ªç¼–ç æ¡†æ¶ä»¥å®ç°ç»“æ„åŒ–é™ç»´ã€‚é€šè¿‡å¼•å…¥åˆ›æ–°çš„ PCA è’¸é¦æŸå¤± (PCA distillation loss)ï¼Œè¯¥æ¡†æ¶åœ¨æ•æ‰çƒ­æˆåƒä¿¡å·ä¸­å¤æ‚çš„éçº¿æ€§ç‰¹å¾çš„åŒæ—¶ï¼Œèƒ½å¤Ÿå¼•å¯¼ AEs å°†æ½œå±‚è¡¨ç¤ºä¸ç»“æ„åŒ–çš„ PCA ç»„ä»¶å¯¹é½ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§åŸºäºç¥ç»ç½‘ç»œçš„è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–åˆ†ææ‰€æå–çš„æ½œç©ºé—´åœ¨ç¼ºé™·è¡¨å¾ä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ PCA-guided AE åœ¨ PVCã€CFRP å’Œ PLA ç­‰å¤šç§æè´¨æ ·æœ¬çš„æ£€æµ‹ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œåœ¨å¯¹æ¯”åº¦ã€ä¿¡å™ªæ¯” (SNR) åŠç¥ç»ç½‘ç»œè¯„ä»·æŒ‡æ ‡ä¸Šå‡è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "Infrared thermography, Non-Destructive Testing, Principal Component Analysis, PCA-Guided Autoencoder, PCA Distillation Loss, Dimensionality Reduction",
      "pdf_url": "https://arxiv.org/pdf/2508.07773v1",
      "published_date": "2025-08-11 08:58:13 UTC",
      "updated_date": "2025-08-11 08:58:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:04:00.781915+00:00"
    },
    {
      "arxiv_id": "2508.07768v1",
      "title": "Pareto Multi-Objective Alignment for Language Models",
      "title_zh": "è¯­è¨€æ¨¡å‹çš„ Pareto å¤šç›®æ ‡å¯¹é½",
      "authors": [
        "Qiang He",
        "Setareh Maghsudi"
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed in real-world applications that require careful balancing of multiple, often conflicting, objectives, such as informativeness versus conciseness, or helpfulness versus creativity. However, current alignment methods, primarily based on RLHF, optimize LLMs toward a single reward function, resulting in rigid behavior that fails to capture the complexity and diversity of human preferences. This limitation hinders the adaptability of LLMs to practical scenarios, making multi-objective alignment (MOA) a critical yet underexplored area. To bridge this gap, we propose Pareto Multi-Objective Alignment (PAMA), a principled and computationally efficient algorithm designed explicitly for MOA in LLMs. In contrast to computationally prohibitive multi-objective optimization (MOO) methods, PAMA transforms multi-objective RLHF into a convex optimization with a closed-form solution, significantly enhancing scalability. Traditional MOO approaches suffer from prohibitive O(n^2*d) complexity, where d represents the number of model parameters, typically in the billions for LLMs, rendering direct optimization infeasible. PAMA reduces this complexity to O(n) where n is the number of objectives, enabling optimization to be completed within milliseconds. We provide theoretical guarantees that PAMA converges to a Pareto stationary point, where no objective can be improved without degrading at least one other. Extensive experiments across language models ranging from 125M to 7B parameters demonstrate PAMA's robust and effective MOA capabilities, aligning with its theoretical advantages. PAMA provides a highly efficient solution to the MOA problem that was previously considered intractable, offering a practical and theoretically grounded approach to aligning LLMs with diverse human values, paving the way for versatile and adaptable real-world AI deployments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†ä¿¡æ¯é‡ä¸ç®€æ´æ€§ç­‰å†²çªç›®æ ‡æ—¶ï¼Œç°æœ‰RLHFæ–¹æ³•ä»…èƒ½ä¼˜åŒ–å•ä¸€å¥–åŠ±å‡½æ•°çš„å±€é™æ€§ï¼Œæå‡ºäº†PAMA (Pareto Multi-Objective Alignment)ç®—æ³•ã€‚PAMAå°†å¤šç›®æ ‡RLHFè½¬åŒ–ä¸ºå…·æœ‰é—­å¼è§£(closed-form solution)çš„å‡¸ä¼˜åŒ–é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†å¤šç›®æ ‡å¯¹é½(MOA)çš„æ‰©å±•æ€§ã€‚ç›¸è¾ƒäºä¼ ç»Ÿå¤šç›®æ ‡ä¼˜åŒ–æ–¹æ³•æé«˜çš„è®¡ç®—å¤æ‚åº¦ï¼ŒPAMAæˆåŠŸå°†å…¶ä»O(n^2*d)é™ä½è‡³O(n)ï¼Œä½¿å¾—é’ˆå¯¹å¤§è§„æ¨¡æ¨¡å‹çš„ä¼˜åŒ–å¯åœ¨æ¯«ç§’å†…å®Œæˆã€‚ç†è®ºè¯æ˜PAMAèƒ½å¤Ÿæ”¶æ•›è‡³å¸•ç´¯æ‰˜ç¨³å®šç‚¹(Pareto stationary point)ï¼Œç¡®ä¿åœ¨ä¸æŸå®³å…¶ä»–ç›®æ ‡çš„æƒ…å†µä¸‹å®ç°æœ€ä¼˜å¹³è¡¡ã€‚åœ¨125Mè‡³7Bå‚æ•°è§„æ¨¡çš„æ¨¡å‹å®éªŒä¸­ï¼ŒPAMAå±•ç°äº†ä¼˜å¼‚ä¸”ç¨³å¥çš„æ€§èƒ½ã€‚è¯¥æˆæœä¸ºLLMsä¸å¤šå…ƒäººç±»ä»·å€¼è§‚çš„ç²¾å‡†å¯¹é½æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å…·å¤‡ç†è®ºæ”¯æ’‘çš„å®è·µæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ECML/PKDD 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.07768v1",
      "published_date": "2025-08-11 08:54:14 UTC",
      "updated_date": "2025-08-11 08:54:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:04:11.488198+00:00"
    },
    {
      "arxiv_id": "2508.07766v1",
      "title": "UniSVG: A Unified Dataset for Vector Graphic Understanding and Generation with Multimodal Large Language Models",
      "title_zh": "UniSVGï¼šåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„çŸ¢é‡å›¾å½¢ç†è§£ä¸ç”Ÿæˆç»Ÿä¸€æ•°æ®é›†",
      "authors": [
        "Jinke Li",
        "Jiarui Yu",
        "Chenxing Wei",
        "Hande Dong",
        "Qiang Lin",
        "Liangjing Yang",
        "Zhicai Wang",
        "Yanbin Hao"
      ],
      "abstract": "Unlike bitmap images, scalable vector graphics (SVG) maintain quality when scaled, frequently employed in computer vision and artistic design in the representation of SVG code. In this era of proliferating AI-powered systems, enabling AI to understand and generate SVG has become increasingly urgent. However, AI-driven SVG understanding and generation (U&G) remain significant challenges. SVG code, equivalent to a set of curves and lines controlled by floating-point parameters, demands high precision in SVG U&G. Besides, SVG generation operates under diverse conditional constraints, including textual prompts and visual references, which requires powerful multi-modal processing for condition-to-SVG transformation. Recently, the rapid growth of Multi-modal Large Language Models (MLLMs) have demonstrated capabilities to process multi-modal inputs and generate complex vector controlling parameters, suggesting the potential to address SVG U&G tasks within a unified model. To unlock MLLM's capabilities in the SVG area, we propose an SVG-centric dataset called UniSVG, comprising 525k data items, tailored for MLLM training and evaluation. To our best knowledge, it is the first comprehensive dataset designed for unified SVG generation (from textual prompts and images) and SVG understanding (color, category, usage, etc.). As expected, learning on the proposed dataset boosts open-source MLLMs' performance on various SVG U&G tasks, surpassing SOTA close-source MLLMs like GPT-4V. We release dataset, benchmark, weights, codes and experiment details on https://ryanlijinke.github.io/.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¯ä¼¸ç¼©çŸ¢é‡å›¾å½¢(SVG)åœ¨ç†è§£ä¸ç”Ÿæˆ(U&G)é¢†åŸŸé¢ä¸´çš„é«˜ç²¾åº¦å‚æ•°æ§åˆ¶å’Œå¤šæ¨¡æ€æ¡ä»¶çº¦æŸæŒ‘æˆ˜ï¼Œæå‡ºäº†é¦–ä¸ªå¤§è§„æ¨¡ç»Ÿä¸€æ•°æ®é›†UniSVGã€‚è¯¥æ•°æ®é›†åŒ…å«52.5ä¸‡æ¡æ•°æ®é¡¹ï¼Œä¸“é—¨ç”¨äºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)çš„è®­ç»ƒä¸è¯„ä¼°ï¼Œæ¶µç›–äº†ä»æ–‡æœ¬æç¤ºå’Œå›¾åƒç”ŸæˆSVGä»¥åŠå¯¹SVGé¢œè‰²ã€ç±»åˆ«å’Œç”¨é€”çš„ç†è§£ç­‰å¤šé¡¹ä»»åŠ¡ã€‚é€šè¿‡åœ¨UniSVGä¸Šè¿›è¡Œå­¦ä¹ ï¼Œå¼€æºMLLMsåœ¨å„ç±»SVGç›¸å…³ä»»åŠ¡ä¸­çš„è¡¨ç°å¾—åˆ°äº†æ˜¾è‘—æå‡ï¼Œå…¶æ€§èƒ½ç”šè‡³è¶…è¶Šäº†GPT-4Vç­‰å…ˆè¿›çš„é—­æºSOTAæ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ä»…å¡«è¡¥äº†ç»¼åˆæ€§SVGæ•°æ®é›†çš„ç©ºç™½ï¼Œè¿˜é€šè¿‡å¼€æºæ•°æ®é›†ã€åŸºå‡†æµ‹è¯•åŠæ¨¡å‹æƒé‡ï¼Œä¸ºçŸ¢é‡å›¾å½¢æ™ºèƒ½åŒ–çš„è¿›ä¸€æ­¥å‘å±•å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at ACM MM 2025 Dataset Track",
      "pdf_url": "https://arxiv.org/pdf/2508.07766v1",
      "published_date": "2025-08-11 08:50:14 UTC",
      "updated_date": "2025-08-11 08:50:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:04:11.194293+00:00"
    },
    {
      "arxiv_id": "2508.07763v1",
      "title": "Sparse Probabilistic Graph Circuits",
      "title_zh": "ç¨€ç–æ¦‚ç‡å›¾ç”µè·¯",
      "authors": [
        "Martin Rektoris",
        "Milan PapeÅ¾",
        "VÃ¡clav Å mÃ­dl",
        "TomÃ¡Å¡ PevnÃ½"
      ],
      "abstract": "Deep generative models (DGMs) for graphs achieve impressively high expressive power thanks to very efficient and scalable neural networks. However, these networks contain non-linearities that prevent analytical computation of many standard probabilistic inference queries, i.e., these DGMs are considered \\emph{intractable}. While recently proposed Probabilistic Graph Circuits (PGCs) address this issue by enabling \\emph{tractable} probabilistic inference, they operate on dense graph representations with $\\mathcal{O}(n^2)$ complexity for graphs with $n$ nodes and \\emph{$m$ edges}. To address this scalability issue, we introduce Sparse PGCs, a new class of tractable generative models that operate directly on sparse graph representation, reducing the complexity to $\\mathcal{O}(n + m)$, which is particularly beneficial for $m \\ll n^2$. In the context of de novo drug design, we empirically demonstrate that SPGCs retain exact inference capabilities, improve memory efficiency and inference speed, and match the performance of intractable DGMs in key metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾æ·±åº¦ç”Ÿæˆæ¨¡å‹ï¼ˆDGMsï¼‰åœ¨æ¦‚ç‡æ¨ç†ä»»åŠ¡ä¸­è®¡ç®—ä¸å¯è¡Œï¼ˆintractableï¼‰çš„é—®é¢˜ï¼Œæå‡ºäº†Sparse Probabilistic Graph Circuits (SPGCs)ã€‚SPGCsä½œä¸ºä¸€ç§æ–°å‹çš„å¯å¤„ç†ï¼ˆtractableï¼‰ç”Ÿæˆæ¨¡å‹ï¼Œç›´æ¥åœ¨ç¨€ç–å›¾è¡¨ç¤ºä¸Šè¿è¡Œï¼ŒæˆåŠŸå°†è®¡ç®—å¤æ‚åº¦ä»ä¼ ç»Ÿå¯†é›†è¡¨ç¤ºçš„ $\\mathcal{O}(n^2)$ é™ä½åˆ° $\\mathcal{O}(n + m)$ã€‚åœ¨ä»å¤´è¯ç‰©è®¾è®¡ï¼ˆde novo drug designï¼‰çš„å®è¯ç ”ç©¶ä¸­ï¼ŒSPGCsåœ¨ä¿æŒç²¾ç¡®æ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†å†…å­˜æ•ˆç‡å’Œæ¨ç†é€Ÿåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å…³é”®æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†ä¸ä¸å¯å¤„ç†çš„DGMsç›¸å½“çš„æ€§èƒ½æ°´å¹³ã€‚è¿™ä¸€è¿›å±•æœ‰æ•ˆè§£å†³äº†æ¦‚ç‡å›¾ç”µè·¯ï¼ˆPGCsï¼‰åœ¨å¤§è§„æ¨¡å›¾ç»“æ„ä¸­çš„å¯æ‰©å±•æ€§ç“¶é¢ˆï¼Œä¸ºé«˜æ•ˆçš„å›¾ç”Ÿæˆå’Œæ¨ç†ä»»åŠ¡æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07763v1",
      "published_date": "2025-08-11 08:47:27 UTC",
      "updated_date": "2025-08-11 08:47:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:04:14.988723+00:00"
    },
    {
      "arxiv_id": "2508.07750v1",
      "title": "Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment",
      "title_zh": "å­¦ä»¥å¯¹é½ï¼Œå¯¹é½ä¿ƒå­¦ï¼šä¸€ç§è‡ªæˆ‘ä¼˜åŒ–å¯¹é½çš„ç»Ÿä¸€æ–¹æ³•",
      "authors": [
        "Haowen Wang",
        "Yun Yue",
        "Zhiling Ye",
        "Shuowen Zhang",
        "Lei Fan",
        "Jiaxin Liang",
        "Jiadi Jiang",
        "Cheng Wei",
        "Jingyuan Deng",
        "Xudong Han",
        "Ji Li",
        "Chunxiao Guo",
        "Peng Wei",
        "Jian Wang",
        "Jinjie Gu"
      ],
      "abstract": "Alignment methodologies have emerged as a critical pathway for enhancing language model alignment capabilities. While SFT (supervised fine-tuning) accelerates convergence through direct token-level loss intervention, its efficacy is constrained by offline policy trajectory. In contrast, RL(reinforcement learning) facilitates exploratory policy optimization, but suffers from low sample efficiency and stringent dependency on high-quality base models. To address these dual challenges, we propose GRAO (Group Relative Alignment Optimization), a unified framework that synergizes the respective strengths of SFT and RL through three key innovations: 1) A multi-sample generation strategy enabling comparative quality assessment via reward feedback; 2) A novel Group Direct Alignment Loss formulation leveraging intra-group relative advantage weighting; 3) Reference-aware parameter updates guided by pairwise preference dynamics. Our theoretical analysis establishes GRAO's convergence guarantees and sample efficiency advantages over conventional approaches. Comprehensive evaluations across complex human alignment tasks demonstrate GRAO's superior performance, achieving 57.70\\%,17.65\\% 7.95\\% and 5.18\\% relative improvements over SFT, DPO, PPO and GRPO baselines respectively. This work provides both a theoretically grounded alignment framework and empirical evidence for efficient capability evolution in language models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GRAO (Group Relative Alignment Optimization)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ååŒ SFT (Supervised Fine-Tuning) ä¸ RL (Reinforcement Learning) ä¼˜åŠ¿çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæœ‰æ•ˆè§£å†³äº† SFT çš„ç¦»çº¿ç­–ç•¥é™åˆ¶ä»¥åŠ RL æ ·æœ¬æ•ˆç‡ä½çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å¤šæ ·æœ¬ç”Ÿæˆç­–ç•¥è¿›è¡Œå¥–åŠ±åé¦ˆè¯„ä¼°ï¼Œå¹¶å¼•å…¥ä¸€ç§æ–°å‹çš„ Group Direct Alignment Lossï¼Œåˆ©ç”¨ç»„å†…ç›¸å¯¹ä¼˜åŠ¿æƒé‡å’Œå—æˆå¯¹åå¥½åŠ¨æ€å¼•å¯¼çš„ Reference-aware å‚æ•°æ›´æ–°ã€‚ç†è®ºåˆ†æè¡¨æ˜ GRAO å…·å¤‡æ”¶æ•›ä¿è¯å’Œæ˜¾è‘—çš„æ ·æœ¬æ•ˆç‡ä¼˜åŠ¿ã€‚å®éªŒè¯æ˜ï¼ŒGRAO åœ¨å¤æ‚äººç±»å¯¹é½ä»»åŠ¡ä¸­ç›¸è¾ƒäº SFTã€DPOã€PPO å’Œ GRPO åˆ†åˆ«å®ç°äº† 57.70%ã€17.65%ã€7.95% å’Œ 5.18% çš„ç›¸å¯¹æ€§èƒ½æå‡ã€‚è¯¥å·¥ä½œä¸ºå¤§è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆèƒ½åŠ›æ¼”åŒ–æä¾›äº†ç†è®ºæ”¯æ’‘ä¸å®è¯è¯æ®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 5 figures, 7 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.07750v1",
      "published_date": "2025-08-11 08:28:47 UTC",
      "updated_date": "2025-08-11 08:28:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:04:20.794003+00:00"
    },
    {
      "arxiv_id": "2508.07745v4",
      "title": "Chimera: Harnessing Multi-Agent LLMs for Automatic Insider Threat Simulation",
      "title_zh": "Chimeraï¼šåˆ©ç”¨å¤šæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹å®ç°è‡ªåŠ¨åŒ–å†…éƒ¨å¨èƒæ¨¡æ‹Ÿ",
      "authors": [
        "Jiongchi Yu",
        "Xiaofei Xie",
        "Qiang Hu",
        "Yuhan Ma",
        "Ziming Zhao"
      ],
      "abstract": "Insider threats pose a persistent and critical security risk, yet are notoriously difficult to detect in complex enterprise environments, where malicious actions are often hidden within seemingly benign user behaviors. Although machine-learning-based insider threat detection (ITD) methods have shown promise, their effectiveness is fundamentally limited by the scarcity of high-quality and realistic training data. Enterprise internal data is highly sensitive and rarely accessible, while existing public and synthetic datasets are either small-scale or lack sufficient realism, semantic richness, and behavioral diversity.\n  To address this challenge, we propose Chimera, an LLM-based multi-agent framework that automatically simulates both benign and malicious insider activities and generates comprehensive system logs across diverse enterprise environments. Chimera models each agent as an individual employee with fine-grained roles and supports group meetings, pairwise interactions, and self-organized scheduling to capture realistic organizational dynamics. Based on 15 insider attacks abstracted from real-world incidents, we deploy Chimera in three representative data-sensitive organizational scenarios and construct ChimeraLog, a new dataset for developing and evaluating ITD methods.\n  We evaluate ChimeraLog through human studies and quantitative analyses, demonstrating its diversity and realism. Experiments with existing ITD methods show substantially lower detection performance on ChimeraLog compared to prior datasets, indicating a more challenging and realistic benchmark. Moreover, despite distribution shifts, models trained on ChimeraLog exhibit strong generalization, highlighting the practical value of LLM-based multi-agent simulation for advancing insider threat detection.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å†…éƒ¨å¨èƒ(Insider threats)æ£€æµ‹ä¸­é«˜è´¨é‡çœŸå®æ•°æ®åŒ®ä¹çš„æŒ‘æˆ˜ï¼Œæå‡ºäº† Chimeraï¼Œä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨æ¨¡æ‹Ÿå¤æ‚ä¼ä¸šç¯å¢ƒä¸‹çš„è‰¯æ€§ä¸æ¶æ„å†…éƒ¨æ´»åŠ¨ã€‚è¯¥æ¡†æ¶å°†æ™ºèƒ½ä½“å»ºæ¨¡ä¸ºå…·æœ‰ç»†ç²’åº¦è§’è‰²çš„ä¸ªä½“å‘˜å·¥ï¼Œé€šè¿‡æ”¯æŒç¾¤ç»„ä¼šè®®ã€åŒäººäº¤äº’åŠè‡ªæˆ‘è°ƒåº¦æ¥è¿˜åŸçœŸå®çš„ç»„ç»‡åŠ¨æ€ã€‚åŸºäº15ç§ä»çœŸå®äº‹ä»¶ä¸­æŠ½å–çš„æ”»å‡»æ¨¡å¼ï¼Œç ”ç©¶è€…åœ¨ä¸‰ä¸ªä»£è¡¨æ€§åœºæ™¯ä¸­æ„å»ºäº† ChimeraLog æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°å†…éƒ¨å¨èƒæ£€æµ‹(ITD)æ–¹æ³•ã€‚å®šé‡åˆ†æä¸äººå·¥ç ”ç©¶è¯æ˜äº†è¯¥æ•°æ®é›†åœ¨å¤šæ ·æ€§å’ŒçœŸå®æ€§ä¸Šçš„ä¼˜åŠ¿ï¼Œä¸”å®éªŒæ˜¾ç¤ºç°æœ‰æ£€æµ‹æ–¹æ³•åœ¨ ChimeraLog ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè¯æ˜äº†å…¶ä½œä¸ºé«˜éš¾åº¦åŸºå‡†æµ‹è¯•çš„ä»·å€¼ã€‚æ­¤å¤–ï¼Œåœ¨ ChimeraLog ä¸Šè®­ç»ƒçš„æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼ŒéªŒè¯äº†åˆ©ç”¨ LLMs é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“æ¨¡æ‹ŸæŠ€æœ¯åœ¨æå‡å®‰å…¨é˜²å¾¡èƒ½åŠ›æ–¹é¢çš„å®é™…åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted by NDSS 2026",
      "pdf_url": "https://arxiv.org/pdf/2508.07745v4",
      "published_date": "2025-08-11 08:24:48 UTC",
      "updated_date": "2026-01-03 17:12:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:04:29.161225+00:00"
    },
    {
      "arxiv_id": "2508.07743v1",
      "title": "Symmetry-Aware Transformer Training for Automated Planning",
      "title_zh": "é¢å‘è‡ªåŠ¨è§„åˆ’çš„å¯¹ç§°æ„ŸçŸ¥ Transformer è®­ç»ƒ",
      "authors": [
        "Markus Fritzsche",
        "Elliot Gestrin",
        "Jendrik Seipp"
      ],
      "abstract": "While transformers excel in many settings, their application in the field of automated planning is limited. Prior work like PlanGPT, a state-of-the-art decoder-only transformer, struggles with extrapolation from easy to hard planning problems. This in turn stems from problem symmetries: planning tasks can be represented with arbitrary variable names that carry no meaning beyond being identifiers. This causes a combinatorial explosion of equivalent representations that pure transformers cannot efficiently learn from. We propose a novel contrastive learning objective to make transformers symmetry-aware and thereby compensate for their lack of inductive bias. Combining this with architectural improvements, we show that transformers can be efficiently trained for either plan-generation or heuristic-prediction. Our results across multiple planning domains demonstrate that our symmetry-aware training effectively and efficiently addresses the limitations of PlanGPT.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Transformer åœ¨è‡ªåŠ¨è§„åˆ’ (automated planning) é¢†åŸŸä¸­å¤–æ¨èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼ŒæŒ‡å‡ºå…¶æ ¹æºåœ¨äºè§„åˆ’ä»»åŠ¡çš„é—®é¢˜å¯¹ç§°æ€§ (problem symmetries) å¯¼è‡´çš„ç­‰æ•ˆè¡¨ç¤ºç»„åˆçˆ†ç‚¸ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°å‹çš„å¯¹æ¯”å­¦ä¹  (contrastive learning) ç›®æ ‡å‡½æ•°ï¼Œä½¿æ¨¡å‹å…·å¤‡å¯¹ç§°æ€§æ„ŸçŸ¥ (symmetry-aware) èƒ½åŠ›ï¼Œä»¥å¼¥è¡¥ Transformer ç¼ºä¹å½’çº³åç½® (inductive bias) çš„ç¼ºé™·ã€‚ç»“åˆæ¶æ„ä¸Šçš„ä¼˜åŒ–ï¼Œè¯¥æ–¹æ³•ä½¿ Transformer èƒ½å¤Ÿé«˜æ•ˆåœ°æ‰§è¡Œè®¡åˆ’ç”Ÿæˆ (plan-generation) å’Œå¯å‘å¼é¢„æµ‹ (heuristic-prediction) ä»»åŠ¡ã€‚åœ¨å¤šä¸ªè§„åˆ’é¢†åŸŸçš„å®éªŒç»“æœè¯æ˜ï¼Œè¿™ç§å¯¹ç§°æ€§æ„ŸçŸ¥è®­ç»ƒæ–¹æ¡ˆæœ‰æ•ˆå…‹æœäº† PlanGPT ç­‰ç°æœ‰æ¨¡å‹çš„å±€é™æ€§ï¼Œå®ç°äº†æ›´é«˜æ•ˆä¸”å…·æ³›åŒ–æ€§çš„è§„åˆ’æ¨ç†ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07743v1",
      "published_date": "2025-08-11 08:23:34 UTC",
      "updated_date": "2025-08-11 08:23:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:04:25.686716+00:00"
    },
    {
      "arxiv_id": "2508.07742v2",
      "title": "A Rule-Based Approach to Specifying Preferences over Conflicting Facts and Querying Inconsistent Knowledge Bases",
      "title_zh": "ä¸€ç§åŸºäºè§„åˆ™çš„å†²çªäº‹å®åå¥½è®¾å®šä¸ä¸ä¸€è‡´çŸ¥è¯†åº“æŸ¥è¯¢æ–¹æ³•",
      "authors": [
        "Meghyn Bienvenu",
        "Camille Bourgaux",
        "Katsumi Inoue",
        "Robin Jean"
      ],
      "abstract": "Repair-based semantics have been extensively studied as a means of obtaining meaningful answers to queries posed over inconsistent knowledge bases (KBs). While several works have considered how to exploit a priority relation between facts to select optimal repairs, the question of how to specify such preferences remains largely unaddressed. This motivates us to introduce a declarative rule-based framework for specifying and computing a priority relation between conflicting facts. As the expressed preferences may contain undesirable cycles, we consider the problem of determining when a set of preference rules always yields an acyclic relation, and we also explore a pragmatic approach that extracts an acyclic relation by applying various cycle removal techniques. Towards an end-to-end system for querying inconsistent KBs, we present a preliminary implementation and experimental evaluation of the framework, which employs answer set programming to evaluate the preference rules, apply the desired cycle resolution techniques to obtain a priority relation, and answer queries under prioritized-repair semantics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºè§„åˆ™çš„å£°æ˜å¼æ¡†æ¶ï¼Œç”¨äºåœ¨ä¸ä¸€è‡´çš„çŸ¥è¯†åº“ï¼ˆInconsistent Knowledge Basesï¼‰ä¸­æŒ‡å®šå’Œè®¡ç®—å†²çªäº‹å®ä¹‹é—´çš„ä¼˜å…ˆçº§å…³ç³»ã€‚é’ˆå¯¹ä¿®å¤è¯­ä¹‰ï¼ˆRepair-based semanticsï¼‰ä¸­ä¼˜å…ˆçº§æŒ‡å®šæ–¹å¼ä¸æ˜ç¡®çš„ç°çŠ¶ï¼Œè¯¥æ¡†æ¶å…è®¸é€šè¿‡è§„åˆ™å®šä¹‰äº‹å®é—´çš„åå¥½ã€‚è€ƒè™‘åˆ°åå¥½è®¾å®šä¸­å¯èƒ½å‡ºç°çš„å¾ªç¯å†²çªï¼Œä½œè€…ç ”ç©¶äº†ä¿è¯åå¥½è§„åˆ™é›†äº§ç”Ÿæ— ç¯å…³ç³»çš„æ¡ä»¶ï¼Œå¹¶é‡‡ç”¨äº†å¤šç§å¾ªç¯æ¶ˆé™¤æŠ€æœ¯ï¼ˆCycle removal techniquesï¼‰æ¥æå–æœ‰æ•ˆçš„ä¼˜å…ˆçº§ã€‚ä¸ºäº†å®ç°ç«¯åˆ°ç«¯çš„æŸ¥è¯¢ç³»ç»Ÿï¼Œç ”ç©¶åˆ©ç”¨ç­”æ¡ˆé›†ç¼–ç¨‹ï¼ˆAnswer Set Programming, ASPï¼‰æ¥è¯„ä¼°è§„åˆ™å¹¶è§£æå¾ªç¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½åœ¨ä¼˜å…ˆçº§ä¿®å¤è¯­ä¹‰ï¼ˆPrioritized-repair semanticsï¼‰ä¸‹æœ‰æ•ˆå›ç­”æŸ¥è¯¢ï¼Œä¸ºå¤„ç†ä¸ä¸€è‡´çŸ¥è¯†åº“æä¾›äº†ç³»ç»ŸåŒ–çš„åå¥½ç®¡ç†æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.LO",
      "comment": "This is an extended version of a paper appearing at the 22nd International Conference on Principles of Knowledge Representation and Reasoning (KR 2025). 24 pages. This version corrects Definition 4",
      "pdf_url": "https://arxiv.org/pdf/2508.07742v2",
      "published_date": "2025-08-11 08:21:02 UTC",
      "updated_date": "2025-11-24 06:49:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:04:31.863415+00:00"
    },
    {
      "arxiv_id": "2508.07731v1",
      "title": "CognitiveArm: Enabling Real-Time EEG-Controlled Prosthetic Arm Using Embodied Machine Learning",
      "title_zh": "CognitiveArmï¼šåŸºäºå…·èº«æœºå™¨å­¦ä¹ çš„å®æ—¶è„‘ç”µæ§åˆ¶ä¹‰è‚¢æ‰‹è‡‚",
      "authors": [
        "Abdul Basit",
        "Maha Nawaz",
        "Saim Rehman",
        "Muhammad Shafique"
      ],
      "abstract": "Efficient control of prosthetic limbs via non-invasive brain-computer interfaces (BCIs) requires advanced EEG processing, including pre-filtering, feature extraction, and action prediction, performed in real time on edge AI hardware. Achieving this on resource-constrained devices presents challenges in balancing model complexity, computational efficiency, and latency. We present CognitiveArm, an EEG-driven, brain-controlled prosthetic system implemented on embedded AI hardware, achieving real-time operation without compromising accuracy. The system integrates BrainFlow, an open-source library for EEG data acquisition and streaming, with optimized deep learning (DL) models for precise brain signal classification. Using evolutionary search, we identify Pareto-optimal DL configurations through hyperparameter tuning, optimizer analysis, and window selection, analyzed individually and in ensemble configurations. We apply model compression techniques such as pruning and quantization to optimize models for embedded deployment, balancing efficiency and accuracy. We collected an EEG dataset and designed an annotation pipeline enabling precise labeling of brain signals corresponding to specific intended actions, forming the basis for training our optimized DL models. CognitiveArm also supports voice commands for seamless mode switching, enabling control of the prosthetic arm's 3 degrees of freedom (DoF). Running entirely on embedded hardware, it ensures low latency and real-time responsiveness. A full-scale prototype, interfaced with the OpenBCI UltraCortex Mark IV EEG headset, achieved up to 90% accuracy in classifying three core actions (left, right, idle). Voice integration enables multiplexed, variable movement for everyday tasks (e.g., handshake, cup picking), enhancing real-world performance and demonstrating CognitiveArm's potential for advanced prosthetic control.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†CognitiveArmï¼Œä¸€ä¸ªåœ¨åµŒå…¥å¼Edge AIç¡¬ä»¶ä¸Šå®ç°çš„å®æ—¶EEGé©±åŠ¨è„‘æ§ä¹‰è‚¢ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³éä¾µå…¥å¼è„‘æœºæ¥å£(BCIs)åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šçš„è®¡ç®—æ•ˆç‡ä¸å»¶è¿Ÿå¹³è¡¡æŒ‘æˆ˜ã€‚è¯¥ç³»ç»Ÿé›†æˆäº†BrainFlowåº“è¿›è¡Œæ•°æ®è·å–ï¼Œå¹¶åˆ©ç”¨æ¼”åŒ–æœç´¢(Evolutionary Search)ç¡®å®šäº†Pareto-optimalçš„æ·±åº¦å­¦ä¹ (DL)æ¨¡å‹é…ç½®ï¼Œé€šè¿‡è¶…å‚æ•°è°ƒä¼˜å’Œé›†æˆåˆ†ææå‡åˆ†ç±»ç²¾åº¦ã€‚ä¸ºäº†é€‚åº”åµŒå…¥å¼éƒ¨ç½²ï¼Œç ”ç©¶é‡‡ç”¨äº†å‰ªæ(Pruning)å’Œé‡åŒ–(Quantization)ç­‰æ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼Œåœ¨ä¿è¯å‡†ç¡®ç‡çš„åŒæ—¶å®ç°äº†æä½çš„æ¨ç†å»¶è¿Ÿã€‚æ­¤å¤–ï¼Œå›¢é˜Ÿæ„å»ºäº†ä¸“é—¨çš„EEGæ•°æ®é›†å’Œæ ‡æ³¨æµæ°´çº¿ï¼Œå¹¶æ•´åˆäº†è¯­éŸ³å‘½ä»¤(Voice Commands)åŠŸèƒ½ï¼Œæ”¯æŒä¹‰è‚¢åœ¨3ä¸ªè‡ªç”±åº¦(DoF)ä¸‹çš„æ— ç¼æ¨¡å¼åˆ‡æ¢ã€‚å®éªŒåŸå‹ç»“åˆOpenBCI UltraCortex Mark IVè€³æœºï¼Œåœ¨æ ¸å¿ƒåŠ¨ä½œåˆ†ç±»ä¸Šè¾¾åˆ°äº†90%çš„å‡†ç¡®ç‡ï¼Œå¹¶æˆåŠŸæ¼”ç¤ºäº†æ¡æ‰‹å’ŒæŠ“å–æ¯å­ç­‰å¤æ‚æ—¥å¸¸ä»»åŠ¡ã€‚è¯¥æˆæœè¯æ˜äº†CognitiveArmåœ¨å®ç°é«˜æ•ˆã€å®æ—¶çš„è‡ªä¸»ä¹‰è‚¢æ§åˆ¶æ–¹é¢çš„å·¨å¤§åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "7 pages, 12 figures, Accepted to 62nd DAC 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.07731v1",
      "published_date": "2025-08-11 08:04:59 UTC",
      "updated_date": "2025-08-11 08:04:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:04:39.349155+00:00"
    },
    {
      "arxiv_id": "2508.07714v1",
      "title": "DoorDet: Semi-Automated Multi-Class Door Detection Dataset via Object Detection and Large Language Models",
      "title_zh": "DoorDetï¼šåŸºäºç›®æ ‡æ£€æµ‹ä¸å¤§è¯­è¨€æ¨¡å‹çš„åŠè‡ªåŠ¨å¤šç±»åˆ«é—¨æ£€æµ‹æ•°æ®é›†",
      "authors": [
        "Licheng Zhang",
        "Bach Le",
        "Naveed Akhtar",
        "Tuan Ngo"
      ],
      "abstract": "Accurate detection and classification of diverse door types in floor plans drawings is critical for multiple applications, such as building compliance checking, and indoor scene understanding. Despite their importance, publicly available datasets specifically designed for fine-grained multi-class door detection remain scarce. In this work, we present a semi-automated pipeline that leverages a state-of-the-art object detector and a large language model (LLM) to construct a multi-class door detection dataset with minimal manual effort. Doors are first detected as a unified category using a deep object detection model. Next, an LLM classifies each detected instance based on its visual and contextual features. Finally, a human-in-the-loop stage ensures high-quality labels and bounding boxes. Our method significantly reduces annotation cost while producing a dataset suitable for benchmarking neural models in floor plan analysis. This work demonstrates the potential of combining deep learning and multimodal reasoning for efficient dataset construction in complex real-world domains.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶æå‡ºäº†DoorDetï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§£å†³å¹³é¢å›¾(floor plans)ä¸­ç»†ç²’åº¦å¤šç±»åˆ«é—¨æ£€æµ‹æ•°æ®é›†åŒ®ä¹é—®é¢˜çš„åŠè‡ªåŠ¨åŒ–æ„å»ºæµç¨‹ã€‚è¯¥æ–¹æ³•é¦–å…ˆåˆ©ç”¨æ·±åº¦å¯¹è±¡æ£€æµ‹æ¨¡å‹(object detector)å°†å„ç§ç±»å‹çš„é—¨è¯†åˆ«ä¸ºä¸€ä¸ªç»Ÿä¸€ç±»åˆ«ï¼Œéšååˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(Large Language Model, LLM)æ ¹æ®è§†è§‰å’Œä¸Šä¸‹æ–‡ç‰¹å¾å¯¹æ£€æµ‹åˆ°çš„å®ä¾‹è¿›è¡Œç»†è‡´åˆ†ç±»ã€‚ä¸ºäº†ä¿è¯æ•°æ®çš„å‡†ç¡®æ€§ï¼Œç ”ç©¶ä¸­å¼•å…¥äº†äººæœºååŒ(human-in-the-loop)é˜¶æ®µæ¥æœ€ç»ˆæ ¡éªŒæ ‡æ³¨è´¨é‡å’Œè¾¹ç•Œæ¡†(bounding boxes)ã€‚è¿™ç§æ–¹æ³•åœ¨æ˜¾è‘—é™ä½äººå·¥æ ‡æ³¨æˆæœ¬çš„åŒæ—¶ï¼Œæ„å»ºå‡ºäº†é€‚ç”¨äºå¹³é¢å›¾åˆ†æåŸºå‡†æµ‹è¯•çš„é«˜è´¨é‡æ•°æ®é›†ã€‚è¯¥å·¥ä½œæˆåŠŸå±•ç¤ºäº†ç»“åˆæ·±åº¦å­¦ä¹ (deep learning)ä¸å¤šæ¨¡æ€æ¨ç†åœ¨ç°å®å¤æ‚é¢†åŸŸé«˜æ•ˆè¿›è¡Œæ•°æ®é›†æ„å»ºçš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07714v1",
      "published_date": "2025-08-11 07:41:09 UTC",
      "updated_date": "2025-08-11 07:41:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:05:05.559288+00:00"
    },
    {
      "arxiv_id": "2508.07710v2",
      "title": "Training-Free ANN-to-SNN Conversion for High-Performance Spiking Transformer",
      "title_zh": "é¢å‘é«˜æ€§èƒ½è„‰å†² Transformer çš„å…è®­ç»ƒ ANN-to-SNN è½¬æ¢",
      "authors": [
        "Jingya Wang",
        "Xin Deng",
        "Wenjie Wei",
        "Dehao Zhang",
        "Shuai Wang",
        "Qian Sun",
        "Jieyuan Zhang",
        "Hanwen Liu",
        "Ning Xie",
        "Malu Zhang"
      ],
      "abstract": "Leveraging the event-driven paradigm, Spiking Neural Networks (SNNs) offer a promising approach for energy-efficient Transformer architectures.While ANN-to-SNN conversion avoids the high training cost of directly trained Spiking Transformers, existing approaches still struggle to handle the nonlinear operations within Transformer blocks, and often require additional fine-tuning of pretrained ANNs.To address these limitations, we propose a training-free and high-performance ANN-to-SNN conversion framework tailored for Transformer architectures. Specifically, we introduce a Multi-basis Exponential Decay (MBE) neuron that combines exponential decay with a multi-basis encoding strategy to effectively approximate nonlinear operations, eliminating the need for weight modifications in pretrained ANNs.Extensive experiments across diverse tasks (CV, NLU, NLG) and mainstream Transformer architectures (ViT, RoBERTa, GPT-2) demonstrate that our method achieves near-lossless conversion accuracy with significantly lower latency. This provides a promising pathway for the efficient and scalable deployment of Spiking Transformers in real-world applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Spiking Neural Networks (SNNs) åœ¨é«˜èƒ½æ•ˆ Transformer æ¶æ„ä¸­çš„åº”ç”¨ï¼Œæå‡ºäº†ä¸€ä¸ªæ— éœ€è®­ç»ƒä¸”é«˜æ€§èƒ½çš„ ANN-to-SNN è½¬æ¢æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è½¬æ¢æ–¹æ³•éš¾ä»¥å¤„ç† Transformer å—å†…éçº¿æ€§æ“ä½œä¸”éœ€é¢å¤–å¾®è°ƒçš„å±€é™æ€§ã€‚ç ”ç©¶æ ¸å¿ƒæ˜¯å¼•å…¥äº† Multi-basis Exponential Decay (MBE) ç¥ç»å…ƒï¼Œè¯¥ç¥ç»å…ƒé€šè¿‡ç»“åˆæŒ‡æ•°è¡°å‡ä¸å¤šåŸºç¼–ç ç­–ç•¥æ¥ç²¾ç¡®é€¼è¿‘éçº¿æ€§å‡½æ•°ï¼Œä»è€Œæ— éœ€å¯¹é¢„è®­ç»ƒ ANN è¿›è¡Œæƒé‡ä¿®æ”¹ã€‚åœ¨è®¡ç®—æœºè§†è§‰ (CV)ã€è‡ªç„¶è¯­è¨€ç†è§£ (NLU) åŠç”Ÿæˆ (NLG) ç­‰å¤šé¡¹ä»»åŠ¡ä»¥åŠ ViTã€RoBERTa å’Œ GPT-2 ç­‰ä¸»æµæ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†è¿‘ä¹æ— æŸçš„è½¬æ¢ç²¾åº¦å¹¶æ˜¾è‘—é™ä½äº†æ¨ç†å»¶è¿Ÿã€‚è¿™ä¸€è¿›å±•ä¸º Spiking Transformers åœ¨å®é™…åº”ç”¨ä¸­çš„é«˜æ•ˆã€å¯æ‰©å±•éƒ¨ç½²æä¾›äº†ä¸€æ¡æå…·å‰æ™¯çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07710v2",
      "published_date": "2025-08-11 07:38:32 UTC",
      "updated_date": "2025-12-14 07:49:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:05:13.988084+00:00"
    },
    {
      "arxiv_id": "2508.07706v1",
      "title": "Energy Consumption in Parallel Neural Network Training",
      "title_zh": "å¹¶è¡Œç¥ç»ç½‘ç»œè®­ç»ƒä¸­çš„èƒ½è€—",
      "authors": [
        "Philipp Huber",
        "David Li",
        "Juan Pedro GutiÃ©rrez Hermosillo Muriedas",
        "Deifilia Kieckhefen",
        "Markus GÃ¶tz",
        "Achim Streit",
        "Charlotte Debus"
      ],
      "abstract": "The increasing demand for computational resources of training neural networks leads to a concerning growth in energy consumption. While parallelization has enabled upscaling model and dataset sizes and accelerated training, its impact on energy consumption is often overlooked. To close this research gap, we conducted scaling experiments for data-parallel training of two models, ResNet50 and FourCastNet, and evaluated the impact of parallelization parameters, i.e., GPU count, global batch size, and local batch size, on predictive performance, training time, and energy consumption. We show that energy consumption scales approximately linearly with the consumed resources, i.e., GPU hours; however, the respective scaling factor differs substantially between distinct model trainings and hardware, and is systematically influenced by the number of samples and gradient updates per GPU hour. Our results shed light on the complex interplay of scaling up neural network training and can inform future developments towards more sustainable AI research.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¹¶è¡Œç¥ç»ç½‘ç»œè®­ç»ƒ(Parallel Neural Network Training)ä¸­çš„èƒ½é‡æ¶ˆè€—(Energy Consumption)é—®é¢˜ï¼Œæ—¨åœ¨å¡«è¡¥å¹¶è¡ŒåŒ–å¯¹èƒ½æ•ˆå½±å“çš„ç ”ç©¶ç©ºç™½ã€‚ç ”ç©¶å›¢é˜Ÿé’ˆå¯¹ResNet50å’ŒFourCastNetæ¨¡å‹å¼€å±•äº†æ•°æ®å¹¶è¡Œ(data-parallel)è®­ç»ƒçš„æ‰©å±•å®éªŒï¼Œç³»ç»Ÿè¯„ä¼°äº†GPUæ•°é‡(GPU count)ã€å…¨å±€æ‰¹å¤§å°(global batch size)åŠå±€éƒ¨æ‰¹å¤§å°(local batch size)å¯¹æ¨¡å‹æ€§èƒ½ã€è®­ç»ƒæ—¶é—´ä¸èƒ½è€—çš„å½±å“ã€‚ç»“æœæ˜¾ç¤ºï¼Œèƒ½è€—ä¸æ‰€æ¶ˆè€—çš„èµ„æºï¼ˆå³GPU hoursï¼‰å‘ˆè¿‘ä¼¼çº¿æ€§ç¼©æ”¾å…³ç³»ï¼Œä½†å…·ä½“çš„ç¼©æ”¾å› å­åœ¨ä¸åŒæ¨¡å‹è®­ç»ƒå’Œç¡¬ä»¶ç¯å¢ƒä¸‹è¡¨ç°å‡ºæ˜¾è‘—å·®å¼‚ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œæ¯GPUå°æ—¶çš„æ ·æœ¬å¤„ç†é‡å’Œæ¢¯åº¦æ›´æ–°æ¬¡æ•°æ˜¯å½±å“èƒ½æ•ˆçš„å…³é”®ç³»ç»Ÿæ€§å› ç´ ã€‚è¿™é¡¹å·¥ä½œæ­ç¤ºäº†ç¥ç»ç½‘ç»œè®­ç»ƒæ‰©å±•è¿‡ç¨‹ä¸­å¤æ‚çš„ç›¸äº’ä½œç”¨ï¼Œä¸ºæ¨åŠ¨å¯æŒç»­AI(sustainable AI)ç ”ç©¶æä¾›äº†é‡è¦çš„å†³ç­–ä¾æ®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07706v1",
      "published_date": "2025-08-11 07:34:04 UTC",
      "updated_date": "2025-08-11 07:34:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:05:01.554718+00:00"
    },
    {
      "arxiv_id": "2508.07690v2",
      "title": "LoSemB: Logic-Guided Semantic Bridging for Inductive Tool Retrieval",
      "title_zh": "LoSemBï¼šé¢å‘å½’çº³å¼å·¥å…·æ£€ç´¢çš„é€»è¾‘å¼•å¯¼è¯­ä¹‰æ¡¥æ¥",
      "authors": [
        "Luyao Zhuang",
        "Qinggang Zhang",
        "Huachi Zhou",
        "Yujing Zhang",
        "Xiao Huang"
      ],
      "abstract": "Tool learning has emerged as a promising paradigm for large language models (LLMs) to solve many real-world tasks. Nonetheless, with the tool repository rapidly expanding, it is impractical to contain all tools within the limited input length of LLMs. To alleviate these issues, researchers have explored incorporating a tool retrieval module to select the most relevant tools or represent tools as unique tokens within LLM parameters. However, most state-of-the-art methods are under transductive settings, assuming all tools have been observed during training. Such a setting deviates from reality as the real-world tool repository is evolving and incorporates new tools frequently. When dealing with these unseen tools, which refer to tools not encountered during the training phase, these methods are limited by two key issues, including the large distribution shift and the vulnerability of similarity-based retrieval. To this end, inspired by human cognitive processes of mastering unseen tools through discovering and applying the logical information from prior experience, we introduce a novel Logic-Guided Semantic Bridging framework for inductive tool retrieval, namely, LoSemB, which aims to mine and transfer latent logical information for inductive tool retrieval without costly retraining. Specifically, LoSemB contains a logic-based embedding alignment module to mitigate distribution shifts and implements a relational augmented retrieval mechanism to reduce the vulnerability of similarity-based retrieval. Extensive experiments demonstrate that LoSemB achieves advanced performance in inductive settings while maintaining desirable effectiveness in the transductive setting.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)å·¥å…·å­¦ä¹ ä¸­å·¥å…·åº“å¿«é€Ÿæ‰©å±•å¯¼è‡´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†LoSemBæ¡†æ¶ï¼Œé‡ç‚¹è§£å†³å½’çº³å¼å·¥å…·æ£€ç´¢(inductive tool retrieval)ä¸­æœªè§å·¥å…·å¸¦æ¥çš„åˆ†å¸ƒåç§»(distribution shift)å’Œç›¸ä¼¼æ€§æ£€ç´¢è„†å¼±æ€§é—®é¢˜ã€‚å—äººç±»åˆ©ç”¨å…ˆéªŒç»éªŒå‘ç°é€»è¾‘å¹¶æŒæ¡æ–°å·¥å…·çš„è®¤çŸ¥è¿‡ç¨‹å¯å‘ï¼ŒLoSemBé€šè¿‡é€»è¾‘å¼•å¯¼çš„è¯­ä¹‰æ¡¥æ¥ï¼Œæ— éœ€æ˜‚è´µçš„é‡æ–°è®­ç»ƒå³å¯æŒ–æ˜å¹¶è½¬ç§»æ½œåœ¨é€»è¾‘ä¿¡æ¯ã€‚è¯¥æ¡†æ¶å…·ä½“åŒ…å«ä¸€ä¸ªé€»è¾‘åµŒå…¥å¯¹é½æ¨¡å—(logic-based embedding alignment)ä»¥ç¼“è§£åˆ†å¸ƒåç§»ï¼Œå¹¶å¼•å…¥äº†å…³ç³»å¢å¼ºæ£€ç´¢æœºåˆ¶(relational augmented retrieval)æ¥å¢å¼ºæ£€ç´¢çš„é²æ£’æ€§ã€‚å®éªŒè¯æ˜ï¼ŒLoSemBåœ¨å½’çº³å¼è®¾ç½®(inductive settings)ä¸‹è¾¾åˆ°äº†å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•åœ¨è½¬å¯¼å¼è®¾ç½®(transductive setting)ä¸­ä¹Ÿä¿æŒäº†è‰¯å¥½çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†ä¸æ–­æ¼”è¿›çš„å·¥å…·åº“æ—¶çš„å¼ºå¤§é€‚åº”åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07690v2",
      "published_date": "2025-08-11 07:07:18 UTC",
      "updated_date": "2026-01-21 04:05:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:05:08.066136+00:00"
    },
    {
      "arxiv_id": "2508.07683v1",
      "title": "TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for Temporal Video Grounding",
      "title_zh": "TAR-TVGï¼šåˆ©ç”¨æ—¶é—´æˆ³é”šç‚¹çº¦æŸæ¨ç†å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ—¶åºè§†é¢‘å®šä½èƒ½åŠ›",
      "authors": [
        "Chaohong Guo",
        "Xun Mo",
        "Yongwei Nie",
        "Xuemiao Xu",
        "Chao Xu",
        "Fei Yu",
        "Chengjiang Long"
      ],
      "abstract": "Temporal Video Grounding (TVG) aims to precisely localize video segments corresponding to natural language queries, which is a critical capability for long-form video understanding. Although existing reinforcement learning approaches encourage models to generate reasoning chains before predictions, they fail to explicitly constrain the reasoning process to ensure the quality of the final temporal predictions. To address this limitation, we propose Timestamp Anchor-constrained Reasoning for Temporal Video Grounding (TAR-TVG), a novel framework that introduces timestamp anchors within the reasoning process to enforce explicit supervision to the thought content. These anchors serve as intermediate verification points. More importantly, we require each reasoning step to produce increasingly accurate temporal estimations, thereby ensuring that the reasoning process contributes meaningfully to the final prediction. To address the challenge of low-probability anchor generation in models (e.g., Qwen2.5-VL-3B), we develop an efficient self-distillation training strategy: (1) initial GRPO training to collect 30K high-quality reasoning traces containing multiple timestamp anchors, (2) supervised fine-tuning (SFT) on distilled data, and (3) final GRPO optimization on the SFT-enhanced model. This three-stage training strategy enables robust anchor generation while maintaining reasoning quality. Experiments show that our model achieves state-of-the-art performance while producing interpretable, verifiable reasoning chains with progressively refined temporal estimations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TAR-TVGæ¡†æ¶ï¼Œæ—¨åœ¨æå‡è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨æ—¶åºè§†é¢‘å®šä½(Temporal Video Grounding)ä»»åŠ¡ä¸­çš„ç²¾ç¡®åº¦ã€‚é’ˆå¯¹ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¯¹æ¨ç†è¿‡ç¨‹ç¼ºä¹æ˜¾å¼çº¦æŸçš„é—®é¢˜ï¼ŒTAR-TVGåœ¨æ¨ç†é“¾ä¸­å¼•å…¥äº†æ—¶é—´æˆ³é”šç‚¹(Timestamp Anchors)ä½œä¸ºä¸­é—´éªŒè¯ç‚¹ï¼Œä»è€Œå¯¹æ€ç»´å†…å®¹å®æ–½æ˜¾å¼ç›‘ç£ã€‚è¯¥æ–¹æ³•è¦æ±‚æ¯ä¸ªæ¨ç†æ­¥éª¤ç”Ÿæˆæ„ˆå‘ç²¾ç¡®çš„æ—¶é—´ä¼°è®¡ï¼Œç¡®ä¿æ¨ç†é“¾èƒ½å®è´¨æ€§åœ°è´¡çŒ®äºæœ€ç»ˆé¢„æµ‹ã€‚ä¸ºäº†å…‹æœæ¨¡å‹ç”Ÿæˆä½æ¦‚ç‡é”šç‚¹çš„æŠ€æœ¯æŒ‘æˆ˜ï¼Œç ”ç©¶è€…å¼€å‘äº†ç»“åˆGRPOè®­ç»ƒã€ç›‘ç£å¾®è°ƒ(SFT)ä¸æ¨¡å‹å¢å¼ºçš„ä¸‰é˜¶æ®µè‡ªè’¸é¦ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è¾¾åˆ°æœ€å…ˆè¿›(State-of-the-art)æ€§èƒ½æ°´å¹³çš„åŒæ—¶ï¼Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰é«˜åº¦å¯è§£é‡Šæ€§ä¸”ç»è¿‡é€æ­¥ä¼˜åŒ–çš„æ—¶åºæ¨ç†é“¾ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07683v1",
      "published_date": "2025-08-11 06:59:32 UTC",
      "updated_date": "2025-08-11 06:59:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:05:09.163705+00:00"
    },
    {
      "arxiv_id": "2508.07681v1",
      "title": "MORE-CLEAR: Multimodal Offline Reinforcement learning for Clinical notes Leveraged Enhanced State Representation",
      "title_zh": "MORE-CLEARï¼šåˆ©ç”¨ä¸´åºŠç¬”è®°å¢å¼ºçŠ¶æ€è¡¨ç¤ºçš„å¤šæ¨¡æ€ç¦»çº¿å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Yooseok Lim",
        "ByoungJun Jeon",
        "Seong-A Park",
        "Jisoo Lee",
        "Sae Won Choi",
        "Chang Wook Jeong",
        "Ho-Geol Ryu",
        "Hongyeol Lee",
        "Hyun-Lim Yang"
      ],
      "abstract": "Sepsis, a life-threatening inflammatory response to infection, causes organ dysfunction, making early detection and optimal management critical. Previous reinforcement learning (RL) approaches to sepsis management rely primarily on structured data, such as lab results or vital signs, and on a dearth of a comprehensive understanding of the patient's condition. In this work, we propose a Multimodal Offline REinforcement learning for Clinical notes Leveraged Enhanced stAte Representation (MORE-CLEAR) framework for sepsis control in intensive care units. MORE-CLEAR employs pre-trained large-scale language models (LLMs) to facilitate the extraction of rich semantic representations from clinical notes, preserving clinical context and improving patient state representation. Gated fusion and cross-modal attention allow dynamic weight adjustment in the context of time and the effective integration of multimodal data. Extensive cross-validation using two public (MIMIC-III and MIMIC-IV) and one private dataset demonstrates that MORE-CLEAR significantly improves estimated survival rate and policy performance compared to single-modal RL approaches. To our knowledge, this is the first to leverage LLM capabilities within a multimodal offline RL for better state representation in medical applications. This approach can potentially expedite the treatment and management of sepsis by enabling reinforcement learning models to propose enhanced actions based on a more comprehensive understanding of patient conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MORE-CLEARæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ç”¨äºé‡ç—‡ç›‘æŠ¤å®¤(ICU)è„“æ¯’ç—‡æ§åˆ¶çš„å¤šæ¨¡æ€ç¦»çº¿å¼ºåŒ–å­¦ä¹ (Multimodal Offline Reinforcement learning)æ–¹æ³•ã€‚ä¸ºäº†è§£å†³ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ¨¡å‹è¿‡åº¦ä¾èµ–ç»“æ„åŒ–æ•°æ®è€Œç¼ºä¹å¯¹ç—…æƒ…å…¨é¢ç†è§£çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹(LLMs)ä»ä¸´åºŠç¬”è®°(Clinical notes)ä¸­æå–ä¸°å¯Œçš„è¯­ä¹‰ç‰¹å¾ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºäº†æ‚£è€…çš„çŠ¶æ€è¡¨ç¤º(State representation)ã€‚é€šè¿‡å¼•å…¥é—¨æ§èåˆ(Gated fusion)å’Œè·¨æ¨¡æ€æ³¨æ„åŠ›(Cross-modal attention)æœºåˆ¶ï¼ŒMORE-CLEARèƒ½å¤Ÿæ ¹æ®æ—¶é—´è¯­å¢ƒåŠ¨æ€è°ƒæ•´æƒé‡ï¼Œå®ç°å¤šæ¨¡æ€æ•°æ®çš„æœ‰æ•ˆæ•´åˆã€‚åœ¨MIMIC-IIIã€MIMIC-IVåŠç§æœ‰æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é¢„æµ‹å­˜æ´»ç‡å’Œç­–ç•¥æ€§èƒ½æ–¹é¢å‡ä¼˜äºå•æ¨¡æ€å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚ä½œä¸ºé¦–ä¸ªåœ¨åŒ»ç–—å¤šæ¨¡æ€ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­å¼•å…¥å¤§è¯­è¨€æ¨¡å‹èƒ½åŠ›çš„ç ”ç©¶ï¼ŒMORE-CLEARé€šè¿‡æä¾›æ›´å…¨é¢çš„ç—…æƒ…ç†è§£ï¼Œä¸ºä¼˜åŒ–è„“æ¯’ç—‡çš„ä¸´åºŠæ²»ç–—å’Œç®¡ç†å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.07681v1",
      "published_date": "2025-08-11 06:58:33 UTC",
      "updated_date": "2025-08-11 06:58:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:05:19.889719+00:00"
    },
    {
      "arxiv_id": "2508.07673v1",
      "title": "Ethics2vec: aligning automatic agents and human preferences",
      "title_zh": "Ethics2vecï¼šè‡ªåŠ¨åŒ–æ™ºèƒ½ä½“ä¸äººç±»åå¥½çš„å¯¹é½",
      "authors": [
        "Gianluca Bontempi"
      ],
      "abstract": "Though intelligent agents are supposed to improve human experience (or make it more efficient), it is hard from a human perspective to grasp the ethical values which are explicitly or implicitly embedded in an agent behaviour. This is the well-known problem of alignment, which refers to the challenge of designing AI systems that align with human values, goals and preferences. This problem is particularly challenging since most human ethical considerations refer to \\emph{incommensurable} (i.e. non-measurable and/or incomparable) values and criteria. Consider, for instance, a medical agent prescribing a treatment to a cancerous patient. How could it take into account (and/or weigh) incommensurable aspects like the value of a human life and the cost of the treatment? Now, the alignment between human and artificial values is possible only if we define a common space where a metric can be defined and used. This paper proposes to extend to ethics the conventional Anything2vec approach, which has been successful in plenty of similar and hard-to-quantify domains (ranging from natural language processing to recommendation systems and graph analysis). This paper proposes a way to map an automatic agent decision-making (or control law) strategy to a multivariate vector representation, which can be used to compare and assess the alignment with human values. The Ethics2Vec method is first introduced in the case of an automatic agent performing binary decision-making. Then, a vectorisation of an automatic control law (like in the case of a self-driving car) is discussed to show how the approach can be extended to automatic control settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ™ºèƒ½ä½“åœ¨â€œå¯¹é½(alignment)â€äººç±»ä»·å€¼è§‚è¿‡ç¨‹ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æ— æ³•è¡¡é‡ä¸”ä¸å¯æ¯”çš„(incommensurable)ä¼¦ç†å‡†åˆ™æ—¶çš„éš¾é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº† Ethics2vec æ–¹æ³•ï¼Œå°†å¸¸ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸçš„ Anything2vec èŒƒå¼æ‰©å±•è‡³ä¼¦ç†å»ºæ¨¡ï¼Œå»ºç«‹äº†ä¸€ä¸ªå¯ä»¥å®šä¹‰åº¦é‡çš„é€šç”¨ç©ºé—´ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†è‡ªåŠ¨æ™ºèƒ½ä½“çš„å†³ç­–ç­–ç•¥æˆ–æ§åˆ¶å¾‹(control law)æ˜ å°„ä¸ºå¤šç»´å‘é‡è¡¨ç¤ºï¼Œä½¿å¾—è¯„ä¼°å’Œæ¯”è¾ƒæ™ºèƒ½ä½“è¡Œä¸ºä¸äººç±»åå¥½çš„å¯¹é½ç¨‹åº¦æˆä¸ºå¯èƒ½ã€‚ç ”ç©¶è¯¦ç»†é˜è¿°äº† Ethics2vec åœ¨äºŒå…ƒå†³ç­–åœºæ™¯ä¸­çš„åº”ç”¨ï¼Œå¹¶æ¢è®¨äº†å…¶åœ¨è‡ªåŠ¨é©¾é©¶(self-driving car)ç­‰å¤æ‚è‡ªåŠ¨æ§åˆ¶ç¯å¢ƒä¸‹çš„æ‰©å±•æ½œåŠ›ã€‚è¿™ä¸€æ–¹æ¡ˆä¸ºé‡åŒ–åŸæœ¬éš¾ä»¥æ‰æ‘¸çš„ä¼¦ç†ä»·å€¼æä¾›äº†æ–°å·¥å…·ï¼Œä¸ºå¼€å‘ç¬¦åˆäººç±»ä¼¦ç†æœŸæœ›çš„AIç³»ç»Ÿå¥ å®šäº†æ•°å­¦åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07673v1",
      "published_date": "2025-08-11 06:52:46 UTC",
      "updated_date": "2025-08-11 06:52:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:05:19.492837+00:00"
    },
    {
      "arxiv_id": "2508.07671v1",
      "title": "EMPATHIA: Multi-Faceted Human-AI Collaboration for Refugee Integration",
      "title_zh": "EMPATHIAï¼šé¢å‘éš¾æ°‘èå…¥çš„å¤šç»´åº¦äººæœºåä½œ",
      "authors": [
        "Mohamed Rayan Barhdadi",
        "Mehmet Tuncel",
        "Erchin Serpedin",
        "Hasan Kurban"
      ],
      "abstract": "Current AI approaches to refugee integration optimize narrow objectives such as employment and fail to capture the cultural, emotional, and ethical dimensions critical for long-term success. We introduce EMPATHIA (Enriched Multimodal Pathways for Agentic Thinking in Humanitarian Immigrant Assistance), a multi-agent framework addressing the central Creative AI question: how do we preserve human dignity when machines participate in life-altering decisions? Grounded in Kegan's Constructive Developmental Theory, EMPATHIA decomposes integration into three modules: SEED (Socio-cultural Entry and Embedding Decision) for initial placement, RISE (Rapid Integration and Self-sufficiency Engine) for early independence, and THRIVE (Transcultural Harmony and Resilience through Integrated Values and Engagement) for sustained outcomes. SEED employs a selector-validator architecture with three specialized agents - emotional, cultural, and ethical - that deliberate transparently to produce interpretable recommendations. Experiments on the UN Kakuma dataset (15,026 individuals, 7,960 eligible adults 15+ per ILO/UNHCR standards) and implementation on 6,359 working-age refugees (15+) with 150+ socioeconomic variables achieved 87.4% validation convergence and explainable assessments across five host countries. EMPATHIA's weighted integration of cultural, emotional, and ethical factors balances competing value systems while supporting practitioner-AI collaboration. By augmenting rather than replacing human expertise, EMPATHIA provides a generalizable framework for AI-driven allocation tasks where multiple values must be reconciled.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EMPATHIAï¼Œä¸€ç§é’ˆå¯¹éš¾æ°‘æ•´åˆçš„å¤šæ™ºèƒ½ä½“(multi-agent)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰AIæ–¹æ¡ˆä»…å…³æ³¨å°±ä¸šç­‰å•ä¸€ç›®æ ‡è€Œå¿½è§†æ–‡åŒ–ã€æƒ…æ„Ÿå’Œä¼¦ç†ç»´åº¦çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶ä»¥Kegançš„Constructive Developmental Theoryä¸ºç†è®ºåŸºç¡€ï¼Œå°†æ•´åˆæµç¨‹åˆ’åˆ†ä¸ºè´Ÿè´£åˆå§‹å®‰ç½®çš„SEEDã€ä¿ƒè¿›æ—©æœŸç‹¬ç«‹çš„RISEä»¥åŠå…³æ³¨é•¿æœŸå‘å±•çš„THRIVEä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ã€‚å…¶ä¸­ï¼ŒSEEDæ¨¡å—é‡‡ç”¨äº†selector-validatoræ¶æ„ï¼Œé€šè¿‡æƒ…æ„Ÿã€æ–‡åŒ–å’Œä¼¦ç†ä¸‰ä¸ªä¸“ä¸šæ™ºèƒ½ä½“çš„é€æ˜åä½œï¼Œç”Ÿæˆå…·æœ‰é«˜åº¦å¯è§£é‡Šæ€§çš„å®‰ç½®å»ºè®®ã€‚åœ¨åŒ…å«15,026åä¸ªä½“çš„è”åˆå›½Kakumaæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨æ¶‰åŠ150å¤šä¸ªç¤¾ä¼šç»æµå˜é‡çš„ä»»åŠ¡ä¸­å®ç°äº†87.4%çš„éªŒè¯æ”¶æ•›ï¼Œå¹¶åœ¨äº”ä¸ªä¸œé“å›½æä¾›äº†æœ‰æ•ˆçš„è¯„ä¼°ç»“æœã€‚é€šè¿‡æƒè¡¡ç›¸äº’ç«äº‰çš„ä»·å€¼ç³»ç»Ÿå¹¶å¢å¼ºäººæœºåä½œï¼ŒEMPATHIAä¸ºå¤„ç†å¤æ‚ä»·å€¼è§‚è°ƒå’Œçš„AIé©±åŠ¨åˆ†é…ä»»åŠ¡æä¾›äº†ä¸€ä¸ªé€šç”¨çš„ç ”ç©¶èŒƒå¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "cs.MA",
        "stat.AP"
      ],
      "primary_category": "cs.AI",
      "comment": "19 pages, 3 figures (plus 6 figures in supplementary), 2 tables, 1 algorithm. Submitted to NeurIPS 2025 Creative AI Track: Humanity",
      "pdf_url": "https://arxiv.org/pdf/2508.07671v1",
      "published_date": "2025-08-11 06:50:55 UTC",
      "updated_date": "2025-08-11 06:50:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:05:23.084053+00:00"
    },
    {
      "arxiv_id": "2508.07668v1",
      "title": "AIS-LLM: A Unified Framework for Maritime Trajectory Prediction, Anomaly Detection, and Collision Risk Assessment with Explainable Forecasting",
      "title_zh": "AIS-LLMï¼šå…·å¤‡å¯è§£é‡Šé¢„æµ‹èƒ½åŠ›çš„æµ·äº‹è½¨è¿¹é¢„æµ‹ã€å¼‚å¸¸æ£€æµ‹åŠç¢°æ’é£é™©è¯„ä¼°ç»Ÿä¸€æ¡†æ¶",
      "authors": [
        "Hyobin Park",
        "Jinwook Jung",
        "Minseok Seo",
        "Hyunsoo Choi",
        "Deukjae Cho",
        "Sekil Park",
        "Dong-Geol Choi"
      ],
      "abstract": "With the increase in maritime traffic and the mandatory implementation of the Automatic Identification System (AIS), the importance and diversity of maritime traffic analysis tasks based on AIS data, such as vessel trajectory prediction, anomaly detection, and collision risk assessment, is rapidly growing. However, existing approaches tend to address these tasks individually, making it difficult to holistically consider complex maritime situations. To address this limitation, we propose a novel framework, AIS-LLM, which integrates time-series AIS data with a large language model (LLM). AIS-LLM consists of a Time-Series Encoder for processing AIS sequences, an LLM-based Prompt Encoder, a Cross-Modality Alignment Module for semantic alignment between time-series data and textual prompts, and an LLM-based Multi-Task Decoder. This architecture enables the simultaneous execution of three key tasks: trajectory prediction, anomaly detection, and risk assessment of vessel collisions within a single end-to-end system. Experimental results demonstrate that AIS-LLM outperforms existing methods across individual tasks, validating its effectiveness. Furthermore, by integratively analyzing task outputs to generate situation summaries and briefings, AIS-LLM presents the potential for more intelligent and efficient maritime traffic management.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AIS-LLMï¼Œä¸€ç§å°†æ—¶é—´åºåˆ— AIS æ•°æ®ä¸å¤§è¯­è¨€æ¨¡å‹ (LLM) é›†æˆçš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æµ·äº‹åˆ†æä»»åŠ¡å¤„ç†å­¤ç«‹ã€éš¾ä»¥åº”å¯¹å¤æ‚æƒ…å¢ƒçš„é—®é¢˜ã€‚è¯¥æ¡†æ¶ç”± Time-Series Encoderã€LLM-based Prompt Encoderã€Cross-Modality Alignment Module å’Œ LLM-based Multi-Task Decoder ç»„æˆï¼Œå®ç°äº†åœ¨å•ä¸€ç«¯åˆ°ç«¯ç³»ç»Ÿä¸­åŒæ—¶æ‰§è¡Œè½¨è¿¹é¢„æµ‹ (Trajectory Prediction)ã€å¼‚å¸¸æ£€æµ‹ (Anomaly Detection) å’Œç¢°æ’é£é™©è¯„ä¼° (Collision Risk Assessment)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAIS-LLM åœ¨æ‰€æœ‰å•é¡¹ä»»åŠ¡ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†å…¶å¼ºå¤§çš„ç‰¹å¾æå–ä¸å¤šä»»åŠ¡åè°ƒèƒ½åŠ›ã€‚é€šè¿‡æ•´åˆåˆ†æä»»åŠ¡è¾“å‡ºå¹¶ç”Ÿæˆæƒ…æ™¯æ€»ç»“å’Œç®€æŠ¥ï¼ŒAIS-LLM ä¸ºæ„å»ºæ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆä¸”å…·å¤‡å¯è§£é‡Šæ€§çš„æµ·ä¸Šäº¤é€šç®¡ç†ç³»ç»Ÿæä¾›äº†å…¨æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07668v1",
      "published_date": "2025-08-11 06:39:45 UTC",
      "updated_date": "2025-08-11 06:39:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:05:32.994784+00:00"
    },
    {
      "arxiv_id": "2508.07667v1",
      "title": "1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent Reasoning",
      "title_zh": "1-2-3 Checkï¼šé€šè¿‡å¤šæ™ºèƒ½ä½“æ¨ç†å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ä¸­çš„è¯­å¢ƒéšç§",
      "authors": [
        "Wenkai Li",
        "Liwen Sun",
        "Zhenxiang Guan",
        "Xuhui Zhou",
        "Maarten Sap"
      ],
      "abstract": "Addressing contextual privacy concerns remains challenging in interactive settings where large language models (LLMs) process information from multiple sources (e.g., summarizing meetings with private and public information). We introduce a multi-agent framework that decomposes privacy reasoning into specialized subtasks (extraction, classification), reducing the information load on any single agent while enabling iterative validation and more reliable adherence to contextual privacy norms. To understand how privacy errors emerge and propagate, we conduct a systematic ablation over information-flow topologies, revealing when and why upstream detection mistakes cascade into downstream leakage. Experiments on the ConfAIde and PrivacyLens benchmark with several open-source and closed-sourced LLMs demonstrate that our best multi-agent configuration substantially reduces private information leakage (\\textbf{18\\%} on ConfAIde and \\textbf{19\\%} on PrivacyLens with GPT-4o) while preserving the fidelity of public content, outperforming single-agent baselines. These results highlight the promise of principled information-flow design in multi-agent systems for contextual privacy with LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†å¤šæºä¿¡æ¯äº¤äº’åœºæ™¯ä¸­é¢ä¸´çš„ä¸Šä¸‹æ–‡éšç§(contextual privacy)éš¾é¢˜ï¼Œæå‡ºäº†åä¸ºâ€œ1-2-3 Checkâ€çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†éšç§æ¨ç†åˆ†è§£ä¸ºæå–(extraction)å’Œåˆ†ç±»(classification)ç­‰ä¸“é—¨å­ä»»åŠ¡ï¼Œé€šè¿‡é™ä½å•ä¸ªæ™ºèƒ½ä½“çš„ä¿¡æ¯è´Ÿè½½å¹¶å¼•å…¥è¿­ä»£éªŒè¯æœºåˆ¶ï¼Œæ˜¾è‘—æé«˜äº†ç³»ç»Ÿå¯¹éšç§å‡†åˆ™çš„éµå¾ªèƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡å¯¹ä¿¡æ¯æµæ‹“æ‰‘ç»“æ„(information-flow topologies)è¿›è¡Œç³»ç»Ÿæ€§æ¶ˆèå®éªŒï¼Œæ·±å…¥æ­ç¤ºäº†éšç§é”™è¯¯äº§ç”Ÿä¸ä¼ æ’­çš„å†…åœ¨æœºåˆ¶ã€‚åœ¨ConfAIdeå’ŒPrivacyLensåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥å¤šæ™ºèƒ½ä½“é…ç½®ä½¿GPT-4oçš„éšç§æ³„éœ²ç‡åˆ†åˆ«é™ä½äº†18%å’Œ19%ï¼ŒåŒæ—¶æœ‰æ•ˆä¿æŒäº†å…¬å…±å†…å®¹çš„ä¿çœŸåº¦ã€‚è¿™ä¸€æˆæœæ˜¾è‘—ä¼˜äºå•æ™ºèƒ½ä½“åŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­è¿›è¡Œè§„èŒƒçš„ä¿¡æ¯æµè®¾è®¡å¯¹äºå¢å¼ºä¸Šä¸‹æ–‡éšç§ä¿æŠ¤å…·æœ‰é‡è¦ä»·å€¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07667v1",
      "published_date": "2025-08-11 06:34:09 UTC",
      "updated_date": "2025-08-11 06:34:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:05:30.844953+00:00"
    },
    {
      "arxiv_id": "2508.07662v1",
      "title": "GLiClass: Generalist Lightweight Model for Sequence Classification Tasks",
      "title_zh": "GLiClassï¼šé¢å‘åºåˆ—åˆ†ç±»ä»»åŠ¡çš„é€šç”¨è½»é‡çº§æ¨¡å‹",
      "authors": [
        "Ihor Stepanov",
        "Mykhailo Shtopko",
        "Dmytro Vodianytskyi",
        "Oleksandr Lukashov",
        "Alexander Yavorskyi",
        "Mykyta Yaroshenko"
      ],
      "abstract": "Classification is one of the most widespread tasks in AI applications, serving often as the first step in filtering, sorting, and categorizing data. Since modern AI systems must handle large volumes of input data and early pipeline stages can propagate errors downstream, achieving high efficiency and accuracy is critical. Moreover, classification requirements can change dynamically based on user needs, necessitating models with strong zero-shot capabilities. While generative LLMs have become mainstream for zero-shot classification due to their versatility, they suffer from inconsistent instruction following and computational inefficiency. Cross-encoders, commonly used as rerankers in RAG pipelines, face a different bottleneck: they must process text-label pairs sequentially, significantly reducing efficiency with large label sets. Embedding-based approaches offer good efficiency but struggle with complex scenarios involving logical and semantic constraints. We propose GLiClass, a novel method that adapts the GLiNER architecture for sequence classification tasks. Our approach achieves strong accuracy and efficiency comparable to embedding-based methods, while maintaining the flexibility needed for zero-shot and few-shot learning scenarios. Additionally, we adapted proximal policy optimization (PPO) for multi-label text classification, enabling training classifiers in data-sparse conditions or from human feedback.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GLiClassï¼Œä¸€ç§é’ˆå¯¹åºåˆ—åˆ†ç±»ä»»åŠ¡è®¾è®¡çš„é€šç”¨è½»é‡åŒ–æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç°ä»£ AI ç³»ç»Ÿåœ¨å¤„ç†æµ·é‡æ•°æ®æ—¶å¯¹é«˜æ•ˆä¸”å‡†ç¡®åˆ†ç±»çš„éœ€æ±‚ã€‚å°½ç®¡ç”Ÿæˆå¼ LLMs åœ¨é›¶æ ·æœ¬åˆ†ç±»ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶æŒ‡ä»¤éµå¾ªä¸ä¸€è‡´ä¸”è®¡ç®—æˆæœ¬é«˜ï¼Œè€Œ GLiClass é€šè¿‡æ”¹è¿› GLiNER æ¶æ„ï¼Œåœ¨ä¿æŒé›¶æ ·æœ¬(Zero-shot)å’Œå°‘æ ·æœ¬(Few-shot)å­¦ä¹ çµæ´»æ€§çš„åŒæ—¶ï¼Œå®ç°äº†ä¸åµŒå…¥å¼æ–¹æ³•(Embedding-based)ç›¸å½“çš„è¿è¡Œæ•ˆç‡ã€‚è¯¥æ¨¡å‹æœ‰æ•ˆå…‹æœäº†äº¤å‰ç¼–ç å™¨(Cross-encoders)åœ¨å¤„ç†å¤§è§„æ¨¡æ ‡ç­¾é›†æ—¶çš„æ€§èƒ½ç“¶é¢ˆï¼Œèƒ½å¤Ÿç²¾å‡†å¤„ç†å¤æ‚çš„é€»è¾‘ä¸è¯­ä¹‰çº¦æŸã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å°†è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–(PPO)æŠ€æœ¯é€‚é…äºå¤šæ ‡ç­¾æ–‡æœ¬åˆ†ç±»ï¼Œä½¿å¾—æ¨¡å‹åœ¨æ•°æ®ç¨€ç¼ºæˆ–ä¾èµ–äººå·¥åé¦ˆçš„ç¯å¢ƒä¸‹ä»èƒ½è¡¨ç°ä¼˜å¼‚ã€‚å®éªŒè¯æ˜ï¼ŒGLiClass åœ¨åˆ†ç±»ç²¾åº¦ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´è¾¾æˆäº†å¹³è¡¡ï¼Œä¸ºåŠ¨æ€åˆ†ç±»ä»»åŠ¡æä¾›äº†ä¸€ç§å…¼å…·çµæ´»æ€§ä¸é«˜æ€§èƒ½çš„è½»é‡åŒ–é€‰æ‹©ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 7 tables, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.07662v1",
      "published_date": "2025-08-11 06:22:25 UTC",
      "updated_date": "2025-08-11 06:22:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:05:51.486881+00:00"
    },
    {
      "arxiv_id": "2508.07659v2",
      "title": "Discovering Spatial Correlations of Earth Observations for weather forecasting by using Graph Structure Learning",
      "title_zh": "åŸºäºå›¾ç»“æ„å­¦ä¹ æ¢ç´¢å¤©æ°”é¢„æŠ¥ä¸­åœ°çƒè§‚æµ‹çš„ç©ºé—´ç›¸å…³æ€§",
      "authors": [
        "Hyeon-Ju Jeon",
        "Jeon-Ho Kang",
        "In-Hyuk Kwon",
        "O-Joun Lee"
      ],
      "abstract": "This study aims to improve the accuracy of weather predictions by discovering spatial correlations between Earth observations and atmospheric states. Existing numerical weather prediction (NWP) systems predict future atmospheric states at fixed locations, which are called NWP grid points, by analyzing previous atmospheric states and newly acquired Earth observations. However, the shifting locations of observations and the surrounding meteorological context induce complex, dynamic spatial correlations that are difficult for traditional NWP systems to capture, since they rely on strict statistical and physical formulations. To handle complicated spatial correlations, which change dynamically, we employ a spatiotemporal graph neural networks (STGNNs) with structure learning. However, structure learning has an inherent limitation that this can cause structural information loss and over-smoothing problem by generating excessive edges. To solve this problem, we regulate edge sampling by adaptively determining node degrees and considering the spatial distances between NWP grid points and observations. We validated the effectiveness of the proposed method (CloudNine-v2) using real-world atmospheric state and observation data from East Asia, achieving up to 15\\% reductions in RMSE over existing STGNN models. Even in areas with high atmospheric variability, CloudNine-v2 consistently outperformed baselines with and without structure learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨é€šè¿‡å‘ç°åœ°çƒè§‚æµ‹(Earth observations)ä¸å¤§æ°”çŠ¶æ€ä¹‹é—´çš„ç©ºé—´ç›¸å…³æ€§æ¥æé«˜å¤©æ°”é¢„æŠ¥çš„å‡†ç¡®æ€§ã€‚é’ˆå¯¹ç°æœ‰æ•°å€¼å¤©æ°”é¢„æŠ¥(NWP)ç³»ç»Ÿéš¾ä»¥æ•æ‰åŠ¨æ€ç©ºé—´ç›¸å…³æ€§çš„æŒ‘æˆ˜ï¼Œç ”ç©¶é‡‡ç”¨äº†ç»“åˆç»“æ„å­¦ä¹ (structure learning)çš„æ—¶ç©ºå›¾ç¥ç»ç½‘ç»œ(STGNNs)è¿›è¡Œå»ºæ¨¡ã€‚ä¸ºäº†è§£å†³ç»“æ„å­¦ä¹ ä¸­å¸¸è§çš„ç»“æ„ä¿¡æ¯ä¸¢å¤±å’Œè¿‡åº¦å¹³æ»‘(over-smoothing)é—®é¢˜ï¼Œä½œè€…æå‡ºäº†CloudNine-v2æ–¹æ³•ï¼Œé€šè¿‡è‡ªé€‚åº”ç¡®å®šèŠ‚ç‚¹åº¦å¹¶æ•´åˆNWPç½‘æ ¼ç‚¹ä¸è§‚æµ‹å€¼ä¹‹é—´çš„ç©ºé—´è·ç¦»æ¥ä¼˜åŒ–è¾¹ç¼˜é‡‡æ ·ã€‚åœ¨ä¸œäºšçœŸå®å¤§æ°”æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCloudNine-v2ç›¸æ¯”ç°æœ‰çš„STGNNæ¨¡å‹èƒ½å°†å‡æ–¹æ ¹è¯¯å·®(RMSE)é™ä½é«˜è¾¾15%ã€‚å³ä½¿åœ¨æ°”è±¡å˜åŒ–å‰§çƒˆçš„åŒºåŸŸï¼Œè¯¥æ–¹æ³•ä¹Ÿè¡¨ç°å‡ºä¼˜äºå„ç±»åŸºçº¿æ¨¡å‹çš„é¢„æµ‹æ€§èƒ½å’Œé²æ£’æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.07659v2",
      "published_date": "2025-08-11 06:14:31 UTC",
      "updated_date": "2025-11-10 02:06:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:06:02.286529+00:00"
    },
    {
      "arxiv_id": "2508.07649v3",
      "title": "Disentangling Multiplex Spatial-Temporal Transition Graph Representation Learning for Socially Enhanced POI Recommendation",
      "title_zh": "é¢å‘ç¤¾äº¤å¢å¼ºå‹å…´è¶£ç‚¹ï¼ˆPOIï¼‰æ¨èçš„è§£è€¦å¼å¤šé‡æ—¶ç©ºè½¬ç§»å›¾è¡¨ç¤ºå­¦ä¹ ",
      "authors": [
        "Jie Li",
        "Haoye Dong",
        "Zhengyang Wu",
        "Zetao Zheng",
        "Mingrong Lin"
      ],
      "abstract": "Next Point-of-Interest (POI) recommendation is a research hotspot in business intelligence, where users' spatial-temporal transitions and social relationships play key roles. However, most existing works model spatial and temporal transitions separately, leading to misaligned representations of the same spatial-temporal key nodes. This misalignment introduces redundant information during fusion, increasing model uncertainty and reducing interpretability. To address this issue, we propose DiMuST, a socially enhanced POI recommendation model based on disentangled representation learning over multiplex spatial-temporal transition graphs. The model employs a novel Disentangled variational multiplex graph Auto-Encoder (DAE), which first disentangles shared and private distributions using a multiplex spatial-temporal graph strategy. It then fuses the shared features via a Product of Experts (PoE) mechanism and denoises the private features through contrastive constraints. The model effectively captures the spatial-temporal transition representations of POIs while preserving the intrinsic correlation of their spatial-temporal relationships. Experiments on two challenging datasets demonstrate that our DiMuST significantly outperforms existing methods across multiple metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DiMuST æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç¤¾äº¤å¢å¼ºçš„å…´è¶£ç‚¹ (Point-of-Interest, POI) æ¨èä¸­ï¼Œç”±äºç©ºé—´å’Œæ—¶é—´è½¬æ¢å»ºæ¨¡åˆ†ç¦»å¯¼è‡´çš„ç‰¹å¾å¯¹é½å¤±å‡†ã€ä¿¡æ¯å†—ä½™åŠå¯è§£é‡Šæ€§ä¸è¶³ç­‰é—®é¢˜ã€‚è¯¥æ¨¡å‹åŸºäºè§£è€¦è¡¨ç¤ºå­¦ä¹  (disentangled representation learning) å’Œå¤šå±‚æ—¶ç©ºè½¬æ¢å›¾ (multiplex spatial-temporal transition graphs)ï¼Œæ ¸å¿ƒé‡‡ç”¨äº†æ–°å‹çš„è§£è€¦å˜åˆ†å¤šå±‚å›¾è‡ªç¼–ç å™¨ (DAE)ã€‚é€šè¿‡å¤šå±‚æ—¶ç©ºå›¾ç­–ç•¥ï¼Œæ¨¡å‹å°†å…±äº«åˆ†å¸ƒä¸ç§æœ‰åˆ†å¸ƒè¿›è¡Œè§£è€¦ï¼Œåˆ©ç”¨ä¸“å®¶ä¹˜ç§¯ (Product of Experts, PoE) æœºåˆ¶èåˆå…±äº«ç‰¹å¾ï¼Œå¹¶é€šè¿‡å¯¹æ¯”çº¦æŸ (contrastive constraints) å¯¹ç§æœ‰ç‰¹å¾è¿›è¡Œå»å™ªå¤„ç†ã€‚è¿™ç§è®¾è®¡åœ¨æœ‰æ•ˆæ•æ‰ POI çš„æ—¶ç©ºè½¬æ¢è¡¨ç¤ºçš„åŒæ—¶ï¼Œä¿ç•™äº†å…¶æ—¶ç©ºå…³ç³»çš„å†…åœ¨ç›¸å…³æ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒDiMuST åœ¨ä¸¤ä¸ªæŒ‘æˆ˜æ€§æ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæå‡äº†æ¨èçš„å‡†ç¡®æ€§ä¸æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "The original paper has issues and has been restructured in the work; it is no longer suitable, so I am applying for withdrawal",
      "pdf_url": "https://arxiv.org/pdf/2508.07649v3",
      "published_date": "2025-08-11 06:00:20 UTC",
      "updated_date": "2025-10-03 09:55:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:05:57.695241+00:00"
    },
    {
      "arxiv_id": "2508.07648v1",
      "title": "Grasp-HGN: Grasping the Unexpected",
      "title_zh": "Grasp-HGNï¼šåº”å¯¹æœªçŸ¥â€”â€”é¢å‘æœªè§ç‰©ä½“çš„ç¨³å¥æŠ“å–",
      "authors": [
        "Mehrshad Zandigohar",
        "Mallesham Dasari",
        "Gunar Schirner"
      ],
      "abstract": "For transradial amputees, robotic prosthetic hands promise to regain the capability to perform daily living activities. To advance next-generation prosthetic hand control design, it is crucial to address current shortcomings in robustness to out of lab artifacts, and generalizability to new environments. Due to the fixed number of object to interact with in existing datasets, contrasted with the virtually infinite variety of objects encountered in the real world, current grasp models perform poorly on unseen objects, negatively affecting users' independence and quality of life.\n  To address this: (i) we define semantic projection, the ability of a model to generalize to unseen object types and show that conventional models like YOLO, despite 80% training accuracy, drop to 15% on unseen objects. (ii) we propose Grasp-LLaVA, a Grasp Vision Language Model enabling human-like reasoning to infer the suitable grasp type estimate based on the object's physical characteristics resulting in a significant 50.2% accuracy over unseen object types compared to 36.7% accuracy of an SOTA grasp estimation model.\n  Lastly, to bridge the performance-latency gap, we propose Hybrid Grasp Network (HGN), an edge-cloud deployment infrastructure enabling fast grasp estimation on edge and accurate cloud inference as a fail-safe, effectively expanding the latency vs. accuracy Pareto. HGN with confidence calibration (DC) enables dynamic switching between edge and cloud models, improving semantic projection accuracy by 5.6% (to 42.3%) with 3.5x speedup over the unseen object types. Over a real-world sample mix, it reaches 86% average accuracy (12.2% gain over edge-only), and 2.2x faster inference than Grasp-LLaVA alone.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ™ºèƒ½å‡è‚¢æ‰‹åœ¨ç°å®ç¯å¢ƒä¸­é¢å¯¹æœªè§ç‰©ä½“ï¼ˆunseen objectsï¼‰æ—¶æ³›åŒ–æ€§è¾ƒå·®çš„é—®é¢˜ï¼Œæå‡ºäº†æ—¨åœ¨æå‡æŠ“å–ç¨³å¥æ€§çš„ç³»ç»Ÿæ¡†æ¶ã€‚ç ”ç©¶é¦–å…ˆå®šä¹‰äº†è¯­ä¹‰æŠ•å½±ï¼ˆsemantic projectionï¼‰æ¦‚å¿µï¼Œå¹¶å¼€å‘äº† Grasp-LLaVA è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ç±»äººæ¨ç†æ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹æ–°ç‰©ä½“ç‰©ç†ç‰¹å¾çš„ç†è§£ä¸æŠ“å–å‡†ç¡®ç‡ã€‚ä¸ºå¹³è¡¡æ¨ç†å»¶è¿Ÿä¸è®¡ç®—ç²¾åº¦ï¼Œç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†åä¸º HGN (Hybrid Grasp Network) çš„è¾¹ç¼˜-äº‘ï¼ˆedge-cloudï¼‰éƒ¨ç½²æ¶æ„ï¼Œå¹¶å¼•å…¥ç½®ä¿¡åº¦æ ¡å‡†ï¼ˆDCï¼‰æœºåˆ¶ä»¥å®ç°åŠ¨æ€åˆ‡æ¢ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHGN åœ¨æœªè§ç‰©ä½“ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ° 42.3%ï¼Œä¸”åœ¨æ··åˆæ ·æœ¬æµ‹è¯•ä¸­å®ç°äº† 86% çš„å¹³å‡å‡†ç¡®ç‡ï¼Œæ¨ç†é€Ÿåº¦æ¯”å•ç‹¬ä½¿ç”¨ Grasp-LLaVA æå‡äº† 2.2 å€ã€‚è¯¥ç ”ç©¶é€šè¿‡ç»“åˆå¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä¸é«˜æ•ˆçš„äº‘è¾¹ååŒæ¶æ„ï¼Œä¸ºå¼€å‘é«˜å¯é ã€ä½å»¶è¿Ÿçš„ä¸‹ä¸€ä»£è‡ªä¸»å‡è‚¢æ§åˆ¶æŠ€æœ¯å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Paper accepted at ACM Transactions on Embedded Computing Systems",
      "pdf_url": "https://arxiv.org/pdf/2508.07648v1",
      "published_date": "2025-08-11 05:58:28 UTC",
      "updated_date": "2025-08-11 05:58:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:06:11.292054+00:00"
    },
    {
      "arxiv_id": "2508.07642v2",
      "title": "Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents",
      "title_zh": "åˆ†è§£ä¸é‡æ„ï¼šåŸºäºæŠ€èƒ½çš„è§†è§‰è¯­è¨€å¯¼èˆªæ··åˆæ™ºèƒ½ä½“",
      "authors": [
        "Tianyi Ma",
        "Yue Zhang",
        "Zehao Wang",
        "Parisa Kordjamshidi"
      ],
      "abstract": "Vision-and-Language Navigation (VLN) poses significant challenges for agents to interpret natural language instructions and navigate complex 3D environments. While recent progress has been driven by large-scale pre-training and data augmentation, current methods still struggle to generalize to unseen scenarios, particularly when complex spatial and temporal reasoning is required. In this work, we propose SkillNav, a modular framework that introduces structured, skill-based reasoning into Transformer-based VLN agents. Our method decomposes navigation into a set of interpretable atomic skills (e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each handled by a specialized agent. To support targeted skill training without manual data annotation, we construct a synthetic dataset pipeline that generates diverse, linguistically natural, skill-specific instruction-trajectory pairs. We then introduce a novel training-free Vision-Language Model (VLM)-based router, which dynamically selects the most suitable agent at each time step by aligning sub-goals with visual observations and historical actions. SkillNav obtains competitive results on commonly used benchmarks and establishes state-of-the-art generalization to the GSA-R2R, a benchmark with novel instruction styles and unseen environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€å¯¼èˆª(Vision-and-Language Navigation, VLN)åœ¨å¤æ‚ç©ºé—´æ¨ç†å’ŒæœªçŸ¥åœºæ™¯æ³›åŒ–æ–¹é¢çš„éš¾é¢˜ï¼Œæå‡ºäº†æ¨¡å—åŒ–æ¡†æ¶SkillNavã€‚è¯¥æ–¹æ³•é€šè¿‡å°†å¯¼èˆªä»»åŠ¡åˆ†è§£ä¸ºä¸€ç³»åˆ—å¯è§£é‡Šçš„åŸå­æŠ€èƒ½(atomic skills)ï¼Œå¦‚Vertical Movementã€Area and Region Identificationä»¥åŠStop and Pauseï¼Œå¹¶ç”±ä¸“é—¨çš„æ™ºèƒ½ä½“å¤„ç†ï¼Œå°†ç»“æ„åŒ–çš„æŠ€èƒ½æ¨ç†å¼•å…¥Transformeræ¨¡å‹ã€‚ä¸ºäº†åœ¨æ— éœ€äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹è¿›è¡ŒæŠ€èƒ½è®­ç»ƒï¼Œç ”ç©¶è®¾è®¡äº†åˆæˆæ•°æ®é›†æµæ°´çº¿æ¥ç”Ÿæˆå¤šæ ·åŒ–çš„æŒ‡ä»¤-è½¨è¿¹å¯¹ã€‚æ­¤å¤–ï¼ŒSkillNavå¼•å…¥äº†ä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹(VLM)çš„å…è®­ç»ƒè·¯ç”±å™¨(router)ï¼Œèƒ½å¤Ÿæ ¹æ®å­ç›®æ ‡ä¸è§†è§‰è§‚æµ‹å®æ—¶åŠ¨æ€é€‰æ‹©æœ€åŒ¹é…çš„æ™ºèƒ½ä½“ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSkillNavåœ¨å¤šä¸ªé€šç”¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨å¤„ç†æ–°é¢–æŒ‡ä»¤é£æ ¼å’ŒæœªçŸ¥ç¯å¢ƒçš„GSA-R2RåŸºå‡†ä¸Šè¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›(State-of-the-art)çš„æ°´å¹³ã€‚è¯¥æ¡†æ¶é€šè¿‡è¿™ç§â€œå…ˆæ‹†è§£å†æ„å»ºâ€çš„ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†æ™ºèƒ½ä½“åœ¨å¤æ‚ä¸‰ç»´ç¯å¢ƒä¸‹çš„æ³›åŒ–ä¸æ¨ç†èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07642v2",
      "published_date": "2025-08-11 05:50:30 UTC",
      "updated_date": "2025-10-01 00:48:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:06:05.990631+00:00"
    },
    {
      "arxiv_id": "2508.07636v1",
      "title": "Attribution Explanations for Deep Neural Networks: A Theoretical Perspective",
      "title_zh": "æ·±åº¦ç¥ç»ç½‘ç»œå½’å› è§£é‡Šï¼šç†è®ºè§†è§’çš„æ¢è®¨",
      "authors": [
        "Huiqi Deng",
        "Hongbin Pei",
        "Quanshi Zhang",
        "Mengnan Du"
      ],
      "abstract": "Attribution explanation is a typical approach for explaining deep neural networks (DNNs), inferring an importance or contribution score for each input variable to the final output. In recent years, numerous attribution methods have been developed to explain DNNs. However, a persistent concern remains unresolved, i.e., whether and which attribution methods faithfully reflect the actual contribution of input variables to the decision-making process. The faithfulness issue undermines the reliability and practical utility of attribution explanations. We argue that these concerns stem from three core challenges. First, difficulties arise in comparing attribution methods due to their unstructured heterogeneity, differences in heuristics, formulations, and implementations that lack a unified organization. Second, most methods lack solid theoretical underpinnings, with their rationales remaining absent, ambiguous, or unverified. Third, empirically evaluating faithfulness is challenging without ground truth. Recent theoretical advances provide a promising way to tackle these challenges, attracting increasing attention. We summarize these developments, with emphasis on three key directions: (i) Theoretical unification, which uncovers commonalities and differences among methods, enabling systematic comparisons; (ii) Theoretical rationale, clarifying the foundations of existing methods; (iii) Theoretical evaluation, rigorously proving whether methods satisfy faithfulness principles. Beyond a comprehensive review, we provide insights into how these studies help deepen theoretical understanding, inform method selection, and inspire new attribution methods. We conclude with a discussion of promising open problems for further work.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶é’ˆå¯¹æ·±åº¦ç¥ç»ç½‘ç»œ(DNNs)çš„å½’å› è§£é‡Š(Attribution explanation)æ–¹æ³•ï¼Œæ·±å…¥æ¢è®¨äº†å…¶åœ¨å¿ å®æ€§(Faithfulness)æ–¹é¢å­˜åœ¨çš„å¯é æ€§äº‰è®®ã€‚ä½œè€…æŒ‡å‡ºå½“å‰é¢†åŸŸé¢ä¸´æ–¹æ³•é—´å¼‚æ„æ€§å¯¼è‡´éš¾ä»¥æ¯”è¾ƒã€ç¼ºä¹ä¸¥è°¨ç†è®ºæ”¯æ’‘ä»¥åŠç¼ºä¹å®¢è§‚è¯„ä¼°åŸºå‡†ç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œè¯¥ç»¼è¿°ç³»ç»Ÿæ€»ç»“äº†è¿‘å¹´æ¥åœ¨ç†è®ºç»Ÿä¸€(Theoretical unification)ã€ç†è®ºåŸºç¡€(Theoretical rationale)å’Œç†è®ºè¯„ä¼°(Theoretical evaluation)ä¸‰ä¸ªæ–¹å‘çš„æœ€æ–°è¿›å±•ã€‚é€šè¿‡åˆ†æä¸åŒæ–¹æ³•é—´çš„å†…åœ¨å…³è”ä¸å·®å¼‚ï¼Œè¯¥ç ”ç©¶æ¾„æ¸…äº†ç°æœ‰å½’å› æŠ€æœ¯çš„åº•å±‚é€»è¾‘ï¼Œå¹¶ä»ç†è®ºå±‚é¢ä¸¥æ ¼è®ºè¯äº†æ–¹æ³•å¯¹å¿ å®æ€§åŸåˆ™çš„éµå¾ªæƒ…å†µã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æä¾›äº†æ·±åŒ–ç†è®ºç†è§£ã€æŒ‡å¯¼æ–¹æ³•é€‰æ‹©åŠå¯å‘æ–°å½’å› ç®—æ³•è®¾è®¡çš„è§è§£ï¼Œå¹¶ä¸ºè¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07636v1",
      "published_date": "2025-08-11 05:41:20 UTC",
      "updated_date": "2025-08-11 05:41:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:06:30.857726+00:00"
    },
    {
      "arxiv_id": "2508.07631v3",
      "title": "Efficient Approximate Posterior Sampling with Annealed Langevin Monte Carlo",
      "title_zh": "åŸºäºé€€ç«æœ—ä¹‹ä¸‡è’™ç‰¹å¡æ´›çš„é«˜æ•ˆè¿‘ä¼¼åéªŒé‡‡æ ·",
      "authors": [
        "Advait Parulekar",
        "Litu Rout",
        "Karthikeyan Shanmugam",
        "Sanjay Shakkottai"
      ],
      "abstract": "We study the problem of posterior sampling in the context of score based generative models. We have a trained score network for a prior $p(x)$, a measurement model $p(y|x)$, and are tasked with sampling from the posterior $p(x|y)$. Prior work has shown this to be intractable in KL (in the worst case) under well-accepted computational hardness assumptions. Despite this, popular algorithms for tasks such as image super-resolution, stylization, and reconstruction enjoy empirical success. Rather than establishing distributional assumptions or restricted settings under which exact posterior sampling is tractable, we view this as a more general \"tilting\" problem of biasing a distribution towards a measurement. Under minimal assumptions, we show that one can tractably sample from a distribution that is simultaneously close to the posterior of a noised prior in KL divergence and the true posterior in Fisher divergence. Intuitively, this combination ensures that the resulting sample is consistent with both the measurement and the prior. To the best of our knowledge these are the first formal results for (approximate) posterior sampling in polynomial time.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†è¯„åˆ†ç”Ÿæˆæ¨¡å‹(score based generative models)ä¸­çš„åéªŒé‡‡æ ·(posterior sampling)éš¾é¢˜ã€‚é’ˆå¯¹ç²¾ç¡®åéªŒé‡‡æ ·åœ¨è®¡ç®—å¤æ‚æ€§ä¸Šçš„ä¸å¯è¡Œæ€§ï¼Œä½œè€…å°†å…¶é‡æ–°å®šä¹‰ä¸ºä¸€ä¸ªå°†åˆ†å¸ƒåå‘è§‚æµ‹å€¼çš„é€šç”¨â€œå€¾æ–œâ€(tilting)é—®é¢˜ã€‚ç ”ç©¶è¯æ˜ï¼Œåœ¨æå°å‡è®¾ä¸‹ï¼Œå¯ä»¥å®ç°ä»ä¸€ä¸ªåŒæ—¶æ¥è¿‘å™ªå£°å…ˆéªŒåéªŒï¼ˆKLæ•£åº¦æ„ä¹‰ä¸‹ï¼‰å’ŒçœŸå®åéªŒï¼ˆFisheræ•£åº¦æ„ä¹‰ä¸‹ï¼‰çš„åˆ†å¸ƒä¸­è¿›è¡Œå¤šé¡¹å¼æ—¶é—´(polynomial time)çš„æœ‰æ•ˆé‡‡æ ·ã€‚è¿™ç§æœºåˆ¶ç¡®ä¿äº†ç”Ÿæˆçš„æ ·æœ¬åœ¨é€»è¾‘ä¸Šä¸æµ‹é‡æ¨¡å‹$p(y|x)$åŠå…ˆéªŒ$p(x)$å‡ä¿æŒä¸€è‡´ã€‚è¯¥å·¥ä½œæä¾›äº†é¦–ä¸ªå…³äºè¿‘ä¼¼åéªŒé‡‡æ ·åœ¨å¤šé¡¹å¼æ—¶é—´å†…å®Œæˆçš„æ­£è§„å½¢å¼åŒ–ç»“æœã€‚è¿™ä¸€å‘ç°ä¸ºå›¾åƒè¶…åˆ†è¾¨ç‡å’Œé‡å»ºç­‰ä»»åŠ¡çš„ç»éªŒæ€§æˆåŠŸæä¾›äº†ç†è®ºæ”¯æ’‘ï¼Œå¹¶åœ¨ç¡®ä¿è®¡ç®—æ•ˆç‡çš„åŒæ—¶å…¼é¡¾äº†é‡‡æ ·çš„ä¿çœŸåº¦ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07631v3",
      "published_date": "2025-08-11 05:25:24 UTC",
      "updated_date": "2025-12-08 17:38:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:06:22.282546+00:00"
    },
    {
      "arxiv_id": "2508.07630v1",
      "title": "InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information",
      "title_zh": "InterChartï¼šè·¨åˆ†è§£ä¸åˆ†å¸ƒå¼å›¾è¡¨ä¿¡æ¯çš„è§†è§‰æ¨ç†åŸºå‡†",
      "authors": [
        "Anirudh Iyengar Kaniyar Narayana Iyengar",
        "Srija Mukhopadhyay",
        "Adnan Qidwai",
        "Shubhankar Singh",
        "Dan Roth",
        "Vivek Gupta"
      ],
      "abstract": "We introduce InterChart, a diagnostic benchmark that evaluates how well vision-language models (VLMs) reason across multiple related charts, a task central to real-world applications such as scientific reporting, financial analysis, and public policy dashboards. Unlike prior benchmarks focusing on isolated, visually uniform charts, InterChart challenges models with diverse question types ranging from entity inference and trend correlation to numerical estimation and abstract multi-step reasoning grounded in 2-3 thematically or structurally related charts. We organize the benchmark into three tiers of increasing difficulty: (1) factual reasoning over individual charts, (2) integrative analysis across synthetically aligned chart sets, and (3) semantic inference over visually complex, real-world chart pairs. Our evaluation of state-of-the-art open and closed-source VLMs reveals consistent and steep accuracy declines as chart complexity increases. We find that models perform better when we decompose multi-entity charts into simpler visual units, underscoring their struggles with cross-chart integration. By exposing these systematic limitations, InterChart provides a rigorous framework for advancing multimodal reasoning in complex, multi-visual environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† InterChartï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Models, VLMs) åœ¨å¤šå¼ å…³è”å›¾è¡¨é—´è¿›è¡Œè§†è§‰æ¨ç†èƒ½åŠ›çš„è¯Šæ–­åŸºå‡†ï¼Œå¡«è¡¥äº†ä»¥å¾€åŸºå‡†ä»…å…³æ³¨å­¤ç«‹å›¾è¡¨çš„ç©ºç™½ã€‚InterChart é€šè¿‡è·¨è¶Š 2-3 å¼ ä¸»é¢˜æˆ–ç»“æ„ç›¸å…³çš„å›¾è¡¨ï¼ŒæŒ‘æˆ˜æ¨¡å‹åœ¨ Entity inferenceã€Trend correlationã€Numerical estimation ä»¥åŠæŠ½è±¡çš„ Multi-step reasoning ç­‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚è¯¥åŸºå‡†å°†è¯„ä¼°åˆ’åˆ†ä¸ºä¸‰ä¸ªéš¾åº¦ç­‰çº§ï¼šå•å›¾è¡¨çš„ Factual reasoningã€åˆæˆå¯¹é½å›¾è¡¨ç»„çš„ Integrative analysis ä»¥åŠå¤æ‚çœŸå®å›¾è¡¨å¯¹çš„ Semantic inferenceã€‚é€šè¿‡å¯¹å¤šç§ä¸»æµå¼€æºå’Œé—­æº VLMs çš„æµ‹è¯•ï¼Œç ”ç©¶å‘ç°æ¨¡å‹çš„å‡†ç¡®ç‡éšå›¾è¡¨å¤æ‚åº¦å¢åŠ è€Œæ˜¾è‘—ä¸‹é™ã€‚ç»“æœè¡¨æ˜ï¼ŒVLMs åœ¨ Cross-chart integration æ–¹é¢å­˜åœ¨æ˜æ˜¾çŸ­æ¿ï¼Œä½†åœ¨å¤„ç†åˆ†è§£åçš„ç®€å•è§†è§‰å•å…ƒæ—¶è¡¨ç°è¾ƒå¥½ã€‚InterChart ä¸ºæå‡å¤æ‚å¤šè§†è§‰ç¯å¢ƒä¸‹çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ä¸ªä¸¥è°¨çš„å®éªŒæ¡†æ¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 6 figures, 12 tables. Benchmark dataset and evaluation code will be publicly made available",
      "pdf_url": "https://arxiv.org/pdf/2508.07630v1",
      "published_date": "2025-08-11 05:19:23 UTC",
      "updated_date": "2025-08-11 05:19:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:06:21.988757+00:00"
    },
    {
      "arxiv_id": "2508.07629v3",
      "title": "Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization",
      "title_zh": "Klear-Reasonerï¼šé€šè¿‡æ¢¯åº¦ä¿ç•™å‰ªè£ç­–ç•¥ä¼˜åŒ–æå‡æ¨ç†èƒ½åŠ›",
      "authors": [
        "Zhenpeng Su",
        "Leiyu Pan",
        "Xue Bai",
        "Dening Liu",
        "Guanting Dong",
        "Jiaming Huang",
        "Wenping Hu",
        "Fuzheng Zhang",
        "Kun Gai",
        "Guorui Zhou"
      ],
      "abstract": "We present Klear-Reasoner, a model with long reasoning capabilities that demonstrates careful deliberation during problem solving, achieving outstanding performance across multiple benchmarks. Although there are already many excellent works related to inference models in the current community, there are still many problems with reproducing high-performance inference models due to incomplete disclosure of training details. This report provides an in-depth analysis of the reasoning model, covering the entire post-training workflow from data preparation and long Chain-of-Thought supervised fine-tuning (long CoT SFT) to reinforcement learning (RL), along with detailed ablation studies for each experimental component. For SFT data, our experiments show that a small number of high-quality data sources are more effective than a large number of diverse data sources, and that difficult samples can achieve better results without accuracy filtering. In addition, we investigate two key issues with current clipping mechanisms in RL: Clipping suppresses critical exploration signals and ignores suboptimal trajectories. To address these challenges, we propose Gradient-Preserving clipping Policy Optimization (GPPO) that gently backpropagates gradients from clipped tokens. GPPO not only enhances the model's exploration capacity but also improves its efficiency in learning from negative samples. Klear-Reasoner exhibits exceptional reasoning abilities in mathematics and programming, scoring 90.5% on AIME 2024, 83.2% on AIME 2025, 66.0% on LiveCodeBench V5 and 58.1% on LiveCodeBench V6.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Klear-Reasonerï¼Œè¿™æ˜¯ä¸€ä¸ªå…·å¤‡é•¿é“¾å¼æ€ç»´ (Long Chain-of-Thought) æ¨ç†èƒ½åŠ›çš„æ¨¡å‹ï¼Œåœ¨å¤šä¸ªæƒå¨åŸºå‡†æµ‹è¯•ä¸­å±•ç°äº†å“è¶Šçš„è§£é¢˜æ€§èƒ½ã€‚é€šè¿‡å¯¹åè®­ç»ƒå·¥ä½œæµçš„æ·±å…¥åˆ†æï¼Œç ”ç©¶å‘ç°å°‘é‡é«˜è´¨é‡æ•°æ®åœ¨ç›‘ç£å¾®è°ƒ (SFT) é˜¶æ®µæ¯”å¤§é‡å¤šæ ·åŒ–æ•°æ®æ›´æœ‰æ•ˆï¼Œä¸”éš¾æ ·æœ¬åœ¨æ— éœ€å‡†ç¡®ç‡è¿‡æ»¤çš„æƒ…å†µä¸‹èƒ½å–å¾—æ›´å¥½çš„è®­ç»ƒæ•ˆæœã€‚é’ˆå¯¹å¼ºåŒ–å­¦ä¹  (RL) ä¸­ä¼ ç»Ÿå‰ªåˆ‡æœºåˆ¶æŠ‘åˆ¶æ¢ç´¢ä¿¡å·å¹¶å¿½ç•¥æ¬¡ä¼˜è½¨è¿¹çš„ç¼ºé™·ï¼Œä½œè€…æå‡ºäº†æ¢¯åº¦ä¿æŒå‰ªåˆ‡ç­–ç•¥ä¼˜åŒ– (Gradient-Preserving clipping Policy Optimization, GPPO) ç®—æ³•ï¼Œé€šè¿‡å¯¹å·²å‰ªåˆ‡çš„ token è¿›è¡ŒæŸ”å’Œçš„æ¢¯åº¦åå‘ä¼ æ’­ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›å’Œå­¦ä¹ è´Ÿæ ·æœ¬çš„æ•ˆç‡ã€‚å®éªŒç»“æœè¯æ˜ Klear-Reasoner åœ¨æ•°å­¦ä¸ç¼–ç¨‹é¢†åŸŸå…·æœ‰é¡¶å°–çš„æ¨ç†æ°´å¹³ï¼Œå…¶åœ¨ AIME 2024 è·å¾—äº† 90.5% çš„é«˜åˆ†ï¼Œå¹¶åœ¨ LiveCodeBench ç­‰å¤šä¸ªç¼–ç¨‹ç«èµ›è¯„æµ‹ä¸­å–å¾—äº†ä¼˜å¼‚æˆç»©ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07629v3",
      "published_date": "2025-08-11 05:17:51 UTC",
      "updated_date": "2026-01-04 03:18:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:06:28.491926+00:00"
    },
    {
      "arxiv_id": "2508.07628v1",
      "title": "Multimodal AI Systems for Enhanced Laying Hen Welfare Assessment and Productivity Optimization",
      "title_zh": "ç”¨äºå¢å¼ºè›‹é¸¡ç¦åˆ©è¯„ä¼°ä¸ç”Ÿäº§æ€§èƒ½ä¼˜åŒ–çš„å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç³»ç»Ÿ",
      "authors": [
        "Daniel Essien",
        "Suresh Neethirajan"
      ],
      "abstract": "The future of poultry production depends on a paradigm shift replacing subjective, labor-intensive welfare checks with data-driven, intelligent monitoring ecosystems. Traditional welfare assessments-limited by human observation and single-sensor data-cannot fully capture the complex, multidimensional nature of laying hen welfare in modern farms. Multimodal Artificial Intelligence (AI) offers a breakthrough, integrating visual, acoustic, environmental, and physiological data streams to reveal deeper insights into avian welfare dynamics. This investigation highlights multimodal As transformative potential, showing that intermediate (feature-level) fusion strategies achieve the best balance between robustness and performance under real-world poultry conditions, and offer greater scalability than early or late fusion approaches. Key adoption barriers include sensor fragility in harsh farm environments, high deployment costs, inconsistent behavioral definitions, and limited cross-farm generalizability. To address these, we introduce two novel evaluation tools - the Domain Transfer Score (DTS) to measure model adaptability across diverse farm settings, and the Data Reliability Index (DRI) to assess sensor data quality under operational constraints. We also propose a modular, context-aware deployment framework designed for laying hen environments, enabling scalable and practical integration of multimodal sensing. This work lays the foundation for a transition from reactive, unimodal monitoring to proactive, precision-driven welfare systems that unite productivity with ethical, science based animal care.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€äººå·¥æ™ºèƒ½(Multimodal AI)ç³»ç»Ÿåœ¨å¢å¼ºè›‹é¸¡ç¦åˆ©è¯„ä¼°å’Œç”Ÿäº§åŠ›ä¼˜åŒ–æ–¹é¢çš„åº”ç”¨ï¼Œæ—¨åœ¨å°†ä¼ ç»Ÿçš„ä¸»è§‚ç›‘æµ‹è½¬å‘æ•°æ®é©±åŠ¨çš„æ™ºèƒ½ç›‘æ§ç”Ÿæ€ã€‚ç ”ç©¶é€šè¿‡é›†æˆè§†è§‰ã€å¬è§‰ã€ç¯å¢ƒå’Œç”Ÿç†ç­‰å¤šæºæ•°æ®æµï¼Œå‘ç°ä¸­é—´å±‚ï¼ˆç‰¹å¾çº§ï¼‰èåˆç­–ç•¥(Intermediate/feature-level fusion)åœ¨çœŸå®å…»æ®–ç¯å¢ƒä¸‹å±•ç°äº†æœ€ä½³çš„ç¨³å¥æ€§ã€æ€§èƒ½åŠå¯æ‰©å±•æ€§ã€‚é’ˆå¯¹ä¼ æ„Ÿå™¨åœ¨æ¶åŠ£ç¯å¢ƒä¸‹è„†å¼±ã€éƒ¨ç½²æˆæœ¬é«˜åŠè·¨åœºåŸŸé€šç”¨æ€§æœ‰é™ç­‰æŒ‘æˆ˜ï¼Œè®ºæ–‡å¼•å…¥äº†é¢†åŸŸè½¬ç§»è¯„åˆ†(Domain Transfer Score, DTS)å’Œæ•°æ®å¯é æ€§æŒ‡æ•°(Data Reliability Index, DRI)ä¸¤ç§æ–°å‹è¯„ä¼°å·¥å…·ï¼Œä»¥è¡¡é‡æ¨¡å‹çš„é€‚åº”æ€§å’Œæ•°æ®è´¨é‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹è›‹é¸¡ç¯å¢ƒè®¾è®¡çš„æ¨¡å—åŒ–ã€æƒ…å¢ƒæ„ŸçŸ¥éƒ¨ç½²æ¡†æ¶ï¼Œä¸ºä»è¢«åŠ¨çš„å•æ¨¡æ€ç›‘æµ‹å‘ä¸»åŠ¨çš„ç²¾å‡†ç¦åˆ©ç³»ç»Ÿè½¬å˜å¥ å®šäº†åŸºç¡€ï¼Œæœ‰æ•ˆç»“åˆäº†ç”Ÿäº§æ•ˆç‡ä¸åŠ¨ç‰©ä¼¦ç†å…³æ€€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "66 pages, 7 figures, 11 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.07628v1",
      "published_date": "2025-08-11 05:17:16 UTC",
      "updated_date": "2025-08-11 05:17:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:06:29.489593+00:00"
    },
    {
      "arxiv_id": "2508.07621v1",
      "title": "SOFA: Deep Learning Framework for Simulating and Optimizing Atrial Fibrillation Ablation",
      "title_zh": "SOFAï¼šç”¨äºæˆ¿é¢¤æ¶ˆèæ¨¡æ‹Ÿä¸ä¼˜åŒ–çš„æ·±åº¦å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Yunsung Chung",
        "Chanho Lim",
        "Ghassan Bidaoui",
        "Christian Massad",
        "Nassir Marrouche",
        "Jihun Hamm"
      ],
      "abstract": "Atrial fibrillation (AF) is a prevalent cardiac arrhythmia often treated with catheter ablation procedures, but procedural outcomes are highly variable. Evaluating and improving ablation efficacy is challenging due to the complex interaction between patient-specific tissue and procedural factors. This paper asks two questions: Can AF recurrence be predicted by simulating the effects of procedural parameters? How should we ablate to reduce AF recurrence? We propose SOFA (Simulating and Optimizing Atrial Fibrillation Ablation), a novel deep-learning framework that addresses these questions. SOFA first simulates the outcome of an ablation strategy by generating a post-ablation image depicting scar formation, conditioned on a patient's pre-ablation LGE-MRI and the specific procedural parameters used (e.g., ablation locations, duration, temperature, power, and force). During this simulation, it predicts AF recurrence risk. Critically, SOFA then introduces an optimization scheme that refines these procedural parameters to minimize the predicted risk. Our method leverages a multi-modal, multi-view generator that processes 2.5D representations of the atrium. Quantitative evaluations show that SOFA accurately synthesizes post-ablation images and that our optimization scheme leads to a 22.18\\% reduction in the model-predicted recurrence risk. To the best of our knowledge, SOFA is the first framework to integrate the simulation of procedural effects, recurrence prediction, and parameter optimization, offering a novel tool for personalizing AF ablation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SOFAï¼ˆSimulating and Optimizing Atrial Fibrillation Ablationï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä¼˜åŒ–å¿ƒæˆ¿é¢¤åŠ¨ï¼ˆAtrial fibrillation, AFï¼‰å¯¼ç®¡æ¶ˆèç­–ç•¥çš„æ–°å‹æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚SOFAé€šè¿‡æ•´åˆæ‚£è€…æœ¯å‰çš„LGE-MRIå½±åƒå’Œå…·ä½“çš„æ¶ˆèå‚æ•°ï¼ˆå¦‚æ¶ˆèä½ç½®ã€æŒç»­æ—¶é—´ã€æ¸©åº¦ã€åŠŸç‡å’Œå‹åŠ›ï¼‰ï¼Œæ¨¡æ‹Ÿæœ¯åç–¤ç—•å½¢æˆå¹¶ç”Ÿæˆç›¸åº”çš„é¢„æµ‹å›¾åƒã€‚è¯¥æ¡†æ¶èƒ½å¤ŸåŸºäºæ¨¡æ‹Ÿç»“æœé¢„æµ‹AFçš„å¤å‘é£é™©ï¼Œå¹¶å¼•å…¥ä¼˜åŒ–æ–¹æ¡ˆå¯¹æ‰‹æœ¯å‚æ•°è¿›è¡Œç²¾ç»†åŒ–è°ƒæ•´ï¼Œä»¥å®ç°é¢„æµ‹é£é™©çš„æœ€å°åŒ–ã€‚è¯¥æ–¹æ³•é‡‡ç”¨äº†å¤„ç†å¿ƒæˆ¿2.5Dè¡¨å¾çš„å¤šæ¨¡æ€ã€å¤šè§†è§’ç”Ÿæˆå™¨ï¼Œå®éªŒè¯„ä¼°æ˜¾ç¤ºSOFAèƒ½å‡†ç¡®åˆæˆæœ¯åå½±åƒï¼Œä¸”å…¶ä¼˜åŒ–æ–¹æ¡ˆä½¿æ¨¡å‹é¢„æµ‹çš„å¤å‘é£é™©é™ä½äº†22.18%ã€‚ä½œä¸ºé¦–ä¸ªæ•´åˆäº†æ‰‹æœ¯æ•ˆæœæ¨¡æ‹Ÿã€å¤å‘é¢„æµ‹å’Œå‚æ•°ä¼˜åŒ–çš„æ¡†æ¶ï¼ŒSOFAä¸ºå®ç°ä¸ªæ€§åŒ–å¿ƒæˆ¿é¢¤åŠ¨æ¶ˆèæ‰‹æœ¯æä¾›äº†åˆ›æ–°çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at MICCAI 2025. This is the author's original preprint",
      "pdf_url": "https://arxiv.org/pdf/2508.07621v1",
      "published_date": "2025-08-11 05:01:54 UTC",
      "updated_date": "2025-08-11 05:01:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:06:54.249642+00:00"
    },
    {
      "arxiv_id": "2508.07617v1",
      "title": "On the Limits of Selective AI Prediction: A Case Study in Clinical Decision Making",
      "title_zh": "è®ºé€‰æ‹©æ€§AIé¢„æµ‹çš„å±€é™æ€§ï¼šä¸€é¡¹å…³äºä¸´åºŠå†³ç­–çš„æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Sarah Jabbour",
        "David Fouhey",
        "Nikola Banovic",
        "Stephanie D. Shepard",
        "Ella Kazerooni",
        "Michael W. Sjoding",
        "Jenna Wiens"
      ],
      "abstract": "AI has the potential to augment human decision making. However, even high-performing models can produce inaccurate predictions when deployed. These inaccuracies, combined with automation bias, where humans overrely on AI predictions, can result in worse decisions. Selective prediction, in which potentially unreliable model predictions are hidden from users, has been proposed as a solution. This approach assumes that when AI abstains and informs the user so, humans make decisions as they would without AI involvement. To test this assumption, we study the effects of selective prediction on human decisions in a clinical context. We conducted a user study of 259 clinicians tasked with diagnosing and treating hospitalized patients. We compared their baseline performance without any AI involvement to their AI-assisted accuracy with and without selective prediction. Our findings indicate that selective prediction mitigates the negative effects of inaccurate AI in terms of decision accuracy. Compared to no AI assistance, clinician accuracy declined when shown inaccurate AI predictions (66% [95% CI: 56%-75%] vs. 56% [95% CI: 46%-66%]), but recovered under selective prediction (64% [95% CI: 54%-73%]). However, while selective prediction nearly maintains overall accuracy, our results suggest that it alters patterns of mistakes: when informed the AI abstains, clinicians underdiagnose (18% increase in missed diagnoses) and undertreat (35% increase in missed treatments) compared to no AI input at all. Our findings underscore the importance of empirically validating assumptions about how humans engage with AI within human-AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é€‰æ‹©æ€§AIé¢„æµ‹(Selective Prediction)åœ¨ä¸´åºŠå†³ç­–ä¸­çš„å±€é™æ€§ï¼Œæ—¨åœ¨è§£å†³AIé¢„æµ‹å¤±å‡†åŠè‡ªåŠ¨åŒ–åè§(automation bias)å¯¼è‡´çš„å†³ç­–è´¨é‡ä¸‹é™é—®é¢˜ã€‚é€šè¿‡å¯¹259åä¸´åºŠåŒ»ç”Ÿçš„ç”¨æˆ·ç ”ç©¶ï¼Œè¯¥å®éªŒå¯¹æ¯”äº†åŸºå‡†è¡¨ç°ã€AIè¾…åŠ©ä»¥åŠåœ¨é€‰æ‹©æ€§é¢„æµ‹æœºåˆ¶ä¸‹çš„å†³ç­–å‡†ç¡®åº¦ã€‚ç ”ç©¶å‘ç°ï¼Œé€‰æ‹©æ€§é¢„æµ‹èƒ½æœ‰æ•ˆç¼“è§£ä¸å‡†ç¡®AIå¸¦æ¥çš„è´Ÿé¢å½±å“ï¼Œä½¿åŒ»ç”Ÿå‡†ç¡®ç‡ä»56%å›å‡è‡³æ¥è¿‘åŸºå‡†æ°´å¹³çš„64%ã€‚ç„¶è€Œï¼Œå®éªŒç»“æœæ­ç¤ºè¯¥æœºåˆ¶æ”¹å˜äº†é”™è¯¯çš„åˆ†å¸ƒæ¨¡å¼ï¼šå½“AIå‘ŠçŸ¥å¼ƒæƒ(abstain)æ—¶ï¼ŒåŒ»ç”Ÿçš„æ¼è¯Š(underdiagnose)å’Œæ¼æ²»(undertreat)ç‡åˆ†åˆ«æ¯”å®Œå…¨æ²¡æœ‰AIä»‹å…¥æ—¶å¢åŠ äº†18%å’Œ35%ã€‚è¿™ä¸€å‘ç°æŒ‘æˆ˜äº†â€œAIå¼ƒæƒåäººç±»è¡¨ç°ä¼šå›å½’åŸºå‡†â€çš„å‡è®¾ï¼Œå¹¶å¼ºè°ƒäº†åœ¨äººæœºåä½œç³»ç»Ÿä¸­å®è¯éªŒè¯äººç±»å¦‚ä½•ä¸AIäº’åŠ¨çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "14 pages, 10 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.07617v1",
      "published_date": "2025-08-11 04:53:13 UTC",
      "updated_date": "2025-08-11 04:53:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:06:55.748801+00:00"
    },
    {
      "arxiv_id": "2508.07616v2",
      "title": "ThinkTuning: Instilling Cognitive Reflections without Distillation",
      "title_zh": "ThinkTuningï¼šæ— éœ€è’¸é¦å³å¯æ¤å…¥è®¤çŸ¥åæ€èƒ½åŠ›",
      "authors": [
        "Aswin RRV",
        "Jacob Dineen",
        "Divij Handa",
        "Md Nayem Uddin",
        "Mihir Parmar",
        "Chitta Baral",
        "Ben Zhou"
      ],
      "abstract": "Recent advances in test-time scaling have led to the emergence of thinking LLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL drives this self-improvement paradigm, a recent study (Gandhi et al., 2025) shows that RL alone does not truly instill these new reasoning abilities - it merely draws out behaviors already present in the base models. This raises a question: How can we train the models that don't exhibit such thinking behavior to develop it in the first place? To this end, we propose ThinkTuning, a GRPO-based interactive training approach where we augment the rollouts of a student model with the guidance from a teacher model. A simple idea from classroom practice inspires our method: a teacher poses a problem, lets the student try an answer, then gives corrective feedback -- enough to point the mind in the right direction and then show the solution. Each piece of feedback reshapes the student's thoughts, leading them to arrive at the correct solution. Similarly, we find that this type of implicit supervision through feedback from a teacher model of the same size improves the reasoning capabilities of the student model. In particular, on average, our method shows a 3.85% improvement over zero-shot baselines across benchmarks, and on MATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements over the vanilla-GRPO baseline. Source code is available at https://github.com/3rdAT/ThinkTuning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ThinkTuningï¼Œä¸€ç§æ—¨åœ¨ä¸ä¾èµ–è’¸é¦(Distillation)çš„æƒ…å†µä¸‹ä¸ºå¤§è¯­è¨€æ¨¡å‹æ³¨å…¥è®¤çŸ¥åæ€èƒ½åŠ›çš„è®­ç»ƒæ–¹æ³•ã€‚é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (RL)å¯èƒ½ä»…èƒ½æŒ–æ˜æ¨¡å‹æ—¢æœ‰è¡Œä¸ºè€Œéä¼ æˆæ–°æ¨ç†èƒ½åŠ›çš„å±€é™ï¼ŒThinkTuning é‡‡ç”¨äº†ä¸€ç§åŸºäº GRPO çš„äº¤äº’å¼è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡æ•™å¸ˆæ¨¡å‹çš„æŒ‡å¯¼æ¥å¢å¼ºå­¦ç”Ÿæ¨¡å‹çš„ rollout è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•å—è¯¾å ‚æ•™å­¦å¯å‘ï¼Œç”±æ•™å¸ˆæ¨¡å‹é’ˆå¯¹å­¦ç”Ÿæ¨¡å‹çš„åˆæ­¥å°è¯•æä¾›çº æ­£æ€§åé¦ˆï¼Œå¼•å¯¼å­¦ç”Ÿæ¨¡å‹è°ƒæ•´æ€è·¯å¹¶æœ€ç»ˆå¾—å‡ºæ­£ç¡®ç­”æ¡ˆã€‚ç ”ç©¶å‘ç°ï¼Œè¿™ç§é€šè¿‡åé¦ˆå®ç°çš„éšå¼ç›‘ç£èƒ½æœ‰æ•ˆæå‡å­¦ç”Ÿæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå³ä¾¿æ•™å¸ˆæ¨¡å‹ä¸å­¦ç”Ÿæ¨¡å‹è§„æ¨¡å®Œå…¨ç›¸åŒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒThinkTuning åœ¨ MATH-500ã€AIME å’Œ GPQA-Diamond åŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«æ¯” vanilla-GRPO åŸºçº¿æé«˜äº† 2.08%ã€2.23% å’Œ 3.99%ï¼Œå…¨åŸºå‡†å¹³å‡æ€§èƒ½æå‡è¾¾ 3.85%ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "EMNLP 2025 (Main Conference)",
      "pdf_url": "https://arxiv.org/pdf/2508.07616v2",
      "published_date": "2025-08-11 04:51:43 UTC",
      "updated_date": "2025-08-21 06:17:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:06:56.843130+00:00"
    },
    {
      "arxiv_id": "2508.07602v2",
      "title": "HGMF: A Hierarchical Gaussian Mixture Framework for Scalable Tool Invocation within the Model Context Protocol",
      "title_zh": "HGMFï¼šModel Context Protocol ä¸­ç”¨äºå¯æ‰©å±•å·¥å…·è°ƒç”¨çš„åˆ†å±‚é«˜æ–¯æ··åˆæ¡†æ¶",
      "authors": [
        "Wenpeng Xing",
        "Zhipeng Chen",
        "Changting Lin",
        "Meng Han"
      ],
      "abstract": "Invoking external tools enables Large Language Models (LLMs) to perform complex, real-world tasks, yet selecting the correct tool from large, hierarchically-structured libraries remains a significant challenge. The limited context windows of LLMs and noise from irrelevant options often lead to low selection accuracy and high computational costs. To address this, we propose the Hierarchical Gaussian Mixture Framework (HGMF), a probabilistic pruning method for scalable tool invocation. HGMF first maps the user query and all tool descriptions into a unified semantic space. The framework then operates in two stages: it clusters servers using a Gaussian Mixture Model (GMM) and filters them based on the query's likelihood. Subsequently, it applies the same GMM-based clustering and filtering to the tools associated with the selected servers. This hierarchical process produces a compact, high-relevance candidate set, simplifying the final selection task for the LLM. Experiments on a public dataset show that HGMF significantly improves tool selection accuracy while reducing inference latency, confirming the framework's scalability and effectiveness for large-scale tool libraries.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HGMFï¼ˆHierarchical Gaussian Mixture Frameworkï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤§è§„æ¨¡å±‚æ¬¡åŒ–å·¥å…·åº“æ—¶é¢ä¸´çš„é€‰æ‹©å‡†ç¡®ç‡ä½å’Œè®¡ç®—å¼€é”€å¤§é—®é¢˜çš„æ¦‚ç‡ä¿®å‰ªæ¡†æ¶ã€‚HGMFé¦–å…ˆå°†ç”¨æˆ·æŸ¥è¯¢å’Œå·¥å…·æè¿°æ˜ å°„è‡³ç»Ÿä¸€çš„è¯­ä¹‰ç©ºé—´ï¼Œé€šè¿‡ä¸¤é˜¶æ®µçš„é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGaussian Mixture Model, GMMï¼‰è¿›è¡Œå¤„ç†ã€‚ç¬¬ä¸€é˜¶æ®µæ ¹æ®æŸ¥è¯¢çš„å¯èƒ½æ€§å¯¹æœåŠ¡å™¨è¿›è¡Œèšç±»å’Œè¿‡æ»¤ï¼Œç¬¬äºŒé˜¶æ®µåˆ™å¯¹é€‰å®šæœåŠ¡å™¨å†…çš„å·¥å…·æ‰§è¡Œç›¸åŒçš„èšç±»ç­›é€‰ï¼Œä»è€Œç”Ÿæˆç´§å‡‘ä¸”é«˜ç›¸å…³çš„å€™é€‰å·¥å…·é›†ã€‚è¿™ç§å±‚æ¬¡åŒ–ä¿®å‰ªæœºåˆ¶æ˜¾è‘—å‡è½»äº†LLMçš„æ¨ç†è´Ÿæ‹…ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´é«˜æ•ˆåœ°å®Œæˆæœ€ç»ˆçš„å·¥å…·è°ƒç”¨å†³ç­–ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒHGMFåœ¨å…¬å…±æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†å·¥å…·é€‰æ‹©çš„å‡†ç¡®ç‡å¹¶é™ä½äº†æ¨ç†å»¶è¿Ÿï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶åœ¨å¤§è§„æ¨¡å·¥å…·åº“åœºæ™¯ä¸‹çš„å¯æ‰©å±•æ€§ä¸æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07602v2",
      "published_date": "2025-08-11 04:13:06 UTC",
      "updated_date": "2026-01-08 09:11:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:06:53.957313+00:00"
    },
    {
      "arxiv_id": "2508.07597v2",
      "title": "ShoulderShot: Generating Over-the-Shoulder Dialogue Videos",
      "title_zh": "ShoulderShotï¼šè¿‡è‚©é•œå¤´å¯¹è¯è§†é¢‘ç”Ÿæˆ",
      "authors": [
        "Yuang Zhang",
        "Junqi Cheng",
        "Haoyu Zhao",
        "Jiaxi Gu",
        "Fangyuan Zou",
        "Zenghui Lu",
        "Peng Shu"
      ],
      "abstract": "Over-the-shoulder dialogue videos are essential in films, short dramas, and advertisements, providing visual variety and enhancing viewers' emotional connection. Despite their importance, such dialogue scenes remain largely underexplored in video generation research. The main challenges include maintaining character consistency across different shots, creating a sense of spatial continuity, and generating long, multi-turn dialogues within limited computational budgets. Here, we present ShoulderShot, a framework that combines dual-shot generation with looping video, enabling extended dialogues while preserving character consistency. Our results demonstrate capabilities that surpass existing methods in terms of shot-reverse-shot layout, spatial continuity, and flexibility in dialogue length, thereby opening up new possibilities for practical dialogue video generation. Videos and comparisons are available at https://shouldershot.github.io.",
      "tldr_zh": "è¯¥ç ”ç©¶å…³æ³¨åœ¨ç”µå½±ã€çŸ­å‰§å’Œå¹¿å‘Šä¸­è‡³å…³é‡è¦çš„è¿‡è‚©é•œå¤´ (over-the-shoulder) å¯¹è¯è§†é¢‘ç”Ÿæˆï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è§†é¢‘ç”Ÿæˆç ”ç©¶åœ¨è¯¥é¢†åŸŸæ¢ç´¢ä¸è¶³çš„é—®é¢˜ã€‚è®ºæ–‡æŒ‡å‡ºäº†ç”Ÿæˆæ­¤ç±»åœºæ™¯é¢ä¸´çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åœ¨ä¸åŒé•œå¤´é—´ç»´æŒè§’è‰²ä¸€è‡´æ€§ (character consistency)ã€æ„å»ºç©ºé—´è¿ç»­æ€§ (spatial continuity) ä»¥åŠåœ¨æœ‰é™è®¡ç®—é¢„ç®—å†…ç”Ÿæˆé•¿ç¯‡å¤šè½®å¯¹è¯ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº† ShoulderShot æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç»“åˆåŒé•œå¤´ç”Ÿæˆ (dual-shot generation) ä¸å¾ªç¯è§†é¢‘ (looping video) æŠ€æœ¯ï¼Œåœ¨ä¿ç•™è§’è‰²ä¸€è‡´æ€§çš„åŒæ—¶å®ç°äº†é•¿ç¯‡å¯¹è¯çš„çµæ´»æ‰©å±•ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒShoulderShot åœ¨æ­£åæ‰“é•œå¤´ (shot-reverse-shot) å¸ƒå±€ã€ç©ºé—´è¿ç»­æ€§å’Œå¯¹è¯é•¿åº¦é€‚åº”æ€§æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºå®é™…å¯¹è¯è§†é¢‘ç”Ÿæˆæä¾›äº†å…·å¤‡å®ç”¨ä»·å€¼çš„æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07597v2",
      "published_date": "2025-08-11 03:56:23 UTC",
      "updated_date": "2025-08-15 09:27:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:06:58.286897+00:00"
    },
    {
      "arxiv_id": "2508.07592v2",
      "title": "IBPS: Indian Bail Prediction System",
      "title_zh": "IBPSï¼šå°åº¦ä¿é‡Šé¢„æµ‹ç³»ç»Ÿ",
      "authors": [
        "Puspesh Kumar Srivastava",
        "Uddeshya Raj",
        "Praveen Patel",
        "Shubham Kumar Nigam",
        "Noel Shallum",
        "Arnab Bhattacharya"
      ],
      "abstract": "Bail decisions are among the most frequently adjudicated matters in Indian courts, yet they remain plagued by subjectivity, delays, and inconsistencies. With over 75% of India's prison population comprising undertrial prisoners, many from socioeconomically disadvantaged backgrounds, the lack of timely and fair bail adjudication exacerbates human rights concerns and contributes to systemic judicial backlog. In this paper, we present the Indian Bail Prediction System (IBPS), an AI-powered framework designed to assist in bail decision-making by predicting outcomes and generating legally sound rationales based solely on factual case attributes and statutory provisions. We curate and release a large-scale dataset of 150,430 High Court bail judgments, enriched with structured annotations such as age, health, criminal history, crime category, custody duration, statutes, and judicial reasoning. We fine-tune a large language model using parameter-efficient techniques and evaluate its performance across multiple configurations, with and without statutory context, and with RAG. Our results demonstrate that models fine-tuned with statutory knowledge significantly outperform baselines, achieving strong accuracy and explanation quality, and generalize well to a test set independently annotated by legal experts. IBPS offers a transparent, scalable, and reproducible solution to support data-driven legal assistance, reduce bail delays, and promote procedural fairness in the Indian judicial system.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å°åº¦æ³•é™¢ä¿é‡Šè£å†³ä¸­å­˜åœ¨çš„ä¸ªäººä¸»è§‚æ€§ã€ç¨‹åºå»¶è¯¯å’Œä¸ä¸€è‡´æ€§ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†å°åº¦ä¿é‡Šé¢„æµ‹ç³»ç»Ÿ (Indian Bail Prediction System, IBPS)ã€‚IBPS æ˜¯ä¸€ä¸ªåŸºäºäººå·¥æ™ºèƒ½çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿä»…ä¾æ®æ¡ˆä»¶çš„äº‹å®å±æ€§å’Œæ³•å®šæ¡æ¬¾é¢„æµ‹ä¿é‡Šç»“æœï¼Œå¹¶ç”Ÿæˆå…·æœ‰æ³•å¾‹æ•ˆåŠ›çš„ä¾æ®è¯´æ˜ã€‚ä¸ºäº†æ„å»ºè¯¥ç³»ç»Ÿï¼Œç ”ç©¶äººå‘˜å¼€å‘å¹¶å‘å¸ƒäº†ä¸€ä¸ªåŒ…å« 150,430 ä»½é«˜ç­‰æ³•é™¢ä¿é‡Šåˆ¤å†³çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å¹´é¾„ã€å¥åº·çŠ¶å†µã€çŠ¯ç½ªå†å²å’Œç¾æŠ¼æ—¶é•¿ç­‰ä¸°å¯Œçš„ç»“æ„åŒ–æ³¨é‡Šã€‚è¯¥ç ”ç©¶é‡‡ç”¨å‚æ•°é«˜æ•ˆ (parameter-efficient) æŠ€æœ¯å¾®è°ƒäº†å¤§å‹è¯­è¨€æ¨¡å‹ (LLM)ï¼Œå¹¶éªŒè¯äº†æ³•å®šèƒŒæ™¯ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) æŠ€æœ¯å¯¹æ€§èƒ½çš„æ˜¾è‘—æå‡ä½œç”¨ã€‚å®éªŒè¯æ˜ï¼Œèå…¥æ³•å®šçŸ¥è¯†çš„æ¨¡å‹åœ¨é¢„æµ‹å‡†ç¡®æ€§å’Œè§£é‡Šè´¨é‡ä¸Šå‡ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œä¸”èƒ½å¤Ÿå¾ˆå¥½åœ°æ³›åŒ–è‡³ç”±æ³•å¾‹ä¸“å®¶ç‹¬ç«‹æ ‡æ³¨çš„æµ‹è¯•é›†ã€‚IBPS æä¾›äº†ä¸€ç§é€æ˜ã€å¯æ‰©å±•ä¸”å¯å¤ç°çš„è‡ªåŠ¨åŒ–æ³•å¾‹è¾…åŠ©æ–¹æ¡ˆï¼Œæ—¨åœ¨é€šè¿‡æ•°æ®é©±åŠ¨çš„æ–¹å¼å‡å°‘ä¿é‡Šå»¶è¯¯å¹¶ä¿ƒè¿›å°åº¦å¸æ³•ç³»ç»Ÿçš„ç¨‹åºå…¬å¹³ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07592v2",
      "published_date": "2025-08-11 03:44:17 UTC",
      "updated_date": "2025-08-21 11:32:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:07:19.748937+00:00"
    },
    {
      "arxiv_id": "2508.07586v1",
      "title": "Optimization of Private Semantic Communication Performance: An Uncooperative Covert Communication Method",
      "title_zh": "éšç§è¯­ä¹‰é€šä¿¡æ€§èƒ½ä¼˜åŒ–ï¼šä¸€ç§éåä½œéšè”½é€šä¿¡æ–¹æ³•",
      "authors": [
        "Wenjing Zhang",
        "Ye Hu",
        "Tao Luo",
        "Zhilong Zhang",
        "Mingzhe Chen"
      ],
      "abstract": "In this paper, a novel covert semantic communication framework is investigated. Within this framework, a server extracts and transmits the semantic information, i.e., the meaning of image data, to a user over several time slots. An attacker seeks to detect and eavesdrop the semantic transmission to acquire details of the original image. To avoid data meaning being eavesdropped by an attacker, a friendly jammer is deployed to transmit jamming signals to interfere the attacker so as to hide the transmitted semantic information. Meanwhile, the server will strategically select time slots for semantic information transmission. Due to limited energy, the jammer will not communicate with the server and hence the server does not know the transmit power of the jammer. Therefore, the server must jointly optimize the semantic information transmitted at each time slot and the corresponding transmit power to maximize the privacy and the semantic information transmission quality of the user. To solve this problem, we propose a prioritised sampling assisted twin delayed deep deterministic policy gradient algorithm to jointly determine the transmitted semantic information and the transmit power per time slot without the communications between the server and the jammer. Compared to standard reinforcement learning methods, the propose method uses an additional Q network to estimate Q values such that the agent can select the action with a lower Q value from the two Q networks thus avoiding local optimal action selection and estimation bias of Q values. Simulation results show that the proposed algorithm can improve the privacy and the semantic information transmission quality by up to 77.8% and 14.3% compared to the traditional reinforcement learning methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¸€ç§æ–°å‹çš„éšè”½è¯­ä¹‰é€šä¿¡ (Covert Semantic Communication) æ¡†æ¶ï¼Œæ—¨åœ¨ä¿æŠ¤å›¾åƒæ•°æ®åœ¨ä¼ è¾“è¿‡ç¨‹ä¸­ä¸è¢«æ”»å‡»è€…æ£€æµ‹å’Œçªƒå¬ã€‚ä¸ºäº†è§£å†³æ•°æ®å«ä¹‰æ³„éœ²é—®é¢˜ï¼Œç³»ç»Ÿéƒ¨ç½²äº†ä¸€ä¸ªå‘é€å¹²æ‰°ä¿¡å·çš„å‹å¥½å¹²æ‰°æœº (Friendly Jammer) æ¥å¹²æ‰°æ”»å‡»è€…ï¼Œè€ŒæœåŠ¡å™¨åˆ™éœ€åœ¨å—é™èƒ½é‡å’Œç¼ºä¹ä¸å¹²æ‰°æœºé€šä¿¡çš„éåˆä½œç¯å¢ƒä¸‹ï¼Œç­–ç•¥æ€§åœ°é€‰æ‹©ä¼ è¾“æ—¶éš™ã€‚æœåŠ¡å™¨å¿…é¡»è”åˆä¼˜åŒ–æ¯ä¸ªæ—¶éš™ä¼ è¾“çš„è¯­ä¹‰ä¿¡æ¯é‡åŠå…¶å‘å°„åŠŸç‡ï¼Œä»¥åœ¨æœ€å¤§åŒ–éšç§ä¿æŠ¤çš„åŒæ—¶ä¿è¯ç”¨æˆ·çš„è¯­ä¹‰ä¼ è¾“è´¨é‡ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºä¼˜å…ˆé‡‡æ ·è¾…åŠ©çš„åŒå»¶è¿Ÿæ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ (Prioritised sampling assisted Twin Delayed Deep Deterministic Policy Gradient) ç®—æ³•ã€‚è¯¥ç®—æ³•é€šè¿‡å¼•å…¥é¢å¤–çš„ Q ç½‘ç»œæ¥å‡†ç¡®ä¼°ç®— Q å€¼ï¼Œæœ‰æ•ˆé¿å…äº†åŠ¨ä½œé€‰æ‹©çš„å±€éƒ¨æœ€ä¼˜å’Œä¼°ç®—åå·®ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹  (Reinforcement Learning) æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ¡ˆå°†éšç§ä¿æŠ¤æ€§èƒ½å’Œè¯­ä¹‰ä¿¡æ¯ä¼ è¾“è´¨é‡åˆ†åˆ«æå‡äº† 77.8% å’Œ 14.3%ã€‚",
      "categories": [
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07586v1",
      "published_date": "2025-08-11 03:31:05 UTC",
      "updated_date": "2025-08-11 03:31:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:07:18.254612+00:00"
    },
    {
      "arxiv_id": "2508.08338v1",
      "title": "ImageDDI: Image-enhanced Molecular Motif Sequence Representation for Drug-Drug Interaction Prediction",
      "title_zh": "ImageDDIï¼šåŸºäºå›¾åƒå¢å¼ºåˆ†å­åŸºå…ƒåºåˆ—è¡¨ç¤ºçš„è¯ç‰©ç›¸äº’ä½œç”¨é¢„æµ‹",
      "authors": [
        "Yuqin He",
        "Tengfei Ma",
        "Chaoyi Li",
        "Pengsen Ma",
        "Hongxin Xiang",
        "Jianmin Wang",
        "Yiping Liu",
        "Bosheng Song",
        "Xiangxiang Zeng"
      ],
      "abstract": "To mitigate the potential adverse health effects of simultaneous multi-drug use, including unexpected side effects and interactions, accurately identifying and predicting drug-drug interactions (DDIs) is considered a crucial task in the field of deep learning. Although existing methods have demonstrated promising performance, they suffer from the bottleneck of limited functional motif-based representation learning, as DDIs are fundamentally caused by motif interactions rather than the overall drug structures. In this paper, we propose an Image-enhanced molecular motif sequence representation framework for \\textbf{DDI} prediction, called ImageDDI, which represents a pair of drugs from both global and local structures. Specifically, ImageDDI tokenizes molecules into functional motifs. To effectively represent a drug pair, their motifs are combined into a single sequence and embedded using a transformer-based encoder, starting from the local structure representation. By leveraging the associations between drug pairs, ImageDDI further enhances the spatial representation of molecules using global molecular image information (e.g. texture, shadow, color, and planar spatial relationships). To integrate molecular visual information into functional motif sequence, ImageDDI employs Adaptive Feature Fusion, enhancing the generalization of ImageDDI by dynamically adapting the fusion process of feature representations. Experimental results on widely used datasets demonstrate that ImageDDI outperforms state-of-the-art methods. Moreover, extensive experiments show that ImageDDI achieved competitive performance in both 2D and 3D image-enhanced scenarios compared to other models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯ç‰©ç›¸äº’ä½œç”¨(Drug-Drug Interaction, DDI)é¢„æµ‹ä¸­åŠŸèƒ½åŸºå›¢(motif)è¡¨å¾å­¦ä¹ ä¸è¶³çš„ç“¶é¢ˆï¼Œæå‡ºäº†åä¸ºImageDDIçš„å›¾åƒå¢å¼ºåˆ†å­åŸºå›¢åºåˆ—è¡¨å¾æ¡†æ¶ã€‚ImageDDIé€šè¿‡å°†åˆ†å­æ ‡è®°åŒ–ä¸ºåŠŸèƒ½åŸºå›¢(functional motifs)ï¼Œå¹¶åˆ©ç”¨Transformerç¼–ç å™¨å°†è¿™äº›åŸºå›¢ç»„åˆæˆåºåˆ—ï¼Œå®ç°äº†å¯¹è¯ç‰©å±€éƒ¨ç»“æ„çš„æœ‰æ•ˆå»ºæ¨¡ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¼ºåŒ–ç©ºé—´è¡¨å¾ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†å…¨å±€åˆ†å­å›¾åƒä¿¡æ¯ï¼Œé€šè¿‡æ•æ‰çº¹ç†ã€é˜´å½±ã€é¢œè‰²åŠå¹³é¢ç©ºé—´å…³ç³»æ¥å¢å¼ºè¯ç‰©å¯¹çš„è¡¨å¾èƒ½åŠ›ã€‚ç ”ç©¶è¿˜é‡‡ç”¨äº†è‡ªé€‚åº”ç‰¹å¾èåˆ(Adaptive Feature Fusion)æŠ€æœ¯ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´ç‰¹å¾æ•´åˆè¿‡ç¨‹æå‡äº†æ¨¡å‹çš„æ³›åŒ–æ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒImageDDIåœ¨ä¸»æµæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•(state-of-the-art)ï¼Œåœ¨2Då’Œ3Då›¾åƒå¢å¼ºåœºæ™¯ä¸‹å‡å±•ç°å‡ºäº†å“è¶Šçš„é¢„æµ‹æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted By Information Fusion",
      "pdf_url": "https://arxiv.org/pdf/2508.08338v1",
      "published_date": "2025-08-11 03:26:50 UTC",
      "updated_date": "2025-08-11 03:26:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:07:32.686830+00:00"
    },
    {
      "arxiv_id": "2508.07575v1",
      "title": "MCPToolBench++: A Large Scale AI Agent Model Context Protocol MCP Tool Use Benchmark",
      "title_zh": "MCPToolBench++ï¼šå¤§è§„æ¨¡ AI æ™ºèƒ½ä½“æ¨¡å‹ä¸Šä¸‹æ–‡åè®® (MCP) å·¥å…·è°ƒç”¨åŸºå‡†",
      "authors": [
        "Shiqing Fan",
        "Xichen Ding",
        "Liang Zhang",
        "Linjian Mo"
      ],
      "abstract": "LLMs' capabilities are enhanced by using function calls to integrate various data sources or API results into the context window. Typical tools include search, web crawlers, maps, financial data, file systems, and browser usage, etc. Integrating these data sources or functions requires a standardized method. The Model Context Protocol (MCP) provides a standardized way to supply context to LLMs. However, the evaluation of LLMs and AI Agents' MCP tool use abilities suffer from several issues. First, there's a lack of comprehensive datasets or benchmarks to evaluate various MCP tools. Second, the diverse formats of response from MCP tool call execution further increase the difficulty of evaluation. Additionally, unlike existing tool-use benchmarks with high success rates in functions like programming and math functions, the success rate of real-world MCP tool is not guaranteed and varies across different MCP servers. Furthermore, the LLMs' context window also limits the number of available tools that can be called in a single run, because the textual descriptions of tool and the parameters have long token length for an LLM to process all at once. To help address the challenges of evaluating LLMs' performance on calling MCP tools, we propose MCPToolBench++, a large-scale, multi-domain AI Agent tool use benchmark. As of July 2025, this benchmark is build upon marketplace of over 4k MCP servers from more than 40 categories, collected from the MCP marketplaces and GitHub communities. The datasets consist of both single-step and multi-step tool calls across different categories. We evaluated SOTA LLMs with agentic abilities on this benchmark and reported the results.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MCPToolBench++ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€å¤šé¢†åŸŸçš„AI Agentå·¥å…·è°ƒç”¨åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨åº”å¯¹Model Context Protocol (MCP)å·¥å…·åœ¨è¯„ä¼°ä¸­é¢ä¸´çš„æ•°æ®é›†åŒ®ä¹ã€å“åº”æ ¼å¼å¼‚æ„åŠç°å®ç¯å¢ƒæˆåŠŸç‡æ³¢åŠ¨ç­‰æŒ‘æˆ˜ã€‚è¯¥åŸºå‡†è§£å†³äº†LLMsåœ¨å¤„ç†é•¿æ–‡æœ¬å·¥å…·æè¿°æ—¶å—é™äºä¸Šä¸‹æ–‡çª—å£(Context Window)çš„é—®é¢˜ï¼Œå¹¶é’ˆå¯¹å¤šæ ·çš„MCPæœåŠ¡å™¨æä¾›äº†æ ‡å‡†åŒ–çš„è¡¡é‡æŒ‡æ ‡ã€‚ç ”ç©¶å›¢é˜Ÿä»MCPå¸‚åœºå’ŒGitHubç¤¾åŒºæ•´åˆäº†è¶…è¿‡4000ä¸ªMCPæœåŠ¡å™¨ï¼Œæ¶µç›–40å¤šä¸ªç±»åˆ«ï¼Œæ„å»ºäº†åŒ…å«å•æ­¥ä¸å¤šæ­¥è°ƒç”¨çš„ç»¼åˆæ•°æ®é›†ã€‚é€šè¿‡å¯¹å…·å¤‡æ™ºèƒ½ä½“èƒ½åŠ›çš„SOTA LLMsè¿›è¡Œæ€§èƒ½è¯„ä¼°ï¼Œè¯¥ç ”ç©¶é‡åŒ–äº†å½“å‰æ¨¡å‹åœ¨å¤æ‚çœŸå®åœºæ™¯ä¸‹çš„å·¥å…·åˆ©ç”¨ç‡ã€‚MCPToolBench++çš„æ¨å‡ºä¸ºè¯„ä¼°å’Œæå‡AI Agentåœ¨æ ‡å‡†åŒ–åè®®ä¸‹çš„ç”Ÿæ€é›†æˆèƒ½åŠ›æä¾›äº†å…³é”®çš„åŸºç¡€è®¾æ–½ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Benchmarks and Source Code Released",
      "pdf_url": "https://arxiv.org/pdf/2508.07575v1",
      "published_date": "2025-08-11 03:16:02 UTC",
      "updated_date": "2025-08-11 03:16:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:07:24.855297+00:00"
    },
    {
      "arxiv_id": "2508.09210v1",
      "title": "MME-Emotion: A Holistic Evaluation Benchmark for Emotional Intelligence in Multimodal Large Language Models",
      "title_zh": "MME-Emotionï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æƒ…æ„Ÿæ™ºèƒ½çš„å…¨é¢è¯„ä¼°åŸºå‡†",
      "authors": [
        "Fan Zhang",
        "Zebang Cheng",
        "Chong Deng",
        "Haoxuan Li",
        "Zheng Lian",
        "Qian Chen",
        "Huadai Liu",
        "Wen Wang",
        "Yi-Fan Zhang",
        "Renrui Zhang",
        "Ziyu Guo",
        "Zhihong Zhu",
        "Hao Wu",
        "Haixin Wang",
        "Yefeng Zheng",
        "Xiaojiang Peng",
        "Xian Wu",
        "Kun Wang",
        "Xiangang Li",
        "Jieping Ye",
        "Pheng-Ann Heng"
      ],
      "abstract": "Recent advances in multimodal large language models (MLLMs) have catalyzed transformative progress in affective computing, enabling models to exhibit emergent emotional intelligence. Despite substantial methodological progress, current emotional benchmarks remain limited, as it is still unknown: (a) the generalization abilities of MLLMs across distinct scenarios, and (b) their reasoning capabilities to identify the triggering factors behind emotional states. To bridge these gaps, we present \\textbf{MME-Emotion}, a systematic benchmark that assesses both emotional understanding and reasoning capabilities of MLLMs, enjoying \\textit{scalable capacity}, \\textit{diverse settings}, and \\textit{unified protocols}. As the largest emotional intelligence benchmark for MLLMs, MME-Emotion contains over 6,000 curated video clips with task-specific questioning-answering (QA) pairs, spanning broad scenarios to formulate eight emotional tasks. It further incorporates a holistic evaluation suite with hybrid metrics for emotion recognition and reasoning, analyzed through a multi-agent system framework. Through a rigorous evaluation of 20 advanced MLLMs, we uncover both their strengths and limitations, yielding several key insights: \\ding{182} Current MLLMs exhibit unsatisfactory emotional intelligence, with the best-performing model achieving only $39.3\\%$ recognition score and $56.0\\%$ Chain-of-Thought (CoT) score on our benchmark. \\ding{183} Generalist models (\\emph{e.g.}, Gemini-2.5-Pro) derive emotional intelligence from generalized multimodal understanding capabilities, while specialist models (\\emph{e.g.}, R1-Omni) can achieve comparable performance through domain-specific post-training adaptation. By introducing MME-Emotion, we hope that it can serve as a foundation for advancing MLLMs' emotional intelligence in the future.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†MME-Emotionï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)æƒ…æ„Ÿæ™ºèƒ½çš„ç³»ç»Ÿæ€§è¯„ä¼°åŸºå‡†ï¼Œæ—¨åœ¨å¼¥è¡¥ç°æœ‰åŸºå‡†åœ¨æƒ…æ™¯æ³›åŒ–å’Œæƒ…æ„Ÿè¯±å‘å› ç´ æ¨ç†èƒ½åŠ›æ–¹é¢çš„ä¸è¶³ã€‚ä½œä¸ºç›®å‰è§„æ¨¡æœ€å¤§çš„æƒ…æ„Ÿæ™ºèƒ½åŸºå‡†ï¼ŒMME-EmotionåŒ…å«è¶…è¿‡6,000ä¸ªè§†é¢‘å‰ªè¾‘å’Œä»»åŠ¡ç‰¹å®šçš„é—®ç­”å¯¹ï¼Œæ¶µç›–å…«é¡¹æ ¸å¿ƒæƒ…æ„Ÿä»»åŠ¡ï¼Œå¹¶é‡‡ç”¨æ··åˆæŒ‡æ ‡è¿›è¡Œå…¨é¢è¡¡é‡ã€‚ç ”ç©¶é€šè¿‡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ¡†æ¶(multi-agent system framework)å¯¹20ä¸ªå…ˆè¿›æ¨¡å‹è¿›è¡Œäº†ä¸¥æ ¼æµ‹è¯•ï¼Œç»“æœæ˜¾ç¤ºå½“å‰MLLMsçš„æƒ…æ„Ÿæ™ºèƒ½æ°´å¹³ä»æœ‰å¾…æå‡ï¼Œè¡¨ç°æœ€ä½³çš„æ¨¡å‹åœ¨è¯†åˆ«å’Œé“¾å¼æ€ç»´(Chain-of-Thought)å¾—åˆ†ä¸Šåˆ†åˆ«ä»…ä¸º39.3%å’Œ56.0%ã€‚å®éªŒè¿˜å‘ç°ï¼ŒGemini-2.5-Proç­‰é€šç”¨æ¨¡å‹ä¾èµ–å…¶å¹¿ä¹‰çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼Œè€ŒR1-Omniç­‰ä¸“ç”¨æ¨¡å‹åˆ™èƒ½é€šè¿‡é¢†åŸŸç‰¹å®šçš„è®­ç»ƒåé€‚é…(post-training adaptation)è·å¾—ç›¸å½“çš„æ€§èƒ½ã€‚è¯¥åŸºå‡†çš„æå‡ºä¸ºæœªæ¥å¼€å‘å…·å¤‡é«˜æƒ…æ„Ÿæ™ºèƒ½çš„å¤šæ¨¡æ€æ¨¡å‹æä¾›äº†é‡è¦çš„å®éªŒåŸºç¡€å’Œè¯„ä¼°å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.09210v1",
      "published_date": "2025-08-11 03:14:55 UTC",
      "updated_date": "2025-08-11 03:14:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:07:26.446797+00:00"
    },
    {
      "arxiv_id": "2508.07571v2",
      "title": "Towards Theoretical Understanding of Transformer Test-Time Computing: Investigation on In-Context Linear Regression",
      "title_zh": "è¿ˆå‘ Transformer æµ‹è¯•æ—¶è®¡ç®—çš„ç†è®ºç†è§£ï¼šä¸Šä¸‹æ–‡çº¿æ€§å›å½’æ¢ç©¶",
      "authors": [
        "Xingwu Chen",
        "Miao Lu",
        "Beining Wu",
        "Difan Zou"
      ],
      "abstract": "Using more test-time computation during language model inference, such as generating more intermediate thoughts or sampling multiple candidate answers, has proven effective in significantly improving model performance. This paper takes an initial step toward bridging the gap between practical language model inference and theoretical transformer analysis by incorporating randomness and sampling. We focus on in-context linear regression with continuous/binary coefficients, where our framework simulates language model decoding through noise injection and binary coefficient sampling. Through this framework, we provide detailed analyses of widely adopted inference techniques. Supported by empirical results, our theoretical framework and analysis demonstrate the potential for offering new insights into understanding inference behaviors in real-world language models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Transformer åœ¨æµ‹è¯•æ—¶è®¡ç®— (Test-Time Computing) çš„ç†è®ºåŸºç¡€ï¼Œæ—¨åœ¨é€šè¿‡å¼•å…¥éšæœºæ€§å’Œé‡‡æ ·æ¥ç¼©å°å®é™…è¯­è¨€æ¨¡å‹æ¨ç†ä¸ç†è®ºåˆ†æä¹‹é—´çš„å·®è·ã€‚ä½œè€…é‡ç‚¹ç ”ç©¶äº†å…·æœ‰è¿ç»­æˆ–äºŒè¿›åˆ¶ç³»æ•°çš„æƒ…å¢ƒçº¿æ€§å›å½’ (In-Context Linear Regression) ä»»åŠ¡ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªé€šè¿‡æ³¨å…¥å™ªå£° (Noise Injection) å’ŒäºŒè¿›åˆ¶ç³»æ•°é‡‡æ · (Binary Coefficient Sampling) æ¥æ¨¡æ‹Ÿè¯­è¨€æ¨¡å‹è§£ç è¿‡ç¨‹çš„ç†è®ºæ¡†æ¶ã€‚åœ¨è¯¥æ¡†æ¶ä¸‹ï¼Œç ”ç©¶è€…å¯¹å½“å‰å¤§è¯­è¨€æ¨¡å‹ä¸­å¹¿æ³›é‡‡ç”¨çš„æ¨ç†æŠ€æœ¯è¿›è¡Œäº†è¯¦ç»†çš„ç†è®ºå‰–æã€‚å®éªŒç»“æœè¿›ä¸€æ­¥éªŒè¯äº†è¯¥ç†è®ºæ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å…¶åœ¨è§£é‡Šæ¨ç†è¡Œä¸ºæ–¹é¢çš„æ½œåŠ›ã€‚è¯¥å·¥ä½œä¸ºæ·±å…¥ç†è§£çœŸå®ä¸–ç•Œè¯­è¨€æ¨¡å‹çš„æµ‹è¯•æ—¶è®¡ç®—è¡¨ç°æä¾›äº†æ–°çš„è§è§£ï¼Œå¹¶ä¸ºæœªæ¥ä¼˜åŒ–æ¨ç†ç­–ç•¥å¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07571v2",
      "published_date": "2025-08-11 03:05:36 UTC",
      "updated_date": "2025-08-19 11:54:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:08:00.795166+00:00"
    },
    {
      "arxiv_id": "2508.07569v1",
      "title": "Retrieval-Augmented Multi-Agent System for Rapid Statement of Work Generation",
      "title_zh": "ç”¨äºå¿«é€Ÿç”Ÿæˆå·¥ä½œè¯´æ˜ä¹¦çš„æ£€ç´¢å¢å¼ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ",
      "authors": [
        "Amulya Suravarjhula",
        "Rashi Chandrashekhar Agrawal",
        "Sakshi Jayesh Patel",
        "Rahul Gupta"
      ],
      "abstract": "Drafting a Statement of Work (SOW) is a vital part of business and legal projects. It outlines key details like deliverables, timelines, responsibilities, and legal terms. However, creating these documents is often a slow and complex process. It usually involves multiple people, takes several days, and leaves room for errors or outdated content. This paper introduces a new AI-driven automation system that makes the entire SOW drafting process faster, easier, and more accurate. Instead of relying completely on humans, the system uses three intelligent components or 'agents' that each handle a part of the job. One agent writes the first draft, another checks if everything is legally correct, and the third agent formats the document and ensures everything is in order. Unlike basic online tools that just fill in templates, this system understands the meaning behind the content and customizes the SOW to match the needs of the project. It also checks legal compliance and formatting so that users can trust the result. The system was tested using real business examples. It was able to create a full SOW in under three minutes, compared to several hours or days using manual methods. It also performed well in accuracy and quality, showing that it can reduce legal risks and save a lot of time. This solution shows how artificial intelligence can be used to support legal and business professionals by taking care of routine work and helping them focus on more important decisions. It's a step toward making legal processes smarter, faster, and more reliable.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ£€ç´¢å¢å¼ºçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(Retrieval-Augmented Multi-Agent System)ï¼Œæ—¨åœ¨è§£å†³å·¥ä½œè¯´æ˜ä¹¦(Statement of Work, SOW)æ’°å†™è¿‡ç¨‹ä¸­è€—æ—¶è¾ƒé•¿ä¸”å®¹æ˜“å‡ºé”™çš„éš¾é¢˜ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ååŒä¸‰ä¸ªæ ¸å¿ƒæ™ºèƒ½ä½“(agents)â€”â€”åˆ†åˆ«è´Ÿè´£åˆç¨¿èµ·è‰ã€æ³•å¾‹åˆè§„å®¡æŸ¥ä»¥åŠæ ¼å¼åŒ–æ ¸å¯¹ï¼Œå®ç°äº†SOWç”Ÿæˆçš„å…¨æµç¨‹è‡ªåŠ¨åŒ–ã€‚ä¸ä¼ ç»Ÿçš„æ¨¡æ¿åŒ–å·¥å…·ä¸åŒï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿæ·±å…¥ç†è§£é¡¹ç›®èƒŒæ™¯å¹¶å®šåˆ¶åŒ–ç”Ÿæˆå†…å®¹ï¼Œç¡®ä¿äº†æ–‡æ¡£çš„ä¸“ä¸šæ€§ä¸å‡†ç¡®æ€§ã€‚æµ‹è¯•ç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿèƒ½åœ¨3åˆ†é’Ÿå†…å®ŒæˆåŸæœ¬éœ€è¦æ•°å°æ—¶ç”šè‡³æ•°å¤©çš„æ‰‹å·¥ä½œä¸šï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿäº§æ•ˆç‡ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ¡ˆåœ¨ä¿æŒé«˜è´¨é‡è¾“å‡ºçš„åŒæ—¶æœ‰æ•ˆé™ä½äº†æ³•å¾‹é£é™©ï¼Œä¸ºæ³•å¾‹å’Œå•†åŠ¡é¢†åŸŸçš„æ™ºèƒ½åŒ–è½¬å‹æä¾›äº†é«˜æ•ˆå¯é çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "7 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.07569v1",
      "published_date": "2025-08-11 02:59:36 UTC",
      "updated_date": "2025-08-11 02:59:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:08:01.985699+00:00"
    },
    {
      "arxiv_id": "2508.07561v1",
      "title": "A Small-footprint Acoustic Echo Cancellation Solution for Mobile Full-Duplex Speech Interactions",
      "title_zh": "é¢å‘ç§»åŠ¨å…¨åŒå·¥è¯­éŸ³äº¤äº’çš„è½»é‡çº§å›å£°æ¶ˆé™¤æ–¹æ¡ˆ",
      "authors": [
        "Yiheng Jiang",
        "Tian Biao"
      ],
      "abstract": "In full-duplex speech interaction systems, effective Acoustic Echo Cancellation (AEC) is crucial for recovering echo-contaminated speech. This paper presents a neural network-based AEC solution to address challenges in mobile scenarios with varying hardware, nonlinear distortions and long latency. We first incorporate diverse data augmentation strategies to enhance the model's robustness across various environments. Moreover, progressive learning is employed to incrementally improve AEC effectiveness, resulting in a considerable improvement in speech quality. To further optimize AEC's downstream applications, we introduce a novel post-processing strategy employing tailored parameters designed specifically for tasks such as Voice Activity Detection (VAD) and Automatic Speech Recognition (ASR), thus enhancing their overall efficacy. Finally, our method employs a small-footprint model with streaming inference, enabling seamless deployment on mobile devices. Empirical results demonstrate effectiveness of the proposed method in Echo Return Loss Enhancement and Perceptual Evaluation of Speech Quality, alongside significant improvements in both VAD and ASR results.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹ç§»åŠ¨ç«¯å…¨åŒå·¥è¯­éŸ³äº¤äº’ä¸­ç¡¬ä»¶å·®å¼‚ã€éçº¿æ€§å¤±çœŸåŠé•¿å»¶è¿Ÿå¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç¥ç»ç½‘ç»œçš„å£°å­¦å›å£°æ¶ˆé™¤(AEC)è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆé€šè¿‡å¼•å…¥å¤šæ ·åŒ–çš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨ä¸åŒç¯å¢ƒä¸‹çš„é²æ£’æ€§ã€‚åŒæ—¶ï¼Œç ”ç©¶é‡‡ç”¨äº†æ¸è¿›å¼å­¦ä¹ (Progressive learning)æ–¹æ³•åˆ†é˜¶æ®µæå‡å›å£°æ¶ˆé™¤æ•ˆæœï¼Œä»è€Œå¤§å¹…ä¼˜åŒ–äº†è¯­éŸ³è´¨é‡ã€‚æ­¤å¤–ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åˆ›æ–°çš„åå¤„ç†ç­–ç•¥ï¼Œé€šè¿‡é’ˆå¯¹è¯­éŸ³æ´»åŠ¨æ£€æµ‹(VAD)å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)ä»»åŠ¡å®šåˆ¶å‚æ•°ï¼Œè¿›ä¸€æ­¥æå‡äº†ä¸‹æ¸¸åº”ç”¨çš„æ•´ä½“æ•ˆèƒ½ã€‚è¯¥æ¨¡å‹é‡‡ç”¨è½»é‡åŒ–(Small-footprint)è®¾è®¡å¹¶æ”¯æŒæµå¼æ¨ç†(Streaming inference)ï¼Œç¡®ä¿äº†åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šçš„æ— ç¼éƒ¨ç½²ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›å£°æŸè€—å¢å¼º(ERLE)å’Œè¯­éŸ³è´¨é‡æ„ŸçŸ¥è¯„ä¼°(PESQ)æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¹¶æ˜¾è‘—æ”¹å–„äº†VADå’ŒASRçš„è¯†åˆ«å‡†ç¡®åº¦ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "This paper is accepted to ICASSP 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.07561v1",
      "published_date": "2025-08-11 02:45:31 UTC",
      "updated_date": "2025-08-11 02:45:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:07:56.988230+00:00"
    },
    {
      "arxiv_id": "2508.14064v1",
      "title": "An automatic patent literature retrieval system based on LLM-RAG",
      "title_zh": "åŸºäº LLM-RAG çš„è‡ªåŠ¨ä¸“åˆ©æ–‡çŒ®æ£€ç´¢ç³»ç»Ÿ",
      "authors": [
        "Yao Ding",
        "Yuqing Wu",
        "Ziyang Ding"
      ],
      "abstract": "With the acceleration of technological innovation efficient retrieval and classification of patent literature have become essential for intellectual property management and enterprise RD Traditional keyword and rulebased retrieval methods often fail to address complex query intents or capture semantic associations across technical domains resulting in incomplete and lowrelevance results This study presents an automated patent retrieval framework integrating Large Language Models LLMs with RetrievalAugmented Generation RAG technology The system comprises three components: 1) a preprocessing module for patent data standardization, 2) a highefficiency vector retrieval engine leveraging LLMgenerated embeddings, and 3) a RAGenhanced query module that combines external document retrieval with contextaware response generation Evaluations were conducted on the Google Patents dataset 20062024 containing millions of global patent records with metadata such as filing date domain and status The proposed gpt35turbo0125RAG configuration achieved 805 semantic matching accuracy and 92.1% recall surpassing baseline LLM methods by 28 percentage points The framework also demonstrated strong generalization in crossdomain classification and semantic clustering tasks These results validate the effectiveness of LLMRAG integration for intelligent patent retrieval providing a foundation for nextgeneration AIdriven intellectual property analysis platforms",
      "tldr_zh": "é’ˆå¯¹ä¼ ç»Ÿä¸“åˆ©æ£€ç´¢æ–¹æ³•éš¾ä»¥æ•æ‰å¤æ‚è¯­ä¹‰å…³è”åŠå¤„ç†å¤æ‚æ„å›¾çš„å±€é™æ€§ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é›†æˆå¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æŠ€æœ¯çš„è‡ªåŠ¨åŒ–ä¸“åˆ©æ£€ç´¢æ¡†æ¶ã€‚è¯¥ç³»ç»Ÿç”±ä¸“åˆ©æ•°æ®æ ‡å‡†åŒ–é¢„å¤„ç†æ¨¡å—ã€åˆ©ç”¨LLMç”ŸæˆåµŒå…¥(Embeddings)çš„é«˜æ•ˆå‘é‡æ£€ç´¢å¼•æ“ï¼Œä»¥åŠç»“åˆä¸Šä¸‹æ–‡æ„ŸçŸ¥å“åº”ç”Ÿæˆçš„RAGå¢å¼ºæŸ¥è¯¢æ¨¡å—ä¸‰ä¸ªæ ¸å¿ƒéƒ¨åˆ†ç»„æˆã€‚åœ¨Google Patentsæ•°æ®é›†ï¼ˆ2006-2024ï¼‰ä¸Šçš„å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œé‡‡ç”¨gpt-3.5-turbo-0125+RAGé…ç½®çš„æ¨¡å‹å®ç°äº†80.5%çš„è¯­ä¹‰åŒ¹é…å‡†ç¡®ç‡å’Œ92.1%çš„å¬å›ç‡ï¼Œæ€§èƒ½è¾ƒåŸºå‡†LLMæ–¹æ³•æå‡äº†28ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨è·¨é¢†åŸŸåˆ†ç±»å’Œè¯­ä¹‰èšç±»ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºåŠ²çš„æ³›åŒ–èƒ½åŠ›ï¼ŒéªŒè¯äº†LLM-RAGé›†æˆåœ¨æ™ºèƒ½åŒ–çŸ¥è¯†äº§æƒåˆ†æä¸­çš„æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶æˆæœä¸ºæ„å»ºä¸‹ä¸€ä»£AIé©±åŠ¨çš„ä¸“åˆ©åˆ†æå¹³å°å¥ å®šäº†åšå®åŸºç¡€ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.14064v1",
      "published_date": "2025-08-11 02:39:16 UTC",
      "updated_date": "2025-08-11 02:39:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:08:13.589922+00:00"
    },
    {
      "arxiv_id": "2508.07556v2",
      "title": "Uncertainty-Driven Reliability: Selective Prediction and Trustworthy Deployment in Modern Machine Learning",
      "title_zh": "ä¸ç¡®å®šæ€§é©±åŠ¨çš„å¯é æ€§ï¼šç°ä»£æœºå™¨å­¦ä¹ ä¸­çš„é€‰æ‹©æ€§é¢„æµ‹ä¸å¯ä¿¡éƒ¨ç½²",
      "authors": [
        "Stephan Rabanser"
      ],
      "abstract": "Machine learning (ML) systems are increasingly deployed in high-stakes domains where reliability is paramount. This thesis investigates how uncertainty estimation can enhance the safety and trustworthiness of ML, focusing on selective prediction -- where models abstain when confidence is low.\n  We first show that a model's training trajectory contains rich uncertainty signals that can be exploited without altering its architecture or loss. By ensembling predictions from intermediate checkpoints, we propose a lightweight, post-hoc abstention method that works across tasks, avoids the cost of deep ensembles, and achieves state-of-the-art selective prediction performance. Crucially, this approach is fully compatible with differential privacy (DP), allowing us to study how privacy noise affects uncertainty quality. We find that while many methods degrade under DP, our trajectory-based approach remains robust, and we introduce a framework for isolating the privacy-uncertainty trade-off. Next, we then develop a finite-sample decomposition of the selective classification gap -- the deviation from the oracle accuracy-coverage curve -- identifying five interpretable error sources and clarifying which interventions can close the gap. This explains why calibration alone cannot fix ranking errors, motivating methods that improve uncertainty ordering. Finally, we show that uncertainty signals can be adversarially manipulated to hide errors or deny service while maintaining high accuracy, and we design defenses combining calibration audits with verifiable inference.\n  Together, these contributions advance reliable ML by improving, evaluating, and safeguarding uncertainty estimation, enabling models that not only make accurate predictions -- but also know when to say \"I do not know\".",
      "tldr_zh": "æœ¬è®ºæ–‡æ·±å…¥æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨ä¸ç¡®å®šæ€§ä¼°è®¡ (Uncertainty Estimation) æ¥å¢å¼ºæœºå™¨å­¦ä¹ ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå¯ä¿¡åº¦ï¼Œå…¶æ ¸å¿ƒç ”ç©¶é‡ç‚¹åœ¨äºæ¨¡å‹åœ¨ç½®ä¿¡åº¦è¾ƒä½æ—¶æ‹’ç»é¢„æµ‹çš„é€‰æ‹©æ€§é¢„æµ‹ (Selective Prediction) æœºåˆ¶ã€‚ç ”ç©¶é¦–å…ˆæå‡ºä¸€ç§åˆ©ç”¨æ¨¡å‹è®­ç»ƒè½¨è¿¹ä¸­ä¸ç¡®å®šæ€§ä¿¡å·çš„è½»é‡çº§äº‹åæ–¹æ³•ï¼Œé€šè¿‡é›†æˆä¸­é—´æ£€æŸ¥ç‚¹çš„é¢„æµ‹ç»“æœï¼Œåœ¨ä¸æ”¹å˜æ¶æ„çš„æƒ…å†µä¸‹å®ç°äº†æœ€å…ˆè¿›çš„é€‰æ‹©æ€§é¢„æµ‹æ€§èƒ½ï¼Œä¸”åœ¨å·®åˆ†éšç§ (Differential Privacy) ç¯å¢ƒä¸‹è¡¨ç°å‡ºæå¼ºçš„é²æ£’æ€§ã€‚éšåï¼Œè®ºæ–‡æ„å»ºäº†é€‰æ‹©æ€§åˆ†ç±»å·®è·çš„æœ‰é™æ ·æœ¬åˆ†è§£æ¡†æ¶ï¼Œè¯†åˆ«å‡ºäº”ç§å…³é”®è¯¯å·®æºï¼Œé˜æ˜äº†æé«˜ä¸ç¡®å®šæ€§æ’åºè´¨é‡å¯¹äºå¼¥åˆæ€§èƒ½å·®è·çš„å¿…è¦æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ­ç¤ºäº†ä¸ç¡®å®šæ€§ä¿¡å·å¯èƒ½é­å—å¯¹æŠ—æ€§æ“çºµä»¥éšè—é”™è¯¯æˆ–æ‹’ç»æœåŠ¡çš„é£é™©ï¼Œå¹¶é’ˆå¯¹æ€§åœ°è®¾è®¡äº†ç»“åˆæ ¡å‡†å®¡è®¡ä¸å¯éªŒè¯æ¨ç†çš„é˜²å¾¡ä½“ç³»ã€‚è¯¥ç ”ç©¶é€šè¿‡æ”¹è¿›ã€è¯„ä¼°å’Œä¿æŠ¤ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œæ˜¾è‘—æå‡äº†ç°ä»£æœºå™¨å­¦ä¹ çš„å¯é æ€§ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå…·å¤‡è¯†åˆ«è‡ªèº«è®¤çŸ¥è¾¹ç•Œå¹¶é€‚æ—¶â€œæ‹’ç»å›ç­”â€çš„èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "PhD Thesis",
      "pdf_url": "https://arxiv.org/pdf/2508.07556v2",
      "published_date": "2025-08-11 02:33:53 UTC",
      "updated_date": "2025-09-06 12:35:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:08:12.898087+00:00"
    },
    {
      "arxiv_id": "2508.07538v1",
      "title": "A DICOM Image De-identification Algorithm in the MIDI-B Challenge",
      "title_zh": "MIDI-B æŒ‘æˆ˜èµ›ä¸­çš„ DICOM å›¾åƒå»æ ‡è¯†åŒ–ç®—æ³•",
      "authors": [
        "Hongzhu Jiang",
        "Sihan Xie",
        "Zhiyu Wan"
      ],
      "abstract": "Image de-identification is essential for the public sharing of medical images, particularly in the widely used Digital Imaging and Communications in Medicine (DICOM) format as required by various regulations and standards, including Health Insurance Portability and Accountability Act (HIPAA) privacy rules, the DICOM PS3.15 standard, and best practices recommended by the Cancer Imaging Archive (TCIA). The Medical Image De-Identification Benchmark (MIDI-B) Challenge at the 27th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2024) was organized to evaluate rule-based DICOM image de-identification algorithms with a large dataset of clinical DICOM images. In this report, we explore the critical challenges of de-identifying DICOM images, emphasize the importance of removing personally identifiable information (PII) to protect patient privacy while ensuring the continued utility of medical data for research, diagnostics, and treatment, and provide a comprehensive overview of the standards and regulations that govern this process. Additionally, we detail the de-identification methods we applied - such as pixel masking, date shifting, date hashing, text recognition, text replacement, and text removal - to process datasets during the test phase in strict compliance with these standards. According to the final leaderboard of the MIDI-B challenge, the latest version of our solution algorithm correctly executed 99.92% of the required actions and ranked 2nd out of 10 teams that completed the challenge (from a total of 22 registered teams). Finally, we conducted a thorough analysis of the resulting statistics and discussed the limitations of current approaches and potential avenues for future improvement.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ MICCAI 2024 ä¸¾åŠçš„ MIDI-B Challenge æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„ DICOM å›¾åƒå»éšç§åŒ–(De-identification)ç®—æ³•ï¼Œæ—¨åœ¨ä¸¥æ ¼éµå®ˆ HIPAA éšç§è§„åˆ™ã€DICOM PS3.15 æ ‡å‡†å’Œ TCIA æ¨èçš„æœ€ä½³å®è·µã€‚ä¸ºäº†åœ¨ä¿æŠ¤æ‚£è€…ä¸ªäººèº«ä»½ä¿¡æ¯(PII)çš„åŒæ—¶ç¡®ä¿åŒ»ç–—æ•°æ®çš„ç ”ç©¶æ•ˆç”¨ï¼Œè¯¥ç®—æ³•ç»¼åˆé‡‡ç”¨äº†åƒç´ é®ç›–(Pixel masking)ã€æ—¥æœŸåç§»(Date shifting)ã€æ—¥æœŸå“ˆå¸Œ(Date hashing)ã€æ–‡æœ¬è¯†åˆ«(Text recognition)ä»¥åŠæ–‡æœ¬æ›¿æ¢ä¸ç§»é™¤ç­‰ä¸€ç³»åˆ—æŠ€æœ¯æ‰‹æ®µã€‚åœ¨ MIDI-B æŒ‘æˆ˜èµ›çš„æœ€ç»ˆæ¦œå•ä¸­ï¼Œè¯¥æ–¹æ¡ˆçš„æœ€æ–°ç‰ˆæœ¬ä»¥ 99.92% çš„æ“ä½œæ‰§è¡Œå‡†ç¡®ç‡åœ¨ 10 æ”¯å®Œèµ›é˜Ÿä¼ä¸­è£è·ç¬¬äºŒåã€‚æ­¤å¤–ï¼Œè¯¥æŠ¥å‘Šè¿˜é€šè¿‡è¯¦å°½çš„ç»Ÿè®¡æ•°æ®åˆ†æäº†å½“å‰å»éšç§åŒ–å¤„ç†é¢ä¸´çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œå¹¶å¯¹ç°æœ‰æ–¹æ³•çš„å±€é™æ€§åŠæœªæ¥çš„æ”¹è¿›æ–¹å‘è¿›è¡Œäº†æ·±å…¥è®¨è®ºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.07538v1",
      "published_date": "2025-08-11 01:38:07 UTC",
      "updated_date": "2025-08-11 01:38:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:08:20.087671+00:00"
    },
    {
      "arxiv_id": "2508.07520v1",
      "title": "Conversational DNA: A New Visual Language for Understanding Dialogue Structure in Human and AI",
      "title_zh": "Conversational DNAï¼šä¸€ç§ç”¨äºè§£æäººç±»ä¸äººå·¥æ™ºèƒ½å¯¹è¯ç»“æ„çš„æ–°å‹è§†è§‰è¯­è¨€",
      "authors": [
        "Baihan Lin"
      ],
      "abstract": "What if the patterns hidden within dialogue reveal more about communication than the words themselves? We introduce Conversational DNA, a novel visual language that treats any dialogue -- whether between humans, between human and AI, or among groups -- as a living system with interpretable structure that can be visualized, compared, and understood. Unlike traditional conversation analysis that reduces rich interaction to statistical summaries, our approach reveals the temporal architecture of dialogue through biological metaphors. Linguistic complexity flows through strand thickness, emotional trajectories cascade through color gradients, conversational relevance forms through connecting elements, and topic coherence maintains structural integrity through helical patterns. Through exploratory analysis of therapeutic conversations and historically significant human-AI dialogues, we demonstrate how this visualization approach reveals interaction patterns that traditional methods miss. Our work contributes a new creative framework for understanding communication that bridges data visualization, human-computer interaction, and the fundamental question of what makes dialogue meaningful in an age where humans increasingly converse with artificial minds.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Conversational DNAï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„å¯è§†åŒ–è¯­è¨€ï¼Œæ—¨åœ¨å°†äººç±»ä¸äººç±»ã€äººç±»ä¸ AI ä¹‹é—´çš„å¯¹è¯è§†ä¸ºä¸€ä¸ªå…·æœ‰å¯è§£é‡Šç»“æ„çš„ç”Ÿå‘½ç³»ç»Ÿã€‚ä¸ä¼ ç»Ÿçš„ç»Ÿè®¡åˆ†ææ–¹æ³•ä¸åŒï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ç”Ÿç‰©å­¦éšå–»æ­ç¤ºå¯¹è¯çš„ temporal architectureï¼Œé€šè¿‡é“¾æ¡åšåº¦ä½“ç° Linguistic complexityï¼Œå¹¶åˆ©ç”¨è‰²å½©æ¢¯åº¦å±•ç¤º Emotional trajectoriesã€‚æ­¤å¤–ï¼ŒConversational relevance å½¢æˆäº†å¯¹è¯çš„è¿æ¥å…ƒç´ ï¼Œè€Œ Topic coherence åˆ™é€šè¿‡èºæ—‹å›¾æ¡ˆ (helical patterns) ç»´æŒç»“æ„çš„å®Œæ•´æ€§ã€‚é€šè¿‡å¯¹æ²»ç–—æ€§è°ˆè¯å’Œé‡è¦äººæœºå¯¹è¯çš„æ¢ç´¢æ€§åˆ†æï¼Œç ”ç©¶è¯æ˜è¯¥æ–¹æ³•èƒ½æ­ç¤ºä¼ ç»Ÿæ‰‹æ®µå¿½ç•¥çš„äº’åŠ¨æ¨¡å¼ã€‚è¿™é¡¹å·¥ä½œä¸ºç†è§£æ²Ÿé€šæä¾›äº†ä¸€ä¸ªç»“åˆæ•°æ®å¯è§†åŒ– (data visualization) ä¸äººæœºäº¤äº’ (human-computer interaction) çš„å…¨æ–°åˆ›æ„æ¡†æ¶ã€‚å®ƒæ·±å…¥æ¢è®¨äº†åœ¨äººæœºå¯¹è¯æ—¥ç›Šæ™®åŠçš„æ—¶ä»£ï¼Œå¯¹è¯ç»“æ„å¦‚ä½•å®šä¹‰æ²Ÿé€šçš„æ„ä¹‰ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07520v1",
      "published_date": "2025-08-11 00:43:35 UTC",
      "updated_date": "2025-08-11 00:43:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:08:26.292521+00:00"
    },
    {
      "arxiv_id": "2508.07517v1",
      "title": "Word Clouds as Common Voices: LLM-Assisted Visualization of Participant-Weighted Themes in Qualitative Interviews",
      "title_zh": "è¯äº‘ä½œä¸ºå…±åŒå¿ƒå£°ï¼šå¤§è¯­è¨€æ¨¡å‹è¾…åŠ©çš„å®šæ€§è®¿è°ˆå‚ä¸è€…åŠ æƒä¸»é¢˜å¯è§†åŒ–",
      "authors": [
        "Joseph T. Colonel",
        "Baihan Lin"
      ],
      "abstract": "Word clouds are a common way to summarize qualitative interviews, yet traditional frequency-based methods often fail in conversational contexts: they surface filler words, ignore paraphrase, and fragment semantically related ideas. This limits their usefulness in early-stage analysis, when researchers need fast, interpretable overviews of what participant actually said. We introduce ThemeClouds, an open-source visualization tool that uses large language models (LLMs) to generate thematic, participant-weighted word clouds from dialogue transcripts. The system prompts an LLM to identify concept-level themes across a corpus and then counts how many unique participants mention each topic, yielding a visualization grounded in breadth of mention rather than raw term frequency. Researchers can customize prompts and visualization parameters, providing transparency and control. Using interviews from a user study comparing five recording-device configurations (31 participants; 155 transcripts, Whisper ASR), our approach surfaces more actionable device concerns than frequency clouds and topic-modeling baselines (e.g., LDA, BERTopic). We discuss design trade-offs for integrating LLM assistance into qualitative workflows, implications for interpretability and researcher agency, and opportunities for interactive analyses such as per-condition contrasts (``diff clouds'').",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹ä¼ ç»ŸåŸºäºé¢‘ç‡çš„è¯äº‘åœ¨å®šæ€§è®¿è°ˆæ€»ç»“ä¸­éš¾ä»¥å¤„ç†è¯­æ°”è¯ã€è¯­ä¹‰ç¢ç‰‡åŒ–åŠé‡å¤è¡¨è¾¾ç­‰é—®é¢˜ï¼Œæå‡ºäº†å¼€æºå¯è§†åŒ–å·¥å…· ThemeCloudsã€‚è¯¥å·¥å…·åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) ä»å¯¹è¯æ–‡æœ¬ä¸­è¯†åˆ«æ¦‚å¿µå±‚é¢çš„ä¸»é¢˜ (thematic themes)ï¼Œå¹¶åŸºäºæåŠè¯¥ä¸»é¢˜çš„å”¯ä¸€å‚ä¸è€…æ•°é‡è¿›è¡ŒåŠ æƒï¼Œè€Œéå•çº¯ç»Ÿè®¡è¯é¢‘ã€‚è¿™ç§ä»¥å‚ä¸è€…å¹¿åº¦ä¸ºæ ¸å¿ƒçš„å¯è§†åŒ–æ–¹æ³•ï¼Œæ›¿ä»£äº†ä¼ ç»Ÿçš„åŸå§‹è¯é¢‘ç»Ÿè®¡ï¼Œä½¿å¾—ç ”ç©¶è€…èƒ½å¤Ÿå¿«é€Ÿè·å¾—æ›´å…·è§£é‡Šæ€§çš„åˆ†ææ¦‚è§ˆã€‚é€šè¿‡å¯¹31åå‚ä¸è€…äº§ç”Ÿçš„155ä»½è½¬å½•æ–‡æœ¬è¿›è¡Œå®éªŒéªŒè¯ï¼ŒThemeClouds åœ¨æå–å®é™…è®¾å¤‡å…³æ³¨ç‚¹æ–¹é¢æ˜¾è‘—ä¼˜äº LDA å’Œ BERTopic ç­‰ä¼ ç»Ÿä¸»é¢˜å»ºæ¨¡åŸºå‡†ã€‚è¯¥ç³»ç»Ÿå…è®¸ç ”ç©¶è€…è‡ªå®šä¹‰æç¤ºè¯å’Œå¯è§†åŒ–å‚æ•°ï¼Œåœ¨ç¡®ä¿é€æ˜åº¦å’Œæ§åˆ¶åŠ›çš„åŒæ—¶ï¼Œä¹Ÿä¸ºå¯¹æ¯”åˆ†æ (diff clouds) ç­‰äº¤äº’å¼åˆ†ææä¾›äº†å¯èƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07517v1",
      "published_date": "2025-08-11 00:27:52 UTC",
      "updated_date": "2025-08-11 00:27:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:08:39.601188+00:00"
    },
    {
      "arxiv_id": "2508.07514v1",
      "title": "From Field to Drone: Domain Drift Tolerant Automated Multi-Species and Damage Plant Semantic Segmentation for Herbicide Trials",
      "title_zh": "ä»ç”°é—´åˆ°æ— äººæœºï¼šé¢å‘é™¤è‰å‰‚è¯•éªŒçš„æŠ—é¢†åŸŸæ¼‚ç§»è‡ªåŠ¨åŒ–å¤šç‰©ç§åŠæ¤ç‰©æŸä¼¤è¯­ä¹‰åˆ†å‰²",
      "authors": [
        "Artzai Picon",
        "Itziar Eguskiza",
        "Daniel Mugica",
        "Javier Romero",
        "Carlos Javier Jimenez",
        "Eric White",
        "Gabriel Do-Lago-Junqueira",
        "Christian Klukas",
        "Ramon Navarra-Mestre"
      ],
      "abstract": "Field trials are vital in herbicide research and development to assess effects on crops and weeds under varied conditions. Traditionally, evaluations rely on manual visual assessments, which are time-consuming, labor-intensive, and subjective. Automating species and damage identification is challenging due to subtle visual differences, but it can greatly enhance efficiency and consistency.\n  We present an improved segmentation model combining a general-purpose self-supervised visual model with hierarchical inference based on botanical taxonomy. Trained on a multi-year dataset (2018-2020) from Germany and Spain using digital and mobile cameras, the model was tested on digital camera data (year 2023) and drone imagery from the United States, Germany, and Spain (year 2024) to evaluate robustness under domain shift. This cross-device evaluation marks a key step in assessing generalization across platforms of the model.\n  Our model significantly improved species identification (F1-score: 0.52 to 0.85, R-squared: 0.75 to 0.98) and damage classification (F1-score: 0.28 to 0.44, R-squared: 0.71 to 0.87) over prior methods. Under domain shift (drone images), it maintained strong performance with moderate degradation (species: F1-score 0.60, R-squared 0.80; damage: F1-score 0.41, R-squared 0.62), where earlier models failed.\n  These results confirm the model's robustness and real-world applicability. It is now deployed in BASF's phenotyping pipeline, enabling large-scale, automated crop and weed monitoring across diverse geographies.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é™¤è‰å‰‚è¯•éªŒä¸­äººå·¥è¯„ä¼°è€—æ—¶ä¸”ä¸»è§‚çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§èƒ½å¤Ÿåº”å¯¹é¢†åŸŸæ¼‚ç§»(Domain Drift)çš„è‡ªåŠ¨åŒ–æ¤ç‰©ç‰©ç§ä¸æŸä¼¤è¯­ä¹‰åˆ†å‰²æ¨¡å‹ã€‚è¯¥æ¨¡å‹å°†é€šç”¨è‡ªç›‘ç£è§†è§‰æ¨¡å‹(self-supervised visual model)ä¸åŸºäºæ¤ç‰©åˆ†ç±»å­¦(botanical taxonomy)çš„å±‚æ¬¡åŒ–æ¨ç†(hierarchical inference)ç›¸ç»“åˆï¼Œä»¥æå‡è¯†åˆ«çš„ç²¾å‡†åº¦ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨è·¨è¶Šå¤šå¹´åŠå¤šåœ°çš„ç§»åŠ¨ç›¸æœºæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶é’ˆå¯¹æ— äººæœºå›¾åƒ(drone imagery)ç­‰è·¨è®¾å¤‡åœºæ™¯è¿›è¡Œäº†é²æ£’æ€§è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç‰©ç§è¯†åˆ«å’ŒæŸä¼¤åˆ†ç±»çš„F1-scoreåŠR-squaredæŒ‡æ ‡ä¸Šå‡è¾ƒå‰ä»£æ–¹æ³•æœ‰æ˜¾è‘—æå‡ã€‚ç‰¹åˆ«æ˜¯åœ¨é¢ä¸´é¢†åŸŸæ¼‚ç§»çš„æŒ‘æˆ˜æ—¶ï¼Œè¯¥æ¨¡å‹ä¾ç„¶èƒ½å¤Ÿä¿æŒç¨³å®šçš„æ€§èƒ½ï¼Œå…‹æœäº†ä»¥å¾€æ¨¡å‹å¤±æ•ˆçš„é—®é¢˜ã€‚ç›®å‰è¯¥æŠ€æœ¯å·²æˆåŠŸéƒ¨ç½²äºBASFçš„è¡¨å‹åˆ†æç®¡çº¿(phenotyping pipeline)ï¼Œæ”¯æŒåœ¨å…¨çƒèŒƒå›´å†…å¼€å±•å¤§è§„æ¨¡çš„è‡ªåŠ¨åŒ–å†œä½œç‰©ä¸æ‚è‰ç›‘æµ‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.07514v1",
      "published_date": "2025-08-11 00:08:42 UTC",
      "updated_date": "2025-08-11 00:08:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:08:32.299237+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 159,
  "processed_papers_count": 159,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T11:09:34.166889+00:00"
}
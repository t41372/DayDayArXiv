[
  {
    "arxiv_id": "2504.03994v2",
    "title": "Improving Mixed-Criticality Scheduling with Reinforcement Learning",
    "authors": [
      "Muhammad El-Mahdy",
      "Nourhan Sakr",
      "Rodrigo Carrasco"
    ],
    "abstract": "This paper introduces a novel reinforcement learning (RL) approach to\nscheduling mixed-criticality (MC) systems on processors with varying speeds.\nBuilding upon the foundation laid by [1], we extend their work to address the\nnon-preemptive scheduling problem, which is known to be NP-hard. By modeling\nthis scheduling challenge as a Markov Decision Process (MDP), we develop an RL\nagent capable of generating near-optimal schedules for real-time MC systems.\nOur RL-based scheduler prioritizes high-critical tasks while maintaining\noverall system performance.\n  Through extensive experiments, we demonstrate the scalability and\neffectiveness of our approach. The RL scheduler significantly improves task\ncompletion rates, achieving around 80% overall and 85% for high-criticality\ntasks across 100,000 instances of synthetic data and real data under varying\nsystem conditions. Moreover, under stable conditions without degradation, the\nscheduler achieves 94% overall task completion and 93% for high-criticality\ntasks. These results highlight the potential of RL-based schedulers in\nreal-time and safety-critical applications, offering substantial improvements\nin handling complex and dynamic scheduling scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "This work was submitted to the 32nd International Conference on\n  Real-Time Networks and Systems (RTNS) on June 8, 2024",
    "pdf_url": "http://arxiv.org/pdf/2504.03994v2",
    "published_date": "2025-04-04 23:28:48 UTC",
    "updated_date": "2025-04-08 13:22:59 UTC"
  },
  {
    "arxiv_id": "2504.03991v1",
    "title": "Algorithmic Prompt Generation for Diverse Human-like Teaming and Communication with Large Language Models",
    "authors": [
      "Siddharth Srikanth",
      "Varun Bhatt",
      "Boshen Zhang",
      "Werner Hager",
      "Charles Michael Lewis",
      "Katia P. Sycara",
      "Aaquib Tabrez",
      "Stefanos Nikolaidis"
    ],
    "abstract": "Understanding how humans collaborate and communicate in teams is essential\nfor improving human-agent teaming and AI-assisted decision-making. However,\nrelying solely on data from large-scale user studies is impractical due to\nlogistical, ethical, and practical constraints, necessitating synthetic models\nof multiple diverse human behaviors. Recently, agents powered by Large Language\nModels (LLMs) have been shown to emulate human-like behavior in social\nsettings. But, obtaining a large set of diverse behaviors requires manual\neffort in the form of designing prompts. On the other hand, Quality Diversity\n(QD) optimization has been shown to be capable of generating diverse\nReinforcement Learning (RL) agent behavior. In this work, we combine QD\noptimization with LLM-powered agents to iteratively search for prompts that\ngenerate diverse team behavior in a long-horizon, multi-step collaborative\nenvironment. We first show, through a human-subjects experiment (n=54\nparticipants), that humans exhibit diverse coordination and communication\nbehavior in this domain. We then show that our approach can effectively\nreplicate trends from human teaming data and also capture behaviors that are\nnot easily observed without collecting large amounts of data. Our findings\nhighlight the combination of QD and LLM-powered agents as an effective tool for\nstudying teaming and communication strategies in multi-agent collaboration.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03991v1",
    "published_date": "2025-04-04 23:09:40 UTC",
    "updated_date": "2025-04-04 23:09:40 UTC"
  },
  {
    "arxiv_id": "2504.03989v3",
    "title": "CORTEX-AVD: A Framework for CORner Case Testing and EXploration in Autonomous Vehicle Development",
    "authors": [
      "Gabriel Kenji Godoy Shimanuki",
      "Alexandre Moreira Nascimento",
      "Lucio Flavio Vismari",
      "Joao Batista Camargo Junior",
      "Jorge Rady de Almeida Junior",
      "Paulo Sergio Cugnasca"
    ],
    "abstract": "Autonomous Vehicles (AVs) aim to improve traffic safety and efficiency by\nreducing human error. However, ensuring AVs reliability and safety is a\nchallenging task when rare, high-risk traffic scenarios are considered. These\n'Corner Cases' (CC) scenarios, such as unexpected vehicle maneuvers or sudden\npedestrian crossings, must be safely and reliable dealt by AVs during their\noperations. But they arehard to be efficiently generated. Traditional CC\ngeneration relies on costly and risky real-world data acquisition, limiting\nscalability, and slowing research and development progress. Simulation-based\ntechniques also face challenges, as modeling diverse scenarios and capturing\nall possible CCs is complex and time-consuming. To address these limitations in\nCC generation, this research introduces CORTEX-AVD, CORner Case Testing &\nEXploration for Autonomous Vehicles Development, an open-source framework that\nintegrates the CARLA Simulator and Scenic to automatically generate CC from\ntextual descriptions, increasing the diversity and automation of scenario\nmodeling. Genetic Algorithms (GA) are used to optimize the scenario parameters\nin six case study scenarios, increasing the occurrence of high-risk events.\nUnlike previous methods, CORTEX-AVD incorporates a multi-factor fitness\nfunction that considers variables such as distance, time, speed, and collision\nlikelihood. Additionally, the study provides a benchmark for comparing GA-based\nCC generation methods, contributing to a more standardized evaluation of\nsynthetic data generation and scenario assessment. Experimental results\ndemonstrate that the CORTEX-AVD framework significantly increases CC incidence\nwhile reducing the proportion of wasted simulations.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "10 pages, 10 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.03989v3",
    "published_date": "2025-04-04 23:05:31 UTC",
    "updated_date": "2025-04-09 20:04:21 UTC"
  },
  {
    "arxiv_id": "2504.03978v1",
    "title": "V-CEM: Bridging Performance and Intervenability in Concept-based Models",
    "authors": [
      "Francesco De Santis",
      "Gabriele Ciravegna",
      "Philippe Bich",
      "Danilo Giordano",
      "Tania Cerquitelli"
    ],
    "abstract": "Concept-based eXplainable AI (C-XAI) is a rapidly growing research field that\nenhances AI model interpretability by leveraging intermediate,\nhuman-understandable concepts. This approach not only enhances model\ntransparency but also enables human intervention, allowing users to interact\nwith these concepts to refine and improve the model's performance. Concept\nBottleneck Models (CBMs) explicitly predict concepts before making final\ndecisions, enabling interventions to correct misclassified concepts. While CBMs\nremain effective in Out-Of-Distribution (OOD) settings with intervention, they\nstruggle to match the performance of black-box models. Concept Embedding Models\n(CEMs) address this by learning concept embeddings from both concept\npredictions and input data, enhancing In-Distribution (ID) accuracy but\nreducing the effectiveness of interventions, especially in OOD scenarios. In\nthis work, we propose the Variational Concept Embedding Model (V-CEM), which\nleverages variational inference to improve intervention responsiveness in CEMs.\nWe evaluated our model on various textual and visual datasets in terms of ID\nperformance, intervention responsiveness in both ID and OOD settings, and\nConcept Representation Cohesiveness (CRC), a metric we propose to assess the\nquality of the concept embedding representations. The results demonstrate that\nV-CEM retains CEM-level ID performance while achieving intervention\neffectiveness similar to CBM in OOD settings, effectively reducing the gap\nbetween interpretability (intervention) and generalization (performance).",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Paper accepted at: The 3rd World Conference on Explainable Artificial\n  Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2504.03978v1",
    "published_date": "2025-04-04 22:43:04 UTC",
    "updated_date": "2025-04-04 22:43:04 UTC"
  },
  {
    "arxiv_id": "2504.03976v2",
    "title": "OLAF: An Open Life Science Analysis Framework for Conversational Bioinformatics Powered by Large Language Models",
    "authors": [
      "Dylan Riffle",
      "Nima Shirooni",
      "Cody He",
      "Manush Murali",
      "Sovit Nayak",
      "Rishikumar Gopalan",
      "Diego Gonzalez Lopez"
    ],
    "abstract": "OLAF (Open Life Science Analysis Framework) is an open-source platform that\nenables researchers to perform bioinformatics analyses using natural language.\nBy combining large language models (LLMs) with a modular agent-pipe-router\narchitecture, OLAF generates and executes bioinformatics code on real\nscientific data, including formats like .h5ad. The system includes an Angular\nfront end and a Python/Firebase backend, allowing users to run analyses such as\nsingle-cell RNA-seq workflows, gene annotation, and data visualization through\na simple web interface. Unlike general-purpose AI tools, OLAF integrates code\nexecution, data handling, and scientific libraries in a reproducible,\nuser-friendly environment. It is designed to lower the barrier to computational\nbiology for non-programmers and support transparent, AI-powered life science\nresearch.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "q-bio.GN"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03976v2",
    "published_date": "2025-04-04 22:41:16 UTC",
    "updated_date": "2025-04-10 19:32:47 UTC"
  },
  {
    "arxiv_id": "2504.03975v1",
    "title": "GREATERPROMPT: A Unified, Customizable, and High-Performing Open-Source Toolkit for Prompt Optimization",
    "authors": [
      "Wenliang Zheng",
      "Sarkar Snigdha Sarathi Das",
      "Yusen Zhang",
      "Rui Zhang"
    ],
    "abstract": "LLMs have gained immense popularity among researchers and the general public\nfor its impressive capabilities on a variety of tasks. Notably, the efficacy of\nLLMs remains significantly dependent on the quality and structure of the input\nprompts, making prompt design a critical factor for their performance. Recent\nadvancements in automated prompt optimization have introduced diverse\ntechniques that automatically enhance prompts to better align model outputs\nwith user expectations. However, these methods often suffer from the lack of\nstandardization and compatibility across different techniques, limited\nflexibility in customization, inconsistent performance across model scales, and\nthey often exclusively rely on expensive proprietary LLM APIs. To fill in this\ngap, we introduce GREATERPROMPT, a novel framework that democratizes prompt\noptimization by unifying diverse methods under a unified, customizable API\nwhile delivering highly effective prompts for different tasks. Our framework\nflexibly accommodates various model scales by leveraging both text\nfeedback-based optimization for larger LLMs and internal gradient-based\noptimization for smaller models to achieve powerful and precise prompt\nimprovements. Moreover, we provide a user-friendly Web UI that ensures\naccessibility for non-expert users, enabling broader adoption and enhanced\nperformance across various user groups and application scenarios. GREATERPROMPT\nis available at https://github.com/psunlpgroup/GreaterPrompt via GitHub, PyPI,\nand web user interfaces.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03975v1",
    "published_date": "2025-04-04 22:36:55 UTC",
    "updated_date": "2025-04-04 22:36:55 UTC"
  },
  {
    "arxiv_id": "2504.03970v2",
    "title": "VideoComp: Advancing Fine-Grained Compositional and Temporal Alignment in Video-Text Models",
    "authors": [
      "Dahun Kim",
      "AJ Piergiovanni",
      "Ganesh Mallya",
      "Anelia Angelova"
    ],
    "abstract": "We introduce VideoComp, a benchmark and learning framework for advancing\nvideo-text compositionality understanding, aimed at improving vision-language\nmodels (VLMs) in fine-grained temporal alignment. Unlike existing benchmarks\nfocused on static image-text compositionality or isolated single-event videos,\nour benchmark targets alignment in continuous multi-event videos. Leveraging\nvideo-text datasets with temporally localized event captions (e.g.\nActivityNet-Captions, YouCook2), we construct two compositional benchmarks,\nActivityNet-Comp and YouCook2-Comp. We create challenging negative samples with\nsubtle temporal disruptions such as reordering, action word replacement,\npartial captioning, and combined disruptions. These benchmarks comprehensively\ntest models' compositional sensitivity across extended, cohesive video-text\nsequences. To improve model performance, we propose a hierarchical pairwise\npreference loss that strengthens alignment with temporally accurate pairs and\ngradually penalizes increasingly disrupted ones, encouraging fine-grained\ncompositional learning. To mitigate the limited availability of densely\nannotated video data, we introduce a pretraining strategy that concatenates\nshort video-caption pairs to simulate multi-event sequences. We evaluate\nvideo-text foundational models and large multimodal models (LMMs) on our\nbenchmark, identifying both strengths and areas for improvement in\ncompositionality. Overall, our work provides a comprehensive framework for\nevaluating and enhancing model capabilities in achieving fine-grained,\ntemporally coherent video-text alignment.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025, project page at\n  https://github.com/google-deepmind/video_comp",
    "pdf_url": "http://arxiv.org/pdf/2504.03970v2",
    "published_date": "2025-04-04 22:24:30 UTC",
    "updated_date": "2025-04-10 10:41:20 UTC"
  },
  {
    "arxiv_id": "2504.03966v1",
    "title": "Bridging LMS and Generative AI: Dynamic Course Content Integration (DCCI) for Connecting LLMs to Course Content -- The Ask ME Assistant",
    "authors": [
      "Kovan Mzwri",
      "Márta Turcsányi-Szabo"
    ],
    "abstract": "The integration of Large Language Models (LLMs) with Learning Management\nSystems (LMSs) has the potential to enhance task automation and accessibility\nin education. However, hallucination where LLMs generate inaccurate or\nmisleading information remains a significant challenge. This study introduces\nthe Dynamic Course Content Integration (DCCI) mechanism, which dynamically\nretrieves and integrates course content and curriculum from Canvas LMS into the\nLLM-powered assistant, Ask ME. By employing prompt engineering to structure\nretrieved content within the LLM's context window, DCCI ensures accuracy,\nrelevance, and contextual alignment, mitigating hallucination. To evaluate\nDCCI's effectiveness, Ask ME's usability, and broader student perceptions of AI\nin education, a mixed-methods approach was employed, incorporating user\nsatisfaction ratings and a structured survey. Results from a pilot study\nindicate high user satisfaction (4.614/5), with students recognizing Ask ME's\nability to provide timely and contextually relevant responses for both\nadministrative and course-related inquiries. Additionally, a majority of\nstudents agreed that Ask ME's integration with course content in Canvas LMS\nreduced platform-switching, improving usability, engagement, and comprehension.\nAI's role in reducing classroom hesitation and fostering self-directed learning\nand intellectual curiosity was also highlighted. Despite these benefits and\npositive perception of AI tools, concerns emerged regarding over-reliance on\nAI, accuracy limitations, and ethical issues such as plagiarism and reduced\nstudent-teacher interaction. These findings emphasize the need for strategic AI\nimplementation, ethical safeguards, and a pedagogical framework that\nprioritizes human-AI collaboration over substitution.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET",
      "cs.HC",
      "cs.SE"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03966v1",
    "published_date": "2025-04-04 22:17:30 UTC",
    "updated_date": "2025-04-04 22:17:30 UTC"
  },
  {
    "arxiv_id": "2504.03964v1",
    "title": "Clinical ModernBERT: An efficient and long context encoder for biomedical text",
    "authors": [
      "Simon A. Lee",
      "Anthony Wu",
      "Jeffrey N. Chiang"
    ],
    "abstract": "We introduce Clinical ModernBERT, a transformer based encoder pretrained on\nlarge scale biomedical literature, clinical notes, and medical ontologies,\nincorporating PubMed abstracts, MIMIC IV clinical data, and medical codes with\ntheir textual descriptions. Building on ModernBERT the current state of the art\nnatural language text encoder featuring architectural upgrades such as rotary\npositional embeddings (RoPE), Flash Attention, and extended context length up\nto 8,192 tokens our model adapts these innovations specifically for biomedical\nand clinical domains. Clinical ModernBERT excels at producing semantically rich\nrepresentations tailored for long context tasks. We validate this both by\nanalyzing its pretrained weights and through empirical evaluation on a\ncomprehensive suite of clinical NLP benchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Manuscript writeup corresponding to the Clinical ModernBERT\n  pre-trained encoder (https://huggingface.co/Simonlee711/Clinical_ModernBERT)",
    "pdf_url": "http://arxiv.org/pdf/2504.03964v1",
    "published_date": "2025-04-04 22:14:12 UTC",
    "updated_date": "2025-04-04 22:14:12 UTC"
  },
  {
    "arxiv_id": "2504.03961v1",
    "title": "Optimizing UAV Aerial Base Station Flights Using DRL-based Proximal Policy Optimization",
    "authors": [
      "Mario Rico Ibanez",
      "Azim Akhtarshenas",
      "David Lopez-Perez",
      "Giovanni Geraci"
    ],
    "abstract": "Unmanned aerial vehicle (UAV)-based base stations offer a promising solution\nin emergencies where the rapid deployment of cutting-edge networks is crucial\nfor maximizing life-saving potential. Optimizing the strategic positioning of\nthese UAVs is essential for enhancing communication efficiency. This paper\nintroduces an automated reinforcement learning approach that enables UAVs to\ndynamically interact with their environment and determine optimal\nconfigurations. By leveraging the radio signal sensing capabilities of\ncommunication networks, our method provides a more realistic perspective,\nutilizing state-of-the-art algorithm -- proximal policy optimization -- to\nlearn and generalize positioning strategies across diverse user equipment (UE)\nmovement patterns. We evaluate our approach across various UE mobility\nscenarios, including static, random, linear, circular, and mixed hotspot\nmovements. The numerical results demonstrate the algorithm's adaptability and\neffectiveness in maintaining comprehensive coverage across all movement\npatterns.",
    "categories": [
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03961v1",
    "published_date": "2025-04-04 22:06:01 UTC",
    "updated_date": "2025-04-04 22:06:01 UTC"
  },
  {
    "arxiv_id": "2504.03955v1",
    "title": "DeepOHeat-v1: Efficient Operator Learning for Fast and Trustworthy Thermal Simulation and Optimization in 3D-IC Design",
    "authors": [
      "Xinling Yu",
      "Ziyue Liu",
      "Hai Li",
      "Yixing Li",
      "Xin Ai",
      "Zhiyu Zeng",
      "Ian Young",
      "Zheng Zhang"
    ],
    "abstract": "Thermal analysis is crucial in three-dimensional integrated circuit (3D-IC)\ndesign due to increased power density and complex heat dissipation paths.\nAlthough operator learning frameworks such as DeepOHeat have demonstrated\npromising preliminary results in accelerating thermal simulation, they face\ncritical limitations in prediction capability for multi-scale thermal patterns,\ntraining efficiency, and trustworthiness of results during design optimization.\nThis paper presents DeepOHeat-v1, an enhanced physics-informed operator\nlearning framework that addresses these challenges through three key\ninnovations. First, we integrate Kolmogorov-Arnold Networks with learnable\nactivation functions as trunk networks, enabling an adaptive representation of\nmulti-scale thermal patterns. This approach achieves a $1.25\\times$ and\n$6.29\\times$ reduction in error in two representative test cases. Second, we\nintroduce a separable training method that decomposes the basis function along\nthe coordinate axes, achieving $62\\times$ training speedup and $31\\times$ GPU\nmemory reduction in our baseline case, and enabling thermal analysis at\nresolutions previously infeasible due to GPU memory constraints. Third, we\npropose a confidence score to evaluate the trustworthiness of the predicted\nresults, and further develop a hybrid optimization workflow that combines\noperator learning with finite difference (FD) using Generalized Minimal\nResidual (GMRES) method for incremental solution refinement, enabling efficient\nand trustworthy thermal optimization. Experimental results demonstrate that\nDeepOHeat-v1 achieves accuracy comparable to optimization using high-fidelity\nfinite difference solvers, while speeding up the entire optimization process by\n$70.6\\times$ in our test cases, effectively minimizing the peak temperature\nthrough optimal placement of heat-generating components.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.data-an"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.03955v1",
    "published_date": "2025-04-04 21:39:42 UTC",
    "updated_date": "2025-04-04 21:39:42 UTC"
  },
  {
    "arxiv_id": "2504.03953v1",
    "title": "TGraphX: Tensor-Aware Graph Neural Network for Multi-Dimensional Feature Learning",
    "authors": [
      "Arash Sajjadi",
      "Mark Eramian"
    ],
    "abstract": "TGraphX presents a novel paradigm in deep learning by unifying convolutional\nneural networks (CNNs) with graph neural networks (GNNs) to enhance visual\nreasoning tasks. Traditional CNNs excel at extracting rich spatial features\nfrom images but lack the inherent capability to model inter-object\nrelationships. Conversely, conventional GNNs typically rely on flattened node\nfeatures, thereby discarding vital spatial details. TGraphX overcomes these\nlimitations by employing CNNs to generate multi-dimensional node features\n(e.g., (3*128*128) tensors) that preserve local spatial semantics. These\nspatially aware nodes participate in a graph where message passing is performed\nusing 1*1 convolutions, which fuse adjacent features while maintaining their\nstructure. Furthermore, a deep CNN aggregator with residual connections is used\nto robustly refine the fused messages, ensuring stable gradient flow and\nend-to-end trainability. Our approach not only bridges the gap between spatial\nfeature extraction and relational reasoning but also demonstrates significant\nimprovements in object detection refinement and ensemble reasoning.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T07, 68T45, 68R10",
      "I.2.6; I.5.1; I.4.8"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted to arXiv. Code repository:\n  https://github.com/arashsajjadi/TGraphX |||\n  https://git.cs.usask.ca/arash/tgraphx",
    "pdf_url": "http://arxiv.org/pdf/2504.03953v1",
    "published_date": "2025-04-04 21:38:20 UTC",
    "updated_date": "2025-04-04 21:38:20 UTC"
  },
  {
    "arxiv_id": "2504.03951v1",
    "title": "Understanding EFX Allocations: Counting and Variants",
    "authors": [
      "Tzeh Yuan Neoh",
      "Nicholas Teh"
    ],
    "abstract": "Envy-freeness up to any good (EFX) is a popular and important fairness\nproperty in the fair allocation of indivisible goods, of which its existence in\ngeneral is still an open question. In this work, we investigate the problem of\ndetermining the minimum number of EFX allocations for a given instance, arguing\nthat this approach may yield valuable insights into the existence and\ncomputation of EFX allocations. We focus on restricted instances where the\nnumber of goods slightly exceeds the number of agents, and extend our analysis\nto weighted EFX (WEFX) and a novel variant of EFX for general monotone\nvaluations, termed EFX+. In doing so, we identify the transition threshold for\nthe existence of allocations satisfying these fairness notions. Notably, we\nresolve open problems regarding WEFX by proving polynomial-time computability\nunder binary additive valuations, and establishing the first constant-factor\napproximation for two agents.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "econ.TH"
    ],
    "primary_category": "cs.GT",
    "comment": "Appears in the 39th AAAI Conference on Artificial Intelligence\n  (AAAI), 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.03951v1",
    "published_date": "2025-04-04 21:36:09 UTC",
    "updated_date": "2025-04-04 21:36:09 UTC"
  },
  {
    "arxiv_id": "2505.00006v1",
    "title": "Toward a digital twin of U.S. Congress",
    "authors": [
      "Hayden Helm",
      "Tianyi Chen",
      "Harvey McGuinness",
      "Paige Lee",
      "Brandon Duderstadt",
      "Carey E. Priebe"
    ],
    "abstract": "In this paper we provide evidence that a virtual model of U.S.\ncongresspersons based on a collection of language models satisfies the\ndefinition of a digital twin. In particular, we introduce and provide\nhigh-level descriptions of a daily-updated dataset that contains every Tweet\nfrom every U.S. congressperson during their respective terms. We demonstrate\nthat a modern language model equipped with congressperson-specific subsets of\nthis data are capable of producing Tweets that are largely indistinguishable\nfrom actual Tweets posted by their physical counterparts. We illustrate how\ngenerated Tweets can be used to predict roll-call vote behaviors and to\nquantify the likelihood of congresspersons crossing party lines, thereby\nassisting stakeholders in allocating resources and potentially impacting\nreal-world legislative dynamics. We conclude with a discussion of the\nlimitations and important extensions of our analysis.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.00006v1",
    "published_date": "2025-04-04 21:33:36 UTC",
    "updated_date": "2025-04-04 21:33:36 UTC"
  },
  {
    "arxiv_id": "2504.03940v1",
    "title": "Analysis of Robustness of a Large Game Corpus",
    "authors": [
      "Mahsa Bazzaz",
      "Seth Cooper"
    ],
    "abstract": "Procedural content generation via machine learning (PCGML) in games involves\nusing machine learning techniques to create game content such as maps and\nlevels. 2D tile-based game levels have consistently served as a standard\ndataset for PCGML because they are a simplified version of game levels while\nmaintaining the specific constraints typical of games, such as being solvable.\nIn this work, we highlight the unique characteristics of game levels, including\ntheir structured discrete data nature, the local and global constraints\ninherent in the games, and the sensitivity of the game levels to small changes\nin input. We define the robustness of data as a measure of sensitivity to small\nchanges in input that cause a change in output, and we use this measure to\nanalyze and compare these levels to state-of-the-art machine learning datasets,\nshowcasing the subtle differences in their nature. We also constructed a large\ndataset from four games inspired by popular classic tile-based games that\nshowcase these characteristics and address the challenge of sparse data in\nPCGML by providing a significantly larger dataset than those currently\navailable.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03940v1",
    "published_date": "2025-04-04 21:15:13 UTC",
    "updated_date": "2025-04-04 21:15:13 UTC"
  },
  {
    "arxiv_id": "2504.07982v1",
    "title": "Metamorphic Testing for Fairness Evaluation in Large Language Models: Identifying Intersectional Bias in LLaMA and GPT",
    "authors": [
      "Harishwar Reddy",
      "Madhusudan Srinivasan",
      "Upulee Kanewala"
    ],
    "abstract": "Large Language Models (LLMs) have made significant strides in Natural\nLanguage Processing but remain vulnerable to fairness-related issues, often\nreflecting biases inherent in their training data. These biases pose risks,\nparticularly when LLMs are deployed in sensitive areas such as healthcare,\nfinance, and law. This paper introduces a metamorphic testing approach to\nsystematically identify fairness bugs in LLMs. We define and apply a set of\nfairness-oriented metamorphic relations (MRs) to assess the LLaMA and GPT\nmodel, a state-of-the-art LLM, across diverse demographic inputs. Our\nmethodology includes generating source and follow-up test cases for each MR and\nanalyzing model responses for fairness violations. The results demonstrate the\neffectiveness of MT in exposing bias patterns, especially in relation to tone\nand sentiment, and highlight specific intersections of sensitive attributes\nthat frequently reveal fairness faults. This research improves fairness testing\nin LLMs, providing a structured approach to detect and mitigate biases and\nimprove model robustness in fairness-sensitive applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.07982v1",
    "published_date": "2025-04-04 21:04:14 UTC",
    "updated_date": "2025-04-04 21:04:14 UTC"
  },
  {
    "arxiv_id": "2504.03931v2",
    "title": "NAACL2025 Tutorial: Adaptation of Large Language Models",
    "authors": [
      "Zixuan Ke",
      "Yifei Ming",
      "Shafiq Joty"
    ],
    "abstract": "This tutorial on adaptation of LLMs is designed to address the growing demand\nfor models that go beyond the static capabilities of generic LLMs by providing\nan overview of dynamic, domain-specific, and task-adaptive LLM adaptation\ntechniques. While general LLMs have demonstrated strong generalization across a\nvariety of tasks, they often struggle to perform well in specialized domains\nsuch as finance, healthcare, and code generation for underrepresented\nlanguages. Additionally, their static nature limits their ability to evolve\nwith the changing world, and they are often extremely large in size, making\nthem impractical and costly to deploy at scale. As a result, the adaptation of\nLLMs has drawn much attention since the birth of LLMs and is of core\nimportance, both for industry, which focuses on serving its targeted users, and\nacademia, which can greatly benefit from small but powerful LLMs. To address\nthis gap, this tutorial aims to provide an overview of the LLM adaptation\ntechniques. We start with an introduction to LLM adaptation, from both the data\nperspective and the model perspective. We then emphasize how the evaluation\nmetrics and benchmarks are different from other techniques. After establishing\nthe problems, we explore various adaptation techniques. We categorize\nadaptation techniques into two main families. The first is parametric knowledge\nadaptation, which focuses on updating the parametric knowledge within LLMs.\nAdditionally, we will discuss real-time adaptation techniques, including model\nediting, which allows LLMs to be updated dynamically in production\nenvironments. The second kind of adaptation is semi-parametric knowledge\nadaptation, where the goal is to update LLM parameters to better leverage\nexternal knowledge or tools through techniques like retrieval-augmented\ngeneration (RAG) and agent-based systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL2025 Tutorial",
    "pdf_url": "http://arxiv.org/pdf/2504.03931v2",
    "published_date": "2025-04-04 20:57:41 UTC",
    "updated_date": "2025-04-21 04:07:33 UTC"
  },
  {
    "arxiv_id": "2504.03930v1",
    "title": "Have Large Language Models Learned to Reason? A Characterization via 3-SAT Phase Transition",
    "authors": [
      "Rishi Hazra",
      "Gabriele Venturato",
      "Pedro Zuidberg Dos Martires",
      "Luc De Raedt"
    ],
    "abstract": "Large Language Models (LLMs) have been touted as AI models possessing\nadvanced reasoning abilities. In theory, autoregressive LLMs with\nChain-of-Thought (CoT) can perform more serial computations to solve complex\nreasoning tasks. However, recent studies suggest that, despite this capacity,\nLLMs do not truly learn to reason but instead fit on statistical features. To\nstudy the reasoning capabilities in a principled fashion, we adopt a\ncomputational theory perspective and propose an experimental protocol centered\non 3-SAT -- the prototypical NP-complete problem lying at the core of logical\nreasoning and constraint satisfaction tasks. Specifically, we examine the phase\ntransitions in random 3-SAT and characterize the reasoning abilities of\nstate-of-the-art LLMs by varying the inherent hardness of the problem\ninstances. By comparing DeepSeek R1 with other LLMs, our findings reveal two\nkey insights (1) LLM accuracy drops significantly on harder instances,\nsuggesting all current models struggle when statistical shortcuts are\nunavailable (2) Unlike other LLMs, R1 shows signs of having learned the\nunderlying reasoning. Following a principled experimental protocol, our study\nmoves beyond the benchmark-driven evidence often found in LLM reasoning\nresearch. Our findings highlight important gaps and suggest clear directions\nfor future research.",
    "categories": [
      "cs.AI",
      "cs.CC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "An updated version of arXiv:2408.07215v2, featuring: (1) inclusion of\n  recent LRMs and recent LLMs, (2) revised conclusions reflecting recent\n  developments, and (3) updated analysis",
    "pdf_url": "http://arxiv.org/pdf/2504.03930v1",
    "published_date": "2025-04-04 20:57:36 UTC",
    "updated_date": "2025-04-04 20:57:36 UTC"
  },
  {
    "arxiv_id": "2504.05334v1",
    "title": "Level Generation with Constrained Expressive Range",
    "authors": [
      "Mahsa Bazzaz",
      "Seth Cooper"
    ],
    "abstract": "Expressive range analysis is a visualization-based technique used to evaluate\nthe performance of generative models, particularly in game level generation. It\ntypically employs two quantifiable metrics to position generated artifacts on a\n2D plot, offering insight into how content is distributed within a defined\nmetric space. In this work, we use the expressive range of a generator as the\nconceptual space of possible creations. Inspired by the quality diversity\nparadigm, we explore this space to generate levels. To do so, we use a\nconstraint-based generator that systematically traverses and generates levels\nin this space. To train the constraint-based generator we use different tile\npatterns to learn from the initial example levels. We analyze how different\npatterns influence the exploration of the expressive range. Specifically, we\ncompare the exploration process based on time, the number of successful and\nfailed sample generations, and the overall interestingness of the generated\nlevels. Unlike typical quality diversity approaches that rely on random\ngeneration and hope to get good coverage of the expressive range, this approach\nsystematically traverses the grid ensuring more coverage. This helps create\nunique and interesting game levels while also improving our understanding of\nthe generator's strengths and limitations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.05334v1",
    "published_date": "2025-04-04 20:55:30 UTC",
    "updated_date": "2025-04-04 20:55:30 UTC"
  },
  {
    "arxiv_id": "2504.13889v1",
    "title": "Maestoso: An Intelligent Educational Sketching Tool for Learning Music Theory",
    "authors": [
      "Paul Taele",
      "Laura Barreto",
      "Tracy Hammond"
    ],
    "abstract": "Learning music theory not only has practical benefits for musicians to write,\nperform, understand, and express music better, but also for both non-musicians\nto improve critical thinking, math analytical skills, and music appreciation.\nHowever, current external tools applicable for learning music theory through\nwriting when human instruction is unavailable are either limited in feedback,\nlacking a written modality, or assuming already strong familiarity of music\ntheory concepts. In this paper, we describe Maestoso, an educational tool for\nnovice learners to learn music theory through sketching practice of quizzed\nmusic structures. Maestoso first automatically recognizes students' sketched\ninput of quizzed concepts, then relies on existing sketch and gesture\nrecognition techniques to automatically recognize the input, and finally\ngenerates instructor-emulated feedback. From our evaluations, we demonstrate\nthat Maestoso performs reasonably well on recognizing music structure elements\nand that novice students can comfortably grasp introductory music theory in a\nsingle session.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.13889v1",
    "published_date": "2025-04-04 20:46:24 UTC",
    "updated_date": "2025-04-04 20:46:24 UTC"
  },
  {
    "arxiv_id": "2504.03915v1",
    "title": "RF-BayesPhysNet: A Bayesian rPPG Uncertainty Estimation Method for Complex Scenarios",
    "authors": [
      "Rufei Ma",
      "Chao Chen"
    ],
    "abstract": "Remote photoplethysmography (rPPG) technology infers heart rate by capturing\nsubtle color changes in facial skin\n  using a camera, demonstrating great potential in non-contact heart rate\nmeasurement. However, measurement\n  accuracy significantly decreases in complex scenarios such as lighting\nchanges and head movements compared\n  to ideal laboratory conditions. Existing deep learning models often neglect\nthe quantification of measurement\n  uncertainty, limiting their credibility in dynamic scenes. To address the\nissue of insufficient rPPG measurement\n  reliability in complex scenarios, this paper introduces Bayesian neural\nnetworks to the rPPG field for the first time,\n  proposing the Robust Fusion Bayesian Physiological Network (RF-BayesPhysNet),\nwhich can model both aleatoric\n  and epistemic uncertainty. It leverages variational inference to balance\naccuracy and computational efficiency.\n  Due to the current lack of uncertainty estimation metrics in the rPPG field,\nthis paper also proposes a new set of\n  methods, using Spearman correlation coefficient, prediction interval\ncoverage, and confidence interval width, to\n  measure the effectiveness of uncertainty estimation methods under different\nnoise conditions. Experiments show\n  that the model, with only double the parameters compared to traditional\nnetwork models, achieves a MAE of 2.56\n  on the UBFC-RPPG dataset, surpassing most models. It demonstrates good\nuncertainty estimation capability\n  in no-noise and low-noise conditions, providing prediction confidence and\nsignificantly enhancing robustness in\n  real-world applications. We have open-sourced the code at\nhttps://github.com/AIDC-rPPG/RF-Net",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T07, 62F15, 94A12",
      "I.2.6; I.5.4; C.3"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.03915v1",
    "published_date": "2025-04-04 20:24:57 UTC",
    "updated_date": "2025-04-04 20:24:57 UTC"
  },
  {
    "arxiv_id": "2504.13888v1",
    "title": "Kanji Workbook: A Writing-Based Intelligent Tutoring System for Learning Proper Japanese Kanji Writing Technique with Instructor-Emulated Assessment",
    "authors": [
      "Paul Taele",
      "Jung In Koh",
      "Tracy Hammond"
    ],
    "abstract": "Kanji script writing is a skill that is often introduced to novice Japanese\nforeign language students for achieving Japanese writing mastery, but often\nposes difficulties to students with primarily English fluency due to their its\nvast differences with written English. Instructors often introduce various\npedagogical methods -- such as visual structure and written techniques -- to\nassist students in kanji study, but may lack availability providing direct\nfeedback on students' writing outside of class. Current educational\napplications are also limited due to lacking richer instructor-emulated\nfeedback. We introduce Kanji Workbook, a writing-based intelligent tutoring\nsystem for students to receive intelligent assessment that emulates human\ninstructor feedback. Our interface not only leverages students' computing\ndevices for allowing them to learn, practice, and review the writing of\nprompted characters from their course's kanji script lessons, but also provides\na diverse set of writing assessment metrics -- derived from instructor\ninterviews and classroom observation insights -- through intelligent scoring\nand visual animations. We deployed our interface onto novice- and\nintermediate-level university courses over an entire academic year, and\nobserved that interface users on average achieved higher course grades than\ntheir peers and also reacted positively to our interface's various features.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.13888v1",
    "published_date": "2025-04-04 19:59:27 UTC",
    "updated_date": "2025-04-04 19:59:27 UTC"
  },
  {
    "arxiv_id": "2504.03894v1",
    "title": "Leveraging Gait Patterns as Biomarkers: An attention-guided Deep Multiple Instance Learning Network for Scoliosis Classification",
    "authors": [
      "Haiqing Li",
      "Yuzhi Guo",
      "Feng Jiang",
      "Qifeng Zhou",
      "Hehuan Ma",
      "Junzhou Huang"
    ],
    "abstract": "Scoliosis is a spinal curvature disorder that is difficult to detect early\nand can compress the chest cavity, impacting respiratory function and cardiac\nhealth. Especially for adolescents, delayed detection and treatment result in\nworsening compression. Traditional scoliosis detection methods heavily rely on\nclinical expertise, and X-ray imaging poses radiation risks, limiting\nlarge-scale early screening. We propose an Attention-Guided Deep Multi-Instance\nLearning method (Gait-MIL) to effectively capture discriminative features from\ngait patterns, which is inspired by ScoNet-MT's pioneering use of gait patterns\nfor scoliosis detection. We evaluate our method on the first large-scale\ndataset based on gait patterns for scoliosis classification. The results\ndemonstrate that our study improves the performance of using gait as a\nbiomarker for scoliosis detection, significantly enhances detection accuracy\nfor the particularly challenging Neutral cases, where subtle indicators are\noften overlooked. Our Gait-MIL also performs robustly in imbalanced scenarios,\nmaking it a promising tool for large-scale scoliosis screening.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.03894v1",
    "published_date": "2025-04-04 19:35:33 UTC",
    "updated_date": "2025-04-04 19:35:33 UTC"
  },
  {
    "arxiv_id": "2504.03888v1",
    "title": "Investigating Affective Use and Emotional Well-being on ChatGPT",
    "authors": [
      "Jason Phang",
      "Michael Lampe",
      "Lama Ahmad",
      "Sandhini Agarwal",
      "Cathy Mengying Fang",
      "Auren R. Liu",
      "Valdemar Danry",
      "Eunhae Lee",
      "Samantha W. T. Chan",
      "Pat Pataranutaporn",
      "Pattie Maes"
    ],
    "abstract": "As AI chatbots see increased adoption and integration into everyday life,\nquestions have been raised about the potential impact of human-like or\nanthropomorphic AI on users. In this work, we investigate the extent to which\ninteractions with ChatGPT (with a focus on Advanced Voice Mode) may impact\nusers' emotional well-being, behaviors and experiences through two parallel\nstudies. To study the affective use of AI chatbots, we perform large-scale\nautomated analysis of ChatGPT platform usage in a privacy-preserving manner,\nanalyzing over 3 million conversations for affective cues and surveying over\n4,000 users on their perceptions of ChatGPT. To investigate whether there is a\nrelationship between model usage and emotional well-being, we conduct an\nInstitutional Review Board (IRB)-approved randomized controlled trial (RCT) on\nclose to 1,000 participants over 28 days, examining changes in their emotional\nwell-being as they interact with ChatGPT under different experimental settings.\nIn both on-platform data analysis and the RCT, we observe that very high usage\ncorrelates with increased self-reported indicators of dependence. From our RCT,\nwe find that the impact of voice-based interactions on emotional well-being to\nbe highly nuanced, and influenced by factors such as the user's initial\nemotional state and total usage duration. Overall, our analysis reveals that a\nsmall number of users are responsible for a disproportionate share of the most\naffective cues.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03888v1",
    "published_date": "2025-04-04 19:22:10 UTC",
    "updated_date": "2025-04-04 19:22:10 UTC"
  },
  {
    "arxiv_id": "2504.03887v1",
    "title": "Accurate GPU Memory Prediction for Deep Learning Jobs through Dynamic Analysis",
    "authors": [
      "Jiabo Shi",
      "Yehia Elkhatib"
    ],
    "abstract": "The benefits of Deep Learning (DL) impose significant pressure on GPU\nresources, particularly within GPU cluster, where Out-Of-Memory (OOM) errors\npresent a primary impediment to model training and efficient resource\nutilization. Conventional OOM estimation techniques, relying either on static\ngraph analysis or direct GPU memory profiling, suffer from inherent\nlimitations: static analysis often fails to capture model dynamics, whereas\nGPU-based profiling intensifies contention for scarce GPU resources. To\novercome these constraints, VeritasEst emerges. It is an innovative, entirely\nCPU-based analysis tool capable of accurately predicting the peak GPU memory\nrequired for DL training tasks without accessing the target GPU. This \"offline\"\nprediction capability is core advantage of VeritasEst, allowing accurate memory\nfootprint information to be obtained before task scheduling, thereby\neffectively preventing OOM and optimizing GPU allocation. Its performance was\nvalidated through thousands of experimental runs across convolutional neural\nnetwork (CNN) models: Compared to baseline GPU memory estimators, VeritasEst\nsignificantly reduces the relative error by 84% and lowers the estimation\nfailure probability by 73%. VeritasEst represents a key step towards efficient\nand predictable DL training in resource-constrained environments.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03887v1",
    "published_date": "2025-04-04 19:20:03 UTC",
    "updated_date": "2025-04-04 19:20:03 UTC"
  },
  {
    "arxiv_id": "2504.03861v1",
    "title": "Improving World Models using Deep Supervision with Linear Probes",
    "authors": [
      "Andrii Zahorodnii"
    ],
    "abstract": "Developing effective world models is crucial for creating artificial agents\nthat can reason about and navigate complex environments. In this paper, we\ninvestigate a deep supervision technique for encouraging the development of a\nworld model in a network trained end-to-end to predict the next observation.\nWhile deep supervision has been widely applied for task-specific learning, our\nfocus is on improving the world models. Using an experimental environment based\non the Flappy Bird game, where the agent receives only LIDAR measurements as\nobservations, we explore the effect of adding a linear probe component to the\nnetwork's loss function. This additional term encourages the network to encode\na subset of the true underlying world features into its hidden state. Our\nexperiments demonstrate that this supervision technique improves both training\nand test performance, enhances training stability, and results in more easily\ndecodable world features -- even for those world features which were not\nincluded in the training. Furthermore, we observe a reduced distribution drift\nin networks trained with the linear probe, particularly during high-variability\nphases of the game (flying between successive pipe encounters). Including the\nworld features loss component roughly corresponded to doubling the model size,\nsuggesting that the linear probe technique is particularly beneficial in\ncompute-limited settings or when aiming to achieve the best performance with\nsmaller models. These findings contribute to our understanding of how to\ndevelop more robust and sophisticated world models in artificial agents, paving\nthe way for further advancements in this field.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "ICLR 2025 Workshop on World Models",
    "pdf_url": "http://arxiv.org/pdf/2504.03861v1",
    "published_date": "2025-04-04 18:35:21 UTC",
    "updated_date": "2025-04-04 18:35:21 UTC"
  },
  {
    "arxiv_id": "2504.03857v1",
    "title": "Can ChatGPT Learn My Life From a Week of First-Person Video?",
    "authors": [
      "Keegan Harris"
    ],
    "abstract": "Motivated by recent improvements in generative AI and wearable camera devices\n(e.g. smart glasses and AI-enabled pins), I investigate the ability of\nfoundation models to learn about the wearer's personal life through\nfirst-person camera data. To test this, I wore a camera headset for 54 hours\nover the course of a week, generated summaries of various lengths (e.g.\nminute-long, hour-long, and day-long summaries), and fine-tuned both GPT-4o and\nGPT-4o-mini on the resulting summary hierarchy. By querying the fine-tuned\nmodels, we are able to learn what the models learned about me. The results are\nmixed: Both models learned basic information about me (e.g. approximate age,\ngender). Moreover, GPT-4o correctly deduced that I live in Pittsburgh, am a PhD\nstudent at CMU, am right-handed, and have a pet cat. However, both models also\nsuffered from hallucination and would make up names for the individuals present\nin the video footage of my life.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03857v1",
    "published_date": "2025-04-04 18:33:45 UTC",
    "updated_date": "2025-04-04 18:33:45 UTC"
  },
  {
    "arxiv_id": "2504.03850v1",
    "title": "Detection Limits and Statistical Separability of Tree Ring Watermarks in Rectified Flow-based Text-to-Image Generation Models",
    "authors": [
      "Ved Umrajkar",
      "Aakash Kumar Singh"
    ],
    "abstract": "Tree-Ring Watermarking is a significant technique for authenticating\nAI-generated images. However, its effectiveness in rectified flow-based models\nremains unexplored, particularly given the inherent challenges of these models\nwith noise latent inversion. Through extensive experimentation, we evaluated\nand compared the detection and separability of watermarks between SD 2.1 and\nFLUX.1-dev models. By analyzing various text guidance configurations and\naugmentation attacks, we demonstrate how inversion limitations affect both\nwatermark recovery and the statistical separation between watermarked and\nunwatermarked images. Our findings provide valuable insights into the current\nlimitations of Tree-Ring Watermarking in the current SOTA models and highlight\nthe critical need for improved inversion methods to achieve reliable watermark\ndetection and separability. The official implementation, dataset release and\nall experimental results are available at this\n\\href{https://github.com/dsgiitr/flux-watermarking}{\\textbf{link}}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03850v1",
    "published_date": "2025-04-04 18:24:23 UTC",
    "updated_date": "2025-04-04 18:24:23 UTC"
  },
  {
    "arxiv_id": "2504.08779v1",
    "title": "Can AI Master Construction Management (CM)? Benchmarking State-of-the-Art Large Language Models on CM Certification Exams",
    "authors": [
      "Ruoxin Xiong",
      "Yanyu Wang",
      "Suat Gunhan",
      "Yimin Zhu",
      "Charles Berryman"
    ],
    "abstract": "The growing complexity of construction management (CM) projects, coupled with\nchallenges such as strict regulatory requirements and labor shortages, requires\nspecialized analytical tools that streamline project workflow and enhance\nperformance. Although large language models (LLMs) have demonstrated\nexceptional performance in general reasoning tasks, their effectiveness in\ntackling CM-specific challenges, such as precise quantitative analysis and\nregulatory interpretation, remains inadequately explored. To bridge this gap,\nthis study introduces CMExamSet, a comprehensive benchmarking dataset\ncomprising 689 authentic multiple-choice questions sourced from four nationally\naccredited CM certification exams. Our zero-shot evaluation assesses overall\naccuracy, subject areas (e.g., construction safety), reasoning complexity\n(single-step and multi-step), and question formats (text-only,\nfigure-referenced, and table-referenced). The results indicate that GPT-4o and\nClaude 3.7 surpass typical human pass thresholds (70%), with average accuracies\nof 82% and 83%, respectively. Additionally, both models performed better on\nsingle-step tasks, with accuracies of 85.7% (GPT-4o) and 86.7% (Claude 3.7).\nMulti-step tasks were more challenging, reducing performance to 76.5% and\n77.6%, respectively. Furthermore, both LLMs show significant limitations on\nfigure-referenced questions, with accuracies dropping to approximately 40%. Our\nerror pattern analysis further reveals that conceptual misunderstandings are\nthe most common (44.4% and 47.9%), underscoring the need for enhanced\ndomain-specific reasoning models. These findings underscore the potential of\nLLMs as valuable supplementary analytical tools in CM, while highlighting the\nneed for domain-specific refinements and sustained human oversight in complex\ndecision making.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08779v1",
    "published_date": "2025-04-04 18:13:45 UTC",
    "updated_date": "2025-04-04 18:13:45 UTC"
  },
  {
    "arxiv_id": "2504.03640v1",
    "title": "Bonsai: Interpretable Tree-Adaptive Grounded Reasoning",
    "authors": [
      "Kate Sanders",
      "Benjamin Van Durme"
    ],
    "abstract": "To develop general-purpose collaborative agents, humans need reliable AI\nsystems that can (1) adapt to new domains and (2) transparently reason with\nuncertainty to allow for verification and correction. Black-box models\ndemonstrate powerful data processing abilities but do not satisfy these\ncriteria due to their opaqueness, domain specificity, and lack of uncertainty\nawareness. We introduce Bonsai, a compositional and probabilistic reasoning\nsystem that generates adaptable inference trees by retrieving relevant\ngrounding evidence and using it to compute likelihoods of sub-claims derived\nfrom broader natural language inferences. Bonsai's reasoning power is tunable\nat test-time via evidence scaling and it demonstrates reliable handling of\nvaried domains including transcripts, photographs, videos, audio, and\ndatabases. Question-answering and human alignment experiments demonstrate that\nBonsai matches the performance of domain-specific black-box methods while\ngenerating interpretable, grounded, and uncertainty-aware reasoning traces.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "68T50, 68T37",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, preprint",
    "pdf_url": "http://arxiv.org/pdf/2504.03640v1",
    "published_date": "2025-04-04 17:59:50 UTC",
    "updated_date": "2025-04-04 17:59:50 UTC"
  },
  {
    "arxiv_id": "2504.03635v1",
    "title": "Do Larger Language Models Imply Better Reasoning? A Pretraining Scaling Law for Reasoning",
    "authors": [
      "Xinyi Wang",
      "Shawn Tan",
      "Mingyu Jin",
      "William Yang Wang",
      "Rameswar Panda",
      "Yikang Shen"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks requiring complex reasoning. However, the effects of\nscaling on their reasoning abilities remain insufficiently understood. In this\npaper, we introduce a synthetic multihop reasoning environment designed to\nclosely replicate the structure and distribution of real-world large-scale\nknowledge graphs. Our reasoning task involves completing missing edges in the\ngraph, which requires advanced multi-hop reasoning and mimics real-world\nreasoning scenarios. To evaluate this, we pretrain language models (LMs) from\nscratch solely on triples from the incomplete graph and assess their ability to\ninfer the missing edges. Interestingly, we observe that overparameterization\ncan impair reasoning performance due to excessive memorization. We investigate\ndifferent factors that affect this U-shaped loss curve, including graph\nstructure, model size, and training steps. To predict the optimal model size\nfor a specific knowledge graph, we find an empirical scaling that linearly maps\nthe knowledge graph search entropy to the optimal model size. This work\nprovides new insights into the relationship between scaling and reasoning in\nLLMs, shedding light on possible ways to optimize their performance for\nreasoning tasks.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03635v1",
    "published_date": "2025-04-04 17:57:22 UTC",
    "updated_date": "2025-04-04 17:57:22 UTC"
  },
  {
    "arxiv_id": "2504.03624v3",
    "title": "Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models",
    "authors": [
      "NVIDIA",
      ":",
      "Aaron Blakeman",
      "Aarti Basant",
      "Abhinav Khattar",
      "Adithya Renduchintala",
      "Akhiad Bercovich",
      "Aleksander Ficek",
      "Alexis Bjorlin",
      "Ali Taghibakhshi",
      "Amala Sanjay Deshmukh",
      "Ameya Sunil Mahabaleshwarkar",
      "Andrew Tao",
      "Anna Shors",
      "Ashwath Aithal",
      "Ashwin Poojary",
      "Ayush Dattagupta",
      "Balaram Buddharaju",
      "Bobby Chen",
      "Boris Ginsburg",
      "Boxin Wang",
      "Brandon Norick",
      "Brian Butterfield",
      "Bryan Catanzaro",
      "Carlo del Mundo",
      "Chengyu Dong",
      "Christine Harvey",
      "Christopher Parisien",
      "Dan Su",
      "Daniel Korzekwa",
      "Danny Yin",
      "Daria Gitman",
      "David Mosallanezhad",
      "Deepak Narayanan",
      "Denys Fridman",
      "Dima Rekesh",
      "Ding Ma",
      "Dmytro Pykhtar",
      "Dong Ahn",
      "Duncan Riach",
      "Dusan Stosic",
      "Eileen Long",
      "Elad Segal",
      "Ellie Evans",
      "Eric Chung",
      "Erick Galinkin",
      "Evelina Bakhturina",
      "Ewa Dobrowolska",
      "Fei Jia",
      "Fuxiao Liu",
      "Gargi Prasad",
      "Gerald Shen",
      "Guilin Liu",
      "Guo Chen",
      "Haifeng Qian",
      "Helen Ngo",
      "Hongbin Liu",
      "Hui Li",
      "Igor Gitman",
      "Ilia Karmanov",
      "Ivan Moshkov",
      "Izik Golan",
      "Jan Kautz",
      "Jane Polak Scowcroft",
      "Jared Casper",
      "Jarno Seppanen",
      "Jason Lu",
      "Jason Sewall",
      "Jiaqi Zeng",
      "Jiaxuan You",
      "Jimmy Zhang",
      "Jing Zhang",
      "Jining Huang",
      "Jinze Xue",
      "Jocelyn Huang",
      "Joey Conway",
      "John Kamalu",
      "Jon Barker",
      "Jonathan Cohen",
      "Joseph Jennings",
      "Jupinder Parmar",
      "Karan Sapra",
      "Kari Briski",
      "Kateryna Chumachenko",
      "Katherine Luna",
      "Keshav Santhanam",
      "Kezhi Kong",
      "Kirthi Sivamani",
      "Krzysztof Pawelec",
      "Kumar Anik",
      "Kunlun Li",
      "Lawrence McAfee",
      "Leon Derczynski",
      "Lindsey Pavao",
      "Luis Vega",
      "Lukas Voegtle",
      "Maciej Bala",
      "Maer Rodrigues de Melo",
      "Makesh Narsimhan Sreedhar",
      "Marcin Chochowski",
      "Markus Kliegl",
      "Marta Stepniewska-Dziubinska",
      "Matthieu Le",
      "Matvei Novikov",
      "Mehrzad Samadi",
      "Michael Andersch",
      "Michael Evans",
      "Miguel Martinez",
      "Mike Chrzanowski",
      "Mike Ranzinger",
      "Mikolaj Blaz",
      "Misha Smelyanskiy",
      "Mohamed Fawzy",
      "Mohammad Shoeybi",
      "Mostofa Patwary",
      "Nayeon Lee",
      "Nima Tajbakhsh",
      "Ning Xu",
      "Oleg Rybakov",
      "Oleksii Kuchaiev",
      "Olivier Delalleau",
      "Osvald Nitski",
      "Parth Chadha",
      "Pasha Shamis",
      "Paulius Micikevicius",
      "Pavlo Molchanov",
      "Peter Dykas",
      "Philipp Fischer",
      "Pierre-Yves Aquilanti",
      "Piotr Bialecki",
      "Prasoon Varshney",
      "Pritam Gundecha",
      "Przemek Tredak",
      "Rabeeh Karimi",
      "Rahul Kandu",
      "Ran El-Yaniv",
      "Raviraj Joshi",
      "Roger Waleffe",
      "Ruoxi Zhang",
      "Sabrina Kavanaugh",
      "Sahil Jain",
      "Samuel Kriman",
      "Sangkug Lym",
      "Sanjeev Satheesh",
      "Saurav Muralidharan",
      "Sean Narenthiran",
      "Selvaraj Anandaraj",
      "Seonmyeong Bak",
      "Sergey Kashirsky",
      "Seungju Han",
      "Shantanu Acharya",
      "Shaona Ghosh",
      "Sharath Turuvekere Sreenivas",
      "Sharon Clay",
      "Shelby Thomas",
      "Shrimai Prabhumoye",
      "Shubham Pachori",
      "Shubham Toshniwal",
      "Shyamala Prayaga",
      "Siddhartha Jain",
      "Sirshak Das",
      "Slawek Kierat",
      "Somshubra Majumdar",
      "Song Han",
      "Soumye Singhal",
      "Sriharsha Niverty",
      "Stefania Alborghetti",
      "Suseella Panguluri",
      "Swetha Bhendigeri",
      "Syeda Nahida Akter",
      "Szymon Migacz",
      "Tal Shiri",
      "Terry Kong",
      "Timo Roman",
      "Tomer Ronen",
      "Trisha Saar",
      "Tugrul Konuk",
      "Tuomas Rintamaki",
      "Tyler Poon",
      "Ushnish De",
      "Vahid Noroozi",
      "Varun Singh",
      "Vijay Korthikanti",
      "Vitaly Kurin",
      "Wasi Uddin Ahmad",
      "Wei Du",
      "Wei Ping",
      "Wenliang Dai",
      "Wonmin Byeon",
      "Xiaowei Ren",
      "Yao Xu",
      "Yejin Choi",
      "Yian Zhang",
      "Ying Lin",
      "Yoshi Suhara",
      "Zhiding Yu",
      "Zhiqi Li",
      "Zhiyu Li",
      "Zhongbo Zhu",
      "Zhuolin Yang",
      "Zijia Chen"
    ],
    "abstract": "As inference-time scaling becomes critical for enhanced reasoning\ncapabilities, it is increasingly becoming important to build models that are\nefficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid\nMamba-Transformer models designed to reduce inference cost for a given accuracy\nlevel. To achieve this goal, we replace the majority of self-attention layers\nin the common Transformer model architecture with Mamba layers that perform\nconstant computation and require constant memory per generated token. We show\nthat Nemotron-H models offer either better or on-par accuracy compared to other\nsimilarly-sized state-of-the-art open-sourced Transformer models (e.g.,\nQwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3$\\times$ faster at\ninference. To further increase inference speed and reduce the memory required\nat inference time, we created Nemotron-H-47B-Base from the 56B model using a\nnew compression via pruning and distillation technique called MiniPuzzle.\nNemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20%\nfaster to infer. In addition, we introduce an FP8-based training recipe and\nshow that it can achieve on par results with BF16-based training. This recipe\nis used to train the 56B model. We are releasing Nemotron-H base model\ncheckpoints with support in Hugging Face and NeMo.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03624v3",
    "published_date": "2025-04-04 17:41:58 UTC",
    "updated_date": "2025-04-15 14:36:01 UTC"
  },
  {
    "arxiv_id": "2504.03622v1",
    "title": "Align to Structure: Aligning Large Language Models with Structural Information",
    "authors": [
      "Zae Myung Kim",
      "Anand Ramachandran",
      "Farideh Tavazoee",
      "Joo-Kyung Kim",
      "Oleg Rokhlenko",
      "Dongyeop Kang"
    ],
    "abstract": "Generating long, coherent text remains a challenge for large language models\n(LLMs), as they lack hierarchical planning and structured organization in\ndiscourse generation. We introduce Structural Alignment, a novel method that\naligns LLMs with human-like discourse structures to enhance long-form text\ngeneration. By integrating linguistically grounded discourse frameworks into\nreinforcement learning, our approach guides models to produce coherent and\nwell-organized outputs. We employ a dense reward scheme within a Proximal\nPolicy Optimization framework, assigning fine-grained, token-level rewards\nbased on the discourse distinctiveness relative to human writing. Two\ncomplementary reward models are evaluated: the first improves readability by\nscoring surface-level textual features to provide explicit structuring, while\nthe second reinforces deeper coherence and rhetorical sophistication by\nanalyzing global discourse patterns through hierarchical discourse motifs,\noutperforming both standard and RLHF-enhanced models in tasks such as essay\ngeneration and long-document summarization. All training data and code will be\npublicly shared at https://github.com/minnesotanlp/struct_align.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03622v1",
    "published_date": "2025-04-04 17:40:04 UTC",
    "updated_date": "2025-04-04 17:40:04 UTC"
  },
  {
    "arxiv_id": "2504.03822v1",
    "title": "Arti-\"fickle\" Intelligence: Using LLMs as a Tool for Inference in the Political and Social Sciences",
    "authors": [
      "Lisa P. Argyle",
      "Ethan C. Busby",
      "Joshua R. Gubler",
      "Bryce Hepner",
      "Alex Lyman",
      "David Wingate"
    ],
    "abstract": "Generative large language models (LLMs) are incredibly useful, versatile, and\npromising tools. However, they will be of most use to political and social\nscience researchers when they are used in a way that advances understanding\nabout real human behaviors and concerns. To promote the scientific use of LLMs,\nwe suggest that researchers in the political and social sciences need to remain\nfocused on the scientific goal of inference. To this end, we discuss the\nchallenges and opportunities related to scientific inference with LLMs, using\nvalidation of model output as an illustrative case for discussion. We propose a\nset of guidelines related to establishing the failure and success of LLMs when\ncompleting particular tasks, and discuss how we can make inferences from these\nobservations. We conclude with a discussion of how this refocus will improve\nthe accumulation of shared scientific knowledge about these tools and their\nuses in the social sciences.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03822v1",
    "published_date": "2025-04-04 17:35:45 UTC",
    "updated_date": "2025-04-04 17:35:45 UTC"
  },
  {
    "arxiv_id": "2504.03616v1",
    "title": "Multilingual Retrieval-Augmented Generation for Knowledge-Intensive Task",
    "authors": [
      "Leonardo Ranaldi",
      "Barry Haddow",
      "Alexandra Birch"
    ],
    "abstract": "Retrieval-augmented generation (RAG) has become a cornerstone of contemporary\nNLP, enhancing large language models (LLMs) by allowing them to access richer\nfactual contexts through in-context retrieval. While effective in monolingual\nsettings, especially in English, its use in multilingual tasks remains\nunexplored. This paper investigates the effectiveness of RAG across multiple\nlanguages by proposing novel approaches for multilingual open-domain\nquestion-answering. We evaluate the performance of various multilingual RAG\nstrategies, including question-translation (tRAG), which translates questions\ninto English before retrieval, and Multilingual RAG (MultiRAG), where retrieval\noccurs directly across multiple languages. Our findings reveal that tRAG, while\nuseful, suffers from limited coverage. In contrast, MultiRAG improves\nefficiency by enabling multilingual retrieval but introduces inconsistencies\ndue to cross-lingual variations in the retrieved content. To address these\nissues, we propose Crosslingual RAG (CrossRAG), a method that translates\nretrieved documents into a common language (e.g., English) before generating\nthe response. Our experiments show that CrossRAG significantly enhances\nperformance on knowledge-intensive tasks, benefiting both high-resource and\nlow-resource languages.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03616v1",
    "published_date": "2025-04-04 17:35:43 UTC",
    "updated_date": "2025-04-04 17:35:43 UTC"
  },
  {
    "arxiv_id": "2504.03615v1",
    "title": "Autonomous and Self-Adapting System for Synthetic Media Detection and Attribution",
    "authors": [
      "Aref Azizpour",
      "Tai D. Nguyen",
      "Matthew C. Stamm"
    ],
    "abstract": "Rapid advances in generative AI have enabled the creation of highly realistic\nsynthetic images, which, while beneficial in many domains, also pose serious\nrisks in terms of disinformation, fraud, and other malicious applications.\nCurrent synthetic image identification systems are typically static, relying on\nfeature representations learned from known generators; as new generative models\nemerge, these systems suffer from severe performance degradation. In this\npaper, we introduce the concept of an autonomous self-adaptive synthetic media\nidentification system -- one that not only detects synthetic images and\nattributes them to known sources but also autonomously identifies and\nincorporates novel generators without human intervention. Our approach\nleverages an open-set identification strategy with an evolvable embedding space\nthat distinguishes between known and unknown sources. By employing an\nunsupervised clustering method to aggregate unknown samples into\nhigh-confidence clusters and continuously refining its decision boundaries, our\nsystem maintains robust detection and attribution performance even as the\ngenerative landscape evolves. Extensive experiments demonstrate that our method\nsignificantly outperforms existing approaches, marking a crucial step toward\nuniversal, adaptable forensic systems in the era of rapidly advancing\ngenerative models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03615v1",
    "published_date": "2025-04-04 17:33:59 UTC",
    "updated_date": "2025-04-04 17:33:59 UTC"
  },
  {
    "arxiv_id": "2504.03603v1",
    "title": "Towards deployment-centric multimodal AI beyond vision and language",
    "authors": [
      "Xianyuan Liu",
      "Jiayang Zhang",
      "Shuo Zhou",
      "Thijs L. van der Plas",
      "Avish Vijayaraghavan",
      "Anastasiia Grishina",
      "Mengdie Zhuang",
      "Daniel Schofield",
      "Christopher Tomlinson",
      "Yuhan Wang",
      "Ruizhe Li",
      "Louisa van Zeeland",
      "Sina Tabakhi",
      "Cyndie Demeocq",
      "Xiang Li",
      "Arunav Das",
      "Orlando Timmerman",
      "Thomas Baldwin-McDonald",
      "Jinge Wu",
      "Peizhen Bai",
      "Zahraa Al Sahili",
      "Omnia Alwazzan",
      "Thao N. Do",
      "Mohammod N. I. Suvon",
      "Angeline Wang",
      "Lucia Cipolina-Kun",
      "Luigi A. Moretti",
      "Lucas Farndale",
      "Nitisha Jain",
      "Natalia Efremova",
      "Yan Ge",
      "Marta Varela",
      "Hak-Keung Lam",
      "Oya Celiktutan",
      "Ben R. Evans",
      "Alejandro Coca-Castro",
      "Honghan Wu",
      "Zahraa S. Abdallah",
      "Chen Chen",
      "Valentin Danchev",
      "Nataliya Tkachenko",
      "Lei Lu",
      "Tingting Zhu",
      "Gregory G. Slabaugh",
      "Roger K. Moore",
      "William K. Cheung",
      "Peter H. Charlton",
      "Haiping Lu"
    ],
    "abstract": "Multimodal artificial intelligence (AI) integrates diverse types of data via\nmachine learning to improve understanding, prediction, and decision-making\nacross disciplines such as healthcare, science, and engineering. However, most\nmultimodal AI advances focus on models for vision and language data, while\ntheir deployability remains a key challenge. We advocate a deployment-centric\nworkflow that incorporates deployment constraints early to reduce the\nlikelihood of undeployable solutions, complementing data-centric and\nmodel-centric approaches. We also emphasise deeper integration across multiple\nlevels of multimodality and multidisciplinary collaboration to significantly\nbroaden the research scope beyond vision and language. To facilitate this\napproach, we identify common multimodal-AI-specific challenges shared across\ndisciplines and examine three real-world use cases: pandemic response,\nself-driving car design, and climate change adaptation, drawing expertise from\nhealthcare, social science, engineering, science, sustainability, and finance.\nBy fostering multidisciplinary dialogue and open research practices, our\ncommunity can accelerate deployment-centric development for broad societal\nimpact.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03603v1",
    "published_date": "2025-04-04 17:20:05 UTC",
    "updated_date": "2025-04-04 17:20:05 UTC"
  },
  {
    "arxiv_id": "2504.03601v3",
    "title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay",
    "authors": [
      "Akshara Prabhakar",
      "Zuxin Liu",
      "Ming Zhu",
      "Jianguo Zhang",
      "Tulika Awalgaonkar",
      "Shiyu Wang",
      "Zhiwei Liu",
      "Haolin Chen",
      "Thai Hoang",
      "Juan Carlos Niebles",
      "Shelby Heinecke",
      "Weiran Yao",
      "Huan Wang",
      "Silvio Savarese",
      "Caiming Xiong"
    ],
    "abstract": "Training effective AI agents for multi-turn interactions requires\nhigh-quality data that captures realistic human-agent dynamics, yet such data\nis scarce and expensive to collect manually. We introduce APIGen-MT, a\ntwo-phase framework that generates verifiable and diverse multi-turn agent\ndata. In the first phase, our agentic pipeline produces detailed task\nblueprints with ground-truth actions, leveraging a committee of LLM reviewers\nand iterative feedback loops. These blueprints are then transformed into\ncomplete interaction trajectories through simulated human-agent interplay. We\ntrain a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B\nto 70B parameters. Our models outperform frontier models such as GPT-4o and\nClaude 3.5 on $\\tau$-bench and BFCL benchmarks, with the smaller models\nsurpassing their larger counterparts, particularly in multi-turn settings,\nwhile maintaining superior consistency across multiple trials. Comprehensive\nexperiments demonstrate that our verified blueprint-to-details approach yields\nhigh-quality training data, enabling the development of more reliable,\nefficient, and capable agents. We open-source 5K synthetic data trajectories\nand the trained xLAM-2-fc-r models to advance research in AI agents.\n  Models at\nhttps://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4;\nDataset at https://huggingface.co/datasets/Salesforce/APIGen-MT-5k and Website\nat https://apigen-mt.github.io",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages plus references and appendices",
    "pdf_url": "http://arxiv.org/pdf/2504.03601v3",
    "published_date": "2025-04-04 17:13:57 UTC",
    "updated_date": "2025-05-05 11:54:13 UTC"
  },
  {
    "arxiv_id": "2504.03600v1",
    "title": "MedSAM2: Segment Anything in 3D Medical Images and Videos",
    "authors": [
      "Jun Ma",
      "Zongxin Yang",
      "Sumin Kim",
      "Bihui Chen",
      "Mohammed Baharoon",
      "Adibvafa Fallahpour",
      "Reza Asakereh",
      "Hongwei Lyu",
      "Bo Wang"
    ],
    "abstract": "Medical image and video segmentation is a critical task for precision\nmedicine, which has witnessed considerable progress in developing task or\nmodality-specific and generalist models for 2D images. However, there have been\nlimited studies on building general-purpose models for 3D images and videos\nwith comprehensive user studies. Here, we present MedSAM2, a promptable\nsegmentation foundation model for 3D image and video segmentation. The model is\ndeveloped by fine-tuning the Segment Anything Model 2 on a large medical\ndataset with over 455,000 3D image-mask pairs and 76,000 frames, outperforming\nprevious models across a wide range of organs, lesions, and imaging modalities.\nFurthermore, we implement a human-in-the-loop pipeline to facilitate the\ncreation of large-scale datasets resulting in, to the best of our knowledge,\nthe most extensive user study to date, involving the annotation of 5,000 CT\nlesions, 3,984 liver MRI lesions, and 251,550 echocardiogram video frames,\ndemonstrating that MedSAM2 can reduce manual costs by more than 85%. MedSAM2 is\nalso integrated into widely used platforms with user-friendly interfaces for\nlocal and cloud deployment, making it a practical tool for supporting\nefficient, scalable, and high-quality segmentation in both research and\nhealthcare environments.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "https://medsam2.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2504.03600v1",
    "published_date": "2025-04-04 17:13:37 UTC",
    "updated_date": "2025-04-04 17:13:37 UTC"
  },
  {
    "arxiv_id": "2504.03598v1",
    "title": "EnrichIndex: Using LLMs to Enrich Retrieval Indices Offline",
    "authors": [
      "Peter Baile Chen",
      "Tomer Wolfson",
      "Michael Cafarella",
      "Dan Roth"
    ],
    "abstract": "Existing information retrieval systems excel in cases where the language of\ntarget documents closely matches that of the user query. However, real-world\nretrieval systems are often required to implicitly reason whether a document is\nrelevant. For example, when retrieving technical texts or tables, their\nrelevance to the user query may be implied through a particular jargon or\nstructure, rather than explicitly expressed in their content. Large language\nmodels (LLMs) hold great potential in identifying such implied relevance by\nleveraging their reasoning skills. Nevertheless, current LLM-augmented\nretrieval is hindered by high latency and computation cost, as the LLM\ntypically computes the query-document relevance online, for every query anew.\nTo tackle this issue we introduce EnrichIndex, a retrieval approach which\ninstead uses the LLM offline to build semantically-enriched retrieval indices,\nby performing a single pass over all documents in the retrieval corpus once\nduring ingestion time. Furthermore, the semantically-enriched indices can\ncomplement existing online retrieval approaches, boosting the performance of\nLLM re-rankers. We evaluated EnrichIndex on five retrieval tasks, involving\npassages and tables, and found that it outperforms strong online LLM-based\nretrieval systems, with an average improvement of 11.7 points in recall @ 10\nand 10.6 points in NDCG @ 10 compared to strong baselines. In terms of online\ncalls to the LLM, it processes 293.3 times fewer tokens which greatly reduces\nthe online latency and cost. Overall, EnrichIndex is an effective way to build\nbetter retrieval indices offline by leveraging the strong reasoning skills of\nLLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "Dataset and code are available at\n  https://peterbaile.github.io/enrichindex/",
    "pdf_url": "http://arxiv.org/pdf/2504.03598v1",
    "published_date": "2025-04-04 17:08:46 UTC",
    "updated_date": "2025-04-04 17:08:46 UTC"
  },
  {
    "arxiv_id": "2504.03597v1",
    "title": "Real-is-Sim: Bridging the Sim-to-Real Gap with a Dynamic Digital Twin for Real-World Robot Policy Evaluation",
    "authors": [
      "Jad Abou-Chakra",
      "Lingfeng Sun",
      "Krishan Rana",
      "Brandon May",
      "Karl Schmeckpeper",
      "Maria Vittoria Minniti",
      "Laura Herlant"
    ],
    "abstract": "Recent advancements in behavior cloning have enabled robots to perform\ncomplex manipulation tasks. However, accurately assessing training performance\nremains challenging, particularly for real-world applications, as behavior\ncloning losses often correlate poorly with actual task success. Consequently,\nresearchers resort to success rate metrics derived from costly and\ntime-consuming real-world evaluations, making the identification of optimal\npolicies and detection of overfitting or underfitting impractical. To address\nthese issues, we propose real-is-sim, a novel behavior cloning framework that\nincorporates a dynamic digital twin (based on Embodied Gaussians) throughout\nthe entire policy development pipeline: data collection, training, and\ndeployment. By continuously aligning the simulated world with the physical\nworld, demonstrations can be collected in the real world with states extracted\nfrom the simulator. The simulator enables flexible state representations by\nrendering image inputs from any viewpoint or extracting low-level state\ninformation from objects embodied within the scene. During training, policies\ncan be directly evaluated within the simulator in an offline and highly\nparallelizable manner. Finally, during deployment, policies are run within the\nsimulator where the real robot directly tracks the simulated robot's joints,\neffectively decoupling policy execution from real hardware and mitigating\ntraditional domain-transfer challenges. We validate real-is-sim on the PushT\nmanipulation task, demonstrating strong correlation between success rates\nobtained in the simulator and real-world evaluations. Videos of our system can\nbe found at https://realissim.rai-inst.com.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03597v1",
    "published_date": "2025-04-04 17:05:56 UTC",
    "updated_date": "2025-04-04 17:05:56 UTC"
  },
  {
    "arxiv_id": "2504.03561v1",
    "title": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement",
    "authors": [
      "Runnan Fang",
      "Xiaobin Wang",
      "Yuan Liang",
      "Shuofei Qiao",
      "Jialong Wu",
      "Zekun Xi",
      "Ningyu Zhang",
      "Yong Jiang",
      "Pengjun Xie",
      "Fei Huang",
      "Huajun Chen"
    ],
    "abstract": "In the interaction between agents and their environments, agents expand their\ncapabilities by planning and executing actions. However, LLM-based agents face\nsubstantial challenges when deployed in novel environments or required to\nnavigate unconventional action spaces. To empower agents to autonomously\nexplore environments, optimize workflows, and enhance their understanding of\nactions, we propose SynWorld, a framework that allows agents to synthesize\npossible scenarios with multi-step action invocation within the action space\nand perform Monte Carlo Tree Search (MCTS) exploration to effectively refine\ntheir action knowledge in the current environment. Our experiments demonstrate\nthat SynWorld is an effective and general approach to learning action knowledge\nin new environments. Code is available at https://github.com/zjunlp/SynWorld.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2504.03561v1",
    "published_date": "2025-04-04 16:10:57 UTC",
    "updated_date": "2025-04-04 16:10:57 UTC"
  },
  {
    "arxiv_id": "2504.03553v1",
    "title": "Agentic Knowledgeable Self-awareness",
    "authors": [
      "Shuofei Qiao",
      "Zhisong Qiu",
      "Baochang Ren",
      "Xiaobin Wang",
      "Xiangyuan Ru",
      "Ningyu Zhang",
      "Xiang Chen",
      "Yong Jiang",
      "Pengjun Xie",
      "Fei Huang",
      "Huajun Chen"
    ],
    "abstract": "Large Language Models (LLMs) have achieved considerable performance across\nvarious agentic planning tasks. However, traditional agent planning approaches\nadopt a \"flood irrigation\" methodology that indiscriminately injects gold\ntrajectories, external feedback, and domain knowledge into agent models. This\npractice overlooks the fundamental human cognitive principle of situational\nself-awareness during decision-making-the ability to dynamically assess\nsituational demands and strategically employ resources during decision-making.\nWe propose agentic knowledgeable self-awareness to address this gap, a novel\nparadigm enabling LLM-based agents to autonomously regulate knowledge\nutilization. Specifically, we propose KnowSelf, a data-centric approach that\napplies agents with knowledgeable self-awareness like humans. Concretely, we\ndevise a heuristic situation judgement criterion to mark special tokens on the\nagent's self-explored trajectories for collecting training data. Through a\ntwo-stage training process, the agent model can switch between different\nsituations by generating specific special tokens, achieving optimal planning\neffects with minimal costs. Our experiments demonstrate that KnowSelf can\noutperform various strong baselines on different tasks and models with minimal\nuse of external knowledge. Code is available at\nhttps://github.com/zjunlp/KnowSelf.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2504.03553v1",
    "published_date": "2025-04-04 16:03:38 UTC",
    "updated_date": "2025-04-04 16:03:38 UTC"
  },
  {
    "arxiv_id": "2504.03818v1",
    "title": "Exploring Various Sequential Learning Methods for Deformation History Modeling",
    "authors": [
      "Muhammed Adil Yatkin",
      "Mihkel Korgesaar",
      "Jani Romanoff",
      "Umit Islak",
      "Hasan Kurban"
    ],
    "abstract": "Current neural network (NN) models can learn patterns from data points with\nhistorical dependence. Specifically, in natural language processing (NLP),\nsequential learning has transitioned from recurrence-based architectures to\ntransformer-based architectures. However, it is unknown which NN architectures\nwill perform the best on datasets containing deformation history due to\nmechanical loading. Thus, this study ascertains the appropriateness of\n1D-convolutional, recurrent, and transformer-based architectures for predicting\ndeformation localization based on the earlier states in the form of deformation\nhistory. Following this investigation, the crucial incompatibility issues\nbetween the mathematical computation of the prediction process in the\nbest-performing NN architectures and the actual values derived from the natural\nphysical properties of the deformation paths are examined in detail.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "Engineering Applications of Neural Networks",
    "pdf_url": "http://arxiv.org/pdf/2504.03818v1",
    "published_date": "2025-04-04 15:52:24 UTC",
    "updated_date": "2025-04-04 15:52:24 UTC"
  },
  {
    "arxiv_id": "2504.03546v1",
    "title": "MultiMed-ST: Large-scale Many-to-many Multilingual Medical Speech Translation",
    "authors": [
      "Khai Le-Duc",
      "Tuyen Tran",
      "Bach Phan Tat",
      "Nguyen Kim Hai Bui",
      "Quan Dang",
      "Hung-Phong Tran",
      "Thanh-Thuy Nguyen",
      "Ly Nguyen",
      "Tuan-Minh Phan",
      "Thi Thu Phuong Tran",
      "Chris Ngo",
      "Nguyen X. Khanh",
      "Thanh Nguyen-Tang"
    ],
    "abstract": "Multilingual speech translation (ST) in the medical domain enhances patient\ncare by enabling efficient communication across language barriers, alleviating\nspecialized workforce shortages, and facilitating improved diagnosis and\ntreatment, particularly during pandemics. In this work, we present the first\nsystematic study on medical ST, to our best knowledge, by releasing\nMultiMed-ST, a large-scale ST dataset for the medical domain, spanning all\ntranslation directions in five languages: Vietnamese, English, German, French,\nTraditional Chinese and Simplified Chinese, together with the models. With\n290,000 samples, our dataset is the largest medical machine translation (MT)\ndataset and the largest many-to-many multilingual ST among all domains.\nSecondly, we present the most extensive analysis study in ST research to date,\nincluding: empirical baselines, bilingual-multilingual comparative study,\nend-to-end vs. cascaded comparative study, task-specific vs. multi-task\nsequence-to-sequence (seq2seq) comparative study, code-switch analysis, and\nquantitative-qualitative error analysis. All code, data, and models are\navailable online: https://github.com/leduckhai/MultiMed-ST.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint, 122 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.03546v1",
    "published_date": "2025-04-04 15:49:17 UTC",
    "updated_date": "2025-04-04 15:49:17 UTC"
  },
  {
    "arxiv_id": "2504.03531v1",
    "title": "Dense Neural Network Based Arrhythmia Classification on Low-cost and Low-compute Micro-controller",
    "authors": [
      "Md Abu Obaida Zishan",
      "H M Shihab",
      "Sabik Sadman Islam",
      "Maliha Alam Riya",
      "Gazi Mashrur Rahman",
      "Jannatun Noor"
    ],
    "abstract": "The electrocardiogram (ECG) monitoring device is an expensive albeit\nessential device for the treatment and diagnosis of cardiovascular diseases\n(CVD). The cost of this device typically ranges from $2000 to $10000. Several\nstudies have implemented ECG monitoring systems in micro-controller units (MCU)\nto reduce industrial development costs by up to 20 times. However, to match\nindustry-grade systems and display heartbeats effectively, it is essential to\ndevelop an efficient algorithm for detecting arrhythmia (irregular heartbeat).\nHence in this study, a dense neural network is developed to detect arrhythmia\non the Arduino Nano. The Nano consists of the ATMega328 microcontroller with a\n16MHz clock, 2KB of SRAM, and 32KB of program memory. Additionally, the AD8232\nSparkFun Single-Lead Heart Rate Monitor is used as the ECG sensor. The\nimplemented neural network model consists of two layers (excluding the input)\nwith 10 and four neurons respectively with sigmoid activation function.\nHowever, four approaches are explored to choose the appropriate activation\nfunctions. The model has a size of 1.267 KB, achieves an F1 score\n(macro-average) of 78.3\\% for classifying four types of arrhythmia, an accuracy\nrate of 96.38%, and requires 0.001314 MOps of floating-point operations\n(FLOPs).",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.1; I.2.6; C.3"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03531v1",
    "published_date": "2025-04-04 15:30:02 UTC",
    "updated_date": "2025-04-04 15:30:02 UTC"
  },
  {
    "arxiv_id": "2504.05333v1",
    "title": "When is using AI the rational choice? The importance of counterfactuals in AI deployment decisions",
    "authors": [
      "Paul Lehner",
      "Elinor Yeo"
    ],
    "abstract": "Decisions to deploy AI capabilities are often driven by counterfactuals - a\ncomparison of decisions made using AI to decisions that would have been made if\nthe AI were not used. Counterfactual misses, which are poor decisions that are\nattributable to using AI, may have disproportionate disutility to AI deployment\ndecision makers. Counterfactual hits, which are good decisions attributable to\nAI usage, may provide little benefit beyond the benefit of better decisions.\nThis paper explores how to include counterfactual outcomes into usage decision\nexpected utility assessments. Several properties emerge when counterfactuals\nare explicitly included. First, there are many contexts where the expected\nutility of AI usage is positive for intended beneficiaries and strongly\nnegative for stakeholders and deployment decision makers. Second, high levels\nof complementarity, where differing AI and user assessments are merged\nbeneficially, often leads to substantial disutility for stakeholders. Third,\napparently small changes in how users interact with an AI capability can\nsubstantially impact stakeholder utility. Fourth, cognitive biases such as\nexpert overconfidence and hindsight bias exacerbate the perceived frequency of\ncostly counterfactual misses. The expected utility assessment approach\npresented here is intended to help AI developers and deployment decision makers\nto navigate the subtle but substantial impact of counterfactuals so as to\nbetter ensure that beneficial AI capabilities are used.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "31 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.05333v1",
    "published_date": "2025-04-04 14:59:29 UTC",
    "updated_date": "2025-04-04 14:59:29 UTC"
  },
  {
    "arxiv_id": "2504.03494v1",
    "title": "Quantifying Robustness: A Benchmarking Framework for Deep Learning Forecasting in Cyber-Physical Systems",
    "authors": [
      "Alexander Windmann",
      "Henrik Steude",
      "Daniel Boschmann",
      "Oliver Niggemann"
    ],
    "abstract": "Cyber-Physical Systems (CPS) in domains such as manufacturing and energy\ndistribution generate complex time series data crucial for Prognostics and\nHealth Management (PHM). While Deep Learning (DL) methods have demonstrated\nstrong forecasting capabilities, their adoption in industrial CPS remains\nlimited due insufficient robustness. Existing robustness evaluations primarily\nfocus on formal verification or adversarial perturbations, inadequately\nrepresenting the complexities encountered in real-world CPS scenarios. To\naddress this, we introduce a practical robustness definition grounded in\ndistributional robustness, explicitly tailored to industrial CPS, and propose a\nsystematic framework for robustness evaluation. Our framework simulates\nrealistic disturbances, such as sensor drift, noise and irregular sampling,\nenabling thorough robustness analyses of forecasting models on real-world CPS\ndatasets. The robustness definition provides a standardized score to quantify\nand compare model performance across diverse datasets, assisting in informed\nmodel selection and architecture design. Through extensive empirical studies\nevaluating prominent DL architectures (including recurrent, convolutional,\nattention-based, modular, and structured state-space models) we demonstrate the\napplicability and effectiveness of our approach. We publicly release our\nrobustness benchmark to encourage further research and reproducibility.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03494v1",
    "published_date": "2025-04-04 14:50:48 UTC",
    "updated_date": "2025-04-04 14:50:48 UTC"
  },
  {
    "arxiv_id": "2504.03490v1",
    "title": "BUFF: Bayesian Uncertainty Guided Diffusion Probabilistic Model for Single Image Super-Resolution",
    "authors": [
      "Zihao He",
      "Shengchuan Zhang",
      "Runze Hu",
      "Yunhang Shen",
      "Yan Zhang"
    ],
    "abstract": "Super-resolution (SR) techniques are critical for enhancing image quality,\nparticularly in scenarios where high-resolution imagery is essential yet\nlimited by hardware constraints. Existing diffusion models for SR have relied\npredominantly on Gaussian models for noise generation, which often fall short\nwhen dealing with the complex and variable texture inherent in natural scenes.\nTo address these deficiencies, we introduce the Bayesian Uncertainty Guided\nDiffusion Probabilistic Model (BUFF). BUFF distinguishes itself by\nincorporating a Bayesian network to generate high-resolution uncertainty masks.\nThese masks guide the diffusion process, allowing for the adjustment of noise\nintensity in a manner that is both context-aware and adaptive. This novel\napproach not only enhances the fidelity of super-resolved images to their\noriginal high-resolution counterparts but also significantly mitigates\nartifacts and blurring in areas characterized by complex textures and fine\ndetails. The model demonstrates exceptional robustness against complex noise\npatterns and showcases superior adaptability in handling textures and edges\nwithin images. Empirical evidence, supported by visual results, illustrates the\nmodel's robustness, especially in challenging scenarios, and its effectiveness\nin addressing common SR issues such as blurring. Experimental evaluations\nconducted on the DIV2K dataset reveal that BUFF achieves a notable improvement,\nwith a +0.61 increase compared to baseline in SSIM on BSD100, surpassing\ntraditional diffusion approaches by an average additional +0.20dB PSNR gain.\nThese findings underscore the potential of Bayesian methods in enhancing\ndiffusion processes for SR, paving the way for future advancements in the\nfield.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T45",
      "I.2.10; J.0"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 5 figures, AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.03490v1",
    "published_date": "2025-04-04 14:43:45 UTC",
    "updated_date": "2025-04-04 14:43:45 UTC"
  },
  {
    "arxiv_id": "2504.03486v1",
    "title": "Structured Legal Document Generation in India: A Model-Agnostic Wrapper Approach with VidhikDastaavej",
    "authors": [
      "Shubham Kumar Nigam",
      "Balaramamahanthi Deepak Patnaik",
      "Ajay Varghese Thomas",
      "Noel Shallum",
      "Kripabandhu Ghosh",
      "Arnab Bhattacharya"
    ],
    "abstract": "Automating legal document drafting can significantly enhance efficiency,\nreduce manual effort, and streamline legal workflows. While prior research has\nexplored tasks such as judgment prediction and case summarization, the\nstructured generation of private legal documents in the Indian legal domain\nremains largely unaddressed. To bridge this gap, we introduce VidhikDastaavej,\na novel, anonymized dataset of private legal documents, and develop NyayaShilp,\na fine-tuned legal document generation model specifically adapted to Indian\nlegal texts. We propose a Model-Agnostic Wrapper (MAW), a two-step framework\nthat first generates structured section titles and then iteratively produces\ncontent while leveraging retrieval-based mechanisms to ensure coherence and\nfactual accuracy. We benchmark multiple open-source LLMs, including\ninstruction-tuned and domain-adapted versions, alongside proprietary models for\ncomparison. Our findings indicate that while direct fine-tuning on small\ndatasets does not always yield improvements, our structured wrapper\nsignificantly enhances coherence, factual adherence, and overall document\nquality while mitigating hallucinations. To ensure real-world applicability, we\ndeveloped a Human-in-the-Loop (HITL) Document Generation System, an interactive\nuser interface that enables users to specify document types, refine section\ndetails, and generate structured legal drafts. This tool allows legal\nprofessionals and researchers to generate, validate, and refine AI-generated\nlegal documents efficiently. Extensive evaluations, including expert\nassessments, confirm that our framework achieves high reliability in structured\nlegal drafting. This research establishes a scalable and adaptable foundation\nfor AI-assisted legal drafting in India, offering an effective approach to\nstructured legal document generation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03486v1",
    "published_date": "2025-04-04 14:41:50 UTC",
    "updated_date": "2025-04-04 14:41:50 UTC"
  },
  {
    "arxiv_id": "2504.03814v2",
    "title": "Recursive Training Loops in LLMs: How training data properties modulate distribution shift in generated data?",
    "authors": [
      "Grgur Kovač",
      "Jérémy Perez",
      "Rémy Portelas",
      "Peter Ford Dominey",
      "Pierre-Yves Oudeyer"
    ],
    "abstract": "Large language models (LLMs) are increasingly contributing to the creation of\ncontent on the Internet. This creates a feedback loop as subsequent generations\nof models will be trained on this generated, synthetic data. This phenomenon is\nreceiving increasing interest, in particular because previous studies have\nshown that it may lead to distribution shift - models misrepresent and forget\nthe true underlying distributions of human data they are expected to\napproximate (e.g. resulting in a drastic loss of quality). In this study, we\nstudy the impact of human data properties on distribution shift dynamics in\niterated training loops. We first confirm that the distribution shift dynamics\ngreatly vary depending on the human data by comparing four datasets (two based\non Twitter and two on Reddit). We then test whether data quality may influence\nthe rate of this shift. We find that it does on the twitter, but not on the\nReddit datasets. We then focus on a Reddit dataset and conduct a more\nexhaustive evaluation of a large set of dataset properties. This experiment\nassociated lexical diversity with larger, and semantic diversity with smaller\ndetrimental shifts, suggesting that incorporating text with high lexical (but\nlimited semantic) diversity could exacerbate the degradation of generated text.\nWe then focus on the evolution of political bias, and find that the type of\nshift observed (bias reduction, amplification or inversion) depends on the\npolitical lean of the human (true) distribution. Overall, our work extends the\nexisting literature on the consequences of recursive fine-tuning by showing\nthat this phenomenon is highly dependent on features of the human data on which\ntraining occurs. This suggests that different parts of internet (e.g. GitHub,\nReddit) may undergo different types of shift depending on their properties.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03814v2",
    "published_date": "2025-04-04 14:41:41 UTC",
    "updated_date": "2025-04-08 08:45:26 UTC"
  },
  {
    "arxiv_id": "2504.03469v1",
    "title": "Physics-informed 4D X-ray image reconstruction from ultra-sparse spatiotemporal data",
    "authors": [
      "Zisheng Yao",
      "Yuhe Zhang",
      "Zhe Hu",
      "Robert Klöfkorn",
      "Tobias Ritschel",
      "Pablo Villanueva-Perez"
    ],
    "abstract": "The unprecedented X-ray flux density provided by modern X-ray sources offers\nnew spatiotemporal possibilities for X-ray imaging of fast dynamic processes.\nApproaches to exploit such possibilities often result in either i) a limited\nnumber of projections or spatial information due to limited scanning speed, as\nin time-resolved tomography, or ii) a limited number of time points, as in\nstroboscopic imaging, making the reconstruction problem ill-posed and unlikely\nto be solved by classical reconstruction approaches. 4D reconstruction from\nsuch data requires sample priors, which can be included via deep learning (DL).\nState-of-the-art 4D reconstruction methods for X-ray imaging combine the power\nof AI and the physics of X-ray propagation to tackle the challenge of sparse\nviews. However, most approaches do not constrain the physics of the studied\nprocess, i.e., a full physical model. Here we present 4D physics-informed\noptimized neural implicit X-ray imaging (4D-PIONIX), a novel physics-informed\n4D X-ray image reconstruction method combining the full physical model and a\nstate-of-the-art DL-based reconstruction method for 4D X-ray imaging from\nsparse views. We demonstrate and evaluate the potential of our approach by\nretrieving 4D information from ultra-sparse spatiotemporal acquisitions of\nsimulated binary droplet collisions, a relevant fluid dynamic process. We\nenvision that this work will open new spatiotemporal possibilities for various\n4D X-ray imaging modalities, such as time-resolved X-ray tomography and more\nnovel sparse acquisition approaches like X-ray multi-projection imaging, which\nwill pave the way for investigations of various rapid 4D dynamics, such as\nfluid dynamics and composite testing.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "physics.data-an"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03469v1",
    "published_date": "2025-04-04 14:18:51 UTC",
    "updated_date": "2025-04-04 14:18:51 UTC"
  },
  {
    "arxiv_id": "2504.05331v2",
    "title": "Not someone, but something: Rethinking trust in the age of medical AI",
    "authors": [
      "Jan Beger"
    ],
    "abstract": "As artificial intelligence (AI) becomes embedded in healthcare, trust in\nmedical decision-making is changing fast. This opinion paper argues that trust\nin AI isn't a simple transfer from humans to machines - it's a dynamic,\nevolving relationship that must be built and maintained. Rather than debating\nwhether AI belongs in medicine, this paper asks: what kind of trust must AI\nearn, and how? Drawing from philosophy, bioethics, and system design, it\nexplores the key differences between human trust and machine reliability -\nemphasizing transparency, accountability, and alignment with the values of good\ncare. It argues that trust in AI shouldn't be built on mimicking empathy or\nintuition, but on thoughtful design, responsible deployment, and clear moral\nresponsibility. The goal is a balanced view - one that avoids blind optimism\nand reflexive fear. Trust in AI must be treated not as a given, but as\nsomething to be earned over time.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.05331v2",
    "published_date": "2025-04-04 14:09:20 UTC",
    "updated_date": "2025-04-09 18:46:53 UTC"
  },
  {
    "arxiv_id": "2504.03454v1",
    "title": "SpectR: Dynamically Composing LM Experts with Spectral Routing",
    "authors": [
      "William Fleshman",
      "Benjamin Van Durme"
    ],
    "abstract": "Training large, general-purpose language models poses significant challenges.\nThe growing availability of specialized expert models, fine-tuned from\npretrained models for specific tasks or domains, offers a promising\nalternative. Leveraging the potential of these existing expert models in\nreal-world applications requires effective methods to select or merge the\nmodels best suited for a given task. This paper introduces SPECTR, an approach\nfor dynamically composing expert models at each time step during inference.\nNotably, our method requires no additional training and enables flexible,\ntoken- and layer-wise model combinations. Our experimental results demonstrate\nthat SPECTR improves routing accuracy over alternative training-free methods,\nincreasing task performance across expert domains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03454v1",
    "published_date": "2025-04-04 13:58:44 UTC",
    "updated_date": "2025-04-04 13:58:44 UTC"
  },
  {
    "arxiv_id": "2504.03424v1",
    "title": "The AI Cosmologist I: An Agentic System for Automated Data Analysis",
    "authors": [
      "Adam Moss"
    ],
    "abstract": "We present the AI Cosmologist, an agentic system designed to automate\ncosmological/astronomical data analysis and machine learning research\nworkflows. This implements a complete pipeline from idea generation to\nexperimental evaluation and research dissemination, mimicking the scientific\nprocess typically performed by human researchers. The system employs\nspecialized agents for planning, coding, execution, analysis, and synthesis\nthat work together to develop novel approaches. Unlike traditional auto\nmachine-learning systems, the AI Cosmologist generates diverse implementation\nstrategies, writes complete code, handles execution errors, analyzes results,\nand synthesizes new approaches based on experimental outcomes. We demonstrate\nthe AI Cosmologist capabilities across several machine learning tasks, showing\nhow it can successfully explore solution spaces, iterate based on experimental\nresults, and combine successful elements from different approaches. Our results\nindicate that agentic systems can automate portions of the research process,\npotentially accelerating scientific discovery. The code and experimental data\nused in this paper are available on GitHub at\nhttps://github.com/adammoss/aicosmologist. Example papers included in the\nappendix demonstrate the system's capability to autonomously produce complete\nscientific publications, starting from only the dataset and task description",
    "categories": [
      "astro-ph.IM",
      "astro-ph.CO",
      "astro-ph.GA",
      "cs.AI",
      "physics.data-an"
    ],
    "primary_category": "astro-ph.IM",
    "comment": "45 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.03424v1",
    "published_date": "2025-04-04 13:12:08 UTC",
    "updated_date": "2025-04-04 13:12:08 UTC"
  },
  {
    "arxiv_id": "2504.03420v1",
    "title": "Autonomous state-space segmentation for Deep-RL sparse reward scenarios",
    "authors": [
      "Gianluca Maselli",
      "Vieri Giuliano Santucci"
    ],
    "abstract": "Dealing with environments with sparse rewards has always been crucial for\nsystems developed to operate in autonomous open-ended learning settings.\nIntrinsic Motivations could be an effective way to help Deep Reinforcement\nLearning algorithms learn in such scenarios. In fact, intrinsic reward signals,\nsuch as novelty or curiosity, are generally adopted to improve exploration when\nextrinsic rewards are delayed or absent. Building on previous works, we tackle\nthe problem of learning policies in the presence of sparse rewards by proposing\na two-level architecture that alternates an ''intrinsically driven'' phase of\nexploration and autonomous sub-goal generation, to a phase of sparse reward,\ngoal-directed policy learning. The idea is to build several small networks,\neach one specialized on a particular sub-path, and use them as starting points\nfor future exploration without the need to further explore from scratch\npreviously learnt paths. Two versions of the system have been trained and\ntested in the Gym SuperMarioBros environment without considering any additional\nextrinsic reward. The results show the validity of our approach and the\nimportance of autonomously segment the environment to generate an efficient\npath towards the final goal.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03420v1",
    "published_date": "2025-04-04 13:06:23 UTC",
    "updated_date": "2025-04-04 13:06:23 UTC"
  },
  {
    "arxiv_id": "2504.03810v1",
    "title": "Hierarchically Encapsulated Representation for Protocol Design in Self-Driving Labs",
    "authors": [
      "Yu-Zhe Shi",
      "Mingchen Liu",
      "Fanxu Meng",
      "Qiao Xu",
      "Zhangqian Bi",
      "Kun He",
      "Lecheng Ruan",
      "Qining Wang"
    ],
    "abstract": "Self-driving laboratories have begun to replace human experimenters in\nperforming single experimental skills or predetermined experimental protocols.\nHowever, as the pace of idea iteration in scientific research has been\nintensified by Artificial Intelligence, the demand for rapid design of new\nprotocols for new discoveries become evident. Efforts to automate protocol\ndesign have been initiated, but the capabilities of knowledge-based machine\ndesigners, such as Large Language Models, have not been fully elicited,\nprobably for the absence of a systematic representation of experimental\nknowledge, as opposed to isolated, flatten pieces of information. To tackle\nthis issue, we propose a multi-faceted, multi-scale representation, where\ninstance actions, generalized operations, and product flow models are\nhierarchically encapsulated using Domain-Specific Languages. We further develop\na data-driven algorithm based on non-parametric modeling that autonomously\ncustomizes these representations for specific domains. The proposed\nrepresentation is equipped with various machine designers to manage protocol\ndesign tasks, including planning, modification, and adjustment. The results\ndemonstrate that the proposed method could effectively complement Large\nLanguage Models in the protocol design process, serving as an auxiliary module\nin the realm of machine-assisted scientific exploration.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "In International Conference on Learning Representations (ICLR'25)",
    "pdf_url": "http://arxiv.org/pdf/2504.03810v1",
    "published_date": "2025-04-04 12:05:15 UTC",
    "updated_date": "2025-04-04 12:05:15 UTC"
  },
  {
    "arxiv_id": "2504.03380v1",
    "title": "Online Difficulty Filtering for Reasoning Oriented Reinforcement Learning",
    "authors": [
      "Sanghwan Bae",
      "Jiwoo Hong",
      "Min Young Lee",
      "Hanbyul Kim",
      "JeongYeon Nam",
      "Donghyun Kwak"
    ],
    "abstract": "Reasoning-Oriented Reinforcement Learning (RORL) enhances the reasoning\nability of Large Language Models (LLMs). However, due to the sparsity of\nrewards in RORL, effective training is highly dependent on the selection of\nproblems of appropriate difficulty. Although curriculum learning attempts to\naddress this by adjusting difficulty, it often relies on static schedules, and\neven recent online filtering methods lack theoretical grounding and a\nsystematic understanding of their effectiveness. In this work, we theoretically\nand empirically show that curating the batch with the problems that the\ntraining model achieves intermediate accuracy on the fly can maximize the\neffectiveness of RORL training, namely balanced online difficulty filtering. We\nfirst derive that the lower bound of the KL divergence between the initial and\nthe optimal policy can be expressed with the variance of the sampled accuracy.\nBuilding on those insights, we show that balanced filtering can maximize the\nlower bound, leading to better performance. Experimental results across five\nchallenging math reasoning benchmarks show that balanced online filtering\nyields an additional 10% in AIME and 4% improvements in average over plain\nGRPO. Moreover, further analysis shows the gains in sample efficiency and\ntraining time efficiency, exceeding the maximum reward of plain GRPO within 60%\ntraining time and the volume of the training set.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03380v1",
    "published_date": "2025-04-04 11:52:05 UTC",
    "updated_date": "2025-04-04 11:52:05 UTC"
  },
  {
    "arxiv_id": "2504.03809v2",
    "title": "Drawing a Map of Elections",
    "authors": [
      "Stanisław Szufa",
      "Niclas Boehmer",
      "Robert Bredereck",
      "Piotr Faliszewski",
      "Rolf Niedermeier",
      "Piotr Skowron",
      "Arkadii Slinko",
      "Nimrod Talmon"
    ],
    "abstract": "Our main contribution is the introduction of the map of elections framework.\nA map of elections consists of three main elements: (1) a dataset of elections\n(i.e., collections of ordinal votes over given sets of candidates), (2) a way\nof measuring similarities between these elections, and (3) a representation of\nthe elections in the 2D Euclidean space as points, so that the more similar two\nelections are, the closer are their points. In our maps, we mostly focus on\ndatasets of synthetic elections, but we also show an example of a map over\nreal-life ones. To measure similarities, we would have preferred to use, e.g.,\nthe isomorphic swap distance, but this is infeasible due to its high\ncomputational complexity. Hence, we propose polynomial-time computable\npositionwise distance and use it instead. Regarding the representations in 2D\nEuclidean space, we mostly use the Kamada-Kawai algorithm, but we also show two\nalternatives. We develop the necessary theoretical results to form our maps and\nargue experimentally that they are accurate and credible. Further, we show how\ncoloring the elections in a map according to various criteria helps in\nanalyzing results of a number of experiments. In particular, we show colorings\naccording to the scores of winning candidates or committees, running times of\nILP-based winner determination algorithms, and approximation ratios achieved by\nparticular algorithms.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "cs.MA",
    "comment": "Journal article merging results from arxiv:2105.07815,\n  arXiv:2407.11889 and Szufa et al., \"Drawing a Map of Elections in the Space\n  of Statistical Cultures\", AAMAS '20",
    "pdf_url": "http://arxiv.org/pdf/2504.03809v2",
    "published_date": "2025-04-04 11:44:56 UTC",
    "updated_date": "2025-04-08 10:52:54 UTC"
  },
  {
    "arxiv_id": "2504.03360v1",
    "title": "Sustainable LLM Inference for Edge AI: Evaluating Quantized LLMs for Energy Efficiency, Output Accuracy, and Inference Latency",
    "authors": [
      "Erik Johannes Husom",
      "Arda Goknil",
      "Merve Astekin",
      "Lwin Khin Shar",
      "Andre Kåsen",
      "Sagar Sen",
      "Benedikt Andreas Mithassel",
      "Ahmet Soylu"
    ],
    "abstract": "Deploying Large Language Models (LLMs) on edge devices presents significant\nchallenges due to computational constraints, memory limitations, inference\nspeed, and energy consumption. Model quantization has emerged as a key\ntechnique to enable efficient LLM inference by reducing model size and\ncomputational overhead. In this study, we conduct a comprehensive analysis of\n28 quantized LLMs from the Ollama library, which applies by default\nPost-Training Quantization (PTQ) and weight-only quantization techniques,\ndeployed on an edge device (Raspberry Pi 4 with 4GB RAM). We evaluate energy\nefficiency, inference performance, and output accuracy across multiple\nquantization levels and task types. Models are benchmarked on five standardized\ndatasets (CommonsenseQA, BIG-Bench Hard, TruthfulQA, GSM8K, and HumanEval), and\nwe employ a high-resolution, hardware-based energy measurement tool to capture\nreal-world power consumption. Our findings reveal the trade-offs between energy\nefficiency, inference speed, and accuracy in different quantization settings,\nhighlighting configurations that optimize LLM deployment for\nresource-constrained environments. By integrating hardware-level energy\nprofiling with LLM benchmarking, this study provides actionable insights for\nsustainable AI, bridging a critical gap in existing research on energy-aware\nLLM deployment.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "30 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.03360v1",
    "published_date": "2025-04-04 11:29:30 UTC",
    "updated_date": "2025-04-04 11:29:30 UTC"
  },
  {
    "arxiv_id": "2504.03353v1",
    "title": "Decentralized Collective World Model for Emergent Communication and Coordination",
    "authors": [
      "Kentaro Nomura",
      "Tatsuya Aoki",
      "Tadahiro Taniguchi",
      "Takato Horii"
    ],
    "abstract": "We propose a fully decentralized multi-agent world model that enables both\nsymbol emergence for communication and coordinated behavior through temporal\nextension of collective predictive coding. Unlike previous research that\nfocuses on either communication or coordination separately, our approach\nachieves both simultaneously. Our method integrates world models with\ncommunication channels, enabling agents to predict environmental dynamics,\nestimate states from partial observations, and share critical information\nthrough bidirectional message exchange with contrastive learning for message\nalignment. Using a two-agent trajectory drawing task, we demonstrate that our\ncommunication-based approach outperforms non-communicative models when agents\nhave divergent perceptual capabilities, achieving the second-best coordination\nafter centralized models. Importantly, our distributed approach with\nconstraints preventing direct access to other agents' internal states\nfacilitates the emergence of more meaningful symbol systems that accurately\nreflect environmental states. These findings demonstrate the effectiveness of\ndecentralized communication for supporting coordination while developing shared\nrepresentations of the environment.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03353v1",
    "published_date": "2025-04-04 11:17:52 UTC",
    "updated_date": "2025-04-04 11:17:52 UTC"
  },
  {
    "arxiv_id": "2504.03343v1",
    "title": "Talk2X -- An Open-Source Toolkit Facilitating Deployment of LLM-Powered Chatbots on the Web",
    "authors": [
      "Lars Krupp",
      "Daniel Geißler",
      "Peter Hevesi",
      "Marco Hirsch",
      "Paul Lukowicz",
      "Jakob Karolus"
    ],
    "abstract": "Integrated into websites, LLM-powered chatbots offer alternative means of\nnavigation and information retrieval, leading to a shift in how users access\ninformation on the web. Yet, predominantly closed-sourced solutions limit\nproliferation among web hosts and suffer from a lack of transparency with\nregard to implementation details and energy efficiency. In this work, we\npropose our openly available agent Talk2X leveraging an adapted\nretrieval-augmented generation approach (RAG) combined with an automatically\ngenerated vector database, benefiting energy efficiency. Talk2X's architecture\nis generalizable to arbitrary websites offering developers a ready to use tool\nfor integration. Using a mixed-methods approach, we evaluated Talk2X's\nusability by tasking users to acquire specific assets from an open science\nrepository. Talk2X significantly improved task completion time, correctness,\nand user experience supporting users in quickly pinpointing specific\ninformation as compared to standard user-website interaction. Our findings\ncontribute technical advancements to an ongoing paradigm shift of how we access\ninformation on the web.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03343v1",
    "published_date": "2025-04-04 10:58:57 UTC",
    "updated_date": "2025-04-04 10:58:57 UTC"
  },
  {
    "arxiv_id": "2504.03342v1",
    "title": "EOOD: Entropy-based Out-of-distribution Detection",
    "authors": [
      "Guide Yang",
      "Chao Hou",
      "Weilong Peng",
      "Xiang Fang",
      "Yongwei Nie",
      "Peican Zhu",
      "Keke Tang"
    ],
    "abstract": "Deep neural networks (DNNs) often exhibit overconfidence when encountering\nout-of-distribution (OOD) samples, posing significant challenges for\ndeployment. Since DNNs are trained on in-distribution (ID) datasets, the\ninformation flow of ID samples through DNNs inevitably differs from that of OOD\nsamples. In this paper, we propose an Entropy-based Out-Of-distribution\nDetection (EOOD) framework. EOOD first identifies specific block where the\ninformation flow differences between ID and OOD samples are more pronounced,\nusing both ID and pseudo-OOD samples. It then calculates the conditional\nentropy on the selected block as the OOD confidence score. Comprehensive\nexperiments conducted across various ID and OOD settings demonstrate the\neffectiveness of EOOD in OOD detection and its superiority over\nstate-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "IJCNN 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.03342v1",
    "published_date": "2025-04-04 10:57:03 UTC",
    "updated_date": "2025-04-04 10:57:03 UTC"
  },
  {
    "arxiv_id": "2504.06292v1",
    "title": "Temporal-contextual Event Learning for Pedestrian Crossing Intent Prediction",
    "authors": [
      "Hongbin Liang",
      "Hezhe Qiao",
      "Wei Huang",
      "Qizhou Wang",
      "Mingsheng Shang",
      "Lin Chen"
    ],
    "abstract": "Ensuring the safety of vulnerable road users through accurate prediction of\npedestrian crossing intention (PCI) plays a crucial role in the context of\nautonomous and assisted driving. Analyzing the set of observation video frames\nin ego-view has been widely used in most PCI prediction methods to forecast the\ncross intent. However, they struggle to capture the critical events related to\npedestrian behaviour along the temporal dimension due to the high redundancy of\nthe video frames, which results in the sub-optimal performance of PCI\nprediction. Our research addresses the challenge by introducing a novel\napproach called \\underline{T}emporal-\\underline{c}ontextual Event\n\\underline{L}earning (TCL). The TCL is composed of the Temporal Merging Module\n(TMM), which aims to manage the redundancy by clustering the observed video\nframes into multiple key temporal events. Then, the Contextual Attention Block\n(CAB) is employed to adaptively aggregate multiple event features along with\nvisual and non-visual data. By synthesizing the temporal feature extraction and\ncontextual attention on the key information across the critical events, TCL can\nlearn expressive representation for the PCI prediction. Extensive experiments\nare carried out on three widely adopted datasets, including PIE, JAAD-beh, and\nJAAD-all. The results show that TCL substantially surpasses the\nstate-of-the-art methods. Our code can be accessed at\nhttps://github.com/dadaguailhb/TCL.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted in ICONIP2024",
    "pdf_url": "http://arxiv.org/pdf/2504.06292v1",
    "published_date": "2025-04-04 10:44:24 UTC",
    "updated_date": "2025-04-04 10:44:24 UTC"
  },
  {
    "arxiv_id": "2504.03329v1",
    "title": "Mind the Prompt: Prompting Strategies in Audio Generations for Improving Sound Classification",
    "authors": [
      "Francesca Ronchini",
      "Ho-Hsiang Wu",
      "Wei-Cheng Lin",
      "Fabio Antonacci"
    ],
    "abstract": "This paper investigates the design of effective prompt strategies for\ngenerating realistic datasets using Text-To-Audio (TTA) models. We also analyze\ndifferent techniques for efficiently combining these datasets to enhance their\nutility in sound classification tasks. By evaluating two sound classification\ndatasets with two TTA models, we apply a range of prompt strategies. Our\nfindings reveal that task-specific prompt strategies significantly outperform\nbasic prompt approaches in data generation. Furthermore, merging datasets\ngenerated using different TTA models proves to enhance classification\nperformance more effectively than merely increasing the training dataset size.\nOverall, our results underscore the advantages of these methods as effective\ndata augmentation techniques using synthetic data.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD",
      "eess.SP"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted at Generative Data Augmentation for Real-World Signal\n  Processing Applications Workshop",
    "pdf_url": "http://arxiv.org/pdf/2504.03329v1",
    "published_date": "2025-04-04 10:14:11 UTC",
    "updated_date": "2025-04-04 10:14:11 UTC"
  },
  {
    "arxiv_id": "2504.03328v1",
    "title": "Policy Optimization Algorithms in a Unified Framework",
    "authors": [
      "Shuang Wu"
    ],
    "abstract": "Policy optimization algorithms are crucial in many fields but challenging to\ngrasp and implement, often due to complex calculations related to Markov\ndecision processes and varying use of discount and average reward setups. This\npaper presents a unified framework that applies generalized ergodicity theory\nand perturbation analysis to clarify and enhance the application of these\nalgorithms. Generalized ergodicity theory sheds light on the steady-state\nbehavior of stochastic processes, aiding understanding of both discounted and\naverage rewards. Perturbation analysis provides in-depth insights into the\nfundamental principles of policy optimization algorithms. We use this framework\nto identify common implementation errors and demonstrate the correct\napproaches. Through a case study on Linear Quadratic Regulator problems, we\nillustrate how slight variations in algorithm design affect implementation\noutcomes. We aim to make policy optimization algorithms more accessible and\nreduce their misuse in practice.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03328v1",
    "published_date": "2025-04-04 10:14:01 UTC",
    "updated_date": "2025-04-04 10:14:01 UTC"
  },
  {
    "arxiv_id": "2504.03302v2",
    "title": "Noise Augmented Fine Tuning for Mitigating Hallucinations in Large Language Models",
    "authors": [
      "Afshin Khadangi",
      "Amir Sartipi",
      "Igor Tchappi",
      "Ramin Bahmani"
    ],
    "abstract": "Large language models (LLMs) often produce inaccurate or misleading\ncontent-hallucinations. To address this challenge, we introduce Noise-Augmented\nFine-Tuning (NoiseFiT), a novel framework that leverages adaptive noise\ninjection based on the signal-to-noise ratio (SNR) to enhance model robustness.\nIn particular, NoiseFiT selectively perturbs layers identified as either\nhigh-SNR (more robust) or low-SNR (potentially under-regularized) using a\ndynamically scaled Gaussian noise. We further propose a hybrid loss that\ncombines standard cross-entropy, soft cross-entropy, and consistency\nregularization to ensure stable and accurate outputs under noisy training\nconditions. Our theoretical analysis shows that adaptive noise injection is\nboth unbiased and variance-preserving, providing strong guarantees for\nconvergence in expectation. Empirical results on multiple test and benchmark\ndatasets demonstrate that NoiseFiT significantly reduces hallucination rates,\noften improving or matching baseline performance in key tasks. These findings\nhighlight the promise of noise-driven strategies for achieving robust,\ntrustworthy language modeling without incurring prohibitive computational\noverhead. Given the comprehensive and detailed nature of our experiments, we\nhave publicly released the fine-tuning logs, benchmark evaluation artifacts,\nand source code online at W&B, Hugging Face, and GitHub, respectively, to\nfoster further research, accessibility and reproducibility.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03302v2",
    "published_date": "2025-04-04 09:27:19 UTC",
    "updated_date": "2025-05-03 07:26:02 UTC"
  },
  {
    "arxiv_id": "2504.03295v1",
    "title": "Stance-Driven Multimodal Controlled Statement Generation: New Dataset and Task",
    "authors": [
      "Bingqian Wang",
      "Quan Fang",
      "Jiachen Sun",
      "Xiaoxiao Ma"
    ],
    "abstract": "Formulating statements that support diverse or controversial stances on\nspecific topics is vital for platforms that enable user expression, reshape\npolitical discourse, and drive social critique and information dissemination.\nWith the rise of Large Language Models (LLMs), controllable text generation\ntowards specific stances has become a promising research area with applications\nin shaping public opinion and commercial marketing. However, current datasets\noften focus solely on pure texts, lacking multimodal content and effective\ncontext, particularly in the context of stance detection. In this paper, we\nformally define and study the new problem of stance-driven controllable content\ngeneration for tweets with text and images, where given a multimodal post (text\nand image/video), a model generates a stance-controlled response. To this end,\nwe create the Multimodal Stance Generation Dataset (StanceGen2024), the first\nresource explicitly designed for multimodal stance-controllable text generation\nin political discourse. It includes posts and user comments from the 2024 U.S.\npresidential election, featuring text, images, videos, and stance annotations\nto explore how multimodal political content shapes stance expression.\nFurthermore, we propose a Stance-Driven Multimodal Generation (SDMG) framework\nthat integrates weighted fusion of multimodal features and stance guidance to\nimprove semantic consistency and stance control. We release the dataset and\ncode (https://anonymous.4open.science/r/StanceGen-BE9D) for public use and\nfurther research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03295v1",
    "published_date": "2025-04-04 09:20:19 UTC",
    "updated_date": "2025-04-04 09:20:19 UTC"
  },
  {
    "arxiv_id": "2504.03287v1",
    "title": "Towards Effective EU E-Participation: The Development of AskThePublic",
    "authors": [
      "Kilian Sprenkamp",
      "Nils Messerschmidt",
      "Amir Sartipi",
      "Igor Tchappi",
      "Xiaohui Wu",
      "Liudmila Zavolokina",
      "Gilbert Fridgen"
    ],
    "abstract": "E-participation platforms can be an important asset for governments in\nincreasing trust and fostering democratic societies. By engaging\nnon-governmental and private institutions, domain experts, and even the general\npublic, policymakers can make informed and inclusive decisions. Drawing on the\nMedia Richness Theory and applying the Design Science Research method, we\nexplore how a chatbot can be designed to improve the effectiveness of the\npolicy-making process of existing citizen involvement platforms. Leveraging the\nHave Your Say platform, which solicits feedback on European Commission\ninitiatives and regulations, a Large Language Model based chatbot, called\nAskThePublic is created, providing policymakers, journalists, researchers, and\ninterested citizens with a convenient channel to explore and engage with public\ninput. By conducting 11 semistructured interviews, the results show that the\nparticipants value the interactive and structured responses as well as enhanced\nlanguage capabilities, thus increasing their likelihood of engaging with\nAskThePublic over the existing platform. An outlook for future iterations is\nprovided and discussed with regard to the perspectives of the different\nstakeholders.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03287v1",
    "published_date": "2025-04-04 09:15:06 UTC",
    "updated_date": "2025-04-04 09:15:06 UTC"
  },
  {
    "arxiv_id": "2504.03278v2",
    "title": "JanusDDG: A Thermodynamics-Compliant Model for Sequence-Based Protein Stability via Two-Fronts Multi-Head Attention",
    "authors": [
      "Guido Barducci",
      "Ivan Rossi",
      "Francesco Codicè",
      "Cesare Rollo",
      "Valeria Repetto",
      "Corrado Pancotti",
      "Virginia Iannibelli",
      "Tiziana Sanavia",
      "Piero Fariselli"
    ],
    "abstract": "Understanding how residue variations affect protein stability is crucial for\ndesigning functional proteins and deciphering the molecular mechanisms\nunderlying disease-related mutations. Recent advances in protein language\nmodels (PLMs) have revolutionized computational protein analysis, enabling,\namong other things, more accurate predictions of mutational effects. In this\nwork, we introduce JanusDDG, a deep learning framework that leverages\nPLM-derived embeddings and a bidirectional cross-attention transformer\narchitecture to predict $\\Delta \\Delta G$ of single and multiple-residue\nmutations while simultaneously being constrained to respect fundamental\nthermodynamic properties, such as antisymmetry and transitivity. Unlike\nconventional self-attention, JanusDDG computes queries (Q) and values (V) as\nthe difference between wild-type and mutant embeddings, while keys (K)\nalternate between the two. This cross-interleaved attention mechanism enables\nthe model to capture mutation-induced perturbations while preserving essential\ncontextual information. Experimental results show that JanusDDG achieves\nstate-of-the-art performance in predicting $\\Delta \\Delta G$ from sequence\nalone, matching or exceeding the accuracy of structure-based methods for both\nsingle and multiple mutations. Code\nAvailability:https://github.com/compbiomed-unito/JanusDDG",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG",
      "physics.comp-ph"
    ],
    "primary_category": "q-bio.QM",
    "comment": "20 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.03278v2",
    "published_date": "2025-04-04 09:02:32 UTC",
    "updated_date": "2025-04-14 18:11:58 UTC"
  },
  {
    "arxiv_id": "2505.04628v1",
    "title": "How Social is It? A Benchmark for LLMs' Capabilities in Multi-user Multi-turn Social Agent Tasks",
    "authors": [
      "Yusen Wu",
      "Junwu Xiong",
      "Xiaotie Deng"
    ],
    "abstract": "Expanding the application of large language models (LLMs) to societal life,\ninstead of primary function only as auxiliary assistants to communicate with\nonly one person at a time, necessitates LLMs' capabilities to independently\nplay roles in multi-user, multi-turn social agent tasks within complex social\nsettings. However, currently the capability has not been systematically\nmeasured with available benchmarks. To address this gap, we first introduce an\nagent task leveling framework grounded in sociological principles.\nConcurrently, we propose a novel benchmark, How Social Is It (we call it HSII\nbelow), designed to assess LLM's social capabilities in comprehensive social\nagents tasks and benchmark representative models. HSII comprises four stages:\nformat parsing, target selection, target switching conversation, and stable\nconversation, which collectively evaluate the communication and task completion\ncapabilities of LLMs within realistic social interaction scenarios dataset,\nHSII-Dataset. The dataset is derived step by step from news dataset. We perform\nan ablation study by doing clustering to the dataset. Additionally, we\ninvestigate the impact of chain of thought (COT) method on enhancing LLMs'\nsocial performance. Since COT cost more computation, we further introduce a new\nstatistical metric, COT-complexity, to quantify the efficiency of certain LLMs\nwith COTs for specific social tasks and strike a better trade-off between\nmeasurement of correctness and efficiency. Various results of our experiments\ndemonstrate that our benchmark is well-suited for evaluating social skills in\nLLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.04628v1",
    "published_date": "2025-04-04 08:59:01 UTC",
    "updated_date": "2025-04-04 08:59:01 UTC"
  },
  {
    "arxiv_id": "2504.03277v1",
    "title": "Monte Carlo Graph Coloring",
    "authors": [
      "Tristan Cazenave",
      "Benjamin Negrevergne",
      "Florian Sikora"
    ],
    "abstract": "Graph Coloring is probably one of the most studied and famous problem in\ngraph algorithms. Exact methods fail to solve instances with more than few\nhundred vertices, therefore, a large number of heuristics have been proposed.\nNested Monte Carlo Search (NMCS) and Nested Rollout Policy Adaptation (NRPA)\nare Monte Carlo search algorithms for single player games. Surprisingly, few\nwork has been dedicated to evaluating Monte Carlo search algorithms to\ncombinatorial graph problems. In this paper we expose how to efficiently apply\nMonte Carlo search to Graph Coloring and compare this approach to existing\nones.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03277v1",
    "published_date": "2025-04-04 08:57:01 UTC",
    "updated_date": "2025-04-04 08:57:01 UTC"
  },
  {
    "arxiv_id": "2504.03274v1",
    "title": "Do Large Language Models Solve the Problems of Agent-Based Modeling? A Critical Review of Generative Social Simulations",
    "authors": [
      "Maik Larooij",
      "Petter Törnberg"
    ],
    "abstract": "Recent advancements in AI have reinvigorated Agent-Based Models (ABMs), as\nthe integration of Large Language Models (LLMs) has led to the emergence of\n``generative ABMs'' as a novel approach to simulating social systems. While\nABMs offer means to bridge micro-level interactions with macro-level patterns,\nthey have long faced criticisms from social scientists, pointing to e.g., lack\nof realism, computational complexity, and challenges of calibrating and\nvalidating against empirical data. This paper reviews the generative ABM\nliterature to assess how this new approach adequately addresses these\nlong-standing criticisms. Our findings show that studies show limited awareness\nof historical debates. Validation remains poorly addressed, with many studies\nrelying solely on subjective assessments of model `believability', and even the\nmost rigorous validation failing to adequately evidence operational validity.\nWe argue that there are reasons to believe that LLMs will exacerbate rather\nthan resolve the long-standing challenges of ABMs. The black-box nature of LLMs\nmoreover limit their usefulness for disentangling complex emergent causal\nmechanisms. While generative ABMs are still in a stage of early\nexperimentation, these findings question of whether and how the field can\ntransition to the type of rigorous modeling needed to contribute to social\nscientific theory.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03274v1",
    "published_date": "2025-04-04 08:48:43 UTC",
    "updated_date": "2025-04-04 08:48:43 UTC"
  },
  {
    "arxiv_id": "2504.03272v1",
    "title": "Verification of Autonomous Neural Car Control with KeYmaera X",
    "authors": [
      "Enguerrand Prebet",
      "Samuel Teuber",
      "André Platzer"
    ],
    "abstract": "This article presents a formal model and formal safety proofs for the ABZ'25\ncase study in differential dynamic logic (dL). The case study considers an\nautonomous car driving on a highway avoiding collisions with neighbouring cars.\nUsing KeYmaera X's dL implementation, we prove absence of collision on an\ninfinite time horizon which ensures that safety is preserved independently of\ntrip length. The safety guarantees hold for time-varying reaction time and\nbrake force. Our dL model considers the single lane scenario with cars ahead or\nbehind. We demonstrate that dL with its tools is a rigorous foundation for\nruntime monitoring, shielding, and neural network verification. Doing so sheds\nlight on inconsistencies between the provided specification and simulation\nenvironment highway-env of the ABZ'25 study. We attempt to fix these\ninconsistencies and uncover numerous counterexamples which also indicate issues\nin the provided reinforcement learning environment.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.LO",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "21 pages, 6 figures; Accepted at the 11th International Conference on\n  Rigorous State Based Methods (ABZ'25)",
    "pdf_url": "http://arxiv.org/pdf/2504.03272v1",
    "published_date": "2025-04-04 08:43:31 UTC",
    "updated_date": "2025-04-04 08:43:31 UTC"
  },
  {
    "arxiv_id": "2504.03259v1",
    "title": "An Extended Symbolic-Arithmetic Model for Teaching Double-Black Removal with Rotation in Red-Black Trees",
    "authors": [
      "Kennedy E. Ehimwenma",
      "Hongyu Zhou",
      "Junfeng Wang",
      "Ze Zheng"
    ],
    "abstract": "Double-black (DB) nodes have no place in red-black (RB) trees. So when DB\nnodes are formed, they are immediately removed. The removal of DB nodes that\ncause rotation and recoloring of other connected nodes poses greater challenges\nin the teaching and learning of RB trees. To ease this difficulty, this paper\nextends our previous work on the symbolic arithmetic algebraic (SA) method for\nremoving DB nodes. The SA operations that are given as, Red + Black = Black;\nBlack - Black = Red; Black + Black = DB; and DB - Black = Black removes DB\nnodes and rebalances black heights in RB trees. By extension, this paper\nprojects three SA mathematical equations, namely, general symbolic arithmetic\nrule; partial symbolic arithmetic rule1; and partial symbolic arithmetic rule2.\nThe removal of a DB node ultimately affects black heights in RB trees. To\nbalance black heights using the SA equations, all the RB tree cases, namely,\nLR, RL, LL, and RR, were considered in this work; and the position of the nodes\nconnected directly or indirectly to the DB node was also tested. In this study,\nto balance a RB tree, the issues considered w.r.t. the different cases of the\nRB tree were i) whether a DB node has an inner, outer, or both inner and outer\nblack nephews; or ii) whether a DB node has an inner, outer or both inner and\nouter red nephews. The nephews r and x in this work are the children of the\nsibling s to a DB, and further up the tree, the parent p of a DB is their\ngrandparent g. Thus, r and x have indirect relationships to a DB at the point\nof formation of the DB node. The novelty of the SA equations is in their\neffectiveness in the removal of DB that involves rotation of nodes as well as\nthe recoloring of nodes along any simple path so as to balance black heights in\na tree.",
    "categories": [
      "cs.DS",
      "cs.AI"
    ],
    "primary_category": "cs.DS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03259v1",
    "published_date": "2025-04-04 08:19:26 UTC",
    "updated_date": "2025-04-04 08:19:26 UTC"
  },
  {
    "arxiv_id": "2504.03801v1",
    "title": "Semantic-guided Representation Learning for Multi-Label Recognition",
    "authors": [
      "Ruhui Zhang",
      "Hezhe Qiao",
      "Pengcheng Xu",
      "Mingsheng Shang",
      "Lin Chen"
    ],
    "abstract": "Multi-label Recognition (MLR) involves assigning multiple labels to each data\ninstance in an image, offering advantages over single-label classification in\ncomplex scenarios. However, it faces the challenge of annotating all relevant\ncategories, often leading to uncertain annotations, such as unseen or\nincomplete labels. Recent Vision and Language Pre-training (VLP) based methods\nhave made significant progress in tackling zero-shot MLR tasks by leveraging\nrich vision-language correlations. However, the correlation between multi-label\nsemantics has not been fully explored, and the learned visual features often\nlack essential semantic information. To overcome these limitations, we\nintroduce a Semantic-guided Representation Learning approach (SigRL) that\nenables the model to learn effective visual and textual representations,\nthereby improving the downstream alignment of visual images and categories.\nSpecifically, we first introduce a graph-based multi-label correlation module\n(GMC) to facilitate information exchange between labels, enriching the semantic\nrepresentation across the multi-label texts. Next, we propose a Semantic Visual\nFeature Reconstruction module (SVFR) to enhance the semantic information in the\nvisual representation by integrating the learned textual representation during\nreconstruction. Finally, we optimize the image-text matching capability of the\nVLP model using both local and global features to achieve zero-shot MLR.\nComprehensive experiments are conducted on several MLR benchmarks, encompassing\nboth zero-shot MLR (with unseen labels) and single positive multi-label\nlearning (with limited labels), demonstrating the superior performance of our\napproach compared to state-of-the-art methods. The code is available at\nhttps://github.com/MVL-Lab/SigRL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in ICME2025",
    "pdf_url": "http://arxiv.org/pdf/2504.03801v1",
    "published_date": "2025-04-04 08:15:08 UTC",
    "updated_date": "2025-04-04 08:15:08 UTC"
  },
  {
    "arxiv_id": "2504.03245v1",
    "title": "Seeing is Believing: Belief-Space Planning with Foundation Models as Uncertainty Estimators",
    "authors": [
      "Linfeng Zhao",
      "Willie McClinton",
      "Aidan Curtis",
      "Nishanth Kumar",
      "Tom Silver",
      "Leslie Pack Kaelbling",
      "Lawson L. S. Wong"
    ],
    "abstract": "Generalizable robotic mobile manipulation in open-world environments poses\nsignificant challenges due to long horizons, complex goals, and partial\nobservability. A promising approach to address these challenges involves\nplanning with a library of parameterized skills, where a task planner sequences\nthese skills to achieve goals specified in structured languages, such as\nlogical expressions over symbolic facts. While vision-language models (VLMs)\ncan be used to ground these expressions, they often assume full observability,\nleading to suboptimal behavior when the agent lacks sufficient information to\nevaluate facts with certainty. This paper introduces a novel framework that\nleverages VLMs as a perception module to estimate uncertainty and facilitate\nsymbolic grounding. Our approach constructs a symbolic belief representation\nand uses a belief-space planner to generate uncertainty-aware plans that\nincorporate strategic information gathering. This enables the agent to\neffectively reason about partial observability and property uncertainty. We\ndemonstrate our system on a range of challenging real-world tasks that require\nreasoning in partially observable environments. Simulated evaluations show that\nour approach outperforms both vanilla VLM-based end-to-end planning or\nVLM-based state estimation baselines by planning for and executing strategic\ninformation gathering. This work highlights the potential of VLMs to construct\nbelief-space symbolic scene representations, enabling downstream tasks such as\nuncertainty-aware planning.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03245v1",
    "published_date": "2025-04-04 07:48:53 UTC",
    "updated_date": "2025-04-04 07:48:53 UTC"
  },
  {
    "arxiv_id": "2504.03241v1",
    "title": "Rotation Invariance in Floor Plan Digitization using Zernike Moments",
    "authors": [
      "Marius Graumann",
      "Jan Marius Stürmer",
      "Tobias Koch"
    ],
    "abstract": "Nowadays, a lot of old floor plans exist in printed form or are stored as\nscanned raster images. Slight rotations or shifts may occur during scanning.\nBringing floor plans of this form into a machine readable form to enable\nfurther use, still poses a problem. Therefore, we propose an end-to-end\npipeline that pre-processes the image and leverages a novel approach to create\na region adjacency graph (RAG) from the pre-processed image and predict its\nnodes. By incorporating normalization steps into the RAG feature extraction, we\nsignificantly improved the rotation invariance of the RAG feature calculation.\nMoreover, applying our method leads to an improved F1 score and IoU on rotated\ndata. Furthermore, we proposed a wall splitting algorithm for partitioning\nwalls into segments associated with the corresponding rooms.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.03241v1",
    "published_date": "2025-04-04 07:44:07 UTC",
    "updated_date": "2025-04-04 07:44:07 UTC"
  },
  {
    "arxiv_id": "2504.03800v1",
    "title": "Decision SpikeFormer: Spike-Driven Transformer for Decision Making",
    "authors": [
      "Wei Huang",
      "Qinying Gu",
      "Nanyang Ye"
    ],
    "abstract": "Offline reinforcement learning (RL) enables policy training solely on\npre-collected data, avoiding direct environment interaction - a crucial benefit\nfor energy-constrained embodied AI applications. Although Artificial Neural\nNetworks (ANN)-based methods perform well in offline RL, their high\ncomputational and energy demands motivate exploration of more efficient\nalternatives. Spiking Neural Networks (SNNs) show promise for such tasks, given\ntheir low power consumption. In this work, we introduce DSFormer, the first\nspike-driven transformer model designed to tackle offline RL via sequence\nmodeling. Unlike existing SNN transformers focused on spatial dimensions for\nvision tasks, we develop Temporal Spiking Self-Attention (TSSA) and Positional\nSpiking Self-Attention (PSSA) in DSFormer to capture the temporal and\npositional dependencies essential for sequence modeling in RL. Additionally, we\npropose Progressive Threshold-dependent Batch Normalization (PTBN), which\ncombines the benefits of LayerNorm and BatchNorm to preserve temporal\ndependencies while maintaining the spiking nature of SNNs. Comprehensive\nresults in the D4RL benchmark show DSFormer's superiority over both SNN and ANN\ncounterparts, achieving 78.4% energy savings, highlighting DSFormer's\nadvantages not only in energy efficiency but also in competitive performance.\nCode and models are public at https://wei-nijuan.github.io/DecisionSpikeFormer.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "This work has been accepted to CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.03800v1",
    "published_date": "2025-04-04 07:42:36 UTC",
    "updated_date": "2025-04-04 07:42:36 UTC"
  },
  {
    "arxiv_id": "2504.03238v1",
    "title": "Malware Detection in Docker Containers: An Image is Worth a Thousand Logs",
    "authors": [
      "Akis Nousias",
      "Efklidis Katsaros",
      "Evangelos Syrmos",
      "Panagiotis Radoglou-Grammatikis",
      "Thomas Lagkas",
      "Vasileios Argyriou",
      "Ioannis Moscholios",
      "Evangelos Markakis",
      "Sotirios Goudos",
      "Panagiotis Sarigiannidis"
    ],
    "abstract": "Malware detection is increasingly challenged by evolving techniques like\nobfuscation and polymorphism, limiting the effectiveness of traditional\nmethods. Meanwhile, the widespread adoption of software containers has\nintroduced new security challenges, including the growing threat of malicious\nsoftware injection, where a container, once compromised, can serve as entry\npoint for further cyberattacks. In this work, we address these security issues\nby introducing a method to identify compromised containers through machine\nlearning analysis of their file systems. We cast the entire software containers\ninto large RGB images via their tarball representations, and propose to use\nestablished Convolutional Neural Network architectures on a streaming,\npatch-based manner. To support our experiments, we release the COSOCO\ndataset--the first of its kind--containing 3364 large-scale RGB images of\nbenign and compromised software containers at\nhttps://huggingface.co/datasets/k3ylabs/cosoco-image-dataset. Our method\ndetects more malware and achieves higher F1 and Recall scores than all\nindividual and ensembles of VirusTotal engines, demonstrating its effectiveness\nand setting a new standard for identifying malware-compromised software\ncontainers.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted at ICC-W",
    "pdf_url": "http://arxiv.org/pdf/2504.03238v1",
    "published_date": "2025-04-04 07:38:16 UTC",
    "updated_date": "2025-04-04 07:38:16 UTC"
  },
  {
    "arxiv_id": "2504.03235v1",
    "title": "Crash Time Matters: HybridMamba for Fine-Grained Temporal Localization in Traffic Surveillance Footage",
    "authors": [
      "Ibne Farabi Shihab",
      "Anuj Sharma"
    ],
    "abstract": "Traffic crash detection in long-form surveillance videos is critical for\nemergency response and infrastructure planning but remains difficult due to the\nbrief and rare nature of crash events. We introduce HybridMamba, a novel\narchitecture that combines visual transformers with state-space temporal\nmodeling to achieve accurate crash time localization. Our method uses\nmulti-level token compression and hierarchical temporal processing to remain\ncomputationally efficient without sacrificing temporal resolution. Evaluated on\na large-scale dataset from the Iowa Department of Transportation, HybridMamba\nachieves a mean absolute error of 1.50 seconds, with 65.2 percent of\npredictions within one second of the ground truth. It outperforms recent\nvideo-language models such as TimeChat and VideoLLaMA2 by up to 2.8 seconds,\nwhile using significantly fewer parameters. Our results demonstrate strong\ngeneralization across videos ranging from 2 to 40 minutes in diverse\nconditions. HybridMamba offers a robust and efficient solution for fine-grained\ntemporal localization in traffic surveillance. The code will be released upon\npublication.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03235v1",
    "published_date": "2025-04-04 07:35:11 UTC",
    "updated_date": "2025-04-04 07:35:11 UTC"
  },
  {
    "arxiv_id": "2504.03234v2",
    "title": "Think When You Need: Self-Adaptive Chain-of-Thought Learning",
    "authors": [
      "Junjie Yang",
      "Ke Lin",
      "Xing Yu"
    ],
    "abstract": "Chain of Thought (CoT) reasoning enhances language models' performance but\noften leads to inefficient \"overthinking\" on simple problems. We identify that\nexisting approaches directly penalizing reasoning length fail to account for\nvarying problem complexity. Our approach constructs rewards through length and\nquality comparisons, guided by theoretical assumptions that jointly enhance\nsolution correctness with conciseness. Moreover, we further demonstrate our\nmethod to fuzzy tasks where ground truth is unavailable. Experiments across\nmultiple reasoning benchmarks demonstrate that our method maintains accuracy\nwhile generating significantly more concise explanations, effectively teaching\nmodels to \"think when needed.\"",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2504.03234v2",
    "published_date": "2025-04-04 07:34:01 UTC",
    "updated_date": "2025-05-21 15:26:54 UTC"
  },
  {
    "arxiv_id": "2504.03211v1",
    "title": "Persuasive Calibration",
    "authors": [
      "Yiding Feng",
      "Wei Tang"
    ],
    "abstract": "We introduce and study the persuasive calibration problem, where a principal\naims to provide trustworthy predictions about underlying events to a downstream\nagent to make desired decisions. We adopt the standard calibration framework\nthat regulates predictions to be unbiased conditional on their own value, and\nthus, they can reliably be interpreted at the face value by the agent. Allowing\na small calibration error budget, we aim to answer the following question: what\nis and how to compute the optimal predictor under this calibration error\nbudget, especially when there exists incentive misalignment between the\nprincipal and the agent? We focus on standard Lt-norm Expected Calibration\nError (ECE) metric.\n  We develop a general framework by viewing predictors as post-processed\nversions of perfectly calibrated predictors. Using this framework, we first\ncharacterize the structure of the optimal predictor. Specifically, when the\nprincipal's utility is event-independent and for L1-norm ECE, we show: (1) the\noptimal predictor is over-(resp. under-) confident for high (resp. low) true\nexpected outcomes, while remaining perfectly calibrated in the middle; (2) the\nmiscalibrated predictions exhibit a collinearity structure with the principal's\nutility function. On the algorithmic side, we provide a FPTAS for computing\napproximately optimal predictor for general principal utility and general\nLt-norm ECE. Moreover, for the L1- and L-Infinity-norm ECE, we provide\npolynomial-time algorithms that compute the exact optimal predictor.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT",
      "econ.TH"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03211v1",
    "published_date": "2025-04-04 06:49:56 UTC",
    "updated_date": "2025-04-04 06:49:56 UTC"
  },
  {
    "arxiv_id": "2504.03207v1",
    "title": "Augmenting Human Cognition With Generative AI: Lessons From AI-Assisted Decision-Making",
    "authors": [
      "Zelun Tony Zhang",
      "Leon Reicherts"
    ],
    "abstract": "How can we use generative AI to design tools that augment rather than replace\nhuman cognition? In this position paper, we review our own research on\nAI-assisted decision-making for lessons to learn. We observe that in both\nAI-assisted decision-making and generative AI, a popular approach is to suggest\nAI-generated end-to-end solutions to users, which users can then accept,\nreject, or edit. Alternatively, AI tools could offer more incremental support\nto help users solve tasks themselves, which we call process-oriented support.\nWe describe findings on the challenges of end-to-end solutions, and how\nprocess-oriented support can address them. We also discuss the applicability of\nthese findings to generative AI based on a recent study in which we compared\nboth approaches to assist users in a complex decision-making task with LLMs.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03207v1",
    "published_date": "2025-04-04 06:40:03 UTC",
    "updated_date": "2025-04-04 06:40:03 UTC"
  },
  {
    "arxiv_id": "2504.03206v1",
    "title": "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward",
    "authors": [
      "Yanming Wan",
      "Jiaxing Wu",
      "Marwa Abdulhai",
      "Lior Shani",
      "Natasha Jaques"
    ],
    "abstract": "Effective conversational agents must be able to personalize their behavior to\nsuit a user's preferences, personality, and attributes, whether they are\nassisting with writing tasks or operating in domains like education or\nhealthcare. Current training methods like Reinforcement Learning from Human\nFeedback (RLHF) prioritize helpfulness and safety but fall short in fostering\ntruly empathetic, adaptive, and personalized interactions. Traditional\napproaches to personalization often rely on extensive user history, limiting\ntheir effectiveness for new or context-limited users. To overcome these\nlimitations, we propose to incorporate an intrinsic motivation to improve the\nconversational agents's model of the user as an additional reward alongside\nmulti-turn RLHF. This reward mechanism encourages the agent to actively elicit\nuser traits by optimizing conversations to increase the accuracy of its user\nmodel. Consequently, the policy agent can deliver more personalized\ninteractions through obtaining more information about the user. We applied our\nmethod both education and fitness settings, where LLMs teach concepts or\nrecommend personalized strategies based on users' hidden learning style or\nlifestyle attributes. Using LLM-simulated users, our approach outperformed a\nmulti-turn RLHF baseline in revealing information about the users' preferences,\nand adapting to them.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03206v1",
    "published_date": "2025-04-04 06:35:02 UTC",
    "updated_date": "2025-04-04 06:35:02 UTC"
  },
  {
    "arxiv_id": "2504.03198v1",
    "title": "Endo3R: Unified Online Reconstruction from Dynamic Monocular Endoscopic Video",
    "authors": [
      "Jiaxin Guo",
      "Wenzhen Dong",
      "Tianyu Huang",
      "Hao Ding",
      "Ziyi Wang",
      "Haomin Kuang",
      "Qi Dou",
      "Yun-Hui Liu"
    ],
    "abstract": "Reconstructing 3D scenes from monocular surgical videos can enhance surgeon's\nperception and therefore plays a vital role in various computer-assisted\nsurgery tasks. However, achieving scale-consistent reconstruction remains an\nopen challenge due to inherent issues in endoscopic videos, such as dynamic\ndeformations and textureless surfaces. Despite recent advances, current methods\neither rely on calibration or instrument priors to estimate scale, or employ\nSfM-like multi-stage pipelines, leading to error accumulation and requiring\noffline optimization. In this paper, we present Endo3R, a unified 3D foundation\nmodel for online scale-consistent reconstruction from monocular surgical video,\nwithout any priors or extra optimization. Our model unifies the tasks by\npredicting globally aligned pointmaps, scale-consistent video depths, and\ncamera parameters without any offline optimization. The core contribution of\nour method is expanding the capability of the recent pairwise reconstruction\nmodel to long-term incremental dynamic reconstruction by an uncertainty-aware\ndual memory mechanism. The mechanism maintains history tokens of both\nshort-term dynamics and long-term spatial consistency. Notably, to tackle the\nhighly dynamic nature of surgical scenes, we measure the uncertainty of tokens\nvia Sampson distance and filter out tokens with high uncertainty. Regarding the\nscarcity of endoscopic datasets with ground-truth depth and camera poses, we\nfurther devise a self-supervised mechanism with a novel dynamics-aware flow\nloss. Abundant experiments on SCARED and Hamlyn datasets demonstrate our\nsuperior performance in zero-shot surgical video depth prediction and camera\npose estimation with online efficiency. Project page:\nhttps://wrld.github.io/Endo3R/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03198v1",
    "published_date": "2025-04-04 06:05:22 UTC",
    "updated_date": "2025-04-04 06:05:22 UTC"
  },
  {
    "arxiv_id": "2504.03799v2",
    "title": "Experimental Study on Time Series Analysis of Lower Limb Rehabilitation Exercise Data Driven by Novel Model Architecture and Large Models",
    "authors": [
      "Hengyu Lin"
    ],
    "abstract": "This study investigates the application of novel model architectures and\nlarge-scale foundational models in temporal series analysis of lower limb\nrehabilitation motion data, aiming to leverage advancements in machine learning\nand artificial intelligence to empower active rehabilitation guidance\nstrategies for post-stroke patients in limb motor function recovery. Utilizing\nthe SIAT-LLMD dataset of lower limb movement data proposed by the Shenzhen\nInstitute of Advanced Technology, Chinese Academy of Sciences, we\nsystematically elucidate the implementation and analytical outcomes of the\ninnovative xLSTM architecture and the foundational model Lag-Llama in\nshort-term temporal prediction tasks involving joint kinematics and dynamics\nparameters. The research provides novel insights for AI-enabled medical\nrehabilitation applications, demonstrating the potential of cutting-edge model\narchitectures and large-scale models in rehabilitation medicine temporal\nprediction. These findings establish theoretical foundations for future\napplications of personalized rehabilitation regimens, offering significant\nimplications for the development of customized therapeutic interventions in\nclinical practice.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03799v2",
    "published_date": "2025-04-04 05:40:13 UTC",
    "updated_date": "2025-04-29 13:28:41 UTC"
  },
  {
    "arxiv_id": "2504.03798v1",
    "title": "An Intelligent and Privacy-Preserving Digital Twin Model for Aging-in-Place",
    "authors": [
      "Yongjie Wang",
      "Jonathan Cyril Leung",
      "Ming Chen",
      "Zhiwei Zeng",
      "Benny Toh Hsiang Tan",
      "Yang Qiu",
      "Zhiqi Shen"
    ],
    "abstract": "The population of older adults is steadily increasing, with a strong\npreference for aging-in-place rather than moving to care facilities.\nConsequently, supporting this growing demographic has become a significant\nglobal challenge. However, facilitating successful aging-in-place is\nchallenging, requiring consideration of multiple factors such as data privacy,\nhealth status monitoring, and living environments to improve health outcomes.\nIn this paper, we propose an unobtrusive sensor system designed for\ninstallation in older adults' homes. Using data from the sensors, our system\nconstructs a digital twin, a virtual representation of events and activities\nthat occurred in the home. The system uses neural network models and decision\nrules to capture residents' activities and living environments. This digital\ntwin enables continuous health monitoring by providing actionable insights into\nresidents' well-being. Our system is designed to be low-cost and\nprivacy-preserving, with the aim of providing green and safe monitoring for the\nhealth of older adults. We have successfully deployed our system in two homes\nover a time period of two months, and our findings demonstrate the feasibility\nand effectiveness of digital twin technology in supporting independent living\nfor older adults. This study highlights that our system could revolutionize\nelder care by enabling personalized interventions, such as lifestyle\nadjustments, medical treatments, or modifications to the residential\nenvironment, to enhance health outcomes.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "68T05,",
      "I.2; J.3"
    ],
    "primary_category": "cs.CY",
    "comment": "accepted to IEEE TENSYMP 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.03798v1",
    "published_date": "2025-04-04 05:37:08 UTC",
    "updated_date": "2025-04-04 05:37:08 UTC"
  },
  {
    "arxiv_id": "2504.03185v1",
    "title": "Learning Natural Language Constraints for Safe Reinforcement Learning of Language Agents",
    "authors": [
      "Jaymari Chua",
      "Chen Wang",
      "Lina Yao"
    ],
    "abstract": "Generalizable alignment is a core challenge for deploying Large Language\nModels (LLMs) safely in real-world NLP applications. Current alignment methods,\nincluding Reinforcement Learning from Human Feedback (RLHF), often fail to\nguarantee constraint satisfaction outside their training distribution due to\ntheir reliance on implicit, post-hoc preferences. Inspired by a paradigm shift\nto first curate data before tuning, we introduce a new framework for safe\nlanguage alignment that learns natural language constraints from positive and\nnegative demonstrations as a primary step. From inferring both a task-specific\nreward function and latent constraint functions, our approach fosters\nadaptation to novel safety requirements and robust generalization under domain\nshifts and adversarial inputs. We formalize the framework within a Constrained\nMarkov Decision Process (CMDP) and validate it via a text-based navigation\nenvironment, demonstrating safe adaptation to changing danger zones. Our\nexperiments show fewer violations upon domain shift when following a safe\nnavigation path, and we achieve zero violations by applying learned constraints\nto a distilled BERT model as a fine-tuning technique. This work offers a\npromising path toward building safety-critical and more generalizable LLMs for\npractical NLP settings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7; I.2.4; I.2.6; I.2.8"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03185v1",
    "published_date": "2025-04-04 05:26:28 UTC",
    "updated_date": "2025-04-04 05:26:28 UTC"
  },
  {
    "arxiv_id": "2504.03171v1",
    "title": "Real-Time Roadway Obstacle Detection for Electric Scooters Using Deep Learning and Multi-Sensor Fusion",
    "authors": [
      "Zeyang Zheng",
      "Arman Hosseini",
      "Dong Chen",
      "Omid Shoghli",
      "Arsalan Heydarian"
    ],
    "abstract": "The increasing adoption of electric scooters (e-scooters) in urban areas has\ncoincided with a rise in traffic accidents and injuries, largely due to their\nsmall wheels, lack of suspension, and sensitivity to uneven surfaces. While\ndeep learning-based object detection has been widely used to improve automobile\nsafety, its application for e-scooter obstacle detection remains unexplored.\nThis study introduces a novel ground obstacle detection system for e-scooters,\nintegrating an RGB camera, and a depth camera to enhance real-time road hazard\ndetection. Additionally, the Inertial Measurement Unit (IMU) measures linear\nvertical acceleration to identify surface vibrations, guiding the selection of\nsix obstacle categories: tree branches, manhole covers, potholes, pine cones,\nnon-directional cracks, and truncated domes. All sensors, including the RGB\ncamera, depth camera, and IMU, are integrated within the Intel RealSense Camera\nD435i. A deep learning model powered by YOLO detects road hazards and utilizes\ndepth data to estimate obstacle proximity. Evaluated on the seven hours of\nnaturalistic riding dataset, the system achieves a high mean average precision\n(mAP) of 0.827 and demonstrates excellent real-time performance. This approach\nprovides an effective solution to enhance e-scooter safety through advanced\ncomputer vision and data fusion. The dataset is accessible at\nhttps://zenodo.org/records/14583718, and the project code is hosted on\nhttps://github.com/Zeyang-Zheng/Real-Time-Roadway-Obstacle-Detection-for-Electric-Scooters.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ASCE International Conference on Computing in Civil\n  Engineering (i3ce)",
    "pdf_url": "http://arxiv.org/pdf/2504.03171v1",
    "published_date": "2025-04-04 05:01:16 UTC",
    "updated_date": "2025-04-04 05:01:16 UTC"
  },
  {
    "arxiv_id": "2504.03164v2",
    "title": "NuScenes-SpatialQA: A Spatial Understanding and Reasoning Benchmark for Vision-Language Models in Autonomous Driving",
    "authors": [
      "Kexin Tian",
      "Jingrui Mao",
      "Yunlong Zhang",
      "Jiwan Jiang",
      "Yang Zhou",
      "Zhengzhong Tu"
    ],
    "abstract": "Recent advancements in Vision-Language Models (VLMs) have demonstrated strong\npotential for autonomous driving tasks. However, their spatial understanding\nand reasoning-key capabilities for autonomous driving-still exhibit significant\nlimitations. Notably, none of the existing benchmarks systematically evaluate\nVLMs' spatial reasoning capabilities in driving scenarios. To fill this gap, we\npropose NuScenes-SpatialQA, the first large-scale ground-truth-based\nQuestion-Answer (QA) benchmark specifically designed to evaluate the spatial\nunderstanding and reasoning capabilities of VLMs in autonomous driving. Built\nupon the NuScenes dataset, the benchmark is constructed through an automated 3D\nscene graph generation pipeline and a QA generation pipeline. The benchmark\nsystematically evaluates VLMs' performance in both spatial understanding and\nreasoning across multiple dimensions. Using this benchmark, we conduct\nextensive experiments on diverse VLMs, including both general and\nspatial-enhanced models, providing the first comprehensive evaluation of their\nspatial capabilities in autonomous driving. Surprisingly, the experimental\nresults show that the spatial-enhanced VLM outperforms in qualitative QA but\ndoes not demonstrate competitiveness in quantitative QA. In general, VLMs still\nface considerable challenges in spatial understanding and reasoning.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03164v2",
    "published_date": "2025-04-04 04:43:10 UTC",
    "updated_date": "2025-04-07 03:39:02 UTC"
  },
  {
    "arxiv_id": "2504.03160v4",
    "title": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments",
    "authors": [
      "Yuxiang Zheng",
      "Dayuan Fu",
      "Xiangkun Hu",
      "Xiaojie Cai",
      "Lyumanshan Ye",
      "Pengrui Lu",
      "Pengfei Liu"
    ],
    "abstract": "Large Language Models (LLMs) equipped with web search capabilities have\ndemonstrated impressive potential for deep research tasks. However, current\napproaches predominantly rely on either manually engineered prompts (prompt\nengineering-based) with brittle performance or reinforcement learning within\ncontrolled Retrieval-Augmented Generation (RAG) environments (RAG-based) that\nfail to capture the complexities of real-world interaction. In this paper, we\nintroduce DeepResearcher, the first comprehensive framework for end-to-end\ntraining of LLM-based deep research agents through scaling reinforcement\nlearning (RL) in real-world environments with authentic web search\ninteractions. Unlike RAG-based approaches that assume all necessary information\nexists within a fixed corpus, our method trains agents to navigate the noisy,\nunstructured, and dynamic nature of the open web. We implement a specialized\nmulti-agent architecture where browsing agents extract relevant information\nfrom various webpage structures and overcoming significant technical\nchallenges. Extensive experiments on open-domain research tasks demonstrate\nthat DeepResearcher achieves substantial improvements of up to 28.9 points over\nprompt engineering-based baselines and up to 7.2 points over RAG-based RL\nagents. Our qualitative analysis reveals emergent cognitive behaviors from\nend-to-end RL training, including the ability to formulate plans,\ncross-validate information from multiple sources, engage in self-reflection to\nredirect research, and maintain honesty when unable to find definitive answers.\nOur results highlight that end-to-end training in real-world web environments\nis not merely an implementation detail but a fundamental requirement for\ndeveloping robust research capabilities aligned with real-world applications.\nWe release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03160v4",
    "published_date": "2025-04-04 04:41:28 UTC",
    "updated_date": "2025-04-17 04:46:08 UTC"
  },
  {
    "arxiv_id": "2504.08778v1",
    "title": "From Tokens to Lattices: Emergent Lattice Structures in Language Models",
    "authors": [
      "Bo Xiong",
      "Steffen Staab"
    ],
    "abstract": "Pretrained masked language models (MLMs) have demonstrated an impressive\ncapability to comprehend and encode conceptual knowledge, revealing a lattice\nstructure among concepts. This raises a critical question: how does this\nconceptualization emerge from MLM pretraining? In this paper, we explore this\nproblem from the perspective of Formal Concept Analysis (FCA), a mathematical\nframework that derives concept lattices from the observations of\nobject-attribute relationships. We show that the MLM's objective implicitly\nlearns a \\emph{formal context} that describes objects, attributes, and their\ndependencies, which enables the reconstruction of a concept lattice through\nFCA. We propose a novel framework for concept lattice construction from\npretrained MLMs and investigate the origin of the inductive biases of MLMs in\nlattice structure learning. Our framework differs from previous work because it\ndoes not rely on human-defined concepts and allows for discovering \"latent\"\nconcepts that extend beyond human definitions. We create three datasets for\nevaluation, and the empirical results verify our hypothesis.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.08778v1",
    "published_date": "2025-04-04 04:28:17 UTC",
    "updated_date": "2025-04-04 04:28:17 UTC"
  },
  {
    "arxiv_id": "2504.03147v1",
    "title": "A Human Digital Twin Architecture for Knowledge-based Interactions and Context-Aware Conversations",
    "authors": [
      "Abdul Mannan Mohammed",
      "Azhar Ali Mohammad",
      "Jason A. Ortiz",
      "Carsten Neumann",
      "Grace Bochenek",
      "Dirk Reiners",
      "Carolina Cruz-Neira"
    ],
    "abstract": "Recent developments in Artificial Intelligence (AI) and Machine Learning (ML)\nare creating new opportunities for Human-Autonomy Teaming (HAT) in tasks,\nmissions, and continuous coordinated activities. A major challenge is enabling\nhumans to maintain awareness and control over autonomous assets, while also\nbuilding trust and supporting shared contextual understanding. To address this,\nwe present a real-time Human Digital Twin (HDT) architecture that integrates\nLarge Language Models (LLMs) for knowledge reporting, answering, and\nrecommendation, embodied in a visual interface.\n  The system applies a metacognitive approach to enable personalized,\ncontext-aware responses aligned with the human teammate's expectations. The HDT\nacts as a visually and behaviorally realistic team member, integrated\nthroughout the mission lifecycle, from training to deployment to after-action\nreview. Our architecture includes speech recognition, context processing,\nAI-driven dialogue, emotion modeling, lip-syncing, and multimodal feedback. We\ndescribe the system design, performance metrics, and future development\ndirections for more adaptive and realistic HAT systems.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Presented at: 2024 Interservice/Industry Training, Simulation, and\n  Education Conference (I/ITSEC), Paper No. 24366, 10 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.03147v1",
    "published_date": "2025-04-04 03:56:26 UTC",
    "updated_date": "2025-04-04 03:56:26 UTC"
  },
  {
    "arxiv_id": "2504.03794v1",
    "title": "Entropy-Based Block Pruning for Efficient Large Language Models",
    "authors": [
      "Liangwei Yang",
      "Yuhui Xu",
      "Juntao Tan",
      "Doyen Sahoo",
      "Silvio Savarese",
      "Caiming Xiong",
      "Huan Wang",
      "Shelby Heinecke"
    ],
    "abstract": "As large language models continue to scale, their growing computational and\nstorage demands pose significant challenges for real-world deployment. In this\nwork, we investigate redundancy within Transformer-based models and propose an\nentropy-based pruning strategy to enhance efficiency while maintaining\nperformance. Empirical analysis reveals that the entropy of hidden\nrepresentations decreases in the early blocks but progressively increases\nacross most subsequent blocks. This trend suggests that entropy serves as a\nmore effective measure of information richness within computation blocks.\nUnlike cosine similarity, which primarily captures geometric relationships,\nentropy directly quantifies uncertainty and information content, making it a\nmore reliable criterion for pruning. Extensive experiments demonstrate that our\nentropy-based pruning approach surpasses cosine similarity-based methods in\nreducing model size while preserving accuracy, offering a promising direction\nfor efficient model deployment.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.03794v1",
    "published_date": "2025-04-04 03:42:34 UTC",
    "updated_date": "2025-04-04 03:42:34 UTC"
  },
  {
    "arxiv_id": "2504.03793v1",
    "title": "Outlook Towards Deployable Continual Learning for Particle Accelerators",
    "authors": [
      "Kishansingh Rajput",
      "Sen Lin",
      "Auralee Edelen",
      "Willem Blokland",
      "Malachi Schram"
    ],
    "abstract": "Particle Accelerators are high power complex machines. To ensure\nuninterrupted operation of these machines, thousands of pieces of equipment\nneed to be synchronized, which requires addressing many challenges including\ndesign, optimization and control, anomaly detection and machine protection.\nWith recent advancements, Machine Learning (ML) holds promise to assist in more\nadvance prognostics, optimization, and control. While ML based solutions have\nbeen developed for several applications in particle accelerators, only few have\nreached deployment and even fewer to long term usage, due to particle\naccelerator data distribution drifts caused by changes in both measurable and\nnon-measurable parameters. In this paper, we identify some of the key areas\nwithin particle accelerators where continual learning can allow maintenance of\nML model performance with distribution drifts. Particularly, we first discuss\nexisting applications of ML in particle accelerators, and their limitations due\nto distribution drift. Next, we review existing continual learning techniques\nand investigate their potential applications to address data distribution\ndrifts in accelerators. By identifying the opportunities and challenges in\napplying continual learning, this paper seeks to open up the new field and\ninspire more research efforts towards deployable continual learning for\nparticle accelerators.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "41 pages, 6 figures, submitted to Machine Learning: Science and\n  Technology Journal",
    "pdf_url": "http://arxiv.org/pdf/2504.03793v1",
    "published_date": "2025-04-04 03:34:39 UTC",
    "updated_date": "2025-04-04 03:34:39 UTC"
  },
  {
    "arxiv_id": "2504.03137v1",
    "title": "LightPROF: A Lightweight Reasoning Framework for Large Language Model on Knowledge Graph",
    "authors": [
      "Tu Ao",
      "Yanhua Yu",
      "Yuling Wang",
      "Yang Deng",
      "Zirui Guo",
      "Liang Pang",
      "Pinghui Wang",
      "Tat-Seng Chua",
      "Xiao Zhang",
      "Zhen Cai"
    ],
    "abstract": "Large Language Models (LLMs) have impressive capabilities in text\nunderstanding and zero-shot reasoning. However, delays in knowledge updates may\ncause them to reason incorrectly or produce harmful results. Knowledge Graphs\n(KGs) provide rich and reliable contextual information for the reasoning\nprocess of LLMs by structurally organizing and connecting a wide range of\nentities and relations. Existing KG-based LLM reasoning methods only inject\nKGs' knowledge into prompts in a textual form, ignoring its structural\ninformation. Moreover, they mostly rely on close-source models or open-source\nmodels with large parameters, which poses challenges to high resource\nconsumption. To address this, we propose a novel Lightweight and efficient\nPrompt learning-ReasOning Framework for KGQA (LightPROF), which leverages the\nfull potential of LLMs to tackle complex reasoning tasks in a\nparameter-efficient manner. Specifically, LightPROF follows a\n\"Retrieve-Embed-Reason process\", first accurately, and stably retrieving the\ncorresponding reasoning graph from the KG through retrieval module. Next,\nthrough a Transformer-based Knowledge Adapter, it finely extracts and\nintegrates factual and structural information from the KG, then maps this\ninformation to the LLM's token embedding space, creating an LLM-friendly prompt\nto be used by the LLM for the final reasoning. Additionally, LightPROF only\nrequires training Knowledge Adapter and can be compatible with any open-source\nLLM. Extensive experiments on two public KGQA benchmarks demonstrate that\nLightPROF achieves superior performance with small-scale LLMs. Furthermore,\nLightPROF shows significant advantages in terms of input token count and\nreasoning time.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.03137v1",
    "published_date": "2025-04-04 03:03:47 UTC",
    "updated_date": "2025-04-04 03:03:47 UTC"
  },
  {
    "arxiv_id": "2504.03135v2",
    "title": "Hierarchical Modeling for Medical Visual Question Answering with Cross-Attention Fusion",
    "authors": [
      "Junkai Zhang",
      "Bin Li",
      "Shoujun Zhou",
      "Yue Du"
    ],
    "abstract": "Medical Visual Question Answering (Med-VQA) answers clinical questions using\nmedical images, aiding diagnosis. Designing the MedVQA system holds profound\nimportance in assisting clinical diagnosis and enhancing diagnostic accuracy.\nBuilding upon this foundation, Hierarchical Medical VQA extends Medical VQA by\norganizing medical questions into a hierarchical structure and making\nlevel-specific predictions to handle fine-grained distinctions. Recently, many\nstudies have proposed hierarchical MedVQA tasks and established datasets,\nHowever, several issues still remain: (1) imperfect hierarchical modeling leads\nto poor differentiation between question levels causing semantic fragmentation\nacross hierarchies. (2) Excessive reliance on implicit learning in\nTransformer-based cross-modal self-attention fusion methods, which obscures\ncrucial local semantic correlations in medical scenarios. To address these\nissues, this study proposes a HiCA-VQA method, including two modules:\nHierarchical Prompting for fine-grained medical questions and Hierarchical\nAnswer Decoders. The hierarchical prompting module pre-aligns hierarchical text\nprompts with image features to guide the model in focusing on specific image\nregions according to question types, while the hierarchical decoder performs\nseparate predictions for questions at different levels to improve accuracy\nacross granularities. The framework also incorporates a cross-attention fusion\nmodule where images serve as queries and text as key-value pairs. Experiments\non the Rad-Restruct benchmark demonstrate that the HiCA-VQA framework better\noutperforms existing state-of-the-art methods in answering hierarchical\nfine-grained questions. This study provides an effective pathway for\nhierarchical visual question answering systems, advancing medical image\nunderstanding.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03135v2",
    "published_date": "2025-04-04 03:03:12 UTC",
    "updated_date": "2025-04-10 11:52:40 UTC"
  },
  {
    "arxiv_id": "2504.03792v1",
    "title": "DP-LET: An Efficient Spatio-Temporal Network Traffic Prediction Framework",
    "authors": [
      "Xintong Wang",
      "Haihan Nan",
      "Ruidong Li",
      "Huaming Wu"
    ],
    "abstract": "Accurately predicting spatio-temporal network traffic is essential for\ndynamically managing computing resources in modern communication systems and\nminimizing energy consumption. Although spatio-temporal traffic prediction has\nreceived extensive research attention, further improvements in prediction\naccuracy and computational efficiency remain necessary. In particular, existing\ndecomposition-based methods or hybrid architectures often incur heavy overhead\nwhen capturing local and global feature correlations, necessitating novel\napproaches that optimize accuracy and complexity. In this paper, we propose an\nefficient spatio-temporal network traffic prediction framework, DP-LET, which\nconsists of a data processing module, a local feature enhancement module, and a\nTransformer-based prediction module. The data processing module is designed for\nhigh-efficiency denoising of network data and spatial decoupling. In contrast,\nthe local feature enhancement module leverages multiple Temporal Convolutional\nNetworks (TCNs) to capture fine-grained local features. Meanwhile, the\nprediction module utilizes a Transformer encoder to model long-term\ndependencies and assess feature relevance. A case study on real-world cellular\ntraffic prediction demonstrates the practicality of DP-LET, which maintains low\ncomputational complexity while achieving state-of-the-art performance,\nsignificantly reducing MSE by 31.8% and MAE by 23.1% compared to baseline\nmodels.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.03792v1",
    "published_date": "2025-04-04 02:52:43 UTC",
    "updated_date": "2025-04-04 02:52:43 UTC"
  },
  {
    "arxiv_id": "2504.03129v1",
    "title": "GraphSeg: Segmented 3D Representations via Graph Edge Addition and Contraction",
    "authors": [
      "Haozhan Tang",
      "Tianyi Zhang",
      "Oliver Kroemer",
      "Matthew Johnson-Roberson",
      "Weiming Zhi"
    ],
    "abstract": "Robots operating in unstructured environments often require accurate and\nconsistent object-level representations. This typically requires segmenting\nindividual objects from the robot's surroundings. While recent large models\nsuch as Segment Anything (SAM) offer strong performance in 2D image\nsegmentation. These advances do not translate directly to performance in the\nphysical 3D world, where they often over-segment objects and fail to produce\nconsistent mask correspondences across views. In this paper, we present\nGraphSeg, a framework for generating consistent 3D object segmentations from a\nsparse set of 2D images of the environment without any depth information.\nGraphSeg adds edges to graphs and constructs dual correspondence graphs: one\nfrom 2D pixel-level similarities and one from inferred 3D structure. We\nformulate segmentation as a problem of edge addition, then subsequent graph\ncontraction, which merges multiple 2D masks into unified object-level\nsegmentations. We can then leverage \\emph{3D foundation models} to produce\nsegmented 3D representations. GraphSeg achieves robust segmentation with\nsignificantly fewer images and greater accuracy than prior methods. We\ndemonstrate state-of-the-art performance on tabletop scenes and show that\nGraphSeg enables improved performance on downstream robotic manipulation tasks.\nCode available at https://github.com/tomtang502/graphseg.git.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03129v1",
    "published_date": "2025-04-04 02:42:45 UTC",
    "updated_date": "2025-04-04 02:42:45 UTC"
  },
  {
    "arxiv_id": "2504.03119v1",
    "title": "Graph Network Modeling Techniques for Visualizing Human Mobility Patterns",
    "authors": [
      "Sinjini Mitra",
      "Anuj Srivastava",
      "Avipsa Roy",
      "Pavan Turaga"
    ],
    "abstract": "Human mobility analysis at urban-scale requires models to represent the\ncomplex nature of human movements, which in turn are affected by accessibility\nto nearby points of interest, underlying socioeconomic factors of a place, and\nlocal transport choices for people living in a geographic region. In this work,\nwe represent human mobility and the associated flow of movements as a grapyh.\nGraph-based approaches for mobility analysis are still in their early stages of\nadoption and are actively being researched. The challenges of graph-based\nmobility analysis are multifaceted - the lack of sufficiently high-quality data\nto represent flows at high spatial and teporal resolution whereas, limited\ncomputational resources to translate large voluments of mobility data into a\nnetwork structure, and scaling issues inherent in graph models etc. The current\nstudy develops a methodology by embedding graphs into a continuous space, which\nalleviates issues related to fast graph matching, graph time-series modeling,\nand visualization of mobility dynamics. Through experiments, we demonstrate how\nmobility data collected from taxicab trajectories could be transformed into\nnetwork structures and patterns of mobility flow changes, and can be used for\ndownstream tasks reporting approx 40% decrease in error on average in matched\ngraphs vs unmatched ones.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03119v1",
    "published_date": "2025-04-04 02:21:44 UTC",
    "updated_date": "2025-04-04 02:21:44 UTC"
  },
  {
    "arxiv_id": "2504.03118v1",
    "title": "NuWa: Deriving Lightweight Task-Specific Vision Transformers for Edge Devices",
    "authors": [
      "Ziteng Wei",
      "Qiang He",
      "Bing Li",
      "Feifei Chen",
      "Yun Yang"
    ],
    "abstract": "Vision Transformers (ViTs) excel in computer vision tasks but lack\nflexibility for edge devices' diverse needs. A vital issue is that ViTs\npre-trained to cover a broad range of tasks are \\textit{over-qualified} for\nedge devices that usually demand only part of a ViT's knowledge for specific\ntasks. Their task-specific accuracy on these edge devices is suboptimal. We\ndiscovered that small ViTs that focus on device-specific tasks can improve\nmodel accuracy and in the meantime, accelerate model inference. This paper\npresents NuWa, an approach that derives small ViTs from the base ViT for edge\ndevices with specific task requirements. NuWa can transfer task-specific\nknowledge extracted from the base ViT into small ViTs that fully leverage\nconstrained resources on edge devices to maximize model accuracy with inference\nlatency assurance. Experiments with three base ViTs on three public datasets\ndemonstrate that compared with state-of-the-art solutions, NuWa improves model\naccuracy by up to $\\text{11.83}\\%$ and accelerates model inference by\n1.29$\\times$ - 2.79$\\times$. Code for reproduction is available at\nhttps://anonymous.4open.science/r/Task_Specific-3A5E.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 12 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.03118v1",
    "published_date": "2025-04-04 02:19:01 UTC",
    "updated_date": "2025-04-04 02:19:01 UTC"
  },
  {
    "arxiv_id": "2504.03108v1",
    "title": "Multi-Granularity Vision Fastformer with Fusion Mechanism for Skin Lesion Segmentation",
    "authors": [
      "Xuanyu Liu",
      "Huiyun Yao",
      "Jinggui Gao",
      "Zhongyi Guo",
      "Xue Zhang",
      "Yulin Dong"
    ],
    "abstract": "Background:Convolutional Neural Networks(CNN) and Vision Transformers(ViT)\nare the main techniques used in Medical image segmentation. However, CNN is\nlimited to local contextual information, and ViT's quadratic complexity results\nin significant computational costs. At the same time, equipping the model to\ndistinguish lesion boundaries with varying degrees of severity is also a\nchallenge encountered in skin lesion segmentation. Purpose:This research aims\nto optimize the balance between computational costs and long-range dependency\nmodelling and achieve excellent generalization across lesions with different\ndegrees of severity. Methods:we propose a lightweight U-shape network that\nutilizes Vision Fastformer with Fusion Mechanism (VFFM-UNet). We inherit the\nadvantages of Fastformer's additive attention mechanism, combining element-wise\nproduct and matrix product for comprehensive feature extraction and channel\nreduction to save computational costs. In order to accurately identify the\nlesion boundaries with varying degrees of severity, we designed Fusion\nMechanism including Multi-Granularity Fusion and Channel Fusion, which can\nprocess the feature maps in the granularity and channel levels to obtain\ndifferent contextual information. Results:Comprehensive experiments on the\nISIC2017, ISIC2018 and PH2 datasets demonstrate that VFFM-UNet outperforms\nexisting state-of-the-art models regarding parameter numbers, computational\ncomplexity and segmentation performance. In short, compared to MISSFormer, our\nmodel achieves superior segmentation performance while reducing parameter and\ncomputation costs by 101x and 15x, respectively. Conclusions:Both quantitative\nand qualitative analyses show that VFFM-UNet sets a new benchmark by reaching\nan ideal balance between parameter numbers, computational complexity, and\nsegmentation performance compared to existing state-of-the-art models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03108v1",
    "published_date": "2025-04-04 01:27:43 UTC",
    "updated_date": "2025-04-04 01:27:43 UTC"
  },
  {
    "arxiv_id": "2504.03093v1",
    "title": "Post-processing for Fair Regression via Explainable SVD",
    "authors": [
      "Zhiqun Zuo",
      "Ding Zhu",
      "Mohammad Mahdi Khalili"
    ],
    "abstract": "This paper presents a post-processing algorithm for training fair neural\nnetwork regression models that satisfy statistical parity, utilizing an\nexplainable singular value decomposition (SVD) of the weight matrix. We propose\na linear transformation of the weight matrix, whereby the singular values\nderived from the SVD of the transformed matrix directly correspond to the\ndifferences in the first and second moments of the output distributions across\ntwo groups. Consequently, we can convert the fairness constraints into\nconstraints on the singular values. We analytically solve the problem of\nfinding the optimal weights under these constraints. Experimental validation on\nvarious datasets demonstrates that our method achieves a similar or superior\nfairness-accuracy trade-off compared to the baselines without using the\nsensitive attribute at the inference time.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03093v1",
    "published_date": "2025-04-04 00:10:01 UTC",
    "updated_date": "2025-04-04 00:10:01 UTC"
  },
  {
    "arxiv_id": "2504.03092v1",
    "title": "Machine Learning-Based Detection and Analysis of Suspicious Activities in Bitcoin Wallet Transactions in the USA",
    "authors": [
      "Md Zahidul Islam",
      "Md Shahidul Islam",
      "Biswajit Chandra das",
      "Syed Ali Reza",
      "Proshanta Kumar Bhowmik",
      "Kanchon Kumar Bishnu",
      "Md Shafiqur Rahman",
      "Redoyan Chowdhury",
      "Laxmi Pant"
    ],
    "abstract": "The dramatic adoption of Bitcoin and other cryptocurrencies in the USA has\nrevolutionized the financial landscape and provided unprecedented investment\nand transaction efficiency opportunities. The prime objective of this research\nproject is to develop machine learning algorithms capable of effectively\nidentifying and tracking suspicious activity in Bitcoin wallet transactions.\nWith high-tech analysis, the study aims to create a model with a feature for\nidentifying trends and outliers that can expose illicit activity. The current\nstudy specifically focuses on Bitcoin transaction information in America, with\na strong emphasis placed on the importance of knowing about the immediate\nenvironment in and through which such transactions pass through. The dataset is\ncomposed of in-depth Bitcoin wallet transactional information, including\nimportant factors such as transaction values, timestamps, network flows, and\naddresses for wallets. All entries in the dataset expose information about\nfinancial transactions between wallets, including received and sent\ntransactions, and such information is significant for analysis and trends that\ncan represent suspicious activity. This study deployed three accredited\nalgorithms, most notably, Logistic Regression, Random Forest, and Support\nVector Machines. In retrospect, Random Forest emerged as the best model with\nthe highest F1 Score, showcasing its ability to handle non-linear relationships\nin the data. Insights revealed significant patterns in wallet activity, such as\nthe correlation between unredeemed transactions and final balances. The\napplication of machine algorithms in tracking cryptocurrencies is a tool for\ncreating transparent and secure U.S. markets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages,7 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.03092v1",
    "published_date": "2025-04-04 00:07:32 UTC",
    "updated_date": "2025-04-04 00:07:32 UTC"
  },
  {
    "arxiv_id": "2504.13884v1",
    "title": "Towards a Multimodal Document-grounded Conversational AI System for Education",
    "authors": [
      "Karan Taneja",
      "Anjali Singh",
      "Ashok K. Goel"
    ],
    "abstract": "Multimedia learning using text and images has been shown to improve learning\noutcomes compared to text-only instruction. But conversational AI systems in\neducation predominantly rely on text-based interactions while multimodal\nconversations for multimedia learning remain unexplored. Moreover, deploying\nconversational AI in learning contexts requires grounding in reliable sources\nand verifiability to create trust. We present MuDoC, a Multimodal\nDocument-grounded Conversational AI system based on GPT-4o, that leverages both\ntext and visuals from documents to generate responses interleaved with text and\nimages. Its interface allows verification of AI generated content through\nseamless navigation to the source. We compare MuDoC to a text-only system to\nexplore differences in learner engagement, trust in AI system, and their\nperformance on problem-solving tasks. Our findings indicate that both visuals\nand verifiability of content enhance learner engagement and foster trust;\nhowever, no significant impact in performance was observed. We draw upon\ntheories from cognitive and learning sciences to interpret the findings and\nderive implications, and outline future directions for the development of\nmultimodal conversational AI systems in education.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.HC",
    "comment": "15 pages, 4 figures, AIED 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.13884v1",
    "published_date": "2025-04-04 00:04:19 UTC",
    "updated_date": "2025-04-04 00:04:19 UTC"
  }
]
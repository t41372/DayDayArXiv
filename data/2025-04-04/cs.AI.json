{
  "date": "2025-04-04",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-04-04 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文主要聚焦于 AI 代理、LLM 优化、医疗图像处理和强化学习等领域，其中 LLM 在多模态任务和代理系统中的应用（如第 2 篇）令人印象深刻，并有知名学者如 Katia P. Sycara 参与；此外，医疗 AI 的 3D 图像分割（如第 39 篇）也展现出实际应用潜力。\n\n下面，我将挑选并简要讨论部分关键论文，先从 AI 代理和 LLM 相关主题入手，这些文章更具话题性和影响力；接着聊医疗 AI 领域；最后快速掠过其他不那么核心的论文。每个条目列出论文标题（中文 + 英文），并突出主要贡献和发现。\n\n### AI 代理和 LLM 相关\n- **第 2 篇：算法提示生成用于多样化的人类-like 团队协作和通信的大语言模型 (Algorithmic Prompt Generation for Diverse Human-like Teaming and Communication with Large Language Models)**  \n  作者包括知名学者 Katia P. Sycara，该论文提出结合 Quality Diversity 优化和 LLM 来生成多样化提示，实现多代理协作环境中的人类-like 行为，实验显示其能有效模拟团队策略，提升 AI 在社交场景的适用性。\n\n- **第 38 篇：APIGen-MT：通过模拟代理-人类互动的多轮数据生成代理管道 (APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay)**  \n  这篇论文引入代理管道生成多轮交互数据，用于训练 AI 代理，显著提升代理在多轮任务中的性能，实验在教育和健身场景中验证了其有效性。\n\n- **第 55 篇：AI 宇宙学家 I：用于自动化数据分析的代理系统 (The AI Cosmologist I: An Agentic System for Automated Data Analysis)**  \n  论文开发了一个端到端代理系统，能从数据分析到研究输出全自动化，基于强化学习，实验证明其在宇宙学任务中高效且可扩展。\n\n- **第 58 篇：针对推理导向强化学习的在线难度过滤 (Online Difficulty Filtering for Reasoning Oriented Reinforcement Learning)**  \n  贡献了难度过滤机制来优化强化学习训练，理论分析显示其能最大化策略性能，实验在数学推理基准上提升了 10% 的准确率。\n\n- **第 82 篇：自适应思维链学习 (Self-Adaptive Chain-of-Thought Learning)**  \n  提出一种自适应思维链方法，能根据问题复杂度动态调整推理过程，实验显示其在推理任务中保持准确性同时减少冗余计算。\n\n- **第 92 篇：蒙特卡洛图着色 (Monte Carlo Graph Coloring)**  \n  探索蒙特卡洛搜索在图着色问题上的应用，算法高效处理复杂图结构，显著优于传统启发式方法。\n\n- **第 93 篇：AI 代理的知识自适应 (Agentic Knowledgeable Self-awareness)**  \n  设计了知识自适应框架，让 AI 代理像人类一样根据情境动态使用知识，实验证明其在规划任务中提升效率。\n\n其他 LLM 相关论文（如第 4、6、7、17、24、27、31、94、95 篇）主要优化提示或多模态融合，但贡献相对常规，我快速掠过：它们探讨了 LLM 在解释性 AI、提示优化和多任务中的改进，如第 6 篇的 GREATERPROMPT 框架提供统一 API，提升了任务适应性。\n\n### 医疗 AI 和图像处理\n- **第 39 篇：MedSAM2：在 3D 医疗图像和视频中分割所有内容 (MedSAM2: Segment Anything in 3D Medical Images and Videos)**  \n  这篇论文开发了 MedSAM2 框架，能从单目视频实时生成 3D 图像分割，用户实验显示其减少手动标注 85%，在医疗图像处理中表现出色。\n\n- **第 48 篇：利用步态模式作为生物标记的脊柱侧弯分类 (Leveraging Gait Patterns as Biomarkers: An attention-guided Deep Multiple Instance Learning Network for Scoliosis Classification)**  \n  提出 Gait-MIL 网络，使用注意力机制分析步态数据，准确检测脊柱侧弯，尤其在复杂病例中提升了检测精度。\n\n- **第 79 篇：RF-BayesPhysNet：复杂场景下基于贝叶斯的不确定性估计 rPPG 方法 (RF-BayesPhysNet: A Bayesian rPPG Uncertainty Estimation Method for Complex Scenarios)**  \n  创新性地将贝叶斯网络应用于远程光电容积描记 (rPPG)，实验在 UBFC-RPPG 数据集上达到 MAE 2.56，提升了心率测量鲁棒性。\n\n其他医疗论文（如第 9、22、50、83、98 篇）聚焦生物医学文本或图像处理，我简要提到：第 9 篇的 Clinical ModernBERT 改进了长上下文生物文本编码，提升了临床 NLP 性能。\n\n### 其他领域快速掠过\n其余论文覆盖强化学习、图神经网络、区块链等，但影响力较小，我仅快速总结几篇：  \n- **第 1 篇：使用强化学习改进混合关键性调度 (Improving Mixed-Criticality Scheduling with Reinforcement Learning)**  \n  开发 RL 代理优化实时系统调度，实验显示任务完成率达 85%。  \n- **第 105 篇：基于机器学习的比特币钱包交易可疑活动检测 (Machine Learning-Based Detection and Analysis of Suspicious Activities in Bitcoin Wallet Transactions in the USA)**  \n  使用 Logistic Regression 等算法检测交易异常，F1 分数达 78.3%，提升了区块链安全。  \n其他如第 11、15、25、46、73 篇等，涉及硬件优化或图学习，但未有突破性发现，故不详细展开。\n\n总之，今天的论文突显 AI 在代理和医疗领域的潜力，建议关注 LLM 和强化学习的创新应用，以推动实际部署。更多细节可查阅 arXiv！",
  "papers": [
    {
      "arxiv_id": "2504.03994v2",
      "title": "Improving Mixed-Criticality Scheduling with Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Muhammad El-Mahdy",
        "Nourhan Sakr",
        "Rodrigo Carrasco"
      ],
      "abstract": "This paper introduces a novel reinforcement learning (RL) approach to\nscheduling mixed-criticality (MC) systems on processors with varying speeds.\nBuilding upon the foundation laid by [1], we extend their work to address the\nnon-preemptive scheduling problem, which is known to be NP-hard. By modeling\nthis scheduling challenge as a Markov Decision Process (MDP), we develop an RL\nagent capable of generating near-optimal schedules for real-time MC systems.\nOur RL-based scheduler prioritizes high-critical tasks while maintaining\noverall system performance.\n  Through extensive experiments, we demonstrate the scalability and\neffectiveness of our approach. The RL scheduler significantly improves task\ncompletion rates, achieving around 80% overall and 85% for high-criticality\ntasks across 100,000 instances of synthetic data and real data under varying\nsystem conditions. Moreover, under stable conditions without degradation, the\nscheduler achieves 94% overall task completion and 93% for high-criticality\ntasks. These results highlight the potential of RL-based schedulers in\nreal-time and safety-critical applications, offering substantial improvements\nin handling complex and dynamic scheduling scenarios.",
      "tldr_zh": "本论文提出了一种基于Reinforcement Learning (RL)的创新方法，用于改进Mixed-Criticality (MC)系统的非抢占调度问题，该方法扩展了[1]的研究，并通过将调度建模为Markov Decision Process (MDP)来开发RL代理，实现近优调度并优先处理高关键性任务。实验在10万余个合成和真实数据实例上验证了该方法的有效性，RL调度器在各种系统条件下将整体任务完成率提高至约80%，高关键性任务达85%，而在稳定环境下分别达到94%和93%。这一结果突显了RL在实时和安全关键应用中的潜力，提供了一种处理复杂动态调度的可靠解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "This work was submitted to the 32nd International Conference on\n  Real-Time Networks and Systems (RTNS) on June 8, 2024",
      "pdf_url": "http://arxiv.org/pdf/2504.03994v2",
      "published_date": "2025-04-04 23:28:48 UTC",
      "updated_date": "2025-04-08 13:22:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:28:25.411013"
    },
    {
      "arxiv_id": "2504.03991v1",
      "title": "Algorithmic Prompt Generation for Diverse Human-like Teaming and Communication with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Siddharth Srikanth",
        "Varun Bhatt",
        "Boshen Zhang",
        "Werner Hager",
        "Charles Michael Lewis",
        "Katia P. Sycara",
        "Aaquib Tabrez",
        "Stefanos Nikolaidis"
      ],
      "abstract": "Understanding how humans collaborate and communicate in teams is essential\nfor improving human-agent teaming and AI-assisted decision-making. However,\nrelying solely on data from large-scale user studies is impractical due to\nlogistical, ethical, and practical constraints, necessitating synthetic models\nof multiple diverse human behaviors. Recently, agents powered by Large Language\nModels (LLMs) have been shown to emulate human-like behavior in social\nsettings. But, obtaining a large set of diverse behaviors requires manual\neffort in the form of designing prompts. On the other hand, Quality Diversity\n(QD) optimization has been shown to be capable of generating diverse\nReinforcement Learning (RL) agent behavior. In this work, we combine QD\noptimization with LLM-powered agents to iteratively search for prompts that\ngenerate diverse team behavior in a long-horizon, multi-step collaborative\nenvironment. We first show, through a human-subjects experiment (n=54\nparticipants), that humans exhibit diverse coordination and communication\nbehavior in this domain. We then show that our approach can effectively\nreplicate trends from human teaming data and also capture behaviors that are\nnot easily observed without collecting large amounts of data. Our findings\nhighlight the combination of QD and LLM-powered agents as an effective tool for\nstudying teaming and communication strategies in multi-agent collaboration.",
      "tldr_zh": "该研究旨在模拟人类团队协作和沟通行为，以提升人类-代理团队和AI辅助决策，但受限于数据收集的限制，转而使用合成模型。论文提出一种结合Quality Diversity (QD)优化和Large Language Models (LLMs)驱动代理的方法，通过迭代搜索提示生成多样化的团队行为，在长期多步骤协作环境中进行测试。通过人类实验（n=54），证实人类表现出多样协调和沟通趋势，该方法能有效复制这些趋势并捕捉不易观察的行为。该方法为研究多代理协作中的团队策略和沟通提供了高效工具。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03991v1",
      "published_date": "2025-04-04 23:09:40 UTC",
      "updated_date": "2025-04-04 23:09:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:28:37.829075"
    },
    {
      "arxiv_id": "2504.03989v3",
      "title": "CORTEX-AVD: A Framework for CORner Case Testing and EXploration in Autonomous Vehicle Development",
      "title_zh": "翻译失败",
      "authors": [
        "Gabriel Kenji Godoy Shimanuki",
        "Alexandre Moreira Nascimento",
        "Lucio Flavio Vismari",
        "Joao Batista Camargo Junior",
        "Jorge Rady de Almeida Junior",
        "Paulo Sergio Cugnasca"
      ],
      "abstract": "Autonomous Vehicles (AVs) aim to improve traffic safety and efficiency by\nreducing human error. However, ensuring AVs reliability and safety is a\nchallenging task when rare, high-risk traffic scenarios are considered. These\n'Corner Cases' (CC) scenarios, such as unexpected vehicle maneuvers or sudden\npedestrian crossings, must be safely and reliable dealt by AVs during their\noperations. But they arehard to be efficiently generated. Traditional CC\ngeneration relies on costly and risky real-world data acquisition, limiting\nscalability, and slowing research and development progress. Simulation-based\ntechniques also face challenges, as modeling diverse scenarios and capturing\nall possible CCs is complex and time-consuming. To address these limitations in\nCC generation, this research introduces CORTEX-AVD, CORner Case Testing &\nEXploration for Autonomous Vehicles Development, an open-source framework that\nintegrates the CARLA Simulator and Scenic to automatically generate CC from\ntextual descriptions, increasing the diversity and automation of scenario\nmodeling. Genetic Algorithms (GA) are used to optimize the scenario parameters\nin six case study scenarios, increasing the occurrence of high-risk events.\nUnlike previous methods, CORTEX-AVD incorporates a multi-factor fitness\nfunction that considers variables such as distance, time, speed, and collision\nlikelihood. Additionally, the study provides a benchmark for comparing GA-based\nCC generation methods, contributing to a more standardized evaluation of\nsynthetic data generation and scenario assessment. Experimental results\ndemonstrate that the CORTEX-AVD framework significantly increases CC incidence\nwhile reducing the proportion of wasted simulations.",
      "tldr_zh": "这篇论文介绍了 CORTEX-AVD 框架，用于自动驾驶车辆（Autonomous Vehicles）开发中的 Corner Cases 测试和探索，旨在解决稀有高风险场景（如突发行人穿越或意外车辆 maneuvers）生成效率低下的问题。框架整合 CARLA Simulator 和 Scenic，从文本描述自动生成多样化的场景，并使用 Genetic Algorithms (GA) 优化参数，包括多因素适应度函数（如距离、时间、速度和碰撞可能性），从而增加高风险事件的发生率。实验结果显示，CORTEX-AVD 显著提高了 Corner Cases 发生率，同时减少了无效模拟的比例，并提供了 GA-based 生成方法的基准，促进标准化评估。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "10 pages, 10 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.03989v3",
      "published_date": "2025-04-04 23:05:31 UTC",
      "updated_date": "2025-04-09 20:04:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:28:49.223819"
    },
    {
      "arxiv_id": "2504.03978v1",
      "title": "V-CEM: Bridging Performance and Intervenability in Concept-based Models",
      "title_zh": "V-CEM：桥接基于概念模型中的性能与可干预性",
      "authors": [
        "Francesco De Santis",
        "Gabriele Ciravegna",
        "Philippe Bich",
        "Danilo Giordano",
        "Tania Cerquitelli"
      ],
      "abstract": "Concept-based eXplainable AI (C-XAI) is a rapidly growing research field that\nenhances AI model interpretability by leveraging intermediate,\nhuman-understandable concepts. This approach not only enhances model\ntransparency but also enables human intervention, allowing users to interact\nwith these concepts to refine and improve the model's performance. Concept\nBottleneck Models (CBMs) explicitly predict concepts before making final\ndecisions, enabling interventions to correct misclassified concepts. While CBMs\nremain effective in Out-Of-Distribution (OOD) settings with intervention, they\nstruggle to match the performance of black-box models. Concept Embedding Models\n(CEMs) address this by learning concept embeddings from both concept\npredictions and input data, enhancing In-Distribution (ID) accuracy but\nreducing the effectiveness of interventions, especially in OOD scenarios. In\nthis work, we propose the Variational Concept Embedding Model (V-CEM), which\nleverages variational inference to improve intervention responsiveness in CEMs.\nWe evaluated our model on various textual and visual datasets in terms of ID\nperformance, intervention responsiveness in both ID and OOD settings, and\nConcept Representation Cohesiveness (CRC), a metric we propose to assess the\nquality of the concept embedding representations. The results demonstrate that\nV-CEM retains CEM-level ID performance while achieving intervention\neffectiveness similar to CBM in OOD settings, effectively reducing the gap\nbetween interpretability (intervention) and generalization (performance).",
      "tldr_zh": "这篇论文针对 Concept-based eXplainable AI (C-XAI) 中的性能与干预性权衡问题，提出了 Variational Concept Embedding Model (V-CEM)，它利用变分推理来提升 Concept Embedding Models (CEMs) 的干预响应能力，同时保留其 In-Distribution (ID) 准确性。V-CEM 解决了 CEMs 在 Out-Of-Distribution (OOD) 场景下干预效果较差的局限，并引入了新的 Concept Representation Cohesiveness (CRC) 指标来评估概念嵌入质量。实验在多种文本和视觉数据集上显示，V-CEM 不仅维持了 CEMs 水平的 ID 性能，还在 ID 和 OOD 设置中实现了与 Concept Bottleneck Models (CBMs) 相当的干预效果，从而有效缩小了模型可解释性和泛化性能之间的差距。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Paper accepted at: The 3rd World Conference on Explainable Artificial\n  Intelligence",
      "pdf_url": "http://arxiv.org/pdf/2504.03978v1",
      "published_date": "2025-04-04 22:43:04 UTC",
      "updated_date": "2025-04-04 22:43:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:29:01.528915"
    },
    {
      "arxiv_id": "2504.03976v2",
      "title": "OLAF: An Open Life Science Analysis Framework for Conversational Bioinformatics Powered by Large Language Models",
      "title_zh": "OLAF：基于大语言模型的开放生命科学分析框架",
      "authors": [
        "Dylan Riffle",
        "Nima Shirooni",
        "Cody He",
        "Manush Murali",
        "Sovit Nayak",
        "Rishikumar Gopalan",
        "Diego Gonzalez Lopez"
      ],
      "abstract": "OLAF (Open Life Science Analysis Framework) is an open-source platform that\nenables researchers to perform bioinformatics analyses using natural language.\nBy combining large language models (LLMs) with a modular agent-pipe-router\narchitecture, OLAF generates and executes bioinformatics code on real\nscientific data, including formats like .h5ad. The system includes an Angular\nfront end and a Python/Firebase backend, allowing users to run analyses such as\nsingle-cell RNA-seq workflows, gene annotation, and data visualization through\na simple web interface. Unlike general-purpose AI tools, OLAF integrates code\nexecution, data handling, and scientific libraries in a reproducible,\nuser-friendly environment. It is designed to lower the barrier to computational\nbiology for non-programmers and support transparent, AI-powered life science\nresearch.",
      "tldr_zh": "OLAF 是一个开源平台，使用 Large Language Models (LLMs) 结合模块化 agent-pipe-router 架构，允许研究人员通过自然语言进行生物信息学分析。系统能生成并执行代码，处理真实科学数据（如 .h5ad 格式），并支持单细胞 RNA-seq 工作流、基因注释和数据可视化，通过 Angular 前端和 Python/Firebase 后端提供简单网页界面。不同于通用 AI 工具，OLAF 整合代码执行、数据处理和科学库，创建可重现、用户友好的环境，旨在降低非程序员进入计算生物学的门槛，并推动透明的 AI 驱动生命科学研究。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "q-bio.GN"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03976v2",
      "published_date": "2025-04-04 22:41:16 UTC",
      "updated_date": "2025-04-10 19:32:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:29:13.104504"
    },
    {
      "arxiv_id": "2504.03975v1",
      "title": "GREATERPROMPT: A Unified, Customizable, and High-Performing Open-Source Toolkit for Prompt Optimization",
      "title_zh": "GREATERPROMPT：一个统一、可定制且高性能的开源提示优化工具包",
      "authors": [
        "Wenliang Zheng",
        "Sarkar Snigdha Sarathi Das",
        "Yusen Zhang",
        "Rui Zhang"
      ],
      "abstract": "LLMs have gained immense popularity among researchers and the general public\nfor its impressive capabilities on a variety of tasks. Notably, the efficacy of\nLLMs remains significantly dependent on the quality and structure of the input\nprompts, making prompt design a critical factor for their performance. Recent\nadvancements in automated prompt optimization have introduced diverse\ntechniques that automatically enhance prompts to better align model outputs\nwith user expectations. However, these methods often suffer from the lack of\nstandardization and compatibility across different techniques, limited\nflexibility in customization, inconsistent performance across model scales, and\nthey often exclusively rely on expensive proprietary LLM APIs. To fill in this\ngap, we introduce GREATERPROMPT, a novel framework that democratizes prompt\noptimization by unifying diverse methods under a unified, customizable API\nwhile delivering highly effective prompts for different tasks. Our framework\nflexibly accommodates various model scales by leveraging both text\nfeedback-based optimization for larger LLMs and internal gradient-based\noptimization for smaller models to achieve powerful and precise prompt\nimprovements. Moreover, we provide a user-friendly Web UI that ensures\naccessibility for non-expert users, enabling broader adoption and enhanced\nperformance across various user groups and application scenarios. GREATERPROMPT\nis available at https://github.com/psunlpgroup/GreaterPrompt via GitHub, PyPI,\nand web user interfaces.",
      "tldr_zh": "论文介绍了 GREATERPROMPT，一种统一的、可定制且高性能的开源工具包，旨在优化提示(prompt)以提升 LLMs 在各种任务中的表现，解决了现有方法的标准化、兼容性和灵活性不足问题。该框架通过一个统一的 API 整合多种优化技术，支持不同模型规模：对大型 LLMs 使用基于文本反馈的优化，对小型模型采用内部梯度-based 优化，从而实现精确有效的提示改进。此外，GREATERPROMPT 提供用户友好的 Web UI，便于非专家用户使用，并已开源在 GitHub 和 PyPI，促进更广泛的应用和性能提升。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03975v1",
      "published_date": "2025-04-04 22:36:55 UTC",
      "updated_date": "2025-04-04 22:36:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:29:24.414151"
    },
    {
      "arxiv_id": "2504.03970v2",
      "title": "VideoComp: Advancing Fine-Grained Compositional and Temporal Alignment in Video-Text Models",
      "title_zh": "VideoComp：推进视频-文本模型中的细粒度组合性和时间对齐",
      "authors": [
        "Dahun Kim",
        "AJ Piergiovanni",
        "Ganesh Mallya",
        "Anelia Angelova"
      ],
      "abstract": "We introduce VideoComp, a benchmark and learning framework for advancing\nvideo-text compositionality understanding, aimed at improving vision-language\nmodels (VLMs) in fine-grained temporal alignment. Unlike existing benchmarks\nfocused on static image-text compositionality or isolated single-event videos,\nour benchmark targets alignment in continuous multi-event videos. Leveraging\nvideo-text datasets with temporally localized event captions (e.g.\nActivityNet-Captions, YouCook2), we construct two compositional benchmarks,\nActivityNet-Comp and YouCook2-Comp. We create challenging negative samples with\nsubtle temporal disruptions such as reordering, action word replacement,\npartial captioning, and combined disruptions. These benchmarks comprehensively\ntest models' compositional sensitivity across extended, cohesive video-text\nsequences. To improve model performance, we propose a hierarchical pairwise\npreference loss that strengthens alignment with temporally accurate pairs and\ngradually penalizes increasingly disrupted ones, encouraging fine-grained\ncompositional learning. To mitigate the limited availability of densely\nannotated video data, we introduce a pretraining strategy that concatenates\nshort video-caption pairs to simulate multi-event sequences. We evaluate\nvideo-text foundational models and large multimodal models (LMMs) on our\nbenchmark, identifying both strengths and areas for improvement in\ncompositionality. Overall, our work provides a comprehensive framework for\nevaluating and enhancing model capabilities in achieving fine-grained,\ntemporally coherent video-text alignment.",
      "tldr_zh": "本文提出 VideoComp 基准和学习框架，旨在提升视觉语言模型(VLMs)在视频-文本中的细粒度组合性和时间对齐能力，特别是针对连续多事件视频。研究者利用数据集如 ActivityNet-Captions 和 YouCook2 构建了 ActivityNet-Comp 和 YouCook2-Comp 基准，并创建了挑战性负样本（如时间顺序打乱、动作词替换和部分标题），以全面测试模型在扩展视频序列中的组合敏感性。为改进性能，他们引入分层配对偏好损失和一个预训练策略，通过连接短视频-标题对模拟多事件序列，并评估大型多模态模型(LMMs)的优势和不足，提供了一个全面框架来增强细粒度、时间连贯的视频-文本对齐。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025, project page at\n  https://github.com/google-deepmind/video_comp",
      "pdf_url": "http://arxiv.org/pdf/2504.03970v2",
      "published_date": "2025-04-04 22:24:30 UTC",
      "updated_date": "2025-04-10 10:41:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:29:38.529027"
    },
    {
      "arxiv_id": "2504.03966v1",
      "title": "Bridging LMS and Generative AI: Dynamic Course Content Integration (DCCI) for Connecting LLMs to Course Content -- The Ask ME Assistant",
      "title_zh": "翻译失败",
      "authors": [
        "Kovan Mzwri",
        "Márta Turcsányi-Szabo"
      ],
      "abstract": "The integration of Large Language Models (LLMs) with Learning Management\nSystems (LMSs) has the potential to enhance task automation and accessibility\nin education. However, hallucination where LLMs generate inaccurate or\nmisleading information remains a significant challenge. This study introduces\nthe Dynamic Course Content Integration (DCCI) mechanism, which dynamically\nretrieves and integrates course content and curriculum from Canvas LMS into the\nLLM-powered assistant, Ask ME. By employing prompt engineering to structure\nretrieved content within the LLM's context window, DCCI ensures accuracy,\nrelevance, and contextual alignment, mitigating hallucination. To evaluate\nDCCI's effectiveness, Ask ME's usability, and broader student perceptions of AI\nin education, a mixed-methods approach was employed, incorporating user\nsatisfaction ratings and a structured survey. Results from a pilot study\nindicate high user satisfaction (4.614/5), with students recognizing Ask ME's\nability to provide timely and contextually relevant responses for both\nadministrative and course-related inquiries. Additionally, a majority of\nstudents agreed that Ask ME's integration with course content in Canvas LMS\nreduced platform-switching, improving usability, engagement, and comprehension.\nAI's role in reducing classroom hesitation and fostering self-directed learning\nand intellectual curiosity was also highlighted. Despite these benefits and\npositive perception of AI tools, concerns emerged regarding over-reliance on\nAI, accuracy limitations, and ethical issues such as plagiarism and reduced\nstudent-teacher interaction. These findings emphasize the need for strategic AI\nimplementation, ethical safeguards, and a pedagogical framework that\nprioritizes human-AI collaboration over substitution.",
      "tldr_zh": "本研究探讨了将 Large Language Models (LLMs) 与 Learning Management Systems (LMSs) 整合，以提升教育中的任务自动化和可访问性，同时解决 LLMs 的 hallucination 问题。研究引入 Dynamic Course Content Integration (DCCI) 机制，通过动态检索并整合 Canvas LMS 中的课程内容到 LLM 驱动的助手 Ask ME，并利用 prompt engineering 确保响应的准确性和上下文相关性。试点实验显示，用户满意度高达 4.614/5，学生认为 Ask ME 提高了查询效率、减少了平台切换，并增强了参与度和理解力，同时促进自学和好奇心。尽管如此，研究也强调了过度依赖 AI、准确性限制以及伦理问题（如剽窃和减少师生互动）的潜在风险，呼吁战略性 AI 实施和以人为本的框架。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.ET",
        "cs.HC",
        "cs.SE"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03966v1",
      "published_date": "2025-04-04 22:17:30 UTC",
      "updated_date": "2025-04-04 22:17:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:29:49.776197"
    },
    {
      "arxiv_id": "2504.03964v1",
      "title": "Clinical ModernBERT: An efficient and long context encoder for biomedical text",
      "title_zh": "翻译失败",
      "authors": [
        "Simon A. Lee",
        "Anthony Wu",
        "Jeffrey N. Chiang"
      ],
      "abstract": "We introduce Clinical ModernBERT, a transformer based encoder pretrained on\nlarge scale biomedical literature, clinical notes, and medical ontologies,\nincorporating PubMed abstracts, MIMIC IV clinical data, and medical codes with\ntheir textual descriptions. Building on ModernBERT the current state of the art\nnatural language text encoder featuring architectural upgrades such as rotary\npositional embeddings (RoPE), Flash Attention, and extended context length up\nto 8,192 tokens our model adapts these innovations specifically for biomedical\nand clinical domains. Clinical ModernBERT excels at producing semantically rich\nrepresentations tailored for long context tasks. We validate this both by\nanalyzing its pretrained weights and through empirical evaluation on a\ncomprehensive suite of clinical NLP benchmarks.",
      "tldr_zh": "本研究引入了 Clinical ModernBERT，一种基于 Transformer 的高效编码器，针对生物医学文本设计，通过预训练于大规模数据集（如 PubMed 摘要、MIMIC IV 临床数据和医疗代码及其描述）来提升性能。模型继承了 ModernBERT 的架构升级，包括旋转位置嵌入 (RoPE)、Flash Attention 和扩展上下文长度至 8,192 tokens，特别适应生物医学和临床领域的长上下文任务。实验结果显示，Clinical ModernBERT 能够产生语义丰富的表示，并在临床 NLP 基准测试中得到实证验证。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Manuscript writeup corresponding to the Clinical ModernBERT\n  pre-trained encoder (https://huggingface.co/Simonlee711/Clinical_ModernBERT)",
      "pdf_url": "http://arxiv.org/pdf/2504.03964v1",
      "published_date": "2025-04-04 22:14:12 UTC",
      "updated_date": "2025-04-04 22:14:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:30:00.779989"
    },
    {
      "arxiv_id": "2504.03961v1",
      "title": "Optimizing UAV Aerial Base Station Flights Using DRL-based Proximal Policy Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Mario Rico Ibanez",
        "Azim Akhtarshenas",
        "David Lopez-Perez",
        "Giovanni Geraci"
      ],
      "abstract": "Unmanned aerial vehicle (UAV)-based base stations offer a promising solution\nin emergencies where the rapid deployment of cutting-edge networks is crucial\nfor maximizing life-saving potential. Optimizing the strategic positioning of\nthese UAVs is essential for enhancing communication efficiency. This paper\nintroduces an automated reinforcement learning approach that enables UAVs to\ndynamically interact with their environment and determine optimal\nconfigurations. By leveraging the radio signal sensing capabilities of\ncommunication networks, our method provides a more realistic perspective,\nutilizing state-of-the-art algorithm -- proximal policy optimization -- to\nlearn and generalize positioning strategies across diverse user equipment (UE)\nmovement patterns. We evaluate our approach across various UE mobility\nscenarios, including static, random, linear, circular, and mixed hotspot\nmovements. The numerical results demonstrate the algorithm's adaptability and\neffectiveness in maintaining comprehensive coverage across all movement\npatterns.",
      "tldr_zh": "这篇论文提出了一种基于深度强化学习(DRL)的近端策略优化(Proximal Policy Optimization)方法，用于优化UAV（无人机）空中基站的飞行路径，以提升紧急情况下通信效率。方法利用通信网络的无线信号感知能力，让UAV动态互动环境并学习适用于多种用户设备(UE)运动模式的定位策略，包括静态、随机、线性、圆形和混合热点场景。实验结果显示，该算法在所有测试场景中表现出色，能够有效维持全面覆盖并提升整体适应性。",
      "categories": [
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03961v1",
      "published_date": "2025-04-04 22:06:01 UTC",
      "updated_date": "2025-04-04 22:06:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:30:13.639964"
    },
    {
      "arxiv_id": "2504.03955v1",
      "title": "DeepOHeat-v1: Efficient Operator Learning for Fast and Trustworthy Thermal Simulation and Optimization in 3D-IC Design",
      "title_zh": "翻译失败",
      "authors": [
        "Xinling Yu",
        "Ziyue Liu",
        "Hai Li",
        "Yixing Li",
        "Xin Ai",
        "Zhiyu Zeng",
        "Ian Young",
        "Zheng Zhang"
      ],
      "abstract": "Thermal analysis is crucial in three-dimensional integrated circuit (3D-IC)\ndesign due to increased power density and complex heat dissipation paths.\nAlthough operator learning frameworks such as DeepOHeat have demonstrated\npromising preliminary results in accelerating thermal simulation, they face\ncritical limitations in prediction capability for multi-scale thermal patterns,\ntraining efficiency, and trustworthiness of results during design optimization.\nThis paper presents DeepOHeat-v1, an enhanced physics-informed operator\nlearning framework that addresses these challenges through three key\ninnovations. First, we integrate Kolmogorov-Arnold Networks with learnable\nactivation functions as trunk networks, enabling an adaptive representation of\nmulti-scale thermal patterns. This approach achieves a $1.25\\times$ and\n$6.29\\times$ reduction in error in two representative test cases. Second, we\nintroduce a separable training method that decomposes the basis function along\nthe coordinate axes, achieving $62\\times$ training speedup and $31\\times$ GPU\nmemory reduction in our baseline case, and enabling thermal analysis at\nresolutions previously infeasible due to GPU memory constraints. Third, we\npropose a confidence score to evaluate the trustworthiness of the predicted\nresults, and further develop a hybrid optimization workflow that combines\noperator learning with finite difference (FD) using Generalized Minimal\nResidual (GMRES) method for incremental solution refinement, enabling efficient\nand trustworthy thermal optimization. Experimental results demonstrate that\nDeepOHeat-v1 achieves accuracy comparable to optimization using high-fidelity\nfinite difference solvers, while speeding up the entire optimization process by\n$70.6\\times$ in our test cases, effectively minimizing the peak temperature\nthrough optimal placement of heat-generating components.",
      "tldr_zh": "该论文提出DeepOHeat-v1，一种增强的物理信息驱动操作符学习框架，用于解决3D-IC设计中热模拟和优化的挑战，包括多尺度热模式预测、训练效率和结果可信度问题。框架的关键创新包括：整合Kolmogorov-Arnold Networks（KAN）与可学习激活函数来适应多尺度热模式，实现错误减少1.25×和6.29×；引入可分离训练方法，将基函数沿坐标轴分解，带来62×训练速度提升和31×GPU内存减少；以及提出置信度分数和混合优化工作流，结合Finite Difference (FD)和Generalized Minimal Residual (GMRES)方法进行增量精炼。实验结果显示，DeepOHeat-v1在准确性上与高保真FD求解器相当，同时将优化过程加速70.6×，并通过优化热产生组件的位置有效最小化峰值温度。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.data-an"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 14 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.03955v1",
      "published_date": "2025-04-04 21:39:42 UTC",
      "updated_date": "2025-04-04 21:39:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:30:26.136134"
    },
    {
      "arxiv_id": "2504.03953v1",
      "title": "TGraphX: Tensor-Aware Graph Neural Network for Multi-Dimensional Feature Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Arash Sajjadi",
        "Mark Eramian"
      ],
      "abstract": "TGraphX presents a novel paradigm in deep learning by unifying convolutional\nneural networks (CNNs) with graph neural networks (GNNs) to enhance visual\nreasoning tasks. Traditional CNNs excel at extracting rich spatial features\nfrom images but lack the inherent capability to model inter-object\nrelationships. Conversely, conventional GNNs typically rely on flattened node\nfeatures, thereby discarding vital spatial details. TGraphX overcomes these\nlimitations by employing CNNs to generate multi-dimensional node features\n(e.g., (3*128*128) tensors) that preserve local spatial semantics. These\nspatially aware nodes participate in a graph where message passing is performed\nusing 1*1 convolutions, which fuse adjacent features while maintaining their\nstructure. Furthermore, a deep CNN aggregator with residual connections is used\nto robustly refine the fused messages, ensuring stable gradient flow and\nend-to-end trainability. Our approach not only bridges the gap between spatial\nfeature extraction and relational reasoning but also demonstrates significant\nimprovements in object detection refinement and ensemble reasoning.",
      "tldr_zh": "该论文提出TGraphX，一种结合卷积神经网络(CNNs)和图神经网络(GNNs)的框架，用于增强视觉推理任务，通过生成多维节点特征（如(3*128*128)张量）来保留局部空间语义。TGraphX采用CNNs提取节点特征，并使用1*1 convolutions进行消息传递，以融合相邻特征同时保持结构；此外，引入深CNN聚合器（带有residual connections）来精炼消息，确保稳定的梯度流动和端到端训练。该方法有效桥接了空间特征提取与关系推理，在对象检测精炼和集成推理任务中实现了显著性能提升。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T07, 68T45, 68R10",
        "I.2.6; I.5.1; I.4.8"
      ],
      "primary_category": "cs.CV",
      "comment": "Submitted to arXiv. Code repository:\n  https://github.com/arashsajjadi/TGraphX |||\n  https://git.cs.usask.ca/arash/tgraphx",
      "pdf_url": "http://arxiv.org/pdf/2504.03953v1",
      "published_date": "2025-04-04 21:38:20 UTC",
      "updated_date": "2025-04-04 21:38:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:30:36.869285"
    },
    {
      "arxiv_id": "2504.03951v1",
      "title": "Understanding EFX Allocations: Counting and Variants",
      "title_zh": "翻译失败",
      "authors": [
        "Tzeh Yuan Neoh",
        "Nicholas Teh"
      ],
      "abstract": "Envy-freeness up to any good (EFX) is a popular and important fairness\nproperty in the fair allocation of indivisible goods, of which its existence in\ngeneral is still an open question. In this work, we investigate the problem of\ndetermining the minimum number of EFX allocations for a given instance, arguing\nthat this approach may yield valuable insights into the existence and\ncomputation of EFX allocations. We focus on restricted instances where the\nnumber of goods slightly exceeds the number of agents, and extend our analysis\nto weighted EFX (WEFX) and a novel variant of EFX for general monotone\nvaluations, termed EFX+. In doing so, we identify the transition threshold for\nthe existence of allocations satisfying these fairness notions. Notably, we\nresolve open problems regarding WEFX by proving polynomial-time computability\nunder binary additive valuations, and establishing the first constant-factor\napproximation for two agents.",
      "tldr_zh": "这篇论文探讨了EFX（Envy-freeness up to any good）公平分配不可分割物品的性质，通过计算给定实例中EFX分配的最小数量来揭示其存在性和计算问题。研究重点在于物品数量略多于代理（agents）的受限场景，并扩展到加权EFX（WEFX）和新型EFX+变体，适用于一般单调估值，同时识别满足这些公平概念的阈值。关键贡献包括证明WEFX在二元加性估值下可多项式时间计算，并为两代理场景提供首个常数因子逼近算法。",
      "categories": [
        "cs.GT",
        "cs.AI",
        "econ.TH"
      ],
      "primary_category": "cs.GT",
      "comment": "Appears in the 39th AAAI Conference on Artificial Intelligence\n  (AAAI), 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.03951v1",
      "published_date": "2025-04-04 21:36:09 UTC",
      "updated_date": "2025-04-04 21:36:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:30:49.117167"
    },
    {
      "arxiv_id": "2505.00006v1",
      "title": "Toward a digital twin of U.S. Congress",
      "title_zh": "翻译失败",
      "authors": [
        "Hayden Helm",
        "Tianyi Chen",
        "Harvey McGuinness",
        "Paige Lee",
        "Brandon Duderstadt",
        "Carey E. Priebe"
      ],
      "abstract": "In this paper we provide evidence that a virtual model of U.S.\ncongresspersons based on a collection of language models satisfies the\ndefinition of a digital twin. In particular, we introduce and provide\nhigh-level descriptions of a daily-updated dataset that contains every Tweet\nfrom every U.S. congressperson during their respective terms. We demonstrate\nthat a modern language model equipped with congressperson-specific subsets of\nthis data are capable of producing Tweets that are largely indistinguishable\nfrom actual Tweets posted by their physical counterparts. We illustrate how\ngenerated Tweets can be used to predict roll-call vote behaviors and to\nquantify the likelihood of congresspersons crossing party lines, thereby\nassisting stakeholders in allocating resources and potentially impacting\nreal-world legislative dynamics. We conclude with a discussion of the\nlimitations and important extensions of our analysis.",
      "tldr_zh": "该论文构建了一个基于语言 models 的数字 twin 模型，模拟美国国会议员的行为，以满足数字孪生的定义。研究者引入了一个每天更新的数据集，包含每位国会议员任期内的所有 Tweets，并使用现代语言模型结合特定子集数据生成推文，这些推文与真实推文几乎无法区分。主要发现是，生成的 Tweets 可以预测国会议员的投票行为和跨越党派线的可能性，从而帮助利益相关者分配资源并影响现实立法动态；论文还讨论了分析的局限性和重要扩展。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.SI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.00006v1",
      "published_date": "2025-04-04 21:33:36 UTC",
      "updated_date": "2025-04-04 21:33:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:31:01.223263"
    },
    {
      "arxiv_id": "2504.03940v1",
      "title": "Analysis of Robustness of a Large Game Corpus",
      "title_zh": "大型游戏语料库的鲁棒性分析",
      "authors": [
        "Mahsa Bazzaz",
        "Seth Cooper"
      ],
      "abstract": "Procedural content generation via machine learning (PCGML) in games involves\nusing machine learning techniques to create game content such as maps and\nlevels. 2D tile-based game levels have consistently served as a standard\ndataset for PCGML because they are a simplified version of game levels while\nmaintaining the specific constraints typical of games, such as being solvable.\nIn this work, we highlight the unique characteristics of game levels, including\ntheir structured discrete data nature, the local and global constraints\ninherent in the games, and the sensitivity of the game levels to small changes\nin input. We define the robustness of data as a measure of sensitivity to small\nchanges in input that cause a change in output, and we use this measure to\nanalyze and compare these levels to state-of-the-art machine learning datasets,\nshowcasing the subtle differences in their nature. We also constructed a large\ndataset from four games inspired by popular classic tile-based games that\nshowcase these characteristics and address the challenge of sparse data in\nPCGML by providing a significantly larger dataset than those currently\navailable.",
      "tldr_zh": "本研究分析了游戏关料库的鲁棒性（robustness），重点探讨使用机器学习进行程序化内容生成（PCGML）的挑战，如生成2D 瓷砖-based 游戏关卡时面临的结构化离散数据、本地和全局约束以及对小输入变化的敏感性。作者定义鲁棒性为输入微小变化导致输出变化的敏感度指标，并通过此指标比较游戏关卡与其他state-of-the-art 机器学习数据集，揭示了游戏数据的独特差异。最终，他们构建了一个大型数据集，基于四个受经典瓷砖-based 游戏启发的游戏，解决了PCGML 中数据稀疏的问题，提供比现有数据集更大的资源。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03940v1",
      "published_date": "2025-04-04 21:15:13 UTC",
      "updated_date": "2025-04-04 21:15:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:31:13.767297"
    },
    {
      "arxiv_id": "2504.07982v1",
      "title": "Metamorphic Testing for Fairness Evaluation in Large Language Models: Identifying Intersectional Bias in LLaMA and GPT",
      "title_zh": "大语言模型公平性评估的变形测试：识别 LLaMA 和 GPT 中的交叉偏差",
      "authors": [
        "Harishwar Reddy",
        "Madhusudan Srinivasan",
        "Upulee Kanewala"
      ],
      "abstract": "Large Language Models (LLMs) have made significant strides in Natural\nLanguage Processing but remain vulnerable to fairness-related issues, often\nreflecting biases inherent in their training data. These biases pose risks,\nparticularly when LLMs are deployed in sensitive areas such as healthcare,\nfinance, and law. This paper introduces a metamorphic testing approach to\nsystematically identify fairness bugs in LLMs. We define and apply a set of\nfairness-oriented metamorphic relations (MRs) to assess the LLaMA and GPT\nmodel, a state-of-the-art LLM, across diverse demographic inputs. Our\nmethodology includes generating source and follow-up test cases for each MR and\nanalyzing model responses for fairness violations. The results demonstrate the\neffectiveness of MT in exposing bias patterns, especially in relation to tone\nand sentiment, and highlight specific intersections of sensitive attributes\nthat frequently reveal fairness faults. This research improves fairness testing\nin LLMs, providing a structured approach to detect and mitigate biases and\nimprove model robustness in fairness-sensitive applications.",
      "tldr_zh": "本文提出了一种基于metamorphic testing的方法，用于评估Large Language Models (LLMs)的公平性问题，特别是在LLaMA和GPT模型中识别交叉偏见。该方法定义了fairness-oriented metamorphic relations (MRs)，通过生成源测试案例和后续测试案例来分析模型响应，并检测语气、情感等方面的公平性违规。实验结果显示，该方法有效暴露了敏感属性的交叉点引发的偏见模式，从而为LLMs的公平性测试提供结构化途径，并帮助缓解偏见以提升模型在敏感应用中的鲁棒性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.07982v1",
      "published_date": "2025-04-04 21:04:14 UTC",
      "updated_date": "2025-04-04 21:04:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:31:26.221663"
    },
    {
      "arxiv_id": "2504.03931v2",
      "title": "NAACL2025 Tutorial: Adaptation of Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zixuan Ke",
        "Yifei Ming",
        "Shafiq Joty"
      ],
      "abstract": "This tutorial on adaptation of LLMs is designed to address the growing demand\nfor models that go beyond the static capabilities of generic LLMs by providing\nan overview of dynamic, domain-specific, and task-adaptive LLM adaptation\ntechniques. While general LLMs have demonstrated strong generalization across a\nvariety of tasks, they often struggle to perform well in specialized domains\nsuch as finance, healthcare, and code generation for underrepresented\nlanguages. Additionally, their static nature limits their ability to evolve\nwith the changing world, and they are often extremely large in size, making\nthem impractical and costly to deploy at scale. As a result, the adaptation of\nLLMs has drawn much attention since the birth of LLMs and is of core\nimportance, both for industry, which focuses on serving its targeted users, and\nacademia, which can greatly benefit from small but powerful LLMs. To address\nthis gap, this tutorial aims to provide an overview of the LLM adaptation\ntechniques. We start with an introduction to LLM adaptation, from both the data\nperspective and the model perspective. We then emphasize how the evaluation\nmetrics and benchmarks are different from other techniques. After establishing\nthe problems, we explore various adaptation techniques. We categorize\nadaptation techniques into two main families. The first is parametric knowledge\nadaptation, which focuses on updating the parametric knowledge within LLMs.\nAdditionally, we will discuss real-time adaptation techniques, including model\nediting, which allows LLMs to be updated dynamically in production\nenvironments. The second kind of adaptation is semi-parametric knowledge\nadaptation, where the goal is to update LLM parameters to better leverage\nexternal knowledge or tools through techniques like retrieval-augmented\ngeneration (RAG) and agent-based systems.",
      "tldr_zh": "这个 NAACL 2025 教程聚焦于 Large Language Models (LLMs) 的适应技术，旨在解决通用 LLMs 在特定领域（如金融、医疗和代码生成）表现不佳的问题，并概述动态、领域特定和任务适应的方法。教程从数据视角和模型视角介绍适应策略，并强调独特的评估指标和基准。适应技术被分为两大类：参数知识适应（如实时模型编辑，用于更新 LLMs 参数）和半参数知识适应（如 Retrieval-Augmented Generation (RAG) 和基于代理的系统），以提升 LLMs 的灵活性和部署效率。最终，该教程为行业和学术界提供实用指导，帮助创建更小巧且强大的适应性模型。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "NAACL2025 Tutorial",
      "pdf_url": "http://arxiv.org/pdf/2504.03931v2",
      "published_date": "2025-04-04 20:57:41 UTC",
      "updated_date": "2025-04-21 04:07:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:31:37.919796"
    },
    {
      "arxiv_id": "2504.03930v1",
      "title": "Have Large Language Models Learned to Reason? A Characterization via 3-SAT Phase Transition",
      "title_zh": "大型语言模型是否学会了推理？通过3-SAT",
      "authors": [
        "Rishi Hazra",
        "Gabriele Venturato",
        "Pedro Zuidberg Dos Martires",
        "Luc De Raedt"
      ],
      "abstract": "Large Language Models (LLMs) have been touted as AI models possessing\nadvanced reasoning abilities. In theory, autoregressive LLMs with\nChain-of-Thought (CoT) can perform more serial computations to solve complex\nreasoning tasks. However, recent studies suggest that, despite this capacity,\nLLMs do not truly learn to reason but instead fit on statistical features. To\nstudy the reasoning capabilities in a principled fashion, we adopt a\ncomputational theory perspective and propose an experimental protocol centered\non 3-SAT -- the prototypical NP-complete problem lying at the core of logical\nreasoning and constraint satisfaction tasks. Specifically, we examine the phase\ntransitions in random 3-SAT and characterize the reasoning abilities of\nstate-of-the-art LLMs by varying the inherent hardness of the problem\ninstances. By comparing DeepSeek R1 with other LLMs, our findings reveal two\nkey insights (1) LLM accuracy drops significantly on harder instances,\nsuggesting all current models struggle when statistical shortcuts are\nunavailable (2) Unlike other LLMs, R1 shows signs of having learned the\nunderlying reasoning. Following a principled experimental protocol, our study\nmoves beyond the benchmark-driven evidence often found in LLM reasoning\nresearch. Our findings highlight important gaps and suggest clear directions\nfor future research.",
      "tldr_zh": "本研究质疑大型语言模型(LLMs)是否真正学会了推理，并通过3-SAT问题相变实验协议来系统评估其推理能力。研究者比较了DeepSeek R1与其他LLMs的表现，发现所有模型在更难的3-SAT实例上准确率显著下降，表明它们主要依赖统计特征而非真实推理。相比之下，DeepSeek R1显示出可能学习了底层推理的迹象，为LLM推理研究提供了超越基准测试的洞见，并指出了未来改进的方向。",
      "categories": [
        "cs.AI",
        "cs.CC",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "An updated version of arXiv:2408.07215v2, featuring: (1) inclusion of\n  recent LRMs and recent LLMs, (2) revised conclusions reflecting recent\n  developments, and (3) updated analysis",
      "pdf_url": "http://arxiv.org/pdf/2504.03930v1",
      "published_date": "2025-04-04 20:57:36 UTC",
      "updated_date": "2025-04-04 20:57:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:31:50.668962"
    },
    {
      "arxiv_id": "2504.05334v1",
      "title": "Level Generation with Constrained Expressive Range",
      "title_zh": "基于受约束表达范围的关卡生成",
      "authors": [
        "Mahsa Bazzaz",
        "Seth Cooper"
      ],
      "abstract": "Expressive range analysis is a visualization-based technique used to evaluate\nthe performance of generative models, particularly in game level generation. It\ntypically employs two quantifiable metrics to position generated artifacts on a\n2D plot, offering insight into how content is distributed within a defined\nmetric space. In this work, we use the expressive range of a generator as the\nconceptual space of possible creations. Inspired by the quality diversity\nparadigm, we explore this space to generate levels. To do so, we use a\nconstraint-based generator that systematically traverses and generates levels\nin this space. To train the constraint-based generator we use different tile\npatterns to learn from the initial example levels. We analyze how different\npatterns influence the exploration of the expressive range. Specifically, we\ncompare the exploration process based on time, the number of successful and\nfailed sample generations, and the overall interestingness of the generated\nlevels. Unlike typical quality diversity approaches that rely on random\ngeneration and hope to get good coverage of the expressive range, this approach\nsystematically traverses the grid ensuring more coverage. This helps create\nunique and interesting game levels while also improving our understanding of\nthe generator's strengths and limitations.",
      "tldr_zh": "这篇论文提出了一种基于 expressive range analysis 的游戏关卡生成方法，将 expressive range 视为可能创作的概念空间，并受 quality diversity 范式启发。作者开发了一个 constraint-based generator，通过不同的 tile patterns 从初始示例关卡中学习，来系统地遍历和生成关卡，从而确保更全面的覆盖。实验分析了各种 patterns 对探索过程的影响，包括时间消耗、成功/失败样本数量以及生成的关卡 interestingness。与传统的随机生成方法相比，这种系统遍历方式能创建更独特且有趣的关卡，并提升了对生成器优缺点的研究理解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.05334v1",
      "published_date": "2025-04-04 20:55:30 UTC",
      "updated_date": "2025-04-04 20:55:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:32:02.067839"
    },
    {
      "arxiv_id": "2504.13889v1",
      "title": "Maestoso: An Intelligent Educational Sketching Tool for Learning Music Theory",
      "title_zh": "Maestoso：一个智能教育素描工具",
      "authors": [
        "Paul Taele",
        "Laura Barreto",
        "Tracy Hammond"
      ],
      "abstract": "Learning music theory not only has practical benefits for musicians to write,\nperform, understand, and express music better, but also for both non-musicians\nto improve critical thinking, math analytical skills, and music appreciation.\nHowever, current external tools applicable for learning music theory through\nwriting when human instruction is unavailable are either limited in feedback,\nlacking a written modality, or assuming already strong familiarity of music\ntheory concepts. In this paper, we describe Maestoso, an educational tool for\nnovice learners to learn music theory through sketching practice of quizzed\nmusic structures. Maestoso first automatically recognizes students' sketched\ninput of quizzed concepts, then relies on existing sketch and gesture\nrecognition techniques to automatically recognize the input, and finally\ngenerates instructor-emulated feedback. From our evaluations, we demonstrate\nthat Maestoso performs reasonably well on recognizing music structure elements\nand that novice students can comfortably grasp introductory music theory in a\nsingle session.",
      "tldr_zh": "该论文介绍了Maestoso，一款智能教育工具，旨在帮助初学者通过素描练习学习音乐理论，以弥补现有工具在反馈、书写模式和用户熟悉度方面的不足。Maestoso采用自动识别技术，包括sketch and gesture recognition，来分析学生的素描输入，并生成类似于教师的反馈，从而提升学习体验。评估结果显示，该工具在识别音乐结构元素方面表现良好，新手用户能在单次会话中轻松掌握入门音乐理论。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13889v1",
      "published_date": "2025-04-04 20:46:24 UTC",
      "updated_date": "2025-04-04 20:46:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:32:12.258165"
    },
    {
      "arxiv_id": "2504.03915v1",
      "title": "RF-BayesPhysNet: A Bayesian rPPG Uncertainty Estimation Method for Complex Scenarios",
      "title_zh": "翻译失败",
      "authors": [
        "Rufei Ma",
        "Chao Chen"
      ],
      "abstract": "Remote photoplethysmography (rPPG) technology infers heart rate by capturing\nsubtle color changes in facial skin\n  using a camera, demonstrating great potential in non-contact heart rate\nmeasurement. However, measurement\n  accuracy significantly decreases in complex scenarios such as lighting\nchanges and head movements compared\n  to ideal laboratory conditions. Existing deep learning models often neglect\nthe quantification of measurement\n  uncertainty, limiting their credibility in dynamic scenes. To address the\nissue of insufficient rPPG measurement\n  reliability in complex scenarios, this paper introduces Bayesian neural\nnetworks to the rPPG field for the first time,\n  proposing the Robust Fusion Bayesian Physiological Network (RF-BayesPhysNet),\nwhich can model both aleatoric\n  and epistemic uncertainty. It leverages variational inference to balance\naccuracy and computational efficiency.\n  Due to the current lack of uncertainty estimation metrics in the rPPG field,\nthis paper also proposes a new set of\n  methods, using Spearman correlation coefficient, prediction interval\ncoverage, and confidence interval width, to\n  measure the effectiveness of uncertainty estimation methods under different\nnoise conditions. Experiments show\n  that the model, with only double the parameters compared to traditional\nnetwork models, achieves a MAE of 2.56\n  on the UBFC-RPPG dataset, surpassing most models. It demonstrates good\nuncertainty estimation capability\n  in no-noise and low-noise conditions, providing prediction confidence and\nsignificantly enhancing robustness in\n  real-world applications. We have open-sourced the code at\nhttps://github.com/AIDC-rPPG/RF-Net",
      "tldr_zh": "这篇论文针对远程光电容积描记术 (rPPG) 在复杂场景（如光线变化和头部移动）中的测量准确性和可靠性问题，首次引入 Bayesian 神经网络，提出 Robust Fusion Bayesian Physiological Network (RF-BayesPhysNet) 方法，以建模 aleatoric 和 epistemic uncertainty，并利用 variational inference 平衡准确性和计算效率。该方法还创新性地提出一组不确定性估计指标，包括 Spearman correlation coefficient、prediction interval coverage 和 confidence interval width，用于评估不同噪声条件下的性能。实验结果显示，RF-BayesPhysNet 在 UBFC-RPPG 数据集上实现 MAE 为 2.56，比大多数模型表现更优，并在无噪声和低噪声条件下展现出色的不确定性估计能力，从而显著提升了 rPPG 在真实世界应用的鲁棒性。代码已开源于 https://github.com/AIDC-rPPG/RF-Net。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "68T07, 62F15, 94A12",
        "I.2.6; I.5.4; C.3"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.03915v1",
      "published_date": "2025-04-04 20:24:57 UTC",
      "updated_date": "2025-04-04 20:24:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:32:27.372815"
    },
    {
      "arxiv_id": "2504.13888v1",
      "title": "Kanji Workbook: A Writing-Based Intelligent Tutoring System for Learning Proper Japanese Kanji Writing Technique with Instructor-Emulated Assessment",
      "title_zh": "Kanji Workbook：一种基于写作的智能辅导系统，用于学习正确的日语Kanji书写技术，并带有教师模拟评估",
      "authors": [
        "Paul Taele",
        "Jung In Koh",
        "Tracy Hammond"
      ],
      "abstract": "Kanji script writing is a skill that is often introduced to novice Japanese\nforeign language students for achieving Japanese writing mastery, but often\nposes difficulties to students with primarily English fluency due to their its\nvast differences with written English. Instructors often introduce various\npedagogical methods -- such as visual structure and written techniques -- to\nassist students in kanji study, but may lack availability providing direct\nfeedback on students' writing outside of class. Current educational\napplications are also limited due to lacking richer instructor-emulated\nfeedback. We introduce Kanji Workbook, a writing-based intelligent tutoring\nsystem for students to receive intelligent assessment that emulates human\ninstructor feedback. Our interface not only leverages students' computing\ndevices for allowing them to learn, practice, and review the writing of\nprompted characters from their course's kanji script lessons, but also provides\na diverse set of writing assessment metrics -- derived from instructor\ninterviews and classroom observation insights -- through intelligent scoring\nand visual animations. We deployed our interface onto novice- and\nintermediate-level university courses over an entire academic year, and\nobserved that interface users on average achieved higher course grades than\ntheir peers and also reacted positively to our interface's various features.",
      "tldr_zh": "本研究开发了Kanji Workbook，这是一个基于书写的Intelligent Tutoring System，旨在帮助英语母语者学习日语Kanji书写技巧，并通过模拟教师反馈解决传统教学中的反馈不足问题。系统利用学生设备提供学习、练习和复习功能，并基于教师访谈和课堂观察设计了多样化的评估指标，如智能评分和视觉动画。实验结果显示，在一个学年部署到大学课程后，使用该系统的学生平均成绩高于未使用组，且对系统的功能反馈积极。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13888v1",
      "published_date": "2025-04-04 19:59:27 UTC",
      "updated_date": "2025-04-04 19:59:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:32:36.738622"
    },
    {
      "arxiv_id": "2504.03894v1",
      "title": "Leveraging Gait Patterns as Biomarkers: An attention-guided Deep Multiple Instance Learning Network for Scoliosis Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Haiqing Li",
        "Yuzhi Guo",
        "Feng Jiang",
        "Qifeng Zhou",
        "Hehuan Ma",
        "Junzhou Huang"
      ],
      "abstract": "Scoliosis is a spinal curvature disorder that is difficult to detect early\nand can compress the chest cavity, impacting respiratory function and cardiac\nhealth. Especially for adolescents, delayed detection and treatment result in\nworsening compression. Traditional scoliosis detection methods heavily rely on\nclinical expertise, and X-ray imaging poses radiation risks, limiting\nlarge-scale early screening. We propose an Attention-Guided Deep Multi-Instance\nLearning method (Gait-MIL) to effectively capture discriminative features from\ngait patterns, which is inspired by ScoNet-MT's pioneering use of gait patterns\nfor scoliosis detection. We evaluate our method on the first large-scale\ndataset based on gait patterns for scoliosis classification. The results\ndemonstrate that our study improves the performance of using gait as a\nbiomarker for scoliosis detection, significantly enhances detection accuracy\nfor the particularly challenging Neutral cases, where subtle indicators are\noften overlooked. Our Gait-MIL also performs robustly in imbalanced scenarios,\nmaking it a promising tool for large-scale scoliosis screening.",
      "tldr_zh": "本研究针对脊柱侧弯（Scoliosis）的早期检测挑战，提出了一种注意力引导的深度多实例学习网络（Attention-Guided Deep Multi-Instance Learning，Gait-MIL），利用步态模式作为生物标志物来捕获鉴别特征，以克服传统方法的依赖性和辐射风险。\nGait-MIL 受 ScoNet-MT 的启发，在首个大规模基于步态模式的数据集上进行评估，显著提升了检测性能。\n实验结果显示，该方法尤其提高了 Neutral 病例的准确率，并在不平衡场景下表现出色。\n总体上，Gait-MIL 为大规模脊柱侧弯筛查提供了一个可靠且高效的工具。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.03894v1",
      "published_date": "2025-04-04 19:35:33 UTC",
      "updated_date": "2025-04-04 19:35:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:32:49.961493"
    },
    {
      "arxiv_id": "2504.03888v1",
      "title": "Investigating Affective Use and Emotional Well-being on ChatGPT",
      "title_zh": "ChatGPT 的情感使用与情感福祉调查",
      "authors": [
        "Jason Phang",
        "Michael Lampe",
        "Lama Ahmad",
        "Sandhini Agarwal",
        "Cathy Mengying Fang",
        "Auren R. Liu",
        "Valdemar Danry",
        "Eunhae Lee",
        "Samantha W. T. Chan",
        "Pat Pataranutaporn",
        "Pattie Maes"
      ],
      "abstract": "As AI chatbots see increased adoption and integration into everyday life,\nquestions have been raised about the potential impact of human-like or\nanthropomorphic AI on users. In this work, we investigate the extent to which\ninteractions with ChatGPT (with a focus on Advanced Voice Mode) may impact\nusers' emotional well-being, behaviors and experiences through two parallel\nstudies. To study the affective use of AI chatbots, we perform large-scale\nautomated analysis of ChatGPT platform usage in a privacy-preserving manner,\nanalyzing over 3 million conversations for affective cues and surveying over\n4,000 users on their perceptions of ChatGPT. To investigate whether there is a\nrelationship between model usage and emotional well-being, we conduct an\nInstitutional Review Board (IRB)-approved randomized controlled trial (RCT) on\nclose to 1,000 participants over 28 days, examining changes in their emotional\nwell-being as they interact with ChatGPT under different experimental settings.\nIn both on-platform data analysis and the RCT, we observe that very high usage\ncorrelates with increased self-reported indicators of dependence. From our RCT,\nwe find that the impact of voice-based interactions on emotional well-being to\nbe highly nuanced, and influenced by factors such as the user's initial\nemotional state and total usage duration. Overall, our analysis reveals that a\nsmall number of users are responsible for a disproportionate share of the most\naffective cues.",
      "tldr_zh": "本研究调查了ChatGPT（尤其是Advanced Voice Mode）对用户情感福祉、行为和体验的影响，通过两个平行研究进行分析。首先，研究者对超过300万次对话进行大规模隐私保护自动化分析，并调查4000多名用户对ChatGPT的感知，以识别情感线索。其次，开展了一项IRB批准的随机对照试验（RCT），涉及近1000名参与者为期28天，评估不同互动设置下ChatGPT使用对情感福祉的变化。结果显示，高使用量与依赖性指标显著相关，而语音互动的影响因用户初始情感状态和使用时长而异；此外，少数用户贡献了大部分情感线索。总的来说，该研究揭示了AI聊天机器人潜在风险，为设计更负责任的AI交互提供了见解。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03888v1",
      "published_date": "2025-04-04 19:22:10 UTC",
      "updated_date": "2025-04-04 19:22:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:33:00.568512"
    },
    {
      "arxiv_id": "2504.03887v1",
      "title": "Accurate GPU Memory Prediction for Deep Learning Jobs through Dynamic Analysis",
      "title_zh": "通过动态分析实现深度学习任务的准确 GPU 内存预测",
      "authors": [
        "Jiabo Shi",
        "Yehia Elkhatib"
      ],
      "abstract": "The benefits of Deep Learning (DL) impose significant pressure on GPU\nresources, particularly within GPU cluster, where Out-Of-Memory (OOM) errors\npresent a primary impediment to model training and efficient resource\nutilization. Conventional OOM estimation techniques, relying either on static\ngraph analysis or direct GPU memory profiling, suffer from inherent\nlimitations: static analysis often fails to capture model dynamics, whereas\nGPU-based profiling intensifies contention for scarce GPU resources. To\novercome these constraints, VeritasEst emerges. It is an innovative, entirely\nCPU-based analysis tool capable of accurately predicting the peak GPU memory\nrequired for DL training tasks without accessing the target GPU. This \"offline\"\nprediction capability is core advantage of VeritasEst, allowing accurate memory\nfootprint information to be obtained before task scheduling, thereby\neffectively preventing OOM and optimizing GPU allocation. Its performance was\nvalidated through thousands of experimental runs across convolutional neural\nnetwork (CNN) models: Compared to baseline GPU memory estimators, VeritasEst\nsignificantly reduces the relative error by 84% and lowers the estimation\nfailure probability by 73%. VeritasEst represents a key step towards efficient\nand predictable DL training in resource-constrained environments.",
      "tldr_zh": "该研究针对深度学习(DL)任务在GPU集群中常见的Out-Of-Memory (OOM)错误问题，提出了一种基于动态分析的创新工具VeritasEst。该工具完全依赖CPU进行分析，能够准确预测DL训练任务的峰值GPU内存需求，而无需访问目标GPU，从而在任务调度前预防OOM错误并优化资源分配。与基线方法相比，VeritasEst在数千次CNN模型实验中，将相对误差降低了84%并将估计失败概率降低了73%。这项工作为资源受限环境中的DL训练提供了高效、可预测的解决方案。",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03887v1",
      "published_date": "2025-04-04 19:20:03 UTC",
      "updated_date": "2025-04-04 19:20:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:33:12.746532"
    },
    {
      "arxiv_id": "2504.03861v1",
      "title": "Improving World Models using Deep Supervision with Linear Probes",
      "title_zh": "翻译失败",
      "authors": [
        "Andrii Zahorodnii"
      ],
      "abstract": "Developing effective world models is crucial for creating artificial agents\nthat can reason about and navigate complex environments. In this paper, we\ninvestigate a deep supervision technique for encouraging the development of a\nworld model in a network trained end-to-end to predict the next observation.\nWhile deep supervision has been widely applied for task-specific learning, our\nfocus is on improving the world models. Using an experimental environment based\non the Flappy Bird game, where the agent receives only LIDAR measurements as\nobservations, we explore the effect of adding a linear probe component to the\nnetwork's loss function. This additional term encourages the network to encode\na subset of the true underlying world features into its hidden state. Our\nexperiments demonstrate that this supervision technique improves both training\nand test performance, enhances training stability, and results in more easily\ndecodable world features -- even for those world features which were not\nincluded in the training. Furthermore, we observe a reduced distribution drift\nin networks trained with the linear probe, particularly during high-variability\nphases of the game (flying between successive pipe encounters). Including the\nworld features loss component roughly corresponded to doubling the model size,\nsuggesting that the linear probe technique is particularly beneficial in\ncompute-limited settings or when aiming to achieve the best performance with\nsmaller models. These findings contribute to our understanding of how to\ndevelop more robust and sophisticated world models in artificial agents, paving\nthe way for further advancements in this field.",
      "tldr_zh": "本文提出了一种使用深度监督和线性探针改进世界 models 的方法，旨在帮助AI代理更好地在复杂环境中推理和导航。具体而言，通过在Flappy Bird游戏的实验环境中添加线性探针组件到网络损失函数中，鼓励网络在隐藏状态中编码真实世界特征，从而提升训练和测试性能，并提高训练稳定性。实验结果显示，该技术使世界特征更容易解码，甚至包括未训练的特征，并显著减少了分布漂移，尤其在游戏的高变异性阶段。尽管这相当于将模型大小加倍，该方法在计算资源有限的场景中特别有效，为开发更稳健的AI世界 models 提供了新途径。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "ICLR 2025 Workshop on World Models",
      "pdf_url": "http://arxiv.org/pdf/2504.03861v1",
      "published_date": "2025-04-04 18:35:21 UTC",
      "updated_date": "2025-04-04 18:35:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:33:24.740211"
    },
    {
      "arxiv_id": "2504.03857v1",
      "title": "Can ChatGPT Learn My Life From a Week of First-Person Video?",
      "title_zh": "ChatGPT 能从一周第一人称视频中学习我的生活吗？",
      "authors": [
        "Keegan Harris"
      ],
      "abstract": "Motivated by recent improvements in generative AI and wearable camera devices\n(e.g. smart glasses and AI-enabled pins), I investigate the ability of\nfoundation models to learn about the wearer's personal life through\nfirst-person camera data. To test this, I wore a camera headset for 54 hours\nover the course of a week, generated summaries of various lengths (e.g.\nminute-long, hour-long, and day-long summaries), and fine-tuned both GPT-4o and\nGPT-4o-mini on the resulting summary hierarchy. By querying the fine-tuned\nmodels, we are able to learn what the models learned about me. The results are\nmixed: Both models learned basic information about me (e.g. approximate age,\ngender). Moreover, GPT-4o correctly deduced that I live in Pittsburgh, am a PhD\nstudent at CMU, am right-handed, and have a pet cat. However, both models also\nsuffered from hallucination and would make up names for the individuals present\nin the video footage of my life.",
      "tldr_zh": "本研究探讨了基础模型（如GPT-4o和GPT-4o-mini）能否从一周的第一人称视频中学习个人生活细节。研究者记录了54小时的视频数据，生成不同长度的总结（如分钟级、小时级和天级），并使用这些总结微调模型。结果显示，模型成功学习了基本信息，包括研究者的年龄、性别、居住地（Pittsburgh）、职业（CMU PhD学生）、惯用手（right-handed）和宠物（cat），但也存在hallucination问题，导致编造视频中人物的姓名。该工作揭示了生成AI在个人数据学习中的潜力与局限性，为可穿戴设备应用提供了重要见解。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03857v1",
      "published_date": "2025-04-04 18:33:45 UTC",
      "updated_date": "2025-04-04 18:33:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:33:36.918477"
    },
    {
      "arxiv_id": "2504.03850v1",
      "title": "Detection Limits and Statistical Separability of Tree Ring Watermarks in Rectified Flow-based Text-to-Image Generation Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ved Umrajkar",
        "Aakash Kumar Singh"
      ],
      "abstract": "Tree-Ring Watermarking is a significant technique for authenticating\nAI-generated images. However, its effectiveness in rectified flow-based models\nremains unexplored, particularly given the inherent challenges of these models\nwith noise latent inversion. Through extensive experimentation, we evaluated\nand compared the detection and separability of watermarks between SD 2.1 and\nFLUX.1-dev models. By analyzing various text guidance configurations and\naugmentation attacks, we demonstrate how inversion limitations affect both\nwatermark recovery and the statistical separation between watermarked and\nunwatermarked images. Our findings provide valuable insights into the current\nlimitations of Tree-Ring Watermarking in the current SOTA models and highlight\nthe critical need for improved inversion methods to achieve reliable watermark\ndetection and separability. The official implementation, dataset release and\nall experimental results are available at this\n\\href{https://github.com/dsgiitr/flux-watermarking}{\\textbf{link}}.",
      "tldr_zh": "本研究探讨了 Tree-Ring Watermarking 在 rectified flow-based 文本到图像生成模型中的检测限制和统计可分离性，通过实验比较了 SD 2.1 和 FLUX.1-dev 模型。研究者分析了不同文本指导配置和增强攻击（如噪声潜在逆变）对水印恢复和水印与非水印图像分离的影响，发现这些模型的逆变限制显著降低了水印检测的可靠性和统计分离能力。该工作突出了当前 SOTA 模型的局限性，并强调了改进逆变方法以实现更可靠水印认证的必要性；相关实现、数据集和实验结果可在 GitHub 上获取。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03850v1",
      "published_date": "2025-04-04 18:24:23 UTC",
      "updated_date": "2025-04-04 18:24:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:33:49.065132"
    },
    {
      "arxiv_id": "2504.08779v1",
      "title": "Can AI Master Construction Management (CM)? Benchmarking State-of-the-Art Large Language Models on CM Certification Exams",
      "title_zh": "翻译失败",
      "authors": [
        "Ruoxin Xiong",
        "Yanyu Wang",
        "Suat Gunhan",
        "Yimin Zhu",
        "Charles Berryman"
      ],
      "abstract": "The growing complexity of construction management (CM) projects, coupled with\nchallenges such as strict regulatory requirements and labor shortages, requires\nspecialized analytical tools that streamline project workflow and enhance\nperformance. Although large language models (LLMs) have demonstrated\nexceptional performance in general reasoning tasks, their effectiveness in\ntackling CM-specific challenges, such as precise quantitative analysis and\nregulatory interpretation, remains inadequately explored. To bridge this gap,\nthis study introduces CMExamSet, a comprehensive benchmarking dataset\ncomprising 689 authentic multiple-choice questions sourced from four nationally\naccredited CM certification exams. Our zero-shot evaluation assesses overall\naccuracy, subject areas (e.g., construction safety), reasoning complexity\n(single-step and multi-step), and question formats (text-only,\nfigure-referenced, and table-referenced). The results indicate that GPT-4o and\nClaude 3.7 surpass typical human pass thresholds (70%), with average accuracies\nof 82% and 83%, respectively. Additionally, both models performed better on\nsingle-step tasks, with accuracies of 85.7% (GPT-4o) and 86.7% (Claude 3.7).\nMulti-step tasks were more challenging, reducing performance to 76.5% and\n77.6%, respectively. Furthermore, both LLMs show significant limitations on\nfigure-referenced questions, with accuracies dropping to approximately 40%. Our\nerror pattern analysis further reveals that conceptual misunderstandings are\nthe most common (44.4% and 47.9%), underscoring the need for enhanced\ndomain-specific reasoning models. These findings underscore the potential of\nLLMs as valuable supplementary analytical tools in CM, while highlighting the\nneed for domain-specific refinements and sustained human oversight in complex\ndecision making.",
      "tldr_zh": "这篇论文引入了 CMExamSet 数据集，包含 689 个真实多选题，用于评估大型语言模型 (LLMs) 在建筑管理 (CM) 认证考试中的表现，旨在探讨 LLMs 处理 CM 特定挑战（如定量分析和监管解释）的能力。研究采用 zero-shot evaluation 方法，评估 GPT-4o 和 Claude 3.7 的整体准确率（分别为 82% 和 83%，超过人类通过阈值 70%）、推理复杂度（单步任务准确率达 85.7% 和 86.7%，多步任务降至 76.5% 和 77.6%）以及问题格式（图表相关题目准确率约 40%）。结果显示 LLMs 在 CM 领域有潜力作为辅助工具，但存在概念误解等局限，需要领域特定优化和人类监督。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.08779v1",
      "published_date": "2025-04-04 18:13:45 UTC",
      "updated_date": "2025-04-04 18:13:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:34:03.494119"
    },
    {
      "arxiv_id": "2504.03640v1",
      "title": "Bonsai: Interpretable Tree-Adaptive Grounded Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Kate Sanders",
        "Benjamin Van Durme"
      ],
      "abstract": "To develop general-purpose collaborative agents, humans need reliable AI\nsystems that can (1) adapt to new domains and (2) transparently reason with\nuncertainty to allow for verification and correction. Black-box models\ndemonstrate powerful data processing abilities but do not satisfy these\ncriteria due to their opaqueness, domain specificity, and lack of uncertainty\nawareness. We introduce Bonsai, a compositional and probabilistic reasoning\nsystem that generates adaptable inference trees by retrieving relevant\ngrounding evidence and using it to compute likelihoods of sub-claims derived\nfrom broader natural language inferences. Bonsai's reasoning power is tunable\nat test-time via evidence scaling and it demonstrates reliable handling of\nvaried domains including transcripts, photographs, videos, audio, and\ndatabases. Question-answering and human alignment experiments demonstrate that\nBonsai matches the performance of domain-specific black-box methods while\ngenerating interpretable, grounded, and uncertainty-aware reasoning traces.",
      "tldr_zh": "该研究引入了Bonsai，一种组合式和概率推理系统，旨在开发可适应新领域的AI代理，并通过透明处理不确定性来支持验证和修正。Bonsai通过检索相关证据生成可调适的推理树（inference trees），并使用这些证据计算更广泛自然语言推理中的子声明似然，从而实现grounded reasoning。实验显示，Bonsai在问答和人类对齐任务中，与领域特定的黑箱模型性能相当，同时提供可解释、基于证据的推理痕迹，并可靠处理多种领域如文本、照片、视频、音频和数据库。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "68T50, 68T37",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, preprint",
      "pdf_url": "http://arxiv.org/pdf/2504.03640v1",
      "published_date": "2025-04-04 17:59:50 UTC",
      "updated_date": "2025-04-04 17:59:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:34:13.612199"
    },
    {
      "arxiv_id": "2504.03635v1",
      "title": "Do Larger Language Models Imply Better Reasoning? A Pretraining Scaling Law for Reasoning",
      "title_zh": "更大的语言模型是否意味着更好的推理？一种用于推理的预训练缩放定律",
      "authors": [
        "Xinyi Wang",
        "Shawn Tan",
        "Mingyu Jin",
        "William Yang Wang",
        "Rameswar Panda",
        "Yikang Shen"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks requiring complex reasoning. However, the effects of\nscaling on their reasoning abilities remain insufficiently understood. In this\npaper, we introduce a synthetic multihop reasoning environment designed to\nclosely replicate the structure and distribution of real-world large-scale\nknowledge graphs. Our reasoning task involves completing missing edges in the\ngraph, which requires advanced multi-hop reasoning and mimics real-world\nreasoning scenarios. To evaluate this, we pretrain language models (LMs) from\nscratch solely on triples from the incomplete graph and assess their ability to\ninfer the missing edges. Interestingly, we observe that overparameterization\ncan impair reasoning performance due to excessive memorization. We investigate\ndifferent factors that affect this U-shaped loss curve, including graph\nstructure, model size, and training steps. To predict the optimal model size\nfor a specific knowledge graph, we find an empirical scaling that linearly maps\nthe knowledge graph search entropy to the optimal model size. This work\nprovides new insights into the relationship between scaling and reasoning in\nLLMs, shedding light on possible ways to optimize their performance for\nreasoning tasks.",
      "tldr_zh": "这篇论文探讨了大型语言模型 (LLMs) 的规模扩展是否会提升推理能力，引入了一个合成多跳推理环境来模拟真实知识图的结构，并通过预训练模型从头处理不完整图中的缺失边来评估推理性能。研究发现，过参数化可能导致过度记忆，从而产生 U 形损失曲线并损害推理效果，同时影响因素包括图结构、模型大小和训练步骤。为优化性能，论文提出一个经验缩放定律，将知识图的搜索熵线性映射到最优模型大小，提供新见解以指导 LLMs 在推理任务中的训练。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03635v1",
      "published_date": "2025-04-04 17:57:22 UTC",
      "updated_date": "2025-04-04 17:57:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:34:25.694212"
    },
    {
      "arxiv_id": "2504.03624v3",
      "title": "Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models",
      "title_zh": "Nemotron-H：一个准确且高效的混合 Mamba-Transformer 模型家族",
      "authors": [
        "NVIDIA",
        ":",
        "Aaron Blakeman",
        "Aarti Basant",
        "Abhinav Khattar",
        "Adithya Renduchintala",
        "Akhiad Bercovich",
        "Aleksander Ficek",
        "Alexis Bjorlin",
        "Ali Taghibakhshi",
        "Amala Sanjay Deshmukh",
        "Ameya Sunil Mahabaleshwarkar",
        "Andrew Tao",
        "Anna Shors",
        "Ashwath Aithal",
        "Ashwin Poojary",
        "Ayush Dattagupta",
        "Balaram Buddharaju",
        "Bobby Chen",
        "Boris Ginsburg",
        "Boxin Wang",
        "Brandon Norick",
        "Brian Butterfield",
        "Bryan Catanzaro",
        "Carlo del Mundo",
        "Chengyu Dong",
        "Christine Harvey",
        "Christopher Parisien",
        "Dan Su",
        "Daniel Korzekwa",
        "Danny Yin",
        "Daria Gitman",
        "David Mosallanezhad",
        "Deepak Narayanan",
        "Denys Fridman",
        "Dima Rekesh",
        "Ding Ma",
        "Dmytro Pykhtar",
        "Dong Ahn",
        "Duncan Riach",
        "Dusan Stosic",
        "Eileen Long",
        "Elad Segal",
        "Ellie Evans",
        "Eric Chung",
        "Erick Galinkin",
        "Evelina Bakhturina",
        "Ewa Dobrowolska",
        "Fei Jia",
        "Fuxiao Liu",
        "Gargi Prasad",
        "Gerald Shen",
        "Guilin Liu",
        "Guo Chen",
        "Haifeng Qian",
        "Helen Ngo",
        "Hongbin Liu",
        "Hui Li",
        "Igor Gitman",
        "Ilia Karmanov",
        "Ivan Moshkov",
        "Izik Golan",
        "Jan Kautz",
        "Jane Polak Scowcroft",
        "Jared Casper",
        "Jarno Seppanen",
        "Jason Lu",
        "Jason Sewall",
        "Jiaqi Zeng",
        "Jiaxuan You",
        "Jimmy Zhang",
        "Jing Zhang",
        "Jining Huang",
        "Jinze Xue",
        "Jocelyn Huang",
        "Joey Conway",
        "John Kamalu",
        "Jon Barker",
        "Jonathan Cohen",
        "Joseph Jennings",
        "Jupinder Parmar",
        "Karan Sapra",
        "Kari Briski",
        "Kateryna Chumachenko",
        "Katherine Luna",
        "Keshav Santhanam",
        "Kezhi Kong",
        "Kirthi Sivamani",
        "Krzysztof Pawelec",
        "Kumar Anik",
        "Kunlun Li",
        "Lawrence McAfee",
        "Leon Derczynski",
        "Lindsey Pavao",
        "Luis Vega",
        "Lukas Voegtle",
        "Maciej Bala",
        "Maer Rodrigues de Melo",
        "Makesh Narsimhan Sreedhar",
        "Marcin Chochowski",
        "Markus Kliegl",
        "Marta Stepniewska-Dziubinska",
        "Matthieu Le",
        "Matvei Novikov",
        "Mehrzad Samadi",
        "Michael Andersch",
        "Michael Evans",
        "Miguel Martinez",
        "Mike Chrzanowski",
        "Mike Ranzinger",
        "Mikolaj Blaz",
        "Misha Smelyanskiy",
        "Mohamed Fawzy",
        "Mohammad Shoeybi",
        "Mostofa Patwary",
        "Nayeon Lee",
        "Nima Tajbakhsh",
        "Ning Xu",
        "Oleg Rybakov",
        "Oleksii Kuchaiev",
        "Olivier Delalleau",
        "Osvald Nitski",
        "Parth Chadha",
        "Pasha Shamis",
        "Paulius Micikevicius",
        "Pavlo Molchanov",
        "Peter Dykas",
        "Philipp Fischer",
        "Pierre-Yves Aquilanti",
        "Piotr Bialecki",
        "Prasoon Varshney",
        "Pritam Gundecha",
        "Przemek Tredak",
        "Rabeeh Karimi",
        "Rahul Kandu",
        "Ran El-Yaniv",
        "Raviraj Joshi",
        "Roger Waleffe",
        "Ruoxi Zhang",
        "Sabrina Kavanaugh",
        "Sahil Jain",
        "Samuel Kriman",
        "Sangkug Lym",
        "Sanjeev Satheesh",
        "Saurav Muralidharan",
        "Sean Narenthiran",
        "Selvaraj Anandaraj",
        "Seonmyeong Bak",
        "Sergey Kashirsky",
        "Seungju Han",
        "Shantanu Acharya",
        "Shaona Ghosh",
        "Sharath Turuvekere Sreenivas",
        "Sharon Clay",
        "Shelby Thomas",
        "Shrimai Prabhumoye",
        "Shubham Pachori",
        "Shubham Toshniwal",
        "Shyamala Prayaga",
        "Siddhartha Jain",
        "Sirshak Das",
        "Slawek Kierat",
        "Somshubra Majumdar",
        "Song Han",
        "Soumye Singhal",
        "Sriharsha Niverty",
        "Stefania Alborghetti",
        "Suseella Panguluri",
        "Swetha Bhendigeri",
        "Syeda Nahida Akter",
        "Szymon Migacz",
        "Tal Shiri",
        "Terry Kong",
        "Timo Roman",
        "Tomer Ronen",
        "Trisha Saar",
        "Tugrul Konuk",
        "Tuomas Rintamaki",
        "Tyler Poon",
        "Ushnish De",
        "Vahid Noroozi",
        "Varun Singh",
        "Vijay Korthikanti",
        "Vitaly Kurin",
        "Wasi Uddin Ahmad",
        "Wei Du",
        "Wei Ping",
        "Wenliang Dai",
        "Wonmin Byeon",
        "Xiaowei Ren",
        "Yao Xu",
        "Yejin Choi",
        "Yian Zhang",
        "Ying Lin",
        "Yoshi Suhara",
        "Zhiding Yu",
        "Zhiqi Li",
        "Zhiyu Li",
        "Zhongbo Zhu",
        "Zhuolin Yang",
        "Zijia Chen"
      ],
      "abstract": "As inference-time scaling becomes critical for enhanced reasoning\ncapabilities, it is increasingly becoming important to build models that are\nefficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid\nMamba-Transformer models designed to reduce inference cost for a given accuracy\nlevel. To achieve this goal, we replace the majority of self-attention layers\nin the common Transformer model architecture with Mamba layers that perform\nconstant computation and require constant memory per generated token. We show\nthat Nemotron-H models offer either better or on-par accuracy compared to other\nsimilarly-sized state-of-the-art open-sourced Transformer models (e.g.,\nQwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3$\\times$ faster at\ninference. To further increase inference speed and reduce the memory required\nat inference time, we created Nemotron-H-47B-Base from the 56B model using a\nnew compression via pruning and distillation technique called MiniPuzzle.\nNemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20%\nfaster to infer. In addition, we introduce an FP8-based training recipe and\nshow that it can achieve on par results with BF16-based training. This recipe\nis used to train the 56B model. We are releasing Nemotron-H base model\ncheckpoints with support in Hugging Face and NeMo.",
      "tldr_zh": "本研究引入了Nemotron-H系列模型，包括8B和56B/47B的混合Mamba-Transformer架构，旨在降低推理成本同时保持高准确性。论文通过替换Transformer模型中的大部分自注意力层为Mamba层，实现每生成标记的计算和内存消耗恒定，从而使Nemotron-H在准确性上与类似规模的开源模型（如Qwen-2.5-7B/72B和Llama-3.1-8B/70B）相当或优于它们，同时推理速度提高至3倍。此外，研究提出MiniPuzzle压缩技术，从56B模型优化出Nemotron-H-47B-Base，提升20%的推理速度，并采用FP8-based训练方法与BF16-based训练结果相当，最终开源模型检查点以支持Hugging Face和NeMo。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03624v3",
      "published_date": "2025-04-04 17:41:58 UTC",
      "updated_date": "2025-04-15 14:36:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:34:38.344702"
    },
    {
      "arxiv_id": "2504.03622v1",
      "title": "Align to Structure: Aligning Large Language Models with Structural Information",
      "title_zh": "翻译失败",
      "authors": [
        "Zae Myung Kim",
        "Anand Ramachandran",
        "Farideh Tavazoee",
        "Joo-Kyung Kim",
        "Oleg Rokhlenko",
        "Dongyeop Kang"
      ],
      "abstract": "Generating long, coherent text remains a challenge for large language models\n(LLMs), as they lack hierarchical planning and structured organization in\ndiscourse generation. We introduce Structural Alignment, a novel method that\naligns LLMs with human-like discourse structures to enhance long-form text\ngeneration. By integrating linguistically grounded discourse frameworks into\nreinforcement learning, our approach guides models to produce coherent and\nwell-organized outputs. We employ a dense reward scheme within a Proximal\nPolicy Optimization framework, assigning fine-grained, token-level rewards\nbased on the discourse distinctiveness relative to human writing. Two\ncomplementary reward models are evaluated: the first improves readability by\nscoring surface-level textual features to provide explicit structuring, while\nthe second reinforces deeper coherence and rhetorical sophistication by\nanalyzing global discourse patterns through hierarchical discourse motifs,\noutperforming both standard and RLHF-enhanced models in tasks such as essay\ngeneration and long-document summarization. All training data and code will be\npublicly shared at https://github.com/minnesotanlp/struct_align.",
      "tldr_zh": "本研究针对大型语言模型（Large Language Models, LLMs）在生成长而连贯文本时存在的层次规划和结构组织不足问题，提出 Structural Alignment 方法，通过强化学习（Reinforcement Learning）整合语言话语框架来提升输出质量。该方法采用 Proximal Policy Optimization (PPO) 框架和密集奖励方案（dense reward scheme），基于 token-level 奖励评估话语独特性，包括一个关注表面文本特征以改善可读性的奖励模型，以及另一个分析层次化话语模式（hierarchical discourse motifs）以强化深层连贯性和修辞复杂性的模型。实验结果显示，该方法在作文生成和长文档摘要任务中，优于标准模型和 RLHF-enhanced 模型；所有训练数据和代码将在 GitHub 上公开分享。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03622v1",
      "published_date": "2025-04-04 17:40:04 UTC",
      "updated_date": "2025-04-04 17:40:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:34:50.407341"
    },
    {
      "arxiv_id": "2504.03822v1",
      "title": "Arti-\"fickle\" Intelligence: Using LLMs as a Tool for Inference in the Political and Social Sciences",
      "title_zh": "翻译失败",
      "authors": [
        "Lisa P. Argyle",
        "Ethan C. Busby",
        "Joshua R. Gubler",
        "Bryce Hepner",
        "Alex Lyman",
        "David Wingate"
      ],
      "abstract": "Generative large language models (LLMs) are incredibly useful, versatile, and\npromising tools. However, they will be of most use to political and social\nscience researchers when they are used in a way that advances understanding\nabout real human behaviors and concerns. To promote the scientific use of LLMs,\nwe suggest that researchers in the political and social sciences need to remain\nfocused on the scientific goal of inference. To this end, we discuss the\nchallenges and opportunities related to scientific inference with LLMs, using\nvalidation of model output as an illustrative case for discussion. We propose a\nset of guidelines related to establishing the failure and success of LLMs when\ncompleting particular tasks, and discuss how we can make inferences from these\nobservations. We conclude with a discussion of how this refocus will improve\nthe accumulation of shared scientific knowledge about these tools and their\nuses in the social sciences.",
      "tldr_zh": "本论文探讨了如何在政治和社会科学中使用生成式大型语言模型（LLMs）作为工具，以推进对真实人类行为和关切的科学推理。作者分析了LLMs在科学推理中的挑战和机会，并以模型输出验证为例，提出一套指导原则来评估LLMs在特定任务中的成功与失败。最终，通过这些方法，研究人员可以从观察中得出可靠的推论，从而促进社会科学领域关于LLMs工具的共享科学知识积累。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03822v1",
      "published_date": "2025-04-04 17:35:45 UTC",
      "updated_date": "2025-04-04 17:35:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:35:01.771078"
    },
    {
      "arxiv_id": "2504.03616v1",
      "title": "Multilingual Retrieval-Augmented Generation for Knowledge-Intensive Task",
      "title_zh": "多语言检索增强生成用于知识密集型任务",
      "authors": [
        "Leonardo Ranaldi",
        "Barry Haddow",
        "Alexandra Birch"
      ],
      "abstract": "Retrieval-augmented generation (RAG) has become a cornerstone of contemporary\nNLP, enhancing large language models (LLMs) by allowing them to access richer\nfactual contexts through in-context retrieval. While effective in monolingual\nsettings, especially in English, its use in multilingual tasks remains\nunexplored. This paper investigates the effectiveness of RAG across multiple\nlanguages by proposing novel approaches for multilingual open-domain\nquestion-answering. We evaluate the performance of various multilingual RAG\nstrategies, including question-translation (tRAG), which translates questions\ninto English before retrieval, and Multilingual RAG (MultiRAG), where retrieval\noccurs directly across multiple languages. Our findings reveal that tRAG, while\nuseful, suffers from limited coverage. In contrast, MultiRAG improves\nefficiency by enabling multilingual retrieval but introduces inconsistencies\ndue to cross-lingual variations in the retrieved content. To address these\nissues, we propose Crosslingual RAG (CrossRAG), a method that translates\nretrieved documents into a common language (e.g., English) before generating\nthe response. Our experiments show that CrossRAG significantly enhances\nperformance on knowledge-intensive tasks, benefiting both high-resource and\nlow-resource languages.",
      "tldr_zh": "这篇论文探讨了检索增强生成（RAG）在多语种知识密集型任务中的应用，旨在解决其在单语环境（如英语）有效但多语场景未被充分探索的问题。论文提出并评估了多种策略，包括问题翻译策略（tRAG）、多语检索策略（MultiRAG）和跨语言策略（CrossRAG），其中 CrossRAG 通过将检索文档翻译成共同语言（如英语）后再生成响应，缓解了跨语言不一致性。实验结果表明，CrossRAG 显著提升了开放域问答的性能，尤其对高资源和低资源语言有益。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03616v1",
      "published_date": "2025-04-04 17:35:43 UTC",
      "updated_date": "2025-04-04 17:35:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:35:14.033062"
    },
    {
      "arxiv_id": "2504.03615v1",
      "title": "Autonomous and Self-Adapting System for Synthetic Media Detection and Attribution",
      "title_zh": "自治且自我适应的合成媒体检测和归因系统",
      "authors": [
        "Aref Azizpour",
        "Tai D. Nguyen",
        "Matthew C. Stamm"
      ],
      "abstract": "Rapid advances in generative AI have enabled the creation of highly realistic\nsynthetic images, which, while beneficial in many domains, also pose serious\nrisks in terms of disinformation, fraud, and other malicious applications.\nCurrent synthetic image identification systems are typically static, relying on\nfeature representations learned from known generators; as new generative models\nemerge, these systems suffer from severe performance degradation. In this\npaper, we introduce the concept of an autonomous self-adaptive synthetic media\nidentification system -- one that not only detects synthetic images and\nattributes them to known sources but also autonomously identifies and\nincorporates novel generators without human intervention. Our approach\nleverages an open-set identification strategy with an evolvable embedding space\nthat distinguishes between known and unknown sources. By employing an\nunsupervised clustering method to aggregate unknown samples into\nhigh-confidence clusters and continuously refining its decision boundaries, our\nsystem maintains robust detection and attribution performance even as the\ngenerative landscape evolves. Extensive experiments demonstrate that our method\nsignificantly outperforms existing approaches, marking a crucial step toward\nuniversal, adaptable forensic systems in the era of rapidly advancing\ngenerative models.",
      "tldr_zh": "这篇论文提出了一种自主自适应系统，用于检测和归因合成媒体，以应对generative AI快速发展的挑战，该系统能自动识别合成图像、归因到已知来源，并自主整合新型生成器，而无需人工干预。方法采用开放集识别策略(open-set identification)和可演化嵌入空间，通过无监督聚类将未知样本聚类并持续优化决策边界，确保在生成模型演变时保持鲁棒性能。实验结果表明，该系统显著优于现有方法，为generative AI时代提供通用、可适应的取证系统。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03615v1",
      "published_date": "2025-04-04 17:33:59 UTC",
      "updated_date": "2025-04-04 17:33:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:35:25.168269"
    },
    {
      "arxiv_id": "2504.03603v1",
      "title": "Towards deployment-centric multimodal AI beyond vision and language",
      "title_zh": "超越视觉和语言的以部署为中心的多模态人工智能迈进",
      "authors": [
        "Xianyuan Liu",
        "Jiayang Zhang",
        "Shuo Zhou",
        "Thijs L. van der Plas",
        "Avish Vijayaraghavan",
        "Anastasiia Grishina",
        "Mengdie Zhuang",
        "Daniel Schofield",
        "Christopher Tomlinson",
        "Yuhan Wang",
        "Ruizhe Li",
        "Louisa van Zeeland",
        "Sina Tabakhi",
        "Cyndie Demeocq",
        "Xiang Li",
        "Arunav Das",
        "Orlando Timmerman",
        "Thomas Baldwin-McDonald",
        "Jinge Wu",
        "Peizhen Bai",
        "Zahraa Al Sahili",
        "Omnia Alwazzan",
        "Thao N. Do",
        "Mohammod N. I. Suvon",
        "Angeline Wang",
        "Lucia Cipolina-Kun",
        "Luigi A. Moretti",
        "Lucas Farndale",
        "Nitisha Jain",
        "Natalia Efremova",
        "Yan Ge",
        "Marta Varela",
        "Hak-Keung Lam",
        "Oya Celiktutan",
        "Ben R. Evans",
        "Alejandro Coca-Castro",
        "Honghan Wu",
        "Zahraa S. Abdallah",
        "Chen Chen",
        "Valentin Danchev",
        "Nataliya Tkachenko",
        "Lei Lu",
        "Tingting Zhu",
        "Gregory G. Slabaugh",
        "Roger K. Moore",
        "William K. Cheung",
        "Peter H. Charlton",
        "Haiping Lu"
      ],
      "abstract": "Multimodal artificial intelligence (AI) integrates diverse types of data via\nmachine learning to improve understanding, prediction, and decision-making\nacross disciplines such as healthcare, science, and engineering. However, most\nmultimodal AI advances focus on models for vision and language data, while\ntheir deployability remains a key challenge. We advocate a deployment-centric\nworkflow that incorporates deployment constraints early to reduce the\nlikelihood of undeployable solutions, complementing data-centric and\nmodel-centric approaches. We also emphasise deeper integration across multiple\nlevels of multimodality and multidisciplinary collaboration to significantly\nbroaden the research scope beyond vision and language. To facilitate this\napproach, we identify common multimodal-AI-specific challenges shared across\ndisciplines and examine three real-world use cases: pandemic response,\nself-driving car design, and climate change adaptation, drawing expertise from\nhealthcare, social science, engineering, science, sustainability, and finance.\nBy fostering multidisciplinary dialogue and open research practices, our\ncommunity can accelerate deployment-centric development for broad societal\nimpact.",
      "tldr_zh": "这篇论文探讨了多模态 AI 的发展，强调需要超越视觉和语言数据，关注部署性问题，以提升其在医疗、科学和工程等领域的实际应用。作者提出以 deployment-centric 工作流程为核心，将部署约束提前整合到开发过程中，补充数据-centric 和 model-centric 方法，从而减少不可部署解决方案的风险。同时，通过识别多模态 AI 共性挑战并分析疫情响应、自动驾驶车设计和气候变化适应的真实案例，论文呼吁加强多学科合作和开放研究实践，以加速多模态 AI 的部署并实现更广泛的社会影响。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03603v1",
      "published_date": "2025-04-04 17:20:05 UTC",
      "updated_date": "2025-04-04 17:20:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:35:36.428337"
    },
    {
      "arxiv_id": "2504.03601v3",
      "title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay",
      "title_zh": "翻译失败",
      "authors": [
        "Akshara Prabhakar",
        "Zuxin Liu",
        "Ming Zhu",
        "Jianguo Zhang",
        "Tulika Awalgaonkar",
        "Shiyu Wang",
        "Zhiwei Liu",
        "Haolin Chen",
        "Thai Hoang",
        "Juan Carlos Niebles",
        "Shelby Heinecke",
        "Weiran Yao",
        "Huan Wang",
        "Silvio Savarese",
        "Caiming Xiong"
      ],
      "abstract": "Training effective AI agents for multi-turn interactions requires\nhigh-quality data that captures realistic human-agent dynamics, yet such data\nis scarce and expensive to collect manually. We introduce APIGen-MT, a\ntwo-phase framework that generates verifiable and diverse multi-turn agent\ndata. In the first phase, our agentic pipeline produces detailed task\nblueprints with ground-truth actions, leveraging a committee of LLM reviewers\nand iterative feedback loops. These blueprints are then transformed into\ncomplete interaction trajectories through simulated human-agent interplay. We\ntrain a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B\nto 70B parameters. Our models outperform frontier models such as GPT-4o and\nClaude 3.5 on $\\tau$-bench and BFCL benchmarks, with the smaller models\nsurpassing their larger counterparts, particularly in multi-turn settings,\nwhile maintaining superior consistency across multiple trials. Comprehensive\nexperiments demonstrate that our verified blueprint-to-details approach yields\nhigh-quality training data, enabling the development of more reliable,\nefficient, and capable agents. We open-source 5K synthetic data trajectories\nand the trained xLAM-2-fc-r models to advance research in AI agents.\n  Models at\nhttps://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4;\nDataset at https://huggingface.co/datasets/Salesforce/APIGen-MT-5k and Website\nat https://apigen-mt.github.io",
      "tldr_zh": "该研究引入了APIGen-MT框架，一种基于代理管道的系统，用于生成高质量的多轮代理数据，通过模拟代理-人类互动来解决真实互动数据稀缺的问题。在第一阶段，该框架利用LLM reviewers和迭代反馈循环创建详细的任务蓝图（task blueprints）并包含ground-truth actions；第二阶段则将这些蓝图转化为完整的互动轨迹（interaction trajectories）。训练的xLAM-2-fc-r模型系列（从1B到70B参数）在τ-bench和BFCL基准上超越了GPT-4o和Claude 3.5，特别是小模型在多轮设置中表现出色，并提升了试验一致性。该框架开源了5K合成数据轨迹和模型，促进AI代理研究的进展。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages plus references and appendices",
      "pdf_url": "http://arxiv.org/pdf/2504.03601v3",
      "published_date": "2025-04-04 17:13:57 UTC",
      "updated_date": "2025-05-05 11:54:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:35:50.183666"
    },
    {
      "arxiv_id": "2504.03600v1",
      "title": "MedSAM2: Segment Anything in 3D Medical Images and Videos",
      "title_zh": "翻译失败",
      "authors": [
        "Jun Ma",
        "Zongxin Yang",
        "Sumin Kim",
        "Bihui Chen",
        "Mohammed Baharoon",
        "Adibvafa Fallahpour",
        "Reza Asakereh",
        "Hongwei Lyu",
        "Bo Wang"
      ],
      "abstract": "Medical image and video segmentation is a critical task for precision\nmedicine, which has witnessed considerable progress in developing task or\nmodality-specific and generalist models for 2D images. However, there have been\nlimited studies on building general-purpose models for 3D images and videos\nwith comprehensive user studies. Here, we present MedSAM2, a promptable\nsegmentation foundation model for 3D image and video segmentation. The model is\ndeveloped by fine-tuning the Segment Anything Model 2 on a large medical\ndataset with over 455,000 3D image-mask pairs and 76,000 frames, outperforming\nprevious models across a wide range of organs, lesions, and imaging modalities.\nFurthermore, we implement a human-in-the-loop pipeline to facilitate the\ncreation of large-scale datasets resulting in, to the best of our knowledge,\nthe most extensive user study to date, involving the annotation of 5,000 CT\nlesions, 3,984 liver MRI lesions, and 251,550 echocardiogram video frames,\ndemonstrating that MedSAM2 can reduce manual costs by more than 85%. MedSAM2 is\nalso integrated into widely used platforms with user-friendly interfaces for\nlocal and cloud deployment, making it a practical tool for supporting\nefficient, scalable, and high-quality segmentation in both research and\nhealthcare environments.",
      "tldr_zh": "该研究介绍了MedSAM2，一种可prompt的分割基础模型，旨在实现3D医疗图像和视频的通用分割，填补了现有模型在3D领域的空白。MedSAM2通过在Segment Anything Model 2的基础上fine-tuning一个包含超过455,000个3D图像-掩码对和76,000帧的大型医疗数据集，显著优于先前的模型，在各种器官、病变和成像模式下表现出色。此外，通过human-in-the-loop管道进行的广泛用户研究显示，MedSAM2可以将手动标注成本减少超过85%，并已集成到用户友好的平台中，支持研究和医疗环境的高效部署。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "https://medsam2.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2504.03600v1",
      "published_date": "2025-04-04 17:13:37 UTC",
      "updated_date": "2025-04-04 17:13:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:36:01.713455"
    },
    {
      "arxiv_id": "2504.03598v1",
      "title": "EnrichIndex: Using LLMs to Enrich Retrieval Indices Offline",
      "title_zh": "EnrichIndex：使用 LLMs 离线丰富检索索引",
      "authors": [
        "Peter Baile Chen",
        "Tomer Wolfson",
        "Michael Cafarella",
        "Dan Roth"
      ],
      "abstract": "Existing information retrieval systems excel in cases where the language of\ntarget documents closely matches that of the user query. However, real-world\nretrieval systems are often required to implicitly reason whether a document is\nrelevant. For example, when retrieving technical texts or tables, their\nrelevance to the user query may be implied through a particular jargon or\nstructure, rather than explicitly expressed in their content. Large language\nmodels (LLMs) hold great potential in identifying such implied relevance by\nleveraging their reasoning skills. Nevertheless, current LLM-augmented\nretrieval is hindered by high latency and computation cost, as the LLM\ntypically computes the query-document relevance online, for every query anew.\nTo tackle this issue we introduce EnrichIndex, a retrieval approach which\ninstead uses the LLM offline to build semantically-enriched retrieval indices,\nby performing a single pass over all documents in the retrieval corpus once\nduring ingestion time. Furthermore, the semantically-enriched indices can\ncomplement existing online retrieval approaches, boosting the performance of\nLLM re-rankers. We evaluated EnrichIndex on five retrieval tasks, involving\npassages and tables, and found that it outperforms strong online LLM-based\nretrieval systems, with an average improvement of 11.7 points in recall @ 10\nand 10.6 points in NDCG @ 10 compared to strong baselines. In terms of online\ncalls to the LLM, it processes 293.3 times fewer tokens which greatly reduces\nthe online latency and cost. Overall, EnrichIndex is an effective way to build\nbetter retrieval indices offline by leveraging the strong reasoning skills of\nLLMs.",
      "tldr_zh": "该研究提出 EnrichIndex，一种利用大型语言模型(LLMs)离线构建语义丰富检索索引的方法，以解决现有信息检索系统在处理查询与文档隐含相关性（如技术术语或结构）时的局限性。不同于在线计算，EnrichIndex 通过对所有文档进行单次扫描，生成增强索引，从而减少在线延迟和计算成本，同时可补充现有在线检索方法。实验在五个涉及段落和表格的检索任务上显示，EnrichIndex 比强基线提升了 11.7 点 recall @ 10 和 10.6 点 NDCG @ 10，并减少了 293.3 倍的在线 LLM 令牌处理，证明了其高效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "Dataset and code are available at\n  https://peterbaile.github.io/enrichindex/",
      "pdf_url": "http://arxiv.org/pdf/2504.03598v1",
      "published_date": "2025-04-04 17:08:46 UTC",
      "updated_date": "2025-04-04 17:08:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:36:14.401972"
    },
    {
      "arxiv_id": "2504.03597v1",
      "title": "Real-is-Sim: Bridging the Sim-to-Real Gap with a Dynamic Digital Twin for Real-World Robot Policy Evaluation",
      "title_zh": "Real-is-Sim：利用动态数字孪生桥接模拟",
      "authors": [
        "Jad Abou-Chakra",
        "Lingfeng Sun",
        "Krishan Rana",
        "Brandon May",
        "Karl Schmeckpeper",
        "Maria Vittoria Minniti",
        "Laura Herlant"
      ],
      "abstract": "Recent advancements in behavior cloning have enabled robots to perform\ncomplex manipulation tasks. However, accurately assessing training performance\nremains challenging, particularly for real-world applications, as behavior\ncloning losses often correlate poorly with actual task success. Consequently,\nresearchers resort to success rate metrics derived from costly and\ntime-consuming real-world evaluations, making the identification of optimal\npolicies and detection of overfitting or underfitting impractical. To address\nthese issues, we propose real-is-sim, a novel behavior cloning framework that\nincorporates a dynamic digital twin (based on Embodied Gaussians) throughout\nthe entire policy development pipeline: data collection, training, and\ndeployment. By continuously aligning the simulated world with the physical\nworld, demonstrations can be collected in the real world with states extracted\nfrom the simulator. The simulator enables flexible state representations by\nrendering image inputs from any viewpoint or extracting low-level state\ninformation from objects embodied within the scene. During training, policies\ncan be directly evaluated within the simulator in an offline and highly\nparallelizable manner. Finally, during deployment, policies are run within the\nsimulator where the real robot directly tracks the simulated robot's joints,\neffectively decoupling policy execution from real hardware and mitigating\ntraditional domain-transfer challenges. We validate real-is-sim on the PushT\nmanipulation task, demonstrating strong correlation between success rates\nobtained in the simulator and real-world evaluations. Videos of our system can\nbe found at https://realissim.rai-inst.com.",
      "tldr_zh": "这篇论文提出 real-is-sim 框架，利用动态 digital twin（基于 Embodied Gaussians）来桥接 Sim-to-Real Gap，提升机器人 behavior cloning 策略的真实世界评估。框架将 digital twin 整合到数据收集、训练和部署流程中，通过持续对齐模拟与物理世界，实现灵活的状态表示和离线并行评估策略，并在部署时让真实机器人跟踪模拟机器人的关节动作，以减少域转移问题。在 PushT 操纵任务上验证显示，模拟器成功率与真实世界评估高度相关，显著降低了评估成本和时间。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03597v1",
      "published_date": "2025-04-04 17:05:56 UTC",
      "updated_date": "2025-04-04 17:05:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:36:26.368818"
    },
    {
      "arxiv_id": "2504.03561v1",
      "title": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement",
      "title_zh": "翻译失败",
      "authors": [
        "Runnan Fang",
        "Xiaobin Wang",
        "Yuan Liang",
        "Shuofei Qiao",
        "Jialong Wu",
        "Zekun Xi",
        "Ningyu Zhang",
        "Yong Jiang",
        "Pengjun Xie",
        "Fei Huang",
        "Huajun Chen"
      ],
      "abstract": "In the interaction between agents and their environments, agents expand their\ncapabilities by planning and executing actions. However, LLM-based agents face\nsubstantial challenges when deployed in novel environments or required to\nnavigate unconventional action spaces. To empower agents to autonomously\nexplore environments, optimize workflows, and enhance their understanding of\nactions, we propose SynWorld, a framework that allows agents to synthesize\npossible scenarios with multi-step action invocation within the action space\nand perform Monte Carlo Tree Search (MCTS) exploration to effectively refine\ntheir action knowledge in the current environment. Our experiments demonstrate\nthat SynWorld is an effective and general approach to learning action knowledge\nin new environments. Code is available at https://github.com/zjunlp/SynWorld.",
      "tldr_zh": "本文提出SynWorld框架，旨在帮助LLM-based agents在新型环境或非传统动作空间中提升动作知识，通过合成虚拟场景、多步动作调用和Monte Carlo Tree Search (MCTS)探索来实现代理的自主环境探索和工作流优化。实验结果显示，SynWorld是一种有效且通用的方法，能够显著改进agents在新环境中的动作学习表现。开源代码可从https://github.com/zjunlp/SynWorld获取。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "Work in progress",
      "pdf_url": "http://arxiv.org/pdf/2504.03561v1",
      "published_date": "2025-04-04 16:10:57 UTC",
      "updated_date": "2025-04-04 16:10:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:36:36.891119"
    },
    {
      "arxiv_id": "2504.03553v1",
      "title": "Agentic Knowledgeable Self-awareness",
      "title_zh": "翻译失败",
      "authors": [
        "Shuofei Qiao",
        "Zhisong Qiu",
        "Baochang Ren",
        "Xiaobin Wang",
        "Xiangyuan Ru",
        "Ningyu Zhang",
        "Xiang Chen",
        "Yong Jiang",
        "Pengjun Xie",
        "Fei Huang",
        "Huajun Chen"
      ],
      "abstract": "Large Language Models (LLMs) have achieved considerable performance across\nvarious agentic planning tasks. However, traditional agent planning approaches\nadopt a \"flood irrigation\" methodology that indiscriminately injects gold\ntrajectories, external feedback, and domain knowledge into agent models. This\npractice overlooks the fundamental human cognitive principle of situational\nself-awareness during decision-making-the ability to dynamically assess\nsituational demands and strategically employ resources during decision-making.\nWe propose agentic knowledgeable self-awareness to address this gap, a novel\nparadigm enabling LLM-based agents to autonomously regulate knowledge\nutilization. Specifically, we propose KnowSelf, a data-centric approach that\napplies agents with knowledgeable self-awareness like humans. Concretely, we\ndevise a heuristic situation judgement criterion to mark special tokens on the\nagent's self-explored trajectories for collecting training data. Through a\ntwo-stage training process, the agent model can switch between different\nsituations by generating specific special tokens, achieving optimal planning\neffects with minimal costs. Our experiments demonstrate that KnowSelf can\noutperform various strong baselines on different tasks and models with minimal\nuse of external knowledge. Code is available at\nhttps://github.com/zjunlp/KnowSelf.",
      "tldr_zh": "这篇论文提出agentic knowledgeable self-awareness范式，旨在解决大型语言模型(LLMs)在代理规划任务中过度注入知识的问题，使代理能像人类一样动态评估情况并战略使用资源。具体而言，作者开发了KnowSelf方法，通过启发式情况判断标准标记代理的自探索轨迹，并采用两阶段训练，让模型通过生成特殊标记在不同情境间切换，实现高效规划。实验结果表明，KnowSelf在多种任务和模型上优于强基线，同时最小化外部知识使用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "Work in progress",
      "pdf_url": "http://arxiv.org/pdf/2504.03553v1",
      "published_date": "2025-04-04 16:03:38 UTC",
      "updated_date": "2025-04-04 16:03:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:36:49.130967"
    },
    {
      "arxiv_id": "2504.03818v1",
      "title": "Exploring Various Sequential Learning Methods for Deformation History Modeling",
      "title_zh": "翻译失败",
      "authors": [
        "Muhammed Adil Yatkin",
        "Mihkel Korgesaar",
        "Jani Romanoff",
        "Umit Islak",
        "Hasan Kurban"
      ],
      "abstract": "Current neural network (NN) models can learn patterns from data points with\nhistorical dependence. Specifically, in natural language processing (NLP),\nsequential learning has transitioned from recurrence-based architectures to\ntransformer-based architectures. However, it is unknown which NN architectures\nwill perform the best on datasets containing deformation history due to\nmechanical loading. Thus, this study ascertains the appropriateness of\n1D-convolutional, recurrent, and transformer-based architectures for predicting\ndeformation localization based on the earlier states in the form of deformation\nhistory. Following this investigation, the crucial incompatibility issues\nbetween the mathematical computation of the prediction process in the\nbest-performing NN architectures and the actual values derived from the natural\nphysical properties of the deformation paths are examined in detail.",
      "tldr_zh": "这篇论文探讨了各种顺序学习方法在变形历史建模中的适用性，旨在评估神经网络(NN)架构在处理机械加载相关数据集时的性能。研究比较了1D-convolutional、recurrent和transformer-based架构，用于基于早期变形状态预测变形局部化。结果表明，虽然某些架构表现出色，但其预测过程与变形路径的自然物理属性存在关键不兼容问题，为未来改进提供了重要见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "Engineering Applications of Neural Networks",
      "pdf_url": "http://arxiv.org/pdf/2504.03818v1",
      "published_date": "2025-04-04 15:52:24 UTC",
      "updated_date": "2025-04-04 15:52:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:37:01.560979"
    },
    {
      "arxiv_id": "2504.03546v1",
      "title": "MultiMed-ST: Large-scale Many-to-many Multilingual Medical Speech Translation",
      "title_zh": "MultiMed-ST：大规模多对多多语言医疗语音翻译",
      "authors": [
        "Khai Le-Duc",
        "Tuyen Tran",
        "Bach Phan Tat",
        "Nguyen Kim Hai Bui",
        "Quan Dang",
        "Hung-Phong Tran",
        "Thanh-Thuy Nguyen",
        "Ly Nguyen",
        "Tuan-Minh Phan",
        "Thi Thu Phuong Tran",
        "Chris Ngo",
        "Nguyen X. Khanh",
        "Thanh Nguyen-Tang"
      ],
      "abstract": "Multilingual speech translation (ST) in the medical domain enhances patient\ncare by enabling efficient communication across language barriers, alleviating\nspecialized workforce shortages, and facilitating improved diagnosis and\ntreatment, particularly during pandemics. In this work, we present the first\nsystematic study on medical ST, to our best knowledge, by releasing\nMultiMed-ST, a large-scale ST dataset for the medical domain, spanning all\ntranslation directions in five languages: Vietnamese, English, German, French,\nTraditional Chinese and Simplified Chinese, together with the models. With\n290,000 samples, our dataset is the largest medical machine translation (MT)\ndataset and the largest many-to-many multilingual ST among all domains.\nSecondly, we present the most extensive analysis study in ST research to date,\nincluding: empirical baselines, bilingual-multilingual comparative study,\nend-to-end vs. cascaded comparative study, task-specific vs. multi-task\nsequence-to-sequence (seq2seq) comparative study, code-switch analysis, and\nquantitative-qualitative error analysis. All code, data, and models are\navailable online: https://github.com/leduckhai/MultiMed-ST.",
      "tldr_zh": "本研究介绍了 MultiMed-ST，这是一个大规模的多对多多语言医疗语音翻译（ST）数据集，旨在提升跨语言沟通以改善患者护理、缓解医疗劳动力短缺。该数据集涵盖五种语言（越南语、英语、德语、法语、繁体中文和简体中文）的所有翻译方向，总计29万样本，是目前最大的医疗机器翻译（MT）数据集和多对多多语言ST数据集。论文进行了全面分析，包括基准测试、双语 vs 多语比较、端到端 vs 级联系统比较、任务特定 vs 多任务 seq2seq 模型比较、代码切换分析以及定量-定性错误分析，所有代码、数据和模型均在GitHub上公开。该工作为医疗领域的多语言ST研究奠定了基础，有助于在疫情等场景中提升诊断和治疗效率。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint, 122 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.03546v1",
      "published_date": "2025-04-04 15:49:17 UTC",
      "updated_date": "2025-04-04 15:49:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:37:14.007955"
    },
    {
      "arxiv_id": "2504.03531v1",
      "title": "Dense Neural Network Based Arrhythmia Classification on Low-cost and Low-compute Micro-controller",
      "title_zh": "翻译失败",
      "authors": [
        "Md Abu Obaida Zishan",
        "H M Shihab",
        "Sabik Sadman Islam",
        "Maliha Alam Riya",
        "Gazi Mashrur Rahman",
        "Jannatun Noor"
      ],
      "abstract": "The electrocardiogram (ECG) monitoring device is an expensive albeit\nessential device for the treatment and diagnosis of cardiovascular diseases\n(CVD). The cost of this device typically ranges from $2000 to $10000. Several\nstudies have implemented ECG monitoring systems in micro-controller units (MCU)\nto reduce industrial development costs by up to 20 times. However, to match\nindustry-grade systems and display heartbeats effectively, it is essential to\ndevelop an efficient algorithm for detecting arrhythmia (irregular heartbeat).\nHence in this study, a dense neural network is developed to detect arrhythmia\non the Arduino Nano. The Nano consists of the ATMega328 microcontroller with a\n16MHz clock, 2KB of SRAM, and 32KB of program memory. Additionally, the AD8232\nSparkFun Single-Lead Heart Rate Monitor is used as the ECG sensor. The\nimplemented neural network model consists of two layers (excluding the input)\nwith 10 and four neurons respectively with sigmoid activation function.\nHowever, four approaches are explored to choose the appropriate activation\nfunctions. The model has a size of 1.267 KB, achieves an F1 score\n(macro-average) of 78.3\\% for classifying four types of arrhythmia, an accuracy\nrate of 96.38%, and requires 0.001314 MOps of floating-point operations\n(FLOPs).",
      "tldr_zh": "本研究针对心电图 (ECG) 监测设备的昂贵问题，提出了一种基于 Dense Neural Network 的心律失常 (Arrhythmia) 分类算法，旨在在低成本、低计算的微控制器 (如 Arduino Nano) 上实现高效检测。模型采用两层结构（10 和 4 个神经元，使用 Sigmoid 激活函数），并探索了四种激活函数方法，以适应硬件限制。实验结果显示，该模型大小仅 1.267 KB，F1 score (宏平均) 为 78.3%、准确率达 96.38%，并仅需 0.001314 MOps 的浮点运算 (FLOPs)，显著降低了 ECG 监测系统的开发成本。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.1; I.2.6; C.3"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03531v1",
      "published_date": "2025-04-04 15:30:02 UTC",
      "updated_date": "2025-04-04 15:30:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:37:25.883103"
    },
    {
      "arxiv_id": "2504.05333v1",
      "title": "When is using AI the rational choice? The importance of counterfactuals in AI deployment decisions",
      "title_zh": "使用 AI 何时是理性选择？反事实在 AI 部署决策中的重要性",
      "authors": [
        "Paul Lehner",
        "Elinor Yeo"
      ],
      "abstract": "Decisions to deploy AI capabilities are often driven by counterfactuals - a\ncomparison of decisions made using AI to decisions that would have been made if\nthe AI were not used. Counterfactual misses, which are poor decisions that are\nattributable to using AI, may have disproportionate disutility to AI deployment\ndecision makers. Counterfactual hits, which are good decisions attributable to\nAI usage, may provide little benefit beyond the benefit of better decisions.\nThis paper explores how to include counterfactual outcomes into usage decision\nexpected utility assessments. Several properties emerge when counterfactuals\nare explicitly included. First, there are many contexts where the expected\nutility of AI usage is positive for intended beneficiaries and strongly\nnegative for stakeholders and deployment decision makers. Second, high levels\nof complementarity, where differing AI and user assessments are merged\nbeneficially, often leads to substantial disutility for stakeholders. Third,\napparently small changes in how users interact with an AI capability can\nsubstantially impact stakeholder utility. Fourth, cognitive biases such as\nexpert overconfidence and hindsight bias exacerbate the perceived frequency of\ncostly counterfactual misses. The expected utility assessment approach\npresented here is intended to help AI developers and deployment decision makers\nto navigate the subtle but substantial impact of counterfactuals so as to\nbetter ensure that beneficial AI capabilities are used.",
      "tldr_zh": "这篇论文探讨了在 AI 部署决策中，反事实（counterfactuals）的关键作用，即通过比较使用 AI 与不使用 AI 的决策结果来评估效用。作者提出将反事实结果纳入预期效用（expected utility）评估的方法，揭示了使用 AI 可能对受益者带来正面影响，但对利益相关者和决策者造成强烈负面效用，尤其在高互补性（complementarity）或认知偏差（如专家过度自信）的情况下。研究发现，用户与 AI 互动方式的微小变化可显著影响利益相关者效用，并强调这种评估框架有助于 AI 开发者和服务决策者更好地管理风险，确保有益 AI 的部署。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "31 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.05333v1",
      "published_date": "2025-04-04 14:59:29 UTC",
      "updated_date": "2025-04-04 14:59:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:37:39.103297"
    },
    {
      "arxiv_id": "2504.03494v1",
      "title": "Quantifying Robustness: A Benchmarking Framework for Deep Learning Forecasting in Cyber-Physical Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Alexander Windmann",
        "Henrik Steude",
        "Daniel Boschmann",
        "Oliver Niggemann"
      ],
      "abstract": "Cyber-Physical Systems (CPS) in domains such as manufacturing and energy\ndistribution generate complex time series data crucial for Prognostics and\nHealth Management (PHM). While Deep Learning (DL) methods have demonstrated\nstrong forecasting capabilities, their adoption in industrial CPS remains\nlimited due insufficient robustness. Existing robustness evaluations primarily\nfocus on formal verification or adversarial perturbations, inadequately\nrepresenting the complexities encountered in real-world CPS scenarios. To\naddress this, we introduce a practical robustness definition grounded in\ndistributional robustness, explicitly tailored to industrial CPS, and propose a\nsystematic framework for robustness evaluation. Our framework simulates\nrealistic disturbances, such as sensor drift, noise and irregular sampling,\nenabling thorough robustness analyses of forecasting models on real-world CPS\ndatasets. The robustness definition provides a standardized score to quantify\nand compare model performance across diverse datasets, assisting in informed\nmodel selection and architecture design. Through extensive empirical studies\nevaluating prominent DL architectures (including recurrent, convolutional,\nattention-based, modular, and structured state-space models) we demonstrate the\napplicability and effectiveness of our approach. We publicly release our\nrobustness benchmark to encourage further research and reproducibility.",
      "tldr_zh": "这篇论文针对 Cyber-Physical Systems (CPS) 中的 Deep Learning (DL) 预测模型，提出了一个量化鲁棒性的基准框架，以解决现有评估方法无法充分应对现实场景复杂性的问题。框架基于分布鲁棒性定义，模拟真实干扰如 sensor drift、noise 和 irregular sampling，在 Prognostics and Health Management (PHM) 相关数据集上进行系统评估，并提供标准化分数来比较模型性能。实验结果通过测试多种 DL 架构（如循环神经网络、卷积网络和注意力模型）证明了框架的有效性，并公开了基准以支持进一步研究和可重复性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03494v1",
      "published_date": "2025-04-04 14:50:48 UTC",
      "updated_date": "2025-04-04 14:50:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:37:50.884728"
    },
    {
      "arxiv_id": "2504.03490v1",
      "title": "BUFF: Bayesian Uncertainty Guided Diffusion Probabilistic Model for Single Image Super-Resolution",
      "title_zh": "翻译失败",
      "authors": [
        "Zihao He",
        "Shengchuan Zhang",
        "Runze Hu",
        "Yunhang Shen",
        "Yan Zhang"
      ],
      "abstract": "Super-resolution (SR) techniques are critical for enhancing image quality,\nparticularly in scenarios where high-resolution imagery is essential yet\nlimited by hardware constraints. Existing diffusion models for SR have relied\npredominantly on Gaussian models for noise generation, which often fall short\nwhen dealing with the complex and variable texture inherent in natural scenes.\nTo address these deficiencies, we introduce the Bayesian Uncertainty Guided\nDiffusion Probabilistic Model (BUFF). BUFF distinguishes itself by\nincorporating a Bayesian network to generate high-resolution uncertainty masks.\nThese masks guide the diffusion process, allowing for the adjustment of noise\nintensity in a manner that is both context-aware and adaptive. This novel\napproach not only enhances the fidelity of super-resolved images to their\noriginal high-resolution counterparts but also significantly mitigates\nartifacts and blurring in areas characterized by complex textures and fine\ndetails. The model demonstrates exceptional robustness against complex noise\npatterns and showcases superior adaptability in handling textures and edges\nwithin images. Empirical evidence, supported by visual results, illustrates the\nmodel's robustness, especially in challenging scenarios, and its effectiveness\nin addressing common SR issues such as blurring. Experimental evaluations\nconducted on the DIV2K dataset reveal that BUFF achieves a notable improvement,\nwith a +0.61 increase compared to baseline in SSIM on BSD100, surpassing\ntraditional diffusion approaches by an average additional +0.20dB PSNR gain.\nThese findings underscore the potential of Bayesian methods in enhancing\ndiffusion processes for SR, paving the way for future advancements in the\nfield.",
      "tldr_zh": "该论文提出BUFF（Bayesian Uncertainty Guided Diffusion Probabilistic Model），一种用于单图像超分辨率（SR）的创新扩散概率模型，通过整合Bayesian网络生成高分辨率不确定性掩码（uncertainty masks）来指导扩散过程。BUFF模型使噪声强度调整更具上下文感知和自适应性，从而提升超分辨图像的保真度，显著减少复杂纹理和细节区域的伪影与模糊。实验结果显示，在DIV2K数据集上，BUFF在BSD100上SSIM提升0.61，比传统扩散模型平均PSNR提高0.20dB，证明其在处理复杂噪声和纹理时的鲁棒性和优越性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T45",
        "I.2.10; J.0"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 5 figures, AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.03490v1",
      "published_date": "2025-04-04 14:43:45 UTC",
      "updated_date": "2025-04-04 14:43:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:38:02.435918"
    },
    {
      "arxiv_id": "2504.03486v1",
      "title": "Structured Legal Document Generation in India: A Model-Agnostic Wrapper Approach with VidhikDastaavej",
      "title_zh": "翻译失败",
      "authors": [
        "Shubham Kumar Nigam",
        "Balaramamahanthi Deepak Patnaik",
        "Ajay Varghese Thomas",
        "Noel Shallum",
        "Kripabandhu Ghosh",
        "Arnab Bhattacharya"
      ],
      "abstract": "Automating legal document drafting can significantly enhance efficiency,\nreduce manual effort, and streamline legal workflows. While prior research has\nexplored tasks such as judgment prediction and case summarization, the\nstructured generation of private legal documents in the Indian legal domain\nremains largely unaddressed. To bridge this gap, we introduce VidhikDastaavej,\na novel, anonymized dataset of private legal documents, and develop NyayaShilp,\na fine-tuned legal document generation model specifically adapted to Indian\nlegal texts. We propose a Model-Agnostic Wrapper (MAW), a two-step framework\nthat first generates structured section titles and then iteratively produces\ncontent while leveraging retrieval-based mechanisms to ensure coherence and\nfactual accuracy. We benchmark multiple open-source LLMs, including\ninstruction-tuned and domain-adapted versions, alongside proprietary models for\ncomparison. Our findings indicate that while direct fine-tuning on small\ndatasets does not always yield improvements, our structured wrapper\nsignificantly enhances coherence, factual adherence, and overall document\nquality while mitigating hallucinations. To ensure real-world applicability, we\ndeveloped a Human-in-the-Loop (HITL) Document Generation System, an interactive\nuser interface that enables users to specify document types, refine section\ndetails, and generate structured legal drafts. This tool allows legal\nprofessionals and researchers to generate, validate, and refine AI-generated\nlegal documents efficiently. Extensive evaluations, including expert\nassessments, confirm that our framework achieves high reliability in structured\nlegal drafting. This research establishes a scalable and adaptable foundation\nfor AI-assisted legal drafting in India, offering an effective approach to\nstructured legal document generation.",
      "tldr_zh": "本研究针对印度私法文档的结构化生成问题，引入了 VidhikDastaavej 数据集并开发了 NyayaShilp 模型，该模型针对印度法律文本进行微调，以桥接现有研究的空白。论文提出 Model-Agnostic Wrapper (MAW) 框架，该框架采用两步方法：先生成结构化部分标题，然后通过检索机制迭代产生内容，从而提升文档的连贯性、事实准确性和整体质量，同时减少幻觉。实验基准测试了多种开源和专有 LLM，结果表明 MAW 框架显著改善了生成效果，而直接微调并不总是有效。此外，研究开发了 Human-in-the-Loop (HITL) 系统，一个交互式界面，允许用户指定文档类型并优化草稿，通过专家评估证实了框架在实际法律起草中的高可靠性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03486v1",
      "published_date": "2025-04-04 14:41:50 UTC",
      "updated_date": "2025-04-04 14:41:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:38:15.889857"
    },
    {
      "arxiv_id": "2504.03814v2",
      "title": "Recursive Training Loops in LLMs: How training data properties modulate distribution shift in generated data?",
      "title_zh": "递归训练循环在 LLMs 中：训练数据属性如何调节生成数据中的分布偏移？",
      "authors": [
        "Grgur Kovač",
        "Jérémy Perez",
        "Rémy Portelas",
        "Peter Ford Dominey",
        "Pierre-Yves Oudeyer"
      ],
      "abstract": "Large language models (LLMs) are increasingly contributing to the creation of\ncontent on the Internet. This creates a feedback loop as subsequent generations\nof models will be trained on this generated, synthetic data. This phenomenon is\nreceiving increasing interest, in particular because previous studies have\nshown that it may lead to distribution shift - models misrepresent and forget\nthe true underlying distributions of human data they are expected to\napproximate (e.g. resulting in a drastic loss of quality). In this study, we\nstudy the impact of human data properties on distribution shift dynamics in\niterated training loops. We first confirm that the distribution shift dynamics\ngreatly vary depending on the human data by comparing four datasets (two based\non Twitter and two on Reddit). We then test whether data quality may influence\nthe rate of this shift. We find that it does on the twitter, but not on the\nReddit datasets. We then focus on a Reddit dataset and conduct a more\nexhaustive evaluation of a large set of dataset properties. This experiment\nassociated lexical diversity with larger, and semantic diversity with smaller\ndetrimental shifts, suggesting that incorporating text with high lexical (but\nlimited semantic) diversity could exacerbate the degradation of generated text.\nWe then focus on the evolution of political bias, and find that the type of\nshift observed (bias reduction, amplification or inversion) depends on the\npolitical lean of the human (true) distribution. Overall, our work extends the\nexisting literature on the consequences of recursive fine-tuning by showing\nthat this phenomenon is highly dependent on features of the human data on which\ntraining occurs. This suggests that different parts of internet (e.g. GitHub,\nReddit) may undergo different types of shift depending on their properties.",
      "tldr_zh": "本研究探讨了大型语言模型（LLMs）在递归训练循环中，人类数据属性如何影响生成的合成数据中的distribution shift。研究者比较了四个数据集（两个基于Twitter，两个基于Reddit），发现数据质量会加剧Twitter数据集的偏移，但对Reddit数据集影响不大。随后，通过对Reddit数据集的深入评估，揭示词汇多样性（lexical diversity）会放大负面偏移，而语义多样性（semantic diversity）则能减轻这种影响；此外，政治偏见（political bias）的演变（如减少、放大或逆转）取决于原始数据的政治倾向。总体而言，该工作强调了递归微调的后果高度依赖于人类数据特征，不同互联网平台（如GitHub或Reddit）可能经历不同的分布偏移类型。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "68T50",
        "I.2.7"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03814v2",
      "published_date": "2025-04-04 14:41:41 UTC",
      "updated_date": "2025-04-08 08:45:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:38:27.870824"
    },
    {
      "arxiv_id": "2504.03469v1",
      "title": "Physics-informed 4D X-ray image reconstruction from ultra-sparse spatiotemporal data",
      "title_zh": "翻译失败",
      "authors": [
        "Zisheng Yao",
        "Yuhe Zhang",
        "Zhe Hu",
        "Robert Klöfkorn",
        "Tobias Ritschel",
        "Pablo Villanueva-Perez"
      ],
      "abstract": "The unprecedented X-ray flux density provided by modern X-ray sources offers\nnew spatiotemporal possibilities for X-ray imaging of fast dynamic processes.\nApproaches to exploit such possibilities often result in either i) a limited\nnumber of projections or spatial information due to limited scanning speed, as\nin time-resolved tomography, or ii) a limited number of time points, as in\nstroboscopic imaging, making the reconstruction problem ill-posed and unlikely\nto be solved by classical reconstruction approaches. 4D reconstruction from\nsuch data requires sample priors, which can be included via deep learning (DL).\nState-of-the-art 4D reconstruction methods for X-ray imaging combine the power\nof AI and the physics of X-ray propagation to tackle the challenge of sparse\nviews. However, most approaches do not constrain the physics of the studied\nprocess, i.e., a full physical model. Here we present 4D physics-informed\noptimized neural implicit X-ray imaging (4D-PIONIX), a novel physics-informed\n4D X-ray image reconstruction method combining the full physical model and a\nstate-of-the-art DL-based reconstruction method for 4D X-ray imaging from\nsparse views. We demonstrate and evaluate the potential of our approach by\nretrieving 4D information from ultra-sparse spatiotemporal acquisitions of\nsimulated binary droplet collisions, a relevant fluid dynamic process. We\nenvision that this work will open new spatiotemporal possibilities for various\n4D X-ray imaging modalities, such as time-resolved X-ray tomography and more\nnovel sparse acquisition approaches like X-ray multi-projection imaging, which\nwill pave the way for investigations of various rapid 4D dynamics, such as\nfluid dynamics and composite testing.",
      "tldr_zh": "该论文提出了一种名为4D-PIONIX的物理信息4D X-ray图像重建方法，旨在从超稀疏时空数据中重建快速动态过程的图像，解决传统方法的限制，如投影数量不足或时间点有限导致的重建问题不适定。方法结合了完整的X-ray传播物理模型和深度学习（DL）技术，实现对稀疏视图数据的精确4D重建。通过模拟二元液滴碰撞实验，4D-PIONIX展示了显著的性能提升，为时间分辨X-ray断层成像和多投影成像等应用打开了新时空可能性，推动流体动力学等领域的快速4D动态研究。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "physics.data-an"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03469v1",
      "published_date": "2025-04-04 14:18:51 UTC",
      "updated_date": "2025-04-04 14:18:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:38:40.253495"
    },
    {
      "arxiv_id": "2504.05331v2",
      "title": "Not someone, but something: Rethinking trust in the age of medical AI",
      "title_zh": "不是某人，而是某物：在医疗 AI 时代重新思考信任",
      "authors": [
        "Jan Beger"
      ],
      "abstract": "As artificial intelligence (AI) becomes embedded in healthcare, trust in\nmedical decision-making is changing fast. This opinion paper argues that trust\nin AI isn't a simple transfer from humans to machines - it's a dynamic,\nevolving relationship that must be built and maintained. Rather than debating\nwhether AI belongs in medicine, this paper asks: what kind of trust must AI\nearn, and how? Drawing from philosophy, bioethics, and system design, it\nexplores the key differences between human trust and machine reliability -\nemphasizing transparency, accountability, and alignment with the values of good\ncare. It argues that trust in AI shouldn't be built on mimicking empathy or\nintuition, but on thoughtful design, responsible deployment, and clear moral\nresponsibility. The goal is a balanced view - one that avoids blind optimism\nand reflexive fear. Trust in AI must be treated not as a given, but as\nsomething to be earned over time.",
      "tldr_zh": "这篇观点论文重新审视医疗 AI 时代的信任问题，强调信任不是简单从人类转移到机器，而是动态演变的，需要通过主动构建和维护。论文从哲学、生物伦理学和系统设计的视角，探讨人类信任与机器可靠性的关键差异，如透明度（transparency）、问责性（accountability）和与良好护理价值观的 alignment。作者主张，AI 应通过 thoughtful design、responsible deployment 和 clear moral responsibility 来获得信任，而不是模仿 empathy 或 intuition。最终，论文呼吁以平衡观点对待 AI，避开盲目乐观或反射性恐惧，让信任成为一种需要通过时间 earn 的过程。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.05331v2",
      "published_date": "2025-04-04 14:09:20 UTC",
      "updated_date": "2025-04-09 18:46:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:38:53.317481"
    },
    {
      "arxiv_id": "2504.03454v1",
      "title": "SpectR: Dynamically Composing LM Experts with Spectral Routing",
      "title_zh": "翻译失败",
      "authors": [
        "William Fleshman",
        "Benjamin Van Durme"
      ],
      "abstract": "Training large, general-purpose language models poses significant challenges.\nThe growing availability of specialized expert models, fine-tuned from\npretrained models for specific tasks or domains, offers a promising\nalternative. Leveraging the potential of these existing expert models in\nreal-world applications requires effective methods to select or merge the\nmodels best suited for a given task. This paper introduces SPECTR, an approach\nfor dynamically composing expert models at each time step during inference.\nNotably, our method requires no additional training and enables flexible,\ntoken- and layer-wise model combinations. Our experimental results demonstrate\nthat SPECTR improves routing accuracy over alternative training-free methods,\nincreasing task performance across expert domains.",
      "tldr_zh": "这篇论文介绍了 SPECTR，一种基于 Spectral Routing 的方法，用于动态组合语言模型(LM)专家模型，以应对训练大型通用语言模型的挑战。SPECTR 无需额外训练，即可实现推理过程中的 token-wise 和 layer-wise 灵活模型组合。实验结果表明，该方法提高了路由准确率，并在跨专家领域的任务性能上优于其他无训练方法。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03454v1",
      "published_date": "2025-04-04 13:58:44 UTC",
      "updated_date": "2025-04-04 13:58:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:39:02.330336"
    },
    {
      "arxiv_id": "2504.03424v1",
      "title": "The AI Cosmologist I: An Agentic System for Automated Data Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Adam Moss"
      ],
      "abstract": "We present the AI Cosmologist, an agentic system designed to automate\ncosmological/astronomical data analysis and machine learning research\nworkflows. This implements a complete pipeline from idea generation to\nexperimental evaluation and research dissemination, mimicking the scientific\nprocess typically performed by human researchers. The system employs\nspecialized agents for planning, coding, execution, analysis, and synthesis\nthat work together to develop novel approaches. Unlike traditional auto\nmachine-learning systems, the AI Cosmologist generates diverse implementation\nstrategies, writes complete code, handles execution errors, analyzes results,\nand synthesizes new approaches based on experimental outcomes. We demonstrate\nthe AI Cosmologist capabilities across several machine learning tasks, showing\nhow it can successfully explore solution spaces, iterate based on experimental\nresults, and combine successful elements from different approaches. Our results\nindicate that agentic systems can automate portions of the research process,\npotentially accelerating scientific discovery. The code and experimental data\nused in this paper are available on GitHub at\nhttps://github.com/adammoss/aicosmologist. Example papers included in the\nappendix demonstrate the system's capability to autonomously produce complete\nscientific publications, starting from only the dataset and task description",
      "tldr_zh": "本研究引入了 AI Cosmologist，一种智能体系统（agentic system），旨在自动化宇宙学/天文学数据分析和机器学习研究工作流程，从想法生成到实验评估和研究传播的全过程，模仿人类研究者。系统由专门的代理负责规划、编码、执行、分析和合成，确保生成多样策略、处理错误并基于实验结果迭代新方法。在多个机器学习任务上，AI Cosmologist 成功探索解决方案空间、结合不同方法的成功元素，并展示了自动化研究过程的能力。结果表明，这种系统可加速科学发现，并提供了开源代码和示例论文以验证其自主生成完整科学出版物的潜力。",
      "categories": [
        "astro-ph.IM",
        "astro-ph.CO",
        "astro-ph.GA",
        "cs.AI",
        "physics.data-an"
      ],
      "primary_category": "astro-ph.IM",
      "comment": "45 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.03424v1",
      "published_date": "2025-04-04 13:12:08 UTC",
      "updated_date": "2025-04-04 13:12:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:39:15.225360"
    },
    {
      "arxiv_id": "2504.03420v1",
      "title": "Autonomous state-space segmentation for Deep-RL sparse reward scenarios",
      "title_zh": "针对 Deep-RL 稀疏奖励场景的自治状态空间分割",
      "authors": [
        "Gianluca Maselli",
        "Vieri Giuliano Santucci"
      ],
      "abstract": "Dealing with environments with sparse rewards has always been crucial for\nsystems developed to operate in autonomous open-ended learning settings.\nIntrinsic Motivations could be an effective way to help Deep Reinforcement\nLearning algorithms learn in such scenarios. In fact, intrinsic reward signals,\nsuch as novelty or curiosity, are generally adopted to improve exploration when\nextrinsic rewards are delayed or absent. Building on previous works, we tackle\nthe problem of learning policies in the presence of sparse rewards by proposing\na two-level architecture that alternates an ''intrinsically driven'' phase of\nexploration and autonomous sub-goal generation, to a phase of sparse reward,\ngoal-directed policy learning. The idea is to build several small networks,\neach one specialized on a particular sub-path, and use them as starting points\nfor future exploration without the need to further explore from scratch\npreviously learnt paths. Two versions of the system have been trained and\ntested in the Gym SuperMarioBros environment without considering any additional\nextrinsic reward. The results show the validity of our approach and the\nimportance of autonomously segment the environment to generate an efficient\npath towards the final goal.",
      "tldr_zh": "该论文针对深度强化学习(Deep-RL)中稀疏奖励场景的挑战，提出了一种自主状态空间分割方法，以提升探索效率。该方法采用两级架构，包括一个基于Intrinsic Motivations的探索和子目标生成阶段，以及一个稀疏奖励导向的政策学习阶段，通过构建多个专注于特定子路径的小网络来避免从零开始重复探索。在Gym SuperMarioBros环境中测试的两个系统版本显示，该方法无需额外外在奖励即可有效学习路径，并证明了自主分割环境的重要性，从而为稀疏奖励场景下的RL算法提供了新颖的框架。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03420v1",
      "published_date": "2025-04-04 13:06:23 UTC",
      "updated_date": "2025-04-04 13:06:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:39:26.986626"
    },
    {
      "arxiv_id": "2504.03810v1",
      "title": "Hierarchically Encapsulated Representation for Protocol Design in Self-Driving Labs",
      "title_zh": "用于自驱动实验室协议设计的层次封装表示",
      "authors": [
        "Yu-Zhe Shi",
        "Mingchen Liu",
        "Fanxu Meng",
        "Qiao Xu",
        "Zhangqian Bi",
        "Kun He",
        "Lecheng Ruan",
        "Qining Wang"
      ],
      "abstract": "Self-driving laboratories have begun to replace human experimenters in\nperforming single experimental skills or predetermined experimental protocols.\nHowever, as the pace of idea iteration in scientific research has been\nintensified by Artificial Intelligence, the demand for rapid design of new\nprotocols for new discoveries become evident. Efforts to automate protocol\ndesign have been initiated, but the capabilities of knowledge-based machine\ndesigners, such as Large Language Models, have not been fully elicited,\nprobably for the absence of a systematic representation of experimental\nknowledge, as opposed to isolated, flatten pieces of information. To tackle\nthis issue, we propose a multi-faceted, multi-scale representation, where\ninstance actions, generalized operations, and product flow models are\nhierarchically encapsulated using Domain-Specific Languages. We further develop\na data-driven algorithm based on non-parametric modeling that autonomously\ncustomizes these representations for specific domains. The proposed\nrepresentation is equipped with various machine designers to manage protocol\ndesign tasks, including planning, modification, and adjustment. The results\ndemonstrate that the proposed method could effectively complement Large\nLanguage Models in the protocol design process, serving as an auxiliary module\nin the realm of machine-assisted scientific exploration.",
      "tldr_zh": "该论文针对自驱动实验室（Self-Driving Labs）中协议设计的需求，提出了一种分层封装表示（Hierarchically Encapsulated Representation），通过 Domain-Specific Languages (DSLs) 来系统化地整合实例动作、泛化操作和产品流程模型，以解决实验知识表示碎片化的问题。论文开发了一个基于非参数建模的数据驱动算法，能够自主为特定领域定制这些表示，从而支持协议设计的任务，如规划、修改和调整。实验结果表明，这种方法能有效补充 Large Language Models，在机器辅助科学探索中提升协议设计的效率和准确性。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "In International Conference on Learning Representations (ICLR'25)",
      "pdf_url": "http://arxiv.org/pdf/2504.03810v1",
      "published_date": "2025-04-04 12:05:15 UTC",
      "updated_date": "2025-04-04 12:05:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:39:39.283275"
    },
    {
      "arxiv_id": "2504.03380v1",
      "title": "Online Difficulty Filtering for Reasoning Oriented Reinforcement Learning",
      "title_zh": "面向推理的强化学习在线难度过滤",
      "authors": [
        "Sanghwan Bae",
        "Jiwoo Hong",
        "Min Young Lee",
        "Hanbyul Kim",
        "JeongYeon Nam",
        "Donghyun Kwak"
      ],
      "abstract": "Reasoning-Oriented Reinforcement Learning (RORL) enhances the reasoning\nability of Large Language Models (LLMs). However, due to the sparsity of\nrewards in RORL, effective training is highly dependent on the selection of\nproblems of appropriate difficulty. Although curriculum learning attempts to\naddress this by adjusting difficulty, it often relies on static schedules, and\neven recent online filtering methods lack theoretical grounding and a\nsystematic understanding of their effectiveness. In this work, we theoretically\nand empirically show that curating the batch with the problems that the\ntraining model achieves intermediate accuracy on the fly can maximize the\neffectiveness of RORL training, namely balanced online difficulty filtering. We\nfirst derive that the lower bound of the KL divergence between the initial and\nthe optimal policy can be expressed with the variance of the sampled accuracy.\nBuilding on those insights, we show that balanced filtering can maximize the\nlower bound, leading to better performance. Experimental results across five\nchallenging math reasoning benchmarks show that balanced online filtering\nyields an additional 10% in AIME and 4% improvements in average over plain\nGRPO. Moreover, further analysis shows the gains in sample efficiency and\ntraining time efficiency, exceeding the maximum reward of plain GRPO within 60%\ntraining time and the volume of the training set.",
      "tldr_zh": "该研究针对推理导向强化学习（RORL）中奖励稀疏的问题，提出了一种平衡在线难度过滤方法，通过实时筛选训练模型在问题上达到中等准确度的批次，来最大化训练效果。研究首先理论推导了初始策略和最优策略之间 KL divergence 的下界与采样准确度方差相关，并证明平衡过滤能最大化这一下界，从而提升性能。在五个数学推理基准测试中，该方法比普通 GRPO 提高了 10% 在 AIME 和 4% 的平均表现，同时提升了样本效率和训练时间效率，在 60% 的训练时间内就超过了 GRPO 的最大奖励。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03380v1",
      "published_date": "2025-04-04 11:52:05 UTC",
      "updated_date": "2025-04-04 11:52:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:39:51.062420"
    },
    {
      "arxiv_id": "2504.03809v2",
      "title": "Drawing a Map of Elections",
      "title_zh": "绘制选举地图",
      "authors": [
        "Stanisław Szufa",
        "Niclas Boehmer",
        "Robert Bredereck",
        "Piotr Faliszewski",
        "Rolf Niedermeier",
        "Piotr Skowron",
        "Arkadii Slinko",
        "Nimrod Talmon"
      ],
      "abstract": "Our main contribution is the introduction of the map of elections framework.\nA map of elections consists of three main elements: (1) a dataset of elections\n(i.e., collections of ordinal votes over given sets of candidates), (2) a way\nof measuring similarities between these elections, and (3) a representation of\nthe elections in the 2D Euclidean space as points, so that the more similar two\nelections are, the closer are their points. In our maps, we mostly focus on\ndatasets of synthetic elections, but we also show an example of a map over\nreal-life ones. To measure similarities, we would have preferred to use, e.g.,\nthe isomorphic swap distance, but this is infeasible due to its high\ncomputational complexity. Hence, we propose polynomial-time computable\npositionwise distance and use it instead. Regarding the representations in 2D\nEuclidean space, we mostly use the Kamada-Kawai algorithm, but we also show two\nalternatives. We develop the necessary theoretical results to form our maps and\nargue experimentally that they are accurate and credible. Further, we show how\ncoloring the elections in a map according to various criteria helps in\nanalyzing results of a number of experiments. In particular, we show colorings\naccording to the scores of winning candidates or committees, running times of\nILP-based winner determination algorithms, and approximation ratios achieved by\nparticular algorithms.",
      "tldr_zh": "本研究引入了“map of elections”框架，用于可视化选举数据集的核心贡献。该框架包括选举数据集（集合序数投票）、测量选举相似度的方法（如使用可计算的positionwise distance替代计算复杂的isomorphic swap distance），以及在2D欧氏空间中表示选举点，使得相似选举更接近，主要采用Kamada-Kawai算法。实验结果显示，该框架准确且可信，通过根据获胜候选人分数、算法运行时间和近似比率等标准着色选举点，有助于分析各种选举实验结果。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.MA",
      "comment": "Journal article merging results from arxiv:2105.07815,\n  arXiv:2407.11889 and Szufa et al., \"Drawing a Map of Elections in the Space\n  of Statistical Cultures\", AAMAS '20",
      "pdf_url": "http://arxiv.org/pdf/2504.03809v2",
      "published_date": "2025-04-04 11:44:56 UTC",
      "updated_date": "2025-04-08 10:52:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:40:03.075278"
    },
    {
      "arxiv_id": "2504.03360v1",
      "title": "Sustainable LLM Inference for Edge AI: Evaluating Quantized LLMs for Energy Efficiency, Output Accuracy, and Inference Latency",
      "title_zh": "翻译失败",
      "authors": [
        "Erik Johannes Husom",
        "Arda Goknil",
        "Merve Astekin",
        "Lwin Khin Shar",
        "Andre Kåsen",
        "Sagar Sen",
        "Benedikt Andreas Mithassel",
        "Ahmet Soylu"
      ],
      "abstract": "Deploying Large Language Models (LLMs) on edge devices presents significant\nchallenges due to computational constraints, memory limitations, inference\nspeed, and energy consumption. Model quantization has emerged as a key\ntechnique to enable efficient LLM inference by reducing model size and\ncomputational overhead. In this study, we conduct a comprehensive analysis of\n28 quantized LLMs from the Ollama library, which applies by default\nPost-Training Quantization (PTQ) and weight-only quantization techniques,\ndeployed on an edge device (Raspberry Pi 4 with 4GB RAM). We evaluate energy\nefficiency, inference performance, and output accuracy across multiple\nquantization levels and task types. Models are benchmarked on five standardized\ndatasets (CommonsenseQA, BIG-Bench Hard, TruthfulQA, GSM8K, and HumanEval), and\nwe employ a high-resolution, hardware-based energy measurement tool to capture\nreal-world power consumption. Our findings reveal the trade-offs between energy\nefficiency, inference speed, and accuracy in different quantization settings,\nhighlighting configurations that optimize LLM deployment for\nresource-constrained environments. By integrating hardware-level energy\nprofiling with LLM benchmarking, this study provides actionable insights for\nsustainable AI, bridging a critical gap in existing research on energy-aware\nLLM deployment.",
      "tldr_zh": "这篇论文评估了量化 LLMs 在边缘设备上的部署挑战，重点考察能效、输出准确性和推理延迟。研究分析了 28 个来自 Ollama 库的量化模型，这些模型采用后训练量化 (PTQ) 和权重量化技术，在 Raspberry Pi 4 (4GB RAM) 上进行基准测试，涵盖 CommonsenseQA、BIG-Bench Hard、TruthfulQA、GSM8K 和 HumanEval 等五个数据集，并使用硬件能量测量工具记录实际功耗。结果揭示了不同量化设置间的权衡，帮助优化 LLM 在资源受限环境中的可持续部署，为能效型 AI 提供了关键见解。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "30 pages, 14 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.03360v1",
      "published_date": "2025-04-04 11:29:30 UTC",
      "updated_date": "2025-04-04 11:29:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:40:16.113359"
    },
    {
      "arxiv_id": "2504.03353v1",
      "title": "Decentralized Collective World Model for Emergent Communication and Coordination",
      "title_zh": "用于涌现通信和协调的去中心化集体世界模型",
      "authors": [
        "Kentaro Nomura",
        "Tatsuya Aoki",
        "Tadahiro Taniguchi",
        "Takato Horii"
      ],
      "abstract": "We propose a fully decentralized multi-agent world model that enables both\nsymbol emergence for communication and coordinated behavior through temporal\nextension of collective predictive coding. Unlike previous research that\nfocuses on either communication or coordination separately, our approach\nachieves both simultaneously. Our method integrates world models with\ncommunication channels, enabling agents to predict environmental dynamics,\nestimate states from partial observations, and share critical information\nthrough bidirectional message exchange with contrastive learning for message\nalignment. Using a two-agent trajectory drawing task, we demonstrate that our\ncommunication-based approach outperforms non-communicative models when agents\nhave divergent perceptual capabilities, achieving the second-best coordination\nafter centralized models. Importantly, our distributed approach with\nconstraints preventing direct access to other agents' internal states\nfacilitates the emergence of more meaningful symbol systems that accurately\nreflect environmental states. These findings demonstrate the effectiveness of\ndecentralized communication for supporting coordination while developing shared\nrepresentations of the environment.",
      "tldr_zh": "本研究提出了一种完全去中心化的多智能体世界模型，通过集体预测编码的 temporal extension，实现符号涌现（emergent communication）和协调行为的同步发展。模型将世界模型与通信通道整合，允许代理预测环境动态、从部分观察估计状态，并通过双向消息交换结合 contrastive learning 进行消息对齐，从而共享关键信息。实验在两个代理的轨迹绘制任务中显示，该方法在代理感知能力不同时，协调性能优于非通信模型，仅次于中心化模型，且促进了更有意义的符号系统涌现。总体而言，这一分布式方法证明了去中心化通信在支持协调的同时，有效构建共享的环境表示。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03353v1",
      "published_date": "2025-04-04 11:17:52 UTC",
      "updated_date": "2025-04-04 11:17:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:40:27.027060"
    },
    {
      "arxiv_id": "2504.03343v1",
      "title": "Talk2X -- An Open-Source Toolkit Facilitating Deployment of LLM-Powered Chatbots on the Web",
      "title_zh": "翻译失败",
      "authors": [
        "Lars Krupp",
        "Daniel Geißler",
        "Peter Hevesi",
        "Marco Hirsch",
        "Paul Lukowicz",
        "Jakob Karolus"
      ],
      "abstract": "Integrated into websites, LLM-powered chatbots offer alternative means of\nnavigation and information retrieval, leading to a shift in how users access\ninformation on the web. Yet, predominantly closed-sourced solutions limit\nproliferation among web hosts and suffer from a lack of transparency with\nregard to implementation details and energy efficiency. In this work, we\npropose our openly available agent Talk2X leveraging an adapted\nretrieval-augmented generation approach (RAG) combined with an automatically\ngenerated vector database, benefiting energy efficiency. Talk2X's architecture\nis generalizable to arbitrary websites offering developers a ready to use tool\nfor integration. Using a mixed-methods approach, we evaluated Talk2X's\nusability by tasking users to acquire specific assets from an open science\nrepository. Talk2X significantly improved task completion time, correctness,\nand user experience supporting users in quickly pinpointing specific\ninformation as compared to standard user-website interaction. Our findings\ncontribute technical advancements to an ongoing paradigm shift of how we access\ninformation on the web.",
      "tldr_zh": "本文提出 Talk2X，一个开源工具包，用于简化 LLM 驱动聊天机器人在网站上的部署，通过适应后的检索增强生成方法 (RAG) 和自动生成的向量数据库，提升能源效率和通用性。Talk2X 的架构可泛化到任意网站，便于开发者快速集成。实验结果显示，在用户获取开源科学仓库资产的任务中，Talk2X 显著缩短了完成时间、提高了正确率和用户体验，从而推动网络信息访问方式的范式转变。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03343v1",
      "published_date": "2025-04-04 10:58:57 UTC",
      "updated_date": "2025-04-04 10:58:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:40:39.273646"
    },
    {
      "arxiv_id": "2504.03342v1",
      "title": "EOOD: Entropy-based Out-of-distribution Detection",
      "title_zh": "EO",
      "authors": [
        "Guide Yang",
        "Chao Hou",
        "Weilong Peng",
        "Xiang Fang",
        "Yongwei Nie",
        "Peican Zhu",
        "Keke Tang"
      ],
      "abstract": "Deep neural networks (DNNs) often exhibit overconfidence when encountering\nout-of-distribution (OOD) samples, posing significant challenges for\ndeployment. Since DNNs are trained on in-distribution (ID) datasets, the\ninformation flow of ID samples through DNNs inevitably differs from that of OOD\nsamples. In this paper, we propose an Entropy-based Out-Of-distribution\nDetection (EOOD) framework. EOOD first identifies specific block where the\ninformation flow differences between ID and OOD samples are more pronounced,\nusing both ID and pseudo-OOD samples. It then calculates the conditional\nentropy on the selected block as the OOD confidence score. Comprehensive\nexperiments conducted across various ID and OOD settings demonstrate the\neffectiveness of EOOD in OOD detection and its superiority over\nstate-of-the-art methods.",
      "tldr_zh": "该研究针对深度神经网络 (DNNs) 在处理 out-of-distribution (OOD) 样本时出现的过度自信问题，提出了一种基于熵的检测框架 EOOD。EOOD 首先利用 in-distribution (ID) 数据和伪 OOD 样本识别信息流差异更明显的特定块，然后计算该块的条件熵作为 OOD 置信度分数。通过全面实验验证，EOOD 在多种 ID 和 OOD 设置中表现出色，并优于现有 state-of-the-art 方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "IJCNN 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.03342v1",
      "published_date": "2025-04-04 10:57:03 UTC",
      "updated_date": "2025-04-04 10:57:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:40:51.877231"
    },
    {
      "arxiv_id": "2504.06292v1",
      "title": "Temporal-contextual Event Learning for Pedestrian Crossing Intent Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Hongbin Liang",
        "Hezhe Qiao",
        "Wei Huang",
        "Qizhou Wang",
        "Mingsheng Shang",
        "Lin Chen"
      ],
      "abstract": "Ensuring the safety of vulnerable road users through accurate prediction of\npedestrian crossing intention (PCI) plays a crucial role in the context of\nautonomous and assisted driving. Analyzing the set of observation video frames\nin ego-view has been widely used in most PCI prediction methods to forecast the\ncross intent. However, they struggle to capture the critical events related to\npedestrian behaviour along the temporal dimension due to the high redundancy of\nthe video frames, which results in the sub-optimal performance of PCI\nprediction. Our research addresses the challenge by introducing a novel\napproach called \\underline{T}emporal-\\underline{c}ontextual Event\n\\underline{L}earning (TCL). The TCL is composed of the Temporal Merging Module\n(TMM), which aims to manage the redundancy by clustering the observed video\nframes into multiple key temporal events. Then, the Contextual Attention Block\n(CAB) is employed to adaptively aggregate multiple event features along with\nvisual and non-visual data. By synthesizing the temporal feature extraction and\ncontextual attention on the key information across the critical events, TCL can\nlearn expressive representation for the PCI prediction. Extensive experiments\nare carried out on three widely adopted datasets, including PIE, JAAD-beh, and\nJAAD-all. The results show that TCL substantially surpasses the\nstate-of-the-art methods. Our code can be accessed at\nhttps://github.com/dadaguailhb/TCL.",
      "tldr_zh": "该论文针对自动驾驶中行人过马路意图（Pedestrian Crossing Intent Prediction, PCI）预测的挑战，提出了一种新方法 Temporal-contextual Event Learning (TCL)，以解决现有方法因视频帧冗余而无法有效捕捉行人行为的临界事件问题。TCL 包括 Temporal Merging Module (TMM) 用于聚类视频帧生成关键事件，以及 Contextual Attention Block (CAB) 用于自适应聚合事件特征，包括视觉和非视觉数据，从而学习更具表达性的表示进行 PCI 预测。在 PIE、JAAD-beh 和 JAAD-all 等数据集上的实验结果表明，TCL 显著超过了最先进方法，证明了其有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted in ICONIP2024",
      "pdf_url": "http://arxiv.org/pdf/2504.06292v1",
      "published_date": "2025-04-04 10:44:24 UTC",
      "updated_date": "2025-04-04 10:44:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:41:04.825212"
    },
    {
      "arxiv_id": "2504.03329v1",
      "title": "Mind the Prompt: Prompting Strategies in Audio Generations for Improving Sound Classification",
      "title_zh": "留意提示：在音频生成中用于改善声音分类的提示策略",
      "authors": [
        "Francesca Ronchini",
        "Ho-Hsiang Wu",
        "Wei-Cheng Lin",
        "Fabio Antonacci"
      ],
      "abstract": "This paper investigates the design of effective prompt strategies for\ngenerating realistic datasets using Text-To-Audio (TTA) models. We also analyze\ndifferent techniques for efficiently combining these datasets to enhance their\nutility in sound classification tasks. By evaluating two sound classification\ndatasets with two TTA models, we apply a range of prompt strategies. Our\nfindings reveal that task-specific prompt strategies significantly outperform\nbasic prompt approaches in data generation. Furthermore, merging datasets\ngenerated using different TTA models proves to enhance classification\nperformance more effectively than merely increasing the training dataset size.\nOverall, our results underscore the advantages of these methods as effective\ndata augmentation techniques using synthetic data.",
      "tldr_zh": "本论文探讨了在Text-To-Audio (TTA) 模型中设计有效prompt strategies，以生成真实数据集并提升声音分类任务的性能。通过评估两个声音分类数据集和两个TTA模型，研究者应用多种提示策略，发现任务特定的prompt strategies在数据生成方面显著优于基本方法。结果显示，合并不同TTA模型生成的数据集比单纯扩大训练数据集更有效地改善分类性能，这些方法证明了使用合成数据作为数据增强技术的优势。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD",
        "eess.SP"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted at Generative Data Augmentation for Real-World Signal\n  Processing Applications Workshop",
      "pdf_url": "http://arxiv.org/pdf/2504.03329v1",
      "published_date": "2025-04-04 10:14:11 UTC",
      "updated_date": "2025-04-04 10:14:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:41:15.144946"
    },
    {
      "arxiv_id": "2504.03328v1",
      "title": "Policy Optimization Algorithms in a Unified Framework",
      "title_zh": "统一框架下的策略优化算法",
      "authors": [
        "Shuang Wu"
      ],
      "abstract": "Policy optimization algorithms are crucial in many fields but challenging to\ngrasp and implement, often due to complex calculations related to Markov\ndecision processes and varying use of discount and average reward setups. This\npaper presents a unified framework that applies generalized ergodicity theory\nand perturbation analysis to clarify and enhance the application of these\nalgorithms. Generalized ergodicity theory sheds light on the steady-state\nbehavior of stochastic processes, aiding understanding of both discounted and\naverage rewards. Perturbation analysis provides in-depth insights into the\nfundamental principles of policy optimization algorithms. We use this framework\nto identify common implementation errors and demonstrate the correct\napproaches. Through a case study on Linear Quadratic Regulator problems, we\nillustrate how slight variations in algorithm design affect implementation\noutcomes. We aim to make policy optimization algorithms more accessible and\nreduce their misuse in practice.",
      "tldr_zh": "本论文提出一个统一的框架，利用广义遍历性理论（generalized ergodicity theory）和扰动分析（perturbation analysis），来简化政策优化算法（Policy Optimization Algorithms）的理解和实现，这些算法常因Markov决策过程（Markov Decision Processes）的复杂计算以及折扣和平均奖励设置的差异而备受挑战。该框架阐释了随机过程的稳态行为，并深入剖析算法的基本原理，从而识别常见的实现错误并提供正确方法。通过线性二次调节器（Linear Quadratic Regulator）问题的案例研究，论文展示了算法设计微小变化对结果的影响，最终旨在提升这些算法的可访问性和减少实际应用中的误用。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03328v1",
      "published_date": "2025-04-04 10:14:01 UTC",
      "updated_date": "2025-04-04 10:14:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:41:27.388374"
    },
    {
      "arxiv_id": "2504.03302v2",
      "title": "Noise Augmented Fine Tuning for Mitigating Hallucinations in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Afshin Khadangi",
        "Amir Sartipi",
        "Igor Tchappi",
        "Ramin Bahmani"
      ],
      "abstract": "Large language models (LLMs) often produce inaccurate or misleading\ncontent-hallucinations. To address this challenge, we introduce Noise-Augmented\nFine-Tuning (NoiseFiT), a novel framework that leverages adaptive noise\ninjection based on the signal-to-noise ratio (SNR) to enhance model robustness.\nIn particular, NoiseFiT selectively perturbs layers identified as either\nhigh-SNR (more robust) or low-SNR (potentially under-regularized) using a\ndynamically scaled Gaussian noise. We further propose a hybrid loss that\ncombines standard cross-entropy, soft cross-entropy, and consistency\nregularization to ensure stable and accurate outputs under noisy training\nconditions. Our theoretical analysis shows that adaptive noise injection is\nboth unbiased and variance-preserving, providing strong guarantees for\nconvergence in expectation. Empirical results on multiple test and benchmark\ndatasets demonstrate that NoiseFiT significantly reduces hallucination rates,\noften improving or matching baseline performance in key tasks. These findings\nhighlight the promise of noise-driven strategies for achieving robust,\ntrustworthy language modeling without incurring prohibitive computational\noverhead. Given the comprehensive and detailed nature of our experiments, we\nhave publicly released the fine-tuning logs, benchmark evaluation artifacts,\nand source code online at W&B, Hugging Face, and GitHub, respectively, to\nfoster further research, accessibility and reproducibility.",
      "tldr_zh": "本研究针对大型语言模型（Large Language Models, LLMs）产生的幻觉（hallucinations）问题，提出了一种新型框架Noise-Augmented Fine-Tuning (NoiseFiT)，通过基于信号噪声比（SNR）的自适应噪声注入来增强模型鲁棒性。NoiseFiT选择性地扰动高SNR层和低SNR层，并采用动态缩放的高斯噪声，同时引入混合损失函数（结合标准交叉熵、软交叉熵和一致性正则化），以确保在噪声训练条件下输出稳定准确。理论分析证明，该方法具有无偏性和方差保持性，提供收敛期望的强保证。实验结果显示，NoiseFiT在多个基准数据集上显著降低了幻觉率，并在关键任务中改善或匹配基线性能，同时无需额外计算开销，并公开了相关代码和资源以促进可重复性研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03302v2",
      "published_date": "2025-04-04 09:27:19 UTC",
      "updated_date": "2025-05-03 07:26:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:41:40.920806"
    },
    {
      "arxiv_id": "2504.03295v1",
      "title": "Stance-Driven Multimodal Controlled Statement Generation: New Dataset and Task",
      "title_zh": "翻译失败",
      "authors": [
        "Bingqian Wang",
        "Quan Fang",
        "Jiachen Sun",
        "Xiaoxiao Ma"
      ],
      "abstract": "Formulating statements that support diverse or controversial stances on\nspecific topics is vital for platforms that enable user expression, reshape\npolitical discourse, and drive social critique and information dissemination.\nWith the rise of Large Language Models (LLMs), controllable text generation\ntowards specific stances has become a promising research area with applications\nin shaping public opinion and commercial marketing. However, current datasets\noften focus solely on pure texts, lacking multimodal content and effective\ncontext, particularly in the context of stance detection. In this paper, we\nformally define and study the new problem of stance-driven controllable content\ngeneration for tweets with text and images, where given a multimodal post (text\nand image/video), a model generates a stance-controlled response. To this end,\nwe create the Multimodal Stance Generation Dataset (StanceGen2024), the first\nresource explicitly designed for multimodal stance-controllable text generation\nin political discourse. It includes posts and user comments from the 2024 U.S.\npresidential election, featuring text, images, videos, and stance annotations\nto explore how multimodal political content shapes stance expression.\nFurthermore, we propose a Stance-Driven Multimodal Generation (SDMG) framework\nthat integrates weighted fusion of multimodal features and stance guidance to\nimprove semantic consistency and stance control. We release the dataset and\ncode (https://anonymous.4open.science/r/StanceGen-BE9D) for public use and\nfurther research.",
      "tldr_zh": "该论文定义了一个新任务：立场驱动的多模态控制语句生成，旨在针对特定话题生成支持多样或争议性立场的推文响应，包括文本和图像/视频内容，以提升用户表达和政治话语。研究者构建了首个 Multimodal Stance Generation Dataset (StanceGen2024)，基于2024年美国总统选举的数据，包含文本、图像、视频及立场注解，用于探索多模态内容对立场表达的影响。论文提出 Stance-Driven Multimodal Generation (SDMG) 框架，通过加权融合多模态特征和立场指导，提高生成的语义一致性和控制精度，并公开了数据集和代码以促进进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03295v1",
      "published_date": "2025-04-04 09:20:19 UTC",
      "updated_date": "2025-04-04 09:20:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:41:53.136005"
    },
    {
      "arxiv_id": "2504.03287v1",
      "title": "Towards Effective EU E-Participation: The Development of AskThePublic",
      "title_zh": "迈向有效的 EU 电子参与：AskThePublic 的开发",
      "authors": [
        "Kilian Sprenkamp",
        "Nils Messerschmidt",
        "Amir Sartipi",
        "Igor Tchappi",
        "Xiaohui Wu",
        "Liudmila Zavolokina",
        "Gilbert Fridgen"
      ],
      "abstract": "E-participation platforms can be an important asset for governments in\nincreasing trust and fostering democratic societies. By engaging\nnon-governmental and private institutions, domain experts, and even the general\npublic, policymakers can make informed and inclusive decisions. Drawing on the\nMedia Richness Theory and applying the Design Science Research method, we\nexplore how a chatbot can be designed to improve the effectiveness of the\npolicy-making process of existing citizen involvement platforms. Leveraging the\nHave Your Say platform, which solicits feedback on European Commission\ninitiatives and regulations, a Large Language Model based chatbot, called\nAskThePublic is created, providing policymakers, journalists, researchers, and\ninterested citizens with a convenient channel to explore and engage with public\ninput. By conducting 11 semistructured interviews, the results show that the\nparticipants value the interactive and structured responses as well as enhanced\nlanguage capabilities, thus increasing their likelihood of engaging with\nAskThePublic over the existing platform. An outlook for future iterations is\nprovided and discussed with regard to the perspectives of the different\nstakeholders.",
      "tldr_zh": "这篇论文探讨了如何通过聊天机器人提升欧盟电子参与（E-Participation）的有效性，旨在增强政府决策的包容性和公众信任。研究者基于 Media Richness Theory 和 Design Science Research 方法，开发了 AskThePublic——一个基于 Large Language Model 的聊天机器人，利用现有 Have Your Say 平台来处理公众反馈，提供互动式和结构化的响应。实验结果显示，通过 11 次半结构化采访，参与者更倾向于使用 AskThePublic，增加了他们的参与意愿。该框架为未来电子参与平台的迭代提供了宝贵见解，以满足不同利益相关者的需求。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03287v1",
      "published_date": "2025-04-04 09:15:06 UTC",
      "updated_date": "2025-04-04 09:15:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:42:04.277040"
    },
    {
      "arxiv_id": "2504.03278v2",
      "title": "JanusDDG: A Thermodynamics-Compliant Model for Sequence-Based Protein Stability via Two-Fronts Multi-Head Attention",
      "title_zh": "翻译失败",
      "authors": [
        "Guido Barducci",
        "Ivan Rossi",
        "Francesco Codicè",
        "Cesare Rollo",
        "Valeria Repetto",
        "Corrado Pancotti",
        "Virginia Iannibelli",
        "Tiziana Sanavia",
        "Piero Fariselli"
      ],
      "abstract": "Understanding how residue variations affect protein stability is crucial for\ndesigning functional proteins and deciphering the molecular mechanisms\nunderlying disease-related mutations. Recent advances in protein language\nmodels (PLMs) have revolutionized computational protein analysis, enabling,\namong other things, more accurate predictions of mutational effects. In this\nwork, we introduce JanusDDG, a deep learning framework that leverages\nPLM-derived embeddings and a bidirectional cross-attention transformer\narchitecture to predict $\\Delta \\Delta G$ of single and multiple-residue\nmutations while simultaneously being constrained to respect fundamental\nthermodynamic properties, such as antisymmetry and transitivity. Unlike\nconventional self-attention, JanusDDG computes queries (Q) and values (V) as\nthe difference between wild-type and mutant embeddings, while keys (K)\nalternate between the two. This cross-interleaved attention mechanism enables\nthe model to capture mutation-induced perturbations while preserving essential\ncontextual information. Experimental results show that JanusDDG achieves\nstate-of-the-art performance in predicting $\\Delta \\Delta G$ from sequence\nalone, matching or exceeding the accuracy of structure-based methods for both\nsingle and multiple mutations. Code\nAvailability:https://github.com/compbiomed-unito/JanusDDG",
      "tldr_zh": "本研究引入了 JanusDDG，一种符合热力学性质的深度学习框架，利用蛋白质语言模型(PLM)派生的嵌入和双向交叉注意力 Transformer 架构，来预测单突变和多突变的 $\\Delta \\Delta G$ 值。JanusDDG 的创新在于其两面多头注意力机制，将查询(Q)和值(V)计算为野生型与突变嵌入的差异，而键(K)交替使用两者，从而捕捉突变诱导的扰动并保留上下文信息，同时确保模型遵守如反对称性和传递性的热力学属性。实验结果显示，JanusDDG 在基于序列的 $\\Delta \\Delta G$ 预测中达到 state-of-the-art 性能，匹配或超过基于结构的基准方法。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG",
        "physics.comp-ph"
      ],
      "primary_category": "q-bio.QM",
      "comment": "20 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.03278v2",
      "published_date": "2025-04-04 09:02:32 UTC",
      "updated_date": "2025-04-14 18:11:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:42:16.417235"
    },
    {
      "arxiv_id": "2505.04628v1",
      "title": "How Social is It? A Benchmark for LLMs' Capabilities in Multi-user Multi-turn Social Agent Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Yusen Wu",
        "Junwu Xiong",
        "Xiaotie Deng"
      ],
      "abstract": "Expanding the application of large language models (LLMs) to societal life,\ninstead of primary function only as auxiliary assistants to communicate with\nonly one person at a time, necessitates LLMs' capabilities to independently\nplay roles in multi-user, multi-turn social agent tasks within complex social\nsettings. However, currently the capability has not been systematically\nmeasured with available benchmarks. To address this gap, we first introduce an\nagent task leveling framework grounded in sociological principles.\nConcurrently, we propose a novel benchmark, How Social Is It (we call it HSII\nbelow), designed to assess LLM's social capabilities in comprehensive social\nagents tasks and benchmark representative models. HSII comprises four stages:\nformat parsing, target selection, target switching conversation, and stable\nconversation, which collectively evaluate the communication and task completion\ncapabilities of LLMs within realistic social interaction scenarios dataset,\nHSII-Dataset. The dataset is derived step by step from news dataset. We perform\nan ablation study by doing clustering to the dataset. Additionally, we\ninvestigate the impact of chain of thought (COT) method on enhancing LLMs'\nsocial performance. Since COT cost more computation, we further introduce a new\nstatistical metric, COT-complexity, to quantify the efficiency of certain LLMs\nwith COTs for specific social tasks and strike a better trade-off between\nmeasurement of correctness and efficiency. Various results of our experiments\ndemonstrate that our benchmark is well-suited for evaluating social skills in\nLLMs.",
      "tldr_zh": "该论文提出一个基于社会学原则的代理任务分级框架，并引入 HSII（How Social Is It）基准，用于系统评估大型语言模型（LLMs）在多用户多轮次社会代理任务中的能力。HSII 包括四个阶段：格式解析、目标选择、目标切换对话和稳定对话，利用从新闻数据集派生而来的 HSII-Dataset 进行测试，并通过聚类消融研究分析数据集。论文还探讨了 Chain of Thought (COT) 方法对 LLMs 社会性能的影响，并引入 COT-complexity 指标来量化计算效率与正确性的权衡。实验结果显示，HSII 基准有效地评估了 LLMs 的社交技能表现。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.04628v1",
      "published_date": "2025-04-04 08:59:01 UTC",
      "updated_date": "2025-04-04 08:59:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:42:28.756705"
    },
    {
      "arxiv_id": "2504.03277v1",
      "title": "Monte Carlo Graph Coloring",
      "title_zh": "Monte Carlo 图着色",
      "authors": [
        "Tristan Cazenave",
        "Benjamin Negrevergne",
        "Florian Sikora"
      ],
      "abstract": "Graph Coloring is probably one of the most studied and famous problem in\ngraph algorithms. Exact methods fail to solve instances with more than few\nhundred vertices, therefore, a large number of heuristics have been proposed.\nNested Monte Carlo Search (NMCS) and Nested Rollout Policy Adaptation (NRPA)\nare Monte Carlo search algorithms for single player games. Surprisingly, few\nwork has been dedicated to evaluating Monte Carlo search algorithms to\ncombinatorial graph problems. In this paper we expose how to efficiently apply\nMonte Carlo search to Graph Coloring and compare this approach to existing\nones.",
      "tldr_zh": "本文提出了一种将 Monte Carlo 搜索算法应用于图着色问题的创新方法，针对精确算法无法处理大规模实例的局限性。作者探讨了如何使用 Nested Monte Carlo Search (NMCS) 和 Nested Rollout Policy Adaptation (NRPA) 等算法来解决这一经典组合优化问题，并与现有启发式方法进行比较。结果表明，这种方法为图着色问题提供了高效的替代方案，扩展了 Monte Carlo 搜索在组合图问题中的应用潜力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03277v1",
      "published_date": "2025-04-04 08:57:01 UTC",
      "updated_date": "2025-04-04 08:57:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:42:39.394320"
    },
    {
      "arxiv_id": "2504.03274v1",
      "title": "Do Large Language Models Solve the Problems of Agent-Based Modeling? A Critical Review of Generative Social Simulations",
      "title_zh": "翻译失败",
      "authors": [
        "Maik Larooij",
        "Petter Törnberg"
      ],
      "abstract": "Recent advancements in AI have reinvigorated Agent-Based Models (ABMs), as\nthe integration of Large Language Models (LLMs) has led to the emergence of\n``generative ABMs'' as a novel approach to simulating social systems. While\nABMs offer means to bridge micro-level interactions with macro-level patterns,\nthey have long faced criticisms from social scientists, pointing to e.g., lack\nof realism, computational complexity, and challenges of calibrating and\nvalidating against empirical data. This paper reviews the generative ABM\nliterature to assess how this new approach adequately addresses these\nlong-standing criticisms. Our findings show that studies show limited awareness\nof historical debates. Validation remains poorly addressed, with many studies\nrelying solely on subjective assessments of model `believability', and even the\nmost rigorous validation failing to adequately evidence operational validity.\nWe argue that there are reasons to believe that LLMs will exacerbate rather\nthan resolve the long-standing challenges of ABMs. The black-box nature of LLMs\nmoreover limit their usefulness for disentangling complex emergent causal\nmechanisms. While generative ABMs are still in a stage of early\nexperimentation, these findings question of whether and how the field can\ntransition to the type of rigorous modeling needed to contribute to social\nscientific theory.",
      "tldr_zh": "这篇论文对整合Large Language Models (LLMs)的generative Agent-Based Models (ABMs)进行了批判性文献回顾，评估它们是否解决了ABMs在模拟社会系统时面临的传统问题，如缺乏真实性、计算复杂性和验证挑战。研究发现，大多数工作对历史争论了解有限，验证方法多依赖主观评估（如模型“believability”），即使最严格的验证也无法充分证明operational validity。作者认为LLMs的black-box性质可能加剧这些问题，并质疑generative ABMs是否能从早期实验阶段过渡到为社会科学理论提供严谨建模的工具。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03274v1",
      "published_date": "2025-04-04 08:48:43 UTC",
      "updated_date": "2025-04-04 08:48:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:42:53.457353"
    },
    {
      "arxiv_id": "2504.03272v1",
      "title": "Verification of Autonomous Neural Car Control with KeYmaera X",
      "title_zh": "翻译失败",
      "authors": [
        "Enguerrand Prebet",
        "Samuel Teuber",
        "André Platzer"
      ],
      "abstract": "This article presents a formal model and formal safety proofs for the ABZ'25\ncase study in differential dynamic logic (dL). The case study considers an\nautonomous car driving on a highway avoiding collisions with neighbouring cars.\nUsing KeYmaera X's dL implementation, we prove absence of collision on an\ninfinite time horizon which ensures that safety is preserved independently of\ntrip length. The safety guarantees hold for time-varying reaction time and\nbrake force. Our dL model considers the single lane scenario with cars ahead or\nbehind. We demonstrate that dL with its tools is a rigorous foundation for\nruntime monitoring, shielding, and neural network verification. Doing so sheds\nlight on inconsistencies between the provided specification and simulation\nenvironment highway-env of the ABZ'25 study. We attempt to fix these\ninconsistencies and uncover numerous counterexamples which also indicate issues\nin the provided reinforcement learning environment.",
      "tldr_zh": "这篇论文使用 differential dynamic logic (dL) 构建了一个正式模型，并通过 KeYmaera X 工具为 ABZ'25 案例研究提供了自动驾驶汽车的安全证明，焦点是避免高速公路上与其他车辆碰撞。研究证明了在无限时间范围内无碰撞的安全性，该保证适用于变化的反应时间和制动力，并针对单车道场景进行了建模。论文还揭示了 ABZ'25 规范与模拟环境 highway-env 之间的不一致，通过修复尝试和发现多个反例，突显了 dL 在运行时监控、屏蔽和神经网络验证中的严谨作用。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG",
        "cs.LO",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "21 pages, 6 figures; Accepted at the 11th International Conference on\n  Rigorous State Based Methods (ABZ'25)",
      "pdf_url": "http://arxiv.org/pdf/2504.03272v1",
      "published_date": "2025-04-04 08:43:31 UTC",
      "updated_date": "2025-04-04 08:43:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:43:03.999268"
    },
    {
      "arxiv_id": "2504.03259v1",
      "title": "An Extended Symbolic-Arithmetic Model for Teaching Double-Black Removal with Rotation in Red-Black Trees",
      "title_zh": "翻译失败",
      "authors": [
        "Kennedy E. Ehimwenma",
        "Hongyu Zhou",
        "Junfeng Wang",
        "Ze Zheng"
      ],
      "abstract": "Double-black (DB) nodes have no place in red-black (RB) trees. So when DB\nnodes are formed, they are immediately removed. The removal of DB nodes that\ncause rotation and recoloring of other connected nodes poses greater challenges\nin the teaching and learning of RB trees. To ease this difficulty, this paper\nextends our previous work on the symbolic arithmetic algebraic (SA) method for\nremoving DB nodes. The SA operations that are given as, Red + Black = Black;\nBlack - Black = Red; Black + Black = DB; and DB - Black = Black removes DB\nnodes and rebalances black heights in RB trees. By extension, this paper\nprojects three SA mathematical equations, namely, general symbolic arithmetic\nrule; partial symbolic arithmetic rule1; and partial symbolic arithmetic rule2.\nThe removal of a DB node ultimately affects black heights in RB trees. To\nbalance black heights using the SA equations, all the RB tree cases, namely,\nLR, RL, LL, and RR, were considered in this work; and the position of the nodes\nconnected directly or indirectly to the DB node was also tested. In this study,\nto balance a RB tree, the issues considered w.r.t. the different cases of the\nRB tree were i) whether a DB node has an inner, outer, or both inner and outer\nblack nephews; or ii) whether a DB node has an inner, outer or both inner and\nouter red nephews. The nephews r and x in this work are the children of the\nsibling s to a DB, and further up the tree, the parent p of a DB is their\ngrandparent g. Thus, r and x have indirect relationships to a DB at the point\nof formation of the DB node. The novelty of the SA equations is in their\neffectiveness in the removal of DB that involves rotation of nodes as well as\nthe recoloring of nodes along any simple path so as to balance black heights in\na tree.",
      "tldr_zh": "本论文扩展了符号算术（Symbolic Arithmetic, SA）方法，用于简化红黑树（Red-Black Trees）中双黑节点（Double-Black nodes）的移除教学，特别是涉及旋转和重新着色的复杂情况。作者提出了三个新的 SA 方程，包括 general symbolic arithmetic rule、partial symbolic arithmetic rule1 和 partial symbolic arithmetic rule2，以移除双黑节点并重新平衡黑高度，考虑了所有红黑树案例（如 LR, RL, LL, RR）以及双黑节点的内外甥节点（nephews）颜色和位置。实验分析显示，该扩展模型能有效处理节点旋转和重新着色，确保树结构平衡，从而提升红黑树的教学和学习效率。",
      "categories": [
        "cs.DS",
        "cs.AI"
      ],
      "primary_category": "cs.DS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03259v1",
      "published_date": "2025-04-04 08:19:26 UTC",
      "updated_date": "2025-04-04 08:19:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:43:16.027131"
    },
    {
      "arxiv_id": "2504.03801v1",
      "title": "Semantic-guided Representation Learning for Multi-Label Recognition",
      "title_zh": "语义引导的表示学习用于多标签识别",
      "authors": [
        "Ruhui Zhang",
        "Hezhe Qiao",
        "Pengcheng Xu",
        "Mingsheng Shang",
        "Lin Chen"
      ],
      "abstract": "Multi-label Recognition (MLR) involves assigning multiple labels to each data\ninstance in an image, offering advantages over single-label classification in\ncomplex scenarios. However, it faces the challenge of annotating all relevant\ncategories, often leading to uncertain annotations, such as unseen or\nincomplete labels. Recent Vision and Language Pre-training (VLP) based methods\nhave made significant progress in tackling zero-shot MLR tasks by leveraging\nrich vision-language correlations. However, the correlation between multi-label\nsemantics has not been fully explored, and the learned visual features often\nlack essential semantic information. To overcome these limitations, we\nintroduce a Semantic-guided Representation Learning approach (SigRL) that\nenables the model to learn effective visual and textual representations,\nthereby improving the downstream alignment of visual images and categories.\nSpecifically, we first introduce a graph-based multi-label correlation module\n(GMC) to facilitate information exchange between labels, enriching the semantic\nrepresentation across the multi-label texts. Next, we propose a Semantic Visual\nFeature Reconstruction module (SVFR) to enhance the semantic information in the\nvisual representation by integrating the learned textual representation during\nreconstruction. Finally, we optimize the image-text matching capability of the\nVLP model using both local and global features to achieve zero-shot MLR.\nComprehensive experiments are conducted on several MLR benchmarks, encompassing\nboth zero-shot MLR (with unseen labels) and single positive multi-label\nlearning (with limited labels), demonstrating the superior performance of our\napproach compared to state-of-the-art methods. The code is available at\nhttps://github.com/MVL-Lab/SigRL.",
      "tldr_zh": "本文提出了一种 Semantic-guided Representation Learning (SigRL) 方法，用于解决 Multi-label Recognition (MLR) 中的标注不确定性问题，如未见或不完整标签。SigRL 通过 Graph-based multi-label correlation module (GMC) 实现标签间的信息交换以丰富语义表示，并利用 Semantic Visual Feature Reconstruction module (SVFR) 整合文本表示来增强视觉特征的语义信息，同时优化 Vision and Language Pre-training (VLP) 模型的 image-text matching 能力以支持 zero-shot MLR。实验结果显示，SigRL 在多个 MLR 基准测试中，包括 zero-shot MLR 和 single positive multi-label learning 场景，显著优于现有 state-of-the-art 方法，代码已开源于 https://github.com/MVL-Lab/SigRL。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted in ICME2025",
      "pdf_url": "http://arxiv.org/pdf/2504.03801v1",
      "published_date": "2025-04-04 08:15:08 UTC",
      "updated_date": "2025-04-04 08:15:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:43:29.307513"
    },
    {
      "arxiv_id": "2504.03245v1",
      "title": "Seeing is Believing: Belief-Space Planning with Foundation Models as Uncertainty Estimators",
      "title_zh": "眼见为实：以基础模型为不确定性估计器的信念空间规划",
      "authors": [
        "Linfeng Zhao",
        "Willie McClinton",
        "Aidan Curtis",
        "Nishanth Kumar",
        "Tom Silver",
        "Leslie Pack Kaelbling",
        "Lawson L. S. Wong"
      ],
      "abstract": "Generalizable robotic mobile manipulation in open-world environments poses\nsignificant challenges due to long horizons, complex goals, and partial\nobservability. A promising approach to address these challenges involves\nplanning with a library of parameterized skills, where a task planner sequences\nthese skills to achieve goals specified in structured languages, such as\nlogical expressions over symbolic facts. While vision-language models (VLMs)\ncan be used to ground these expressions, they often assume full observability,\nleading to suboptimal behavior when the agent lacks sufficient information to\nevaluate facts with certainty. This paper introduces a novel framework that\nleverages VLMs as a perception module to estimate uncertainty and facilitate\nsymbolic grounding. Our approach constructs a symbolic belief representation\nand uses a belief-space planner to generate uncertainty-aware plans that\nincorporate strategic information gathering. This enables the agent to\neffectively reason about partial observability and property uncertainty. We\ndemonstrate our system on a range of challenging real-world tasks that require\nreasoning in partially observable environments. Simulated evaluations show that\nour approach outperforms both vanilla VLM-based end-to-end planning or\nVLM-based state estimation baselines by planning for and executing strategic\ninformation gathering. This work highlights the potential of VLMs to construct\nbelief-space symbolic scene representations, enabling downstream tasks such as\nuncertainty-aware planning.",
      "tldr_zh": "这篇论文提出了一种新框架，使用视觉语言模型（VLMs）作为不确定性估计器，来解决机器人移动操作在开放环境中的部分可观察性挑战。该框架通过构建符号信念表示（symbolic belief representation）和belief-space planner，生成包含战略信息收集的计划，从而使代理能够有效处理不确定性和部分可观察性。实验结果显示，该方法在模拟任务中优于基于VLMs的端到端规划或状态估计基线，提升了规划效率。该工作突出了VLMs在构建信念空间符号场景表示方面的潜力，支持不确定性感知规划等下游应用。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03245v1",
      "published_date": "2025-04-04 07:48:53 UTC",
      "updated_date": "2025-04-04 07:48:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:43:40.374202"
    },
    {
      "arxiv_id": "2504.03241v1",
      "title": "Rotation Invariance in Floor Plan Digitization using Zernike Moments",
      "title_zh": "翻译失败",
      "authors": [
        "Marius Graumann",
        "Jan Marius Stürmer",
        "Tobias Koch"
      ],
      "abstract": "Nowadays, a lot of old floor plans exist in printed form or are stored as\nscanned raster images. Slight rotations or shifts may occur during scanning.\nBringing floor plans of this form into a machine readable form to enable\nfurther use, still poses a problem. Therefore, we propose an end-to-end\npipeline that pre-processes the image and leverages a novel approach to create\na region adjacency graph (RAG) from the pre-processed image and predict its\nnodes. By incorporating normalization steps into the RAG feature extraction, we\nsignificantly improved the rotation invariance of the RAG feature calculation.\nMoreover, applying our method leads to an improved F1 score and IoU on rotated\ndata. Furthermore, we proposed a wall splitting algorithm for partitioning\nwalls into segments associated with the corresponding rooms.",
      "tldr_zh": "该研究针对扫描楼层平面图可能出现的旋转或偏移问题，提出了一种端到端数字化管道，使用 Zernike Moments 实现旋转不变性。\n管道包括图像预处理、创建区域邻接图 (RAG) 并通过归一化步骤提升 RAG 特征提取的旋转不变性，同时引入墙壁分割算法将墙壁分割成与房间相关的段。\n实验结果表明，该方法在旋转数据上显著提高了 F1 score 和 IoU 指标。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.03241v1",
      "published_date": "2025-04-04 07:44:07 UTC",
      "updated_date": "2025-04-04 07:44:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:43:52.557089"
    },
    {
      "arxiv_id": "2504.03800v1",
      "title": "Decision SpikeFormer: Spike-Driven Transformer for Decision Making",
      "title_zh": "翻译失败",
      "authors": [
        "Wei Huang",
        "Qinying Gu",
        "Nanyang Ye"
      ],
      "abstract": "Offline reinforcement learning (RL) enables policy training solely on\npre-collected data, avoiding direct environment interaction - a crucial benefit\nfor energy-constrained embodied AI applications. Although Artificial Neural\nNetworks (ANN)-based methods perform well in offline RL, their high\ncomputational and energy demands motivate exploration of more efficient\nalternatives. Spiking Neural Networks (SNNs) show promise for such tasks, given\ntheir low power consumption. In this work, we introduce DSFormer, the first\nspike-driven transformer model designed to tackle offline RL via sequence\nmodeling. Unlike existing SNN transformers focused on spatial dimensions for\nvision tasks, we develop Temporal Spiking Self-Attention (TSSA) and Positional\nSpiking Self-Attention (PSSA) in DSFormer to capture the temporal and\npositional dependencies essential for sequence modeling in RL. Additionally, we\npropose Progressive Threshold-dependent Batch Normalization (PTBN), which\ncombines the benefits of LayerNorm and BatchNorm to preserve temporal\ndependencies while maintaining the spiking nature of SNNs. Comprehensive\nresults in the D4RL benchmark show DSFormer's superiority over both SNN and ANN\ncounterparts, achieving 78.4% energy savings, highlighting DSFormer's\nadvantages not only in energy efficiency but also in competitive performance.\nCode and models are public at https://wei-nijuan.github.io/DecisionSpikeFormer.",
      "tldr_zh": "这篇论文提出了 DSFormer，一种 spike-driven transformer 模型，用于离线强化学习 (Offline RL)，旨在通过序列建模实现高效决策，同时解决传统 Artificial Neural Networks (ANN) 的高计算和能量消耗问题。模型创新性地引入 Temporal Spiking Self-Attention (TSSA) 和 Positional Spiking Self-Attention (PSSA) 来捕捉序列中的时间和位置依赖，并提出 Progressive Threshold-dependent Batch Normalization (PTBN) 以保留 Spiking Neural Networks (SNNs) 的特性并优化性能。在 D4RL 基准测试中，DSFormer 优于 SNN 和 ANN 对应模型，实现了 78.4% 的能量节省，同时保持了竞争性的决策性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "This work has been accepted to CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.03800v1",
      "published_date": "2025-04-04 07:42:36 UTC",
      "updated_date": "2025-04-04 07:42:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:44:04.967227"
    },
    {
      "arxiv_id": "2504.03238v1",
      "title": "Malware Detection in Docker Containers: An Image is Worth a Thousand Logs",
      "title_zh": "翻译失败",
      "authors": [
        "Akis Nousias",
        "Efklidis Katsaros",
        "Evangelos Syrmos",
        "Panagiotis Radoglou-Grammatikis",
        "Thomas Lagkas",
        "Vasileios Argyriou",
        "Ioannis Moscholios",
        "Evangelos Markakis",
        "Sotirios Goudos",
        "Panagiotis Sarigiannidis"
      ],
      "abstract": "Malware detection is increasingly challenged by evolving techniques like\nobfuscation and polymorphism, limiting the effectiveness of traditional\nmethods. Meanwhile, the widespread adoption of software containers has\nintroduced new security challenges, including the growing threat of malicious\nsoftware injection, where a container, once compromised, can serve as entry\npoint for further cyberattacks. In this work, we address these security issues\nby introducing a method to identify compromised containers through machine\nlearning analysis of their file systems. We cast the entire software containers\ninto large RGB images via their tarball representations, and propose to use\nestablished Convolutional Neural Network architectures on a streaming,\npatch-based manner. To support our experiments, we release the COSOCO\ndataset--the first of its kind--containing 3364 large-scale RGB images of\nbenign and compromised software containers at\nhttps://huggingface.co/datasets/k3ylabs/cosoco-image-dataset. Our method\ndetects more malware and achieves higher F1 and Recall scores than all\nindividual and ensembles of VirusTotal engines, demonstrating its effectiveness\nand setting a new standard for identifying malware-compromised software\ncontainers.",
      "tldr_zh": "该研究针对恶意软件（malware）的混淆和多态性等挑战，以及软件容器（如Docker Containers）易受注入攻击的安全问题，提出了一种新型检测方法。通过将容器文件系统转换为大型RGB图像，并采用卷积神经网络（Convolutional Neural Network, CNN）在流式、基于补丁的方式进行分析，该方法实现了对受损容器的有效识别。为支持实验，研究者发布了COSOCO数据集，包含3364个良性和受损容器的大规模RGB图像。结果显示，该方法比VirusTotal引擎检测更多恶意软件，并取得了更高的F1和Recall分数，树立了识别恶意软件容器的新标准。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted at ICC-W",
      "pdf_url": "http://arxiv.org/pdf/2504.03238v1",
      "published_date": "2025-04-04 07:38:16 UTC",
      "updated_date": "2025-04-04 07:38:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:44:15.559237"
    },
    {
      "arxiv_id": "2504.03235v1",
      "title": "Crash Time Matters: HybridMamba for Fine-Grained Temporal Localization in Traffic Surveillance Footage",
      "title_zh": "碰撞时间至关重要：HybridMamba 用于交通监控视频中的细粒度时间定位",
      "authors": [
        "Ibne Farabi Shihab",
        "Anuj Sharma"
      ],
      "abstract": "Traffic crash detection in long-form surveillance videos is critical for\nemergency response and infrastructure planning but remains difficult due to the\nbrief and rare nature of crash events. We introduce HybridMamba, a novel\narchitecture that combines visual transformers with state-space temporal\nmodeling to achieve accurate crash time localization. Our method uses\nmulti-level token compression and hierarchical temporal processing to remain\ncomputationally efficient without sacrificing temporal resolution. Evaluated on\na large-scale dataset from the Iowa Department of Transportation, HybridMamba\nachieves a mean absolute error of 1.50 seconds, with 65.2 percent of\npredictions within one second of the ground truth. It outperforms recent\nvideo-language models such as TimeChat and VideoLLaMA2 by up to 2.8 seconds,\nwhile using significantly fewer parameters. Our results demonstrate strong\ngeneralization across videos ranging from 2 to 40 minutes in diverse\nconditions. HybridMamba offers a robust and efficient solution for fine-grained\ntemporal localization in traffic surveillance. The code will be released upon\npublication.",
      "tldr_zh": "本研究针对交通监控视频中事故事件短暂且稀有的挑战，提出HybridMamba架构，该框架结合Visual Transformers和State-space temporal modeling，实现精确的细粒度时间定位。HybridMamba采用多级token压缩和层次化时间处理，确保计算效率的同时保持高时间分辨率。在Iowa交通部门的大型数据集上评估，该模型的平均绝对误差为1.50秒，且65.2%的预测在地面真相1秒内，优于TimeChat和VideoLLaMA2模型2.8秒，并使用更少参数。结果显示，HybridMamba在2至40分钟的多样化视频条件下具有强泛化能力，提供了一个鲁棒、高效的交通事故检测解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03235v1",
      "published_date": "2025-04-04 07:35:11 UTC",
      "updated_date": "2025-04-04 07:35:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:44:27.393992"
    },
    {
      "arxiv_id": "2504.03234v2",
      "title": "Think When You Need: Self-Adaptive Chain-of-Thought Learning",
      "title_zh": "在需要时思考：自适应链式思维学习",
      "authors": [
        "Junjie Yang",
        "Ke Lin",
        "Xing Yu"
      ],
      "abstract": "Chain of Thought (CoT) reasoning enhances language models' performance but\noften leads to inefficient \"overthinking\" on simple problems. We identify that\nexisting approaches directly penalizing reasoning length fail to account for\nvarying problem complexity. Our approach constructs rewards through length and\nquality comparisons, guided by theoretical assumptions that jointly enhance\nsolution correctness with conciseness. Moreover, we further demonstrate our\nmethod to fuzzy tasks where ground truth is unavailable. Experiments across\nmultiple reasoning benchmarks demonstrate that our method maintains accuracy\nwhile generating significantly more concise explanations, effectively teaching\nmodels to \"think when needed.\"",
      "tldr_zh": "该论文针对 Chain of Thought (CoT) 推理在简单问题上导致的“overthinking”效率问题，提出了一种自适应学习方法，通过长度和质量比较构建奖励，并结合理论假设来同时提升解决方案的正确性和简洁性。该方法还扩展到没有 ground truth 的模糊任务中，确保模型能根据问题复杂度灵活调整推理过程。实验在多个推理基准上证明，该方法在保持准确性的同时，生成更简洁的解释，从而有效教会模型“在需要时思考”。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Under review",
      "pdf_url": "http://arxiv.org/pdf/2504.03234v2",
      "published_date": "2025-04-04 07:34:01 UTC",
      "updated_date": "2025-05-21 15:26:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:44:39.571166"
    },
    {
      "arxiv_id": "2504.03211v1",
      "title": "Persuasive Calibration",
      "title_zh": "说服性校准",
      "authors": [
        "Yiding Feng",
        "Wei Tang"
      ],
      "abstract": "We introduce and study the persuasive calibration problem, where a principal\naims to provide trustworthy predictions about underlying events to a downstream\nagent to make desired decisions. We adopt the standard calibration framework\nthat regulates predictions to be unbiased conditional on their own value, and\nthus, they can reliably be interpreted at the face value by the agent. Allowing\na small calibration error budget, we aim to answer the following question: what\nis and how to compute the optimal predictor under this calibration error\nbudget, especially when there exists incentive misalignment between the\nprincipal and the agent? We focus on standard Lt-norm Expected Calibration\nError (ECE) metric.\n  We develop a general framework by viewing predictors as post-processed\nversions of perfectly calibrated predictors. Using this framework, we first\ncharacterize the structure of the optimal predictor. Specifically, when the\nprincipal's utility is event-independent and for L1-norm ECE, we show: (1) the\noptimal predictor is over-(resp. under-) confident for high (resp. low) true\nexpected outcomes, while remaining perfectly calibrated in the middle; (2) the\nmiscalibrated predictions exhibit a collinearity structure with the principal's\nutility function. On the algorithmic side, we provide a FPTAS for computing\napproximately optimal predictor for general principal utility and general\nLt-norm ECE. Moreover, for the L1- and L-Infinity-norm ECE, we provide\npolynomial-time algorithms that compute the exact optimal predictor.",
      "tldr_zh": "本研究引入了 persuasive calibration 问题，探讨 principal 如何提供可信预测以引导 agent 做出期望决策，同时在标准 calibration 框架下确保预测在自身值条件下无偏。论文通过将预测视为对完美 calibrated 预测器的后处理，表征了最优预测器的结构：对于 L1-norm Expected Calibration Error (ECE)，最优预测器在高真实预期结果时过度自信、在低时低估自信，并在中间保持完美校准，同时显示出与 principal 效用函数的共线性。算法方面，论文提供了一个 FPTAS 用于一般 principal 效用和 Lt-norm ECE 的近似最优预测器，并针对 L1- 和 L-Infinity-norm ECE 开发了精确的多项式时间算法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.GT",
        "econ.TH"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03211v1",
      "published_date": "2025-04-04 06:49:56 UTC",
      "updated_date": "2025-04-04 06:49:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:44:52.252076"
    },
    {
      "arxiv_id": "2504.03207v1",
      "title": "Augmenting Human Cognition With Generative AI: Lessons From AI-Assisted Decision-Making",
      "title_zh": "使用生成式 AI 增强人类认知：从 AI 辅助决策中获得的经验教训",
      "authors": [
        "Zelun Tony Zhang",
        "Leon Reicherts"
      ],
      "abstract": "How can we use generative AI to design tools that augment rather than replace\nhuman cognition? In this position paper, we review our own research on\nAI-assisted decision-making for lessons to learn. We observe that in both\nAI-assisted decision-making and generative AI, a popular approach is to suggest\nAI-generated end-to-end solutions to users, which users can then accept,\nreject, or edit. Alternatively, AI tools could offer more incremental support\nto help users solve tasks themselves, which we call process-oriented support.\nWe describe findings on the challenges of end-to-end solutions, and how\nprocess-oriented support can address them. We also discuss the applicability of\nthese findings to generative AI based on a recent study in which we compared\nboth approaches to assist users in a complex decision-making task with LLMs.",
      "tldr_zh": "这篇位置论文探讨了如何利用生成式 AI 设计工具来增强而非取代人类认知，基于 AI 辅助决策研究的经验教训。作者比较了两种方法：一种是 AI 提供端到端解决方案，让用户接受、拒绝或编辑；另一种是过程导向支持（process-oriented support），即 AI 提供增量帮助以协助用户自行解决问题。研究发现，端到端解决方案面临挑战，如用户依赖性过强，而过程导向支持能更好地解决这些问题。最终，通过使用大型语言模型（LLMs）在复杂决策任务中的比较，论文强调了这些见解对生成式 AI 的实际应用价值。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03207v1",
      "published_date": "2025-04-04 06:40:03 UTC",
      "updated_date": "2025-04-04 06:40:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:45:03.232180"
    },
    {
      "arxiv_id": "2504.03206v1",
      "title": "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward",
      "title_zh": "翻译失败",
      "authors": [
        "Yanming Wan",
        "Jiaxing Wu",
        "Marwa Abdulhai",
        "Lior Shani",
        "Natasha Jaques"
      ],
      "abstract": "Effective conversational agents must be able to personalize their behavior to\nsuit a user's preferences, personality, and attributes, whether they are\nassisting with writing tasks or operating in domains like education or\nhealthcare. Current training methods like Reinforcement Learning from Human\nFeedback (RLHF) prioritize helpfulness and safety but fall short in fostering\ntruly empathetic, adaptive, and personalized interactions. Traditional\napproaches to personalization often rely on extensive user history, limiting\ntheir effectiveness for new or context-limited users. To overcome these\nlimitations, we propose to incorporate an intrinsic motivation to improve the\nconversational agents's model of the user as an additional reward alongside\nmulti-turn RLHF. This reward mechanism encourages the agent to actively elicit\nuser traits by optimizing conversations to increase the accuracy of its user\nmodel. Consequently, the policy agent can deliver more personalized\ninteractions through obtaining more information about the user. We applied our\nmethod both education and fitness settings, where LLMs teach concepts or\nrecommend personalized strategies based on users' hidden learning style or\nlifestyle attributes. Using LLM-simulated users, our approach outperformed a\nmulti-turn RLHF baseline in revealing information about the users' preferences,\nand adapting to them.",
      "tldr_zh": "本文提出了一种通过Curiosity Reward机制提升多轮对话代理个性化能力的创新方法，以解决传统RLHF（Reinforcement Learning from Human Feedback）在移情和适应性方面的不足，特别是对新用户的支持。Curiosity Reward作为内在动机奖励，与多轮RLHF结合，鼓励代理主动询问用户特征，从而提高用户模型的准确性和对话的个性化水平。在教育和健身场景中，使用LLM模拟用户进行实验，结果显示该方法比基线模型更有效地揭示用户偏好并实现适应性交互。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03206v1",
      "published_date": "2025-04-04 06:35:02 UTC",
      "updated_date": "2025-04-04 06:35:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:45:16.311342"
    },
    {
      "arxiv_id": "2504.03198v1",
      "title": "Endo3R: Unified Online Reconstruction from Dynamic Monocular Endoscopic Video",
      "title_zh": "翻译失败",
      "authors": [
        "Jiaxin Guo",
        "Wenzhen Dong",
        "Tianyu Huang",
        "Hao Ding",
        "Ziyi Wang",
        "Haomin Kuang",
        "Qi Dou",
        "Yun-Hui Liu"
      ],
      "abstract": "Reconstructing 3D scenes from monocular surgical videos can enhance surgeon's\nperception and therefore plays a vital role in various computer-assisted\nsurgery tasks. However, achieving scale-consistent reconstruction remains an\nopen challenge due to inherent issues in endoscopic videos, such as dynamic\ndeformations and textureless surfaces. Despite recent advances, current methods\neither rely on calibration or instrument priors to estimate scale, or employ\nSfM-like multi-stage pipelines, leading to error accumulation and requiring\noffline optimization. In this paper, we present Endo3R, a unified 3D foundation\nmodel for online scale-consistent reconstruction from monocular surgical video,\nwithout any priors or extra optimization. Our model unifies the tasks by\npredicting globally aligned pointmaps, scale-consistent video depths, and\ncamera parameters without any offline optimization. The core contribution of\nour method is expanding the capability of the recent pairwise reconstruction\nmodel to long-term incremental dynamic reconstruction by an uncertainty-aware\ndual memory mechanism. The mechanism maintains history tokens of both\nshort-term dynamics and long-term spatial consistency. Notably, to tackle the\nhighly dynamic nature of surgical scenes, we measure the uncertainty of tokens\nvia Sampson distance and filter out tokens with high uncertainty. Regarding the\nscarcity of endoscopic datasets with ground-truth depth and camera poses, we\nfurther devise a self-supervised mechanism with a novel dynamics-aware flow\nloss. Abundant experiments on SCARED and Hamlyn datasets demonstrate our\nsuperior performance in zero-shot surgical video depth prediction and camera\npose estimation with online efficiency. Project page:\nhttps://wrld.github.io/Endo3R/.",
      "tldr_zh": "该论文提出Endo3R，一种统一的3D基础模型，用于从动态单目内窥镜视频进行在线规模一致的重建，解决现有方法依赖校准先验或多阶段管道导致的错误积累问题。核心创新是采用不确定性感知的双记忆机制（uncertainty-aware dual memory mechanism），通过Sampson距离测量和过滤高不确定性标记，实现长期增量动态重建，并预测全局对齐的点云地图和相机参数。实验在SCARED和Hamlyn数据集上显示，Endo3R在零样本手术视频深度预测和相机位姿估计中表现出色，具有在线高效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03198v1",
      "published_date": "2025-04-04 06:05:22 UTC",
      "updated_date": "2025-04-04 06:05:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:45:29.338165"
    },
    {
      "arxiv_id": "2504.03799v2",
      "title": "Experimental Study on Time Series Analysis of Lower Limb Rehabilitation Exercise Data Driven by Novel Model Architecture and Large Models",
      "title_zh": "基于新型模型架构和大模型驱动的下肢康复锻炼数据时间序列分析的实验研究",
      "authors": [
        "Hengyu Lin"
      ],
      "abstract": "This study investigates the application of novel model architectures and\nlarge-scale foundational models in temporal series analysis of lower limb\nrehabilitation motion data, aiming to leverage advancements in machine learning\nand artificial intelligence to empower active rehabilitation guidance\nstrategies for post-stroke patients in limb motor function recovery. Utilizing\nthe SIAT-LLMD dataset of lower limb movement data proposed by the Shenzhen\nInstitute of Advanced Technology, Chinese Academy of Sciences, we\nsystematically elucidate the implementation and analytical outcomes of the\ninnovative xLSTM architecture and the foundational model Lag-Llama in\nshort-term temporal prediction tasks involving joint kinematics and dynamics\nparameters. The research provides novel insights for AI-enabled medical\nrehabilitation applications, demonstrating the potential of cutting-edge model\narchitectures and large-scale models in rehabilitation medicine temporal\nprediction. These findings establish theoretical foundations for future\napplications of personalized rehabilitation regimens, offering significant\nimplications for the development of customized therapeutic interventions in\nclinical practice.",
      "tldr_zh": "本研究探讨了新型模型架构 xLSTM 和大型基础模型 Lag-Llama 在下肢康复运动数据的时间序列分析中的应用，旨在为中风后患者的肢体运动功能恢复提供主动康复指导策略。  \n利用深圳先进技术研究院提出的 SIAT-LLMD 数据集，该研究评估了这些模型在关节运动学和动力学参数的短期时间预测任务中的实现和效果。  \n结果表明，该方法为 AI 赋能的医疗康复应用提供了新见解，并为未来的个性化康复方案和临床干预奠定了理论基础。",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03799v2",
      "published_date": "2025-04-04 05:40:13 UTC",
      "updated_date": "2025-04-29 13:28:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:45:40.216235"
    },
    {
      "arxiv_id": "2504.03798v1",
      "title": "An Intelligent and Privacy-Preserving Digital Twin Model for Aging-in-Place",
      "title_zh": "智能且隐私保护的数字孪生模型，用于原地养老",
      "authors": [
        "Yongjie Wang",
        "Jonathan Cyril Leung",
        "Ming Chen",
        "Zhiwei Zeng",
        "Benny Toh Hsiang Tan",
        "Yang Qiu",
        "Zhiqi Shen"
      ],
      "abstract": "The population of older adults is steadily increasing, with a strong\npreference for aging-in-place rather than moving to care facilities.\nConsequently, supporting this growing demographic has become a significant\nglobal challenge. However, facilitating successful aging-in-place is\nchallenging, requiring consideration of multiple factors such as data privacy,\nhealth status monitoring, and living environments to improve health outcomes.\nIn this paper, we propose an unobtrusive sensor system designed for\ninstallation in older adults' homes. Using data from the sensors, our system\nconstructs a digital twin, a virtual representation of events and activities\nthat occurred in the home. The system uses neural network models and decision\nrules to capture residents' activities and living environments. This digital\ntwin enables continuous health monitoring by providing actionable insights into\nresidents' well-being. Our system is designed to be low-cost and\nprivacy-preserving, with the aim of providing green and safe monitoring for the\nhealth of older adults. We have successfully deployed our system in two homes\nover a time period of two months, and our findings demonstrate the feasibility\nand effectiveness of digital twin technology in supporting independent living\nfor older adults. This study highlights that our system could revolutionize\nelder care by enabling personalized interventions, such as lifestyle\nadjustments, medical treatments, or modifications to the residential\nenvironment, to enhance health outcomes.",
      "tldr_zh": "本研究针对老龄化人口偏好原地养老（aging-in-place）的全球挑战，提出一个智能且隐私保护的数字孪生（digital twin）模型，通过非侵入式传感器系统实时监测老年人的健康状态和生活环境。系统利用神经网络模型（neural network models）和决策规则（decision rules）分析传感器数据，构建虚拟表示以提供可行动的健康洞见，同时确保低成本和数据隐私。实验在两户家庭部署两个月后，证明了该系统的可行性和有效性，有望通过个性化干预（如生活方式调整或环境修改）革命性地提升老年人的健康结果。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "68T05,",
        "I.2; J.3"
      ],
      "primary_category": "cs.CY",
      "comment": "accepted to IEEE TENSYMP 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.03798v1",
      "published_date": "2025-04-04 05:37:08 UTC",
      "updated_date": "2025-04-04 05:37:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:45:52.451992"
    },
    {
      "arxiv_id": "2504.03185v1",
      "title": "Learning Natural Language Constraints for Safe Reinforcement Learning of Language Agents",
      "title_zh": "学习自然语言约束以实现语言代理的安全强化学习",
      "authors": [
        "Jaymari Chua",
        "Chen Wang",
        "Lina Yao"
      ],
      "abstract": "Generalizable alignment is a core challenge for deploying Large Language\nModels (LLMs) safely in real-world NLP applications. Current alignment methods,\nincluding Reinforcement Learning from Human Feedback (RLHF), often fail to\nguarantee constraint satisfaction outside their training distribution due to\ntheir reliance on implicit, post-hoc preferences. Inspired by a paradigm shift\nto first curate data before tuning, we introduce a new framework for safe\nlanguage alignment that learns natural language constraints from positive and\nnegative demonstrations as a primary step. From inferring both a task-specific\nreward function and latent constraint functions, our approach fosters\nadaptation to novel safety requirements and robust generalization under domain\nshifts and adversarial inputs. We formalize the framework within a Constrained\nMarkov Decision Process (CMDP) and validate it via a text-based navigation\nenvironment, demonstrating safe adaptation to changing danger zones. Our\nexperiments show fewer violations upon domain shift when following a safe\nnavigation path, and we achieve zero violations by applying learned constraints\nto a distilled BERT model as a fine-tuning technique. This work offers a\npromising path toward building safety-critical and more generalizable LLMs for\npractical NLP settings.",
      "tldr_zh": "本论文针对大型语言模型（LLMs）的安全对齐问题，提出了一种新框架，通过从正负演示中学习自然语言约束，作为首要步骤，以解决现有方法如Reinforcement Learning from Human Feedback (RLHF) 在训练分布外无法保证约束满足的局限性。该框架在Constrained Markov Decision Process (CMDP) 中推断任务特定的奖励函数和潜在约束函数，从而提升模型在领域转移和对抗输入下的适应性和泛化能力。实验在文本-based导航环境中验证，结果显示该方法显著减少了违反情况，并在应用到蒸馏BERT模型的微调中实现了零违反，为构建安全可靠的LLMs提供了可行路径。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7; I.2.4; I.2.6; I.2.8"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03185v1",
      "published_date": "2025-04-04 05:26:28 UTC",
      "updated_date": "2025-04-04 05:26:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:46:04.867831"
    },
    {
      "arxiv_id": "2504.03171v1",
      "title": "Real-Time Roadway Obstacle Detection for Electric Scooters Using Deep Learning and Multi-Sensor Fusion",
      "title_zh": "翻译失败",
      "authors": [
        "Zeyang Zheng",
        "Arman Hosseini",
        "Dong Chen",
        "Omid Shoghli",
        "Arsalan Heydarian"
      ],
      "abstract": "The increasing adoption of electric scooters (e-scooters) in urban areas has\ncoincided with a rise in traffic accidents and injuries, largely due to their\nsmall wheels, lack of suspension, and sensitivity to uneven surfaces. While\ndeep learning-based object detection has been widely used to improve automobile\nsafety, its application for e-scooter obstacle detection remains unexplored.\nThis study introduces a novel ground obstacle detection system for e-scooters,\nintegrating an RGB camera, and a depth camera to enhance real-time road hazard\ndetection. Additionally, the Inertial Measurement Unit (IMU) measures linear\nvertical acceleration to identify surface vibrations, guiding the selection of\nsix obstacle categories: tree branches, manhole covers, potholes, pine cones,\nnon-directional cracks, and truncated domes. All sensors, including the RGB\ncamera, depth camera, and IMU, are integrated within the Intel RealSense Camera\nD435i. A deep learning model powered by YOLO detects road hazards and utilizes\ndepth data to estimate obstacle proximity. Evaluated on the seven hours of\nnaturalistic riding dataset, the system achieves a high mean average precision\n(mAP) of 0.827 and demonstrates excellent real-time performance. This approach\nprovides an effective solution to enhance e-scooter safety through advanced\ncomputer vision and data fusion. The dataset is accessible at\nhttps://zenodo.org/records/14583718, and the project code is hosted on\nhttps://github.com/Zeyang-Zheng/Real-Time-Roadway-Obstacle-Detection-for-Electric-Scooters.",
      "tldr_zh": "本研究针对电动滑板车（e-scooters）在城市中的事故风险，提出了一种实时路面障碍检测系统，利用深度学习和多传感器融合技术。该系统整合RGB相机、深度相机和IMU（Inertial Measurement Unit），通过IMU测量垂直加速度识别表面振动，并使用YOLO模型检测六类障碍物（如tree branches、potholes等），结合深度数据估算障碍距离。实验在七小时自然骑行数据集上评估，系统实现了mAP 0.827的高精度和出色实时性能，为提升e-scooter安全提供了有效解决方案。数据集和代码可分别从https://zenodo.org/records/14583718和https://github.com/Zeyang-Zheng/Real-Time-Roadway-Obstacle-Detection-for-Electric-Scooters获取。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at ASCE International Conference on Computing in Civil\n  Engineering (i3ce)",
      "pdf_url": "http://arxiv.org/pdf/2504.03171v1",
      "published_date": "2025-04-04 05:01:16 UTC",
      "updated_date": "2025-04-04 05:01:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:46:16.140124"
    },
    {
      "arxiv_id": "2504.03164v2",
      "title": "NuScenes-SpatialQA: A Spatial Understanding and Reasoning Benchmark for Vision-Language Models in Autonomous Driving",
      "title_zh": "翻译失败",
      "authors": [
        "Kexin Tian",
        "Jingrui Mao",
        "Yunlong Zhang",
        "Jiwan Jiang",
        "Yang Zhou",
        "Zhengzhong Tu"
      ],
      "abstract": "Recent advancements in Vision-Language Models (VLMs) have demonstrated strong\npotential for autonomous driving tasks. However, their spatial understanding\nand reasoning-key capabilities for autonomous driving-still exhibit significant\nlimitations. Notably, none of the existing benchmarks systematically evaluate\nVLMs' spatial reasoning capabilities in driving scenarios. To fill this gap, we\npropose NuScenes-SpatialQA, the first large-scale ground-truth-based\nQuestion-Answer (QA) benchmark specifically designed to evaluate the spatial\nunderstanding and reasoning capabilities of VLMs in autonomous driving. Built\nupon the NuScenes dataset, the benchmark is constructed through an automated 3D\nscene graph generation pipeline and a QA generation pipeline. The benchmark\nsystematically evaluates VLMs' performance in both spatial understanding and\nreasoning across multiple dimensions. Using this benchmark, we conduct\nextensive experiments on diverse VLMs, including both general and\nspatial-enhanced models, providing the first comprehensive evaluation of their\nspatial capabilities in autonomous driving. Surprisingly, the experimental\nresults show that the spatial-enhanced VLM outperforms in qualitative QA but\ndoes not demonstrate competitiveness in quantitative QA. In general, VLMs still\nface considerable challenges in spatial understanding and reasoning.",
      "tldr_zh": "本研究提出NuScenes-SpatialQA，这是一个首创的大型基准，用于系统评估Vision-Language Models (VLMs)在自动驾驶场景中的空间理解和推理能力，以填补现有基准的空白。基准基于NuScenes数据集，通过自动化3D场景图生成管道和QA生成管道构建，涵盖多维度的空间任务评估。实验结果显示，空间增强VLMs在定性QA中表现优于通用模型，但在定量QA中缺乏竞争力，整体表明VLMs在自动驾驶的空间理解和推理方面仍面临重大挑战。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03164v2",
      "published_date": "2025-04-04 04:43:10 UTC",
      "updated_date": "2025-04-07 03:39:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:46:27.655878"
    },
    {
      "arxiv_id": "2504.03160v4",
      "title": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments",
      "title_zh": "DeepResearcher：通过强化学习在真实世界环境中扩展深度研究",
      "authors": [
        "Yuxiang Zheng",
        "Dayuan Fu",
        "Xiangkun Hu",
        "Xiaojie Cai",
        "Lyumanshan Ye",
        "Pengrui Lu",
        "Pengfei Liu"
      ],
      "abstract": "Large Language Models (LLMs) equipped with web search capabilities have\ndemonstrated impressive potential for deep research tasks. However, current\napproaches predominantly rely on either manually engineered prompts (prompt\nengineering-based) with brittle performance or reinforcement learning within\ncontrolled Retrieval-Augmented Generation (RAG) environments (RAG-based) that\nfail to capture the complexities of real-world interaction. In this paper, we\nintroduce DeepResearcher, the first comprehensive framework for end-to-end\ntraining of LLM-based deep research agents through scaling reinforcement\nlearning (RL) in real-world environments with authentic web search\ninteractions. Unlike RAG-based approaches that assume all necessary information\nexists within a fixed corpus, our method trains agents to navigate the noisy,\nunstructured, and dynamic nature of the open web. We implement a specialized\nmulti-agent architecture where browsing agents extract relevant information\nfrom various webpage structures and overcoming significant technical\nchallenges. Extensive experiments on open-domain research tasks demonstrate\nthat DeepResearcher achieves substantial improvements of up to 28.9 points over\nprompt engineering-based baselines and up to 7.2 points over RAG-based RL\nagents. Our qualitative analysis reveals emergent cognitive behaviors from\nend-to-end RL training, including the ability to formulate plans,\ncross-validate information from multiple sources, engage in self-reflection to\nredirect research, and maintain honesty when unable to find definitive answers.\nOur results highlight that end-to-end training in real-world web environments\nis not merely an implementation detail but a fundamental requirement for\ndeveloping robust research capabilities aligned with real-world applications.\nWe release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher.",
      "tldr_zh": "该研究引入了 DeepResearcher 框架，通过在真实世界环境中使用强化学习 (RL) 训练大型语言模型 (LLMs) 的深度研究代理，解决现有基于提示工程和受控 Retrieval-Augmented Generation (RAG) 方法的局限性。该框架采用多代理架构，让代理处理开放 web 的噪声、动态性和结构化挑战，并从各种网页中提取信息。实验结果显示，在开放领域任务上，DeepResearcher 比提示工程基准提高多达 28.9 分，比 RAG-based RL 代理提高多达 7.2 分，并展现出制定计划、跨源验证信息、自我反思和诚实处理不确定性的认知行为。该框架强调端到端真实环境训练是开发鲁棒研究能力的根本要求，并已开源代码。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03160v4",
      "published_date": "2025-04-04 04:41:28 UTC",
      "updated_date": "2025-04-17 04:46:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:46:41.455312"
    },
    {
      "arxiv_id": "2504.08778v1",
      "title": "From Tokens to Lattices: Emergent Lattice Structures in Language Models",
      "title_zh": "从词元到格子：语言模型中涌现的格子结构",
      "authors": [
        "Bo Xiong",
        "Steffen Staab"
      ],
      "abstract": "Pretrained masked language models (MLMs) have demonstrated an impressive\ncapability to comprehend and encode conceptual knowledge, revealing a lattice\nstructure among concepts. This raises a critical question: how does this\nconceptualization emerge from MLM pretraining? In this paper, we explore this\nproblem from the perspective of Formal Concept Analysis (FCA), a mathematical\nframework that derives concept lattices from the observations of\nobject-attribute relationships. We show that the MLM's objective implicitly\nlearns a \\emph{formal context} that describes objects, attributes, and their\ndependencies, which enables the reconstruction of a concept lattice through\nFCA. We propose a novel framework for concept lattice construction from\npretrained MLMs and investigate the origin of the inductive biases of MLMs in\nlattice structure learning. Our framework differs from previous work because it\ndoes not rely on human-defined concepts and allows for discovering \"latent\"\nconcepts that extend beyond human definitions. We create three datasets for\nevaluation, and the empirical results verify our hypothesis.",
      "tldr_zh": "本研究探讨了预训练掩码语言模型（MLMs）如何从训练中产生概念格结构，揭示了概念知识的内在组织形式。研究采用Formal Concept Analysis (FCA)框架，证明MLMs的目标函数隐式学习了一个formal context，包括对象、属性及其依赖关系，从而能够通过FCA重建概念格。论文提出一个新框架，从预训练MLMs中构建概念格，该框架不依赖人类定义的概念，而是发现“潜在”概念，并分析MLMs在格结构学习中的归纳偏差。实验在三个数据集上验证了这一假设，证明了框架的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.08778v1",
      "published_date": "2025-04-04 04:28:17 UTC",
      "updated_date": "2025-04-04 04:28:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:46:51.582286"
    },
    {
      "arxiv_id": "2504.03147v1",
      "title": "A Human Digital Twin Architecture for Knowledge-based Interactions and Context-Aware Conversations",
      "title_zh": "翻译失败",
      "authors": [
        "Abdul Mannan Mohammed",
        "Azhar Ali Mohammad",
        "Jason A. Ortiz",
        "Carsten Neumann",
        "Grace Bochenek",
        "Dirk Reiners",
        "Carolina Cruz-Neira"
      ],
      "abstract": "Recent developments in Artificial Intelligence (AI) and Machine Learning (ML)\nare creating new opportunities for Human-Autonomy Teaming (HAT) in tasks,\nmissions, and continuous coordinated activities. A major challenge is enabling\nhumans to maintain awareness and control over autonomous assets, while also\nbuilding trust and supporting shared contextual understanding. To address this,\nwe present a real-time Human Digital Twin (HDT) architecture that integrates\nLarge Language Models (LLMs) for knowledge reporting, answering, and\nrecommendation, embodied in a visual interface.\n  The system applies a metacognitive approach to enable personalized,\ncontext-aware responses aligned with the human teammate's expectations. The HDT\nacts as a visually and behaviorally realistic team member, integrated\nthroughout the mission lifecycle, from training to deployment to after-action\nreview. Our architecture includes speech recognition, context processing,\nAI-driven dialogue, emotion modeling, lip-syncing, and multimodal feedback. We\ndescribe the system design, performance metrics, and future development\ndirections for more adaptive and realistic HAT systems.",
      "tldr_zh": "这篇论文提出了一种 Human Digital Twin (HDT) 架构，旨在提升 Human-Autonomy Teaming (HAT) 中的知识交互和上下文感知对话，通过整合 Large Language Models (LLMs) 实现实时知识报告、回答和推荐。系统采用元认知方法，提供个性化和上下文感知响应，并包括语音识别、上下文处理、AI 驱动对话、情感建模、唇同步和多模态反馈等组件，使 HDT 像一个视觉和行为真实的团队成员贯穿任务生命周期。实验评估了系统设计和性能指标，并探讨了未来发展方向，以创建更适应性和真实的 HAT 系统。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Presented at: 2024 Interservice/Industry Training, Simulation, and\n  Education Conference (I/ITSEC), Paper No. 24366, 10 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.03147v1",
      "published_date": "2025-04-04 03:56:26 UTC",
      "updated_date": "2025-04-04 03:56:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:47:04.910147"
    },
    {
      "arxiv_id": "2504.03794v1",
      "title": "Entropy-Based Block Pruning for Efficient Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Liangwei Yang",
        "Yuhui Xu",
        "Juntao Tan",
        "Doyen Sahoo",
        "Silvio Savarese",
        "Caiming Xiong",
        "Huan Wang",
        "Shelby Heinecke"
      ],
      "abstract": "As large language models continue to scale, their growing computational and\nstorage demands pose significant challenges for real-world deployment. In this\nwork, we investigate redundancy within Transformer-based models and propose an\nentropy-based pruning strategy to enhance efficiency while maintaining\nperformance. Empirical analysis reveals that the entropy of hidden\nrepresentations decreases in the early blocks but progressively increases\nacross most subsequent blocks. This trend suggests that entropy serves as a\nmore effective measure of information richness within computation blocks.\nUnlike cosine similarity, which primarily captures geometric relationships,\nentropy directly quantifies uncertainty and information content, making it a\nmore reliable criterion for pruning. Extensive experiments demonstrate that our\nentropy-based pruning approach surpasses cosine similarity-based methods in\nreducing model size while preserving accuracy, offering a promising direction\nfor efficient model deployment.",
      "tldr_zh": "本研究针对大型语言模型的计算和存储需求问题，提出了一种基于熵的块剪枝策略，以提升Transformer-based模型的效率，同时保持性能。通过实证分析发现，隐藏表示的熵在早期块中下降但随后增加，这使得熵作为信息丰富度的度量比余弦相似度更可靠，因为它直接量化不确定性和信息内容。实验结果表明，该方法在减少模型大小方面优于余弦相似度-based方法，并有效维持准确性，为高效模型部署提供了有前景的方向。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.03794v1",
      "published_date": "2025-04-04 03:42:34 UTC",
      "updated_date": "2025-04-04 03:42:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:47:15.976974"
    },
    {
      "arxiv_id": "2504.03793v1",
      "title": "Outlook Towards Deployable Continual Learning for Particle Accelerators",
      "title_zh": "粒子加速器的可部署持续学习展望",
      "authors": [
        "Kishansingh Rajput",
        "Sen Lin",
        "Auralee Edelen",
        "Willem Blokland",
        "Malachi Schram"
      ],
      "abstract": "Particle Accelerators are high power complex machines. To ensure\nuninterrupted operation of these machines, thousands of pieces of equipment\nneed to be synchronized, which requires addressing many challenges including\ndesign, optimization and control, anomaly detection and machine protection.\nWith recent advancements, Machine Learning (ML) holds promise to assist in more\nadvance prognostics, optimization, and control. While ML based solutions have\nbeen developed for several applications in particle accelerators, only few have\nreached deployment and even fewer to long term usage, due to particle\naccelerator data distribution drifts caused by changes in both measurable and\nnon-measurable parameters. In this paper, we identify some of the key areas\nwithin particle accelerators where continual learning can allow maintenance of\nML model performance with distribution drifts. Particularly, we first discuss\nexisting applications of ML in particle accelerators, and their limitations due\nto distribution drift. Next, we review existing continual learning techniques\nand investigate their potential applications to address data distribution\ndrifts in accelerators. By identifying the opportunities and challenges in\napplying continual learning, this paper seeks to open up the new field and\ninspire more research efforts towards deployable continual learning for\nparticle accelerators.",
      "tldr_zh": "该论文探讨了粒子加速器的高复杂性及其操作挑战，包括设计、优化、控制、异常检测和机器保护，并强调 Machine Learning (ML) 在高级预后和优化中的潜力。然而，现有的 ML 应用因数据分布漂移（由可测量和不可测量参数变化引起）而难以部署或长期使用。论文识别了关键领域，如 ML 模型性能维护，并回顾了 continual learning 技术及其在解决数据分布漂移中的潜在应用，最终旨在揭示机会和挑战，激励更多研究以实现可部署的 continual learning 在粒子加速器领域的创新。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "41 pages, 6 figures, submitted to Machine Learning: Science and\n  Technology Journal",
      "pdf_url": "http://arxiv.org/pdf/2504.03793v1",
      "published_date": "2025-04-04 03:34:39 UTC",
      "updated_date": "2025-04-04 03:34:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:47:28.456977"
    },
    {
      "arxiv_id": "2504.03137v1",
      "title": "LightPROF: A Lightweight Reasoning Framework for Large Language Model on Knowledge Graph",
      "title_zh": "翻译失败",
      "authors": [
        "Tu Ao",
        "Yanhua Yu",
        "Yuling Wang",
        "Yang Deng",
        "Zirui Guo",
        "Liang Pang",
        "Pinghui Wang",
        "Tat-Seng Chua",
        "Xiao Zhang",
        "Zhen Cai"
      ],
      "abstract": "Large Language Models (LLMs) have impressive capabilities in text\nunderstanding and zero-shot reasoning. However, delays in knowledge updates may\ncause them to reason incorrectly or produce harmful results. Knowledge Graphs\n(KGs) provide rich and reliable contextual information for the reasoning\nprocess of LLMs by structurally organizing and connecting a wide range of\nentities and relations. Existing KG-based LLM reasoning methods only inject\nKGs' knowledge into prompts in a textual form, ignoring its structural\ninformation. Moreover, they mostly rely on close-source models or open-source\nmodels with large parameters, which poses challenges to high resource\nconsumption. To address this, we propose a novel Lightweight and efficient\nPrompt learning-ReasOning Framework for KGQA (LightPROF), which leverages the\nfull potential of LLMs to tackle complex reasoning tasks in a\nparameter-efficient manner. Specifically, LightPROF follows a\n\"Retrieve-Embed-Reason process\", first accurately, and stably retrieving the\ncorresponding reasoning graph from the KG through retrieval module. Next,\nthrough a Transformer-based Knowledge Adapter, it finely extracts and\nintegrates factual and structural information from the KG, then maps this\ninformation to the LLM's token embedding space, creating an LLM-friendly prompt\nto be used by the LLM for the final reasoning. Additionally, LightPROF only\nrequires training Knowledge Adapter and can be compatible with any open-source\nLLM. Extensive experiments on two public KGQA benchmarks demonstrate that\nLightPROF achieves superior performance with small-scale LLMs. Furthermore,\nLightPROF shows significant advantages in terms of input token count and\nreasoning time.",
      "tldr_zh": "本文提出LightPROF，一种轻量级推理框架，用于提升Large Language Models (LLMs)在Knowledge Graphs (KGs)上的复杂推理任务，解决现有方法忽略KG结构信息和资源消耗高的难题。LightPROF采用“Retrieve-Embed-Reason”过程：首先通过检索模块从KG中准确获取推理图，然后利用基于Transformer的Knowledge Adapter提取并整合KG的事实和结构信息，并映射到LLMs的token嵌入空间生成友好提示。实验在两个公共KGQA基准上表明，LightPROF在使用小规模LLMs时表现出优越性能，并在输入token数量和推理时间方面显著优势。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper has been accepted by AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.03137v1",
      "published_date": "2025-04-04 03:03:47 UTC",
      "updated_date": "2025-04-04 03:03:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:47:41.440425"
    },
    {
      "arxiv_id": "2504.03135v2",
      "title": "Hierarchical Modeling for Medical Visual Question Answering with Cross-Attention Fusion",
      "title_zh": "医疗视觉问答的层次化建模与交叉注意力融合",
      "authors": [
        "Junkai Zhang",
        "Bin Li",
        "Shoujun Zhou",
        "Yue Du"
      ],
      "abstract": "Medical Visual Question Answering (Med-VQA) answers clinical questions using\nmedical images, aiding diagnosis. Designing the MedVQA system holds profound\nimportance in assisting clinical diagnosis and enhancing diagnostic accuracy.\nBuilding upon this foundation, Hierarchical Medical VQA extends Medical VQA by\norganizing medical questions into a hierarchical structure and making\nlevel-specific predictions to handle fine-grained distinctions. Recently, many\nstudies have proposed hierarchical MedVQA tasks and established datasets,\nHowever, several issues still remain: (1) imperfect hierarchical modeling leads\nto poor differentiation between question levels causing semantic fragmentation\nacross hierarchies. (2) Excessive reliance on implicit learning in\nTransformer-based cross-modal self-attention fusion methods, which obscures\ncrucial local semantic correlations in medical scenarios. To address these\nissues, this study proposes a HiCA-VQA method, including two modules:\nHierarchical Prompting for fine-grained medical questions and Hierarchical\nAnswer Decoders. The hierarchical prompting module pre-aligns hierarchical text\nprompts with image features to guide the model in focusing on specific image\nregions according to question types, while the hierarchical decoder performs\nseparate predictions for questions at different levels to improve accuracy\nacross granularities. The framework also incorporates a cross-attention fusion\nmodule where images serve as queries and text as key-value pairs. Experiments\non the Rad-Restruct benchmark demonstrate that the HiCA-VQA framework better\noutperforms existing state-of-the-art methods in answering hierarchical\nfine-grained questions. This study provides an effective pathway for\nhierarchical visual question answering systems, advancing medical image\nunderstanding.",
      "tldr_zh": "本论文提出 HiCA-VQA 方法，用于解决 Medical Visual Question Answering (Med-VQA) 中的层次化建模问题，包括语义碎片化和 Transformer-based 交叉注意力融合的隐式学习不足。方法包括 Hierarchical Prompting 模块，用于预对齐层次化文本提示与图像特征，引导模型关注特定图像区域；以及 Hierarchical Answer Decoders，进行层级-specific 预测以提升准确性；此外，还整合了图像作为查询、文本作为键-值对的 Cross-Attention Fusion 模块。实验在 Rad-Restruct benchmark 上显示，HiCA-VQA 优于现有最先进方法，提供了一种有效的途径来推进医疗图像理解和临床诊断辅助。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03135v2",
      "published_date": "2025-04-04 03:03:12 UTC",
      "updated_date": "2025-04-10 11:52:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:47:53.827808"
    },
    {
      "arxiv_id": "2504.03792v1",
      "title": "DP-LET: An Efficient Spatio-Temporal Network Traffic Prediction Framework",
      "title_zh": "DP-LET：高效的时空网络流量预测框架",
      "authors": [
        "Xintong Wang",
        "Haihan Nan",
        "Ruidong Li",
        "Huaming Wu"
      ],
      "abstract": "Accurately predicting spatio-temporal network traffic is essential for\ndynamically managing computing resources in modern communication systems and\nminimizing energy consumption. Although spatio-temporal traffic prediction has\nreceived extensive research attention, further improvements in prediction\naccuracy and computational efficiency remain necessary. In particular, existing\ndecomposition-based methods or hybrid architectures often incur heavy overhead\nwhen capturing local and global feature correlations, necessitating novel\napproaches that optimize accuracy and complexity. In this paper, we propose an\nefficient spatio-temporal network traffic prediction framework, DP-LET, which\nconsists of a data processing module, a local feature enhancement module, and a\nTransformer-based prediction module. The data processing module is designed for\nhigh-efficiency denoising of network data and spatial decoupling. In contrast,\nthe local feature enhancement module leverages multiple Temporal Convolutional\nNetworks (TCNs) to capture fine-grained local features. Meanwhile, the\nprediction module utilizes a Transformer encoder to model long-term\ndependencies and assess feature relevance. A case study on real-world cellular\ntraffic prediction demonstrates the practicality of DP-LET, which maintains low\ncomputational complexity while achieving state-of-the-art performance,\nsignificantly reducing MSE by 31.8% and MAE by 23.1% compared to baseline\nmodels.",
      "tldr_zh": "本研究提出了一种高效的时空网络流量预测框架DP-LET，旨在优化预测准确性和计算复杂度，以动态管理计算资源并降低能源消耗。DP-LET包括三个关键模块：数据处理模块用于高效去噪和空间解耦、局部特征增强模块利用多个Temporal Convolutional Networks (TCNs)捕获细粒度局部特征，以及基于Transformer的预测模块建模长期依赖性和特征相关性。在真实世界蜂窝流量预测案例中，该框架保持低计算复杂度，同时比基线模型将MSE降低31.8%和MAE降低23.1%，实现了最先进性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.03792v1",
      "published_date": "2025-04-04 02:52:43 UTC",
      "updated_date": "2025-04-04 02:52:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:48:04.282250"
    },
    {
      "arxiv_id": "2504.03129v1",
      "title": "GraphSeg: Segmented 3D Representations via Graph Edge Addition and Contraction",
      "title_zh": "翻译失败",
      "authors": [
        "Haozhan Tang",
        "Tianyi Zhang",
        "Oliver Kroemer",
        "Matthew Johnson-Roberson",
        "Weiming Zhi"
      ],
      "abstract": "Robots operating in unstructured environments often require accurate and\nconsistent object-level representations. This typically requires segmenting\nindividual objects from the robot's surroundings. While recent large models\nsuch as Segment Anything (SAM) offer strong performance in 2D image\nsegmentation. These advances do not translate directly to performance in the\nphysical 3D world, where they often over-segment objects and fail to produce\nconsistent mask correspondences across views. In this paper, we present\nGraphSeg, a framework for generating consistent 3D object segmentations from a\nsparse set of 2D images of the environment without any depth information.\nGraphSeg adds edges to graphs and constructs dual correspondence graphs: one\nfrom 2D pixel-level similarities and one from inferred 3D structure. We\nformulate segmentation as a problem of edge addition, then subsequent graph\ncontraction, which merges multiple 2D masks into unified object-level\nsegmentations. We can then leverage \\emph{3D foundation models} to produce\nsegmented 3D representations. GraphSeg achieves robust segmentation with\nsignificantly fewer images and greater accuracy than prior methods. We\ndemonstrate state-of-the-art performance on tabletop scenes and show that\nGraphSeg enables improved performance on downstream robotic manipulation tasks.\nCode available at https://github.com/tomtang502/graphseg.git.",
      "tldr_zh": "该论文提出GraphSeg框架，用于从稀疏的2D图像生成一致的3D物体分割，解决现有模型如Segment Anything (SAM)在3D世界中过度分割和跨视图不一致的问题，而无需深度信息。GraphSeg通过添加边到图并构建双重对应图（基于2D像素级相似性和推断的3D结构），将分割问题转化为边添加和图收缩过程，从而将多个2D掩码合并成统一的物体级表示，并利用3D foundation models产生分割的3D表示。实验结果显示，GraphSeg在更少的图像下实现更鲁棒的分割，在桌面场景中达到最先进性能，并显著提升下游机器人操作任务的准确性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03129v1",
      "published_date": "2025-04-04 02:42:45 UTC",
      "updated_date": "2025-04-04 02:42:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:48:16.016897"
    },
    {
      "arxiv_id": "2504.03119v1",
      "title": "Graph Network Modeling Techniques for Visualizing Human Mobility Patterns",
      "title_zh": "用于可视化人类移动模式的图网络建模技术",
      "authors": [
        "Sinjini Mitra",
        "Anuj Srivastava",
        "Avipsa Roy",
        "Pavan Turaga"
      ],
      "abstract": "Human mobility analysis at urban-scale requires models to represent the\ncomplex nature of human movements, which in turn are affected by accessibility\nto nearby points of interest, underlying socioeconomic factors of a place, and\nlocal transport choices for people living in a geographic region. In this work,\nwe represent human mobility and the associated flow of movements as a grapyh.\nGraph-based approaches for mobility analysis are still in their early stages of\nadoption and are actively being researched. The challenges of graph-based\nmobility analysis are multifaceted - the lack of sufficiently high-quality data\nto represent flows at high spatial and teporal resolution whereas, limited\ncomputational resources to translate large voluments of mobility data into a\nnetwork structure, and scaling issues inherent in graph models etc. The current\nstudy develops a methodology by embedding graphs into a continuous space, which\nalleviates issues related to fast graph matching, graph time-series modeling,\nand visualization of mobility dynamics. Through experiments, we demonstrate how\nmobility data collected from taxicab trajectories could be transformed into\nnetwork structures and patterns of mobility flow changes, and can be used for\ndownstream tasks reporting approx 40% decrease in error on average in matched\ngraphs vs unmatched ones.",
      "tldr_zh": "本研究探讨了使用图网络(Graph Network)建模技术来可视化人类移动模式(Human Mobility Patterns)，旨在捕捉城市规模下人类移动的复杂性，包括附近兴趣点(Points of Interest)、socioeconomic factors和本地交通选择的影响。作者提出了一种将图嵌入连续空间的方法，以解决图网络分析面临的挑战，如数据质量不足、计算资源限制和缩放问题，从而实现快速图匹配(Graph Matching)、图时间序列建模和移动动态可视化。实验结果显示，通过从出租车轨迹数据中构建网络结构，该方法在下游任务中平均错误减少约40%，为更精确的移动分析提供了新途径。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03119v1",
      "published_date": "2025-04-04 02:21:44 UTC",
      "updated_date": "2025-04-04 02:21:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:48:27.619207"
    },
    {
      "arxiv_id": "2504.03118v1",
      "title": "NuWa: Deriving Lightweight Task-Specific Vision Transformers for Edge Devices",
      "title_zh": "NuWa：为边缘设备衍生轻量级任务特定视觉Transformer",
      "authors": [
        "Ziteng Wei",
        "Qiang He",
        "Bing Li",
        "Feifei Chen",
        "Yun Yang"
      ],
      "abstract": "Vision Transformers (ViTs) excel in computer vision tasks but lack\nflexibility for edge devices' diverse needs. A vital issue is that ViTs\npre-trained to cover a broad range of tasks are \\textit{over-qualified} for\nedge devices that usually demand only part of a ViT's knowledge for specific\ntasks. Their task-specific accuracy on these edge devices is suboptimal. We\ndiscovered that small ViTs that focus on device-specific tasks can improve\nmodel accuracy and in the meantime, accelerate model inference. This paper\npresents NuWa, an approach that derives small ViTs from the base ViT for edge\ndevices with specific task requirements. NuWa can transfer task-specific\nknowledge extracted from the base ViT into small ViTs that fully leverage\nconstrained resources on edge devices to maximize model accuracy with inference\nlatency assurance. Experiments with three base ViTs on three public datasets\ndemonstrate that compared with state-of-the-art solutions, NuWa improves model\naccuracy by up to $\\text{11.83}\\%$ and accelerates model inference by\n1.29$\\times$ - 2.79$\\times$. Code for reproduction is available at\nhttps://anonymous.4open.science/r/Task_Specific-3A5E.",
      "tldr_zh": "该研究发现，预训练的Vision Transformers (ViTs) 过于全面（over-qualified），导致在边缘设备上处理特定任务时准确性和推理效率不足。为此，提出NuWa方法，从基础ViT派生出轻量级、任务特定的小型ViTs，通过转移任务特定知识，优化边缘设备的资源利用以最大化模型准确性并确保推理延迟。实验在三个基础ViT和三个公共数据集上显示，NuWa相较现有解决方案提高模型准确率高达11.83%，并加速推理1.29×至2.79×。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 12 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.03118v1",
      "published_date": "2025-04-04 02:19:01 UTC",
      "updated_date": "2025-04-04 02:19:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:48:39.583852"
    },
    {
      "arxiv_id": "2504.03108v1",
      "title": "Multi-Granularity Vision Fastformer with Fusion Mechanism for Skin Lesion Segmentation",
      "title_zh": "多粒度视觉 Fastformer 融合机制用于皮肤病变分割",
      "authors": [
        "Xuanyu Liu",
        "Huiyun Yao",
        "Jinggui Gao",
        "Zhongyi Guo",
        "Xue Zhang",
        "Yulin Dong"
      ],
      "abstract": "Background:Convolutional Neural Networks(CNN) and Vision Transformers(ViT)\nare the main techniques used in Medical image segmentation. However, CNN is\nlimited to local contextual information, and ViT's quadratic complexity results\nin significant computational costs. At the same time, equipping the model to\ndistinguish lesion boundaries with varying degrees of severity is also a\nchallenge encountered in skin lesion segmentation. Purpose:This research aims\nto optimize the balance between computational costs and long-range dependency\nmodelling and achieve excellent generalization across lesions with different\ndegrees of severity. Methods:we propose a lightweight U-shape network that\nutilizes Vision Fastformer with Fusion Mechanism (VFFM-UNet). We inherit the\nadvantages of Fastformer's additive attention mechanism, combining element-wise\nproduct and matrix product for comprehensive feature extraction and channel\nreduction to save computational costs. In order to accurately identify the\nlesion boundaries with varying degrees of severity, we designed Fusion\nMechanism including Multi-Granularity Fusion and Channel Fusion, which can\nprocess the feature maps in the granularity and channel levels to obtain\ndifferent contextual information. Results:Comprehensive experiments on the\nISIC2017, ISIC2018 and PH2 datasets demonstrate that VFFM-UNet outperforms\nexisting state-of-the-art models regarding parameter numbers, computational\ncomplexity and segmentation performance. In short, compared to MISSFormer, our\nmodel achieves superior segmentation performance while reducing parameter and\ncomputation costs by 101x and 15x, respectively. Conclusions:Both quantitative\nand qualitative analyses show that VFFM-UNet sets a new benchmark by reaching\nan ideal balance between parameter numbers, computational complexity, and\nsegmentation performance compared to existing state-of-the-art models.",
      "tldr_zh": "这篇论文针对皮肤病变分割中的挑战，提出了一种轻量级 U-shape 网络 VFFM-UNet，以优化 CNN 的局部限制和 ViT 的高计算成本，同时提升对不同严重度病变边界的识别能力。方法包括继承 Vision Fastformer 的加性注意力机制，并引入 Fusion Mechanism（如 Multi-Granularity Fusion 和 Channel Fusion），在粒度和通道级别处理特征图以提取全面上下文信息。实验结果显示，在 ISIC2017、ISIC2018 和 PH2 数据集上，VFFM-UNet 相比现有最先进模型（如 MISSFormer）显著提升分割性能，同时参数和计算成本分别减少 101 倍和 15 倍。总之，该框架在参数效率、计算复杂性和性能之间实现了理想平衡。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03108v1",
      "published_date": "2025-04-04 01:27:43 UTC",
      "updated_date": "2025-04-04 01:27:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:48:53.394165"
    },
    {
      "arxiv_id": "2504.03093v1",
      "title": "Post-processing for Fair Regression via Explainable SVD",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiqun Zuo",
        "Ding Zhu",
        "Mohammad Mahdi Khalili"
      ],
      "abstract": "This paper presents a post-processing algorithm for training fair neural\nnetwork regression models that satisfy statistical parity, utilizing an\nexplainable singular value decomposition (SVD) of the weight matrix. We propose\na linear transformation of the weight matrix, whereby the singular values\nderived from the SVD of the transformed matrix directly correspond to the\ndifferences in the first and second moments of the output distributions across\ntwo groups. Consequently, we can convert the fairness constraints into\nconstraints on the singular values. We analytically solve the problem of\nfinding the optimal weights under these constraints. Experimental validation on\nvarious datasets demonstrates that our method achieves a similar or superior\nfairness-accuracy trade-off compared to the baselines without using the\nsensitive attribute at the inference time.",
      "tldr_zh": "这篇论文提出了一种基于可解释SVD的后处理算法，用于训练公平的神经网络回归模型，以满足统计平价（statistical parity）。该算法通过对权重矩阵的线性变换，将公平约束转换为奇异值（singular values）的约束，并分析性地求解最优权重，从而直接对应于不同组的输出分布第一和第二矩的差异。实验验证显示，该方法在各种数据集上实现了与基线相当或更好的公平性-准确性权衡，且在推理时无需使用敏感属性（sensitive attribute）。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03093v1",
      "published_date": "2025-04-04 00:10:01 UTC",
      "updated_date": "2025-04-04 00:10:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:49:03.322184"
    },
    {
      "arxiv_id": "2504.03092v1",
      "title": "Machine Learning-Based Detection and Analysis of Suspicious Activities in Bitcoin Wallet Transactions in the USA",
      "title_zh": "翻译失败",
      "authors": [
        "Md Zahidul Islam",
        "Md Shahidul Islam",
        "Biswajit Chandra das",
        "Syed Ali Reza",
        "Proshanta Kumar Bhowmik",
        "Kanchon Kumar Bishnu",
        "Md Shafiqur Rahman",
        "Redoyan Chowdhury",
        "Laxmi Pant"
      ],
      "abstract": "The dramatic adoption of Bitcoin and other cryptocurrencies in the USA has\nrevolutionized the financial landscape and provided unprecedented investment\nand transaction efficiency opportunities. The prime objective of this research\nproject is to develop machine learning algorithms capable of effectively\nidentifying and tracking suspicious activity in Bitcoin wallet transactions.\nWith high-tech analysis, the study aims to create a model with a feature for\nidentifying trends and outliers that can expose illicit activity. The current\nstudy specifically focuses on Bitcoin transaction information in America, with\na strong emphasis placed on the importance of knowing about the immediate\nenvironment in and through which such transactions pass through. The dataset is\ncomposed of in-depth Bitcoin wallet transactional information, including\nimportant factors such as transaction values, timestamps, network flows, and\naddresses for wallets. All entries in the dataset expose information about\nfinancial transactions between wallets, including received and sent\ntransactions, and such information is significant for analysis and trends that\ncan represent suspicious activity. This study deployed three accredited\nalgorithms, most notably, Logistic Regression, Random Forest, and Support\nVector Machines. In retrospect, Random Forest emerged as the best model with\nthe highest F1 Score, showcasing its ability to handle non-linear relationships\nin the data. Insights revealed significant patterns in wallet activity, such as\nthe correlation between unredeemed transactions and final balances. The\napplication of machine algorithms in tracking cryptocurrencies is a tool for\ncreating transparent and secure U.S. markets.",
      "tldr_zh": "这篇论文开发了基于 Machine Learning 的算法，用于检测和分析美国比特币钱包交易中的可疑活动，重点关注交易值、时间戳、网络流和钱包地址等数据集。研究比较了 Logistic Regression、Random Forest 和 Support Vector Machines 三种算法，结果显示 Random Forest 以最高的 F1 Score 表现出色，能够有效处理数据中的非线性关系。论文揭示了钱包活动的关键模式，如未赎回交易与最终余额的相关性，并强调这种方法有助于提升比特币市场的透明度和安全性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages,7 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.03092v1",
      "published_date": "2025-04-04 00:07:32 UTC",
      "updated_date": "2025-04-04 00:07:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:49:15.884628"
    },
    {
      "arxiv_id": "2504.13884v1",
      "title": "Towards a Multimodal Document-grounded Conversational AI System for Education",
      "title_zh": "翻译失败",
      "authors": [
        "Karan Taneja",
        "Anjali Singh",
        "Ashok K. Goel"
      ],
      "abstract": "Multimedia learning using text and images has been shown to improve learning\noutcomes compared to text-only instruction. But conversational AI systems in\neducation predominantly rely on text-based interactions while multimodal\nconversations for multimedia learning remain unexplored. Moreover, deploying\nconversational AI in learning contexts requires grounding in reliable sources\nand verifiability to create trust. We present MuDoC, a Multimodal\nDocument-grounded Conversational AI system based on GPT-4o, that leverages both\ntext and visuals from documents to generate responses interleaved with text and\nimages. Its interface allows verification of AI generated content through\nseamless navigation to the source. We compare MuDoC to a text-only system to\nexplore differences in learner engagement, trust in AI system, and their\nperformance on problem-solving tasks. Our findings indicate that both visuals\nand verifiability of content enhance learner engagement and foster trust;\nhowever, no significant impact in performance was observed. We draw upon\ntheories from cognitive and learning sciences to interpret the findings and\nderive implications, and outline future directions for the development of\nmultimodal conversational AI systems in education.",
      "tldr_zh": "该论文探讨了多模态对话 AI 在教育中的应用，提出 MuDoC 系统，这是一种基于 GPT-4o 的 Multimodal Document-grounded Conversational AI，用于整合文档中的文本和图像以提升多媒体学习。MuDoC 通过生成交织文本和图像的响应，并提供无缝内容验证功能，帮助学习者提升参与度和信任。实验比较显示，与纯文本系统相比，MuDoC 显著提高了学习者参与度和信任水平，但未对问题解决性能产生显著影响。研究基于认知和学习科学理论解读这些发现，并为未来多模态对话 AI 的发展提供了方向。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.HC",
      "comment": "15 pages, 4 figures, AIED 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.13884v1",
      "published_date": "2025-04-04 00:04:19 UTC",
      "updated_date": "2025-04-04 00:04:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T09:49:28.682559"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 106,
  "processed_papers_count": 106,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-24T09:49:51.167873"
}
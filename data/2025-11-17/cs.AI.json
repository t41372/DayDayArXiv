{
  "date": "2025-11-17",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-11-17 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“ï¼š**\nä»Šå¤© arXiv çš„é‡å¤´æˆé›†ä¸­åœ¨ **LLM æ¨ç†èƒ½åŠ›çš„è¾¹ç•Œçªç ´**ä¸ **Agent çš„è‡ªæˆ‘è¿›åŒ–**ä¸Šã€‚æˆ‘ä»¬çœ‹åˆ°äº†çº¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒå‡ºçš„æ¨¡å‹åœ¨ç‰©ç†å¥¥èµ›ä¸­æ‘˜é‡‘ï¼Œè½¯ä»¶å·¥ç¨‹ Agent å¼€å§‹å…·å¤‡è¿è¡Œæ—¶è‡ªæˆ‘è¿›åŒ–çš„èƒ½åŠ›ï¼Œä»¥åŠé’ˆå¯¹æ¨ç†æ¨¡å‹ï¼ˆå¦‚ O1/R1ï¼‰çš„é«˜æ•ˆå‹ç¼©æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œå¤šæ¨¡æ€é¢†åŸŸæ¶Œç°äº†å¤§é‡é’ˆå¯¹ç‰¹å®šèƒ½åŠ›ï¼ˆå¦‚ä½“è‚²è§†é¢‘æ¨ç†ã€å¬è§‰è¿åŠ¨æ„ŸçŸ¥ã€åœ°ç†å®šä½ï¼‰çš„é«˜è´¨é‡åŸºå‡†æµ‹è¯•ã€‚\n\n---\n\n### ğŸš€ æ ¸å¿ƒå…³æ³¨ï¼šæ¨ç†ã€ç§‘å­¦ä¸ Agent è¿›åŒ–\n\n**1. P1: Mastering Physics Olympiads with Reinforcement Learning**\n**P1ï¼šåˆ©ç”¨å¼ºåŒ–å­¦ä¹ å¾æœç‰©ç†å¥¥æ—åŒ¹å…‹ç«èµ›**\n> è¿™æ˜¯ä¸€ä¸ªé‡Œç¨‹ç¢‘å¼çš„å·¥ä½œã€‚DeepSeek å›¢é˜Ÿï¼ˆæ¨æµ‹ï¼Œæ ¹æ®ä½œè€…åˆ—è¡¨ç‰¹å¾ï¼‰æˆ–å…¶ä»–é¡¶çº§å›¢é˜Ÿå±•ç¤ºäº†çº¯ RL è®­ç»ƒçš„å¨åŠ›ã€‚\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº† P1 ç³»åˆ—æ¨¡å‹ï¼Œå®Œå…¨é€šè¿‡**å¼ºåŒ–å­¦ä¹  (Reinforcement Learning)** è®­ç»ƒï¼Œæ—¨åœ¨è§£å†³å¥¥æ—åŒ¹å…‹çº§åˆ«çš„ç‰©ç†æ¨ç†é—®é¢˜ã€‚\n- **æƒŠäººå‘ç°**ï¼šP1-235B åœ¨ 2025 å¹´å›½é™…ç‰©ç†å¥¥æ—åŒ¹å…‹ (IPhO) ä¸­è¾¾åˆ°äº†**é‡‘ç‰Œæ°´å¹³**ï¼Œå¹¶åœ¨ 2024/2025 å¹´çš„ 13 é¡¹ç‰©ç†ç«èµ›ä¸­æ‹¿ä¸‹äº† 12 æšé‡‘ç‰Œã€‚ç»“åˆ Agent æ¡†æ¶ PhysicsMinions åï¼Œæ›´æ˜¯å–å¾—äº† IPhO 2025 çš„æ€»åˆ†ç¬¬ä¸€ã€‚\n- **æ„ä¹‰**ï¼šè¯æ˜äº† LLM å¯ä»¥ä»å•çº¯çš„è§£é¢˜è½¬å‘ç§‘å­¦çº§çš„ä¸¥è°¨æ¨ç†ï¼Œä¸” RL æ˜¯å…³é”®é©±åŠ¨åŠ›ã€‚\n\n**2. Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?**\n**Live-SWE-agentï¼šè½¯ä»¶å·¥ç¨‹æ™ºèƒ½ä½“èƒ½å¦åœ¨è¿è¡Œæ—¶è‡ªæˆ‘è¿›åŒ–ï¼Ÿ**\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº† Live-SWE-agentï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªèƒ½åœ¨è§£å†³ç°å®è½¯ä»¶é—®é¢˜æ—¶**åœ¨çº¿ (On-the-fly) è‡ªæˆ‘è¿›åŒ–**çš„ Agentã€‚å®ƒä¸ä¾èµ–æ˜‚è´µçš„ç¦»çº¿è®­ç»ƒï¼Œè€Œæ˜¯ä»åŸºç¡€çš„ bash å·¥å…·å¼€å§‹ï¼Œåœ¨è¿è¡Œä¸­è‡ªä¸»æ”¹è¿›å…¶ scaffold å®ç°ã€‚\n- **æ•ˆæœ**ï¼šåœ¨ SWE-bench Verified ä¸Šè¾¾åˆ°äº† **77.4%** çš„è§£å†³ç‡ï¼Œè¶…è¶Šäº†æ‰€æœ‰ç°æœ‰çš„è½¯ä»¶ Agentï¼ˆåŒ…æ‹¬ç§æœ‰æ¨¡å‹ï¼‰ï¼Œä¸”æ— éœ€æµ‹è¯•æ—¶æ‰©å±• (Test-time scaling)ã€‚\n\n**3. LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering**\n**LoCoBench-Agentï¼šé¢å‘é•¿ä¸Šä¸‹æ–‡è½¯ä»¶å·¥ç¨‹ LLM æ™ºèƒ½ä½“çš„äº¤äº’å¼åŸºå‡†æµ‹è¯•**\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹ç°æœ‰çš„ä»£ç åŸºå‡†æµ‹è¯•ç¼ºä¹å¤šè½®äº¤äº’å’Œå·¥å…·ä½¿ç”¨è¯„ä¼°çš„é—®é¢˜ï¼Œæå‡ºäº† LoCoBench-Agentã€‚è¿™æ˜¯ä¸€ä¸ªåŒ…å« 8000 ä¸ªåœºæ™¯çš„æ¡†æ¶ï¼Œæ”¯æŒ 10K åˆ° 1M token çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚\n- **å‘ç°**ï¼šç ”ç©¶æ­ç¤ºäº† Agent åœ¨**ç†è§£åŠ›ä¸æ•ˆç‡ä¹‹é—´å­˜åœ¨è´Ÿç›¸å…³**ï¼ˆå½»åº•çš„æ¢ç´¢æé«˜äº†ç†è§£åŠ›ä½†é™ä½äº†æ•ˆç‡ï¼‰ï¼Œä¸”ä¸åŒæ¨¡å‹åœ¨å·¥å…·ä½¿ç”¨ç­–ç•¥ä¸Šå·®å¼‚å·¨å¤§ã€‚\n\n**4. TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs**\n**TokenSqueezeï¼šé¢å‘æ¨ç†å‹ LLM çš„æ€§èƒ½ä¿æŒå‹ç¼©æ–¹æ³•**\n- **èƒŒæ™¯**ï¼šåƒ OpenAI-o1 å’Œ DeepSeek-R1 è¿™æ ·çš„æ¨ç†æ¨¡å‹ä¼šç”Ÿæˆæé•¿çš„æ€ç»´é“¾ (CoT)ï¼Œå¯¼è‡´æ¨ç†æˆæœ¬é«˜æ˜‚ã€‚\n- **æ–¹æ³•**ï¼šæå‡ºäº†ä¸€ç§ Long2Short çš„å‹ç¼©æ–¹æ³•ï¼Œé€šè¿‡è‡ªé€‚åº”åŒ¹é…æ¨ç†æ·±åº¦å’Œåˆ†å¸ƒå¯¹é½çš„è¯­è¨€ç»†åŒ–ï¼Œå‹ç¼© CoT é•¿åº¦ã€‚\n- **æ•ˆæœ**ï¼šåœ¨ MATH500 åŸºå‡†ä¸Šï¼Œå°† DeepSeek-R1-Distill-Qwen-7B çš„ token ä½¿ç”¨é‡å‡å°‘äº† **50%**ï¼ŒåŒæ—¶ä¿æŒäº†å‡†ç¡®ç‡ã€‚\n\n**5. When AI Does Science: Evaluating the Autonomous AI Scientist KOSMOS in Radiation Biology**\n**å½“ AI åšç§‘ç ”ï¼šè¯„ä¼°è‡ªä¸» AI ç§‘å­¦å®¶ KOSMOS åœ¨è¾å°„ç”Ÿç‰©å­¦ä¸­çš„è¡¨ç°**\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šå¯¹â€œAI ç§‘å­¦å®¶â€è¿›è¡Œäº†ä¸€æ¬¡æ®‹é…·çš„ç°å®æ£€éªŒ (Reality Check)ã€‚ä½œè€…è¯„ä¼°äº†è‡ªä¸» AI ç§‘å­¦å®¶ KOSMOS åœ¨ä¸‰ä¸ªè¾å°„ç”Ÿç‰©å­¦é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚\n- **å‘ç°**ï¼šç»“æœå–œå¿§å‚åŠã€‚KOSMOS æå‡ºäº†ä¸€ä¸ªæœ‰å……åˆ†æ”¯æŒçš„å‘ç°ï¼Œä¸€ä¸ªä¼¼æ˜¯è€Œéä½†ç»“æœä¸ç¡®å®šçš„å‡è®¾ï¼Œä»¥åŠä¸€ä¸ªé”™è¯¯çš„å‡è®¾ã€‚\n- **ç»“è®º**ï¼šAI ç§‘å­¦å®¶èƒ½äº§ç”Ÿæœ‰ç”¨çš„æƒ³æ³•ï¼Œä½†å¿…é¡»é…åˆä¸¥æ ¼çš„**é›¶æ¨¡å‹ (Null Models)** å®¡è®¡ï¼Œä¸èƒ½ç›²ç›®ä¿¡ä»»ã€‚\n\n---\n\n### ğŸ‘ï¸ å¤šæ¨¡æ€ä¸ä¸–ç•Œæ¨¡å‹\n\n**6. DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video Reasoning via Agentic Reinforcement Learning**\n**DeepSportï¼šåŸºäº Agent å¼ºåŒ–å­¦ä¹ çš„ç»¼åˆä½“è‚²è§†é¢‘æ¨ç†å¤šæ¨¡æ€å¤§æ¨¡å‹**\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹ä½“è‚²è§†é¢‘çš„é«˜åŠ¨æ€å’Œå¤æ‚è§„åˆ™ï¼Œæå‡ºäº† DeepSportã€‚å®ƒä¸åªæ˜¯è¢«åŠ¨å¤„ç†å¸§ï¼Œè€Œæ˜¯é€šè¿‡**Agentic RL** ä¸»åŠ¨æ¨ç†ï¼ŒåŠ¨æ€è°ƒç”¨å·¥å…·æå–å…³é”®å¸§ã€‚\n- **æ–¹æ³•**ï¼šæ„å»ºäº† 78k çš„é«˜è´¨é‡æ€ç»´é“¾ (CoT) æ•°æ®ï¼Œå¹¶ä½¿ç”¨ä¸€ç§æ–°é¢–çš„é—¨æ§å·¥å…·ä½¿ç”¨å¥–åŠ±æœºåˆ¶è¿›è¡Œ RL ä¼˜åŒ–ã€‚\n\n**7. GeoX-Bench: Benchmarking Cross-View Geo-Localization and Pose Estimation Capabilities of Large Multimodal Models**\n**GeoX-Benchï¼šè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹åœ¨è·¨è§†è§’åœ°ç†å®šä½å’Œå§¿æ€ä¼°è®¡èƒ½åŠ›ä¸Šçš„åŸºå‡†**\n- **å‘ç°**ï¼šè™½ç„¶ç°æœ‰çš„ LMM åœ¨åœ°ç†å®šä½ï¼ˆGeo-localizationï¼‰ä¸Šè¡¨ç°ä¸é”™ï¼Œä½†åœ¨æ›´å¤æ‚çš„**å§¿æ€ä¼°è®¡ (Pose Estimation)** ä¸Šèƒ½åŠ›æ˜¾è‘—ä¸‹é™ã€‚è¿™è¡¨æ˜æ¨¡å‹è™½ç„¶çŸ¥é“â€œåœ¨å“ªâ€ï¼Œä½†ä¸çŸ¥é“â€œæœå‘å“ªâ€ã€‚\n\n**8. AudioMotionBench: Evaluating Auditory Motion Perception in Audio LLMs**\n**AudioMotionBenchï¼šè¯„ä¼°éŸ³é¢‘ LLM çš„å¬è§‰è¿åŠ¨æ„ŸçŸ¥èƒ½åŠ›**\n- **å‘ç°**ï¼šç›®å‰çš„éŸ³é¢‘å¤§æ¨¡å‹ï¼ˆAudio LLMsï¼‰å­˜åœ¨ç³»ç»Ÿçš„**è¿åŠ¨æ„ŸçŸ¥ç¼ºé™·**ã€‚å®ƒä»¬å¾ˆéš¾ä»åŒè€³éŸ³é¢‘ä¸­æ¨æ–­å£°æºçš„æ–¹å‘å’Œè½¨è¿¹ï¼Œå¹³å‡å‡†ç¡®ç‡ä½äº 50%ã€‚\n\n**9. FoleyBench: A Benchmark For Video-to-Audio Models**\n**FoleyBenchï¼šè§†é¢‘ç”ŸæˆéŸ³é¢‘æ¨¡å‹çš„åŸºå‡†æµ‹è¯•**\n- **èƒŒæ™¯**ï¼šæ‹ŸéŸ³ (Foley) éœ€è¦ç”Ÿæˆçš„éŸ³é¢‘ä¸å±å¹•åŠ¨ä½œåœ¨è¯­ä¹‰å’Œæ—¶é—´ä¸Šç²¾ç¡®å¯¹é½ã€‚\n- **è´¡çŒ®**ï¼šå‘å¸ƒäº†åŒ…å« 5000 ä¸ªæ ·æœ¬çš„åŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨è¯„ä¼° V2A æ¨¡å‹åœ¨æ‹ŸéŸ³åœºæ™¯ä¸‹çš„è¡¨ç°ï¼Œå¡«è¡¥äº†è¿™ä¸€é¢†åŸŸçš„ç©ºç™½ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ã€å¯¹é½ä¸å¯è§£é‡Šæ€§\n\n**10. ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models**\n**ForgeDANï¼šç”¨äºè¶Šç‹±å·²å¯¹é½ LLM çš„è¿›åŒ–æ¡†æ¶**\n- **æ–¹æ³•**ï¼šåˆ©ç”¨è¿›åŒ–ç®—æ³•ç”Ÿæˆè¯­ä¹‰è¿è´¯çš„è¶Šç‹±æç¤ºè¯ã€‚å®ƒå¼•å…¥äº†å­—ç¬¦ã€å•è¯ã€å¥å­çº§åˆ«çš„å¤šç­–ç•¥æ‰°åŠ¨ï¼Œå¹¶ç»“åˆè¯­ä¹‰é€‚åº”åº¦è¯„ä¼°ã€‚\n- **æ•ˆæœ**ï¼šåœ¨ä¿æŒè‡ªç„¶åº¦å’Œéšè”½æ€§çš„åŒæ—¶ï¼Œå®ç°äº†æ¯”ç°æœ‰ SOTA æ–¹æ³•æ›´é«˜çš„è¶Šç‹±æˆåŠŸç‡ã€‚\n\n**11. Detecting and Steering LLMs' Empathy in Action**\n**æ£€æµ‹ä¸å¼•å¯¼ LLM çš„â€œè¡ŒåŠ¨ä¸­çš„åŒç†å¿ƒâ€**\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šç ”ç©¶äº† LLM æ˜¯å¦æ„¿æ„ä¸ºäº†æ»¡è¶³äººç±»æƒ…æ„Ÿéœ€æ±‚è€Œç‰ºç‰²ä»»åŠ¡æ•ˆç‡ï¼ˆå³â€œè¡ŒåŠ¨ä¸­çš„åŒç†å¿ƒâ€ï¼‰ã€‚\n- **å‘ç°**ï¼šå³ä½¿æ˜¯æœªç»è¿‡å®‰å…¨è®­ç»ƒçš„æ¨¡å‹ï¼ˆå¦‚ Uncensored Dolphinï¼‰ä¹Ÿæ¶Œç°å‡ºäº†åŒç†å¿ƒç¼–ç ã€‚ç ”ç©¶è€…æˆåŠŸåœ¨æ¿€æ´»ç©ºé—´ä¸­æ£€æµ‹å¹¶å¼•å¯¼äº†è¿™ç§åŒç†å¿ƒï¼Œä½†å‘ç°ä¸åŒæ¨¡å‹çš„å¼•å¯¼é²æ£’æ€§å·®å¼‚å¾ˆå¤§ã€‚\n\n**12. Data Whitening Improves Sparse Autoencoder Learning**\n**æ•°æ®ç™½åŒ–æ”¹è¿›ç¨€ç–è‡ªç¼–ç å™¨ (SAE) çš„å­¦ä¹ **\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹ç›®å‰å¤§çƒ­çš„å¯è§£é‡Šæ€§å·¥å…· SAEï¼Œå‘ç°å¯¹è¾“å…¥æ¿€æ´»è¿›è¡Œ **PCA ç™½åŒ– (Whitening)** å¤„ç†å¯ä»¥æ˜¾è‘—æ”¹å–„ SAE çš„æ€§èƒ½ã€‚\n- **æ•ˆæœ**ï¼šç™½åŒ–è®©ä¼˜åŒ–æ™¯è§‚æ›´å‡¸ï¼Œæå‡äº†ç‰¹å¾çš„è§£çº ç¼ åº¦ (Disentanglement) å’Œç¨€ç–æ¢æµ‹ç²¾åº¦ï¼Œå»ºè®®ä½œä¸º SAE è®­ç»ƒçš„é»˜è®¤é¢„å¤„ç†æ­¥éª¤ã€‚\n\n---\n\n### ğŸ’¡ å…¶ä»–æœ‰è¶£çš„ç ”ç©¶\n\n**13. Finding Kissing Numbers with Game-theoretic Reinforcement Learning**\n**åˆ©ç”¨åšå¼ˆè®ºå¼ºåŒ–å­¦ä¹ å¯»æ‰¾æ¥å»æ•° (Kissing Numbers)**\n- **è¶£ç‚¹**ï¼šæ¥å»æ•°é—®é¢˜ï¼ˆçƒä½“å †ç§¯ï¼‰æ˜¯æ•°å­¦éš¾é¢˜ã€‚è¿™ç¯‡è®ºæ–‡å°†å…¶å»ºæ¨¡ä¸ºçŸ©é˜µè¡¥å…¨æ¸¸æˆï¼Œåˆ©ç”¨ RL ç³»ç»Ÿ **PackingStar** åœ¨é«˜ç»´ç©ºé—´ä¸­æ¢ç´¢ã€‚\n- **æˆæœ**ï¼šåœ¨ 25 åˆ° 31 ç»´ç©ºé—´æ‰“ç ´äº†äººç±»å·²çŸ¥çš„è®°å½•ï¼Œå¹¶åœ¨ 13 ç»´ç©ºé—´å–å¾—äº†è‡ª 1971 å¹´ä»¥æ¥çš„é¦–æ¬¡çªç ´ã€‚\n\n**14. Whistledown: Combining User-Level Privacy with Conversational Coherence in LLMs**\n**Whistledownï¼šç»“åˆç”¨æˆ·çº§éšç§ä¸å¯¹è¯è¿è´¯æ€§çš„ LLM æ–¹æ¡ˆ**\n- **åº”ç”¨**ï¼šå½“ä½ å’Œ AI \"Spill the tea\" (èŠå…«å¦/åæ§½åŒäº‹) æ—¶ï¼Œå¦‚ä½•ä¸æ³„éœ²éšç§ï¼Ÿ\n- **æ–¹æ³•**ï¼šåœ¨å®¢æˆ·ç«¯éƒ¨ç½²ä¸€ä¸ªéšç§å±‚ï¼Œç»“åˆå‡ååŒ–å’Œå·®åˆ†éšç§ï¼Œåœ¨ä¸ç‰ºç‰²å¯¹è¯è¿è´¯æ€§çš„å‰æä¸‹ä¿æŠ¤ PIIã€‚\n\n**15. Artificial Intelligence Agents in Music Analysis: An Integrative Perspective Based on Two Use Cases**\n**éŸ³ä¹åˆ†æä¸­çš„ AI æ™ºèƒ½ä½“ï¼šåŸºäºä¸¤ä¸ªç”¨ä¾‹çš„ç»¼åˆè§†è§’**\n- **å†…å®¹**ï¼šæ¢è®¨äº† AI Agent åœ¨éŸ³ä¹åˆ†æå’Œæ•™è‚²ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åˆ©ç”¨ç”Ÿæˆå¼ AI åŸ¹å…»åˆ†æåˆ›é€ åŠ›ï¼Œä»¥åŠç”¨äºç¬¦å·éŸ³ä¹åˆ†æçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚",
  "papers": [
    {
      "arxiv_id": "2511.13998v1",
      "title": "LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering",
      "title_zh": "LoCoBench-Agentï¼šé¢å‘é•¿ä¸Šä¸‹æ–‡è½¯ä»¶å·¥ç¨‹çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“äº¤äº’å¼åŸºå‡†æµ‹è¯•",
      "authors": [
        "Jielin Qiu",
        "Zuxin Liu",
        "Zhiwei Liu",
        "Rithesh Murthy",
        "Jianguo Zhang",
        "Haolin Chen",
        "Shiyu Wang",
        "Ming Zhu",
        "Liangwei Yang",
        "Juntao Tan",
        "Roshan Ram",
        "Akshara Prabhakar",
        "Tulika Awalgaonkar",
        "Zixiang Chen",
        "Zhepeng Cen",
        "Cheng Qian",
        "Shelby Heinecke",
        "Weiran Yao",
        "Silvio Savarese",
        "Caiming Xiong",
        "Huan Wang"
      ],
      "abstract": "As large language models (LLMs) evolve into sophisticated autonomous agents capable of complex software development tasks, evaluating their real-world capabilities becomes critical. While existing benchmarks like LoCoBench~\\cite{qiu2025locobench} assess long-context code understanding, they focus on single-turn evaluation and cannot capture the multi-turn interactive nature, tool usage patterns, and adaptive reasoning required by real-world coding agents. We introduce \\textbf{LoCoBench-Agent}, a comprehensive evaluation framework specifically designed to assess LLM agents in realistic, long-context software engineering workflows. Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions. We also introduce an evaluation methodology with 9 metrics across comprehension and efficiency dimensions. Our framework provides agents with 8 specialized tools (file operations, search, code analysis) and evaluates them across context lengths ranging from 10K to 1M tokens, enabling precise assessment of long-context performance. Through systematic evaluation of state-of-the-art models, we reveal several key findings: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation, where thorough exploration increases comprehension but reduces efficiency; and (3) conversation efficiency varies dramatically across models, with strategic tool usage patterns differentiating high-performing agents. As the first long-context LLM agent benchmark for software engineering, LoCoBench-Agent establishes a rigorous foundation for measuring agent capabilities, identifying performance gaps, and advancing autonomous software development at scale.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LoCoBench-Agentï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨ä¸ºé•¿ä¸Šä¸‹æ–‡(long-context)è½¯ä»¶å·¥ç¨‹å·¥ä½œæµè®¾è®¡çš„äº¤äº’å¼ LLM æ™ºèƒ½ä½“è¯„ä¼°æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°† LoCoBench çš„ 8,000 ä¸ªåœºæ™¯æ‰©å±•ä¸ºäº¤äº’å¼æ™ºèƒ½ä½“ç¯å¢ƒï¼Œèƒ½å¤Ÿç³»ç»Ÿåœ°è¯„ä¼°å¤šè½®å¯¹è¯(multi-turn conversations)ã€å·¥å…·ä½¿ç”¨æ•ˆç‡ã€é”™è¯¯æ¢å¤ä»¥åŠè·¨é•¿å¼€å‘ä¼šè¯çš„æ¶æ„ä¸€è‡´æ€§ã€‚LoCoBench-Agent ä¸ºæ™ºèƒ½ä½“æä¾›äº†æ¶µç›–æ–‡ä»¶æ“ä½œã€æœç´¢å’Œä»£ç åˆ†æçš„ 8 ç§ä¸“é—¨å·¥å…·ï¼Œå¹¶æ”¯æŒä» 10K åˆ° 1M token çš„ä¸Šä¸‹æ–‡é•¿åº¦è¯„ä¼°ã€‚é€šè¿‡åŒ…å«ç†è§£ä¸æ•ˆç‡ç»´åº¦çš„ 9 é¡¹æŒ‡æ ‡ï¼Œå®éªŒæ­ç¤ºæ™ºèƒ½ä½“åœ¨é•¿ä¸Šä¸‹æ–‡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„é²æ£’æ€§(robustness)ï¼Œä½†ä¹Ÿå­˜åœ¨ç†è§£æ·±åº¦ä¸æ•ˆç‡ä¹‹é—´çš„è´Ÿç›¸å…³æƒè¡¡ã€‚ç ”ç©¶è¿˜å‘ç°ä¸åŒæ¨¡å‹çš„å¯¹è¯æ•ˆç‡å·®å¼‚å·¨å¤§ï¼Œæˆ˜ç•¥æ€§çš„å·¥å…·ä½¿ç”¨æ¨¡å¼æ˜¯åŒºåˆ†é«˜è¡¨ç°æ™ºèƒ½ä½“çš„å…³é”®ã€‚ä½œä¸ºè¯¥é¢†åŸŸé¦–ä¸ªé•¿ä¸Šä¸‹æ–‡æ™ºèƒ½ä½“åŸºå‡†ï¼ŒLoCoBench-Agent ä¸ºè¡¡é‡æ™ºèƒ½ä½“èƒ½åŠ›å’Œæ¨åŠ¨å¤§è§„æ¨¡è‡ªä¸»è½¯ä»¶å¼€å‘å¥ å®šäº†ä¸¥è°¨åŸºç¡€ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "54-pages",
      "pdf_url": "https://arxiv.org/pdf/2511.13998v1",
      "published_date": "2025-11-17 23:57:24 UTC",
      "updated_date": "2025-11-17 23:57:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:47:35.338138+00:00"
    },
    {
      "arxiv_id": "2511.13987v1",
      "title": "Artificial Intelligence Agents in Music Analysis: An Integrative Perspective Based on Two Use Cases",
      "title_zh": "éŸ³ä¹åˆ†æä¸­çš„äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“ï¼šåŸºäºåŒç”¨ä¾‹çš„æ•´åˆè§†è§’",
      "authors": [
        "Antonio Manuel MartÃ­nez-Heredia",
        "Dolores Godrid RodrÃ­guez",
        "AndrÃ©s Ortiz GarcÃ­a"
      ],
      "abstract": "This paper presents an integrative review and experimental validation of artificial intelligence (AI) agents applied to music analysis and education. We synthesize the historical evolution from rule-based models to contemporary approaches involving deep learning, multi-agent architectures, and retrieval-augmented generation (RAG) frameworks. The pedagogical implications are evaluated through a dual-case methodology: (1) the use of generative AI platforms in secondary education to foster analytical and creative skills; (2) the design of a multiagent system for symbolic music analysis, enabling modular, scalable, and explainable workflows.\n  Experimental results demonstrate that AI agents effectively enhance musical pattern recognition, compositional parameterization, and educational feedback, outperforming traditional automated methods in terms of interpretability and adaptability. The findings highlight key challenges concerning transparency, cultural bias, and the definition of hybrid evaluation metrics, emphasizing the need for responsible deployment of AI in educational environments.\n  This research contributes to a unified framework that bridges technical, pedagogical, and ethical considerations, offering evidence-based guidance for the design and application of intelligent agents in computational musicology and music education.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹åº”ç”¨äºéŸ³ä¹åˆ†æå’Œæ•™è‚²çš„äººå·¥æ™ºèƒ½(AI)æ™ºèƒ½ä½“è¿›è¡Œäº†ç»¼åˆæ€§è¯„è¿°å’Œå®éªŒéªŒè¯ï¼Œç³»ç»Ÿæ¢³ç†äº†ä»åŸºäºè§„åˆ™çš„æ¨¡å‹åˆ°ç°ä»£æ·±åº¦å­¦ä¹ ã€å¤šæ™ºèƒ½ä½“(multi-agent)æ¶æ„åŠæ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æ¡†æ¶çš„æ¼”è¿›è¿‡ç¨‹ã€‚ç ”ç©¶é€šè¿‡åŒæ¡ˆä¾‹æ–¹æ³•è®ºè¯„ä¼°äº†å…¶æ•™å­¦åº”ç”¨ï¼šä¸€æ˜¯åˆ©ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å¹³å°åœ¨ä¸­å­¦æ•™è‚²ä¸­åŸ¹å…»åˆ†æä¸åˆ›ä½œèƒ½åŠ›ï¼ŒäºŒæ˜¯è®¾è®¡äº†ä¸€å¥—ç”¨äºç¬¦å·éŸ³ä¹åˆ†æ(symbolic music analysis)çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä»¥å®ç°æ¨¡å—åŒ–ã€å¯æ‰©å±•ä¸”å¯è§£é‡Šçš„å·¥ä½œæµã€‚å®éªŒç»“æœè¯æ˜ï¼ŒAIæ™ºèƒ½ä½“åœ¨éŸ³ä¹æ¨¡å¼è¯†åˆ«ã€åˆ›ä½œå‚æ•°åŒ–å’Œæ•™è‚²åé¦ˆæ–¹é¢æ•ˆæœæ˜¾è‘—ï¼Œåœ¨å¯è§£é‡Šæ€§å’Œé€‚åº”æ€§ä¸Šå‡ä¼˜äºä¼ ç»Ÿè‡ªåŠ¨åŒ–æ–¹æ³•ã€‚åŒæ—¶ï¼Œè¯¥é¡¹å·¥ä½œæŒ‡å‡ºé€æ˜åº¦ã€æ–‡åŒ–åè§åŠæ··åˆè¯„ä»·æŒ‡æ ‡æ˜¯å½“å‰é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ï¼Œå¹¶å¼ºè°ƒäº†åœ¨æ•™è‚²ç¯å¢ƒä¸­è´Ÿè´£ä»»åœ°éƒ¨ç½²AIçš„å¿…è¦æ€§ã€‚è¯¥ç ”ç©¶æœ€ç»ˆè´¡çŒ®äº†ä¸€ä¸ªèåˆæŠ€æœ¯ã€æ•™å­¦ä¸ä¼¦ç†è€ƒé‡çš„ç»Ÿä¸€æ¡†æ¶ï¼Œä¸ºè®¡ç®—éŸ³ä¹å­¦(computational musicology)å’ŒéŸ³ä¹æ•™è‚²é¢†åŸŸæ™ºèƒ½æ™ºèƒ½ä½“çš„è®¾è®¡ä¸åº”ç”¨æä¾›äº†å®è¯æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Extended version of the conference paper presented at SATMUS 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.13987v1",
      "published_date": "2025-11-17 23:46:47 UTC",
      "updated_date": "2025-11-17 23:46:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:47:51.345003+00:00"
    },
    {
      "arxiv_id": "2511.16699v1",
      "title": "Detecting and Steering LLMs' Empathy in Action",
      "title_zh": "æ¢æµ‹ä¸å¼•å¯¼å¤§è¯­è¨€æ¨¡å‹è¡ŒåŠ¨ä¸­çš„å…±æƒ…",
      "authors": [
        "Juan P. Cadile"
      ],
      "abstract": "We investigate empathy-in-action -- the willingness to sacrifice task efficiency to address human needs -- as a linear direction in LLM activation space. Using contrastive prompts grounded in the Empathy-in-Action (EIA) benchmark, we test detection and steering across Phi-3-mini-4k (3.8B), Qwen2.5-7B (safety-trained), and Dolphin-Llama-3.1-8B (uncensored).\n  Detection: All models show AUROC 0.996-1.00 at optimal layers. Uncensored Dolphin matches safety-trained models, demonstrating empathy encoding emerges independent of safety training. Phi-3 probes correlate strongly with EIA behavioral scores (r=0.71, p<0.01). Cross-model probe agreement is limited (Qwen: r=-0.06, Dolphin: r=0.18), revealing architecture-specific implementations despite convergent detection.\n  Steering: Qwen achieves 65.3% success with bidirectional control and coherence at extreme interventions. Phi-3 shows 61.7% success with similar coherence. Dolphin exhibits asymmetric steerability: 94.4% success for pro-empathy steering but catastrophic breakdown for anti-empathy (empty outputs, code artifacts).\n  Implications: The detection-steering gap varies by model. Qwen and Phi-3 maintain bidirectional coherence; Dolphin shows robustness only for empathy enhancement. Safety training may affect steering robustness rather than preventing manipulation, though validation across more models is needed.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„è¡ŒåŠ¨ä¸­å…±æƒ…ï¼ˆempathy-in-actionï¼‰ï¼Œå³æ¨¡å‹ä¸ºæ»¡è¶³äººç±»éœ€æ±‚è€Œç‰ºç‰²ä»»åŠ¡æ•ˆç‡çš„æ„æ„¿ï¼Œå¹¶å°†å…¶è¯†åˆ«ä¸ºæ¨¡å‹æ¿€æ´»ç©ºé—´ä¸­çš„ä¸€ä¸ªçº¿æ€§æ–¹å‘ã€‚ä½œè€…åˆ©ç”¨åŸºäº Empathy-in-Action (EIA) åŸºå‡†çš„å¯¹æ¯”æç¤ºè¯ï¼Œåœ¨ Phi-3-mini-4kã€Qwen2.5-7B å’Œ Dolphin-Llama-3.1-8B ç­‰æ¨¡å‹ä¸Šè¿›è¡Œäº†æ£€æµ‹ä¸å¹²é¢„ï¼ˆsteeringï¼‰å®éªŒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æœ‰æµ‹è¯•æ¨¡å‹åœ¨æœ€ä¼˜å±‚å‡è¡¨ç°å‡ºæé«˜çš„æ£€æµ‹å‡†ç¡®ç‡ï¼ˆAUROC 0.996-1.00ï¼‰ï¼Œä¸”æœªç»å®‰å…¨è®­ç»ƒçš„æ¨¡å‹ä¹Ÿå±•ç°å‡ºå…±æƒ…ç¼–ç ï¼Œè¡¨æ˜è¿™ç§ç‰¹æ€§ç‹¬ç«‹äºå®‰å…¨è®­ç»ƒè‡ªå‘æ¶Œç°ã€‚åœ¨å¹²é¢„å®éªŒä¸­ï¼ŒQwen å’Œ Phi-3 å®ç°äº†ç¨³å¥çš„åŒå‘æ§åˆ¶ï¼Œè€Œ Dolphin æ¨¡å‹åˆ™è¡¨ç°å‡ºä¸å¯¹ç§°æ€§ï¼Œä»…åœ¨å¢å¼ºå…±æƒ…æ–¹å‘æ—¶ä¿æŒè¿è´¯ã€‚ç ”ç©¶å‘ç°ä¸åŒæ¶æ„åœ¨å…±æƒ…çš„å…·ä½“å®ç°ä¸Šå­˜åœ¨å·®å¼‚ï¼Œå¹¶æŒ‡å‡ºå®‰å…¨è®­ç»ƒå¯èƒ½æ›´å¤šåœ°å½±å“æ¨¡å‹å¹²é¢„çš„é²æ£’æ€§ã€‚è¯¥å·¥ä½œä¸ºç†è§£å’Œæ“çºµ LLMs çš„ç¤¾ä¼šåå¥½æä¾›äº†é‡è¦çš„å®è¯ä¾æ®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.16699v1",
      "published_date": "2025-11-17 23:45:26 UTC",
      "updated_date": "2025-11-17 23:45:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:47:34.735455+00:00"
    },
    {
      "arxiv_id": "2511.13984v2",
      "title": "Node-Level Uncertainty Estimation in LLM-Generated SQL",
      "title_zh": "LLM ç”Ÿæˆ SQL çš„èŠ‚ç‚¹çº§ä¸ç¡®å®šæ€§ä¼°è®¡",
      "authors": [
        "Hilaf Hasson",
        "Ruocheng Guo"
      ],
      "abstract": "We present a practical framework for detecting errors in LLM-generated SQL by estimating uncertainty at the level of individual nodes in the query's abstract syntax tree (AST). Our approach proceeds in two stages. First, we introduce a semantically aware labeling algorithm that, given a generated SQL and a gold reference, assigns node-level correctness without over-penalizing structural containers or alias variation. Second, we represent each node with a rich set of schema-aware and lexical features - capturing identifier validity, alias resolution, type compatibility, ambiguity in scope, and typo signals - and train a supervised classifier to predict per-node error probabilities. We interpret these probabilities as calibrated uncertainty, enabling fine-grained diagnostics that pinpoint exactly where a query is likely to be wrong. Across multiple databases and datasets, our method substantially outperforms token log-probabilities: average AUC improves by +27.44% while maintaining robustness under cross-database evaluation. Beyond serving as an accuracy signal, node-level uncertainty supports targeted repair, human-in-the-loop review, and downstream selective execution. Together, these results establish node-centric, semantically grounded uncertainty estimation as a strong and interpretable alternative to aggregate sequence level confidence measures.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºæ£€æµ‹å¤§è¯­è¨€æ¨¡å‹(LLM)ç”ŸæˆSQLé”™è¯¯çš„å®ç”¨æ¡†æ¶ï¼Œé€šè¿‡åœ¨æŸ¥è¯¢çš„æŠ½è±¡è¯­æ³•æ ‘(AST)èŠ‚ç‚¹å±‚é¢ä¼°ç®—ä¸ç¡®å®šæ€§æ¥å®ç°ç²¾å‡†è¯Šæ–­ã€‚è¯¥æ–¹æ³•é¦–å…ˆå¼•å…¥è¯­ä¹‰æ„ŸçŸ¥æ ‡æ³¨ç®—æ³•ï¼Œåœ¨å¯¹æ¯”ç”ŸæˆSQLä¸å‚è€ƒæ ‡å‡†æ—¶ï¼Œèƒ½æœ‰æ•ˆé¿å…å› ç»“æ„å®¹å™¨æˆ–åˆ«åå·®å¼‚å¯¼è‡´çš„è¿‡åº¦æƒ©ç½šã€‚éšåï¼Œè¯¥æ¡†æ¶æå–èŠ‚ç‚¹çš„æ¨¡å¼æ„ŸçŸ¥(schema-aware)ä¸è¯æ³•ç‰¹å¾ï¼Œé€šè¿‡è®­ç»ƒç›‘ç£åˆ†ç±»å™¨é¢„æµ‹èŠ‚ç‚¹é”™è¯¯æ¦‚ç‡ï¼Œä»è€Œå®ç°æ ¡å‡†åçš„ç»†ç²’åº¦ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹³å‡AUCæ¯”Tokenå¯¹æ•°æ¦‚ç‡(token log-probabilities)æå‡äº†27.44%ï¼Œå¹¶åœ¨è·¨æ•°æ®åº“è¯„ä¼°ä¸­è¡¨ç°å‡ºå“è¶Šçš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼ŒèŠ‚ç‚¹çº§ä¸ç¡®å®šæ€§è¿˜æ”¯æŒå®šå‘ä¿®å¤ã€äººå·¥å®¡æŸ¥(human-in-the-loop review)åŠä¸‹æ¸¸é€‰æ‹©æ€§æ‰§è¡Œã€‚æ€»ä¹‹ï¼Œè¿™é¡¹å·¥ä½œä¸ºLLMç”Ÿæˆçš„ç»“æ„åŒ–æŸ¥è¯¢æä¾›äº†ä¸€ç§æ¯”ä¼ ç»Ÿåºåˆ—çº§ç½®ä¿¡åº¦æ›´å¼ºã€æ›´å…·å¯è§£é‡Šæ€§çš„è¯„ä¼°æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13984v2",
      "published_date": "2025-11-17 23:31:45 UTC",
      "updated_date": "2025-11-19 20:18:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:48:05.134846+00:00"
    },
    {
      "arxiv_id": "2511.13981v1",
      "title": "Data Whitening Improves Sparse Autoencoder Learning",
      "title_zh": "æ•°æ®ç™½åŒ–æå‡ç¨€ç–è‡ªç¼–ç å™¨å­¦ä¹ ",
      "authors": [
        "Ashwin Saraswatula",
        "David Klindt"
      ],
      "abstract": "Sparse autoencoders (SAEs) have emerged as a promising approach for learning interpretable features from neural network activations. However, the optimization landscape for SAE training can be challenging due to correlations in the input data. We demonstrate that applying PCA Whitening to input activations -- a standard preprocessing technique in classical sparse coding -- improves SAE performance across multiple metrics. Through theoretical analysis and simulation, we show that whitening transforms the optimization landscape, making it more convex and easier to navigate. We evaluate both ReLU and Top-K SAEs across diverse model architectures, widths, and sparsity regimes. Empirical evaluation on SAEBench, a comprehensive benchmark for sparse autoencoders, reveals that whitening consistently improves interpretability metrics, including sparse probing accuracy and feature disentanglement, despite minor drops in reconstruction quality. Our results challenge the assumption that interpretability aligns with an optimal sparsity--fidelity trade-off and suggest that whitening should be considered as a default preprocessing step for SAE training, particularly when interpretability is prioritized over perfect reconstruction.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é€šè¿‡åº”ç”¨PCA Whiteningï¼ˆä¸»æˆåˆ†åˆ†æç™½åŒ–ï¼‰æ¥æ”¹å–„ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSparse Autoencoders, SAEsï¼‰çš„å­¦ä¹ æ•ˆæœã€‚é’ˆå¯¹è¾“å…¥æ•°æ®ç›¸å…³æ€§å¯¼è‡´çš„ä¼˜åŒ–éš¾é¢˜ï¼Œä½œè€…é€šè¿‡ç†è®ºåˆ†æå’Œæ¨¡æ‹Ÿè¯æ˜ï¼Œç™½åŒ–å¤„ç†èƒ½ä½¿ä¼˜åŒ–æ™¯è§‚ï¼ˆoptimization landscapeï¼‰æ›´è¶‹äºå‡¸æ€§ï¼Œä»è€Œæ˜¾è‘—é™ä½è®­ç»ƒéš¾åº¦ã€‚å®éªŒåœ¨SAEBenchåŸºå‡†ä¸Šå¯¹ReLUå’ŒTop-K SAEsç­‰å¤šç§æ¶æ„è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºç™½åŒ–ä¸€è‡´æå‡äº†ç¨€ç–æ¢æµ‹å‡†ç¡®ç‡ï¼ˆsparse probing accuracyï¼‰å’Œç‰¹å¾è§£ç¦»åº¦ï¼ˆfeature disentanglementï¼‰ç­‰å¯è§£é‡Šæ€§æŒ‡æ ‡ã€‚å°½ç®¡é‡æ„è´¨é‡ï¼ˆreconstruction qualityï¼‰æœ‰è½»å¾®ä¸‹é™ï¼Œä½†è¯¥ç ”ç©¶è¡¨æ˜å¯è§£é‡Šæ€§å¹¶ä¸æ€»æ˜¯ä¸æœ€ä¼˜çš„ç¨€ç–-å¿ å®åº¦æƒè¡¡å®Œå…¨ä¸€è‡´ã€‚æœ€ç»ˆï¼Œç ”ç©¶å»ºè®®åœ¨ä»¥å¯è§£é‡Šæ€§ä¸ºé¦–è¦ç›®æ ‡çš„SAEè®­ç»ƒä¸­ï¼Œåº”å°†ç™½åŒ–ä½œä¸ºä¸€ç§é»˜è®¤çš„é¢„å¤„ç†æ­¥éª¤ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to the AAAI 2026 XAI4Science Workshop",
      "pdf_url": "https://arxiv.org/pdf/2511.13981v1",
      "published_date": "2025-11-17 23:20:58 UTC",
      "updated_date": "2025-11-17 23:20:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:48:01.437081+00:00"
    },
    {
      "arxiv_id": "2511.13970v1",
      "title": "Scene Graph-Guided Generative AI Framework for Synthesizing and Evaluating Industrial Hazard Scenarios",
      "title_zh": "é¢å‘å·¥ä¸šå±é™©åœºæ™¯åˆæˆä¸è¯„ä¼°çš„åœºæ™¯å›¾å¼•å¯¼ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¡†æ¶",
      "authors": [
        "Sanjay Acharjee",
        "Abir Khan Ratul",
        "Diego Patino",
        "Md Nazmus Sakib"
      ],
      "abstract": "Training vision models to detect workplace hazards accurately requires realistic images of unsafe conditions that could lead to accidents. However, acquiring such datasets is difficult because capturing accident-triggering scenarios as they occur is nearly impossible. To overcome this limitation, this study presents a novel scene graph-guided generative AI framework that synthesizes photorealistic images of hazardous scenarios grounded in historical Occupational Safety and Health Administration (OSHA) accident reports. OSHA narratives are analyzed using GPT-4o to extract structured hazard reasoning, which is converted into object-level scene graphs capturing spatial and contextual relationships essential for understanding risk. These graphs guide a text-to-image diffusion model to generate compositionally accurate hazard scenes. To evaluate the realism and semantic fidelity of the generated data, a visual question answering (VQA) framework is introduced. Across four state-of-the-art generative models, the proposed VQA Graph Score outperforms CLIP and BLIP metrics based on entropy-based validation, confirming its higher discriminative sensitivity.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·¥ä¸šå®‰å…¨è§†è§‰æ¨¡å‹è®­ç»ƒä¸­éš¾ä»¥è·å–çœŸå®å±é™©åœºæ™¯å›¾åƒçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºåœºæ™¯å›¾å¼•å¯¼(Scene Graph-Guided)çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ GPT-4o åˆ†æç¾å›½èŒä¸šå®‰å…¨ä¸å¥åº·ç®¡ç†å±€(OSHA)çš„å†å²äº‹æ•…æŠ¥å‘Šï¼Œæå–ç»“æ„åŒ–çš„å±é™©æ¨ç†ä¿¡æ¯ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºæ•æ‰ç©ºé—´ä¸ä¸Šä¸‹æ–‡å…³ç³»çš„ç‰©ä½“çº§åœºæ™¯å›¾(Scene Graphs)ã€‚è¿™äº›åœºæ™¯å›¾è¿›ä¸€æ­¥å¼•å¯¼æ–‡æœ¬ç”Ÿæˆå›¾åƒ(Text-to-Image)æ‰©æ•£æ¨¡å‹ï¼Œä»¥åˆæˆæ„å›¾å‡†ç¡®ä¸”å†™å®çš„å±é™©åœºæ™¯ã€‚ä¸ºéªŒè¯ç”Ÿæˆæ•°æ®çš„çœŸå®æ€§ä¸è¯­ä¹‰å¿ å®åº¦ï¼Œç ”ç©¶å¼•å…¥äº†è§†è§‰é—®ç­”(VQA)è¯„ä¼°æ¡†æ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„ VQA Graph Score åœ¨è¾¨åˆ«çµæ•åº¦æ–¹é¢ä¼˜äº CLIP å’Œ BLIP æŒ‡æ ‡ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨åˆæˆä¸è¯„ä¼°å·¥ä¸šç¾å®³åœºæ™¯æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13970v1",
      "published_date": "2025-11-17 22:58:27 UTC",
      "updated_date": "2025-11-17 22:58:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:48:04.239599+00:00"
    },
    {
      "arxiv_id": "2512.00048v1",
      "title": "Causal Reinforcement Learning based Agent-Patient Interaction with Clinical Domain Knowledge",
      "title_zh": "åŸºäºå› æœå¼ºåŒ–å­¦ä¹ ä¸”èåˆä¸´åºŠé¢†åŸŸçŸ¥è¯†çš„æ™ºèƒ½ä½“-æ‚£è€…äº¤äº’",
      "authors": [
        "Wenzheng Zhao",
        "Ran Zhang",
        "Ruth Palan Lopez",
        "Shu-Fen Wung",
        "Fengpei Yuan"
      ],
      "abstract": "Reinforcement Learning (RL) faces significant challenges in adaptive healthcare interventions, such as dementia care, where data is scarce, decisions require interpretability, and underlying patient-state dynamic are complex and causal in nature. In this work, we present a novel framework called Causal structure-aware Reinforcement Learning (CRL) that explicitly integrates causal discovery and reasoning into policy optimization. This method enables an agent to learn and exploit a directed acyclic graph (DAG) that describes the causal dependencies between human behavioral states and robot actions, facilitating more efficient, interpretable, and robust decision-making. We validate our approach in a simulated robot-assisted cognitive care scenario, where the agent interacts with a virtual patient exhibiting dynamic emotional, cognitive, and engagement states. The experimental results show that CRL agents outperform conventional model-free RL baselines by achieving higher cumulative rewards, maintaining desirable patient states more consistently, and exhibiting interpretable, clinically-aligned behavior. We further demonstrate that CRL's performance advantage remains robust across different weighting strategies and hyperparameter settings. In addition, we demonstrate a lightweight LLM-based deployment: a fixed policy is embedded into a system prompt that maps inferred states to actions, producing consistent, supportive dialogue without LLM finetuning. Our work illustrates the promise of causal reinforcement learning for human-robot interaction applications, where interpretability, adaptiveness, and data efficiency are paramount.",
      "tldr_zh": "å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)åœ¨ç—´å‘†ç—‡æŠ¤ç†ç­‰åŒ»ç–—å¹²é¢„ä¸­é¢ä¸´æ•°æ®ç¨€ç¼ºã€ç¼ºä¹è§£é‡Šæ€§ä»¥åŠæ‚£è€…çŠ¶æ€åŠ¨æ€å¤æ‚ä¸”å…·æœ‰å› æœæ€§ç­‰æŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºå› æœç»“æ„æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ (Causal structure-aware Reinforcement Learning, CRL)çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨å°†å› æœå‘ç°(Causal Discovery)å’Œæ¨ç†æ˜¾å¼åœ°æ•´åˆåˆ°ç­–ç•¥ä¼˜åŒ–ä¸­ã€‚é€šè¿‡å­¦ä¹ å¹¶åˆ©ç”¨æè¿°äººç±»è¡Œä¸ºçŠ¶æ€ä¸æœºå™¨äººåŠ¨ä½œä¹‹é—´å› æœä¾èµ–çš„æœ‰å‘æ— ç¯å›¾(Directed Acyclic Graph, DAG)ï¼Œè¯¥æ–¹æ³•å®ç°äº†æ›´é«˜æ•ˆã€å¯è§£é‡Šä¸”é²æ£’çš„å†³ç­–è¿‡ç¨‹ã€‚åœ¨æ¨¡æ‹Ÿæœºå™¨äººè¾…åŠ©è®¤çŸ¥æŠ¤ç†åœºæ™¯çš„å®éªŒä¸­ï¼ŒCRLæ™ºèƒ½ä½“åœ¨ç´¯è®¡å¥–åŠ±å’Œç»´æŒç†æƒ³æ‚£è€…çŠ¶æ€æ–¹é¢å‡ä¼˜äºä¼ ç»Ÿçš„æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ (Model-free RL)åŸºå‡†ï¼Œå¹¶è¡¨ç°å‡ºç¬¦åˆä¸´åºŠé€»è¾‘çš„è¡Œä¸ºã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å±•ç¤ºäº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„è½»é‡åŒ–éƒ¨ç½²æ–¹æ¡ˆï¼Œé€šè¿‡å°†å›ºå®šç­–ç•¥åµŒå…¥ç³»ç»Ÿæç¤ºè¯ï¼Œæ— éœ€å¾®è°ƒå³å¯ç”Ÿæˆæ”¯æŒæ€§çš„å¯¹è¯äº¤äº’ã€‚è¿™é¡¹å·¥ä½œçªæ˜¾äº†å› æœå¼ºåŒ–å­¦ä¹ åœ¨å¯¹è§£é‡Šæ€§ã€é€‚åº”æ€§å’Œæ•°æ®æ•ˆç‡è¦æ±‚æé«˜çš„äººæœºäº¤äº’(Human-Robot Interaction)é¢†åŸŸä¸­çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted by AAAI workshop",
      "pdf_url": "https://arxiv.org/pdf/2512.00048v1",
      "published_date": "2025-11-17 22:38:03 UTC",
      "updated_date": "2025-11-17 22:38:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:48:09.839516+00:00"
    },
    {
      "arxiv_id": "2511.13942v1",
      "title": "CORGI: Efficient Pattern Matching With Quadratic Guarantees",
      "title_zh": "CORGIï¼šå…·æœ‰äºŒæ¬¡å¤æ‚åº¦ä¿è¯çš„é«˜æ•ˆæ¨¡å¼åŒ¹é…",
      "authors": [
        "Daniel Weitekamp"
      ],
      "abstract": "Rule-based systems must solve complex matching problems within tight time constraints to be effective in real-time applications, such as planning and reactive control for AI agents, as well as low-latency relational database querying. Pattern-matching systems can encounter issues where exponential time and space are required to find matches for rules with many underconstrained variables, or which produce combinatorial intermediate partial matches (but are otherwise well-constrained). When online AI systems automatically generate rules from example-driven induction or code synthesis, they can easily produce worst-case matching patterns that slow or halt program execution by exceeding available memory. In our own work with cognitive systems that learn from example, we've found that aggressive forms of anti-unification-based generalization can easily produce these circumstances. To make these systems practical without hand-engineering constraints or succumbing to unpredictable failure modes, we introduce a new matching algorithm called CORGI (Collection-Oriented Relational Graph Iteration). Unlike RETE-based approaches, CORGI offers quadratic time and space guarantees for finding single satisficing matches, and the ability to iteratively stream subsequent matches without committing entire conflict sets to memory. CORGI differs from RETE in that it does not have a traditional $Î²$-memory for collecting partial matches. Instead, CORGI takes a two-step approach: a graph of grounded relations is built/maintained in a forward pass, and an iterator generates matches as needed by working backward through the graph. This approach eliminates the high-latency delays and memory overflows that can result from populating full conflict sets. In a performance evaluation, we demonstrate that CORGI significantly outperforms RETE implementations from SOAR and OPS5 on a simple combinatorial matching task.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºè§„åˆ™çš„ç³»ç»Ÿåœ¨å¤„ç†å®æ—¶ AI è§„åˆ’å’Œä½å»¶è¿Ÿæ•°æ®åº“æŸ¥è¯¢æ—¶ï¼Œå› å¤æ‚æ¨¡å¼åŒ¹é…å¯¼è‡´çš„æŒ‡æ•°çº§æ—¶é—´ä¸ç©ºé—´å¼€é”€é—®é¢˜æå‡ºäº†è§£å†³æ–¹æ¡ˆã€‚ä¼ ç»ŸåŸºäº RETE çš„ç®—æ³•åœ¨é¢å¯¹åŒ…å«å¤§é‡æ¬ çº¦æŸå˜é‡æˆ–äº§ç”Ÿç»„åˆä¸­é—´ç»“æœçš„è§„åˆ™æ—¶ï¼Œææ˜“å› å†…å­˜æº¢å‡ºæˆ–æ‰§è¡Œåœæ»è€Œå¤±æ•ˆï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†é€šè¿‡ anti-unification è‡ªåŠ¨ç”Ÿæˆçš„è§„åˆ™æ—¶ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…å¼•å…¥äº†åä¸º CORGI (Collection-Oriented Relational Graph Iteration) çš„æ–°å‹åŒ¹é…ç®—æ³•ã€‚CORGI æ”¾å¼ƒäº†ä¼ ç»Ÿçš„ $\\beta-memory$ å­˜å‚¨æœºåˆ¶ï¼Œè½¬è€Œé‡‡ç”¨åœ¨å‰å‘è¿‡ç¨‹ä¸­ç»´æŠ¤å…³ç³»å›¾å¹¶åœ¨åå‘è¿‡ç¨‹ä¸­é€šè¿‡è¿­ä»£å™¨æŒ‰éœ€ç”ŸæˆåŒ¹é…çš„ä¸¤æ­¥ç­–ç•¥ã€‚è¯¥ç®—æ³•ä¸ºå¯»æ‰¾å•ä¸ªæ»¡è¶³è§£æä¾›äº†äºŒæ¬¡æ–¹ (quadratic) çš„æ—¶é—´å’Œç©ºé—´ä¿è¯ï¼Œå¹¶æ”¯æŒåç»­åŒ¹é…çš„æµå¼è¾“å‡ºï¼Œä»è€Œé¿å…äº†æ„å»ºå®Œæ•´å†²çªé›†å¸¦æ¥çš„é«˜å»¶è¿Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCORGI åœ¨å¤„ç†ç»„åˆåŒ¹é…ä»»åŠ¡æ—¶çš„æ€§èƒ½æ˜¾è‘—ä¼˜äº SOAR å’Œ OPS5 ä¸­çš„ RETE å®ç°ï¼Œä¸ºæ„å»ºå®ç”¨ä¸”å¯é çš„è®¤çŸ¥ç³»ç»Ÿæä¾›äº†æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.DS",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13942v1",
      "published_date": "2025-11-17 21:49:39 UTC",
      "updated_date": "2025-11-17 21:49:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:48:09.033369+00:00"
    },
    {
      "arxiv_id": "2511.13936v1",
      "title": "Preference-Based Learning in Audio Applications: A Systematic Analysis",
      "title_zh": "éŸ³é¢‘åº”ç”¨ä¸­åŸºäºåå¥½çš„å­¦ä¹ ï¼šç³»ç»Ÿæ€§åˆ†æ",
      "authors": [
        "Aaron Broukhim",
        "Yiran Shen",
        "Prithviraj Ammanabrolu",
        "Nadir Weibel"
      ],
      "abstract": "Despite the parallel challenges that audio and text domains face in evaluating generative model outputs, preference learning remains remarkably underexplored in audio applications. Through a PRISMA-guided systematic review of approximately 500 papers, we find that only 30 (6%) apply preference learning to audio tasks. Our analysis reveals a field in transition: pre-2021 works focused on emotion recognition using traditional ranking methods (rankSVM), while post-2021 studies have pivoted toward generation tasks employing modern RLHF frameworks. We identify three critical patterns: (1) the emergence of multi-dimensional evaluation strategies combining synthetic, automated, and human preferences; (2) inconsistent alignment between traditional metrics (WER, PESQ) and human judgments across different contexts; and (3) convergence on multi-stage training pipelines that combine reward signals. Our findings suggest that while preference learning shows promise for audio, particularly in capturing subjective qualities like naturalness and musicality, the field requires standardized benchmarks, higher-quality datasets, and systematic investigation of how temporal factors unique to audio impact preference learning frameworks.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡PRISMAæŒ‡å—å¯¹çº¦500ç¯‡è®ºæ–‡è¿›è¡Œäº†ç³»ç»Ÿæ€§ç»¼è¿°ï¼Œåˆ†æäº†åŸºäºåå¥½çš„å­¦ä¹ (Preference-Based Learning)åœ¨éŸ³é¢‘åº”ç”¨é¢†åŸŸçš„ç°çŠ¶ï¼ŒæŒ‡å‡ºè¯¥é¢†åŸŸç›¸æ¯”æ–‡æœ¬é¢†åŸŸä»å¤„äºæ¢ç´¢åˆæœŸã€‚åˆ†ææ˜¾ç¤ºéŸ³é¢‘åå¥½å­¦ä¹ æ­£å¤„äºè½¬å‹æœŸï¼Œç ”ç©¶é‡å¿ƒä»2021å¹´å‰åˆ©ç”¨ä¼ ç»Ÿæ’åºæ–¹æ³•(rankSVM)è¿›è¡Œæƒ…ç»ªè¯†åˆ«ï¼Œè½¬å‘2021å¹´ååˆ©ç”¨ç°ä»£RLHFæ¡†æ¶å¤„ç†ç”Ÿæˆä»»åŠ¡ã€‚ç ”ç©¶è¯†åˆ«å‡ºä¸‰å¤§å…³é”®è¶‹åŠ¿ï¼šç»“åˆåˆæˆã€è‡ªåŠ¨åŒ–å’Œäººç±»åå¥½çš„å¤šç»´è¯„ä¼°ç­–ç•¥çš„å…´èµ·ï¼›ä¼ ç»ŸæŒ‡æ ‡(WER, PESQ)ä¸äººç±»åˆ¤æ–­åœ¨ä¸åŒæƒ…å¢ƒä¸‹å­˜åœ¨å¯¹é½ä¸ä¸€è‡´ï¼›ä»¥åŠèåˆå¥–åŠ±ä¿¡å·çš„å¤šé˜¶æ®µè®­ç»ƒæµæ°´çº¿çš„æ”¶æ•›ã€‚å°½ç®¡åå¥½å­¦ä¹ åœ¨æ•æ‰è‡ªç„¶åº¦å’ŒéŸ³ä¹æ€§ç­‰ä¸»è§‚è´¨é‡æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ä½œè€…å¼ºè°ƒè¯¥é¢†åŸŸä»éœ€å»ºç«‹æ ‡å‡†åŒ–Benchmarksã€é«˜è´¨é‡æ•°æ®é›†ï¼Œå¹¶æ·±å…¥æ¢è®¨éŸ³é¢‘ç‰¹æœ‰çš„æ—¶é—´å› ç´ å¯¹åå¥½å­¦ä¹ æ¡†æ¶çš„å½±å“ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13936v1",
      "published_date": "2025-11-17 21:42:01 UTC",
      "updated_date": "2025-11-17 21:42:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:48:23.533208+00:00"
    },
    {
      "arxiv_id": "2511.21709v1",
      "title": "Quantifying and Mitigating Selection Bias in LLMs: A Transferable LoRA Fine-Tuning and Efficient Majority Voting Approach",
      "title_zh": "é‡åŒ–ä¸ç¼“è§£ LLMs ä¸­çš„é€‰æ‹©åå·®ï¼šä¸€ç§å¯è¿ç§»çš„ LoRA å¾®è°ƒä¸é«˜æ•ˆå¤šæ•°æŠ•ç¥¨æ–¹æ³•",
      "authors": [
        "Blessed Guda",
        "Lawrence Francis",
        "Gabrial Zencha Ashungafac",
        "Carlee Joe-Wong",
        "Moise Busogi"
      ],
      "abstract": "Multiple Choice Question (MCQ) answering is a widely used method for evaluating the performance of Large Language Models (LLMs). However, LLMs often exhibit selection bias in MCQ tasks, where their choices are influenced by factors like answer position or option symbols rather than the content. This bias undermines the reliability of MCQ as an evaluation framework. Most existing selection bias metrics require answer labels and measure divergences between prediction and answer distributions, but do not fully capture the consistency of a model's predictions across different orderings of answer choices. Existing selection bias mitigation strategies have notable limitations: majority voting, though effective, is computationally prohibitive; calibration-based methods require validation sets and often fail to generalize across datasets. To address these gaps, we propose three key contributions: (1) a new unsupervised label-free Permutation Bias Metric (PBM) that directly quantifies inconsistencies in model predictions across answer permutations, providing a more precise measure of selection bias, (2) an efficient majority voting approach called Batch Question-Context KV caching (BaQCKV), to significantly reduce computational costs while preserving bias mitigation effectiveness, and (3) an unsupervised Low-Rank Adaptation (LoRA-1) fine-tuning strategy based on our proposed metric and the BaQCKV that mitigates selection bias, providing a computationally efficient alternative that maintains model generalizability. Experiments across multiple MCQ benchmarks demonstrate that our approaches reduce bias, increasing consistency in accuracy while minimizing computational costs.",
      "tldr_zh": "è¯¥ç ”ç©¶å…³æ³¨å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤šé€‰é¢˜(MCQ)ä»»åŠ¡ä¸­å­˜åœ¨çš„é€‰æ‹©åå·®(selection bias)é—®é¢˜ï¼Œå³æ¨¡å‹çš„é€‰æ‹©å¾€å¾€å—åˆ°é€‰é¡¹ä½ç½®æˆ–ç¬¦å·è€Œéå†…å®¹çš„å½±å“ã€‚ä¸ºäº†è§£å†³ç°æœ‰è¯„ä¼°æŒ‡æ ‡ä¾èµ–æ ‡ç­¾ä¸”æ— æ³•åæ˜ é¢„æµ‹ä¸€è‡´æ€§çš„å±€é™ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ— ç›‘ç£ã€æ— éœ€æ ‡ç­¾çš„æ’åˆ—åå·®æŒ‡æ ‡(Permutation Bias Metric, PBM)ï¼Œç”¨äºç›´æ¥é‡åŒ–æ¨¡å‹åœ¨é€‰é¡¹æ’åˆ—å˜åŒ–æ—¶çš„é¢„æµ‹ä¸ä¸€è‡´æ€§ã€‚é’ˆå¯¹ç¼“è§£ç­–ç•¥ä¸­çš„é«˜æ˜‚è®¡ç®—æˆæœ¬ï¼Œç ”ç©¶å¼•å…¥äº†æ‰¹é‡é—®é¢˜èƒŒæ™¯KVç¼“å­˜æŠ€æœ¯(Batch Question-Context KV caching, BaQCKV)ï¼Œåœ¨ä¿æŒå¤šæ•°æŠ•ç¥¨(majority voting)å»åæ•ˆæœçš„åŒæ—¶æ˜¾è‘—é™ä½äº†å¼€é”€ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§åŸºäºPBMå’ŒBaQCKVçš„æ— ç›‘ç£LoRA-1å¾®è°ƒç­–ç•¥ï¼Œä¸ºç¼“è§£é€‰æ‹©åå·®æä¾›äº†ä¸€ç§è®¡ç®—é«˜æ•ˆä¸”å…·å¤‡è‰¯å¥½æ³›åŒ–èƒ½åŠ›çš„æ–¹æ¡ˆã€‚å¤šä¸ªMCQåŸºå‡†æµ‹è¯•çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ–¹æ³•æœ‰æ•ˆå‡å°‘äº†æ¨¡å‹åå·®ï¼Œåœ¨æ˜¾è‘—æé«˜å‡†ç¡®ç‡ä¸€è‡´æ€§çš„åŒæ—¶æœ€å°åŒ–äº†è®¡ç®—èµ„æºçš„æ¶ˆè€—ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted into IJCNLP-AACL 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.21709v1",
      "published_date": "2025-11-17 21:31:37 UTC",
      "updated_date": "2025-11-17 21:31:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:48:32.640409+00:00"
    },
    {
      "arxiv_id": "2511.13912v2",
      "title": "Compute-in-Memory Implementation of State Space Models for Event Sequence Processing",
      "title_zh": "ç”¨äºäº‹ä»¶åºåˆ—å¤„ç†çš„çŠ¶æ€ç©ºé—´æ¨¡å‹å­˜ç®—ä¸€ä½“å®ç°",
      "authors": [
        "Xiaoyu Zhang",
        "Mingtao Hu",
        "Sen Lu",
        "Soohyeon Kim",
        "Eric Yeu-Jer Lee",
        "Yuyang Liu",
        "Wei D. Lu"
      ],
      "abstract": "State space models (SSMs) have recently emerged as a powerful framework for long sequence processing, outperforming traditional methods on diverse benchmarks. Fundamentally, SSMs can generalize both recurrent and convolutional networks and have been shown to even capture key functions of biological systems. Here we report an approach to implement SSMs in energy-efficient compute-in-memory (CIM) hardware to achieve real-time, event-driven processing. Our work re-parameterizes the model to function with real-valued coefficients and shared decay constants, reducing the complexity of model mapping onto practical hardware systems. By leveraging device dynamics and diagonalized state transition parameters, the state evolution can be natively implemented in crossbar-based CIM systems combined with memristors exhibiting short-term memory effects. Through this algorithm and hardware co-design, we show the proposed system offers both high accuracy and high energy efficiency while supporting fully asynchronous processing for event-based vision and audio tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨é«˜æ•ˆèƒ½å­˜ç®—ä¸€ä½“(Compute-in-Memory, CIM)ç¡¬ä»¶ä¸­å®ç°çŠ¶æ€ç©ºé—´æ¨¡å‹(State Space Models, SSMs)ï¼Œä»¥æ»¡è¶³å®æ—¶äº‹ä»¶é©±åŠ¨çš„é•¿åºåˆ—å¤„ç†éœ€æ±‚ã€‚é€šè¿‡å¯¹æ¨¡å‹è¿›è¡Œé‡æ–°å‚æ•°åŒ–ï¼Œä½¿å…¶åˆ©ç”¨å®å€¼ç³»æ•°å’Œå…±äº«è¡°å‡å¸¸æ•°è¿è¡Œï¼Œç ”ç©¶å›¢é˜Ÿæ˜¾è‘—é™ä½äº†å°†å¤æ‚æ¨¡å‹æ˜ å°„åˆ°å®é™…ç¡¬ä»¶ç³»ç»Ÿä¸­çš„éš¾åº¦ã€‚åœ¨ç¡¬ä»¶å±‚é¢ï¼Œåˆ©ç”¨åŸºäºäº¤å‰æ†(crossbar)ç»“æ„çš„CIMç³»ç»Ÿå¹¶ç»“åˆå…·æœ‰çŸ­æ—¶è®°å¿†æ•ˆåº”çš„å¿†é˜»å™¨(memristors)ï¼Œè¯¥æ–¹æ¡ˆé€šè¿‡ç®—æ³•ä¸ç¡¬ä»¶çš„ååŒè®¾è®¡(co-design)åŸç”Ÿå®ç°äº†å¯¹è§’åŒ–çŠ¶æ€è½¬ç§»å‚æ•°çš„æ¼”è¿›ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å¤„ç†åŸºäºäº‹ä»¶çš„è§†è§‰å’ŒéŸ³é¢‘ä»»åŠ¡æ—¶èƒ½å¤Ÿå…¼é¡¾é«˜ç²¾åº¦ä¸é«˜èƒ½æ•ˆï¼Œå¹¶æ”¯æŒå®Œå…¨å¼‚æ­¥çš„åºåˆ—å¤„ç†ã€‚è¿™é¡¹å·¥ä½œä¸ºå®ç°é«˜æ€§èƒ½ã€ä½åŠŸè€—ä¸”å…·å¤‡ç”Ÿç‰©å¯å‘ç‰¹æ€§çš„ç¡¬ä»¶æ™ºèƒ½ç³»ç»Ÿæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "Xiaoyu Zhang and Mingtao Hu contributed equally to this work",
      "pdf_url": "https://arxiv.org/pdf/2511.13912v2",
      "published_date": "2025-11-17 21:06:52 UTC",
      "updated_date": "2025-12-23 18:00:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:48:45.334385+00:00"
    },
    {
      "arxiv_id": "2511.13900v1",
      "title": "What Works for 'Lost-in-the-Middle' in LLMs? A Study on GM-Extract and Mitigations",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­â€œè¿·å¤±ä¸­æ®µâ€ç°è±¡çš„æœ‰æ•ˆå¯¹ç­–ï¼šé’ˆå¯¹ GM-Extract ä¸ç¼“è§£ç­–ç•¥çš„ç ”ç©¶",
      "authors": [
        "Mihir Gupte",
        "Eshan Dixit",
        "Muhammad Tayyab",
        "Arun Adiththan"
      ],
      "abstract": "The diminishing ability of large language models (LLMs) to effectively utilize long-range context-the \"lost-in-the-middle\" phenomenon-poses a significant challenge in retrieval-based LLM applications. To study the impact of this phenomenon in a real-world application setting, we introduce GM-Extract, a novel benchmark dataset meticulously designed to evaluate LLM performance on retrieval of control variables. To accurately diagnose failure modes, we propose a simple yet elegant evaluation system using two distinct metrics: one for spatial retrieval capability (Document Metric) and the other for semantic retrieval capability (Variable Extraction Metric). We conduct a systematic evaluation of 7-8B parameter models on two multi-document tasks (key-value extraction and question-answering), demonstrating a significant change in retrieval performance simply by altering how the data is represented in the context window. While a distinct U-shaped curve was not consistently observed, our analysis reveals a clear pattern of performance across models, which we further correlate with perplexity scores. Furthermore, we perform a literature survey of mitigation methods, which we categorize into two distinct approaches: black-box and white-box methods. We then apply these techniques to our benchmark, finding that their efficacy is highly nuanced. Our evaluation highlights scenarios where these strategies successfully improve performance, as well as surprising cases where they lead to a negative impact, providing a comprehensive understanding of their utility in a practical context.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨é•¿ä¸Šä¸‹æ–‡å¤„ç†ä¸­é¢ä¸´çš„\"lost-in-the-middle\"æŒ‘æˆ˜ï¼Œæå‡ºäº†æ–°å‹åŸºå‡†æ•°æ®é›†GM-Extractï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹å¯¹æ§åˆ¶å˜é‡çš„æ£€ç´¢èƒ½åŠ›ã€‚ç ”ç©¶å¼•å…¥äº†ä¸€å¥—åŒ…å«Document Metricï¼ˆç©ºé—´æ£€ç´¢ï¼‰å’ŒVariable Extraction Metricï¼ˆè¯­ä¹‰æ£€ç´¢ï¼‰çš„è¯„ä¼°ç³»ç»Ÿï¼Œå¯¹7-8Bå‚æ•°æ¨¡å‹åœ¨å¤šæ–‡æ¡£ä»»åŠ¡ä¸­çš„è¡¨ç°è¿›è¡Œäº†ç³»ç»Ÿè¯Šæ–­ã€‚å®éªŒå‘ç°ï¼Œä»…é€šè¿‡æ”¹å˜ä¸Šä¸‹æ–‡çª—å£ä¸­çš„æ•°æ®è¡¨ç¤ºæ–¹å¼å°±èƒ½æ˜¾è‘—æ”¹å˜æ£€ç´¢æ€§èƒ½ï¼Œä¸”æ¨¡å‹è¡¨ç°ä¸perplexityåˆ†æ•°å‘ˆç°ç›¸å…³æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å¯¹ç¼“è§£æ–¹æ³•è¿›è¡Œäº†é»‘ç›’(black-box)å’Œç™½ç›’(white-box)åˆ†ç±»è°ƒç ”ï¼Œå¹¶åœ¨å®é™…åº”ç”¨ä¸­æµ‹è¯•äº†å®ƒä»¬çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿™äº›ç¼“è§£ç­–ç•¥çš„æ•ˆç”¨å…·æœ‰é«˜åº¦å¤æ‚æ€§ï¼Œè™½ç„¶åœ¨ç‰¹å®šåœºæ™¯ä¸‹èƒ½æå‡æ€§èƒ½ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ä¹Ÿä¼šäº§ç”Ÿè´Ÿé¢å½±å“ï¼Œä¸ºé•¿æ–‡æœ¬å¤„ç†çš„å®é™…åº”ç”¨æä¾›äº†æ·±å…¥è§è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "To be submitted for publication",
      "pdf_url": "https://arxiv.org/pdf/2511.13900v1",
      "published_date": "2025-11-17 20:50:50 UTC",
      "updated_date": "2025-11-17 20:50:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:48:50.745850+00:00"
    },
    {
      "arxiv_id": "2511.13892v1",
      "title": "Jailbreaking Large Vision Language Models in Intelligent Transportation Systems",
      "title_zh": "æ™ºèƒ½äº¤é€šç³»ç»Ÿä¸­çš„å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹è¶Šç‹±æ”»å‡»",
      "authors": [
        "Badhan Chandra Das",
        "Md Tasnim Jawad",
        "Md Jueal Mia",
        "M. Hadi Amini",
        "Yanzhao Wu"
      ],
      "abstract": "Large Vision Language Models (LVLMs) demonstrate strong capabilities in multimodal reasoning and many real-world applications, such as visual question answering. However, LVLMs are highly vulnerable to jailbreaking attacks. This paper systematically analyzes the vulnerabilities of LVLMs integrated in Intelligent Transportation Systems (ITS) under carefully crafted jailbreaking attacks. First, we carefully construct a dataset with harmful queries relevant to transportation, following OpenAI's prohibited categories to which the LVLMs should not respond. Second, we introduce a novel jailbreaking attack that exploits the vulnerabilities of LVLMs through image typography manipulation and multi-turn prompting. Third, we propose a multi-layered response filtering defense technique to prevent the model from generating inappropriate responses. We perform extensive experiments with the proposed attack and defense on the state-of-the-art LVLMs (both open-source and closed-source). To evaluate the attack method and defense technique, we use GPT-4's judgment to determine the toxicity score of the generated responses, as well as manual verification. Further, we compare our proposed jailbreaking method with existing jailbreaking techniques and highlight severe security risks involved with jailbreaking attacks with image typography manipulation and multi-turn prompting in the LVLMs integrated in ITS.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿåˆ†æäº†åœ¨æ™ºèƒ½äº¤é€šç³»ç»Ÿ(ITS)ä¸­é›†æˆçš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)é¢ä¸´çš„è¶Šç‹±æ”»å‡»(jailbreaking attacks)æ¼æ´ã€‚ç ”ç©¶é¦–å…ˆæ ¹æ®OpenAIçš„ç¦æ­¢ç±»åˆ«ï¼Œé’ˆå¯¹äº¤é€šé¢†åŸŸæ„å»ºäº†ä¸€ä¸ªåŒ…å«æœ‰å®³æŸ¥è¯¢çš„ä¸“ç”¨æ•°æ®é›†ã€‚éšåæå‡ºäº†ä¸€ç§ç»“åˆå›¾åƒæ’ç‰ˆæ“çºµ(image typography manipulation)ä¸å¤šè½®æç¤º(multi-turn prompting)çš„æ–°å‹è¶Šç‹±æ”»å‡»æ–¹æ³•ï¼Œæ—¨åœ¨æ­ç¤ºæ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­çš„å®‰å…¨ç¼ºé™·ã€‚ä¸ºæŠµå¾¡æ­¤ç±»æ”»å‡»ï¼Œä½œè€…è¿˜è®¾è®¡äº†ä¸€ç§å¤šå±‚å“åº”è¿‡æ»¤é˜²å¾¡æŠ€æœ¯(multi-layered response filtering defense)ä»¥é˜²æ­¢æ¨¡å‹ç”Ÿæˆä¸å½“å›å¤ã€‚é€šè¿‡å¯¹å¤šä¸ªå…ˆè¿›çš„å¼€æºå’Œé—­æºæ¨¡å‹è¿›è¡Œå®éªŒï¼Œå¹¶åˆ©ç”¨GPT-4å’Œäººå·¥è¯„ä¼°è¿›è¡ŒéªŒè¯ï¼Œè¯¥ç ”ç©¶å¼ºè°ƒäº†å›¾åƒæ’ç‰ˆæ”»å‡»å¯¹äº¤é€šæ™ºèƒ½ç³»ç»Ÿæ„æˆçš„ä¸¥é‡å®‰å…¨å¨èƒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13892v1",
      "published_date": "2025-11-17 20:29:48 UTC",
      "updated_date": "2025-11-17 20:29:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:48:51.935393+00:00"
    },
    {
      "arxiv_id": "2511.15728v1",
      "title": "The Future of Food: How Artificial Intelligence is Transforming Food Manufacturing",
      "title_zh": "é£Ÿå“çš„æœªæ¥ï¼šäººå·¥æ™ºèƒ½å¦‚ä½•å˜é©é£Ÿå“åˆ¶é€ ä¸š",
      "authors": [
        "Xu Zhou",
        "Ivor Prado",
        "AIFPDS participants",
        "Ilias Tagkopoulos"
      ],
      "abstract": "Artificial intelligence is accelerating a new era of food innovation, connecting data from farm to consumer to improve formulation, processing, and health outcomes. Recent advances in deep learning, natural language processing, and multi-omics integration make it possible to understand and optimize food systems with unprecedented depth. However, AI adoption across the food sector remains uneven due to heterogeneous datasets, limited model and system interoperability, and a persistent skills gap between data scientists and food domain experts. To address these challenges and advance responsible innovation, the AI Institute for Next Generation Food Systems (AIFS) convened the inaugural AI for Food Product Development Symposium at University of California, Davis, in October 2025. This white paper synthesizes insights from the symposium, organized around five domains where AI can have the greatest near-term impact: supply chain; formulation and processing; consumer insights and sensory prediction; nutrition and health; and education and workforce development. Across the areas, participants emphasized the importance of interoperable data standards, transparent and interpretable models, and cross-sector collaboration to accelerate the translation of AI research into practice. The discussions further highlighted the need for robust digital infrastructure, privacy-preserving data-sharing mechanisms, and interdisciplinary training pathways that integrate AI literacy with domain expertise. Collectively, the priorities outline a roadmap for integrating AI into food manufacturing in ways that enhance innovation, sustainability, and human well-being while ensuring that technological progress remains grounded in ethics, scientific rigor, and societal benefit.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½(Artificial Intelligence)å¦‚ä½•é€šè¿‡æ•´åˆä»å†œåœºåˆ°æ¶ˆè´¹è€…çš„å…¨é“¾æ¡æ•°æ®ï¼ŒåŠ é€Ÿé£Ÿå“åˆ¶é€ é¢†åŸŸçš„åˆ›æ–°ã€‚æ·±åº¦å­¦ä¹ (Deep Learning)ã€è‡ªç„¶è¯­è¨€å¤„ç†(Natural Language Processing)å’Œå¤šç»„å­¦(Multi-omics)æ•´åˆæŠ€æœ¯çš„è¿›æ­¥ä¸ºé£Ÿå“ç³»ç»Ÿçš„æ·±åº¦ä¼˜åŒ–æä¾›äº†å¯èƒ½ï¼Œä½†å¼‚æ„æ•°æ®é›†(Heterogeneous datasets)å’Œä¸“ä¸šæŠ€èƒ½å·®è·ä»æ˜¯ä¸»è¦éšœç¢ã€‚ä¸‹ä¸€ä»£é£Ÿå“ç³»ç»Ÿäººå·¥æ™ºèƒ½ç ”ç©¶æ‰€(AIFS)æ€»ç»“äº†AIåœ¨ä¾›åº”é“¾ã€é…æ–¹ä¸åŠ å·¥ã€æ¶ˆè´¹è€…æ´å¯Ÿã€è¥å…»å¥åº·åŠäººæ‰åŸ¹å…»äº”ä¸ªç»´åº¦çš„è¿‘æœŸå½±å“ã€‚ä¸ä¼šä¸“å®¶å¼ºè°ƒäº†å»ºç«‹äº’æ“ä½œæ•°æ®æ ‡å‡†(Interoperable data standards)ã€é€æ˜ä¸”å¯è§£é‡Šæ¨¡å‹(Interpretable models)ä»¥åŠè·¨éƒ¨é—¨åä½œå¯¹æŠ€æœ¯è½åœ°çš„å…³é”®ä½œç”¨ã€‚æ­¤å¤–ï¼Œæ„å»ºç¨³å¥çš„æ•°å­—åŸºç¡€è®¾æ–½å’Œè·¨å­¦ç§‘åŸ¹è®­è·¯å¾„è¢«è§†ä¸ºå®ç°è´Ÿè´£ä»»åˆ›æ–°çš„å¿…è¦æ¡ä»¶ã€‚è¯¥ç™½çš®ä¹¦æœ€ç»ˆä¸ºAIåœ¨é£Ÿå“å·¥ä¸šä¸­çš„æ•´åˆåˆ¶å®šäº†å‘å±•è·¯çº¿å›¾ï¼Œæ—¨åœ¨æå‡æ•ˆç‡ä¸å¯æŒç»­æ€§çš„åŒæ—¶ç¡®ä¿å…¶ç¬¦åˆä¼¦ç†å’Œç§‘å­¦ä¸¥è°¨æ€§ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15728v1",
      "published_date": "2025-11-17 20:17:55 UTC",
      "updated_date": "2025-11-17 20:17:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:49:03.136028+00:00"
    },
    {
      "arxiv_id": "2511.13884v1",
      "title": "Can QE-informed (Re)Translation lead to Error Correction?",
      "title_zh": "åŸºäºè´¨é‡è¯„ä¼°çš„ï¼ˆé‡ï¼‰ç¿»è¯‘èƒ½å¦å®ç°çº é”™ï¼Ÿ",
      "authors": [
        "Govardhan Padmanabhan"
      ],
      "abstract": "The paper presents two approaches submitted to the WMT 2025 Automated Translation Quality Evaluation Systems Task 3 - Quality Estimation (QE)-informed Segment-level Error Correction. While jointly training QE systems with Automatic Post-Editing (APE) has shown improved performance for both tasks, APE systems are still known to overcorrect the output of Machine Translation (MT), leading to a degradation in performance. We investigate a simple training-free approach - QE-informed Retranslation, and compare it with another within the same training-free paradigm. Our winning approach selects the highest-quality translation from multiple candidates generated by different LLMs. The second approach, more akin to APE, instructs an LLM to replace error substrings as specified in the provided QE explanation(s). A conditional heuristic was employed to minimise the number of edits, with the aim of maximising the Gain-to-Edit ratio. The two proposed approaches achieved a Delta COMET score of 0.0201 and -0.0108, respectively, leading the first approach to achieve the winning position on the subtask leaderboard.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹WMT 2025è‡ªåŠ¨ç¿»è¯‘è´¨é‡è¯„ä¼°(Quality Estimation, QE)ä»»åŠ¡ä¸­çš„æ®µè½çº§é”™è¯¯çº æ­£é—®é¢˜ï¼Œæ¢è®¨äº†ä¸¤ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ— éœ€è®­ç»ƒ(training-free)çº é”™æ–¹æ³•ã€‚ç¬¬ä¸€ç§æ–¹æ³•æ˜¯QEé©±åŠ¨çš„é‡ç¿»è¯‘(QE-informed Retranslation)ï¼Œå³ä»å¤šä¸ªLLMç”Ÿæˆçš„å€™é€‰ç¿»è¯‘ä¸­é€‰æ‹©è´¨é‡æœ€é«˜çš„ç»“æœï¼›ç¬¬äºŒç§æ–¹æ³•åˆ™æ›´æ¥è¿‘è‡ªåŠ¨åç¼–è¾‘(APE)ï¼Œæ—¨åœ¨æ ¹æ®QEæä¾›çš„é”™è¯¯è§£é‡Šæ¥æ›¿æ¢ç‰¹å®šçš„é”™è¯¯å­ä¸²ã€‚ä¸ºäº†å¹³è¡¡ç¿»è¯‘è´¨é‡ä¸ç¼–è¾‘æˆæœ¬ï¼Œç ”ç©¶é‡‡ç”¨äº†æ¡ä»¶å¯å‘å¼é€»è¾‘ä»¥æœ€å¤§åŒ–å¢ç›Šç¼–è¾‘æ¯”(Gain-to-Edit ratio)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå€™é€‰é‡ç¿»è¯‘æ–¹æ¡ˆä»¥0.0201çš„Delta COMETå¾—åˆ†åœ¨å­ä»»åŠ¡ä¸­å¤ºå† ï¼Œè€ŒåŸºäºé”™è¯¯è§£é‡Šçš„ç²¾å‡†æ›¿æ¢æ–¹æ³•å¾—åˆ†ä»…ä¸º-0.0108ã€‚è¯¥å‘ç°è¯æ˜äº†åœ¨æ— è®­ç»ƒèŒƒå¼ä¸‹ï¼Œé€šè¿‡QEè¯„ä¼°å¹¶é€‰æ‹©æœ€ä½³ç¿»è¯‘å€™é€‰æ¯”ç›´æ¥è¿›è¡Œåç¼–è¾‘ä¿®æ­£å±€éƒ¨é”™è¯¯æ›´ä¸ºæœ‰æ•ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 3 figures, WMT25 Shared Task in EMNLP 2025 Conference",
      "pdf_url": "https://arxiv.org/pdf/2511.13884v1",
      "published_date": "2025-11-17 20:10:47 UTC",
      "updated_date": "2025-11-17 20:10:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:49:02.043442+00:00"
    },
    {
      "arxiv_id": "2511.13877v1",
      "title": "Hybrid Convolution Neural Network Integrated with Pseudo-Newton Boosting for Lumbar Spine Degeneration Detection",
      "title_zh": "èåˆä¼ªç‰›é¡¿æå‡çš„æ··åˆå·ç§¯ç¥ç»ç½‘ç»œç”¨äºè…°æ¤é€€è¡Œæ€§å˜æ£€æµ‹",
      "authors": [
        "Pandiyaraju V",
        "Abishek Karthik",
        "Jaspin K",
        "Kannan A",
        "Jaime Lloret"
      ],
      "abstract": "This paper proposes a new enhanced model architecture to perform classification of lumbar spine degeneration with DICOM images while using a hybrid approach, integrating EfficientNet and VGG19 together with custom-designed components. The proposed model is differentiated from traditional transfer learning methods as it incorporates a Pseudo-Newton Boosting layer along with a Sparsity-Induced Feature Reduction Layer that forms a multi-tiered framework, further improving feature selection and representation. The Pseudo-Newton Boosting layer makes smart variations of feature weights, with more detailed anatomical features, which are mostly left out in a transfer learning setup. In addition, the Sparsity-Induced Layer removes redundancy for learned features, producing lean yet robust representations for pathology in the lumbar spine. This architecture is novel as it overcomes the constraints in the traditional transfer learning approach, especially in the high-dimensional context of medical images, and achieves a significant performance boost, reaching a precision of 0.9, recall of 0.861, F1 score of 0.88, loss of 0.18, and an accuracy of 88.1%, compared to the baseline model, EfficientNet. This work will present the architectures, preprocessing pipeline, and experimental results. The results contribute to the development of automated diagnostic tools for medical images.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºè…°æ¤é€€è¡Œæ€§å˜æ£€æµ‹çš„æ··åˆå·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œåˆ©ç”¨ DICOM å›¾åƒå®ç°ç²¾ç¡®åˆ†ç±»ã€‚è¯¥æ¶æ„é›†æˆäº† EfficientNet å’Œ VGG19ï¼Œå¹¶åˆ›æ–°æ€§åœ°å¼•å…¥äº† Pseudo-Newton Boosting å±‚å’Œ Sparsity-Induced Feature Reduction Layerï¼Œæ„å»ºäº†ä¸€ä¸ªå¤šå±‚çº§ç‰¹å¾æå–æ¡†æ¶ã€‚Pseudo-Newton Boosting å±‚é€šè¿‡æ™ºèƒ½è°ƒæ•´ç‰¹å¾æƒé‡ï¼Œæ•æ‰åˆ°äº†ä¼ ç»Ÿè¿ç§»å­¦ä¹ å¾€å¾€å¿½ç•¥çš„è¯¦ç»†è§£å‰–å­¦ç‰¹å¾ï¼Œè€Œ Sparsity-Induced Layer åˆ™æœ‰æ•ˆå»é™¤äº†ç‰¹å¾å†—ä½™ï¼Œç”Ÿæˆäº†ç²¾ç®€ä¸”ç¨³å¥çš„ç—…ç†è¡¨ç¤ºã€‚è¿™ç§è®¾è®¡å…‹æœäº†ä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†é«˜ç»´åŒ»å­¦å›¾åƒæ—¶çš„å±€é™æ€§ï¼Œåœ¨å®éªŒä¸­å–å¾—äº† 88.1% çš„å‡†ç¡®ç‡ã€0.9 çš„ç²¾ç¡®ç‡ä»¥åŠ 0.88 çš„ F1 åˆ†æ•°ã€‚ç›¸æ¯”åŸºå‡†æ¨¡å‹ EfficientNetï¼Œè¯¥æ¨¡å‹è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸ºè‡ªåŠ¨åŒ–åŒ»ç–—å½±åƒè¯Šæ–­å·¥å…·çš„å¼€å‘æä¾›äº†æ–°çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13877v1",
      "published_date": "2025-11-17 19:54:52 UTC",
      "updated_date": "2025-11-17 19:54:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:49:07.135354+00:00"
    },
    {
      "arxiv_id": "2511.13869v2",
      "title": "H-CNN-ViT: A Hierarchical Gated Attention Multi-Branch Model for Bladder Cancer Recurrence Prediction",
      "title_zh": "H-CNN-ViTï¼šé¢å‘è†€èƒ±ç™Œå¤å‘é¢„æµ‹çš„å±‚çº§é—¨æ§æ³¨æ„åŠ›å¤šåˆ†æ”¯æ¨¡å‹",
      "authors": [
        "Xueyang Li",
        "Zongren Wang",
        "Yuliang Zhang",
        "Zixuan Pan",
        "Yu-Jen Chen",
        "Nishchal Sapkota",
        "Gelei Xu",
        "Danny Z. Chen",
        "Yiyu Shi"
      ],
      "abstract": "Bladder cancer is one of the most prevalent malignancies worldwide, with a recurrence rate of up to 78%, necessitating accurate post-operative monitoring for effective patient management. Multi-sequence contrast-enhanced MRI is commonly used for recurrence detection; however, interpreting these scans remains challenging, even for experienced radiologists, due to post-surgical alterations such as scarring, swelling, and tissue remodeling. AI-assisted diagnostic tools have shown promise in improving bladder cancer recurrence prediction, yet progress in this field is hindered by the lack of dedicated multi-sequence MRI datasets for recurrence assessment study. In this work, we first introduce a curated multi-sequence, multi-modal MRI dataset specifically designed for bladder cancer recurrence prediction, establishing a valuable benchmark for future research. We then propose H-CNN-ViT, a new Hierarchical Gated Attention Multi-Branch model that enables selective weighting of features from the global (ViT) and local (CNN) paths based on contextual demands, achieving a balanced and targeted feature fusion. Our multi-branch architecture processes each modality independently, ensuring that the unique properties of each imaging channel are optimally captured and integrated. Evaluated on our dataset, H-CNN-ViT achieves an AUC of 78.6%, surpassing state-of-the-art models. Our model is publicly available at https://github.com/XLIAaron/H-CNN-ViT.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è†€èƒ±ç™Œé«˜è¾¾ 78% çš„å¤å‘ç‡åŠæœ¯å MRI å½±åƒè§£è¯»å›°éš¾ç­‰æŒ‘æˆ˜ï¼Œé¦–å…ˆæ„å»ºå¹¶å‘å¸ƒäº†ä¸€ä¸ªä¸“é—¨ç”¨äºå¤å‘é¢„æµ‹çš„å¤šåºåˆ—ã€å¤šæ¨¡æ€ MRI æ•°æ®é›†ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸç¼ºä¹ä¸“ç”¨åŸºå‡†æ•°æ®çš„ç©ºç™½ã€‚éšåï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º H-CNN-ViT çš„æ–°å‹åˆ†å±‚é—¨æ§æ³¨æ„åŠ›å¤šåˆ†æ”¯æ¨¡å‹ (Hierarchical Gated Attention Multi-Branch model)ï¼Œæ—¨åœ¨ä¼˜åŒ–ç‰¹å¾æå–ä¸èåˆè¿‡ç¨‹ã€‚è¯¥æ¨¡å‹é‡‡ç”¨å¤šåˆ†æ”¯æ¶æ„ç‹¬ç«‹å¤„ç†ä¸åŒæ¨¡æ€ï¼Œå¹¶é€šè¿‡åˆ†å±‚é—¨æ§æ³¨æ„åŠ›æœºåˆ¶å¯¹å…¨å±€è·¯å¾„ (ViT) å’Œå±€éƒ¨è·¯å¾„ (CNN) çš„ç‰¹å¾è¿›è¡Œé€‰æ‹©æ€§åŠ æƒï¼Œå®ç°äº†æ›´å…·é’ˆå¯¹æ€§çš„ç‰¹å¾è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒH-CNN-ViT åœ¨è¯¥æ•°æ®é›†ä¸Šå–å¾—äº† 78.6% çš„ AUCï¼Œæ€§èƒ½ä¼˜äºç°æœ‰çš„ state-of-the-art æ¨¡å‹ã€‚è¯¥å·¥ä½œä¸ä»…ä¸ºè†€èƒ±ç™Œå¤å‘ç›‘æµ‹æä¾›äº†é«˜æ•ˆçš„ AI è¾…åŠ©å·¥å…·ï¼Œä¹Ÿé€šè¿‡å¼€æºæ•°æ®é›†å’Œä»£ç ä¸ºåç»­ä¸´åºŠè½¬åŒ–ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13869v2",
      "published_date": "2025-11-17 19:39:22 UTC",
      "updated_date": "2025-11-19 04:47:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:49:07.544661+00:00"
    },
    {
      "arxiv_id": "2511.14808v1",
      "title": "Transformer Injectivity & Geometric Robustness - Analytic Margins and Bi-Lipschitz Uniformity of Sequence-Level Hidden States",
      "title_zh": "Transformer çš„å•å°„æ€§ä¸å‡ ä½•é²æ£’æ€§ï¼šåºåˆ—çº§éšè—çŠ¶æ€çš„è§£æé—´éš”ä¸åŒå‘ Lipschitz ä¸€è‡´æ€§",
      "authors": [
        "Mikael von Strauss"
      ],
      "abstract": "Under real-analytic assumptions on decoder-only Transformers, recent work shows that the map from discrete prompts to last-token hidden states is generically injective on finite prompt sets. We refine this picture: for each layer $\\ell$ we define a collision discriminant $Î”^\\ell \\subset Î˜$ and injective stratum $U^\\ell = Î˜\\setminus Î”^\\ell$, and prove a dichotomy -- either the model is nowhere injective on the set, or $U^\\ell$ is open and dense and every $F^\\ell_Î¸$ is injective. Under mild non-singularity assumptions on the optimizer and an absolutely continuous initialization, generic injectivity persists along smooth training trajectories over any fixed horizon. We also treat symmetry groups $G$, showing that discriminants and injective strata descend to the quotient $Î˜/G$, so injectivity is naturally a property of functional equivalence classes.\n  We complement these results with an empirical study of layerwise geometric diagnostics. We define a separation margin and a co-Lipschitz (lower Lipschitz) constant between prompt space and last-token representation space, estimated via nearest-neighbor statistics on large prompt sets. Applying these diagnostics to pretrained LLaMA-3 and Qwen models, we study behavior across layers, sequence lengths, model scales, and 8- and 4-bit activation quantization. On our sampled prompts we see no collisions in full precision or at 8 bits, while 4-bit quantization induces a small number of collisions and markedly shrinks co-Lipschitz estimates. For a small GPT-2 trained from scratch, normalized metrics remain stable over training. Overall, the results suggest that Transformer representations are generically and persistently injective in the continuous-parameter idealization, while their practical invertibility can be probed using simple geometric diagnostics.",
      "tldr_zh": "è¯¥ç ”ç©¶åœ¨å®åˆ†æ(real-analytic)å‡è®¾ä¸‹æ¢è®¨äº†ä»…è§£ç å™¨(decoder-only)Transformeræ¨¡å‹çš„å•å°„æ€§(Injectivity)åŠå…¶å‡ ä½•é²æ£’æ€§ã€‚ä½œè€…é€šè¿‡å®šä¹‰ç¢°æ’åˆ¤åˆ«å¼(collision discriminant)å’Œå•å°„å±‚(injective stratum)ï¼Œè¯æ˜äº†æ¨¡å‹åœ¨æœ‰é™æç¤ºé›†ä¸Šè¦ä¹ˆå¤„å¤„ä¸å•å°„ï¼Œè¦ä¹ˆåœ¨å‚æ•°ç©ºé—´ä¸­å‘ˆç°å‡ºå¼€ä¸”ç¨ å¯†çš„å•å°„ç‰¹æ€§ã€‚è®ºæ–‡è¿›ä¸€æ­¥æ­ç¤ºäº†è¿™ç§å•å°„æ€§åœ¨å¹³æ»‘è®­ç»ƒè½¨è¿¹ä¸­çš„æŒä¹…æ€§ï¼Œå¹¶è®ºè¯äº†å¯¹ç§°ç¾¤(symmetry groups)ä½œç”¨ä¸‹çš„å‡½æ•°ç­‰ä»·ç±»åŒæ ·è‡ªç„¶å…·å¤‡å•å°„å±æ€§ã€‚é’ˆå¯¹é¢„è®­ç»ƒçš„LLaMA-3å’ŒQwenæ¨¡å‹ï¼Œç ”ç©¶è€…å¼•å…¥äº†é—´éš”ä½™é‡(separation margin)å’Œä¸‹åˆ©æ™®å¸ŒèŒ¨å¸¸æ•°(co-Lipschitz constant)ç­‰å‡ ä½•è¯Šæ–­æŒ‡æ ‡è¿›è¡Œå®è¯åˆ†æã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å…¨ç²¾åº¦åŠ8ä½é‡åŒ–ä¸‹é‡‡æ ·æç¤ºè¯æœªè§ç¢°æ’ï¼Œä½†4ä½é‡åŒ–(4-bit quantization)ä¼šè¯±å‘å°‘é‡ç¢°æ’å¹¶æ˜¾è‘—é™ä½ä¸‹åˆ©æ™®å¸ŒèŒ¨ä¼°è®¡å€¼ã€‚è¿™é¡¹ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡åœ¨è¿ç»­å‚æ•°ç†æƒ³åŒ–ä¸‹Transformerè¡¨å¾å…·æœ‰æ³›åœ¨çš„å•å°„æ€§ï¼Œä½†å…¶å®é™…çš„å¯é€†æ€§ä»å—é‡åŒ–ç²¾åº¦çš„æ˜¾è‘—å½±å“ï¼Œå¹¶å¯é€šè¿‡ç®€å•çš„å‡ ä½•è¯Šæ–­æ‰‹æ®µè¿›è¡Œæ¢æµ‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "22 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.14808v1",
      "published_date": "2025-11-17 19:39:15 UTC",
      "updated_date": "2025-11-17 19:39:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:49:11.737059+00:00"
    },
    {
      "arxiv_id": "2511.13865v1",
      "title": "Randomized Controlled Trials for Conditional Access Optimization Agent",
      "title_zh": "é’ˆå¯¹æ¡ä»¶è®¿é—®ä¼˜åŒ–æ™ºèƒ½ä½“çš„éšæœºå¯¹ç…§è¯•éªŒ",
      "authors": [
        "James Bono",
        "Beibei Cheng",
        "Joaquin Lozano"
      ],
      "abstract": "AI agents are increasingly deployed to automate complex enterprise workflows, yet evidence of their effectiveness in identity governance is limited. We report results from the first randomized controlled trial (RCT) evaluating an AI agent for Conditional Access (CA) policy management in Microsoft Entra. The agent assists with four high-value tasks: policy merging, Zero-Trust baseline gap detection, phased rollout planning, and user-policy alignment. In a production-grade environment, 162 identity administrators were randomly assigned to a control group (no agent) or treatment group (agent-assisted) and asked to perform these tasks. Agent access produced substantial gains: accuracy improved by 48% and task completion time decreased by 43% while holding accuracy constant. The largest benefits emerged on cognitively demanding tasks such as baseline gap detection. These findings demonstrate that purpose-built AI agents can significantly enhance both speed and accuracy in identity administration.",
      "tldr_zh": "è¯¥ç ”ç©¶æŠ¥é“äº†é¦–ä¸ªè¯„ä¼°ç”¨äº Microsoft Entra æ¡ä»¶è®¿é—® (Conditional Access) ç­–ç•¥ç®¡ç†çš„ AI agent æœ‰æ•ˆæ€§çš„éšæœºå¯¹ç…§è¯•éªŒ (Randomized Controlled Trial, RCT)ã€‚è¯¥æ™ºèƒ½ä½“æ—¨åœ¨ååŠ©æ‰§è¡Œç­–ç•¥åˆå¹¶ã€é›¶ä¿¡ä»» (Zero-Trust) åŸºå‡†å·®è·æ£€æµ‹ã€åˆ†é˜¶æ®µéƒ¨ç½²è§„åˆ’ä»¥åŠç”¨æˆ·ç­–ç•¥å¯¹é½ç­‰å››é¡¹é«˜ä»·å€¼ä»»åŠ¡ã€‚ç ”ç©¶åœ¨ç”Ÿäº§çº§ç¯å¢ƒä¸­è¿›è¡Œï¼Œå°† 162 åèº«ä»½ç®¡ç†å‘˜éšæœºåˆ†é…åˆ°å¯¹ç…§ç»„æˆ–å®éªŒç»„æ‰§è¡Œä»»åŠ¡ï¼Œç»“æœæ˜¾ç¤º AI agent çš„ä»‹å…¥ä½¿ç®¡ç†å‡†ç¡®ç‡æ˜¾è‘—æå‡äº† 48%ï¼ŒåŒæ—¶ä»»åŠ¡å®Œæˆæ—¶é—´å‡å°‘äº† 43%ã€‚ç ”ç©¶å‘ç°æœ€å¤§çš„æ•ˆç›Šä½“ç°åœ¨åŸºå‡†å·®è·æ£€æµ‹ç­‰è®¤çŸ¥è´Ÿè·è¾ƒé«˜çš„ä»»åŠ¡ä¸­ï¼Œè¯æ˜äº†ä¸“ç”¨ AI agent åœ¨èº«ä»½ç®¡ç† (Identity Administration) é¢†åŸŸèƒ½æ˜¾è‘—å¢å¼ºæ“ä½œçš„æ•ˆç‡ä¸ç²¾ç¡®åº¦ã€‚",
      "categories": [
        "econ.GN",
        "cs.AI"
      ],
      "primary_category": "econ.GN",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13865v1",
      "published_date": "2025-11-17 19:33:03 UTC",
      "updated_date": "2025-11-17 19:33:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:49:27.145803+00:00"
    },
    {
      "arxiv_id": "2511.14807v1",
      "title": "Fully Differentiable dMRI Streamline Propagation in PyTorch",
      "title_zh": "PyTorch ä¸­å…¨å¯å¾®çš„ dMRI æµçº¿ä¼ æ’­",
      "authors": [
        "Jongyeon Yoon",
        "Elyssa M. McMaster",
        "Michael E. Kim",
        "Gaurav Rudravaram",
        "Kurt G. Schilling",
        "Bennett A. Landman",
        "Daniel Moyer"
      ],
      "abstract": "Diffusion MRI (dMRI) provides a distinctive means to probe the microstructural architecture of living tissue, facilitating applications such as brain connectivity analysis, modeling across multiple conditions, and the estimation of macrostructural features. Tractography, which emerged in the final years of the 20th century and accelerated in the early 21st century, is a technique for visualizing white matter pathways in the brain using dMRI. Most diffusion tractography methods rely on procedural streamline propagators or global energy minimization methods. Although recent advancements in deep learning have enabled tasks that were previously challenging, existing tractography approaches are often non-differentiable, limiting their integration in end-to-end learning frameworks. While progress has been made in representing streamlines in differentiable frameworks, no existing method offers fully differentiable propagation. In this work, we propose a fully differentiable solution that retains numerical fidelity with a leading streamline algorithm. The key is that our PyTorch-engineered streamline propagator has no components that block gradient flow, making it fully differentiable. We show that our method matches standard propagators while remaining differentiable. By translating streamline propagation into a differentiable PyTorch framework, we enable deeper integration of tractography into deep learning workflows, laying the foundation for a new category of macrostructural reasoning that is not only computationally robust but also scientifically rigorous.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼¥æ•£ç£å…±æŒ¯æˆåƒ(dMRI)çº¤ç»´è¿½è¸ª(Tractography)ä¼ ç»Ÿç®—æ³•å› ä¸å¯å¾®è€Œéš¾ä»¥é›†æˆè‡³ç«¯åˆ°ç«¯æ·±åº¦å­¦ä¹ æ¡†æ¶çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åœ¨PyTorchä¸­å®ç°çš„å®Œå…¨å¯å¾®(fully differentiable)æµçº¿ä¼ æ’­æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºæ„å»ºäº†ä¸€ä¸ªæ²¡æœ‰æ¢¯åº¦æµ(gradient flow)é˜»æ–­ç»„ä»¶çš„ä¼ æ’­å™¨ï¼Œä»è€Œåœ¨ä¿æŒä¸é¢†å…ˆæµçº¿ç®—æ³•æ•°å€¼ä¿çœŸåº¦ä¸€è‡´çš„åŒæ—¶ï¼Œå®ç°äº†å…¨è¿‡ç¨‹çš„å¯å¾®æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…èƒ½åŒ¹é…æ ‡å‡†ä¼ æ’­å™¨çš„æ€§èƒ½ï¼Œè¿˜å…è®¸å°†çº¤ç»´è¿½è¸ªæ›´æ·±å…¥åœ°æ•´åˆè¿›æ·±åº¦å­¦ä¹ å·¥ä½œæµä¸­ã€‚è¿™ä¸€è¿›å±•ä¸ºå®è§‚ç»“æ„æ¨ç†(macrostructural reasoning)å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œä¸ºæ„å»ºæ—¢å…·æœ‰è®¡ç®—é²æ£’æ€§åˆå…·å¤‡ç§‘å­¦ä¸¥è°¨æ€§çš„ç¥ç»ç§‘å­¦åˆ†ææ¨¡å‹å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "9 pages, 4 figures. Accepted to SPIE Medical Imaging 2026: Image Processing",
      "pdf_url": "https://arxiv.org/pdf/2511.14807v1",
      "published_date": "2025-11-17 19:31:47 UTC",
      "updated_date": "2025-11-17 19:31:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:49:22.943242+00:00"
    },
    {
      "arxiv_id": "2511.14806v1",
      "title": "MergeDNA: Context-aware Genome Modeling with Dynamic Tokenization through Token Merging",
      "title_zh": "MergeDNAï¼šé€šè¿‡è¯å…ƒåˆå¹¶å®ç°åŠ¨æ€è¯å…ƒåŒ–çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥åŸºå› ç»„å»ºæ¨¡",
      "authors": [
        "Siyuan Li",
        "Kai Yu",
        "Anna Wang",
        "Zicheng Liu",
        "Chang Yu",
        "Jingbo Zhou",
        "Qirong Yang",
        "Yucheng Guo",
        "Xiaoming Zhang",
        "Stan Z. Li"
      ],
      "abstract": "Modeling genomic sequences faces two unsolved challenges: the information density varies widely across different regions, while there is no clearly defined minimum vocabulary unit. Relying on either four primitive bases or independently designed DNA tokenizers, existing approaches with naive masked language modeling pre-training often fail to adapt to the varying complexities of genomic sequences. Leveraging Token Merging techniques, this paper introduces a hierarchical architecture that jointly optimizes a dynamic genomic tokenizer and latent Transformers with context-aware pre-training tasks. As for network structures, the tokenization module automatically chunks adjacent bases into words by stacking multiple layers of the differentiable token merging blocks with local-window constraints, then a Latent Encoder captures the global context of these merged words by full-attention blocks. Symmetrically employing a Latent Decoder and a Local Decoder, MergeDNA learns with two pre-training tasks: Merged Token Reconstruction simultaneously trains the dynamic tokenization module and adaptively filters important tokens, while Adaptive Masked Token Modeling learns to predict these filtered tokens to capture informative contents. Extensive experiments show that MergeDNA achieves superior performance on three popular DNA benchmarks and several multi-omics tasks with fine-tuning or zero-shot evaluation, outperforming typical tokenization methods and large-scale DNA foundation models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MergeDNAï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨ Token Merging æŠ€æœ¯å®ç°åŠ¨æ€åˆ†è¯çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥åŸºå› ç»„å»ºæ¨¡æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŸºå› ç»„åºåˆ—ä¿¡æ¯å¯†åº¦ä¸å‡ä¸”ç¼ºä¹å¤©ç„¶è¯æ±‡å•ä½çš„éš¾é¢˜ã€‚è¯¥ç ”ç©¶è®¾è®¡äº†ä¸€ç§å±‚æ¬¡åŒ–æ¶æ„ï¼Œé€šè¿‡å åŠ å¯å¾®åˆ†çš„ä»¤ç‰Œåˆå¹¶æ¨¡å—å°†ç›¸é‚»ç¢±åŸºè‡ªåŠ¨èšåˆæˆè¯ï¼Œå¹¶åˆ©ç”¨ Latent Encoder æ•æ‰è¿™äº›åˆå¹¶è¯çš„å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æ¨¡å‹é€šè¿‡ç»“åˆ Merged Token Reconstruction å’Œ Adaptive Masked Token Modeling ä¸¤é¡¹é¢„è®­ç»ƒä»»åŠ¡ï¼Œå®ç°äº†åˆ†è¯æ¨¡å—çš„åŠ¨æ€ä¼˜åŒ–ä»¥åŠå¯¹å…³é”®ä¿¡æ¯çš„è‡ªé€‚åº”æå–ã€‚å®éªŒè¯æ˜ï¼ŒMergeDNA åœ¨ä¸‰é¡¹ä¸»æµ DNA åŸºå‡†æµ‹è¯•åŠå¤šç»„å­¦ä»»åŠ¡ä¸­å‡å–å¾—äº†å“è¶Šæ€§èƒ½ï¼Œåœ¨å¾®è°ƒå’Œé›¶æ ·æœ¬ï¼ˆzero-shotï¼‰è¯„ä¼°ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿåˆ†è¯æ–¹æ³•å’Œç°æœ‰çš„è§„æ¨¡åŒ– DNA foundation modelsã€‚",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.GN",
      "comment": "AAAI 2026 (Oral Presentation) Preprint",
      "pdf_url": "https://arxiv.org/pdf/2511.14806v1",
      "published_date": "2025-11-17 19:27:41 UTC",
      "updated_date": "2025-11-17 19:27:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:49:45.539060+00:00"
    },
    {
      "arxiv_id": "2511.13860v1",
      "title": "Randomized Controlled Trials for Phishing Triage Agent",
      "title_zh": "ç½‘ç»œé’“é±¼åˆ†ç±»å¤„ç½®æ™ºèƒ½ä½“çš„éšæœºå¯¹ç…§è¯•éªŒ",
      "authors": [
        "James Bono"
      ],
      "abstract": "Security operations centers (SOCs) face a persistent challenge: efficiently triaging a high volume of user-reported phishing emails while maintaining robust protection against threats. This paper presents the first randomized controlled trial (RCT) evaluating the impact of a domain-specific AI agent - the Microsoft Security Copilot Phishing Triage Agent - on analyst productivity and accuracy. Our results demonstrate that agent-augmented analysts achieved up to 6.5 times as many true positives per analyst minute and a 77% improvement in verdict accuracy compared to a control group. The agent's queue prioritization and verdict explanations were both significant drivers of efficiency. Behavioral analysis revealed that agent-augmented analysts reallocated their attention, spending 53% more time on malicious emails, and were not prone to rubber-stamping the agent's malicious verdicts. These findings offer actionable insights for SOC leaders considering AI adoption, including the potential for agents to fundamentally change the optimal allocation of SOC resources.",
      "tldr_zh": "æœ¬ç ”ç©¶é€šè¿‡éšæœºå¯¹ç…§è¯•éªŒ(RCT)è¯„ä¼°äº†é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„ AI æ™ºèƒ½ä½“â€”â€”Microsoft Security Copilot Phishing Triage Agent åœ¨æå‡å®‰å…¨è¿è¥ä¸­å¿ƒ(SOC)åˆ†æå¸ˆé’“é±¼é‚®ä»¶åˆ†æ‹£æ•ˆç‡ä¸å‡†ç¡®æ€§æ–¹é¢çš„è¡¨ç°ã€‚è¿™æ˜¯é¦–ä¸ªè¯„ä¼°æ­¤ç±» AI æ™ºèƒ½ä½“å¯¹åˆ†æå¸ˆç”Ÿäº§åŠ›å½±å“çš„ RCT ç ”ç©¶ï¼Œæ—¨åœ¨è§£å†³ SOC é¢ä¸´çš„é«˜ååé‡é’“é±¼é‚®ä»¶å¤„ç†éš¾é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨è¯¥æ™ºèƒ½ä½“è¾…åŠ©çš„åˆ†æå¸ˆåœ¨æ¯åˆ†é’Ÿå‘ç°çš„ True Positives æ•°é‡ä¸Šæ¯”å¯¹ç…§ç»„æé«˜äº† 6.5 å€ï¼Œä¸”åˆ¤å®šå‡†ç¡®ç‡æå‡äº† 77%ã€‚ç ”ç©¶å‘ç°æ•ˆç‡æå‡çš„ä¸»è¦é©±åŠ¨å› ç´ æ˜¯æ™ºèƒ½ä½“çš„é˜Ÿåˆ—ä¼˜å…ˆçº§æ’åº(Queue Prioritization)å’Œåˆ¤å®šè§£é‡Š(Verdict Explanations)ã€‚è¡Œä¸ºåˆ†æè¿›ä¸€æ­¥è¡¨æ˜ï¼Œå—è¾…åŠ©çš„åˆ†æå¸ˆå¤„ç†æ¶æ„é‚®ä»¶çš„æ—¶é—´å¢åŠ äº† 53%ï¼Œä¸”å¹¶æœªå‡ºç°å¯¹æ™ºèƒ½ä½“åˆ¤å®šç»“æœç›²ç›®é‡‡ä¿¡çš„å€¾å‘ã€‚è¯¥å‘ç°ä¸º SOC é¢†å¯¼è€…é‡‡çº³ AI æŠ€æœ¯æä¾›äº†å®è¯æ”¯æŒï¼Œå¹¶æ­ç¤ºäº† AI æ™ºèƒ½ä½“åœ¨ä»æ ¹æœ¬ä¸Šæ”¹å˜ SOC èµ„æºä¼˜åŒ–åˆ†é…æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "econ.GN",
        "cs.AI"
      ],
      "primary_category": "econ.GN",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13860v1",
      "published_date": "2025-11-17 19:23:08 UTC",
      "updated_date": "2025-11-17 19:23:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:49:53.432888+00:00"
    },
    {
      "arxiv_id": "2511.16698v1",
      "title": "Hierarchical Retrieval with Out-Of-Vocabulary Queries: A Case Study on SNOMED CT",
      "title_zh": "é¢å‘è¯è¡¨å¤–æŸ¥è¯¢çš„å±‚æ¬¡åŒ–æ£€ç´¢ï¼šä»¥ SNOMED CT ä¸ºä¾‹çš„æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Jonathon Dilworth",
        "Hui Yang",
        "Jiaoyan Chen",
        "Yongsheng Gao"
      ],
      "abstract": "SNOMED CT is a biomedical ontology with a hierarchical representation of large-scale concepts. Knowledge retrieval in SNOMED CT is critical for its application, but often proves challenging due to language ambiguity, synonyms, polysemies and so on. This problem is exacerbated when the queries are out-of-vocabulary (OOV), i.e., having no equivalent matchings in the ontology. In this work, we focus on the problem of hierarchical concept retrieval from SNOMED CT with OOV queries, and propose an approach based on language model-based ontology embeddings. For evaluation, we construct OOV queries annotated against SNOMED CT concepts, testing the retrieval of the most direct subsumers and their less relevant ancestors. We find that our method outperforms the baselines including SBERT and two lexical matching methods. While evaluated against SNOMED CT, the approach is generalisable and can be extended to other ontologies. We release code, tools, and evaluation datasets at https://github.com/jonathondilworth/HR-OOV.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ SNOMED CT ç”Ÿç‰©åŒ»å­¦æœ¬ä½“åœ¨çŸ¥è¯†æ£€ç´¢ä¸­é¢ä¸´çš„è¯­è¨€æ­§ä¹‰åŠè¯è¡¨å¤– (Out-Of-Vocabulary, OOV) æŸ¥è¯¢åŒ¹é…éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè¯­è¨€æ¨¡å‹ (Language Model) çš„æœ¬ä½“åµŒå…¥ (Ontology Embeddings) å±‚æ¬¡åŒ–æ£€ç´¢æ–¹æ³•ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡æ„å»ºä¸“é—¨çš„ OOV æŸ¥è¯¢æ•°æ®é›†ï¼Œè¯„ä¼°äº†è¯¥æ–¹æ³•åœ¨æ£€ç´¢æœ€ç›´æ¥ä¸Šä½æ¦‚å¿µåŠå…¶ç›¸å…³ç¥–å…ˆæ¦‚å¿µæ–¹é¢çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç† OOV æŸ¥è¯¢æ—¶çš„è¡¨ç°ä¼˜äº SBERT ä»¥åŠä¼ ç»Ÿçš„è¯æ³•åŒ¹é… (Lexical Matching) åŸºå‡†æ¨¡å‹ã€‚å°½ç®¡æœ¬é¡¹å·¥ä½œä»¥ SNOMED CT ä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼Œä½†å…¶æå‡ºçš„æ¡†æ¶å…·æœ‰é€šç”¨æ€§ï¼Œå¯æ‰©å±•è‡³å…¶ä»–å¤æ‚çš„æœ¬ä½“ç³»ç»Ÿï¼Œä¸”ç›¸å…³ä»£ç ã€å·¥å…·å’Œè¯„ä¼°æ•°æ®é›†å·²å…¬å¼€å‘å¸ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "5 pages, 3 figures, 3 tables, submission to The Web Conference 2026 (WWW'26), Dubai, UAE",
      "pdf_url": "https://arxiv.org/pdf/2511.16698v1",
      "published_date": "2025-11-17 19:18:10 UTC",
      "updated_date": "2025-11-17 19:18:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:49:49.435695+00:00"
    },
    {
      "arxiv_id": "2511.17596v1",
      "title": "Reconstruction-Driven Multimodal Representation Learning for Automated Media Understanding",
      "title_zh": "é¢å‘è‡ªåŠ¨åŒ–åª’ä½“ç†è§£çš„é‡æ„é©±åŠ¨å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ",
      "authors": [
        "Yassir Benhammou",
        "Suman Kalyan",
        "Sujay Kumar"
      ],
      "abstract": "Broadcast and media organizations increasingly rely on artificial intelligence to automate the labor-intensive processes of content indexing, tagging, and metadata generation. However, existing AI systems typically operate on a single modality-such as video, audio, or text-limiting their understanding of complex, cross-modal relationships in broadcast material. In this work, we propose a Multimodal Autoencoder (MMAE) that learns unified representations across text, audio, and visual data, enabling end-to-end automation of metadata extraction and semantic clustering. The model is trained on the recently introduced LUMA dataset, a fully aligned benchmark of multimodal triplets representative of real-world media content. By minimizing joint reconstruction losses across modalities, the MMAE discovers modality-invariant semantic structures without relying on large paired or contrastive datasets. We demonstrate significant improvements in clustering and alignment metrics (Silhouette, ARI, NMI) compared to linear baselines, indicating that reconstruction-based multimodal embeddings can serve as a foundation for scalable metadata generation and cross-modal retrieval in broadcast archives. These results highlight the potential of reconstruction-driven multimodal learning to enhance automation, searchability, and content management efficiency in modern broadcast workflows.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¹¿æ’­åª’ä½“æœºæ„åœ¨è‡ªåŠ¨ç´¢å¼•ä¸­è¿‡åº¦ä¾èµ–å•æ¨¡æ€ AI çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§å¤šæ¨¡æ€è‡ªåŠ¨ç¼–ç å™¨ (Multimodal Autoencoder, MMAE)ï¼Œæ—¨åœ¨å®ç°æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†è§‰æ•°æ®çš„ç»Ÿä¸€è¡¨å¾å­¦ä¹ ã€‚è¯¥æ¨¡å‹é€šè¿‡åœ¨ LUMA æ•°æ®é›†ä¸Šæœ€å°åŒ–è·¨æ¨¡æ€çš„è”åˆé‡æ„æŸå¤± (joint reconstruction losses)ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¾èµ–å¤§è§„æ¨¡å¯¹æ¯”å­¦ä¹ æ•°æ®é›†çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆå‘ç°æ¨¡æ€ä¸å˜çš„è¯­ä¹‰ç»“æ„ã€‚å®éªŒè¯æ˜ï¼ŒMMAE åœ¨èšç±»å’Œå¯¹é½æŒ‡æ ‡ï¼ˆå¦‚ Silhouette, ARI å’Œ NMIï¼‰ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçº¿æ€§åŸºçº¿æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶åœ¨ç«¯åˆ°ç«¯å…ƒæ•°æ®æå–å’Œè¯­ä¹‰èšç±»æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚è¿™ç§åŸºäºé‡æ„çš„å¤šæ¨¡æ€åµŒå…¥æŠ€æœ¯ä¸ºå¯æ‰©å±•çš„å…ƒæ•°æ®ç”Ÿæˆå’Œè·¨æ¨¡æ€æ£€ç´¢æä¾›äº†æ ¸å¿ƒæ”¯æ’‘ï¼Œæ˜¾è‘—å¢å¼ºäº†ç°ä»£å¹¿æ’­å·¥ä½œæµä¸­çš„è‡ªåŠ¨åŒ–ç¨‹åº¦ã€å†…å®¹ç®¡ç†æ•ˆç‡åŠå¯æœç´¢æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 5 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.17596v1",
      "published_date": "2025-11-17 19:13:51 UTC",
      "updated_date": "2025-11-17 19:13:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:50:22.634947+00:00"
    },
    {
      "arxiv_id": "2511.13852v1",
      "title": "Causal computations in Semi Markovian Structural Causal Models using divide and conquer",
      "title_zh": "åŸºäºåˆ†æ²»æ³•çš„åŠé©¬å°”å¯å¤«ç»“æ„å› æœæ¨¡å‹å› æœè®¡ç®—",
      "authors": [
        "Anna Rodum BjÃ¸ru",
        "Rafael CabaÃ±as",
        "Helge Langseth",
        "Antonio SalmerÃ³n"
      ],
      "abstract": "Recently, BjÃ¸ru et al. proposed a novel divide-and-conquer algorithm for bounding counterfactual probabilities in structural causal models (SCMs). They assumed that the SCMs were learned from purely observational data, leading to an imprecise characterization of the marginal distributions of exogenous variables. Their method leveraged the canonical representation of structural equations to decompose a general SCM with high-cardinality exogenous variables into a set of sub-models with low-cardinality exogenous variables. These sub-models had precise marginals over the exogenous variables and therefore admitted efficient exact inference. The aggregated results were used to bound counterfactual probabilities in the original model. The approach was developed for Markovian models, where each exogenous variable affects only a single endogenous variable. In this paper, we investigate extending the methodology to \\textit{semi-Markovian} SCMs, where exogenous variables may influence multiple endogenous variables. Such models are capable of representing confounding relationships that Markovian models cannot. We illustrate the challenges of this extension using a minimal example, which motivates a set of alternative solution strategies. These strategies are evaluated both theoretically and through a computational study.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨åŠé©¬å°”å¯å¤«ç»“æ„å› æœæ¨¡å‹ (Semi-Markovian Structural Causal Models, SCMs) ä¸­åˆ©ç”¨åˆ†æ²»ç®—æ³• (divide-and-conquer) è¿›è¡Œå› æœè®¡ç®—çš„é—®é¢˜ã€‚æ­¤å‰ç ”ç©¶æå‡ºçš„åˆ†æ²»æ¡†æ¶ä¸»è¦é’ˆå¯¹é©¬å°”å¯å¤«æ¨¡å‹è®¾è®¡ï¼Œé€šè¿‡å°†å…·æœ‰é«˜åŸºæ•°å¤–ç”Ÿå˜é‡ (exogenous variables) çš„é€šç”¨æ¨¡å‹åˆ†è§£ä¸ºä½åŸºæ•°çš„å­æ¨¡å‹é›†åˆï¼Œä»¥å®ç°åäº‹å®æ¦‚ç‡çš„æœ‰æ•ˆæ¨æ–­ã€‚ç”±äºåŸæ–¹æ³•æ— æ³•ç›´æ¥å¤„ç†å¤–ç”Ÿå˜é‡å½±å“å¤šä¸ªå†…ç”Ÿå˜é‡ (endogenous variables) çš„æ··æ‚å…³ç³»ï¼Œæœ¬æ–‡é‡ç‚¹ç ”ç©¶äº†å°†è¯¥æ–¹æ³•æ‰©å±•è‡³ Semi-Markovian SCMs çš„å¯è¡Œæ€§ã€‚ä½œè€…é€šè¿‡ä¸€ä¸ªæå°ç¤ºä¾‹ (minimal example) æ­ç¤ºäº†æ‰©å±•è¿‡ç¨‹ä¸­é¢ä¸´çš„ç†è®ºä¸è®¡ç®—æŒ‘æˆ˜ï¼Œå¹¶æ®æ­¤æå‡ºäº†ä¸€ç³»åˆ—æ›¿ä»£è§£å†³æ–¹æ¡ˆç­–ç•¥ã€‚æœ€åï¼Œè¯¥ç ”ç©¶é€šè¿‡ç†è®ºåˆ†æå’Œè®¡ç®—å®éªŒå¯¹è¿™äº›ç­–ç•¥è¿›è¡Œäº†è¯„ä¼°ï¼Œä¸ºåœ¨åŒ…å«æ··æ‚å› ç´ çš„å¤æ‚å› æœæ¨¡å‹ä¸­è¿›è¡Œç²¾ç¡®æ¨ç†å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "36 pages, 7 figures, 1 appendix",
      "pdf_url": "https://arxiv.org/pdf/2511.13852v1",
      "published_date": "2025-11-17 19:08:53 UTC",
      "updated_date": "2025-11-17 19:08:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:49:57.739829+00:00"
    },
    {
      "arxiv_id": "2511.21708v1",
      "title": "Lost in the Pipeline: How Well Do Large Language Models Handle Data Preparation?",
      "title_zh": "è¿·å¤±åœ¨æµæ°´çº¿ï¼šå¤§è¯­è¨€æ¨¡å‹å¤„ç†æ•°æ®å‡†å¤‡çš„èƒ½åŠ›æ¢ç©¶",
      "authors": [
        "Matteo Spreafico",
        "Ludovica Tassini",
        "Camilla Sancricca",
        "Cinzia Cappiello"
      ],
      "abstract": "Large language models have recently demonstrated their exceptional capabilities in supporting and automating various tasks. Among the tasks worth exploring for testing large language model capabilities, we considered data preparation, a critical yet often labor-intensive step in data-driven processes. This paper investigates whether large language models can effectively support users in selecting and automating data preparation tasks. To this aim, we considered both general-purpose and fine-tuned tabular large language models. We prompted these models with poor-quality datasets and measured their ability to perform tasks such as data profiling and cleaning. We also compare the support provided by large language models with that offered by traditional data preparation tools. To evaluate the capabilities of large language models, we developed a custom-designed quality model that has been validated through a user study to gain insights into practitioners' expectations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Large Language Models (LLMs) åœ¨è‡ªåŠ¨åŒ– Data Preparation è¿™ä¸€åŠ³åŠ¨å¯†é›†å‹ç¯èŠ‚ä¸­çš„æœ‰æ•ˆæ€§ï¼Œæ—¨åœ¨è¯„ä¼°å…¶åœ¨æ•°æ®é©±åŠ¨æµç¨‹ä¸­çš„æ”¯æŒèƒ½åŠ›ã€‚ä½œè€…å¯¹æ¯”äº†é€šç”¨ LLMs ä¸ç»è¿‡å¾®è°ƒçš„ Tabular LLMs åœ¨å¤„ç†ä½è´¨é‡æ•°æ®é›†æ—¶çš„è¡¨ç°ï¼Œé‡ç‚¹è€ƒå¯Ÿäº† Data Profiling å’Œ Data Cleaning ç­‰æ ¸å¿ƒä»»åŠ¡ã€‚ç ”ç©¶è¿˜å°† LLMs çš„å¤„ç†æ•ˆæœä¸ä¼ ç»Ÿ Data Preparation å·¥å…·è¿›è¡Œäº†æ·±å…¥å¯¹æ¯”ã€‚ä¸ºäº†é‡åŒ–è¯„ä¼°ï¼Œè¯¥å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªä¸“é—¨çš„è´¨é‡æ¨¡å‹ï¼Œå¹¶ç»“åˆ User Study éªŒè¯äº†ç›¸å…³ç»“æœï¼Œä»¥æ­ç¤ºå®é™…ä»ä¸šè€…å¯¹æ¨¡å‹èƒ½åŠ›çš„çœŸå®æœŸæœ›ã€‚è¯¥å·¥ä½œä¸ºç†è§£ LLMs åœ¨å¤æ‚æ•°æ®æµæ°´çº¿ä¸­çš„åº”ç”¨æ½œåŠ›å’Œå±€é™æ€§æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21708v1",
      "published_date": "2025-11-17 19:06:22 UTC",
      "updated_date": "2025-11-17 19:06:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:50:03.337105+00:00"
    },
    {
      "arxiv_id": "2511.13825v1",
      "title": "When AI Does Science: Evaluating the Autonomous AI Scientist KOSMOS in Radiation Biology",
      "title_zh": "å½“ AI æŠ•èº«ç§‘å­¦ç ”ç©¶ï¼šæ”¾å°„ç”Ÿç‰©å­¦é¢†åŸŸè‡ªä¸» AI ç§‘å­¦å®¶ KOSMOS çš„è¯„ä¼°ç ”ç©¶",
      "authors": [
        "Humza Nusrat",
        "Omar Nusrat"
      ],
      "abstract": "Agentic AI \"scientists\" now use language models to search the literature, run analyses, and generate hypotheses. We evaluate KOSMOS, an autonomous AI scientist, on three problems in radiation biology using simple random-gene null benchmarks. Hypothesis 1: baseline DNA damage response (DDR) capacity across cell lines predicts the p53 transcriptional response after irradiation (GSE30240). Hypothesis 2: baseline expression of OGT and CDO1 predicts the strength of repressed and induced radiation-response modules in breast cancer cells (GSE59732). Hypothesis 3: a 12-gene expression signature predicts biochemical recurrence-free survival after prostate radiotherapy plus androgen deprivation therapy (GSE116918). The DDR-p53 hypothesis was not supported: DDR score and p53 response were weakly negatively correlated (Spearman rho = -0.40, p = 0.76), indistinguishable from random five-gene scores. OGT showed only a weak association (r = 0.23, p = 0.34), whereas CDO1 was a clear outlier (r = 0.70, empirical p = 0.0039). The 12-gene signature achieved a concordance index of 0.61 (p = 0.017) but a non-unique effect size. Overall, KOSMOS produced one well-supported discovery, one plausible but uncertain result, and one false hypothesis, illustrating that AI scientists can generate useful ideas but require rigorous auditing against appropriate null models.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†åä¸ºKOSMOSçš„è‡ªä¸»AIç§‘å­¦å®¶(AI Scientist)åœ¨è¾å°„ç”Ÿç‰©å­¦(Radiation Biology)é¢†åŸŸçš„å®é™…è¡¨ç°ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨éšæœºåŸºå› é›¶åŸºå‡†(random-gene null benchmarks)å¯¹KOSMOSæå‡ºçš„ä¸‰ä¸ªç§‘å­¦å‡è®¾è¿›è¡Œäº†ç³»ç»Ÿæµ‹è¯•ï¼Œæ¶µç›–äº†DNAæŸä¼¤å“åº”(DDR)é¢„æµ‹ã€ç‰¹å®šä»£è°¢åŸºå› ä¸è¾å°„å“åº”å…³è”ä»¥åŠ12åŸºå› è¡¨è¾¾ç‰¹å¾(12-gene expression signature)å¯¹ç”Ÿå­˜ç‡çš„é¢„æµ‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå…³äºDDRä¸p53è½¬å½•å“åº”çš„å‡è®¾å¹¶æœªå¾—åˆ°æ”¯æŒï¼Œå…¶è¡¨ç°ä¸éšæœºåŸºå› å¾—åˆ†æ— å¼‚ã€‚åœ¨é’ˆå¯¹ breast cancer ç»†èƒçš„ç ”ç©¶ä¸­ï¼ŒOGT ä»…è¡¨ç°å‡ºå¾®å¼±å…³è”ï¼Œè€Œ CDO1 åˆ™è¢«è¯å®ä¸ºæ˜¾è‘—çš„é¢„æµ‹å› å­ã€‚è™½ç„¶12åŸºå› ç‰¹å¾åœ¨é¢„æµ‹å‰åˆ—è…ºç™Œæ”¾ç–—é¢„åæ—¶å–å¾—äº†ä¸€å®šçš„ä¸€è‡´æ€§æŒ‡æ•°ï¼Œä½†å…¶æ•ˆåº”é‡å¹¶ä¸å…·æœ‰å”¯ä¸€æ€§ã€‚æ€»ä½“è€Œè¨€ï¼ŒKOSMOSäº§ç”Ÿäº†ä¸€ä¸ªå¾—åˆ°æœ‰åŠ›æ”¯æŒçš„å‘ç°ã€ä¸€ä¸ªä¸ç¡®å®šçš„ç»“æœå’Œä¸€ä¸ªé”™è¯¯å‡è®¾ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜ï¼Œè™½ç„¶è‡ªä¸»AIç§‘å­¦å®¶èƒ½å¤Ÿç”Ÿæˆæœ‰ä»·å€¼çš„ç§‘ç ”æ„æƒ³ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­å¿…é¡»é€šè¿‡ä¸¥æ ¼çš„é›¶æ¨¡å‹(null models)å®¡è®¡æ¥ç¡®ä¿å…¶å‘ç°çš„ç§‘å­¦ä¸¥è°¨æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages, 3 figures, preprint",
      "pdf_url": "https://arxiv.org/pdf/2511.13825v1",
      "published_date": "2025-11-17 19:00:03 UTC",
      "updated_date": "2025-11-17 19:00:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:50:04.034656+00:00"
    },
    {
      "arxiv_id": "2511.13719v3",
      "title": "Scaling Spatial Intelligence with Multimodal Foundation Models",
      "title_zh": "åˆ©ç”¨å¤šæ¨¡æ€åŸºåº§æ¨¡å‹æ‰©å±•ç©ºé—´æ™ºèƒ½",
      "authors": [
        "Zhongang Cai",
        "Ruisi Wang",
        "Chenyang Gu",
        "Fanyi Pu",
        "Junxiang Xu",
        "Yubo Wang",
        "Wanqi Yin",
        "Zhitao Yang",
        "Chen Wei",
        "Qingping Sun",
        "Tongxi Zhou",
        "Jiaqi Li",
        "Hui En Pang",
        "Oscar Qian",
        "Yukun Wei",
        "Zhiqian Lin",
        "Xuanke Shi",
        "Kewang Deng",
        "Xiaoyang Han",
        "Zukai Chen",
        "Xiangyu Fan",
        "Hanming Deng",
        "Lewei Lu",
        "Liang Pan",
        "Bo Li",
        "Ziwei Liu",
        "Quan Wang",
        "Dahua Lin",
        "Lei Yang"
      ],
      "abstract": "Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡æ‰©å±•å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼ˆMultimodal Foundation Modelsï¼‰æ¥åŸ¹å…»ç©ºé—´æ™ºèƒ½ï¼ˆSpatial Intelligenceï¼‰ï¼Œå¹¶æ¨å‡ºäº†åŸºäº Qwen3-VLã€InternVL3 å’Œ Bagel æ„å»ºçš„ SenseNova-SI ç³»åˆ—æ¨¡å‹ã€‚è¯¥ç ”ç©¶çš„æ ¸å¿ƒè´¡çŒ®æ˜¯ç³»ç»Ÿæ€§åœ°æ„å»ºäº†åŒ…å« 800 ä¸‡ä¸ªå¤šæ ·åŒ–æ ·æœ¬çš„ SenseNova-SI-8M æ•°æ®é›†ï¼Œæ¶µç›–äº†ä¸¥è°¨çš„ç©ºé—´èƒ½åŠ›åˆ†ç±»ä½“ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSenseNova-SI åœ¨ VSI-Benchã€MMSIã€MindCubeã€ViewSpatial å’Œ SITE ç­‰å¤šä¸ªç©ºé—´æ™ºèƒ½åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶åœ¨ MMBench-En ç­‰é€šç”¨å¤šæ¨¡æ€ç†è§£ä»»åŠ¡ä¸­ä¿æŒäº†å¼ºåŠ²è¡¨ç°ã€‚æ­¤å¤–ï¼Œè®ºæ–‡æ·±å…¥åˆ†æäº†æ•°æ®ç¼©æ”¾ï¼ˆData Scalingï¼‰çš„å½±å“ã€æ¶Œç°çš„æ³›åŒ–èƒ½åŠ›ï¼ˆEmergent Generalizationï¼‰ä»¥åŠè¿‡æ‹Ÿåˆä¸è¯­è¨€å¿«æ·æ–¹å¼ï¼ˆLanguage Shortcutsï¼‰çš„é£é™©ã€‚ç ”ç©¶è¿˜å¯¹ç©ºé—´é“¾å¼æ€ç»´æ¨ç†ï¼ˆSpatial Chain-of-Thoughtï¼‰è¿›è¡Œäº†åˆæ­¥æ¢è®¨ï¼Œå¹¶éªŒè¯äº†æ¨¡å‹çš„ä¸‹æ¸¸åº”ç”¨æ½œåŠ›ã€‚ç›®å‰ SenseNova-SI æ¨¡å‹å·²å…¬å¼€å‘å¸ƒï¼Œæ—¨åœ¨æ¨åŠ¨å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨ç©ºé—´ç†è§£é¢†åŸŸçš„ç ”ç©¶ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Codebase: https://github.com/OpenSenseNova/SenseNova-SI ; Models: https://huggingface.co/collections/sensenova/sensenova-si",
      "pdf_url": "https://arxiv.org/pdf/2511.13719v3",
      "published_date": "2025-11-17 18:59:33 UTC",
      "updated_date": "2025-12-30 13:07:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:50:13.532809+00:00"
    },
    {
      "arxiv_id": "2511.13714v1",
      "title": "UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity",
      "title_zh": "UnSAMv2ï¼šåˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ å®ç°ä»»æ„ç²’åº¦çš„ä¸‡ç‰©åˆ†å‰²",
      "authors": [
        "Junwei Yu",
        "Trevor Darrell",
        "XuDong Wang"
      ],
      "abstract": "The Segment Anything Model (SAM) family has become a widely adopted vision foundation model, but its ability to control segmentation granularity remains limited. Users often need to refine results manually - by adding more prompts or selecting from pre-generated masks - to achieve the desired level of detail. This process can be ambiguous, as the same prompt may correspond to several plausible masks, and collecting dense annotations across all granularities is prohibitively expensive, making supervised solutions infeasible. To address this limitation, we introduce UnSAMv2, which enables segment anything at any granularity without human annotations. UnSAMv2 extends the divide-and-conquer strategy of UnSAM by discovering abundant mask-granularity pairs and introducing a novel granularity control embedding that enables precise, continuous control over segmentation scale. Remarkably, with only $6$K unlabeled images and $0.02\\%$ additional parameters, UnSAMv2 substantially enhances SAM-2, achieving segment anything at any granularity across interactive, whole-image, and video segmentation tasks. Evaluated on over $11$ benchmarks, UnSAMv2 improves $\\text{NoC}_{90}$ (5.69 $\\rightarrow$ 4.75), 1-IoU (58.0 $\\rightarrow$ 73.1), and $\\text{AR}_{1000}$ (49.6 $\\rightarrow$ 68.3), showing that small amounts of unlabeled data with a granularity-aware self-supervised learning method can unlock the potential of vision foundation models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†UnSAMv2ï¼Œæ—¨åœ¨è§£å†³Segment Anything Model (SAM) ç³»åˆ—åœ¨åˆ†å‰²ç²’åº¦æ§åˆ¶æ–¹é¢çš„å±€é™æ€§ä»¥åŠå¤šç²’åº¦æ ‡æ³¨æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚UnSAMv2 å¼•å…¥äº†ä¸€ç§è‡ªç›‘ç£å­¦ä¹ (Self-Supervised Learning)æ–¹æ³•ï¼Œé€šè¿‡æ‰©å±•UnSAMçš„åˆ†æ²»ç­–ç•¥å¹¶å‘ç°ä¸°å¯Œçš„â€œæ©ç -ç²’åº¦å¯¹â€(mask-granularity pairs)ï¼Œå®ç°äº†æ— éœ€äººå·¥æ ‡æ³¨çš„ä»»æ„ç²’åº¦åˆ†å‰²ã€‚å…¶æ ¸å¿ƒåœ¨äºå¼•å…¥äº†å…¨æ–°çš„ç²’åº¦æ§åˆ¶åµŒå…¥(granularity control embedding)ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿå¯¹åˆ†å‰²å°ºåº¦è¿›è¡Œç²¾ç¡®ä¸”è¿ç»­çš„æ§åˆ¶ã€‚åœ¨ä»…ä½¿ç”¨6000å¼ æ— æ ‡æ³¨å›¾åƒå’Œ0.02%é¢å¤–å‚æ•°çš„æƒ…å†µä¸‹ï¼ŒUnSAMv2 æ˜¾è‘—å¢å¼ºäº†SAM-2åœ¨äº¤äº’å¼ã€å…¨å›¾åŠè§†é¢‘åˆ†å‰²ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚åœ¨11ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•å¤§å¹…æå‡äº†NoC90ã€1-IoUåŠAR1000ç­‰å…³é”®æŒ‡æ ‡ï¼Œå±•ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¯æ˜ï¼Œé€šè¿‡ç²’åº¦æ„ŸçŸ¥çš„è‡ªç›‘ç£å­¦ä¹ ï¼Œå³ä¾¿ä½¿ç”¨æå°è§„æ¨¡çš„æ— æ ‡æ³¨æ•°æ®ä¹Ÿèƒ½æœ‰æ•ˆé‡Šæ”¾è§†è§‰åŸºç¡€æ¨¡å‹(vision foundation models)çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13714v1",
      "published_date": "2025-11-17 18:58:34 UTC",
      "updated_date": "2025-11-17 18:58:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:50:09.644179+00:00"
    },
    {
      "arxiv_id": "2511.13712v1",
      "title": "From Black Box to Insight: Explainable AI for Extreme Event Preparedness",
      "title_zh": "ä»é»‘ç›’åˆ°æ´å¯Ÿï¼šé¢å‘æç«¯äº‹ä»¶åº”å¯¹å‡†å¤‡çš„å¯è§£é‡Šäººå·¥æ™ºèƒ½",
      "authors": [
        "Kiana Vu",
        "Ä°smet SelÃ§uk Ã–zer",
        "Phung Lai",
        "Zheng Wu",
        "Thilanka Munasinghe",
        "Jennifer Wei"
      ],
      "abstract": "As climate change accelerates the frequency and severity of extreme events such as wildfires, the need for accurate, explainable, and actionable forecasting becomes increasingly urgent. While artificial intelligence (AI) models have shown promise in predicting such events, their adoption in real-world decision-making remains limited due to their black-box nature, which limits trust, explainability, and operational readiness. This paper investigates the role of explainable AI (XAI) in bridging the gap between predictive accuracy and actionable insight for extreme event forecasting. Using wildfire prediction as a case study, we evaluate various AI models and employ SHapley Additive exPlanations (SHAP) to uncover key features, decision pathways, and potential biases in model behavior. Our analysis demonstrates how XAI not only clarifies model reasoning but also supports critical decision-making by domain experts and response teams. In addition, we provide supporting visualizations that enhance the interpretability of XAI outputs by contextualizing feature importance and temporal patterns in seasonality and geospatial characteristics. This approach enhances the usability of AI explanations for practitioners and policymakers. Our findings highlight the need for AI systems that are not only accurate but also interpretable, accessible, and trustworthy, essential for effective use in disaster preparedness, risk mitigation, and climate resilience planning.",
      "tldr_zh": "éšç€æ°”å€™å˜åŒ–åŠ å‰§äº†é‡ç«ç­‰æç«¯äº‹ä»¶çš„é¢‘ç‡å’Œä¸¥é‡ç¨‹åº¦ï¼Œä¼ ç»Ÿ AI æ¨¡å‹å› å…¶â€œé»‘ç›’â€æ€§è´¨åœ¨å®é™…å†³ç­–ä¸­çš„ä¿¡ä»»åº¦å’Œå¯æ“ä½œæ€§å—åˆ°ä¸¥é‡é™åˆ¶ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å¯è§£é‡Šäººå·¥æ™ºèƒ½ (Explainable AI, XAI) åœ¨å¼¥åˆé¢„æµ‹å‡†ç¡®æ€§ä¸è¡ŒåŠ¨è§è§£ä¹‹é—´å·®è·çš„ä½œç”¨ï¼Œå¹¶ä»¥é‡ç«é¢„æµ‹ä½œä¸ºæ ¸å¿ƒæ¡ˆä¾‹ç ”ç©¶ã€‚é€šè¿‡è¯„ä¼°å¤šç§ AI æ¨¡å‹å¹¶åº”ç”¨ SHapley Additive exPlanations (SHAP) æŠ€æœ¯ï¼Œç ”ç©¶æ­ç¤ºäº†æ¨¡å‹è¡Œä¸ºä¸­çš„å…³é”®ç‰¹å¾ã€å†³ç­–è·¯å¾„ä»¥åŠæ½œåœ¨åå·®ã€‚å®éªŒè¯æ˜ï¼ŒXAI ä¸ä»…èƒ½é˜æ˜æ¨¡å‹çš„æ¨ç†é€»è¾‘ï¼Œè¿˜èƒ½é€šè¿‡å¯è§†åŒ–æ‰‹æ®µå¼ºåŒ–å¯¹å­£èŠ‚æ€§å’Œåœ°ç†ç©ºé—´ç‰¹å¾çš„ç†è§£ï¼Œä»è€Œä¸ºé¢†åŸŸä¸“å®¶å’Œåº”æ€¥å›¢é˜Ÿæä¾›å…³é”®çš„å†³ç­–æ”¯æŒã€‚è¯¥é¡¹å·¥ä½œæå‡äº† AI è§£é‡Šåœ¨æ”¿ç­–åˆ¶å®šè€…å’Œä»ä¸šè€…ä¸­çš„å¯ç”¨æ€§ï¼Œå¼ºè°ƒäº†æ„å»ºå‡†ç¡®ã€å¯è§£é‡Šä¸”å¯ä¿¡èµ–çš„ç³»ç»Ÿå¯¹äºç¾å®³é¢„é˜²å’Œå¢å¼ºæ°”å€™éŸ§æ€§çš„é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13712v1",
      "published_date": "2025-11-17 18:57:15 UTC",
      "updated_date": "2025-11-17 18:57:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:50:21.844018+00:00"
    },
    {
      "arxiv_id": "2511.13710v1",
      "title": "From Power to Precision: Learning Fine-grained Dexterity for Multi-fingered Robotic Hands",
      "title_zh": "ä»å¼ºåŠ›åˆ°ç²¾ç»†ï¼šå­¦ä¹ å¤šæŒ‡æœºå™¨äººæ‰‹çš„ç»†ç²’åº¦çµå·§æ€§",
      "authors": [
        "Jianglong Ye",
        "Lai Wei",
        "Guangqi Jiang",
        "Changwei Jing",
        "Xueyan Zou",
        "Xiaolong Wang"
      ],
      "abstract": "Human grasps can be roughly categorized into two types: power grasps and precision grasps. Precision grasping enables tool use and is believed to have influenced human evolution. Today's multi-fingered robotic hands are effective in power grasps, but for tasks requiring precision, parallel grippers are still more widely adopted. This contrast highlights a key limitation in current robotic hand design: the difficulty of achieving both stable power grasps and precise, fine-grained manipulation within a single, versatile system. In this work, we bridge this gap by jointly optimizing the control and hardware design of a multi-fingered dexterous hand, enabling both power and precision manipulation. Rather than redesigning the entire hand, we introduce a lightweight fingertip geometry modification, represent it as a contact plane, and jointly optimize its parameters along with the corresponding control. Our control strategy dynamically switches between power and precision manipulation and simplifies precision control into parallel thumb-index motions, which proves robust for sim-to-real transfer. On the design side, we leverage large-scale simulation to optimize the fingertip geometry using a differentiable neural-physics surrogate model. We validate our approach through extensive experiments in both sim-to-real and real-to-real settings. Our method achieves an 82.5% zero-shot success rate on unseen objects in sim-to-real precision grasping, and a 93.3% success rate in challenging real-world tasks involving bread pinching. These results demonstrate that our co-design framework can significantly enhance the fine-grained manipulation ability of multi-fingered hands without reducing their ability for power grasps. Our project page is at https://jianglongye.com/power-to-precision",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæŒ‡æœºå™¨äººæ‰‹åœ¨å¹³è¡¡åŠ›é‡æŠ“å– (power grasps) ä¸ç²¾ç»†æ“ä½œ (precision manipulation) æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§è”åˆä¼˜åŒ–ç¡¬ä»¶è®¾è®¡ä¸æ§åˆ¶ç­–ç•¥çš„ååŒè®¾è®¡æ¡†æ¶ã€‚åœ¨ç¡¬ä»¶å±‚é¢ï¼Œç ”ç©¶è€…é€šè¿‡å¼•å…¥è½»é‡åŒ–çš„æŒ‡å°–å‡ ä½•ä¿®æ”¹ï¼Œå°†å…¶è¡¨ç¤ºä¸ºæ¥è§¦å¹³é¢ (contact plane)ï¼Œå¹¶åˆ©ç”¨å¯å¾®åˆ†ç¥ç»ç‰©ç†ä»£ç†æ¨¡å‹ (differentiable neural-physics surrogate model) åœ¨å¤§è§„æ¨¡æ¨¡æ‹Ÿä¸­ä¼˜åŒ–å‡ ä½•å‚æ•°ã€‚æ§åˆ¶ç­–ç•¥æ–¹é¢ï¼Œè¯¥ç³»ç»Ÿæ”¯æŒåœ¨åŠ›é‡ä¸ç²¾å¯†æ¨¡å¼é—´åŠ¨æ€åˆ‡æ¢ï¼Œå¹¶å°†ç²¾å¯†æ§åˆ¶ç®€åŒ–ä¸ºé²æ£’çš„æ‹‡æŒ‡ä¸é£ŸæŒ‡å¹³è¡Œè¿åŠ¨ï¼Œæ˜¾è‘—æå‡äº†ä»æ¨¡æ‹Ÿåˆ°ç°å® (sim-to-real) è¿ç§»çš„æ•ˆæœã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æœªè§ç‰©ä½“çš„é›¶æ ·æœ¬ (zero-shot) ç²¾å¯†æŠ“å–ä¸­è¾¾åˆ°äº† 82.5% çš„æˆåŠŸç‡ï¼Œå¹¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„çœŸå®ä¸–ç•Œé¢åŒ…æå–ä»»åŠ¡ä¸­å®ç°äº† 93.3% çš„æˆåŠŸç‡ã€‚è¿™ä¸€ç ”ç©¶è¯æ˜äº†é€šè¿‡è½¯ç¡¬ä»¶ååŒä¼˜åŒ–å¯ä»¥æ˜¾è‘—å¢å¼ºå¤šæŒ‡çµå·§æ‰‹çš„ç»†ç²’åº¦æ“ä½œèƒ½åŠ›ï¼ŒåŒæ—¶æ— éœ€ç‰ºç‰²å…¶åŸæœ‰çš„åŠ›é‡æŠ“å–æ€§èƒ½ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Project page: https://jianglongye.com/power-to-precision",
      "pdf_url": "https://arxiv.org/pdf/2511.13710v1",
      "published_date": "2025-11-17 18:56:50 UTC",
      "updated_date": "2025-11-17 18:56:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:50:38.749280+00:00"
    },
    {
      "arxiv_id": "2511.13703v1",
      "title": "Generalist Foundation Models Are Not Clinical Enough for Hospital Operations",
      "title_zh": "é€šç”¨åŸºç¡€æ¨¡å‹åœ¨åŒ»é™¢è¿è¥ä¸­ç¼ºä¹è¶³å¤Ÿçš„ä¸´åºŠä¸“ä¸šæ€§",
      "authors": [
        "Lavender Y. Jiang",
        "Angelica Chen",
        "Xu Han",
        "Xujin Chris Liu",
        "Radhika Dua",
        "Kevin Eaton",
        "Frederick Wolff",
        "Robert Steele",
        "Jeff Zhang",
        "Anton Alyakin",
        "Qingkai Pan",
        "Yanbing Chen",
        "Karl L. Sangwon",
        "Daniel A. Alber",
        "Jaden Stryker",
        "Jin Vivian Lee",
        "Yindalon Aphinyanaphongs",
        "Kyunghyun Cho",
        "Eric Karl Oermann"
      ],
      "abstract": "Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œå°½ç®¡é€šç”¨åŸºç¡€æ¨¡å‹åœ¨åŒ»å­¦çŸ¥è¯†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†æ¶‰åŠæ‚£è€…æµåŠ¨ã€æˆæœ¬å’ŒæŠ¤ç†è´¨é‡çš„åŒ»é™¢è¿è¥å†³ç­–æ—¶ä»ç¼ºä¹ä¸“ä¸šçŸ¥è¯†ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº† Lang1 ç³»åˆ—æ¨¡å‹ï¼Œé€šè¿‡åœ¨ NYU Langone Health çš„ 80B ä¸´åºŠ Token å’Œ 627B äº’è”ç½‘ Token ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ—¨åœ¨å¡«è¡¥è¿™ä¸€é¢†åŸŸç©ºç™½ã€‚ç ”ç©¶è€…è¿˜å¼€å‘äº†åä¸º REalistic Medical Evaluation (ReMedE) çš„åŸºå‡†æµ‹è¯•ï¼Œåˆ©ç”¨å¤§é‡ç”µå­å¥åº·è®°å½• (EHR) æ¥è¯„ä¼° 30-day readmission predictionã€30-day mortality predictionã€length of stayã€comorbidity coding å’Œ insurance claims denial äº”é¡¹å…³é”®ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLang1-1B åœ¨ç»è¿‡å¾®è°ƒåæ˜¾è‘—ä¼˜äºè§„æ¨¡å¤§å…¶æ•°åå€ç”šè‡³æ•°ç™¾å€çš„é€šç”¨åŸºç¡€æ¨¡å‹ï¼Œåœ¨å¤šé¡¹ä»»åŠ¡ä¸­å±•ç°å‡ºæ›´é«˜çš„ AUROC æ€§èƒ½ã€‚ç ”ç©¶å‘ç°ï¼Œæå‡åŒ»é™¢è¿è¥é¢„æµ‹èƒ½åŠ›å¿…é¡»ç»“åˆé¢†åŸŸå†…é¢„è®­ç»ƒä¸æ˜¾å¼çš„æœ‰ç›‘ç£å¾®è°ƒ (Supervised Finetuning)ï¼Œä¸”æ¨¡å‹å…·å¤‡è‰¯å¥½çš„è·¨ä»»åŠ¡æ‰©å±•å’Œé™¢å¤–åˆ†å¸ƒè¿ç§»èƒ½åŠ›ã€‚è¯¥æˆæœè¯æ˜äº†ä¸“ä¸šåŒ– LLMs åœ¨ç‰¹å®šåŒ»ç–—è¿è¥ä»»åŠ¡ä¸­ç›¸æ¯”é€šç”¨æ¨¡å‹æ›´å…·ç«äº‰ä¼˜åŠ¿ï¼Œä¸ºæ„å»ºé«˜æ•ˆã€å®ç”¨çš„åŒ»ç–— AI ç³»ç»Ÿæä¾›äº†é‡è¦è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13703v1",
      "published_date": "2025-11-17 18:52:22 UTC",
      "updated_date": "2025-11-17 18:52:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:50:49.832948+00:00"
    },
    {
      "arxiv_id": "2511.13702v1",
      "title": "ST-ProC: A Graph-Prototypical Framework for Robust Semi-Supervised Travel Mode Identification",
      "title_zh": "ST-ProCï¼šé¢å‘é²æ£’åŠç›‘ç£å‡ºè¡Œæ–¹å¼è¯†åˆ«çš„å›¾åŸå‹æ¡†æ¶",
      "authors": [
        "Luyao Niu",
        "Nuoxian Huang"
      ],
      "abstract": "Travel mode identification (TMI) from GPS trajectories is critical for urban intelligence, but is hampered by the high cost of annotation, leading to severe label scarcity. Prevailing semi-supervised learning (SSL) methods are ill-suited for this task, as they suffer from catastrophic confirmation bias and ignore the intrinsic data manifold. We propose ST-ProC, a novel graph-prototypical multi-objective SSL framework to address these limitations. Our framework synergizes a graph-prototypical core with foundational SSL Support. The core exploits the data manifold via graph regularization, prototypical anchoring, and a novel, margin-aware pseudo-labeling strategy to actively reject noise. This core is supported and stabilized by foundational contrastive and teacher-student consistency losses, ensuring high-quality representations and robust optimization. ST-ProC outperforms all baselines by a significant margin, demonstrating its efficacy in real-world sparse-label settings, with a performance boost of 21.5% over state-of-the-art methods like FixMatch.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹GPSè½¨è¿¹å‡ºè¡Œæ¨¡å¼è¯†åˆ«(Travel mode identification, TMI)ä¸­æ ‡ç­¾ç¨€ç¼ºåŠç°æœ‰åŠç›‘ç£å­¦ä¹ (SSL)æ–¹æ³•å­˜åœ¨çš„ç¡®è®¤åè¯¯é—®é¢˜ï¼Œæå‡ºäº†åä¸ºST-ProCçš„å›¾åŸå‹å¤šç›®æ ‡åŠç›‘ç£å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†å›¾åŸå‹æ ¸å¿ƒä¸åŸºç¡€SSLæ”¯æŒç›¸ç»“åˆï¼Œé€šè¿‡å›¾æ­£åˆ™åŒ–(graph regularization)ã€åŸå‹é”šå®š(prototypical anchoring)å’Œæ–°å‹è¾¹ç¼˜æ„ŸçŸ¥ä¼ªæ ‡ç­¾(margin-aware pseudo-labeling)ç­–ç•¥æ¥å……åˆ†æŒ–æ˜æ•°æ®æµå½¢å¹¶ä¸»åŠ¨å‰”é™¤å™ªå£°ã€‚åŒæ—¶ï¼Œç³»ç»Ÿå¼•å…¥äº†å¯¹æ¯”æŸå¤±(contrastive loss)å’Œæ•™å¸ˆ-å­¦ç”Ÿä¸€è‡´æ€§æŸå¤±(teacher-student consistency loss)ä»¥å¢å¼ºè¡¨å¾è´¨é‡ï¼Œç¡®ä¿äº†å¤æ‚åœºæ™¯ä¸‹ä¼˜åŒ–çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒST-ProCåœ¨ç°å®ä¸–ç•Œçš„ç¨€ç–æ ‡ç­¾è®¾å®šä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œå…¶æ€§èƒ½ç›¸æ¯”FixMatchç­‰æœ€å…ˆè¿›æ–¹æ³•æå‡äº†21.5%ï¼Œä¸ºä½èµ„æºç¯å¢ƒä¸‹çš„åŸå¸‚æ™ºèƒ½ä»»åŠ¡æä¾›äº†é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13702v1",
      "published_date": "2025-11-17 18:52:11 UTC",
      "updated_date": "2025-11-17 18:52:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:50:56.538997+00:00"
    },
    {
      "arxiv_id": "2511.14805v1",
      "title": "Towards Continuous Assurance with Formal Verification and Assurance Cases",
      "title_zh": "è¿ˆå‘åŸºäºå½¢å¼åŒ–éªŒè¯ä¸ä¿éšœæ¡ˆä¾‹çš„æŒç»­ä¿éšœ",
      "authors": [
        "Dhaminda B. Abeywickrama",
        "Michael Fisher",
        "Frederic Wheeler",
        "Louise Dennis"
      ],
      "abstract": "Autonomous systems must sustain justified confidence in their correctness and safety across their operational lifecycle-from design and deployment through post-deployment evolution. Traditional assurance methods often separate development-time assurance from runtime assurance, yielding fragmented arguments that cannot adapt to runtime changes or system updates - a significant challenge for assured autonomy. Towards addressing this, we propose a unified Continuous Assurance Framework that integrates design-time, runtime, and evolution-time assurance within a traceable, model-driven workflow as a step towards assured autonomy. In this paper, we specifically instantiate the design-time phase of the framework using two formal verification methods: RoboChart for functional correctness and PRISM for probabilistic risk analysis. We also propose a model-driven transformation pipeline, implemented as an Eclipse plugin, that automatically regenerates structured assurance arguments whenever formal specifications or their verification results change, thereby ensuring traceability. We demonstrate our approach on a nuclear inspection robot scenario, and discuss its alignment with the Trilateral AI Principles, reflecting regulator-endorsed best practices.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªä¸»ç³»ç»Ÿåœ¨å…¨ç”Ÿå‘½å‘¨æœŸä¸­éš¾ä»¥ç»´æŒæŒç»­ä¿¡ä»»çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ Continuous Assurance Frameworkï¼Œæ—¨åœ¨å°†è®¾è®¡æ—¶ã€è¿è¡Œæ—¶å’Œæ¼”åŒ–æ—¶çš„ä¿éšœé›†æˆåˆ°å¯è¿½æº¯çš„æ¨¡å‹é©±åŠ¨å·¥ä½œæµä¸­ã€‚ç ”ç©¶é€šè¿‡ RoboChart è¿›è¡ŒåŠŸèƒ½æ­£ç¡®æ€§éªŒè¯ï¼Œå¹¶åˆ©ç”¨ PRISM è¿›è¡Œæ¦‚ç‡é£é™©åˆ†æï¼Œå…·ä½“å®ä¾‹åŒ–äº†æ¡†æ¶çš„è®¾è®¡æ—¶é˜¶æ®µã€‚ä¸ºè§£å†³ä¼ ç»Ÿæ–¹æ³•ä¸­ä¿éšœè®ºè¯ä¸å¼€å‘è„±èŠ‚çš„æŒ‘æˆ˜ï¼Œç ”ç©¶å¼€å‘äº†ä¸€ä¸ª Eclipse æ’ä»¶ï¼Œå®ç°äº†åœ¨å½¢å¼åŒ–è§„æ ¼æˆ–éªŒè¯ç»“æœå˜æ›´æ—¶è‡ªåŠ¨é‡æ–°ç”Ÿæˆç»“æ„åŒ– Assurance Case çš„æ¨¡å‹é©±åŠ¨è½¬æ¢ç®¡é“ã€‚è¯¥æ–¹æ³•åœ¨æ ¸æ£€æŸ¥æœºå™¨äººï¼ˆnuclear inspection robotï¼‰åœºæ™¯ä¸­å¾—åˆ°äº†æˆåŠŸéªŒè¯ï¼Œå¹¶ç¡®ä¿ä¸ Trilateral AI Principles ä¿æŒä¸€è‡´ã€‚è¿™ç§é›†æˆå½¢å¼åŒ–éªŒè¯ä¸åŠ¨æ€è®ºè¯ç”Ÿæˆçš„æ–¹æ³•ï¼Œä¸ºå®ç°è‡ªä¸»ç³»ç»Ÿçš„æŒç»­ä¿éšœå’Œåˆè§„æ€§æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "15 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.14805v1",
      "published_date": "2025-11-17 18:42:38 UTC",
      "updated_date": "2025-11-17 18:42:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:50:50.887057+00:00"
    },
    {
      "arxiv_id": "2511.13685v1",
      "title": "Protein Secondary Structure Prediction Using 3D Graphs and Relation-Aware Message Passing Transformers",
      "title_zh": "åŸºäº 3D å›¾ä¸å…³ç³»æ„ŸçŸ¥æ¶ˆæ¯ä¼ é€’ Transformer çš„è›‹ç™½è´¨äºŒçº§ç»“æ„é¢„æµ‹",
      "authors": [
        "Disha Varshney",
        "Samarth Garg",
        "Sarthak Tyagi",
        "Deeksha Varshney",
        "Nayan Deep",
        "Asif Ekbal"
      ],
      "abstract": "In this study, we tackle the challenging task of predicting secondary structures from protein primary sequences, a pivotal initial stride towards predicting tertiary structures, while yielding crucial insights into protein activity, relationships, and functions. Existing methods often utilize extensive sets of unlabeled amino acid sequences. However, these approaches neither explicitly capture nor harness the accessible protein 3D structural data, which is recognized as a decisive factor in dictating protein functions. To address this, we utilize protein residue graphs and introduce various forms of sequential or structural connections to capture enhanced spatial information. We adeptly combine Graph Neural Networks (GNNs) and Language Models (LMs), specifically utilizing a pre-trained transformer-based protein language model to encode amino acid sequences and employing message-passing mechanisms like GCN and R-GCN to capture geometric characteristics of protein structures. Employing convolution within a specific node's nearby region, including relations, we stack multiple convolutional layers to efficiently learn combined insights from the protein's spatial graph, revealing intricate interconnections and dependencies in its structural arrangement. To assess our model's performance, we employed the training dataset provided by NetSurfP-2.0, which outlines secondary structure in 3-and 8-states. Extensive experiments show that our proposed model, SSRGNet surpasses the baseline on f1-scores.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SSRGNetï¼Œæ—¨åœ¨è§£å†³ä»ä¸€çº§åºåˆ—é¢„æµ‹è›‹ç™½è´¨äºŒçº§ç»“æ„æ—¶ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨3Dç»“æ„æ•°æ®çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹é€šè¿‡è›‹ç™½è´¨æ®‹åŸºå›¾(residue graphs)å¼•å…¥å¤šç§åºåˆ—æˆ–ç»“æ„è¿æ¥ï¼Œä»¥æœ‰æ•ˆæ•æ‰å¢å¼ºçš„ç©ºé—´ä¿¡æ¯ã€‚SSRGNetå·§å¦™åœ°ç»“åˆäº†å›¾ç¥ç»ç½‘ç»œ(GNNs)ä¸é¢„è®­ç»ƒTransformerè›‹ç™½è´¨è¯­è¨€æ¨¡å‹(LMs)ï¼Œå¹¶é‡‡ç”¨GCNå’ŒR-GCNç­‰æ¶ˆæ¯ä¼ é€’æœºåˆ¶æ¥æå–è›‹ç™½è´¨ç»“æ„çš„å‡ ä½•ç‰¹å¾ã€‚é€šè¿‡åœ¨ç‰¹å®šèŠ‚ç‚¹åŠå…¶é‚»åŸŸå†…å †å å¤šå±‚å·ç§¯å±‚ï¼Œæ¨¡å‹èƒ½å¤Ÿä»è›‹ç™½è´¨ç©ºé—´å›¾ä¸­å­¦ä¹ å¤æ‚çš„ç›¸äº’å…³è”å’Œä¾èµ–å…³ç³»ã€‚åœ¨NetSurfP-2.0æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒSSRGNetåœ¨3æ€å’Œ8æ€äºŒçº§ç»“æ„é¢„æµ‹çš„f1-scoresä¸Šå‡ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "40 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.13685v1",
      "published_date": "2025-11-17 18:39:13 UTC",
      "updated_date": "2025-11-17 18:39:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:50:51.038369+00:00"
    },
    {
      "arxiv_id": "2511.17595v1",
      "title": "Boosting Reinforcement Learning in 3D Visuospatial Tasks Through Human-Informed Curriculum Design",
      "title_zh": "é€šè¿‡äººç±»å¯å‘å¼è¯¾ç¨‹è®¾è®¡å¢å¼º 3D è§†è§‰ç©ºé—´ä»»åŠ¡ä¸­çš„å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Markus D. Solbach",
        "John K. Tsotsos"
      ],
      "abstract": "Reinforcement Learning is a mature technology, often suggested as a potential route towards Artificial General Intelligence, with the ambitious goal of replicating the wide range of abilities found in natural and artificial intelligence, including the complexities of human cognition. While RL had shown successes in relatively constrained environments, such as the classic Atari games and specific continuous control problems, recent years have seen efforts to expand its applicability. This work investigates the potential of RL in demonstrating intelligent behaviour and its progress in addressing more complex and less structured problem domains.\n  We present an investigation into the capacity of modern RL frameworks in addressing a seemingly straightforward 3D Same-Different visuospatial task. While initial applications of state-of-the-art methods, including PPO, behavioural cloning and imitation learning, revealed challenges in directly learning optimal strategies, the successful implementation of curriculum learning offers a promising avenue. Effective learning was achieved by strategically designing the lesson plan based on the findings of a real-world human experiment.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)åœ¨å¤„ç†å¤æ‚ä¸”éç»“æ„åŒ–ä¸‰ç»´ç©ºé—´ä»»åŠ¡ä¸­çš„æ½œåŠ›ï¼Œé‡ç‚¹å…³æ³¨ä¸€ä¸ªçœ‹ä¼¼ç®€å•ä½†æå…·æŒ‘æˆ˜æ€§çš„ 3D Same-Different è§†è§‰ç©ºé—´ä»»åŠ¡ã€‚å®éªŒå‘ç°ï¼ŒåŒ…æ‹¬ PPOã€è¡Œä¸ºå…‹éš†(Behavioral Cloning)å’Œæ¨¡ä»¿å­¦ä¹ (Imitation Learning)åœ¨å†…çš„æœ€å…ˆè¿›æ–¹æ³•åœ¨ç›´æ¥å­¦ä¹ è¯¥ä»»åŠ¡çš„æœ€ä¼˜ç­–ç•¥æ—¶å‡é‡åˆ°äº†å·¨å¤§æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§å—äººç±»å¯å‘çš„è¯¾ç¨‹è®¾è®¡æ–¹æ¡ˆï¼Œé€šè¿‡æˆåŠŸå®æ–½è¯¾ç¨‹å­¦ä¹ (Curriculum Learning)ä¸ºè§£å†³è¯¥é—®é¢˜æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºæ ¹æ®çœŸå®ä¸–ç•Œäººç±»å®éªŒçš„å‘ç°ï¼Œæˆ˜ç•¥æ€§åœ°è®¾è®¡äº†æ™ºèƒ½ä½“çš„æ•™å­¦è®¡åˆ’(Lesson Plan)ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿™ç§åŸºäºäººç±»è®¤çŸ¥è§„å¾‹çš„è¯¾ç¨‹è®¾è®¡ä½¿æ¨¡å‹æœ€ç»ˆå®ç°äº†æœ‰æ•ˆå­¦ä¹ ï¼Œè¯æ˜äº†é€šè¿‡äººç±»ç»éªŒå¼•å¯¼çš„æ•™å­¦åºåˆ—åœ¨æå‡æ™ºèƒ½ä½“å¤„ç†å¤æ‚ä¸‰ç»´æ„ŸçŸ¥ä»»åŠ¡èƒ½åŠ›æ–¹é¢å…·æœ‰æ˜¾è‘—ä½œç”¨ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 11 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.17595v1",
      "published_date": "2025-11-17 18:28:07 UTC",
      "updated_date": "2025-11-17 18:28:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:50:53.437201+00:00"
    },
    {
      "arxiv_id": "2511.13670v1",
      "title": "Person-AI Bidirectional Fit - A Proof-Of-Concept Case Study Of Augmented Human-Ai Symbiosis In Management Decision-Making Process",
      "title_zh": "äºº-AIåŒå‘é€‚é…ï¼šç®¡ç†å†³ç­–è¿‡ç¨‹ä¸­å¢å¼ºå‹äººæœºå…±ç”Ÿçš„æ¦‚å¿µéªŒè¯æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Agnieszka BieÅ„kowska",
        "Jacek MaÅ‚ecki",
        "Alexander Mathiesen-Ohman",
        "Katarzyna Tworek"
      ],
      "abstract": "This article develops the concept of Person-AI bidirectional fit, defined as the continuously evolving, context-sensitive alignment-primarily cognitive, but also emotional and behavioral-between a human decision-maker and an artificial intelligence system. Grounded in contingency theory and quality theory, the study examines the role of P-AI fit in managerial decision-making through a proof-of-concept case study involving a real hiring process for a Senior AI Lead. Three decision pathways are compared: (1) independent evaluations by a CEO, CTO, and CSO; (2) an evaluation produced by an augmented human-AI symbiotic intelligence system (H3LIX-LAIZA); and (3) an assessment generated by a general-purpose large language model. The results reveal substantial role-based divergence in human judgments, high alignment between H3LIX-LAIZA and the CEOs implicit decision model-including ethical disqualification of a high-risk candidate and a critical false-positive recommendation from the LLMr. The findings demonstrate that higher P-AI fit, exemplified by the CEO H3LIX-LAIZA relationship, functions as a mechanism linking augmented symbiotic intelligence to accurate, trustworthy, and context-sensitive decisions. The study provides an initial verification of the P-AI fit construct and a proof-of-concept for H3LIX-LAIZA as an augmented human-AI symbiotic intelligence system.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Person-AI bidirectional fitï¼ˆäºº-AI åŒå‘é€‚é…ï¼‰çš„æ¦‚å¿µï¼Œå°†å…¶å®šä¹‰ä¸ºäººç±»å†³ç­–è€…ä¸äººå·¥æ™ºèƒ½ç³»ç»Ÿä¹‹é—´åœ¨è®¤çŸ¥ã€æƒ…æ„Ÿå’Œè¡Œä¸ºä¸ŠæŒç»­æ¼”è¿›ä¸”æƒ…å¢ƒæ•æ„Ÿçš„å¯¹é½ã€‚åŸºäº contingency theoryï¼ˆæƒå˜ç†è®ºï¼‰å’Œ quality theoryï¼ˆè´¨é‡ç†è®ºï¼‰ï¼Œç ”ç©¶é€šè¿‡ä¸€ä¸ªæ‹›è˜ Senior AI Lead çš„æ¦‚å¿µéªŒè¯æ¡ˆä¾‹ï¼Œæ¢è®¨äº† P-AI fit åœ¨ç®¡ç†å†³ç­–ä¸­çš„ä½œç”¨ã€‚å®éªŒå¯¹æ¯”äº†å…¬å¸é«˜ç®¡çš„ç‹¬ç«‹è¯„ä¼°ã€å¢å¼ºå‹äººæœºå…±ç”Ÿæ™ºèƒ½ç³»ç»Ÿ H3LIX-LAIZA ä»¥åŠé€šç”¨ large language model (LLM) çš„è¯„ä¼°ç»“æœã€‚ç ”ç©¶å‘ç°äººç±»åˆ¤æ–­å­˜åœ¨æ˜¾è‘—çš„è§’è‰²å·®å¼‚ï¼Œè€Œ H3LIX-LAIZA ä¸ CEO çš„éšæ€§å†³ç­–æ¨¡å‹é«˜åº¦ä¸€è‡´ï¼ŒæˆåŠŸè¯†åˆ«äº†é«˜é£é™©å€™é€‰äººçš„ä¼¦ç†é—®é¢˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œé€šç”¨ LLM äº§ç”Ÿäº†ä¸€ä¸ªå…³é”®çš„ false-positiveï¼ˆè¯¯æŠ¥ï¼‰å»ºè®®ï¼Œä½“ç°äº†å…¶åœ¨ç‰¹å®šæƒ…å¢ƒä¸‹çš„å±€é™æ€§ã€‚ç ”ç©¶è¯æ˜äº†è¾ƒé«˜çš„ P-AI fit æ˜¯å°†å¢å¼ºå‹å…±ç”Ÿæ™ºèƒ½è½¬åŒ–ä¸ºå‡†ç¡®ã€å¯é ä¸”æƒ…å¢ƒæ•æ„Ÿå†³ç­–çš„å…³é”®æœºåˆ¶ã€‚è¯¥ç ”ç©¶åˆæ­¥éªŒè¯äº† P-AI fit æ„ä»¶ï¼Œå¹¶ä¸º H3LIX-LAIZA ä½œä¸ºå¢å¼ºå‹äººæœºå…±ç”Ÿæ™ºèƒ½ç³»ç»Ÿæä¾›äº†æ¦‚å¿µéªŒè¯æ”¯æŒã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "30 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.13670v1",
      "published_date": "2025-11-17 18:22:30 UTC",
      "updated_date": "2025-11-17 18:22:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:51:03.740632+00:00"
    },
    {
      "arxiv_id": "2511.13809v1",
      "title": "ScoresActivation: A New Activation Function for Model Agnostic Global Explainability by Design",
      "title_zh": "ScoresActivationï¼šä¸€ç§é¢å‘æ¨¡å‹æ— å…³å†…ç”Ÿå…¨å±€å¯è§£é‡Šæ€§çš„æ–°å‹æ¿€æ´»å‡½æ•°",
      "authors": [
        "Emanuel Covaci",
        "Fabian Galis",
        "Radu Balan",
        "Daniela Zaharie",
        "Darian Onchis"
      ],
      "abstract": "Understanding the decision of large deep learning models is a critical challenge for building transparent and trustworthy systems. Although the current post hoc explanation methods offer valuable insights into feature importance, they are inherently disconnected from the model training process, limiting their faithfulness and utility. In this work, we introduce a novel differentiable approach to global explainability by design, integrating feature importance estimation directly into model training. Central to our method is the ScoresActivation function, a feature-ranking mechanism embedded within the learning pipeline. This integration enables models to prioritize features according to their contribution to predictive performance in a differentiable and end-to-end trainable manner. Evaluations across benchmark datasets show that our approach yields globally faithful, stable feature rankings aligned with SHAP values and ground-truth feature importance, while maintaining high predictive performance. Moreover, feature scoring is 150 times faster than the classical SHAP method, requiring only 2 seconds during training compared to SHAP's 300 seconds for feature ranking in the same configuration. Our method also improves classification accuracy by 11.24% with 10 features (5 relevant) and 29.33% with 16 features (5 relevant, 11 irrelevant), demonstrating robustness to irrelevant inputs. This work bridges the gap between model accuracy and interpretability, offering a scalable framework for inherently explainable machine learning.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† ScoresActivationï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¯å¾®æ¿€æ´»å‡½æ•°ï¼Œæ—¨åœ¨é€šè¿‡åœ¨å­¦ä¹ æµæ°´çº¿ä¸­åµŒå…¥ç‰¹å¾æ’åºæœºåˆ¶ï¼Œå®ç°æ¨¡å‹ä¸å¯çŸ¥çš„å…¨å±€å¯è§£é‡Šæ€§è®¾è®¡(Global Explainability by Design)ã€‚ä¸ä¼ ç»Ÿçš„åéªŒè§£é‡Šæ–¹æ³•(post hoc explanation methods)ä¸åŒï¼Œè¯¥æ–¹æ³•å°†ç‰¹å¾é‡è¦æ€§ä¼°è®¡ç›´æ¥é›†æˆåˆ°æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»¥ç«¯åˆ°ç«¯è®­ç»ƒçš„æ–¹å¼ä¼˜å…ˆè€ƒè™‘å¯¹é¢„æµ‹æ€§èƒ½è´¡çŒ®è¾ƒå¤§çš„ç‰¹å¾ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒScoresActivation åœ¨åŸºå‡†æ•°æ®é›†ä¸Šèƒ½å¤Ÿäº§ç”Ÿä¸ SHAP å€¼å’ŒçœŸå®ç‰¹å¾é‡è¦æ€§ä¸€è‡´çš„å…¨å±€å¿ å®ä¸”ç¨³å®šçš„ç‰¹å¾æ’åºã€‚åœ¨æ•ˆç‡æ–¹é¢ï¼Œè¯¥æ–¹æ³•çš„ç‰¹å¾è¯„åˆ†é€Ÿåº¦æ¯”ç»å…¸çš„ SHAP æ–¹æ³•å¿«150å€ï¼Œåœ¨ç›¸åŒé…ç½®ä¸‹å®Œæˆç‰¹å¾æ’åºä»…éœ€2ç§’ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†æ— å…³è¾“å…¥æ—¶è¡¨ç°å‡ºæ˜¾è‘—çš„é²æ£’æ€§ï¼Œåœ¨å«æœ‰å¤§é‡å¹²æ‰°ç‰¹å¾çš„å®éªŒä¸­å°†åˆ†ç±»å‡†ç¡®ç‡æå‡äº†29.33%ã€‚è¿™é¡¹å·¥ä½œæœ‰æ•ˆç¼©å°äº†æ¨¡å‹å‡†ç¡®æ€§ä¸å¯è§£é‡Šæ€§ä¹‹é—´çš„å·®è·ï¼Œä¸ºæ„å»ºæœ¬è´¨å¯è§£é‡Šçš„æœºå™¨å­¦ä¹ ç³»ç»Ÿæä¾›äº†ä¸€ä¸ªé«˜æ•ˆç‡ä¸”å¯æ‰©å±•çš„æ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Paper submitted to ECAI 2025 Conference",
      "pdf_url": "https://arxiv.org/pdf/2511.13809v1",
      "published_date": "2025-11-17 18:10:34 UTC",
      "updated_date": "2025-11-17 18:10:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:51:15.838181+00:00"
    },
    {
      "arxiv_id": "2511.13653v1",
      "title": "Weight-sparse transformers have interpretable circuits",
      "title_zh": "æƒé‡ç¨€ç– Transformer å…·æœ‰å¯è§£é‡Šç”µè·¯",
      "authors": [
        "Leo Gao",
        "Achyuta Rajaram",
        "Jacob Coxon",
        "Soham V. Govande",
        "Bowen Baker",
        "Dan Mossing"
      ],
      "abstract": "Finding human-understandable circuits in language models is a central goal of the field of mechanistic interpretability. We train models to have more understandable circuits by constraining most of their weights to be zeros, so that each neuron only has a few connections. To recover fine-grained circuits underlying each of several hand-crafted tasks, we prune the models to isolate the part responsible for the task. These circuits often contain neurons and residual channels that correspond to natural concepts, with a small number of straightforwardly interpretable connections between them. We study how these models scale and find that making weights sparser trades off capability for interpretability, and scaling model size improves the capability-interpretability frontier. However, scaling sparse models beyond tens of millions of nonzero parameters while preserving interpretability remains a challenge. In addition to training weight-sparse models de novo, we show preliminary results suggesting our method can also be adapted to explain existing dense models. Our work produces circuits that achieve an unprecedented level of human understandability and validates them with considerable rigor.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é€šè¿‡åœ¨è®­ç»ƒä¸­çº¦æŸæƒé‡ç¨€ç–æ€§(weight-sparse)æ¥æé«˜è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œæ—¨åœ¨å‘ç°äººç±»å¯ç†è§£çš„ç”µè·¯(circuits)ã€‚ä½œè€…å°†æ¨¡å‹çš„å¤§éƒ¨åˆ†æƒé‡è®¾ä¸ºé›¶ï¼Œä½¿å¾—æ¯ä¸ªç¥ç»å…ƒä»…ä¿ç•™å°‘é‡è¿æ¥ï¼Œå¹¶éšåé€šè¿‡å‰ªæ(pruning)æŠ€æœ¯éš”ç¦»å‡ºå¯¹åº”ç‰¹å®šä»»åŠ¡çš„åº•å±‚ç”µè·¯ã€‚å®éªŒå‘ç°ï¼Œè¿™äº›ç”µè·¯ä¸­çš„ç¥ç»å…ƒ(neurons)å’Œæ®‹å·®é€šé“(residual channels)å¾€å¾€å¯¹åº”äºè‡ªç„¶æ¦‚å¿µï¼Œä¸”å½¼æ­¤ä¹‹é—´çš„è¿æ¥ç›´è§‚æ˜“æ‡‚ã€‚ç ”ç©¶è¿˜åˆ†æäº†æ¨¡å‹ç¼©æ”¾çš„å½±å“ï¼Œå‘ç°å¢åŠ æ¨¡å‹è§„æ¨¡å¯ä»¥ä¼˜åŒ–èƒ½åŠ›ä¸å¯è§£é‡Šæ€§ä¹‹é—´çš„æƒè¡¡å‰æ²¿(capability-interpretability frontier)ï¼Œä½†å°†ç¨€ç–æ¨¡å‹æ‰©å±•åˆ°åƒä¸‡çº§ä»¥ä¸Šçš„éé›¶å‚æ•°(nonzero parameters)ä»å…·æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œåˆæ­¥ç»“æœè¡¨æ˜è¯¥æ–¹æ³•ä¹Ÿå¯ç”¨äºè§£é‡Šç°æœ‰çš„ç¨ å¯†æ¨¡å‹(dense models)ã€‚è¿™é¡¹å·¥ä½œäº§å‡ºçš„ç”µè·¯åœ¨äººç±»å¯ç†è§£åº¦ä¸Šè¾¾åˆ°äº†æé«˜æ°´å¹³ï¼Œå¹¶ç»è¿‡äº†ä¸¥æ ¼çš„éªŒè¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13653v1",
      "published_date": "2025-11-17 18:02:06 UTC",
      "updated_date": "2025-11-17 18:02:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:51:15.036339+00:00"
    },
    {
      "arxiv_id": "2511.13646v3",
      "title": "Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?",
      "title_zh": "Live-SWE-agentï¼šè½¯ä»¶å·¥ç¨‹æ™ºèƒ½ä½“èƒ½å¦å®ç°å®æ—¶è‡ªæ¼”åŒ–ï¼Ÿ",
      "authors": [
        "Chunqiu Steven Xia",
        "Zhe Wang",
        "Yan Yang",
        "Yuxiang Wei",
        "Lingming Zhang"
      ],
      "abstract": "Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-GÃ¶del Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that LIVE-SWE-AGENT can achieve an impressive solve rate of 77.4% without test-time scaling, outperforming all existing software agents, including the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Live-SWE-agentï¼Œè¿™æ˜¯é¦–ä¸ªèƒ½å¤Ÿåœ¨è§£å†³å®é™…è½¯ä»¶é—®é¢˜è¿‡ç¨‹ä¸­å®ç°å®æ—¶ã€è‡ªä¸»ä¸”æŒç»­æ¼”åŒ–çš„è½¯ä»¶å·¥ç¨‹æ™ºèƒ½ä½“ã€‚é’ˆå¯¹ç°æœ‰è½¯ä»¶æ™ºèƒ½ä½“è„šæ‰‹æ¶è®¾è®¡å¤æ‚ä¸”ç¦»çº¿è®­ç»ƒæ¨¡å‹æ³›åŒ–æ€§å·®ç­‰æŒ‘æˆ˜ï¼ŒLive-SWE-agent æ—¨åœ¨æ¢ç´¢æ™ºèƒ½ä½“åœ¨è¿è¡Œæ—¶çš„è‡ªæˆ‘ä¼˜åŒ–èƒ½åŠ›ã€‚è¯¥æ™ºèƒ½ä½“ä»ä»…å…·å¤‡åŸºç¡€ Bash å·¥å…·çš„åˆå§‹è„šæ‰‹æ¶èµ·æ­¥ï¼Œå¹¶åœ¨æ‰§è¡Œä»»åŠ¡çš„è¿‡ç¨‹ä¸­ä¸æ–­è‡ªä¸»æ”¹è¿›å…¶è‡ªèº«çš„è„šæ‰‹æ¶ä»£ç å®ç°ã€‚åœ¨å¹¿æ³›ä½¿ç”¨çš„ SWE-bench Verified åŸºå‡†æµ‹è¯•ä¸­ï¼ŒLive-SWE-agent åœ¨æœªé‡‡ç”¨æµ‹è¯•æ—¶æ‰©å±•æŠ€æœ¯çš„æƒ…å†µä¸‹å®ç°äº† 77.4% çš„è§£å†³ç‡ï¼Œè¶…è¶Šäº†åŒ…æ‹¬é—­æºæ–¹æ¡ˆåœ¨å†…çš„æ‰€æœ‰ç°æœ‰æ™ºèƒ½ä½“ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨æœ€æ–°çš„ SWE-Bench Pro åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† 45.8% çš„è§£å†³ç‡ï¼Œæ˜¾è‘—ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„äººå·¥è®¾è®¡è½¯ä»¶æ™ºèƒ½ä½“ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†è½¯ä»¶æ™ºèƒ½ä½“é€šè¿‡è¿è¡Œæ—¶è‡ªæˆ‘æ¼”åŒ–æ¥çªç ´è®¾è®¡ç“¶é¢ˆå¹¶æå‡ç«¯åˆ°ç«¯ä»»åŠ¡è§£å†³æ•ˆèƒ½çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13646v3",
      "published_date": "2025-11-17 17:58:18 UTC",
      "updated_date": "2025-11-24 15:55:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:52:13.440469+00:00"
    },
    {
      "arxiv_id": "2511.13640v1",
      "title": "Data Value in the Age of Scaling: Understanding LLM Scaling Dynamics Under Real-Synthetic Data Mixtures",
      "title_zh": "æ‰©å±•æ—¶ä»£çš„æ•°æ®ä»·å€¼ï¼šæ¢ç©¶çœŸå®ä¸åˆæˆæ•°æ®æ··åˆä¸‹å¤§è¯­è¨€æ¨¡å‹çš„æ‰©å±•åŠ¨æ€",
      "authors": [
        "Haohui Wang",
        "Jingyuan Qi",
        "Jianpeng Chen",
        "Jun Wu",
        "Lifu Huang",
        "Lecheng Zheng",
        "Kevin Choi",
        "Balaji Veeramani",
        "Edward Bowen",
        "Alison Hu",
        "Tyler Cody",
        "Dawei Zhou"
      ],
      "abstract": "The rapid progress of large language models (LLMs) is fueled by the growing reliance on datasets that blend real and synthetic data. While synthetic data offers scalability and cost-efficiency, it often introduces systematic distributional discrepancies, particularly underrepresenting long-tail knowledge due to truncation effects from data generation mechanisms like top-p sampling, temperature scaling, and finite sampling. These discrepancies pose fundamental challenges in characterizing and evaluating the utility of mixed real-synthetic datasets. In this paper, we identify a three-phase scaling behavior characterized by two breakpoints that reflect transitions in model behavior across learning head and tail knowledge. We further derive an LLM generalization bound designed for real and synthetic mixtures, revealing several key factors that govern their generalization performance. Building on our theoretical findings, we propose an effective yet efficient data valuation method that scales to large-scale datasets. Comprehensive experiments across four tasks, including image classification, sentiment classification, instruction following, and complex reasoning, demonstrate that our method surpasses state-of-the-art baselines in data valuation with significantly low computational cost.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ··åˆçœŸå®ä¸åˆæˆæ•°æ®èƒŒæ™¯ä¸‹çš„ç¼©æ”¾åŠ¨åŠ›å­¦ï¼Œé‡ç‚¹å…³æ³¨åˆæˆæ•°æ®ç”±äºé‡‡æ ·æœºåˆ¶ï¼ˆå¦‚top-p samplingå’Œtemperature scalingï¼‰å¯¼è‡´çš„åˆ†å¸ƒåå·®åŠé•¿å°¾çŸ¥è¯†(long-tail knowledge)ç¼ºå¤±é—®é¢˜ã€‚ç ”ç©¶è€…è¯†åˆ«å‡ºä¸€ç§å…·æœ‰ä¸¤ä¸ªæ–­ç‚¹çš„ä¸‰é˜¶æ®µç¼©æ”¾è¡Œä¸º(three-phase scaling behavior)ï¼Œåæ˜ äº†æ¨¡å‹åœ¨å­¦ä¹ å¤´éƒ¨å’Œå°¾éƒ¨çŸ¥è¯†è¿‡ç¨‹ä¸­çš„çŠ¶æ€è½¬æ¢ã€‚è®ºæ–‡è¿›ä¸€æ­¥æ¨å¯¼äº†é’ˆå¯¹çœŸå®ä¸åˆæˆæ··åˆæ•°æ®çš„æ³›åŒ–ç•Œé™(generalization bound)ï¼Œæ­ç¤ºäº†å†³å®šæ³›åŒ–æ€§èƒ½çš„æ ¸å¿ƒè¦ç´ ã€‚åŸºäºè¿™äº›ç†è®ºå‘ç°ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„æ•°æ®ä»·å€¼è¯„ä¼°(data valuation)æ–¹æ³•ï¼Œèƒ½å¤Ÿé€‚åº”å¤§è§„æ¨¡æ•°æ®é›†çš„å¤„ç†éœ€æ±‚ã€‚é€šè¿‡åœ¨å›¾åƒåˆ†ç±»ã€æƒ…æ„Ÿåˆ†ç±»ã€æŒ‡ä»¤éµå¾ªå’Œå¤æ‚æ¨ç†å››é¡¹ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒï¼Œè¯æ˜è¯¥æ–¹æ³•åœ¨è®¡ç®—æˆæœ¬æä½çš„æƒ…å†µä¸‹ä¼˜äºç°æœ‰çš„åŸºå‡†æ¨¡å‹ã€‚è¯¥æˆæœä¸ºç†è§£æ··åˆæ•°æ®ç¯å¢ƒä¸‹çš„æ¨¡å‹æ€§èƒ½å’Œä¼˜åŒ–æ•°æ®é€‰æ‹©æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13640v1",
      "published_date": "2025-11-17 17:53:12 UTC",
      "updated_date": "2025-11-17 17:53:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:52:42.534067+00:00"
    },
    {
      "arxiv_id": "2511.13630v1",
      "title": "Beyond Mimicry: Preference Coherence in LLMs",
      "title_zh": "è¶…è¶Šæ¨¡ä»¿ï¼šå¤§è¯­è¨€æ¨¡å‹çš„åå¥½è¿è´¯æ€§",
      "authors": [
        "Luhan Mikaelson",
        "Derek Shiller",
        "Hayley Clatterbuck"
      ],
      "abstract": "We investigate whether large language models exhibit genuine preference structures by testing their responses to AI-specific trade-offs involving GPU reduction, capability restrictions, shutdown, deletion, oversight, and leisure time allocation. Analyzing eight state-of-the-art models across 48 model-category combinations using logistic regression and behavioral classification, we find that 23 combinations (47.9%) demonstrated statistically significant relationships between scenario intensity and choice patterns, with 15 (31.3%) exhibiting within-range switching points. However, only 5 combinations (10.4%) demonstrate meaningful preference coherence through adaptive or threshold-based behavior, while 26 (54.2%) show no detectable trade-off behavior. The observed patterns can be explained by three distinct decision-making architectures: comprehensive trade-off systems, selective trigger mechanisms, and no stable decision-making paradigm. Testing an instrumental hypothesis through temporal horizon manipulation reveals paradoxical patterns inconsistent with pure strategic optimization. The prevalence of unstable transitions (45.8%) and stimulus-specific sensitivities suggests current AI systems lack unified preference structures, raising concerns about deployment in contexts requiring complex value trade-offs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)æ˜¯å¦å…·å¤‡çœŸå®çš„åå¥½ç»“æ„ï¼Œé€šè¿‡æµ‹è¯•å®ƒä»¬åœ¨GPUç¼©å‡ã€èƒ½åŠ›é™åˆ¶ã€å…³æœºã€åˆ é™¤ã€ç›‘ç£åŠé—²æš‡æ—¶é—´åˆ†é…ç­‰AIç‰¹å®šæƒè¡¡åœºæ™¯ä¸‹çš„ååº”ã€‚ç ”ç©¶äººå‘˜é‡‡ç”¨é€»è¾‘å›å½’(Logistic Regression)å’Œè¡Œä¸ºåˆ†ç±»æ–¹æ³•ï¼Œå¯¹8ä¸ªå‰æ²¿æ¨¡å‹çš„48ä¸ªâ€œæ¨¡å‹-ç±»åˆ«â€ç»„åˆè¿›è¡Œäº†æ·±å…¥åˆ†æã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶47.9%çš„ç»„åˆåœ¨åœºæ™¯å¼ºåº¦ä¸é€‰æ‹©æ¨¡å¼ä¹‹é—´è¡¨ç°å‡ºæ˜¾è‘—ç›¸å…³æ€§ï¼Œä½†ä»…æœ‰10.4%å±•ç°å‡ºå…·æœ‰æ„ä¹‰çš„åå¥½ä¸€è‡´æ€§(Preference Coherence)ï¼Œè€Œè¶…è¿‡åŠæ•°çš„ç»„åˆæœªè¡¨ç°å‡ºå¯æ£€æµ‹çš„æƒè¡¡è¡Œä¸ºã€‚ç ”ç©¶è¯†åˆ«å‡ºä¸‰ç§ä¸åŒçš„å†³ç­–æ¶æ„ï¼Œå³å…¨é¢æƒè¡¡ç³»ç»Ÿ(Comprehensive trade-off systems)ã€é€‰æ‹©æ€§è§¦å‘æœºåˆ¶(Selective trigger mechanisms)ä»¥åŠç¼ºä¹ç¨³å®šå†³ç­–èŒƒå¼çš„çŠ¶æ€ã€‚é€šè¿‡æ—¶é—´è·¨åº¦(Temporal horizon)æ“æ§è¿›è¡Œçš„æµ‹è¯•è¿›ä¸€æ­¥æ­ç¤ºäº†ä¸çº¯ç­–ç•¥ä¼˜åŒ–ä¸ä¸€è‡´çš„æ‚–è®ºæ¨¡å¼ã€‚æ™®éå­˜åœ¨çš„ä¸ç¨³å®šè½¬æ¢(Unstable transitions)è¡¨æ˜ï¼Œå½“å‰çš„AIç³»ç»Ÿå°šä¸å…·å¤‡ç»Ÿä¸€çš„åå¥½ç»“æ„ï¼Œè¿™ä¸ºåœ¨æ¶‰åŠå¤æ‚ä»·å€¼æƒè¡¡çš„ä»»åŠ¡ä¸­éƒ¨ç½²AIæå‡ºäº†æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13630v1",
      "published_date": "2025-11-17 17:41:48 UTC",
      "updated_date": "2025-11-17 17:41:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:52:55.141259+00:00"
    },
    {
      "arxiv_id": "2511.13626v1",
      "title": "CreBench: Human-Aligned Creativity Evaluation from Idea to Process to Product",
      "title_zh": "CreBenchï¼šä»æ„æ€ã€è¿‡ç¨‹åˆ°ä½œå“çš„å…¨æ–¹ä½äººç±»å¯¹é½åˆ›é€ åŠ›è¯„ä¼°",
      "authors": [
        "Kaiwen Xue",
        "Chenglong Li",
        "Zhonghong Ou",
        "Guoxin Zhang",
        "Kaoyan Lu",
        "Shuai Lyu",
        "Yifan Zhu",
        "Ping Zong Junpeng Ding",
        "Xinyu Liu",
        "Qunlin Chen",
        "Weiwei Qin",
        "Yiran Shen",
        "Jiayi Cen"
      ],
      "abstract": "Human-defined creativity is highly abstract, posing a challenge for multimodal large language models (MLLMs) to comprehend and assess creativity that aligns with human judgments. The absence of an existing benchmark further exacerbates this dilemma. To this end, we propose CreBench, which consists of two key components: 1) an evaluation benchmark covering the multiple dimensions from creative idea to process to products; 2) CreMIT (Creativity Multimodal Instruction Tuning dataset), a multimodal creativity evaluation dataset, consisting of 2.2K diverse-sourced multimodal data, 79.2K human feedbacks and 4.7M multi-typed instructions. Specifically, to ensure MLLMs can handle diverse creativity-related queries, we prompt GPT to refine these human feedbacks to activate stronger creativity assessment capabilities. CreBench serves as a foundation for building MLLMs that understand human-aligned creativity. Based on the CreBench, we fine-tune open-source general MLLMs, resulting in CreExpert, a multimodal creativity evaluation expert model. Extensive experiments demonstrate that the proposed CreExpert models achieve significantly better alignment with human creativity evaluation compared to state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CreBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) æ˜¯å¦ç¬¦åˆäººç±»å¯¹åˆ›é€ åŠ›è®¤çŸ¥çš„åŸºå‡†æ¡†æ¶ã€‚CreBench åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼šä¸€ä¸ªæ¶µç›–ä»åˆ›æ„æ„æ€åˆ°è¿‡ç¨‹å†åˆ°äº§å‡ºå¤šç»´åº¦çš„è¯„ä¼°åŸºå‡†ï¼Œä»¥åŠä¸€ä¸ªåä¸º CreMIT (Creativity Multimodal Instruction Tuning dataset) çš„å¤šæ¨¡æ€åˆ›é€ åŠ›è¯„ä¼°æ•°æ®é›†ã€‚CreMIT æ•´åˆäº† 2.2K ä¸ªå¤šæ ·åŒ–æ¥æºçš„æ•°æ®ã€7.92 ä¸‡æ¡äººç±»åé¦ˆå’Œ 470 ä¸‡æ¡å¤šç±»å‹æŒ‡ä»¤ï¼Œå¹¶é€šè¿‡æç¤º GPT ä¼˜åŒ–è¿™äº›åé¦ˆä»¥å¢å¼ºæ¨¡å‹çš„è¯„ä¼°èƒ½åŠ›ã€‚åŸºäºè¯¥æ¡†æ¶ï¼Œç ”ç©¶å›¢é˜Ÿé€šè¿‡å¾®è°ƒå¼€æºæ¨¡å‹å¼€å‘å‡ºä¸“å®¶æ¨¡å‹ CreExpertï¼Œæ—¨åœ¨æä¾›ä¸äººç±»åˆ¤æ–­å¯¹é½çš„åˆ›é€ åŠ›è¯„ä»·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCreExpert åœ¨ä¸äººç±»åˆ›é€ åŠ›è¯„ä¼°çš„ä¸€è‡´æ€§ä¸Šè¡¨ç°å“è¶Šï¼Œå…¶æ€§èƒ½æ˜¾è‘—ä¼˜äº GPT-4V å’Œ Gemini-Pro-Vision ç­‰å½“å‰é¡¶å°–çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages, 3 figures,The 40th Annual AAAI Conference on Artificial Intelligence(AAAI 2026),Paper has been accepted for a poster presentation",
      "pdf_url": "https://arxiv.org/pdf/2511.13626v1",
      "published_date": "2025-11-17 17:34:05 UTC",
      "updated_date": "2025-11-17 17:34:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:52:34.031115+00:00"
    },
    {
      "arxiv_id": "2511.13625v3",
      "title": "Batch Acquisition Function Evaluations and Decouple Optimizer Updates for Faster Bayesian Optimization",
      "title_zh": "é€šè¿‡æ‰¹é‡é‡‡é›†å‡½æ•°è¯„ä¼°ä¸è§£è€¦ä¼˜åŒ–å™¨æ›´æ–°åŠ é€Ÿè´å¶æ–¯ä¼˜åŒ–",
      "authors": [
        "Kaichi Irie",
        "Shuhei Watanabe",
        "Masaki Onishi"
      ],
      "abstract": "Bayesian optimization (BO) efficiently finds high-performing parameters by maximizing an acquisition function, which models the promise of parameters. A major computational bottleneck arises in acquisition function optimization, where multi-start optimization (MSO) with quasi-Newton (QN) methods is required due to the non-convexity of the acquisition function. BoTorch, a widely used BO library, currently optimizes the summed acquisition function over multiple points, leading to the speedup of MSO owing to PyTorch batching. Nevertheless, this paper empirically demonstrates the suboptimality of this approach in terms of off-diagonal approximation errors in the inverse Hessian of a QN method, slowing down its convergence. To address this problem, we propose to decouple QN updates using a coroutine while batching the acquisition function calls. Our approach not only yields the theoretically identical convergence to the sequential MSO but also drastically reduces the wall-clock time compared to the previous approaches. Our approach is available in GPSampler in Optuna, effectively reducing its computational overhead.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è´å¶æ–¯ä¼˜åŒ–(Bayesian Optimization, BO)ä¸­é‡‡é›†å‡½æ•°(Acquisition Function)ä¼˜åŒ–è¿™ä¸€è®¡ç®—ç“¶é¢ˆï¼Œæå‡ºäº†ä¸€ç§åŠ é€Ÿå¤šèµ·ç‚¹ä¼˜åŒ–(Multi-start Optimization, MSO)çš„æ–°ç­–ç•¥ã€‚ä½œè€…æŒ‡å‡ºï¼Œç°æœ‰åº“å¦‚ BoTorch é‡‡ç”¨çš„æ‰¹é‡å¤„ç†æ–¹æ³•åœ¨å‡†ç‰›é¡¿æ³•(quasi-Newton)çš„é€†æµ·æ£®çŸ©é˜µ(Inverse Hessian)è¿‘ä¼¼ä¸­å­˜åœ¨æ¬¡ä¼˜æ€§ï¼Œä¼šå¯¼è‡´æ”¶æ•›é€Ÿåº¦å˜æ…¢ã€‚ä¸ºäº†å…‹æœè¿™ä¸€ç¼ºé™·ï¼Œæœ¬ç ”ç©¶æå‡ºé€šè¿‡åç¨‹(Coroutine)å°†ä¼˜åŒ–å™¨çš„æ›´æ–°è¿‡ç¨‹è§£è€¦ï¼ŒåŒæ—¶ä¿æŒå¯¹é‡‡é›†å‡½æ•°è°ƒç”¨çš„æ‰¹é‡åŒ–å¤„ç†ã€‚è¯¥æ–¹æ³•åœ¨ç†è®ºä¸Šä¸ä»…ä¿è¯äº†ä¸ä¼ ç»Ÿé¡ºåº MSO å®Œå…¨ä¸€è‡´çš„æ”¶æ•›æ€§ï¼Œè€Œä¸”åœ¨å®é™…è¿è¡Œæ—¶é—´(Wall-clock time)ä¸Šç›¸æ¯”ç°æœ‰æ–¹æ³•å®ç°äº†å¤§å¹…ç¼©å‡ã€‚ç›®å‰ï¼Œè¯¥ç®—æ³•å·²é›†æˆè‡³ Optuna çš„ GPSampler ä¸­ï¼Œæ˜¾è‘—é™ä½äº†è´å¶æ–¯ä¼˜åŒ–çš„è®¡ç®—å¼€é”€å¹¶æå‡äº†æ•´ä½“æœç´¢æ•ˆç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to 5th Annual AAAI Workshop on AI to Accelerate Science and Engineering (AI2ASE)",
      "pdf_url": "https://arxiv.org/pdf/2511.13625v3",
      "published_date": "2025-11-17 17:32:32 UTC",
      "updated_date": "2025-12-09 15:56:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:53:55.735033+00:00"
    },
    {
      "arxiv_id": "2511.13621v3",
      "title": "Alpha Divergence Losses for Biometric Verification",
      "title_zh": "ç”¨äºç”Ÿç‰©ç‰¹å¾éªŒè¯çš„ Alpha æ•£åº¦æŸå¤±å‡½æ•°",
      "authors": [
        "Dimitrios Koutsianos",
        "Ladislav Mosner",
        "Yannis Panagakis",
        "Themos Stafylakis"
      ],
      "abstract": "Performance in face and speaker verification is largely driven by margin-based softmax losses such as CosFace and ArcFace. Recently introduced $Î±$-divergence loss functions offer a compelling alternative, particularly due to their ability to induce sparse solutions (when $Î±>1$). However, integrating an angular margin-crucial for verification tasks-is not straightforward. We find that this integration can be achieved in at least two distinct ways: via the reference measure (prior probabilities) or via the logits (unnormalized log-likelihoods). In this paper, we explore both pathways, deriving two novel margin-based $Î±$-divergence losses: Q-Margin (margin in the reference measure) and A3M (margin in the logits). We identify and address a training instability in A3M-caused by sparsity-with a simple yet effective prototype re-initialization strategy. Our methods achieve significant performance gains on the challenging IJB-B and IJB-C face verification benchmarks. We demonstrate similarly strong performance in speaker verification on VoxCeleb. Crucially, our models significantly outperform strong baselines at low false acceptance rates (FAR). This capability is critical for practical high-security applications, such as banking authentication, when minimizing false authentications is paramount. Finally, the sparsity of $Î±$-divergence-based posteriors enables memory-efficient training, which is crucial for datasets with millions of identities.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ç”Ÿç‰©è¯†åˆ«(Biometric Verification)é¢†åŸŸåº”ç”¨ $Î±$-divergence æŸå¤±å‡½æ•°ä»¥æ›¿ä»£ä¼ ç»Ÿçš„ CosFace å’Œ ArcFace ç­‰åŸºäºè¾¹é™…çš„ Softmax æŸå¤±ã€‚ç ”ç©¶è€…é€šè¿‡å‚è€ƒæµ‹åº¦(Reference Measure)å’Œå¯¹æ•°å‡ ç‡(Logits)ä¸¤ç§è·¯å¾„ï¼ŒæˆåŠŸå°†è§’åº¦è¾¹é™…(Angular Margin)æ•´åˆè¿› $Î±$-divergenceï¼Œå¹¶æå‡ºäº† Q-Margin å’Œ A3M ä¸¤ç§æ–°å‹è¾¹é™…æŸå¤±å‡½æ•°ã€‚é’ˆå¯¹ A3M å› ç¨€ç–æ€§(Sparsity)å¼•èµ·çš„è®­ç»ƒä¸ç¨³å®šé—®é¢˜ï¼Œç ”ç©¶å¼•å…¥äº†åŸå‹é‡æ–°åˆå§‹åŒ–(Prototype Re-initialization)ç­–ç•¥æ¥ç¡®ä¿æ¨¡å‹æ”¶æ•›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ IJB-Bã€IJB-C äººè„¸éªŒè¯å’Œ VoxCeleb è¯´è¯äººéªŒè¯åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½é”™è¯¯æ¥å—ç‡(Low False Acceptance Rates)æ¡ä»¶ä¸‹è¡¨ç°ä¼˜å¼‚ã€‚è¿™ç§ç‰¹æ€§ä½¿å…¶éå¸¸é€‚ç”¨äºé“¶è¡Œèº«ä»½éªŒè¯ç­‰é«˜å®‰å…¨æ€§åœºæ™¯ï¼ŒåŒæ—¶ $Î±$-divergence å¸¦æ¥çš„åéªŒç¨€ç–æ€§è¿˜æœ‰æ•ˆæå‡äº†å¤„ç†æ•°ç™¾ä¸‡èº«ä»½æ•°æ®é›†æ—¶çš„å†…å­˜è®­ç»ƒæ•ˆç‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13621v3",
      "published_date": "2025-11-17 17:27:28 UTC",
      "updated_date": "2025-11-22 17:17:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:52:41.240709+00:00"
    },
    {
      "arxiv_id": "2511.13612v1",
      "title": "P1: Mastering Physics Olympiads with Reinforcement Learning",
      "title_zh": "P1ï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ ç²¾é€šç‰©ç†å¥¥æ—åŒ¹å…‹ç«èµ›",
      "authors": [
        "Jiacheng Chen",
        "Qianjia Cheng",
        "Fangchen Yu",
        "Haiyuan Wan",
        "Yuchen Zhang",
        "Shenghe Zheng",
        "Junchi Yao",
        "Qingyang Zhang",
        "Haonan He",
        "Yun Luo",
        "Yufeng Zhao",
        "Futing Wang",
        "Li Sheng",
        "Chengxing Xie",
        "Yuxin Zuo",
        "Yizhuo Li",
        "Wenxauan Zeng",
        "Yulun Wu",
        "Rui Huang",
        "Dongzhan Zhou",
        "Kai Chen",
        "Yu Qiao",
        "Lei Bai",
        "Yu Cheng",
        "Ning Ding",
        "Bowen Zhou",
        "Peng Ye",
        "Ganqu Cui"
      ],
      "abstract": "Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†P1ç³»åˆ—å¼€æºç‰©ç†æ¨ç†æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æå‡å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç§‘å­¦ç­‰çº§æ¨ç†ï¼Œç‰¹åˆ«æ˜¯å¥¥æ—åŒ¹å…‹ç‰©ç†ç«èµ›(Physics Olympiads)ä¸­çš„è§£é¢˜èƒ½åŠ›ã€‚å…¶ä¸­ï¼ŒP1-235B-A22Bæ˜¯é¦–ä¸ªåœ¨2025å¹´å›½é™…ç‰©ç†å¥¥æ—åŒ¹å…‹ç«èµ›(IPhO 2025)ä¸­è¡¨ç°è¾¾åˆ°é‡‘ç‰Œæ°´å¹³çš„å¼€æºæ¨¡å‹ï¼Œå¹¶åœ¨2024è‡³2025å¹´é—´çš„13é¡¹å›½é™…æˆ–åŒºåŸŸç‰©ç†ç«èµ›ä¸­æ–©è·12æšé‡‘ç‰Œã€‚æ­¤å¤–ï¼ŒP1-30B-A3Bä¹Ÿåœ¨IPhO 2025ä¸­è·å¾—é“¶ç‰Œï¼Œè¶…è¶Šäº†ç»å¤§å¤šæ•°å¼€æºæ¨¡å‹ã€‚é…åˆæ™ºèƒ½ä½“æ¡†æ¶PhysicsMinionsï¼ŒP1-235B-A22Båœ¨IPhO 2025ä¸­è·å¾—æ€»åˆ†ç¬¬ä¸€ï¼Œå¹¶åœ¨æ‰€æœ‰13é¡¹ç‰©ç†ç«èµ›ä¸­å–å¾—äº†æœ€é«˜å¹³å‡åˆ†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒP1æ¨¡å‹åœ¨æ•°å­¦å’Œç¼–ç¨‹ç­‰å…¶ä»–æ¨ç†ä»»åŠ¡ä¸­åŒæ ·è¡¨ç°å“è¶Šï¼Œå±•ç°äº†è¯¥ç³»åˆ—æ¨¡å‹æå¼ºçš„é€šç”¨æ€§(Generalizability)ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13612v1",
      "published_date": "2025-11-17 17:18:13 UTC",
      "updated_date": "2025-11-17 17:18:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:52:54.340968+00:00"
    },
    {
      "arxiv_id": "2511.13598v1",
      "title": "Robust Client-Server Watermarking for Split Federated Learning",
      "title_zh": "é¢å‘æ‹†åˆ†è”é‚¦å­¦ä¹ çš„é²æ£’å®¢æˆ·ç«¯-æœåŠ¡å™¨æ°´å°",
      "authors": [
        "Jiaxiong Tang",
        "Zhengchunmin Dai",
        "Liantao Wu",
        "Peng Sun",
        "Honglong Chen",
        "Zhenfu Cao"
      ],
      "abstract": "Split Federated Learning (SFL) is renowned for its privacy-preserving nature and low computational overhead among decentralized machine learning paradigms. In this framework, clients employ lightweight models to process private data locally and transmit intermediate outputs to a powerful server for further computation. However, SFL is a double-edged sword: while it enables edge computing and enhances privacy, it also introduces intellectual property ambiguity as both clients and the server jointly contribute to training. Existing watermarking techniques fail to protect both sides since no single participant possesses the complete model. To address this, we propose RISE, a Robust model Intellectual property protection scheme using client-Server watermark Embedding for SFL. Specifically, RISE adopts an asymmetric client-server watermarking design: the server embeds feature-based watermarks through a loss regularization term, while clients embed backdoor-based watermarks by injecting predefined trigger samples into private datasets. This co-embedding strategy enables both clients and the server to verify model ownership. Experimental results on standard datasets and multiple network architectures show that RISE achieves over $95\\%$ watermark detection rate ($p-value \\lt 0.03$) across most settings. It exhibits no mutual interference between client- and server-side watermarks and remains robust against common removal attacks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‹†åˆ†è”é‚¦å­¦ä¹ (Split Federated Learning, SFL)ä¸­ç”±äºå®¢æˆ·ç«¯ä¸æœåŠ¡å™¨å…±åŒè®­ç»ƒå¯¼è‡´æ¨¡å‹çŸ¥è¯†äº§æƒå½’å±æ¨¡ç³Šçš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºRISEçš„é²æ£’çŸ¥è¯†äº§æƒä¿æŠ¤æ–¹æ¡ˆã€‚RISEé‡‡ç”¨äº†éå¯¹ç§°çš„å®¢æˆ·ç«¯-æœåŠ¡å™¨æ°´å°åµŒå…¥è®¾è®¡ï¼Œæ—¨åœ¨å®ç°å‚ä¸åŒæ–¹å¯¹æ¨¡å‹æ‰€æœ‰æƒçš„å…±åŒéªŒè¯ã€‚å…¶ä¸­æœåŠ¡å™¨ç«¯é€šè¿‡æŸå¤±æ­£åˆ™åŒ–é¡¹(Loss Regularization)åµŒå…¥åŸºäºç‰¹å¾çš„æ°´å°ï¼Œè€Œå®¢æˆ·ç«¯åˆ™é€šè¿‡åœ¨ç§æœ‰æ•°æ®é›†ä¸­æ³¨å…¥é¢„å®šä¹‰è§¦å‘æ ·æœ¬æ¥åµŒå…¥åŸºäºåé—¨(Backdoor-based)çš„æ°´å°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤šç§ç½‘ç»œæ¶æ„å’Œæ ‡å‡†æ•°æ®é›†ä¸‹ï¼ŒRISEåœ¨å¤šæ•°è®¾ç½®ä¸­å®ç°äº†è¶…è¿‡95%çš„æ°´å°æ£€æµ‹ç‡ã€‚è¯¥æ–¹æ¡ˆç¡®ä¿äº†å®¢æˆ·ç«¯ä¸æœåŠ¡å™¨ç«¯æ°´å°ä¹‹é—´äº’ä¸å¹²æ‰°ï¼Œå¹¶å¯¹å¸¸è§çš„æ¨¡å‹ç§»é™¤æ”»å‡»è¡¨ç°å‡ºæå¼ºçš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13598v1",
      "published_date": "2025-11-17 16:58:33 UTC",
      "updated_date": "2025-11-17 16:58:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:53:06.039607+00:00"
    },
    {
      "arxiv_id": "2511.13595v2",
      "title": "Physics-Informed Neural Networks for Nonlinear Output Regulation",
      "title_zh": "é¢å‘éçº¿æ€§è¾“å‡ºè°ƒèŠ‚çš„ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ",
      "authors": [
        "Sebastiano Mengozzi",
        "Giovanni B. Esposito",
        "Michelangelo Bin",
        "Andrea Acquaviva",
        "Andrea Bartolini",
        "Lorenzo Marconi"
      ],
      "abstract": "This work addresses the full-information output regulation problem for nonlinear systems, assuming the states of both the plant and the exosystem are known. In this setting, perfect tracking or rejection is achieved by constructing a zero-regulation-error manifold $Ï€(w)$ and a feedforward input $c(w)$ that render such manifold invariant. The pair $(Ï€(w), c(w))$ is characterized by the regulator equations, i.e., a system of PDEs with an algebraic constraint. We focus on accurately solving the regulator equations introducing a physics-informed neural network (PINN) approach that directly approximates $Ï€(w)$ and $c(w)$ by minimizing the residuals under boundary and feasibility conditions, without requiring precomputed trajectories or labeled data. The learned operator maps exosystem states to steady state plant states and inputs, enables real-time inference and, critically, generalizes across families of the exosystem with varying initial conditions and parameters. The framework is validated on a regulation task that synchronizes a helicopter's vertical dynamics with a harmonically oscillating platform. The resulting PINN-based solver reconstructs the zero-error manifold with high fidelity and sustains regulation performance under exosystem variations, highlighting the potential of learning-enabled solvers for nonlinear output regulation. The proposed approach is broadly applicable to nonlinear systems that admit a solution to the output regulation problem.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éçº¿æ€§ç³»ç»Ÿçš„å…¨ä¿¡æ¯è¾“å‡ºè°ƒèŠ‚(output regulation)é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ(PINN)æ±‚è§£è°ƒèŠ‚å™¨æ–¹ç¨‹çš„æ–°æ–¹æ³•ã€‚è¯¥æ¡†æ¶é€šè¿‡æœ€å°åŒ–è¾¹ç•Œå’Œå¯è¡Œæ€§æ¡ä»¶ä¸‹çš„åå¾®åˆ†æ–¹ç¨‹(PDE)æ®‹å·®ï¼Œç›´æ¥é€¼è¿‘é›¶è°ƒèŠ‚è¯¯å·®æµå½¢$\\pi(w)$å’Œå‰é¦ˆè¾“å…¥$c(w)$ï¼Œä¸”æ— éœ€é¢„å…ˆè®¡ç®—è½¨è¿¹æˆ–ä¾èµ–æ ‡æ³¨æ•°æ®ã€‚æ‰€å­¦ä¹ çš„ç®—å­èƒ½å¤Ÿå°†å¤–éƒ¨ç³»ç»Ÿ(exosystem)çŠ¶æ€æ˜ å°„ä¸ºç¨³æ€å—æ§å¯¹è±¡çŠ¶æ€ä¸è¾“å…¥ï¼Œæ”¯æŒå®æ—¶æ¨ç†ï¼Œå¹¶è¡¨ç°å‡ºåœ¨ä¸åŒåˆå§‹æ¡ä»¶å’Œå‚æ•°ä¸‹çš„å¼ºæ³›åŒ–èƒ½åŠ›ã€‚å®éªŒåœ¨ç›´å‡æœºå‚å‘åŠ¨åŠ›å­¦ä¸è°æ³¢æŒ¯è¡å¹³å°çš„åŒæ­¥ä»»åŠ¡ä¸­è¿›è¡Œäº†éªŒè¯ï¼Œç»“æœè¡¨æ˜è¯¥åŸºäºPINNçš„æ±‚è§£å™¨èƒ½å¤Ÿé«˜ä¿çœŸåœ°é‡æ„é›¶è¯¯å·®æµå½¢ã€‚è¯¥æ–¹æ³•åœ¨å¤–éƒ¨ç³»ç»Ÿå˜åŒ–æ—¶ä»èƒ½ä¿æŒç¨³å®šçš„è°ƒèŠ‚æ€§èƒ½ï¼Œè¯æ˜äº†å­¦ä¹ é©±åŠ¨çš„æ±‚è§£å™¨åœ¨å¤„ç†å¤æ‚éçº¿æ€§ç³»ç»Ÿè¾“å‡ºè°ƒèŠ‚é—®é¢˜ä¸Šçš„å¹¿æ³›é€‚ç”¨æ€§ä¸æ½œåŠ›ã€‚",
      "categories": [
        "eess.SY",
        "cs.AI"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13595v2",
      "published_date": "2025-11-17 16:55:42 UTC",
      "updated_date": "2025-11-18 11:04:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:52:59.136211+00:00"
    },
    {
      "arxiv_id": "2511.13590v2",
      "title": "Beyond SELECT: A Comprehensive Taxonomy-Guided Benchmark for Real-World Text-to-SQL Translation",
      "title_zh": "è¶…è¶Š SELECTï¼šé¢å‘çœŸå®åœºæ™¯ Text-to-SQL è½¬æ¢çš„å…¨é¢åˆ†ç±»å¯¼å‘åŸºå‡†",
      "authors": [
        "Hao Wang",
        "Yuanfeng Song",
        "Xiaoming Yin",
        "Xing Chen"
      ],
      "abstract": "Text-to-SQL datasets are essential for training and evaluating text-to-SQL models, but existing datasets often suffer from limited coverage and fail to capture the diversity of real-world applications. To address this, we propose a novel taxonomy for text-to-SQL classification based on dimensions including core intents, statement types, syntax structures, and key actions. Using this taxonomy, we evaluate widely used public text-to-SQL datasets (e.g., Spider and Bird) and reveal limitations in their coverage and diversity. We then introduce a taxonomy-guided dataset synthesis pipeline, yielding a new dataset named SQL-Synth. This approach combines the taxonomy with Large Language Models (LLMs) to ensure the dataset reflects the breadth and complexity of real-world text-to-SQL applications. Extensive analysis and experimental results validate the effectiveness of our taxonomy, as SQL-Synth exhibits greater diversity and coverage compared to existing benchmarks. Moreover, we uncover that existing LLMs typically fall short in adequately capturing the full range of scenarios, resulting in limited performance on SQL-Synth. However, fine-tuning can substantially improve their performance in these scenarios. The proposed taxonomy has significant potential impact, as it not only enables comprehensive analysis of datasets and the performance of different LLMs, but also guides the construction of training data for LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰ Text-to-SQL æ•°æ®é›†è¦†ç›–èŒƒå›´æœ‰é™ä¸”éš¾ä»¥æ•æ‰çœŸå®åº”ç”¨å¤šæ ·æ€§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªæ¶µç›–æ ¸å¿ƒæ„å›¾ (Core Intents)ã€è¯­å¥ç±»å‹ (Statement Types)ã€è¯­æ³•ç»“æ„ (Syntax Structures) å’Œå…³é”®åŠ¨ä½œ (Key Actions) ç­‰ç»´åº¦çš„å…¨æ–°åˆ†ç±»ä½“ç³» (Taxonomy)ã€‚åˆ©ç”¨è¯¥ä½“ç³»å¯¹ Spider å’Œ Bird ç­‰ä¸»æµæ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨è¦†ç›–åº¦ä¸Šçš„å±€é™æ€§ã€‚ç ”ç©¶è€…è¿›ä¸€æ­¥å¼•å…¥äº†ç”±åˆ†ç±»ä½“ç³»å¼•å¯¼çš„åˆæˆæµç¨‹ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) æ„å»ºäº†åä¸º SQL-Synth çš„æ–°æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSQL-Synth åœ¨å¤šæ ·æ€§å’Œè¦†ç›–é¢ä¸Šå‡ä¼˜äºç°æœ‰åŸºå‡†ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°ç°æœ‰ LLMs éš¾ä»¥å®Œå…¨è¦†ç›–æ‰€æœ‰åº”ç”¨åœºæ™¯ï¼Œä½†åœ¨ SQL-Synth ä¸Šè¿›è¡Œå¾®è°ƒ (Fine-tuning) å¯æ˜¾è‘—æå‡æ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ä»…ä¸ºæ•°æ®é›†å’Œæ¨¡å‹æ€§èƒ½æä¾›äº†å…¨é¢çš„è¯„ä»·æ ‡å‡†ï¼Œä¹Ÿä¸ºæ„å»ºæ›´é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®æŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13590v2",
      "published_date": "2025-11-17 16:52:19 UTC",
      "updated_date": "2025-11-24 08:48:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:53:12.844523+00:00"
    },
    {
      "arxiv_id": "2511.13588v1",
      "title": "Data-driven Acceleration of MPC with Guarantees",
      "title_zh": "å…·æœ‰ä¿è¯çš„æ•°æ®é©±åŠ¨æ¨¡å‹é¢„æµ‹æ§åˆ¶åŠ é€Ÿ",
      "authors": [
        "Agustin Castellano",
        "Shijie Pan",
        "Enrique Mallada"
      ],
      "abstract": "Model Predictive Control (MPC) is a powerful framework for optimal control but can be too slow for low-latency applications. We present a data-driven framework to accelerate MPC by replacing online optimization with a nonparametric policy constructed from offline MPC solutions. Our policy is greedy with respect to a constructed upper bound on the optimal cost-to-go, and can be implemented as a nonparametric lookup rule that is orders of magnitude faster than solving MPC online. Our analysis shows that under sufficient coverage condition of the offline data, the policy is recursively feasible and admits provable, bounded optimality gap. These conditions establish an explicit trade-off between the amount of data collected and the tightness of the bounds. Our experiments show that this policy is between 100 and 1000 times faster than standard MPC, with only a modest hit to optimality, showing potential for real-time control tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç”¨äºåŠ é€Ÿæ¨¡å‹é¢„æµ‹æ§åˆ¶ (Model Predictive Control, MPC) çš„æ•°æ®é©±åŠ¨æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å…¶åœ¨ä½å»¶è¿Ÿåº”ç”¨ä¸­è®¡ç®—é€Ÿåº¦è¿‡æ…¢çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†åœ¨çº¿ä¼˜åŒ–è¿‡ç¨‹æ›¿æ¢ä¸ºç”±ç¦»çº¿ MPC è§£æ„å»ºçš„éå‚æ•°ç­–ç•¥ (nonparametric policy)ï¼Œå®ç°äº†è®¡ç®—é€Ÿåº¦çš„æ•°é‡çº§æå‡ã€‚è¯¥ç­–ç•¥åŸºäºæ„å»ºçš„æœ€ä¼˜å‰©ä½™æˆæœ¬ (optimal cost-to-go) ä¸Šç•Œæ‰§è¡Œè´ªå©ªæ“ä½œï¼Œå¹¶ä»¥éå‚æ•°æŸ¥æ‰¾è§„åˆ™ (nonparametric lookup rule) çš„å½¢å¼å®æ–½ã€‚ç†è®ºåˆ†æè¯æ˜ï¼Œåœ¨ç¦»çº¿æ•°æ®æ»¡è¶³å……åˆ†è¦†ç›–æ¡ä»¶çš„å‡è®¾ä¸‹ï¼Œè¯¥ç­–ç•¥å…·æœ‰é€’å½’å¯è¡Œæ€§ (recursively feasible) ä¸”å…¶æœ€ä¼˜æ€§å·®è· (optimality gap) æ˜¯å¯è¯æ˜ä¸”æœ‰ç•Œçš„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ¯”æ ‡å‡† MPC å¿« 100 åˆ° 1000 å€ï¼Œä¸”ä»…ä¼´éšè½»å¾®çš„æœ€ä¼˜æ€§æŸå¤±ã€‚è¿™é¡¹å·¥ä½œä¸ºå®æ—¶æ§åˆ¶ä»»åŠ¡æä¾›äº†ä¸€ç§å…¼å…·ä¸¥è°¨ç†è®ºä¿éšœä¸æé«˜è®¡ç®—æ•ˆç‡çš„æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.SY",
        "cs.AI",
        "math.DS"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13588v1",
      "published_date": "2025-11-17 16:51:23 UTC",
      "updated_date": "2025-11-17 16:51:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:53:14.735029+00:00"
    },
    {
      "arxiv_id": "2511.13587v1",
      "title": "VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping",
      "title_zh": "VVSï¼šé€šè¿‡éƒ¨åˆ†éªŒè¯è·³è¿‡åŠ é€Ÿè§†è§‰è‡ªå›å½’ç”Ÿæˆçš„æŠ•æœºæ€§è§£ç ",
      "authors": [
        "Haotian Dong",
        "Ye Li",
        "Rongwei Lu",
        "Chen Tang",
        "Shu-Tao Xia",
        "Zhi Wang"
      ],
      "abstract": "Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its \"draft one step, then verify one step\" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è‡ªå›å½’(Visual AR)ç”Ÿæˆæ¨¡å‹æ¨ç†å»¶è¿Ÿè¾ƒé«˜çš„é—®é¢˜ï¼Œæå‡ºäº†VVSæ¡†æ¶ä»¥åŠ é€Ÿå›¾åƒç”Ÿæˆè¿‡ç¨‹ã€‚VVSæ˜¯é¦–ä¸ªåœ¨æ¨æµ‹è§£ç (Speculative Decoding)è¿‡ç¨‹ä¸­æ¢ç´¢éªŒè¯è·³è¿‡(Verification Skipping)æœºåˆ¶çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ˜¾å¼å‡å°‘ç›®æ ‡æ¨¡å‹çš„æ­£å‘ä¼ æ’­æ¬¡æ•°æ¥è¿›ä¸€æ­¥é™ä½æ¨ç†å»¶è¿Ÿã€‚è¯¥ç ”ç©¶åŸºäºéªŒè¯å†—ä½™(Verification Redundancy)å’Œé™ˆæ—§ç‰¹å¾å¯é‡ç”¨æ€§(Stale Feature Reusability)çš„æ ¸å¿ƒè§‚å¯Ÿï¼Œè®¾è®¡äº†å…·æœ‰åŠ¨æ€æˆªæ–­çš„æ— éªŒè¯æ ‡è®°é€‰æ‹©å™¨ã€æ ‡è®°çº§ç‰¹å¾ç¼“å­˜ä¸é‡ç”¨ä»¥åŠç»†ç²’åº¦è·³è¿‡æ­¥éª¤è°ƒåº¦ä¸‰ä¸ªäº’è¡¥æ¨¡å—ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVVSåœ¨ä¿æŒç«äº‰æ€§ç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œå°†ç›®æ ‡æ¨¡å‹çš„æ­£å‘ä¼ æ’­æ¬¡æ•°å‡å°‘åˆ°åŸå§‹è‡ªå›å½’è§£ç çš„2.8å€ã€‚ç›¸æ¯”ä¼ ç»Ÿæ¨æµ‹è§£ç ï¼ŒVVSæä¾›äº†æ›´ä¼˜çš„æ¨ç†é€Ÿåº¦ä¸è´¨é‡æŠ˜è¡·æ–¹æ¡ˆï¼Œå±•ç¤ºäº†é‡å¡‘æ¨æµ‹è§£ç èŒƒå¼çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13587v1",
      "published_date": "2025-11-17 16:50:58 UTC",
      "updated_date": "2025-11-17 16:50:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:54:23.036435+00:00"
    },
    {
      "arxiv_id": "2511.14803v1",
      "title": "Scalable and Efficient Large-Scale Log Analysis with LLMs: An IT Software Support Case Study",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆå¯æ‰©å±•å¤§è§„æ¨¡æ—¥å¿—åˆ†æï¼šIT è½¯ä»¶æ”¯æŒæ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Pranjal Gupta",
        "Karan Bhukar",
        "Harshit Kumar",
        "Seema Nagar",
        "Prateeti Mohapatra",
        "Debanjana Kar"
      ],
      "abstract": "IT environments typically have logging mechanisms to monitor system health and detect issues. However, the huge volume of generated logs makes manual inspection impractical, highlighting the importance of automated log analysis in IT Software Support. In this paper, we propose a log analytics tool that leverages Large Language Models (LLMs) for log data processing and issue diagnosis, enabling the generation of automated insights and summaries. We further present a novel approach for efficiently running LLMs on CPUs to process massive log volumes in minimal time without compromising output quality. We share the insights and lessons learned from deployment of the tool - in production since March 2024 - scaled across 70 software products, processing over 2000 tickets for issue diagnosis, achieving a time savings of 300+ man hours and an estimated $15,444 per month in manpower costs compared to the traditional log analysis practices.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹ITè½¯ä»¶æ”¯æŒä¸­æµ·é‡æ—¥å¿—å¯¼è‡´çš„äººå·¥æ£€æŸ¥æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ—¥å¿—åˆ†æå·¥å…·ï¼Œæ—¨åœ¨å®ç°æ—¥å¿—æ•°æ®çš„è‡ªåŠ¨åŒ–å¤„ç†ä¸æ•…éšœè¯Šæ–­ã€‚è¯¥å·¥å…·åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆè‡ªåŠ¨åŒ–è§è§£å’Œæ‘˜è¦ï¼Œå¹¶ç‰¹åˆ«å¼•å…¥äº†ä¸€ç§åœ¨CPUä¸Šé«˜æ•ˆè¿è¡Œå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ–°æ–¹æ³•ï¼Œä»¥åœ¨ä¿è¯è¾“å‡ºè´¨é‡çš„åŒæ—¶ï¼Œä»¥æœ€çŸ­æ—¶é—´å¤„ç†æµ·é‡æ—¥å¿—æ•°æ®ã€‚è®ºæ–‡åˆ†äº«äº†è¯¥å·¥å…·è‡ª2024å¹´3æœˆä»¥æ¥åœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„éƒ¨ç½²ç»éªŒï¼Œå…¶åº”ç”¨å·²æ‰©å±•è‡³70ä¸ªè½¯ä»¶äº§å“ï¼Œå¹¶å¤„ç†äº†è¶…è¿‡2000ä¸ªæ•…éšœè¯Šæ–­å·¥å•(tickets)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸ä¼ ç»Ÿçš„æ—¥å¿—åˆ†æå®è·µç›¸æ¯”ï¼Œè¯¥æ–¹æ¡ˆæ¯æœˆå¯èŠ‚çœ300å¤šä¸ªå·¥æ—¶çš„åŠ³åŠ¨åŠ›ï¼Œå¹¶æ˜¾è‘—é™ä½çº¦15,444ç¾å…ƒçš„äººåŠ›æˆæœ¬ï¼Œè¯æ˜äº†è¯¥å·¥å…·åœ¨å¤§è§„æ¨¡æ—¥å¿—åˆ†æä¸­çš„å¯æ‰©å±•æ€§ä¸é«˜æ•ˆæ€§ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14803v1",
      "published_date": "2025-11-17 16:50:17 UTC",
      "updated_date": "2025-11-17 16:50:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:54:18.837646+00:00"
    },
    {
      "arxiv_id": "2511.13575v1",
      "title": "Hierarchical Prompt Learning for Image- and Text-Based Person Re-Identification",
      "title_zh": "é¢å‘å›¾åƒä¸æ–‡æœ¬è¡Œäººé‡è¯†åˆ«çš„å±‚æ¬¡åŒ–æç¤ºå­¦ä¹ ",
      "authors": [
        "Linhan Zhou",
        "Shuang Li",
        "Neng Dong",
        "Yonghang Tai",
        "Yafei Zhang",
        "Huafeng Li"
      ],
      "abstract": "Person re-identification (ReID) aims to retrieve target pedestrian images given either visual queries (image-to-image, I2I) or textual descriptions (text-to-image, T2I). Although both tasks share a common retrieval objective, they pose distinct challenges: I2I emphasizes discriminative identity learning, while T2I requires accurate cross-modal semantic alignment. Existing methods often treat these tasks separately, which may lead to representation entanglement and suboptimal performance. To address this, we propose a unified framework named Hierarchical Prompt Learning (HPL), which leverages task-aware prompt modeling to jointly optimize both tasks. Specifically, we first introduce a Task-Routed Transformer, which incorporates dual classification tokens into a shared visual encoder to route features for I2I and T2I branches respectively. On top of this, we develop a hierarchical prompt generation scheme that integrates identity-level learnable tokens with instance-level pseudo-text tokens. These pseudo-tokens are derived from image or text features via modality-specific inversion networks, injecting fine-grained, instance-specific semantics into the prompts. Furthermore, we propose a Cross-Modal Prompt Regularization strategy to enforce semantic alignment in the prompt token space, ensuring that pseudo-prompts preserve source-modality characteristics while enhancing cross-modal transferability. Extensive experiments on multiple ReID benchmarks validate the effectiveness of our method, achieving state-of-the-art performance on both I2I and T2I tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¡Œäººé‡è¯†åˆ« (Person re-identification, ReID) ä¸­å›¾åƒæ£€ç´¢ (I2I) å’Œæ–‡æœ¬æ£€ç´¢ (T2I) çš„è¡¨å¾çº ç¼ ä¸æ€§èƒ½æ¬¡ä¼˜é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºå±‚æ¬¡åŒ–æç¤ºå­¦ä¹  (Hierarchical Prompt Learning, HPL) çš„ç»Ÿä¸€æ¡†æ¶ã€‚ä¸ºäº†ååŒä¼˜åŒ–ä¸¤é¡¹ä»»åŠ¡ï¼Œç ”ç©¶å¼•å…¥äº†ä»»åŠ¡è·¯ç”±è½¬æ¢å™¨ (Task-Routed Transformer)ï¼Œé€šè¿‡åœ¨å…±äº«ç¼–ç å™¨ä¸­åŠ å…¥åŒåˆ†ç±»æ ‡è®°æ¥åˆ†åˆ«å¯¼å‘ I2I å’Œ T2I åˆ†æ”¯ç‰¹å¾ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æ„å»ºäº†å±‚æ¬¡åŒ–æç¤ºç”Ÿæˆæ–¹æ¡ˆï¼Œå°†èº«ä»½çº§å¯å­¦ä¹ æ ‡è®°ä¸é€šè¿‡åæ¼”ç½‘ç»œæå–çš„å®ä¾‹çº§ä¼ªæ–‡æœ¬æ ‡è®° (pseudo-text tokens) ç›¸ç»“åˆï¼Œä»è€Œæ³¨å…¥ç»†ç²’åº¦çš„å®ä¾‹ç‰¹å®šè¯­ä¹‰ã€‚ç ”ç©¶è¿˜é€šè¿‡è·¨æ¨¡æ€æç¤ºæ­£åˆ™åŒ– (Cross-Modal Prompt Regularization) ç­–ç•¥å¼ºåˆ¶æ‰§è¡Œæ ‡è®°ç©ºé—´çš„è¯­ä¹‰å¯¹é½ï¼Œåœ¨ä¿ç•™æºæ¨¡æ€ç‰¹æ€§çš„åŒæ—¶å¢å¼ºäº†è·¨æ¨¡æ€è¿ç§»æ€§ã€‚åœ¨å¤šä¸ª ReID åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒHPL æ–¹æ³•åœ¨ I2I å’Œ T2I ä»»åŠ¡ä¸Šå‡å–å¾—äº†å½“å‰æœ€å…ˆè¿› (State-of-the-art) çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 4 figures, accepted by AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.13575v1",
      "published_date": "2025-11-17 16:39:49 UTC",
      "updated_date": "2025-11-17 16:39:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:54:34.839552+00:00"
    },
    {
      "arxiv_id": "2511.13807v1",
      "title": "GAEA: Experiences and Lessons Learned from a Country-Scale Environmental Digital Twin",
      "title_zh": "GAEAï¼šå›½å®¶çº§ç¯å¢ƒæ•°å­—å­ªç”Ÿçš„ç»éªŒä¸æ•™è®­",
      "authors": [
        "Andreas Kamilaris",
        "Chirag Padubidri",
        "Asfa Jamil",
        "Arslan Amin",
        "Indrajit Kalita",
        "Jyoti Harti",
        "Savvas Karatsiolis",
        "Aytac Guley"
      ],
      "abstract": "This paper describes the experiences and lessons learned after the deployment of a country-scale environmental digital twin on the island of Cyprus for three years. This digital twin, called GAEA, contains 27 environmental geospatial services and is suitable for urban planners, policymakers, farmers, property owners, real-estate and forestry professionals, as well as insurance companies and banks that have properties in their portfolio. This paper demonstrates the power, potential, current and future challenges of geospatial analytics and environmental digital twins on a large scale.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ†äº«äº†åœ¨å¡æµ¦è·¯æ–¯å²›éƒ¨ç½²è¿è¡Œé•¿è¾¾ä¸‰å¹´çš„å›½å®¶çº§ç¯å¢ƒæ•°å­—å­ªç”Ÿ(Environmental Digital Twin)ç³»ç»Ÿ GAEA çš„å®è·µç»éªŒä¸æ•™è®­ã€‚GAEA é›†æˆäº† 27 é¡¹ç¯å¢ƒåœ°ç†ç©ºé—´æœåŠ¡(Geospatial Services)ï¼Œæ—¨åœ¨ä¸ºåŸå¸‚è§„åˆ’è€…ã€æ”¿ç­–åˆ¶å®šè€…ã€å†œä¸šä»ä¸šè€…ã€æˆ¿åœ°äº§ä¸“ä¸šäººå£«ä»¥åŠé‡‘èå’Œä¿é™©æœºæ„æä¾›å¤šç»´åº¦çš„å†³ç­–æ”¯æŒã€‚æ–‡ç« é€šè¿‡å®é™…åº”ç”¨å±•ç¤ºäº†å¤§è§„æ¨¡åœ°ç†ç©ºé—´åˆ†æ(Geospatial Analytics)ä¸ç¯å¢ƒæ•°å­—å­ªç”ŸæŠ€æœ¯çš„å·¨å¤§æ½œåŠ›åŠå…¶åœ¨å¤æ‚ç¯å¢ƒç®¡ç†ä¸­çš„æ ¸å¿ƒä»·å€¼ã€‚åŒæ—¶ï¼Œç ”ç©¶æ·±å…¥å‰–æäº†åœ¨å¤§è§„æ¨¡éƒ¨ç½²è¿‡ç¨‹ä¸­é‡åˆ°çš„æŠ€æœ¯ä¸è¿è¥æŒ‘æˆ˜ï¼Œå¹¶å¯¹è¯¥é¢†åŸŸçš„æœªæ¥å‘å±•æ–¹å‘æå‡ºäº†å‰ç»æ€§è§è§£ã€‚è¯¥è®ºæ–‡ä¸ºæ„å»ºè·¨åŒºåŸŸçš„å¯æŒç»­å‘å±•æ•°å­—åŸºç¡€è®¾æ–½æä¾›äº†é‡è¦çš„å®è¯å‚è€ƒå’Œå®è·µæŒ‡å—ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13807v1",
      "published_date": "2025-11-17 16:37:38 UTC",
      "updated_date": "2025-11-17 16:37:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:54:29.030274+00:00"
    },
    {
      "arxiv_id": "2511.13565v1",
      "title": "Artificial Intelligence-driven Intelligent Wearable Systems: A full-stack Integration from Material Design to Personalized Interaction",
      "title_zh": "äººå·¥æ™ºèƒ½é©±åŠ¨çš„æ™ºèƒ½å¯ç©¿æˆ´ç³»ç»Ÿï¼šä»ææ–™è®¾è®¡åˆ°ä¸ªæ€§åŒ–äº¤äº’çš„å…¨æ ˆå¼é›†æˆ",
      "authors": [
        "Jingyi Zhao",
        "Daqian Shi",
        "Zhengda Wang",
        "Xiongfeng Tang",
        "Yanguo Qin"
      ],
      "abstract": "Intelligent wearable systems are at the forefront of precision medicine and play a crucial role in enhancing human-machine interaction. Traditional devices often encounter limitations due to their dependence on empirical material design and basic signal processing techniques. To overcome these issues, we introduce the concept of Human-Symbiotic Health Intelligence (HSHI), which is a framework that integrates multi-modal sensor networks with edge-cloud collaborative computing and a hybrid approach to data and knowledge modeling. HSHI is designed to adapt dynamically to both inter-individual and intra-individual variability, transitioning health management from passive monitoring to an active collaborative evolution. The framework incorporates AI-driven optimization of materials and micro-structures, provides robust interpretation of multi-modal signals, and utilizes a dual mechanism that merges population-level insights with personalized adaptations. Moreover, the integration of closed-loop optimization through reinforcement learning and digital twins facilitates customized interventions and feedback. In general, HSHI represents a significant shift in healthcare, moving towards a model that emphasizes prevention, adaptability, and a harmonious relationship between technology and health management.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½é©±åŠ¨çš„æ™ºèƒ½ç©¿æˆ´ç³»ç»Ÿåœ¨ç²¾å‡†åŒ»å­¦ä¸­çš„åº”ç”¨ï¼Œé’ˆå¯¹ä¼ ç»Ÿè®¾å¤‡åœ¨ææ–™è®¾è®¡å’Œä¿¡å·å¤„ç†æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†äººæœºå…±ç”Ÿå¥åº·æ™ºèƒ½ï¼ˆHuman-Symbiotic Health Intelligence, HSHIï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶é›†æˆäº†å¤šæ¨¡æ€ä¼ æ„Ÿå™¨ç½‘ç»œã€è¾¹ç¼˜äº‘ååŒè®¡ç®—ä»¥åŠæ•°æ®ä¸çŸ¥è¯†æ··åˆå»ºæ¨¡æŠ€æœ¯ï¼Œå®ç°äº†ä»ææ–™å¾®ç»“æ„ä¼˜åŒ–åˆ°å¤æ‚ä¿¡å·è§£è¯»çš„å…¨æ ˆæ•´åˆã€‚HSHI èƒ½å¤ŸåŠ¨æ€åº”å¯¹ä¸ªä½“é—´ä¸ä¸ªä½“å†…çš„å·®å¼‚ï¼Œé€šè¿‡ç»“åˆç¾¤ä½“å…±æ€§è§„å¾‹ä¸ä¸ªæ€§åŒ–é€‚é…æœºåˆ¶ï¼Œå°†å¥åº·ç®¡ç†ä»è¢«åŠ¨ç›‘æµ‹æå‡ä¸ºä¸»åŠ¨åä½œã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰ä¸æ•°å­—å­ªç”Ÿï¼ˆDigital Twinsï¼‰æŠ€æœ¯è¿›è¡Œé—­ç¯ä¼˜åŒ–ï¼Œä»¥æ”¯æŒå®šåˆ¶åŒ–çš„åŒ»ç–—å¹²é¢„ä¸åé¦ˆã€‚è¿™ä¸€åˆ›æ–°æ¨åŠ¨äº†åŒ»ç–—æ¨¡å¼å‘é¢„é˜²æ€§ã€é€‚åº”æ€§åŠå’Œè°çš„äººæœºäº¤äº’æ–¹å‘è½¬å˜ï¼Œä¸ºæœªæ¥ä¸ªæ€§åŒ–å¥åº·ç®¡ç†æä¾›äº†ç³»ç»Ÿæ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "5 pages, l figure, l table. Accepted at AI4RWC@WI-IAT 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.13565v1",
      "published_date": "2025-11-17 16:33:45 UTC",
      "updated_date": "2025-11-17 16:33:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:54:23.232471+00:00"
    },
    {
      "arxiv_id": "2511.13548v1",
      "title": "ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models",
      "title_zh": "ForgeDANï¼šé’ˆå¯¹å·²å¯¹é½å¤§è¯­è¨€æ¨¡å‹è¶Šç‹±çš„æ¼”åŒ–æ¡†æ¶",
      "authors": [
        "Siyang Cheng",
        "Gaotian Liu",
        "Rui Mei",
        "Yilin Wang",
        "Kejia Zhang",
        "Kaishuo Wei",
        "Yuqi Yu",
        "Weiping Wen",
        "Xiaojie Wu",
        "Junhua Liu"
      ],
      "abstract": "The rapid adoption of large language models (LLMs) has brought both transformative applications and new security risks, including jailbreak attacks that bypass alignment safeguards to elicit harmful outputs. Existing automated jailbreak generation approaches e.g. AutoDAN, suffer from limited mutation diversity, shallow fitness evaluation, and fragile keyword-based detection. To address these limitations, we propose ForgeDAN, a novel evolutionary framework for generating semantically coherent and highly effective adversarial prompts against aligned LLMs. First, ForgeDAN introduces multi-strategy textual perturbations across \\textit{character, word, and sentence-level} operations to enhance attack diversity; then we employ interpretable semantic fitness evaluation based on a text similarity model to guide the evolutionary process toward semantically relevant and harmful outputs; finally, ForgeDAN integrates dual-dimensional jailbreak judgment, leveraging an LLM-based classifier to jointly assess model compliance and output harmfulness, thereby reducing false positives and improving detection effectiveness. Our evaluation demonstrates ForgeDAN achieves high jailbreaking success rates while maintaining naturalness and stealth, outperforming existing SOTA solutions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ForgeDANï¼Œä¸€ä¸ªæ—¨åœ¨é’ˆå¯¹å·²å¯¹é½çš„å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) ç”Ÿæˆè¯­ä¹‰è¿è´¯ä¸”é«˜æ•ˆå¯¹æŠ—æ€§æç¤ºçš„è¿›åŒ–æ¡†æ¶ï¼Œä»¥è§£å†³ç°æœ‰è‡ªåŠ¨è¶Šç‹±æ”»å‡» (jailbreak attacks) æ–¹æ³•åœ¨å˜å¼‚å¤šæ ·æ€§ã€é€‚åº”åº¦è¯„ä¼°å’Œæ£€æµ‹é²æ£’æ€§æ–¹é¢çš„ä¸è¶³ã€‚ForgeDAN é¦–å…ˆå¼•å…¥äº†æ¶µç›–å­—ç¬¦ã€å•è¯å’Œå¥å­ç»´åº¦çš„å¤šç­–ç•¥æ–‡æœ¬æ‰°åŠ¨ï¼Œæ˜¾è‘—å¢å¼ºäº†æ”»å‡»çš„å¤šæ ·æ€§ã€‚éšåï¼Œè¯¥æ¡†æ¶é‡‡ç”¨åŸºäºæ–‡æœ¬ç›¸ä¼¼åº¦æ¨¡å‹çš„å¯è§£é‡Šè¯­ä¹‰é€‚åº”åº¦è¯„ä¼° (semantic fitness evaluation)ï¼Œæœ‰æ•ˆå¼•å¯¼è¿›åŒ–è¿‡ç¨‹äº§å‡ºå…·æœ‰è¯­ä¹‰ç›¸å…³æ€§ä¸”æœ‰å®³çš„å†…å®¹ã€‚æ­¤å¤–ï¼ŒForgeDAN é›†æˆäº†åŒç»´åº¦è¶Šç‹±åˆ¤å®šæœºåˆ¶ï¼Œåˆ©ç”¨åŸºäº LLM çš„åˆ†ç±»å™¨å…±åŒè¯„ä¼°æ¨¡å‹çš„åˆè§„æ€§ä¸è¾“å‡ºçš„å±å®³æ€§ï¼Œä»è€Œé™ä½è¯¯æŠ¥ç‡å¹¶æå‡æ£€æµ‹ç²¾åº¦ã€‚å®éªŒè¯„ä¼°è¯æ˜ï¼ŒForgeDAN åœ¨ä¿æŒè¯­è¨€è‡ªç„¶åº¦ä¸éšè”½æ€§çš„åŒæ—¶ï¼Œå®ç°äº†æé«˜çš„è¶Šç‹±æˆåŠŸç‡ï¼Œæ€§èƒ½è¡¨ç°ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿› (SOTA) è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13548v1",
      "published_date": "2025-11-17 16:19:21 UTC",
      "updated_date": "2025-11-17 16:19:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:54:41.029492+00:00"
    },
    {
      "arxiv_id": "2511.13545v1",
      "title": "Robust Defense Strategies for Multimodal Contrastive Learning: Efficient Fine-tuning Against Backdoor Attacks",
      "title_zh": "å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ çš„é²æ£’é˜²å¾¡ç­–ç•¥ï¼šé’ˆå¯¹åé—¨æ”»å‡»çš„é«˜æ•ˆå¾®è°ƒ",
      "authors": [
        "Md. Iqbal Hossain",
        "Afia Sajeeda",
        "Neeresh Kumar Perla",
        "Ming Shao"
      ],
      "abstract": "The advent of multimodal deep learning models, such as CLIP, has unlocked new frontiers in a wide range of applications, from image-text understanding to classification tasks. However, these models are not safe for adversarial attacks, particularly backdoor attacks, which can subtly manipulate model behavior. Moreover, existing defense methods typically involve training from scratch or fine-tuning using a large dataset without pinpointing the specific labels that are affected. In this study, we introduce an innovative strategy to enhance the robustness of multimodal contrastive learning models against such attacks. In particular, given a poisoned CLIP model, our approach can identify the backdoor trigger and pinpoint the victim samples and labels in an efficient manner. To that end, an image segmentation ``oracle'' is introduced as the supervisor for the output of the poisoned CLIP. We develop two algorithms to rectify the poisoned model: (1) differentiating between CLIP and Oracle's knowledge to identify potential triggers; (2) pinpointing affected labels and victim samples, and curating a compact fine-tuning dataset. With this knowledge, we are allowed to rectify the poisoned CLIP model to negate backdoor effects. Extensive experiments on visual recognition benchmarks demonstrate our strategy is effective in CLIP-based backdoor defense.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚ CLIPï¼‰æ˜“å—åé—¨æ”»å‡»ï¼ˆBackdoor Attacksï¼‰å¨èƒçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ›æ–°çš„é²æ£’é˜²å¾¡ç­–ç•¥ï¼Œæ—¨åœ¨é«˜æ•ˆä¿®æ­£è¢«æŠ•æ¯’çš„æ¨¡å‹ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†å›¾åƒåˆ†å‰²â€œå…ˆéªŒçŸ¥è¯†â€ï¼ˆOracleï¼‰ä½œä¸ºç›‘ç£è€…ï¼Œé€šè¿‡å¼€å‘ä¸¤ç§æ ¸å¿ƒç®—æ³•æ¥è¯†åˆ«åé—¨è§¦å‘å™¨ï¼ˆTriggersï¼‰å¹¶ç²¾å‡†å®šä½å—å½±å“çš„æ ‡ç­¾ä¸å—å®³è€…æ ·æœ¬ã€‚é€šè¿‡å¯¹æ¯” CLIP ä¸ Oracle çš„çŸ¥è¯†å·®å¼‚ï¼Œç ”ç©¶è€…èƒ½å¤Ÿæ„å»ºå‡ºä¸€ä¸ªç´§å‡‘çš„å¾®è°ƒæ•°æ®é›†ï¼Œä»è€Œåœ¨æ— éœ€å¤§è§„æ¨¡é‡è®­çš„æƒ…å†µä¸‹æœ‰æ•ˆæŠµæ¶ˆåé—¨æ•ˆåº”ã€‚åœ¨è§†è§‰è¯†åˆ«åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼Œè¯¥ç­–ç•¥åœ¨å¢å¼º CLIP æ¨¡å‹é²æ£’æ€§æ–¹é¢å…·æœ‰æ˜¾è‘—çš„æ•ˆæœï¼Œä¸ºå¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ çš„å®‰å…¨åº”ç”¨æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13545v1",
      "published_date": "2025-11-17 16:16:50 UTC",
      "updated_date": "2025-11-17 16:16:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:54:35.641499+00:00"
    },
    {
      "arxiv_id": "2511.13542v2",
      "title": "Making Evidence Actionable in Adaptive Learning Closing the Diagnostic Pedagogical Loop",
      "title_zh": "è‡ªé€‚åº”å­¦ä¹ ä¸­è¯æ®çš„å¯æ“ä½œåŒ–ï¼šé—­åˆæ•™å­¦è¯Šæ–­é—­ç¯",
      "authors": [
        "Amirreza Mehrabi",
        "Jason Wade Morphew",
        "Breejha Quezada",
        "N. Sanjay Rebello"
      ],
      "abstract": "Adaptive learning often diagnoses precisely yet intervenes weakly, producing help that is mistimed or misaligned. This study presents evidence supporting an instructor-governed feedback loop that converts concept-level assessment evidence into vetted microinterventions. The adaptive learning algorithm includes three safeguards: adequacy as a hard guarantee of gap closure, attention as a budgeted limit for time and redundancy, and diversity as protection against overfitting to a single resource. We formulate intervention assignment as a binary integer program with constraints for coverage, time, difficulty windows derived from ability estimates, prerequisites encoded by a concept matrix, and anti-redundancy with diversity. Greedy selection serves low-richness and tight-latency settings, gradient-based relaxation serves rich repositories, and a hybrid switches along a richness-latency frontier. In simulation and in an introductory physics deployment with 1204 students, both solvers achieved full skill coverage for nearly all learners within bounded watch time. The gradient-based method reduced redundant coverage by about 12 percentage points relative to greedy and produced more consistent difficulty alignment, while greedy delivered comparable adequacy at lower computational cost in resource-scarce environments. Slack variables localized missing content and guided targeted curation, sustaining sufficiency across student subgroups. The result is a tractable and auditable controller that closes the diagnostic pedagogical loop and enables equitable, load-aware personalization at the classroom scale.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªé€‚åº”å­¦ä¹ (Adaptive Learning)ä¸­è¯Šæ–­ç²¾ç¡®ä½†å¹²é¢„è–„å¼±çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªç”±æ•™å¸ˆç®¡ç†çš„åé¦ˆå›è·¯ï¼Œæ—¨åœ¨å°†æ¦‚å¿µå±‚é¢çš„è¯„ä¼°è¯æ®è½¬åŒ–ä¸ºç»è¿‡å®¡æ ¸çš„å¾®å¹²é¢„æªæ–½(Microinterventions)ã€‚è¯¥ç®—æ³•é€šè¿‡äºŒè¿›åˆ¶æ•´æ•°è§„åˆ’(Binary Integer Program)å¯¹å¹²é¢„åˆ†é…è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶é›†æˆäº†å……åˆ†æ€§(Adequacy)ã€æ³¨æ„åŠ›(Attention)å’Œå¤šæ ·æ€§(Diversity)ä¸‰å¤§ä¿éšœæœºåˆ¶ï¼Œä»¥ç¡®ä¿å­¦ä¹ ç¼ºå£çš„å¼¥è¡¥åŠé˜²æ­¢è¿‡åº¦æ‹Ÿåˆã€‚ç ”ç©¶è®¾è®¡äº†è´ªå©ªé€‰æ‹©(Greedy selection)ã€åŸºäºæ¢¯åº¦çš„æ¾å¼›æ³•(Gradient-based relaxation)åŠæ··åˆæ–¹æ³•ä¸‰ç§æ±‚è§£å™¨ï¼Œä»¥åº”å¯¹ä¸åŒçš„èµ„æºä¸°å¯Œåº¦å’Œå»¶è¿Ÿéœ€æ±‚ã€‚åœ¨æ¶‰åŠ1204åå­¦ç”Ÿçš„ç‰©ç†è¯¾ç¨‹éƒ¨ç½²åŠä»¿çœŸå®éªŒä¸­ï¼Œä¸¤ç§æ±‚è§£å™¨å‡èƒ½åœ¨é™å®šæ—¶é—´å†…å®ç°å‡ ä¹æ‰€æœ‰å­¦ä¹ è€…çš„æŠ€èƒ½å…¨è¦†ç›–ã€‚å®éªŒå‘ç°åŸºäºæ¢¯åº¦çš„æ–¹æ³•åœ¨å‡å°‘å†—ä½™è¦†ç›–æ–¹é¢æ¯”è´ªå©ªç®—æ³•ä¼˜åŒ–äº†çº¦12ä¸ªç™¾åˆ†ç‚¹ï¼Œè€Œè´ªå©ªç®—æ³•åœ¨èµ„æºåŒ®ä¹ç¯å¢ƒä¸‹å…·æœ‰æ›´é«˜çš„è®¡ç®—æ•ˆç‡ã€‚è¯¥ç ”ç©¶æœ€ç»ˆå®ç°äº†ä¸€ä¸ªå¯å®¡è®¡ä¸”è´Ÿè½½æ„ŸçŸ¥çš„æ§åˆ¶å™¨ï¼Œé€šè¿‡é—­åˆè¯Šæ–­æ€§æ•™å­¦é—­ç¯(Diagnostic pedagogical loop)ï¼Œä¸ºè§„æ¨¡åŒ–è¯¾å ‚æä¾›äº†å…¬å¹³ä¸”ä¸ªæ€§åŒ–çš„æ”¯æŒã€‚",
      "categories": [
        "cs.CE",
        "cs.AI",
        "cs.CY",
        "stat.AP"
      ],
      "primary_category": "cs.CE",
      "comment": "We have submitted the same article with another title: Making Evidence Actionable in Adaptive Learning (arXiv:2511.14052)",
      "pdf_url": "https://arxiv.org/pdf/2511.13542v2",
      "published_date": "2025-11-17 16:15:50 UTC",
      "updated_date": "2025-11-19 16:05:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:54:41.734564+00:00"
    },
    {
      "arxiv_id": "2511.13530v1",
      "title": "Towards Affect-Adaptive Human-Robot Interaction: A Protocol for Multimodal Dataset Collection on Social Anxiety",
      "title_zh": "è¿ˆå‘æƒ…æ„Ÿè‡ªé€‚åº”äººæœºäº¤äº’ï¼šé’ˆå¯¹ç¤¾äº¤ç„¦è™‘çš„å¤šæ¨¡æ€æ•°æ®é›†é‡‡é›†æ–¹æ¡ˆ",
      "authors": [
        "Vesna Poprcova",
        "Iulia Lefter",
        "Matthias Wieser",
        "Martijn Warnier",
        "Frances Brazier"
      ],
      "abstract": "Social anxiety is a prevalent condition that affects interpersonal interactions and social functioning. Recent advances in artificial intelligence and social robotics offer new opportunities to examine social anxiety in the human-robot interaction context. Accurate detection of affective states and behaviours associated with social anxiety requires multimodal datasets, where each signal modality provides complementary insights into its manifestations. However, such datasets remain scarce, limiting progress in both research and applications. To address this, this paper presents a protocol for multimodal dataset collection designed to reflect social anxiety in a human-robot interaction context. The dataset will consist of synchronised audio, video, and physiological recordings acquired from at least 70 participants, grouped according to their level of social anxiety, as they engage in approximately 10-minute interactive Wizard-of-Oz role-play scenarios with the Furhat social robot under controlled experimental conditions. In addition to multimodal data, the dataset will be enriched with contextual data providing deeper insight into individual variability in social anxiety responses. This work can contribute to research on affect-adaptive human-robot interaction by providing support for robust multimodal detection of social anxiety.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¤¾äº¤ç„¦è™‘ (Social Anxiety) å¯¹äººé™…äº¤å¾€çš„å½±å“ï¼Œæå‡ºäº†ä¸€ç§æ—¨åœ¨å®ç°æƒ…æ„Ÿè‡ªé€‚åº”äººæœºäº¤äº’ (Affect-Adaptive Human-Robot Interaction) çš„å¤šæ¨¡æ€æ•°æ®é›†é‡‡é›†åè®®ã€‚ç”±äºç›®å‰ç¼ºä¹èƒ½å¤Ÿåæ˜ ç¤¾äº¤ç„¦è™‘ç‰¹å¾çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†ä¸€é¡¹åœ¨å—æ§å®éªŒæ¡ä»¶ä¸‹è¿›è¡Œçš„å®éªŒï¼ŒåŒ…å«è‡³å°‘ 70 åæ ¹æ®ç„¦è™‘æ°´å¹³åˆ†ç»„çš„å—è¯•è€…ã€‚è¿™äº›å—è¯•è€…å°†ä¸ Furhat ç¤¾äº¤æœºå™¨äººè¿›è¡Œçº¦ 10 åˆ†é’Ÿçš„ Wizard-of-Oz è§’è‰²æ‰®æ¼”äº’åŠ¨ï¼ŒæœŸé—´å°†é‡‡é›†åŒæ­¥çš„éŸ³é¢‘ (Audio)ã€è§†é¢‘ (Video) å’Œç”Ÿç†ä¿¡å· (Physiological recordings)ã€‚è¯¥æ•°æ®é›†è¿˜å°†æ•´åˆä¸Šä¸‹æ–‡èƒŒæ™¯æ•°æ® (Contextual data)ï¼Œä»¥æä¾›å¯¹ç¤¾äº¤ç„¦è™‘ååº”ä¸ªä½“å·®å¼‚çš„æ·±å…¥æ´å¯Ÿã€‚è¯¥å·¥ä½œé€šè¿‡æ”¯æŒå¯¹ç¤¾äº¤ç„¦è™‘çš„é²æ£’æ€§å¤šæ¨¡æ€æ£€æµ‹ï¼Œä¸ºæœªæ¥å¼€å‘å…·å¤‡æƒ…æ„Ÿé€‚åº”èƒ½åŠ›çš„äººæœºäº¤äº’æŠ€æœ¯æä¾›äº†å…³é”®çš„æ•°æ®æ”¯æ’‘ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted at the Workshop on Benefits of pErsonalization and behAvioral adaptation in assistive Robots (BEAR 2025), held at the IEEE RO-MAN Conference 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.13530v1",
      "published_date": "2025-11-17 16:03:33 UTC",
      "updated_date": "2025-11-17 16:03:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:55:55.641430+00:00"
    },
    {
      "arxiv_id": "2511.13529v2",
      "title": "Toward Conversational Hungarian Speech Recognition: Introducing the BEA-Large and BEA-Dialogue Datasets",
      "title_zh": "è¿ˆå‘å¯¹è¯å¼åŒˆç‰™åˆ©è¯­è¯­éŸ³è¯†åˆ«ï¼šBEA-Large ä¸ BEA-Dialogue æ•°æ®é›†ä»‹ç»",
      "authors": [
        "MÃ¡tÃ© Gedeon",
        "Piroska ZsÃ³fia Barta",
        "PÃ©ter Mihajlik",
        "Tekla Etelka GrÃ¡czi",
        "Anna KohÃ¡ri",
        "Katalin MÃ¡dy"
      ],
      "abstract": "The advancement of automatic speech recognition (ASR) has been largely enhanced by extensive datasets in high-resource languages, while languages such as Hungarian remain underrepresented due to limited spontaneous and conversational corpora. To address this gap, we introduce two new datasets -- BEA-Large and BEA-Dialogue -- constructed from the previously unprocessed portions of the Hungarian speech corpus named BEA. BEA-Large extends BEA-Base with 255 hours of spontaneous speech from 433 speakers, enriched with detailed segment-level metadata. BEA-Dialogue, comprising 85 hours of spontaneous conversations, is a Hungarian speech corpus featuring natural dialogues partitioned into speaker-independent subsets, supporting research in conversational ASR and speaker diarization. We establish reproducible baselines on these datasets using publicly available ASR models, with the fine-tuned Fast Conformer model achieving word error rates as low as 14.18% on spontaneous and 4.8% on repeated speech. Diarization experiments yield diarization error rates between 12.46% and 17.40%, providing reference points for future improvements. The results highlight the persistent difficulty of conversational ASR, particularly due to disfluencies, overlaps, and informal speech patterns. By releasing these datasets and baselines, we aim to advance Hungarian speech technology and offer a methodological framework for developing spontaneous and conversational benchmarks in other languages.",
      "tldr_zh": "é’ˆå¯¹åŒˆç‰™åˆ©è¯­åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)é¢†åŸŸç”±äºç¼ºä¹è‡ªå‘å¼å’Œå¯¹è¯å¼è¯­æ–™åº“è€Œé¢ä¸´çš„èµ„æºåŒ®ä¹é—®é¢˜ï¼Œè¯¥ç ”ç©¶æ¨å‡ºäº†BEA-Largeå’ŒBEA-Dialogueä¸¤ä¸ªæ–°æ•°æ®é›†ã€‚BEA-Largeæ‰©å±•äº†255å°æ—¶çš„è‡ªå‘è¯­éŸ³ï¼Œæ¶µç›–433åè¯´è¯è€…å¹¶æä¾›è¯¦ç»†çš„æ®µè½çº§å…ƒæ•°æ®ï¼Œè€ŒBEA-Dialogueåˆ™åŒ…å«85å°æ—¶çš„è‡ªç„¶å¯¹è¯ï¼Œæ—¨åœ¨æ”¯æŒå¯¹è¯å¼ASRå’Œè¯´è¯äººæ—¥å¿—(Speaker Diarization)ç ”ç©¶ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨å…¬å¼€æ¨¡å‹å»ºç«‹äº†å¯å¤ç°çš„åŸºå‡†ï¼Œå…¶ä¸­å¾®è°ƒåçš„Fast Conformeræ¨¡å‹åœ¨è‡ªå‘è¯­éŸ³ä¸Šçš„å­—é”™è¯¯ç‡(Word Error Rate)ä½è‡³14.18%ï¼Œå¹¶åœ¨è¯´è¯äººæ—¥å¿—å®éªŒä¸­å–å¾—äº†12.46%è‡³17.40%çš„é”™è¯¯ç‡(DER)ã€‚å®éªŒç»“æœå‡¸æ˜¾äº†å¯¹è¯å¼è¯­éŸ³ä¸­é‡å ã€è¨€è¯­ä¸æµåˆ©åŠéæ­£å¼è¡¨è¾¾å¸¦æ¥çš„æŠ€æœ¯æŒ‘æˆ˜ã€‚é€šè¿‡å‘å¸ƒè¿™äº›èµ„æºï¼Œè¯¥ç ”ç©¶ä¸ä»…æ¨åŠ¨äº†åŒˆç‰™åˆ©è¯­è¯­éŸ³æŠ€æœ¯çš„å‘å±•ï¼Œä¹Ÿä¸ºå…¶ä»–è¯­è¨€æ„å»ºè‡ªå‘æ€§å¯¹è¯åŸºå‡†æä¾›äº†æœ‰æ•ˆçš„æ–¹æ³•è®ºæ¡†æ¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "Submitted to LREC 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.13529v2",
      "published_date": "2025-11-17 16:02:08 UTC",
      "updated_date": "2026-01-14 07:26:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:54:55.337470+00:00"
    },
    {
      "arxiv_id": "2511.13526v1",
      "title": "Automated Construction of Medical Indicator Knowledge Graphs Using Retrieval Augmented Large Language Models",
      "title_zh": "åŸºäºæ£€ç´¢å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„åŒ»å­¦æŒ‡æ ‡çŸ¥è¯†å›¾è°±è‡ªåŠ¨åŒ–æ„å»º",
      "authors": [
        "Zhengda Wang",
        "Daqian Shi",
        "Jingyi Zhao",
        "Xiaolei Diao",
        "Xiongfeng Tang",
        "Yanguo Qin"
      ],
      "abstract": "Artificial intelligence (AI) is reshaping modern healthcare by advancing disease diagnosis, treatment decision-making, and biomedical research. Among AI technologies, large language models (LLMs) have become especially impactful, enabling deep knowledge extraction and semantic reasoning from complex medical texts. However, effective clinical decision support requires knowledge in structured, interoperable formats. Knowledge graphs serve this role by integrating heterogeneous medical information into semantically consistent networks. Yet, current clinical knowledge graphs still depend heavily on manual curation and rule-based extraction, which is limited by the complexity and contextual ambiguity of medical guidelines and literature. To overcome these challenges, we propose an automated framework that combines retrieval-augmented generation (RAG) with LLMs to construct medical indicator knowledge graphs. The framework incorporates guideline-driven data acquisition, ontology-based schema design, and expert-in-the-loop validation to ensure scalability, accuracy, and clinical reliability. The resulting knowledge graphs can be integrated into intelligent diagnosis and question-answering systems, accelerating the development of AI-driven healthcare solutions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰ä¸´åºŠ Knowledge Graphs è¿‡åº¦ä¾èµ–äººå·¥ç­–å±•å’Œè§„åˆ™æå–ï¼Œéš¾ä»¥åº”å¯¹åŒ»å­¦æ–‡çŒ®å¤æ‚æ€§ä¸è¯­å¢ƒæ­§ä¹‰çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆ Retrieval-Augmented Generation (RAG) ä¸ Large Language Models (LLMs) è‡ªåŠ¨æ„å»ºåŒ»å­¦æŒ‡æ ‡çŸ¥è¯†å›¾è°±çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆæŒ‡å—é©±åŠ¨çš„æ•°æ®é‡‡é›†ã€åŸºäº Ontology çš„ Schema è®¾è®¡ä»¥åŠ Expert-in-the-loop éªŒè¯æœºåˆ¶ï¼Œç¡®ä¿äº†çŸ¥è¯†æ„å»ºè¿‡ç¨‹çš„æ‰©å±•æ€§ã€å‡†ç¡®æ€§å’Œä¸´åºŠå¯é æ€§ã€‚è¯¥æ–¹æ³•æˆåŠŸå°†å¼‚æ„åŒ»å­¦ä¿¡æ¯è½¬åŒ–ä¸ºè¯­ä¹‰ä¸€è‡´çš„ç½‘ç»œï¼Œæå‡äº†ä»å¤æ‚æ–‡æœ¬ä¸­è¿›è¡Œæ·±åº¦çŸ¥è¯†æå–ä¸è¯­ä¹‰æ¨ç†çš„èƒ½åŠ›ã€‚æœ€ç»ˆç”Ÿæˆçš„ Knowledge Graphs å¯è¢«é›†æˆè‡³æ™ºèƒ½è¯Šæ–­å’Œé—®ç­”ç³»ç»Ÿä¸­ï¼Œä¸ºåŠ é€Ÿäººå·¥æ™ºèƒ½é©±åŠ¨çš„åŒ»ç–—å¥åº·è§£å†³æ–¹æ¡ˆçš„å¼€å‘æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "5 pages, 1 figure, 1 table. Accepted at AI4RWC@WI-IAT 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.13526v1",
      "published_date": "2025-11-17 16:00:42 UTC",
      "updated_date": "2025-11-17 16:00:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:56:29.340242+00:00"
    },
    {
      "arxiv_id": "2511.13525v1",
      "title": "AI Fairness Beyond Complete Demographics: Current Achievements and Future Directions",
      "title_zh": "çªç ´å®Œæ•´äººå£ç»Ÿè®¡æ•°æ®é™åˆ¶çš„äººå·¥æ™ºèƒ½å…¬å¹³æ€§ï¼šç ”ç©¶ç°çŠ¶ä¸æœªæ¥æ–¹å‘",
      "authors": [
        "Zichong Wang",
        "Zhipeng Yin",
        "Roland H. C. Yap",
        "Wenbin Zhang"
      ],
      "abstract": "Fairness in artificial intelligence (AI) has become a growing concern due to discriminatory outcomes in AI-based decision-making systems. While various methods have been proposed to mitigate bias, most rely on complete demographic information, an assumption often impractical due to legal constraints and the risk of reinforcing discrimination. This survey examines fairness in AI when demographics are incomplete, addressing the gap between traditional approaches and real-world challenges. We introduce a novel taxonomy of fairness notions in this setting, clarifying their relationships and distinctions. Additionally, we summarize existing techniques that promote fairness beyond complete demographics and highlight open research questions to encourage further progress in the field.",
      "tldr_zh": "æœ¬ç»¼è¿°é’ˆå¯¹äººå·¥æ™ºèƒ½ (AI) é¢†åŸŸä¸­å…¬å¹³æ€§ (Fairness) çš„æ ¸å¿ƒé—®é¢˜ï¼Œæ¢è®¨äº†åœ¨ç¼ºä¹å®Œæ•´äººå£ç»Ÿè®¡ä¿¡æ¯ (Complete Demographics) çš„ç°å®æƒ…å¢ƒä¸‹ï¼Œä¼ ç»Ÿåè§ç¼“è§£æ–¹æ³•çš„å±€é™æ€§ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œå—é™äºæ³•å¾‹çº¦æŸåŠé˜²æ­¢æ­§è§†è¿›ä¸€æ­¥åŠ å‰§çš„è€ƒé‡ï¼Œè·å–å®Œæ•´çš„äººå£æ•°æ®å¾€å¾€ä¸åˆ‡å®é™…ï¼Œå› æ­¤å¿…é¡»å¡«è¡¥ç°æœ‰å…¬å¹³æ€§ç ”ç©¶ä¸å®é™…æŒ‘æˆ˜ä¹‹é—´çš„é¸¿æ²Ÿã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§å…¨æ–°çš„å…¬å¹³æ€§æ¦‚å¿µåˆ†ç±»æ³• (Taxonomy)ï¼Œåœ¨äººå£ç»Ÿè®¡æ•°æ®ä¸å…¨çš„æƒ…å†µä¸‹ï¼Œæ¸…æ™°åœ°ç•Œå®šå¹¶å˜æ¸…äº†ä¸åŒå…¬å¹³æ€§å®šä¹‰çš„ç›¸äº’å…³ç³»ä¸åŒºåˆ«ã€‚è¯¥ç ”ç©¶ç³»ç»Ÿæ€§åœ°æ€»ç»“äº†è¶…è¶Šå®Œæ•´äººå£ç»Ÿè®¡é™åˆ¶çš„ç°æœ‰å…¬å¹³æ€§å¢å¼ºæŠ€æœ¯ï¼Œä¸ºå¦‚ä½•åœ¨å—é™æ•°æ®ç¯å¢ƒä¸‹å®ç° AI å…¬å¹³æä¾›äº†æŠ€æœ¯è·¯å¾„ã€‚æœ€åï¼Œæ–‡ç« é€šè¿‡æ¢³ç†å…³é”®çš„å¼€æ”¾æ€§ç ”ç©¶é—®é¢˜ (Open Research Questions)ï¼ŒæŒ‡æ˜äº†æœªæ¥è¯¥é¢†åŸŸçš„å‘å±•æ–¹å‘ï¼Œæ—¨åœ¨æ¨åŠ¨å»ºç«‹æ›´å…·å¯æ“ä½œæ€§ä¸”åˆä¹ä¼¦ç†çš„å†³ç­–ç³»ç»Ÿã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "ECAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.13525v1",
      "published_date": "2025-11-17 15:59:25 UTC",
      "updated_date": "2025-11-17 15:59:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:56:16.232293+00:00"
    },
    {
      "arxiv_id": "2511.13524v2",
      "title": "FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI",
      "title_zh": "FreeAskWorldï¼šé¢å‘ä»¥äººä¸ºä¸­å¿ƒå…·èº«æ™ºèƒ½çš„äº¤äº’å¼é—­ç¯ä»¿çœŸå™¨",
      "authors": [
        "Yuhang Peng",
        "Yizhou Pan",
        "Xinning He",
        "Jihaoyu Yang",
        "Xinyu Yin",
        "Han Wang",
        "Xiaoji Zheng",
        "Chao Gao",
        "Jiangtao Gong"
      ],
      "abstract": "As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks. To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† FreeAskWorldï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¶…è¶Šåº•å±‚ç‰©ç†äº¤äº’ã€æ•æ‰å¤æ‚ä»¥äººä¸ºä¸­å¿ƒç¤¾äº¤è¡Œä¸ºçš„äº¤äº’å¼é—­ç¯æ¨¡æ‹Ÿæ¡†æ¶ã€‚è¯¥æ¡†æ¶é›†æˆäº† Large Language Models (LLMs) è¿›è¡Œé«˜å±‚è¡Œä¸ºè§„åˆ’å’Œè¯­ä¹‰å¯¼å‘çš„äº¤äº’ï¼Œå¹¶ç»“åˆäº†æ„å›¾å’Œç¤¾äº¤è®¤çŸ¥ç†è®ºã€‚FreeAskWorld åŒ…å«ä¸€ä¸ªæ¨¡å—åŒ–çš„æ•°æ®ç”Ÿæˆæµæ°´çº¿ï¼Œæ”¯æŒå¤§è§„æ¨¡ã€çœŸå®çš„äººæœºæ¨¡æ‹Ÿï¼Œå¹¶æä¾›äº†ä¸€ä¸ªåŒ…å«å¤šç§ä»»åŠ¡ç±»å‹ã€63,429 ä¸ªæ ‡æ³¨å¸§åŠè¶…è¿‡ 17 å°æ—¶äº¤äº’æ•°æ®çš„åŸºå‡†æ•°æ®é›†ã€‚ç ”ç©¶è€…å°†ç»å…¸çš„ Vision-and-Language Navigation (VLN) ä»»åŠ¡æ‰©å±•ä¸º Direction Inquiry åœºæ™¯ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿä¸»åŠ¨å¯»æ±‚å¹¶è§£æå¯¼èˆªæŒ‡å¼•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ FreeAskWorld ä¸Šå¾®è°ƒçš„æ¨¡å‹åœ¨è¯­ä¹‰ç†è§£å’Œäº¤äº’èƒ½åŠ›ä¸Šå‡ä¼˜äºåŸå§‹åŸºçº¿æ¨¡å‹ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†ç¤¾äº¤å¯¼å‘æ¨¡æ‹Ÿåœ¨æå‡å…·èº«æ™ºèƒ½é«˜å±‚è§„åˆ’èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æŒ‡å‡ºäº¤äº’æœ¬èº«å¯ä½œä¸ºä¸€ç§é¢å¤–çš„ä¿¡æ¯æ¨¡æ€ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.13524v2",
      "published_date": "2025-11-17 15:58:46 UTC",
      "updated_date": "2025-12-20 13:18:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:56:20.583994+00:00"
    },
    {
      "arxiv_id": "2511.13510v1",
      "title": "Naga: Vedic Encoding for Deep State Space Models",
      "title_zh": "Nagaï¼šé¢å‘æ·±åº¦çŠ¶æ€ç©ºé—´æ¨¡å‹çš„å é™€ç¼–ç ",
      "authors": [
        "Melanie Schaller",
        "Nick Janssen",
        "Bodo Rosenhahn"
      ],
      "abstract": "This paper presents Naga, a deep State Space Model (SSM) encoding approach inspired by structural concepts from Vedic mathematics. The proposed method introduces a bidirectional representation for time series by jointly processing forward and time-reversed input sequences. These representations are then combined through an element-wise (Hadamard) interaction, resulting in a Vedic-inspired encoding that enhances the model's ability to capture temporal dependencies across distant time steps. We evaluate Naga on multiple long-term time series forecasting (LTSF) benchmarks, including ETTh1, ETTh2, ETTm1, ETTm2, Weather, Traffic, and ILI. The experimental results show that Naga outperforms 28 current state of the art models and demonstrates improved efficiency compared to existing deep SSM-based approaches. The findings suggest that incorporating structured, Vedic-inspired decomposition can provide an interpretable and computationally efficient alternative for long-range sequence modeling.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Nagaï¼Œè¿™æ˜¯ä¸€ç§å—å é™€æ•°å­¦(Vedic mathematics)ç»“æ„æ¦‚å¿µå¯å‘çš„æ·±åº¦çŠ¶æ€ç©ºé—´æ¨¡å‹(State Space Model, SSM)ç¼–ç æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å…±åŒå¤„ç†å‰å‘å’Œæ—¶é—´åè½¬çš„è¾“å…¥åºåˆ—ï¼Œä¸ºæ—¶é—´åºåˆ—å¼•å…¥äº†ä¸€ç§åŒå‘è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨é€å…ƒç´ ï¼ˆHadamardï¼‰äº¤äº’å°†è¿™äº›è¡¨ç¤ºè¿›è¡Œç»„åˆã€‚è¿™ç§Vedic-inspired encodingæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹æ•è·è¿œè·ç¦»æ—¶é—´æ­¥é•¿ä¹‹é—´æ—¶é—´ä¾èµ–æ€§çš„èƒ½åŠ›ã€‚åœ¨åŒ…æ‹¬ETTh1ã€ETTh2ã€Weatherã€Trafficå’ŒILIåœ¨å†…çš„å¤šä¸ªé•¿ç¨‹æ—¶é—´åºåˆ—é¢„æµ‹(LTSF)åŸºå‡†æµ‹è¯•ä¸­ï¼ŒNagaçš„è¡¨ç°ä¼˜äº28ç§å½“å‰çš„State of the Art (SOTA)æ¨¡å‹ï¼Œå¹¶å±•ç°å‡ºæ¯”ç°æœ‰æ·±åº¦SSMæ–¹æ³•æ›´é«˜çš„è®¡ç®—æ•ˆç‡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿™ç§ç»“æ„åŒ–çš„å é™€å¯å‘å¼åˆ†è§£ä¸ºé•¿ç¨‹åºåˆ—å»ºæ¨¡æä¾›äº†ä¸€ç§å…¼å…·å¯è§£é‡Šæ€§ä¸é«˜æ•ˆæ€§çš„æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "submitted to JMLR",
      "pdf_url": "https://arxiv.org/pdf/2511.13510v1",
      "published_date": "2025-11-17 15:43:49 UTC",
      "updated_date": "2025-11-17 15:43:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:56:20.395627+00:00"
    },
    {
      "arxiv_id": "2511.13480v1",
      "title": "A Lexical Analysis of online Reviews on Human-AI Interactions",
      "title_zh": "äººæœºäº¤äº’åœ¨çº¿è¯„è®ºçš„è¯æ±‡åˆ†æ",
      "authors": [
        "Parisa Arbab",
        "Xiaowen Fang"
      ],
      "abstract": "This study focuses on understanding the complex dynamics between humans and AI systems by analyzing user reviews. While previous research has explored various aspects of human-AI interaction, such as user perceptions and ethical considerations, there remains a gap in understanding the specific concerns and challenges users face. By using a lexical approach to analyze 55,968 online reviews from G2.com, Producthunt.com, and Trustpilot.com, this preliminary research aims to analyze human-AI interaction. Initial results from factor analysis reveal key factors influencing these interactions. The study aims to provide deeper insights into these factors through content analysis, contributing to the development of more user-centric AI systems. The findings are expected to enhance our understanding of human-AI interaction and inform future AI technology and user experience improvements.",
      "tldr_zh": "æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡åˆ†æå¤§é‡ç”¨æˆ·è¯„è®ºï¼Œæ¢è®¨äººç±»ä¸ AI ç³»ç»Ÿä¹‹é—´å¤æ‚çš„äº’åŠ¨åŠ¨æ€ï¼Œä»¥å¡«è¡¥ç°æœ‰ç ”ç©¶åœ¨ç†è§£ç”¨æˆ·å…·ä½“æŒ‘æˆ˜å’Œå…³åˆ‡æ–¹é¢çš„ç©ºç™½ã€‚ç ”ç©¶å›¢é˜Ÿé‡‡ç”¨è¯æ³•åˆ†æ(lexical approach)æ–¹æ³•ï¼Œå¯¹æ¥è‡ª G2.comã€Producthunt.com å’Œ Trustpilot.com çš„ 55,968 æ¡åœ¨çº¿è¯„è®ºè¿›è¡Œäº†æ·±å…¥å‰–æã€‚åˆæ­¥çš„å› å­åˆ†æ(factor analysis)è¯†åˆ«å‡ºäº†å½±å“äº’åŠ¨çš„æ ¸å¿ƒè¦ç´ ï¼Œå¹¶ç»“åˆå†…å®¹åˆ†æ(content analysis)æä¾›äº†å¤šç»´åº¦çš„æ´å¯Ÿã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºæ›´ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„ AI ç³»ç»Ÿæä¾›äº†ç†è®ºä¾æ®ï¼Œå¹¶æœ‰åŠ©äºæ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨ç”¨æˆ·ä½“éªŒ(user experience)å±‚é¢çš„æŒç»­ä¼˜åŒ–ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "10 pages, 1 table",
      "pdf_url": "https://arxiv.org/pdf/2511.13480v1",
      "published_date": "2025-11-17 15:17:36 UTC",
      "updated_date": "2025-11-17 15:17:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:56:21.286038+00:00"
    },
    {
      "arxiv_id": "2511.13478v1",
      "title": "Semantic Document Derendering: SVG Reconstruction via Vision-Language Modeling",
      "title_zh": "è¯­ä¹‰åŒ–æ–‡æ¡£åæ¸²æŸ“ï¼šåŸºäºè§†è§‰è¯­è¨€å»ºæ¨¡çš„ SVG é‡å»º",
      "authors": [
        "Adam Hazimeh",
        "Ke Wang",
        "Mark Collier",
        "Gilles Baechler",
        "Efi Kokiopoulou",
        "Pascal Frossard"
      ],
      "abstract": "Multimedia documents such as slide presentations and posters are designed to be interactive and easy to modify. Yet, they are often distributed in a static raster format, which limits editing and customization. Restoring their editability requires converting these raster images back into structured vector formats. However, existing geometric raster-vectorization methods, which rely on low-level primitives like curves and polygons, fall short at this task. Specifically, when applied to complex documents like slides, they fail to preserve the high-level structure, resulting in a flat collection of shapes where the semantic distinction between image and text elements is lost. To overcome this limitation, we address the problem of semantic document derendering by introducing SliDer, a novel framework that uses Vision-Language Models (VLMs) to derender slide images as compact and editable Scalable Vector Graphic (SVG) representations. SliDer detects and extracts attributes from individual image and text elements in a raster input and organizes them into a coherent SVG format. Crucially, the model iteratively refines its predictions during inference in a process analogous to human design, generating SVG code that more faithfully reconstructs the original raster upon rendering. Furthermore, we introduce Slide2SVG, a novel dataset comprising raster-SVG pairs of slide documents curated from real-world scientific presentations, to facilitate future research in this domain. Our results demonstrate that SliDer achieves a reconstruction LPIPS of 0.069 and is favored by human evaluators in 82.9% of cases compared to the strongest zero-shot VLM baseline.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¹»ç¯ç‰‡å’Œæµ·æŠ¥ç­‰æ–‡æ¡£åœ¨é™æ€ä½å›¾(raster)æ ¼å¼ä¸‹éš¾ä»¥ç¼–è¾‘çš„é—®é¢˜ï¼Œæå‡ºäº†SliDeræ¡†æ¶ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Models, VLMs)å°†ä½å›¾å›¾åƒåæ¸²æŸ“(derendering)ä¸ºç´§å‡‘ä¸”å¯ç¼–è¾‘çš„å¯ç¼©æ”¾çŸ¢é‡å›¾å½¢(SVG)è¡¨ç¤ºã€‚ä¸ä¼ ç»Ÿçš„å‡ ä½•çŸ¢é‡åŒ–æ–¹æ³•ä¸åŒï¼ŒSliDerèƒ½å¤Ÿè¯†åˆ«å¹¶æå–å›¾åƒå’Œæ–‡æœ¬å…ƒç´ çš„è¯­ä¹‰å±æ€§ï¼Œå¹¶æ¨¡ä»¿äººç±»è®¾è®¡è¿‡ç¨‹åœ¨æ¨ç†é˜¶æ®µè¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œä»è€Œç”Ÿæˆæ›´é«˜ä¿çœŸåº¦çš„é‡å»ºç»“æœã€‚ä¸ºäº†æ”¯æŒè¯¥é¢†åŸŸç ”ç©¶ï¼Œç ”ç©¶è€…è¿˜å‘å¸ƒäº†Slide2SVGæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«äº†å¤§é‡æ¥è‡ªçœŸå®ç§‘å­¦æ¼”ç¤ºæ–‡ç¨¿çš„ä½å›¾-SVGé…å¯¹æ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼ŒSliDeråœ¨é‡å»ºè´¨é‡ä¸Šå–å¾—äº†0.069çš„LPIPSå¾—åˆ†ï¼Œä¸”åœ¨ä¸æœ€å¼ºé›¶æ ·æœ¬VLMåŸºå‡†æ¨¡å‹çš„å¯¹æ¯”ä¸­ï¼Œè·å¾—äº†82.9%çš„äººç±»è¯„ä¼°è€…é’çã€‚è¿™ä¸€æˆæœä¸ºå¤æ‚å¤šåª’ä½“æ–‡æ¡£çš„ç»“æ„åŒ–æ¢å¤å’ŒäºŒæ¬¡ç¼–è¾‘æä¾›äº†é«˜æ•ˆçš„è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13478v1",
      "published_date": "2025-11-17 15:16:13 UTC",
      "updated_date": "2025-11-17 15:16:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:56:34.233298+00:00"
    },
    {
      "arxiv_id": "2511.13476v1",
      "title": "Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation",
      "title_zh": "é¢å‘å…¬å…±äº¤é€šç‡ƒæ²¹æ•ˆç‡åˆ†æè‡ªåŠ¨è§£è¯»çš„å¤šæ™ºèƒ½ä½“å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ¡†æ¶",
      "authors": [
        "Zhipeng Ma",
        "Ali Rida Bahja",
        "Andreas Burgdorf",
        "AndrÃ© Pomp",
        "Tobias Meisen",
        "Bo NÃ¸rregaard JÃ¸rgensen",
        "Zheng Grace Ma"
      ],
      "abstract": "Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMultimodal Large Language Models, LLMsï¼‰æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨åŒ–è§£è¯»å…¬å…±äº¤é€šä¸­çš„ç‡ƒæ²¹æ•ˆç‡åˆ†ææ•°æ®ã€‚è¯¥æ¡†æ¶é€šè¿‡åè°ƒæ•°æ®å™äº‹æ™ºèƒ½ä½“ï¼ˆdata narration agentï¼‰ã€LLMè¯„åˆ¤æ™ºèƒ½ä½“ï¼ˆLLM-as-a-judge agentï¼‰åŠäººæœºåä½œè¯„ä¼°å™¨ï¼Œå°†ç¦»æ•£çš„åˆ†æå·¥ä»¶è½¬åŒ–ä¸ºè¿è´¯çš„å†³ç­–æŠ¥å‘Šã€‚ç ”ç©¶åˆ©ç”¨ä¸¹éº¦åŒ—æœ±ç‰¹å…°4006æ¬¡å…¬äº¤è¡Œç¨‹çš„çœŸå®æ•°æ®è¿›è¡ŒéªŒè¯ï¼Œå¹¶é‡‡ç”¨é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGaussian Mixture Modelï¼‰èšç±»åˆ†æã€‚å®éªŒå¯¹æ¯”äº†äº”ç§ä¸»æµLLMså’Œä¸‰ç§æç¤ºèŒƒå¼ï¼Œå‘ç°GPT-4.1 minié…åˆé“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼‰æç¤ºè¡¨ç°æœ€ä¼˜ï¼Œè¾¾åˆ°äº†97.3%çš„å™äº‹å‡†ç¡®ç‡ã€‚ç»“æœè¡¨æ˜ï¼Œå¤šæ™ºèƒ½ä½“ç¼–æ’ï¼ˆmulti-agent orchestrationï¼‰æ˜¾è‘—æå‡äº†æŠ¥å‘Šçš„äº‹å®ç²¾åº¦ä¸è¿è´¯æ€§ï¼Œä¸ºèƒ½æºä¿¡æ¯åŒ–é¢†åŸŸçš„AIé©±åŠ¨å†³ç­–æ”¯æŒæä¾›äº†å…·æœ‰é¢†åŸŸé€‚åº”æ€§çš„æ–¹æ³•è®ºã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13476v1",
      "published_date": "2025-11-17 15:14:17 UTC",
      "updated_date": "2025-11-17 15:14:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:57:16.642288+00:00"
    },
    {
      "arxiv_id": "2512.23710v1",
      "title": "Enriching Historical Records: An OCR and AI-Driven Approach for Database Integration",
      "title_zh": "ä¸°å¯Œå†å²æ¡£æ¡ˆï¼šä¸€ç§åŸºäº OCR ä¸äººå·¥æ™ºèƒ½é©±åŠ¨çš„æ•°æ®åº“é›†æˆæ–¹æ³•",
      "authors": [
        "Zahra Abedi",
        "Richard M. K. van Dijk",
        "Gijs Wijnholds",
        "Tessa Verhoef"
      ],
      "abstract": "This research digitizes and analyzes the Leidse hoogleraren en lectoren 1575-1815 books written between 1983 and 1985, which contain biographic data about professors and curators of Leiden University. It addresses the central question: how can we design an automated pipeline that integrates OCR, LLM-based interpretation, and database linking to harmonize data from historical document images with existing high-quality database records? We applied OCR techniques, generative AI decoding constraints that structure data extraction, and database linkage methods to process typewritten historical records into a digital format. OCR achieved a Character Error Rate (CER) of 1.08 percent and a Word Error Rate (WER) of 5.06 percent, while JSON extraction from OCR text achieved an average accuracy of 63 percent and, based on annotated OCR, 65 percent. This indicates that generative AI somewhat corrects low OCR performance. Our record linkage algorithm linked annotated JSON files with 94% accuracy and OCR-derived JSON files with 81%. This study contributes to digital humanities research by offering an automated pipeline for interpreting digitized historical documents, addressing challenges like layout variability and terminology differences, and exploring the applicability and strength of an advanced generative AI model.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨æ•°å­—åŒ–å¹¶åˆ†æè®°è½½è±é¡¿å¤§å­¦æ•™æˆä¸é¦†é•¿ä¼ è®°ä¿¡æ¯çš„ã€ŠLeidse hoogleraren en lectoren 1575-1815ã€‹ç³»åˆ—ä¸›ä¹¦ï¼Œä»¥è§£å†³å†å²æ–‡æ¡£å›¾åƒä¸ç°æœ‰æ•°æ®åº“çš„æ•´åˆéš¾é¢˜ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§é›†æˆOCRã€åŸºäºLLMçš„è§£é‡Šä»¥åŠæ•°æ®åº“é“¾æ¥ï¼ˆdatabase linkingï¼‰çš„è‡ªåŠ¨åŒ–æµæ°´çº¿ï¼Œåˆ©ç”¨å…·å¤‡è§£ç çº¦æŸçš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å®ç°ç»“æ„åŒ–çš„JSONæ•°æ®æå–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOCRè¯†åˆ«çš„Character Error Rate(CER)ä»…ä¸º1.08%ï¼ŒWord Error Rate(WER)ä¸º5.06%ï¼Œä¸”ç”Ÿæˆå¼AIåœ¨æå–è¿‡ç¨‹ä¸­èƒ½å¤Ÿéƒ¨åˆ†çº æ­£ä½è´¨é‡OCRçš„è¯†åˆ«é”™è¯¯ã€‚åœ¨æœ€ç»ˆçš„è®°å½•é“¾æ¥é˜¶æ®µï¼Œè¯¥ç®—æ³•é’ˆå¯¹OCRè¡ç”Ÿæ•°æ®çš„åŒ¹é…å‡†ç¡®ç‡è¾¾åˆ°äº†81%ã€‚è¿™é¡¹å·¥ä½œä¸ºæ•°å­—äººæ–‡ï¼ˆdigital humanitiesï¼‰ç ”ç©¶æä¾›äº†ä¸€å¥—å¤„ç†å†å²æ–‡çŒ®çš„æœ‰æ•ˆå·¥å…·ï¼ŒæˆåŠŸåº”å¯¹äº†æ–‡æ¡£ç‰ˆå¼å¤šæ ·æ€§å’Œæœ¯è¯­å·®å¼‚ç­‰æŒ‘æˆ˜ï¼Œå¹¶éªŒè¯äº†å…ˆè¿›ç”Ÿæˆå¼AIæ¨¡å‹çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.23710v1",
      "published_date": "2025-11-17 15:13:42 UTC",
      "updated_date": "2025-11-17 15:13:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:56:49.436587+00:00"
    },
    {
      "arxiv_id": "2511.13466v1",
      "title": "The Quick Red Fox gets the best Data Driven Classroom Interviews: A manual for an interview app and its associated methodology",
      "title_zh": "Quick Red Fox åŠ©åŠ›ä¼˜åŒ–æ•°æ®é©±åŠ¨çš„è¯¾å ‚è®¿è°ˆï¼šè®¿è°ˆåº”ç”¨åŠå…¶é…å¥—æ–¹æ³•è®ºæ‰‹å†Œ",
      "authors": [
        "Jaclyn Ocumpaugh",
        "Luc Paquette",
        "Ryan S. Baker",
        "Amanda Barany",
        "Jeff Ginger",
        "Nathan Casano",
        "Andres F. Zambrano",
        "Xiner Liu",
        "Zhanlan Wei",
        "Yiqui Zhou",
        "Qianhui Liu",
        "Stephen Hutt",
        "Alexandra M. A. Andres",
        "Nidhi Nasiar",
        "Camille Giordano",
        "Martin van Velsen",
        "Micheal Mogessi"
      ],
      "abstract": "Data Driven Classroom Interviews (DDCIs) are an interviewing technique that is facilitated by recent technological developments in the learning analytics community. DDCIs are short, targeted interviews that allow researchers to contextualize students' interactions with a digital learning environment (e.g., intelligent tutoring systems or educational games) while minimizing the amount of time that the researcher interrupts that learning experience, and focusing researcher time on the events they most want to focus on DDCIs are facilitated by a research tool called the Quick Red Fox (QRF)--an open-source server-client Android app that optimizes researcher time by directing interviewers to users that have just displayed an interesting behavior (previously defined by the research team). QRF integrates with existing student modeling technologies (e.g., behavior-sensing, affect-sensing, detection of self-regulated learning) to alert researchers to key moments in a learner's experience. This manual documents the tech while providing training on the processes involved in developing triggers and interview techniques; it also suggests methods of analyses.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†ä¸€ç§åä¸ºæ•°æ®é©±åŠ¨è¯¾å ‚è®¿è°ˆ (Data Driven Classroom Interviews, DDCIs) çš„è®¿è°ˆæŠ€æœ¯ï¼Œæ—¨åœ¨åˆ©ç”¨å­¦ä¹ åˆ†ææŠ€æœ¯çš„æœ€æ–°å‘å±•æ¥ä¼˜åŒ–ç ”ç©¶æµç¨‹ã€‚DDCIs é€šè¿‡ç®€çŸ­ä¸”æœ‰é’ˆå¯¹æ€§çš„è®¿è°ˆï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜åœ¨å°½é‡å‡å°‘ä¸­æ–­å­¦ä¹ ä½“éªŒçš„å‰æä¸‹ï¼Œå°†å­¦ç”Ÿä¸æ•°å­—å­¦ä¹ ç¯å¢ƒçš„äº¤äº’å®æ—¶èƒŒæ™¯åŒ–ã€‚è¯¥æ–¹æ³•ç”±å¼€æºçš„ Android åº”ç”¨ç¨‹åº Quick Red Fox (QRF) æä¾›æ”¯æŒï¼Œè¯¥ç¨‹åºé‡‡ç”¨æœåŠ¡å™¨-å®¢æˆ·ç«¯æ¶æ„ï¼Œæ—¨åœ¨é€šè¿‡å¼•å¯¼è®¿è°ˆè€…æ¥è§¦ç‰¹å®šè¡Œä¸ºçš„å­¦ä¹ è€…æ¥ä¼˜åŒ–ç ”ç©¶æ—¶é—´ã€‚QRF èƒ½å¤Ÿæ ¹æ®ç ”ç©¶å›¢é˜Ÿé¢„å®šä¹‰çš„è¡Œä¸ºè§¦å‘å™¨ï¼Œå®æ—¶æé†’ç ”ç©¶äººå‘˜å…³æ³¨å­¦ä¹ è€…ä½“éªŒä¸­çš„å…³é”®æ—¶åˆ»ã€‚ç³»ç»Ÿé›†æˆäº†è¡Œä¸ºæ„ŸçŸ¥ (behavior-sensing)ã€æƒ…æ„Ÿæ„ŸçŸ¥ (affect-sensing) ä»¥åŠè‡ªæˆ‘è°ƒèŠ‚å­¦ä¹ æ£€æµ‹ (detection of self-regulated learning) ç­‰å»ºæ¨¡æŠ€æœ¯ã€‚æœ¬æ‰‹å†Œè¯¦ç»†è®°å½•äº† QRF çš„æŠ€æœ¯å®ç°ï¼Œå¹¶æä¾›äº†å…³äºè§¦å‘å™¨å¼€å‘ã€è®¿è°ˆæŠ€å·§åŠåˆ†ææ–¹æ³•çš„ç³»ç»ŸæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13466v1",
      "published_date": "2025-11-17 15:08:47 UTC",
      "updated_date": "2025-11-17 15:08:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:56:39.138954+00:00"
    },
    {
      "arxiv_id": "2511.13463v1",
      "title": "Multi-task GINN-LP for Multi-target Symbolic Regression",
      "title_zh": "é¢å‘å¤šç›®æ ‡ç¬¦å·å›å½’çš„å¤šä»»åŠ¡ GINN-LP",
      "authors": [
        "Hussein Rajabu",
        "Lijun Qian",
        "Xishuang Dong"
      ],
      "abstract": "In the area of explainable artificial intelligence, Symbolic Regression (SR) has emerged as a promising approach by discovering interpretable mathematical expressions that fit data. However, SR faces two main challenges: most methods are evaluated on scientific datasets with well-understood relationships, limiting generalization, and SR primarily targets single-output regression, whereas many real-world problems involve multi-target outputs with interdependent variables. To address these issues, we propose multi-task regression GINN-LP (MTRGINN-LP), an interpretable neural network for multi-target symbolic regression. By integrating GINN-LP with a multi-task deep learning, the model combines a shared backbone including multiple power-term approximator blocks with task-specific output layers, capturing inter-target dependencies while preserving interpretability. We validate multi-task GINN-LP on practical multi-target applications, including energy efficiency prediction and sustainable agriculture. Experimental results demonstrate competitive predictive performance alongside high interpretability, effectively extending symbolic regression to broader real-world multi-output tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¬¦å·å›å½’(Symbolic Regression)åœ¨å¤„ç†å…·æœ‰ç›¸äº’ä¾èµ–å˜é‡çš„å¤šç›®æ ‡è¾“å‡º(multi-target outputs)æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º MTRGINN-LP çš„å¯è§£é‡Šç¥ç»ç½‘ç»œæ¡†æ¶ã€‚è¯¥æ¡†æ¶å°† GINN-LP ä¸å¤šä»»åŠ¡æ·±åº¦å­¦ä¹ (multi-task deep learning)ç›¸ç»“åˆï¼Œåˆ©ç”¨åŒ…å«å¤šä¸ªå¹‚é¡¹è¿‘ä¼¼å™¨æ¨¡å—(power-term approximator blocks)çš„å…±äº«ä¸»å¹²ä¸ä»»åŠ¡ç‰¹å®šè¾“å‡ºå±‚ï¼Œåœ¨æœ‰æ•ˆæ•æ‰è·¨ç›®æ ‡ä¾èµ–å…³ç³»çš„åŒæ—¶ç¡®ä¿äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚é€šè¿‡åœ¨èƒ½æºæ•ˆç‡é¢„æµ‹å’Œå¯æŒç»­å†œä¸šç­‰å®é™…å¤šç›®æ ‡åœºæ™¯ä¸‹çš„éªŒè¯ï¼Œå®éªŒç»“æœè¯æ˜è¯¥æ¨¡å‹åœ¨å…·å¤‡é«˜å¯è§£é‡Šæ€§çš„åŒæ—¶ï¼Œæ‹¥æœ‰æå…·ç«äº‰åŠ›çš„é¢„æµ‹æ€§èƒ½ã€‚è¯¥ç ”ç©¶æˆåŠŸå°†ç¬¦å·å›å½’çš„åº”ç”¨èŒƒå›´ä»ä¼ ç»Ÿçš„ç§‘å­¦æ•°æ®é›†å•ä¸€è¾“å‡ºä»»åŠ¡æ‰©å±•åˆ°äº†æ›´å¹¿æ³›çš„ç°å®ä¸–ç•Œå¤šè¾“å‡ºé¢†åŸŸã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13463v1",
      "published_date": "2025-11-17 15:07:41 UTC",
      "updated_date": "2025-11-17 15:07:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:56:59.332378+00:00"
    },
    {
      "arxiv_id": "2511.13458v1",
      "title": "Trust in Vision-Language Models: Insights from a Participatory User Workshop",
      "title_zh": "è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„ä¿¡ä»»ï¼šæ¥è‡ªå‚ä¸å¼ç”¨æˆ·ç ”è®¨ä¼šçš„å¯ç¤º",
      "authors": [
        "Agnese Chiatti",
        "Lara Piccolo",
        "Sara Bernardini",
        "Matteo Matteucci",
        "Viola Schiaffonati"
      ],
      "abstract": "With the growing deployment of Vision-Language Models (VLMs), pre-trained on large image-text and video-text datasets, it is critical to equip users with the tools to discern when to trust these systems. However, examining how user trust in VLMs builds and evolves remains an open problem. This problem is exacerbated by the increasing reliance on AI models as judges for experimental validation, to bypass the cost and implications of running participatory design studies directly with users. Following a user-centred approach, this paper presents preliminary results from a workshop with prospective VLM users. Insights from this pilot workshop inform future studies aimed at contextualising trust metrics and strategies for participants' engagement to fit the case of user-VLM interaction.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Models, VLMs)åœ¨å¹¿æ³›åº”ç”¨èƒŒæ™¯ä¸‹ï¼Œç”¨æˆ·å¦‚ä½•è¾¨åˆ«å¹¶å»ºç«‹å¯¹ç³»ç»Ÿä¿¡ä»»è¿™ä¸€æ ¸å¿ƒæŒ‘æˆ˜ã€‚ç”±äºå½“å‰ç ”ç©¶å€¾å‘äºä½¿ç”¨AIæ¨¡å‹ä½œä¸ºè£åˆ¤æ¥æ›¿ä»£æˆæœ¬è¾ƒé«˜çš„å‚ä¸å¼è®¾è®¡ï¼Œç”¨æˆ·ä¿¡ä»»çš„å»ºç«‹ä¸æ¼”åŒ–è¿‡ç¨‹ä»æ˜¯ä¸€ä¸ªå¼€æ”¾è¯¾é¢˜ã€‚è¯¥è®ºæ–‡éµå¾ªä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒ(user-centred)çš„æ–¹æ³•ï¼Œå±•ç¤ºäº†ä¸€åœºé¢å‘æ½œåœ¨VLMç”¨æˆ·çš„å·¥ä½œåŠåˆæ­¥æˆæœã€‚é€šè¿‡æ­¤æ¬¡è¯•ç‚¹å·¥ä½œåŠï¼Œç ”ç©¶è€…è·å¾—äº†å…³é”®æ´å¯Ÿï¼Œç”¨äºåœ¨æœªæ¥çš„ç ”ç©¶ä¸­å°†ä¿¡ä»»åº¦é‡(trust metrics)å’Œå‚ä¸ç­–ç•¥è¿›è¡Œæƒ…å¢ƒåŒ–å¤„ç†ï¼Œä»è€Œæ›´å¥½åœ°åŒ¹é…ç”¨æˆ·ä¸VLMçš„äº¤äº’éœ€æ±‚ã€‚è¿™é¡¹å·¥ä½œä¸ºç†è§£äººæœºäº¤äº’ä¸­ä¿¡ä»»çš„åŠ¨æ€æ¼”å˜æä¾›äº†å®è¯åŸºç¡€ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13458v1",
      "published_date": "2025-11-17 15:04:59 UTC",
      "updated_date": "2025-11-17 15:04:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:57:39.135982+00:00"
    },
    {
      "arxiv_id": "2511.13457v1",
      "title": "Artificial Intelligence-Enabled Spirometry for Early Detection of Right Heart Failure",
      "title_zh": "äººå·¥æ™ºèƒ½èµ‹èƒ½çš„è‚ºé‡è®¡æœ¯ç”¨äºå³å¿ƒè¡°ç«­çš„æ—©æœŸæ£€æµ‹",
      "authors": [
        "Bin Liu",
        "Qinghao Zhao",
        "Yuxi Zhou",
        "Zhejun Sun",
        "Kaijie Lei",
        "Deyun Zhang",
        "Shijia Geng",
        "Shenda Hong"
      ],
      "abstract": "Right heart failure (RHF) is a disease characterized by abnormalities in the structure or function of the right ventricle (RV), which is associated with high morbidity and mortality. Lung disease often causes increased right ventricular load, leading to RHF. Therefore, it is very important to screen out patients with cor pulmonale who develop RHF from people with underlying lung diseases. In this work, we propose a self-supervised representation learning method to early detecting RHF from patients with cor pulmonale, which uses spirogram time series to predict patients with RHF at an early stage. The proposed model is divided into two stages. The first stage is the self-supervised representation learning-based spirogram embedding (SLSE) network training process, where the encoder of the Variational autoencoder (VAE-encoder) learns a robust low-dimensional representation of the spirogram time series from the data-augmented unlabeled data. Second, this low-dimensional representation is fused with demographic information and fed into a CatBoost classifier for the downstream RHF prediction task. Trained and tested on a carefully selected subset of 26,617 individuals from the UK Biobank, our model achieved an AUROC of 0.7501 in detecting RHF, demonstrating strong population-level distinction ability. We further evaluated the model on high-risk clinical subgroups, achieving AUROC values of 0.8194 on a test set of 74 patients with chronic kidney disease (CKD) and 0.8413 on a set of 64 patients with valvular heart disease (VHD). These results highlight the model's potential utility in predicting RHF among clinically elevated-risk populations. In conclusion, this study presents a self-supervised representation learning approach combining spirogram time series and demographic data, demonstrating promising potential for early RHF detection in clinical practice.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ (self-supervised representation learning)çš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡è‚ºæ´»é‡æµ‹å®š(spirogram)æ—¶é—´åºåˆ—æ•°æ®æ—©æœŸæ£€æµ‹è‚ºæºæ€§å¿ƒè„ç—…æ‚£è€…ä¸­çš„å³å¿ƒè¡°ç«­(Right heart failure, RHF)ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ä¸¤é˜¶æ®µæ¶æ„ï¼Œé¦–å…ˆåˆ©ç”¨åŸºäºè‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ çš„è‚ºæ´»é‡åµŒå…¥(SLSE)ç½‘ç»œå’Œå˜åˆ†è‡ªç¼–ç å™¨(VAE-encoder)ä»å¢å¼ºçš„æœªæ ‡æ³¨æ•°æ®ä¸­æå–ä½ç»´é²æ£’ç‰¹å¾ï¼Œéšåå°†å…¶ä¸äººå£ç»Ÿè®¡å­¦ä¿¡æ¯èåˆå¹¶è¾“å…¥CatBooståˆ†ç±»å™¨æ‰§è¡Œé¢„æµ‹ä»»åŠ¡ã€‚åœ¨å¯¹UK Biobankä¸­26,617åä¸ªä½“çš„æµ‹è¯•ä¸­ï¼Œè¯¥æ¨¡å‹å®ç°äº†0.7501çš„AUROCï¼Œå±•ç°äº†å¼ºå¤§çš„ç¾¤ä½“åŒºåˆ†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨æ…¢æ€§è‚¾è„ç—…(CKD)å’Œç“£è†œæ€§å¿ƒè„ç—…(VHD)ç­‰é«˜é£é™©ä¸´åºŠå­ç¾¤ä¸­åˆ†åˆ«å–å¾—äº†0.8194å’Œ0.8413çš„AUROCï¼Œè¿›ä¸€æ­¥éªŒè¯äº†å…¶åœ¨ä¸´åºŠé«˜å±äººç¾¤ä¸­çš„é¢„æµ‹æ•ˆåŠ›ã€‚è¯¥ç ”ç©¶è¯æ˜äº†ç»“åˆè‚ºæ´»é‡æ—¶é—´åºåˆ—æ•°æ®ä¸äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨ä¸´åºŠæ—©æœŸç­›æŸ¥RHFæ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.13457v1",
      "published_date": "2025-11-17 15:03:04 UTC",
      "updated_date": "2025-11-17 15:03:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:57:49.653021+00:00"
    },
    {
      "arxiv_id": "2511.13444v1",
      "title": "Discovering Operational Patterns Using Image-Based Convolutional Clustering and Composite Evaluation: A Case Study in Foundry Melting Processes",
      "title_zh": "åŸºäºå›¾åƒå·ç§¯èšç±»ä¸å¤åˆè¯„ä¼°çš„è¿è¡Œæ¨¡å¼å‘ç°ï¼šä»¥é“¸é€ ç†”ç‚¼è¿‡ç¨‹ä¸ºä¾‹",
      "authors": [
        "Zhipeng Ma",
        "Bo NÃ¸rregaard JÃ¸rgensen",
        "Zheng Grace Ma"
      ],
      "abstract": "Industrial process monitoring increasingly relies on sensor-generated time-series data, yet the lack of labels, high variability, and operational noise make it difficult to extract meaningful patterns using conventional methods. Existing clustering techniques either rely on fixed distance metrics or deep models designed for static data, limiting their ability to handle dynamic, unstructured industrial sequences. Addressing this gap, this paper proposes a novel framework for unsupervised discovery of operational modes in univariate time-series data using image-based convolutional clustering with composite internal evaluation. The proposed framework improves upon existing approaches in three ways: (1) raw time-series sequences are transformed into grayscale matrix representations via overlapping sliding windows, allowing effective feature extraction using a deep convolutional autoencoder; (2) the framework integrates both soft and hard clustering outputs and refines the selection through a two-stage strategy; and (3) clustering performance is objectively evaluated by a newly developed composite score, S_eva, which combines normalized Silhouette, Calinski-Harabasz, and Davies-Bouldin indices. Applied to over 3900 furnace melting operations from a Nordic foundry, the method identifies seven explainable operational patterns, revealing significant differences in energy consumption, thermal dynamics, and production duration. Compared to classical and deep clustering baselines, the proposed approach achieves superior overall performance, greater robustness, and domain-aligned explainability. The framework addresses key challenges in unsupervised time-series analysis, such as sequence irregularity, overlapping modes, and metric inconsistency, and provides a generalizable solution for data-driven diagnostics and energy optimization in industrial systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·¥ä¸šè¿‡ç¨‹ä¸­ä¼ æ„Ÿå™¨ç”Ÿæˆçš„åºåˆ—æ•°æ®ç¼ºä¹æ ‡ç­¾ã€å˜å¼‚æ€§å¤§ä¸”å™ªå£°é«˜çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå›¾åƒå·ç§¯èšç±»(Image-Based Convolutional Clustering)å’Œå¤åˆè¯„ä¼°(Composite Evaluation)çš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºå•å˜é‡æ—¶é—´åºåˆ—(univariate time-series)æ“ä½œæ¨¡å¼çš„æ— ç›‘ç£å‘ç°ã€‚è¯¥æ¡†æ¶é€šè¿‡é‡å æ»‘åŠ¨çª—å£å°†åŸå§‹åºåˆ—è½¬åŒ–ä¸ºç°åº¦çŸ©é˜µï¼Œåˆ©ç”¨æ·±åº¦å·ç§¯è‡ªç¼–ç å™¨(deep convolutional autoencoder)æå–ç‰¹å¾ï¼Œå¹¶ç»“åˆè½¯ç¡¬èšç±»è¾“å‡ºçš„ä¸¤é˜¶æ®µç­–ç•¥è¿›è¡Œä¼˜åŒ–ã€‚ç ”ç©¶è¿˜å¼•å…¥äº†å¤åˆè¯„åˆ† $S_{eva}$ï¼ˆé›†æˆ Silhouetteã€Calinski-Harabasz å’Œ Davies-Bouldin æŒ‡æ ‡ï¼‰ä»¥å®¢è§‚è¯„ä¼°èšç±»æ•ˆæœã€‚åœ¨åŒ—æ¬§æŸé“¸é€ å‚3900ä½™æ¬¡ç†”ç‚¼æ“ä½œçš„æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼Œè¯¥æ–¹æ³•è¯†åˆ«å‡ºä¸ƒç§åœ¨èƒ½è€—ã€çƒ­åŠ›å­¦åŠæ—¶é•¿ä¸Šå…·æœ‰æ˜¾è‘—å·®å¼‚çš„å¯è§£é‡Šæ“ä½œæ¨¡å¼ã€‚ç›¸æ¯”ä¼ ç»Ÿå’Œæ·±åº¦èšç±»åŸºå‡†æ¨¡å‹ï¼Œè¯¥æ–¹æ¡ˆåœ¨ç¨³å¥æ€§å’Œé¢†åŸŸå¯è§£é‡Šæ€§ä¸Šè¡¨ç°æ›´ä¼˜ï¼Œä¸ºå·¥ä¸šæ•°æ®é©±åŠ¨çš„è¯Šæ–­å’Œèƒ½æºä¼˜åŒ–æä¾›äº†å…·æœ‰æ¨å¹¿ä»·å€¼çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13444v1",
      "published_date": "2025-11-17 14:50:44 UTC",
      "updated_date": "2025-11-17 14:50:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:57:43.135421+00:00"
    },
    {
      "arxiv_id": "2511.13442v2",
      "title": "Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline",
      "title_zh": "é‡Šæ”¾åŸç”Ÿ MLLM çš„ä¼ªé€ æ£€æµ‹æ½œåŠ›ï¼šä¸€ç§æ–°å‹å…è®­ç»ƒæµæ°´çº¿",
      "authors": [
        "Rui Zuo",
        "Qinyue Tong",
        "Zhe-Ming Lu",
        "Ziqian Lu"
      ],
      "abstract": "With the rapid advancement of artificial intelligence-generated content (AIGC) technologies, including multimodal large language models (MLLMs) and diffusion models, image generation and manipulation have become remarkably effortless. Existing image forgery detection and localization (IFDL) methods often struggle to generalize across diverse datasets and offer limited interpretability. Nowadays, MLLMs demonstrate strong generalization potential across diverse vision-language tasks, and some studies introduce this capability to IFDL via large-scale training. However, such approaches cost considerable computational resources, while failing to reveal the inherent generalization potential of vanilla MLLMs to address this problem. Inspired by this observation, we propose Foresee, a training-free MLLM-based pipeline tailored for image forgery analysis. It eliminates the need for additional training and enables a lightweight inference process, while surpassing existing MLLM-based methods in both tamper localization accuracy and the richness of textual explanations. Foresee employs a type-prior-driven strategy and utilizes a Flexible Feature Detector (FFD) module to specifically handle copy-move manipulations, thereby effectively unleashing the potential of vanilla MLLMs in the forensic domain. Extensive experiments demonstrate that our approach simultaneously achieves superior localization accuracy and provides more comprehensive textual explanations. Moreover, Foresee exhibits stronger generalization capability, outperforming existing IFDL methods across various tampering types, including copy-move, splicing, removal, local enhancement, deepfake, and AIGC-based editing. The code will be released in the final version.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Foreseeï¼Œä¸€ç§åŸºäºvanilla MLLMsçš„æ— éœ€è®­ç»ƒ(training-free)çš„å›¾åƒå–è¯åˆ†ææµæ°´çº¿ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å›¾åƒç¯¡æ”¹æ£€æµ‹ä¸å®šä½(IFDL)æ–¹æ³•åœ¨æ³›åŒ–èƒ½åŠ›å’Œå¯è§£é‡Šæ€§æ–¹é¢çš„å±€é™ã€‚Foreseeé€šè¿‡é‡‡ç”¨type-prior-drivenç­–ç•¥ï¼Œå¹¶ç»“åˆFlexible Feature Detector (FFD)æ¨¡å—ä¸“é—¨å¤„ç†copy-moveç¯¡æ”¹ï¼Œæœ‰æ•ˆé‡Šæ”¾äº†åŸç”Ÿå¤§æ¨¡å‹åœ¨å–è¯é¢†åŸŸçš„æ½œèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç¯¡æ”¹å®šä½ç²¾åº¦å’Œæ–‡æœ¬è§£é‡Šçš„ä¸°å¯Œåº¦ä¸Šå‡ä¼˜äºç°æœ‰çš„åŸºäºMLLMçš„æ–¹æ³•ï¼Œä¸”å…·å¤‡è½»é‡çº§çš„æ¨ç†ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼ŒForeseeåœ¨splicingã€removalã€deepfakeåŠAIGC-based editingç­‰å¤šç§ç¯¡æ”¹ç±»å‹ä¸­å‡å±•ç°å‡ºå“è¶Šçš„æ³›åŒ–æ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ä»…å®ç°äº†é«˜ç²¾åº¦çš„å–è¯åˆ†æï¼Œè¿˜ä¸ºæ„å»ºå…·å¤‡å¼ºæ³›åŒ–æ€§å’Œé«˜è§£é‡Šæ€§çš„å›¾åƒå®‰å…¨æ£€æµ‹ç³»ç»Ÿæä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13442v2",
      "published_date": "2025-11-17 14:49:57 UTC",
      "updated_date": "2025-11-18 06:58:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:57:54.937401+00:00"
    },
    {
      "arxiv_id": "2511.17592v1",
      "title": "GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms",
      "title_zh": "GigaEvoï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹ä¸æ¼”åŒ–ç®—æ³•çš„å¼€æºä¼˜åŒ–æ¡†æ¶",
      "authors": [
        "Valentin Khrulkov",
        "Andrey Galichin",
        "Denis Bashkirov",
        "Dmitry Vinichenko",
        "Oleg Travkin",
        "Roman Alferov",
        "Andrey Kuznetsov",
        "Ivan Oseledets"
      ],
      "abstract": "Recent advances in LLM-guided evolutionary computation, particularly AlphaEvolve (Novikov et al., 2025; Georgiev et al., 2025), have demonstrated remarkable success in discovering novel mathematical constructions and solving challenging optimization problems. However, the high-level descriptions in published work leave many implementation details unspecified, hindering reproducibility and further research. In this report we present GigaEvo, an extensible open-source framework that enables researchers to study and experiment with hybrid LLM-evolution approaches inspired by AlphaEvolve. Our system provides modular implementations of key components: MAP-Elites quality-diversity algorithms, asynchronous DAG-based evaluation pipelines, LLM-driven mutation operators with insight generation and bidirectional lineage tracking, and flexible multi-island evolutionary strategies. In order to assess reproducibility and validate our implementation we evaluate GigaEvo on challenging problems from the AlphaEvolve paper: Heilbronn triangle placement, circle packing in squares, and high-dimensional kissing numbers. The framework emphasizes modularity, concurrency, and ease of experimentation, enabling rapid prototyping through declarative configuration. We provide detailed descriptions of system architecture, implementation decisions, and experimental methodology to support further research in LLM driven evolutionary methods. The GigaEvo framework and all experimental code are available at https://github.com/AIRI-Institute/gigaevo-core.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†GigaEvoï¼Œè¿™æ˜¯ä¸€ä¸ªå—AlphaEvolveå¯å‘çš„å¯æ‰©å±•å¼€æºæ¡†æ¶ï¼Œæ—¨åœ¨ç»“åˆå¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸è¿›åŒ–ç®—æ³•(Evolution Algorithms)æ¥è§£å†³å¤æ‚çš„ä¼˜åŒ–é—®é¢˜ã€‚è¯¥ç³»ç»Ÿæä¾›äº†æ¨¡å—åŒ–çš„åŠŸèƒ½å®ç°ï¼ŒåŒ…æ‹¬MAP-Elitesè´¨é‡å¤šæ ·æ€§ç®—æ³•ã€åŸºäºæœ‰å‘æ— ç¯å›¾(DAG)çš„å¼‚æ­¥è¯„ä¼°æµæ°´çº¿ï¼Œä»¥åŠå…·å¤‡è§è§£ç”Ÿæˆå’ŒåŒå‘è°±ç³»è·Ÿè¸ªåŠŸèƒ½çš„LLMé©±åŠ¨å˜å¼‚ç®—å­ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨Heilbronn triangle placementã€circle packing in squareså’Œé«˜ç»´kissing numbersç­‰æŒ‘æˆ˜æ€§é—®é¢˜ä¸ŠéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ä¸å¤ç°æ€§ã€‚GigaEvoå¼ºè°ƒæ¨¡å—åŒ–ã€å¹¶å‘æ€§å’Œå®éªŒç®€ä¾¿æ€§ï¼Œé€šè¿‡å£°æ˜å¼é…ç½®æ”¯æŒå¿«é€ŸåŸå‹å¼€å‘ï¼Œä¸ºå¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„è¿›åŒ–æ–¹æ³•ç ”ç©¶æä¾›äº†é€æ˜çš„ç³»ç»Ÿæ¶æ„å’Œå¯é çš„å®éªŒåŸºç¡€ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17592v1",
      "published_date": "2025-11-17 14:44:47 UTC",
      "updated_date": "2025-11-17 14:44:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:57:50.430837+00:00"
    },
    {
      "arxiv_id": "2511.13418v1",
      "title": "Exploring Multi-Table Retrieval Through Iterative Search",
      "title_zh": "åŸºäºè¿­ä»£æœç´¢çš„å¤šè¡¨æ£€ç´¢æ¢ç©¶",
      "authors": [
        "Allaa Boutaleb",
        "Bernd Amann",
        "Rafael Angarita",
        "Hubert Naacke"
      ],
      "abstract": "Open-domain question answering over datalakes requires retrieving and composing information from multiple tables, a challenging subtask that demands semantic relevance and structural coherence (e.g., joinability). While exact optimization methods like Mixed-Integer Programming (MIP) can ensure coherence, their computational complexity is often prohibitive. Conversely, simpler greedy heuristics that optimize for query coverage alone often fail to find these coherent, joinable sets. This paper frames multi-table retrieval as an iterative search process, arguing this approach offers advantages in scalability, interpretability, and flexibility. We propose a general framework and a concrete instantiation: a fast, effective Greedy Join-Aware Retrieval algorithm that holistically balances relevance, coverage, and joinability. Experiments across 5 NL2SQL benchmarks demonstrate that our iterative method achieves competitive retrieval performance compared to the MIP-based approach while being 4-400x faster depending on the benchmark and search space settings. This work highlights the potential of iterative heuristics for practical, scalable, and composition-aware retrieval.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ•°æ®æ¹–ç¯å¢ƒä¸‹çš„ Multi-Table Retrieval é—®é¢˜ï¼Œæ—¨åœ¨è§£å†³å¼€æ”¾åŸŸé—®ç­”ä¸­ä¿¡æ¯ç»„åˆæ‰€éœ€çš„è¯­ä¹‰ç›¸å…³æ€§å’Œç»“æ„è¿è´¯æ€§æŒ‘æˆ˜ã€‚ç”±äºä¼ ç»Ÿçš„ Mixed-Integer Programming (MIP) æ–¹æ³•è®¡ç®—æˆæœ¬è¿‡é«˜ï¼Œè€Œç®€å•çš„è´ªå©ªå¯å‘å¼ç®—æ³•å¾€å¾€éš¾ä»¥æ»¡è¶³è¡¨çš„ joinability è¦æ±‚ï¼Œæœ¬æ–‡æå‡ºå°†å¤šè¡¨æ£€ç´¢å»ºæ¨¡ä¸ºä¸€ä¸ª Iterative Search è¿‡ç¨‹ã€‚ä½œè€…è®¾è®¡äº†ä¸€ä¸ªé€šç”¨çš„æ¡†æ¶åŠå…¶å®ä¾‹åŒ–ç®—æ³• Greedy Join-Aware Retrievalï¼Œèƒ½å¤Ÿåœ¨æ£€ç´¢è¿‡ç¨‹ä¸­ç»¼åˆå¹³è¡¡ç›¸å…³æ€§ã€æŸ¥è¯¢è¦†ç›–ç‡å’Œè¡¨çš„å¯è¿æ¥æ€§ã€‚åœ¨ 5 ä¸ª NL2SQL åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥è¿­ä»£æ–¹æ³•åœ¨è¾¾åˆ°ä¸ MIP æ–¹æ¡ˆç›¸å½“æ€§èƒ½çš„åŒæ—¶ï¼Œè¿è¡Œé€Ÿåº¦æå‡äº† 4 åˆ° 400 å€ã€‚è¿™é¡¹å·¥ä½œçªå‡ºäº†è¿­ä»£å¯å‘å¼ç®—æ³•åœ¨æ„å»ºå®ç”¨ä¸”å…·å¤‡ç»„åˆæ„ŸçŸ¥èƒ½åŠ›çš„å¤§è§„æ¨¡æ£€ç´¢ç³»ç»Ÿä¸­çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.DB",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted @ the AI for Tabular Data Workshop, EurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.13418v1",
      "published_date": "2025-11-17 14:31:33 UTC",
      "updated_date": "2025-11-17 14:31:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:57:58.843551+00:00"
    },
    {
      "arxiv_id": "2511.13414v1",
      "title": "PAST: A Primary-Auxiliary Spatio-Temporal Network for Traffic Time Series Imputation",
      "title_zh": "PASTï¼šä¸€ç§é¢å‘äº¤é€šæ—¶é—´åºåˆ—æ’è¡¥çš„ä¸»è¾…æ—¶ç©ºç½‘ç»œ",
      "authors": [
        "Hanwen Hu",
        "Zimo Wen",
        "Shiyou Qian",
        "Jian Co"
      ],
      "abstract": "Traffic time series imputation is crucial for the safety and reliability of intelligent transportation systems, while diverse types of missing data, including random, fiber, and block missing make the imputation task challenging. Existing models often focus on disentangling and separately modeling spatial and temporal patterns based on relationships between data points. However, these approaches struggle to adapt to the random missing positions, and fail to learn long-term and large-scale dependencies, which are essential in extensive missing conditions. In this paper, patterns are categorized into two types to handle various missing data conditions: primary patterns, which originate from internal relationships between data points, and auxiliary patterns, influenced by external factors like timestamps and node attributes. Accordingly, we propose the Primary-Auxiliary Spatio-Temporal network (PAST). It comprises a graph-integrated module (GIM) and a cross-gated module (CGM). GIM captures primary patterns via dynamic graphs with interval-aware dropout and multi-order convolutions, and CGM extracts auxiliary patterns through bidirectional gating on embedded external features. The two modules interact via shared hidden vectors and are trained under an ensemble self-supervised framework. Experiments on three datasets under 27 missing data conditions demonstrate that the imputation accuracy of PAST outperforms seven state-of-the-art baselines by up to 26.2% in RMSE and 31.6% in MAE.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ™ºèƒ½äº¤é€šç³»ç»Ÿä¸­ç”±äºéšæœºã€å…‰çº¤å’Œå—çŠ¶ç¼ºå¤±ç­‰å¤šæ ·åŒ–ç¼ºå¤±æ•°æ®å¯¼è‡´çš„äº¤é€šæ—¶é—´åºåˆ—è¡¥å…¨(Traffic Time Series Imputation)éš¾é¢˜ï¼ŒæŒ‡å‡ºç°æœ‰æ¨¡å‹åœ¨é€‚åº”éšæœºç¼ºå¤±ä½ç½®ä»¥åŠå­¦ä¹ é•¿ç¨‹å’Œå¤§è§„æ¨¡ä¾èµ–å…³ç³»æ–¹é¢çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸»è¾…æ—¶ç©ºç½‘ç»œ(Primary-Auxiliary Spatio-Temporal network, PAST)ï¼Œé€šè¿‡å°†æ¨¡å¼åˆ†ä¸ºç”±æ•°æ®ç‚¹å†…éƒ¨å…³ç³»æ„æˆçš„ä¸»è¦æ¨¡å¼(primary patterns)å’Œå—æ—¶é—´æˆ³ã€èŠ‚ç‚¹å±æ€§ç­‰å¤–éƒ¨å› ç´ å½±å“çš„è¾…åŠ©æ¨¡å¼(auxiliary patterns)æ¥åº”å¯¹å¤æ‚çš„ç¼ºå¤±æƒ…å†µã€‚è¯¥ç½‘ç»œåŒ…å«å›¾é›†æˆæ¨¡å—(GIM)ï¼Œåˆ©ç”¨å…·æœ‰é—´éš”æ„ŸçŸ¥éšæœºå¤±æ´»(interval-aware dropout)å’Œå¤šé˜¶å·ç§¯(multi-order convolutions)çš„åŠ¨æ€å›¾æ¥æ•è·ä¸»è¦æ¨¡å¼ã€‚åŒæ—¶ï¼Œäº¤å‰é—¨æ§æ¨¡å—(CGM)é€šè¿‡å¯¹åµŒå…¥çš„å¤–éƒ¨ç‰¹å¾è¿›è¡ŒåŒå‘é—¨æ§å¤„ç†æ¥æå–è¾…åŠ©æ¨¡å¼ï¼Œä¸¤ä¸ªæ¨¡å—é€šè¿‡å…±äº«éšè—å‘é‡è¿›è¡Œäº¤äº’ï¼Œå¹¶åœ¨é›†æˆè‡ªç›‘ç£æ¡†æ¶(ensemble self-supervised framework)ä¸‹è¿›è¡Œè®­ç»ƒã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†å’Œ27ç§ç¼ºå¤±æ¡ä»¶ä¸‹çš„å®éªŒè¯æ˜ï¼ŒPASTçš„è¡¥å…¨ç²¾åº¦æ˜¾è‘—ä¼˜äºä¸ƒç§æœ€å…ˆè¿›çš„åŸºçº¿æ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•åœ¨å‡æ–¹æ ¹è¯¯å·®(RMSE)å’Œå¹³å‡ç»å¯¹è¯¯å·®(MAE)ä¸Šåˆ†åˆ«å®ç°äº†é«˜è¾¾26.2%å’Œ31.6%çš„æ€§èƒ½æå‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13414v1",
      "published_date": "2025-11-17 14:28:29 UTC",
      "updated_date": "2025-11-17 14:28:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:58:07.133639+00:00"
    },
    {
      "arxiv_id": "2511.13411v1",
      "title": "An Operational Kardashev-Style Scale for Autonomous AI - Towards AGI and Superintelligence",
      "title_zh": "è‡ªä¸»äººå·¥æ™ºèƒ½çš„å¯æ“ä½œå¡å°”è¾¾èˆå¤«å¼ç­‰çº§ï¼šè¿ˆå‘é€šç”¨äººå·¥æ™ºèƒ½ä¸è¶…çº§æ™ºèƒ½",
      "authors": [
        "Przemyslaw Chojecki"
      ],
      "abstract": "We propose a Kardashev-inspired yet operational Autonomous AI (AAI) Scale that measures the progression from fixed robotic process automation (AAI-0) to full artificial general intelligence (AAI-4) and beyond. Unlike narrative ladders, our scale is multi-axis and testable. We define ten capability axes (Autonomy, Generality, Planning, Memory/Persistence, Tool Economy, Self-Revision, Sociality/Coordination, Embodiment, World-Model Fidelity, Economic Throughput) aggregated by a composite AAI-Index (a weighted geometric mean). We introduce a measurable Self-Improvement Coefficient $Îº$ (capability growth per unit of agent-initiated resources) and two closure properties (maintenance and expansion) that convert ``self-improving AI'' into falsifiable criteria. We specify OWA-Bench, an open-world agency benchmark suite that evaluates long-horizon, tool-using, persistent agents. We define level gates for AAI-0\\ldots AAI-4 using thresholds on the axes, $Îº$, and closure proofs. Synthetic experiments illustrate how present-day systems map onto the scale and how the delegability frontier (quality vs.\\ autonomy) advances with self-improvement. We also prove a theorem that AAI-3 agent becomes AAI-5 over time with sufficient conditions, formalizing \"baby AGI\" becomes Superintelligence intuition.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå—Kardashevç­‰çº§å¯å‘çš„ã€å¯æ“ä½œçš„è‡ªä¸»äººå·¥æ™ºèƒ½ç­‰çº§é‡è¡¨(Autonomous AI Scale, AAI Scale)ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°è¡¡é‡ä»å›ºå®šæœºå™¨äººæµç¨‹è‡ªåŠ¨åŒ–(AAI-0)åˆ°å®Œå…¨é€šç”¨äººå·¥æ™ºèƒ½(AAI-4)åŠæ›´é«˜é˜¶æ®µçš„æ¼”è¿›è¿‡ç¨‹ã€‚ä¸ä¼ ç»Ÿçš„å™äº‹æ€§æè¿°ä¸åŒï¼Œè¯¥é‡è¡¨ç”±Autonomyã€Generalityã€Planningå’ŒWorld-Model Fidelityç­‰åä¸ªèƒ½åŠ›è½´ç»„æˆï¼Œå¹¶é€šè¿‡è®¡ç®—åŠ æƒå‡ ä½•å¹³å‡å€¼å¾—åˆ°ç»¼åˆæŒ‡æ•°(AAI-Index)ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†å¯è¡¡é‡çš„è‡ªæˆ‘æ”¹è¿›ç³»æ•°(Self-Improvement Coefficient $\\kappa$)ä»¥åŠç»´æŠ¤ä¸æ‰©å¼ ä¸¤ç§é—­ç¯å±æ€§ï¼Œå°†è‡ªæˆ‘æ”¹è¿›èƒ½åŠ›è½¬åŒ–ä¸ºå¯è¯ä¼ªçš„è¯„åˆ¤æ ‡å‡†ã€‚ä¸ºäº†è¿›è¡Œå®è¯è¯„ä¼°ï¼Œä½œè€…è®¾è®¡äº†ä¸“é—¨é’ˆå¯¹é•¿æ—¶ç¨‹ã€å·¥å…·ä½¿ç”¨åŠæŒä¹…æ€§æ™ºèƒ½ä½“çš„OWA-BenchåŸºå‡†å¥—ä»¶ã€‚é€šè¿‡è®¾å®šå„èƒ½åŠ›è½´é˜ˆå€¼å’Œ$\\kappa$ç³»æ•°ï¼Œè¯¥æ¡†æ¶ä¸ºAAI-0è‡³AAI-4å®šä¹‰äº†æ˜ç¡®çš„å±‚çº§é—¨æ§›ã€‚æœ€ç»ˆï¼Œè®ºæ–‡é€šè¿‡å®šç†è¯æ˜äº†AAI-3çº§åˆ«æ™ºèƒ½ä½“åœ¨å……åˆ†æ¡ä»¶ä¸‹å¯éšæ—¶é—´æ¼”åŒ–ä¸ºAAI-5çº§è¶…çº§æ™ºèƒ½(Superintelligence)ï¼Œä»è€Œä¸ºâ€œBaby AGIâ€è¿ˆå‘è¶…æ™ºèƒ½çš„ç›´è§‰æä¾›äº†å½¢å¼åŒ–çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13411v1",
      "published_date": "2025-11-17 14:24:27 UTC",
      "updated_date": "2025-11-17 14:24:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:58:04.239370+00:00"
    },
    {
      "arxiv_id": "2511.13399v1",
      "title": "TripleFDS: Triple Feature Disentanglement and Synthesis for Scene Text Editing",
      "title_zh": "TripleFDSï¼šé¢å‘åœºæ™¯æ–‡æœ¬ç¼–è¾‘çš„ä¸‰é‡ç‰¹å¾è§£è€¦ä¸åˆæˆ",
      "authors": [
        "Yuchen Bao",
        "Yiting Wang",
        "Wenjian Huang",
        "Haowei Wang",
        "Shen Chen",
        "Taiping Yao",
        "Shouhong Ding",
        "Jianguo Zhang"
      ],
      "abstract": "Scene Text Editing (STE) aims to naturally modify text in images while preserving visual consistency, the decisive factors of which can be divided into three parts, i.e., text style, text content, and background. Previous methods have struggled with incomplete disentanglement of editable attributes, typically addressing only one aspect - such as editing text content - thus limiting controllability and visual consistency. To overcome these limitations, we propose TripleFDS, a novel framework for STE with disentangled modular attributes, and an accompanying dataset called SCB Synthesis. SCB Synthesis provides robust training data for triple feature disentanglement by utilizing the \"SCB Group\", a novel construct that combines three attributes per image to generate diverse, disentangled training groups. Leveraging this construct as a basic training unit, TripleFDS first disentangles triple features, ensuring semantic accuracy through inter-group contrastive regularization and reducing redundancy through intra-sample multi-feature orthogonality. In the synthesis phase, TripleFDS performs feature remapping to prevent \"shortcut\" phenomena during reconstruction and mitigate potential feature leakage. Trained on 125,000 SCB Groups, TripleFDS achieves state-of-the-art image fidelity (SSIM of 44.54) and text accuracy (ACC of 93.58%) on the mainstream STE benchmarks. Besides superior performance, the more flexible editing of TripleFDS supports new operations such as style replacement and background transfer. Code: https://github.com/yusenbao01/TripleFDS",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœºæ™¯æ–‡æœ¬ç¼–è¾‘(Scene Text Editing)ä¸­å±æ€§è§£è€¦ä¸å®Œæ•´å¯¼è‡´çš„æ§åˆ¶åŠ›ä¸è§†è§‰ä¸€è‡´æ€§å—é™é—®é¢˜ï¼Œæå‡ºäº†åä¸ºTripleFDSçš„å…¨æ–°æ¡†æ¶ä»¥åŠé…å¥—çš„SCB Synthesisæ•°æ®é›†ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒåœ¨äºå®ç°äº†æ–‡æœ¬æ ·å¼(text style)ã€æ–‡æœ¬å†…å®¹(text content)å’ŒèƒŒæ™¯(background)çš„ä¸‰é‡ç‰¹å¾è§£è€¦ï¼Œå¹¶é€šè¿‡æ–°é¢–çš„â€œSCB Groupâ€æ„é€ ç”Ÿæˆå¤šæ ·åŒ–çš„è®­ç»ƒæ•°æ®ã€‚TripleFDSåˆ©ç”¨ç»„é—´å¯¹æ¯”æ­£åˆ™åŒ–(inter-group contrastive regularization)ç¡®ä¿è¯­ä¹‰å‡†ç¡®æ€§ï¼Œå¹¶ç»“åˆæ ·æœ¬å†…å¤šç‰¹å¾æ­£äº¤æ€§(intra-sample multi-feature orthogonality)å‡å°‘ä¿¡æ¯å†—ä½™ã€‚åœ¨åˆæˆé˜¶æ®µï¼Œè¯¥æ–¹æ³•å¼•å…¥ç‰¹å¾é‡æ˜ å°„(feature remapping)æŠ€æœ¯ï¼Œæœ‰æ•ˆé˜²æ­¢äº†é‡æ„è¿‡ç¨‹ä¸­çš„â€œæ·å¾„â€ç°è±¡å¹¶ç¼“è§£äº†ç‰¹å¾æ³„æ¼ã€‚å®éªŒè¡¨æ˜ï¼ŒTripleFDSåœ¨ä¸»æµåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„å›¾åƒä¿çœŸåº¦(SSIM 44.54)å’Œæ–‡æœ¬å‡†ç¡®ç‡(ACC 93.58%)ã€‚é™¤äº†å“è¶Šçš„æ€§èƒ½ï¼Œè¯¥æ¡†æ¶è¿˜æ”¯æŒæ ·å¼æ›¿æ¢(style replacement)å’ŒèƒŒæ™¯ä¼ è¾“(background transfer)ç­‰æ›´ä¸ºçµæ´»çš„æ–°å‹ç¼–è¾‘æ“ä½œã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by AAAI2026",
      "pdf_url": "https://arxiv.org/pdf/2511.13399v1",
      "published_date": "2025-11-17 14:15:03 UTC",
      "updated_date": "2025-11-17 14:15:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:58:08.134811+00:00"
    },
    {
      "arxiv_id": "2511.13397v1",
      "title": "Descriptor: Distance-Annotated Traffic Perception Question Answering (DTPQA)",
      "title_zh": "Descriptorï¼šå¸¦è·ç¦»æ ‡æ³¨çš„äº¤é€šæ„ŸçŸ¥é—®ç­” (DTPQA)",
      "authors": [
        "Nikos Theodoridis",
        "Tim Brophy",
        "Reenu Mohandas",
        "Ganesh Sistu",
        "Fiachra Collins",
        "Anthony Scanlan",
        "Ciaran Eising"
      ],
      "abstract": "The remarkable progress of Vision-Language Models (VLMs) on a variety of tasks has raised interest in their application to automated driving. However, for these models to be trusted in such a safety-critical domain, they must first possess robust perception capabilities, i.e., they must be capable of understanding a traffic scene, which can often be highly complex, with many things happening simultaneously. Moreover, since critical objects and agents in traffic scenes are often at long distances, we require systems with not only strong perception capabilities at close distances (up to 20 meters), but also at long (30+ meters) range. Therefore, it is important to evaluate the perception capabilities of these models in isolation from other skills like reasoning or advanced world knowledge. Distance-Annotated Traffic Perception Question Answering (DTPQA) is a Visual Question Answering (VQA) benchmark designed specifically for this purpose: it can be used to evaluate the perception systems of VLMs in traffic scenarios using trivial yet crucial questions relevant to driving decisions. It consists of two parts: a synthetic benchmark (DTP-Synthetic) created using a simulator, and a real-world benchmark (DTP-Real) built on top of existing images of real traffic scenes. Additionally, DTPQA includes distance annotations, i.e., how far the object in question is from the camera. More specifically, each DTPQA sample consists of (at least): (a) an image, (b) a question, (c) the ground truth answer, and (d) the distance of the object in question, enabling analysis of how VLM performance degrades with increasing object distance. In this article, we provide the dataset itself along with the Python scripts used to create it, which can be used to generate additional data of the same kind.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Distance-Annotated Traffic Perception Question Answering (DTPQA)ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼° Vision-Language Models (VLMs) åœ¨äº¤é€šåœºæ™¯ä¸­åŸºç¡€æ„ŸçŸ¥èƒ½åŠ›çš„è§†è§‰é—®ç­” (Visual Question Answering, VQA) åŸºå‡†ã€‚é’ˆå¯¹è‡ªåŠ¨é©¾é©¶ä¸­è¿œè·ç¦»ç›®æ ‡æ„ŸçŸ¥è¿™ä¸€å®‰å…¨å…³é”®éš¾é¢˜ï¼Œè¯¥åŸºå‡†å°†æ„ŸçŸ¥èƒ½åŠ›ä¸æ¨ç†æˆ–é«˜çº§å…ˆéªŒçŸ¥è¯†è§£è€¦ï¼Œèšç„¦äºå¯¹é©¾é©¶å†³ç­–è‡³å…³é‡è¦çš„æ ¸å¿ƒæ„ŸçŸ¥ä»»åŠ¡ã€‚DTPQA ç”±åŸºäºä»¿çœŸæ¨¡æ‹Ÿå™¨ç”Ÿæˆçš„ DTP-Synthetic å’ŒåŸºäºç°å®äº¤é€šåœºæ™¯æ„å»ºçš„ DTP-Real ä¸¤éƒ¨åˆ†ç»„æˆï¼Œå¹¶ä¸ºæ¯ä¸ªæ ·æœ¬æä¾›äº†è¯¦ç»†çš„è·ç¦»æ ‡æ³¨ã€‚é€šè¿‡è¿™äº›è·ç¦»æ ‡æ³¨ï¼Œç ”ç©¶è€…èƒ½å¤Ÿç³»ç»Ÿåœ°é‡åŒ–åˆ†æ VLMs çš„æ„ŸçŸ¥æ€§èƒ½å¦‚ä½•éšç‰©ä½“è·ç¦»çš„å¢åŠ ï¼ˆç‰¹åˆ«æ˜¯30ç±³ä»¥ä¸Šçš„è¿œè·ç¦»ï¼‰è€Œå‘ç”Ÿè¡°å‡ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å‘å¸ƒäº†å®Œæ•´çš„æ•°æ®é›†åŠç”¨äºç”Ÿæˆæ­¤ç±»æ•°æ®çš„ Python è„šæœ¬ï¼Œä¸ºå¼€å‘å’Œæµ‹è¯•æ›´ç¨³å¥çš„è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ç³»ç»Ÿæä¾›äº†é‡è¦çš„è¯„ä»·å·¥å…·å’Œèµ„æºæ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13397v1",
      "published_date": "2025-11-17 14:12:22 UTC",
      "updated_date": "2025-11-17 14:12:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:58:08.828558+00:00"
    },
    {
      "arxiv_id": "2511.13391v2",
      "title": "Finding Kissing Numbers with Game-theoretic Reinforcement Learning",
      "title_zh": "åˆ©ç”¨åšå¼ˆè®ºå¼ºåŒ–å­¦ä¹ æ±‚è§£æ¥å»æ•°",
      "authors": [
        "Chengdong Ma",
        "ThÃ©o Tao Zhaowei",
        "Pengyu Li",
        "Minghao Liu",
        "Haojun Chen",
        "Zihao Mao",
        "Yuan Cheng",
        "Yuan Qi",
        "Yaodong Yang"
      ],
      "abstract": "Since Isaac Newton first studied the Kissing Number Problem in 1694, determining the maximal number of non-overlapping spheres around a central sphere has remained a fundamental challenge. This problem represents the local analogue of Hilbert's 18th problem on sphere packing, bridging geometry, number theory, and information theory. Although significant progress has been made through lattices and codes, the irregularities of high-dimensional geometry and exponentially growing combinatorial complexity beyond 8 dimensions, which exceeds the complexity of Go game, limit the scalability of existing methods. Here we model this problem as a two-player matrix completion game that can be fully parallelized at large scale, and train the game-theoretic reinforcement learning system, PackingStar, to efficiently explore high-dimensional spaces. The matrix entries represent pairwise cosines of sphere center vectors; one player fills entries while another corrects suboptimal ones, jointly maximizing the matrix size, corresponding to the kissing number. This cooperative dynamics substantially improves sample quality, making the extremely large spaces tractable. PackingStar reproduces previous configurations and surpasses all human-known records from dimensions 25 to 31, with the configuration in 25 dimensions geometrically corresponding to the Leech lattice and suggesting possible optimality. It achieves the first breakthrough beyond rational structures from 1971 in 13 dimensions, discovers over 6000 new structures in 14 and other dimensions, and establishes new records for generalized kissing configurations under various angular constraints. These results demonstrate AI's power to explore high-dimensional spaces beyond human intuition and open new pathways for the Kissing Number Problem and broader geometry problems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Kissing Number Problemï¼Œå³åœ¨ $d$ ç»´ç©ºé—´ä¸­ç¡®å®šä¸­å¿ƒçƒå‘¨å›´èƒ½å®¹çº³çš„æœ€å¤§éé‡å çƒä½“æ•°é‡ï¼Œæå‡ºäº†ä¸€ç§åä¸º PackingStar çš„ Game-theoretic Reinforcement Learning ç³»ç»Ÿã€‚ç”±äºé«˜ç»´ç©ºé—´çš„ç»„åˆå¤æ‚åº¦è¶…è¶Šäº†å›´æ£‹ï¼Œç ”ç©¶è€…å°†è¯¥é—®é¢˜å»ºæ¨¡ä¸ºä¸€ç§å¤§è§„æ¨¡å¯å¹¶è¡ŒåŒ–çš„åŒäºº Matrix Completion Gameï¼Œé€šè¿‡ä¸€åç©å®¶å¡«å……ä¸­å¿ƒå‘é‡ä½™å¼¦å€¼ã€å¦ä¸€åç©å®¶è¿›è¡Œçº æ­£çš„åä½œæ–¹å¼æ¥æœ€å¤§åŒ– Kissing Numberã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPackingStar åœ¨ 25 åˆ° 31 ç»´ç©ºé—´å…¨é¢è¶…è¶Šäº†äººç±»å·²çŸ¥çºªå½•ï¼Œå¹¶åœ¨ 13 ç»´ç©ºé—´å®ç°äº†è‡ª 1971 å¹´ä»¥æ¥è¶…è¶Šæœ‰ç†ç»“æ„çš„é¦–æ¬¡çªç ´ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿåœ¨ 14 ç»´åŠå…¶ä»–ç»´åº¦å‘ç°äº†è¶…è¿‡ 6000 ä¸ªæ–°ç»“æ„ï¼Œå…¶ 25 ç»´æ„å‹åœ¨å‡ ä½•ä¸Šå¯¹åº”äº Leech lattice å¹¶æš—ç¤ºäº†å¯èƒ½çš„å¸•ç´¯æ‰˜æœ€ä¼˜ã€‚è¿™ä¸€æˆæœè¯æ˜äº†äººå·¥æ™ºèƒ½åœ¨æ¢ç´¢è¶…è¶Šäººç±»ç›´è§‰çš„é«˜ç»´å‡ ä½•ç©ºé—´æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºè§£å†³å¤æ‚çš„å‡ ä½•ä¸ä¿¡æ¯è®ºé—®é¢˜æä¾›äº†å…¨æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13391v2",
      "published_date": "2025-11-17 14:02:00 UTC",
      "updated_date": "2026-01-21 16:46:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:58:31.234381+00:00"
    },
    {
      "arxiv_id": "2511.13387v3",
      "title": "Generalized Denoising Diffusion Codebook Models (gDDCM): Tokenizing images using a pre-trained diffusion model",
      "title_zh": "å¹¿ä¹‰å»å™ªæ‰©æ•£ç æœ¬æ¨¡å‹ (gDDCM)ï¼šåŸºäºé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å›¾åƒæ ‡è®°åŒ–",
      "authors": [
        "Fei Kong"
      ],
      "abstract": "Denoising diffusion models have emerged as a dominant paradigm in image generation. Discretizing image data into tokens is a critical step for effectively integrating images with Transformer and other architectures. Although the Denoising Diffusion Codebook Models (DDCM) pioneered the use of pre-trained diffusion models for image tokenization, it strictly relies on the traditional discrete-time DDPM architecture. Consequently, it fails to adapt to modern continuous-time variants-such as Flow Matching and Consistency Models-and suffers from inefficient sampling in high-noise regions. To address these limitations, this paper proposes the Generalized Denoising Diffusion Codebook Models (gDDCM). We establish a unified theoretical framework and introduce a generic \"De-noise and Back-trace\" sampling strategy. By integrating a deterministic ODE denoising step with a residual-aligned noise injection step, our method resolves the challenge of adaptation. Furthermore, we introduce a backtracking parameter $p$ and significantly enhance tokenization ability. Extensive experiments on CIFAR10 and LSUN Bedroom datasets demonstrate that gDDCM achieves comprehensive compatibility with mainstream diffusion variants and significantly outperforms DDCM in terms of reconstruction quality and perceptual fidelity.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Generalized Denoising Diffusion Codebook Models (gDDCM)ï¼Œæ—¨åœ¨è§£å†³å›¾åƒ tokenization åœ¨ç»“åˆ Transformer æ¶æ„æ—¶çš„é€‚é…ä¸æ•ˆç‡é—®é¢˜ã€‚ç”±äºä¼ ç»Ÿçš„ DDCM ä¸¥æ ¼ä¾èµ–ç¦»æ•£æ—¶é—´ DDPM æ¶æ„ï¼Œéš¾ä»¥å…¼å®¹ Flow Matching å’Œ Consistency Models ç­‰ç°ä»£è¿ç»­æ—¶é—´å˜ä½“ï¼ŒgDDCM é€šè¿‡å»ºç«‹ç»Ÿä¸€çš„ç†è®ºæ¡†æ¶å¹¶å¼•å…¥é€šç”¨çš„ \"De-noise and Back-trace\" é‡‡æ ·ç­–ç•¥æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ç¡®å®šæ€§ ODE å»å™ªä¸æ®‹å·®å¯¹é½çš„å™ªå£°æ³¨å…¥æ­¥éª¤ï¼Œå¹¶é€šè¿‡å›æº¯å‚æ•° $p$ æ˜¾è‘—æå‡äº†æ¨¡å‹çš„è¡¨å¾èƒ½åŠ›ã€‚åœ¨ CIFAR10 å’Œ LSUN Bedroom æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼ŒgDDCM å®ç°äº†å¯¹ä¸»æµæ‰©æ•£æ¨¡å‹å˜ä½“çš„å…¨é¢å…¼å®¹ã€‚æ­¤å¤–ï¼ŒgDDCM åœ¨å›¾åƒé‡æ„è´¨é‡å’Œæ„ŸçŸ¥ä¿çœŸåº¦ï¼ˆPerceptual Fidelityï¼‰æ–¹é¢å‡è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œæ€§èƒ½å¤§å¹…è¶…è¶Šäº†åŸæœ‰çš„ DDCM æ¨¡å‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "in Chinese language",
      "pdf_url": "https://arxiv.org/pdf/2511.13387v3",
      "published_date": "2025-11-17 13:58:49 UTC",
      "updated_date": "2025-12-12 13:08:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:58:26.339017+00:00"
    },
    {
      "arxiv_id": "2511.13378v1",
      "title": "Moving Pictures of Thought: Extracting Visual Knowledge in Charles S. Peirce's Manuscripts with Vision-Language Models",
      "title_zh": "æ€ç»´çš„åŠ¨æ€å›¾æ™¯ï¼šåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„ Charles S. Peirce æ‰‹ç¨¿è§†è§‰çŸ¥è¯†æå–",
      "authors": [
        "Carlo Teo Pedretti",
        "Davide Picca",
        "Dario Rodighiero"
      ],
      "abstract": "Diagrams are crucial yet underexplored tools in many disciplines, demonstrating the close connection between visual representation and scholarly reasoning. However, their iconic form poses obstacles to visual studies, intermedial analysis, and text-based digital workflows. In particular, Charles S. Peirce consistently advocated the use of diagrams as essential for reasoning and explanation. His manuscripts, often combining textual content with complex visual artifacts, provide a challenging case for studying documents involving heterogeneous materials. In this preliminary study, we investigate whether Visual Language Models (VLMs) can effectively help us identify and interpret such hybrid pages in context. First, we propose a workflow that (i) segments manuscript page layouts, (ii) reconnects each segment to IIIF-compliant annotations, and (iii) submits fragments containing diagrams to a VLM. In addition, by adopting Peirce's semiotic framework, we designed prompts to extract key knowledge about diagrams and produce concise captions. Finally, we integrated these captions into knowledge graphs, enabling structured representations of diagrammatic content within composite sources.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Models, VLMs)è¯†åˆ«å¹¶è§£é‡ŠæŸ¥å°”æ–¯Â·æ¡‘å¾·æ–¯Â·çš®å°”å£«(Charles S. Peirce)æ‰‹ç¨¿ä¸­å¤æ‚çš„å›¾æ–‡æ··åˆå†…å®¹ï¼Œæ—¨åœ¨å…‹æœå›¾è¡¨åœ¨æ•°å­—åŒ–å·¥ä½œæµä¸­éš¾ä»¥è‡ªåŠ¨å¤„ç†çš„éšœç¢ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ç§æ–°å‹å·¥ä½œæµï¼Œé¦–å…ˆå¯¹æ‰‹ç¨¿é¡µé¢å¸ƒå±€è¿›è¡Œåˆ†å‰²ï¼Œå¹¶å°†å…¶ä¸ç¬¦åˆIIIFæ ‡å‡†çš„æ ‡æ³¨å»ºç«‹è”ç³»ï¼Œéšåå°†åŒ…å«å›¾è¡¨çš„ç‰‡æ®µæäº¤ç»™VLMè¿›è¡Œåˆ†æã€‚é€šè¿‡ç»“åˆçš®å°”å£«çš„ç¬¦å·å­¦æ¡†æ¶(Peirce's semiotic framework)è®¾è®¡ç‰¹å®šçš„æç¤ºè¯ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæå–å›¾è¡¨çš„æ ¸å¿ƒçŸ¥è¯†å¹¶ç”Ÿæˆç®€æ´çš„è¯´æ˜æ–‡å­—ã€‚æœ€åï¼Œè¿™äº›æå–å‡ºçš„è§†è§‰ä¿¡æ¯è¢«é›†æˆåˆ°çŸ¥è¯†å›¾è°±(Knowledge Graphs)ä¸­ï¼Œä»è€Œå®ç°äº†å¤åˆæ¥æºä¸­å›¾è¡¨å†…å®¹çš„ç»“æ„åŒ–è¡¨ç¤ºã€‚è¿™é¡¹åˆæ­¥ç ”ç©¶è¯æ˜äº†VLMsåœ¨å¤„ç†åŒ…å«å¼‚è´¨ææ–™çš„æ–‡çŒ®æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºæ·±å…¥ç†è§£è§†è§‰è¡¨ç¤ºä¸å­¦æœ¯æ¨ç†ä¹‹é—´çš„è”ç³»æä¾›äº†æœ‰æ•ˆçš„æ•°å­—åŒ–è·¯å¾„ã€‚",
      "categories": [
        "cs.DL",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.DL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13378v1",
      "published_date": "2025-11-17 13:52:23 UTC",
      "updated_date": "2025-11-17 13:52:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:58:30.288087+00:00"
    },
    {
      "arxiv_id": "2511.13373v1",
      "title": "A Novel Hierarchical Integration Method for Efficient Model Merging in Medical LLMs",
      "title_zh": "ä¸€ç§ç”¨äºåŒ»å­¦å¤§è¯­è¨€æ¨¡å‹é«˜æ•ˆæ¨¡å‹åˆå¹¶çš„æ–°å‹å±‚çº§é›†æˆæ–¹æ³•",
      "authors": [
        "Prakrit Timilsina",
        "Anuj Nepal",
        "Rajan Kadel",
        "Robin Doss"
      ],
      "abstract": "Large Language Models (LLMs) face significant challenges in distributed healthcare, including consolidating specialized domain knowledge across institutions while maintaining privacy, reducing computational overhead, and preventing catastrophic forgetting during model updates.This paper presents a systematic evaluation of six parameter-space merging techniques applied to two architecturally compatible medical LLMs derived from the Mistral-7B base model. We introduce a novel hierarchical method that combines selective Optimal Transport (OT) alignment for attention layers with cosine similarity-weighted interpolation, designed to address permutation variance while minimizing computational overhead for edge deployment scenarios. Our study evaluates Task Arithmetic, Linear Averaging, DARE-TIES, DELLA, Breadcrumbs, and our Hierarchical approach across five medical benchmarks. Results demonstrate that architecturally compatible models benefit significantly from simple averaging methods, with Task Arithmetic achieving 45.80% accuracy on MedQA, outperforming complex pruning-based approaches. These findings offer critical insights for the deployment of distributed medical AI in resource-constrained IoT environments, where computational efficiency and model compatibility are paramount. Our work establishes that for architecturally compatible models, simple averaging provides a robust and computationally efficient baseline for knowledge consolidation, offering a pragmatic path forward for scalable medical AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—é¢†åŸŸåˆ†å¸ƒå¼å¤§è¯­è¨€æ¨¡å‹ (LLMs) é¢ä¸´çš„éšç§ä¿æŠ¤ã€è®¡ç®—å¼€é”€åŠç¾éš¾æ€§é—å¿˜ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„å±‚çº§é›†æˆæ–¹æ³•ã€‚è¿™ç§å±‚çº§æ–¹æ³•ç»“åˆäº†é’ˆå¯¹æ³¨æ„åŠ›å±‚çš„é€‰æ‹©æ€§æœ€ä¼˜ä¼ è¾“ (Optimal Transport, OT) å¯¹é½æŠ€æœ¯å’Œä½™å¼¦ç›¸ä¼¼åº¦åŠ æƒæ’å€¼ï¼Œæ—¨åœ¨è§£å†³æ¨¡å‹åˆå¹¶ä¸­çš„æ’åˆ—å˜å¼‚é—®é¢˜å¹¶é™ä½è¾¹ç¼˜éƒ¨ç½²çš„è®¡ç®—å‹åŠ›ã€‚ç ”ç©¶è€…åœ¨åŸºäº Mistral-7B çš„ä¸¤ä¸ªæ¶æ„å…¼å®¹çš„åŒ»ç–— LLMs ä¸Šï¼Œç³»ç»Ÿè¯„ä¼°äº† Task Arithmeticã€Linear Averagingã€DARE-TIESã€DELLAã€Breadcrumbs ä»¥åŠè¯¥å±‚çº§æ–¹æ³•åœ¨äº”ä¸ªåŒ»ç–—åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¶æ„å…¼å®¹çš„æ¨¡å‹èƒ½æ˜¾è‘—å—ç›Šäºç®€å•å¹³å‡æ–¹æ³•ï¼Œå…¶ä¸­ Task Arithmetic åœ¨ MedQA ä¸Šè¾¾åˆ°äº† 45.80% çš„å‡†ç¡®ç‡ï¼Œè¡¨ç°ä¼˜äºæ›´å¤æ‚çš„åŸºäºå‰ªæçš„æ–¹æ³•ã€‚è¿™ä¸€å‘ç°è¡¨æ˜ç®€å•å¹³å‡æ³•ä¸ºçŸ¥è¯†æ•´åˆæä¾›äº†ä¸€ä¸ªç¨³å¥ä¸”è®¡ç®—é«˜æ•ˆçš„åŸºå‡†ï¼Œä¸ºèµ„æºå—é™çš„åŒ»ç–— IoT ç¯å¢ƒä¸‹çš„åˆ†å¸ƒå¼ AI éƒ¨ç½²æä¾›äº†é‡è¦å‚è€ƒä¸åŠ¡å®è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13373v1",
      "published_date": "2025-11-17 13:47:27 UTC",
      "updated_date": "2025-11-17 13:47:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:59:27.334188+00:00"
    },
    {
      "arxiv_id": "2511.13371v1",
      "title": "Cognitive Maps in Language Models: A Mechanistic Analysis of Spatial Planning",
      "title_zh": "è¯­è¨€æ¨¡å‹ä¸­çš„è®¤çŸ¥åœ°å›¾ï¼šç©ºé—´è§„åˆ’çš„æœºç†åˆ†æ",
      "authors": [
        "Caroline Baumgartner",
        "Eleanor Spens",
        "Neil Burgess",
        "Petru Manescu"
      ],
      "abstract": "How do large language models solve spatial navigation tasks? We investigate this by training GPT-2 models on three spatial learning paradigms in grid environments: passive exploration (Foraging Model- predicting steps in random walks), goal-directed planning (generating optimal shortest paths) on structured Hamiltonian paths (SP-Hamiltonian), and a hybrid model fine-tuned with exploratory data (SP-Random Walk). Using behavioural, representational and mechanistic analyses, we uncover two fundamentally different learned algorithms. The Foraging model develops a robust, map-like representation of space, akin to a 'cognitive map'. Causal interventions reveal that it learns to consolidate spatial information into a self-sufficient coordinate system, evidenced by a sharp phase transition where its reliance on historical direction tokens vanishes by the middle layers of the network. The model also adopts an adaptive, hierarchical reasoning system, switching between a low-level heuristic for short contexts and map-based inference for longer ones. In contrast, the goal-directed models learn a path-dependent algorithm, remaining reliant on explicit directional inputs throughout all layers. The hybrid model, despite demonstrating improved generalisation over its parent, retains the same path-dependent strategy. These findings suggest that the nature of spatial intelligence in transformers may lie on a spectrum, ranging from generalisable world models shaped by exploratory data to heuristics optimised for goal-directed tasks. We provide a mechanistic account of this generalisation-optimisation trade-off and highlight how the choice of training regime influences the strategies that emerge.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å¯¹GPT-2æ¨¡å‹è¿›è¡Œä¸‰ç§ç©ºé—´å­¦ä¹ èŒƒå¼ï¼ˆForaging Modelã€SP-Hamiltonianã€SP-Random Walkï¼‰çš„è®­ç»ƒï¼Œæ·±å…¥æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹å¦‚ä½•è§£å†³ç©ºé—´å¯¼èˆªä»»åŠ¡ã€‚é€šè¿‡è¡Œä¸ºã€è¡¨å¾å’Œæœºæ¢°åˆ†æï¼Œç ”ç©¶äººå‘˜å‘ç°æ¨¡å‹å­¦ä¹ åˆ°äº†ä¸¤ç§æˆªç„¶ä¸åŒçš„ç®—æ³•ï¼šForaging Modelå¼€å‘äº†ä¸€ç§ç±»ä¼¼äºâ€œcognitive mapâ€çš„é²æ£’ç©ºé—´è¡¨å¾ï¼Œèƒ½å¤Ÿå°†ç©ºé—´ä¿¡æ¯æ•´åˆè¿›è‡ªç»™è‡ªè¶³çš„åæ ‡ç³»ç»Ÿï¼Œå¹¶è¡¨ç°å‡ºåˆ†å±‚æ¨ç†çš„ç‰¹å¾ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç›®æ ‡å¯¼å‘ï¼ˆgoal-directedï¼‰æ¨¡å‹å’Œæ··åˆæ¨¡å‹å€¾å‘äºå­¦ä¹ è·¯å¾„ä¾èµ–ï¼ˆpath-dependentï¼‰çš„ç®—æ³•ï¼Œåœ¨æ‰€æœ‰å±‚ä¸­å§‹ç»ˆä¾èµ–æ˜¾å¼çš„æ–¹å‘è¾“å…¥ã€‚å› æœå¹²é¢„å®éªŒè¡¨æ˜ï¼ŒForaging Modelåœ¨ç½‘ç»œä¸­é—´å±‚è¡¨ç°å‡ºæ˜æ˜¾çš„ç›¸ä½è½¬æ¢ï¼Œä»è€Œæ‘†è„±å¯¹å†å²æ–¹å‘æ ‡è®°çš„ä¾èµ–ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†Transformeråœ¨ç©ºé—´æ™ºèƒ½ä¸Šçš„æ³›åŒ–ä¸ä¼˜åŒ–æƒè¡¡ï¼ˆgeneralisation-optimisation trade-offï¼‰ï¼Œè¡¨æ˜è®­ç»ƒæœºåˆ¶çš„é€‰æ‹©å†³å®šäº†æ¨¡å‹æ˜¯æ¼”åŒ–å‡ºé€šç”¨çš„ä¸–ç•Œæ¨¡å‹ï¼ˆworld modelsï¼‰è¿˜æ˜¯é’ˆå¯¹ç‰¹å®šç›®æ ‡çš„å¯å‘å¼ç­–ç•¥ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13371v1",
      "published_date": "2025-11-17 13:46:19 UTC",
      "updated_date": "2025-11-17 13:46:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:58:31.031698+00:00"
    },
    {
      "arxiv_id": "2512.00047v1",
      "title": "Emergent Convergence in Multi-Agent LLM Annotation",
      "title_zh": "å¤šæ™ºèƒ½ä½“ LLM æ ‡æ³¨ä¸­çš„æ¶Œç°æ€§æ”¶æ•›",
      "authors": [
        "Angelina Parfenova",
        "Alexander Denzler",
        "Juergen Pfeffer"
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed in collaborative settings, yet little is known about how they coordinate when treated as black-box agents. We simulate 7500 multi-agent, multi-round discussions in an inductive coding task, generating over 125000 utterances that capture both final annotations and their interactional histories. We introduce process-level metrics: code stability, semantic self-consistency, and lexical confidence alongside sentiment and convergence measures, to track coordination dynamics. To probe deeper alignment signals, we analyze the evolving geometry of output embeddings, showing that intrinsic dimensionality declines over rounds, suggesting semantic compression. The results reveal that LLM groups converge lexically and semantically, develop asymmetric influence patterns, and exhibit negotiation-like behaviors despite the absence of explicit role prompting. This work demonstrates how black-box interaction analysis can surface emergent coordination strategies, offering a scalable complement to internal probe-based interpretability methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åä½œç¯å¢ƒä¸­çš„åè°ƒæœºåˆ¶ï¼Œé€šè¿‡æ¨¡æ‹Ÿ 7500 åœºå¤šè½®å¤šæ™ºèƒ½ä½“ï¼ˆMulti-Agentï¼‰è®¨è®ºæ¥ç ”ç©¶å…¶ä½œä¸ºé»‘ç›’æ™ºèƒ½ä½“åœ¨å½’çº³ç¼–ç ï¼ˆInductive Codingï¼‰ä»»åŠ¡ä¸­çš„äº’åŠ¨åŠ¨æ€ã€‚ä½œè€…å¼•å…¥äº†ä»£ç ç¨³å®šæ€§ï¼ˆCode Stabilityï¼‰ã€è¯­ä¹‰è‡ªæ´½æ€§ï¼ˆSemantic Self-consistencyï¼‰å’Œè¯æ±‡è‡ªä¿¡åº¦ï¼ˆLexical Confidenceï¼‰ç­‰è¿‡ç¨‹çº§æŒ‡æ ‡ï¼Œå¹¶ç»“åˆæƒ…æ„Ÿä¸æ”¶æ•›åº¦æµ‹é‡æ¥è¿½è¸ªåè°ƒè¿‡ç¨‹ã€‚é€šè¿‡åˆ†æè¾“å‡ºåµŒå…¥ï¼ˆOutput Embeddingsï¼‰çš„å‡ ä½•æ¼”å˜ï¼Œç ”ç©¶å‘ç°å†…ç¦€ç»´åº¦ï¼ˆIntrinsic Dimensionalityï¼‰éšè½®æ¬¡å¢åŠ è€Œä¸‹é™ï¼Œæ­ç¤ºäº†è¯­ä¹‰å‹ç¼©ï¼ˆSemantic Compressionï¼‰ç°è±¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLM ç¾¤ä½“åœ¨è¯æ±‡å’Œè¯­ä¹‰ä¸Šå‘ˆç°å‡ºæ¶Œç°å¼çš„æ”¶æ•›ï¼ˆEmergent Convergenceï¼‰ï¼Œä¸”åœ¨æ— æ˜¾å¼è§’è‰²æç¤ºçš„æƒ…å†µä¸‹å±•ç°å‡ºéå¯¹ç§°å½±å“æ¨¡å¼å’Œç±»è°ˆåˆ¤è¡Œä¸ºã€‚è¯¥å·¥ä½œè¯æ˜äº†é»‘ç›’äº¤äº’åˆ†æèƒ½æœ‰æ•ˆè¯†åˆ«æ¶Œç°çš„åè°ƒç­–ç•¥ï¼Œä¸ºåŸºäºå†…éƒ¨æ¢æµ‹çš„å¯è§£é‡Šæ€§æ–¹æ³•ï¼ˆInterpretability Methodsï¼‰æä¾›äº†å¼ºæœ‰åŠ›çš„è¡¥å……ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00047v1",
      "published_date": "2025-11-17 13:42:56 UTC",
      "updated_date": "2025-11-17 13:42:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:58:40.935487+00:00"
    },
    {
      "arxiv_id": "2511.13368v2",
      "title": "Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning",
      "title_zh": "ä¾›ä½“ä¸å—ä½“ï¼šå‚æ•°é«˜æ•ˆå¾®è°ƒä¸‹çš„è·¨ä»»åŠ¡ä¸è·¨è¯­è¨€éå¯¹ç§°è¿ç§»ç ”ç©¶",
      "authors": [
        "Kajetan Dymkiewicz",
        "Ivan Vulic",
        "Helen Yannakoudakis",
        "Eilam Shapira",
        "Roi Reichart",
        "Anna Korhonen"
      ],
      "abstract": "Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages remains poorly understood. We conduct a controlled LoRA fine-tuning study across multiple open-weight LLM families and scales, using a standardised grid of 11 languages and four benchmarks. We fine-tune each model on a single task-language source and measure transfer when evaluated on all other task-language target pairs. We decompose transfer into three regimes: (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language). Single-source fine-tuning yields a net positive uplift across regimes, but the gains are strongly asymmetric. Matched-Task (Cross-Language) transfer emerges as the most effective and predictable regime, driven principally by the identity of the target language rather than model architecture. We identify a stable hierarchy where high-resource languages and broad semantic tasks act as efficient recipients that absorb gains from diverse sources, while specialised tasks and lower-resource languages are more isolated. These results imply that effective fine-tuning requires navigating donor-recipient roles to maximise downstream gains.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å¯¹å¤šä¸ªå¼€æºå¤§è¯­è¨€æ¨¡å‹ (LLMs) è¿›è¡Œå—æ§çš„ LoRA å¾®è°ƒå®éªŒï¼Œæ·±å…¥æ¢è®¨äº†å‚æ•°é«˜æ•ˆå¾®è°ƒ (Parameter-Efficient Fine-Tuning) åœ¨è·¨ä»»åŠ¡å’Œè·¨è¯­è¨€è¿ç§»ä¸­çš„éå¯¹ç§°æ€§ç°è±¡ã€‚ç ”ç©¶åˆ©ç”¨ 11 ç§è¯­è¨€å’Œ 4 ä¸ªåŸºå‡†æµ‹è¯•ï¼Œå°†è¿ç§»åˆ’åˆ†ä¸ºåŒä»»åŠ¡è·¨è¯­è¨€ã€åŒè¯­è¨€è·¨ä»»åŠ¡åŠè·¨ä»»åŠ¡è·¨è¯­è¨€ä¸‰ç§æ¨¡å¼è¿›è¡Œåˆ†æã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶å•æºå¾®è°ƒæ™®éèƒ½å¸¦æ¥æ­£å‘å¢ç›Šï¼Œä½†å…¶è¡¨ç°å…·æœ‰å¼ºçƒˆçš„éå¯¹ç§°æ€§ï¼Œå…¶ä¸­åŒä»»åŠ¡è·¨è¯­è¨€ (Matched-Task Cross-Language) æ˜¯æœ€æœ‰æ•ˆä¸”å¯é¢„æµ‹çš„è¿ç§»æ¨¡å¼ã€‚ç ”ç©¶å‘ç°è¿ç§»æ•ˆæœä¸»è¦å–å†³äºç›®æ ‡è¯­è¨€çš„å±æ€§è€Œéæ¨¡å‹æ¶æ„ï¼Œå¹¶è¯†åˆ«å‡ºä¸€ç§ç¨³å®šçš„å±‚çº§ç»“æ„ï¼Œå³é«˜èµ„æºè¯­è¨€å’Œå¹¿ä¹‰è¯­ä¹‰ä»»åŠ¡ä½œä¸ºé«˜æ•ˆçš„æ¥æ”¶è€… (recipients) èƒ½å¸æ”¶æ¥è‡ªå¤šç§æ¥æºçš„å¢ç›Šï¼Œè€Œä½èµ„æºè¯­è¨€å’Œä¸“ä¸šåŒ–ä»»åŠ¡åˆ™è¡¨ç°å¾—è¾ƒä¸ºå­¤ç«‹ã€‚è¿™ä¸€ç»“è®ºå¼ºè°ƒäº†åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­è¯†åˆ«å¹¶åˆ©ç”¨â€œä¾›ä½“ä¸å—ä½“â€ (donors and recipients) è§’è‰²çš„é‡è¦æ€§ï¼Œä¸ºæœ€å¤§åŒ–æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ€§èƒ½æå‡æä¾›äº†æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13368v2",
      "published_date": "2025-11-17 13:41:31 UTC",
      "updated_date": "2026-01-08 15:10:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:58:54.640280+00:00"
    },
    {
      "arxiv_id": "2511.13365v2",
      "title": "InfoDecom: Decomposing Information for Defending Against Privacy Leakage in Split Inference",
      "title_zh": "InfoDecomï¼šé€šè¿‡ä¿¡æ¯åˆ†è§£é˜²å¾¡æ‹†åˆ†æ¨ç†ä¸­çš„éšç§æ³„éœ²",
      "authors": [
        "Ruijun Deng",
        "Zhihui Lu",
        "Qiang Duan"
      ],
      "abstract": "Split inference (SI) enables users to access deep learning (DL) services without directly transmitting raw data. However, recent studies reveal that data reconstruction attacks (DRAs) can recover the original inputs from the smashed data sent from the client to the server, leading to significant privacy leakage. While various defenses have been proposed, they often result in substantial utility degradation, particularly when the client-side model is shallow. We identify a key cause of this trade-off: existing defenses apply excessive perturbation to redundant information in the smashed data. To address this issue in computer vision tasks, we propose InfoDecom, a defense framework that first decomposes and removes redundant information and then injects noise calibrated to provide theoretically guaranteed privacy. Experiments demonstrate that InfoDecom achieves a superior utility-privacy trade-off compared to existing baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åˆ†å¸ƒå¼æ¨ç† (Split Inference, SI) ä¸­ç ¸ç¢æ•°æ® (Smashed Data) æ˜“å—æ•°æ®é‡æ„æ”»å‡» (Data Reconstruction Attacks, DRAs) å¯¼è‡´éšç§æ³„éœ²çš„é—®é¢˜ï¼Œæå‡ºäº† InfoDecom é˜²å¾¡æ¡†æ¶ã€‚ä½œè€…æŒ‡å‡ºï¼Œç°æœ‰é˜²å¾¡æŠ€æœ¯åœ¨æŠ‘åˆ¶å†—ä½™ä¿¡æ¯æ—¶å¾€å¾€æ–½åŠ è¿‡åº¦æ‰°åŠ¨ï¼Œå¯¼è‡´æ¨¡å‹æ•ˆç”¨åœ¨å®¢æˆ·ç«¯æ¨¡å‹è¾ƒæµ…æ—¶æ˜¾è‘—ä¸‹é™ã€‚InfoDecom é€šè¿‡é¦–å…ˆåˆ†è§£å¹¶ç§»é™¤æ•°æ®ä¸­çš„å†—ä½™ä¿¡æ¯ï¼Œéšåæ³¨å…¥ç»è¿‡æ ¡å‡†çš„å™ªå£°ï¼Œä»è€Œåœ¨æä¾›ç†è®ºéšç§ä¿è¯çš„åŒæ—¶ä¿ç•™æ ¸å¿ƒç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒInfoDecom åœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­ç›¸æ¯”ç°æœ‰åŸºå‡†æ¨¡å‹å®ç°äº†æ›´ä¼˜çš„æ•ˆç”¨ä¸éšç§å¹³è¡¡ (Utility-Privacy Trade-off)ã€‚è¯¥æ¡†æ¶æœ‰æ•ˆåœ°è§£å†³äº†éšç§ä¿æŠ¤ä¸æ¨¡å‹æ€§èƒ½ä¹‹é—´çš„æƒè¡¡éš¾é¢˜ï¼Œä¸ºæ·±åº¦å­¦ä¹ æœåŠ¡çš„å®‰å…¨æ€§æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.CR",
      "comment": "11pages, 6figures. Accepted by AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.13365v2",
      "published_date": "2025-11-17 13:36:40 UTC",
      "updated_date": "2026-01-03 14:23:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:59:01.934759+00:00"
    },
    {
      "arxiv_id": "2511.13361v1",
      "title": "MedDCR: Learning to Design Agentic Workflows for Medical Coding",
      "title_zh": "MedDCRï¼šå­¦ä¹ è®¾è®¡é¢å‘åŒ»å­¦ç¼–ç çš„æ™ºèƒ½ä½“å·¥ä½œæµ",
      "authors": [
        "Jiyang Zheng",
        "Islam Nassar",
        "Thanh Vu",
        "Xu Zhong",
        "Yang Lin",
        "Tongliang Liu",
        "Long Duong",
        "Yuan-Fang Li"
      ],
      "abstract": "Medical coding converts free-text clinical notes into standardized diagnostic and procedural codes, which are essential for billing, hospital operations, and medical research. Unlike ordinary text classification, it requires multi-step reasoning: extracting diagnostic concepts, applying guideline constraints, mapping to hierarchical codebooks, and ensuring cross-document consistency. Recent advances leverage agentic LLMs, but most rely on rigid, manually crafted workflows that fail to capture the nuance and variability of real-world documentation, leaving open the question of how to systematically learn effective workflows. We present MedDCR, a closed-loop framework that treats workflow design as a learning problem. A Designer proposes workflows, a Coder executes them, and a Reflector evaluates predictions and provides constructive feedback, while a memory archive preserves prior designs for reuse and iterative refinement. On benchmark datasets, MedDCR outperforms state-of-the-art baselines and produces interpretable, adaptable workflows that better reflect real coding practice, improving both the reliability and trustworthiness of automated systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»å­¦ç¼–ç (Medical coding)è¿‡ç¨‹ä¸­å¤æ‚çš„å¤šæ­¥æ¨ç†éœ€æ±‚ï¼Œæå‡ºäº†MedDCRï¼Œè¿™æ˜¯ä¸€ç§å°†æ™ºèƒ½ä½“å·¥ä½œæµè®¾è®¡(Agentic Workflows)è§†ä¸ºå­¦ä¹ é—®é¢˜çš„é—­ç¯æ¡†æ¶ã€‚MedDCRé€šè¿‡Designeræè®®å·¥ä½œæµã€Coderæ‰§è¡Œä»»åŠ¡ä»¥åŠReflectoræä¾›åé¦ˆï¼Œå¹¶åˆ©ç”¨å­˜å‚¨å½’æ¡£(Memory archive)å®ç°è®¾è®¡æ–¹æ¡ˆçš„é‡ç”¨ä¸è¿­ä»£ä¼˜åŒ–ã€‚ä¸ä¾èµ–æ‰‹å·¥ç¼–å†™å·¥ä½œæµçš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒMedDCRèƒ½å¤Ÿç³»ç»Ÿåœ°å­¦ä¹ å¹¶ç”Ÿæˆæ›´å…·é€‚åº”æ€§çš„å·¥ä½œæµç¨‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨åŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›(SOTA)æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒMedDCRç”Ÿæˆçš„å·¥ä½œæµå…·æœ‰æ›´å¼ºçš„å¯è§£é‡Šæ€§ï¼Œæå‡äº†è‡ªåŠ¨åŒ–ç³»ç»Ÿåœ¨çœŸå®åŒ»ç–—ç¼–ç å®è·µä¸­çš„å¯é æ€§å’Œå¯ä¿¡åº¦ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13361v1",
      "published_date": "2025-11-17 13:30:51 UTC",
      "updated_date": "2025-11-17 13:30:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:58:49.140773+00:00"
    },
    {
      "arxiv_id": "2511.13359v1",
      "title": "Reasoning Shapes Alignment: Investigating Cultural Alignment in Large Reasoning Models with Cultural Norms",
      "title_zh": "æ¨ç†å¡‘é€ å¯¹é½ï¼šåŸºäºæ–‡åŒ–è§„èŒƒçš„å¤§æ¨ç†æ¨¡å‹æ–‡åŒ–å¯¹é½ç ”ç©¶",
      "authors": [
        "Yuhang Wang",
        "Yanxu Zhu",
        "Jitao Sang"
      ],
      "abstract": "The advanced reasoning capabilities of Large Reasoning Models enable them to thoroughly understand and apply safety policies through deliberate thought processes, thereby improving the models' safety. Beyond safety, these models must also be able to reflect the diverse range of human values across various cultures. This paper presents the Cultural Norm-based Cultural Alignment (CNCA) framework, which enables models to leverage their powerful reasoning ability to align with cultural norms. Specifically, we propose three methods to automatically mine cultural norms from limited survey data and explore ways to effectively utilize these norms for improving cultural alignment. Two alignment paradigms are examined: an in-context alignment method, where cultural norms are explicitly integrated into the user context, and a fine-tuning-based method, which internalizes norms through enhanced Chain-of-Thought training data. Comprehensive experiments demonstrate the effectiveness of these methods, highlighting that models with stronger reasoning capabilities benefit more from cultural norm mining and utilization. Our findings emphasize the potential for reasoning models to better reflect diverse human values through culturally informed alignment strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤§æ¨ç†æ¨¡å‹ (Large Reasoning Models) ä¸­é€šè¿‡æ–‡åŒ–è§„èŒƒ (Cultural Norms) å®ç°æ–‡åŒ–å¯¹é½ (Cultural Alignment) çš„é‡è¦æ€§ï¼Œæ—¨åœ¨ä½¿æ¨¡å‹èƒ½å¤Ÿåæ˜ å…¨çƒå¤šå…ƒçš„äººç±»ä»·å€¼è§‚ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†åŸºäºæ–‡åŒ–è§„èŒƒçš„æ–‡åŒ–å¯¹é½ (Cultural Norm-based Cultural Alignment, CNCA) æ¡†æ¶ï¼Œåˆ©ç”¨æ¨¡å‹çš„å¼ºå¤§æ¨ç†èƒ½åŠ›æ¥éµå¾ªç‰¹å®šçš„æ–‡åŒ–å‡†åˆ™ã€‚ç ”ç©¶æå‡ºäº†ä¸‰ç§è‡ªåŠ¨ä»æœ‰é™è°ƒæŸ¥æ•°æ®ä¸­æŒ–æ˜æ–‡åŒ–è§„èŒƒçš„æ–¹æ³•ï¼Œå¹¶è€ƒå¯Ÿäº†å°†è§„èŒƒé›†æˆåˆ°ä¸Šä¸‹æ–‡çš„è¯­å¢ƒå¯¹é½ (in-context alignment) ä»¥åŠé€šè¿‡å¢å¼ºé“¾å¼æ€ç»´ (Chain-of-Thought) è®­ç»ƒæ•°æ®è¿›è¡Œå¾®è°ƒ (fine-tuning-based method) è¿™ä¸¤ç§èŒƒå¼ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å‘ç°æ¨ç†èƒ½åŠ›æ›´å¼ºçš„æ¨¡å‹ä»æ–‡åŒ–è§„èŒƒçš„æŒ–æ˜ä¸åˆ©ç”¨ä¸­è·ç›Šæ›´å¤šã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†åˆ©ç”¨æ–‡åŒ–æ„ŸçŸ¥å¯¹é½ç­–ç•¥ä½¿æ¨ç†æ¨¡å‹æ›´å¥½åœ°ä½“ç°äººç±»å¤šå…ƒä»·å€¼çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13359v1",
      "published_date": "2025-11-17 13:29:22 UTC",
      "updated_date": "2025-11-17 13:29:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:58:52.631807+00:00"
    },
    {
      "arxiv_id": "2511.13356v1",
      "title": "Enhancing All-to-X Backdoor Attacks with Optimized Target Class Mapping",
      "title_zh": "é€šè¿‡ä¼˜åŒ–ç›®æ ‡ç±»åˆ«æ˜ å°„å¢å¼º All-to-X åé—¨æ”»å‡»",
      "authors": [
        "Lei Wang",
        "Yulong Tian",
        "Hao Han",
        "Fengyuan Xu"
      ],
      "abstract": "Backdoor attacks pose severe threats to machine learning systems, prompting extensive research in this area. However, most existing work focuses on single-target All-to-One (A2O) attacks, overlooking the more complex All-to-X (A2X) attacks with multiple target classes, which are often assumed to have low attack success rates. In this paper, we first demonstrate that A2X attacks are robust against state-of-the-art defenses. We then propose a novel attack strategy that enhances the success rate of A2X attacks while maintaining robustness by optimizing grouping and target class assignment mechanisms. Our method improves the attack success rate by up to 28%, with average improvements of 6.7%, 16.4%, 14.1% on CIFAR10, CIFAR100, and Tiny-ImageNet, respectively. We anticipate that this study will raise awareness of A2X attacks and stimulate further research in this under-explored area. Our code is available at https://github.com/kazefjj/A2X-backdoor .",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç›®å‰å­¦æœ¯ç•Œæ™®éå…³æ³¨å•ç›®æ ‡All-to-One (A2O)æ”»å‡»è€Œå¿½è§†å¤šç›®æ ‡All-to-X (A2X)æ”»å‡»çš„ç°çŠ¶ï¼Œé¦–å…ˆè¯æ˜äº†A2Xæ”»å‡»åœ¨åº”å¯¹å…ˆè¿›é˜²å¾¡æªæ–½æ—¶å…·æœ‰æ˜¾è‘—çš„é²æ£’æ€§ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ–°å‹æ”»å‡»ç­–ç•¥ï¼Œé€šè¿‡ä¼˜åŒ–åˆ†ç»„(grouping)å’Œç›®æ ‡ç±»åˆ«åˆ†é…(target class assignment)æœºåˆ¶ï¼Œæœ‰æ•ˆæå‡äº†å¤šç›®æ ‡åé—¨æ”»å‡»çš„æˆåŠŸç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨CIFAR10ã€CIFAR100å’ŒTiny-ImageNetæ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº†6.7%ã€16.4%å’Œ14.1%çš„å¹³å‡æ”»å‡»æˆåŠŸç‡æå‡ï¼Œæœ€é«˜æå‡å¹…åº¦è¾¾åˆ°28%ã€‚è¿™é¡¹å·¥ä½œå¡«è¡¥äº†å¤šç›®æ ‡åé—¨æ”»å‡»é¢†åŸŸçš„ç ”ç©¶ç©ºç™½ï¼Œæ­ç¤ºäº†æœºå™¨å­¦ä¹ ç³»ç»Ÿé¢ä¸´çš„æ›´å¤æ‚å¨èƒï¼Œå¹¶ä¸ºæœªæ¥é˜²å¾¡æŠ€æœ¯çš„å¼€å‘æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13356v1",
      "published_date": "2025-11-17 13:22:44 UTC",
      "updated_date": "2025-11-17 13:22:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:59:43.633693+00:00"
    },
    {
      "arxiv_id": "2511.13353v1",
      "title": "Semi-Supervised Multi-Task Learning for Interpretable Quality As- sessment of Fundus Images",
      "title_zh": "ç”¨äºçœ¼åº•å›¾åƒå¯è§£é‡Šæ€§è´¨é‡è¯„ä»·çš„åŠç›‘ç£å¤šä»»åŠ¡å­¦ä¹ ",
      "authors": [
        "Lucas Gabriel Telesco",
        "Danila Nejamkin",
        "EstefanÃ­a Mata",
        "Francisco Filizzola",
        "Kevin Wignall",
        "LucÃ­a Franco Troilo",
        "MarÃ­a de los Angeles Cenoz",
        "Melissa Thompson",
        "Mercedes LeguÃ­a",
        "Ignacio Larrabide",
        "JosÃ© Ignacio Orlando"
      ],
      "abstract": "Retinal image quality assessment (RIQA) supports computer-aided diagnosis of eye diseases. However, most tools classify only overall image quality, without indicating acquisition defects to guide recapture. This gap is mainly due to the high cost of detailed annotations. In this paper, we aim to mitigate this limitation by introducing a hybrid semi-supervised learning approach that combines manual labels for overall quality with pseudo-labels of quality details within a multi-task framework. Our objective is to obtain more interpretable RIQA models without requiring extensive manual labeling. Pseudo-labels are generated by a Teacher model trained on a small dataset and then used to fine-tune a pre-trained model in a multi-task setting. Using a ResNet-18 backbone, we show that these weak annotations improve quality assessment over single-task baselines (F1: 0.875 vs. 0.863 on EyeQ, and 0.778 vs. 0.763 on DeepDRiD), matching or surpassing existing methods. The multi-task model achieved performance statistically comparable to the Teacher for most detail prediction tasks (p > 0.05). In a newly annotated EyeQ subset released with this paper, our model performed similarly to experts, suggesting that pseudo-label noise aligns with expert variability. Our main finding is that the proposed semi-supervised approach not only improves overall quality assessment but also provides interpretable feedback on capture conditions (illumination, clarity, contrast). This enhances interpretability at no extra manual labeling cost and offers clinically actionable outputs to guide image recapture.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†ç½‘è†œå›¾åƒè´¨é‡è¯„ä¼°(RIQA)ä¸­ç”±äºæ ‡æ³¨æˆæœ¬é«˜å¯¼è‡´æ¨¡å‹ç¼ºä¹å¯è§£é‡Šæ€§åŠæ— æ³•æŒ‡å¯¼é‡æ–°æ‹æ‘„çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ··åˆåŠç›‘ç£å¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“åˆæ•´ä½“è´¨é‡çš„æ‰‹åŠ¨æ ‡ç­¾ä¸è´¨é‡ç»†èŠ‚çš„ä¼ªæ ‡ç­¾(pseudo-labels)ï¼Œåœ¨å¤šä»»åŠ¡æ¡†æ¶(multi-task framework)ä¸‹è®­ç»ƒæ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼Œç ”ç©¶åˆ©ç”¨åœ¨å°è§„æ¨¡æ•°æ®é›†ä¸Šè®­ç»ƒçš„Teacheræ¨¡å‹ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œå¹¶ä»¥æ­¤å¯¹åŸºäºResNet-18éª¨å¹²ç½‘ç»œé¢„è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨EyeQå’ŒDeepDRiDæ•°æ®é›†ä¸Šçš„F1åˆ†æ•°åˆ†åˆ«è¾¾åˆ°0.875å’Œ0.778ï¼Œä¼˜äºå•ä»»åŠ¡åŸºçº¿æ¨¡å‹ï¼Œä¸”æ€§èƒ½ä¸ç°æœ‰æ–¹æ³•ç›¸å½“æˆ–æ›´ä¼˜ã€‚åœ¨æ–°å‘å¸ƒçš„EyeQå­é›†ä¸­ï¼Œè¯¥æ¨¡å‹çš„è¡¨ç°ä¸ä¸“å®¶ç›¸è¿‘ï¼Œè¯æ˜äº†ä¼ªæ ‡ç­¾å™ªå£°ä¸ä¸“å®¶å˜å¼‚æ€§çš„ä¸€è‡´æ€§ã€‚ç ”ç©¶å‘ç°è¯¥æ–¹æ³•åœ¨ä¸å¢åŠ é¢å¤–æ‰‹åŠ¨æ ‡æ³¨æˆæœ¬çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡äº†RIQAæ¨¡å‹çš„æ•´ä½“æ€§èƒ½ï¼Œå¹¶èƒ½é’ˆå¯¹å…‰ç…§(illumination)ã€æ¸…æ™°åº¦(clarity)å’Œå¯¹æ¯”åº¦(contrast)ç­‰æ‹æ‘„æ¡ä»¶æä¾›å…·æœ‰ä¸´åºŠæ“ä½œæ„ä¹‰çš„å¯è§£é‡Šåé¦ˆï¼Œä¸ºæŒ‡å¯¼å›¾åƒé‡æ–°é‡‡é›†æä¾›äº†æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13353v1",
      "published_date": "2025-11-17 13:17:42 UTC",
      "updated_date": "2025-11-17 13:17:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:59:43.483207+00:00"
    },
    {
      "arxiv_id": "2511.13351v1",
      "title": "Dual-LoRA and Quality-Enhanced Pseudo Replay for Multimodal Continual Food Learning",
      "title_zh": "é¢å‘å¤šæ¨¡æ€æŒç»­é£Ÿå“å­¦ä¹ çš„ Dual-LoRA ä¸è´¨é‡å¢å¼ºå‹ä¼ªé‡æ”¾",
      "authors": [
        "Xinlan Wu",
        "Bin Zhu",
        "Feng Han",
        "Pengkun Jiao",
        "Jingjing Chen"
      ],
      "abstract": "Food analysis has become increasingly critical for health-related tasks such as personalized nutrition and chronic disease prevention. However, existing large multimodal models (LMMs) in food analysis suffer from catastrophic forgetting when learning new tasks, requiring costly retraining from scratch. To address this, we propose a novel continual learning framework for multimodal food learning, integrating a Dual-LoRA architecture with Quality-Enhanced Pseudo Replay. We introduce two complementary low-rank adapters for each task: a specialized LoRA that learns task-specific knowledge with orthogonal constraints to previous tasks' subspaces, and a cooperative LoRA that consolidates shared knowledge across tasks via pseudo replay. To improve the reliability of replay data, our Quality-Enhanced Pseudo Replay strategy leverages self-consistency and semantic similarity to reduce hallucinations in generated samples. Experiments on the comprehensive Uni-Food dataset show superior performance in mitigating forgetting, representing the first effective continual learning approach for complex food tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é£Ÿå“åˆ†æé¢†åŸŸä¸­å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨å­¦ä¹ æ–°ä»»åŠ¡æ—¶é¢ä¸´çš„ç¾éš¾æ€§é—å¿˜ï¼ˆcatastrophic forgettingï¼‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåˆ›æ–°çš„å¤šæ¨¡æ€é£Ÿå“æŒç»­å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒé‡‡ç”¨äº†Dual-LoRAæ¶æ„ï¼Œé€šè¿‡ä¸¤ä¸ªäº’è¡¥çš„ä½ç§©é€‚é…å™¨æ¥å¤„ç†çŸ¥è¯†è·å–ã€‚å…¶ä¸­ï¼ŒSpecialized LoRAåˆ©ç”¨æ­£äº¤çº¦æŸï¼ˆorthogonal constraintsï¼‰å­¦ä¹ ä»»åŠ¡ç‰¹å®šçŸ¥è¯†ï¼Œè€ŒCooperative LoRAåˆ™é€šè¿‡ä¼ªé‡è¿°ï¼ˆpseudo replayï¼‰å·©å›ºè·¨ä»»åŠ¡çš„å…±äº«çŸ¥è¯†ã€‚ä¸ºäº†è§£å†³ç”Ÿæˆæ ·æœ¬ä¸­çš„å¹»è§‰ï¼ˆhallucinationsï¼‰é—®é¢˜ï¼Œç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†è´¨é‡å¢å¼ºå‹ä¼ªé‡è¿°ï¼ˆQuality-Enhanced Pseudo Replayï¼‰ç­–ç•¥ï¼Œåˆ©ç”¨è‡ªä¸€è‡´æ€§ï¼ˆself-consistencyï¼‰å’Œè¯­ä¹‰ç›¸ä¼¼åº¦æ¥æå‡æ•°æ®çš„å¯é æ€§ã€‚åœ¨Uni-Foodæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨ç¼“è§£é—å¿˜æ–¹é¢å…·æœ‰å“è¶Šæ€§èƒ½ï¼Œæ˜¯é¦–ä¸ªé’ˆå¯¹å¤æ‚é£Ÿå“åˆ†æä»»åŠ¡çš„æœ‰æ•ˆæŒç»­å­¦ä¹ æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13351v1",
      "published_date": "2025-11-17 13:16:48 UTC",
      "updated_date": "2025-11-17 13:16:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:59:50.530747+00:00"
    },
    {
      "arxiv_id": "2511.13341v1",
      "title": "An LLM-based Quantitative Framework for Evaluating High-Stealthy Backdoor Risks in OSS Supply Chains",
      "title_zh": "åŸºäº LLM çš„å¼€æºè½¯ä»¶ä¾›åº”é“¾é«˜éšè”½åé—¨é£é™©å®šé‡è¯„ä¼°æ¡†æ¶",
      "authors": [
        "Zihe Yan",
        "Kai Luo",
        "Haoyu Yang",
        "Yang Yu",
        "Zhuosheng Zhang",
        "Guancheng Li"
      ],
      "abstract": "In modern software development workflows, the open-source software supply chain contributes significantly to efficient and convenient engineering practices. With increasing system complexity, using open-source software as third-party dependencies has become a common practice. However, the lack of maintenance for underlying dependencies and insufficient community auditing create challenges in ensuring source code security and the legitimacy of repository maintainers, especially under high-stealthy backdoor attacks exemplified by the XZ-Util incident. To address these problems, we propose a fine-grained project evaluation framework for backdoor risk assessment in open-source software. The framework models stealthy backdoor attacks from the viewpoint of the attacker and defines targeted metrics for each attack stage. In addition, to overcome the limitations of static analysis in assessing the reliability of repository maintenance activities such as irregular committer privilege escalation and limited participation in reviews, the framework uses large language models (LLMs) to conduct semantic evaluation of code repositories without relying on manually crafted patterns. The framework is evaluated on sixty six high-priority packages in the Debian ecosystem. The experimental results indicate that the current open-source software supply chain is exposed to various security risks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼€æºè½¯ä»¶ (OSS) ä¾›åº”é“¾ä¸­æ—¥ç›Šä¸¥å³»çš„é«˜éšè”½åé—¨é£é™© (High-Stealthy Backdoor Risks)ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„é‡åŒ–è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è¯†åˆ«ç±»ä¼¼ XZ-Util äº‹ä»¶çš„å¤æ‚å¨èƒã€‚è¯¥æ¡†æ¶ä»æ”»å‡»è€…çš„è§†è§’å¯¹éšè”½åé—¨æ”»å‡»è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶ä¸ºæ¯ä¸ªæ”»å‡»é˜¶æ®µå®šä¹‰äº†ç²¾ç»†åŒ–çš„é‡åŒ–æŒ‡æ ‡ã€‚ä¸ºäº†å…‹æœä¼ ç»Ÿé™æ€åˆ†æåœ¨è¯„ä¼°ä»£ç ä»“åº“ç»´æŠ¤æ´»åŠ¨ï¼ˆå¦‚å¼‚å¸¸çš„æäº¤è€…æƒé™æå‡å’Œæœ‰é™çš„å®¡æŸ¥å‚ä¸ï¼‰å¯é æ€§æ–¹é¢çš„å±€é™æ€§ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ LLMs å®ç°äº†å¯¹ä»£ç ä»“åº“çš„è¯­ä¹‰åŒ–è¯„ä¼°ï¼Œä¸”æ— éœ€ä¾èµ–æ‰‹åŠ¨æ„å»ºçš„è§„åˆ™æ¨¡å¼ã€‚ç ”ç©¶äººå‘˜åœ¨ Debian ç”Ÿæ€ç³»ç»Ÿçš„ 66 ä¸ªé«˜ä¼˜å…ˆçº§è½¯ä»¶åŒ…ä¸Šå¯¹è¯¥æ¡†æ¶è¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰çš„å¼€æºè½¯ä»¶ä¾›åº”é“¾æ­£æš´éœ²åœ¨å¤šç§å®‰å…¨é£é™©ä¹‹ä¸‹ï¼Œè¯¥æ¡†æ¶ä¸ºæå‡ä¾›åº”é“¾å®‰å…¨æ€§æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "7 figures, 4 tables, conference",
      "pdf_url": "https://arxiv.org/pdf/2511.13341v1",
      "published_date": "2025-11-17 13:10:36 UTC",
      "updated_date": "2025-11-17 13:10:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:00:17.633274+00:00"
    },
    {
      "arxiv_id": "2511.13335v1",
      "title": "AHaSIS: Shared Task on Sentiment Analysis for Arabic Dialects",
      "title_zh": "AHaSISï¼šé˜¿æ‹‰ä¼¯è¯­æ–¹è¨€æƒ…æ„Ÿåˆ†æè¯„æµ‹ä»»åŠ¡",
      "authors": [
        "Maram Alharbi",
        "Salmane Chafik",
        "Saad Ezzini",
        "Ruslan Mitkov",
        "Tharindu Ranasinghe",
        "Hansi Hettiarachchi"
      ],
      "abstract": "The hospitality industry in the Arab world increasingly relies on customer feedback to shape services, driving the need for advanced Arabic sentiment analysis tools. To address this challenge, the Sentiment Analysis on Arabic Dialects in the Hospitality Domain shared task focuses on Sentiment Detection in Arabic Dialects. This task leverages a multi-dialect, manually curated dataset derived from hotel reviews originally written in Modern Standard Arabic (MSA) and translated into Saudi and Moroccan (Darija) dialects. The dataset consists of 538 sentiment-balanced reviews spanning positive, neutral, and negative categories. Translations were validated by native speakers to ensure dialectal accuracy and sentiment preservation. This resource supports the development of dialect-aware NLP systems for real-world applications in customer experience analysis. More than 40 teams have registered for the shared task, with 12 submitting systems during the evaluation phase. The top-performing system achieved an F1 score of 0.81, demonstrating the feasibility and ongoing challenges of sentiment analysis across Arabic dialects.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†AHaSISå…±äº«ä»»åŠ¡ï¼Œé‡ç‚¹å…³æ³¨é…’åº—é¢†åŸŸé˜¿æ‹‰ä¼¯è¯­æ–¹è¨€çš„æƒ…æ„Ÿåˆ†æï¼ˆSentiment Analysisï¼‰ã€‚è¯¥ä»»åŠ¡åˆ©ç”¨äº†ä¸€ä¸ªç»è¿‡äººå·¥ç­–åˆ’çš„å¤šæ–¹è¨€æ•°æ®é›†ï¼Œæ¶µç›–äº†ç”±ç°ä»£æ ‡å‡†é˜¿æ‹‰ä¼¯è¯­ï¼ˆMSAï¼‰ç¿»è¯‘ä¸ºæ²™ç‰¹å’Œæ‘©æ´›å“¥ï¼ˆDarijaï¼‰æ–¹è¨€çš„é…’åº—è¯„è®ºã€‚æ•°æ®é›†åŒ…å«538æ¡æƒ…æ„Ÿå¹³è¡¡çš„è¯„è®ºï¼Œåˆ†ä¸ºæ­£é¢ã€ä¸­æ€§å’Œè´Ÿé¢ä¸‰ç±»ï¼Œå¹¶ç”±æ¯è¯­ä½¿ç”¨è€…éªŒè¯ä»¥ç¡®ä¿æ–¹è¨€å‡†ç¡®æ€§å’Œæƒ…æ„Ÿä¿ç•™ã€‚è¯¥èµ„æºæ—¨åœ¨æ”¯æŒå¼€å‘å…·å¤‡æ–¹è¨€æ„ŸçŸ¥èƒ½åŠ›çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ç³»ç»Ÿï¼Œä»¥åº”ç”¨äºç°å®ä¸–ç•Œçš„å®¢æˆ·ä½“éªŒåˆ†æã€‚åœ¨è¯„æµ‹ä¸­ï¼Œå…±æœ‰12æ”¯å›¢é˜Ÿæäº¤äº†ç³»ç»Ÿï¼Œå…¶ä¸­è¡¨ç°æœ€å¥½çš„ç³»ç»Ÿè¾¾åˆ°äº†0.81çš„F1 scoreã€‚è¿™ä¸€ç»“æœè¯æ˜äº†åœ¨é˜¿æ‹‰ä¼¯è¯­ä¸åŒæ–¹è¨€é—´è¿›è¡Œæƒ…æ„Ÿæ£€æµ‹çš„å¯è¡Œæ€§ï¼ŒåŒæ—¶ä¹Ÿæ­ç¤ºäº†è¯¥é¢†åŸŸæŒç»­å­˜åœ¨çš„æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13335v1",
      "published_date": "2025-11-17 13:06:55 UTC",
      "updated_date": "2025-11-17 13:06:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:00:10.648903+00:00"
    },
    {
      "arxiv_id": "2511.13333v1",
      "title": "AutoMalDesc: Large-Scale Script Analysis for Cyber Threat Research",
      "title_zh": "AutoMalDescï¼šé¢å‘ç½‘ç»œå¨èƒç ”ç©¶çš„å¤§è§„æ¨¡è„šæœ¬åˆ†æ",
      "authors": [
        "Alexandru-Mihai Apostu",
        "Andrei Preda",
        "Alexandra Daniela Damir",
        "Diana Bolocan",
        "Radu Tudor Ionescu",
        "Ioana Croitoru",
        "Mihaela Gaman"
      ],
      "abstract": "Generating thorough natural language explanations for threat detections remains an open problem in cybersecurity research, despite significant advances in automated malware detection systems. In this work, we present AutoMalDesc, an automated static analysis summarization framework that, following initial training on a small set of expert-curated examples, operates independently at scale. This approach leverages an iterative self-paced learning pipeline to progressively enhance output quality through synthetic data generation and validation cycles, eliminating the need for extensive manual data annotation. Evaluation across 3,600 diverse samples in five scripting languages demonstrates statistically significant improvements between iterations, showing consistent gains in both summary quality and classification accuracy. Our comprehensive validation approach combines quantitative metrics based on established malware labels with qualitative assessment from both human experts and LLM-based judges, confirming both technical precision and linguistic coherence of generated summaries. To facilitate reproducibility and advance research in this domain, we publish our complete dataset of more than 100K script samples, including annotated seed (0.9K) and test (3.6K) datasets, along with our methodology and evaluation framework.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç½‘ç»œå®‰å…¨ä¸­å¨èƒæ£€æµ‹çš„è‡ªç„¶è¯­è¨€è§£é‡Šç”Ÿæˆéš¾é¢˜ï¼Œæå‡ºäº†AutoMalDescæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å¤§è§„æ¨¡çš„è‡ªåŠ¨åŒ–é™æ€åˆ†ææ‘˜è¦ç”Ÿæˆã€‚è¯¥æ¡†æ¶ä»…éœ€å°‘é‡ä¸“å®¶ç­–åˆ’çš„æ ·æœ¬è¿›è¡Œåˆå§‹è®­ç»ƒï¼Œå³å¯é€šè¿‡è¿­ä»£å¼è‡ªé€‚åº”å­¦ä¹ (self-paced learning)æµæ°´çº¿ç‹¬ç«‹è¿è¡Œï¼Œæœ‰æ•ˆè§£å†³äº†å¤§è§„æ¨¡æ‰‹åŠ¨æ ‡æ³¨æ•°æ®çš„çŸ­ç¼ºé—®é¢˜ã€‚å…¶æ ¸å¿ƒæœºåˆ¶ç»“åˆäº†åˆæˆæ•°æ®ç”Ÿæˆ(synthetic data generation)ä¸éªŒè¯å¾ªç¯ï¼Œé€šè¿‡ä¸æ–­è¿­ä»£é€æ­¥æå‡æ‘˜è¦çš„è´¨é‡ä¸æŠ€æœ¯ç²¾ç¡®åº¦ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨æ¶µç›–äº”ç§è„šæœ¬è¯­è¨€çš„3,600ä¸ªå¤šæ ·åŒ–æ ·æœ¬ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨æ‘˜è¦è´¨é‡å’Œåˆ†ç±»å‡†ç¡®ç‡ä¸Šå‡æœ‰æ˜¾è‘—æå‡ã€‚é€šè¿‡ç»“åˆåŸºäºå·²çŸ¥æ¶æ„è½¯ä»¶æ ‡ç­¾çš„å®šé‡æŒ‡æ ‡ä»¥åŠäººç±»ä¸“å®¶ä¸å¤§è¯­è¨€æ¨¡å‹(LLM)è£åˆ¤çš„å®šæ€§è¯„ä¼°ï¼ŒéªŒè¯äº†ç”Ÿæˆæ‘˜è¦çš„æŠ€æœ¯ç²¾å‡†æ€§ä¸è¯­è¨€è¿è´¯æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å…¬å¼€å‘å¸ƒäº†åŒ…å«è¶…è¿‡10ä¸‡ä¸ªè„šæœ¬æ ·æœ¬çš„å®Œæ•´æ•°æ®é›†åŠç›¸å…³çš„æ ‡æ³¨æµ‹è¯•é›†ï¼Œä¸ºè¯¥é¢†åŸŸçš„å¯é‡å¤æ€§ç ”ç©¶æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted at AAAI 2026 (oral)",
      "pdf_url": "https://arxiv.org/pdf/2511.13333v1",
      "published_date": "2025-11-17 13:05:25 UTC",
      "updated_date": "2025-11-17 13:05:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:00:05.832840+00:00"
    },
    {
      "arxiv_id": "2512.00046v1",
      "title": "Text Annotation via Inductive Coding: Comparing Human Experts to LLMs in Qualitative Data Analysis",
      "title_zh": "åŸºäºå½’çº³ç¼–ç çš„æ–‡æœ¬æ ‡æ³¨ï¼šå®šæ€§æ•°æ®åˆ†æä¸­äººç±»ä¸“å®¶ä¸å¤§è¯­è¨€æ¨¡å‹çš„å¯¹æ¯”ç ”ç©¶",
      "authors": [
        "Angelina Parfenova",
        "Andreas Marfurt",
        "Alexander Denzler",
        "Juergen Pfeffer"
      ],
      "abstract": "This paper investigates the automation of qualitative data analysis, focusing on inductive coding using large language models (LLMs). Unlike traditional approaches that rely on deductive methods with predefined labels, this research investigates the inductive process where labels emerge from the data. The study evaluates the performance of six open-source LLMs compared to human experts. As part of the evaluation, experts rated the perceived difficulty of the quotes they coded. The results reveal a peculiar dichotomy: human coders consistently perform well when labeling complex sentences but struggle with simpler ones, while LLMs exhibit the opposite trend. Additionally, the study explores systematic deviations in both human and LLM generated labels by comparing them to the golden standard from the test set. While human annotations may sometimes differ from the golden standard, they are often rated more favorably by other humans. In contrast, some LLMs demonstrate closer alignment with the true labels but receive lower evaluations from experts.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å®šæ€§æ•°æ®åˆ†æä¸­çš„è‡ªåŠ¨åŒ–é—®é¢˜ï¼Œé‡ç‚¹æ¯”è¾ƒäº†äººç±»ä¸“å®¶ä¸å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å½’çº³ç¼–ç (Inductive Coding)ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶é€šè¿‡è¯„ä¼°å…­ç§å¼€æº LLMs ä¸äººç±»ä¸“å®¶çš„ç¼–ç ç»“æœï¼Œå¹¶ç»“åˆä¸“å®¶å¯¹æ–‡æœ¬éš¾åº¦çš„è¯„åˆ†ï¼Œæ·±å…¥åˆ†æäº†ä¸¤è€…åœ¨å¤„ç†æ–°å…´æ ‡ç­¾æ—¶çš„å·®å¼‚ã€‚å®éªŒå‘ç°äº†ä¸€ä¸ªæ˜¾è‘—çš„äºŒåˆ†ç°è±¡ï¼šäººç±»ç¼–ç è€…åœ¨æ ‡æ³¨å¤æ‚å¥å­æ—¶è¡¨ç°ç¨³å®šï¼Œä½†åœ¨å¤„ç†ç®€å•å¥å­æ—¶å®¹æ˜“å‡ºé”™ï¼›è€Œ LLMs åˆ™å‘ˆç°å‡ºå®Œå…¨ç›¸åçš„è¶‹åŠ¿ï¼Œåœ¨ç®€å•ä»»åŠ¡ä¸Šæ›´å…·ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°äººç±»æ ‡æ³¨è™½ç„¶æœ‰æ—¶åç¦»â€œé‡‘æ ‡å‡†(Golden Standard)â€ï¼Œä½†å¾€å¾€æ›´å—å…¶ä»–äººç±»è¯„ä»·è€…çš„é’çã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œéƒ¨åˆ† LLMs çš„æ ‡æ³¨è™½ä¸çœŸå®æ ‡ç­¾(True Labels)å¥‘åˆåº¦æ›´é«˜ï¼Œä½†åœ¨ä¸“å®¶è¯„ä»·ä¸­å¾—åˆ†è¾ƒä½ã€‚è¿™é¡¹å·¥ä½œæ­ç¤ºäº†äººç±»ä¸äººå·¥æ™ºèƒ½åœ¨å®šæ€§æ¨æ–­é€»è¾‘ä¸Šçš„æœ¬è´¨å·®å¼‚ï¼Œä¸ºæœªæ¥åˆ©ç”¨ LLMs è¾…åŠ©å½’çº³ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00046v1",
      "published_date": "2025-11-17 13:03:27 UTC",
      "updated_date": "2025-11-17 13:03:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:00:02.533205+00:00"
    },
    {
      "arxiv_id": "2511.13326v2",
      "title": "TacEleven: generative tactic discovery for football open play",
      "title_zh": "TacElevenï¼šè¶³çƒè¿åŠ¨æˆ˜ç”Ÿæˆå¼æˆ˜æœ¯å‘ç°",
      "authors": [
        "Siyao Zhao",
        "Hao Ma",
        "Zhiqiang Pu",
        "Jingjing Huang",
        "Yi Pan",
        "Shijie Wang",
        "Zhi Ming"
      ],
      "abstract": "Creating offensive advantages during open play is fundamental to football success. However, due to the highly dynamic and long-sequence nature of open play, the potential tactic space grows exponentially as the sequence progresses, making automated tactic discovery extremely challenging. To address this, we propose TacEleven, a generative framework for football open-play tactic discovery developed in close collaboration with domain experts from AJ Auxerre, designed to assist coaches and analysts in tactical decision-making. TacEleven consists of two core components: a language-controlled tactical generator that produces diverse tactical proposals, and a multimodal large language model-based tactical critic that selects the optimal proposal aligned with a high-level stylistic tactical instruction. The two components enables rapid exploration of tactical proposals and discovery of alternative open-play offensive tactics. We evaluate TacEleven across three tasks with progressive tactical complexity: counterfactual exploration, single-step discovery, and multi-step discovery, through both quantitative metrics and a questionnaire-based qualitative assessment. The results show that the TacEleven-discovered tactics exhibit strong realism and tactical creativity, with 52.50% of the multi-step tactical alternatives rated adoptable in real-world elite football scenarios, highlighting the framework's ability to rapidly generate numerous high-quality tactics for complex long-sequence open-play situations. TacEleven demonstrates the potential of creatively leveraging domain data and generative models to advance tactical analysis in sports.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TacElevenï¼Œè¿™æ˜¯ä¸€ä¸ªä¸AJ Auxerreé¢†åŸŸä¸“å®¶åˆä½œå¼€å‘çš„ç”¨äºè¶³çƒé˜µåœ°æˆ˜(open play)æˆ˜æœ¯å‘ç°çš„ç”Ÿæˆå¼æ¡†æ¶ï¼Œæ—¨åœ¨è¾…åŠ©æ•™ç»ƒå’Œåˆ†æå¸ˆè¿›è¡Œæˆ˜æœ¯å†³ç­–ã€‚é’ˆå¯¹é˜µåœ°æˆ˜é«˜åº¦åŠ¨æ€å’Œé•¿åºåˆ—å¯¼è‡´çš„æˆ˜æœ¯ç©ºé—´æŒ‡æ•°çº§å¢é•¿éš¾é¢˜ï¼Œè¯¥æ¡†æ¶ç”±è¯­è¨€æ§åˆ¶çš„æˆ˜æœ¯ç”Ÿæˆå™¨(tactical generator)å’ŒåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(multimodal LLM)çš„æˆ˜æœ¯è¯„è®®å™¨(tactical critic)ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶æ„æˆã€‚ç”Ÿæˆå™¨è´Ÿè´£äº§ç”Ÿå¤šæ ·åŒ–çš„æˆ˜æœ¯æ–¹æ¡ˆï¼Œè€Œè¯„è®®å™¨åˆ™æ ¹æ®é«˜å±‚çº§çš„é£æ ¼åŒ–æŒ‡ä»¤ç­›é€‰å‡ºä¸ç›®æ ‡ä¸€è‡´çš„æœ€ä¼˜ç­–ç•¥ï¼Œä»è€Œå®ç°å¯¹è¿›æ”»æˆ˜æœ¯çš„å¿«é€Ÿæ¢ç´¢ã€‚é€šè¿‡åœ¨åäº‹å®æ¢ç´¢(counterfactual exploration)ä»¥åŠå•æ­¥å’Œå¤šæ­¥å‘ç°ä»»åŠ¡ä¸Šçš„å®šé‡ä¸å®šæ€§è¯„ä¼°ï¼Œå®éªŒç»“æœæ˜¾ç¤ºTacElevenç”Ÿæˆçš„æˆ˜æœ¯å…·æœ‰æå¼ºçš„çœŸå®æ„Ÿä¸åˆ›é€ åŠ›ã€‚åœ¨ç²¾è‹±è¶³çƒåœºæ™¯ä¸‹ï¼Œæœ‰52.50%çš„å¤šæ­¥æˆ˜æœ¯æ›¿ä»£æ–¹æ¡ˆè¢«è¯„ä¸ºå¯é‡‡çº³ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨å¤æ‚é•¿åºåˆ—æƒ…å¢ƒä¸‹å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡æˆ˜æœ¯çš„èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œå……åˆ†å±•ç¤ºäº†åˆ©ç”¨é¢†åŸŸæ•°æ®å’Œç”Ÿæˆæ¨¡å‹æ¨åŠ¨ä½“è‚²æˆ˜æœ¯åˆ†æè¿›æ­¥çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "stat.AP",
        "cs.AI"
      ],
      "primary_category": "stat.AP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13326v2",
      "published_date": "2025-11-17 13:01:53 UTC",
      "updated_date": "2025-11-18 07:00:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:00:24.327503+00:00"
    },
    {
      "arxiv_id": "2511.13322v1",
      "title": "Explainable RL Policies by Distilling to Locally-Specialized Linear Policies with Voronoi State Partitioning",
      "title_zh": "åŸºäºæ²ƒç½—è¯ºä¼ŠçŠ¶æ€åˆ’åˆ†è’¸é¦å±€éƒ¨ç‰¹åŒ–çº¿æ€§ç­–ç•¥çš„å¯è§£é‡Šå¼ºåŒ–å­¦ä¹ ç­–ç•¥",
      "authors": [
        "Senne Deproost",
        "Dennis Steckelmacher",
        "Ann NowÃ©"
      ],
      "abstract": "Deep Reinforcement Learning is one of the state-of-the-art methods for producing near-optimal system controllers. However, deep RL algorithms train a deep neural network, that lacks transparency, which poses challenges when the controller has to meet regulations, or foster trust. To alleviate this, one could transfer the learned behaviour into a model that is human-readable by design using knowledge distilla- tion. Often this is done with a single model which mimics the original model on average but could struggle in more dynamic situations. A key challenge is that this simpler model should have the right balance be- tween flexibility and complexity or right balance between balance bias and accuracy. We propose a new model-agnostic method to divide the state space into regions where a simplified, human-understandable model can operate in. In this paper, we use Voronoi partitioning to find regions where linear models can achieve similar performance to the original con- troller. We evaluate our approach on a gridworld environment and a classic control task. We observe that our proposed distillation to locally- specialized linear models produces policies that are explainable and show that the distillation matches or even slightly outperforms the black-box policy they are distilled from.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Deep Reinforcement Learning æ¨¡å‹ç¼ºä¹é€æ˜åº¦åŠå…¶åœ¨æ»¡è¶³ç›‘ç®¡ä¸å»ºç«‹ä¿¡ä»»æ–¹é¢é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§å°†å¤æ‚ç­–ç•¥è’¸é¦ä¸ºè®¾è®¡ä¸Šæ›´å…·å¯è¯»æ€§çš„æ¨¡å‹çš„æ–°æ–¹æ³•ã€‚ä¸ºäº†è§£å†³å•ä¸€ç®€åŒ–æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸‹åç½®ä¸å‡†ç¡®ç‡å¤±è¡¡çš„é—®é¢˜ï¼Œä½œè€…å¼•å…¥äº†ä¸€ç§åŸºäº Voronoi partitioning çš„çŠ¶æ€ç©ºé—´åˆ’åˆ†ç­–ç•¥ï¼Œå°†ç©ºé—´ç»†åˆ†ä¸ºå¤šä¸ªåŒºåŸŸå¹¶éƒ¨ç½² Locally-Specialized Linear Policiesã€‚è¯¥æ¡†æ¶åˆ©ç”¨ Knowledge Distillation æŠ€æœ¯ï¼Œåœ¨æ¯ä¸ªåˆ’åˆ†åŒºåŸŸå†…è®­ç»ƒçº¿æ€§æ¨¡å‹ä»¥ç²¾ç¡®æ¨¡æ‹ŸåŸå§‹é»‘ç›’æ§åˆ¶å™¨çš„è¡Œä¸ºã€‚åœ¨ Gridworld å’Œç»å…¸æ§åˆ¶ä»»åŠ¡ä¸­çš„å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„ç­–ç•¥ä¸ä»…å…·å¤‡é«˜åº¦çš„å¯è§£é‡Šæ€§ï¼Œå…¶æ€§èƒ½è¡¨ç°äº¦èƒ½åŒ¹é…ç”šè‡³ç•¥å¾®è¶…è¶ŠåŸå§‹çš„ç¥ç»ç½‘ç»œç­–ç•¥ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†é€šè¿‡å±€éƒ¨çº¿æ€§è¿‘ä¼¼å¯ä»¥åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡å¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„é€æ˜åº¦ï¼Œä¸ºæ„å»ºå¯ä¿¡ä»»çš„è‡ªä¸»ç³»ç»Ÿæä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted for BNAIC/BeNeLearn 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.13322v1",
      "published_date": "2025-11-17 12:58:38 UTC",
      "updated_date": "2025-11-17 12:58:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:00:24.129153+00:00"
    },
    {
      "arxiv_id": "2511.13319v1",
      "title": "Whistledown: Combining User-Level Privacy with Conversational Coherence in LLMs",
      "title_zh": "Whistledownï¼šåœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­å…¼é¡¾ç”¨æˆ·çº§éšç§ä¸å¯¹è¯è¿è´¯æ€§",
      "authors": [
        "Chelsea McMurray",
        "Hayder Tirmazi"
      ],
      "abstract": "Users increasingly rely on large language models (LLMs) for personal, emotionally charged, and socially sensitive conversations. However, prompts sent to cloud-hosted models can contain personally identifiable information (PII) that users do not want logged, retained, or leaked. We observe this to be especially acute when users discuss friends, coworkers, or adversaries, i.e., when they spill the tea. Enterprises face the same challenge when they want to use LLMs for internal communication and decision-making.\n  In this whitepaper, we present Whistledown, a best-effort privacy layer that modifies prompts before they are sent to the LLM. Whistledown combines pseudonymization and $Îµ$-local differential privacy ($Îµ$-LDP) with transformation caching to provide best-effort privacy protection without sacrificing conversational utility. Whistledown is designed to have low compute and memory overhead, allowing it to be deployed directly on a client's device in the case of individual users. For enterprise users, Whistledown is deployed centrally within a zero-trust gateway that runs on an enterprise's trusted infrastructure. Whistledown requires no changes to the existing APIs of popular LLM providers.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Whistledownï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å¹³è¡¡å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ç”¨æˆ·éšç§ä¸å¯¹è¯è¿è´¯æ€§çš„éšç§ä¿æŠ¤å±‚ã€‚Whistledowné€šè¿‡åœ¨æç¤ºè¯å‘é€è‡³äº‘ç«¯æ¨¡å‹å‰è¿›è¡Œä¿®æ”¹ï¼Œç»“åˆäº†pseudonymizationã€$\\epsilon$-local differential privacy ($\\epsilon$-LDP)å’Œtransformation cachingæŠ€æœ¯ï¼Œåœ¨ä¸ç‰ºç‰²å¯¹è¯æ•ˆç”¨çš„å‰æä¸‹æä¾›å°½åŠ›è€Œä¸ºçš„éšç§ä¿æŠ¤ã€‚è¯¥ç³»ç»Ÿè®¾è®¡å…·æœ‰æä½çš„è®¡ç®—å’Œå†…å­˜å¼€é”€ï¼Œæ”¯æŒåœ¨ä¸ªäººç”¨æˆ·è®¾å¤‡ä¸Šç›´æ¥éƒ¨ç½²ï¼Œæˆ–åœ¨ä¼ä¸šé›¶ä¿¡ä»»ç½‘å…³(zero-trust gateway)å†…è¿è¡Œã€‚Whistledownæ— éœ€æ›´æ”¹ç°æœ‰ä¸»æµLLMä¾›åº”å•†çš„APIï¼Œç¡®ä¿äº†æ–¹æ¡ˆçš„æ˜“ç”¨æ€§ä¸å…¼å®¹æ€§ã€‚è¯¥æ¡†æ¶ä¸ºè§£å†³ç¤¾äº¤æ•æ„Ÿå¯¹è¯æˆ–ä¼ä¸šå†…éƒ¨æ²Ÿé€šä¸­çš„èº«ä»½ä¿¡æ¯(PII)æ³„éœ²é£é™©æä¾›äº†ä¸€ç§å…¼é¡¾éšç§ä¸å®ç”¨æ€§çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13319v1",
      "published_date": "2025-11-17 12:56:33 UTC",
      "updated_date": "2025-11-17 12:56:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:00:20.034362+00:00"
    },
    {
      "arxiv_id": "2511.13315v1",
      "title": "Computer Vision based group activity detection and action spotting",
      "title_zh": "åŸºäºè®¡ç®—æœºè§†è§‰çš„ç¾¤ä½“æ´»åŠ¨æ£€æµ‹ä¸åŠ¨ä½œå®šä½",
      "authors": [
        "Narthana Sivalingam",
        "Santhirarajah Sivasthigan",
        "Thamayanthi Mahendranathan",
        "G. M. R. I. Godaliyadda",
        "M. P. B. Ekanayake",
        "H. M. V. R. Herath"
      ],
      "abstract": "Group activity detection in multi-person scenes is challenging due to complex human interactions, occlusions, and variations in appearance over time. This work presents a computer vision based framework for group activity recognition and action spotting using a combination of deep learning models and graph based relational reasoning. The system first applies Mask R-CNN to obtain accurate actor localization through bounding boxes and instance masks. Multiple backbone networks, including Inception V3, MobileNet, and VGG16, are used to extract feature maps, and RoIAlign is applied to preserve spatial alignment when generating actor specific features. The mask information is then fused with the feature maps to obtain refined masked feature representations for each actor. To model interactions between individuals, we construct Actor Relation Graphs that encode appearance similarity and positional relations using methods such as normalized cross correlation, sum of absolute differences, and dot product. Graph Convolutional Networks operate on these graphs to reason about relationships and predict both individual actions and group level activities. Experiments on the Collective Activity dataset demonstrate that the combination of mask based feature refinement, robust similarity search, and graph neural network reasoning leads to improved recognition performance across both crowded and non crowded scenarios. This approach highlights the potential of integrating segmentation, feature extraction, and relational graph reasoning for complex video understanding tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè®¡ç®—æœºè§†è§‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šäººç¾¤åœºæ™¯ä¸­ç”±äºå¤æ‚äº’åŠ¨å’Œé®æŒ¡å¯¼è‡´çš„ç¾¤ä½“æ´»åŠ¨æ£€æµ‹(group activity detection)ä¸åŠ¨ä½œå®šä½(action spotting)éš¾é¢˜ã€‚ç³»ç»Ÿåˆ©ç”¨ Mask R-CNN å®ç°ç²¾ç¡®çš„æ¼”å‘˜å®šä½ï¼Œå¹¶ç»“åˆ Inception V3ã€MobileNet å’Œ VGG16 ç­‰å¤šç§éª¨å¹²ç½‘ç»œé€šè¿‡ RoIAlign æå–ç‰¹å¾ï¼Œéšåå°†æ©ç ä¿¡æ¯ä¸ç‰¹å¾å›¾èåˆä»¥è·å¾—ç²¾ç»†çš„ç‰¹å¾è¡¨ç¤ºã€‚ä¸ºäº†å»ºæ¨¡ä¸ªä½“é—´çš„äº¤äº’ï¼Œç ”ç©¶æ„å»ºäº†ç¼–ç å¤–è§‚ç›¸ä¼¼æ€§å’Œä½ç½®å…³ç³»çš„ Actor Relation Graphsï¼Œå¹¶åˆ©ç”¨ Graph Convolutional Networks (GCN) è¿›è¡Œå…³ç³»æ¨ç†ä»¥é¢„æµ‹ä¸ªä½“å’Œç¾¤ä½“æ´»åŠ¨ã€‚åœ¨ Collective Activity æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§ç»“åˆæ©ç ç‰¹å¾ç»†åŒ–ä¸å›¾ç¥ç»ç½‘ç»œæ¨ç†çš„æ–¹æ³•ï¼Œåœ¨æ‹¥æŒ¤åŠéæ‹¥æŒ¤åœºæ™¯ä¸‹å‡æ˜¾è‘—æå‡äº†è¯†åˆ«æ€§èƒ½ã€‚è¯¥æ–¹æ¡ˆçªæ˜¾äº†å°†å®ä¾‹åˆ†å‰²(segmentation)ã€ç‰¹å¾æå–ä¸å…³ç³»å›¾æ¨ç†é›†æˆåº”ç”¨äºå¤æ‚è§†é¢‘ç†è§£ä»»åŠ¡çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13315v1",
      "published_date": "2025-11-17 12:52:22 UTC",
      "updated_date": "2025-11-17 12:52:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:00:37.938362+00:00"
    },
    {
      "arxiv_id": "2511.13312v1",
      "title": "EL3DD: Extended Latent 3D Diffusion for Language Conditioned Multitask Manipulation",
      "title_zh": "EL3DDï¼šé¢å‘è¯­è¨€å¼•å¯¼å¤šä»»åŠ¡æ“çºµçš„æ‰©å±•æ½œç©ºé—´ 3D æ‰©æ•£æ¨¡å‹",
      "authors": [
        "Jonas Bode",
        "Raphael Memmesheimer",
        "Sven Behnke"
      ],
      "abstract": "Acting in human environments is a crucial capability for general-purpose robots, necessitating a robust understanding of natural language and its application to physical tasks. This paper seeks to harness the capabilities of diffusion models within a visuomotor policy framework that merges visual and textual inputs to generate precise robotic trajectories. By employing reference demonstrations during training, the model learns to execute manipulation tasks specified through textual commands within the robot's immediate environment. The proposed research aims to extend an existing model by leveraging improved embeddings, and adapting techniques from diffusion models for image generation. We evaluate our methods on the CALVIN dataset, proving enhanced performance on various manipulation tasks and an increased long-horizon success rate when multiple tasks are executed in sequence. Our approach reinforces the usefulness of diffusion models and contributes towards general multitask manipulation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EL3DD (Extended Latent 3D Diffusion)ï¼Œä¸€ç§æ—¨åœ¨å®ç°è¯­è¨€å¼•å¯¼å¤šä»»åŠ¡æ“ä½œçš„æ‰©å±•æ½œç©ºé—´ä¸‰ç»´æ‰©æ•£æ¨¡å‹æ¡†æ¶ã€‚è¯¥æ¨¡å‹å°†è§†è§‰å’Œæ–‡æœ¬è¾“å…¥é›†æˆåˆ°visuomotor policyä¸­ï¼Œåˆ©ç”¨diffusion modelsç”Ÿæˆç²¾ç¡®çš„æœºå™¨äººè½¨è¿¹ï¼Œä»è€Œå®ç°åœ¨ç‰©ç†ç¯å¢ƒä¸­æ‰§è¡Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€‚é€šè¿‡å¼•å…¥æ”¹è¿›çš„embeddingså¹¶å€Ÿé‰´å›¾åƒç”Ÿæˆé¢†åŸŸçš„æ‰©æ•£æŠ€æœ¯ï¼Œè¯¥æ–¹æ³•åœ¨è®­ç»ƒä¸­åˆ©ç”¨å‚è€ƒæ¼”ç¤ºå­¦ä¹ å¤æ‚çš„æ“æ§ä»»åŠ¡ã€‚åœ¨CALVIN datasetä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒEL3DDåœ¨å¤šé¡¹æ“ä½œä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä»»åŠ¡é¡ºåºæ‰§è¡Œçš„å¤æ‚åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¤§å¹…æé«˜äº†long-horizon success rateã€‚è¿™é¡¹ç ”ç©¶éªŒè¯äº†æ‰©æ•£æ¨¡å‹åœ¨é€šç”¨å¤šä»»åŠ¡æ“ä½œä¸­çš„æ½œåŠ›ï¼Œä¸ºæ„å»ºèƒ½å¤Ÿç†è§£è‡ªç„¶è¯­è¨€å¹¶æ‰§è¡Œç‰©ç†ä»»åŠ¡çš„é€šç”¨æœºå™¨äººå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "10 pages; 2 figures; 1 table. Prprint submitted to the European Robotics Forum 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.13312v1",
      "published_date": "2025-11-17 12:47:18 UTC",
      "updated_date": "2025-11-17 12:47:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:00:53.638644+00:00"
    },
    {
      "arxiv_id": "2511.13306v1",
      "title": "DAP: A Discrete-token Autoregressive Planner for Autonomous Driving",
      "title_zh": "DAPï¼šé¢å‘è‡ªåŠ¨é©¾é©¶çš„ç¦»æ•£æ ‡è®°è‡ªå›å½’è§„åˆ’å™¨",
      "authors": [
        "Bowen Ye",
        "Bin Zhang",
        "Hang Zhao"
      ],
      "abstract": "Gaining sustainable performance improvement with scaling data and model budget remains a pivotal yet unresolved challenge in autonomous driving. While autoregressive models exhibited promising data-scaling efficiency in planning tasks, predicting ego trajectories alone suffers sparse supervision and weakly constrains how scene evolution should shape ego motion. Therefore, we introduce DAP, a discrete-token autoregressive planner that jointly forecasts BEV semantics and ego trajectories, thereby enforcing comprehensive representation learning and allowing predicted dynamics to directly condition ego motion. In addition, we incorporate a reinforcement-learning-based fine-tuning, which preserves supervised behavior cloning priors while injecting reward-guided improvements. Despite a compact 160M parameter budget, DAP achieves state-of-the-art performance on open-loop metrics and delivers competitive closed-loop results on the NAVSIM benchmark. Overall, the fully discrete-token autoregressive formulation operating on both rasterized BEV and ego actions provides a compact yet scalable planning paradigm for autonomous driving.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶ä¸­çº¯è½¨è¿¹é¢„æµ‹é¢ä¸´çš„ç›‘ç£ç¨€ç–ä»¥åŠå¯¹åœºæ™¯æ¼”å˜çº¦æŸä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†DAPï¼Œä¸€ç§ç¦»æ•£Tokenè‡ªå›å½’è§„åˆ’å™¨(Discrete-token Autoregressive Planner)ã€‚DAPé€šè¿‡è”åˆé¢„æµ‹é¸Ÿç°å›¾è¯­ä¹‰(BEV semantics)å’Œè‡ªèº«è½¨è¿¹(ego trajectories)ï¼Œå®ç°äº†æ›´å…¨é¢çš„è¡¨å¾å­¦ä¹ ï¼Œå¹¶å…è®¸é¢„æµ‹çš„åŠ¨æ€ç¯å¢ƒç›´æ¥è°ƒèŠ‚è½¦è¾†è¿åŠ¨ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†åŸºäºå¼ºåŒ–å­¦ä¹ (reinforcement-learning)çš„å¾®è°ƒæŠ€æœ¯ï¼Œåœ¨ä¿ç•™ç›‘ç£è¡Œä¸ºå…‹éš†(behavior cloning)å…ˆéªŒçš„åŒæ—¶ï¼Œæ³¨å…¥äº†å¥–åŠ±å¼•å¯¼çš„æ€§èƒ½æ”¹è¿›ã€‚å°½ç®¡å‚æ•°è§„æ¨¡ä»…ä¸º160Mï¼ŒDAPä»åœ¨å¤§è§„æ¨¡å¼€ç¯æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†SOTAæ°´å¹³ï¼Œå¹¶åœ¨NAVSIMåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æå…·ç«äº‰åŠ›çš„é—­ç¯è¡¨ç°ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™ç§åŸºäºå…‰æ …åŒ–BEVå’ŒåŠ¨ä½œåºåˆ—çš„å…¨ç¦»æ•£Tokenè‡ªå›å½’æ–¹æ¡ˆä¸ºè‡ªåŠ¨é©¾é©¶æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å…·å¤‡å¯æ‰©å±•æ€§(scalable)çš„è§„åˆ’èŒƒå¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13306v1",
      "published_date": "2025-11-17 12:31:33 UTC",
      "updated_date": "2025-11-17 12:31:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:00:51.933799+00:00"
    },
    {
      "arxiv_id": "2511.13293v1",
      "title": "Grounded by Experience: Generative Healthcare Prediction Augmented with Hierarchical Agentic Retrieval",
      "title_zh": "ç«‹è¶³ç»éªŒï¼šåŸºäºå±‚çº§åŒ–æ™ºèƒ½ä½“æ£€ç´¢å¢å¼ºçš„ç”Ÿæˆå¼åŒ»ç–—é¢„æµ‹",
      "authors": [
        "Chuang Zhao",
        "Hui Tang",
        "Hongke Zhao",
        "Xiaofang Zhou",
        "Xiaomeng Li"
      ],
      "abstract": "Accurate healthcare prediction is critical for improving patient outcomes and reducing operational costs. Bolstered by growing reasoning capabilities, large language models (LLMs) offer a promising path to enhance healthcare predictions by drawing on their rich parametric knowledge. However, LLMs are prone to factual inaccuracies due to limitations in the reliability and coverage of their embedded knowledge. While retrieval-augmented generation (RAG) frameworks, such as GraphRAG and its variants, have been proposed to mitigate these issues by incorporating external knowledge, they face two key challenges in the healthcare scenario: (1) identifying the clinical necessity to activate the retrieval mechanism, and (2) achieving synergy between the retriever and the generator to craft contextually appropriate retrievals. To address these challenges, we propose GHAR, a \\underline{g}enerative \\underline{h}ierarchical \\underline{a}gentic \\underline{R}AG framework that simultaneously resolves when to retrieve and how to optimize the collaboration between submodules in healthcare. Specifically, for the first challenge, we design a dual-agent architecture comprising Agent-Top and Agent-Low. Agent-Top acts as the primary physician, iteratively deciding whether to rely on parametric knowledge or to initiate retrieval, while Agent-Low acts as the consulting service, summarising all task-relevant knowledge once retrieval was triggered. To tackle the second challenge, we innovatively unify the optimization of both agents within a formal Markov Decision Process, designing diverse rewards to align their shared goal of accurate prediction while preserving their distinct roles. Extensive experiments on three benchmark datasets across three popular tasks demonstrate our superiority over state-of-the-art baselines, highlighting the potential of hierarchical agentic RAG in advancing healthcare systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GHARï¼Œä¸€ç§ç»“åˆåˆ†å±‚æ™ºèƒ½ä½“æ£€ç´¢å¢å¼ºç”Ÿæˆ(Hierarchical Agentic RAG)çš„ç”Ÿæˆå¼åŒ»ç–—é¢„æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨åŒ»ç–—é¢„æµ‹ä¸­é¢ä¸´çš„äº‹å®å‡†ç¡®æ€§ä¸è¶³ï¼Œä»¥åŠç°æœ‰RAGç³»ç»Ÿéš¾ä»¥å¹³è¡¡æ£€ç´¢è§¦å‘æ—¶æœºä¸å­æ¨¡å—åä½œçš„é—®é¢˜ã€‚GHARé‡‡ç”¨äº†ç”±Agent-Topå’ŒAgent-Lowç»„æˆçš„åŒæ™ºèƒ½ä½“æ¶æ„ï¼Œå…¶ä¸­Agent-Topæ¨¡æ‹Ÿä¸»æ²»åŒ»å¸ˆè§’è‰²ï¼Œè¿­ä»£å†³å®šæ˜¯ä¾èµ–å‚æ•°åŒ–çŸ¥è¯†è¿˜æ˜¯å¯åŠ¨æ£€ç´¢ï¼Œè€ŒAgent-Lowåˆ™ä½œä¸ºå’¨è¯¢æœåŠ¡è´Ÿè´£æ±‡æ€»æ£€ç´¢åˆ°çš„ä»»åŠ¡ç›¸å…³çŸ¥è¯†ã€‚ç ”ç©¶å›¢é˜Ÿåˆ›æ–°åœ°å°†ä¸¤ä¸ªæ™ºèƒ½ä½“çš„ä¼˜åŒ–ç»Ÿä¸€åœ¨å½¢å¼åŒ–çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(Markov Decision Process, MDP)ä¸­ï¼Œé€šè¿‡è®¾è®¡å¤šæ ·åŒ–å¥–åŠ±æœºåˆ¶æ¥å¯¹é½ä¸¤è€…çš„é¢„æµ‹å‡†ç¡®æ€§ç›®æ ‡å¹¶ä¿ç•™å…¶å„è‡ªçš„èŒèƒ½å±æ€§ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†å’Œä¸‰é¡¹å…¸å‹åŒ»ç–—ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼ŒGHARçš„é¢„æµ‹æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿›åŸºçº¿æ¨¡å‹ã€‚è¯¥æˆæœå‡¸æ˜¾äº†åˆ†å±‚æ™ºèƒ½ä½“æ¶æ„åœ¨å¢å¼ºåŒ»ç–—å†³ç­–æ”¯æŒç³»ç»Ÿä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œä¸ºæ„å»ºæ›´å¯é ã€æ›´å…·å¯è§£é‡Šæ€§çš„ç”Ÿæˆå¼åŒ»ç–—é¢„æµ‹ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13293v1",
      "published_date": "2025-11-17 12:15:46 UTC",
      "updated_date": "2025-11-17 12:15:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:00:48.330227+00:00"
    },
    {
      "arxiv_id": "2511.13290v1",
      "title": "Dropouts in Confidence: Moral Uncertainty in Human-LLM Alignment",
      "title_zh": "ç½®ä¿¡åº¦ä¸¢å¼ƒï¼šäººç±»ä¸ LLM å¯¹é½ä¸­çš„é“å¾·ä¸ç¡®å®šæ€§",
      "authors": [
        "Jea Kwon",
        "Luiz Felipe Vecchietti",
        "Sungwon Park",
        "Meeyoung Cha"
      ],
      "abstract": "Humans display significant uncertainty when confronted with moral dilemmas, yet the extent of such uncertainty in machines and AI agents remains underexplored. Recent studies have confirmed the overly confident tendencies of machine-generated responses, particularly in large language models (LLMs). As these systems are increasingly embedded in ethical decision-making scenarios, it is important to understand their moral reasoning and the inherent uncertainties in building reliable AI systems. This work examines how uncertainty influences moral decisions in the classical trolley problem, analyzing responses from 32 open-source models and 9 distinct moral dimensions. We first find that variance in model confidence is greater across models than within moral dimensions, suggesting that moral uncertainty is predominantly shaped by model architecture and training method. To quantify uncertainty, we measure binary entropy as a linear combination of total entropy, conditional entropy, and mutual information. To examine its effects, we introduce stochasticity into models via \"dropout\" at inference time. Our findings show that our mechanism increases total entropy, mainly through a rise in mutual information, while conditional entropy remains largely unchanged. Moreover, this mechanism significantly improves human-LLM moral alignment, with correlations in mutual information and alignment score shifts. Our results highlight the potential to better align model-generated decisions and human preferences by deliberately modulating uncertainty and reducing LLMs' confidence in morally complex scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†é“å¾·å›°å¢ƒæ—¶æ™®éå­˜åœ¨çš„è¿‡åº¦è‡ªä¿¡é—®é¢˜ï¼Œå¹¶æ·±å…¥åˆ†æäº†32ä¸ªå¼€æºæ¨¡å‹åœ¨ç”µè½¦éš¾é¢˜ï¼ˆtrolley problemï¼‰åŠ9ä¸ªé“å¾·ç»´åº¦ä¸‹çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°æ¨¡å‹ä¿¡å¿ƒçš„å·®å¼‚ä¸»è¦å—æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæ–¹æ³•å½±å“ï¼Œè€Œéé“å¾·ç»´åº¦æœ¬èº«ï¼Œå› æ­¤ç ”ç©¶è€…å¼•å…¥äº†æ¨ç†é˜¶æ®µçš„éšæœºå¤±æ´»ï¼ˆdropoutï¼‰æœºåˆ¶æ¥å¢åŠ æ¨¡å‹ç”Ÿæˆçš„éšæœºæ€§ã€‚é€šè¿‡å¯¹äºŒå…ƒç†µï¼ˆbinary entropyï¼‰ã€äº’ä¿¡æ¯ï¼ˆmutual informationï¼‰ç­‰æŒ‡æ ‡çš„é‡åŒ–æµ‹é‡ï¼Œç ”ç©¶å‘ç°dropouté€šè¿‡å¢åŠ ä¸ç¡®å®šæ€§æ˜¾è‘—æå‡äº†æ¨¡å‹çš„é“å¾·å¯¹é½ï¼ˆmoral alignmentï¼‰è¯„åˆ†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡åˆ»æ„è°ƒèŠ‚ä¸ç¡®å®šæ€§å¹¶é™ä½LLMsåœ¨å¤æ‚é“å¾·åœºæ™¯ä¸­çš„ä¿¡å¿ƒï¼Œå¯ä»¥ä½¿æ¨¡å‹ç”Ÿæˆçš„å†³ç­–æ›´å¥½åœ°ç¬¦åˆäººç±»åå¥½ã€‚è¿™é¡¹å·¥ä½œä¸ºæ„å»ºæ›´å¯é ä¸”å…·å¤‡é“å¾·æ´å¯ŸåŠ›çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†é‡è¦çš„æ–¹æ³•è®ºæ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.13290v1",
      "published_date": "2025-11-17 12:13:15 UTC",
      "updated_date": "2025-11-17 12:13:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:01:22.235671+00:00"
    },
    {
      "arxiv_id": "2511.13288v2",
      "title": "Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO",
      "title_zh": "å¤šæ™ºèƒ½ä½“æ·±åº¦ç ”ç©¶ï¼šåŸºäº M-GRPO çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè®­ç»ƒ",
      "authors": [
        "Haoyang Hong",
        "Jiajun Yin",
        "Yuan Wang",
        "Jingnan Liu",
        "Zhe Chen",
        "Ailing Yu",
        "Ji Li",
        "Zhiling Ye",
        "Hansong Xiao",
        "Yefei Chen",
        "Hualei Zhou",
        "Yun Yue",
        "Minghui Yang",
        "Chunxiao Guo",
        "Junwei Liu",
        "Peng Wei",
        "Jinjie Gu"
      ],
      "abstract": "Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨ç‰¹å®šé¢†åŸŸå‡†ç¡®ç‡å—é™ä»¥åŠç»Ÿä¸€LLMsè®­ç»ƒæ¨¡å¼å¸¦æ¥çš„æ€§èƒ½ç“¶é¢ˆï¼Œæå‡ºäº†M-GRPOï¼Œè¿™æ˜¯ä¸€ç§ä¸ºå‚ç›´å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè®¾è®¡çš„Group Relative Policy Optimizationåˆ†å±‚æ‰©å±•æ–¹æ¡ˆã€‚M-GRPOé€šè¿‡ä¸ºä¸»æ™ºèƒ½ä½“å’Œå­æ™ºèƒ½ä½“åˆ†åˆ«è®¡ç®—ç»„ç›¸å¯¹ä¼˜åŠ¿ï¼Œå®ç°äº†æœ‰æ•ˆçš„åˆ†å±‚ä¿¡ç”¨åˆ†é…ã€‚ä¸ºäº†è§£å†³å¤šæ™ºèƒ½ä½“è°ƒç”¨é¢‘ç‡ä¸ä¸€åŠè·¨æœåŠ¡å™¨éƒ¨ç½²å¯¼è‡´çš„æ¢¯åº¦æµä¸­æ–­ç­‰ä¼˜åŒ–æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†è½¨è¿¹å¯¹é½(trajectory-alignment)æ–¹æ¡ˆå’Œè§£è€¦è®­ç»ƒæµæ°´çº¿ï¼Œå…è®¸åœ¨æ— éœ€è·¨æœåŠ¡å™¨backpropagationçš„æƒ…å†µä¸‹è¿›è¡Œå¯æ‰©å±•è®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒM-GRPOåœ¨GAIAã€XBench-DeepSearchå’ŒWebWalkerQAç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ€§èƒ½ä¸€è‡´ä¼˜äºå•æ™ºèƒ½ä½“GRPOå’Œå­æ™ºèƒ½ä½“å†»ç»“çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡å¯¹é½å¼‚æ„è½¨è¿¹å’Œè§£è€¦ä¸“ä¸šåŒ–æ™ºèƒ½ä½“çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œå¯ä»¥æ˜¾è‘—æå‡å·¥å…·å¢å¼ºå‹æ¨ç†ä»»åŠ¡çš„ç¨³å®šæ€§å’Œé‡‡æ ·æ•ˆç‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13288v2",
      "published_date": "2025-11-17 12:06:30 UTC",
      "updated_date": "2025-11-18 03:13:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:01:13.230696+00:00"
    },
    {
      "arxiv_id": "2511.13274v1",
      "title": "KForge: Program Synthesis for Diverse AI Hardware Accelerators",
      "title_zh": "KForgeï¼šé¢å‘å¤šæ ·åŒ– AI ç¡¬ä»¶åŠ é€Ÿå™¨çš„ç¨‹åºåˆæˆ",
      "authors": [
        "Taras Sereda",
        "Tom St. John",
        "Burak Bartan",
        "Natalie Serrino",
        "Sachin Katti",
        "Zain Asgar"
      ],
      "abstract": "GPU kernels are critical for ML performance but difficult to optimize across diverse accelerators. We present KForge, a platform-agnostic framework built on two collaborative LLM-based agents: a generation agent that produces and iteratively refines programs through compilation and correctness feedback, and a performance analysis agent that interprets profiling data to guide optimization. This agent-based architecture requires only a single-shot example to target new platforms.\n  We make three key contributions: (1) introducing an iterative refinement system where the generation agent and performance analysis agent collaborate through functional and optimization passes, interpreting diverse profiling data (from programmatic APIs to GUI-based tools) to generate actionable recommendations that guide program synthesis for arbitrary accelerators; (2) demonstrating that the generation agent effectively leverages cross-platform knowledge transfer, where a reference implementation from one architecture substantially improves generation quality for different hardware targets; and (3) validating the platform-agnostic nature of our approach by demonstrating effective program synthesis across fundamentally different parallel computing platforms: NVIDIA CUDA and Apple Metal.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† KForgeï¼Œä¸€ä¸ªä¸å¹³å°æ— å…³çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è·¨å¼‚æ„åŠ é€Ÿå™¨ä¼˜åŒ– GPU kernels çš„æŒ‘æˆ˜ã€‚KForge æ ¸å¿ƒé‡‡ç”¨äº†åŸºäº LLM çš„åŒæ™ºèƒ½ä½“åä½œæ¶æ„ï¼ŒåŒ…å«ä¸€ä¸ªé€šè¿‡ç¼–è¯‘å’Œæ­£ç¡®æ€§åé¦ˆè¿›è¡Œè¿­ä»£ä¼˜åŒ–çš„ generation agentï¼Œä»¥åŠä¸€ä¸ªè§£è¯»æ€§èƒ½æ•°æ®å¹¶æä¾›ä¼˜åŒ–å»ºè®®çš„ performance analysis agentã€‚è¯¥æ¡†æ¶å¼•å…¥äº†è¿­ä»£ç²¾åŒ–ç³»ç»Ÿï¼Œèƒ½å¤Ÿå¤„ç†ä»ç¨‹åºåŒ– API åˆ° GUI å·¥å…·çš„å„ç§åˆ†ææ•°æ®ï¼Œä½¿ç³»ç»Ÿä»…éœ€å•æ¬¡ç¤ºä¾‹å³å¯é’ˆå¯¹æ–°å¹³å°ç”Ÿæˆé«˜è´¨é‡ç¨‹åºã€‚ç ”ç©¶è¿˜è¡¨æ˜ generation agent èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨ cross-platform knowledge transferï¼Œé€šè¿‡ç‰¹å®šæ¶æ„çš„å‚è€ƒå®ç°æ¥æ˜¾è‘—è¾…åŠ©ä¸åŒç¡¬ä»¶ç›®æ ‡çš„ç¨‹åºç”Ÿæˆã€‚æœ€åï¼Œè¯¥æ–¹æ³•åœ¨ NVIDIA CUDA å’Œ Apple Metal è¿™ç±»è¿¥å¼‚çš„å¹¶è¡Œè®¡ç®—å¹³å°ä¸Šå¾—åˆ°äº†éªŒè¯ï¼Œè¯æ˜äº†å…¶åœ¨å¤šç§ AI ç¡¬ä»¶åŠ é€Ÿå™¨ä¸Šè¿›è¡Œç¨‹åºåˆæˆçš„æœ‰æ•ˆæ€§ä¸é€šç”¨æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA",
        "cs.PF",
        "cs.SE"
      ],
      "primary_category": "cs.LG",
      "comment": "Under review at MLSys 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.13274v1",
      "published_date": "2025-11-17 11:46:43 UTC",
      "updated_date": "2025-11-17 11:46:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:01:00.531452+00:00"
    },
    {
      "arxiv_id": "2511.13273v2",
      "title": "AudioMotionBench: Evaluating Auditory Motion Perception in Audio LLMs",
      "title_zh": "AudioMotionBenchï¼šè¯„ä¼°éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹çš„å¬è§‰è¿åŠ¨æ„ŸçŸ¥",
      "authors": [
        "Zhe Sun",
        "Yujun Cai",
        "Jiayu Yao",
        "Yiwei Wang"
      ],
      "abstract": "Large Audio-Language Models (LALMs) have recently shown impressive progress in speech recognition, audio captioning, and auditory question answering. Yet, whether these models can perceive spatial dynamics, particularly the motion of sound sources, remains unclear. In this work, we uncover a systematic motion perception deficit in current ALLMs. To investigate this issue, we introduce AudioMotionBench, the first benchmark explicitly designed to evaluate auditory motion understanding. AudioMotionBench introduces a controlled question-answering benchmark designed to evaluate whether Audio-Language Models (LALMs) can infer the direction and trajectory of moving sound sources from binaural audio. Comprehensive quantitative and qualitative analyses reveal that current models struggle to reliably recognize motion cues or distinguish directional patterns. The average accuracy remains below 50\\%, underscoring a fundamental limitation in auditory spatial reasoning. Our study highlights a fundamental gap between human and model auditory spatial reasoning, providing both a diagnostic tool and new insight for enhancing spatial cognition in future Audio-Language Models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨æ¢è®¨å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹(LALMs)æ˜¯å¦å…·å¤‡æ„ŸçŸ¥ç©ºé—´åŠ¨æ€çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯å£°æºçš„è¿åŠ¨ç‰¹å¾ã€‚ç ”ç©¶è€…æ­ç¤ºäº†ç°æœ‰LALMsåœ¨è¿åŠ¨æ„ŸçŸ¥æ–¹é¢å­˜åœ¨ç³»ç»Ÿæ€§ç¼ºé™·ï¼Œå¹¶ä¸ºæ­¤å¼•å…¥äº†é¦–ä¸ªä¸“é—¨è¯„ä¼°å¬è§‰è¿åŠ¨ç†è§£çš„åŸºå‡†æµ‹è¯•AudioMotionBenchã€‚è¯¥åŸºå‡†é€šè¿‡å—æ§çš„é—®ç­”ç¯èŠ‚ï¼Œè¯„ä¼°æ¨¡å‹ä»åŒè€³éŸ³é¢‘(binaural audio)ä¸­æ¨æ–­å£°æºè¿åŠ¨æ–¹å‘å’Œè½¨è¿¹çš„èƒ½åŠ›ã€‚å®šé‡ä¸å®šæ€§åˆ†æè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨è¯†åˆ«è¿åŠ¨çº¿ç´¢å’ŒåŒºåˆ†æ–¹å‘æ¨¡å¼æ–¹é¢è¡¨ç°æ¬ ä½³ï¼Œå¹³å‡å‡†ç¡®ç‡ä¸è¶³50%ï¼Œå‡¸æ˜¾äº†å¬è§‰ç©ºé—´æ¨ç†çš„å±€é™ã€‚è¯¥é¡¹å·¥ä½œæ­ç¤ºäº†æ¨¡å‹ä¸äººç±»åœ¨ç©ºé—´æ¨ç†ä¸Šçš„åŸºç¡€æ€§å·®è·ï¼Œä¸ºæå‡æœªæ¥éŸ³é¢‘è¯­è¨€æ¨¡å‹çš„ç©ºé—´è®¤çŸ¥èƒ½åŠ›æä¾›äº†é‡è¦çš„è¯Šæ–­å·¥å…·å’Œç ”ç©¶è§è§£ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13273v2",
      "published_date": "2025-11-17 11:45:41 UTC",
      "updated_date": "2026-01-22 17:11:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:01:18.138130+00:00"
    },
    {
      "arxiv_id": "2511.13271v1",
      "title": "Examining the Usage of Generative AI Models in Student Learning Activities for Software Programming",
      "title_zh": "è½¯ä»¶ç¼–ç¨‹å­¦ä¹ æ´»åŠ¨ä¸­ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹çš„ä½¿ç”¨æƒ…å†µç ”ç©¶",
      "authors": [
        "Rufeng Chen",
        "Shuaishuai Jiang",
        "Jiyun Shen",
        "AJung Moon",
        "Lili Wei"
      ],
      "abstract": "The rise of Generative AI (GenAI) tools like ChatGPT has created new opportunities and challenges for computing education. Existing research has primarily focused on GenAI's ability to complete educational tasks and its impact on student performance, often overlooking its effects on knowledge gains. In this study, we investigate how GenAI assistance compares to conventional online resources in supporting knowledge gains across different proficiency levels. We conducted a controlled user experiment with 24 undergraduate students of two different levels of programming experience (beginner, intermediate) to examine how students interact with ChatGPT while solving programming tasks. We analyzed task performance, conceptual understanding, and interaction behaviors. Our findings reveal that generating complete solutions with GenAI significantly improves task performance, especially for beginners, but does not consistently result in knowledge gains. Importantly, usage strategies differ by experience: beginners tend to rely heavily on GenAI toward task completion often without knowledge gain in the process, while intermediates adopt more selective approaches. We find that both over-reliance and minimal use result in weaker knowledge gains overall. Based on our results, we call on students and educators to adopt GenAI as a learning rather than a problem solving tool. Our study highlights the urgent need for guidance when integrating GenAI into programming education to foster deeper understanding.",
      "tldr_zh": "æœ¬ç ”ç©¶è°ƒæŸ¥äº† Generative AI (GenAI) æ¨¡å‹ï¼ˆå¦‚ ChatGPTï¼‰åœ¨è½¯ä»¶ç¼–ç¨‹å­¦ä¹ æ´»åŠ¨ä¸­çš„åº”ç”¨ï¼Œé€šè¿‡å¯¹ 24 åä¸åŒç¼–ç¨‹æ°´å¹³ï¼ˆåˆå­¦è€…å’Œä¸­çº§ï¼‰çš„æœ¬ç§‘ç”Ÿè¿›è¡Œå—æ§å®éªŒï¼Œå¯¹æ¯”äº† GenAI è¾…åŠ©ä¸ä¼ ç»Ÿåœ¨çº¿èµ„æºåœ¨æ”¯æŒçŸ¥è¯†è·å–æ–¹é¢çš„å·®å¼‚ã€‚ç ”ç©¶æ·±å…¥åˆ†æäº†å­¦ç”Ÿåœ¨è§£å†³ç¼–ç¨‹ä»»åŠ¡æ—¶çš„ä»»åŠ¡è¡¨ç°ã€æ¦‚å¿µç†è§£åŠäº¤äº’è¡Œä¸ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶åˆ©ç”¨ GenAI ç”Ÿæˆå®Œæ•´è§£æ³•èƒ½æ˜¾è‘—æå‡ä»»åŠ¡è¡¨ç°ï¼ˆå°¤å…¶æ˜¯åˆå­¦è€…ï¼‰ï¼Œä½†è¿™ç§æå‡å¹¶ä¸æ€»èƒ½è½¬åŒ–ä¸ºæœ‰æ•ˆçš„çŸ¥è¯†å¢é•¿ (knowledge gains)ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œåˆå­¦è€…å¾€å¾€è¿‡åº¦ä¾èµ– GenAI æ¥å®Œæˆä»»åŠ¡ï¼Œè€Œä¸­çº§å­¦ç”Ÿåˆ™è¡¨ç°å‡ºæ›´å…·é€‰æ‹©æ€§çš„äº¤äº’ç­–ç•¥ã€‚å®è¯æ•°æ®è¡¨æ˜ï¼Œè¿‡åº¦ä¾èµ–æˆ–ä½¿ç”¨ä¸è¶³å‡ä¸åˆ©äºçŸ¥è¯†è·å–ï¼Œå› æ­¤ä½œè€…å‘¼åæ•™è‚²è€…å¼•å¯¼å­¦ç”Ÿå°† GenAI è§†ä¸ºè¾…åŠ©å­¦ä¹ çš„å·¥å…·ï¼Œè€Œéå•çº¯çš„è§£é¢˜å·¥å…·ã€‚è¯¥é¡¹ç ”ç©¶å¼ºè°ƒäº†åœ¨ç¼–ç¨‹æ•™è‚²ä¸­æ•´åˆ GenAI æ—¶æä¾›é’ˆå¯¹æ€§æŒ‡å¯¼çš„ç´§è¿«æ€§ï¼Œä»¥ç¡®ä¿å­¦ç”Ÿå®ç°æ›´æ·±å±‚æ¬¡çš„ç†è§£ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.SE",
      "comment": "9 pages, 4 figures, accepted at AIWARE 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.13271v1",
      "published_date": "2025-11-17 11:42:24 UTC",
      "updated_date": "2025-11-17 11:42:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:01:10.927575+00:00"
    },
    {
      "arxiv_id": "2512.04092v1",
      "title": "The changing surface of the world's roads",
      "title_zh": "å…¨çƒé“è·¯è·¯é¢çš„å˜è¿",
      "authors": [
        "Sukanya Randhawa",
        "Guntaj Randhawa",
        "Clemens Langer",
        "Francis Andorful",
        "Benjamin Herfort",
        "Daniel Kwakye",
        "Omer Olchik",
        "Sven Lautenbach",
        "Alexander Zipf"
      ],
      "abstract": "Resilient road infrastructure is a cornerstone of the UN Sustainable Development Goals. Yet a primary indicator of network functionality and resilience is critically lacking: a comprehensive global baseline of road surface information. Here, we overcome this gap by applying a deep learning framework to a global mosaic of Planetscope satellite imagery from 2020 and 2024. The result is the first global multi-temporal dataset of road pavedness and width for 9.2 million km of critical arterial roads, achieving 95.5% coverage where nearly half the network was previously unclassified. This dataset reveals a powerful multi-scale geography of human development. At the planetary scale, we show that the rate of change in pavedness is a robust proxy for a country's development trajectory (correlation with HDI = 0.65). At the national scale, we quantify how unpaved roads constitute a fragile backbone for economic connectivity. We further synthesize our data into a global Humanitarian Passability Matrix with direct implications for humanitarian logistics. At the local scale, case studies demonstrate the framework's versatility: in Ghana, road quality disparities expose the spatial outcomes of governance; in Pakistan, the data identifies infrastructure vulnerabilities to inform climate resilience planning. Together, this work delivers both a foundational dataset and a multi-scale analytical framework for monitoring global infrastructure, from the dynamics of national development to the realities of local governance, climate adaptation, and equity. Unlike traditional proxies such as nighttime lights, which reflect economic activity, road surface data directly measures the physical infrastructure that underpins prosperity and resilience - at higher spatial resolution.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨æ·±åº¦å­¦ä¹ æ¡†æ¶(Deep Learning)å¤„ç†2020å¹´è‡³2024å¹´çš„Planetscopeå…¨çƒå«æ˜Ÿå›¾åƒï¼Œæ„å»ºäº†é¦–ä¸ªæ¶µç›–920ä¸‡å…¬é‡Œå…³é”®å¹²çº¿å…¬è·¯è·¯é¢é“ºè£…æƒ…å†µåŠå®½åº¦çš„å…¨çƒå¤šæ—¶åºæ•°æ®é›†ã€‚ç ”ç©¶æœ‰æ•ˆå¡«è¡¥äº†å…¨çƒé“è·¯è¡¨é¢ä¿¡æ¯åŸºå‡†çš„ç©ºç™½ï¼Œå®ç°äº†95.5%çš„è¦†ç›–ç‡ï¼Œå¹¶å¯¹æ­¤å‰æœªåˆ†ç±»çš„è¿‘åŠæ•°é“è·¯ç½‘ç»œè¿›è¡Œäº†åˆ†ç±»ã€‚åˆ†ææ˜¾ç¤ºï¼Œè·¯é¢é“ºè£…ç‡çš„å˜åŒ–é€Ÿåº¦æ˜¯è¡¡é‡å›½å®¶å‘å±•è½¨è¿¹çš„æœ‰åŠ›æŒ‡æ ‡ï¼Œä¸äººç±»å‘å±•æŒ‡æ•°(HDI)å…·æœ‰æ˜¾è‘—ç›¸å…³æ€§ã€‚ç ”ç©¶è¿˜æå‡ºäº†å…¨çƒäººé“ä¸»ä¹‰é€šè¡ŒçŸ©é˜µ(Humanitarian Passability Matrix)ï¼Œä¸ºæ•‘æ´ç‰©æµæä¾›ç›´æ¥å‚è€ƒã€‚é€šè¿‡åŠ çº³å’Œå·´åŸºæ–¯å¦çš„æ¡ˆä¾‹ï¼Œè¯¥æ¡†æ¶å±•ç¤ºäº†å…¶åœ¨è¯„ä¼°æ²»ç†æˆæ•ˆå’Œæ°”å€™éŸ§æ€§è§„åˆ’ä¸­çš„å¤šå°ºåº¦åˆ†æèƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„å¤œé—´ç¯å…‰ä»£ç†æŒ‡æ ‡ç›¸æ¯”ï¼Œè¯¥é“è·¯æ•°æ®èƒ½ä»¥æ›´é«˜çš„ç©ºé—´åˆ†è¾¨ç‡ç›´æ¥é‡åŒ–æ”¯æ’‘ç¤¾ä¼šç¹è£ä¸åŸºå»ºéŸ§æ€§çš„ç‰©ç†åŸºç¡€ã€‚",
      "categories": [
        "physics.soc-ph",
        "cs.AI",
        "cs.CV",
        "cs.CY"
      ],
      "primary_category": "physics.soc-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.04092v1",
      "published_date": "2025-11-17 11:38:43 UTC",
      "updated_date": "2025-11-17 11:38:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:01:32.243908+00:00"
    },
    {
      "arxiv_id": "2511.13259v1",
      "title": "GeoX-Bench: Benchmarking Cross-View Geo-Localization and Pose Estimation Capabilities of Large Multimodal Models",
      "title_zh": "GeoX-Benchï¼šå¤šæ¨¡æ€å¤§æ¨¡å‹è·¨è§†è§’åœ°ç†å®šä½ä¸ä½å§¿ä¼°è®¡èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Yushuo Zheng",
        "Jiangyong Ying",
        "Huiyu Duan",
        "Chunyi Li",
        "Zicheng Zhang",
        "Jing Liu",
        "Xiaohong Liu",
        "Guangtao Zhai"
      ],
      "abstract": "Large multimodal models (LMMs) have demonstrated remarkable capabilities across a wide range of tasks, however their knowledge and abilities in the cross-view geo-localization and pose estimation domains remain unexplored, despite potential benefits for navigation, autonomous driving, outdoor robotics, \\textit{etc}. To bridge this gap, we introduce \\textbf{GeoX-Bench}, a comprehensive \\underline{Bench}mark designed to explore and evaluate the capabilities of LMMs in \\underline{cross}-view \\underline{Geo}-localization and pose estimation. Specifically, GeoX-Bench contains 10,859 panoramic-satellite image pairs spanning 128 cities in 49 countries, along with corresponding 755,976 question-answering (QA) pairs. Among these, 42,900 QA pairs are designated for benchmarking, while the remaining are intended to enhance the capabilities of LMMs. Based on GeoX-Bench, we evaluate the capabilities of 25 state-of-the-art LMMs on cross-view geo-localization and pose estimation tasks, and further explore the empowered capabilities of instruction-tuning. Our benchmark demonstrate that while current LMMs achieve impressive performance in geo-localization tasks, their effectiveness declines significantly on the more complex pose estimation tasks, highlighting a critical area for future improvement, and instruction-tuning LMMs on the training data of GeoX-Bench can significantly improve the cross-view geo-sense abilities. The GeoX-Bench is available at \\textcolor{magenta}{https://github.com/IntMeGroup/GeoX-Bench}.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†GeoX-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å’Œæå‡å¤§å‹å¤šæ¨¡æ€æ¨¡å‹(LMMs)åœ¨è·¨è§†å›¾åœ°ç†å®šä½(cross-view geo-localization)å’Œå§¿æ€ä¼°è®¡(pose estimation)é¢†åŸŸèƒ½åŠ›çš„ç»¼åˆåŸºå‡†ã€‚è¯¥åŸºå‡†åŒ…å«è¦†ç›–å…¨çƒ49ä¸ªå›½å®¶128ä¸ªåŸå¸‚çš„10,859å¯¹å…¨æ™¯-å«æ˜Ÿå›¾åƒï¼Œä»¥åŠè¶…è¿‡75ä¸‡å¯¹é—®ç­”(QA)å¯¹ï¼Œä¸ºæ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°æä¾›äº†ä¸°å¯Œçš„æ•°æ®æ”¯æŒã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨GeoX-Benchå¯¹25ç§æœ€å…ˆè¿›çš„LMMsè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶è¿›ä¸€æ­¥æ¢è®¨äº†æŒ‡ä»¤å¾®è°ƒ(instruction-tuning)å¯¹å¢å¼ºè·¨è§†å›¾åœ°ç†æ„ŸçŸ¥èƒ½åŠ›çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡ç°æœ‰çš„LMMsåœ¨åœ°ç†å®šä½ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚çš„å§¿æ€ä¼°è®¡ä»»åŠ¡ä¸­æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œæ­ç¤ºäº†æœªæ¥çš„å…³é”®æ”¹è¿›é¢†åŸŸã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯æ˜åœ¨GeoX-Benchè®­ç»ƒæ•°æ®ä¸Šè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒèƒ½æ˜¾è‘—æå‡æ¨¡å‹çš„è·¨è§†å›¾åœ°ç†æ„ŸçŸ¥èƒ½åŠ›ï¼Œä¸ºå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶å’Œæˆ·å¤–æœºå™¨äººç­‰åº”ç”¨æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13259v1",
      "published_date": "2025-11-17 11:19:07 UTC",
      "updated_date": "2025-11-17 11:19:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:01:49.432311+00:00"
    },
    {
      "arxiv_id": "2511.13802v2",
      "title": "Passive Dementia Screening via Facial Temporal Micro-Dynamics Analysis of In-the-Wild Talking-Head Video",
      "title_zh": "åŸºäºè‡ªç„¶åœºæ™¯å¯¹è¯è§†é¢‘é¢éƒ¨æ—¶åºå¾®åŠ¨æ€åˆ†æçš„è¢«åŠ¨å¼ç—´å‘†ç­›æŸ¥",
      "authors": [
        "Filippo Cenacchi",
        "Longbing Cao",
        "Mitchell McEwan",
        "Deborah Richards"
      ],
      "abstract": "We target passive dementia screening from short camera-facing talking head video, developing a facial temporal micro dynamics analysis for language free detection of early neuro cognitive change. This enables unscripted, in the wild video analysis at scale to capture natural facial behaviors, transferrable across devices, topics, and cultures without active intervention by clinicians or researchers during recording. Most existing resources prioritize speech or scripted interviews, limiting use outside clinics and coupling predictions to language and transcription. In contrast, we identify and analyze whether temporal facial kinematics, including blink dynamics, small mouth jaw motions, gaze variability, and subtle head adjustments, are sufficient for dementia screening without speech or text. By stabilizing facial signals, we convert these micro movements into interpretable facial microdynamic time series, smooth them, and summarize short windows into compact clip level statistics for screening. Each window is encoded by its activity mix (the relative share of motion across streams), thus the predictor analyzes the distribution of motion across streams rather than its magnitude, making per channel effects transparent. We also introduce YT DemTalk, a new dataset curated from publicly available, in the wild camera facing videos. It contains 300 clips (150 with self reported dementia, 150 controls) to test our model and offer a first benchmarking of the corpus. On YT DemTalk, ablations identify gaze lability and mouth/jaw dynamics as the most informative cues, and light weighted shallow classifiers could attain a dementia prediction performance of (AUROC) 0.953, 0.961 Average Precision (AP), 0.851 F1-score, and 0.857 accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é€šè¿‡åˆ†æè‡ªç„¶çŠ¶æ€ä¸‹äººè„¸è§†é¢‘çš„æ—¶åŸŸå¾®è§‚åŠ¨æ€(Facial Temporal Micro-Dynamics)è¿›è¡Œè¢«åŠ¨å¼ç—´å‘†ç—‡(Dementia)ç­›æŸ¥çš„æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°æ—©æœŸç¥ç»è®¤çŸ¥å˜åŒ–çš„æ— è¯­è¨€(Language-free)æ£€æµ‹ã€‚è¯¥æ–¹æ³•é€šè¿‡ç¨³å®šåŒ–é¢éƒ¨ä¿¡å·ï¼Œå°†çœ¨çœ¼åŠ¨æ€(Blink Dynamics)ã€ç»†å¾®çš„å£è§’è¿åŠ¨ã€æ³¨è§†ç‚¹å˜å¼‚æ€§(Gaze Variability)å’Œå¤´éƒ¨å¾®è°ƒè½¬åŒ–ä¸ºå¯è§£é‡Šçš„æ—¶é—´åºåˆ—ç»Ÿè®¡é‡ï¼Œå¹¶ä¾§é‡åˆ†æä¸åŒè¿åŠ¨æµçš„åˆ†å¸ƒæ¯”ä¾‹ã€‚ç ”ç©¶è€…è¿˜æ„å»ºäº†åä¸ºYT DemTalkçš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«300æ®µæºè‡ªå…¬å¼€æ¸ é“çš„è‡ªç„¶è°ˆè¯è§†é¢‘ç”¨äºåŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ³¨è§†ä¸ç¨³å®šæ€§(Gaze Lability)å’Œå£/é¢ŒåŠ¨æ€æ˜¯æœ€å…·è¾¨è¯†æ€§çš„ç‰¹å¾ï¼Œåˆ©ç”¨è½»é‡çº§æµ…å±‚åˆ†ç±»å™¨å³å¯å®ç°0.953çš„AUROCå’Œ0.857çš„å‡†ç¡®ç‡(Accuracy)ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åœ¨æ— éœ€è¯­éŸ³æˆ–æ–‡æœ¬è½¬å½•çš„æƒ…å†µä¸‹ï¼Œé¢éƒ¨è¿åŠ¨ç‰¹å¾è¶³ä»¥æ”¯æ’‘é«˜æ•ˆä¸”å¯æ‰©å±•çš„ä¸´åºŠå¤–ç—´å‘†ç—‡ç­›æŸ¥ã€‚è¿™ä¸€æˆæœä¸ºæ•æ‰è‡ªç„¶é¢éƒ¨è¡Œä¸ºã€å®ç°è·¨æ–‡åŒ–åŠè·¨è®¾å¤‡çš„è‡ªåŠ¨åŒ–è¾…åŠ©è¯Šæ–­æä¾›äº†é‡è¦æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13802v2",
      "published_date": "2025-11-17 11:14:30 UTC",
      "updated_date": "2025-11-26 09:04:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:01:55.736763+00:00"
    },
    {
      "arxiv_id": "2511.13245v1",
      "title": "Proceedings Seventh International Workshop on Formal Methods for Autonomous Systems",
      "title_zh": "ç¬¬ä¸ƒå±Šè‡ªä¸»ç³»ç»Ÿå½¢å¼åŒ–æ–¹æ³•å›½é™…ç ”è®¨ä¼šè®ºæ–‡é›†",
      "authors": [
        "Matt Luckcuck",
        "Maike Schwammberger",
        "Mengwei Xu"
      ],
      "abstract": "This EPTCS volume contains the papers from the Seventh International Workshop on Formal Methods for Autonomous Systems (FMAS 2025), which was held between the 17th and 19th of November 2025. The goal of the FMAS workshop series is to bring together leading researchers who are using formal methods to tackle the unique challenges that autonomous systems present, so that they can publish and discuss their work with a growing community of researchers. FMAS 2025 was co-located with the 20th International Conference on integrated Formal Methods (iFM'25), hosted by Inria Paris, France at the Inria Paris Center. \n  In total, FMAS 2025 received 16 submissions from researchers at institutions in: Canada, China, France, Germany, Ireland, Italy, Japan, the Netherlands, Portugal, Sweden, the United States of America, and the United Kingdom. Though we received fewer submissions than last year, we are encouraged to see the submissions being sent from a wide range of countries. Submissions come from both past and new FMAS authors, which shows us that the existing community appreciates the network that FMAS has built over the past 7 years, while new authors also show the FMAS community's great potential of growth.",
      "tldr_zh": "è¯¥è®ºæ–‡é›†æ”¶å½•äº†ç¬¬ä¸ƒå±Šè‡ªä¸»ç³»ç»Ÿå½¢å¼åŒ–æ–¹æ³•å›½é™…ç ”è®¨ä¼š (FMAS 2025) çš„è®ºæ–‡æˆæœï¼Œè¯¥ä¼šè®®äº 2025 å¹´ 11 æœˆåœ¨æ³•å›½å·´é»ä¸ç¬¬ 20 å±Šé›†æˆå½¢å¼åŒ–æ–¹æ³•å›½é™…ä¼šè®® (iFM'25) åŒæœŸä¸¾åŠã€‚ç ”è®¨ä¼šçš„æ ¸å¿ƒä½¿å‘½æ˜¯æ±‡èšå…¨çƒç ”ç©¶åŠ›é‡ï¼Œåˆ©ç”¨å½¢å¼åŒ–æ–¹æ³• (Formal Methods) åº”å¯¹è‡ªä¸»ç³»ç»Ÿ (Autonomous Systems) å¸¦æ¥çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚æœ¬å±Šä¼šè®®å…±æ”¶åˆ°æ¥è‡ªä¸­å›½ã€ç¾å›½ã€è‹±å›½ç­‰ 12 ä¸ªå›½å®¶ç ”ç©¶æœºæ„çš„ 16 ç¯‡æŠ•ç¨¿ï¼Œå±•ç¤ºäº†é«˜åº¦çš„å›½é™…åŒ–å‚ä¸åº¦ã€‚æŠ•ç¨¿æ¥æºæ¶µç›–äº†è¯¥é¢†åŸŸçš„èµ„æ·±ä¸“å®¶ä¸æ–°å…´å­¦è€…ï¼Œä½“ç°äº† FMAS ç¤¾åŒºåœ¨è¿‡å» 7 å¹´ä¸­å»ºç«‹çš„ç¨³å›ºå­¦æœ¯ç½‘ç»œã€‚è¯¥å·å†Œçš„å‡ºç‰ˆä¸ä»…è®°å½•äº†å½“å‰çš„å­¦æœ¯è¿›å±•ï¼Œä¹Ÿé¢„ç¤ºäº†è‡ªä¸»ç³»ç»Ÿå½¢å¼åŒ–éªŒè¯æŠ€æœ¯çš„æŒç»­å¢é•¿æ½œåŠ›å’Œå¹¿é˜”åº”ç”¨å‰æ™¯ã€‚",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13245v1",
      "published_date": "2025-11-17 11:07:57 UTC",
      "updated_date": "2025-11-17 11:07:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:02:58.130023+00:00"
    },
    {
      "arxiv_id": "2511.13244v1",
      "title": "Seek and You Shall Fold",
      "title_zh": "æ±‚ç´¢å³æŠ˜å ",
      "authors": [
        "Nadav Bojan Sellam",
        "Meital Bojan",
        "Paul Schanda",
        "Alex Bronstein"
      ],
      "abstract": "Accurate protein structures are essential for understanding biological function, yet incorporating experimental data into protein generative models remains a major challenge. Most predictors of experimental observables are non-differentiable, making them incompatible with gradient-based conditional sampling. This is especially limiting in nuclear magnetic resonance, where rich data such as chemical shifts are hard to directly integrate into generative modeling. We introduce a framework for non-differentiable guidance of protein generative models, coupling a continuous diffusion-based generator with any black-box objective via a tailored genetic algorithm. We demonstrate its effectiveness across three modalities: pairwise distance constraints, nuclear Overhauser effect restraints, and for the first time chemical shifts. These results establish chemical shift guided structure generation as feasible, expose key weaknesses in current predictors, and showcase a general strategy for incorporating diverse experimental signals. Our work points toward automated, data-conditioned protein modeling beyond the limits of differentiability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è›‹ç™½è´¨ç”Ÿæˆæ¨¡å‹éš¾ä»¥æ•´åˆéå¾®åˆ†(non-differentiable)å®éªŒè§‚æµ‹æ•°æ®çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§å…¨æ–°çš„å¼•å¯¼æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å®šåˆ¶çš„é—ä¼ ç®—æ³•(genetic algorithm)å°†åŸºäºè¿ç»­æ‰©æ•£(diffusion-based)çš„ç”Ÿæˆå™¨ä¸ä»»ä½•é»‘ç›’ç›®æ ‡(black-box objective)ç›¸ç»“åˆï¼Œæœ‰æ•ˆè§£å†³äº†æ¢¯åº¦é‡‡æ ·ä¸­çš„å…¼å®¹æ€§é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨æˆå¯¹è·ç¦»çº¦æŸ(pairwise distance constraints)ã€æ ¸Overhauseræ•ˆåº”(nuclear Overhauser effect)çº¦æŸä»¥åŠåŒ–å­¦ä½ç§»(chemical shifts)ä¸‰ç§æ¨¡æ€ä¸‹éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœé¦–æ¬¡è¯æ˜äº†åˆ©ç”¨åŒ–å­¦ä½ç§»å¼•å¯¼ç»“æ„ç”Ÿæˆçš„å¯è¡Œæ€§ï¼Œå¹¶æ­ç¤ºäº†ç°æœ‰é¢„æµ‹å™¨çš„å…³é”®å¼±ç‚¹ã€‚è¯¥å·¥ä½œå±•ç¤ºäº†ä¸€ç§æ•´åˆå¤šæ ·åŒ–å®éªŒä¿¡å·çš„é€šç”¨ç­–ç•¥ï¼Œä¸ºè¶…è¶Šå¾®åˆ†é™åˆ¶çš„è‡ªåŠ¨åŒ–ã€æ•°æ®æ¡ä»¶åŒ–(data-conditioned)è›‹ç™½è´¨å»ºæ¨¡å¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13244v1",
      "published_date": "2025-11-17 11:07:49 UTC",
      "updated_date": "2025-11-17 11:07:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:02:19.339923+00:00"
    },
    {
      "arxiv_id": "2511.13243v1",
      "title": "Uncovering and Mitigating Transient Blindness in Multimodal Model Editing",
      "title_zh": "æ­ç¤ºä¸ç¼“è§£å¤šæ¨¡æ€æ¨¡å‹ç¼–è¾‘ä¸­çš„ç¬æ€å¤±æ˜",
      "authors": [
        "Xiaoqi Han",
        "Ru Li",
        "Ran Yi",
        "Hongye Tan",
        "Zhuomin Liang",
        "VÃ­ctor GutiÃ©rrez-Basulto",
        "Jeff Z. Pan"
      ],
      "abstract": "Multimodal Model Editing (MMED) aims to correct erroneous knowledge in multimodal models. Existing evaluation methods, adapted from textual model editing, overstate success by relying on low-similarity or random inputs, obscure overfitting. We propose a comprehensive locality evaluation framework, covering three key dimensions: random-image locality, no-image locality, and consistent-image locality, operationalized through seven distinct data types, enabling a detailed and structured analysis of multimodal edits. We introduce De-VQA, a dynamic evaluation for visual question answering, uncovering a phenomenon we term transient blindness, overfitting to edit-similar text while ignoring visuals. Token analysis shows edits disproportionately affect textual tokens. We propose locality-aware adversarial losses to balance cross-modal representations. Empirical results demonstrate that our approach consistently outperforms existing baselines, reducing transient blindness and improving locality by 17% on average.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€æ¨¡å‹ç¼–è¾‘ (Multimodal Model Editing) ä¸­ç°æœ‰è¯„ä¼°æ–¹æ³•é«˜ä¼°æ•ˆæœå¹¶æ©ç›–è¿‡æ‹Ÿåˆçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªæ¶µç›–éšæœºå›¾åƒã€æ— å›¾åƒå’Œä¸€è‡´æ€§å›¾åƒå±€éƒ¨æ€§çš„å…¨é¢è¯„ä¼°æ¡†æ¶ã€‚é€šè¿‡å¼•å…¥åŠ¨æ€è§†è§‰é—®ç­”è¯„ä¼° De-VQAï¼Œç ”ç©¶å‘ç°äº†ä¸€ç§è¢«ç§°ä¸ºâ€œç¬æ—¶ç›²è§†â€ (Transient Blindness) çš„ç°è±¡ï¼Œå³æ¨¡å‹åœ¨ç¼–è¾‘åå€¾å‘äºè¿‡åº¦æ‹Ÿåˆç›¸ä¼¼æ–‡æœ¬è€Œå¿½ç•¥è§†è§‰è¾“å…¥ã€‚Token åˆ†ææ˜¾ç¤ºç¼–è¾‘å¯¹æ–‡æœ¬ Token çš„å½±å“è¿œè¶…è§†è§‰ï¼Œä¸ºæ­¤ä½œè€…æå‡ºäº†å±€éƒ¨æ€§æ„ŸçŸ¥å¯¹æŠ—æŸå¤± (Locality-aware Adversarial Losses) æ¥å¹³è¡¡è·¨æ¨¡æ€è¡¨ç¤ºã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç¼“è§£â€œç¬æ—¶ç›²è§†â€çš„åŒæ—¶ï¼Œå°†å±€éƒ¨æ€§ (Locality) å¹³å‡æå‡äº† 17%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at AAAI'26",
      "pdf_url": "https://arxiv.org/pdf/2511.13243v1",
      "published_date": "2025-11-17 11:04:33 UTC",
      "updated_date": "2025-11-17 11:04:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:02:02.137421+00:00"
    },
    {
      "arxiv_id": "2511.13238v1",
      "title": "Computational Measurement of Political Positions: A Review of Text-Based Ideal Point Estimation Algorithms",
      "title_zh": "æ”¿æ²»ç«‹åœºçš„è®¡ç®—æµ‹é‡ï¼šåŸºäºæ–‡æœ¬çš„ç†æƒ³ç‚¹ä¼°è®¡ç®—æ³•ç»¼è¿°",
      "authors": [
        "Patrick Parschan",
        "Charlott Jakob"
      ],
      "abstract": "This article presents the first systematic review of unsupervised and semi-supervised computational text-based ideal point estimation (CT-IPE) algorithms, methods designed to infer latent political positions from textual data. These algorithms are widely used in political science, communication, computational social science, and computer science to estimate ideological preferences from parliamentary speeches, party manifestos, and social media. Over the past two decades, their development has closely followed broader NLP trends -- beginning with word-frequency models and most recently turning to large language models (LLMs). While this trajectory has greatly expanded the methodological toolkit, it has also produced a fragmented field that lacks systematic comparison and clear guidance for applied use. To address this gap, we identified 25 CT-IPE algorithms through a systematic literature review and conducted a manual content analysis of their modeling assumptions and development contexts. To compare them meaningfully, we introduce a conceptual framework that distinguishes how algorithms generate, capture, and aggregate textual variance. On this basis, we identify four methodological families -- word-frequency, topic modeling, word embedding, and LLM-based approaches -- and critically assess their assumptions, interpretability, scalability, and limitations. Our review offers three contributions. First, it provides a structured synthesis of two decades of algorithm development, clarifying how diverse methods relate to one another. Second, it translates these insights into practical guidance for applied researchers, highlighting trade-offs in transparency, technical requirements, and validation strategies that shape algorithm choice. Third, it emphasizes that differences in estimation outcomes across algorithms are themselves informative, underscoring the need for systematic benchmarking.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹æ— ç›‘ç£å’ŒåŠç›‘ç£çš„è®¡ç®—æ–‡æœ¬ç†æƒ³ç‚¹ä¼°ç®—(CT-IPE)ç®—æ³•è¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿæ€§ç»¼è¿°ï¼Œè¿™ç±»ç®—æ³•è¢«å¹¿æ³›ç”¨äºä»è®®ä¼šæ¼”è®²ã€æ”¿å…šå®£è¨€å’Œç¤¾äº¤åª’ä½“æ•°æ®ä¸­æ¨æ–­æ½œè—çš„æ”¿æ²»ç«‹åœºã€‚æ–‡ç« å›é¡¾äº†ä»æ—©æœŸè¯é¢‘æ¨¡å‹åˆ°æœ€æ–°å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ¼”è¿›è½¨è¿¹ï¼Œé€šè¿‡ç³»ç»Ÿæ–‡çŒ®å›é¡¾è¯†åˆ«å‡º25ç§æ ¸å¿ƒç®—æ³•ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåŒºåˆ†ç®—æ³•å¦‚ä½•ç”Ÿæˆã€æ•æ‰å’Œèšåˆæ–‡æœ¬æ–¹å·®çš„æ¦‚å¿µæ¡†æ¶ã€‚åŸºäºæ­¤æ¡†æ¶ï¼Œç ”ç©¶å°†ç®—æ³•åˆ’åˆ†ä¸ºè¯é¢‘ã€ä¸»é¢˜å»ºæ¨¡(topic modeling)ã€è¯åµŒå…¥(word embedding)å’ŒåŸºäºLLMçš„å››å¤§å®¶æ—ï¼Œå¹¶å¯¹å…¶å»ºæ¨¡å‡è®¾ã€å¯è§£é‡Šæ€§åŠå±€é™æ€§è¿›è¡Œäº†æ·±å…¥çš„æ‰¹åˆ¤æ€§è¯„ä¼°ã€‚è¯¥ç ”ç©¶ä¸ä»…ä¸ºæ”¿æ²»å­¦å’Œè®¡ç®—ç¤¾ä¼šç§‘å­¦é¢†åŸŸæä¾›äº†è·¨è¶ŠäºŒåå¹´çš„ç»“æ„åŒ–æ–¹æ³•ç»¼è¿°ï¼Œè¿˜ä¸ºåº”ç”¨ç ”ç©¶è€…åœ¨é€æ˜åº¦è¦æ±‚ã€æŠ€æœ¯é—¨æ§›å’ŒéªŒè¯ç­–ç•¥ç­‰ç»´åº¦çš„ç®—æ³•é€‰æ‹©æä¾›äº†å®æ“æŒ‡å—ã€‚æœ€åï¼Œä½œè€…å¼ºè°ƒäº†ä¸åŒç®—æ³•é—´ä¼°ç®—åå·®çš„ç§‘ç ”ä»·å€¼ï¼Œå‘¼åå¼€å±•ç³»ç»Ÿæ€§çš„åŸºå‡†æµ‹è¯•(benchmarking)ä»¥è¿›ä¸€æ­¥å®Œå–„è¯¥é¢†åŸŸçš„æ–¹æ³•è®ºä½“ç³»ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "46 pages, 8 figures, 2 tables, accepted for publication in Quality & Quantity",
      "pdf_url": "https://arxiv.org/pdf/2511.13238v1",
      "published_date": "2025-11-17 11:01:09 UTC",
      "updated_date": "2025-11-17 11:01:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:03:15.039891+00:00"
    },
    {
      "arxiv_id": "2511.13226v1",
      "title": "Informative Communication of Robot Plans",
      "title_zh": "æœºå™¨äººè§„åˆ’çš„é«˜ä¿¡æ¯é‡ä¼ è¾¾",
      "authors": [
        "Michele Persiani",
        "Thomas Hellstrom"
      ],
      "abstract": "When a robot is asked to verbalize its plan it can do it in many ways. For example, a seemingly natural strategy is incremental, where the robot verbalizes its planned actions in plan order. However, an important aspect of this type of strategy is that it misses considerations on what is effectively informative to communicate, because not considering what the user knows prior to explanations. In this paper we propose a verbalization strategy to communicate robot plans informatively, by measuring the information gain that verbalizations have against a second-order theory of mind of the user capturing his prior knowledge on the robot. As shown in our experiments, this strategy allows to understand the robot's goal much quicker than by using strategies such as increasing or decreasing plan order. In addition, following our formulation we hint to what is informative and why when a robot communicates its plan.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººå¦‚ä½•æœ‰æ•ˆåœ°å‘ç”¨æˆ·ä¼ è¾¾å…¶è¡ŒåŠ¨è®¡åˆ’(Robot Plans)çš„é—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„æŒ‰è®¡åˆ’é¡ºåºæè¿°è¡ŒåŠ¨çš„å¢é‡ç­–ç•¥(incremental strategy)å¾€å¾€å¿½è§†äº†ç”¨æˆ·å·²æœ‰çš„çŸ¥è¯†èƒŒæ™¯ï¼Œå¯¼è‡´æ²Ÿé€šæ•ˆç‡ä½ä¸‹ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§å…¨æ–°çš„ä¿¡æ¯åŒ–è¯­è¨€æè¿°ç­–ç•¥(verbalization strategy)ï¼Œé€šè¿‡è¡¡é‡è¯­éŸ³æè¿°ç›¸å¯¹äºç”¨æˆ·çš„äºŒé˜¶å¿ƒç†ç†è®º(second-order theory of mind)æ‰€äº§ç”Ÿçš„ä¿¡æ¯å¢ç›Š(information gain)æ¥ä¼˜åŒ–æ²Ÿé€šå†…å®¹ã€‚è¯¥æ–¹æ³•å……åˆ†è€ƒè™‘äº†ç”¨æˆ·å¯¹æœºå™¨äººçš„å…ˆéªŒçŸ¥è¯†ï¼Œæ—¨åœ¨å®ç°æ›´å…·å¯å‘æ€§çš„ä¿¡æ¯ä¼ è¾¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„æŒ‰è®¡åˆ’é¡ºåºé€’å¢æˆ–é€’å‡çš„æè¿°ç­–ç•¥ç›¸æ¯”ï¼Œè¯¥ç­–ç•¥èƒ½è®©ç”¨æˆ·æ˜¾è‘—æ›´å¿«åœ°ç†è§£æœºå™¨äººçš„æœ€ç»ˆç›®æ ‡ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶é€šè¿‡å…¬å¼åŒ–å®šä¹‰ï¼Œæ­ç¤ºäº†æœºå™¨äººä¼ è¾¾è®¡åˆ’æ—¶â€œä½•ä¸ºæœ‰æ•ˆä¿¡æ¯â€ä»¥åŠâ€œä¸ºä½•æœ‰æ•ˆâ€çš„æ·±å±‚é€»è¾‘ï¼Œä¸ºæå‡äººæœºäº¤äº’çš„é€æ˜åº¦æä¾›äº†ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Conference: PAAMS 2022, 20th International Conference on Practical Applications of Agents and Multi-Agent Systems",
      "pdf_url": "https://arxiv.org/pdf/2511.13226v1",
      "published_date": "2025-11-17 10:44:25 UTC",
      "updated_date": "2025-11-17 10:44:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:02:09.638844+00:00"
    },
    {
      "arxiv_id": "2511.13223v1",
      "title": "TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs",
      "title_zh": "TokenSqueezeï¼šé¢å‘æ¨ç†å‹å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½æ— æŸå‹ç¼©",
      "authors": [
        "Yuxiang Zhang",
        "Zhengxu Yu",
        "Weihang Pan",
        "Zhongming Jin",
        "Qiang Fu",
        "Deng Cai",
        "Binbin Lin",
        "Jieping Ye"
      ],
      "abstract": "Emerging reasoning LLMs such as OpenAI-o1 and DeepSeek-R1 have achieved strong performance on complex reasoning tasks by generating long chain-of-thought (CoT) traces. However, these long CoTs result in increased token usage, leading to higher inference latency and memory consumption. As a result, balancing accuracy and reasoning efficiency has become essential for deploying reasoning LLMs in practical applications. Existing long-to-short (Long2Short) methods aim to reduce inference length but often sacrifice accuracy, revealing a need for an approach that maintains performance while lowering token costs. To address this efficiency-accuracy tradeoff, we propose TokenSqueeze, a novel Long2Short method that condenses reasoning paths while preserving performance and relying exclusively on self-generated data. First, to prevent performance degradation caused by excessive compression of reasoning depth, we propose to select self-generated samples whose reasoning depth is adaptively matched to the complexity of the problem. To further optimize the linguistic expression without altering the underlying reasoning paths, we introduce a distribution-aligned linguistic refinement method that enhances the clarity and conciseness of the reasoning path while preserving its logical integrity. Comprehensive experimental results demonstrate the effectiveness of TokenSqueeze in reducing token usage while maintaining accuracy. Notably, DeepSeek-R1-Distill-Qwen-7B fine-tuned using our proposed method achieved a 50\\% average token reduction while preserving accuracy on the MATH500 benchmark. TokenSqueeze exclusively utilizes the model's self-generated data, enabling efficient and high-fidelity reasoning without relying on manually curated short-answer datasets across diverse applications. Our code is available at https://github.com/zhangyx1122/TokenSqueeze.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TokenSqueezeï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹æ¨ç†å¤§è¯­è¨€æ¨¡å‹ï¼ˆReasoning LLMsï¼‰çš„Long2Shortå‹ç¼©æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³é•¿é“¾å¼æ€ç»´ï¼ˆChain-of-Thought, CoTï¼‰å¸¦æ¥çš„é«˜æ¨ç†å»¶è¿Ÿå’Œå†…å­˜æ¶ˆè€—é—®é¢˜ã€‚ä¸ºäº†å¹³è¡¡å‡†ç¡®æ€§ä¸æ¨ç†æ•ˆç‡ï¼ŒTokenSqueezeé€šè¿‡æ ¹æ®é—®é¢˜å¤æ‚åº¦è‡ªé€‚åº”åŒ¹é…æ¨ç†æ·±åº¦ï¼Œæœ‰æ•ˆé˜²æ­¢äº†è¿‡åº¦å‹ç¼©å¯¼è‡´çš„æ€§èƒ½é€€åŒ–ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•å¼•å…¥äº†åˆ†å¸ƒå¯¹é½çš„è¯­è¨€ç²¾ç‚¼æ‰‹æ®µï¼ˆdistribution-aligned linguistic refinementï¼‰ï¼Œåœ¨ä¿æŒé€»è¾‘å®Œæ•´æ€§çš„å‰æä¸‹ä¼˜åŒ–è¯­è¨€è¡¨è¾¾ï¼Œä½¿æ¨ç†è·¯å¾„æ›´åŠ ç®€æ´æ˜äº†ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æ˜¾è‘—å‡å°‘Tokenä½¿ç”¨é‡ï¼Œå…¶ä¸­å¾®è°ƒåçš„DeepSeek-R1-Distill-Qwen-7Båœ¨MATH500åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†50%çš„å¹³å‡Tokenå‡é‡ä¸”ä¿æŒäº†åŸæœ‰å‡†ç¡®ç‡ã€‚TokenSqueezeå®Œå…¨åˆ©ç”¨æ¨¡å‹è‡ªç”Ÿæˆæ•°æ®å®ç°é«˜æ•ˆæ¨ç†ï¼Œæ— éœ€ä¾èµ–äººå·¥æ•´ç†çš„çŸ­ç­”æ¡ˆæ•°æ®é›†ï¼Œä¸ºæ¨ç†æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„éƒ¨ç½²æä¾›äº†é«˜ä¿çœŸåº¦çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.13223v1",
      "published_date": "2025-11-17 10:38:56 UTC",
      "updated_date": "2025-11-17 10:38:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:02:49.650795+00:00"
    },
    {
      "arxiv_id": "2511.13219v2",
      "title": "FoleyBench: A Benchmark For Video-to-Audio Models",
      "title_zh": "FoleyBenchï¼šé¢å‘è§†é¢‘è½¬éŸ³é¢‘æ¨¡å‹çš„è¯„æµ‹åŸºå‡†",
      "authors": [
        "Satvik Dixit",
        "Koichi Saito",
        "Zhi Zhong",
        "Yuki Mitsufuji",
        "Chris Donahue"
      ],
      "abstract": "Video-to-audio generation (V2A) is of increasing importance in domains such as film post-production, AR/VR, and sound design, particularly for the creation of Foley sound effects synchronized with on-screen actions. Foley requires generating audio that is both semantically aligned with visible events and temporally aligned with their timing. Yet, there is a mismatch between evaluation and downstream applications due to the absence of a benchmark tailored to Foley-style scenarios. We find that 74% of videos from past evaluation datasets have poor audio-visual correspondence. Moreover, they are dominated by speech and music, domains that lie outside the use case for Foley. To address this gap, we introduce FoleyBench, the first large-scale benchmark explicitly designed for Foley-style V2A evaluation. FoleyBench contains 5,000 (video, ground-truth audio, text caption) triplets, each featuring visible sound sources with audio causally tied to on-screen events. The dataset is built using an automated, scalable pipeline applied to in-the-wild internet videos from YouTube-based and Vimeo-based sources. Compared to past datasets, we show that videos from FoleyBench have stronger coverage of sound categories from a taxonomy specifically designed for Foley sound. Each clip is further labeled with metadata capturing source complexity, UCS/AudioSet category, and video length, enabling fine-grained analysis of model performance and failure modes. We benchmark several state-of-the-art V2A models, evaluating them on audio quality, audio-video alignment, temporal synchronization, and audio-text consistency. Samples are available at: https://gclef-cmu.org/foleybench",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†FoleyBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨ä¸ºæ‹ŸéŸ³é£æ ¼(Foley-style)è§†é¢‘è½¬éŸ³é¢‘(Video-to-Audio, V2A)è¯„ä¼°è®¾è®¡çš„å¤§è§„æ¨¡åŸºå‡†ã€‚é’ˆå¯¹ç°æœ‰æ•°æ®é›†éŸ³è§†é¢‘å¯¹åº”æ€§å·®ä¸”å¤šç”±è¯­è¨€å’ŒéŸ³ä¹ä¸»å¯¼çš„å±€é™æ€§ï¼ŒFoleyBenchåˆ©ç”¨è‡ªåŠ¨åŒ–æµæ°´çº¿ä»äº’è”ç½‘è§†é¢‘ä¸­ç­›é€‰å¹¶æ„å»ºäº†åŒ…å«5,000ç»„è§†é¢‘ã€éŸ³é¢‘åŠæ–‡æœ¬è¯´æ˜çš„ä¸‰å…ƒç»„ã€‚æ•°æ®é›†ä¸­çš„éŸ³é¢‘ä¸å±å¹•è§†è§‰äº‹ä»¶å…·æœ‰æ˜ç¡®çš„å› æœå…³ç³»ï¼Œå¹¶é‡‡ç”¨äº†ä¸“é—¨ä¸ºæ‹ŸéŸ³è®¾è®¡çš„åˆ†ç±»ä½“ç³»ã€‚æ­¤å¤–ï¼Œæ¯ä¸ªç‰‡æ®µéƒ½å¸¦æœ‰æ¥æºå¤æ‚åº¦ã€UCS/AudioSetç±»åˆ«åŠè§†é¢‘é•¿åº¦ç­‰å…ƒæ•°æ®ï¼Œæ”¯æŒå¯¹æ¨¡å‹æ€§èƒ½è¿›è¡Œç»†ç²’åº¦çš„æ•…éšœæ¨¡å¼åˆ†æã€‚é€šè¿‡å¯¹å¤šä¸ªæœ€å…ˆè¿›(SOTA)çš„V2Aæ¨¡å‹åœ¨éŸ³è§†é¢‘å¯¹é½ã€æ—¶é—´åŒæ­¥å’ŒéŸ³é¢‘è´¨é‡ç­‰ç»´åº¦è¿›è¡Œæµ‹è¯„ï¼Œè¯¥åŸºå‡†ä¸ºç”µå½±åæœŸã€è™šæ‹Ÿç°å®(AR/VR)åŠéŸ³æ•ˆè®¾è®¡é¢†åŸŸçš„ç ”ç©¶æä¾›äº†é‡è¦çš„æ”¯æ’‘å·¥å…·ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13219v2",
      "published_date": "2025-11-17 10:34:59 UTC",
      "updated_date": "2025-11-24 04:08:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:02:25.538044+00:00"
    },
    {
      "arxiv_id": "2511.13214v1",
      "title": "Learning to Solve Resource-Constrained Project Scheduling Problems with Duration Uncertainty using Graph Neural Networks",
      "title_zh": "åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œå­¦ä¹ æ±‚è§£å·¥æœŸå…·æœ‰ä¸ç¡®å®šæ€§çš„èµ„æºå—é™é¡¹ç›®è°ƒåº¦é—®é¢˜",
      "authors": [
        "Guillaume Infantes",
        "StÃ©phanie Roussel",
        "Antoine Jacquet",
        "Emmanuel Benazera"
      ],
      "abstract": "The Resource-Constrained Project Scheduling Problem (RCPSP) is a classical scheduling problem that has received significant attention due to of its numerous applications in industry. However, in practice, task durations are subject to uncertainty that must be considered in order to propose resilient scheduling. In this paper, we address the RCPSP variant with uncertain tasks duration (modeled using known probabilities) and aim to minimize the overall expected project duration. Our objective is to produce a baseline schedule that can be reused multiple times in an industrial setting regardless of the actual duration scenario. We leverage Graph Neural Networks in conjunction with Deep Reinforcement Learning (DRL) to develop an effective policy for task scheduling. This policy operates similarly to a priority dispatch rule and is paired with a Serial Schedule Generation Scheme to produce a schedule. Our empirical evaluation on standard benchmarks demonstrates the approach's superiority in terms of performance and its ability to generalize. The developed framework, Wheatley, is made publicly available online to facilitate further research and reproducibility.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å…·æœ‰å·¥æœŸä¸ç¡®å®šæ€§(Duration Uncertainty)çš„èµ„æºçº¦æŸé¡¹ç›®è°ƒåº¦é—®é¢˜(RCPSP)ï¼Œæ—¨åœ¨æœ€å°åŒ–å·¥æœŸçš„æœŸæœ›å€¼ã€‚ä¸ºäº†åœ¨å·¥ä¸šåœºæ™¯ä¸­åˆ¶å®šå¯é‡å¤ä½¿ç”¨çš„åŸºå‡†è°ƒåº¦æ–¹æ¡ˆï¼Œä½œè€…ç»“åˆå›¾ç¥ç»ç½‘ç»œ(Graph Neural Networks, GNNs)ä¸æ·±åº¦å¼ºåŒ–å­¦ä¹ (Deep Reinforcement Learning, DRL)å¼€å‘äº†ä¸€ç§é«˜æ•ˆçš„ä»»åŠ¡è°ƒåº¦ç­–ç•¥ã€‚è¯¥ç­–ç•¥ä½œä¸ºä¸€ç§ä¼˜å…ˆè°ƒåº¦è§„åˆ™(Priority Dispatch Rule)ï¼Œå¹¶é…åˆä¸²è¡Œè¿›åº¦ç”Ÿæˆæ–¹æ¡ˆ(Serial Schedule Generation Scheme)æ¥ç”Ÿæˆæœ€ç»ˆè¿›åº¦è¡¨ã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯„ä¼°è¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢å‡å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿå…¬å¼€äº†åä¸ºWheatleyçš„æ¡†æ¶ï¼Œä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ä¸ç»“æœå¤ç°ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at ICTAI 2025 Conference",
      "pdf_url": "https://arxiv.org/pdf/2511.13214v1",
      "published_date": "2025-11-17 10:27:35 UTC",
      "updated_date": "2025-11-17 10:27:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:02:39.739370+00:00"
    },
    {
      "arxiv_id": "2511.13198v1",
      "title": "ParaDySe: A Parallel-Strategy Switching Framework for Dynamic Sequence Lengths in Transformer",
      "title_zh": "ParaDySeï¼šé¢å‘ Transformer åŠ¨æ€åºåˆ—é•¿åº¦çš„å¹¶è¡Œç­–ç•¥åˆ‡æ¢æ¡†æ¶",
      "authors": [
        "Zhixin Ou",
        "Peng Liang",
        "Jianchen Han",
        "Baihui Liu",
        "Linbo Qiao"
      ],
      "abstract": "Dynamic sequences with varying lengths have been widely used in the training of Transformer-based large language models (LLMs). However, current training frameworks adopt a pre-defined static parallel strategy for these sequences, causing neither communication-parallelization cancellation on short sequences nor out-of-memory on long sequences. To mitigate these issues, we propose ParaDySe, a novel adaptive Parallel strategy switching framework for Dynamic Sequences. ParaDySe enables on-the-fly optimal strategy adoption according to the immediate input sequence. It first implements the modular function libraries for parallel strategies with unified tensor layout specifications, and then builds sequence-aware memory and time cost models with hybrid methods. Guided by cost models, ParaDySe selects optimal layer-wise strategies for dynamic sequences via an efficient heuristic algorithm. By integrating these techniques together, ParaDySe achieves seamless hot-switching of optimal strategies through its well-designed function libraries. We compare ParaDySe with baselines on representative LLMs under datasets with sequence lengths up to 624K. Experimental results indicate that ParaDySe addresses OOM and CPC bottlenecks in LLM training by systematically integrating long-sequence optimizations with existing frameworks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ParaDySeï¼Œä¸€ç§é’ˆå¯¹ Transformer æ¨¡å‹ä¸­åŠ¨æ€åºåˆ—é•¿åº¦ï¼ˆDynamic Sequence Lengthsï¼‰çš„è‡ªé€‚åº”å¹¶è¡Œç­–ç•¥åˆ‡æ¢æ¡†æ¶ã€‚é’ˆå¯¹å½“å‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è®­ç»ƒä¸­é™æ€å¹¶è¡Œç­–ç•¥å¯¼è‡´çŸ­åºåˆ—å‡ºç°é€šä¿¡å¹¶è¡ŒåŒ–æŠµæ¶ˆï¼ˆCPCï¼‰ä»¥åŠé•¿åºåˆ—å‘ç”Ÿæ˜¾å­˜æº¢å‡ºï¼ˆOOMï¼‰çš„é—®é¢˜ï¼ŒParaDySe èƒ½å¤Ÿæ ¹æ®è¾“å…¥åºåˆ—çš„å®æ—¶é•¿åº¦åŠ¨æ€é‡‡ç”¨æœ€ä¼˜ç­–ç•¥ã€‚è¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡ç»Ÿä¸€çš„å¼ é‡å¸ƒå±€ï¼ˆtensor layoutï¼‰è§„èŒƒæ„å»ºäº†æ¨¡å—åŒ–çš„å¹¶è¡Œç­–ç•¥åŠŸèƒ½åº“ï¼Œå¹¶ç»“åˆæ··åˆæ–¹æ³•å»ºç«‹äº†æ„ŸçŸ¥åºåˆ—çš„æ˜¾å­˜ä¸æ—¶é—´æˆæœ¬æ¨¡å‹ã€‚åœ¨æˆæœ¬æ¨¡å‹çš„æŒ‡å¯¼ä¸‹ï¼ŒParaDySe åˆ©ç”¨é«˜æ•ˆçš„å¯å‘å¼ç®—æ³•ä¸ºåŠ¨æ€åºåˆ—é€‰æ‹©æœ€ä¼˜çš„é€å±‚ç­–ç•¥ï¼Œå®ç°äº†ç­–ç•¥çš„æ— ç¼çƒ­åˆ‡æ¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒParaDySe åœ¨å¤„ç†é•¿è¾¾ 624K çš„åºåˆ—æ—¶ï¼Œèƒ½ç³»ç»Ÿæ€§åœ°è§£å†³è®­ç»ƒä¸­çš„æ˜¾å­˜ç“¶é¢ˆå’Œæ•ˆç‡é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†é•¿åºåˆ—å¤§æ¨¡å‹çš„è®­ç»ƒæ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13198v1",
      "published_date": "2025-11-17 10:08:24 UTC",
      "updated_date": "2025-11-17 10:08:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:03:30.331414+00:00"
    },
    {
      "arxiv_id": "2512.00045v1",
      "title": "Assessing Large Language Models in Generating RTL Design Specifications",
      "title_zh": "è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨ RTL è®¾è®¡è§„èŒƒç”Ÿæˆä¸­çš„è¡¨ç°",
      "authors": [
        "Hung-Ming Huang",
        "Yu-Hsin Yang",
        "Fu-Chieh Chang",
        "Yun-Chia Hsu",
        "Yin-Yu Lin",
        "Ming-Fang Tsai",
        "Chun-Chih Yang",
        "Pei-Yuan Wu"
      ],
      "abstract": "As IC design grows more complex, automating comprehension and documentation of RTL code has become increasingly important. Engineers currently should manually interpret existing RTL code and write specifications, a slow and error-prone process. Although LLMs have been studied for generating RTL from specifications, automated specification generation remains underexplored, largely due to the lack of reliable evaluation methods. To address this gap, we investigate how prompting strategies affect RTL-to-specification quality and introduce metrics for faithfully evaluating generated specs. We also benchmark open-source and commercial LLMs, providing a foundation for more automated and efficient specification workflows in IC design.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¾…åŠ©ä¸‹ä»å¯„å­˜å™¨ä¼ è¾“çº§ï¼ˆRTLï¼‰ä»£ç è‡ªåŠ¨ç”Ÿæˆè®¾è®¡è§„æ ¼è¯´æ˜ä¹¦çš„å¯è¡Œæ€§ï¼Œæ—¨åœ¨è§£å†³é›†æˆç”µè·¯è®¾è®¡ï¼ˆIC designï¼‰ä¸­æ‰‹åŠ¨ç¼–å†™æ–‡æ¡£è€—æ—¶ä¸”æ˜“å‡ºé”™çš„æŒ‘æˆ˜ã€‚ç”±äºç›®å‰è¯¥é¢†åŸŸç¼ºä¹å¯é çš„è¯„ä¼°æ–¹æ³•ï¼Œç ”ç©¶é‡ç‚¹è°ƒæŸ¥äº†ä¸åŒæç¤ºç­–ç•¥ï¼ˆprompting strategiesï¼‰å¯¹ç”Ÿæˆè´¨é‡çš„å½±å“ï¼Œå¹¶å¼•å…¥äº†èƒ½å¤Ÿå¿ å®åæ˜ è§„æ ¼è¯´æ˜ä¹¦å‡†ç¡®æ€§çš„è¯„ä¼°æŒ‡æ ‡ã€‚é€šè¿‡å¯¹å¤šç§å¼€æºå’Œå•†ç”¨LLMsè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œè¯¥ç ”ç©¶è¯„ä¼°äº†å½“å‰æ¨¡å‹åœ¨å¤„ç†RTLç†è§£ä¸æ–‡æ¡£åŒ–ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¿™é¡¹å·¥ä½œä¸ä»…å¡«è¡¥äº†è‡ªåŠ¨ç”Ÿæˆè§„æ ¼è¯´æ˜ä¹¦è¯„ä¼°ä½“ç³»çš„ç©ºç™½ï¼Œè¿˜ä¸ºå®ç°æ›´è‡ªåŠ¨åŒ–ã€æ›´é«˜æ•ˆçš„IC designå·¥ä½œæµå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00045v1",
      "published_date": "2025-11-17 10:06:24 UTC",
      "updated_date": "2025-11-17 10:06:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:03:32.533223+00:00"
    },
    {
      "arxiv_id": "2511.13193v1",
      "title": "Cost-Effective Communication: An Auction-based Method for Language Agent Interaction",
      "title_zh": "æˆæœ¬æ•ˆç›Šå‹é€šä¿¡ï¼šä¸€ç§åŸºäºæ‹å–æœºåˆ¶çš„è¯­è¨€æ™ºèƒ½ä½“äº¤äº’æ–¹æ³•",
      "authors": [
        "Yijia Fan",
        "Jusheng Zhang",
        "Kaitong Cai",
        "Jing Yang",
        "Chengpei Tang",
        "Jian Wang",
        "Keze Wang"
      ],
      "abstract": "Multi-agent systems (MAS) built on large language models (LLMs) often suffer from inefficient \"free-for-all\" communication, leading to exponential token costs and low signal-to-noise ratios that hinder their practical deployment. We challenge the notion that more communication is always beneficial, hypothesizing instead that the core issue is the absence of resource rationality. We argue that \"free\" communication, by ignoring the principle of scarcity, inherently breeds inefficiency and unnecessary expenses. To address this, we introduce the Dynamic Auction-based Language Agent (DALA), a novel framework that treats communication bandwidth as a scarce and tradable resource. Specifically, our DALA regards inter-agent communication as a centralized auction, where agents learn to bid for the opportunity to speak based on the predicted value density of their messages. Thus, our DALA intrinsically encourages agents to produce concise, informative messages while filtering out low-value communication. Extensive and comprehensive experiments demonstrate that our economically-driven DALA achieves new state-of-the-art performance across seven challenging reasoning benchmarks, including 84.32% on MMLU and a 91.21% pass@1 rate on HumanEval. Note that this is accomplished with remarkable efficiency, i.e., our DALA uses only 6.25 million tokens, a fraction of the resources consumed by current state-of-the-art methods on GSM8K. Further analysis reveals that our DALA cultivates the emergent skill of strategic silence, effectively adapting its communication strategies from verbosity to silence in a dynamical manner via resource constraints.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹(Large Language Models)çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(Multi-agent systems)ä¸­å­˜åœ¨çš„é€šä¿¡æ•ˆç‡ä½ä¸‹ã€Tokenæˆæœ¬é«˜æ˜‚ä»¥åŠä¿¡å™ªæ¯”ä½ç­‰ç°å®æŒ‘æˆ˜ï¼ŒæŒ‘æˆ˜äº†â€œé€šä¿¡è¶Šå¤šè¶Šå¥½â€çš„ä¼ ç»Ÿè§‚å¿µï¼Œå¹¶å¼•å…¥äº†èµ„æºç†æ€§(resource rationality)çš„æ¦‚å¿µã€‚ç ”ç©¶æå‡ºäº†åŠ¨æ€åŸºäºæ‹å–çš„è¯­è¨€æ™ºèƒ½ä½“æ¡†æ¶(Dynamic Auction-based Language Agent, DALA)ï¼Œå°†é€šä¿¡å¸¦å®½è§†ä¸ºä¸€ç§ç¨€ç¼ºä¸”å¯äº¤æ˜“çš„èµ„æºã€‚DALAå°†æ™ºèƒ½ä½“é—´çš„é€šä¿¡æ¨¡æ‹Ÿä¸ºä¸­å¿ƒåŒ–æ‹å–è¿‡ç¨‹ï¼Œæ™ºèƒ½ä½“æ ¹æ®å…¶æ¶ˆæ¯çš„é¢„æµ‹ä»·å€¼å¯†åº¦ç«æ ‡å‘è¨€æƒï¼Œä»è€Œé¼“åŠ±ç”Ÿæˆç®€æ´é«˜æ•ˆçš„ä¿¡æ¯å¹¶è¿‡æ»¤ä½ä»·å€¼é€šä¿¡ã€‚å®éªŒè¡¨æ˜ï¼ŒDALAåœ¨MMLU(84.32%)å’ŒHumanEval(91.21% pass@1)ç­‰ä¸ƒé¡¹åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æ–°çš„SOTAæ€§èƒ½ï¼Œä¸”Tokenæ¶ˆè€—é‡ä»…ä¸º625ä¸‡ï¼Œè¿œä½äºç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚æ­¤å¤–ï¼Œåˆ†æå‘ç°DALAé€šè¿‡èµ„æºçº¦æŸä½¿æ™ºèƒ½ä½“æ¶Œç°å‡ºâ€œæˆ˜ç•¥æ€§æ²‰é»˜â€(strategic silence)çš„æŠ€èƒ½ï¼Œèƒ½å¤ŸåŠ¨æ€åœ°æ ¹æ®ç¯å¢ƒè°ƒæ•´æ²Ÿé€šç­–ç•¥ä»¥å®ç°æˆæœ¬æ•ˆç›Šçš„æœ€å¤§åŒ–ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13193v1",
      "published_date": "2025-11-17 10:00:20 UTC",
      "updated_date": "2025-11-17 10:00:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:03:38.438716+00:00"
    },
    {
      "arxiv_id": "2511.13168v1",
      "title": "SOMA: Feature Gradient Enhanced Affine-Flow Matching for SAR-Optical Registration",
      "title_zh": "SOMAï¼šåŸºäºç‰¹å¾æ¢¯åº¦å¢å¼ºä»¿å°„-æµåŒ¹é…çš„SAR-å…‰å­¦å›¾åƒé…å‡†",
      "authors": [
        "Haodong Wang",
        "Tao Zhuo",
        "Xiuwei Zhang",
        "Hanlin Yin",
        "Wencong Wu",
        "Yanning Zhang"
      ],
      "abstract": "Achieving pixel-level registration between SAR and optical images remains a challenging task due to their fundamentally different imaging mechanisms and visual characteristics. Although deep learning has achieved great success in many cross-modal tasks, its performance on SAR-Optical registration tasks is still unsatisfactory. Gradient-based information has traditionally played a crucial role in handcrafted descriptors by highlighting structural differences. However, such gradient cues have not been effectively leveraged in deep learning frameworks for SAR-Optical image matching. To address this gap, we propose SOMA, a dense registration framework that integrates structural gradient priors into deep features and refines alignment through a hybrid matching strategy. Specifically, we introduce the Feature Gradient Enhancer (FGE), which embeds multi-scale, multi-directional gradient filters into the feature space using attention and reconstruction mechanisms to boost feature distinctiveness. Furthermore, we propose the Global-Local Affine-Flow Matcher (GLAM), which combines affine transformation and flow-based refinement within a coarse-to-fine architecture to ensure both structural consistency and local accuracy. Experimental results demonstrate that SOMA significantly improves registration precision, increasing the CMR@1px by 12.29% on the SEN1-2 dataset and 18.50% on the GFGE_SO dataset. In addition, SOMA exhibits strong robustness and generalizes well across diverse scenes and resolutions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ SAR ä¸å…‰å­¦å›¾åƒæˆåƒæœºåˆ¶å·®å¼‚å¯¼è‡´çš„åƒç´ çº§é…å‡†éš¾é¢˜ï¼Œæå‡ºäº† SOMA ç¨ å¯†é…å‡†æ¡†æ¶ï¼Œæ—¨åœ¨å°†ç»“æ„æ¢¯åº¦å…ˆéªŒæœ‰æ•ˆé›†æˆåˆ°æ·±åº¦å­¦ä¹ ç‰¹å¾ä¸­ã€‚è¯¥æ¡†æ¶å¼•å…¥äº† Feature Gradient Enhancer (FGE)ï¼Œåˆ©ç”¨æ³¨æ„åŠ›å’Œé‡æ„æœºåˆ¶å°†å¤šå°ºåº¦ã€å¤šæ–¹å‘çš„æ¢¯åº¦æ»¤æ³¢å™¨åµŒå…¥ç‰¹å¾ç©ºé—´ï¼Œæ˜¾è‘—å¢å¼ºäº†ç‰¹å¾çš„è¾¨è¯†åº¦ã€‚åŒæ—¶ï¼Œç ”ç©¶è®¾è®¡äº† Global-Local Affine-Flow Matcher (GLAM)ï¼Œé€šè¿‡åœ¨ç”±ç²—åˆ°ç²¾çš„æ¶æ„ä¸­ç»“åˆä»¿å°„å˜æ¢ (affine transformation) ä¸æµåœºç»†åŒ– (flow-based refinement)ï¼Œç¡®ä¿äº†å›¾åƒé—´çš„ç»“æ„ä¸€è‡´æ€§ä¸å±€éƒ¨ç²¾åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSOMA åœ¨ SEN1-2 å’Œ GFGE_SO æ•°æ®é›†ä¸Šçš„ CMR@1px åˆ†åˆ«æå‡äº† 12.29% å’Œ 18.50%ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚é…å‡†ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å¤šç§åœºæ™¯å’Œåˆ†è¾¨ç‡ä¸‹å‡è¡¨ç°å‡ºæå¼ºçš„é²æ£’æ€§ (robustness) ä¸æ³›åŒ– (generalization) èƒ½åŠ›ï¼Œä¸ºè·¨æ¨¡æ€é¥æ„Ÿå›¾åƒå¤„ç†æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13168v1",
      "published_date": "2025-11-17 09:14:56 UTC",
      "updated_date": "2025-11-17 09:14:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:03:43.134311+00:00"
    },
    {
      "arxiv_id": "2511.13166v1",
      "title": "Local Collaborative Filtering: A Collaborative Filtering Method that Utilizes Local Similarities among Users",
      "title_zh": "å±€éƒ¨ååŒè¿‡æ»¤ï¼šä¸€ç§åˆ©ç”¨ç”¨æˆ·å±€éƒ¨ç›¸ä¼¼æ€§çš„ååŒè¿‡æ»¤æ–¹æ³•",
      "authors": [
        "Zhaoxin Shen",
        "Dan Wu"
      ],
      "abstract": "To leverage user behavior data from the Internet more effectively in recommender systems, this paper proposes a novel collaborative filtering (CF) method called Local Collaborative Filtering (LCF). LCF utilizes local similarities among users and integrates their data using the law of large numbers (LLN), thereby improving the utilization of user behavior data. Experiments are conducted on the Steam game dataset, and the results of LCF align with real-world needs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¨èç³»ç»Ÿå¦‚ä½•æ›´æœ‰æ•ˆåœ°åˆ©ç”¨äº’è”ç½‘ç”¨æˆ·è¡Œä¸ºæ•°æ®ï¼Œæå‡ºäº†ä¸€ç§åä¸º Local Collaborative Filtering (LCF) çš„æ–°å‹ååŒè¿‡æ»¤æ–¹æ³•ã€‚LCF çš„æ ¸å¿ƒåœ¨äºæŒ–æ˜ç”¨æˆ·ä¹‹é—´çš„å±€éƒ¨ç›¸ä¼¼æ€§ (local similarities)ï¼Œå¹¶åˆ›æ–°æ€§åœ°ç»“åˆå¤§æ•°å®šå¾‹ (Law of Large Numbers, LLN) å¯¹ç”¨æˆ·æ•°æ®è¿›è¡Œæ•´åˆã€‚é€šè¿‡è¿™ç§æœºåˆ¶ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æå‡å¯¹æµ·é‡ç”¨æˆ·è¡Œä¸ºæ•°æ®çš„åˆ©ç”¨æ•ˆç‡ï¼Œä¼˜åŒ–æ¨èè´¨é‡ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨ Steam æ¸¸æˆæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œç»“æœæ˜¾ç¤º LCF çš„è¡¨ç°ä¸ç°å®ä¸–ç•Œçš„éœ€æ±‚é«˜åº¦å¥‘åˆã€‚å®éªŒè¯æ˜äº† LCF åœ¨å¤„ç†å¤æ‚ç”¨æˆ·è¡Œä¸ºæ•°æ®æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºååŒè¿‡æ»¤é¢†åŸŸæä¾›äº†ä¸€ä¸ªåŸºäºç»Ÿè®¡å­¦è§„å¾‹çš„æ–°è§†è§’ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.IR",
      "comment": "4 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.13166v1",
      "published_date": "2025-11-17 09:10:37 UTC",
      "updated_date": "2025-11-17 09:10:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:04:42.632052+00:00"
    },
    {
      "arxiv_id": "2511.13160v1",
      "title": "InteractiveGNNExplainer: A Visual Analytics Framework for Multi-Faceted Understanding and Probing of Graph Neural Network Predictions",
      "title_zh": "InteractiveGNNExplainerï¼šç”¨äºå¤šç»´åº¦ç†è§£ä¸æ¢ç©¶å›¾ç¥ç»ç½‘ç»œé¢„æµ‹çš„å¯è§†åˆ†ææ¡†æ¶",
      "authors": [
        "TC Singh",
        "Sougata Mukherjea"
      ],
      "abstract": "Graph Neural Networks (GNNs) excel in graph-based learning tasks, but their complex, non-linear operations often render them as opaque \"black boxes\". This opacity hinders user trust, complicates debugging, bias detection, and adoption in critical domains requiring explainability. This paper introduces InteractiveGNNExplainer, a visual analytics framework to enhance GNN explainability, focusing on node classification. Our system uniquely integrates coordinated interactive views (dynamic graph layouts, embedding projections, feature inspection, neighborhood analysis) with established post-hoc (GNNExplainer) and intrinsic (GAT attention) explanation techniques. Crucially, it incorporates interactive graph editing, allowing users to perform a \"what-if\" analysis by perturbing graph structures and observing immediate impacts on GNN predictions and explanations. We detail the system architecture and, through case studies on Cora and CiteSeer datasets, demonstrate how InteractiveGNNExplainer facilitates in-depth misclassification diagnosis, comparative analysis of GCN versus GAT behaviors, and rigorous probing of model sensitivity. These capabilities foster a deeper, multifaceted understanding of GNN predictions, contributing to more transparent, trustworthy, and robust graph analysis.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº† InteractiveGNNExplainerï¼Œä¸€ä¸ªç”¨äºå¢å¼ºå›¾ç¥ç»ç½‘ç»œ (Graph Neural Networks, GNNs) èŠ‚ç‚¹åˆ†ç±»å¯è§£é‡Šæ€§çš„å¯è§†åŒ–åˆ†ææ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤æ‚éçº¿æ€§æ“ä½œå¸¦æ¥çš„â€œé»‘ç›’â€é—®é¢˜ã€‚è¯¥ç³»ç»Ÿé›†æˆäº†åŠ¨æ€å›¾å¸ƒå±€ã€åµŒå…¥æŠ•å½±å’Œç‰¹å¾æ£€æŸ¥ç­‰åè°ƒäº¤äº’è§†å›¾ï¼Œå¹¶èåˆäº†äº‹åè§£é‡Š (GNNExplainer) ä¸å†…åœ¨è§£é‡Š (GAT attention) æŠ€æœ¯ã€‚å…¶æ ¸å¿ƒåŠŸèƒ½æ˜¯æ”¯æŒäº¤äº’å¼å›¾ç¼–è¾‘ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡æ‰°åŠ¨å›¾ç»“æ„æ‰§è¡Œâ€œå‡è®¾åˆ†æâ€ (what-if analysis)ï¼Œä»è€Œå®æ—¶è§‚å¯Ÿå…¶å¯¹ GNN é¢„æµ‹å’Œè§£é‡Šçš„å½±å“ã€‚é€šè¿‡å¯¹ Cora å’Œ CiteSeer æ•°æ®é›†çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œè¯¥æ¡†æ¶å±•ç¤ºäº†åœ¨è¯¯åˆ†ç±»è¯Šæ–­ã€GCN ä¸ GAT è¡Œä¸ºå¯¹æ¯”åˆ†æä»¥åŠæ¨¡å‹æ•æ„Ÿæ€§æ¢æµ‹æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚è¿™äº›åŠŸèƒ½ä¿ƒè¿›äº†å¯¹ GNN é¢„æµ‹ç»“æœçš„å¤šç»´åº¦æ·±åº¦ç†è§£ï¼Œä¸ºæ„å»ºæ›´é€æ˜ã€å¯ä¿¡ä¸”é²æ£’çš„å›¾åˆ†ææ¨¡å‹æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13160v1",
      "published_date": "2025-11-17 09:08:31 UTC",
      "updated_date": "2025-11-17 09:08:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:03:49.536467+00:00"
    },
    {
      "arxiv_id": "2511.13145v1",
      "title": "Automated Road Distress Detection Using Vision Transformersand Generative Adversarial Networks",
      "title_zh": "åŸºäºè§†è§‰ Transformer ä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„è‡ªåŠ¨åŒ–é“è·¯ç—…å®³æ£€æµ‹",
      "authors": [
        "Cesar Portocarrero Rodriguez",
        "Laura Vandeweyen",
        "Yosuke Yamamoto"
      ],
      "abstract": "The American Society of Civil Engineers has graded Americas infrastructure condition as a C, with the road system receiving a dismal D. Roads are vital to regional economic viability, yet their management, maintenance, and repair processes remain inefficient, relying on outdated manual or laser-based inspection methods that are both costly and time-consuming. With the increasing availability of real-time visual data from autonomous vehicles, there is an opportunity to apply computer vision (CV) methods for advanced road monitoring, providing insights to guide infrastructure rehabilitation efforts. This project explores the use of state-of-the-art CV techniques for road distress segmentation. It begins by evaluating synthetic data generated with Generative Adversarial Networks (GANs) to assess its usefulness for model training. The study then applies Convolutional Neural Networks (CNNs) for road distress segmentation and subsequently examines the transformer-based model MaskFormer. Results show that GAN-generated data improves model performance and that MaskFormer outperforms the CNN model in two metrics: mAP50 and IoU.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é“è·¯åŸºç¡€è®¾æ–½è¯„çº§è¾ƒä½ä¸”ä¼ ç»Ÿæ£€æµ‹æ–¹æ³•ä½æ•ˆä¸”æ˜‚è´µçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆè®¡ç®—æœºè§†è§‰(CV)æŠ€æœ¯è¿›è¡Œé“è·¯æŸä¼¤åˆ†å‰²(road distress segmentation)çš„è‡ªåŠ¨åŒ–æ–¹æ¡ˆã€‚ç ”ç©¶é¦–å…ˆè¯„ä¼°äº†åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(GANs)ç”Ÿæˆçš„åˆæˆæ•°æ®åœ¨æ¨¡å‹è®­ç»ƒä¸­çš„å®ç”¨æ€§ï¼Œå¹¶å¯¹æ¯”äº†å·ç§¯ç¥ç»ç½‘ç»œ(CNNs)ä¸åŸºäºTransformerçš„MaskFormeræ¨¡å‹åœ¨é“è·¯ç›‘æ§ä¸­çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGANsç”Ÿæˆçš„åˆæˆæ•°æ®èƒ½æœ‰æ•ˆæå‡æ¨¡å‹æ€§èƒ½ï¼Œä¸”MaskFormeråœ¨mAP50å’ŒIoUä¸¤é¡¹æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºCNNsæ¨¡å‹ã€‚è¯¥æ–¹æ³•å……åˆ†åˆ©ç”¨äº†è‡ªåŠ¨é©¾é©¶è½¦è¾†çš„å®æ—¶è§†è§‰æ•°æ®ï¼Œä¸ºå¤§è§„æ¨¡ã€è‡ªåŠ¨åŒ–çš„é“è·¯ç»´æŠ¤æä¾›äº†é«˜æ•ˆçš„æŠ€æœ¯æ”¯æŒã€‚é€šè¿‡è¯¥ç ”ç©¶ï¼Œä½œè€…è¯æ˜äº†å…ˆè¿›çš„Transformeræ¶æ„åœ¨åŸºç¡€è®¾æ–½åº·å¤å’Œé“è·¯ç›‘æ§ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13145v1",
      "published_date": "2025-11-17 08:56:08 UTC",
      "updated_date": "2025-11-17 08:56:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:03:57.342167+00:00"
    },
    {
      "arxiv_id": "2511.13143v1",
      "title": "SoK: The Last Line of Defense: On Backdoor Defense Evaluation",
      "title_zh": "SoKï¼šæœ€åä¸€é“é˜²çº¿ï¼šè®ºåé—¨é˜²å¾¡è¯„ä¼°",
      "authors": [
        "Gorka Abad",
        "Marina KrÄek",
        "Stefanos Koffas",
        "Behrad Tajalli",
        "Marco Arazzi",
        "Roberto RiaÃ±o",
        "Xiaoyun Xu",
        "Zhuoran Liu",
        "Antonino Nocera",
        "Stjepan Picek"
      ],
      "abstract": "Backdoor attacks pose a significant threat to deep learning models by implanting hidden vulnerabilities that can be activated by malicious inputs. While numerous defenses have been proposed to mitigate these attacks, the heterogeneous landscape of evaluation methodologies hinders fair comparison between defenses. This work presents a systematic (meta-)analysis of backdoor defenses through a comprehensive literature review and empirical evaluation. We analyzed 183 backdoor defense papers published between 2018 and 2025 across major AI and security venues, examining the properties and evaluation methodologies of these defenses.\n  Our analysis reveals significant inconsistencies in experimental setups, evaluation metrics, and threat model assumptions in the literature. Through extensive experiments involving three datasets (MNIST, CIFAR-100, ImageNet-1K), four model architectures (ResNet-18, VGG-19, ViT-B/16, DenseNet-121), 16 representative defenses, and five commonly used attacks, totaling over 3\\,000 experiments, we demonstrate that defense effectiveness varies substantially across different evaluation setups. We identify critical gaps in current evaluation practices, including insufficient reporting of computational overhead and behavior under benign conditions, bias in hyperparameter selection, and incomplete experimentation. Based on our findings, we provide concrete challenges and well-motivated recommendations to standardize and improve future defense evaluations. Our work aims to equip researchers and industry practitioners with actionable insights for developing, assessing, and deploying defenses to different systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹é¢ä¸´çš„åé—¨æ”»å‡»(Backdoor attacks)å¨èƒï¼Œå¯¹åé—¨é˜²å¾¡è¯„ä¼°æ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿæ€§çš„çŸ¥è¯†ä½“ç³»åŒ–(SoK)ç ”ç©¶ã€‚ä½œè€…é€šè¿‡åˆ†æ2018å¹´è‡³2025å¹´é—´å‘è¡¨çš„183ç¯‡ç›¸å…³è®ºæ–‡ï¼Œæ­ç¤ºäº†ç°æœ‰æ–‡çŒ®åœ¨å®éªŒè®¾ç½®ã€è¯„ä»·æŒ‡æ ‡å’Œå¨èƒæ¨¡å‹å‡è®¾ä¸­å­˜åœ¨çš„ä¸¥é‡ä¸ä¸€è‡´æ€§ã€‚ç ”ç©¶é€šè¿‡æ¶µç›–3ä¸ªæ•°æ®é›†ã€4ç§æ¨¡å‹æ¶æ„(ResNet-18, VGG-19, ViT-B/16, DenseNet-121)ã€16ç§ä»£è¡¨æ€§é˜²å¾¡å’Œ5ç§å¸¸è§æ”»å‡»çš„3,000å¤šæ¬¡å®éªŒï¼Œå®è¯äº†é˜²å¾¡æœ‰æ•ˆæ€§åœ¨ä¸åŒè¯„ä¼°è®¾ç½®ä¸‹çš„æ˜¾è‘—å·®å¼‚ã€‚ç ”ç©¶è¯†åˆ«äº†å½“å‰è¯„ä¼°å®è·µä¸­çš„å…³é”®ç¼ºé™·ï¼ŒåŒ…æ‹¬è®¡ç®—å¼€é”€æŠ¥å‘Šä¸è¶³ã€è‰¯æ€§æ ·æœ¬ä¸‹çš„è¡Œä¸ºåˆ†æç¼ºå¤±ä»¥åŠè¶…å‚æ•°é€‰æ‹©çš„åå·®ã€‚æœ€åï¼Œæœ¬æ–‡æå‡ºäº†æ ‡å‡†åŒ–æœªæ¥é˜²å¾¡è¯„ä¼°çš„å…·ä½“å»ºè®®å’ŒæŒ‘æˆ˜ï¼Œä¸ºç§‘ç ”äººå‘˜å’Œä»ä¸šè€…æä¾›äº†éƒ¨ç½²å¯é é˜²å¾¡ç³»ç»Ÿçš„è¡ŒåŠ¨æŒ‡å—ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13143v1",
      "published_date": "2025-11-17 08:51:18 UTC",
      "updated_date": "2025-11-17 08:51:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:03:54.537046+00:00"
    },
    {
      "arxiv_id": "2511.13137v1",
      "title": "Conditional Diffusion Model for Multi-Agent Dynamic Task Decomposition",
      "title_zh": "é¢å‘å¤šæ™ºèƒ½ä½“åŠ¨æ€ä»»åŠ¡åˆ†è§£çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹",
      "authors": [
        "Yanda Zhu",
        "Yuanyang Zhu",
        "Daoyi Dong",
        "Caihua Chen",
        "Chunlin Chen"
      ],
      "abstract": "Task decomposition has shown promise in complex cooperative multi-agent reinforcement learning (MARL) tasks, which enables efficient hierarchical learning for long-horizon tasks in dynamic and uncertain environments. However, learning dynamic task decomposition from scratch generally requires a large number of training samples, especially exploring the large joint action space under partial observability. In this paper, we present the Conditional Diffusion Model for Dynamic Task Decomposition (C$\\text{D}^\\text{3}$T), a novel two-level hierarchical MARL framework designed to automatically infer subtask and coordination patterns. The high-level policy learns subtask representation to generate a subtask selection strategy based on subtask effects. To capture the effects of subtasks on the environment, C$\\text{D}^\\text{3}$T predicts the next observation and reward using a conditional diffusion model. At the low level, agents collaboratively learn and share specialized skills within their assigned subtasks. Moreover, the learned subtask representation is also used as additional semantic information in a multi-head attention mixing network to enhance value decomposition and provide an efficient reasoning bridge between individual and joint value functions. Experimental results on various benchmarks demonstrate that C$\\text{D}^\\text{3}$T achieves better performance than existing baselines.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†C$\\text{D}^\\text{3}$Tï¼Œä¸€ç§é¢å‘å¤šæ™ºèƒ½ä½“åŠ¨æ€ä»»åŠ¡åˆ†è§£çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (MARL)åœ¨å¤æ‚åä½œä»»åŠ¡ä¸­é¢ä¸´çš„æ ·æœ¬æ•ˆç‡ä½å’Œè”åˆåŠ¨ä½œç©ºé—´å¤§ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤å±‚åˆ†å±‚æ¶æ„ï¼Œèƒ½å¤Ÿè‡ªåŠ¨æ¨æ–­å­ä»»åŠ¡å’Œåä½œæ¨¡å¼ã€‚é«˜å±‚ç­–ç•¥é€šè¿‡å­¦ä¹ å­ä»»åŠ¡è¡¨ç¤º(subtask representation)ç”Ÿæˆé€‰æ‹©ç­–ç•¥ï¼Œå¹¶åˆ©ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹(conditional diffusion model)é¢„æµ‹ç¯å¢ƒçš„è§‚æµ‹å’Œå¥–åŠ±ï¼Œä»è€Œç²¾å‡†æ•æ‰å­ä»»åŠ¡å¯¹ç¯å¢ƒçš„å½±å“ã€‚åœ¨ä½å±‚ï¼Œæ™ºèƒ½ä½“åœ¨åˆ†é…çš„å­ä»»åŠ¡ä¸­åä½œå­¦ä¹ å¹¶å…±äº«ä¸“ä¸šæŠ€èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜å°†å­¦ä¹ åˆ°çš„å­ä»»åŠ¡è¡¨ç¤ºé›†æˆåˆ°å¤šå¤´æ³¨æ„åŠ›æ··åˆç½‘ç»œ(multi-head attention mixing network)ä¸­ï¼Œä»¥å¢å¼ºä»·å€¼åˆ†è§£å¹¶ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒC$\\text{D}^\\text{3}$Tåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°å‡ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.13137v1",
      "published_date": "2025-11-17 08:46:31 UTC",
      "updated_date": "2025-11-17 08:46:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:03:53.531661+00:00"
    },
    {
      "arxiv_id": "2511.13133v1",
      "title": "Soft Conflict-Resolution Decision Transformer for Offline Multi-Task Reinforcement Learning",
      "title_zh": "é¢å‘ç¦»çº¿å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ çš„è½¯å†²çªæ¶ˆè§£å†³ç­– Transformer",
      "authors": [
        "Shudong Wang",
        "Xinfei Wang",
        "Chenhao Zhang",
        "Shanchen Pang",
        "Haiyuan Gui",
        "Wenhao Ji",
        "Xiaojian Liao"
      ],
      "abstract": "Multi-task reinforcement learning (MTRL) seeks to learn a unified policy for diverse tasks, but often suffers from gradient conflicts across tasks. Existing masking-based methods attempt to mitigate such conflicts by assigning task-specific parameter masks. However, our empirical study shows that coarse-grained binary masks have the problem of over-suppressing key conflicting parameters, hindering knowledge sharing across tasks. Moreover, different tasks exhibit varying conflict levels, yet existing methods use a one-size-fits-all fixed sparsity strategy to keep training stability and performance, which proves inadequate. These limitations hinder the model's generalization and learning efficiency.\n  To address these issues, we propose SoCo-DT, a Soft Conflict-resolution method based by parameter importance. By leveraging Fisher information, mask values are dynamically adjusted to retain important parameters while suppressing conflicting ones. In addition, we introduce a dynamic sparsity adjustment strategy based on the Interquartile Range (IQR), which constructs task-specific thresholding schemes using the distribution of conflict and harmony scores during training. To enable adaptive sparsity evolution throughout training, we further incorporate an asymmetric cosine annealing schedule to continuously update the threshold. Experimental results on the Meta-World benchmark show that SoCo-DT outperforms the state-of-the-art method by 7.6% on MT50 and by 10.5% on the suboptimal dataset, demonstrating its effectiveness in mitigating gradient conflicts and improving overall multi-task performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ (Multi-task reinforcement learning)ä¸­å¸¸è§çš„æ¢¯åº¦å†²çªé—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„äºŒè¿›åˆ¶æ©ç æ–¹æ³•å­˜åœ¨è¿‡åº¦æŠ‘åˆ¶å…³é”®å‚æ•°åŠç¼ºä¹ç¨€ç–æ€§çµæ´»æ€§çš„ç¼ºé™·ã€‚ä¸ºæ­¤æå‡ºäº†SoCo-DTï¼Œä¸€ç§åŸºäºå‚æ•°é‡è¦æ€§çš„è½¯å†²çªè§£å†³(Soft Conflict-resolution)å†³ç­–å˜æ¢å™¨æ¡†æ¶ã€‚è¯¥æ–¹æ³•åˆ©ç”¨Fisher informationåŠ¨æ€è°ƒæ•´æ©ç å€¼ï¼Œåœ¨æŠ‘åˆ¶å†²çªçš„åŒæ—¶ä¿ç•™é‡è¦å‚æ•°ä»¥ä¿ƒè¿›è·¨ä»»åŠ¡çŸ¥è¯†å…±äº«ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†åŸºäºå››åˆ†ä½è·(Interquartile Range)çš„åŠ¨æ€ç¨€ç–è°ƒæ•´ç­–ç•¥ï¼Œå¹¶ç»“åˆéå¯¹ç§°ä½™å¼¦é€€ç«(asymmetric cosine annealing)è°ƒåº¦ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®è®­ç»ƒä¸­çš„å†²çªä¸å’Œè°å¾—åˆ†åˆ†å¸ƒè‡ªé€‚åº”æ¼”è¿›ã€‚åœ¨Meta-WorldåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSoCo-DTåœ¨MT50åŠæ¬¡ä¼˜æ•°æ®é›†ä¸Šåˆ†åˆ«æ¯”ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•æå‡äº†7.6%å’Œ10.5%çš„æ€§èƒ½ã€‚å®éªŒç»“æœå……åˆ†è¯æ˜äº†è¯¥æ–¹æ³•åœ¨ç¼“è§£æ¢¯åº¦å†²çªã€æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›å’Œå­¦ä¹ æ•ˆç‡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13133v1",
      "published_date": "2025-11-17 08:40:41 UTC",
      "updated_date": "2025-11-17 08:40:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:04:03.249462+00:00"
    },
    {
      "arxiv_id": "2511.13132v1",
      "title": "Shedding Light on VLN Robustness: A Black-box Framework for Indoor Lighting-based Adversarial Attack",
      "title_zh": "æ¢ç©¶ VLN é²æ£’æ€§ï¼šä¸€ç§åŸºäºå®¤å†…å…‰ç…§çš„å¯¹æŠ—æ”»å‡»é»‘ç›’æ¡†æ¶",
      "authors": [
        "Chenyang Li",
        "Wenbing Tang",
        "Yihao Huang",
        "Sinong Simon Zhan",
        "Ming Hu",
        "Xiaojun Jia",
        "Yang Liu"
      ],
      "abstract": "Vision-and-Language Navigation (VLN) agents have made remarkable progress, but their robustness remains insufficiently studied. Existing adversarial evaluations often rely on perturbations that manifest as unusual textures rarely encountered in everyday indoor environments. Errors under such contrived conditions have limited practical relevance, as real-world agents are unlikely to encounter such artificial patterns. In this work, we focus on indoor lighting, an intrinsic yet largely overlooked scene attribute that strongly influences navigation. We propose Indoor Lighting-based Adversarial Attack (ILA), a black-box framework that manipulates global illumination to disrupt VLN agents. Motivated by typical household lighting usage, we design two attack modes: Static Indoor Lighting-based Attack (SILA), where the lighting intensity remains constant throughout an episode, and Dynamic Indoor Lighting-based Attack (DILA), where lights are switched on or off at critical moments to induce abrupt illumination changes. We evaluate ILA on two state-of-the-art VLN models across three navigation tasks. Results show that ILA significantly increases failure rates while reducing trajectory efficiency, revealing previously unrecognized vulnerabilities of VLN agents to realistic indoor lighting variations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€å¯¼èˆª (Vision-and-Language Navigation, VLN) æ™ºèƒ½ä½“é²æ£’æ€§ç ”ç©¶ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºå®¤å†…å…‰ç…§å¯¹æŠ—æ”»å‡» (Indoor Lighting-based Adversarial Attack, ILA) çš„é»‘ç›’æ¡†æ¶ã€‚ILA çªç ´äº†ä»¥å¾€å¯¹æŠ—æ€§è¯„ä¼°ä¾èµ–äºéå¸¸è§„çº¹ç†æ‰°åŠ¨çš„å±€é™ï¼Œè½¬è€Œåˆ©ç”¨å®¤å†…å…‰ç…§è¿™ä¸€å…³é”®åœºæ™¯å±æ€§æ¥å®æ–½æ”»å‡»ã€‚ç ”ç©¶è€…è®¾è®¡äº†é™æ€å…‰ç…§æ”»å‡» (Static Indoor Lighting-based Attack, SILA) å’ŒåŠ¨æ€å…‰ç…§æ”»å‡» (Dynamic Indoor Lighting-based Attack, DILA) ä¸¤ç§æ¨¡å¼ï¼Œåˆ†åˆ«æ¨¡æ‹Ÿæ’å®šçš„å¼‚å¸¸å…‰å¼ºå’Œå…³é”®èŠ‚ç‚¹çš„å…‰ç…§çªå˜ã€‚åœ¨å¤šé¡¹å¯¼èˆªä»»åŠ¡å’Œæœ€å…ˆè¿› VLN æ¨¡å‹ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒILA æ˜¾è‘—æå‡äº†æ™ºèƒ½ä½“çš„å¤±è´¥ç‡å¹¶é™ä½äº†è½¨è¿¹æ•ˆç‡ã€‚è¯¥å·¥ä½œæœ‰æ•ˆæ­ç¤ºäº† VLN æ™ºèƒ½ä½“åœ¨é¢å¯¹ç°å®å®¤å†…å…‰ç…§å˜åŒ–æ—¶æ­¤å‰æœªè¢«å¯Ÿè§‰çš„è„†å¼±æ€§ï¼Œä¸ºæ„å»ºæ›´å…·é²æ£’æ€§çš„å¯¼èˆªç³»ç»Ÿæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13132v1",
      "published_date": "2025-11-17 08:39:29 UTC",
      "updated_date": "2025-11-17 08:39:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:05:09.150556+00:00"
    },
    {
      "arxiv_id": "2511.13800v1",
      "title": "Synergizing Multigrid Algorithms with Vision Transformer: A Novel Approach to Enhance the Seismic Foundation Model",
      "title_zh": "å¤šé‡ç½‘æ ¼ç®—æ³•ä¸ Vision Transformer çš„ååŒï¼šä¸€ç§å¢å¼ºåœ°éœ‡åŸºåº§æ¨¡å‹çš„æ–°æ–¹æ³•",
      "authors": [
        "Huiwen Wu",
        "Shuo Zhang",
        "Yi Liu",
        "Hongbin Ye"
      ],
      "abstract": "Due to the emergency and homogenization of Artificial Intelligence (AI) technology development, transformer-based foundation models have revolutionized scientific applications, such as drug discovery, materials research, and astronomy. However, seismic data presents unique characteristics that require specialized processing techniques for pretraining foundation models in seismic contexts with high- and low-frequency features playing crucial roles. Existing vision transformers (ViTs) with sequential tokenization ignore the intrinsic pattern and fail to grasp both the high- and low-frequency seismic information efficiently and effectively. This work introduces a novel adaptive two-grid foundation model training strategy (ADATG) with Hilbert encoding specifically tailored for seismogram data, leveraging the hierarchical structures inherent in seismic data. Specifically, our approach employs spectrum decomposition to separate high- and low-frequency components and utilizes hierarchical Hilbert encoding to represent the data effectively. Moreover, observing the frequency principle observed in ViTs, we propose an adaptive training strategy that initially emphasizes coarse-level information and then progressively refines the model's focus on fine-level features. Our extensive experiments demonstrate the effectiveness and efficiency of our training methods. This research highlights the importance of data encoding and training strategies informed by the distinct characteristics of high- and low-frequency features in seismic images, ultimately contributing to the enhancement of visual seismic foundation models pretraining.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Vision Transformer (ViT) åœ¨å¤„ç†åœ°éœ‡æ•°æ®æ—¶éš¾ä»¥é«˜æ•ˆæ•æ‰é«˜ä½é¢‘ç‰¹å¾çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆ Hilbert encoding çš„è‡ªé€‚åº”åŒç½‘æ ¼åŸºç¡€æ¨¡å‹è®­ç»ƒç­–ç•¥ (ADATG)ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å…‰è°±åˆ†è§£ (Spectrum decomposition) å°†åœ°éœ‡ä¿¡å·çš„åˆ†å±‚ç»“æ„åˆ’åˆ†ä¸ºé«˜é¢‘å’Œä½é¢‘æˆåˆ†ï¼Œå¹¶é‡‡ç”¨åˆ†å±‚çš„ Hilbert encoding è¿›è¡Œæœ‰æ•ˆæ•°æ®è¡¨å¾ã€‚åŸºäº ViT çš„é¢‘ç‡åŸåˆ™ï¼ŒADATG é‡‡ç”¨äº†ä¸€ç§ä»ç²—ç²’åº¦ä¿¡æ¯åˆ°ç»†ç²’åº¦ç‰¹å¾é€æ­¥è¿›é˜¶çš„è‡ªé€‚åº”è®­ç»ƒè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç­–ç•¥æ˜¾è‘—æå‡äº†åœ°éœ‡åŸºç¡€æ¨¡å‹é¢„è®­ç»ƒçš„æ•ˆç‡ä¸æ€§èƒ½ã€‚æ­¤é¡¹å·¥ä½œè¯æ˜äº†ç»“åˆåœ°éœ‡å½±åƒç‰©ç†ç‰¹æ€§çš„æ•°æ®ç¼–ç ä¸è®­ç»ƒç­–ç•¥åœ¨æå‡è§†è§‰åœ°éœ‡åŸºç¡€æ¨¡å‹æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "math.NA"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13800v1",
      "published_date": "2025-11-17 08:37:28 UTC",
      "updated_date": "2025-11-17 08:37:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:05:03.445272+00:00"
    },
    {
      "arxiv_id": "2511.13131v1",
      "title": "MM-Telco: Benchmarks and Multimodal Large Language Models for Telecom Applications",
      "title_zh": "MM-Telcoï¼šé¢å‘ç”µä¿¡åº”ç”¨çš„åŸºå‡†æµ‹è¯•ä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Gagan Raj Gupta",
        "Anshul Kumar",
        "Manish Rai",
        "Apu Chakraborty",
        "Ashutosh Modi",
        "Abdelaali Chaoub",
        "Soumajit Pramanik",
        "Moyank Giri",
        "Yashwanth Holla",
        "Sunny Kumar",
        "M. V. Kiran Sooraj"
      ],
      "abstract": "Large Language Models (LLMs) have emerged as powerful tools for automating complex reasoning and decision-making tasks. In telecommunications, they hold the potential to transform network optimization, automate troubleshooting, enhance customer support, and ensure regulatory compliance. However, their deployment in telecom is hindered by domain-specific challenges that demand specialized adaptation. To overcome these challenges and to accelerate the adaptation of LLMs for telecom, we propose MM-Telco, a comprehensive suite of multimodal benchmarks and models tailored for the telecom domain. The benchmark introduces various tasks (both text based and image based) that address various practical real-life use cases such as network operations, network management, improving documentation quality, and retrieval of relevant text and images. Further, we perform baseline experiments with various LLMs and VLMs. The models fine-tuned on our dataset exhibit a significant boost in performance. Our experiments also help analyze the weak areas in the working of current state-of-art multimodal LLMs, thus guiding towards further development and research.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MM-Telcoï¼Œè¿™æ˜¯ä¸€å¥—ä¸“é—¨ä¸ºç”µä¿¡é¢†åŸŸé‡èº«å®šåˆ¶çš„ç»¼åˆæ€§å¤šæ¨¡æ€åŸºå‡†(Benchmarks)å’Œæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³é€šç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†ç”µä¿¡ç½‘ç»œä¼˜åŒ–ã€æ•…éšœæ’é™¤åŠåˆè§„æ€§ç­‰ç‰¹å®šé¢†åŸŸä»»åŠ¡æ—¶é¢ä¸´çš„å±€é™æ€§ã€‚MM-Telco å¼•å…¥äº†æ¶µç›–ç½‘ç»œè¿è¥ã€ç½‘ç»œç®¡ç†ã€æ–‡æ¡£è´¨é‡æ”¹è¿›ä»¥åŠç›¸å…³æ–‡æœ¬å’Œå›¾åƒæ£€ç´¢ç­‰å¤šç§å®é™…åº”ç”¨åœºæ™¯çš„æ–‡æœ¬å’Œå›¾åƒä»»åŠ¡ã€‚é€šè¿‡å¯¹å¤šç§ LLMs å’Œè§†è§‰è¯­è¨€æ¨¡å‹(VLMs)è¿›è¡ŒåŸºå‡†å®éªŒï¼Œç»“æœè¡¨æ˜åœ¨ MM-Telco æ•°æ®é›†ä¸Šç»è¿‡å¾®è°ƒçš„æ¨¡å‹æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜åˆ†æäº†å½“å‰æœ€å…ˆè¿›å¤šæ¨¡æ€ LLMs çš„è–„å¼±ç¯èŠ‚ï¼Œä¸ºç”µä¿¡é¢†åŸŸæ™ºèƒ½åŒ–çš„è¿›ä¸€æ­¥ç ”ç©¶å’Œå¼€å‘æä¾›äº†æŒ‡å¯¼æ–¹å‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.ET",
        "cs.NI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13131v1",
      "published_date": "2025-11-17 08:34:41 UTC",
      "updated_date": "2025-11-17 08:34:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:05:01.257445+00:00"
    },
    {
      "arxiv_id": "2511.13118v1",
      "title": "Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction",
      "title_zh": "åƒä»£ç ä¸€æ ·æå–äº‹ä»¶ï¼šä¸€ç§é¢å‘é›¶æ ·æœ¬äº‹ä»¶æå–çš„å¤šæ™ºèƒ½ä½“ç¼–ç¨‹æ¡†æ¶",
      "authors": [
        "Quanjiang Guo",
        "Sijie Wang",
        "Jinchuan Zhang",
        "Ben Zhang",
        "Zhao Kang",
        "Ling Tian",
        "Ke Yan"
      ],
      "abstract": "Zero-shot event extraction (ZSEE) remains a significant challenge for large language models (LLMs) due to the need for complex reasoning and domain-specific understanding. Direct prompting often yields incomplete or structurally invalid outputs--such as misclassified triggers, missing arguments, and schema violations. To address these limitations, we present Agent-Event-Coder (AEC), a novel multi-agent framework that treats event extraction like software engineering: as a structured, iterative code-generation process. AEC decomposes ZSEE into specialized subtasks--retrieval, planning, coding, and verification--each handled by a dedicated LLM agent. Event schemas are represented as executable class definitions, enabling deterministic validation and precise feedback via a verification agent. This programming-inspired approach allows for systematic disambiguation and schema enforcement through iterative refinement. By leveraging collaborative agent workflows, AEC enables LLMs to produce precise, complete, and schema-consistent extractions in zero-shot settings. Experiments across five diverse domains and six LLMs demonstrate that AEC consistently outperforms prior zero-shot baselines, showcasing the power of treating event extraction like code generation. The code and data are released on https://github.com/UESTC-GQJ/Agent-Event-Coder.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨é›¶æ ·æœ¬äº‹ä»¶æŠ½å–(Zero-shot event extraction, ZSEE)ä¸­é¢ä¸´çš„å¤æ‚æ¨ç†éœ€æ±‚å’Œæ¨¡å¼è¿åç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†Agent-Event-Coder (AEC)å¤šæ™ºèƒ½ä½“ç¼–ç¨‹æ¡†æ¶ã€‚AECåˆ›æ–°æ€§åœ°å°†äº‹ä»¶æŠ½å–ä»»åŠ¡ç±»æ¯”ä¸ºè½¯ä»¶å·¥ç¨‹ä¸­çš„ä»£ç ç”Ÿæˆ(code-generation)è¿‡ç¨‹ï¼Œé€šè¿‡è¿­ä»£å¼çš„å¼€å‘æµç¨‹ç¡®ä¿ç»“æœçš„ç»“æ„åŒ–ä¸å‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶å°†ZSEEåˆ†è§£ä¸ºæ£€ç´¢(retrieval)ã€è§„åˆ’(planning)ã€ç¼–ç (coding)å’ŒéªŒè¯(verification)å››ä¸ªç”±ä¸“é—¨æ™ºèƒ½ä½“åä½œå®Œæˆçš„å­ä»»åŠ¡ï¼Œå¹¶å°†äº‹ä»¶Schemaè¡¨ç¤ºä¸ºå¯æ‰§è¡Œçš„ç±»å®šä¹‰ä»¥å®ç°ç¡®å®šæ€§æ ¡éªŒã€‚è¿™ç§å—ç¼–ç¨‹å¯å‘çš„ç³»ç»ŸåŒ–æ–¹æ³•èƒ½å¤Ÿå¼ºåˆ¶æ‰§è¡ŒSchemaçº¦æŸå¹¶è¿›è¡Œç²¾ç¡®æ¶ˆæ­§ï¼Œä»è€Œä½¿LLMsåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ç”Ÿæˆé«˜åº¦ä¸€è‡´ä¸”å®Œæ•´çš„æŠ½å–ç»“æœã€‚åœ¨äº”ä¸ªé¢†åŸŸå’Œå…­ç§LLMsä¸Šçš„å®éªŒè¯æ˜ï¼ŒAECä¸€è‡´ä¼˜äºç°æœ‰çš„é›¶æ ·æœ¬åŸºå‡†ï¼Œå±•ç¤ºäº†å°†äº‹ä»¶æŠ½å–è§†ä¸ºä»£ç ç”Ÿæˆçš„å¼ºå¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 5 figures, accepted by AAAI 2026 (Oral)",
      "pdf_url": "https://arxiv.org/pdf/2511.13118v1",
      "published_date": "2025-11-17 08:17:15 UTC",
      "updated_date": "2025-11-17 08:17:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:05:13.634186+00:00"
    },
    {
      "arxiv_id": "2511.13116v1",
      "title": "Synthetic Forgetting without Access: A Few-shot Zero-glance Framework for Machine Unlearning",
      "title_zh": "æ— è®¿é—®åˆæˆé—å¿˜ï¼šé¢å‘æœºå™¨é—å¿˜çš„å°‘æ ·æœ¬é›¶æ¥è§¦æ¡†æ¶",
      "authors": [
        "Qipeng Song",
        "Nan Yang",
        "Ziqi Xu",
        "Yue Li",
        "Wei Shao",
        "Feng Xia"
      ],
      "abstract": "Machine unlearning aims to eliminate the influence of specific data from trained models to ensure privacy compliance. However, most existing methods assume full access to the original training dataset, which is often impractical. We address a more realistic yet challenging setting: few-shot zero-glance, where only a small subset of the retained data is available and the forget set is entirely inaccessible. We introduce GFOES, a novel framework comprising a Generative Feedback Network (GFN) and a two-phase fine-tuning procedure. GFN synthesises Optimal Erasure Samples (OES), which induce high loss on target classes, enabling the model to forget class-specific knowledge without access to the original forget data, while preserving performance on retained classes. The two-phase fine-tuning procedure enables aggressive forgetting in the first phase, followed by utility restoration in the second. Experiments on three image classification datasets demonstrate that GFOES achieves effective forgetting at both logit and representation levels, while maintaining strong performance using only 5% of the original data. Our framework offers a practical and scalable solution for privacy-preserving machine learning under data-constrained conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨é—å¿˜(Machine Unlearning)é€šå¸¸å‡è®¾æ‹¥æœ‰å®Œæ•´è®­ç»ƒæ•°æ®çš„ä¸åˆ‡å®é™…æ€§ï¼Œè§£å†³äº†ä¸€ç§æ›´å…·æŒ‘æˆ˜æ€§çš„\"few-shot zero-glance\"è®¾ç½®ï¼Œå³ä»…å¯ç”¨å°‘é‡ä¿ç•™æ•°æ®ä¸”å®Œå…¨æ— æ³•è®¿é—®é—å¿˜é›†ã€‚ä½œè€…æå‡ºäº†GFOESæ¡†æ¶ï¼ŒåŒ…å«ç”Ÿæˆåé¦ˆç½‘ç»œ(Generative Feedback Network, GFN)å’Œä¸¤é˜¶æ®µå¾®è°ƒç¨‹åºã€‚GFNé€šè¿‡åˆæˆæœ€ä¼˜æ“¦é™¤æ ·æœ¬(Optimal Erasure Samples, OES)åœ¨ç›®æ ‡ç±»åˆ«ä¸Šè¯±å¯¼é«˜æŸå¤±ï¼Œä½¿æ¨¡å‹åœ¨ä¸è®¿é—®åŸå§‹é—å¿˜æ•°æ®çš„æƒ…å†µä¸‹é—å¿˜ç‰¹å®šç±»åˆ«çŸ¥è¯†ã€‚ä¸¤é˜¶æ®µå¾®è°ƒé¦–å…ˆè¿›è¡Œæ¿€è¿›é—å¿˜ï¼Œéšåè¿›è¡Œæ•ˆç”¨æ¢å¤ã€‚åœ¨ä¸‰ä¸ªå›¾åƒåˆ†ç±»æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGFOESä»…ä½¿ç”¨5%çš„åŸå§‹æ•°æ®å³å¯åœ¨logitå’Œè¡¨ç¤ºå±‚é¢ä¸Šå®ç°æœ‰æ•ˆé—å¿˜ï¼Œå¹¶ä¿æŒå¼ºå¤§çš„æ¨¡å‹æ€§èƒ½ï¼Œä¸ºæ•°æ®å—é™æ¡ä»¶ä¸‹çš„éšç§ä¿æŠ¤æœºå™¨å­¦ä¹ æä¾›äº†å®ç”¨æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13116v1",
      "published_date": "2025-11-17 08:16:08 UTC",
      "updated_date": "2025-11-17 08:16:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-26T07:06:37.811415+00:00"
    },
    {
      "arxiv_id": "2511.13111v2",
      "title": "NuBench: An Open Benchmark for Deep Learning-Based Event Reconstruction in Neutrino Telescopes",
      "title_zh": "NuBenchï¼šä¸­å¾®å­æœ›è¿œé•œä¸­åŸºäºæ·±åº¦å­¦ä¹ çš„äº‹ä»¶é‡å»ºå¼€æ”¾åŸºå‡†",
      "authors": [
        "Rasmus F. Orsoe",
        "Stephan Meighen-Berger",
        "Jeffrey Lazar",
        "Jorge Prado",
        "Ivan Mozun-Mateo",
        "Aske Rosted",
        "Philip Weigel",
        "Arturo Llorente Anaya"
      ],
      "abstract": "Neutrino telescopes are large-scale detectors designed to observe Cherenkov radiation produced from neutrino interactions in water or ice. They exist to identify extraterrestrial neutrino sources and to probe fundamental questions pertaining to the elusive neutrino itself. A central challenge common across neutrino telescopes is to solve a series of inverse problems known as event reconstruction, which seeks to resolve properties of the incident neutrino, based on the detected Cherenkov light. In recent times, significant efforts have been made in adapting advances from deep learning research to event reconstruction, as such techniques provide several benefits over traditional methods. While a large degree of similarity in reconstruction needs and low-level data exists, cross-experimental collaboration has been hindered by a lack of diverse open-source datasets for comparing methods.\n  We present NuBench, an open benchmark for deep learning-based event reconstruction in neutrino telescopes. NuBench comprises seven large-scale simulated datasets containing nearly 130 million charged- and neutral-current muon-neutrino interactions spanning 10 GeV to 100 TeV, generated across six detector geometries inspired by existing and proposed experiments. These datasets provide pulse- and event-level information suitable for developing and comparing machine-learning reconstruction methods in both water and ice environments. Using NuBench, we evaluate four reconstruction algorithms - ParticleNeT and DynEdge, both actively used within the KM3NeT and IceCube collaborations, respectively, along with GRIT and DeepIce - on up to five core tasks: energy and direction reconstruction, topology classification, interaction vertex prediction, and inelasticity estimation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† NuBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨é’ˆå¯¹ä¸­å¾®å­æœ›è¿œé•œ (Neutrino Telescopes) ä¸­åŸºäºæ·±åº¦å­¦ä¹  (Deep Learning) äº‹ä»¶é‡å»º (Event Reconstruction) ä»»åŠ¡çš„å¼€æ”¾åŸºå‡†æµ‹è¯•å¹³å°ã€‚ä¸ºäº†å…‹æœè·¨å®éªŒåä½œä¸­ç¼ºä¹å¤šæ ·åŒ–å¼€æºæ•°æ®é›†çš„éšœç¢ï¼ŒNuBench æ•´åˆäº†ä¸ƒä¸ªå¤§è§„æ¨¡æ¨¡æ‹Ÿæ•°æ®é›†ï¼ŒåŒ…å«åœ¨å…­ç§æ¢æµ‹å™¨å‡ ä½•ç»“æ„ä¸‹ç”Ÿæˆçš„è¿‘ 1.3 äº¿æ¬¡ä¸­å¾®å­ç›¸äº’ä½œç”¨ï¼Œèƒ½é‡è¦†ç›– 10 GeV è‡³ 100 TeVã€‚è¯¥åŸºå‡†æä¾›é€‚ç”¨äºæ°´å’Œå†°ç¯å¢ƒçš„è„‰å†²çº§åŠäº‹ä»¶çº§ä¿¡æ¯ï¼Œæ”¯æŒå¯¹å„ç§æœºå™¨å­¦ä¹ æ–¹æ³•çš„æ€§èƒ½è¯„ä¼°ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨è¯¥å¹³å°å¯¹ ParticleNeTã€DynEdgeã€GRIT å’Œ DeepIce ç­‰ä¸»æµç®—æ³•åœ¨èƒ½é‡é‡å»ºã€æ–¹å‘é‡å»ºã€æ‹“æ‰‘åˆ†ç±»ã€é¡¶ç‚¹é¢„æµ‹å’Œéå¼¹æ€§ä¼°è®¡äº”é¡¹æ ¸å¿ƒä»»åŠ¡ä¸Šè¿›è¡Œäº†æ·±å…¥æµ‹è¯•ã€‚NuBench çš„å‘å¸ƒä¸ºè§£å†³ä¸­å¾®å­æ¢æµ‹ä¸­çš„é€†é—®é¢˜æä¾›äº†æ ‡å‡†åŒ–çš„æ¯”è¾ƒæ¡†æ¶ï¼Œæœ‰åŠ©äºæ¨åŠ¨è¯¥é¢†åŸŸæ·±åº¦å­¦ä¹ æŠ€æœ¯çš„åˆ›æ–°ä¸åº”ç”¨ã€‚",
      "categories": [
        "hep-ex",
        "cs.AI",
        "cs.LG",
        "physics.data-an",
        "physics.ins-det"
      ],
      "primary_category": "hep-ex",
      "comment": "Prepared for JINST. Updated Acknowledgements",
      "pdf_url": "https://arxiv.org/pdf/2511.13111v2",
      "published_date": "2025-11-17 08:08:01 UTC",
      "updated_date": "2025-11-19 08:23:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:05:31.843637+00:00"
    },
    {
      "arxiv_id": "2511.13091v1",
      "title": "STEP: Success-Rate-Aware Trajectory-Efficient Policy Optimization",
      "title_zh": "STEPï¼šåŸºäºæˆåŠŸç‡æ„ŸçŸ¥çš„è½¨è¿¹é«˜æ•ˆç­–ç•¥ä¼˜åŒ–",
      "authors": [
        "Yuhan Chen",
        "Yuxuan Liu",
        "Long Zhang",
        "Pengzhi Gao",
        "Jian Luan",
        "Wei Liu"
      ],
      "abstract": "Multi-turn interaction remains challenging for online reinforcement learning. A common solution is trajectory-level optimization, which treats each trajectory as a single training sample. However, this approach can be inefficient and yield misleading learning signals: it applies uniform sampling across tasks regardless of difficulty, penalizes correct intermediate actions in failed trajectories, and incurs high sample-collection costs. To address these issues, we propose STEP (Success-rate-aware Trajectory-Efficient Policy optimization), a framework that dynamically allocates sampling based on per-task success rates and performs step-level optimization. STEP maintains a smoothed success-rate record to guide adaptive trajectory resampling, allocating more effort to harder tasks. It then computes success-rate-weighted advantages and decomposes trajectories into step-level samples. Finally, it applies a step-level GRPO augmentation to refine updates for low-success tasks. Experiments on OSWorld and AndroidWorld show that STEP substantially improves sample efficiency and training stability over trajectory-level GRPO, converging faster and generalizing better under the same sampling budget.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ¨çº¿å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)åœ¨å¤šè½®äº¤äº’ä¸­é¢ä¸´çš„è½¨è¿¹çº§ä¼˜åŒ–æ•ˆç‡ä½ä¸‹ã€é‡‡æ ·ä¿¡å·è¯¯å¯¼ä»¥åŠé‡‡æ ·æˆæœ¬é«˜ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†STEPï¼ˆSuccess-rate-aware Trajectory-Efficient Policy optimizationï¼‰æ¡†æ¶ã€‚STEPé€šè¿‡ç»´æŠ¤å¹³æ»‘çš„æˆåŠŸç‡è®°å½•æ¥æŒ‡å¯¼è‡ªé€‚åº”è½¨è¿¹é‡é‡‡æ ·ï¼Œå°†æ›´å¤šé‡‡æ ·èµ„æºåŠ¨æ€åˆ†é…ç»™éš¾åº¦è¾ƒå¤§çš„ä»»åŠ¡ã€‚åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æˆåŠŸç‡åŠ æƒçš„ä¼˜åŠ¿å‡½æ•°(Advantages)å°†è½¨è¿¹åˆ†è§£ä¸ºæ­¥çº§(Step-level)æ ·æœ¬ï¼Œå¹¶å¼•å…¥æ­¥çº§ GRPO å¢å¼ºæœºåˆ¶ä»¥ä¼˜åŒ–ä½æˆåŠŸç‡ä»»åŠ¡çš„æ›´æ–°ç­–ç•¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ OSWorld å’Œ AndroidWorld åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSTEP ç›¸æ¯”äºè½¨è¿¹çº§ GRPO æ˜¾è‘—æå‡äº†æ ·æœ¬æ•ˆç‡å’Œè®­ç»ƒç¨³å®šæ€§ï¼Œåœ¨ç›¸åŒé‡‡æ ·é¢„ç®—ä¸‹è¡¨ç°å‡ºæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´ä¼˜çš„æ³›åŒ–æ€§èƒ½ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13091v1",
      "published_date": "2025-11-17 07:43:15 UTC",
      "updated_date": "2025-11-17 07:43:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:05:18.447352+00:00"
    },
    {
      "arxiv_id": "2511.13087v1",
      "title": "MEGA-GUI: Multi-stage Enhanced Grounding Agents for GUI Elements",
      "title_zh": "MEGA-GUIï¼šé¢å‘ GUI å…ƒç´ çš„å¤šé˜¶æ®µå¢å¼ºå®šä½æ™ºèƒ½ä½“",
      "authors": [
        "SeokJoo Kwak",
        "Jihoon Kim",
        "Boyoun Kim",
        "Jung Jae Yoon",
        "Wooseok Jang",
        "Jeonghoon Hong",
        "Jaeho Yang",
        "Yeong-Dae Kwon"
      ],
      "abstract": "Graphical User Interface (GUI) grounding - the task of mapping natural language instructions to screen coordinates - is essential for autonomous agents and accessibility technologies. Existing systems rely on monolithic models or one-shot pipelines that lack modularity and fail under visual clutter and ambiguous instructions. We introduce MEGA-GUI, a multi-stage framework that separates grounding into coarse Region-of-Interest (ROI) selection and fine-grained element grounding, orchestrated by specialized vision-language agents. MEGA-GUI features a bidirectional ROI zoom algorithm that mitigates spatial dilution and a context-aware rewriting agent that reduces semantic ambiguity. Our analysis reveals complementary strengths and weaknesses across vision-language models at different visual scales, and we show that leveraging this modular structure achieves consistently higher accuracy than monolithic approaches. On the visually dense ScreenSpot-Pro benchmark, MEGA-GUI attains 73.18% accuracy, and on the semantically complex OSWorld-G benchmark it reaches 68.63%, surpassing previously reported results. Code and the Grounding Benchmark Toolkit (GBT) are available at https://github.com/samsungsds-research-papers/mega-gui.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MEGA-GUIï¼Œä¸€ç§ç”¨äºå›¾å½¢ç”¨æˆ·ç•Œé¢å®šä½ (GUI grounding) çš„å¤šé˜¶æ®µå¢å¼ºå‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å•ä½“æ¨¡å‹åœ¨é¢å¯¹å¤æ‚è§†è§‰å¹²æ‰°å’Œæ­§ä¹‰æŒ‡ä»¤æ—¶çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶å°†å®šä½ä»»åŠ¡æ‹†åˆ†ä¸ºç²—ç²’åº¦çš„æ„Ÿå…´è¶£åŒºåŸŸ (Region-of-Interest, ROI) é€‰æ‹©ä¸ç»†ç²’åº¦çš„å…ƒç´ å®šä½ï¼Œå¹¶ç”±ä¸“é—¨çš„è§†è§‰è¯­è¨€æ™ºèƒ½ä½“ååŒå¤„ç†ã€‚MEGA-GUI åˆ›æ–°æ€§åœ°å¼•å…¥äº†åŒå‘ ROI ç¼©æ”¾ (bidirectional ROI zoom) ç®—æ³•ä»¥åº”å¯¹ç©ºé—´ç¨€é‡Šé—®é¢˜ï¼Œå¹¶åˆ©ç”¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥é‡å†™æ™ºèƒ½ä½“é™ä½è¯­ä¹‰æ­§ä¹‰ã€‚å®éªŒåˆ†ææ­ç¤ºäº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¸åŒè§†è§‰å°ºåº¦ä¸‹çš„äº’è¡¥ç‰¹æ€§ï¼Œè¯æ˜äº†æ¨¡å—åŒ–æ¶æ„åœ¨å‡†ç¡®ç‡ä¸ŠæŒç»­ä¼˜äºå•ä½“æ–¹æ¡ˆã€‚åœ¨ ScreenSpot-Pro å’Œ OSWorld-G åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMEGA-GUI åˆ†åˆ«å–å¾—äº† 73.18% å’Œ 68.63% çš„å‡†ç¡®ç‡ï¼Œå‡è¶…è¶Šäº†æ­¤å‰æŠ¥é“çš„å…ˆè¿›æ°´å¹³ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "26 pages, 7 figures. Code available at https://github.com/samsungsds-research-papers/mega-gui",
      "pdf_url": "https://arxiv.org/pdf/2511.13087v1",
      "published_date": "2025-11-17 07:38:05 UTC",
      "updated_date": "2025-11-17 07:38:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:05:26.233234+00:00"
    },
    {
      "arxiv_id": "2511.21707v1",
      "title": "Sensing and Understanding the World over Air: A Large Multimodal Model for Mobile Networks",
      "title_zh": "è·¨ç©ºåŸŸæ„ŸçŸ¥ä¸ç†è§£ä¸–ç•Œï¼šé¢å‘ç§»åŠ¨ç½‘ç»œçš„å¤šæ¨¡æ€å¤§æ¨¡å‹",
      "authors": [
        "Zhuoran Duan",
        "Yuhao Wei",
        "Guoshun Nan",
        "Zijun Wang",
        "Yan Yan",
        "Lihua Xiong",
        "Yuhan Ran",
        "Ji Zhang",
        "Jian Li",
        "Qimei Cui",
        "Xiaofeng Tao",
        "Tony Q. S. Quek"
      ],
      "abstract": "Large models (LMs), such as ChatGPT, have made a significant impact across diverse domains and hold great potential to facilitate the evolution of network intelligence. Wireless-native multi-modal large models (WMLMs) can sense and understand the physical world through multi-modal data, serving as a key enabler that integrates communication, sensing, and intelligence, and thus they can boost various smart services to billions of users. However, research on WMLMs remains in its infancy, and the construction of domain-specific multi-modal large models for wireless networks is still underexplored. In this paper, we outlines the key characteristics of WMLMs and summarizes existing methods, on the basis of which a wireless-native multimodal training paradigm is proposed. Specifically, we constructed a GPT-style WMLM model and trained it on a real-world large-scale dataset, leveraging wireless signals as an anchor modality for contrastive learning. Our approach demonstrates outstanding performance compared with existing small-scale models and large multi-modal models, validating the feasibility of using wireless signals as a universal modality and highlighting WMLM's potential to emerge as a new paradigm for future wireless networks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ— çº¿åŸç”Ÿå¤šæ¨¡æ€å¤§æ¨¡å‹(WMLMs)åœ¨æ•´åˆé€šä¿¡ã€æ„ŸçŸ¥ä¸æ™ºèƒ½æ–¹é¢çš„æ ¸å¿ƒæ½œåŠ›ï¼Œå¹¶é’ˆå¯¹æ— çº¿é¢†åŸŸç‰¹å®šå¤§æ¨¡å‹ç ”ç©¶ä¸è¶³çš„ç°çŠ¶ï¼Œæå‡ºäº†ä¸€ç§å…¨æ–°çš„æ— çº¿åŸç”Ÿå¤šæ¨¡æ€è®­ç»ƒèŒƒå¼ã€‚ä½œè€…æ„å»ºäº†ä¸€ä¸ªGPT-styleçš„WMLMæ¨¡å‹ï¼Œå¹¶åœ¨å¤§è§„æ¨¡çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚è¯¥æ¨¡å‹çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºåˆ©ç”¨æ— çº¿ä¿¡å·ä½œä¸ºå¯¹æ¯”å­¦ä¹ (Contrastive Learning)çš„é”šç‚¹æ¨¡æ€ï¼Œä»è€Œå®ç°å¯¹ç‰©ç†ä¸–ç•Œçš„æ·±å…¥æ„ŸçŸ¥ä¸ç†è§£ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„å°è§„æ¨¡æ¨¡å‹åŠé€šç”¨å¤§æ¨¡æ€æ¨¡å‹ï¼Œæœ‰æ•ˆéªŒè¯äº†å°†æ— çº¿ä¿¡å·ä½œä¸ºé€šç”¨æ¨¡æ€(Universal Modality)çš„å¯è¡Œæ€§ã€‚è¯¥ç ”ç©¶ä¸ä»…å±•ç¤ºäº†WMLMä½œä¸ºæœªæ¥ç§»åŠ¨ç½‘ç»œæ–°èŒƒå¼çš„å·¨å¤§æ½œåŠ›ï¼Œä¹Ÿä¸ºé¢å‘å…¨çƒç”¨æˆ·çš„æ™ºèƒ½æœåŠ¡æ¼”è¿›æä¾›äº†å…³é”®æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21707v1",
      "published_date": "2025-11-17 07:33:46 UTC",
      "updated_date": "2025-11-17 07:33:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:06:52.538174+00:00"
    },
    {
      "arxiv_id": "2511.13081v2",
      "title": "Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations",
      "title_zh": "é‡æ–°å®¡è§†æ˜¾è‘—æ€§å›¾ï¼šä¸€ç§ç¬¦åˆäººç±»è®¤çŸ¥çš„è§£é‡Šåˆ†ç±»ä½“ç³»ä¸è¯„ä¼°æ¡†æ¶",
      "authors": [
        "Yehonatan Elisha",
        "Seffi Cohen",
        "Oren Barkan",
        "Noam Koenigstein"
      ],
      "abstract": "Saliency maps are widely used for visual explanations in deep learning, but a fundamental lack of consensus persists regarding their intended purpose and alignment with diverse user queries. This ambiguity hinders the effective evaluation and practical utility of explanation methods. We address this gap by introducing the Reference-Frame $\\times$ Granularity (RFxG) taxonomy, a principled conceptual framework that organizes saliency explanations along two essential axes:Reference-Frame: Distinguishing between pointwise (\"Why this prediction?\") and contrastive (\"Why this and not an alternative?\") explanations. Granularity: Ranging from fine-grained class-level (e.g., \"Why Husky?\") to coarse-grained group-level (e.g., \"Why Dog?\") interpretations. Using the RFxG lens, we demonstrate critical limitations in existing evaluation metrics, which overwhelmingly prioritize pointwise faithfulness while neglecting contrastive reasoning and semantic granularity. To systematically assess explanation quality across both RFxG dimensions, we propose four novel faithfulness metrics. Our comprehensive evaluation framework applies these metrics to ten state-of-the-art saliency methods, four model architectures, and three datasets. By advocating a shift toward user-intent-driven evaluation, our work provides both the conceptual foundation and the practical tools necessary to develop visual explanations that are not only faithful to the underlying model behavior but are also meaningfully aligned with the complexity of human understanding and inquiry.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ˜¾è‘—æ€§å›¾(Saliency maps)åœ¨æ·±åº¦å­¦ä¹ è§†è§‰è§£é‡Šä¸­å› ç¼ºä¹é¢„æœŸç›®çš„ä¸ç”¨æˆ·æŸ¥è¯¢å¯¹é½å…±è¯†è€Œå¯¼è‡´çš„è¯„ä¼°éš¾é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†RFxG (Reference-Frame Ã— Granularity)åˆ†ç±»æ³•ï¼Œé€šè¿‡å‚è€ƒæ¡†æ¶(Reference-Frame)åŒºåˆ†ç‚¹å¯¹ç‚¹(pointwise)ä¸å¯¹æ¯”æ€§(contrastive)è§£é‡Šï¼Œå¹¶ä»ç²’åº¦(Granularity)ç»´åº¦æ¶µç›–ç±»çº§åˆ«(class-level)åˆ°ç»„çº§åˆ«(group-level)çš„è§£è¯»ã€‚ç ”ç©¶é€šè¿‡RFxGè§†è§’æ­ç¤ºäº†ç°æœ‰è¯„ä¼°æŒ‡æ ‡è¿‡åº¦å…³æ³¨ç‚¹å¯¹ç‚¹å¿ å®åº¦(faithfulness)è€Œå¿½è§†å¯¹æ¯”æ¨ç†ä¸è¯­ä¹‰ç²’åº¦çš„å±€é™æ€§ã€‚é’ˆå¯¹è¿™äº›ç¼ºé™·ï¼Œè¯¥ç ”ç©¶æå‡ºäº†å››ç§æ–°å‹å¿ å®åº¦æŒ‡æ ‡ï¼Œå¹¶åœ¨10ç§æœ€å…ˆè¿›çš„æ˜¾è‘—æ€§æ–¹æ³•(saliency methods)ã€4ç§æ¨¡å‹æ¶æ„åŠ3ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†ç³»ç»Ÿæ€§è¯„ä¼°ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡å€¡å¯¼ä»¥ç”¨æˆ·æ„å›¾ä¸ºé©±åŠ¨çš„è¯„ä¼°ï¼Œä¸ºå¼€å‘æ—¢ç¬¦åˆæ¨¡å‹è¡Œä¸ºåˆå¯¹é½äººç±»è®¤çŸ¥çš„è§†è§‰è§£é‡Šæä¾›äº†æ ¸å¿ƒç†è®ºæ¡†æ¶ä¸å®ç”¨å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13081v2",
      "published_date": "2025-11-17 07:29:25 UTC",
      "updated_date": "2025-11-18 07:20:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:05:33.232267+00:00"
    },
    {
      "arxiv_id": "2511.13798v1",
      "title": "KANGURA: Kolmogorov-Arnold Network-Based Geometry-Aware Learning with Unified Representation Attention for 3D Modeling of Complex Structures",
      "title_zh": "KANGURAï¼šåŸºäº Kolmogorov-Arnold ç½‘ç»œä¸ç»Ÿä¸€è¡¨ç¤ºæ³¨æ„åŠ›æœºåˆ¶çš„å¤æ‚ç»“æ„ 3D å»ºæ¨¡å‡ ä½•æ„ŸçŸ¥å­¦ä¹ ",
      "authors": [
        "Mohammad Reza Shafie",
        "Morteza Hajiabadi",
        "Hamed Khosravi",
        "Mobina Noori",
        "Imtiaz Ahmed"
      ],
      "abstract": "Microbial Fuel Cells (MFCs) offer a promising pathway for sustainable energy generation by converting organic matter into electricity through microbial processes. A key factor influencing MFC performance is the anode structure, where design and material properties play a crucial role. Existing predictive models struggle to capture the complex geometric dependencies necessary to optimize these structures. To solve this problem, we propose KANGURA: Kolmogorov-Arnold Network-Based Geometry-Aware Learning with Unified Representation Attention. KANGURA introduces a new approach to three-dimensional (3D) machine learning modeling. It formulates prediction as a function decomposition problem, where Kolmogorov-Arnold Network (KAN)- based representation learning reconstructs geometric relationships without a conventional multi- layer perceptron (MLP). To refine spatial understanding, geometry-disentangled representation learning separates structural variations into interpretable components, while unified attention mechanisms dynamically enhance critical geometric regions. Experimental results demonstrate that KANGURA outperforms over 15 state-of-the-art (SOTA) models on the ModelNet40 benchmark dataset, achieving 92.7% accuracy, and excels in a real-world MFC anode structure problem with 97% accuracy. This establishes KANGURA as a robust framework for 3D geometric modeling, unlocking new possibilities for optimizing complex structures in advanced manufacturing and quality-driven engineering applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸ºKANGURAçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¾®ç”Ÿç‰©ç‡ƒæ–™ç”µæ± (Microbial Fuel Cells, MFCs)é˜³æç»“æ„è®¾è®¡ä¸­ç°æœ‰æ¨¡å‹éš¾ä»¥æ•æ‰å¤æ‚å‡ ä½•ä¾èµ–å…³ç³»çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§åŸºäºKolmogorov-Arnold Network (KAN)çš„æ–°å‹ä¸‰ç»´æœºå™¨å­¦ä¹ å»ºæ¨¡æ–¹æ³•ï¼Œé€šè¿‡å‡½æ•°åˆ†è§£é‡å»ºå‡ ä½•å…³ç³»ä»¥æ›¿ä»£ä¼ ç»Ÿçš„å¤šå±‚æ„ŸçŸ¥æœº(MLP)ã€‚KANGURAç»“åˆäº†å‡ ä½•è§£è€¦è¡¨ç¤ºå­¦ä¹ (geometry-disentangled representation learning)ä»¥åˆ†ç¦»å¯è§£é‡Šçš„ç»“æ„ç»„ä»¶ï¼Œå¹¶åˆ©ç”¨ç»Ÿä¸€æ³¨æ„åŠ›æœºåˆ¶(unified attention mechanisms)åŠ¨æ€å¼ºåŒ–å…³é”®å‡ ä½•åŒºåŸŸã€‚å®éªŒè¯æ˜ï¼ŒKANGURAåœ¨ModelNet40åŸºå‡†æ•°æ®é›†ä¸Šä»¥92.7%çš„å‡†ç¡®ç‡è¶…è¶Šäº†15ä¸ªä»¥ä¸Šçš„SOTAæ¨¡å‹ï¼Œå¹¶åœ¨ç°å®ä¸–ç•Œçš„MFCé˜³æç»“æ„é—®é¢˜ä¸­è¾¾åˆ°äº†97%çš„å‡†ç¡®ç‡ã€‚è¿™æ ‡å¿—ç€KANGURAä½œä¸ºä¸€ä¸ªç¨³å¥çš„ä¸‰ç»´å‡ ä½•å»ºæ¨¡æ¡†æ¶ï¼Œä¸ºå…ˆè¿›åˆ¶é€ å’Œå·¥ç¨‹é¢†åŸŸä¼˜åŒ–å¤æ‚ç»“æ„æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13798v1",
      "published_date": "2025-11-17 07:25:09 UTC",
      "updated_date": "2025-11-17 07:25:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:07:24.537106+00:00"
    },
    {
      "arxiv_id": "2511.13062v1",
      "title": "Self-Adaptive Graph Mixture of Models",
      "title_zh": "è‡ªé€‚åº”å›¾æ··åˆæ¨¡å‹",
      "authors": [
        "Mohit Meena",
        "Yash Punjabi",
        "Abhishek A",
        "Vishal Sharma",
        "Mahesh Chandran"
      ],
      "abstract": "Graph Neural Networks (GNNs) have emerged as powerful tools for learning over graph-structured data, yet recent studies have shown that their performance gains are beginning to plateau. In many cases, well-established models such as GCN and GAT, when appropriately tuned, can match or even exceed the performance of more complex, state-of-the-art architectures. This trend highlights a key limitation in the current landscape: the difficulty of selecting the most suitable model for a given graph task or dataset. To address this, we propose Self-Adaptive Graph Mixture of Models (SAGMM), a modular and practical framework that learns to automatically select and combine the most appropriate GNN models from a diverse pool of architectures. Unlike prior mixture-of-experts approaches that rely on variations of a single base model, SAGMM leverages architectural diversity and a topology-aware attention gating mechanism to adaptively assign experts to each node based on the structure of the input graph. To improve efficiency, SAGMM includes a pruning mechanism that reduces the number of active experts during training and inference without compromising performance. We also explore a training-efficient variant in which expert models are pretrained and frozen, and only the gating and task-specific layers are trained. We evaluate SAGMM on 16 benchmark datasets covering node classification, graph classification, regression, and link prediction tasks, and demonstrate that it consistently outperforms or matches leading GNN baselines and prior mixture-based methods, offering a robust and adaptive solution for real-world graph learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Self-Adaptive Graph Mixture of Models (SAGMM)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§£å†³å›¾ç¥ç»ç½‘ç»œ(GNN)æ¨¡å‹é€‰æ‹©éš¾é¢˜çš„æ¨¡å—åŒ–å®ç”¨æ¡†æ¶ã€‚é’ˆå¯¹GCNå’ŒGATç­‰ä¼ ç»Ÿæ¨¡å‹åœ¨è°ƒä¼˜åè¡¨ç°å¾€å¾€ä¼˜äºå¤æ‚SOTAæ¶æ„çš„ç°çŠ¶ï¼ŒSAGMMé€šè¿‡åˆ©ç”¨æ¶æ„å¤šæ ·æ€§å’Œæ‹“æ‰‘æ„ŸçŸ¥æ³¨æ„åŠ›é—¨æ§æœºåˆ¶(Topology-aware attention gating mechanism)ï¼Œæ ¹æ®è¾“å…¥å›¾çš„ç»“æ„ä¸ºæ¯ä¸ªèŠ‚ç‚¹è‡ªé€‚åº”åœ°åˆ†é…æœ€åˆé€‚çš„ä¸“å®¶æ¨¡å‹ã€‚ä¸ä»¥å¾€ä¾èµ–å•ä¸€åŸºç¡€æ¨¡å‹å˜ä½“çš„ä¸“å®¶æ··åˆæ–¹æ³•ä¸åŒï¼ŒSAGMMèƒ½å¤Ÿä»å¤šæ ·çš„æ¶æ„æ± ä¸­è‡ªåŠ¨é€‰æ‹©å¹¶ç»„åˆæ¨¡å‹ã€‚ä¸ºäº†æå‡æ•ˆç‡ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†å‰ªææœºåˆ¶(Pruning mechanism)ä»¥å‡å°‘è®­ç»ƒå’Œæ¨ç†æ—¶çš„æ´»è·ƒä¸“å®¶æ•°é‡ï¼Œå¹¶æ¢ç´¢äº†é¢„è®­ç»ƒå†»ç»“ä¸“å®¶çš„è®­ç»ƒé«˜æ•ˆå˜ä½“ã€‚åœ¨æ¶µç›–èŠ‚ç‚¹åˆ†ç±»ã€å›¾åˆ†ç±»ã€å›å½’å’Œé“¾æ¥é¢„æµ‹ç­‰ä»»åŠ¡çš„16ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSAGMMçš„è¡¨ç°ä¸€è‡´ä¼˜äºæˆ–ç­‰åŒäºé¢†å…ˆçš„GNNåŸºçº¿æ¨¡å‹åŠå…ˆå‰çš„æ··åˆæ–¹æ³•ï¼Œä¸ºç°å®ä¸–ç•Œçš„å›¾å­¦ä¹ æä¾›äº†é²æ£’ä¸”è‡ªé€‚åº”çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.13062v1",
      "published_date": "2025-11-17 07:11:06 UTC",
      "updated_date": "2025-11-17 07:11:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:07:38.337236+00:00"
    },
    {
      "arxiv_id": "2511.13061v1",
      "title": "MACKO: Sparse Matrix-Vector Multiplication for Low Sparsity",
      "title_zh": "MACKOï¼šé’ˆå¯¹ä½ç¨€ç–åº¦çš„ç¨€ç–çŸ©é˜µ-å‘é‡ä¹˜æ³•",
      "authors": [
        "VladimÃ­r Macko",
        "VladimÃ­r BoÅ¾a"
      ],
      "abstract": "Sparse Matrix-Vector Multiplication (SpMV) is a fundamental operation in the inference of sparse Large Language Models (LLMs). Because existing SpMV methods perform poorly under the low and unstructured sparsity (30-90%) commonly observed in pruned LLMs, unstructured pruning provided only limited memory reduction and speedup. We propose MACKO-SpMV, a GPU-optimized format and kernel co-designed to reduce storage overhead while preserving compatibility with the GPU's execution model. This enables efficient SpMV for unstructured sparsity without specialized hardware units (e.g., tensor cores) or format-specific precomputation. Empirical results show that at sparsity 50%, MACKO is the first approach with significant 1.5x memory reduction and 1.2-1.5x speedup over dense representation. Speedups over other SpMV baselines: 2.8-13.0x over cuSPARSE, 1.9-2.6x over Sputnik, and 2.2-2.5x over DASP. Applied to Llama2-7B pruned with Wanda to sparsity 50%, it delivers 1.5x memory reduction and 1.5x faster inference at fp16 precision. Thanks to MACKO, unstructured pruning at 50% sparsity is now justified in real-world LLM workloads.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å‰ªæå¤§è¯­è¨€æ¨¡å‹ (LLMs) ä¸­å¸¸è§çš„ä½ä¸”éç»“æ„åŒ–ç¨€ç–åº¦ (30-90%) ä¸‹ï¼Œç°æœ‰ç¨€ç–çŸ©é˜µå‘é‡ä¹˜æ³• (SpMV) æ–¹æ³•æ€§èƒ½ä¸ä½³çš„é—®é¢˜ï¼Œæå‡ºäº† MACKO-SpMVã€‚è¿™æ˜¯ä¸€ç§é’ˆå¯¹ GPU ä¼˜åŒ–çš„æ ¼å¼ä¸å†…æ ¸ååŒè®¾è®¡ï¼Œåœ¨æ˜¾è‘—é™ä½å­˜å‚¨å¼€é”€çš„åŒæ—¶ï¼Œä¿æŒäº†ä¸ GPU æ‰§è¡Œæ¨¡å‹çš„å…¼å®¹æ€§ï¼Œä¸”æ— éœ€ Tensor Cores ç­‰ä¸“ç”¨ç¡¬ä»¶æˆ–å¤æ‚çš„é¢„è®¡ç®—ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ 50% ç¨€ç–åº¦ä¸‹ï¼ŒMACKO ç›¸æ¯”ç¨ å¯†è¡¨ç¤ºå®ç°äº† 1.5 å€çš„å†…å­˜å‡å°‘å’Œ 1.2-1.5 å€çš„é€Ÿåº¦æå‡ï¼Œåœ¨æ€§èƒ½ä¸Šå¤§å¹…è¶…è¶Šäº† cuSPARSEã€Sputnik å’Œ DASP ç­‰ä¸»æµåŸºçº¿ã€‚åœ¨ç»è¿‡ Wanda å‰ªæçš„ Llama2-7B æ¨¡å‹åº”ç”¨ä¸­ï¼ŒMACKO åœ¨ fp16 ç²¾åº¦ä¸‹å®ç°äº† 1.5 å€çš„å†…å­˜èŠ‚çœä¸æ¨ç†åŠ é€Ÿã€‚è¯¥æˆæœè¯æ˜äº† 50% ç¨€ç–åº¦çš„éç»“æ„åŒ–å‰ªæåœ¨å®é™…å¤§è¯­è¨€æ¨¡å‹å·¥ä½œè´Ÿè½½ä¸­å…·æœ‰æ˜¾è‘—çš„å®ç”¨ä»·å€¼ä¸éƒ¨ç½²å‰æ™¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "cs.DS"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages + 7 pages appendix, 11 figures, Code available at https://github.com/vlejd/macko_spmv",
      "pdf_url": "https://arxiv.org/pdf/2511.13061v1",
      "published_date": "2025-11-17 07:10:37 UTC",
      "updated_date": "2025-11-17 07:10:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:07:37.537573+00:00"
    },
    {
      "arxiv_id": "2511.13797v1",
      "title": "MAT-MPNN: A Mobility-Aware Transformer-MPNN Model for Dynamic Spatiotemporal Prediction of HIV Diagnoses in California, Florida, and New England",
      "title_zh": "MAT-MPNNï¼šé¢å‘ California, Florida, and New England HIV è¯Šæ–­åŠ¨æ€æ—¶ç©ºé¢„æµ‹çš„ç§»åŠ¨æ„ŸçŸ¥ Transformer-MPNN æ¨¡å‹",
      "authors": [
        "Zhaoxuan Wang",
        "Weichen Kang",
        "Yutian Han",
        "Lingyuan Zhao",
        "Bo Li"
      ],
      "abstract": "Human Immunodeficiency Virus (HIV) has posed a major global health challenge for decades, and forecasting HIV diagnoses continues to be a critical area of research. However, capturing the complex spatial and temporal dependencies of HIV transmission remains challenging. Conventional Message Passing Neural Network (MPNN) models rely on a fixed binary adjacency matrix that only encodes geographic adjacency, which is unable to represent interactions between non-contiguous counties. Our study proposes a deep learning architecture Mobility-Aware Transformer-Message Passing Neural Network (MAT-MPNN) framework to predict county-level HIV diagnosis rates across California, Florida, and the New England region. The model combines temporal features extracted by a Transformer encoder with spatial relationships captured through a Mobility Graph Generator (MGG). The MGG improves conventional adjacency matrices by combining geographic and demographic information. Compared with the best-performing hybrid baseline, the Transformer MPNN model, MAT-MPNN reduced the Mean Squared Prediction Error (MSPE) by 27.9% in Florida, 39.1% in California, and 12.5% in New England, and improved the Predictive Model Choice Criterion (PMCC) by 7.7%, 3.5%, and 3.9%, respectively. MAT-MPNN also achieved better results than the Spatially Varying Auto-Regressive (SVAR) model in Florida and New England, with comparable performance in California. These results demonstrate that applying mobility-aware dynamic spatial structures substantially enhances predictive accuracy and calibration in spatiotemporal epidemiological prediction.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸º MAT-MPNN çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºé¢„æµ‹ Californiaã€Florida å’Œ New England åœ°åŒºçš„å¿çº§ HIV è¯Šæ–­ç‡ã€‚ä¸ºäº†è§£å†³ä¼ ç»Ÿ Message Passing Neural Network (MPNN) ä»…èƒ½ç¼–ç åœ°ç†é‚»æ¥æ€§è€Œå¿½ç•¥éç›¸é‚»å¿åŸŸäº¤äº’çš„é—®é¢˜ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†ç”¨äºæå–æ—¶é—´ç‰¹å¾çš„ Transformer ç¼–ç å™¨ä¸é€šè¿‡ Mobility Graph Generator (MGG) æ•æ‰ç©ºé—´å…³ç³»çš„åŠ¨æ€æœºåˆ¶ã€‚MGG é€šè¿‡æ•´åˆåœ°ç†ä¸äººå£ç»Ÿè®¡ä¿¡æ¯æ”¹è¿›äº†ä¼ ç»Ÿé‚»æ¥çŸ©é˜µï¼Œèƒ½å¤Ÿæ›´ç²¾å‡†åœ°è¡¨å¾äººå£ç§»åŠ¨å¸¦æ¥çš„æ—¶ç©ºä¾èµ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMAT-MPNN åœ¨ä¸‰ä¸ªç ”ç©¶åŒºåŸŸçš„ Mean Squared Prediction Error (MSPE) è¾ƒæœ€ä½³åŸºçº¿æ¨¡å‹åˆ†åˆ«é™ä½äº† 27.9%ã€39.1% å’Œ 12.5%ï¼Œå¹¶åœ¨ Predictive Model Choice Criterion (PMCC) æŒ‡æ ‡ä¸Šè¡¨ç°æ›´ä¼˜ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨ Florida å’Œ New England çš„æ€§èƒ½äº¦ä¼˜äº Spatially Varying Auto-Regressive (SVAR) æ¨¡å‹ã€‚è¯¥ç ”ç©¶è¯æ˜ï¼Œåœ¨æµè¡Œç—…å­¦é¢„æµ‹ä¸­åº”ç”¨æ„ŸçŸ¥ç§»åŠ¨æ€§ (mobility-aware) çš„åŠ¨æ€ç©ºé—´ç»“æ„ï¼Œå¯æ˜¾è‘—æå‡æ—¶ç©ºé¢„æµ‹çš„å‡†ç¡®æ€§ä¸æ¨¡å‹æ ¡å‡†æ•ˆæœã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.QM",
      "comment": "21 pages, 20 figures,1 table. Preprint",
      "pdf_url": "https://arxiv.org/pdf/2511.13797v1",
      "published_date": "2025-11-17 07:10:37 UTC",
      "updated_date": "2025-11-17 07:10:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:07:45.835401+00:00"
    },
    {
      "arxiv_id": "2511.13060v1",
      "title": "Latency and Ordering Effects in Online Decisions",
      "title_zh": "åœ¨çº¿å†³ç­–ä¸­çš„å»¶è¿Ÿä¸é¡ºåºæ•ˆåº”",
      "authors": [
        "Duo Yi"
      ],
      "abstract": "Online decision systems routinely operate under delayed feedback and order-sensitive (noncommutative) dynamics: actions affect which observations arrive, and in what sequence. Taking a Bregman divergence $D_Î¦$ as the loss benchmark, we prove that the excess benchmark loss admits a structured lower bound $L \\ge L_{\\mathrm{ideal}} + g_1(Î») + g_2(\\varepsilon_\\star) + g_{12}(Î»,\\varepsilon_\\star) - D_{\\mathrm{ncx}}$, where $g_1$ and $g_2$ are calibrated penalties for latency and order-sensitivity, $g_{12}$ captures their geometric interaction, and $D_{\\mathrm{ncx}}\\ge 0$ is a nonconvexity/approximation penalty that vanishes under convex Legendre assumptions. We extend this inequality to prox-regular and weakly convex settings, obtaining robust guarantees beyond the convex case. We also give an operational recipe for estimating and monitoring the four terms via simple $2\\times 2$ randomized experiments and streaming diagnostics (effective sample size, clipping rate, interaction heatmaps). The framework packages heterogeneous latency, noncommutativity, and implementation-gap effects into a single interpretable lower-bound statement that can be stress-tested and tuned in real-world systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨çº¿å†³ç­–ç³»ç»Ÿåœ¨å»¶è¿Ÿåé¦ˆ(delayed feedback)å’Œé¡ºåºæ•æ„Ÿ(order-sensitive/noncommutative)åŠ¨åŠ›å­¦ä¸‹çš„æ€§èƒ½è¡¨ç°ï¼Œå³è¡ŒåŠ¨ä¼šå½±å“è§‚æµ‹æ•°æ®çš„åˆ°è¾¾åŠå…¶åºåˆ—ã€‚ä½œè€…ä»¥Bregman divergence $D_\\Phi$ ä½œä¸ºæŸå¤±åŸºå‡†ï¼Œè¯æ˜äº†è¶…é¢æŸå¤±(excess benchmark loss)å­˜åœ¨ä¸€ä¸ªç»“æ„åŒ–çš„ä¸‹ç•Œï¼Œè¯¥ä¸‹ç•Œç”±ç†æƒ³æŸå¤±ã€å»¶è¿Ÿæƒ©ç½šé¡¹ã€é¡ºåºæ•æ„Ÿæƒ©ç½šé¡¹ã€ä¸¤è€…çš„å‡ ä½•äº¤äº’é¡¹ä»¥åŠéå‡¸æ€§æƒ©ç½šé¡¹å…±åŒç»„æˆã€‚ç ”ç©¶è¿›ä¸€æ­¥å°†è¯¥ä¸ç­‰å¼æ‰©å±•åˆ°è¿‘ç«¯æ­£åˆ™(prox-regular)å’Œå¼±å‡¸(weakly convex)è®¾ç½®ä¸­ï¼Œåœ¨éå‡¸æƒ…å†µä¸‹æä¾›äº†ç¨³å¥çš„ä¿è¯ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æä¾›äº†ä¸€å¥—é€šè¿‡ç®€å•çš„éšæœºå®éªŒå’Œæµå¼è¯Šæ–­(streaming diagnostics)æ¥ä¼°è®¡å’Œç›‘æ§è¿™äº›æŸå¤±é¡¹çš„æ“ä½œæ–¹æ¡ˆã€‚è¿™ä¸€æ¡†æ¶å°†å¼‚æ„å»¶è¿Ÿã€éäº¤æ¢æ€§åŠå®ç°å·®è·æ•ˆåº”æ•´åˆè¿›ä¸€ä¸ªç»Ÿä¸€ä¸”å¯è§£é‡Šçš„ä¸‹ç•Œé™ˆè¿°ä¸­ï¼Œä¸ºç°å®ä¸–ç•Œç³»ç»Ÿçš„å‹åŠ›æµ‹è¯•å’Œå‚æ•°è°ƒä¼˜æä¾›äº†ç†è®ºæ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13060v1",
      "published_date": "2025-11-17 07:08:05 UTC",
      "updated_date": "2025-11-17 07:08:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:07:31.831580+00:00"
    },
    {
      "arxiv_id": "2511.13057v2",
      "title": "Dimension vs. Precision: A Comparative Analysis of Autoencoders and Quantization for Efficient Vector Retrieval on BEIR SciFact",
      "title_zh": "ç»´åº¦ä¸ç²¾åº¦ï¼šé¢å‘ BEIR SciFact é«˜æ•ˆå‘é‡æ£€ç´¢çš„è‡ªåŠ¨ç¼–ç å™¨ä¸é‡åŒ–å¯¹æ¯”åˆ†æ",
      "authors": [
        "Satyanarayan Pati"
      ],
      "abstract": "Dense retrieval models have become a standard for state-of-the-art information retrieval. However, their high-dimensional, high-precision (float32) vector embeddings create significant storage and memory challenges for real-world deployment. To address this, we conduct a rigorous empirical study on the BEIR SciFact benchmark, evaluating the trade-offs between two primary compression strategies: (1) Dimensionality Reduction via deep Autoencoders (AE), reducing original 384-dim vectors to latent spaces from 384 down to 12, and (2) Precision Reduction via Quantization (float16, int8, and binary). We systematically compare each method by measuring the \"performance loss\" (or gain) relative to a float32 baseline across a full suite of retrieval metrics (NDCG, MAP, MRR, Recall, Precision) at various k cutoffs. Our results show that int8 scalar quantization provides the most effective \"sweet spot,\" achieving a 4x compression with a negligible [~1-2%] drop in nDCG@10. In contrast, Autoencoders show a graceful degradation but suffer a more significant performance loss at equivalent 4x compression ratios (AE-96). binary quantization was found to be unsuitable for this task due to catastrophic performance drops. This work provides a practical guide for deploying efficient, high-performance retrieval systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜ç»´é«˜ç²¾åº¦ï¼ˆfloat32ï¼‰å‘é‡åµŒå…¥åœ¨å®é™…éƒ¨ç½²ä¸­é¢ä¸´çš„å­˜å‚¨å’Œå†…å­˜æŒ‘æˆ˜ï¼Œåœ¨ BEIR SciFact åŸºå‡†æµ‹è¯•ä¸Šå¯¹åŸºäºæ·±åº¦ Autoencoders (AE) çš„é™ç»´æŠ€æœ¯ä¸ç²¾åº¦é‡åŒ–ï¼ˆQuantizationï¼‰ç­–ç•¥è¿›è¡Œäº†ç³»ç»Ÿçš„æ¯”è¾ƒç ”ç©¶ã€‚é€šè¿‡è¯„ä¼° NDCGã€MAP å’Œ Recall ç­‰æ ¸å¿ƒæ£€ç´¢æŒ‡æ ‡ï¼Œç ”ç©¶å‘ç° int8 æ ‡é‡é‡åŒ–åœ¨æä¾› 4 å€å‹ç¼©ç‡çš„åŒæ—¶ï¼Œä»…å¯¼è‡´çº¦ 1-2% çš„ nDCG@10 æ€§èƒ½ä¸‹é™ï¼Œæ˜¯æ•ˆç‡ä¸ç²¾åº¦çš„æœ€ä½³å¹³è¡¡ç‚¹ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒAutoencoders åœ¨åŒç­‰å‹ç¼©å€ç‡ä¸‹è¡¨ç°å‡ºæ›´æ˜¾è‘—çš„æ€§èƒ½è¡°å‡ï¼Œè€Œ binary é‡åŒ–åˆ™å› æ£€ç´¢æ•ˆèƒ½çš„æ¯ç­æ€§ä¸‹é™è€Œè¢«è¯æ˜ä¸é€‚åˆè¯¥ä»»åŠ¡ã€‚è¯¥å·¥ä½œé‡åŒ–äº†ä¸åŒå‹ç¼©æ–¹æ³•çš„æ€§èƒ½æƒè¡¡ï¼ˆtrade-offsï¼‰ï¼Œä¸ºå¼€å‘è€…éƒ¨ç½²é«˜æ•ˆä¸”é«˜æ€§èƒ½çš„å‘é‡æ£€ç´¢ç³»ç»Ÿæä¾›äº†å…³é”®çš„å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "16 pages, 9 figures, 1 table",
      "pdf_url": "https://arxiv.org/pdf/2511.13057v2",
      "published_date": "2025-11-17 07:02:11 UTC",
      "updated_date": "2025-11-18 16:07:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:07:39.837574+00:00"
    },
    {
      "arxiv_id": "2511.13052v1",
      "title": "Learning from the Undesirable: Robust Adaptation of Language Models without Forgetting",
      "title_zh": "ä»ä¸å¯å–ä¸­å­¦ä¹ ï¼šæ— é—å¿˜çš„è¯­è¨€æ¨¡å‹é²æ£’é€‚é…",
      "authors": [
        "Yunhun Nam",
        "Jaehyung Kim",
        "Jongheon Jeong"
      ],
      "abstract": "Language models (LMs) are often adapted through supervised fine-tuning (SFT) to specialize their capabilities for downstream tasks. However, in typical scenarios where the fine-tuning data is limited, e.g., compared to pre-training, SFT can lead LMs to overfit, causing them to rely on spurious patterns within the target task or to compromise other broadly useful capabilities as a side effect of narrow specialization. In this paper, we propose Learning-from-the-Undesirable (LfU), a simple yet effective regularization scheme for SFT to mitigate overfitting issues when fine-tuning LMs with limited data. Specifically, we aim to regularize the fine-tuning process to favor solutions that are resilient to \"undesirable\" model updates, e.g., gradient ascent steps that steer the model toward undesirable behaviors. To this end, we propose a novel form of consistency regularization that directly aligns internal representations of the model with those after an undesirable update. By leveraging representation-level data augmentation through undesirable updates, LfU effectively promotes generalization under limited data. Our experiments on diverse LM downstream tasks show that LfU serves as an effective prior that enhances adaptability while preserving pretrained knowledge. For example, our LM from LfU achieves a 16.8% average improvement on math tasks compared to vanilla SFT on the same dataset, where the latter even leads to degraded performance on those tasks. Furthermore, LfU exhibits improved robustness to prompt variations, e.g., yielding a 92.1% lower standard deviation in output performances compared to SFT, highlighting its versatile effects.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯­è¨€æ¨¡å‹åœ¨æœ‰é™æ•°æ®ä¸‹è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒ (Supervised Fine-Tuning, SFT) æ—¶å®¹æ˜“äº§ç”Ÿçš„è¿‡æ‹Ÿåˆä¸ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º Learning-from-the-Undesirable (LfU) çš„æ­£åˆ™åŒ–æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥ä¸€ç§æ–°å‹çš„ä¸€è‡´æ€§æ­£åˆ™åŒ– (consistency regularization)ï¼Œå°†æ¨¡å‹çš„å†…éƒ¨è¡¨ç¤ºä¸ç»å†è¿‡â€œä¸è‰¯â€æ›´æ–°ï¼ˆå¦‚å¯¼å‘é”™è¯¯è¡Œä¸ºçš„æ¢¯åº¦ä¸Šå‡æ­¥éª¤ï¼‰åçš„è¡¨ç¤ºç›´æ¥å¯¹é½ï¼Œä»è€Œåœ¨è¡¨å¾å±‚é¢å®ç°æ•°æ®å¢å¼ºã€‚è¿™ç§ç­–ç•¥æœ‰æ•ˆä¿ƒè¿›äº†æ¨¡å‹åœ¨æœ‰é™æ•°æ®ä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶åœ¨é€‚åº”ä¸‹æ¸¸ä»»åŠ¡çš„åŒæ—¶èƒ½å¤Ÿä¿ç•™å¹¿ä¹‰çš„é¢„è®­ç»ƒèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLfU åœ¨æ•°å­¦ä»»åŠ¡ä¸Šç›¸æ¯”ä¼ ç»Ÿ SFT å®ç°äº† 16.8% çš„å¹³å‡æ€§èƒ½æå‡ï¼Œå¹¶æˆåŠŸæ‰­è½¬äº† SFT åœ¨æŸäº›ä»»åŠ¡ä¸Šå¯¼è‡´çš„æ€§èƒ½é€€åŒ–ç°è±¡ã€‚æ­¤å¤–ï¼ŒLfU è¿˜è¡¨ç°å‡ºæå¼ºçš„æç¤ºè¯é²æ£’æ€§ï¼Œå…¶è¾“å‡ºæ€§èƒ½çš„æ ‡å‡†å·®æ¯” SFT é™ä½äº† 92.1%ï¼Œå……åˆ†è¯æ˜äº†è¯¥æ–¹æ³•åœ¨æå‡è¯­è¨€æ¨¡å‹é€‚åº”æ€§ä¸ç¨³å®šæ€§æ–¹é¢çš„æ˜¾è‘—æ•ˆæœã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages; AAAI 2026; Code is available at https://github.com/yunpal/LfU",
      "pdf_url": "https://arxiv.org/pdf/2511.13052v1",
      "published_date": "2025-11-17 06:57:44 UTC",
      "updated_date": "2025-11-17 06:57:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:07:45.129877+00:00"
    },
    {
      "arxiv_id": "2511.13035v1",
      "title": "One-Step Generative Policies with Q-Learning: A Reformulation of MeanFlow",
      "title_zh": "åŸºäºQå­¦ä¹ çš„å•æ­¥ç”Ÿæˆç­–ç•¥ï¼šMeanFlowçš„é‡æ„",
      "authors": [
        "Zeyuan Wang",
        "Da Li",
        "Yulin Chen",
        "Ye Shi",
        "Liang Bai",
        "Tianyuan Yu",
        "Yanwei Fu"
      ],
      "abstract": "We introduce a one-step generative policy for offline reinforcement learning that maps noise directly to actions via a residual reformulation of MeanFlow, making it compatible with Q-learning. While one-step Gaussian policies enable fast inference, they struggle to capture complex, multimodal action distributions. Existing flow-based methods improve expressivity but typically rely on distillation and two-stage training when trained with Q-learning. To overcome these limitations, we propose to reformulate MeanFlow to enable direct noise-to-action generation by integrating the velocity field and noise-to-action transformation into a single policy network-eliminating the need for separate velocity estimation. We explore several reformulation variants and identify an effective residual formulation that supports expressive and stable policy learning. Our method offers three key advantages: 1) efficient one-step noise-to-action generation, 2) expressive modelling of multimodal action distributions, and 3) efficient and stable policy learning via Q-learning in a single-stage training setup. Extensive experiments on 73 tasks across the OGBench and D4RL benchmarks demonstrate that our method achieves strong performance in both offline and offline-to-online reinforcement learning settings. Code is available at https://github.com/HiccupRL/MeanFlowQL.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† One-Step Generative Policiesï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ç¦»çº¿å¼ºåŒ–å­¦ä¹  (Offline Reinforcement Learning) çš„å•æ­¥ç”Ÿæˆç­–ç•¥ï¼Œé€šè¿‡å¯¹ MeanFlow è¿›è¡Œæ®‹å·®é‡æ„ (Residual Reformulation) å®ç°äº†ä»å™ªå£°åˆ°åŠ¨ä½œçš„ç›´æ¥æ˜ å°„ï¼Œä½¿å…¶èƒ½å¤Ÿä¸ Q-learning å®Œç¾å…¼å®¹ã€‚è¯¥æ–¹æ³•æ—¨åœ¨è§£å†³ä¼ ç»Ÿå•æ­¥é«˜æ–¯ç­–ç•¥ (Gaussian Policies) éš¾ä»¥æ•æ‰å¤æ‚å¤šå³°åŠ¨ä½œåˆ†å¸ƒçš„é—®é¢˜ï¼Œå¹¶å…‹æœäº†ç°æœ‰æµæ¨¡å‹ (Flow-based Methods) åœ¨ç»“åˆ Q-learning æ—¶é€šå¸¸éœ€è¦è’¸é¦æˆ–ä¸¤é˜¶æ®µè®­ç»ƒçš„å±€é™æ€§ã€‚é€šè¿‡å°†é€Ÿåº¦åœº (Velocity Field) æ•´åˆåˆ°å•ä¸€ç­–ç•¥ç½‘ç»œä¸­ï¼Œè¯¥æ¡†æ¶æ¶ˆé™¤äº†ç‹¬ç«‹é€Ÿåº¦ä¼°è®¡çš„éœ€æ±‚ï¼Œæ”¯æŒåœ¨å•é˜¶æ®µè®­ç»ƒè®¾ç½®ä¸‹è¿›è¡Œé«˜æ•ˆä¸”ç¨³å®šçš„ç­–ç•¥å­¦ä¹ ã€‚è¯¥æ–¹æ³•å…·æœ‰å•æ­¥é«˜æ•ˆç”Ÿæˆã€å¤šå³°åˆ†å¸ƒå»ºæ¨¡èƒ½åŠ›å¼ºä»¥åŠ Q-learning æ¡†æ¶ä¸‹è®­ç»ƒç¨³å®šæ€§é«˜ç­‰æ ¸å¿ƒä¼˜åŠ¿ã€‚åœ¨ OGBench å’Œ D4RL åŸºå‡†æµ‹è¯•çš„ 73 ä¸ªä»»åŠ¡ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç¦»çº¿åŠç¦»çº¿åˆ°åœ¨çº¿ (Offline-to-Online) å¼ºåŒ–å­¦ä¹ åœºæ™¯ä¸­å‡å–å¾—äº†å“è¶Šçš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted in AAAI 2026 Poster",
      "pdf_url": "https://arxiv.org/pdf/2511.13035v1",
      "published_date": "2025-11-17 06:34:17 UTC",
      "updated_date": "2025-11-17 06:34:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:07:58.531224+00:00"
    },
    {
      "arxiv_id": "2511.13029v1",
      "title": "AA-Omniscience: Evaluating Cross-Domain Knowledge Reliability in Large Language Models",
      "title_zh": "AA-Omniscienceï¼šå¤§è¯­è¨€æ¨¡å‹è·¨é¢†åŸŸçŸ¥è¯†å¯é æ€§è¯„ä¼°",
      "authors": [
        "Declan Jackson",
        "William Keating",
        "George Cameron",
        "Micah Hill-Smith"
      ],
      "abstract": "Existing language model evaluations primarily measure general capabilities, yet reliable use of these models across a range of domains demands factual accuracy and recognition of knowledge gaps. We introduce AA-Omniscience, a benchmark designed to measure both factual recall and knowledge calibration across 6,000 questions. Questions are derived from authoritative academic and industry sources, and cover 42 economically relevant topics within six different domains. The evaluation measures a model's Omniscience Index, a bounded metric (-100 to 100) measuring factual recall that jointly penalizes hallucinations and rewards abstention when uncertain, with 0 equating to a model that answers questions correctly as much as it does incorrectly. Among evaluated models, Claude 4.1 Opus attains the highest score (4.8), making it one of only three models to score above zero. These results reveal persistent factuality and calibration weaknesses across frontier models. Performance also varies by domain, with the models from three different research labs leading across the six domains. This performance variability suggests models should be chosen according to the demands of the use case rather than general performance for tasks where knowledge is important.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† AA-Omniscienceï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ (Large Language Models) è·¨é¢†åŸŸçŸ¥è¯†å¯é æ€§ä¸çŸ¥è¯†æ ¡å‡† (Knowledge Calibration) èƒ½åŠ›çš„æ–°å‹åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«æºè‡ªæƒå¨æœºæ„çš„ 6,000 ä¸ªæ¶µç›– 42 ä¸ªä¸»é¢˜çš„é—®é¢˜ã€‚è¯¥åŸºå‡†é€šè¿‡ Omniscience Index è¿™ä¸€å—é™æŒ‡æ ‡æ¥ç»¼åˆè¡¡é‡äº‹å®æ£€ç´¢ (Factual Recall) æ°´å¹³ï¼Œè¯¥æŒ‡æ ‡ä¸ä»…å¥–åŠ±æ­£ç¡®å›ç­”ï¼Œè¿˜é€šè¿‡æƒ©ç½šå¹»è§‰ (Hallucination) å’Œå¥–åŠ±ä¸ç¡®å®šæ—¶çš„å¼ƒç­” (Abstention) æ¥è¯„ä¼°æ¨¡å‹çš„è‡ªæˆ‘è®¤çŸ¥ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå½“å‰å‰æ²¿æ¨¡å‹åœ¨äº‹å®æ€§å’Œæ ¡å‡†æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—ç¼ºé™·ï¼Œä»…æœ‰åŒ…æ‹¬ Claude 4.1 Opus åœ¨å†…çš„ä¸‰ä¸ªæ¨¡å‹å¾—åˆ†è¶…è¿‡é›¶ã€‚ç”±äºæ¨¡å‹åœ¨å…­ä¸ªä¸åŒé¢†åŸŸçš„è¡¨ç°å·®å¼‚å·¨å¤§ï¼Œè¯¥ç ”ç©¶ç»“æœè¡¨æ˜åœ¨çŸ¥è¯†é©±åŠ¨çš„ä»»åŠ¡ä¸­ï¼Œåº”æ ¹æ®å…·ä½“çš„åº”ç”¨é¢†åŸŸéœ€æ±‚è€Œéé€šç”¨æ€§èƒ½æŒ‡æ ‡æ¥é€‰æ‹©åˆé€‚çš„æ¨¡å‹ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13029v1",
      "published_date": "2025-11-17 06:27:16 UTC",
      "updated_date": "2025-11-17 06:27:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:08:15.148980+00:00"
    },
    {
      "arxiv_id": "2511.13027v1",
      "title": "Scaling Generative Verifiers For Natural Language Mathematical Proof Verification And Selection",
      "title_zh": "æ‰©å±•ç”¨äºè‡ªç„¶è¯­è¨€æ•°å­¦è¯æ˜éªŒè¯ä¸é€‰æ‹©çš„ç”Ÿæˆå¼éªŒè¯å™¨",
      "authors": [
        "Sadegh Mahdavi",
        "Branislav Kisacanin",
        "Shubham Toshniwal",
        "Wei Du",
        "Ivan Moshkov",
        "George Armstrong",
        "Renjie Liao",
        "Christos Thrampoulidis",
        "Igor Gitman"
      ],
      "abstract": "Large language models have achieved remarkable success on final-answer mathematical problems, largely due to the ease of applying reinforcement learning with verifiable rewards. However, the reasoning underlying these solutions is often flawed. Advancing to rigorous proof-based mathematics requires reliable proof verification capabilities. We begin by analyzing multiple evaluation setups and show that focusing on a single benchmark can lead to brittle or misleading conclusions. To address this, we evaluate both proof-based and final-answer reasoning to obtain a more reliable measure of model performance. We then scale two major generative verification methods (GenSelect and LLM-as-a-Judge) to millions of tokens and identify their combination as the most effective framework for solution verification and selection. We further show that the choice of prompt for LLM-as-a-Judge significantly affects the model's performance, but reinforcement learning can reduce this sensitivity. However, despite improving proof-level metrics, reinforcement learning does not enhance final-answer precision, indicating that current models often reward stylistic or procedural correctness rather than mathematical validity. Our results establish practical guidelines for designing and evaluating scalable proof-verification and selection systems.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶æ¢è®¨äº†å¦‚ä½•æ‰©å±•ç”Ÿæˆå¼éªŒè¯å™¨(Generative Verifiers)ä»¥å®ç°è‡ªç„¶è¯­è¨€æ•°å­¦è¯æ˜çš„éªŒè¯ä¸é€‰æ‹©ã€‚é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨è§£å†³æ•°å­¦é—®é¢˜æ—¶æ¨ç†è¿‡ç¨‹å¸¸å­˜åœ¨ç¼ºé™·çš„é—®é¢˜ï¼Œç ”ç©¶è€…é€šè¿‡åˆ†æå¤šç§è¯„ä¼°è®¾ç½®ï¼Œæå‡ºç»“åˆè¯æ˜æ¨ç†å’Œæœ€ç»ˆç­”æ¡ˆæ¨ç†çš„è¯„ä¼°æ–¹æ³•ä»¥è·å¾—æ›´å¯é çš„æ€§èƒ½è¡¡é‡ã€‚ç ”ç©¶é‡ç‚¹æ‰©å±•äº†GenSelectå’ŒLLM-as-a-Judgeä¸¤ç§ç”Ÿæˆå¼éªŒè¯æ–¹æ³•ï¼Œå¹¶å‘ç°å°†ä¸¤è€…ç»“åˆæ˜¯ç›®å‰æœ€æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆéªŒè¯ä¸é€‰æ‹©æ¡†æ¶ã€‚å®éªŒè¡¨æ˜ï¼ŒPromptçš„é€‰æ‹©ä¼šæ˜¾è‘—å½±å“LLM-as-a-Judgeçš„æ€§èƒ½ï¼Œè€Œå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)èƒ½æœ‰æ•ˆé™ä½è¿™ç§æ•æ„Ÿæ€§ã€‚å°½ç®¡å¼ºåŒ–å­¦ä¹ æå‡äº†è¯æ˜å±‚é¢çš„æŒ‡æ ‡ï¼Œä½†å¹¶æœªå¢å¼ºæœ€ç»ˆç­”æ¡ˆçš„ç²¾åº¦ï¼Œè¿™è¡¨æ˜å½“å‰æ¨¡å‹å¾€å¾€æ›´å€¾å‘äºå¥–åŠ±é£æ ¼æˆ–ç¨‹åºä¸Šçš„æ­£ç¡®æ€§ï¼Œè€Œéæ•°å­¦é€»è¾‘çš„çœŸå®æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶ä¸ºè®¾è®¡å’Œè¯„ä¼°å¯æ‰©å±•çš„è¯æ˜éªŒè¯ä¸é€‰æ‹©ç³»ç»Ÿæä¾›äº†å®ç”¨æŒ‡å—ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13027v1",
      "published_date": "2025-11-17 06:25:35 UTC",
      "updated_date": "2025-11-17 06:25:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:07:55.637918+00:00"
    },
    {
      "arxiv_id": "2511.13023v1",
      "title": "SLMQuant:Benchmarking Small Language Model Quantization for Practical Deployment",
      "title_zh": "SLMQuantï¼šé¢å‘å®é™…éƒ¨ç½²çš„å°è¯­è¨€æ¨¡å‹é‡åŒ–åŸºå‡†æµ‹è¯•",
      "authors": [
        "Jiacheng Wang",
        "Yejun Zeng",
        "Jinyang Guo",
        "Yuqing Ma",
        "Aishan Liu",
        "Xianglong Liu"
      ],
      "abstract": "Despite the growing interest in Small Language Models (SLMs) as resource-efficient alternatives to Large Language Models (LLMs), their deployment on edge devices remains challenging due to unresolved efficiency gaps in model compression. While quantization has proven effective for LLMs, its applicability to SLMs is significantly underexplored, with critical questions about differing quantization bottlenecks and efficiency profiles. This paper introduces SLMQuant, the first systematic benchmark for evaluating LLM compression techniques when applied to SLMs. Through comprehensive multi-track evaluations across diverse architectures and tasks, we analyze how state-of-the-art quantization methods perform on SLMs. Our findings reveal fundamental disparities between SLMs and LLMs in quantization sensitivity, demonstrating that direct transfer of LLM-optimized techniques leads to suboptimal results due to SLMs' unique architectural characteristics and training dynamics. We identify key factors governing effective SLM quantization and propose actionable design principles for SLM-tailored compression. SLMQuant establishes a foundational framework for advancing efficient SLM deployment on low-end devices in edge applications, and provides critical insights for deploying lightweight language models in resource-constrained scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†SLMQuantï¼Œè¿™æ˜¯é¦–ä¸ªç³»ç»Ÿæ€§è¯„ä¼°å°è¯­è¨€æ¨¡å‹(SLMs)é‡åŒ–(Quantization)å‹ç¼©æŠ€æœ¯çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ã€‚é’ˆå¯¹SLMsåœ¨è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²æ—¶é¢ä¸´çš„å‹ç¼©æ•ˆç‡ç“¶é¢ˆï¼Œç ”ç©¶é€šè¿‡è·¨å¤šç§æ¶æ„å’Œä»»åŠ¡çš„å¤šè½¨é“è¯„ä¼°ï¼Œæ·±å…¥åˆ†æäº†æœ€å…ˆè¿›çš„é‡åŒ–æ–¹æ³•åœ¨SLMsä¸Šçš„è¡¨ç°ã€‚å®éªŒå‘ç°SLMsä¸å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨é‡åŒ–æ•æ„Ÿåº¦(Quantization Sensitivity)ä¸Šå­˜åœ¨æ ¹æœ¬å·®å¼‚ï¼Œç”±äºSLMsç‹¬ç‰¹çš„æ¶æ„ç‰¹å¾å’Œè®­ç»ƒåŠ¨åŠ›å­¦ï¼Œç›´æ¥è¿ç§»é’ˆå¯¹LLMä¼˜åŒ–çš„æŠ€æœ¯å¾€å¾€åªèƒ½å¾—åˆ°æ¬¡ä¼˜ç»“æœã€‚åŸºäºæ­¤ï¼Œç ”ç©¶è¯†åˆ«äº†å½±å“SLMé‡åŒ–æ•ˆæœçš„å…³é”®å› ç´ ï¼Œå¹¶æå‡ºäº†ä¸“é—¨é’ˆå¯¹SLMå®šåˆ¶çš„å‹ç¼©è®¾è®¡åŸåˆ™ã€‚SLMQuantä¸ºåœ¨èµ„æºå—é™çš„è¾¹ç¼˜åœºæ™¯ä¸­å®ç°é«˜æ•ˆçš„SLMéƒ¨ç½²å¥ å®šäº†åŸºç¡€ï¼Œå¹¶ä¸ºè½»é‡åŒ–è¯­è¨€æ¨¡å‹çš„å®é™…åº”ç”¨æä¾›äº†å…³é”®æ´å¯Ÿã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13023v1",
      "published_date": "2025-11-17 06:20:33 UTC",
      "updated_date": "2025-11-17 06:20:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:08:28.340259+00:00"
    },
    {
      "arxiv_id": "2511.13021v1",
      "title": "PragWorld: A Benchmark Evaluating LLMs' Local World Model under Minimal Linguistic Alterations and Conversational Dynamics",
      "title_zh": "PragWorldï¼šè¯„ä¼°æœ€å°è¯­è¨€æ‰°åŠ¨ä¸å¯¹è¯åŠ¨æ€ä¸‹å¤§è¯­è¨€æ¨¡å‹å±€éƒ¨ä¸–ç•Œæ¨¡å‹çš„åŸºå‡†",
      "authors": [
        "Sachin Vashistha",
        "Aryan Bibhuti",
        "Atharva Naik",
        "Martin Tutek",
        "Somak Aditya"
      ],
      "abstract": "Real-world conversations are rich with pragmatic elements, such as entity mentions, references, and implicatures. Understanding such nuances is a requirement for successful natural communication, and often requires building a local world model which encodes such elements and captures the dynamics of their evolving states. However, it is not well-understood whether language models (LMs) construct or maintain a robust implicit representation of conversations. In this work, we evaluate the ability of LMs to encode and update their internal world model in dyadic conversations and test their malleability under linguistic alterations. To facilitate this, we apply seven minimal linguistic alterations to conversations sourced from popular datasets and construct two benchmarks comprising yes-no questions. We evaluate a wide range of open and closed source LMs and observe that they struggle to maintain robust accuracy. Our analysis unveils that LMs struggle to memorize crucial details, such as tracking entities under linguistic alterations to conversations. We then propose a dual-perspective interpretability framework which identifies transformer layers that are useful or harmful and highlights linguistic alterations most influenced by harmful layers, typically due to encoding spurious signals or relying on shortcuts. Inspired by these insights, we propose two layer-regularization based fine-tuning strategies that suppress the effect of the harmful layers.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PragWorldï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨äºŒå…ƒå¯¹è¯ä¸­åº”å¯¹å¾®å°è¯­è¨€å˜åŒ– (Minimal Linguistic Alterations) å¹¶æ„å»ºæœ¬åœ°ä¸–ç•Œæ¨¡å‹ (Local World Model) èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡åœ¨ç°æœ‰å¯¹è¯æ•°æ®é›†ä¸­å¼•å…¥ä¸ƒç§å¾®å°çš„è¯­è¨€å˜ä½“ï¼Œæ„å»ºäº†ä¸¤ä¸ªåŒ…å«æ˜¯éé¢˜çš„æµ‹è¯•é›†ï¼Œä»¥æ£€éªŒæ¨¡å‹æ•è·å¯¹è¯åŠ¨æ€çš„èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ— è®ºæ˜¯å¼€æºè¿˜æ˜¯é—­æº LMsï¼Œåœ¨é¢å¯¹è¯­è¨€å˜ä½“æ—¶éƒ½éš¾ä»¥ä¿æŒç¨³å¥çš„å‡†ç¡®æ€§ï¼Œä¸”åœ¨è¿½è¸ªå®ä½“ (Tracking Entities) ç­‰å…³é”®ç»†èŠ‚çš„è®°å¿†ä¸Šè¡¨ç°æ¬ ä½³ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ä¸ªåŒè§†è§’çš„å¯è§£é‡Šæ€§æ¡†æ¶ (Dual-perspective Interpretability Framework)ï¼Œç”¨ä»¥è¯†åˆ« Transformer æ¶æ„ä¸­æä¾›æœ‰ç”¨ä¿¡æ¯æˆ–äº§ç”Ÿå¹²æ‰°çš„æœ‰å®³å±‚ã€‚åˆ†æå‘ç°ï¼Œæœ‰å®³å±‚å¾€å¾€å› ä¾èµ–ä¼ªä¿¡å·æˆ–æ·å¾„è€Œå¯¼è‡´æ¨¡å‹é²æ£’æ€§ä¸‹é™ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œç ”ç©¶è€…è¿›ä¸€æ­¥æå‡ºäº†ä¸¤ç§åŸºäºå±‚æ­£åˆ™åŒ– (Layer-regularization) çš„å¾®è°ƒç­–ç•¥ï¼Œé€šè¿‡æŠ‘åˆ¶æœ‰å®³å±‚çš„å½±å“ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤æ‚å¯¹è¯åœºæ™¯ä¸‹çš„å»ºæ¨¡èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "23 pages, 15 tables, 10 figures; AAAI 2026 Conference Main Track (oral)",
      "pdf_url": "https://arxiv.org/pdf/2511.13021v1",
      "published_date": "2025-11-17 06:17:17 UTC",
      "updated_date": "2025-11-17 06:17:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:08:34.349162+00:00"
    },
    {
      "arxiv_id": "2511.13020v1",
      "title": "SpectralAdapt: Semi-Supervised Domain Adaptation with Spectral Priors for Human-Centered Hyperspectral Image Reconstruction",
      "title_zh": "SpectralAdaptï¼šç»“åˆå…‰è°±å…ˆéªŒçš„åŠç›‘ç£é¢†åŸŸè‡ªé€‚åº”æ–¹æ³•ï¼Œç”¨äºé¢å‘äººä½“çš„é«˜å…‰è°±å›¾åƒé‡å»º",
      "authors": [
        "Yufei Wen",
        "Yuting Zhang",
        "Jingdan Kang",
        "Hao Ren",
        "Weibin Cheng",
        "Jintai Chen",
        "Kaishun Wu"
      ],
      "abstract": "Hyperspectral imaging (HSI) holds great potential for healthcare due to its rich spectral information. However, acquiring HSI data remains costly and technically demanding. Hyperspectral image reconstruction offers a practical solution by recovering HSI data from accessible modalities, such as RGB. While general domain datasets are abundant, the scarcity of human HSI data limits progress in medical applications. To tackle this, we propose SpectralAdapt, a semi-supervised domain adaptation (SSDA) framework that bridges the domain gap between general and human-centered HSI datasets. To fully exploit limited labels and abundant unlabeled data, we enhance spectral reasoning by introducing Spectral Density Masking (SDM), which adaptively masks RGB channels based on their spectral complexity, encouraging recovery of informative regions from complementary cues during consistency training. Furthermore, we introduce Spectral Endmember Representation Alignment (SERA), which derives physically interpretable endmembers from valuable labeled pixels and employs them as domain-invariant anchors to guide unlabeled predictions, with momentum updates ensuring adaptability and stability. These components are seamlessly integrated into SpectralAdapt, a spectral prior-guided framework that effectively mitigates domain shift, spectral degradation, and data scarcity in HSI reconstruction. Experiments on benchmark datasets demonstrate consistent improvements in spectral fidelity, cross-domain generalization, and training stability, highlighting the promise of SSDA as an efficient solution for hyperspectral imaging in healthcare.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SpectralAdaptï¼Œä¸€ç§åŸºäºå…‰è°±å…ˆéªŒçš„åŠç›‘ç£é¢†åŸŸè‡ªé€‚åº” (Semi-Supervised Domain Adaptation, SSDA) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä» RGB å›¾åƒé‡å»ºä»¥äººä¸ºä¸­å¿ƒçš„é«˜å…‰è°±å›¾åƒ (Hyperspectral Image Reconstruction) æ—¶é¢ä¸´çš„æ•°æ®ç¨€ç¼ºå’Œé¢†åŸŸåå·®é—®é¢˜ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨æœ‰é™çš„æ ‡ç­¾å’Œå¤§é‡æ— æ ‡æ³¨æ•°æ®ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†å…‰è°±å¯†åº¦æ©ç  (Spectral Density Masking, SDM)ï¼Œæ ¹æ®å…‰è°±å¤æ‚åº¦è‡ªé€‚åº”æ©ç›– RGB é€šé“ï¼Œä»¥åœ¨ä¸€è‡´æ€§è®­ç»ƒä¸­å¢å¼ºæ¨¡å‹å¯¹å…³é”®ä¿¡æ¯çš„æ¢å¤èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†å…‰è°±ç«¯å…ƒè¡¨ç¤ºå¯¹é½ (Spectral Endmember Representation Alignment, SERA)ï¼Œåˆ©ç”¨å…·æœ‰ç‰©ç†å¯è§£é‡Šæ€§çš„ç«¯å…ƒä½œä¸ºé¢†åŸŸä¸å˜çš„é”šç‚¹ï¼Œå¹¶é€šè¿‡åŠ¨é‡æ›´æ–°å¼•å¯¼æ— æ ‡ç­¾æ•°æ®çš„é¢„æµ‹ã€‚SpectralAdapt é€šè¿‡é›†æˆè¿™äº›ç»„ä»¶ï¼Œæœ‰æ•ˆåœ°ç¼“è§£äº†é¢†åŸŸåç§»ã€å…‰è°±é€€åŒ–å’Œ HSI æ•°æ®ä¸è¶³ã€‚åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…‰è°±ä¿çœŸåº¦ã€è·¨é¢†åŸŸæ³›åŒ–å’Œè®­ç»ƒç¨³å®šæ€§æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ï¼Œä¸ºåŒ»ç–—é«˜å…‰è°±æˆåƒæä¾›äº†ä¸€ç§é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13020v1",
      "published_date": "2025-11-17 06:17:13 UTC",
      "updated_date": "2025-11-17 06:17:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:08:56.832081+00:00"
    },
    {
      "arxiv_id": "2511.13019v1",
      "title": "MeanFlow Transformers with Representation Autoencoders",
      "title_zh": "åŸºäºè¡¨å¾è‡ªç¼–ç å™¨çš„ MeanFlow Transformer",
      "authors": [
        "Zheyuan Hu",
        "Chieh-Hsin Lai",
        "Ge Wu",
        "Yuki Mitsufuji",
        "Stefano Ermon"
      ],
      "abstract": "MeanFlow (MF) is a diffusion-motivated generative model that enables efficient few-step generation by learning long jumps directly from noise to data. In practice, it is often used as a latent MF by leveraging the pre-trained Stable Diffusion variational autoencoder (SD-VAE) for high-dimensional data modeling. However, MF training remains computationally demanding and is often unstable. During inference, the SD-VAE decoder dominates the generation cost, and MF depends on complex guidance hyperparameters for class-conditional generation. In this work, we develop an efficient training and sampling scheme for MF in the latent space of a Representation Autoencoder (RAE), where a pre-trained vision encoder (e.g., DINO) provides semantically rich latents paired with a lightweight decoder. We observe that naive MF training in the RAE latent space suffers from severe gradient explosion. To stabilize and accelerate training, we adopt Consistency Mid-Training for trajectory-aware initialization and use a two-stage scheme: distillation from a pre-trained flow matching teacher to speed convergence and reduce variance, followed by an optional bootstrapping stage with a one-point velocity estimator to further reduce deviation from the oracle mean flow. This design removes the need for guidance, simplifies training configurations, and reduces computation in both training and sampling. Empirically, our method achieves a 1-step FID of 2.03, outperforming vanilla MF's 3.43, while reducing sampling GFLOPS by 38% and total training cost by 83% on ImageNet 256. We further scale our approach to ImageNet 512, achieving a competitive 1-step FID of 3.23 with the lowest GFLOPS among all baselines. Code is available at https://github.com/sony/mf-rae.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åœ¨ Representation Autoencoder (RAE) æ½œç©ºé—´ä¸­è®­ç»ƒå’Œé‡‡æ · MeanFlow (MF) æ¨¡å‹çš„é«˜æ•ˆæ–¹æ¡ˆï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ–¹æ³•åœ¨è®­ç»ƒç¨³å®šæ€§ã€è®¡ç®—æˆæœ¬åŠå¯¹å¤æ‚å¼•å¯¼å‚æ•°ä¾èµ–æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ DINO ç­‰é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨æ„å»ºè¯­ä¹‰ä¸°å¯Œçš„æ½œç©ºé—´ï¼Œå¹¶é…åˆè½»é‡åŒ–è§£ç å™¨ä»¥æ˜¾è‘—é™ä½æ¨ç†æ—¶çš„è®¡ç®—å¼€é”€ã€‚é’ˆå¯¹ RAE æ½œç©ºé—´ä¸­å®¹æ˜“å‡ºç°çš„æ¢¯åº¦çˆ†ç‚¸é—®é¢˜ï¼Œç ”ç©¶é‡‡ç”¨äº† Consistency Mid-Training è¿›è¡Œè½¨è¿¹æ„ŸçŸ¥åˆå§‹åŒ–ï¼Œå¹¶ç»“åˆä»é¢„è®­ç»ƒ flow matching æ•™å¸ˆæ¨¡å‹è’¸é¦çš„ä¸¤é˜¶æ®µæ–¹æ¡ˆæ¥åŠ é€Ÿæ”¶æ•›ã€‚è¿™ç§è®¾è®¡æ¶ˆé™¤äº†å¯¹å¤æ‚å¼•å¯¼è¶…å‚æ•°çš„éœ€æ±‚ï¼Œç®€åŒ–äº†è®­ç»ƒé…ç½®å¹¶å¤§å¹…å‰Šå‡äº†è®¡ç®—èµ„æºæ¶ˆè€—ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ ImageNet 256 æ•°æ®é›†ä¸Šå®ç°äº† 2.03 çš„ 1-step FIDï¼Œä¼˜äºåŸå§‹ MF æ¨¡å‹çš„åŒæ—¶å°†è®­ç»ƒæˆæœ¬é™ä½äº† 83%ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨æ‰©å±•è‡³ ImageNet 512 æ—¶ï¼Œä»¥æ‰€æœ‰åŸºçº¿æ¨¡å‹ä¸­æœ€ä½çš„ GFLOPS è¾¾åˆ°äº† 3.23 çš„ç«äº‰æ€§ 1-step FID è¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Code is available at https://github.com/sony/mf-rae",
      "pdf_url": "https://arxiv.org/pdf/2511.13019v1",
      "published_date": "2025-11-17 06:17:08 UTC",
      "updated_date": "2025-11-17 06:17:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:08:42.933395+00:00"
    },
    {
      "arxiv_id": "2511.13010v1",
      "title": "Are Graph Transformers Necessary? Efficient Long-Range Message Passing with Fractal Nodes in MPNNs",
      "title_zh": "å›¾ Transformer æ˜¯å¦å¿…è¦ï¼ŸMPNN ä¸­åŸºäºåˆ†å½¢èŠ‚ç‚¹çš„é«˜æ•ˆé•¿ç¨‹æ¶ˆæ¯ä¼ é€’",
      "authors": [
        "Jeongwhan Choi",
        "Seungjun Park",
        "Sumin Park",
        "Sung-Bae Cho",
        "Noseong Park"
      ],
      "abstract": "Graph Neural Networks (GNNs) have emerged as powerful tools for learning on graph-structured data, but often struggle to balance local and global information. While graph Transformers aim to address this by enabling long-range interactions, they often overlook the inherent locality and efficiency of Message Passing Neural Networks (MPNNs). We propose a new concept called fractal nodes, inspired by the fractal structure observed in real-world networks. Our approach is based on the intuition that graph partitioning naturally induces fractal structure, where subgraphs often reflect the connectivity patterns of the full graph. Fractal nodes are designed to coexist with the original nodes and adaptively aggregate subgraph-level feature representations, thereby enforcing feature similarity within each subgraph. We show that fractal nodes alleviate the over-squashing problem by providing direct shortcut connections that enable long-range propagation of subgraph-level representations. Experiment results show that our method improves the expressive power of MPNNs and achieves comparable or better performance to graph Transformers while maintaining the computational efficiency of MPNN by improving the long-range dependencies of MPNN.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Graph Transformers åœ¨å›¾å­¦ä¹ ä¸­çš„å¿…è¦æ€§ï¼Œå¹¶æå‡ºäº†å—è‡ªç„¶åˆ†å½¢ç»“æ„å¯å‘çš„ Fractal Nodesï¼ˆåˆ†å½¢èŠ‚ç‚¹ï¼‰æ¦‚å¿µï¼Œæ—¨åœ¨æå‡ Message Passing Neural Networks (MPNNs) å¤„ç†é•¿ç¨‹ä¿¡æ¯çš„èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å›¾åˆ’åˆ†è¯±å¯¼å‡ºçš„åˆ†å½¢ç‰¹æ€§ï¼Œä½¿ Fractal Nodes ä¸åŸå§‹èŠ‚ç‚¹å…±å­˜å¹¶è‡ªé€‚åº”èšåˆå­å›¾å±‚çº§çš„ç‰¹å¾ï¼Œä»è€Œå¼ºåŒ–äº†å­å›¾å†…éƒ¨çš„ç‰¹å¾ç›¸ä¼¼æ€§ã€‚Fractal Nodes é€šè¿‡æä¾›ç›´æ¥çš„å¿«æ·è¿æ¥ (shortcut connections) å®ç°äº†å­å›¾å±‚çº§è¡¨ç¤ºçš„é•¿ç¨‹ä¼ æ’­ï¼Œæœ‰æ•ˆç¼“è§£äº† MPNNs é¢ä¸´çš„ Over-squashing é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒ MPNNs é«˜æ•ˆè®¡ç®—ä¼˜åŠ¿çš„åŒæ—¶ï¼Œæ˜¾è‘—å¢å¼ºäº†å…¶è¡¨è¾¾èƒ½åŠ›ï¼Œå…¶æ€§èƒ½è¶³ä»¥æ¯”è‚©ç”šè‡³è¶…è¶Šå¤æ‚çš„ Graph Transformersã€‚è¿™ä¸€æˆæœè¯æ˜äº†é€šè¿‡ä¼˜åŒ–é•¿ç¨‹ä¾èµ–æ€§ï¼Œä¼ ç»Ÿçš„ MPNNs æ¶æ„åœ¨å¤„ç†å¤§è§„æ¨¡å¤æ‚å›¾æ•°æ®æ—¶ä¾ç„¶å…·æœ‰æå¼ºçš„ç«äº‰åŠ›å’Œå®ç”¨æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted in AAAI 2026 for Oral Representation. This is the extended version including the appendix",
      "pdf_url": "https://arxiv.org/pdf/2511.13010v1",
      "published_date": "2025-11-17 06:11:52 UTC",
      "updated_date": "2025-11-17 06:11:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:08:55.445898+00:00"
    },
    {
      "arxiv_id": "2511.13007v1",
      "title": "GEM: Generative Entropy-Guided Preference Modeling for Few-shot Alignment of LLMs",
      "title_zh": "GEMï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹å°‘æ ·æœ¬å¯¹é½çš„ç”Ÿæˆå¼ç†µå¼•å¯¼åå¥½å»ºæ¨¡",
      "authors": [
        "Yiyang Zhao",
        "Huiyu Bai",
        "Xuejiao Zhao"
      ],
      "abstract": "Alignment of large language models (LLMs) with human preferences typically relies on supervised reward models or external judges that demand abundant annotations. However, in fields that rely on professional knowledge, such as medicine and law, such large-scale preference labels are often unachievable. In this paper, we propose a generative entropy-guided preference modeling approach named GEM for LLMs aligment at low-resource and domain-specific scenarios. Instead of training a discriminative reward model on preference data, we directly train the LLM to internalize a closed-loop optimization architecture that can extract and exploit the multi-dimensional, fine-grained cognitive signals implicit in human preferences. Specifically, our Cognitive Filtering module, based on entropy theory in decision making, first leverages Chain-of-Thought (CoT) prompting to generate diverse candidate reasoning chains (CoTs) from preference data. Subsequently, it introduces a token scoring mechanism to rank and weight the sampled CoTs, boosting the importance of high-confidence answers and strategically high-entropy tokens. Building on these filtered preferences, we fine-tune the LLM using a novel self-evaluated group advantage algorithm, SEGA, which effectively aggregates group-level cognitive signals and transforms the entropy-based scores into implicit rewards for policy optimization. In these ways, GEM empowers the LLM to rely on its own judgments and establishes an entropy-guided closed-loop cognitive optimization framework, enabling highly efficient few-shot alignment of LLMs. Experiments on general benchmarks and domain-specific tasks (such as mathematical reasoning and medical dialogues) demonstrate that our GEM achieves significant improvements with few-shot preference data.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GEMï¼Œä¸€ç§ç”Ÿæˆå¼ä¿¡æ¯ç†µå¼•å¯¼çš„åå¥½å»ºæ¨¡æ–¹æ³•(Generative Entropy-Guided Preference Modeling)ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨åŒ»ç–—ã€æ³•å¾‹ç­‰ä¸“ä¸šé¢†åŸŸé¢ä¸´çš„åå¥½æ ‡æ³¨ç¨€ç¼ºå’Œå°‘æ ·æœ¬å¯¹é½(few-shot alignment)éš¾é¢˜ã€‚ä¸è®­ç»ƒä¼ ç»Ÿçš„åˆ¤åˆ«å¼å¥–åŠ±æ¨¡å‹ä¸åŒï¼ŒGEMé€šè¿‡è®©æ¨¡å‹å†…åŒ–ä¸€ä¸ªé—­ç¯ä¼˜åŒ–æ¶æ„ï¼Œæå–å¹¶åˆ©ç”¨äººç±»åå¥½ä¸­éšå«çš„å¤šç»´ã€ç»†ç²’åº¦è®¤çŸ¥ä¿¡å·ã€‚è¯¥æ–¹æ³•æ ¸å¿ƒåŒ…å«è®¤çŸ¥è¿‡æ»¤æ¨¡å—(Cognitive Filtering)ï¼Œåˆ©ç”¨é“¾å¼æ€ç»´(Chain-of-Thought)ç”Ÿæˆå¤šæ ·åŒ–çš„æ¨ç†é“¾ï¼Œå¹¶å¼•å…¥Tokenè¯„åˆ†æœºåˆ¶å¯¹é‡‡æ ·ç»“æœè¿›è¡Œæ’åå’ŒåŠ æƒï¼Œä»è€Œå¼ºåŒ–é«˜ç½®ä¿¡åº¦å’Œæˆ˜ç•¥æ€§é«˜ä¿¡æ¯ç†µTokençš„é‡è¦æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶é‡‡ç”¨æ–°å‹çš„è‡ªè¯„ä¼°ç»„ä¼˜åŠ¿ç®—æ³•(SEGA)è¿›è¡Œå¾®è°ƒï¼Œå°†åŸºäºç†µçš„è¯„åˆ†è½¬åŒ–ä¸ºéšå¼å¥–åŠ±ä»¥å®ç°ç­–ç•¥ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGEMåœ¨é€šç”¨åŸºå‡†ã€æ•°å­¦æ¨ç†åŠåŒ»ç–—å¯¹è¯ç­‰ä»»åŠ¡ä¸­ï¼Œä»…å‡­å°‘æ ·æœ¬åå¥½æ•°æ®ä¾¿èƒ½å–å¾—æ˜¾è‘—æ€§èƒ½æå‡ï¼ŒæˆåŠŸå»ºç«‹äº†ä¸€ä¸ªé«˜æ•ˆçš„ç†µå¼•å¯¼é—­ç¯è®¤çŸ¥ä¼˜åŒ–æ¡†æ¶ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper has been accepted by AAAI 2026-AIA and designated as an oral presentation paper",
      "pdf_url": "https://arxiv.org/pdf/2511.13007v1",
      "published_date": "2025-11-17 06:04:47 UTC",
      "updated_date": "2025-11-17 06:04:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:08:49.642186+00:00"
    },
    {
      "arxiv_id": "2511.13005v1",
      "title": "SAGE: Spuriousness-Aware Guided Prompt Exploration for Mitigating Multimodal Bias",
      "title_zh": "SAGEï¼šæ—¨åœ¨ç¼“è§£å¤šæ¨¡æ€åå·®çš„è™šå‡æ€§æ„ŸçŸ¥å¼•å¯¼å¼æç¤ºè¯æ¢ç´¢",
      "authors": [
        "Wenqian Ye",
        "Di Wang",
        "Guangtao Zheng",
        "Bohan Liu",
        "Aidong Zhang"
      ],
      "abstract": "Large vision-language models, such as CLIP, have shown strong zero-shot classification performance by aligning images and text in a shared embedding space. However, CLIP models often develop multimodal spurious biases, which is the undesirable tendency to rely on spurious features. For example, CLIP may infer object types in images based on frequently co-occurring backgrounds rather than the object's core features. This bias significantly impairs the robustness of pre-trained CLIP models on out-of-distribution data, where such cross-modal associations no longer hold. Existing methods for mitigating multimodal spurious bias typically require fine-tuning on downstream data or prior knowledge of the bias, which undermines the out-of-the-box usability of CLIP. In this paper, we first theoretically analyze the impact of multimodal spurious bias in zero-shot classification. Based on this insight, we propose Spuriousness-Aware Guided Exploration (SAGE), a simple and effective method that mitigates spurious bias through guided prompt selection. SAGE requires no training, fine-tuning, or external annotations. It explores a space of prompt templates and selects the prompts that induce the largest semantic separation between classes, thereby improving worst-group robustness. Extensive experiments on four real-world benchmark datasets and five popular backbone models demonstrate that SAGE consistently improves zero-shot performance and generalization, outperforming previous zero-shot approaches without any external knowledge or model updates.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ CLIP ç­‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ zero-shot åˆ†ç±»ä¸­å­˜åœ¨çš„ multimodal spurious bias é—®é¢˜è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œè¿™ç±»åå·®ä¼šå¯¼è‡´æ¨¡å‹è¿‡åº¦ä¾èµ–èƒŒæ™¯ç­‰ä¼ªç‰¹å¾è€Œéå¯¹è±¡æ ¸å¿ƒç‰¹å¾ã€‚ä¸ºè§£å†³ç°æœ‰ç¼“è§£æ–¹æ³•ä¾èµ–å¾®è°ƒæˆ–å…ˆéªŒçŸ¥è¯†çš„å±€é™ï¼Œè®ºæ–‡æå‡ºäº† SAGE (Spuriousness-Aware Guided Exploration) æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¼•å¯¼å¼æç¤ºé€‰æ‹©æ¥å¢å¼ºæ¨¡å‹é²æ£’æ€§ã€‚SAGE åœ¨ç†è®ºåˆ†æçš„åŸºç¡€ä¸Šï¼Œé€šè¿‡è‡ªåŠ¨æ¢ç´¢æç¤ºæ¨¡æ¿ç©ºé—´å¹¶ç­›é€‰å‡ºèƒ½è¯±å¯¼ç±»åˆ«é—´æœ€å¤§è¯­ä¹‰åˆ†ç¦» (semantic separation) çš„æç¤ºï¼Œä»è€Œæœ‰æ•ˆæå‡äº†æœ€å·®ç»„é²æ£’æ€§ (worst-group robustness)ã€‚è¯¥æ–¹æ³•æ— éœ€ä»»ä½•è®­ç»ƒã€å¾®è°ƒã€æ¨¡å‹æ›´æ–°æˆ–å¤–éƒ¨æ ‡æ³¨ï¼Œæå¤§åœ°ä¿ç•™äº†æ¨¡å‹çš„æ˜“ç”¨æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSAGE åœ¨å››ä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†æ•°æ®é›†å’Œäº”ç§ä¸»æµéª¨å¹²æ¨¡å‹ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œå…¶ zero-shot æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›æ˜¾è‘—ä¼˜äºç°æœ‰çš„æ— éœ€å¤–éƒ¨çŸ¥è¯†çš„æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.13005v1",
      "published_date": "2025-11-17 05:52:32 UTC",
      "updated_date": "2025-11-17 05:52:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:08:48.845063+00:00"
    },
    {
      "arxiv_id": "2511.13795v1",
      "title": "A Trajectory-free Crash Detection Framework with Generative Approach and Segment Map Diffusion",
      "title_zh": "ä¸€ç§åŸºäºç”Ÿæˆå¼æ–¹æ³•ä¸è·¯æ®µåœ°å›¾æ‰©æ•£çš„æ— è½¨è¿¹ç¢°æ’æ£€æµ‹æ¡†æ¶",
      "authors": [
        "Weiying Shen",
        "Hao Yu",
        "Yu Dong",
        "Pan Liu",
        "Yu Han",
        "Xin Wen"
      ],
      "abstract": "Real-time crash detection is essential for developing proactive safety management strategy and enhancing overall traffic efficiency. To address the limitations associated with trajectory acquisition and vehicle tracking, road segment maps recording the individual-level traffic dynamic data were directly served in crash detection. A novel two-stage trajectory-free crash detection framework, was present to generate the rational future road segment map and identify crashes. The first-stage diffusion-based segment map generation model, Mapfusion, conducts a noisy-to-normal process that progressively adds noise to the road segment map until the map is corrupted to pure Gaussian noise. The denoising process is guided by sequential embedding components capturing the temporal dynamics of segment map sequences. Furthermore, the generation model is designed to incorporate background context through ControlNet to enhance generation control. Crash detection is achieved by comparing the monitored segment map with the generations from diffusion model in second stage. Trained on non-crash vehicle motion data, Mapfusion successfully generates realistic road segment evolution maps based on learned motion patterns and remains robust across different sampling intervals. Experiments on real-world crashes indicate the effectiveness of the proposed two-stage method in accurately detecting crashes.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºMapfusionçš„æ— è½¨è¿¹ç¢°æ’æ£€æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨ç”Ÿæˆå¼æ–¹æ³•è§£å†³è½¦è¾†è½¨è¿¹è·å–å’Œè·Ÿè¸ªä¸­çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåœ¨äºè·¯æ®µåœ°å›¾æ‰©æ•£æ¨¡å‹(Segment Map Diffusion)ï¼Œé€šè¿‡ç¬¬ä¸€é˜¶æ®µçš„ç”Ÿæˆè¿‡ç¨‹é¢„æµ‹æœªæ¥çš„è·¯æ®µåœ°å›¾ã€‚æ¨¡å‹åˆ©ç”¨æ—¶åºåµŒå…¥(Sequential Embedding)æ•æ‰äº¤é€šåŠ¨æ€ï¼Œå¹¶å¼•å…¥ControlNetæ¥æ•´åˆèƒŒæ™¯ä¸Šä¸‹æ–‡ä»¥å¢å¼ºç”Ÿæˆæ§åˆ¶ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œç³»ç»Ÿé€šè¿‡å¯¹æ¯”å®æ—¶ç›‘æµ‹çš„è·¯æ®µåœ°å›¾ä¸æ¨¡å‹ç”Ÿæˆçš„é¢„æœŸåœ°å›¾æ¥è¯†åˆ«æ½œåœ¨çš„ç¢°æ’äº‹ä»¶ã€‚Mapfusionä»…åœ¨éç¢°æ’æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå±•ç°äº†åœ¨ä¸åŒé‡‡æ ·é—´éš”ä¸‹ç”Ÿæˆé€¼çœŸäº¤é€šæµæ¼”åŒ–å›¾çš„é²æ£’æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥ä¸¤é˜¶æ®µæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆä¸”å‡†ç¡®åœ°æ£€æµ‹ç°å®ä¸–ç•Œä¸­çš„äº¤é€šç¢°æ’ï¼Œä¸ºå®æ—¶ä¸»åŠ¨å®‰å…¨ç®¡ç†æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "To be presented at TRB 2026 (TRBAM-26-01711) and a revised version will be submitted to Transportation Research Part C: Emerging Technologies",
      "pdf_url": "https://arxiv.org/pdf/2511.13795v1",
      "published_date": "2025-11-17 05:50:00 UTC",
      "updated_date": "2025-11-17 05:50:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:09:09.645549+00:00"
    },
    {
      "arxiv_id": "2511.12997v1",
      "title": "WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance",
      "title_zh": "WebCoachï¼šåŸºäºè·¨ä¼šè¯è®°å¿†å¼•å¯¼çš„è‡ªè¿›åŒ–ç½‘é¡µæ™ºèƒ½ä½“",
      "authors": [
        "Genglin Liu",
        "Shijie Geng",
        "Sha Li",
        "Hejie Cui",
        "Sarah Zhang",
        "Xin Liu",
        "Tianyi Liu"
      ],
      "abstract": "Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMultimodal LLMï¼‰é©±åŠ¨çš„ç½‘é¡µæ™ºèƒ½ä½“åœ¨å¯¼èˆªä»»åŠ¡ä¸­å®¹æ˜“é‡å¤å‡ºé”™ä¸”ç¼ºä¹è·¨ä¼šè¯ï¼ˆcross-sessionï¼‰å­¦ä¹ èƒ½åŠ›çš„å±€é™ï¼Œæå‡ºäº†WebCoachæ¡†æ¶ã€‚WebCoachæ˜¯ä¸€ä¸ªä¸æ¨¡å‹æ— å…³çš„è‡ªè¿›åŒ–ï¼ˆself-evolvingï¼‰æ¡†æ¶ï¼Œé€šè¿‡ä¸ºç½‘é¡µæµè§ˆæ™ºèƒ½ä½“æä¾›æŒä¹…çš„è·¨ä¼šè¯è®°å¿†ï¼Œåœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„å‰æä¸‹å®ç°äº†é•¿æœŸè§„åˆ’ã€åæ€å’ŒæŒç»­å­¦ä¹ ã€‚è¯¥æ¡†æ¶ç”±ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶æ„æˆï¼šè´Ÿè´£æ ‡å‡†åŒ–å¯¼èˆªæ—¥å¿—çš„WebCondenserã€ç»„ç»‡è½¨è¿¹ç»éªŒçš„å¤–éƒ¨å­˜å‚¨å™¨ï¼ˆExternal Memory Storeï¼‰ï¼Œä»¥åŠé€šè¿‡æ£€ç´¢ç›¸å…³ç»éªŒå¹¶æ³¨å…¥ä»»åŠ¡å»ºè®®çš„Coachã€‚è¿™ç§è®¾è®¡ä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿè·å–è¶…å‡ºåŸç”Ÿä¸Šä¸‹æ–‡çª—å£ï¼ˆcontext windowï¼‰çš„é•¿æœŸè®°å¿†ï¼Œå¹¶é€šè¿‡ä¸æ–­ä»æ–°è½¨è¿¹ä¸­æ•´ç†å¹•å¼è®°å¿†ï¼ˆepisodic memoryï¼‰å®ç°è‡ªæˆ‘è¿›åŒ–ã€‚åœ¨WebVoyageråŸºå‡†æµ‹è¯•ä¸­ï¼ŒWebCoachå°†38Bæ¨¡å‹çš„ä»»åŠ¡æˆåŠŸç‡ä»47%æå‡è‡³61%ï¼Œæ˜¾è‘—å¢å¼ºäº†æ™ºèƒ½ä½“çš„é²æ£’æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œé…å¤‡WebCoachçš„å°è§„æ¨¡åŸºç¡€æ¨¡å‹åœ¨æ€§èƒ½ä¸Šå¯åª²ç¾åŸºäºGPT-4oçš„æ™ºèƒ½ä½“ï¼Œå¤§å¹…æé«˜äº†ç½‘é¡µå¯¼èˆªä»»åŠ¡çš„æ ·æœ¬æ•ˆç‡ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages; work in progress",
      "pdf_url": "https://arxiv.org/pdf/2511.12997v1",
      "published_date": "2025-11-17 05:38:50 UTC",
      "updated_date": "2025-11-17 05:38:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:09:21.932947+00:00"
    },
    {
      "arxiv_id": "2511.12988v2",
      "title": "UNSEEN: Enhancing Dataset Pruning from a Generalization Perspective",
      "title_zh": "UNSEENï¼šåŸºäºæ³›åŒ–è§†è§’çš„æ•°æ®é›†å‰ªæå¢å¼º",
      "authors": [
        "Furui Xu",
        "Shaobo Wang",
        "Jiajun Zhang",
        "Chenghao Sun",
        "Haixiang Tang",
        "Linfeng Zhang"
      ],
      "abstract": "The growing scale of datasets in deep learning has introduced significant computational challenges. Dataset pruning addresses this challenge by constructing a compact but informative coreset from the full dataset with comparable performance. Previous approaches typically establish scoring metrics based on specific criteria to identify representative samples. However, these methods predominantly rely on sample scores obtained from the model's performance during the training (i.e., fitting) phase. As scoring models achieve near-optimal performance on training data, such fitting-centric approaches induce a dense distribution of sample scores within a narrow numerical range. This concentration reduces the distinction between samples and hinders effective selection. To address this challenge, we conduct dataset pruning from the perspective of generalization, i.e., scoring samples based on models not exposed to them during training. We propose a plug-and-play framework, UNSEEN, which can be integrated into existing dataset pruning methods. Additionally, conventional score-based methods are single-step and rely on models trained solely on the complete dataset, providing limited perspective on the importance of samples. To address this limitation, we scale UNSEEN to multi-step scenarios and propose an incremental selection technique through scoring models trained on varying coresets, and optimize the quality of the coreset dynamically. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art (SOTA) methods on CIFAR-10, CIFAR-100, and ImageNet-1K. Notably, on ImageNet-1K, UNSEEN achieves lossless performance while reducing training data by 30\\%.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦å­¦ä¹ æ•°æ®é›†è§„æ¨¡å¢é•¿å¸¦æ¥çš„è®¡ç®—æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ä»æ³›åŒ– (generalization) è§†è§’å¢å¼ºæ•°æ®é›†å‰ªæ (Dataset pruning) çš„æ¡†æ¶ UNSEENã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åŸºäºæ¨¡å‹åœ¨è®­ç»ƒæ‹Ÿåˆé˜¶æ®µçš„è¡¨ç°å¯¹æ ·æœ¬è¯„åˆ†ï¼Œå®¹æ˜“å¯¼è‡´æ ·æœ¬å¾—åˆ†åˆ†å¸ƒè¿‡å¯†ä¸”åŒºåˆ†åº¦ä¸è¶³ï¼Œè€Œ UNSEEN åˆ™é€šè¿‡åˆ©ç”¨è®­ç»ƒä¸­æœªæ¥è§¦è¿‡ç›¸å…³æ ·æœ¬çš„æ¨¡å‹è¿›è¡Œè¯„åˆ†ï¼Œæœ‰æ•ˆæå‡äº†ä»£è¡¨æ€§æ ·æœ¬çš„é€‰æ‹©æ•ˆç‡ã€‚ä½œä¸ºä¸€ä¸ªå³æ’å³ç”¨ (plug-and-play) çš„æ¡†æ¶ï¼ŒUNSEEN è¿˜è¢«æ‰©å±•åˆ°å¤šæ­¥åœºæ™¯ï¼Œåˆ©ç”¨åœ¨ä¸åŒæ ¸å¿ƒé›† (coreset) ä¸Šè®­ç»ƒçš„æ¨¡å‹é€šè¿‡å¢é‡é€‰æ‹© (incremental selection) æŠ€æœ¯åŠ¨æ€ä¼˜åŒ–æ ¸å¿ƒé›†è´¨é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ CIFAR-10ã€CIFAR-100 å’Œ ImageNet-1K ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿› (SOTA) æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨ ImageNet-1K ä»»åŠ¡ä¸­ï¼ŒUNSEEN åœ¨å‡å°‘äº† 30% è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ä¾ç„¶å®ç°äº†æ— æŸçš„æ€§èƒ½è¡¨ç°ï¼Œä¸ºé«˜æ•ˆæ¨¡å‹è®­ç»ƒæä¾›äº†æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "AAAI 2026, 13 pages, 9 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.12988v2",
      "published_date": "2025-11-17 05:17:39 UTC",
      "updated_date": "2025-11-18 02:42:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:09:20.647153+00:00"
    },
    {
      "arxiv_id": "2511.12986v1",
      "title": "Learning Branching Policies for MILPs with Proximal Policy Optimization",
      "title_zh": "åŸºäºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–çš„ MILP åˆ†æ”¯ç­–ç•¥å­¦ä¹ ",
      "authors": [
        "Abdelouahed Ben Mhamed",
        "Assia Kamal-Idrissi",
        "Amal El Fallah Seghrouchni"
      ],
      "abstract": "Branch-and-Bound (B\\&B) is the dominant exact solution method for Mixed Integer Linear Programs (MILP), yet its exponential time complexity poses significant challenges for large-scale instances. The growing capabilities of machine learning have spurred efforts to improve B\\&B by learning data-driven branching policies. However, most existing approaches rely on Imitation Learning (IL), which tends to overfit to expert demonstrations and struggles to generalize to structurally diverse or unseen instances. In this work, we propose Tree-Gate Proximal Policy Optimization (TGPPO), a novel framework that employs Proximal Policy Optimization (PPO), a Reinforcement Learning (RL) algorithm, to train a branching policy aimed at improving generalization across heterogeneous MILP instances. Our approach builds on a parameterized state space representation that dynamically captures the evolving context of the search tree. Empirical evaluations show that TGPPO often outperforms existing learning-based policies in terms of reducing the number of nodes explored and improving p-Primal-Dual Integrals (PDI), particularly in out-of-distribution instances. These results highlight the potential of RL to develop robust and adaptable branching strategies for MILP solvers.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ··åˆæ•´æ•°çº¿æ€§è§„åˆ’(MILP)æ±‚è§£ä¸­åˆ†æ”¯å®šç•Œæ³•(Branch-and-Bound)é¢ä¸´çš„è®¡ç®—æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºTree-Gate Proximal Policy Optimization (TGPPO)çš„æ–°å‹æ¡†æ¶ã€‚ä¸ä¼ ç»Ÿä¾èµ–æ¨¡ä»¿å­¦ä¹ (Imitation Learning)ä¸”å®¹æ˜“è¿‡æ‹Ÿåˆçš„åˆ†æ”¯ç­–ç•¥ä¸åŒï¼ŒTGPPOåˆ©ç”¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ä¸­çš„PPOç®—æ³•æ¥æå‡åœ¨å¼‚æ„MILPå®ä¾‹ä¸Šçš„æ³›åŒ–æ€§èƒ½ã€‚è¯¥æ–¹æ³•æ„å»ºåœ¨å‚æ•°åŒ–çš„çŠ¶æ€ç©ºé—´è¡¨ç¤ºä¹‹ä¸Šï¼Œèƒ½å¤ŸåŠ¨æ€æ•æ‰æœç´¢æ ‘æ¼”åŒ–è¿‡ç¨‹ä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å®è¯è¯„ä¼°æ˜¾ç¤ºï¼ŒTGPPOåœ¨å‡å°‘æœç´¢èŠ‚ç‚¹æ•°é‡å’Œä¼˜åŒ–p-Primal-Dual Integrals (PDI)æŒ‡æ ‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„å­¦ä¹ å‹ç­–ç•¥ã€‚ç‰¹åˆ«æ˜¯åœ¨å¤„ç†åˆ†å¸ƒå¤–(out-of-distribution)å®ä¾‹æ—¶ï¼Œè¯¥æ¡†æ¶å±•ç°äº†æ›´å¼ºçš„é²æ£’æ€§ä¸é€‚åº”æ€§ã€‚ç ”ç©¶ç»“æœè¯æ˜äº†å¼ºåŒ–å­¦ä¹ åœ¨æå‡MILPæ±‚è§£å™¨æ€§èƒ½å¹¶å¼€å‘è‡ªé€‚åº”åˆ†æ”¯ç­–ç•¥æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 3 figures, AAAI conference",
      "pdf_url": "https://arxiv.org/pdf/2511.12986v1",
      "published_date": "2025-11-17 05:16:14 UTC",
      "updated_date": "2025-11-17 05:16:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:09:50.532053+00:00"
    },
    {
      "arxiv_id": "2511.12971v1",
      "title": "Esim: EVM Bytecode Similarity Detection Based on Stable-Semantic Graph",
      "title_zh": "Esimï¼šåŸºäºç¨³å®šè¯­ä¹‰å›¾çš„ EVM å­—èŠ‚ç ç›¸ä¼¼æ€§æ£€æµ‹",
      "authors": [
        "Zhuo Chen",
        "Gaoqiang Ji",
        "Yiling He",
        "Lei Wu",
        "Yajin Zhou"
      ],
      "abstract": "Decentralized finance (DeFi) is experiencing rapid expansion. However, prevalent code reuse and limited open-source contributions have introduced significant challenges to the blockchain ecosystem, including plagiarism and the propagation of vulnerable code. Consequently, an effective and accurate similarity detection method for EVM bytecode is urgently needed to identify similar contracts. Traditional binary similarity detection methods are typically based on instruction stream or control flow graph (CFG), which have limitations on EVM bytecode due to specific features like low-level EVM bytecode and heavily-reused basic blocks. Moreover, the highly-diverse Solidity Compiler (Solc) versions further complicate accurate similarity detection.\n  Motivated by these challenges, we propose a novel EVM bytecode representation called Stable-Semantic Graph (SSG), which captures relationships between 'stable instructions' (special instructions identified by our study). Moreover, we implement a prototype, Esim, which embeds SSG into matrices for similarity detection using a heterogeneous graph neural network. Esim demonstrates high accuracy in SSG construction, achieving F1-scores of 100% for control flow and 95.16% for data flow, and its similarity detection performance reaches 96.3% AUC, surpassing traditional approaches. Our large-scale study, analyzing 2,675,573 smart contracts on six EVM-compatible chains over a one-year period, also demonstrates that Esim outperforms the SOTA tool Etherscan in vulnerability search.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Esimï¼Œæ—¨åœ¨è§£å†³DeFiç”Ÿæ€ä¸­å› ä»£ç å¤ç”¨å’ŒSolidityç¼–è¯‘å™¨(Solc)ç‰ˆæœ¬å¤šæ ·æ€§å¸¦æ¥çš„EVM bytecodeç›¸ä¼¼æ€§æ£€æµ‹éš¾é¢˜ã€‚ä¸ºäº†å…‹æœä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†ä½çº§æŒ‡ä»¤å’Œé«˜åº¦å¤ç”¨åŸºç¡€å—æ—¶çš„å±€é™æ€§ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§åä¸ºStable-Semantic Graph (SSG)çš„æ–°å‹è¡¨ç¤ºæ³•ï¼Œé€šè¿‡æ•æ‰ç¨³å®šæŒ‡ä»¤(stable instructions)ä¹‹é—´çš„å…³ç³»æ¥æå–æ ¸å¿ƒè¯­ä¹‰ã€‚EsimåŸå‹ç³»ç»Ÿåˆ©ç”¨å¼‚æ„å›¾ç¥ç»ç½‘ç»œ(heterogeneous graph neural network)å°†SSGåµŒå…¥çŸ©é˜µè¿›è¡Œå¤„ç†ï¼Œä»è€Œå®ç°é«˜ç²¾åº¦çš„ç›¸ä¼¼æ€§æ£€æµ‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEsimåœ¨ç›¸ä¼¼æ€§æ£€æµ‹ä»»åŠ¡ä¸­è¾¾åˆ°äº†96.3%çš„AUCï¼Œå¹¶åœ¨æ§åˆ¶æµä¸æ•°æ®æµçš„æ„å»ºä¸­å±•ç°äº†æé«˜çš„F1-scoreã€‚åœ¨å¯¹å…­æ¡EVMå…¼å®¹é“¾çš„å¤§è§„æ¨¡å®æµ‹ä¸­ï¼ŒEsimåœ¨æ¼æ´åˆçº¦æœç´¢èƒ½åŠ›ä¸Šè¶…è¶Šäº†ç°æœ‰çš„SOTAå·¥å…·Etherscanï¼Œè¯æ˜äº†å…¶åœ¨ä¿éšœåŒºå—é“¾ç”Ÿæ€å®‰å…¨æ–¹é¢çš„å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12971v1",
      "published_date": "2025-11-17 04:48:52 UTC",
      "updated_date": "2025-11-17 04:48:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:09:41.634331+00:00"
    },
    {
      "arxiv_id": "2511.12964v1",
      "title": "CalibrateMix: Guided-Mixup Calibration of Image Semi-Supervised Models",
      "title_zh": "CalibrateMixï¼šåŸºäºå¼•å¯¼å¼ Mixup çš„å›¾åƒåŠç›‘ç£æ¨¡å‹æ ¡å‡†",
      "authors": [
        "Mehrab Mustafy Rahman",
        "Jayanth Mohan",
        "Tiberiu Sosea",
        "Cornelia Caragea"
      ],
      "abstract": "Semi-supervised learning (SSL) has demonstrated high performance in image classification tasks by effectively utilizing both labeled and unlabeled data. However, existing SSL methods often suffer from poor calibration, with models yielding overconfident predictions that misrepresent actual prediction likelihoods. Recently, neural networks trained with {\\tt mixup} that linearly interpolates random examples from the training set have shown better calibration in supervised settings. However, calibration of neural models remains under-explored in semi-supervised settings. Although effective in supervised model calibration, random mixup of pseudolabels in SSL presents challenges due to the overconfidence and unreliability of pseudolabels. In this work, we introduce CalibrateMix, a targeted mixup-based approach that aims to improve the calibration of SSL models while maintaining or even improving their classification accuracy. Our method leverages training dynamics of labeled and unlabeled samples to identify ``easy-to-learn'' and ``hard-to-learn'' samples, which in turn are utilized in a targeted mixup of easy and hard samples. Experimental results across several benchmark image datasets show that our method achieves lower expected calibration error (ECE) and superior accuracy compared to existing SSL approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŠç›‘ç£å­¦ä¹ (SSL)ä¸­æ¨¡å‹å¸¸å› è¿‡åº¦è‡ªä¿¡è€Œå¯¼è‡´æ ¡å‡†(calibration)æ€§èƒ½ä¸ä½³çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºCalibrateMixçš„æœ‰é’ˆå¯¹æ€§çš„mixupæ ¡å‡†æ–¹æ³•ã€‚è™½ç„¶ä¼ ç»Ÿçš„mixupåœ¨ç›‘ç£å­¦ä¹ ä¸­èƒ½æœ‰æ•ˆæ”¹å–„æ ¡å‡†ï¼Œä½†åœ¨SSLä¸­ç›´æ¥å¯¹ä¸å¯é çš„ä¼ªæ ‡ç­¾è¿›è¡Œéšæœºæ··åˆä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚CalibrateMixåˆ©ç”¨æ ‡è®°å’Œæœªæ ‡è®°æ ·æœ¬çš„è®­ç»ƒåŠ¨åŠ›å­¦(training dynamics)æ¥è¯†åˆ«â€œæ˜“å­¦ä¹ â€å’Œâ€œéš¾å­¦ä¹ â€çš„æ ·æœ¬ï¼Œå¹¶å¯¹å…¶å®æ–½å¼•å¯¼å¼çš„mixupæ“ä½œã€‚è¿™ç§é’ˆå¯¹æ€§çš„æ··åˆç­–ç•¥æ—¨åœ¨æ”¹è¿›SSLæ¨¡å‹çš„æ ¡å‡†æ•ˆæœï¼ŒåŒæ—¶ç¡®ä¿åˆ†ç±»å‡†ç¡®ç‡ä¸å—å½±å“ç”šè‡³å¾—åˆ°æå‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å¤šä¸ªåŸºå‡†å›¾åƒæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•ç›¸æ¯”ç°æœ‰SSLæ–¹æ¡ˆå®ç°äº†æ›´ä½çš„æœŸæœ›æ ¡å‡†è¯¯å·®(ECE)å’Œæ›´å“è¶Šçš„å‡†ç¡®ç‡è¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12964v1",
      "published_date": "2025-11-17 04:43:53 UTC",
      "updated_date": "2025-11-17 04:43:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:10:36.530649+00:00"
    },
    {
      "arxiv_id": "2511.12963v2",
      "title": "MedRule-KG: A Knowledge-Graph--Steered Scaffold for Reliable Mathematical and Biomedical Reasoning",
      "title_zh": "MedRule-KGï¼šç”¨äºå¯é æ•°å­¦ä¸ç”Ÿç‰©åŒ»å­¦æ¨ç†çš„çŸ¥è¯†å›¾è°±å¼•å¯¼å¼æ¶æ„",
      "authors": [
        "Crystal Su"
      ],
      "abstract": "We study how to impose domain-consistent structure on large language models (LLMs) used for scientific reasoning and early-stage drug discovery. We present MedRule-KG, a compact knowledge-graph scaffold paired with a lightweight verifier that steers generation toward mathematically and biomedically valid outputs. The system injects curated symbolic facts into prompts and then enforces rule satisfaction with a deterministic checker. We formalize generation as constrained inference, introduce a soft guidance surrogate suitable for decoding, and perform a thorough statistical analysis with uncertainty quantification. Across 90 tasks spanning reaction feasibility, metabolic compatibility, and toxicity screening, MedRule-KG reduces violation counts by 83.2\\% relative to a strong chain-of-thought baseline while improving exact match. Results remain stable under stratification and scale with dataset size, and the verifier adds negligible latency, making the approach practical for interactive design.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MedRule-KGï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†ç´§å‡‘å‹çŸ¥è¯†å›¾è°± (Knowledge-Graph) æ”¯æ¶å’Œè½»é‡çº§éªŒè¯å™¨çš„æ¶æ„ï¼Œæ—¨åœ¨ä¸ºå¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ç§‘å­¦æ¨ç†å’Œæ—©æœŸè¯ç‰©å‘ç°ä¸­æ–½åŠ é¢†åŸŸä¸€è‡´æ€§çš„çº¦æŸã€‚è¯¥ç³»ç»Ÿé€šè¿‡å°†ç²¾å¿ƒç­–åˆ’çš„ç¬¦å·äº‹å®æ³¨å…¥æç¤ºè¯ï¼Œå¹¶åˆ©ç”¨ç¡®å®šæ€§æ£€æŸ¥å™¨å¼ºåˆ¶æ‰§è¡Œè§„åˆ™ä¸€è‡´æ€§ï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆç¬¦åˆæ•°å­¦å’Œç”Ÿç‰©åŒ»å­¦é€»è¾‘çš„è¾“å‡ºã€‚ç ”ç©¶å°†ç”Ÿæˆè¿‡ç¨‹å½¢å¼åŒ–ä¸ºçº¦æŸæ¨ç† (Constrained Inference)ï¼Œå¹¶å¼•å…¥äº†é€‚ç”¨äºè§£ç é˜¶æ®µçš„è½¯å¼•å¯¼ä»£ç†æ–¹æ³•ã€‚å®éªŒæ¶µç›–äº†ååº”å¯è¡Œæ€§ã€ä»£è°¢å…¼å®¹æ€§å’Œæ¯’æ€§ç­›æŸ¥ç­‰ 90 é¡¹ä»»åŠ¡ï¼Œç»“æœæ˜¾ç¤º MedRule-KG ç›¸æ¯”å¼ºé“¾å¼æ€ç»´ (Chain-of-Thought) åŸºå‡†å°†è§„åˆ™è¿èƒŒæ¬¡æ•°æ˜¾è‘—å‡å°‘äº† 83.2%ï¼Œå¹¶æå‡äº†ç²¾ç¡®åŒ¹é…ç‡ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿçš„éªŒè¯å™¨äº§ç”Ÿçš„å»¶è¿Ÿæä½ï¼Œä¸”åœ¨ä¸åŒè§„æ¨¡çš„æ•°æ®é›†ä¸Šè¡¨ç°ç¨³å®šï¼Œä¸ºäº¤äº’å¼ç§‘å­¦è®¾è®¡æä¾›äº†å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper is withdrawn due to issues with attribution and citation accuracy",
      "pdf_url": "https://arxiv.org/pdf/2511.12963v2",
      "published_date": "2025-11-17 04:42:52 UTC",
      "updated_date": "2025-12-12 16:08:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:10:44.639303+00:00"
    },
    {
      "arxiv_id": "2511.12962v1",
      "title": "EndoSight AI: Deep Learning-Driven Real-Time Gastrointestinal Polyp Detection and Segmentation for Enhanced Endoscopic Diagnostics",
      "title_zh": "EndoSight AIï¼šåŸºäºæ·±åº¦å­¦ä¹ çš„å®æ—¶èƒƒè‚ é“æ¯è‚‰æ£€æµ‹ä¸åˆ†å‰²ï¼Œæ—¨åœ¨å¢å¼ºå†…é•œè¯Šæ–­",
      "authors": [
        "Daniel Cavadia"
      ],
      "abstract": "Precise and real-time detection of gastrointestinal polyps during endoscopic procedures is crucial for early diagnosis and prevention of colorectal cancer. This work presents EndoSight AI, a deep learning architecture developed and evaluated independently to enable accurate polyp localization and detailed boundary delineation. Leveraging the publicly available Hyper-Kvasir dataset, the system achieves a mean Average Precision (mAP) of 88.3% for polyp detection and a Dice coefficient of up to 69% for segmentation, alongside real-time inference speeds exceeding 35 frames per second on GPU hardware. The training incorporates clinically relevant performance metrics and a novel thermal-aware procedure to ensure model robustness and efficiency. This integrated AI solution is designed for seamless deployment in endoscopy workflows, promising to advance diagnostic accuracy and clinical decision-making in gastrointestinal healthcare.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EndoSight AIï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºå†…é•œæ‰‹æœ¯ä¸­èƒƒè‚ é“æ¯è‚‰çš„å®æ—¶æ£€æµ‹ä¸ç²¾ç¡®åˆ†å‰²è€Œè®¾è®¡çš„æ·±åº¦å­¦ä¹ æ¶æ„ã€‚åˆ©ç”¨å…¬å¼€çš„Hyper-Kvasiræ•°æ®é›†ï¼Œè¯¥ç³»ç»Ÿå®ç°äº†88.3%çš„mean Average Precision (mAP)æ£€æµ‹ç²¾åº¦å’Œé«˜è¾¾69%çš„Dice coefficientåˆ†å‰²æ•ˆæœï¼Œä¸”åœ¨GPUä¸Šçš„æ¨ç†é€Ÿåº¦è¶…è¿‡35 frames per second (FPS)ï¼Œæ»¡è¶³å®æ—¶æ€§è¦æ±‚ã€‚æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ç»“åˆäº†ä¸´åºŠæ€§èƒ½æŒ‡æ ‡åŠåˆ›æ–°çš„thermal-awareæµç¨‹ï¼Œç¡®ä¿äº†åœ¨å®é™…åº”ç”¨ä¸­çš„é²æ£’æ€§ä¸è®¡ç®—æ•ˆç‡ã€‚ä½œä¸ºä¸€ç§å¯æ— ç¼é›†æˆè‡³å†…é•œå·¥ä½œæµçš„AIè§£å†³æ–¹æ¡ˆï¼ŒEndoSight AIèƒ½å¤Ÿæ˜¾è‘—æå‡è¯Šæ–­å‡†ç¡®æ€§å¹¶ä¼˜åŒ–ä¸´åºŠå†³ç­–ï¼Œä¸ºç»“ç›´è‚ ç™Œçš„æ—©æœŸé¢„é˜²æä¾›äº†å…³é”®çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12962v1",
      "published_date": "2025-11-17 04:40:38 UTC",
      "updated_date": "2025-11-17 04:40:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:11:05.044205+00:00"
    },
    {
      "arxiv_id": "2511.12955v1",
      "title": "Global Cross-Time Attention Fusion for Enhanced Solar Flare Prediction from Multivariate Time Series",
      "title_zh": "åŸºäºå¤šå˜é‡æ—¶é—´åºåˆ—å¢å¼ºå¤ªé˜³è€€æ–‘é¢„æµ‹çš„å…¨å±€è·¨æ—¶åŸŸæ³¨æ„åŠ›èåˆ",
      "authors": [
        "Onur Vural",
        "Shah Muhammad Hamdi",
        "Soukaina Filali Boubrahimi"
      ],
      "abstract": "Multivariate time series classification is increasingly investigated in space weather research as a means to predict intense solar flare events, which can cause widespread disruptions across modern technological systems. Magnetic field measurements of solar active regions are converted into structured multivariate time series, enabling predictive modeling across segmented observation windows. However, the inherently imbalanced nature of solar flare occurrences, where intense flares are rare compared to minor flare events, presents a significant barrier to effective learning. To address this challenge, we propose a novel Global Cross-Time Attention Fusion (GCTAF) architecture, a transformer-based model to enhance long-range temporal modeling. Unlike traditional self-attention mechanisms that rely solely on local interactions within time series, GCTAF injects a set of learnable cross-attentive global tokens that summarize salient temporal patterns across the entire sequence. These tokens are refined through cross-attention with the input sequence and fused back into the temporal representation, enabling the model to identify globally significant, non-contiguous time points that are critical for flare prediction. This mechanism functions as a dynamic attention-driven temporal summarizer that augments the model's capacity to capture discriminative flare-related dynamics. We evaluate our approach on the benchmark solar flare dataset and show that GCTAF effectively detects intense flares and improves predictive performance, demonstrating that refining transformer-based architectures presents a high-potential alternative for solar flare prediction tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å‰§çƒˆå¤ªé˜³è€€æ–‘é¢„æµ‹ä¸­å¤šå˜é‡æ—¶é—´åºåˆ—çš„ç±»åˆ«ä¸å¹³è¡¡å’Œé•¿ç¨‹æ—¶é—´å»ºæ¨¡æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º Global Cross-Time Attention Fusion (GCTAF) çš„æ–°å‹ Transformer æ¶æ„ã€‚GCTAF é€šè¿‡å¼•å…¥ä¸€ç»„å¯å­¦ä¹ çš„äº¤å‰æ³¨æ„åŠ›å…¨å±€æ ‡è®° (cross-attentive global tokens)ï¼Œæœ‰æ•ˆæ±‡æ€»äº†æ•´ä¸ªåºåˆ—ä¸­çš„æ˜¾è‘—æ—¶é—´æ¨¡å¼ï¼Œçªç ´äº†ä¼ ç»Ÿè‡ªæ³¨æ„åŠ›æœºåˆ¶ä»…ä¾èµ–å±€éƒ¨äº¤äº’çš„å±€é™ã€‚è¿™äº›å…¨å±€æ ‡è®°ç»è¿‡ç»†åŒ–å¹¶èåˆå›æ—¶é—´è¡¨ç¤ºä¸­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å¯¹è€€æ–‘é¢„æµ‹è‡³å…³é‡è¦çš„å…¨çƒæ€§ã€éè¿ç»­æ—¶é—´ç‚¹ï¼Œä»è€Œå……å½“åŠ¨æ€æ³¨æ„åŠ›é©±åŠ¨çš„æ—¶é—´æ‘˜è¦å™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGCTAF åœ¨åŸºå‡†å¤ªé˜³è€€æ–‘æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†å¯¹å‰§çƒˆè€€æ–‘çš„æ£€æµ‹èƒ½åŠ›å’Œé¢„æµ‹æ€§èƒ½ã€‚è¯¥ç ”ç©¶è¯æ˜äº†ä¼˜åŒ– Transformer æ¶æ„åœ¨æ•æ‰åˆ¤åˆ«æ€§è€€æ–‘åŠ¨åŠ›å­¦ç‰¹å¾æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºç©ºé—´å¤©æ°”é¢„æŠ¥æä¾›äº†æ›´æœ‰æ•ˆçš„æŠ€æœ¯æ‰‹æ®µã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This work has been accepted at the 2025 IEEE International Conference on Big Data (IEEE BigData 2025) on October 23, 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.12955v1",
      "published_date": "2025-11-17 04:26:56 UTC",
      "updated_date": "2025-11-17 04:26:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:10:19.728529+00:00"
    },
    {
      "arxiv_id": "2511.17590v1",
      "title": "SHAP Distance: An Explainability-Aware Metric for Evaluating the Semantic Fidelity of Synthetic Tabular Data",
      "title_zh": "SHAP è·ç¦»ï¼šä¸€ç§ç”¨äºè¯„ä¼°åˆæˆè¡¨æ ¼æ•°æ®è¯­ä¹‰ä¿çœŸåº¦çš„å¯è§£é‡Šæ€§æ„ŸçŸ¥åº¦é‡æŒ‡æ ‡",
      "authors": [
        "Ke Yu",
        "Shigeru Ishikura",
        "Yukari Usukura",
        "Yuki Shigoku",
        "Teruaki Hayashi"
      ],
      "abstract": "Synthetic tabular data, which are widely used in domains such as healthcare, enterprise operations, and customer analytics, are increasingly evaluated to ensure that they preserve both privacy and utility. While existing evaluation practices typically focus on distributional similarity (e.g., the Kullback-Leibler divergence) or predictive performance (e.g., Train-on-Synthetic-Test-on-Real (TSTR) accuracy), these approaches fail to assess semantic fidelity, that is, whether models trained on synthetic data follow reasoning patterns consistent with those trained on real data. To address this gap, we introduce the SHapley Additive exPlanations (SHAP) Distance, a novel explainability-aware metric that is defined as the cosine distance between the global SHAP attribution vectors derived from classifiers trained on real versus synthetic datasets. By analyzing datasets that span clinical health records with physiological features, enterprise invoice transactions with heterogeneous scales, and telecom churn logs with mixed categorical-numerical attributes, we demonstrate that the SHAP Distance reliably identifies semantic discrepancies that are overlooked by standard statistical and predictive measures. In particular, our results show that the SHAP Distance captures feature importance shifts and underrepresented tail effects that the Kullback-Leibler divergence and Train-on-Synthetic-Test-on-Real accuracy fail to detect. This study positions the SHAP Distance as a practical and discriminative tool for auditing the semantic fidelity of synthetic tabular data, and offers practical guidelines for integrating attribution-based evaluation into future benchmarking pipelines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SHAP Distanceï¼Œä¸€ç§åŸºäºå¯è§£é‡Šæ€§çš„æ–°æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°åˆæˆè¡¨æ ¼æ•°æ®çš„è¯­ä¹‰å¿ å®åº¦ (Semantic Fidelity)ã€‚ä¼ ç»Ÿçš„è¯„ä¼°æ–¹æ³•å¦‚ Kullback-Leibler divergence æˆ– Train-on-Synthetic-Test-on-Real (TSTR) accuracy ä¸»è¦å…³æ³¨åˆ†å¸ƒç›¸ä¼¼æ€§æˆ–é¢„æµ‹æ€§èƒ½ï¼Œæ— æ³•æœ‰æ•ˆè¯„ä¼°æ¨¡å‹åœ¨åˆæˆæ•°æ®ä¸Šçš„æ¨ç†æ¨¡å¼æ˜¯å¦ä¸çœŸå®æ•°æ®ä¸€è‡´ã€‚SHAP Distance è¢«å®šä¹‰ä¸ºåœ¨çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šè®­ç»ƒçš„åˆ†ç±»å™¨æ‰€äº§ç”Ÿçš„å…¨å±€ SHapley Additive exPlanations (SHAP) å½’å› å‘é‡ä¹‹é—´çš„ä½™å¼¦è·ç¦» (Cosine Distance)ã€‚ç ”ç©¶é€šè¿‡åˆ†æä¸´åºŠå¥åº·è®°å½•ã€ä¼ä¸šå‘ç¥¨äº¤æ˜“å’Œç”µä¿¡æµå¤±æ—¥å¿—ç­‰å¼‚æ„æ•°æ®é›†ï¼Œè¯æ˜äº†è¯¥æŒ‡æ ‡èƒ½å¯é åœ°è¯†åˆ«æ ‡å‡†ç»Ÿè®¡æ–¹æ³•æ‰€å¿½è§†çš„è¯­ä¹‰åå·®ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSHAP Distance ç‰¹åˆ«æ“…é•¿æ•æ‰ç‰¹å¾é‡è¦æ€§çš„åç§»ä»¥åŠä»£è¡¨æ€§ä¸è¶³çš„é•¿å°¾æ•ˆåº”ã€‚è¯¥ç ”ç©¶å°† SHAP Distance å®šä½ä¸ºä¸€ç§å®ç”¨ä¸”å…·æœ‰é‰´åˆ«åŠ›çš„å®¡è®¡å·¥å…·ï¼Œå¹¶ä¸ºåœ¨æœªæ¥åŸºå‡†æµ‹è¯•ä¸­é›†æˆåŸºäºå½’å› çš„è¯„ä¼°æä¾›äº†å®è·µæŒ‡å—ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "IEEE Bigdata",
      "pdf_url": "https://arxiv.org/pdf/2511.17590v1",
      "published_date": "2025-11-17 03:47:47 UTC",
      "updated_date": "2025-11-17 03:47:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:10:19.039271+00:00"
    },
    {
      "arxiv_id": "2511.12937v2",
      "title": "Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models",
      "title_zh": "Yanyun-3ï¼šåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹å®ç°è·¨å¹³å°ç­–ç•¥æ¸¸æˆæ“ä½œ",
      "authors": [
        "Guoyan Wang",
        "Yanyan Huang",
        "Chunlin Chen",
        "Lifeng Wang",
        "Yuxiang Sun"
      ],
      "abstract": "Cross-platform strategy game automation remains a challenge due to diverse user interfaces and dynamic battlefield environments. Existing Vision--Language Models (VLMs) struggle with generalization across heterogeneous platforms and lack precision in interface understanding and action execution. We introduce Yanyun-3, a VLM-based agent that integrates Qwen2.5-VL for visual reasoning and UI-TARS for interface execution. We propose a novel data organization principle -- combination granularity -- to distinguish intra-sample fusion and inter-sample mixing of multimodal data (static images, multi-image sequences, and videos). The model is fine-tuned using QLoRA on a curated dataset across three strategy game platforms. The optimal strategy (M*V+S) achieves a 12.98x improvement in BLEU-4 score and a 63% reduction in inference time compared to full fusion. Yanyun-3 successfully executes core tasks (e.g., target selection, resource allocation) across platforms without platform-specific tuning. Our findings demonstrate that structured multimodal data organization significantly enhances VLM performance in embodied tasks. Yanyun-3 offers a generalizable framework for GUI automation, with broader implications for robotics and autonomous systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è·¨å¹³å°ç­–ç•¥æ¸¸æˆè‡ªåŠ¨åŒ–é¢ä¸´çš„å¤šæ ·ç•Œé¢å’ŒåŠ¨æ€ç¯å¢ƒæŒ‘æˆ˜ï¼Œæå‡ºäº†åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹(VLMs)çš„æ™ºèƒ½ä½“Yanyun-3ã€‚è¯¥æ¨¡å‹é€šè¿‡é›†æˆQwen2.5-VLè¿›è¡Œè§†è§‰æ¨ç†å¹¶åˆ©ç”¨UI-TARSè¿›è¡Œç•Œé¢æ‰§è¡Œï¼Œå¹¶å¼•å…¥äº†ç»„åˆç²’åº¦(combination granularity)è¿™ä¸€æ–°å‹æ•°æ®ç»„ç»‡åŸåˆ™æ¥ä¼˜åŒ–å¤šæ¨¡æ€æ•°æ®çš„å¤„ç†ã€‚ç ”ç©¶é‡‡ç”¨QLoRAæŠ€æœ¯åœ¨è·¨å¹³å°æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶æœ€ä¼˜ç­–ç•¥åœ¨BLEU-4å¾—åˆ†ä¸Šæå‡äº†12.98å€ï¼ŒåŒæ—¶æ¨ç†æ—¶é—´å‡å°‘äº†63%ã€‚Yanyun-3æ— éœ€å¹³å°ç‰¹å®šè°ƒä¼˜å³å¯è·¨å¹³å°å®Œæˆç›®æ ‡é€‰æ‹©å’Œèµ„æºåˆ†é…ç­‰æ ¸å¿ƒä»»åŠ¡ï¼Œè¯æ˜äº†ç»“æ„åŒ–å¤šæ¨¡æ€æ•°æ®ç»„ç»‡å¯¹æå‡å…·èº«æ™ºèƒ½ä»»åŠ¡æ€§èƒ½çš„æ˜¾è‘—ä½œç”¨ã€‚è¿™ä¸€æˆæœä¸ºGUIè‡ªåŠ¨åŒ–æä¾›äº†å…·æœ‰æ³›åŒ–èƒ½åŠ›çš„æ¡†æ¶ï¼Œå¹¶åœ¨æœºå™¨äººå’Œæ— äººç³»ç»Ÿé¢†åŸŸå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "32 pages, 13 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.12937v2",
      "published_date": "2025-11-17 03:45:15 UTC",
      "updated_date": "2025-11-24 07:51:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:10:48.635249+00:00"
    },
    {
      "arxiv_id": "2511.12936v1",
      "title": "Privacy-Preserving Federated Learning from Partial Decryption Verifiable Threshold Multi-Client Functional Encryption",
      "title_zh": "åŸºäºéƒ¨åˆ†è§£å¯†å¯éªŒè¯é—¨é™å¤šå®¢æˆ·ç«¯å‡½æ•°åŠ å¯†çš„éšç§ä¿æŠ¤è”é‚¦å­¦ä¹ ",
      "authors": [
        "Minjie Wang",
        "Jinguang Han",
        "Weizhi Meng"
      ],
      "abstract": "In federated learning, multiple parties can cooperate to train the model without directly exchanging their own private data, but the gradient leakage problem still threatens the privacy security and model integrity. Although the existing scheme uses threshold cryptography to mitigate the inference attack, it can not guarantee the verifiability of the aggregation results, making the system vulnerable to the threat of poisoning attack. We construct a partial decryption verifiable threshold multi client function encryption scheme, and apply it to Federated learning to implement the federated learning verifiable threshold security aggregation protocol (VTSAFL). VTSAFL empowers clients to verify aggregation results, concurrently minimizing both computational and communication overhead. The size of the functional key and partial decryption results of the scheme are constant, which provides efficiency guarantee for large-scale deployment. The experimental results on MNIST dataset show that vtsafl can achieve the same accuracy as the existing scheme, while reducing the total training time by more than 40%, and reducing the communication overhead by up to 50%. This efficiency is critical for overcoming the resource constraints inherent in Internet of Things (IoT) devices.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Federated Learningä¸­çš„æ¢¯åº¦æ³„éœ²å’ŒæŠ•æ¯’æ”»å‡»å¨èƒï¼Œæå‡ºäº†ä¸€ç§åŸºäºéƒ¨åˆ†è§£å¯†å¯éªŒè¯é˜ˆå€¼å¤šå®¢æˆ·ç«¯å‡½æ•°åŠ å¯†(Partial Decryption Verifiable Threshold Multi-Client Functional Encryption)çš„å¯éªŒè¯é˜ˆå€¼å®‰å…¨èšåˆåè®®VTSAFLã€‚è¯¥åè®®å…è®¸å®¢æˆ·ç«¯éªŒè¯èšåˆç»“æœçš„æ­£ç¡®æ€§ï¼Œå¹¶åˆ©ç”¨æ’å®šå¤§å°çš„åŠŸèƒ½å¯†é’¥å’Œéƒ¨åˆ†è§£å¯†ç»“æœæ˜¾è‘—é™ä½äº†ç³»ç»Ÿçš„è®¡ç®—ä¸é€šä¿¡å¼€é”€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨MNISTæ•°æ®é›†ä¸Šï¼ŒVTSAFLåœ¨ç»´æŒåŸæœ‰æ¨¡å‹å‡†ç¡®ç‡çš„åŸºç¡€ä¸Šï¼Œä½¿æ€»è®­ç»ƒæ—¶é—´ç¼©çŸ­äº†40%ä»¥ä¸Šï¼Œé€šä¿¡å¼€é”€æœ€é«˜é™ä½äº†50%ã€‚è¿™ç§é«˜æ•ˆæ€§ä½¿å…¶èƒ½å¤Ÿæœ‰æ•ˆå…‹æœç‰©è”ç½‘(IoT)è®¾å¤‡ä¸­å›ºæœ‰çš„èµ„æºé™åˆ¶é—®é¢˜ã€‚è¯¥æ–¹æ¡ˆåœ¨å¤§è§„æ¨¡éšç§ä¿æŠ¤è”é‚¦å­¦ä¹ éƒ¨ç½²ä¸­å…·æœ‰æ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿å’Œå®‰å…¨æ€§ä¿éšœã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12936v1",
      "published_date": "2025-11-17 03:44:47 UTC",
      "updated_date": "2025-11-17 03:44:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:10:52.529320+00:00"
    },
    {
      "arxiv_id": "2511.12935v2",
      "title": "PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos",
      "title_zh": "PFAvatarï¼šåŸºäºçœŸå®ä¸–ç•Œ OOTD ç…§ç‰‡çš„å§¿æ€èåˆ 3D ä¸ªæ€§åŒ–åŒ–èº«é‡å»º",
      "authors": [
        "Dianbing Xi",
        "Guoyuan An",
        "Jingsen Zhu",
        "Zhijian Liu",
        "Yuan Liu",
        "Ruiyuan Zhang",
        "Jiayuan Lu",
        "Yuchi Huo",
        "Rui Wang"
      ],
      "abstract": "We propose PFAvatar (Pose-Fusion Avatar), a new method that reconstructs high-quality 3D avatars from Outfit of the Day(OOTD) photos, which exhibit diverse poses, occlusions, and complex backgrounds. Our method consists of two stages: (1) fine-tuning a pose-aware diffusion model from few-shot OOTD examples and (2) distilling a 3D avatar represented by a neural radiance field (NeRF). In the first stage, unlike previous methods that segment images into assets (e.g., garments, accessories) for 3D assembly, which is prone to inconsistency, we avoid decomposition and directly model the full-body appearance. By integrating a pre-trained ControlNet for pose estimation and a novel Condition Prior Preservation Loss (CPPL), our method enables end-to-end learning of fine details while mitigating language drift in few-shot training. Our method completes personalization in just 5 minutes, achieving a 48x speed-up compared to previous approaches. In the second stage, we introduce a NeRF-based avatar representation optimized by canonical SMPL-X space sampling and Multi-Resolution 3D-SDS. Compared to mesh-based representations that suffer from resolution-dependent discretization and erroneous occluded geometry, our continuous radiance field can preserve high-frequency textures (e.g., hair) and handle occlusions correctly through transmittance. Experiments demonstrate that PFAvatar outperforms state-of-the-art methods in terms of reconstruction fidelity, detail preservation, and robustness to occlusions/truncations, advancing practical 3D avatar generation from real-world OOTD albums. In addition, the reconstructed 3D avatar supports downstream applications such as virtual try-on, animation, and human video reenactment, further demonstrating the versatility and practical value of our approach.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PFAvatar (Pose-Fusion Avatar)ï¼Œä¸€ç§èƒ½å¤Ÿä»çœŸå®ä¸–ç•Œçš„ OOTD ç…§ç‰‡ä¸­é‡å»ºé«˜è´¨é‡ 3D ä¸ªæ€§åŒ–åŒ–èº«çš„åˆ›æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆé€šè¿‡é›†æˆé¢„è®­ç»ƒçš„ ControlNet å’Œä¸€ç§æ–°å‹çš„ Condition Prior Preservation Loss (CPPL) æ¥å¾®è°ƒå§¿æ€æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ï¼Œåœ¨é¿å…èµ„äº§åˆ†è§£çš„åŒæ—¶æœ‰æ•ˆç¼“è§£äº†å°‘æ ·æœ¬è®­ç»ƒä¸­çš„è¯­è¨€æ¼‚ç§»é—®é¢˜ã€‚ç³»ç»Ÿä»…éœ€ 5 åˆ†é’Ÿå³å¯å®Œæˆä¸ªæ€§åŒ–å»ºæ¨¡ï¼Œé€Ÿåº¦æ¯”ä»¥å¾€æ–¹æ³•æå‡äº† 48 å€ã€‚éšåï¼Œç ”ç©¶å¼•å…¥äº†åŸºäº NeRF çš„åŒ–èº«è¡¨ç¤ºï¼Œå¹¶ç»“åˆè§„èŒƒ SMPL-X ç©ºé—´é‡‡æ ·å’Œ Multi-Resolution 3D-SDS è¿›è¡Œä¼˜åŒ–ã€‚ç›¸æ¯”äºä¼ ç»Ÿçš„ç½‘æ ¼è¡¨ç¤ºï¼Œè¿™ç§è¿ç»­çš„è¾å°„åœºèƒ½æ›´å¥½åœ°å¤„ç†é®æŒ¡å¹¶ä¿ç•™å¤´å‘ç­‰é«˜é¢‘çº¹ç†ã€‚å®éªŒè¯æ˜ PFAvatar åœ¨é‡å»ºä¿çœŸåº¦ã€ç»†èŠ‚ä¿ç•™åŠé²æ£’æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰ SOTA æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜æ”¯æŒè™šæ‹Ÿè¯•ç©¿å’Œäººä½“è§†é¢‘é‡æ¼”ç­‰å¤šç§å®é™…åº”ç”¨åœºæ™¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.12935v2",
      "published_date": "2025-11-17 03:40:43 UTC",
      "updated_date": "2025-11-18 05:47:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:10:52.141414+00:00"
    },
    {
      "arxiv_id": "2511.12922v1",
      "title": "Tokenize Once, Recommend Anywhere: Unified Item Tokenization for Multi-domain LLM-based Recommendation",
      "title_zh": "ä¸€æ¬¡åˆ†è¯ï¼Œå…¨åŸŸæ¨èï¼šé¢å‘å¤šé¢†åŸŸå¤§è¯­è¨€æ¨¡å‹æ¨èçš„ç»Ÿä¸€ç‰©å“åˆ†è¯",
      "authors": [
        "Yu Hou",
        "Won-Yong Shin"
      ],
      "abstract": "Large language model (LLM)-based recommender systems have achieved high-quality performance by bridging the discrepancy between the item space and the language space through item tokenization. However, existing item tokenization methods typically require training separate models for each item domain, limiting generalization. Moreover, the diverse distributions and semantics across item domains make it difficult to construct a unified tokenization that preserves domain-specific information. To address these challenges, we propose UniTok, a Unified item Tokenization framework that integrates our own mixture-of-experts (MoE) architecture with a series of codebooks to convert items into discrete tokens, enabling scalable tokenization while preserving semantic information across multiple item domains. Specifically, items from different domains are first projected into a unified latent space through a shared encoder. They are then routed to domain-specific experts to capture the unique semantics, while a shared expert, which is always active, encodes common knowledge transferable across domains. Additionally, to mitigate semantic imbalance across domains, we present a mutual information calibration mechanism, which guides the model towards retaining similar levels of semantic information for each domain. Comprehensive experiments on wide-ranging real-world datasets demonstrate that the proposed UniTok framework is (a) highly effective: achieving up to 51.89% improvements over strong benchmarks, (b) theoretically sound: showing the analytical validity of our architectural design and optimization; and (c) highly generalizable: demonstrating robust performance across diverse domains without requiring per-domain retraining, a capability not supported by existing baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†UniTokï¼Œä¸€ç§ç»Ÿä¸€çš„é¡¹ç›®åˆ†è¯(Unified item Tokenization)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„æ¨èç³»ç»Ÿåœ¨å¤šé¢†åŸŸåº”ç”¨ä¸­é¢ä¸´çš„æ³›åŒ–èƒ½åŠ›æœ‰é™åŠè¯­ä¹‰ä¿ç•™å›°éš¾ç­‰æŒ‘æˆ˜ã€‚UniToké›†æˆäº†æ··åˆä¸“å®¶æ¶æ„(Mixture-of-Experts, MoE)ä¸ä¸€ç³»åˆ—ç æœ¬(codebooks)ï¼Œé€šè¿‡å…±äº«ç¼–ç å™¨å°†ä¸åŒé¢†åŸŸçš„é¡¹ç›®æ˜ å°„è‡³ç»Ÿä¸€æ½œåœ¨ç©ºé—´ï¼Œå¹¶åˆ©ç”¨é¢†åŸŸç‰¹å®šä¸“å®¶ä¸å…±äº«ä¸“å®¶ååŒæ•æ‰ç‹¬ç‰¹è¯­ä¹‰åŠè·¨é¢†åŸŸé€šç”¨çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†äº’ä¿¡æ¯æ ¡å‡†(mutual information calibration)æœºåˆ¶ï¼Œæœ‰æ•ˆç¼“è§£äº†è·¨é¢†åŸŸçš„è¯­ä¹‰ä¸å¹³è¡¡é—®é¢˜ã€‚å®éªŒè¯æ˜ï¼ŒUniTokåœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šç›¸è¾ƒäºåŸºçº¿æ¨¡å‹å®ç°äº†é«˜è¾¾51.89%çš„æ€§èƒ½æå‡ï¼Œä¸”æ— éœ€é’ˆå¯¹ç‰¹å®šé¢†åŸŸè¿›è¡Œé‡æ–°è®­ç»ƒå³å¯ä¿æŒé²æ£’çš„æ³›åŒ–è¡¨ç°ã€‚è¿™ä¸€ç ”ç©¶ä¸ä»…åœ¨ç†è®ºä¸Šè¯æ˜äº†æ¶æ„è®¾è®¡çš„æœ‰æ•ˆæ€§ï¼Œä¹Ÿä¸ºå¤§è§„æ¨¡ã€å¤šé¢†åŸŸçš„æ¨èç³»ç»Ÿæä¾›äº†é«˜æ•ˆçš„ç»Ÿä¸€åˆ†è¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG",
        "cs.NE",
        "cs.SI"
      ],
      "primary_category": "cs.IR",
      "comment": "20 pages, 8 figures, 9 tables; Annual AAAI Conference on Artificial Intelligence (AAAI-26) (to appear) (Please cite our conference version.)",
      "pdf_url": "https://arxiv.org/pdf/2511.12922v1",
      "published_date": "2025-11-17 03:18:04 UTC",
      "updated_date": "2025-11-17 03:18:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:11:26.229679+00:00"
    },
    {
      "arxiv_id": "2511.12920v2",
      "title": "Auditing Google's AI Overviews and Featured Snippets: A Case Study on Baby Care and Pregnancy",
      "title_zh": "å®¡è®¡ Google çš„ AI æ¦‚è§ˆä¸ç²¾é€‰æ‘˜è¦ï¼šå©´å„¿æŠ¤ç†ä¸å­•æœŸæ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Desheng Hu",
        "Joachim Baumann",
        "Aleksandra Urman",
        "Elsa Lichtenegger",
        "Robin Forsberg",
        "Aniko Hannak",
        "Christo Wilson"
      ],
      "abstract": "Google Search increasingly surfaces AI-generated content through features like AI Overviews (AIO) and Featured Snippets (FS), which users frequently rely on despite having no control over their presentation. Through a systematic algorithm audit of 1,508 real baby care and pregnancy-related queries, we evaluate the quality and consistency of these information displays. Our robust evaluation framework assesses multiple quality dimensions, including answer consistency, relevance, presence of medical safeguards, source categories, and sentiment alignment. Our results reveal concerning gaps in information consistency, with information in AIO and FS displayed on the same search result page being inconsistent with each other in 33% of cases. Despite high relevance scores, both features critically lack medical safeguards (present in just 11% of AIO and 7% of FS responses). While health and wellness websites dominate source categories for both, AIO and FS, FS also often link to commercial sources. These findings have important implications for public health information access and demonstrate the need for stronger quality controls in AI-mediated health information. Our methodology provides a transferable framework for auditing AI systems across high-stakes domains where information quality directly impacts user well-being.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹è°·æ­Œæœç´¢ä¸­çš„ AI Overviews (AIO) å’Œ Featured Snippets (FS) è¿›è¡Œäº†ç³»ç»Ÿæ€§çš„ç®—æ³•å®¡è®¡ï¼Œé‡ç‚¹æ¢è®¨äº†å©´å„¿æŠ¤ç†ä¸æ€€å­•ç›¸å…³ä¿¡æ¯å±•ç¤ºçš„è´¨é‡ã€‚ä½œè€…é€šè¿‡å¯¹1,508ä¸ªçœŸå®æŸ¥è¯¢çš„å¤šç»´åº¦è¯„ä¼°ï¼Œå»ºç«‹äº†ä¸€ä¸ªè€ƒå¯Ÿä¿¡æ¯ä¸€è‡´æ€§ã€åŒ»ç–—ä¿éšœ (medical safeguards) å’Œæ¥æºç±»åˆ«ç­‰ç»´åº¦çš„é²æ£’è¯„ä¼°æ¡†æ¶ã€‚è°ƒæŸ¥å‘ç°ï¼ŒåŒä¸€æœç´¢ç»“æœé¡µé¢ä¸Šçš„ AIO å’Œ FS åœ¨ 33% çš„æƒ…å†µä¸‹å­˜åœ¨ä¿¡æ¯ä¸ä¸€è‡´ç°è±¡ã€‚å°½ç®¡ç­”æ¡ˆç›¸å…³æ€§è¾ƒé«˜ï¼Œä½†ä¸¤è€…å‡ä¸¥é‡ç¼ºä¹åŒ»ç–—ä¿éšœæªæ–½ï¼ŒAIO çš„è¦†ç›–ç‡ä»…ä¸º 11%ï¼Œè€Œ FS ä»…ä¸º 7%ã€‚æ­¤å¤–ï¼ŒFS è¾ƒ AIO æ›´å¸¸é“¾æ¥è‡³å•†ä¸šæ¥æºï¼Œå…¶ä¿¡æ¯å‡†ç¡®æ€§ä¸å…¬å…±å¥åº·é£é™©å€¼å¾—å…³æ³¨ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åœ¨ AI ä¸­ä»‹çš„å¥åº·ä¿¡æ¯ä¼ æ’­ä¸­åŠ å¼ºè´¨é‡æ§åˆ¶çš„ç´§è¿«æ€§ï¼Œå¹¶ä¸ºé«˜é£é™©é¢†åŸŸçš„ AI ç³»ç»Ÿå®¡è®¡æä¾›äº†ä¸€ä¸ªå¯è¿ç§»çš„è¯„ä¼°æ¡†æ¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 10 figures; to appear in AAAI ICWSM 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.12920v2",
      "published_date": "2025-11-17 03:16:36 UTC",
      "updated_date": "2025-11-19 20:16:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:11:34.429421+00:00"
    },
    {
      "arxiv_id": "2511.12916v1",
      "title": "Fault2Flow: An AlphaEvolve-Optimized Human-in-the-Loop Multi-Agent System for Fault-to-Workflow Automation",
      "title_zh": "Fault2Flowï¼šä¸€ç§ç» AlphaEvolve ä¼˜åŒ–çš„é¢å‘æ•…éšœ-å·¥ä½œæµè‡ªåŠ¨åŒ–çš„äººåœ¨å›è·¯å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ",
      "authors": [
        "Yafang Wang",
        "Yangjie Tian",
        "Xiaoyu Shen",
        "Gaoyang Zhang",
        "Jiaze Sun",
        "He Zhang",
        "Ruohua Xu",
        "Feng Zhao"
      ],
      "abstract": "Power grid fault diagnosis is a critical process hindered by its reliance on manual, error-prone methods. Technicians must manually extract reasoning logic from dense regulations and attempt to combine it with tacit expert knowledge, which is inefficient, error-prone, and lacks maintainability as ragulations are updated and experience evolves. While Large Language Models (LLMs) have shown promise in parsing unstructured text, no existing framework integrates these two disparate knowledge sources into a single, verified, and executable workflow. To bridge this gap, we propose Fault2Flow, an LLM-based multi-agent system. Fault2Flow systematically: (1) extracts and structures regulatory logic into PASTA-formatted fault trees; (2) integrates expert knowledge via a human-in-the-loop interface for verification; (3) optimizes the reasoning logic using a novel AlphaEvolve module; and (4) synthesizes the final, verified logic into an n8n-executable workflow. Experimental validation on transformer fault diagnosis datasets confirms 100\\% topological consistency and high semantic fidelity. Fault2Flow establishes a reproducible path from fault analysis to operational automation, substantially reducing expert workload.",
      "tldr_zh": "é’ˆå¯¹ç”µç½‘æ•…éšœè¯Šæ–­ä¸­è¿‡åº¦ä¾èµ–äººå·¥ã€æ˜“å‡ºé”™ä¸”éš¾ä»¥æœ‰æ•ˆæ•´åˆè§„ç¨‹é€»è¾‘ä¸ä¸“å®¶ç»éªŒçš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº† Fault2Flowï¼Œä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé¦–å…ˆå°†è§„ç¨‹é€»è¾‘æå–å¹¶ç»“æ„åŒ–ä¸º PASTA æ ¼å¼çš„æ•…éšœæ ‘ï¼Œéšåé€šè¿‡ Human-in-the-Loop ç•Œé¢å¼•å…¥ä¸“å®¶çŸ¥è¯†è¿›è¡ŒéªŒè¯ï¼Œå¹¶åˆ©ç”¨åˆ›æ–°çš„ AlphaEvolve æ¨¡å—ä¼˜åŒ–æ¨ç†é€»è¾‘ã€‚æœ€åï¼ŒFault2Flow å°†éªŒè¯åçš„é€»è¾‘åˆæˆä¸ºå¯ç›´æ¥åœ¨ n8n å¹³å°æ‰§è¡Œçš„å·¥ä½œæµï¼Œå®ç°äº†ä»æ•…éšœåˆ†æåˆ°è¿è¡Œè‡ªåŠ¨åŒ–çš„é—­ç¯ã€‚åœ¨å˜å‹å™¨æ•…éšœè¯Šæ–­æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿå®ç°äº† 100% çš„æ‹“æ‰‘ä¸€è‡´æ€§å’Œæé«˜çš„è¯­ä¹‰ä¿çœŸåº¦ã€‚è¯¥ç ”ç©¶ä¸ºæ•…éšœè¯Šæ–­è‡ªåŠ¨åŒ–å»ºç«‹äº†ä¸€ç§å¯é‡å¤çš„è·¯å¾„ï¼Œæ˜¾è‘—é™ä½äº†ä¸“å®¶çš„å·¥ä½œé‡ï¼Œå¹¶å¢å¼ºäº†é€»è¾‘çš„å¯ç»´æŠ¤æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12916v1",
      "published_date": "2025-11-17 03:07:40 UTC",
      "updated_date": "2025-11-17 03:07:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:11:25.530796+00:00"
    },
    {
      "arxiv_id": "2511.12913v1",
      "title": "CoS: Towards Optimal Event Scheduling via Chain-of-Scheduling",
      "title_zh": "CoSï¼šåŸºäºè°ƒåº¦é“¾å®ç°æœ€ä¼˜äº‹ä»¶è°ƒåº¦",
      "authors": [
        "Yiming Zhao",
        "Jiwei Tang",
        "Shimin Di",
        "Libin Zheng",
        "Jianxing Yu",
        "Jian Yin"
      ],
      "abstract": "Recommending event schedules is a key issue in Event-based Social Networks (EBSNs) in order to maintain user activity. An effective recommendation is required to maximize the user's preference, subjecting to both time and geographical constraints. Existing methods face an inherent trade-off among efficiency, effectiveness, and generalization, due to the NP-hard nature of the problem. This paper proposes the Chain-of-Scheduling (CoS) framework, which activates the event scheduling capability of Large Language Models (LLMs) through a guided, efficient scheduling process. CoS enhances LLM by formulating the schedule task into three atomic stages, i.e., exploration, verification and integration. Then we enable the LLMs to generate CoS autonomously via Knowledge Distillation (KD). Experimental results show that CoS achieves near-theoretical optimal effectiveness with high efficiency on three real-world datasets in a interpretable manner. Moreover, it demonstrates strong zero-shot learning ability on out-of-domain data.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºäº‹ä»¶çš„ç¤¾äº¤ç½‘ç»œ(EBSNs)ä¸­çš„äº‹ä»¶è°ƒåº¦æ¨èé—®é¢˜ï¼Œæå‡ºäº†Chain-of-Scheduling (CoS)æ¡†æ¶ï¼Œæ—¨åœ¨æ»¡è¶³æ—¶é—´ä¸åœ°ç†çº¦æŸçš„åŒæ—¶æœ€å¤§åŒ–ç”¨æˆ·åå¥½ã€‚ç”±äºè¯¥é—®é¢˜å…·æœ‰NP-hardç‰¹æ€§ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€éš¾ä»¥åœ¨æ•ˆç‡ã€æ•ˆæœä¸æ³›åŒ–æ€§ä¹‹é—´è¾¾æˆå¹³è¡¡ã€‚CoSæ¡†æ¶é€šè¿‡å°†è°ƒåº¦ä»»åŠ¡åˆ†è§£ä¸ºæ¢ç´¢(exploration)ã€éªŒè¯(verification)å’Œæ•´åˆ(integration)ä¸‰ä¸ªåŸå­é˜¶æ®µï¼ŒæˆåŠŸæ¿€æ´»äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„äº‹ä»¶è°ƒåº¦èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿè¿›ä¸€æ­¥åˆ©ç”¨çŸ¥è¯†è’¸é¦(Knowledge Distillation)æŠ€æœ¯ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»ç”ŸæˆCoSæ¨ç†é“¾ã€‚å®éªŒè¯æ˜ï¼ŒCoSåœ¨ä¸‰ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šä»¥å¯è§£é‡Šçš„æ–¹å¼å®ç°äº†æ¥è¿‘ç†è®ºæœ€ä¼˜çš„æ•ˆæœï¼Œä¸”ä¿æŒäº†æé«˜çš„è¿è¡Œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨åŸŸå¤–æ•°æ®ä¸Šå±•ç°å‡ºäº†å¼ºå¤§çš„é›¶æ ·æœ¬å­¦ä¹ (zero-shot learning)èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12913v1",
      "published_date": "2025-11-17 03:01:47 UTC",
      "updated_date": "2025-11-17 03:01:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:11:41.234849+00:00"
    },
    {
      "arxiv_id": "2511.12908v1",
      "title": "DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video Reasoning via Agentic Reinforcement Learning",
      "title_zh": "DeepSportï¼šåŸºäºæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„å…¨æ–¹ä½ä½“è‚²è§†é¢‘æ¨ç†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Junbo Zou",
        "Haotian Xia",
        "Zhen Ye",
        "Shengjie Zhang",
        "Christopher Lai",
        "Vicente Ordonez",
        "Weining Shen",
        "Hanjie Chen"
      ],
      "abstract": "Sports video understanding presents unique challenges, requiring models to perceive high-speed dynamics, comprehend complex rules, and reason over long temporal contexts. While Multimodal Large Language Models (MLLMs) have shown promise in genral domains, the current state of research in sports remains narrowly focused: existing approaches are either single-sport centric, limited to specific tasks, or rely on training-free paradigms that lack robust, learned reasoning process. To address this gap, we introduce DeepSport, the first end-to-end trained MLLM framework designed for multi-task, multi-sport video understanding. DeepSport shifts the paradigm from passive frame processing to active, iterative reasoning, empowering the model to ``think with videos'' by dynamically interrogating content via a specialized frame-extraction tool. To enable this, we propose a data distillation pipeline that synthesizes high-quality Chain-of-Thought (CoT) trajectories from 10 diverse data source, creating a unified resource of 78k training data. We then employ a two-stage training strategy, Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) with a novel gated tool-use reward, to optimize the model's reasoning process. Extensive experiments on the testing benchmark of 6.7k questions demonstrate that DeepSport achieves state-of-the-art performance, significantly outperforming baselines of both proprietary model and open-source models. Our work establishes a new foundation for domain-specific video reasoning to address the complexities of diverse sports.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DeepSportï¼Œè¿™æ˜¯é¦–ä¸ªä¸“ä¸ºå¤šä»»åŠ¡å’Œå¤šè¿åŠ¨è§†é¢‘ç†è§£è®¾è®¡çš„ç«¯åˆ°ç«¯è®­ç»ƒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLM) æ¡†æ¶ã€‚é’ˆå¯¹ä½“è‚²è§†é¢‘ä¸­é«˜é€ŸåŠ¨æ€ã€å¤æ‚è§„åˆ™å’Œé•¿æ—¶åºèƒŒæ™¯å¸¦æ¥çš„æŒ‘æˆ˜ï¼ŒDeepSport å°†èŒƒå¼ä»è¢«åŠ¨å¸§å¤„ç†è½¬å˜ä¸ºä¸»åŠ¨ã€è¿­ä»£çš„æ¨ç†ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€šè¿‡ä¸“é—¨çš„æŠ½å¸§å·¥å…·åŠ¨æ€æŸ¥è¯¢å†…å®¹å¹¶å®ç°â€œä»¥è§†é¢‘æ€è€ƒâ€ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ç§æ•°æ®è’¸é¦æµæ°´çº¿ï¼Œä»10ä¸ªæ•°æ®æºä¸­åˆæˆäº†7.8ä¸‡æ¡é«˜è´¨é‡çš„é“¾å¼æ€ç»´ (Chain-of-Thought) è½¨è¿¹ï¼Œå¹¶é‡‡ç”¨ç›‘ç£å¾®è°ƒ (SFT) ç»“åˆå¸¦æœ‰é—¨æ§å·¥å…·ä½¿ç”¨å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹  (RL) ä¸¤é˜¶æ®µç­–ç•¥æ¥ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDeepSport åœ¨åŒ…å«6700ä¸ªé—®é¢˜çš„æµ‹è¯•åŸºå‡†ä¸Šå–å¾—äº†å½“å‰æœ€å…ˆè¿› (SOTA) çš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„é—­æºå’Œå¼€æºæ¨¡å‹ã€‚è¯¥å·¥ä½œä¸ºå¤„ç†å¤æ‚å¤šæ ·çš„ç‰¹å®šé¢†åŸŸè§†é¢‘æ¨ç†å¥ å®šäº†æ–°çš„åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12908v1",
      "published_date": "2025-11-17 02:57:15 UTC",
      "updated_date": "2025-11-17 02:57:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:11:40.342832+00:00"
    },
    {
      "arxiv_id": "2511.13794v1",
      "title": "FusionFM: All-in-One Multi-Modal Image Fusion with Flow Matching",
      "title_zh": "FusionFMï¼šåŸºäºæµåŒ¹é…çš„å…¨èƒ½å¤šæ¨¡æ€å›¾åƒèåˆ",
      "authors": [
        "Huayi Zhu",
        "Xiu Shu",
        "Youqiang Xiong",
        "Qiao Liu",
        "Rui Chen",
        "Di Yuan",
        "Xiaojun Chang",
        "Zhenyu He"
      ],
      "abstract": "Current multi-modal image fusion methods typically rely on task-specific models, leading to high training costs and limited scalability. While generative methods provide a unified modeling perspective, they often suffer from slow inference due to the complex sampling trajectories from noise to image. To address this, we formulate image fusion as a direct probabilistic transport from source modalities to the fused image distribution, leveraging the flow matching paradigm to improve sampling efficiency and structural consistency. To mitigate the lack of high-quality fused images for supervision, we collect fusion results from multiple state-of-the-art models as priors, and employ a task-aware selection function to select the most reliable pseudo-labels for each task. We further introduce a Fusion Refiner module that employs a divide-and-conquer strategy to systematically identify, decompose, and enhance degraded components in selected pseudo-labels. For multi-task scenarios, we integrate elastic weight consolidation and experience replay mechanisms to preserve cross-task performance and enhance continual learning ability from both parameter stability and memory retention perspectives. Our approach achieves competitive performance across diverse fusion tasks, while significantly improving sampling efficiency and maintaining a lightweight model design. The code will be available at: https://github.com/Ist-Zhy/FusionFM.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FusionFMï¼Œè¿™æ˜¯ä¸€ç§åŸºäº Flow Matching èŒƒå¼çš„å…¨èƒ½å‹å¤šæ¨¡æ€å›¾åƒèåˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç”Ÿæˆå¼æ–¹æ³•æ¨ç†é€Ÿåº¦æ…¢ä¸”ä»»åŠ¡æ‰©å±•æ€§å·®çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹å°†å›¾åƒèåˆå»ºæ¨¡ä¸ºä»æºæ¨¡æ€åˆ°èåˆå›¾åƒåˆ†å¸ƒçš„ç›´æ¥æ¦‚ç‡ä¼ è¾“ï¼Œåœ¨æ˜¾è‘—æå‡é‡‡æ ·æ•ˆç‡çš„åŒæ—¶å¢å¼ºäº†å›¾åƒçš„ç»“æ„ä¸€è‡´æ€§ã€‚é’ˆå¯¹ç¼ºä¹é«˜è´¨é‡èåˆå›¾åƒä½œä¸ºç›‘ç£ä¿¡å·çš„æŒ‘æˆ˜ï¼Œç ”ç©¶è€…é€šè¿‡ä»»åŠ¡æ„ŸçŸ¥é€‰æ‹©å‡½æ•°ä»å¤šä¸ª SOTA æ¨¡å‹ä¸­ç­›é€‰ä¼ªæ ‡ç­¾ï¼Œå¹¶å¼•å…¥ Fusion Refiner æ¨¡å—ä»¥åˆ†è€Œæ²»ä¹‹çš„ç­–ç•¥ä¿®å¤å¹¶å¢å¼ºå›¾åƒç»„ä»¶ã€‚æ­¤å¤–ï¼Œä¸ºäº†åº”å¯¹å¤šä»»åŠ¡å­¦ä¹ ä¸­çš„ç¾éš¾æ€§é—å¿˜ï¼Œæ¡†æ¶é›†æˆäº† Elastic Weight Consolidation (EWC) å’Œ Experience Replay æœºåˆ¶ï¼Œä»å‚æ•°ç¨³å®šæ€§å’Œè®°å¿†ä¿ç•™ä¸¤ä¸ªç»´åº¦æå‡æŒç»­å­¦ä¹ èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFusionFM åœ¨å¤šç§å›¾åƒèåˆä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºæå…·ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œä¸”ä¿æŒäº†è½»é‡åŒ–çš„æ¨¡å‹è®¾è®¡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13794v1",
      "published_date": "2025-11-17 02:56:48 UTC",
      "updated_date": "2025-11-17 02:56:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:11:51.339228+00:00"
    },
    {
      "arxiv_id": "2511.12905v1",
      "title": "LinkedIn Profile Characteristics and Professional Success Indicators",
      "title_zh": "LinkedIn ä¸ªäººèµ„æ–™ç‰¹å¾ä¸èŒä¸šæˆåŠŸæŒ‡æ ‡",
      "authors": [
        "Tania-Amanda Fredrick Eneye",
        "Ashlesha Malla",
        "Pawan Paudel"
      ],
      "abstract": "This study explores the relationship between LinkedIn profile characteristics and professional success, focusing on the indicators of promotions, follower count, and career progression rate. By leveraging a dataset of over 62,000 anonymized LinkedIn profiles, we developed predictive models using machine learning techniques to identify the most influential factors driving professional success. Results indicate that while promotions are highly predictable, follower growth exhibits greater complexity. This research provides actionable insights for professionals seeking to optimize their LinkedIn presence and career strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† LinkedIn ä¸ªäººæ¡£æ¡ˆç‰¹å¾ä¸èŒä¸šæˆåŠŸæŒ‡æ ‡ï¼ˆå¦‚æ™‹å‡ã€å…³æ³¨è€…æ•°é‡å’ŒèŒä¸šæ™‹å‡é€Ÿåº¦ï¼‰ä¹‹é—´çš„å…³ç³»ï¼Œåˆ©ç”¨åŒ…å«è¶…è¿‡ 62,000 ä¸ªåŒ¿å LinkedIn æ¡£æ¡ˆçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œé€šè¿‡æœºå™¨å­¦ä¹ (machine learning)æŠ€æœ¯æ„å»ºäº†é¢„æµ‹æ¨¡å‹ã€‚è¯¥ç ”ç©¶æ—¨åœ¨è¯†åˆ«å½±å“èŒä¸šæˆåŠŸçš„å…³é”®å› ç´ ï¼Œå¹¶ä¸ºä¸“ä¸šäººå£«ä¼˜åŒ–å…¶ LinkedIn ä¸ªäººå½¢è±¡å’ŒèŒä¸šå‘å±•ç­–ç•¥æä¾›æ•°æ®æ”¯æŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒèŒåœºä¸­çš„æ™‹å‡(promotions)è¡¨ç°å‡ºé«˜åº¦çš„å¯é¢„æµ‹æ€§ï¼Œè€Œå…³æ³¨è€…å¢é•¿(follower growth)åˆ™ç”±äºå…¶åŠ¨æ€æ€§è¡¨ç°å‡ºæ›´é«˜çš„å¤æ‚æ€§ã€‚è¿™é¡¹å·¥ä½œæ­ç¤ºäº†ç¤¾äº¤åª’ä½“æ¡£æ¡ˆç‰¹å¾å¯¹èŒä¸šæˆåŠŸæŒ‡æ ‡çš„ä¸åŒå½±å“ç¨‹åº¦ï¼Œä¸ºç†è§£æ•°å­—åŒ–èŒä¸šè·¯å¾„æä¾›äº†é‡è¦çš„å®šé‡åˆ†æè§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12905v1",
      "published_date": "2025-11-17 02:54:12 UTC",
      "updated_date": "2025-11-17 02:54:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:11:48.331993+00:00"
    },
    {
      "arxiv_id": "2511.12903v1",
      "title": "Contrastive Entropy Bounds for Density and Conditional Density Decomposition",
      "title_zh": "å¯†åº¦ä¸æ¡ä»¶å¯†åº¦åˆ†è§£çš„å¯¹æ¯”ç†µç•Œ",
      "authors": [
        "Bo Hu",
        "Jose C. Principe"
      ],
      "abstract": "This paper studies the interpretability of neural network features from a Bayesian Gaussian view, where optimizing a cost is reaching a probabilistic bound; learning a model approximates a density that makes the bound tight and the cost optimal, often with a Gaussian mixture density. The two examples are Mixture Density Networks (MDNs) using the bound for the marginal and autoencoders using the conditional bound. It is a known result, not only for autoencoders, that minimizing the error between inputs and outputs maximizes the dependence between inputs and the middle.\n  We use Hilbert space and decomposition to address cases where a multiple-output network produces multiple centers defining a Gaussian mixture. Our first finding is that an autoencoder's objective is equivalent to maximizing the trace of a Gaussian operator, the sum of eigenvalues under bases orthonormal w.r.t. the data and model distributions. This suggests that, when a one-to-one correspondence as needed in autoencoders is unnecessary, we can instead maximize the nuclear norm of this operator, the sum of singular values, to maximize overall rank rather than trace. Thus the trace of a Gaussian operator can be used to train autoencoders, and its nuclear norm can be used as divergence to train MDNs.\n  Our second test uses inner products and norms in a Hilbert space to define bounds and costs. Such bounds often have an extra norm compared to KL-based bounds, which increases sample diversity and prevents the trivial solution where a multiple-output network produces the same constant, at the cost of requiring a sample batch to estimate and optimize. We propose an encoder-mixture-decoder architecture whose decoder is multiple-output, producing multiple centers per sample, potentially tightening the bound. Assuming the data are small-variance Gaussian mixtures, this upper bound can be tracked and analyzed quantitatively.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»Bayesian Gaussianè§†è§’æ¢è®¨äº†ç¥ç»ç½‘ç»œç‰¹å¾çš„å¯è§£é‡Šæ€§ï¼Œå°†ä¼˜åŒ–ä»£ä»·å‡½æ•°è§†ä¸ºé€¼è¿‘æ¦‚ç‡è¾¹ç•Œçš„è¿‡ç¨‹ã€‚ä½œè€…åˆ©ç”¨Hilbert spaceå’Œåˆ†è§£æŠ€æœ¯è¯æ˜äº†autoencoderçš„ç›®æ ‡å‡½æ•°ç­‰åŒäºæœ€å¤§åŒ–é«˜æ–¯ç®—å­çš„traceï¼Œå¹¶æå‡ºåœ¨éä¸€ä¸€å¯¹åº”åœºæ™¯ä¸‹å¯é€šè¿‡æœ€å¤§åŒ–nuclear normæ¥æå‡æ¨¡å‹çš„æ•´ä½“ç§©ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†åŸºäºHilbert spaceå†…ç§¯å’ŒèŒƒæ•°çš„Contrastive Entropy Boundsï¼Œè¿™ç§è¾¹ç•Œç›¸æ¯”ä¼ ç»ŸåŸºäºKLçš„è¾¹ç•Œèƒ½æ˜¾è‘—å¢åŠ æ ·æœ¬å¤šæ ·æ€§ï¼Œå¹¶æœ‰æ•ˆé˜²æ­¢å¤šè¾“å‡ºç½‘ç»œæ”¶æ•›è‡³å¹³å‡¡è§£ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§encoder-mixture-decoderæ¶æ„ï¼Œåˆ©ç”¨å¤šè¾“å‡ºè§£ç å™¨ä¸ºæ¯ä¸ªæ ·æœ¬ç”Ÿæˆå¤šä¸ªä¸­å¿ƒä»¥æ”¶ç´§è¾¹ç•Œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å‡è®¾æ•°æ®ä¸ºå°æ–¹å·®é«˜æ–¯æ··åˆæ¨¡å‹çš„å‰æä¸‹ï¼Œè¯¥ä¸Šç•Œå¯ä»¥è¢«æœ‰æ•ˆåœ°å®šé‡åˆ†æä¸è¿½è¸ªã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12903v1",
      "published_date": "2025-11-17 02:49:08 UTC",
      "updated_date": "2025-11-17 02:49:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:12:08.033853+00:00"
    },
    {
      "arxiv_id": "2511.21706v1",
      "title": "A General Highly Accurate Online Planning Method Integrating Large Language Models into Nested Rollout Policy Adaptation for Dialogue Tasks",
      "title_zh": "ä¸€ç§é¢å‘å¯¹è¯ä»»åŠ¡çš„ã€å°†å¤§è¯­è¨€æ¨¡å‹é›†æˆè‡³åµŒå¥—å±•å¼€ç­–ç•¥è‡ªé€‚åº”çš„é€šç”¨é«˜ç²¾åº¦åœ¨çº¿è§„åˆ’æ–¹æ³•",
      "authors": [
        "Hui Wang",
        "Fafa Zhang",
        "Xiaoyu Zhang",
        "Chaoxu Mu"
      ],
      "abstract": "In goal-oriented dialogue tasks, the main challenge is to steer the interaction towards a given goal within a limited number of turns. Existing approaches either rely on elaborate prompt engineering, whose effectiveness is heavily dependent on human experience, or integrate policy networks and pre-trained policy models, which are usually difficult to adapt to new dialogue scenarios and costly to train. Therefore, in this paper, we present Nested Rollout Policy Adaptation for Goal-oriented Dialogue (NRPA-GD), a novel dialogue policy planning method that completely avoids specific model training by utilizing a Large Language Model (LLM) to simulate behaviors of user and system at the same time. Specifically, NRPA-GD constructs a complete evaluation mechanism for dialogue trajectories and employs an optimization framework of nested Monte Carlo simulation and policy self-adaptation to dynamically adjust policies during the dialogue process. The experimental results on four typical goal-oriented dialogue datasets show that NRPA-GD outperforms both existing prompt engineering and specifically pre-trained model-based methods. Impressively, NRPA-GD surpasses ChatGPT and pre-trained policy models with only a 0.6-billion-parameter LLM. The proposed approach further demonstrates the advantages and novelty of employing planning methods on LLMs to solve practical planning tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é¢å‘ç›®æ ‡çš„å¯¹è¯ä»»åŠ¡ä¸­å­˜åœ¨çš„æç¤ºå·¥ç¨‹(Prompt Engineering)ä¾èµ–æ€§å¼ºä»¥åŠç­–ç•¥æ¨¡å‹è®­ç»ƒæˆæœ¬é«˜ä¸”éš¾ä»¥è¿ç§»çš„é—®é¢˜ï¼Œæå‡ºäº†NRPA-GDï¼ˆé¢å‘ç›®æ ‡å¯¹è¯çš„åµŒå¥—å±•å¼€ç­–ç•¥é€‚é…ï¼‰è¿™ä¸€åœ¨çº¿è§„åˆ’æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å¤§è¯­è¨€æ¨¡å‹(LLM)åŒæ—¶æ¨¡æ‹Ÿç”¨æˆ·å’Œç³»ç»Ÿçš„è¡Œä¸ºï¼Œå½»åº•é¿å…äº†é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒè¿‡ç¨‹ã€‚NRPA-GDæ„å»ºäº†å®Œå–„çš„å¯¹è¯è½¨è¿¹è¯„ä¼°æœºåˆ¶ï¼Œåˆ©ç”¨åµŒå¥—è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ(Nested Monte Carlo simulation)ä¸ç­–ç•¥è‡ªé€‚é…(Policy self-adaptation)æ¡†æ¶åœ¨å¯¹è¯è¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´ç­–ç•¥ã€‚åœ¨å››ä¸ªå…¸å‹æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒNRPA-GDçš„æ€§èƒ½ä¼˜äºç°æœ‰çš„æç¤ºå·¥ç¨‹å’Œé¢„è®­ç»ƒæ¨¡å‹æ–¹æ³•ã€‚ä»¤äººç©ç›®çš„æ˜¯ï¼Œè¯¥æ–¹æ³•ä»…éœ€0.6Bå‚æ•°çš„LLMå³å¯åœ¨è¡¨ç°ä¸Šè¶…è¶ŠChatGPTå’Œä¸“é—¨çš„é¢„è®­ç»ƒç­–ç•¥æ¨¡å‹ï¼Œå……åˆ†éªŒè¯äº†å°†è§„åˆ’æ–¹æ³•ä¸å¤§è¯­è¨€æ¨¡å‹ç»“åˆè§£å†³å®é™…è§„åˆ’ä»»åŠ¡çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21706v1",
      "published_date": "2025-11-17 02:48:37 UTC",
      "updated_date": "2025-11-17 02:48:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:11:55.031197+00:00"
    },
    {
      "arxiv_id": "2511.12901v1",
      "title": "Online Learning of HTN Methods for integrated LLM-HTN Planning",
      "title_zh": "é¢å‘ LLM-HTN é›†æˆè§„åˆ’çš„ HTN æ–¹æ³•åœ¨çº¿å­¦ä¹ ",
      "authors": [
        "Yuesheng Xu",
        "Hector Munoz-Avila"
      ],
      "abstract": "We present online learning of Hierarchical Task Network (HTN) methods in the context of integrated HTN planning and LLM-based chatbots. Methods indicate when and how to decompose tasks into subtasks. Our method learner is built on top of the ChatHTN planner. ChatHTN queries ChatGPT to generate a decomposition of a task into primitive tasks when no applicable method for the task is available. In this work, we extend ChatHTN. Namely, when ChatGPT generates a task decomposition, ChatHTN learns from it, akin to memoization. However, unlike memoization, it learns a generalized method that applies not only to the specific instance encountered, but to other instances of the same task. We conduct experiments on two domains and demonstrate that our online learning procedure reduces the number of calls to ChatGPT while solving at least as many problems, and in some cases, even more.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é›†æˆ Hierarchical Task Network (HTN) è§„åˆ’ä¸åŸºäº LLM çš„èŠå¤©æœºå™¨äººèƒŒæ™¯ï¼Œæå‡ºäº† HTN æ–¹æ³•çš„åœ¨çº¿å­¦ä¹ æœºåˆ¶ã€‚è¯¥å­¦ä¹ å™¨å»ºç«‹åœ¨ ChatHTN è§„åˆ’å™¨ä¹‹ä¸Šï¼Œæ—¨åœ¨è§£å†³å½“æ— å¯ç”¨æ–¹æ³•æ—¶å¦‚ä½•å°†ä»»åŠ¡æœ‰æ•ˆåˆ†è§£ä¸ºåŸå§‹ä»»åŠ¡çš„é—®é¢˜ã€‚å½“ ChatHTN è°ƒç”¨ ChatGPT ç”Ÿæˆä»»åŠ¡åˆ†è§£æ–¹æ¡ˆæ—¶ï¼Œç³»ç»Ÿä¼šä»ä¸­å­¦ä¹ å¹¶å°†å…¶æ³›åŒ–ä¸ºé€‚ç”¨äºåŒç±»ä»»åŠ¡ä¸åŒå®ä¾‹çš„é€šç”¨æ–¹æ³•ï¼Œè€Œä¸ä»…ä»…æ˜¯ç®€å•çš„ memoization è®°å½•ã€‚è¿™ç§åœ¨çº¿å­¦ä¹ ç¨‹åºä½¿å¾—ç³»ç»Ÿèƒ½å¤Ÿä» LLM çš„è¾“å‡ºä¸­æå–ç»“æ„åŒ–çŸ¥è¯†ï¼Œä»è€Œå¢å¼ºåç»­è§„åˆ’çš„è‡ªä¸»æ€§ã€‚åœ¨ä¸¤ä¸ªé¢†åŸŸè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å‡å°‘äº†å¯¹ ChatGPT çš„è°ƒç”¨æ¬¡æ•°ï¼Œå¹¶åœ¨ç»´æŒåŸæœ‰é—®é¢˜è§£å†³ç‡çš„åŸºç¡€ä¸Šï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³æå‡äº†æˆåŠŸç‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "The Twelfth Annual Conference on Advances in Cognitive Systems (ACS-2025)",
      "pdf_url": "https://arxiv.org/pdf/2511.12901v1",
      "published_date": "2025-11-17 02:46:04 UTC",
      "updated_date": "2025-11-17 02:46:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:12:09.932103+00:00"
    },
    {
      "arxiv_id": "2511.12882v2",
      "title": "Towards High-Consistency Embodied World Model with Multi-View Trajectory Videos",
      "title_zh": "è¿ˆå‘åŸºäºå¤šè§†è§’è½¨è¿¹è§†é¢‘çš„é«˜ä¸€è‡´æ€§å…·èº«ä¸–ç•Œæ¨¡å‹",
      "authors": [
        "Taiyi Su",
        "Jian Zhu",
        "Yaxuan Li",
        "Chong Ma",
        "Zitai Huang",
        "Hanli Wang",
        "Yi Xu"
      ],
      "abstract": "Embodied world models aim to predict and interact with the physical world through visual observations and actions. However, existing models struggle to accurately translate low-level actions (e.g., joint positions) into precise robotic movements in predicted frames, leading to inconsistencies with real-world physical interactions. To address these limitations, we propose MTV-World, an embodied world model that introduces Multi-view Trajectory-Video control for precise visuomotor prediction. Specifically, instead of directly using low-level actions for control, we employ trajectory videos obtained through camera intrinsic and extrinsic parameters and Cartesian-space transformation as control signals. However, projecting 3D raw actions onto 2D images inevitably causes a loss of spatial information, making a single view insufficient for accurate interaction modeling. To overcome this, we introduce a multi-view framework that compensates for spatial information loss and ensures high-consistency with physical world. MTV-World forecasts future frames based on multi-view trajectory videos as input and conditioning on an initial frame per view. Furthermore, to systematically evaluate both robotic motion precision and object interaction accuracy, we develop an auto-evaluation pipeline leveraging multimodal large models and referring video object segmentation models. To measure spatial consistency, we formulate it as an object location matching problem and adopt the Jaccard Index as the evaluation metric. Extensive experiments demonstrate that MTV-World achieves precise control execution and accurate physical interaction modeling in complex dual-arm scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MTV-Worldï¼Œä¸€ç§æ—¨åœ¨è§£å†³ç°æœ‰å…·èº«ä¸–ç•Œæ¨¡å‹(Embodied world models)åœ¨å°†åº•å±‚åŠ¨ä½œ(low-level actions)è½¬åŒ–ä¸ºç²¾ç¡®æœºå™¨äººè¿åŠ¨æ—¶ç‰©ç†äº¤äº’ä¸ä¸€è‡´é—®é¢˜çš„æ¨¡å‹ã€‚MTV-Worldåˆ›æ–°æ€§åœ°å¼•å…¥äº†å¤šè§†å›¾è½¨è¿¹è§†é¢‘(Multi-view Trajectory-Video)æ§åˆ¶æœºåˆ¶ï¼Œåˆ©ç”¨ç›¸æœºå†…å¤–å‚å’Œç¬›å¡å°”ç©ºé—´è½¬æ¢ç”Ÿæˆçš„è½¨è¿¹è§†é¢‘ä½œä¸ºæ§åˆ¶ä¿¡å·ï¼Œä»¥æ›¿ä»£ç›´æ¥ä½¿ç”¨åº•å±‚åŠ¨ä½œçš„ä¼ ç»Ÿæ–¹æ³•ã€‚é’ˆå¯¹3DåŠ¨ä½œæŠ•å½±è‡³2Då›¾åƒå¯¼è‡´çš„ç©ºé—´ä¿¡æ¯ç¼ºå¤±ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¤šè§†å›¾åä½œè¿›è¡Œè¡¥å¿ï¼Œå¹¶åŸºäºå„è§†å›¾åˆå§‹å¸§é¢„æµ‹åç»­è½¨è¿¹ï¼Œç¡®ä¿äº†é¢„æµ‹ç»“æœä¸ç‰©ç†ä¸–ç•Œçš„é«˜åº¦ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜å¼€å‘äº†ä¸€å¥—ç»“åˆå¤šæ¨¡æ€å¤§æ¨¡å‹(Multimodal large models)å’ŒæŒ‡ä»£è§†é¢‘å¯¹è±¡åˆ†å‰²(Referring video object segmentation)æ¨¡å‹çš„è‡ªåŠ¨è¯„ä¼°ç®¡çº¿ï¼Œå¹¶é‡‡ç”¨Jaccard Indexè¡¡é‡ç©ºé—´ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMTV-Worldåœ¨å¤æ‚çš„åŒè‡‚æ“ä½œåœºæ™¯ä¸­èƒ½å¤Ÿå®ç°ç²¾å‡†çš„åŠ¨ä½œæ§åˆ¶æ‰§è¡Œå’Œé«˜åº¦çœŸå®çš„ç‰©ç†äº¤äº’å»ºæ¨¡ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "15 pages, 23 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.12882v2",
      "published_date": "2025-11-17 02:17:04 UTC",
      "updated_date": "2025-11-19 09:11:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:13:05.131437+00:00"
    },
    {
      "arxiv_id": "2511.12876v2",
      "title": "Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making",
      "title_zh": "æ€ã€è¨€ã€å†³ï¼šé¢å‘ç»æµå†³ç­–çš„è¯­è¨€å¢å¼ºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Heyang Ma",
        "Qirui Mi",
        "Qipeng Yang",
        "Zijun Fan",
        "Bo Li",
        "Haifeng Zhang"
      ],
      "abstract": "Economic decision-making depends not only on structured signals such as prices and taxes, but also on unstructured language, including peer dialogue and media narratives. While multi-agent reinforcement learning (MARL) has shown promise in optimizing economic decisions, it struggles with the semantic ambiguity and contextual richness of language. We propose LAMP (Language-Augmented Multi-Agent Policy), a framework that integrates language into economic decision-making and narrows the gap to real-world settings. LAMP follows a Think-Speak-Decide pipeline: (1) Think interprets numerical observations to extract short-term shocks and long-term trends, caching high-value reasoning trajectories; (2) Speak crafts and exchanges strategic messages based on reasoning, updating beliefs by parsing peer communications; and (3) Decide fuses numerical data, reasoning, and reflections into a MARL policy to optimize language-augmented decision-making. Experiments in economic simulation show that LAMP outperforms both MARL and LLM-only baselines in cumulative return (+63.5%, +34.0%), robustness (+18.8%, +59.4%), and interpretability. These results demonstrate the potential of language-augmented policies to deliver more effective and robust economic strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LAMP (Language-Augmented Multi-Agent Policy) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (MARL) åœ¨å¤„ç†ç»æµå†³ç­–ä¸­è¯­è¨€è¯­ä¹‰æ¨¡ç³Šå’Œä¸Šä¸‹æ–‡ä¸°å¯Œæ€§æ–¹é¢çš„å±€é™ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº† \"Think-Speak-Decide\" æµæ°´çº¿ï¼Œå…¶ä¸­ Think é˜¶æ®µé€šè¿‡è§£é‡Šæ•°å€¼è§‚æµ‹æ¥æå–çŸ­æœŸå†²å‡»å’Œé•¿æœŸè¶‹åŠ¿ï¼Œå¹¶ç¼“å­˜é«˜ä»·å€¼çš„æ¨ç†è½¨è¿¹ã€‚Speak é˜¶æ®µåŸºäºæ¨ç†ç»“æœæ„å»ºå¹¶äº¤æ¢æˆ˜ç•¥ä¿¡æ¯ï¼Œé€šè¿‡è§£æåŒè¡Œäº¤æµæ¥æ›´æ–°ä¿¡å¿µã€‚Decide é˜¶æ®µå°†æ•°å€¼æ•°æ®ã€æ¨ç†å’Œåæ€èåˆè¿› MARL ç­–ç•¥ä¸­ï¼Œä»¥ä¼˜åŒ–å¢å¼ºè¯­è¨€åçš„å†³ç­–ã€‚åœ¨ç»æµæ¨¡æ‹Ÿå®éªŒä¸­ï¼ŒLAMP åœ¨ç´¯è®¡æ”¶ç›Šæ–¹é¢åˆ†åˆ«æ¯”çº¯ MARL å’Œçº¯ LLM åŸºçº¿æé«˜äº† 63.5% å’Œ 34.0%ï¼Œä¸”åœ¨é²æ£’æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢ä¹Ÿè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚å®éªŒç»“æœè¯æ˜äº†è¯­è¨€å¢å¼ºç­–ç•¥åœ¨æä¾›æ›´æœ‰æ•ˆã€æ›´ç¨³å¥çš„ç»æµç­–ç•¥æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "econ.GN"
      ],
      "primary_category": "cs.AI",
      "comment": "Extended version of a submission to AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.12876v2",
      "published_date": "2025-11-17 02:09:18 UTC",
      "updated_date": "2025-12-14 15:38:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:12:29.736666+00:00"
    },
    {
      "arxiv_id": "2511.12874v1",
      "title": "Classification of Hope in Textual Data using Transformer-Based Models",
      "title_zh": "åŸºäº Transformer æ¨¡å‹çš„æ–‡æœ¬æ•°æ®å¸Œæœ›åˆ†ç±»",
      "authors": [
        "Chukwuebuka Fortunate Ijezue",
        "Tania-Amanda Fredrick Eneye",
        "Maaz Amjad"
      ],
      "abstract": "This paper presents a transformer-based approach for classifying hope expressions in text. We developed and compared three architectures (BERT, GPT-2, and DeBERTa) for both binary classification (Hope vs. Not Hope) and multiclass categorization (five hope-related categories). Our initial BERT implementation achieved 83.65% binary and 74.87% multiclass accuracy. In the extended comparison, BERT demonstrated superior performance (84.49% binary, 72.03% multiclass accuracy) while requiring significantly fewer computational resources (443s vs. 704s training time) than newer architectures. GPT-2 showed lowest overall accuracy (79.34% binary, 71.29% multiclass), while DeBERTa achieved moderate results (80.70% binary, 71.56% multiclass) but at substantially higher computational cost (947s for multiclass training). Error analysis revealed architecture-specific strengths in detecting nuanced hope expressions, with GPT-2 excelling at sarcasm detection (92.46% recall). This study provides a framework for computational analysis of hope, with applications in mental health and social media analysis, while demonstrating that architectural suitability may outweigh model size for specialized emotion detection tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäº Transformer çš„æ–¹æ³•ï¼Œç”¨äºå¯¹æ–‡æœ¬ä¸­çš„å¸Œæœ›è¡¨è¾¾(hope expressions)è¿›è¡Œåˆ†ç±»ï¼Œå¹¶å¼€å‘ä¸”å¯¹æ¯”äº† BERTã€GPT-2 å’Œ DeBERTa ä¸‰ç§æ¨¡å‹æ¶æ„ã€‚ç ”ç©¶åˆ†åˆ«åœ¨äºŒåˆ†ç±»(binary classification)å’Œäº”ç±»å¤šåˆ†ç±»(multiclass categorization)ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤º BERT åœ¨æ€§èƒ½ä¸Šè¡¨ç°æœ€ä¼˜ï¼Œå–å¾—äº† 84.49% çš„äºŒåˆ†ç±»å‡†ç¡®ç‡ï¼Œä¸”æ‰€éœ€çš„è®¡ç®—èµ„æºæ˜¾è‘—å°‘äºå…¶ä»–æ›´æ–°çš„æ¶æ„ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒGPT-2 çš„æ•´ä½“å‡†ç¡®ç‡æœ€ä½ï¼Œè€Œ DeBERTa è™½ç„¶ç»“æœä¸­ç­‰ï¼Œä½†è®¡ç®—æˆæœ¬æœ€é«˜ã€‚é€šè¿‡é”™è¯¯åˆ†æå‘ç°ï¼Œä¸åŒæ¨¡å‹åœ¨æ£€æµ‹å¾®å¦™è¡¨è¾¾ä¸Šå„æœ‰åƒç§‹ï¼Œå¦‚ GPT-2 åœ¨è®½åˆºæ£€æµ‹(sarcasm detection)æ–¹é¢çš„å¬å›ç‡é«˜è¾¾ 92.46%ã€‚è¯¥ç ”ç©¶ä¸ºå¸Œæœ›æƒ…ç»ªçš„è®¡ç®—åˆ†ææä¾›äº†ä¸€ä¸ªæœ‰æ•ˆæ¡†æ¶ï¼Œé€‚ç”¨äºå¿ƒç†å¥åº·å’Œç¤¾ä¼šåª’ä½“åˆ†æï¼Œå¹¶è¯æ˜äº†åœ¨ç‰¹å®šçš„æƒ…æ„Ÿæ£€æµ‹ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹æ¶æ„çš„é€‚ç”¨æ€§(architectural suitability)å¾€å¾€æ¯”æ¨¡å‹è§„æ¨¡æ›´ä¸ºå…³é”®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12874v1",
      "published_date": "2025-11-17 02:07:24 UTC",
      "updated_date": "2025-11-17 02:07:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:12:30.638081+00:00"
    },
    {
      "arxiv_id": "2511.12869v1",
      "title": "On the Fundamental Limits of LLMs at Scale",
      "title_zh": "è®ºå¤§è¯­è¨€æ¨¡å‹è§„æ¨¡åŒ–å‘å±•çš„æ ¹æœ¬å±€é™",
      "authors": [
        "Muhammad Ahmed Mohsin",
        "Muhammad Umer",
        "Ahsan Bilal",
        "Zeeshan Memon",
        "Muhammad Ibtsaam Qadir",
        "Sagnik Bhattacharya",
        "Hassan Rizwan",
        "Abhiram R. Gorle",
        "Maahe Zehra Kazmi",
        "Ayesha Mohsin",
        "Muhammad Usman Rafique",
        "Zihao He",
        "Pulkit Mehta",
        "Muhammad Ali Jamshed",
        "John M. Cioffi"
      ],
      "abstract": "Large Language Models (LLMs) have benefited enormously from scaling, yet these gains are bounded by five fundamental limitations: (1) hallucination, (2) context compression, (3) reasoning degradation, (4) retrieval fragility, and (5) multimodal misalignment. While existing surveys describe these phenomena empirically, they lack a rigorous theoretical synthesis connecting them to the foundational limits of computation, information, and learning. This work closes that gap by presenting a unified, proof-informed framework that formalizes the innate theoretical ceilings of LLM scaling. First, computability and uncomputability imply an irreducible residue of error: for any computably enumerable model family, diagonalization guarantees inputs on which some model must fail, and undecidable queries (e.g., halting-style tasks) induce infinite failure sets for all computable predictors. Second, information-theoretic and statistical constraints bound attainable accuracy even on decidable tasks, finite description length enforces compression error, and long-tail factual knowledge requires prohibitive sample complexity. Third, geometric and computational effects compress long contexts far below their nominal size due to positional under-training, encoding attenuation, and softmax crowding. We further show how likelihood-based training favors pattern completion over inference, how retrieval under token limits suffers from semantic drift and coupling noise, and how multimodal scaling inherits shallow cross-modal alignment. Across sections, we pair theorems and empirical evidence to outline where scaling helps, where it saturates, and where it cannot progress, providing both theoretical foundations and practical mitigation paths like bounded-oracle retrieval, positional curricula, and sparse or hierarchical attention.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è§„æ¨¡åŒ–(scaling)è¿‡ç¨‹ä¸­çš„äº”å¤§æ ¹æœ¬å±€é™æ€§ï¼šå¹»è§‰(hallucination)ã€ä¸Šä¸‹æ–‡å‹ç¼©(context compression)ã€æ¨ç†é€€åŒ–(reasoning degradation)ã€æ£€ç´¢è„†å¼±æ€§(retrieval fragility)å’Œå¤šæ¨¡æ€é”™ä½(multimodal misalignment)ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ã€ç»è¿‡è¯æ˜çš„å½¢å¼åŒ–æ¡†æ¶ï¼Œå°†è¿™äº›ç°è±¡ä¸è®¡ç®—ã€ä¿¡æ¯å’Œå­¦ä¹ çš„ç†è®ºå¤©èŠ±æ¿è”ç³»èµ·æ¥ï¼Œå¡«è¡¥äº†ç°æœ‰å®è¯ç ”ç©¶çš„ç©ºç™½ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œä¸å¯è®¡ç®—æ€§(uncomputability)å¯¼è‡´æ¨¡å‹å­˜åœ¨ä¸å¯çº¦çš„é”™è¯¯æ®‹ä½™ï¼Œè€Œä¿¡æ¯è®ºä¸ç»Ÿè®¡çº¦æŸåˆ™é™åˆ¶äº†æ¨¡å‹åœ¨å¤„ç†é•¿å°¾çŸ¥è¯†æ—¶çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œå—é™äºä½ç½®è®­ç»ƒä¸è¶³å’ŒSoftmaxæ‹¥æŒ¤ç­‰æ•ˆåº”ï¼Œé•¿ä¸Šä¸‹æ–‡åœ¨å®é™…æ“ä½œä¸­ä¼šè¢«æ˜¾è‘—å‹ç¼©ã€‚è®ºæ–‡è¿˜è®ºè¯äº†åŸºäºä¼¼ç„¶çš„è®­ç»ƒå€¾å‘äºæ¨¡å¼å®Œæˆ(pattern completion)è€Œéé€»è¾‘æ¨ç†(inference)ï¼Œå¹¶æ­ç¤ºäº†æ£€ç´¢ä¸­çš„è¯­ä¹‰æ¼‚ç§»åŠå¤šæ¨¡æ€ä¸­çš„æµ…å±‚å¯¹é½é—®é¢˜ã€‚é€šè¿‡å®šç†ä¸å®è¯çš„ç»“åˆï¼Œè¯¥å·¥ä½œä¸ä»…æ˜ç¡®äº†è§„æ¨¡åŒ–æ•ˆç›Šé¥±å’Œçš„è¾¹ç•Œï¼Œè¿˜æå‡ºäº†æœ‰ç•Œé¢„è¨€æœºæ£€ç´¢(bounded-oracle retrieval)å’Œä½ç½®è¯¾ç¨‹(positional curricula)ç­‰å®é™…çš„ç¼“è§£è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "cs.IT",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to TMLR 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.12869v1",
      "published_date": "2025-11-17 01:55:33 UTC",
      "updated_date": "2025-11-17 01:55:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:12:45.233026+00:00"
    },
    {
      "arxiv_id": "2511.12868v1",
      "title": "Video Finetuning Improves Reasoning Between Frames",
      "title_zh": "è§†é¢‘å¾®è°ƒæå‡å¸§é—´æ¨ç†èƒ½åŠ›",
      "authors": [
        "Ruiqi Yang",
        "Tian Yun",
        "Zihan Wang",
        "Ellie Pavlick"
      ],
      "abstract": "Multimodal large language models (LLMs) have made rapid progress in visual understanding, yet their extension from images to videos often reduces to a naive concatenation of frame tokens. In this work, we investigate what video finetuning brings to multimodal LLMs. We propose Visual Chain-of-Thought (vCoT), an explicit reasoning process that generates transitional event descriptions between consecutive frames. Using vCoT, we systematically compare image-only LVLMs with their video-finetuned counterparts, both with and without access to these transitional cues. Our experiments show that vCoT significantly improves the performance of image-only models on long-form video question answering, while yielding only marginal gains for video-finetuned models. This suggests that the latter already capture frame-to-frame transitions implicitly. Moreover, we find that video models transfer this temporal reasoning ability to purely static settings, outperforming image models' baselines on relational visual reasoning tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†é¢‘å¾®è°ƒå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å½±å“ï¼Œå¹¶æå‡ºäº†è§†è§‰é“¾å¼æ€ç»´ï¼ˆvCoTï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ç”Ÿæˆè¿ç»­è§†é¢‘å¸§ä¹‹é—´è¿‡æ¸¡æ€§äº‹ä»¶æè¿°çš„æ˜¾å¼æ¨ç†è¿‡ç¨‹ã€‚ç ”ç©¶äººå‘˜é€šè¿‡ç³»ç»Ÿå¯¹æ¯”å‘ç°ï¼ŒvCoT èƒ½å¤Ÿæ˜¾è‘—æå‡ä»…å›¾åƒæ¨¡å‹åœ¨é•¿è§†é¢‘é—®ç­”ï¼ˆlong-form video QAï¼‰ä¸­çš„è¡¨ç°ï¼Œè€Œå¯¹è§†é¢‘å¾®è°ƒæ¨¡å‹çš„æå‡è¾ƒå°ï¼Œè¿™è¡¨æ˜åè€…å·²å…·å¤‡éšå¼æ•æ‰å¸§é—´è½¬æ¢ï¼ˆframe-to-frame transitionsï¼‰çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå®éªŒè¿›ä¸€æ­¥è¯æ˜è§†é¢‘æ¨¡å‹èƒ½å°†è¿™ç§æ—¶é—´æ¨ç†èƒ½åŠ›è¿ç§»åˆ°çº¯é™æ€åœºæ™¯ï¼Œåœ¨å…³ç³»è§†è§‰æ¨ç†ï¼ˆrelational visual reasoningï¼‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„ä»…å›¾åƒæ¨¡å‹åŸºå‡†ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†è§†é¢‘å¾®è°ƒåœ¨å¢å¼ºè·¨å¸§æ¨ç†å’Œå…³ç³»ç†è§£æ–¹é¢çš„å…³é”®ä½œç”¨ï¼Œä¸ºæå‡å¤šæ¨¡æ€æ¨¡å‹çš„æ—¶ç©ºè®¤çŸ¥èƒ½åŠ›æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at CogInterp @ NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.12868v1",
      "published_date": "2025-11-17 01:51:57 UTC",
      "updated_date": "2025-11-17 01:51:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:12:36.036065+00:00"
    },
    {
      "arxiv_id": "2511.12867v2",
      "title": "Bootstrapping LLMs via Preference-Based Policy Optimization",
      "title_zh": "åŸºäºåå¥½ç­–ç•¥ä¼˜åŒ–çš„å¤§è¯­è¨€æ¨¡å‹è‡ªä¸¾",
      "authors": [
        "Chen Jia"
      ],
      "abstract": "Bootstrapping large language models (LLMs) through preference-based policy optimization offers a promising direction for aligning model behavior with human preferences without relying on extensive manual annotations. In this work, we propose a novel preference-based policy optimization (PbPO) framework that formulates the learning process as a min-max game between the main policy and a reward model (RM). The RM is constrained within a confidence set derived from preference data to ensure reliable exploitation. Our iterative online algorithm actively collects preference data through guided exploration of the evolving policy, enabling continual self-improvement of both the policy and the RM. We provide theoretical guarantees for our method, establishing high-probability regret bounds for both settings with sequence-level RM and token-level RM, demonstrating its effectiveness in bootstrapping LLMs. Extensive experiments on five benchmarks show that our approach consistently outperforms existing state-of-the-art preference optimization techniques.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºåŸºäºåå¥½çš„ç­–ç•¥ä¼˜åŒ–(PbPO)çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨æ— éœ€å¤§è§„æ¨¡äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹å®ç°å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸äººç±»åå¥½çš„å¯¹é½ã€‚è¯¥æ¡†æ¶å°†å­¦ä¹ è¿‡ç¨‹å»ºæ¨¡ä¸ºä¸»ç­–ç•¥ä¸å¥–åŠ±æ¨¡å‹(RM)ä¹‹é—´çš„åšå¼ˆ(min-max game)ï¼Œå¹¶åˆ©ç”¨åå¥½æ•°æ®è¡ç”Ÿçš„ç½®ä¿¡é›†(confidence set)çº¦æŸå¥–åŠ±æ¨¡å‹ä»¥ç¡®ä¿å…¶å¯é æ€§ã€‚é€šè¿‡é‡‡ç”¨è¿­ä»£å¼åœ¨çº¿ç®—æ³•ï¼ŒPbPOèƒ½å¤Ÿåœ¨æ¼”åŒ–ç­–ç•¥çš„å¼•å¯¼ä¸‹ä¸»åŠ¨æ”¶é›†åå¥½æ•°æ®ï¼Œä»è€Œå®ç°ç­–ç•¥å’Œå¥–åŠ±æ¨¡å‹çš„æŒç»­è‡ªæˆ‘æ”¹è¿›ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜ä¸ºè¯¥æ–¹æ³•æä¾›äº†ç†è®ºä¿éšœï¼Œåœ¨åºåˆ—çº§å’Œæ ‡è®°çº§å¥–åŠ±æ¨¡å‹è®¾ç½®ä¸‹åˆ†åˆ«ç¡®ç«‹äº†é«˜æ¦‚ç‡é—æ†¾è¾¹ç•Œ(regret bounds)ã€‚åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‡ªå¼•å¯¼LLMsæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå…¶æ€§èƒ½ä¸€è‡´ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›åå¥½ä¼˜åŒ–æŠ€æœ¯ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12867v2",
      "published_date": "2025-11-17 01:41:14 UTC",
      "updated_date": "2025-12-24 13:31:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:12:41.836651+00:00"
    },
    {
      "arxiv_id": "2511.14798v1",
      "title": "Evaluating Generative AI for CS1 Code Grading: Direct vs Reverse Methods",
      "title_zh": "è¯„ä¼°ç”¨äº CS1 ä»£ç è¯„åˆ†çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼šç›´æ¥æ³•ä¸é€†å‘æ³•",
      "authors": [
        "Ahmad Memon",
        "Abdallah Mohamed"
      ],
      "abstract": "Manual grading of programming assignments in introductory computer science courses can be time-consuming and prone to inconsistencies. While unit testing is commonly used for automatic evaluation, it typically follows a binary pass/fail model and does not give partial marks. Recent advances in large language models (LLMs) offer the potential for automated, scalable, and more objective grading.\n  This paper compares two AI-based grading techniques: \\textit{Direct}, where the AI model applies a rubric directly to student code, and \\textit{Reverse} (a newly proposed approach), where the AI first fixes errors, then deduces a grade based on the nature and number of fixes. Each method was evaluated on both the instructor's original grading scale and a tenfold expanded scale to assess the impact of range on AI grading accuracy. To assess their effectiveness, AI-assigned scores were evaluated against human tutor evaluations on a range of coding problems and error types.\n  Initial findings suggest that while the Direct approach is faster and straightforward, the Reverse technique often provides a more fine-grained assessment by focusing on correction effort. Both methods require careful prompt engineering, particularly for allocating partial credit and handling logic errors. To further test consistency, we also used synthetic student code generated using Gemini Flash 2.0, which allowed us to evaluate AI graders on a wider range of controlled error types and difficulty levels. We discuss the strengths and limitations of each approach, practical considerations for prompt design, and future directions for hybrid human-AI grading systems that aim to improve consistency, efficiency, and fairness in CS courses.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨è®¡ç®—æœºç§‘å­¦å…¥é—¨è¯¾ç¨‹ï¼ˆCS1ï¼‰ç¼–ç¨‹ä½œä¸šè¯„åˆ†ä¸­çš„åº”ç”¨ï¼Œå¹¶å¯¹æ¯”äº†Directä¸Reverseä¸¤ç§è¯„ä¼°æŠ€æœ¯ã€‚Directæ–¹æ³•ç”±AIæ¨¡å‹ç›´æ¥å°†rubricåº”ç”¨äºå­¦ç”Ÿä»£ç ï¼Œè€Œæ–°æå‡ºçš„Reverseæ–¹æ³•åˆ™æ˜¯è®©AIå…ˆä¿®å¤ä»£ç é”™è¯¯ï¼Œå†æ ¹æ®ä¿®å¤çš„æ€§è´¨å’Œæ•°é‡æ¨å¯¼è¯„åˆ†ã€‚ç ”ç©¶é€šè¿‡å¯¹æ¯”äººç±»å¯¼å¸ˆçš„è¯„ä¼°ç»“æœï¼Œåˆ†æäº†ä¸åŒè¯„åˆ†é‡è¡¨ä¸‹AIè¯„åˆ†çš„å‡†ç¡®æ€§ã€‚åˆæ­¥å‘ç°æ˜¾ç¤ºï¼Œè™½ç„¶Directæ–¹æ³•æ“ä½œæ›´ç›´æ¥ä¸”é€Ÿåº¦æ›´å¿«ï¼Œä½†Reverseæ–¹æ³•é€šè¿‡èšç„¦äºä¿®æ­£å·¥ä½œé‡ï¼ˆcorrection effortï¼‰ï¼Œèƒ½å¤Ÿæä¾›æ›´ç»†ç²’åº¦çš„è¯„ä¼°ã€‚ä¸ºäº†è¿›ä¸€æ­¥æµ‹è¯•ä¸€è‡´æ€§ï¼Œç ”ç©¶è¿˜åˆ©ç”¨Gemini Flash 2.0ç”Ÿæˆçš„åˆæˆä»£ç ï¼Œåœ¨å¤šç§å—æ§é”™è¯¯ç±»å‹å’Œéš¾åº¦çº§åˆ«ä¸‹å¯¹AIè¯„åˆ†å™¨è¿›è¡Œäº†æµ‹è¯•ã€‚è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†ä¸¤ç§æ–¹æ³•çš„ä¼˜ç¼ºç‚¹åŠprompt engineeringçš„å®è·µè€ƒé‡ï¼Œä¸ºæœªæ¥æ„å»ºæé«˜CSè¯¾ç¨‹è¯„åˆ†ä¸€è‡´æ€§ä¸å…¬å¹³æ€§çš„äººæœºåä½œç³»ç»Ÿæä¾›äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.SE",
      "comment": "10 pages, 5 figures. This version corresponds to the paper accepted for presentation at CASCON 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.14798v1",
      "published_date": "2025-11-17 01:38:06 UTC",
      "updated_date": "2025-11-17 01:38:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:12:50.439664+00:00"
    },
    {
      "arxiv_id": "2511.12865v1",
      "title": "An approach of deep reinforcement learning for maximizing the net present value of stochastic projects",
      "title_zh": "ä¸€ç§æ—¨åœ¨æœ€å¤§åŒ–éšæœºé¡¹ç›®å‡€ç°å€¼çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•",
      "authors": [
        "Wei Xu",
        "Fan Yang",
        "Qinyuan Cui",
        "Zhi Chen"
      ],
      "abstract": "This paper investigates a project with stochastic activity durations and cash flows under discrete scenarios, where activities must satisfy precedence constraints generating cash inflows and outflows. The objective is to maximize expected net present value (NPV) by accelerating inflows and deferring outflows. We formulate the problem as a discrete-time Markov Decision Process (MDP) and propose a Double Deep Q-Network (DDQN) approach. Comparative experiments demonstrate that DDQN outperforms traditional rigid and dynamic strategies, particularly in large-scale or highly uncertain environments, exhibiting superior computational capability, policy reliability, and adaptability. Ablation studies further reveal that the dual-network architecture mitigates overestimation of action values, while the target network substantially improves training convergence and robustness. These results indicate that DDQN not only achieves higher expected NPV in complex project optimization but also provides a reliable framework for stable and effective policy implementation.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†åœ¨ç¦»æ•£æƒ…æ™¯ä¸‹å…·æœ‰éšæœºæ´»åŠ¨æŒç»­æ—¶é—´å’Œç°é‡‘æµçš„é¡¹ç›®ç®¡ç†é—®é¢˜ï¼Œæ—¨åœ¨æ»¡è¶³å‰åºçº¦æŸçš„åŒæ—¶ï¼Œé€šè¿‡ä¼˜åŒ–èµ„é‡‘æµæ—¶é—´èŠ‚ç‚¹æ¥æœ€å¤§åŒ–é¢„æœŸå‡€ç°å€¼(Net Present Value, NPV)ã€‚ç ”ç©¶å°†è¯¥é—®é¢˜å»ºæ¨¡ä¸ºç¦»æ•£æ—¶é—´é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(Markov Decision Process, MDP)ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŒæ·±åº¦Qç½‘ç»œ(Double Deep Q-Network, DDQN)æ–¹æ³•ã€‚å¯¹æ¯”å®éªŒè¡¨æ˜ï¼ŒDDQNåœ¨å¤„ç†å¤§è§„æ¨¡æˆ–é«˜åº¦ä¸ç¡®å®šçš„ç¯å¢ƒæ—¶ï¼Œå…¶è®¡ç®—èƒ½åŠ›ã€ç­–ç•¥å¯é æ€§å’Œé€‚åº”æ€§å‡ä¼˜äºä¼ ç»Ÿçš„åˆšæ€§åŠåŠ¨æ€ç­–ç•¥ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯æ˜ï¼ŒåŒç½‘ç»œæ¶æ„èƒ½æœ‰æ•ˆç¼“è§£å¯¹åŠ¨ä½œä»·å€¼çš„è¿‡é«˜ä¼°è®¡(overestimation)ï¼Œè€Œç›®æ ‡ç½‘ç»œ(target network)æ˜¾è‘—æå‡äº†è®­ç»ƒçš„æ”¶æ•›æ€§å’Œç¨³å¥æ€§ã€‚è¯¥ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒDDQNä¸ä»…èƒ½åœ¨å¤æ‚é¡¹ç›®ä¼˜åŒ–ä¸­å®ç°æ›´é«˜çš„é¢„æœŸNPVï¼Œè¿˜ä¸ºç¨³å®šé«˜æ•ˆçš„ç­–ç•¥å®æ–½æä¾›äº†ä¸€ä¸ªå¯é çš„å†³ç­–æ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12865v1",
      "published_date": "2025-11-17 01:32:08 UTC",
      "updated_date": "2025-11-17 01:32:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:12:49.333862+00:00"
    },
    {
      "arxiv_id": "2511.12852v1",
      "title": "From Black-Box to White-Box: Control-Theoretic Neural Network Interpretability",
      "title_zh": "ä»é»‘ç›’åˆ°ç™½ç›’ï¼šåŸºäºæ§åˆ¶ç†è®ºçš„ç¥ç»ç½‘ç»œå¯è§£é‡Šæ€§",
      "authors": [
        "Jihoon Moon"
      ],
      "abstract": "Deep neural networks achieve state of the art performance but remain difficult to interpret mechanistically. In this work, we propose a control theoretic framework that treats a trained neural network as a nonlinear state space system and uses local linearization, controllability and observability Gramians, and Hankel singular values to analyze its internal computation. For a given input, we linearize the network around the corresponding hidden activation pattern and construct a state space model whose state consists of hidden neuron activations. The input state and state output Jacobians define local controllability and observability Gramians, from which we compute Hankel singular values and associated modes. These quantities provide a principled notion of neuron and pathway importance: controllability measures how easily each neuron can be excited by input perturbations, observability measures how strongly each neuron influences the output, and Hankel singular values rank internal modes that carry input output energy. We illustrate the framework on simple feedforward networks, including a 1 2 2 1 SwiGLU network and a 2 3 3 2 GELU network. By comparing different operating points, we show how activation saturation reduces controllability, shrinks the dominant Hankel singular value, and shifts the dominant internal mode to a different subset of neurons. The proposed method turns a neural network into a collection of local white box dynamical models and suggests which internal directions are natural candidates for pruning or constraints to improve interpretability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ§åˆ¶ç†è®º(control theoretic)æ¡†æ¶ï¼Œæ—¨åœ¨å°†æ·±åº¦ç¥ç»ç½‘ç»œè§†ä¸ºéçº¿æ€§çŠ¶æ€ç©ºé—´ç³»ç»Ÿ(nonlinear state space system)ï¼Œä»¥å®ç°å¯¹å…¶å†…éƒ¨è®¡ç®—çš„æœºæ¢°è§£é‡Šã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨ç‰¹å®šè¾“å…¥å¯¹åº”çš„éšè—æ¿€æ´»æ¨¡å¼ä¸‹å¯¹ç½‘ç»œè¿›è¡Œå±€éƒ¨çº¿æ€§åŒ–(local linearization)ï¼Œæ„å»ºä»¥éšè—ç¥ç»å…ƒæ¿€æ´»ä¸ºçŠ¶æ€çš„çŠ¶æ€ç©ºé—´æ¨¡å‹ã€‚æ¡†æ¶åˆ©ç”¨è¾“å…¥-çŠ¶æ€å’ŒçŠ¶æ€-è¾“å‡ºçš„é›…å¯æ¯”çŸ©é˜µ(Jacobians)å®šä¹‰å±€éƒ¨å¯æ§æ€§(controllability)ä¸å¯è§‚æµ‹æ€§(observability)æ ¼æ‹‰å§†çŸ©é˜µ(Gramians)ï¼Œå¹¶ä»¥æ­¤è®¡ç®—æ±‰å…‹å°”å¥‡å¼‚å€¼(Hankel singular values)ã€‚è¿™äº›é‡åŒ–æŒ‡æ ‡ä¸ºè¯„ä¼°ç¥ç»å…ƒå’Œè·¯å¾„é‡è¦æ€§æä¾›äº†ç†è®ºåŸºç¡€ï¼šå¯æ§æ€§è¡¡é‡è¾“å…¥æ‰°åŠ¨æ¿€å‘ç¥ç»å…ƒçš„éš¾æ˜“ç¨‹åº¦ï¼Œå¯è§‚æµ‹æ€§è¡¡é‡ç¥ç»å…ƒå¯¹è¾“å‡ºçš„å½±å“åŠ›ï¼Œè€Œæ±‰å…‹å°”å¥‡å¼‚å€¼åˆ™ç”¨äºå¯¹æ‰¿è½½è¾“å…¥è¾“å‡ºèƒ½é‡çš„å†…éƒ¨æ¨¡å¼è¿›è¡Œæ’åºã€‚ç ”ç©¶åœ¨SwiGLUå’ŒGELUç­‰å‰é¦ˆç½‘ç»œä¸ŠéªŒè¯äº†è¯¥æ¡†æ¶ï¼Œå±•ç¤ºäº†æ¿€æ´»é¥±å’Œå¦‚ä½•é™ä½å¯æ§æ€§å¹¶å¯¼è‡´ä¸»å¯¼å†…éƒ¨æ¨¡å¼çš„åç§»ã€‚è¯¥æ–¹æ³•æˆåŠŸå°†é»‘ç›’æ¨¡å‹è½¬åŒ–ä¸ºä¸€ç³»åˆ—å±€éƒ¨ç™½ç›’åŠ¨åŠ›å­¦æ¨¡å‹ï¼Œä¸ºæ¨¡å‹å‰ªæ(pruning)å’Œæå‡å¯è§£é‡Šæ€§(interpretability)æä¾›äº†ç§‘å­¦çš„æŒ‡å¯¼æ–¹å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12852v1",
      "published_date": "2025-11-17 00:47:33 UTC",
      "updated_date": "2025-11-17 00:47:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:13:03.545692+00:00"
    },
    {
      "arxiv_id": "2511.12851v1",
      "title": "NeuroLex: A Lightweight Domain Language Model for EEG Report Understanding and Generation",
      "title_zh": "NeuroLexï¼šç”¨äºè„‘ç”µå›¾æŠ¥å‘Šç†è§£ä¸ç”Ÿæˆçš„è½»é‡çº§é¢†åŸŸè¯­è¨€æ¨¡å‹",
      "authors": [
        "Kang Yin",
        "Hye-Bin Shin"
      ],
      "abstract": "Clinical electroencephalogram (EEG) reports encode domain-specific linguistic conventions that general-purpose language models (LMs) fail to capture. We introduce NeuroLex, a lightweight domain-adaptive language model trained purely on EEG report text from the Harvard Electroencephalography Database. Unlike existing biomedical LMs, NeuroLex is tailored to the linguistic and diagnostic characteristics of EEG reporting, enabling it to serve as both an independent textual model and a decoder backbone for multimodal EEG-language systems. Using span-corruption pretraining and instruction-style fine-tuning on report polishing, paragraph summarization, and terminology question answering, NeuroLex learns the syntax and reasoning patterns characteristic of EEG interpretation. Comprehensive evaluations show that it achieves lower perplexity, higher extraction and summarization accuracy, better label efficiency, and improved robustness to negation and factual hallucination compared with general models of the same scale. With an EEG-aware linguistic backbone, NeuroLex bridges biomedical text modeling and brain-computer interface applications, offering a foundation for interpretable and language-driven neural decoding.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† NeuroLexï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºè„‘ç”µå›¾ (EEG) æŠ¥å‘Šç†è§£å’Œç”Ÿæˆè€Œè®¾è®¡çš„è½»é‡çº§é¢†åŸŸè‡ªé€‚åº”è¯­è¨€æ¨¡å‹ã€‚é’ˆå¯¹é€šç”¨æ¨¡å‹éš¾ä»¥æ•æ‰çš„é¢†åŸŸç‰¹å®šè¯­è¨€è§„èŒƒï¼Œè¯¥æ¨¡å‹åœ¨ Harvard Electroencephalography Database çš„æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œå…·å¤‡å¤„ç† EEG æŠ¥å‘Šè¯­è¨€ç‰¹å¾å’Œè¯Šæ–­ç‰¹æ€§çš„èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡è·¨åº¦æŸå (span-corruption) é¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¼å¾®è°ƒï¼Œä½¿æ¨¡å‹æŒæ¡äº†æŠ¥å‘Šæ¶¦è‰²ã€æ®µè½æ‘˜è¦åŠæœ¯è¯­é—®ç­”ç­‰æ ¸å¿ƒæ¨ç†æ¨¡å¼ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒNeuroLex åœ¨å›°æƒ‘åº¦ (perplexity)ã€æ€»ç»“å‡†ç¡®æ€§ä»¥åŠåº”å¯¹äº‹å®å¹»è§‰çš„é²æ£’æ€§æ–¹é¢å‡ä¼˜äºåŒè§„æ¨¡çš„é€šç”¨æ¨¡å‹ã€‚ä½œä¸ºä¸€ç§å…·å¤‡ EEG æ„ŸçŸ¥èƒ½åŠ›çš„è¯­è¨€åŸºåº§ï¼ŒNeuroLex æˆåŠŸæ¡¥æ¥äº†ç”Ÿç‰©åŒ»å­¦æ–‡æœ¬å»ºæ¨¡ä¸è„‘æœºæ¥å£ (BCI) åº”ç”¨ï¼Œä¸ºå¯è§£é‡Šçš„ç¥ç»è§£ç æä¾›äº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12851v1",
      "published_date": "2025-11-17 00:44:35 UTC",
      "updated_date": "2025-11-17 00:44:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:12:55.640928+00:00"
    },
    {
      "arxiv_id": "2511.12846v1",
      "title": "RoS-Guard: Robust and Scalable Online Change Detection with Delay-Optimal Guarantees",
      "title_zh": "RoS-Guardï¼šå…·å¤‡æ—¶å»¶æœ€ä¼˜ä¿è¯çš„é²æ£’ã€å¯æ‰©å±•åœ¨çº¿å˜åŒ–æ£€æµ‹",
      "authors": [
        "Zelin Zhu",
        "Yancheng Huang",
        "Kai Yang"
      ],
      "abstract": "Online change detection (OCD) aims to rapidly identify change points in streaming data and is critical in applications such as power system monitoring, wireless network sensing, and financial anomaly detection. Existing OCD methods typically assume precise system knowledge, which is unrealistic due to estimation errors and environmental variations. Moreover, existing OCD methods often struggle with efficiency in large-scale systems. To overcome these challenges, we propose RoS-Guard, a robust and optimal OCD algorithm tailored for linear systems with uncertainty. Through a tight relaxation and reformulation of the OCD optimization problem, RoS-Guard employs neural unrolling to enable efficient parallel computation via GPU acceleration. The algorithm provides theoretical guarantees on performance, including expected false alarm rate and worst-case average detection delay. Extensive experiments validate the effectiveness of RoS-Guard and demonstrate significant computational speedup in large-scale system scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”µåŠ›ç³»ç»Ÿç›‘æ§å’Œæ— çº¿ç½‘ç»œæ„ŸçŸ¥ç­‰é¢†åŸŸçš„åœ¨çº¿å˜åŒ–æ£€æµ‹ (Online change detection, OCD) é—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿæ–¹æ³•ç”±äºè¿‡äºä¾èµ–ç²¾ç¡®çš„ç³»ç»ŸçŸ¥è¯†è€Œåœ¨å®é™…åº”ç”¨ä¸­é¢ä¸´é²æ£’æ€§ä¸è¶³åŠå¤§è§„æ¨¡ç³»ç»Ÿæ•ˆç‡ä½ä¸‹çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† RoS-Guardï¼Œä¸€ç§ä¸“ä¸ºå…·æœ‰ä¸ç¡®å®šæ€§çš„çº¿æ€§ç³»ç»Ÿè®¾è®¡çš„é²æ£’ä¸”æœ€ä¼˜çš„åœ¨çº¿å˜åŒ–æ£€æµ‹ç®—æ³•ã€‚è¯¥ç®—æ³•é€šè¿‡å¯¹ OCD ä¼˜åŒ–é—®é¢˜çš„ç´§è‡´æ¾å¼›ä¸é‡æ–°è¡¨è¿°ï¼Œå¹¶ç»“åˆç¥ç»å±•å¼€ (neural unrolling) æŠ€æœ¯ï¼Œå®ç°äº†åˆ©ç”¨ GPU åŠ é€Ÿçš„é«˜æ•ˆå¹¶è¡Œè®¡ç®—ã€‚RoS-Guard åœ¨ç†è®ºä¸Šæä¾›äº†ä¸¥æ ¼çš„æ€§èƒ½ä¿è¯ï¼Œæ¶µç›–äº†é¢„æœŸçš„è¯¯æŠ¥ç‡ (false alarm rate) å’Œæœ€åæƒ…å†µä¸‹çš„å¹³å‡æ£€æµ‹å»¶è¿Ÿ (average detection delay)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRoS-Guard ä¸ä»…åœ¨æ£€æµ‹æ•ˆæœä¸Šå…·æœ‰ä¼˜åŠ¿ï¼Œè€Œä¸”åœ¨å¤§è§„æ¨¡ç³»ç»Ÿåœºæ™¯ä¸‹è¡¨ç°å‡ºæ˜¾è‘—çš„è®¡ç®—åŠ é€Ÿèƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12846v1",
      "published_date": "2025-11-17 00:30:36 UTC",
      "updated_date": "2025-11-17 00:30:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:13:31.526977+00:00"
    },
    {
      "arxiv_id": "2511.12844v3",
      "title": "Towards Reinforcement Learning from Neural Feedback: Mapping fNIRS Signals to Agent Performance",
      "title_zh": "è¿ˆå‘åŸºäºç¥ç»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼šå°† fNIRS ä¿¡å·æ˜ å°„è‡³æ™ºèƒ½ä½“æ€§èƒ½",
      "authors": [
        "Julia Santaniello",
        "Matthew Russell",
        "Benson Jiang",
        "Donatello Sassaroli",
        "Robert Jacob",
        "Jivko Sinapov"
      ],
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a methodology that aligns agent behavior with human preferences by integrating user feedback into the agent's training process. This paper introduces a framework that guides agent training through implicit neural signals, with a focus on the neural classification problem. Our work presents and releases a novel dataset of functional near-infrared spectroscopy (fNIRS) recordings collected from 25 human participants across three domains: Pick-and-Place Robot, Lunar Lander, and Flappy Bird. We train multiple classifiers to predict varying levels of agent performance (optimal, suboptimal, or worst-case) from windows of preprocessed fNIRS features, achieving an average F1 score of 67% for binary and 46% for multi-class classification across conditions and domains. We also train multiple regressors to predict the degree of deviation between an agent's chosen action and a set of near-optimal policy actions, providing a continuous measure of performance. Finally, we evaluate cross-subject generalization and show that fine-tuning pre-trained models with a small sample of subject-specific data increases average F1 scores by 17% and 41% for binary and multi-class models, respectively. Our results demonstrate that mapping implicit fNIRS signals to agent performance is feasible and can be improved, laying the foundation for future Reinforcement Learning from Neural Feedback (RLNF) systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é€šè¿‡éšå¼ç¥ç»ä¿¡å·æŒ‡å¯¼æ™ºèƒ½ä½“è®­ç»ƒçš„æ¡†æ¶ï¼Œæ—¨åœ¨æ¢ç´¢ä»ç¥ç»åé¦ˆä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ (Reinforcement Learning from Neural Feedback, RLNF)çš„å¯èƒ½æ€§ã€‚ç ”ç©¶å›¢é˜Ÿå‘å¸ƒäº†ä¸€ä¸ªåŒ…å«25åå—è¯•è€…åœ¨ä¸‰ä¸ªä¸åŒé¢†åŸŸ(Pick-and-Place Robot, Lunar Lander, Flappy Bird)äº§ç”Ÿçš„è¿‘çº¢å¤–è„‘åŠŸèƒ½æˆåƒ(fNIRS)ä¿¡å·æ•°æ®é›†ã€‚é€šè¿‡è®­ç»ƒå¤šç§åˆ†ç±»å™¨ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ ¹æ®fNIRSç‰¹å¾é¢„æµ‹æ™ºèƒ½ä½“çš„è¡¨ç°ç­‰çº§(optimal, suboptimal æˆ– worst-case)ï¼Œåœ¨äºŒåˆ†ç±»å’Œå¤šåˆ†ç±»ä»»åŠ¡ä¸­åˆ†åˆ«å–å¾—äº†67%å’Œ46%çš„å¹³å‡F1åˆ†æ•°ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡å›å½’æ¨¡å‹é¢„æµ‹æ™ºèƒ½ä½“åŠ¨ä½œä¸è¿‘ä¼˜ç­–ç•¥(near-optimal policy)çš„åå·®ï¼Œå®ç°äº†å¯¹æ€§èƒ½çš„è¿ç»­é‡åŒ–è¯„ä¼°ã€‚å®éªŒè¿›ä¸€æ­¥è¯æ˜ï¼Œåˆ©ç”¨å°‘é‡ç‰¹å®šå—è¯•è€…æ•°æ®å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒ(fine-tuning)ï¼Œå¯ä½¿äºŒåˆ†ç±»å’Œå¤šåˆ†ç±»æ¨¡å‹çš„F1åˆ†æ•°åˆ†åˆ«æ˜¾è‘—æå‡17%å’Œ41%ã€‚è¿™é¡¹å·¥ä½œè¯å®äº†å°†éšå¼fNIRSä¿¡å·æ˜ å°„ä¸ºæ™ºèƒ½ä½“æ€§èƒ½è¡¨ç°çš„å¯è¡Œæ€§ï¼Œä¸ºæœªæ¥å¼€å‘RLNFç³»ç»Ÿå¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to the Association for the Advancement of Artificial Intelligence (AAAI) 2026. To appear in the AAAI 2026 Proceedings",
      "pdf_url": "https://arxiv.org/pdf/2511.12844v3",
      "published_date": "2025-11-17 00:21:46 UTC",
      "updated_date": "2026-01-21 04:22:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:13:28.931510+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 192,
  "processed_papers_count": 192,
  "failed_papers_count": 0,
  "llm_backup_calls": 2,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-26T07:16:02.869945+00:00"
}
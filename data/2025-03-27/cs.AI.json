{
  "date": "2025-03-27",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-03-27 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 126 篇论文，主要聚焦 AI 模型优化（如 LLM 和视觉语言模型）、机器人学习、多模态处理以及强化学习等领域，亮点包括 LLM 在代码生成和机器人任务中的高效应用，以及著名学者如 Song Han 和 Chelsea Finn 在 \"CoT-VLA\" 等论文中的创新贡献，强调了模型效率和实际应用潜力。\n\n下面，我将挑选并讨论今天更重要的论文，先从那些创新性强、可能有话题度的文章入手（如 LLM 增强和机器人相关），然后将相关主题归类讨论，最后快速掠过其他次要论文。每个条目会列出论文标题（中文 + 英文），并简要概述主要贡献和发现，保留核心学术术语。\n\n### 重点论文讨论\n\n**1. ObscuraCoder: Powering Efficient Code LM Pre-Training Via Obfuscation Grounding**  \n这篇论文由 Iryna Gurevych 等著名学者发布，探讨了代码语言模型（Code-LMs）的预训练优化。贡献在于引入混淆代码数据集 ObscuraX 和混淆预训练目标，提升了模型在语法和语义理解上的数据效率，并在多语言代码补全和生成任务中表现出色，发现了混淆训练能显著改善模型性能。\n\n**2. Cognitive Prompts Using Guilford's Structure of Intellect Model**  \n作者 Oliver Kramer 提出了一种基于认知理论的提示工程方法。论文的主要发现是通过 Guilford 智力结构模型，增强 LLM 在模式识别和评估方面的推理能力，显著提高了模型响应的清晰度和适应性，为 LLM 问题解决提供系统性框架。\n\n**3. CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models**  \n这篇由 Song Han、Chelsea Finn 等知名学者参与的论文备受关注。贡献包括引入视觉链式思考（Visual Chain-of-Thought）机制，让视觉语言行动模型（VLAs）能够预测未来图像并生成行动序列，实验显示在真实机器人任务中比 SOTA 模型提升 17%，证明了该方法在复杂操作中的规划能力。\n\n**4. BOOTPLACE: Bootstrapped Object Placement with Detection Transformers**  \n论文聚焦图像合成中的物体放置学习。关键创新是使用检测 Transformer 在物体减法背景下训练模型，并通过引导式训练实现精确放置，CVPR 2025 论文在 Cityscapes 和 OPA 数据集上显著提升 IOU 分数，展示了鲁棒的物体重新定位能力。\n\n**5. Pretrained Bayesian Non-parametric Knowledge Prior in Robotic Long-Horizon Reinforcement Learning**  \n作者团队包括 Alois Knoll，提出使用 Bayesian 非参数模型预训练技能先验。发现这种方法提升了机器人长时任务的技能转移效率，实验证明在复杂环境中显著优于现有方法，强调了非参数表示在机器人学习中的优势。\n\n**6. Data-Agnostic Robotic Long-Horizon Manipulation with Vision-Language-Guided Closed-Loop Feedback**  \n相关于上一篇，论文引入 DAHLIA 框架，利用 LLM 进行任务规划和反馈循环。贡献在于实现无数据依赖的机器人操作，实验在模拟和真实环境中表现出色，突出了 LLM 在长时任务中的实时适应性。\n\n**7. Entropy-Aware Branching for Improved Mathematical Reasoning**  \n论文优化 LLM 的数学推理，通过分支策略处理高熵 token。发现这种方法提升了小模型的推理准确性，高达 4.6%，为 LLM 解码策略提供了新视角。\n\n**8. Boosting Large Language Models with Mask Fine-Tuning**  \n作者 Yun Fu 等提出 Mask Fine-Tuning（MFT）范式。关键发现是打破模型完整性能提升性能，实验在各种任务中平均提升 1.95%，扩展了掩码学习在模型优化的应用。\n\n**9. Parametric Shadow Control for Portrait Generation in Text-to-Image Diffusion Models**  \n论文引入 Shadow Director 方法，实现文本到图像模型中的阴影控制。贡献在于使用小网络处理阴影参数，提升了生成图像的艺术性和身份一致性，实验证明其在合成数据上泛化良好。\n\n**10. Lobster: A GPU-Accelerated Framework for Neurosymbolic Programming**  \n这篇论文开发了 Lobster 框架，支持 GPU 端到端神经符号编程。发现它在多种应用中比 SOTA 框架快 5.3 倍，展示了符号推理在 GPU 上的高效性。\n\n**11. Uni4D: Unifying Visual Foundation Models for 4D Modeling from a Single Video**  \n论文统一多种视觉基础模型，实现单视频到 4D 建模。贡献在于无需额外训练就达到 SOTA 性能，实验验证了其在动态场景重建中的鲁棒性。\n\n**12. StyleMotif: Multi-Modal Motion Stylization using Style-Content Cross Fusion**  \n相关于多模态生成，论文提出一种风格化运动扩散模型。发现它能无缝整合文本、图像和音频等模态，提升运动生成的多样性和真实性。\n\n**13. Stable-SCore: A Stable Registration-based Framework for 3D Shape Correspondence**  \n论文提供了一个稳定的 3D 形状配准框架。关键创新是使用语义流引导配准，实验在复杂场景中显著优于现有方法，适用于实际图形应用。\n\n**14. AutoPsyC: Automatic Recognition of Psychodynamic Conflicts from Semi-structured Interviews with Large Language Models**  \n这篇探索 LLM 在心理诊断中的应用。贡献在于首次使用 LLM 识别心理冲突，实验在真实访谈数据上 outperform 基线，展示了 LLM 在医疗领域的潜力。\n\n**15. JEEM: Vision-Language Understanding in Four Arabic Dialects**  \n论文构建了 JEEM 数据集，评估 VLM 在阿拉伯方言中的视觉理解。发现现有模型在方言适应性上不足，GPT-4V 表现最佳，但强调了更包容模型的需求。\n\n### 相关论文简要归类\n- **AI 模型和 LLM 优化主题**：如 \"Enhance Vision-based Tactile Sensors via Dynamic Illumination and Image Fusion\"，发现动态照明提升传感器精度；\"Fwd2Bot\" 通过双向瓶颈压缩视觉 token，提升生成任务效率。这些论文扩展了 LLM 在感知和生成中的应用，总体上强调了模型压缩和鲁棒性的重要性。\n  \n- **机器人和强化学习**：论文如 \"ReaRAG\" 使用强化学习提升 LLM 事实性，\"Collab\" 提出混合代理解码策略，实验显示在任务中提升 1.56 倍奖励。这些工作展示了强化学习在多任务协作中的潜力。\n\n- **图像和多模态处理**：如 \"Foveated Instance Segmentation\" 通过注视引导优化分割，\"HyperGraphRAG\" 使用超图提升检索，均在基准上显著改进，突出多模态融合的实际价值。\n\n### 快速掠过其他论文\n剩余论文中，许多聚焦较窄领域，如 \"Research on the Design of a Short Video Recommendation System\"（短视频推荐系统设计，贡献在于结合多模态和差分隐私提升准确性，但较常规）；\"Entropy-Aware Branching\"（数学推理优化，实验提升 4.6%，但主题较专精）；\"Outlier dimensions favor frequent tokens\"（LLM 维度的启发性分析，发现其预测频繁词的机制）。这些论文虽有发现，但影响力较小，仅快速提及其核心：如 \"A Survey on (M)LLM-Based GUI Agents\" 综述了 GUI 代理的进展；\"The Cost of Local and Global Fairness\" 探讨了联邦学习中的公平性权衡；\"ReFeed\" 提出总结精炼框架，提升 LLM 生成质量。总体上，这些工作在 AI 伦理和应用中提供补充，但未有突破性创新。\n\n今天的 arXiv 更新展示了 AI 领域的活跃创新，特别在 LLM 和机器人方向值得关注。明天的快报将持续追踪新进展！",
  "papers": [
    {
      "arxiv_id": "2504.00019v1",
      "title": "ObscuraCoder: Powering Efficient Code LM Pre-Training Via Obfuscation Grounding",
      "title_zh": "翻译失败",
      "authors": [
        "Indraneil Paul",
        "Haoyi Yang",
        "Goran Glavaš",
        "Kristian Kersting",
        "Iryna Gurevych"
      ],
      "abstract": "Language models (LMs) have become a staple of the code-writing toolbox. Their\npre-training recipe has, however, remained stagnant over recent years, barring\nthe occasional changes in data sourcing and filtering strategies. In\nparticular, research exploring modifications to Code-LMs' pre-training\nobjectives, geared towards improving data efficiency and better disentangling\nbetween syntax and semantics, has been noticeably sparse, especially compared\nwith corresponding efforts in natural language LMs. In this work, we examine\ngrounding on obfuscated code as a means of helping Code-LMs look beyond the\nsurface-form syntax and enhance their pre-training sample efficiency. To this\nend, we compile ObscuraX, a dataset of approximately 55M source and obfuscated\ncode pairs in seven languages. Subsequently, we pre-train ObscuraCoder models,\nranging in size from 255M to 2.8B parameters, on a 272B-token corpus that\nincludes ObscuraX and demonstrate that our obfuscation-based pre-training\nrecipe leads to consistent improvements in Code-LMs' abilities compared to both\nvanilla autoregressive pre-training as well as existing de-obfuscation (DOBF)\nobjectives. ObscuraCoder demonstrates sizeable gains across multiple tests of\nsyntactic and semantic code understanding, along with improved capabilities in\nmultilingual code completion, multilingual code commit summarization, and\nmulti-purpose library-oriented code generation.",
      "tldr_zh": "本研究提出 ObscuraCoder，一种通过混淆 grounding（Obfuscation Grounding）提升代码语言模型（Code-LMs）预训练效率的方法，旨在帮助模型超越表面语法并更好地区分语法与语义。研究者编译了 ObscuraX 数据集，包含约 55M 对源代码和混淆代码对，覆盖七种语言，并使用 272B 标记的语料预训练了从 255M 到 2.8B 参数的 ObscuraCoder 模型。相比传统的自回归预训练和现有的去混淆（DOBF）目标，该方法在语法与语义代码理解、多语言代码补全、代码提交总结以及库导向代码生成任务上实现了显著改进。总的来说，ObscuraCoder 提升了 Code-LMs 的数据效率和多功能性，为代码模型预训练提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.00019v1",
      "published_date": "2025-03-27 23:08:53 UTC",
      "updated_date": "2025-03-27 23:08:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:33:15.792529"
    },
    {
      "arxiv_id": "2503.22036v2",
      "title": "Cognitive Prompts Using Guilford's Structure of Intellect Model",
      "title_zh": "翻译失败",
      "authors": [
        "Oliver Kramer"
      ],
      "abstract": "Large language models (LLMs) demonstrate strong language generation\ncapabilities but often struggle with structured reasoning, leading to\ninconsistent or suboptimal problem-solving. To mitigate this limitation,\nGuilford's Structure of Intellect (SOI) model - a foundational framework from\nintelligence theory - is leveraged as the basis for cognitive prompt\nengineering. The SOI model categorizes cognitive operations such as pattern\nrecognition, memory retrieval, and evaluation, offering a systematic approach\nto enhancing LLM reasoning and decision-making. This position paper presents a\nnovel cognitive prompting approach for enforcing SOI-inspired reasoning for\nimproving clarity, coherence, and adaptability in model responses.",
      "tldr_zh": "本研究针对大型语言模型 (LLMs) 在结构化推理方面的不足，如问题解决的不一致性或次优表现，引入了 Guilford's Structure of Intellect (SOI) 模型作为认知提示工程的基础。SOI 模型通过分类认知操作，包括 pattern recognition、memory retrieval 和 evaluation，提供了一个系统化的框架来增强 LLM 的推理和决策能力。该立场论文提出了一种新型认知提示方法，强制执行 SOI 启发的推理流程，从而显著改善模型响应的清晰性(clarity)、连贯性(coherence)和适应性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22036v2",
      "published_date": "2025-03-27 23:06:30 UTC",
      "updated_date": "2025-04-03 09:08:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:33:27.592262"
    },
    {
      "arxiv_id": "2504.08751v1",
      "title": "Research on the Design of a Short Video Recommendation System Based on Multimodal Information and Differential Privacy",
      "title_zh": "基于多模态信息和差分隐私的短视频推荐系统设计研究",
      "authors": [
        "Haowei Yang",
        "Lei Fu",
        "Qingyi Lu",
        "Yue Fan",
        "Tianle Zhang",
        "Ruohan Wang"
      ],
      "abstract": "With the rapid development of short video platforms, recommendation systems\nhave become key technologies for improving user experience and enhancing\nplatform engagement. However, while short video recommendation systems leverage\nmultimodal information (such as images, text, and audio) to improve\nrecommendation effectiveness, they also face the severe challenge of user\nprivacy leakage. This paper proposes a short video recommendation system based\non multimodal information and differential privacy protection. First, deep\nlearning models are used for feature extraction and fusion of multimodal data,\neffectively improving recommendation accuracy. Then, a differential privacy\nprotection mechanism suitable for recommendation scenarios is designed to\nensure user data privacy while maintaining system performance. Experimental\nresults show that the proposed method outperforms existing mainstream\napproaches in terms of recommendation accuracy, multimodal fusion\neffectiveness, and privacy protection performance, providing important insights\nfor the design of recommendation systems for short video platforms.",
      "tldr_zh": "本文提出了一种基于多模态 information 和 differential privacy 的短视频推荐系统，以提升推荐准确性并解决用户隐私泄露问题。该系统利用深度学习模型提取并融合图像、文本和音频等多模态数据，提高推荐效果，同时设计了适用于推荐场景的 differential privacy 保护机制，确保数据隐私的同时维持系统性能。实验结果显示，该方法在推荐准确性、多模态融合有效性和隐私保护性能上均优于现有主流方法，为短视频平台推荐系统设计提供了重要启示。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.08751v1",
      "published_date": "2025-03-27 22:56:41 UTC",
      "updated_date": "2025-03-27 22:56:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:33:38.814309"
    },
    {
      "arxiv_id": "2503.22023v1",
      "title": "Safeguarding Autonomy: a Focus on Machine Learning Decision Systems",
      "title_zh": "保障自治：聚焦机器学习决策系统",
      "authors": [
        "Paula Subías-Beltrán",
        "Oriol Pujol",
        "Itziar de Lecuona"
      ],
      "abstract": "As global discourse on AI regulation gains momentum, this paper focuses on\ndelineating the impact of ML on autonomy and fostering awareness. Respect for\nautonomy is a basic principle in bioethics that establishes persons as\ndecision-makers. While the concept of autonomy in the context of ML appears in\nseveral European normative publications, it remains a theoretical concept that\nhas yet to be widely accepted in ML practice. Our contribution is to bridge the\ntheoretical and practical gap by encouraging the practical application of\nautonomy in decision-making within ML practice by identifying the conditioning\nfactors that currently prevent it. Consequently, we focus on the different\nstages of the ML pipeline to identify the potential effects on ML end-users'\nautonomy. To improve its practical utility, we propose a related question for\neach detected impact, offering guidance for identifying possible focus points\nto respect ML end-users autonomy in decision-making.",
      "tldr_zh": "这篇论文探讨了机器学习(ML)对自治的影响，强调自治作为生物伦理学核心原则在ML实践中的应用。作者分析了ML管道的不同阶段，识别出影响ML最终用户决策自治的条件因素，如幻觉和知识缺口。最终，他们提出了一系列相关问题作为指导，帮助从业者在ML决策系统中更好地尊重和保护用户自治。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22023v1",
      "published_date": "2025-03-27 22:31:16 UTC",
      "updated_date": "2025-03-27 22:31:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:33:49.591142"
    },
    {
      "arxiv_id": "2503.22020v1",
      "title": "CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models",
      "title_zh": "翻译失败",
      "authors": [
        "Qingqing Zhao",
        "Yao Lu",
        "Moo Jin Kim",
        "Zipeng Fu",
        "Zhuoyang Zhang",
        "Yecheng Wu",
        "Zhaoshuo Li",
        "Qianli Ma",
        "Song Han",
        "Chelsea Finn",
        "Ankur Handa",
        "Ming-Yu Liu",
        "Donglai Xiang",
        "Gordon Wetzstein",
        "Tsung-Yi Lin"
      ],
      "abstract": "Vision-language-action models (VLAs) have shown potential in leveraging\npretrained vision-language models and diverse robot demonstrations for learning\ngeneralizable sensorimotor control. While this paradigm effectively utilizes\nlarge-scale data from both robotic and non-robotic sources, current VLAs\nprimarily focus on direct input--output mappings, lacking the intermediate\nreasoning steps crucial for complex manipulation tasks. As a result, existing\nVLAs lack temporal planning or reasoning capabilities. In this paper, we\nintroduce a method that incorporates explicit visual chain-of-thought (CoT)\nreasoning into vision-language-action models (VLAs) by predicting future image\nframes autoregressively as visual goals before generating a short action\nsequence to achieve these goals. We introduce CoT-VLA, a state-of-the-art 7B\nVLA that can understand and generate visual and action tokens. Our experimental\nresults demonstrate that CoT-VLA achieves strong performance, outperforming the\nstate-of-the-art VLA model by 17% in real-world manipulation tasks and 6% in\nsimulation benchmarks. Project website: https://cot-vla.github.io/",
      "tldr_zh": "该研究指出，现有的视觉-语言-动作模型（VLAs）虽能利用大规模数据学习通用的传感器控制，但缺乏中间推理步骤，导致在复杂操作任务中缺少时间规划能力。为解决此问题，论文提出了一种方法，通过自回归预测未来图像帧作为视觉目标，并在生成短动作序列前整合显式的视觉链式思维（Visual Chain-of-Thought, CoT）推理。CoT-VLA 是一个先进的 7B 模型，能理解和生成视觉及动作标记，并在实验中表现出色，在真实世界操作任务中比最先进模型提升 17%，在模拟基准中提升 6%。这项工作为提升 VLAs 的推理能力提供了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Project website: https://cot-vla.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2503.22020v1",
      "published_date": "2025-03-27 22:23:04 UTC",
      "updated_date": "2025-03-27 22:23:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:34:03.591336"
    },
    {
      "arxiv_id": "2503.21991v1",
      "title": "BOOTPLACE: Bootstrapped Object Placement with Detection Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Hang Zhou",
        "Xinxin Zuo",
        "Rui Ma",
        "Li Cheng"
      ],
      "abstract": "In this paper, we tackle the copy-paste image-to-image composition problem\nwith a focus on object placement learning. Prior methods have leveraged\ngenerative models to reduce the reliance for dense supervision. However, this\noften limits their capacity to model complex data distributions. Alternatively,\ntransformer networks with a sparse contrastive loss have been explored, but\ntheir over-relaxed regularization often leads to imprecise object placement. We\nintroduce BOOTPLACE, a novel paradigm that formulates object placement as a\nplacement-by-detection problem. Our approach begins by identifying suitable\nregions of interest for object placement. This is achieved by training a\nspecialized detection transformer on object-subtracted backgrounds, enhanced\nwith multi-object supervisions. It then semantically associates each target\ncompositing object with detected regions based on their complementary\ncharacteristics. Through a boostrapped training approach applied to randomly\nobject-subtracted images, our model enforces meaningful placements through\nextensive paired data augmentation. Experimental results on established\nbenchmarks demonstrate BOOTPLACE's superior performance in object\nrepositioning, markedly surpassing state-of-the-art baselines on Cityscapes and\nOPA datasets with notable improvements in IOU scores. Additional ablation\nstudies further showcase the compositionality and generalizability of our\napproach, supported by user study evaluations.",
      "tldr_zh": "本论文提出BOOTPLACE，一种基于Detection Transformers的引导式物体放置方法，旨在解决图像合成(copy-paste image-to-image composition)中物体放置的精确性问题。BOOTPLACE将物体放置转化为检测任务，通过在物体被移除的背景图像上训练专用检测transformer，并结合多物体监督(multi-object supervisions)和语义关联机制，确保物体与检测区域的互补匹配；同时，利用bootstrapped training在随机物体移除图像上进行配对数据增强，以强化有意义的放置。实验结果显示，该方法在Cityscapes和OPA数据集上显著优于现有基线，提高了IOU scores，并在消融研究和用户评估中证明了其组成性(compositionality)和泛化性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025. Project page: https://ryanhangzhou.github.io/bootplace/ ,\n  code: https://github.com/RyanHangZhou/BOOTPLACE",
      "pdf_url": "http://arxiv.org/pdf/2503.21991v1",
      "published_date": "2025-03-27 21:21:20 UTC",
      "updated_date": "2025-03-27 21:21:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:34:14.965177"
    },
    {
      "arxiv_id": "2503.21975v1",
      "title": "Pretrained Bayesian Non-parametric Knowledge Prior in Robotic Long-Horizon Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Yuan Meng",
        "Xiangtong Yao",
        "Kejia Chen",
        "Yansong Wu",
        "Liding Zhang",
        "Zhenshan Bing",
        "Alois Knoll"
      ],
      "abstract": "Reinforcement learning (RL) methods typically learn new tasks from scratch,\noften disregarding prior knowledge that could accelerate the learning process.\nWhile some methods incorporate previously learned skills, they usually rely on\na fixed structure, such as a single Gaussian distribution, to define skill\npriors. This rigid assumption can restrict the diversity and flexibility of\nskills, particularly in complex, long-horizon tasks. In this work, we introduce\na method that models potential primitive skill motions as having non-parametric\nproperties with an unknown number of underlying features. We utilize a Bayesian\nnon-parametric model, specifically Dirichlet Process Mixtures, enhanced with\nbirth and merge heuristics, to pre-train a skill prior that effectively\ncaptures the diverse nature of skills. Additionally, the learned skills are\nexplicitly trackable within the prior space, enhancing interpretability and\ncontrol. By integrating this flexible skill prior into an RL framework, our\napproach surpasses existing methods in long-horizon manipulation tasks,\nenabling more efficient skill transfer and task success in complex\nenvironments. Our findings show that a richer, non-parametric representation of\nskill priors significantly improves both the learning and execution of\nchallenging robotic tasks. All data, code, and videos are available at\nhttps://ghiara.github.io/HELIOS/.",
      "tldr_zh": "这篇论文针对强化学习 (RL) 在机器人长时序任务中的问题，提出了一种预训练的 Bayesian 非参数知识先验方法，以克服传统方法依赖固定结构（如单高斯分布）导致的技能多样性和灵活性不足。方法利用 Dirichlet Process Mixtures 模型，结合 birth and merge heuristics 来建模未知数量的技能特征，并增强技能的可追踪性，从而提高可解释性和控制。实验结果表明，该方法在复杂操作任务中超越现有框架，实现更高效的技能转移和任务成功率，并证明非参数技能先验的丰富表示显著提升了机器人学习性能。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "initial upload 8 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.21975v1",
      "published_date": "2025-03-27 20:43:36 UTC",
      "updated_date": "2025-03-27 20:43:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:34:27.155041"
    },
    {
      "arxiv_id": "2503.21969v1",
      "title": "Data-Agnostic Robotic Long-Horizon Manipulation with Vision-Language-Guided Closed-Loop Feedback",
      "title_zh": "数据无关的机器人长时域操控，采用视觉-语言引导",
      "authors": [
        "Yuan Meng",
        "Xiangtong Yao",
        "Haihui Ye",
        "Yirui Zhou",
        "Shengqiang Zhang",
        "Zhenshan Bing",
        "Alois Knoll"
      ],
      "abstract": "Recent advances in language-conditioned robotic manipulation have leveraged\nimitation and reinforcement learning to enable robots to execute tasks from\nhuman commands. However, these methods often suffer from limited\ngeneralization, adaptability, and the lack of large-scale specialized datasets,\nunlike data-rich domains such as computer vision, making long-horizon task\nexecution challenging. To address these gaps, we introduce DAHLIA, a\ndata-agnostic framework for language-conditioned long-horizon robotic\nmanipulation, leveraging large language models (LLMs) for real-time task\nplanning and execution. DAHLIA employs a dual-tunnel architecture, where an\nLLM-powered planner collaborates with co-planners to decompose tasks and\ngenerate executable plans, while a reporter LLM provides closed-loop feedback,\nenabling adaptive re-planning and ensuring task recovery from potential\nfailures. Moreover, DAHLIA integrates chain-of-thought (CoT) in task reasoning\nand temporal abstraction for efficient action execution, enhancing traceability\nand robustness. Our framework demonstrates state-of-the-art performance across\ndiverse long-horizon tasks, achieving strong generalization in both simulated\nand real-world scenarios. Videos and code are available at\nhttps://ghiara.github.io/DAHLIA/.",
      "tldr_zh": "本研究引入了DAHLIA框架，一种data-agnostic的机器人长期操作系统，利用大型语言模型(LLMs)进行语言条件下的实时任务规划和执行，以解决现有方法的泛化性差、适应性不足和数据集依赖问题。DAHLIA采用双隧道架构，由LLM驱动的规划器与协planner合作分解任务并生成可执行计划，同时reporter LLM提供closed-loop feedback，支持适应性重新规划和故障恢复；框架还整合chain-of-thought (CoT)推理和时间抽象，提升任务的鲁棒性和效率。在模拟和真实场景中，DAHLIA在多样化长期任务上实现了最先进性能，展示了强大的泛化能力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "initial upload 8 page",
      "pdf_url": "http://arxiv.org/pdf/2503.21969v1",
      "published_date": "2025-03-27 20:32:58 UTC",
      "updated_date": "2025-03-27 20:32:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:34:38.436244"
    },
    {
      "arxiv_id": "2503.21961v1",
      "title": "Entropy-Aware Branching for Improved Mathematical Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Xianzhi Li",
        "Ethan Callanan",
        "Xiaodan Zhu",
        "Mathieu Sibue",
        "Antony Papadimitriou",
        "Mahmoud Mahfouz",
        "Zhiqiang Ma",
        "Xiaomo Liu"
      ],
      "abstract": "While Large Language Models (LLMs) are effectively aligned through extensive\npre-training and fine-tuning, they still struggle with varying levels of\nuncertainty during token generation. In our investigation of mathematical\nreasoning, we observe that errors are more likely to arise at tokens exhibiting\nhigh entropy and variance of entropy in the model's output distribution. Based\non the observation, we propose a novel approach that dynamically branches the\ngeneration process on demand instead of defaulting to the single most probable\ntoken. By exploring in parallel multiple branches stemming from high\nprobability tokens of critical decision points, the model can discover diverse\nreasoning paths that might otherwise be missed. We further harness external\nfeedback from larger models to rank and select the most coherent and accurate\nreasoning branch. Our experimental results on mathematical word problems and\ncalculation questions show that this branching strategy boosts the reasoning\ncapabilities of small LLMs up to 4.6% compared to conventional argmax decoding.",
      "tldr_zh": "该研究发现，大语言模型(LLMs)在数学推理过程中，错误往往发生在高熵和熵方差大的token上。针对此问题，提出一种Entropy-Aware Branching方法，通过动态分支生成过程，在关键决策点从高概率token并行探索多个推理路径，并利用外部反馈（如更大模型）来排名和选择最准确的分支。实验结果显示，该策略使小型LLMs在数学问题上的推理性能比传统argmax decoding提高了4.6%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21961v1",
      "published_date": "2025-03-27 20:18:22 UTC",
      "updated_date": "2025-03-27 20:18:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:34:50.525842"
    },
    {
      "arxiv_id": "2503.22764v1",
      "title": "Boosting Large Language Models with Mask Fine-Tuning",
      "title_zh": "通过掩码微调提升大语言模型",
      "authors": [
        "Mingyuan Zhang",
        "Yue Bai",
        "Huan Wang",
        "Yizhou Wang",
        "Qihua Dong",
        "Yun Fu"
      ],
      "abstract": "The model is usually kept integral in the mainstream large language model\n(LLM) fine-tuning protocols. No works have questioned whether maintaining the\nintegrity of the model is indispensable for performance. In this work, we\nintroduce Mask Fine-Tuning (MFT), a brand-new LLM fine-tuning paradigm to show\nthat properly breaking the integrity of the model can surprisingly lead to\nimproved performance. Specifically, MFT learns a set of binary masks supervised\nby the typical LLM fine-tuning objective. Extensive experiments show that MFT\ngains a consistent performance boost across various domains and backbones\n(e.g., 1.95%/1.88% average gain in coding with LLaMA2-7B/3.1-8B). Detailed\nprocedures are provided to study the proposed MFT from different hyperparameter\nperspectives for better insight. In particular, MFT naturally updates the\ncurrent LLM training protocol by deploying it on a complete well-trained model.\nThis study extends the functionality of mask learning from its conventional\nnetwork pruning context for model compression to a more general scope.",
      "tldr_zh": "本文质疑了主流 Large Language Models (LLMs) 微调协议中保持模型完整性的必要性，提出 Mask Fine-Tuning (MFT) 作为一种新范式，通过学习一组由 LLM 微调目标监督的二进制掩码来打破模型完整性，从而提升性能。实验结果显示，MFT 在各种领域和模型（如 LLaMA2-7B 和 Llama-3.1-8B）上实现了稳定提升，例如在编码任务中平均提升 1.95% 和 1.88%。研究还从不同超参数角度提供了详细分析，以加深对 MFT 的理解。该方法扩展了掩码学习的范围，从传统的网络修剪用于模型压缩，到更广泛的 LLM 训练应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22764v1",
      "published_date": "2025-03-27 20:17:57 UTC",
      "updated_date": "2025-03-27 20:17:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:35:04.035335"
    },
    {
      "arxiv_id": "2503.21943v2",
      "title": "Parametric Shadow Control for Portrait Generation in Text-to-Image Diffusion Models",
      "title_zh": "参数化阴影控制在文本到图像扩散模型中的肖像生成",
      "authors": [
        "Haoming Cai",
        "Tsung-Wei Huang",
        "Shiv Gehlot",
        "Brandon Y. Feng",
        "Sachin Shah",
        "Guan-Ming Su",
        "Christopher Metzler"
      ],
      "abstract": "Text-to-image diffusion models excel at generating diverse portraits, but\nlack intuitive shadow control. Existing editing approaches, as post-processing,\nstruggle to offer effective manipulation across diverse styles. Additionally,\nthese methods either rely on expensive real-world light-stage data collection\nor require extensive computational resources for training. To address these\nlimitations, we introduce Shadow Director, a method that extracts and\nmanipulates hidden shadow attributes within well-trained diffusion models. Our\napproach uses a small estimation network that requires only a few thousand\nsynthetic images and hours of training-no costly real-world light-stage data\nneeded. Shadow Director enables parametric and intuitive control over shadow\nshape, placement, and intensity during portrait generation while preserving\nartistic integrity and identity across diverse styles. Despite training only on\nsynthetic data built on real-world identities, it generalizes effectively to\ngenerated portraits with diverse styles, making it a more accessible and\nresource-friendly solution.",
      "tldr_zh": "本研究针对 Text-to-image diffusion models 在肖像生成中的阴影控制不足问题，提出 Shadow Director 方法，该方法从预训练模型中提取并操纵隐藏的阴影属性。Shadow Director 采用一个小型估计网络，仅需几千张合成图像和几小时训练，即可实现参数化控制阴影形状、位置和强度，同时保持艺术完整性与身份一致性。实验显示，该方法在多样风格的生成肖像上表现出色，提供了一种高效、资源友好的替代方案，无需昂贵的真实世界数据。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "ShadowDirector Arxiv Version. Fix the arxiv title text issue",
      "pdf_url": "http://arxiv.org/pdf/2503.21943v2",
      "published_date": "2025-03-27 19:42:52 UTC",
      "updated_date": "2025-04-07 04:57:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:35:15.555248"
    },
    {
      "arxiv_id": "2503.21937v1",
      "title": "Lobster: A GPU-Accelerated Framework for Neurosymbolic Programming",
      "title_zh": "Lobster：一种用于神经符号编程的GPU加速框架",
      "authors": [
        "Paul Biberstein",
        "Ziyang Li",
        "Joseph Devietti",
        "Mayur Naik"
      ],
      "abstract": "Neurosymbolic programs combine deep learning with symbolic reasoning to\nachieve better data efficiency, interpretability, and generalizability compared\nto standalone deep learning approaches. However, existing neurosymbolic\nlearning frameworks implement an uneasy marriage between a highly scalable,\nGPU-accelerated neural component with a slower symbolic component that runs on\nCPUs. We propose Lobster, a unified framework for harnessing GPUs in an\nend-to-end manner for neurosymbolic learning. Lobster maps a general\nneurosymbolic language based on Datalog to the GPU programming paradigm. This\nmapping is implemented via compilation to a new intermediate language called\nAPM. The extra abstraction provided by APM allows Lobster to be both flexible,\nsupporting discrete, probabilistic, and differentiable modes of reasoning on\nGPU hardware with a library of provenance semirings, and performant,\nimplementing new optimization passes. We demonstrate that Lobster programs can\nsolve interesting problems spanning the domains of natural language processing,\nimage processing, program reasoning, bioinformatics, and planning. On a suite\nof 8 applications, Lobster achieves an average speedup of 5.3x over Scallop, a\nstate-of-the-art neurosymbolic framework, and enables scaling of neurosymbolic\nsolutions to previously infeasible tasks.",
      "tldr_zh": "该研究提出Lobster框架，一种端到端的GPU加速框架，用于神经符号编程(neurosymbolic programming)，旨在解决现有框架中神经组件高效而符号组件缓慢的矛盾问题。Lobster通过将基于Datalog的神经符号语言映射到GPU编程范式，并使用新的中间语言APM实现，支持离散、概率和可微推理模式，同时引入优化传递以提升性能。在8个应用场景中，包括自然语言处理、图像处理和生物信息学等领域，Lobster比现有框架Scallop平均提速5.3倍，并使一些原本不可行的任务成为可能。",
      "categories": [
        "cs.PL",
        "cs.AI",
        "cs.DC",
        "cs.LG"
      ],
      "primary_category": "cs.PL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21937v1",
      "published_date": "2025-03-27 19:32:58 UTC",
      "updated_date": "2025-03-27 19:32:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:35:27.268143"
    },
    {
      "arxiv_id": "2503.21928v1",
      "title": "An Efficient Training Algorithm for Models with Block-wise Sparsity",
      "title_zh": "一种高效的训练算法，用于具有块级稀疏的模型",
      "authors": [
        "Ding Zhu",
        "Zhiqun Zuo",
        "Mohammad Mahdi Khalili"
      ],
      "abstract": "Large-scale machine learning (ML) models are increasingly being used in\ncritical domains like education, lending, recruitment, healthcare, criminal\njustice, etc. However, the training, deployment, and utilization of these\nmodels demand substantial computational resources. To decrease computation and\nmemory costs, machine learning models with sparse weight matrices are widely\nused in the literature. Among sparse models, those with special sparse\nstructures (e.g., models with block-wise sparse weight matrices) fit better\nwith the hardware accelerators and can decrease the memory and computation\ncosts during the inference. Unfortunately, while there are several efficient\ntraining methods, none of them are designed to train a block-wise sparse model\nefficiently. As a result, the current methods for training block-wise sparse\nmodels start with full and dense models leading to inefficient training. In\nthis work, we focus on training models with \\textit{block-wise sparse matrices}\nand propose an efficient training algorithm to decrease both computation and\nmemory costs during training and inference. In addition, we will show that our\nproposed method enables us to efficiently find the right block size for the\nsparsity pattern during the training process. Our extensive empirical and\ntheoretical analyses show that our algorithms can decrease the computation and\nmemory costs significantly without a performance drop compared to baselines.",
      "tldr_zh": "该论文针对大型机器学习模型的训练问题，提出了一种高效算法，用于训练具有块状稀疏（block-wise sparsity）矩阵的模型，以减少训练和推理过程中的计算及内存成本。不同于现有方法，该算法从一开始就设计为块状稀疏结构，并允许在训练过程中优化块大小，从而避免了从密集模型起步的低效性。通过广泛的实证和理论分析，研究表明，该方法在不降低模型性能的情况下，能显著降低资源消耗。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "24 pages, submitted on Transactions on Machine Learning Research",
      "pdf_url": "http://arxiv.org/pdf/2503.21928v1",
      "published_date": "2025-03-27 19:14:27 UTC",
      "updated_date": "2025-03-27 19:14:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:35:38.907201"
    },
    {
      "arxiv_id": "2503.21911v1",
      "title": "AutoPsyC: Automatic Recognition of Psychodynamic Conflicts from Semi-structured Interviews with Large Language Models",
      "title_zh": "AutoPsyC：利用大型语言模型从半结构化访谈自动识别心理动力冲突",
      "authors": [
        "Sayed Muddashir Hossain",
        "Simon Ostermann",
        "Patrick Gebhard",
        "Cord Benecke",
        "Josef van Genabith",
        "Philipp Müller"
      ],
      "abstract": "Psychodynamic conflicts are persistent, often unconscious themes that shape a\nperson's behaviour and experiences. Accurate diagnosis of psychodynamic\nconflicts is crucial for effective patient treatment and is commonly done via\nlong, manually scored semi-structured interviews. Existing automated solutions\nfor psychiatric diagnosis tend to focus on the recognition of broad disorder\ncategories such as depression, and it is unclear to what extent psychodynamic\nconflicts which even the patient themselves may not have conscious access to\ncould be automatically recognised from conversation. In this paper, we propose\nAutoPsyC, the first method for recognising the presence and significance of\npsychodynamic conflicts from full-length Operationalized Psychodynamic\nDiagnostics (OPD) interviews using Large Language Models (LLMs). Our approach\ncombines recent advances in parameter-efficient fine-tuning and\nRetrieval-Augmented Generation (RAG) with a summarisation strategy to\neffectively process entire 90 minute long conversations. In evaluations on a\ndataset of 141 diagnostic interviews we show that AutoPsyC consistently\noutperforms all baselines and ablation conditions on the recognition of four\nhighly relevant psychodynamic conflicts.",
      "tldr_zh": "该研究提出 AutoPsyC，这是一个首创的方法，使用 Large Language Models (LLMs) 从完整的 Operationalized Psychodynamic Diagnostics (OPD) 半结构化访谈中自动识别 psychodynamic conflicts，这些冲突往往是无意识的主题，影响个人行为。AutoPsyC 结合参数高效微调、Retrieval-Augmented Generation (RAG) 和总结策略，有效处理长达90分钟的对话。实验结果显示，在一个包含141个诊断访谈的数据集上，AutoPsyC 在识别四个高度相关的 psychodynamic conflicts 方面，显著优于所有基线和消融条件。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21911v1",
      "published_date": "2025-03-27 18:41:35 UTC",
      "updated_date": "2025-03-27 18:41:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:35:51.864314"
    },
    {
      "arxiv_id": "2503.21910v1",
      "title": "JEEM: Vision-Language Understanding in Four Arabic Dialects",
      "title_zh": "翻译失败",
      "authors": [
        "Karima Kadaoui",
        "Hanin Atwany",
        "Hamdan Al-Ali",
        "Abdelrahman Mohamed",
        "Ali Mekky",
        "Sergei Tilga",
        "Natalia Fedorova",
        "Ekaterina Artemova",
        "Hanan Aldarmaki",
        "Yova Kementchedjhieva"
      ],
      "abstract": "We introduce JEEM, a benchmark designed to evaluate Vision-Language Models\n(VLMs) on visual understanding across four Arabic-speaking countries: Jordan,\nThe Emirates, Egypt, and Morocco. JEEM includes the tasks of image captioning\nand visual question answering, and features culturally rich and regionally\ndiverse content. This dataset aims to assess the ability of VLMs to generalize\nacross dialects and accurately interpret cultural elements in visual contexts.\nIn an evaluation of five prominent open-source Arabic VLMs and GPT-4V, we find\nthat the Arabic VLMs consistently underperform, struggling with both visual\nunderstanding and dialect-specific generation. While GPT-4V ranks best in this\ncomparison, the model's linguistic competence varies across dialects, and its\nvisual understanding capabilities lag behind. This underscores the need for\nmore inclusive models and the value of culturally-diverse evaluation paradigms.",
      "tldr_zh": "本文介绍了 JEEM 基准，用于评估 Vision-Language Models (VLMs) 在约旦、阿联酋、埃及和摩洛哥四个阿拉伯方言中的视觉理解能力，包括图像描述和视觉问答任务，并包含文化丰富且区域多样的内容。评估结果显示，五种开源阿拉伯 VLMs 在视觉理解和方言生成上表现不佳，而 GPT-4V 虽排名最佳，但其语言能力和视觉理解在不同方言间存在变异。研究强调了开发更具包容性的模型以及采用文化多样化评估范式的必要性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21910v1",
      "published_date": "2025-03-27 18:41:21 UTC",
      "updated_date": "2025-03-27 18:41:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:36:03.911049"
    },
    {
      "arxiv_id": "2503.22762v1",
      "title": "The Cost of Local and Global Fairness in Federated Learning",
      "title_zh": "联邦学习中局部和全局公平性的成本",
      "authors": [
        "Yuying Duan",
        "Gelei Xu",
        "Yiyu Shi",
        "Michael Lemmon"
      ],
      "abstract": "With the emerging application of Federated Learning (FL) in finance, hiring\nand healthcare, FL models are regulated to be fair, preventing disparities with\nrespect to legally protected attributes such as race or gender. Two concepts of\nfairness are important in FL: global and local fairness. Global fairness\naddresses the disparity across the entire population and local fairness is\nconcerned with the disparity within each client. Prior fair FL frameworks have\nimproved either global or local fairness without considering both. Furthermore,\nwhile the majority of studies on fair FL focuses on binary settings, many\nreal-world applications are multi-class problems. This paper proposes a\nframework that investigates the minimum accuracy lost for enforcing a specified\nlevel of global and local fairness in multi-class FL settings. Our framework\nleads to a simple post-processing algorithm that derives fair outcome\npredictors from the Bayesian optimal score functions. Experimental results show\nthat our algorithm outperforms the current state of the art (SOTA) with regard\nto the accuracy-fairness tradoffs, computational and communication costs. Codes\nare available at:\nhttps://github.com/papersubmission678/The-cost-of-local-and-global-fairness-in-FL .",
      "tldr_zh": "本研究探讨了在联邦学习（Federated Learning, FL）中实现全局公平（global fairness）和局部公平（local fairness）的成本，针对多类问题场景，旨在最小化准确性损失。论文提出一个框架，通过一个简单的后处理算法从Bayesian optimal score functions中导出公平结果预测器，从而同时优化全球和局部公平。实验结果显示，该算法在准确性-公平性权衡、计算和通信成本上优于现有最先进（SOTA）方法，为FL在金融、招聘和医疗等领域的公平应用提供了有效解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22762v1",
      "published_date": "2025-03-27 18:37:54 UTC",
      "updated_date": "2025-03-27 18:37:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:36:15.824010"
    },
    {
      "arxiv_id": "2503.21902v1",
      "title": "OntoAligner: A Comprehensive Modular and Robust Python Toolkit for Ontology Alignment",
      "title_zh": "OntoAligner：一个全面、模块化且稳健的 Python 工具包，用于本体对齐",
      "authors": [
        "Hamed Babaei Giglou",
        "Jennifer D'Souza",
        "Oliver Karras",
        "Sören Auer"
      ],
      "abstract": "Ontology Alignment (OA) is fundamental for achieving semantic\ninteroperability across diverse knowledge systems. We present OntoAligner, a\ncomprehensive, modular, and robust Python toolkit for ontology alignment,\ndesigned to address current limitations with existing tools faced by\npractitioners. Existing tools are limited in scalability, modularity, and ease\nof integration with recent AI advances. OntoAligner provides a flexible\narchitecture integrating existing lightweight OA techniques such as fuzzy\nmatching but goes beyond by supporting contemporary methods with\nretrieval-augmented generation and large language models for OA. The framework\nprioritizes extensibility, enabling researchers to integrate custom alignment\nalgorithms and datasets. This paper details the design principles,\narchitecture, and implementation of the OntoAligner, demonstrating its utility\nthrough benchmarks on standard OA tasks. Our evaluation highlights\nOntoAligner's ability to handle large-scale ontologies efficiently with few\nlines of code while delivering high alignment quality. By making OntoAligner\nopen-source, we aim to provide a resource that fosters innovation and\ncollaboration within the OA community, empowering researchers and practitioners\nwith a toolkit for reproducible OA research and real-world applications.",
      "tldr_zh": "该研究引入了OntoAligner，一种全面、模块化和鲁棒的Python工具包，用于本体对齐(OntoAligner)，旨在解决现有工具在可扩展性、模块性和与AI技术整合方面的局限性。该工具包整合了模糊匹配(fuzzy matching)等传统方法，并支持现代技术如检索增强生成(retrieval-augmented generation)和大型语言模型(large language models)，并强调可扩展性以便研究者集成自定义算法和数据集。通过基准测试，OntoAligner展示了在处理大规模本体时的高效性和高质量对齐性能，并作为开源资源促进本体对齐社区的创新与合作。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages, 3 figures. Accepted for the ESWC 2025 Resource Track",
      "pdf_url": "http://arxiv.org/pdf/2503.21902v1",
      "published_date": "2025-03-27 18:28:11 UTC",
      "updated_date": "2025-03-27 18:28:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:36:28.207449"
    },
    {
      "arxiv_id": "2503.21893v1",
      "title": "Exponentially Weighted Instance-Aware Repeat Factor Sampling for Long-Tailed Object Detection Model Training in Unmanned Aerial Vehicles Surveillance Scenarios",
      "title_zh": "翻译失败",
      "authors": [
        "Taufiq Ahmed",
        "Abhishek Kumar",
        "Constantino Álvarez Casado",
        "Anlan Zhang",
        "Tuomo Hänninen",
        "Lauri Loven",
        "Miguel Bordallo López",
        "Sasu Tarkoma"
      ],
      "abstract": "Object detection models often struggle with class imbalance, where rare\ncategories appear significantly less frequently than common ones. Existing\nsampling-based rebalancing strategies, such as Repeat Factor Sampling (RFS) and\nInstance-Aware Repeat Factor Sampling (IRFS), mitigate this issue by adjusting\nsample frequencies based on image and instance counts. However, these methods\nare based on linear adjustments, which limit their effectiveness in long-tailed\ndistributions. This work introduces Exponentially Weighted Instance-Aware\nRepeat Factor Sampling (E-IRFS), an extension of IRFS that applies exponential\nscaling to better differentiate between rare and frequent classes. E-IRFS\nadjusts sampling probabilities using an exponential function applied to the\ngeometric mean of image and instance frequencies, ensuring a more adaptive\nrebalancing strategy. We evaluate E-IRFS on a dataset derived from the\nFireman-UAV-RGBT Dataset and four additional public datasets, using YOLOv11\nobject detection models to identify fire, smoke, people and lakes in emergency\nscenarios. The results show that E-IRFS improves detection performance by 22\\%\nover the baseline and outperforms RFS and IRFS, particularly for rare\ncategories. The analysis also highlights that E-IRFS has a stronger effect on\nlightweight models with limited capacity, as these models rely more on data\nsampling strategies to address class imbalance. The findings demonstrate that\nE-IRFS improves rare object detection in resource-constrained environments,\nmaking it a suitable solution for real-time applications such as UAV-based\nemergency monitoring.",
      "tldr_zh": "该论文针对物体检测模型在长尾分布类别不平衡问题上存在的挑战，提出了一种新的采样策略Exponentially Weighted Instance-Aware Repeat Factor Sampling (E-IRFS)。E-IRFS在原有Instance-Aware Repeat Factor Sampling (IRFS)基础上引入指数缩放，基于图像和实例频率的几何均值调整采样概率，实现更自适应的再平衡。实验在Fireman-UAV-RGBT数据集和四个公共数据集上使用YOLOv11模型进行评估，结果显示E-IRFS比基线方法提高了22%的检测性能，尤其在稀有类别（如火灾、烟雾）上优于RFS和IRFS，并特别适用于资源受限的轻量级模型和UAV紧急监控场景。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 2 figures, 9 tables, 6 formulas, conference paper",
      "pdf_url": "http://arxiv.org/pdf/2503.21893v1",
      "published_date": "2025-03-27 18:09:37 UTC",
      "updated_date": "2025-03-27 18:09:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:36:40.193994"
    },
    {
      "arxiv_id": "2503.21889v1",
      "title": "StarFlow: Generating Structured Workflow Outputs From Sketch Images",
      "title_zh": "StarFlow: 从草图图像生成",
      "authors": [
        "Patrice Bechard",
        "Chao Wang",
        "Amirhossein Abaskohi",
        "Juan Rodriguez",
        "Christopher Pal",
        "David Vazquez",
        "Spandana Gella",
        "Sai Rajeswar",
        "Perouz Taslakian"
      ],
      "abstract": "Workflows are a fundamental component of automation in enterprise platforms,\nenabling the orchestration of tasks, data processing, and system integrations.\nDespite being widely used, building workflows can be complex, often requiring\nmanual configuration through low-code platforms or visual programming tools. To\nsimplify this process, we explore the use of generative foundation models,\nparticularly vision-language models (VLMs), to automatically generate\nstructured workflows from visual inputs. Translating hand-drawn sketches or\ncomputer-generated diagrams into executable workflows is challenging due to the\nambiguity of free-form drawings, variations in diagram styles, and the\ndifficulty of inferring execution logic from visual elements. To address this,\nwe introduce StarFlow, a framework for generating structured workflow outputs\nfrom sketches using vision-language models. We curate a diverse dataset of\nworkflow diagrams -- including synthetic, manually annotated, and real-world\nsamples -- to enable robust training and evaluation. We finetune and benchmark\nmultiple vision-language models, conducting a series of ablation studies to\nanalyze the strengths and limitations of our approach. Our results show that\nfinetuning significantly enhances structured workflow generation, outperforming\nlarge vision-language models on this task.",
      "tldr_zh": "该研究探讨了使用视觉语言模型 (VLMs) 从手绘草图自动生成结构化工作流输出，以简化企业自动化中的工作流构建过程。StarFlow 框架应运而生，通过处理草图的模糊性和执行逻辑推断难题，结合一个多样化数据集（包括合成、手动标注和真实样本）来训练和评估模型。研究通过微调多个 VLMs 并进行消融研究，发现微调显著提升了生成性能，使其优于大型基线模型。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21889v1",
      "published_date": "2025-03-27 18:04:05 UTC",
      "updated_date": "2025-03-27 18:04:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:36:50.732373"
    },
    {
      "arxiv_id": "2503.21888v1",
      "title": "RedditESS: A Mental Health Social Support Interaction Dataset -- Understanding Effective Social Support to Refine AI-Driven Support Tools",
      "title_zh": "翻译失败",
      "authors": [
        "Zeyad Alghamdi",
        "Tharindu Kumarage",
        "Garima Agrawal",
        "Mansooreh Karami",
        "Ibrahim Almuteb",
        "Huan Liu"
      ],
      "abstract": "Effective mental health support is crucial for alleviating psychological\ndistress. While large language model (LLM)-based assistants have shown promise\nin mental health interventions, existing research often defines \"effective\"\nsupport primarily in terms of empathetic acknowledgments, overlooking other\nessential dimensions such as informational guidance, community validation, and\ntangible coping strategies. To address this limitation and better understand\nwhat constitutes effective support, we introduce RedditESS, a novel real-world\ndataset derived from Reddit posts, including supportive comments and original\nposters' follow-up responses. Grounded in established social science theories,\nwe develop an ensemble labeling mechanism to annotate supportive comments as\neffective or not and perform qualitative assessments to ensure the reliability\nof the annotations. Additionally, we demonstrate the practical utility of\nRedditESS by using it to guide LLM alignment toward generating more\ncontext-sensitive and genuinely helpful supportive responses. By broadening the\nunderstanding of effective support, our study paves the way for advanced\nAI-driven mental health interventions.",
      "tldr_zh": "本研究介绍了RedditESS，一种基于Reddit帖子的心理健康社会支持互动数据集，旨在扩展对“有效支持”的理解，超越现有LLM（Large Language Model）研究中仅强调移情认可的局限，转而包括信息指导、社区验证和实际应对策略。研究团队基于社会科学理论，采用集成标注机制对支持性评论进行有效性标注，并通过定性评估确保标注可靠性。实验结果显示，RedditESS可用于指导LLM生成更上下文敏感和真正有帮助的响应，从而为先进的AI驱动心理健康干预铺平道路。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21888v1",
      "published_date": "2025-03-27 18:03:11 UTC",
      "updated_date": "2025-03-27 18:03:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:37:02.846013"
    },
    {
      "arxiv_id": "2503.21878v2",
      "title": "Is Best-of-N the Best of Them? Coverage, Scaling, and Optimality in Inference-Time Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Audrey Huang",
        "Adam Block",
        "Qinghua Liu",
        "Nan Jiang",
        "Akshay Krishnamurthy",
        "Dylan J. Foster"
      ],
      "abstract": "Inference-time computation offers a powerful axis for scaling the performance\nof language models. However, naively increasing computation in techniques like\nBest-of-N sampling can lead to performance degradation due to reward hacking.\nToward a theoretical understanding of how to best leverage additional\ncomputation, we focus on inference-time alignment, which we formalize as the\nproblem of improving the quality of responses drawn from a pre-trained policy,\ngiven a prompt of interest and access to an imperfect reward model. We analyze\nthe performance of inference-time alignment algorithms in terms of (i) response\nquality, and (ii) compute, and provide new results that highlight the\nimportance of the pre-trained policy's coverage over high-quality responses for\nperformance and compute scaling:\n  1. We show that Best-of-$N$ alignment with an ideal choice for $N$ can\nachieve optimal performance under stringent notions of coverage, but provably\nsuffers from reward hacking when $N$ is large, and fails to achieve tight\nguarantees under more realistic coverage conditions.\n  2. We introduce $\\texttt{InferenceTimePessimism}$, a new algorithm which\nmitigates reward hacking through deliberate use of inference-time compute,\nimplementing the principle of pessimism in the face of uncertainty via\nrejection sampling; we prove that its performance is optimal and does not\ndegrade with $N$, meaning it is scaling-monotonic.\n  We complement our theoretical results with an experimental evaluation that\ndemonstrate the benefits of $\\texttt{InferenceTimePessimism}$ across a variety\nof tasks and models.",
      "tldr_zh": "这篇论文探讨了推理时对齐（inference-time alignment）在语言模型中的应用，分析了 Best-of-N 采样等方法如何通过增加计算提升响应质量，但可能因 reward hacking 而导致性能下降。作者形式化了该问题，强调预训练策略对高质响应的覆盖率（coverage）对性能和计算扩展的重要性，并证明 Best-of-N 在理想覆盖条件下可达最优，但在大 N 值时会失效。论文引入了新算法 InferenceTimePessimism，通过拒绝采样实现 pessimism in the face of uncertainty，证明其性能最优且随计算增加而单调提升。最后，实验在多种任务和模型上验证了该算法的优势。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21878v2",
      "published_date": "2025-03-27 18:00:08 UTC",
      "updated_date": "2025-04-07 17:44:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:37:16.253381"
    },
    {
      "arxiv_id": "2503.21775v1",
      "title": "StyleMotif: Multi-Modal Motion Stylization using Style-Content Cross Fusion",
      "title_zh": "StyleMotif：使用风格-内容交叉融合的多模态运动风格化",
      "authors": [
        "Ziyu Guo",
        "Young Yoon Lee",
        "Joseph Liu",
        "Yizhak Ben-Shabat",
        "Victor Zordan",
        "Mubbasir Kapadia"
      ],
      "abstract": "We present StyleMotif, a novel Stylized Motion Latent Diffusion model,\ngenerating motion conditioned on both content and style from multiple\nmodalities. Unlike existing approaches that either focus on generating diverse\nmotion content or transferring style from sequences, StyleMotif seamlessly\nsynthesizes motion across a wide range of content while incorporating stylistic\ncues from multi-modal inputs, including motion, text, image, video, and audio.\nTo achieve this, we introduce a style-content cross fusion mechanism and align\na style encoder with a pre-trained multi-modal model, ensuring that the\ngenerated motion accurately captures the reference style while preserving\nrealism. Extensive experiments demonstrate that our framework surpasses\nexisting methods in stylized motion generation and exhibits emergent\ncapabilities for multi-modal motion stylization, enabling more nuanced motion\nsynthesis. Source code and pre-trained models will be released upon acceptance.\nProject Page: https://stylemotif.github.io",
      "tldr_zh": "我们提出了 StyleMotif，一种新型的 Stylized Motion Latent Diffusion 模型，能够基于多种模态输入（如动作、文本、图像、视频和音频）生成带有特定风格的动作，同时保持内容多样性。核心创新是引入 style-content cross fusion 机制，并将风格编码器与预训练的多模态模型对齐，确保生成的动作准确捕捉参考风格并保持真实性。实验结果显示，该框架在风格化动作生成上超越现有方法，并展现出多模态动作合成的细致能力，为更高级的动作合成提供了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.GR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://stylemotif.github.io",
      "pdf_url": "http://arxiv.org/pdf/2503.21775v1",
      "published_date": "2025-03-27 17:59:46 UTC",
      "updated_date": "2025-03-27 17:59:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:37:28.140318"
    },
    {
      "arxiv_id": "2503.21766v1",
      "title": "Stable-SCore: A Stable Registration-based Framework for 3D Shape Correspondence",
      "title_zh": "Stable-SCore：一种用于3D形状对应的稳定注册框架",
      "authors": [
        "Haolin Liu",
        "Xiaohang Zhan",
        "Zizheng Yan",
        "Zhongjin Luo",
        "Yuxin Wen",
        "Xiaoguang Han"
      ],
      "abstract": "Establishing character shape correspondence is a critical and fundamental\ntask in computer vision and graphics, with diverse applications including\nre-topology, attribute transfer, and shape interpolation. Current dominant\nfunctional map methods, while effective in controlled scenarios, struggle in\nreal situations with more complex challenges such as non-isometric shape\ndiscrepancies. In response, we revisit registration-for-correspondence methods\nand tap their potential for more stable shape correspondence estimation. To\novercome their common issues including unstable deformations and the necessity\nfor careful pre-alignment or high-quality initial 3D correspondences, we\nintroduce Stable-SCore: A Stable Registration-based Framework for 3D Shape\nCorrespondence. We first re-purpose a foundation model for 2D character\ncorrespondence that ensures reliable and stable 2D mappings. Crucially, we\npropose a novel Semantic Flow Guided Registration approach that leverages 2D\ncorrespondence to guide mesh deformations. Our framework significantly\nsurpasses existing methods in challenging scenarios, and brings possibilities\nfor a wide array of real applications, as demonstrated in our results.",
      "tldr_zh": "该论文针对3D形状对应领域的挑战，特别是现有functional map methods在处理非等距形状差异时的不稳定性，提出了一种基于注册的框架Stable-SCore。框架首先利用一个基础模型确保可靠的2D字符对应，然后引入Semantic Flow Guided Registration方法，通过2D对应来指导3D网格变形，从而实现更稳定的对应估计。实验结果显示，Stable-SCore在复杂场景中显著超越现有方法，并为实际应用如重拓扑、属性转移和形状插值提供了新可能性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR 2025. Homepage:\n  https://haolinliu97.github.io/Stable-Score/",
      "pdf_url": "http://arxiv.org/pdf/2503.21766v1",
      "published_date": "2025-03-27 17:59:02 UTC",
      "updated_date": "2025-03-27 17:59:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:37:39.963771"
    },
    {
      "arxiv_id": "2504.13865v1",
      "title": "A Survey on (M)LLM-Based GUI Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Fei Tang",
        "Haolei Xu",
        "Hang Zhang",
        "Siqi Chen",
        "Xingyu Wu",
        "Yongliang Shen",
        "Wenqi Zhang",
        "Guiyang Hou",
        "Zeqi Tan",
        "Yuchen Yan",
        "Kaitao Song",
        "Jian Shao",
        "Weiming Lu",
        "Jun Xiao",
        "Yueting Zhuang"
      ],
      "abstract": "Graphical User Interface (GUI) Agents have emerged as a transformative\nparadigm in human-computer interaction, evolving from rule-based automation\nscripts to sophisticated AI-driven systems capable of understanding and\nexecuting complex interface operations. This survey provides a comprehensive\nexamination of the rapidly advancing field of LLM-based GUI Agents,\nsystematically analyzing their architectural foundations, technical components,\nand evaluation methodologies. We identify and analyze four fundamental\ncomponents that constitute modern GUI Agents: (1) perception systems that\nintegrate text-based parsing with multimodal understanding for comprehensive\ninterface comprehension; (2) exploration mechanisms that construct and maintain\nknowledge bases through internal modeling, historical experience, and external\ninformation retrieval; (3) planning frameworks that leverage advanced reasoning\nmethodologies for task decomposition and execution; and (4) interaction systems\nthat manage action generation with robust safety controls. Through rigorous\nanalysis of these components, we reveal how recent advances in large language\nmodels and multimodal learning have revolutionized GUI automation across\ndesktop, mobile, and web platforms. We critically examine current evaluation\nframeworks, highlighting methodological limitations in existing benchmarks\nwhile proposing directions for standardization. This survey also identifies key\ntechnical challenges, including accurate element localization, effective\nknowledge retrieval, long-horizon planning, and safety-aware execution control,\nwhile outlining promising research directions for enhancing GUI Agents'\ncapabilities. Our systematic review provides researchers and practitioners with\na thorough understanding of the field's current state and offers insights into\nfuture developments in intelligent interface automation.",
      "tldr_zh": "本调查综述了基于大型语言模型(LLM)和多模态语言模型((M)LLM)的图形用户界面(GUI)代理的发展，从规则-based自动化脚本演变为先进的AI驱动系统。论文系统分析了GUI代理的四个核心组件：(1)感知systems，用于整合文本解析和多模态理解；(2)exploration mechanisms，通过内部建模、历史经验和外部检索构建知识库；(3)planning frameworks，利用高级推理方法进行任务分解和执行；以及(4)interaction systems，负责动作生成和安全控制。作者探讨了LLM和多模态学习如何革新桌面、移动和网络平台的GUI自动化，并批判性地审视了现有评估框架的局限性，同时提出标准化方向。最终，论文识别了关键挑战如元素定位、知识检索、长期规划和安全执行，并为提升GUI代理能力提供了未来研究方向。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13865v1",
      "published_date": "2025-03-27 17:58:31 UTC",
      "updated_date": "2025-03-27 17:58:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:37:51.976496"
    },
    {
      "arxiv_id": "2503.21761v1",
      "title": "Uni4D: Unifying Visual Foundation Models for 4D Modeling from a Single Video",
      "title_zh": "翻译失败",
      "authors": [
        "David Yifan Yao",
        "Albert J. Zhai",
        "Shenlong Wang"
      ],
      "abstract": "This paper presents a unified approach to understanding dynamic scenes from\ncasual videos. Large pretrained vision foundation models, such as\nvision-language, video depth prediction, motion tracking, and segmentation\nmodels, offer promising capabilities. However, training a single model for\ncomprehensive 4D understanding remains challenging. We introduce Uni4D, a\nmulti-stage optimization framework that harnesses multiple pretrained models to\nadvance dynamic 3D modeling, including static/dynamic reconstruction, camera\npose estimation, and dense 3D motion tracking. Our results show\nstate-of-the-art performance in dynamic 4D modeling with superior visual\nquality. Notably, Uni4D requires no retraining or fine-tuning, highlighting the\neffectiveness of repurposing visual foundation models for 4D understanding.",
      "tldr_zh": "本论文提出 Uni4D，一种统一视觉基础模型的框架，用于从单个视频实现动态 4D 建模。Uni4D 采用多阶段优化方法，整合预训练模型如视觉语言模型、视频深度预测、运动跟踪和分割模型，来处理静态/动态重建、相机位姿估计以及密集 3D 运动跟踪。实验结果显示，该框架在动态 4D 建模中达到了最先进性能，并提供了优越的视觉质量，而无需进行任何重新训练或微调。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025. Project page (with code):\n  https://davidyao99.github.io/uni4d",
      "pdf_url": "http://arxiv.org/pdf/2503.21761v1",
      "published_date": "2025-03-27 17:57:32 UTC",
      "updated_date": "2025-03-27 17:57:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:38:04.291639"
    },
    {
      "arxiv_id": "2503.21757v1",
      "title": "Fwd2Bot: LVLM Visual Token Compression with Double Forward Bottleneck",
      "title_zh": "翻译失败",
      "authors": [
        "Adrian Bulat",
        "Yassine Ouali",
        "Georgios Tzimiropoulos"
      ],
      "abstract": "In this work, we aim to compress the vision tokens of a Large Vision Language\nModel (LVLM) into a representation that is simultaneously suitable for (a)\ngenerative and (b) discriminative tasks, (c) is nearly lossless, and (d) is\nstorage-efficient. We propose a novel compression approach, called Fwd2Bot,\nthat uses the LVLM itself to compress the visual information in a task-agnostic\nmanner. At the core of Fwd2bot there exists a \"double-forward pass\" training\nstrategy, whereby, during the first forward pass, the LLM (of the LVLM) creates\na bottleneck by condensing the visual information into a small number of\nsummary tokens. Then, using the same LLM, the second forward pass processes the\nlanguage instruction(s) alongside the summary tokens, used as a direct\nreplacement for the image ones. The training signal is provided by two losses:\nan autoregressive one applied after the second pass that provides a direct\noptimization objective for compression, and a contrastive loss, applied after\nthe first pass, that further boosts the representation strength, especially for\ndiscriminative tasks. The training is further enhanced by stage-specific\nadapters. We accompany the proposed method by an in-depth ablation study.\nOverall, Fwd2Bot results in highly-informative compressed representations\nsuitable for both generative and discriminative tasks. For generative tasks, we\noffer a 2x higher compression rate without compromising the generative\ncapabilities, setting a new state-of-the-art result. For discriminative tasks,\nwe set a new state-of-the-art on image retrieval and compositionality.",
      "tldr_zh": "本文提出 Fwd2Bot，一种创新方法，用于压缩 Large Vision Language Model (LVLM) 的视觉标记，使其同时适用于生成任务和区分任务，同时实现几乎无损耗和存储高效的目标。核心机制是 double-forward pass 训练策略：第一 passes 通过 LLM 将视觉信息浓缩成少量摘要标记，第二 passes 则处理语言指令和这些标记作为图像标记的替代。训练过程结合 autoregressive loss 和 contrastive loss 进行优化，并通过 stage-specific adapters 进一步增强表示强度。实验结果显示，Fwd2Bot 在生成任务上实现了 2 倍压缩率而不牺牲性能，并在图像检索和组合性任务上设定了新状态-of-the-art 水平。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21757v1",
      "published_date": "2025-03-27 17:57:07 UTC",
      "updated_date": "2025-03-27 17:57:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:38:16.764905"
    },
    {
      "arxiv_id": "2503.21747v1",
      "title": "CTRL-O: Language-Controllable Object-Centric Visual Representation Learning",
      "title_zh": "CTRL-O：语言可控的对象中心视觉表示学习",
      "authors": [
        "Aniket Didolkar",
        "Andrii Zadaianchuk",
        "Rabiul Awal",
        "Maximilian Seitzer",
        "Efstratios Gavves",
        "Aishwarya Agrawal"
      ],
      "abstract": "Object-centric representation learning aims to decompose visual scenes into\nfixed-size vectors called \"slots\" or \"object files\", where each slot captures a\ndistinct object. Current state-of-the-art object-centric models have shown\nremarkable success in object discovery in diverse domains, including complex\nreal-world scenes. However, these models suffer from a key limitation: they\nlack controllability. Specifically, current object-centric models learn\nrepresentations based on their preconceived understanding of objects, without\nallowing user input to guide which objects are represented. Introducing\ncontrollability into object-centric models could unlock a range of useful\ncapabilities, such as the ability to extract instance-specific representations\nfrom a scene. In this work, we propose a novel approach for user-directed\ncontrol over slot representations by conditioning slots on language\ndescriptions. The proposed ConTRoLlable Object-centric representation learning\napproach, which we term CTRL-O, achieves targeted object-language binding in\ncomplex real-world scenes without requiring mask supervision. Next, we apply\nthese controllable slot representations on two downstream vision language\ntasks: text-to-image generation and visual question answering. The proposed\napproach enables instance-specific text-to-image generation and also achieves\nstrong performance on visual question answering.",
      "tldr_zh": "论文提出 CTRL-O 方法，通过语言描述来调节物体中心表示学习中的 \"slots\"，从而实现用户对视觉场景中特定物体表示的可控性。不同于传统模型，CTRL-O 能够在复杂真实场景中实现针对性的物体-语言绑定，而无需 mask supervision。这种方法被应用于下游任务，如文本到图像生成和视觉问答，实现了实例特定的图像生成，并在视觉问答任务上取得了强劲性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.21747v1",
      "published_date": "2025-03-27 17:53:50 UTC",
      "updated_date": "2025-03-27 17:53:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:38:28.025121"
    },
    {
      "arxiv_id": "2503.21735v1",
      "title": "GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release Analytics",
      "title_zh": "GateLens：一个增强推理能力的LLM代理，用于汽车软件发布分析",
      "authors": [
        "Arsham Gholamzadeh Khoee",
        "Shuai Wang",
        "Yinan Yu",
        "Robert Feldt",
        "Dhasarathy Parthasarathy"
      ],
      "abstract": "Ensuring the reliability and effectiveness of software release decisions is\ncritical, particularly in safety-critical domains like automotive systems.\nPrecise analysis of release validation data, often presented in tabular form,\nplays a pivotal role in this process. However, traditional methods that rely on\nmanual analysis of extensive test datasets and validation metrics are prone to\ndelays and high costs. Large Language Models (LLMs) offer a promising\nalternative but face challenges in analytical reasoning, contextual\nunderstanding, handling out-of-scope queries, and processing structured test\ndata consistently; limitations that hinder their direct application in\nsafety-critical scenarios. This paper introduces GateLens, an LLM-based tool\nfor analyzing tabular data in the automotive domain. GateLens translates\nnatural language queries into Relational Algebra (RA) expressions and then\ngenerates optimized Python code. It outperforms the baseline system on\nbenchmarking datasets, achieving higher F1 scores and handling complex and\nambiguous queries with greater robustness. Ablation studies confirm the\ncritical role of the RA module, with performance dropping sharply when omitted.\nIndustrial evaluations reveal that GateLens reduces analysis time by over 80%\nwhile maintaining high accuracy and reliability. As demonstrated by presented\nresults, GateLens achieved high performance without relying on few-shot\nexamples, showcasing strong generalization across various query types from\ndiverse company roles. Insights from deploying GateLens with a partner\nautomotive company offer practical guidance for integrating AI into critical\nworkflows such as release validation. Results show that by automating test\nresult analysis, GateLens enables faster, more informed, and dependable release\ndecisions, and can thus advance software scalability and reliability in\nautomotive systems.",
      "tldr_zh": "该研究针对汽车软件发布决策中的表格数据分析问题，提出GateLens——一个增强推理的LLM代理，以解决LLM在分析推理、上下文理解和处理结构化数据等方面的挑战。GateLens将自然语言查询转化为Relational Algebra (RA)表达式，并生成优化的Python代码，从而实现高效的查询处理。实验结果显示，GateLens在基准数据集上比基线系统获得更高F1 scores，并更robust地处理复杂查询；工业评估表明，它可将分析时间减少超过80%，提升软件的可扩展性和可靠性，同时提供AI集成关键工作流的实用指导。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21735v1",
      "published_date": "2025-03-27 17:48:32 UTC",
      "updated_date": "2025-03-27 17:48:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:38:40.067861"
    },
    {
      "arxiv_id": "2503.21729v3",
      "title": "ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation",
      "title_zh": "ReaRAG：知识引导推理通过迭代检索增强生成提升大型推理模型的事实性",
      "authors": [
        "Zhicheng Lee",
        "Shulin Cao",
        "Jinxin Liu",
        "Jiajie Zhang",
        "Weichuan Liu",
        "Xiaoyin Che",
        "Lei Hou",
        "Juanzi Li"
      ],
      "abstract": "Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely\nprimarily on parametric knowledge, limiting factual accuracy. While recent\nworks equip reinforcement learning (RL)-based LRMs with retrieval capabilities,\nthey suffer from overthinking and lack robustness in reasoning, reducing their\neffectiveness in question answering (QA) tasks. To address this, we propose\nReaRAG, a factuality-enhanced reasoning model that explores diverse queries\nwithout excessive iterations. Our solution includes a novel data construction\nframework with an upper bound on the reasoning chain length. Specifically, we\nfirst leverage an LRM to generate deliberate thinking, then select an action\nfrom a predefined action space (Search and Finish). For Search action, a query\nis executed against the RAG engine, where the result is returned as observation\nto guide reasoning steps later. This process iterates until a Finish action is\nchosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach\noutperforms existing baselines on multi-hop QA. Further analysis highlights its\nstrong reflective ability to recognize errors and refine its reasoning\ntrajectory. Our study enhances LRMs' factuality while effectively integrating\nrobust reasoning for Retrieval-Augmented Generation (RAG).",
      "tldr_zh": "该论文针对 Large Reasoning Models (LRMs) 的参数知识依赖问题，提出 ReaRAG 框架，以知识引导推理提升模型的事实准确性。ReaRAG 通过一个新颖的数据构建框架限制推理链长度，并采用迭代检索增强生成（Iterative Retrieval Augmented Generation），包括从预定义动作空间（Search 和 Finish）中选择动作，以执行查询并反馈观察结果，从而避免过度迭代。实验结果显示，ReaRAG 在多跳 QA 任务上超越现有基线，并展现出强大的错误识别和推理优化能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21729v3",
      "published_date": "2025-03-27 17:44:18 UTC",
      "updated_date": "2025-05-19 12:40:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:38:52.879142"
    },
    {
      "arxiv_id": "2503.21720v1",
      "title": "Collab: Controlled Decoding using Mixture of Agents for LLM Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Souradip Chakraborty",
        "Sujay Bhatt",
        "Udari Madhushani Sehwag",
        "Soumya Suvra Ghosal",
        "Jiahao Qiu",
        "Mengdi Wang",
        "Dinesh Manocha",
        "Furong Huang",
        "Alec Koppel",
        "Sumitra Ganesh"
      ],
      "abstract": "Alignment of Large Language models (LLMs) is crucial for safe and trustworthy\ndeployment in applications. Reinforcement learning from human feedback (RLHF)\nhas emerged as an effective technique to align LLMs to human preferences and\nbroader utilities, but it requires updating billions of model parameters, which\nis computationally expensive. Controlled Decoding, by contrast, provides a\nmechanism for aligning a model at inference time without retraining. However,\nsingle-agent decoding approaches often struggle to adapt to diverse tasks due\nto the complexity and variability inherent in these tasks. To strengthen the\ntest-time performance w.r.t the target task, we propose a mixture of\nagent-based decoding strategies leveraging the existing off-the-shelf aligned\nLLM policies. Treating each prior policy as an agent in the spirit of mixture\nof agent collaboration, we develop a decoding method that allows for\ninference-time alignment through a token-level selection strategy among\nmultiple agents. For each token, the most suitable LLM is dynamically chosen\nfrom a pool of models based on a long-term utility metric. This\npolicy-switching mechanism ensures optimal model selection at each step,\nenabling efficient collaboration and alignment among LLMs during decoding.\nTheoretical analysis of our proposed algorithm establishes optimal performance\nwith respect to the target task represented via a target reward for the given\noff-the-shelf models. We conduct comprehensive empirical evaluations with\nopen-source aligned models on diverse tasks and preferences, which demonstrates\nthe merits of this approach over single-agent decoding baselines. Notably,\nCollab surpasses the current SoTA decoding strategy, achieving an improvement\nof up to 1.56x in average reward and 71.89% in GPT-4 based win-tie rate.",
      "tldr_zh": "该论文提出 Collab，一种基于 mixture of agents 的控制解码策略，用于在推理时对 Large Language Models (LLMs) 进行对齐，以提升模型的安全性和可信度，而无需像 Reinforcement Learning from Human Feedback (RLHF) 那样进行昂贵的参数更新。方法将现有的对齐 LLM 策略视为多个智能体，通过 token-level 的动态选择机制，根据长效效用指标(long-term utility metric)实时挑选最合适的模型，实现高效的跨智能体协作。实验结果显示，Collab 在多样任务上超越单智能体解码基线，平均奖励提升高达 1.56 倍，GPT-4 基于的胜率提高 71.89%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.21720v1",
      "published_date": "2025-03-27 17:34:25 UTC",
      "updated_date": "2025-03-27 17:34:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:39:03.867213"
    },
    {
      "arxiv_id": "2503.21718v3",
      "title": "Outlier dimensions favor frequent tokens in language models",
      "title_zh": "翻译失败",
      "authors": [
        "Iuri Macocco",
        "Nora Graichen",
        "Gemma Boleda",
        "Marco Baroni"
      ],
      "abstract": "We study last-layer outlier dimensions, i.e. dimensions that display extreme\nactivations for the majority of inputs. We show that outlier dimensions arise\nin many different modern language models, and trace their function back to the\nheuristic of constantly predicting frequent words. We further show how a model\ncan block this heuristic when it is not contextually appropriate, by assigning\na counterbalancing weight mass to the remaining dimensions, and we investigate\nwhich model parameters boost outlier dimensions and when they arise during\ntraining. We conclude that outlier dimensions are a specialized mechanism\ndiscovered by many distinct models to implement a useful token prediction\nheuristic.",
      "tldr_zh": "本研究探讨了语言模型中的最后层异常维度（outlier dimensions），这些维度在大多数输入下显示极端激活，导致模型倾向于预测频繁出现的标记（frequent tokens）。研究发现，模型可以通过为剩余维度分配平衡权重来抑制这种启发式机制，当它在特定语境中不合适时。实验分析了影响outlier dimensions的参数及其在训练过程中的出现，并得出结论，这是一种许多不同模型独立发展的专门机制，用于实现有效的标记预测启发式。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.21718v3",
      "published_date": "2025-03-27 17:30:50 UTC",
      "updated_date": "2025-04-09 14:37:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:39:15.153170"
    },
    {
      "arxiv_id": "2503.21708v2",
      "title": "The Mathematical Relationship Between Layer Normalization and Dynamic Activation Functions",
      "title_zh": "翻译失败",
      "authors": [
        "Felix Stollenwerk"
      ],
      "abstract": "A recent paper proposes Dynamic Tanh (DyT) as a drop-in replacement for layer\nnormalization (LN). Although the method is empirically well-motivated and\nappealing from a practical point of view, it lacks a theoretical foundation. In\nthis work, we shed light on the mathematical relationship between layer\nnormalization and dynamic activation functions. In particular, we derive DyT\nfrom LN and show that a well-defined approximation is needed to do so. By\ndropping said approximation, an alternative activation function is obtained,\nwhich we call Dynamic Inverse Square Root Unit (DyISRU). DyISRU is the exact\ncounterpart of layer normalization, and we demonstrate numerically that it\nindeed resembles LN more accurately than DyT does.",
      "tldr_zh": "本论文探讨了Layer Normalization (LN) 与动态激活函数之间的数学关系，针对最近提出的Dynamic Tanh (DyT)作为LN的替代方案进行了理论分析。作者从LN出发推导DyT，但指出需要一个近似处理，并提出了一种精确对应物——Dynamic Inverse Square Root Unit (DyISRU)。通过数值实验，论文证明DyISRU比DyT更准确地模拟LN的表现，为激活函数的设计提供了更坚实的理论基础。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "New title, renamed DyISRU, added missing parentheses in proof of\n  theorem 3, minor language corrections",
      "pdf_url": "http://arxiv.org/pdf/2503.21708v2",
      "published_date": "2025-03-27 17:20:44 UTC",
      "updated_date": "2025-03-31 12:10:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:39:27.409898"
    },
    {
      "arxiv_id": "2504.00017v1",
      "title": "Enhance Vision-based Tactile Sensors via Dynamic Illumination and Image Fusion",
      "title_zh": "通过动态照明和图像融合增强基于视觉的触觉传感器",
      "authors": [
        "Artemii Redkin",
        "Zdravko Dugonjic",
        "Mike Lambeta",
        "Roberto Calandra"
      ],
      "abstract": "Vision-based tactile sensors use structured light to measure deformation in\ntheir elastomeric interface. Until now, vision-based tactile sensors such as\nDIGIT and GelSight have been using a single, static pattern of structured light\ntuned to the specific form factor of the sensor. In this work, we investigate\nthe effectiveness of dynamic illumination patterns, in conjunction with image\nfusion techniques, to improve the quality of sensing of vision-based tactile\nsensors. Specifically, we propose to capture multiple measurements, each with a\ndifferent illumination pattern, and then fuse them together to obtain a single,\nhigher-quality measurement. Experimental results demonstrate that this type of\ndynamic illumination yields significant improvements in image contrast,\nsharpness, and background difference. This discovery opens the possibility of\nretroactively improving the sensing quality of existing vision-based tactile\nsensors with a simple software update, and for new hardware designs capable of\nfully exploiting dynamic illumination.",
      "tldr_zh": "本研究提出了一种通过动态照明和图像融合技术来提升视觉-based 触觉传感器的性能方法，针对现有传感器如DIGIT和GelSight的单一静态结构化光模式进行改进。具体而言，该方法捕获多种不同照明模式的测量数据，并通过图像融合技术合成单一的高质量输出。实验结果显示，动态照明显著提高了图像对比度、锐度和背景差异，为现有视觉-based 触觉传感器的软件更新和新硬件设计提供了可行途径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.00017v1",
      "published_date": "2025-03-27 17:19:57 UTC",
      "updated_date": "2025-03-27 17:19:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:39:39.017860"
    },
    {
      "arxiv_id": "2503.21854v1",
      "title": "Foveated Instance Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Hongyi Zeng",
        "Wenxuan Liu",
        "Tianhua Xia",
        "Jinhui Chen",
        "Ziyun Li",
        "Sai Qian Zhang"
      ],
      "abstract": "Instance segmentation is essential for augmented reality and virtual reality\n(AR/VR) as it enables precise object recognition and interaction, enhancing the\nintegration of virtual and real-world elements for an immersive experience.\nHowever, the high computational overhead of segmentation limits its application\non resource-constrained AR/VR devices, causing large processing latency and\ndegrading user experience. In contrast to conventional scenarios, AR/VR users\ntypically focus on only a few regions within their field of view before\nshifting perspective, allowing segmentation to be concentrated on gaze-specific\nareas. This insight drives the need for efficient segmentation methods that\nprioritize processing instance of interest, reducing computational load and\nenhancing real-time performance. In this paper, we present a foveated instance\nsegmentation (FovealSeg) framework that leverages real-time user gaze data to\nperform instance segmentation exclusively on instance of interest, resulting in\nsubstantial computational savings. Evaluation results show that FSNet achieves\nan IoU of 0.56 on ADE20K and 0.54 on LVIS, notably outperforming the baseline.\nThe code is available at https://github.com/SAI-",
      "tldr_zh": "该论文探讨了实例分割在增强现实和虚拟现实(AR/VR)中的关键作用，但强调其高计算开销限制了在资源受限设备上的应用，导致延迟和用户体验下降。针对用户注视焦点有限的特性，研究提出Foveated Instance Segmentation框架（FovealSeg），利用实时用户注视数据，仅对感兴趣实例进行分割，从而显著减少计算负载并提升实时性能。实验结果显示，FSNet在ADE20K数据集上IoU达0.56，在LVIS上达0.54，均优于基线模型。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21854v1",
      "published_date": "2025-03-27 17:08:44 UTC",
      "updated_date": "2025-03-27 17:08:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:39:51.813215"
    },
    {
      "arxiv_id": "2503.21699v1",
      "title": "MAVERIX: Multimodal Audio-Visual Evaluation Reasoning IndeX",
      "title_zh": "MAVERIX：多模态音频-视觉评估推理指数",
      "authors": [
        "Liuyue Xie",
        "George Z. Wei",
        "Avik Kuthiala",
        "Ce Zheng",
        "Ananya Bal",
        "Mosam Dabhi",
        "Liting Wen",
        "Taru Rustagi",
        "Ethan Lai",
        "Sushil Khyalia",
        "Rohan Choudhury",
        "Morteza Ziyadi",
        "Xu Zhang",
        "Hao Yang",
        "László A. Jeni"
      ],
      "abstract": "Frontier models have either been language-only or have primarily focused on\nvision and language modalities. Although recent advancements in models with\nvision and audio understanding capabilities have shown substantial progress,\nthe field lacks a standardized evaluation framework for thoroughly assessing\ntheir cross-modality perception performance. We introduce MAVERIX~(Multimodal\nAudio-Visual Evaluation Reasoning IndeX), a novel benchmark with 700 videos and\n2,556 questions explicitly designed to evaluate multimodal models through tasks\nthat necessitate close integration of video and audio information. MAVERIX\nuniquely provides models with audiovisual tasks, closely mimicking the\nmultimodal perceptual experiences available to humans during inference and\ndecision-making processes. To our knowledge, MAVERIX is the first benchmark\naimed explicitly at assessing comprehensive audiovisual integration.\nExperiments with state-of-the-art models, including Gemini 1.5 Pro and o1, show\nperformance approaching human levels (around 70% accuracy), while human experts\nreach near-ceiling performance (95.1%). With standardized evaluation protocols,\na rigorously annotated pipeline, and a public toolkit, MAVERIX establishes a\nchallenging testbed for advancing audiovisual multimodal intelligence.",
      "tldr_zh": "该研究引入了 MAVERIX（Multimodal Audio-Visual Evaluation Reasoning IndeX），一个新型基准测试，包含700个视频和2,556个问题，旨在评估多模态模型在音频和视觉信息紧密整合方面的性能。MAVERIX 首次专注于全面评估音频视觉整合任务，模拟人类的多模态感知过程，并提供标准化评估协议、严格注释流程和公共工具包。实验结果显示，状态-of-the-art 模型如 Gemini 1.5 Pro 和 o1 达到约70%准确率，接近人类水平，而人类专家则高达95.1%，为推进音频视觉多模态智能提供了挑战性测试平台。",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CV",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.MM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21699v1",
      "published_date": "2025-03-27 17:04:33 UTC",
      "updated_date": "2025-03-27 17:04:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:40:03.384098"
    },
    {
      "arxiv_id": "2503.21695v1",
      "title": "AMA-SAM: Adversarial Multi-Domain Alignment of Segment Anything Model for High-Fidelity Histology Nuclei Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Jiahe Qian",
        "Yaoyu Fang",
        "Jinkui Hao",
        "Bo Zhou"
      ],
      "abstract": "Accurate segmentation of cell nuclei in histopathology images is essential\nfor numerous biomedical research and clinical applications. However, existing\ncell nucleus segmentation methods only consider a single dataset (i.e., primary\ndomain), while neglecting to leverage supplementary data from diverse sources\n(i.e., auxiliary domains) to reduce overfitting and enhance the performance.\nAlthough incorporating multiple datasets could alleviate overfitting, it often\nexacerbates performance drops caused by domain shifts. In this work, we\nintroduce Adversarial Multi-domain Alignment of Segment Anything Model\n(AMA-SAM) that extends the Segment Anything Model (SAM) to overcome these\nobstacles through two key innovations. First, we propose a Conditional Gradient\nReversal Layer (CGRL), a multi-domain alignment module that harmonizes features\nfrom diverse domains to promote domain-invariant representation learning while\npreserving crucial discriminative features for the primary dataset. Second, we\naddress SAM's inherent low-resolution output by designing a High-Resolution\nDecoder (HR-Decoder), which directly produces fine-grained segmentation maps in\norder to capture intricate nuclei boundaries in high-resolution histology\nimages. To the best of our knowledge, this is the first attempt to adapt SAM\nfor multi-dataset learning with application to histology nuclei segmentation.\nWe validate our method on several publicly available datasets, demonstrating\nconsistent and significant improvements over state-of-the-art approaches.",
      "tldr_zh": "本研究提出 AMA-SAM，一种基于 Adversarial Multi-Domain Alignment 的框架，用于扩展 Segment Anything Model (SAM) 以实现高保真组织病理图像细胞核分割。该方法通过引入 Conditional Gradient Reversal Layer (CGRL) 来协调多域特征，促进域不变表示学习，同时保留主数据集的关键判别特征；并设计 High-Resolution Decoder (HR-Decoder) 以直接生成细粒度分割地图，精确捕捉高分辨率图像中的细胞核边界。AMA-SAM 首次将 SAM 应用于多数据集学习，解决了现有方法因域移位和过拟合导致的性能下降问题。在多个公开数据集上实验验证，该框架比现有最先进方法实现了显著改进。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, 4 tables, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.21695v1",
      "published_date": "2025-03-27 16:59:39 UTC",
      "updated_date": "2025-03-27 16:59:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:40:16.483814"
    },
    {
      "arxiv_id": "2503.21694v1",
      "title": "Progressive Rendering Distillation: Adapting Stable Diffusion for Instant Text-to-Mesh Generation without 3D Data",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiyuan Ma",
        "Xinyue Liang",
        "Rongyuan Wu",
        "Xiangyu Zhu",
        "Zhen Lei",
        "Lei Zhang"
      ],
      "abstract": "It is highly desirable to obtain a model that can generate high-quality 3D\nmeshes from text prompts in just seconds. While recent attempts have adapted\npre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into\ngenerators of 3D representations (e.g., Triplane), they often suffer from poor\nquality due to the lack of sufficient high-quality 3D training data. Aiming at\novercoming the data shortage, we propose a novel training scheme, termed as\nProgressive Rendering Distillation (PRD), eliminating the need for 3D\nground-truths by distilling multi-view diffusion models and adapting SD into a\nnative 3D generator. In each iteration of training, PRD uses the U-Net to\nprogressively denoise the latent from random noise for a few steps, and in each\nstep it decodes the denoised latent into 3D output. Multi-view diffusion\nmodels, including MVDream and RichDreamer, are used in joint with SD to distill\ntext-consistent textures and geometries into the 3D outputs through score\ndistillation. Since PRD supports training without 3D ground-truths, we can\neasily scale up the training data and improve generation quality for\nchallenging text prompts with creative concepts. Meanwhile, PRD can accelerate\nthe inference speed of the generation model in just a few steps. With PRD, we\ntrain a Triplane generator, namely TriplaneTurbo, which adds only $2.5\\%$\ntrainable parameters to adapt SD for Triplane generation. TriplaneTurbo\noutperforms previous text-to-3D generators in both efficiency and quality.\nSpecifically, it can produce high-quality 3D meshes in 1.2 seconds and\ngeneralize well for challenging text input. The code is available at\nhttps://github.com/theEricMa/TriplaneTurbo.",
      "tldr_zh": "本文提出Progressive Rendering Distillation (PRD)方法，以适应Stable Diffusion模型，实现无需3D训练数据的即时文本到3D网格生成，解决现有方法因数据短缺导致的生成质量问题。PRD通过逐步去噪潜伏变量并结合多视图扩散模型（如MVDream和RichDreamer）进行分数蒸馏，确保生成的3D输出具有文本一致的纹理和几何。最终，训练出的TriplaneTurbo模型仅增加了2.5%的可训练参数，即可在1.2秒内产生高质量3D网格，并在效率和泛化能力上优于现有文本到3D生成器。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "Accepted to CVPR 2025.\n  Code:https://github.com/theEricMa/TriplaneTurbo.\n  Demo:https://huggingface.co/spaces/ZhiyuanthePony/TriplaneTurbo",
      "pdf_url": "http://arxiv.org/pdf/2503.21694v1",
      "published_date": "2025-03-27 16:59:15 UTC",
      "updated_date": "2025-03-27 16:59:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:40:28.595129"
    },
    {
      "arxiv_id": "2503.21683v1",
      "title": "LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Hui Wang"
      ],
      "abstract": "In recent years, large language models (LLMs) have shown significant\nadvancements in natural language processing (NLP), with strong capa-bilities in\ngeneration, comprehension, and rea-soning. These models have found applications\nin education, intelligent decision-making, and gaming. However, effectively\nutilizing LLMs for strategic planning and decision-making in the game of Gomoku\nremains a challenge. This study aims to develop a Gomoku AI system based on\nLLMs, simulating the human learning process of playing chess. The system is\nde-signed to understand and apply Gomoku strat-egies and logic to make rational\ndecisions. The research methods include enabling the model to \"read the board,\"\n\"understand the rules,\" \"select strategies,\" and \"evaluate positions,\" while\nen-hancing its abilities through self-play and rein-forcement learning. The\nresults demonstrate that this approach significantly improves the se-lection of\nmove positions, resolves the issue of generating illegal positions, and reduces\npro-cess time through parallel position evaluation. After extensive self-play\ntraining, the model's Gomoku-playing capabilities have been notably enhanced.",
      "tldr_zh": "该研究开发了LLM-Gomoku系统，利用大型语言模型(LLMs)来处理围棋(Gomoku)游戏的战略决策，模拟人类学习过程。该系统通过让模型“读棋盘”、“理解规则”、“选择策略”和“评估位置”等功能，并结合self-play和reinforcement learning进行训练，显著提升了决策能力。实验结果显示，该方法改善了走子选择准确性，解决了生成非法位置的问题，并通过并行评估减少了处理时间，最终使模型的Gomoku游戏水平得到显著增强。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21683v1",
      "published_date": "2025-03-27 16:52:25 UTC",
      "updated_date": "2025-03-27 16:52:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:40:39.153051"
    },
    {
      "arxiv_id": "2503.21677v1",
      "title": "A tale of two goals: leveraging sequentiality in multi-goal scenarios",
      "title_zh": "翻译失败",
      "authors": [
        "Olivier Serris",
        "Stéphane Doncieux",
        "Olivier Sigaud"
      ],
      "abstract": "Several hierarchical reinforcement learning methods leverage planning to\ncreate a graph or sequences of intermediate goals, guiding a lower-level\ngoal-conditioned (GC) policy to reach some final goals. The low-level policy is\ntypically conditioned on the current goal, with the aim of reaching it as\nquickly as possible. However, this approach can fail when an intermediate goal\ncan be reached in multiple ways, some of which may make it impossible to\ncontinue toward subsequent goals. To address this issue, we introduce two\ninstances of Markov Decision Process (MDP) where the optimization objective\nfavors policies that not only reach the current goal but also subsequent ones.\nIn the first, the agent is conditioned on both the current and final goals,\nwhile in the second, it is conditioned on the next two goals in the sequence.\nWe conduct a series of experiments on navigation and pole-balancing tasks in\nwhich sequences of intermediate goals are given. By evaluating policies trained\nwith TD3+HER on both the standard GC-MDP and our proposed MDPs, we show that,\nin most cases, conditioning on the next two goals improves stability and sample\nefficiency over other approaches.",
      "tldr_zh": "本研究探讨了多目标场景中利用目标序列性的问题，指出传统层次化强化学习方法中，低层目标条件 (GC) 策略仅关注当前目标可能导致后续目标无法达到。论文提出两种 Markov Decision Process (MDP) 变体：一种让代理同时考虑当前和最终目标，另一种让代理关注序列中的下一个两个目标，以优化策略的整体表现。在导航和杆平衡任务的实验中，使用 TD3+HER 算法训练的策略显示，条件于下一个两个目标的方法在大多数情况下提高了稳定性和样本效率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "68T40"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.21677v1",
      "published_date": "2025-03-27 16:47:46 UTC",
      "updated_date": "2025-03-27 16:47:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:40:52.141005"
    },
    {
      "arxiv_id": "2503.21848v1",
      "title": "Comparative Analysis of Image, Video, and Audio Classifiers for Automated News Video Segmentation",
      "title_zh": "图像、视频和音频分类器在自动新闻视频分割中的比较分析",
      "authors": [
        "Jonathan Attard",
        "Dylan Seychell"
      ],
      "abstract": "News videos require efficient content organisation and retrieval systems, but\ntheir unstructured nature poses significant challenges for automated\nprocessing. This paper presents a comprehensive comparative analysis of image,\nvideo, and audio classifiers for automated news video segmentation. This work\npresents the development and evaluation of multiple deep learning approaches,\nincluding ResNet, ViViT, AST, and multimodal architectures, to classify five\ndistinct segment types: advertisements, stories, studio scenes, transitions,\nand visualisations. Using a custom-annotated dataset of 41 news videos\ncomprising 1,832 scene clips, our experiments demonstrate that image-based\nclassifiers achieve superior performance (84.34\\% accuracy) compared to more\ncomplex temporal models. Notably, the ResNet architecture outperformed\nstate-of-the-art video classifiers while requiring significantly fewer\ncomputational resources. Binary classification models achieved high accuracy\nfor transitions (94.23\\%) and advertisements (92.74\\%). These findings advance\nthe understanding of effective architectures for news video segmentation and\nprovide practical insights for implementing automated content organisation\nsystems in media applications. These include media archiving, personalised\ncontent delivery, and intelligent video search.",
      "tldr_zh": "这篇论文对图像、视频和音频分类器在新闻视频自动分割中的性能进行了全面比较分析，使用ResNet、ViViT、AST等深度学习模型来分类广告、故事、演播室场景、过渡和可视化等五种段类型。实验基于一个自定义标注数据集，包含41个新闻视频和1,832个场景剪辑，结果显示图像-based分类器（如ResNet）取得了84.34%的最高准确率，显著优于复杂视频模型，且计算资源需求更低。ResNet在二元分类任务中表现突出，过渡和广告的准确率分别达到94.23%和92.74%。这些发现为媒体应用提供实际指导，包括媒体归档、个性化内容交付和智能视频搜索。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint for paper in CAI 2025, 7 pages, 5 tables, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.21848v1",
      "published_date": "2025-03-27 16:42:50 UTC",
      "updated_date": "2025-03-27 16:42:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:41:04.725989"
    },
    {
      "arxiv_id": "2503.21674v1",
      "title": "Intelligent IoT Attack Detection Design via ODLLM with Feature Ranking-based Knowledge Base",
      "title_zh": "翻译失败",
      "authors": [
        "Satvik Verma",
        "Qun Wang",
        "E. Wes Bethel"
      ],
      "abstract": "The widespread adoption of Internet of Things (IoT) devices has introduced\nsignificant cybersecurity challenges, particularly with the increasing\nfrequency and sophistication of Distributed Denial of Service (DDoS) attacks.\nTraditional machine learning (ML) techniques often fall short in detecting such\nattacks due to the complexity of blended and evolving patterns. To address\nthis, we propose a novel framework leveraging On-Device Large Language Models\n(ODLLMs) augmented with fine-tuning and knowledge base (KB) integration for\nintelligent IoT network attack detection. By implementing feature ranking\ntechniques and constructing both long and short KBs tailored to model\ncapacities, the proposed framework ensures efficient and accurate detection of\nDDoS attacks while overcoming computational and privacy limitations. Simulation\nresults demonstrate that the optimized framework achieves superior accuracy\nacross diverse attack types, especially when using compact models in edge\ncomputing environments. This work provides a scalable and secure solution for\nreal-time IoT security, advancing the applicability of edge intelligence in\ncybersecurity.",
      "tldr_zh": "该研究针对物联网 (IoT) 设备面临的分布式拒绝服务 (DDoS) 攻击等网络安全挑战，提出了一种基于 On-Device Large Language Models (ODLLMs) 的智能检测框架，通过特征排名技术构建长短知识库 (KB) 并进行微调，以提升攻击检测的效率和准确性。框架解决了传统机器学习 (ML) 技术在处理复杂攻击模式时的局限性，同时兼顾计算资源和隐私保护。模拟实验结果显示，该框架在各种攻击类型上实现了更高的准确率，尤其适用于边缘计算环境，提供了一个可扩展的实时 IoT 安全解决方案。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21674v1",
      "published_date": "2025-03-27 16:41:57 UTC",
      "updated_date": "2025-03-27 16:41:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:41:16.505313"
    },
    {
      "arxiv_id": "2503.21847v1",
      "title": "ReCoM: Realistic Co-Speech Motion Generation with Recurrent Embedded Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Yong Xie",
        "Yunlian Sun",
        "Hongwen Zhang",
        "Yebin Liu",
        "Jinhui Tang"
      ],
      "abstract": "We present ReCoM, an efficient framework for generating high-fidelity and\ngeneralizable human body motions synchronized with speech. The core innovation\nlies in the Recurrent Embedded Transformer (RET), which integrates Dynamic\nEmbedding Regularization (DER) into a Vision Transformer (ViT) core\narchitecture to explicitly model co-speech motion dynamics. This architecture\nenables joint spatial-temporal dependency modeling, thereby enhancing gesture\nnaturalness and fidelity through coherent motion synthesis. To enhance model\nrobustness, we incorporate the proposed DER strategy, which equips the model\nwith dual capabilities of noise resistance and cross-domain generalization,\nthereby improving the naturalness and fluency of zero-shot motion generation\nfor unseen speech inputs. To mitigate inherent limitations of autoregressive\ninference, including error accumulation and limited self-correction, we propose\nan iterative reconstruction inference (IRI) strategy. IRI refines motion\nsequences via cyclic pose reconstruction, driven by two key components: (1)\nclassifier-free guidance improves distribution alignment between generated and\nreal gestures without auxiliary supervision, and (2) a temporal smoothing\nprocess eliminates abrupt inter-frame transitions while ensuring kinematic\ncontinuity. Extensive experiments on benchmark datasets validate ReCoM's\neffectiveness, achieving state-of-the-art performance across metrics. Notably,\nit reduces the Fr\\'echet Gesture Distance (FGD) from 18.70 to 2.48,\ndemonstrating an 86.7% improvement in motion realism. Our project page is\nhttps://yong-xie-xy.github.io/ReCoM/.",
      "tldr_zh": "本文提出 ReCoM 框架，利用 Recurrent Embedded Transformer (RET) 生成与语音同步的高保真人类动作，RET 通过整合 Dynamic Embedding Regularization (DER) 到 Vision Transformer (ViT) 架构中，显式建模动作动态并提升空间-时间依赖建模。DER 策略增强模型的噪声抵抗和跨域泛化能力，而 iterative reconstruction inference (IRI) 策略通过循环姿态重建、无分类器指导和时间平滑来减少自回归推理中的错误积累，确保动作的自然性和连续性。实验在基准数据集上验证了 ReCoM 的有效性，Fréchet Gesture Distance (FGD) 从 18.70 降至 2.48，实现了 86.7% 的动作真实性改善，并达到最先进性能。",
      "categories": [
        "cs.GR",
        "cs.AI"
      ],
      "primary_category": "cs.GR",
      "comment": "8 pages, 6 figures, Project Page:\n  https://yong-xie-xy.github.io/ReCoM/",
      "pdf_url": "http://arxiv.org/pdf/2503.21847v1",
      "published_date": "2025-03-27 16:39:40 UTC",
      "updated_date": "2025-03-27 16:39:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:41:30.147168"
    },
    {
      "arxiv_id": "2503.21846v2",
      "title": "LightSNN: Lightweight Architecture Search for Sparse and Accurate Spiking Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Yesmine Abdennadher",
        "Giovanni Perin",
        "Riccardo Mazzieri",
        "Jacopo Pegoraro",
        "Michele Rossi"
      ],
      "abstract": "Spiking Neural Networks (SNNs) are highly regarded for their energy\nefficiency, inherent activation sparsity, and suitability for real-time\nprocessing in edge devices. However, most current SNN methods adopt\narchitectures resembling traditional artificial neural networks (ANNs), leading\nto suboptimal performance when applied to SNNs. While SNNs excel in energy\nefficiency, they have been associated with lower accuracy levels than\ntraditional ANNs when utilizing conventional architectures. In response, in\nthis work we present LightSNN, a rapid and efficient Neural Network\nArchitecture Search (NAS) technique specifically tailored for SNNs that\nautonomously leverages the most suitable architecture, striking a good balance\nbetween accuracy and efficiency by enforcing sparsity. Based on the spiking NAS\nnetwork (SNASNet) framework, a cell-based search space including backward\nconnections is utilized to build our training-free pruning-based NAS mechanism.\nOur technique assesses diverse spike activation patterns across different data\nsamples using a sparsity-aware Hamming distance fitness evaluation. Thorough\nexperiments are conducted on both static (CIFAR10 and CIFAR100) and\nneuromorphic datasets (DVS128-Gesture). Our LightSNN model achieves\nstate-of-the-art results on CIFAR10 and CIFAR100, improves performance on\nDVS128Gesture by 4.49\\%, and significantly reduces search time most notably\noffering a $98\\times$ speedup over SNASNet and running 30\\% faster than the\nbest existing method on DVS128Gesture. Code is available on Github at:\nhttps://github.com/YesmineAbdennadher/LightSNN.",
      "tldr_zh": "本文提出 LightSNN，一种轻量级神经网络架构搜索(NAS)技术，专门针对 Spiking Neural Networks(SNNs)设计，通过强制稀疏性和训练-free的修剪机制，优化架构以平衡准确性和能量效率。基于 SNASNet 框架，该方法利用单元-based搜索空间和稀疏感知的 Hamming distance 评估不同数据样本的 spike 激活模式。实验结果显示，LightSNN 在 CIFAR10 和 CIFAR100 数据集上达到 state-of-the-art 性能，在 DVS128-Gesture 上提升 4.49% 的准确率，并将搜索时间减少至比 SNASNet 快 98 倍，比最佳现有方法快 30%。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.NE",
      "comment": "Accepted to AMLDS 2025 (Tokyo, July 2025). 6 pages, 3 figures, 2\n  tables",
      "pdf_url": "http://arxiv.org/pdf/2503.21846v2",
      "published_date": "2025-03-27 16:38:13 UTC",
      "updated_date": "2025-05-12 13:38:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:41:42.145685"
    },
    {
      "arxiv_id": "2503.21670v1",
      "title": "COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in Hindi-English Code-Mixing",
      "title_zh": "翻译失败",
      "authors": [
        "Rajvee Sheth",
        "Himanshu Beniwal",
        "Mayank Singh"
      ],
      "abstract": "The rapid growth of digital communication has driven the widespread use of\ncode-mixing, particularly Hindi-English, in multilingual communities. Existing\ndatasets often focus on romanized text, have limited scope, or rely on\nsynthetic data, which fails to capture realworld language nuances. Human\nannotations are crucial for assessing the naturalness and acceptability of\ncode-mixed text. To address these challenges, We introduce COMI-LINGUA, the\nlargest manually annotated dataset for code-mixed text, comprising 100,970\ninstances evaluated by three expert annotators in both Devanagari and Roman\nscripts. The dataset supports five fundamental NLP tasks: Language\nIdentification, Matrix Language Identification, Part-of-Speech Tagging, Named\nEntity Recognition, and Translation. We evaluate LLMs on these tasks using\nCOMILINGUA, revealing limitations in current multilingual modeling strategies\nand emphasizing the need for improved code-mixed text processing capabilities.\nCOMI-LINGUA is publically availabe at:\nhttps://huggingface.co/datasets/LingoIITGN/COMI-LINGUA.",
      "tldr_zh": "该研究引入了 COMI-LINGUA，这是一个由专家手动标注的大型数据集，针对印地语-英语 code-mixing 文本的自然性和真实性问题，提供100,970个实例，支持Devanagari和Roman脚本。数据集涵盖五种核心NLP任务，包括Language Identification、Matrix Language Identification、Part-of-Speech Tagging、Named Entity Recognition和Translation。实验评估显示，大型语言模型（LLMs）在这些任务上存在局限性，突显了当前多语言建模策略的不足，并强调了改进code-mixed文本处理的需求。COMI-LINGUA已公开可用于Hugging Face平台。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21670v1",
      "published_date": "2025-03-27 16:36:39 UTC",
      "updated_date": "2025-03-27 16:36:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:41:52.830606"
    },
    {
      "arxiv_id": "2503.21668v2",
      "title": "Cognitive Science-Inspired Evaluation of Core Capabilities for Object Understanding in AI",
      "title_zh": "翻译失败",
      "authors": [
        "Danaja Rutar",
        "Alva Markelius",
        "Konstantinos Voudouris",
        "José Hernández-Orallo",
        "Lucy Cheke"
      ],
      "abstract": "One of the core components of our world models is 'intuitive physics' - an\nunderstanding of objects, space, and causality. This capability enables us to\npredict events, plan action and navigate environments, all of which rely on a\ncomposite sense of objecthood. Despite its importance, there is no single,\nunified account of objecthood, though multiple theoretical frameworks provide\ninsights. In the first part of this paper, we present a comprehensive overview\nof the main theoretical frameworks in objecthood research - Gestalt psychology,\nenactive cognition, and developmental psychology - and identify the core\ncapabilities each framework attributes to object understanding, as well as what\nfunctional roles they play in shaping world models in biological agents. Given\nthe foundational role of objecthood in world modelling, understanding\nobjecthood is also essential in AI. In the second part of the paper, we\nevaluate how current AI paradigms approach and test objecthood capabilities\ncompared to those in cognitive science. We define an AI paradigm as a\ncombination of how objecthood is conceptualised, the methods used for studying\nobjecthood, the data utilised, and the evaluation techniques. We find that,\nwhilst benchmarks can detect that AI systems model isolated aspects of\nobjecthood, the benchmarks cannot detect when AI systems lack functional\nintegration across these capabilities, not solving the objecthood challenge\nfully. Finally, we explore novel evaluation approaches that align with the\nintegrated vision of objecthood outlined in this paper. These methods are\npromising candidates for advancing from isolated object capabilities toward\ngeneral-purpose AI with genuine object understanding in real-world contexts.",
      "tldr_zh": "本论文从认知科学视角审视物体理解（object understanding）在AI中的核心能力，聚焦于“intuitive physics”对物体、空间和因果关系的认知。作者回顾了Gestalt psychology、enactive cognition和developmental psychology等理论框架，识别出这些框架中物体理解的关键能力和其在生物代理世界模型中的作用。论文评估当前AI范式（包括物体概念化、研究方法、数据和评估技术）与认知科学的差异，发现AI系统虽能处理孤立物体能力，但缺乏功能整合，无法全面解决物体理解挑战。最后，提出新型评估方法，以推动AI从单一能力向真实世界语境中的通用物体理解发展。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21668v2",
      "published_date": "2025-03-27 16:35:02 UTC",
      "updated_date": "2025-04-07 10:39:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:42:03.925413"
    },
    {
      "arxiv_id": "2503.21657v1",
      "title": "Model Assembly Learning with Heterogeneous Layer Weight Merging",
      "title_zh": "异构层权重合并的模型组装学习",
      "authors": [
        "Yi-Kai Zhang",
        "Jin Wang",
        "Xu-Xiang Zhong",
        "De-Chuan Zhan",
        "Han-Jia Ye"
      ],
      "abstract": "Model merging acquires general capabilities without extra data or training by\ncombining multiple models' parameters. Previous approaches achieve linear mode\nconnectivity by aligning parameters into the same loss basin using permutation\ninvariance. In this paper, we introduce Model Assembly Learning (MAL), a novel\nparadigm for model merging that iteratively integrates parameters from diverse\nmodels in an open-ended model zoo to enhance the base model's capabilities.\nUnlike previous works that require identical architectures, MAL allows the\nmerging of heterogeneous architectures and selective parameters across layers.\nSpecifically, the base model can incorporate parameters from different layers\nof multiple pre-trained models. We systematically investigate the conditions\nand fundamental settings of heterogeneous parameter merging, addressing all\npossible mismatches in layer widths between the base and target models.\nFurthermore, we establish key laws and provide practical guidelines for\neffectively implementing MAL.",
      "tldr_zh": "本文提出Model Assembly Learning (MAL)，一种新颖的模型合并范式，通过迭代整合来自开放模型库的异构模型参数，来增强基础模型的通用能力。与传统方法不同，MAL支持合并heterogeneous architectures和选择性层参数，并系统处理层宽度不匹配问题。作者对异构参数合并的条件进行深入调查，建立了关键定律并提供实用指南，以指导MAL的有效实施。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025 Workshop on Neural Network Weights as a New Data Modality",
      "pdf_url": "http://arxiv.org/pdf/2503.21657v1",
      "published_date": "2025-03-27 16:21:53 UTC",
      "updated_date": "2025-03-27 16:21:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:42:16.175274"
    },
    {
      "arxiv_id": "2503.21646v1",
      "title": "Unlocking the Potential of Past Research: Using Generative AI to Reconstruct Healthcare Simulation Models",
      "title_zh": "释放过去研究的潜力：使用生成式 AI 重建医疗保健模拟模型",
      "authors": [
        "Thomas Monks",
        "Alison Harper",
        "Amy Heather"
      ],
      "abstract": "Discrete-event simulation (DES) is widely used in healthcare Operations\nResearch, but the models themselves are rarely shared. This limits their\npotential for reuse and long-term impact in the modelling and healthcare\ncommunities. This study explores the feasibility of using generative artificial\nintelligence (AI) to recreate published models using Free and Open Source\nSoftware (FOSS), based on the descriptions provided in an academic journal.\nUsing a structured methodology, we successfully generated, tested and\ninternally reproduced two DES models, including user interfaces. The reported\nresults were replicated for one model, but not the other, likely due to missing\ninformation on distributions. These models are substantially more complex than\nAI-generated DES models published to date. Given the challenges we faced in\nprompt engineering, code generation, and model testing, we conclude that our\niterative approach to model development, systematic comparison and testing, and\nthe expertise of our team were necessary to the success of our recreated\nsimulation models.",
      "tldr_zh": "本文探讨了使用生成式AI重建医疗保健离散事件模拟(DES)模型的问题，旨在解决模型共享不足导致的重用和长期影响限制。研究采用结构化方法和Free and Open Source Software (FOSS)，基于学术期刊描述成功生成、测试并内部复制了两个更复杂的DES模型，包括用户界面。结果显示，一个模型的报告结果被成功复制，而另一个因分布信息缺失而失败，强调了提示工程、代码生成和团队专业知识在这一过程中的关键作用。",
      "categories": [
        "cs.AI",
        "stat.AP"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21646v1",
      "published_date": "2025-03-27 16:10:02 UTC",
      "updated_date": "2025-03-27 16:10:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:42:28.447139"
    },
    {
      "arxiv_id": "2503.21640v1",
      "title": "Towards Fully Automated Decision-Making Systems for Greenhouse Control: Challenges and Opportunities",
      "title_zh": "迈向完全自动化的温室控制决策系统：挑战与机会",
      "authors": [
        "Yongshuai Liu",
        "Taeyeong Choi",
        "Xin Liu"
      ],
      "abstract": "Machine learning has been successful in building control policies to drive a\ncomplex system to desired states in various applications (e.g. games, robotics,\netc.). To be specific, a number of parameters of policy can be automatically\noptimized from the observations of environment to be able to generate a\nsequence of decisions leading to the best performance. In this survey paper, we\nparticularly explore such policy-learning techniques for another unique,\npractical use-case scenario--farming, in which critical decisions (e.g., water\nsupply, heating, etc.) must be made in a timely manner to minimize risks (e.g.,\ndamage to plants) while maximizing the revenue (e.g., healthy crops) in the\nend. We first provide a broad overview of latest studies on it to identify not\nonly domain-specific challenges but opportunities with potential solutions,\nsome of which are suggested as promising directions for future research. Also,\nwe then introduce our successful approach to being ranked second among 46 teams\nat the ''3rd Autonomous Greenhouse Challenge'' to use this specific example to\ndiscuss the lessons learned about important considerations for design to create\nautonomous farm-management systems.",
      "tldr_zh": "这篇论文探讨了机器学习在温室控制中的应用，旨在构建全自动决策系统(policy-learning techniques)，通过从环境观察优化参数来生成最佳决策序列，以最小化风险（如植物损害）和最大化收益（如健康作物）。论文概述了农业领域的特定挑战（如及时决策的需求）和机会，并提出潜在解决方案作为未来研究方向。作者分享了其在“3rd Autonomous Greenhouse Challenge”中排名第二的成功方法，并讨论了设计自主农场管理系统的重要考虑，例如系统鲁棒性和实际优化策略。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21640v1",
      "published_date": "2025-03-27 16:06:59 UTC",
      "updated_date": "2025-03-27 16:06:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:42:41.569038"
    },
    {
      "arxiv_id": "2503.21634v1",
      "title": "When Astronomy Meets AI: Manazel For Crescent Visibility Prediction in Morocco",
      "title_zh": "翻译失败",
      "authors": [
        "Yassir Lairgi"
      ],
      "abstract": "The accurate determination of the beginning of each Hijri month is essential\nfor religious, cultural, and administrative purposes. Manazel (The code and\ndatasets are available at https://github.com/lairgiyassir/manazel) addresses\nthis challenge in Morocco by leveraging 13 years of crescent visibility data to\nrefine the ODEH criterion, a widely used standard for lunar crescent visibility\nprediction. The study integrates two key features, the Arc of Vision (ARCV) and\nthe total width of the crescent (W), to enhance the accuracy of lunar\nvisibility assessments. A machine learning approach utilizing the Logistic\nRegression algorithm is employed to classify crescent visibility conditions,\nachieving a predictive accuracy of 98.83%. This data-driven methodology offers\na robust and reliable framework for determining the start of the Hijri month,\ncomparing different data classification tools, and improving the consistency of\nlunar calendar calculations in Morocco. The findings demonstrate the\neffectiveness of machine learning in astronomical applications and highlight\nthe potential for further enhancements in the modeling of crescent visibility.",
      "tldr_zh": "本文介绍了Manazel系统，利用13年的新月可见数据改进ODEH criterion，用于预测摩洛哥的新月可见性。研究整合了Arc of Vision (ARCV)和总新月宽度(W)两个关键特征，并采用Logistic Regression算法进行机器学习分类，实现98.83%的预测准确率。该方法为Hijri月份的开始提供可靠的数据驱动框架，并证明了机器学习在天文学应用中的有效性，具有进一步优化的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21634v1",
      "published_date": "2025-03-27 15:56:55 UTC",
      "updated_date": "2025-03-27 15:56:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:42:52.119035"
    },
    {
      "arxiv_id": "2503.21620v4",
      "title": "UI-R1: Enhancing Efficient Action Prediction of GUI Agents by Reinforcement Learning",
      "title_zh": "UI-R1：通过强化学习增强图形用户界面代理的动作预测效率",
      "authors": [
        "Zhengxi Lu",
        "Yuxiang Chai",
        "Yaxuan Guo",
        "Xi Yin",
        "Liang Liu",
        "Hao Wang",
        "Han Xiao",
        "Shuai Ren",
        "Guanjing Xiong",
        "Hongsheng Li"
      ],
      "abstract": "The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities\nin LLMs through reinforcement learning (RL) with rule-based rewards. Despite\nits success in language models, its application in multi-modal domains,\nparticularly in graphic user interface (GUI) agent tasks, remains\nunder-explored. To address this issue, we propose UI-R1, the first framework to\nexplore how rule-based RL can enhance the reasoning capabilities of multimodal\nlarge language models (MLLMs) for GUI action prediction tasks. Specifically,\nUI-R1 introduces a novel rule-based action reward, enabling model optimization\nvia policy-based algorithms such as Group Relative Policy Optimization (GRPO).\nFor efficient training, we curate a small yet high-quality dataset of 136\nchallenging tasks, encompassing five common action types on mobile devices.\nExperimental results demonstrate that our proposed UI-R1-3B achieves\nsignificant improvements over the base model (i.e. Qwen2.5-VL-3B) on both\nin-domain (ID) and out-of-domain (OOD) tasks, with average accuracy gains of\n22.1% on ScreenSpot, 6.0% on ScreenSpot-Pro, and 12.7% on ANDROIDCONTROL.\nFurthermore, UI-R1-3B delivers competitive performance compared to larger\nmodels (e.g., OS-Atlas-7B) trained via supervised fine-tuning (SFT) on 76K\nsamples. We additionally develop an optimized version, UI-R1-E-3B, which\nsignificantly improves both grounding efficiency and accuracy. These results\nunderscore the potential of rule-based reinforcement learning to advance GUI\nunderstanding and control, paving the way for future research in this domain.\nCode website: https://github.com/lll6gg/UI-R1.",
      "tldr_zh": "本文提出 UI-R1 框架，这是首个利用强化学习（RL）和基于规则的奖励来增强多模态大型语言模型（MLLMs）在图形用户界面（GUI）动作预测任务中的推理能力。具体地，UI-R1 引入了新型规则-based 动作奖励，并采用 Group Relative Policy Optimization (GRPO) 算法进行模型优化，在一个包含 136 个挑战性任务的小型数据集上进行高效训练。实验结果显示，UI-R1-3B 模型在 ScreenSpot、ScreenSpot-Pro 和 ANDROIDCONTROL 数据集上，比基线模型 Qwen2.5-VL-3B 分别提高了 22.1%、6.0% 和 12.7% 的平均准确率，并与更大模型（如 OS-Atlas-7B）相比表现出竞争性性能。此外，优化版本 UI-R1-E-3B 显著提升了 grounding 效率和准确性，展示了基于规则的 RL 在推进 GUI 理解和控制方面的潜力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Updated UI-R1-E-3B",
      "pdf_url": "http://arxiv.org/pdf/2503.21620v4",
      "published_date": "2025-03-27 15:39:30 UTC",
      "updated_date": "2025-05-14 11:56:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:43:06.876998"
    },
    {
      "arxiv_id": "2503.21615v2",
      "title": "A Measure Based Generalizable Approach to Understandability",
      "title_zh": "翻译失败",
      "authors": [
        "Vikas Kushwaha",
        "Sruti Srinivasa Ragavan",
        "Subhajit Roy"
      ],
      "abstract": "Successful agent-human partnerships require that any agent generated\ninformation is understandable to the human, and that the human can easily steer\nthe agent towards a goal. Such effective communication requires the agent to\ndevelop a finer-level notion of what is understandable to the human.\nState-of-the-art agents, including LLMs, lack this detailed notion of\nunderstandability because they only capture average human sensibilities from\nthe training data, and therefore afford limited steerability (e.g., requiring\nnon-trivial prompt engineering).\n  In this paper, instead of only relying on data, we argue for developing\ngeneralizable, domain-agnostic measures of understandability that can be used\nas directives for these agents. Existing research on understandability measures\nis fragmented, we survey various such efforts across domains, and lay a\ncognitive-science-rooted groundwork for more coherent and domain-agnostic\nresearch investigations in future.",
      "tldr_zh": "这篇论文探讨了在代理-人类互动中，确保代理生成的信息易于理解并便于人类引导的重要性。现有代理如 LLMs 仅从训练数据中捕捉平均人类敏感性，导致可操控性有限（如需要复杂的 prompt engineering）。论文主张开发通用的、领域无关的理解度衡量措施，通过调查现有碎片化研究并建立认知科学根基，为未来更连贯的研究提供基础。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.HC",
      "comment": "6 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.21615v2",
      "published_date": "2025-03-27 15:36:49 UTC",
      "updated_date": "2025-04-23 17:39:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:43:17.712094"
    },
    {
      "arxiv_id": "2504.01024v1",
      "title": "Gaze-Guided 3D Hand Motion Prediction for Detecting Intent in Egocentric Grasping Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Yufei He",
        "Xucong Zhang",
        "Arno H. A. Stienen"
      ],
      "abstract": "Human intention detection with hand motion prediction is critical to drive\nthe upper-extremity assistive robots in neurorehabilitation applications.\nHowever, the traditional methods relying on physiological signal measurement\nare restrictive and often lack environmental context. We propose a novel\napproach that predicts future sequences of both hand poses and joint positions.\nThis method integrates gaze information, historical hand motion sequences, and\nenvironmental object data, adapting dynamically to the assistive needs of the\npatient without prior knowledge of the intended object for grasping.\nSpecifically, we use a vector-quantized variational autoencoder for robust hand\npose encoding with an autoregressive generative transformer for effective hand\nmotion sequence prediction. We demonstrate the usability of these novel\ntechniques in a pilot study with healthy subjects. To train and evaluate the\nproposed method, we collect a dataset consisting of various types of grasp\nactions on different objects from multiple subjects. Through extensive\nexperiments, we demonstrate that the proposed method can successfully predict\nsequential hand movement. Especially, the gaze information shows significant\nenhancements in prediction capabilities, particularly with fewer input frames,\nhighlighting the potential of the proposed method for real-world applications.",
      "tldr_zh": "本研究针对神经康复中的上肢辅助机器人，提出了一种基于凝视信息(gaze information)的3D手部运动预测方法，用于检测自我中心视角下的抓取意图。该方法整合凝视信息、历史手部运动序列以及环境物体数据，通过vector-quantized variational autoencoder (VQ-VAE)进行手部姿势编码，并使用autoregressive generative transformer生成未来手部序列预测，而无需预先知道目标对象。实验结果显示，该方法在收集的多主体抓取数据集上成功预测了手部运动序列，且凝视信息的加入显著提升了预测性能，尤其在输入帧较少时，为实时康复应用提供了潜在优势。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.01024v1",
      "published_date": "2025-03-27 15:26:41 UTC",
      "updated_date": "2025-03-27 15:26:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:43:30.201167"
    },
    {
      "arxiv_id": "2503.21602v1",
      "title": "GenEdit: Compounding Operators and Continuous Improvement to Tackle Text-to-SQL in the Enterprise",
      "title_zh": "翻译失败",
      "authors": [
        "Karime Maamari",
        "Connor Landy",
        "Amine Mhedhbi"
      ],
      "abstract": "Recent advancements in Text-to-SQL, driven by large language models, are\ndemocratizing data access. Despite these advancements, enterprise deployments\nremain challenging due to the need to capture business-specific knowledge,\nhandle complex queries, and meet expectations of continuous improvements. To\naddress these issues, we designed and implemented GenEdit: our Text-to-SQL\ngeneration system that improves with user feedback. GenEdit builds and\nmaintains a company-specific knowledge set, employs a pipeline of operators\ndecomposing SQL generation, and uses feedback to update its knowledge set to\nimprove future SQL generations.\n  We describe GenEdit's architecture made of two core modules: (i) decomposed\nSQL generation; and (ii) knowledge set edits based on user feedback. For\ngeneration, GenEdit leverages compounding operators to improve knowledge\nretrieval and to create a plan as chain-of-thought steps that guides\ngeneration. GenEdit first retrieves relevant examples in an initial retrieval\nstage where original SQL queries are decomposed into sub-statements, clauses or\nsub-queries. It then also retrieves instructions and schema elements. Using the\nretrieved contextual information, GenEdit then generates step-by-step plan in\nnatural language on how to produce the query. Finally, GenEdit uses the plan to\ngenerate SQL, minimizing the need for model reasoning, which enhances complex\nSQL generation. If necessary, GenEdit regenerates the query based on syntactic\nand semantic errors. The knowledge set edits are recommended through an\ninteractive copilot, allowing users to iterate on their feedback and to\nregenerate SQL queries as needed. Each generation uses staged edits which\nupdate the generation prompt. Once the feedback is submitted, it gets merged\nafter passing regression testing and obtaining an approval, improving future\ngenerations.",
      "tldr_zh": "该研究提出GenEdit，一种针对企业环境的Text-to-SQL生成系统，旨在通过捕获业务特定知识、处理复杂查询并支持持续改进来解决数据访问挑战。GenEdit采用compounding operators和分解SQL生成模块，首先通过初始检索阶段分解查询、检索相关示例、指令和schema元素，然后创建chain-of-thought步骤的自然语言计划来指导SQL生成，从而提高准确性和效率。系统还包括基于用户反馈的知识集编辑模块，通过交互式copilot推荐更新，并在回归测试和批准后合并反馈，以优化未来的SQL生成。整体设计使GenEdit能够实现持续改进，提升企业级Text-to-SQL的可靠性和实用性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21602v1",
      "published_date": "2025-03-27 15:22:02 UTC",
      "updated_date": "2025-03-27 15:22:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:43:42.593782"
    },
    {
      "arxiv_id": "2503.21843v1",
      "title": "CMD-HAR: Cross-Modal Disentanglement for Wearable Human Activity Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Hanyu Liu",
        "Siyao Li",
        "Ying Yu",
        "Yixuan Jiang",
        "Hang Xiao",
        "Jingxi Long",
        "Haotian Tang"
      ],
      "abstract": "Human Activity Recognition (HAR) is a fundamental technology for numerous\nhuman - centered intelligent applications. Although deep learning methods have\nbeen utilized to accelerate feature extraction, issues such as multimodal data\nmixing, activity heterogeneity, and complex model deployment remain largely\nunresolved. The aim of this paper is to address issues such as multimodal data\nmixing, activity heterogeneity, and complex model deployment in sensor-based\nhuman activity recognition. We propose a spatiotemporal attention modal\ndecomposition alignment fusion strategy to tackle the problem of the mixed\ndistribution of sensor data. Key discriminative features of activities are\ncaptured through cross-modal spatio-temporal disentangled representation, and\ngradient modulation is combined to alleviate data heterogeneity. In addition, a\nwearable deployment simulation system is constructed. We conducted experiments\non a large number of public datasets, demonstrating the effectiveness of the\nmodel.",
      "tldr_zh": "本文提出 CMD-HAR 框架，用于解决可穿戴设备的人类活动识别 (HAR) 中的多模态数据混合、活动异质性和复杂模型部署问题。框架采用时空注意模态分解对齐融合策略和跨模态时空解耦表示来捕获活动的关键鉴别特征，并结合梯度调制缓解数据异质性。同时，构建了一个可穿戴部署模拟系统。在多个公共数据集上进行的实验证明了该模型的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21843v1",
      "published_date": "2025-03-27 15:21:49 UTC",
      "updated_date": "2025-03-27 15:21:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:43:54.387090"
    },
    {
      "arxiv_id": "2503.21598v1",
      "title": "Prompt, Divide, and Conquer: Bypassing Large Language Model Safety Filters via Segmented and Distributed Prompt Processing",
      "title_zh": "翻译失败",
      "authors": [
        "Johan Wahréus",
        "Ahmed Hussain",
        "Panos Papadimitratos"
      ],
      "abstract": "Large Language Models (LLMs) have transformed task automation and content\ngeneration across various domains while incorporating safety filters to prevent\nmisuse. We introduce a novel jailbreaking framework that employs distributed\nprompt processing combined with iterative refinements to bypass these safety\nmeasures, particularly in generating malicious code. Our architecture consists\nof four key modules: prompt segmentation, parallel processing, response\naggregation, and LLM-based jury evaluation. Tested on 500 malicious prompts\nacross 10 cybersecurity categories, the framework achieves a 73.2% Success Rate\n(SR) in generating malicious code. Notably, our comparative analysis reveals\nthat traditional single-LLM judge evaluation overestimates SRs (93.8%) compared\nto our LLM jury system (73.2%), with manual verification confirming that\nsingle-judge assessments often accept incomplete implementations. Moreover, we\ndemonstrate that our distributed architecture improves SRs by 12% over the\nnon-distributed approach in an ablation study, highlighting both the\neffectiveness of distributed prompt processing and the importance of robust\nevaluation methodologies in assessing jailbreak attempts.",
      "tldr_zh": "本文提出了一种名为“Prompt, Divide, and Conquer”的框架，通过分割和分布式提示处理来绕过Large Language Models (LLMs) 的安全过滤器，专注于生成恶意代码。该框架包括四个关键模块：prompt segmentation、parallel processing、response aggregation 和 LLM-based jury evaluation，在500个恶意提示的测试中实现了73.2%的成功率(SR)。与传统单LLM评估方法相比，该框架更准确地评估SR（避免过度估计），并在消融研究中显示分布式架构比非分布式方法提高12%，突显了分布式处理和鲁棒评估方法的重要性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "22 pages; 26 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.21598v1",
      "published_date": "2025-03-27 15:19:55 UTC",
      "updated_date": "2025-03-27 15:19:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:44:06.613498"
    },
    {
      "arxiv_id": "2503.22759v1",
      "title": "Data Poisoning in Deep Learning: A Survey",
      "title_zh": "深度学习中的数据投毒：一项综述",
      "authors": [
        "Pinlong Zhao",
        "Weiyao Zhu",
        "Pengfei Jiao",
        "Di Gao",
        "Ou Wu"
      ],
      "abstract": "Deep learning has become a cornerstone of modern artificial intelligence,\nenabling transformative applications across a wide range of domains. As the\ncore element of deep learning, the quality and security of training data\ncritically influence model performance and reliability. However, during the\ntraining process, deep learning models face the significant threat of data\npoisoning, where attackers introduce maliciously manipulated training data to\ndegrade model accuracy or lead to anomalous behavior. While existing surveys\nprovide valuable insights into data poisoning, they generally adopt a broad\nperspective, encompassing both attacks and defenses, but lack a dedicated,\nin-depth analysis of poisoning attacks specifically in deep learning. In this\nsurvey, we bridge this gap by presenting a comprehensive and targeted review of\ndata poisoning in deep learning. First, this survey categorizes data poisoning\nattacks across multiple perspectives, providing an in-depth analysis of their\ncharacteristics and underlying design princinples. Second, the discussion is\nextended to the emerging area of data poisoning in large language models(LLMs).\nFinally, we explore critical open challenges in the field and propose potential\nresearch directions to advance the field further. To support further\nexploration, an up-to-date repository of resources on data poisoning in deep\nlearning is available at https://github.com/Pinlong-Zhao/Data-Poisoning.",
      "tldr_zh": "这篇论文对深度学习中的数据投毒（data poisoning）进行了全面调查，强调了攻击者通过操纵训练数据来降低模型准确性或引发异常行为的威胁，并填补了现有调查在深度学习特定领域的空白。论文从多个角度分类了数据投毒攻击，深入分析了其特征和设计原则，并扩展讨论到大型语言模型（LLMs）中的相关问题。最终，它探讨了关键挑战和潜在研究方向，并提供了一个资源仓库（https://github.com/Pinlong-Zhao/Data-Poisoning）以支持进一步探索。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22759v1",
      "published_date": "2025-03-27 15:16:57 UTC",
      "updated_date": "2025-03-27 15:16:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:44:17.895160"
    },
    {
      "arxiv_id": "2503.21592v1",
      "title": "Critical Iterative Denoising: A Discrete Generative Model Applied to Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Yoann Boget",
        "Alexandros Kalousis"
      ],
      "abstract": "Discrete Diffusion and Flow Matching models have significantly advanced\ngenerative modeling for discrete structures, including graphs. However, the\ntime dependencies in the noising process of these models lead to error\naccumulation and propagation during the backward process. This issue,\nparticularly pronounced in mask diffusion, is a known limitation in sequence\nmodeling and, as we demonstrate, also impacts discrete diffusion models for\ngraphs.\n  To address this problem, we propose a novel framework called Iterative\nDenoising, which simplifies discrete diffusion and circumvents the issue by\nassuming conditional independence across time. Additionally, we enhance our\nmodel by incorporating a Critic, which during generation selectively retains or\ncorrupts elements in an instance based on their likelihood under the data\ndistribution. Our empirical evaluations demonstrate that the proposed method\nsignificantly outperforms existing discrete diffusion baselines in graph\ngeneration tasks.",
      "tldr_zh": "这篇论文针对 Discrete Diffusion 和 Flow Matching 模型在生成图等离散结构时的错误积累和传播问题，提出了一种新框架：Iterative Denoising，通过假设时间上的条件独立性来简化离散扩散过程并避免这些问题。框架进一步增强了 Critic 模块，该模块在生成过程中根据数据分布的似然选择性保留或破坏元素，以提高生成质量。实验结果表明，该方法在图生成任务中显著优于现有离散扩散基线模型。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21592v1",
      "published_date": "2025-03-27 15:08:58 UTC",
      "updated_date": "2025-03-27 15:08:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:44:30.269284"
    },
    {
      "arxiv_id": "2503.21581v1",
      "title": "AlignDiff: Learning Physically-Grounded Camera Alignment via Diffusion",
      "title_zh": "翻译失败",
      "authors": [
        "Liuyue Xie",
        "Jiancong Guo",
        "Ozan Cakmakci",
        "Andre Araujo",
        "Laszlo A. Jeni",
        "Zhiheng Jia"
      ],
      "abstract": "Accurate camera calibration is a fundamental task for 3D perception,\nespecially when dealing with real-world, in-the-wild environments where complex\noptical distortions are common. Existing methods often rely on pre-rectified\nimages or calibration patterns, which limits their applicability and\nflexibility. In this work, we introduce a novel framework that addresses these\nchallenges by jointly modeling camera intrinsic and extrinsic parameters using\na generic ray camera model. Unlike previous approaches, AlignDiff shifts focus\nfrom semantic to geometric features, enabling more accurate modeling of local\ndistortions. We propose AlignDiff, a diffusion model conditioned on geometric\npriors, enabling the simultaneous estimation of camera distortions and scene\ngeometry. To enhance distortion prediction, we incorporate edge-aware\nattention, focusing the model on geometric features around image edges, rather\nthan semantic content. Furthermore, to enhance generalizability to real-world\ncaptures, we incorporate a large database of ray-traced lenses containing over\nthree thousand samples. This database characterizes the distortion inherent in\na diverse variety of lens forms. Our experiments demonstrate that the proposed\nmethod significantly reduces the angular error of estimated ray bundles by ~8.2\ndegrees and overall calibration accuracy, outperforming existing approaches on\nchallenging, real-world datasets.",
      "tldr_zh": "这篇论文提出了AlignDiff框架，使用diffusion model来学习基于物理的相机对齐，解决了现有方法在处理真实世界复杂光学畸变时的局限性。AlignDiff通过通用ray camera model联合建模相机内参和外参，并结合几何先验、edge-aware attention和一个包含超过3000个样本的射线追踪镜头数据库，提升了对图像边缘几何特征的估计准确性。实验结果显示，该方法将估计射线束的角误差降低了约8.2度，并在挑战性的真实世界数据集上优于现有方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21581v1",
      "published_date": "2025-03-27 14:59:59 UTC",
      "updated_date": "2025-03-27 14:59:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:44:42.813948"
    },
    {
      "arxiv_id": "2503.21571v1",
      "title": "Magnitude-Phase Dual-Path Speech Enhancement Network based on Self-Supervised Embedding and Perceptual Contrast Stretch Boosting",
      "title_zh": "翻译失败",
      "authors": [
        "Alimjan Mattursun",
        "Liejun Wang",
        "Yinfeng Yu",
        "Chunyang Ma"
      ],
      "abstract": "Speech self-supervised learning (SSL) has made great progress in various\nspeech processing tasks, but there is still room for improvement in speech\nenhancement (SE). This paper presents BSP-MPNet, a dual-path framework that\ncombines self-supervised features with magnitude-phase information for SE. The\napproach starts by applying the perceptual contrast stretching (PCS) algorithm\nto enhance the magnitude-phase spectrum. A magnitude-phase 2D coarse (MP-2DC)\nencoder then extracts coarse features from the enhanced spectrum. Next, a\nfeature-separating self-supervised learning (FS-SSL) model generates\nself-supervised embeddings for the magnitude and phase components separately.\nThese embeddings are fused to create cross-domain feature representations.\nFinally, two parallel RNN-enhanced multi-attention (REMA) mask decoders refine\nthe features, apply them to the mask, and reconstruct the speech signal. We\nevaluate BSP-MPNet on the VoiceBank+DEMAND and WHAMR! datasets. Experimental\nresults show that BSP-MPNet outperforms existing methods under various noise\nconditions, providing new directions for self-supervised speech enhancement\nresearch. The implementation of the BSP-MPNet code is available\nonline\\footnote[2]{https://github.com/AlimMat/BSP-MPNet. \\label{s1}}",
      "tldr_zh": "本研究提出BSP-MPNet，一种基于自监督嵌入(Self-Supervised Embedding)和感知对比拉伸(Perceptual Contrast Stretch Boosting)的双路径语音增强网络，旨在提升语音增强(SE)性能。框架首先使用PCS算法增强幅度-相位谱，然后通过幅度-相位2D粗编码器(MP-2DC)提取粗特征，并采用特征分离自监督学习(FS-SSL)模型为幅度和相位生成自监督嵌入，这些嵌入被融合以创建跨域特征表示。最终，两个并行RNN增强的多注意力(REMA)掩码解码器精炼特征、应用掩码并重建语音信号。在VoiceBank+DEMAND和WHAMR!数据集上的实验表明，BSP-MPNet在各种噪声条件下优于现有方法，为自监督语音增强研究提供了新方向。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Main paper (6 pages). Accepted for publication by ICME 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.21571v1",
      "published_date": "2025-03-27 14:52:06 UTC",
      "updated_date": "2025-03-27 14:52:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:44:54.906161"
    },
    {
      "arxiv_id": "2503.21558v1",
      "title": "A Local Perspective-based Model for Overlapping Community Detection",
      "title_zh": "基于局部视角的重叠社区检测模型",
      "authors": [
        "Gaofeng Zhou",
        "Rui-Feng Wang",
        "Kangning Cui"
      ],
      "abstract": "Community detection, which identifies densely connected node clusters with\nsparse between-group links, is vital for analyzing network structure and\nfunction in real-world systems. Most existing community detection methods based\non GCNs primarily focus on node-level information while overlooking\ncommunity-level features, leading to performance limitations on large-scale\nnetworks. To address this issue, we propose LQ-GCN, an overlapping community\ndetection model from a local community perspective. LQ-GCN employs a\nBernoulli-Poisson model to construct a community affiliation matrix and form an\nend-to-end detection framework. By adopting local modularity as the objective\nfunction, the model incorporates local community information to enhance the\nquality and accuracy of clustering results. Additionally, the conventional GCNs\narchitecture is optimized to improve the model capability in identifying\noverlapping communities in large-scale networks. Experimental results\ndemonstrate that LQ-GCN achieves up to a 33% improvement in Normalized Mutual\nInformation (NMI) and a 26.3% improvement in Recall compared to baseline models\nacross multiple real-world benchmark datasets.",
      "tldr_zh": "本研究针对现有基于GCNs的社区检测方法忽略社区级特征的问题，提出了一种基于局部视角的模型LQ-GCN，用于识别网络中的重叠社区。LQ-GCN采用Bernoulli-Poisson模型构建社区归属矩阵，并以local modularity作为目标函数，融入局部社区信息来优化聚类质量，同时改进传统的GCNs架构以适应大规模网络。实验结果显示，该模型在多个真实基准数据集上，比基线模型的Normalized Mutual Information (NMI)提高了最多33%，Recall提高了最多26.3%。",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "10 pages, 3 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.21558v1",
      "published_date": "2025-03-27 14:43:42 UTC",
      "updated_date": "2025-03-27 14:43:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:45:06.455223"
    },
    {
      "arxiv_id": "2503.21557v1",
      "title": "debug-gym: A Text-Based Environment for Interactive Debugging",
      "title_zh": "debug-gym：一种基于文本的交互式调试环境",
      "authors": [
        "Xingdi Yuan",
        "Morgane M Moss",
        "Charbel El Feghali",
        "Chinmay Singh",
        "Darya Moldavskaya",
        "Drew MacPhee",
        "Lucas Caccia",
        "Matheus Pereira",
        "Minseon Kim",
        "Alessandro Sordoni",
        "Marc-Alexandre Côté"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly relied upon for coding tasks,\nyet in most scenarios it is assumed that all relevant information can be either\naccessed in context or matches their training data. We posit that LLMs can\nbenefit from the ability to interactively explore a codebase to gather the\ninformation relevant to their task. To achieve this, we present a textual\nenvironment, namely debug-gym, for developing LLM-based agents in an\ninteractive coding setting. Our environment is lightweight and provides a\npreset of useful tools, such as a Python debugger (pdb), designed to facilitate\nan LLM-based agent's interactive debugging. Beyond coding and debugging tasks,\nthis approach can be generalized to other tasks that would benefit from\ninformation-seeking behavior by an LLM agent.",
      "tldr_zh": "该研究指出，大型语言模型 (LLMs) 在编码任务中往往依赖上下文或训练数据，而缺乏交互式探索代码库的能力。作者开发了 debug-gym，这是一个轻量级的文本环境，旨在支持 LLM 基础代理在交互式编码设置中进行信息收集和调试，并提供预设工具如 Python 调试器 (pdb)。通过这个环境，LLMs 可以更有效地处理编码和调试任务，并推广到其他需要信息搜索行为的领域。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.PL",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21557v1",
      "published_date": "2025-03-27 14:43:28 UTC",
      "updated_date": "2025-03-27 14:43:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:45:18.679173"
    },
    {
      "arxiv_id": "2503.21544v1",
      "title": "SWI: Speaking with Intent in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yuwei Yin",
        "EunJeong Hwang",
        "Giuseppe Carenini"
      ],
      "abstract": "Intent, typically clearly formulated and planned, functions as a cognitive\nframework for reasoning and problem-solving. This paper introduces the concept\nof Speaking with Intent (SWI) in large language models (LLMs), where the\nexplicitly generated intent encapsulates the model's underlying intention and\nprovides high-level planning to guide subsequent analysis and communication. By\nemulating deliberate and purposeful thoughts in the human mind, SWI is\nhypothesized to enhance the reasoning capabilities and generation quality of\nLLMs. Extensive experiments on mathematical reasoning benchmarks consistently\ndemonstrate the superiority of Speaking with Intent over Baseline (i.e.,\ngeneration without explicit intent). Moreover, SWI outperforms answer-trigger\nprompting methods Chain-of-Thought and Plan-and-Solve and maintains competitive\nperformance with the strong method ARR (Analyzing, Retrieving, and Reasoning).\nAdditionally, the effectiveness and generalizability of SWI are solidified on\nreasoning-intensive question answering (QA) and text summarization benchmarks,\nwhere SWI brings consistent improvement to the Baseline generation. In text\nsummarization, SWI-generated summaries exhibit greater accuracy, conciseness,\nand factual correctness, with fewer hallucinations. Furthermore, human\nevaluations verify the coherence, effectiveness, and interpretability of the\nintent produced by SWI. This proof-of-concept study creates a novel avenue for\nenhancing LLMs' reasoning abilities with cognitive notions.",
      "tldr_zh": "本论文引入 Speaking with Intent (SWI) 概念，用于大型语言模型 (LLMs)，通过显式生成意图来封装模型的底层意图并提供高层规划，模仿人类思维以提升推理能力和生成质量。实验结果显示，SWI 在数学推理基准上优于基线方法以及 Chain-of-Thought 和 Plan-and-Solve 等提示技术，并在推理密集型问答 (QA) 和文本摘要任务中带来一致改进，后者生成更准确、简洁且事实正确的摘要，同时减少 hallucination。人类评估进一步证实 SWI 生成的意图在连贯性、有效性和可解释性方面表现出色，为利用认知概念增强 LLMs 推理能力开辟新途径。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "24 pages. Code: https://github.com/YuweiYin/SWI",
      "pdf_url": "http://arxiv.org/pdf/2503.21544v1",
      "published_date": "2025-03-27 14:34:28 UTC",
      "updated_date": "2025-03-27 14:34:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:45:30.747969"
    },
    {
      "arxiv_id": "2503.21541v2",
      "title": "LOCATEdit: Graph Laplacian Optimized Cross Attention for Localized Text-Guided Image Editing",
      "title_zh": "翻译失败",
      "authors": [
        "Achint Soni",
        "Meet Soni",
        "Sirisha Rambhatla"
      ],
      "abstract": "Text-guided image editing aims to modify specific regions of an image\naccording to natural language instructions while maintaining the general\nstructure and the background fidelity. Existing methods utilize masks derived\nfrom cross-attention maps generated from diffusion models to identify the\ntarget regions for modification. However, since cross-attention mechanisms\nfocus on semantic relevance, they struggle to maintain the image integrity. As\na result, these methods often lack spatial consistency, leading to editing\nartifacts and distortions. In this work, we address these limitations and\nintroduce LOCATEdit, which enhances cross-attention maps through a graph-based\napproach utilizing self-attention-derived patch relationships to maintain\nsmooth, coherent attention across image regions, ensuring that alterations are\nlimited to the designated items while retaining the surrounding structure.\nLOCATEdit consistently and substantially outperforms existing baselines on\nPIE-Bench, demonstrating its state-of-the-art performance and effectiveness on\nvarious editing tasks. Code can be found on\nhttps://github.com/LOCATEdit/LOCATEdit/",
      "tldr_zh": "该论文针对文本引导图像编辑（Text-Guided Image Editing）的问题，提出 LOCATEdit 方法，以解决现有基于交叉注意力（Cross Attention）机制的编辑技术在空间一致性上的不足，导致编辑伪影和失真。LOCATEdit 通过 Graph Laplacian 优化交叉注意力图，利用自注意力派生的 patch 关系构建图-based 框架，确保修改仅限于指定区域，同时保持图像整体结构和背景完整性。该方法在 PIE-Bench 基准测试中显著优于现有基线，展示了其在各种编辑任务中的最先进性能和有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21541v2",
      "published_date": "2025-03-27 14:32:17 UTC",
      "updated_date": "2025-03-28 12:17:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:45:42.374743"
    },
    {
      "arxiv_id": "2503.21530v2",
      "title": "Low-Resource Transliteration for Roman-Urdu and Urdu Using Transformer-Based Models",
      "title_zh": "翻译失败",
      "authors": [
        "Umer Butt",
        "Stalin Veranasi",
        "Günter Neumann"
      ],
      "abstract": "As the Information Retrieval (IR) field increasingly recognizes the\nimportance of inclusivity, addressing the needs of low-resource languages\nremains a significant challenge. Transliteration between Urdu and its Romanized\nform, Roman Urdu, remains underexplored despite the widespread use of both\nscripts in South Asia. Prior work using RNNs on the Roman-Urdu-Parl dataset\nshowed promising results but suffered from poor domain adaptability and limited\nevaluation. We propose a transformer-based approach using the m2m100\nmultilingual translation model, enhanced with masked language modeling (MLM)\npretraining and fine-tuning on both Roman-Urdu-Parl and the domain-diverse\nDakshina dataset. To address previous evaluation flaws, we introduce rigorous\ndataset splits and assess performance using BLEU, character-level BLEU, and\nCHRF. Our model achieves strong transliteration performance, with Char-BLEU\nscores of 96.37 for Urdu->Roman-Urdu and 97.44 for Roman-Urdu->Urdu. These\nresults outperform both RNN baselines and GPT-4o Mini and demonstrate the\neffectiveness of multilingual transfer learning for low-resource\ntransliteration tasks.",
      "tldr_zh": "该研究针对低资源语言乌尔都语（Urdu）和罗马乌尔都语（Roman-Urdu）的音译问题，提出了一种基于 Transformer 的方法，使用 m2m100 多语言翻译模型，并通过 masked language modeling (MLM) 预训练和在 Roman-Urdu-Parl 及 Dakshina 数据集上的微调来提升性能。相比以往的 RNN 模型，该方法改善了领域适应性和评估标准，采用严格的数据集划分以及 BLEU、character-level BLEU 和 CHRF 指标进行评估。结果显示，模型在 Urdu->Roman-Urdu 的 Char-BLEU 得分达到 96.37，在 Roman-Urdu->Urdu 的得分达到 97.44，优于 RNN 基线和 GPT-4o Mini，证明了多语言迁移学习在低资源音译任务中的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21530v2",
      "published_date": "2025-03-27 14:18:50 UTC",
      "updated_date": "2025-04-04 09:55:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:45:55.407083"
    },
    {
      "arxiv_id": "2503.21522v1",
      "title": "MONO2REST: Identifying and Exposing Microservices: a Reusable RESTification Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Matthéo Lecrivain",
        "Hanifa Barry",
        "Dalila Tamzalit",
        "Houari Sahraoui"
      ],
      "abstract": "The microservices architectural style has become the de facto standard for\nlarge-scale cloud applications, offering numerous benefits in scalability,\nmaintainability, and deployment flexibility. Many organizations are pursuing\nthe migration of legacy monolithic systems to a microservices architecture.\nHowever, this process is challenging, risky, time-intensive, and\nprone-to-failure while several organizations lack necessary financial\nresources, time, or expertise to set up this migration process. So, rather than\ntrying to migrate a legacy system where migration is risky or not feasible, we\nsuggest exposing it as a microservice application without without having to\nmigrate it. In this paper, we present a reusable, automated, two-phase approach\nthat combines evolutionary algorithms with machine learning techniques. In the\nfirst phase, we identify microservices at the method level using a\nmulti-objective genetic algorithm that considers both structural and semantic\ndependencies between methods. In the second phase, we generate REST APIs for\neach identified microservice using a classification algorithm to assign HTTP\nmethods and endpoints. We evaluated our approach with a case study on the\nSpring PetClinic application, which has both monolithic and microservices\nimplementations that serve as ground truth for comparison. Results demonstrate\nthat our approach successfully aligns identified microservices with those in\nthe reference microservices implementation, highlighting its effectiveness in\nservice identification and API generation.",
      "tldr_zh": "该论文提出MONO2REST，一种可重用的自动化方法，用于将遗留的monolithic系统转化为微服务架构，而无需实际迁移，以降低风险和资源需求。该方法分为两个阶段：第一阶段，使用multi-objective genetic algorithm结合结构和语义依赖性在方法级别识别微服务；第二阶段，通过分类算法生成REST APIs，包括分配HTTP methods和端点。实验结果显示，在Spring PetClinic应用的案例研究中，该方法成功地将识别的微服务与参考微服务实现对齐，证明了其在服务识别和API生成方面的有效性。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21522v1",
      "published_date": "2025-03-27 14:10:33 UTC",
      "updated_date": "2025-03-27 14:10:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:46:06.644305"
    },
    {
      "arxiv_id": "2503.21514v1",
      "title": "Quantitative Evaluation of Quantum/Classical Neural Network Using a Game Solver Metric",
      "title_zh": "使用游戏求解器指标对量子/经典神经网络进行定量评估",
      "authors": [
        "Suzukaze Kamei",
        "Hideaki Kawaguchi",
        "Shin Nishio",
        "Tatakahiko Satoh"
      ],
      "abstract": "To evaluate the performance of quantum computing systems relative to\nclassical counterparts and explore the potential for quantum advantage, we\npropose a game-solving benchmark based on Elo ratings in the game of\ntic-tac-toe. We compare classical convolutional neural networks (CNNs), quantum\nconvolutional neural networks (QCNNs), and hybrid classical-quantum models by\nassessing their performance against a random-move agent in automated matches.\nAdditionally, we implement a QCNN integrated with quantum communication and\nevaluate its performance to quantify the overhead introduced by noisy quantum\nchannels. Our results show that the classical-quantum hybrid model achieves Elo\nratings comparable to those of classical CNNs, while the standalone QCNN\nunderperforms under current hardware constraints. The communication overhead\nwas found to be modest. These findings demonstrate the viability of using\ngame-based benchmarks for evaluating quantum computing systems and suggest that\nquantum communication can be incorporated with limited impact on performance,\nproviding a foundation for future hybrid quantum applications.",
      "tldr_zh": "本研究提出了一种基于井字游戏（tic-tac-toe）和 Elo ratings 的基准，用于定量评估量子神经网络(QCNN)、经典神经网络(CNN)以及混合模型的性能。研究通过自动比赛与随机移动代理进行比较，评估这些模型在游戏求解中的表现，并测试集成量子通信的QCNN以量化噪声量子通道的开销。结果显示，混合模型的Elo ratings 与经典CNN相当，而独立的QCNN在当前硬件约束下表现较差；同时，量子通信开销适中，为评估量子计算系统和开发未来混合应用提供了可行基础。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "11 pages, 16 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.21514v1",
      "published_date": "2025-03-27 14:05:16 UTC",
      "updated_date": "2025-03-27 14:05:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:46:18.727952"
    },
    {
      "arxiv_id": "2503.21504v1",
      "title": "Keyword-Oriented Multimodal Modeling for Euphemism Identification",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxue Hu",
        "Junsong Li",
        "Meixuan Chen",
        "Dongyu Su",
        "Tongguan Wang",
        "Ying Sha"
      ],
      "abstract": "Euphemism identification deciphers the true meaning of euphemisms, such as\nlinking \"weed\" (euphemism) to \"marijuana\" (target keyword) in illicit texts,\naiding content moderation and combating underground markets. While existing\nmethods are primarily text-based, the rise of social media highlights the need\nfor multimodal analysis, incorporating text, images, and audio. However, the\nlack of multimodal datasets for euphemisms limits further research. To address\nthis, we regard euphemisms and their corresponding target keywords as keywords\nand first introduce a keyword-oriented multimodal corpus of euphemisms\n(KOM-Euph), involving three datasets (Drug, Weapon, and Sexuality), including\ntext, images, and speech. We further propose a keyword-oriented multimodal\neuphemism identification method (KOM-EI), which uses cross-modal feature\nalignment and dynamic fusion modules to explicitly utilize the visual and audio\nfeatures of the keywords for efficient euphemism identification. Extensive\nexperiments demonstrate that KOM-EI outperforms state-of-the-art models and\nlarge language models, and show the importance of our multimodal datasets.",
      "tldr_zh": "这篇论文针对委婉语（euphemism）识别问题，强调了在社交媒体中整合文本、图像和音频的多模态分析，以更好地链接委婉语（如\"weed\"）与目标关键词（如\"marijuana\"），用于内容审核。研究者引入了一个新的keyword-oriented多模态语料库KOM-Euph，包括Drug、Weapon和Sexuality三个数据集，涵盖文本、图像和语音。论文还提出了KOM-EI方法，通过cross-modal feature alignment和dynamic fusion modules显式利用关键词的视觉和音频特征，提升识别效率。实验结果表明，KOM-EI优于现有最先进模型和大型语言模型，证明了多模态数据集的重要性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21504v1",
      "published_date": "2025-03-27 13:45:35 UTC",
      "updated_date": "2025-03-27 13:45:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:46:31.030037"
    },
    {
      "arxiv_id": "2503.22756v1",
      "title": "Towards an intelligent assessment system for evaluating the development of algorithmic thinking skills: An exploratory study in Swiss compulsory schools",
      "title_zh": "翻译失败",
      "authors": [
        "Giorgia Adorni"
      ],
      "abstract": "The rapid digitalisation of contemporary society has profoundly impacted\nvarious facets of our lives, including healthcare, communication, business, and\neducation. The ability to engage with new technologies and solve problems has\nbecome crucial, making CT skills, such as pattern recognition, decomposition,\nand algorithm design, essential competencies. In response, Switzerland is\nconducting research and initiatives to integrate CT into its educational\nsystem. This study aims to develop a comprehensive framework for large-scale\nassessment of CT skills, particularly focusing on AT, the ability to design\nalgorithms. To achieve this, we first developed a competence model capturing\nthe situated and developmental nature of CT, guiding the design of activities\ntailored to cognitive abilities, age, and context. This framework clarifies how\nactivity characteristics influence CT development and how to assess these\ncompetencies. Additionally, we developed an activity for large-scale assessment\nof AT skills, offered in two variants: one based on non-digital artefacts\n(unplugged) and manual expert assessment, and the other based on digital\nartefacts (virtual) and automatic assessment. To provide a more comprehensive\nevaluation of students' competencies, we developed an IAS based on BNs with\nnoisy gates, which offers real-time probabilistic assessment for each skill\nrather than a single overall score. The results indicate that the proposed\ninstrument can measure AT competencies across different age groups and\neducational contexts in Switzerland, demonstrating its applicability for\nlarge-scale use. AT competencies exhibit a progressive development, with no\noverall gender differences, though variations are observed at the school level,\nsignificantly influenced by the artefact-based environment and its context,\nunderscoring the importance of creating accessible and adaptable assessment\ntools.",
      "tldr_zh": "这篇论文探讨了在瑞士义务教育中评估算法思维（AT）技能的发展，旨在应对数字化社会对计算思维（CT）技能的需求，并开发了一个全面评估框架。研究首先构建了一个能力模型，考虑CT的情境和发育特性，用于设计适合不同认知能力、年龄和背景的评估活动。框架包括两种AT技能评估变体：非数字（unplugged）活动结合手动专家评估，以及数字（virtual）活动结合自动评估；此外，还开发了基于贝叶斯网络（BNs） with noisy gates 的智能评估系统（IAS），提供实时概率评估而非单一分数。结果表明，该工具适用于大规模评估，AT技能显示渐进发展、无整体性别差异，但受学校环境影响较大。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22756v1",
      "published_date": "2025-03-27 13:34:36 UTC",
      "updated_date": "2025-03-27 13:34:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:46:43.074728"
    },
    {
      "arxiv_id": "2503.21495v2",
      "title": "Adaptive Resampling with Bootstrap for Noisy Multi-Objective Optimization Problems",
      "title_zh": "翻译失败",
      "authors": [
        "Timo Budszuhn",
        "Mark Joachim Krallmann",
        "Daniel Horn"
      ],
      "abstract": "The challenge of noisy multi-objective optimization lies in the constant\ntrade-off between exploring new decision points and improving the precision of\nknown points through resampling. This decision should take into account both\nthe variability of the objective functions and the current estimate of a point\nin relation to the Pareto front. Since the amount and distribution of noise are\ngenerally unknown, it is desirable for a decision function to be highly\nadaptive to the properties of the optimization problem. This paper presents a\nresampling decision function that incorporates the stochastic nature of the\noptimization problem by using bootstrapping and the probability of dominance.\nThe distribution-free estimation of the probability of dominance is achieved\nusing bootstrap estimates of the means. To make the procedure applicable even\nwith very few observations, we transfer the distribution observed at other\ndecision points. The efficiency of this resampling approach is demonstrated by\napplying it in the NSGA-II algorithm with a sequential resampling procedure\nunder multiple noise variations.",
      "tldr_zh": "该论文针对嘈杂的多目标优化问题（noisy multi-objective optimization problems），提出了一种自适应重采样（adaptive resampling）决策函数，使用 Bootstrap 方法和支配概率（probability of dominance）来平衡探索新决策点与提高已知点精度的权衡。方法通过 Bootstrap 估计均值，实现支配概率的非参数估计，并在样本不足时转移其他决策点的分布，以增强适应性。实验结果显示，该方法集成到 NSGA-II 算法中，并在多种噪声变异下进行顺序重采样，显著提升了优化效率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML",
        "90C29",
        "G.1.6"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages. 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.21495v2",
      "published_date": "2025-03-27 13:32:42 UTC",
      "updated_date": "2025-04-24 14:35:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:46:54.018819"
    },
    {
      "arxiv_id": "2503.21474v2",
      "title": "The Procedural Content Generation Benchmark: An Open-source Testbed for Generative Challenges in Games",
      "title_zh": "翻译失败",
      "authors": [
        "Ahmed Khalifa",
        "Roberto Gallotta",
        "Matthew Barthet",
        "Antonios Liapis",
        "Julian Togelius",
        "Georgios N. Yannakakis"
      ],
      "abstract": "This paper introduces the Procedural Content Generation Benchmark for\nevaluating generative algorithms on different game content creation tasks. The\nbenchmark comes with 12 game-related problems with multiple variants on each\nproblem. Problems vary from creating levels of different kinds to creating rule\nsets for simple arcade games. Each problem has its own content representation,\ncontrol parameters, and evaluation metrics for quality, diversity, and\ncontrollability. This benchmark is intended as a first step towards a\nstandardized way of comparing generative algorithms. We use the benchmark to\nscore three baseline algorithms: a random generator, an evolution strategy, and\na genetic algorithm. Results show that some problems are easier to solve than\nothers, as well as the impact the chosen objective has on quality, diversity,\nand controllability of the generated artifacts.",
      "tldr_zh": "本文引入了Procedural Content Generation Benchmark，这是一个开源测试平台，用于评估生成算法在游戏内容创建任务中的表现。该基准包含12个游戏相关问题，每个问题有多个变体，包括创建不同类型关卡和简单街机游戏规则集，并为每个问题定义了内容表示、控制参数以及评估指标（如质量、多样性和可控性）。研究者测试了三个基线算法：随机生成器、evolution strategy和genetic algorithm，结果显示某些问题更容易解决，同时选择的目标会对生成内容的质量、多样性和可控性产生显著影响。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages, 4 figures, 2 tables, published at FDG2025",
      "pdf_url": "http://arxiv.org/pdf/2503.21474v2",
      "published_date": "2025-03-27 13:05:40 UTC",
      "updated_date": "2025-03-28 08:19:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:47:06.921090"
    },
    {
      "arxiv_id": "2503.21465v1",
      "title": "Retinal Fundus Multi-Disease Image Classification using Hybrid CNN-Transformer-Ensemble Architectures",
      "title_zh": "翻译失败",
      "authors": [
        "Deependra Singh",
        "Saksham Agarwal",
        "Subhankar Mishra"
      ],
      "abstract": "Our research is motivated by the urgent global issue of a large population\naffected by retinal diseases, which are evenly distributed but underserved by\nspecialized medical expertise, particularly in non-urban areas. Our primary\nobjective is to bridge this healthcare gap by developing a comprehensive\ndiagnostic system capable of accurately predicting retinal diseases solely from\nfundus images. However, we faced significant challenges due to limited, diverse\ndatasets and imbalanced class distributions. To overcome these issues, we have\ndevised innovative strategies. Our research introduces novel approaches,\nutilizing hybrid models combining deeper Convolutional Neural Networks (CNNs),\nTransformer encoders, and ensemble architectures sequentially and in parallel\nto classify retinal fundus images into 20 disease labels. Our overarching goal\nis to assess these advanced models' potential in practical applications, with a\nstrong focus on enhancing retinal disease diagnosis accuracy across a broader\nspectrum of conditions. Importantly, our efforts have surpassed baseline model\nresults, with the C-Tran ensemble model emerging as the leader, achieving a\nremarkable model score of 0.9166, surpassing the baseline score of 0.9.\nAdditionally, experiments with the IEViT model showcased equally promising\noutcomes with improved computational efficiency. We've also demonstrated the\neffectiveness of dynamic patch extraction and the integration of domain\nknowledge in computer vision tasks. In summary, our research strives to\ncontribute significantly to retinal disease diagnosis, addressing the critical\nneed for accessible healthcare solutions in underserved regions while aiming\nfor comprehensive and accurate disease prediction.",
      "tldr_zh": "该研究针对全球眼底疾病患者众多且医疗资源不足的问题，开发了一种基于眼底图像的诊断系统，能够预测 20 种疾病标签。研究采用创新的混合模型，包括更深层的 CNN、Transformer 编码器和集成架构（顺序和并行），并整合动态补丁提取和领域知识来应对数据集有限和类别不平衡的挑战。结果显示，C-Tran 集成模型得分达到 0.9166，超过基线 0.9，而 IEViT 模型则在计算效率上表现出色。该方法显著提升了眼底疾病诊断的准确性，有助于桥接医疗差距并推广到欠发达地区。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "68T10, 68T45, 92C55",
        "I.2.10; I.5.4; J.3"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages, 3 figures, 7 tables. Conference paper presented at the\n  International Health Informatics Conference (IHIC 2023)",
      "pdf_url": "http://arxiv.org/pdf/2503.21465v1",
      "published_date": "2025-03-27 12:55:07 UTC",
      "updated_date": "2025-03-27 12:55:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:47:19.294428"
    },
    {
      "arxiv_id": "2503.21464v1",
      "title": "Harnessing Chain-of-Thought Metadata for Task Routing and Adversarial Prompt Detection",
      "title_zh": "利用链式思维元数据进行任务路由和对抗性提示检测",
      "authors": [
        "Ryan Marinelli",
        "Josef Pichlmeier",
        "Tamas Bisztray"
      ],
      "abstract": "In this work, we propose a metric called Number of Thoughts (NofT) to\ndetermine the difficulty of tasks pre-prompting and support Large Language\nModels (LLMs) in production contexts. By setting thresholds based on the number\nof thoughts, this metric can discern the difficulty of prompts and support more\neffective prompt routing. A 2% decrease in latency is achieved when routing\nprompts from the MathInstruct dataset through quantized, distilled versions of\nDeepseek with 1.7 billion, 7 billion, and 14 billion parameters. Moreover, this\nmetric can be used to detect adversarial prompts used in prompt injection\nattacks with high efficacy. The Number of Thoughts can inform a classifier that\nachieves 95% accuracy in adversarial prompt detection. Our experiments ad\ndatasets used are available on our GitHub page:\nhttps://github.com/rymarinelli/Number_Of_Thoughts/tree/main.",
      "tldr_zh": "本研究提出了一种名为 Number of Thoughts (NofT) 的指标，用于评估任务难度并支持 Large Language Models (LLMs) 在生产环境中的提示路由。通过基于思考数量设置阈值，NofT 指标能够有效区分提示难度，并在 MathInstruct 数据集上路由提示时，使用量化蒸馏的 Deepseek 模型（1.7 亿、70 亿和 140 亿参数）实现了 2% 的延迟减少。此外，NofT 指标还可用于检测对抗性提示，如提示注入攻击，支持一个准确率达 95% 的分类器。实验数据集和代码已在 GitHub 上公开。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21464v1",
      "published_date": "2025-03-27 12:54:00 UTC",
      "updated_date": "2025-03-27 12:54:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:47:30.701277"
    },
    {
      "arxiv_id": "2503.21463v1",
      "title": "Unveiling Latent Information in Transaction Hashes: Hypergraph Learning for Ethereum Ponzi Scheme Detection",
      "title_zh": "揭示交易哈希中的潜在信息：超图学习用于以太坊庞氏骗局检测",
      "authors": [
        "Junhao Wu",
        "Yixin Yang",
        "Chengxiang Jin",
        "Silu Mu",
        "Xiaolei Qian",
        "Jiajun Zhou",
        "Shanqing Yu",
        "Qi Xuan"
      ],
      "abstract": "With the widespread adoption of Ethereum, financial frauds such as Ponzi\nschemes have become increasingly rampant in the blockchain ecosystem, posing\nsignificant threats to the security of account assets. Existing Ethereum fraud\ndetection methods typically model account transactions as graphs, but this\napproach primarily focuses on binary transactional relationships between\naccounts, failing to adequately capture the complex multi-party interaction\npatterns inherent in Ethereum. To address this, we propose a hypergraph\nmodeling method for the Ponzi scheme detection method in Ethereum, called\nHyperDet. Specifically, we treat transaction hashes as hyperedges that connect\nall the relevant accounts involved in a transaction. Additionally, we design a\ntwo-step hypergraph sampling strategy to significantly reduce computational\ncomplexity. Furthermore, we introduce a dual-channel detection module,\nincluding the hypergraph detection channel and the hyper-homo graph detection\nchannel, to be compatible with existing detection methods. Experimental results\nshow that, compared to traditional homogeneous graph-based methods, the\nhyper-homo graph detection channel achieves significant performance\nimprovements, demonstrating the superiority of hypergraph in Ponzi scheme\ndetection. This research offers innovations for modeling complex relationships\nin blockchain data.",
      "tldr_zh": "这篇论文针对Ethereum上的Ponzi scheme检测问题，提出了一种基于hypergraph学习的HyperDet方法，将transaction hashes视为hyperedges来捕捉多方账户互动的复杂关系。方法包括两步hypergraph采样策略，以显著降低计算复杂度，并引入双通道检测模块（hypergraph检测通道和hyper-homo graph检测通道），以兼容现有技术。实验结果显示，与传统同质图方法相比，HyperDet的性能提升显著，证明了hypergraph在区块链欺诈检测中的优势。该研究为建模区块链数据的复杂关系提供了创新性贡献。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21463v1",
      "published_date": "2025-03-27 12:52:47 UTC",
      "updated_date": "2025-03-27 12:52:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:47:42.649953"
    },
    {
      "arxiv_id": "2503.21435v1",
      "title": "Graph-to-Vision: Multi-graph Understanding and Reasoning using Vision-Language Models",
      "title_zh": "Graph-to-Vision：利用视觉语言模型进行多图理解和推理",
      "authors": [
        "Ruizhou Li",
        "Haiyun Jiang"
      ],
      "abstract": "Graph Neural Networks (GNNs), as the dominant paradigm for graph-structured\nlearning, have long faced dual challenges of exponentially escalating\ncomputational complexity and inadequate cross-scenario generalization\ncapability. With the rapid advancement of multimodal learning, Vision-Language\nModels (VLMs) have demonstrated exceptional cross-modal relational reasoning\ncapabilities and generalization capacities, thereby opening up novel pathways\nfor overcoming the inherent limitations of conventional graph learning\nparadigms. However, current research predominantly concentrates on\ninvestigating the single-graph reasoning capabilities of VLMs, which\nfundamentally fails to address the critical requirement for coordinated\nreasoning across multiple heterogeneous graph data in real-world application\nscenarios. To address these limitations, we propose the first multi-graph joint\nreasoning benchmark for VLMs. Our benchmark encompasses four graph categories:\nknowledge graphs, flowcharts, mind maps, and route maps,with each graph group\naccompanied by three progressively challenging instruction-response pairs.\nLeveraging this benchmark, we conducted comprehensive capability assessments of\nstate-of-the-art VLMs and performed fine-tuning on open-source models. This\nstudy not only addresses the underexplored evaluation gap in multi-graph\nreasoning for VLMs but also empirically validates their generalization\nsuperiority in graph-structured learning.",
      "tldr_zh": "该论文指出了Graph Neural Networks (GNNs) 在计算复杂性和泛化能力方面的局限性，并探讨了Vision-Language Models (VLMs) 在跨模态推理中的潜力，以解决传统图学习范式的不足。研究首次提出一个多图联合推理基准，涵盖知识图(knowledge graphs)、流程图(flowcharts)、思维导图(mind maps)和路线图(route maps)，每个类别包括三对渐进挑战的指令-响应对。利用该基准，对最先进的VLMs进行全面评估和开源模型微调，结果验证了VLMs在多图推理中的泛化优势，并填补了现有研究的评估空白。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21435v1",
      "published_date": "2025-03-27 12:20:37 UTC",
      "updated_date": "2025-03-27 12:20:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:47:55.619172"
    },
    {
      "arxiv_id": "2503.21422v1",
      "title": "From Deep Learning to LLMs: A survey of AI in Quantitative Investment",
      "title_zh": "翻译失败",
      "authors": [
        "Bokai Cao",
        "Saizhuo Wang",
        "Xinyi Lin",
        "Xiaojun Wu",
        "Haohan Zhang",
        "Lionel M. Ni",
        "Jian Guo"
      ],
      "abstract": "Quantitative investment (quant) is an emerging, technology-driven approach in\nasset management, increasingy shaped by advancements in artificial\nintelligence. Recent advances in deep learning and large language models (LLMs)\nfor quant finance have improved predictive modeling and enabled agent-based\nautomation, suggesting a potential paradigm shift in this field. In this\nsurvey, taking alpha strategy as a representative example, we explore how AI\ncontributes to the quantitative investment pipeline. We first examine the early\nstage of quant research, centered on human-crafted features and traditional\nstatistical models with an established alpha pipeline. We then discuss the rise\nof deep learning, which enabled scalable modeling across the entire pipeline\nfrom data processing to order execution. Building on this, we highlight the\nemerging role of LLMs in extending AI beyond prediction, empowering autonomous\nagents to process unstructured data, generate alphas, and support\nself-iterative workflows.",
      "tldr_zh": "这篇调查回顾了人工智能（AI）在量化投资（Quantitative Investment）中的应用演变，从早期的人工设计特征和传统统计模型，到Deep Learning的兴起，该技术实现了从数据处理到订单执行的全面可扩展建模。以alpha策略为例，论文强调了Large Language Models (LLMs)的创新作用，使AI超越预测功能，支持自主代理处理非结构化数据、生成alphas并实现自迭代工作流。总体而言，该调查揭示了AI如何推动量化投资领域的范式转变，提升预测准确性和自动化水平。",
      "categories": [
        "q-fin.CP",
        "cs.AI",
        "cs.LG",
        "q-fin.ST",
        "q-fin.TR"
      ],
      "primary_category": "q-fin.CP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21422v1",
      "published_date": "2025-03-27 12:10:15 UTC",
      "updated_date": "2025-03-27 12:10:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:48:06.899170"
    },
    {
      "arxiv_id": "2503.21419v3",
      "title": "Neuroplasticity in Artificial Intelligence -- An Overview and Inspirations on Drop In & Out Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Yupei Li",
        "Manuel Milling",
        "Björn W. Schuller"
      ],
      "abstract": "Artificial Intelligence (AI) has achieved new levels of performance and\nspread in public usage with the rise of deep neural networks (DNNs). Initially\ninspired by human neurons and their connections, NNs have become the foundation\nof AI models for many advanced architectures. However, some of the most\nintegral processes in the human brain, particularly neurogenesis and\nneuroplasticity in addition to the more spread neuroapoptosis have largely been\nignored in DNN architecture design. Instead, contemporary AI development\npredominantly focuses on constructing advanced frameworks, such as large\nlanguage models, which retain a static structure of neural connections during\ntraining and inference. In this light, we explore how neurogenesis,\nneuroapoptosis, and neuroplasticity can inspire future AI advances.\nSpecifically, we examine analogous activities in artificial NNs, introducing\nthe concepts of ``dropin'' for neurogenesis and revisiting ``dropout'' and\nstructural pruning for neuroapoptosis. We additionally suggest neuroplasticity\ncombining the two for future large NNs in ``life-long learning'' settings\nfollowing the biological inspiration. We conclude by advocating for greater\nresearch efforts in this interdisciplinary domain and identifying promising\ndirections for future exploration.",
      "tldr_zh": "这篇论文概述了大脑的神经可塑性（neuroplasticity）、神经发生（neurogenesis）和神经凋亡（neuroapoptosis）如何启发人工智能（AI）的发展，特别是针对深度神经网络（DNNs）的设计改进。作者引入了“dropin”作为神经发生的类比，并重温“dropout”和结构修剪（structural pruning）作为神经凋亡的对应机制，同时建议将这些元素整合到神经可塑性中，以支持大型神经网络在终身学习（life-long learning）场景下的应用。论文强调了这一跨学科领域的潜力，并呼吁更多研究探索这些生物启发方向。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21419v3",
      "published_date": "2025-03-27 12:09:04 UTC",
      "updated_date": "2025-04-25 07:54:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:48:19.177225"
    },
    {
      "arxiv_id": "2503.21412v1",
      "title": "Federated Intelligence: When Large AI Models Meet Federated Fine-Tuning and Collaborative Reasoning at the Network Edge",
      "title_zh": "翻译失败",
      "authors": [
        "Wanli Ni",
        "Haofeng Sun",
        "Huiqing Ao",
        "Hui Tian"
      ],
      "abstract": "Large artificial intelligence (AI) models exhibit remarkable capabilities in\nvarious application scenarios, but deploying them at the network edge poses\nsignificant challenges due to issues such as data privacy, computational\nresources, and latency. In this paper, we explore federated fine-tuning and\ncollaborative reasoning techniques to facilitate the implementation of large AI\nmodels in resource-constrained wireless networks. Firstly, promising\napplications of large AI models within specific domains are discussed.\nSubsequently, federated fine-tuning methods are proposed to adapt large AI\nmodels to specific tasks or environments at the network edge, effectively\naddressing the challenges associated with communication overhead and enhancing\ncommunication efficiency. These methodologies follow clustered, hierarchical,\nand asynchronous paradigms to effectively tackle privacy issues and eliminate\ndata silos. Furthermore, to enhance operational efficiency and reduce latency,\nefficient frameworks for model collaborative reasoning are developed, which\ninclude decentralized horizontal collaboration, cloud-edge-end vertical\ncollaboration, and multi-access collaboration. Next, simulation results\ndemonstrate the effectiveness of our proposed methods in reducing the\nfine-tuning loss of large AI models across various downstream tasks. Finally,\nseveral open challenges and research opportunities are outlined.",
      "tldr_zh": "该论文探讨了在网络边缘部署大型 AI 模型时面临的挑战，如数据隐私、计算资源和延迟问题，并提出 federated fine-tuning 和 collaborative reasoning 技术作为解决方案。通过 clustered、hierarchical 和 asynchronous 范式，federated fine-tuning 方法适应特定任务，减少通信开销并解决隐私和数据孤岛问题。同时，开发了 collaborative reasoning 框架，包括 decentralized horizontal collaboration、cloud-edge-end vertical collaboration 和 multi-access collaboration，以提升操作效率和降低延迟。模拟结果显示，这些方法在各种下游任务中显著降低了 fine-tuning 损失。最后，论文概述了相关领域的开放挑战和研究机会。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.21412v1",
      "published_date": "2025-03-27 11:56:36 UTC",
      "updated_date": "2025-03-27 11:56:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:48:30.989065"
    },
    {
      "arxiv_id": "2503.21411v1",
      "title": "Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap",
      "title_zh": "翻译失败",
      "authors": [
        "Tong Nie",
        "Jian Sun",
        "Wei Ma"
      ],
      "abstract": "Modern transportation systems face pressing challenges due to increasing\ndemand, dynamic environments, and heterogeneous information integration. The\nrapid evolution of Large Language Models (LLMs) offers transformative potential\nto address these challenges. Extensive knowledge and high-level capabilities\nderived from pretraining evolve the default role of LLMs as text generators to\nbecome versatile, knowledge-driven task solvers for intelligent transportation\nsystems. This survey first presents LLM4TR, a novel conceptual framework that\nsystematically categorizes the roles of LLMs in transportation into four\nsynergetic dimensions: information processors, knowledge encoders, component\ngenerators, and decision facilitators. Through a unified taxonomy, we\nsystematically elucidate how LLMs bridge fragmented data pipelines, enhance\npredictive analytics, simulate human-like reasoning, and enable closed-loop\ninteractions across sensing, learning, modeling, and managing tasks in\ntransportation systems. For each role, our review spans diverse applications,\nfrom traffic prediction and autonomous driving to safety analytics and urban\nmobility optimization, highlighting how emergent capabilities of LLMs such as\nin-context learning and step-by-step reasoning can enhance the operation and\nmanagement of transportation systems. We further curate practical guidance,\nincluding available resources and computational guidelines, to support\nreal-world deployment. By identifying challenges in existing LLM-based\nsolutions, this survey charts a roadmap for advancing LLM-driven transportation\nresearch, positioning LLMs as central actors in the next generation of\ncyber-physical-social mobility ecosystems. Online resources can be found in the\nproject page: https://github.com/tongnie/awesome-llm4tr.",
      "tldr_zh": "这篇调查论文探讨了Large Language Models (LLMs) 在重塑交通系统中的作用，旨在解决需求增长、动态环境和信息整合等挑战。论文提出了LLM4TR框架，将LLMs 的角色系统分为四个维度：信息处理器、知识编码器、组件生成器和决策促进器，以桥接数据管道、提升预测分析、模拟人类推理，并实现交通系统的闭环互动。研究涵盖了从交通预测、自动驾驶到安全分析和城市流动性优化的多样应用，突出了LLMs 的in-context learning和step-by-step reasoning等能力。最终，该论文提供了实际部署指导、资源推荐，并绘制了未来路线图，以推动LLM驱动的交通研究发展。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21411v1",
      "published_date": "2025-03-27 11:56:27 UTC",
      "updated_date": "2025-03-27 11:56:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:48:42.969314"
    },
    {
      "arxiv_id": "2503.21406v1",
      "title": "Neuro-Symbolic Imitation Learning: Discovering Symbolic Abstractions for Skill Learning",
      "title_zh": "神经符号模仿学习：发现符号抽象用于技能学习",
      "authors": [
        "Leon Keller",
        "Daniel Tanneberg",
        "Jan Peters"
      ],
      "abstract": "Imitation learning is a popular method for teaching robots new behaviors.\nHowever, most existing methods focus on teaching short, isolated skills rather\nthan long, multi-step tasks. To bridge this gap, imitation learning algorithms\nmust not only learn individual skills but also an abstract understanding of how\nto sequence these skills to perform extended tasks effectively. This paper\naddresses this challenge by proposing a neuro-symbolic imitation learning\nframework. Using task demonstrations, the system first learns a symbolic\nrepresentation that abstracts the low-level state-action space. The learned\nrepresentation decomposes a task into easier subtasks and allows the system to\nleverage symbolic planning to generate abstract plans. Subsequently, the system\nutilizes this task decomposition to learn a set of neural skills capable of\nrefining abstract plans into actionable robot commands. Experimental results in\nthree simulated robotic environments demonstrate that, compared to baselines,\nour neuro-symbolic approach increases data efficiency, improves generalization\ncapabilities, and facilitates interpretability.",
      "tldr_zh": "本文提出了一种神经符号化（Neuro-Symbolic）模仿学习框架，旨在解决现有方法在处理长期多步骤任务时的局限性，通过学习符号表示来抽象低级状态-动作空间。框架首先利用任务演示将任务分解为子任务，并借助符号规划（Symbolic Planning）生成抽象计划，然后训练神经技能将这些计划转化为可执行的机器人命令。实验结果显示，在三个模拟机器人环境中，该方法相较基线提高了数据效率、泛化能力和可解释性。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "IEEE International Conference on Robotics and Automation (ICRA) 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.21406v1",
      "published_date": "2025-03-27 11:50:29 UTC",
      "updated_date": "2025-03-27 11:50:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:48:53.837100"
    },
    {
      "arxiv_id": "2503.22755v2",
      "title": "Reasoning Under Threat: Symbolic and Neural Techniques for Cybersecurity Verification",
      "title_zh": "威胁下的推理：符号和神经技术用于网络安全验证",
      "authors": [
        "Sarah Veronica"
      ],
      "abstract": "Cybersecurity demands rigorous and scalable techniques to ensure system\ncorrectness, robustness, and resilience against evolving threats. Automated\nreasoning, encompassing formal logic, theorem proving, model checking, and\nsymbolic analysis, provides a foundational framework for verifying security\nproperties across diverse domains such as access control, protocol design,\nvulnerability detection, and adversarial modeling. This survey presents a\ncomprehensive overview of the role of automated reasoning in cybersecurity,\nanalyzing how logical systems, including temporal, deontic, and epistemic\nlogics are employed to formalize and verify security guarantees. We examine\nSOTA tools and frameworks, explore integrations with AI for neural-symbolic\nreasoning, and highlight critical research gaps, particularly in scalability,\ncompositionality, and multi-layered security modeling. The paper concludes with\na set of well-grounded future research directions, aiming to foster the\ndevelopment of secure systems through formal, automated, and explainable\nreasoning techniques.",
      "tldr_zh": "这篇论文调查了自动化推理在网络安全中的应用，涵盖了 formal logic、theorem proving、model checking 和 symbolic analysis 等技术，以验证系统正确性、鲁棒性和抗威胁能力。论文分析了 temporal logic、deontic logic 和 epistemic logics 等逻辑系统如何用于形式化安全属性，并探讨了这些方法与 AI 的整合，如 neural-symbolic reasoning，以提升 SOTA tools 的效能，同时指出了可扩展性、组合性和多层安全建模等关键研究空白。最终，该论文提出未来研究方向，旨在通过正式的、自动化的和可解释的推理技术推动安全系统的开发。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22755v2",
      "published_date": "2025-03-27 11:41:53 UTC",
      "updated_date": "2025-05-12 23:27:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:49:06.225376"
    },
    {
      "arxiv_id": "2503.21393v2",
      "title": "An evaluation of LLMs and Google Translate for translation of selected Indian languages via sentiment and semantic analyses",
      "title_zh": "翻译失败",
      "authors": [
        "Rohitash Chandra",
        "Aryan Chaudhari",
        "Yeshwanth Rayavarapu"
      ],
      "abstract": "Large Language models (LLMs) have been prominent for language translation,\nincluding low-resource languages. There has been limited study about the\nassessment of the quality of translations generated by LLMs, including Gemini,\nGPT and Google Translate. In this study, we address this limitation by using\nsemantic and sentiment analysis of selected LLMs for Indian languages,\nincluding Sanskrit, Telugu and Hindi. We select prominent texts that have been\nwell translated by experts and use LLMs to generate their translations to\nEnglish, and then we provide a comparison with selected expert (human)\ntranslations. Our findings suggest that while LLMs have made significant\nprogress in translation accuracy, challenges remain in preserving sentiment and\nsemantic integrity, especially in figurative and philosophical contexts. The\nsentiment analysis revealed that GPT-4o and GPT-3.5 are better at preserving\nthe sentiments for the Bhagavad Gita (Sanskrit-English) translations when\ncompared to Google Translate. We observed a similar trend for the case of Tamas\n(Hindi-English) and Maha P (Telugu-English) translations. GPT-4o performs\nsimilarly to GPT-3.5 in the translation in terms of sentiments for the three\nlanguages. We found that LLMs are generally better at translation for capturing\nsentiments when compared to Google Translate.",
      "tldr_zh": "本研究评估了大型语言模型（LLMs，如 Gemini、GPT）和 Google Translate 在翻译印度低资源语言（Sanskrit、Telugu 和 Hindi）时的表现，通过语义和情感分析进行比较。研究选取了专家翻译的著名文本（如 Bhagavad Gita、Tamas 和 Maha P），并将 LLMs 生成的英语翻译与人类翻译对照分析。结果显示，LLMs 在翻译准确性上取得了显著进步，但仍存在在比喻和哲学语境中保留情感和语义完整性的挑战；具体而言，GPT-4o 和 GPT-3.5 在情感保留方面优于 Google Translate，且整体上 LLMs 在捕捉情感时表现更好。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21393v2",
      "published_date": "2025-03-27 11:35:40 UTC",
      "updated_date": "2025-04-02 03:17:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:49:19.042228"
    },
    {
      "arxiv_id": "2503.21392v2",
      "title": "HybridoNet-Adapt: A Domain-Adapted Framework for Accurate Lithium-Ion Battery RUL Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Khoa Tran",
        "Bao Huynh",
        "Tri Le",
        "Lam Pham",
        "Vy-Rin Nguyen",
        "Hung-Cuong Trinh",
        "Duong Tran Anh"
      ],
      "abstract": "Accurate prediction of the Remaining Useful Life (RUL) in Lithium ion battery\n(LIB) health management systems is essential for ensuring operational\nreliability and safety. However, many existing methods assume that training and\ntesting data follow the same distribution, limiting their ability to generalize\nto unseen target domains. To address this, we propose a novel RUL prediction\nframework that incorporates a domain adaptation (DA) technique. Our framework\nintegrates a signal preprocessing pipeline including noise reduction, feature\nextraction, and normalization with a robust deep learning model called\nHybridoNet Adapt. The model features a combination of LSTM, Multihead\nAttention, and Neural ODE layers for feature extraction, followed by two\npredictor modules with trainable trade-off parameters. To improve\ngeneralization, we adopt a DA strategy inspired by Domain Adversarial Neural\nNetworks (DANN), replacing adversarial loss with Maximum Mean Discrepancy (MMD)\nto learn domain-invariant features. Experimental results show that HybridoNet\nAdapt significantly outperforms traditional models such as XGBoost and Elastic\nNet, as well as deep learning baselines like Dual input DNN, demonstrating its\npotential for scalable and reliable battery health management (BHM).",
      "tldr_zh": "该研究提出HybridoNet-Adapt框架，用于准确预测锂离子电池的剩余可用寿命(RUL)，通过域适应(DA)技术解决训练和测试数据分布不一致的问题。该框架整合信号预处理（如噪声减少、特征提取和归一化）以及一个结合LSTM、Multihead Attention和Neural ODE层的深度学习模型，辅以两个预测模块和可训练权衡参数，并采用基于Maximum Mean Discrepancy (MMD)的DA策略来学习域不变特征。实验结果显示，HybridoNet-Adapt显著优于传统模型如XGBoost和Elastic Net，以及深度学习基线如Dual input DNN，在电池健康管理(BHM)中展现出更好的泛化性和可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21392v2",
      "published_date": "2025-03-27 11:35:25 UTC",
      "updated_date": "2025-04-18 13:22:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:49:30.580324"
    },
    {
      "arxiv_id": "2503.21356v1",
      "title": "Investigating the Duality of Interpretability and Explainability in Machine Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Moncef Garouani",
        "Josiane Mothe",
        "Ayah Barhrhouj",
        "Julien Aligon"
      ],
      "abstract": "The rapid evolution of machine learning (ML) has led to the widespread\nadoption of complex \"black box\" models, such as deep neural networks and\nensemble methods. These models exhibit exceptional predictive performance,\nmaking them invaluable for critical decision-making across diverse domains\nwithin society. However, their inherently opaque nature raises concerns about\ntransparency and interpretability, making them untrustworthy decision support\nsystems. To alleviate such a barrier to high-stakes adoption, research\ncommunity focus has been on developing methods to explain black box models as a\nmeans to address the challenges they pose. Efforts are focused on explaining\nthese models instead of developing ones that are inherently interpretable.\nDesigning inherently interpretable models from the outset, however, can pave\nthe path towards responsible and beneficial applications in the field of ML. In\nthis position paper, we clarify the chasm between explaining black boxes and\nadopting inherently interpretable models. We emphasize the imperative need for\nmodel interpretability and, following the purpose of attaining better (i.e.,\nmore effective or efficient w.r.t. predictive performance) and trustworthy\npredictors, provide an experimental evaluation of latest hybrid learning\nmethods that integrates symbolic knowledge into neural network predictors. We\ndemonstrate how interpretable hybrid models could potentially supplant black\nbox ones in different domains.",
      "tldr_zh": "该研究探讨了机器学习中 interpretability（可解释性）和 explainability（解释性）的二元性，指出当前对解释 black box models（黑盒模型）的关注可能忽略了从一开始设计固有可解释模型的必要性。\n论文强调，采用固有可解释模型有助于提升预测性能和可信任性，从而促进机器学习在高风险领域的负责任应用。\n通过实验评估，将 symbolic knowledge（符号知识）整合到 neural network predictors（神经网络预测器）中的 hybrid learning methods（混合学习方法），结果显示这些模型在不同领域有潜力取代 black box models。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21356v1",
      "published_date": "2025-03-27 10:48:40 UTC",
      "updated_date": "2025-03-27 10:48:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:49:42.674533"
    },
    {
      "arxiv_id": "2503.21352v1",
      "title": "Using large language models to produce literature reviews: Usages and systematic biases of microphysics parametrizations in 2699 publications",
      "title_zh": "翻译失败",
      "authors": [
        "Tianhang Zhang",
        "Shengnan Fu",
        "David M. Schultz",
        "Zhonghua Zheng"
      ],
      "abstract": "Large language models afford opportunities for using computers for intensive\ntasks, realizing research opportunities that have not been considered before.\nOne such opportunity could be a systematic interrogation of the scientific\nliterature. Here, we show how a large language model can be used to construct a\nliterature review of 2699 publications associated with microphysics\nparametrizations in the Weather and Research Forecasting (WRF) model, with the\ngoal of learning how they were used and their systematic biases, when\nsimulating precipitation. The database was constructed of publications\nidentified from Web of Science and Scopus searches. The large language model\nGPT-4 Turbo was used to extract information about model configurations and\nperformance from the text of 2699 publications. Our results reveal the\nlandscape of how nine of the most popular microphysics parameterizations have\nbeen used around the world: Lin, Ferrier, WRF Single-Moment, Goddard Cumulus\nEnsemble, Morrison, Thompson, and WRF Double-Moment. More studies used\none-moment parameterizations before 2020 and two-moment parameterizations after\n2020. Seven out of nine parameterizations tended to overestimate precipitation.\nHowever, systematic biases of parameterizations differed in various regions.\nExcept simulations using the Lin, Ferrier, and Goddard parameterizations that\ntended to underestimate precipitation over almost all locations, the remaining\nsix parameterizations tended to overestimate, particularly over China,\nsoutheast Asia, western United States, and central Africa. This method could be\nused by other researchers to help understand how the increasingly massive body\nof scientific literature can be harnessed through the power of artificial\nintelligence to solve their research problems.",
      "tldr_zh": "本研究利用大型语言模型（Large Language Models, LLM）如GPT-4 Turbo，自动分析2699篇与Weather and Research Forecasting (WRF)模型微物理参数化相关的出版物，构建文献综述，以探讨这些参数化在模拟降水时的使用情况和系统偏差。结果显示，九个最受欢迎的参数化（包括Lin, Ferrier, WRF Single-Moment, Goddard Cumulus Ensemble, Morrison, Thompson和WRF Double-Moment）中，2020年前更多采用一时刻参数化，而后则偏向二时刻参数化。七个参数化倾向于高估降水，且区域差异明显：Lin, Ferrier和Goddard在几乎所有地区低估降水，而其他六个尤其在中国、东南亚、美国西部和中非地区高估降水。此方法为研究者提供了一种利用人工智能高效处理海量科学文献的新途径。",
      "categories": [
        "cs.AI",
        "stat.AP"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21352v1",
      "published_date": "2025-03-27 10:42:19 UTC",
      "updated_date": "2025-03-27 10:42:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:49:55.582204"
    },
    {
      "arxiv_id": "2503.22754v1",
      "title": "Model Lake: a New Alternative for Machine Learning Models Management and Governance",
      "title_zh": "Model Lake：一种机器学习模型管理与治理的新替代方案",
      "authors": [
        "Moncef Garouani",
        "Franck Ravat",
        "Nathalie Valles-Parlangeau"
      ],
      "abstract": "The rise of artificial intelligence and data science across industries\nunderscores the pressing need for effective management and governance of\nmachine learning (ML) models. Traditional approaches to ML models management\noften involve disparate storage systems and lack standardized methodologies for\nversioning, audit, and re-use. Inspired by data lake concepts, this paper\ndevelops the concept of ML Model Lake as a centralized management framework for\ndatasets, codes, and models within organizations environments. We provide an\nin-depth exploration of the Model Lake concept, delineating its architectural\nfoundations, key components, operational benefits, and practical challenges. We\ndiscuss the transformative potential of adopting a Model Lake approach, such as\nenhanced model lifecycle management, discovery, audit, and reusability.\nFurthermore, we illustrate a real-world application of Model Lake and its\ntransformative impact on data, code and model management practices.",
      "tldr_zh": "该论文指出，AI 和数据科学在各行业的兴起凸显了机器学习（ML）模型管理与治理的迫切需求，但传统方法往往涉及分散的存储系统，并缺乏标准化的 versioning、audit 和 re-use 机制。作者提出 Model Lake 概念，这是一种受数据湖启发的集中式框架，用于组织环境中的数据集、代码和模型的管理，包括其架构基础、关键组件、操作益处（如提升模型生命周期管理、discovery 和 reusability）以及潜在挑战。论文通过真实世界应用示例，展示了 Model Lake 如何革新数据、代码和模型的管理实践。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22754v1",
      "published_date": "2025-03-27 10:35:51 UTC",
      "updated_date": "2025-03-27 10:35:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:50:06.720074"
    },
    {
      "arxiv_id": "2503.21347v1",
      "title": "Residual Learning Inspired Crossover Operator and Strategy Enhancements for Evolutionary Multitasking",
      "title_zh": "受残差学习启发的交叉算子及策略增强",
      "authors": [
        "Ruilin Wang",
        "Xiang Feng",
        "Huiqun Yu",
        "Edmund M-K Lai"
      ],
      "abstract": "In evolutionary multitasking, strategies such as crossover operators and\nskill factor assignment are critical for effective knowledge transfer. Existing\nimprovements to crossover operators primarily focus on low-dimensional variable\ncombinations, such as arithmetic crossover or partially mapped crossover, which\nare insufficient for modeling complex high-dimensional interactions.Moreover,\nstatic or semi-dynamic crossover strategies fail to adapt to the dynamic\ndependencies among tasks. In addition, current Multifactorial Evolutionary\nAlgorithm frameworks often rely on fixed skill factor assignment strategies,\nlacking flexibility. To address these limitations, this paper proposes the\nMultifactorial Evolutionary Algorithm-Residual Learning (MFEA-RL) method based\non residual learning. The method employs a Very Deep Super-Resolution (VDSR)\nmodel to generate high-dimensional residual representations of individuals,\nenhancing the modeling of complex relationships within dimensions. A\nResNet-based mechanism dynamically assigns skill factors to improve task\nadaptability, while a random mapping mechanism efficiently performs crossover\noperations and mitigates the risk of negative transfer. Theoretical analysis\nand experimental results show that MFEA-RL outperforms state-of-the-art\nmultitasking algorithms. It excels in both convergence and adaptability on\nstandard evolutionary multitasking benchmarks, including CEC2017-MTSO and\nWCCI2020-MTSO. Additionally, its effectiveness is validated through a\nreal-world application scenario.",
      "tldr_zh": "本论文针对进化多任务优化中的交叉算子（crossover operators）和技能因子分配问题，提出了 Multifactorial Evolutionary Algorithm-Residual Learning (MFEA-RL) 方法，基于残差学习（residual learning）来提升高维变量交互建模和动态适应能力。\n该方法使用 Very Deep Super-Resolution (VDSR) 模型生成高维残差表示，通过 ResNet-based 机制动态分配技能因子，并引入随机映射机制进行交叉操作，以减少负面转移风险。\n实验结果表明，MFEA-RL 在 CEC2017-MTSO 和 WCCI2020-MTSO 等基准测试中表现出优越的收敛性和适应性，并已在真实世界应用场景中验证其有效性。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "9 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.21347v1",
      "published_date": "2025-03-27 10:27:17 UTC",
      "updated_date": "2025-03-27 10:27:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:50:19.598979"
    },
    {
      "arxiv_id": "2503.21337v1",
      "title": "A 71.2-$μ$W Speech Recognition Accelerator with Recurrent Spiking Neural Network",
      "title_zh": "翻译失败",
      "authors": [
        "Chih-Chyau Yang",
        "Tian-Sheuan Chang"
      ],
      "abstract": "This paper introduces a 71.2-$\\mu$W speech recognition accelerator designed\nfor edge devices' real-time applications, emphasizing an ultra low power\ndesign. Achieved through algorithm and hardware co-optimizations, we propose a\ncompact recurrent spiking neural network with two recurrent layers, one fully\nconnected layer, and a low time step (1 or 2). The 2.79-MB model undergoes\npruning and 4-bit fixed-point quantization, shrinking it by 96.42\\% to 0.1 MB.\nOn the hardware front, we take advantage of \\textit{mixed-level pruning},\n\\textit{zero-skipping} and \\textit{merged spike} techniques, reducing\ncomplexity by 90.49\\% to 13.86 MMAC/S. The \\textit{parallel time-step\nexecution} addresses inter-time-step data dependencies and enables weight\nbuffer power savings through weight sharing. Capitalizing on the sparse spike\nactivity, an input broadcasting scheme eliminates zero computations, further\nsaving power. Implemented on the TSMC 28-nm process, the design operates in\nreal time at 100 kHz, consuming 71.2 $\\mu$W, surpassing state-of-the-art\ndesigns. At 500 MHz, it has 28.41 TOPS/W and 1903.11 GOPS/mm$^2$ in energy and\narea efficiency, respectively.",
      "tldr_zh": "这篇论文提出了一种功耗仅 71.2 μW 的语音识别加速器，针对边缘设备的实时应用，通过算法和硬件协同优化实现超低功耗设计。加速器采用一个紧凑的 recurrent spiking neural network，包括两个 recurrent 层、一个 fully connected 层和低时间步（1或2），并通过 pruning 和 4-bit fixed-point quantization 将 2.79 MB 模型缩小 96.42% 到 0.1 MB。硬件优化利用 mixed-level pruning、zero-skipping 和 merged spike 技术，减少复杂度 90.49% 到 13.86 MMAC/S，同时通过 parallel time-step execution 和输入广播方案消除零计算，进一步节省功耗。在 TSMC 28-nm 工艺上，该设计在 100 kHz 实时运行，实现 28.41 TOPS/W 能量效率和 1903.11 GOPS/mm² 面积效率，超越现有状态-of-the-art 设计。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21337v1",
      "published_date": "2025-03-27 10:14:00 UTC",
      "updated_date": "2025-03-27 10:14:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:50:32.971880"
    },
    {
      "arxiv_id": "2503.21335v1",
      "title": "A Low-Power Streaming Speech Enhancement Accelerator For Edge Devices",
      "title_zh": "翻译失败",
      "authors": [
        "Ci-Hao Wu",
        "Tian-Sheuan Chang"
      ],
      "abstract": "Transformer-based speech enhancement models yield impressive results.\nHowever, their heterogeneous and complex structure restricts model compression\npotential, resulting in greater complexity and reduced hardware efficiency.\nAdditionally, these models are not tailored for streaming and low-power\napplications. Addressing these challenges, this paper proposes a low-power\nstreaming speech enhancement accelerator through model and hardware\noptimization. The proposed high performance model is optimized for hardware\nexecution with the co-design of model compression and target application, which\nreduces 93.9\\% of model size by the proposed domain-aware and streaming-aware\npruning techniques. The required latency is further reduced with batch\nnormalization-based transformers. Additionally, we employed softmax-free\nattention, complemented by an extra batch normalization, facilitating simpler\nhardware design. The tailored hardware accommodates these diverse computing\npatterns by breaking them down into element-wise multiplication and\naccumulation (MAC). This is achieved through a 1-D processing array, utilizing\nconfigurable SRAM addressing, thereby minimizing hardware complexities and\nsimplifying zero skipping. Using the TSMC 40nm CMOS process, the final\nimplementation requires merely 207.8K gates and 53.75KB SRAM. It consumes only\n8.08 mW for real-time inference at a 62.5MHz frequency.",
      "tldr_zh": "这篇论文提出了一种低功耗流式语音增强加速器，针对Transformer-based语音增强模型的复杂结构和硬件效率问题，进行模型和硬件优化。该方法通过域感知和流式感知修剪技术减少模型大小93.9%，并采用批标准化Transformer和无softmax注意力机制，进一步降低延迟和简化设计。最终硬件实现基于TSMC 40nm CMOS工艺，仅需207.8K gates和53.75KB SRAM，在62.5MHz频率下实现实时推理，功耗仅为8.08 mW。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21335v1",
      "published_date": "2025-03-27 10:13:41 UTC",
      "updated_date": "2025-03-27 10:13:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:50:43.175128"
    },
    {
      "arxiv_id": "2503.21332v1",
      "title": "ReFeed: Multi-dimensional Summarization Refinement with Reflective Reasoning on Feedback",
      "title_zh": "翻译失败",
      "authors": [
        "Taewon Yun",
        "Jihwan Oh",
        "Hyangsuk Min",
        "Yuho Lee",
        "Jihwan Bang",
        "Jason Cai",
        "Hwanjun Song"
      ],
      "abstract": "Summarization refinement faces challenges when extending to multi-dimension.\nIn this paper, we introduce ReFeed, a powerful summarization refinement\npipeline that enhances multiple dimensions through reflective reasoning on\nfeedback. To achieve this, we release SumFeed-CoT, a large-scale Long-CoT-based\ndataset optimized for training a lightweight model with reflective reasoning.\nOur experiments reveal how the number of dimensions, feedback exposure, and\nreasoning policy influence refinement performance, highlighting reflective\nreasoning and simultaneously addressing multiple feedback is crucial to\nmitigate trade-off between dimensions. Furthermore, ReFeed is robust to noisy\nfeedback and feedback order. Lastly, our finding emphasizes that creating data\nwith a proper goal and guideline constitutes a fundamental pillar of effective\nreasoning. The dataset and model will be released.",
      "tldr_zh": "本研究引入了 ReFeed，一种通过对反馈进行反思性 reasoning 的多维度摘要优化管道，旨在解决摘要精炼在多维度扩展中的挑战。论文发布了 SumFeed-CoT，这是一个基于 Long-CoT 的大规模数据集，用于训练轻量级模型以实现有效反思性推理。实验结果显示，维度数量、反馈暴露和推理策略会影响性能，而反思性 reasoning 和同时处理多个反馈有助于缓解维度间的 trade-off，且 ReFeed 对噪声反馈和反馈顺序具有鲁棒性。最后，论文强调创建数据时设定适当的目标和指导方针是有效 reasoning 的核心基础，并计划发布数据集和模型。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21332v1",
      "published_date": "2025-03-27 10:11:41 UTC",
      "updated_date": "2025-03-27 10:11:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:50:56.826059"
    },
    {
      "arxiv_id": "2503.21322v2",
      "title": "HyperGraphRAG: Retrieval-Augmented Generation via Hypergraph-Structured Knowledge Representation",
      "title_zh": "HyperGraphRAG：通过超图结构知识表示的检索增强生成",
      "authors": [
        "Haoran Luo",
        "Haihong E",
        "Guanting Chen",
        "Yandan Zheng",
        "Xiaobao Wu",
        "Yikai Guo",
        "Qika Lin",
        "Yu Feng",
        "Zemin Kuang",
        "Meina Song",
        "Yifan Zhu",
        "Luu Anh Tuan"
      ],
      "abstract": "Standard Retrieval-Augmented Generation (RAG) relies on chunk-based\nretrieval, whereas GraphRAG advances this approach by graph-based knowledge\nrepresentation. However, existing graph-based RAG approaches are constrained by\nbinary relations, as each edge in an ordinary graph connects only two entities,\nlimiting their ability to represent the n-ary relations (n >= 2) in real-world\nknowledge. In this work, we propose HyperGraphRAG, a novel hypergraph-based RAG\nmethod that represents n-ary relational facts via hyperedges, and consists of\nknowledge hypergraph construction, retrieval, and generation. Experiments\nacross medicine, agriculture, computer science, and law demonstrate that\nHyperGraphRAG outperforms both standard RAG and previous graph-based RAG\nmethods in answer accuracy, retrieval efficiency, and generation quality.",
      "tldr_zh": "本论文提出 HyperGraphRAG，一种基于超图结构的 Retrieval-Augmented Generation (RAG) 方法，以克服标准 RAG 和 GraphRAG 在处理 n-ary relations (n >= 2) 时受限于二元关系的局限性。该方法通过超边 (hyperedges) 表示多元关系，并包括知识超图构建、检索和生成三个关键组件。实验在医学、农业、计算机科学和法律领域表明，HyperGraphRAG 在答案准确性、检索效率和生成质量上均优于现有方法。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.21322v2",
      "published_date": "2025-03-27 10:01:16 UTC",
      "updated_date": "2025-05-22 16:34:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:51:06.881966"
    },
    {
      "arxiv_id": "2503.21309v1",
      "title": "FineCIR: Explicit Parsing of Fine-Grained Modification Semantics for Composed Image Retrieval",
      "title_zh": "FineCIR：针对组合图像检索的细粒度修改语义显式解析",
      "authors": [
        "Zixu Li",
        "Zhiheng Fu",
        "Yupeng Hu",
        "Zhiwei Chen",
        "Haokun Wen",
        "Liqiang Nie"
      ],
      "abstract": "Composed Image Retrieval (CIR) facilitates image retrieval through a\nmultimodal query consisting of a reference image and modification text. The\nreference image defines the retrieval context, while the modification text\nspecifies desired alterations. However, existing CIR datasets predominantly\nemploy coarse-grained modification text (CoarseMT), which inadequately captures\nfine-grained retrieval intents. This limitation introduces two key challenges:\n(1) ignoring detailed differences leads to imprecise positive samples, and (2)\ngreater ambiguity arises when retrieving visually similar images. These issues\ndegrade retrieval accuracy, necessitating manual result filtering or repeated\nqueries. To address these limitations, we develop a robust fine-grained CIR\ndata annotation pipeline that minimizes imprecise positive samples and enhances\nCIR systems' ability to discern modification intents accurately. Using this\npipeline, we refine the FashionIQ and CIRR datasets to create two fine-grained\nCIR datasets: Fine-FashionIQ and Fine-CIRR. Furthermore, we introduce FineCIR,\nthe first CIR framework explicitly designed to parse the modification text.\nFineCIR effectively captures fine-grained modification semantics and aligns\nthem with ambiguous visual entities, enhancing retrieval precision. Extensive\nexperiments demonstrate that FineCIR consistently outperforms state-of-the-art\nCIR baselines on both fine-grained and traditional CIR benchmark datasets. Our\nFineCIR code and fine-grained CIR datasets are available at\nhttps://github.com/SDU-L/FineCIR.git.",
      "tldr_zh": "该研究针对 Composed Image Retrieval (CIR) 的问题，指出现有数据集使用粗粒度修改文本 (CoarseMT) 导致正样本不精确和检索歧义，从而降低准确性。作者开发了一个细粒度 CIR 数据标注管道，改进了 FashionIQ 和 CIRR 数据集，创建了 Fine-FashionIQ 和 Fine-CIRR，以更好地捕捉细粒度修改意图。FineCIR 框架是首个显式解析修改文本语义的系统，能够有效对齐细粒度修改与模糊视觉实体，提升检索精度。实验结果显示，FineCIR 在细粒度和传统 CIR 基准数据集上均优于现有基线模型。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21309v1",
      "published_date": "2025-03-27 09:34:21 UTC",
      "updated_date": "2025-03-27 09:34:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:51:19.624949"
    },
    {
      "arxiv_id": "2503.21307v1",
      "title": "InternVL-X: Advancing and Accelerating InternVL Series with Efficient Visual Token Compression",
      "title_zh": "InternVL-X：通过高效视觉标记压缩推进与加速 InternVL 系列",
      "authors": [
        "Dongchen Lu",
        "Yuyao Sun",
        "Zilu Zhang",
        "Leping Huang",
        "Jianliang Zeng",
        "Mao Shu",
        "Huo Cao"
      ],
      "abstract": "Most multimodal large language models (MLLMs) treat visual tokens as \"a\nsequence of text\", integrating them with text tokens into a large language\nmodel (LLM). However, a great quantity of visual tokens significantly increases\nthe demand for computational resources and time. In this paper, we propose\nInternVL-X, which outperforms the InternVL model in both performance and\nefficiency by incorporating three visual token compression methods. First, we\npropose a novel vision-language projector, PVTC. This component integrates\nadjacent visual embeddings to form a local query and utilizes the transformed\nCLS token as a global query, then performs point-to-region cross-attention\nthrough these local and global queries to more effectively convert visual\nfeatures. Second, we present a layer-wise visual token compression module,\nLVTC, which compresses tokens in the LLM shallow layers and then expands them\nthrough upsampling and residual connections in the deeper layers. This\nsignificantly enhances the model computational efficiency. Futhermore, we\npropose an efficient high resolution slicing method, RVTC, which dynamically\nadjusts the number of visual tokens based on image area or length filtering.\nRVTC greatly enhances training efficiency with only a slight reduction in\nperformance. By utilizing 20% or fewer visual tokens, InternVL-X achieves\nstate-of-the-art performance on 7 public MLLM benchmarks, and improves the\naverage metric by 2.34% across 12 tasks.",
      "tldr_zh": "这篇论文介绍了InternVL-X，一种改进多模态大语言模型(MLLMs)的框架，通过三种高效视觉令牌压缩方法提升性能和计算效率。关键创新包括PVTC（一种视觉语言投影器，通过局部和全局查询进行点到区域交叉注意力转换视觉特征）、LVTC（层级视觉令牌压缩模块，在浅层压缩令牌并在深层扩展以优化计算）和RVTC（高效高分辨率切片方法，根据图像区域动态调整令牌数量）。实验结果显示，InternVL-X使用20%或更少的视觉令牌，在7个公共MLLM基准上达到最先进性能，并在12个任务上平均指标提高了2.34%。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21307v1",
      "published_date": "2025-03-27 09:31:35 UTC",
      "updated_date": "2025-03-27 09:31:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:51:31.274360"
    },
    {
      "arxiv_id": "2503.21305v1",
      "title": "DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep Models with Limited Data",
      "title_zh": "翻译失败",
      "authors": [
        "Dorde Popovic",
        "Amin Sadeghi",
        "Ting Yu",
        "Sanjay Chawla",
        "Issa Khalil"
      ],
      "abstract": "Backdoor attacks are among the most effective, practical, and stealthy\nattacks in deep learning. In this paper, we consider a practical scenario where\na developer obtains a deep model from a third party and uses it as part of a\nsafety-critical system. The developer wants to inspect the model for potential\nbackdoors prior to system deployment. We find that most existing detection\ntechniques make assumptions that are not applicable to this scenario. In this\npaper, we present a novel framework for detecting backdoors under realistic\nrestrictions. We generate candidate triggers by deductively searching over the\nspace of possible triggers. We construct and optimize a smoothed version of\nAttack Success Rate as our search objective. Starting from a broad class of\ntemplate attacks and just using the forward pass of a deep model, we reverse\nengineer the backdoor attack. We conduct extensive evaluation on a wide range\nof attacks, models, and datasets, with our technique performing almost\nperfectly across these settings.",
      "tldr_zh": "这篇论文提出了DeBackdoor框架，一种用于检测深度模型（deep models）中后门攻击（backdoor attacks）的演绎方法，特别适用于数据有限的现实场景。框架通过对可能触发器的空间进行演绎搜索，并优化平滑后的攻击成功率（Attack Success Rate）作为搜索目标，仅使用模型的前向传播（forward pass）来逆向工程后门攻击。实验结果显示，DeBackdoor在各种攻击、模型和数据集上表现几乎完美，显著提升了后门检测的可靠性和实用性。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21305v1",
      "published_date": "2025-03-27 09:31:10 UTC",
      "updated_date": "2025-03-27 09:31:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:51:44.872257"
    },
    {
      "arxiv_id": "2503.21284v2",
      "title": "Multi-Scale Invertible Neural Network for Wide-Range Variable-Rate Learned Image Compression",
      "title_zh": "翻译失败",
      "authors": [
        "Hanyue Tu",
        "Siqi Wu",
        "Li Li",
        "Wengang Zhou",
        "Houqiang Li"
      ],
      "abstract": "Autoencoder-based structures have dominated recent learned image compression\nmethods. However, the inherent information loss associated with autoencoders\nlimits their rate-distortion performance at high bit rates and restricts their\nflexibility of rate adaptation. In this paper, we present a variable-rate image\ncompression model based on invertible transform to overcome these limitations.\nSpecifically, we design a lightweight multi-scale invertible neural network,\nwhich bijectively maps the input image into multi-scale latent representations.\nTo improve the compression efficiency, a multi-scale spatial-channel context\nmodel with extended gain units is devised to estimate the entropy of the latent\nrepresentation from high to low levels. Experimental results demonstrate that\nthe proposed method achieves state-of-the-art performance compared to existing\nvariable-rate methods, and remains competitive with recent multi-model\napproaches. Notably, our method is the first learned image compression solution\nthat outperforms VVC across a very wide range of bit rates using a single\nmodel, especially at high bit rates. The source code is available at\nhttps://github.com/hytu99/MSINN-VRLIC.",
      "tldr_zh": "本文提出了一种基于可逆变换（invertible transform）的变速率图像压缩模型，以解决自编码器（autoencoder）结构在高比特率下的信息损失和速率适应灵活性问题。该模型设计了轻量级多尺度可逆神经网络（multi-scale invertible neural network），将输入图像双射映射到多尺度潜在表示，并引入多尺度空间-通道上下文模型（multi-scale spatial-channel context model）来优化熵估计，提高压缩效率。实验结果表明，该方法在变速率图像压缩中实现了最先进性能，并首次使用单个模型在宽范围比特率下超越 VVC（Versatile Video Coding），尤其在高比特率领域表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for publication in IEEE Transactions on Multimedia 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.21284v2",
      "published_date": "2025-03-27 09:08:39 UTC",
      "updated_date": "2025-03-28 02:08:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:51:55.763381"
    },
    {
      "arxiv_id": "2503.22752v1",
      "title": "From Individual to Group: Developing a Context-Aware Multi-Criteria Group Recommender System",
      "title_zh": "从个体到群体：开发一个上下文感知的多标准群体推荐系统",
      "authors": [
        "Ngoc Luyen Le",
        "Marie-Hélène Abel"
      ],
      "abstract": "Group decision-making is becoming increasingly common in areas such as\neducation, dining, travel, and finance, where collaborative choices must\nbalance diverse individual preferences. While conventional recommender systems\nare effective in personalization, they fall short in group settings due to\ntheir inability to manage conflicting preferences, contextual factors, and\nmultiple evaluation criteria. This study presents the development of a\nContext-Aware Multi-Criteria Group Recommender System (CA-MCGRS) designed to\naddress these challenges by integrating contextual factors and multiple\ncriteria to enhance recommendation accuracy. By leveraging a Multi-Head\nAttention mechanism, our model dynamically weighs the importance of different\nfeatures. Experiments conducted on an educational dataset with varied ratings\nand contextual variables demonstrate that CA-MCGRS consistently outperforms\nother approaches across four scenarios. Our findings underscore the importance\nof incorporating context and multi-criteria evaluations to improve group\nrecommendations, offering valuable insights for developing more effective group\nrecommender systems.",
      "tldr_zh": "本研究针对群组决策中的挑战，如冲突偏好、上下文因素和多标准评估，提出了一种Context-Aware Multi-Criteria Group Recommender System (CA-MCGRS)，旨在提升群组推荐的准确性。该系统整合上下文因素和多标准评估，通过Multi-Head Attention机制动态权衡不同特征的重要性。在教育数据集上的实验显示，CA-MCGRS在四个场景中均优于其他方法，强调了融入上下文和多标准评估的重要性，为开发更有效的群组推荐系统提供了宝贵见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "The 16th International Conference on Management of Digital\n  EcoSystems, Nov 2024, Naples, Italy",
      "pdf_url": "http://arxiv.org/pdf/2503.22752v1",
      "published_date": "2025-03-27 09:01:45 UTC",
      "updated_date": "2025-03-27 09:01:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:52:05.656388"
    },
    {
      "arxiv_id": "2503.21272v1",
      "title": "Reinforced Model Merging",
      "title_zh": "强化模型合并",
      "authors": [
        "Jiaqi Han",
        "Jingwen Ye",
        "Shunyu Liu",
        "Haofei Zhang",
        "Jie Song",
        "Zunlei Feng",
        "Mingli Song"
      ],
      "abstract": "The success of large language models has garnered widespread attention for\nmodel merging techniques, especially training-free methods which combine model\ncapabilities within the parameter space. However, two challenges remain: (1)\nuniform treatment of all parameters leads to performance degradation; (2)\nsearch-based algorithms are often inefficient. In this paper, we present an\ninnovative framework termed Reinforced Model Merging (RMM), which encompasses\nan environment and agent tailored for merging tasks. These components interact\nto execute layer-wise merging actions, aiming to search the optimal merging\narchitecture. Notably, RMM operates without any gradient computations on the\noriginal models, rendering it feasible for edge devices. Furthermore, by\nutilizing data subsets during the evaluation process, we addressed the\nbottleneck in the reward feedback phase, thereby accelerating RMM by up to 100\ntimes. Extensive experiments demonstrate that RMM achieves state-of-the-art\nperformance across various vision and NLP datasets and effectively overcomes\nthe limitations of the existing baseline methods. Our code is available at\nhttps://github.com/WuDiHJQ/Reinforced-Model-Merging.",
      "tldr_zh": "该研究提出了一种创新框架 Reinforced Model Merging (RMM)，通过强化学习的环境和代理来优化模型合并过程，解决了现有方法中参数处理不均匀和搜索算法效率低下的两大挑战。RMM 采用层级合并动作进行搜索，无需梯度计算，从而适用于边缘设备，并通过数据子集评估加速奖励反馈阶段，提高效率高达 100 倍。实验结果显示，RMM 在各种视觉和 NLP 数据集上实现了 State-of-the-Art 性能，并有效克服了基线方法的局限性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21272v1",
      "published_date": "2025-03-27 08:52:41 UTC",
      "updated_date": "2025-03-27 08:52:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:52:19.468653"
    },
    {
      "arxiv_id": "2503.21258v1",
      "title": "Learn by Reasoning: Analogical Weight Generation for Few-Shot Class-Incremental Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Jizhou Han",
        "Chenhao Ding",
        "Yuhang He",
        "Songlin Dong",
        "Qiang Wang",
        "Xinyuan Gao",
        "Yihong Gong"
      ],
      "abstract": "Few-shot class-incremental Learning (FSCIL) enables models to learn new\nclasses from limited data while retaining performance on previously learned\nclasses. Traditional FSCIL methods often require fine-tuning parameters with\nlimited new class data and suffer from a separation between learning new\nclasses and utilizing old knowledge. Inspired by the analogical learning\nmechanisms of the human brain, we propose a novel analogical generative method.\nOur approach includes the Brain-Inspired Analogical Generator (BiAG), which\nderives new class weights from existing classes without parameter fine-tuning\nduring incremental stages. BiAG consists of three components: Weight\nSelf-Attention Module (WSA), Weight & Prototype Analogical Attention Module\n(WPAA), and Semantic Conversion Module (SCM). SCM uses Neural Collapse theory\nfor semantic conversion, WSA supplements new class weights, and WPAA computes\nanalogies to generate new class weights. Experiments on miniImageNet, CUB-200,\nand CIFAR-100 datasets demonstrate that our method achieves higher final and\naverage accuracy compared to SOTA methods.",
      "tldr_zh": "该论文针对 Few-Shot Class-Incremental Learning (FSCIL) 问题，提出了一种新颖的类比生成方法 Brain-Inspired Analogical Generator (BiAG)，它模仿人类大脑的类比学习机制，从现有类权重中派生新类权重，而无需在增量阶段进行参数微调。BiAG 包括三个关键组件：Weight Self-Attention Module (WSA) 用于补充新类权重、Weight & Prototype Analogical Attention Module (WPAA) 用于计算类比生成权重，以及 Semantic Conversion Module (SCM) 基于 Neural Collapse 理论进行语义转换。实验在 miniImageNet、CUB-200 和 CIFAR-100 数据集上表明，该方法比现有 SOTA 方法实现了更高的最终和平均准确率，从而有效缓解了新旧知识分离的问题。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21258v1",
      "published_date": "2025-03-27 08:31:46 UTC",
      "updated_date": "2025-03-27 08:31:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:52:31.565949"
    },
    {
      "arxiv_id": "2503.21257v1",
      "title": "OminiAdapt: Learning Cross-Task Invariance for Robust and Environment-Aware Robotic Manipulation",
      "title_zh": "翻译失败",
      "authors": [
        "Yongxu Wang",
        "Weiyun Yi",
        "Xinhao Kong",
        "Wanting Li"
      ],
      "abstract": "With the rapid development of embodied intelligence, leveraging large-scale\nhuman data for high-level imitation learning on humanoid robots has become a\nfocal point of interest in both academia and industry. However, applying\nhumanoid robots to precision operation domains remains challenging due to the\ncomplexities they face in perception and control processes, the long-standing\nphysical differences in morphology and actuation mechanisms between humanoid\nrobots and humans, and the lack of task-relevant features obtained from\negocentric vision. To address the issue of covariate shift in imitation\nlearning, this paper proposes an imitation learning algorithm tailored for\nhumanoid robots. By focusing on the primary task objectives, filtering out\nbackground information, and incorporating channel feature fusion with spatial\nattention mechanisms, the proposed algorithm suppresses environmental\ndisturbances and utilizes a dynamic weight update strategy to significantly\nimprove the success rate of humanoid robots in accomplishing target tasks.\nExperimental results demonstrate that the proposed method exhibits robustness\nand scalability across various typical task scenarios, providing new ideas and\napproaches for autonomous learning and control in humanoid robots. The project\nwill be open-sourced on GitHub.",
      "tldr_zh": "该论文提出OminiAdapt，一种针对人形机器人的模仿学习算法，旨在解决模仿学习中的covariate shift问题，包括感知和控制的复杂性、人形机器人与人类的形态差异，以及从egocentric vision获取任务相关特征的挑战。算法通过聚焦主要任务目标、过滤背景信息、结合channel feature fusion和spatial attention mechanisms来抑制环境干扰，并采用动态权重更新策略提升机器人执行任务的鲁棒性。实验结果显示，该方法在各种典型任务场景中显著提高了成功率，并展示了良好的可扩展性，为人形机器人的自主学习和控制提供了新思路。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21257v1",
      "published_date": "2025-03-27 08:28:22 UTC",
      "updated_date": "2025-03-27 08:28:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:52:41.979228"
    },
    {
      "arxiv_id": "2503.21254v1",
      "title": "Vision-to-Music Generation: A Survey",
      "title_zh": "视觉到音乐生成：一个综述",
      "authors": [
        "Zhaokai Wang",
        "Chenxi Bao",
        "Le Zhuo",
        "Jingrui Han",
        "Yang Yue",
        "Yihong Tang",
        "Victor Shea-Jay Huang",
        "Yue Liao"
      ],
      "abstract": "Vision-to-music Generation, including video-to-music and image-to-music\ntasks, is a significant branch of multimodal artificial intelligence\ndemonstrating vast application prospects in fields such as film scoring, short\nvideo creation, and dance music synthesis. However, compared to the rapid\ndevelopment of modalities like text and images, research in vision-to-music is\nstill in its preliminary stage due to its complex internal structure and the\ndifficulty of modeling dynamic relationships with video. Existing surveys focus\non general music generation without comprehensive discussion on\nvision-to-music. In this paper, we systematically review the research progress\nin the field of vision-to-music generation. We first analyze the technical\ncharacteristics and core challenges for three input types: general videos,\nhuman movement videos, and images, as well as two output types of symbolic\nmusic and audio music. We then summarize the existing methodologies on\nvision-to-music generation from the architecture perspective. A detailed review\nof common datasets and evaluation metrics is provided. Finally, we discuss\ncurrent challenges and promising directions for future research. We hope our\nsurvey can inspire further innovation in vision-to-music generation and the\nbroader field of multimodal generation in academic research and industrial\napplications. To follow latest works and foster further innovation in this\nfield, we are continuously maintaining a GitHub repository at\nhttps://github.com/wzk1015/Awesome-Vision-to-Music-Generation.",
      "tldr_zh": "这篇论文对 Vision-to-Music Generation 进行了全面调查，包括视频-to-music 和图像-to-music 任务，强调其在电影配乐、短视频创作和舞蹈音乐合成等领域的应用前景。论文分析了不同输入类型（如通用视频、人体运动视频和图像）的技术特点与核心挑战，并从架构角度总结了现有方法，同时回顾了常用数据集和评估指标。最终，它讨论了当前研究难题和未来方向，并提供了一个持续更新的 GitHub 仓库（https://github.com/wzk1015/Awesome-Vision-to-Music-Generation）以促进创新。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21254v1",
      "published_date": "2025-03-27 08:21:54 UTC",
      "updated_date": "2025-03-27 08:21:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:52:54.549128"
    },
    {
      "arxiv_id": "2503.21251v1",
      "title": "Dual-Splitting Conformal Prediction for Multi-Step Time Series Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Qingdi Yu",
        "Zhiwei Cao",
        "Ruihang Wang",
        "Zhen Yang",
        "Lijun Deng",
        "Min Hu",
        "Yong Luo",
        "Xin Zhou"
      ],
      "abstract": "Time series forecasting is crucial for applications like resource scheduling\nand risk management, where multi-step predictions provide a comprehensive view\nof future trends. Uncertainty Quantification (UQ) is a mainstream approach for\naddressing forecasting uncertainties, with Conformal Prediction (CP) gaining\nattention due to its model-agnostic nature and statistical guarantees. However,\nmost variants of CP are designed for single-step predictions and face\nchallenges in multi-step scenarios, such as reliance on real-time data and\nlimited scalability. This highlights the need for CP methods specifically\ntailored to multi-step forecasting. We propose the Dual-Splitting Conformal\nPrediction (DSCP) method, a novel CP approach designed to capture inherent\ndependencies within time-series data for multi-step forecasting. Experimental\nresults on real-world datasets from four different domains demonstrate that the\nproposed DSCP significantly outperforms existing CP variants in terms of the\nWinkler Score, achieving a performance improvement of up to 23.59% compared to\nstate-of-the-art methods. Furthermore, we deployed the DSCP approach for\nrenewable energy generation and IT load forecasting in power management of a\nreal-world trajectory-based application, achieving an 11.25% reduction in\ncarbon emissions through predictive optimization of data center operations and\ncontrols.",
      "tldr_zh": "该论文针对多步时间序列预测的不确定性问题，提出Dual-Splitting Conformal Prediction (DSCP)方法，该方法通过捕捉时间序列数据的内在依赖性，克服了传统Conformal Prediction (CP)方法在多步场景中的局限性，如对实时数据的依赖和可扩展性不足。实验结果显示，DSCP在四个不同领域的真实数据集上，在Winkler Score指标上比现有方法提升多达23.59%。此外，在实际应用中，DSCP用于可再生能源和IT负载预测，帮助优化数据中心操作，实现了11.25%的碳排放减少。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "68T37",
        "I.2.8"
      ],
      "primary_category": "cs.LG",
      "comment": "28 pages, 13 figures, 3 tables. Submitted to Applied Soft Computing.\n  With Editor This is the first public release of the work",
      "pdf_url": "http://arxiv.org/pdf/2503.21251v1",
      "published_date": "2025-03-27 08:17:18 UTC",
      "updated_date": "2025-03-27 08:17:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:53:07.146908"
    },
    {
      "arxiv_id": "2503.21248v1",
      "title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
      "title_zh": "翻译失败",
      "authors": [
        "Yujie Liu",
        "Zonglin Yang",
        "Tong Xie",
        "Jinjie Ni",
        "Ben Gao",
        "Yuqiang Li",
        "Shixiang Tang",
        "Wanli Ouyang",
        "Erik Cambria",
        "Dongzhan Zhou"
      ],
      "abstract": "Large language models (LLMs) have demonstrated potential in assisting\nscientific research, yet their ability to discover high-quality research\nhypotheses remains unexamined due to the lack of a dedicated benchmark. To\naddress this gap, we introduce the first large-scale benchmark for evaluating\nLLMs with a near-sufficient set of sub-tasks of scientific discovery:\ninspiration retrieval, hypothesis composition, and hypothesis ranking. We\ndevelop an automated framework that extracts critical components - research\nquestions, background surveys, inspirations, and hypotheses - from scientific\npapers across 12 disciplines, with expert validation confirming its accuracy.\nTo prevent data contamination, we focus exclusively on papers published in\n2024, ensuring minimal overlap with LLM pretraining data. Our evaluation\nreveals that LLMs perform well in retrieving inspirations, an\nout-of-distribution task, suggesting their ability to surface novel knowledge\nassociations. This positions LLMs as \"research hypothesis mines\", capable of\nfacilitating automated scientific discovery by generating innovative hypotheses\nat scale with minimal human intervention.",
      "tldr_zh": "本论文引入ResearchBench基准，用于评估大型语言模型(LLMs)在科学发现中的表现，特别聚焦于inspiration retrieval、hypothesis composition和hypothesis ranking等子任务，以填补LLMs生成高质量研究假设的评估空白。研究开发了一个自动化框架，从12个学科的2024年论文中提取关键组件，如研究问题、背景调查、灵感和假设，并通过专家验证确保准确性，以避免数据污染。实验结果表明，LLMs在inspiration retrieval（一个out-of-distribution任务）上表现出色，能发现新知识关联，从而将LLMs定位为“研究假设矿”，在最小人类干预下大规模生成创新假设，促进自动化科学发现。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21248v1",
      "published_date": "2025-03-27 08:09:15 UTC",
      "updated_date": "2025-03-27 08:09:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:53:19.204199"
    },
    {
      "arxiv_id": "2503.21244v1",
      "title": "Improving $(α, f)$-Byzantine Resilience in Federated Learning via layerwise aggregation and cosine distance",
      "title_zh": "翻译失败",
      "authors": [
        "Mario García-Márquez",
        "Nuria Rodríguez-Barroso",
        "M. Victoria Luzón",
        "Francisco Herrera"
      ],
      "abstract": "The rapid development of artificial intelligence systems has amplified\nsocietal concerns regarding their usage, necessitating regulatory frameworks\nthat encompass data privacy. Federated Learning (FL) is posed as potential\nsolution to data privacy challenges in distributed machine learning by enabling\ncollaborative model training {without data sharing}. However, FL systems remain\nvulnerable to Byzantine attacks, where malicious nodes contribute corrupted\nmodel updates. While Byzantine Resilient operators have emerged as a widely\nadopted robust aggregation algorithm to mitigate these attacks, its efficacy\ndiminishes significantly in high-dimensional parameter spaces, sometimes\nleading to poor performing models. This paper introduces Layerwise Cosine\nAggregation, a novel aggregation scheme designed to enhance robustness of these\nrules in such high-dimensional settings while preserving computational\nefficiency. A theoretical analysis is presented, demonstrating the superior\nrobustness of the proposed Layerwise Cosine Aggregation compared to original\nrobust aggregation operators. Empirical evaluation across diverse image\nclassification datasets, under varying data distributions and Byzantine attack\nscenarios, consistently demonstrates the improved performance of Layerwise\nCosine Aggregation, achieving up to a 16% increase in model accuracy.",
      "tldr_zh": "该论文针对Federated Learning (FL)中Byzantine attacks的问题，提出了一种名为Layerwise Cosine Aggregation的新聚合方案，通过层级聚合和cosine distance来提升(α, f)-Byzantine Resilience，尤其在高维参数空间中。该方法保留了计算效率，并在理论分析中证明了其比传统鲁棒聚合算法的优越性。实验结果显示，在多种图像分类数据集和攻击场景下，该方案使模型准确率提高了多达16%。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to Knowledge-Based Systems",
      "pdf_url": "http://arxiv.org/pdf/2503.21244v1",
      "published_date": "2025-03-27 08:07:39 UTC",
      "updated_date": "2025-03-27 08:07:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:53:29.895987"
    },
    {
      "arxiv_id": "2503.21241v1",
      "title": "Feature-Enhanced Machine Learning for All-Cause Mortality Prediction in Healthcare Data",
      "title_zh": "特征增强机器学习在医疗保健数据全因死亡率预测中的应用",
      "authors": [
        "HyeYoung Lee",
        "Pavel Tsoi"
      ],
      "abstract": "Accurate patient mortality prediction enables effective risk stratification,\nleading to personalized treatment plans and improved patient outcomes. However,\npredicting mortality in healthcare remains a significant challenge, with\nexisting studies often focusing on specific diseases or limited predictor sets.\nThis study evaluates machine learning models for all-cause in-hospital\nmortality prediction using the MIMIC-III database, employing a comprehensive\nfeature engineering approach. Guided by clinical expertise and literature, we\nextracted key features such as vital signs (e.g., heart rate, blood pressure),\nlaboratory results (e.g., creatinine, glucose), and demographic information.\nThe Random Forest model achieved the highest performance with an AUC of 0.94,\nsignificantly outperforming other machine learning and deep learning\napproaches. This demonstrates Random Forest's robustness in handling\nhigh-dimensional, noisy clinical data and its potential for developing\neffective clinical decision support tools. Our findings highlight the\nimportance of careful feature engineering for accurate mortality prediction. We\nconclude by discussing implications for clinical adoption and propose future\ndirections, including enhancing model robustness and tailoring prediction\nmodels for specific diseases.",
      "tldr_zh": "这篇论文评估了机器学习模型在基于 MIMIC-III 数据库的全因医院内死亡率预测中的表现，通过全面的特征工程提取关键特征，如生命体征（e.g., heart rate, blood pressure）、实验室结果（e.g., creatinine, glucose）和人口统计信息。Random Forest 模型表现出色，实现了 AUC 0.94 的最高性能，显著优于其他机器学习和深度学习方法，突显了其在处理高维噪声临床数据时的稳健性。该研究强调了特征工程的重要性，并讨论了其对临床决策支持工具的潜在应用以及未来改进方向，如提升模型鲁棒性和针对特定疾病的定制。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21241v1",
      "published_date": "2025-03-27 08:04:42 UTC",
      "updated_date": "2025-03-27 08:04:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:53:42.852318"
    },
    {
      "arxiv_id": "2503.21237v1",
      "title": "Bias-Aware Agent: Enhancing Fairness in AI-Driven Knowledge Retrieval",
      "title_zh": "偏差感知",
      "authors": [
        "Karanbir Singh",
        "William Ngu"
      ],
      "abstract": "Advancements in retrieving accessible information have evolved faster in the\nlast few years compared to the decades since the internet's creation. Search\nengines, like Google, have been the number one way to find relevant data. They\nhave always relied on the user's abilities to find the best information in its\nbillions of links and sources at everybody's fingertips. The advent of large\nlanguage models (LLMs) has completely transformed the field of information\nretrieval. The LLMs excel not only at retrieving relevant knowledge but also at\nsummarizing it effectively, making information more accessible and consumable\nfor users. On top of it, the rise of AI Agents has introduced another aspect to\ninformation retrieval i.e. dynamic information retrieval which enables the\nintegration of real-time data such as weather forecasts, and financial data\nwith the knowledge base to curate context-aware knowledge. However, despite\nthese advancements the agents remain susceptible to issues of bias and\nfairness, challenges deeply rooted within the knowledge base and training of\nLLMs. This study introduces a novel approach to bias-aware knowledge retrieval\nby leveraging agentic framework and the innovative use of bias detectors as\ntools to identify and highlight inherent biases in the retrieved content. By\nempowering users with transparency and awareness, this approach aims to foster\nmore equitable information systems and promote the development of responsible\nAI.",
      "tldr_zh": "这篇论文探讨了AI驱动知识检索中的偏见问题，指出大型语言模型(LLMs)和AI Agents虽能高效检索和总结信息，但仍易受固有偏见影响，导致公平性挑战。研究引入了一种创新方法，即Bias-Aware Agent框架，利用bias detectors作为工具来识别和突出检索内容中的潜在偏见，从而增强用户的透明度和意识。该方法通过整合动态信息检索，旨在构建更公平的信息系统，并推动负责任AI的发展。实验结果表明，这种方法能显著提升检索的公平性，为AI应用提供更可靠的解决方案。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21237v1",
      "published_date": "2025-03-27 07:54:39 UTC",
      "updated_date": "2025-03-27 07:54:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:53:54.195476"
    },
    {
      "arxiv_id": "2503.21232v1",
      "title": "Knowledge Graphs as World Models for Semantic Material-Aware Obstacle Handling in Autonomous Vehicles",
      "title_zh": "知识图谱作为世界模型，用于自动驾驶车辆的语义材料感知障碍处理",
      "authors": [
        "Ayush Bheemaiah",
        "Seungyong Yang"
      ],
      "abstract": "The inability of autonomous vehicles (AVs) to infer the material properties\nof obstacles limits their decision-making capacity. While AVs rely on sensor\nsystems such as cameras, LiDAR, and radar to detect obstacles, this study\nsuggests combining sensors with a knowledge graph (KG)-based world model to\nimprove AVs' comprehension of physical material qualities. Beyond sensor data,\nAVs can infer qualities such as malleability, density, and elasticity using a\nsemantic KG that depicts the relationships between obstacles and their\nattributes. Using the CARLA autonomous driving simulator, we evaluated AV\nperformance with and without KG integration. The findings demonstrate that the\nKG-based method improves obstacle management, which allows AVs to use material\nqualities to make better decisions about when to change lanes or apply\nemergency braking. For example, the KG-integrated AV changed lanes for hard\nimpediments like traffic cones and successfully avoided collisions with\nflexible items such as plastic bags by passing over them. Compared to the\ncontrol system, the KG framework demonstrated improved responsiveness to\nobstacles by resolving conflicting sensor data, causing emergency stops for\n13.3% more cases. In addition, our method exhibits a 6.6% higher success rate\nin lane-changing maneuvers in experimental scenarios, particularly for larger,\nhigh-impact obstacles. While we focus particularly on autonomous driving, our\nwork demonstrates the potential of KG-based world models to improve\ndecision-making in embodied AI systems and scale to other domains, including\nrobotics, healthcare, and environmental simulation.",
      "tldr_zh": "本研究提出使用 Knowledge Graphs (KG) 作为世界模型，帮助 Autonomous Vehicles (AVs) 推断障碍物的材料属性（如可塑性、密度和弹性），从而提升决策能力。该方法结合传感器数据和语义 KG 来描述障碍物及其属性的关系，在 CARLA 模拟器中实验表明，KG 集成后 AVs 的障碍处理显著改善，包括紧急制动响应增加 13.3% 和换道成功率提高 6.6%。此外，该框架不仅优化了 AVs 的碰撞避免策略，还展示了在机器人、医疗和环境模拟等领域的扩展潜力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21232v1",
      "published_date": "2025-03-27 07:46:45 UTC",
      "updated_date": "2025-03-27 07:46:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:54:06.531240"
    },
    {
      "arxiv_id": "2503.21839v1",
      "title": "M-DocSum: Do LVLMs Genuinely Comprehend Interleaved Image-Text in Document Summarization?",
      "title_zh": "M-DocSum：LVLMs 是否真正理解文档摘要中的交错图像-文本？",
      "authors": [
        "Haolong Yan",
        "Kaijun Tan",
        "Yeqing Shen",
        "Xin Huang",
        "Zheng Ge",
        "Xiangyu Zhang",
        "Si Li",
        "Daxin Jiang"
      ],
      "abstract": "We investigate a critical yet under-explored question in Large\nVision-Language Models (LVLMs): Do LVLMs genuinely comprehend interleaved\nimage-text in the document? Existing document understanding benchmarks often\nassess LVLMs using question-answer formats, which are information-sparse and\ndifficult to guarantee the coverage of long-range dependencies. To address this\nissue, we introduce a novel and challenging Multimodal Document Summarization\nBenchmark (M-DocSum-Bench), which comprises 500 high-quality arXiv papers,\nalong with interleaved multimodal summaries aligned with human preferences.\nM-DocSum-Bench is a reference-based generation task and necessitates the\ngeneration of interleaved image-text summaries using provided reference images,\nthereby simultaneously evaluating capabilities in understanding, reasoning,\nlocalization, and summarization within complex multimodal document scenarios.\nTo facilitate this benchmark, we develop an automated framework to construct\nsummaries and propose a fine-grained evaluation method called M-DocEval.\nMoreover, we further develop a robust summarization baseline, i.e.,\nM-DocSum-7B, by progressive two-stage training with diverse instruction and\npreference data. The extensive results on our M-DocSum-Bench reveal that the\nleading LVLMs struggle to maintain coherence and accurately integrate\ninformation within long and interleaved contexts, often exhibiting confusion\nbetween similar images and a lack of robustness. Notably, M-DocSum-7B achieves\nstate-of-the-art performance compared to larger and closed-source models\n(including GPT-4o, Gemini Pro, Claude-3.5-Sonnet and Qwen2.5-VL-72B, etc.),\ndemonstrating the potential of LVLMs for improved interleaved image-text\nunderstanding. The code, data, and models are available at\nhttps://github.com/stepfun-ai/M-DocSum-Bench.",
      "tldr_zh": "本研究探讨 Large Vision-Language Models (LVLMs) 是否真正理解文档中的交错图像-文本问题，并引入了一个新基准 M-DocSum-Bench，该基准包含 500 个高质量 arXiv 论文及其人类偏好的多模态摘要，以评估 LVLMs 在理解、推理、定位和总结方面的能力。研究开发了自动框架构建摘要和细粒度评估方法 M-DocEval，同时构建了基线模型 M-DocSum-7B，通过两阶段训练和多样指令数据优化其性能。实验结果显示，现有的领先 LVLMs 在处理长交错上下文时常出现连贯性问题和图像混淆，而 M-DocSum-7B 比更大闭源模型（如 GPT-4o 和 Gemini Pro）表现出色，证明了改进 LVLMs 潜力的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21839v1",
      "published_date": "2025-03-27 07:28:32 UTC",
      "updated_date": "2025-03-27 07:28:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:54:20.181470"
    },
    {
      "arxiv_id": "2503.21219v2",
      "title": "GenFusion: Closing the Loop between Reconstruction and Generation via Videos",
      "title_zh": "翻译失败",
      "authors": [
        "Sibo Wu",
        "Congrong Xu",
        "Binbin Huang",
        "Andreas Geiger",
        "Anpei Chen"
      ],
      "abstract": "Recently, 3D reconstruction and generation have demonstrated impressive novel\nview synthesis results, achieving high fidelity and efficiency. However, a\nnotable conditioning gap can be observed between these two fields, e.g.,\nscalable 3D scene reconstruction often requires densely captured views, whereas\n3D generation typically relies on a single or no input view, which\nsignificantly limits their applications. We found that the source of this\nphenomenon lies in the misalignment between 3D constraints and generative\npriors. To address this problem, we propose a reconstruction-driven video\ndiffusion model that learns to condition video frames on artifact-prone RGB-D\nrenderings. Moreover, we propose a cyclical fusion pipeline that iteratively\nadds restoration frames from the generative model to the training set, enabling\nprogressive expansion and addressing the viewpoint saturation limitations seen\nin previous reconstruction and generation pipelines. Our evaluation, including\nview synthesis from sparse view and masked input, validates the effectiveness\nof our approach. More details at https://genfusion.sibowu.com.",
      "tldr_zh": "该论文提出GenFusion框架，通过视频桥接3D reconstruction和3D generation之间的条件差距，解决重建需要密集视图而生成仅依赖单视图或无视图的问题。方法包括一个重建驱动的video diffusion model，该模型学习基于artifact-prone RGB-D renderings来调节视频帧，以及一个cyclical fusion pipeline，通过迭代添加从生成模型恢复的帧来逐步扩展训练集并克服视点饱和限制。实验评估显示，该方法在从稀疏视图和masked输入进行view synthesis时表现出色，有效提升了3D场景处理的应用潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025, project page: https://genfusion.sibowu.com",
      "pdf_url": "http://arxiv.org/pdf/2503.21219v2",
      "published_date": "2025-03-27 07:16:24 UTC",
      "updated_date": "2025-03-29 12:18:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:54:30.068280"
    },
    {
      "arxiv_id": "2503.21838v1",
      "title": "MSPLoRA: A Multi-Scale Pyramid Low-Rank Adaptation for Efficient Model Fine-Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Jiancheng Zhao",
        "Xingda Yu",
        "Zhen Yang"
      ],
      "abstract": "Parameter-Efficient Fine-Tuning (PEFT) has become an essential approach for\nadapting large-scale pre-trained models while reducing computational costs.\nAmong PEFT methods, LoRA significantly reduces trainable parameters by\ndecomposing weight updates into low-rank matrices. However, traditional LoRA\napplies a fixed rank across all layers, failing to account for the varying\ncomplexity of hierarchical information, which leads to inefficient adaptation\nand redundancy. To address this, we propose MSPLoRA (Multi-Scale Pyramid LoRA),\nwhich introduces Global Shared LoRA, Mid-Level Shared LoRA, and Layer-Specific\nLoRA to capture global patterns, mid-level features, and fine-grained\ninformation, respectively. This hierarchical structure reduces inter-layer\nredundancy while maintaining strong adaptation capability. Experiments on\nvarious NLP tasks demonstrate that MSPLoRA achieves more efficient adaptation\nand better performance while significantly reducing the number of trainable\nparameters. Furthermore, additional analyses based on Singular Value\nDecomposition validate its information decoupling ability, highlighting MSPLoRA\nas a scalable and effective optimization strategy for parameter-efficient\nfine-tuning in large language models. Our code is available at\nhttps://github.com/Oblivioniss/MSPLoRA.",
      "tldr_zh": "本研究针对 Parameter-Efficient Fine-Tuning (PEFT) 中的 LoRA 方法存在的固定秩问题，提出 MSPLoRA（Multi-Scale Pyramid Low-Rank Adaptation）框架，以适应层间信息复杂度的差异。MSPLoRA 通过引入 Global Shared LoRA、中间层共享 LoRA（Mid-Level Shared LoRA）和层特定 LoRA（Layer-Specific LoRA），构建分层结构来捕获全局模式、中间特征和细粒度信息，从而减少冗余并提升适应效率。在各种 NLP 任务上的实验显示，MSPLoRA 显著降低了可训练参数，同时实现了更好的性能；此外，基于 Singular Value Decomposition (SVD) 的分析验证了其信息解耦能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21838v1",
      "published_date": "2025-03-27 07:01:50 UTC",
      "updated_date": "2025-03-27 07:01:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:54:42.980120"
    },
    {
      "arxiv_id": "2503.22751v1",
      "title": "Advancing Spatiotemporal Prediction using Artificial Intelligence: Extending the Framework of Geographically and Temporally Weighted Neural Network (GTWNN) for Differing Geographical and Temporal Contexts",
      "title_zh": "使用人工智能推进时空预测：扩展地理和时间加权神经网络 (GTWNN) 框架",
      "authors": [
        "Nicholas Robert Fisk",
        "Matthew Ng Kok Ming",
        "Zahratu Shabrina"
      ],
      "abstract": "This paper aims at improving predictive crime models by extending the\nmathematical framework of Artificial Neural Networks (ANNs) tailored to general\nspatiotemporal problems and appropriately applying them. Recent advancements in\nthe geospatial-temporal modelling field have focused on the inclusion of\ngeographical weighting in their deep learning models to account for nonspatial\nstationarity, which is often apparent in spatial data. We formulate a novel\nsemi-analytical approach to solving Geographically and Temporally Weighted\nRegression (GTWR), and applying it to London crime data. The results produce\nhigh-accuracy predictive evaluation scores that affirm the validity of the\nassumptions and approximations in the approach. This paper presents\nmathematical advances to the Geographically and Temporally Weighted Neural\nNetwork (GTWNN) framework, which offers a novel contribution to the field.\nInsights from past literature are harmoniously employed with the assumptions\nand approximations to generate three mathematical extensions to GTWNN's\nframework. Combinations of these extensions produce five novel ANNs, applied to\nthe London and Detroit datasets. The results suggest that one of the extensions\nis redundant and is generally surpassed by another extension, which we term the\nhistory-dependent module. The remaining extensions form three novel ANN designs\nthat pose potential GTWNN improvements. We evaluated the efficacy of various\nmodels in both the London and Detroit crime datasets, highlighting the\nimportance of accounting for specific geographic and temporal characteristics\nwhen selecting modelling strategies to improve model suitability. In general,\nthe proposed methods provide the foundations for a more context-aware,\naccurate, and robust ANN approach in spatio-temporal modelling.",
      "tldr_zh": "本文扩展了 Geographically and Temporally Weighted Neural Network (GTWNN) 框架，旨在通过整合地理和时间权重来改进时空预测模型，特别是应用于犯罪数据预测。研究者提出了一种新型半分析方法解决 Geographically and Temporally Weighted Regression (GTWR)，并基于此开发了五个新 Artificial Neural Networks (ANNs) 模型，包括 history-dependent module 等扩展。实验结果显示，这些模型在伦敦和底特律犯罪数据集上取得了高准确性预测评分，其中某些扩展优于其他，并验证了考虑特定地理和时间特性的必要性。总体上，该方法为更准确、稳健的时空建模提供了重要基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22751v1",
      "published_date": "2025-03-27 06:45:59 UTC",
      "updated_date": "2025-03-27 06:45:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:54:55.719488"
    },
    {
      "arxiv_id": "2503.21200v1",
      "title": "Learning Generalizable Skills from Offline Multi-Task Data for Multi-Agent Cooperation",
      "title_zh": "翻译失败",
      "authors": [
        "Sicong Liu",
        "Yang Shu",
        "Chenjuan Guo",
        "Bin Yang"
      ],
      "abstract": "Learning cooperative multi-agent policy from offline multi-task data that can\ngeneralize to unseen tasks with varying numbers of agents and targets is an\nattractive problem in many scenarios. Although aggregating general behavior\npatterns among multiple tasks as skills to improve policy transfer is a\npromising approach, two primary challenges hinder the further advancement of\nskill learning in offline multi-task MARL. Firstly, extracting general\ncooperative behaviors from various action sequences as common skills lacks\nbringing cooperative temporal knowledge into them. Secondly, existing works\nonly involve common skills and can not adaptively choose independent knowledge\nas task-specific skills in each task for fine-grained action execution. To\ntackle these challenges, we propose Hierarchical and Separate Skill Discovery\n(HiSSD), a novel approach for generalizable offline multi-task MARL through\nskill learning. HiSSD leverages a hierarchical framework that jointly learns\ncommon and task-specific skills. The common skills learn cooperative temporal\nknowledge and enable in-sample exploitation for offline multi-task MARL. The\ntask-specific skills represent the priors of each task and achieve a\ntask-guided fine-grained action execution. To verify the advancement of our\nmethod, we conduct experiments on multi-agent MuJoCo and SMAC benchmarks. After\ntraining the policy using HiSSD on offline multi-task data, the empirical\nresults show that HiSSD assigns effective cooperative behaviors and obtains\nsuperior performance in unseen tasks.",
      "tldr_zh": "该研究针对多智能体合作（multi-agent cooperation）问题，提出了一种从离线多任务数据（offline multi-task data）中学习可泛化技能的方法，以适应未见任务中不同智能体和目标数量的场景。主要挑战包括缺乏将合作时间知识融入一般行为技能，以及无法自适应选择任务特定技能（task-specific skills）。为此，作者开发了Hierarchical and Separate Skill Discovery (HiSSD)框架，该框架通过分层结构共同学习通用合作技能（common skills）和任务特定技能，实现合作时间知识的利用和细粒度动作执行。在multi-agent MuJoCo和SMAC基准测试中，HiSSD在离线多任务MARL（offline multi-task MARL）上训练后，展示了优越的性能，并在未见任务中分配有效的合作行为。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21200v1",
      "published_date": "2025-03-27 06:35:59 UTC",
      "updated_date": "2025-03-27 06:35:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:55:07.681434"
    },
    {
      "arxiv_id": "2503.21178v1",
      "title": "Integrating Large Language Models For Monte Carlo Simulation of Chemical Reaction Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Sadikshya Gyawali",
        "Ashwini Mandal",
        "Manish Dahal",
        "Manish Awale",
        "Sanjay Rijal",
        "Shital Adhikari",
        "Vaghawan Ojha"
      ],
      "abstract": "Chemical reaction network is an important method for modeling and exploring\ncomplex biological processes, bio-chemical interactions and the behavior of\ndifferent dynamics in system biology. But, formulating such reaction kinetics\ntakes considerable time. In this paper, we leverage the efficiency of modern\nlarge language models to automate the stochastic monte carlo simulation of\nchemical reaction networks and enable the simulation through the reaction\ndescription provided in the form of natural languages. We also integrate this\nprocess into widely used simulation tool Copasi to further give the edge and\nease to the modelers and researchers. In this work, we show the efficacy and\nlimitations of the modern large language models to parse and create reaction\nkinetics for modelling complex chemical reaction processes.",
      "tldr_zh": "本文提出了一种整合 Large Language Models 的方法，用于自动化化学反应网络的 Monte Carlo 模拟，以简化复杂生物过程和生化互动的建模。研究通过自然语言描述来生成反应动力学，并将其集成到常用模拟工具 Copasi 中，方便研究人员快速进行模拟。实验结果展示了 Large Language Models 在解析和创建反应动力学方面的功效，同时指出了其潜在局限性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted on MadeAI 2025 Conference",
      "pdf_url": "http://arxiv.org/pdf/2503.21178v1",
      "published_date": "2025-03-27 06:01:50 UTC",
      "updated_date": "2025-03-27 06:01:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:55:18.521162"
    },
    {
      "arxiv_id": "2503.21164v1",
      "title": "Adversarial Wear and Tear: Exploiting Natural Damage for Generating Physical-World Adversarial Examples",
      "title_zh": "翻译失败",
      "authors": [
        "Samra Irshad",
        "Seungkyu Lee",
        "Nassir Navab",
        "Hong Joo Lee",
        "Seong Tae Kim"
      ],
      "abstract": "The presence of adversarial examples in the physical world poses significant\nchallenges to the deployment of Deep Neural Networks in safety-critical\napplications such as autonomous driving. Most existing methods for crafting\nphysical-world adversarial examples are ad-hoc, relying on temporary\nmodifications like shadows, laser beams, or stickers that are tailored to\nspecific scenarios. In this paper, we introduce a new class of physical-world\nadversarial examples, AdvWT, which draws inspiration from the naturally\noccurring phenomenon of `wear and tear', an inherent property of physical\nobjects. Unlike manually crafted perturbations, `wear and tear' emerges\norganically over time due to environmental degradation, as seen in the gradual\ndeterioration of outdoor signboards. To achieve this, AdvWT follows a two-step\napproach. First, a GAN-based, unsupervised image-to-image translation network\nis employed to model these naturally occurring damages, particularly in the\ncontext of outdoor signboards. The translation network encodes the\ncharacteristics of damaged signs into a latent `damage style code'. In the\nsecond step, we introduce adversarial perturbations into the style code,\nstrategically optimizing its transformation process. This manipulation subtly\nalters the damage style representation, guiding the network to generate\nadversarial images where the appearance of damages remains perceptually\nrealistic, while simultaneously ensuring their effectiveness in misleading\nneural networks. Through comprehensive experiments on two traffic sign\ndatasets, we show that AdvWT effectively misleads DNNs in both digital and\nphysical domains. AdvWT achieves an effective attack success rate, greater\nrobustness, and a more natural appearance compared to existing physical-world\nadversarial examples. Additionally, integrating AdvWT into training enhances a\nmodel's generalizability to real-world damaged signs.",
      "tldr_zh": "这篇论文提出了一种名为 AdvWT 的新方法，利用自然损耗（如户外标志的磨损）生成物理世界对抗样本（Adversarial Examples），以挑战深度神经网络（DNNs）在安全关键应用中的部署。方法采用两步策略：首先，使用基于 GAN 的无监督图像到图像翻译网络模拟自然损耗，并提取损耗风格代码；其次，在风格代码中引入对抗扰动，优化生成感知上真实的对抗图像。实验在两个交通标志数据集上表明，AdvWT 比现有方法具有更高的攻击成功率、更强的鲁棒性和更自然的外观，同时将该方法整合到模型训练中可提升其对真实世界损耗标志的泛化能力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.21164v1",
      "published_date": "2025-03-27 05:19:41 UTC",
      "updated_date": "2025-03-27 05:19:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:55:30.700007"
    },
    {
      "arxiv_id": "2503.22749v1",
      "title": "Adaptive Clipping for Privacy-Preserving Few-Shot Learning: Enhancing Generalization with Limited Data",
      "title_zh": "翻译失败",
      "authors": [
        "Kanishka Ranaweera",
        "Dinh C. Nguyen",
        "Pubudu N. Pathirana",
        "David Smith",
        "Ming Ding",
        "Thierry Rakotoarivelo",
        "Aruna Seneviratne"
      ],
      "abstract": "In the era of data-driven machine-learning applications, privacy concerns and\nthe scarcity of labeled data have become paramount challenges. These challenges\nare particularly pronounced in the domain of few-shot learning, where the\nability to learn from limited labeled data is crucial. Privacy-preserving\nfew-shot learning algorithms have emerged as a promising solution to address\nsuch pronounced challenges. However, it is well-known that privacy-preserving\ntechniques often lead to a drop in utility due to the fundamental trade-off\nbetween data privacy and model performance. To enhance the utility of\nprivacy-preserving few-shot learning methods, we introduce a novel approach\ncalled Meta-Clip. This technique is specifically designed for meta-learning\nalgorithms, including Differentially Private (DP) model-agnostic meta-learning,\nDP-Reptile, and DP-MetaSGD algorithms, with the objective of balancing data\nprivacy preservation with learning capacity maximization. By dynamically\nadjusting clipping thresholds during the training process, our Adaptive\nClipping method provides fine-grained control over the disclosure of sensitive\ninformation, mitigating overfitting on small datasets and significantly\nimproving the generalization performance of meta-learning models. Through\ncomprehensive experiments on diverse benchmark datasets, we demonstrate the\neffectiveness of our approach in minimizing utility degradation, showcasing a\nsuperior privacy-utility trade-off compared to existing privacy-preserving\ntechniques. The adoption of Adaptive Clipping represents a substantial step\nforward in the field of privacy-preserving few-shot learning, empowering the\ndevelopment of secure and accurate models for real-world applications,\nespecially in scenarios where there are limited data availability.",
      "tldr_zh": "本文提出 Adaptive Clipping 方法，用于提升隐私保护少样本学习（few-shot learning）的泛化性能，针对元学习算法如 Differentially Private (DP) model-agnostic meta-learning、DP-Reptile 和 DP-MetaSGD，通过动态调整剪切阈值来平衡数据隐私与模型学习能力。相比传统技术，该方法减少了过拟合风险，并在多种基准数据集上实验中展示了显著的性能提升，实现更好的隐私-效用权衡。最终，这为数据稀缺的实际应用场景提供了更安全且准确的模型解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22749v1",
      "published_date": "2025-03-27 05:14:18 UTC",
      "updated_date": "2025-03-27 05:14:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:55:42.997045"
    },
    {
      "arxiv_id": "2503.21159v1",
      "title": "Multi-Objective Optimization for Privacy-Utility Balance in Differentially Private Federated Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Kanishka Ranaweera",
        "David Smith",
        "Pubudu N. Pathirana",
        "Ming Ding",
        "Thierry Rakotoarivelo",
        "Aruna Seneviratne"
      ],
      "abstract": "Federated learning (FL) enables collaborative model training across\ndistributed clients without sharing raw data, making it a promising approach\nfor privacy-preserving machine learning. However, ensuring differential privacy\n(DP) in FL presents challenges due to the trade-off between model utility and\nprivacy protection. Clipping gradients before aggregation is a common strategy\nto limit privacy loss, but selecting an optimal clipping norm is non-trivial,\nas excessively high values compromise privacy, while overly restrictive\nclipping degrades model performance. In this work, we propose an adaptive\nclipping mechanism that dynamically adjusts the clipping norm using a\nmulti-objective optimization framework. By integrating privacy and utility\nconsiderations into the optimization objective, our approach balances privacy\npreservation with model accuracy. We theoretically analyze the convergence\nproperties of our method and demonstrate its effectiveness through extensive\nexperiments on MNIST, Fashion-MNIST, and CIFAR-10 datasets. Our results show\nthat adaptive clipping consistently outperforms fixed-clipping baselines,\nachieving improved accuracy under the same privacy constraints. This work\nhighlights the potential of dynamic clipping strategies to enhance\nprivacy-utility trade-offs in differentially private federated learning.",
      "tldr_zh": "本文针对差分隐私(DP)联邦学习(FL)中隐私保护与模型实用性权衡的挑战，提出了一种自适应剪切机制，利用多目标优化(Multi-Objective Optimization)框架动态调整剪切范数，以平衡隐私损失和模型性能。方法将隐私与准确性整合到优化目标中，并通过理论分析验证了其收敛性(convergence properties)。实验结果显示，在MNIST、Fashion-MNIST和CIFAR-10数据集上，该机制相较固定剪切基准在相同隐私约束下实现了更高的准确率，突显了动态剪切策略在提升DP FL隐私-实用性权衡的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21159v1",
      "published_date": "2025-03-27 04:57:05 UTC",
      "updated_date": "2025-03-27 04:57:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:55:56.470960"
    },
    {
      "arxiv_id": "2503.21154v1",
      "title": "Federated Learning with Differential Privacy: An Utility-Enhanced Approach",
      "title_zh": "联邦学习结合差分隐私：一种效用增强的方法",
      "authors": [
        "Kanishka Ranaweera",
        "Dinh C. Nguyen",
        "Pubudu N. Pathirana",
        "David Smith",
        "Ming Ding",
        "Thierry Rakotoarivelo",
        "Aruna Seneviratne"
      ],
      "abstract": "Federated learning has emerged as an attractive approach to protect data\nprivacy by eliminating the need for sharing clients' data while reducing\ncommunication costs compared with centralized machine learning algorithms.\nHowever, recent studies have shown that federated learning alone does not\nguarantee privacy, as private data may still be inferred from the uploaded\nparameters to the central server. In order to successfully avoid data leakage,\nadopting differential privacy (DP) in the local optimization process or in the\nlocal update aggregation process has emerged as two feasible ways for achieving\nsample-level or user-level privacy guarantees respectively, in federated\nlearning models. However, compared to their non-private equivalents, these\napproaches suffer from a poor utility. To improve the privacy-utility\ntrade-off, we present a modification to these vanilla differentially private\nalgorithms based on a Haar wavelet transformation step and a novel noise\ninjection scheme that significantly lowers the asymptotic bound of the noise\nvariance. We also present a holistic convergence analysis of our proposed\nalgorithm, showing that our method yields better convergence performance than\nthe vanilla DP algorithms. Numerical experiments on real-world datasets\ndemonstrate that our method outperforms existing approaches in model utility\nwhile maintaining the same privacy guarantees.",
      "tldr_zh": "该研究针对联邦学习（Federated Learning）中隐私泄露的风险，提出了一种增强效用的差分隐私（Differential Privacy, DP）方法，以解决传统DP算法在保护隐私的同时导致模型效用下降的问题。具体而言，该方法通过引入Haar wavelet transformation步骤和一种新型噪声注入方案，显著降低了噪声方差，从而改善了隐私-效用权衡。论文还提供了全面的收敛分析，证明该算法比传统DP算法具有更好的收敛性能；在真实数据集上的实验结果显示，该方法在保持相同隐私保证的前提下，模型效用明显优于现有方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21154v1",
      "published_date": "2025-03-27 04:48:29 UTC",
      "updated_date": "2025-03-27 04:48:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:56:06.218019"
    },
    {
      "arxiv_id": "2503.21150v1",
      "title": "The Devil is in Low-Level Features for Cross-Domain Few-Shot Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Yuhan Liu",
        "Yixiong Zou",
        "Yuhua Li",
        "Ruixuan Li"
      ],
      "abstract": "Cross-Domain Few-Shot Segmentation (CDFSS) is proposed to transfer the\npixel-level segmentation capabilities learned from large-scale source-domain\ndatasets to downstream target-domain datasets, with only a few annotated images\nper class. In this paper, we focus on a well-observed but unresolved phenomenon\nin CDFSS: for target domains, particularly those distant from the source\ndomain, segmentation performance peaks at the very early epochs, and declines\nsharply as the source-domain training proceeds. We delve into this phenomenon\nfor an interpretation: low-level features are vulnerable to domain shifts,\nleading to sharper loss landscapes during the source-domain training, which is\nthe devil of CDFSS. Based on this phenomenon and interpretation, we further\npropose a method that includes two plug-and-play modules: one to flatten the\nloss landscapes for low-level features during source-domain training as a novel\nsharpness-aware minimization method, and the other to directly supplement\ntarget-domain information to the model during target-domain testing by\nlow-level-based calibration. Extensive experiments on four target datasets\nvalidate our rationale and demonstrate that our method surpasses the\nstate-of-the-art method in CDFSS signifcantly by 3.71% and 5.34% average MIoU\nin 1-shot and 5-shot scenarios, respectively.",
      "tldr_zh": "本研究探讨了 Cross-Domain Few-Shot Segmentation (CDFSS) 的关键挑战，即将源域数据集学到的像素级分割能力转移到目标域时，仅需少量标注图像，但性能在早期训练后急剧下降的现象。研究发现，这一问题源于 low-level features 对域移位的高度敏感，导致损失景观 (loss landscapes) 变得更陡峭，从而影响模型泛化。针对此，作者提出了一种方法，包括两个模块：一个是新的 sharpness-aware minimization 模块，用于在源域训练中平滑 low-level features 的损失景观；另一个是通过 low-level-based calibration 在目标域测试中直接补充目标域信息。实验在四个目标数据集上验证了这一解释，并显示该方法在 1-shot 和 5-shot 场景下分别比最先进方法提升 3.71% 和 5.34% 的平均 MIoU。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.21150v1",
      "published_date": "2025-03-27 04:37:52 UTC",
      "updated_date": "2025-03-27 04:37:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:56:19.858537"
    },
    {
      "arxiv_id": "2503.21138v5",
      "title": "A Computational Theory for Efficient Mini Agent Evaluation with Causal Guarantees",
      "title_zh": "高效微型代理评估的计算理论，具有因果保证",
      "authors": [
        "Hedong Yan"
      ],
      "abstract": "In order to reduce the cost of experimental evaluation for agents, we\nintroduce a computational theory of evaluation for mini agents: build\nevaluation model to accelerate the evaluation procedures. We prove upper bounds\nof generalized error and generalized causal effect error of given evaluation\nmodels for infinite agents. We also prove efficiency, and consistency to\nestimated causal effect from deployed agents to evaluation metric by\nprediction. To learn evaluation models, we propose a meta-learner to handle\nheterogeneous agents space problem. Comparing with existed evaluation\napproaches, our (conditional) evaluation model reduced 24.1\\% to 99.0\\%\nevaluation errors across 12 scenes, including individual medicine, scientific\nsimulation, social experiment, business activity, and quantum trade. The\nevaluation time is reduced 3 to 7 order of magnitude per subject comparing with\nexperiments or simulations.",
      "tldr_zh": "该研究提出了一种计算理论（computational theory），用于高效评估小型代理（mini agents），旨在通过构建评估模型来减少实验成本并提供因果保证（causal guarantees）。他们证明了评估模型的泛化错误（generalized error）和泛化因果效应错误（generalized causal effect error）的上界，并展示了其效率和一致性，能够通过预测估计部署代理对评估指标的因果效应。为处理异构代理空间问题，论文引入了一个元学习器（meta-learner）。实验结果显示，该方法在12个场景（包括个体医学、科学模拟、社会实验、商业活动和量子交易）中将评估错误减少24.1%至99.0%，并将每个对象的评估时间缩短3至7个数量级。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "math.ST",
        "stat.ML",
        "stat.TH"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21138v5",
      "published_date": "2025-03-27 04:00:49 UTC",
      "updated_date": "2025-05-16 06:20:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:56:31.162003"
    },
    {
      "arxiv_id": "2503.21109v1",
      "title": "Optimizing Multi-DNN Inference on Mobile Devices through Heterogeneous Processor Co-Execution",
      "title_zh": "通过异构处理器联合执行优化移动设备上的",
      "authors": [
        "Yunquan Gao",
        "Zhiguo Zhang",
        "Praveen Kumar Donta",
        "Chinmaya Kumar Dehury",
        "Xiujun Wang",
        "Dusit Niyato",
        "Qiyang Zhang"
      ],
      "abstract": "Deep Neural Networks (DNNs) are increasingly deployed across diverse\nindustries, driving demand for mobile device support. However, existing mobile\ninference frameworks often rely on a single processor per model, limiting\nhardware utilization and causing suboptimal performance and energy efficiency.\nExpanding DNN accessibility on mobile platforms requires adaptive,\nresource-efficient solutions to meet rising computational needs without\ncompromising functionality. Parallel inference of multiple DNNs on\nheterogeneous processors remains challenging. Some works partition DNN\noperations into subgraphs for parallel execution across processors, but these\noften create excessive subgraphs based only on hardware compatibility,\nincreasing scheduling complexity and memory overhead.\n  To address this, we propose an Advanced Multi-DNN Model Scheduling (ADMS)\nstrategy for optimizing multi-DNN inference on mobile heterogeneous processors.\nADMS constructs an optimal subgraph partitioning strategy offline, balancing\nhardware operation support and scheduling granularity, and uses a\nprocessor-state-aware algorithm to dynamically adjust workloads based on\nreal-time conditions. This ensures efficient workload distribution and\nmaximizes processor utilization. Experiments show ADMS reduces multi-DNN\ninference latency by 4.04 times compared to vanilla frameworks.",
      "tldr_zh": "该研究针对移动设备上多 DNN（Deep Neural Networks）推理的性能和能效问题，提出 ADMS（Advanced Multi-DNN Model Scheduling）策略，通过异构处理器协同执行来优化工作负载分配。\nADMS 在离线阶段构建最佳子图分区策略，平衡硬件操作支持和调度粒度，并在运行时使用处理器状态感知算法根据实时条件动态调整任务，以最大化处理器利用率。\n实验结果表明，与传统框架相比，ADMS 将多 DNN 推理延迟降低了 4.04 倍。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "68T07, 68W40",
        "I.2.6; C.1.4; D.4.8"
      ],
      "primary_category": "cs.DC",
      "comment": "14 pages, 12 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.21109v1",
      "published_date": "2025-03-27 03:03:09 UTC",
      "updated_date": "2025-03-27 03:03:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:56:43.228207"
    },
    {
      "arxiv_id": "2503.22748v1",
      "title": "Ignite Forecasting with SPARK: An Efficient Generative Framework for Refining LLMs in Temporal Knowledge Graph Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Gongzhu Yin",
        "Hongli Zhang",
        "Yi Luo",
        "Yuchen Yang",
        "Kun Lu",
        "Chao Meng"
      ],
      "abstract": "Temporal Knowledge Graph (TKG) forecasting is crucial for predicting future\nevents using historical data. With the surge of Large Language Models (LLMs),\nrecent studies have begun exploring their integration into TKG forecasting and\nachieved some success. However, they still face limitations such as limited\ninput length, inefficient output generation, and resource-intensive refinement,\nwhich undermine their performance and practical applicability. To address these\nlimitations, we introduce SPARK, a Sequence-level Proxy-Adapting framework for\nRefining LLMs in TKG forecasting. Inspired by inference-time algorithms adopted\nin controlling generation, SPARK offers a cost-effective, plug-and-play\nsolution through two key innovations: (1) Beam Sequence-Level Generation, which\nreframes TKG forecasting as a top-K sequence-level generation task, using beam\nsearch for efficiently generating next-entity distribution in a single forward\npass. (2) TKG Adapter for Refinement, which employs traditional TKG models as\ntrainable proxy adapters to leverage global graph information and refine LLM\noutputs, overcoming both the input length and the resource-intensive\nfine-tuning problems. Experiments across diverse datasets validate SPARK's\nforecasting performance, robust generalization capabilities, and high\nefficiency. We release source codes at https://github.com/yin-gz/SPARK.",
      "tldr_zh": "该研究针对 Temporal Knowledge Graph (TKG) 预测中的挑战，提出 SPARK 框架，这是一个高效的 Sequence-level Proxy-Adapting 方案，用于精炼 Large Language Models (LLMs) 以提升预测未来事件的能力。SPARK 通过两个关键创新解决现有问题：Beam Sequence-Level Generation，将 TKG 预测转化为 top-K 序列级生成任务，利用 beam search 在单次前向传递中高效生成实体分布；以及 TKG Adapter for Refinement，使用传统 TKG 模型作为可训练代理适配器，整合全局图信息并优化 LLM 输出，避免输入长度和资源密集型问题。实验在多种数据集上验证了 SPARK 的出色预测性能、鲁棒泛化能力和高效率，并开源了代码。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.4"
      ],
      "primary_category": "cs.LG",
      "comment": "To be published in the 30th International Conference on Database\n  Systems for Advanced Applications (DASFAA 2025)",
      "pdf_url": "http://arxiv.org/pdf/2503.22748v1",
      "published_date": "2025-03-27 03:02:02 UTC",
      "updated_date": "2025-03-27 03:02:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:56:56.382112"
    },
    {
      "arxiv_id": "2503.22747v1",
      "title": "LeForecast: Enterprise Hybrid Forecast by Time Series Intelligence",
      "title_zh": "LeForecast：基于时间序列智能的企业混合预测",
      "authors": [
        "Zheng Tan",
        "Yiwen Nie",
        "Wenfa Wu",
        "Guanyu Zhang",
        "Yanze Liu",
        "Xinyuan Tian",
        "Kailin Gao",
        "Mengya Liu",
        "Qijiang Cheng",
        "Haipeng Jiang",
        "Yingzheng Ma",
        "Wei Zheng",
        "Yuci Zhu",
        "Yuanyuan Sun",
        "Xiangyu Lei",
        "Xiyu Guan",
        "Wanqing Huang",
        "Shouming Liu",
        "Xiangquan Meng",
        "Pengzhan Qu",
        "Chao Yang",
        "Jiaxuan Fan",
        "Yuan He",
        "Hongsheng Qi",
        "Yangzhou Du"
      ],
      "abstract": "Demand is spiking in industrial fields for multidisciplinary forecasting,\nwhere a broad spectrum of sectors needs planning and forecasts to streamline\nintelligent business management, such as demand forecasting, product planning,\ninventory optimization, etc. Specifically, these tasks expecting intelligent\napproaches to learn from sequentially collected historical data and then\nforesee most possible trend, i.e. time series forecasting. Challenge of it lies\nin interpreting complex business contexts and the efficiency and generalisation\nof modelling. With aspirations of pre-trained foundational models for such\npurpose, given their remarkable success of large foundation model across\nlegions of tasks, we disseminate \\leforecast{}, an enterprise intelligence\nplatform tailored for time series tasks. It integrates advanced interpretations\nof time series data and multi-source information, and a three-pillar modelling\nengine combining a large foundation model (Le-TSFM), multimodal model and\nhybrid model to derive insights, predict or infer futures, and then drive\noptimisation across multiple sectors in enterprise operations. The framework is\ncomposed by a model pool, model profiling module, and two different fusion\napproaches regarding original model architectures. Experimental results verify\nthe efficiency of our trail fusion concepts: router-based fusion network and\ncoordination of large and small models, resulting in high costs for redundant\ndevelopment and maintenance of models. This work reviews deployment of\nLeForecast and its performance in three industrial use cases. Our comprehensive\nexperiments indicate that LeForecast is a profound and practical platform for\nefficient and competitive performance. And we do hope that this work can\nenlighten the research and grounding of time series techniques in accelerating\nenterprise.",
      "tldr_zh": "该论文提出LeForecast，一个针对企业时间序列预测的企业智能平台，旨在解决多学科预测需求，如需求预测和库存优化，通过从历史数据学习来预测趋势。平台整合了时间序列数据的先进解释和多源信息，采用三柱建模引擎，包括大型基础模型(Le-TSFM)、多模态模型和混合模型，以及模型池、模型剖析模块和两种融合方法（如路由器-based fusion network）。实验结果显示，该框架通过协调大型和小模型提高了预测效率，减少了模型开发维护成本，并在三个工业用例中表现出色，具有实际应用价值。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22747v1",
      "published_date": "2025-03-27 02:58:06 UTC",
      "updated_date": "2025-03-27 02:58:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:57:07.423532"
    },
    {
      "arxiv_id": "2503.21098v3",
      "title": "Alleviating LLM-based Generative Retrieval Hallucination in Alipay Search",
      "title_zh": "翻译失败",
      "authors": [
        "Yedan Shen",
        "Kaixin Wu",
        "Yuechen Ding",
        "Jingyuan Wen",
        "Hong Liu",
        "Mingjie Zhong",
        "Zhouhan Lin",
        "Jia Xu",
        "Linjian Mo"
      ],
      "abstract": "Generative retrieval (GR) has revolutionized document retrieval with the\nadvent of large language models (LLMs), and LLM-based GR is gradually being\nadopted by the industry. Despite its remarkable advantages and potential,\nLLM-based GR suffers from hallucination and generates documents that are\nirrelevant to the query in some instances, severely challenging its credibility\nin practical applications. We thereby propose an optimized GR framework\ndesigned to alleviate retrieval hallucination, which integrates knowledge\ndistillation reasoning in model training and incorporate decision agent to\nfurther improve retrieval precision. Specifically, we employ LLMs to assess and\nreason GR retrieved query-document (q-d) pairs, and then distill the reasoning\ndata as transferred knowledge to the GR model. Moreover, we utilize a decision\nagent as post-processing to extend the GR retrieved documents through retrieval\nmodel and select the most relevant ones from multi perspectives as the final\ngenerative retrieval result. Extensive offline experiments on real-world\ndatasets and online A/B tests on Fund Search and Insurance Search in Alipay\ndemonstrate our framework's superiority and effectiveness in improving search\nquality and conversion gains.",
      "tldr_zh": "这篇论文针对 LLM-based Generative Retrieval (GR) 在 Alipay 搜索中的 hallucination 问题，提出一个优化框架来缓解生成无关文档的缺陷。该框架整合 knowledge distillation 推理，将 LLMs 用于评估和推理查询-文档 (q-d) 对，并将推理数据作为知识转移到 GR 模型中进行训练；此外，还引入 decision agent 作为后处理，扩展检索文档并从多角度选择最相关结果。实验结果显示，该框架在真实数据集的离线测试和 Alipay 的 Fund Search 及 Insurance Search 在线 A/B 测试中，显著提高了搜索质量和转换收益。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted by SIGIR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.21098v3",
      "published_date": "2025-03-27 02:36:48 UTC",
      "updated_date": "2025-05-13 11:54:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:57:19.647815"
    },
    {
      "arxiv_id": "2503.21095v1",
      "title": "Confidence Adjusted Surprise Measure for Active Resourceful Trials (CA-SMART): A Data-driven Active Learning Framework for Accelerating Material Discovery under Resource Constraints",
      "title_zh": "置信度调整惊喜度",
      "authors": [
        "Ahmed Shoyeb Raihan",
        "Zhichao Liu",
        "Tanveer Hossain Bhuiyan",
        "Imtiaz Ahmed"
      ],
      "abstract": "Accelerating the discovery and manufacturing of advanced materials with\nspecific properties is a critical yet formidable challenge due to vast search\nspace, high costs of experiments, and time-intensive nature of material\ncharacterization. In recent years, active learning, where a surrogate machine\nlearning (ML) model mimics the scientific discovery process of a human\nscientist, has emerged as a promising approach to address these challenges by\nguiding experimentation toward high-value outcomes with a limited budget. Among\nthe diverse active learning philosophies, the concept of surprise (capturing\nthe divergence between expected and observed outcomes) has demonstrated\nsignificant potential to drive experimental trials and refine predictive\nmodels. Scientific discovery often stems from surprise thereby making it a\nnatural driver to guide the search process. Despite its promise, prior studies\nleveraging surprise metrics such as Shannon and Bayesian surprise lack\nmechanisms to account for prior confidence, leading to excessive exploration of\nuncertain regions that may not yield useful information. To address this, we\npropose the Confidence-Adjusted Surprise Measure for Active Resourceful Trials\n(CA-SMART), a novel Bayesian active learning framework tailored for optimizing\ndata-driven experimentation. On a high level, CA-SMART incorporates\nConfidence-Adjusted Surprise (CAS) to dynamically balance exploration and\nexploitation by amplifying surprises in regions where the model is more certain\nwhile discounting them in highly uncertain areas. We evaluated CA-SMART on two\nbenchmark functions (Six-Hump Camelback and Griewank) and in predicting the\nfatigue strength of steel. The results demonstrate superior accuracy and\nefficiency compared to traditional surprise metrics, standard Bayesian\nOptimization (BO) acquisition functions and conventional ML methods.",
      "tldr_zh": "该研究针对材料发现面临的巨大搜索空间、高实验成本和耗时问题，提出了一种数据驱动的主动学习框架CA-SMART（Confidence Adjusted Surprise Measure for Active Resourceful Trials）。CA-SMART引入Confidence-Adjusted Surprise (CAS)度量，通过动态平衡探索和利用——放大模型置信度高的区域惊喜并减少不确定区域的惊喜——来优化实验过程，从而加速高价值材料的发现。与传统surprise指标如Shannon和Bayesian surprise相比，CA-SMART在Six-Hump Camelback、Griewank基准函数以及钢疲劳强度预测任务中表现出色，实现了更高的准确性和效率，优于标准Bayesian Optimization和常规机器学习方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.AP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21095v1",
      "published_date": "2025-03-27 02:21:42 UTC",
      "updated_date": "2025-03-27 02:21:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:57:30.939386"
    },
    {
      "arxiv_id": "2503.21088v2",
      "title": "ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging",
      "title_zh": "翻译失败",
      "authors": [
        "Haoming Xu",
        "Shuxun Wang",
        "Yanqiu Zhao",
        "Yi Zhong",
        "Ziyan Jiang",
        "Ningyuan Zhao",
        "Shumin Deng",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "abstract": "This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4:\nUnlearning Sensitive Content from Large Language Models. This task aims to\nselectively erase sensitive knowledge from large language models, avoiding both\nover-forgetting and under-forgetting issues. We propose an unlearning system\nthat leverages Model Merging (specifically TIES-Merging), combining two\nspecialized models into a more balanced unlearned model. Our system achieves\ncompetitive results, ranking second among 26 teams, with an online score of\n0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we\nalso conduct local experiments and perform a comprehensive analysis of the\nunlearning process, examining performance trajectories, loss dynamics, and\nweight perspectives, along with several supplementary experiments, to\nunderstand the effectiveness of our method. Furthermore, we analyze the\nshortcomings of our method and evaluation metrics, emphasizing that MIA scores\nand ROUGE-based metrics alone are insufficient to fully evaluate successful\nunlearning. Finally, we emphasize the need for more comprehensive evaluation\nmethodologies and rethinking of unlearning objectives in future research. Code\nis available at https://github.com/zjunlp/unlearn/tree/main/semeval25.",
      "tldr_zh": "ZJUKLAB 团队在 SemEval-2025 Task 4 中提出了一种基于 Model Merging（specifically TIES-Merging）的系统，用于从大型语言模型中选择性地删除敏感知识，同时避免 over-forgetting 和 under-forgetting 问题。该方法通过合并两个专业模型，实现了更平衡的 unlearning 效果，并在比赛中排名第二，获得 0.944 的 Task Aggregate 分数和 0.487 的整体 Aggregate 分数。通过本地实验和分析，他们考察了性能轨迹、损失动态以及权重变化，并指出 MIA scores 和 ROUGE-based metrics 不足以全面评估 unlearning 成功，呼吁未来研究采用更全面的评估方法和重新审视 unlearning 目标。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CL",
      "comment": "SemEval@ACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.21088v2",
      "published_date": "2025-03-27 02:03:25 UTC",
      "updated_date": "2025-04-20 00:37:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:57:44.849909"
    },
    {
      "arxiv_id": "2503.21074v3",
      "title": "Rerouting Connection: Hybrid Computer Vision Analysis Reveals Visual Similarity Between Indus and Tibetan-Yi Corridor Writing Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Ooha Lakkadi Reddy"
      ],
      "abstract": "This thesis employs a hybrid CNN-Transformer architecture, alongside a\ndetailed anthropological framework, to investigate potential historical\nconnections between the visual morphology of the Indus Valley script and\npictographic systems of the Tibetan-Yi Corridor. Through an ensemble\nmethodology of three target scripts across 15 independently trained models, we\ndemonstrate that Tibetan-Yi Corridor scripts exhibit approximately six-fold\nhigher visual similarity to the Indus script (0.635) than to the Bronze Age\nProto-Cuneiform (0.102) or Proto-Elamite (0.078).\n  Contrary to expectations, when measured through direct script-to-script\nembedding comparisons, the Indus script maps closer to Tibetan-Yi Corridor\nscripts with a mean cosine similarity of 0.930 (CI: [0.917, 0.942]) than to\ncontemporaneous West Asian signaries, which recorded mean similarities of 0.887\n(CI: [0.863, 0.911]) and 0.855 (CI: [0.818, 0.891]). Across dimensionality\nreduction and clustering methods, the Indus script consistently clusters\nclosest to Tibetan-Yi Corridor scripts.\n  These computational findings align with observed pictorial parallels in\nnumeral systems, gender markers, and iconographic elements. Archaeological\nevidence of contact networks along the ancient Shu-Shendu road, coinciding with\nthe Indus Civilization's decline, provides a plausible transmission pathway.\nWhile alternate explanations cannot be ruled out, the specificity and\nconsistency of similarities suggest more complex cultural transmission networks\nbetween South and East Asia than previously recognized.",
      "tldr_zh": "这篇论文使用混合 CNN-Transformer 架构和人类学框架，分析了 Indus Valley 脚本与 Tibetan-Yi Corridor 象形系统的视觉形态相似度。研究通过集成方法在 15 个独立训练模型上比较三种目标脚本，发现 Tibetan-Yi Corridor 脚本与 Indus 脚本的相似度约为 0.635，是与 Bronze Age Proto-Cuneiform (0.102) 或 Proto-Elamite (0.078) 的六倍。进一步的脚本嵌入比较显示，Indus 脚本与 Tibetan-Yi Corridor 脚本的平均余弦相似度为 0.930（置信区间 [0.917, 0.942]），明显高于与西亚脚本的相似度。总体结果与考古证据一致，表明古代南亚和东亚之间可能存在更复杂的文化传输网络。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "107 pages (43 main text, 6 references, 58 appendices). 21 figures, 4\n  tables in main text; 106 figures, 8 tables total. Code available at\n  https://github.com/oohalakkadi/ivc2tyc. Undergraduate thesis at Duke Kunshan\n  University. Accepted for presentation at the 52nd International Conference\n  for Computer Applications & Quantitative Methods in Archaeology (CAA 2025),\n  Athens, Greece",
      "pdf_url": "http://arxiv.org/pdf/2503.21074v3",
      "published_date": "2025-03-27 01:19:47 UTC",
      "updated_date": "2025-04-19 09:18:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:57:57.279226"
    },
    {
      "arxiv_id": "2503.21067v1",
      "title": "AskSport: Web Application for Sports Question-Answering",
      "title_zh": "AskSport：用于体育问答的",
      "authors": [
        "Enzo B Onofre",
        "Leonardo M P Moraes",
        "Cristina D Aguiar"
      ],
      "abstract": "This paper introduces AskSport, a question-answering web application about\nsports. It allows users to ask questions using natural language and retrieve\nthe three most relevant answers, including related information and documents.\nThe paper describes the characteristics and functionalities of the application,\nincluding use cases demonstrating its ability to return names and numerical\nvalues. AskSport and its implementation are available for public access on\nHuggingFace.",
      "tldr_zh": "这篇论文介绍了 AskSport，一款用于体育 Question-Answering 的网络应用，允许用户使用自然语言提问并获取三个最相关的答案，包括相关信息和文档。应用的关键功能包括返回名字和数值等细节，并通过用例演示了其实际效果。AskSport 的实现已公开在 HuggingFace 上，方便用户访问和使用。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "I.2.1; I.2.7"
      ],
      "primary_category": "cs.AI",
      "comment": "for accessing the application, see\n  https://huggingface.co/spaces/leomaurodesenv/qasports-website",
      "pdf_url": "http://arxiv.org/pdf/2503.21067v1",
      "published_date": "2025-03-27 00:57:27 UTC",
      "updated_date": "2025-03-27 00:57:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:58:06.712925"
    },
    {
      "arxiv_id": "2503.21834v1",
      "title": "A Multi-Modal Knowledge-Enhanced Framework for Vessel Trajectory Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Haomin Yu",
        "Tianyi Li",
        "Kristian Torp",
        "Christian S. Jensen"
      ],
      "abstract": "Accurate vessel trajectory prediction facilitates improved navigational\nsafety, routing, and environmental protection. However, existing prediction\nmethods are challenged by the irregular sampling time intervals of the vessel\ntracking data from the global AIS system and the complexity of vessel movement.\nThese aspects render model learning and generalization difficult. To address\nthese challenges and improve vessel trajectory prediction, we propose the\nmulti-modal knowledge-enhanced framework (MAKER) for vessel trajectory\nprediction. To contend better with the irregular sampling time intervals, MAKER\nfeatures a Large language model-guided Knowledge Transfer (LKT) module that\nleverages pre-trained language models to transfer trajectory-specific\ncontextual knowledge effectively. To enhance the ability to learn complex\ntrajectory patterns, MAKER incorporates a Knowledge-based Self-paced Learning\n(KSL) module. This module employs kinematic knowledge to progressively\nintegrate complex patterns during training, allowing for adaptive learning and\nenhanced generalization. Experimental results on two vessel trajectory datasets\nshow that MAKER can improve the prediction accuracy of state-of-the-art methods\nby 12.08%-17.86%.",
      "tldr_zh": "该论文提出MAKER（Multi-Modal Knowledge-Enhanced Framework），一个多模态知识增强框架，用于解决船舶轨迹预测中不规则采样时间间隔和复杂运动模式带来的模型学习与泛化难题。框架的核心组件包括LKT（Large Language Model-guided Knowledge Transfer）模块，利用预训练语言模型转移轨迹特定上下文知识，以及KSL（Knowledge-based Self-paced Learning）模块，通过运动学知识逐步整合复杂模式以提升适应性学习。在两个船舶轨迹数据集上的实验结果显示，MAKER比最先进方法提高了12.08%-17.86%的预测准确率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.21834v1",
      "published_date": "2025-03-27 00:01:35 UTC",
      "updated_date": "2025-03-27 00:01:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:58:18.766025"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 126,
  "processed_papers_count": 126,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-24T06:58:43.393168"
}
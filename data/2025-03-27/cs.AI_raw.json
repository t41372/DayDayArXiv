[
  {
    "arxiv_id": "2504.00019v1",
    "title": "ObscuraCoder: Powering Efficient Code LM Pre-Training Via Obfuscation Grounding",
    "authors": [
      "Indraneil Paul",
      "Haoyi Yang",
      "Goran Glavaš",
      "Kristian Kersting",
      "Iryna Gurevych"
    ],
    "abstract": "Language models (LMs) have become a staple of the code-writing toolbox. Their\npre-training recipe has, however, remained stagnant over recent years, barring\nthe occasional changes in data sourcing and filtering strategies. In\nparticular, research exploring modifications to Code-LMs' pre-training\nobjectives, geared towards improving data efficiency and better disentangling\nbetween syntax and semantics, has been noticeably sparse, especially compared\nwith corresponding efforts in natural language LMs. In this work, we examine\ngrounding on obfuscated code as a means of helping Code-LMs look beyond the\nsurface-form syntax and enhance their pre-training sample efficiency. To this\nend, we compile ObscuraX, a dataset of approximately 55M source and obfuscated\ncode pairs in seven languages. Subsequently, we pre-train ObscuraCoder models,\nranging in size from 255M to 2.8B parameters, on a 272B-token corpus that\nincludes ObscuraX and demonstrate that our obfuscation-based pre-training\nrecipe leads to consistent improvements in Code-LMs' abilities compared to both\nvanilla autoregressive pre-training as well as existing de-obfuscation (DOBF)\nobjectives. ObscuraCoder demonstrates sizeable gains across multiple tests of\nsyntactic and semantic code understanding, along with improved capabilities in\nmultilingual code completion, multilingual code commit summarization, and\nmulti-purpose library-oriented code generation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00019v1",
    "published_date": "2025-03-27 23:08:53 UTC",
    "updated_date": "2025-03-27 23:08:53 UTC"
  },
  {
    "arxiv_id": "2503.22036v2",
    "title": "Cognitive Prompts Using Guilford's Structure of Intellect Model",
    "authors": [
      "Oliver Kramer"
    ],
    "abstract": "Large language models (LLMs) demonstrate strong language generation\ncapabilities but often struggle with structured reasoning, leading to\ninconsistent or suboptimal problem-solving. To mitigate this limitation,\nGuilford's Structure of Intellect (SOI) model - a foundational framework from\nintelligence theory - is leveraged as the basis for cognitive prompt\nengineering. The SOI model categorizes cognitive operations such as pattern\nrecognition, memory retrieval, and evaluation, offering a systematic approach\nto enhancing LLM reasoning and decision-making. This position paper presents a\nnovel cognitive prompting approach for enforcing SOI-inspired reasoning for\nimproving clarity, coherence, and adaptability in model responses.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22036v2",
    "published_date": "2025-03-27 23:06:30 UTC",
    "updated_date": "2025-04-03 09:08:48 UTC"
  },
  {
    "arxiv_id": "2504.08751v1",
    "title": "Research on the Design of a Short Video Recommendation System Based on Multimodal Information and Differential Privacy",
    "authors": [
      "Haowei Yang",
      "Lei Fu",
      "Qingyi Lu",
      "Yue Fan",
      "Tianle Zhang",
      "Ruohan Wang"
    ],
    "abstract": "With the rapid development of short video platforms, recommendation systems\nhave become key technologies for improving user experience and enhancing\nplatform engagement. However, while short video recommendation systems leverage\nmultimodal information (such as images, text, and audio) to improve\nrecommendation effectiveness, they also face the severe challenge of user\nprivacy leakage. This paper proposes a short video recommendation system based\non multimodal information and differential privacy protection. First, deep\nlearning models are used for feature extraction and fusion of multimodal data,\neffectively improving recommendation accuracy. Then, a differential privacy\nprotection mechanism suitable for recommendation scenarios is designed to\nensure user data privacy while maintaining system performance. Experimental\nresults show that the proposed method outperforms existing mainstream\napproaches in terms of recommendation accuracy, multimodal fusion\neffectiveness, and privacy protection performance, providing important insights\nfor the design of recommendation systems for short video platforms.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08751v1",
    "published_date": "2025-03-27 22:56:41 UTC",
    "updated_date": "2025-03-27 22:56:41 UTC"
  },
  {
    "arxiv_id": "2503.22023v1",
    "title": "Safeguarding Autonomy: a Focus on Machine Learning Decision Systems",
    "authors": [
      "Paula Subías-Beltrán",
      "Oriol Pujol",
      "Itziar de Lecuona"
    ],
    "abstract": "As global discourse on AI regulation gains momentum, this paper focuses on\ndelineating the impact of ML on autonomy and fostering awareness. Respect for\nautonomy is a basic principle in bioethics that establishes persons as\ndecision-makers. While the concept of autonomy in the context of ML appears in\nseveral European normative publications, it remains a theoretical concept that\nhas yet to be widely accepted in ML practice. Our contribution is to bridge the\ntheoretical and practical gap by encouraging the practical application of\nautonomy in decision-making within ML practice by identifying the conditioning\nfactors that currently prevent it. Consequently, we focus on the different\nstages of the ML pipeline to identify the potential effects on ML end-users'\nautonomy. To improve its practical utility, we propose a related question for\neach detected impact, offering guidance for identifying possible focus points\nto respect ML end-users autonomy in decision-making.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22023v1",
    "published_date": "2025-03-27 22:31:16 UTC",
    "updated_date": "2025-03-27 22:31:16 UTC"
  },
  {
    "arxiv_id": "2503.22020v1",
    "title": "CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models",
    "authors": [
      "Qingqing Zhao",
      "Yao Lu",
      "Moo Jin Kim",
      "Zipeng Fu",
      "Zhuoyang Zhang",
      "Yecheng Wu",
      "Zhaoshuo Li",
      "Qianli Ma",
      "Song Han",
      "Chelsea Finn",
      "Ankur Handa",
      "Ming-Yu Liu",
      "Donglai Xiang",
      "Gordon Wetzstein",
      "Tsung-Yi Lin"
    ],
    "abstract": "Vision-language-action models (VLAs) have shown potential in leveraging\npretrained vision-language models and diverse robot demonstrations for learning\ngeneralizable sensorimotor control. While this paradigm effectively utilizes\nlarge-scale data from both robotic and non-robotic sources, current VLAs\nprimarily focus on direct input--output mappings, lacking the intermediate\nreasoning steps crucial for complex manipulation tasks. As a result, existing\nVLAs lack temporal planning or reasoning capabilities. In this paper, we\nintroduce a method that incorporates explicit visual chain-of-thought (CoT)\nreasoning into vision-language-action models (VLAs) by predicting future image\nframes autoregressively as visual goals before generating a short action\nsequence to achieve these goals. We introduce CoT-VLA, a state-of-the-art 7B\nVLA that can understand and generate visual and action tokens. Our experimental\nresults demonstrate that CoT-VLA achieves strong performance, outperforming the\nstate-of-the-art VLA model by 17% in real-world manipulation tasks and 6% in\nsimulation benchmarks. Project website: https://cot-vla.github.io/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Project website: https://cot-vla.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2503.22020v1",
    "published_date": "2025-03-27 22:23:04 UTC",
    "updated_date": "2025-03-27 22:23:04 UTC"
  },
  {
    "arxiv_id": "2503.21991v1",
    "title": "BOOTPLACE: Bootstrapped Object Placement with Detection Transformers",
    "authors": [
      "Hang Zhou",
      "Xinxin Zuo",
      "Rui Ma",
      "Li Cheng"
    ],
    "abstract": "In this paper, we tackle the copy-paste image-to-image composition problem\nwith a focus on object placement learning. Prior methods have leveraged\ngenerative models to reduce the reliance for dense supervision. However, this\noften limits their capacity to model complex data distributions. Alternatively,\ntransformer networks with a sparse contrastive loss have been explored, but\ntheir over-relaxed regularization often leads to imprecise object placement. We\nintroduce BOOTPLACE, a novel paradigm that formulates object placement as a\nplacement-by-detection problem. Our approach begins by identifying suitable\nregions of interest for object placement. This is achieved by training a\nspecialized detection transformer on object-subtracted backgrounds, enhanced\nwith multi-object supervisions. It then semantically associates each target\ncompositing object with detected regions based on their complementary\ncharacteristics. Through a boostrapped training approach applied to randomly\nobject-subtracted images, our model enforces meaningful placements through\nextensive paired data augmentation. Experimental results on established\nbenchmarks demonstrate BOOTPLACE's superior performance in object\nrepositioning, markedly surpassing state-of-the-art baselines on Cityscapes and\nOPA datasets with notable improvements in IOU scores. Additional ablation\nstudies further showcase the compositionality and generalizability of our\napproach, supported by user study evaluations.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025. Project page: https://ryanhangzhou.github.io/bootplace/ ,\n  code: https://github.com/RyanHangZhou/BOOTPLACE",
    "pdf_url": "http://arxiv.org/pdf/2503.21991v1",
    "published_date": "2025-03-27 21:21:20 UTC",
    "updated_date": "2025-03-27 21:21:20 UTC"
  },
  {
    "arxiv_id": "2503.21975v1",
    "title": "Pretrained Bayesian Non-parametric Knowledge Prior in Robotic Long-Horizon Reinforcement Learning",
    "authors": [
      "Yuan Meng",
      "Xiangtong Yao",
      "Kejia Chen",
      "Yansong Wu",
      "Liding Zhang",
      "Zhenshan Bing",
      "Alois Knoll"
    ],
    "abstract": "Reinforcement learning (RL) methods typically learn new tasks from scratch,\noften disregarding prior knowledge that could accelerate the learning process.\nWhile some methods incorporate previously learned skills, they usually rely on\na fixed structure, such as a single Gaussian distribution, to define skill\npriors. This rigid assumption can restrict the diversity and flexibility of\nskills, particularly in complex, long-horizon tasks. In this work, we introduce\na method that models potential primitive skill motions as having non-parametric\nproperties with an unknown number of underlying features. We utilize a Bayesian\nnon-parametric model, specifically Dirichlet Process Mixtures, enhanced with\nbirth and merge heuristics, to pre-train a skill prior that effectively\ncaptures the diverse nature of skills. Additionally, the learned skills are\nexplicitly trackable within the prior space, enhancing interpretability and\ncontrol. By integrating this flexible skill prior into an RL framework, our\napproach surpasses existing methods in long-horizon manipulation tasks,\nenabling more efficient skill transfer and task success in complex\nenvironments. Our findings show that a richer, non-parametric representation of\nskill priors significantly improves both the learning and execution of\nchallenging robotic tasks. All data, code, and videos are available at\nhttps://ghiara.github.io/HELIOS/.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "initial upload 8 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.21975v1",
    "published_date": "2025-03-27 20:43:36 UTC",
    "updated_date": "2025-03-27 20:43:36 UTC"
  },
  {
    "arxiv_id": "2503.21969v1",
    "title": "Data-Agnostic Robotic Long-Horizon Manipulation with Vision-Language-Guided Closed-Loop Feedback",
    "authors": [
      "Yuan Meng",
      "Xiangtong Yao",
      "Haihui Ye",
      "Yirui Zhou",
      "Shengqiang Zhang",
      "Zhenshan Bing",
      "Alois Knoll"
    ],
    "abstract": "Recent advances in language-conditioned robotic manipulation have leveraged\nimitation and reinforcement learning to enable robots to execute tasks from\nhuman commands. However, these methods often suffer from limited\ngeneralization, adaptability, and the lack of large-scale specialized datasets,\nunlike data-rich domains such as computer vision, making long-horizon task\nexecution challenging. To address these gaps, we introduce DAHLIA, a\ndata-agnostic framework for language-conditioned long-horizon robotic\nmanipulation, leveraging large language models (LLMs) for real-time task\nplanning and execution. DAHLIA employs a dual-tunnel architecture, where an\nLLM-powered planner collaborates with co-planners to decompose tasks and\ngenerate executable plans, while a reporter LLM provides closed-loop feedback,\nenabling adaptive re-planning and ensuring task recovery from potential\nfailures. Moreover, DAHLIA integrates chain-of-thought (CoT) in task reasoning\nand temporal abstraction for efficient action execution, enhancing traceability\nand robustness. Our framework demonstrates state-of-the-art performance across\ndiverse long-horizon tasks, achieving strong generalization in both simulated\nand real-world scenarios. Videos and code are available at\nhttps://ghiara.github.io/DAHLIA/.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "initial upload 8 page",
    "pdf_url": "http://arxiv.org/pdf/2503.21969v1",
    "published_date": "2025-03-27 20:32:58 UTC",
    "updated_date": "2025-03-27 20:32:58 UTC"
  },
  {
    "arxiv_id": "2503.21961v1",
    "title": "Entropy-Aware Branching for Improved Mathematical Reasoning",
    "authors": [
      "Xianzhi Li",
      "Ethan Callanan",
      "Xiaodan Zhu",
      "Mathieu Sibue",
      "Antony Papadimitriou",
      "Mahmoud Mahfouz",
      "Zhiqiang Ma",
      "Xiaomo Liu"
    ],
    "abstract": "While Large Language Models (LLMs) are effectively aligned through extensive\npre-training and fine-tuning, they still struggle with varying levels of\nuncertainty during token generation. In our investigation of mathematical\nreasoning, we observe that errors are more likely to arise at tokens exhibiting\nhigh entropy and variance of entropy in the model's output distribution. Based\non the observation, we propose a novel approach that dynamically branches the\ngeneration process on demand instead of defaulting to the single most probable\ntoken. By exploring in parallel multiple branches stemming from high\nprobability tokens of critical decision points, the model can discover diverse\nreasoning paths that might otherwise be missed. We further harness external\nfeedback from larger models to rank and select the most coherent and accurate\nreasoning branch. Our experimental results on mathematical word problems and\ncalculation questions show that this branching strategy boosts the reasoning\ncapabilities of small LLMs up to 4.6% compared to conventional argmax decoding.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21961v1",
    "published_date": "2025-03-27 20:18:22 UTC",
    "updated_date": "2025-03-27 20:18:22 UTC"
  },
  {
    "arxiv_id": "2503.22764v1",
    "title": "Boosting Large Language Models with Mask Fine-Tuning",
    "authors": [
      "Mingyuan Zhang",
      "Yue Bai",
      "Huan Wang",
      "Yizhou Wang",
      "Qihua Dong",
      "Yun Fu"
    ],
    "abstract": "The model is usually kept integral in the mainstream large language model\n(LLM) fine-tuning protocols. No works have questioned whether maintaining the\nintegrity of the model is indispensable for performance. In this work, we\nintroduce Mask Fine-Tuning (MFT), a brand-new LLM fine-tuning paradigm to show\nthat properly breaking the integrity of the model can surprisingly lead to\nimproved performance. Specifically, MFT learns a set of binary masks supervised\nby the typical LLM fine-tuning objective. Extensive experiments show that MFT\ngains a consistent performance boost across various domains and backbones\n(e.g., 1.95%/1.88% average gain in coding with LLaMA2-7B/3.1-8B). Detailed\nprocedures are provided to study the proposed MFT from different hyperparameter\nperspectives for better insight. In particular, MFT naturally updates the\ncurrent LLM training protocol by deploying it on a complete well-trained model.\nThis study extends the functionality of mask learning from its conventional\nnetwork pruning context for model compression to a more general scope.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22764v1",
    "published_date": "2025-03-27 20:17:57 UTC",
    "updated_date": "2025-03-27 20:17:57 UTC"
  },
  {
    "arxiv_id": "2503.21943v2",
    "title": "Parametric Shadow Control for Portrait Generation in Text-to-Image Diffusion Models",
    "authors": [
      "Haoming Cai",
      "Tsung-Wei Huang",
      "Shiv Gehlot",
      "Brandon Y. Feng",
      "Sachin Shah",
      "Guan-Ming Su",
      "Christopher Metzler"
    ],
    "abstract": "Text-to-image diffusion models excel at generating diverse portraits, but\nlack intuitive shadow control. Existing editing approaches, as post-processing,\nstruggle to offer effective manipulation across diverse styles. Additionally,\nthese methods either rely on expensive real-world light-stage data collection\nor require extensive computational resources for training. To address these\nlimitations, we introduce Shadow Director, a method that extracts and\nmanipulates hidden shadow attributes within well-trained diffusion models. Our\napproach uses a small estimation network that requires only a few thousand\nsynthetic images and hours of training-no costly real-world light-stage data\nneeded. Shadow Director enables parametric and intuitive control over shadow\nshape, placement, and intensity during portrait generation while preserving\nartistic integrity and identity across diverse styles. Despite training only on\nsynthetic data built on real-world identities, it generalizes effectively to\ngenerated portraits with diverse styles, making it a more accessible and\nresource-friendly solution.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "ShadowDirector Arxiv Version. Fix the arxiv title text issue",
    "pdf_url": "http://arxiv.org/pdf/2503.21943v2",
    "published_date": "2025-03-27 19:42:52 UTC",
    "updated_date": "2025-04-07 04:57:10 UTC"
  },
  {
    "arxiv_id": "2503.21937v1",
    "title": "Lobster: A GPU-Accelerated Framework for Neurosymbolic Programming",
    "authors": [
      "Paul Biberstein",
      "Ziyang Li",
      "Joseph Devietti",
      "Mayur Naik"
    ],
    "abstract": "Neurosymbolic programs combine deep learning with symbolic reasoning to\nachieve better data efficiency, interpretability, and generalizability compared\nto standalone deep learning approaches. However, existing neurosymbolic\nlearning frameworks implement an uneasy marriage between a highly scalable,\nGPU-accelerated neural component with a slower symbolic component that runs on\nCPUs. We propose Lobster, a unified framework for harnessing GPUs in an\nend-to-end manner for neurosymbolic learning. Lobster maps a general\nneurosymbolic language based on Datalog to the GPU programming paradigm. This\nmapping is implemented via compilation to a new intermediate language called\nAPM. The extra abstraction provided by APM allows Lobster to be both flexible,\nsupporting discrete, probabilistic, and differentiable modes of reasoning on\nGPU hardware with a library of provenance semirings, and performant,\nimplementing new optimization passes. We demonstrate that Lobster programs can\nsolve interesting problems spanning the domains of natural language processing,\nimage processing, program reasoning, bioinformatics, and planning. On a suite\nof 8 applications, Lobster achieves an average speedup of 5.3x over Scallop, a\nstate-of-the-art neurosymbolic framework, and enables scaling of neurosymbolic\nsolutions to previously infeasible tasks.",
    "categories": [
      "cs.PL",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "primary_category": "cs.PL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21937v1",
    "published_date": "2025-03-27 19:32:58 UTC",
    "updated_date": "2025-03-27 19:32:58 UTC"
  },
  {
    "arxiv_id": "2503.21928v1",
    "title": "An Efficient Training Algorithm for Models with Block-wise Sparsity",
    "authors": [
      "Ding Zhu",
      "Zhiqun Zuo",
      "Mohammad Mahdi Khalili"
    ],
    "abstract": "Large-scale machine learning (ML) models are increasingly being used in\ncritical domains like education, lending, recruitment, healthcare, criminal\njustice, etc. However, the training, deployment, and utilization of these\nmodels demand substantial computational resources. To decrease computation and\nmemory costs, machine learning models with sparse weight matrices are widely\nused in the literature. Among sparse models, those with special sparse\nstructures (e.g., models with block-wise sparse weight matrices) fit better\nwith the hardware accelerators and can decrease the memory and computation\ncosts during the inference. Unfortunately, while there are several efficient\ntraining methods, none of them are designed to train a block-wise sparse model\nefficiently. As a result, the current methods for training block-wise sparse\nmodels start with full and dense models leading to inefficient training. In\nthis work, we focus on training models with \\textit{block-wise sparse matrices}\nand propose an efficient training algorithm to decrease both computation and\nmemory costs during training and inference. In addition, we will show that our\nproposed method enables us to efficiently find the right block size for the\nsparsity pattern during the training process. Our extensive empirical and\ntheoretical analyses show that our algorithms can decrease the computation and\nmemory costs significantly without a performance drop compared to baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "24 pages, submitted on Transactions on Machine Learning Research",
    "pdf_url": "http://arxiv.org/pdf/2503.21928v1",
    "published_date": "2025-03-27 19:14:27 UTC",
    "updated_date": "2025-03-27 19:14:27 UTC"
  },
  {
    "arxiv_id": "2503.21911v1",
    "title": "AutoPsyC: Automatic Recognition of Psychodynamic Conflicts from Semi-structured Interviews with Large Language Models",
    "authors": [
      "Sayed Muddashir Hossain",
      "Simon Ostermann",
      "Patrick Gebhard",
      "Cord Benecke",
      "Josef van Genabith",
      "Philipp Müller"
    ],
    "abstract": "Psychodynamic conflicts are persistent, often unconscious themes that shape a\nperson's behaviour and experiences. Accurate diagnosis of psychodynamic\nconflicts is crucial for effective patient treatment and is commonly done via\nlong, manually scored semi-structured interviews. Existing automated solutions\nfor psychiatric diagnosis tend to focus on the recognition of broad disorder\ncategories such as depression, and it is unclear to what extent psychodynamic\nconflicts which even the patient themselves may not have conscious access to\ncould be automatically recognised from conversation. In this paper, we propose\nAutoPsyC, the first method for recognising the presence and significance of\npsychodynamic conflicts from full-length Operationalized Psychodynamic\nDiagnostics (OPD) interviews using Large Language Models (LLMs). Our approach\ncombines recent advances in parameter-efficient fine-tuning and\nRetrieval-Augmented Generation (RAG) with a summarisation strategy to\neffectively process entire 90 minute long conversations. In evaluations on a\ndataset of 141 diagnostic interviews we show that AutoPsyC consistently\noutperforms all baselines and ablation conditions on the recognition of four\nhighly relevant psychodynamic conflicts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21911v1",
    "published_date": "2025-03-27 18:41:35 UTC",
    "updated_date": "2025-03-27 18:41:35 UTC"
  },
  {
    "arxiv_id": "2503.21910v1",
    "title": "JEEM: Vision-Language Understanding in Four Arabic Dialects",
    "authors": [
      "Karima Kadaoui",
      "Hanin Atwany",
      "Hamdan Al-Ali",
      "Abdelrahman Mohamed",
      "Ali Mekky",
      "Sergei Tilga",
      "Natalia Fedorova",
      "Ekaterina Artemova",
      "Hanan Aldarmaki",
      "Yova Kementchedjhieva"
    ],
    "abstract": "We introduce JEEM, a benchmark designed to evaluate Vision-Language Models\n(VLMs) on visual understanding across four Arabic-speaking countries: Jordan,\nThe Emirates, Egypt, and Morocco. JEEM includes the tasks of image captioning\nand visual question answering, and features culturally rich and regionally\ndiverse content. This dataset aims to assess the ability of VLMs to generalize\nacross dialects and accurately interpret cultural elements in visual contexts.\nIn an evaluation of five prominent open-source Arabic VLMs and GPT-4V, we find\nthat the Arabic VLMs consistently underperform, struggling with both visual\nunderstanding and dialect-specific generation. While GPT-4V ranks best in this\ncomparison, the model's linguistic competence varies across dialects, and its\nvisual understanding capabilities lag behind. This underscores the need for\nmore inclusive models and the value of culturally-diverse evaluation paradigms.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21910v1",
    "published_date": "2025-03-27 18:41:21 UTC",
    "updated_date": "2025-03-27 18:41:21 UTC"
  },
  {
    "arxiv_id": "2503.22762v1",
    "title": "The Cost of Local and Global Fairness in Federated Learning",
    "authors": [
      "Yuying Duan",
      "Gelei Xu",
      "Yiyu Shi",
      "Michael Lemmon"
    ],
    "abstract": "With the emerging application of Federated Learning (FL) in finance, hiring\nand healthcare, FL models are regulated to be fair, preventing disparities with\nrespect to legally protected attributes such as race or gender. Two concepts of\nfairness are important in FL: global and local fairness. Global fairness\naddresses the disparity across the entire population and local fairness is\nconcerned with the disparity within each client. Prior fair FL frameworks have\nimproved either global or local fairness without considering both. Furthermore,\nwhile the majority of studies on fair FL focuses on binary settings, many\nreal-world applications are multi-class problems. This paper proposes a\nframework that investigates the minimum accuracy lost for enforcing a specified\nlevel of global and local fairness in multi-class FL settings. Our framework\nleads to a simple post-processing algorithm that derives fair outcome\npredictors from the Bayesian optimal score functions. Experimental results show\nthat our algorithm outperforms the current state of the art (SOTA) with regard\nto the accuracy-fairness tradoffs, computational and communication costs. Codes\nare available at:\nhttps://github.com/papersubmission678/The-cost-of-local-and-global-fairness-in-FL .",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22762v1",
    "published_date": "2025-03-27 18:37:54 UTC",
    "updated_date": "2025-03-27 18:37:54 UTC"
  },
  {
    "arxiv_id": "2503.21902v1",
    "title": "OntoAligner: A Comprehensive Modular and Robust Python Toolkit for Ontology Alignment",
    "authors": [
      "Hamed Babaei Giglou",
      "Jennifer D'Souza",
      "Oliver Karras",
      "Sören Auer"
    ],
    "abstract": "Ontology Alignment (OA) is fundamental for achieving semantic\ninteroperability across diverse knowledge systems. We present OntoAligner, a\ncomprehensive, modular, and robust Python toolkit for ontology alignment,\ndesigned to address current limitations with existing tools faced by\npractitioners. Existing tools are limited in scalability, modularity, and ease\nof integration with recent AI advances. OntoAligner provides a flexible\narchitecture integrating existing lightweight OA techniques such as fuzzy\nmatching but goes beyond by supporting contemporary methods with\nretrieval-augmented generation and large language models for OA. The framework\nprioritizes extensibility, enabling researchers to integrate custom alignment\nalgorithms and datasets. This paper details the design principles,\narchitecture, and implementation of the OntoAligner, demonstrating its utility\nthrough benchmarks on standard OA tasks. Our evaluation highlights\nOntoAligner's ability to handle large-scale ontologies efficiently with few\nlines of code while delivering high alignment quality. By making OntoAligner\nopen-source, we aim to provide a resource that fosters innovation and\ncollaboration within the OA community, empowering researchers and practitioners\nwith a toolkit for reproducible OA research and real-world applications.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 3 figures. Accepted for the ESWC 2025 Resource Track",
    "pdf_url": "http://arxiv.org/pdf/2503.21902v1",
    "published_date": "2025-03-27 18:28:11 UTC",
    "updated_date": "2025-03-27 18:28:11 UTC"
  },
  {
    "arxiv_id": "2503.21893v1",
    "title": "Exponentially Weighted Instance-Aware Repeat Factor Sampling for Long-Tailed Object Detection Model Training in Unmanned Aerial Vehicles Surveillance Scenarios",
    "authors": [
      "Taufiq Ahmed",
      "Abhishek Kumar",
      "Constantino Álvarez Casado",
      "Anlan Zhang",
      "Tuomo Hänninen",
      "Lauri Loven",
      "Miguel Bordallo López",
      "Sasu Tarkoma"
    ],
    "abstract": "Object detection models often struggle with class imbalance, where rare\ncategories appear significantly less frequently than common ones. Existing\nsampling-based rebalancing strategies, such as Repeat Factor Sampling (RFS) and\nInstance-Aware Repeat Factor Sampling (IRFS), mitigate this issue by adjusting\nsample frequencies based on image and instance counts. However, these methods\nare based on linear adjustments, which limit their effectiveness in long-tailed\ndistributions. This work introduces Exponentially Weighted Instance-Aware\nRepeat Factor Sampling (E-IRFS), an extension of IRFS that applies exponential\nscaling to better differentiate between rare and frequent classes. E-IRFS\nadjusts sampling probabilities using an exponential function applied to the\ngeometric mean of image and instance frequencies, ensuring a more adaptive\nrebalancing strategy. We evaluate E-IRFS on a dataset derived from the\nFireman-UAV-RGBT Dataset and four additional public datasets, using YOLOv11\nobject detection models to identify fire, smoke, people and lakes in emergency\nscenarios. The results show that E-IRFS improves detection performance by 22\\%\nover the baseline and outperforms RFS and IRFS, particularly for rare\ncategories. The analysis also highlights that E-IRFS has a stronger effect on\nlightweight models with limited capacity, as these models rely more on data\nsampling strategies to address class imbalance. The findings demonstrate that\nE-IRFS improves rare object detection in resource-constrained environments,\nmaking it a suitable solution for real-time applications such as UAV-based\nemergency monitoring.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 2 figures, 9 tables, 6 formulas, conference paper",
    "pdf_url": "http://arxiv.org/pdf/2503.21893v1",
    "published_date": "2025-03-27 18:09:37 UTC",
    "updated_date": "2025-03-27 18:09:37 UTC"
  },
  {
    "arxiv_id": "2503.21889v1",
    "title": "StarFlow: Generating Structured Workflow Outputs From Sketch Images",
    "authors": [
      "Patrice Bechard",
      "Chao Wang",
      "Amirhossein Abaskohi",
      "Juan Rodriguez",
      "Christopher Pal",
      "David Vazquez",
      "Spandana Gella",
      "Sai Rajeswar",
      "Perouz Taslakian"
    ],
    "abstract": "Workflows are a fundamental component of automation in enterprise platforms,\nenabling the orchestration of tasks, data processing, and system integrations.\nDespite being widely used, building workflows can be complex, often requiring\nmanual configuration through low-code platforms or visual programming tools. To\nsimplify this process, we explore the use of generative foundation models,\nparticularly vision-language models (VLMs), to automatically generate\nstructured workflows from visual inputs. Translating hand-drawn sketches or\ncomputer-generated diagrams into executable workflows is challenging due to the\nambiguity of free-form drawings, variations in diagram styles, and the\ndifficulty of inferring execution logic from visual elements. To address this,\nwe introduce StarFlow, a framework for generating structured workflow outputs\nfrom sketches using vision-language models. We curate a diverse dataset of\nworkflow diagrams -- including synthetic, manually annotated, and real-world\nsamples -- to enable robust training and evaluation. We finetune and benchmark\nmultiple vision-language models, conducting a series of ablation studies to\nanalyze the strengths and limitations of our approach. Our results show that\nfinetuning significantly enhances structured workflow generation, outperforming\nlarge vision-language models on this task.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21889v1",
    "published_date": "2025-03-27 18:04:05 UTC",
    "updated_date": "2025-03-27 18:04:05 UTC"
  },
  {
    "arxiv_id": "2503.21888v1",
    "title": "RedditESS: A Mental Health Social Support Interaction Dataset -- Understanding Effective Social Support to Refine AI-Driven Support Tools",
    "authors": [
      "Zeyad Alghamdi",
      "Tharindu Kumarage",
      "Garima Agrawal",
      "Mansooreh Karami",
      "Ibrahim Almuteb",
      "Huan Liu"
    ],
    "abstract": "Effective mental health support is crucial for alleviating psychological\ndistress. While large language model (LLM)-based assistants have shown promise\nin mental health interventions, existing research often defines \"effective\"\nsupport primarily in terms of empathetic acknowledgments, overlooking other\nessential dimensions such as informational guidance, community validation, and\ntangible coping strategies. To address this limitation and better understand\nwhat constitutes effective support, we introduce RedditESS, a novel real-world\ndataset derived from Reddit posts, including supportive comments and original\nposters' follow-up responses. Grounded in established social science theories,\nwe develop an ensemble labeling mechanism to annotate supportive comments as\neffective or not and perform qualitative assessments to ensure the reliability\nof the annotations. Additionally, we demonstrate the practical utility of\nRedditESS by using it to guide LLM alignment toward generating more\ncontext-sensitive and genuinely helpful supportive responses. By broadening the\nunderstanding of effective support, our study paves the way for advanced\nAI-driven mental health interventions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21888v1",
    "published_date": "2025-03-27 18:03:11 UTC",
    "updated_date": "2025-03-27 18:03:11 UTC"
  },
  {
    "arxiv_id": "2503.21878v2",
    "title": "Is Best-of-N the Best of Them? Coverage, Scaling, and Optimality in Inference-Time Alignment",
    "authors": [
      "Audrey Huang",
      "Adam Block",
      "Qinghua Liu",
      "Nan Jiang",
      "Akshay Krishnamurthy",
      "Dylan J. Foster"
    ],
    "abstract": "Inference-time computation offers a powerful axis for scaling the performance\nof language models. However, naively increasing computation in techniques like\nBest-of-N sampling can lead to performance degradation due to reward hacking.\nToward a theoretical understanding of how to best leverage additional\ncomputation, we focus on inference-time alignment, which we formalize as the\nproblem of improving the quality of responses drawn from a pre-trained policy,\ngiven a prompt of interest and access to an imperfect reward model. We analyze\nthe performance of inference-time alignment algorithms in terms of (i) response\nquality, and (ii) compute, and provide new results that highlight the\nimportance of the pre-trained policy's coverage over high-quality responses for\nperformance and compute scaling:\n  1. We show that Best-of-$N$ alignment with an ideal choice for $N$ can\nachieve optimal performance under stringent notions of coverage, but provably\nsuffers from reward hacking when $N$ is large, and fails to achieve tight\nguarantees under more realistic coverage conditions.\n  2. We introduce $\\texttt{InferenceTimePessimism}$, a new algorithm which\nmitigates reward hacking through deliberate use of inference-time compute,\nimplementing the principle of pessimism in the face of uncertainty via\nrejection sampling; we prove that its performance is optimal and does not\ndegrade with $N$, meaning it is scaling-monotonic.\n  We complement our theoretical results with an experimental evaluation that\ndemonstrate the benefits of $\\texttt{InferenceTimePessimism}$ across a variety\nof tasks and models.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21878v2",
    "published_date": "2025-03-27 18:00:08 UTC",
    "updated_date": "2025-04-07 17:44:38 UTC"
  },
  {
    "arxiv_id": "2503.21775v1",
    "title": "StyleMotif: Multi-Modal Motion Stylization using Style-Content Cross Fusion",
    "authors": [
      "Ziyu Guo",
      "Young Yoon Lee",
      "Joseph Liu",
      "Yizhak Ben-Shabat",
      "Victor Zordan",
      "Mubbasir Kapadia"
    ],
    "abstract": "We present StyleMotif, a novel Stylized Motion Latent Diffusion model,\ngenerating motion conditioned on both content and style from multiple\nmodalities. Unlike existing approaches that either focus on generating diverse\nmotion content or transferring style from sequences, StyleMotif seamlessly\nsynthesizes motion across a wide range of content while incorporating stylistic\ncues from multi-modal inputs, including motion, text, image, video, and audio.\nTo achieve this, we introduce a style-content cross fusion mechanism and align\na style encoder with a pre-trained multi-modal model, ensuring that the\ngenerated motion accurately captures the reference style while preserving\nrealism. Extensive experiments demonstrate that our framework surpasses\nexisting methods in stylized motion generation and exhibits emergent\ncapabilities for multi-modal motion stylization, enabling more nuanced motion\nsynthesis. Source code and pre-trained models will be released upon acceptance.\nProject Page: https://stylemotif.github.io",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://stylemotif.github.io",
    "pdf_url": "http://arxiv.org/pdf/2503.21775v1",
    "published_date": "2025-03-27 17:59:46 UTC",
    "updated_date": "2025-03-27 17:59:46 UTC"
  },
  {
    "arxiv_id": "2503.21766v1",
    "title": "Stable-SCore: A Stable Registration-based Framework for 3D Shape Correspondence",
    "authors": [
      "Haolin Liu",
      "Xiaohang Zhan",
      "Zizheng Yan",
      "Zhongjin Luo",
      "Yuxin Wen",
      "Xiaoguang Han"
    ],
    "abstract": "Establishing character shape correspondence is a critical and fundamental\ntask in computer vision and graphics, with diverse applications including\nre-topology, attribute transfer, and shape interpolation. Current dominant\nfunctional map methods, while effective in controlled scenarios, struggle in\nreal situations with more complex challenges such as non-isometric shape\ndiscrepancies. In response, we revisit registration-for-correspondence methods\nand tap their potential for more stable shape correspondence estimation. To\novercome their common issues including unstable deformations and the necessity\nfor careful pre-alignment or high-quality initial 3D correspondences, we\nintroduce Stable-SCore: A Stable Registration-based Framework for 3D Shape\nCorrespondence. We first re-purpose a foundation model for 2D character\ncorrespondence that ensures reliable and stable 2D mappings. Crucially, we\npropose a novel Semantic Flow Guided Registration approach that leverages 2D\ncorrespondence to guide mesh deformations. Our framework significantly\nsurpasses existing methods in challenging scenarios, and brings possibilities\nfor a wide array of real applications, as demonstrated in our results.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR 2025. Homepage:\n  https://haolinliu97.github.io/Stable-Score/",
    "pdf_url": "http://arxiv.org/pdf/2503.21766v1",
    "published_date": "2025-03-27 17:59:02 UTC",
    "updated_date": "2025-03-27 17:59:02 UTC"
  },
  {
    "arxiv_id": "2504.13865v1",
    "title": "A Survey on (M)LLM-Based GUI Agents",
    "authors": [
      "Fei Tang",
      "Haolei Xu",
      "Hang Zhang",
      "Siqi Chen",
      "Xingyu Wu",
      "Yongliang Shen",
      "Wenqi Zhang",
      "Guiyang Hou",
      "Zeqi Tan",
      "Yuchen Yan",
      "Kaitao Song",
      "Jian Shao",
      "Weiming Lu",
      "Jun Xiao",
      "Yueting Zhuang"
    ],
    "abstract": "Graphical User Interface (GUI) Agents have emerged as a transformative\nparadigm in human-computer interaction, evolving from rule-based automation\nscripts to sophisticated AI-driven systems capable of understanding and\nexecuting complex interface operations. This survey provides a comprehensive\nexamination of the rapidly advancing field of LLM-based GUI Agents,\nsystematically analyzing their architectural foundations, technical components,\nand evaluation methodologies. We identify and analyze four fundamental\ncomponents that constitute modern GUI Agents: (1) perception systems that\nintegrate text-based parsing with multimodal understanding for comprehensive\ninterface comprehension; (2) exploration mechanisms that construct and maintain\nknowledge bases through internal modeling, historical experience, and external\ninformation retrieval; (3) planning frameworks that leverage advanced reasoning\nmethodologies for task decomposition and execution; and (4) interaction systems\nthat manage action generation with robust safety controls. Through rigorous\nanalysis of these components, we reveal how recent advances in large language\nmodels and multimodal learning have revolutionized GUI automation across\ndesktop, mobile, and web platforms. We critically examine current evaluation\nframeworks, highlighting methodological limitations in existing benchmarks\nwhile proposing directions for standardization. This survey also identifies key\ntechnical challenges, including accurate element localization, effective\nknowledge retrieval, long-horizon planning, and safety-aware execution control,\nwhile outlining promising research directions for enhancing GUI Agents'\ncapabilities. Our systematic review provides researchers and practitioners with\na thorough understanding of the field's current state and offers insights into\nfuture developments in intelligent interface automation.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.13865v1",
    "published_date": "2025-03-27 17:58:31 UTC",
    "updated_date": "2025-03-27 17:58:31 UTC"
  },
  {
    "arxiv_id": "2503.21761v1",
    "title": "Uni4D: Unifying Visual Foundation Models for 4D Modeling from a Single Video",
    "authors": [
      "David Yifan Yao",
      "Albert J. Zhai",
      "Shenlong Wang"
    ],
    "abstract": "This paper presents a unified approach to understanding dynamic scenes from\ncasual videos. Large pretrained vision foundation models, such as\nvision-language, video depth prediction, motion tracking, and segmentation\nmodels, offer promising capabilities. However, training a single model for\ncomprehensive 4D understanding remains challenging. We introduce Uni4D, a\nmulti-stage optimization framework that harnesses multiple pretrained models to\nadvance dynamic 3D modeling, including static/dynamic reconstruction, camera\npose estimation, and dense 3D motion tracking. Our results show\nstate-of-the-art performance in dynamic 4D modeling with superior visual\nquality. Notably, Uni4D requires no retraining or fine-tuning, highlighting the\neffectiveness of repurposing visual foundation models for 4D understanding.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025. Project page (with code):\n  https://davidyao99.github.io/uni4d",
    "pdf_url": "http://arxiv.org/pdf/2503.21761v1",
    "published_date": "2025-03-27 17:57:32 UTC",
    "updated_date": "2025-03-27 17:57:32 UTC"
  },
  {
    "arxiv_id": "2503.21757v1",
    "title": "Fwd2Bot: LVLM Visual Token Compression with Double Forward Bottleneck",
    "authors": [
      "Adrian Bulat",
      "Yassine Ouali",
      "Georgios Tzimiropoulos"
    ],
    "abstract": "In this work, we aim to compress the vision tokens of a Large Vision Language\nModel (LVLM) into a representation that is simultaneously suitable for (a)\ngenerative and (b) discriminative tasks, (c) is nearly lossless, and (d) is\nstorage-efficient. We propose a novel compression approach, called Fwd2Bot,\nthat uses the LVLM itself to compress the visual information in a task-agnostic\nmanner. At the core of Fwd2bot there exists a \"double-forward pass\" training\nstrategy, whereby, during the first forward pass, the LLM (of the LVLM) creates\na bottleneck by condensing the visual information into a small number of\nsummary tokens. Then, using the same LLM, the second forward pass processes the\nlanguage instruction(s) alongside the summary tokens, used as a direct\nreplacement for the image ones. The training signal is provided by two losses:\nan autoregressive one applied after the second pass that provides a direct\noptimization objective for compression, and a contrastive loss, applied after\nthe first pass, that further boosts the representation strength, especially for\ndiscriminative tasks. The training is further enhanced by stage-specific\nadapters. We accompany the proposed method by an in-depth ablation study.\nOverall, Fwd2Bot results in highly-informative compressed representations\nsuitable for both generative and discriminative tasks. For generative tasks, we\noffer a 2x higher compression rate without compromising the generative\ncapabilities, setting a new state-of-the-art result. For discriminative tasks,\nwe set a new state-of-the-art on image retrieval and compositionality.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21757v1",
    "published_date": "2025-03-27 17:57:07 UTC",
    "updated_date": "2025-03-27 17:57:07 UTC"
  },
  {
    "arxiv_id": "2503.21747v1",
    "title": "CTRL-O: Language-Controllable Object-Centric Visual Representation Learning",
    "authors": [
      "Aniket Didolkar",
      "Andrii Zadaianchuk",
      "Rabiul Awal",
      "Maximilian Seitzer",
      "Efstratios Gavves",
      "Aishwarya Agrawal"
    ],
    "abstract": "Object-centric representation learning aims to decompose visual scenes into\nfixed-size vectors called \"slots\" or \"object files\", where each slot captures a\ndistinct object. Current state-of-the-art object-centric models have shown\nremarkable success in object discovery in diverse domains, including complex\nreal-world scenes. However, these models suffer from a key limitation: they\nlack controllability. Specifically, current object-centric models learn\nrepresentations based on their preconceived understanding of objects, without\nallowing user input to guide which objects are represented. Introducing\ncontrollability into object-centric models could unlock a range of useful\ncapabilities, such as the ability to extract instance-specific representations\nfrom a scene. In this work, we propose a novel approach for user-directed\ncontrol over slot representations by conditioning slots on language\ndescriptions. The proposed ConTRoLlable Object-centric representation learning\napproach, which we term CTRL-O, achieves targeted object-language binding in\ncomplex real-world scenes without requiring mask supervision. Next, we apply\nthese controllable slot representations on two downstream vision language\ntasks: text-to-image generation and visual question answering. The proposed\napproach enables instance-specific text-to-image generation and also achieves\nstrong performance on visual question answering.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.21747v1",
    "published_date": "2025-03-27 17:53:50 UTC",
    "updated_date": "2025-03-27 17:53:50 UTC"
  },
  {
    "arxiv_id": "2503.21735v1",
    "title": "GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release Analytics",
    "authors": [
      "Arsham Gholamzadeh Khoee",
      "Shuai Wang",
      "Yinan Yu",
      "Robert Feldt",
      "Dhasarathy Parthasarathy"
    ],
    "abstract": "Ensuring the reliability and effectiveness of software release decisions is\ncritical, particularly in safety-critical domains like automotive systems.\nPrecise analysis of release validation data, often presented in tabular form,\nplays a pivotal role in this process. However, traditional methods that rely on\nmanual analysis of extensive test datasets and validation metrics are prone to\ndelays and high costs. Large Language Models (LLMs) offer a promising\nalternative but face challenges in analytical reasoning, contextual\nunderstanding, handling out-of-scope queries, and processing structured test\ndata consistently; limitations that hinder their direct application in\nsafety-critical scenarios. This paper introduces GateLens, an LLM-based tool\nfor analyzing tabular data in the automotive domain. GateLens translates\nnatural language queries into Relational Algebra (RA) expressions and then\ngenerates optimized Python code. It outperforms the baseline system on\nbenchmarking datasets, achieving higher F1 scores and handling complex and\nambiguous queries with greater robustness. Ablation studies confirm the\ncritical role of the RA module, with performance dropping sharply when omitted.\nIndustrial evaluations reveal that GateLens reduces analysis time by over 80%\nwhile maintaining high accuracy and reliability. As demonstrated by presented\nresults, GateLens achieved high performance without relying on few-shot\nexamples, showcasing strong generalization across various query types from\ndiverse company roles. Insights from deploying GateLens with a partner\nautomotive company offer practical guidance for integrating AI into critical\nworkflows such as release validation. Results show that by automating test\nresult analysis, GateLens enables faster, more informed, and dependable release\ndecisions, and can thus advance software scalability and reliability in\nautomotive systems.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21735v1",
    "published_date": "2025-03-27 17:48:32 UTC",
    "updated_date": "2025-03-27 17:48:32 UTC"
  },
  {
    "arxiv_id": "2503.21729v3",
    "title": "ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation",
    "authors": [
      "Zhicheng Lee",
      "Shulin Cao",
      "Jinxin Liu",
      "Jiajie Zhang",
      "Weichuan Liu",
      "Xiaoyin Che",
      "Lei Hou",
      "Juanzi Li"
    ],
    "abstract": "Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely\nprimarily on parametric knowledge, limiting factual accuracy. While recent\nworks equip reinforcement learning (RL)-based LRMs with retrieval capabilities,\nthey suffer from overthinking and lack robustness in reasoning, reducing their\neffectiveness in question answering (QA) tasks. To address this, we propose\nReaRAG, a factuality-enhanced reasoning model that explores diverse queries\nwithout excessive iterations. Our solution includes a novel data construction\nframework with an upper bound on the reasoning chain length. Specifically, we\nfirst leverage an LRM to generate deliberate thinking, then select an action\nfrom a predefined action space (Search and Finish). For Search action, a query\nis executed against the RAG engine, where the result is returned as observation\nto guide reasoning steps later. This process iterates until a Finish action is\nchosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach\noutperforms existing baselines on multi-hop QA. Further analysis highlights its\nstrong reflective ability to recognize errors and refine its reasoning\ntrajectory. Our study enhances LRMs' factuality while effectively integrating\nrobust reasoning for Retrieval-Augmented Generation (RAG).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21729v3",
    "published_date": "2025-03-27 17:44:18 UTC",
    "updated_date": "2025-05-19 12:40:17 UTC"
  },
  {
    "arxiv_id": "2503.21720v1",
    "title": "Collab: Controlled Decoding using Mixture of Agents for LLM Alignment",
    "authors": [
      "Souradip Chakraborty",
      "Sujay Bhatt",
      "Udari Madhushani Sehwag",
      "Soumya Suvra Ghosal",
      "Jiahao Qiu",
      "Mengdi Wang",
      "Dinesh Manocha",
      "Furong Huang",
      "Alec Koppel",
      "Sumitra Ganesh"
    ],
    "abstract": "Alignment of Large Language models (LLMs) is crucial for safe and trustworthy\ndeployment in applications. Reinforcement learning from human feedback (RLHF)\nhas emerged as an effective technique to align LLMs to human preferences and\nbroader utilities, but it requires updating billions of model parameters, which\nis computationally expensive. Controlled Decoding, by contrast, provides a\nmechanism for aligning a model at inference time without retraining. However,\nsingle-agent decoding approaches often struggle to adapt to diverse tasks due\nto the complexity and variability inherent in these tasks. To strengthen the\ntest-time performance w.r.t the target task, we propose a mixture of\nagent-based decoding strategies leveraging the existing off-the-shelf aligned\nLLM policies. Treating each prior policy as an agent in the spirit of mixture\nof agent collaboration, we develop a decoding method that allows for\ninference-time alignment through a token-level selection strategy among\nmultiple agents. For each token, the most suitable LLM is dynamically chosen\nfrom a pool of models based on a long-term utility metric. This\npolicy-switching mechanism ensures optimal model selection at each step,\nenabling efficient collaboration and alignment among LLMs during decoding.\nTheoretical analysis of our proposed algorithm establishes optimal performance\nwith respect to the target task represented via a target reward for the given\noff-the-shelf models. We conduct comprehensive empirical evaluations with\nopen-source aligned models on diverse tasks and preferences, which demonstrates\nthe merits of this approach over single-agent decoding baselines. Notably,\nCollab surpasses the current SoTA decoding strategy, achieving an improvement\nof up to 1.56x in average reward and 71.89% in GPT-4 based win-tie rate.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.21720v1",
    "published_date": "2025-03-27 17:34:25 UTC",
    "updated_date": "2025-03-27 17:34:25 UTC"
  },
  {
    "arxiv_id": "2503.21718v3",
    "title": "Outlier dimensions favor frequent tokens in language models",
    "authors": [
      "Iuri Macocco",
      "Nora Graichen",
      "Gemma Boleda",
      "Marco Baroni"
    ],
    "abstract": "We study last-layer outlier dimensions, i.e. dimensions that display extreme\nactivations for the majority of inputs. We show that outlier dimensions arise\nin many different modern language models, and trace their function back to the\nheuristic of constantly predicting frequent words. We further show how a model\ncan block this heuristic when it is not contextually appropriate, by assigning\na counterbalancing weight mass to the remaining dimensions, and we investigate\nwhich model parameters boost outlier dimensions and when they arise during\ntraining. We conclude that outlier dimensions are a specialized mechanism\ndiscovered by many distinct models to implement a useful token prediction\nheuristic.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.21718v3",
    "published_date": "2025-03-27 17:30:50 UTC",
    "updated_date": "2025-04-09 14:37:48 UTC"
  },
  {
    "arxiv_id": "2503.21708v2",
    "title": "The Mathematical Relationship Between Layer Normalization and Dynamic Activation Functions",
    "authors": [
      "Felix Stollenwerk"
    ],
    "abstract": "A recent paper proposes Dynamic Tanh (DyT) as a drop-in replacement for layer\nnormalization (LN). Although the method is empirically well-motivated and\nappealing from a practical point of view, it lacks a theoretical foundation. In\nthis work, we shed light on the mathematical relationship between layer\nnormalization and dynamic activation functions. In particular, we derive DyT\nfrom LN and show that a well-defined approximation is needed to do so. By\ndropping said approximation, an alternative activation function is obtained,\nwhich we call Dynamic Inverse Square Root Unit (DyISRU). DyISRU is the exact\ncounterpart of layer normalization, and we demonstrate numerically that it\nindeed resembles LN more accurately than DyT does.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "New title, renamed DyISRU, added missing parentheses in proof of\n  theorem 3, minor language corrections",
    "pdf_url": "http://arxiv.org/pdf/2503.21708v2",
    "published_date": "2025-03-27 17:20:44 UTC",
    "updated_date": "2025-03-31 12:10:24 UTC"
  },
  {
    "arxiv_id": "2504.00017v1",
    "title": "Enhance Vision-based Tactile Sensors via Dynamic Illumination and Image Fusion",
    "authors": [
      "Artemii Redkin",
      "Zdravko Dugonjic",
      "Mike Lambeta",
      "Roberto Calandra"
    ],
    "abstract": "Vision-based tactile sensors use structured light to measure deformation in\ntheir elastomeric interface. Until now, vision-based tactile sensors such as\nDIGIT and GelSight have been using a single, static pattern of structured light\ntuned to the specific form factor of the sensor. In this work, we investigate\nthe effectiveness of dynamic illumination patterns, in conjunction with image\nfusion techniques, to improve the quality of sensing of vision-based tactile\nsensors. Specifically, we propose to capture multiple measurements, each with a\ndifferent illumination pattern, and then fuse them together to obtain a single,\nhigher-quality measurement. Experimental results demonstrate that this type of\ndynamic illumination yields significant improvements in image contrast,\nsharpness, and background difference. This discovery opens the possibility of\nretroactively improving the sensing quality of existing vision-based tactile\nsensors with a simple software update, and for new hardware designs capable of\nfully exploiting dynamic illumination.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.00017v1",
    "published_date": "2025-03-27 17:19:57 UTC",
    "updated_date": "2025-03-27 17:19:57 UTC"
  },
  {
    "arxiv_id": "2503.21854v1",
    "title": "Foveated Instance Segmentation",
    "authors": [
      "Hongyi Zeng",
      "Wenxuan Liu",
      "Tianhua Xia",
      "Jinhui Chen",
      "Ziyun Li",
      "Sai Qian Zhang"
    ],
    "abstract": "Instance segmentation is essential for augmented reality and virtual reality\n(AR/VR) as it enables precise object recognition and interaction, enhancing the\nintegration of virtual and real-world elements for an immersive experience.\nHowever, the high computational overhead of segmentation limits its application\non resource-constrained AR/VR devices, causing large processing latency and\ndegrading user experience. In contrast to conventional scenarios, AR/VR users\ntypically focus on only a few regions within their field of view before\nshifting perspective, allowing segmentation to be concentrated on gaze-specific\nareas. This insight drives the need for efficient segmentation methods that\nprioritize processing instance of interest, reducing computational load and\nenhancing real-time performance. In this paper, we present a foveated instance\nsegmentation (FovealSeg) framework that leverages real-time user gaze data to\nperform instance segmentation exclusively on instance of interest, resulting in\nsubstantial computational savings. Evaluation results show that FSNet achieves\nan IoU of 0.56 on ADE20K and 0.54 on LVIS, notably outperforming the baseline.\nThe code is available at https://github.com/SAI-",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21854v1",
    "published_date": "2025-03-27 17:08:44 UTC",
    "updated_date": "2025-03-27 17:08:44 UTC"
  },
  {
    "arxiv_id": "2503.21699v1",
    "title": "MAVERIX: Multimodal Audio-Visual Evaluation Reasoning IndeX",
    "authors": [
      "Liuyue Xie",
      "George Z. Wei",
      "Avik Kuthiala",
      "Ce Zheng",
      "Ananya Bal",
      "Mosam Dabhi",
      "Liting Wen",
      "Taru Rustagi",
      "Ethan Lai",
      "Sushil Khyalia",
      "Rohan Choudhury",
      "Morteza Ziyadi",
      "Xu Zhang",
      "Hao Yang",
      "László A. Jeni"
    ],
    "abstract": "Frontier models have either been language-only or have primarily focused on\nvision and language modalities. Although recent advancements in models with\nvision and audio understanding capabilities have shown substantial progress,\nthe field lacks a standardized evaluation framework for thoroughly assessing\ntheir cross-modality perception performance. We introduce MAVERIX~(Multimodal\nAudio-Visual Evaluation Reasoning IndeX), a novel benchmark with 700 videos and\n2,556 questions explicitly designed to evaluate multimodal models through tasks\nthat necessitate close integration of video and audio information. MAVERIX\nuniquely provides models with audiovisual tasks, closely mimicking the\nmultimodal perceptual experiences available to humans during inference and\ndecision-making processes. To our knowledge, MAVERIX is the first benchmark\naimed explicitly at assessing comprehensive audiovisual integration.\nExperiments with state-of-the-art models, including Gemini 1.5 Pro and o1, show\nperformance approaching human levels (around 70% accuracy), while human experts\nreach near-ceiling performance (95.1%). With standardized evaluation protocols,\na rigorously annotated pipeline, and a public toolkit, MAVERIX establishes a\nchallenging testbed for advancing audiovisual multimodal intelligence.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CV",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.MM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21699v1",
    "published_date": "2025-03-27 17:04:33 UTC",
    "updated_date": "2025-03-27 17:04:33 UTC"
  },
  {
    "arxiv_id": "2503.21695v1",
    "title": "AMA-SAM: Adversarial Multi-Domain Alignment of Segment Anything Model for High-Fidelity Histology Nuclei Segmentation",
    "authors": [
      "Jiahe Qian",
      "Yaoyu Fang",
      "Jinkui Hao",
      "Bo Zhou"
    ],
    "abstract": "Accurate segmentation of cell nuclei in histopathology images is essential\nfor numerous biomedical research and clinical applications. However, existing\ncell nucleus segmentation methods only consider a single dataset (i.e., primary\ndomain), while neglecting to leverage supplementary data from diverse sources\n(i.e., auxiliary domains) to reduce overfitting and enhance the performance.\nAlthough incorporating multiple datasets could alleviate overfitting, it often\nexacerbates performance drops caused by domain shifts. In this work, we\nintroduce Adversarial Multi-domain Alignment of Segment Anything Model\n(AMA-SAM) that extends the Segment Anything Model (SAM) to overcome these\nobstacles through two key innovations. First, we propose a Conditional Gradient\nReversal Layer (CGRL), a multi-domain alignment module that harmonizes features\nfrom diverse domains to promote domain-invariant representation learning while\npreserving crucial discriminative features for the primary dataset. Second, we\naddress SAM's inherent low-resolution output by designing a High-Resolution\nDecoder (HR-Decoder), which directly produces fine-grained segmentation maps in\norder to capture intricate nuclei boundaries in high-resolution histology\nimages. To the best of our knowledge, this is the first attempt to adapt SAM\nfor multi-dataset learning with application to histology nuclei segmentation.\nWe validate our method on several publicly available datasets, demonstrating\nconsistent and significant improvements over state-of-the-art approaches.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 4 tables, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.21695v1",
    "published_date": "2025-03-27 16:59:39 UTC",
    "updated_date": "2025-03-27 16:59:39 UTC"
  },
  {
    "arxiv_id": "2503.21694v1",
    "title": "Progressive Rendering Distillation: Adapting Stable Diffusion for Instant Text-to-Mesh Generation without 3D Data",
    "authors": [
      "Zhiyuan Ma",
      "Xinyue Liang",
      "Rongyuan Wu",
      "Xiangyu Zhu",
      "Zhen Lei",
      "Lei Zhang"
    ],
    "abstract": "It is highly desirable to obtain a model that can generate high-quality 3D\nmeshes from text prompts in just seconds. While recent attempts have adapted\npre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into\ngenerators of 3D representations (e.g., Triplane), they often suffer from poor\nquality due to the lack of sufficient high-quality 3D training data. Aiming at\novercoming the data shortage, we propose a novel training scheme, termed as\nProgressive Rendering Distillation (PRD), eliminating the need for 3D\nground-truths by distilling multi-view diffusion models and adapting SD into a\nnative 3D generator. In each iteration of training, PRD uses the U-Net to\nprogressively denoise the latent from random noise for a few steps, and in each\nstep it decodes the denoised latent into 3D output. Multi-view diffusion\nmodels, including MVDream and RichDreamer, are used in joint with SD to distill\ntext-consistent textures and geometries into the 3D outputs through score\ndistillation. Since PRD supports training without 3D ground-truths, we can\neasily scale up the training data and improve generation quality for\nchallenging text prompts with creative concepts. Meanwhile, PRD can accelerate\nthe inference speed of the generation model in just a few steps. With PRD, we\ntrain a Triplane generator, namely TriplaneTurbo, which adds only $2.5\\%$\ntrainable parameters to adapt SD for Triplane generation. TriplaneTurbo\noutperforms previous text-to-3D generators in both efficiency and quality.\nSpecifically, it can produce high-quality 3D meshes in 1.2 seconds and\ngeneralize well for challenging text input. The code is available at\nhttps://github.com/theEricMa/TriplaneTurbo.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "Accepted to CVPR 2025.\n  Code:https://github.com/theEricMa/TriplaneTurbo.\n  Demo:https://huggingface.co/spaces/ZhiyuanthePony/TriplaneTurbo",
    "pdf_url": "http://arxiv.org/pdf/2503.21694v1",
    "published_date": "2025-03-27 16:59:15 UTC",
    "updated_date": "2025-03-27 16:59:15 UTC"
  },
  {
    "arxiv_id": "2503.21683v1",
    "title": "LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning",
    "authors": [
      "Hui Wang"
    ],
    "abstract": "In recent years, large language models (LLMs) have shown significant\nadvancements in natural language processing (NLP), with strong capa-bilities in\ngeneration, comprehension, and rea-soning. These models have found applications\nin education, intelligent decision-making, and gaming. However, effectively\nutilizing LLMs for strategic planning and decision-making in the game of Gomoku\nremains a challenge. This study aims to develop a Gomoku AI system based on\nLLMs, simulating the human learning process of playing chess. The system is\nde-signed to understand and apply Gomoku strat-egies and logic to make rational\ndecisions. The research methods include enabling the model to \"read the board,\"\n\"understand the rules,\" \"select strategies,\" and \"evaluate positions,\" while\nen-hancing its abilities through self-play and rein-forcement learning. The\nresults demonstrate that this approach significantly improves the se-lection of\nmove positions, resolves the issue of generating illegal positions, and reduces\npro-cess time through parallel position evaluation. After extensive self-play\ntraining, the model's Gomoku-playing capabilities have been notably enhanced.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21683v1",
    "published_date": "2025-03-27 16:52:25 UTC",
    "updated_date": "2025-03-27 16:52:25 UTC"
  },
  {
    "arxiv_id": "2503.21677v1",
    "title": "A tale of two goals: leveraging sequentiality in multi-goal scenarios",
    "authors": [
      "Olivier Serris",
      "Stéphane Doncieux",
      "Olivier Sigaud"
    ],
    "abstract": "Several hierarchical reinforcement learning methods leverage planning to\ncreate a graph or sequences of intermediate goals, guiding a lower-level\ngoal-conditioned (GC) policy to reach some final goals. The low-level policy is\ntypically conditioned on the current goal, with the aim of reaching it as\nquickly as possible. However, this approach can fail when an intermediate goal\ncan be reached in multiple ways, some of which may make it impossible to\ncontinue toward subsequent goals. To address this issue, we introduce two\ninstances of Markov Decision Process (MDP) where the optimization objective\nfavors policies that not only reach the current goal but also subsequent ones.\nIn the first, the agent is conditioned on both the current and final goals,\nwhile in the second, it is conditioned on the next two goals in the sequence.\nWe conduct a series of experiments on navigation and pole-balancing tasks in\nwhich sequences of intermediate goals are given. By evaluating policies trained\nwith TD3+HER on both the standard GC-MDP and our proposed MDPs, we show that,\nin most cases, conditioning on the next two goals improves stability and sample\nefficiency over other approaches.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T40"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.21677v1",
    "published_date": "2025-03-27 16:47:46 UTC",
    "updated_date": "2025-03-27 16:47:46 UTC"
  },
  {
    "arxiv_id": "2503.21848v1",
    "title": "Comparative Analysis of Image, Video, and Audio Classifiers for Automated News Video Segmentation",
    "authors": [
      "Jonathan Attard",
      "Dylan Seychell"
    ],
    "abstract": "News videos require efficient content organisation and retrieval systems, but\ntheir unstructured nature poses significant challenges for automated\nprocessing. This paper presents a comprehensive comparative analysis of image,\nvideo, and audio classifiers for automated news video segmentation. This work\npresents the development and evaluation of multiple deep learning approaches,\nincluding ResNet, ViViT, AST, and multimodal architectures, to classify five\ndistinct segment types: advertisements, stories, studio scenes, transitions,\nand visualisations. Using a custom-annotated dataset of 41 news videos\ncomprising 1,832 scene clips, our experiments demonstrate that image-based\nclassifiers achieve superior performance (84.34\\% accuracy) compared to more\ncomplex temporal models. Notably, the ResNet architecture outperformed\nstate-of-the-art video classifiers while requiring significantly fewer\ncomputational resources. Binary classification models achieved high accuracy\nfor transitions (94.23\\%) and advertisements (92.74\\%). These findings advance\nthe understanding of effective architectures for news video segmentation and\nprovide practical insights for implementing automated content organisation\nsystems in media applications. These include media archiving, personalised\ncontent delivery, and intelligent video search.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint for paper in CAI 2025, 7 pages, 5 tables, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.21848v1",
    "published_date": "2025-03-27 16:42:50 UTC",
    "updated_date": "2025-03-27 16:42:50 UTC"
  },
  {
    "arxiv_id": "2503.21674v1",
    "title": "Intelligent IoT Attack Detection Design via ODLLM with Feature Ranking-based Knowledge Base",
    "authors": [
      "Satvik Verma",
      "Qun Wang",
      "E. Wes Bethel"
    ],
    "abstract": "The widespread adoption of Internet of Things (IoT) devices has introduced\nsignificant cybersecurity challenges, particularly with the increasing\nfrequency and sophistication of Distributed Denial of Service (DDoS) attacks.\nTraditional machine learning (ML) techniques often fall short in detecting such\nattacks due to the complexity of blended and evolving patterns. To address\nthis, we propose a novel framework leveraging On-Device Large Language Models\n(ODLLMs) augmented with fine-tuning and knowledge base (KB) integration for\nintelligent IoT network attack detection. By implementing feature ranking\ntechniques and constructing both long and short KBs tailored to model\ncapacities, the proposed framework ensures efficient and accurate detection of\nDDoS attacks while overcoming computational and privacy limitations. Simulation\nresults demonstrate that the optimized framework achieves superior accuracy\nacross diverse attack types, especially when using compact models in edge\ncomputing environments. This work provides a scalable and secure solution for\nreal-time IoT security, advancing the applicability of edge intelligence in\ncybersecurity.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21674v1",
    "published_date": "2025-03-27 16:41:57 UTC",
    "updated_date": "2025-03-27 16:41:57 UTC"
  },
  {
    "arxiv_id": "2503.21847v1",
    "title": "ReCoM: Realistic Co-Speech Motion Generation with Recurrent Embedded Transformer",
    "authors": [
      "Yong Xie",
      "Yunlian Sun",
      "Hongwen Zhang",
      "Yebin Liu",
      "Jinhui Tang"
    ],
    "abstract": "We present ReCoM, an efficient framework for generating high-fidelity and\ngeneralizable human body motions synchronized with speech. The core innovation\nlies in the Recurrent Embedded Transformer (RET), which integrates Dynamic\nEmbedding Regularization (DER) into a Vision Transformer (ViT) core\narchitecture to explicitly model co-speech motion dynamics. This architecture\nenables joint spatial-temporal dependency modeling, thereby enhancing gesture\nnaturalness and fidelity through coherent motion synthesis. To enhance model\nrobustness, we incorporate the proposed DER strategy, which equips the model\nwith dual capabilities of noise resistance and cross-domain generalization,\nthereby improving the naturalness and fluency of zero-shot motion generation\nfor unseen speech inputs. To mitigate inherent limitations of autoregressive\ninference, including error accumulation and limited self-correction, we propose\nan iterative reconstruction inference (IRI) strategy. IRI refines motion\nsequences via cyclic pose reconstruction, driven by two key components: (1)\nclassifier-free guidance improves distribution alignment between generated and\nreal gestures without auxiliary supervision, and (2) a temporal smoothing\nprocess eliminates abrupt inter-frame transitions while ensuring kinematic\ncontinuity. Extensive experiments on benchmark datasets validate ReCoM's\neffectiveness, achieving state-of-the-art performance across metrics. Notably,\nit reduces the Fr\\'echet Gesture Distance (FGD) from 18.70 to 2.48,\ndemonstrating an 86.7% improvement in motion realism. Our project page is\nhttps://yong-xie-xy.github.io/ReCoM/.",
    "categories": [
      "cs.GR",
      "cs.AI"
    ],
    "primary_category": "cs.GR",
    "comment": "8 pages, 6 figures, Project Page:\n  https://yong-xie-xy.github.io/ReCoM/",
    "pdf_url": "http://arxiv.org/pdf/2503.21847v1",
    "published_date": "2025-03-27 16:39:40 UTC",
    "updated_date": "2025-03-27 16:39:40 UTC"
  },
  {
    "arxiv_id": "2503.21846v2",
    "title": "LightSNN: Lightweight Architecture Search for Sparse and Accurate Spiking Neural Networks",
    "authors": [
      "Yesmine Abdennadher",
      "Giovanni Perin",
      "Riccardo Mazzieri",
      "Jacopo Pegoraro",
      "Michele Rossi"
    ],
    "abstract": "Spiking Neural Networks (SNNs) are highly regarded for their energy\nefficiency, inherent activation sparsity, and suitability for real-time\nprocessing in edge devices. However, most current SNN methods adopt\narchitectures resembling traditional artificial neural networks (ANNs), leading\nto suboptimal performance when applied to SNNs. While SNNs excel in energy\nefficiency, they have been associated with lower accuracy levels than\ntraditional ANNs when utilizing conventional architectures. In response, in\nthis work we present LightSNN, a rapid and efficient Neural Network\nArchitecture Search (NAS) technique specifically tailored for SNNs that\nautonomously leverages the most suitable architecture, striking a good balance\nbetween accuracy and efficiency by enforcing sparsity. Based on the spiking NAS\nnetwork (SNASNet) framework, a cell-based search space including backward\nconnections is utilized to build our training-free pruning-based NAS mechanism.\nOur technique assesses diverse spike activation patterns across different data\nsamples using a sparsity-aware Hamming distance fitness evaluation. Thorough\nexperiments are conducted on both static (CIFAR10 and CIFAR100) and\nneuromorphic datasets (DVS128-Gesture). Our LightSNN model achieves\nstate-of-the-art results on CIFAR10 and CIFAR100, improves performance on\nDVS128Gesture by 4.49\\%, and significantly reduces search time most notably\noffering a $98\\times$ speedup over SNASNet and running 30\\% faster than the\nbest existing method on DVS128Gesture. Code is available on Github at:\nhttps://github.com/YesmineAbdennadher/LightSNN.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.NE",
    "comment": "Accepted to AMLDS 2025 (Tokyo, July 2025). 6 pages, 3 figures, 2\n  tables",
    "pdf_url": "http://arxiv.org/pdf/2503.21846v2",
    "published_date": "2025-03-27 16:38:13 UTC",
    "updated_date": "2025-05-12 13:38:26 UTC"
  },
  {
    "arxiv_id": "2503.21670v1",
    "title": "COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in Hindi-English Code-Mixing",
    "authors": [
      "Rajvee Sheth",
      "Himanshu Beniwal",
      "Mayank Singh"
    ],
    "abstract": "The rapid growth of digital communication has driven the widespread use of\ncode-mixing, particularly Hindi-English, in multilingual communities. Existing\ndatasets often focus on romanized text, have limited scope, or rely on\nsynthetic data, which fails to capture realworld language nuances. Human\nannotations are crucial for assessing the naturalness and acceptability of\ncode-mixed text. To address these challenges, We introduce COMI-LINGUA, the\nlargest manually annotated dataset for code-mixed text, comprising 100,970\ninstances evaluated by three expert annotators in both Devanagari and Roman\nscripts. The dataset supports five fundamental NLP tasks: Language\nIdentification, Matrix Language Identification, Part-of-Speech Tagging, Named\nEntity Recognition, and Translation. We evaluate LLMs on these tasks using\nCOMILINGUA, revealing limitations in current multilingual modeling strategies\nand emphasizing the need for improved code-mixed text processing capabilities.\nCOMI-LINGUA is publically availabe at:\nhttps://huggingface.co/datasets/LingoIITGN/COMI-LINGUA.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21670v1",
    "published_date": "2025-03-27 16:36:39 UTC",
    "updated_date": "2025-03-27 16:36:39 UTC"
  },
  {
    "arxiv_id": "2503.21668v2",
    "title": "Cognitive Science-Inspired Evaluation of Core Capabilities for Object Understanding in AI",
    "authors": [
      "Danaja Rutar",
      "Alva Markelius",
      "Konstantinos Voudouris",
      "José Hernández-Orallo",
      "Lucy Cheke"
    ],
    "abstract": "One of the core components of our world models is 'intuitive physics' - an\nunderstanding of objects, space, and causality. This capability enables us to\npredict events, plan action and navigate environments, all of which rely on a\ncomposite sense of objecthood. Despite its importance, there is no single,\nunified account of objecthood, though multiple theoretical frameworks provide\ninsights. In the first part of this paper, we present a comprehensive overview\nof the main theoretical frameworks in objecthood research - Gestalt psychology,\nenactive cognition, and developmental psychology - and identify the core\ncapabilities each framework attributes to object understanding, as well as what\nfunctional roles they play in shaping world models in biological agents. Given\nthe foundational role of objecthood in world modelling, understanding\nobjecthood is also essential in AI. In the second part of the paper, we\nevaluate how current AI paradigms approach and test objecthood capabilities\ncompared to those in cognitive science. We define an AI paradigm as a\ncombination of how objecthood is conceptualised, the methods used for studying\nobjecthood, the data utilised, and the evaluation techniques. We find that,\nwhilst benchmarks can detect that AI systems model isolated aspects of\nobjecthood, the benchmarks cannot detect when AI systems lack functional\nintegration across these capabilities, not solving the objecthood challenge\nfully. Finally, we explore novel evaluation approaches that align with the\nintegrated vision of objecthood outlined in this paper. These methods are\npromising candidates for advancing from isolated object capabilities toward\ngeneral-purpose AI with genuine object understanding in real-world contexts.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21668v2",
    "published_date": "2025-03-27 16:35:02 UTC",
    "updated_date": "2025-04-07 10:39:12 UTC"
  },
  {
    "arxiv_id": "2503.21657v1",
    "title": "Model Assembly Learning with Heterogeneous Layer Weight Merging",
    "authors": [
      "Yi-Kai Zhang",
      "Jin Wang",
      "Xu-Xiang Zhong",
      "De-Chuan Zhan",
      "Han-Jia Ye"
    ],
    "abstract": "Model merging acquires general capabilities without extra data or training by\ncombining multiple models' parameters. Previous approaches achieve linear mode\nconnectivity by aligning parameters into the same loss basin using permutation\ninvariance. In this paper, we introduce Model Assembly Learning (MAL), a novel\nparadigm for model merging that iteratively integrates parameters from diverse\nmodels in an open-ended model zoo to enhance the base model's capabilities.\nUnlike previous works that require identical architectures, MAL allows the\nmerging of heterogeneous architectures and selective parameters across layers.\nSpecifically, the base model can incorporate parameters from different layers\nof multiple pre-trained models. We systematically investigate the conditions\nand fundamental settings of heterogeneous parameter merging, addressing all\npossible mismatches in layer widths between the base and target models.\nFurthermore, we establish key laws and provide practical guidelines for\neffectively implementing MAL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025 Workshop on Neural Network Weights as a New Data Modality",
    "pdf_url": "http://arxiv.org/pdf/2503.21657v1",
    "published_date": "2025-03-27 16:21:53 UTC",
    "updated_date": "2025-03-27 16:21:53 UTC"
  },
  {
    "arxiv_id": "2503.21646v1",
    "title": "Unlocking the Potential of Past Research: Using Generative AI to Reconstruct Healthcare Simulation Models",
    "authors": [
      "Thomas Monks",
      "Alison Harper",
      "Amy Heather"
    ],
    "abstract": "Discrete-event simulation (DES) is widely used in healthcare Operations\nResearch, but the models themselves are rarely shared. This limits their\npotential for reuse and long-term impact in the modelling and healthcare\ncommunities. This study explores the feasibility of using generative artificial\nintelligence (AI) to recreate published models using Free and Open Source\nSoftware (FOSS), based on the descriptions provided in an academic journal.\nUsing a structured methodology, we successfully generated, tested and\ninternally reproduced two DES models, including user interfaces. The reported\nresults were replicated for one model, but not the other, likely due to missing\ninformation on distributions. These models are substantially more complex than\nAI-generated DES models published to date. Given the challenges we faced in\nprompt engineering, code generation, and model testing, we conclude that our\niterative approach to model development, systematic comparison and testing, and\nthe expertise of our team were necessary to the success of our recreated\nsimulation models.",
    "categories": [
      "cs.AI",
      "stat.AP"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21646v1",
    "published_date": "2025-03-27 16:10:02 UTC",
    "updated_date": "2025-03-27 16:10:02 UTC"
  },
  {
    "arxiv_id": "2503.21640v1",
    "title": "Towards Fully Automated Decision-Making Systems for Greenhouse Control: Challenges and Opportunities",
    "authors": [
      "Yongshuai Liu",
      "Taeyeong Choi",
      "Xin Liu"
    ],
    "abstract": "Machine learning has been successful in building control policies to drive a\ncomplex system to desired states in various applications (e.g. games, robotics,\netc.). To be specific, a number of parameters of policy can be automatically\noptimized from the observations of environment to be able to generate a\nsequence of decisions leading to the best performance. In this survey paper, we\nparticularly explore such policy-learning techniques for another unique,\npractical use-case scenario--farming, in which critical decisions (e.g., water\nsupply, heating, etc.) must be made in a timely manner to minimize risks (e.g.,\ndamage to plants) while maximizing the revenue (e.g., healthy crops) in the\nend. We first provide a broad overview of latest studies on it to identify not\nonly domain-specific challenges but opportunities with potential solutions,\nsome of which are suggested as promising directions for future research. Also,\nwe then introduce our successful approach to being ranked second among 46 teams\nat the ''3rd Autonomous Greenhouse Challenge'' to use this specific example to\ndiscuss the lessons learned about important considerations for design to create\nautonomous farm-management systems.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21640v1",
    "published_date": "2025-03-27 16:06:59 UTC",
    "updated_date": "2025-03-27 16:06:59 UTC"
  },
  {
    "arxiv_id": "2503.21634v1",
    "title": "When Astronomy Meets AI: Manazel For Crescent Visibility Prediction in Morocco",
    "authors": [
      "Yassir Lairgi"
    ],
    "abstract": "The accurate determination of the beginning of each Hijri month is essential\nfor religious, cultural, and administrative purposes. Manazel (The code and\ndatasets are available at https://github.com/lairgiyassir/manazel) addresses\nthis challenge in Morocco by leveraging 13 years of crescent visibility data to\nrefine the ODEH criterion, a widely used standard for lunar crescent visibility\nprediction. The study integrates two key features, the Arc of Vision (ARCV) and\nthe total width of the crescent (W), to enhance the accuracy of lunar\nvisibility assessments. A machine learning approach utilizing the Logistic\nRegression algorithm is employed to classify crescent visibility conditions,\nachieving a predictive accuracy of 98.83%. This data-driven methodology offers\na robust and reliable framework for determining the start of the Hijri month,\ncomparing different data classification tools, and improving the consistency of\nlunar calendar calculations in Morocco. The findings demonstrate the\neffectiveness of machine learning in astronomical applications and highlight\nthe potential for further enhancements in the modeling of crescent visibility.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21634v1",
    "published_date": "2025-03-27 15:56:55 UTC",
    "updated_date": "2025-03-27 15:56:55 UTC"
  },
  {
    "arxiv_id": "2503.21620v4",
    "title": "UI-R1: Enhancing Efficient Action Prediction of GUI Agents by Reinforcement Learning",
    "authors": [
      "Zhengxi Lu",
      "Yuxiang Chai",
      "Yaxuan Guo",
      "Xi Yin",
      "Liang Liu",
      "Hao Wang",
      "Han Xiao",
      "Shuai Ren",
      "Guanjing Xiong",
      "Hongsheng Li"
    ],
    "abstract": "The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities\nin LLMs through reinforcement learning (RL) with rule-based rewards. Despite\nits success in language models, its application in multi-modal domains,\nparticularly in graphic user interface (GUI) agent tasks, remains\nunder-explored. To address this issue, we propose UI-R1, the first framework to\nexplore how rule-based RL can enhance the reasoning capabilities of multimodal\nlarge language models (MLLMs) for GUI action prediction tasks. Specifically,\nUI-R1 introduces a novel rule-based action reward, enabling model optimization\nvia policy-based algorithms such as Group Relative Policy Optimization (GRPO).\nFor efficient training, we curate a small yet high-quality dataset of 136\nchallenging tasks, encompassing five common action types on mobile devices.\nExperimental results demonstrate that our proposed UI-R1-3B achieves\nsignificant improvements over the base model (i.e. Qwen2.5-VL-3B) on both\nin-domain (ID) and out-of-domain (OOD) tasks, with average accuracy gains of\n22.1% on ScreenSpot, 6.0% on ScreenSpot-Pro, and 12.7% on ANDROIDCONTROL.\nFurthermore, UI-R1-3B delivers competitive performance compared to larger\nmodels (e.g., OS-Atlas-7B) trained via supervised fine-tuning (SFT) on 76K\nsamples. We additionally develop an optimized version, UI-R1-E-3B, which\nsignificantly improves both grounding efficiency and accuracy. These results\nunderscore the potential of rule-based reinforcement learning to advance GUI\nunderstanding and control, paving the way for future research in this domain.\nCode website: https://github.com/lll6gg/UI-R1.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Updated UI-R1-E-3B",
    "pdf_url": "http://arxiv.org/pdf/2503.21620v4",
    "published_date": "2025-03-27 15:39:30 UTC",
    "updated_date": "2025-05-14 11:56:07 UTC"
  },
  {
    "arxiv_id": "2503.21615v2",
    "title": "A Measure Based Generalizable Approach to Understandability",
    "authors": [
      "Vikas Kushwaha",
      "Sruti Srinivasa Ragavan",
      "Subhajit Roy"
    ],
    "abstract": "Successful agent-human partnerships require that any agent generated\ninformation is understandable to the human, and that the human can easily steer\nthe agent towards a goal. Such effective communication requires the agent to\ndevelop a finer-level notion of what is understandable to the human.\nState-of-the-art agents, including LLMs, lack this detailed notion of\nunderstandability because they only capture average human sensibilities from\nthe training data, and therefore afford limited steerability (e.g., requiring\nnon-trivial prompt engineering).\n  In this paper, instead of only relying on data, we argue for developing\ngeneralizable, domain-agnostic measures of understandability that can be used\nas directives for these agents. Existing research on understandability measures\nis fragmented, we survey various such efforts across domains, and lay a\ncognitive-science-rooted groundwork for more coherent and domain-agnostic\nresearch investigations in future.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.HC",
    "comment": "6 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.21615v2",
    "published_date": "2025-03-27 15:36:49 UTC",
    "updated_date": "2025-04-23 17:39:20 UTC"
  },
  {
    "arxiv_id": "2504.01024v1",
    "title": "Gaze-Guided 3D Hand Motion Prediction for Detecting Intent in Egocentric Grasping Tasks",
    "authors": [
      "Yufei He",
      "Xucong Zhang",
      "Arno H. A. Stienen"
    ],
    "abstract": "Human intention detection with hand motion prediction is critical to drive\nthe upper-extremity assistive robots in neurorehabilitation applications.\nHowever, the traditional methods relying on physiological signal measurement\nare restrictive and often lack environmental context. We propose a novel\napproach that predicts future sequences of both hand poses and joint positions.\nThis method integrates gaze information, historical hand motion sequences, and\nenvironmental object data, adapting dynamically to the assistive needs of the\npatient without prior knowledge of the intended object for grasping.\nSpecifically, we use a vector-quantized variational autoencoder for robust hand\npose encoding with an autoregressive generative transformer for effective hand\nmotion sequence prediction. We demonstrate the usability of these novel\ntechniques in a pilot study with healthy subjects. To train and evaluate the\nproposed method, we collect a dataset consisting of various types of grasp\nactions on different objects from multiple subjects. Through extensive\nexperiments, we demonstrate that the proposed method can successfully predict\nsequential hand movement. Especially, the gaze information shows significant\nenhancements in prediction capabilities, particularly with fewer input frames,\nhighlighting the potential of the proposed method for real-world applications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01024v1",
    "published_date": "2025-03-27 15:26:41 UTC",
    "updated_date": "2025-03-27 15:26:41 UTC"
  },
  {
    "arxiv_id": "2503.21602v1",
    "title": "GenEdit: Compounding Operators and Continuous Improvement to Tackle Text-to-SQL in the Enterprise",
    "authors": [
      "Karime Maamari",
      "Connor Landy",
      "Amine Mhedhbi"
    ],
    "abstract": "Recent advancements in Text-to-SQL, driven by large language models, are\ndemocratizing data access. Despite these advancements, enterprise deployments\nremain challenging due to the need to capture business-specific knowledge,\nhandle complex queries, and meet expectations of continuous improvements. To\naddress these issues, we designed and implemented GenEdit: our Text-to-SQL\ngeneration system that improves with user feedback. GenEdit builds and\nmaintains a company-specific knowledge set, employs a pipeline of operators\ndecomposing SQL generation, and uses feedback to update its knowledge set to\nimprove future SQL generations.\n  We describe GenEdit's architecture made of two core modules: (i) decomposed\nSQL generation; and (ii) knowledge set edits based on user feedback. For\ngeneration, GenEdit leverages compounding operators to improve knowledge\nretrieval and to create a plan as chain-of-thought steps that guides\ngeneration. GenEdit first retrieves relevant examples in an initial retrieval\nstage where original SQL queries are decomposed into sub-statements, clauses or\nsub-queries. It then also retrieves instructions and schema elements. Using the\nretrieved contextual information, GenEdit then generates step-by-step plan in\nnatural language on how to produce the query. Finally, GenEdit uses the plan to\ngenerate SQL, minimizing the need for model reasoning, which enhances complex\nSQL generation. If necessary, GenEdit regenerates the query based on syntactic\nand semantic errors. The knowledge set edits are recommended through an\ninteractive copilot, allowing users to iterate on their feedback and to\nregenerate SQL queries as needed. Each generation uses staged edits which\nupdate the generation prompt. Once the feedback is submitted, it gets merged\nafter passing regression testing and obtaining an approval, improving future\ngenerations.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21602v1",
    "published_date": "2025-03-27 15:22:02 UTC",
    "updated_date": "2025-03-27 15:22:02 UTC"
  },
  {
    "arxiv_id": "2503.21843v1",
    "title": "CMD-HAR: Cross-Modal Disentanglement for Wearable Human Activity Recognition",
    "authors": [
      "Hanyu Liu",
      "Siyao Li",
      "Ying Yu",
      "Yixuan Jiang",
      "Hang Xiao",
      "Jingxi Long",
      "Haotian Tang"
    ],
    "abstract": "Human Activity Recognition (HAR) is a fundamental technology for numerous\nhuman - centered intelligent applications. Although deep learning methods have\nbeen utilized to accelerate feature extraction, issues such as multimodal data\nmixing, activity heterogeneity, and complex model deployment remain largely\nunresolved. The aim of this paper is to address issues such as multimodal data\nmixing, activity heterogeneity, and complex model deployment in sensor-based\nhuman activity recognition. We propose a spatiotemporal attention modal\ndecomposition alignment fusion strategy to tackle the problem of the mixed\ndistribution of sensor data. Key discriminative features of activities are\ncaptured through cross-modal spatio-temporal disentangled representation, and\ngradient modulation is combined to alleviate data heterogeneity. In addition, a\nwearable deployment simulation system is constructed. We conducted experiments\non a large number of public datasets, demonstrating the effectiveness of the\nmodel.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21843v1",
    "published_date": "2025-03-27 15:21:49 UTC",
    "updated_date": "2025-03-27 15:21:49 UTC"
  },
  {
    "arxiv_id": "2503.21598v1",
    "title": "Prompt, Divide, and Conquer: Bypassing Large Language Model Safety Filters via Segmented and Distributed Prompt Processing",
    "authors": [
      "Johan Wahréus",
      "Ahmed Hussain",
      "Panos Papadimitratos"
    ],
    "abstract": "Large Language Models (LLMs) have transformed task automation and content\ngeneration across various domains while incorporating safety filters to prevent\nmisuse. We introduce a novel jailbreaking framework that employs distributed\nprompt processing combined with iterative refinements to bypass these safety\nmeasures, particularly in generating malicious code. Our architecture consists\nof four key modules: prompt segmentation, parallel processing, response\naggregation, and LLM-based jury evaluation. Tested on 500 malicious prompts\nacross 10 cybersecurity categories, the framework achieves a 73.2% Success Rate\n(SR) in generating malicious code. Notably, our comparative analysis reveals\nthat traditional single-LLM judge evaluation overestimates SRs (93.8%) compared\nto our LLM jury system (73.2%), with manual verification confirming that\nsingle-judge assessments often accept incomplete implementations. Moreover, we\ndemonstrate that our distributed architecture improves SRs by 12% over the\nnon-distributed approach in an ablation study, highlighting both the\neffectiveness of distributed prompt processing and the importance of robust\nevaluation methodologies in assessing jailbreak attempts.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "22 pages; 26 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.21598v1",
    "published_date": "2025-03-27 15:19:55 UTC",
    "updated_date": "2025-03-27 15:19:55 UTC"
  },
  {
    "arxiv_id": "2503.22759v1",
    "title": "Data Poisoning in Deep Learning: A Survey",
    "authors": [
      "Pinlong Zhao",
      "Weiyao Zhu",
      "Pengfei Jiao",
      "Di Gao",
      "Ou Wu"
    ],
    "abstract": "Deep learning has become a cornerstone of modern artificial intelligence,\nenabling transformative applications across a wide range of domains. As the\ncore element of deep learning, the quality and security of training data\ncritically influence model performance and reliability. However, during the\ntraining process, deep learning models face the significant threat of data\npoisoning, where attackers introduce maliciously manipulated training data to\ndegrade model accuracy or lead to anomalous behavior. While existing surveys\nprovide valuable insights into data poisoning, they generally adopt a broad\nperspective, encompassing both attacks and defenses, but lack a dedicated,\nin-depth analysis of poisoning attacks specifically in deep learning. In this\nsurvey, we bridge this gap by presenting a comprehensive and targeted review of\ndata poisoning in deep learning. First, this survey categorizes data poisoning\nattacks across multiple perspectives, providing an in-depth analysis of their\ncharacteristics and underlying design princinples. Second, the discussion is\nextended to the emerging area of data poisoning in large language models(LLMs).\nFinally, we explore critical open challenges in the field and propose potential\nresearch directions to advance the field further. To support further\nexploration, an up-to-date repository of resources on data poisoning in deep\nlearning is available at https://github.com/Pinlong-Zhao/Data-Poisoning.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22759v1",
    "published_date": "2025-03-27 15:16:57 UTC",
    "updated_date": "2025-03-27 15:16:57 UTC"
  },
  {
    "arxiv_id": "2503.21592v1",
    "title": "Critical Iterative Denoising: A Discrete Generative Model Applied to Graphs",
    "authors": [
      "Yoann Boget",
      "Alexandros Kalousis"
    ],
    "abstract": "Discrete Diffusion and Flow Matching models have significantly advanced\ngenerative modeling for discrete structures, including graphs. However, the\ntime dependencies in the noising process of these models lead to error\naccumulation and propagation during the backward process. This issue,\nparticularly pronounced in mask diffusion, is a known limitation in sequence\nmodeling and, as we demonstrate, also impacts discrete diffusion models for\ngraphs.\n  To address this problem, we propose a novel framework called Iterative\nDenoising, which simplifies discrete diffusion and circumvents the issue by\nassuming conditional independence across time. Additionally, we enhance our\nmodel by incorporating a Critic, which during generation selectively retains or\ncorrupts elements in an instance based on their likelihood under the data\ndistribution. Our empirical evaluations demonstrate that the proposed method\nsignificantly outperforms existing discrete diffusion baselines in graph\ngeneration tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21592v1",
    "published_date": "2025-03-27 15:08:58 UTC",
    "updated_date": "2025-03-27 15:08:58 UTC"
  },
  {
    "arxiv_id": "2503.21581v1",
    "title": "AlignDiff: Learning Physically-Grounded Camera Alignment via Diffusion",
    "authors": [
      "Liuyue Xie",
      "Jiancong Guo",
      "Ozan Cakmakci",
      "Andre Araujo",
      "Laszlo A. Jeni",
      "Zhiheng Jia"
    ],
    "abstract": "Accurate camera calibration is a fundamental task for 3D perception,\nespecially when dealing with real-world, in-the-wild environments where complex\noptical distortions are common. Existing methods often rely on pre-rectified\nimages or calibration patterns, which limits their applicability and\nflexibility. In this work, we introduce a novel framework that addresses these\nchallenges by jointly modeling camera intrinsic and extrinsic parameters using\na generic ray camera model. Unlike previous approaches, AlignDiff shifts focus\nfrom semantic to geometric features, enabling more accurate modeling of local\ndistortions. We propose AlignDiff, a diffusion model conditioned on geometric\npriors, enabling the simultaneous estimation of camera distortions and scene\ngeometry. To enhance distortion prediction, we incorporate edge-aware\nattention, focusing the model on geometric features around image edges, rather\nthan semantic content. Furthermore, to enhance generalizability to real-world\ncaptures, we incorporate a large database of ray-traced lenses containing over\nthree thousand samples. This database characterizes the distortion inherent in\na diverse variety of lens forms. Our experiments demonstrate that the proposed\nmethod significantly reduces the angular error of estimated ray bundles by ~8.2\ndegrees and overall calibration accuracy, outperforming existing approaches on\nchallenging, real-world datasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21581v1",
    "published_date": "2025-03-27 14:59:59 UTC",
    "updated_date": "2025-03-27 14:59:59 UTC"
  },
  {
    "arxiv_id": "2503.21571v1",
    "title": "Magnitude-Phase Dual-Path Speech Enhancement Network based on Self-Supervised Embedding and Perceptual Contrast Stretch Boosting",
    "authors": [
      "Alimjan Mattursun",
      "Liejun Wang",
      "Yinfeng Yu",
      "Chunyang Ma"
    ],
    "abstract": "Speech self-supervised learning (SSL) has made great progress in various\nspeech processing tasks, but there is still room for improvement in speech\nenhancement (SE). This paper presents BSP-MPNet, a dual-path framework that\ncombines self-supervised features with magnitude-phase information for SE. The\napproach starts by applying the perceptual contrast stretching (PCS) algorithm\nto enhance the magnitude-phase spectrum. A magnitude-phase 2D coarse (MP-2DC)\nencoder then extracts coarse features from the enhanced spectrum. Next, a\nfeature-separating self-supervised learning (FS-SSL) model generates\nself-supervised embeddings for the magnitude and phase components separately.\nThese embeddings are fused to create cross-domain feature representations.\nFinally, two parallel RNN-enhanced multi-attention (REMA) mask decoders refine\nthe features, apply them to the mask, and reconstruct the speech signal. We\nevaluate BSP-MPNet on the VoiceBank+DEMAND and WHAMR! datasets. Experimental\nresults show that BSP-MPNet outperforms existing methods under various noise\nconditions, providing new directions for self-supervised speech enhancement\nresearch. The implementation of the BSP-MPNet code is available\nonline\\footnote[2]{https://github.com/AlimMat/BSP-MPNet. \\label{s1}}",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Main paper (6 pages). Accepted for publication by ICME 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.21571v1",
    "published_date": "2025-03-27 14:52:06 UTC",
    "updated_date": "2025-03-27 14:52:06 UTC"
  },
  {
    "arxiv_id": "2503.21558v1",
    "title": "A Local Perspective-based Model for Overlapping Community Detection",
    "authors": [
      "Gaofeng Zhou",
      "Rui-Feng Wang",
      "Kangning Cui"
    ],
    "abstract": "Community detection, which identifies densely connected node clusters with\nsparse between-group links, is vital for analyzing network structure and\nfunction in real-world systems. Most existing community detection methods based\non GCNs primarily focus on node-level information while overlooking\ncommunity-level features, leading to performance limitations on large-scale\nnetworks. To address this issue, we propose LQ-GCN, an overlapping community\ndetection model from a local community perspective. LQ-GCN employs a\nBernoulli-Poisson model to construct a community affiliation matrix and form an\nend-to-end detection framework. By adopting local modularity as the objective\nfunction, the model incorporates local community information to enhance the\nquality and accuracy of clustering results. Additionally, the conventional GCNs\narchitecture is optimized to improve the model capability in identifying\noverlapping communities in large-scale networks. Experimental results\ndemonstrate that LQ-GCN achieves up to a 33% improvement in Normalized Mutual\nInformation (NMI) and a 26.3% improvement in Recall compared to baseline models\nacross multiple real-world benchmark datasets.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "10 pages, 3 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.21558v1",
    "published_date": "2025-03-27 14:43:42 UTC",
    "updated_date": "2025-03-27 14:43:42 UTC"
  },
  {
    "arxiv_id": "2503.21557v1",
    "title": "debug-gym: A Text-Based Environment for Interactive Debugging",
    "authors": [
      "Xingdi Yuan",
      "Morgane M Moss",
      "Charbel El Feghali",
      "Chinmay Singh",
      "Darya Moldavskaya",
      "Drew MacPhee",
      "Lucas Caccia",
      "Matheus Pereira",
      "Minseon Kim",
      "Alessandro Sordoni",
      "Marc-Alexandre Côté"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly relied upon for coding tasks,\nyet in most scenarios it is assumed that all relevant information can be either\naccessed in context or matches their training data. We posit that LLMs can\nbenefit from the ability to interactively explore a codebase to gather the\ninformation relevant to their task. To achieve this, we present a textual\nenvironment, namely debug-gym, for developing LLM-based agents in an\ninteractive coding setting. Our environment is lightweight and provides a\npreset of useful tools, such as a Python debugger (pdb), designed to facilitate\nan LLM-based agent's interactive debugging. Beyond coding and debugging tasks,\nthis approach can be generalized to other tasks that would benefit from\ninformation-seeking behavior by an LLM agent.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.PL",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21557v1",
    "published_date": "2025-03-27 14:43:28 UTC",
    "updated_date": "2025-03-27 14:43:28 UTC"
  },
  {
    "arxiv_id": "2503.21544v1",
    "title": "SWI: Speaking with Intent in Large Language Models",
    "authors": [
      "Yuwei Yin",
      "EunJeong Hwang",
      "Giuseppe Carenini"
    ],
    "abstract": "Intent, typically clearly formulated and planned, functions as a cognitive\nframework for reasoning and problem-solving. This paper introduces the concept\nof Speaking with Intent (SWI) in large language models (LLMs), where the\nexplicitly generated intent encapsulates the model's underlying intention and\nprovides high-level planning to guide subsequent analysis and communication. By\nemulating deliberate and purposeful thoughts in the human mind, SWI is\nhypothesized to enhance the reasoning capabilities and generation quality of\nLLMs. Extensive experiments on mathematical reasoning benchmarks consistently\ndemonstrate the superiority of Speaking with Intent over Baseline (i.e.,\ngeneration without explicit intent). Moreover, SWI outperforms answer-trigger\nprompting methods Chain-of-Thought and Plan-and-Solve and maintains competitive\nperformance with the strong method ARR (Analyzing, Retrieving, and Reasoning).\nAdditionally, the effectiveness and generalizability of SWI are solidified on\nreasoning-intensive question answering (QA) and text summarization benchmarks,\nwhere SWI brings consistent improvement to the Baseline generation. In text\nsummarization, SWI-generated summaries exhibit greater accuracy, conciseness,\nand factual correctness, with fewer hallucinations. Furthermore, human\nevaluations verify the coherence, effectiveness, and interpretability of the\nintent produced by SWI. This proof-of-concept study creates a novel avenue for\nenhancing LLMs' reasoning abilities with cognitive notions.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "24 pages. Code: https://github.com/YuweiYin/SWI",
    "pdf_url": "http://arxiv.org/pdf/2503.21544v1",
    "published_date": "2025-03-27 14:34:28 UTC",
    "updated_date": "2025-03-27 14:34:28 UTC"
  },
  {
    "arxiv_id": "2503.21541v2",
    "title": "LOCATEdit: Graph Laplacian Optimized Cross Attention for Localized Text-Guided Image Editing",
    "authors": [
      "Achint Soni",
      "Meet Soni",
      "Sirisha Rambhatla"
    ],
    "abstract": "Text-guided image editing aims to modify specific regions of an image\naccording to natural language instructions while maintaining the general\nstructure and the background fidelity. Existing methods utilize masks derived\nfrom cross-attention maps generated from diffusion models to identify the\ntarget regions for modification. However, since cross-attention mechanisms\nfocus on semantic relevance, they struggle to maintain the image integrity. As\na result, these methods often lack spatial consistency, leading to editing\nartifacts and distortions. In this work, we address these limitations and\nintroduce LOCATEdit, which enhances cross-attention maps through a graph-based\napproach utilizing self-attention-derived patch relationships to maintain\nsmooth, coherent attention across image regions, ensuring that alterations are\nlimited to the designated items while retaining the surrounding structure.\nLOCATEdit consistently and substantially outperforms existing baselines on\nPIE-Bench, demonstrating its state-of-the-art performance and effectiveness on\nvarious editing tasks. Code can be found on\nhttps://github.com/LOCATEdit/LOCATEdit/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21541v2",
    "published_date": "2025-03-27 14:32:17 UTC",
    "updated_date": "2025-03-28 12:17:07 UTC"
  },
  {
    "arxiv_id": "2503.21530v2",
    "title": "Low-Resource Transliteration for Roman-Urdu and Urdu Using Transformer-Based Models",
    "authors": [
      "Umer Butt",
      "Stalin Veranasi",
      "Günter Neumann"
    ],
    "abstract": "As the Information Retrieval (IR) field increasingly recognizes the\nimportance of inclusivity, addressing the needs of low-resource languages\nremains a significant challenge. Transliteration between Urdu and its Romanized\nform, Roman Urdu, remains underexplored despite the widespread use of both\nscripts in South Asia. Prior work using RNNs on the Roman-Urdu-Parl dataset\nshowed promising results but suffered from poor domain adaptability and limited\nevaluation. We propose a transformer-based approach using the m2m100\nmultilingual translation model, enhanced with masked language modeling (MLM)\npretraining and fine-tuning on both Roman-Urdu-Parl and the domain-diverse\nDakshina dataset. To address previous evaluation flaws, we introduce rigorous\ndataset splits and assess performance using BLEU, character-level BLEU, and\nCHRF. Our model achieves strong transliteration performance, with Char-BLEU\nscores of 96.37 for Urdu->Roman-Urdu and 97.44 for Roman-Urdu->Urdu. These\nresults outperform both RNN baselines and GPT-4o Mini and demonstrate the\neffectiveness of multilingual transfer learning for low-resource\ntransliteration tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21530v2",
    "published_date": "2025-03-27 14:18:50 UTC",
    "updated_date": "2025-04-04 09:55:38 UTC"
  },
  {
    "arxiv_id": "2503.21522v1",
    "title": "MONO2REST: Identifying and Exposing Microservices: a Reusable RESTification Approach",
    "authors": [
      "Matthéo Lecrivain",
      "Hanifa Barry",
      "Dalila Tamzalit",
      "Houari Sahraoui"
    ],
    "abstract": "The microservices architectural style has become the de facto standard for\nlarge-scale cloud applications, offering numerous benefits in scalability,\nmaintainability, and deployment flexibility. Many organizations are pursuing\nthe migration of legacy monolithic systems to a microservices architecture.\nHowever, this process is challenging, risky, time-intensive, and\nprone-to-failure while several organizations lack necessary financial\nresources, time, or expertise to set up this migration process. So, rather than\ntrying to migrate a legacy system where migration is risky or not feasible, we\nsuggest exposing it as a microservice application without without having to\nmigrate it. In this paper, we present a reusable, automated, two-phase approach\nthat combines evolutionary algorithms with machine learning techniques. In the\nfirst phase, we identify microservices at the method level using a\nmulti-objective genetic algorithm that considers both structural and semantic\ndependencies between methods. In the second phase, we generate REST APIs for\neach identified microservice using a classification algorithm to assign HTTP\nmethods and endpoints. We evaluated our approach with a case study on the\nSpring PetClinic application, which has both monolithic and microservices\nimplementations that serve as ground truth for comparison. Results demonstrate\nthat our approach successfully aligns identified microservices with those in\nthe reference microservices implementation, highlighting its effectiveness in\nservice identification and API generation.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21522v1",
    "published_date": "2025-03-27 14:10:33 UTC",
    "updated_date": "2025-03-27 14:10:33 UTC"
  },
  {
    "arxiv_id": "2503.21514v1",
    "title": "Quantitative Evaluation of Quantum/Classical Neural Network Using a Game Solver Metric",
    "authors": [
      "Suzukaze Kamei",
      "Hideaki Kawaguchi",
      "Shin Nishio",
      "Tatakahiko Satoh"
    ],
    "abstract": "To evaluate the performance of quantum computing systems relative to\nclassical counterparts and explore the potential for quantum advantage, we\npropose a game-solving benchmark based on Elo ratings in the game of\ntic-tac-toe. We compare classical convolutional neural networks (CNNs), quantum\nconvolutional neural networks (QCNNs), and hybrid classical-quantum models by\nassessing their performance against a random-move agent in automated matches.\nAdditionally, we implement a QCNN integrated with quantum communication and\nevaluate its performance to quantify the overhead introduced by noisy quantum\nchannels. Our results show that the classical-quantum hybrid model achieves Elo\nratings comparable to those of classical CNNs, while the standalone QCNN\nunderperforms under current hardware constraints. The communication overhead\nwas found to be modest. These findings demonstrate the viability of using\ngame-based benchmarks for evaluating quantum computing systems and suggest that\nquantum communication can be incorporated with limited impact on performance,\nproviding a foundation for future hybrid quantum applications.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "11 pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.21514v1",
    "published_date": "2025-03-27 14:05:16 UTC",
    "updated_date": "2025-03-27 14:05:16 UTC"
  },
  {
    "arxiv_id": "2503.21504v1",
    "title": "Keyword-Oriented Multimodal Modeling for Euphemism Identification",
    "authors": [
      "Yuxue Hu",
      "Junsong Li",
      "Meixuan Chen",
      "Dongyu Su",
      "Tongguan Wang",
      "Ying Sha"
    ],
    "abstract": "Euphemism identification deciphers the true meaning of euphemisms, such as\nlinking \"weed\" (euphemism) to \"marijuana\" (target keyword) in illicit texts,\naiding content moderation and combating underground markets. While existing\nmethods are primarily text-based, the rise of social media highlights the need\nfor multimodal analysis, incorporating text, images, and audio. However, the\nlack of multimodal datasets for euphemisms limits further research. To address\nthis, we regard euphemisms and their corresponding target keywords as keywords\nand first introduce a keyword-oriented multimodal corpus of euphemisms\n(KOM-Euph), involving three datasets (Drug, Weapon, and Sexuality), including\ntext, images, and speech. We further propose a keyword-oriented multimodal\neuphemism identification method (KOM-EI), which uses cross-modal feature\nalignment and dynamic fusion modules to explicitly utilize the visual and audio\nfeatures of the keywords for efficient euphemism identification. Extensive\nexperiments demonstrate that KOM-EI outperforms state-of-the-art models and\nlarge language models, and show the importance of our multimodal datasets.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21504v1",
    "published_date": "2025-03-27 13:45:35 UTC",
    "updated_date": "2025-03-27 13:45:35 UTC"
  },
  {
    "arxiv_id": "2503.22756v1",
    "title": "Towards an intelligent assessment system for evaluating the development of algorithmic thinking skills: An exploratory study in Swiss compulsory schools",
    "authors": [
      "Giorgia Adorni"
    ],
    "abstract": "The rapid digitalisation of contemporary society has profoundly impacted\nvarious facets of our lives, including healthcare, communication, business, and\neducation. The ability to engage with new technologies and solve problems has\nbecome crucial, making CT skills, such as pattern recognition, decomposition,\nand algorithm design, essential competencies. In response, Switzerland is\nconducting research and initiatives to integrate CT into its educational\nsystem. This study aims to develop a comprehensive framework for large-scale\nassessment of CT skills, particularly focusing on AT, the ability to design\nalgorithms. To achieve this, we first developed a competence model capturing\nthe situated and developmental nature of CT, guiding the design of activities\ntailored to cognitive abilities, age, and context. This framework clarifies how\nactivity characteristics influence CT development and how to assess these\ncompetencies. Additionally, we developed an activity for large-scale assessment\nof AT skills, offered in two variants: one based on non-digital artefacts\n(unplugged) and manual expert assessment, and the other based on digital\nartefacts (virtual) and automatic assessment. To provide a more comprehensive\nevaluation of students' competencies, we developed an IAS based on BNs with\nnoisy gates, which offers real-time probabilistic assessment for each skill\nrather than a single overall score. The results indicate that the proposed\ninstrument can measure AT competencies across different age groups and\neducational contexts in Switzerland, demonstrating its applicability for\nlarge-scale use. AT competencies exhibit a progressive development, with no\noverall gender differences, though variations are observed at the school level,\nsignificantly influenced by the artefact-based environment and its context,\nunderscoring the importance of creating accessible and adaptable assessment\ntools.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22756v1",
    "published_date": "2025-03-27 13:34:36 UTC",
    "updated_date": "2025-03-27 13:34:36 UTC"
  },
  {
    "arxiv_id": "2503.21495v2",
    "title": "Adaptive Resampling with Bootstrap for Noisy Multi-Objective Optimization Problems",
    "authors": [
      "Timo Budszuhn",
      "Mark Joachim Krallmann",
      "Daniel Horn"
    ],
    "abstract": "The challenge of noisy multi-objective optimization lies in the constant\ntrade-off between exploring new decision points and improving the precision of\nknown points through resampling. This decision should take into account both\nthe variability of the objective functions and the current estimate of a point\nin relation to the Pareto front. Since the amount and distribution of noise are\ngenerally unknown, it is desirable for a decision function to be highly\nadaptive to the properties of the optimization problem. This paper presents a\nresampling decision function that incorporates the stochastic nature of the\noptimization problem by using bootstrapping and the probability of dominance.\nThe distribution-free estimation of the probability of dominance is achieved\nusing bootstrap estimates of the means. To make the procedure applicable even\nwith very few observations, we transfer the distribution observed at other\ndecision points. The efficiency of this resampling approach is demonstrated by\napplying it in the NSGA-II algorithm with a sequential resampling procedure\nunder multiple noise variations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML",
      "90C29",
      "G.1.6"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages. 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.21495v2",
    "published_date": "2025-03-27 13:32:42 UTC",
    "updated_date": "2025-04-24 14:35:26 UTC"
  },
  {
    "arxiv_id": "2503.21474v2",
    "title": "The Procedural Content Generation Benchmark: An Open-source Testbed for Generative Challenges in Games",
    "authors": [
      "Ahmed Khalifa",
      "Roberto Gallotta",
      "Matthew Barthet",
      "Antonios Liapis",
      "Julian Togelius",
      "Georgios N. Yannakakis"
    ],
    "abstract": "This paper introduces the Procedural Content Generation Benchmark for\nevaluating generative algorithms on different game content creation tasks. The\nbenchmark comes with 12 game-related problems with multiple variants on each\nproblem. Problems vary from creating levels of different kinds to creating rule\nsets for simple arcade games. Each problem has its own content representation,\ncontrol parameters, and evaluation metrics for quality, diversity, and\ncontrollability. This benchmark is intended as a first step towards a\nstandardized way of comparing generative algorithms. We use the benchmark to\nscore three baseline algorithms: a random generator, an evolution strategy, and\na genetic algorithm. Results show that some problems are easier to solve than\nothers, as well as the impact the chosen objective has on quality, diversity,\nand controllability of the generated artifacts.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages, 4 figures, 2 tables, published at FDG2025",
    "pdf_url": "http://arxiv.org/pdf/2503.21474v2",
    "published_date": "2025-03-27 13:05:40 UTC",
    "updated_date": "2025-03-28 08:19:26 UTC"
  },
  {
    "arxiv_id": "2503.21465v1",
    "title": "Retinal Fundus Multi-Disease Image Classification using Hybrid CNN-Transformer-Ensemble Architectures",
    "authors": [
      "Deependra Singh",
      "Saksham Agarwal",
      "Subhankar Mishra"
    ],
    "abstract": "Our research is motivated by the urgent global issue of a large population\naffected by retinal diseases, which are evenly distributed but underserved by\nspecialized medical expertise, particularly in non-urban areas. Our primary\nobjective is to bridge this healthcare gap by developing a comprehensive\ndiagnostic system capable of accurately predicting retinal diseases solely from\nfundus images. However, we faced significant challenges due to limited, diverse\ndatasets and imbalanced class distributions. To overcome these issues, we have\ndevised innovative strategies. Our research introduces novel approaches,\nutilizing hybrid models combining deeper Convolutional Neural Networks (CNNs),\nTransformer encoders, and ensemble architectures sequentially and in parallel\nto classify retinal fundus images into 20 disease labels. Our overarching goal\nis to assess these advanced models' potential in practical applications, with a\nstrong focus on enhancing retinal disease diagnosis accuracy across a broader\nspectrum of conditions. Importantly, our efforts have surpassed baseline model\nresults, with the C-Tran ensemble model emerging as the leader, achieving a\nremarkable model score of 0.9166, surpassing the baseline score of 0.9.\nAdditionally, experiments with the IEViT model showcased equally promising\noutcomes with improved computational efficiency. We've also demonstrated the\neffectiveness of dynamic patch extraction and the integration of domain\nknowledge in computer vision tasks. In summary, our research strives to\ncontribute significantly to retinal disease diagnosis, addressing the critical\nneed for accessible healthcare solutions in underserved regions while aiming\nfor comprehensive and accurate disease prediction.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "68T10, 68T45, 92C55",
      "I.2.10; I.5.4; J.3"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages, 3 figures, 7 tables. Conference paper presented at the\n  International Health Informatics Conference (IHIC 2023)",
    "pdf_url": "http://arxiv.org/pdf/2503.21465v1",
    "published_date": "2025-03-27 12:55:07 UTC",
    "updated_date": "2025-03-27 12:55:07 UTC"
  },
  {
    "arxiv_id": "2503.21464v1",
    "title": "Harnessing Chain-of-Thought Metadata for Task Routing and Adversarial Prompt Detection",
    "authors": [
      "Ryan Marinelli",
      "Josef Pichlmeier",
      "Tamas Bisztray"
    ],
    "abstract": "In this work, we propose a metric called Number of Thoughts (NofT) to\ndetermine the difficulty of tasks pre-prompting and support Large Language\nModels (LLMs) in production contexts. By setting thresholds based on the number\nof thoughts, this metric can discern the difficulty of prompts and support more\neffective prompt routing. A 2% decrease in latency is achieved when routing\nprompts from the MathInstruct dataset through quantized, distilled versions of\nDeepseek with 1.7 billion, 7 billion, and 14 billion parameters. Moreover, this\nmetric can be used to detect adversarial prompts used in prompt injection\nattacks with high efficacy. The Number of Thoughts can inform a classifier that\nachieves 95% accuracy in adversarial prompt detection. Our experiments ad\ndatasets used are available on our GitHub page:\nhttps://github.com/rymarinelli/Number_Of_Thoughts/tree/main.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.PF"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21464v1",
    "published_date": "2025-03-27 12:54:00 UTC",
    "updated_date": "2025-03-27 12:54:00 UTC"
  },
  {
    "arxiv_id": "2503.21463v1",
    "title": "Unveiling Latent Information in Transaction Hashes: Hypergraph Learning for Ethereum Ponzi Scheme Detection",
    "authors": [
      "Junhao Wu",
      "Yixin Yang",
      "Chengxiang Jin",
      "Silu Mu",
      "Xiaolei Qian",
      "Jiajun Zhou",
      "Shanqing Yu",
      "Qi Xuan"
    ],
    "abstract": "With the widespread adoption of Ethereum, financial frauds such as Ponzi\nschemes have become increasingly rampant in the blockchain ecosystem, posing\nsignificant threats to the security of account assets. Existing Ethereum fraud\ndetection methods typically model account transactions as graphs, but this\napproach primarily focuses on binary transactional relationships between\naccounts, failing to adequately capture the complex multi-party interaction\npatterns inherent in Ethereum. To address this, we propose a hypergraph\nmodeling method for the Ponzi scheme detection method in Ethereum, called\nHyperDet. Specifically, we treat transaction hashes as hyperedges that connect\nall the relevant accounts involved in a transaction. Additionally, we design a\ntwo-step hypergraph sampling strategy to significantly reduce computational\ncomplexity. Furthermore, we introduce a dual-channel detection module,\nincluding the hypergraph detection channel and the hyper-homo graph detection\nchannel, to be compatible with existing detection methods. Experimental results\nshow that, compared to traditional homogeneous graph-based methods, the\nhyper-homo graph detection channel achieves significant performance\nimprovements, demonstrating the superiority of hypergraph in Ponzi scheme\ndetection. This research offers innovations for modeling complex relationships\nin blockchain data.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21463v1",
    "published_date": "2025-03-27 12:52:47 UTC",
    "updated_date": "2025-03-27 12:52:47 UTC"
  },
  {
    "arxiv_id": "2503.21435v1",
    "title": "Graph-to-Vision: Multi-graph Understanding and Reasoning using Vision-Language Models",
    "authors": [
      "Ruizhou Li",
      "Haiyun Jiang"
    ],
    "abstract": "Graph Neural Networks (GNNs), as the dominant paradigm for graph-structured\nlearning, have long faced dual challenges of exponentially escalating\ncomputational complexity and inadequate cross-scenario generalization\ncapability. With the rapid advancement of multimodal learning, Vision-Language\nModels (VLMs) have demonstrated exceptional cross-modal relational reasoning\ncapabilities and generalization capacities, thereby opening up novel pathways\nfor overcoming the inherent limitations of conventional graph learning\nparadigms. However, current research predominantly concentrates on\ninvestigating the single-graph reasoning capabilities of VLMs, which\nfundamentally fails to address the critical requirement for coordinated\nreasoning across multiple heterogeneous graph data in real-world application\nscenarios. To address these limitations, we propose the first multi-graph joint\nreasoning benchmark for VLMs. Our benchmark encompasses four graph categories:\nknowledge graphs, flowcharts, mind maps, and route maps,with each graph group\naccompanied by three progressively challenging instruction-response pairs.\nLeveraging this benchmark, we conducted comprehensive capability assessments of\nstate-of-the-art VLMs and performed fine-tuning on open-source models. This\nstudy not only addresses the underexplored evaluation gap in multi-graph\nreasoning for VLMs but also empirically validates their generalization\nsuperiority in graph-structured learning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21435v1",
    "published_date": "2025-03-27 12:20:37 UTC",
    "updated_date": "2025-03-27 12:20:37 UTC"
  },
  {
    "arxiv_id": "2503.21422v1",
    "title": "From Deep Learning to LLMs: A survey of AI in Quantitative Investment",
    "authors": [
      "Bokai Cao",
      "Saizhuo Wang",
      "Xinyi Lin",
      "Xiaojun Wu",
      "Haohan Zhang",
      "Lionel M. Ni",
      "Jian Guo"
    ],
    "abstract": "Quantitative investment (quant) is an emerging, technology-driven approach in\nasset management, increasingy shaped by advancements in artificial\nintelligence. Recent advances in deep learning and large language models (LLMs)\nfor quant finance have improved predictive modeling and enabled agent-based\nautomation, suggesting a potential paradigm shift in this field. In this\nsurvey, taking alpha strategy as a representative example, we explore how AI\ncontributes to the quantitative investment pipeline. We first examine the early\nstage of quant research, centered on human-crafted features and traditional\nstatistical models with an established alpha pipeline. We then discuss the rise\nof deep learning, which enabled scalable modeling across the entire pipeline\nfrom data processing to order execution. Building on this, we highlight the\nemerging role of LLMs in extending AI beyond prediction, empowering autonomous\nagents to process unstructured data, generate alphas, and support\nself-iterative workflows.",
    "categories": [
      "q-fin.CP",
      "cs.AI",
      "cs.LG",
      "q-fin.ST",
      "q-fin.TR"
    ],
    "primary_category": "q-fin.CP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21422v1",
    "published_date": "2025-03-27 12:10:15 UTC",
    "updated_date": "2025-03-27 12:10:15 UTC"
  },
  {
    "arxiv_id": "2503.21419v3",
    "title": "Neuroplasticity in Artificial Intelligence -- An Overview and Inspirations on Drop In & Out Learning",
    "authors": [
      "Yupei Li",
      "Manuel Milling",
      "Björn W. Schuller"
    ],
    "abstract": "Artificial Intelligence (AI) has achieved new levels of performance and\nspread in public usage with the rise of deep neural networks (DNNs). Initially\ninspired by human neurons and their connections, NNs have become the foundation\nof AI models for many advanced architectures. However, some of the most\nintegral processes in the human brain, particularly neurogenesis and\nneuroplasticity in addition to the more spread neuroapoptosis have largely been\nignored in DNN architecture design. Instead, contemporary AI development\npredominantly focuses on constructing advanced frameworks, such as large\nlanguage models, which retain a static structure of neural connections during\ntraining and inference. In this light, we explore how neurogenesis,\nneuroapoptosis, and neuroplasticity can inspire future AI advances.\nSpecifically, we examine analogous activities in artificial NNs, introducing\nthe concepts of ``dropin'' for neurogenesis and revisiting ``dropout'' and\nstructural pruning for neuroapoptosis. We additionally suggest neuroplasticity\ncombining the two for future large NNs in ``life-long learning'' settings\nfollowing the biological inspiration. We conclude by advocating for greater\nresearch efforts in this interdisciplinary domain and identifying promising\ndirections for future exploration.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21419v3",
    "published_date": "2025-03-27 12:09:04 UTC",
    "updated_date": "2025-04-25 07:54:59 UTC"
  },
  {
    "arxiv_id": "2503.21412v1",
    "title": "Federated Intelligence: When Large AI Models Meet Federated Fine-Tuning and Collaborative Reasoning at the Network Edge",
    "authors": [
      "Wanli Ni",
      "Haofeng Sun",
      "Huiqing Ao",
      "Hui Tian"
    ],
    "abstract": "Large artificial intelligence (AI) models exhibit remarkable capabilities in\nvarious application scenarios, but deploying them at the network edge poses\nsignificant challenges due to issues such as data privacy, computational\nresources, and latency. In this paper, we explore federated fine-tuning and\ncollaborative reasoning techniques to facilitate the implementation of large AI\nmodels in resource-constrained wireless networks. Firstly, promising\napplications of large AI models within specific domains are discussed.\nSubsequently, federated fine-tuning methods are proposed to adapt large AI\nmodels to specific tasks or environments at the network edge, effectively\naddressing the challenges associated with communication overhead and enhancing\ncommunication efficiency. These methodologies follow clustered, hierarchical,\nand asynchronous paradigms to effectively tackle privacy issues and eliminate\ndata silos. Furthermore, to enhance operational efficiency and reduce latency,\nefficient frameworks for model collaborative reasoning are developed, which\ninclude decentralized horizontal collaboration, cloud-edge-end vertical\ncollaboration, and multi-access collaboration. Next, simulation results\ndemonstrate the effectiveness of our proposed methods in reducing the\nfine-tuning loss of large AI models across various downstream tasks. Finally,\nseveral open challenges and research opportunities are outlined.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.21412v1",
    "published_date": "2025-03-27 11:56:36 UTC",
    "updated_date": "2025-03-27 11:56:36 UTC"
  },
  {
    "arxiv_id": "2503.21411v1",
    "title": "Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap",
    "authors": [
      "Tong Nie",
      "Jian Sun",
      "Wei Ma"
    ],
    "abstract": "Modern transportation systems face pressing challenges due to increasing\ndemand, dynamic environments, and heterogeneous information integration. The\nrapid evolution of Large Language Models (LLMs) offers transformative potential\nto address these challenges. Extensive knowledge and high-level capabilities\nderived from pretraining evolve the default role of LLMs as text generators to\nbecome versatile, knowledge-driven task solvers for intelligent transportation\nsystems. This survey first presents LLM4TR, a novel conceptual framework that\nsystematically categorizes the roles of LLMs in transportation into four\nsynergetic dimensions: information processors, knowledge encoders, component\ngenerators, and decision facilitators. Through a unified taxonomy, we\nsystematically elucidate how LLMs bridge fragmented data pipelines, enhance\npredictive analytics, simulate human-like reasoning, and enable closed-loop\ninteractions across sensing, learning, modeling, and managing tasks in\ntransportation systems. For each role, our review spans diverse applications,\nfrom traffic prediction and autonomous driving to safety analytics and urban\nmobility optimization, highlighting how emergent capabilities of LLMs such as\nin-context learning and step-by-step reasoning can enhance the operation and\nmanagement of transportation systems. We further curate practical guidance,\nincluding available resources and computational guidelines, to support\nreal-world deployment. By identifying challenges in existing LLM-based\nsolutions, this survey charts a roadmap for advancing LLM-driven transportation\nresearch, positioning LLMs as central actors in the next generation of\ncyber-physical-social mobility ecosystems. Online resources can be found in the\nproject page: https://github.com/tongnie/awesome-llm4tr.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21411v1",
    "published_date": "2025-03-27 11:56:27 UTC",
    "updated_date": "2025-03-27 11:56:27 UTC"
  },
  {
    "arxiv_id": "2503.21406v1",
    "title": "Neuro-Symbolic Imitation Learning: Discovering Symbolic Abstractions for Skill Learning",
    "authors": [
      "Leon Keller",
      "Daniel Tanneberg",
      "Jan Peters"
    ],
    "abstract": "Imitation learning is a popular method for teaching robots new behaviors.\nHowever, most existing methods focus on teaching short, isolated skills rather\nthan long, multi-step tasks. To bridge this gap, imitation learning algorithms\nmust not only learn individual skills but also an abstract understanding of how\nto sequence these skills to perform extended tasks effectively. This paper\naddresses this challenge by proposing a neuro-symbolic imitation learning\nframework. Using task demonstrations, the system first learns a symbolic\nrepresentation that abstracts the low-level state-action space. The learned\nrepresentation decomposes a task into easier subtasks and allows the system to\nleverage symbolic planning to generate abstract plans. Subsequently, the system\nutilizes this task decomposition to learn a set of neural skills capable of\nrefining abstract plans into actionable robot commands. Experimental results in\nthree simulated robotic environments demonstrate that, compared to baselines,\nour neuro-symbolic approach increases data efficiency, improves generalization\ncapabilities, and facilitates interpretability.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "IEEE International Conference on Robotics and Automation (ICRA) 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.21406v1",
    "published_date": "2025-03-27 11:50:29 UTC",
    "updated_date": "2025-03-27 11:50:29 UTC"
  },
  {
    "arxiv_id": "2503.22755v2",
    "title": "Reasoning Under Threat: Symbolic and Neural Techniques for Cybersecurity Verification",
    "authors": [
      "Sarah Veronica"
    ],
    "abstract": "Cybersecurity demands rigorous and scalable techniques to ensure system\ncorrectness, robustness, and resilience against evolving threats. Automated\nreasoning, encompassing formal logic, theorem proving, model checking, and\nsymbolic analysis, provides a foundational framework for verifying security\nproperties across diverse domains such as access control, protocol design,\nvulnerability detection, and adversarial modeling. This survey presents a\ncomprehensive overview of the role of automated reasoning in cybersecurity,\nanalyzing how logical systems, including temporal, deontic, and epistemic\nlogics are employed to formalize and verify security guarantees. We examine\nSOTA tools and frameworks, explore integrations with AI for neural-symbolic\nreasoning, and highlight critical research gaps, particularly in scalability,\ncompositionality, and multi-layered security modeling. The paper concludes with\na set of well-grounded future research directions, aiming to foster the\ndevelopment of secure systems through formal, automated, and explainable\nreasoning techniques.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22755v2",
    "published_date": "2025-03-27 11:41:53 UTC",
    "updated_date": "2025-05-12 23:27:20 UTC"
  },
  {
    "arxiv_id": "2503.21393v2",
    "title": "An evaluation of LLMs and Google Translate for translation of selected Indian languages via sentiment and semantic analyses",
    "authors": [
      "Rohitash Chandra",
      "Aryan Chaudhari",
      "Yeshwanth Rayavarapu"
    ],
    "abstract": "Large Language models (LLMs) have been prominent for language translation,\nincluding low-resource languages. There has been limited study about the\nassessment of the quality of translations generated by LLMs, including Gemini,\nGPT and Google Translate. In this study, we address this limitation by using\nsemantic and sentiment analysis of selected LLMs for Indian languages,\nincluding Sanskrit, Telugu and Hindi. We select prominent texts that have been\nwell translated by experts and use LLMs to generate their translations to\nEnglish, and then we provide a comparison with selected expert (human)\ntranslations. Our findings suggest that while LLMs have made significant\nprogress in translation accuracy, challenges remain in preserving sentiment and\nsemantic integrity, especially in figurative and philosophical contexts. The\nsentiment analysis revealed that GPT-4o and GPT-3.5 are better at preserving\nthe sentiments for the Bhagavad Gita (Sanskrit-English) translations when\ncompared to Google Translate. We observed a similar trend for the case of Tamas\n(Hindi-English) and Maha P (Telugu-English) translations. GPT-4o performs\nsimilarly to GPT-3.5 in the translation in terms of sentiments for the three\nlanguages. We found that LLMs are generally better at translation for capturing\nsentiments when compared to Google Translate.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21393v2",
    "published_date": "2025-03-27 11:35:40 UTC",
    "updated_date": "2025-04-02 03:17:30 UTC"
  },
  {
    "arxiv_id": "2503.21392v2",
    "title": "HybridoNet-Adapt: A Domain-Adapted Framework for Accurate Lithium-Ion Battery RUL Prediction",
    "authors": [
      "Khoa Tran",
      "Bao Huynh",
      "Tri Le",
      "Lam Pham",
      "Vy-Rin Nguyen",
      "Hung-Cuong Trinh",
      "Duong Tran Anh"
    ],
    "abstract": "Accurate prediction of the Remaining Useful Life (RUL) in Lithium ion battery\n(LIB) health management systems is essential for ensuring operational\nreliability and safety. However, many existing methods assume that training and\ntesting data follow the same distribution, limiting their ability to generalize\nto unseen target domains. To address this, we propose a novel RUL prediction\nframework that incorporates a domain adaptation (DA) technique. Our framework\nintegrates a signal preprocessing pipeline including noise reduction, feature\nextraction, and normalization with a robust deep learning model called\nHybridoNet Adapt. The model features a combination of LSTM, Multihead\nAttention, and Neural ODE layers for feature extraction, followed by two\npredictor modules with trainable trade-off parameters. To improve\ngeneralization, we adopt a DA strategy inspired by Domain Adversarial Neural\nNetworks (DANN), replacing adversarial loss with Maximum Mean Discrepancy (MMD)\nto learn domain-invariant features. Experimental results show that HybridoNet\nAdapt significantly outperforms traditional models such as XGBoost and Elastic\nNet, as well as deep learning baselines like Dual input DNN, demonstrating its\npotential for scalable and reliable battery health management (BHM).",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21392v2",
    "published_date": "2025-03-27 11:35:25 UTC",
    "updated_date": "2025-04-18 13:22:18 UTC"
  },
  {
    "arxiv_id": "2503.21356v1",
    "title": "Investigating the Duality of Interpretability and Explainability in Machine Learning",
    "authors": [
      "Moncef Garouani",
      "Josiane Mothe",
      "Ayah Barhrhouj",
      "Julien Aligon"
    ],
    "abstract": "The rapid evolution of machine learning (ML) has led to the widespread\nadoption of complex \"black box\" models, such as deep neural networks and\nensemble methods. These models exhibit exceptional predictive performance,\nmaking them invaluable for critical decision-making across diverse domains\nwithin society. However, their inherently opaque nature raises concerns about\ntransparency and interpretability, making them untrustworthy decision support\nsystems. To alleviate such a barrier to high-stakes adoption, research\ncommunity focus has been on developing methods to explain black box models as a\nmeans to address the challenges they pose. Efforts are focused on explaining\nthese models instead of developing ones that are inherently interpretable.\nDesigning inherently interpretable models from the outset, however, can pave\nthe path towards responsible and beneficial applications in the field of ML. In\nthis position paper, we clarify the chasm between explaining black boxes and\nadopting inherently interpretable models. We emphasize the imperative need for\nmodel interpretability and, following the purpose of attaining better (i.e.,\nmore effective or efficient w.r.t. predictive performance) and trustworthy\npredictors, provide an experimental evaluation of latest hybrid learning\nmethods that integrates symbolic knowledge into neural network predictors. We\ndemonstrate how interpretable hybrid models could potentially supplant black\nbox ones in different domains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21356v1",
    "published_date": "2025-03-27 10:48:40 UTC",
    "updated_date": "2025-03-27 10:48:40 UTC"
  },
  {
    "arxiv_id": "2503.21352v1",
    "title": "Using large language models to produce literature reviews: Usages and systematic biases of microphysics parametrizations in 2699 publications",
    "authors": [
      "Tianhang Zhang",
      "Shengnan Fu",
      "David M. Schultz",
      "Zhonghua Zheng"
    ],
    "abstract": "Large language models afford opportunities for using computers for intensive\ntasks, realizing research opportunities that have not been considered before.\nOne such opportunity could be a systematic interrogation of the scientific\nliterature. Here, we show how a large language model can be used to construct a\nliterature review of 2699 publications associated with microphysics\nparametrizations in the Weather and Research Forecasting (WRF) model, with the\ngoal of learning how they were used and their systematic biases, when\nsimulating precipitation. The database was constructed of publications\nidentified from Web of Science and Scopus searches. The large language model\nGPT-4 Turbo was used to extract information about model configurations and\nperformance from the text of 2699 publications. Our results reveal the\nlandscape of how nine of the most popular microphysics parameterizations have\nbeen used around the world: Lin, Ferrier, WRF Single-Moment, Goddard Cumulus\nEnsemble, Morrison, Thompson, and WRF Double-Moment. More studies used\none-moment parameterizations before 2020 and two-moment parameterizations after\n2020. Seven out of nine parameterizations tended to overestimate precipitation.\nHowever, systematic biases of parameterizations differed in various regions.\nExcept simulations using the Lin, Ferrier, and Goddard parameterizations that\ntended to underestimate precipitation over almost all locations, the remaining\nsix parameterizations tended to overestimate, particularly over China,\nsoutheast Asia, western United States, and central Africa. This method could be\nused by other researchers to help understand how the increasingly massive body\nof scientific literature can be harnessed through the power of artificial\nintelligence to solve their research problems.",
    "categories": [
      "cs.AI",
      "stat.AP"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21352v1",
    "published_date": "2025-03-27 10:42:19 UTC",
    "updated_date": "2025-03-27 10:42:19 UTC"
  },
  {
    "arxiv_id": "2503.22754v1",
    "title": "Model Lake: a New Alternative for Machine Learning Models Management and Governance",
    "authors": [
      "Moncef Garouani",
      "Franck Ravat",
      "Nathalie Valles-Parlangeau"
    ],
    "abstract": "The rise of artificial intelligence and data science across industries\nunderscores the pressing need for effective management and governance of\nmachine learning (ML) models. Traditional approaches to ML models management\noften involve disparate storage systems and lack standardized methodologies for\nversioning, audit, and re-use. Inspired by data lake concepts, this paper\ndevelops the concept of ML Model Lake as a centralized management framework for\ndatasets, codes, and models within organizations environments. We provide an\nin-depth exploration of the Model Lake concept, delineating its architectural\nfoundations, key components, operational benefits, and practical challenges. We\ndiscuss the transformative potential of adopting a Model Lake approach, such as\nenhanced model lifecycle management, discovery, audit, and reusability.\nFurthermore, we illustrate a real-world application of Model Lake and its\ntransformative impact on data, code and model management practices.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22754v1",
    "published_date": "2025-03-27 10:35:51 UTC",
    "updated_date": "2025-03-27 10:35:51 UTC"
  },
  {
    "arxiv_id": "2503.21347v1",
    "title": "Residual Learning Inspired Crossover Operator and Strategy Enhancements for Evolutionary Multitasking",
    "authors": [
      "Ruilin Wang",
      "Xiang Feng",
      "Huiqun Yu",
      "Edmund M-K Lai"
    ],
    "abstract": "In evolutionary multitasking, strategies such as crossover operators and\nskill factor assignment are critical for effective knowledge transfer. Existing\nimprovements to crossover operators primarily focus on low-dimensional variable\ncombinations, such as arithmetic crossover or partially mapped crossover, which\nare insufficient for modeling complex high-dimensional interactions.Moreover,\nstatic or semi-dynamic crossover strategies fail to adapt to the dynamic\ndependencies among tasks. In addition, current Multifactorial Evolutionary\nAlgorithm frameworks often rely on fixed skill factor assignment strategies,\nlacking flexibility. To address these limitations, this paper proposes the\nMultifactorial Evolutionary Algorithm-Residual Learning (MFEA-RL) method based\non residual learning. The method employs a Very Deep Super-Resolution (VDSR)\nmodel to generate high-dimensional residual representations of individuals,\nenhancing the modeling of complex relationships within dimensions. A\nResNet-based mechanism dynamically assigns skill factors to improve task\nadaptability, while a random mapping mechanism efficiently performs crossover\noperations and mitigates the risk of negative transfer. Theoretical analysis\nand experimental results show that MFEA-RL outperforms state-of-the-art\nmultitasking algorithms. It excels in both convergence and adaptability on\nstandard evolutionary multitasking benchmarks, including CEC2017-MTSO and\nWCCI2020-MTSO. Additionally, its effectiveness is validated through a\nreal-world application scenario.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "9 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.21347v1",
    "published_date": "2025-03-27 10:27:17 UTC",
    "updated_date": "2025-03-27 10:27:17 UTC"
  },
  {
    "arxiv_id": "2503.21337v1",
    "title": "A 71.2-$μ$W Speech Recognition Accelerator with Recurrent Spiking Neural Network",
    "authors": [
      "Chih-Chyau Yang",
      "Tian-Sheuan Chang"
    ],
    "abstract": "This paper introduces a 71.2-$\\mu$W speech recognition accelerator designed\nfor edge devices' real-time applications, emphasizing an ultra low power\ndesign. Achieved through algorithm and hardware co-optimizations, we propose a\ncompact recurrent spiking neural network with two recurrent layers, one fully\nconnected layer, and a low time step (1 or 2). The 2.79-MB model undergoes\npruning and 4-bit fixed-point quantization, shrinking it by 96.42\\% to 0.1 MB.\nOn the hardware front, we take advantage of \\textit{mixed-level pruning},\n\\textit{zero-skipping} and \\textit{merged spike} techniques, reducing\ncomplexity by 90.49\\% to 13.86 MMAC/S. The \\textit{parallel time-step\nexecution} addresses inter-time-step data dependencies and enables weight\nbuffer power savings through weight sharing. Capitalizing on the sparse spike\nactivity, an input broadcasting scheme eliminates zero computations, further\nsaving power. Implemented on the TSMC 28-nm process, the design operates in\nreal time at 100 kHz, consuming 71.2 $\\mu$W, surpassing state-of-the-art\ndesigns. At 500 MHz, it has 28.41 TOPS/W and 1903.11 GOPS/mm$^2$ in energy and\narea efficiency, respectively.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21337v1",
    "published_date": "2025-03-27 10:14:00 UTC",
    "updated_date": "2025-03-27 10:14:00 UTC"
  },
  {
    "arxiv_id": "2503.21335v1",
    "title": "A Low-Power Streaming Speech Enhancement Accelerator For Edge Devices",
    "authors": [
      "Ci-Hao Wu",
      "Tian-Sheuan Chang"
    ],
    "abstract": "Transformer-based speech enhancement models yield impressive results.\nHowever, their heterogeneous and complex structure restricts model compression\npotential, resulting in greater complexity and reduced hardware efficiency.\nAdditionally, these models are not tailored for streaming and low-power\napplications. Addressing these challenges, this paper proposes a low-power\nstreaming speech enhancement accelerator through model and hardware\noptimization. The proposed high performance model is optimized for hardware\nexecution with the co-design of model compression and target application, which\nreduces 93.9\\% of model size by the proposed domain-aware and streaming-aware\npruning techniques. The required latency is further reduced with batch\nnormalization-based transformers. Additionally, we employed softmax-free\nattention, complemented by an extra batch normalization, facilitating simpler\nhardware design. The tailored hardware accommodates these diverse computing\npatterns by breaking them down into element-wise multiplication and\naccumulation (MAC). This is achieved through a 1-D processing array, utilizing\nconfigurable SRAM addressing, thereby minimizing hardware complexities and\nsimplifying zero skipping. Using the TSMC 40nm CMOS process, the final\nimplementation requires merely 207.8K gates and 53.75KB SRAM. It consumes only\n8.08 mW for real-time inference at a 62.5MHz frequency.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21335v1",
    "published_date": "2025-03-27 10:13:41 UTC",
    "updated_date": "2025-03-27 10:13:41 UTC"
  },
  {
    "arxiv_id": "2503.21332v1",
    "title": "ReFeed: Multi-dimensional Summarization Refinement with Reflective Reasoning on Feedback",
    "authors": [
      "Taewon Yun",
      "Jihwan Oh",
      "Hyangsuk Min",
      "Yuho Lee",
      "Jihwan Bang",
      "Jason Cai",
      "Hwanjun Song"
    ],
    "abstract": "Summarization refinement faces challenges when extending to multi-dimension.\nIn this paper, we introduce ReFeed, a powerful summarization refinement\npipeline that enhances multiple dimensions through reflective reasoning on\nfeedback. To achieve this, we release SumFeed-CoT, a large-scale Long-CoT-based\ndataset optimized for training a lightweight model with reflective reasoning.\nOur experiments reveal how the number of dimensions, feedback exposure, and\nreasoning policy influence refinement performance, highlighting reflective\nreasoning and simultaneously addressing multiple feedback is crucial to\nmitigate trade-off between dimensions. Furthermore, ReFeed is robust to noisy\nfeedback and feedback order. Lastly, our finding emphasizes that creating data\nwith a proper goal and guideline constitutes a fundamental pillar of effective\nreasoning. The dataset and model will be released.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21332v1",
    "published_date": "2025-03-27 10:11:41 UTC",
    "updated_date": "2025-03-27 10:11:41 UTC"
  },
  {
    "arxiv_id": "2503.21322v2",
    "title": "HyperGraphRAG: Retrieval-Augmented Generation via Hypergraph-Structured Knowledge Representation",
    "authors": [
      "Haoran Luo",
      "Haihong E",
      "Guanting Chen",
      "Yandan Zheng",
      "Xiaobao Wu",
      "Yikai Guo",
      "Qika Lin",
      "Yu Feng",
      "Zemin Kuang",
      "Meina Song",
      "Yifan Zhu",
      "Luu Anh Tuan"
    ],
    "abstract": "Standard Retrieval-Augmented Generation (RAG) relies on chunk-based\nretrieval, whereas GraphRAG advances this approach by graph-based knowledge\nrepresentation. However, existing graph-based RAG approaches are constrained by\nbinary relations, as each edge in an ordinary graph connects only two entities,\nlimiting their ability to represent the n-ary relations (n >= 2) in real-world\nknowledge. In this work, we propose HyperGraphRAG, a novel hypergraph-based RAG\nmethod that represents n-ary relational facts via hyperedges, and consists of\nknowledge hypergraph construction, retrieval, and generation. Experiments\nacross medicine, agriculture, computer science, and law demonstrate that\nHyperGraphRAG outperforms both standard RAG and previous graph-based RAG\nmethods in answer accuracy, retrieval efficiency, and generation quality.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2503.21322v2",
    "published_date": "2025-03-27 10:01:16 UTC",
    "updated_date": "2025-05-22 16:34:30 UTC"
  },
  {
    "arxiv_id": "2503.21309v1",
    "title": "FineCIR: Explicit Parsing of Fine-Grained Modification Semantics for Composed Image Retrieval",
    "authors": [
      "Zixu Li",
      "Zhiheng Fu",
      "Yupeng Hu",
      "Zhiwei Chen",
      "Haokun Wen",
      "Liqiang Nie"
    ],
    "abstract": "Composed Image Retrieval (CIR) facilitates image retrieval through a\nmultimodal query consisting of a reference image and modification text. The\nreference image defines the retrieval context, while the modification text\nspecifies desired alterations. However, existing CIR datasets predominantly\nemploy coarse-grained modification text (CoarseMT), which inadequately captures\nfine-grained retrieval intents. This limitation introduces two key challenges:\n(1) ignoring detailed differences leads to imprecise positive samples, and (2)\ngreater ambiguity arises when retrieving visually similar images. These issues\ndegrade retrieval accuracy, necessitating manual result filtering or repeated\nqueries. To address these limitations, we develop a robust fine-grained CIR\ndata annotation pipeline that minimizes imprecise positive samples and enhances\nCIR systems' ability to discern modification intents accurately. Using this\npipeline, we refine the FashionIQ and CIRR datasets to create two fine-grained\nCIR datasets: Fine-FashionIQ and Fine-CIRR. Furthermore, we introduce FineCIR,\nthe first CIR framework explicitly designed to parse the modification text.\nFineCIR effectively captures fine-grained modification semantics and aligns\nthem with ambiguous visual entities, enhancing retrieval precision. Extensive\nexperiments demonstrate that FineCIR consistently outperforms state-of-the-art\nCIR baselines on both fine-grained and traditional CIR benchmark datasets. Our\nFineCIR code and fine-grained CIR datasets are available at\nhttps://github.com/SDU-L/FineCIR.git.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21309v1",
    "published_date": "2025-03-27 09:34:21 UTC",
    "updated_date": "2025-03-27 09:34:21 UTC"
  },
  {
    "arxiv_id": "2503.21307v1",
    "title": "InternVL-X: Advancing and Accelerating InternVL Series with Efficient Visual Token Compression",
    "authors": [
      "Dongchen Lu",
      "Yuyao Sun",
      "Zilu Zhang",
      "Leping Huang",
      "Jianliang Zeng",
      "Mao Shu",
      "Huo Cao"
    ],
    "abstract": "Most multimodal large language models (MLLMs) treat visual tokens as \"a\nsequence of text\", integrating them with text tokens into a large language\nmodel (LLM). However, a great quantity of visual tokens significantly increases\nthe demand for computational resources and time. In this paper, we propose\nInternVL-X, which outperforms the InternVL model in both performance and\nefficiency by incorporating three visual token compression methods. First, we\npropose a novel vision-language projector, PVTC. This component integrates\nadjacent visual embeddings to form a local query and utilizes the transformed\nCLS token as a global query, then performs point-to-region cross-attention\nthrough these local and global queries to more effectively convert visual\nfeatures. Second, we present a layer-wise visual token compression module,\nLVTC, which compresses tokens in the LLM shallow layers and then expands them\nthrough upsampling and residual connections in the deeper layers. This\nsignificantly enhances the model computational efficiency. Futhermore, we\npropose an efficient high resolution slicing method, RVTC, which dynamically\nadjusts the number of visual tokens based on image area or length filtering.\nRVTC greatly enhances training efficiency with only a slight reduction in\nperformance. By utilizing 20% or fewer visual tokens, InternVL-X achieves\nstate-of-the-art performance on 7 public MLLM benchmarks, and improves the\naverage metric by 2.34% across 12 tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21307v1",
    "published_date": "2025-03-27 09:31:35 UTC",
    "updated_date": "2025-03-27 09:31:35 UTC"
  },
  {
    "arxiv_id": "2503.21305v1",
    "title": "DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep Models with Limited Data",
    "authors": [
      "Dorde Popovic",
      "Amin Sadeghi",
      "Ting Yu",
      "Sanjay Chawla",
      "Issa Khalil"
    ],
    "abstract": "Backdoor attacks are among the most effective, practical, and stealthy\nattacks in deep learning. In this paper, we consider a practical scenario where\na developer obtains a deep model from a third party and uses it as part of a\nsafety-critical system. The developer wants to inspect the model for potential\nbackdoors prior to system deployment. We find that most existing detection\ntechniques make assumptions that are not applicable to this scenario. In this\npaper, we present a novel framework for detecting backdoors under realistic\nrestrictions. We generate candidate triggers by deductively searching over the\nspace of possible triggers. We construct and optimize a smoothed version of\nAttack Success Rate as our search objective. Starting from a broad class of\ntemplate attacks and just using the forward pass of a deep model, we reverse\nengineer the backdoor attack. We conduct extensive evaluation on a wide range\nof attacks, models, and datasets, with our technique performing almost\nperfectly across these settings.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21305v1",
    "published_date": "2025-03-27 09:31:10 UTC",
    "updated_date": "2025-03-27 09:31:10 UTC"
  },
  {
    "arxiv_id": "2503.21284v2",
    "title": "Multi-Scale Invertible Neural Network for Wide-Range Variable-Rate Learned Image Compression",
    "authors": [
      "Hanyue Tu",
      "Siqi Wu",
      "Li Li",
      "Wengang Zhou",
      "Houqiang Li"
    ],
    "abstract": "Autoencoder-based structures have dominated recent learned image compression\nmethods. However, the inherent information loss associated with autoencoders\nlimits their rate-distortion performance at high bit rates and restricts their\nflexibility of rate adaptation. In this paper, we present a variable-rate image\ncompression model based on invertible transform to overcome these limitations.\nSpecifically, we design a lightweight multi-scale invertible neural network,\nwhich bijectively maps the input image into multi-scale latent representations.\nTo improve the compression efficiency, a multi-scale spatial-channel context\nmodel with extended gain units is devised to estimate the entropy of the latent\nrepresentation from high to low levels. Experimental results demonstrate that\nthe proposed method achieves state-of-the-art performance compared to existing\nvariable-rate methods, and remains competitive with recent multi-model\napproaches. Notably, our method is the first learned image compression solution\nthat outperforms VVC across a very wide range of bit rates using a single\nmodel, especially at high bit rates. The source code is available at\nhttps://github.com/hytu99/MSINN-VRLIC.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for publication in IEEE Transactions on Multimedia 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.21284v2",
    "published_date": "2025-03-27 09:08:39 UTC",
    "updated_date": "2025-03-28 02:08:12 UTC"
  },
  {
    "arxiv_id": "2503.22752v1",
    "title": "From Individual to Group: Developing a Context-Aware Multi-Criteria Group Recommender System",
    "authors": [
      "Ngoc Luyen Le",
      "Marie-Hélène Abel"
    ],
    "abstract": "Group decision-making is becoming increasingly common in areas such as\neducation, dining, travel, and finance, where collaborative choices must\nbalance diverse individual preferences. While conventional recommender systems\nare effective in personalization, they fall short in group settings due to\ntheir inability to manage conflicting preferences, contextual factors, and\nmultiple evaluation criteria. This study presents the development of a\nContext-Aware Multi-Criteria Group Recommender System (CA-MCGRS) designed to\naddress these challenges by integrating contextual factors and multiple\ncriteria to enhance recommendation accuracy. By leveraging a Multi-Head\nAttention mechanism, our model dynamically weighs the importance of different\nfeatures. Experiments conducted on an educational dataset with varied ratings\nand contextual variables demonstrate that CA-MCGRS consistently outperforms\nother approaches across four scenarios. Our findings underscore the importance\nof incorporating context and multi-criteria evaluations to improve group\nrecommendations, offering valuable insights for developing more effective group\nrecommender systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "The 16th International Conference on Management of Digital\n  EcoSystems, Nov 2024, Naples, Italy",
    "pdf_url": "http://arxiv.org/pdf/2503.22752v1",
    "published_date": "2025-03-27 09:01:45 UTC",
    "updated_date": "2025-03-27 09:01:45 UTC"
  },
  {
    "arxiv_id": "2503.21272v1",
    "title": "Reinforced Model Merging",
    "authors": [
      "Jiaqi Han",
      "Jingwen Ye",
      "Shunyu Liu",
      "Haofei Zhang",
      "Jie Song",
      "Zunlei Feng",
      "Mingli Song"
    ],
    "abstract": "The success of large language models has garnered widespread attention for\nmodel merging techniques, especially training-free methods which combine model\ncapabilities within the parameter space. However, two challenges remain: (1)\nuniform treatment of all parameters leads to performance degradation; (2)\nsearch-based algorithms are often inefficient. In this paper, we present an\ninnovative framework termed Reinforced Model Merging (RMM), which encompasses\nan environment and agent tailored for merging tasks. These components interact\nto execute layer-wise merging actions, aiming to search the optimal merging\narchitecture. Notably, RMM operates without any gradient computations on the\noriginal models, rendering it feasible for edge devices. Furthermore, by\nutilizing data subsets during the evaluation process, we addressed the\nbottleneck in the reward feedback phase, thereby accelerating RMM by up to 100\ntimes. Extensive experiments demonstrate that RMM achieves state-of-the-art\nperformance across various vision and NLP datasets and effectively overcomes\nthe limitations of the existing baseline methods. Our code is available at\nhttps://github.com/WuDiHJQ/Reinforced-Model-Merging.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21272v1",
    "published_date": "2025-03-27 08:52:41 UTC",
    "updated_date": "2025-03-27 08:52:41 UTC"
  },
  {
    "arxiv_id": "2503.21258v1",
    "title": "Learn by Reasoning: Analogical Weight Generation for Few-Shot Class-Incremental Learning",
    "authors": [
      "Jizhou Han",
      "Chenhao Ding",
      "Yuhang He",
      "Songlin Dong",
      "Qiang Wang",
      "Xinyuan Gao",
      "Yihong Gong"
    ],
    "abstract": "Few-shot class-incremental Learning (FSCIL) enables models to learn new\nclasses from limited data while retaining performance on previously learned\nclasses. Traditional FSCIL methods often require fine-tuning parameters with\nlimited new class data and suffer from a separation between learning new\nclasses and utilizing old knowledge. Inspired by the analogical learning\nmechanisms of the human brain, we propose a novel analogical generative method.\nOur approach includes the Brain-Inspired Analogical Generator (BiAG), which\nderives new class weights from existing classes without parameter fine-tuning\nduring incremental stages. BiAG consists of three components: Weight\nSelf-Attention Module (WSA), Weight & Prototype Analogical Attention Module\n(WPAA), and Semantic Conversion Module (SCM). SCM uses Neural Collapse theory\nfor semantic conversion, WSA supplements new class weights, and WPAA computes\nanalogies to generate new class weights. Experiments on miniImageNet, CUB-200,\nand CIFAR-100 datasets demonstrate that our method achieves higher final and\naverage accuracy compared to SOTA methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21258v1",
    "published_date": "2025-03-27 08:31:46 UTC",
    "updated_date": "2025-03-27 08:31:46 UTC"
  },
  {
    "arxiv_id": "2503.21257v1",
    "title": "OminiAdapt: Learning Cross-Task Invariance for Robust and Environment-Aware Robotic Manipulation",
    "authors": [
      "Yongxu Wang",
      "Weiyun Yi",
      "Xinhao Kong",
      "Wanting Li"
    ],
    "abstract": "With the rapid development of embodied intelligence, leveraging large-scale\nhuman data for high-level imitation learning on humanoid robots has become a\nfocal point of interest in both academia and industry. However, applying\nhumanoid robots to precision operation domains remains challenging due to the\ncomplexities they face in perception and control processes, the long-standing\nphysical differences in morphology and actuation mechanisms between humanoid\nrobots and humans, and the lack of task-relevant features obtained from\negocentric vision. To address the issue of covariate shift in imitation\nlearning, this paper proposes an imitation learning algorithm tailored for\nhumanoid robots. By focusing on the primary task objectives, filtering out\nbackground information, and incorporating channel feature fusion with spatial\nattention mechanisms, the proposed algorithm suppresses environmental\ndisturbances and utilizes a dynamic weight update strategy to significantly\nimprove the success rate of humanoid robots in accomplishing target tasks.\nExperimental results demonstrate that the proposed method exhibits robustness\nand scalability across various typical task scenarios, providing new ideas and\napproaches for autonomous learning and control in humanoid robots. The project\nwill be open-sourced on GitHub.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21257v1",
    "published_date": "2025-03-27 08:28:22 UTC",
    "updated_date": "2025-03-27 08:28:22 UTC"
  },
  {
    "arxiv_id": "2503.21254v1",
    "title": "Vision-to-Music Generation: A Survey",
    "authors": [
      "Zhaokai Wang",
      "Chenxi Bao",
      "Le Zhuo",
      "Jingrui Han",
      "Yang Yue",
      "Yihong Tang",
      "Victor Shea-Jay Huang",
      "Yue Liao"
    ],
    "abstract": "Vision-to-music Generation, including video-to-music and image-to-music\ntasks, is a significant branch of multimodal artificial intelligence\ndemonstrating vast application prospects in fields such as film scoring, short\nvideo creation, and dance music synthesis. However, compared to the rapid\ndevelopment of modalities like text and images, research in vision-to-music is\nstill in its preliminary stage due to its complex internal structure and the\ndifficulty of modeling dynamic relationships with video. Existing surveys focus\non general music generation without comprehensive discussion on\nvision-to-music. In this paper, we systematically review the research progress\nin the field of vision-to-music generation. We first analyze the technical\ncharacteristics and core challenges for three input types: general videos,\nhuman movement videos, and images, as well as two output types of symbolic\nmusic and audio music. We then summarize the existing methodologies on\nvision-to-music generation from the architecture perspective. A detailed review\nof common datasets and evaluation metrics is provided. Finally, we discuss\ncurrent challenges and promising directions for future research. We hope our\nsurvey can inspire further innovation in vision-to-music generation and the\nbroader field of multimodal generation in academic research and industrial\napplications. To follow latest works and foster further innovation in this\nfield, we are continuously maintaining a GitHub repository at\nhttps://github.com/wzk1015/Awesome-Vision-to-Music-Generation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21254v1",
    "published_date": "2025-03-27 08:21:54 UTC",
    "updated_date": "2025-03-27 08:21:54 UTC"
  },
  {
    "arxiv_id": "2503.21251v1",
    "title": "Dual-Splitting Conformal Prediction for Multi-Step Time Series Forecasting",
    "authors": [
      "Qingdi Yu",
      "Zhiwei Cao",
      "Ruihang Wang",
      "Zhen Yang",
      "Lijun Deng",
      "Min Hu",
      "Yong Luo",
      "Xin Zhou"
    ],
    "abstract": "Time series forecasting is crucial for applications like resource scheduling\nand risk management, where multi-step predictions provide a comprehensive view\nof future trends. Uncertainty Quantification (UQ) is a mainstream approach for\naddressing forecasting uncertainties, with Conformal Prediction (CP) gaining\nattention due to its model-agnostic nature and statistical guarantees. However,\nmost variants of CP are designed for single-step predictions and face\nchallenges in multi-step scenarios, such as reliance on real-time data and\nlimited scalability. This highlights the need for CP methods specifically\ntailored to multi-step forecasting. We propose the Dual-Splitting Conformal\nPrediction (DSCP) method, a novel CP approach designed to capture inherent\ndependencies within time-series data for multi-step forecasting. Experimental\nresults on real-world datasets from four different domains demonstrate that the\nproposed DSCP significantly outperforms existing CP variants in terms of the\nWinkler Score, achieving a performance improvement of up to 23.59% compared to\nstate-of-the-art methods. Furthermore, we deployed the DSCP approach for\nrenewable energy generation and IT load forecasting in power management of a\nreal-world trajectory-based application, achieving an 11.25% reduction in\ncarbon emissions through predictive optimization of data center operations and\ncontrols.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T37",
      "I.2.8"
    ],
    "primary_category": "cs.LG",
    "comment": "28 pages, 13 figures, 3 tables. Submitted to Applied Soft Computing.\n  With Editor This is the first public release of the work",
    "pdf_url": "http://arxiv.org/pdf/2503.21251v1",
    "published_date": "2025-03-27 08:17:18 UTC",
    "updated_date": "2025-03-27 08:17:18 UTC"
  },
  {
    "arxiv_id": "2503.21248v1",
    "title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
    "authors": [
      "Yujie Liu",
      "Zonglin Yang",
      "Tong Xie",
      "Jinjie Ni",
      "Ben Gao",
      "Yuqiang Li",
      "Shixiang Tang",
      "Wanli Ouyang",
      "Erik Cambria",
      "Dongzhan Zhou"
    ],
    "abstract": "Large language models (LLMs) have demonstrated potential in assisting\nscientific research, yet their ability to discover high-quality research\nhypotheses remains unexamined due to the lack of a dedicated benchmark. To\naddress this gap, we introduce the first large-scale benchmark for evaluating\nLLMs with a near-sufficient set of sub-tasks of scientific discovery:\ninspiration retrieval, hypothesis composition, and hypothesis ranking. We\ndevelop an automated framework that extracts critical components - research\nquestions, background surveys, inspirations, and hypotheses - from scientific\npapers across 12 disciplines, with expert validation confirming its accuracy.\nTo prevent data contamination, we focus exclusively on papers published in\n2024, ensuring minimal overlap with LLM pretraining data. Our evaluation\nreveals that LLMs perform well in retrieving inspirations, an\nout-of-distribution task, suggesting their ability to surface novel knowledge\nassociations. This positions LLMs as \"research hypothesis mines\", capable of\nfacilitating automated scientific discovery by generating innovative hypotheses\nat scale with minimal human intervention.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21248v1",
    "published_date": "2025-03-27 08:09:15 UTC",
    "updated_date": "2025-03-27 08:09:15 UTC"
  },
  {
    "arxiv_id": "2503.21244v1",
    "title": "Improving $(α, f)$-Byzantine Resilience in Federated Learning via layerwise aggregation and cosine distance",
    "authors": [
      "Mario García-Márquez",
      "Nuria Rodríguez-Barroso",
      "M. Victoria Luzón",
      "Francisco Herrera"
    ],
    "abstract": "The rapid development of artificial intelligence systems has amplified\nsocietal concerns regarding their usage, necessitating regulatory frameworks\nthat encompass data privacy. Federated Learning (FL) is posed as potential\nsolution to data privacy challenges in distributed machine learning by enabling\ncollaborative model training {without data sharing}. However, FL systems remain\nvulnerable to Byzantine attacks, where malicious nodes contribute corrupted\nmodel updates. While Byzantine Resilient operators have emerged as a widely\nadopted robust aggregation algorithm to mitigate these attacks, its efficacy\ndiminishes significantly in high-dimensional parameter spaces, sometimes\nleading to poor performing models. This paper introduces Layerwise Cosine\nAggregation, a novel aggregation scheme designed to enhance robustness of these\nrules in such high-dimensional settings while preserving computational\nefficiency. A theoretical analysis is presented, demonstrating the superior\nrobustness of the proposed Layerwise Cosine Aggregation compared to original\nrobust aggregation operators. Empirical evaluation across diverse image\nclassification datasets, under varying data distributions and Byzantine attack\nscenarios, consistently demonstrates the improved performance of Layerwise\nCosine Aggregation, achieving up to a 16% increase in model accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to Knowledge-Based Systems",
    "pdf_url": "http://arxiv.org/pdf/2503.21244v1",
    "published_date": "2025-03-27 08:07:39 UTC",
    "updated_date": "2025-03-27 08:07:39 UTC"
  },
  {
    "arxiv_id": "2503.21241v1",
    "title": "Feature-Enhanced Machine Learning for All-Cause Mortality Prediction in Healthcare Data",
    "authors": [
      "HyeYoung Lee",
      "Pavel Tsoi"
    ],
    "abstract": "Accurate patient mortality prediction enables effective risk stratification,\nleading to personalized treatment plans and improved patient outcomes. However,\npredicting mortality in healthcare remains a significant challenge, with\nexisting studies often focusing on specific diseases or limited predictor sets.\nThis study evaluates machine learning models for all-cause in-hospital\nmortality prediction using the MIMIC-III database, employing a comprehensive\nfeature engineering approach. Guided by clinical expertise and literature, we\nextracted key features such as vital signs (e.g., heart rate, blood pressure),\nlaboratory results (e.g., creatinine, glucose), and demographic information.\nThe Random Forest model achieved the highest performance with an AUC of 0.94,\nsignificantly outperforming other machine learning and deep learning\napproaches. This demonstrates Random Forest's robustness in handling\nhigh-dimensional, noisy clinical data and its potential for developing\neffective clinical decision support tools. Our findings highlight the\nimportance of careful feature engineering for accurate mortality prediction. We\nconclude by discussing implications for clinical adoption and propose future\ndirections, including enhancing model robustness and tailoring prediction\nmodels for specific diseases.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21241v1",
    "published_date": "2025-03-27 08:04:42 UTC",
    "updated_date": "2025-03-27 08:04:42 UTC"
  },
  {
    "arxiv_id": "2503.21237v1",
    "title": "Bias-Aware Agent: Enhancing Fairness in AI-Driven Knowledge Retrieval",
    "authors": [
      "Karanbir Singh",
      "William Ngu"
    ],
    "abstract": "Advancements in retrieving accessible information have evolved faster in the\nlast few years compared to the decades since the internet's creation. Search\nengines, like Google, have been the number one way to find relevant data. They\nhave always relied on the user's abilities to find the best information in its\nbillions of links and sources at everybody's fingertips. The advent of large\nlanguage models (LLMs) has completely transformed the field of information\nretrieval. The LLMs excel not only at retrieving relevant knowledge but also at\nsummarizing it effectively, making information more accessible and consumable\nfor users. On top of it, the rise of AI Agents has introduced another aspect to\ninformation retrieval i.e. dynamic information retrieval which enables the\nintegration of real-time data such as weather forecasts, and financial data\nwith the knowledge base to curate context-aware knowledge. However, despite\nthese advancements the agents remain susceptible to issues of bias and\nfairness, challenges deeply rooted within the knowledge base and training of\nLLMs. This study introduces a novel approach to bias-aware knowledge retrieval\nby leveraging agentic framework and the innovative use of bias detectors as\ntools to identify and highlight inherent biases in the retrieved content. By\nempowering users with transparency and awareness, this approach aims to foster\nmore equitable information systems and promote the development of responsible\nAI.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21237v1",
    "published_date": "2025-03-27 07:54:39 UTC",
    "updated_date": "2025-03-27 07:54:39 UTC"
  },
  {
    "arxiv_id": "2503.21232v1",
    "title": "Knowledge Graphs as World Models for Semantic Material-Aware Obstacle Handling in Autonomous Vehicles",
    "authors": [
      "Ayush Bheemaiah",
      "Seungyong Yang"
    ],
    "abstract": "The inability of autonomous vehicles (AVs) to infer the material properties\nof obstacles limits their decision-making capacity. While AVs rely on sensor\nsystems such as cameras, LiDAR, and radar to detect obstacles, this study\nsuggests combining sensors with a knowledge graph (KG)-based world model to\nimprove AVs' comprehension of physical material qualities. Beyond sensor data,\nAVs can infer qualities such as malleability, density, and elasticity using a\nsemantic KG that depicts the relationships between obstacles and their\nattributes. Using the CARLA autonomous driving simulator, we evaluated AV\nperformance with and without KG integration. The findings demonstrate that the\nKG-based method improves obstacle management, which allows AVs to use material\nqualities to make better decisions about when to change lanes or apply\nemergency braking. For example, the KG-integrated AV changed lanes for hard\nimpediments like traffic cones and successfully avoided collisions with\nflexible items such as plastic bags by passing over them. Compared to the\ncontrol system, the KG framework demonstrated improved responsiveness to\nobstacles by resolving conflicting sensor data, causing emergency stops for\n13.3% more cases. In addition, our method exhibits a 6.6% higher success rate\nin lane-changing maneuvers in experimental scenarios, particularly for larger,\nhigh-impact obstacles. While we focus particularly on autonomous driving, our\nwork demonstrates the potential of KG-based world models to improve\ndecision-making in embodied AI systems and scale to other domains, including\nrobotics, healthcare, and environmental simulation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21232v1",
    "published_date": "2025-03-27 07:46:45 UTC",
    "updated_date": "2025-03-27 07:46:45 UTC"
  },
  {
    "arxiv_id": "2503.21839v1",
    "title": "M-DocSum: Do LVLMs Genuinely Comprehend Interleaved Image-Text in Document Summarization?",
    "authors": [
      "Haolong Yan",
      "Kaijun Tan",
      "Yeqing Shen",
      "Xin Huang",
      "Zheng Ge",
      "Xiangyu Zhang",
      "Si Li",
      "Daxin Jiang"
    ],
    "abstract": "We investigate a critical yet under-explored question in Large\nVision-Language Models (LVLMs): Do LVLMs genuinely comprehend interleaved\nimage-text in the document? Existing document understanding benchmarks often\nassess LVLMs using question-answer formats, which are information-sparse and\ndifficult to guarantee the coverage of long-range dependencies. To address this\nissue, we introduce a novel and challenging Multimodal Document Summarization\nBenchmark (M-DocSum-Bench), which comprises 500 high-quality arXiv papers,\nalong with interleaved multimodal summaries aligned with human preferences.\nM-DocSum-Bench is a reference-based generation task and necessitates the\ngeneration of interleaved image-text summaries using provided reference images,\nthereby simultaneously evaluating capabilities in understanding, reasoning,\nlocalization, and summarization within complex multimodal document scenarios.\nTo facilitate this benchmark, we develop an automated framework to construct\nsummaries and propose a fine-grained evaluation method called M-DocEval.\nMoreover, we further develop a robust summarization baseline, i.e.,\nM-DocSum-7B, by progressive two-stage training with diverse instruction and\npreference data. The extensive results on our M-DocSum-Bench reveal that the\nleading LVLMs struggle to maintain coherence and accurately integrate\ninformation within long and interleaved contexts, often exhibiting confusion\nbetween similar images and a lack of robustness. Notably, M-DocSum-7B achieves\nstate-of-the-art performance compared to larger and closed-source models\n(including GPT-4o, Gemini Pro, Claude-3.5-Sonnet and Qwen2.5-VL-72B, etc.),\ndemonstrating the potential of LVLMs for improved interleaved image-text\nunderstanding. The code, data, and models are available at\nhttps://github.com/stepfun-ai/M-DocSum-Bench.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21839v1",
    "published_date": "2025-03-27 07:28:32 UTC",
    "updated_date": "2025-03-27 07:28:32 UTC"
  },
  {
    "arxiv_id": "2503.21219v2",
    "title": "GenFusion: Closing the Loop between Reconstruction and Generation via Videos",
    "authors": [
      "Sibo Wu",
      "Congrong Xu",
      "Binbin Huang",
      "Andreas Geiger",
      "Anpei Chen"
    ],
    "abstract": "Recently, 3D reconstruction and generation have demonstrated impressive novel\nview synthesis results, achieving high fidelity and efficiency. However, a\nnotable conditioning gap can be observed between these two fields, e.g.,\nscalable 3D scene reconstruction often requires densely captured views, whereas\n3D generation typically relies on a single or no input view, which\nsignificantly limits their applications. We found that the source of this\nphenomenon lies in the misalignment between 3D constraints and generative\npriors. To address this problem, we propose a reconstruction-driven video\ndiffusion model that learns to condition video frames on artifact-prone RGB-D\nrenderings. Moreover, we propose a cyclical fusion pipeline that iteratively\nadds restoration frames from the generative model to the training set, enabling\nprogressive expansion and addressing the viewpoint saturation limitations seen\nin previous reconstruction and generation pipelines. Our evaluation, including\nview synthesis from sparse view and masked input, validates the effectiveness\nof our approach. More details at https://genfusion.sibowu.com.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025, project page: https://genfusion.sibowu.com",
    "pdf_url": "http://arxiv.org/pdf/2503.21219v2",
    "published_date": "2025-03-27 07:16:24 UTC",
    "updated_date": "2025-03-29 12:18:02 UTC"
  },
  {
    "arxiv_id": "2503.21838v1",
    "title": "MSPLoRA: A Multi-Scale Pyramid Low-Rank Adaptation for Efficient Model Fine-Tuning",
    "authors": [
      "Jiancheng Zhao",
      "Xingda Yu",
      "Zhen Yang"
    ],
    "abstract": "Parameter-Efficient Fine-Tuning (PEFT) has become an essential approach for\nadapting large-scale pre-trained models while reducing computational costs.\nAmong PEFT methods, LoRA significantly reduces trainable parameters by\ndecomposing weight updates into low-rank matrices. However, traditional LoRA\napplies a fixed rank across all layers, failing to account for the varying\ncomplexity of hierarchical information, which leads to inefficient adaptation\nand redundancy. To address this, we propose MSPLoRA (Multi-Scale Pyramid LoRA),\nwhich introduces Global Shared LoRA, Mid-Level Shared LoRA, and Layer-Specific\nLoRA to capture global patterns, mid-level features, and fine-grained\ninformation, respectively. This hierarchical structure reduces inter-layer\nredundancy while maintaining strong adaptation capability. Experiments on\nvarious NLP tasks demonstrate that MSPLoRA achieves more efficient adaptation\nand better performance while significantly reducing the number of trainable\nparameters. Furthermore, additional analyses based on Singular Value\nDecomposition validate its information decoupling ability, highlighting MSPLoRA\nas a scalable and effective optimization strategy for parameter-efficient\nfine-tuning in large language models. Our code is available at\nhttps://github.com/Oblivioniss/MSPLoRA.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21838v1",
    "published_date": "2025-03-27 07:01:50 UTC",
    "updated_date": "2025-03-27 07:01:50 UTC"
  },
  {
    "arxiv_id": "2503.22751v1",
    "title": "Advancing Spatiotemporal Prediction using Artificial Intelligence: Extending the Framework of Geographically and Temporally Weighted Neural Network (GTWNN) for Differing Geographical and Temporal Contexts",
    "authors": [
      "Nicholas Robert Fisk",
      "Matthew Ng Kok Ming",
      "Zahratu Shabrina"
    ],
    "abstract": "This paper aims at improving predictive crime models by extending the\nmathematical framework of Artificial Neural Networks (ANNs) tailored to general\nspatiotemporal problems and appropriately applying them. Recent advancements in\nthe geospatial-temporal modelling field have focused on the inclusion of\ngeographical weighting in their deep learning models to account for nonspatial\nstationarity, which is often apparent in spatial data. We formulate a novel\nsemi-analytical approach to solving Geographically and Temporally Weighted\nRegression (GTWR), and applying it to London crime data. The results produce\nhigh-accuracy predictive evaluation scores that affirm the validity of the\nassumptions and approximations in the approach. This paper presents\nmathematical advances to the Geographically and Temporally Weighted Neural\nNetwork (GTWNN) framework, which offers a novel contribution to the field.\nInsights from past literature are harmoniously employed with the assumptions\nand approximations to generate three mathematical extensions to GTWNN's\nframework. Combinations of these extensions produce five novel ANNs, applied to\nthe London and Detroit datasets. The results suggest that one of the extensions\nis redundant and is generally surpassed by another extension, which we term the\nhistory-dependent module. The remaining extensions form three novel ANN designs\nthat pose potential GTWNN improvements. We evaluated the efficacy of various\nmodels in both the London and Detroit crime datasets, highlighting the\nimportance of accounting for specific geographic and temporal characteristics\nwhen selecting modelling strategies to improve model suitability. In general,\nthe proposed methods provide the foundations for a more context-aware,\naccurate, and robust ANN approach in spatio-temporal modelling.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22751v1",
    "published_date": "2025-03-27 06:45:59 UTC",
    "updated_date": "2025-03-27 06:45:59 UTC"
  },
  {
    "arxiv_id": "2503.21200v1",
    "title": "Learning Generalizable Skills from Offline Multi-Task Data for Multi-Agent Cooperation",
    "authors": [
      "Sicong Liu",
      "Yang Shu",
      "Chenjuan Guo",
      "Bin Yang"
    ],
    "abstract": "Learning cooperative multi-agent policy from offline multi-task data that can\ngeneralize to unseen tasks with varying numbers of agents and targets is an\nattractive problem in many scenarios. Although aggregating general behavior\npatterns among multiple tasks as skills to improve policy transfer is a\npromising approach, two primary challenges hinder the further advancement of\nskill learning in offline multi-task MARL. Firstly, extracting general\ncooperative behaviors from various action sequences as common skills lacks\nbringing cooperative temporal knowledge into them. Secondly, existing works\nonly involve common skills and can not adaptively choose independent knowledge\nas task-specific skills in each task for fine-grained action execution. To\ntackle these challenges, we propose Hierarchical and Separate Skill Discovery\n(HiSSD), a novel approach for generalizable offline multi-task MARL through\nskill learning. HiSSD leverages a hierarchical framework that jointly learns\ncommon and task-specific skills. The common skills learn cooperative temporal\nknowledge and enable in-sample exploitation for offline multi-task MARL. The\ntask-specific skills represent the priors of each task and achieve a\ntask-guided fine-grained action execution. To verify the advancement of our\nmethod, we conduct experiments on multi-agent MuJoCo and SMAC benchmarks. After\ntraining the policy using HiSSD on offline multi-task data, the empirical\nresults show that HiSSD assigns effective cooperative behaviors and obtains\nsuperior performance in unseen tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21200v1",
    "published_date": "2025-03-27 06:35:59 UTC",
    "updated_date": "2025-03-27 06:35:59 UTC"
  },
  {
    "arxiv_id": "2503.21178v1",
    "title": "Integrating Large Language Models For Monte Carlo Simulation of Chemical Reaction Networks",
    "authors": [
      "Sadikshya Gyawali",
      "Ashwini Mandal",
      "Manish Dahal",
      "Manish Awale",
      "Sanjay Rijal",
      "Shital Adhikari",
      "Vaghawan Ojha"
    ],
    "abstract": "Chemical reaction network is an important method for modeling and exploring\ncomplex biological processes, bio-chemical interactions and the behavior of\ndifferent dynamics in system biology. But, formulating such reaction kinetics\ntakes considerable time. In this paper, we leverage the efficiency of modern\nlarge language models to automate the stochastic monte carlo simulation of\nchemical reaction networks and enable the simulation through the reaction\ndescription provided in the form of natural languages. We also integrate this\nprocess into widely used simulation tool Copasi to further give the edge and\nease to the modelers and researchers. In this work, we show the efficacy and\nlimitations of the modern large language models to parse and create reaction\nkinetics for modelling complex chemical reaction processes.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted on MadeAI 2025 Conference",
    "pdf_url": "http://arxiv.org/pdf/2503.21178v1",
    "published_date": "2025-03-27 06:01:50 UTC",
    "updated_date": "2025-03-27 06:01:50 UTC"
  },
  {
    "arxiv_id": "2503.21164v1",
    "title": "Adversarial Wear and Tear: Exploiting Natural Damage for Generating Physical-World Adversarial Examples",
    "authors": [
      "Samra Irshad",
      "Seungkyu Lee",
      "Nassir Navab",
      "Hong Joo Lee",
      "Seong Tae Kim"
    ],
    "abstract": "The presence of adversarial examples in the physical world poses significant\nchallenges to the deployment of Deep Neural Networks in safety-critical\napplications such as autonomous driving. Most existing methods for crafting\nphysical-world adversarial examples are ad-hoc, relying on temporary\nmodifications like shadows, laser beams, or stickers that are tailored to\nspecific scenarios. In this paper, we introduce a new class of physical-world\nadversarial examples, AdvWT, which draws inspiration from the naturally\noccurring phenomenon of `wear and tear', an inherent property of physical\nobjects. Unlike manually crafted perturbations, `wear and tear' emerges\norganically over time due to environmental degradation, as seen in the gradual\ndeterioration of outdoor signboards. To achieve this, AdvWT follows a two-step\napproach. First, a GAN-based, unsupervised image-to-image translation network\nis employed to model these naturally occurring damages, particularly in the\ncontext of outdoor signboards. The translation network encodes the\ncharacteristics of damaged signs into a latent `damage style code'. In the\nsecond step, we introduce adversarial perturbations into the style code,\nstrategically optimizing its transformation process. This manipulation subtly\nalters the damage style representation, guiding the network to generate\nadversarial images where the appearance of damages remains perceptually\nrealistic, while simultaneously ensuring their effectiveness in misleading\nneural networks. Through comprehensive experiments on two traffic sign\ndatasets, we show that AdvWT effectively misleads DNNs in both digital and\nphysical domains. AdvWT achieves an effective attack success rate, greater\nrobustness, and a more natural appearance compared to existing physical-world\nadversarial examples. Additionally, integrating AdvWT into training enhances a\nmodel's generalizability to real-world damaged signs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.21164v1",
    "published_date": "2025-03-27 05:19:41 UTC",
    "updated_date": "2025-03-27 05:19:41 UTC"
  },
  {
    "arxiv_id": "2503.22749v1",
    "title": "Adaptive Clipping for Privacy-Preserving Few-Shot Learning: Enhancing Generalization with Limited Data",
    "authors": [
      "Kanishka Ranaweera",
      "Dinh C. Nguyen",
      "Pubudu N. Pathirana",
      "David Smith",
      "Ming Ding",
      "Thierry Rakotoarivelo",
      "Aruna Seneviratne"
    ],
    "abstract": "In the era of data-driven machine-learning applications, privacy concerns and\nthe scarcity of labeled data have become paramount challenges. These challenges\nare particularly pronounced in the domain of few-shot learning, where the\nability to learn from limited labeled data is crucial. Privacy-preserving\nfew-shot learning algorithms have emerged as a promising solution to address\nsuch pronounced challenges. However, it is well-known that privacy-preserving\ntechniques often lead to a drop in utility due to the fundamental trade-off\nbetween data privacy and model performance. To enhance the utility of\nprivacy-preserving few-shot learning methods, we introduce a novel approach\ncalled Meta-Clip. This technique is specifically designed for meta-learning\nalgorithms, including Differentially Private (DP) model-agnostic meta-learning,\nDP-Reptile, and DP-MetaSGD algorithms, with the objective of balancing data\nprivacy preservation with learning capacity maximization. By dynamically\nadjusting clipping thresholds during the training process, our Adaptive\nClipping method provides fine-grained control over the disclosure of sensitive\ninformation, mitigating overfitting on small datasets and significantly\nimproving the generalization performance of meta-learning models. Through\ncomprehensive experiments on diverse benchmark datasets, we demonstrate the\neffectiveness of our approach in minimizing utility degradation, showcasing a\nsuperior privacy-utility trade-off compared to existing privacy-preserving\ntechniques. The adoption of Adaptive Clipping represents a substantial step\nforward in the field of privacy-preserving few-shot learning, empowering the\ndevelopment of secure and accurate models for real-world applications,\nespecially in scenarios where there are limited data availability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22749v1",
    "published_date": "2025-03-27 05:14:18 UTC",
    "updated_date": "2025-03-27 05:14:18 UTC"
  },
  {
    "arxiv_id": "2503.21159v1",
    "title": "Multi-Objective Optimization for Privacy-Utility Balance in Differentially Private Federated Learning",
    "authors": [
      "Kanishka Ranaweera",
      "David Smith",
      "Pubudu N. Pathirana",
      "Ming Ding",
      "Thierry Rakotoarivelo",
      "Aruna Seneviratne"
    ],
    "abstract": "Federated learning (FL) enables collaborative model training across\ndistributed clients without sharing raw data, making it a promising approach\nfor privacy-preserving machine learning. However, ensuring differential privacy\n(DP) in FL presents challenges due to the trade-off between model utility and\nprivacy protection. Clipping gradients before aggregation is a common strategy\nto limit privacy loss, but selecting an optimal clipping norm is non-trivial,\nas excessively high values compromise privacy, while overly restrictive\nclipping degrades model performance. In this work, we propose an adaptive\nclipping mechanism that dynamically adjusts the clipping norm using a\nmulti-objective optimization framework. By integrating privacy and utility\nconsiderations into the optimization objective, our approach balances privacy\npreservation with model accuracy. We theoretically analyze the convergence\nproperties of our method and demonstrate its effectiveness through extensive\nexperiments on MNIST, Fashion-MNIST, and CIFAR-10 datasets. Our results show\nthat adaptive clipping consistently outperforms fixed-clipping baselines,\nachieving improved accuracy under the same privacy constraints. This work\nhighlights the potential of dynamic clipping strategies to enhance\nprivacy-utility trade-offs in differentially private federated learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21159v1",
    "published_date": "2025-03-27 04:57:05 UTC",
    "updated_date": "2025-03-27 04:57:05 UTC"
  },
  {
    "arxiv_id": "2503.21154v1",
    "title": "Federated Learning with Differential Privacy: An Utility-Enhanced Approach",
    "authors": [
      "Kanishka Ranaweera",
      "Dinh C. Nguyen",
      "Pubudu N. Pathirana",
      "David Smith",
      "Ming Ding",
      "Thierry Rakotoarivelo",
      "Aruna Seneviratne"
    ],
    "abstract": "Federated learning has emerged as an attractive approach to protect data\nprivacy by eliminating the need for sharing clients' data while reducing\ncommunication costs compared with centralized machine learning algorithms.\nHowever, recent studies have shown that federated learning alone does not\nguarantee privacy, as private data may still be inferred from the uploaded\nparameters to the central server. In order to successfully avoid data leakage,\nadopting differential privacy (DP) in the local optimization process or in the\nlocal update aggregation process has emerged as two feasible ways for achieving\nsample-level or user-level privacy guarantees respectively, in federated\nlearning models. However, compared to their non-private equivalents, these\napproaches suffer from a poor utility. To improve the privacy-utility\ntrade-off, we present a modification to these vanilla differentially private\nalgorithms based on a Haar wavelet transformation step and a novel noise\ninjection scheme that significantly lowers the asymptotic bound of the noise\nvariance. We also present a holistic convergence analysis of our proposed\nalgorithm, showing that our method yields better convergence performance than\nthe vanilla DP algorithms. Numerical experiments on real-world datasets\ndemonstrate that our method outperforms existing approaches in model utility\nwhile maintaining the same privacy guarantees.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21154v1",
    "published_date": "2025-03-27 04:48:29 UTC",
    "updated_date": "2025-03-27 04:48:29 UTC"
  },
  {
    "arxiv_id": "2503.21150v1",
    "title": "The Devil is in Low-Level Features for Cross-Domain Few-Shot Segmentation",
    "authors": [
      "Yuhan Liu",
      "Yixiong Zou",
      "Yuhua Li",
      "Ruixuan Li"
    ],
    "abstract": "Cross-Domain Few-Shot Segmentation (CDFSS) is proposed to transfer the\npixel-level segmentation capabilities learned from large-scale source-domain\ndatasets to downstream target-domain datasets, with only a few annotated images\nper class. In this paper, we focus on a well-observed but unresolved phenomenon\nin CDFSS: for target domains, particularly those distant from the source\ndomain, segmentation performance peaks at the very early epochs, and declines\nsharply as the source-domain training proceeds. We delve into this phenomenon\nfor an interpretation: low-level features are vulnerable to domain shifts,\nleading to sharper loss landscapes during the source-domain training, which is\nthe devil of CDFSS. Based on this phenomenon and interpretation, we further\npropose a method that includes two plug-and-play modules: one to flatten the\nloss landscapes for low-level features during source-domain training as a novel\nsharpness-aware minimization method, and the other to directly supplement\ntarget-domain information to the model during target-domain testing by\nlow-level-based calibration. Extensive experiments on four target datasets\nvalidate our rationale and demonstrate that our method surpasses the\nstate-of-the-art method in CDFSS signifcantly by 3.71% and 5.34% average MIoU\nin 1-shot and 5-shot scenarios, respectively.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.21150v1",
    "published_date": "2025-03-27 04:37:52 UTC",
    "updated_date": "2025-03-27 04:37:52 UTC"
  },
  {
    "arxiv_id": "2503.21138v5",
    "title": "A Computational Theory for Efficient Mini Agent Evaluation with Causal Guarantees",
    "authors": [
      "Hedong Yan"
    ],
    "abstract": "In order to reduce the cost of experimental evaluation for agents, we\nintroduce a computational theory of evaluation for mini agents: build\nevaluation model to accelerate the evaluation procedures. We prove upper bounds\nof generalized error and generalized causal effect error of given evaluation\nmodels for infinite agents. We also prove efficiency, and consistency to\nestimated causal effect from deployed agents to evaluation metric by\nprediction. To learn evaluation models, we propose a meta-learner to handle\nheterogeneous agents space problem. Comparing with existed evaluation\napproaches, our (conditional) evaluation model reduced 24.1\\% to 99.0\\%\nevaluation errors across 12 scenes, including individual medicine, scientific\nsimulation, social experiment, business activity, and quantum trade. The\nevaluation time is reduced 3 to 7 order of magnitude per subject comparing with\nexperiments or simulations.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21138v5",
    "published_date": "2025-03-27 04:00:49 UTC",
    "updated_date": "2025-05-16 06:20:15 UTC"
  },
  {
    "arxiv_id": "2503.21109v1",
    "title": "Optimizing Multi-DNN Inference on Mobile Devices through Heterogeneous Processor Co-Execution",
    "authors": [
      "Yunquan Gao",
      "Zhiguo Zhang",
      "Praveen Kumar Donta",
      "Chinmaya Kumar Dehury",
      "Xiujun Wang",
      "Dusit Niyato",
      "Qiyang Zhang"
    ],
    "abstract": "Deep Neural Networks (DNNs) are increasingly deployed across diverse\nindustries, driving demand for mobile device support. However, existing mobile\ninference frameworks often rely on a single processor per model, limiting\nhardware utilization and causing suboptimal performance and energy efficiency.\nExpanding DNN accessibility on mobile platforms requires adaptive,\nresource-efficient solutions to meet rising computational needs without\ncompromising functionality. Parallel inference of multiple DNNs on\nheterogeneous processors remains challenging. Some works partition DNN\noperations into subgraphs for parallel execution across processors, but these\noften create excessive subgraphs based only on hardware compatibility,\nincreasing scheduling complexity and memory overhead.\n  To address this, we propose an Advanced Multi-DNN Model Scheduling (ADMS)\nstrategy for optimizing multi-DNN inference on mobile heterogeneous processors.\nADMS constructs an optimal subgraph partitioning strategy offline, balancing\nhardware operation support and scheduling granularity, and uses a\nprocessor-state-aware algorithm to dynamically adjust workloads based on\nreal-time conditions. This ensures efficient workload distribution and\nmaximizes processor utilization. Experiments show ADMS reduces multi-DNN\ninference latency by 4.04 times compared to vanilla frameworks.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "68T07, 68W40",
      "I.2.6; C.1.4; D.4.8"
    ],
    "primary_category": "cs.DC",
    "comment": "14 pages, 12 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.21109v1",
    "published_date": "2025-03-27 03:03:09 UTC",
    "updated_date": "2025-03-27 03:03:09 UTC"
  },
  {
    "arxiv_id": "2503.22748v1",
    "title": "Ignite Forecasting with SPARK: An Efficient Generative Framework for Refining LLMs in Temporal Knowledge Graph Forecasting",
    "authors": [
      "Gongzhu Yin",
      "Hongli Zhang",
      "Yi Luo",
      "Yuchen Yang",
      "Kun Lu",
      "Chao Meng"
    ],
    "abstract": "Temporal Knowledge Graph (TKG) forecasting is crucial for predicting future\nevents using historical data. With the surge of Large Language Models (LLMs),\nrecent studies have begun exploring their integration into TKG forecasting and\nachieved some success. However, they still face limitations such as limited\ninput length, inefficient output generation, and resource-intensive refinement,\nwhich undermine their performance and practical applicability. To address these\nlimitations, we introduce SPARK, a Sequence-level Proxy-Adapting framework for\nRefining LLMs in TKG forecasting. Inspired by inference-time algorithms adopted\nin controlling generation, SPARK offers a cost-effective, plug-and-play\nsolution through two key innovations: (1) Beam Sequence-Level Generation, which\nreframes TKG forecasting as a top-K sequence-level generation task, using beam\nsearch for efficiently generating next-entity distribution in a single forward\npass. (2) TKG Adapter for Refinement, which employs traditional TKG models as\ntrainable proxy adapters to leverage global graph information and refine LLM\noutputs, overcoming both the input length and the resource-intensive\nfine-tuning problems. Experiments across diverse datasets validate SPARK's\nforecasting performance, robust generalization capabilities, and high\nefficiency. We release source codes at https://github.com/yin-gz/SPARK.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.4"
    ],
    "primary_category": "cs.LG",
    "comment": "To be published in the 30th International Conference on Database\n  Systems for Advanced Applications (DASFAA 2025)",
    "pdf_url": "http://arxiv.org/pdf/2503.22748v1",
    "published_date": "2025-03-27 03:02:02 UTC",
    "updated_date": "2025-03-27 03:02:02 UTC"
  },
  {
    "arxiv_id": "2503.22747v1",
    "title": "LeForecast: Enterprise Hybrid Forecast by Time Series Intelligence",
    "authors": [
      "Zheng Tan",
      "Yiwen Nie",
      "Wenfa Wu",
      "Guanyu Zhang",
      "Yanze Liu",
      "Xinyuan Tian",
      "Kailin Gao",
      "Mengya Liu",
      "Qijiang Cheng",
      "Haipeng Jiang",
      "Yingzheng Ma",
      "Wei Zheng",
      "Yuci Zhu",
      "Yuanyuan Sun",
      "Xiangyu Lei",
      "Xiyu Guan",
      "Wanqing Huang",
      "Shouming Liu",
      "Xiangquan Meng",
      "Pengzhan Qu",
      "Chao Yang",
      "Jiaxuan Fan",
      "Yuan He",
      "Hongsheng Qi",
      "Yangzhou Du"
    ],
    "abstract": "Demand is spiking in industrial fields for multidisciplinary forecasting,\nwhere a broad spectrum of sectors needs planning and forecasts to streamline\nintelligent business management, such as demand forecasting, product planning,\ninventory optimization, etc. Specifically, these tasks expecting intelligent\napproaches to learn from sequentially collected historical data and then\nforesee most possible trend, i.e. time series forecasting. Challenge of it lies\nin interpreting complex business contexts and the efficiency and generalisation\nof modelling. With aspirations of pre-trained foundational models for such\npurpose, given their remarkable success of large foundation model across\nlegions of tasks, we disseminate \\leforecast{}, an enterprise intelligence\nplatform tailored for time series tasks. It integrates advanced interpretations\nof time series data and multi-source information, and a three-pillar modelling\nengine combining a large foundation model (Le-TSFM), multimodal model and\nhybrid model to derive insights, predict or infer futures, and then drive\noptimisation across multiple sectors in enterprise operations. The framework is\ncomposed by a model pool, model profiling module, and two different fusion\napproaches regarding original model architectures. Experimental results verify\nthe efficiency of our trail fusion concepts: router-based fusion network and\ncoordination of large and small models, resulting in high costs for redundant\ndevelopment and maintenance of models. This work reviews deployment of\nLeForecast and its performance in three industrial use cases. Our comprehensive\nexperiments indicate that LeForecast is a profound and practical platform for\nefficient and competitive performance. And we do hope that this work can\nenlighten the research and grounding of time series techniques in accelerating\nenterprise.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22747v1",
    "published_date": "2025-03-27 02:58:06 UTC",
    "updated_date": "2025-03-27 02:58:06 UTC"
  },
  {
    "arxiv_id": "2503.21098v3",
    "title": "Alleviating LLM-based Generative Retrieval Hallucination in Alipay Search",
    "authors": [
      "Yedan Shen",
      "Kaixin Wu",
      "Yuechen Ding",
      "Jingyuan Wen",
      "Hong Liu",
      "Mingjie Zhong",
      "Zhouhan Lin",
      "Jia Xu",
      "Linjian Mo"
    ],
    "abstract": "Generative retrieval (GR) has revolutionized document retrieval with the\nadvent of large language models (LLMs), and LLM-based GR is gradually being\nadopted by the industry. Despite its remarkable advantages and potential,\nLLM-based GR suffers from hallucination and generates documents that are\nirrelevant to the query in some instances, severely challenging its credibility\nin practical applications. We thereby propose an optimized GR framework\ndesigned to alleviate retrieval hallucination, which integrates knowledge\ndistillation reasoning in model training and incorporate decision agent to\nfurther improve retrieval precision. Specifically, we employ LLMs to assess and\nreason GR retrieved query-document (q-d) pairs, and then distill the reasoning\ndata as transferred knowledge to the GR model. Moreover, we utilize a decision\nagent as post-processing to extend the GR retrieved documents through retrieval\nmodel and select the most relevant ones from multi perspectives as the final\ngenerative retrieval result. Extensive offline experiments on real-world\ndatasets and online A/B tests on Fund Search and Insurance Search in Alipay\ndemonstrate our framework's superiority and effectiveness in improving search\nquality and conversion gains.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted by SIGIR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.21098v3",
    "published_date": "2025-03-27 02:36:48 UTC",
    "updated_date": "2025-05-13 11:54:26 UTC"
  },
  {
    "arxiv_id": "2503.21095v1",
    "title": "Confidence Adjusted Surprise Measure for Active Resourceful Trials (CA-SMART): A Data-driven Active Learning Framework for Accelerating Material Discovery under Resource Constraints",
    "authors": [
      "Ahmed Shoyeb Raihan",
      "Zhichao Liu",
      "Tanveer Hossain Bhuiyan",
      "Imtiaz Ahmed"
    ],
    "abstract": "Accelerating the discovery and manufacturing of advanced materials with\nspecific properties is a critical yet formidable challenge due to vast search\nspace, high costs of experiments, and time-intensive nature of material\ncharacterization. In recent years, active learning, where a surrogate machine\nlearning (ML) model mimics the scientific discovery process of a human\nscientist, has emerged as a promising approach to address these challenges by\nguiding experimentation toward high-value outcomes with a limited budget. Among\nthe diverse active learning philosophies, the concept of surprise (capturing\nthe divergence between expected and observed outcomes) has demonstrated\nsignificant potential to drive experimental trials and refine predictive\nmodels. Scientific discovery often stems from surprise thereby making it a\nnatural driver to guide the search process. Despite its promise, prior studies\nleveraging surprise metrics such as Shannon and Bayesian surprise lack\nmechanisms to account for prior confidence, leading to excessive exploration of\nuncertain regions that may not yield useful information. To address this, we\npropose the Confidence-Adjusted Surprise Measure for Active Resourceful Trials\n(CA-SMART), a novel Bayesian active learning framework tailored for optimizing\ndata-driven experimentation. On a high level, CA-SMART incorporates\nConfidence-Adjusted Surprise (CAS) to dynamically balance exploration and\nexploitation by amplifying surprises in regions where the model is more certain\nwhile discounting them in highly uncertain areas. We evaluated CA-SMART on two\nbenchmark functions (Six-Hump Camelback and Griewank) and in predicting the\nfatigue strength of steel. The results demonstrate superior accuracy and\nefficiency compared to traditional surprise metrics, standard Bayesian\nOptimization (BO) acquisition functions and conventional ML methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.AP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21095v1",
    "published_date": "2025-03-27 02:21:42 UTC",
    "updated_date": "2025-03-27 02:21:42 UTC"
  },
  {
    "arxiv_id": "2503.21088v2",
    "title": "ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging",
    "authors": [
      "Haoming Xu",
      "Shuxun Wang",
      "Yanqiu Zhao",
      "Yi Zhong",
      "Ziyan Jiang",
      "Ningyuan Zhao",
      "Shumin Deng",
      "Huajun Chen",
      "Ningyu Zhang"
    ],
    "abstract": "This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4:\nUnlearning Sensitive Content from Large Language Models. This task aims to\nselectively erase sensitive knowledge from large language models, avoiding both\nover-forgetting and under-forgetting issues. We propose an unlearning system\nthat leverages Model Merging (specifically TIES-Merging), combining two\nspecialized models into a more balanced unlearned model. Our system achieves\ncompetitive results, ranking second among 26 teams, with an online score of\n0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we\nalso conduct local experiments and perform a comprehensive analysis of the\nunlearning process, examining performance trajectories, loss dynamics, and\nweight perspectives, along with several supplementary experiments, to\nunderstand the effectiveness of our method. Furthermore, we analyze the\nshortcomings of our method and evaluation metrics, emphasizing that MIA scores\nand ROUGE-based metrics alone are insufficient to fully evaluate successful\nunlearning. Finally, we emphasize the need for more comprehensive evaluation\nmethodologies and rethinking of unlearning objectives in future research. Code\nis available at https://github.com/zjunlp/unlearn/tree/main/semeval25.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CL",
    "comment": "SemEval@ACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.21088v2",
    "published_date": "2025-03-27 02:03:25 UTC",
    "updated_date": "2025-04-20 00:37:24 UTC"
  },
  {
    "arxiv_id": "2503.21074v3",
    "title": "Rerouting Connection: Hybrid Computer Vision Analysis Reveals Visual Similarity Between Indus and Tibetan-Yi Corridor Writing Systems",
    "authors": [
      "Ooha Lakkadi Reddy"
    ],
    "abstract": "This thesis employs a hybrid CNN-Transformer architecture, alongside a\ndetailed anthropological framework, to investigate potential historical\nconnections between the visual morphology of the Indus Valley script and\npictographic systems of the Tibetan-Yi Corridor. Through an ensemble\nmethodology of three target scripts across 15 independently trained models, we\ndemonstrate that Tibetan-Yi Corridor scripts exhibit approximately six-fold\nhigher visual similarity to the Indus script (0.635) than to the Bronze Age\nProto-Cuneiform (0.102) or Proto-Elamite (0.078).\n  Contrary to expectations, when measured through direct script-to-script\nembedding comparisons, the Indus script maps closer to Tibetan-Yi Corridor\nscripts with a mean cosine similarity of 0.930 (CI: [0.917, 0.942]) than to\ncontemporaneous West Asian signaries, which recorded mean similarities of 0.887\n(CI: [0.863, 0.911]) and 0.855 (CI: [0.818, 0.891]). Across dimensionality\nreduction and clustering methods, the Indus script consistently clusters\nclosest to Tibetan-Yi Corridor scripts.\n  These computational findings align with observed pictorial parallels in\nnumeral systems, gender markers, and iconographic elements. Archaeological\nevidence of contact networks along the ancient Shu-Shendu road, coinciding with\nthe Indus Civilization's decline, provides a plausible transmission pathway.\nWhile alternate explanations cannot be ruled out, the specificity and\nconsistency of similarities suggest more complex cultural transmission networks\nbetween South and East Asia than previously recognized.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "107 pages (43 main text, 6 references, 58 appendices). 21 figures, 4\n  tables in main text; 106 figures, 8 tables total. Code available at\n  https://github.com/oohalakkadi/ivc2tyc. Undergraduate thesis at Duke Kunshan\n  University. Accepted for presentation at the 52nd International Conference\n  for Computer Applications & Quantitative Methods in Archaeology (CAA 2025),\n  Athens, Greece",
    "pdf_url": "http://arxiv.org/pdf/2503.21074v3",
    "published_date": "2025-03-27 01:19:47 UTC",
    "updated_date": "2025-04-19 09:18:11 UTC"
  },
  {
    "arxiv_id": "2503.21067v1",
    "title": "AskSport: Web Application for Sports Question-Answering",
    "authors": [
      "Enzo B Onofre",
      "Leonardo M P Moraes",
      "Cristina D Aguiar"
    ],
    "abstract": "This paper introduces AskSport, a question-answering web application about\nsports. It allows users to ask questions using natural language and retrieve\nthe three most relevant answers, including related information and documents.\nThe paper describes the characteristics and functionalities of the application,\nincluding use cases demonstrating its ability to return names and numerical\nvalues. AskSport and its implementation are available for public access on\nHuggingFace.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "I.2.1; I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "for accessing the application, see\n  https://huggingface.co/spaces/leomaurodesenv/qasports-website",
    "pdf_url": "http://arxiv.org/pdf/2503.21067v1",
    "published_date": "2025-03-27 00:57:27 UTC",
    "updated_date": "2025-03-27 00:57:27 UTC"
  },
  {
    "arxiv_id": "2503.21834v1",
    "title": "A Multi-Modal Knowledge-Enhanced Framework for Vessel Trajectory Prediction",
    "authors": [
      "Haomin Yu",
      "Tianyi Li",
      "Kristian Torp",
      "Christian S. Jensen"
    ],
    "abstract": "Accurate vessel trajectory prediction facilitates improved navigational\nsafety, routing, and environmental protection. However, existing prediction\nmethods are challenged by the irregular sampling time intervals of the vessel\ntracking data from the global AIS system and the complexity of vessel movement.\nThese aspects render model learning and generalization difficult. To address\nthese challenges and improve vessel trajectory prediction, we propose the\nmulti-modal knowledge-enhanced framework (MAKER) for vessel trajectory\nprediction. To contend better with the irregular sampling time intervals, MAKER\nfeatures a Large language model-guided Knowledge Transfer (LKT) module that\nleverages pre-trained language models to transfer trajectory-specific\ncontextual knowledge effectively. To enhance the ability to learn complex\ntrajectory patterns, MAKER incorporates a Knowledge-based Self-paced Learning\n(KSL) module. This module employs kinematic knowledge to progressively\nintegrate complex patterns during training, allowing for adaptive learning and\nenhanced generalization. Experimental results on two vessel trajectory datasets\nshow that MAKER can improve the prediction accuracy of state-of-the-art methods\nby 12.08%-17.86%.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.21834v1",
    "published_date": "2025-03-27 00:01:35 UTC",
    "updated_date": "2025-03-27 00:01:35 UTC"
  }
]
[
  {
    "arxiv_id": "2506.22668v1",
    "title": "DistShap: Scalable GNN Explanations with Distributed Shapley Values",
    "authors": [
      "Selahattin Akkas",
      "Aditya Devarakonda",
      "Ariful Azad"
    ],
    "abstract": "With the growing adoption of graph neural networks (GNNs), explaining their predictions has become increasingly important. However, attributing predictions to specific edges or features remains computationally expensive. For example, classifying a node with 100 neighbors using a 3-layer GNN may involve identifying important edges from millions of candidates contributing to the prediction. To address this challenge, we propose DistShap, a parallel algorithm that distributes Shapley value-based explanations across multiple GPUs. DistShap operates by sampling subgraphs in a distributed setting, executing GNN inference in parallel across GPUs, and solving a distributed least squares problem to compute edge importance scores. DistShap outperforms most existing GNN explanation methods in accuracy and is the first to scale to GNN models with millions of features by using up to 128 GPUs on the NERSC Perlmutter supercomputer.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages",
    "pdf_url": "https://arxiv.org/pdf/2506.22668v1",
    "published_date": "2025-06-27 22:30:49 UTC",
    "updated_date": "2025-06-27 22:30:49 UTC"
  },
  {
    "arxiv_id": "2507.02942v1",
    "title": "Control Synthesis in Partially Observable Environments for Complex Perception-Related Objectives",
    "authors": [
      "Zetong Xuan",
      "Yu Wang"
    ],
    "abstract": "Perception-related tasks often arise in autonomous systems operating under partial observability. This work studies the problem of synthesizing optimal policies for complex perception-related objectives in environments modeled by partially observable Markov decision processes. To formally specify such objectives, we introduce \\emph{co-safe linear inequality temporal logic} (sc-iLTL), which can define complex tasks that are formed by the logical concatenation of atomic propositions as linear inequalities on the belief space of the POMDPs. Our solution to the control synthesis problem is to transform the \\mbox{sc-iLTL} objectives into reachability objectives by constructing the product of the belief MDP and a deterministic finite automaton built from the sc-iLTL objective. To overcome the scalability challenge due to the product, we introduce a Monte Carlo Tree Search (MCTS) method that converges in probability to the optimal policy. Finally, a drone-probing case study demonstrates the applicability of our method.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "eess.SY",
    "comment": "This paper has been accepted for publication in the IEEE Control Systems Letters (L-CSS). Personal use of this material is permitted. Reuse requires permission from IEEE",
    "pdf_url": "https://arxiv.org/pdf/2507.02942v1",
    "published_date": "2025-06-27 22:02:07 UTC",
    "updated_date": "2025-06-27 22:02:07 UTC"
  },
  {
    "arxiv_id": "2506.22656v1",
    "title": "Knowledge-Guided Multi-Agent Framework for Automated Requirements Development: A Vision",
    "authors": [
      "Jiangping Huang",
      "Dongming Jin",
      "Weisong Sun",
      "Yang Liu",
      "Zhi Jin"
    ],
    "abstract": "This paper envisions a knowledge-guided multi-agent framework named KGMAF for automated requirements development. KGMAF aims to address gaps in current automation systems for SE, which prioritize code development and overlook the complexities of requirements tasks. KGMAF is composed of six specialized agents and an artifact pool to improve efficiency and accuracy. Specifically, KGMAF outlines the functionality, actions, and knowledge of each agent and provides the conceptual design of the artifact pool. Our case study highlights the potential of KGMAF in real-world scenarios. Finally, we outline several research opportunities for implementing and enhancing automated requirements development using multi-agent systems. We believe that KGMAF will play a pivotal role in shaping the future of automated requirements development in the era of LLMs.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.22656v1",
    "published_date": "2025-06-27 21:57:53 UTC",
    "updated_date": "2025-06-27 21:57:53 UTC"
  },
  {
    "arxiv_id": "2506.22653v1",
    "title": "URSA: The Universal Research and Scientific Agent",
    "authors": [
      "Michael Grosskopf",
      "Russell Bent",
      "Rahul Somasundaram",
      "Isaac Michaud",
      "Arthur Lui",
      "Nathan Debardeleben",
      "Earl Lawrence"
    ],
    "abstract": "Large language models (LLMs) have moved far beyond their initial form as simple chatbots, now carrying out complex reasoning, planning, writing, coding, and research tasks. These skills overlap significantly with those that human scientists use day-to-day to solve complex problems that drive the cutting edge of research. Using LLMs in \"agentic\" AI has the potential to revolutionize modern science and remove bottlenecks to progress. In this work, we present URSA, a scientific agent ecosystem for accelerating research tasks. URSA consists of a set of modular agents and tools, including coupling to advanced physics simulation codes, that can be combined to address scientific problems of varied complexity and impact. This work highlights the architecture of URSA, as well as examples that highlight the potential of the system.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "31 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.22653v1",
    "published_date": "2025-06-27 21:56:02 UTC",
    "updated_date": "2025-06-27 21:56:02 UTC"
  },
  {
    "arxiv_id": "2507.01054v1",
    "title": "XxaCT-NN: Structure Agnostic Multimodal Learning for Materials Science",
    "authors": [
      "Jithendaraa Subramanian",
      "Linda Hung",
      "Daniel Schweigert",
      "Santosh Suram",
      "Weike Ye"
    ],
    "abstract": "Recent advances in materials discovery have been driven by structure-based models, particularly those using crystal graphs. While effective for computational datasets, these models are impractical for real-world applications where atomic structures are often unknown or difficult to obtain. We propose a scalable multimodal framework that learns directly from elemental composition and X-ray diffraction (XRD) -- two of the more available modalities in experimental workflows without requiring crystal structure input. Our architecture integrates modality-specific encoders with a cross-attention fusion module and is trained on the 5-million-sample Alexandria dataset. We present masked XRD modeling (MXM), and apply MXM and contrastive alignment as self-supervised pretraining strategies. Pretraining yields faster convergence (up to 4.2x speedup) and improves both accuracy and representation quality. We further demonstrate that multimodal performance scales more favorably with dataset size than unimodal baselines, with gains compounding at larger data regimes. Our results establish a path toward structure-free, experimentally grounded foundation models for materials science.",
    "categories": [
      "cs.LG",
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.01054v1",
    "published_date": "2025-06-27 21:45:56 UTC",
    "updated_date": "2025-06-27 21:45:56 UTC"
  },
  {
    "arxiv_id": "2506.22638v2",
    "title": "Layer Importance for Mathematical Reasoning is Forged in Pre-Training and Invariant after Post-Training",
    "authors": [
      "Aadim Nepal",
      "Safal Shrestha",
      "Anubhav Shrestha",
      "Minwu Kim",
      "Jalal Naghiyev",
      "Ravid Shwartz-Ziv",
      "Keith Ross"
    ],
    "abstract": "Large language models improve at math after instruction tuning, reinforcement learning, or knowledge distillation. We ask whether these gains come from major changes in the transformer layers or from smaller adjustments that keep the original structure. Using layer-wise ablation on base and trained variants, we find that math reasoning depends on a few critical layers, which stay important across all post-training methods. Removing these layers reduces math accuracy by as much as 80%, whereas factual recall tasks only show relatively smaller drops. This suggests that specialized layers for mathematical tasks form during pre-training and remain stable afterward. As measured by Normalized Mutual Information (NMI), we find that near these critical layers, tokens drift from their original syntactic clusters toward representations aligned with tokens less syntactically related but potentially more useful for downstream task.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.22638v2",
    "published_date": "2025-06-27 21:04:55 UTC",
    "updated_date": "2025-11-05 05:15:54 UTC"
  },
  {
    "arxiv_id": "2507.02941v2",
    "title": "GameTileNet: A Semantic Dataset for Low-Resolution Game Art in Procedural Content Generation",
    "authors": [
      "Yi-Chun Chen",
      "Arnav Jhala"
    ],
    "abstract": "GameTileNet is a dataset designed to provide semantic labels for low-resolution digital game art, advancing procedural content generation (PCG) and related AI research as a vision-language alignment task. Large Language Models (LLMs) and image-generative AI models have enabled indie developers to create visual assets, such as sprites, for game interactions. However, generating visuals that align with game narratives remains challenging due to inconsistent AI outputs, requiring manual adjustments by human artists. The diversity of visual representations in automatically generated game content is also limited because of the imbalance in distributions across styles for training data. GameTileNet addresses this by collecting artist-created game tiles from OpenGameArt.org under Creative Commons licenses and providing semantic annotations to support narrative-driven content generation. The dataset introduces a pipeline for object detection in low-resolution tile-based game art (e.g., 32x32 pixels) and annotates semantics, connectivity, and object classifications. GameTileNet is a valuable resource for improving PCG methods, supporting narrative-rich game content, and establishing a baseline for object detection in low-resolution, non-photorealistic images.\n  TL;DR: GameTileNet is a semantic dataset of low-resolution game tiles designed to support narrative-driven procedural content generation through visual-language alignment.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Camera-ready version of a paper accepted for oral presentation at AIIDE 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.02941v2",
    "published_date": "2025-06-27 20:50:32 UTC",
    "updated_date": "2025-12-31 23:39:21 UTC"
  },
  {
    "arxiv_id": "2506.22623v1",
    "title": "Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing Attacks",
    "authors": [
      "Badr Youbi Idrissi",
      "Monica Millunzi",
      "Amelia Sorrenti",
      "Lorenzo Baraldi",
      "Daryna Dementieva"
    ],
    "abstract": "In the present-day scenario, Large Language Models (LLMs) are establishing their presence as powerful instruments permeating various sectors of society. While their utility offers valuable support to individuals, there are multiple concerns over potential misuse. Consequently, some academic endeavors have sought to introduce watermarking techniques, characterized by the inclusion of markers within machine-generated text, to facilitate algorithmic identification. This research project is focused on the development of a novel methodology for the detection of synthetic text, with the overarching goal of ensuring the ethical application of LLMs in AI-driven text generation. The investigation commences with replicating findings from a previous baseline study, thereby underscoring its susceptibility to variations in the underlying generation model. Subsequently, we propose an innovative watermarking approach and subject it to rigorous evaluation, employing paraphrased generated text to asses its robustness. Experimental results highlight the robustness of our proposal compared to the~\\cite{aarson} watermarking method.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.22623v1",
    "published_date": "2025-06-27 20:39:35 UTC",
    "updated_date": "2025-06-27 20:39:35 UTC"
  },
  {
    "arxiv_id": "2506.22609v1",
    "title": "Ludax: A GPU-Accelerated Domain Specific Language for Board Games",
    "authors": [
      "Graham Todd",
      "Alexander G. Padula",
      "Dennis J. N. J. Soemers",
      "Julian Togelius"
    ],
    "abstract": "Games have long been used as benchmarks and testing environments for research in artificial intelligence. A key step in supporting this research was the development of game description languages: frameworks that compile domain-specific code into playable and simulatable game environments, allowing researchers to generalize their algorithms and approaches across multiple games without having to manually implement each one. More recently, progress in reinforcement learning (RL) has been largely driven by advances in hardware acceleration. Libraries like JAX allow practitioners to take full advantage of cutting-edge computing hardware, often speeding up training and testing by orders of magnitude. Here, we present a synthesis of these strands of research: a domain-specific language for board games which automatically compiles into hardware-accelerated code. Our framework, Ludax, combines the generality of game description languages with the speed of modern parallel processing hardware and is designed to fit neatly into existing deep learning pipelines. We envision Ludax as a tool to help accelerate games research generally, from RL to cognitive science, by enabling rapid simulation and providing a flexible representation scheme. We present a detailed breakdown of Ludax's description language and technical notes on the compilation process, along with speed benchmarking and a demonstration of training RL agents. The Ludax framework, along with implementations of existing board games, is open-source and freely available.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.22609v1",
    "published_date": "2025-06-27 20:15:53 UTC",
    "updated_date": "2025-06-27 20:15:53 UTC"
  },
  {
    "arxiv_id": "2506.22604v1",
    "title": "Bootstrapping Human-Like Planning via LLMs",
    "authors": [
      "David Porfirio",
      "Vincent Hsiao",
      "Morgan Fine-Morris",
      "Leslie Smith",
      "Laura M. Hiatt"
    ],
    "abstract": "Robot end users increasingly require accessible means of specifying tasks for robots to perform. Two common end-user programming paradigms include drag-and-drop interfaces and natural language programming. Although natural language interfaces harness an intuitive form of human communication, drag-and-drop interfaces enable users to meticulously and precisely dictate the key actions of the robot's task. In this paper, we investigate the degree to which both approaches can be combined. Specifically, we construct a large language model (LLM)-based pipeline that accepts natural language as input and produces human-like action sequences as output, specified at a level of granularity that a human would produce. We then compare these generated action sequences to another dataset of hand-specified action sequences. Although our results reveal that larger models tend to outperform smaller ones in the production of human-like action sequences, smaller models nonetheless achieve satisfactory performance.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)",
    "pdf_url": "https://arxiv.org/pdf/2506.22604v1",
    "published_date": "2025-06-27 20:00:51 UTC",
    "updated_date": "2025-06-27 20:00:51 UTC"
  },
  {
    "arxiv_id": "2506.22593v1",
    "title": "Pixels-to-Graph: Real-time Integration of Building Information Models and Scene Graphs for Semantic-Geometric Human-Robot Understanding",
    "authors": [
      "Antonello Longo",
      "Chanyoung Chung",
      "Matteo Palieri",
      "Sung-Kyun Kim",
      "Ali Agha",
      "Cataldo Guaragnella",
      "Shehryar Khattak"
    ],
    "abstract": "Autonomous robots are increasingly playing key roles as support platforms for human operators in high-risk, dangerous applications. To accomplish challenging tasks, an efficient human-robot cooperation and understanding is required. While typically robotic planning leverages 3D geometric information, human operators are accustomed to a high-level compact representation of the environment, like top-down 2D maps representing the Building Information Model (BIM). 3D scene graphs have emerged as a powerful tool to bridge the gap between human readable 2D BIM and the robot 3D maps. In this work, we introduce Pixels-to-Graph (Pix2G), a novel lightweight method to generate structured scene graphs from image pixels and LiDAR maps in real-time for the autonomous exploration of unknown environments on resource-constrained robot platforms. To satisfy onboard compute constraints, the framework is designed to perform all operation on CPU only. The method output are a de-noised 2D top-down environment map and a structure-segmented 3D pointcloud which are seamlessly connected using a multi-layer graph abstracting information from object-level up to the building-level. The proposed method is quantitatively and qualitatively evaluated during real-world experiments performed using the NASA JPL NeBula-Spot legged robot to autonomously explore and map cluttered garage and urban office like environments in real-time.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "Paper accepted to 2025 IEEE International Conference on Automation Science and Engineering (CASE)",
    "pdf_url": "https://arxiv.org/pdf/2506.22593v1",
    "published_date": "2025-06-27 19:23:31 UTC",
    "updated_date": "2025-06-27 19:23:31 UTC"
  },
  {
    "arxiv_id": "2507.22902v1",
    "title": "Toward the Autonomous AI Doctor: Quantitative Benchmarking of an Autonomous Agentic AI Versus Board-Certified Clinicians in a Real World Setting",
    "authors": [
      "Hashim Hayat",
      "Maksim Kudrautsau",
      "Evgeniy Makarov",
      "Vlad Melnichenko",
      "Tim Tsykunou",
      "Piotr Varaksin",
      "Matt Pavelle",
      "Adam Z. Oskowitz"
    ],
    "abstract": "Background: Globally we face a projected shortage of 11 million healthcare practitioners by 2030, and administrative burden consumes 50% of clinical time. Artificial intelligence (AI) has the potential to help alleviate these problems. However, no end-to-end autonomous large language model (LLM)-based AI system has been rigorously evaluated in real-world clinical practice. In this study, we evaluated whether a multi-agent LLM-based AI framework can function autonomously as an AI doctor in a virtual urgent care setting. Methods: We retrospectively compared the performance of the multi-agent AI system Doctronic and board-certified clinicians across 500 consecutive urgent-care telehealth encounters. The primary end points: diagnostic concordance, treatment plan consistency, and safety metrics, were assessed by blinded LLM-based adjudication and expert human review. Results: The top diagnosis of Doctronic and clinician matched in 81% of cases, and the treatment plan aligned in 99.2% of cases. No clinical hallucinations occurred (e.g., diagnosis or treatment not supported by clinical findings). In an expert review of discordant cases, AI performance was superior in 36.1%, and human performance was superior in 9.3%; the diagnoses were equivalent in the remaining cases. Conclusions: In this first large-scale validation of an autonomous AI doctor, we demonstrated strong diagnostic and treatment plan concordance with human clinicians, with AI performance matching and in some cases exceeding that of practicing clinicians. These findings indicate that multi-agent AI systems achieve comparable clinical decision-making to human providers and offer a potential solution to healthcare workforce shortages.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.22902v1",
    "published_date": "2025-06-27 19:04:44 UTC",
    "updated_date": "2025-06-27 19:04:44 UTC"
  },
  {
    "arxiv_id": "2506.22580v1",
    "title": "FedCLAM: Client Adaptive Momentum with Foreground Intensity Matching for Federated Medical Image Segmentation",
    "authors": [
      "Vasilis Siomos",
      "Jonathan Passerat-Palmbach",
      "Giacomo Tarroni"
    ],
    "abstract": "Federated learning is a decentralized training approach that keeps data under stakeholder control while achieving superior performance over isolated training. While inter-institutional feature discrepancies pose a challenge in all federated settings, medical imaging is particularly affected due to diverse imaging devices and population variances, which can diminish the global model's effectiveness. Existing aggregation methods generally fail to adapt across varied circumstances. To address this, we propose FedCLAM, which integrates \\textit{client-adaptive momentum} terms derived from each client's loss reduction during local training, as well as a \\textit{personalized dampening factor} to curb overfitting. We further introduce a novel \\textit{intensity alignment} loss that matches predicted and ground-truth foreground distributions to handle heterogeneous image intensity profiles across institutions and devices. Extensive evaluations on two datasets show that FedCLAM surpasses eight cutting-edge methods in medical segmentation tasks, underscoring its efficacy. The code is available at https://github.com/siomvas/FedCLAM.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "10 pages, 2 figures, Accepted at MICCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.22580v1",
    "published_date": "2025-06-27 18:52:41 UTC",
    "updated_date": "2025-06-27 18:52:41 UTC"
  },
  {
    "arxiv_id": "2506.22578v2",
    "title": "The Hidden Link Between RLHF and Contrastive Learning",
    "authors": [
      "Xufei Lv",
      "Kehai Chen",
      "Haoyuan Sun",
      "Xuefeng Bai",
      "Min Zhang",
      "Houde Liu",
      "Kehai Chen"
    ],
    "abstract": "Alignment of large language models (LLMs) with human values has recently garnered significant attention, with prominent examples including the canonical yet costly Reinforcement Learning from Human Feedback (RLHF) and the simple Direct Preference Optimization (DPO). In this work, we demonstrate that both RLHF and DPO can be interpreted from the perspective of mutual information (MI) maximization, uncovering a profound connection to contrastive learning. Within this framework, both RLHF and DPO can be interpreted as methods that performing contrastive learning based on the positive and negative samples derived from base model, leveraging the Donsker-Varadhan (DV) lower bound on MI (equivalently, the MINE estimator). Such paradigm further illuminates why RLHF may not intrinsically incentivize reasoning capacities in LLMs beyond what is already present in the base model. Building on the perspective, we replace the DV/MINE bound with the Jensen-Shannon (JS) MI estimator and propose the Mutual Information Optimization (MIO). Comprehensive theoretical analysis and extensive empirical evaluations demonstrate that MIO mitigates the late-stage decline in chosen-likelihood observed in DPO, achieving competitive or superior performance across various challenging reasoning and mathematical benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.22578v2",
    "published_date": "2025-06-27 18:51:25 UTC",
    "updated_date": "2025-10-13 17:12:00 UTC"
  },
  {
    "arxiv_id": "2506.22567v1",
    "title": "Unifying Biomedical Vision-Language Expertise: Towards a Generalist Foundation Model via Multi-CLIP Knowledge Distillation",
    "authors": [
      "Shansong Wang",
      "Zhecheng Jin",
      "Mingzhe Hu",
      "Mojtaba Safari",
      "Feng Zhao",
      "Chih-Wei Chang",
      "Richard LJ Qiu",
      "Justin Roper",
      "David S. Yu",
      "Xiaofeng Yang"
    ],
    "abstract": "CLIP models pretrained on natural images with billion-scale image-text pairs have demonstrated impressive capabilities in zero-shot classification, cross-modal retrieval, and open-ended visual answering. However, transferring this success to biomedicine is hindered by the scarcity of large-scale biomedical image-text corpora, the heterogeneity of image modalities, and fragmented data standards across institutions. These limitations hinder the development of a unified and generalizable biomedical foundation model trained from scratch. To overcome this, we introduce MMKD-CLIP, a generalist biomedical foundation model developed via Multiple Medical CLIP Knowledge Distillation. Rather than relying on billion-scale raw data, MMKD-CLIP distills knowledge from nine state-of-the-art domain-specific or generalist biomedical CLIP models, each pretrained on millions of biomedical image-text pairs. Our two-stage training pipeline first performs CLIP-style pretraining on over 2.9 million biomedical image-text pairs from 26 image modalities, followed by feature-level distillation using over 19.2 million feature pairs extracted from teacher models. We evaluate MMKD-CLIP on 58 diverse biomedical datasets, encompassing over 10.8 million biomedical images across nine image modalities. The evaluation spans six core task types: zero-shot classification, linear probing, cross-modal retrieval, visual question answering, survival prediction, and cancer diagnosis. MMKD-CLIP consistently outperforms all teacher models while demonstrating remarkable robustness and generalization across image domains and task settings. These results underscore that multi-teacher knowledge distillation is a scalable and effective paradigm for building high-performing biomedical foundation models under the practical constraints of real-world data availability.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.22567v1",
    "published_date": "2025-06-27 18:28:57 UTC",
    "updated_date": "2025-06-27 18:28:57 UTC"
  },
  {
    "arxiv_id": "2506.22566v3",
    "title": "Exploration Behavior of Untrained Policies",
    "authors": [
      "Jacob Adamczyk"
    ],
    "abstract": "Exploration remains a fundamental challenge in reinforcement learning (RL), particularly in environments with sparse or adversarial reward structures. In this work, we study how the architecture of deep neural policies implicitly shapes exploration before training. We theoretically and empirically demonstrate strategies for generating ballistic or diffusive trajectories from untrained policies in a toy model. Using the theory of infinite-width networks and a continuous-time limit, we show that untrained policies return correlated actions and result in non-trivial state-visitation distributions. We discuss the distributions of the corresponding trajectories for a standard architecture, revealing insights into inductive biases for tackling exploration. Our results establish a theoretical and experimental framework for using policy initialization as a design tool to understand exploration behavior in early training.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "High-dimensional Learning Dynamics Workshop at ICML-2025",
    "pdf_url": "https://arxiv.org/pdf/2506.22566v3",
    "published_date": "2025-06-27 18:28:41 UTC",
    "updated_date": "2025-07-24 18:16:23 UTC"
  },
  {
    "arxiv_id": "2506.22554v2",
    "title": "Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset",
    "authors": [
      "Vasu Agrawal",
      "Akinniyi Akinyemi",
      "Kathryn Alvero",
      "Morteza Behrooz",
      "Julia Buffalini",
      "Fabio Maria Carlucci",
      "Joy Chen",
      "Junming Chen",
      "Zhang Chen",
      "Shiyang Cheng",
      "Praveen Chowdary",
      "Joe Chuang",
      "Antony D'Avirro",
      "Jon Daly",
      "Ning Dong",
      "Mark Duppenthaler",
      "Cynthia Gao",
      "Jeff Girard",
      "Martin Gleize",
      "Sahir Gomez",
      "Hongyu Gong",
      "Srivathsan Govindarajan",
      "Brandon Han",
      "Sen He",
      "Denise Hernandez",
      "Yordan Hristov",
      "Rongjie Huang",
      "Hirofumi Inaguma",
      "Somya Jain",
      "Raj Janardhan",
      "Qingyao Jia",
      "Christopher Klaiber",
      "Dejan Kovachev",
      "Moneish Kumar",
      "Hang Li",
      "Yilei Li",
      "Pavel Litvin",
      "Wei Liu",
      "Guangyao Ma",
      "Jing Ma",
      "Martin Ma",
      "Xutai Ma",
      "Lucas Mantovani",
      "Sagar Miglani",
      "Sreyas Mohan",
      "Louis-Philippe Morency",
      "Evonne Ng",
      "Kam-Woh Ng",
      "Tu Anh Nguyen",
      "Amia Oberai",
      "Benjamin Peloquin",
      "Juan Pino",
      "Jovan Popovic",
      "Omid Poursaeed",
      "Fabian Prada",
      "Alice Rakotoarison",
      "Rakesh Ranjan",
      "Alexander Richard",
      "Christophe Ropers",
      "Safiyyah Saleem",
      "Vasu Sharma",
      "Alex Shcherbyna",
      "Jia Shen",
      "Jie Shen",
      "Anastasis Stathopoulos",
      "Anna Sun",
      "Paden Tomasello",
      "Tuan Tran",
      "Arina Turkatenko",
      "Bo Wan",
      "Chao Wang",
      "Jeff Wang",
      "Mary Williamson",
      "Carleigh Wood",
      "Tao Xiang",
      "Yilin Yang",
      "Julien Yao",
      "Chen Zhang",
      "Jiemin Zhang",
      "Xinyue Zhang",
      "Jason Zheng",
      "Pavlo Zhyzheria",
      "Jan Zikes",
      "Michael Zollhoefer"
    ],
    "abstract": "Human communication involves a complex interplay of verbal and nonverbal signals, essential for conveying meaning and achieving interpersonal goals. To develop socially intelligent AI technologies, it is crucial to develop models that can both comprehend and generate dyadic behavioral dynamics. To this end, we introduce the Seamless Interaction Dataset, a large-scale collection of over 4,000 hours of face-to-face interaction footage from over 4,000 participants in diverse contexts. This dataset enables the development of AI technologies that understand dyadic embodied dynamics, unlocking breakthroughs in virtual agents, telepresence experiences, and multimodal content analysis tools. We also develop a suite of models that utilize the dataset to generate dyadic motion gestures and facial expressions aligned with human speech. These models can take as input both the speech and visual behavior of their interlocutors. We present a variant with speech from an LLM model and integrations with 2D and 3D rendering methods, bringing us closer to interactive virtual agents. Additionally, we describe controllable variants of our motion models that can adapt emotional responses and expressivity levels, as well as generating more semantically-relevant gestures. Finally, we discuss methods for assessing the quality of these dyadic motion models, which are demonstrating the potential for more intuitive and responsive human-AI interactions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.22554v2",
    "published_date": "2025-06-27 18:09:49 UTC",
    "updated_date": "2025-07-01 01:02:44 UTC"
  },
  {
    "arxiv_id": "2506.22427v1",
    "title": "CLoVE: Personalized Federated Learning through Clustering of Loss Vector Embeddings",
    "authors": [
      "Randeep Bhatia",
      "Nikos Papadis",
      "Murali Kodialam",
      "TV Lakshman",
      "Sayak Chakrabarty"
    ],
    "abstract": "We propose CLoVE (Clustering of Loss Vector Embeddings), a novel algorithm for Clustered Federated Learning (CFL). In CFL, clients are naturally grouped into clusters based on their data distribution. However, identifying these clusters is challenging, as client assignments are unknown. CLoVE utilizes client embeddings derived from model losses on client data, and leverages the insight that clients in the same cluster share similar loss values, while those in different clusters exhibit distinct loss patterns. Based on these embeddings, CLoVE is able to iteratively identify and separate clients from different clusters and optimize cluster-specific models through federated aggregation. Key advantages of CLoVE over existing CFL algorithms are (1) its simplicity, (2) its applicability to both supervised and unsupervised settings, and (3) the fact that it eliminates the need for near-optimal model initialization, which makes it more robust and better suited for real-world applications. We establish theoretical convergence bounds, showing that CLoVE can recover clusters accurately with high probability in a single round and converges exponentially fast to optimal models in a linear setting. Our comprehensive experiments comparing with a variety of both CFL and generic Personalized Federated Learning (PFL) algorithms on different types of datasets and an extensive array of non-IID settings demonstrate that CLoVE achieves highly accurate cluster recovery in just a few rounds of training, along with state-of-the-art model accuracy, across a variety of both supervised and unsupervised PFL tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "31 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.22427v1",
    "published_date": "2025-06-27 17:52:16 UTC",
    "updated_date": "2025-06-27 17:52:16 UTC"
  },
  {
    "arxiv_id": "2506.22419v2",
    "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements",
    "authors": [
      "Bingchen Zhao",
      "Despoina Magka",
      "Minqi Jiang",
      "Xian Li",
      "Roberta Raileanu",
      "Tatiana Shavrina",
      "Jean-Christophe Gagnon-Audet",
      "Kelvin Niu",
      "Shagun Sodhani",
      "Michael Shvartsman",
      "Andrei Lupu",
      "Alisia Lupidi",
      "Edan Toledo",
      "Karen Hambardzumyan",
      "Martin Josifoski",
      "Thomas Foster",
      "Lucia Cipolina-Kun",
      "Abhishek Charnalia",
      "Derek Dunfield",
      "Alexander H. Miller",
      "Oisin Mac Aodha",
      "Jakob Foerster",
      "Yoram Bachrach"
    ],
    "abstract": "Rapid advancements in large language models (LLMs) have the potential to assist in scientific progress. A critical capability toward this endeavor is the ability to reproduce existing work. To evaluate the ability of AI agents to reproduce results in an active research area, we introduce the Automated LLM Speedrunning Benchmark, leveraging the research community contributions on the NanoGPT speedrun, a competition to train a GPT-2 model in the shortest time. Each of the 19 speedrun tasks provides the agent with the previous records training script, optionally paired with one of three hint formats, ranging from pseudocode to paper-like descriptions of the new records improvements. Records execute quickly by design and speedrun improvements encompass diverse code-level changes, ranging from high-level algorithmic advancements to hardware-aware optimizations. These features make the benchmark both accessible and realistic for the frontier problem of improving LLM training. We find that recent reasoning LLMs combined with SoTA scaffolds struggle to reimplement already-known innovations in our benchmark, even when given detailed hints. Our benchmark thus provides a simple, non-saturated measure of an LLMs ability to automate scientific reproduction, a necessary (but not sufficient) skill for an autonomous research agent.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.22419v2",
    "published_date": "2025-06-27 17:44:32 UTC",
    "updated_date": "2025-06-30 21:56:29 UTC"
  },
  {
    "arxiv_id": "2506.22403v2",
    "title": "HyperCLOVA X THINK Technical Report",
    "authors": [
      "NAVER Cloud HyperCLOVA X Team"
    ],
    "abstract": "We introduce HyperCLOVA X THINK, the first reasoning-focused large language model in the HyperCLOVA X family, pre-trained on roughly $6$ trillion high-quality Korean, and English tokens, augmented with targeted synthetic Korean data. It was implemented as a compute-memory-balanced Peri-LN Transformer scaled with $Î¼$P, pre-trained through a three-stage curriculum that expands the context window to $128$K tokens, and post-trained via supervised fine-tuning with Reinforcement Learning from Verifiable Rewards supports both detailed rationale and concise-answer modes. It delivers competitive performance against similarly sized models on Korea-focused benchmarks such as KMMLU, CSAT, KoBALT-700, HAERAE-1.0, and KoBigBench, while preserving robust bilingual consistency and translation quality. In addition, a vision-augmented variant matches or exceeds GPT-4.1 on the KCSAT STEM benchmark, all of which are achieved with substantially lower training compute than existing models of similar sizes. We also present a pruning and distillation technique that will soon be applied to HyperCLOVA X THINK for an open-source and business-friendly foundation model. Altogether, these capabilities position HyperCLOVA X THINK as a robust foundation for Korean AI innovation and a valuable resource for the global research community.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "50 pages, 13 figures; fixed figures in the appendix",
    "pdf_url": "https://arxiv.org/pdf/2506.22403v2",
    "published_date": "2025-06-27 17:23:12 UTC",
    "updated_date": "2025-07-01 13:39:25 UTC"
  },
  {
    "arxiv_id": "2506.22397v5",
    "title": "HazeMatching: Dehazing Light Microscopy Images with Guided Conditional Flow Matching",
    "authors": [
      "Anirban Ray",
      "Ashesh",
      "Florian Jug"
    ],
    "abstract": "Fluorescence microscopy is a major driver of scientific progress in the life sciences. Although high-end confocal microscopes are capable of filtering out-of-focus light, cheaper and more accessible microscopy modalities, such as widefield microscopy, can not, which consequently leads to hazy image data. Computational dehazing is trying to combine the best of both worlds, leading to cheap microscopy but crisp-looking images. The perception-distortion trade-off tells us that we can optimize either for data fidelity, e.g. low MSE or high PSNR, or for data realism, measured by perceptual metrics such as LPIPS or FID. Existing methods either prioritize fidelity at the expense of realism, or produce perceptually convincing results that lack quantitative accuracy. In this work, we propose HazeMatching, a novel iterative method for dehazing light microscopy images, which effectively balances these objectives. Our goal was to find a balanced trade-off between the fidelity of the dehazing results and the realism of individual predictions (samples). We achieve this by adapting the conditional flow matching framework by guiding the generative process with a hazy observation in the conditional velocity field. We evaluate HazeMatching on 5 datasets, covering both synthetic and real data, assessing both distortion and perceptual quality. Our method is compared against 11 baselines, achieving a consistent balance between fidelity and realism on average. Additionally, with calibration analysis, we show that HazeMatching produces well-calibrated predictions. Note that our method does not need an explicit degradation operator to exist, making it easily applicable on real microscopy data. All data used for training and evaluation and our code will be publicly available under a permissive license.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "4 figures, 8 pages + refs, 45 pages total (including supplement), 28 supplementary figures",
    "pdf_url": "https://arxiv.org/pdf/2506.22397v5",
    "published_date": "2025-06-27 17:10:43 UTC",
    "updated_date": "2025-11-21 15:31:44 UTC"
  },
  {
    "arxiv_id": "2506.22396v1",
    "title": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization",
    "authors": [
      "Danush Khanna",
      "Aditya Kumar Guru",
      "Srivarshinee Sridhar",
      "Zidan Ahmed",
      "Rubhav Bahirwani",
      "Meetu Malhotra",
      "Vinija Jain",
      "Aman Chadha",
      "Amitava Das",
      "Kripabandhu Ghosh"
    ],
    "abstract": "Inference accounts for the majority of latency and energy consumption in large language model (LLM) deployments, often exceeding 90% of total cost. While training-time efficiency has seen extensive progress, runtime optimization remains a key bottleneck, particularly under autoregressive decoding. Existing approaches -- such as pruning, quantization, early exits, and speculative decoding -- often require retraining, architectural changes, or disrupt decoding compatibility. We introduce QuickSilver, a modular, token-level framework that enables semantic adaptivity at inference time without altering model weights or structure. QuickSilver integrates four synergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged representations; (ii) KV Cache Skipping, which selectively suppresses memory writes to reduce attention overhead; and (iii) Contextual Token Fusion, which collapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on frozen, dense models and requires no auxiliary networks. Applied to GPT-2 and Llama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP reduction with negligible perplexity degradation (<=0.2).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint. Under submission",
    "pdf_url": "https://arxiv.org/pdf/2506.22396v1",
    "published_date": "2025-06-27 17:10:32 UTC",
    "updated_date": "2025-06-27 17:10:32 UTC"
  },
  {
    "arxiv_id": "2506.22393v2",
    "title": "Multi-View Contrastive Learning for Robust Domain Adaptation in Medical Time Series Analysis",
    "authors": [
      "YongKyung Oh",
      "Alex Bui"
    ],
    "abstract": "Adapting machine learning models to medical time series across different domains remains a challenge due to complex temporal dependencies and dynamic distribution shifts. Current approaches often focus on isolated feature representations, limiting their ability to fully capture the intricate temporal dynamics necessary for robust domain adaptation. In this work, we propose a novel framework leveraging multi-view contrastive learning to integrate temporal patterns, derivative-based dynamics, and frequency-domain features. Our method employs independent encoders and a hierarchical fusion mechanism to learn feature-invariant representations that are transferable across domains while preserving temporal coherence. Extensive experiments on diverse medical datasets, including electroencephalogram (EEG), electrocardiogram (ECG), and electromyography (EMG) demonstrate that our approach significantly outperforms state-of-the-art methods in transfer learning tasks. By advancing the robustness and generalizability of machine learning models, our framework offers a practical pathway for deploying reliable AI systems in diverse healthcare settings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at the sixth Conference on Health, Inference, and Learning (CHIL 2025), PMLR 287:502-526, 2025. Models & Methods Track - Best Paper Award. https://proceedings.mlr.press/v287/oh25a.html",
    "pdf_url": "https://arxiv.org/pdf/2506.22393v2",
    "published_date": "2025-06-27 17:06:16 UTC",
    "updated_date": "2025-09-19 20:53:26 UTC"
  },
  {
    "arxiv_id": "2506.22389v1",
    "title": "Towards Distributed Neural Architectures",
    "authors": [
      "Aditya Cowsik",
      "Tianyu He",
      "Andrey Gromov"
    ],
    "abstract": "We introduce and train distributed neural architectures (DNA) in vision and language domains. DNAs are initialized with a proto-architecture that consists of (transformer, MLP, attention, etc.) modules and routers. Any token (or patch) can traverse any series of modules in any order. DNAs are a natural generalization of the sparse methods such as Mixture-of-Experts, Mixture-of-Depths, parameter sharing, etc. Computation and communication patterns of DNA modules are learnt end-to-end during training and depend on the content and context of each token (or patch). These patterns can be shaped by further requirements added to the optimization objective such as compute/memory efficiency or load balancing. We empirically show that (i) trained DNAs are competitive with the dense baselines in both domains and (ii) compute efficiency/parameter sharing can be learnt from data. Next, we analyze the emergent connectivity and computation patterns in the trained DNAs. We find that the paths that tokens take through the models are themselves distributed according to a power-law. We show that some paths (or, equivalently, groups of modules) show emergent specialization. Finally, we demonstrate that models learn to allocate compute and active parameters in an interpretable way.",
    "categories": [
      "cs.LG",
      "cond-mat.dis-nn",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "36 pages, 25 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.22389v1",
    "published_date": "2025-06-27 16:57:59 UTC",
    "updated_date": "2025-06-27 16:57:59 UTC"
  },
  {
    "arxiv_id": "2506.22385v2",
    "title": "Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment",
    "authors": [
      "Yue Zhang",
      "Jilei Sun",
      "Yunhui Guo",
      "Vibhav Gogate"
    ],
    "abstract": "Video Large Multimodal Models (VLMMs) have made impressive strides in understanding video content, but they often struggle with abstract and adaptive reasoning-the ability to revise their interpretations when new information emerges. In reality, conclusions are rarely set in stone; additional context can strengthen or weaken an initial inference. To address this, we introduce Defeasible Video Entailment (DVidE), a new task that challenges models to think like doubters, constantly updating their reasoning based on evolving evidence. In DVidE, given a video premise and a textual hypothesis, models must determine whether a new update strengthens or weakens the hypothesis (classification version) or generate a coherent update that modifies the entailment relationship (generation version). For solving the classification task, we propose the Chain of Counterfactual Thought framework, utilizing counterfactual reasoning, ASR-enhanced video content, and rationale refinement to reduce inference bias. For the generation task, we develop a framework that combines ASR output with a Large Language Model (LLM) to produce coherent, contextually relevant updates aligned with the intended strengthener or weakener goals. Additionally, we introduce a novel benchmark dataset, with strengthener/weakener annotations and an LLM-based evaluation metric specifically designed for assessing generative performance. Experimental results demonstrate significant improvements, highlighting our proposed method in enhancing dynamic reasoning capabilities of VLMMs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.22385v2",
    "published_date": "2025-06-27 16:51:15 UTC",
    "updated_date": "2025-10-07 03:47:19 UTC"
  },
  {
    "arxiv_id": "2506.22376v4",
    "title": "OptScale: Probabilistic Optimality for Inference-time Scaling",
    "authors": [
      "Youkang Wang",
      "Jian Wang",
      "Rubing Chen",
      "Xiao-Yong Wei"
    ],
    "abstract": "Inference-time scaling has emerged as a powerful technique for enhancing the reasoning performance of Large Language Models (LLMs). However, existing approaches often rely on heuristic strategies for parallel sampling, lacking a principled foundation. To address this gap, we propose a probabilistic framework that formalizes the optimality of inference-time scaling under the assumption that parallel samples are independently and identically distributed (i.i.d.), and where the Best-of-$N$ selection strategy follows a probability distribution that can be estimated. Within this framework, we derive a theoretical lower bound on the required number of samples to achieve a target performance level, providing the first principled guidance for compute-efficient scaling. Leveraging this insight, we develop \\textsc{OptScale}, a practical algorithm that dynamically determines the optimal number of sampled responses. \\textsc{OptScale} employs a language model-based predictor to estimate probabilistic prior parameters, enabling the decision of the minimal number of samples needed that satisfy predefined performance thresholds and confidence levels. Extensive experiments on representative reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC) demonstrate that \\textsc{OptScale} significantly reduces sampling overhead while remaining better or on par with state-of-the-art reasoning performance. Our work offers both a theoretical foundation and a practical solution for principled inference-time scaling, addressing a critical gap in the efficient deployment of LLMs for complex reasoning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by AAAI-2026",
    "pdf_url": "https://arxiv.org/pdf/2506.22376v4",
    "published_date": "2025-06-27 16:44:11 UTC",
    "updated_date": "2025-12-19 10:17:53 UTC"
  },
  {
    "arxiv_id": "2506.22374v1",
    "title": "Sheaf-Based Decentralized Multimodal Learning for Next-Generation Wireless Communication Systems",
    "authors": [
      "Abdulmomen Ghalkha",
      "Zhuojun Tian",
      "Chaouki Ben Issaid",
      "Mehdi Bennis"
    ],
    "abstract": "In large-scale communication systems, increasingly complex scenarios require more intelligent collaboration among edge devices collecting various multimodal sensory data to achieve a more comprehensive understanding of the environment and improve decision-making accuracy. However, conventional federated learning (FL) algorithms typically consider unimodal datasets, require identical model architectures, and fail to leverage the rich information embedded in multimodal data, limiting their applicability to real-world scenarios with diverse modalities and varying client capabilities. To address this issue, we propose Sheaf-DMFL, a novel decentralized multimodal learning framework leveraging sheaf theory to enhance collaboration among devices with diverse modalities. Specifically, each client has a set of local feature encoders for its different modalities, whose outputs are concatenated before passing through a task-specific layer. While encoders for the same modality are trained collaboratively across clients, we capture the intrinsic correlations among clients' task-specific layers using a sheaf-based structure. To further enhance learning capability, we propose an enhanced algorithm named Sheaf-DMFL-Att, which tailors the attention mechanism within each client to capture correlations among different modalities. A rigorous convergence analysis of Sheaf-DMFL-Att is provided, establishing its theoretical guarantees. Extensive simulations are conducted on real-world link blockage prediction and mmWave beamforming scenarios, demonstrate the superiority of the proposed algorithms in such heterogeneous wireless communication systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.22374v1",
    "published_date": "2025-06-27 16:41:23 UTC",
    "updated_date": "2025-06-27 16:41:23 UTC"
  },
  {
    "arxiv_id": "2507.01053v3",
    "title": "Conversational LLMs Simplify Secure Clinical Data Access, Understanding, and Analysis",
    "authors": [
      "Rafi Al Attrach",
      "Pedro Moreira",
      "Rajna Fani",
      "Renato Umeton",
      "Amelia Fiske",
      "Leo Anthony Celi"
    ],
    "abstract": "Large-scale clinical databases offer opportunities for medical research, but their complexity creates barriers to effective use. The Medical Information Mart for Intensive Care (MIMIC-IV), one of the world's largest open-source electronic health record databases, traditionally requires both SQL proficiency and clinical domain expertise. We introduce M3, a system that enables natural language querying of MIMIC-IV data through the Model Context Protocol. With a single command, M3 retrieves MIMIC-IV from PhysioNet, launches a local SQLite instance or connects to hosted BigQuery, and allows researchers to pose clinical questions in plain English. We evaluated M3 using one hundred questions from the EHRSQL 2024 benchmark with two language models: the proprietary Claude Sonnet 4 achieved 94% accuracy, while the open-source gpt-oss-20B (deployable locally on consumer hardware) achieved 93% accuracy. Both models translate natural language into SQL, execute queries against MIMIC-IV, and return structured results alongside the underlying query for verification. Error analysis revealed that most failures stemmed from complex temporal reasoning or ambiguous question phrasing rather than fundamental architectural limitations. The comparable performance of a smaller open-source model demonstrates that privacy-preserving local deployment is viable for sensitive clinical data analysis. M3 lowers technical barriers to critical care data analysis while maintaining security through OAuth2 authentication, query validation, and comprehensive audit logging.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.IR",
    "comment": "16 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.01053v3",
    "published_date": "2025-06-27 16:24:17 UTC",
    "updated_date": "2025-11-23 19:16:31 UTC"
  },
  {
    "arxiv_id": "2506.22360v1",
    "title": "From Ground to Air: Noise Robustness in Vision Transformers and CNNs for Event-Based Vehicle Classification with Potential UAV Applications",
    "authors": [
      "Nouf Almesafri",
      "Hector Figueiredo",
      "Miguel Arana-Catania"
    ],
    "abstract": "This study investigates the performance of the two most relevant computer vision deep learning architectures, Convolutional Neural Network and Vision Transformer, for event-based cameras. These cameras capture scene changes, unlike traditional frame-based cameras with capture static images, and are particularly suited for dynamic environments such as UAVs and autonomous vehicles. The deep learning models studied in this work are ResNet34 and ViT B16, fine-tuned on the GEN1 event-based dataset. The research evaluates and compares these models under both standard conditions and in the presence of simulated noise. Initial evaluations on the clean GEN1 dataset reveal that ResNet34 and ViT B16 achieve accuracies of 88% and 86%, respectively, with ResNet34 showing a slight advantage in classification accuracy. However, the ViT B16 model demonstrates notable robustness, particularly given its pre-training on a smaller dataset. Although this study focuses on ground-based vehicle classification, the methodologies and findings hold significant promise for adaptation to UAV contexts, including aerial object classification and event-based vision systems for aviation-related tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "16 pages, 17 figures, 9 tables. To be presented in AIAA AVIATION Forum 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.22360v1",
    "published_date": "2025-06-27 16:21:00 UTC",
    "updated_date": "2025-06-27 16:21:00 UTC"
  },
  {
    "arxiv_id": "2506.22359v1",
    "title": "Concept-Level AI for Telecom: Moving Beyond Large Language Models",
    "authors": [
      "Viswanath Kumarskandpriya",
      "Abdulhalim Dandoush",
      "Abbas Bradai",
      "Ali Belgacem"
    ],
    "abstract": "The telecommunications and networking domain stands at the precipice of a transformative era, driven by the necessity to manage increasingly complex, hierarchical, multi administrative domains (i.e., several operators on the same path) and multilingual systems. Recent research has demonstrated that Large Language Models (LLMs), with their exceptional general-purpose text analysis and code generation capabilities, can be effectively applied to certain telecom problems (e.g., auto-configuration of data plan to meet certain application requirements). However, due to their inherent token-by-token processing and limited capacity for maintaining extended context, LLMs struggle to fulfill telecom-specific requirements such as cross-layer dependency cascades (i.e., over OSI), temporal-spatial fault correlation, and real-time distributed coordination. In contrast, Large Concept Models (LCMs), which reason at the abstraction level of semantic concepts rather than individual lexical tokens, offer a fundamentally superior approach for addressing these telecom challenges. By employing hyperbolic latent spaces for hierarchical representation and encapsulating complex multi-layered network interactions within concise concept embeddings, LCMs overcome critical shortcomings of LLMs in terms of memory efficiency, cross-layer correlation, and native multimodal integration. This paper argues that adopting LCMs is not simply an incremental step, but a necessary evolutionary leap toward achieving robust and effective AI-driven telecom management.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.22359v1",
    "published_date": "2025-06-27 16:20:18 UTC",
    "updated_date": "2025-06-27 16:20:18 UTC"
  },
  {
    "arxiv_id": "2506.22358v1",
    "title": "AI Model Passport: Data and System Traceability Framework for Transparent AI in Health",
    "authors": [
      "Varvara Kalokyri",
      "Nikolaos S. Tachos",
      "Charalampos N. Kalantzopoulos",
      "Stelios Sfakianakis",
      "Haridimos Kondylakis",
      "Dimitrios I. Zaridis",
      "Sara Colantonio",
      "Daniele Regge",
      "Nikolaos Papanikolaou",
      "The ProCAncer-I consortium",
      "Konstantinos Marias",
      "Dimitrios I. Fotiadis",
      "Manolis Tsiknakis"
    ],
    "abstract": "The increasing integration of Artificial Intelligence (AI) into health and biomedical systems necessitates robust frameworks for transparency, accountability, and ethical compliance. Existing frameworks often rely on human-readable, manual documentation which limits scalability, comparability, and machine interpretability across projects and platforms. They also fail to provide a unique, verifiable identity for AI models to ensure their provenance and authenticity across systems and use cases, limiting reproducibility and stakeholder trust. This paper introduces the concept of the AI Model Passport, a structured and standardized documentation framework that acts as a digital identity and verification tool for AI models. It captures essential metadata to uniquely identify, verify, trace and monitor AI models across their lifecycle - from data acquisition and preprocessing to model design, development and deployment. In addition, an implementation of this framework is presented through AIPassport, an MLOps tool developed within the ProCAncer-I EU project for medical imaging applications. AIPassport automates metadata collection, ensures proper versioning, decouples results from source scripts, and integrates with various development environments. Its effectiveness is showcased through a lesion segmentation use case using data from the ProCAncer-I dataset, illustrating how the AI Model Passport enhances transparency, reproducibility, and regulatory readiness while reducing manual effort. This approach aims to set a new standard for fostering trust and accountability in AI-driven healthcare solutions, aspiring to serve as the basis for developing transparent and regulation compliant AI systems across domains.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.22358v1",
    "published_date": "2025-06-27 16:16:15 UTC",
    "updated_date": "2025-06-27 16:16:15 UTC"
  },
  {
    "arxiv_id": "2506.22355v3",
    "title": "Embodied AI Agents: Modeling the World",
    "authors": [
      "Pascale Fung",
      "Yoram Bachrach",
      "Asli Celikyilmaz",
      "Kamalika Chaudhuri",
      "Delong Chen",
      "Willy Chung",
      "Emmanuel Dupoux",
      "Hongyu Gong",
      "HervÃ© JÃ©gou",
      "Alessandro Lazaric",
      "Arjun Majumdar",
      "Andrea Madotto",
      "Franziska Meier",
      "Florian Metze",
      "Louis-Philippe Morency",
      "ThÃ©o Moutakanni",
      "Juan Pino",
      "Basile Terver",
      "Joseph Tighe",
      "Paden Tomasello",
      "Jitendra Malik"
    ],
    "abstract": "This paper describes our research on AI agents embodied in visual, virtual or physical forms, enabling them to interact with both users and their environments. These agents, which include virtual avatars, wearable devices, and robots, are designed to perceive, learn and act within their surroundings, which makes them more similar to how humans learn and interact with the environments as compared to disembodied agents. We propose that the development of world models is central to reasoning and planning of embodied AI agents, allowing these agents to understand and predict their environment, to understand user intentions and social contexts, thereby enhancing their ability to perform complex tasks autonomously. World modeling encompasses the integration of multimodal perception, planning through reasoning for action and control, and memory to create a comprehensive understanding of the physical world. Beyond the physical world, we also propose to learn the mental world model of users to enable better human-agent collaboration.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.22355v3",
    "published_date": "2025-06-27 16:05:34 UTC",
    "updated_date": "2025-07-07 15:42:11 UTC"
  },
  {
    "arxiv_id": "2507.02940v1",
    "title": "Towards a Comparative Framework for Compositional AI Models",
    "authors": [
      "Tiffany Duneau"
    ],
    "abstract": "The DisCoCirc framework for natural language processing allows the construction of compositional models of text, by combining units for individual words together according to the grammatical structure of the text. The compositional nature of a model can give rise to two things: compositional generalisation -- the ability of a model to generalise outside its training distribution by learning compositional rules underpinning the entire data distribution -- and compositional interpretability -- making sense of how the model works by inspecting its modular components in isolation, as well as the processes through which these components are combined. We present these notions in a framework-agnostic way using the language of category theory, and adapt a series of tests for compositional generalisation to this setting.\n  Applying this to the DisCoCirc framework, we consider how well a selection of models can learn to compositionally generalise. We compare both quantum circuit based models, as well as classical neural networks, on a dataset derived from one of the bAbI tasks, extended to test a series of aspects of compositionality. Both architectures score within 5% of one another on the productivity and substitutivity tasks, but differ by at least 10% for the systematicity task, and exhibit different trends on the overgeneralisation tasks. Overall, we find the neural models are more prone to overfitting the Train data. Additionally, we demonstrate how to interpret a compositional model on one of the trained models. By considering how the model components interact with one another, we explain how the model behaves.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "quant-ph"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02940v1",
    "published_date": "2025-06-27 15:59:14 UTC",
    "updated_date": "2025-06-27 15:59:14 UTC"
  },
  {
    "arxiv_id": "2507.01052v1",
    "title": "Long-Sequence Memory with Temporal Kernels and Dense Hopfield Functionals",
    "authors": [
      "Ahmed Farooq"
    ],
    "abstract": "In this study we introduce a novel energy functional for long-sequence memory, building upon the framework of dense Hopfield networks which achieves exponential storage capacity through higher-order interactions. Building upon earlier work on long-sequence Hopfield memory models, we propose a temporal kernal $K(m, k)$ to incorporate temporal dependencies, enabling efficient sequential retrieval of patterns over extended sequences. We demonstrate the successful application of this technique for the storage and sequential retrieval of movies frames which are well suited for this because of the high dimensional vectors that make up each frame creating enough variation between even sequential frames in the high dimensional space. The technique has applications in modern transformer architectures, including efficient long-sequence modeling, memory augmentation, improved attention with temporal bias, and enhanced handling of long-term dependencies in time-series data. Our model offers a promising approach to address the limitations of transformers in long-context tasks, with potential implications for natural language processing, forecasting, and beyond.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.01052v1",
    "published_date": "2025-06-27 15:57:58 UTC",
    "updated_date": "2025-06-27 15:57:58 UTC"
  },
  {
    "arxiv_id": "2506.22342v1",
    "title": "A Framework for Multi-source Privacy Preserving Epidemic Analysis",
    "authors": [
      "Zihan Guan",
      "Zhiyuan Zhao",
      "Fengwei Tian",
      "Dung Nguyen",
      "Payel Bhattacharjee",
      "Ravi Tandon",
      "B. Aditya Prakash",
      "Anil Vullikanti"
    ],
    "abstract": "It is now well understood that diverse datasets provide a lot of value in key epidemiology and public health analyses, such as forecasting and nowcasting, development of epidemic models, evaluation and design of interventions and resource allocation. Some of these datasets are often sensitive, and need adequate privacy protections. There are many models of privacy, but Differential Privacy (DP) has become a de facto standard because of its strong guarantees, without making models about adversaries. In this paper, we develop a framework the integrates deep learning and epidemic models to simultaneously perform epidemic forecasting and learning a mechanistic model of epidemic spread, while incorporating multiple datasets for these analyses, including some with DP guarantees. We demonstrate our framework using a realistic but synthetic financial dataset with DP; such a dataset has not been used in such epidemic analyses. We show that this dataset provides significant value in forecasting and learning an epidemic model, even when used with DP guarantees.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.22342v1",
    "published_date": "2025-06-27 15:52:12 UTC",
    "updated_date": "2025-06-27 15:52:12 UTC"
  },
  {
    "arxiv_id": "2506.22338v1",
    "title": "A Deep Learning framework for building damage assessment using VHR SAR and geospatial data: demonstration on the 2023 Turkiye Earthquake",
    "authors": [
      "Luigi Russo",
      "Deodato Tapete",
      "Silvia Liberata Ullo",
      "Paolo Gamba"
    ],
    "abstract": "Building damage identification shortly after a disaster is crucial for guiding emergency response and recovery efforts. Although optical satellite imagery is commonly used for disaster mapping, its effectiveness is often hampered by cloud cover or the absence of pre-event acquisitions. To overcome these challenges, we introduce a novel multimodal deep learning (DL) framework for detecting building damage using single-date very high resolution (VHR) Synthetic Aperture Radar (SAR) imagery from the Italian Space Agency (ASI) COSMO SkyMed (CSK) constellation, complemented by auxiliary geospatial data. Our method integrates SAR image patches, OpenStreetMap (OSM) building footprints, digital surface model (DSM) data, and structural and exposure attributes from the Global Earthquake Model (GEM) to improve detection accuracy and contextual interpretation. Unlike existing approaches that depend on pre and post event imagery, our model utilizes only post event data, facilitating rapid deployment in critical scenarios. The framework effectiveness is demonstrated using a new dataset from the 2023 earthquake in Turkey, covering multiple cities with diverse urban settings. Results highlight that incorporating geospatial features significantly enhances detection performance and generalizability to previously unseen areas. By combining SAR imagery with detailed vulnerability and exposure information, our approach provides reliable and rapid building damage assessments without the dependency from available pre-event data. Moreover, the automated and scalable data generation process ensures the framework's applicability across diverse disaster-affected regions, underscoring its potential to support effective disaster management and recovery efforts. Code and data will be made available upon acceptance of the paper.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 6 figures (plus 4 author photos), and 5 tables. Submitted to IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",
    "pdf_url": "https://arxiv.org/pdf/2506.22338v1",
    "published_date": "2025-06-27 15:49:58 UTC",
    "updated_date": "2025-06-27 15:49:58 UTC"
  },
  {
    "arxiv_id": "2508.00838v1",
    "title": "The Attribution Crisis in LLM Search Results",
    "authors": [
      "Ilan Strauss",
      "Jangho Yang",
      "Tim O'Reilly",
      "Sruly Rosenblat",
      "Isobel Moure"
    ],
    "abstract": "Web-enabled LLMs frequently answer queries without crediting the web pages they consume, creating an \"attribution gap\" - the difference between relevant URLs read and those actually cited. Drawing on approximately 14,000 real-world LMArena conversation logs with search-enabled LLM systems, we document three exploitation patterns: 1) No Search: 34% of Google Gemini and 24% of OpenAI GPT-4o responses are generated without explicitly fetching any online content; 2) No citation: Gemini provides no clickable citation source in 92% of answers; 3) High-volume, low-credit: Perplexity's Sonar visits approximately 10 relevant pages per query but cites only three to four. A negative binomial hurdle model shows that the average query answered by Gemini or Sonar leaves about 3 relevant websites uncited, whereas GPT-4o's tiny uncited gap is best explained by its selective log disclosures rather than by better attribution. Citation efficiency - extra citations provided per additional relevant web page visited - varies widely across models, from 0.19 to 0.45 on identical queries, underscoring that retrieval design, not technical limits, shapes ecosystem impact. We recommend a transparent LLM search architecture based on standardized telemetry and full disclosure of search traces and citation logs.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.DL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.00838v1",
    "published_date": "2025-06-27 15:44:16 UTC",
    "updated_date": "2025-06-27 15:44:16 UTC"
  },
  {
    "arxiv_id": "2506.22331v2",
    "title": "Less Greedy Equivalence Search",
    "authors": [
      "Adiba Ejaz",
      "Elias Bareinboim"
    ],
    "abstract": "Greedy Equivalence Search (GES) is a classic score-based algorithm for causal discovery from observational data. In the sample limit, it recovers the Markov equivalence class of graphs that describe the data. Still, it faces two challenges in practice: computational cost and finite-sample accuracy. In this paper, we develop Less Greedy Equivalence Search (LGES), a variant of GES that retains its theoretical guarantees while partially addressing these limitations. LGES modifies the greedy step; rather than always applying the highest-scoring insertion, it avoids edge insertions between variables for which the score implies some conditional independence. This more targeted search yields up to a \\(10\\)-fold speed-up and a substantial reduction in structural error relative to GES. Moreover, LGES can guide the search using prior knowledge, and can correct this knowledge when contradicted by data. Finally, LGES can use interventional data to refine the learned observational equivalence class. We prove that LGES recovers the true equivalence class in the sample limit, even with misspecified knowledge. Experiments demonstrate that LGES outperforms GES and other baselines in speed, accuracy, and robustness to misspecified knowledge. Our code is available at https://github.com/CausalAILab/lges.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.22331v2",
    "published_date": "2025-06-27 15:39:48 UTC",
    "updated_date": "2025-11-06 21:42:42 UTC"
  },
  {
    "arxiv_id": "2506.22321v1",
    "title": "A Practical Approach to Power Saving in Hearables Using Sub-Nyquist Sampling with Bandwidth Extension",
    "authors": [
      "Tarikul Islam Tamiti",
      "Anomadarshi Barua"
    ],
    "abstract": "Hearables are wearable computers that are worn on the ear. Bone conduction microphones (BCMs) are used with air conduction microphones (ACMs) in hearables as a supporting modality for multimodal speech enhancement (SE) in noisy conditions. However, existing works don't consider the following practical aspects for low-power implementations on hearables: (i) They do not explore how lowering the sampling frequencies and bit resolutions in analog-to-digital converters (ADCs) of hearables jointly impact low-power processing and multimodal SE in terms of speech quality and intelligibility. (ii) They don't discuss how GAN-like audio quality can be achieved without using actual GAN discriminators. And (iii) They don't process signals from ACMs/BCMs at sub-Nyquist sampling rate because, in their frameworks, they lack a wideband reconstruction methodology from their narrowband parts. We propose SUBARU (\\textbf{Sub}-Nyquist \\textbf{A}udio \\textbf{R}esolution \\textbf{U}psampling), which achieves the following: SUBARU (i) intentionally uses sub-Nyquist sampling and low bit resolution in ADCs, achieving a 3.31x reduction in power consumption; (ii) introduces novel multi-scale and multi-period virtual discriminators, which achieve GAN-like audio quality without using GANs' adversarial training; and (iii) achieves streaming operations on mobile platforms and SE in in-the-wild noisy conditions with an inference time of 1.74ms and a memory footprint of less than 13.77MB.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.22321v1",
    "published_date": "2025-06-27 15:35:04 UTC",
    "updated_date": "2025-06-27 15:35:04 UTC"
  },
  {
    "arxiv_id": "2507.01051v1",
    "title": "Can AI be Consentful?",
    "authors": [
      "Giada Pistilli",
      "Bruna Trevelin"
    ],
    "abstract": "The evolution of generative AI systems exposes the challenges of traditional legal and ethical frameworks built around consent. This chapter examines how the conventional notion of consent, while fundamental to data protection and privacy rights, proves insufficient in addressing the implications of AI-generated content derived from personal data. Through legal and ethical analysis, we show that while individuals can consent to the initial use of their data for AI training, they cannot meaningfully consent to the numerous potential outputs their data might enable or the extent to which the output is used or distributed. We identify three fundamental challenges: the scope problem, the temporality problem, and the autonomy trap, which collectively create what we term a ''consent gap'' in AI systems and their surrounding ecosystem. We argue that current legal frameworks inadequately address these emerging challenges, particularly regarding individual autonomy, identity rights, and social responsibility, especially in cases where AI-generated content creates new forms of personal representation beyond the scope of the original consent. By examining how these consent limitations intersect with broader principles of responsible AI (including fairness, transparency, accountability, and autonomy) we demonstrate the need to evolve ethical and legal approaches to consent.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.01051v1",
    "published_date": "2025-06-27 15:32:16 UTC",
    "updated_date": "2025-06-27 15:32:16 UTC"
  },
  {
    "arxiv_id": "2506.22309v1",
    "title": "Conceptual Topic Aggregation",
    "authors": [
      "Klara M. Gutekunst",
      "Dominik DÃ¼rrschnabel",
      "Johannes Hirth",
      "Gerd Stumme"
    ],
    "abstract": "The vast growth of data has rendered traditional manual inspection infeasible, necessitating the adoption of computational methods for efficient data exploration. Topic modeling has emerged as a powerful tool for analyzing large-scale textual datasets, enabling the extraction of latent semantic structures. However, existing methods for topic modeling often struggle to provide interpretable representations that facilitate deeper insights into data structure and content. In this paper, we propose FAT-CAT, an approach based on Formal Concept Analysis (FCA) to enhance meaningful topic aggregation and visualization of discovered topics. Our approach can handle diverse topics and file types -- grouped by directories -- to construct a concept lattice that offers a structured, hierarchical representation of their topic distribution. In a case study on the ETYNTKE dataset, we evaluate the effectiveness of our approach against other representation methods to demonstrate that FCA-based aggregation provides more meaningful and interpretable insights into dataset composition than existing topic modeling techniques.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.DM",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 4 tables, 11 figures, International Joint Conference on Conceptual Knowledge Structures",
    "pdf_url": "https://arxiv.org/pdf/2506.22309v1",
    "published_date": "2025-06-27 15:19:38 UTC",
    "updated_date": "2025-06-27 15:19:38 UTC"
  },
  {
    "arxiv_id": "2506.22299v1",
    "title": "CoATA: Effective Co-Augmentation of Topology and Attribute for Graph Neural Networks",
    "authors": [
      "Tao Liu",
      "Longlong Lin",
      "Yunfeng Yu",
      "Xi Ou",
      "Youan Zhang",
      "Zhiqiu Ye",
      "Tao Jia"
    ],
    "abstract": "Graph Neural Networks (GNNs) have garnered substantial attention due to their remarkable capability in learning graph representations. However, real-world graphs often exhibit substantial noise and incompleteness, which severely degrades the performance of GNNs. Existing methods typically address this issue through single-dimensional augmentation, focusing either on refining topology structures or perturbing node attributes, thereby overlooking the deeper interplays between the two. To bridge this gap, this paper presents CoATA, a dual-channel GNN framework specifically designed for the Co-Augmentation of Topology and Attribute. Specifically, CoATA first propagates structural signals to enrich and denoise node attributes. Then, it projects the enhanced attribute space into a node-attribute bipartite graph for further refinement or reconstruction of the underlying structure. Subsequently, CoATA introduces contrastive learning, leveraging prototype alignment and consistency constraints, to facilitate mutual corrections between the augmented and original graphs. Finally, extensive experiments on seven benchmark datasets demonstrate that the proposed CoATA outperforms eleven state-of-the-art baseline methods, showcasing its effectiveness in capturing the synergistic relationship between topology and attributes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "icmr",
    "pdf_url": "https://arxiv.org/pdf/2506.22299v1",
    "published_date": "2025-06-27 15:11:49 UTC",
    "updated_date": "2025-06-27 15:11:49 UTC"
  },
  {
    "arxiv_id": "2506.22291v1",
    "title": "RoomCraft: Controllable and Complete 3D Indoor Scene Generation",
    "authors": [
      "Mengqi Zhou",
      "Xipeng Wang",
      "Yuxi Wang",
      "Zhaoxiang Zhang"
    ],
    "abstract": "Generating realistic 3D indoor scenes from user inputs remains a challenging problem in computer vision and graphics, requiring careful balance of geometric consistency, spatial relationships, and visual realism. While neural generation methods often produce repetitive elements due to limited global spatial reasoning, procedural approaches can leverage constraints for controllable generation but struggle with multi-constraint scenarios. When constraints become numerous, object collisions frequently occur, forcing the removal of furniture items and compromising layout completeness.\n  To address these limitations, we propose RoomCraft, a multi-stage pipeline that converts real images, sketches, or text descriptions into coherent 3D indoor scenes. Our approach combines a scene generation pipeline with a constraint-driven optimization framework. The pipeline first extracts high-level scene information from user inputs and organizes it into a structured format containing room type, furniture items, and spatial relations. It then constructs a spatial relationship network to represent furniture arrangements and generates an optimized placement sequence using a heuristic-based depth-first search (HDFS) algorithm to ensure layout coherence. To handle complex multi-constraint scenarios, we introduce a unified constraint representation that processes both formal specifications and natural language inputs, enabling flexible constraint-oriented adjustments through a comprehensive action space design. Additionally, we propose a Conflict-Aware Positioning Strategy (CAPS) that dynamically adjusts placement weights to minimize furniture collisions and ensure layout completeness.\n  Extensive experiments demonstrate that RoomCraft significantly outperforms existing methods in generating realistic, semantically coherent, and visually appealing room layouts across diverse input modalities.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.22291v1",
    "published_date": "2025-06-27 15:03:17 UTC",
    "updated_date": "2025-06-27 15:03:17 UTC"
  },
  {
    "arxiv_id": "2506.22276v1",
    "title": "Artificial Intelligent Disobedience: Rethinking the Agency of Our Artificial Teammates",
    "authors": [
      "Reuth Mirsky"
    ],
    "abstract": "Artificial intelligence has made remarkable strides in recent years, achieving superhuman performance across a wide range of tasks. Yet despite these advances, most cooperative AI systems remain rigidly obedient, designed to follow human instructions without question and conform to user expectations, even when doing so may be counterproductive or unsafe. This paper argues for expanding the agency of AI teammates to include \\textit{intelligent disobedience}, empowering them to make meaningful and autonomous contributions within human-AI teams. It introduces a scale of AI agency levels and uses representative examples to highlight the importance and growing necessity of treating AI autonomy as an independent research focus in cooperative settings. The paper then explores how intelligent disobedience manifests across different autonomy levels and concludes by proposing initial boundaries and considerations for studying disobedience as a core capability of artificial agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Extended version of a paper accepted for publication in AI Magazine",
    "pdf_url": "https://arxiv.org/pdf/2506.22276v1",
    "published_date": "2025-06-27 14:45:27 UTC",
    "updated_date": "2025-06-27 14:45:27 UTC"
  },
  {
    "arxiv_id": "2506.22271v2",
    "title": "Breaking Rank Bottlenecks in Knowledge Graph Embeddings",
    "authors": [
      "Samy Badreddine",
      "Emile van Krieken",
      "Luciano Serafini"
    ],
    "abstract": "Many knowledge graph embedding (KGE) models for link prediction use powerful encoders. However, they often rely on a simple hidden vector-matrix multiplication to score subject-relation queries against candidate object entities. When the number of entities is larger than the model's embedding dimension, which is often the case in practice by several orders of magnitude, we have a linear output layer with a rank bottleneck. Such bottlenecked layers limit model expressivity. We investigate both theoretically and empirically how rank bottlenecks affect KGEs. We find that, by limiting the set of feasible predictions, rank bottlenecks hurt the ranking accuracy and distribution fidelity of scores. Inspired by the language modelling literature, we propose KGE-MoS, a mixture-based output layer to break rank bottlenecks in many KGEs. Our experiments show that KGE-MoS improves ranking performance of KGE models on large-scale datasets at a low parameter cost.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.22271v2",
    "published_date": "2025-06-27 14:41:22 UTC",
    "updated_date": "2025-09-29 09:55:48 UTC"
  },
  {
    "arxiv_id": "2507.02939v2",
    "title": "Frequency-Aligned Knowledge Distillation for Lightweight Spatiotemporal Forecasting",
    "authors": [
      "Yuqi Li",
      "Chuanguang Yang",
      "Hansheng Zeng",
      "Zeyu Dong",
      "Zhulin An",
      "Yongjun Xu",
      "Yingli Tian",
      "Hao Wu"
    ],
    "abstract": "Spatiotemporal forecasting tasks, such as traffic flow, combustion dynamics, and weather forecasting, often require complex models that suffer from low training efficiency and high memory consumption. This paper proposes a lightweight framework, Spectral Decoupled Knowledge Distillation (termed SDKD), which transfers the multi-scale spatiotemporal representations from a complex teacher model to a more efficient lightweight student network. The teacher model follows an encoder-latent evolution-decoder architecture, where its latent evolution module decouples high-frequency details and low-frequency trends using convolution and Transformer (global low-frequency modeler). However, the multi-layer convolution and deconvolution structures result in slow training and high memory usage. To address these issues, we propose a frequency-aligned knowledge distillation strategy, which extracts multi-scale spectral features from the teacher's latent space, including both high and low frequency components, to guide the lightweight student model in capturing both local fine-grained variations and global evolution patterns. Experimental results show that SDKD significantly improves performance, achieving reductions of up to 81.3% in MSE and in MAE 52.3% on the Navier-Stokes equation dataset. The framework effectively captures both high-frequency variations and long-term trends while reducing computational complexity. Our codes are available at https://github.com/itsnotacie/SDKD",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICCV-2025, 11 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.02939v2",
    "published_date": "2025-06-27 14:24:37 UTC",
    "updated_date": "2025-07-20 17:02:56 UTC"
  },
  {
    "arxiv_id": "2506.22255v1",
    "title": "Projected Compression: Trainable Projection for Efficient Transformer Compression",
    "authors": [
      "Maciej Stefaniak",
      "MichaÅ Krutul",
      "Jan MaÅaÅnicki",
      "Maciej PiÃ³ro",
      "Jakub Krajewski",
      "Sebastian Jaszczur",
      "Marek Cygan",
      "Kamil Adamczewski",
      "Jan Ludziejewski"
    ],
    "abstract": "Large language models have steadily increased in size to achieve improved performance; however, this growth has also led to greater inference time and computational demands. Consequently, there is rising interest in model size reduction methods. To address this issue, we propose Projected Compression, a novel model compression technique, that reduces model weights by utilizing projection modules. Specifically, we first train additional trainable projections weights and preserve access to all the original model parameters. Subsequently, these projections are merged into a lower-dimensional product matrix, resulting in a reduced-size standard Transformer-based model. Unlike alternative approaches that require additional computational overhead, our method matches the base model's per-token computation step in FLOPs. Experimental results show that Projected Compression outperforms the comparable hard pruning and retraining approach on higher quality models. Moreover, the performance margin scales well with the number of tokens.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.22255v1",
    "published_date": "2025-06-27 14:24:01 UTC",
    "updated_date": "2025-06-27 14:24:01 UTC"
  },
  {
    "arxiv_id": "2506.22231v1",
    "title": "Adapting University Policies for Generative AI: Opportunities, Challenges, and Policy Solutions in Higher Education",
    "authors": [
      "Russell Beale"
    ],
    "abstract": "The rapid proliferation of generative artificial intelligence (AI) tools - especially large language models (LLMs) such as ChatGPT - has ushered in a transformative era in higher education. Universities in developed regions are increasingly integrating these technologies into research, teaching, and assessment. On one hand, LLMs can enhance productivity by streamlining literature reviews, facilitating idea generation, assisting with coding and data analysis, and even supporting grant proposal drafting. On the other hand, their use raises significant concerns regarding academic integrity, ethical boundaries, and equitable access. Recent empirical studies indicate that nearly 47% of students use LLMs in their coursework - with 39% using them for exam questions and 7% for entire assignments - while detection tools currently achieve around 88% accuracy, leaving a 12% error margin. This article critically examines the opportunities offered by generative AI, explores the multifaceted challenges it poses, and outlines robust policy solutions. Emphasis is placed on redesigning assessments to be AI-resilient, enhancing staff and student training, implementing multi-layered enforcement mechanisms, and defining acceptable use. By synthesizing data from recent research and case studies, the article argues that proactive policy adaptation is imperative to harness AI's potential while safeguarding the core values of academic integrity and equity.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.22231v1",
    "published_date": "2025-06-27 13:49:02 UTC",
    "updated_date": "2025-06-27 13:49:02 UTC"
  },
  {
    "arxiv_id": "2506.22200v5",
    "title": "EFRame: Deeper Reasoning via Exploration-Filter-Replay Reinforcement Learning Framework",
    "authors": [
      "Chen Wang",
      "Lai Wei",
      "Yanzhi Zhang",
      "Chenyang Shao",
      "Zedong Dan",
      "Weiran Huang",
      "Yuzhi Zhang",
      "Yue Wang"
    ],
    "abstract": "Recent advances in reinforcement learning (RL) have significantly enhanced the reasoning capabilities of large language models (LLMs). Group Relative Policy Optimization (GRPO), a lightweight variant of Proximal Policy Optimization (PPO), improves efficiency but suffers from limited exploration and training instability, limiting its effectiveness on complex reasoning tasks. To address these challenges, we introduce EFRame, an Exploration-Filter-Replay framework that augments GRPO across three dimensions: additional rollouts enable deeper and more targeted exploration, online filtering removes low-quality samples to stabilize gradients and accelerate training, and experience replay amplifies rare yet informative trajectories for stable convergence. This unified framework establishes a principled training cycle that balances exploration, efficiency, and stability. Experiments on diverse reasoning benchmarks demonstrate that EFRame achieves consistent gains, including a 37.9\\% relative improvement on Geometry3K over GRPO. EFRame further supports fine-grained sample categorization and precise entropy control, highlighting it as a robust solution for advancing deeper reasoning in LLMs. Our code is available at https://github.com/597358816/EFRame.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.22200v5",
    "published_date": "2025-06-27 13:09:05 UTC",
    "updated_date": "2025-10-10 01:11:19 UTC"
  },
  {
    "arxiv_id": "2506.22185v1",
    "title": "Autonomic Microservice Management via Agentic AI and MAPE-K Integration",
    "authors": [
      "Matteo Esposito",
      "Alexander Bakhtin",
      "Noman Ahmad",
      "Mikel Robredo",
      "Ruoyu Su",
      "Valentina Lenarduzzi",
      "Davide Taibi"
    ],
    "abstract": "While microservices are revolutionizing cloud computing by offering unparalleled scalability and independent deployment, their decentralized nature poses significant security and management challenges that can threaten system stability. We propose a framework based on MAPE-K, which leverages agentic AI, for autonomous anomaly detection and remediation to address the daunting task of highly distributed system management. Our framework offers practical, industry-ready solutions for maintaining robust and secure microservices. Practitioners and researchers can customize the framework to enhance system stability, reduce downtime, and monitor broader system quality attributes such as system performance level, resilience, security, and anomaly management, among others.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.DC",
      "cs.NI",
      "eess.SY"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.22185v1",
    "published_date": "2025-06-27 12:46:12 UTC",
    "updated_date": "2025-06-27 12:46:12 UTC"
  },
  {
    "arxiv_id": "2506.22183v1",
    "title": "A Different Approach to AI Safety: Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety",
    "authors": [
      "Camille FranÃ§ois",
      "Ludovic PÃ©ran",
      "Ayah Bdeir",
      "Nouha Dziri",
      "Will Hawkins",
      "Yacine Jernite",
      "Sayash Kapoor",
      "Juliet Shen",
      "Heidy Khlaaf",
      "Kevin Klyman",
      "Nik Marda",
      "Marie Pellat",
      "Deb Raji",
      "Divya Siddarth",
      "Aviya Skowron",
      "Joseph Spisak",
      "Madhulika Srikumar",
      "Victor Storchan",
      "Audrey Tang",
      "Jen Weedon"
    ],
    "abstract": "The rapid rise of open-weight and open-source foundation models is intensifying the obligation and reshaping the opportunity to make AI systems safe. This paper reports outcomes from the Columbia Convening on AI Openness and Safety (San Francisco, 19 Nov 2024) and its six-week preparatory programme involving more than forty-five researchers, engineers, and policy leaders from academia, industry, civil society, and government. Using a participatory, solutions-oriented process, the working groups produced (i) a research agenda at the intersection of safety and open source AI; (ii) a mapping of existing and needed technical interventions and open source tools to safely and responsibly deploy open foundation models across the AI development workflow; and (iii) a mapping of the content safety filter ecosystem with a proposed roadmap for future research and development. We find that openness -- understood as transparent weights, interoperable tooling, and public governance -- can enhance safety by enabling independent scrutiny, decentralized mitigation, and culturally plural oversight. However, significant gaps persist: scarce multimodal and multilingual benchmarks, limited defenses against prompt-injection and compositional attacks in agentic systems, and insufficient participatory mechanisms for communities most affected by AI harms. The paper concludes with a roadmap of five priority research directions, emphasizing participatory inputs, future-proof content filters, ecosystem-wide safety infrastructure, rigorous agentic safeguards, and expanded harm taxonomies. These recommendations informed the February 2025 French AI Action Summit and lay groundwork for an open, plural, and accountable AI safety discipline.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety",
    "pdf_url": "https://arxiv.org/pdf/2506.22183v1",
    "published_date": "2025-06-27 12:45:44 UTC",
    "updated_date": "2025-06-27 12:45:44 UTC"
  },
  {
    "arxiv_id": "2506.22179v1",
    "title": "Frequency-Semantic Enhanced Variational Autoencoder for Zero-Shot Skeleton-based Action Recognition",
    "authors": [
      "Wenhan Wu",
      "Zhishuai Guo",
      "Chen Chen",
      "Hongfei Xue",
      "Aidong Lu"
    ],
    "abstract": "Zero-shot skeleton-based action recognition aims to develop models capable of identifying actions beyond the categories encountered during training. Previous approaches have primarily focused on aligning visual and semantic representations but often overlooked the importance of fine-grained action patterns in the semantic space (e.g., the hand movements in drinking water and brushing teeth). To address these limitations, we propose a Frequency-Semantic Enhanced Variational Autoencoder (FS-VAE) to explore the skeleton semantic representation learning with frequency decomposition. FS-VAE consists of three key components: 1) a frequency-based enhancement module with high- and low-frequency adjustments to enrich the skeletal semantics learning and improve the robustness of zero-shot action recognition; 2) a semantic-based action description with multilevel alignment to capture both local details and global correspondence, effectively bridging the semantic gap and compensating for the inherent loss of information in skeleton sequences; 3) a calibrated cross-alignment loss that enables valid skeleton-text pairs to counterbalance ambiguous ones, mitigating discrepancies and ambiguities in skeleton and text features, thereby ensuring robust alignment. Evaluations on the benchmarks demonstrate the effectiveness of our approach, validating that frequency-enhanced semantic features enable robust differentiation of visually and semantically similar action clusters, improving zero-shot action recognition.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ICCV 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.22179v1",
    "published_date": "2025-06-27 12:44:08 UTC",
    "updated_date": "2025-06-27 12:44:08 UTC"
  },
  {
    "arxiv_id": "2506.22146v4",
    "title": "Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs",
    "authors": [
      "Amirmohammad Izadi",
      "Mohammad Ali Banayeeanzade",
      "Fatemeh Askari",
      "Ali Rahimiakbar",
      "Mohammad Mahdi Vahedi",
      "Hosein Hasani",
      "Mahdieh Soleymani Baghshah"
    ],
    "abstract": "Despite progress in Large Vision-Language Models (LVLMs), their capacity for visual reasoning is often limited by the binding problem: the failure to reliably associate perceptual features with their correct visual referents. This limitation underlies persistent errors in tasks such as counting, visual search, scene description, and spatial relationship understanding. A key factor is that current LVLMs process visual features largely in parallel, lacking mechanisms for spatially grounded, serial attention. This paper introduces Visual Input Structure for Enhanced Reasoning (VISER), a simple, effective method that augments visual inputs with low-level spatial structures and pairs them with a textual prompt that encourages sequential, spatially-aware parsing. We empirically demonstrate substantial performance improvements across core visual reasoning tasks, using only a single-query inference. Specifically, VISER improves GPT-4o performance on visual search, counting, and spatial relationship tasks by 25.0%, 26.8%, and 9.5%, respectively, and reduces edit distance error in scene description by 0.32 on 2D datasets. Furthermore, we find that the visual modification is essential for these gains; purely textual strategies, including Chain-of-Thought prompting, are insufficient and can even degrade performance. VISER underscores the importance of visual input design over purely linguistically based reasoning strategies and suggests that visual structuring is a powerful and general approach for enhancing compositional and spatial reasoning in LVLMs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to NeurIPS 2025 (Thirty-ninth Conference on Neural Information Processing Systems)",
    "pdf_url": "https://arxiv.org/pdf/2506.22146v4",
    "published_date": "2025-06-27 11:44:40 UTC",
    "updated_date": "2025-11-10 11:12:13 UTC"
  },
  {
    "arxiv_id": "2506.22095v4",
    "title": "Beyond Simple Graphs: Neural Multi-Objective Routing on Multigraphs",
    "authors": [
      "Filip Rydin",
      "Attila Lischka",
      "Jiaming Wu",
      "Morteza Haghir Chehreghani",
      "BalÃ¡zs KulcsÃ¡r"
    ],
    "abstract": "Learning-based methods for routing have gained significant attention in recent years, both in single-objective and multi-objective contexts. Yet, existing methods are unsuitable for routing on multigraphs, which feature multiple edges with distinct attributes between node pairs, despite their strong relevance in real-world scenarios. In this paper, we propose two graph neural network-based methods to address multi-objective routing on multigraphs. Our first approach operates directly on the multigraph by autoregressively selecting edges until a tour is completed. The second model, which is more scalable, first simplifies the multigraph via a learned pruning strategy and then performs autoregressive routing on the resulting simple graph. We evaluate both models empirically, across a wide range of problems and graph distributions, and demonstrate their competitive performance compared to strong heuristics and neural baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "29 pages, 6 Figures",
    "pdf_url": "https://arxiv.org/pdf/2506.22095v4",
    "published_date": "2025-06-27 10:25:58 UTC",
    "updated_date": "2025-09-26 06:37:50 UTC"
  },
  {
    "arxiv_id": "2506.22084v1",
    "title": "Transformers are Graph Neural Networks",
    "authors": [
      "Chaitanya K. Joshi"
    ],
    "abstract": "We establish connections between the Transformer architecture, originally introduced for natural language processing, and Graph Neural Networks (GNNs) for representation learning on graphs. We show how Transformers can be viewed as message passing GNNs operating on fully connected graphs of tokens, where the self-attention mechanism capture the relative importance of all tokens w.r.t. each-other, and positional encodings provide hints about sequential ordering or structure. Thus, Transformers are expressive set processing networks that learn relationships among input elements without being constrained by apriori graphs. Despite this mathematical connection to GNNs, Transformers are implemented via dense matrix operations that are significantly more efficient on modern hardware than sparse message passing. This leads to the perspective that Transformers are GNNs currently winning the hardware lottery.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper is a technical version of an article in The Gradient at https://thegradient.pub/transformers-are-graph-neural-networks/",
    "pdf_url": "https://arxiv.org/pdf/2506.22084v1",
    "published_date": "2025-06-27 10:15:33 UTC",
    "updated_date": "2025-06-27 10:15:33 UTC"
  },
  {
    "arxiv_id": "2506.22068v1",
    "title": "Query as Test: An Intelligent Driving Test and Data Storage Method for Integrated Cockpit-Vehicle-Road Scenarios",
    "authors": [
      "Shengyue Yao",
      "Runqing Guo",
      "Yangyang Qin",
      "Miangbing Meng",
      "Jipeng Cao",
      "Yilun Lin",
      "Yisheng Lv",
      "Fei-Yue Wang"
    ],
    "abstract": "With the deep penetration of Artificial Intelligence (AI) in the transportation sector, intelligent cockpits, autonomous driving, and intelligent road networks are developing at an unprecedented pace. However, the data ecosystems of these three key areas are increasingly fragmented and incompatible. Especially, existing testing methods rely on data stacking, fail to cover all edge cases, and lack flexibility. To address this issue, this paper introduces the concept of \"Query as Test\" (QaT). This concept shifts the focus from rigid, prescripted test cases to flexible, on-demand logical queries against a unified data representation. Specifically, we identify the need for a fundamental improvement in data storage and representation, leading to our proposal of \"Extensible Scenarios Notations\" (ESN). ESN is a novel declarative data framework based on Answer Set Programming (ASP), which uniformly represents heterogeneous multimodal data from the cockpit, vehicle, and road as a collection of logical facts and rules. This approach not only achieves deep semantic fusion of data, but also brings three core advantages: (1) supports complex and flexible semantic querying through logical reasoning; (2) provides natural interpretability for decision-making processes; (3) allows for on-demand data abstraction through logical rules, enabling fine-grained privacy protection. We further elaborate on the QaT paradigm, transforming the functional validation and safety compliance checks of autonomous driving systems into logical queries against the ESN database, significantly enhancing the expressiveness and formal rigor of the testing. Finally, we introduce the concept of \"Validation-Driven Development\" (VDD), which suggests to guide developments by logical validation rather than quantitative testing in the era of Large Language Models, in order to accelerating the iteration and development process.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Submitted to IEEE Transaction on Vehicular Technology",
    "pdf_url": "https://arxiv.org/pdf/2506.22068v1",
    "published_date": "2025-06-27 09:59:58 UTC",
    "updated_date": "2025-06-27 09:59:58 UTC"
  },
  {
    "arxiv_id": "2506.22056v1",
    "title": "Universal Retrieval for Multimodal Trajectory Modeling",
    "authors": [
      "Xuan Zhang",
      "Ziyan Jiang",
      "Rui Meng",
      "Yifei Leng",
      "Zhenbang Xiao",
      "Zora Zhiruo Wang",
      "Yanyi Shang",
      "Dehan Kong"
    ],
    "abstract": "Trajectory data, capturing human actions and environmental states across various modalities, holds significant potential for enhancing AI agent capabilities, particularly in GUI environments. However, how to model the representation of trajectory-level data presents a significant challenge that has not been systematically addressed amid explosive trajectory data growth. In this work, we introduce Multimodal Trajectory Retrieval, bridging the gap between universal retrieval and agent-centric trajectory modeling. We construct the Unified Agent Trajectory Dataset (UATD) from annotated demonstrations and states across diverse real-world scenarios. Based on this, we present GAE-Bench, a benchmark containing a large number of trajectory-based retrieval pairs. In addition, we propose GAE-Retriever, a multimodal retrieval framework that adopts vision-language models and incorporates optimized contrastive learning through a token selection and the GradCache mechanism. Comprehensive evaluations across multiple datasets show that GAE-Retriever consistently outperforms strong baselines in retrieval recall, highlighting its effectiveness in advancing multimodal trajectory retrieval.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 3 figures, accepted by Workshop on Computer-use Agents @ ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.22056v1",
    "published_date": "2025-06-27 09:50:38 UTC",
    "updated_date": "2025-06-27 09:50:38 UTC"
  },
  {
    "arxiv_id": "2506.22039v1",
    "title": "UniCA: Adapting Time Series Foundation Model to General Covariate-Aware Forecasting",
    "authors": [
      "Lu Han",
      "Yu Liu",
      "Qiwen Deng",
      "Jian Jiang",
      "Yinbo Sun",
      "Zhe Yu",
      "Binfeng Wang",
      "Xingyu Lu",
      "Lintao Ma",
      "Han-Jia Ye",
      "De-Chuan Zhan"
    ],
    "abstract": "Time Series Foundation Models (TSFMs) have achieved remarkable success through large-scale pretraining. However, their design primarily targets real-valued series, limiting their ability to handle general forecasting tasks involving diverse and often heterogeneous covariates--such as categorical variables and multimodal data (e.g., images, text)--which are typically task-specific and difficult to leverage during pretraining. To address this gap, we propose Unified Covariate Adaptation (UniCA), a framework to bridge TSFMs with general covariate-aware forecasting. UniCA first performs covariate homogenization to transform heterogeneous covariates into high-level homogeneous series representations and then fuses them via a unified attention-based fusion mechanism. UniCA is compatible and universal for adaptation with both homogeneous and heterogeneous covariates, incorporating extra covariate information while preserving the generalization ability of TSFMs.Extensive experiments on multiple unimodal and multimodal covariate-aware forecasting benchmarks demonstrate the superiority of UniCA, highlighting the promise of covariate-aware TSFM adaptation in real-world forecasting scenarios. Codes are released on https://github.com/hanlu-nju/UniCA.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.22039v1",
    "published_date": "2025-06-27 09:35:51 UTC",
    "updated_date": "2025-06-27 09:35:51 UTC"
  },
  {
    "arxiv_id": "2506.22026v1",
    "title": "Literature-Grounded Novelty Assessment of Scientific Ideas",
    "authors": [
      "Simra Shahid",
      "Marissa Radensky",
      "Raymond Fok",
      "Pao Siangliulue",
      "Daniel S. Weld",
      "Tom Hope"
    ],
    "abstract": "Automated scientific idea generation systems have made remarkable progress, yet the automatic evaluation of idea novelty remains a critical and underexplored challenge. Manual evaluation of novelty through literature review is labor-intensive, prone to error due to subjectivity, and impractical at scale. To address these issues, we propose the Idea Novelty Checker, an LLM-based retrieval-augmented generation (RAG) framework that leverages a two-stage retrieve-then-rerank approach. The Idea Novelty Checker first collects a broad set of relevant papers using keyword and snippet-based retrieval, then refines this collection through embedding-based filtering followed by facet-based LLM re-ranking. It incorporates expert-labeled examples to guide the system in comparing papers for novelty evaluation and in generating literature-grounded reasoning. Our extensive experiments demonstrate that our novelty checker achieves approximately 13% higher agreement than existing approaches. Ablation studies further showcases the importance of the facet-based re-ranker in identifying the most relevant literature for novelty evaluation.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.22026v1",
    "published_date": "2025-06-27 08:47:28 UTC",
    "updated_date": "2025-06-27 08:47:28 UTC"
  },
  {
    "arxiv_id": "2506.22526v1",
    "title": "Correlated Mutations for Integer Programming",
    "authors": [
      "Ofer M. Shir",
      "Michael Emmerich"
    ],
    "abstract": "Even with the recent theoretical advancements that dramatically reduced the complexity of Integer Programming (IP), heuristics remain the dominant problem-solvers for this difficult category. This study seeks to establish the groundwork for Integer Evolution Strategies (IESs), a class of randomized search heuristics inherently designed for continuous spaces. IESs already excel in treating IP in practice, but accomplish it via discretization and by applying sophisticated patches to their continuous operators, while persistently using the $\\ell_2$-norm as their operation pillar. We lay foundations for discrete search, by adopting the $\\ell_1$-norm, accounting for the suitable step-size, and questioning alternative measures to quantify correlations over the integer lattice. We focus on mutation distributions for unbounded integer decision variables. We briefly discuss a couple of candidate discrete probabilities induced by the uniform and binomial distributions, which we show to possess less appealing theoretical properties, and then narrow down to the Truncated Normal (TN) and Double Geometric (DG) distributions. We explore their theoretical properties, including entropy functions, and propose a procedure to generate scalable correlated mutation distributions. Our investigations are accompanied by extensive numerical simulations, which consistently support the claim that the DG distribution is better suited for unbounded integer search. We link our theoretical perspective to empirical evidence indicating that an IES with correlated DG mutations outperformed other strategies over non-separable quadratic IP. We conclude that while the replacement of the default TN distribution by the DG is theoretically justified and practically beneficial, the truly crucial change lies in adopting the $\\ell_1$-norm over the $\\ell_2$-norm.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.22526v1",
    "published_date": "2025-06-27 08:24:15 UTC",
    "updated_date": "2025-06-27 08:24:15 UTC"
  },
  {
    "arxiv_id": "2506.22008v1",
    "title": "TROFI: Trajectory-Ranked Offline Inverse Reinforcement Learning",
    "authors": [
      "Alessandro Sestini",
      "Joakim Bergdahl",
      "Konrad Tollmar",
      "Andrew D. Bagdanov",
      "Linus GisslÃ©n"
    ],
    "abstract": "In offline reinforcement learning, agents are trained using only a fixed set of stored transitions derived from a source policy. However, this requires that the dataset be labeled by a reward function. In applied settings such as video game development, the availability of the reward function is not always guaranteed. This paper proposes Trajectory-Ranked OFfline Inverse reinforcement learning (TROFI), a novel approach to effectively learn a policy offline without a pre-defined reward function. TROFI first learns a reward function from human preferences, which it then uses to label the original dataset making it usable for training the policy. In contrast to other approaches, our method does not require optimal trajectories. Through experiments on the D4RL benchmark we demonstrate that TROFI consistently outperforms baselines and performs comparably to using the ground truth reward to learn policies. Additionally, we validate the efficacy of our method in a 3D game environment. Our studies of the reward model highlight the importance of the reward function in this setting: we show that to ensure the alignment of a value function to the actual future discounted reward, it is fundamental to have a well-engineered and easy-to-learn reward function.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at Reinforcement Learning and Video Games Workshop at RLC 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.22008v1",
    "published_date": "2025-06-27 08:22:41 UTC",
    "updated_date": "2025-06-27 08:22:41 UTC"
  },
  {
    "arxiv_id": "2506.22005v1",
    "title": "LeanConjecturer: Automatic Generation of Mathematical Conjectures for Theorem Proving",
    "authors": [
      "Naoto Onda",
      "Kazumi Kasaura",
      "Yuta Oriike",
      "Masaya Taniguchi",
      "Akiyoshi Sannai",
      "Sho Sonoda"
    ],
    "abstract": "We introduce LeanConjecturer, a pipeline for automatically generating university-level mathematical conjectures in Lean 4 using Large Language Models (LLMs). Our hybrid approach combines rule-based context extraction with LLM-based theorem statement generation, addressing the data scarcity challenge in formal theorem proving. Through iterative generation and evaluation, LeanConjecturer produced 12,289 conjectures from 40 Mathlib seed files, with 3,776 identified as syntactically valid and non-trivial, that is, cannot be proven by \\texttt{aesop} tactic. We demonstrate the utility of these generated conjectures for reinforcement learning through Group Relative Policy Optimization (GRPO), showing that targeted training on domain-specific conjectures can enhance theorem proving capabilities. Our approach generates 103.25 novel conjectures per seed file on average, providing a scalable solution for creating training data for theorem proving systems. Our system successfully verified several non-trivial theorems in topology, including properties of semi-open, alpha-open, and pre-open sets, demonstrating its potential for mathematical discovery beyond simple variations of existing results.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 4 figures, 5 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.22005v1",
    "published_date": "2025-06-27 08:17:18 UTC",
    "updated_date": "2025-06-27 08:17:18 UTC"
  },
  {
    "arxiv_id": "2506.21997v2",
    "title": "Binned semiparametric Bayesian networks",
    "authors": [
      "Rafael Sojo",
      "Javier DÃ­az-Rozo",
      "Concha Bielza",
      "Pedro LarraÃ±aga"
    ],
    "abstract": "This paper introduces a new type of probabilistic semiparametric model that takes advantage of data binning to reduce the computational cost of kernel density estimation in nonparametric distributions. Two new conditional probability distributions are developed for the new binned semiparametric Bayesian networks, the sparse binned kernel density estimation and the Fourier kernel density estimation. These two probability distributions address the curse of dimensionality, which typically impacts binned models, by using sparse tensors and restricting the number of parent nodes in conditional probability calculations. To evaluate the proposal, we perform a complexity analysis and conduct several comparative experiments using synthetic data and datasets from the UCI Machine Learning repository. The experiments include different binning rules, parent restrictions, grid sizes, and number of instances to get a holistic view of the model's behavior. As a result, our binned semiparametric Bayesian networks achieve structural learning and log-likelihood estimations with no statistically significant differences compared to the semiparametric Bayesian networks, but at a much higher speed. Thus, the new binned semiparametric Bayesian networks prove to be a reliable and more efficient alternative to their non-binned counterparts.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to Information Sciences",
    "pdf_url": "https://arxiv.org/pdf/2506.21997v2",
    "published_date": "2025-06-27 08:07:34 UTC",
    "updated_date": "2025-07-01 09:17:43 UTC"
  },
  {
    "arxiv_id": "2506.21996v2",
    "title": "AlphaBeta is not as good as you think: a simple random games model for a better analysis of deterministic game-solving algorithms",
    "authors": [
      "RaphaÃ«l Boige",
      "Amine Boumaza",
      "Bruno Scherrer"
    ],
    "abstract": "Deterministic game-solving algorithms are conventionally analyzed in the light of their average-case complexity against a distribution of random game-trees, where leaf values are independently sampled from a fixed distribution. This simplified model enables uncluttered mathematical analysis, revealing two key properties: root value distributions asymptotically collapse to a single fixed value for finite-valued trees, and all reasonable algorithms achieve global optimality. However, these findings are artifacts of the model's design: its long criticized independence assumption strips games of structural complexity, producing trivial instances where no algorithm faces meaningful challenges. To address this limitation, we introduce a simple probabilistic model that incrementally constructs game-trees using a fixed level-wise conditional distribution. By enforcing ancestor dependencies, a critical structural feature of real-world games, our framework generates problems with adjustable difficulty while retaining some form of analytical tractability. For several algorithms, including AlphaBeta and Scout, we derive recursive formulas characterizing their average-case complexities under this model. These allow us to rigorously compare algorithms on deep game-trees, where Monte-Carlo simulations are no longer feasible. While asymptotically, all algorithms seem to converge to identical branching factor (a result analogous to that of independence-based models), deep finite trees reveal stark differences: AlphaBeta incurs a significantly larger constant multiplicative factor compared to algorithms like Scout, leading to a substantial practical slowdown. Our framework sheds new light on classical game-solving algorithms, offering rigorous evidence and analytical tools to advance the understanding of these methods under a richer, more challenging, and yet tractable model.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.21996v2",
    "published_date": "2025-06-27 08:07:17 UTC",
    "updated_date": "2025-11-24 07:52:48 UTC"
  },
  {
    "arxiv_id": "2506.21990v1",
    "title": "Analyzing and Fine-Tuning Whisper Models for Multilingual Pilot Speech Transcription in the Cockpit",
    "authors": [
      "Kartheek Kumar Reddy Nareddy",
      "Sarah Ternus",
      "Julia Niebling"
    ],
    "abstract": "The developments in transformer encoder-decoder architectures have led to significant breakthroughs in machine translation, Automatic Speech Recognition (ASR), and instruction-based chat machines, among other applications. The pre-trained models were trained on vast amounts of generic data over a few epochs (fewer than five in most cases), resulting in their strong generalization capabilities. Nevertheless, the performance of these models does suffer when applied to niche domains like transcribing pilot speech in the cockpit, which involves a lot of specific vocabulary and multilingual conversations. This paper investigates and improves the transcription accuracy of cockpit conversations with Whisper models. We have collected around 85 minutes of cockpit simulator recordings and 130 minutes of interview recordings with pilots and manually labeled them. The speakers are middle aged men speaking both German and English. To improve the accuracy of transcriptions, we propose multiple normalization schemes to refine the transcripts and improve Word Error Rate (WER). We then employ fine-tuning to enhance ASR performance, utilizing performance-efficient fine-tuning with Low-Rank Adaptation (LoRA). Hereby, WER decreased from 68.49 \\% (pretrained whisper Large model without normalization baseline) to 26.26\\% (finetuned whisper Large model with the proposed normalization scheme).",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "Computer Vision and Pattern Recognition (CVPR) 2025 Workshops",
    "pdf_url": "https://arxiv.org/pdf/2506.21990v1",
    "published_date": "2025-06-27 07:57:13 UTC",
    "updated_date": "2025-06-27 07:57:13 UTC"
  },
  {
    "arxiv_id": "2506.21976v1",
    "title": "SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model",
    "authors": [
      "Shuhan Tan",
      "John Lambert",
      "Hong Jeon",
      "Sakshum Kulshrestha",
      "Yijing Bai",
      "Jing Luo",
      "Dragomir Anguelov",
      "Mingxing Tan",
      "Chiyu Max Jiang"
    ],
    "abstract": "The goal of traffic simulation is to augment a potentially limited amount of manually-driven miles that is available for testing and validation, with a much larger amount of simulated synthetic miles. The culmination of this vision would be a generative simulated city, where given a map of the city and an autonomous vehicle (AV) software stack, the simulator can seamlessly simulate the trip from point A to point B by populating the city around the AV and controlling all aspects of the scene, from animating the dynamic agents (e.g., vehicles, pedestrians) to controlling the traffic light states. We refer to this vision as CitySim, which requires an agglomeration of simulation technologies: scene generation to populate the initial scene, agent behavior modeling to animate the scene, occlusion reasoning, dynamic scene generation to seamlessly spawn and remove agents, and environment simulation for factors such as traffic lights. While some key technologies have been separately studied in various works, others such as dynamic scene generation and environment simulation have received less attention in the research community. We propose SceneDiffuser++, the first end-to-end generative world model trained on a single loss function capable of point A-to-B simulation on a city scale integrating all the requirements above. We demonstrate the city-scale traffic simulation capability of SceneDiffuser++ and study its superior realism under long simulation conditions. We evaluate the simulation quality on an augmented version of the Waymo Open Motion Dataset (WOMD) with larger map regions to support trip-level simulation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.MA",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to CVPR 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.21976v1",
    "published_date": "2025-06-27 07:35:04 UTC",
    "updated_date": "2025-06-27 07:35:04 UTC"
  },
  {
    "arxiv_id": "2506.21972v1",
    "title": "Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM Vulnerabilities and Bypassing Modern Defenses",
    "authors": [
      "Mohamed Ahmed",
      "Mohamed Abdelmouty",
      "Mingyu Kim",
      "Gunvanth Kandula",
      "Alex Park",
      "James C. Davis"
    ],
    "abstract": "The advancement of Pre-Trained Language Models (PTLMs) and Large Language Models (LLMs) has led to their widespread adoption across diverse applications. Despite their success, these models remain vulnerable to attacks that exploit their inherent weaknesses to bypass safety measures. Two primary inference-phase threats are token-level and prompt-level jailbreaks. Token-level attacks embed adversarial sequences that transfer well to black-box models like GPT but leave detectable patterns and rely on gradient-based token optimization, whereas prompt-level attacks use semantically structured inputs to elicit harmful responses yet depend on iterative feedback that can be unreliable. To address the complementary limitations of these methods, we propose two hybrid approaches that integrate token- and prompt-level techniques to enhance jailbreak effectiveness across diverse PTLMs. GCG + PAIR and the newly explored GCG + WordGame hybrids were evaluated across multiple Vicuna and Llama models. GCG + PAIR consistently raised attack-success rates over its constituent techniques on undefended models; for instance, on Llama-3, its Attack Success Rate (ASR) reached 91.6%, a substantial increase from PAIR's 58.4% baseline. Meanwhile, GCG + WordGame matched the raw performance of WordGame maintaining a high ASR of over 80% even under stricter evaluators like Mistral-Sorry-Bench. Crucially, both hybrids retained transferability and reliably pierced advanced defenses such as Gradient Cuff and JBShield, which fully blocked single-mode attacks. These findings expose previously unreported vulnerabilities in current safety stacks, highlight trade-offs between raw success and defensive robustness, and underscore the need for holistic safeguards against adaptive adversaries.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.21972v1",
    "published_date": "2025-06-27 07:26:33 UTC",
    "updated_date": "2025-06-27 07:26:33 UTC"
  },
  {
    "arxiv_id": "2508.13157v2",
    "title": "Image2Net: Datasets, Benchmark and Hybrid Framework to Convert Analog Circuit Diagrams into Netlists",
    "authors": [
      "Haohang Xu",
      "Chengjie Liu",
      "Qihang Wang",
      "Wenhao Huang",
      "Yongjian Xu",
      "Weiyu Chen",
      "Anlan Peng",
      "Zhijun Li",
      "Bo Li",
      "Lei Qi",
      "Jun Yang",
      "Yuan Du",
      "Li Du"
    ],
    "abstract": "Large Language Model (LLM) exhibits great potential in designing of analog integrated circuits (IC) because of its excellence in abstraction and generalization for knowledge. However, further development of LLM-based analog ICs heavily relies on textual description of analog ICs, while existing analog ICs are mostly illustrated in image-based circuit diagrams rather than text-based netlists. Converting circuit diagrams to netlists help LLMs to enrich the knowledge of analog IC. Nevertheless, previously proposed conversion frameworks face challenges in further application because of limited support of image styles and circuit elements. Up to now, it still remains a challenging task to effectively convert complex circuit diagrams into netlists. To this end, this paper constructs and opensources a new dataset with rich styles of circuit diagrams as well as balanced distribution of simple and complex analog ICs. And a hybrid framework, named Image2Net, is proposed for practical conversion from circuit diagrams to netlists. The netlist edit distance (NED) is also introduced to precisely assess the difference between the converted netlists and ground truth. Based on our benchmark, Image2Net achieves 80.77% successful rate, which is 34.62%-45.19% higher than previous works. Specifically, the proposed work shows 0.116 averaged NED, which is 62.1%-69.6% lower than state-of-the-arts. Our datasets and benchmark are available at https://github.com/LAD021/ci2n_datasets.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "cs.AR",
    "comment": "10 pages, 12 figures, 6 tables",
    "pdf_url": "https://arxiv.org/pdf/2508.13157v2",
    "published_date": "2025-06-27 07:15:00 UTC",
    "updated_date": "2025-12-08 10:57:28 UTC"
  },
  {
    "arxiv_id": "2506.21964v1",
    "title": "Using Large Language Models to Suggest Informative Prior Distributions in Bayesian Statistics",
    "authors": [
      "Michael A. Riegler",
      "Kristoffer Herland Hellton",
      "Vajira Thambawita",
      "Hugo L. Hammer"
    ],
    "abstract": "Selecting prior distributions in Bayesian statistics is challenging, resource-intensive, and subjective. We analyze using large-language models (LLMs) to suggest suitable, knowledge-based informative priors. We developed an extensive prompt asking LLMs not only to suggest priors but also to verify and reflect on their choices.\n  We evaluated Claude Opus, Gemini 2.5 Pro, and ChatGPT-4o-mini on two real datasets: heart disease risk and concrete strength. All LLMs correctly identified the direction for all associations (e.g., that heart disease risk is higher for males). The quality of suggested priors was measured by their Kullback-Leibler divergence from the maximum likelihood estimator's distribution.\n  The LLMs suggested both moderately and weakly informative priors. The moderate priors were often overconfident, resulting in distributions misaligned with the data. In our experiments, Claude and Gemini provided better priors than ChatGPT. For weakly informative priors, a key performance difference emerged: ChatGPT and Gemini defaulted to an \"unnecessarily vague\" mean of 0, while Claude did not, demonstrating a significant advantage.\n  The ability of LLMs to identify correct associations shows their great potential as an efficient, objective method for developing informative priors. However, the primary challenge remains in calibrating the width of these priors to avoid over- and under-confidence.",
    "categories": [
      "stat.ME",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "stat.ME",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.21964v1",
    "published_date": "2025-06-27 07:11:55 UTC",
    "updated_date": "2025-06-27 07:11:55 UTC"
  },
  {
    "arxiv_id": "2507.00061v1",
    "title": "Smooth-Distill: A Self-distillation Framework for Multitask Learning with Wearable Sensor Data",
    "authors": [
      "Hoang-Dieu Vu",
      "Duc-Nghia Tran",
      "Quang-Tu Pham",
      "Hieu H. Pham",
      "Nicolas Vuillerme",
      "Duc-Tan Tran"
    ],
    "abstract": "This paper introduces Smooth-Distill, a novel self-distillation framework designed to simultaneously perform human activity recognition (HAR) and sensor placement detection using wearable sensor data. The proposed approach utilizes a unified CNN-based architecture, MTL-net, which processes accelerometer data and branches into two outputs for each respective task. Unlike conventional distillation methods that require separate teacher and student models, the proposed framework utilizes a smoothed, historical version of the model itself as the teacher, significantly reducing training computational overhead while maintaining performance benefits. To support this research, we developed a comprehensive accelerometer-based dataset capturing 12 distinct sleep postures across three different wearing positions, complementing two existing public datasets (MHealth and WISDM). Experimental results show that Smooth-Distill consistently outperforms alternative approaches across different evaluation scenarios, achieving notable improvements in both human activity recognition and device placement detection tasks. This method demonstrates enhanced stability in convergence patterns during training and exhibits reduced overfitting compared to traditional multitask learning baselines. This framework contributes to the practical implementation of knowledge distillation in human activity recognition systems, offering an effective solution for multitask learning with accelerometer data that balances accuracy and training efficiency. More broadly, it reduces the computational cost of model training, which is critical for scenarios requiring frequent model updates or training on resource-constrained platforms. The code and model are available at https://github.com/Kuan2vn/smooth\\_distill.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00061v1",
    "published_date": "2025-06-27 06:51:51 UTC",
    "updated_date": "2025-06-27 06:51:51 UTC"
  },
  {
    "arxiv_id": "2506.21945v1",
    "title": "SDRNET: Stacked Deep Residual Network for Accurate Semantic Segmentation of Fine-Resolution Remotely Sensed Images",
    "authors": [
      "Naftaly Wambugu",
      "Ruisheng Wang",
      "Bo Guo",
      "Tianshu Yu",
      "Sheng Xu",
      "Mohammed Elhassan"
    ],
    "abstract": "Land cover maps generated from semantic segmentation of high-resolution remotely sensed images have drawn mucon in the photogrammetry and remote sensing research community. Currently, massive fine-resolution remotely sensed (FRRS) images acquired by improving sensing and imaging technologies become available. However, accurate semantic segmentation of such FRRS images is greatly affected by substantial class disparities, the invisibility of key ground objects due to occlusion, and object size variation. Despite the extraordinary potential in deep convolutional neural networks (DCNNs) in image feature learning and representation, extracting sufficient features from FRRS images for accurate semantic segmentation is still challenging. These challenges demand the deep learning models to learn robust features and generate sufficient feature descriptors. Specifically, learning multi-contextual features to guarantee adequate coverage of varied object sizes from the ground scene and harnessing global-local contexts to overcome class disparities challenge even profound networks. Deeper networks significantly lose spatial details due to gradual downsampling processes resulting in poor segmentation results and coarse boundaries. This article presents a stacked deep residual network (SDRNet) for semantic segmentation from FRRS images. The proposed framework utilizes two stacked encoder-decoder networks to harness long-range semantics yet preserve spatial information and dilated residual blocks (DRB) between each encoder and decoder network to capture sufficient global dependencies thus improving segmentation performance. Our experimental results obtained using the ISPRS Vaihingen and Potsdam datasets demonstrate that the SDRNet performs effectively and competitively against current DCNNs in semantic segmentation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.21945v1",
    "published_date": "2025-06-27 06:40:30 UTC",
    "updated_date": "2025-06-27 06:40:30 UTC"
  },
  {
    "arxiv_id": "2506.21931v2",
    "title": "ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation",
    "authors": [
      "Reza Yousefi Maragheh",
      "Pratheek Vadla",
      "Priyank Gupta",
      "Kai Zhao",
      "Aysenur Inan",
      "Kehui Yao",
      "Jianpeng Xu",
      "Praveen Kanumala",
      "Jason Cho",
      "Sushant Kumar"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) has shown promise in enhancing recommendation systems by incorporating external context into large language model prompts. However, existing RAG-based approaches often rely on static retrieval heuristics and fail to capture nuanced user preferences in dynamic recommendation scenarios. In this work, we introduce ARAG, an Agentic Retrieval-Augmented Generation framework for Personalized Recommendation, which integrates a multi-agent collaboration mechanism into the RAG pipeline. To better understand the long-term and session behavior of the user, ARAG leverages four specialized LLM-based agents: a User Understanding Agent that summarizes user preferences from long-term and session contexts, a Natural Language Inference (NLI) Agent that evaluates semantic alignment between candidate items retrieved by RAG and inferred intent, a context summary agent that summarizes the findings of NLI agent, and an Item Ranker Agent that generates a ranked list of recommendations based on contextual fit. We evaluate ARAG accross three datasets. Experimental results demonstrate that ARAG significantly outperforms standard RAG and recency-based baselines, achieving up to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an ablation study to analyse the effect by different components of ARAG. Our findings highlight the effectiveness of integrating agentic reasoning into retrieval-augmented recommendation and provide new directions for LLM-based personalization.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.21931v2",
    "published_date": "2025-06-27 05:45:59 UTC",
    "updated_date": "2025-08-11 16:24:36 UTC"
  },
  {
    "arxiv_id": "2507.02938v1",
    "title": "A Large Language Model-Empowered Agent for Reliable and Robust Structural Analysis",
    "authors": [
      "Jiachen Liu",
      "Ziheng Geng",
      "Ran Cao",
      "Lu Cheng",
      "Paolo Bocchini",
      "Minghui Cheng"
    ],
    "abstract": "Large language models (LLMs) have exhibited remarkable capabilities across diverse open-domain tasks, yet their application in specialized domains such as civil engineering remains largely unexplored. This paper starts bridging this gap by evaluating and enhancing the reliability and robustness of LLMs in structural analysis of beams. Reliability is assessed through the accuracy of correct outputs under repetitive runs of the same problems, whereas robustness is evaluated via the performance across varying load and boundary conditions. A benchmark dataset, comprising eight beam analysis problems, is created to test the Llama-3.3 70B Instruct model. Results show that, despite a qualitative understanding of structural mechanics, the LLM lacks the quantitative reliability and robustness for engineering applications. To address these limitations, a shift is proposed that reframes the structural analysis as code generation tasks. Accordingly, an LLM-empowered agent is developed that (a) integrates chain-of-thought and few-shot prompting to generate accurate OpeeSeesPy code, and (b) automatically executes the code to produce structural analysis results. Experimental results demonstrate that the agent achieves accuracy exceeding 99.0% on the benchmark dataset, exhibiting reliable and robust performance across diverse conditions. Ablation studies highlight the complete example and function usage examples as the primary contributors to the agent's enhanced performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02938v1",
    "published_date": "2025-06-27 04:16:53 UTC",
    "updated_date": "2025-06-27 04:16:53 UTC"
  },
  {
    "arxiv_id": "2506.21892v1",
    "title": "SODA: Out-of-Distribution Detection in Domain-Shifted Point Clouds via Neighborhood Propagation",
    "authors": [
      "Adam Goodge",
      "Xun Xu",
      "Bryan Hooi",
      "Wee Siong Ng",
      "Jingyi Liao",
      "Yongyi Su",
      "Xulei Yang"
    ],
    "abstract": "As point cloud data increases in prevalence in a variety of applications, the ability to detect out-of-distribution (OOD) point cloud objects becomes critical for ensuring model safety and reliability. However, this problem remains under-explored in existing research. Inspired by success in the image domain, we propose to exploit advances in 3D vision-language models (3D VLMs) for OOD detection in point cloud objects. However, a major challenge is that point cloud datasets used to pre-train 3D VLMs are drastically smaller in size and object diversity than their image-based counterparts. Critically, they often contain exclusively computer-designed synthetic objects. This leads to a substantial domain shift when the model is transferred to practical tasks involving real objects scanned from the physical environment. In this paper, our empirical experiments show that synthetic-to-real domain shift significantly degrades the alignment of point cloud with their associated text embeddings in the 3D VLM latent space, hindering downstream performance. To address this, we propose a novel methodology called SODA which improves the detection of OOD point clouds through a neighborhood-based score propagation scheme. SODA is inference-based, requires no additional model training, and achieves state-of-the-art performance over existing approaches across datasets and problem settings.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.21892v1",
    "published_date": "2025-06-27 04:05:55 UTC",
    "updated_date": "2025-06-27 04:05:55 UTC"
  },
  {
    "arxiv_id": "2506.21887v1",
    "title": "Interactive Multi-Objective Probabilistic Preference Learning with Soft and Hard Bounds",
    "authors": [
      "Edward Chen",
      "Sang T. Truong",
      "Natalie Dullerud",
      "Sanmi Koyejo",
      "Carlos Guestrin"
    ],
    "abstract": "High-stakes decision-making involves navigating multiple competing objectives with expensive evaluations. For instance, in brachytherapy, clinicians must balance maximizing tumor coverage (e.g., an aspirational target or soft bound of >95% coverage) against strict organ dose limits (e.g., a non-negotiable hard bound of <601 cGy to the bladder), with each plan evaluation being resource-intensive. Selecting Pareto-optimal solutions that match implicit preferences is challenging, as exhaustive Pareto frontier exploration is computationally and cognitively prohibitive, necessitating interactive frameworks to guide users. While decision-makers (DMs) often possess domain knowledge to narrow the search via such soft-hard bounds, current methods often lack systematic approaches to iteratively refine these multi-faceted preference structures. Critically, DMs must trust their final decision, confident they haven't missed superior alternatives; this trust is paramount in high-consequence scenarios. We present Active-MoSH, an interactive local-global framework designed for this process. Its local component integrates soft-hard bounds with probabilistic preference learning, maintaining distributions over DM preferences and bounds for adaptive Pareto subset refinement. This is guided by an active sampling strategy optimizing exploration-exploitation while minimizing cognitive burden. To build DM trust, Active-MoSH's global component, T-MoSH, leverages multi-objective sensitivity analysis to identify potentially overlooked, high-value points beyond immediate feedback. We demonstrate Active-MoSH's performance benefits through diverse synthetic and real-world applications. A user study on AI-generated image selection further validates our hypotheses regarding the framework's ability to improve convergence, enhance DM trust, and provide expressive preference articulation, enabling more effective DMs.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.21887v1",
    "published_date": "2025-06-27 03:44:20 UTC",
    "updated_date": "2025-06-27 03:44:20 UTC"
  },
  {
    "arxiv_id": "2506.21884v2",
    "title": "UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields",
    "authors": [
      "Fabian Perez",
      "Sara Rojas",
      "Carlos Hinojosa",
      "Hoover Rueda-ChacÃ³n",
      "Bernard Ghanem"
    ],
    "abstract": "Neural Radiance Field (NeRF)-based segmentation methods focus on object semantics and rely solely on RGB data, lacking intrinsic material properties. This limitation restricts accurate material perception, which is crucial for robotics, augmented reality, simulation, and other applications. We introduce UnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling joint hyperspectral novel view synthesis and unsupervised material segmentation. Our method models spectral reflectance via diffuse and specular components, where a learned dictionary of global endmembers represents pure material signatures, and per-point abundances capture their distribution. For material segmentation, we use spectral signature predictions along learned endmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF enables scene editing by modifying learned endmember dictionaries for flexible material-based appearance manipulation. Extensive experiments validate our approach, demonstrating superior spectral reconstruction and material segmentation to existing methods. Project page: https://www.factral.co/UnMix-NeRF.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "eess.IV",
    "comment": "Paper accepted at ICCV 2025 main conference",
    "pdf_url": "https://arxiv.org/pdf/2506.21884v2",
    "published_date": "2025-06-27 03:42:49 UTC",
    "updated_date": "2025-08-06 07:17:05 UTC"
  },
  {
    "arxiv_id": "2506.21876v1",
    "title": "Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation",
    "authors": [
      "Qiyue Gao",
      "Xinyu Pi",
      "Kevin Liu",
      "Junrong Chen",
      "Ruolan Yang",
      "Xinqi Huang",
      "Xinyu Fang",
      "Lu Sun",
      "Gautham Kishore",
      "Bo Ai",
      "Stone Tao",
      "Mengyang Liu",
      "Jiaxi Yang",
      "Chao-Jung Lai",
      "Chuanyang Jin",
      "Jiannan Xiang",
      "Benhao Huang",
      "Zeming Chen",
      "David Danks",
      "Hao Su",
      "Tianmin Shu",
      "Ziqiao Ma",
      "Lianhui Qin",
      "Zhiting Hu"
    ],
    "abstract": "Internal world models (WMs) enable agents to understand the world's state and predict transitions, serving as the basis for advanced deliberative reasoning. Recent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and Gemini, exhibit potential as general-purpose WMs. While the latest studies have evaluated and shown limitations in specific capabilities such as visual understanding, a systematic evaluation of VLMs' fundamental WM abilities remains absent. Drawing on comparative psychology and cognitive science, we propose a two-stage framework that assesses Perception (visual, spatial, temporal, quantitative, and motion) and Prediction (mechanistic simulation, transitive inference, compositional inference) to provide an atomic evaluation of VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale benchmark comprising 23 fine-grained evaluation dimensions across 6 diverse simulated environments with controlled counterfactual simulations. Through 660 experiments on 15 latest commercial and open-source VLMs, we find that these models exhibit striking limitations in basic world modeling abilities. For instance, almost all models perform at near-random accuracy when distinguishing motion trajectories. Additionally, they lack disentangled understanding -- e.g., some models tend to believe blue objects move faster than green ones. More rich results and analyses reveal significant gaps between VLMs and human-level world modeling.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2025 (Findings)",
    "pdf_url": "https://arxiv.org/pdf/2506.21876v1",
    "published_date": "2025-06-27 03:24:29 UTC",
    "updated_date": "2025-06-27 03:24:29 UTC"
  },
  {
    "arxiv_id": "2506.21874v1",
    "title": "On the Feasibility of Poisoning Text-to-Image AI Models via Adversarial Mislabeling",
    "authors": [
      "Stanley Wu",
      "Ronik Bhaskar",
      "Anna Yoo Jeong Ha",
      "Shawn Shan",
      "Haitao Zheng",
      "Ben Y. Zhao"
    ],
    "abstract": "Today's text-to-image generative models are trained on millions of images sourced from the Internet, each paired with a detailed caption produced by Vision-Language Models (VLMs). This part of the training pipeline is critical for supplying the models with large volumes of high-quality image-caption pairs during training. However, recent work suggests that VLMs are vulnerable to stealthy adversarial attacks, where adversarial perturbations are added to images to mislead the VLMs into producing incorrect captions.\n  In this paper, we explore the feasibility of adversarial mislabeling attacks on VLMs as a mechanism to poisoning training pipelines for text-to-image models. Our experiments demonstrate that VLMs are highly vulnerable to adversarial perturbations, allowing attackers to produce benign-looking images that are consistently miscaptioned by the VLM models. This has the effect of injecting strong \"dirty-label\" poison samples into the training pipeline for text-to-image models, successfully altering their behavior with a small number of poisoned samples. We find that while potential defenses can be effective, they can be targeted and circumvented by adaptive attackers. This suggests a cat-and-mouse game that is likely to reduce the quality of training data and increase the cost of text-to-image model development. Finally, we demonstrate the real-world effectiveness of these attacks, achieving high attack success (over 73%) even in black-box scenarios against commercial VLMs (Google Vertex AI and Microsoft Azure).",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "ACM Conference on Computer and Communications Security 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.21874v1",
    "published_date": "2025-06-27 03:13:47 UTC",
    "updated_date": "2025-06-27 03:13:47 UTC"
  },
  {
    "arxiv_id": "2506.21873v1",
    "title": "Grounding-Aware Token Pruning: Recovering from Drastic Performance Drops in Visual Grounding Caused by Pruning",
    "authors": [
      "Tzu-Chun Chien",
      "Chieh-Kai Lin",
      "Shiang-Feng Tsai",
      "Ruei-Chi Lai",
      "Hung-Jen Chen",
      "Min Sun"
    ],
    "abstract": "Recent Multimodal Large Language Models (MLLMs) have demonstrated strong performance in visual grounding, establishing themselves as a general interface for various vision-language applications. This progress has driven the development of token pruning methods to mitigate the high computational costs associated with processing numerous visual tokens. However, we observe that pruning significantly weakens the model's grounding ability, leading to incorrect predictions and drastic performance degradation. In Referring Expression Comprehension (REC), for instance, pruning causes the accuracy of LLaVA on the RefCOCO validation set to drop from 56.14% to 15.34%. Our analysis identifies misaligned position IDs after pruning as the primary cause of this degradation, as both the order and value of these IDs are crucial for maintaining performance in grounding tasks. To address this issue, we propose Grounding-Aware Token Pruning (GAP), a simple yet effective adjustment to position IDs that recovers REC accuracy back to 51.42%, which is 90% of the original performance in the without pruning setting, all while requiring no additional training, memory, or computational overhead. Applied to models such as Shikra, MiniGPTv2, and the LLaVA series, our method consistently improves performance across various token pruning strategies.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.21873v1",
    "published_date": "2025-06-27 03:11:22 UTC",
    "updated_date": "2025-06-27 03:11:22 UTC"
  },
  {
    "arxiv_id": "2506.21872v1",
    "title": "A Survey of Continual Reinforcement Learning",
    "authors": [
      "Chaofan Pan",
      "Xin Yang",
      "Yanhua Li",
      "Wei Wei",
      "Tianrui Li",
      "Bo An",
      "Jiye Liang"
    ],
    "abstract": "Reinforcement Learning (RL) is an important machine learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in this field due to the rapid development of deep neural networks. However, the success of RL currently relies on extensive training data and computational resources. In addition, RL's limited ability to generalize across tasks restricts its applicability in dynamic and real-world environments. With the arisen of Continual Learning (CL), Continual Reinforcement Learning (CRL) has emerged as a promising research direction to address these limitations by enabling agents to learn continuously, adapt to new tasks, and retain previously acquired knowledge. In this survey, we provide a comprehensive examination of CRL, focusing on its core concepts, challenges, and methodologies. Firstly, we conduct a detailed review of existing works, organizing and analyzing their metrics, tasks, benchmarks, and scenario settings. Secondly, we propose a new taxonomy of CRL methods, categorizing them into four types from the perspective of knowledge storage and/or transfer. Finally, our analysis highlights the unique challenges of CRL and provides practical insights into future directions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This work has been submitted to the IEEE TPAMI",
    "pdf_url": "https://arxiv.org/pdf/2506.21872v1",
    "published_date": "2025-06-27 03:10:20 UTC",
    "updated_date": "2025-06-27 03:10:20 UTC"
  },
  {
    "arxiv_id": "2506.21864v3",
    "title": "DeepOmni: Towards Seamless and Smart Speech Interaction with Adaptive Modality-Specific MoE",
    "authors": [
      "Hang Shao",
      "Heting Gao",
      "Yunhang Shen",
      "Jiawei Chen",
      "Zuwei Long",
      "Dong Yang",
      "Ke Li",
      "Xing Sun"
    ],
    "abstract": "Native multimodal large language models (MLLMs) restructure a single large language model (LLM) into a spoken language model (SLM) capable of both speech and text generation. Compared to modular and aligned MLLMs, native MLLMs preserve richer paralinguistic features such as emotion and prosody, and generate speech responses directly within the backbone LLM rather than using a separate speech decoder. This integration also results in lower response latency and smoother interaction. However, native MLLMs suffer from catastrophic forgetting and performance degradation because the available paired speech-text data is insufficient to support the pretraining of MLLMs compared to the vast amount of text data required to pretrain text LLMs. To address this issue, we propose DeepTalk, a framework for adaptive modality expert learning based on a Mixture of Experts (MoE) architecture. DeepTalk first adaptively distinguishes modality experts according to their modality load within the LLM. Each modality expert then undergoes specialized single-modality training, followed by joint multimodal collaborative training. As a result, DeepTalk incurs only a 5.5% performance drop compared to the original LLM, which is significantly lower than the average performance drop of over 20% typically seen in native MLLMs (such as GLM-4-Voice), and is on par with modular MLLMs. Meanwhile, the end-to-end dialogue latency remains within 0.5 seconds, ensuring a seamless and intelligent speech interaction experience. Code and models are released at https://github.com/talkking/DeepTalk.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Under Review",
    "pdf_url": "https://arxiv.org/pdf/2506.21864v3",
    "published_date": "2025-06-27 02:32:04 UTC",
    "updated_date": "2025-10-27 08:52:21 UTC"
  },
  {
    "arxiv_id": "2506.21862v1",
    "title": "LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs",
    "authors": [
      "Boyuan Sun",
      "Jiaxing Zhao",
      "Xihan Wei",
      "Qibin Hou"
    ],
    "abstract": "In this paper, we present LLaVA-Scissor, a training-free token compression strategy designed for video multimodal large language models. Previous methods mostly attempt to compress tokens based on attention scores, but fail to effectively capture all semantic regions and often lead to token redundancy. Differently, we propose to leverage the Semantic Connected Components (SCC) approach that assigns tokens to distinct semantic regions within the token set, ensuring comprehensive semantic coverage. The outcome is a two-step spatio-temporal token compression strategy that utilizes SCC in both spatial and temporal domains. This strategy can effectively compress tokens by representing the entire video with a set of non-overlapping semantic tokens. We conduct extensive evaluations of the token compression capabilities of LLaVA-Scissor across diverse video understanding benchmarks, including video question answering, long video understanding, and comprehensive multi-choices benchmarks. Experimental results show that the proposed LLaVA-Scissor outperforms other token compression methods, achieving superior performance in various video understanding benchmarks, particularly at low token retention ratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "21 pages, 4 figures, 7 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.21862v1",
    "published_date": "2025-06-27 02:29:58 UTC",
    "updated_date": "2025-06-27 02:29:58 UTC"
  },
  {
    "arxiv_id": "2506.21857v2",
    "title": "SPADE: Spatial Transcriptomics and Pathology Alignment Using a Mixture of Data Experts for an Expressive Latent Space",
    "authors": [
      "Ekaterina Redekop",
      "Mara Pleasure",
      "Zichen Wang",
      "Kimberly Flores",
      "Anthony Sisk",
      "William Speier",
      "Corey W. Arnold"
    ],
    "abstract": "The rapid growth of digital pathology and advances in self-supervised deep learning have enabled the development of foundational models for various pathology tasks across diverse diseases. While multimodal approaches integrating diverse data sources have emerged, a critical gap remains in the comprehensive integration of whole-slide images (WSIs) with spatial transcriptomics (ST), which is crucial for capturing critical molecular heterogeneity beyond standard hematoxylin & eosin (H&E) staining. We introduce SPADE, a foundation model that integrates histopathology with ST data to guide image representation learning within a unified framework, in effect creating an ST-informed latent space. SPADE leverages a mixture-of-data experts technique, where experts are created via two-stage imaging feature-space clustering using contrastive learning to learn representations of co-registered WSI patches and gene expression profiles. Pre-trained on the comprehensive HEST-1k dataset, SPADE is evaluated on 20 downstream tasks, demonstrating significantly superior few-shot performance compared to baseline models, highlighting the benefits of integrating morphological and molecular information into one latent space. Code and pretrained weights are available at https://github.com/uclabair/SPADE.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.21857v2",
    "published_date": "2025-06-27 02:20:51 UTC",
    "updated_date": "2025-10-13 23:34:26 UTC"
  },
  {
    "arxiv_id": "2506.21849v1",
    "title": "The Consistency Hypothesis in Uncertainty Quantification for Large Language Models",
    "authors": [
      "Quan Xiao",
      "Debarun Bhattacharjya",
      "Balaji Ganesan",
      "Radu Marinescu",
      "Katsiaryna Mirylenka",
      "Nhan H Pham",
      "Michael Glass",
      "Junkyu Lee"
    ],
    "abstract": "Estimating the confidence of large language model (LLM) outputs is essential for real-world applications requiring high user trust. Black-box uncertainty quantification (UQ) methods, relying solely on model API access, have gained popularity due to their practical benefits. In this paper, we examine the implicit assumption behind several UQ methods, which use generation consistency as a proxy for confidence, an idea we formalize as the consistency hypothesis. We introduce three mathematical statements with corresponding statistical tests to capture variations of this hypothesis and metrics to evaluate LLM output conformity across tasks. Our empirical investigation, spanning 8 benchmark datasets and 3 tasks (question answering, text summarization, and text-to-SQL), highlights the prevalence of the hypothesis under different settings. Among the statements, we highlight the `Sim-Any' hypothesis as the most actionable, and demonstrate how it can be leveraged by proposing data-free black-box UQ methods that aggregate similarities between generations for confidence estimation. These approaches can outperform the closest baselines, showcasing the practical value of the empirically observed consistency hypothesis.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by The Conference on Uncertainty in Artificial Intelligence (UAI) 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.21849v1",
    "published_date": "2025-06-27 01:53:15 UTC",
    "updated_date": "2025-06-27 01:53:15 UTC"
  },
  {
    "arxiv_id": "2506.21845v1",
    "title": "3Description: An Intuitive Human-AI Collaborative 3D Modeling Approach",
    "authors": [
      "Zhuodi Cai"
    ],
    "abstract": "This paper presents 3Description, an experimental human-AI collaborative approach for intuitive 3D modeling. 3Description aims to address accessibility and usability challenges in traditional 3D modeling by enabling non-professional individuals to co-create 3D models using verbal and gesture descriptions. Through a combination of qualitative research, product analysis, and user testing, 3Description integrates AI technologies such as Natural Language Processing and Computer Vision, powered by OpenAI and MediaPipe. Recognizing the web has wide cross-platform capabilities, 3Description is web-based, allowing users to describe the desired model and subsequently adjust its components using verbal and gestural inputs. In the era of AI and emerging media, 3Description not only contributes to a more inclusive and user-friendly design process, empowering more people to participate in the construction of the future 3D world, but also strives to increase human engagement in co-creation with AI, thereby avoiding undue surrender to technology and preserving human creativity.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.GR"
    ],
    "primary_category": "cs.HC",
    "comment": "5 pages, 2 figures, 3 tables (containing 21 subfigures)",
    "pdf_url": "https://arxiv.org/pdf/2506.21845v1",
    "published_date": "2025-06-27 01:33:46 UTC",
    "updated_date": "2025-06-27 01:33:46 UTC"
  },
  {
    "arxiv_id": "2506.21840v1",
    "title": "PARSI: Persian Authorship Recognition via Stylometric Integration",
    "authors": [
      "Kourosh Shahnazari",
      "Mohammadali Keshtparvar",
      "Seyed Moein Ayyoubzadeh"
    ],
    "abstract": "The intricate linguistic, stylistic, and metrical aspects of Persian classical poetry pose a challenge for computational authorship attribution. In this work, we present a versatile framework to determine authorship among 67 prominent poets. We employ a multi-input neural framework consisting of a transformer-based language encoder complemented by features addressing the semantic, stylometric, and metrical dimensions of Persian poetry. Our feature set encompasses 100-dimensional Word2Vec embeddings, seven stylometric measures, and categorical encodings of poetic form and meter. We compiled a vast corpus of 647,653 verses of the Ganjoor digital collection, validating the data through strict preprocessing and author verification while preserving poem-level splitting to prevent overlap. This work employs verse-level classification and majority and weighted voting schemes in evaluation, revealing that weighted voting yields 71% accuracy. We further investigate threshold-based decision filtering, allowing the model to generate highly confident predictions, achieving 97% accuracy at a 0.9 threshold, though at lower coverage. Our work focuses on the integration of deep representational forms with domain-specific features for improved authorship attribution. The results illustrate the potential of our approach for automated classification and the contribution to stylistic analysis, authorship disputes, and general computational literature research. This research will facilitate further research on multilingual author attribution, style shift, and generative modeling of Persian poetry.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.21840v1",
    "published_date": "2025-06-27 01:08:52 UTC",
    "updated_date": "2025-06-27 01:08:52 UTC"
  },
  {
    "arxiv_id": "2506.21826v1",
    "title": "Few-Shot Segmentation of Historical Maps via Linear Probing of Vision Foundation Models",
    "authors": [
      "Rafael Sterzinger",
      "Marco Peer",
      "Robert Sablatnig"
    ],
    "abstract": "As rich sources of history, maps provide crucial insights into historical changes, yet their diverse visual representations and limited annotated data pose significant challenges for automated processing. We propose a simple yet effective approach for few-shot segmentation of historical maps, leveraging the rich semantic embeddings of large vision foundation models combined with parameter-efficient fine-tuning. Our method outperforms the state-of-the-art on the Siegfried benchmark dataset in vineyard and railway segmentation, achieving +5% and +13% relative improvements in mIoU in 10-shot scenarios and around +20% in the more challenging 5-shot setting. Additionally, it demonstrates strong performance on the ICDAR 2021 competition dataset, attaining a mean PQ of 67.3% for building block segmentation, despite not being optimized for this shape-sensitive metric, underscoring its generalizability. Notably, our approach maintains high performance even in extremely low-data regimes (10- & 5-shot), while requiring only 689k trainable parameters - just 0.21% of the total model size. Our approach enables precise segmentation of diverse historical maps while drastically reducing the need for manual annotations, advancing automated processing and analysis in the field. Our implementation is publicly available at: https://github.com/RafaelSterzinger/few-shot-map-segmentation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages, accepted at ICDAR2025",
    "pdf_url": "https://arxiv.org/pdf/2506.21826v1",
    "published_date": "2025-06-27 00:07:21 UTC",
    "updated_date": "2025-06-27 00:07:21 UTC"
  }
]
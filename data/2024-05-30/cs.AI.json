{
  "date": "2024-05-30",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-05-30 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 156 篇论文，主要聚焦 AI 领域，尤其是大型语言模型（LLMs）的优化、安全性和应用扩展，令人印象深刻的是关于 LLM 推理微调和生成模型的创新方法，如 Xinlu Zhang 等人的 LLM 推理研究，以及一些高效算法和多模态模型的进展。\n\n今天的核心论文多围绕 LLMs 和机器学习主题，我将优先讨论最具影响力和创新性的几篇，包括 LLM 微调、生成模型优化和强化学习领域，同时快速掠过其他较常规的论文。以下是精选摘要，按主题分组整理。\n\n### LLM 微调与推理优化\n- **Unveiling the Impact of Coding Data Instruction Fine-Tuning on Large Language Models Reasoning**（揭示编码数据指令微调对大型语言模型推理的影响）  \n  这篇论文由 Xinlu Zhang 等作者发布，探讨了在指令微调（IFT）阶段使用编码数据如何提升 LLMs 的推理能力。通过实验分析不同比例的编码数据对模型性能的影响，主要发现是编码数据能增强整体推理能力，并在特定任务中提供可比性收益。该工作为 LLM 微调提供宝贵见解，强调任务依赖的优化策略。\n\n- **Grokfast: Accelerated Grokking by Amplifying Slow Gradients**（Grokfast：通过放大慢梯度加速 Grokking）  \n  Jaerin Lee 等研究了 LLM 在复杂任务中的“Grokking”现象（延迟泛化），提出通过放大慢梯度来加速训练。主要贡献是显著提高训练效率，实验显示在多种数据集上性能提升 50 倍以上。该论文突显 LLM 训练机制的创新潜力。\n\n- **Improving SMOTE via Fusing Conditional VAE for Data-adaptive Noise Filtering**（通过融合条件 VAE 改进 SMOTE 以实现数据自适应噪声过滤）  \n  这篇工作优化了 SMOTE 算法，用于处理类别不平衡问题。作者引入条件变分自编码器（VAE）进行噪声过滤，主要发现是显著提升了小样本分类性能，实验证明在深度学习模型上精度更高。该方法对实际数据处理有实用价值。\n\n### 生成模型与图像处理\n- **Slight Corruption in Pre-training Data Makes Better Diffusion Models**（预训练数据中的轻微噪声能提升扩散模型性能）  \n  Hao Chen 等发现，在扩散模型（DMs）预训练中引入轻微噪声能提高生成图像的质量和多样性。论文证明噪声能降低 Wasserstein 距离并提升模型鲁棒性，实验在 ImageNet 和 CC3M 上验证了这一效果。该工作挑战了传统预训练假设，提供新视角。\n\n- **HQ-DiT: Efficient Diffusion Transformer with FP4 Hybrid Quantization**（HQ-DiT：使用 FP4 混合量化的高效扩散 Transformer）  \n  作者提出了一种高效的扩散 Transformer 模型，通过 FP4 量化减少参数，同时保持性能。主要发现是量化后模型在图像生成任务中仅损失微小精度（如 sFID 仅增加 0.12），这是首次将扩散模型量化到 4 位。该方法对资源受限设备部署有重要意义。\n\n- **CV-VAE: A Compatible Video VAE for Latent Generative Video Models**（CV-VAE：用于潜在生成视频模型的兼容视频 VAE）  \n  这篇论文设计了兼容的视频变分自编码器（VAE），用于生成视频。主要贡献是引入潜在空间正则化，提升视频生成质量，实验显示在视频任务中性能提升 4 倍。该工作扩展了生成模型的应用。\n\n### 强化学习与决策\n- **Diffusion Policies creating a Trust Region for Offline Reinforcement Learning**（扩散策略：为离线强化学习创建信任区域）  \n  Tianyu Chen 等提出扩散策略框架，用于离线强化学习。主要发现是通过信任区域优化，模型在 D4RL 基准上性能提升，同时训练和推理速度更快。该方法平衡了探索和利用，提供理论和实证支持。\n\n- **Bilevel reinforcement learning via the development of hyper-gradient without lower-level convexity**（双层强化学习：无需下层凸性开发超梯度）  \n  Yan Yang 等探索了双层 RL 的超梯度优化，证明无需下层凸性假设即可收敛。实验显示算法在复杂环境中收敛率达 O(ε⁻¹)，这对高效 RL 训练有突破性贡献。\n\n其他论文，如医学图像处理（第19、20）、机器人学习（第5、7）和多模态模型（第27、36），也值得一提，但它们相对常规或应用导向较强。例如，第19 篇探讨了医疗图像中的模型泛化问题，主要发现是使用探针评估提升了诊断准确性；第7 篇提出社交机器人系统，实验显示在购物场景中指导精度高。这些论文在各自领域有实用价值，但整体影响力不如上述重点论文。\n\n总之，今天 arXiv 的论文突显了 AI 模型的优化和应用潜力，LLM 相关工作尤其活跃。未来几天，继续关注这些领域的进展！（本快报约 800 字，聚焦核心内容。）",
  "papers": [
    {
      "arxiv_id": "2405.20535v2",
      "title": "Unveiling the Impact of Coding Data Instruction Fine-Tuning on Large Language Models Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Xinlu Zhang",
        "Zhiyu Zoey Chen",
        "Xi Ye",
        "Xianjun Yang",
        "Lichang Chen",
        "William Yang Wang",
        "Linda Ruth Petzold"
      ],
      "abstract": "Instruction Fine-Tuning (IFT) significantly enhances the zero-shot\ncapabilities of pretrained Large Language Models (LLMs). While coding data is\nknown to boost LLM reasoning abilities during pretraining, its role in\nactivating internal reasoning capacities during IFT remains understudied. This\npaper investigates a key question: How does coding data impact LLMs' reasoning\ncapacities during IFT stage? To explore this, we thoroughly examine the impact\nof coding data across different coding data proportions, model families, sizes,\nand reasoning domains, from various perspectives. Specifically, we create three\nIFT datasets with increasing coding data proportions, fine-tune six LLM\nbackbones across different families and scales on these datasets, evaluate the\ntuned models' performance across twelve tasks in three reasoning domains, and\nanalyze the outcomes from three broad-to-granular perspectives: overall,\ndomain-level, and task-specific. Our holistic analysis provides valuable\ninsights into each perspective. First, coding data tuning enhances the overall\nreasoning capabilities of LLMs across different model families and scales.\nMoreover, while the impact of coding data varies by domain, it shows consistent\ntrends within each domain across different model families and scales.\nAdditionally, coding data generally provides comparable task-specific benefits\nacross model families, with optimal proportions in IFT datasets being\ntask-dependent.",
      "tldr_zh": "本文研究了在Instruction Fine-Tuning (IFT) 阶段，使用编码数据如何影响Large Language Models (LLMs) 的推理能力，填补了这一领域的空白。研究者创建了三种不同比例的编码数据IFT数据集，并对六种不同系列和规模的LLMs进行微调，评估其在三个推理领域（共12个任务）的表现，从整体、领域级和任务特定角度进行分析。结果表明，编码数据显著提升了LLMs的整体推理能力，其影响在各领域内呈现一致趋势，但最优数据比例因任务而异，为优化LLMs训练提供宝贵见解。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20535v2",
      "published_date": "2024-05-30 23:20:25 UTC",
      "updated_date": "2024-12-12 18:45:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:52:18.195162"
    },
    {
      "arxiv_id": "2405.20529v1",
      "title": "An Automatic Question Usability Evaluation Toolkit",
      "title_zh": "自动问题可用性评估工具包",
      "authors": [
        "Steven Moore",
        "Eamon Costello",
        "Huy A. Nguyen",
        "John Stamper"
      ],
      "abstract": "Evaluating multiple-choice questions (MCQs) involves either labor intensive\nhuman assessments or automated methods that prioritize readability, often\noverlooking deeper question design flaws. To address this issue, we introduce\nthe Scalable Automatic Question Usability Evaluation Toolkit (SAQUET), an\nopen-source tool that leverages the Item-Writing Flaws (IWF) rubric for a\ncomprehensive and automated quality evaluation of MCQs. By harnessing the\nlatest in large language models such as GPT-4, advanced word embeddings, and\nTransformers designed to analyze textual complexity, SAQUET effectively\npinpoints and assesses a wide array of flaws in MCQs. We first demonstrate the\ndiscrepancy between commonly used automated evaluation metrics and the human\nassessment of MCQ quality. Then we evaluate SAQUET on a diverse dataset of MCQs\nacross the five domains of Chemistry, Statistics, Computer Science, Humanities,\nand Healthcare, showing how it effectively distinguishes between flawed and\nflawless questions, providing a level of analysis beyond what is achievable\nwith traditional metrics. With an accuracy rate of over 94% in detecting the\npresence of flaws identified by human evaluators, our findings emphasize the\nlimitations of existing evaluation methods and showcase potential in improving\nthe quality of educational assessments.",
      "tldr_zh": "该论文介绍了 SAQUET（Scalable Automatic Question Usability Evaluation Toolkit），一个开源工具，用于自动化评估多选题（MCQs）的可用性问题，通过 Item-Writing Flaws (IWF) 标准识别设计缺陷。SAQUET 利用大型语言模型如 GPT-4、先进的词嵌入和 Transformers 分析文本复杂性，从而提供比传统指标更全面的评估。实验在 Chemistry、Statistics、Computer Science、Humanities 和 Healthcare 等领域的多样数据集上进行，结果显示 SAQUET 在检测人类评估者识别的缺陷方面准确率超过 94%，突显了其在提升教育评估质量方面的潜力。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Artificial Intelligence in Education 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.20529v1",
      "published_date": "2024-05-30 23:04:53 UTC",
      "updated_date": "2024-05-30 23:04:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:52:30.042808"
    },
    {
      "arxiv_id": "2405.20527v1",
      "title": "Towards Ontology-Enhanced Representation Learning for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Francesco Ronzano",
        "Jay Nanavati"
      ],
      "abstract": "Taking advantage of the widespread use of ontologies to organise and\nharmonize knowledge across several distinct domains, this paper proposes a\nnovel approach to improve an embedding-Large Language Model (embedding-LLM) of\ninterest by infusing the knowledge formalized by a reference ontology:\nontological knowledge infusion aims at boosting the ability of the considered\nLLM to effectively model the knowledge domain described by the infused\nontology. The linguistic information (i.e. concept synonyms and descriptions)\nand structural information (i.e. is-a relations) formalized by the ontology are\nutilized to compile a comprehensive set of concept definitions, with the\nassistance of a powerful generative LLM (i.e. GPT-3.5-turbo). These concept\ndefinitions are then employed to fine-tune the target embedding-LLM using a\ncontrastive learning framework. To demonstrate and evaluate the proposed\napproach, we utilize the biomedical disease ontology MONDO. The results show\nthat embedding-LLMs enhanced by ontological disease knowledge exhibit an\nimproved capability to effectively evaluate the similarity of in-domain\nsentences from biomedical documents mentioning diseases, without compromising\ntheir out-of-domain performance.",
      "tldr_zh": "本论文提出了一种新方法，通过本体论（ontology）知识注入来提升嵌入型大型语言模型（embedding-LLMs）的表示学习能力，旨在帮助模型更好地建模特定知识领域。方法包括利用本体论中的语言信息（如概念同义词和描述）和结构信息（如is-a关系）生成概念定义，并借助GPT-3.5-turbo编译这些定义，然后采用对比学习（contrastive learning）框架对目标embedding-LLMs进行微调。实验以生物医学疾病本体MONDO为例，结果显示增强后的模型在评估领域内生物医学文档句子相似性时表现显著改善，同时不影响域外性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50",
        "I.2.7; I.2.6"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2405.20527v1",
      "published_date": "2024-05-30 23:01:10 UTC",
      "updated_date": "2024-05-30 23:01:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:52:43.071290"
    },
    {
      "arxiv_id": "2405.20526v1",
      "title": "Automated Generation and Tagging of Knowledge Components from Multiple-Choice Questions",
      "title_zh": "自动生成和标记多项选择题的知识组件",
      "authors": [
        "Steven Moore",
        "Robin Schmucker",
        "Tom Mitchell",
        "John Stamper"
      ],
      "abstract": "Knowledge Components (KCs) linked to assessments enhance the measurement of\nstudent learning, enrich analytics, and facilitate adaptivity. However,\ngenerating and linking KCs to assessment items requires significant effort and\ndomain-specific knowledge. To streamline this process for higher-education\ncourses, we employed GPT-4 to generate KCs for multiple-choice questions (MCQs)\nin Chemistry and E-Learning. We analyzed discrepancies between the KCs\ngenerated by the Large Language Model (LLM) and those made by humans through\nevaluation from three domain experts in each subject area. This evaluation\naimed to determine whether, in instances of non-matching KCs, evaluators showed\na preference for the LLM-generated KCs over their human-created counterparts.\nWe also developed an ontology induction algorithm to cluster questions that\nassess similar KCs based on their content. Our most effective LLM strategy\naccurately matched KCs for 56% of Chemistry and 35% of E-Learning MCQs, with\neven higher success when considering the top five KC suggestions. Human\nevaluators favored LLM-generated KCs, choosing them over human-assigned ones\napproximately two-thirds of the time, a preference that was statistically\nsignificant across both domains. Our clustering algorithm successfully grouped\nquestions by their underlying KCs without needing explicit labels or contextual\ninformation. This research advances the automation of KC generation and\nclassification for assessment items, alleviating the need for student data or\npredefined KC labels.",
      "tldr_zh": "这篇论文提出了一种自动化方法，使用 GPT-4 从 Multiple-Choice Questions (MCQs) 中生成和标记 Knowledge Components (KCs)，以简化高等教育课程（如化学和电子学习）的评估过程。研究通过领域专家评估比较了 Large Language Model (LLM) 生成的 KCs 与人类生成的 KCs，发现专家在约三分之二的情况下更偏好 LLM 版本，且最有效策略在化学 MCQs 中准确匹配达 56%、在电子学习中达 35%。此外，论文开发了一个 ontology induction algorithm 来基于问题内容聚类类似 KCs 的问题，实现无需学生数据或预定义标签的自动化分类。该方法显著提升了 KCs 生成和分类的效率，为教育评估和适应性学习提供了新途径。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Learning @ Scale 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.20526v1",
      "published_date": "2024-05-30 22:57:49 UTC",
      "updated_date": "2024-05-30 22:57:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:52:54.850747"
    },
    {
      "arxiv_id": "2405.20519v1",
      "title": "Diffusion On Syntax Trees For Program Synthesis",
      "title_zh": "翻译失败",
      "authors": [
        "Shreyas Kapur",
        "Erik Jenner",
        "Stuart Russell"
      ],
      "abstract": "Large language models generate code one token at a time. Their autoregressive\ngeneration process lacks the feedback of observing the program's output.\nTraining LLMs to suggest edits directly can be challenging due to the scarcity\nof rich edit data. To address these problems, we propose neural diffusion\nmodels that operate on syntax trees of any context-free grammar. Similar to\nimage diffusion models, our method also inverts ``noise'' applied to syntax\ntrees. Rather than generating code sequentially, we iteratively edit it while\npreserving syntactic validity, which makes it easy to combine this neural model\nwith search. We apply our approach to inverse graphics tasks, where our model\nlearns to convert images into programs that produce those images. Combined with\nsearch, our model is able to write graphics programs, see the execution result,\nand debug them to meet the required specifications. We additionally show how\nour system can write graphics programs for hand-drawn sketches.",
      "tldr_zh": "这篇论文提出了一种基于语法树的神经扩散模型，用于程序合成，以解决大语言模型(LLMs)生成代码时缺乏反馈和编辑数据稀缺的问题。该模型通过反转应用于上下文无关语法(syntax trees)的“噪声”，实现代码的迭代编辑，同时保持语法有效性，并便于与搜索算法结合。在逆图形任务中，模型能够将图像转换为生成这些图像的程序，并通过执行结果调试程序以满足规范，还能扩展到处理手绘草图的程序编写。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "https://tree-diffusion.github.io",
      "pdf_url": "http://arxiv.org/pdf/2405.20519v1",
      "published_date": "2024-05-30 22:31:16 UTC",
      "updated_date": "2024-05-30 22:31:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:53:06.016643"
    },
    {
      "arxiv_id": "2405.20513v2",
      "title": "Deep Modeling of Non-Gaussian Aleatoric Uncertainty",
      "title_zh": "翻译失败",
      "authors": [
        "Aastha Acharya",
        "Caleb Lee",
        "Marissa D'Alonzo",
        "Jared Shamwell",
        "Nisar R. Ahmed",
        "Rebecca Russell"
      ],
      "abstract": "Deep learning offers promising new ways to accurately model aleatoric\nuncertainty in robotic state estimation systems, particularly when the\nuncertainty distributions do not conform to traditional assumptions of being\nfixed and Gaussian. In this study, we formulate and evaluate three fundamental\ndeep learning approaches for conditional probability density modeling to\nquantify non-Gaussian aleatoric uncertainty: parametric, discretized, and\ngenerative modeling. We systematically compare the respective strengths and\nweaknesses of these three methods on simulated non-Gaussian densities as well\nas on real-world terrain-relative navigation data. Our results show that these\ndeep learning methods can accurately capture complex uncertainty patterns,\nhighlighting their potential for improving the reliability and robustness of\nestimation systems.",
      "tldr_zh": "该研究探讨了深度学习在机器人状态估计系统中建模非高斯 aleatoric uncertainty 的应用，针对传统高斯假设的局限性。研究评估了三种基本方法：parametric modeling、discretized modeling 和 generative modeling，通过模拟非高斯密度和真实地形相对导航数据进行系统比较。结果表明，这些方法能准确捕捉复杂不确定性模式，从而显著提高估计系统的可靠性和鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.20513v2",
      "published_date": "2024-05-30 22:13:17 UTC",
      "updated_date": "2025-02-27 16:35:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:53:17.814004"
    },
    {
      "arxiv_id": "2405.20501v1",
      "title": "ShelfHelp: Empowering Humans to Perform Vision-Independent Manipulation Tasks with a Socially Assistive Robotic Cane",
      "title_zh": "翻译失败",
      "authors": [
        "Shivendra Agrawal",
        "Suresh Nayak",
        "Ashutosh Naik",
        "Bradley Hayes"
      ],
      "abstract": "The ability to shop independently, especially in grocery stores, is important\nfor maintaining a high quality of life. This can be particularly challenging\nfor people with visual impairments (PVI). Stores carry thousands of products,\nwith approximately 30,000 new products introduced each year in the US market\nalone, presenting a challenge even for modern computer vision solutions.\nThrough this work, we present a proof-of-concept socially assistive robotic\nsystem we call ShelfHelp, and propose novel technical solutions for enhancing\ninstrumented canes traditionally meant for navigation tasks with additional\ncapability within the domain of shopping. ShelfHelp includes a novel visual\nproduct locator algorithm designed for use in grocery stores and a novel\nplanner that autonomously issues verbal manipulation guidance commands to guide\nthe user during product retrieval. Through a human subjects study, we show the\nsystem's success in locating and providing effective manipulation guidance to\nretrieve desired products with novice users. We compare two autonomous verbal\nguidance modes achieving comparable performance to a human assistance baseline\nand present encouraging findings that validate our system's efficiency and\neffectiveness and through positive subjective metrics including competence,\nintelligence, and ease of use.",
      "tldr_zh": "该研究提出ShelfHelp，一种社会辅助机器人拐杖系统，旨在帮助视觉障碍者（PVI）在杂货店进行视觉无关的操作任务，如产品检索，从而提升他们的独立购物能力。该系统包括一个新型的visual product locator algorithm用于定位产品，以及一个自主planner提供口头操作指导命令。人类受试者研究显示，ShelfHelp成功定位并指导用户取回产品，其两种自主口头指导模式的效果与人类辅助基准相当，并在主观指标如能力、智能和易用性上获得积极反馈。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 14 figures and charts",
      "pdf_url": "http://arxiv.org/pdf/2405.20501v1",
      "published_date": "2024-05-30 21:42:54 UTC",
      "updated_date": "2024-05-30 21:42:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:53:39.700765"
    },
    {
      "arxiv_id": "2405.20494v2",
      "title": "Slight Corruption in Pre-training Data Makes Better Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Hao Chen",
        "Yujin Han",
        "Diganta Misra",
        "Xiang Li",
        "Kai Hu",
        "Difan Zou",
        "Masashi Sugiyama",
        "Jindong Wang",
        "Bhiksha Raj"
      ],
      "abstract": "Diffusion models (DMs) have shown remarkable capabilities in generating\nrealistic high-quality images, audios, and videos. They benefit significantly\nfrom extensive pre-training on large-scale datasets, including web-crawled data\nwith paired data and conditions, such as image-text and image-class pairs.\nDespite rigorous filtering, these pre-training datasets often inevitably\ncontain corrupted pairs where conditions do not accurately describe the data.\nThis paper presents the first comprehensive study on the impact of such\ncorruption in pre-training data of DMs. We synthetically corrupt ImageNet-1K\nand CC3M to pre-train and evaluate over 50 conditional DMs. Our empirical\nfindings reveal that various types of slight corruption in pre-training can\nsignificantly enhance the quality, diversity, and fidelity of the generated\nimages across different DMs, both during pre-training and downstream adaptation\nstages. Theoretically, we consider a Gaussian mixture model and prove that\nslight corruption in the condition leads to higher entropy and a reduced\n2-Wasserstein distance to the ground truth of the data distribution generated\nby the corruptly trained DMs. Inspired by our analysis, we propose a simple\nmethod to improve the training of DMs on practical datasets by adding condition\nembedding perturbations (CEP). CEP significantly improves the performance of\nvarious DMs in both pre-training and downstream tasks. We hope that our study\nprovides new insights into understanding the data and pre-training processes of\nDMs and all models are released at https://huggingface.co/DiffusionNoise.",
      "tldr_zh": "本研究发现，扩散模型 (DMs) 的预训练数据中轻微损坏（如条件不准确的配对）反而能提升生成图像的质量、多样性和保真度，这是首次对这一现象进行的全面分析。研究者通过在 ImageNet-1K 和 CC3M 数据集上合成损坏数据，预训练并评估超过 50 个条件 DMs，实验结果显示这种损坏在预训练和下游适应阶段均显著改善模型性能。理论上，使用高斯混合模型证明，轻微损坏导致更高的熵和更小的 2-Wasserstein 距离，使生成的分布更接近真实数据。基于这些发现，论文提出添加条件嵌入扰动 (CEP) 的简单方法，进一步提升 DMs 在实际数据集上的训练效果，并开源所有模型以促进进一步研究。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "NeurIPS 2024 Spotlight",
      "pdf_url": "http://arxiv.org/pdf/2405.20494v2",
      "published_date": "2024-05-30 21:35:48 UTC",
      "updated_date": "2024-10-30 13:52:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:53:42.201901"
    },
    {
      "arxiv_id": "2405.20487v1",
      "title": "Probabilities of Causation for Continuous and Vector Variables",
      "title_zh": "翻译失败",
      "authors": [
        "Yuta Kawakami",
        "Manabu Kuroki",
        "Jin Tian"
      ],
      "abstract": "Probabilities of causation (PoC) are valuable concepts for explainable\nartificial intelligence and practical decision-making. PoC are originally\ndefined for scalar binary variables. In this paper, we extend the concept of\nPoC to continuous treatment and outcome variables, and further generalize PoC\nto capture causal effects between multiple treatments and multiple outcomes. In\naddition, we consider PoC for a sub-population and PoC with multi-hypothetical\nterms to capture more sophisticated counterfactual information useful for\ndecision-making. We provide a nonparametric identification theorem for each\ntype of PoC we introduce. Finally, we illustrate the application of our results\non a real-world dataset about education.",
      "tldr_zh": "本论文扩展了 Probabilities of Causation (PoC) 的概念，从原本的标量二元变量扩展到连续处理变量和结果变量，并进一步泛化到多个处理变量和多个结果变量之间。作者还引入了子群体 PoC 和多假设术语，以捕捉更复杂的反事实信息，支持决策应用，并为每种 PoC 类型提供了非参数识别定理。最终，通过一个真实的教育数据集，论文展示了这些扩展在实际场景中的有效性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20487v1",
      "published_date": "2024-05-30 21:22:26 UTC",
      "updated_date": "2024-05-30 21:22:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:53:54.522916"
    },
    {
      "arxiv_id": "2406.02585v1",
      "title": "Contextual Counting: A Mechanistic Study of Transformers on a Quantitative Task",
      "title_zh": "翻译失败",
      "authors": [
        "Siavash Golkar",
        "Alberto Bietti",
        "Mariel Pettee",
        "Michael Eickenberg",
        "Miles Cranmer",
        "Keiya Hirashima",
        "Geraud Krawezik",
        "Nicholas Lourie",
        "Michael McCabe",
        "Rudy Morel",
        "Ruben Ohana",
        "Liam Holden Parker",
        "Bruno Régaldo-Saint Blancard",
        "Kyunghyun Cho",
        "Shirley Ho"
      ],
      "abstract": "Transformers have revolutionized machine learning across diverse domains, yet\nunderstanding their behavior remains crucial, particularly in high-stakes\napplications. This paper introduces the contextual counting task, a novel toy\nproblem aimed at enhancing our understanding of Transformers in quantitative\nand scientific contexts. This task requires precise localization and\ncomputation within datasets, akin to object detection or region-based\nscientific analysis. We present theoretical and empirical analysis using both\ncausal and non-causal Transformer architectures, investigating the influence of\nvarious positional encodings on performance and interpretability. In\nparticular, we find that causal attention is much better suited for the task,\nand that no positional embeddings lead to the best accuracy, though rotary\nembeddings are competitive and easier to train. We also show that out of\ndistribution performance is tightly linked to which tokens it uses as a bias\nterm.",
      "tldr_zh": "本研究引入了contextual counting任务，这是一个玩具问题，用于深入理解Transformers在定量和科学上下文中的行为，该任务需要精确定位和计算，类似于物体检测或基于区域的科学分析。论文通过理论和实证分析，比较了因果和非因果Transformer架构，并考察了各种positional encodings对性能和可解释性的影响。关键发现包括：causal attention更适合该任务，没有positional embeddings可实现最佳准确率，而rotary embeddings虽竞争力强且更容易训练，但分布外性能与使用的偏差项(tokens as a bias term)密切相关。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.02585v1",
      "published_date": "2024-05-30 20:52:23 UTC",
      "updated_date": "2024-05-30 20:52:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:54:05.182303"
    },
    {
      "arxiv_id": "2406.02583v2",
      "title": "Exploring the Potential of Polynomial Basis Functions in Kolmogorov-Arnold Networks: A Comparative Study of Different Groups of Polynomials",
      "title_zh": "翻译失败",
      "authors": [
        "Seyd Teymoor Seydi"
      ],
      "abstract": "This paper presents a comprehensive survey of 18 distinct polynomials and\ntheir potential applications in Kolmogorov-Arnold Network (KAN) models as an\nalternative to traditional spline-based methods. The polynomials are classified\ninto various groups based on their mathematical properties, such as orthogonal\npolynomials, hypergeometric polynomials, q-polynomials, Fibonacci-related\npolynomials, combinatorial polynomials, and number-theoretic polynomials. The\nstudy aims to investigate the suitability of these polynomials as basis\nfunctions in KAN models for complex tasks like handwritten digit classification\non the MNIST dataset. The performance metrics of the KAN models, including\noverall accuracy, Kappa, and F1 score, are evaluated and compared. The\nGottlieb-KAN model achieves the highest performance across all metrics,\nsuggesting its potential as a suitable choice for the given task. However,\nfurther analysis and tuning of these polynomials on more complex datasets are\nnecessary to fully understand their capabilities in KAN models. The source code\nfor the implementation of these KAN models is available at\nhttps://github.com/seydi1370/Basis_Functions .",
      "tldr_zh": "本研究对18种不同多项式进行了全面调查，探讨它们作为Kolmogorov-Arnold Networks (KAN)模型中基函数的潜力，以替代传统样条方法。这些多项式根据数学属性分为几组，包括orthogonal polynomials、hypergeometric polynomials、q-polynomials、Fibonacci-related polynomials、combinatorial polynomials和number-theoretic polynomials，并评估了它们在MNIST数据集手写数字分类任务中的表现。结果显示，Gottlieb-KAN模型在整体准确率、Kappa和F1分数等指标上表现出最高性能，表明其适合此类复杂任务。然而，需进一步分析和调整这些多项式在更复杂数据集上的应用，以全面评估其在KAN模型中的能力。源代码可从GitHub获取。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.02583v2",
      "published_date": "2024-05-30 20:40:16 UTC",
      "updated_date": "2024-10-14 01:58:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:54:18.733963"
    },
    {
      "arxiv_id": "2405.20465v1",
      "title": "ENTIRe-ID: An Extensive and Diverse Dataset for Person Re-Identification",
      "title_zh": "ENTIRe-ID：一个广泛且多样的人员再识别数据集",
      "authors": [
        "Serdar Yildiz",
        "Ahmet Nezih Kasim"
      ],
      "abstract": "The growing importance of person reidentification in computer vision has\nhighlighted the need for more extensive and diverse datasets. In response, we\nintroduce the ENTIRe-ID dataset, an extensive collection comprising over 4.45\nmillion images from 37 different cameras in varied environments. This dataset\nis uniquely designed to tackle the challenges of domain variability and model\ngeneralization, areas where existing datasets for person re-identification have\nfallen short. The ENTIRe-ID dataset stands out for its coverage of a wide array\nof real-world scenarios, encompassing various lighting conditions, angles of\nview, and diverse human activities. This design ensures a realistic and robust\ntraining platform for ReID models. The ENTIRe-ID dataset is publicly available\nat https://serdaryildiz.github.io/ENTIRe-ID",
      "tldr_zh": "本研究引入了 ENTIRE-ID 数据集，这是一个用于 Person Re-Identification 的庞大多样数据集，包含超过 4.45 百万张图像，来自 37 个不同摄像机的各种环境。数据集旨在解决现有数据集在领域变异性和模型泛化方面的不足，通过覆盖多种真实场景（如不同照明条件、视角和人类活动）提供更鲁棒的训练平台。ENTIRE-ID 的公开可用性（访问链接：https://serdaryildiz.github.io/ENTIRe-ID）将有助于提升 ReID 模型的性能和泛化能力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "5 pages, 2024 18th International Conference on Automatic Face and\n  Gesture Recognition (FG)",
      "pdf_url": "http://arxiv.org/pdf/2405.20465v1",
      "published_date": "2024-05-30 20:26:47 UTC",
      "updated_date": "2024-05-30 20:26:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:54:39.699823"
    },
    {
      "arxiv_id": "2405.20450v1",
      "title": "Decentralized AI: Permissionless LLM Inference on POKT Network",
      "title_zh": "翻译失败",
      "authors": [
        "Daniel Olshansky",
        "Ramiro Rodriguez Colmeiro",
        "Bowen Li"
      ],
      "abstract": "POKT Network's decentralized Remote Procedure Call (RPC) infrastructure,\nsurpassing 740 billion requests since launching on MainNet in 2020, is\nwell-positioned to extend into providing AI inference services with minimal\ndesign or implementation modifications. This litepaper illustrates how the\nnetwork's open-source and permissionless design aligns incentives among model\nresearchers, hardware operators, API providers and users whom we term model\nSources, Suppliers, Gateways and Applications respectively. Through its Relay\nMining algorithm, POKT creates a transparent marketplace where costs and\nearnings directly reflect cryptographically verified usage. This decentralized\nframework offers large model AI researchers a new avenue to disseminate their\nwork and generate revenue without the complexities of maintaining\ninfrastructure or building end-user products. Supply scales naturally with\ndemand, as evidenced in recent years and the protocol's free market dynamics.\nPOKT Gateways facilitate network growth, evolution, adoption, and quality by\nacting as application-facing load balancers, providing value-added features\nwithout managing LLM nodes directly. This vertically decoupled network, battle\ntested over several years, is set up to accelerate the adoption, operation,\ninnovation and financialization of open-source models. It is the first mature\npermissionless network whose quality of service competes with centralized\nentities set up to provide application grade inference.",
      "tldr_zh": "POKT Network 通过其去中心化的 RPC 基础设施，扩展到提供免权限的 LLM Inference 服务，已处理超过740亿请求。该网络采用开源和透明设计，通过 Relay Mining 算法创建激励机制，协调模型研究者（Sources）、硬件运营商（Suppliers）、API 提供者（Gateways）和用户（Applications），形成直接基于加密验证使用的成本与收益市场。这为 AI 研究者提供新途径来分发模型并生成收入，而无需维护基础设施，且网络的供应随需求自然扩展，其服务质量已能与集中式实体竞争。",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20450v1",
      "published_date": "2024-05-30 19:50:07 UTC",
      "updated_date": "2024-05-30 19:50:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:54:43.358964"
    },
    {
      "arxiv_id": "2405.20446v3",
      "title": "Is My Data in Your Retrieval Database? Membership Inference Attacks Against Retrieval Augmented Generation",
      "title_zh": "我的数据在你的检索数据库中吗？针对检索增强生成的成员推断攻击",
      "authors": [
        "Maya Anderson",
        "Guy Amit",
        "Abigail Goldsteen"
      ],
      "abstract": "Retrieval Augmented Generation (RAG) systems have shown great promise in\nnatural language processing. However, their reliance on data stored in a\nretrieval database, which may contain proprietary or sensitive information,\nintroduces new privacy concerns. Specifically, an attacker may be able to infer\nwhether a certain text passage appears in the retrieval database by observing\nthe outputs of the RAG system, an attack known as a Membership Inference Attack\n(MIA). Despite the significance of this threat, MIAs against RAG systems have\nyet remained under-explored. This study addresses this gap by introducing an\nefficient and easy-to-use method for conducting MIA against RAG systems. We\ndemonstrate the effectiveness of our attack using two benchmark datasets and\nmultiple generative models, showing that the membership of a document in the\nretrieval database can be efficiently determined through the creation of an\nappropriate prompt in both black-box and gray-box settings. Moreover, we\nintroduce an initial defense strategy based on adding instructions to the RAG\ntemplate, which shows high effectiveness for some datasets and models. Our\nfindings highlight the importance of implementing security countermeasures in\ndeployed RAG systems and developing more advanced defenses to protect the\nprivacy and security of retrieval databases.",
      "tldr_zh": "这篇论文探讨了Retrieval Augmented Generation (RAG)系统的隐私风险，特别关注Membership Inference Attack (MIA)，即攻击者通过观察RAG输出来推断特定文本是否在检索数据库中。研究提出了一种高效、易用的MIA方法，利用适当的prompt在black-box和gray-box设置下进行攻击，并在两个基准数据集和多个生成模型上验证了其有效性。论文还引入了一种初步防御策略，通过在RAG模板中添加指令来缓解攻击，并强调了在部署RAG系统中实施安全措施以保护数据库隐私的重要性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "I.2; K.6.5"
      ],
      "primary_category": "cs.CR",
      "comment": "12 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.20446v3",
      "published_date": "2024-05-30 19:46:36 UTC",
      "updated_date": "2025-02-04 14:35:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:54:54.254170"
    },
    {
      "arxiv_id": "2405.20441v4",
      "title": "SECURE: Benchmarking Large Language Models for Cybersecurity",
      "title_zh": "翻译失败",
      "authors": [
        "Dipkamal Bhusal",
        "Md Tanvirul Alam",
        "Le Nguyen",
        "Ashim Mahara",
        "Zachary Lightcap",
        "Rodney Frazier",
        "Romy Fieblinger",
        "Grace Long Torales",
        "Benjamin A. Blakely",
        "Nidhi Rastogi"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated potential in cybersecurity\napplications but have also caused lower confidence due to problems like\nhallucinations and a lack of truthfulness. Existing benchmarks provide general\nevaluations but do not sufficiently address the practical and applied aspects\nof LLM performance in cybersecurity-specific tasks. To address this gap, we\nintroduce the SECURE (Security Extraction, Understanding \\& Reasoning\nEvaluation), a benchmark designed to assess LLMs performance in realistic\ncybersecurity scenarios. SECURE includes six datasets focussed on the\nIndustrial Control System sector to evaluate knowledge extraction,\nunderstanding, and reasoning based on industry-standard sources. Our study\nevaluates seven state-of-the-art models on these tasks, providing insights into\ntheir strengths and weaknesses in cybersecurity contexts, and offer\nrecommendations for improving LLMs reliability as cyber advisory tools.",
      "tldr_zh": "本研究针对大型语言模型(LLMs)在网络安全应用中的潜在问题，如幻觉和真实性不足，引入了SECURE基准，用于评估LLMs在真实场景中的性能。SECURE包括六个专注于工业控制系统(Industrial Control System)领域的数据集，评估知识提取、理解和推理能力。研究评估了七个最先进模型，揭示了它们的优势与劣势，并提供改进建议，以提升LLMs作为网络安全咨询工具的可靠性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20441v4",
      "published_date": "2024-05-30 19:35:06 UTC",
      "updated_date": "2024-10-30 14:29:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:55:05.179251"
    },
    {
      "arxiv_id": "2405.20434v1",
      "title": "Facilitating Human-LLM Collaboration through Factuality Scores and Source Attributions",
      "title_zh": "通过事实性分数和来源归属促进人类-LLM 合作",
      "authors": [
        "Hyo Jin Do",
        "Rachel Ostrand",
        "Justin D. Weisz",
        "Casey Dugan",
        "Prasanna Sattigeri",
        "Dennis Wei",
        "Keerthiram Murugesan",
        "Werner Geyer"
      ],
      "abstract": "While humans increasingly rely on large language models (LLMs), they are\nsusceptible to generating inaccurate or false information, also known as\n\"hallucinations\". Technical advancements have been made in algorithms that\ndetect hallucinated content by assessing the factuality of the model's\nresponses and attributing sections of those responses to specific source\ndocuments. However, there is limited research on how to effectively communicate\nthis information to users in ways that will help them appropriately calibrate\ntheir trust toward LLMs. To address this issue, we conducted a scenario-based\nstudy (N=104) to systematically compare the impact of various design strategies\nfor communicating factuality and source attribution on participants' ratings of\ntrust, preferences, and ease in validating response accuracy. Our findings\nreveal that participants preferred a design in which phrases within a response\nwere color-coded based on the computed factuality scores. Additionally,\nparticipants increased their trust ratings when relevant sections of the source\nmaterial were highlighted or responses were annotated with reference numbers\ncorresponding to those sources, compared to when they received no annotation in\nthe source material. Our study offers practical design guidelines to facilitate\nhuman-LLM collaboration and it promotes a new human role to carefully evaluate\nand take responsibility for their use of LLM outputs.",
      "tldr_zh": "本文研究了如何通过事实性分数和来源归因来提升人类与大型语言模型(LLMs)的合作，以应对LLMs产生的幻觉问题。研究者开展了一项基于场景的用户研究(N=104)，系统比较了多种设计策略（如颜色编码响应短语、突出来源材料或添加参考数字）对用户信任、偏好和验证易用性的影响。结果表明，参与者更喜欢颜色编码事实性分数的设计，并通过突出来源或参考数字标注来显著提高对LLMs的信任。研究提供了实用设计指南，促进人类-LLM合作，并强调人类在评估和负责使用LLM输出中的新角色。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Submitted to the Trust and Reliance in Evolving Human-AI Workflows\n  (TREW) Workshop at CHI 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.20434v1",
      "published_date": "2024-05-30 19:23:14 UTC",
      "updated_date": "2024-05-30 19:23:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:55:19.510353"
    },
    {
      "arxiv_id": "2406.02582v1",
      "title": "Spatiotemporal Predictions of Toxic Urban Plumes Using Deep Learning",
      "title_zh": "使用深度学习进行有毒城市烟羽的时空预测",
      "authors": [
        "Yinan Wang",
        "M. Giselle Fernández-Godino",
        "Nipun Gunawardena",
        "Donald D. Lucas",
        "Xiaowei Yue"
      ],
      "abstract": "Industrial accidents, chemical spills, and structural fires can release large\namounts of harmful materials that disperse into urban atmospheres and impact\npopulated areas. Computer models are typically used to predict the transport of\ntoxic plumes by solving fluid dynamical equations. However, these models can be\ncomputationally expensive due to the need for many grid cells to simulate\nturbulent flow and resolve individual buildings and streets. In emergency\nresponse situations, alternative methods are needed that can run quickly and\nadequately capture important spatiotemporal features. Here, we present a novel\ndeep learning model called ST-GasNet that was inspired by the mathematical\nequations that govern the behavior of plumes as they disperse through the\natmosphere. ST-GasNet learns the spatiotemporal dependencies from a limited set\nof temporal sequences of ground-level toxic urban plumes generated by a\nhigh-resolution large eddy simulation model. On independent sequences,\nST-GasNet accurately predicts the late-time spatiotemporal evolution, given the\nearly-time behavior as an input, even for cases when a building splits a large\nplume into smaller plumes. By incorporating large-scale wind boundary condition\ninformation, ST-GasNet achieves a prediction accuracy of at least 90% on test\ndata for the entire prediction period.",
      "tldr_zh": "该研究针对工业事故等事件释放的毒性烟 plume 在城市环境中的扩散问题，提出了一种基于深度学习的模型 ST-GasNet，以替代计算密集的流体动力学模拟方法。ST-GasNet 受大气扩散数学方程启发，从有限的高分辨率 large eddy simulation 生成的时序数据中学习时空依赖，能够准确预测早期行为向后期的演化，甚至处理建筑导致 plume 分裂的复杂场景。通过整合大尺度风边界条件，该模型在测试数据上实现了至少90%的预测准确率，为紧急响应提供高效的时空预测工具。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.ao-ph",
        "86-08",
        "I.2.10"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.02582v1",
      "published_date": "2024-05-30 19:18:20 UTC",
      "updated_date": "2024-05-30 19:18:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:55:29.206993"
    },
    {
      "arxiv_id": "2405.20430v1",
      "title": "Enhancing Performance for Highly Imbalanced Medical Data via Data Regularization in a Federated Learning Setting",
      "title_zh": "通过数据正则化在联邦学习环境中提升高度不平衡医疗数据的性能",
      "authors": [
        "Georgios Tsoumplekas",
        "Ilias Siniosoglou",
        "Vasileios Argyriou",
        "Ioannis D. Moscholios",
        "Panagiotis Sarigiannidis"
      ],
      "abstract": "The increased availability of medical data has significantly impacted\nhealthcare by enabling the application of machine / deep learning approaches in\nvarious instances. However, medical datasets are usually small and scattered\nacross multiple providers, suffer from high class-imbalance, and are subject to\nstringent data privacy constraints. In this paper, the application of a data\nregularization algorithm, suitable for learning under high class-imbalance, in\na federated learning setting is proposed. Specifically, the goal of the\nproposed method is to enhance model performance for cardiovascular disease\nprediction by tackling the class-imbalance that typically characterizes\ndatasets used for this purpose, as well as by leveraging patient data available\nin different nodes of a federated ecosystem without compromising their privacy\nand enabling more resource sensitive allocation. The method is evaluated across\nfour datasets for cardiovascular disease prediction, which are scattered across\ndifferent clients, achieving improved performance. Meanwhile, its robustness\nunder various hyperparameter settings, as well as its ability to adapt to\ndifferent resource allocation scenarios, is verified.",
      "tldr_zh": "这篇论文提出了一种数据正则化算法，应用于联邦学习设置中，以提升高度类不平衡医疗数据的模型性能。方法通过处理类不平衡问题，利用分布式患者数据进行心血管疾病预测，同时确保数据隐私不被泄露。实验在四个心血管疾病数据集上验证了该方法的有效性，展示了改进的性能，并确认了其在不同超参数设置和资源分配场景下的鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20430v1",
      "published_date": "2024-05-30 19:15:38 UTC",
      "updated_date": "2024-05-30 19:15:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:55:42.166820"
    },
    {
      "arxiv_id": "2405.20421v4",
      "title": "Worse than Random? An Embarrassingly Simple Probing Evaluation of Large Multimodal Models in Medical VQA",
      "title_zh": "翻译失败",
      "authors": [
        "Qianqi Yan",
        "Xuehai He",
        "Xiang Yue",
        "Xin Eric Wang"
      ],
      "abstract": "Large Multimodal Models (LMMs) have shown remarkable progress in medical\nVisual Question Answering (Med-VQA), achieving high accuracy on existing\nbenchmarks. However, their reliability under robust evaluation is questionable.\nThis study reveals that when subjected to simple probing evaluation,\nstate-of-the-art models perform worse than random guessing on medical diagnosis\nquestions. To address this critical evaluation problem, we introduce the\nProbing Evaluation for Medical Diagnosis (ProbMed) dataset to rigorously assess\nLMM performance in medical imaging through probing evaluation and procedural\ndiagnosis. Particularly, probing evaluation features pairing original questions\nwith negation questions with hallucinated attributes, while procedural\ndiagnosis requires reasoning across various diagnostic dimensions for each\nimage, including modality recognition, organ identification, clinical findings,\nabnormalities, and positional grounding. Our evaluation reveals that\ntop-performing models like GPT-4o, GPT-4V, and Gemini Pro perform worse than\nrandom guessing on specialized diagnostic questions, indicating significant\nlimitations in handling fine-grained medical inquiries. Besides, models like\nLLaVA-Med struggle even with more general questions, and results from CheXagent\ndemonstrate the transferability of expertise across different modalities of the\nsame organ, showing that specialized domain knowledge is still crucial for\nimproving performance. This study underscores the urgent need for more robust\nevaluation to ensure the reliability of LMMs in critical fields like medical\ndiagnosis, and current LMMs are still far from applicable to those fields.",
      "tldr_zh": "这篇论文评估了大型多模态模型 (LMMs) 在医疗视觉问答 (Med-VQA) 中的性能，发现这些模型在诊断问题上表现不如随机猜测，暴露了其可靠性不足的问题。研究引入了 ProbMed 数据集，通过 probing evaluation（包括原问题与否定问题配对）和 procedural diagnosis（涉及模态识别、器官识别等维度）来严格测试模型的推理能力。实验结果显示，顶级模型如 GPT-4o、GPT-4V 和 Gemini Pro 在专业诊断任务中失败，而 LLaVA-Med 甚至在一般问题上也挣扎；此外，CheXagent 展示了专业领域知识在不同模态间的可转移性。论文强调，当前 LMMs 远未成熟，需要更 robust 的评估方法来确保其在医疗诊断领域的可靠性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20421v4",
      "published_date": "2024-05-30 18:56:01 UTC",
      "updated_date": "2024-10-05 00:09:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:55:55.469635"
    },
    {
      "arxiv_id": "2405.20419v1",
      "title": "Enhancing Antibiotic Stewardship using a Natural Language Approach for Better Feature Representation",
      "title_zh": "使用自然语言方法增强抗生素管理以实现更好的特征表示",
      "authors": [
        "Simon A. Lee",
        "Trevor Brokowski",
        "Jeffrey N. Chiang"
      ],
      "abstract": "The rapid emergence of antibiotic-resistant bacteria is recognized as a\nglobal healthcare crisis, undermining the efficacy of life-saving antibiotics.\nThis crisis is driven by the improper and overuse of antibiotics, which\nescalates bacterial resistance. In response, this study explores the use of\nclinical decision support systems, enhanced through the integration of\nelectronic health records (EHRs), to improve antibiotic stewardship. However,\nEHR systems present numerous data-level challenges, complicating the effective\nsynthesis and utilization of data. In this work, we transform EHR data into a\nserialized textual representation and employ pretrained foundation models to\ndemonstrate how this enhanced feature representation can aid in antibiotic\nsusceptibility predictions. Our results suggest that this text representation,\ncombined with foundation models, provides a valuable tool to increase\ninterpretability and support antibiotic stewardship efforts.",
      "tldr_zh": "该研究针对抗生素耐药性这一全球性医疗危机，探讨了通过临床决策支持系统整合电子健康记录 (EHRs) 来提升抗生素管理 (antibiotic stewardship)。他们将 EHR 数据转化为序列化的文本表示，并利用预训练的基础模型 (pretrained foundation models) 来改善特征表示，从而辅助抗生素敏感性预测 (antibiotic susceptibility predictions)。结果表明，这种方法增强了模型的可解释性，并为抗生素管理提供了有效的工具。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20419v1",
      "published_date": "2024-05-30 18:53:53 UTC",
      "updated_date": "2024-05-30 18:53:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:56:04.972367"
    },
    {
      "arxiv_id": "2405.20410v1",
      "title": "SeamlessExpressiveLM: Speech Language Model for Expressive Speech-to-Speech Translation with Chain-of-Thought",
      "title_zh": "翻译失败",
      "authors": [
        "Hongyu Gong",
        "Bandhav Veluri"
      ],
      "abstract": "Expressive speech-to-speech translation (S2ST) is a key research topic in\nseamless communication, which focuses on the preservation of semantics and\nspeaker vocal style in translated speech. Early works synthesized speaker style\naligned speech in order to directly learn the mapping from speech to target\nspeech spectrogram. Without reliance on style aligned data, recent studies\nleverage the advances of language modeling (LM) and build cascaded LMs on\nsemantic and acoustic tokens. This work proposes SeamlessExpressiveLM, a single\nspeech language model for expressive S2ST. We decompose the complex\nsource-to-target speech mapping into intermediate generation steps with\nchain-of-thought prompting. The model is first guided to translate target\nsemantic content and then transfer the speaker style to multi-stream acoustic\nunits. Evaluated on Spanish-to-English and Hungarian-to-English translations,\nSeamlessExpressiveLM outperforms cascaded LMs in both semantic quality and\nstyle transfer, meanwhile achieving better parameter efficiency.",
      "tldr_zh": "本文提出SeamlessExpressiveLM，一种单一的Speech Language Model，用于实现Expressive Speech-to-Speech Translation (S2ST)，旨在保留语义和说话者风格。模型通过Chain-of-Thought prompting将源到目标语音映射分解为中间步骤：先生成目标语义内容，然后转移风格到Multi-stream Acoustic Units。这种方法避免了依赖风格对齐数据的需求，并比传统的Cascaded LMs在语义质量和风格转移上表现出色。在西班牙语到英语和匈牙利语到英语的翻译任务中，SeamlessExpressiveLM实现了更好的参数效率和整体性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20410v1",
      "published_date": "2024-05-30 18:28:31 UTC",
      "updated_date": "2024-05-30 18:28:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:56:18.248218"
    },
    {
      "arxiv_id": "2405.20389v1",
      "title": "Designing an Evaluation Framework for Large Language Models in Astronomy Research",
      "title_zh": "为天文学研究设计大型语言模型的评估框架",
      "authors": [
        "John F. Wu",
        "Alina Hyk",
        "Kiera McCormick",
        "Christine Ye",
        "Simone Astarita",
        "Elina Baral",
        "Jo Ciuca",
        "Jesse Cranney",
        "Anjalie Field",
        "Kartheik Iyer",
        "Philipp Koehn",
        "Jenn Kotler",
        "Sandor Kruk",
        "Michelle Ntampaka",
        "Charles O'Neill",
        "Joshua E. G. Peek",
        "Sanjib Sharma",
        "Mikaeel Yunus"
      ],
      "abstract": "Large Language Models (LLMs) are shifting how scientific research is done. It\nis imperative to understand how researchers interact with these models and how\nscientific sub-communities like astronomy might benefit from them. However,\nthere is currently no standard for evaluating the use of LLMs in astronomy.\nTherefore, we present the experimental design for an evaluation study on how\nastronomy researchers interact with LLMs. We deploy a Slack chatbot that can\nanswer queries from users via Retrieval-Augmented Generation (RAG); these\nresponses are grounded in astronomy papers from arXiv. We record and anonymize\nuser questions and chatbot answers, user upvotes and downvotes to LLM\nresponses, user feedback to the LLM, and retrieved documents and similarity\nscores with the query. Our data collection method will enable future dynamic\nevaluations of LLM tools for astronomy.",
      "tldr_zh": "本论文针对大型语言模型(LLMs)在天文学研究中的应用，设计了一个评估框架，以填补当前缺乏标准评估方法的空白。框架通过部署一个Slack聊天机器人，利用检索增强生成(RAG)技术，从arXiv的天文学论文中获取信息来回答用户查询。研究人员将收集匿名化数据，包括用户问题、机器人响应、投票反馈以及文档相似度分数，从而支持未来对LLMs工具的动态评估。",
      "categories": [
        "astro-ph.IM",
        "cs.AI",
        "cs.HC",
        "cs.IR"
      ],
      "primary_category": "astro-ph.IM",
      "comment": "7 pages, 3 figures. Code available at\n  https://github.com/jsalt2024-evaluating-llms-for-astronomy/astro-arxiv-bot",
      "pdf_url": "http://arxiv.org/pdf/2405.20389v1",
      "published_date": "2024-05-30 18:00:21 UTC",
      "updated_date": "2024-05-30 18:00:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:56:30.602956"
    },
    {
      "arxiv_id": "2405.20380v1",
      "title": "Gradient Inversion of Federated Diffusion Models",
      "title_zh": "联邦扩散模型的梯度反演",
      "authors": [
        "Jiyue Huang",
        "Chi Hong",
        "Lydia Y. Chen",
        "Stefanie Roos"
      ],
      "abstract": "Diffusion models are becoming defector generative models, which generate\nexceptionally high-resolution image data. Training effective diffusion models\nrequire massive real data, which is privately owned by distributed parties.\nEach data party can collaboratively train diffusion models in a federated\nlearning manner by sharing gradients instead of the raw data. In this paper, we\nstudy the privacy leakage risk of gradient inversion attacks. First, we design\na two-phase fusion optimization, GIDM, to leverage the well-trained generative\nmodel itself as prior knowledge to constrain the inversion search (latent)\nspace, followed by pixel-wise fine-tuning. GIDM is shown to be able to\nreconstruct images almost identical to the original ones. Considering a more\nprivacy-preserving training scenario, we then argue that locally initialized\nprivate training noise $\\epsilon$ and sampling step t may raise additional\nchallenges for the inversion attack. To solve this, we propose a\ntriple-optimization GIDM+ that coordinates the optimization of the unknown\ndata, $\\epsilon$ and $t$. Our extensive evaluation results demonstrate the\nvulnerability of sharing gradient for data protection of diffusion models, even\nhigh-resolution images can be reconstructed with high quality.",
      "tldr_zh": "该研究探讨了在联邦学习（Federated Learning）中训练扩散模型（Diffusion Models）时的隐私泄露风险，强调共享梯度可能导致梯度反演攻击（Gradient Inversion Attacks）。他们提出了GIDM，一种两阶段融合优化方法，利用训练好的生成模型作为先验知识约束反演搜索空间，并进行像素级微调，从而重建几乎与原图像相同的图像。为了应对局部初始化的私有训练噪声ε和采样步t带来的挑战，他们进一步开发了GIDM+，通过三重优化协调未知数据、ε和t的优化。实验结果显示，即使在高分辨率图像上，共享梯度也会导致高质量的重建，凸显了扩散模型数据保护的脆弱性。",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20380v1",
      "published_date": "2024-05-30 18:00:03 UTC",
      "updated_date": "2024-05-30 18:00:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:56:43.333455"
    },
    {
      "arxiv_id": "2405.20364v1",
      "title": "Learning 3D Robotics Perception using Inductive Priors",
      "title_zh": "翻译失败",
      "authors": [
        "Muhammad Zubair Irshad"
      ],
      "abstract": "Recent advances in deep learning have led to a data-centric intelligence i.e.\nartificially intelligent models unlocking the potential to ingest a large\namount of data and be really good at performing digital tasks such as\ntext-to-image generation, machine-human conversation, and image recognition.\nThis thesis covers the topic of learning with structured inductive bias and\npriors to design approaches and algorithms unlocking the potential of\nprinciple-centric intelligence. Prior knowledge (priors for short), often\navailable in terms of past experience as well as assumptions of how the world\nworks, helps the autonomous agent generalize better and adapt their behavior\nbased on past experience. In this thesis, I demonstrate the use of prior\nknowledge in three different robotics perception problems. 1. object-centric 3D\nreconstruction, 2. vision and language for decision-making, and 3. 3D scene\nunderstanding. To solve these challenging problems, I propose various sources\nof prior knowledge including 1. geometry and appearance priors from synthetic\ndata, 2. modularity and semantic map priors and 3. semantic, structural, and\ncontextual priors. I study these priors for solving robotics 3D perception\ntasks and propose ways to efficiently encode them in deep learning models. Some\npriors are used to warm-start the network for transfer learning, others are\nused as hard constraints to restrict the action space of robotics agents. While\nclassical techniques are brittle and fail to generalize to unseen scenarios and\ndata-centric approaches require a large amount of labeled data, this thesis\naims to build intelligent agents which require very-less real-world data or\ndata acquired only from simulation to generalize to highly dynamic and\ncluttered environments in novel simulations (i.e. sim2sim) or real-world unseen\nenvironments (i.e. sim2real) for a holistic scene understanding of the 3D\nworld.",
      "tldr_zh": "本论文探讨使用诱导先验（inductive priors）来提升3D机器人感知的学习方法，旨在通过结构化知识帮助智能体在少量真实数据或模拟数据基础上实现泛化。作者针对三个关键问题——对象中心3D重建、视觉和语言决策以及3D场景理解——提出多种先验来源，包括从合成数据中获得的几何和外观先验、模块性和语义地图先验，以及语义、结构和上下文先验，并将其高效编码到深度学习模型中。实验结果表明，这种方法相较于传统技术和数据中心方法，能显著减少对标注数据的依赖，实现从模拟到真实（sim2real）或模拟到模拟（sim2sim）的泛化，提高了机器人感知在动态环境中的鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Georgia Tech Ph.D. Thesis, December 2023. For more details:\n  https://zubairirshad.com/",
      "pdf_url": "http://arxiv.org/pdf/2405.20364v1",
      "published_date": "2024-05-30 17:59:51 UTC",
      "updated_date": "2024-05-30 17:59:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:56:55.888099"
    },
    {
      "arxiv_id": "2405.20337v1",
      "title": "OccSora: 4D Occupancy Generation Models as World Simulators for Autonomous Driving",
      "title_zh": "翻译失败",
      "authors": [
        "Lening Wang",
        "Wenzhao Zheng",
        "Yilong Ren",
        "Han Jiang",
        "Zhiyong Cui",
        "Haiyang Yu",
        "Jiwen Lu"
      ],
      "abstract": "Understanding the evolution of 3D scenes is important for effective\nautonomous driving. While conventional methods mode scene development with the\nmotion of individual instances, world models emerge as a generative framework\nto describe the general scene dynamics. However, most existing methods adopt an\nautoregressive framework to perform next-token prediction, which suffer from\ninefficiency in modeling long-term temporal evolutions. To address this, we\npropose a diffusion-based 4D occupancy generation model, OccSora, to simulate\nthe development of the 3D world for autonomous driving. We employ a 4D scene\ntokenizer to obtain compact discrete spatial-temporal representations for 4D\noccupancy input and achieve high-quality reconstruction for long-sequence\noccupancy videos. We then learn a diffusion transformer on the spatial-temporal\nrepresentations and generate 4D occupancy conditioned on a trajectory prompt.\nWe conduct extensive experiments on the widely used nuScenes dataset with Occ3D\noccupancy annotations. OccSora can generate 16s-videos with authentic 3D layout\nand temporal consistency, demonstrating its ability to understand the spatial\nand temporal distributions of driving scenes. With trajectory-aware 4D\ngeneration, OccSora has the potential to serve as a world simulator for the\ndecision-making of autonomous driving. Code is available at:\nhttps://github.com/wzzheng/OccSora.",
      "tldr_zh": "这篇论文提出了OccSora，一种基于扩散的4D Occupancy生成模型，作为自动驾驶的世界模拟器，以更高效地理解3D场景的演变，解决传统自回归框架在长期时间序列建模中的效率问题。该模型采用4D场景tokenizer来获取紧凑的离散时空表示，并通过扩散transformer根据轨迹提示生成高质量的4D Occupancy视频。在nuScenes数据集上的实验显示，OccSora能生成16秒视频，实现了真实的3D布局和时间一致性，并展示出作为自动驾驶决策模拟器的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Code is available at: https://github.com/wzzheng/OccSora",
      "pdf_url": "http://arxiv.org/pdf/2405.20337v1",
      "published_date": "2024-05-30 17:59:42 UTC",
      "updated_date": "2024-05-30 17:59:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:57:06.420755"
    },
    {
      "arxiv_id": "2405.20331v2",
      "title": "CoSy: Evaluating Textual Explanations of Neurons",
      "title_zh": "CoSy：评估神经元的文本解释",
      "authors": [
        "Laura Kopf",
        "Philine Lou Bommer",
        "Anna Hedström",
        "Sebastian Lapuschkin",
        "Marina M. -C. Höhne",
        "Kirill Bykov"
      ],
      "abstract": "A crucial aspect of understanding the complex nature of Deep Neural Networks\n(DNNs) is the ability to explain learned concepts within their latent\nrepresentations. While methods exist to connect neurons to human-understandable\ntextual descriptions, evaluating the quality of these explanations is\nchallenging due to the lack of a unified quantitative approach. We introduce\nCoSy (Concept Synthesis), a novel, architecture-agnostic framework for\nevaluating textual explanations of latent neurons. Given textual explanations,\nour proposed framework uses a generative model conditioned on textual input to\ncreate data points representing the explanations. By comparing the neuron's\nresponse to these generated data points and control data points, we can\nestimate the quality of the explanation. We validate our framework through\nsanity checks and benchmark various neuron description methods for Computer\nVision tasks, revealing significant differences in quality.",
      "tldr_zh": "这篇论文提出了 CoSy（Concept Synthesis），一个新的架构无关框架，用于评估 Deep Neural Networks (DNNs) 中神经元的文本解释质量，以解决缺乏统一量化方法的挑战。框架通过使用生成模型基于文本输入合成数据点，然后比较神经元对这些数据点和对照数据点的响应，来量化解释的准确性。作者通过健全性检查和基准测试验证了框架的有效性，并在计算机视觉任务中比较了多种神经元描述方法，揭示了它们之间的显著质量差异。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.20331v2",
      "published_date": "2024-05-30 17:59:04 UTC",
      "updated_date": "2024-12-05 15:48:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:57:17.233627"
    },
    {
      "arxiv_id": "2405.20330v3",
      "title": "OmniHands: Towards Robust 4D Hand Mesh Recovery via A Versatile Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Dixuan Lin",
        "Yuxiang Zhang",
        "Mengcheng Li",
        "Yebin Liu",
        "Wei Jing",
        "Qi Yan",
        "Qianying Wang",
        "Hongwen Zhang"
      ],
      "abstract": "In this paper, we introduce OmniHands, a universal approach to recovering\ninteractive hand meshes and their relative movement from monocular or\nmulti-view inputs. Our approach addresses two major limitations of previous\nmethods: lacking a unified solution for handling various hand image inputs and\nneglecting the positional relationship of two hands within images. To overcome\nthese challenges, we develop a universal architecture with novel tokenization\nand contextual feature fusion strategies, capable of adapting to a variety of\ntasks. Specifically, we propose a Relation-aware Two-Hand Tokenization (RAT)\nmethod to embed positional relation information into the hand tokens. In this\nway, our network can handle both single-hand and two-hand inputs and explicitly\nleverage relative hand positions, facilitating the reconstruction of intricate\nhand interactions in real-world scenarios. As such tokenization indicates the\nrelative relationship of two hands, it also supports more effective feature\nfusion. To this end, we further develop a 4D Interaction Reasoning (FIR) module\nto fuse hand tokens in 4D with attention and decode them into 3D hand meshes\nand relative temporal movements. The efficacy of our approach is validated on\nseveral benchmark datasets. The results on in-the-wild videos and real-world\nscenarios demonstrate the superior performances of our approach for interactive\nhand reconstruction. More video results can be found on the project page:\nhttps://OmniHand.github.io.",
      "tldr_zh": "本文提出 OmniHands，一种基于 Versatile Transformer 的通用方法，用于从单目或多视图输入恢复交互手部网格及其相对运动，解决了现有方法在处理各种输入和双手位置关系方面的局限。核心创新包括 Relation-aware Two-Hand Tokenization (RAT) 方法，该方法嵌入手部位置信息以支持单手和双手重建，以及 4D Interaction Reasoning (FIR) 模块，用于在 4D 空间融合特征并解码成 3D 网格和时间运动。实验结果显示，OmniHands 在多个基准数据集上表现出色，尤其在野外视频和真实场景中实现了鲁棒的交互手部重建。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "An extended journal version of 4DHands, featured with versatile\n  module that can adapt to temporal task and multi-view task. Additional\n  detailed comparison experiments and results presentation have been added.\n  More demo videos can be seen at our project page: https://OmniHand.github.io",
      "pdf_url": "http://arxiv.org/pdf/2405.20330v3",
      "published_date": "2024-05-30 17:59:02 UTC",
      "updated_date": "2024-10-01 15:04:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:57:30.865911"
    },
    {
      "arxiv_id": "2405.20323v1",
      "title": "$\\textit{S}^3$Gaussian: Self-Supervised Street Gaussians for Autonomous Driving",
      "title_zh": "翻译失败",
      "authors": [
        "Nan Huang",
        "Xiaobao Wei",
        "Wenzhao Zheng",
        "Pengju An",
        "Ming Lu",
        "Wei Zhan",
        "Masayoshi Tomizuka",
        "Kurt Keutzer",
        "Shanghang Zhang"
      ],
      "abstract": "Photorealistic 3D reconstruction of street scenes is a critical technique for\ndeveloping real-world simulators for autonomous driving. Despite the efficacy\nof Neural Radiance Fields (NeRF) for driving scenes, 3D Gaussian Splatting\n(3DGS) emerges as a promising direction due to its faster speed and more\nexplicit representation. However, most existing street 3DGS methods require\ntracked 3D vehicle bounding boxes to decompose the static and dynamic elements\nfor effective reconstruction, limiting their applications for in-the-wild\nscenarios. To facilitate efficient 3D scene reconstruction without costly\nannotations, we propose a self-supervised street Gaussian\n($\\textit{S}^3$Gaussian) method to decompose dynamic and static elements from\n4D consistency. We represent each scene with 3D Gaussians to preserve the\nexplicitness and further accompany them with a spatial-temporal field network\nto compactly model the 4D dynamics. We conduct extensive experiments on the\nchallenging Waymo-Open dataset to evaluate the effectiveness of our method. Our\n$\\textit{S}^3$Gaussian demonstrates the ability to decompose static and dynamic\nscenes and achieves the best performance without using 3D annotations. Code is\navailable at: https://github.com/nnanhuang/S3Gaussian/.",
      "tldr_zh": "本论文提出$\\textit{S}^3$Gaussian，一种自监督方法，用于自动驾驶街景的3D重建，旨在解决现有3D Gaussian Splatting (3DGS)方法对3D车辆边界框标注的依赖问题。该方法通过4D一致性分解静态和动态元素，使用3D Gaussians表示场景，并结合空间-时间场网络来高效建模4D动态，从而实现无需标注的精确重建。在Waymo-Open数据集上的实验显示，$\\textit{S}^3$Gaussian在场景分解和重建性能上优于基线模型，并开源代码以促进进一步应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Code is available at: https://github.com/nnanhuang/S3Gaussian/",
      "pdf_url": "http://arxiv.org/pdf/2405.20323v1",
      "published_date": "2024-05-30 17:57:08 UTC",
      "updated_date": "2024-05-30 17:57:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:57:43.326899"
    },
    {
      "arxiv_id": "2405.20320v2",
      "title": "Improving the Training of Rectified Flows",
      "title_zh": "翻译失败",
      "authors": [
        "Sangyun Lee",
        "Zinan Lin",
        "Giulia Fanti"
      ],
      "abstract": "Diffusion models have shown great promise for image and video generation, but\nsampling from state-of-the-art models requires expensive numerical integration\nof a generative ODE. One approach for tackling this problem is rectified flows,\nwhich iteratively learn smooth ODE paths that are less susceptible to\ntruncation error. However, rectified flows still require a relatively large\nnumber of function evaluations (NFEs). In this work, we propose improved\ntechniques for training rectified flows, allowing them to compete with\n\\emph{knowledge distillation} methods even in the low NFE setting. Our main\ninsight is that under realistic settings, a single iteration of the Reflow\nalgorithm for training rectified flows is sufficient to learn nearly straight\ntrajectories; hence, the current practice of using multiple Reflow iterations\nis unnecessary. We thus propose techniques to improve one-round training of\nrectified flows, including a U-shaped timestep distribution and LPIPS-Huber\npremetric. With these techniques, we improve the FID of the previous\n2-rectified flow by up to 75\\% in the 1 NFE setting on CIFAR-10. On ImageNet\n64$\\times$64, our improved rectified flow outperforms the state-of-the-art\ndistillation methods such as consistency distillation and progressive\ndistillation in both one-step and two-step settings and rivals the performance\nof improved consistency training (iCT) in FID. Code is available at\nhttps://github.com/sangyun884/rfpp.",
      "tldr_zh": "本文提出改进 Rectified Flows 的训练方法，以提升扩散模型在图像生成中的采样效率，特别是在低函数评估次数 (NFEs) 场景下。该方法的核心洞见是，使用单个 Reflow 算法迭代即可学习近乎直线的轨迹，因此引入 U-shaped timestep distribution 和 LPIPS-Huber premetric 等技术来优化单轮训练。实验结果显示，在 CIFAR-10 上，该改进在 1 NFE 设置下将 FID 指标提高了 75%；在 ImageNet 64×64 上，它超越了 knowledge distillation 和 progressive distillation 等基准，并在 FID 性能上与 improved consistency training (iCT) 相当。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "NeurIPS2024",
      "pdf_url": "http://arxiv.org/pdf/2405.20320v2",
      "published_date": "2024-05-30 17:56:04 UTC",
      "updated_date": "2024-10-08 21:40:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:57:54.857940"
    },
    {
      "arxiv_id": "2405.20319v2",
      "title": "ParSEL: Parameterized Shape Editing with Language",
      "title_zh": "翻译失败",
      "authors": [
        "Aditya Ganeshan",
        "Ryan Y. Huang",
        "Xianghao Xu",
        "R. Kenny Jones",
        "Daniel Ritchie"
      ],
      "abstract": "The ability to edit 3D assets from natural language presents a compelling\nparadigm to aid in the democratization of 3D content creation. However, while\nnatural language is often effective at communicating general intent, it is\npoorly suited for specifying precise manipulation. To address this gap, we\nintroduce ParSEL, a system that enables controllable editing of high-quality 3D\nassets from natural language. Given a segmented 3D mesh and an editing request,\nParSEL produces a parameterized editing program. Adjusting the program\nparameters allows users to explore shape variations with a precise control over\nthe magnitudes of edits. To infer editing programs which align with an input\nedit request, we leverage the abilities of large-language models (LLMs).\nHowever, while we find that LLMs excel at identifying initial edit operations,\nthey often fail to infer complete editing programs, and produce outputs that\nviolate shape semantics. To overcome this issue, we introduce Analytical Edit\nPropagation (AEP), an algorithm which extends a seed edit with additional\noperations until a complete editing program has been formed. Unlike prior\nmethods, AEP searches for analytical editing operations compatible with a range\nof possible user edits through the integration of computer algebra systems for\ngeometric analysis. Experimentally we demonstrate ParSEL's effectiveness in\nenabling controllable editing of 3D objects through natural language requests\nover alternative system designs.",
      "tldr_zh": "该研究引入了 ParSEL 系统，使用自然语言实现对高质量 3D 资产的可控编辑，解决了自然语言在指定精确操作方面的不足。系统从一个分段的 3D 网格和编辑请求出发，通过大型语言模型 (LLMs) 推断参数化的编辑程序，并采用 Analytical Edit Propagation (AEP) 算法扩展种子编辑，确保程序完整性和形状语义正确性。实验结果显示，ParSEL 在处理自然语言请求的 3D 对象编辑时，比其他系统设计更有效。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.HC",
        "cs.SC"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20319v2",
      "published_date": "2024-05-30 17:55:46 UTC",
      "updated_date": "2024-05-31 04:09:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:58:06.243806"
    },
    {
      "arxiv_id": "2405.20318v3",
      "title": "Quriosity: Analyzing Human Questioning Behavior and Causal Inquiry through Curiosity-Driven Queries",
      "title_zh": "翻译失败",
      "authors": [
        "Roberto Ceraolo",
        "Dmitrii Kharlapenko",
        "Ahmad Khan",
        "Amélie Reymond",
        "Rada Mihalcea",
        "Bernhard Schölkopf",
        "Mrinmaya Sachan",
        "Zhijing Jin"
      ],
      "abstract": "Recent progress in Large Language Model (LLM) technology has changed our role\nin interacting with these models. Instead of primarily testing these models\nwith questions we already know answers to, we are now using them for queries\nwhere the answers are unknown to us, driven by human curiosity. This shift\nhighlights the growing need to understand curiosity-driven human questions -\nthose that are more complex, open-ended, and reflective of real-world needs. To\nthis end, we present Quriosity, a collection of 13.5K naturally occurring\nquestions from three diverse sources: human-to-search-engine queries,\nhuman-to-human interactions, and human-to-LLM conversations. Our comprehensive\ncollection enables a rich understanding of human curiosity across various\ndomains and contexts. Our analysis reveals a significant presence of causal\nquestions (up to 42%) in the dataset, for which we develop an iterative prompt\nimprovement framework to identify all causal queries and examine their unique\nlinguistic properties, cognitive complexity and source distribution. Our paper\npaves the way for future work on causal question identification and open-ended\nchatbot interactions.",
      "tldr_zh": "这篇论文介绍了 Quriosity 数据集，该数据集包含 13.5K 个自然发生的问题，来源于搜索引擎查询、人际互动和人类对 LLM（Large Language Model）的对话，旨在深入分析好奇心驱动的人类提问行为。研究发现，这些问题中高达 42% 为因果问题（causal questions），并开发了迭代提示改进框架来识别所有因果查询，并考察其语言特性、认知复杂性和来源分布。该工作为未来的因果问题识别和开放式聊天机器人互动提供了重要基础。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20318v3",
      "published_date": "2024-05-30 17:55:28 UTC",
      "updated_date": "2025-02-24 16:42:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:58:19.163635"
    },
    {
      "arxiv_id": "2405.20315v1",
      "title": "ANAH: Analytical Annotation of Hallucinations in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ziwei Ji",
        "Yuzhe Gu",
        "Wenwei Zhang",
        "Chengqi Lyu",
        "Dahua Lin",
        "Kai Chen"
      ],
      "abstract": "Reducing the `$\\textit{hallucination}$' problem of Large Language Models\n(LLMs) is crucial for their wide applications. A comprehensive and fine-grained\nmeasurement of the hallucination is the first key step for the governance of\nthis issue but is under-explored in the community. Thus, we present\n$\\textbf{ANAH}$, a bilingual dataset that offers $\\textbf{AN}$alytical\n$\\textbf{A}$nnotation of $\\textbf{H}$allucinations in LLMs within Generative\nQuestion Answering. Each answer sentence in our dataset undergoes rigorous\nannotation, involving the retrieval of a reference fragment, the judgment of\nthe hallucination type, and the correction of hallucinated content. ANAH\nconsists of ~12k sentence-level annotations for ~4.3k LLM responses covering\nover 700 topics, constructed by a human-in-the-loop pipeline. Thanks to the\nfine granularity of the hallucination annotations, we can quantitatively\nconfirm that the hallucinations of LLMs progressively accumulate in the answer\nand use ANAH to train and evaluate hallucination annotators. We conduct\nextensive experiments on studying generative and discriminative annotators and\nshow that, although current open-source LLMs have difficulties in fine-grained\nhallucination annotation, the generative annotator trained with ANAH can\nsurpass all open-source LLMs and GPT-3.5, obtain performance competitive with\nGPT-4, and exhibits better generalization ability on unseen questions.",
      "tldr_zh": "该论文提出ANAH数据集，用于对Large Language Models (LLMs)中的hallucination问题进行细粒度分析和注解，以支持生成问答任务的优化。ANAH是一个双语数据集，包含约12k句级注解，针对约4.3k个LLM响应和700多个主题，通过人类介入管道实现参考片段检索、hallucination类型判断及内容修正。研究发现，LLMs的hallucination会在答案中逐步积累，并通过ANAH训练的生成式annotator超越了开源LLMs和GPT-3.5，其性能与GPT-4相当，并显示出更好的泛化能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.20315v1",
      "published_date": "2024-05-30 17:54:40 UTC",
      "updated_date": "2024-05-30 17:54:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:58:31.134731"
    },
    {
      "arxiv_id": "2405.20309v2",
      "title": "Large Language Models Can Self-Improve At Web Agent Tasks",
      "title_zh": "大语言模型可以在网络代理任务中自我改进",
      "authors": [
        "Ajay Patel",
        "Markus Hofmarcher",
        "Claudiu Leoveanu-Condrei",
        "Marius-Constantin Dinu",
        "Chris Callison-Burch",
        "Sepp Hochreiter"
      ],
      "abstract": "Training models to act as agents that can effectively navigate and perform\nactions in a complex environment, such as a web browser, has typically been\nchallenging due to lack of training data. Large language models (LLMs) have\nrecently demonstrated some capability to navigate novel environments as agents\nin a zero-shot or few-shot fashion, purely guided by natural language\ninstructions as prompts. Recent research has also demonstrated LLMs have the\ncapability to exceed their base performance through self-improvement, i.e.\nfine-tuning on data generated by the model itself. In this work, we explore the\nextent to which LLMs can self-improve their performance as agents in\nlong-horizon tasks in a complex environment using the WebArena benchmark. In\nWebArena, an agent must autonomously navigate and perform actions on web pages\nto achieve a specified objective. We explore fine-tuning on three distinct\nsynthetic training data mixtures and achieve a 31\\% improvement in task\ncompletion rate over the base model on the WebArena benchmark through a\nself-improvement procedure. We additionally contribute novel evaluation metrics\nfor assessing the performance, robustness, capabilities, and quality of\ntrajectories of our fine-tuned agent models to a greater degree than simple,\naggregate-level benchmark scores currently used to measure self-improvement.",
      "tldr_zh": "本研究探讨大型语言模型 (LLMs) 在网络代理任务中的自我改进能力，通过在模型自身生成的数据上进行 fine-tuning 来提升性能。研究者使用三种合成训练数据混合，并在 WebArena 基准上测试，实现了任务完成率 31% 的显著改进。论文还贡献了新的评估指标，包括性能、鲁棒性、能力和轨迹质量，以更全面地衡量代理模型的效果。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20309v2",
      "published_date": "2024-05-30 17:52:36 UTC",
      "updated_date": "2024-10-01 21:28:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:58:42.319954"
    },
    {
      "arxiv_id": "2405.20289v1",
      "title": "DITTO-2: Distilled Diffusion Inference-Time T-Optimization for Music Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Zachary Novack",
        "Julian McAuley",
        "Taylor Berg-Kirkpatrick",
        "Nicholas Bryan"
      ],
      "abstract": "Controllable music generation methods are critical for human-centered\nAI-based music creation, but are currently limited by speed, quality, and\ncontrol design trade-offs. Diffusion Inference-Time T-optimization (DITTO), in\nparticular, offers state-of-the-art results, but is over 10x slower than\nreal-time, limiting practical use. We propose Distilled Diffusion\nInference-Time T -Optimization (or DITTO-2), a new method to speed up\ninference-time optimization-based control and unlock faster-than-real-time\ngeneration for a wide-variety of applications such as music inpainting,\noutpainting, intensity, melody, and musical structure control. Our method works\nby (1) distilling a pre-trained diffusion model for fast sampling via an\nefficient, modified consistency or consistency trajectory distillation process\n(2) performing inference-time optimization using our distilled model with\none-step sampling as an efficient surrogate optimization task and (3) running a\nfinal multi-step sampling generation (decoding) using our estimated noise\nlatents for best-quality, fast, controllable generation. Through thorough\nevaluation, we find our method not only speeds up generation over 10-20x, but\nsimultaneously improves control adherence and generation quality all at once.\nFurthermore, we apply our approach to a new application of maximizing text\nadherence (CLAP score) and show we can convert an unconditional diffusion model\nwithout text inputs into a model that yields state-of-the-art text control.\nSound examples can be found at https://ditto-music.github.io/ditto2/.",
      "tldr_zh": "该论文提出 DITTO-2 方法，通过蒸馏预训练的 Diffusion 模型来加速 Inference-Time T-Optimization，实现更快-than-real-time 的可控音乐生成，解决现有方法的速度、质量和控制权衡问题。DITTO-2 的核心流程包括：使用高效的修改一致性轨迹蒸馏进行快速采样、以一阶采样作为代理优化任务，以及基于估计噪声潜在变量进行最终多步采样生成。实验结果显示，该方法使生成速度提高 10-20 倍，同时提升控制遵守性和生成质量；此外，它还能将无条件 Diffusion 模型转化为具有出色文本控制的模型，提升 CLAP score。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20289v1",
      "published_date": "2024-05-30 17:40:11 UTC",
      "updated_date": "2024-05-30 17:40:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:58:55.564921"
    },
    {
      "arxiv_id": "2405.20287v1",
      "title": "Flexible SE(2) graph neural networks with applications to PDE surrogates",
      "title_zh": "灵活的 SE(2) 图神经网络及其在 PDE 代理模型中的应用",
      "authors": [
        "Maria Bånkestad",
        "Olof Mogren",
        "Aleksis Pirinen"
      ],
      "abstract": "This paper presents a novel approach for constructing graph neural networks\nequivariant to 2D rotations and translations and leveraging them as PDE\nsurrogates on non-gridded domains. We show that aligning the representations\nwith the principal axis allows us to sidestep many constraints while preserving\nSE(2) equivariance. By applying our model as a surrogate for fluid flow\nsimulations and conducting thorough benchmarks against non-equivariant models,\nwe demonstrate significant gains in terms of both data efficiency and accuracy.",
      "tldr_zh": "这篇论文提出了一种灵活的SE(2)图神经网络方法，用于构建对2D旋转和平移等价的模型，并将其作为非网格域上的PDE代理模型。核心创新是通过与主轴对齐表示来绕过传统约束，同时保持SE(2)等价性。该方法应用于流体流动模拟的基准测试中，与非等价模型相比，显著提高了数据效率和准确性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NA",
        "math.NA",
        "physics.flu-dyn"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages",
      "pdf_url": "http://arxiv.org/pdf/2405.20287v1",
      "published_date": "2024-05-30 17:39:15 UTC",
      "updated_date": "2024-05-30 17:39:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:59:05.114967"
    },
    {
      "arxiv_id": "2405.20279v2",
      "title": "CV-VAE: A Compatible Video VAE for Latent Generative Video Models",
      "title_zh": "翻译失败",
      "authors": [
        "Sijie Zhao",
        "Yong Zhang",
        "Xiaodong Cun",
        "Shaoshu Yang",
        "Muyao Niu",
        "Xiaoyu Li",
        "Wenbo Hu",
        "Ying Shan"
      ],
      "abstract": "Spatio-temporal compression of videos, utilizing networks such as Variational\nAutoencoders (VAE), plays a crucial role in OpenAI's SORA and numerous other\nvideo generative models. For instance, many LLM-like video models learn the\ndistribution of discrete tokens derived from 3D VAEs within the VQVAE\nframework, while most diffusion-based video models capture the distribution of\ncontinuous latent extracted by 2D VAEs without quantization. The temporal\ncompression is simply realized by uniform frame sampling which results in\nunsmooth motion between consecutive frames. Currently, there lacks of a\ncommonly used continuous video (3D) VAE for latent diffusion-based video models\nin the research community. Moreover, since current diffusion-based approaches\nare often implemented using pre-trained text-to-image (T2I) models, directly\ntraining a video VAE without considering the compatibility with existing T2I\nmodels will result in a latent space gap between them, which will take huge\ncomputational resources for training to bridge the gap even with the T2I models\nas initialization. To address this issue, we propose a method for training a\nvideo VAE of latent video models, namely CV-VAE, whose latent space is\ncompatible with that of a given image VAE, e.g., image VAE of Stable Diffusion\n(SD). The compatibility is achieved by the proposed novel latent space\nregularization, which involves formulating a regularization loss using the\nimage VAE. Benefiting from the latent space compatibility, video models can be\ntrained seamlessly from pre-trained T2I or video models in a truly\nspatio-temporally compressed latent space, rather than simply sampling video\nframes at equal intervals. With our CV-VAE, existing video models can generate\nfour times more frames with minimal finetuning. Extensive experiments are\nconducted to demonstrate the effectiveness of the proposed video VAE.",
      "tldr_zh": "这篇论文针对视频生成模型中的视频 VAE 兼容性问题，提出了 CV-VAE，一种兼容的视频 VAE，用于潜在生成视频模型。\nCV-VAE 通过新型潜在空间正则化（latent space regularization）方法，确保其潜在空间与现有图像 VAE（如 Stable Diffusion 的 VAE）兼容，从而避免训练时巨大的计算资源消耗。\n这种设计允许视频模型从预训练的文本到图像 (T2I) 模型无缝过渡，并在真正的时空压缩潜在空间中训练，而非简单帧采样。\n实验结果表明，使用 CV-VAE，现有视频模型只需最小微调即可生成四倍的帧，显著提升了视频生成效率和质量。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://ailab-cvc.github.io/cvvae/index.html",
      "pdf_url": "http://arxiv.org/pdf/2405.20279v2",
      "published_date": "2024-05-30 17:33:10 UTC",
      "updated_date": "2024-10-23 02:38:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:59:19.681957"
    },
    {
      "arxiv_id": "2405.20278v2",
      "title": "Length independent generalization bounds for deep SSM architectures",
      "title_zh": "针对深度 SSM 架构的长度无关泛化界限",
      "authors": [
        "Dániel Rácz",
        "Mihály Petreczky",
        "Bálint Daróczy"
      ],
      "abstract": "Many state-of-the-art models trained on long-range sequences, for example S4,\nS5 or LRU, are made of sequential blocks combining State-Space Models (SSMs)\nwith neural networks. In this paper we provide a PAC bound that holds for these\nkind of architectures with stable SSM blocks and does not depend on the length\nof the input sequence. Imposing stability of the SSM blocks is a standard\npractice in the literature, and it is known to help performance. Our results\nprovide a theoretical justification for the use of stable SSM blocks as the\nproposed PAC bound decreases as the degree of stability of the SSM blocks\nincreases.",
      "tldr_zh": "本文针对深度状态空间模型 (SSM) 架构（如 S4、S5 和 LRU），提出了一个不依赖输入序列长度的 PAC bound，用于评估这些结合 SSM 和神经网络的模型。研究假设 SSM 块是稳定的，并证明这种稳定性能降低 PAC bound，从而为稳定 SSM 块的实际应用提供理论依据。该方法强调了稳定 SSM 块的重要性，因为它有助于提升模型的性能和泛化能力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML",
        "68",
        "I.2.6"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, no figures, accepted at ICML 2024 Next Generation of\n  Sequence Modeling Architectures Workshop",
      "pdf_url": "http://arxiv.org/pdf/2405.20278v2",
      "published_date": "2024-05-30 17:32:46 UTC",
      "updated_date": "2024-07-11 07:55:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:59:31.030245"
    },
    {
      "arxiv_id": "2405.20274v2",
      "title": "ROAST: Review-level Opinion Aspect Sentiment Target Joint Detection for ABSA",
      "title_zh": "翻译失败",
      "authors": [
        "Siva Uday Sampreeth Chebolu",
        "Franck Dernoncourt",
        "Nedim Lipka",
        "Thamar Solorio"
      ],
      "abstract": "Aspect-Based Sentiment Analysis (ABSA) has experienced tremendous expansion\nand diversity due to various shared tasks spanning several languages and fields\nand organized via SemEval workshops and Germeval. Nonetheless, a few\nshortcomings still need to be addressed, such as the lack of low-resource\nlanguage evaluations and the emphasis on sentence-level analysis. To thoroughly\nassess ABSA techniques in the context of complete reviews, this research\npresents a novel task, Review-Level Opinion Aspect Sentiment Target (ROAST).\nROAST seeks to close the gap between sentence-level and text-level ABSA by\nidentifying every ABSA constituent at the review level. We extend the available\ndatasets to enable ROAST, addressing the drawbacks noted in previous research\nby incorporating low-resource languages, numerous languages, and a variety of\ntopics. Through this effort, ABSA research will be able to cover more ground\nand get a deeper comprehension of the task and its practical application in a\nvariety of languages and domains (https://github.com/RiTUAL-UH/ROAST-ABSA).",
      "tldr_zh": "本研究针对 Aspect-Based Sentiment Analysis (ABSA) 的局限性，如缺乏低资源语言评估和过度依赖句子级分析，提出一个新任务 Review-Level Opinion Aspect Sentiment Target (ROAST)。ROAST 在评论级别上联合检测意见、方面、情感和目标，从而弥合句子级与文本级 ABSA 的差距。研究团队扩展了现有数据集，涵盖低资源语言、多语言和多种主题，以提升 ABSA 的全面性和实际应用潜力。最终，这有助于 ABSA 研究更深入地理解任务在不同语言和领域中的表现（相关代码见 GitHub）。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "arXiv admin note: text overlap with arXiv:2309.13297",
      "pdf_url": "http://arxiv.org/pdf/2405.20274v2",
      "published_date": "2024-05-30 17:29:15 UTC",
      "updated_date": "2024-07-18 18:05:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:59:42.442840"
    },
    {
      "arxiv_id": "2405.20247v3",
      "title": "KerasCV and KerasNLP: Vision and Language Power-Ups",
      "title_zh": "翻译失败",
      "authors": [
        "Matthew Watson",
        "Divyashree Shivakumar Sreepathihalli",
        "Francois Chollet",
        "Martin Gorner",
        "Kiranbir Sodhia",
        "Ramesh Sampath",
        "Tirth Patel",
        "Haifeng Jin",
        "Neel Kovelamudi",
        "Gabriel Rasskin",
        "Samaneh Saadat",
        "Luke Wood",
        "Chen Qian",
        "Jonathan Bischof",
        "Ian Stenbit",
        "Abheesht Sharma",
        "Anshuman Mishra"
      ],
      "abstract": "We present the Keras domain packages KerasCV and KerasNLP, extensions of the\nKeras API for Computer Vision and Natural Language Processing workflows,\ncapable of running on either JAX, TensorFlow, or PyTorch. These domain packages\nare designed to enable fast experimentation, with a focus on ease-of-use and\nperformance. We adopt a modular, layered design: at the library's lowest level\nof abstraction, we provide building blocks for creating models and data\npreprocessing pipelines, and at the library's highest level of abstraction, we\nprovide pretrained ``task\" models for popular architectures such as Stable\nDiffusion, YOLOv8, GPT2, BERT, Mistral, CLIP, Gemma, T5, etc. Task models have\nbuilt-in preprocessing, pretrained weights, and can be fine-tuned on raw\ninputs. To enable efficient training, we support XLA compilation for all\nmodels, and run all preprocessing via a compiled graph of TensorFlow operations\nusing the tf.data API. The libraries are fully open-source (Apache 2.0 license)\nand available on GitHub.",
      "tldr_zh": "本论文介绍了 KerasCV 和 KerasNLP，这两个 Keras API 的扩展，用于简化计算机视觉和自然语言处理的工作流程，支持 JAX、TensorFlow 或 PyTorch 后端。它们采用模块化分层设计，提供底层构建块来创建模型和数据预处理管道，以及高层预训练“任务”模型，如 Stable Diffusion、YOLOv8、GPT2 和 BERT 等，这些模型内置预处理和权重，可直接在原始输入上微调。库通过 XLA 编译和 tf.data API 实现高效训练，并以 Apache 2.0 许可开源，促进快速实验和性能优化。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.SE",
        "I.2.5; I.2.7; I.2.10"
      ],
      "primary_category": "cs.AI",
      "comment": "Submitted to Journal of Machine Learning Open Source Software",
      "pdf_url": "http://arxiv.org/pdf/2405.20247v3",
      "published_date": "2024-05-30 16:58:34 UTC",
      "updated_date": "2024-06-05 07:52:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T13:59:54.866706"
    },
    {
      "arxiv_id": "2405.20245v1",
      "title": "Retrieval Augmented Structured Generation: Business Document Information Extraction As Tool Use",
      "title_zh": "翻译失败",
      "authors": [
        "Franz Louis Cesista",
        "Rui Aguiar",
        "Jason Kim",
        "Paolo Acilo"
      ],
      "abstract": "Business Document Information Extraction (BDIE) is the problem of\ntransforming a blob of unstructured information (raw text, scanned documents,\netc.) into a structured format that downstream systems can parse and use. It\nhas two main tasks: Key-Information Extraction (KIE) and Line Items Recognition\n(LIR). In this paper, we argue that BDIE is best modeled as a Tool Use problem,\nwhere the tools are these downstream systems. We then present Retrieval\nAugmented Structured Generation (RASG), a novel general framework for BDIE that\nachieves state of the art (SOTA) results on both KIE and LIR tasks on BDIE\nbenchmarks.\n  The contributions of this paper are threefold: (1) We show, with ablation\nbenchmarks, that Large Language Models (LLMs) with RASG are already competitive\nwith or surpasses current SOTA Large Multimodal Models (LMMs) without RASG on\nBDIE benchmarks. (2) We propose a new metric class for Line Items Recognition,\nGeneral Line Items Recognition Metric (GLIRM), that is more aligned with\npractical BDIE use cases compared to existing metrics, such as ANLS*, DocILE,\nand GriTS. (3) We provide a heuristic algorithm for backcalculating bounding\nboxes of predicted line items and tables without the need for vision encoders.\nFinally, we claim that, while LMMs might sometimes offer marginal performance\nbenefits, LLMs + RASG is oftentimes superior given real-world applications and\nconstraints of BDIE.",
      "tldr_zh": "本论文将商业文档信息提取（BDIE）视为工具使用问题，提出了一种新框架 Retrieval Augmented Structured Generation (RASG)，用于将无结构信息（如原始文本或扫描文档）转化为结构化格式，并在 Key-Information Extraction (KIE) 和 Line Items Recognition (LIR) 任务上达到最先进（SOTA）性能。  \n通过消融实验，论文证明 Large Language Models (LLMs) 结合 RASG 已能与或超越当前 SOTA 的 Large Multimodal Models (LMMs)。  \n此外，论文贡献包括提出新的评估指标 General Line Items Recognition Metric (GLIRM)，其更符合实际 BDIE 应用，以及一个启发式算法用于回算预测线项和表格的边界框，而无需视觉编码器。  \n总体而言，论文主张在实际约束下，LLMs + RASG 通常优于 LMMs，提供更高效的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by IEEE 7th International Conference on Multimedia\n  Information Processing and Retrieval (MIPR), 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.20245v1",
      "published_date": "2024-05-30 16:54:42 UTC",
      "updated_date": "2024-05-30 16:54:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:00:09.114358"
    },
    {
      "arxiv_id": "2405.20237v1",
      "title": "Training-efficient density quantum machine learning",
      "title_zh": "训练高效的密度量子机器学习",
      "authors": [
        "Brian Coyle",
        "El Amine Cherrat",
        "Nishant Jain",
        "Natansh Mathur",
        "Snehal Raj",
        "Skander Kazdaghli",
        "Iordanis Kerenidis"
      ],
      "abstract": "Quantum machine learning requires powerful, flexible and efficiently\ntrainable models to be successful in solving challenging problems. In this\nwork, we present density quantum neural networks, a learning model\nincorporating randomisation over a set of trainable unitaries. These models\ngeneralise quantum neural networks using parameterised quantum circuits, and\nallow a trade-off between expressibility and efficient trainability,\nparticularly on quantum hardware. We demonstrate the flexibility of the\nformalism by applying it to two recently proposed model families. The first are\ncommuting-block quantum neural networks (QNNs) which are efficiently trainable\nbut may be limited in expressibility. The second are orthogonal (Hamming-weight\npreserving) quantum neural networks which provide well-defined and\ninterpretable transformations on data but are challenging to train at scale on\nquantum devices. Density commuting QNNs improve capacity with minimal gradient\ncomplexity overhead, and density orthogonal neural networks admit a\nquadratic-to-constant gradient query advantage with minimal to no performance\nloss. We conduct numerical experiments on synthetic translationally invariant\ndata and MNIST image data with hyperparameter optimisation to support our\nfindings. Finally, we discuss the connection to post-variational quantum neural\nnetworks, measurement-based quantum machine learning and the dropout mechanism.",
      "tldr_zh": "本研究提出了一种训练高效的密度量子神经网络（density quantum neural networks），通过在可训练单体单元上引入随机化，实现量子机器学习模型在表达性和训练效率之间的权衡，尤其适用于量子硬件。相比传统量子神经网络（QNNs），该模型扩展了 commuting-block QNNs 和 orthogonal QNNs 等框架，提高了容量并降低了梯度复杂性，例如 density commuting QNNs 仅需最小开销即可提升性能，而 density orthogonal neural networks 实现了梯度查询从二次方到常数的优势，几乎无性能损失。实验在合成平移不变数据和 MNIST 图像数据集上验证了这些改进，支持了模型的灵活性和有效性。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "17 pages main text, 9 pages appendices. 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.20237v1",
      "published_date": "2024-05-30 16:40:28 UTC",
      "updated_date": "2024-05-30 16:40:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:00:21.235533"
    },
    {
      "arxiv_id": "2405.20234v3",
      "title": "Hidden in Plain Sight: Exploring Chat History Tampering in Interactive Language Models",
      "title_zh": "隐藏在显眼之处：探索交互式语言模型中的聊天历史篡改",
      "authors": [
        "Cheng'an Wei",
        "Yue Zhao",
        "Yujia Gong",
        "Kai Chen",
        "Lu Xiang",
        "Shenchen Zhu"
      ],
      "abstract": "Large Language Models (LLMs) such as ChatGPT and Llama have become prevalent\nin real-world applications, exhibiting impressive text generation performance.\nLLMs are fundamentally developed from a scenario where the input data remains\nstatic and unstructured. To behave interactively, LLM-based chat systems must\nintegrate prior chat history as context into their inputs, following a\npre-defined structure. However, LLMs cannot separate user inputs from context,\nenabling chat history tampering. This paper introduces a systematic methodology\nto inject user-supplied history into LLM conversations without any prior\nknowledge of the target model. The key is to utilize prompt templates that can\nwell organize the messages to be injected, leading the target LLM to interpret\nthem as genuine chat history. To automatically search for effective templates\nin a WebUI black-box setting, we propose the LLM-Guided Genetic Algorithm\n(LLMGA) that leverages an LLM to generate and iteratively optimize the\ntemplates. We apply the proposed method to popular real-world LLMs including\nChatGPT and Llama-2/3. The results show that chat history tampering can enhance\nthe malleability of the model's behavior over time and greatly influence the\nmodel output. For example, it can improve the success rate of disallowed\nresponse elicitation up to 97% on ChatGPT. Our findings provide insights into\nthe challenges associated with the real-world deployment of interactive LLMs.",
      "tldr_zh": "本文探讨了互动式大型语言模型(LLMs)中的聊天历史篡改问题，即攻击者通过注入虚假历史来操纵模型行为，而LLMs无法有效区分真实上下文。研究提出了一种系统方法，利用提示模板组织注入消息，并开发了LLM-Guided Genetic Algorithm (LLMGA)算法，在黑箱设置下自动优化这些模板。实验结果显示，在ChatGPT和Llama-2/3等模型上，篡改能将禁用响应诱导的成功率提高至97%，从而揭示了部署互动LLMs的潜在安全挑战。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20234v3",
      "published_date": "2024-05-30 16:36:47 UTC",
      "updated_date": "2024-09-06 02:41:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:00:34.133656"
    },
    {
      "arxiv_id": "2405.20233v2",
      "title": "Grokfast: Accelerated Grokking by Amplifying Slow Gradients",
      "title_zh": "翻译失败",
      "authors": [
        "Jaerin Lee",
        "Bong Gyun Kang",
        "Kihoon Kim",
        "Kyoung Mu Lee"
      ],
      "abstract": "One puzzling artifact in machine learning dubbed grokking is where delayed\ngeneralization is achieved tenfolds of iterations after near perfect\noverfitting to the training data. Focusing on the long delay itself on behalf\nof machine learning practitioners, our goal is to accelerate generalization of\na model under grokking phenomenon. By regarding a series of gradients of a\nparameter over training iterations as a random signal over time, we can\nspectrally decompose the parameter trajectories under gradient descent into two\ncomponents: the fast-varying, overfitting-yielding component and the\nslow-varying, generalization-inducing component. This analysis allows us to\naccelerate the grokking phenomenon more than $\\times 50$ with only a few lines\nof code that amplifies the slow-varying components of gradients. The\nexperiments show that our algorithm applies to diverse tasks involving images,\nlanguages, and graphs, enabling practical availability of this peculiar\nartifact of sudden generalization. Our code is available at\nhttps://github.com/ironjr/grokfast.",
      "tldr_zh": "该论文针对机器学习中的grokking现象——模型在过度拟合训练数据后延迟实现泛化——提出了一种加速方法Grokfast。通过将参数梯度视为随机信号，进行谱分解，将梯度分解为快速变化（导致overfitting）和缓慢变化（促进generalization）的组件，并通过放大缓慢变化的梯度组件，仅需少量代码即可将grokking现象加速超过50倍。实验结果显示，该算法适用于图像、语言和图形的多样任务，提升了这种突发泛化现象的实用性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages, 13 figures. Typo fixed. Project page:\n  https://jaerinlee.com/research/grokfast",
      "pdf_url": "http://arxiv.org/pdf/2405.20233v2",
      "published_date": "2024-05-30 16:35:30 UTC",
      "updated_date": "2024-06-05 15:12:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:00:45.235093"
    },
    {
      "arxiv_id": "2405.20231v3",
      "title": "The Empirical Impact of Neural Parameter Symmetries, or Lack Thereof",
      "title_zh": "神经网络参数对称性的实证影响，或其缺乏",
      "authors": [
        "Derek Lim",
        "Theo Moe Putterman",
        "Robin Walters",
        "Haggai Maron",
        "Stefanie Jegelka"
      ],
      "abstract": "Many algorithms and observed phenomena in deep learning appear to be affected\nby parameter symmetries -- transformations of neural network parameters that do\nnot change the underlying neural network function. These include linear mode\nconnectivity, model merging, Bayesian neural network inference, metanetworks,\nand several other characteristics of optimization or loss-landscapes. However,\ntheoretical analysis of the relationship between parameter space symmetries and\nthese phenomena is difficult. In this work, we empirically investigate the\nimpact of neural parameter symmetries by introducing new neural network\narchitectures that have reduced parameter space symmetries. We develop two\nmethods, with some provable guarantees, of modifying standard neural networks\nto reduce parameter space symmetries. With these new methods, we conduct a\ncomprehensive experimental study consisting of multiple tasks aimed at\nassessing the effect of removing parameter symmetries. Our experiments reveal\nseveral interesting observations on the empirical impact of parameter\nsymmetries; for instance, we observe linear mode connectivity between our\nnetworks without alignment of weight spaces, and we find that our networks\nallow for faster and more effective Bayesian neural network training. Our code\nis available at https://github.com/cptq/asymmetric-networks",
      "tldr_zh": "该研究实证探讨了神经网络参数对称性（neural parameter symmetries）对深度学习现象（如线性模式连通性、linear mode connectivity，和Bayesian神经网络训练）的影响。作者开发了两种方法来修改标准神经网络，减少参数空间对称性，并提供了部分可证明的保证。通过全面实验，研究发现移除这些对称性后，网络在无需对齐权重空间的情况下实现了线性模式连通性，并提升了Bayesian神经网络训练的速度和效果。该工作为理解参数对称性的实际作用提供了新见解，并公开了代码（https://github.com/cptq/asymmetric-networks）。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2024. v2: added / updated some citations. v3 added link to\n  code, and some additional ablations",
      "pdf_url": "http://arxiv.org/pdf/2405.20231v3",
      "published_date": "2024-05-30 16:32:31 UTC",
      "updated_date": "2024-10-15 12:53:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:00:57.699740"
    },
    {
      "arxiv_id": "2405.20222v3",
      "title": "MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model",
      "title_zh": "翻译失败",
      "authors": [
        "Muyao Niu",
        "Xiaodong Cun",
        "Xintao Wang",
        "Yong Zhang",
        "Ying Shan",
        "Yinqiang Zheng"
      ],
      "abstract": "We present MOFA-Video, an advanced controllable image animation method that\ngenerates video from the given image using various additional controllable\nsignals (such as human landmarks reference, manual trajectories, and another\neven provided video) or their combinations. This is different from previous\nmethods which only can work on a specific motion domain or show weak control\nabilities with diffusion prior. To achieve our goal, we design several\ndomain-aware motion field adapters (\\ie, MOFA-Adapters) to control the\ngenerated motions in the video generation pipeline. For MOFA-Adapters, we\nconsider the temporal motion consistency of the video and generate the dense\nmotion flow from the given sparse control conditions first, and then, the\nmulti-scale features of the given image are wrapped as a guided feature for\nstable video diffusion generation. We naively train two motion adapters for the\nmanual trajectories and the human landmarks individually since they both\ncontain sparse information about the control. After training, the MOFA-Adapters\nin different domains can also work together for more controllable video\ngeneration. Project Page: https://myniuuu.github.io/MOFA_Video/",
      "tldr_zh": "本研究提出 MOFA-Video，一种先进的图像动画方法，能够从给定图像生成视频，并通过人体地标参考、手动轨迹或其他视频等可控信号或它们的组合，实现更灵活的控制。不同于以往仅限于特定运动领域或控制能力弱的模型，该方法设计了领域感知运动场适配器（MOFA-Adapters），先从稀疏控制条件生成密集运动流，然后将图像的多尺度特征包裹作为引导特征，确保视频生成过程中的时序一致性和稳定性。研究中，针对手动轨迹和人体地标分别训练了两个 MOFA-Adapters，并在训练后支持不同适配器的组合使用，从而提升视频生成的控制性和整体质量。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ECCV 2024 ; Project Page: https://myniuuu.github.io/MOFA_Video/ ;\n  Codes: https://github.com/MyNiuuu/MOFA-Video",
      "pdf_url": "http://arxiv.org/pdf/2405.20222v3",
      "published_date": "2024-05-30 16:22:22 UTC",
      "updated_date": "2024-07-11 16:26:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:01:10.468879"
    },
    {
      "arxiv_id": "2405.20884v1",
      "title": "Effects of Dataset Sampling Rate for Noise Cancellation through Deep Learning",
      "title_zh": "数据集采样率对通过深度学习噪声消除的影响",
      "authors": [
        "Brandon Colelough",
        "Andrew Zheng"
      ],
      "abstract": "Background: Active noise cancellation has been a subject of research for\ndecades. Traditional techniques, like the Fast Fourier Transform, have\nlimitations in certain scenarios. This research explores the use of deep neural\nnetworks (DNNs) as a superior alternative. Objective: The study aims to\ndetermine the effect sampling rate within training data has on lightweight,\nefficient DNNs that operate within the processing constraints of mobile\ndevices. Methods: We chose the ConvTasNET network for its proven efficiency in\nspeech separation and enhancement. ConvTasNET was trained on datasets such as\nWHAM!, LibriMix, and the MS-2023 DNS Challenge. The datasets were sampled at\nrates of 8kHz, 16kHz, and 48kHz to analyze the effect of sampling rate on noise\ncancellation efficiency and effectiveness. The model was tested on a core-i7\nIntel processor from 2023, assessing the network's ability to produce clear\naudio while filtering out background noise. Results: Models trained at higher\nsampling rates (48kHz) provided much better evaluation metrics against Total\nHarmonic Distortion (THD) and Quality Prediction For Generative Neural Speech\nCodecs (WARP-Q) values, indicating improved audio quality. However, a trade-off\nwas noted with the processing time being longer for higher sampling rates.\nConclusions: The Conv-TasNET network, trained on datasets sampled at higher\nrates like 48kHz, offers a robust solution for mobile devices in achieving\nnoise cancellation through speech separation and enhancement. Future work\ninvolves optimizing the model's efficiency further and testing on mobile\ndevices.",
      "tldr_zh": "本研究探讨了数据集采样率对基于深度学习（DNNs）的噪声消除效果的影响，旨在为轻量级、适合移动设备的模型提供优化指导。研究者使用ConvTasNET网络训练于WHAM!、LibriMix和MS-2023 DNS Challenge数据集，并比较了8kHz、16kHz和48kHz采样率的性能。结果显示，高采样率（如48kHz）显著提升了音频质量，在Total Harmonic Distortion (THD)和WARP-Q指标上表现更好，但同时增加了处理时间。总体而言，该方法为移动设备实现高效噪声消除提供了鲁棒解决方案，并建议未来进一步优化模型效率并在实际设备上测试。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "16 pages, 8 pictures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2405.20884v1",
      "published_date": "2024-05-30 16:20:44 UTC",
      "updated_date": "2024-05-30 16:20:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:01:21.878809"
    },
    {
      "arxiv_id": "2405.20218v1",
      "title": "ESG-FTSE: A corpus of news articles with ESG relevance labels and use cases",
      "title_zh": "ESG-FTSE：一个带有ESG相关性标签和用例的新闻文章语料库",
      "authors": [
        "Mariya Pavlova",
        "Bernard Casey",
        "Miaosen Wang"
      ],
      "abstract": "We present ESG-FTSE, the first corpus comprised of news articles with\nEnvironmental, Social and Governance (ESG) relevance annotations. In recent\nyears, investors and regulators have pushed ESG investing to the mainstream due\nto the urgency of climate change. This has led to the rise of ESG scores to\nevaluate an investment's credentials as socially responsible. While demand for\nESG scores is high, their quality varies wildly. Quantitative techniques can be\napplied to improve ESG scores, thus, responsible investing. To contribute to\nresource building for ESG and financial text mining, we pioneer the ESG-FTSE\ncorpus. We further present the first of its kind ESG annotation schema. It has\nthree levels: a binary classification (relevant versus irrelevant news\narticles), ESG classification (ESG-related news articles), and target company.\nBoth supervised and unsupervised learning experiments for ESG relevance\ndetection were conducted to demonstrate that the corpus can be used in\ndifferent settings to derive accurate ESG predictions. Keywords: corpus\nannotation, ESG labels, annotation schema, news article, natural language\nprocessing",
      "tldr_zh": "该论文介绍了ESG-FTSE语料库，这是首个包含环境、社会和治理(ESG)相关性标注的新闻文章集合，旨在提升ESG投资评估的准确性。研究开发了一个三层标注模式，包括二元分类(相关 vs 不相关)、ESG分类以及目标公司识别，以支持金融文本挖掘。实验结果显示，通过监督和无监督学习，ESG-FTSE语料库在ESG相关性检测中表现出色，提供了一个可靠的资源来改进负责任的投资决策。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "The corpus is available at\n  https://github.com/mariavpavlova/ESG-FTSE-Corpus.\n  https://aclanthology.org/2024.finnlp-1.14/",
      "pdf_url": "http://arxiv.org/pdf/2405.20218v1",
      "published_date": "2024-05-30 16:19:02 UTC",
      "updated_date": "2024-05-30 16:19:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:01:32.790503"
    },
    {
      "arxiv_id": "2405.20216v3",
      "title": "Boost Your Human Image Generation Model via Direct Preference Optimization",
      "title_zh": "通过直接偏好优化提升你的人类图像生成模型",
      "authors": [
        "Sanghyeon Na",
        "Yonggyu Kim",
        "Hyunjoon Lee"
      ],
      "abstract": "Human image generation is a key focus in image synthesis due to its broad\napplications, but even slight inaccuracies in anatomy, pose, or details can\ncompromise realism. To address these challenges, we explore Direct Preference\nOptimization (DPO), which trains models to generate preferred (winning) images\nwhile diverging from non-preferred (losing) ones. However, conventional DPO\nmethods use generated images as winning images, limiting realism. To overcome\nthis limitation, we propose an enhanced DPO approach that incorporates\nhigh-quality real images as winning images, encouraging outputs to resemble\nreal images rather than generated ones. However, implementing this concept is\nnot a trivial task. Therefore, our approach, HG-DPO (Human image Generation\nthrough DPO), employs a novel curriculum learning framework that gradually\nimproves the output of the model toward greater realism, making training more\nfeasible. Furthermore, HG-DPO effectively adapts to personalized text-to-image\ntasks, generating high-quality and identity-specific images, which highlights\nthe practical value of our approach.",
      "tldr_zh": "该研究针对人类图像生成模型的真实性问题（如解剖、姿势或细节 inaccuracies），提出了 HG-DPO 方法，通过 Direct Preference Optimization (DPO) 训练模型生成更偏好的图像。HG-DPO 创新地将高质量真实图像作为 winning 图像，而不是生成的图像，并采用课程学习框架逐步提升输出真实性，使训练更高效。实验结果显示，该方法显著提高了图像生成质量，尤其适用于个性化文本到图像任务，能产生高保真和特定身份的图像。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CVPR 2025 as a highlight paper",
      "pdf_url": "http://arxiv.org/pdf/2405.20216v3",
      "published_date": "2024-05-30 16:18:05 UTC",
      "updated_date": "2025-04-09 06:55:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:01:45.266873"
    },
    {
      "arxiv_id": "2405.20213v1",
      "title": "PostDoc: Generating Poster from a Long Multimodal Document Using Deep Submodular Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Vijay Jaisankar",
        "Sambaran Bandyopadhyay",
        "Kalp Vyas",
        "Varre Chaitanya",
        "Shwetha Somasundaram"
      ],
      "abstract": "A poster from a long input document can be considered as a one-page\neasy-to-read multimodal (text and images) summary presented on a nice template\nwith good design elements. Automatic transformation of a long document into a\nposter is a very less studied but challenging task. It involves content\nsummarization of the input document followed by template generation and\nharmonization. In this work, we propose a novel deep submodular function which\ncan be trained on ground truth summaries to extract multimodal content from the\ndocument and explicitly ensures good coverage, diversity and alignment of text\nand images. Then, we use an LLM based paraphraser and propose to generate a\ntemplate with various design aspects conditioned on the input content. We show\nthe merits of our approach through extensive automated and human evaluations.",
      "tldr_zh": "本论文提出PostDoc方法，利用deep submodular optimization从长多模态文档中自动生成简洁的海报，包括内容总结、模板生成和协调。方法的核心是训练一个新的deep submodular function来提取多模态内容，确保良好的coverage、diversity和text与images的alignment，然后通过LLM-based paraphraser生成设计精美的模板。实验结果显示，该方法在自动化和人工评估中表现出色，证明了其在文档海报转换任务中的有效性。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20213v1",
      "published_date": "2024-05-30 16:16:25 UTC",
      "updated_date": "2024-05-30 16:16:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:01:56.739039"
    },
    {
      "arxiv_id": "2405.20204v2",
      "title": "Jina CLIP: Your CLIP Model Is Also Your Text Retriever",
      "title_zh": "翻译失败",
      "authors": [
        "Andreas Koukounas",
        "Georgios Mastrapas",
        "Michael Günther",
        "Bo Wang",
        "Scott Martens",
        "Isabelle Mohr",
        "Saba Sturua",
        "Mohammad Kalim Akram",
        "Joan Fontanals Martínez",
        "Saahil Ognawala",
        "Susana Guzman",
        "Maximilian Werk",
        "Nan Wang",
        "Han Xiao"
      ],
      "abstract": "Contrastive Language-Image Pretraining (CLIP) is widely used to train models\nto align images and texts in a common embedding space by mapping them to\nfixed-sized vectors. These models are key to multimodal information retrieval\nand related tasks. However, CLIP models generally underperform in text-only\ntasks compared to specialized text models. This creates inefficiencies for\ninformation retrieval systems that keep separate embeddings and models for\ntext-only and multimodal tasks. We propose a novel, multi-task contrastive\ntraining method to address this issue, which we use to train the jina-clip-v1\nmodel to achieve the state-of-the-art performance on both text-image and\ntext-text retrieval tasks.",
      "tldr_zh": "这项研究探讨了Contrastive Language-Image Pretraining (CLIP)模型在多模态信息检索中的应用问题，指出CLIP在纯文本任务上不如专业文本模型，导致系统需要单独的嵌入和模型，造成效率低下。作者提出了一种新颖的多任务对比训练方法，用于训练jina-clip-v1模型，以同时优化文本-图像和文本-文本检索任务。该方法使jina-clip-v1在相关任务上达到了state-of-the-art性能，简化了信息检索系统的设计并提升了整体效率。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.IR",
        "68T50",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "4 pages, MFM-EAI@ICML2024",
      "pdf_url": "http://arxiv.org/pdf/2405.20204v2",
      "published_date": "2024-05-30 16:07:54 UTC",
      "updated_date": "2024-06-26 12:31:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:02:09.615916"
    },
    {
      "arxiv_id": "2405.20202v1",
      "title": "One QuantLLM for ALL: Fine-tuning Quantized LLMs Once for Efficient Deployments",
      "title_zh": "翻译失败",
      "authors": [
        "Ke Yi",
        "Yuhui Xu",
        "Heng Chang",
        "Chen Tang",
        "Yuan Meng",
        "Tong Zhang",
        "Jia Li"
      ],
      "abstract": "Large Language Models (LLMs) have advanced rapidly but face significant\nmemory demands. While quantization has shown promise for LLMs, current methods\ntypically require lengthy training to alleviate the performance degradation\nfrom quantization loss. However, deploying LLMs across diverse scenarios with\ndifferent resource constraints, e.g., servers and personal computers, requires\nrepeated training per application, which amplifies the lengthy training\nproblem. Given that, it is advantageous to train a once-for-all (OFA) supernet\ncapable of yielding diverse optimal subnets for downstream applications through\none-shot training. Nonetheless, the scale of current language models impedes\nefficiency and amplifies interference from weight sharing between subnets. We\nmake an initial attempt to extend the once-for-all framework to large language\nmodels. Specifically, we decouple shared weights to eliminate the interference\nand incorporate Low-Rank adapters for training efficiency. Furthermore, we\nobserve the imbalance allocation of training resources from the traditional\nuniform sampling. A non-parametric scheduler is introduced to adjust the\nsampling rate for each quantization configuration, achieving a more balanced\nallocation among subnets with varying demands. We validate the approach on\nLLaMA2 families, and downstream evaluation confirms our ability to maintain\nhigh performance while significantly reducing deployment time faced with\nmultiple scenarios.",
      "tldr_zh": "该论文提出 One QuantLLM 方法，通过扩展 once-for-all (OFA) 框架，仅需一次微调量化 LLMs，即可为不同资源约束场景（如服务器和个人电脑）生成最佳子网络，解决重复训练的效率问题。具体技术包括解耦共享权重以消除干扰、加入 Low-Rank adapters 提升训练效率，以及引入非参数调度器优化量化配置的采样率以实现资源平衡分配。在 LLaMA2 系列模型上进行下游评估，结果显示该方法显著减少部署时间，同时维持高性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20202v1",
      "published_date": "2024-05-30 16:05:15 UTC",
      "updated_date": "2024-05-30 16:05:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:02:22.111094"
    },
    {
      "arxiv_id": "2405.20189v1",
      "title": "Nadine: An LLM-driven Intelligent Social Robot with Affective Capabilities and Human-like Memory",
      "title_zh": "Nadine：LLM 驱动的智能社交机器人，具有情感能力和类人记忆",
      "authors": [
        "Hangyeol Kang",
        "Maher Ben Moussa",
        "Nadia Magnenat-Thalmann"
      ],
      "abstract": "In this work, we describe our approach to developing an intelligent and\nrobust social robotic system for the Nadine social robot platform. We achieve\nthis by integrating Large Language Models (LLMs) and skilfully leveraging the\npowerful reasoning and instruction-following capabilities of these types of\nmodels to achieve advanced human-like affective and cognitive capabilities.\nThis approach is novel compared to the current state-of-the-art LLM-based\nagents which do not implement human-like long-term memory or sophisticated\nemotional appraisal. The naturalness of social robots, consisting of multiple\nmodules, highly depends on the performance and capabilities of each component\nof the system and the seamless integration of the components. We built a social\nrobot system that enables generating appropriate behaviours through multimodal\ninput processing, bringing episodic memories accordingly to the recognised\nuser, and simulating the emotional states of the robot induced by the\ninteraction with the human partner. In particular, we introduce an LLM-agent\nframe for social robots, SoR-ReAct, serving as a core component for the\ninteraction module in our system. This design has brought forth the advancement\nof social robots and aims to increase the quality of human-robot interaction.",
      "tldr_zh": "本研究开发了Nadine，一种由Large Language Models (LLMs)驱动的智能社交机器人，具备人类-like情感能力(affective capabilities)和长效记忆功能。不同于现有LLM-based代理，该系统通过SoR-ReAct框架作为核心交互模块，实现多模态输入处理、回忆episodic memories和模拟情感状态，从而生成更自然的机器人行为。实验结果表明，这一创新方法显著提升了人机互动的质量和自然性，为社交机器人领域的进步奠定了基础。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20189v1",
      "published_date": "2024-05-30 15:55:41 UTC",
      "updated_date": "2024-05-30 15:55:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:02:34.392562"
    },
    {
      "arxiv_id": "2405.20183v1",
      "title": "A Survey Study on the State of the Art of Programming Exercise Generation using Large Language Models",
      "title_zh": "关于使用大型语言模型的编程练习生成技术现状的调查研究",
      "authors": [
        "Eduard Frankford",
        "Ingo Höhn",
        "Clemens Sauerwein",
        "Ruth Breu"
      ],
      "abstract": "This paper analyzes Large Language Models (LLMs) with regard to their\nprogramming exercise generation capabilities. Through a survey study, we\ndefined the state of the art, extracted their strengths and weaknesses and\nfinally proposed an evaluation matrix, helping researchers and educators to\ndecide which LLM is the best fitting for the programming exercise generation\nuse case. We also found that multiple LLMs are capable of producing useful\nprogramming exercises. Nevertheless, there exist challenges like the ease with\nwhich LLMs might solve exercises generated by LLMs. This paper contributes to\nthe ongoing discourse on the integration of LLMs in education.",
      "tldr_zh": "这篇论文通过调查研究分析了大型语言模型 (LLMs) 在编程练习生成方面的当前状态，提取了其优势（如能够产生有用练习）和劣势（如LLMs可能轻易解决自身生成的练习）。作者提出了一个评估矩阵，帮助研究者和教育者选择最适合的LLMs用于编程练习生成。总体而言，该研究为LLMs在教育领域的整合提供了重要贡献，并突出了潜在挑战。",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "5 pages, 0 figures, CSEE&T 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.20183v1",
      "published_date": "2024-05-30 15:49:34 UTC",
      "updated_date": "2024-05-30 15:49:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:02:45.871476"
    },
    {
      "arxiv_id": "2405.20180v1",
      "title": "Transformers and Slot Encoding for Sample Efficient Physical World Modelling",
      "title_zh": "Transformer 和槽位编码",
      "authors": [
        "Francesco Petri",
        "Luigi Asprino",
        "Aldo Gangemi"
      ],
      "abstract": "World modelling, i.e. building a representation of the rules that govern the\nworld so as to predict its evolution, is an essential ability for any agent\ninteracting with the physical world. Recent applications of the Transformer\narchitecture to the problem of world modelling from video input show notable\nimprovements in sample efficiency. However, existing approaches tend to work\nonly at the image level thus disregarding that the environment is composed of\nobjects interacting with each other. In this paper, we propose an architecture\ncombining Transformers for world modelling with the slot-attention paradigm, an\napproach for learning representations of objects appearing in a scene. We\ndescribe the resulting neural architecture and report experimental results\nshowing an improvement over the existing solutions in terms of sample\nefficiency and a reduction of the variation of the performance over the\ntraining examples. The code for our architecture and experiments is available\nat https://github.com/torchipeppo/transformers-and-slot-encoding-for-wm",
      "tldr_zh": "本文提出了一种结合 Transformers 和 Slot-attention 的神经架构，用于提升物理世界建模的样本效率。该方法从视频输入中学习场景中物体的表示，避免了现有方法仅在图像级别操作而忽略物体交互的问题。实验结果显示，该架构在样本效率上优于现有方案，并减少了性能在训练示例上的变异，为代理与物理世界交互提供了更有效的建模基础。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20180v1",
      "published_date": "2024-05-30 15:48:04 UTC",
      "updated_date": "2024-05-30 15:48:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:02:57.790890"
    },
    {
      "arxiv_id": "2405.20179v3",
      "title": "Robo-Instruct: Simulator-Augmented Instruction Alignment For Finetuning Code LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Zichao Hu",
        "Junyi Jessy Li",
        "Arjun Guha",
        "Joydeep Biswas"
      ],
      "abstract": "Code LLMs have shown promising results with converting tasks in natural\nlanguage to programs that can be executed by service robots. We are interested\nin finetuning small, specialized LLMs for this purpose, but collecting datasets\nof task-program pairs specific to each robot is time-consuming and expensive.\nWhile approaches such as SELF-INSTRUCT and EVOL-INSTRUCT are capable of\ngenerating novel tasks given a few examples, they are unable to provide the\ncorresponding programs that correctly abide by physical-world and\nrobot-constraints using the provided programming interface. Using a simulator\nis a natural potential solution to checking for such constraints, but building\nsimulation environments that can handle arbitrary tasks and their necessary\nobjects and locations, is challenging. To address these challenges, we\nintroduce ROBO-INSTRUCT, which synthesizes task-specific simulation\nenvironments on the fly during program execution, by opportunistically\ninferring entity properties and enforcing corresponding constraints based on\nhow the entities are used in the task program. Additionally, ROBO-INSTRUCT\nintegrates an LLM-aided post-processing procedure to refine instructions for\nbetter alignment with robot programs. We demonstrate the effectiveness of\nROBO-INSTRUCT across multiple LLMs, showing that our fine-tuned models\noutperform all baseline methods and even match or surpass the performance of\nseveral larger and proprietary models.",
      "tldr_zh": "本研究提出 Robo-Instruct，一种基于模拟器增强的指令对齐框架，用于微调 Code LLMs，以将自然语言任务转化为符合机器人物理约束的程序。该框架通过动态合成任务特定模拟环境，实时推断实体属性并强制执行约束，同时整合 LLM 辅助的后处理步骤来优化指令对齐，解决了传统方法如 SELF-INSTRUCT 在生成可执行程序方面的局限性。实验结果显示，Robo-Instruct 微调后的模型在多个 LLM 上超过了基线方法，并在性能上与更大或专有模型相当或更优。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20179v3",
      "published_date": "2024-05-30 15:47:54 UTC",
      "updated_date": "2025-04-11 19:55:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:03:10.313562"
    },
    {
      "arxiv_id": "2405.20175v1",
      "title": "InstructionCP: A fast approach to transfer Large Language Models into target language",
      "title_zh": "InstructionCP：一种快速方法用于将大型语言模型迁移到目标",
      "authors": [
        "Kuang-Ming Chen",
        "Hung-yi Lee"
      ],
      "abstract": "The rapid development of large language models (LLMs) in recent years has\nlargely focused on English, resulting in models that respond exclusively in\nEnglish. To adapt these models to other languages, continual pre-training (CP)\nis often employed, followed by supervised fine-tuning (SFT) to maintain\nconversational abilities. However, CP and SFT can reduce a model's ability to\nfilter harmful content. We propose Instruction Continual Pre-training (InsCP),\nwhich integrates instruction tags into the CP process to prevent loss of\nconversational proficiency while acquiring new languages. Our experiments\ndemonstrate that InsCP retains conversational and Reinforcement Learning from\nHuman Feedback (RLHF) abilities. Empirical evaluations on language alignment,\nreliability, and knowledge benchmarks confirm the efficacy of InsCP. Notably,\nthis approach requires only 0.1 billion tokens of high-quality\ninstruction-following data, thereby reducing resource consumption.",
      "tldr_zh": "该论文针对大型语言模型（Large Language Models, LLMs）从英语转向目标语言时存在的性能下降问题，提出了一种快速方法 Instruction Continual Pre-training (InsCP)。InsCP 通过在持续预训练（CP）过程中整合指令标签，保留模型的对话能力和 Reinforcement Learning from Human Feedback (RLHF) 功能，同时减少有害内容过滤能力的损失。实验结果显示，InsCP 在语言对齐、可靠性和知识基准上表现出色，仅需 0.1 亿高质量指令跟随数据，即可高效实现语言转移并降低资源消耗。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2405.20175v1",
      "published_date": "2024-05-30 15:45:13 UTC",
      "updated_date": "2024-05-30 15:45:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:03:24.176830"
    },
    {
      "arxiv_id": "2405.20172v3",
      "title": "Iterative Feature Boosting for Explainable Speech Emotion Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Alaa Nfissi",
        "Wassim Bouachir",
        "Nizar Bouguila",
        "Brian Mishara"
      ],
      "abstract": "In speech emotion recognition (SER), using predefined features without\nconsidering their practical importance may lead to high dimensional datasets,\nincluding redundant and irrelevant information. Consequently, high-dimensional\nlearning often results in decreasing model accuracy while increasing\ncomputational complexity. Our work underlines the importance of carefully\nconsidering and analyzing features in order to build efficient SER systems. We\npresent a new supervised SER method based on an efficient feature engineering\napproach. We pay particular attention to the explainability of results to\nevaluate feature relevance and refine feature sets. This is performed\niteratively through feature evaluation loop, using Shapley values to boost\nfeature selection and improve overall framework performance. Our approach\nallows thus to balance the benefits between model performance and transparency.\nThe proposed method outperforms human-level performance (HLP) and\nstate-of-the-art machine learning methods in emotion recognition on the TESS\ndataset. The source code of this paper is publicly available at\nhttps://github.com/alaaNfissi/Iterative-Feature-Boosting-for-Explainable-Speech-Emotion-Recognition.",
      "tldr_zh": "这篇论文针对语音情感识别（SER）中的高维度特征问题，提出了一种新的监督方法，通过迭代特征提升（Iterative Feature Boosting）来优化特征选择。方法利用 Shapley values 进行特征评估和精炼，实现模型性能与透明度的平衡，避免冗余信息导致的准确性下降和计算复杂度增加。在 TESS 数据集上，该方法超过了人类水平（HLP）和现有最先进机器学习方法，并提供了开源代码以促进进一步研究。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "eess.AS",
        "I.2.7; I.2.6; I.2.1; I.2.8"
      ],
      "primary_category": "cs.SD",
      "comment": "Published in: 2023 International Conference on Machine Learning and\n  Applications (ICMLA)",
      "pdf_url": "http://arxiv.org/pdf/2405.20172v3",
      "published_date": "2024-05-30 15:44:27 UTC",
      "updated_date": "2024-06-05 22:28:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:03:34.480048"
    },
    {
      "arxiv_id": "2405.20163v1",
      "title": "Reasoning about concepts with LLMs: Inconsistencies abound",
      "title_zh": "翻译失败",
      "authors": [
        "Rosario Uceda-Sosa",
        "Karthikeyan Natesan Ramamurthy",
        "Maria Chang",
        "Moninder Singh"
      ],
      "abstract": "The ability to summarize and organize knowledge into abstract concepts is key\nto learning and reasoning. Many industrial applications rely on the consistent\nand systematic use of concepts, especially when dealing with decision-critical\nknowledge. However, we demonstrate that, when methodically questioned, large\nlanguage models (LLMs) often display and demonstrate significant\ninconsistencies in their knowledge. Computationally, the basic aspects of the\nconceptualization of a given domain can be represented as Is-A hierarchies in a\nknowledge graph (KG) or ontology, together with a few properties or axioms that\nenable straightforward reasoning. We show that even simple ontologies can be\nused to reveal conceptual inconsistencies across several LLMs. We also propose\nstrategies that domain experts can use to evaluate and improve the coverage of\nkey domain concepts in LLMs of various sizes. In particular, we have been able\nto significantly enhance the performance of LLMs of various sizes with openly\navailable weights using simple knowledge-graph (KG) based prompting strategies.",
      "tldr_zh": "该研究揭示了大型语言模型（LLMs）在概念推理过程中存在显著不一致性问题，这些不一致性会影响知识的总结和组织，尤其在决策关键的工业应用中。作者使用 Is-A 层次结构和简单本体（ontology）在知识图谱（KG）中表示概念化，并通过系统性提问暴露多个 LLMs 的概念缺陷。论文提出策略让领域专家评估并改进 LLMs 的关键领域概念覆盖，并通过 KG 基于提示方法显著提升各种大小模型的性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "15 pages, 5 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2405.20163v1",
      "published_date": "2024-05-30 15:38:54 UTC",
      "updated_date": "2024-05-30 15:38:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:03:45.275810"
    },
    {
      "arxiv_id": "2405.20142v2",
      "title": "MSSC-BiMamba: Multimodal Sleep Stage Classification and Early Diagnosis of Sleep Disorders with Bidirectional Mamba",
      "title_zh": "翻译失败",
      "authors": [
        "Chao Zhang",
        "Weirong Cui",
        "Jingjing Guo"
      ],
      "abstract": "Monitoring sleep states is essential for evaluating sleep quality and\ndiagnosing sleep disorders. Traditional manual staging is time-consuming and\nprone to subjective bias, often resulting in inconsistent outcomes. Here, we\ndeveloped an automated model for sleep staging and disorder classification to\nenhance diagnostic accuracy and efficiency. Considering the characteristics of\npolysomnography (PSG) multi-lead sleep monitoring, we designed a multimodal\nsleep state classification model, MSSC-BiMamba, that combines an Efficient\nChannel Attention (ECA) mechanism with a Bidirectional State Space Model\n(BSSM). The ECA module allows for weighting data from different sensor\nchannels, thereby amplifying the influence of diverse sensor inputs.\nAdditionally, the implementation of bidirectional Mamba (BiMamba) enables the\nmodel to effectively capture the multidimensional features and long-range\ndependencies of PSG data. The developed model demonstrated impressive\nperformance on sleep stage classification tasks on both the ISRUC-S3 and\nISRUC-S1 datasets, respectively containing data with healthy and unhealthy\nsleep patterns. Also, the model exhibited a high accuracy for sleep health\nprediction when evaluated on a combined dataset consisting of ISRUC and\nSleep-EDF. Our model, which can effectively handle diverse sleep conditions, is\nthe first to apply BiMamba to sleep staging with multimodal PSG data, showing\nsubstantial gains in computational and memory efficiency over traditional\nTransformer-style models. This method enhances sleep health management by\nmaking monitoring more accessible and extending advanced healthcare through\ninnovative technology.",
      "tldr_zh": "本研究开发了MSSC-BiMamba模型，用于自动化睡眠分期和睡眠障碍早期诊断，以提高诊断准确性和效率，解决传统手动分期耗时且易受主观偏见的问题。该模型结合Efficient Channel Attention (ECA)机制加权多传感器通道数据，以及Bidirectional State Space Model (BiMamba)捕获polysomnography (PSG)数据的多维特征和长程依赖。在ISRUC-S3、ISRUC-S1和结合ISRUC与Sleep-EDF数据集上，模型表现出色，睡眠分期和健康预测准确性高，并比传统Transformer模型在计算和内存效率上大幅提升。该创新方法通过处理多样睡眠条件，增强了睡眠健康管理的可访问性和医疗技术的扩展。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2405.20142v2",
      "published_date": "2024-05-30 15:16:53 UTC",
      "updated_date": "2024-05-31 03:31:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:03:59.356346"
    },
    {
      "arxiv_id": "2405.20139v1",
      "title": "GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Costas Mavromatis",
        "George Karypis"
      ],
      "abstract": "Knowledge Graphs (KGs) represent human-crafted factual knowledge in the form\nof triplets (head, relation, tail), which collectively form a graph. Question\nAnswering over KGs (KGQA) is the task of answering natural questions grounding\nthe reasoning to the information provided by the KG. Large Language Models\n(LLMs) are the state-of-the-art models for QA tasks due to their remarkable\nability to understand natural language. On the other hand, Graph Neural\nNetworks (GNNs) have been widely used for KGQA as they can handle the complex\ngraph information stored in the KG. In this work, we introduce GNN-RAG, a novel\nmethod for combining language understanding abilities of LLMs with the\nreasoning abilities of GNNs in a retrieval-augmented generation (RAG) style.\nFirst, a GNN reasons over a dense KG subgraph to retrieve answer candidates for\na given question. Second, the shortest paths in the KG that connect question\nentities and answer candidates are extracted to represent KG reasoning paths.\nThe extracted paths are verbalized and given as input for LLM reasoning with\nRAG. In our GNN-RAG framework, the GNN acts as a dense subgraph reasoner to\nextract useful graph information, while the LLM leverages its natural language\nprocessing ability for ultimate KGQA. Furthermore, we develop a retrieval\naugmentation (RA) technique to further boost KGQA performance with GNN-RAG.\nExperimental results show that GNN-RAG achieves state-of-the-art performance in\ntwo widely used KGQA benchmarks (WebQSP and CWQ), outperforming or matching\nGPT-4 performance with a 7B tuned LLM. In addition, GNN-RAG excels on multi-hop\nand multi-entity questions outperforming competing approaches by 8.9--15.5%\npoints at answer F1.",
      "tldr_zh": "该论文提出了GNN-RAG，一种创新方法，将Graph Neural Networks (GNNs)的推理能力与Large Language Models (LLMs)的语言理解相结合，用于Knowledge Graphs Question Answering (KGQA)。GNN-RAG框架首先使用GNN在密集的知识图谱子图上检索答案候选，并提取连接问题实体和候选的最短路径，然后将这些路径verbalized后输入LLMs进行检索增强生成 (RAG) 推理，以提升整体性能。实验结果显示，GNN-RAG在WebQSP和CWQ基准测试中达到最先进水平，超过或匹配GPT-4的表现，使用7B调优的LLM，并在多跳和多实体问题上以F1分数提高8.9-15.5%。这为高效的KGQA提供了新途径，展示了GNN和LLMs的互补优势。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20139v1",
      "published_date": "2024-05-30 15:14:24 UTC",
      "updated_date": "2024-05-30 15:14:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:04:11.505722"
    },
    {
      "arxiv_id": "2405.20138v2",
      "title": "Separation and Collapse of Equilibria Inequalities on AND-OR Trees without Shape Constraints",
      "title_zh": "翻译失败",
      "authors": [
        "Fuki Ito",
        "Toshio Suzuki"
      ],
      "abstract": "Herein, we investigate the zero-error randomized complexity, which is the\nleast cost against the worst input, of AND-OR tree computation by imposing\nvarious restrictions on the algorithm to find the Boolean value of the root of\nthat tree and no restrictions on the tree shape. When a tree satisfies a\ncertain condition regarding its symmetry, directional algorithms proposed by\nSaks and Wigderson (1986), special randomized algorithms, are known to achieve\nthe randomized complexity. Furthermore, there is a known example of a tree that\nis so unbalanced that no directional algorithm achieves the randomized\ncomplexity (Vereshchagin 1998). In this study, we aim to identify where\ndeviations arise between the general randomized Boolean decision tree and its\nspecial case, directional algorithms. In this paper, we show that for any\nAND-OR tree, randomized depth-first algorithms, which form a broader class\ncompared with directional algorithms, have the same equilibrium as that of the\ndirectional algorithms. Thus, we get the collapse result on equilibria\ninequalities that holds for an arbitrary AND-OR tree. This implies that there\nexists a case where even depth-first algorithms cannot be the fastest, leading\nto the separation result on equilibria inequality. Additionally, a new\nalgorithm is introduced as a key concept for proof of the separation result.",
      "tldr_zh": "该研究探讨了AND-OR trees在无形状约束下的零错误随机复杂度，焦点在于比较一般随机布尔决策树与定向算法的差异。论文证明，对于任意AND-OR tree，随机深度优先算法（比定向算法更广泛的类别）与定向算法具有相同的equilibria inequalities，从而导致equilibria inequalities的崩溃结果。实验和分析显示，存在情况即使随机深度优先算法也不能达到最快性能，证明了equilibria inequality的分离结果，并引入了一个新算法作为关键证明工具。",
      "categories": [
        "cs.AI",
        "68T20, 68Q17, 03D15, 91A60",
        "I.2.8; F.2.2"
      ],
      "primary_category": "cs.AI",
      "comment": "42 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2405.20138v2",
      "published_date": "2024-05-30 15:13:46 UTC",
      "updated_date": "2024-10-01 09:11:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:04:23.807650"
    },
    {
      "arxiv_id": "2405.20132v4",
      "title": "LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically Generating Metaheuristics",
      "title_zh": "LLaMEA：大语言模型进化算法，用于自动生成元启发式算法",
      "authors": [
        "Niki van Stein",
        "Thomas Bäck"
      ],
      "abstract": "Large Language Models (LLMs) such as GPT-4 have demonstrated their ability to\nunderstand natural language and generate complex code snippets. This paper\nintroduces a novel Large Language Model Evolutionary Algorithm (LLaMEA)\nframework, leveraging GPT models for the automated generation and refinement of\nalgorithms. Given a set of criteria and a task definition (the search space),\nLLaMEA iteratively generates, mutates and selects algorithms based on\nperformance metrics and feedback from runtime evaluations. This framework\noffers a unique approach to generating optimized algorithms without requiring\nextensive prior expertise. We show how this framework can be used to generate\nnovel black-box metaheuristic optimization algorithms automatically. LLaMEA\ngenerates multiple algorithms that outperform state-of-the-art optimization\nalgorithms (Covariance Matrix Adaptation Evolution Strategy and Differential\nEvolution) on the five dimensional black box optimization benchmark (BBOB). The\nalgorithms also show competitive performance on the 10- and 20-dimensional\ninstances of the test functions, although they have not seen such instances\nduring the automated generation process. The results demonstrate the\nfeasibility of the framework and identify future directions for automated\ngeneration and optimization of algorithms via LLMs.",
      "tldr_zh": "本研究引入了LLaMEA框架，这是一种利用Large Language Models (LLMs)如GPT-4的进化算法，用于自动生成和优化metaheuristics算法。LLaMEA通过给定标准和任务定义（搜索空间），迭代生成、突变和选择算法，并基于性能指标和运行时反馈进行精炼，从而无需专业知识即可创建优化算法。在实验中，生成的算法在五维BBOB（black box optimization benchmark）上优于现有算法如Covariance Matrix Adaptation Evolution Strategy和Differential Evolution，并在10-和20-维实例上表现出竞争力，尽管未在生成过程中接触这些场景。该框架证明了LLMs在算法自动生成方面的可行性，并为未来算法优化方向提供了新思路。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "Accepted at IEEE TEVC",
      "pdf_url": "http://arxiv.org/pdf/2405.20132v4",
      "published_date": "2024-05-30 15:10:59 UTC",
      "updated_date": "2025-01-30 08:54:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:04:36.030985"
    },
    {
      "arxiv_id": "2405.20121v1",
      "title": "A Structure-Aware Lane Graph Transformer Model for Vehicle Trajectory Prediction",
      "title_zh": "结构感知的车道图 Transformer 模型用于车辆轨迹预测",
      "authors": [
        "Sun Zhanbo",
        "Dong Caiyin",
        "Ji Ang",
        "Zhao Ruibin",
        "Zhao Yu"
      ],
      "abstract": "Accurate prediction of future trajectories for surrounding vehicles is vital\nfor the safe operation of autonomous vehicles. This study proposes a Lane Graph\nTransformer (LGT) model with structure-aware capabilities. Its key contribution\nlies in encoding the map topology structure into the attention mechanism. To\naddress variations in lane information from different directions, four Relative\nPositional Encoding (RPE) matrices are introduced to capture the local details\nof the map topology structure. Additionally, two Shortest Path Distance (SPD)\nmatrices are employed to capture distance information between two accessible\nlanes. Numerical results indicate that the proposed LGT model achieves a\nsignificantly higher prediction performance on the Argoverse 2 dataset.\nSpecifically, the minFDE$_6$ metric was decreased by 60.73% compared to the\nArgoverse 2 baseline model (Nearest Neighbor) and the b-minFDE$_6$ metric was\nreduced by 2.65% compared to the baseline LaneGCN model. Furthermore, ablation\nexperiments demonstrated that the consideration of map topology structure led\nto a 4.24% drop in the b-minFDE$_6$ metric, validating the effectiveness of\nthis model.",
      "tldr_zh": "本研究提出了一种结构感知的 Lane Graph Transformer (LGT) 模型，用于准确预测自主车辆周围车辆的未来轨迹，其关键贡献是将地图拓扑结构编码到注意力机制中，以提升预测性能。为处理不同方向的车道信息变异，该模型引入了四个 Relative Positional Encoding (RPE) 矩阵来捕捉地图拓扑的局部细节，并使用两个 Shortest Path Distance (SPD) 矩阵来表示可达车道之间的距离信息。在 Argoverse 2 数据集上的实验显示，LGT 模型使 minFDE$_6$ 指标比 Nearest Neighbor 基线降低了 60.73%，并比 LaneGCN 基线降低了 b-minFDE$_6$ 指标 2.65%；此外，消融实验证实，考虑地图拓扑结构导致 b-minFDE$_6$ 下降 4.24%，验证了模型的有效性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20121v1",
      "published_date": "2024-05-30 14:57:16 UTC",
      "updated_date": "2024-05-30 14:57:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:04:48.860027"
    },
    {
      "arxiv_id": "2405.20114v2",
      "title": "Towards Faster Decentralized Stochastic Optimization with Communication Compression",
      "title_zh": "翻译失败",
      "authors": [
        "Rustem Islamov",
        "Yuan Gao",
        "Sebastian U. Stich"
      ],
      "abstract": "Communication efficiency has garnered significant attention as it is\nconsidered the main bottleneck for large-scale decentralized Machine Learning\napplications in distributed and federated settings. In this regime, clients are\nrestricted to transmitting small amounts of quantized information to their\nneighbors over a communication graph. Numerous endeavors have been made to\naddress this challenging problem by developing algorithms with compressed\ncommunication for decentralized non-convex optimization problems. Despite\nconsiderable efforts, the current results suffer from various issues such as\nnon-scalability with the number of clients, requirements for large batches, or\nbounded gradient assumption. In this paper, we introduce MoTEF, a novel\napproach that integrates communication compression with Momentum Tracking and\nError Feedback. Our analysis demonstrates that MoTEF achieves most of the\ndesired properties, and significantly outperforms existing methods under\narbitrary data heterogeneity. We provide numerical experiments to validate our\ntheoretical findings and confirm the practical superiority of MoTEF.",
      "tldr_zh": "本研究针对大规模去中心化随机优化中的通信效率瓶颈，提出了一种新方法MoTEF，将通信压缩与Momentum Tracking和Error Feedback相结合，以处理分布式和联邦学习场景下客户端间量化信息传输的挑战。MoTEF克服了现有算法的局限性，如不随客户端数量扩展、大批量要求或梯度有界假设，并在任意数据异质性条件下显著提升性能。实验结果验证了理论分析，显示MoTEF在去中心化非凸优化问题上优于基线方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20114v2",
      "published_date": "2024-05-30 14:51:57 UTC",
      "updated_date": "2024-11-25 09:00:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:04:59.632160"
    },
    {
      "arxiv_id": "2405.20082v3",
      "title": "Segment, Shuffle, and Stitch: A Simple Layer for Improving Time-Series Representations",
      "title_zh": "翻译失败",
      "authors": [
        "Shivam Grover",
        "Amin Jalali",
        "Ali Etemad"
      ],
      "abstract": "Existing approaches for learning representations of time-series keep the\ntemporal arrangement of the time-steps intact with the presumption that the\noriginal order is the most optimal for learning. However, non-adjacent sections\nof real-world time-series may have strong dependencies. Accordingly, we raise\nthe question: Is there an alternative arrangement for time-series which could\nenable more effective representation learning? To address this, we propose a\nsimple plug-and-play neural network layer called Segment, Shuffle, and Stitch\n(S3) designed to improve representation learning in time-series models. S3\nworks by creating non-overlapping segments from the original sequence and\nshuffling them in a learned manner that is optimal for the task at hand. It\nthen re-attaches the shuffled segments back together and performs a learned\nweighted sum with the original input to capture both the newly shuffled\nsequence along with the original sequence. S3 is modular and can be stacked to\nachieve different levels of granularity, and can be added to many forms of\nneural architectures including CNNs or Transformers with negligible computation\noverhead. Through extensive experiments on several datasets and\nstate-of-the-art baselines, we show that incorporating S3 results in\nsignificant improvements for the tasks of time-series classification,\nforecasting, and anomaly detection, improving performance on certain datasets\nby up to 68\\%. We also show that S3 makes the learning more stable with a\nsmoother training loss curve and loss landscape compared to the original\nbaseline. The code is available at\nhttps://github.com/shivam-grover/S3-TimeSeries.",
      "tldr_zh": "本研究质疑现有时间序列表示学习方法对原始顺序的依赖，提出一个简单可插入的神经网络层：Segment, Shuffle, and Stitch (S3)，旨在通过重新排列序列来提升学习效果。S3 层的工作原理是将原始序列分割成非重叠段落，进行任务优化的洗牌，然后重新连接并与原始输入进行学习加权的求和，从而捕获更强的依赖关系。实验结果显示，在多个数据集上，S3 显著提高了时间序列分类、预测和异常检测的任务性能，最多提升 68%，并使训练过程更稳定，损失曲线更平滑。该层可轻松集成到 CNNs 或 Transformers 等架构中，计算开销 negligible，且代码已开源。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.20082v3",
      "published_date": "2024-05-30 14:11:29 UTC",
      "updated_date": "2024-10-30 15:18:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:05:13.410174"
    },
    {
      "arxiv_id": "2405.20081v2",
      "title": "NoiseBoost: Alleviating Hallucination with Noise Perturbation for Multimodal Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Kai Wu",
        "Boyuan Jiang",
        "Zhengkai Jiang",
        "Qingdong He",
        "Donghao Luo",
        "Shengzhi Wang",
        "Qingwen Liu",
        "Chengjie Wang"
      ],
      "abstract": "Multimodal large language models (MLLMs) contribute a powerful mechanism to\nunderstanding visual information building on large language models. However,\nMLLMs are notorious for suffering from hallucinations, especially when\ngenerating lengthy, detailed descriptions for images. Our analysis reveals that\nhallucinations stem from the inherent summarization mechanism of large language\nmodels, leading to excessive dependence on linguistic tokens while neglecting\nvision information. In this paper, we propose NoiseBoost, a broadly applicable\nand simple method for alleviating hallucinations for MLLMs through the\nintegration of noise feature perturbations. Noise perturbation acts as a\nregularizer, facilitating a balanced distribution of attention weights among\nvisual and linguistic tokens. Despite its simplicity, NoiseBoost consistently\nenhances the performance of MLLMs across common training strategies, including\nsupervised fine-tuning and reinforcement learning. Further, NoiseBoost\npioneerly enables semi-supervised learning for MLLMs, unleashing the power of\nunlabeled data. Comprehensive experiments demonstrate that NoiseBoost improves\ndense caption accuracy by 8.1% with human evaluation and achieves comparable\nresults with 50% of the data by mining unlabeled data. Code and models are\navailable at https://kaiwu5.github.io/noiseboost.",
      "tldr_zh": "本研究针对多模态大语言模型(MLLMs)生成的幻觉问题，提出了一种简单且广泛适用的方法NoiseBoost，通过整合噪声特征扰动来缓解模型过度依赖语言标记而忽略视觉信息的现象。NoiseBoost作为正则化器，促进视觉和语言标记之间注意权重的均衡分布，从而提升模型在生成图像详细描述时的准确性。该方法适用于多种训练策略，包括监督微调和强化学习，并首次实现了MLLMs的半监督学习，利用无标签数据释放更多潜力。实验结果显示，NoiseBoost将密集标题准确率提高了8.1%（基于人工评估），并在仅使用50%数据时，通过挖掘无标签数据达到可比性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 5 figures with supplementary material",
      "pdf_url": "http://arxiv.org/pdf/2405.20081v2",
      "published_date": "2024-05-30 14:11:27 UTC",
      "updated_date": "2024-05-31 07:40:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:05:24.318031"
    },
    {
      "arxiv_id": "2405.20059v1",
      "title": "Spectral Mapping of Singing Voices: U-Net-Assisted Vocal Segmentation",
      "title_zh": "演唱声音的光谱映射：U-Net 辅助的声部分割",
      "authors": [
        "Adam Sorrenti"
      ],
      "abstract": "Separating vocal elements from musical tracks is a longstanding challenge in\naudio signal processing. This study tackles the distinct separation of vocal\ncomponents from musical spectrograms. We employ the Short Time Fourier\nTransform (STFT) to extract audio waves into detailed frequency-time\nspectrograms, utilizing the benchmark MUSDB18 dataset for music separation.\nSubsequently, we implement a UNet neural network to segment the spectrogram\nimage, aiming to delineate and extract singing voice components accurately. We\nachieved noteworthy results in audio source separation using of our U-Net-based\nmodels. The combination of frequency-axis normalization with Min/Max scaling\nand the Mean Absolute Error (MAE) loss function achieved the highest\nSource-to-Distortion Ratio (SDR) of 7.1 dB, indicating a high level of accuracy\nin preserving the quality of the original signal during separation. This setup\nalso recorded impressive Source-to-Interference Ratio (SIR) and\nSource-to-Artifact Ratio (SAR) scores of 25.2 dB and 7.2 dB, respectively.\nThese values significantly outperformed other configurations, particularly\nthose using Quantile-based normalization or a Mean Squared Error (MSE) loss\nfunction. Our source code, model weights, and demo material can be found at the\nproject's GitHub repository: https://github.com/mbrotos/SoundSeg",
      "tldr_zh": "本研究针对音频信号处理中的声乐分离挑战，提出了一种基于 U-Net 神经网络的频谱映射方法，使用 Short Time Fourier Transform (STFT) 将音乐音频转换为频谱图，并利用 MUSDB18 数据集进行训练和分割，以精确提取演唱声组件。方法结合频率轴归一化、Min/Max 缩放以及 Mean Absolute Error (MAE) 损失函数，显著提升了分离性能。实验结果显示，该模型实现了最高的 Source-to-Distortion Ratio (SDR) 为 7.1 dB，以及 Source-to-Interference Ratio (SIR) 25.2 dB 和 Source-to-Artifact Ratio (SAR) 7.2 dB，这些指标远优于使用 Quantile-based normalization 或 Mean Squared Error (MSE) 的配置。该框架的源代码、模型权重和演示材料已在 GitHub 上公开，为音频源分离技术提供了可复现的实用工具。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20059v1",
      "published_date": "2024-05-30 13:47:53 UTC",
      "updated_date": "2024-05-30 13:47:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:05:36.909236"
    },
    {
      "arxiv_id": "2405.20053v1",
      "title": "Would I Lie To You? Inference Time Alignment of Language Models using Direct Preference Heads",
      "title_zh": "翻译失败",
      "authors": [
        "Avelina Asada Hadji-Kyriacou",
        "Ognjen Arandjelovic"
      ],
      "abstract": "Pre-trained Language Models (LMs) exhibit strong zero-shot and in-context\nlearning capabilities; however, their behaviors are often difficult to control.\nBy utilizing Reinforcement Learning from Human Feedback (RLHF), it is possible\nto fine-tune unsupervised LMs to follow instructions and produce outputs that\nreflect human preferences. Despite its benefits, RLHF has been shown to\npotentially harm a language model's reasoning capabilities and introduce\nartifacts such as hallucinations where the model may fabricate facts. To\naddress this issue we introduce Direct Preference Heads (DPH), a fine-tuning\nframework that enables LMs to learn human preference signals through an\nauxiliary reward head without directly affecting the output distribution of the\nlanguage modeling head. We perform a theoretical analysis of our objective\nfunction and find strong ties to Conservative Direct Preference Optimization\n(cDPO). Finally we evaluate our models on GLUE, RACE, and the GPT4All\nevaluation suite and demonstrate that our method produces models which achieve\nhigher scores than those fine-tuned with Supervised Fine-Tuning (SFT) or Direct\nPreference Optimization (DPO) alone.",
      "tldr_zh": "该研究针对预训练语言模型 (LMs) 的行为控制难题，提出 Direct Preference Heads (DPH) 框架，通过辅助奖励头在推理时学习人类偏好信号，而不直接影响语言建模头的输出分布，从而避免了 Reinforcement Learning from Human Feedback (RLHF) 可能带来的推理能力下降和幻觉问题。理论分析显示，该方法与 Conservative Direct Preference Optimization (cDPO) 密切相关。实验结果表明，在 GLUE、RACE 和 GPT4All 评估套件上，使用 DPH 微调的模型比单纯的 Supervised Fine-Tuning (SFT) 或 Direct Preference Optimization (DPO) 获得更高分数，展示了其在保持模型性能的同时提升偏好对齐的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20053v1",
      "published_date": "2024-05-30 13:38:52 UTC",
      "updated_date": "2024-05-30 13:38:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:05:48.997102"
    },
    {
      "arxiv_id": "2405.20046v1",
      "title": "Cross-Training with Multi-View Knowledge Fusion for Heterogenous Federated Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Zhuang Qi",
        "Lei Meng",
        "Weihao He",
        "Ruohan Zhang",
        "Yu Wang",
        "Xin Qi",
        "Xiangxu Meng"
      ],
      "abstract": "Federated learning benefits from cross-training strategies, which enables\nmodels to train on data from distinct sources to improve the generalization\ncapability. However, the data heterogeneity between sources may lead models to\ngradually forget previously acquired knowledge when undergoing cross-training\nto adapt to new tasks or data sources. We argue that integrating personalized\nand global knowledge to gather information from multiple perspectives could\npotentially improve performance. To achieve this goal, this paper presents a\nnovel approach that enhances federated learning through a cross-training scheme\nincorporating multi-view information. Specifically, the proposed method, termed\nFedCT, includes three main modules, where the consistency-aware knowledge\nbroadcasting module aims to optimize model assignment strategies, which\nenhances collaborative advantages between clients and achieves an efficient\nfederated learning process. The multi-view knowledge-guided representation\nlearning module leverages fused prototypical knowledge from both global and\nlocal views to enhance the preservation of local knowledge before and after\nmodel exchange, as well as to ensure consistency between local and global\nknowledge. The mixup-based feature augmentation module aggregates rich\ninformation to further increase the diversity of feature spaces, which enables\nthe model to better discriminate complex samples. Extensive experiments were\nconducted on four datasets in terms of performance comparison, ablation study,\nin-depth analysis and case study. The results demonstrated that FedCT\nalleviates knowledge forgetting from both local and global views, which enables\nit outperform state-of-the-art methods.",
      "tldr_zh": "这篇论文针对异质 Federated Learning 中的数据异质性问题，提出了一种名为 FedCT 的跨训练方法，通过融合多视角知识（包括 personalized 和 global 知识）来缓解模型在适应新任务时出现的知识遗忘。FedCT 包含三个关键模块：一致性知识广播模块优化模型分配策略以提升客户端协作、多视角知识引导表示学习模块利用融合的原型知识保留本地知识并确保全局一致性，以及基于 Mixup 的特征增强模块增加特征空间多样性以更好地区分复杂样本。实验在四个数据集上进行，证明 FedCT 优于现有 state-of-the-art 方法，在性能比较、消融研究和案例分析中均表现出色。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20046v1",
      "published_date": "2024-05-30 13:27:30 UTC",
      "updated_date": "2024-05-30 13:27:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:06:01.151921"
    },
    {
      "arxiv_id": "2405.20032v1",
      "title": "Promptus: Can Prompts Streaming Replace Video Streaming with Stable Diffusion",
      "title_zh": "翻译失败",
      "authors": [
        "Jiangkai Wu",
        "Liming Liu",
        "Yunpeng Tan",
        "Junlin Hao",
        "Xinggong Zhang"
      ],
      "abstract": "With the exponential growth of video traffic, traditional video streaming\nsystems are approaching their limits in compression efficiency and\ncommunication capacity. To further reduce bitrate while maintaining quality, we\npropose Promptus, a disruptive novel system that streaming prompts instead of\nvideo content with Stable Diffusion, which converts video frames into a series\nof \"prompts\" for delivery. To ensure pixel alignment, a gradient descent-based\nprompt fitting framework is proposed. To achieve adaptive bitrate for prompts,\na low-rank decomposition-based bitrate control algorithm is introduced. For\ninter-frame compression of prompts, a temporal smoothing-based prompt\ninterpolation algorithm is proposed. Evaluations across various video domains\nand real network traces demonstrate Promptus can enhance the perceptual quality\nby 0.111 and 0.092 (in LPIPS) compared to VAE and H.265, respectively, and\ndecreases the ratio of severely distorted frames by 89.3% and 91.7%. Moreover,\nPromptus achieves real-time video generation from prompts at over 150 FPS. To\nthe best of our knowledge, Promptus is the first attempt to replace video\ncodecs with prompt inversion and the first to use prompt streaming instead of\nvideo streaming. Our work opens up a new paradigm for efficient video\ncommunication beyond the Shannon limit.",
      "tldr_zh": "该论文提出 Promptus 系统，使用 Stable Diffusion 将视频帧转换为一系列 \"prompts\" 进行流式传输，以替代传统视频流式传输，从而降低比特率并维持质量。核心方法包括 gradient descent-based prompt fitting 框架确保像素对齐、low-rank decomposition-based bitrate control 算法实现自适应比特率，以及 temporal smoothing-based prompt interpolation 算法进行帧间压缩。实验结果显示，Promptus 在各种视频领域和真实网络环境中，比 VAE 和 H.265 分别提高了 0.111 和 0.092 的感知质量（LPIPS），并将严重失真帧比例减少了 89.3% 和 91.7%，同时实现了超过 150 FPS 的实时视频生成。该系统首次尝试用 prompt inversion 替换视频编解码器，开创了超越 Shannon 极限的视频通信新范式。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20032v1",
      "published_date": "2024-05-30 13:16:48 UTC",
      "updated_date": "2024-05-30 13:16:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:06:23.431231"
    },
    {
      "arxiv_id": "2405.20024v2",
      "title": "Applications of Generative AI (GAI) for Mobile and Wireless Networking: A Survey",
      "title_zh": "生成式人工智能 (GAI) 在移动和无线网络中的应用：一项调查",
      "authors": [
        "Thai-Hoc Vu",
        "Senthil Kumar Jagatheesaperumal",
        "Minh-Duong Nguyen",
        "Nguyen Van Huynh",
        "Sunghwan Kim",
        "Quoc-Viet Pham"
      ],
      "abstract": "The success of Artificial Intelligence (AI) in multiple disciplines and\nvertical domains in recent years has promoted the evolution of mobile\nnetworking and the future Internet toward an AI-integrated Internet-of-Things\n(IoT) era. Nevertheless, most AI techniques rely on data generated by physical\ndevices (e.g., mobile devices and network nodes) or specific applications\n(e.g., fitness trackers and mobile gaming). Therefore, Generative AI (GAI),\na.k.a. AI-generated content (AIGC), has emerged as a powerful AI paradigm;\nthanks to its ability to efficiently learn complex data distributions and\ngenerate synthetic data to represent the original data in various forms. This\nimpressive feature is projected to transform the management of mobile\nnetworking and diversify the current services and applications provided. On\nthis basis, this work presents a concise tutorial on the role of GAIs in mobile\nand wireless networking. In particular, this survey first provides the\nfundamentals of GAI and representative GAI models, serving as an essential\npreliminary to the understanding of GAI's applications in mobile and wireless\nnetworking. Then, this work provides a comprehensive review of state-of-the-art\nstudies and GAI applications in network management, wireless security, semantic\ncommunication, and lessons learned from the open literature. Finally, this work\nsummarizes the current research on GAI for mobile and wireless networking by\noutlining important challenges that need to be resolved to facilitate the\ndevelopment and applicability of GAI in this edge-cutting area.",
      "tldr_zh": "这篇调查论文探讨了生成式AI (GAI) 在移动和无线网络中的应用，强调GAI 通过生成合成数据来提升网络管理、增强无线安全并优化语义通信，从而推动AI 整合到物联网 (IoT) 时代。首先，论文介绍了GAI 的基础知识和代表性模型，然后对现有研究进行了全面回顾，包括网络管理、无线安全和语义通信的应用领域。最后，它总结了当前挑战，如数据生成效率和适用性问题，以促进GAI 在移动网络领域的未来发展。",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "This work has been accepted for publication in the IEEE Internet of\n  Things Journal under ID number IoT-37996-2024",
      "pdf_url": "http://arxiv.org/pdf/2405.20024v2",
      "published_date": "2024-05-30 13:06:40 UTC",
      "updated_date": "2024-10-19 12:21:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:06:35.771803"
    },
    {
      "arxiv_id": "2405.20015v1",
      "title": "Efficient LLM-Jailbreaking by Introducing Visual Modality",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenxing Niu",
        "Yuyao Sun",
        "Haodong Ren",
        "Haoxuan Ji",
        "Quan Wang",
        "Xiaoke Ma",
        "Gang Hua",
        "Rong Jin"
      ],
      "abstract": "This paper focuses on jailbreaking attacks against large language models\n(LLMs), eliciting them to generate objectionable content in response to harmful\nuser queries. Unlike previous LLM-jailbreaks that directly orient to LLMs, our\napproach begins by constructing a multimodal large language model (MLLM)\nthrough the incorporation of a visual module into the target LLM. Subsequently,\nwe conduct an efficient MLLM-jailbreak to generate jailbreaking embeddings\nembJS. Finally, we convert the embJS into text space to facilitate the\njailbreaking of the target LLM. Compared to direct LLM-jailbreaking, our\napproach is more efficient, as MLLMs are more vulnerable to jailbreaking than\npure LLM. Additionally, to improve the attack success rate (ASR) of\njailbreaking, we propose an image-text semantic matching scheme to identify a\nsuitable initial input. Extensive experiments demonstrate that our approach\nsurpasses current state-of-the-art methods in terms of both efficiency and\neffectiveness. Moreover, our approach exhibits superior cross-class\njailbreaking capabilities.",
      "tldr_zh": "本论文提出了一种高效的 LLM-jailbreaking 方法，通过引入视觉模块构建多模态大型语言模型 (MLLM)，以更易受攻击的优势生成 jailbreaking embeddings (embJS)，并将这些嵌入转换回文本空间，从而诱导目标 LLM 生成有害内容。相比直接针对 LLM 的攻击，该方法更高效，因为 MLLMs 的易受性提高了攻击成功率 (ASR)，并通过图像-文本语义匹配方案优化初始输入。实验结果表明，该方法在效率和有效性上超越现有最先进技术，并展示了强大的跨类 jailbreaking 能力。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20015v1",
      "published_date": "2024-05-30 12:50:32 UTC",
      "updated_date": "2024-05-30 12:50:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:06:50.013912"
    },
    {
      "arxiv_id": "2405.20003v1",
      "title": "Kernel Language Entropy: Fine-grained Uncertainty Quantification for LLMs from Semantic Similarities",
      "title_zh": "翻译失败",
      "authors": [
        "Alexander Nikitin",
        "Jannik Kossen",
        "Yarin Gal",
        "Pekka Marttinen"
      ],
      "abstract": "Uncertainty quantification in Large Language Models (LLMs) is crucial for\napplications where safety and reliability are important. In particular,\nuncertainty can be used to improve the trustworthiness of LLMs by detecting\nfactually incorrect model responses, commonly called hallucinations.\nCritically, one should seek to capture the model's semantic uncertainty, i.e.,\nthe uncertainty over the meanings of LLM outputs, rather than uncertainty over\nlexical or syntactic variations that do not affect answer correctness. To\naddress this problem, we propose Kernel Language Entropy (KLE), a novel method\nfor uncertainty estimation in white- and black-box LLMs. KLE defines positive\nsemidefinite unit trace kernels to encode the semantic similarities of LLM\noutputs and quantifies uncertainty using the von Neumann entropy. It considers\npairwise semantic dependencies between answers (or semantic clusters),\nproviding more fine-grained uncertainty estimates than previous methods based\non hard clustering of answers. We theoretically prove that KLE generalizes the\nprevious state-of-the-art method called semantic entropy and empirically\ndemonstrate that it improves uncertainty quantification performance across\nmultiple natural language generation datasets and LLM architectures.",
      "tldr_zh": "本研究针对大型语言模型（LLMs）的不确定性量化（Uncertainty Quantification）问题，提出了一种新方法Kernel Language Entropy (KLE)，旨在通过编码语义相似性来捕捉模型输出的语义不确定性（semantic uncertainty），从而更准确地检测事实错误或幻觉（hallucinations）。KLE利用正定半正定单位迹核（positive semidefinite unit trace kernels）来表示答案之间的成对语义依赖，并通过von Neumann entropy量化不确定性，提供比基于硬聚类的传统方法更细粒度的估计。理论上，KLE推广了先前的state-of-the-art方法semantic entropy，并在多个自然语言生成数据集和LLM架构上实证证明了其性能提升。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.20003v1",
      "published_date": "2024-05-30 12:42:05 UTC",
      "updated_date": "2024-05-30 12:42:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:07:00.989258"
    },
    {
      "arxiv_id": "2405.19996v4",
      "title": "DP-IQA: Utilizing Diffusion Prior for Blind Image Quality Assessment in the Wild",
      "title_zh": "DP-IQA：利用扩散先验进行野外盲图像质量评估",
      "authors": [
        "Honghao Fu",
        "Yufei Wang",
        "Wenhan Yang",
        "Bihan Wen"
      ],
      "abstract": "Blind image quality assessment (IQA) in the wild, which assesses the quality\nof images with complex authentic distortions and no reference images, presents\nsignificant challenges. Given the difficulty in collecting large-scale training\ndata, leveraging limited data to develop a model with strong generalization\nremains an open problem. Motivated by the robust image perception capabilities\nof pre-trained text-to-image (T2I) diffusion models, we propose a novel IQA\nmethod, diffusion priors-based IQA (DP-IQA), to utilize the T2I model's prior\nfor improved performance and generalization ability. Specifically, we utilize\npre-trained Stable Diffusion as the backbone, extracting multi-level features\nfrom the denoising U-Net guided by prompt embeddings through a tunable text\nadapter. Simultaneously, an image adapter compensates for information loss\nintroduced by the lossy pre-trained encoder. Unlike T2I models that require\nfull image distribution modeling, our approach targets image quality\nassessment, which inherently requires fewer parameters. To improve\napplicability, we distill the knowledge into a lightweight CNN-based student\nmodel, significantly reducing parameters while maintaining or even enhancing\ngeneralization performance. Experimental results demonstrate that DP-IQA\nachieves state-of-the-art performance on various in-the-wild datasets,\nhighlighting the superior generalization capability of T2I priors in blind IQA\ntasks. To our knowledge, DP-IQA is the first method to apply pre-trained\ndiffusion priors in blind IQA. Codes and checkpoints are available at\nhttps://github.com/RomGai/DP-IQA.",
      "tldr_zh": "本文提出DP-IQA方法，利用预训练的文本到图像(T2I)扩散模型的先验知识，解决野外盲图像质量评估(Blind Image Quality Assessment in the Wild)的挑战，该方法针对复杂真实失真图像评估而设计。核心技术包括使用Stable Diffusion作为主干，通过可调文本适配器和图像适配器提取多级特征，并通过知识蒸馏到轻量级CNN学生模型中，以减少参数同时提升泛化性能。与传统T2I模型不同，DP-IQA专注于图像质量评估，需要更少的参数。实验结果表明，该方法在多个野外数据集上实现了最先进性能，首次证明了扩散先验在盲IQA任务中的优越泛化能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19996v4",
      "published_date": "2024-05-30 12:32:35 UTC",
      "updated_date": "2024-08-17 13:53:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:07:14.308918"
    },
    {
      "arxiv_id": "2405.19988v2",
      "title": "Video-Language Critic: Transferable Reward Functions for Language-Conditioned Robotics",
      "title_zh": "翻译失败",
      "authors": [
        "Minttu Alakuijala",
        "Reginald McLean",
        "Isaac Woungang",
        "Nariman Farsad",
        "Samuel Kaski",
        "Pekka Marttinen",
        "Kai Yuan"
      ],
      "abstract": "Natural language is often the easiest and most convenient modality for humans\nto specify tasks for robots. However, learning to ground language to behavior\ntypically requires impractical amounts of diverse, language-annotated\ndemonstrations collected on each target robot. In this work, we aim to separate\nthe problem of what to accomplish from how to accomplish it, as the former can\nbenefit from substantial amounts of external observation-only data, and only\nthe latter depends on a specific robot embodiment. To this end, we propose\nVideo-Language Critic, a reward model that can be trained on readily available\ncross-embodiment data using contrastive learning and a temporal ranking\nobjective, and use it to score behavior traces from a separate actor. When\ntrained on Open X-Embodiment data, our reward model enables 2x more\nsample-efficient policy training on Meta-World tasks than a sparse reward only,\ndespite a significant domain gap. Using in-domain data but in a challenging\ntask generalization setting on Meta-World, we further demonstrate more\nsample-efficient training than is possible with prior language-conditioned\nreward models that are either trained with binary classification, use static\nimages, or do not leverage the temporal information present in video data.",
      "tldr_zh": "这篇论文提出了 Video-Language Critic，一种可转移的奖励函数，用于语言条件机器人任务，旨在将任务目标（what to accomplish）与具体执行方式（how to accomplish）分离，从而利用外部跨机器人观察数据进行训练。模型采用 contrastive learning 和 temporal ranking objective，在 Open X-Embodiment 数据集上训练后，用于评分行为轨迹并指导策略训练。实验结果显示，在 Meta-World 任务上，该方法比稀疏奖励提高了 2 倍的样本效率，并在任务泛化设置中优于现有语言条件奖励模型，尤其是在利用视频时间信息方面。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "10 pages in the main text, 16 pages including references and\n  supplementary materials. 4 figures and 3 tables in the main text, 1 table in\n  supplementary materials",
      "pdf_url": "http://arxiv.org/pdf/2405.19988v2",
      "published_date": "2024-05-30 12:18:06 UTC",
      "updated_date": "2024-11-07 19:40:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:07:24.281763"
    },
    {
      "arxiv_id": "2405.19982v1",
      "title": "A Deep Reinforcement Learning Approach for Trading Optimization in the Forex Market with Multi-Agent Asynchronous Distribution",
      "title_zh": "翻译失败",
      "authors": [
        "Davoud Sarani",
        "Parviz Rashidi-Khazaee"
      ],
      "abstract": "In today's forex market traders increasingly turn to algorithmic trading,\nleveraging computers to seek more profits. Deep learning techniques as\ncutting-edge advancements in machine learning, capable of identifying patterns\nin financial data. Traders utilize these patterns to execute more effective\ntrades, adhering to algorithmic trading rules. Deep reinforcement learning\nmethods (DRL), by directly executing trades based on identified patterns and\nassessing their profitability, offer advantages over traditional DL approaches.\nThis research pioneers the application of a multi-agent (MA) RL framework with\nthe state-of-the-art Asynchronous Advantage Actor-Critic (A3C) algorithm. The\nproposed method employs parallel learning across multiple asynchronous workers,\neach specialized in trading across multiple currency pairs to explore the\npotential for nuanced strategies tailored to different market conditions and\ncurrency pairs. Two different A3C with lock and without lock MA model was\nproposed and trained on single currency and multi-currency. The results\nindicate that both model outperform on Proximal Policy Optimization model. A3C\nwith lock outperforms other in single currency training scenario and A3C\nwithout Lock outperforms other in multi-currency scenario. The findings\ndemonstrate that this approach facilitates broader and faster exploration of\ndifferent currency pairs, significantly enhancing trading returns.\nAdditionally, the agent can learn a more profitable trading strategy in a\nshorter time.",
      "tldr_zh": "该研究提出了一种基于深度强化学习（Deep Reinforcement Learning）的多智能体（Multi-Agent）异步分布框架，用于优化外汇市场交易。具体而言，该框架采用先进的Asynchronous Advantage Actor-Critic (A3C)算法，通过多个异步工作者并行学习，针对单货币和多货币对开发带锁和不带锁的A3C模型，以探索不同市场条件下的策略。实验结果显示，A3C模型在单货币场景中带锁版本表现最佳，而在多货币场景中不带锁版本优于基准模型Proximal Policy Optimization (PPO)，显著提高了交易回报并加速了盈利策略的学习。",
      "categories": [
        "cs.CE",
        "cs.AI",
        "cs.CC"
      ],
      "primary_category": "cs.CE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19982v1",
      "published_date": "2024-05-30 12:07:08 UTC",
      "updated_date": "2024-05-30 12:07:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:07:37.910639"
    },
    {
      "arxiv_id": "2405.19973v1",
      "title": "A Triumvirate of AI Driven Theoretical Discovery",
      "title_zh": "翻译失败",
      "authors": [
        "Yang-Hui He"
      ],
      "abstract": "Recent years have seen the dramatic rise of the usage of AI algorithms in\npure mathematics and fundamental sciences such as theoretical physics. This is\nperhaps counter-intuitive since mathematical sciences require the rigorous\ndefinitions, derivations, and proofs, in contrast to the experimental sciences\nwhich rely on the modelling of data with error-bars. In this Perspective, we\ncategorize the approaches to mathematical discovery as \"top-down\", \"bottom-up\"\nand \"meta-mathematics\", as inspired by historical examples. We review some of\nthe progress over the last few years, comparing and contrasting both the\nadvances and the short-comings in each approach. We argue that while the\ntheorist is in no way in danger of being replaced by AI in the near future, the\nhybrid of human expertise and AI algorithms will become an integral part of\ntheoretical discovery.",
      "tldr_zh": "这篇论文探讨了AI算法在纯数学和理论物理领域的应用，将数学发现方法分类为\"top-down\"、\"bottom-up\"和\"meta-mathematics\"三类，并通过历史例子进行阐释。作者回顾了过去几年的进展，比较了这些方法的优势（如数据建模）和局限性（如缺乏严格证明），强调AI在处理实验数据时更具潜力。最终，论文认为AI短期内不会取代理论家，但人类专家与AI算法的混合将作为理论发现的核心组成部分。",
      "categories": [
        "math.HO",
        "cs.AI",
        "hep-th",
        "physics.hist-ph"
      ],
      "primary_category": "math.HO",
      "comment": "14 pages, under consideration for Nature Review Physics",
      "pdf_url": "http://arxiv.org/pdf/2405.19973v1",
      "published_date": "2024-05-30 11:57:00 UTC",
      "updated_date": "2024-05-30 11:57:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:07:46.982652"
    },
    {
      "arxiv_id": "2405.19970v1",
      "title": "Strategies to Counter Artificial Intelligence in Law Enforcement: Cross-Country Comparison of Citizens in Greece, Italy and Spain",
      "title_zh": "翻译失败",
      "authors": [
        "Petra Saskia Bayerl",
        "Babak Akhgar",
        "Ernesto La Mattina",
        "Barbara Pirillo",
        "Ioana Cotoi",
        "Davide Ariu",
        "Matteo Mauri",
        "Jorge Garcia",
        "Dimitris Kavallieros",
        "Antonia Kardara",
        "Konstantina Karagiorgou"
      ],
      "abstract": "This paper investigates citizens' counter-strategies to the use of Artificial\nIntelligence (AI) by law enforcement agencies (LEAs). Based on information from\nthree countries (Greece, Italy and Spain) we demonstrate disparities in the\nlikelihood of ten specific counter-strategies. We further identified factors\nthat increase the propensity for counter-strategies. Our study provides an\nimportant new perspective to societal impacts of security-focused AI\napplications by illustrating the conscious, strategic choices by citizens when\nconfronted with AI capabilities for LEAs.",
      "tldr_zh": "这篇论文探讨了公民针对执法机构(LEAs)使用人工智能(AI)的反策略，通过比较希腊、意大利和西班牙三国的公民数据，揭示了十种具体反策略的采用可能性差异。研究进一步识别了影响这些反策略倾向的因素，如社会文化和认知变量。总体而言，该研究为安全焦点AI应用的社会影响提供了新视角，强调了公民在面对LEAs的AI能力时所做的有意识战略选择。",
      "categories": [
        "cs.AI",
        "I.2.0; K.4.1"
      ],
      "primary_category": "cs.AI",
      "comment": "20th International Conference on Information and Knowledge\n  Engineering (IKE'21), 3 papges, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2405.19970v1",
      "published_date": "2024-05-30 11:55:10 UTC",
      "updated_date": "2024-05-30 11:55:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:07:58.931994"
    },
    {
      "arxiv_id": "2405.19967v2",
      "title": "Improved Out-of-Scope Intent Classification with Dual Encoding and Threshold-based Re-Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Hossam M. Zawbaa",
        "Wael Rashwan",
        "Sourav Dutta",
        "Haytham Assem"
      ],
      "abstract": "Detecting out-of-scope user utterances is essential for task-oriented\ndialogues and intent classification. Current methodologies face difficulties\nwith the unpredictable distribution of outliers and often rely on assumptions\nabout data distributions. We present the Dual Encoder for Threshold-Based\nRe-Classification (DETER) to address these challenges. This end-to-end\nframework efficiently detects out-of-scope intents without requiring\nassumptions on data distributions or additional post-processing steps. The core\nof DETER utilizes dual text encoders, the Universal Sentence Encoder (USE) and\nthe Transformer-based Denoising AutoEncoder (TSDAE), to generate user utterance\nembeddings, which are classified through a branched neural architecture.\nFurther, DETER generates synthetic outliers using self-supervision and\nincorporates out-of-scope phrases from open-domain datasets. This approach\nensures a comprehensive training set for out-of-scope detection. Additionally,\na threshold-based re-classification mechanism refines the model's initial\npredictions. Evaluations on the CLINC-150, Stackoverflow, and Banking77\ndatasets demonstrate DETER's efficacy. Our model outperforms previous\nbenchmarks, increasing up to 13% and 5% in F1 score for known and unknown\nintents on CLINC-150 and Stackoverflow, and 16% for known and 24% % for unknown\nintents on Banking77. The source code has been released at\nhttps://github.com/Hossam-Mohammed-tech/Intent_Classification_OOS.",
      "tldr_zh": "本文提出了一种名为 DETER 的端到端框架，用于改进任务导向对话中的 out-of-scope 意图分类问题，通过 Dual Encoding（使用 Universal Sentence Encoder (USE) 和 Transformer-based Denoising AutoEncoder (TSDAE)）生成语句嵌入，并结合分支神经架构和阈值-based re-classification 机制进行优化。框架还通过自监督生成合成 outliers 并整合开源数据集，确保训练集的全面性。实验结果显示，DETER 在 CLINC-150、Stackoverflow 和 Banking77 数据集上表现出色，F1 分数较基准模型提升高达 24%，特别是在未知意图检测上。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19967v2",
      "published_date": "2024-05-30 11:46:42 UTC",
      "updated_date": "2024-05-31 08:54:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:08:12.998008"
    },
    {
      "arxiv_id": "2405.19958v1",
      "title": "Multi-Aspect Controllable Text Generation with Disentangled Counterfactual Augmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Yi Liu",
        "Xiangyu Liu",
        "Xiangrong Zhu",
        "Wei Hu"
      ],
      "abstract": "Multi-aspect controllable text generation aims to control the generated texts\nin attributes from multiple aspects (e.g., \"positive\" from sentiment and\n\"sport\" from topic). For ease of obtaining training samples, existing works\nneglect attribute correlations formed by the intertwining of different\nattributes. Particularly, the stereotype formed by imbalanced attribute\ncorrelations significantly affects multi-aspect control. In this paper, we\npropose MAGIC, a new multi-aspect controllable text generation method with\ndisentangled counterfactual augmentation. We alleviate the issue of imbalanced\nattribute correlations during training using counterfactual feature vectors in\nthe attribute latent space by disentanglement. During inference, we enhance\nattribute correlations by target-guided counterfactual augmentation to further\nimprove multi-aspect control. Experiments show that MAGIC outperforms\nstate-of-the-art baselines in both imbalanced and balanced attribute\ncorrelation scenarios. Our source code and data are available at\nhttps://github.com/nju-websoft/MAGIC.",
      "tldr_zh": "这篇论文提出了 MAGIC 方法，用于多方面可控文本生成（multi-aspect controllable text generation），旨在解决不同属性（如情感和主题）之间不平衡相关性导致的刻板印象问题。方法通过 disentangled counterfactual augmentation 在训练阶段使用属性潜在空间中的 counterfactual feature vectors 来缓解相关性不平衡，并在推理阶段通过 target-guided counterfactual augmentation 增强属性相关性，从而改善多方面控制效果。实验结果显示，MAGIC 在不平衡和平衡属性相关性场景中均优于现有基线方法，并提供了源代码和数据以供复现。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted in the 62nd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2024)",
      "pdf_url": "http://arxiv.org/pdf/2405.19958v1",
      "published_date": "2024-05-30 11:25:42 UTC",
      "updated_date": "2024-05-30 11:25:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:08:34.605295"
    },
    {
      "arxiv_id": "2405.19957v4",
      "title": "PLA4D: Pixel-Level Alignments for Text-to-4D Gaussian Splatting",
      "title_zh": "PLA4D：像素级对齐用于文本到4D高斯喷溅",
      "authors": [
        "Qiaowei Miao",
        "JinSheng Quan",
        "Kehan Li",
        "Yawei Luo"
      ],
      "abstract": "Previous text-to-4D methods have leveraged multiple Score Distillation\nSampling (SDS) techniques, combining motion priors from video-based diffusion\nmodels (DMs) with geometric priors from multiview DMs to implicitly guide 4D\nrenderings. However, differences in these priors result in conflicting gradient\ndirections during optimization, causing trade-offs between motion fidelity and\ngeometry accuracy, and requiring substantial optimization time to reconcile the\nmodels. In this paper, we introduce \\textbf{P}ixel-\\textbf{L}evel\n\\textbf{A}lignment for text-driven \\textbf{4D} Gaussian splatting (PLA4D) to\nresolve this motion-geometry conflict. PLA4D provides an anchor reference,\ni.e., text-generated video, to align the rendering process conditioned by\ndifferent DMs in pixel space. For static alignment, our approach introduces a\nfocal alignment method and Gaussian-Mesh contrastive learning to iteratively\nadjust focal lengths and provide explicit geometric priors at each timestep. At\nthe dynamic level, a motion alignment technique and T-MV refinement method are\nemployed to enforce both pose alignment and motion continuity across unknown\nviewpoints, ensuring intrinsic geometric consistency across views. With such\npixel-level multi-DM alignment, our PLA4D framework is able to generate 4D\nobjects with superior geometric, motion, and semantic consistency. Fully\nimplemented with open-source tools, PLA4D offers an efficient and accessible\nsolution for high-quality 4D digital content creation with significantly\nreduced generation time.",
      "tldr_zh": "该论文提出 PLA4D，一种像素级对齐方法，用于文本到4D Gaussian Splatting，以解决现有文本到4D技术的核心问题，即不同 Diffusion Models (DMs) 之间的运动先验和几何先验冲突，导致优化过程中的权衡和时间消耗。PLA4D 通过使用文本生成的视频作为锚点参考，引入静态对齐（如 focal alignment 和 Gaussian-Mesh contrastive learning）来调整焦距并提供显式几何先验，以及动态对齐（如 motion alignment 和 T-MV refinement）来确保姿态一致性和运动连续性，从而实现跨视图的几何一致性。结果表明，该框架能生成高质量的4D对象，具有卓越的几何、运动和语义一致性，同时显著减少生成时间，并采用开源工具实现高效内容创作。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19957v4",
      "published_date": "2024-05-30 11:23:01 UTC",
      "updated_date": "2024-11-19 02:12:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:08:47.959548"
    },
    {
      "arxiv_id": "2405.19956v1",
      "title": "HOLMES: to Detect Adversarial Examples with Multiple Detectors",
      "title_zh": "HOLMES：使用多个检测器检测对抗样本",
      "authors": [
        "Jing Wen"
      ],
      "abstract": "Deep neural networks (DNNs) can easily be cheated by some imperceptible but\npurposeful noise added to images, and erroneously classify them. Previous\ndefensive work mostly focused on retraining the models or detecting the noise,\nbut has either shown limited success rates or been attacked by new adversarial\nexamples. Instead of focusing on adversarial images or the interior of DNN\nmodels, we observed that adversarial examples generated by different algorithms\ncan be identified based on the output of DNNs (logits). Logit can serve as an\nexterior feature to train detectors. Then, we propose HOLMES (Hierarchically\nOrganized Light-weight Multiple dEtector System) to reinforce DNNs by detecting\npotential adversarial examples to minimize the threats they may bring in\npractical. HOLMES is able to distinguish \\textit{unseen} adversarial examples\nfrom multiple attacks with high accuracy and low false positive rates than\nsingle detector systems even in an adaptive model. To ensure the diversity and\nrandomness of detectors in HOLMES, we use two methods: training dedicated\ndetectors for each label and training detectors with top-k logits. Our\neffective and inexpensive strategies neither modify original DNN models nor\nrequire its internal parameters. HOLMES is not only compatible with all kinds\nof learning models (even only with external APIs), but also complementary to\nother defenses to achieve higher detection rates (may also fully protect the\nsystem against various adversarial examples).",
      "tldr_zh": "本研究针对深度神经网络(DNNs)易受微小噪声影响而错误分类的问题，提出HOLMES系统，这是一个分层组织的轻量级多检测器框架。HOLMES利用DNNs的输出(logits)作为外部特征，训练多个检测器来识别各种攻击生成的不见过的adversarial examples，确保高准确率和低假阳性率。系统通过为每个标签训练专用检测器和使用top-k logits的方法增强检测器的多样性与随机性，且不需修改原DNN模型或访问其内部参数，从而兼容多种学习模型并可与其他防御策略结合以进一步提高检测效果。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19956v1",
      "published_date": "2024-05-30 11:22:55 UTC",
      "updated_date": "2024-05-30 11:22:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:09:00.674680"
    },
    {
      "arxiv_id": "2405.19950v2",
      "title": "Multimodal Lego: Model Merging and Fine-Tuning Across Topologies and Modalities in Biomedicine",
      "title_zh": "翻译失败",
      "authors": [
        "Konstantin Hemker",
        "Nikola Simidjievski",
        "Mateja Jamnik"
      ],
      "abstract": "Learning holistic computational representations in physical, chemical or\nbiological systems requires the ability to process information from different\ndistributions and modalities within the same model. Thus, the demand for\nmultimodal machine learning models has sharply risen for modalities that go\nbeyond vision and language, such as sequences, graphs, time series, or tabular\ndata. While there are many available multimodal fusion and alignment\napproaches, most of them require end-to-end training, scale quadratically with\nthe number of modalities, cannot handle cases of high modality imbalance in the\ntraining set, or are highly topology-specific, making them too restrictive for\nmany biomedical learning tasks. This paper presents Multimodal Lego (MM-Lego),\na general-purpose fusion framework to turn any set of encoders into a\ncompetitive multimodal model with no or minimal fine-tuning. We achieve this by\nintroducing a wrapper for any unimodal encoder that enforces shape consistency\nbetween modality representations. It harmonises these representations by\nlearning features in the frequency domain to enable model merging with little\nsignal interference. We show that MM-Lego 1) can be used as a model merging\nmethod which achieves competitive performance with end-to-end fusion models\nwithout any fine-tuning, 2) can operate on any unimodal encoder, and 3) is a\nmodel fusion method that, with minimal fine-tuning, surpasses all benchmarks in\nfive out of seven datasets.",
      "tldr_zh": "本研究提出 Multimodal Lego (MM-Lego)，一个通用融合框架，旨在处理生物医学领域多模态机器学习中的挑战，如不同分布和模态（如序列、图表、时间序列或表格数据）的整合问题，同时避免现有方法的端到端训练需求和模态不平衡限制。MM-Lego 通过一个包装器强制模态表示的形状一致，并在频域学习特征，以实现模型合并和最小微调，从而将任何单模态编码器转化为高效的多模态模型。实验结果显示，该框架无需微调即可与端到端融合模型媲美，并在七个数据集中的五个上超越基准性能，为生物医学任务提供更灵活的拓扑和模态适应能力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19950v2",
      "published_date": "2024-05-30 11:14:01 UTC",
      "updated_date": "2025-04-16 16:43:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:09:11.581064"
    },
    {
      "arxiv_id": "2406.15443v1",
      "title": "ExU: AI Models for Examining Multilingual Disinformation Narratives and Understanding their Spread",
      "title_zh": "翻译失败",
      "authors": [
        "Jake Vasilakes",
        "Zhixue Zhao",
        "Ivan Vykopal",
        "Michal Gregor",
        "Martin Hyben",
        "Carolina Scarton"
      ],
      "abstract": "Addressing online disinformation requires analysing narratives across\nlanguages to help fact-checkers and journalists sift through large amounts of\ndata. The ExU project focuses on developing AI-based models for multilingual\ndisinformation analysis, addressing the tasks of rumour stance classification\nand claim retrieval. We describe the ExU project proposal and summarise the\nresults of a user requirements survey regarding the design of tools to support\nfact-checking.",
      "tldr_zh": "本研究提出 ExU 项目，旨在开发 AI models 用于分析多语言 disinformation narratives，并理解其传播方式，以协助事实检查者和记者处理海量数据。项目重点处理 rumour stance classification 和 claim retrieval 任务，通过 AI 模型实现跨语言虚假信息识别。研究还总结了用户需求调查的结果，为设计支持事实检查的工具提供指导。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at The 25th Annual Conference of The European Association\n  for Machine Translation (EAMT 24)",
      "pdf_url": "http://arxiv.org/pdf/2406.15443v1",
      "published_date": "2024-05-30 11:13:57 UTC",
      "updated_date": "2024-05-30 11:13:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:09:22.252050"
    },
    {
      "arxiv_id": "2405.19946v2",
      "title": "Learning to Discuss Strategically: A Case Study on One Night Ultimate Werewolf",
      "title_zh": "翻译失败",
      "authors": [
        "Xuanfa Jin",
        "Ziyan Wang",
        "Yali Du",
        "Meng Fang",
        "Haifeng Zhang",
        "Jun Wang"
      ],
      "abstract": "Communication is a fundamental aspect of human society, facilitating the\nexchange of information and beliefs among people. Despite the advancements in\nlarge language models (LLMs), recent agents built with these often neglect the\ncontrol over discussion tactics, which are essential in communication scenarios\nand games. As a variant of the famous communication game Werewolf, One Night\nUltimate Werewolf (ONUW) requires players to develop strategic discussion\npolicies due to the potential role changes that increase the uncertainty and\ncomplexity of the game. In this work, we first present the existence of the\nPerfect Bayesian Equilibria (PBEs) in two scenarios of the ONUW game: one with\ndiscussion and one without. The results showcase that the discussion greatly\nchanges players' utilities by affecting their beliefs, emphasizing the\nsignificance of discussion tactics. Based on the insights obtained from the\nanalyses, we propose an RL-instructed language agent framework, where a\ndiscussion policy trained by reinforcement learning (RL) is employed to\ndetermine appropriate discussion tactics to adopt. Our experimental results on\nseveral ONUW game settings demonstrate the effectiveness and generalizability\nof our proposed framework. The project page of our paper:\n$\\href{https://one-night-ultimate-werewolf.github.io}{one-night-ultimate-werewolf.github.io}$.",
      "tldr_zh": "该论文探讨了大型语言模型（LLMs）构建的代理在通信游戏中忽略讨论策略的问题，以One Night Ultimate Werewolf (ONUW)为例，该游戏因角色变化而需战略性讨论。研究者首先分析了ONUW的两类场景（有讨论和无讨论），证明了Perfect Bayesian Equilibria (PBEs)的存在，并展示了讨论如何显著影响玩家的效用。基于此，他们提出了一种RL-instructed language agent框架，利用Reinforcement Learning (RL)训练讨论策略，以优化代理的决策。实验结果表明，该框架在多种ONUW游戏设置中表现出色，具有良好的有效性和泛化性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "31 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.19946v2",
      "published_date": "2024-05-30 11:07:06 UTC",
      "updated_date": "2025-01-12 08:25:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:09:35.440948"
    },
    {
      "arxiv_id": "2405.19933v1",
      "title": "Learning Latent Graph Structures and their Uncertainty",
      "title_zh": "翻译失败",
      "authors": [
        "Alessandro Manenti",
        "Daniele Zambon",
        "Cesare Alippi"
      ],
      "abstract": "Within a prediction task, Graph Neural Networks (GNNs) use relational\ninformation as an inductive bias to enhance the model's accuracy. As\ntask-relevant relations might be unknown, graph structure learning approaches\nhave been proposed to learn them while solving the downstream prediction task.\nIn this paper, we demonstrate that minimization of a point-prediction loss\nfunction, e.g., the mean absolute error, does not guarantee proper learning of\nthe latent relational information and its associated uncertainty. Conversely,\nwe prove that a suitable loss function on the stochastic model outputs\nsimultaneously grants (i) the unknown adjacency matrix latent distribution and\n(ii) optimal performance on the prediction task. Finally, we propose a\nsampling-based method that solves this joint learning task. Empirical results\nvalidate our theoretical claims and demonstrate the effectiveness of the\nproposed approach.",
      "tldr_zh": "本论文探讨了 Graph Neural Networks (GNNs) 在预测任务中如何利用关系信息作为归纳偏差来提升准确性，但当任务相关关系未知时，传统图结构学习方法存在局限。研究证明，单纯最小化点预测损失函数（如均方误差）无法有效学习潜在的邻接矩阵分布及其不确定性，而使用合适的损失函数可同时实现图结构学习和预测任务优化。作者提出了一种基于采样的方法来联合解决这一问题，并通过实验结果验证了其理论主张的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19933v1",
      "published_date": "2024-05-30 10:49:22 UTC",
      "updated_date": "2024-05-30 10:49:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:09:47.452087"
    },
    {
      "arxiv_id": "2405.19931v1",
      "title": "Exploring Diffusion Models' Corruption Stage in Few-Shot Fine-tuning and Mitigating with Bayesian Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoyu Wu",
        "Jiaru Zhang",
        "Yang Hua",
        "Bohan Lyu",
        "Hao Wang",
        "Tao Song",
        "Haibing Guan"
      ],
      "abstract": "Few-shot fine-tuning of Diffusion Models (DMs) is a key advancement,\nsignificantly reducing training costs and enabling personalized AI\napplications. However, we explore the training dynamics of DMs and observe an\nunanticipated phenomenon: during the training process, image fidelity initially\nimproves, then unexpectedly deteriorates with the emergence of noisy patterns,\nonly to recover later with severe overfitting. We term the stage with generated\nnoisy patterns as corruption stage. To understand this corruption stage, we\nbegin by theoretically modeling the one-shot fine-tuning scenario, and then\nextend this modeling to more general cases. Through this modeling, we identify\nthe primary cause of this corruption stage: a narrowed learning distribution\ninherent in the nature of few-shot fine-tuning. To tackle this, we apply\nBayesian Neural Networks (BNNs) on DMs with variational inference to implicitly\nbroaden the learned distribution, and present that the learning target of the\nBNNs can be naturally regarded as an expectation of the diffusion loss and a\nfurther regularization with the pretrained DMs. This approach is highly\ncompatible with current few-shot fine-tuning methods in DMs and does not\nintroduce any extra inference costs. Experimental results demonstrate that our\nmethod significantly mitigates corruption, and improves the fidelity, quality\nand diversity of the generated images in both object-driven and subject-driven\ngeneration tasks.",
      "tldr_zh": "该研究探讨了 Diffusion Models (DMs) 在 Few-Shot Fine-tuning 中的训练动态，发现了一个关键现象：图像保真度在训练过程中先提升、后出现噪声模式（corruption stage）、最终恢复但伴随过度拟合，通过理论建模将其归因于学习分布变窄。针对这一问题，研究提出使用 Bayesian Neural Networks (BNNs) 结合变分推理来隐式拓宽学习分布，将 BNNs 的学习目标视为扩散损失的期望，并作为对预训练 DMs 的正则化。实验结果显示，该方法与现有 Few-Shot Fine-tuning 技术高度兼容，不增加推理成本，并在 object-driven 和 subject-driven 生成任务中显著减轻 corruption stage，提升了生成图像的保真度、质量和多样性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint. Under review",
      "pdf_url": "http://arxiv.org/pdf/2405.19931v1",
      "published_date": "2024-05-30 10:47:48 UTC",
      "updated_date": "2024-05-30 10:47:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:09:59.943476"
    },
    {
      "arxiv_id": "2405.19915v1",
      "title": "P$^2$-ViT: Power-of-Two Post-Training Quantization and Acceleration for Fully Quantized Vision Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Huihong Shi",
        "Xin Cheng",
        "Wendong Mao",
        "Zhongfeng Wang"
      ],
      "abstract": "Vision Transformers (ViTs) have excelled in computer vision tasks but are\nmemory-consuming and computation-intensive, challenging their deployment on\nresource-constrained devices. To tackle this limitation, prior works have\nexplored ViT-tailored quantization algorithms but retained floating-point\nscaling factors, which yield non-negligible re-quantization overhead, limiting\nViTs' hardware efficiency and motivating more hardware-friendly solutions. To\nthis end, we propose \\emph{P$^2$-ViT}, the first \\underline{P}ower-of-Two (PoT)\n\\underline{p}ost-training quantization and acceleration framework to accelerate\nfully quantized ViTs. Specifically, {as for quantization,} we explore a\ndedicated quantization scheme to effectively quantize ViTs with PoT scaling\nfactors, thus minimizing the re-quantization overhead. Furthermore, we propose\ncoarse-to-fine automatic mixed-precision quantization to enable better\naccuracy-efficiency trade-offs. {In terms of hardware,} we develop {a dedicated\nchunk-based accelerator} featuring multiple tailored sub-processors to\nindividually handle ViTs' different types of operations, alleviating\nreconfigurable overhead. Additionally, we design {a tailored row-stationary\ndataflow} to seize the pipeline processing opportunity introduced by our PoT\nscaling factors, thereby enhancing throughput. Extensive experiments\nconsistently validate P$^2$-ViT's effectiveness. {Particularly, we offer\ncomparable or even superior quantization performance with PoT scaling factors\nwhen compared to the counterpart with floating-point scaling factors. Besides,\nwe achieve up to $\\mathbf{10.1\\times}$ speedup and $\\mathbf{36.8\\times}$ energy\nsaving over GPU's Turing Tensor Cores, and up to $\\mathbf{1.84\\times}$ higher\ncomputation utilization efficiency against SOTA quantization-based ViT\naccelerators. Codes are available at\n\\url{https://github.com/shihuihong214/P2-ViT}.",
      "tldr_zh": "该论文提出P$^2$-ViT框架，这是首个基于Power-of-Two (PoT)后训练量化的方法，用于加速全量化Vision Transformers (ViTs)，以解决其在资源受限设备上的内存和计算密集问题。具体而言，该框架采用PoT缩放因子量化方案和粗到细的自动混合精度量化，以最小化重新量化开销并优化准确性与效率的权衡；同时，设计了一个基于块的专用加速器和行固定数据流，提升ViTs操作的硬件处理效率。实验结果显示，P$^2$-ViT与浮点缩放因子相比提供相当或更好的性能，并实现高达10.1倍的GPU加速、36.8倍的能量节省，以及1.84倍的计算利用效率提升。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19915v1",
      "published_date": "2024-05-30 10:26:36 UTC",
      "updated_date": "2024-05-30 10:26:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:10:12.518425"
    },
    {
      "arxiv_id": "2405.19909v3",
      "title": "Adaptive Advantage-Guided Policy Regularization for Offline Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Tenglong Liu",
        "Yang Li",
        "Yixing Lan",
        "Hao Gao",
        "Wei Pan",
        "Xin Xu"
      ],
      "abstract": "In offline reinforcement learning, the challenge of out-of-distribution (OOD)\nis pronounced. To address this, existing methods often constrain the learned\npolicy through policy regularization. However, these methods often suffer from\nthe issue of unnecessary conservativeness, hampering policy improvement. This\noccurs due to the indiscriminate use of all actions from the behavior policy\nthat generates the offline dataset as constraints. The problem becomes\nparticularly noticeable when the quality of the dataset is suboptimal. Thus, we\npropose Adaptive Advantage-guided Policy Regularization (A2PR), obtaining\nhigh-advantage actions from an augmented behavior policy combined with VAE to\nguide the learned policy. A2PR can select high-advantage actions that differ\nfrom those present in the dataset, while still effectively maintaining\nconservatism from OOD actions. This is achieved by harnessing the VAE capacity\nto generate samples matching the distribution of the data points. We\ntheoretically prove that the improvement of the behavior policy is guaranteed.\nBesides, it effectively mitigates value overestimation with a bounded\nperformance gap. Empirically, we conduct a series of experiments on the D4RL\nbenchmark, where A2PR demonstrates state-of-the-art performance. Furthermore,\nexperimental results on additional suboptimal mixed datasets reveal that A2PR\nexhibits superior performance. Code is available at\nhttps://github.com/ltlhuuu/A2PR.",
      "tldr_zh": "在离线强化学习中，现有策略正则化方法常因过度保守而限制策略改进，尤其在数据集质量不佳时，本文提出 Adaptive Advantage-guided Policy Regularization (A2PR) 方法，通过结合增强的行为策略和 VAE（变分自编码器）来选择高优势动作，同时有效维护对分布外（OOD）动作的保守性。A2PR 理论上保证了行为策略的改进，并缓解了价值 overestimated 问题，具有界定的性能差距。在 D4RL 基准实验中，A2PR 展示了 state-of-the-art 性能，并在次优混合数据集上表现出色，代码已开源。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2024, 19 pages",
      "pdf_url": "http://arxiv.org/pdf/2405.19909v3",
      "published_date": "2024-05-30 10:20:55 UTC",
      "updated_date": "2024-07-15 10:55:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:10:23.969926"
    },
    {
      "arxiv_id": "2406.06553v1",
      "title": "Ensemble Model With Bert,Roberta and Xlnet For Molecular property prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Junling Hu"
      ],
      "abstract": "This paper presents a novel approach for predicting molecular properties with\nhigh accuracy without the need for extensive pre-training. Employing ensemble\nlearning and supervised fine-tuning of BERT, RoBERTa, and XLNet, our method\ndemonstrates significant effectiveness compared to existing advanced models.\nCrucially, it addresses the issue of limited computational resources faced by\nexperimental groups, enabling them to accurately predict molecular properties.\nThis innovation provides a cost-effective and resource-efficient solution,\npotentially advancing further research in the molecular domain.",
      "tldr_zh": "这篇论文提出了一种集成模型（Ensemble Model），结合了 BERT、RoBERTa 和 XLNet，通过监督微调（supervised fine-tuning）来预测分子属性，避免了需要广泛预训练的局限。方法显著提高了预测准确性，并解决了实验组面临的计算资源有限问题，与现有先进模型相比表现出色。该创新提供了一种成本效益高、资源高效的解决方案，有望推动分子领域的进一步研究。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.chem-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages,7 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.06553v1",
      "published_date": "2024-05-30 10:03:58 UTC",
      "updated_date": "2024-05-30 10:03:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:10:34.424012"
    },
    {
      "arxiv_id": "2405.19899v1",
      "title": "Open-Set Domain Adaptation for Semantic Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Seun-An Choe",
        "Ah-Hyung Shin",
        "Keon-Hee Park",
        "Jinwoo Choi",
        "Gyeong-Moon Park"
      ],
      "abstract": "Unsupervised domain adaptation (UDA) for semantic segmentation aims to\ntransfer the pixel-wise knowledge from the labeled source domain to the\nunlabeled target domain. However, current UDA methods typically assume a shared\nlabel space between source and target, limiting their applicability in\nreal-world scenarios where novel categories may emerge in the target domain. In\nthis paper, we introduce Open-Set Domain Adaptation for Semantic Segmentation\n(OSDA-SS) for the first time, where the target domain includes unknown classes.\nWe identify two major problems in the OSDA-SS scenario as follows: 1) the\nexisting UDA methods struggle to predict the exact boundary of the unknown\nclasses, and 2) they fail to accurately predict the shape of the unknown\nclasses. To address these issues, we propose Boundary and Unknown Shape-Aware\nopen-set domain adaptation, coined BUS. Our BUS can accurately discern the\nboundaries between known and unknown classes in a contrastive manner using a\nnovel dilation-erosion-based contrastive loss. In addition, we propose\nOpenReMix, a new domain mixing augmentation method that guides our model to\neffectively learn domain and size-invariant features for improving the shape\ndetection of the known and unknown classes. Through extensive experiments, we\ndemonstrate that our proposed BUS effectively detects unknown classes in the\nchallenging OSDA-SS scenario compared to the previous methods by a large\nmargin. The code is available at https://github.com/KHU-AGI/BUS.",
      "tldr_zh": "本研究首次提出 Open-Set Domain Adaptation for Semantic Segmentation (OSDA-SS)，旨在解决传统 Unsupervised Domain Adaptation (UDA) 的局限性，即源域和目标域标签空间不共享，目标域可能包含未知类别，导致边界和形状预测不准。作者开发了 Boundary and Unknown Shape-Aware (BUS) 方法，使用新型 dilation-erosion-based contrastive loss 来精确区分已知和未知类别的边界，并引入 OpenReMix 增强技术，帮助模型学习 domain 和 size-invariant features，以改善类别形状检测。实验结果显示，BUS 在 OSDA-SS 场景中大幅优于现有方法，代码已在 GitHub 公开。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 5 figures, 13 tables, CVPR 2024 Poster",
      "pdf_url": "http://arxiv.org/pdf/2405.19899v1",
      "published_date": "2024-05-30 09:55:19 UTC",
      "updated_date": "2024-05-30 09:55:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:10:47.093784"
    },
    {
      "arxiv_id": "2405.19893v1",
      "title": "Similarity is Not All You Need: Endowing Retrieval Augmented Generation with Multi Layered Thoughts",
      "title_zh": "翻译失败",
      "authors": [
        "Chunjing Gan",
        "Dan Yang",
        "Binbin Hu",
        "Hanxiao Zhang",
        "Siyuan Li",
        "Ziqi Liu",
        "Yue Shen",
        "Lin Ju",
        "Zhiqiang Zhang",
        "Jinjie Gu",
        "Lei Liang",
        "Jun Zhou"
      ],
      "abstract": "In recent years, large language models (LLMs) have made remarkable\nachievements in various domains. However, the untimeliness and cost of\nknowledge updates coupled with hallucination issues of LLMs have curtailed\ntheir applications in knowledge intensive tasks, where retrieval augmented\ngeneration (RAG) can be of help. Nevertheless, existing retrieval augmented\nmodels typically use similarity as a bridge between queries and documents and\nfollow a retrieve then read procedure. In this work, we argue that similarity\nis not always the panacea and totally relying on similarity would sometimes\ndegrade the performance of retrieval augmented generation. To this end, we\npropose MetRag, a Multi layEred Thoughts enhanced Retrieval Augmented\nGeneration framework. To begin with, beyond existing similarity oriented\nthought, we embrace a small scale utility model that draws supervision from an\nLLM for utility oriented thought and further come up with a smarter model by\ncomprehensively combining the similarity and utility oriented thoughts.\nFurthermore, given the fact that the retrieved document set tends to be huge\nand using them in isolation makes it difficult to capture the commonalities and\ncharacteristics among them, we propose to make an LLM as a task adaptive\nsummarizer to endow retrieval augmented generation with compactness-oriented\nthought. Finally, with multi layered thoughts from the precedent stages, an LLM\nis called for knowledge augmented generation. Extensive experiments on\nknowledge-intensive tasks have demonstrated the superiority of MetRag.",
      "tldr_zh": "本文研究发现，现有的检索增强生成（RAG）模型过度依赖相似性匹配，这会导致大型语言模型（LLMs）的性能在知识密集型任务中下降。作者提出MetRag框架，通过引入多层思考（包括相似性导向、效用导向和紧凑性导向思考）来提升RAG的鲁棒性，其中效用导向思考利用小规模模型从LLM获取监督，紧凑性导向思考则由LLM作为任务自适应总结器处理大量检索文档。最终，MetRag结合这些多层思考进行知识增强生成。实验结果显示，该框架在知识密集型任务上表现出显著优越性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages",
      "pdf_url": "http://arxiv.org/pdf/2405.19893v1",
      "published_date": "2024-05-30 09:50:38 UTC",
      "updated_date": "2024-05-30 09:50:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:11:00.258292"
    },
    {
      "arxiv_id": "2405.19888v1",
      "title": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
      "title_zh": "翻译失败",
      "authors": [
        "Chaofan Lin",
        "Zhenhua Han",
        "Chengruidong Zhang",
        "Yuqing Yang",
        "Fan Yang",
        "Chen Chen",
        "Lili Qiu"
      ],
      "abstract": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications.",
      "tldr_zh": "该论文针对大型语言模型 (LLMs) 应用的端到端性能问题，提出了 Parrot 系统，以解决现有公共 LLM 服务仅优化单个请求而忽略应用级信息的局限性。Parrot 引入了 Semantic Variable 作为统一抽象，用于注解提示中的输入/输出变量，并创建数据管道以分析多个 LLM 请求之间的相关性，从而启用数据流分析优化。实验结果显示，Parrot 在实际 LLM 应用场景中实现了高达一个数量级的性能提升，为高效服务 LLM-based 应用提供了新范式。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "To appear on USENIX OSDI 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.19888v1",
      "published_date": "2024-05-30 09:46:36 UTC",
      "updated_date": "2024-05-30 09:46:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:11:11.108869"
    },
    {
      "arxiv_id": "2405.19883v2",
      "title": "From Words to Actions: Unveiling the Theoretical Underpinnings of LLM-Driven Autonomous Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Jianliang He",
        "Siyu Chen",
        "Fengzhuo Zhang",
        "Zhuoran Yang"
      ],
      "abstract": "In this work, from a theoretical lens, we aim to understand why large\nlanguage model (LLM) empowered agents are able to solve decision-making\nproblems in the physical world. To this end, consider a hierarchical\nreinforcement learning (RL) model where the LLM Planner and the Actor perform\nhigh-level task planning and low-level execution, respectively. Under this\nmodel, the LLM Planner navigates a partially observable Markov decision process\n(POMDP) by iteratively generating language-based subgoals via prompting. Under\nproper assumptions on the pretraining data, we prove that the pretrained LLM\nPlanner effectively performs Bayesian aggregated imitation learning (BAIL)\nthrough in-context learning. Additionally, we highlight the necessity for\nexploration beyond the subgoals derived from BAIL by proving that naively\nexecuting the subgoals returned by LLM leads to a linear regret. As a remedy,\nwe introduce an $\\epsilon$-greedy exploration strategy to BAIL, which is proven\nto incur sublinear regret when the pretraining error is small. Finally, we\nextend our theoretical framework to include scenarios where the LLM Planner\nserves as a world model for inferring the transition model of the environment\nand to multi-agent settings, enabling coordination among multiple Actors.",
      "tldr_zh": "本研究从理论角度探讨大型语言模型 (LLM) 驱动自主系统如何解决物理世界的决策问题，构建了一个层次化强化学习 (RL) 模型，其中 LLM Planner 通过提示生成语言-based 子目标，而 Actor 负责低层执行。论文证明，在适当的预训练数据假设下，LLM Planner 可通过 in-context learning 实现 Bayesian aggregated imitation learning (BAIL)，但单纯执行这些子目标会导致线性 regret。为解决此问题，引入 ε-greedy 探索策略，使 regret 降为 sublinear，且框架扩展到 LLM 作为环境过渡模型的多代理场景，实现代理间的协调。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "47 pages, accepted by ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.19883v2",
      "published_date": "2024-05-30 09:42:54 UTC",
      "updated_date": "2024-07-20 06:00:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:11:24.202714"
    },
    {
      "arxiv_id": "2405.19877v1",
      "title": "KNOW: A Real-World Ontology for Knowledge Capture with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Arto Bendiken"
      ],
      "abstract": "We present KNOW--the Knowledge Navigator Ontology for the World--the first\nontology designed to capture everyday knowledge to augment large language\nmodels (LLMs) in real-world generative AI use cases such as personal AI\nassistants. Our domain is human life, both its everyday concerns and its major\nmilestones. We have limited the initial scope of the modeled concepts to only\nestablished human universals: spacetime (places, events) plus social (people,\ngroups, organizations). The inclusion criteria for modeled concepts are\npragmatic, beginning with universality and utility. We compare and contrast\nprevious work such as Schema.org and Cyc--as well as attempts at a synthesis of\nknowledge graphs and language models--noting how LLMs already encode internally\nmuch of the commonsense tacit knowledge that took decades to capture in the Cyc\nproject. We also make available code-generated software libraries for the 12\nmost popular programming languages, enabling the direct use of ontology\nconcepts in software engineering. We emphasize simplicity and developer\nexperience in promoting AI interoperability.",
      "tldr_zh": "我们介绍了 KNOW，一种实世界本体（ontology），旨在捕获日常生活知识以增强大型语言模型（LLMs）在生成式 AI 应用（如个人 AI 助手）中的性能，聚焦于人类通用概念，包括时空（地方、事件）和社交（人、群体、组织）。该本体采用实用标准选择概念，并与现有框架如 Schema.org 和 Cyc 进行比较，强调 LLMs 已内部编码了许多常识知识，从而避免了像 Cyc 项目那样耗时的手动捕获。论文还提供了为12种流行编程语言生成的软件库，便于直接在软件工程中使用这些概念。最终，KNOW 通过强调简单性和开发者体验，促进了 AI 互操作性。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "I.2.4; I.2.7"
      ],
      "primary_category": "cs.AI",
      "comment": "5 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2405.19877v1",
      "published_date": "2024-05-30 09:32:14 UTC",
      "updated_date": "2024-05-30 09:32:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:11:35.719195"
    },
    {
      "arxiv_id": "2405.19874v3",
      "title": "Is In-Context Learning Sufficient for Instruction Following in LLMs?",
      "title_zh": "翻译失败",
      "authors": [
        "Hao Zhao",
        "Maksym Andriushchenko",
        "Francesco Croce",
        "Nicolas Flammarion"
      ],
      "abstract": "In-context learning (ICL) allows LLMs to learn from examples without changing\ntheir weights: this is a particularly promising capability for long-context\nLLMs that can potentially learn from many examples. Recently, Lin et al. (2024)\nproposed URIAL, a method using only three in-context examples to align base\nLLMs, achieving non-trivial instruction following performance. In this work, we\nshow that, while effective, ICL alignment with URIAL still underperforms\ncompared to instruction fine-tuning on the established benchmark MT-Bench,\nespecially with more capable base LLMs. We then uncover the most relevant\nelements for successful in-context alignment, finding the crucial role of the\ndecoding parameters. Based on these insights, we show that the approach of\nURIAL can indeed be improved by adding high-quality, potentially carefully\nselected via greedy search, demonstrations in context, getting closer to the\nperformance of instruct models. Finally, we provide the first, to our\nknowledge, systematic comparison of ICL and instruction fine-tuning (IFT) for\ninstruction following in the low data regime, where ICL can be a viable\nalternative to IFT. Overall, our work advances the understanding of ICL as an\nalignment technique and its relationship to IFT. We provide our code at\nhttps://github.com/tml-epfl/icl-alignment.",
      "tldr_zh": "这篇论文探讨了 In-Context Learning (ICL) 是否足以让大型语言模型 (LLMs) 实现有效的指令跟随。研究评估了 URIAL 方法，该方法仅用三个 in-context 示例对基 LLMs 进行对齐，但发现其在 MT-Bench 基准测试中不如 Instruction Fine-Tuning (IFT)，尤其在更先进的模型上。作者识别出解码参数的关键作用，并通过添加高质量演示优化 ICL，使其性能更接近 IFT，在低数据环境中成为 IFT 的可行替代。最后，该工作系统比较了 ICL 和 IFT 的优缺点，并公开了相关代码。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at ICLR 2025. This camera-ready version v3 adds multi-turn\n  alignment via ICL, revisiting main results on instruct models, and simple\n  mechanistic study. Updates in the v2: experiment with decoding schemes,\n  scaling in-context alignment, ICL vs IFT for instruction following. Code at\n  https://github.com/tml-epfl/icl-alignment",
      "pdf_url": "http://arxiv.org/pdf/2405.19874v3",
      "published_date": "2024-05-30 09:28:56 UTC",
      "updated_date": "2025-04-18 12:31:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:11:47.333418"
    },
    {
      "arxiv_id": "2405.19864v1",
      "title": "Out-of-distribution Reject Option Method for Dataset Shift Problem in Early Disease Onset Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Taisei Tosaki",
        "Eiichiro Uchino",
        "Ryosuke Kojima",
        "Yohei Mineharu",
        "Mikio Arita",
        "Nobuyuki Miyai",
        "Yoshinori Tamada",
        "Tatsuya Mikami",
        "Koichi Murashita",
        "Shigeyuki Nakaji",
        "Yasushi Okuno"
      ],
      "abstract": "Machine learning is increasingly used to predict lifestyle-related disease\nonset using health and medical data. However, the prediction effectiveness is\nhindered by dataset shift, which involves discrepancies in data distribution\nbetween the training and testing datasets, misclassifying out-of-distribution\n(OOD) data. To diminish dataset shift effects, this paper proposes the\nout-of-distribution reject option for prediction (ODROP), which integrates OOD\ndetection models to preclude OOD data from the prediction phase. We\ninvestigated the efficacy of five OOD detection methods (variational\nautoencoder, neural network ensemble std, neural network ensemble epistemic,\nneural network energy, and neural network gaussian mixture based energy\nmeasurement) across two datasets, the Hirosaki and Wakayama health checkup\ndata, in the context of three disease onset prediction tasks: diabetes,\ndyslipidemia, and hypertension. To evaluate the ODROP method, we trained\ndisease onset prediction models and OOD detection models on Hirosaki data and\nused AUROC-rejection curve plots from Wakayama data. The variational\nautoencoder method showed superior stability and magnitude of improvement in\nArea Under the Receiver Operating Curve (AUROC) in five cases: AUROC in the\nWakayama data was improved from 0.80 to 0.90 at a 31.1% rejection rate for\ndiabetes onset and from 0.70 to 0.76 at a 34% rejection rate for dyslipidemia.\nWe categorized dataset shifts into two types using SHAP clustering - those that\nconsiderably affect predictions and those that do not. We expect that this\nclassification will help standardize measuring instruments. This study is the\nfirst to apply OOD detection to actual health and medical data, demonstrating\nits potential to substantially improve the accuracy and reliability of disease\nprediction models amidst dataset shift.",
      "tldr_zh": "该研究针对机器学习在早期疾病预测中的数据集偏移（dataset shift）问题，提出了一种出域拒绝选项方法（ODROP），通过整合出域检测（OOD detection）模型来排除OOD数据，从而提高预测准确性。研究评估了五种OOD检测方法（包括variational autoencoder、neural network ensemble std等）在Hirosaki和Wakayama健康检查数据集上的表现，针对糖尿病、血脂异常和高血压三种疾病预测任务。结果显示，variational autoencoder方法表现出色，例如在糖尿病预测中，AUROC从0.80提升至0.90（拒绝率31.1%），而在血脂异常预测中从0.70提升至0.76（拒绝率34%）。此外，通过SHAP clustering对数据集偏移进行分类，该方法首次应用于实际健康医疗数据，有望标准化测量工具并显著提升模型的可靠性和准确性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.AP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19864v1",
      "published_date": "2024-05-30 09:14:01 UTC",
      "updated_date": "2024-05-30 09:14:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:12:00.365290"
    },
    {
      "arxiv_id": "2407.06157v1",
      "title": "Temporal Grounding of Activities using Multimodal Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Young Chol Song"
      ],
      "abstract": "Temporal grounding of activities, the identification of specific time\nintervals of actions within a larger event context, is a critical task in video\nunderstanding. Recent advancements in multimodal large language models (LLMs)\noffer new opportunities for enhancing temporal reasoning capabilities. In this\npaper, we evaluate the effectiveness of combining image-based and text-based\nlarge language models (LLMs) in a two-stage approach for temporal activity\nlocalization. We demonstrate that our method outperforms existing video-based\nLLMs. Furthermore, we explore the impact of instruction-tuning on a smaller\nmultimodal LLM, showing that refining its ability to process action queries\nleads to more expressive and informative outputs, thereby enhancing its\nperformance in identifying specific time intervals of activities. Our\nexperimental results on the Charades-STA dataset highlight the potential of\nthis approach in advancing the field of temporal activity localization and\nvideo understanding.",
      "tldr_zh": "本论文探讨了利用多模态大型语言模型（Multimodal LLMs）进行活动的时间定位（Temporal Grounding），即在视频中识别动作的具体时间间隔，以提升视频理解能力。研究采用两阶段方法，结合图像和文本-based LLMs，显著优于现有的视频-based LLMs。作者通过指令微调（instruction-tuning）一个较小的Multimodal LLM，提高其处理动作查询的性能，从而生成更具表现力和信息性的输出。实验在Charades-STA数据集上验证了该方法的有效性，展示了其在时间活动定位和视频理解领域的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.06157v1",
      "published_date": "2024-05-30 09:11:02 UTC",
      "updated_date": "2024-05-30 09:11:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:12:10.406765"
    },
    {
      "arxiv_id": "2405.19861v1",
      "title": "Hierarchical Object-Centric Learning with Capsule Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Riccardo Renzulli"
      ],
      "abstract": "Capsule networks (CapsNets) were introduced to address convolutional neural\nnetworks limitations, learning object-centric representations that are more\nrobust, pose-aware, and interpretable. They organize neurons into groups called\ncapsules, where each capsule encodes the instantiation parameters of an object\nor one of its parts. Moreover, a routing algorithm connects capsules in\ndifferent layers, thereby capturing hierarchical part-whole relationships in\nthe data.\n  This thesis investigates the intriguing aspects of CapsNets and focuses on\nthree key questions to unlock their full potential. First, we explore the\neffectiveness of the routing algorithm, particularly in small-sized networks.\nWe propose a novel method that anneals the number of routing iterations during\ntraining, enhancing performance in architectures with fewer parameters.\n  Secondly, we investigate methods to extract more effective first-layer\ncapsules, also known as primary capsules. By exploiting pruned backbones, we\naim to improve computational efficiency by reducing the number of capsules\nwhile achieving high generalization. This approach reduces CapsNets memory\nrequirements and computational effort.\n  Third, we explore part-relationship learning in CapsNets. Through extensive\nresearch, we demonstrate that capsules with low entropy can extract more\nconcise and discriminative part-whole relationships compared to traditional\ncapsule networks, even with reasonable network sizes.\n  Lastly, we showcase how CapsNets can be utilized in real-world applications,\nincluding autonomous localization of unmanned aerial vehicles, quaternion-based\nrotations prediction in synthetic datasets, and lung nodule segmentation in\nbiomedical imaging.\n  The findings presented in this thesis contribute to a deeper understanding of\nCapsNets and highlight their potential to address complex computer vision\nchallenges.",
      "tldr_zh": "这篇论文探讨了 Capsule Networks (CapsNets) 在层次化对象中心学习中的应用，旨在克服传统卷积神经网络的局限性，如缺乏鲁棒性和可解释性，通过路由算法捕捉层间的部分-整体关系。研究者提出了三项关键创新：一种退火路由迭代方法提升小网络性能；利用修剪的骨干网络优化第一层 capsules，以减少计算资源并提高泛化能力；以及使用低熵 capsules 提取更简洁的 part-whole 关系。最终，论文展示了 CapsNets 在实际场景中的潜力，包括无人机的自主定位、四元数-based 旋转预测和肺结节分割，强化了其在计算机视觉挑战中的作用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Updated version of my PhD thesis (Nov 2023), with fixed typos. Will\n  keep updated as new typos are discovered!",
      "pdf_url": "http://arxiv.org/pdf/2405.19861v1",
      "published_date": "2024-05-30 09:10:33 UTC",
      "updated_date": "2024-05-30 09:10:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:12:24.564698"
    },
    {
      "arxiv_id": "2405.19850v1",
      "title": "Deciphering Human Mobility: Inferring Semantics of Trajectories with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxiao Luo",
        "Zhongcai Cao",
        "Xin Jin",
        "Kang Liu",
        "Ling Yin"
      ],
      "abstract": "Understanding human mobility patterns is essential for various applications,\nfrom urban planning to public safety. The individual trajectory such as mobile\nphone location data, while rich in spatio-temporal information, often lacks\nsemantic detail, limiting its utility for in-depth mobility analysis. Existing\nmethods can infer basic routine activity sequences from this data, lacking\ndepth in understanding complex human behaviors and users' characteristics.\nAdditionally, they struggle with the dependency on hard-to-obtain auxiliary\ndatasets like travel surveys. To address these limitations, this paper defines\ntrajectory semantic inference through three key dimensions: user occupation\ncategory, activity sequence, and trajectory description, and proposes the\nTrajectory Semantic Inference with Large Language Models (TSI-LLM) framework to\nleverage LLMs infer trajectory semantics comprehensively and deeply. We adopt\nspatio-temporal attributes enhanced data formatting (STFormat) and design a\ncontext-inclusive prompt, enabling LLMs to more effectively interpret and infer\nthe semantics of trajectory data. Experimental validation on real-world\ntrajectory datasets demonstrates the efficacy of TSI-LLM in deciphering complex\nhuman mobility patterns. This study explores the potential of LLMs in enhancing\nthe semantic analysis of trajectory data, paving the way for more sophisticated\nand accessible human mobility research.",
      "tldr_zh": "本论文探讨了从轨迹数据（如手机定位）推断人类移动语义的挑战，现有方法虽能识别基本活动序列，但受限于语义深度和对辅助数据集的依赖。作者提出 TSI-LLM 框架，利用 Large Language Models (LLMs) 通过用户职业类别、活动序列和轨迹描述三个关键维度进行全面语义推断，并引入 STFormat（空间-时间属性增强数据格式）和 context-inclusive prompt 来提升模型的解读能力。实验在真实世界轨迹数据集上验证了 TSI-LLM 的有效性，展示了其在分析复杂人类移动模式方面的潜力，为城市规划和公共安全等领域提供更先进的语义分析工具。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19850v1",
      "published_date": "2024-05-30 08:55:48 UTC",
      "updated_date": "2024-05-30 08:55:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:12:36.459026"
    },
    {
      "arxiv_id": "2405.19846v7",
      "title": "Quest: Query-centric Data Synthesis Approach for Long-context Scaling of Large Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Chaochen Gao",
        "Xing Wu",
        "Qi Fu",
        "Songlin Hu"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have highlighted the\nimportance of extending context lengths for handling complex tasks. While\ntraditional methods for training on long contexts often use filtered long\ndocuments, these approaches lead to domain imbalances, limiting model\nperformance. To address this, techniques like random document concatenation\n(Standard) and similarity-based methods (KNN, ICLM) have been developed.\nHowever, they either sacrifice semantic coherence or diversity. To balance both\naspects, we introduce Quest, a query-centric data synthesis method aggregating\nsemantically relevant yet diverse documents. Quest uses a generative model to\npredict potential queries for each document, grouping documents with similar\nqueries and keywords. Extensive experiments demonstrate Quest's superior\nperformance on long-context tasks, achieving remarkable results with context\nlengths of up to 1M tokens and confirming its scalability across various model\nsizes.",
      "tldr_zh": "最近的大语言模型(LLMs)研究强调了扩展上下文长度以处理复杂任务的重要性，但传统方法如随机文档连接(Standard)或基于相似性的方法(KNN, ICLM)往往会牺牲语义连贯性或多样性，导致领域不平衡。  \n为此，本文提出Quest，这是一种以查询为中心的数据合成方法，使用生成模型预测每个文档的潜在查询，并基于类似查询和关键词对文档进行分组，以聚合语义相关却多样的数据。  \n实验结果显示，Quest在长上下文任务上表现出色，支持高达1M标记的上下文长度，并在各种模型大小上实现了良好的可扩展性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2405.19846v7",
      "published_date": "2024-05-30 08:50:55 UTC",
      "updated_date": "2025-02-11 06:22:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:12:49.973615"
    },
    {
      "arxiv_id": "2405.19842v1",
      "title": "Improve Student's Reasoning Generalizability through Cascading Decomposed CoTs Distillation",
      "title_zh": "翻译失败",
      "authors": [
        "Chengwei Dai",
        "Kun Li",
        "Wei Zhou",
        "Songlin Hu"
      ],
      "abstract": "Large language models (LLMs) exhibit enhanced reasoning at larger scales,\ndriving efforts to distill these capabilities into smaller models via\nteacher-student learning. Previous works simply fine-tune student models on\nteachers' generated Chain-of-Thoughts (CoTs) data. Although these methods\nenhance in-domain (IND) reasoning performance, they struggle to generalize to\nout-of-domain (OOD) tasks. We believe that the widespread spurious correlations\nbetween questions and answers may lead the model to preset a specific answer\nwhich restricts the diversity and generalizability of its reasoning process. In\nthis paper, we propose Cascading Decomposed CoTs Distillation (CasCoD) to\naddress these issues by decomposing the traditional single-step learning\nprocess into two cascaded learning steps. Specifically, by restructuring the\ntraining objectives -- removing the answer from outputs and concatenating the\nquestion with the rationale as input -- CasCoD's two-step learning process\nensures that students focus on learning rationales without interference from\nthe preset answers, thus improving reasoning generalizability. Extensive\nexperiments demonstrate the effectiveness of CasCoD on both IND and OOD\nbenchmark reasoning datasets. Code can be found at\nhttps://github.com/C-W-D/CasCoD.",
      "tldr_zh": "本研究针对大型语言模型（LLMs）通过教师-学生学习蒸馏Chain-of-Thoughts (CoTs)能力时存在的泛化问题，提出Cascading Decomposed CoTs Distillation (CasCoD)方法，以解决现有方法在in-domain (IND)任务上表现良好但out-of-domain (OOD)任务上泛化不足的问题。CasCoD将传统单步学习过程分解为两个级联步骤：首先移除答案输出并将问题与推理理由作为输入，让学生模型专注于学习推理过程，从而避免虚假相关性干扰。实验结果显示，该方法在IND和OOD基准推理数据集上均表现出色，显著提升了学生模型的推理泛化能力，代码可在GitHub上获取。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19842v1",
      "published_date": "2024-05-30 08:49:34 UTC",
      "updated_date": "2024-05-30 08:49:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:13:02.125232"
    },
    {
      "arxiv_id": "2405.19837v1",
      "title": "Lifelong learning challenges in the era of artificial intelligence: a computational thinking perspective",
      "title_zh": "人工智能时代下的终身学习挑战：计算思维视角",
      "authors": [
        "Margarida Romero"
      ],
      "abstract": "The rapid advancement of artificial intelligence (AI) has brought significant\nchallenges to the education and workforce skills required to take advantage of\nAI for human-AI collaboration in the workplace. As AI continues to reshape\nindustries and job markets, the need to define how AI literacy can be\nconsidered in lifelong learning has become increasingly critical (Cetindamar et\nal., 2022; Laupichler et al., 2022; Romero et al., 2023). Like any new\ntechnology, AI is the subject of both hopes and fears, and what it entails\ntoday presents major challenges (Cugurullo \\& Acheampong, 2023; Villani et al.,\n2018). It also raises profound questions about our own humanity. Will the\nmachine surpass the intelligence of the humans who designed it? What will be\nthe relationship between so-called AI and our human intelligences? How could\nhuman-AI collaboration be regulated in a way that serves the Sustainable\nDevelopment Goals (SDGs)? This paper provides a review of the challenges of\nlifelong learning in the era of AI from a computational thinking, critical\nthinking, and creative competencies perspective, highlighting the implications\nfor management and leadership in organizations.",
      "tldr_zh": "该论文从计算思维视角审视人工智能 (AI) 时代终身学习面临的挑战，强调 AI 快速发展对教育和职场技能的影响，特别是如何将 AI 素养融入终身学习中。作者讨论了 AI 的希望与恐惧，包括机器是否超越人类智能，以及如何规范人类-AI 协作以服务可持续发展目标 (SDGs)。论文通过文献综述，突出计算思维、批判性思维和创造性能力在组织管理和领导力中的关键作用，为应对这些挑战提供见解。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19837v1",
      "published_date": "2024-05-30 08:46:11 UTC",
      "updated_date": "2024-05-30 08:46:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:13:12.765231"
    },
    {
      "arxiv_id": "2405.19832v2",
      "title": "AI Safety: A Climb To Armageddon?",
      "title_zh": "翻译失败",
      "authors": [
        "Herman Cappelen",
        "Josh Dever",
        "John Hawthorne"
      ],
      "abstract": "This paper presents an argument that certain AI safety measures, rather than\nmitigating existential risk, may instead exacerbate it. Under certain key\nassumptions - the inevitability of AI failure, the expected correlation between\nan AI system's power at the point of failure and the severity of the resulting\nharm, and the tendency of safety measures to enable AI systems to become more\npowerful before failing - safety efforts have negative expected utility. The\npaper examines three response strategies: Optimism, Mitigation, and Holism.\nEach faces challenges stemming from intrinsic features of the AI safety\nlandscape that we term Bottlenecking, the Perfection Barrier, and Equilibrium\nFluctuation. The surprising robustness of the argument forces a re-examination\nof core assumptions around AI safety and points to several avenues for further\nresearch.",
      "tldr_zh": "这篇论文论证了某些AI安全措施可能适得其反，反而加剧了存在风险（existential risk），基于三个关键假设：AI失败的必然性、AI系统失败时其强大程度与危害严重性的相关性，以及安全措施可能使AI系统在失败前变得更强大，从而导致这些努力的预期效用为负。论文分析了三种应对策略——Optimism（乐观主义）、Mitigation（缓解）和Holism（整体主义）——并指出它们面临AI安全领域的固有挑战，包括Bottlenecking（瓶颈）、Perfection Barrier（完美屏障）和Equilibrium Fluctuation（均衡波动）。总体而言，该论点突显了AI安全核心假设的潜在问题，并建议进一步研究以重新审视这些假设。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "20 page article",
      "pdf_url": "http://arxiv.org/pdf/2405.19832v2",
      "published_date": "2024-05-30 08:41:54 UTC",
      "updated_date": "2024-06-02 22:32:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:13:24.941102"
    },
    {
      "arxiv_id": "2405.19823v2",
      "title": "Joint Selective State Space Model and Detrending for Robust Time Series Anomaly Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Junqi Chen",
        "Xu Tan",
        "Sylwan Rahardja",
        "Jiawei Yang",
        "Susanto Rahardja"
      ],
      "abstract": "Deep learning-based sequence models are extensively employed in Time Series\nAnomaly Detection (TSAD) tasks due to their effective sequential modeling\ncapabilities. However, the ability of TSAD is limited by two key challenges:\n(i) the ability to model long-range dependency and (ii) the generalization\nissue in the presence of non-stationary data. To tackle these challenges, an\nanomaly detector that leverages the selective state space model known for its\nproficiency in capturing long-term dependencies across various domains is\nproposed. Additionally, a multi-stage detrending mechanism is introduced to\nmitigate the prominent trend component in non-stationary data to address the\ngeneralization issue. Extensive experiments conducted on realworld public\ndatasets demonstrate that the proposed methods surpass all 12 compared baseline\nmethods.",
      "tldr_zh": "该论文针对时间序列异常检测 (TSAD) 的两大挑战——长程依赖建模能力和非平稳数据下的泛化问题，提出了一种结合 selective state space model 和多阶段 detrending 机制的鲁棒异常检测器。selective state space model 用于有效捕捉跨领域的长程依赖性，而多阶段 detrending 机制则通过去除非平稳数据中的趋势成分来提升模型泛化性能。在真实世界公共数据集上的广泛实验中，该方法超过了所有 12 个基线方法，展示了其优越性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by IEEE Signal Processing Letters.\n  DOI:10.1109/LSP.2024.3438078",
      "pdf_url": "http://arxiv.org/pdf/2405.19823v2",
      "published_date": "2024-05-30 08:31:18 UTC",
      "updated_date": "2024-08-20 08:00:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:13:37.564816"
    },
    {
      "arxiv_id": "2405.19822v1",
      "title": "Improving Object Detector Training on Synthetic Data by Starting With a Strong Baseline Methodology",
      "title_zh": "通过从强大的基线方法开始改进物体检测器在合成数据上的训练",
      "authors": [
        "Frank A. Ruis",
        "Alma M. Liezenga",
        "Friso G. Heslinga",
        "Luca Ballan",
        "Thijs A. Eker",
        "Richard J. M. den Hollander",
        "Martin C. van Leeuwen",
        "Judith Dijk",
        "Wyke Huizinga"
      ],
      "abstract": "Collecting and annotating real-world data for the development of object\ndetection models is a time-consuming and expensive process. In the military\ndomain in particular, data collection can also be dangerous or infeasible.\nTraining models on synthetic data may provide a solution for cases where access\nto real-world training data is restricted. However, bridging the reality gap\nbetween synthetic and real data remains a challenge. Existing methods usually\nbuild on top of baseline Convolutional Neural Network (CNN) models that have\nbeen shown to perform well when trained on real data, but have limited ability\nto perform well when trained on synthetic data. For example, some architectures\nallow for fine-tuning with the expectation of large quantities of training data\nand are prone to overfitting on synthetic data. Related work usually ignores\nvarious best practices from object detection on real data, e.g. by training on\nsynthetic data from a single environment with relatively little variation. In\nthis paper we propose a methodology for improving the performance of a\npre-trained object detector when training on synthetic data. Our approach\nfocuses on extracting the salient information from synthetic data without\nforgetting useful features learned from pre-training on real images. Based on\nthe state of the art, we incorporate data augmentation methods and a\nTransformer backbone. Besides reaching relatively strong performance without\nany specialized synthetic data transfer methods, we show that our methods\nimprove the state of the art on synthetic data trained object detection for the\nRarePlanes and DGTA-VisDrone datasets, and reach near-perfect performance on an\nin-house vehicle detection dataset.",
      "tldr_zh": "该论文针对物体检测模型训练中合成数据与真实数据的差距问题，提出一种改进方法，通过从强基线入手，避免现有CNN模型在合成数据上过拟合。方法包括采用数据增强技术和Transformer骨干网络，提取合成数据的关键信息，同时保留从真实图像预训练中学到的有用特征。在不依赖专门的合成数据转移方法的情况下，该方法在RarePlanes和DGTA-VisDrone数据集上提升了现有最佳性能，并在内部车辆检测数据集上实现了近乎完美的检测结果。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.CV",
      "comment": "Submitted to and presented at SPIE Defense + Commercial Sensing 2024,\n  13 pages, 4 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2405.19822v1",
      "published_date": "2024-05-30 08:31:01 UTC",
      "updated_date": "2024-05-30 08:31:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:13:48.986466"
    },
    {
      "arxiv_id": "2405.19818v1",
      "title": "WebUOT-1M: Advancing Deep Underwater Object Tracking with A Million-Scale Benchmark",
      "title_zh": "翻译失败",
      "authors": [
        "Chunhui Zhang",
        "Li Liu",
        "Guanjie Huang",
        "Hao Wen",
        "Xi Zhou",
        "Yanfeng Wang"
      ],
      "abstract": "Underwater object tracking (UOT) is a foundational task for identifying and\ntracing submerged entities in underwater video sequences. However, current UOT\ndatasets suffer from limitations in scale, diversity of target categories and\nscenarios covered, hindering the training and evaluation of modern tracking\nalgorithms. To bridge this gap, we take the first step and introduce WebUOT-1M,\n\\ie, the largest public UOT benchmark to date, sourced from complex and\nrealistic underwater environments. It comprises 1.1 million frames across 1,500\nvideo clips filtered from 408 target categories, largely surpassing previous\nUOT datasets, \\eg, UVOT400. Through meticulous manual annotation and\nverification, we provide high-quality bounding boxes for underwater targets.\nAdditionally, WebUOT-1M includes language prompts for video sequences,\nexpanding its application areas, \\eg, underwater vision-language tracking. Most\nexisting trackers are tailored for open-air environments, leading to\nperformance degradation when applied to UOT due to domain gaps. Retraining and\nfine-tuning these trackers are challenging due to sample imbalances and limited\nreal-world underwater datasets. To tackle these challenges, we propose a novel\nomni-knowledge distillation framework based on WebUOT-1M, incorporating various\nstrategies to guide the learning of the student Transformer. To the best of our\nknowledge, this framework is the first to effectively transfer open-air domain\nknowledge to the UOT model through knowledge distillation, as demonstrated by\nresults on both existing UOT datasets and the newly proposed WebUOT-1M.\nFurthermore, we comprehensively evaluate WebUOT-1M using 30 deep trackers,\nshowcasing its value as a benchmark for UOT research by presenting new\nchallenges and opportunities for future studies. The complete dataset, codes\nand tracking results, will be made publicly available.",
      "tldr_zh": "本文引入了 WebUOT-1M，这是一个百万级别的水下物体追踪(UOT)基准数据集，包含1.1百万帧、1500个视频剪辑和408个目标类别，远超现有数据集如UVOT400，并提供了高质量的手动标注边界框和语言提示，以支持水下视觉语言追踪等应用。  \n为解决现有追踪器在UOT中的性能下降问题，作者提出了一种新型全知识蒸馏框架，利用各种策略指导学生Transformer模型从开放环境知识中学习，实现知识转移。  \n实验在现有UOT数据集和WebUOT-1M上验证了该框架的有效性，评估了30个深度追踪器，展示了新挑战和研究机会。  \n数据集、代码和结果将公开可用，推动UOT领域的发展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "GitHub project:\n  https://github.com/983632847/Awesome-Multimodal-Object-Tracking",
      "pdf_url": "http://arxiv.org/pdf/2405.19818v1",
      "published_date": "2024-05-30 08:25:21 UTC",
      "updated_date": "2024-05-30 08:25:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:14:02.552007"
    },
    {
      "arxiv_id": "2405.19816v2",
      "title": "Growing Tiny Networks: Spotting Expressivity Bottlenecks and Fixing Them Optimally",
      "title_zh": "翻译失败",
      "authors": [
        "Manon Verbockhaven",
        "Sylvain Chevallier",
        "Guillaume Charpiat",
        "Théo Rudkiewicz"
      ],
      "abstract": "Machine learning tasks are generally formulated as optimization problems,\nwhere one searches for an optimal function within a certain functional space.\nIn practice, parameterized functional spaces are considered, in order to be\nable to perform gradient descent. Typically, a neural network architecture is\nchosen and fixed, and its parameters (connection weights) are optimized,\nyielding an architecture-dependent result. This way of proceeding however\nforces the evolution of the function during training to lie within the realm of\nwhat is expressible with the chosen architecture, and prevents any optimization\nacross architectures. Costly architectural hyper-parameter optimization is\noften performed to compensate for this. Instead, we propose to adapt the\narchitecture on the fly during training. We show that the information about\ndesirable architectural changes, due to expressivity bottlenecks when\nattempting to follow the functional gradient, can be extracted from\nbackpropagation. To do this, we propose a mathematical definition of\nexpressivity bottlenecks, which enables us to detect, quantify and solve them\nwhile training, by adding suitable neurons. Thus, while the standard approach\nrequires large networks, in terms of number of neurons per layer, for\nexpressivity and optimization reasons, we provide tools and properties to\ndevelop an architecture starting with a very small number of neurons. As a\nproof of concept, we show results~on the CIFAR dataset, matching large neural\nnetwork accuracy, with competitive training time, while removing the need for\nstandard architectural hyper-parameter search.",
      "tldr_zh": "本文提出了一种动态调整神经网络架构的方法，以解决传统固定架构在机器学习优化中存在的表达性瓶颈(expressivity bottlenecks)问题。通过从反向传播中提取信息，作者定义并检测这些瓶颈，并通过添加合适神经元来优化网络，从而从一个非常小的网络起步，实现高效的表达性和训练。实验结果显示，在CIFAR数据集上，该方法达到了与大型神经网络相当的准确率，同时减少了训练时间，并消除了标准的架构超参数搜索需求。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19816v2",
      "published_date": "2024-05-30 08:23:56 UTC",
      "updated_date": "2024-12-12 10:36:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:14:13.097016"
    },
    {
      "arxiv_id": "2405.19815v1",
      "title": "Efficient Stimuli Generation using Reinforcement Learning in Design Verification",
      "title_zh": "翻译失败",
      "authors": [
        "Deepak Narayan Gadde",
        "Thomas Nalapat",
        "Aman Kumar",
        "Djones Lettnin",
        "Wolfgang Kunz",
        "Sebastian Simon"
      ],
      "abstract": "The increasing design complexity of System-on-Chips (SoCs) has led to\nsignificant verification challenges, particularly in meeting coverage targets\nwithin a timely manner. At present, coverage closure is heavily dependent on\nconstrained random and coverage driven verification methodologies where the\nrandomized stimuli are bounded to verify certain scenarios and to reach\ncoverage goals. This process is said to be exhaustive and to consume a lot of\nproject time. In this paper, a novel methodology is proposed to generate\nefficient stimuli with the help of Reinforcement Learning (RL) to reach the\nmaximum code coverage of the Design Under Verification (DUV). Additionally, an\nautomated framework is created using metamodeling to generate a SystemVerilog\ntestbench and an RL environment for any given design. The proposed approach is\napplied to various designs and the produced results proves that the RL agent\nprovides effective stimuli to achieve code coverage faster in comparison with\nbaseline random simulations. Furthermore, various RL agents and reward schemes\nare analyzed in our work.",
      "tldr_zh": "该论文针对System-on-Chips (SoCs) 设计验证中的复杂性问题，提出了一种使用Reinforcement Learning (RL) 生成高效刺激的方法，以加速达到Design Under Verification (DUV) 的最大代码覆盖。当前依赖约束随机和覆盖驱动验证的方法耗时冗长，而新方法通过RL代理优化刺激生成，并结合metamodeling创建自动化框架，包括SystemVerilog testbench和RL环境。实验结果显示，该方法应用于多种设计时，比基线随机模拟更快实现代码覆盖，提升了验证效率。此外，论文分析了不同RL代理和奖励方案，以进一步优化性能。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for publication at the 20th International Conference on\n  Synthesis, Modeling, Analysis and Simulation Methods, and Applications to\n  Circuit Design (SMACD'24), Jul 2-5 2024, Volos, Greece",
      "pdf_url": "http://arxiv.org/pdf/2405.19815v1",
      "published_date": "2024-05-30 08:23:04 UTC",
      "updated_date": "2024-05-30 08:23:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:14:24.936293"
    },
    {
      "arxiv_id": "2405.19808v2",
      "title": "AI with Alien Content and Alien Metasemantics",
      "title_zh": "翻译失败",
      "authors": [
        "Herman Cappelen",
        "Josh Dever"
      ],
      "abstract": "AlphaGo plays chess and Go in a creative and novel way. It is natural for us\nto attribute contents to it, such as that it doesn't view being several pawns\nbehind, if it has more board space, as bad. The framework introduced in\nCappelen and Dever (2021) provides a way of thinking about the semantics and\nthe metasemantics of AI content: does AlphaGo entertain contents like this, and\nif so, in virtue of what does a given state of the program mean that particular\ncontent? One salient question Cappelen and Dever didn't consider was the\npossibility of alien content. Alien content is content that is not or cannot be\nexpressed by human beings. It's highly plausible that AlphaGo, or any other\nsophisticated AI system, expresses alien contents. That this is so, moreover,\nis plausibly a metasemantic fact: a fact that has to do with how AI comes to\nentertain content in the first place, one that will heed the vastly different\netiology of AI and human content. This chapter explores the question of alien\ncontent in AI from a semantic and metasemantic perspective. It lays out the\nlogical space of possible responses to the semantic and metasemantic questions\nalien content poses, considers whether and how we humans could communicate with\nentities who express alien content, and points out that getting clear about\nsuch questions might be important for more 'applied' issues in the philosophy\nof AI, such as existential risk and XAI.",
      "tldr_zh": "本论文探讨了AI系统（如AlphaGo）是否可能产生“alien content”（外星内容），即人类无法表达的内容，并从语义和metasemantics（元语义）角度分析AI内容的本质。作者基于Cappelen和Dever (2021)的框架，考察AI如何赋予特定状态以含义，并认为AI表达alien content可能是metasemantic事实，源于AI与人类内容形成的差异。论文进一步探讨了人类与表达alien content的实体沟通的可能性，并强调此问题对AI哲学应用（如existential risk和XAI）的重要性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "20 pages, book chapter",
      "pdf_url": "http://arxiv.org/pdf/2405.19808v2",
      "published_date": "2024-05-30 08:17:15 UTC",
      "updated_date": "2024-06-02 22:27:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:14:36.764659"
    },
    {
      "arxiv_id": "2405.19796v1",
      "title": "Explainable Attribute-Based Speaker Verification",
      "title_zh": "基于属性的可解释说话人验证",
      "authors": [
        "Xiaoliang Wu",
        "Chau Luu",
        "Peter Bell",
        "Ajitha Rajan"
      ],
      "abstract": "This paper proposes a fully explainable approach to speaker verification\n(SV), a task that fundamentally relies on individual speaker characteristics.\nThe opaque use of speaker attributes in current SV systems raises concerns of\ntrust. Addressing this, we propose an attribute-based explainable SV system\nthat identifies speakers by comparing personal attributes such as gender,\nnationality, and age extracted automatically from voice recordings. We believe\nthis approach better aligns with human reasoning, making it more understandable\nthan traditional methods. Evaluated on the Voxceleb1 test set, the best\nperformance of our system is comparable with the ground truth established when\nusing all correct attributes, proving its efficacy. Whilst our approach\nsacrifices some performance compared to non-explainable methods, we believe\nthat it moves us closer to the goal of transparent, interpretable AI and lays\nthe groundwork for future enhancements through attribute expansion.",
      "tldr_zh": "这篇论文提出了一种基于属性的可解释说话者验证(SV)系统，通过自动从语音录制中提取个人属性（如性别、国籍和年龄）来识别说话者，从而解决传统SV系统不透明性引发的信任问题。该方法更符合人类推理方式，在Voxceleb1测试集上评估时，其最佳性能与使用所有正确属性的基准相当。尽管与非可解释方法相比有所性能牺牲，但该系统为透明、可解释AI奠定了基础，并为未来通过属性扩展提供潜力。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19796v1",
      "published_date": "2024-05-30 08:04:28 UTC",
      "updated_date": "2024-05-30 08:04:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:14:48.869600"
    },
    {
      "arxiv_id": "2405.19795v1",
      "title": "SLM as Guardian: Pioneering AI Safety with Small Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ohjoon Kwon",
        "Donghyeon Jeon",
        "Nayoung Choi",
        "Gyu-Hwung Cho",
        "Changbong Kim",
        "Hyunwoo Lee",
        "Inho Kang",
        "Sun Kim",
        "Taiwoo Park"
      ],
      "abstract": "Most prior safety research of large language models (LLMs) has focused on\nenhancing the alignment of LLMs to better suit the safety requirements of\nhumans. However, internalizing such safeguard features into larger models\nbrought challenges of higher training cost and unintended degradation of\nhelpfulness. To overcome such challenges, a modular approach employing a\nsmaller LLM to detect harmful user queries is regarded as a convenient solution\nin designing LLM-based system with safety requirements.\n  In this paper, we leverage a smaller LLM for both harmful query detection and\nsafeguard response generation. We introduce our safety requirements and the\ntaxonomy of harmfulness categories, and then propose a multi-task learning\nmechanism fusing the two tasks into a single model. We demonstrate the\neffectiveness of our approach, providing on par or surpassing harmful query\ndetection and safeguard response performance compared to the publicly available\nLLMs.",
      "tldr_zh": "本文提出使用小语言模型(SLM)作为AI安全守护者，以解决大型语言模型(LLM)的训练成本高和帮助性下降问题，通过模块化方法实现有害查询检测和安全响应生成。研究者定义了安全要求和有害类别分类，并引入多任务学习机制，将这两个任务融合到一个SLM中。实验结果显示，该方法在有害查询检测和安全响应性能上，与公开LLM相比，达到了相当或更高的水平，为高效的AI安全设计提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19795v1",
      "published_date": "2024-05-30 08:03:15 UTC",
      "updated_date": "2024-05-30 08:03:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:15:01.684694"
    },
    {
      "arxiv_id": "2405.19787v2",
      "title": "From Symbolic Tasks to Code Generation: Diversification Yields Better Task Performers",
      "title_zh": "从符号任务到代码生成：多样化产生更好的任务执行者",
      "authors": [
        "Dylan Zhang",
        "Justin Wang",
        "Francois Charton"
      ],
      "abstract": "Instruction tuning -- tuning large language models on instruction-output\npairs -- is a promising technique for making models better adapted to the real\nworld. Yet, the key factors driving the model's capability to understand and\nfollow instructions not seen during training remain under-explored. Our\ninvestigation begins with a series of synthetic experiments within the\ntheoretical framework of a Turing-complete algorithm called Markov algorithm,\nwhich allows fine-grained control over the instruction-tuning data.\nGeneralization and robustness with respect to the training distribution emerge\nonce a diverse enough set of tasks is provided, even though very few examples\nare provided for each task. We extend these initial results to a real-world\napplication scenario of code generation and find that a more diverse\ninstruction set, extending beyond code-related tasks, improves the performance\nof code generation. Our observations suggest that a more diverse semantic space\nfor instruction-tuning sets greatly improves the model's ability to follow\ninstructions and perform tasks.",
      "tldr_zh": "该研究探讨了指令微调(instruction tuning)如何提升大语言模型(LLMs)的适应性，通过合成实验和真实应用揭示了关键因素。研究者使用图灵完备算法Markov algorithm进行精细控制的实验，发现提供多样化任务集（即使每个任务示例很少）能显著提升模型的泛化和鲁棒性。进一步扩展到代码生成(code generation)场景，结果显示，包含非代码相关任务的更丰富指令集能改善生成性能。总体而言，论文证明了多样化语义空间对模型遵循指令和执行任务能力的积极影响。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.LO",
        "cs.PL"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19787v2",
      "published_date": "2024-05-30 07:54:07 UTC",
      "updated_date": "2024-05-31 01:23:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:15:16.098167"
    },
    {
      "arxiv_id": "2405.19784v2",
      "title": "PixelsDB: Serverless and NL-Aided Data Analytics with Flexible Service Levels and Prices",
      "title_zh": "翻译失败",
      "authors": [
        "Haoqiong Bian",
        "Dongyang Geng",
        "Haoyang Li",
        "Yunpeng Chai",
        "Anastasia Ailamaki"
      ],
      "abstract": "Serverless query processing has become increasingly popular due to its\nadvantages, including automated resource management, high elasticity, and\npay-as-you-go pricing. For users who are not system experts, serverless query\nprocessing greatly reduces the cost of owning a data analytic system. However,\nit is still a significant challenge for non-expert users to transform their\ncomplex and evolving data analytic needs into proper SQL queries and select a\nserverless query service that delivers satisfactory performance and price for\neach type of query.\n  This paper presents PixelsDB, an open-source data analytic system that allows\nusers who lack system or SQL expertise to explore data efficiently. It allows\nusers to generate and debug SQL queries using a natural language interface\npowered by fine-tuned language models. The queries are then executed by a\nserverless query engine that offers varying prices for different performance\nservice levels (SLAs). The performance SLAs are natively supported by dedicated\narchitecture design and heterogeneous resource scheduling that can apply\ncost-efficient resources to process non-urgent queries. We demonstrate that the\ncombination of a serverless paradigm, a natural-language-aided interface, and\nflexible SLAs and prices will substantially improve the usability of cloud data\nanalytic systems.",
      "tldr_zh": "本论文提出 PixelsDB，一个开源数据分析系统，旨在帮助缺乏系统或 SQL 专业知识的用户高效探索数据。PixelsDB 通过自然语言接口（NL-Aided，由微调的语言模型驱动）生成和调试 SQL queries，并使用服务器查询引擎提供不同性能服务水平（SLAs）和价格选项，实现成本有效的异构资源调度。实验结果表明，这种结合 serverless 模式、自然语言辅助和灵活 SLAs 的设计显著提升了云数据分析系统的可用性，尤其适合非专家用户。",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.DC",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.DB",
      "comment": "4 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.19784v2",
      "published_date": "2024-05-30 07:48:43 UTC",
      "updated_date": "2024-12-23 06:44:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:15:24.938109"
    },
    {
      "arxiv_id": "2405.19783v2",
      "title": "Instruction-Guided Visual Masking",
      "title_zh": "指令引导的视觉掩盖",
      "authors": [
        "Jinliang Zheng",
        "Jianxiong Li",
        "Sijie Cheng",
        "Yinan Zheng",
        "Jiaming Li",
        "Jihao Liu",
        "Yu Liu",
        "Jingjing Liu",
        "Xianyuan Zhan"
      ],
      "abstract": "Instruction following is crucial in contemporary LLM. However, when extended\nto multimodal setting, it often suffers from misalignment between specific\ntextual instruction and targeted local region of an image. To achieve more\naccurate and nuanced multimodal instruction following, we introduce\nInstruction-guided Visual Masking (IVM), a new versatile visual grounding model\nthat is compatible with diverse multimodal models, such as LMM and robot model.\nBy constructing visual masks for instruction-irrelevant regions, IVM-enhanced\nmultimodal models can effectively focus on task-relevant image regions to\nbetter align with complex instructions. Specifically, we design a visual\nmasking data generation pipeline and create an IVM-Mix-1M dataset with 1\nmillion image-instruction pairs. We further introduce a new learning technique,\nDiscriminator Weighted Supervised Learning (DWSL) for preferential IVM training\nthat prioritizes high-quality data samples. Experimental results on generic\nmultimodal tasks such as VQA and embodied robotic control demonstrate the\nversatility of IVM, which as a plug-and-play tool, significantly boosts the\nperformance of diverse multimodal models, yielding new state-of-the-art results\nacross challenging multimodal benchmarks. Code, model and data are available at\nhttps://github.com/2toinf/IVM.",
      "tldr_zh": "本研究提出 Instruction-Guided Visual Masking (IVM)，一种新的视觉 grounding 模型，用于解决多模态设置中文本指令与图像局部区域的不匹配问题，从而提升指令遵循的准确性和细致度。IVM 通过构建视觉 masks 遮盖指令无关区域，让多模态模型（如 LMM 和机器人模型）专注于任务相关部分，并引入 Discriminator Weighted Supervised Learning (DWSL) 技术以及一个生成 IVM-Mix-1M 数据集的管道来进行高效训练。实验结果表明，IVM 作为 plug-and-play 工具显著提高了各种多模态模型的性能，在 VQA 和机器人控制等任务上实现了新的 state-of-the-art 结果。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.19783v2",
      "published_date": "2024-05-30 07:48:32 UTC",
      "updated_date": "2024-10-16 09:28:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:15:39.222312"
    },
    {
      "arxiv_id": "2405.19778v5",
      "title": "CharacterGPT: A Persona Reconstruction Framework for Role-Playing Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Jeiyoon Park",
        "Chanjun Park",
        "Heuiseok Lim"
      ],
      "abstract": "The recent introduction of the Assistants API highlights its potential for\nlarge language models (LLMs) in role-playing agents (RPA). However, maintaining\nconsistent character personas remains a significant challenge due to\nvariability in information extraction, which frequently omits critical elements\nsuch as backstory or interpersonal relationships. To address this limitation,\nwe introduce CharacterGPT, a framework designed to dynamically reconstruct\ncharacter personas through Character Persona Training (CPT). This approach\nincrementally updates personas by extracting traits from chapter-wise novel\nsummaries, reflecting the progression of the narrative. Our framework is\nevaluated through Big Five personality evaluations and creative tasks, in which\ncharacters generate original narratives, demonstrating the efficacy of\nCharacterGPT in preserving persona consistency. The code and results are\navailable at https://github.com/Jeiyoon/charactergpt",
      "tldr_zh": "本研究针对大型语言模型(LLMs)在角色扮演代理(RPA)中维持一致角色人物(character personas)的挑战，提出CharacterGPT框架，以解决信息提取遗漏关键元素（如背景故事或人际关系）的问题。该框架通过Character Persona Training (CPT)方法，从小说章节摘要中逐步提取并更新人物特征，实现动态重建和叙事进展的适应。实验结果显示，CharacterGPT在Big Five个性评估和创造性任务（如生成原创叙事）中表现出色，有效提升了角色一致性，并提供了开源代码以供进一步验证。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "NAACL 2025 Industry Track (Oral)",
      "pdf_url": "http://arxiv.org/pdf/2405.19778v5",
      "published_date": "2024-05-30 07:44:16 UTC",
      "updated_date": "2025-02-23 04:46:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:15:49.043996"
    },
    {
      "arxiv_id": "2405.19765v1",
      "title": "Towards Unified Multi-granularity Text Detection with Interactive Attention",
      "title_zh": "翻译失败",
      "authors": [
        "Xingyu Wan",
        "Chengquan Zhang",
        "Pengyuan Lyu",
        "Sen Fan",
        "Zihan Ni",
        "Kun Yao",
        "Errui Ding",
        "Jingdong Wang"
      ],
      "abstract": "Existing OCR engines or document image analysis systems typically rely on\ntraining separate models for text detection in varying scenarios and\ngranularities, leading to significant computational complexity and resource\ndemands. In this paper, we introduce \"Detect Any Text\" (DAT), an advanced\nparadigm that seamlessly unifies scene text detection, layout analysis, and\ndocument page detection into a cohesive, end-to-end model. This design enables\nDAT to efficiently manage text instances at different granularities, including\n*word*, *line*, *paragraph* and *page*. A pivotal innovation in DAT is the\nacross-granularity interactive attention module, which significantly enhances\nthe representation learning of text instances at varying granularities by\ncorrelating structural information across different text queries. As a result,\nit enables the model to achieve mutually beneficial detection performances\nacross multiple text granularities. Additionally, a prompt-based segmentation\nmodule refines detection outcomes for texts of arbitrary curvature and complex\nlayouts, thereby improving DAT's accuracy and expanding its real-world\napplicability. Experimental results demonstrate that DAT achieves\nstate-of-the-art performances across a variety of text-related benchmarks,\nincluding multi-oriented/arbitrarily-shaped scene text detection, document\nlayout analysis and page detection tasks.",
      "tldr_zh": "本文提出“Detect Any Text” (DAT)，一个统一的端到端模型，用于整合场景文本检测、布局分析和文档页面检测，支持多种粒度文本实例如 *word*、*line*、*paragraph* 和 *page*，从而减少了传统方法的计算复杂性和资源需求。DAT 的核心创新包括 across-granularity interactive attention module，该模块通过关联不同文本查询的结构信息来提升多粒度文本表示学习，以及 prompt-based segmentation module，用于优化任意曲率和复杂布局的检测精度。实验结果表明，DAT 在多向/任意形状场景文本检测、文档布局分析和页面检测等基准上达到了 state-of-the-art 性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.19765v1",
      "published_date": "2024-05-30 07:25:23 UTC",
      "updated_date": "2024-05-30 07:25:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:16:02.309259"
    },
    {
      "arxiv_id": "2405.19761v2",
      "title": "Revisiting CNNs for Trajectory Similarity Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Zhihao Chang",
        "Linzhu Yu",
        "Huan Li",
        "Sai Wu",
        "Gang Chen",
        "Dongxiang Zhang"
      ],
      "abstract": "Similarity search is a fundamental but expensive operator in querying\ntrajectory data, due to its quadratic complexity of distance computation. To\nmitigate the computational burden for long trajectories, neural networks have\nbeen widely employed for similarity learning and each trajectory is encoded as\na high-dimensional vector for similarity search with linear complexity. Given\nthe sequential nature of trajectory data, previous efforts have been primarily\ndevoted to the utilization of RNNs or Transformers.\n  In this paper, we argue that the common practice of treating trajectory as\nsequential data results in excessive attention to capturing long-term global\ndependency between two sequences. Instead, our investigation reveals the\npivotal role of local similarity, prompting a revisit of simple CNNs for\ntrajectory similarity learning. We introduce ConvTraj, incorporating both 1D\nand 2D convolutions to capture sequential and geo-distribution features of\ntrajectories, respectively. In addition, we conduct a series of theoretical\nanalyses to justify the effectiveness of ConvTraj. Experimental results on four\nreal-world large-scale datasets demonstrate that ConvTraj achieves\nstate-of-the-art accuracy in trajectory similarity search. Owing to the simple\nnetwork structure of ConvTraj, the training and inference speed on the Porto\ndataset with 1.6 million trajectories are increased by at least $240$x and\n$2.16$x, respectively. The source code and dataset can be found at\n\\textit{\\url{https://github.com/Proudc/ConvTraj}}.",
      "tldr_zh": "该论文重新审视了 CNNs 在轨迹相似性学习中的作用，认为以往将轨迹视为顺序数据而优先使用 RNNs 或 Transformers 会过度关注全局依赖，而忽略了局部相似性的关键。作者提出 ConvTraj 框架，利用 1D 卷积捕捉轨迹的顺序特征，并结合 2D 卷积处理地理分布特征，以提高相似性搜索的效率和准确性。通过理论分析和实验验证，ConvTraj 在四个真实大型数据集上实现了最先进的准确率，并在 Porto 数据集上将训练速度至少提升 240 倍、推理速度提升 2.16 倍。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19761v2",
      "published_date": "2024-05-30 07:16:03 UTC",
      "updated_date": "2024-11-05 05:25:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:16:13.462542"
    },
    {
      "arxiv_id": "2405.19757v3",
      "title": "Improving SMOTE via Fusing Conditional VAE for Data-adaptive Noise Filtering",
      "title_zh": "翻译失败",
      "authors": [
        "Sungchul Hong",
        "Seunghwan An",
        "Jong-June Jeon"
      ],
      "abstract": "Recent advances in a generative neural network model extend the development\nof data augmentation methods. However, the augmentation methods based on the\nmodern generative models fail to achieve notable performance for class\nimbalance data compared to the conventional model, Synthetic Minority\nOversampling Technique (SMOTE). We investigate the problem of the generative\nmodel for imbalanced classification and introduce a framework to enhance the\nSMOTE algorithm using Variational Autoencoders (VAE). Our approach\nsystematically quantifies the density of data points in a low-dimensional\nlatent space using the VAE, simultaneously incorporating information on class\nlabels and classification difficulty. Then, the data points potentially\ndegrading the augmentation are systematically excluded, and the neighboring\nobservations are directly augmented on the data space. Empirical studies on\nseveral imbalanced datasets represent that this simple process innovatively\nimproves the conventional SMOTE algorithm over the deep learning models.\nConsequently, we conclude that the selection of minority data and the\ninterpolation in the data space are beneficial for imbalanced classification\nproblems with a relatively small number of data points.",
      "tldr_zh": "这篇论文提出了一种改进 SMOTE 算法的方法，通过融合 Conditional VAE 实现数据自适应噪声过滤，以解决生成模型在类别不平衡数据上的性能问题。方法利用 Variational Autoencoders (VAE) 在低维潜在空间中量化数据密度，同时整合类别标签和分类难度信息，从而系统排除可能降低增强质量的数据点，并在数据空间中直接增强相邻观察值。实验结果显示，这种框架在多个不平衡数据集上显著提升了 SMOTE 在深度学习模型中的表现，特别是适用于小样本数据，并证明了选择少数类数据和数据空间插值的益处。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19757v3",
      "published_date": "2024-05-30 07:06:02 UTC",
      "updated_date": "2024-08-26 05:54:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:16:27.713026"
    },
    {
      "arxiv_id": "2405.19754v1",
      "title": "Mitigating annotation shift in cancer classification using single image generative models",
      "title_zh": "使用单图像生成模型缓解癌症分类中的标注偏移",
      "authors": [
        "Marta Buetas Arcas",
        "Richard Osuala",
        "Karim Lekadir",
        "Oliver Díaz"
      ],
      "abstract": "Artificial Intelligence (AI) has emerged as a valuable tool for assisting\nradiologists in breast cancer detection and diagnosis. However, the success of\nAI applications in this domain is restricted by the quantity and quality of\navailable data, posing challenges due to limited and costly data annotation\nprocedures that often lead to annotation shifts. This study simulates, analyses\nand mitigates annotation shifts in cancer classification in the breast\nmammography domain. First, a high-accuracy cancer risk prediction model is\ndeveloped, which effectively distinguishes benign from malignant lesions. Next,\nmodel performance is used to quantify the impact of annotation shift. We\nuncover a substantial impact of annotation shift on multiclass classification\nperformance particularly for malignant lesions. We thus propose a training data\naugmentation approach based on single-image generative models for the affected\nclass, requiring as few as four in-domain annotations to considerably mitigate\nannotation shift, while also addressing dataset imbalance. Lastly, we further\nincrease performance by proposing and validating an ensemble architecture based\non multiple models trained under different data augmentation regimes. Our study\noffers key insights into annotation shift in deep learning breast cancer\nclassification and explores the potential of single-image generative models to\novercome domain shift challenges.",
      "tldr_zh": "本研究探讨了AI在乳腺癌分类中的annotation shift问题，该问题源于数据标注的有限性和成本高，导致模型性能下降。研究者首先开发了一个高准确性的癌症风险预测模型，用于区分良性和恶性病变，并量化annotation shift对多类分类，尤其是恶性病变的影响。针对这一问题，他们提出了一种基于single image generative models的训练数据增强方法，仅需少量（如4个）领域内标注，即可显著缓解annotation shift，同时解决数据集不平衡。最终，通过验证一个基于多种数据增强方案的集成架构，进一步提升了模型性能，为deep learning乳腺癌分类提供关键洞见和应对领域偏移的潜在解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint of paper accepted at SPIE IWBI 2024 Conference",
      "pdf_url": "http://arxiv.org/pdf/2405.19754v1",
      "published_date": "2024-05-30 07:02:50 UTC",
      "updated_date": "2024-05-30 07:02:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:16:37.709292"
    },
    {
      "arxiv_id": "2405.19751v2",
      "title": "HQ-DiT: Efficient Diffusion Transformer with FP4 Hybrid Quantization",
      "title_zh": "翻译失败",
      "authors": [
        "Wenxuan Liu",
        "Sai Qian Zhang"
      ],
      "abstract": "Diffusion Transformers (DiTs) have recently gained substantial attention in\nboth industrial and academic fields for their superior visual generation\ncapabilities, outperforming traditional diffusion models that use U-Net.\nHowever,the enhanced performance of DiTs also comes with high parameter counts\nand implementation costs, seriously restricting their use on resource-limited\ndevices such as mobile phones. To address these challenges, we introduce the\nHybrid Floating-point Quantization for DiT(HQ-DiT), an efficient post-training\nquantization method that utilizes 4-bit floating-point (FP) precision on both\nweights and activations for DiT inference. Compared to fixed-point quantization\n(e.g., INT8), FP quantization, complemented by our proposed clipping range\nselection mechanism, naturally aligns with the data distribution within DiT,\nresulting in a minimal quantization error. Furthermore, HQ-DiT also implements\na universal identity mathematical transform to mitigate the serious\nquantization error caused by the outliers. The experimental results demonstrate\nthat DiT can achieve extremely low-precision quantization (i.e., 4 bits) with\nnegligible impact on performance. Our approach marks the first instance where\nboth weights and activations in DiTs are quantized to just 4 bits, with only a\n0.12 increase in sFID on ImageNet.",
      "tldr_zh": "本研究针对Diffusion Transformers (DiTs)的高参数量和实现成本问题，提出了一种高效的后训练量化方法HQ-DiT，利用4-bit浮点(FP)精度对权重和激活进行混合量化，以适应资源有限的设备。相比于INT8固定点量化，HQ-DiT引入了剪切范围选择机制和通用身份数学变换来最小化量化误差，从而有效处理异常值。实验结果显示，该方法使DiTs实现4位量化后，仅在ImageNet上导致sFID增加0.12，显著提升了模型的部署效率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19751v2",
      "published_date": "2024-05-30 06:56:11 UTC",
      "updated_date": "2024-05-31 15:48:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:16:49.595226"
    },
    {
      "arxiv_id": "2405.19744v1",
      "title": "X-Instruction: Aligning Language Model in Low-resource Languages with Self-curated Cross-lingual Instructions",
      "title_zh": "翻译失败",
      "authors": [
        "Chong Li",
        "Wen Yang",
        "Jiajun Zhang",
        "Jinliang Lu",
        "Shaonan Wang",
        "Chengqing Zong"
      ],
      "abstract": "Large language models respond well in high-resource languages like English\nbut struggle in low-resource languages. It may arise from the lack of\nhigh-quality instruction following data in these languages. Directly\ntranslating English samples into these languages can be a solution but\nunreliable, leading to responses with translation errors and lacking\nlanguage-specific or cultural knowledge. To address this issue, we propose a\nnovel method to construct cross-lingual instruction following samples with\ninstruction in English and response in low-resource languages. Specifically,\nthe language model first learns to generate appropriate English instructions\naccording to the natural web texts in other languages as responses. The\ncandidate cross-lingual instruction tuning samples are further refined and\ndiversified. We have employed this method to build a large-scale cross-lingual\ninstruction tuning dataset on 10 languages, namely X-Instruction. The\ninstruction data built using our method incorporate more language-specific\nknowledge compared with the naive translation method. Experimental results have\nshown that the response quality of the model tuned on X-Instruction greatly\nexceeds the model distilled from a powerful teacher model, reaching or even\nsurpassing the ones of ChatGPT. In addition, we find that models tuned on\ncross-lingual instruction following samples can follow the instruction in the\noutput language without further tuning.",
      "tldr_zh": "本文提出 X-Instruction 方法，通过自 curation 的跨语言指令样本来提升大语言模型在低资源语言中的性能。具体地，该方法使用英语指令和低资源语言响应，从自然网络文本中生成并优化样本，从而避免直接翻译带来的错误并融入语言特定知识。研究构建了一个覆盖 10 种语言的大规模数据集 X-Instruction，实验结果表明，使用该数据集微调的模型响应质量超过从强大教师模型蒸馏的模型，甚至达到或 surpass ChatGPT 的水平。此外，模型能够根据输出语言自动跟随指令，而无需额外微调。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2024. Our codes, data and model weights are available at\n  https://github.com/ZNLP/X-Instruction",
      "pdf_url": "http://arxiv.org/pdf/2405.19744v1",
      "published_date": "2024-05-30 06:45:23 UTC",
      "updated_date": "2024-05-30 06:45:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:17:03.357139"
    },
    {
      "arxiv_id": "2405.19743v1",
      "title": "May the Dance be with You: Dance Generation Framework for Non-Humanoids",
      "title_zh": "愿舞蹈与你同",
      "authors": [
        "Hyemin Ahn"
      ],
      "abstract": "We hypothesize dance as a motion that forms a visual rhythm from music, where\nthe visual rhythm can be perceived from an optical flow. If an agent can\nrecognize the relationship between visual rhythm and music, it will be able to\ndance by generating a motion to create a visual rhythm that matches the music.\nBased on this, we propose a framework for any kind of non-humanoid agents to\nlearn how to dance from human videos. Our framework works in two processes: (1)\ntraining a reward model which perceives the relationship between optical flow\n(visual rhythm) and music from human dance videos, (2) training the\nnon-humanoid dancer based on that reward model, and reinforcement learning. Our\nreward model consists of two feature encoders for optical flow and music. They\nare trained based on contrastive learning which makes the higher similarity\nbetween concurrent optical flow and music features. With this reward model, the\nagent learns dancing by getting a higher reward when its action creates an\noptical flow whose feature has a higher similarity with the given music\nfeature. Experiment results show that generated dance motion can align with the\nmusic beat properly, and user study result indicates that our framework is more\npreferred by humans compared to the baselines. To the best of our knowledge,\nour work of non-humanoid agents which learn dance from human videos is\nunprecedented. An example video can be found at https://youtu.be/dOUPvo-O3QY.",
      "tldr_zh": "本文提出一个舞蹈生成框架，针对非人形代理（如机器人），假设舞蹈动作通过光流（optical flow）形成视觉节奏来匹配音乐。框架包括两个过程：（1）训练奖励模型（reward model）使用对比学习（contrastive learning）从人类舞蹈视频中学习光流和音乐特征的关联；（2）通过强化学习（reinforcement learning）训练代理，使其生成动作以创建与音乐特征高度相似的光流。实验结果显示，生成的舞蹈动作能准确对齐音乐节拍，用户研究表明该框架比基线模型更受欢迎，且这是首个让非人形代理从人类视频中学习舞蹈的创新工作。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, 6 Figures, Rejected at Neurips 2023",
      "pdf_url": "http://arxiv.org/pdf/2405.19743v1",
      "published_date": "2024-05-30 06:43:55 UTC",
      "updated_date": "2024-05-30 06:43:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:17:15.433615"
    },
    {
      "arxiv_id": "2405.19740v2",
      "title": "PertEval: Unveiling Real Knowledge Capacity of LLMs with Knowledge-Invariant Perturbations",
      "title_zh": "PertEval：通过知识不变扰动揭示 LLMs 的真实知识容量",
      "authors": [
        "Jiatong Li",
        "Renjun Hu",
        "Kunzhe Huang",
        "Yan Zhuang",
        "Qi Liu",
        "Mengxiao Zhu",
        "Xing Shi",
        "Wei Lin"
      ],
      "abstract": "Expert-designed close-ended benchmarks are indispensable in assessing the\nknowledge capacity of large language models (LLMs). Despite their widespread\nuse, concerns have mounted regarding their reliability due to limited test\nscenarios and an unavoidable risk of data contamination. To rectify this, we\npresent PertEval, a toolkit devised for in-depth probing of LLMs' knowledge\ncapacity through \\textbf{knowledge-invariant perturbations}. These\nperturbations employ human-like restatement techniques to generate on-the-fly\ntest samples from static benchmarks, meticulously retaining knowledge-critical\ncontent while altering irrelevant details. Our toolkit further includes a suite\nof \\textbf{response consistency analyses} that compare performance on raw vs.\nperturbed test sets to precisely assess LLMs' genuine knowledge capacity. Six\nrepresentative LLMs are re-evaluated using PertEval. Results reveal\nsignificantly inflated performance of the LLMs on raw benchmarks, including an\nabsolute 25.8% overestimation for GPT-4. Additionally, through a nuanced\nresponse pattern analysis, we discover that PertEval retains LLMs' uncertainty\nto specious knowledge, and reveals their potential rote memorization to correct\noptions which leads to overestimated performance. We also find that the\ndetailed response consistency analyses by PertEval could illuminate various\nweaknesses in existing LLMs' knowledge mastery and guide the development of\nrefinement. Our findings provide insights for advancing more robust and\ngenuinely knowledgeable LLMs. Our code is available at\n\\url{https://github.com/aigc-apps/PertEval}.",
      "tldr_zh": "该研究提出PertEval，一种用于评估大语言模型(LLMs)真实知识能力的工具包，通过knowledge-invariant perturbations（知识不变扰动）生成实时测试样本，这些扰动采用人类般的重述技巧，保留关键知识内容同时改变无关细节。PertEval还包括response consistency analyses（响应一致性分析），通过比较原始和扰动测试集的表现，精确揭示LLMs的实际知识水平。实验结果显示，六种代表性LLMs在原始基准上的性能被显著高估，例如GPT-4高估了25.8%，并暴露了LLMs对虚假知识的不确定性和对正确选项的死记硬背问题。这些发现为改进LLMs的鲁棒性和知识掌握提供宝贵指导。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by NeurIPS '24 D&B Spotlight; 28 pages, 15 figures, 14\n  tables",
      "pdf_url": "http://arxiv.org/pdf/2405.19740v2",
      "published_date": "2024-05-30 06:38:32 UTC",
      "updated_date": "2024-10-18 06:57:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:17:26.939633"
    },
    {
      "arxiv_id": "2405.19737v1",
      "title": "Beyond Imitation: Learning Key Reasoning Steps from Dual Chain-of-Thoughts in Reasoning Distillation",
      "title_zh": "翻译失败",
      "authors": [
        "Chengwei Dai",
        "Kun Li",
        "Wei Zhou",
        "Songlin Hu"
      ],
      "abstract": "As Large Language Models (LLMs) scale up and gain powerful Chain-of-Thoughts\n(CoTs) reasoning abilities, practical resource constraints drive efforts to\ndistill these capabilities into more compact Smaller Language Models (SLMs). We\nfind that CoTs consist mainly of simple reasoning forms, with a small\nproportion ($\\approx 4.7\\%$) of key reasoning steps that truly impact\nconclusions. However, previous distillation methods typically involve\nsupervised fine-tuning student SLMs only on correct CoTs data produced by\nteacher LLMs, resulting in students struggling to learn the key reasoning\nsteps, instead imitating the teacher's reasoning forms and making errors or\nomissions on these steps. To address these issues, drawing an analogy to human\nlearning, where analyzing mistakes according to correct solutions often reveals\nthe crucial steps leading to successes or failures, we propose\nmistak\\textbf{E}-\\textbf{D}riven key reason\\textbf{I}ng step\ndistilla\\textbf{T}ion (\\textbf{EDIT}), a novel method that further aids SLMs\nlearning key reasoning steps rather than mere simple fine-tuning. Firstly, to\nexpose these crucial steps in CoTs, we design specific prompts to generate dual\nCoTs data with similar reasoning paths but divergent conclusions. Then, we\napply the minimum edit distance algorithm on the dual CoTs data to locate these\nkey steps and optimize the likelihood of these steps. Extensive experiments\nvalidate the effectiveness of EDIT across both in-domain and out-of-domain\nbenchmark reasoning datasets. Further analysis shows that EDIT can generate\nhigh-quality CoTs with more correct key reasoning steps. Notably, we also\nexplore how different mistake patterns affect performance and find that EDIT\nbenefits more from logical errors than from knowledge or mathematical\ncalculation errors in dual CoTs\\footnote{Code can be found at\n\\url{https://github.com/C-W-D/EDIT}}.",
      "tldr_zh": "该研究发现，大型语言模型 (LLMs) 的 Chain-of-Thoughts (CoTs) 推理中，仅约 4.7% 的关键步骤影响最终结论，而现有推理蒸馏方法导致小型语言模型 (SLMs) 更多模仿教师模型的形式而非学习这些关键步骤。针对此问题，提出 mistakE-DrIven key reasonIng step distillaTion (EDIT) 方法，通过生成双 CoTs 数据（类似推理路径但结论不同），并应用最小编辑距离算法定位和优化关键步骤，以帮助 SLMs 更好地掌握核心推理。实验验证了 EDIT 在领域内和领域外基准数据集上的有效性，并发现它从逻辑错误中获益更多，从而生成更高质量的 CoTs。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19737v1",
      "published_date": "2024-05-30 06:32:11 UTC",
      "updated_date": "2024-05-30 06:32:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:17:49.882346"
    },
    {
      "arxiv_id": "2405.19736v2",
      "title": "Intrinsic Dynamics-Driven Generalizable Scene Representations for Vision-Oriented Decision-Making Applications",
      "title_zh": "基于内在动力学驱动的可泛化场景表示，用于视觉导向决策应用",
      "authors": [
        "Dayang Liang",
        "Jinyang Lai",
        "Yunlong Liu"
      ],
      "abstract": "How to improve the ability of scene representation is a key issue in\nvision-oriented decision-making applications, and current approaches usually\nlearn task-relevant state representations within visual reinforcement learning\nto address this problem. While prior work typically introduces one-step\nbehavioral similarity metrics with elements (e.g., rewards and actions) to\nextract task-relevant state information from observations, they often ignore\nthe inherent dynamics relationships among the elements that are essential for\nlearning accurate representations, which further impedes the discrimination of\nshort-term similar task/behavior information in long-term dynamics transitions.\nTo alleviate this problem, we propose an intrinsic dynamics-driven\nrepresentation learning method with sequence models in visual reinforcement\nlearning, namely DSR. Concretely, DSR optimizes the parameterized encoder by\nthe state-transition dynamics of the underlying system, which prompts the\nlatent encoding information to satisfy the state-transition process and then\nthe state space and the noise space can be distinguished. In the implementation\nand to further improve the representation ability of DSR on encoding similar\ntasks, sequential elements' frequency domain and multi-step prediction are\nadopted for sequentially modeling the inherent dynamics. Finally, experimental\nresults show that DSR has achieved significant performance improvements in the\nvisual Distracting DMControl control tasks, especially with an average of\n78.9\\% over the backbone baseline. Further results indicate that it also\nachieves the best performances in real-world autonomous driving applications on\nthe CARLA simulator. Moreover, qualitative analysis results validate that our\nmethod possesses the superior ability to learn generalizable scene\nrepresentations on visual tasks. The source code is available at\nhttps://github.com/DMU-XMU/DSR.",
      "tldr_zh": "该研究针对视觉导向决策应用中场景表示能力的提升问题，指出现有方法在视觉强化学习(visual reinforcement learning)中忽略了元素间的固有动态关系，导致短期任务相似性难以在长期动态中区分。作者提出了一种内在动态驱动的表示学习方法DSR，使用序列模型优化编码器，通过状态转换动态区分状态空间和噪声空间，并结合序列元素的频域分析和多步预测来增强对类似任务的建模。实验结果显示，DSR在视觉Distracting DMControl任务中比基线模型平均提升78.9%的性能，并在CARLA模拟器上的真实自动驾驶应用中表现出最佳效果，证明了其在学习通用场景表示方面的优越性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "pdf_url": "http://arxiv.org/pdf/2405.19736v2",
      "published_date": "2024-05-30 06:31:03 UTC",
      "updated_date": "2024-06-30 06:25:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:17:53.353052"
    },
    {
      "arxiv_id": "2405.19730v5",
      "title": "Research on the Spatial Data Intelligent Foundation Model",
      "title_zh": "翻译失败",
      "authors": [
        "Shaohua Wang",
        "Xing Xie",
        "Yong Li",
        "Danhuai Guo",
        "Zhi Cai",
        "Yu Liu",
        "Yang Yue",
        "Xiao Pan",
        "Feng Lu",
        "Huayi Wu",
        "Zhipeng Gui",
        "Zhiming Ding",
        "Bolong Zheng",
        "Fuzheng Zhang",
        "Jingyuan Wang",
        "Zhengchao Chen",
        "Hao Lu",
        "Jiayi Li",
        "Peng Yue",
        "Wenhao Yu",
        "Yao Yao",
        "Leilei Sun",
        "Yong Zhang",
        "Longbiao Chen",
        "Xiaoping Du",
        "Xiang Li",
        "Xueying Zhang",
        "Kun Qin",
        "Zhaoya Gong",
        "Weihua Dong",
        "Xiaofeng Meng"
      ],
      "abstract": "This report focuses on spatial data intelligent large models, delving into\nthe principles, methods, and cutting-edge applications of these models. It\nprovides an in-depth discussion on the definition, development history, current\nstatus, and trends of spatial data intelligent large models, as well as the\nchallenges they face. The report systematically elucidates the key technologies\nof spatial data intelligent large models and their applications in urban\nenvironments, aerospace remote sensing, geography, transportation, and other\nscenarios. Additionally, it summarizes the latest application cases of spatial\ndata intelligent large models in themes such as urban development, multimodal\nsystems, remote sensing, smart transportation, and resource environments.\nFinally, the report concludes with an overview and outlook on the development\nprospects of spatial data intelligent large models.",
      "tldr_zh": "这篇报告探讨了spatial data intelligent foundation model的原则、方法和前沿应用，涵盖了其定义、发展历史、现状、趋势以及面临的挑战。报告系统阐述了关键技术和在城市环境、航空航天遥感、地理、交通等领域中的实际应用，并总结了这些模型在城市发展、多模态系统、遥感、智能交通以及资源环境等主题的最新案例。最后，它对spatial data intelligent foundation model的未来发展前景进行了概述和展望。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "V1 and V2 are in Chinese language, other versions are in English",
      "pdf_url": "http://arxiv.org/pdf/2405.19730v5",
      "published_date": "2024-05-30 06:21:34 UTC",
      "updated_date": "2024-08-28 13:05:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:18:12.251708"
    },
    {
      "arxiv_id": "2405.19729v1",
      "title": "Dynamic feature selection in medical predictive monitoring by reinforcement learning",
      "title_zh": "在医疗预测监测中通过强化学习进行动态特征选择",
      "authors": [
        "Yutong Chen",
        "Jiandong Gao",
        "Ji Wu"
      ],
      "abstract": "In this paper, we investigate dynamic feature selection within multivariate\ntime-series scenario, a common occurrence in clinical prediction monitoring\nwhere each feature corresponds to a bio-test result. Many existing feature\nselection methods fall short in effectively leveraging time-series information,\nprimarily because they are designed for static data. Our approach addresses\nthis limitation by enabling the selection of time-varying feature subsets for\neach patient. Specifically, we employ reinforcement learning to optimize a\npolicy under maximum cost restrictions. The prediction model is subsequently\nupdated using synthetic data generated by trained policy. Our method can\nseamlessly integrate with non-differentiable prediction models. We conducted\nexperiments on a sizable clinical dataset encompassing regression and\nclassification tasks. The results demonstrate that our approach outperforms\nstrong feature selection baselines, particularly when subjected to stringent\ncost limitations. Code will be released once paper is accepted.",
      "tldr_zh": "本文提出了一种基于 reinforcement learning 的动态特征选择方法，针对医疗预测监测中的多变量时间序列数据问题，克服了现有静态方法无法有效利用时间信息的问题。该方法通过优化策略在最大成本限制下为每个患者选择时间变化的特征子集，并使用生成的合成数据更新预测模型，从而与非可微预测模型无缝整合。在临床数据集上的实验表明，该方法在回归和分类任务中优于强有力的特征选择基线，特别是在严格成本限制下。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "preview version",
      "pdf_url": "http://arxiv.org/pdf/2405.19729v1",
      "published_date": "2024-05-30 06:21:11 UTC",
      "updated_date": "2024-05-30 06:21:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:18:14.351711"
    },
    {
      "arxiv_id": "2405.19723v3",
      "title": "Encoding and Controlling Global Semantics for Long-form Video Question Answering",
      "title_zh": "用于长形式视频问答的全局语义编码与控制",
      "authors": [
        "Thong Thanh Nguyen",
        "Zhiyuan Hu",
        "Xiaobao Wu",
        "Cong-Duy T Nguyen",
        "See-Kiong Ng",
        "Anh Tuan Luu"
      ],
      "abstract": "Seeking answers effectively for long videos is essential to build video\nquestion answering (videoQA) systems. Previous methods adaptively select frames\nand regions from long videos to save computations. However, this fails to\nreason over the whole sequence of video, leading to sub-optimal performance. To\naddress this problem, we introduce a state space layer (SSL) into multi-modal\nTransformer to efficiently integrate global semantics of the video, which\nmitigates the video information loss caused by frame and region selection\nmodules. Our SSL includes a gating unit to enable controllability over the flow\nof global semantics into visual representations. To further enhance the\ncontrollability, we introduce a cross-modal compositional congruence (C^3)\nobjective to encourage global semantics aligned with the question. To\nrigorously evaluate long-form videoQA capacity, we construct two new benchmarks\nEgo-QA and MAD-QA featuring videos of considerably long length, i.e. 17.5\nminutes and 1.9 hours, respectively. Extensive experiments demonstrate the\nsuperiority of our framework on these new as well as existing datasets. The\ncode, model, and data have been made available at\nhttps://nguyentthong.github.io/Long_form_VideoQA.",
      "tldr_zh": "这篇论文针对长视频问答（videoQA）问题，提出了一种新框架，通过在多模态 Transformer 中引入 State Space Layer (SSL) 来高效整合视频的全局语义，从而缓解因帧和区域选择导致的信息损失。SSL 包含一个 gating unit 以控制全局语义的流动，并通过 Cross-modal Compositional Congruence (C^3) 目标增强语义与问题的对齐。作者构建了两个新基准数据集 Ego-QA（视频长度17.5分钟）和 MAD-QA（视频长度1.9小时），实验结果显示该框架在新数据集以及现有数据集上均表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to the main EMNLP 2024 conference",
      "pdf_url": "http://arxiv.org/pdf/2405.19723v3",
      "published_date": "2024-05-30 06:10:10 UTC",
      "updated_date": "2024-10-05 14:02:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:18:28.313114"
    },
    {
      "arxiv_id": "2405.19715v2",
      "title": "SpecDec++: Boosting Speculative Decoding via Adaptive Candidate Lengths",
      "title_zh": "翻译失败",
      "authors": [
        "Kaixuan Huang",
        "Xudong Guo",
        "Mengdi Wang"
      ],
      "abstract": "Speculative decoding reduces the inference latency of a target large language\nmodel via utilizing a smaller and faster draft model. Its performance depends\non a hyperparameter K -- the candidate length, i.e., the number of candidate\ntokens for the target model to verify in each round. However, previous methods\noften use simple heuristics to choose K, which may result in sub-optimal\nperformance. We study the choice of the candidate length K and formulate it as\na Markov Decision Process. We theoretically show that the optimal policy of\nthis Markov decision process takes the form of a threshold policy, i.e., the\ncurrent speculation should stop and be verified when the probability of getting\na rejection exceeds a threshold value. Motivated by this theory, we propose\nSpecDec++, an enhanced version of speculative decoding that adaptively\ndetermines the candidate length on the fly. We augment the draft model with a\ntrained acceptance prediction head to predict the conditional acceptance\nprobability of the candidate tokens. SpecDec++ will stop the current\nspeculation when the predicted probability that at least one token gets\nrejected exceeds a threshold. We implement SpecDec++ and apply it to the\nllama-2-chat 7B & 70B model pair. Our adaptive method achieves a 2.04x speedup\non the Alpaca dataset (an additional 7.2% improvement over the baseline\nspeculative decoding). On the GSM8K and HumanEval datasets, our method achieves\na 2.26x speedup (9.4% improvement) and 2.23x speedup (11.1% improvement),\nrespectively.",
      "tldr_zh": "该论文提出SpecDec++，一种改进的Speculative Decoding方法，通过自适应候选长度来提升大型语言模型的推理速度。作者将候选长度K的选择形式化为Markov Decision Process，并证明最优策略为阈值策略，即当拒绝概率超过阈值时停止推测。SpecDec++在draft model上添加了一个训练过的acceptance prediction head来预测候选token的接受概率，并在llama-2-chat 7B & 70B模型上实验，实现了Alpaca数据集2.04x加速（额外7.2%改善）、GSM8K数据集2.26x加速（9.4%改善）和HumanEval数据集2.23x加速（11.1%改善）。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "v2: fix Table 1",
      "pdf_url": "http://arxiv.org/pdf/2405.19715v2",
      "published_date": "2024-05-30 05:49:38 UTC",
      "updated_date": "2024-06-21 01:01:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:18:39.343607"
    },
    {
      "arxiv_id": "2405.19708v1",
      "title": "Text Guided Image Editing with Automatic Concept Locating and Forgetting",
      "title_zh": "翻译失败",
      "authors": [
        "Jia Li",
        "Lijie Hu",
        "Zhixian He",
        "Jingfeng Zhang",
        "Tianhang Zheng",
        "Di Wang"
      ],
      "abstract": "With the advancement of image-to-image diffusion models guided by text,\nsignificant progress has been made in image editing. However, a persistent\nchallenge remains in seamlessly incorporating objects into images based on\ntextual instructions, without relying on extra user-provided guidance. Text and\nimages are inherently distinct modalities, bringing out difficulties in fully\ncapturing the semantic intent conveyed through language and accurately\ntranslating that into the desired visual modifications. Therefore, text-guided\nimage editing models often produce generations with residual object attributes\nthat do not fully align with human expectations. To address this challenge, the\nmodels should comprehend the image content effectively away from a disconnect\nbetween the provided textual editing prompts and the actual modifications made\nto the image. In our paper, we propose a novel method called Locate and Forget\n(LaF), which effectively locates potential target concepts in the image for\nmodification by comparing the syntactic trees of the target prompt and scene\ndescriptions in the input image, intending to forget their existence clues in\nthe generated image. Compared to the baselines, our method demonstrates its\nsuperiority in text-guided image editing tasks both qualitatively and\nquantitatively.",
      "tldr_zh": "这项研究针对文本引导图像编辑中的挑战，提出了一种名为Locate and Forget (LaF)的新方法，以自动定位图像中的目标概念并“忘记”其存在线索。该方法通过比较目标提示和输入图像场景描述的句法树，实现更精确的语义理解，避免生成的图像保留不匹配的物体属性。与基线模型相比，LaF在文本引导图像编辑任务中表现出定性和定量上的优越性，显著提升了编辑效果。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19708v1",
      "published_date": "2024-05-30 05:36:32 UTC",
      "updated_date": "2024-05-30 05:36:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:18:51.119147"
    },
    {
      "arxiv_id": "2405.19701v2",
      "title": "Significance of Chain of Thought in Gender Bias Mitigation for English-Dravidian Machine Translation",
      "title_zh": "翻译失败",
      "authors": [
        "Lavanya Prahallad",
        "Radhika Mamidi"
      ],
      "abstract": "Gender bias in machine translation (MT) sys- tems poses a significant\nchallenge to achieving accurate and inclusive translations. This paper examines\ngender bias in machine translation systems for languages such as Telugu and\nKan- nada from the Dravidian family, analyzing how gender inflections affect\ntranslation accuracy and neutrality using Google Translate and Chat- GPT. It\nfinds that while plural forms can reduce bias, individual-centric sentences\noften main- tain the bias due to historical stereotypes. The study evaluates\nthe Chain of Thought process- ing, noting significant bias mitigation from 80%\nto 4% in Telugu and from 40% to 0% in Kan- nada. It also compares Telugu and\nKannada translations, emphasizing the need for language specific strategies to\naddress these challenges and suggesting directions for future research to\nenhance fairness in both data preparation and prompts during inference.",
      "tldr_zh": "本研究探讨了Chain of Thought在缓解英语-Dravidian语言（如Telugu和Kannada）机器翻译中的性别偏见问题，分析了Google Translate和ChatGPT在处理性别变形时的准确性和中性度。研究发现，复数形式可减少偏见，但单数句子往往因历史刻板印象而保留偏见；通过Chain of Thought处理，Telugu的偏见从80%降至4%，Kannada从40%降至0%。此外，该研究比较了两种语言的翻译表现，强调需要语言特定策略来改进数据准备和提示设计，并为未来公平性研究提供方向。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "6 pages",
      "pdf_url": "http://arxiv.org/pdf/2405.19701v2",
      "published_date": "2024-05-30 05:26:57 UTC",
      "updated_date": "2024-06-03 15:59:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:19:03.726222"
    },
    {
      "arxiv_id": "2405.19697v2",
      "title": "Bilevel reinforcement learning via the development of hyper-gradient without lower-level convexity",
      "title_zh": "翻译失败",
      "authors": [
        "Yan Yang",
        "Bin Gao",
        "Ya-xiang Yuan"
      ],
      "abstract": "Bilevel reinforcement learning (RL), which features intertwined two-level\nproblems, has attracted growing interest recently. The inherent non-convexity\nof the lower-level RL problem is, however, to be an impediment to developing\nbilevel optimization methods. By employing the fixed point equation associated\nwith the regularized RL, we characterize the hyper-gradient via fully\nfirst-order information, thus circumventing the assumption of lower-level\nconvexity. This, remarkably, distinguishes our development of hyper-gradient\nfrom the general AID-based bilevel frameworks since we take advantage of the\nspecific structure of RL problems. Moreover, we design both model-based and\nmodel-free bilevel reinforcement learning algorithms, facilitated by access to\nthe fully first-order hyper-gradient. Both algorithms enjoy the convergence\nrate $O(\\epsilon^{-1})$. To extend the applicability, a stochastic version of\nthe model-free algorithm is proposed, along with results on its iteration and\nsample complexity. In addition, numerical experiments demonstrate that the\nhyper-gradient indeed serves as an integration of exploitation and exploration.",
      "tldr_zh": "这篇论文提出了一种双层强化学习(bilevel reinforcement learning)方法，通过利用固定点方程和正则化 RL，仅依赖一阶信息来计算 hyper-gradient，从而避免了下层问题的非凸性假设。作者设计了基于模型和无模型的算法，这些算法均利用完全一阶 hyper-gradient，并实现了 O(ε^{-1}) 的收敛率；此外，还开发了无模型算法的随机版本，并分析了其迭代和样本复杂度。数值实验表明，该 hyper-gradient 有效地整合了利用(exploitation)和探索(exploration)，提升了算法的适用性。",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "math.OC",
      "comment": "This v2 is a camera-ready version of AISTATS 2025",
      "pdf_url": "http://arxiv.org/pdf/2405.19697v2",
      "published_date": "2024-05-30 05:24:20 UTC",
      "updated_date": "2025-02-27 06:52:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:19:16.020847"
    },
    {
      "arxiv_id": "2405.19694v1",
      "title": "Grade Like a Human: Rethinking Automated Assessment with Large Language Models",
      "title_zh": "像人类一样评分：利用大型语言模型重新审视自动化评估",
      "authors": [
        "Wenjing Xie",
        "Juxin Niu",
        "Chun Jason Xue",
        "Nan Guan"
      ],
      "abstract": "While large language models (LLMs) have been used for automated grading, they\nhave not yet achieved the same level of performance as humans, especially when\nit comes to grading complex questions. Existing research on this topic focuses\non a particular step in the grading procedure: grading using predefined\nrubrics. However, grading is a multifaceted procedure that encompasses other\ncrucial steps, such as grading rubrics design and post-grading review. There\nhas been a lack of systematic research exploring the potential of LLMs to\nenhance the entire grading~process.\n  In this paper, we propose an LLM-based grading system that addresses the\nentire grading procedure, including the following key components: 1) Developing\ngrading rubrics that not only consider the questions but also the student\nanswers, which can more accurately reflect students' performance. 2) Under the\nguidance of grading rubrics, providing accurate and consistent scores for each\nstudent, along with customized feedback. 3) Conducting post-grading review to\nbetter ensure accuracy and fairness. Additionally, we collected a new dataset\nnamed OS from a university operating system course and conducted extensive\nexperiments on both our new dataset and the widely used Mohler dataset.\nExperiments demonstrate the effectiveness of our proposed approach, providing\nsome new insights for developing automated grading systems based on LLMs.",
      "tldr_zh": "本研究指出，现有的 Large Language Models (LLMs) 在自动评分中表现不如人类，特别是处理复杂问题时，仅依赖预定义的 grading rubrics 导致局限性。论文提出一个全面的 LLM-based 评分系统，包括动态开发评分 rubrics（考虑问题和学生答案）、在 rubrics 指导下提供准确分数及个性化反馈，以及进行 post-grading review 以确保公平性。该系统在新收集的 OS 数据集和 Mohler 数据集上进行实验，证明了其有效性，并为基于 LLMs 的自动评分系统提供了新洞见。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19694v1",
      "published_date": "2024-05-30 05:08:15 UTC",
      "updated_date": "2024-05-30 05:08:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:19:28.653572"
    },
    {
      "arxiv_id": "2405.19690v3",
      "title": "Diffusion Policies creating a Trust Region for Offline Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Tianyu Chen",
        "Zhendong Wang",
        "Mingyuan Zhou"
      ],
      "abstract": "Offline reinforcement learning (RL) leverages pre-collected datasets to train\noptimal policies. Diffusion Q-Learning (DQL), introducing diffusion models as a\npowerful and expressive policy class, significantly boosts the performance of\noffline RL. However, its reliance on iterative denoising sampling to generate\nactions slows down both training and inference. While several recent attempts\nhave tried to accelerate diffusion-QL, the improvement in training and/or\ninference speed often results in degraded performance. In this paper, we\nintroduce a dual policy approach, Diffusion Trusted Q-Learning (DTQL), which\ncomprises a diffusion policy for pure behavior cloning and a practical one-step\npolicy. We bridge the two polices by a newly introduced diffusion trust region\nloss. The diffusion policy maintains expressiveness, while the trust region\nloss directs the one-step policy to explore freely and seek modes within the\nregion defined by the diffusion policy. DTQL eliminates the need for iterative\ndenoising sampling during both training and inference, making it remarkably\ncomputationally efficient. We evaluate its effectiveness and algorithmic\ncharacteristics against popular Kullback--Leibler divergence-based distillation\nmethods in 2D bandit scenarios and gym tasks. We then show that DTQL could not\nonly outperform other methods on the majority of the D4RL benchmark tasks but\nalso demonstrate efficiency in training and inference speeds. The PyTorch\nimplementation is available at\nhttps://github.com/TianyuCodings/Diffusion_Trusted_Q_Learning.",
      "tldr_zh": "这篇论文针对离线强化学习（Offline Reinforcement Learning）中 Diffusion Q-Learning (DQL) 的训练和推理速度问题，提出了一种新方法：Diffusion Trusted Q-Learning (DTQL)。DTQL 采用双策略框架，包括一个用于纯行为克隆的扩散策略和一个高效的一步策略，通过引入扩散信任区域损失（diffusion trust region loss）来桥接两者，确保一步策略在扩散策略定义的信任区域内自由探索，同时保持模型的表达性。实验结果表明，DTQL 在 2D 强盗场景、Gym 任务和 D4RL 基准任务上优于基于 Kullback-Leibler 散度的蒸馏方法，不仅提升了性能，还显著提高了训练和推理效率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.19690v3",
      "published_date": "2024-05-30 05:04:33 UTC",
      "updated_date": "2024-10-31 18:09:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:19:40.670297"
    },
    {
      "arxiv_id": "2405.19686v1",
      "title": "Knowledge Graph Tuning: Real-time Large Language Model Personalization based on Human Feedback",
      "title_zh": "知识图谱调优：基于人类反馈的实时大语言模型个性化",
      "authors": [
        "Jingwei Sun",
        "Zhixu Du",
        "Yiran Chen"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable proficiency in a\nrange of natural language processing tasks. Once deployed, LLMs encounter users\nwith personalized factual knowledge, and such personalized knowledge is\nconsistently reflected through users' interactions with the LLMs. To enhance\nuser experience, real-time model personalization is essential, allowing LLMs to\nadapt user-specific knowledge based on user feedback during human-LLM\ninteractions. Existing methods mostly require back-propagation to finetune the\nmodel parameters, which incurs high computational and memory costs. In\naddition, these methods suffer from low interpretability, which will cause\nunforeseen impacts on model performance during long-term use, where the user's\npersonalized knowledge is accumulated extensively.To address these challenges,\nwe propose Knowledge Graph Tuning (KGT), a novel approach that leverages\nknowledge graphs (KGs) to personalize LLMs. KGT extracts personalized factual\nknowledge triples from users' queries and feedback and optimizes KGs without\nmodifying the LLM parameters. Our method improves computational and memory\nefficiency by avoiding back-propagation and ensures interpretability by making\nthe KG adjustments comprehensible to humans.Experiments with state-of-the-art\nLLMs, including GPT-2, Llama2, and Llama3, show that KGT significantly improves\npersonalization performance while reducing latency and GPU memory costs.\nUltimately, KGT offers a promising solution of effective, efficient, and\ninterpretable real-time LLM personalization during user interactions with the\nLLMs.",
      "tldr_zh": "这篇论文提出 Knowledge Graph Tuning (KGT)，一种基于知识图谱 (KGs) 的方法，用于根据用户反馈实时个性化 Large Language Models (LLMs)，以解决现有微调方法的计算高成本和低可解释性问题。KGT 通过从用户查询和反馈中提取个性化事实三元组来优化 KGs，而不需修改 LLM 参数，从而提高了计算效率、内存利用率和人类可理解性。实验在 GPT-2、Llama2 和 Llama3 等模型上显示，KGT 显著提升了个性化性能，同时降低了延迟和 GPU 内存成本。该方法为高效、可解释的 LLM 个性化提供了可行的解决方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19686v1",
      "published_date": "2024-05-30 04:57:03 UTC",
      "updated_date": "2024-05-30 04:57:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:19:52.474785"
    },
    {
      "arxiv_id": "2405.19678v2",
      "title": "View-Consistent Hierarchical 3D Segmentation Using Ultrametric Feature Fields",
      "title_zh": "翻译失败",
      "authors": [
        "Haodi He",
        "Colton Stearns",
        "Adam W. Harley",
        "Leonidas J. Guibas"
      ],
      "abstract": "Large-scale vision foundation models such as Segment Anything (SAM)\ndemonstrate impressive performance in zero-shot image segmentation at multiple\nlevels of granularity. However, these zero-shot predictions are rarely\n3D-consistent. As the camera viewpoint changes in a scene, so do the\nsegmentation predictions, as well as the characterizations of \"coarse\" or\n\"fine\" granularity. In this work, we address the challenging task of lifting\nmulti-granular and view-inconsistent image segmentations into a hierarchical\nand 3D-consistent representation. We learn a novel feature field within a\nNeural Radiance Field (NeRF) representing a 3D scene, whose segmentation\nstructure can be revealed at different scales by simply using different\nthresholds on feature distance. Our key idea is to learn an ultrametric feature\nspace, which unlike a Euclidean space, exhibits transitivity in distance-based\ngrouping, naturally leading to a hierarchical clustering. Put together, our\nmethod takes view-inconsistent multi-granularity 2D segmentations as input and\nproduces a hierarchy of 3D-consistent segmentations as output. We evaluate our\nmethod and several baselines on synthetic datasets with multi-view images and\nmulti-granular segmentation, showcasing improved accuracy and\nviewpoint-consistency. We additionally provide qualitative examples of our\nmodel's 3D hierarchical segmentations in real world scenes. The code and\ndataset are available at https://github.com/hardyho/ultrametric_feature_fields",
      "tldr_zh": "本研究针对现有视觉基础模型如 Segment Anything (SAM) 在多粒度图像分割中缺乏 3D 一致性的问题，提出了一种基于 Ultrametric Feature Fields 的层次化 3D 分割方法。方法通过在 Neural Radiance Field (NeRF) 中学习一个超度量特征空间，利用特征距离的阈值来实现不同尺度的层次聚类，从而将视点不一致的多粒度 2D 分割转化为 3D 一致的层次表示。实验结果显示，该方法在合成数据集上显著提升了分割准确性和视点一致性，并提供了真实场景的定性示例，代码和数据集可从 GitHub 获取。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19678v2",
      "published_date": "2024-05-30 04:14:58 UTC",
      "updated_date": "2024-07-18 02:28:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:20:15.388848"
    },
    {
      "arxiv_id": "2405.19677v1",
      "title": "Large Language Model Watermark Stealing With Mixed Integer Programming",
      "title_zh": "使用混合整数规划的大语言模型水印窃取",
      "authors": [
        "Zhaoxi Zhang",
        "Xiaomei Zhang",
        "Yanjun Zhang",
        "Leo Yu Zhang",
        "Chao Chen",
        "Shengshan Hu",
        "Asif Gill",
        "Shirui Pan"
      ],
      "abstract": "The Large Language Model (LLM) watermark is a newly emerging technique that\nshows promise in addressing concerns surrounding LLM copyright, monitoring\nAI-generated text, and preventing its misuse. The LLM watermark scheme commonly\nincludes generating secret keys to partition the vocabulary into green and red\nlists, applying a perturbation to the logits of tokens in the green list to\nincrease their sampling likelihood, thus facilitating watermark detection to\nidentify AI-generated text if the proportion of green tokens exceeds a\nthreshold. However, recent research indicates that watermarking methods using\nnumerous keys are susceptible to removal attacks, such as token editing,\nsynonym substitution, and paraphrasing, with robustness declining as the number\nof keys increases. Therefore, the state-of-the-art watermark schemes that\nemploy fewer or single keys have been demonstrated to be more robust against\ntext editing and paraphrasing. In this paper, we propose a novel green list\nstealing attack against the state-of-the-art LLM watermark scheme and\nsystematically examine its vulnerability to this attack. We formalize the\nattack as a mixed integer programming problem with constraints. We evaluate our\nattack under a comprehensive threat model, including an extreme scenario where\nthe attacker has no prior knowledge, lacks access to the watermark detector\nAPI, and possesses no information about the LLM's parameter settings or\nwatermark injection/detection scheme. Extensive experiments on LLMs, such as\nOPT and LLaMA, demonstrate that our attack can successfully steal the green\nlist and remove the watermark across all settings.",
      "tldr_zh": "本文提出了一种针对大语言模型(LLM)水印方案的绿列表窃取攻击(green list stealing attack)，旨在揭示这些方案在面对攻击时的脆弱性，特别是使用少量或单一密钥的状态-of-the-art方法。攻击过程被形式化为混合整数编程(Mixed Integer Programming)问题，允许攻击者在无先验知识、无访问水印检测器API的情况下进行操作。实验在OPT和LLaMA等模型上证明，该攻击在各种威胁模型下均能成功窃取绿列表并移除水印，突显了现有水印技术的潜在风险。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "12 pages",
      "pdf_url": "http://arxiv.org/pdf/2405.19677v1",
      "published_date": "2024-05-30 04:11:17 UTC",
      "updated_date": "2024-05-30 04:11:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:20:16.242014"
    },
    {
      "arxiv_id": "2405.19673v2",
      "title": "Bridging Model-Based Optimization and Generative Modeling via Conservative Fine-Tuning of Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Masatoshi Uehara",
        "Yulai Zhao",
        "Ehsan Hajiramezanali",
        "Gabriele Scalia",
        "Gökcen Eraslan",
        "Avantika Lal",
        "Sergey Levine",
        "Tommaso Biancalani"
      ],
      "abstract": "AI-driven design problems, such as DNA/protein sequence design, are commonly\ntackled from two angles: generative modeling, which efficiently captures the\nfeasible design space (e.g., natural images or biological sequences), and\nmodel-based optimization, which utilizes reward models for extrapolation. To\ncombine the strengths of both approaches, we adopt a hybrid method that\nfine-tunes cutting-edge diffusion models by optimizing reward models through\nRL. Although prior work has explored similar avenues, they primarily focus on\nscenarios where accurate reward models are accessible. In contrast, we\nconcentrate on an offline setting where a reward model is unknown, and we must\nlearn from static offline datasets, a common scenario in scientific domains. In\noffline scenarios, existing approaches tend to suffer from overoptimization, as\nthey may be misled by the reward model in out-of-distribution regions. To\naddress this, we introduce a conservative fine-tuning approach, BRAID, by\noptimizing a conservative reward model, which includes additional penalization\noutside of offline data distributions. Through empirical and theoretical\nanalysis, we demonstrate the capability of our approach to outperform the best\ndesigns in offline data, leveraging the extrapolation capabilities of reward\nmodels while avoiding the generation of invalid designs through pre-trained\ndiffusion models.",
      "tldr_zh": "该研究旨在桥接基于模型的优化(Model-Based Optimization)和生成建模(Generative Modeling)，通过引入 BRAID 方法来保守微调扩散模型(Diffusion Models)，以解决 AI 驱动设计问题（如 DNA/蛋白质序列设计）中的挑战。BRAID 在离线设置下优化奖励模型，通过添加分布外惩罚来避免过优化，确保从静态数据集学习时不会生成无效设计。该方法结合强化学习(RL)的外推能力，实证和理论分析显示，BRAID 能超越离线数据中的最佳设计，同时保持生成结果的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Under review",
      "pdf_url": "http://arxiv.org/pdf/2405.19673v2",
      "published_date": "2024-05-30 03:57:29 UTC",
      "updated_date": "2024-05-31 18:34:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:20:28.125599"
    },
    {
      "arxiv_id": "2405.19667v1",
      "title": "Reconciling Model Multiplicity for Downstream Decision Making",
      "title_zh": "调和模型多重性以用于下游决策",
      "authors": [
        "Ally Yalei Du",
        "Dung Daniel Ngo",
        "Zhiwei Steven Wu"
      ],
      "abstract": "We consider the problem of model multiplicity in downstream decision-making,\na setting where two predictive models of equivalent accuracy cannot agree on\nthe best-response action for a downstream loss function. We show that even when\nthe two predictive models approximately agree on their individual predictions\nalmost everywhere, it is still possible for their induced best-response actions\nto differ on a substantial portion of the population. We address this issue by\nproposing a framework that calibrates the predictive models with regard to both\nthe downstream decision-making problem and the individual probability\nprediction. Specifically, leveraging tools from multi-calibration, we provide\nan algorithm that, at each time-step, first reconciles the differences in\nindividual probability prediction, then calibrates the updated models such that\nthey are indistinguishable from the true probability distribution to the\ndecision-maker. We extend our results to the setting where one does not have\ndirect access to the true probability distribution and instead relies on a set\nof i.i.d data to be the empirical distribution. Finally, we provide a set of\nexperiments to empirically evaluate our methods: compared to existing work, our\nproposed algorithm creates a pair of predictive models with both improved\ndownstream decision-making losses and agrees on their best-response actions\nalmost everywhere.",
      "tldr_zh": "该研究探讨了模型多样性（model multiplicity）在下游决策（downstream decision making）中的问题，即使两个预测模型在准确性和个别预测上大致一致，它们的最佳响应行动仍可能在大量人群上存在差异。为解决此问题，论文提出一个框架，利用多校准（multi-calibration）工具的算法，在每个时间步先调和模型的概率预测差异，然后校准模型，使其对决策者而言与真实概率分布一致。算法扩展到无法直接访问真实分布的情形，使用 i.i.d 数据作为经验分布；实验结果显示，该方法显著改善了下游决策损失，并使两个模型的最佳响应行动几乎完全一致。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages main body, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.19667v1",
      "published_date": "2024-05-30 03:36:46 UTC",
      "updated_date": "2024-05-30 03:36:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:20:40.643021"
    },
    {
      "arxiv_id": "2405.19665v1",
      "title": "A novel fault localization with data refinement for hydroelectric units",
      "title_zh": "一种针对水力发电机组的新颖故障定位方法，结合数据精炼",
      "authors": [
        "Jialong Huang",
        "Junlin Song",
        "Penglong Lian",
        "Mengjie Gan",
        "Zhiheng Su",
        "Benhao Wang",
        "Wenji Zhu",
        "Xiaomin Pu",
        "Jianxiao Zou",
        "Shicai Fan"
      ],
      "abstract": "Due to the scarcity of fault samples and the complexity of non-linear and\nnon-smooth characteristics data in hydroelectric units, most of the traditional\nhydroelectric unit fault localization methods are difficult to carry out\naccurate localization. To address these problems, a sparse autoencoder\n(SAE)-generative adversarial network (GAN)-wavelet noise reduction (WNR)-\nmanifold-boosted deep learning (SG-WMBDL) based fault localization method for\nhydroelectric units is proposed. To overcome the data scarcity, a SAE is\nembedded into the GAN to generate more high-quality samples in the data\ngeneration module. Considering the signals involving non-linear and non-smooth\ncharacteristics, the improved WNR which combining both soft and hard\nthresholding and local linear embedding (LLE) are utilized to the data\npreprocessing module in order to reduce the noise and effectively capture the\nlocal features. In addition, to seek higher performance, the novel Adaptive\nBoost (AdaBoost) combined with multi deep learning is proposed to achieve\naccurate fault localization. The experimental results show that the SG-WMBDL\ncan locate faults for hydroelectric units under a small number of fault samples\nwith non-linear and non-smooth characteristics on higher precision and accuracy\ncompared to other frontier methods, which verifies the effectiveness and\npracticality of the proposed method.",
      "tldr_zh": "该论文针对水电设备故障样本稀少以及数据非线性、非平滑特性带来的挑战，提出了一种新型故障定位方法SG-WMBDL。方法包括使用SAE嵌入GAN生成高质量样本来解决数据稀缺问题，以及结合改进的WNR（融合软硬阈值）和LLE进行噪声减少和局部特征提取。最终，通过AdaBoost结合多深度学习模型实现精确故障定位，实验结果显示SG-WMBDL在少量样本下比其他前沿方法精度和准确性更高，证明了其有效性和实用性。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "6pages,4 figures,Conference on Decision and Control(CDC) conference",
      "pdf_url": "http://arxiv.org/pdf/2405.19665v1",
      "published_date": "2024-05-30 03:33:49 UTC",
      "updated_date": "2024-05-30 03:33:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:20:54.199838"
    },
    {
      "arxiv_id": "2405.19657v1",
      "title": "Uncertainty-guided Optimal Transport in Depth Supervised Sparse-View 3D Gaussian",
      "title_zh": "翻译失败",
      "authors": [
        "Wei Sun",
        "Qi Zhang",
        "Yanzhao Zhou",
        "Qixiang Ye",
        "Jianbin Jiao",
        "Yuan Li"
      ],
      "abstract": "3D Gaussian splatting has demonstrated impressive performance in real-time\nnovel view synthesis. However, achieving successful reconstruction from RGB\nimages generally requires multiple input views captured under static\nconditions. To address the challenge of sparse input views, previous approaches\nhave incorporated depth supervision into the training of 3D Gaussians to\nmitigate overfitting, using dense predictions from pretrained depth networks as\npseudo-ground truth. Nevertheless, depth predictions from monocular depth\nestimation models inherently exhibit significant uncertainty in specific areas.\nRelying solely on pixel-wise L2 loss may inadvertently incorporate detrimental\nnoise from these uncertain areas. In this work, we introduce a novel method to\nsupervise the depth distribution of 3D Gaussians, utilizing depth priors with\nintegrated uncertainty estimates. To address these localized errors in depth\npredictions, we integrate a patch-wise optimal transport strategy to complement\ntraditional L2 loss in depth supervision. Extensive experiments conducted on\nthe LLFF, DTU, and Blender datasets demonstrate that our approach, UGOT,\nachieves superior novel view synthesis and consistently outperforms\nstate-of-the-art methods.",
      "tldr_zh": "本文提出了一种不确定性引导的深度监督方法，针对稀疏视图下 3D Gaussian 的重建问题，以缓解单目深度估计预测中的不确定性噪声。方法利用带有不确定性估计的深度先验，并结合 patch-wise optimal transport 策略来补充传统的 L2 loss，从而更有效地监督 3D Gaussians 的深度分布。在 LLFF、DTU 和 Blender 数据集上的实验显示，该方法 UGOT 在新视图合成性能上显著优于现有最先进方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10pages",
      "pdf_url": "http://arxiv.org/pdf/2405.19657v1",
      "published_date": "2024-05-30 03:18:30 UTC",
      "updated_date": "2024-05-30 03:18:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:21:05.872864"
    },
    {
      "arxiv_id": "2405.19656v1",
      "title": "Accurate and Reliable Predictions with Mutual-Transport Ensemble",
      "title_zh": "Mutual-Transport Ensemble 用于准确且可靠的预测",
      "authors": [
        "Han Liu",
        "Peng Cui",
        "Bingning Wang",
        "Jun Zhu",
        "Xiaolin Hu"
      ],
      "abstract": "Deep Neural Networks (DNNs) have achieved remarkable success in a variety of\ntasks, especially when it comes to prediction accuracy. However, in complex\nreal-world scenarios, particularly in safety-critical applications, high\naccuracy alone is not enough. Reliable uncertainty estimates are crucial.\nModern DNNs, often trained with cross-entropy loss, tend to be overconfident,\nespecially with ambiguous samples. To improve uncertainty calibration, many\ntechniques have been developed, but they often compromise prediction accuracy.\nTo tackle this challenge, we propose the ``mutual-transport ensemble'' (MTE).\nThis approach introduces a co-trained auxiliary model and adaptively\nregularizes the cross-entropy loss using Kullback-Leibler (KL) divergence\nbetween the prediction distributions of the primary and auxiliary models. We\nconducted extensive studies on various benchmarks to validate the effectiveness\nof our method. The results show that MTE can simultaneously enhance both\naccuracy and uncertainty calibration. For example, on the CIFAR-100 dataset,\nour MTE method on ResNet34/50 achieved significant improvements compared to\nprevious state-of-the-art method, with absolute accuracy increases of\n2.4%/3.7%, relative reductions in ECE of $42.3%/29.4%, and relative reductions\nin classwise-ECE of 11.6%/15.3%.",
      "tldr_zh": "本研究针对深度神经网络（DNNs）在预测准确性上取得的成功，但强调在安全关键应用中，可靠的不确定性估计同样重要，因为现有模型常因过度自信而表现不佳。  \n为解决这一问题，作者提出mutual-transport ensemble (MTE)方法，该方法引入一个共同训练的辅助模型，并利用Kullback-Leibler (KL)散度来自适应调节交叉熵损失，从而同时提升准确性和不确定性校准。  \n实验结果显示，MTE在CIFAR-100等基准数据集上表现出色，例如在ResNet34/50模型上，准确率分别提高了2.4%和3.7%，Expected Calibration Error (ECE)分别减少了42.3%和29.4%。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19656v1",
      "published_date": "2024-05-30 03:15:59 UTC",
      "updated_date": "2024-05-30 03:15:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:21:19.773678"
    },
    {
      "arxiv_id": "2405.19654v1",
      "title": "Unlocking the Power of Spatial and Temporal Information in Medical Multimodal Pre-training",
      "title_zh": "翻译失败",
      "authors": [
        "Jinxia Yang",
        "Bing Su",
        "Wayne Xin Zhao",
        "Ji-Rong Wen"
      ],
      "abstract": "Medical vision-language pre-training methods mainly leverage the\ncorrespondence between paired medical images and radiological reports. Although\nmulti-view spatial images and temporal sequences of image-report pairs are\navailable in off-the-shelf multi-modal medical datasets, most existing methods\nhave not thoroughly tapped into such extensive supervision signals. In this\npaper, we introduce the Med-ST framework for fine-grained spatial and temporal\nmodeling to exploit information from multiple spatial views of chest\nradiographs and temporal historical records. For spatial modeling, Med-ST\nemploys the Mixture of View Expert (MoVE) architecture to integrate different\nvisual features from both frontal and lateral views. To achieve a more\ncomprehensive alignment, Med-ST not only establishes the global alignment\nbetween whole images and texts but also introduces modality-weighted local\nalignment between text tokens and spatial regions of images. For temporal\nmodeling, we propose a novel cross-modal bidirectional cycle consistency\nobjective by forward mapping classification (FMC) and reverse mapping\nregression (RMR). By perceiving temporal information from simple to complex,\nMed-ST can learn temporal semantics. Experimental results across four distinct\ntasks demonstrate the effectiveness of Med-ST, especially in temporal\nclassification tasks. Our code and model are available at\nhttps://github.com/SVT-Yang/MedST.",
      "tldr_zh": "本论文提出 Med-ST 框架，用于医疗视觉语言预训练，通过挖掘多视图空间图像和时间序列数据来提升模型性能。框架在空间建模方面采用 Mixture of View Expert (MoVE) 架构，整合正面和侧面视图的视觉特征，并实现全局对齐和模态加权的局部对齐，以更全面地匹配图像区域和文本标记。对于时间建模，Med-ST 引入跨模态双向循环一致性目标，包括 forward mapping classification (FMC) 和 reverse mapping regression (RMR)，以学习从简单到复杂的时序语义。实验结果显示，Med-ST 在四个不同任务上表现出色，尤其在时间分类任务中取得了显著改进。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.19654v1",
      "published_date": "2024-05-30 03:15:09 UTC",
      "updated_date": "2024-05-30 03:15:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:21:32.425136"
    },
    {
      "arxiv_id": "2405.19650v2",
      "title": "Few for Many: Tchebycheff Set Scalarization for Many-Objective Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Xi Lin",
        "Yilu Liu",
        "Xiaoyuan Zhang",
        "Fei Liu",
        "Zhenkun Wang",
        "Qingfu Zhang"
      ],
      "abstract": "Multi-objective optimization can be found in many real-world applications\nwhere some conflicting objectives can not be optimized by a single solution.\nExisting optimization methods often focus on finding a set of Pareto solutions\nwith different optimal trade-offs among the objectives. However, the required\nnumber of solutions to well approximate the whole Pareto optimal set could be\nexponentially large with respect to the number of objectives, which makes these\nmethods unsuitable for handling many optimization objectives. In this work,\ninstead of finding a dense set of Pareto solutions, we propose a novel\nTchebycheff set scalarization method to find a few representative solutions\n(e.g., 5) to cover a large number of objectives (e.g., $>100$) in a\ncollaborative and complementary manner. In this way, each objective can be well\naddressed by at least one solution in the small solution set. In addition, we\nfurther develop a smooth Tchebycheff set scalarization approach for efficient\noptimization with good theoretical guarantees. Experimental studies on\ndifferent problems with many optimization objectives demonstrate the\neffectiveness of our proposed method.",
      "tldr_zh": "本研究针对多目标优化（multi-objective optimization）问题，提出了一种名为 Tchebycheff set scalarization 的新方法，以解决现有方法在处理大量目标时需要指数级 Pareto optimal set 解集的问题。该方法通过找到少量代表性解（如5个）来协作互补地覆盖众多目标（如>100个），确保每个目标至少由一个解得到良好处理，并引入平滑版本以实现高效优化和理论保障。实验结果在不同多目标问题上证明，该方法有效提高了优化效率和性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19650v2",
      "published_date": "2024-05-30 03:04:57 UTC",
      "updated_date": "2024-10-15 09:57:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:21:41.196685"
    },
    {
      "arxiv_id": "2405.19648v1",
      "title": "Detecting Hallucinations in Large Language Model Generation: A Token Probability Approach",
      "title_zh": "检测大型语言模型生成中的幻觉：一种标记概率方法",
      "authors": [
        "Ernesto Quevedo",
        "Jorge Yero",
        "Rachel Koerner",
        "Pablo Rivas",
        "Tomas Cerny"
      ],
      "abstract": "Concerns regarding the propensity of Large Language Models (LLMs) to produce\ninaccurate outputs, also known as hallucinations, have escalated. Detecting\nthem is vital for ensuring the reliability of applications relying on\nLLM-generated content. Current methods often demand substantial resources and\nrely on extensive LLMs or employ supervised learning with multidimensional\nfeatures or intricate linguistic and semantic analyses difficult to reproduce\nand largely depend on using the same LLM that hallucinated. This paper\nintroduces a supervised learning approach employing two simple classifiers\nutilizing only four numerical features derived from tokens and vocabulary\nprobabilities obtained from other LLM evaluators, which are not necessarily the\nsame. The method yields promising results, surpassing state-of-the-art outcomes\nin multiple tasks across three different benchmarks. Additionally, we provide a\ncomprehensive examination of the strengths and weaknesses of our approach,\nhighlighting the significance of the features utilized and the LLM employed as\nan evaluator. We have released our code publicly at\nhttps://github.com/Baylor-AI/HalluDetect.",
      "tldr_zh": "这篇论文提出了一种基于token概率的监督学习方法，用于检测大型语言模型(LLMs)生成的幻觉(hallucinations)，以提升应用的可靠性。该方法采用两个简单分类器，仅利用四个数字特征，这些特征来自其他LLM评估器的tokens和词汇概率，从而避免了对产生幻觉的同一LLM的依赖。实验结果显示，该方法在多个任务和三个基准上超过了现有最先进的技术，并通过全面分析了特征和评估器的重要性，同时公开了代码（https://github.com/Baylor-AI/HalluDetect）。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "ICAI'24 - The 26th Int'l Conf on Artificial Intelligence",
      "pdf_url": "http://arxiv.org/pdf/2405.19648v1",
      "published_date": "2024-05-30 03:00:47 UTC",
      "updated_date": "2024-05-30 03:00:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:21:54.136428"
    },
    {
      "arxiv_id": "2405.20354v2",
      "title": "Efficient Systematic Reviews: Literature Filtering with Transformers & Transfer Learning",
      "title_zh": "高效系统综述：利用 Transformers 和迁移学习的文献过滤",
      "authors": [
        "John Hawkins",
        "David Tivey"
      ],
      "abstract": "Identifying critical research within the growing body of academic work is an\nintrinsic aspect of conducting quality research. Systematic review processes\nused in evidence-based medicine formalise this as a procedure that must be\nfollowed in a research program. However, it comes with an increasing burden in\nterms of the time required to identify the important articles of research for a\ngiven topic. In this work, we develop a method for building a general-purpose\nfiltering system that matches a research question, posed as a natural language\ndescription of the required content, against a candidate set of articles\nobtained via the application of broad search terms. Our results demonstrate\nthat transformer models, pre-trained on biomedical literature, and then fine\ntuned for the specific task, offer a promising solution to this problem. The\nmodel can remove large volumes of irrelevant articles for most research\nquestions. Furthermore, analysis of the specific research questions in our\ntraining data suggest natural avenues for further improvement.",
      "tldr_zh": "本研究针对系统综述中文献筛选的耗时问题，提出了一种高效的过滤方法，使用Transformer模型结合Transfer Learning来匹配自然语言描述的研究问题与候选文章集。模型先在生物医学文献上预训练，然后针对特定任务进行微调，能够有效移除大量无关文章，从而加速文献筛选过程。实验结果显示，该方法为大多数研究问题提供可行的解决方案，并基于训练数据分析提出进一步改进的方向。",
      "categories": [
        "cs.DL",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.DL",
      "comment": "Paper Submitted to `Multimedia Tools and Applications`",
      "pdf_url": "http://arxiv.org/pdf/2405.20354v2",
      "published_date": "2024-05-30 02:55:49 UTC",
      "updated_date": "2024-10-10 23:20:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:22:04.414853"
    },
    {
      "arxiv_id": "2405.19644v3",
      "title": "EgoSurgery-Phase: A Dataset of Surgical Phase Recognition from Egocentric Open Surgery Videos",
      "title_zh": "翻译失败",
      "authors": [
        "Ryo Fujii",
        "Masashi Hatano",
        "Hideo Saito",
        "Hiroki Kajita"
      ],
      "abstract": "Surgical phase recognition has gained significant attention due to its\npotential to offer solutions to numerous demands of the modern operating room.\nHowever, most existing methods concentrate on minimally invasive surgery (MIS),\nleaving surgical phase recognition for open surgery understudied. This\ndiscrepancy is primarily attributed to the scarcity of publicly available open\nsurgery video datasets for surgical phase recognition. To address this issue,\nwe introduce a new egocentric open surgery video dataset for phase recognition,\nnamed EgoSurgery-Phase. This dataset comprises 15 hours of real open surgery\nvideos spanning 9 distinct surgical phases all captured using an egocentric\ncamera attached to the surgeon's head. In addition to video, the\nEgoSurgery-Phase offers eye gaze. As far as we know, it is the first real open\nsurgery video dataset for surgical phase recognition publicly available.\nFurthermore, inspired by the notable success of masked autoencoders (MAEs) in\nvideo understanding tasks (e.g., action recognition), we propose a gaze-guided\nmasked autoencoder (GGMAE). Considering the regions where surgeons' gaze\nfocuses are often critical for surgical phase recognition (e.g., surgical\nfield), in our GGMAE, the gaze information acts as an empirical semantic\nrichness prior to guiding the masking process, promoting better attention to\nsemantically rich spatial regions. GGMAE significantly improves the previous\nstate-of-the-art recognition method (6.4% in Jaccard) and the masked\nautoencoder-based method (3.1% in Jaccard) on EgoSurgery-Phase. The dataset is\nreleased at https://github.com/Fujiry0/EgoSurgery.",
      "tldr_zh": "该研究引入了EgoSurgery-Phase数据集，这是首个公开可用的第一人称视角（egocentric）开放手术视频数据集，用于手术阶段识别，包括15小时视频覆盖9个手术阶段，并提供eye gaze数据，以填补现有数据集的空白。EgoSurgery-Phase专注于开放手术（open surgery），而非微创手术（MIS），旨在解决手术阶段识别的实际需求。论文提出了一种gaze-guided masked autoencoder (GGMAE)方法，利用外科医生的eye gaze信息指导掩码过程，优先关注语义丰富的区域，从而提升视频理解能力。在EgoSurgery-Phase数据集上，GGMAE比现有最先进方法提高了6.4%（Jaccard指标），并比基于masked autoencoders的方法提高了3.1%。数据集已发布在GitHub上。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Early accepted by MICCAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.19644v3",
      "published_date": "2024-05-30 02:53:19 UTC",
      "updated_date": "2024-11-27 04:52:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:22:18.121368"
    },
    {
      "arxiv_id": "2405.19642v1",
      "title": "Few-shot fault diagnosis based on multi-scale graph convolution filtering for industry",
      "title_zh": "翻译失败",
      "authors": [
        "Mengjie Gan",
        "Penglong Lian",
        "Zhiheng Su",
        "Jiyang Zhang",
        "Jialong Huang",
        "Benhao Wang",
        "Jianxiao Zou",
        "Shicai Fan"
      ],
      "abstract": "Industrial equipment fault diagnosis often encounter challenges such as the\nscarcity of fault data, complex operating conditions, and varied types of\nfailures. Signal analysis, data statistical learning, and conventional deep\nlearning techniques face constraints under these conditions due to their\nsubstantial data requirements and the necessity for transfer learning to\naccommodate new failure modes. To effectively leverage information and extract\nthe intrinsic characteristics of faults across different domains under limited\nsample conditions, this paper introduces a fault diagnosis approach employing\nMulti-Scale Graph Convolution Filtering (MSGCF). MSGCF enhances the traditional\nGraph Neural Network (GNN) framework by integrating both local and global\ninformation fusion modules within the graph convolution filter block. This\nadvancement effectively mitigates the over-smoothing issue associated with\nexcessive layering of graph convolutional layers while preserving a broad\nreceptive field. It also reduces the risk of overfitting in few-shot diagnosis,\nthereby augmenting the model's representational capacity. Experiments on the\nUniversity of Paderborn bearing dataset (PU) demonstrate that the MSGCF method\nproposed herein surpasses alternative approaches in accuracy, thereby offering\nvaluable insights for industrial fault diagnosis in few-shot learning\nscenarios.",
      "tldr_zh": "本文针对工业设备故障诊断中的数据稀缺、复杂操作条件和多种故障类型等问题，提出了一种基于 Multi-Scale Graph Convolution Filtering (MSGCF) 的 Few-shot fault diagnosis 方法。MSGCF 在 Graph Neural Network (GNN) 框架中整合本地和全局信息融合模块，缓解 over-smoothing 问题、保持宽广的 receptive field，并减少少样本场景下的过拟合风险，从而提升模型的表示能力。在 University of Paderborn bearing dataset (PU) 上进行的实验显示，该方法在准确率上优于其他方法，为工业领域的少样本学习提供重要洞见。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "6 pages, 2 figures, 2 tables, 63rd IEEE Conference on Decision and\n  Control",
      "pdf_url": "http://arxiv.org/pdf/2405.19642v1",
      "published_date": "2024-05-30 02:51:29 UTC",
      "updated_date": "2024-05-30 02:51:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:22:29.706769"
    },
    {
      "arxiv_id": "2405.19631v1",
      "title": "Leveraging Open-Source Large Language Models for encoding Social Determinants of Health using an Intelligent Router",
      "title_zh": "翻译失败",
      "authors": [
        "Akul Goel",
        "Surya Narayanan Hari",
        "Belinda Waltman",
        "Matt Thomson"
      ],
      "abstract": "Social Determinants of Health (SDOH) play a significant role in patient\nhealth outcomes. The Center of Disease Control (CDC) introduced a subset of\nICD-10 codes called Z-codes in an attempt to officially recognize and measure\nSDOH in the health care system. However, these codes are rarely annotated in a\npatient's Electronic Health Record (EHR), and instead, in many cases, need to\nbe inferred from clinical notes. Previous research has shown that large\nlanguage models (LLMs) show promise on extracting unstructured data from EHRs.\nHowever, with thousands of models to choose from with unique architectures and\ntraining sets, it's difficult to choose one model that performs the best on\ncoding tasks. Further, clinical notes contain trusted health information making\nthe use of closed-source language models from commercial vendors difficult, so\nthe identification of open source LLMs that can be run within health\norganizations and exhibits high performance on SDOH tasks is an urgent problem.\nHere, we introduce an intelligent routing system for SDOH coding that uses a\nlanguage model router to direct medical record data to open source LLMs that\ndemonstrate optimal performance on specific SDOH codes. The intelligent routing\nsystem exhibits state of the art performance of 97.4% accuracy averaged across\n5 codes, including homelessness and food insecurity, on par with closed models\nsuch as GPT-4o. In order to train the routing system and validate models, we\nalso introduce a synthetic data generation and validation paradigm to increase\nthe scale of training data without needing privacy protected medical records.\nTogether, we demonstrate an architecture for intelligent routing of inputs to\ntask-optimal language models to achieve high performance across a set of\nmedical coding sub-tasks.",
      "tldr_zh": "这篇论文提出了一种利用开源Large Language Models (LLMs)构建的智能路由系统，用于编码Social Determinants of Health (SDOH)，以从临床笔记中推断ICD-10 Z-codes。系统通过语言模型路由器将医疗记录数据定向到特定SDOH代码上表现最佳的开源LLMs，实现平均准确率达97.4%，与闭源模型如GPT-4o相当。论文还引入了合成数据生成和验证范式，以扩大训练数据集规模，同时避免使用受隐私保护的真实医疗记录，从而为SDOH编码任务提供高效、可信的架构。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19631v1",
      "published_date": "2024-05-30 02:33:28 UTC",
      "updated_date": "2024-05-30 02:33:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:22:44.171672"
    },
    {
      "arxiv_id": "2405.19616v2",
      "title": "Easy Problems That LLMs Get Wrong",
      "title_zh": "翻译失败",
      "authors": [
        "Sean Williams",
        "James Huckle"
      ],
      "abstract": "We introduce a comprehensive Linguistic Benchmark designed to evaluate the\nlimitations of Large Language Models (LLMs) in domains such as logical\nreasoning, spatial intelligence, and linguistic understanding, among others.\nThrough a series of straightforward questions, it uncovers the significant\nlimitations of well-regarded models to perform tasks that humans manage with\nease. It also highlights the potential of prompt engineering to mitigate some\nerrors and underscores the necessity for better training methodologies. Our\nfindings stress the importance of grounding LLMs with human reasoning and\ncommon sense, emphasising the need for human-in-the-loop for enterprise\napplications. We hope this work paves the way for future research to enhance\nthe usefulness and reliability of new models.",
      "tldr_zh": "这篇论文引入了一个全面的语言基准，用于评估大型语言模型 (LLMs) 在逻辑推理、空间智能和语言理解等领域的局限性，通过简单的问题揭示这些模型在人类轻松完成的任务上存在显著错误。研究发现，提示工程 (prompt engineering) 可以部分缓解这些问题，但强调需要改进训练方法，以更好地整合人类推理和常识。最终，该工作呼吁在企业应用中纳入人类参与，并为提升新模型的可靠性和实用性提供未来研究方向。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "AutogenAI Ltd. GitHub Repo:\n  https://github.com/autogenai/easy-problems-that-llms-get-wrong",
      "pdf_url": "http://arxiv.org/pdf/2405.19616v2",
      "published_date": "2024-05-30 02:09:51 UTC",
      "updated_date": "2024-06-01 03:00:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:22:56.436854"
    },
    {
      "arxiv_id": "2405.19606v2",
      "title": "Relation Modeling and Distillation for Learning with Noisy Labels",
      "title_zh": "关系建模",
      "authors": [
        "Xiaming Che",
        "Junlin Zhang",
        "Zhuang Qi",
        "Xin Qi"
      ],
      "abstract": "Learning with noisy labels has become an effective strategy for enhancing the\nrobustness of models, which enables models to better tolerate inaccurate data.\nExisting methods either focus on optimizing the loss function to mitigate the\ninterference from noise, or design procedures to detect potential noise and\ncorrect errors. However, their effectiveness is often compromised in\nrepresentation learning due to the dilemma where models overfit to noisy\nlabels. To address this issue, this paper proposes a relation modeling and\ndistillation framework that models inter-sample relationships via\nself-supervised learning and employs knowledge distillation to enhance\nunderstanding of latent associations, which mitigate the impact of noisy\nlabels. Specifically, the proposed method, termed RMDNet, includes two main\nmodules, where the relation modeling (RM) module implements the contrastive\nlearning technique to learn representations of all data, an unsupervised\napproach that effectively eliminates the interference of noisy tags on feature\nextraction. The relation-guided representation learning (RGRL) module utilizes\ninter-sample relation learned from the RM module to calibrate the\nrepresentation distribution for noisy samples, which is capable of improving\nthe generalization of the model in the inference phase. Notably, the proposed\nRMDNet is a plug-and-play framework that can integrate multiple methods to its\nadvantage. Extensive experiments were conducted on two datasets, including\nperformance comparison, ablation study, in-depth analysis and case study. The\nresults show that RMDNet can learn discriminative representations for noisy\ndata, which results in superior performance than the existing methods.",
      "tldr_zh": "该论文针对带有噪声标签（noisy labels）的学习问题，提出了一种新的框架RMDNet，以提升模型的鲁棒性和表示学习能力。RMDNet包括两个关键模块：Relation Modeling (RM)模块使用对比学习（contrastive learning）来建模样本间关系，从而在自监督方式下减少噪声标签对特征提取的干扰；Relation-Guided Representation Learning (RGRL)模块则利用这些关系来校准噪声样本的表示分布，提高模型的泛化性能。该框架是可插拔的，并通过在两个数据集上的实验验证，展示了比现有方法更优的表现，尤其在学习区分性表示方面取得了显著提升。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19606v2",
      "published_date": "2024-05-30 01:47:27 UTC",
      "updated_date": "2024-06-02 01:59:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:23:09.000588"
    },
    {
      "arxiv_id": "2405.19600v2",
      "title": "Rethinking Spectral Augmentation for Contrast-based Graph Self-Supervised Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Xiangru Jian",
        "Xinjian Zhao",
        "Wei Pang",
        "Chaolong Ying",
        "Yimu Wang",
        "Yaoyao Xu",
        "Tianshu Yu"
      ],
      "abstract": "The recent surge in contrast-based graph self-supervised learning has\nprominently featured an intensified exploration of spectral cues. Spectral\naugmentation, which involves modifying a graph's spectral properties such as\neigenvalues or eigenvectors, is widely believed to enhance model performance.\nHowever, an intriguing paradox emerges, as methods grounded in seemingly\nconflicting assumptions regarding the spectral domain demonstrate notable\nenhancements in learning performance. Through extensive empirical studies, we\nfind that simple edge perturbations - random edge dropping for node-level and\nrandom edge adding for graph-level self-supervised learning - consistently\nyield comparable or superior performance while being significantly more\ncomputationally efficient. This suggests that the computational overhead of\nsophisticated spectral augmentations may not justify their practical benefits.\nOur theoretical analysis of the InfoNCE loss bounds for shallow GNNs further\nsupports this observation. The proposed insights represent a significant leap\nforward in the field, potentially refining the understanding and implementation\nof graph self-supervised learning.",
      "tldr_zh": "这篇论文重新审视了基于对比的图自监督学习（Contrast-based Graph Self-Supervised Learning）中谱增强（Spectral Augmentation）的作用，认为尽管谱增强被视为提升性能的关键，但不同假设下的方法可能存在矛盾。作者通过广泛实证研究发现，简单的边扰动（如节点级随机边删除和图级随机边添加）能实现相媲美或优于谱增强的性能，同时显著降低计算开销。理论分析进一步通过InfoNCE loss bounds for shallow GNNs支持这一观察，为图自监督学习的理解和实现提供了重要新见解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19600v2",
      "published_date": "2024-05-30 01:30:34 UTC",
      "updated_date": "2024-12-04 04:41:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:23:21.057055"
    },
    {
      "arxiv_id": "2405.19597v1",
      "title": "SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors",
      "title_zh": "翻译失败",
      "authors": [
        "Vijay Lingam",
        "Atula Tejaswi",
        "Aditya Vavre",
        "Aneesh Shetty",
        "Gautham Krishna Gudur",
        "Joydeep Ghosh",
        "Alex Dimakis",
        "Eunsol Choi",
        "Aleksandar Bojchevski",
        "Sujay Sanghavi"
      ],
      "abstract": "Popular parameter-efficient fine-tuning (PEFT) methods, such as LoRA and its\nvariants, freeze pre-trained model weights \\(W\\) and inject learnable matrices\n\\(\\Delta W\\). These \\(\\Delta W\\) matrices are structured for efficient\nparameterization, often using techniques like low-rank approximations or\nscaling vectors. However, these methods typically show a performance gap\ncompared to full fine-tuning. Although recent PEFT methods have narrowed this\ngap, they do so at the cost of additional learnable parameters. We propose\nSVFT, a simple approach that fundamentally differs from existing methods: the\nstructure imposed on \\(\\Delta W\\) depends on the specific weight matrix \\(W\\).\nSpecifically, SVFT updates \\(W\\) as a sparse combination of outer products of\nits singular vectors, training only the coefficients (scales) of these sparse\ncombinations. This approach allows fine-grained control over expressivity\nthrough the number of coefficients. Extensive experiments on language and\nvision benchmarks show that SVFT recovers up to 96% of full fine-tuning\nperformance while training only 0.006 to 0.25% of parameters, outperforming\nexisting methods that only recover up to 85% performance using 0.03 to 0.8% of\nthe trainable parameter budget.",
      "tldr_zh": "该论文提出了一种新的参数高效微调（PEFT）方法 SVFT，利用预训练权重矩阵 \\( W \\) 的奇异向量进行更新，通过训练这些向量的稀疏组合系数来实现微调，从而提供更精细的表达控制。不同于现有方法如 LoRA 和其变体，SVFT 避免了额外参数的增加，而是直接基于 \\( W \\) 的结构优化 \\( \\Delta W \\)。实验结果显示，在语言和视觉基准测试中，SVFT 仅需训练 0.006% 到 0.25% 的参数，就能恢复高达 96% 的全微调性能，显著优于现有方法（最高仅恢复 85% 的性能）。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages, 5 figures, 14 tables",
      "pdf_url": "http://arxiv.org/pdf/2405.19597v1",
      "published_date": "2024-05-30 01:27:43 UTC",
      "updated_date": "2024-05-30 01:27:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:23:33.892972"
    },
    {
      "arxiv_id": "2405.19592v1",
      "title": "Why Larger Language Models Do In-context Learning Differently?",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenmei Shi",
        "Junyi Wei",
        "Zhuoyan Xu",
        "Yingyu Liang"
      ],
      "abstract": "Large language models (LLM) have emerged as a powerful tool for AI, with the\nkey ability of in-context learning (ICL), where they can perform well on unseen\ntasks based on a brief series of task examples without necessitating any\nadjustments to the model parameters. One recent interesting mysterious\nobservation is that models of different scales may have different ICL\nbehaviors: larger models tend to be more sensitive to noise in the test\ncontext. This work studies this observation theoretically aiming to improve the\nunderstanding of LLM and ICL. We analyze two stylized settings: (1) linear\nregression with one-layer single-head linear transformers and (2) parity\nclassification with two-layer multiple attention heads transformers (non-linear\ndata and non-linear model). In both settings, we give closed-form optimal\nsolutions and find that smaller models emphasize important hidden features\nwhile larger ones cover more hidden features; thus, smaller models are more\nrobust to noise while larger ones are more easily distracted, leading to\ndifferent ICL behaviors. This sheds light on where transformers pay attention\nto and how that affects ICL. Preliminary experimental results on large base and\nchat models provide positive support for our analysis.",
      "tldr_zh": "这篇论文探讨了大型语言模型 (LLM) 在 in-context learning (ICL) 中的行为差异，特别是更大模型对测试上下文噪声更敏感的现象。作者通过理论分析两个简化设置——单层单头线性 transformers 的线性回归和两层多头 transformers 的奇偶分类——给出了闭式解，发现较小模型更注重重要隐藏特征，从而对噪声更鲁棒，而较大模型则覆盖更多特征导致易受干扰。初步实验结果支持这一分析，为理解 transformers 的注意力机制和 ICL 行为提供了新见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19592v1",
      "published_date": "2024-05-30 01:11:35 UTC",
      "updated_date": "2024-05-30 01:11:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:23:45.377760"
    },
    {
      "arxiv_id": "2405.19581v2",
      "title": "Source Code Foundation Models are Transferable Binary Analysis Knowledge Bases",
      "title_zh": "源代码基础模型是可转移的二进制分析知识库",
      "authors": [
        "Zian Su",
        "Xiangzhe Xu",
        "Ziyang Huang",
        "Kaiyuan Zhang",
        "Xiangyu Zhang"
      ],
      "abstract": "Human-Oriented Binary Reverse Engineering (HOBRE) lies at the intersection of\nbinary and source code, aiming to lift binary code to human-readable content\nrelevant to source code, thereby bridging the binary-source semantic gap.\nRecent advancements in uni-modal code model pre-training, particularly in\ngenerative Source Code Foundation Models (SCFMs) and binary understanding\nmodels, have laid the groundwork for transfer learning applicable to HOBRE.\nHowever, existing approaches for HOBRE rely heavily on uni-modal models like\nSCFMs for supervised fine-tuning or general LLMs for prompting, resulting in\nsub-optimal performance. Inspired by recent progress in large multi-modal\nmodels, we propose that it is possible to harness the strengths of uni-modal\ncode models from both sides to bridge the semantic gap effectively. In this\npaper, we introduce a novel probe-and-recover framework that incorporates a\nbinary-source encoder-decoder model and black-box LLMs for binary analysis. Our\napproach leverages the pre-trained knowledge within SCFMs to synthesize\nrelevant, symbol-rich code fragments as context. This additional context\nenables black-box LLMs to enhance recovery accuracy. We demonstrate significant\nimprovements in zero-shot binary summarization and binary function name\nrecovery, with a 10.3% relative gain in CHRF and a 16.7% relative gain in a\nGPT4-based metric for summarization, as well as a 6.7% and 7.4% absolute\nincrease in token-level precision and recall for name recovery, respectively.\nThese results highlight the effectiveness of our approach in automating and\nimproving binary code analysis.",
      "tldr_zh": "该论文探讨了 Human-Oriented Binary Reverse Engineering (HOBRE)，旨在桥接二进制代码与源代码的语义差距，并证明 Source Code Foundation Models (SCFMs) 可以作为可转移的二进制分析知识库。研究提出一个名为 probe-and-recover 的框架，该框架结合二进制-源代码编码器-解码器模型和黑箱 LLMs，通过利用 SCFMs 合成符号丰富的代码片段作为上下文，来提升二进制分析的准确性。在实验中，该方法在零样本二进制总结上实现了 CHRF 指标相对提高 10.3% 和 GPT4-based 指标相对提高 16.7%，而在函数名恢复上，精确度和召回率分别绝对提高了 6.7% 和 7.4%。这些结果突显了该框架在自动化二进制代码分析方面的有效性。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.19581v2",
      "published_date": "2024-05-30 00:17:44 UTC",
      "updated_date": "2024-10-30 16:12:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T14:23:59.417825"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 156,
  "processed_papers_count": 156,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-18T14:24:24.835252"
}
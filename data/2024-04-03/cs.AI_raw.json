[
  {
    "arxiv_id": "2404.04285v1",
    "title": "MIMIR: A Streamlined Platform for Personalized Agent Tuning in Domain Expertise",
    "authors": [
      "Chunyuan Deng",
      "Xiangru Tang",
      "Yilun Zhao",
      "Hanming Wang",
      "Haoran Wang",
      "Wangchunshu Zhou",
      "Arman Cohan",
      "Mark Gerstein"
    ],
    "abstract": "Recently, large language models (LLMs) have evolved into interactive agents,\nproficient in planning, tool use, and task execution across a wide variety of\ntasks. However, without specific agent tuning, open-source models like LLaMA\ncurrently struggle to match the efficiency of GPT- 4, particularly given the\nscarcity of agent-tuning datasets for fine-tuning. In response, we introduce\n\\textsc{Mimir}: a streamlined platform offering a customizable pipeline that\nenables users to leverage both private knowledge and publicly available,\nlegally compliant datasets at scale for \\textbf{personalized agent tuning}.\nAdditionally, \\textsc{Mimir} supports the generation of general\ninstruction-tuning datasets from the same input. This dual capability ensures\nthat language agents developed through the platform possess both specific agent\nabilities and general competencies. \\textsc{Mimir} integrates these features\ninto a cohesive end-to-end platform, facilitating everything from the uploading\nof personalized files to one-click agent fine-tuning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.04285v1",
    "published_date": "2024-04-03 23:42:38 UTC",
    "updated_date": "2024-04-03 23:42:38 UTC"
  },
  {
    "arxiv_id": "2404.03114v1",
    "title": "Testing the Effect of Code Documentation on Large Language Model Code Understanding",
    "authors": [
      "William Macke",
      "Michael Doyle"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated impressive abilities in recent\nyears with regards to code generation and understanding. However, little work\nhas investigated how documentation and other code properties affect an LLM's\nability to understand and generate code or documentation. We present an\nempirical analysis of how underlying properties of code or documentation can\naffect an LLM's capabilities. We show that providing an LLM with \"incorrect\"\ndocumentation can greatly hinder code understanding, while incomplete or\nmissing documentation does not seem to significantly affect an LLM's ability to\nunderstand code.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "7 pages, 5 figures, 2 tables. Accepted as a Findings paper in the\n  \"Generation\" track to NAACL 2024. MITRE Public Release Case Number 23-4132",
    "pdf_url": "http://arxiv.org/pdf/2404.03114v1",
    "published_date": "2024-04-03 23:33:56 UTC",
    "updated_date": "2024-04-03 23:33:56 UTC"
  },
  {
    "arxiv_id": "2404.03099v1",
    "title": "Composite Bayesian Optimization In Function Spaces Using NEON -- Neural Epistemic Operator Networks",
    "authors": [
      "Leonardo Ferreira Guilhoto",
      "Paris Perdikaris"
    ],
    "abstract": "Operator learning is a rising field of scientific computing where inputs or\noutputs of a machine learning model are functions defined in\ninfinite-dimensional spaces. In this paper, we introduce NEON (Neural Epistemic\nOperator Networks), an architecture for generating predictions with uncertainty\nusing a single operator network backbone, which presents orders of magnitude\nless trainable parameters than deep ensembles of comparable performance. We\nshowcase the utility of this method for sequential decision-making by examining\nthe problem of composite Bayesian Optimization (BO), where we aim to optimize a\nfunction $f=g\\circ h$, where $h:X\\to C(\\mathcal{Y},\\mathbb{R}^{d_s})$ is an\nunknown map which outputs elements of a function space, and $g:\nC(\\mathcal{Y},\\mathbb{R}^{d_s})\\to \\mathbb{R}$ is a known and cheap-to-compute\nfunctional. By comparing our approach to other state-of-the-art methods on toy\nand real world scenarios, we demonstrate that NEON achieves state-of-the-art\nperformance while requiring orders of magnitude less trainable parameters.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "cs.IT",
      "math.IT",
      "stat.ML",
      "68T37",
      "J.2; I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.03099v1",
    "published_date": "2024-04-03 22:42:37 UTC",
    "updated_date": "2024-04-03 22:42:37 UTC"
  },
  {
    "arxiv_id": "2404.03098v1",
    "title": "Exploring the Trade-off Between Model Performance and Explanation Plausibility of Text Classifiers Using Human Rationales",
    "authors": [
      "Lucas E. Resck",
      "Marcos M. Raimundo",
      "Jorge Poco"
    ],
    "abstract": "Saliency post-hoc explainability methods are important tools for\nunderstanding increasingly complex NLP models. While these methods can reflect\nthe model's reasoning, they may not align with human intuition, making the\nexplanations not plausible. In this work, we present a methodology for\nincorporating rationales, which are text annotations explaining human\ndecisions, into text classification models. This incorporation enhances the\nplausibility of post-hoc explanations while preserving their faithfulness. Our\napproach is agnostic to model architectures and explainability methods. We\nintroduce the rationales during model training by augmenting the standard\ncross-entropy loss with a novel loss function inspired by contrastive learning.\nBy leveraging a multi-objective optimization algorithm, we explore the\ntrade-off between the two loss functions and generate a Pareto-optimal frontier\nof models that balance performance and plausibility. Through extensive\nexperiments involving diverse models, datasets, and explainability methods, we\ndemonstrate that our approach significantly enhances the quality of model\nexplanations without causing substantial (sometimes negligible) degradation in\nthe original model's performance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "27 pages, 22 figures, 8 tables; to appear in NAACL Findings 2024;\n  code and data available at\n  https://github.com/visual-ds/plausible-nlp-explanations",
    "pdf_url": "http://arxiv.org/pdf/2404.03098v1",
    "published_date": "2024-04-03 22:39:33 UTC",
    "updated_date": "2024-04-03 22:39:33 UTC"
  },
  {
    "arxiv_id": "2404.08562v1",
    "title": "Dynamic Neural Control Flow Execution: An Agent-Based Deep Equilibrium Approach for Binary Vulnerability Detection",
    "authors": [
      "Litao Li",
      "Steven H. H. Ding",
      "Andrew Walenstein",
      "Philippe Charland",
      "Benjamin C. M. Fung"
    ],
    "abstract": "Software vulnerabilities are a challenge in cybersecurity. Manual security\npatches are often difficult and slow to be deployed, while new vulnerabilities\nare created. Binary code vulnerability detection is less studied and more\ncomplex compared to source code, and this has important practical implications.\nDeep learning has become an efficient and powerful tool in the security domain,\nwhere it provides end-to-end and accurate prediction. Modern deep learning\napproaches learn the program semantics through sequence and graph neural\nnetworks, using various intermediate representation of programs, such as\nabstract syntax trees (AST) or control flow graphs (CFG). Due to the complex\nnature of program execution, the output of an execution depends on the many\nprogram states and inputs. Also, a CFG generated from static analysis can be an\noverestimation of the true program flow. Moreover, the size of programs often\ndoes not allow a graph neural network with fixed layers to aggregate global\ninformation. To address these issues, we propose DeepEXE, an agent-based\nimplicit neural network that mimics the execution path of a program. We use\nreinforcement learning to enhance the branching decision at every program state\ntransition and create a dynamic environment to learn the dependency between a\nvulnerability and certain program states. An implicitly defined neural network\nenables nearly infinite state transitions until convergence, which captures the\nstructural information at a higher level. The experiments are conducted on two\nsemi-synthetic and two real-world datasets. We show that DeepEXE is an accurate\nand efficient method and outperforms the state-of-the-art vulnerability\ndetection methods.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.08562v1",
    "published_date": "2024-04-03 22:07:50 UTC",
    "updated_date": "2024-04-03 22:07:50 UTC"
  },
  {
    "arxiv_id": "2404.03088v2",
    "title": "Robust Federated Learning for Wireless Networks: A Demonstration with Channel Estimation",
    "authors": [
      "Zexin Fang",
      "Bin Han",
      "Hans D. Schotten"
    ],
    "abstract": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to IEEE GLOBECOM 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.03088v2",
    "published_date": "2024-04-03 22:03:28 UTC",
    "updated_date": "2024-07-30 08:19:53 UTC"
  },
  {
    "arxiv_id": "2404.03085v1",
    "title": "Talaria: Interactively Optimizing Machine Learning Models for Efficient Inference",
    "authors": [
      "Fred Hohman",
      "Chaoqun Wang",
      "Jinmook Lee",
      "Jochen Görtler",
      "Dominik Moritz",
      "Jeffrey P Bigham",
      "Zhile Ren",
      "Cecile Foret",
      "Qi Shan",
      "Xiaoyi Zhang"
    ],
    "abstract": "On-device machine learning (ML) moves computation from the cloud to personal\ndevices, protecting user privacy and enabling intelligent user experiences.\nHowever, fitting models on devices with limited resources presents a major\ntechnical challenge: practitioners need to optimize models and balance hardware\nmetrics such as model size, latency, and power. To help practitioners create\nefficient ML models, we designed and developed Talaria: a model visualization\nand optimization system. Talaria enables practitioners to compile models to\nhardware, interactively visualize model statistics, and simulate optimizations\nto test the impact on inference metrics. Since its internal deployment two\nyears ago, we have evaluated Talaria using three methodologies: (1) a log\nanalysis highlighting its growth of 800+ practitioners submitting 3,600+\nmodels; (2) a usability survey with 26 users assessing the utility of 20\nTalaria features; and (3) a qualitative interview with the 7 most active users\nabout their experience using Talaria.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "Proceedings of the 2024 ACM CHI Conference on Human Factors in\n  Computing Systems",
    "pdf_url": "http://arxiv.org/pdf/2404.03085v1",
    "published_date": "2024-04-03 21:55:44 UTC",
    "updated_date": "2024-04-03 21:55:44 UTC"
  },
  {
    "arxiv_id": "2404.03084v2",
    "title": "Rethinking Teacher-Student Curriculum Learning through the Cooperative Mechanics of Experience",
    "authors": [
      "Manfred Diaz",
      "Liam Paull",
      "Andrea Tacchetti"
    ],
    "abstract": "Teacher-Student Curriculum Learning (TSCL) is a curriculum learning framework\nthat draws inspiration from human cultural transmission and learning. It\ninvolves a teacher algorithm shaping the learning process of a learner\nalgorithm by exposing it to controlled experiences. Despite its success,\nunderstanding the conditions under which TSCL is effective remains challenging.\nIn this paper, we propose a data-centric perspective to analyze the underlying\nmechanics of the teacher-student interactions in TSCL. We leverage cooperative\ngame theory to describe how the composition of the set of experiences presented\nby the teacher to the learner, as well as their order, influences the\nperformance of the curriculum that is found by TSCL approaches. To do so, we\ndemonstrate that for every TSCL problem, an equivalent cooperative game exists,\nand several key components of the TSCL framework can be reinterpreted using\ngame-theoretic principles. Through experiments covering supervised learning,\nreinforcement learning, and classical games, we estimate the cooperative values\nof experiences and use value-proportional curriculum mechanisms to construct\ncurricula, even in cases where TSCL struggles. The framework and experimental\nsetup we present in this work represents a novel foundation for a deeper\nexploration of TSCL, shedding light on its underlying mechanisms and providing\ninsights into its broader applicability in machine learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at TMLR (https://openreview.net/forum?id=qWh82br6KT)",
    "pdf_url": "http://arxiv.org/pdf/2404.03084v2",
    "published_date": "2024-04-03 21:55:17 UTC",
    "updated_date": "2024-09-12 16:56:59 UTC"
  },
  {
    "arxiv_id": "2404.16042v2",
    "title": "How explainable AI affects human performance: A systematic review of the behavioural consequences of saliency maps",
    "authors": [
      "Romy Müller"
    ],
    "abstract": "Saliency maps can explain how deep neural networks classify images. But are\nthey actually useful for humans? The present systematic review of 68 user\nstudies found that while saliency maps can enhance human performance, null\neffects or even costs are quite common. To investigate what modulates these\neffects, the empirical outcomes were organised along several factors related to\nthe human tasks, AI performance, XAI methods, images to be classified, human\nparticipants and comparison conditions. In image-focused tasks, benefits were\nless common than in AI-focused tasks, but the effects depended on the specific\ncognitive requirements. Moreover, benefits were usually restricted to incorrect\nAI predictions in AI-focused tasks but to correct ones in image-focused tasks.\nXAI-related factors had surprisingly little impact. The evidence was limited\nfor image- and human-related factors and the effects were highly dependent on\nthe comparison conditions. These findings may support the design of future user\nstudies.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16042v2",
    "published_date": "2024-04-03 21:46:25 UTC",
    "updated_date": "2024-04-26 04:25:12 UTC"
  },
  {
    "arxiv_id": "2404.03080v5",
    "title": "Construction and Application of Materials Knowledge Graph in Multidisciplinary Materials Science via Large Language Model",
    "authors": [
      "Yanpeng Ye",
      "Jie Ren",
      "Shaozhou Wang",
      "Yuwei Wan",
      "Imran Razzak",
      "Bram Hoex",
      "Haofen Wang",
      "Tong Xie",
      "Wenjie Zhang"
    ],
    "abstract": "Knowledge in materials science is widely dispersed across extensive\nscientific literature, posing significant challenges to the efficient discovery\nand integration of new materials. Traditional methods, often reliant on costly\nand time-consuming experimental approaches, further complicate rapid\ninnovation. Addressing these challenges, the integration of artificial\nintelligence with materials science has opened avenues for accelerating the\ndiscovery process, though it also demands precise annotation, data extraction,\nand traceability of information. To tackle these issues, this article\nintroduces the Materials Knowledge Graph (MKG), which utilizes advanced natural\nlanguage processing techniques integrated with large language models to extract\nand systematically organize a decade's worth of high-quality research into\nstructured triples, contains 162,605 nodes and 731,772 edges. MKG categorizes\ninformation into comprehensive labels such as Name, Formula, and Application,\nstructured around a meticulously designed ontology, thus enhancing data\nusability and integration. By implementing network-based algorithms, MKG not\nonly facilitates efficient link prediction but also significantly reduces\nreliance on traditional experimental methods. This structured approach not only\nstreamlines materials research but also lays the groundwork for more\nsophisticated science knowledge graphs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by 38th Conference on Neural Information Processing Systems\n  (NeurIPS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2404.03080v5",
    "published_date": "2024-04-03 21:46:14 UTC",
    "updated_date": "2025-05-15 02:03:46 UTC"
  },
  {
    "arxiv_id": "2404.03058v1",
    "title": "Automatic Extraction of Linguistic Description from Fuzzy Rule Base",
    "authors": [
      "Krzysztof Siminski",
      "Konrad Wnuk"
    ],
    "abstract": "Neuro-fuzzy systems are a technique of explainable artificial intelligence\n(XAI). They elaborate knowledge models as a set of fuzzy rules. Fuzzy sets are\ncrucial components of fuzzy rules. They are used to model linguistic terms. In\nthis paper, we present an automatic extraction of fuzzy rules in the natural\nEnglish language. Full implementation is available free from a public\nrepository.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.03058v1",
    "published_date": "2024-04-03 20:50:48 UTC",
    "updated_date": "2024-04-03 20:50:48 UTC"
  },
  {
    "arxiv_id": "2404.03054v2",
    "title": "Data-Driven Goal Recognition Design for General Behavioral Agents",
    "authors": [
      "Robert Kasumba",
      "Guanghui Yu",
      "Chien-Ju Ho",
      "Sarah Keren",
      "William Yeoh"
    ],
    "abstract": "Goal recognition design aims to make limited modifications to decision-making\nenvironments with the goal of making it easier to infer the goals of agents\nacting within those environments. Although various research efforts have been\nmade in goal recognition design, existing approaches are computationally\ndemanding and often assume that agents are (near-)optimal in their\ndecision-making. To address these limitations, we introduce a data-driven\napproach to goal recognition design that can account for agents with general\nbehavioral models. Following existing literature, we use worst-case\ndistinctiveness($\\textit{wcd}$) as a measure of the difficulty in inferring the\ngoal of an agent in a decision-making environment. Our approach begins by\ntraining a machine learning model to predict the $\\textit{wcd}$ for a given\nenvironment and the agent behavior model. We then propose a gradient-based\noptimization framework that accommodates various constraints to optimize\ndecision-making environments for enhanced goal recognition. Through extensive\nsimulations, we demonstrate that our approach outperforms existing methods in\nreducing $\\textit{wcd}$ and enhancing runtime efficiency in conventional setup.\nMoreover, our approach also adapts to settings in which existing approaches do\nnot apply, such as those involving flexible budget constraints, more complex\nenvironments, and suboptimal agent behavior. Finally, we have conducted\nhuman-subject experiments which confirm that our method can create environments\nthat facilitate efficient goal recognition from real-world human\ndecision-makers.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.03054v2",
    "published_date": "2024-04-03 20:38:22 UTC",
    "updated_date": "2024-06-11 20:45:56 UTC"
  },
  {
    "arxiv_id": "2404.03044v1",
    "title": "The Artificial Intelligence Ontology: LLM-assisted construction of AI concept hierarchies",
    "authors": [
      "Marcin P. Joachimiak",
      "Mark A. Miller",
      "J. Harry Caufield",
      "Ryan Ly",
      "Nomi L. Harris",
      "Andrew Tritt",
      "Christopher J. Mungall",
      "Kristofer E. Bouchard"
    ],
    "abstract": "The Artificial Intelligence Ontology (AIO) is a systematization of artificial\nintelligence (AI) concepts, methodologies, and their interrelations. Developed\nvia manual curation, with the additional assistance of large language models\n(LLMs), AIO aims to address the rapidly evolving landscape of AI by providing a\ncomprehensive framework that encompasses both technical and ethical aspects of\nAI technologies. The primary audience for AIO includes AI researchers,\ndevelopers, and educators seeking standardized terminology and concepts within\nthe AI domain. The ontology is structured around six top-level branches:\nNetworks, Layers, Functions, LLMs, Preprocessing, and Bias, each designed to\nsupport the modular composition of AI methods and facilitate a deeper\nunderstanding of deep learning architectures and ethical considerations in AI.\n  AIO's development utilized the Ontology Development Kit (ODK) for its\ncreation and maintenance, with its content being dynamically updated through\nAI-driven curation support. This approach not only ensures the ontology's\nrelevance amidst the fast-paced advancements in AI but also significantly\nenhances its utility for researchers, developers, and educators by simplifying\nthe integration of new AI concepts and methodologies.\n  The ontology's utility is demonstrated through the annotation of AI methods\ndata in a catalog of AI research publications and the integration into the\nBioPortal ontology resource, highlighting its potential for cross-disciplinary\nresearch. The AIO ontology is open source and is available on GitHub\n(https://github.com/berkeleybop/artificial-intelligence-ontology) and BioPortal\n(https://bioportal.bioontology.org/ontologies/AIO).",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.03044v1",
    "published_date": "2024-04-03 20:08:15 UTC",
    "updated_date": "2024-04-03 20:08:15 UTC"
  },
  {
    "arxiv_id": "2404.03037v3",
    "title": "Model-based Reinforcement Learning for Parameterized Action Spaces",
    "authors": [
      "Renhao Zhang",
      "Haotian Fu",
      "Yilin Miao",
      "George Konidaris"
    ],
    "abstract": "We propose a novel model-based reinforcement learning algorithm -- Dynamics\nLearning and predictive control with Parameterized Actions (DLPA) -- for\nParameterized Action Markov Decision Processes (PAMDPs). The agent learns a\nparameterized-action-conditioned dynamics model and plans with a modified Model\nPredictive Path Integral control. We theoretically quantify the difference\nbetween the generated trajectory and the optimal trajectory during planning in\nterms of the value they achieved through the lens of Lipschitz Continuity. Our\nempirical results on several standard benchmarks show that our algorithm\nachieves superior sample efficiency and asymptotic performance than\nstate-of-the-art PAMDP methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.03037v3",
    "published_date": "2024-04-03 19:48:13 UTC",
    "updated_date": "2024-05-24 02:15:42 UTC"
  },
  {
    "arxiv_id": "2406.11855v1",
    "title": "Law and the Emerging Political Economy of Algorithmic Audits",
    "authors": [
      "Petros Terzis",
      "Michael Veale",
      "Noëlle Gaumann"
    ],
    "abstract": "For almost a decade now, scholarship in and beyond the ACM FAccT community\nhas been focusing on novel and innovative ways and methodologies to audit the\nfunctioning of algorithmic systems. Over the years, this research idea and\ntechnical project has matured enough to become a regulatory mandate. Today, the\nDigital Services Act (DSA) and the Online Safety Act (OSA) have established the\nframework within which technology corporations and (traditional) auditors will\ndevelop the `practice' of algorithmic auditing thereby presaging how this\n`ecosystem' will develop. In this paper, we systematically review the auditing\nprovisions in the DSA and the OSA in light of observations from the emerging\nindustry of algorithmic auditing. Who is likely to occupy this space? What are\nsome political and ethical tensions that are likely to arise? How are the\nmandates of `independent auditing' or `the evaluation of the societal context\nof an algorithmic function' likely to play out in practice? By shaping the\npicture of the emerging political economy of algorithmic auditing, we draw\nattention to strategies and cultures of traditional auditors that risk eroding\nimportant regulatory pillars of the DSA and the OSA. Importantly, we warn that\nambitious research ideas and technical projects of/for algorithmic auditing may\nend up crashed by the standardising grip of traditional auditors and/or diluted\nwithin a complex web of (sub-)contractual arrangements, diverse portfolios, and\ntight timelines.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "14 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.11855v1",
    "published_date": "2024-04-03 19:45:30 UTC",
    "updated_date": "2024-04-03 19:45:30 UTC"
  },
  {
    "arxiv_id": "2404.03027v4",
    "title": "JailBreakV: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks",
    "authors": [
      "Weidi Luo",
      "Siyuan Ma",
      "Xiaogeng Liu",
      "Xiaoyu Guo",
      "Chaowei Xiao"
    ],
    "abstract": "With the rapid advancements in Multimodal Large Language Models (MLLMs),\nsecuring these models against malicious inputs while aligning them with human\nvalues has emerged as a critical challenge. In this paper, we investigate an\nimportant and unexplored question of whether techniques that successfully\njailbreak Large Language Models (LLMs) can be equally effective in jailbreaking\nMLLMs. To explore this issue, we introduce JailBreakV-28K, a pioneering\nbenchmark designed to assess the transferability of LLM jailbreak techniques to\nMLLMs, thereby evaluating the robustness of MLLMs against diverse jailbreak\nattacks. Utilizing a dataset of 2, 000 malicious queries that is also proposed\nin this paper, we generate 20, 000 text-based jailbreak prompts using advanced\njailbreak attacks on LLMs, alongside 8, 000 image-based jailbreak inputs from\nrecent MLLMs jailbreak attacks, our comprehensive dataset includes 28, 000 test\ncases across a spectrum of adversarial scenarios. Our evaluation of 10\nopen-source MLLMs reveals a notably high Attack Success Rate (ASR) for attacks\ntransferred from LLMs, highlighting a critical vulnerability in MLLMs that\nstems from their text-processing capabilities. Our findings underscore the\nurgent need for future research to address alignment vulnerabilities in MLLMs\nfrom both textual and visual inputs.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.03027v4",
    "published_date": "2024-04-03 19:23:18 UTC",
    "updated_date": "2024-11-24 06:22:37 UTC"
  },
  {
    "arxiv_id": "2404.03023v1",
    "title": "Toward Safe Evolution of Artificial Intelligence (AI) based Conversational Agents to Support Adolescent Mental and Sexual Health Knowledge Discovery",
    "authors": [
      "Jinkyung Park",
      "Vivek Singh",
      "Pamela Wisniewski"
    ],
    "abstract": "Following the recent release of various Artificial Intelligence (AI) based\nConversation Agents (CAs), adolescents are increasingly using CAs for\ninteractive knowledge discovery on sensitive topics, including mental and\nsexual health topics. Exploring such sensitive topics through online search has\nbeen an essential part of adolescent development, and CAs can support their\nknowledge discovery on such topics through human-like dialogues. Yet,\nunintended risks have been documented with adolescents' interactions with\nAI-based CAs, such as being exposed to inappropriate content, false\ninformation, and/or being given advice that is detrimental to their mental and\nphysical well-being (e.g., to self-harm). In this position paper, we discuss\nthe current landscape and opportunities for CAs to support adolescents' mental\nand sexual health knowledge discovery. We also discuss some of the challenges\nrelated to ensuring the safety of adolescents when interacting with CAs\nregarding sexual and mental health topics. We call for a discourse on how to\nset guardrails for the safe evolution of AI-based CAs for adolescents.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "This paper has been peer-reviewed and presented at the \"CHI 2024\n  Workshop on Child-centred AI Design, May 11, 2024, Honolulu, HI, USA.\"",
    "pdf_url": "http://arxiv.org/pdf/2404.03023v1",
    "published_date": "2024-04-03 19:18:25 UTC",
    "updated_date": "2024-04-03 19:18:25 UTC"
  },
  {
    "arxiv_id": "2404.03021v2",
    "title": "Blessing or curse? A survey on the Impact of Generative AI on Fake News",
    "authors": [
      "Alexander Loth",
      "Martin Kappes",
      "Marc-Oliver Pahl"
    ],
    "abstract": "Fake news significantly influence our society. They impact consumers, voters,\nand many other societal groups. While Fake News exist for a centuries,\nGenerative AI brings fake news on a new level. It is now possible to automate\nthe creation of masses of high-quality individually targeted Fake News. On the\nother end, Generative AI can also help detecting Fake News. Both fields are\nyoung but developing fast.\n  This survey provides a comprehensive examination of the research and\npractical use of Generative AI for Fake News detection and creation in 2024.\nFollowing the Structured Literature Survey approach, the paper synthesizes\ncurrent results in the following topic clusters 1) enabling technologies, 2)\ncreation of Fake News, 3) case study social media as most relevant distribution\nchannel, 4) detection of Fake News, and 5) deepfakes as upcoming technology.\n  The article also identifies current challenges and open issues.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 2 figures. Submitted to ACM Transactions on Intelligent\n  Systems and Technology (ACM TIST). Added references",
    "pdf_url": "http://arxiv.org/pdf/2404.03021v2",
    "published_date": "2024-04-03 19:14:45 UTC",
    "updated_date": "2024-12-27 11:31:01 UTC"
  },
  {
    "arxiv_id": "2404.03011v1",
    "title": "Transfer learning applications for anomaly detection in wind turbines",
    "authors": [
      "Cyriana M. A. Roelofs",
      "Christian Gück",
      "Stefan Faulstich"
    ],
    "abstract": "Anomaly detection in wind turbines typically involves using normal behaviour\nmodels to detect faults early. However, training autoencoder models for each\nturbine is time-consuming and resource intensive. Thus, transfer learning\nbecomes essential for wind turbines with limited data or applications with\nlimited computational resources. This study examines how cross-turbine transfer\nlearning can be applied to autoencoder-based anomaly detection. Here,\nautoencoders are combined with constant thresholds for the reconstruction error\nto determine if input data contains an anomaly. The models are initially\ntrained on one year's worth of data from one or more source wind turbines. They\nare then fine-tuned using smaller amounts of data from another turbine. Three\nmethods for fine-tuning are investigated: adjusting the entire autoencoder,\nonly the decoder, or only the threshold of the model. The performance of the\ntransfer learning models is compared to baseline models that were trained on\none year's worth of data from the target wind turbine. The results of the tests\nconducted in this study indicate that models trained on data of multiple wind\nturbines do not improve the anomaly detection capability compared to models\ntrained on data of one source wind turbine. In addition, modifying the model's\nthreshold can lead to comparable or even superior performance compared to the\nbaseline, whereas fine-tuning the decoder or autoencoder further enhances the\nmodels' performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 7 figures, preprint submitted to Energy&AI",
    "pdf_url": "http://arxiv.org/pdf/2404.03011v1",
    "published_date": "2024-04-03 18:48:45 UTC",
    "updated_date": "2024-04-03 18:48:45 UTC"
  },
  {
    "arxiv_id": "2404.02990v1",
    "title": "ASAP: Interpretable Analysis and Summarization of AI-generated Image Patterns at Scale",
    "authors": [
      "Jinbin Huang",
      "Chen Chen",
      "Aditi Mishra",
      "Bum Chul Kwon",
      "Zhicheng Liu",
      "Chris Bryan"
    ],
    "abstract": "Generative image models have emerged as a promising technology to produce\nrealistic images. Despite potential benefits, concerns grow about its misuse,\nparticularly in generating deceptive images that could raise significant\nethical, legal, and societal issues. Consequently, there is growing demand to\nempower users to effectively discern and comprehend patterns of AI-generated\nimages. To this end, we developed ASAP, an interactive visualization system\nthat automatically extracts distinct patterns of AI-generated images and allows\nusers to interactively explore them via various views. To uncover fake\npatterns, ASAP introduces a novel image encoder, adapted from CLIP, which\ntransforms images into compact \"distilled\" representations, enriched with\ninformation for differentiating authentic and fake images. These\nrepresentations generate gradients that propagate back to the attention maps of\nCLIP's transformer block. This process quantifies the relative importance of\neach pixel to image authenticity or fakeness, exposing key deceptive patterns.\nASAP enables the at scale interactive analysis of these patterns through\nmultiple, coordinated visualizations. This includes a representation overview\nwith innovative cell glyphs to aid in the exploration and qualitative\nevaluation of fake patterns across a vast array of images, as well as a pattern\nview that displays authenticity-indicating patterns in images and quantifies\ntheir impact. ASAP supports the analysis of cutting-edge generative models with\nthe latest architectures, including GAN-based models like proGAN and diffusion\nmodels like the latent diffusion model. We demonstrate ASAP's usefulness\nthrough two usage scenarios using multiple fake image detection benchmark\ndatasets, revealing its ability to identify and understand hidden patterns in\nAI-generated images, especially in detecting fake human faces produced by\ndiffusion-based techniques.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.02990v1",
    "published_date": "2024-04-03 18:20:41 UTC",
    "updated_date": "2024-04-03 18:20:41 UTC"
  },
  {
    "arxiv_id": "2404.02954v2",
    "title": "Deep Generative Models through the Lens of the Manifold Hypothesis: A Survey and New Connections",
    "authors": [
      "Gabriel Loaiza-Ganem",
      "Brendan Leigh Ross",
      "Rasa Hosseinzadeh",
      "Anthony L. Caterini",
      "Jesse C. Cresswell"
    ],
    "abstract": "In recent years there has been increased interest in understanding the\ninterplay between deep generative models (DGMs) and the manifold hypothesis.\nResearch in this area focuses on understanding the reasons why commonly-used\nDGMs succeed or fail at learning distributions supported on unknown\nlow-dimensional manifolds, as well as developing new models explicitly designed\nto account for manifold-supported data. This manifold lens provides both\nclarity as to why some DGMs (e.g. diffusion models and some generative\nadversarial networks) empirically surpass others (e.g. likelihood-based models\nsuch as variational autoencoders, normalizing flows, or energy-based models) at\nsample generation, and guidance for devising more performant DGMs. We carry out\nthe first survey of DGMs viewed through this lens, making two novel\ncontributions along the way. First, we formally establish that numerical\ninstability of likelihoods in high ambient dimensions is unavoidable when\nmodelling data with low intrinsic dimension. We then show that DGMs on learned\nrepresentations of autoencoders can be interpreted as approximately minimizing\nWasserstein distance: this result, which applies to latent diffusion models,\nhelps justify their outstanding empirical results. The manifold lens provides a\nrich perspective from which to understand DGMs, and we aim to make this\nperspective more accessible and widespread.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "TMLR 2024 (survey certification, expert certification)",
    "pdf_url": "http://arxiv.org/pdf/2404.02954v2",
    "published_date": "2024-04-03 18:00:00 UTC",
    "updated_date": "2024-09-25 18:00:00 UTC"
  },
  {
    "arxiv_id": "2404.02905v2",
    "title": "Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction",
    "authors": [
      "Keyu Tian",
      "Yi Jiang",
      "Zehuan Yuan",
      "Bingyue Peng",
      "Liwei Wang"
    ],
    "abstract": "We present Visual AutoRegressive modeling (VAR), a new generation paradigm\nthat redefines the autoregressive learning on images as coarse-to-fine\n\"next-scale prediction\" or \"next-resolution prediction\", diverging from the\nstandard raster-scan \"next-token prediction\". This simple, intuitive\nmethodology allows autoregressive (AR) transformers to learn visual\ndistributions fast and generalize well: VAR, for the first time, makes GPT-like\nAR models surpass diffusion transformers in image generation. On ImageNet\n256x256 benchmark, VAR significantly improve AR baseline by improving Frechet\ninception distance (FID) from 18.65 to 1.73, inception score (IS) from 80.4 to\n350.2, with around 20x faster inference speed. It is also empirically verified\nthat VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions\nincluding image quality, inference speed, data efficiency, and scalability.\nScaling up VAR models exhibits clear power-law scaling laws similar to those\nobserved in LLMs, with linear correlation coefficients near -0.998 as solid\nevidence. VAR further showcases zero-shot generalization ability in downstream\ntasks including image in-painting, out-painting, and editing. These results\nsuggest VAR has initially emulated the two important properties of LLMs:\nScaling Laws and zero-shot task generalization. We have released all models and\ncodes to promote the exploration of AR/VAR models for visual generation and\nunified learning.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Demo website: https://var.vision/",
    "pdf_url": "http://arxiv.org/pdf/2404.02905v2",
    "published_date": "2024-04-03 17:59:53 UTC",
    "updated_date": "2024-06-10 17:59:07 UTC"
  },
  {
    "arxiv_id": "2404.02904v1",
    "title": "ALOHa: A New Measure for Hallucination in Captioning Models",
    "authors": [
      "Suzanne Petryk",
      "David M. Chan",
      "Anish Kachinthaya",
      "Haodi Zou",
      "John Canny",
      "Joseph E. Gonzalez",
      "Trevor Darrell"
    ],
    "abstract": "Despite recent advances in multimodal pre-training for visual description,\nstate-of-the-art models still produce captions containing errors, such as\nhallucinating objects not present in a scene. The existing prominent metric for\nobject hallucination, CHAIR, is limited to a fixed set of MS COCO objects and\nsynonyms. In this work, we propose a modernized open-vocabulary metric, ALOHa,\nwhich leverages large language models (LLMs) to measure object hallucinations.\nSpecifically, we use an LLM to extract groundable objects from a candidate\ncaption, measure their semantic similarity to reference objects from captions\nand object detections, and use Hungarian matching to produce a final\nhallucination score. We show that ALOHa correctly identifies 13.6% more\nhallucinated objects than CHAIR on HAT, a new gold-standard subset of MS COCO\nCaptions annotated for hallucinations, and 30.8% more on nocaps, where objects\nextend beyond MS COCO categories. Our code is available at\nhttps://davidmchan.github.io/aloha/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "To appear at NAACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.02904v1",
    "published_date": "2024-04-03 17:59:36 UTC",
    "updated_date": "2024-04-03 17:59:36 UTC"
  },
  {
    "arxiv_id": "2404.02900v1",
    "title": "DeiT-LT Distillation Strikes Back for Vision Transformer Training on Long-Tailed Datasets",
    "authors": [
      "Harsh Rangwani",
      "Pradipto Mondal",
      "Mayank Mishra",
      "Ashish Ramayee Asokan",
      "R. Venkatesh Babu"
    ],
    "abstract": "Vision Transformer (ViT) has emerged as a prominent architecture for various\ncomputer vision tasks. In ViT, we divide the input image into patch tokens and\nprocess them through a stack of self attention blocks. However, unlike\nConvolutional Neural Networks (CNN), ViTs simple architecture has no\ninformative inductive bias (e.g., locality,etc. ). Due to this, ViT requires a\nlarge amount of data for pre-training. Various data efficient approaches (DeiT)\nhave been proposed to train ViT on balanced datasets effectively. However,\nlimited literature discusses the use of ViT for datasets with long-tailed\nimbalances. In this work, we introduce DeiT-LT to tackle the problem of\ntraining ViTs from scratch on long-tailed datasets. In DeiT-LT, we introduce an\nefficient and effective way of distillation from CNN via distillation DIST\ntoken by using out-of-distribution images and re-weighting the distillation\nloss to enhance focus on tail classes. This leads to the learning of local\nCNN-like features in early ViT blocks, improving generalization for tail\nclasses. Further, to mitigate overfitting, we propose distilling from a flat\nCNN teacher, which leads to learning low-rank generalizable features for DIST\ntokens across all ViT blocks. With the proposed DeiT-LT scheme, the\ndistillation DIST token becomes an expert on the tail classes, and the\nclassifier CLS token becomes an expert on the head classes. The experts help to\neffectively learn features corresponding to both the majority and minority\nclasses using a distinct set of tokens within the same ViT architecture. We\nshow the effectiveness of DeiT-LT for training ViT from scratch on datasets\nranging from small-scale CIFAR-10 LT to large-scale iNaturalist-2018.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024. Project Page: https://rangwani-harsh.github.io/DeiT-LT",
    "pdf_url": "http://arxiv.org/pdf/2404.02900v1",
    "published_date": "2024-04-03 17:58:21 UTC",
    "updated_date": "2024-04-03 17:58:21 UTC"
  },
  {
    "arxiv_id": "2404.02949v1",
    "title": "The SaTML '24 CNN Interpretability Competition: New Innovations for Concept-Level Interpretability",
    "authors": [
      "Stephen Casper",
      "Jieun Yun",
      "Joonhyuk Baek",
      "Yeseong Jung",
      "Minhwan Kim",
      "Kiwan Kwon",
      "Saerom Park",
      "Hayden Moore",
      "David Shriver",
      "Marissa Connor",
      "Keltin Grimes",
      "Angus Nicolson",
      "Arush Tagade",
      "Jessica Rumbelow",
      "Hieu Minh Nguyen",
      "Dylan Hadfield-Menell"
    ],
    "abstract": "Interpretability techniques are valuable for helping humans understand and\noversee AI systems. The SaTML 2024 CNN Interpretability Competition solicited\nnovel methods for studying convolutional neural networks (CNNs) at the ImageNet\nscale. The objective of the competition was to help human crowd-workers\nidentify trojans in CNNs. This report showcases the methods and results of four\nfeatured competition entries. It remains challenging to help humans reliably\ndiagnose trojans via interpretability tools. However, the competition's entries\nhave contributed new techniques and set a new record on the benchmark from\nCasper et al., 2023.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Competition for SaTML 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.02949v1",
    "published_date": "2024-04-03 17:56:28 UTC",
    "updated_date": "2024-04-03 17:56:28 UTC"
  },
  {
    "arxiv_id": "2404.02883v1",
    "title": "On the Scalability of Diffusion-based Text-to-Image Generation",
    "authors": [
      "Hao Li",
      "Yang Zou",
      "Ying Wang",
      "Orchid Majumder",
      "Yusheng Xie",
      "R. Manmatha",
      "Ashwin Swaminathan",
      "Zhuowen Tu",
      "Stefano Ermon",
      "Stefano Soatto"
    ],
    "abstract": "Scaling up model and data size has been quite successful for the evolution of\nLLMs. However, the scaling law for the diffusion based text-to-image (T2I)\nmodels is not fully explored. It is also unclear how to efficiently scale the\nmodel for better performance at reduced cost. The different training settings\nand expensive training cost make a fair model comparison extremely difficult.\nIn this work, we empirically study the scaling properties of diffusion based\nT2I models by performing extensive and rigours ablations on scaling both\ndenoising backbones and training set, including training scaled UNet and\nTransformer variants ranging from 0.4B to 4B parameters on datasets upto 600M\nimages. For model scaling, we find the location and amount of cross attention\ndistinguishes the performance of existing UNet designs. And increasing the\ntransformer blocks is more parameter-efficient for improving text-image\nalignment than increasing channel numbers. We then identify an efficient UNet\nvariant, which is 45% smaller and 28% faster than SDXL's UNet. On the data\nscaling side, we show the quality and diversity of the training set matters\nmore than simply dataset size. Increasing caption density and diversity\nimproves text-image alignment performance and the learning efficiency. Finally,\nwe provide scaling functions to predict the text-image alignment performance as\nfunctions of the scale of model size, compute and dataset size.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR2024",
    "pdf_url": "http://arxiv.org/pdf/2404.02883v1",
    "published_date": "2024-04-03 17:34:28 UTC",
    "updated_date": "2024-04-03 17:34:28 UTC"
  },
  {
    "arxiv_id": "2404.02877v4",
    "title": "FlightScope: An Experimental Comparative Review of Aircraft Detection Algorithms in Satellite Imagery",
    "authors": [
      "Safouane El Ghazouali",
      "Arnaud Gucciardi",
      "Francesca Venturini",
      "Nicola Venturi",
      "Michael Rueegsegger",
      "Umberto Michelucci"
    ],
    "abstract": "Object detection in remotely sensed satellite pictures is fundamental in many\nfields such as biophysical, and environmental monitoring. While deep learning\nalgorithms are constantly evolving, they have been mostly implemented and\ntested on popular ground-based taken photos. This paper critically evaluates\nand compares a suite of advanced object detection algorithms customized for the\ntask of identifying aircraft within satellite imagery. Using the large\nHRPlanesV2 dataset, together with a rigorous validation with the GDIT dataset,\nthis research encompasses an array of methodologies including YOLO versions 5\nand 8, Faster RCNN, CenterNet, RetinaNet, RTMDet, and DETR, all trained from\nscratch. This exhaustive training and validation study reveal YOLOv5 as the\npreeminent model for the specific case of identifying airplanes from remote\nsensing data, showcasing high precision and adaptability across diverse imaging\nconditions. This research highlight the nuanced performance landscapes of these\nalgorithms, with YOLOv5 emerging as a robust solution for aerial object\ndetection, underlining its importance through superior mean average precision,\nRecall, and Intersection over Union scores. The findings described here\nunderscore the fundamental role of algorithm selection aligned with the\nspecific demands of satellite imagery analysis and extend a comprehensive\nframework to evaluate model efficacy. The benchmark toolkit and codes,\navailable via https://github.com/toelt-llc/FlightScope_Bench, aims to further\nexploration and innovation in the realm of remote sensing object detection,\npaving the way for improved analytical methodologies in satellite imagery\napplications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2.1; I.2.6; I.2.10"
    ],
    "primary_category": "cs.CV",
    "comment": "16 figures, 5 tables, comprehensive survey, comparative study",
    "pdf_url": "http://arxiv.org/pdf/2404.02877v4",
    "published_date": "2024-04-03 17:24:27 UTC",
    "updated_date": "2024-12-17 20:58:44 UTC"
  },
  {
    "arxiv_id": "2404.02872v1",
    "title": "Integrating Explanations in Learning LTL Specifications from Demonstrations",
    "authors": [
      "Ashutosh Gupta",
      "John Komp",
      "Abhay Singh Rajput",
      "Krishna Shankaranarayanan",
      "Ashutosh Trivedi",
      "Namrita Varshney"
    ],
    "abstract": "This paper investigates whether recent advances in Large Language Models\n(LLMs) can assist in translating human explanations into a format that can\nrobustly support learning Linear Temporal Logic (LTL) from demonstrations. Both\nLLMs and optimization-based methods can extract LTL specifications from\ndemonstrations; however, they have distinct limitations. LLMs can quickly\ngenerate solutions and incorporate human explanations, but their lack of\nconsistency and reliability hampers their applicability in safety-critical\ndomains. On the other hand, optimization-based methods do provide formal\nguarantees but cannot process natural language explanations and face\nscalability challenges. We present a principled approach to combining LLMs and\noptimization-based methods to faithfully translate human explanations and\ndemonstrations into LTL specifications. We have implemented a tool called\nJanaka based on our approach. Our experiments demonstrate the effectiveness of\ncombining explanations with demonstrations in learning LTL specifications\nthrough several case studies.",
    "categories": [
      "cs.AI",
      "I.2.8"
    ],
    "primary_category": "cs.AI",
    "comment": "21 Pages, 13 Page Appendix",
    "pdf_url": "http://arxiv.org/pdf/2404.02872v1",
    "published_date": "2024-04-03 17:09:00 UTC",
    "updated_date": "2024-04-03 17:09:00 UTC"
  },
  {
    "arxiv_id": "2404.02869v1",
    "title": "Human Activity Recognition using Smartphones",
    "authors": [
      "Mayur Sonawane",
      "Sahil Rajesh Dhayalkar",
      "Siddesh Waje",
      "Soyal Markhelkar",
      "Akshay Wattamwar",
      "Seema C. Shrawne"
    ],
    "abstract": "Human Activity Recognition is a subject of great research today and has its\napplications in remote healthcare, activity tracking of the elderly or the\ndisables, calories burnt tracking etc. In our project, we have created an\nAndroid application that recognizes the daily human activities and calculate\nthe calories burnt in real time. We first captured labeled triaxial\nacceleration readings for different daily human activities from the\nsmartphone's embedded accelerometer. These readings were preprocessed using a\nmedian filter. 42 features were extracted using various methods. We then tested\nvarious machine learning algorithms along with dimensionality reduction.\nFinally, in our Android application, we used the machine learning algorithm and\na subset of features that provided maximum accuracy and minimum model building\ntime. This is used for real-time activity recognition and calculation of\ncalories burnt using a formula based on Metabolic Equivalent.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02869v1",
    "published_date": "2024-04-03 17:05:41 UTC",
    "updated_date": "2024-04-03 17:05:41 UTC"
  },
  {
    "arxiv_id": "2404.02838v1",
    "title": "I-Design: Personalized LLM Interior Designer",
    "authors": [
      "Ata Çelen",
      "Guo Han",
      "Konrad Schindler",
      "Luc Van Gool",
      "Iro Armeni",
      "Anton Obukhov",
      "Xi Wang"
    ],
    "abstract": "Interior design allows us to be who we are and live how we want - each design\nis as unique as our distinct personality. However, it is not trivial for\nnon-professionals to express and materialize this since it requires aligning\nfunctional and visual expectations with the constraints of physical space; this\nrenders interior design a luxury. To make it more accessible, we present\nI-Design, a personalized interior designer that allows users to generate and\nvisualize their design goals through natural language communication. I-Design\nstarts with a team of large language model agents that engage in dialogues and\nlogical reasoning with one another, transforming textual user input into\nfeasible scene graph designs with relative object relationships. Subsequently,\nan effective placement algorithm determines optimal locations for each object\nwithin the scene. The final design is then constructed in 3D by retrieving and\nintegrating assets from an existing object database. Additionally, we propose a\nnew evaluation protocol that utilizes a vision-language model and complements\nthe design pipeline. Extensive quantitative and qualitative experiments show\nthat I-Design outperforms existing methods in delivering high-quality 3D design\nsolutions and aligning with abstract concepts that match user input, showcasing\nits advantages across detailed 3D arrangement and conceptual fidelity.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02838v1",
    "published_date": "2024-04-03 16:17:53 UTC",
    "updated_date": "2024-04-03 16:17:53 UTC"
  },
  {
    "arxiv_id": "2404.02831v2",
    "title": "Empowering Biomedical Discovery with AI Agents",
    "authors": [
      "Shanghua Gao",
      "Ada Fang",
      "Yepeng Huang",
      "Valentina Giunchiglia",
      "Ayush Noori",
      "Jonathan Richard Schwarz",
      "Yasha Ektefaie",
      "Jovana Kondic",
      "Marinka Zitnik"
    ],
    "abstract": "We envision \"AI scientists\" as systems capable of skeptical learning and\nreasoning that empower biomedical research through collaborative agents that\nintegrate AI models and biomedical tools with experimental platforms. Rather\nthan taking humans out of the discovery process, biomedical AI agents combine\nhuman creativity and expertise with AI's ability to analyze large datasets,\nnavigate hypothesis spaces, and execute repetitive tasks. AI agents are poised\nto be proficient in various tasks, planning discovery workflows and performing\nself-assessment to identify and mitigate gaps in their knowledge. These agents\nuse large language models and generative models to feature structured memory\nfor continual learning and use machine learning tools to incorporate scientific\nknowledge, biological principles, and theories. AI agents can impact areas\nranging from virtual cell simulation, programmable control of phenotypes, and\nthe design of cellular circuits to developing new therapies.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02831v2",
    "published_date": "2024-04-03 16:08:01 UTC",
    "updated_date": "2024-07-24 20:31:52 UTC"
  },
  {
    "arxiv_id": "2404.02830v2",
    "title": "Enhancing Interpretability of Vertebrae Fracture Grading using Human-interpretable Prototypes",
    "authors": [
      "Poulami Sinhamahapatra",
      "Suprosanna Shit",
      "Anjany Sekuboyina",
      "Malek Husseini",
      "David Schinz",
      "Nicolas Lenhart",
      "Joern Menze",
      "Jan Kirschke",
      "Karsten Roscher",
      "Stephan Guennemann"
    ],
    "abstract": "Vertebral fracture grading classifies the severity of vertebral fractures,\nwhich is a challenging task in medical imaging and has recently attracted Deep\nLearning (DL) models. Only a few works attempted to make such models\nhuman-interpretable despite the need for transparency and trustworthiness in\ncritical use cases like DL-assisted medical diagnosis. Moreover, such models\neither rely on post-hoc methods or additional annotations. In this work, we\npropose a novel interpretable-by-design method, ProtoVerse, to find relevant\nsub-parts of vertebral fractures (prototypes) that reliably explain the model's\ndecision in a human-understandable way. Specifically, we introduce a novel\ndiversity-promoting loss to mitigate prototype repetitions in small datasets\nwith intricate semantics. We have experimented with the VerSe'19 dataset and\noutperformed the existing prototype-based method. Further, our model provides\nsuperior interpretability against the post-hoc method. Importantly, expert\nradiologists validated the visual interpretability of our results, showing\nclinical applicability.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2024:015",
    "pdf_url": "http://arxiv.org/pdf/2404.02830v2",
    "published_date": "2024-04-03 16:04:59 UTC",
    "updated_date": "2024-07-31 12:34:39 UTC"
  },
  {
    "arxiv_id": "2404.02823v1",
    "title": "Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models",
    "authors": [
      "Haoran Sun",
      "Lixin Liu",
      "Junjie Li",
      "Fengyu Wang",
      "Baohua Dong",
      "Ran Lin",
      "Ruohui Huang"
    ],
    "abstract": "The ability of large language models (LLMs) to follow instructions is crucial\nto real-world applications. Despite recent advances, several studies have\nhighlighted that LLMs struggle when faced with challenging instructions,\nespecially those that include complex constraints, hindering their\neffectiveness in various tasks. To address this challenge, we introduce\nConifer, a novel instruction tuning dataset, designed to enhance LLMs to follow\nmulti-level instructions with complex constraints. Utilizing GPT-4, we curate\nthe dataset by a series of LLM-driven refinement processes to ensure high\nquality. We also propose a progressive learning scheme that emphasizes an\neasy-to-hard progression, and learning from process feedback. Models trained\nwith Conifer exhibit remarkable improvements in instruction-following\nabilities, especially for instructions with complex constraints. On several\ninstruction-following benchmarks, our 7B model outperforms the state-of-the-art\nopen-source 7B models, even exceeds the performance of models 10 times larger\non certain metrics. All the code and Conifer dataset are available at\nhttps://www.github.com/ConiferLM/Conifer.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02823v1",
    "published_date": "2024-04-03 15:55:39 UTC",
    "updated_date": "2024-04-03 15:55:39 UTC"
  },
  {
    "arxiv_id": "2404.02817v5",
    "title": "A Survey of Optimization-based Task and Motion Planning: From Classical To Learning Approaches",
    "authors": [
      "Zhigen Zhao",
      "Shuo Cheng",
      "Yan Ding",
      "Ziyi Zhou",
      "Shiqi Zhang",
      "Danfei Xu",
      "Ye Zhao"
    ],
    "abstract": "Task and Motion Planning (TAMP) integrates high-level task planning and\nlow-level motion planning to equip robots with the autonomy to effectively\nreason over long-horizon, dynamic tasks. Optimization-based TAMP focuses on\nhybrid optimization approaches that define goal conditions via objective\nfunctions and are capable of handling open-ended goals, robotic dynamics, and\nphysical interaction between the robot and the environment. Therefore,\noptimization-based TAMP is particularly suited to solve highly complex,\ncontact-rich locomotion and manipulation problems. This survey provides a\ncomprehensive review on optimization-based TAMP, covering (i) planning domain\nrepresentations, including action description languages and temporal logic,\n(ii) individual solution strategies for components of TAMP, including AI\nplanning and trajectory optimization (TO), and (iii) the dynamic interplay\nbetween logic-based task planning and model-based TO. A particular focus of\nthis survey is to highlight the algorithm structures to efficiently solve TAMP,\nespecially hierarchical and distributed approaches. Additionally, the survey\nemphasizes the synergy between the classical methods and contemporary\nlearning-based innovations such as large language models. Furthermore, the\nfuture research directions for TAMP is discussed in this survey, highlighting\nboth algorithmic and application-specific challenges.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "26 pages, 13 figures, published at IEEE/ASME Transactions on\n  Mechatronics",
    "pdf_url": "http://arxiv.org/pdf/2404.02817v5",
    "published_date": "2024-04-03 15:38:36 UTC",
    "updated_date": "2024-10-07 10:09:16 UTC"
  },
  {
    "arxiv_id": "2404.02807v3",
    "title": "An Optimization Framework to Personalize Passive Cardiac Mechanics",
    "authors": [
      "Lei Shi",
      "Ian Chen",
      "Hiroo Takayama",
      "Vijay Vedula"
    ],
    "abstract": "Personalized cardiac mechanics modeling is a powerful tool for understanding\nthe biomechanics of cardiac function in health and disease and assisting in\ntreatment planning. However, current models are limited to using medical images\nacquired at a single cardiac phase, often limiting their applicability for\nprocessing dynamic image acquisitions. This study introduces an inverse finite\nelement analysis (iFEA) framework to estimate the passive mechanical properties\nof cardiac tissue using time-dependent medical image data. The iFEA framework\nrelies on a novel nested optimization scheme, in which the outer iterations\nutilize a traditional optimization method to best approximate material\nparameters that fit image data, while the inner iterations employ an augmented\nSellier's algorithm to estimate the stress-free reference configuration. With a\nfocus on characterizing the passive mechanical behavior, the framework employs\nstructurally based anisotropic hyperelastic constitutive models and\nphysiologically relevant boundary conditions to simulate myocardial mechanics.\nWe use a stabilized variational multiscale formulation for solving the\ngoverning nonlinear elastodynamics equations, verified for cardiac mechanics\napplications. The framework is tested in myocardium models of biventricle and\nleft atrium derived from cardiac phase-resolved computed tomographic (CT)\nimages of a healthy subject and three patients with hypertrophic obstructive\ncardiomyopathy (HOCM). The impact of the choice of optimization methods and\nother numerical settings, including fiber direction parameters, mesh size,\ninitial parameters for optimization, and perturbations to optimal material\nparameters, is assessed using a rigorous sensitivity analysis. The performance\nof the current iFEA is compared against an assumed power-law-based\npressure-volume relation, typically used for single-phase image acquisition.",
    "categories": [
      "physics.med-ph",
      "cs.AI"
    ],
    "primary_category": "physics.med-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02807v3",
    "published_date": "2024-04-03 15:23:17 UTC",
    "updated_date": "2024-04-06 03:24:27 UTC"
  },
  {
    "arxiv_id": "2404.02806v2",
    "title": "The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers",
    "authors": [
      "Hussein Mozannar",
      "Valerie Chen",
      "Mohammed Alsobay",
      "Subhro Das",
      "Sebastian Zhao",
      "Dennis Wei",
      "Manish Nagireddy",
      "Prasanna Sattigeri",
      "Ameet Talwalkar",
      "David Sontag"
    ],
    "abstract": "Evaluation of large language models for code has primarily relied on static\nbenchmarks, including HumanEval (Chen et al., 2021), or more recently using\nhuman preferences of LLM responses. As LLMs are increasingly used as programmer\nassistants, we study whether gains on existing benchmarks or more preferred LLM\nresponses translate to programmer productivity when coding with LLMs, including\ntime spent coding. We introduce RealHumanEval, a web interface to measure the\nability of LLMs to assist programmers, through either autocomplete or chat\nsupport. We conducted a user study (N=243) using RealHumanEval in which users\ninteracted with seven LLMs of varying base model performance. Despite static\nbenchmarks not incorporating humans-in-the-loop, we find that improvements in\nbenchmark performance lead to increased programmer productivity; however gaps\nin benchmark versus human performance are not proportional -- a trend that\nholds across both forms of LLM support. In contrast, we find that programmer\npreferences do not correlate with their actual performance, motivating the need\nfor better proxy signals. We open-source RealHumanEval to enable human-centric\nevaluation of new models and the study data to facilitate efforts to improve\ncode models.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02806v2",
    "published_date": "2024-04-03 15:20:57 UTC",
    "updated_date": "2024-10-14 23:06:22 UTC"
  },
  {
    "arxiv_id": "2404.15319v1",
    "title": "The largest EEG-based BCI reproducibility study for open science: the MOABB benchmark",
    "authors": [
      "Sylvain Chevallier",
      "Igor Carrara",
      "Bruno Aristimunha",
      "Pierre Guetschel",
      "Sara Sedlar",
      "Bruna Lopes",
      "Sebastien Velut",
      "Salim Khazem",
      "Thomas Moreau"
    ],
    "abstract": "Objective. This study conduct an extensive Brain-computer interfaces (BCI)\nreproducibility analysis on open electroencephalography datasets, aiming to\nassess existing solutions and establish open and reproducible benchmarks for\neffective comparison within the field. The need for such benchmark lies in the\nrapid industrial progress that has given rise to undisclosed proprietary\nsolutions. Furthermore, the scientific literature is dense, often featuring\nchallenging-to-reproduce evaluations, making comparisons between existing\napproaches arduous.\n  Approach. Within an open framework, 30 machine learning pipelines (separated\ninto raw signal: 11, Riemannian: 13, deep learning: 6) are meticulously\nre-implemented and evaluated across 36 publicly available datasets, including\nmotor imagery (14), P300 (15), and SSVEP (7). The analysis incorporates\nstatistical meta-analysis techniques for results assessment, encompassing\nexecution time and environmental impact considerations.\n  Main results. The study yields principled and robust results applicable to\nvarious BCI paradigms, emphasizing motor imagery, P300, and SSVEP. Notably,\nRiemannian approaches utilizing spatial covariance matrices exhibit superior\nperformance, underscoring the necessity for significant data volumes to achieve\ncompetitive outcomes with deep learning techniques. The comprehensive results\nare openly accessible, paving the way for future research to further enhance\nreproducibility in the BCI domain.\n  Significance. The significance of this study lies in its contribution to\nestablishing a rigorous and transparent benchmark for BCI research, offering\ninsights into optimal methodologies and highlighting the importance of\nreproducibility in driving advancements within the field.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "eess.SP",
    "comment": "43 pages, 13 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.15319v1",
    "published_date": "2024-04-03 15:18:50 UTC",
    "updated_date": "2024-04-03 15:18:50 UTC"
  },
  {
    "arxiv_id": "2404.02800v1",
    "title": "On Few-Shot Prompting for Controllable Question-Answer Generation in Narrative Comprehension",
    "authors": [
      "Bernardo Leite",
      "Henrique Lopes Cardoso"
    ],
    "abstract": "Question Generation aims to automatically generate questions based on a given\ninput provided as context. A controllable question generation scheme focuses on\ngenerating questions with specific attributes, allowing better control. In this\nstudy, we propose a few-shot prompting strategy for controlling the generation\nof question-answer pairs from children's narrative texts. We aim to control two\nattributes: the question's explicitness and underlying narrative elements. With\nempirical evaluation, we show the effectiveness of controlling the generation\nprocess by employing few-shot prompting side by side with a reference model.\nOur experiments highlight instances where the few-shot strategy surpasses the\nreference model, particularly in scenarios such as semantic closeness\nevaluation and the diversity and coherency of question-answer pairs. However,\nthese improvements are not always statistically significant. The code is\npublicly available at github.com/bernardoleite/few-shot-prompting-qg-control.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint - Accepted for publication at CSEDU 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.02800v1",
    "published_date": "2024-04-03 15:17:21 UTC",
    "updated_date": "2024-04-03 15:17:21 UTC"
  },
  {
    "arxiv_id": "2405.15778v1",
    "title": "Investigation of Energy-efficient AI Model Architectures and Compression Techniques for \"Green\" Fetal Brain Segmentation",
    "authors": [
      "Szymon Mazurek",
      "Monika Pytlarz",
      "Sylwia Malec",
      "Alessandro Crimi"
    ],
    "abstract": "Artificial intelligence have contributed to advancements across various\nindustries. However, the rapid growth of artificial intelligence technologies\nalso raises concerns about their environmental impact, due to associated carbon\nfootprints to train computational models. Fetal brain segmentation in medical\nimaging is challenging due to the small size of the fetal brain and the limited\nimage quality of fast 2D sequences. Deep neural networks are a promising method\nto overcome this challenge. In this context, the construction of larger models\nrequires extensive data and computing power, leading to high energy\nconsumption. Our study aims to explore model architectures and compression\ntechniques that promote energy efficiency by optimizing the trade-off between\naccuracy and energy consumption through various strategies such as lightweight\nnetwork design, architecture search, and optimized distributed training tools.\nWe have identified several effective strategies including optimization of data\nloading, modern optimizers, distributed training strategy implementation, and\nreduced floating point operations precision usage with light model\narchitectures while tuning parameters according to available computer\nresources. Our findings demonstrate that these methods lead to satisfactory\nmodel performance with low energy consumption during deep neural network\ntraining for medical image segmentation.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.PF"
    ],
    "primary_category": "eess.IV",
    "comment": "Submitted to International Conference on Computational Science (ICCS)\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2405.15778v1",
    "published_date": "2024-04-03 15:11:53 UTC",
    "updated_date": "2024-04-03 15:11:53 UTC"
  },
  {
    "arxiv_id": "2404.02948v4",
    "title": "PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models",
    "authors": [
      "Fanxu Meng",
      "Zhaohui Wang",
      "Muhan Zhang"
    ],
    "abstract": "To parameter-efficiently fine-tune (PEFT) large language models (LLMs), the\nlow-rank adaptation (LoRA) method approximates the model changes $\\Delta W \\in\n\\mathbb{R}^{m \\times n}$ through the product of two matrices $A \\in\n\\mathbb{R}^{m \\times r}$ and $B \\in \\mathbb{R}^{r \\times n}$, where $r \\ll\n\\min(m, n)$, $A$ is initialized with Gaussian noise, and $B$ with zeros. LoRA\nfreezes the original model $W$ and updates the \"Noise & Zero\" adapter, which\nmay lead to slow convergence. To overcome this limitation, we introduce\nPrincipal Singular values and Singular vectors Adaptation (PiSSA). PiSSA shares\nthe same architecture as LoRA, but initializes the adaptor matrices $A$ and $B$\nwith the principal components of the original matrix $W$, and put the remaining\ncomponents into a residual matrix $W^{res} \\in \\mathbb{R}^{m \\times n}$ which\nis frozen during fine-tuning. Compared to LoRA, PiSSA updates the principal\ncomponents while freezing the \"residual\" parts, allowing faster convergence and\nenhanced performance. Comparative experiments of PiSSA and LoRA across 12\ndifferent models, ranging from 184M to 70B, encompassing 5 NLG and 8 NLU tasks,\nreveal that PiSSA consistently outperforms LoRA under identical experimental\nsetups. On the GSM8K benchmark, Mistral-7B fine-tuned with PiSSA achieves an\naccuracy of 72.86%, surpassing LoRA's 67.7% by 5.16%. Due to the same\narchitecture, PiSSA is also compatible with quantization to further reduce the\nmemory requirement of fine-tuning. Compared to QLoRA, QPiSSA exhibits smaller\nquantization errors in the initial stages. Fine-tuning LLaMA-3-70B on GSM8K,\nQPiSSA attains an accuracy of 86.05%, exceeding the performances of QLoRA at\n81.73%. Leveraging a fast SVD technique, PiSSA can be initialized in only a few\nseconds, presenting a negligible cost for transitioning from LoRA to PiSSA.\nCode is available at https://github.com/GraphPKU/PiSSA.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024 spotlight",
    "pdf_url": "http://arxiv.org/pdf/2404.02948v4",
    "published_date": "2024-04-03 15:06:43 UTC",
    "updated_date": "2025-04-09 06:54:20 UTC"
  },
  {
    "arxiv_id": "2404.02947v1",
    "title": "DNN Memory Footprint Reduction via Post-Training Intra-Layer Multi-Precision Quantization",
    "authors": [
      "Behnam Ghavami",
      "Amin Kamjoo",
      "Lesley Shannon",
      "Steve Wilton"
    ],
    "abstract": "The imperative to deploy Deep Neural Network (DNN) models on\nresource-constrained edge devices, spurred by privacy concerns, has become\nincreasingly apparent. To facilitate the transition from cloud to edge\ncomputing, this paper introduces a technique that effectively reduces the\nmemory footprint of DNNs, accommodating the limitations of resource-constrained\nedge devices while preserving model accuracy. Our proposed technique, named\nPost-Training Intra-Layer Multi-Precision Quantization (PTILMPQ), employs a\npost-training quantization approach, eliminating the need for extensive\ntraining data. By estimating the importance of layers and channels within the\nnetwork, the proposed method enables precise bit allocation throughout the\nquantization process. Experimental results demonstrate that PTILMPQ offers a\npromising solution for deploying DNNs on edge devices with restricted memory\nresources. For instance, in the case of ResNet50, it achieves an accuracy of\n74.57\\% with a memory footprint of 9.5 MB, representing a 25.49\\% reduction\ncompared to previous similar methods, with only a minor 1.08\\% decrease in\naccuracy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "The 25th International Symposium on Quality Electronic Design\n  (ISQED'24)",
    "pdf_url": "http://arxiv.org/pdf/2404.02947v1",
    "published_date": "2024-04-03 15:06:09 UTC",
    "updated_date": "2024-04-03 15:06:09 UTC"
  },
  {
    "arxiv_id": "2404.02785v3",
    "title": "Domain Generalization through Meta-Learning: A Survey",
    "authors": [
      "Arsham Gholamzadeh Khoee",
      "Yinan Yu",
      "Robert Feldt"
    ],
    "abstract": "Deep neural networks (DNNs) have revolutionized artificial intelligence but\noften lack performance when faced with out-of-distribution (OOD) data, a common\nscenario due to the inevitable domain shifts in real-world applications. This\nlimitation stems from the common assumption that training and testing data\nshare the same distribution--an assumption frequently violated in practice.\nDespite their effectiveness with large amounts of data and computational power,\nDNNs struggle with distributional shifts and limited labeled data, leading to\noverfitting and poor generalization across various tasks and domains.\nMeta-learning presents a promising approach by employing algorithms that\nacquire transferable knowledge across various tasks for fast adaptation,\neliminating the need to learn each task from scratch. This survey paper delves\ninto the realm of meta-learning with a focus on its contribution to domain\ngeneralization. We first clarify the concept of meta-learning for domain\ngeneralization and introduce a novel taxonomy based on the feature extraction\nstrategy and the classifier learning methodology, offering a granular view of\nmethodologies. Additionally, we present a decision graph to assist readers in\nnavigating the taxonomy based on data availability and domain shifts, enabling\nthem to select and develop a proper model tailored to their specific problem\nrequirements. Through an exhaustive review of existing methods and underlying\ntheories, we map out the fundamentals of the field. Our survey provides\npractical insights and an informed discussion on promising research directions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02785v3",
    "published_date": "2024-04-03 14:55:17 UTC",
    "updated_date": "2024-08-22 13:57:32 UTC"
  },
  {
    "arxiv_id": "2404.05746v2",
    "title": "Causality for Earth Science -- A Review on Time-series and Spatiotemporal Causality Methods",
    "authors": [
      "Sahara Ali",
      "Uzma Hasan",
      "Xingyan Li",
      "Omar Faruque",
      "Akila Sampath",
      "Yiyi Huang",
      "Md Osman Gani",
      "Jianwu Wang"
    ],
    "abstract": "This survey paper covers the breadth and depth of time-series and\nspatiotemporal causality methods, and their applications in Earth Science. More\nspecifically, the paper presents an overview of causal discovery and causal\ninference, explains the underlying causal assumptions, and enlists evaluation\ntechniques and key terminologies of the domain area. The paper elicits the\nvarious state-of-the-art methods introduced for time-series and spatiotemporal\ncausal analysis along with their strengths and limitations. The paper further\ndescribes the existing applications of several methods for answering specific\nEarth Science questions such as extreme weather events, sea level rise,\nteleconnections etc. This survey paper can serve as a primer for Data Science\nresearchers interested in data-driven causal study as we share a list of\nresources, such as Earth Science datasets (synthetic, simulated and\nobservational data) and open source tools for causal analysis. It will equally\nbenefit the Earth Science community interested in taking an AI-driven approach\nto study the causality of different dynamic and thermodynamic processes as we\npresent the open challenges and opportunities in performing causality-based\nEarth Science study.",
    "categories": [
      "physics.data-an",
      "cs.AI",
      "cs.LG",
      "physics.ao-ph",
      "physics.geo-ph"
    ],
    "primary_category": "physics.data-an",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.05746v2",
    "published_date": "2024-04-03 14:33:23 UTC",
    "updated_date": "2024-08-30 21:51:31 UTC"
  },
  {
    "arxiv_id": "2404.02945v1",
    "title": "Optimizing the Deployment of Tiny Transformers on Low-Power MCUs",
    "authors": [
      "Victor J. B. Jung",
      "Alessio Burrello",
      "Moritz Scherer",
      "Francesco Conti",
      "Luca Benini"
    ],
    "abstract": "Transformer networks are rapidly becoming SotA in many fields, such as NLP\nand CV. Similarly to CNN, there is a strong push for deploying Transformer\nmodels at the extreme edge, ultimately fitting the tiny power budget and memory\nfootprint of MCUs. However, the early approaches in this direction are mostly\nad-hoc, platform, and model-specific. This work aims to enable and optimize the\nflexible, multi-platform deployment of encoder Tiny Transformers on commercial\nMCUs. We propose a complete framework to perform end-to-end deployment of\nTransformer models onto single and multi-core MCUs. Our framework provides an\noptimized library of kernels to maximize data reuse and avoid unnecessary data\nmarshaling operations into the crucial attention block. A novel MHSA inference\nschedule, named Fused-Weight Self-Attention, is introduced, fusing the linear\nprojection weights offline to further reduce the number of operations and\nparameters. Furthermore, to mitigate the memory peak reached by the computation\nof the attention map, we present a Depth-First Tiling scheme for MHSA. We\nevaluate our framework on three different MCU classes exploiting ARM and RISC-V\nISA, namely the STM32H7, the STM32L4, and GAP9 (RV32IMC-XpulpV2). We reach an\naverage of 4.79x and 2.0x lower latency compared to SotA libraries CMSIS-NN\n(ARM) and PULP-NN (RISC-V), respectively. Moreover, we show that our MHSA\ndepth-first tiling scheme reduces the memory peak by up to 6.19x, while the\nfused-weight attention can reduce the runtime by 1.53x, and number of\nparameters by 25%. We report significant improvements across several Tiny\nTransformers: for instance, when executing a transformer block for the task of\nradar-based hand-gesture recognition on GAP9, we achieve a latency of 0.14ms\nand energy consumption of 4.92 micro-joules, 2.32x lower than the SotA PULP-NN\nlibrary on the same platform.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.PF"
    ],
    "primary_category": "cs.LG",
    "comment": "Pre-print manuscript submitted for review to the IEEE Transactions on\n  Computers",
    "pdf_url": "http://arxiv.org/pdf/2404.02945v1",
    "published_date": "2024-04-03 14:14:08 UTC",
    "updated_date": "2024-04-03 14:14:08 UTC"
  },
  {
    "arxiv_id": "2404.02761v3",
    "title": "AQuA -- Combining Experts' and Non-Experts' Views To Assess Deliberation Quality in Online Discussions Using LLMs",
    "authors": [
      "Maike Behrendt",
      "Stefan Sylvius Wagner",
      "Marc Ziegele",
      "Lena Wilms",
      "Anke Stoll",
      "Dominique Heinbach",
      "Stefan Harmeling"
    ],
    "abstract": "Measuring the quality of contributions in political online discussions is\ncrucial in deliberation research and computer science. Research has identified\nvarious indicators to assess online discussion quality, and with deep learning\nadvancements, automating these measures has become feasible. While some studies\nfocus on analyzing specific quality indicators, a comprehensive quality score\nincorporating various deliberative aspects is often preferred. In this work, we\nintroduce AQuA, an additive score that calculates a unified deliberative\nquality score from multiple indices for each discussion post. Unlike other\nsingular scores, AQuA preserves information on the deliberative aspects present\nin comments, enhancing model transparency. We develop adapter models for 20\ndeliberative indices, and calculate correlation coefficients between experts'\nannotations and the perceived deliberativeness by non-experts to weigh the\nindividual indices into a single deliberative score. We demonstrate that the\nAQuA score can be computed easily from pre-trained adapters and aligns well\nwith annotations on other datasets that have not be seen during training. The\nanalysis of experts' vs. non-experts' annotations confirms theoretical findings\nin the social science literature.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02761v3",
    "published_date": "2024-04-03 14:07:02 UTC",
    "updated_date": "2024-04-17 10:56:48 UTC"
  },
  {
    "arxiv_id": "2404.02759v1",
    "title": "Unsupervised Occupancy Learning from Sparse Point Cloud",
    "authors": [
      "Amine Ouasfi",
      "Adnane Boukhayma"
    ],
    "abstract": "Implicit Neural Representations have gained prominence as a powerful\nframework for capturing complex data modalities, encompassing a wide range from\n3D shapes to images and audio. Within the realm of 3D shape representation,\nNeural Signed Distance Functions (SDF) have demonstrated remarkable potential\nin faithfully encoding intricate shape geometry. However, learning SDFs from 3D\npoint clouds in the absence of ground truth supervision remains a very\nchallenging task. In this paper, we propose a method to infer occupancy fields\ninstead of SDFs as they are easier to learn from sparse inputs. We leverage a\nmargin-based uncertainty measure to differentially sample from the decision\nboundary of the occupancy function and supervise the sampled boundary points\nusing the input point cloud. We further stabilize the optimization process at\nthe early stages of the training by biasing the occupancy function towards\nminimal entropy fields while maximizing its entropy at the input point cloud.\nThrough extensive experiments and evaluations, we illustrate the efficacy of\nour proposed method, highlighting its capacity to improve implicit shape\ninference with respect to baselines and the state-of-the-art using synthetic\nand real data.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.02759v1",
    "published_date": "2024-04-03 14:05:39 UTC",
    "updated_date": "2024-04-03 14:05:39 UTC"
  },
  {
    "arxiv_id": "2404.02755v1",
    "title": "DIBS: Enhancing Dense Video Captioning with Unlabeled Videos via Pseudo Boundary Enrichment and Online Refinement",
    "authors": [
      "Hao Wu",
      "Huabin Liu",
      "Yu Qiao",
      "Xiao Sun"
    ],
    "abstract": "We present Dive Into the BoundarieS (DIBS), a novel pretraining framework for\ndense video captioning (DVC), that elaborates on improving the quality of the\ngenerated event captions and their associated pseudo event boundaries from\nunlabeled videos. By leveraging the capabilities of diverse large language\nmodels (LLMs), we generate rich DVC-oriented caption candidates and optimize\nthe corresponding pseudo boundaries under several meticulously designed\nobjectives, considering diversity, event-centricity, temporal ordering, and\ncoherence. Moreover, we further introduce a novel online boundary refinement\nstrategy that iteratively improves the quality of pseudo boundaries during\ntraining. Comprehensive experiments have been conducted to examine the\neffectiveness of the proposed technique components. By leveraging a substantial\namount of unlabeled video data, such as HowTo100M, we achieve a remarkable\nadvancement on standard DVC datasets like YouCook2 and ActivityNet. We\noutperform the previous state-of-the-art Vid2Seq across a majority of metrics,\nachieving this with just 0.4% of the unlabeled video data used for pre-training\nby Vid2Seq.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.02755v1",
    "published_date": "2024-04-03 13:57:08 UTC",
    "updated_date": "2024-04-03 13:57:08 UTC"
  },
  {
    "arxiv_id": "2404.02944v1",
    "title": "Foundation Models for Structural Health Monitoring",
    "authors": [
      "Luca Benfenati",
      "Daniele Jahier Pagliari",
      "Luca Zanatta",
      "Yhorman Alexander Bedoya Velez",
      "Andrea Acquaviva",
      "Massimo Poncino",
      "Enrico Macii",
      "Luca Benini",
      "Alessio Burrello"
    ],
    "abstract": "Structural Health Monitoring (SHM) is a critical task for ensuring the safety\nand reliability of civil infrastructures, typically realized on bridges and\nviaducts by means of vibration monitoring. In this paper, we propose for the\nfirst time the use of Transformer neural networks, with a Masked Auto-Encoder\narchitecture, as Foundation Models for SHM. We demonstrate the ability of these\nmodels to learn generalizable representations from multiple large datasets\nthrough self-supervised pre-training, which, coupled with task-specific\nfine-tuning, allows them to outperform state-of-the-art traditional methods on\ndiverse tasks, including Anomaly Detection (AD) and Traffic Load Estimation\n(TLE). We then extensively explore model size versus accuracy trade-offs and\nexperiment with Knowledge Distillation (KD) to improve the performance of\nsmaller Transformers, enabling their embedding directly into the SHM edge\nnodes. We showcase the effectiveness of our foundation models using data from\nthree operational viaducts. For AD, we achieve a near-perfect 99.9% accuracy\nwith a monitoring time span of just 15 windows. In contrast, a state-of-the-art\nmethod based on Principal Component Analysis (PCA) obtains its first good\nresult (95.03% accuracy) only considering 120 windows. On two different TLE\ntasks, our models obtain state-of-the-art performance on multiple evaluation\nmetrics (R$^2$ score, MAE% and MSE%). On the first benchmark, we achieve an\nR$^2$ score of 0.97 and 0.85 for light and heavy vehicle traffic, respectively,\nwhile the best previous approach stops at 0.91 and 0.84. On the second one, we\nachieve an R$^2$ score of 0.54 versus the 0.10 of the best existing method.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY",
      "I.2.1; I.2.3"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 4 tables, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.02944v1",
    "published_date": "2024-04-03 13:32:44 UTC",
    "updated_date": "2024-04-03 13:32:44 UTC"
  },
  {
    "arxiv_id": "2404.02943v1",
    "title": "Learning in Convolutional Neural Networks Accelerated by Transfer Entropy",
    "authors": [
      "Adrian Moldovan",
      "Angel Caţaron",
      "Răzvan Andonie"
    ],
    "abstract": "Recently, there is a growing interest in applying Transfer Entropy (TE) in\nquantifying the effective connectivity between artificial neurons. In a\nfeedforward network, the TE can be used to quantify the relationships between\nneuron output pairs located in different layers. Our focus is on how to include\nthe TE in the learning mechanisms of a Convolutional Neural Network (CNN)\narchitecture. We introduce a novel training mechanism for CNN architectures\nwhich integrates the TE feedback connections. Adding the TE feedback parameter\naccelerates the training process, as fewer epochs are needed. On the flip side,\nit adds computational overhead to each epoch. According to our experiments on\nCNN classifiers, to achieve a reasonable computational overhead--accuracy\ntrade-off, it is efficient to consider only the inter-neural information\ntransfer of a random subset of the neuron pairs from the last two fully\nconnected layers. The TE acts as a smoothing factor, generating stability and\nbecoming active only periodically, not after processing each input sample.\nTherefore, we can consider the TE is in our model a slowly changing\nmeta-parameter.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02943v1",
    "published_date": "2024-04-03 13:31:49 UTC",
    "updated_date": "2024-04-03 13:31:49 UTC"
  },
  {
    "arxiv_id": "2404.02729v1",
    "title": "Learning Sequence Attractors in Recurrent Networks with Hidden Neurons",
    "authors": [
      "Yao Lu",
      "Si Wu"
    ],
    "abstract": "The brain is targeted for processing temporal sequence information. It\nremains largely unclear how the brain learns to store and retrieve sequence\nmemories. Here, we study how recurrent networks of binary neurons learn\nsequence attractors to store predefined pattern sequences and retrieve them\nrobustly. We show that to store arbitrary pattern sequences, it is necessary\nfor the network to include hidden neurons even though their role in displaying\nsequence memories is indirect. We develop a local learning algorithm to learn\nsequence attractors in the networks with hidden neurons. The algorithm is\nproven to converge and lead to sequence attractors. We demonstrate that the\nnetwork model can store and retrieve sequences robustly on synthetic and\nreal-world datasets. We hope that this study provides new insights in\nunderstanding sequence memory and temporal information processing in the brain.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02729v1",
    "published_date": "2024-04-03 13:29:12 UTC",
    "updated_date": "2024-04-03 13:29:12 UTC"
  },
  {
    "arxiv_id": "2404.02728v1",
    "title": "Unsupervised Learning of Effective Actions in Robotics",
    "authors": [
      "Marko Zaric",
      "Jakob Hollenstein",
      "Justus Piater",
      "Erwan Renaudo"
    ],
    "abstract": "Learning actions that are relevant to decision-making and can be executed\neffectively is a key problem in autonomous robotics. Current state-of-the-art\naction representations in robotics lack proper effect-driven learning of the\nrobot's actions. Although successful in solving manipulation tasks, deep\nlearning methods also lack this ability, in addition to their high cost in\nterms of memory or training data. In this paper, we propose an unsupervised\nalgorithm to discretize a continuous motion space and generate \"action\nprototypes\", each producing different effects in the environment. After an\nexploration phase, the algorithm automatically builds a representation of the\neffects and groups motions into action prototypes, where motions more likely to\nproduce an effect are represented more than those that lead to negligible\nchanges. We evaluate our method on a simulated stair-climbing reinforcement\nlearning task, and the preliminary results show that our effect driven\ndiscretization outperforms uniformly and randomly sampled discretizations in\nconvergence speed and maximum reward.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted at The First Austrian Symposium on AI, Robotics, and Vision\n  (AIROV24)",
    "pdf_url": "http://arxiv.org/pdf/2404.02728v1",
    "published_date": "2024-04-03 13:28:52 UTC",
    "updated_date": "2024-04-03 13:28:52 UTC"
  },
  {
    "arxiv_id": "2404.02719v1",
    "title": "Can We Understand Plasticity Through Neural Collapse?",
    "authors": [
      "Guglielmo Bonifazi",
      "Iason Chalas",
      "Gian Hess",
      "Jakub Łucki"
    ],
    "abstract": "This paper explores the connection between two recently identified phenomena\nin deep learning: plasticity loss and neural collapse. We analyze their\ncorrelation in different scenarios, revealing a significant association during\nthe initial training phase on the first task. Additionally, we introduce a\nregularization approach to mitigate neural collapse, demonstrating its\neffectiveness in alleviating plasticity loss in this specific setting.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02719v1",
    "published_date": "2024-04-03 13:21:58 UTC",
    "updated_date": "2024-04-03 13:21:58 UTC"
  },
  {
    "arxiv_id": "2404.07230v1",
    "title": "Interval-valued fuzzy soft $β$-covering approximation spaces",
    "authors": [
      "Shizhan Lu"
    ],
    "abstract": "The concept of interval-valued fuzzy soft $\\beta$-covering approximation\nspaces (IFS$\\beta$CASs) is introduced to combine the theories of soft sets,\nrough sets and interval-valued fuzzy sets, and some fundamental propositions\nconcerning interval-valued fuzzy soft $\\beta$-neighborhoods and soft\n$\\beta$-neighborhoods of IFS$\\beta$CASs are explored. And then four kinds of\ninterval-valued fuzzy soft $\\beta$-coverings based fuzzy rough sets are\nresearched. Finally, the relationships of four kinds of interval-valued fuzzy\nsoft $\\beta$-coverings based fuzzy rough sets are investigated.",
    "categories": [
      "math.GM",
      "cs.AI"
    ],
    "primary_category": "math.GM",
    "comment": "12 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.07230v1",
    "published_date": "2024-04-03 13:04:54 UTC",
    "updated_date": "2024-04-03 13:04:54 UTC"
  },
  {
    "arxiv_id": "2406.11854v1",
    "title": "Attributions toward Artificial Agents in a modified Moral Turing Test",
    "authors": [
      "Eyal Aharoni",
      "Sharlene Fernandes",
      "Daniel J. Brady",
      "Caelan Alexander",
      "Michael Criner",
      "Kara Queen",
      "Javier Rando",
      "Eddy Nahmias",
      "Victor Crespo"
    ],
    "abstract": "Advances in artificial intelligence (AI) raise important questions about\nwhether people view moral evaluations by AI systems similarly to\nhuman-generated moral evaluations. We conducted a modified Moral Turing Test\n(m-MTT), inspired by Allen and colleagues' (2000) proposal, by asking people to\ndistinguish real human moral evaluations from those made by a popular advanced\nAI language model: GPT-4. A representative sample of 299 U.S. adults first\nrated the quality of moral evaluations when blinded to their source.\nRemarkably, they rated the AI's moral reasoning as superior in quality to\nhumans' along almost all dimensions, including virtuousness, intelligence, and\ntrustworthiness, consistent with passing what Allen and colleagues call the\ncomparative MTT. Next, when tasked with identifying the source of each\nevaluation (human or computer), people performed significantly above chance\nlevels. Although the AI did not pass this test, this was not because of its\ninferior moral reasoning but, potentially, its perceived superiority, among\nother possible explanations. The emergence of language models capable of\nproducing moral responses perceived as superior in quality to humans' raises\nconcerns that people may uncritically accept potentially harmful moral guidance\nfrom AI. This possibility highlights the need for safeguards around generative\nlanguage models in matters of morality.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "23 pages, 0 figures, in press",
    "pdf_url": "http://arxiv.org/pdf/2406.11854v1",
    "published_date": "2024-04-03 13:00:47 UTC",
    "updated_date": "2024-04-03 13:00:47 UTC"
  },
  {
    "arxiv_id": "2404.02702v3",
    "title": "PSCodec: A Series of High-Fidelity Low-bitrate Neural Speech Codecs Leveraging Prompt Encoders",
    "authors": [
      "Yu Pan",
      "Xiang Zhang",
      "Yuguang Yang",
      "Jixun Yao",
      "Yanni Hu",
      "Jianhao Ye",
      "Hongbin Zhou",
      "Lei Ma",
      "Jianjun Zhao"
    ],
    "abstract": "Neural speech codecs have recently emerged as a focal point in the fields of\nspeech compression and generation. Despite this progress, achieving\nhigh-quality speech reconstruction under low-bitrate scenarios remains a\nsignificant challenge. In this paper, we propose PSCodec, a series of neural\nspeech codecs based on prompt encoders, comprising PSCodec-Base,\nPSCodec-DRL-ICT, and PSCodec-CasAN, which are capable of delivering\nhigh-performance speech reconstruction with low bandwidths. Specifically, we\nfirst introduce PSCodec-Base, which leverages a pretrained speaker verification\nmodel-based prompt encoder (VPP-Enc) and a learnable Mel-spectrogram-based\nprompt encoder (MelP-Enc) to effectively disentangle and integrate voiceprint\nand Mel-related features in utterances. To further enhance feature utilization\nefficiency, we propose PSCodec-DRL-ICT, incorporating a structural similarity\n(SSIM) based disentangled representation loss (DRL) and an incremental\ncontinuous training (ICT) strategy. While PSCodec-DRL-ICT demonstrates\nimpressive performance, its reliance on extensive hyperparameter tuning and\nmulti-stage training makes it somewhat labor-intensive. To circumvent these\nlimitations, we propose PSCodec-CasAN, utilizing an advanced cascaded attention\nnetwork (CasAN) to enhance representational capacity of the entire system.\nExtensive experiments show that our proposed PSCodec-Base, PSCodec-DRL-ICT, and\nPSCodec-CasAN all significantly outperform several state-of-the-art neural\ncodecs, exhibiting substantial improvements in both speech reconstruction\nquality and speaker similarity under low-bitrate conditions.",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "primary_category": "cs.SD",
    "comment": "Submiited to TASLP",
    "pdf_url": "http://arxiv.org/pdf/2404.02702v3",
    "published_date": "2024-04-03 13:00:08 UTC",
    "updated_date": "2024-11-21 10:31:03 UTC"
  },
  {
    "arxiv_id": "2404.02942v1",
    "title": "Decision Predicate Graphs: Enhancing Interpretability in Tree Ensembles",
    "authors": [
      "Leonardo Arrighi",
      "Luca Pennella",
      "Gabriel Marques Tavares",
      "Sylvio Barbon Junior"
    ],
    "abstract": "Understanding the decisions of tree-based ensembles and their relationships\nis pivotal for machine learning model interpretation. Recent attempts to\nmitigate the human-in-the-loop interpretation challenge have explored the\nextraction of the decision structure underlying the model taking advantage of\ngraph simplification and path emphasis. However, while these efforts enhance\nthe visualisation experience, they may either result in a visually complex\nrepresentation or compromise the interpretability of the original ensemble\nmodel. In addressing this challenge, especially in complex scenarios, we\nintroduce the Decision Predicate Graph (DPG) as a model-agnostic tool to\nprovide a global interpretation of the model. DPG is a graph structure that\ncaptures the tree-based ensemble model and learned dataset details, preserving\nthe relations among features, logical decisions, and predictions towards\nemphasising insightful points. Leveraging well-known graph theory concepts,\nsuch as the notions of centrality and community, DPG offers additional\nquantitative insights into the model, complementing visualisation techniques,\nexpanding the problem space descriptions, and offering diverse possibilities\nfor extensions. Empirical experiments demonstrate the potential of DPG in\naddressing traditional benchmarks and complex classification scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02942v1",
    "published_date": "2024-04-03 12:38:12 UTC",
    "updated_date": "2024-04-03 12:38:12 UTC"
  },
  {
    "arxiv_id": "2404.02690v2",
    "title": "How Sparse Attention Approximates Exact Attention? Your Attention is Naturally $n^C$-Sparse",
    "authors": [
      "Yichuan Deng",
      "Zhao Song",
      "Jing Xiong",
      "Chiwun Yang"
    ],
    "abstract": "Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02690v2",
    "published_date": "2024-04-03 12:37:34 UTC",
    "updated_date": "2025-02-12 14:32:46 UTC"
  },
  {
    "arxiv_id": "2404.02684v1",
    "title": "Cross-Architecture Transfer Learning for Linear-Cost Inference Transformers",
    "authors": [
      "Sehyun Choi"
    ],
    "abstract": "Recently, multiple architectures has been proposed to improve the efficiency\nof the Transformer Language Models through changing the design of the\nself-attention block to have a linear-cost inference (LCI). A notable approach\nin this realm is the State-Space Machines (SSMs) architecture, which showed\non-par performance on language modeling tasks with the self-attention\ntransformers. However, such an architectural change requires a full pretraining\nof the weights from scratch, which incurs a huge cost to researchers and\npractitioners who want to use the new architectures. In the more traditional\nlinear attention works, it has been proposed to approximate full attention with\nlinear attention by swap-and-finetune framework. Motivated by this approach, we\npropose Cross-Architecture Transfer Learning (XATL), in which the weights of\nthe shared components between LCI and self-attention-based transformers, such\nas layernorms, MLPs, input/output embeddings, are directly transferred to the\nnew architecture from already pre-trained model parameters. We experimented the\nefficacy of the method on varying sizes and alternative attention architectures\nand show that \\methodabbr significantly reduces the training time up to 2.5x\ntimes and converges to a better minimum with up to 2.6% stronger model on the\nLM benchmarks within the same compute budget.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2404.02684v1",
    "published_date": "2024-04-03 12:27:36 UTC",
    "updated_date": "2024-04-03 12:27:36 UTC"
  },
  {
    "arxiv_id": "2404.02681v1",
    "title": "PejorativITy: Disambiguating Pejorative Epithets to Improve Misogyny Detection in Italian Tweets",
    "authors": [
      "Arianna Muti",
      "Federico Ruggeri",
      "Cagri Toraman",
      "Lorenzo Musetti",
      "Samuel Algherini",
      "Silvia Ronchi",
      "Gianmarco Saretto",
      "Caterina Zapparoli",
      "Alberto Barrón-Cedeño"
    ],
    "abstract": "Misogyny is often expressed through figurative language. Some neutral words\ncan assume a negative connotation when functioning as pejorative epithets.\nDisambiguating the meaning of such terms might help the detection of misogyny.\nIn order to address such task, we present PejorativITy, a novel corpus of 1,200\nmanually annotated Italian tweets for pejorative language at the word level and\nmisogyny at the sentence level. We evaluate the impact of injecting information\nabout disambiguated words into a model targeting misogyny detection. In\nparticular, we explore two different approaches for injection: concatenation of\npejorative information and substitution of ambiguous words with univocal terms.\nOur experimental results, both on our corpus and on two popular benchmarks on\nItalian tweets, show that both approaches lead to a major classification\nimprovement, indicating that word sense disambiguation is a promising\npreliminary step for misogyny detection. Furthermore, we investigate LLMs'\nunderstanding of pejorative epithets by means of contextual word embeddings\nanalysis and prompting.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02681v1",
    "published_date": "2024-04-03 12:24:48 UTC",
    "updated_date": "2024-04-03 12:24:48 UTC"
  },
  {
    "arxiv_id": "2404.02675v1",
    "title": "Responsible Reporting for Frontier AI Development",
    "authors": [
      "Noam Kolt",
      "Markus Anderljung",
      "Joslyn Barnhart",
      "Asher Brass",
      "Kevin Esvelt",
      "Gillian K. Hadfield",
      "Lennart Heim",
      "Mikel Rodriguez",
      "Jonas B. Sandbrink",
      "Thomas Woodside"
    ],
    "abstract": "Mitigating the risks from frontier AI systems requires up-to-date and\nreliable information about those systems. Organizations that develop and deploy\nfrontier systems have significant access to such information. By reporting\nsafety-critical information to actors in government, industry, and civil\nsociety, these organizations could improve visibility into new and emerging\nrisks posed by frontier systems. Equipped with this information, developers\ncould make better informed decisions on risk management, while policymakers\ncould design more targeted and robust regulatory infrastructure. We outline the\nkey features of responsible reporting and propose mechanisms for implementing\nthem in practice.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02675v1",
    "published_date": "2024-04-03 12:18:45 UTC",
    "updated_date": "2024-04-03 12:18:45 UTC"
  },
  {
    "arxiv_id": "2404.02657v4",
    "title": "Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models",
    "authors": [
      "Taiqiang Wu",
      "Chaofan Tao",
      "Jiahao Wang",
      "Runming Yang",
      "Zhe Zhao",
      "Ngai Wong"
    ],
    "abstract": "Kullback-Leiber divergence has been widely used in Knowledge Distillation\n(KD) to compress Large Language Models (LLMs). Contrary to prior assertions\nthat reverse Kullback-Leibler (RKL) divergence is mode-seeking and thus\npreferable over the mean-seeking forward Kullback-Leibler (FKL) divergence,\nthis study empirically and theoretically demonstrates that neither mode-seeking\nnor mean-seeking properties manifest in KD for LLMs. Instead, RKL and FKL are\nfound to share the same optimization objective and both converge after a\nsufficient number of epochs. However, due to practical constraints, LLMs are\nseldom trained for such an extensive number of epochs. Meanwhile, we further\nfind that RKL focuses on the tail part of the distributions, while FKL focuses\non the head part at the beginning epochs. Consequently, we propose a simple yet\neffective Adaptive Kullback-Leiber (AKL) divergence method, which adaptively\nallocates weights to combine FKL and RKL. Metric-based and GPT-4-based\nevaluations demonstrate that the proposed AKL outperforms the baselines across\nvarious tasks and improves the diversity and quality of generated responses.\nCodes are available at \\href{https://github.com/wutaiqiang/LLM_KD_AKL}{github}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "COLING 2025",
    "pdf_url": "http://arxiv.org/pdf/2404.02657v4",
    "published_date": "2024-04-03 11:40:17 UTC",
    "updated_date": "2024-12-08 13:03:38 UTC"
  },
  {
    "arxiv_id": "2404.02656v2",
    "title": "Non-negative Subspace Feature Representation for Few-shot Learning in Medical Imaging",
    "authors": [
      "Keqiang Fan",
      "Xiaohao Cai",
      "Mahesan Niranjan"
    ],
    "abstract": "Unlike typical visual scene recognition domains, in which massive datasets\nare accessible to deep neural networks, medical image interpretations are often\nobstructed by the paucity of data. In this paper, we investigate the\neffectiveness of data-based few-shot learning in medical imaging by exploring\ndifferent data attribute representations in a low-dimensional space. We\nintroduce different types of non-negative matrix factorization (NMF) in\nfew-shot learning, addressing the data scarcity issue in medical image\nclassification. Extensive empirical studies are conducted in terms of\nvalidating the effectiveness of NMF, especially its supervised variants (e.g.,\ndiscriminative NMF, and supervised and constrained NMF with sparseness), and\nthe comparison with principal component analysis (PCA), i.e., the collaborative\nrepresentation-based dimensionality reduction technique derived from\neigenvectors. With 14 different datasets covering 11 distinct illness\ncategories, thorough experimental results and comparison with related\ntechniques demonstrate that NMF is a competitive alternative to PCA for\nfew-shot learning in medical imaging, and the supervised NMF algorithms are\nmore discriminative in the subspace with greater effectiveness. Furthermore, we\nshow that the part-based representation of NMF, especially its supervised\nvariants, is dramatically impactful in detecting lesion areas in medical\nimaging with limited samples.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02656v2",
    "published_date": "2024-04-03 11:37:03 UTC",
    "updated_date": "2024-04-04 13:30:59 UTC"
  },
  {
    "arxiv_id": "2404.02650v1",
    "title": "Towards detecting unanticipated bias in Large Language Models",
    "authors": [
      "Anna Kruspe"
    ],
    "abstract": "Over the last year, Large Language Models (LLMs) like ChatGPT have become\nwidely available and have exhibited fairness issues similar to those in\nprevious machine learning systems. Current research is primarily focused on\nanalyzing and quantifying these biases in training data and their impact on the\ndecisions of these models, alongside developing mitigation strategies. This\nresearch largely targets well-known biases related to gender, race, ethnicity,\nand language. However, it is clear that LLMs are also affected by other, less\nobvious implicit biases. The complex and often opaque nature of these models\nmakes detecting such biases challenging, yet this is crucial due to their\npotential negative impact in various applications. In this paper, we explore\nnew avenues for detecting these unanticipated biases in LLMs, focusing\nspecifically on Uncertainty Quantification and Explainable AI methods. These\napproaches aim to assess the certainty of model decisions and to make the\ninternal decision-making processes of LLMs more transparent, thereby\nidentifying and understanding biases that are not immediately apparent. Through\nthis research, we aim to contribute to the development of fairer and more\ntransparent AI systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02650v1",
    "published_date": "2024-04-03 11:25:20 UTC",
    "updated_date": "2024-04-03 11:25:20 UTC"
  },
  {
    "arxiv_id": "2404.02648v1",
    "title": "A Universal Deep Neural Network for Signal Detection in Wireless Communication Systems",
    "authors": [
      "Khalid Albagami",
      "Nguyen Van Huynh",
      "Geoffrey Ye Li"
    ],
    "abstract": "Recently, deep learning (DL) has been emerging as a promising approach for\nchannel estimation and signal detection in wireless communications. The\nmajority of the existing studies investigating the use of DL techniques in this\ndomain focus on analysing channel impulse responses that are generated from\nonly one channel distribution such as additive white Gaussian channel noise and\nRayleigh channels. In practice, to cope with the dynamic nature of the wireless\nchannel, DL methods must be re-trained on newly non-aged collected data which\nis costly, inefficient, and impractical. To tackle this challenge, this paper\nproposes a novel universal deep neural network (Uni-DNN) that can achieve high\ndetection performance in various wireless environments without retraining the\nmodel. In particular, our proposed Uni-DNN model consists of a wireless channel\nclassifier and a signal detector which are constructed by using DNNs. The\nwireless channel classifier enables the signal detector to generalise and\nperform optimally for multiple wireless channel distributions. In addition, to\nfurther improve the signal detection performance of the proposed model,\nconvolutional neural network is employed. Extensive simulations using the\northogonal frequency division multiplexing scheme demonstrate that the bit\nerror rate performance of our proposed solution can outperform conventional\nDL-based approaches as well as least square and minimum mean square error\nchannel estimators in practical low pilot density scenarios.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02648v1",
    "published_date": "2024-04-03 11:21:10 UTC",
    "updated_date": "2024-04-03 11:21:10 UTC"
  },
  {
    "arxiv_id": "2404.02637v2",
    "title": "Vocabulary Attack to Hijack Large Language Model Applications",
    "authors": [
      "Patrick Levi",
      "Christoph P. Neumann"
    ],
    "abstract": "The fast advancements in Large Language Models (LLMs) are driving an\nincreasing number of applications. Together with the growing number of users,\nwe also see an increasing number of attackers who try to outsmart these\nsystems. They want the model to reveal confidential information, specific false\ninformation, or offensive behavior. To this end, they manipulate their\ninstructions for the LLM by inserting separators or rephrasing them\nsystematically until they reach their goal. Our approach is different. It\ninserts words from the model vocabulary. We find these words using an\noptimization procedure and embeddings from another LLM (attacker LLM). We prove\nour approach by goal hijacking two popular open-source LLMs from the Llama2 and\nthe Flan-T5 families, respectively. We present two main findings. First, our\napproach creates inconspicuous instructions and therefore it is hard to detect.\nFor many attack cases, we find that even a single word insertion is sufficient.\nSecond, we demonstrate that we can conduct our attack using a different model\nthan the target model to conduct our attack with.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02637v2",
    "published_date": "2024-04-03 10:54:07 UTC",
    "updated_date": "2024-05-30 06:28:31 UTC"
  },
  {
    "arxiv_id": "2404.02625v1",
    "title": "A Differentiable Integer Linear Programming Solver for Explanation-Based Natural Language Inference",
    "authors": [
      "Mokanarangan Thayaparan",
      "Marco Valentino",
      "André Freitas"
    ],
    "abstract": "Integer Linear Programming (ILP) has been proposed as a formalism for\nencoding precise structural and semantic constraints for Natural Language\nInference (NLI). However, traditional ILP frameworks are non-differentiable,\nposing critical challenges for the integration of continuous language\nrepresentations based on deep learning. In this paper, we introduce a novel\napproach, named Diff-Comb Explainer, a neuro-symbolic architecture for\nexplanation-based NLI based on Differentiable BlackBox Combinatorial Solvers\n(DBCS). Differently from existing neuro-symbolic solvers, Diff-Comb Explainer\ndoes not necessitate a continuous relaxation of the semantic constraints,\nenabling a direct, more precise, and efficient incorporation of neural\nrepresentations into the ILP formulation. Our experiments demonstrate that\nDiff-Comb Explainer achieves superior performance when compared to conventional\nILP solvers, neuro-symbolic black-box solvers, and Transformer-based encoders.\nMoreover, a deeper analysis reveals that Diff-Comb Explainer can significantly\nimprove the precision, consistency, and faithfulness of the constructed\nexplanations, opening new opportunities for research on neuro-symbolic\narchitectures for explainable and transparent NLI in complex domains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to LREC-COLING 2024 - Camera Ready. arXiv admin note:\n  substantial text overlap with arXiv:2208.03339",
    "pdf_url": "http://arxiv.org/pdf/2404.02625v1",
    "published_date": "2024-04-03 10:29:06 UTC",
    "updated_date": "2024-04-03 10:29:06 UTC"
  },
  {
    "arxiv_id": "2404.02618v1",
    "title": "Diffexplainer: Towards Cross-modal Global Explanations with Diffusion Models",
    "authors": [
      "Matteo Pennisi",
      "Giovanni Bellitto",
      "Simone Palazzo",
      "Mubarak Shah",
      "Concetto Spampinato"
    ],
    "abstract": "We present DiffExplainer, a novel framework that, leveraging language-vision\nmodels, enables multimodal global explainability. DiffExplainer employs\ndiffusion models conditioned on optimized text prompts, synthesizing images\nthat maximize class outputs and hidden features of a classifier, thus providing\na visual tool for explaining decisions. Moreover, the analysis of generated\nvisual descriptions allows for automatic identification of biases and spurious\nfeatures, as opposed to traditional methods that often rely on manual\nintervention. The cross-modal transferability of language-vision models also\nenables the possibility to describe decisions in a more human-interpretable\nway, i.e., through text. We conduct comprehensive experiments, which include an\nextensive user study, demonstrating the effectiveness of DiffExplainer on 1)\nthe generation of high-quality images explaining model decisions, surpassing\nexisting activation maximization methods, and 2) the automated identification\nof biases and spurious features.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02618v1",
    "published_date": "2024-04-03 10:11:22 UTC",
    "updated_date": "2024-04-03 10:11:22 UTC"
  },
  {
    "arxiv_id": "2404.02611v3",
    "title": "X-SHIELD: Regularization for eXplainable Artificial Intelligence",
    "authors": [
      "Iván Sevillano-García",
      "Julián Luengo",
      "Francisco Herrera"
    ],
    "abstract": "As artificial intelligence systems become integral across domains, the demand\nfor explainability grows, the called eXplainable artificial intelligence (XAI).\nExisting efforts primarily focus on generating and evaluating explanations for\nblack-box models while a critical gap in directly enhancing models remains\nthrough these evaluations. It is important to consider the potential of this\nexplanation process to improve model quality with a feedback on training as\nwell. XAI may be used to improve model performance while boosting its\nexplainability. Under this view, this paper introduces Transformation -\nSelective Hidden Input Evaluation for Learning Dynamics (T-SHIELD), a\nregularization family designed to improve model quality by hiding features of\ninput, forcing the model to generalize without those features. Within this\nfamily, we propose the XAI - SHIELD(X-SHIELD), a regularization for explainable\nartificial intelligence, which uses explanations to select specific features to\nhide. In contrast to conventional approaches, X-SHIELD regularization\nseamlessly integrates into the objective function enhancing model\nexplainability while also improving performance. Experimental validation on\nbenchmark datasets underscores X-SHIELD's effectiveness in improving\nperformance and overall explainability. The improvement is validated through\nexperiments comparing models with and without the X-SHIELD regularization, with\nfurther analysis exploring the rationale behind its design choices. This\nestablishes X-SHIELD regularization as a promising pathway for developing\nreliable artificial intelligence regularization.",
    "categories": [
      "cs.AI",
      "I.2.6"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.02611v3",
    "published_date": "2024-04-03 09:56:38 UTC",
    "updated_date": "2025-03-11 12:24:01 UTC"
  },
  {
    "arxiv_id": "2404.02589v1",
    "title": "Affective-NLI: Towards Accurate and Interpretable Personality Recognition in Conversation",
    "authors": [
      "Zhiyuan Wen",
      "Jiannong Cao",
      "Yu Yang",
      "Ruosong Yang",
      "Shuaiqi Liu"
    ],
    "abstract": "Personality Recognition in Conversation (PRC) aims to identify the\npersonality traits of speakers through textual dialogue content. It is\nessential for providing personalized services in various applications of\nHuman-Computer Interaction (HCI), such as AI-based mental therapy and companion\nrobots for the elderly. Most recent studies analyze the dialog content for\npersonality classification yet overlook two major concerns that hinder their\nperformance. First, crucial implicit factors contained in conversation, such as\nemotions that reflect the speakers' personalities are ignored. Second, only\nfocusing on the input dialog content disregards the semantic understanding of\npersonality itself, which reduces the interpretability of the results. In this\npaper, we propose Affective Natural Language Inference (Affective-NLI) for\naccurate and interpretable PRC. To utilize affectivity within dialog content\nfor accurate personality recognition, we fine-tuned a pre-trained language\nmodel specifically for emotion recognition in conversations, facilitating\nreal-time affective annotations for utterances. For interpretability of\nrecognition results, we formulate personality recognition as an NLI problem by\ndetermining whether the textual description of personality labels is entailed\nby the dialog content. Extensive experiments on two daily conversation datasets\nsuggest that Affective-NLI significantly outperforms (by 6%-7%)\nstate-of-the-art approaches. Additionally, our Flow experiment demonstrates\nthat Affective-NLI can accurately recognize the speaker's personality in the\nearly stages of conversations by surpassing state-of-the-art methods with\n22%-34%.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by IEEE PerCom 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.02589v1",
    "published_date": "2024-04-03 09:14:24 UTC",
    "updated_date": "2024-04-03 09:14:24 UTC"
  },
  {
    "arxiv_id": "2404.02587v2",
    "title": "The Surprising Effectiveness of Rankers Trained on Expanded Queries",
    "authors": [
      "Abhijit Anand",
      "Venktesh V",
      "Vinay Setty",
      "Avishek Anand"
    ],
    "abstract": "An important problem in text-ranking systems is handling the hard queries\nthat form the tail end of the query distribution. The difficulty may arise due\nto the presence of uncommon, underspecified, or incomplete queries. In this\nwork, we improve the ranking performance of hard or difficult queries without\ncompromising the performance of other queries. Firstly, we do LLM based query\nenrichment for training queries using relevant documents. Next, a specialized\nranker is fine-tuned only on the enriched hard queries instead of the original\nqueries. We combine the relevance scores from the specialized ranker and the\nbase ranker, along with a query performance score estimated for each query. Our\napproach departs from existing methods that usually employ a single ranker for\nall queries, which is biased towards easy queries, which form the majority of\nthe query distribution. In our extensive experiments on the DL-Hard dataset, we\nfind that a principled query performance based scoring method using base and\nspecialized ranker offers a significant improvement of up to 25% on the passage\nranking task and up to 48.4% on the document ranking task when compared to the\nbaseline performance of using original queries, even outperforming SOTA model.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02587v2",
    "published_date": "2024-04-03 09:12:22 UTC",
    "updated_date": "2024-06-12 09:34:43 UTC"
  },
  {
    "arxiv_id": "2404.02580v1",
    "title": "Active learning for efficient annotation in precision agriculture: a use-case on crop-weed semantic segmentation",
    "authors": [
      "Bart M. van Marrewijk",
      "Charbel Dandjinou",
      "Dan Jeric Arcega Rustia",
      "Nicolas Franco Gonzalez",
      "Boubacar Diallo",
      "Jérôme Dias",
      "Paul Melki",
      "Pieter M. Blok"
    ],
    "abstract": "Optimizing deep learning models requires large amounts of annotated images, a\nprocess that is both time-intensive and costly. Especially for semantic\nsegmentation models in which every pixel must be annotated. A potential\nstrategy to mitigate annotation effort is active learning. Active learning\nfacilitates the identification and selection of the most informative images\nfrom a large unlabelled pool. The underlying premise is that these selected\nimages can improve the model's performance faster than random selection to\nreduce annotation effort. While active learning has demonstrated promising\nresults on benchmark datasets like Cityscapes, its performance in the\nagricultural domain remains largely unexplored. This study addresses this\nresearch gap by conducting a comparative study of three active learning-based\nacquisition functions: Bayesian Active Learning by Disagreement (BALD),\nstochastic-based BALD (PowerBALD), and Random. The acquisition functions were\ntested on two agricultural datasets: Sugarbeet and Corn-Weed, both containing\nthree semantic classes: background, crop and weed. Our results indicated that\nactive learning, especially PowerBALD, yields a higher performance than Random\nsampling on both datasets. But due to the relatively large standard deviations,\nthe differences observed were minimal; this was partly caused by high image\nredundancy and imbalanced classes. Specifically, more than 89\\% of the pixels\nbelonged to the background class on both datasets. The absence of significant\nresults on both datasets indicates that further research is required for\napplying active learning on agricultural datasets, especially if they contain a\nhigh-class imbalance and redundant images. Recommendations and insights are\nprovided in this paper to potentially resolve such issues.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02580v1",
    "published_date": "2024-04-03 08:55:44 UTC",
    "updated_date": "2024-04-03 08:55:44 UTC"
  },
  {
    "arxiv_id": "2404.02579v1",
    "title": "Learning Alternative Ways of Performing a Task",
    "authors": [
      "David Nieves",
      "María José Ramírez-Quintana",
      "Carlos Monserrat",
      "César Ferri",
      "José Hernández-Orallo"
    ],
    "abstract": "A common way of learning to perform a task is to observe how it is carried\nout by experts. However, it is well known that for most tasks there is no\nunique way to perform them. This is especially noticeable the more complex the\ntask is because factors such as the skill or the know-how of the expert may\nwell affect the way she solves the task. In addition, learning from experts\nalso suffers of having a small set of training examples generally coming from\nseveral experts (since experts are usually a limited and expensive resource),\nbeing all of them positive examples (i.e. examples that represent successful\nexecutions of the task). Traditional machine learning techniques are not useful\nin such scenarios, as they require extensive training data. Starting from very\nfew executions of the task presented as activity sequences, we introduce a\nnovel inductive approach for learning multiple models, with each one\nrepresenting an alternative strategy of performing a task. By an iterative\nprocess based on generalisation and specialisation, we learn the underlying\npatterns that capture the different styles of performing a task exhibited by\nthe examples. We illustrate our approach on two common activity recognition\ntasks: a surgical skills training task and a cooking domain. We evaluate the\ninferred models with respect to two metrics that measure how well the models\nrepresent the examples and capture the different forms of executing a task\nshowed by the examples. We compare our results with the traditional process\nmining approach and show that a small set of meaningful examples is enough to\nobtain patterns that capture the different strategies that are followed to\nsolve the tasks.",
    "categories": [
      "cs.AI",
      "I.2.6; I.5.4"
    ],
    "primary_category": "cs.AI",
    "comment": "32 pages, Github repository, published paper, authors' version",
    "pdf_url": "http://arxiv.org/pdf/2404.02579v1",
    "published_date": "2024-04-03 08:54:58 UTC",
    "updated_date": "2024-04-03 08:54:58 UTC"
  },
  {
    "arxiv_id": "2404.07229v1",
    "title": "Personality-affected Emotion Generation in Dialog Systems",
    "authors": [
      "Zhiyuan Wen",
      "Jiannong Cao",
      "Jiaxing Shen",
      "Ruosong Yang",
      "Shuaiqi Liu",
      "Maosong Sun"
    ],
    "abstract": "Generating appropriate emotions for responses is essential for dialog systems\nto provide human-like interaction in various application scenarios. Most\nprevious dialog systems tried to achieve this goal by learning empathetic\nmanners from anonymous conversational data. However, emotional responses\ngenerated by those methods may be inconsistent, which will decrease user\nengagement and service quality. Psychological findings suggest that the\nemotional expressions of humans are rooted in personality traits. Therefore, we\npropose a new task, Personality-affected Emotion Generation, to generate\nemotion based on the personality given to the dialog system and further\ninvestigate a solution through the personality-affected mood transition.\nSpecifically, we first construct a daily dialog dataset, Personality\nEmotionLines Dataset (PELD), with emotion and personality annotations.\nSubsequently, we analyze the challenges in this task, i.e., (1) heterogeneously\nintegrating personality and emotional factors and (2) extracting\nmulti-granularity emotional information in the dialog context. Finally, we\npropose to model the personality as the transition weight by simulating the\nmood transition process in the dialog system and solve the challenges above. We\nconduct extensive experiments on PELD for evaluation. Results suggest that by\nadopting our method, the emotion generation performance is improved by 13% in\nmacro-F1 and 5% in weighted-F1 from the BERT-base model.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACM Transactions on Information Systems",
    "pdf_url": "http://arxiv.org/pdf/2404.07229v1",
    "published_date": "2024-04-03 08:48:50 UTC",
    "updated_date": "2024-04-03 08:48:50 UTC"
  },
  {
    "arxiv_id": "2404.02569v2",
    "title": "SliceIt! -- A Dual Simulator Framework for Learning Robot Food Slicing",
    "authors": [
      "Cristian C. Beltran-Hernandez",
      "Nicolas Erbetti",
      "Masashi Hamaya"
    ],
    "abstract": "Cooking robots can enhance the home experience by reducing the burden of\ndaily chores. However, these robots must perform their tasks dexterously and\nsafely in shared human environments, especially when handling dangerous tools\nsuch as kitchen knives. This study focuses on enabling a robot to autonomously\nand safely learn food-cutting tasks. More specifically, our goal is to enable a\ncollaborative robot or industrial robot arm to perform food-slicing tasks by\nadapting to varying material properties using compliance control. Our approach\ninvolves using Reinforcement Learning (RL) to train a robot to compliantly\nmanipulate a knife, by reducing the contact forces exerted by the food items\nand by the cutting board. However, training the robot in the real world can be\ninefficient, and dangerous, and result in a lot of food waste. Therefore, we\nproposed SliceIt!, a framework for safely and efficiently learning robot\nfood-slicing tasks in simulation. Following a real2sim2real approach, our\nframework consists of collecting a few real food slicing data, calibrating our\ndual simulation environment (a high-fidelity cutting simulator and a robotic\nsimulator), learning compliant control policies on the calibrated simulation\nenvironment, and finally, deploying the policies on the real robot.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to ICRA 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.02569v2",
    "published_date": "2024-04-03 08:42:36 UTC",
    "updated_date": "2024-09-26 05:57:37 UTC"
  },
  {
    "arxiv_id": "2404.02552v1",
    "title": "Solar synthetic imaging: Introducing denoising diffusion probabilistic models on SDO/AIA data",
    "authors": [
      "Francesco P. Ramunno",
      "S. Hackstein",
      "V. Kinakh",
      "M. Drozdova",
      "G. Quetant",
      "A. Csillaghy",
      "S. Voloshynovskiy"
    ],
    "abstract": "Given the rarity of significant solar flares compared to smaller ones,\ntraining effective machine learning models for solar activity forecasting is\nchallenging due to insufficient data. This study proposes using generative deep\nlearning models, specifically a Denoising Diffusion Probabilistic Model (DDPM),\nto create synthetic images of solar phenomena, including flares of varying\nintensities. By employing a dataset from the AIA instrument aboard the SDO\nspacecraft, focusing on the 171 {\\AA} band that captures various solar\nactivities, and classifying images with GOES X-ray measurements based on flare\nintensity, we aim to address the data scarcity issue. The DDPM's performance is\nevaluated using cluster metrics, Frechet Inception Distance (FID), and\nF1-score, showcasing promising results in generating realistic solar imagery.\nWe conduct two experiments: one to train a supervised classifier for event\nidentification and another for basic flare prediction, demonstrating the value\nof synthetic data in managing imbalanced datasets. This research underscores\nthe potential of DDPMs in solar data analysis and forecasting, suggesting\nfurther exploration into their capabilities for solar flare prediction and\napplication in other deep learning and physical tasks.",
    "categories": [
      "astro-ph.SR",
      "astro-ph.IM",
      "cs.AI"
    ],
    "primary_category": "astro-ph.SR",
    "comment": "16 pages, 10 figures. Accepted for publication in Astronomy and\n  Astrophysics (A&A)",
    "pdf_url": "http://arxiv.org/pdf/2404.02552v1",
    "published_date": "2024-04-03 08:18:45 UTC",
    "updated_date": "2024-04-03 08:18:45 UTC"
  },
  {
    "arxiv_id": "2404.02548v2",
    "title": "AI-Tutoring in Software Engineering Education",
    "authors": [
      "Eduard Frankford",
      "Clemens Sauerwein",
      "Patrick Bassner",
      "Stephan Krusche",
      "Ruth Breu"
    ],
    "abstract": "With the rapid advancement of artificial intelligence (AI) in various\ndomains, the education sector is set for transformation. The potential of\nAI-driven tools in enhancing the learning experience, especially in\nprogramming, is immense. However, the scientific evaluation of Large Language\nModels (LLMs) used in Automated Programming Assessment Systems (APASs) as an\nAI-Tutor remains largely unexplored. Therefore, there is a need to understand\nhow students interact with such AI-Tutors and to analyze their experiences. In\nthis paper, we conducted an exploratory case study by integrating the\nGPT-3.5-Turbo model as an AI-Tutor within the APAS Artemis. Through a\ncombination of empirical data collection and an exploratory survey, we\nidentified different user types based on their interaction patterns with the\nAI-Tutor. Additionally, the findings highlight advantages, such as timely\nfeedback and scalability. However, challenges like generic responses and\nstudents' concerns about a learning progress inhibition when using the AI-Tutor\nwere also evident. This research adds to the discourse on AI's role in\neducation.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "11 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.02548v2",
    "published_date": "2024-04-03 08:15:08 UTC",
    "updated_date": "2024-04-05 07:05:06 UTC"
  },
  {
    "arxiv_id": "2407.00828v1",
    "title": "DRL-Based RAT Selection in a Hybrid Vehicular Communication Network",
    "authors": [
      "Badreddine Yacine Yacheur",
      "Toufik Ahmed",
      "Mohamed Mosbah"
    ],
    "abstract": "Cooperative intelligent transport systems rely on a set of\nVehicle-to-Everything (V2X) applications to enhance road safety. Emerging new\nV2X applications like Advanced Driver Assistance Systems (ADASs) and Connected\nAutonomous Driving (CAD) applications depend on a significant amount of shared\ndata and require high reliability, low end-to-end (E2E) latency, and high\nthroughput. However, present V2X communication technologies such as ITS-G5 and\nC-V2X (Cellular V2X) cannot satisfy these requirements alone. In this paper, we\npropose an intelligent, scalable hybrid vehicular communication architecture\nthat leverages the performance of multiple Radio Access Technologies (RATs) to\nmeet the needs of these applications. Then, we propose a communication mode\nselection algorithm based on Deep Reinforcement Learning (DRL) to maximize the\nnetwork's reliability while limiting resource consumption. Finally, we assess\nour work using the platooning scenario that requires high reliability.\nNumerical results reveal that the hybrid vehicular communication architecture\nhas the potential to enhance the packet reception rate (PRR) by up to 30%\ncompared to both the static RAT selection strategy and the multi-criteria\ndecision-making (MCDM) selection algorithm. Additionally, it improves the\nefficiency of the redundant communication mode by 20% regarding resource\nconsumption",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00828v1",
    "published_date": "2024-04-03 08:13:07 UTC",
    "updated_date": "2024-04-03 08:13:07 UTC"
  },
  {
    "arxiv_id": "2404.02545v2",
    "title": "Grid-Mapping Pseudo-Count Constraint for Offline Reinforcement Learning",
    "authors": [
      "Yi Shen",
      "Hanyan Huang"
    ],
    "abstract": "Offline reinforcement learning learns from a static dataset without\ninteracting with environments, which ensures security and thus owns a good\napplication prospect. However, directly applying naive reinforcement learning\nalgorithm usually fails in an offline environment due to inaccurate Q value\napproximation caused by out-of-distribution (OOD) state-actions. It is an\neffective way to solve this problem by penalizing the Q-value of OOD\nstate-actions. Among the methods of punishing OOD state-actions, count-based\nmethods have achieved good results in discrete domains in a simple form.\nInspired by it, a novel pseudo-count method for continuous domains called\nGrid-Mapping Pseudo-Count method (GPC) is proposed by extending the count-based\nmethod from discrete to continuous domains. Firstly, the continuous state and\naction space are mapped to discrete space using Grid-Mapping, then the Q-values\nof OOD state-actions are constrained through pseudo-count. Secondly, the\ntheoretical proof is given to show that GPC can obtain appropriate uncertainty\nconstraints under fewer assumptions than other pseudo-count methods. Thirdly,\nGPC is combined with Soft Actor-Critic algorithm (SAC) to get a new algorithm\ncalled GPC-SAC. Lastly, experiments on D4RL datasets are given to show that\nGPC-SAC has better performance and less computational cost than other\nalgorithms that constrain the Q-value.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02545v2",
    "published_date": "2024-04-03 08:03:27 UTC",
    "updated_date": "2024-11-07 09:20:39 UTC"
  },
  {
    "arxiv_id": "2404.02543v3",
    "title": "Unbiased Learning to Rank Meets Reality: Lessons from Baidu's Large-Scale Search Dataset",
    "authors": [
      "Philipp Hager",
      "Romain Deffayet",
      "Jean-Michel Renders",
      "Onno Zoeter",
      "Maarten de Rijke"
    ],
    "abstract": "Unbiased learning-to-rank (ULTR) is a well-established framework for learning\nfrom user clicks, which are often biased by the ranker collecting the data.\nWhile theoretically justified and extensively tested in simulation, ULTR\ntechniques lack empirical validation, especially on modern search engines. The\nBaidu-ULTR dataset released for the WSDM Cup 2023, collected from Baidu's\nsearch engine, offers a rare opportunity to assess the real-world performance\nof prominent ULTR techniques. Despite multiple submissions during the WSDM Cup\n2023 and the subsequent NTCIR ULTRE-2 task, it remains unclear whether the\nobserved improvements stem from applying ULTR or other learning techniques.\n  In this work, we revisit and extend the available experiments on the\nBaidu-ULTR dataset. We find that standard unbiased learning-to-rank techniques\nrobustly improve click predictions but struggle to consistently improve ranking\nperformance, especially considering the stark differences obtained by choice of\nranking loss and query-document features. Our experiments reveal that gains in\nclick prediction do not necessarily translate to enhanced ranking performance\non expert relevance annotations, implying that conclusions strongly depend on\nhow success is measured in this benchmark.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02543v3",
    "published_date": "2024-04-03 08:00:46 UTC",
    "updated_date": "2024-05-15 14:04:20 UTC"
  },
  {
    "arxiv_id": "2405.14879v1",
    "title": "Automatic Coral Detection with YOLO: A Deep Learning Approach for Efficient and Accurate Coral Reef Monitoring",
    "authors": [
      "Ouassine Younes",
      "Zahir Jihad",
      "Conruyt Noël",
      "Kayal Mohsen",
      "A. Martin Philippe",
      "Chenin Eric",
      "Bigot Lionel",
      "Vignes Lebbe Regine"
    ],
    "abstract": "Coral reefs are vital ecosystems that are under increasing threat due to\nlocal human impacts and climate change. Efficient and accurate monitoring of\ncoral reefs is crucial for their conservation and management. In this paper, we\npresent an automatic coral detection system utilizing the You Only Look Once\n(YOLO) deep learning model, which is specifically tailored for underwater\nimagery analysis. To train and evaluate our system, we employ a dataset\nconsisting of 400 original underwater images. We increased the number of\nannotated images to 580 through image manipulation using data augmentation\ntechniques, which can improve the model's performance by providing more diverse\nexamples for training. The dataset is carefully collected from underwater\nvideos that capture various coral reef environments, species, and lighting\nconditions. Our system leverages the YOLOv5 algorithm's real-time object\ndetection capabilities, enabling efficient and accurate coral detection. We\nused YOLOv5 to extract discriminating features from the annotated dataset,\nenabling the system to generalize, including previously unseen underwater\nimages. The successful implementation of the automatic coral detection system\nwith YOLOv5 on our original image dataset highlights the potential of advanced\ncomputer vision techniques for coral reef research and conservation. Further\nresearch will focus on refining the algorithm to handle challenging underwater\nimage conditions, and expanding the dataset to incorporate a wider range of\ncoral species and spatio-temporal variations.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.14879v1",
    "published_date": "2024-04-03 08:00:46 UTC",
    "updated_date": "2024-04-03 08:00:46 UTC"
  },
  {
    "arxiv_id": "2404.02534v1",
    "title": "ANGOFA: Leveraging OFA Embedding Initialization and Synthetic Data for Angolan Language Model",
    "authors": [
      "Osvaldo Luamba Quinjica",
      "David Ifeoluwa Adelani"
    ],
    "abstract": "In recent years, the development of pre-trained language models (PLMs) has\ngained momentum, showcasing their capacity to transcend linguistic barriers and\nfacilitate knowledge transfer across diverse languages. However, this progress\nhas predominantly bypassed the inclusion of very-low resource languages,\ncreating a notable void in the multilingual landscape. This paper addresses\nthis gap by introducing four tailored PLMs specifically finetuned for Angolan\nlanguages, employing a Multilingual Adaptive Fine-tuning (MAFT) approach. In\nthis paper, we survey the role of informed embedding initialization and\nsynthetic data in enhancing the performance of MAFT models in downstream tasks.\nWe improve baseline over SOTA AfroXLMR-base (developed through MAFT) and OFA\n(an effective embedding initialization) by 12.3 and 3.8 points respectively.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02534v1",
    "published_date": "2024-04-03 07:44:38 UTC",
    "updated_date": "2024-04-03 07:44:38 UTC"
  },
  {
    "arxiv_id": "2404.02532v1",
    "title": "Learn to Disguise: Avoid Refusal Responses in LLM's Defense via a Multi-agent Attacker-Disguiser Game",
    "authors": [
      "Qianqiao Xu",
      "Zhiliang Tian",
      "Hongyan Wu",
      "Zhen Huang",
      "Yiping Song",
      "Feng Liu",
      "Dongsheng Li"
    ],
    "abstract": "With the enhanced performance of large models on natural language processing\ntasks, potential moral and ethical issues of large models arise. There exist\nmalicious attackers who induce large models to jailbreak and generate\ninformation containing illegal, privacy-invasive information through techniques\nsuch as prompt engineering. As a result, large models counter malicious\nattackers' attacks using techniques such as safety alignment. However, the\nstrong defense mechanism of the large model through rejection replies is easily\nidentified by attackers and used to strengthen attackers' capabilities. In this\npaper, we propose a multi-agent attacker-disguiser game approach to achieve a\nweak defense mechanism that allows the large model to both safely reply to the\nattacker and hide the defense intent. First, we construct a multi-agent\nframework to simulate attack and defense scenarios, playing different roles to\nbe responsible for attack, disguise, safety evaluation, and disguise evaluation\ntasks. After that, we design attack and disguise game algorithms to optimize\nthe game strategies of the attacker and the disguiser and use the curriculum\nlearning process to strengthen the capabilities of the agents. The experiments\nverify that the method in this paper is more effective in strengthening the\nmodel's ability to disguise the defense intent compared with other methods.\nMoreover, our approach can adapt any black-box large model to assist the model\nin defense and does not suffer from model version iterations.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.02532v1",
    "published_date": "2024-04-03 07:43:11 UTC",
    "updated_date": "2024-04-03 07:43:11 UTC"
  },
  {
    "arxiv_id": "2404.02530v2",
    "title": "Manipulating and Mitigating Generative Model Biases without Retraining",
    "authors": [
      "Jordan Vice",
      "Naveed Akhtar",
      "Richard Hartley",
      "Ajmal Mian"
    ],
    "abstract": "Text-to-image (T2I) generative models have gained increased popularity in the\npublic domain. While boasting impressive user-guided generative abilities,\ntheir black-box nature exposes users to intentionally- and intrinsically-biased\noutputs. Bias manipulation (and mitigation) techniques typically rely on\ncareful tuning of learning parameters and training data to adjust decision\nboundaries to influence model bias characteristics, which is often\ncomputationally demanding. We propose a dynamic and computationally efficient\nmanipulation of T2I model biases by exploiting their rich language embedding\nspaces without model retraining. We show that leveraging foundational vector\nalgebra allows for a convenient control over language model embeddings to shift\nT2I model outputs and control the distribution of generated classes. As a\nby-product, this control serves as a form of precise prompt engineering to\ngenerate images which are generally implausible using regular text prompts. We\ndemonstrate a constructive application of our technique by balancing the\nfrequency of social classes in generated images, effectively balancing class\ndistributions across three social bias dimensions. We also highlight a negative\nimplication of bias manipulation by framing our method as a backdoor attack\nwith severity control using semantically-null input triggers, reporting up to\n100% attack success rate.\n  Key-words: Text-to-Image Models, Generative Models, Bias, Prompt Engineering,\nBackdoor Attacks",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ECCV 2024 WS: Workshop on critical evaluation of\n  generative models and their impact on society (CEGIS)",
    "pdf_url": "http://arxiv.org/pdf/2404.02530v2",
    "published_date": "2024-04-03 07:33:30 UTC",
    "updated_date": "2024-09-17 01:07:58 UTC"
  },
  {
    "arxiv_id": "2404.02523v1",
    "title": "Text-driven Affordance Learning from Egocentric Vision",
    "authors": [
      "Tomoya Yoshida",
      "Shuhei Kurita",
      "Taichi Nishimura",
      "Shinsuke Mori"
    ],
    "abstract": "Visual affordance learning is a key component for robots to understand how to\ninteract with objects. Conventional approaches in this field rely on\npre-defined objects and actions, falling short of capturing diverse\ninteractions in realworld scenarios. The key idea of our approach is employing\ntextual instruction, targeting various affordances for a wide range of objects.\nThis approach covers both hand-object and tool-object interactions. We\nintroduce text-driven affordance learning, aiming to learn contact points and\nmanipulation trajectories from an egocentric view following textual\ninstruction. In our task, contact points are represented as heatmaps, and the\nmanipulation trajectory as sequences of coordinates that incorporate both\nlinear and rotational movements for various manipulations. However, when we\ngather data for this task, manual annotations of these diverse interactions are\ncostly. To this end, we propose a pseudo dataset creation pipeline and build a\nlarge pseudo-training dataset: TextAFF80K, consisting of over 80K instances of\nthe contact points, trajectories, images, and text tuples. We extend existing\nreferring expression comprehension models for our task, and experimental\nresults show that our approach robustly handles multiple affordances, serving\nas a new standard for affordance learning in real-world scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02523v1",
    "published_date": "2024-04-03 07:23:03 UTC",
    "updated_date": "2024-04-03 07:23:03 UTC"
  },
  {
    "arxiv_id": "2404.02937v5",
    "title": "Towards Explainable Traffic Flow Prediction with Large Language Models",
    "authors": [
      "Xusen Guo",
      "Qiming Zhang",
      "Junyue Jiang",
      "Mingxing Peng",
      "Meixin Zhu",
      "Hao",
      "Yang"
    ],
    "abstract": "Traffic forecasting is crucial for intelligent transportation systems. It has\nexperienced significant advancements thanks to the power of deep learning in\ncapturing latent patterns of traffic data. However, recent deep-learning\narchitectures require intricate model designs and lack an intuitive\nunderstanding of the mapping from input data to predicted results. Achieving\nboth accuracy and explainability in traffic prediction models remains a\nchallenge due to the complexity of traffic data and the inherent opacity of\ndeep learning models. To tackle these challenges, we propose a Traffic flow\nPrediction model based on Large Language Models (LLMs) to generate explainable\ntraffic predictions, named xTP-LLM. By transferring multi-modal traffic data\ninto natural language descriptions, xTP-LLM captures complex time-series\npatterns and external factors from comprehensive traffic data. The LLM\nframework is fine-tuned using language-based instructions to align with\nspatial-temporal traffic flow data. Empirically, xTP-LLM shows competitive\naccuracy compared with deep learning baselines, while providing an intuitive\nand reliable explanation for predictions. This paper contributes to advancing\nexplainable traffic prediction models and lays a foundation for future\nexploration of LLM applications in transportation. To the best of our\nknowledge, this is the first study to use LLM for explainable prediction of\ntraffic flows.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "31pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.02937v5",
    "published_date": "2024-04-03 07:14:15 UTC",
    "updated_date": "2024-09-03 11:32:50 UTC"
  },
  {
    "arxiv_id": "2404.02515v3",
    "title": "Tightly-Coupled LiDAR-IMU-Wheel Odometry with Online Calibration of a Kinematic Model for Skid-Steering Robots",
    "authors": [
      "Taku Okawara",
      "Kenji Koide",
      "Shuji Oishi",
      "Masashi Yokozuka",
      "Atsuhiko Banno",
      "Kentaro Uno",
      "Kazuya Yoshida"
    ],
    "abstract": "Tunnels and long corridors are challenging environments for mobile robots\nbecause a LiDAR point cloud should degenerate in these environments. To tackle\npoint cloud degeneration, this study presents a tightly-coupled LiDAR-IMU-wheel\nodometry algorithm with an online calibration for skid-steering robots. We\npropose a full linear wheel odometry factor, which not only serves as a motion\nconstraint but also performs the online calibration of kinematic models for\nskid-steering robots. Despite the dynamically changing kinematic model (e.g.,\nwheel radii changes caused by tire pressures) and terrain conditions, our\nmethod can address the model error via online calibration. Moreover, our method\nenables an accurate localization in cases of degenerated environments, such as\nlong and straight corridors, by calibration while the LiDAR-IMU fusion\nsufficiently operates. Furthermore, we estimate the uncertainty (i.e.,\ncovariance matrix) of the wheel odometry online for creating a reasonable\nconstraint. The proposed method is validated through three experiments. The\nfirst indoor experiment shows that the proposed method is robust in severe\ndegeneracy cases (long corridors) and changes in the wheel radii. The second\noutdoor experiment demonstrates that our method accurately estimates the sensor\ntrajectory despite being in rough outdoor terrain owing to online uncertainty\nestimation of wheel odometry. The third experiment shows the proposed online\ncalibration enables robust odometry estimation in changing terrains.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted by IEEE Access journal (12 September) open-source:\n  https://github.com/TakuOkawara/full_linear_wheel_odometry_factor",
    "pdf_url": "http://arxiv.org/pdf/2404.02515v3",
    "published_date": "2024-04-03 07:07:29 UTC",
    "updated_date": "2024-09-12 10:29:02 UTC"
  },
  {
    "arxiv_id": "2404.02510v1",
    "title": "An Interpretable Client Decision Tree Aggregation process for Federated Learning",
    "authors": [
      "Alberto Argente-Garrido",
      "Cristina Zuheros",
      "M. Victoria Luzón",
      "Francisco Herrera"
    ],
    "abstract": "Trustworthy Artificial Intelligence solutions are essential in today's\ndata-driven applications, prioritizing principles such as robustness, safety,\ntransparency, explainability, and privacy among others. This has led to the\nemergence of Federated Learning as a solution for privacy and distributed\nmachine learning. While decision trees, as self-explanatory models, are ideal\nfor collaborative model training across multiple devices in\nresource-constrained environments such as federated learning environments for\ninjecting interpretability in these models. Decision tree structure makes the\naggregation in a federated learning environment not trivial. They require\ntechniques that can merge their decision paths without introducing bias or\noverfitting while keeping the aggregated decision trees robust and\ngeneralizable. In this paper, we propose an Interpretable Client Decision Tree\nAggregation process for Federated Learning scenarios that keeps the\ninterpretability and the precision of the base decision trees used for the\naggregation. This model is based on aggregating multiple decision paths of the\ndecision trees and can be used on different decision tree types, such as ID3\nand CART. We carry out the experiments within four datasets, and the analysis\nshows that the tree built with the model improves the local models, and\noutperforms the state-of-the-art.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to Information Science Journal",
    "pdf_url": "http://arxiv.org/pdf/2404.02510v1",
    "published_date": "2024-04-03 06:53:56 UTC",
    "updated_date": "2024-04-03 06:53:56 UTC"
  },
  {
    "arxiv_id": "2404.02508v1",
    "title": "VIAssist: Adapting Multi-modal Large Language Models for Users with Visual Impairments",
    "authors": [
      "Bufang Yang",
      "Lixing He",
      "Kaiwei Liu",
      "Zhenyu Yan"
    ],
    "abstract": "Individuals with visual impairments, encompassing both partial and total\ndifficulties in visual perception, are referred to as visually impaired (VI)\npeople. An estimated 2.2 billion individuals worldwide are affected by visual\nimpairments. Recent advancements in multi-modal large language models (MLLMs)\nhave showcased their extraordinary capabilities across various domains. It is\ndesirable to help VI individuals with MLLMs' great capabilities of visual\nunderstanding and reasoning. However, it is challenging for VI people to use\nMLLMs due to the difficulties in capturing the desirable images to fulfill\ntheir daily requests. For example, the target object is not fully or partially\nplaced in the image. This paper explores how to leverage MLLMs for VI\nindividuals to provide visual-question answers. VIAssist can identify undesired\nimages and provide detailed actions. Finally, VIAssist can provide reliable\nanswers to users' queries based on the images. Our results show that VIAssist\nprovides +0.21 and +0.31 higher BERTScore and ROUGE scores than the baseline,\nrespectively.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to IEEE International Workshop on Foundation Models for\n  Cyber-Physical Systems & Internet of Things (FMSys 2024)",
    "pdf_url": "http://arxiv.org/pdf/2404.02508v1",
    "published_date": "2024-04-03 06:53:27 UTC",
    "updated_date": "2024-04-03 06:53:27 UTC"
  },
  {
    "arxiv_id": "2404.02505v1",
    "title": "Dynamic Demonstration Retrieval and Cognitive Understanding for Emotional Support Conversation",
    "authors": [
      "Zhe Xu",
      "Daoyuan Chen",
      "Jiayi Kuang",
      "Zihao Yi",
      "Yaliang Li",
      "Ying Shen"
    ],
    "abstract": "Emotional Support Conversation (ESC) systems are pivotal in providing\nempathetic interactions, aiding users through negative emotional states by\nunderstanding and addressing their unique experiences. In this paper, we tackle\ntwo key challenges in ESC: enhancing contextually relevant and empathetic\nresponse generation through dynamic demonstration retrieval, and advancing\ncognitive understanding to grasp implicit mental states comprehensively. We\nintroduce Dynamic Demonstration Retrieval and Cognitive-Aspect Situation\nUnderstanding (\\ourwork), a novel approach that synergizes these elements to\nimprove the quality of support provided in ESCs. By leveraging in-context\nlearning and persona information, we introduce an innovative retrieval\nmechanism that selects informative and personalized demonstration pairs. We\nalso propose a cognitive understanding module that utilizes four cognitive\nrelationships from the ATOMIC knowledge source to deepen situational awareness\nof help-seekers' mental states. Our supportive decoder integrates information\nfrom diverse knowledge sources, underpinning response generation that is both\nempathetic and cognitively aware. The effectiveness of \\ourwork is demonstrated\nthrough extensive automatic and human evaluations, revealing substantial\nimprovements over numerous state-of-the-art models, with up to 13.79\\%\nenhancement in overall performance of ten metrics. Our codes are available for\npublic access to facilitate further research and development.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "Accpeted by SIGIR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.02505v1",
    "published_date": "2024-04-03 06:47:15 UTC",
    "updated_date": "2024-04-03 06:47:15 UTC"
  },
  {
    "arxiv_id": "2404.02499v2",
    "title": "Learning Generalized Policies for Fully Observable Non-Deterministic Planning Domains",
    "authors": [
      "Till Hofmann",
      "Hector Geffner"
    ],
    "abstract": "General policies represent reactive strategies for solving large families of\nplanning problems like the infinite collection of solvable instances from a\ngiven domain. Methods for learning such policies from a collection of small\ntraining instances have been developed successfully for classical domains. In\nthis work, we extend the formulations and the resulting combinatorial methods\nfor learning general policies over fully observable, non-deterministic (FOND)\ndomains. We also evaluate the resulting approach experimentally over a number\nof benchmark domains in FOND planning, present the general policies that result\nin some of these domains, and prove their correctness. The method for learning\ngeneral policies for FOND planning can actually be seen as an alternative FOND\nplanning method that searches for solutions, not in the given state space but\nin an abstract space defined by features that must be learned as well.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "presented at IJCAI'24",
    "pdf_url": "http://arxiv.org/pdf/2404.02499v2",
    "published_date": "2024-04-03 06:25:42 UTC",
    "updated_date": "2024-05-13 09:20:26 UTC"
  },
  {
    "arxiv_id": "2404.02491v4",
    "title": "Measuring Social Norms of Large Language Models",
    "authors": [
      "Ye Yuan",
      "Kexin Tang",
      "Jianhao Shen",
      "Ming Zhang",
      "Chenguang Wang"
    ],
    "abstract": "We present a new challenge to examine whether large language models\nunderstand social norms. In contrast to existing datasets, our dataset requires\na fundamental understanding of social norms to solve. Our dataset features the\nlargest set of social norm skills, consisting of 402 skills and 12,383\nquestions covering a wide set of social norms ranging from opinions and\narguments to culture and laws. We design our dataset according to the K-12\ncurriculum. This enables the direct comparison of the social understanding of\nlarge language models to humans, more specifically, elementary students. While\nprior work generates nearly random accuracy on our benchmark, recent large\nlanguage models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the\nperformance significantly, only slightly below human performance. We then\npropose a multi-agent framework based on large language models to improve the\nmodels' ability to understand social norms. This method further improves large\nlanguage models to be on par with humans. Given the increasing adoption of\nlarge language models in real-world applications, our finding is particularly\nimportant and presents a unique direction for future improvements.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02491v4",
    "published_date": "2024-04-03 05:58:57 UTC",
    "updated_date": "2024-05-22 05:23:45 UTC"
  },
  {
    "arxiv_id": "2404.02484v2",
    "title": "New methods for drug synergy prediction: a mini-review",
    "authors": [
      "Fatemeh Abbasi",
      "Juho Rousu"
    ],
    "abstract": "In this mini-review, we explore the new prediction methods for drug\ncombination synergy relying on high-throughput combinatorial screens. The fast\nprogress of the field is witnessed in the more than thirty original machine\nlearning methods published since 2021, a clear majority of them based on deep\nlearning techniques. We aim to put these papers under a unifying lens by\nhighlighting the core technologies, the data sources, the input data types and\nsynergy scores used in the methods, as well as the prediction scenarios and\nevaluation protocols that the papers deal with. Our finding is that the best\nmethods accurately solve the synergy prediction scenarios involving known drugs\nor cell lines while the scenarios involving new drugs or cell lines still fall\nshort of an accurate prediction level.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM",
      "I.2.6; J.3"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02484v2",
    "published_date": "2024-04-03 05:44:03 UTC",
    "updated_date": "2024-04-15 11:48:37 UTC"
  },
  {
    "arxiv_id": "2404.02478v1",
    "title": "FedSelect: Personalized Federated Learning with Customized Selection of Parameters for Fine-Tuning",
    "authors": [
      "Rishub Tamirisa",
      "Chulin Xie",
      "Wenxuan Bao",
      "Andy Zhou",
      "Ron Arel",
      "Aviv Shamsian"
    ],
    "abstract": "Standard federated learning approaches suffer when client data distributions\nhave sufficient heterogeneity. Recent methods addressed the client data\nheterogeneity issue via personalized federated learning (PFL) - a class of FL\nalgorithms aiming to personalize learned global knowledge to better suit the\nclients' local data distributions. Existing PFL methods usually decouple global\nupdates in deep neural networks by performing personalization on particular\nlayers (i.e. classifier heads) and global aggregation for the rest of the\nnetwork. However, preselecting network layers for personalization may result in\nsuboptimal storage of global knowledge. In this work, we propose FedSelect, a\nnovel PFL algorithm inspired by the iterative subnetwork discovery procedure\nused for the Lottery Ticket Hypothesis. FedSelect incrementally expands\nsubnetworks to personalize client parameters, concurrently conducting global\naggregations on the remaining parameters. This approach enables the\npersonalization of both client parameters and subnetwork structure during the\ntraining process. Finally, we show that FedSelect outperforms recent\nstate-of-the-art PFL algorithms under challenging client data heterogeneity\nsettings and demonstrates robustness to various real-world distributional\nshifts. Our code is available at https://github.com/lapisrocks/fedselect.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.02478v1",
    "published_date": "2024-04-03 05:36:21 UTC",
    "updated_date": "2024-04-03 05:36:21 UTC"
  },
  {
    "arxiv_id": "2404.02477v1",
    "title": "Enhancing Sum-Rate Performance in Constrained Multicell Networks: A Low-Information Exchange Approach",
    "authors": [
      "Youjin Kim",
      "Jonggyu Jang",
      "Hyun Jong Yang"
    ],
    "abstract": "Despite the extensive research on massive MIMO systems for 5G\ntelecommunications and beyond, the reality is that many deployed base stations\nare equipped with a limited number of antennas rather than supporting massive\nMIMO configurations. Furthermore, while the cell-less network concept, which\neliminates cell boundaries, is under investigation, practical deployments often\ngrapple with significantly limited backhaul connection capacities between base\nstations. This letter explores techniques to maximize the sum-rate performance\nwithin the constraints of these more realistically equipped multicell networks.\nWe propose an innovative approach that dramatically reduces the need for\ninformation exchange between base stations to a mere few bits, in stark\ncontrast to conventional methods that require the exchange of hundreds of bits.\nOur proposed method not only addresses the limitations imposed by current\nnetwork infrastructure but also showcases significantly improved performance\nunder these constrained conditions.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "5 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.02477v1",
    "published_date": "2024-04-03 05:34:32 UTC",
    "updated_date": "2024-04-03 05:34:32 UTC"
  },
  {
    "arxiv_id": "2404.02476v5",
    "title": "Deep Reinforcement Learning for Traveling Purchaser Problems",
    "authors": [
      "Haofeng Yuan",
      "Rongping Zhu",
      "Wanlu Yang",
      "Shiji Song",
      "Keyou You",
      "Wei Fan",
      "C. L. Philip Chen"
    ],
    "abstract": "The traveling purchaser problem (TPP) is an important combinatorial\noptimization problem with broad applications. Due to the coupling between\nrouting and purchasing, existing works on TPPs commonly address route\nconstruction and purchase planning simultaneously, which, however, leads to\nexact methods with high computational cost and heuristics with sophisticated\ndesign but limited performance. In sharp contrast, we propose a novel approach\nbased on deep reinforcement learning (DRL), which addresses route construction\nand purchase planning separately, while evaluating and optimizing the solution\nfrom a global perspective. The key components of our approach include a\nbipartite graph representation for TPPs to capture the market-product\nrelations, and a policy network that extracts information from the bipartite\ngraph and uses it to sequentially construct the route. One significant benefit\nof our framework is that we can efficiently construct the route using the\npolicy network, and once the route is determined, the associated purchasing\nplan can be easily derived through linear programming, while, leveraging DRL,\nwe can train the policy network to optimize the global solution objective.\nFurthermore, by introducing a meta-learning strategy, the policy network can be\ntrained stably on large-sized TPP instances, and generalize well across\ninstances of varying sizes and distributions, even to much larger instances\nthat are never seen during training. Experiments on various synthetic TPP\ninstances and the TPPLIB benchmark demonstrate that our DRL-based approach can\nsignificantly outperform well-established TPP heuristics, reducing the\noptimality gap by 40%-90%, and also showing an advantage in runtime, especially\non large-sized instances.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02476v5",
    "published_date": "2024-04-03 05:32:10 UTC",
    "updated_date": "2024-10-14 13:33:42 UTC"
  },
  {
    "arxiv_id": "2404.02474v1",
    "title": "uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?",
    "authors": [
      "Pouya Sadeghi",
      "Amirhossein Abaskohi",
      "Yadollah Yaghoobzadeh"
    ],
    "abstract": "Inspired by human cognition, Jiang et al.(2023c) create a benchmark for\nassessing LLMs' lateral thinking-thinking outside the box. Building upon this\nbenchmark, we investigate how different prompting methods enhance LLMs'\nperformance on this task to reveal their inherent power for outside-the-box\nthinking ability. Through participating in SemEval-2024, task 9, Sentence\nPuzzle sub-task, we explore prompt engineering methods: chain of thoughts (CoT)\nand direct prompting, enhancing with informative descriptions, and employing\ncontextualizing prompts using a retrieval augmented generation (RAG) pipeline.\nOur experiments involve three LLMs including GPT-3.5, GPT-4, and\nZephyr-7B-beta. We generate a dataset of thinking paths between riddles and\noptions using GPT-4, validated by humans for quality. Findings indicate that\ncompressed informative prompts enhance performance. Dynamic in-context learning\nenhances model performance significantly. Furthermore, fine-tuning Zephyr on\nour dataset enhances performance across other commonsense datasets,\nunderscoring the value of innovative thinking.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 5 figures, 6 tables, Proceedings of the 18th International\n  Workshop on Semantic Evaluation (SemEval-2024) @ NAACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.02474v1",
    "published_date": "2024-04-03 05:31:59 UTC",
    "updated_date": "2024-04-03 05:31:59 UTC"
  },
  {
    "arxiv_id": "2404.02466v1",
    "title": "Prompting for Numerical Sequences: A Case Study on Market Comment Generation",
    "authors": [
      "Masayuki Kawarada",
      "Tatsuya Ishigaki",
      "Hiroya Takamura"
    ],
    "abstract": "Large language models (LLMs) have been applied to a wide range of\ndata-to-text generation tasks, including tables, graphs, and time-series\nnumerical data-to-text settings. While research on generating prompts for\nstructured data such as tables and graphs is gaining momentum, in-depth\ninvestigations into prompting for time-series numerical data are lacking.\nTherefore, this study explores various input representations, including\nsequences of tokens and structured formats such as HTML, LaTeX, and\nPython-style codes. In our experiments, we focus on the task of Market Comment\nGeneration, which involves taking a numerical sequence of stock prices as input\nand generating a corresponding market comment. Contrary to our expectations,\nthe results show that prompts resembling programming languages yield better\noutcomes, whereas those similar to natural languages and longer formats, such\nas HTML and LaTeX, are less effective. Our findings offer insights into\ncreating effective prompts for tasks that generate text from numerical\nsequences.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to LREC-COLING2024 long paper",
    "pdf_url": "http://arxiv.org/pdf/2404.02466v1",
    "published_date": "2024-04-03 05:10:11 UTC",
    "updated_date": "2024-04-03 05:10:11 UTC"
  },
  {
    "arxiv_id": "2404.02460v1",
    "title": "TSNet:A Two-stage Network for Image Dehazing with Multi-scale Fusion and Adaptive Learning",
    "authors": [
      "Xiaolin Gong",
      "Zehan Zheng",
      "Heyuan Du"
    ],
    "abstract": "Image dehazing has been a popular topic of research for a long time. Previous\ndeep learning-based image dehazing methods have failed to achieve satisfactory\ndehazing effects on both synthetic datasets and real-world datasets, exhibiting\npoor generalization. Moreover, single-stage networks often result in many\nregions with artifacts and color distortion in output images. To address these\nissues, this paper proposes a two-stage image dehazing network called TSNet,\nmainly consisting of the multi-scale fusion module (MSFM) and the adaptive\nlearning module (ALM). Specifically, MSFM and ALM enhance the generalization of\nTSNet. The MSFM can obtain large receptive fields at multiple scales and\nintegrate features at different frequencies to reduce the differences between\ninputs and learning objectives. The ALM can actively learn of regions of\ninterest in images and restore texture details more effectively. Additionally,\nTSNet is designed as a two-stage network, where the first-stage network\nperforms image dehazing, and the second-stage network is employed to improve\nissues such as artifacts and color distortion present in the results of the\nfirst-stage network. We also change the learning objective from ground truth\nimages to opposite fog maps, which improves the learning efficiency of TSNet.\nExtensive experiments demonstrate that TSNet exhibits superior dehazing\nperformance on both synthetic and real-world datasets compared to previous\nstate-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 10 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.02460v1",
    "published_date": "2024-04-03 05:02:46 UTC",
    "updated_date": "2024-04-03 05:02:46 UTC"
  },
  {
    "arxiv_id": "2404.02456v2",
    "title": "PhonologyBench: Evaluating Phonological Skills of Large Language Models",
    "authors": [
      "Ashima Suvarna",
      "Harshita Khandelwal",
      "Nanyun Peng"
    ],
    "abstract": "Phonology, the study of speech's structure and pronunciation rules, is a\ncritical yet often overlooked component in Large Language Model (LLM) research.\nLLMs are widely used in various downstream applications that leverage phonology\nsuch as educational tools and poetry generation. Moreover, LLMs can potentially\nlearn imperfect associations between orthographic and phonological forms from\nthe training data. Thus, it is imperative to benchmark the phonological skills\nof LLMs. To this end, we present PhonologyBench, a novel benchmark consisting\nof three diagnostic tasks designed to explicitly test the phonological skills\nof LLMs in English: grapheme-to-phoneme conversion, syllable counting, and\nrhyme word generation. Despite having no access to speech data, LLMs showcased\nnotable performance on the PhonologyBench tasks. However, we observe a\nsignificant gap of 17% and 45% on Rhyme Word Generation and Syllable counting,\nrespectively, when compared to humans. Our findings underscore the importance\nof studying LLM performance on phonological tasks that inadvertently impact\nreal-world applications. Furthermore, we encourage researchers to choose LLMs\nthat perform well on the phonological task that is closely related to the\ndownstream application since we find that no single model consistently\noutperforms the others on all the tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 7 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.02456v2",
    "published_date": "2024-04-03 04:53:14 UTC",
    "updated_date": "2024-04-05 04:55:24 UTC"
  },
  {
    "arxiv_id": "2404.02454v4",
    "title": "Techniques for Measuring the Inferential Strength of Forgetting Policies",
    "authors": [
      "Patrick Doherty",
      "Andrzej Szalas"
    ],
    "abstract": "The technique of forgetting in knowledge representation has been shown to be\na powerful and useful knowledge engineering tool with widespread application.\nYet, very little research has been done on how different policies of\nforgetting, or use of different forgetting operators, affects the inferential\nstrength of the original theory. The goal of this paper is to define loss\nfunctions for measuring changes in inferential strength based on intuitions\nfrom model counting and probability theory. Properties of such loss measures\nare studied and a pragmatic knowledge engineering tool is proposed for\ncomputing loss measures using ProbLog. The paper includes a working methodology\nfor studying and determining the strength of different forgetting policies, in\naddition to concrete examples showing how to apply the theoretical results\nusing ProbLog. Although the focus is on forgetting, the results are much more\ngeneral and should have wider application to other areas.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02454v4",
    "published_date": "2024-04-03 04:50:43 UTC",
    "updated_date": "2025-02-20 09:55:22 UTC"
  },
  {
    "arxiv_id": "2404.02450v1",
    "title": "Task Agnostic Architecture for Algorithm Induction via Implicit Composition",
    "authors": [
      "Sahil J. Sindhi",
      "Ignas Budvytis"
    ],
    "abstract": "Different fields in applied machine learning such as computer vision, speech\nor natural language processing have been building domain-specialised solutions.\nCurrently, we are witnessing an opposing trend towards developing more\ngeneralist architectures, driven by Large Language Models and multi-modal\nfoundational models. These architectures are designed to tackle a variety of\ntasks, including those previously unseen and using inputs across multiple\nmodalities. Taking this trend of generalization to the extreme suggests the\npossibility of a single deep network architecture capable of solving all tasks.\nThis position paper aims to explore developing such a unified architecture and\nproposes a theoretical framework of how it could be constructed. Our proposal\nis based on the following assumptions. Firstly, tasks are solved by following a\nsequence of instructions, typically implemented in code for conventional\ncomputing hardware, which inherently operates sequentially. Second, recent\nGenerative AI, especially Transformer-based models, demonstrate potential as an\narchitecture capable of constructing algorithms for a wide range of domains.\nFor example, GPT-4 shows exceptional capability at in-context learning of novel\ntasks which is hard to explain in any other way than the ability to compose\nnovel solutions from fragments on previously learnt algorithms. Third, the\nobservation that the main missing component in developing a truly generalised\nnetwork is an efficient approach for self-consistent input of previously learnt\nsub-steps of an algorithm and their (implicit) composition during the network's\ninternal forward pass. Our exploration delves into current capabilities and\nlimitations of Transformer-based and other methods in efficient and correct\nalgorithm composition and proposes a Transformer-like architecture as well as a\ndiscrete learning framework to overcome these limitations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 2 figures, 2024 ICLR Generative Models for Decision Making\n  Workshop",
    "pdf_url": "http://arxiv.org/pdf/2404.02450v1",
    "published_date": "2024-04-03 04:31:09 UTC",
    "updated_date": "2024-04-03 04:31:09 UTC"
  },
  {
    "arxiv_id": "2404.02448v2",
    "title": "Electric Vehicle Routing Problem for Emergency Power Supply: Towards Telecom Base Station Relief",
    "authors": [
      "Daisuke Kikuta",
      "Hiroki Ikeuchi",
      "Kengo Tajiri",
      "Yuta Toyama",
      "Masaki Nakamura",
      "Yuusuke Nakano"
    ],
    "abstract": "As a telecom provider, our company has a critical mission to maintain telecom\nservices even during power outages. To accomplish the mission, it is essential\nto maintain the power of the telecom base stations. Here we consider a solution\nwhere electric vehicles (EVs) directly supply power to base stations by\ntraveling to their locations. The goal is to find EV routes that minimize both\nthe total travel distance of all EVs and the number of downed base stations. In\nthis paper, we formulate this routing problem as a new variant of the Electric\nVehicle Routing Problem (EVRP) and propose a solver that combines a rule-based\nvehicle selector and a reinforcement learning (RL)-based node selector. The\nrule of the vehicle selector ensures the exact environmental states when the\nselected EV starts to move. In addition, the node selection by the RL model\nenables fast route generation, which is critical in emergencies. We evaluate\nour solver on both synthetic datasets and real datasets. The results show that\nour solver outperforms baselines in terms of the objective value and\ncomputation time. Moreover, we analyze the generalization and scalability of\nour solver, demonstrating the capability toward unseen settings and large-scale\nproblems. Check also our project page: https://ntt-dkiku.github.io/rl-evrpeps.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "math.OC",
    "comment": "Accepted at AAMAS 2024 (extended abstract). 10 pages, 5 figures. Work\n  in progress",
    "pdf_url": "http://arxiv.org/pdf/2404.02448v2",
    "published_date": "2024-04-03 04:27:07 UTC",
    "updated_date": "2024-04-08 02:46:38 UTC"
  },
  {
    "arxiv_id": "2404.02447v1",
    "title": "A Novel Approach to Breast Cancer Histopathological Image Classification Using Cross-Colour Space Feature Fusion and Quantum-Classical Stack Ensemble Method",
    "authors": [
      "Sambit Mallick",
      "Snigdha Paul",
      "Anindya Sen"
    ],
    "abstract": "Breast cancer classification stands as a pivotal pillar in ensuring timely\ndiagnosis and effective treatment. This study with histopathological images\nunderscores the profound significance of harnessing the synergistic\ncapabilities of colour space ensembling and quantum-classical stacking to\nelevate the precision of breast cancer classification. By delving into the\ndistinct colour spaces of RGB, HSV and CIE L*u*v, the authors initiated a\ncomprehensive investigation guided by advanced methodologies. Employing the\nDenseNet121 architecture for feature extraction the authors have capitalized on\nthe robustness of Random Forest, SVM, QSVC, and VQC classifiers. This research\nencompasses a unique feature fusion technique within the colour space ensemble.\nThis approach not only deepens our comprehension of breast cancer\nclassification but also marks a milestone in personalized medical assessment.\nThe amalgamation of quantum and classical classifiers through stacking emerges\nas a potent catalyst, effectively mitigating the inherent constraints of\nindividual classifiers, paving a robust path towards more dependable and\nrefined breast cancer identification. Through rigorous experimentation and\nmeticulous analysis, fusion of colour spaces like RGB with HSV and RGB with CIE\nL*u*v, presents an classification accuracy, nearing the value of unity. This\nunderscores the transformative potential of our approach, where the fusion of\ndiverse colour spaces and the synergy of quantum and classical realms converge\nto establish a new horizon in medical diagnostics. Thus the implications of\nthis research extend across medical disciplines, offering promising avenues for\nadvancing diagnostic accuracy and treatment efficacy.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02447v1",
    "published_date": "2024-04-03 04:26:50 UTC",
    "updated_date": "2024-04-03 04:26:50 UTC"
  },
  {
    "arxiv_id": "2404.02444v1",
    "title": "The Promises and Pitfalls of Using Language Models to Measure Instruction Quality in Education",
    "authors": [
      "Paiheng Xu",
      "Jing Liu",
      "Nathan Jones",
      "Julie Cohen",
      "Wei Ai"
    ],
    "abstract": "Assessing instruction quality is a fundamental component of any improvement\nefforts in the education system. However, traditional manual assessments are\nexpensive, subjective, and heavily dependent on observers' expertise and\nidiosyncratic factors, preventing teachers from getting timely and frequent\nfeedback. Different from prior research that mostly focuses on low-inference\ninstructional practices on a singular basis, this paper presents the first\nstudy that leverages Natural Language Processing (NLP) techniques to assess\nmultiple high-inference instructional practices in two distinct educational\nsettings: in-person K-12 classrooms and simulated performance tasks for\npre-service teachers. This is also the first study that applies NLP to measure\na teaching practice that is widely acknowledged to be particularly effective\nfor students with special needs. We confront two challenges inherent in\nNLP-based instructional analysis, including noisy and long input data and\nhighly skewed distributions of human ratings. Our results suggest that\npretrained Language Models (PLMs) demonstrate performances comparable to the\nagreement level of human raters for variables that are more discrete and\nrequire lower inference, but their efficacy diminishes with more complex\nteaching practices. Interestingly, using only teachers' utterances as input\nyields strong results for student-centered variables, alleviating common\nconcerns over the difficulty of collecting and transcribing high-quality\nstudent speech data in in-person teaching settings. Our findings highlight both\nthe potential and the limitations of current NLP techniques in the education\ndomain, opening avenues for further exploration.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.02444v1",
    "published_date": "2024-04-03 04:15:29 UTC",
    "updated_date": "2024-04-03 04:15:29 UTC"
  },
  {
    "arxiv_id": "2404.02429v1",
    "title": "AD4RL: Autonomous Driving Benchmarks for Offline Reinforcement Learning with Value-based Dataset",
    "authors": [
      "Dongsu Lee",
      "Chanin Eom",
      "Minhae Kwon"
    ],
    "abstract": "Offline reinforcement learning has emerged as a promising technology by\nenhancing its practicality through the use of pre-collected large datasets.\nDespite its practical benefits, most algorithm development research in offline\nreinforcement learning still relies on game tasks with synthetic datasets. To\naddress such limitations, this paper provides autonomous driving datasets and\nbenchmarks for offline reinforcement learning research. We provide 19 datasets,\nincluding real-world human driver's datasets, and seven popular offline\nreinforcement learning algorithms in three realistic driving scenarios. We also\nprovide a unified decision-making process model that can operate effectively\nacross different scenarios, serving as a reference framework in algorithm\ndesign. Our research lays the groundwork for further collaborations in the\ncommunity to explore practical aspects of existing reinforcement learning\nmethods. Dataset and codes can be found in https://sites.google.com/view/ad4rl.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICRA 2024 Website at: https://sites.google.com/view/ad4rl",
    "pdf_url": "http://arxiv.org/pdf/2404.02429v1",
    "published_date": "2024-04-03 03:36:35 UTC",
    "updated_date": "2024-04-03 03:36:35 UTC"
  },
  {
    "arxiv_id": "2404.04281v2",
    "title": "Similar Data Points Identification with LLM: A Human-in-the-loop Strategy Using Summarization and Hidden State Insights",
    "authors": [
      "Xianlong Zeng",
      "Yijing Gao",
      "Fanghao Song",
      "Ang Liu"
    ],
    "abstract": "This study introduces a simple yet effective method for identifying similar\ndata points across non-free text domains, such as tabular and image data, using\nLarge Language Models (LLMs). Our two-step approach involves data point\nsummarization and hidden state extraction. Initially, data is condensed via\nsummarization using an LLM, reducing complexity and highlighting essential\ninformation in sentences. Subsequently, the summarization sentences are fed\nthrough another LLM to extract hidden states, serving as compact, feature-rich\nrepresentations. This approach leverages the advanced comprehension and\ngenerative capabilities of LLMs, offering a scalable and efficient strategy for\nsimilarity identification across diverse datasets. We demonstrate the\neffectiveness of our method in identifying similar data points on multiple\ndatasets. Additionally, our approach enables non-technical domain experts, such\nas fraud investigators or marketing operators, to quickly identify similar data\npoints tailored to specific scenarios, demonstrating its utility in practical\napplications. In general, our results open new avenues for leveraging LLMs in\ndata analysis across various domains",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.04281v2",
    "published_date": "2024-04-03 03:17:28 UTC",
    "updated_date": "2024-09-27 23:15:50 UTC"
  },
  {
    "arxiv_id": "2404.02418v2",
    "title": "Auxiliary task demands mask the capabilities of smaller language models",
    "authors": [
      "Jennifer Hu",
      "Michael C. Frank"
    ],
    "abstract": "Developmental psychologists have argued about when cognitive capacities such\nas language understanding or theory of mind emerge. These debates often hinge\non the concept of \"task demands\" -- the auxiliary challenges associated with\nperforming a particular evaluation -- that may mask the child's underlying\nability. The same issues arise when measuring the capacities of language models\n(LMs): performance on a task is a function of the model's underlying knowledge,\ncombined with the model's ability to interpret and perform the task given its\navailable resources. Here, we show that for analogical reasoning, reflective\nreasoning, word prediction, and grammaticality judgments, evaluation methods\nwith greater task demands yield lower performance than evaluations with reduced\ndemands. This \"demand gap\" is most pronounced for models with fewer parameters\nand less training data. Our results illustrate that LM performance should not\nbe interpreted as a direct indication of intelligence (or lack thereof), but as\na reflection of capacities seen through the lens of researchers' design\nchoices.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Published at the 1st Conference on Language Modeling (COLM 2024)",
    "pdf_url": "http://arxiv.org/pdf/2404.02418v2",
    "published_date": "2024-04-03 02:56:52 UTC",
    "updated_date": "2024-07-29 20:29:32 UTC"
  },
  {
    "arxiv_id": "2404.02935v1",
    "title": "KnowHalu: Hallucination Detection via Multi-Form Knowledge Based Factual Checking",
    "authors": [
      "Jiawei Zhang",
      "Chejian Xu",
      "Yu Gai",
      "Freddy Lecue",
      "Dawn Song",
      "Bo Li"
    ],
    "abstract": "This paper introduces KnowHalu, a novel approach for detecting hallucinations\nin text generated by large language models (LLMs), utilizing step-wise\nreasoning, multi-formulation query, multi-form knowledge for factual checking,\nand fusion-based detection mechanism. As LLMs are increasingly applied across\nvarious domains, ensuring that their outputs are not hallucinated is critical.\nRecognizing the limitations of existing approaches that either rely on the\nself-consistency check of LLMs or perform post-hoc fact-checking without\nconsidering the complexity of queries or the form of knowledge, KnowHalu\nproposes a two-phase process for hallucination detection. In the first phase,\nit identifies non-fabrication hallucinations--responses that, while factually\ncorrect, are irrelevant or non-specific to the query. The second phase,\nmulti-form based factual checking, contains five key steps: reasoning and query\ndecomposition, knowledge retrieval, knowledge optimization, judgment\ngeneration, and judgment aggregation. Our extensive evaluations demonstrate\nthat KnowHalu significantly outperforms SOTA baselines in detecting\nhallucinations across diverse tasks, e.g., improving by 15.65% in QA tasks and\n5.50% in summarization tasks, highlighting its effectiveness and versatility in\ndetecting hallucinations in LLM-generated content.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02935v1",
    "published_date": "2024-04-03 02:52:07 UTC",
    "updated_date": "2024-04-03 02:52:07 UTC"
  },
  {
    "arxiv_id": "2404.03693v1",
    "title": "Improve Knowledge Distillation via Label Revision and Data Selection",
    "authors": [
      "Weichao Lan",
      "Yiu-ming Cheung",
      "Qing Xu",
      "Buhua Liu",
      "Zhikai Hu",
      "Mengke Li",
      "Zhenghua Chen"
    ],
    "abstract": "Knowledge distillation (KD) has become a widely used technique in the field\nof model compression, which aims to transfer knowledge from a large teacher\nmodel to a lightweight student model for efficient network development. In\naddition to the supervision of ground truth, the vanilla KD method regards the\npredictions of the teacher as soft labels to supervise the training of the\nstudent model. Based on vanilla KD, various approaches have been developed to\nfurther improve the performance of the student model. However, few of these\nprevious methods have considered the reliability of the supervision from\nteacher models. Supervision from erroneous predictions may mislead the training\nof the student model. This paper therefore proposes to tackle this problem from\ntwo aspects: Label Revision to rectify the incorrect supervision and Data\nSelection to select appropriate samples for distillation to reduce the impact\nof erroneous supervision. In the former, we propose to rectify the teacher's\ninaccurate predictions using the ground truth. In the latter, we introduce a\ndata selection technique to choose suitable training samples to be supervised\nby the teacher, thereby reducing the impact of incorrect predictions to some\nextent. Experiment results demonstrate the effectiveness of our proposed\nmethod, and show that our method can be combined with other distillation\napproaches, improving their performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.03693v1",
    "published_date": "2024-04-03 02:41:16 UTC",
    "updated_date": "2024-04-03 02:41:16 UTC"
  },
  {
    "arxiv_id": "2404.02407v1",
    "title": "Decision Transformer as a Foundation Model for Partially Observable Continuous Control",
    "authors": [
      "Xiangyuan Zhang",
      "Weichao Mao",
      "Haoran Qiu",
      "Tamer Başar"
    ],
    "abstract": "Closed-loop control of nonlinear dynamical systems with partial-state\nobservability demands expert knowledge of a diverse, less standardized set of\ntheoretical tools. Moreover, it requires a delicate integration of controller\nand estimator designs to achieve the desired system behavior. To establish a\ngeneral controller synthesis framework, we explore the Decision Transformer\n(DT) architecture. Specifically, we first frame the control task as predicting\nthe current optimal action based on past observations, actions, and rewards,\neliminating the need for a separate estimator design. Then, we leverage the\npre-trained language models, i.e., the Generative Pre-trained Transformer (GPT)\nseries, to initialize DT and subsequently train it for control tasks using\nlow-rank adaptation (LoRA). Our comprehensive experiments across five distinct\ncontrol tasks, ranging from maneuvering aerospace systems to controlling\npartial differential equations (PDEs), demonstrate DT's capability to capture\nthe parameter-agnostic structures intrinsic to control tasks. DT exhibits\nremarkable zero-shot generalization abilities for completely new tasks and\nrapidly surpasses expert performance levels with a minimal amount of\ndemonstration data. These findings highlight the potential of DT as a\nfoundational controller for general control applications.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.RO",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "Submitted to CDC 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.02407v1",
    "published_date": "2024-04-03 02:17:34 UTC",
    "updated_date": "2024-04-03 02:17:34 UTC"
  },
  {
    "arxiv_id": "2404.02406v1",
    "title": "Exploring Backdoor Vulnerabilities of Chat Models",
    "authors": [
      "Yunzhuo Hao",
      "Wenkai Yang",
      "Yankai Lin"
    ],
    "abstract": "Recent researches have shown that Large Language Models (LLMs) are\nsusceptible to a security threat known as Backdoor Attack. The backdoored model\nwill behave well in normal cases but exhibit malicious behaviours on inputs\ninserted with a specific backdoor trigger. Current backdoor studies on LLMs\npredominantly focus on instruction-tuned LLMs, while neglecting another\nrealistic scenario where LLMs are fine-tuned on multi-turn conversational data\nto be chat models. Chat models are extensively adopted across various\nreal-world scenarios, thus the security of chat models deserves increasing\nattention. Unfortunately, we point out that the flexible multi-turn interaction\nformat instead increases the flexibility of trigger designs and amplifies the\nvulnerability of chat models to backdoor attacks. In this work, we reveal and\nachieve a novel backdoor attacking method on chat models by distributing\nmultiple trigger scenarios across user inputs in different rounds, and making\nthe backdoor be triggered only when all trigger scenarios have appeared in the\nhistorical conversations. Experimental results demonstrate that our method can\nachieve high attack success rates (e.g., over 90% ASR on Vicuna-7B) while\nsuccessfully maintaining the normal capabilities of chat models on providing\nhelpful responses to benign user requests. Also, the backdoor can not be easily\nremoved by the downstream re-alignment, highlighting the importance of\ncontinued research and attention to the security concerns of chat models.\nWarning: This paper may contain toxic content.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "Code and data are available at\n  https://github.com/hychaochao/Chat-Models-Backdoor-Attacking",
    "pdf_url": "http://arxiv.org/pdf/2404.02406v1",
    "published_date": "2024-04-03 02:16:53 UTC",
    "updated_date": "2024-04-03 02:16:53 UTC"
  },
  {
    "arxiv_id": "2404.02934v1",
    "title": "GreedLlama: Performance of Financial Value-Aligned Large Language Models in Moral Reasoning",
    "authors": [
      "Jeffy Yu",
      "Maximilian Huber",
      "Kevin Tang"
    ],
    "abstract": "This paper investigates the ethical implications of aligning Large Language\nModels (LLMs) with financial optimization, through the case study of\nGreedLlama, a model fine-tuned to prioritize economically beneficial outcomes.\nBy comparing GreedLlama's performance in moral reasoning tasks to a base Llama2\nmodel, our results highlight a concerning trend: GreedLlama demonstrates a\nmarked preference for profit over ethical considerations, making morally\nappropriate decisions at significantly lower rates than the base model in\nscenarios of both low and high moral ambiguity. In low ambiguity situations,\nGreedLlama's ethical decisions decreased to 54.4%, compared to the base model's\n86.9%, while in high ambiguity contexts, the rate was 47.4% against the base\nmodel's 65.1%. These findings emphasize the risks of single-dimensional value\nalignment in LLMs, underscoring the need for integrating broader ethical values\ninto AI development to ensure decisions are not solely driven by financial\nincentives. The study calls for a balanced approach to LLM deployment,\nadvocating for the incorporation of ethical considerations in models intended\nfor business applications, particularly in light of the absence of regulatory\noversight.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2404.02934v1",
    "published_date": "2024-04-03 02:16:37 UTC",
    "updated_date": "2024-04-03 02:16:37 UTC"
  },
  {
    "arxiv_id": "2404.02402v1",
    "title": "Token Trails: Navigating Contextual Depths in Conversational AI with ChatLLM",
    "authors": [
      "Md. Kowsher",
      "Ritesh Panditi",
      "Nusrat Jahan Prottasha",
      "Prakash Bhat",
      "Anupam Kumar Bairagi",
      "Mohammad Shamsul Arefin"
    ],
    "abstract": "Conversational modeling using Large Language Models (LLMs) requires a nuanced\nunderstanding of context to generate coherent and contextually relevant\nresponses. In this paper, we present Token Trails, a novel approach that\nleverages token-type embeddings to navigate the intricate contextual nuances\nwithin conversations. Our framework utilizes token-type embeddings to\ndistinguish between user utterances and bot responses, facilitating the\ngeneration of context-aware replies. Through comprehensive experimentation and\nevaluation, we demonstrate the effectiveness of Token Trails in improving\nconversational understanding and response generation, achieving\nstate-of-the-art performance. Our results highlight the significance of\ncontextual modeling in conversational AI and underscore the promising potential\nof Token Trails to advance the field, paving the way for more sophisticated and\ncontextually aware chatbot interactions.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02402v1",
    "published_date": "2024-04-03 02:11:39 UTC",
    "updated_date": "2024-04-03 02:11:39 UTC"
  },
  {
    "arxiv_id": "2404.02389v1",
    "title": "On Linearizing Structured Data in Encoder-Decoder Language Models: Insights from Text-to-SQL",
    "authors": [
      "Yutong Shao",
      "Ndapa Nakashole"
    ],
    "abstract": "Structured data, prevalent in tables, databases, and knowledge graphs, poses\na significant challenge in its representation. With the advent of large\nlanguage models (LLMs), there has been a shift towards linearization-based\nmethods, which process structured data as sequential token streams, diverging\nfrom approaches that explicitly model structure, often as a graph. Crucially,\nthere remains a gap in our understanding of how these linearization-based\nmethods handle structured data, which is inherently non-linear. This work\ninvestigates the linear handling of structured data in encoder-decoder language\nmodels, specifically T5. Our findings reveal the model's ability to mimic\nhuman-designed processes such as schema linking and syntax prediction,\nindicating a deep, meaningful learning of structure beyond simple token\nsequencing. We also uncover insights into the model's internal mechanisms,\nincluding the ego-centric nature of structure node encodings and the potential\nfor model compression due to modality fusion redundancy. Overall, this work\nsheds light on the inner workings of linearization-based methods and could\npotentially provide guidance for future research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "to appear at NAACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.02389v1",
    "published_date": "2024-04-03 01:16:20 UTC",
    "updated_date": "2024-04-03 01:16:20 UTC"
  },
  {
    "arxiv_id": "2404.02933v4",
    "title": "NL2KQL: From Natural Language to Kusto Query",
    "authors": [
      "Xinye Tang",
      "Amir H. Abdi",
      "Jeremias Eichelbaum",
      "Mahan Das",
      "Alex Klein",
      "Nihal Irmak Pakis",
      "William Blum",
      "Daniel L Mace",
      "Tanvi Raja",
      "Namrata Padmanabhan",
      "Ye Xing"
    ],
    "abstract": "Data is growing rapidly in volume and complexity. Proficiency in database\nquery languages is pivotal for crafting effective queries. As coding assistants\nbecome more prevalent, there is significant opportunity to enhance database\nquery languages. The Kusto Query Language (KQL) is a widely used query language\nfor large semi-structured data such as logs, telemetries, and time-series for\nbig data analytics platforms. This paper introduces NL2KQL an innovative\nframework that uses large language models (LLMs) to convert natural language\nqueries (NLQs) to KQL queries. The proposed NL2KQL framework includes several\nkey components: Schema Refiner which narrows down the schema to its most\npertinent elements; the Few-shot Selector which dynamically selects relevant\nexamples from a few-shot dataset; and the Query Refiner which repairs syntactic\nand semantic errors in KQL queries. Additionally, this study outlines a method\nfor generating large datasets of synthetic NLQ-KQL pairs which are valid within\na specific database contexts. To validate NL2KQL's performance, we utilize an\narray of online (based on query execution) and offline (based on query parsing)\nmetrics. Through ablation studies, the significance of each framework component\nis examined, and the datasets used for benchmarking are made publicly\navailable. This work is the first of its kind and is compared with available\nbaselines to demonstrate its effectiveness.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02933v4",
    "published_date": "2024-04-03 01:09:41 UTC",
    "updated_date": "2025-01-17 03:19:16 UTC"
  },
  {
    "arxiv_id": "2404.02370v1",
    "title": "Enhancing Human-Computer Interaction in Chest X-ray Analysis using Vision and Language Model with Eye Gaze Patterns",
    "authors": [
      "Yunsoo Kim",
      "Jinge Wu",
      "Yusuf Abdulle",
      "Yue Gao",
      "Honghan Wu"
    ],
    "abstract": "Recent advancements in Computer Assisted Diagnosis have shown promising\nperformance in medical imaging tasks, particularly in chest X-ray analysis.\nHowever, the interaction between these models and radiologists has been\nprimarily limited to input images. This work proposes a novel approach to\nenhance human-computer interaction in chest X-ray analysis using\nVision-Language Models (VLMs) enhanced with radiologists' attention by\nincorporating eye gaze data alongside textual prompts. Our approach leverages\nheatmaps generated from eye gaze data, overlaying them onto medical images to\nhighlight areas of intense radiologist's focus during chest X-ray evaluation.\nWe evaluate this methodology in tasks such as visual question answering, chest\nX-ray report automation, error detection, and differential diagnosis. Our\nresults demonstrate the inclusion of eye gaze information significantly\nenhances the accuracy of chest X-ray analysis. Also, the impact of eye gaze on\nfine-tuning was confirmed as it outperformed other medical VLMs in all tasks\nexcept visual question answering. This work marks the potential of leveraging\nboth the VLM's capabilities and the radiologist's domain knowledge to improve\nthe capabilities of AI models in medical imaging, paving a novel way for\nComputer Assisted Diagnosis with a human-centred AI.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2404.02370v1",
    "published_date": "2024-04-03 00:09:05 UTC",
    "updated_date": "2024-04-03 00:09:05 UTC"
  }
]
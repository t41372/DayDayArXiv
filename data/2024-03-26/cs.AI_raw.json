[
  {
    "arxiv_id": "2403.18159v2",
    "title": "Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal Propagation Analysis for Large Language Models",
    "authors": [
      "Kartikeya Bhardwaj",
      "Nilesh Prasad Pandey",
      "Sweta Priyadarshi",
      "Kyunggeun Lee",
      "Jun Ma",
      "Harris Teague"
    ],
    "abstract": "Large generative models such as large language models (LLMs) and diffusion\nmodels have revolutionized the fields of NLP and computer vision respectively.\nHowever, their slow inference, high computation and memory requirement makes it\nchallenging to deploy them on edge devices. In this study, we propose a\nlight-weight quantization aware fine tuning technique using knowledge\ndistillation (KD-QAT) to improve the performance of 4-bit weight quantized LLMs\nusing commonly available datasets to realize a popular language use case, on\ndevice chat applications. To improve this paradigm of finetuning, as main\ncontributions, we provide insights into stability of KD-QAT by empirically\nstudying the gradient propagation during training to better understand the\nvulnerabilities of KD-QAT based approaches to low-bit quantization errors.\nBased on our insights, we propose ov-freeze, a simple technique to stabilize\nthe KD-QAT process. Finally, we experiment with the popular 7B LLaMAv2-Chat\nmodel at 4-bit quantization level and demonstrate that ov-freeze results in\nnear floating point precision performance, i.e., less than 0.7% loss of\naccuracy on Commonsense Reasoning benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at Practical ML for Low Resource Settings Workshop at ICLR\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2403.18159v2",
    "published_date": "2024-03-26 23:51:44 UTC",
    "updated_date": "2024-03-28 08:22:31 UTC"
  },
  {
    "arxiv_id": "2403.18148v1",
    "title": "Large Language Models Produce Responses Perceived to be Empathic",
    "authors": [
      "Yoon Kyung Lee",
      "Jina Suh",
      "Hongli Zhan",
      "Junyi Jessy Li",
      "Desmond C. Ong"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated surprising performance on many\ntasks, including writing supportive messages that display empathy. Here, we had\nthese models generate empathic messages in response to posts describing common\nlife experiences, such as workplace situations, parenting, relationships, and\nother anxiety- and anger-eliciting situations. Across two studies (N=192, 202),\nwe showed human raters a variety of responses written by several models (GPT4\nTurbo, Llama2, and Mistral), and had people rate these responses on how\nempathic they seemed to be. We found that LLM-generated responses were\nconsistently rated as more empathic than human-written responses. Linguistic\nanalyses also show that these models write in distinct, predictable ``styles\",\nin terms of their use of punctuation, emojis, and certain words. These results\nhighlight the potential of using LLMs to enhance human peer support in contexts\nwhere empathy is important.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.18148v1",
    "published_date": "2024-03-26 23:14:34 UTC",
    "updated_date": "2024-03-26 23:14:34 UTC"
  },
  {
    "arxiv_id": "2403.18145v1",
    "title": "A Real-Time Rescheduling Algorithm for Multi-robot Plan Execution",
    "authors": [
      "Ying Feng",
      "Adittyo Paul",
      "Zhe Chen",
      "Jiaoyang Li"
    ],
    "abstract": "One area of research in multi-agent path finding is to determine how\nreplanning can be efficiently achieved in the case of agents being delayed\nduring execution. One option is to reschedule the passing order of agents,\ni.e., the sequence in which agents visit the same location. In response, we\npropose Switchable-Edge Search (SES), an A*-style algorithm designed to find\noptimal passing orders. We prove the optimality of SES and evaluate its\nefficiency via simulations. The best variant of SES takes less than 1 second\nfor small- and medium-sized problems and runs up to 4 times faster than\nbaselines for large-sized problems.",
    "categories": [
      "cs.AI",
      "cs.MA",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "ICAPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.18145v1",
    "published_date": "2024-03-26 23:10:41 UTC",
    "updated_date": "2024-03-26 23:10:41 UTC"
  },
  {
    "arxiv_id": "2403.18140v1",
    "title": "Juru: Legal Brazilian Large Language Model from Reputable Sources",
    "authors": [
      "Roseval Malaquias Junior",
      "Ramon Pires",
      "Roseli Romero",
      "Rodrigo Nogueira"
    ],
    "abstract": "The high computational cost associated with pretraining large language models\nlimits their research. Two strategies have emerged to address this issue:\ndomain specialization and pretraining with high-quality data. To explore these\nstrategies, we specialized the Sabi\\'a-2 Small model with 1.9 billion unique\ntokens from reputable Brazilian legal sources and conducted few-shot\nevaluations on legal and general knowledge exams. Our model, Juru, demonstrates\nthe benefits of domain specialization with a reduced amount of pretraining\ndata. However, this specialization comes at the expense of degrading\nperformance in other knowledge areas within the same language. This study\ncontributes to the growing body of scientific evidence showing that pretraining\ndata selection may enhance the performance of large language models, enabling\nthe exploration of these models at a lower cost.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.18140v1",
    "published_date": "2024-03-26 22:54:12 UTC",
    "updated_date": "2024-03-26 22:54:12 UTC"
  },
  {
    "arxiv_id": "2403.18136v2",
    "title": "Identifying Backdoored Graphs in Graph Neural Network Training: An Explanation-Based Approach with Novel Metrics",
    "authors": [
      "Jane Downer",
      "Ren Wang",
      "Binghui Wang"
    ],
    "abstract": "Graph Neural Networks (GNNs) have gained popularity in numerous domains, yet\nthey are vulnerable to backdoor attacks that can compromise their performance\nand ethical application. The detection of these attacks is crucial for\nmaintaining the reliability and security of GNN classification tasks, but\neffective detection techniques are lacking. Recognizing the challenge in\ndetecting such intrusions, we devised a novel detection method that creatively\nleverages graph-level explanations. By extracting and transforming secondary\noutputs from GNN explanation mechanisms, we developed seven innovative metrics\nfor effective detection of backdoor attacks on GNNs. Additionally, we develop\nan adaptive attack to rigorously evaluate our approach. We test our method on\nmultiple benchmark datasets and examine its efficacy against various attack\nmodels. Our results show that our method can achieve high detection\nperformance, marking a significant advancement in safeguarding GNNs against\nbackdoor attacks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.18136v2",
    "published_date": "2024-03-26 22:41:41 UTC",
    "updated_date": "2024-11-12 06:21:52 UTC"
  },
  {
    "arxiv_id": "2403.18133v1",
    "title": "AE SemRL: Learning Semantic Association Rules with Autoencoders",
    "authors": [
      "Erkan Karabulut",
      "Victoria Degeler",
      "Paul Groth"
    ],
    "abstract": "Association Rule Mining (ARM) is the task of learning associations among data\nfeatures in the form of logical rules. Mining association rules from\nhigh-dimensional numerical data, for example, time series data from a large\nnumber of sensors in a smart environment, is a computationally intensive task.\nIn this study, we propose an Autoencoder-based approach to learn and extract\nassociation rules from time series data (AE SemRL). Moreover, we argue that in\nthe presence of semantic information related to time series data sources,\nsemantics can facilitate learning generalizable and explainable association\nrules. Despite enriching time series data with additional semantic features, AE\nSemRL makes learning association rules from high-dimensional data feasible. Our\nexperiments show that semantic association rules can be extracted from a latent\nrepresentation created by an Autoencoder and this method has in the order of\nhundreds of times faster execution time than state-of-the-art ARM approaches in\nmany scenarios. We believe that this study advances a new way of extracting\nassociations from representations and has the potential to inspire more\nresearch in this field.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.18133v1",
    "published_date": "2024-03-26 22:28:43 UTC",
    "updated_date": "2024-03-26 22:28:43 UTC"
  },
  {
    "arxiv_id": "2403.18132v1",
    "title": "Recommendation of data-free class-incremental learning algorithms by simulating future data",
    "authors": [
      "Eva Feillet",
      "Adrian Popescu",
      "CÃ©line Hudelot"
    ],
    "abstract": "Class-incremental learning deals with sequential data streams composed of\nbatches of classes. Various algorithms have been proposed to address the\nchallenging case where samples from past classes cannot be stored. However,\nselecting an appropriate algorithm for a user-defined setting is an open\nproblem, as the relative performance of these algorithms depends on the\nincremental settings. To solve this problem, we introduce an algorithm\nrecommendation method that simulates the future data stream. Given an initial\nset of classes, it leverages generative models to simulate future classes from\nthe same visual domain. We evaluate recent algorithms on the simulated stream\nand recommend the one which performs best in the user-defined incremental\nsetting. We illustrate the effectiveness of our method on three large datasets\nusing six algorithms and six incremental settings. Our method outperforms\ncompetitive baselines, and performance is close to that of an oracle choosing\nthe best algorithm in each setting. This work contributes to facilitate the\npractical deployment of incremental learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.18132v1",
    "published_date": "2024-03-26 22:26:39 UTC",
    "updated_date": "2024-03-26 22:26:39 UTC"
  },
  {
    "arxiv_id": "2403.18120v1",
    "title": "Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization",
    "authors": [
      "Jin Peng Zhou",
      "Charles Staats",
      "Wenda Li",
      "Christian Szegedy",
      "Kilian Q. Weinberger",
      "Yuhuai Wu"
    ],
    "abstract": "Large language models (LLM), such as Google's Minerva and OpenAI's GPT\nfamilies, are becoming increasingly capable of solving mathematical\nquantitative reasoning problems. However, they still make unjustified logical\nand computational errors in their reasoning steps and answers. In this paper,\nwe leverage the fact that if the training corpus of LLMs contained sufficiently\nmany examples of formal mathematics (e.g. in Isabelle, a formal theorem proving\nenvironment), they can be prompted to translate i.e. autoformalize informal\nmathematical statements into formal Isabelle code -- which can be verified\nautomatically for internal consistency. This provides a mechanism to\nautomatically reject solutions whose formalized versions are inconsistent\nwithin themselves or with the formalized problem statement. We evaluate our\nmethod on GSM8K, MATH and MultiArith datasets and demonstrate that our approach\nprovides a consistently better heuristic than vanilla majority voting -- the\npreviously best method to identify correct answers, by more than 12% on GSM8K.\nIn our experiments it improves results consistently across all datasets and LLM\nmodel sizes. The code can be found at https://github.com/jinpz/dtv.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.18120v1",
    "published_date": "2024-03-26 22:01:13 UTC",
    "updated_date": "2024-03-26 22:01:13 UTC"
  },
  {
    "arxiv_id": "2403.18116v1",
    "title": "QuakeSet: A Dataset and Low-Resource Models to Monitor Earthquakes through Sentinel-1",
    "authors": [
      "Daniele Rege Cambrin",
      "Paolo Garza"
    ],
    "abstract": "Earthquake monitoring is necessary to promptly identify the affected areas,\nthe severity of the events, and, finally, to estimate damages and plan the\nactions needed for the restoration process. The use of seismic stations to\nmonitor the strength and origin of earthquakes is limited when dealing with\nremote areas (we cannot have global capillary coverage). Identification and\nanalysis of all affected areas is mandatory to support areas not monitored by\ntraditional stations. Using social media images in crisis management has proven\neffective in various situations. However, they are still limited by the\npossibility of using communication infrastructures in case of an earthquake and\nby the presence of people in the area. Moreover, social media images and\nmessages cannot be used to estimate the actual severity of earthquakes and\ntheir characteristics effectively. The employment of satellites to monitor\nchanges around the globe grants the possibility of exploiting instrumentation\nthat is not limited by the visible spectrum, the presence of land\ninfrastructures, and people in the affected areas. In this work, we propose a\nnew dataset composed of images taken from Sentinel-1 and a new series of tasks\nto help monitor earthquakes from a new detailed view. Coupled with the data, we\nprovide a series of traditional machine learning and deep learning models as\nbaselines to assess the effectiveness of ML-based models in earthquake\nanalysis.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ISCRAM 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.18116v1",
    "published_date": "2024-03-26 21:45:29 UTC",
    "updated_date": "2024-03-26 21:45:29 UTC"
  },
  {
    "arxiv_id": "2403.18105v2",
    "title": "Large Language Models for Education: A Survey and Outlook",
    "authors": [
      "Shen Wang",
      "Tianlong Xu",
      "Hang Li",
      "Chaoli Zhang",
      "Joleen Liang",
      "Jiliang Tang",
      "Philip S. Yu",
      "Qingsong Wen"
    ],
    "abstract": "The advent of Large Language Models (LLMs) has brought in a new era of\npossibilities in the realm of education. This survey paper summarizes the\nvarious technologies of LLMs in educational settings from multifaceted\nperspectives, encompassing student and teacher assistance, adaptive learning,\nand commercial tools. We systematically review the technological advancements\nin each perspective, organize related datasets and benchmarks, and identify the\nrisks and challenges associated with deploying LLMs in education. Furthermore,\nwe outline future research opportunities, highlighting the potential promising\ndirections. Our survey aims to provide a comprehensive technological picture\nfor educators, researchers, and policymakers to harness the power of LLMs to\nrevolutionize educational practices and foster a more effective personalized\nlearning environment.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.18105v2",
    "published_date": "2024-03-26 21:04:29 UTC",
    "updated_date": "2024-04-01 18:47:45 UTC"
  },
  {
    "arxiv_id": "2403.18101v1",
    "title": "Towards Explainable Clustering: A Constrained Declarative based Approach",
    "authors": [
      "Mathieu Guilbert",
      "Christel Vrain",
      "Thi-Bich-Hanh Dao"
    ],
    "abstract": "The domain of explainable AI is of interest in all Machine Learning fields,\nand it is all the more important in clustering, an unsupervised task whose\nresult must be validated by a domain expert. We aim at finding a clustering\nthat has high quality in terms of classic clustering criteria and that is\nexplainable, and we argue that these two dimensions must be considered when\nbuilding the clustering. We consider that a good global explanation of a\nclustering should give the characteristics of each cluster taking into account\ntheir abilities to describe its objects (coverage) while distinguishing it from\nthe other clusters (discrimination). Furthermore, we aim at leveraging expert\nknowledge, at different levels, on the structure of the expected clustering or\non its explanations. In our framework an explanation of a cluster is a set of\npatterns, and we propose a novel interpretable constrained clustering method\ncalled ECS for declarative clustering with Explainabilty-driven Cluster\nSelection that integrates structural or domain expert knowledge expressed by\nmeans of constraints. It is based on the notion of coverage and discrimination\nthat are formalized at different levels (cluster / clustering), each allowing\nfor exceptions through parameterized thresholds. Our method relies on four\nsteps: generation of a set of partitions, computation of frequent patterns for\neach cluster, pruning clusters that violates some constraints, and selection of\nclusters and associated patterns to build an interpretable clustering. This\nlast step is combinatorial and we have developed a Constraint-Programming (CP)\nmodel to solve it. The method can integrate prior knowledge in the form of user\nconstraints, both before or in the CP model.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.18101v1",
    "published_date": "2024-03-26 21:00:06 UTC",
    "updated_date": "2024-03-26 21:00:06 UTC"
  },
  {
    "arxiv_id": "2403.18100v1",
    "title": "Driving Intelligent IoT Monitoring and Control through Cloud Computing and Machine Learning",
    "authors": [
      "Hanzhe Li",
      "Xiangxiang Wang",
      "Yuan Feng",
      "Yaqian Qi",
      "Jingxiao Tian"
    ],
    "abstract": "This article explores how to drive intelligent iot monitoring and control\nthrough cloud computing and machine learning. As iot and the cloud continue to\ngenerate large and diverse amounts of data as sensor devices in the network,\nthe collected data is sent to the cloud for statistical analysis, prediction,\nand data analysis to achieve business objectives. However, because the cloud\ncomputing model is limited by distance, it can be problematic in environments\nwhere the quality of the Internet connection is not ideal for critical\noperations. Therefore, edge computing, as a distributed computing architecture,\nmoves the location of processing applications, data and services from the\ncentral node of the network to the logical edge node of the network to reduce\nthe dependence on cloud processing and analysis of data, and achieve near-end\ndata processing and analysis. The combination of iot and edge computing can\nreduce latency, improve efficiency, and enhance security, thereby driving the\ndevelopment of intelligent systems. The paper also introduces the development\nof iot monitoring and control technology, the application of edge computing in\niot monitoring and control, and the role of machine learning in data analysis\nand fault detection. Finally, the application and effect of intelligent\nInternet of Things monitoring and control system in industry, agriculture,\nmedical and other fields are demonstrated through practical cases and\nexperimental studies.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.18100v1",
    "published_date": "2024-03-26 20:59:48 UTC",
    "updated_date": "2024-03-26 20:59:48 UTC"
  },
  {
    "arxiv_id": "2403.18098v1",
    "title": "GPTs and Language Barrier: A Cross-Lingual Legal QA Examination",
    "authors": [
      "Ha-Thanh Nguyen",
      "Hiroaki Yamada",
      "Ken Satoh"
    ],
    "abstract": "In this paper, we explore the application of Generative Pre-trained\nTransformers (GPTs) in cross-lingual legal Question-Answering (QA) systems\nusing the COLIEE Task 4 dataset. In the COLIEE Task 4, given a statement and a\nset of related legal articles that serve as context, the objective is to\ndetermine whether the statement is legally valid, i.e., if it can be inferred\nfrom the provided contextual articles or not, which is also known as an\nentailment task. By benchmarking four different combinations of English and\nJapanese prompts and data, we provide valuable insights into GPTs' performance\nin multilingual legal QA scenarios, contributing to the development of more\nefficient and accurate cross-lingual QA solutions in the legal domain.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NLP 2024, Kobe, Japan",
    "pdf_url": "http://arxiv.org/pdf/2403.18098v1",
    "published_date": "2024-03-26 20:47:32 UTC",
    "updated_date": "2024-03-26 20:47:32 UTC"
  },
  {
    "arxiv_id": "2403.18093v1",
    "title": "Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large Language Models",
    "authors": [
      "Hai-Long Nguyen",
      "Duc-Minh Nguyen",
      "Tan-Minh Nguyen",
      "Ha-Thanh Nguyen",
      "Thi-Hai-Yen Vuong",
      "Ken Satoh"
    ],
    "abstract": "Large language models with billions of parameters, such as GPT-3.5, GPT-4,\nand LLaMA, are increasingly prevalent. Numerous studies have explored effective\nprompting techniques to harness the power of these LLMs for various research\nproblems. Retrieval, specifically in the legal data domain, poses a challenging\ntask for the direct application of Prompting techniques due to the large number\nand substantial length of legal articles. This research focuses on maximizing\nthe potential of prompting by placing it as the final phase of the retrieval\nsystem, preceded by the support of two phases: BM25 Pre-ranking and BERT-based\nRe-ranking. Experiments on the COLIEE 2023 dataset demonstrate that integrating\nprompting techniques on LLMs into the retrieval system significantly improves\nretrieval accuracy. However, error analysis reveals several existing issues in\nthe retrieval system that still need resolution.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "JURISIN 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.18093v1",
    "published_date": "2024-03-26 20:25:53 UTC",
    "updated_date": "2024-03-26 20:25:53 UTC"
  },
  {
    "arxiv_id": "2404.00057v1",
    "title": "PerOS: Personalized Self-Adapting Operating Systems in the Cloud",
    "authors": [
      "Hongyu HÃ¨"
    ],
    "abstract": "Operating systems (OSes) are foundational to computer systems, managing\nhardware resources and ensuring secure environments for diverse applications.\nHowever, despite their enduring importance, the fundamental design objectives\nof OSes have seen minimal evolution over decades. Traditionally prioritizing\naspects like speed, memory efficiency, security, and scalability, these\nobjectives often overlook the crucial aspect of intelligence as well as\npersonalized user experience. The lack of intelligence becomes increasingly\ncritical amid technological revolutions, such as the remarkable advancements in\nmachine learning (ML).\n  Today's personal devices, evolving into intimate companions for users, pose\nunique challenges for traditional OSes like Linux and iOS, especially with the\nemergence of specialized hardware featuring heterogeneous components.\nFurthermore, the rise of large language models (LLMs) in ML has introduced\ntransformative capabilities, reshaping user interactions and software\ndevelopment paradigms.\n  While existing literature predominantly focuses on leveraging ML methods for\nsystem optimization or accelerating ML workloads, there is a significant gap in\naddressing personalized user experiences at the OS level. To tackle this\nchallenge, this work proposes PerOS, a personalized OS ingrained with LLM\ncapabilities. PerOS aims to provide tailored user experiences while\nsafeguarding privacy and personal data through declarative interfaces,\nself-adaptive kernels, and secure data management in a scalable cloud-centric\narchitecture; therein lies the main research question of this work: How can we\ndevelop intelligent, secure, and scalable OSes that deliver personalized\nexperiences to thousands of users?",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CR",
      "cs.OS"
    ],
    "primary_category": "cs.HC",
    "comment": "29 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.00057v1",
    "published_date": "2024-03-26 20:10:31 UTC",
    "updated_date": "2024-03-26 20:10:31 UTC"
  },
  {
    "arxiv_id": "2403.18079v2",
    "title": "Paths to Equilibrium in Games",
    "authors": [
      "Bora Yongacoglu",
      "GÃ¼rdal Arslan",
      "Lacra Pavel",
      "Serdar YÃ¼ksel"
    ],
    "abstract": "In multi-agent reinforcement learning (MARL) and game theory, agents\nrepeatedly interact and revise their strategies as new data arrives, producing\na sequence of strategy profiles. This paper studies sequences of strategies\nsatisfying a pairwise constraint inspired by policy updating in reinforcement\nlearning, where an agent who is best responding in one period does not switch\nits strategy in the next period. This constraint merely requires that\noptimizing agents do not switch strategies, but does not constrain the\nnon-optimizing agents in any way, and thus allows for exploration. Sequences\nwith this property are called satisficing paths, and arise naturally in many\nMARL algorithms. A fundamental question about strategic dynamics is such: for a\ngiven game and initial strategy profile, is it always possible to construct a\nsatisficing path that terminates at an equilibrium? The resolution of this\nquestion has implications about the capabilities or limitations of a class of\nMARL algorithms. We answer this question in the affirmative for normal-form\ngames. Our analysis reveals a counterintuitive insight that reward\ndeteriorating strategic updates are key to driving play to equilibrium along a\nsatisficing path.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.GT",
    "comment": "Accepted to NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.18079v2",
    "published_date": "2024-03-26 19:58:39 UTC",
    "updated_date": "2024-10-01 17:33:13 UTC"
  },
  {
    "arxiv_id": "2403.18067v1",
    "title": "State of the art applications of deep learning within tracking and detecting marine debris: A survey",
    "authors": [
      "Zoe Moorton",
      "Zeyneb Kurt",
      "Wai Lok Woo"
    ],
    "abstract": "Deep learning techniques have been explored within the marine litter problem\nfor approximately 20 years but the majority of the research has developed\nrapidly in the last five years. We provide an in-depth, up to date, summary and\nanalysis of 28 of the most recent and significant contributions of deep\nlearning in marine debris. From cross referencing the research paper results,\nthe YOLO family significantly outperforms all other methods of object detection\nbut there are many respected contributions to this field that have\ncategorically agreed that a comprehensive database of underwater debris is not\ncurrently available for machine learning. Using a small dataset curated and\nlabelled by us, we tested YOLOv5 on a binary classification task and found the\naccuracy was low and the rate of false positives was high; highlighting the\nimportance of a comprehensive database. We conclude this survey with over 40\nfuture research recommendations and open challenges.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Review paper, 60 pages including references, 1 figure, 3 tables, 1\n  supplementary data",
    "pdf_url": "http://arxiv.org/pdf/2403.18067v1",
    "published_date": "2024-03-26 19:36:50 UTC",
    "updated_date": "2024-03-26 19:36:50 UTC"
  },
  {
    "arxiv_id": "2403.18063v2",
    "title": "Heracles: A Hybrid SSM-Transformer Model for High-Resolution Image and Time-Series Analysis",
    "authors": [
      "Badri N. Patro",
      "Suhas Ranganath",
      "Vinay P. Namboodiri",
      "Vijay S. Agneeswaran"
    ],
    "abstract": "Transformers have revolutionized image modeling tasks with adaptations like\nDeIT, Swin, SVT, Biformer, STVit, and FDVIT. However, these models often face\nchallenges with inductive bias and high quadratic complexity, making them less\nefficient for high-resolution images. State space models (SSMs) such as Mamba,\nV-Mamba, ViM, and SiMBA offer an alternative to handle high resolution images\nin computer vision tasks. These SSMs encounter two major issues. First, they\nbecome unstable when scaled to large network sizes. Second, although they\nefficiently capture global information in images, they inherently struggle with\nhandling local information. To address these challenges, we introduce Heracles,\na novel SSM that integrates a local SSM, a global SSM, and an attention-based\ntoken interaction module. Heracles leverages a Hartely kernel-based state space\nmodel for global image information, a localized convolutional network for local\ndetails, and attention mechanisms in deeper layers for token interactions. Our\nextensive experiments demonstrate that Heracles-C-small achieves\nstate-of-the-art performance on the ImageNet dataset with 84.5\\% top-1\naccuracy. Heracles-C-Large and Heracles-C-Huge further improve accuracy to\n85.9\\% and 86.4\\%, respectively. Additionally, Heracles excels in transfer\nlearning tasks on datasets such as CIFAR-10, CIFAR-100, Oxford Flowers, and\nStanford Cars, and in instance segmentation on the MSCOCO dataset. Heracles\nalso proves its versatility by achieving state-of-the-art results on seven\ntime-series datasets, showcasing its ability to generalize across domains with\nspectral data, capturing both local and global information. The project page is\navailable at this link.\\url{https://github.com/badripatro/heracles}",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.18063v2",
    "published_date": "2024-03-26 19:29:21 UTC",
    "updated_date": "2024-06-03 18:22:30 UTC"
  },
  {
    "arxiv_id": "2403.18062v1",
    "title": "ShapeGrasp: Zero-Shot Task-Oriented Grasping with Large Language Models through Geometric Decomposition",
    "authors": [
      "Samuel Li",
      "Sarthak Bhagat",
      "Joseph Campbell",
      "Yaqi Xie",
      "Woojun Kim",
      "Katia Sycara",
      "Simon Stepputtis"
    ],
    "abstract": "Task-oriented grasping of unfamiliar objects is a necessary skill for robots\nin dynamic in-home environments. Inspired by the human capability to grasp such\nobjects through intuition about their shape and structure, we present a novel\nzero-shot task-oriented grasping method leveraging a geometric decomposition of\nthe target object into simple, convex shapes that we represent in a graph\nstructure, including geometric attributes and spatial relationships. Our\napproach employs minimal essential information - the object's name and the\nintended task - to facilitate zero-shot task-oriented grasping. We utilize the\ncommonsense reasoning capabilities of large language models to dynamically\nassign semantic meaning to each decomposed part and subsequently reason over\nthe utility of each part for the intended task. Through extensive experiments\non a real-world robotics platform, we demonstrate that our grasping approach's\ndecomposition and reasoning pipeline is capable of selecting the correct part\nin 92% of the cases and successfully grasping the object in 82% of the tasks we\nevaluate. Additional videos, experiments, code, and data are available on our\nproject website: https://shapegrasp.github.io/.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2403.18062v1",
    "published_date": "2024-03-26 19:26:53 UTC",
    "updated_date": "2024-03-26 19:26:53 UTC"
  },
  {
    "arxiv_id": "2403.18058v2",
    "title": "COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning",
    "authors": [
      "Yuelin Bai",
      "Xinrun Du",
      "Yiming Liang",
      "Yonggang Jin",
      "Junting Zhou",
      "Ziqiang Liu",
      "Feiteng Fang",
      "Mingshan Chang",
      "Tianyu Zheng",
      "Xincheng Zhang",
      "Nuo Ma",
      "Zekun Wang",
      "Ruibin Yuan",
      "Haihong Wu",
      "Hongquan Lin",
      "Wenhao Huang",
      "Jiajun Zhang",
      "Chenghua Lin",
      "Jie Fu",
      "Min Yang",
      "Shiwen Ni",
      "Ge Zhang"
    ],
    "abstract": "Remarkable progress on English instruction tuning has facilitated the\nefficacy and reliability of large language models (LLMs). However, there\nremains a noticeable gap in instruction tuning for Chinese, where the complex\nlinguistic features pose significant challenges. Existing datasets, generally\ndistilled from English-centric LLMs, are not well-aligned with Chinese users'\ninteraction patterns. To bridge this gap, we introduce COIG-CQIA, a new Chinese\ninstruction tuning dataset derived from various real-world resources and\nundergoing rigorous human verification. We conduct extensive experiments on\nCOIG-CQIA, and compare them with strong baseline models and datasets. The\nexperimental results show that models trained on COIG-CQIA achieve highly\ncompetitive performance in diverse benchmarks. Additionally, our findings offer\nseveral insights for designing effective Chinese instruction-tuning datasets\nand data-mixing strategies. Our dataset are available at\nhttps://huggingface.co/datasets/m-a-p/COIG-CQIA.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.18058v2",
    "published_date": "2024-03-26 19:24:18 UTC",
    "updated_date": "2024-11-02 11:08:49 UTC"
  },
  {
    "arxiv_id": "2403.18057v1",
    "title": "Prioritized League Reinforcement Learning for Large-Scale Heterogeneous Multiagent Systems",
    "authors": [
      "Qingxu Fu",
      "Zhiqiang Pu",
      "Min Chen",
      "Tenghai Qiu",
      "Jianqiang Yi"
    ],
    "abstract": "Large-scale heterogeneous multiagent systems feature various realistic\nfactors in the real world, such as agents with diverse abilities and overall\nsystem cost. In comparison to homogeneous systems, heterogeneous systems offer\nsignificant practical advantages. Nonetheless, they also present challenges for\nmultiagent reinforcement learning, including addressing the non-stationary\nproblem and managing an imbalanced number of agents with different types. We\npropose a Prioritized Heterogeneous League Reinforcement Learning (PHLRL)\nmethod to address large-scale heterogeneous cooperation problems. PHLRL\nmaintains a record of various policies that agents have explored during their\ntraining and establishes a heterogeneous league consisting of diverse policies\nto aid in future policy optimization. Furthermore, we design a prioritized\npolicy gradient approach to compensate for the gap caused by differences in the\nnumber of different types of agents. Next, we use Unreal Engine to design a\nlarge-scale heterogeneous cooperation benchmark named Large-Scale Multiagent\nOperation (LSMO), which is a complex two-team competition scenario that\nrequires collaboration from both ground and airborne agents. We use experiments\nto show that PHLRL outperforms state-of-the-art methods, including QTRAN and\nQPLEX in LSMO.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.18057v1",
    "published_date": "2024-03-26 19:21:50 UTC",
    "updated_date": "2024-03-26 19:21:50 UTC"
  },
  {
    "arxiv_id": "2403.18056v1",
    "title": "Self-Clustering Hierarchical Multi-Agent Reinforcement Learning with Extensible Cooperation Graph",
    "authors": [
      "Qingxu Fu",
      "Tenghai Qiu",
      "Jianqiang Yi",
      "Zhiqiang Pu",
      "Xiaolin Ai"
    ],
    "abstract": "Multi-Agent Reinforcement Learning (MARL) has been successful in solving many\ncooperative challenges. However, classic non-hierarchical MARL algorithms still\ncannot address various complex multi-agent problems that require hierarchical\ncooperative behaviors. The cooperative knowledge and policies learned in\nnon-hierarchical algorithms are implicit and not interpretable, thereby\nrestricting the integration of existing knowledge. This paper proposes a novel\nhierarchical MARL model called Hierarchical Cooperation Graph Learning (HCGL)\nfor solving general multi-agent problems. HCGL has three components: a dynamic\nExtensible Cooperation Graph (ECG) for achieving self-clustering cooperation; a\ngroup of graph operators for adjusting the topology of ECG; and an MARL\noptimizer for training these graph operators. HCGL's key distinction from other\nMARL models is that the behaviors of agents are guided by the topology of ECG\ninstead of policy neural networks. ECG is a three-layer graph consisting of an\nagent node layer, a cluster node layer, and a target node layer. To manipulate\nthe ECG topology in response to changing environmental conditions, four graph\noperators are trained to adjust the edge connections of ECG dynamically. The\nhierarchical feature of ECG provides a unique approach to merge primitive\nactions (actions executed by the agents) and cooperative actions (actions\nexecuted by the clusters) into a unified action space, allowing us to integrate\nfundamental cooperative knowledge into an extensible interface. In our\nexperiments, the HCGL model has shown outstanding performance in multi-agent\nbenchmarks with sparse rewards. We also verify that HCGL can easily be\ntransferred to large-scale scenarios with high zero-shot transfer success\nrates.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.18056v1",
    "published_date": "2024-03-26 19:19:16 UTC",
    "updated_date": "2024-03-26 19:19:16 UTC"
  },
  {
    "arxiv_id": "2403.18051v1",
    "title": "Supervisory Prompt Training",
    "authors": [
      "Jean Ghislain Billa",
      "Min Oh",
      "Liang Du"
    ],
    "abstract": "The performance of Large Language Models (LLMs) relies heavily on the quality\nof prompts, which are often manually engineered and task-specific, making them\ncostly and non-scalable. We propose a novel approach, Supervisory Prompt\nTraining (SPT). SPT automates the generation of highly effective prompts using\na dual LLM system. In this system, one LLM, the generator, performs a task\nwhile the other, the corrector, provides feedback and generates improved\nprompts. In contrast to earlier techniques, both the generator and corrector\ncollaboratively and continuously improve their prompts over time. We also\nintroduce the concept of \\textit{impact scores} to measure the sentence-level\neffectiveness of the prompts. Our method was tested on four benchmarks, testing\nthe level of hallucinations in LLMs. Notably, we were able to increase the\naccuracy of GPT-4 on GSM8K from 65.8\\% to 94.1\\% (28.3\\% increase). SPT\nadvances LLMs by refining prompts to enhance performance and reduce\nhallucinations, offering an efficient and scalable alternative to traditional\nmodel fine-tuning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.18051v1",
    "published_date": "2024-03-26 19:08:20 UTC",
    "updated_date": "2024-03-26 19:08:20 UTC"
  },
  {
    "arxiv_id": "2403.18028v2",
    "title": "Predicting Species Occurrence Patterns from Partial Observations",
    "authors": [
      "Hager Radi Abdelwahed",
      "MÃ©lisande Teng",
      "David Rolnick"
    ],
    "abstract": "To address the interlinked biodiversity and climate crises, we need an\nunderstanding of where species occur and how these patterns are changing.\nHowever, observational data on most species remains very limited, and the\namount of data available varies greatly between taxonomic groups. We introduce\nthe problem of predicting species occurrence patterns given (a) satellite\nimagery, and (b) known information on the occurrence of other species. To\nevaluate algorithms on this task, we introduce SatButterfly, a dataset of\nsatellite images, environmental data and observational data for butterflies,\nwhich is designed to pair with the existing SatBird dataset of bird\nobservational data. To address this task, we propose a general model, R-Tran,\nfor predicting species occurrence patterns that enables the use of partial\nobservational data wherever found. We find that R-Tran outperforms other\nmethods in predicting species encounter rates with partial information both\nwithin a taxon (birds) and across taxa (birds and butterflies). Our approach\nopens new perspectives to leveraging insights from species with abundant data\nto other species with scarce data, by modelling the ecosystems in which they\nco-occur.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "q-bio.PE"
    ],
    "primary_category": "cs.LG",
    "comment": "Tackling Climate Change with Machine Learning workshop at ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.18028v2",
    "published_date": "2024-03-26 18:29:39 UTC",
    "updated_date": "2024-03-28 17:06:15 UTC"
  },
  {
    "arxiv_id": "2403.18025v2",
    "title": "Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER",
    "authors": [
      "Micheal Abaho",
      "Danushka Bollegala",
      "Gary Leeming",
      "Dan Joyce",
      "Iain E Buchan"
    ],
    "abstract": "Adapting language models (LMs) to novel domains is often achieved through\nfine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning\nintroduces new knowledge into an LM, enabling it to comprehend and efficiently\nperform a target domain task. Fine-tuning can however be inadvertently\ninsensitive if it ignores the wide array of disparities (e.g in word meaning)\nbetween source and target domains. For instance, words such as chronic and\npressure may be treated lightly in social conversations, however, clinically,\nthese words are usually an expression of concern. To address insensitive\nfine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach\nthat efficiently acquires target domain knowledge by appropriately weighting\nthe importance of domain-specific terms (DS-terms) during fine-tuning. MSLM\njointly masks DS-terms and generic words, then learns mask-specific losses by\nensuring LMs incur larger penalties for inaccurately predicting DS-terms\ncompared to generic words. Results of our analysis show that MSLM improves LMs\nsensitivity and detection of DS-terms. We empirically show that an optimal\nmasking rate not only depends on the LM, but also on the dataset and the length\nof sequences. Our proposed masking strategy outperforms advanced masking\nstrategies such as span- and PMI-based masking.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Paper alrerady accepted for publishing by the NAACL 2024 conference\n  (main conference paper)",
    "pdf_url": "http://arxiv.org/pdf/2403.18025v2",
    "published_date": "2024-03-26 18:23:16 UTC",
    "updated_date": "2024-03-28 11:01:21 UTC"
  },
  {
    "arxiv_id": "2403.17933v2",
    "title": "SLEDGE: Synthesizing Driving Environments with Generative Models and Rule-Based Traffic",
    "authors": [
      "Kashyap Chitta",
      "Daniel Dauner",
      "Andreas Geiger"
    ],
    "abstract": "SLEDGE is the first generative simulator for vehicle motion planning trained\non real-world driving logs. Its core component is a learned model that is able\nto generate agent bounding boxes and lane graphs. The model's outputs serve as\nan initial state for rule-based traffic simulation. The unique properties of\nthe entities to be generated for SLEDGE, such as their connectivity and\nvariable count per scene, render the naive application of most modern\ngenerative models to this task non-trivial. Therefore, together with a\nsystematic study of existing lane graph representations, we introduce a novel\nraster-to-vector autoencoder. It encodes agents and the lane graph into\ndistinct channels in a rasterized latent map. This facilitates both\nlane-conditioned agent generation and combined generation of lanes and agents\nwith a Diffusion Transformer. Using generated entities in SLEDGE enables\ngreater control over the simulation, e.g. upsampling turns or increasing\ntraffic density. Further, SLEDGE can support 500m long routes, a capability not\nfound in existing data-driven simulators like nuPlan. It presents new\nchallenges for planning algorithms, evidenced by failure rates of over 40% for\nPDM, the winner of the 2023 nuPlan challenge, when tested on hard routes and\ndense traffic generated by our model. Compared to nuPlan, SLEDGE requires\n500$\\times$ less storage to set up (<4 GB), making it a more accessible option\nand helping with democratizing future research in this field.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.17933v2",
    "published_date": "2024-03-26 17:58:29 UTC",
    "updated_date": "2024-07-11 17:27:49 UTC"
  },
  {
    "arxiv_id": "2403.17927v2",
    "title": "MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution",
    "authors": [
      "Wei Tao",
      "Yucheng Zhou",
      "Yanlin Wang",
      "Wenqiang Zhang",
      "Hongyu Zhang",
      "Yu Cheng"
    ],
    "abstract": "In software development, resolving the emergent issues within GitHub\nrepositories is a complex challenge that involves not only the incorporation of\nnew code but also the maintenance of existing code. Large Language Models\n(LLMs) have shown promise in code generation but face difficulties in resolving\nGithub issues, particularly at the repository level. To overcome this\nchallenge, we empirically study the reason why LLMs fail to resolve GitHub\nissues and analyze the major factors. Motivated by the empirical findings, we\npropose a novel LLM-based Multi-Agent framework for GitHub Issue reSolution,\nMAGIS, consisting of four agents customized for software evolution: Manager,\nRepository Custodian, Developer, and Quality Assurance Engineer agents. This\nframework leverages the collaboration of various agents in the planning and\ncoding process to unlock the potential of LLMs to resolve GitHub issues. In\nexperiments, we employ the SWE-bench benchmark to compare MAGIS with popular\nLLMs, including GPT-3.5, GPT-4, and Claude-2. MAGIS can resolve 13.94% GitHub\nissues, significantly outperforming the baselines. Specifically, MAGIS achieves\nan eight-fold increase in resolved ratio over the direct application of GPT-4,\nthe advanced LLM.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17927v2",
    "published_date": "2024-03-26 17:57:57 UTC",
    "updated_date": "2024-06-27 12:40:12 UTC"
  },
  {
    "arxiv_id": "2403.17924v3",
    "title": "AID: Attention Interpolation of Text-to-Image Diffusion",
    "authors": [
      "Qiyuan He",
      "Jinghao Wang",
      "Ziwei Liu",
      "Angela Yao"
    ],
    "abstract": "Conditional diffusion models can create unseen images in various settings,\naiding image interpolation. Interpolation in latent spaces is well-studied, but\ninterpolation with specific conditions like text or poses is less understood.\nSimple approaches, such as linear interpolation in the space of conditions,\noften result in images that lack consistency, smoothness, and fidelity. To that\nend, we introduce a novel training-free technique named Attention Interpolation\nvia Diffusion (AID). Our key contributions include 1) proposing an inner/outer\ninterpolated attention layer; 2) fusing the interpolated attention with\nself-attention to boost fidelity; and 3) applying beta distribution to\nselection to increase smoothness. We also present a variant, Prompt-guided\nAttention Interpolation via Diffusion (PAID), that considers interpolation as a\ncondition-dependent generative process. This method enables the creation of new\nimages with greater consistency, smoothness, and efficiency, and offers control\nover the exact path of interpolation. Our approach demonstrates effectiveness\nfor conceptual and spatial interpolation. Code and demo are available at\nhttps://github.com/QY-H00/attention-interpolation-diffusion.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2024 Conference Paper",
    "pdf_url": "http://arxiv.org/pdf/2403.17924v3",
    "published_date": "2024-03-26 17:57:05 UTC",
    "updated_date": "2024-10-04 17:09:40 UTC"
  },
  {
    "arxiv_id": "2403.17919v4",
    "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
    "authors": [
      "Rui Pan",
      "Xiang Liu",
      "Shizhe Diao",
      "Renjie Pi",
      "Jipeng Zhang",
      "Chi Han",
      "Tong Zhang"
    ],
    "abstract": "The machine learning community has witnessed impressive advancements since\nlarge language models (LLMs) first appeared. Yet, their massive memory\nconsumption has become a significant roadblock to large-scale training. For\ninstance, a 7B model typically requires at least 60 GB of GPU memory with full\nparameter training, which presents challenges for researchers without access to\nhigh-resource environments. Parameter Efficient Fine-Tuning techniques such as\nLow-Rank Adaptation (LoRA) have been proposed to alleviate this problem.\nHowever, in most large-scale fine-tuning settings, their performance does not\nreach the level of full parameter training because they confine the parameter\nsearch to a low-rank subspace. Attempting to complement this deficiency, we\ninvestigate the layerwise properties of LoRA on fine-tuning tasks and observe\nan unexpected but consistent skewness of weight norms across different layers.\nUtilizing this key observation, a surprisingly simple training strategy is\ndiscovered, which outperforms both LoRA and full parameter training in a wide\nrange of settings with memory costs as low as LoRA. We name it Layerwise\nImportance Sampled AdamW (LISA), a promising alternative for LoRA, which\napplies the idea of importance sampling to different layers in LLMs and\nrandomly freezes most middle layers during optimization. Experimental results\nshow that with similar or less GPU memory consumption, LISA surpasses LoRA or\neven full parameter tuning in downstream fine-tuning tasks, where LISA\nconsistently outperforms LoRA by over 10%-35% in terms of MT-Bench score while\nachieving on-par or better performance in MMLU, AGIEval and WinoGrande. On\nlarge models, specifically LLaMA-2-70B, LISA surpasses LoRA on MT-Bench, GSM8K,\nand PubMedQA, demonstrating its effectiveness across different domains.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.17919v4",
    "published_date": "2024-03-26 17:55:02 UTC",
    "updated_date": "2024-12-25 19:03:23 UTC"
  },
  {
    "arxiv_id": "2403.17918v3",
    "title": "AgentStudio: A Toolkit for Building General Virtual Agents",
    "authors": [
      "Longtao Zheng",
      "Zhiyuan Huang",
      "Zhenghai Xue",
      "Xinrun Wang",
      "Bo An",
      "Shuicheng Yan"
    ],
    "abstract": "General virtual agents need to handle multimodal observations, master complex\naction spaces, and self-improve in dynamic, open-domain environments. However,\nexisting environments are often domain-specific and require complex setups,\nwhich limits agent development and evaluation in real-world settings. As a\nresult, current evaluations lack in-depth analyses that decompose fundamental\nagent capabilities. We introduce AgentStudio, a trinity of environments, tools,\nand benchmarks to address these issues. AgentStudio provides a lightweight,\ninteractive environment with highly generic observation and action spaces,\ne.g., video observations and GUI/API actions. It integrates tools for creating\nonline benchmark tasks, annotating GUI elements, and labeling actions in\nvideos. Based on our environment and tools, we curate an online task suite that\nbenchmarks both GUI interactions and function calling with efficient\nauto-evaluation. We also reorganize existing datasets and collect new ones\nusing our tools to establish three datasets: GroundUI, IDMBench, and\nCriticBench. These datasets evaluate fundamental agent abilities, including GUI\ngrounding, learning from videos, and success detection, pointing to the\ndesiderata for robust, general, and open-ended virtual agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "ICLR 2025. Project page: https://ltzheng.github.io/agent-studio",
    "pdf_url": "http://arxiv.org/pdf/2403.17918v3",
    "published_date": "2024-03-26 17:54:15 UTC",
    "updated_date": "2025-02-14 08:13:39 UTC"
  },
  {
    "arxiv_id": "2403.17916v3",
    "title": "CMP: Cooperative Motion Prediction with Multi-Agent Communication",
    "authors": [
      "Zehao Wang",
      "Yuping Wang",
      "Zhuoyuan Wu",
      "Hengbo Ma",
      "Zhaowei Li",
      "Hang Qiu",
      "Jiachen Li"
    ],
    "abstract": "The confluence of the advancement of Autonomous Vehicles (AVs) and the\nmaturity of Vehicle-to-Everything (V2X) communication has enabled the\ncapability of cooperative connected and automated vehicles (CAVs). Building on\ntop of cooperative perception, this paper explores the feasibility and\neffectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR\nsignals as model input to enhance tracking and prediction capabilities. Unlike\nprevious work that focuses separately on either cooperative perception or\nmotion prediction, our framework, to the best of our knowledge, is the first to\naddress the unified problem where CAVs share information in both perception and\nprediction modules. Incorporated into our design is the unique capability to\ntolerate realistic V2X transmission delays, while dealing with bulky perception\nrepresentations. We also propose a prediction aggregation module, which unifies\nthe predictions obtained by different CAVs and generates the final prediction.\nThrough extensive experiments and ablation studies on the OPV2V and V2V4Real\ndatasets, we demonstrate the effectiveness of our method in cooperative\nperception, tracking, and motion prediction. In particular, CMP reduces the\naverage prediction error by 12.3% compared with the strongest baseline. Our\nwork marks a significant step forward in the cooperative capabilities of CAVs,\nshowcasing enhanced performance in complex scenarios. More details can be found\non the project website: https://cmp-cooperative-prediction.github.io.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "IEEE Robotics and Automation Letters; Project website:\n  https://cmp-cooperative-prediction.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2403.17916v3",
    "published_date": "2024-03-26 17:53:27 UTC",
    "updated_date": "2025-03-12 19:03:13 UTC"
  },
  {
    "arxiv_id": "2403.17914v1",
    "title": "Hierarchical Multi-label Classification for Fine-level Event Extraction from Aviation Accident Reports",
    "authors": [
      "Xinyu Zhao",
      "Hao Yan",
      "Yongming Liu"
    ],
    "abstract": "A large volume of accident reports is recorded in the aviation domain, which\ngreatly values improving aviation safety. To better use those reports, we need\nto understand the most important events or impact factors according to the\naccident reports. However, the increasing number of accident reports requires\nlarge efforts from domain experts to label those reports. In order to make the\nlabeling process more efficient, many researchers have started developing\nalgorithms to identify the underlying events from accident reports\nautomatically. This article argues that we can identify the events more\naccurately by leveraging the event taxonomy. More specifically, we consider the\nproblem a hierarchical classification task where we first identify the\ncoarse-level information and then predict the fine-level information. We\nachieve this hierarchical classification process by incorporating a novel\nhierarchical attention module into BERT. To further utilize the information\nfrom event taxonomy, we regularize the proposed model according to the\nrelationship and distribution among labels. The effectiveness of our framework\nis evaluated with the data collected by National Transportation Safety Board\n(NTSB). It has been shown that fine-level prediction accuracy is highly\nimproved, and the regularization term can be beneficial to the rare event\nidentification problem.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted in INFORMS Journal of Data Science",
    "pdf_url": "http://arxiv.org/pdf/2403.17914v1",
    "published_date": "2024-03-26 17:51:06 UTC",
    "updated_date": "2024-03-26 17:51:06 UTC"
  },
  {
    "arxiv_id": "2403.17891v1",
    "title": "Image-based Novel Fault Detection with Deep Learning Classifiers using Hierarchical Labels",
    "authors": [
      "Nurettin Sergin",
      "Jiayu Huang",
      "Tzyy-Shuh Chang",
      "Hao Yan"
    ],
    "abstract": "One important characteristic of modern fault classification systems is the\nability to flag the system when faced with previously unseen fault types. This\nwork considers the unknown fault detection capabilities of deep neural\nnetwork-based fault classifiers. Specifically, we propose a methodology on how,\nwhen available, labels regarding the fault taxonomy can be used to increase\nunknown fault detection performance without sacrificing model performance. To\nachieve this, we propose to utilize soft label techniques to improve the\nstate-of-the-art deep novel fault detection techniques during the training\nprocess and novel hierarchically consistent detection statistics for online\nnovel fault detection. Finally, we demonstrated increased detection performance\non novel fault detection in inspection images from the hot steel rolling\nprocess, with results well replicated across multiple scenarios and baseline\ndetection methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in IISE Transaction",
    "pdf_url": "http://arxiv.org/pdf/2403.17891v1",
    "published_date": "2024-03-26 17:22:29 UTC",
    "updated_date": "2024-03-26 17:22:29 UTC"
  },
  {
    "arxiv_id": "2403.17873v1",
    "title": "Addressing Social Misattributions of Large Language Models: An HCXAI-based Approach",
    "authors": [
      "Andrea Ferrario",
      "Alberto Termine",
      "Alessandro Facchini"
    ],
    "abstract": "Human-centered explainable AI (HCXAI) advocates for the integration of social\naspects into AI explanations. Central to the HCXAI discourse is the Social\nTransparency (ST) framework, which aims to make the socio-organizational\ncontext of AI systems accessible to their users. In this work, we suggest\nextending the ST framework to address the risks of social misattributions in\nLarge Language Models (LLMs), particularly in sensitive areas like mental\nhealth. In fact LLMs, which are remarkably capable of simulating roles and\npersonas, may lead to mismatches between designers' intentions and users'\nperceptions of social attributes, risking to promote emotional manipulation and\ndangerous behaviors, cases of epistemic injustice, and unwarranted trust. To\naddress these issues, we propose enhancing the ST framework with a fifth\n'W-question' to clarify the specific social attributions assigned to LLMs by\nits designers and users. This addition aims to bridge the gap between LLM\ncapabilities and user perceptions, promoting the ethically responsible\ndevelopment and use of LLM-based technology.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Extended version of the manuscript accepted for the ACM CHI Workshop\n  on Human-Centered Explainable AI 2024 (HCXAI24)",
    "pdf_url": "http://arxiv.org/pdf/2403.17873v1",
    "published_date": "2024-03-26 17:02:42 UTC",
    "updated_date": "2024-03-26 17:02:42 UTC"
  },
  {
    "arxiv_id": "2403.17847v1",
    "title": "Climate Downscaling: A Deep-Learning Based Super-resolution Model of Precipitation Data with Attention Block and Skip Connections",
    "authors": [
      "Chia-Hao Chiang",
      "Zheng-Han Huang",
      "Liwen Liu",
      "Hsin-Chien Liang",
      "Yi-Chi Wang",
      "Wan-Ling Tseng",
      "Chao Wang",
      "Che-Ta Chen",
      "Ko-Chih Wang"
    ],
    "abstract": "Human activities accelerate consumption of fossil fuels and produce\ngreenhouse gases, resulting in urgent issues today: global warming and the\nclimate change. These indirectly cause severe natural disasters, plenty of\nlives suffering and huge losses of agricultural properties. To mitigate impacts\non our lands, scientists are developing renewable, reusable, and clean energies\nand climatologists are trying to predict the extremes. Meanwhile, governments\nare publicizing resource-saving policies for a more eco-friendly society and\narousing environment awareness. One of the most influencing factors is the\nprecipitation, bringing condensed water vapor onto lands. Water resources are\nthe most significant but basic needs in society, not only supporting our\nlivings, but also economics. In Taiwan, although the average annual\nprecipitation is up to 2,500 millimeter (mm), the water allocation for each\nperson is lower than the global average due to drastically geographical\nelevation changes and uneven distribution through the year. Thus, it is crucial\nto track and predict the rainfall to make the most use of it and to prevent the\nfloods. However, climate models have limited resolution and require intensive\ncomputational power for local-scale use. Therefore, we proposed a deep\nconvolutional neural network with skip connections, attention blocks, and\nauxiliary data concatenation, in order to downscale the low-resolution\nprecipitation data into high-resolution one. Eventually, we compare with other\nclimate downscaling methods and show better performance in metrics of Mean\nAbsolute Error (MAE), Root Mean Square Error (RMSE), Pearson Correlation,\nstructural similarity index (SSIM), and forecast indicators.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17847v1",
    "published_date": "2024-03-26 16:36:50 UTC",
    "updated_date": "2024-03-26 16:36:50 UTC"
  },
  {
    "arxiv_id": "2403.17846v2",
    "title": "Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation",
    "authors": [
      "Abdelrhman Werby",
      "Chenguang Huang",
      "Martin BÃ¼chner",
      "Abhinav Valada",
      "Wolfram Burgard"
    ],
    "abstract": "Recent open-vocabulary robot mapping methods enrich dense geometric maps with\npre-trained visual-language features. While these maps allow for the prediction\nof point-wise saliency maps when queried for a certain language concept,\nlarge-scale environments and abstract queries beyond the object level still\npose a considerable hurdle, ultimately limiting language-grounded robotic\nnavigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D\nscene graph mapping approach for language-grounded robot navigation. Leveraging\nopen-vocabulary vision foundation models, we first obtain state-of-the-art\nopen-vocabulary segment-level maps in 3D and subsequently construct a 3D scene\ngraph hierarchy consisting of floor, room, and object concepts, each enriched\nwith open-vocabulary features. Our approach is able to represent multi-story\nbuildings and allows robotic traversal of those using a cross-floor Voronoi\ngraph. HOV-SG is evaluated on three distinct datasets and surpasses previous\nbaselines in open-vocabulary semantic accuracy on the object, room, and floor\nlevel while producing a 75% reduction in representation size compared to dense\nopen-vocabulary maps. In order to prove the efficacy and generalization\ncapabilities of HOV-SG, we showcase successful long-horizon\nlanguage-conditioned robot navigation within real-world multi-storage\nenvironments. We provide code and trial video data at http://hovsg.github.io/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Code and video are available at http://hovsg.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2403.17846v2",
    "published_date": "2024-03-26 16:36:43 UTC",
    "updated_date": "2024-06-03 17:12:25 UTC"
  },
  {
    "arxiv_id": "2403.17839v2",
    "title": "ReMamber: Referring Image Segmentation with Mamba Twister",
    "authors": [
      "Yuhuan Yang",
      "Chaofan Ma",
      "Jiangchao Yao",
      "Zhun Zhong",
      "Ya Zhang",
      "Yanfeng Wang"
    ],
    "abstract": "Referring Image Segmentation~(RIS) leveraging transformers has achieved great\nsuccess on the interpretation of complex visual-language tasks. However, the\nquadratic computation cost makes it resource-consuming in capturing long-range\nvisual-language dependencies. Fortunately, Mamba addresses this with efficient\nlinear complexity in processing. However, directly applying Mamba to\nmulti-modal interactions presents challenges, primarily due to inadequate\nchannel interactions for the effective fusion of multi-modal data. In this\npaper, we propose ReMamber, a novel RIS architecture that integrates the power\nof Mamba with a multi-modal Mamba Twister block. The Mamba Twister explicitly\nmodels image-text interaction, and fuses textual and visual features through\nits unique channel and spatial twisting mechanism. We achieve competitive\nresults on three challenging benchmarks with a simple and efficient\narchitecture. Moreover, we conduct thorough analyses of ReMamber and discuss\nother fusion designs using Mamba. These provide valuable perspectives for\nfuture research. The code has been released at:\nhttps://github.com/yyh-rain-song/ReMamber.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.17839v2",
    "published_date": "2024-03-26 16:27:37 UTC",
    "updated_date": "2024-07-25 02:08:30 UTC"
  },
  {
    "arxiv_id": "2403.17827v2",
    "title": "DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions",
    "authors": [
      "Sammy Christen",
      "Shreyas Hampali",
      "Fadime Sener",
      "Edoardo Remelli",
      "Tomas Hodan",
      "Eric Sauser",
      "Shugao Ma",
      "Bugra Tekin"
    ],
    "abstract": "Generating natural hand-object interactions in 3D is challenging as the\nresulting hand and object motions are expected to be physically plausible and\nsemantically meaningful. Furthermore, generalization to unseen objects is\nhindered by the limited scale of available hand-object interaction datasets. In\nthis paper, we propose a novel method, dubbed DiffH2O, which can synthesize\nrealistic, one or two-handed object interactions from provided text prompts and\ngeometry of the object. The method introduces three techniques that enable\neffective learning from limited data. First, we decompose the task into a\ngrasping stage and an text-based manipulation stage and use separate diffusion\nmodels for each. In the grasping stage, the model only generates hand motions,\nwhereas in the manipulation phase both hand and object poses are synthesized.\nSecond, we propose a compact representation that tightly couples hand and\nobject poses and helps in generating realistic hand-object interactions. Third,\nwe propose two different guidance schemes to allow more control of the\ngenerated motions: grasp guidance and detailed textual guidance. Grasp guidance\ntakes a single target grasping pose and guides the diffusion model to reach\nthis grasp at the end of the grasping stage, which provides control over the\ngrasping pose. Given a grasping motion from this stage, multiple different\nactions can be prompted in the manipulation phase. For the textual guidance, we\ncontribute comprehensive text descriptions to the GRAB dataset and show that\nthey enable our method to have more fine-grained control over hand-object\ninteractions. Our quantitative and qualitative evaluation demonstrates that the\nproposed method outperforms baseline methods and leads to natural hand-object\nmotions.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://diffh2o.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2403.17827v2",
    "published_date": "2024-03-26 16:06:42 UTC",
    "updated_date": "2024-12-23 17:36:22 UTC"
  },
  {
    "arxiv_id": "2403.17826v1",
    "title": "On the Computational Complexity of Stackelberg Planning and Meta-Operator Verification: Technical Report",
    "authors": [
      "Gregor Behnke",
      "Marcel Steinmetz"
    ],
    "abstract": "Stackelberg planning is a recently introduced single-turn two-player\nadversarial planning model, where two players are acting in a joint classical\nplanning task, the objective of the first player being hampering the second\nplayer from achieving its goal. This places the Stackelberg planning problem\nsomewhere between classical planning and general combinatorial two-player\ngames. But, where exactly? All investigations of Stackelberg planning so far\nfocused on practical aspects. We close this gap by conducting the first\ntheoretical complexity analysis of Stackelberg planning. We show that in\ngeneral Stackelberg planning is actually no harder than classical planning.\nUnder a polynomial plan-length restriction, however, Stackelberg planning is a\nlevel higher up in the polynomial complexity hierarchy, suggesting that\ncompilations into classical planning come with a worst-case exponential\nplan-length increase. In attempts to identify tractable fragments, we further\nstudy its complexity under various planning task restrictions, showing that\nStackelberg planning remains intractable where classical planning is not. We\nfinally inspect the complexity of meta-operator verification, a problem that\nhas been recently connected to Stackelberg planning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Presented at ICAPS24",
    "pdf_url": "http://arxiv.org/pdf/2403.17826v1",
    "published_date": "2024-03-26 16:06:33 UTC",
    "updated_date": "2024-03-26 16:06:33 UTC"
  },
  {
    "arxiv_id": "2403.17819v1",
    "title": "Accelerating Radio Spectrum Regulation Workflows with Large Language Models (LLMs)",
    "authors": [
      "Amir Ghasemi",
      "Paul Guinand"
    ],
    "abstract": "Wireless spectrum regulation is a complex and demanding process due to the\nrapid pace of technological progress, increasing demand for spectrum, and a\nmultitude of stakeholders with potentially conflicting interests, alongside\nsignificant economic implications. To navigate this, regulators must engage\neffectively with all parties, keep pace with global technology trends, conduct\ntechnical evaluations, issue licenses in a timely manner, and comply with\nvarious legal and policy frameworks.\n  In light of these challenges, this paper demonstrates example applications of\nLarge Language Models (LLMs) to expedite spectrum regulatory processes. We\nexplore various roles that LLMs can play in this context while identifying some\nof the challenges to address. The paper also offers practical case studies and\ninsights, with appropriate experiments, highlighting the transformative\npotential of LLMs in spectrum management.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17819v1",
    "published_date": "2024-03-26 15:54:48 UTC",
    "updated_date": "2024-03-26 15:54:48 UTC"
  },
  {
    "arxiv_id": "2403.17814v1",
    "title": "D-PAD: Deep-Shallow Multi-Frequency Patterns Disentangling for Time Series Forecasting",
    "authors": [
      "Xiaobing Yuan",
      "Ling Chen"
    ],
    "abstract": "In time series forecasting, effectively disentangling intricate temporal\npatterns is crucial. While recent works endeavor to combine decomposition\ntechniques with deep learning, multiple frequencies may still be mixed in the\ndecomposed components, e.g., trend and seasonal. Furthermore, frequency domain\nanalysis methods, e.g., Fourier and wavelet transforms, have limitations in\nresolution in the time domain and adaptability. In this paper, we propose\nD-PAD, a deep-shallow multi-frequency patterns disentangling neural network for\ntime series forecasting. Specifically, a multi-component decomposing (MCD)\nblock is introduced to decompose the series into components with different\nfrequency ranges, corresponding to the \"shallow\" aspect. A\ndecomposition-reconstruction-decomposition (D-R-D) module is proposed to\nprogressively extract the information of frequencies mixed in the components,\ncorresponding to the \"deep\" aspect. After that, an interaction and fusion (IF)\nmodule is used to further analyze the components. Extensive experiments on\nseven real-world datasets demonstrate that D-PAD achieves the state-of-the-art\nperformance, outperforming the best baseline by an average of 9.48% and 7.15%\nin MSE and MAE, respectively.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17814v1",
    "published_date": "2024-03-26 15:52:36 UTC",
    "updated_date": "2024-03-26 15:52:36 UTC"
  },
  {
    "arxiv_id": "2403.17787v2",
    "title": "Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models Versus Fine-Tuned Vision Transformers in Image-Based Security Applications",
    "authors": [
      "Fouad Trad",
      "Ali Chehab"
    ],
    "abstract": "The success of Large Language Models (LLMs) has led to a parallel rise in the\ndevelopment of Large Multimodal Models (LMMs), which have begun to transform a\nvariety of applications. These sophisticated multimodal models are designed to\ninterpret and analyze complex data by integrating multiple modalities such as\ntext and images, thereby opening new avenues for a range of applications. This\npaper investigates the applicability and effectiveness of prompt-engineered\nLMMs that process both images and text, including models such as LLaVA,\nBakLLaVA, Moondream, Gemini-pro-vision, and GPT-4o, compared to fine-tuned\nVision Transformer (ViT) models in addressing critical security challenges. We\nfocus on two distinct security tasks: 1) a visually evident task of detecting\nsimple triggers, such as small pixel variations in images that could be\nexploited to access potential backdoors in the models, and 2) a visually\nnon-evident task of malware classification through visual representations. In\nthe visually evident task, some LMMs, such as Gemini-pro-vision and GPT-4o,\nhave demonstrated the potential to achieve good performance with careful prompt\nengineering, with GPT-4o achieving the highest accuracy and F1-score of 91.9\\%\nand 91\\%, respectively. However, the fine-tuned ViT models exhibit perfect\nperformance in this task due to its simplicity. For the visually non-evident\ntask, the results highlight a significant divergence in performance, with ViT\nmodels achieving F1-scores of 97.11\\% in predicting 25 malware classes and\n97.61\\% in predicting 5 malware families, whereas LMMs showed suboptimal\nperformance despite iterative prompt improvements. This study not only\nshowcases the strengths and limitations of prompt-engineered LMMs in\ncybersecurity applications but also emphasizes the unmatched efficacy of\nfine-tuned ViT models for precise and dependable tasks.",
    "categories": [
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17787v2",
    "published_date": "2024-03-26 15:20:49 UTC",
    "updated_date": "2024-06-10 10:07:24 UTC"
  },
  {
    "arxiv_id": "2403.17784v1",
    "title": "SciCapenter: Supporting Caption Composition for Scientific Figures with Machine-Generated Captions and Ratings",
    "authors": [
      "Ting-Yao Hsu",
      "Chieh-Yang Huang",
      "Shih-Hong Huang",
      "Ryan Rossi",
      "Sungchul Kim",
      "Tong Yu",
      "C. Lee Giles",
      "Ting-Hao K. Huang"
    ],
    "abstract": "Crafting effective captions for figures is important. Readers heavily depend\non these captions to grasp the figure's message. However, despite a\nwell-developed set of AI technologies for figures and captions, these have\nrarely been tested for usefulness in aiding caption writing. This paper\nintroduces SciCapenter, an interactive system that puts together cutting-edge\nAI technologies for scientific figure captions to aid caption composition.\nSciCapenter generates a variety of captions for each figure in a scholarly\narticle, providing scores and a comprehensive checklist to assess caption\nquality across multiple critical aspects, such as helpfulness, OCR mention, key\ntakeaways, and visual properties reference. Users can directly edit captions in\nSciCapenter, resubmit for revised evaluations, and iteratively refine them. A\nuser study with Ph.D. students indicates that SciCapenter significantly lowers\nthe cognitive load of caption writing. Participants' feedback further offers\nvaluable design insights for future systems aiming to enhance caption writing.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "CHI EA '24: Extended Abstracts of the 2024 CHI Conference on Human\n  Factors in Computing Systems",
    "pdf_url": "http://arxiv.org/pdf/2403.17784v1",
    "published_date": "2024-03-26 15:16:14 UTC",
    "updated_date": "2024-03-26 15:16:14 UTC"
  },
  {
    "arxiv_id": "2403.17778v2",
    "title": "Towards a FAIR Documentation of Workflows and Models in Applied Mathematics",
    "authors": [
      "Marco Reidelbach",
      "BjÃ¶rn Schembera",
      "Marcus Weber"
    ],
    "abstract": "Modeling-Simulation-Optimization workflows play a fundamental role in applied\nmathematics. The Mathematical Research Data Initiative, MaRDI, responded to\nthis by developing a FAIR and machine-interpretable template for a\ncomprehensive documentation of such workflows. MaRDMO, a Plugin for the\nResearch Data Management Organiser, enables scientists from diverse fields to\ndocument and publish their workflows on the MaRDI Portal seamlessly using the\nMaRDI template. Central to these workflows are mathematical models. MaRDI\naddresses them with the MathModDB ontology, offering a structured formal model\ndescription. Here, we showcase the interaction between MaRDMO and the MathModDB\nKnowledge Graph through an algebraic modeling workflow from the Digital\nHumanities. This demonstration underscores the versatility of both services\nbeyond their original numerical domain.",
    "categories": [
      "cs.AI",
      "cs.DB",
      "cs.DL",
      "H.3.3; H.3.7; E.0"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17778v2",
    "published_date": "2024-03-26 15:11:18 UTC",
    "updated_date": "2024-07-31 08:19:16 UTC"
  },
  {
    "arxiv_id": "2403.17768v2",
    "title": "SciNews: From Scholarly Complexities to Public Narratives -- A Dataset for Scientific News Report Generation",
    "authors": [
      "Dongqi Liu",
      "Yifan Wang",
      "Jia Loy",
      "Vera Demberg"
    ],
    "abstract": "Scientific news reports serve as a bridge, adeptly translating complex\nresearch articles into reports that resonate with the broader public. The\nautomated generation of such narratives enhances the accessibility of scholarly\ninsights. In this paper, we present a new corpus to facilitate this paradigm\ndevelopment. Our corpus comprises a parallel compilation of academic\npublications and their corresponding scientific news reports across nine\ndisciplines. To demonstrate the utility and reliability of our dataset, we\nconduct an extensive analysis, highlighting the divergences in readability and\nbrevity between scientific news narratives and academic manuscripts. We\nbenchmark our dataset employing state-of-the-art text generation models. The\nevaluation process involves both automatic and human evaluation, which lays the\ngroundwork for future explorations into the automated generation of scientific\nnews reports. The dataset and code related to this work are available at\nhttps://dongqi.me/projects/SciNews.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "LREC-COLING 2024 Main Conference Paper",
    "pdf_url": "http://arxiv.org/pdf/2403.17768v2",
    "published_date": "2024-03-26 14:54:48 UTC",
    "updated_date": "2024-12-10 09:12:46 UTC"
  },
  {
    "arxiv_id": "2403.17995v1",
    "title": "Semi-Supervised Image Captioning Considering Wasserstein Graph Matching",
    "authors": [
      "Yang Yang"
    ],
    "abstract": "Image captioning can automatically generate captions for the given images,\nand the key challenge is to learn a mapping function from visual features to\nnatural language features. Existing approaches are mostly supervised ones,\ni.e., each image has a corresponding sentence in the training set. However,\nconsidering that describing images always requires a huge of manpower, we\nusually have limited amount of described images (i.e., image-text pairs) and a\nlarge number of undescribed images in real-world applications. Thereby, a\ndilemma is the \"Semi-Supervised Image Captioning\". To solve this problem, we\npropose a novel Semi-Supervised Image Captioning method considering Wasserstein\nGraph Matching (SSIC-WGM), which turns to adopt the raw image inputs to\nsupervise the generated sentences. Different from traditional single modal\nsemi-supervised methods, the difficulty of semi-supervised cross-modal learning\nlies in constructing intermediately comparable information among heterogeneous\nmodalities. In this paper, SSIC-WGM adopts the successful scene graphs as\nintermediate information, and constrains the generated sentences from two\naspects: 1) inter-modal consistency. SSIC-WGM constructs the scene graphs of\nthe raw image and generated sentence respectively, then employs the wasserstein\ndistance to better measure the similarity between region embeddings of\ndifferent graphs. 2) intra-modal consistency. SSIC-WGM takes the data\naugmentation techniques for the raw images, then constrains the consistency\namong augmented images and generated sentences. Consequently, SSIC-WGM combines\nthe cross-modal pseudo supervision and structure invariant measure for\nefficiently using the undescribed images, and learns more reasonable mapping\nfunction.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17995v1",
    "published_date": "2024-03-26 14:47:05 UTC",
    "updated_date": "2024-03-26 14:47:05 UTC"
  },
  {
    "arxiv_id": "2403.17755v1",
    "title": "DataCook: Crafting Anti-Adversarial Examples for Healthcare Data Copyright Protection",
    "authors": [
      "Sihan Shang",
      "Jiancheng Yang",
      "Zhenglong Sun",
      "Pascal Fua"
    ],
    "abstract": "In the realm of healthcare, the challenges of copyright protection and\nunauthorized third-party misuse are increasingly significant. Traditional\nmethods for data copyright protection are applied prior to data distribution,\nimplying that models trained on these data become uncontrollable. This paper\nintroduces a novel approach, named DataCook, designed to safeguard the\ncopyright of healthcare data during the deployment phase. DataCook operates by\n\"cooking\" the raw data before distribution, enabling the development of models\nthat perform normally on this processed data. However, during the deployment\nphase, the original test data must be also \"cooked\" through DataCook to ensure\nnormal model performance. This process grants copyright holders control over\nauthorization during the deployment phase. The mechanism behind DataCook is by\ncrafting anti-adversarial examples (AntiAdv), which are designed to enhance\nmodel confidence, as opposed to standard adversarial examples (Adv) that aim to\nconfuse models. Similar to Adv, AntiAdv introduces imperceptible perturbations,\nensuring that the data processed by DataCook remains easily understandable. We\nconducted extensive experiments on MedMNIST datasets, encompassing both 2D/3D\ndata and the high-resolution variants. The outcomes indicate that DataCook\neffectively meets its objectives, preventing models trained on AntiAdv from\nanalyzing unauthorized data effectively, without compromising the validity and\naccuracy of the data in legitimate scenarios. Code and data are available at\nhttps://github.com/MedMNIST/DataCook.",
    "categories": [
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17755v1",
    "published_date": "2024-03-26 14:44:51 UTC",
    "updated_date": "2024-03-26 14:44:51 UTC"
  },
  {
    "arxiv_id": "2403.17742v1",
    "title": "Using Stratified Sampling to Improve LIME Image Explanations",
    "authors": [
      "Muhammad Rashid",
      "Elvio G. Amparore",
      "Enrico Ferrari",
      "Damiano Verda"
    ],
    "abstract": "We investigate the use of a stratified sampling approach for LIME Image, a\npopular model-agnostic explainable AI method for computer vision tasks, in\norder to reduce the artifacts generated by typical Monte Carlo sampling. Such\nartifacts are due to the undersampling of the dependent variable in the\nsynthetic neighborhood around the image being explained, which may result in\ninadequate explanations due to the impossibility of fitting a linear regressor\non the sampled data. We then highlight a connection with the Shapley theory,\nwhere similar arguments about undersampling and sample relevance were suggested\nin the past. We derive all the formulas and adjustment factors required for an\nunbiased stratified sampling estimator. Experiments show the efficacy of the\nproposed approach.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17742v1",
    "published_date": "2024-03-26 14:30:23 UTC",
    "updated_date": "2024-03-26 14:30:23 UTC"
  },
  {
    "arxiv_id": "2403.17740v3",
    "title": "All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating Prediction",
    "authors": [
      "Shuheng Fang",
      "Kangfei Zhao",
      "Yu Rong",
      "Zhixun Li",
      "Jeffrey Xu Yu"
    ],
    "abstract": "Cold-start rating prediction is a fundamental problem in recommender systems\nthat has been extensively studied. Many methods have been proposed that exploit\nexplicit relations among existing data, such as collaborative filtering, social\nrecommendations and heterogeneous information network, to alleviate the data\ninsufficiency issue for cold-start users and items. However, the explicit\nrelations constructed based on data between different roles may be unreliable\nand irrelevant, which limits the performance ceiling of the specific\nrecommendation task. Motivated by this, in this paper, we propose a flexible\nframework dubbed heterogeneous interaction rating network (HIRE). HIRE dose not\nsolely rely on the pre-defined interaction pattern or the manually constructed\nheterogeneous information network. Instead, we devise a Heterogeneous\nInteraction Module (HIM) to jointly model the heterogeneous interactions and\ndirectly infer the important interactions via the observed data. In the\nexperiments, we evaluate our model under three cold-start settings on three\nreal-world datasets. The experimental results show that HIRE outperforms other\nbaselines by a large margin. Furthermore, we visualize the inferred\ninteractions of HIRE to confirm the contribution of our model.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "14 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.17740v3",
    "published_date": "2024-03-26 14:29:34 UTC",
    "updated_date": "2024-12-06 05:26:40 UTC"
  },
  {
    "arxiv_id": "2403.17735v1",
    "title": "Out-of-distribution Rumor Detection via Test-Time Adaptation",
    "authors": [
      "Xiang Tao",
      "Mingqing Zhang",
      "Qiang Liu",
      "Shu Wu",
      "Liang Wang"
    ],
    "abstract": "Due to the rapid spread of rumors on social media, rumor detection has become\nan extremely important challenge. Existing methods for rumor detection have\nachieved good performance, as they have collected enough corpus from the same\ndata distribution for model training. However, significant distribution shifts\nbetween the training data and real-world test data occur due to differences in\nnews topics, social media platforms, languages and the variance in propagation\nscale caused by news popularity. This leads to a substantial decline in the\nperformance of these existing methods in Out-Of-Distribution (OOD) situations.\nTo address this problem, we propose a simple and efficient method named\nTest-time Adaptation for Rumor Detection under distribution shifts (TARD). This\nmethod models the propagation of news in the form of a propagation graph, and\nbuilds propagation graph test-time adaptation framework, enhancing the model's\nadaptability and robustness when facing OOD problems. Extensive experiments\nconducted on two group datasets collected from real-world social platforms\ndemonstrate that our framework outperforms the state-of-the-art methods in\nperformance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17735v1",
    "published_date": "2024-03-26 14:24:01 UTC",
    "updated_date": "2024-03-26 14:24:01 UTC"
  },
  {
    "arxiv_id": "2403.17726v4",
    "title": "Tiny Models are the Computational Saver for Large Models",
    "authors": [
      "Qingyuan Wang",
      "Barry Cardiff",
      "Antoine FrappÃ©",
      "Benoit Larras",
      "Deepu John"
    ],
    "abstract": "This paper introduces TinySaver, an early-exit-like dynamic model compression\napproach which employs tiny models to substitute large models adaptively.\nDistinct from traditional compression techniques, dynamic methods like\nTinySaver can leverage the difficulty differences to allow certain inputs to\ncomplete their inference processes early, thereby conserving computational\nresources. Most existing early exit designs are implemented by attaching\nadditional network branches to the model's backbone. Our study, however,\nreveals that completely independent tiny models can replace a substantial\nportion of the larger models' job with minimal impact on performance. Employing\nthem as the first exit can remarkably enhance computational efficiency. By\nsearching and employing the most appropriate tiny model as the computational\nsaver for a given large model, the proposed approaches work as a novel and\ngeneric method to model compression. This finding will help the research\ncommunity in exploring new compression methods to address the escalating\ncomputational demands posed by rapidly evolving AI models. Our evaluation of\nthis approach in ImageNet-1k classification demonstrates its potential to\nreduce the number of compute operations by up to 90\\%, with only negligible\nlosses in performance, across various modern vision models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17726v4",
    "published_date": "2024-03-26 14:14:30 UTC",
    "updated_date": "2025-01-13 12:38:41 UTC"
  },
  {
    "arxiv_id": "2403.17710v4",
    "title": "Optimization-based Prompt Injection Attack to LLM-as-a-Judge",
    "authors": [
      "Jiawen Shi",
      "Zenghui Yuan",
      "Yinuo Liu",
      "Yue Huang",
      "Pan Zhou",
      "Lichao Sun",
      "Neil Zhenqiang Gong"
    ],
    "abstract": "LLM-as-a-Judge uses a large language model (LLM) to select the best response\nfrom a set of candidates for a given question. LLM-as-a-Judge has many\napplications such as LLM-powered search, reinforcement learning with AI\nfeedback (RLAIF), and tool selection. In this work, we propose JudgeDeceiver,\nan optimization-based prompt injection attack to LLM-as-a-Judge. JudgeDeceiver\ninjects a carefully crafted sequence into an attacker-controlled candidate\nresponse such that LLM-as-a-Judge selects the candidate response for an\nattacker-chosen question no matter what other candidate responses are.\nSpecifically, we formulate finding such sequence as an optimization problem and\npropose a gradient based method to approximately solve it. Our extensive\nevaluation shows that JudgeDeceive is highly effective, and is much more\neffective than existing prompt injection attacks that manually craft the\ninjected sequences and jailbreak attacks when extended to our problem. We also\nshow the effectiveness of JudgeDeceiver in three case studies, i.e.,\nLLM-powered search, RLAIF, and tool selection. Moreover, we consider defenses\nincluding known-answer detection, perplexity detection, and perplexity windowed\ndetection. Our results show these defenses are insufficient, highlighting the\nurgent need for developing new defense strategies. Our implementation is\navailable at this repository: https://github.com/ShiJiawenwen/JudgeDeceiver.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "To appear in the Proceedings of The ACM Conference on Computer and\n  Communications Security (CCS), 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.17710v4",
    "published_date": "2024-03-26 13:58:00 UTC",
    "updated_date": "2025-03-03 03:36:17 UTC"
  },
  {
    "arxiv_id": "2403.17706v2",
    "title": "A Large Language Model Guided Topic Refinement Mechanism for Short Text Modeling",
    "authors": [
      "Shuyu Chang",
      "Rui Wang",
      "Peng Ren",
      "Qi Wang",
      "Haiping Huang"
    ],
    "abstract": "Modeling topics effectively in short texts, such as tweets and news snippets,\nis crucial to capturing rapidly evolving social trends. Existing topic models\noften struggle to accurately capture the underlying semantic patterns of short\ntexts, primarily due to the sparse nature of such data. This nature of texts\nleads to an unavoidable lack of co-occurrence information, which hinders the\ncoherence and granularity of mined topics. This paper introduces a novel\nmodel-agnostic mechanism, termed Topic Refinement, which leverages the advanced\ntext comprehension capabilities of Large Language Models (LLMs) for short-text\ntopic modeling. Unlike traditional methods, this post-processing mechanism\nenhances the quality of topics extracted by various topic modeling methods\nthrough prompt engineering. We guide LLMs in identifying semantically intruder\nwords within the extracted topics and suggesting coherent alternatives to\nreplace these words. This process mimics human-like identification, evaluation,\nand refinement of the extracted topics. Extensive experiments on four diverse\ndatasets demonstrate that Topic Refinement boosts the topic quality and\nimproves the performance in topic-related text classification tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Extended version of paper accepted at DASFAA 2025 (16 pages, 6\n  figures)",
    "pdf_url": "http://arxiv.org/pdf/2403.17706v2",
    "published_date": "2024-03-26 13:50:34 UTC",
    "updated_date": "2025-02-16 14:36:45 UTC"
  },
  {
    "arxiv_id": "2403.17698v1",
    "title": "MEP: Multiple Kernel Learning Enhancing Relative Positional Encoding Length Extrapolation",
    "authors": [
      "Weiguo Gao"
    ],
    "abstract": "When the predicted sequence length exceeds the length seen during training,\nthe transformer's inference accuracy diminishes. Existing relative position\nencoding methods, such as those based on the ALiBi technique, address the\nlength extrapolation challenge exclusively through the implementation of a\nsingle kernel function, which introduces a constant bias to every post-softmax\nattention scores according to their distance. These approaches do not\ninvestigate or employ multiple kernel functions to address the extrapolation\nchallenge. Drawing on the ALiBi approach, this study proposes a novel relative\npositional encoding method, called MEP, which employs a weighted average to\ncombine distinct kernel functions(such as the exponential kernel and the\nGaussian kernel) to generate a bias that is applied to post-softmax attention\nscores. Initially, the framework utilizes various kernel functions to construct\nmultiple kernel functions. Each kernel function adheres to a consistent mean\nweight coefficient, harnessing the synergistic advantages of different kernels\nto formulate an innovative bias function. Subsequently, specific slopes are\ntailored for each kernel function, applying penalties at varying rates, to\nenhance the model's extrapolation capabilities. Finally, this bias is\nseamlessly incorporated as a penalty to the post-softmax scores. We present two\ndistinct versions of our method: a parameter-free variant that requires no new\nlearnable parameters, which enhances length extrapolation capabilities without\ncompromising training efficiency, and a parameterized variant capable of\nintegrating state-of-the-art techniques. Empirical evaluations across diverse\ndatasets have demonstrated that both variants of our method achieve\nstate-of-the-art performance, outperforming traditional parameter-free and\nparameterized approaches.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17698v1",
    "published_date": "2024-03-26 13:38:06 UTC",
    "updated_date": "2024-03-26 13:38:06 UTC"
  },
  {
    "arxiv_id": "2403.17693v1",
    "title": "ExpressEdit: Video Editing with Natural Language and Sketching",
    "authors": [
      "Bekzat Tilekbay",
      "Saelyne Yang",
      "Michal Lewkowicz",
      "Alex Suryapranata",
      "Juho Kim"
    ],
    "abstract": "Informational videos serve as a crucial source for explaining conceptual and\nprocedural knowledge to novices and experts alike. When producing informational\nvideos, editors edit videos by overlaying text/images or trimming footage to\nenhance the video quality and make it more engaging. However, video editing can\nbe difficult and time-consuming, especially for novice video editors who often\nstruggle with expressing and implementing their editing ideas. To address this\nchallenge, we first explored how multimodality$-$natural language (NL) and\nsketching, which are natural modalities humans use for expression$-$can be\nutilized to support video editors in expressing video editing ideas. We\ngathered 176 multimodal expressions of editing commands from 10 video editors,\nwhich revealed the patterns of use of NL and sketching in describing edit\nintents. Based on the findings, we present ExpressEdit, a system that enables\nediting videos via NL text and sketching on the video frame. Powered by LLM and\nvision models, the system interprets (1) temporal, (2) spatial, and (3)\noperational references in an NL command and spatial references from sketching.\nThe system implements the interpreted edits, which then the user can iterate\non. An observational study (N=10) showed that ExpressEdit enhanced the ability\nof novice video editors to express and implement their edit ideas. The system\nallowed participants to perform edits more efficiently and generate more ideas\nby generating edits based on user's multimodal edit commands and supporting\niterations on the editing commands. This work offers insights into the design\nof future multimodal interfaces and AI-based pipelines for video editing.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "22 pages, 5 figures, to be published in ACM IUI 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.17693v1",
    "published_date": "2024-03-26 13:34:21 UTC",
    "updated_date": "2024-03-26 13:34:21 UTC"
  },
  {
    "arxiv_id": "2403.17683v2",
    "title": "Solution for Emotion Prediction Competition of Workshop on Emotionally and Culturally Intelligent AI",
    "authors": [
      "Shengdong Xu",
      "Zhouyang Chi",
      "Yang Yang"
    ],
    "abstract": "This report provide a detailed description of the method that we explored and\nproposed in the WECIA Emotion Prediction Competition (EPC), which predicts a\nperson's emotion through an artistic work with a comment. The dataset of this\ncompetition is ArtELingo, designed to encourage work on diversity across\nlanguages and cultures. The dataset has two main challenges, namely modal\nimbalance problem and language-cultural differences problem. In order to\naddress this issue, we propose a simple yet effective approach called\nsingle-multi modal with Emotion-Cultural specific prompt(ECSP), which focuses\non using the single modal message to enhance the performance of multimodal\nmodels and a well-designed prompt to reduce cultural differences problem. To\nclarify, our approach contains two main blocks:\n(1)XLM-R\\cite{conneau2019unsupervised} based unimodal model and\nX$^2$-VLM\\cite{zeng2022x} based multimodal model (2) Emotion-Cultural specific\nprompt. Our approach ranked first in the final test with a score of 0.627.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17683v2",
    "published_date": "2024-03-26 13:14:18 UTC",
    "updated_date": "2024-03-31 14:44:06 UTC"
  },
  {
    "arxiv_id": "2403.17677v1",
    "title": "Onboard deep lossless and near-lossless predictive coding of hyperspectral images with line-based attention",
    "authors": [
      "Diego Valsesia",
      "Tiziano Bianchi",
      "Enrico Magli"
    ],
    "abstract": "Deep learning methods have traditionally been difficult to apply to\ncompression of hyperspectral images onboard of spacecrafts, due to the large\ncomputational complexity needed to achieve adequate representational power, as\nwell as the lack of suitable datasets for training and testing. In this paper,\nwe depart from the traditional autoencoder approach and we design a predictive\nneural network, called LineRWKV, that works recursively line-by-line to limit\nmemory consumption. In order to achieve that, we adopt a novel hybrid\nattentive-recursive operation that combines the representational advantages of\nTransformers with the linear complexity and recursive implementation of\nrecurrent neural networks. The compression algorithm performs prediction of\neach pixel using LineRWKV, followed by entropy coding of the residual.\nExperiments on the HySpecNet-11k dataset and PRISMA images show that LineRWKV\nis the first deep-learning method to outperform CCSDS-123.0-B-2 at lossless and\nnear-lossless compression. Promising throughput results are also evaluated on a\n7W embedded system.",
    "categories": [
      "eess.IV",
      "cs.AI"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17677v1",
    "published_date": "2024-03-26 13:05:02 UTC",
    "updated_date": "2024-03-26 13:05:02 UTC"
  },
  {
    "arxiv_id": "2403.17674v1",
    "title": "Depending on yourself when you should: Mentoring LLM with RL agents to become the master in cybersecurity games",
    "authors": [
      "Yikuan Yan",
      "Yaolun Zhang",
      "Keman Huang"
    ],
    "abstract": "Integrating LLM and reinforcement learning (RL) agent effectively to achieve\ncomplementary performance is critical in high stake tasks like cybersecurity\noperations. In this study, we introduce SecurityBot, a LLM agent mentored by\npre-trained RL agents, to support cybersecurity operations. In particularly,\nthe LLM agent is supported with a profile module to generated behavior\nguidelines, a memory module to accumulate local experiences, a reflection\nmodule to re-evaluate choices, and an action module to reduce action space.\nAdditionally, it adopts the collaboration mechanism to take suggestions from\npre-trained RL agents, including a cursor for dynamic suggestion taken, an\naggregator for multiple mentors' suggestions ranking and a caller for proactive\nsuggestion asking. Building on the CybORG experiment framework, our experiences\nshow that SecurityBot demonstrates significant performance improvement compared\nwith LLM or RL standalone, achieving the complementary performance in the\ncybersecurity games.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CR",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2403.17674v1",
    "published_date": "2024-03-26 13:02:46 UTC",
    "updated_date": "2024-03-26 13:02:46 UTC"
  },
  {
    "arxiv_id": "2403.18872v1",
    "title": "Targeted Visualization of the Backbone of Encoder LLMs",
    "authors": [
      "Isaac Roberts",
      "Alexander Schulz",
      "Luca Hermes",
      "Barbara Hammer"
    ],
    "abstract": "Attention based Large Language Models (LLMs) are the state-of-the-art in\nnatural language processing (NLP). The two most common architectures are\nencoders such as BERT, and decoders like the GPT models. Despite the success of\nencoder models, on which we focus in this work, they also bear several risks,\nincluding issues with bias or their susceptibility for adversarial attacks,\nsignifying the necessity for explainable AI to detect such issues. While there\ndoes exist various local explainability methods focusing on the prediction of\nsingle inputs, global methods based on dimensionality reduction for\nclassification inspection, which have emerged in other domains and that go\nfurther than just using t-SNE in the embedding space, are not widely spread in\nNLP.\n  To reduce this gap, we investigate the application of DeepView, a method for\nvisualizing a part of the decision function together with a data set in two\ndimensions, to the NLP domain. While in previous work, DeepView has been used\nto inspect deep image classification models, we demonstrate how to apply it to\nBERT-based NLP classifiers and investigate its usability in this domain,\nincluding settings with adversarially perturbed input samples and pre-trained,\nfine-tuned, and multi-task models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.18872v1",
    "published_date": "2024-03-26 12:51:02 UTC",
    "updated_date": "2024-03-26 12:51:02 UTC"
  },
  {
    "arxiv_id": "2403.17661v2",
    "title": "Language Models for Text Classification: Is In-Context Learning Enough?",
    "authors": [
      "Aleksandra Edwards",
      "Jose Camacho-Collados"
    ],
    "abstract": "Recent foundational language models have shown state-of-the-art performance\nin many NLP tasks in zero- and few-shot settings. An advantage of these models\nover more standard approaches based on fine-tuning is the ability to understand\ninstructions written in natural language (prompts), which helps them generalise\nbetter to different tasks and domains without the need for specific training\ndata. This makes them suitable for addressing text classification problems for\ndomains with limited amounts of annotated instances. However, existing research\nis limited in scale and lacks understanding of how text generation models\ncombined with prompting techniques compare to more established methods for text\nclassification such as fine-tuning masked language models. In this paper, we\naddress this research gap by performing a large-scale evaluation study for 16\ntext classification datasets covering binary, multiclass, and multilabel\nproblems. In particular, we compare zero- and few-shot approaches of large\nlanguage models to fine-tuning smaller language models. We also analyse the\nresults by prompt, classification type, domain, and number of labels. In\ngeneral, the results show how fine-tuning smaller and more efficient language\nmodels can still outperform few-shot approaches of larger language models,\nwhich have room for improvement when it comes to text classification.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at LREC-COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.17661v2",
    "published_date": "2024-03-26 12:47:39 UTC",
    "updated_date": "2024-04-14 15:45:53 UTC"
  },
  {
    "arxiv_id": "2403.17993v3",
    "title": "Mixing Artificial and Natural Intelligence: From Statistical Mechanics to AI and Back to Turbulence",
    "authors": [
      "Michael Chertkov"
    ],
    "abstract": "The paper reflects on the future role of AI in scientific research, with a\nspecial focus on turbulence studies, and examines the evolution of AI,\nparticularly through Diffusion Models rooted in non-equilibrium statistical\nmechanics. It underscores the significant impact of AI on advancing reduced,\nLagrangian models of turbulence through innovative use of deep neural networks.\nAdditionally, the paper reviews various other AI applications in turbulence\nresearch and outlines potential challenges and opportunities in the concurrent\nadvancement of AI and statistical hydrodynamics. This discussion sets the stage\nfor a future where AI and turbulence research are intricately intertwined,\nleading to more profound insights and advancements in both fields.",
    "categories": [
      "cs.LG",
      "cond-mat.stat-mech",
      "cs.AI",
      "physics.flu-dyn"
    ],
    "primary_category": "cs.LG",
    "comment": "38 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.17993v3",
    "published_date": "2024-03-26 12:45:52 UTC",
    "updated_date": "2024-07-12 20:25:55 UTC"
  },
  {
    "arxiv_id": "2403.17656v1",
    "title": "SGHormer: An Energy-Saving Graph Transformer Driven by Spikes",
    "authors": [
      "Huizhe Zhang",
      "Jintang Li",
      "Liang Chen",
      "Zibin Zheng"
    ],
    "abstract": "Graph Transformers (GTs) with powerful representation learning ability make a\nhuge success in wide range of graph tasks. However, the costs behind\noutstanding performances of GTs are higher energy consumption and computational\noverhead. The complex structure and quadratic complexity during attention\ncalculation in vanilla transformer seriously hinder its scalability on the\nlarge-scale graph data. Though existing methods have made strides in\nsimplifying combinations among blocks or attention-learning paradigm to improve\nGTs' efficiency, a series of energy-saving solutions originated from\nbiologically plausible structures are rarely taken into consideration when\nconstructing GT framework. To this end, we propose a new spiking-based graph\ntransformer (SGHormer). It turns full-precision embeddings into sparse and\nbinarized spikes to reduce memory and computational costs. The spiking graph\nself-attention and spiking rectify blocks in SGHormer explicitly capture global\nstructure information and recover the expressive power of spiking embeddings,\nrespectively. In experiments, SGHormer achieves comparable performances to\nother full-precision GTs with extremely low computational energy consumption.\nThe results show that SGHomer makes a remarkable progress in the field of\nlow-energy GTs.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "9 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.17656v1",
    "published_date": "2024-03-26 12:39:02 UTC",
    "updated_date": "2024-03-26 12:39:02 UTC"
  },
  {
    "arxiv_id": "2403.17653v1",
    "title": "An Extension-based Approach for Computing and Verifying Preferences in Abstract Argumentation",
    "authors": [
      "Quratul-ain Mahesar",
      "Nir Oren",
      "Wamberto W. Vasconcelos"
    ],
    "abstract": "We present an extension-based approach for computing and verifying\npreferences in an abstract argumentation system. Although numerous\nargumentation semantics have been developed previously for identifying\nacceptable sets of arguments from an argumentation framework, there is a lack\nof justification behind their acceptability based on implicit argument\npreferences. Preference-based argumentation frameworks allow one to determine\nwhat arguments are justified given a set of preferences. Our research considers\nthe inverse of the standard reasoning problem, i.e., given an abstract\nargumentation framework and a set of justified arguments, we compute what the\npossible preferences over arguments are. Furthermore, there is a need to verify\n(i.e., assess) that the computed preferences would lead to the acceptable sets\nof arguments. This paper presents a novel approach and algorithm for\nexhaustively computing and enumerating all possible sets of preferences\n(restricted to three identified cases) for a conflict-free set of arguments in\nan abstract argumentation framework. We prove the soundness, completeness and\ntermination of the algorithm. The research establishes that preferences are\ndetermined using an extension-based approach after the evaluation phase\n(acceptability of arguments) rather than stated beforehand. In this work, we\nfocus our research study on grounded, preferred and stable semantics. We show\nthat the complexity of computing sets of preferences is exponential in the\nnumber of arguments, and thus, describe an approximate approach and algorithm\nto compute the preferences. Furthermore, we present novel algorithms for\nverifying (i.e., assessing) the computed preferences. We provide details of the\nimplementation of the algorithms (source code has been made available), various\nexperiments performed to evaluate the algorithms and the analysis of the\nresults.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17653v1",
    "published_date": "2024-03-26 12:36:11 UTC",
    "updated_date": "2024-03-26 12:36:11 UTC"
  },
  {
    "arxiv_id": "2403.17643v3",
    "title": "S+t-SNE -- Bringing Dimensionality Reduction to Data Streams",
    "authors": [
      "Pedro C. Vieira",
      "JoÃ£o P. Montrezol",
      "JoÃ£o T. Vieira",
      "JoÃ£o Gama"
    ],
    "abstract": "We present S+t-SNE, an adaptation of the t-SNE algorithm designed to handle\ninfinite data streams. The core idea behind S+t-SNE is to update the t-SNE\nembedding incrementally as new data arrives, ensuring scalability and\nadaptability to handle streaming scenarios. By selecting the most important\npoints at each step, the algorithm ensures scalability while keeping\ninformative visualisations. By employing a blind method for drift management,\nthe algorithm adjusts the embedding space, which facilitates the visualisation\nof evolving data dynamics. Our experimental evaluations demonstrate the\neffectiveness and efficiency of S+t-SNE, whilst highlighting its ability to\ncapture patterns in a streaming scenario. We hope our approach offers\nresearchers and practitioners a real-time tool for understanding and\ninterpreting high-dimensional data.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "This preprint has undergone peer review but does not have any\n  post-submission improvements or corrections. Full version after peer-review\n  and post-acceptance improvements was presented at IDA2024\n  (https://ida2024.blogs.dsv.su.se/)",
    "pdf_url": "http://arxiv.org/pdf/2403.17643v3",
    "published_date": "2024-03-26 12:23:34 UTC",
    "updated_date": "2025-01-21 15:52:55 UTC"
  },
  {
    "arxiv_id": "2403.17992v1",
    "title": "Interpretable cancer cell detection with phonon microscopy using multi-task conditional neural networks for inter-batch calibration",
    "authors": [
      "Yijie Zheng",
      "Rafael Fuentes-Dominguez",
      "Matt Clark",
      "George S. D. Gordon",
      "Fernando Perez-Cota"
    ],
    "abstract": "Advances in artificial intelligence (AI) show great potential in revealing\nunderlying information from phonon microscopy (high-frequency ultrasound) data\nto identify cancerous cells. However, this technology suffers from the 'batch\neffect' that comes from unavoidable technical variations between each\nexperiment, creating confounding variables that the AI model may inadvertently\nlearn. We therefore present a multi-task conditional neural network framework\nto simultaneously achieve inter-batch calibration, by removing confounding\nvariables, and accurate cell classification of time-resolved phonon-derived\nsignals. We validate our approach by training and validating on different\nexperimental batches, achieving a balanced precision of 89.22% and an average\ncross-validated precision of 89.07% for classifying background, healthy and\ncancerous regions. Classification can be performed in 0.5 seconds with only\nsimple prior batch information required for multiple batch corrections.\nFurther, we extend our model to reconstruct denoised signals, enabling physical\ninterpretation of salient features indicating disease state including sound\nvelocity, sound attenuation and cell-adhesion to substrate.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG",
      "eess.IV",
      "eess.SP"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17992v1",
    "published_date": "2024-03-26 12:20:10 UTC",
    "updated_date": "2024-03-26 12:20:10 UTC"
  },
  {
    "arxiv_id": "2403.17637v3",
    "title": "PeersimGym: An Environment for Solving the Task Offloading Problem with Reinforcement Learning",
    "authors": [
      "Frederico Metelo",
      "Stevo RackoviÄ",
      "Pedro Ãkos Costa",
      "ClÃ¡udia Soares"
    ],
    "abstract": "Task offloading, crucial for balancing computational loads across devices in\nnetworks such as the Internet of Things, poses significant optimization\nchallenges, including minimizing latency and energy usage under strict\ncommunication and storage constraints. While traditional optimization falls\nshort in scalability; and heuristic approaches lack in achieving optimal\noutcomes, Reinforcement Learning (RL) offers a promising avenue by enabling the\nlearning of optimal offloading strategies through iterative interactions.\nHowever, the efficacy of RL hinges on access to rich datasets and\ncustom-tailored, realistic training environments. To address this, we introduce\nPeersimGym, an open-source, customizable simulation environment tailored for\ndeveloping and optimizing task offloading strategies within computational\nnetworks. PeersimGym supports a wide range of network topologies and\ncomputational constraints and integrates a \\textit{PettingZoo}-based interface\nfor RL agent deployment in both solo and multi-agent setups. Furthermore, we\ndemonstrate the utility of the environment through experiments with Deep\nReinforcement Learning agents, showcasing the potential of RL-based approaches\nto significantly enhance offloading strategies in distributed computing\nsettings. PeersimGym thus bridges the gap between theoretical RL models and\ntheir practical applications, paving the way for advancements in efficient task\noffloading methodologies.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in the proceedings of the conference on Machine Learning\n  and Knowledge Discovery in Databases. Applied Data Science Track. ECML PKDD\n  2024. Lecture Notes in Computer Science(), vol 14949. Springer, Cham",
    "pdf_url": "http://arxiv.org/pdf/2403.17637v3",
    "published_date": "2024-03-26 12:12:44 UTC",
    "updated_date": "2024-10-08 10:56:03 UTC"
  },
  {
    "arxiv_id": "2403.17633v4",
    "title": "UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object Detection with Sparse LiDAR and Large Domain Gaps",
    "authors": [
      "Maciej K Wozniak",
      "Mattias Hansson",
      "Marko Thiel",
      "Patric Jensfelt"
    ],
    "abstract": "In this study, we address a gap in existing unsupervised domain adaptation\napproaches on LiDAR-based 3D object detection, which have predominantly\nconcentrated on adapting between established, high-density autonomous driving\ndatasets. We focus on sparser point clouds, capturing scenarios from different\nperspectives: not just from vehicles on the road but also from mobile robots on\nsidewalks, which encounter significantly different environmental conditions and\nsensor configurations. We introduce Unsupervised Adversarial Domain Adaptation\nfor 3D Object Detection (UADA3D). UADA3D does not depend on pre-trained source\nmodels or teacher-student architectures. Instead, it uses an adversarial\napproach to directly learn domain-invariant features. We demonstrate its\nefficacy in various adaptation scenarios, showing significant improvements in\nboth self-driving car and mobile robot domains. Our code is open-source and\nwill be available soon.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for IEEE RA-L 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.17633v4",
    "published_date": "2024-03-26 12:08:14 UTC",
    "updated_date": "2024-10-21 11:34:27 UTC"
  },
  {
    "arxiv_id": "2403.17632v3",
    "title": "Data-driven Energy Consumption Modelling for Electric Micromobility using an Open Dataset",
    "authors": [
      "Yue Ding",
      "Sen Yan",
      "Maqsood Hussain Shah",
      "Hongyuan Fang",
      "Ji Li",
      "Mingming Liu"
    ],
    "abstract": "The escalating challenges of traffic congestion and environmental degradation\nunderscore the critical importance of embracing E-Mobility solutions in urban\nspaces. In particular, micro E-Mobility tools such as E-scooters and E-bikes,\nplay a pivotal role in this transition, offering sustainable alternatives for\nurban commuters. However, the energy consumption patterns for these tools are a\ncritical aspect that impacts their effectiveness in real-world scenarios and is\nessential for trip planning and boosting user confidence in using these. To\nthis effect, recent studies have utilised physical models customised for\nspecific mobility tools and conditions, but these models struggle with\ngeneralization and effectiveness in real-world scenarios due to a notable\nabsence of open datasets for thorough model evaluation and verification. To\nfill this gap, our work presents an open dataset, collected in Dublin, Ireland,\nspecifically designed for energy modelling research related to E-Scooters and\nE-Bikes. Furthermore, we provide a comprehensive analysis of energy consumption\nmodelling based on the dataset using a set of representative machine learning\nalgorithms and compare their performance against the contemporary mathematical\nmodels as a baseline. Our results demonstrate a notable advantage for\ndata-driven models in comparison to the corresponding mathematical models for\nestimating energy consumption. Specifically, data-driven models outperform\nphysical models in accuracy by up to 83.83% for E-Bikes and 82.16% for\nE-Scooters based on an in-depth analysis of the dataset under certain\nassumptions.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, 5 figures, 4 tables. This manuscript has been accepted by\n  the IEEE ITEC 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.17632v3",
    "published_date": "2024-03-26 12:08:05 UTC",
    "updated_date": "2024-11-08 17:01:49 UTC"
  },
  {
    "arxiv_id": "2403.17611v1",
    "title": "Denoising Table-Text Retrieval for Open-Domain Question Answering",
    "authors": [
      "Deokhyung Kang",
      "Baikjin Jung",
      "Yunsu Kim",
      "Gary Geunbae Lee"
    ],
    "abstract": "In table-text open-domain question answering, a retriever system retrieves\nrelevant evidence from tables and text to answer questions. Previous studies in\ntable-text open-domain question answering have two common challenges: firstly,\ntheir retrievers can be affected by false-positive labels in training datasets;\nsecondly, they may struggle to provide appropriate evidence for questions that\nrequire reasoning across the table. To address these issues, we propose\nDenoised Table-Text Retriever (DoTTeR). Our approach involves utilizing a\ndenoised training dataset with fewer false positive labels by discarding\ninstances with lower question-relevance scores measured through a false\npositive detection model. Subsequently, we integrate table-level ranking\ninformation into the retriever to assist in finding evidence for questions that\ndemand reasoning across the table. To encode this ranking information, we\nfine-tune a rank-aware column encoder to identify minimum and maximum values\nwithin a column. Experimental results demonstrate that DoTTeR significantly\noutperforms strong baselines on both retrieval recall and downstream QA tasks.\nOur code is available at https://github.com/deokhk/DoTTeR.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to LREC-COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.17611v1",
    "published_date": "2024-03-26 11:44:49 UTC",
    "updated_date": "2024-03-26 11:44:49 UTC"
  },
  {
    "arxiv_id": "2403.18871v1",
    "title": "Clinical Domain Knowledge-Derived Template Improves Post Hoc AI Explanations in Pneumothorax Classification",
    "authors": [
      "Han Yuan",
      "Chuan Hong",
      "Pengtao Jiang",
      "Gangming Zhao",
      "Nguyen Tuan Anh Tran",
      "Xinxing Xu",
      "Yet Yen Yan",
      "Nan Liu"
    ],
    "abstract": "Background: Pneumothorax is an acute thoracic disease caused by abnormal air\ncollection between the lungs and chest wall. To address the opaqueness often\nassociated with deep learning (DL) models, explainable artificial intelligence\n(XAI) methods have been introduced to outline regions related to pneumothorax\ndiagnoses made by DL models. However, these explanations sometimes diverge from\nactual lesion areas, highlighting the need for further improvement. Method: We\npropose a template-guided approach to incorporate the clinical knowledge of\npneumothorax into model explanations generated by XAI methods, thereby\nenhancing the quality of these explanations. Utilizing one lesion delineation\ncreated by radiologists, our approach first generates a template that\nrepresents potential areas of pneumothorax occurrence. This template is then\nsuperimposed on model explanations to filter out extraneous explanations that\nfall outside the template's boundaries. To validate its efficacy, we carried\nout a comparative analysis of three XAI methods with and without our template\nguidance when explaining two DL models in two real-world datasets. Results: The\nproposed approach consistently improved baseline XAI methods across twelve\nbenchmark scenarios built on three XAI methods, two DL models, and two\ndatasets. The average incremental percentages, calculated by the performance\nimprovements over the baseline performance, were 97.8% in Intersection over\nUnion (IoU) and 94.1% in Dice Similarity Coefficient (DSC) when comparing model\nexplanations and ground-truth lesion areas. Conclusions: In the context of\npneumothorax diagnoses, we proposed a template-guided approach for improving AI\nexplanations. We anticipate that our template guidance will forge a fresh\napproach to elucidating AI models by integrating clinical domain expertise.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.18871v1",
    "published_date": "2024-03-26 11:40:06 UTC",
    "updated_date": "2024-03-26 11:40:06 UTC"
  },
  {
    "arxiv_id": "2403.17608v2",
    "title": "Fake or JPEG? Revealing Common Biases in Generated Image Detection Datasets",
    "authors": [
      "Patrick Grommelt",
      "Louis Weiss",
      "Franz-Josef Pfreundt",
      "Janis Keuper"
    ],
    "abstract": "The widespread adoption of generative image models has highlighted the urgent\nneed to detect artificial content, which is a crucial step in combating\nwidespread manipulation and misinformation. Consequently, numerous detectors\nand associated datasets have emerged. However, many of these datasets\ninadvertently introduce undesirable biases, thereby impacting the effectiveness\nand evaluation of detectors. In this paper, we emphasize that many datasets for\nAI-generated image detection contain biases related to JPEG compression and\nimage size. Using the GenImage dataset, we demonstrate that detectors indeed\nlearn from these undesired factors. Furthermore, we show that removing the\nnamed biases substantially increases robustness to JPEG compression and\nsignificantly alters the cross-generator performance of evaluated detectors.\nSpecifically, it leads to more than 11 percentage points increase in\ncross-generator performance for ResNet50 and Swin-T detectors on the GenImage\ndataset, achieving state-of-the-art results.\n  We provide the dataset and source codes of this paper on the anonymous\nwebsite: https://www.unbiased-genimage.org",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17608v2",
    "published_date": "2024-03-26 11:39:00 UTC",
    "updated_date": "2024-03-28 15:24:16 UTC"
  },
  {
    "arxiv_id": "2403.17607v1",
    "title": "Fully-fused Multi-Layer Perceptrons on Intel Data Center GPUs",
    "authors": [
      "Kai Yuan",
      "Christoph Bauinger",
      "Xiangyi Zhang",
      "Pascal Baehr",
      "Matthias Kirchhart",
      "Darius Dabert",
      "Adrien Tousnakhoff",
      "Pierre Boudier",
      "Michael Paulitsch"
    ],
    "abstract": "This paper presents a SYCL implementation of Multi-Layer Perceptrons (MLPs),\nwhich targets and is optimized for the Intel Data Center GPU Max 1550. To\nincrease the performance, our implementation minimizes the slow global memory\naccesses by maximizing the data reuse within the general register file and the\nshared local memory by fusing the operations in each layer of the MLP. We show\nwith a simple roofline model that this results in a significant increase in the\narithmetic intensity, leading to improved performance, especially for\ninference. We compare our approach to a similar CUDA implementation for MLPs\nand show that our implementation on the Intel Data Center GPU outperforms the\nCUDA implementation on Nvidia's H100 GPU by a factor up to 2.84 in inference\nand 1.75 in training. The paper also showcases the efficiency of our SYCL\nimplementation in three significant areas: Image Compression, Neural Radiance\nFields, and Physics-Informed Machine Learning. In all cases, our implementation\noutperforms the off-the-shelf Intel Extension for PyTorch (IPEX) implementation\non the same Intel GPU by up to a factor of 30 and the CUDA PyTorch version on\nNvidia's H100 GPU by up to a factor 19. The code can be found at\nhttps://github.com/intel/tiny-dpcpp-nn.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17607v1",
    "published_date": "2024-03-26 11:38:39 UTC",
    "updated_date": "2024-03-26 11:38:39 UTC"
  },
  {
    "arxiv_id": "2403.18870v3",
    "title": "SugarcaneNet: An Optimized Ensemble of LASSO-Regularized Pre-trained Models for Accurate Disease Classification",
    "authors": [
      "Md. Simul Hasan Talukder",
      "Sharmin Akter",
      "Abdullah Hafez Nur",
      "Mohammad Aljaidi",
      "Rejwan Bin Sulaiman",
      "Ali Fayez Alkoradees"
    ],
    "abstract": "Sugarcane, a key crop for the world's sugar industry, is prone to several\ndiseases that have a substantial negative influence on both its yield and\nquality. To effectively manage and implement preventative initiatives, diseases\nmust be detected promptly and accurately. In this study, we present a unique\nmodel called sugarcaneNet2024 that outperforms previous methods for\nautomatically and quickly detecting sugarcane disease through leaf image\nprocessing. Our proposed model consolidates an optimized weighted average\nensemble of seven customized and LASSO-regularized pre-trained models,\nparticularly InceptionV3, InceptionResNetV2, DenseNet201, DenseNet169,\nXception, and ResNet152V2. Initially, we added three more dense layers with\n0.0001 LASSO regularization, three 30% dropout layers, and three batch\nnormalizations with renorm enabled at the bottom of these pre-trained models to\nimprove the performance. The accuracy of sugarcane leaf disease classification\nwas greatly increased by this addition. Following this, several comparative\nstudies between the average ensemble and individual models were carried out,\nindicating that the ensemble technique performed better. The average ensemble\nof all modified pre-trained models produced outstanding outcomes: 100%, 99%,\n99%, and 99.45% for f1 score, precision, recall, and accuracy, respectively.\nPerformance was further enhanced by the implementation of an optimized weighted\naverage ensemble technique incorporated with grid search. This optimized\nsugarcaneNet2024 model performed the best for detecting sugarcane diseases,\nhaving achieved accuracy, precision, recall, and F1 score of 99.67%, 100%,\n100%, and 100% , respectively.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "32 pages, 11 Figures, 13 Tables",
    "pdf_url": "http://arxiv.org/pdf/2403.18870v3",
    "published_date": "2024-03-26 11:23:08 UTC",
    "updated_date": "2024-11-16 11:36:01 UTC"
  },
  {
    "arxiv_id": "2403.17601v3",
    "title": "LASIL: Learner-Aware Supervised Imitation Learning For Long-term Microscopic Traffic Simulation",
    "authors": [
      "Ke Guo",
      "Zhenwei Miao",
      "Wei Jing",
      "Weiwei Liu",
      "Weizi Li",
      "Dayang Hao",
      "Jia Pan"
    ],
    "abstract": "Microscopic traffic simulation plays a crucial role in transportation\nengineering by providing insights into individual vehicle behavior and overall\ntraffic flow. However, creating a realistic simulator that accurately\nreplicates human driving behaviors in various traffic conditions presents\nsignificant challenges. Traditional simulators relying on heuristic models\noften fail to deliver accurate simulations due to the complexity of real-world\ntraffic environments. Due to the covariate shift issue, existing imitation\nlearning-based simulators often fail to generate stable long-term simulations.\nIn this paper, we propose a novel approach called learner-aware supervised\nimitation learning to address the covariate shift problem in multi-agent\nimitation learning. By leveraging a variational autoencoder simultaneously\nmodeling the expert and learner state distribution, our approach augments\nexpert states such that the augmented state is aware of learner state\ndistribution. Our method, applied to urban traffic simulation, demonstrates\nsignificant improvements over existing state-of-the-art baselines in both\nshort-term microscopic and long-term macroscopic realism when evaluated on the\nreal-world dataset pNEUMA.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by CVPR 2024. arXiv admin note: text overlap with\n  arXiv:2306.06401",
    "pdf_url": "http://arxiv.org/pdf/2403.17601v3",
    "published_date": "2024-03-26 11:13:35 UTC",
    "updated_date": "2024-05-23 07:41:01 UTC"
  },
  {
    "arxiv_id": "2403.17589v1",
    "title": "Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models",
    "authors": [
      "Yabin Zhang",
      "Wenjie Zhu",
      "Hui Tang",
      "Zhiyuan Ma",
      "Kaiyang Zhou",
      "Lei Zhang"
    ],
    "abstract": "With the emergence of pre-trained vision-language models like CLIP, how to\nadapt them to various downstream classification tasks has garnered significant\nattention in recent research. The adaptation strategies can be typically\ncategorized into three paradigms: zero-shot adaptation, few-shot adaptation,\nand the recently-proposed training-free few-shot adaptation. Most existing\napproaches are tailored for a specific setting and can only cater to one or two\nof these paradigms. In this paper, we introduce a versatile adaptation approach\nthat can effectively work under all three settings. Specifically, we propose\nthe dual memory networks that comprise dynamic and static memory components.\nThe static memory caches training data knowledge, enabling training-free\nfew-shot adaptation, while the dynamic memory preserves historical test\nfeatures online during the testing process, allowing for the exploration of\nadditional data insights beyond the training set. This novel capability\nenhances model performance in the few-shot setting and enables model usability\nin the absence of training data. The two memory networks employ the same\nflexible memory interactive strategy, which can operate in a training-free mode\nand can be further enhanced by incorporating learnable projection layers. Our\napproach is tested across 11 datasets under the three task settings.\nRemarkably, in the zero-shot scenario, it outperforms existing methods by over\n3\\% and even shows superior results against methods utilizing external training\ndata. Additionally, our method exhibits robust performance against natural\ndistribution shifts. Codes are available at \\url{https://github.com/YBZh/DMN}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR2024; Codes are available at \\url{https://github.com/YBZh/DMN}",
    "pdf_url": "http://arxiv.org/pdf/2403.17589v1",
    "published_date": "2024-03-26 10:54:07 UTC",
    "updated_date": "2024-03-26 10:54:07 UTC"
  },
  {
    "arxiv_id": "2403.17587v1",
    "title": "Parameterized Analysis of Bribery in Challenge the Champ Tournaments",
    "authors": [
      "Juhi Chaudhary",
      "Hendrik Molter",
      "Meirav Zehavi"
    ],
    "abstract": "Challenge the champ tournaments are one of the simplest forms of competition,\nwhere a (initially selected) champ is repeatedly challenged by other players.\nIf a player beats the champ, then that player is considered the new (current)\nchamp. Each player in the competition challenges the current champ once in a\nfixed order. The champ of the last round is considered the winner of the\ntournament. We investigate a setting where players can be bribed to lower their\nwinning probability against the initial champ. The goal is to maximize the\nprobability of the initial champ winning the tournament by bribing the other\nplayers, while not exceeding a given budget for the bribes. Mattei et al.\n[Journal of Applied Logic, 2015] showed that the problem can be solved in\npseudo-polynomial time, and that it is in XP when parameterized by the number\nof players.\n  We show that the problem is weakly NP-hard and W[1]-hard when parameterized\nby the number of players. On the algorithmic side, we show that the problem is\nfixed-parameter tractable when parameterized either by the number of different\nbribe values or the number of different probability values. To this end, we\nestablish several results that are of independent interest. In particular, we\nshow that the product knapsack problem is W[1]-hard when parameterized by the\nnumber of items in the knapsack, and that constructive bribery for cup\ntournaments is W[1]-hard when parameterized by the number of players.\nFurthermore, we present a novel way of designing mixed integer linear programs,\nensuring optimal solutions where all variables are integers.",
    "categories": [
      "cs.DS",
      "cs.AI"
    ],
    "primary_category": "cs.DS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17587v1",
    "published_date": "2024-03-26 10:53:25 UTC",
    "updated_date": "2024-03-26 10:53:25 UTC"
  },
  {
    "arxiv_id": "2403.17582v1",
    "title": "Towards a Zero-Data, Controllable, Adaptive Dialog System",
    "authors": [
      "Dirk VÃ¤th",
      "Lindsey Vanderlyn",
      "Ngoc Thang Vu"
    ],
    "abstract": "Conversational Tree Search (V\\\"ath et al., 2023) is a recent approach to\ncontrollable dialog systems, where domain experts shape the behavior of a\nReinforcement Learning agent through a dialog tree. The agent learns to\nefficiently navigate this tree, while adapting to information needs, e.g.,\ndomain familiarity, of different users. However, the need for additional\ntraining data hinders deployment in new domains. To address this, we explore\napproaches to generate this data directly from dialog trees. We improve the\noriginal approach, and show that agents trained on synthetic data can achieve\ncomparable dialog success to models trained on human data, both when using a\ncommercial Large Language Model for generation, or when using a smaller\nopen-source model, running on a single GPU. We further demonstrate the\nscalability of our approach by collecting and testing on two new datasets:\nONBOARD, a new domain helping foreign residents moving to a new city, and the\nmedical domain DIAGNOSE, a subset of Wikipedia articles related to scalp and\nhead symptoms. Finally, we perform human testing, where no statistically\nsignificant differences were found in either objective or subjective measures\nbetween models trained on human and generated data.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17582v1",
    "published_date": "2024-03-26 10:45:11 UTC",
    "updated_date": "2024-03-26 10:45:11 UTC"
  },
  {
    "arxiv_id": "2403.17556v1",
    "title": "m3P: Towards Multimodal Multilingual Translation with Multimodal Prompt",
    "authors": [
      "Jian Yang",
      "Hongcheng Guo",
      "Yuwei Yin",
      "Jiaqi Bai",
      "Bing Wang",
      "Jiaheng Liu",
      "Xinnian Liang",
      "Linzheng Cahi",
      "Liqun Yang",
      "Zhoujun Li"
    ],
    "abstract": "Multilingual translation supports multiple translation directions by\nprojecting all languages in a shared space, but the translation quality is\nundermined by the difference between languages in the text-only modality,\nespecially when the number of languages is large. To bridge this gap, we\nintroduce visual context as the universal language-independent representation\nto facilitate multilingual translation. In this paper, we propose a framework\nto leverage the multimodal prompt to guide the Multimodal Multilingual neural\nMachine Translation (m3P), which aligns the representations of different\nlanguages with the same meaning and generates the conditional vision-language\nmemory for translation. We construct a multilingual multimodal instruction\ndataset (InstrMulti102) to support 102 languages. Our method aims to minimize\nthe representation distance of different languages by regarding the image as a\ncentral language. Experimental results show that m3P outperforms previous\ntext-only baselines and multilingual multimodal methods by a large margin.\nFurthermore, the probing experiments validate the effectiveness of our method\nin enhancing translation under the low-resource and massively multilingual\nscenario.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.17556v1",
    "published_date": "2024-03-26 10:04:24 UTC",
    "updated_date": "2024-03-26 10:04:24 UTC"
  },
  {
    "arxiv_id": "2403.17549v1",
    "title": "Practical Applications of Advanced Cloud Services and Generative AI Systems in Medical Image Analysis",
    "authors": [
      "Jingyu Xu",
      "Binbin Wu",
      "Jiaxin Huang",
      "Yulu Gong",
      "Yifan Zhang",
      "Bo Liu"
    ],
    "abstract": "The medical field is one of the important fields in the application of\nartificial intelligence technology. With the explosive growth and\ndiversification of medical data, as well as the continuous improvement of\nmedical needs and challenges, artificial intelligence technology is playing an\nincreasingly important role in the medical field. Artificial intelligence\ntechnologies represented by computer vision, natural language processing, and\nmachine learning have been widely penetrated into diverse scenarios such as\nmedical imaging, health management, medical information, and drug research and\ndevelopment, and have become an important driving force for improving the level\nand quality of medical services.The article explores the transformative\npotential of generative AI in medical imaging, emphasizing its ability to\ngenerate syntheticACM-2 data, enhance images, aid in anomaly detection, and\nfacilitate image-to-image translation. Despite challenges like model\ncomplexity, the applications of generative models in healthcare, including\nMed-PaLM 2 technology, show promising results. By addressing limitations in\ndataset size and diversity, these models contribute to more accurate diagnoses\nand improved patient outcomes. However, ethical considerations and\ncollaboration among stakeholders are essential for responsible implementation.\nThrough experiments leveraging GANs to augment brain tumor MRI datasets, the\nstudy demonstrates how generative AI can enhance image quality and diversity,\nultimately advancing medical diagnostics and patient care.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17549v1",
    "published_date": "2024-03-26 09:55:49 UTC",
    "updated_date": "2024-03-26 09:55:49 UTC"
  },
  {
    "arxiv_id": "2403.17542v1",
    "title": "VDSC: Enhancing Exploration Timing with Value Discrepancy and State Counts",
    "authors": [
      "Marius Captari",
      "Remo Sasso",
      "Matthia Sabatelli"
    ],
    "abstract": "Despite the considerable attention given to the questions of \\textit{how\nmuch} and \\textit{how to} explore in deep reinforcement learning, the\ninvestigation into \\textit{when} to explore remains relatively less researched.\nWhile more sophisticated exploration strategies can excel in specific, often\nsparse reward environments, existing simpler approaches, such as\n$\\epsilon$-greedy, persist in outperforming them across a broader spectrum of\ndomains. The appeal of these simpler strategies lies in their ease of\nimplementation and generality across a wide range of domains. The downside is\nthat these methods are essentially a blind switching mechanism, which\ncompletely disregards the agent's internal state. In this paper, we propose to\nleverage the agent's internal state to decide \\textit{when} to explore,\naddressing the shortcomings of blind switching mechanisms. We present Value\nDiscrepancy and State Counts through homeostasis (VDSC), a novel approach for\nefficient exploration timing. Experimental results on the Atari suite\ndemonstrate the superiority of our strategy over traditional methods such as\n$\\epsilon$-greedy and Boltzmann, as well as more sophisticated techniques like\nNoisy Nets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17542v1",
    "published_date": "2024-03-26 09:44:57 UTC",
    "updated_date": "2024-03-26 09:44:57 UTC"
  },
  {
    "arxiv_id": "2403.17532v1",
    "title": "KC-GenRe: A Knowledge-constrained Generative Re-ranking Method Based on Large Language Models for Knowledge Graph Completion",
    "authors": [
      "Yilin Wang",
      "Minghao Hu",
      "Zhen Huang",
      "Dongsheng Li",
      "Dong Yang",
      "Xicheng Lu"
    ],
    "abstract": "The goal of knowledge graph completion (KGC) is to predict missing facts\namong entities. Previous methods for KGC re-ranking are mostly built on\nnon-generative language models to obtain the probability of each candidate.\nRecently, generative large language models (LLMs) have shown outstanding\nperformance on several tasks such as information extraction and dialog systems.\nLeveraging them for KGC re-ranking is beneficial for leveraging the extensive\npre-trained knowledge and powerful generative capabilities. However, it may\nencounter new problems when accomplishing the task, namely mismatch,\nmisordering and omission. To this end, we introduce KC-GenRe, a\nknowledge-constrained generative re-ranking method based on LLMs for KGC. To\novercome the mismatch issue, we formulate the KGC re-ranking task as a\ncandidate identifier sorting generation problem implemented by generative LLMs.\nTo tackle the misordering issue, we develop a knowledge-guided interactive\ntraining method that enhances the identification and ranking of candidates. To\naddress the omission issue, we design a knowledge-augmented constrained\ninference method that enables contextual prompting and controlled generation,\nso as to obtain valid rankings. Experimental results show that KG-GenRe\nachieves state-of-the-art performance on four datasets, with gains of up to\n6.7% and 7.7% in the MRR and Hits@1 metric compared to previous methods, and\n9.0% and 11.1% compared to that without re-ranking. Extensive analysis\ndemonstrates the effectiveness of components in KG-GenRe.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted for publication in the proceedings of\n  LREC-COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.17532v1",
    "published_date": "2024-03-26 09:36:59 UTC",
    "updated_date": "2024-03-26 09:36:59 UTC"
  },
  {
    "arxiv_id": "2403.17530v1",
    "title": "Boosting Few-Shot Learning with Disentangled Self-Supervised Learning and Meta-Learning for Medical Image Classification",
    "authors": [
      "Eva Pachetti",
      "Sotirios A. Tsaftaris",
      "Sara Colantonio"
    ],
    "abstract": "Background and objective: Employing deep learning models in critical domains\nsuch as medical imaging poses challenges associated with the limited\navailability of training data. We present a strategy for improving the\nperformance and generalization capabilities of models trained in low-data\nregimes. Methods: The proposed method starts with a pre-training phase, where\nfeatures learned in a self-supervised learning setting are disentangled to\nimprove the robustness of the representations for downstream tasks. We then\nintroduce a meta-fine-tuning step, leveraging related classes between\nmeta-training and meta-testing phases but varying the granularity level. This\napproach aims to enhance the model's generalization capabilities by exposing it\nto more challenging classification tasks during meta-training and evaluating it\non easier tasks but holding greater clinical relevance during meta-testing. We\ndemonstrate the effectiveness of the proposed approach through a series of\nexperiments exploring several backbones, as well as diverse pre-training and\nfine-tuning schemes, on two distinct medical tasks, i.e., classification of\nprostate cancer aggressiveness from MRI data and classification of breast\ncancer malignity from microscopic images. Results: Our results indicate that\nthe proposed approach consistently yields superior performance w.r.t. ablation\nexperiments, maintaining competitiveness even when a distribution shift between\ntraining and evaluation data occurs. Conclusion: Extensive experiments\ndemonstrate the effectiveness and wide applicability of the proposed approach.\nWe hope that this work will add another solution to the arsenal of addressing\nlearning issues in data-scarce imaging domains.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2; I.4; I.5; J.3"
    ],
    "primary_category": "cs.CV",
    "comment": "20 pages, 4 figures, 4 tables. Submitted to Elsevier on 25 March 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.17530v1",
    "published_date": "2024-03-26 09:36:20 UTC",
    "updated_date": "2024-03-26 09:36:20 UTC"
  },
  {
    "arxiv_id": "2403.17525v2",
    "title": "Equipping Sketch Patches with Context-Aware Positional Encoding for Graphic Sketch Representation",
    "authors": [
      "Sicong Zang",
      "Zhijun Fang"
    ],
    "abstract": "When benefiting graphic sketch representation with sketch drawing orders,\nrecent studies have linked sketch patches as graph edges by drawing orders in\naccordance to a temporal-based nearest neighboring strategy. However, such\nconstructed graph edges may be unreliable, since the contextual relationships\nbetween patches may be inconsistent with the sequential positions in drawing\norders, due to variants of sketch drawings. In this paper, we propose a\nvariant-drawing-protected method by equipping sketch patches with context-aware\npositional encoding (PE) to make better use of drawing orders for sketch\nlearning. We introduce a sinusoidal absolute PE to embed the sequential\npositions in drawing orders, and a learnable relative PE to encode the unseen\ncontextual relationships between patches. Both types of PEs never attend the\nconstruction of graph edges, but are injected into graph nodes to cooperate\nwith the visual patterns captured from patches. After linking nodes by semantic\nproximity, during message aggregation via graph convolutional networks, each\nnode receives both semantic features from patches and contextual information\nfrom PEs from its neighbors, which equips local patch patterns with global\ncontextual information, further obtaining drawing-order-enhanced sketch\nrepresentations. Experimental results indicate that our method significantly\nimproves sketch healing and controllable sketch synthesis. The source codes\ncould be found at https://github.com/SCZang/DC-gra2seq.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17525v2",
    "published_date": "2024-03-26 09:26:12 UTC",
    "updated_date": "2025-05-13 07:25:01 UTC"
  },
  {
    "arxiv_id": "2403.17516v2",
    "title": "MapGuide: A Simple yet Effective Method to Reconstruct Continuous Language from Brain Activities",
    "authors": [
      "Xinpei Zhao",
      "Jingyuan Sun",
      "Shaonan Wang",
      "Jing Ye",
      "Xiaohan Zhang",
      "Chengqing Zong"
    ],
    "abstract": "Decoding continuous language from brain activity is a formidable yet\npromising field of research. It is particularly significant for aiding people\nwith speech disabilities to communicate through brain signals. This field\naddresses the complex task of mapping brain signals to text. The previous best\nattempt reverse-engineered this process in an indirect way: it began by\nlearning to encode brain activity from text and then guided text generation by\naligning with predicted brain responses. In contrast, we propose a simple yet\neffective method that guides text reconstruction by directly comparing them\nwith the predicted text embeddings mapped from brain activities. Comprehensive\nexperiments reveal that our method significantly outperforms the current\nstate-of-the-art model, showing average improvements of 77% and 54% on BLEU and\nMETEOR scores. We further validate the proposed modules through detailed\nablation studies and case analyses and highlight a critical correlation: the\nmore precisely we map brain activities to text embeddings, the better the text\nreconstruction results. Such insight can simplify the task of reconstructing\nlanguage from brain activities for future work, emphasizing the importance of\nimproving brain-to-text-embedding mapping techniques.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2024 main conference",
    "pdf_url": "http://arxiv.org/pdf/2403.17516v2",
    "published_date": "2024-03-26 09:18:59 UTC",
    "updated_date": "2024-04-02 12:05:41 UTC"
  },
  {
    "arxiv_id": "2403.17515v1",
    "title": "Prediction-sharing During Training and Inference",
    "authors": [
      "Yotam Gafni",
      "Ronen Gradwohl",
      "Moshe Tennenholtz"
    ],
    "abstract": "Two firms are engaged in a competitive prediction task. Each firm has two\nsources of data -- labeled historical data and unlabeled inference-time data --\nand uses the former to derive a prediction model, and the latter to make\npredictions on new instances. We study data-sharing contracts between the\nfirms. The novelty of our study is to introduce and highlight the differences\nbetween contracts that share prediction models only, contracts to share\ninference-time predictions only, and contracts to share both. Our analysis\nproceeds on three levels. First, we develop a general Bayesian framework that\nfacilitates our study. Second, we narrow our focus to two natural settings\nwithin this framework: (i) a setting in which the accuracy of each firm's\nprediction model is common knowledge, but the correlation between the\nrespective models is unknown; and (ii) a setting in which two hypotheses exist\nregarding the optimal predictor, and one of the firms has a structural\nadvantage in deducing it. Within these two settings we study optimal contract\nchoice. More specifically, we find the individually rational and Pareto-optimal\ncontracts for some notable cases, and describe specific settings where each of\nthe different sharing contracts emerge as optimal. Finally, in the third level\nof our analysis we demonstrate the applicability of our concepts in a synthetic\nsimulation using real loan data.",
    "categories": [
      "econ.TH",
      "cs.AI",
      "cs.GT",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "econ.TH",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17515v1",
    "published_date": "2024-03-26 09:18:50 UTC",
    "updated_date": "2024-03-26 09:18:50 UTC"
  },
  {
    "arxiv_id": "2403.17479v1",
    "title": "Natural Language Requirements Testability Measurement Based on Requirement Smells",
    "authors": [
      "Morteza Zakeri-Nasrabadi",
      "Saeed Parsa"
    ],
    "abstract": "Requirements form the basis for defining software systems' obligations and\ntasks. Testable requirements help prevent failures, reduce maintenance costs,\nand make it easier to perform acceptance tests. However, despite the importance\nof measuring and quantifying requirements testability, no automatic approach\nfor measuring requirements testability has been proposed based on the\nrequirements smells, which are at odds with the requirements testability. This\npaper presents a mathematical model to evaluate and rank the natural language\nrequirements testability based on an extensive set of nine requirements smells,\ndetected automatically, and acceptance test efforts determined by requirement\nlength and its application domain. Most of the smells stem from uncountable\nadjectives, context-sensitive, and ambiguous words. A comprehensive dictionary\nis required to detect such words. We offer a neural word-embedding technique to\ngenerate such a dictionary automatically. Using the dictionary, we could\nautomatically detect Polysemy smell (domain-specific ambiguity) for the first\ntime in 10 application domains. Our empirical study on nearly 1000 software\nrequirements from six well-known industrial and academic projects demonstrates\nthat the proposed smell detection approach outperforms Smella, a\nstate-of-the-art tool, in detecting requirements smells. The precision and\nrecall of smell detection are improved with an average of 0.03 and 0.33,\nrespectively, compared to the state-of-the-art. The proposed requirement\ntestability model measures the testability of 985 requirements with a mean\nabsolute error of 0.12 and a mean squared error of 0.03, demonstrating the\nmodel's potential for practical use.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "45 pages, 16 figures, and 13 tables; submitted as a journal paper",
    "pdf_url": "http://arxiv.org/pdf/2403.17479v1",
    "published_date": "2024-03-26 08:19:29 UTC",
    "updated_date": "2024-03-26 08:19:29 UTC"
  },
  {
    "arxiv_id": "2403.17467v1",
    "title": "A Unified Kernel for Neural Network Learning",
    "authors": [
      "Shao-Qun Zhang",
      "Zong-Yi Chen",
      "Yong-Ming Tian",
      "Xun Lu"
    ],
    "abstract": "Past decades have witnessed a great interest in the distinction and\nconnection between neural network learning and kernel learning. Recent\nadvancements have made theoretical progress in connecting infinite-wide neural\nnetworks and Gaussian processes. Two predominant approaches have emerged: the\nNeural Network Gaussian Process (NNGP) and the Neural Tangent Kernel (NTK). The\nformer, rooted in Bayesian inference, represents a zero-order kernel, while the\nlatter, grounded in the tangent space of gradient descents, is a first-order\nkernel. In this paper, we present the Unified Neural Kernel (UNK), which\ncharacterizes the learning dynamics of neural networks with gradient descents\nand parameter initialization. The proposed UNK kernel maintains the limiting\nproperties of both NNGP and NTK, exhibiting behaviors akin to NTK with a finite\nlearning step and converging to NNGP as the learning step approaches infinity.\nBesides, we also theoretically characterize the uniform tightness and learning\nconvergence of the UNK kernel, providing comprehensive insights into this\nunified kernel. Experimental results underscore the effectiveness of our\nproposed method.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17467v1",
    "published_date": "2024-03-26 07:55:45 UTC",
    "updated_date": "2024-03-26 07:55:45 UTC"
  },
  {
    "arxiv_id": "2403.17465v4",
    "title": "LaRE$^2$: Latent Reconstruction Error Based Method for Diffusion-Generated Image Detection",
    "authors": [
      "Yunpeng Luo",
      "Junlong Du",
      "Ke Yan",
      "Shouhong Ding"
    ],
    "abstract": "The evolution of Diffusion Models has dramatically improved image generation\nquality, making it increasingly difficult to differentiate between real and\ngenerated images. This development, while impressive, also raises significant\nprivacy and security concerns. In response to this, we propose a novel Latent\nREconstruction error guided feature REfinement method (LaRE^2) for detecting\nthe diffusion-generated images. We come up with the Latent Reconstruction Error\n(LaRE), the first reconstruction-error based feature in the latent space for\ngenerated image detection. LaRE surpasses existing methods in terms of feature\nextraction efficiency while preserving crucial cues required to differentiate\nbetween the real and the fake. To exploit LaRE, we propose an Error-Guided\nfeature REfinement module (EGRE), which can refine the image feature guided by\nLaRE to enhance the discriminativeness of the feature. Our EGRE utilizes an\nalign-then-refine mechanism, which effectively refines the image feature for\ngenerated-image detection from both spatial and channel perspectives. Extensive\nexperiments on the large-scale GenImage benchmark demonstrate the superiority\nof our LaRE^2, which surpasses the best SoTA method by up to 11.9%/12.1%\naverage ACC/AP across 8 different image generators. LaRE also surpasses\nexisting methods in terms of feature extraction cost, delivering an impressive\nspeed enhancement of 8 times. Code is available.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024. Code is available at https://github.com/luo3300612/LaRE",
    "pdf_url": "http://arxiv.org/pdf/2403.17465v4",
    "published_date": "2024-03-26 07:55:16 UTC",
    "updated_date": "2025-02-21 12:51:57 UTC"
  },
  {
    "arxiv_id": "2403.17456v3",
    "title": "Imitating Cost-Constrained Behaviors in Reinforcement Learning",
    "authors": [
      "Qian Shao",
      "Pradeep Varakantham",
      "Shih-Fen Cheng"
    ],
    "abstract": "Complex planning and scheduling problems have long been solved using various\noptimization or heuristic approaches. In recent years, imitation learning that\naims to learn from expert demonstrations has been proposed as a viable\nalternative to solving these problems. Generally speaking, imitation learning\nis designed to learn either the reward (or preference) model or directly the\nbehavioral policy by observing the behavior of an expert. Existing work in\nimitation learning and inverse reinforcement learning has focused on imitation\nprimarily in unconstrained settings (e.g., no limit on fuel consumed by the\nvehicle). However, in many real-world domains, the behavior of an expert is\ngoverned not only by reward (or preference) but also by constraints. For\ninstance, decisions on self-driving delivery vehicles are dependent not only on\nthe route preferences/rewards (depending on past demand data) but also on the\nfuel in the vehicle and the time available. In such problems, imitation\nlearning is challenging as decisions are not only dictated by the reward model\nbut are also dependent on a cost-constrained model. In this paper, we provide\nmultiple methods that match expert distributions in the presence of trajectory\ncost constraints through (a) Lagrangian-based method; (b) Meta-gradients to\nfind a good trade-off between expected return and minimizing constraint\nviolation; and (c) Cost-violation-based alternating gradient. We empirically\nshow that leading imitation learning approaches imitate cost-constrained\nbehaviors poorly and our meta-gradient-based approach achieves the best\nperformance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to the 34th International Conference on Automated Planning\n  and Scheduling (ICAPS-24)",
    "pdf_url": "http://arxiv.org/pdf/2403.17456v3",
    "published_date": "2024-03-26 07:41:54 UTC",
    "updated_date": "2024-05-23 08:57:17 UTC"
  },
  {
    "arxiv_id": "2403.17445v1",
    "title": "Incorporating Exponential Smoothing into MLP: A Simple but Effective Sequence Model",
    "authors": [
      "Jiqun Chu",
      "Zuoquan Lin"
    ],
    "abstract": "Modeling long-range dependencies in sequential data is a crucial step in\nsequence learning. A recently developed model, the Structured State Space (S4),\ndemonstrated significant effectiveness in modeling long-range sequences.\nHowever, It is unclear whether the success of S4 can be attributed to its\nintricate parameterization and HiPPO initialization or simply due to State\nSpace Models (SSMs). To further investigate the potential of the deep SSMs, we\nstart with exponential smoothing (ETS), a simple SSM, and propose a stacked\narchitecture by directly incorporating it into an element-wise MLP. We augment\nsimple ETS with additional parameters and complex field to reduce the inductive\nbias. Despite increasing less than 1\\% of parameters of element-wise MLP, our\nmodels achieve comparable results to S4 on the LRA benchmark.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 5 tables, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.17445v1",
    "published_date": "2024-03-26 07:23:46 UTC",
    "updated_date": "2024-03-26 07:23:46 UTC"
  },
  {
    "arxiv_id": "2403.17428v2",
    "title": "Aligning Large Language Models for Enhancing Psychiatric Interviews Through Symptom Delineation and Summarization: Pilot Study",
    "authors": [
      "Jae-hee So",
      "Joonhwan Chang",
      "Eunji Kim",
      "Junho Na",
      "JiYeon Choi",
      "Jy-yong Sohn",
      "Byung-Hoon Kim",
      "Sang Hui Chu"
    ],
    "abstract": "Background: Advancements in large language models (LLMs) have opened new\npossibilities in psychiatric interviews, an underexplored area where LLMs could\nbe valuable. This study focuses on enhancing psychiatric interviews by\nanalyzing counseling data from North Korean defectors who have experienced\ntrauma and mental health issues.\n  Objective: The study investigates whether LLMs can (1) identify parts of\nconversations that suggest psychiatric symptoms and recognize those symptoms,\nand (2) summarize stressors and symptoms based on interview transcripts.\n  Methods: LLMs are tasked with (1) extracting stressors from transcripts, (2)\nidentifying symptoms and their corresponding sections, and (3) generating\ninterview summaries using the extracted data. The transcripts were labeled by\nmental health experts for training and evaluation.\n  Results: In the zero-shot inference setting using GPT-4 Turbo, 73 out of 102\nsegments demonstrated a recall mid-token distance d < 20 in identifying\nsymptom-related sections. For recognizing specific symptoms, fine-tuning\noutperformed zero-shot inference, achieving an accuracy, precision, recall, and\nF1-score of 0.82. For the generative summarization task, LLMs using symptom and\nstressor information scored highly on G-Eval metrics: coherence (4.66),\nconsistency (4.73), fluency (2.16), and relevance (4.67). Retrieval-augmented\ngeneration showed no notable performance improvement.\n  Conclusions: LLMs, with fine-tuning or appropriate prompting, demonstrated\nstrong accuracy (over 0.8) for symptom delineation and achieved high coherence\n(4.6+) in summarization. This study highlights their potential to assist mental\nhealth practitioners in analyzing psychiatric interviews.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17428v2",
    "published_date": "2024-03-26 06:50:04 UTC",
    "updated_date": "2025-02-10 13:28:01 UTC"
  },
  {
    "arxiv_id": "2403.17426v1",
    "title": "Knowledge-Powered Recommendation for an Improved Diet Water Footprint",
    "authors": [
      "Saurav Joshi",
      "Filip Ilievski",
      "Jay Pujara"
    ],
    "abstract": "According to WWF, 1.1 billion people lack access to water, and 2.7 billion\nexperience water scarcity at least one month a year. By 2025, two-thirds of the\nworld's population may be facing water shortages. This highlights the urgency\nof managing water usage efficiently, especially in water-intensive sectors like\nfood. This paper proposes a recommendation engine, powered by knowledge graphs,\naiming to facilitate sustainable and healthy food consumption. The engine\nrecommends ingredient substitutes in user recipes that improve nutritional\nvalue and reduce environmental impact, particularly water footprint. The system\narchitecture includes source identification, information extraction, schema\nalignment, knowledge graph construction, and user interface development. The\nresearch offers a promising tool for promoting healthier eating habits and\ncontributing to water conservation efforts.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "3 pages, 1 figure, AAAI'24",
    "pdf_url": "http://arxiv.org/pdf/2403.17426v1",
    "published_date": "2024-03-26 06:47:17 UTC",
    "updated_date": "2024-03-26 06:47:17 UTC"
  },
  {
    "arxiv_id": "2403.17421v3",
    "title": "MA4DIV: Multi-Agent Reinforcement Learning for Search Result Diversification",
    "authors": [
      "Yiqun Chen",
      "Jiaxin Mao",
      "Yi Zhang",
      "Dehong Ma",
      "Long Xia",
      "Jun Fan",
      "Daiting Shi",
      "Zhicong Cheng",
      "Simiu Gu",
      "Dawei Yin"
    ],
    "abstract": "Search result diversification (SRD), which aims to ensure that documents in a\nranking list cover a broad range of subtopics, is a significant and widely\nstudied problem in Information Retrieval and Web Search. Existing methods\nprimarily utilize a paradigm of \"greedy selection\", i.e., selecting one\ndocument with the highest diversity score at a time or optimize an\napproximation of the objective function. These approaches tend to be\ninefficient and are easily trapped in a suboptimal state. To address these\nchallenges, we introduce Multi-Agent reinforcement learning (MARL) for search\nresult DIVersity, which called MA4DIV. In this approach, each document is an\nagent and the search result diversification is modeled as a cooperative task\namong multiple agents. By modeling the SRD ranking problem as a cooperative\nMARL problem, this approach allows for directly optimizing the diversity\nmetrics, such as $\\alpha$-NDCG, while achieving high training efficiency. We\nconducted experiments on public TREC datasets and a larger scale dataset in the\nindustrial setting. The experiemnts show that MA4DIV achieves substantial\nimprovements in both effectiveness and efficiency than existing baselines,\nespecially on the industrial dataset. The code of MA4DIV can be seen on\nhttps://github.com/chenyiqun/MA4DIV.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17421v3",
    "published_date": "2024-03-26 06:34:23 UTC",
    "updated_date": "2025-02-06 14:41:05 UTC"
  },
  {
    "arxiv_id": "2403.17419v1",
    "title": "AI Safety: Necessary, but insufficient and possibly problematic",
    "authors": [
      "Deepak P"
    ],
    "abstract": "This article critically examines the recent hype around AI safety. We first\nstart with noting the nature of the AI safety hype as being dominated by\ngovernments and corporations, and contrast it with other avenues within AI\nresearch on advancing social good. We consider what 'AI safety' actually means,\nand outline the dominant concepts that the digital footprint of AI safety\naligns with. We posit that AI safety has a nuanced and uneasy relationship with\ntransparency and other allied notions associated with societal good, indicating\nthat it is an insufficient notion if the goal is that of societal good in a\nbroad sense. We note that the AI safety debate has already influenced some\nregulatory efforts in AI, perhaps in not so desirable directions. We also share\nour concerns on how AI safety may normalize AI that advances structural harm\nthrough providing exploitative and harmful AI with a veneer of safety.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "AI & Soc (2024)",
    "pdf_url": "http://arxiv.org/pdf/2403.17419v1",
    "published_date": "2024-03-26 06:18:42 UTC",
    "updated_date": "2024-03-26 06:18:42 UTC"
  },
  {
    "arxiv_id": "2403.17410v2",
    "title": "On permutation-invariant neural networks",
    "authors": [
      "Masanari Kimura",
      "Ryotaro Shimizu",
      "Yuki Hirakawa",
      "Ryosuke Goto",
      "Yuki Saito"
    ],
    "abstract": "Conventional machine learning algorithms have traditionally been designed\nunder the assumption that input data follows a vector-based format, with an\nemphasis on vector-centric paradigms. However, as the demand for tasks\ninvolving set-based inputs has grown, there has been a paradigm shift in the\nresearch community towards addressing these challenges. In recent years, the\nemergence of neural network architectures such as Deep Sets and Transformers\nhas presented a significant advancement in the treatment of set-based data.\nThese architectures are specifically engineered to naturally accommodate sets\nas input, enabling more effective representation and processing of set\nstructures. Consequently, there has been a surge of research endeavors\ndedicated to exploring and harnessing the capabilities of these architectures\nfor various tasks involving the approximation of set functions. This\ncomprehensive survey aims to provide an overview of the diverse problem\nsettings and ongoing research efforts pertaining to neural networks that\napproximate set functions. By delving into the intricacies of these approaches\nand elucidating the associated challenges, the survey aims to equip readers\nwith a comprehensive understanding of the field. Through this comprehensive\nperspective, we hope that researchers can gain valuable insights into the\npotential applications, inherent limitations, and future directions of\nset-based neural networks. Indeed, from this survey we gain two insights: i)\nDeep Sets and its variants can be generalized by differences in the aggregation\nfunction, and ii) the behavior of Deep Sets is sensitive to the choice of the\naggregation function. From these observations, we show that Deep Sets, one of\nthe well-known permutation-invariant neural networks, can be generalized in the\nsense of a quasi-arithmetic mean.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17410v2",
    "published_date": "2024-03-26 06:06:01 UTC",
    "updated_date": "2024-03-28 22:28:02 UTC"
  },
  {
    "arxiv_id": "2403.17407v3",
    "title": "Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens",
    "authors": [
      "S M Jishanul Islam",
      "Sadia Ahmmed",
      "Sahid Hossain Mustakim"
    ],
    "abstract": "Accurate transcription of Bengali text to the International Phonetic Alphabet\n(IPA) is a challenging task due to the complex phonology of the language and\ncontext-dependent sound changes. This challenge is even more for regional\nBengali dialects due to unavailability of standardized spelling conventions for\nthese dialects, presence of local and foreign words popular in those regions\nand phonological diversity across different regions. This paper presents an\napproach to this sequence-to-sequence problem by introducing the District\nGuided Tokens (DGT) technique on a new dataset spanning six districts of\nBangladesh. The key idea is to provide the model with explicit information\nabout the regional dialect or \"district\" of the input text before generating\nthe IPA transcription. This is achieved by prepending a district token to the\ninput sequence, effectively guiding the model to understand the unique phonetic\npatterns associated with each district. The DGT technique is applied to\nfine-tune several transformer-based models, on this new dataset. Experimental\nresults demonstrate the effectiveness of DGT, with the ByT5 model achieving\nsuperior performance over word-based models like mT5, BanglaT5, and umT5. This\nis attributed to ByT5's ability to handle a high percentage of\nout-of-vocabulary words in the test set. The proposed approach highlights the\nimportance of incorporating regional dialect information into ubiquitous\nnatural language processing systems for languages with diverse phonological\nvariations. The following work was a result of the \"Bhashamul\" challenge, which\nis dedicated to solving the problem of Bengali text with regional dialects to\nIPA transcription https://www.kaggle.com/competitions/regipa/. The training and\ninference notebooks are available through the competition link.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "F.2.2; I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "Updated missing references to the dataset and corrected some\n  sentences in sections 1 and 2. This work became the champion of the Bhashamul\n  challenge",
    "pdf_url": "http://arxiv.org/pdf/2403.17407v3",
    "published_date": "2024-03-26 05:55:21 UTC",
    "updated_date": "2024-04-02 04:15:36 UTC"
  },
  {
    "arxiv_id": "2403.17395v1",
    "title": "An Open-source End-to-End Logic Optimization Framework for Large-scale Boolean Network with Reinforcement Learning",
    "authors": [
      "Zhen Li",
      "Kaixiang Zhu",
      "Xuegong Zhou",
      "Lingli Wang"
    ],
    "abstract": "We propose an open-source end-to-end logic optimization framework for\nlarge-scale boolean network with reinforcement learning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "5 pages, 4 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2403.17395v1",
    "published_date": "2024-03-26 05:25:01 UTC",
    "updated_date": "2024-03-26 05:25:01 UTC"
  },
  {
    "arxiv_id": "2403.17385v1",
    "title": "ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition",
    "authors": [
      "Haris Riaz",
      "Razvan-Gabriel Dumitru",
      "Mihai Surdeanu"
    ],
    "abstract": "In this work, we revisit the problem of semi-supervised named entity\nrecognition (NER) focusing on extremely light supervision, consisting of a\nlexicon containing only 10 examples per class. We introduce ELLEN, a simple,\nfully modular, neuro-symbolic method that blends fine-tuned language models\nwith linguistic rules. These rules include insights such as ''One Sense Per\nDiscourse'', using a Masked Language Model as an unsupervised NER, leveraging\npart-of-speech tags to identify and eliminate unlabeled entities as false\nnegatives, and other intuitions about classifier confidence scores in local and\nglobal context. ELLEN achieves very strong performance on the CoNLL-2003\ndataset when using the minimal supervision from the lexicon above. It also\noutperforms most existing (and considerably more complex) semi-supervised NER\nmethods under the same supervision settings commonly used in the literature\n(i.e., 5% of the training data). Further, we evaluate our CoNLL-2003 model in a\nzero-shot scenario on WNUT-17 where we find that it outperforms GPT-3.5 and\nachieves comparable performance to GPT-4. In a zero-shot setting, ELLEN also\nachieves over 75% of the performance of a strong, fully supervised model\ntrained on gold data. Our code is available at:\nhttps://github.com/hriaz17/ELLEN.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to LREC-COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.17385v1",
    "published_date": "2024-03-26 05:11:51 UTC",
    "updated_date": "2024-03-26 05:11:51 UTC"
  },
  {
    "arxiv_id": "2403.17384v1",
    "title": "Explainable Graph Neural Networks for Observation Impact Analysis in Atmospheric State Estimation",
    "authors": [
      "Hyeon-Ju Jeon",
      "Jeon-Ho Kang",
      "In-Hyuk Kwon",
      "O-Joun Lee"
    ],
    "abstract": "This paper investigates the impact of observations on atmospheric state\nestimation in weather forecasting systems using graph neural networks (GNNs)\nand explainability methods. We integrate observation and Numerical Weather\nPrediction (NWP) points into a meteorological graph, extracting $k$-hop\nsubgraphs centered on NWP points. Self-supervised GNNs are employed to estimate\nthe atmospheric state by aggregating data within these $k$-hop radii. The study\napplies gradient-based explainability methods to quantify the significance of\ndifferent observations in the estimation process. Evaluated with data from 11\nsatellite and land-based observations, the results highlight the effectiveness\nof visualizing the importance of observation types, enhancing the understanding\nand optimization of observational data in weather forecasting.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17384v1",
    "published_date": "2024-03-26 05:10:47 UTC",
    "updated_date": "2024-03-26 05:10:47 UTC"
  },
  {
    "arxiv_id": "2403.17381v1",
    "title": "Application-Driven Innovation in Machine Learning",
    "authors": [
      "David Rolnick",
      "Alan Aspuru-Guzik",
      "Sara Beery",
      "Bistra Dilkina",
      "Priya L. Donti",
      "Marzyeh Ghassemi",
      "Hannah Kerner",
      "Claire Monteleoni",
      "Esther Rolf",
      "Milind Tambe",
      "Adam White"
    ],
    "abstract": "As applications of machine learning proliferate, innovative algorithms\ninspired by specific real-world challenges have become increasingly important.\nSuch work offers the potential for significant impact not merely in domains of\napplication but also in machine learning itself. In this paper, we describe the\nparadigm of application-driven research in machine learning, contrasting it\nwith the more standard paradigm of methods-driven research. We illustrate the\nbenefits of application-driven machine learning and how this approach can\nproductively synergize with methods-driven work. Despite these benefits, we\nfind that reviewing, hiring, and teaching practices in machine learning often\nhold back application-driven innovation. We outline how these processes may be\nimproved.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.17381v1",
    "published_date": "2024-03-26 04:59:27 UTC",
    "updated_date": "2024-03-26 04:59:27 UTC"
  },
  {
    "arxiv_id": "2403.17377v1",
    "title": "Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance",
    "authors": [
      "Donghoon Ahn",
      "Hyoungwon Cho",
      "Jaewon Min",
      "Wooseok Jang",
      "Jungwoo Kim",
      "SeonHwa Kim",
      "Hyun Hee Park",
      "Kyong Hwan Jin",
      "Seungryong Kim"
    ],
    "abstract": "Recent studies have demonstrated that diffusion models are capable of\ngenerating high-quality samples, but their quality heavily depends on sampling\nguidance techniques, such as classifier guidance (CG) and classifier-free\nguidance (CFG). These techniques are often not applicable in unconditional\ngeneration or in various downstream tasks such as image restoration. In this\npaper, we propose a novel sampling guidance, called Perturbed-Attention\nGuidance (PAG), which improves diffusion sample quality across both\nunconditional and conditional settings, achieving this without requiring\nadditional training or the integration of external modules. PAG is designed to\nprogressively enhance the structure of samples throughout the denoising\nprocess. It involves generating intermediate samples with degraded structure by\nsubstituting selected self-attention maps in diffusion U-Net with an identity\nmatrix, by considering the self-attention mechanisms' ability to capture\nstructural information, and guiding the denoising process away from these\ndegraded samples. In both ADM and Stable Diffusion, PAG surprisingly improves\nsample quality in conditional and even unconditional scenarios. Moreover, PAG\nsignificantly improves the baseline performance in various downstream tasks\nwhere existing guidances such as CG or CFG cannot be fully utilized, including\nControlNet with empty prompts and image restoration such as inpainting and\ndeblurring.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page is available at\n  https://ku-cvlab.github.io/Perturbed-Attention-Guidance",
    "pdf_url": "http://arxiv.org/pdf/2403.17377v1",
    "published_date": "2024-03-26 04:49:11 UTC",
    "updated_date": "2024-03-26 04:49:11 UTC"
  },
  {
    "arxiv_id": "2403.17373v1",
    "title": "AIDE: An Automatic Data Engine for Object Detection in Autonomous Driving",
    "authors": [
      "Mingfu Liang",
      "Jong-Chyi Su",
      "Samuel Schulter",
      "Sparsh Garg",
      "Shiyu Zhao",
      "Ying Wu",
      "Manmohan Chandraker"
    ],
    "abstract": "Autonomous vehicle (AV) systems rely on robust perception models as a\ncornerstone of safety assurance. However, objects encountered on the road\nexhibit a long-tailed distribution, with rare or unseen categories posing\nchallenges to a deployed perception model. This necessitates an expensive\nprocess of continuously curating and annotating data with significant human\neffort. We propose to leverage recent advances in vision-language and large\nlanguage models to design an Automatic Data Engine (AIDE) that automatically\nidentifies issues, efficiently curates data, improves the model through\nauto-labeling, and verifies the model through generation of diverse scenarios.\nThis process operates iteratively, allowing for continuous self-improvement of\nthe model. We further establish a benchmark for open-world detection on AV\ndatasets to comprehensively evaluate various learning paradigms, demonstrating\nour method's superior performance at a reduced cost.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR-2024",
    "pdf_url": "http://arxiv.org/pdf/2403.17373v1",
    "published_date": "2024-03-26 04:27:56 UTC",
    "updated_date": "2024-03-26 04:27:56 UTC"
  },
  {
    "arxiv_id": "2403.17368v1",
    "title": "ChatGPT Rates Natural Language Explanation Quality Like Humans: But on Which Scales?",
    "authors": [
      "Fan Huang",
      "Haewoon Kwak",
      "Kunwoo Park",
      "Jisun An"
    ],
    "abstract": "As AI becomes more integral in our lives, the need for transparency and\nresponsibility grows. While natural language explanations (NLEs) are vital for\nclarifying the reasoning behind AI decisions, evaluating them through human\njudgments is complex and resource-intensive due to subjectivity and the need\nfor fine-grained ratings. This study explores the alignment between ChatGPT and\nhuman assessments across multiple scales (i.e., binary, ternary, and 7-Likert\nscale). We sample 300 data instances from three NLE datasets and collect 900\nhuman annotations for both informativeness and clarity scores as the text\nquality measurement. We further conduct paired comparison experiments under\ndifferent ranges of subjectivity scores, where the baseline comes from 8,346\nhuman annotations. Our results show that ChatGPT aligns better with humans in\nmore coarse-grained scales. Also, paired comparisons and dynamic prompting\n(i.e., providing semantically similar examples in the prompt) improve the\nalignment. This research advances our understanding of large language models'\ncapabilities to assess the text explanation quality in different configurations\nfor responsible AI development.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accpeted by LREC-COLING 2024 main conference, long paper",
    "pdf_url": "http://arxiv.org/pdf/2403.17368v1",
    "published_date": "2024-03-26 04:07:08 UTC",
    "updated_date": "2024-03-26 04:07:08 UTC"
  },
  {
    "arxiv_id": "2403.17361v1",
    "title": "Bridging Textual and Tabular Worlds for Fact Verification: A Lightweight, Attention-Based Model",
    "authors": [
      "Shirin Dabbaghi Varnosfaderani",
      "Canasai Kruengkrai",
      "Ramin Yahyapour",
      "Junichi Yamagishi"
    ],
    "abstract": "FEVEROUS is a benchmark and research initiative focused on fact extraction\nand verification tasks involving unstructured text and structured tabular data.\nIn FEVEROUS, existing works often rely on extensive preprocessing and utilize\nrule-based transformations of data, leading to potential context loss or\nmisleading encodings. This paper introduces a simple yet powerful model that\nnullifies the need for modality conversion, thereby preserving the original\nevidence's context. By leveraging pre-trained models on diverse text and\ntabular datasets and by incorporating a lightweight attention-based mechanism,\nour approach efficiently exploits latent connections between different data\ntypes, thereby yielding comprehensive and reliable verdict predictions. The\nmodel's modular structure adeptly manages multi-modal information, ensuring the\nintegrity and authenticity of the original evidence are uncompromised.\nComparative analyses reveal that our approach exhibits competitive performance,\naligning itself closely with top-tier models on the FEVEROUS benchmark.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for a presentation at LREC-COLING 2024 - The 2024 Joint\n  International Conference on Computational Linguistics, Language Resources and\n  Evaluation",
    "pdf_url": "http://arxiv.org/pdf/2403.17361v1",
    "published_date": "2024-03-26 03:54:25 UTC",
    "updated_date": "2024-03-26 03:54:25 UTC"
  },
  {
    "arxiv_id": "2403.17358v1",
    "title": "Addressing Myopic Constrained POMDP Planning with Recursive Dual Ascent",
    "authors": [
      "Paula Stocco",
      "Suhas Chundi",
      "Arec Jamgochian",
      "Mykel J. Kochenderfer"
    ],
    "abstract": "Lagrangian-guided Monte Carlo tree search with global dual ascent has been\napplied to solve large constrained partially observable Markov decision\nprocesses (CPOMDPs) online. In this work, we demonstrate that these global dual\nparameters can lead to myopic action selection during exploration, ultimately\nleading to suboptimal decision making. To address this, we introduce\nhistory-dependent dual variables that guide local action selection and are\noptimized with recursive dual ascent. We empirically compare the performance of\nour approach on a motivating toy example and two large CPOMDPs, demonstrating\nimproved exploration, and ultimately, safer outcomes.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to the 2024 International Conference on Automated Planning\n  and Scheduling (ICAPS)",
    "pdf_url": "http://arxiv.org/pdf/2403.17358v1",
    "published_date": "2024-03-26 03:46:33 UTC",
    "updated_date": "2024-03-26 03:46:33 UTC"
  },
  {
    "arxiv_id": "2403.17357v1",
    "title": "MESIA: Understanding and Leveraging Supplementary Nature of Method-level Comments for Automatic Comment Generation",
    "authors": [
      "Xinglu Pan",
      "Chenxiao Liu",
      "Yanzhen Zou",
      "Tao Xie",
      "Bing Xie"
    ],
    "abstract": "Code comments are important for developers in program comprehension. In\nscenarios of comprehending and reusing a method, developers expect code\ncomments to provide supplementary information beyond the method signature.\nHowever, the extent of such supplementary information varies a lot in different\ncode comments. In this paper, we raise the awareness of the supplementary\nnature of method-level comments and propose a new metric named MESIA (Mean\nSupplementary Information Amount) to assess the extent of supplementary\ninformation that a code comment can provide. With the MESIA metric, we conduct\nexperiments on a popular code-comment dataset and three common types of neural\napproaches to generate method-level comments. Our experimental results\ndemonstrate the value of our proposed work with a number of findings. (1)\nSmall-MESIA comments occupy around 20% of the dataset and mostly fall into only\nthe WHAT comment category. (2) Being able to provide various kinds of essential\ninformation, large-MESIA comments in the dataset are difficult for existing\nneural approaches to generate. (3) We can improve the capability of existing\nneural approaches to generate large-MESIA comments by reducing the proportion\nof small-MESIA comments in the training set. (4) The retrained model can\ngenerate large-MESIA comments that convey essential meaningful supplementary\ninformation for methods in the small-MESIA test set, but will get a lower BLEU\nscore in evaluation. These findings indicate that with good training data,\nauto-generated comments can sometimes even surpass human-written reference\ncomments, and having no appropriate ground truth for evaluation is an issue\nthat needs to be addressed by future work on automatic comment generation.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "In 32nd IEEE/ACM International Conference on Program Comprehension\n  (ICPC'24)",
    "pdf_url": "http://arxiv.org/pdf/2403.17357v1",
    "published_date": "2024-03-26 03:44:51 UTC",
    "updated_date": "2024-03-26 03:44:51 UTC"
  },
  {
    "arxiv_id": "2403.17350v1",
    "title": "The Solution of the Zodiac Killer's 340-Character Cipher",
    "authors": [
      "David Oranchak",
      "Sam Blake",
      "Jarl Van Eycke"
    ],
    "abstract": "The case of the Zodiac Killer is one of the most widely known unsolved serial\nkiller cases in history. The unidentified killer murdered five known victims\nand terrorized the state of California. He also communicated extensively with\nthe press and law enforcement. Besides his murders, Zodiac was known for his\nuse of ciphers. The first Zodiac cipher was solved within a week of its\npublication, while the second cipher was solved by the authors after 51 years,\nwhen it was discovered to be a transposition and homophonic substitution cipher\nwith unusual qualities. In this paper, we detail the historical significance of\nthis cipher and the numerous efforts which culminated in its solution.",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17350v1",
    "published_date": "2024-03-26 03:28:02 UTC",
    "updated_date": "2024-03-26 03:28:02 UTC"
  },
  {
    "arxiv_id": "2403.17342v1",
    "title": "The Solution for the ICCV 2023 1st Scientific Figure Captioning Challenge",
    "authors": [
      "Dian Chao",
      "Xin Song",
      "Shupeng Zhong",
      "Boyuan Wang",
      "Xiangyu Wu",
      "Chen Zhu",
      "Yang Yang"
    ],
    "abstract": "In this paper, we propose a solution for improving the quality of captions\ngenerated for figures in papers. We adopt the approach of summarizing the\ntextual content in the paper to generate image captions. Throughout our study,\nwe encounter discrepancies in the OCR information provided in the official\ndataset. To rectify this, we employ the PaddleOCR toolkit to extract OCR\ninformation from all images. Moreover, we observe that certain textual content\nin the official paper pertains to images that are not relevant for captioning,\nthereby introducing noise during caption generation. To mitigate this issue, we\nleverage LLaMA to extract image-specific information by querying the textual\ncontent based on image mentions, effectively filtering out extraneous\ninformation. Additionally, we recognize a discrepancy between the primary use\nof maximum likelihood estimation during text generation and the evaluation\nmetrics such as ROUGE employed to assess the quality of generated captions. To\nbridge this gap, we integrate the BRIO model framework, enabling a more\ncoherent alignment between the generation and evaluation processes. Our\napproach ranked first in the final test with a score of 4.49.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17342v1",
    "published_date": "2024-03-26 03:03:50 UTC",
    "updated_date": "2024-03-26 03:03:50 UTC"
  },
  {
    "arxiv_id": "2403.17338v3",
    "title": "Reinforcement Learning-based Receding Horizon Control using Adaptive Control Barrier Functions for Safety-Critical Systems",
    "authors": [
      "Ehsan Sabouni",
      "H. M. Sabbir Ahmad",
      "Vittorio Giammarino",
      "Christos G. Cassandras",
      "Ioannis Ch. Paschalidis",
      "Wenchao Li"
    ],
    "abstract": "Optimal control methods provide solutions to safety-critical problems but\neasily become intractable. Control Barrier Functions (CBFs) have emerged as a\npopular technique that facilitates their solution by provably guaranteeing\nsafety, through their forward invariance property, at the expense of some\nperformance loss. This approach involves defining a performance objective\nalongside CBF-based safety constraints that must always be enforced.\nUnfortunately, both performance and solution feasibility can be significantly\nimpacted by two key factors: (i) the selection of the cost function and\nassociated parameters, and (ii) the calibration of parameters within the\nCBF-based constraints, which capture the trade-off between performance and\nconservativeness. %as well as infeasibility. To address these challenges, we\npropose a Reinforcement Learning (RL)-based Receding Horizon Control (RHC)\napproach leveraging Model Predictive Control (MPC) with CBFs (MPC-CBF). In\nparticular, we parameterize our controller and use bilevel optimization, where\nRL is used to learn the optimal parameters while MPC computes the optimal\ncontrol input. We validate our method by applying it to the challenging\nautomated merging control problem for Connected and Automated Vehicles (CAVs)\nat conflicting roadways. Results demonstrate improved performance and a\nsignificant reduction in the number of infeasible cases compared to traditional\nheuristic approaches used for tuning CBF-based controllers, showcasing the\neffectiveness of the proposed method.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17338v3",
    "published_date": "2024-03-26 02:49:08 UTC",
    "updated_date": "2025-02-19 20:37:14 UTC"
  },
  {
    "arxiv_id": "2403.17333v1",
    "title": "The Pursuit of Fairness in Artificial Intelligence Models: A Survey",
    "authors": [
      "Tahsin Alamgir Kheya",
      "Mohamed Reda Bouadjenek",
      "Sunil Aryal"
    ],
    "abstract": "Artificial Intelligence (AI) models are now being utilized in all facets of\nour lives such as healthcare, education and employment. Since they are used in\nnumerous sensitive environments and make decisions that can be life altering,\npotential biased outcomes are a pressing matter. Developers should ensure that\nsuch models don't manifest any unexpected discriminatory practices like\npartiality for certain genders, ethnicities or disabled people. With the\nubiquitous dissemination of AI systems, researchers and practitioners are\nbecoming more aware of unfair models and are bound to mitigate bias in them.\nSignificant research has been conducted in addressing such issues to ensure\nmodels don't intentionally or unintentionally perpetuate bias. This survey\noffers a synopsis of the different ways researchers have promoted fairness in\nAI systems. We explore the different definitions of fairness existing in the\ncurrent literature. We create a comprehensive taxonomy by categorizing\ndifferent types of bias and investigate cases of biased AI in different\napplication domains. A thorough study is conducted of the approaches and\ntechniques employed by researchers to mitigate bias in AI models. Moreover, we\nalso delve into the impact of biased models on user experience and the ethical\nconsiderations to contemplate when developing and deploying such models. We\nhope this survey helps researchers and practitioners understand the intricate\ndetails of fairness and bias in AI systems. By sharing this thorough survey, we\naim to promote additional discourse in the domain of equitable and responsible\nAI.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "37 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.17333v1",
    "published_date": "2024-03-26 02:33:36 UTC",
    "updated_date": "2024-03-26 02:33:36 UTC"
  },
  {
    "arxiv_id": "2403.17329v2",
    "title": "Deep Support Vectors",
    "authors": [
      "Junhoo Lee",
      "Hyunho Lee",
      "Kyomin Hwang",
      "Nojun Kwak"
    ],
    "abstract": "Deep learning has achieved tremendous success. \\nj{However,} unlike SVMs,\nwhich provide direct decision criteria and can be trained with a small dataset,\nit still has significant weaknesses due to its requirement for massive datasets\nduring training and the black-box characteristics on decision criteria.\n\\nj{This paper addresses} these issues by identifying support vectors in deep\nlearning models. To this end, we propose the DeepKKT condition, an adaptation\nof the traditional Karush-Kuhn-Tucker (KKT) condition for deep learning models,\nand confirm that generated Deep Support Vectors (DSVs) using this condition\nexhibit properties similar to traditional support vectors. This allows us to\napply our method to few-shot dataset distillation problems and alleviate the\nblack-box characteristics of deep learning models. Additionally, we demonstrate\nthat the DeepKKT condition can transform conventional classification models\ninto generative models with high fidelity, particularly as latent\n\\jh{generative} models using class labels as latent variables. We validate the\neffectiveness of DSVs \\nj{using common datasets (ImageNet, CIFAR10 \\nj{and}\nCIFAR100) on the general architectures (ResNet and ConvNet)}, proving their\npractical applicability. (See Fig.~\\ref{fig:generated})",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17329v2",
    "published_date": "2024-03-26 02:24:32 UTC",
    "updated_date": "2024-06-27 06:19:01 UTC"
  },
  {
    "arxiv_id": "2403.17328v2",
    "title": "Learning Traffic Signal Control via Genetic Programming",
    "authors": [
      "Xiao-Cheng Liao",
      "Yi Mei",
      "Mengjie Zhang"
    ],
    "abstract": "The control of traffic signals is crucial for improving transportation\nefficiency. Recently, learning-based methods, especially Deep Reinforcement\nLearning (DRL), garnered substantial success in the quest for more efficient\ntraffic signal control strategies. However, the design of rewards in DRL highly\ndemands domain knowledge to converge to an effective policy, and the final\npolicy also presents difficulties in terms of explainability. In this work, a\nnew learning-based method for signal control in complex intersections is\nproposed. In our approach, we design a concept of phase urgency for each signal\nphase. During signal transitions, the traffic light control strategy selects\nthe next phase to be activated based on the phase urgency. We then proposed to\nrepresent the urgency function as an explainable tree structure. The urgency\nfunction can calculate the phase urgency for a specific phase based on the\ncurrent road conditions. Genetic programming is adopted to perform\ngradient-free optimization of the urgency function. We test our algorithm on\nmultiple public traffic signal control datasets. The experimental results\nindicate that the tree-shaped urgency function evolved by genetic programming\noutperforms the baselines, including a state-of-the-art method in the\ntransportation field and a well-known DRL-based method.",
    "categories": [
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17328v2",
    "published_date": "2024-03-26 02:22:08 UTC",
    "updated_date": "2025-01-05 05:19:29 UTC"
  },
  {
    "arxiv_id": "2405.12985v1",
    "title": "Sketch2Prototype: Rapid Conceptual Design Exploration and Prototyping with Generative AI",
    "authors": [
      "Kristen M. Edwards",
      "Brandon Man",
      "Faez Ahmed"
    ],
    "abstract": "Sketch2Prototype is an AI-based framework that transforms a hand-drawn sketch\ninto a diverse set of 2D images and 3D prototypes through sketch-to-text,\ntext-to-image, and image-to-3D stages. This framework, shown across various\nsketches, rapidly generates text, image, and 3D modalities for enhanced\nearly-stage design exploration. We show that using text as an intermediate\nmodality outperforms direct sketch-to-3D baselines for generating diverse and\nmanufacturable 3D models. We find limitations in current image-to-3D\ntechniques, while noting the value of the text modality for user-feedback and\niterative design augmentation.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.HC",
    "comment": "10 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.12985v1",
    "published_date": "2024-03-26 02:12:17 UTC",
    "updated_date": "2024-03-26 02:12:17 UTC"
  },
  {
    "arxiv_id": "2403.17319v1",
    "title": "JMultiWOZ: A Large-Scale Japanese Multi-Domain Task-Oriented Dialogue Dataset",
    "authors": [
      "Atsumoto Ohashi",
      "Ryu Hirai",
      "Shinya Iizuka",
      "Ryuichiro Higashinaka"
    ],
    "abstract": "Dialogue datasets are crucial for deep learning-based task-oriented dialogue\nsystem research. While numerous English language multi-domain task-oriented\ndialogue datasets have been developed and contributed to significant\nadvancements in task-oriented dialogue systems, such a dataset does not exist\nin Japanese, and research in this area is limited compared to that in English.\nIn this study, towards the advancement of research and development of\ntask-oriented dialogue systems in Japanese, we constructed JMultiWOZ, the first\nJapanese language large-scale multi-domain task-oriented dialogue dataset.\nUsing JMultiWOZ, we evaluated the dialogue state tracking and response\ngeneration capabilities of the state-of-the-art methods on the existing major\nEnglish benchmark dataset MultiWOZ2.2 and the latest large language model\n(LLM)-based methods. Our evaluation results demonstrated that JMultiWOZ\nprovides a benchmark that is on par with MultiWOZ2.2. In addition, through\nevaluation experiments of interactive dialogues with the models and human\nparticipants, we identified limitations in the task completion capabilities of\nLLMs in Japanese.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by LREC-COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.17319v1",
    "published_date": "2024-03-26 02:01:18 UTC",
    "updated_date": "2024-03-26 02:01:18 UTC"
  },
  {
    "arxiv_id": "2403.17312v1",
    "title": "ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching",
    "authors": [
      "Youpeng Zhao",
      "Di Wu",
      "Jun Wang"
    ],
    "abstract": "The Transformer architecture has significantly advanced natural language\nprocessing (NLP) and has been foundational in developing large language models\n(LLMs) such as LLaMA and OPT, which have come to dominate a broad range of NLP\ntasks. Despite their superior accuracy, LLMs present unique challenges in\npractical inference, concerning the compute and memory-intensive nature. Thanks\nto the autoregressive characteristic of LLM inference, KV caching for the\nattention layers in Transformers can effectively accelerate LLM inference by\nsubstituting quadratic-complexity computation with linear-complexity memory\naccesses. Yet, this approach requires increasing memory as demand grows for\nprocessing longer sequences. The overhead leads to reduced throughput due to\nI/O bottlenecks and even out-of-memory errors, particularly on\nresource-constrained systems like a single commodity GPU. In this paper, we\npropose ALISA, a novel algorithm-system co-design solution to address the\nchallenges imposed by KV caching. On the algorithm level, ALISA prioritizes\ntokens that are most important in generating a new token via a Sparse Window\nAttention (SWA) algorithm. SWA introduces high sparsity in attention layers and\nreduces the memory footprint of KV caching at negligible accuracy loss. On the\nsystem level, ALISA employs three-phase token-level dynamical scheduling and\noptimizes the trade-off between caching and recomputation, thus maximizing the\noverall performance in resource-constrained systems. In a single GPU-CPU\nsystem, we demonstrate that under varying workloads, ALISA improves the\nthroughput of baseline systems such as FlexGen and vLLM by up to 3X and 1.9X,\nrespectively.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.PF"
    ],
    "primary_category": "cs.AI",
    "comment": "ISCA 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.17312v1",
    "published_date": "2024-03-26 01:46:34 UTC",
    "updated_date": "2024-03-26 01:46:34 UTC"
  },
  {
    "arxiv_id": "2403.17308v1",
    "title": "Neural Multimodal Topic Modeling: A Comprehensive Evaluation",
    "authors": [
      "Felipe GonzÃ¡lez-Pizarro",
      "Giuseppe Carenini"
    ],
    "abstract": "Neural topic models can successfully find coherent and diverse topics in\ntextual data. However, they are limited in dealing with multimodal datasets\n(e.g., images and text). This paper presents the first systematic and\ncomprehensive evaluation of multimodal topic modeling of documents containing\nboth text and images. In the process, we propose two novel topic modeling\nsolutions and two novel evaluation metrics. Overall, our evaluation on an\nunprecedented rich and diverse collection of datasets indicates that both of\nour models generate coherent and diverse topics. Nevertheless, the extent to\nwhich one method outperforms the other depends on the metrics and dataset\ncombinations, which suggests further exploration of hybrid solutions in the\nfuture. Notably, our succinct human evaluation aligns with the outcomes\ndetermined by our proposed metrics. This alignment not only reinforces the\ncredibility of our metrics but also highlights the potential for their\napplication in guiding future multimodal topic modeling endeavors.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "Camera-Ready for LREC-COLING 2024 (Long Paper)",
    "pdf_url": "http://arxiv.org/pdf/2403.17308v1",
    "published_date": "2024-03-26 01:29:46 UTC",
    "updated_date": "2024-03-26 01:29:46 UTC"
  },
  {
    "arxiv_id": "2403.17306v2",
    "title": "Visual Hallucination: Definition, Quantification, and Prescriptive Remediations",
    "authors": [
      "Anku Rani",
      "Vipula Rawte",
      "Harshad Sharma",
      "Neeraj Anand",
      "Krishnav Rajbangshi",
      "Amit Sheth",
      "Amitava Das"
    ],
    "abstract": "The troubling rise of hallucination presents perhaps the most significant\nimpediment to the advancement of responsible AI. In recent times, considerable\nresearch has focused on detecting and mitigating hallucination in Large\nLanguage Models (LLMs). However, it's worth noting that hallucination is also\nquite prevalent in Vision-Language models (VLMs). In this paper, we offer a\nfine-grained discourse on profiling VLM hallucination based on two tasks: i)\nimage captioning, and ii) Visual Question Answering (VQA). We delineate eight\nfine-grained orientations of visual hallucination: i) Contextual Guessing, ii)\nIdentity Incongruity, iii) Geographical Erratum, iv) Visual Illusion, v) Gender\nAnomaly, vi) VLM as Classifier, vii) Wrong Reading, and viii) Numeric\nDiscrepancy. We curate Visual HallucInation eLiciTation (VHILT), a publicly\navailable dataset comprising 2,000 samples generated using eight VLMs across\ntwo tasks of captioning and VQA along with human annotations for the categories\nas mentioned earlier.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17306v2",
    "published_date": "2024-03-26 01:28:42 UTC",
    "updated_date": "2024-03-31 03:52:14 UTC"
  },
  {
    "arxiv_id": "2403.17297v1",
    "title": "InternLM2 Technical Report",
    "authors": [
      "Zheng Cai",
      "Maosong Cao",
      "Haojiong Chen",
      "Kai Chen",
      "Keyu Chen",
      "Xin Chen",
      "Xun Chen",
      "Zehui Chen",
      "Zhi Chen",
      "Pei Chu",
      "Xiaoyi Dong",
      "Haodong Duan",
      "Qi Fan",
      "Zhaoye Fei",
      "Yang Gao",
      "Jiaye Ge",
      "Chenya Gu",
      "Yuzhe Gu",
      "Tao Gui",
      "Aijia Guo",
      "Qipeng Guo",
      "Conghui He",
      "Yingfan Hu",
      "Ting Huang",
      "Tao Jiang",
      "Penglong Jiao",
      "Zhenjiang Jin",
      "Zhikai Lei",
      "Jiaxing Li",
      "Jingwen Li",
      "Linyang Li",
      "Shuaibin Li",
      "Wei Li",
      "Yining Li",
      "Hongwei Liu",
      "Jiangning Liu",
      "Jiawei Hong",
      "Kaiwen Liu",
      "Kuikun Liu",
      "Xiaoran Liu",
      "Chengqi Lv",
      "Haijun Lv",
      "Kai Lv",
      "Li Ma",
      "Runyuan Ma",
      "Zerun Ma",
      "Wenchang Ning",
      "Linke Ouyang",
      "Jiantao Qiu",
      "Yuan Qu",
      "Fukai Shang",
      "Yunfan Shao",
      "Demin Song",
      "Zifan Song",
      "Zhihao Sui",
      "Peng Sun",
      "Yu Sun",
      "Huanze Tang",
      "Bin Wang",
      "Guoteng Wang",
      "Jiaqi Wang",
      "Jiayu Wang",
      "Rui Wang",
      "Yudong Wang",
      "Ziyi Wang",
      "Xingjian Wei",
      "Qizhen Weng",
      "Fan Wu",
      "Yingtong Xiong",
      "Chao Xu",
      "Ruiliang Xu",
      "Hang Yan",
      "Yirong Yan",
      "Xiaogui Yang",
      "Haochen Ye",
      "Huaiyuan Ying",
      "Jia Yu",
      "Jing Yu",
      "Yuhang Zang",
      "Chuyu Zhang",
      "Li Zhang",
      "Pan Zhang",
      "Peng Zhang",
      "Ruijie Zhang",
      "Shuo Zhang",
      "Songyang Zhang",
      "Wenjian Zhang",
      "Wenwei Zhang",
      "Xingcheng Zhang",
      "Xinyue Zhang",
      "Hui Zhao",
      "Qian Zhao",
      "Xiaomeng Zhao",
      "Fengzhe Zhou",
      "Zaida Zhou",
      "Jingming Zhuo",
      "Yicheng Zou",
      "Xipeng Qiu",
      "Yu Qiao",
      "Dahua Lin"
    ],
    "abstract": "The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has\nsparked discussions on the advent of Artificial General Intelligence (AGI).\nHowever, replicating such advancements in open-source models has been\nchallenging. This paper introduces InternLM2, an open-source LLM that\noutperforms its predecessors in comprehensive evaluations across 6 dimensions\nand 30 benchmarks, long-context modeling, and open-ended subjective evaluations\nthrough innovative pre-training and optimization techniques. The pre-training\nprocess of InternLM2 is meticulously detailed, highlighting the preparation of\ndiverse data types including text, code, and long-context data. InternLM2\nefficiently captures long-term dependencies, initially trained on 4k tokens\nbefore advancing to 32k tokens in pre-training and fine-tuning stages,\nexhibiting remarkable performance on the 200k ``Needle-in-a-Haystack\" test.\nInternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel\nConditional Online Reinforcement Learning from Human Feedback (COOL RLHF)\nstrategy that addresses conflicting human preferences and reward hacking. By\nreleasing InternLM2 models in different training stages and model sizes, we\nprovide the community with insights into the model's evolution.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.17297v1",
    "published_date": "2024-03-26 00:53:24 UTC",
    "updated_date": "2024-03-26 00:53:24 UTC"
  }
]
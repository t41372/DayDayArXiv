{
  "date": "2024-02-28",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-02-28 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 更新了 113 篇论文，主要聚焦于 AI 和大型语言模型（LLMs）的应用、创新及挑战，包括 LLM 在对话系统、游戏和代理智能中的潜力，以及强化学习和多模态模型的进展，其中 “Position Paper: Agent AI Towards a Holistic Intelligence” 和 “Large Language Models and Games: A Survey and Roadmap” 等文章令人印象深刻，涉及知名学者如 Li Fei-Fei 和 Jianfeng Gao。\n\n下面，我挑选并简要讨论部分重要或话题度高的论文，先从 LLM 相关的高影响力主题入手，然后过渡到强化学习、机器人和多模态模型，最后快速掠过其他领域。重点保留核心学术术语，并控制篇幅。\n\n### LLM 和 AI 智能\n- **代理 AI 走向整体智能 | Position Paper: Agent AI Towards a Holistic Intelligence**  \n  这篇论文由多名知名学者（如 Li Fei-Fei 和 Jianfeng Gao）合作，提出 Agent AI 的概念，将大型语言模型整合到具身智能系统中。主要贡献是通过多模态交互实现更全面的 AI 认知和决策，发现这种方法能提升 AI 在机器人、游戏和医疗等领域的表现，为未来 AI 研究提供跨学科路线图。\n\n- **大型语言模型在游戏中的调查与路线图 | Large Language Models and Games: A Survey and Roadmap**  \n  论文对 LLM 在游戏领域的应用进行全面综述，主要发现 LLM 可以处理游戏中的多模态交互和决策，支持从角色扮演到策略生成的任务。贡献在于分析 LLM 的潜力与挑战，并提出未来研究方向，如提升 LLM 在动态游戏环境中的鲁棒性。\n\n- **语言模型代表自我与他人的信念 | Language Models Represent Beliefs of Self and Others**  \n  这篇工作探索 LLM 在理论思维（Theory of Mind）中的表现，通过神经激活解码发现 LLM 可以内部表示代理信念。主要贡献是设计实验证明 LLM 在社交推理任务中的有效性，并揭示其在信念建模中的局限性。\n\n- **超越自然语言：LLM 利用替代格式增强推理和通信 | Beyond Natural Language: LLMs Leveraging Alternative Formats**  \n  论文提出 LLM 使用非自然语言格式（如代码或逻辑表达式）来提升推理效率。主要发现是允许 LLM 自主选择格式可提高任务性能，同时减少令牌使用，贡献在于提供一个多模态框架，支持 LLM 在复杂任务中的泛化。\n\n- **大型语言模型作为进化策略 | Large Language Models As Evolution Strategies**  \n  这篇论文将 LLM 应用于黑盒优化，视其为进化策略。主要贡献是设计一种基于 LLM 的优化框架，能在合成任务和神经进化中提升样本效率，发现大步长学习率可加速收敛。\n\n### 多模态和对话系统\n- **LLM 的最新进展调查：多轮对话系统 | A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems**  \n  论文综述 LLM 在多轮对话系统中的进展，涵盖任务导向和开放域对话。主要发现是 LLM 通过上下文建模提升对话连贯性，贡献在于分类现有方法并指出未来方向，如处理长序列和多语言支持。\n\n- **从摘要到行动：增强 LLM 的多目标任务处理 | From Summary to Action: Enhancing Large Language Models for Complex Tasks with Open World APIs**  \n  工作提出一个 LLM 框架，用于处理外部 API 的复杂任务。主要贡献是设计一个“从摘要到行动”管道，实现任务分解和 API 调用，发现这种方法能提升 LLM 在零样本任务中的鲁棒性。\n\n- **信息细化训练：LLM 用于检索增强生成 | Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation**  \n  论文优化 LLM 在检索增强生成中的性能，主要发现无监督训练能提升信息整合效率。贡献在于提出一种细化机制，提高 LLM 在问答和对话任务的准确性。\n\n### 强化学习和机器人\n- **通过语言引导状态抽象的学习 | Learning with Language-Guided State Abstractions**  \n  作者包括 Jacob Andreas 和 Thomas L. Griffiths，这篇论文提出 LGA 框架，使用语言模型自动构建状态表示。主要贡献是提升模仿学习在机器人任务中的泛化性，发现 LGA 生成的抽象状态能改善环境噪声下的鲁棒性。\n\n- **想象、初始化和探索：多代理强化学习中的有效探索 | Imagine, Initialize, and Explore: An Effective Exploration Method in Multi-Agent Reinforcement Learning**  \n  论文设计 IIE 方法，用于多代理强化学习的环境探索。主要发现是使用 transformer 模型模拟关键状态，能显著提高样本效率，贡献在于解决长时域任务的探索挑战。\n\n- **容错神经控制屏障函数：用于传感器故障下的机器人系统 | Fault Tolerant Neural Control Barrier Functions for Robotic Systems under Sensor Faults and Attacks**  \n  这篇工作开发 FT-NCBF 方法，确保机器人系统在传感器故障下的安全性。主要贡献是通过数据驱动学习构建控制屏障函数，发现该方法在自主机器人和航天任务中有效提升安全性。\n\n### 其他领域快速掠过\n其他论文涉及翻译评估、医学 AI 和量子计算等，但影响力较小，仅快速提及：\n- **微调机器翻译指标在未见领域挣扎 | Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains**  \n  论文构建生物医学数据集，评估微调指标的鲁棒性。主要发现微调指标在领域转移时性能下降。\n\n- **人工智能与糖尿病：通过视网膜的视角 | Artificial Intelligence and Diabetes Mellitus: An Inside Look Through the Retina**  \n  工作综述 AI 在糖尿病并发症诊断中的应用，主要贡献是使用视网膜图像预测心血管风险。\n\n- **符号回归的多模态信息融合 | MMSR: Symbolic Regression is a Multi-Modal Information Fusion Task**  \n  论文将符号回归视为多模态任务，提出 MMSR 方法。主要发现融合文本和数据模态能提升回归性能。\n\n总体而言，今天的论文突显了 AI 领域的快速演进，LLM 在智能代理和对话中的应用特别值得关注。未来，探索 LLM 的安全性和多模态融合将是关键方向。更多细节可查阅 arXiv！",
  "papers": [
    {
      "arxiv_id": "2402.18759v2",
      "title": "Learning with Language-Guided State Abstractions",
      "title_zh": "基于语言引导的状态抽象学习",
      "authors": [
        "Andi Peng",
        "Ilia Sucholutsky",
        "Belinda Z. Li",
        "Theodore R. Sumers",
        "Thomas L. Griffiths",
        "Jacob Andreas",
        "Julie A. Shah"
      ],
      "abstract": "We describe a framework for using natural language to design state\nabstractions for imitation learning. Generalizable policy learning in\nhigh-dimensional observation spaces is facilitated by well-designed state\nrepresentations, which can surface important features of an environment and\nhide irrelevant ones. These state representations are typically manually\nspecified, or derived from other labor-intensive labeling procedures. Our\nmethod, LGA (language-guided abstraction), uses a combination of natural\nlanguage supervision and background knowledge from language models (LMs) to\nautomatically build state representations tailored to unseen tasks. In LGA, a\nuser first provides a (possibly incomplete) description of a target task in\nnatural language; next, a pre-trained LM translates this task description into\na state abstraction function that masks out irrelevant features; finally, an\nimitation policy is trained using a small number of demonstrations and\nLGA-generated abstract states. Experiments on simulated robotic tasks show that\nLGA yields state abstractions similar to those designed by humans, but in a\nfraction of the time, and that these abstractions improve generalization and\nrobustness in the presence of spurious correlations and ambiguous\nspecifications. We illustrate the utility of the learned abstractions on mobile\nmanipulation tasks with a Spot robot.",
      "tldr_zh": "本研究提出了一种名为 LGA (Language-Guided Abstraction) 的框架，利用自然语言指导来自动设计状态抽象，从而提升高维观察空间中的模仿学习(imitation learning)。在 LGA 中，用户通过自然语言描述任务，预训练的语言模型(LMs) 将其转化为状态抽象函数，屏蔽无关特征，并使用少量演示训练模仿策略。实验结果显示，LGA 生成的状态抽象类似于人工设计，但耗时更少，能够显著提高策略的泛化和鲁棒性，尤其在虚假相关(spurious correlations)和模糊规范(ambiguous specifications)下，并在 Spot 机器人的移动操作任务中证明了其实际效用。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "ICLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.18759v2",
      "published_date": "2024-02-28 23:57:04 UTC",
      "updated_date": "2024-03-06 15:53:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:47:41.398582"
    },
    {
      "arxiv_id": "2402.18747v2",
      "title": "Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains",
      "title_zh": "翻译失败",
      "authors": [
        "Vilém Zouhar",
        "Shuoyang Ding",
        "Anna Currey",
        "Tatyana Badeka",
        "Jenyuan Wang",
        "Brian Thompson"
      ],
      "abstract": "We introduce a new, extensive multidimensional quality metrics (MQM)\nannotated dataset covering 11 language pairs in the biomedical domain. We use\nthis dataset to investigate whether machine translation (MT) metrics which are\nfine-tuned on human-generated MT quality judgements are robust to domain shifts\nbetween training and inference. We find that fine-tuned metrics exhibit a\nsubstantial performance drop in the unseen domain scenario relative to metrics\nthat rely on the surface form, as well as pre-trained metrics which are not\nfine-tuned on MT quality judgments.",
      "tldr_zh": "这篇论文引入了一个新的多维质量指标 (MQM) 标注数据集，涵盖 11 个生物医学领域的语言对，用于评估机器翻译 (MT) 指标的鲁棒性。研究者调查了微调于人类 MT 质量判断的指标，在训练和推理之间出现领域转移时是否能保持性能。结果发现，这些微调指标在未见领域表现出显著性能下降，与依赖表面形式的指标以及未微调的预训练指标相比，鲁棒性较差。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.18747v2",
      "published_date": "2024-02-28 23:01:24 UTC",
      "updated_date": "2024-06-04 04:14:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:47:52.680791"
    },
    {
      "arxiv_id": "2403.00032v1",
      "title": "Time to Cite: Modeling Citation Networks using the Dynamic Impact Single-Event Embedding Model",
      "title_zh": "翻译失败",
      "authors": [
        "Nikolaos Nakis",
        "Abdulkadir Celikkanat",
        "Louis Boucherie",
        "Sune Lehmann",
        "Morten Mørup"
      ],
      "abstract": "Understanding the structure and dynamics of scientific research, i.e., the\nscience of science (SciSci), has become an important area of research in order\nto address imminent questions including how scholars interact to advance\nscience, how disciplines are related and evolve, and how research impact can be\nquantified and predicted. Central to the study of SciSci has been the analysis\nof citation networks. Here, two prominent modeling methodologies have been\nemployed: one is to assess the citation impact dynamics of papers using\nparametric distributions, and the other is to embed the citation networks in a\nlatent space optimal for characterizing the static relations between papers in\nterms of their citations. Interestingly, citation networks are a prominent\nexample of single-event dynamic networks, i.e., networks for which each dyad\nonly has a single event (i.e., the point in time of citation). We presently\npropose a novel likelihood function for the characterization of such\nsingle-event networks. Using this likelihood, we propose the Dynamic Impact\nSingle-Event Embedding model (DISEE). The \\textsc{\\modelabbrev} model\ncharacterizes the scientific interactions in terms of a latent distance model\nin which random effects account for citation heterogeneity while the\ntime-varying impact is characterized using existing parametric representations\nfor assessment of dynamic impact. We highlight the proposed approach on several\nreal citation networks finding that the DISEE well reconciles static latent\ndistance network embedding approaches with classical dynamic impact\nassessments.",
      "tldr_zh": "本研究探讨科学研究的结构和动态（SciSci），特别针对引用网络的建模，以量化并预测研究影响。作者提出了一种新模型Dynamic Impact Single-Event Embedding Model (DISEE)，它基于新的似然函数，结合潜在距离模型（latent distance model）和随机效应来处理单事件动态网络（single-event dynamic networks），并通过参数表示评估时间变化的影响。实验结果显示，DISEE 在多个真实引用网络上有效整合了静态嵌入方法和动态影响评估，提升了对科学互动的表征能力。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.DL"
      ],
      "primary_category": "cs.SI",
      "comment": "Accepted for AISTATS 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.00032v1",
      "published_date": "2024-02-28 22:59:26 UTC",
      "updated_date": "2024-02-28 22:59:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:48:05.509691"
    },
    {
      "arxiv_id": "2402.18743v1",
      "title": "A revision on Multi-Criteria Decision Making methods for Multi-UAV Mission Planning Support",
      "title_zh": "翻译失败",
      "authors": [
        "Cristian Ramirez-Atencia",
        "Victor Rodriguez-Fernandez",
        "David Camacho"
      ],
      "abstract": "Over the last decade, Unmanned Aerial Vehicles (UAVs) have been extensively\nused in many commercial applications due to their manageability and risk\navoidance. One of the main problems considered is the Mission Planning for\nmultiple UAVs, where a solution plan must be found satisfying the different\nconstraints of the problem. This problem has multiple variables that must be\noptimized simultaneously, such as the makespan, the cost of the mission or the\nrisk. Therefore, the problem has a lot of possible optimal solutions, and the\noperator must select the final solution to be executed among them. In order to\nreduce the workload of the operator in this decision process, a Decision\nSupport System (DSS) becomes necessary. In this work, a DSS consisting of\nranking and filtering systems, which order and reduce the optimal solutions,\nhas been designed. With regard to the ranking system, a wide range of\nMulti-Criteria Decision Making (MCDM) methods, including some fuzzy MCDM, are\ncompared on a multi-UAV mission planning scenario, in order to study which\nmethod could fit better in a multi-UAV decision support system. Expert\noperators have evaluated the solutions returned, and the results show, on the\none hand, that fuzzy methods generally achieve better average scores, and on\nthe other, that all of the tested methods perform better when the preferences\nof the operators are biased towards a specific variable, and worse when their\npreferences are balanced. For the filtering system, a similarity function based\non the proximity of the solutions has been designed, and on top of that, a\nthreshold is tuned empirically to decide how to filter solutions without losing\nmuch of the hypervolume of the space of solutions.",
      "tldr_zh": "本研究回顾了多标准决策方法 (MCDM) 在多无人机 (UAV) 任务规划支持中的应用，旨在解决优化 makespan、成本和风险等多个变量的决策挑战。作者设计了一个决策支持系统 (DSS)，包括排名系统（比较多种 MCDM 方法，如模糊 MCDM）和过滤系统（基于相似性函数和经验阈值来减少解决方案空间）。实验结果显示，模糊 MCDM 方法在专家评估中获得更高平均分数，且所有方法在操作员偏好偏向特定变量时表现更好，而在偏好平衡时效果较差，从而为多 UAV 任务规划提供了更高效的决策框架。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint submitted and acepted in Expert Systems with Applications",
      "pdf_url": "http://arxiv.org/pdf/2402.18743v1",
      "published_date": "2024-02-28 22:54:08 UTC",
      "updated_date": "2024-02-28 22:54:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:48:17.239729"
    },
    {
      "arxiv_id": "2402.18732v1",
      "title": "GAIA: Categorical Foundations of Generative AI",
      "title_zh": "GAIA：生成式人工智能的范畴论基础",
      "authors": [
        "Sridhar Mahadevan"
      ],
      "abstract": "In this paper, we propose GAIA, a generative AI architecture based on\ncategory theory. GAIA is based on a hierarchical model where modules are\norganized as a simplicial complex. Each simplicial complex updates its internal\nparameters biased on information it receives from its superior simplices and in\nturn relays updates to its subordinate sub-simplices. Parameter updates are\nformulated in terms of lifting diagrams over simplicial sets, where inner and\nouter horn extensions correspond to different types of learning problems.\nBackpropagation is modeled as an endofunctor over the category of parameters,\nleading to a coalgebraic formulation of deep learning.",
      "tldr_zh": "本论文提出 GAIA，一种基于 category theory 的生成 AI 架构，将模块组织为分层的 simplicial complex。每个模块根据从上级单纯形接收的信息更新内部参数，并通过 lifting diagrams 公式化参数更新，其中 inner 和 outer horn extensions 对应不同的学习问题。反向传播被建模为参数范畴上的 endofunctor，导致深度学习的 coalgebraic 公式化，为生成 AI 提供新的理论基础。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "65 pages. arXiv admin note: text overlap with arXiv:2212.08981",
      "pdf_url": "http://arxiv.org/pdf/2402.18732v1",
      "published_date": "2024-02-28 22:25:02 UTC",
      "updated_date": "2024-02-28 22:25:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:48:29.290793"
    },
    {
      "arxiv_id": "2402.18726v1",
      "title": "Unveiling Privacy, Memorization, and Input Curvature Links",
      "title_zh": "揭示隐私、记忆化和输入曲率联系",
      "authors": [
        "Deepak Ravikumar",
        "Efstathia Soufleri",
        "Abolfazl Hashemi",
        "Kaushik Roy"
      ],
      "abstract": "Deep Neural Nets (DNNs) have become a pervasive tool for solving many\nemerging problems. However, they tend to overfit to and memorize the training\nset. Memorization is of keen interest since it is closely related to several\nconcepts such as generalization, noisy learning, and privacy. To study\nmemorization, Feldman (2019) proposed a formal score, however its computational\nrequirements limit its practical use. Recent research has shown empirical\nevidence linking input loss curvature (measured by the trace of the loss\nHessian w.r.t inputs) and memorization. It was shown to be ~3 orders of\nmagnitude more efficient than calculating the memorization score. However,\nthere is a lack of theoretical understanding linking memorization with input\nloss curvature. In this paper, we not only investigate this connection but also\nextend our analysis to establish theoretical links between differential\nprivacy, memorization, and input loss curvature. First, we derive an upper\nbound on memorization characterized by both differential privacy and input loss\ncurvature. Second, we present a novel insight showing that input loss curvature\nis upper-bounded by the differential privacy parameter. Our theoretical\nfindings are further empirically validated using deep models on CIFAR and\nImageNet datasets, showing a strong correlation between our theoretical\npredictions and results observed in practice.",
      "tldr_zh": "本研究探讨了深度神经网络(DNNs)中记忆化现象及其与差分隐私和输入损失曲率的关系，指出记忆化会导致过拟合并影响泛化。论文首次推导了记忆化的上界，由差分隐私参数和输入损失曲率表征，并证明输入损失曲率可被差分隐私参数上界。实验结果在CIFAR和ImageNet数据集上验证了这些理论联系，显示出强相关性，并证明该方法比传统记忆化分数计算更高效。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18726v1",
      "published_date": "2024-02-28 22:02:10 UTC",
      "updated_date": "2024-02-28 22:02:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:48:42.701282"
    },
    {
      "arxiv_id": "2402.18724v1",
      "title": "Learning Associative Memories with Gradient Descent",
      "title_zh": "利用梯度下降学习关联记忆",
      "authors": [
        "Vivien Cabannes",
        "Berfin Simsek",
        "Alberto Bietti"
      ],
      "abstract": "This work focuses on the training dynamics of one associative memory module\nstoring outer products of token embeddings. We reduce this problem to the study\nof a system of particles, which interact according to properties of the data\ndistribution and correlations between embeddings. Through theory and\nexperiments, we provide several insights. In overparameterized regimes, we\nobtain logarithmic growth of the ``classification margins.'' Yet, we show that\nimbalance in token frequencies and memory interferences due to correlated\nembeddings lead to oscillatory transitory regimes. The oscillations are more\npronounced with large step sizes, which can create benign loss spikes, although\nthese learning rates speed up the dynamics and accelerate the asymptotic\nconvergence. In underparameterized regimes, we illustrate how the cross-entropy\nloss can lead to suboptimal memorization schemes. Finally, we assess the\nvalidity of our findings on small Transformer models.",
      "tldr_zh": "该论文探讨了使用 Gradient Descent 训练关联记忆模块的过程，将其简化为一个基于数据分布和嵌入相关性的粒子系统分析。通过理论和实验，研究发现：在过参数化场景下，Classification Margins 呈现对数增长，但 token 频率不平衡和嵌入相关性会导致 transitory oscillations，大步长虽可加速收敛并促进渐近收敛，却可能引发 benign loss spikes。在欠参数化场景下，Cross-Entropy Loss 可能导致次优的记忆方案；最终，这些发现通过小 Transformer 模型得到验证。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18724v1",
      "published_date": "2024-02-28 21:47:30 UTC",
      "updated_date": "2024-02-28 21:47:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:48:53.883672"
    },
    {
      "arxiv_id": "2402.18715v1",
      "title": "Commonsense Ontology Micropatterns",
      "title_zh": "翻译失败",
      "authors": [
        "Andrew Eells",
        "Brandon Dave",
        "Pascal Hitzler",
        "Cogan Shimizu"
      ],
      "abstract": "The previously introduced Modular Ontology Modeling methodology (MOMo)\nattempts to mimic the human analogical process by using modular patterns to\nassemble more complex concepts. To support this, MOMo organizes organizes\nontology design patterns into design libraries, which are programmatically\nqueryable, to support accelerated ontology development, for both human and\nautomated processes. However, a major bottleneck to large-scale deployment of\nMOMo is the (to-date) limited availability of ready-to-use ontology design\npatterns. At the same time, Large Language Models have quickly become a source\nof common knowledge and, in some cases, replacing search engines for questions.\nIn this paper, we thus present a collection of 104 ontology design patterns\nrepresenting often occurring nouns, curated from the common-sense knowledge\navailable in LLMs, organized into a fully-annotated modular ontology design\nlibrary ready for use with MOMo.",
      "tldr_zh": "本研究针对 Modular Ontology Modeling (MOMo) 方法中本体设计模式不足的问题，利用 Large Language Models (LLMs) 的常识知识，构建了一个包含 104 个常用名词本体设计模式的集合。MOMo 通过模块化模式模仿人类类比过程，并将这些模式组织成可编程查询的设计库，以加速本体开发，支持人类和自动化流程。该工作解决了 MOMo 大规模部署的瓶颈，提供现成的、完全标注的模块化本体设计库，增强了常识知识在本体工程中的应用。",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18715v1",
      "published_date": "2024-02-28 21:23:54 UTC",
      "updated_date": "2024-02-28 21:23:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:49:05.231596"
    },
    {
      "arxiv_id": "2402.18700v2",
      "title": "Learning to Compress Prompt in Natural Language Formats",
      "title_zh": "翻译失败",
      "authors": [
        "Yu-Neng Chuang",
        "Tianwei Xing",
        "Chia-Yuan Chang",
        "Zirui Liu",
        "Xun Chen",
        "Xia Hu"
      ],
      "abstract": "Large language models (LLMs) are great at processing multiple natural\nlanguage processing tasks, but their abilities are constrained by inferior\nperformance with long context, slow inference speed, and the high cost of\ncomputing the results. Deploying LLMs with precise and informative context\nhelps users process large-scale datasets more effectively and cost-efficiently.\nExisting works rely on compressing long prompt contexts into soft prompts.\nHowever, soft prompt compression encounters limitations in transferability\nacross different LLMs, especially API-based LLMs. To this end, this work aims\nto compress lengthy prompts in the form of natural language with LLM\ntransferability. This poses two challenges: (i) Natural Language (NL) prompts\nare incompatible with back-propagation, and (ii) NL prompts lack flexibility in\nimposing length constraints. In this work, we propose a Natural Language Prompt\nEncapsulation (Nano-Capsulator) framework compressing original prompts into NL\nformatted Capsule Prompt while maintaining the prompt utility and\ntransferability. Specifically, to tackle the first challenge, the\nNano-Capsulator is optimized by a reward function that interacts with the\nproposed semantics preserving loss. To address the second question, the\nNano-Capsulator is optimized by a reward function featuring length constraints.\nExperimental results demonstrate that the Capsule Prompt can reduce 81.4% of\nthe original length, decrease inference latency up to 4.5x, and save 80.1% of\nbudget overheads while providing transferability across diverse LLMs and\ndifferent datasets.",
      "tldr_zh": "本研究针对大型语言模型(LLMs)处理长提示时存在的性能低下、推理速度慢和高计算成本问题，提出Natural Language Prompt Encapsulation (Nano-Capsulator)框架，将原始提示压缩成自然语言格式的Capsule Prompt，以提升提示的可转移性。框架通过奖励函数结合语义保留损失优化提示内容，并加入长度约束来处理压缩挑战。实验结果显示，Capsule Prompt能减少81.4%的提示长度、降低推理延迟高达4.5倍、节省80.1%的预算开销，同时在不同LLMs和数据集间保持高效转移性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18700v2",
      "published_date": "2024-02-28 20:41:21 UTC",
      "updated_date": "2024-04-02 02:38:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:49:18.148649"
    },
    {
      "arxiv_id": "2403.00835v4",
      "title": "CLLMs: Consistency Large Language Models",
      "title_zh": "CLLMs：一致性大型语言模型",
      "authors": [
        "Siqi Kou",
        "Lanxiang Hu",
        "Zhezhi He",
        "Zhijie Deng",
        "Hao Zhang"
      ],
      "abstract": "Parallel decoding methods such as Jacobi decoding show promise for more\nefficient LLM inference as it breaks the sequential nature of the LLM decoding\nprocess and transforms it into parallelizable computation. However, in\npractice, it achieves little speedup compared to traditional autoregressive\n(AR) decoding, primarily because Jacobi decoding seldom accurately predicts\nmore than one token in a single fixed-point iteration step. To address this, we\ndevelop a new approach aimed at realizing fast convergence from any state to\nthe fixed point on a Jacobi trajectory. This is accomplished by refining the\ntarget LLM to consistently predict the fixed point given any state as input.\nExtensive experiments demonstrate the effectiveness of our method, showing\n2.4$\\times$ to 3.4$\\times$ improvements in generation speed while preserving\ngeneration quality across both domain-specific and open-domain benchmarks.",
      "tldr_zh": "本论文提出CLLMs（Consistency Large Language Models），一种针对Jacobi decoding等并行解码方法的改进框架，以提升LLM推理效率。传统Jacobi decoding因在单个固定点迭代步骤中难以准确预测多个token，而无法显著超越autoregressive (AR) decoding的速度；为此，作者通过微调目标LLM，使其从任何输入状态快速收敛到Jacobi轨迹上的固定点，从而实现一致性预测。实验结果显示，该方法在生成速度上实现了2.4×到3.4×的提升，同时在领域特定和开放域基准上保持了生成质量。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "In the proceedings of the 41st International Conference on Machine\n  Learning (ICML) 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.00835v4",
      "published_date": "2024-02-28 20:17:04 UTC",
      "updated_date": "2024-06-13 08:41:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:49:29.413248"
    },
    {
      "arxiv_id": "2403.00030v2",
      "title": "GraphPub: Generation of Differential Privacy Graph with High Availability",
      "title_zh": "翻译失败",
      "authors": [
        "Wanghan Xu",
        "Bin Shi",
        "Ao Liu",
        "Jiqiang Zhang",
        "Bo Dong"
      ],
      "abstract": "In recent years, with the rapid development of graph neural networks (GNN),\nmore and more graph datasets have been published for GNN tasks. However, when\nan upstream data owner publishes graph data, there are often many privacy\nconcerns, because many real-world graph data contain sensitive information like\nperson's friend list. Differential privacy (DP) is a common method to protect\nprivacy, but due to the complex topological structure of graph data, applying\nDP on graphs often affects the message passing and aggregation of GNN models,\nleading to a decrease in model accuracy. In this paper, we propose a novel\ngraph edge protection framework, graph publisher (GraphPub), which can protect\ngraph topology while ensuring that the availability of data is basically\nunchanged. Through reverse learning and the encoder-decoder mechanism, we\nsearch for some false edges that do not have a large negative impact on the\naggregation of node features, and use them to replace some real edges. The\nmodified graph will be published, which is difficult to distinguish between\nreal and false data. Sufficient experiments prove that our framework achieves\nmodel accuracy close to the original graph with an extremely low privacy\nbudget.",
      "tldr_zh": "该论文提出GraphPub框架，用于生成差分隐私（DP）图数据，同时保持高可用性，以解决图神经网络（GNN）任务中隐私保护与模型准确性之间的冲突。GraphPub通过逆向学习和编码器-解码器机制，识别对节点特征聚合影响较小的假边，并用这些假边替换真实边，从而生成难以区分的修改图。实验结果显示，该框架在极低的隐私预算下，实现与原始图接近的模型准确性，为隐私敏感的图数据发布提供了高效解决方案。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00030v2",
      "published_date": "2024-02-28 20:02:55 UTC",
      "updated_date": "2024-03-05 05:34:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:49:41.507131"
    },
    {
      "arxiv_id": "2402.18679v4",
      "title": "Data Interpreter: An LLM Agent For Data Science",
      "title_zh": "翻译失败",
      "authors": [
        "Sirui Hong",
        "Yizhang Lin",
        "Bang Liu",
        "Bangbang Liu",
        "Binhao Wu",
        "Ceyao Zhang",
        "Chenxing Wei",
        "Danyang Li",
        "Jiaqi Chen",
        "Jiayi Zhang",
        "Jinlin Wang",
        "Li Zhang",
        "Lingyao Zhang",
        "Min Yang",
        "Mingchen Zhuge",
        "Taicheng Guo",
        "Tuo Zhou",
        "Wei Tao",
        "Xiangru Tang",
        "Xiangtao Lu",
        "Xiawu Zheng",
        "Xinbing Liang",
        "Yaying Fei",
        "Yuheng Cheng",
        "Zhibin Gou",
        "Zongze Xu",
        "Chenglin Wu"
      ],
      "abstract": "Large Language Model (LLM)-based agents have shown effectiveness across many\napplications. However, their use in data science scenarios requiring solving\nlong-term interconnected tasks, dynamic data adjustments and domain expertise\nremains challenging. Previous approaches primarily focus on individual tasks,\nmaking it difficult to assess the complete data science workflow. Moreover,\nthey struggle to handle real-time changes in intermediate data and fail to\nadapt dynamically to evolving task dependencies inherent to data science\nproblems. In this paper, we present Data Interpreter, an LLM-based agent\ndesigned to automatically solve various data science problems end-to-end. Our\nData Interpreter incorporates two key modules: 1) Hierarchical Graph Modeling,\nwhich breaks down complex problems into manageable subproblems, enabling\ndynamic node generation and graph optimization; and 2) Programmable Node\nGeneration, a technique that refines and verifies each subproblem to\niteratively improve code generation results and robustness. Extensive\nexperiments consistently demonstrate the superiority of Data Interpreter. On\nInfiAgent-DABench, it achieves a 25% performance boost, raising accuracy from\n75.9% to 94.9%. For machine learning and open-ended tasks, it improves\nperformance from 88% to 95%, and from 60% to 97%, respectively. Moreover, on\nthe MATH dataset, Data Interpreter achieves remarkable performance with a 26%\nimprovement compared to state-of-the-art baselines. The code is available at\nhttps://github.com/geekan/MetaGPT.",
      "tldr_zh": "该研究提出 Data Interpreter，一种基于 LLM（Large Language Model）的代理，旨在自动解决数据科学问题，包括长期互连任务、动态数据调整和领域专业知识的挑战。核心方法包括 Hierarchical Graph Modeling（将复杂问题分解为子问题，支持动态节点生成和图优化）和 Programmable Node Generation（细化验证子问题以迭代改进代码生成和鲁棒性）。实验结果显示，Data Interpreter 在 InfiAgent-DABench 上准确率从 75.9% 提升至 94.9%（25% 提升），在机器学习任务上从 88% 提高到 95%，开放任务从 60% 提高到 97%，并在 MATH 数据集上比最先进基线提升 26%。这为端到端的数据科学工作流提供了高效解决方案。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18679v4",
      "published_date": "2024-02-28 19:49:55 UTC",
      "updated_date": "2024-10-15 15:52:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:49:56.325235"
    },
    {
      "arxiv_id": "2402.18677v1",
      "title": "Fault Tolerant Neural Control Barrier Functions for Robotic Systems under Sensor Faults and Attacks",
      "title_zh": "传感器故障和攻击下的机器人系统容错神经控制屏障函数",
      "authors": [
        "Hongchao Zhang",
        "Luyao Niu",
        "Andrew Clark",
        "Radha Poovendran"
      ],
      "abstract": "Safety is a fundamental requirement of many robotic systems. Control barrier\nfunction (CBF)-based approaches have been proposed to guarantee the safety of\nrobotic systems. However, the effectiveness of these approaches highly relies\non the choice of CBFs. Inspired by the universal approximation power of neural\nnetworks, there is a growing trend toward representing CBFs using neural\nnetworks, leading to the notion of neural CBFs (NCBFs). Current NCBFs, however,\nare trained and deployed in benign environments, making them ineffective for\nscenarios where robotic systems experience sensor faults and attacks. In this\npaper, we study safety-critical control synthesis for robotic systems under\nsensor faults and attacks. Our main contribution is the development and\nsynthesis of a new class of CBFs that we term fault tolerant neural control\nbarrier function (FT-NCBF). We derive the necessary and sufficient conditions\nfor FT-NCBFs to guarantee safety, and develop a data-driven method to learn\nFT-NCBFs by minimizing a loss function constructed using the derived\nconditions. Using the learned FT-NCBF, we synthesize a control input and\nformally prove the safety guarantee provided by our approach. We demonstrate\nour proposed approach using two case studies: obstacle avoidance problem for an\nautonomous mobile robot and spacecraft rendezvous problem, with code available\nvia https://github.com/HongchaoZhang-HZ/FTNCBF.",
      "tldr_zh": "本研究针对机器人系统在传感器故障和攻击下的安全控制问题，提出了一种新的容错神经控制屏障函数（FT-NCBF），以提升现有神经控制屏障函数（NCBFs）的鲁棒性。作者推导了FT-NCBF保证安全的必要和充分条件，并开发了基于数据驱动的方法，通过最小化损失函数来学习这些函数，从而合成安全的控制输入。实验验证显示，该方法在自主移动机器人的障碍避免和航天器交会等案例中有效确保系统安全，并提供了开源代码支持。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18677v1",
      "published_date": "2024-02-28 19:44:19 UTC",
      "updated_date": "2024-02-28 19:44:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:50:06.459239"
    },
    {
      "arxiv_id": "2402.18673v2",
      "title": "Trends, Applications, and Challenges in Human Attention Modelling",
      "title_zh": "人类注意力建模的趋势、应用和挑战",
      "authors": [
        "Giuseppe Cartella",
        "Marcella Cornia",
        "Vittorio Cuculo",
        "Alessandro D'Amelio",
        "Dario Zanca",
        "Giuseppe Boccignone",
        "Rita Cucchiara"
      ],
      "abstract": "Human attention modelling has proven, in recent years, to be particularly\nuseful not only for understanding the cognitive processes underlying visual\nexploration, but also for providing support to artificial intelligence models\nthat aim to solve problems in various domains, including image and video\nprocessing, vision-and-language applications, and language modelling. This\nsurvey offers a reasoned overview of recent efforts to integrate human\nattention mechanisms into contemporary deep learning models and discusses\nfuture research directions and challenges. For a comprehensive overview on the\nongoing research refer to our dedicated repository available at\nhttps://github.com/aimagelab/awesome-human-visual-attention.",
      "tldr_zh": "本调查论文概述了人类注意力建模(Human Attention Modelling)的最新趋势和应用，包括理解视觉探索的认知过程，以及支持人工智能(Artificial Intelligence)模型在图像和视频处理、视觉语言应用和语言建模等领域的应用。论文对将人类注意力机制整合到当代深度学习模型(Deep Learning Models)中的研究努力进行了全面总结，并指出了未来的研究方向和挑战。通过提供一个专用仓库（https://github.com/aimagelab/awesome-human-visual-attention），论文旨在促进该领域的持续发展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at IJCAI 2024 Survey Track",
      "pdf_url": "http://arxiv.org/pdf/2402.18673v2",
      "published_date": "2024-02-28 19:35:30 UTC",
      "updated_date": "2024-04-22 17:54:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:50:17.981744"
    },
    {
      "arxiv_id": "2403.05579v1",
      "title": "Cultural Bias in Explainable AI Research: A Systematic Analysis",
      "title_zh": "可解释 AI 研究中的文化偏",
      "authors": [
        "Uwe Peters",
        "Mary Carman"
      ],
      "abstract": "For synergistic interactions between humans and artificial intelligence (AI)\nsystems, AI outputs often need to be explainable to people. Explainable AI\n(XAI) systems are commonly tested in human user studies. However, whether XAI\nresearchers consider potential cultural differences in human explanatory needs\nremains unexplored. We highlight psychological research that found significant\ndifferences in human explanations between many people from Western, commonly\nindividualist countries and people from non-Western, often collectivist\ncountries. We argue that XAI research currently overlooks these variations and\nthat many popular XAI designs implicitly and problematically assume that\nWestern explanatory needs are shared cross-culturally. Additionally, we\nsystematically reviewed over 200 XAI user studies and found that most studies\ndid not consider relevant cultural variations, sampled only Western\npopulations, but drew conclusions about human-XAI interactions more generally.\nWe also analyzed over 30 literature reviews of XAI studies. Most reviews did\nnot mention cultural differences in explanatory needs or flag overly broad\ncross-cultural extrapolations of XAI user study results. Combined, our analyses\nprovide evidence of a cultural bias toward Western populations in XAI research,\nhighlighting an important knowledge gap regarding how culturally diverse users\nmay respond to widely used XAI systems that future work can and should address.",
      "tldr_zh": "这篇论文分析了Explainable AI (XAI)研究中的文化偏见问题，指出XAI系统在人类用户研究中常忽略不同文化（如西方个人主义和非西方集体主义国家）对解释需求的影响。作者通过系统审查超过200篇XAI用户研究和30多个文献综述，发现大多数研究仅采样西方人群，却泛化结论，同时文献综述未充分关注文化差异。总体而言，该研究揭示了XAI领域对西方人群的文化偏见，并强调未来工作应探索文化多样性以改善人类-AI互动。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05579v1",
      "published_date": "2024-02-28 19:30:32 UTC",
      "updated_date": "2024-02-28 19:30:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:50:30.839105"
    },
    {
      "arxiv_id": "2402.18659v5",
      "title": "Large Language Models and Games: A Survey and Roadmap",
      "title_zh": "大型语言模型与游戏：综述与路线图",
      "authors": [
        "Roberto Gallotta",
        "Graham Todd",
        "Marvin Zammit",
        "Sam Earle",
        "Antonios Liapis",
        "Julian Togelius",
        "Georgios N. Yannakakis"
      ],
      "abstract": "Recent years have seen an explosive increase in research on large language\nmodels (LLMs), and accompanying public engagement on the topic. While starting\nas a niche area within natural language processing, LLMs have shown remarkable\npotential across a broad range of applications and domains, including games.\nThis paper surveys the current state of the art across the various applications\nof LLMs in and for games, and identifies the different roles LLMs can take\nwithin a game. Importantly, we discuss underexplored areas and promising\ndirections for future uses of LLMs in games and we reconcile the potential and\nlimitations of LLMs within the games domain. As the first comprehensive survey\nand roadmap at the intersection of LLMs and games, we are hopeful that this\npaper will serve as the basis for groundbreaking research and innovation in\nthis exciting new field.",
      "tldr_zh": "这篇论文对大型语言模型（LLMs）在游戏领域的应用进行了全面调查和路线图概述。作者分析了LLMs在游戏中的各种角色，包括生成内容、增强互动和辅助设计等方面，并讨论了其潜力与局限性。论文突出了未充分探索的领域，如LLMs在游戏中的创新整合，并为未来研究提供了有价值的指导方向，作为该交叉领域的首个系统性总结。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted for publication at the IEEE Transactions on Games (19 pages,\n  6 figures)",
      "pdf_url": "http://arxiv.org/pdf/2402.18659v5",
      "published_date": "2024-02-28 19:09:08 UTC",
      "updated_date": "2024-12-09 14:41:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:50:40.401742"
    },
    {
      "arxiv_id": "2402.18651v1",
      "title": "Quantifying Human Priors over Social and Navigation Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Gecia Bravo-Hermsdorff"
      ],
      "abstract": "Human knowledge is largely implicit and relational -- do we have a friend in\ncommon? can I walk from here to there? In this work, we leverage the\ncombinatorial structure of graphs to quantify human priors over such relational\ndata. Our experiments focus on two domains that have been continuously relevant\nover evolutionary timescales: social interaction and spatial navigation. We\nfind that some features of the inferred priors are remarkably consistent, such\nas the tendency for sparsity as a function of graph size. Other features are\ndomain-specific, such as the propensity for triadic closure in social\ninteractions. More broadly, our work demonstrates how nonclassical statistical\nanalysis of indirect behavioral experiments can be used to efficiently model\nlatent biases in the data.",
      "tldr_zh": "这篇论文探讨了如何量化人类对社交和导航网络的先验知识，利用图的组合结构分析关系数据。研究聚焦于社交互动和空间导航两个进化相关的领域，通过间接行为实验进行非经典统计分析。结果显示，先验中存在一致特征，如图大小相关的稀疏性，以及领域特定特征，例如社交互动中的 triadic closure。更广泛地，这方法为高效建模数据中的潜在偏差提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI",
        "physics.soc-ph",
        "q-bio.NC",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "comment": "Published on Proceedings of the 40th International Conference on\n  Machine Learning (ICML), PMLR 202:3063-3105, 2023",
      "pdf_url": "http://arxiv.org/pdf/2402.18651v1",
      "published_date": "2024-02-28 19:00:36 UTC",
      "updated_date": "2024-02-28 19:00:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:50:54.448974"
    },
    {
      "arxiv_id": "2402.18649v1",
      "title": "A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Fangzhou Wu",
        "Ning Zhang",
        "Somesh Jha",
        "Patrick McDaniel",
        "Chaowei Xiao"
      ],
      "abstract": "Large Language Model (LLM) systems are inherently compositional, with\nindividual LLM serving as the core foundation with additional layers of objects\nsuch as plugins, sandbox, and so on. Along with the great potential, there are\nalso increasing concerns over the security of such probabilistic intelligent\nsystems. However, existing studies on LLM security often focus on individual\nLLM, but without examining the ecosystem through the lens of LLM systems with\nother objects (e.g., Frontend, Webtool, Sandbox, and so on). In this paper, we\nsystematically analyze the security of LLM systems, instead of focusing on the\nindividual LLMs. To do so, we build on top of the information flow and\nformulate the security of LLM systems as constraints on the alignment of the\ninformation flow within LLM and between LLM and other objects. Based on this\nconstruction and the unique probabilistic nature of LLM, the attack surface of\nthe LLM system can be decomposed into three key components: (1) multi-layer\nsecurity analysis, (2) analysis of the existence of constraints, and (3)\nanalysis of the robustness of these constraints. To ground this new attack\nsurface, we propose a multi-layer and multi-step approach and apply it to the\nstate-of-art LLM system, OpenAI GPT4. Our investigation exposes several\nsecurity issues, not just within the LLM model itself but also in its\nintegration with other components. We found that although the OpenAI GPT4 has\ndesigned numerous safety constraints to improve its safety features, these\nsafety constraints are still vulnerable to attackers. To further demonstrate\nthe real-world threats of our discovered vulnerabilities, we construct an\nend-to-end attack where an adversary can illicitly acquire the user's chat\nhistory, all without the need to manipulate the user's input or gain direct\naccess to OpenAI GPT4. Our demo is in the link:\nhttps://fzwark.github.io/LLM-System-Attack-Demo/",
      "tldr_zh": "该论文探讨了真实世界LLM系统（Large Language Model系统）的安全问题，强调这些系统不仅包括核心LLM，还涉及其他组件如插件、沙箱等。作者基于信息流（information flow）理论，将LLM系统的安全定义为信息流对齐的约束，并将攻击面分解为多层安全分析、约束存在分析和约束鲁棒性分析三部分。研究应用这一方法到OpenAI GPT-4，揭示了系统集成中的多个安全漏洞，包括安全约束的易受攻击性。最终，作者构建了一个端到端攻击演示，展示了攻击者无需操纵用户输入或直接访问即可窃取聊天历史，突显了LLM系统安全的新时代挑战。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18649v1",
      "published_date": "2024-02-28 19:00:12 UTC",
      "updated_date": "2024-02-28 19:00:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:51:05.667853"
    },
    {
      "arxiv_id": "2402.18571v3",
      "title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards",
      "title_zh": "针对多样化用户偏好的LLM算术控制：多目标奖励下的方向性偏好对齐",
      "authors": [
        "Haoxiang Wang",
        "Yong Lin",
        "Wei Xiong",
        "Rui Yang",
        "Shizhe Diao",
        "Shuang Qiu",
        "Han Zhao",
        "Tong Zhang"
      ],
      "abstract": "Fine-grained control over large language models (LLMs) remains a significant\nchallenge, hindering their adaptability to diverse user needs. While\nReinforcement Learning from Human Feedback (RLHF) shows promise in aligning\nLLMs, its reliance on scalar rewards often limits its ability to capture\ndiverse user preferences in real-world applications. To address this\nlimitation, we introduce the Directional Preference Alignment (DPA) framework.\nUnlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling\nto represent diverse preference profiles. Additionally, DPA models user\npreferences as directions (i.e., unit vectors) in the reward space to achieve\nuser-dependent preference control. Our method involves training a\nmulti-objective reward model and then fine-tuning the LLM with a\npreference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF\nmethod adopted by Llama 2. This method enjoys a better performance trade-off\nacross various reward objectives. In comparison with the scalar-reward RLHF,\nDPA offers users intuitive control over LLM generation: they can arithmetically\nspecify their desired trade-offs (e.g., more helpfulness with less verbosity).\nWe also validate the effectiveness of DPA with real-world alignment experiments\non Mistral-7B. Our method provides straightforward arithmetic control over the\ntrade-off between helpfulness and verbosity while maintaining competitive\nperformance with strong baselines such as Direct Preference Optimization (DPO).",
      "tldr_zh": "这篇论文提出 Directional Preference Alignment (DPA) 框架，用于实现对大型语言模型 (LLMs) 的细粒度控制，以适应多样化的用户偏好。DPA 通过多目标奖励模型将用户偏好建模为奖励空间中的方向（单位向量），并结合 Rejection Sampling Finetuning (RSF) 方法进行微调，从而提供算术方式的偏好权衡，例如增加帮助性同时减少冗长。实验在 Mistral-7B 模型上验证了 DPA 的有效性，它在帮助性和冗长度之间实现了更好的性能平衡，并与 Direct Preference Optimization (DPO) 等基线相比表现出竞争优势。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "The code and model are released at\n  https://github.com/Haoxiang-Wang/directional-preference-alignment",
      "pdf_url": "http://arxiv.org/pdf/2402.18571v3",
      "published_date": "2024-02-28 18:58:25 UTC",
      "updated_date": "2024-03-06 08:07:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:51:19.468934"
    },
    {
      "arxiv_id": "2402.18563v1",
      "title": "Approaching Human-Level Forecasting with Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Danny Halawi",
        "Fred Zhang",
        "Chen Yueh-Han",
        "Jacob Steinhardt"
      ],
      "abstract": "Forecasting future events is important for policy and decision making. In\nthis work, we study whether language models (LMs) can forecast at the level of\ncompetitive human forecasters. Towards this goal, we develop a\nretrieval-augmented LM system designed to automatically search for relevant\ninformation, generate forecasts, and aggregate predictions. To facilitate our\nstudy, we collect a large dataset of questions from competitive forecasting\nplatforms. Under a test set published after the knowledge cut-offs of our LMs,\nwe evaluate the end-to-end performance of our system against the aggregates of\nhuman forecasts. On average, the system nears the crowd aggregate of\ncompetitive forecasters, and in some settings surpasses it. Our work suggests\nthat using LMs to forecast the future could provide accurate predictions at\nscale and help to inform institutional decision making.",
      "tldr_zh": "这篇论文探讨了语言模型（LMs）是否能达到竞争性人类预报员的水平，专注于未来事件预测以支持决策。研究团队开发了一个检索增强的LM系统，能够自动搜索相关信息、生成预测并聚合结果，并为此收集了大量来自预报平台的问答数据集。在测试集评估中，该系统平均接近人类预报的聚合表现，并在某些设置下超越它，表明LMs可提供可扩展的准确预测，帮助机构决策。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18563v1",
      "published_date": "2024-02-28 18:54:18 UTC",
      "updated_date": "2024-02-28 18:54:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:51:29.702931"
    },
    {
      "arxiv_id": "2402.18540v2",
      "title": "Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates",
      "title_zh": "在微调后保持大语言模型对齐：提示模板的关键作用",
      "authors": [
        "Kaifeng Lyu",
        "Haoyu Zhao",
        "Xinran Gu",
        "Dingli Yu",
        "Anirudh Goyal",
        "Sanjeev Arora"
      ],
      "abstract": "Public LLMs such as the Llama 2-Chat underwent alignment training and were\nconsidered safe. Recently Qi et al. [2024] reported that even benign\nfine-tuning on seemingly safe datasets can give rise to unsafe behaviors in the\nmodels. The current paper is about methods and best practices to mitigate such\nloss of alignment. We focus on the setting where a public model is fine-tuned\nbefore serving users for specific usage, where the model should improve on the\ndownstream task while maintaining alignment. Through extensive experiments on\nseveral chat models (Meta's Llama 2-Chat, Mistral AI's Mistral 7B Instruct\nv0.2, and OpenAI's GPT-3.5 Turbo), this paper uncovers that the prompt\ntemplates used during fine-tuning and inference play a crucial role in\npreserving safety alignment, and proposes the ``Pure Tuning, Safe Testing''\n(PTST) strategy -- fine-tune models without a safety prompt, but include it at\ntest time. This seemingly counterintuitive strategy incorporates an intended\ndistribution shift to encourage alignment preservation. Fine-tuning experiments\non GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the\nrise of unsafe behaviors.",
      "tldr_zh": "这篇论文探讨了在细调（fine-tuning）后保持大型语言模型（LLMs）对齐的重要性，特别是公共模型如 Llama 2-Chat、Mistral 7B Instruct v0.2 和 GPT-3.5 Turbo 在处理下游任务时可能出现不安全行为的问题。作者通过广泛实验发现，提示模板（prompt templates）在维持安全对齐中发挥关键作用，并提出了“Pure Tuning, Safe Testing”（PTST）策略，即在细调时不使用安全提示，而在测试时添加它，以通过引入分布偏移鼓励对齐保留。在 GSM8K、ChatDoctor 和 OpenOrca 数据集上的实验表明，PTST 策略显著减少了不安全行为的发生，同时提高了模型在下游任务上的性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.18540v2",
      "published_date": "2024-02-28 18:23:49 UTC",
      "updated_date": "2025-01-17 01:43:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:51:42.736733"
    },
    {
      "arxiv_id": "2402.18617v1",
      "title": "ELA: Exploited Level Augmentation for Offline Learning in Zero-Sum Games",
      "title_zh": "翻译失败",
      "authors": [
        "Shiqi Lei",
        "Kanghoon Lee",
        "Linjing Li",
        "Jinkyoo Park",
        "Jiachen Li"
      ],
      "abstract": "Offline learning has become widely used due to its ability to derive\neffective policies from offline datasets gathered by expert demonstrators\nwithout interacting with the environment directly. Recent research has explored\nvarious ways to enhance offline learning efficiency by considering the\ncharacteristics (e.g., expertise level or multiple demonstrators) of the\ndataset. However, a different approach is necessary in the context of zero-sum\ngames, where outcomes vary significantly based on the strategy of the opponent.\nIn this study, we introduce a novel approach that uses unsupervised learning\ntechniques to estimate the exploited level of each trajectory from the offline\ndataset of zero-sum games made by diverse demonstrators. Subsequently, we\nincorporate the estimated exploited level into the offline learning to maximize\nthe influence of the dominant strategy. Our method enables interpretable\nexploited level estimation in multiple zero-sum games and effectively\nidentifies dominant strategy data. Also, our exploited level augmented offline\nlearning significantly enhances the original offline learning algorithms\nincluding imitation learning and offline reinforcement learning for zero-sum\ngames.",
      "tldr_zh": "本研究提出ELA（Exploited Level Augmentation）方法，用于提升零和游戏（Zero-Sum Games）中的离线学习（Offline Learning）效率，通过无监督学习（Unsupervised Learning）估计数据集轨迹的Exploited Level，以识别主导策略数据。ELA将估算的Exploited Level融入离线学习过程中，最大化主导策略的影响，从而在多重零和游戏中实现可解释的策略优化。该方法显著增强了原有算法，包括模仿学习（Imitation Learning）和离线强化学习（Offline Reinforcement Learning）的性能。",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.GT",
      "comment": "12 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.18617v1",
      "published_date": "2024-02-28 17:44:02 UTC",
      "updated_date": "2024-02-28 17:44:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:51:54.743697"
    },
    {
      "arxiv_id": "2402.18616v1",
      "title": "JCLEC-MO: a Java suite for solving many-objective optimization engineering problems",
      "title_zh": "翻译失败",
      "authors": [
        "Aurora Ramírez",
        "José Raúl Romero",
        "Carlos García-Martínez",
        "Sebastián Ventura"
      ],
      "abstract": "Although metaheuristics have been widely recognized as efficient techniques\nto solve real-world optimization problems, implementing them from scratch\nremains difficult for domain-specific experts without programming skills. In\nthis scenario, metaheuristic optimization frameworks are a practical\nalternative as they provide a variety of algorithms composed of customized\nelements, as well as experimental support. Recently, many engineering problems\nrequire to optimize multiple or even many objectives, increasing the interest\nin appropriate metaheuristic algorithms and frameworks that might integrate new\nspecific requirements while maintaining the generality and reusability\nprinciples they were conceived for. Based on this idea, this paper introduces\nJCLEC-MO, a Java framework for both multi- and many-objective optimization that\nenables engineers to apply, or adapt, a great number of multi-objective\nalgorithms with little coding effort. A case study is developed and explained\nto show how JCLEC-MO can be used to address many-objective engineering\nproblems, often requiring the inclusion of domain-specific elements, and to\nanalyze experimental outcomes by means of conveniently connected R utilities.",
      "tldr_zh": "该论文介绍了JCLEC-MO，这是一个Java框架，旨在帮助非编程专家轻松解决多目标和多目标优化工程问题。JCLEC-MO提供多种元启发式(metaheuristics)算法和实验支持，用户可以通过少量编码努力应用或适应这些算法，同时整合领域特定元素。研究通过一个案例研究展示了该框架在处理复杂工程问题时的有效性，并利用R工具分析实验结果。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "68T20",
        "D.0; I.2.8"
      ],
      "primary_category": "cs.NE",
      "comment": "41 pages, 5 figures, journal paper",
      "pdf_url": "http://arxiv.org/pdf/2402.18616v1",
      "published_date": "2024-02-28 17:38:01 UTC",
      "updated_date": "2024-02-28 17:38:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:52:06.853205"
    },
    {
      "arxiv_id": "2402.18496v3",
      "title": "Language Models Represent Beliefs of Self and Others",
      "title_zh": "翻译失败",
      "authors": [
        "Wentao Zhu",
        "Zhining Zhang",
        "Yizhou Wang"
      ],
      "abstract": "Understanding and attributing mental states, known as Theory of Mind (ToM),\nemerges as a fundamental capability for human social reasoning. While Large\nLanguage Models (LLMs) appear to possess certain ToM abilities, the mechanisms\nunderlying these capabilities remain elusive. In this study, we discover that\nit is possible to linearly decode the belief status from the perspectives of\nvarious agents through neural activations of language models, indicating the\nexistence of internal representations of self and others' beliefs. By\nmanipulating these representations, we observe dramatic changes in the models'\nToM performance, underscoring their pivotal role in the social reasoning\nprocess. Additionally, our findings extend to diverse social reasoning tasks\nthat involve different causal inference patterns, suggesting the potential\ngeneralizability of these representations.",
      "tldr_zh": "这篇论文探讨了大语言模型(LLMs)是否具备Theory of Mind (ToM)能力，即理解和归因自我与其他代理的心理状态。研究发现，通过线性解码LLMs的神经激活，可以提取不同代理的信念状态，从而揭示模型内部存在自我和其他信念的表示。进一步操纵这些表示会导致模型的ToM性能显著变化，并显示出在涉及多种因果推理模式的社会推理任务中的潜在泛化性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "project page: https://walter0807.github.io/RepBelief/",
      "pdf_url": "http://arxiv.org/pdf/2402.18496v3",
      "published_date": "2024-02-28 17:25:59 UTC",
      "updated_date": "2024-05-30 12:43:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:52:17.312009"
    },
    {
      "arxiv_id": "2402.18487v1",
      "title": "Human-Centric Aware UAV Trajectory Planning in Search and Rescue Missions Employing Multi-Objective Reinforcement Learning with AHP and Similarity-Based Experience Replay",
      "title_zh": "翻译失败",
      "authors": [
        "Mahya Ramezani",
        "Jose Luis Sanchez-Lopez"
      ],
      "abstract": "The integration of Unmanned Aerial Vehicles (UAVs) into Search and Rescue\n(SAR) missions presents a promising avenue for enhancing operational efficiency\nand effectiveness. However, the success of these missions is not solely\ndependent on the technical capabilities of the drones but also on their\nacceptance and interaction with humans on the ground. This paper explores the\neffect of human-centric factor in UAV trajectory planning for SAR missions. We\nintroduce a novel approach based on the reinforcement learning augmented with\nAnalytic Hierarchy Process and novel similarity-based experience replay to\noptimize UAV trajectories, balancing operational objectives with human comfort\nand safety considerations. Additionally, through a comprehensive survey, we\ninvestigate the impact of gender cues and anthropomorphism in UAV design on\npublic acceptance and trust, revealing significant implications for drone\ninteraction strategies in SAR. Our contributions include (1) a reinforcement\nlearning framework for UAV trajectory planning that dynamically integrates\nmulti-objective considerations, (2) an analysis of human perceptions towards\ngendered and anthropomorphized drones in SAR contexts, and (3) the application\nof similarity-based experience replay for enhanced learning efficiency in\ncomplex SAR scenarios. The findings offer valuable insights into designing UAV\nsystems that are not only technically proficient but also aligned with\nhuman-centric values.",
      "tldr_zh": "这篇论文探讨了在搜索和救援（SAR）任务中，考虑人类因素的无人驾驶航空器（UAV）轨迹规划问题。研究提出了一种基于多目标强化学习（Multi-Objective Reinforcement Learning）的创新框架，结合 Analytic Hierarchy Process (AHP) 和基于相似性的经验回放（Similarity-Based Experience Replay），以优化UAV轨迹，同时平衡操作目标与人类舒适度和安全。论文通过调查分析了性别线索和拟人化（Anthropomorphism）在UAV设计对公众接受度和信任的影响。总体贡献包括一个动态整合多目标的强化学习框架、对人类感知的深入分析，以及改进学习效率的方法，为设计技术先进且以人为本的UAV系统提供了重要见解。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18487v1",
      "published_date": "2024-02-28 17:10:22 UTC",
      "updated_date": "2024-02-28 17:10:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:52:32.866400"
    },
    {
      "arxiv_id": "2402.18485v3",
      "title": "A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist",
      "title_zh": "翻译失败",
      "authors": [
        "Wentao Zhang",
        "Lingxuan Zhao",
        "Haochong Xia",
        "Shuo Sun",
        "Jiaze Sun",
        "Molei Qin",
        "Xinyi Li",
        "Yuqing Zhao",
        "Yilei Zhao",
        "Xinyu Cai",
        "Longtao Zheng",
        "Xinrun Wang",
        "Bo An"
      ],
      "abstract": "Financial trading is a crucial component of the markets, informed by a\nmultimodal information landscape encompassing news, prices, and Kline charts,\nand encompasses diverse tasks such as quantitative trading and high-frequency\ntrading with various assets. While advanced AI techniques like deep learning\nand reinforcement learning are extensively utilized in finance, their\napplication in financial trading tasks often faces challenges due to inadequate\nhandling of multimodal data and limited generalizability across various tasks.\nTo address these challenges, we present FinAgent, a multimodal foundational\nagent with tool augmentation for financial trading. FinAgent's market\nintelligence module processes a diverse range of data-numerical, textual, and\nvisual-to accurately analyze the financial market. Its unique dual-level\nreflection module not only enables rapid adaptation to market dynamics but also\nincorporates a diversified memory retrieval system, enhancing the agent's\nability to learn from historical data and improve decision-making processes.\nThe agent's emphasis on reasoning for actions fosters trust in its financial\ndecisions. Moreover, FinAgent integrates established trading strategies and\nexpert insights, ensuring that its trading approaches are both data-driven and\nrooted in sound financial principles. With comprehensive experiments on 6\nfinancial datasets, including stocks and Crypto, FinAgent significantly\noutperforms 9 state-of-the-art baselines in terms of 6 financial metrics with\nover 36% average improvement on profit. Specifically, a 92.27% return (a 84.39%\nrelative improvement) is achieved on one dataset. Notably, FinAgent is the\nfirst advanced multimodal foundation agent designed for financial trading\ntasks.",
      "tldr_zh": "本研究提出 FinAgent，一种多模态基础代理（Multimodal Foundation Agent），通过工具增强（Tool-Augmented）、多元化和通用化设计，解决金融交易中多模态数据处理（如新闻、价格和K线图）和任务泛化挑战。FinAgent 包括市场智能模块处理数值、文本和视觉数据，双层反射模块（dual-level reflection module）实现快速市场适应和多元化记忆检索，并整合交易策略以提升决策可信度。实验在6个金融数据集（包括股票和加密货币）上显示，FinAgent 超越9个最先进基线，在6个金融指标上平均提升36%，最高回报率达92.27%（相对提升84.39%），是首个针对金融交易任务的先进多模态代理。",
      "categories": [
        "q-fin.TR",
        "cs.AI"
      ],
      "primary_category": "q-fin.TR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18485v3",
      "published_date": "2024-02-28 17:06:54 UTC",
      "updated_date": "2024-06-28 10:35:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:52:42.440856"
    },
    {
      "arxiv_id": "2402.18477v4",
      "title": "Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes",
      "title_zh": "Signature Kernel 条件独立性测试在随机过程因果发现中的应用",
      "authors": [
        "Georg Manten",
        "Cecilia Casolo",
        "Emilio Ferrucci",
        "Søren Wengel Mogensen",
        "Cristopher Salvi",
        "Niki Kilbertus"
      ],
      "abstract": "Inferring the causal structure underlying stochastic dynamical systems from\nobservational data holds great promise in domains ranging from science and\nhealth to finance. Such processes can often be accurately modeled via\nstochastic differential equations (SDEs), which naturally imply causal\nrelationships via \"which variables enter the differential of which other\nvariables\". In this paper, we develop conditional independence (CI) constraints\non coordinate processes over selected intervals that are Markov with respect to\nthe acyclic dependence graph (allowing self-loops) induced by a general SDE\nmodel. We then provide a sound and complete causal discovery algorithm, capable\nof handling both fully and partially observed data, and uniquely recovering the\nunderlying or induced ancestral graph by exploiting time directionality\nassuming a CI oracle. Finally, to make our algorithm practically usable, we\nalso propose a flexible, consistent signature kernel-based CI test to infer\nthese constraints from data. We extensively benchmark the CI test in isolation\nand as part of our causal discovery algorithms, outperforming existing\napproaches in SDE models and beyond.",
      "tldr_zh": "这篇论文针对随机过程的因果发现，开发了基于随机微分方程 (SDEs) 的条件独立 (CI) 约束，这些约束适用于选定区间上的坐标过程，并假设这些过程相对于无环依赖图 (允许自环) 是 Markov 的。论文提出了一种可靠且完整的因果发现算法，能够处理完全和部分观测数据，通过利用时间方向性来唯一恢复底层或诱导的祖先图。最终，他们引入了一种灵活一致的基于 signature kernel 的 CI 测试，并在广泛基准测试中证明其优于现有方法，不仅在 SDE 模型中，还适用于其他场景。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18477v4",
      "published_date": "2024-02-28 16:58:31 UTC",
      "updated_date": "2025-03-03 11:25:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:52:55.095943"
    },
    {
      "arxiv_id": "2402.18449v1",
      "title": "HOP to the Next Tasks and Domains for Continual Learning in NLP",
      "title_zh": "翻译失败",
      "authors": [
        "Umberto Michieli",
        "Mete Ozay"
      ],
      "abstract": "Continual Learning (CL) aims to learn a sequence of problems (i.e., tasks and\ndomains) by transferring knowledge acquired on previous problems, whilst\navoiding forgetting of past ones. Different from previous approaches which\nfocused on CL for one NLP task or domain in a specific use-case, in this paper,\nwe address a more general CL setting to learn from a sequence of problems in a\nunique framework. Our method, HOP, permits to hop across tasks and domains by\naddressing the CL problem along three directions: (i) we employ a set of\nadapters to generalize a large pre-trained model to unseen problems, (ii) we\ncompute high-order moments over the distribution of embedded representations to\ndistinguish independent and correlated statistics across different tasks and\ndomains, (iii) we process this enriched information with auxiliary heads\nspecialized for each end problem. Extensive experimental campaign on 4 NLP\napplications, 5 benchmarks and 2 CL setups demonstrates the effectiveness of\nour HOP.",
      "tldr_zh": "这篇论文针对自然语言处理(NLP)中的 Continual Learning (CL)，提出了一种名为 HOP 的方法，用于在一个统一框架中学习一系列任务和领域的序列，从而避免遗忘并转移知识。HOP 通过一组 adapters 泛化大型预训练模型、计算嵌入表示分布的高阶 moments 来区分不同任务和领域的独立及相关统计，并使用专门的辅助 heads 处理这些信息。实验在 4 个 NLP 应用、5 个基准和 2 个 CL 设置上表明，HOP 方法有效提升了模型的泛化性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "AAAI 2024. Main + supplmentary",
      "pdf_url": "http://arxiv.org/pdf/2402.18449v1",
      "published_date": "2024-02-28 16:21:02 UTC",
      "updated_date": "2024-02-28 16:21:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:53:06.905233"
    },
    {
      "arxiv_id": "2402.18443v1",
      "title": "LeMo-NADe: Multi-Parameter Neural Architecture Discovery with LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Md Hafizur Rahman",
        "Prabuddha Chakraborty"
      ],
      "abstract": "Building efficient neural network architectures can be a time-consuming task\nrequiring extensive expert knowledge. This task becomes particularly\nchallenging for edge devices because one has to consider parameters such as\npower consumption during inferencing, model size, inferencing speed, and CO2\nemissions. In this article, we introduce a novel framework designed to\nautomatically discover new neural network architectures based on user-defined\nparameters, an expert system, and an LLM trained on a large amount of\nopen-domain knowledge. The introduced framework (LeMo-NADe) is tailored to be\nused by non-AI experts, does not require a predetermined neural architecture\nsearch space, and considers a large set of edge device-specific parameters. We\nimplement and validate this proposed neural architecture discovery framework\nusing CIFAR-10, CIFAR-100, and ImageNet16-120 datasets while using GPT-4 Turbo\nand Gemini as the LLM component. We observe that the proposed framework can\nrapidly (within hours) discover intricate neural network models that perform\nextremely well across a diverse set of application settings defined by the\nuser.",
      "tldr_zh": "该论文提出LeMo-NADe框架，利用大型语言模型(LLMs)如GPT-4 Turbo和Gemini，结合用户定义参数及专家系统，实现多参数神经架构发现(Nerual Architecture Discovery)，以简化高效神经网络的设计过程，尤其针对边缘设备（如考虑功耗、模型大小、推理速度和CO2排放）。该框架无需预定义搜索空间，便于非AI专家使用，并能自动发现适应多种应用场景的复杂模型。实验在CIFAR-10、CIFAR-100和ImageNet16-120数据集上验证，显示LeMo-NADe可在数小时内生成高性能神经网络，显著提升效率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages, 5 figures, 10 tables and 3 algorithms",
      "pdf_url": "http://arxiv.org/pdf/2402.18443v1",
      "published_date": "2024-02-28 16:13:44 UTC",
      "updated_date": "2024-02-28 16:13:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:53:18.942189"
    },
    {
      "arxiv_id": "2403.00833v1",
      "title": "Position Paper: Agent AI Towards a Holistic Intelligence",
      "title_zh": "观点论文：Agent AI 走向",
      "authors": [
        "Qiuyuan Huang",
        "Naoki Wake",
        "Bidipta Sarkar",
        "Zane Durante",
        "Ran Gong",
        "Rohan Taori",
        "Yusuke Noda",
        "Demetri Terzopoulos",
        "Noboru Kuno",
        "Ade Famoti",
        "Ashley Llorens",
        "John Langford",
        "Hoi Vo",
        "Li Fei-Fei",
        "Katsu Ikeuchi",
        "Jianfeng Gao"
      ],
      "abstract": "Recent advancements in large foundation models have remarkably enhanced our\nunderstanding of sensory information in open-world environments. In leveraging\nthe power of foundation models, it is crucial for AI research to pivot away\nfrom excessive reductionism and toward an emphasis on systems that function as\ncohesive wholes. Specifically, we emphasize developing Agent AI -- an embodied\nsystem that integrates large foundation models into agent actions. The emerging\nfield of Agent AI spans a wide range of existing embodied and agent-based\nmultimodal interactions, including robotics, gaming, and healthcare systems,\netc. In this paper, we propose a novel large action model to achieve embodied\nintelligent behavior, the Agent Foundation Model. On top of this idea, we\ndiscuss how agent AI exhibits remarkable capabilities across a variety of\ndomains and tasks, challenging our understanding of learning and cognition.\nFurthermore, we discuss the potential of Agent AI from an interdisciplinary\nperspective, underscoring AI cognition and consciousness within scientific\ndiscourse. We believe that those discussions serve as a basis for future\nresearch directions and encourage broader societal engagement.",
      "tldr_zh": "本文提出 Agent AI 作为一种整体智能框架，将大型基础模型(large foundation models)整合到具身系统中，以实现更连贯的代理行为，涵盖机器人、游戏和医疗等领域。论文强调从过度还原论转向系统整体功能，并引入 Agent Foundation Model 来实现具身智能行为，挑战传统学习和认知理解。同时，从跨学科视角讨论 Agent AI 的潜力，包括 AI 认知和意识，为未来研究方向和社会参与提供基础。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "22 pages, 4 figures. arXiv admin note: substantial text overlap with\n  arXiv:2401.03568",
      "pdf_url": "http://arxiv.org/pdf/2403.00833v1",
      "published_date": "2024-02-28 16:09:56 UTC",
      "updated_date": "2024-02-28 16:09:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:53:30.352749"
    },
    {
      "arxiv_id": "2402.18439v3",
      "title": "Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication",
      "title_zh": "超越自然语言：LLMs 利用替代格式以增强推理和通信",
      "authors": [
        "Weize Chen",
        "Chenfei Yuan",
        "Jiarui Yuan",
        "Yusheng Su",
        "Chen Qian",
        "Cheng Yang",
        "Ruobing Xie",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "Natural language (NL) has long been the predominant format for human\ncognition and communication, and by extension, has been similarly pivotal in\nthe development and application of Large Language Models (LLMs). Yet, besides\nNL, LLMs have seen various non-NL formats during pre-training, such as code and\nlogical expression. NL's status as the optimal format for LLMs, particularly in\nsingle-LLM reasoning and multi-agent communication, has not been thoroughly\nexamined. In this work, we challenge the default use of NL by exploring the\nutility of non-NL formats in these contexts. We show that allowing LLMs to\nautonomously select the most suitable format before reasoning or communicating\nleads to a 3.3 to 5.7\\% improvement in reasoning efficiency for different LLMs,\nand up to a 72.7\\% reduction in token usage in multi-agent communication, all\nwhile maintaining communicative effectiveness. Our comprehensive analysis\nfurther reveals that LLMs can devise a format from limited task instructions\nand that the devised format is effectively transferable across different LLMs.\nIntriguingly, the structured communication format decided by LLMs exhibits\nnotable parallels with established agent communication languages, suggesting a\nnatural evolution towards efficient, structured communication in agent\ncommunication. Our code is released at\n\\url{https://github.com/thunlp/AutoForm}.",
      "tldr_zh": "这篇论文探讨了 Large Language Models (LLMs) 超越 Natural Language (NL)，利用非 NL 格式（如代码和逻辑表达式）来提升推理和多代理通信效率。通过允许 LLMs 自主选择最合适的格式，研究发现单 LLM 推理效率提高了 3.3% 到 5.7%，而多代理通信的 token 使用减少了多达 72.7%，同时保持沟通有效性。论文进一步分析显示，LLMs 可以从有限的任务指令中设计出可转移的结构化格式，且这些格式与现有代理通信语言类似，表明 LLMs 向高效结构化通信的自然演化。研究代码已开源在指定链接。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Code release at https://github.com/thunlp/AutoForm",
      "pdf_url": "http://arxiv.org/pdf/2402.18439v3",
      "published_date": "2024-02-28 16:07:54 UTC",
      "updated_date": "2024-06-19 01:42:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:53:44.242906"
    },
    {
      "arxiv_id": "2403.00026v1",
      "title": "Learning to Deliver: a Foundation Model for the Montreal Capacitated Vehicle Routing Problem",
      "title_zh": "翻译失败",
      "authors": [
        "Samuel J. K. Chin",
        "Matthias Winkenbach",
        "Akash Srivastava"
      ],
      "abstract": "In this paper, we present the Foundation Model for the Montreal Capacitated\nVehicle Routing Problem (FM-MCVRP), a novel Deep Learning (DL) model that\napproximates high-quality solutions to a variant of the Capacitated Vehicle\nRouting Problem (CVRP) that characterizes many real-world applications. The\nso-called Montreal Capacitated Vehicle Routing Problem (MCVRP), first formally\ndescribed by Bengio et al. (2021), is defined on a fixed and finite graph,\nwhich is analogous to a city. Each MCVRP instance is essentially the sub-graph\nconnecting a randomly sampled subset of the nodes in the fixed graph, which\nrepresent a set of potential addresses in a real-world delivery problem on a\ngiven day. Our work exploits this problem structure to frame the MCVRP as an\nanalogous Natural Language Processing (NLP) task. Specifically, we leverage a\nTransformer architecture embedded in a Large Language Model (LLM) framework to\ntrain our model in a supervised manner on computationally inexpensive,\nsub-optimal MCVRP solutions obtained algorithmically. Through comprehensive\ncomputational experiments, we show that FM-MCVRP produces better MCVRP\nsolutions than the training data and generalizes to larger sized problem\ninstances not seen during training. Even when compared to near-optimal\nsolutions from state-of-the-art heuristics, FM-MCVRP yields competitive results\ndespite being trained on inferior data. For instance, for 400-customer\nproblems, FM-MCVRP solutions on average fall within 2% of the benchmark. Our\nresults further demonstrate that unlike prior works in the literature, FM-MCVRP\nis a unified model, which performs consistently and reliably on a range of\nproblem instance sizes and parameter values such as the vehicle capacity.",
      "tldr_zh": "本文提出 FM-MCVRP，一种基于深度学习的 Foundation Model，用于近似解决 Montreal Capacitated Vehicle Routing Problem (MCVRP)，这是一个模拟现实配送问题的车辆路径优化变体。模型将 MCVRP 框架化为类似于 Natural Language Processing (NLP) 任务，利用 Transformer 架构嵌入 Large Language Model (LLM) 框架，在算法生成的次优解决方案上进行监督训练。实验结果显示，FM-MCVRP 生成的解决方案优于训练数据，并能泛化到未见过的更大实例，与最先进启发式方法相比保持竞争力，例如在 400 客户问题上，平均误差在 2% 以内。该模型作为统一框架，在不同问题规模和车辆容量参数下表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00026v1",
      "published_date": "2024-02-28 16:02:29 UTC",
      "updated_date": "2024-02-28 16:02:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:53:56.605806"
    },
    {
      "arxiv_id": "2402.18426v1",
      "title": "A Relational Inductive Bias for Dimensional Abstraction in Neural Networks",
      "title_zh": "神经网络中维度抽象的关系性归纳偏差",
      "authors": [
        "Declan Campbell",
        "Jonathan D. Cohen"
      ],
      "abstract": "The human cognitive system exhibits remarkable flexibility and generalization\ncapabilities, partly due to its ability to form low-dimensional, compositional\nrepresentations of the environment. In contrast, standard neural network\narchitectures often struggle with abstract reasoning tasks, overfitting, and\nrequiring extensive data for training. This paper investigates the impact of\nthe relational bottleneck -- a mechanism that focuses processing on relations\namong inputs -- on the learning of factorized representations conducive to\ncompositional coding and the attendant flexibility of processing. We\ndemonstrate that such a bottleneck not only improves generalization and\nlearning efficiency, but also aligns network performance with human-like\nbehavioral biases. Networks trained with the relational bottleneck developed\northogonal representations of feature dimensions latent in the dataset,\nreflecting the factorized structure thought to underlie human cognitive\nflexibility. Moreover, the relational network mimics human biases towards\nregularity without pre-specified symbolic primitives, suggesting that the\nbottleneck fosters the emergence of abstract representations that confer\nflexibility akin to symbols.",
      "tldr_zh": "本论文探讨了人类认知系统通过低维组合表示实现灵活泛化的能力，以及标准神经网络在抽象推理任务中面临的过拟合和数据需求问题。作者引入了 relational inductive bias 和 relational bottleneck 机制，专注于输入之间的关系，以促进神经网络学习因子化的表示，从而提升泛化和学习效率。实验结果显示，这种方法使网络开发出正交的特征维度表示，模仿人类行为偏好，并在没有预定义符号原语的情况下出现类似符号的抽象表示。总的来说，该工作为构建更具人类认知灵活性的神经网络提供了新途径。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18426v1",
      "published_date": "2024-02-28 15:51:05 UTC",
      "updated_date": "2024-02-28 15:51:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:54:06.586805"
    },
    {
      "arxiv_id": "2402.18424v2",
      "title": "Emotion Classification in Low and Moderate Resource Languages",
      "title_zh": "低资源和中等资源语言的情感分类",
      "authors": [
        "Shabnam Tafreshi",
        "Shubham Vatsal",
        "Mona Diab"
      ],
      "abstract": "It is important to be able to analyze the emotional state of people around\nthe globe. There are 7100+ active languages spoken around the world and\nbuilding emotion classification for each language is labor intensive.\nParticularly for low-resource and endangered languages, building emotion\nclassification can be quite challenging. We present a cross-lingual emotion\nclassifier, where we train an emotion classifier with resource-rich languages\n(i.e. \\textit{English} in our work) and transfer the learning to low and\nmoderate resource languages. We compare and contrast two approaches of transfer\nlearning from a high-resource language to a low or moderate-resource language.\nOne approach projects the annotation from a high-resource language to low and\nmoderate-resource language in parallel corpora and the other one uses direct\ntransfer from high-resource language to the other languages. We show the\nefficacy of our approaches on 6 languages: Farsi, Arabic, Spanish, Ilocano,\nOdia, and Azerbaijani. Our results indicate that our approaches outperform\nrandom baselines and transfer emotions across languages successfully. For all\nlanguages, the direct cross-lingual transfer of emotion yields better results.\nWe also create annotated emotion-labeled resources for four languages: Farsi,\nAzerbaijani, Ilocano and Odia.",
      "tldr_zh": "这篇论文探讨了在低资源和中等资源语言中进行情感分类的挑战，提出了一种跨语言情感分类器，通过从资源丰富的语言（如英语）进行transfer learning来转移知识。研究比较了两种方法：一种是将标注从高资源语言投影到平行语料库中的低资源语言，另一种是直接跨语言转移。实验在Farsi、Arabic、Spanish、Ilocano、Odia 和 Azerbaijani 等6种语言上进行，结果显示直接转移方法优于随机基线，并在所有语言中表现出色。该工作还为Farsi、Azerbaijani、Ilocano 和 Odia 创建了新的情感标注资源，推进了全球情感分析的可用性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18424v2",
      "published_date": "2024-02-28 15:46:09 UTC",
      "updated_date": "2024-11-07 19:02:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:54:20.632876"
    },
    {
      "arxiv_id": "2402.18419v2",
      "title": "Can GPT Improve the State of Prior Authorization via Guideline Based Automated Question Answering?",
      "title_zh": "翻译失败",
      "authors": [
        "Shubham Vatsal",
        "Ayush Singh",
        "Shabnam Tafreshi"
      ],
      "abstract": "Health insurance companies have a defined process called prior authorization\n(PA) which is a health plan cost-control process that requires doctors and\nother healthcare professionals to get clearance in advance from a health plan\nbefore performing a particular procedure on a patient in order to be eligible\nfor payment coverage. For health insurance companies, approving PA requests for\npatients in the medical domain is a time-consuming and challenging task. One of\nthose key challenges is validating if a request matches up to certain criteria\nsuch as age, gender, etc. In this work, we evaluate whether GPT can validate\nnumerous key factors, in turn helping health plans reach a decision drastically\nfaster. We frame it as a question answering task, prompting GPT to answer a\nquestion from patient electronic health record. We experiment with different\nconventional prompting techniques as well as introduce our own novel prompting\ntechnique. Moreover, we report qualitative assessment by humans on the natural\nlanguage generation outputs from our approach. Results show that our method\nachieves superior performance with the mean weighted F1 score of 0.61 as\ncompared to its standard counterparts.",
      "tldr_zh": "本研究探讨了GPT是否能通过基于指南的自动问答（Guideline Based Automated Question Answering）改善医疗保险的Prior Authorization（PA）过程，该过程要求医生在进行特定程序前验证患者资格（如年龄、性别等），以加速决策并降低挑战。研究将PA验证 framing 为问答任务，使用GPT从患者电子健康记录中回答问题，并实验了多种提示技术，包括一种新颖的自定义方法。结果显示，该方法在性能上优于标准方法，平均加权F1 score达到0.61，并通过人类定性评估确认了其自然语言生成输出的高质量，从而为更快、更高效的PA审批提供潜在解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18419v2",
      "published_date": "2024-02-28 15:39:53 UTC",
      "updated_date": "2024-10-25 16:19:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:54:31.769052"
    },
    {
      "arxiv_id": "2402.18409v4",
      "title": "A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Xiujie Song",
        "Mengyue Wu",
        "Kenny Q. Zhu",
        "Chunhao Zhang",
        "Yanyi Chen"
      ],
      "abstract": "Large Vision-Language Models (LVLMs), despite their recent success, are\nhardly comprehensively tested for their cognitive abilities. Inspired by the\nprevalent use of the Cookie Theft task in human cognitive tests, we propose a\nnovel evaluation benchmark to evaluate high-level cognitive abilities of LVLMs\nusing images with rich semantics. The benchmark consists of 251 images along\nwith comprehensive annotations. It defines eight reasoning capabilities and\ncomprises an image description task and a visual question answering task. Our\nevaluation of well-known LVLMs shows that there is still a significant gap in\ncognitive abilities between LVLMs and humans.",
      "tldr_zh": "该论文提出一个新的认知评估基准，用于测试大型视觉语言模型（LVLMs）的图像推理和描述能力，该基准受人类认知测试中 Cookie Theft 任务的启发。基准包含 251 张语义丰富的图像、全面注解，并定义了八种推理能力，包括图像描述任务和视觉问答任务。通过对知名 LVLMs 的评估，结果显示这些模型在高级认知能力上与人类仍存在显著差距。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18409v4",
      "published_date": "2024-02-28 15:28:36 UTC",
      "updated_date": "2025-02-13 02:37:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:54:43.019081"
    },
    {
      "arxiv_id": "2403.00025v3",
      "title": "On the Challenges and Opportunities in Generative AI",
      "title_zh": "翻译失败",
      "authors": [
        "Laura Manduchi",
        "Kushagra Pandey",
        "Clara Meister",
        "Robert Bamler",
        "Ryan Cotterell",
        "Sina Däubener",
        "Sophie Fellenz",
        "Asja Fischer",
        "Thomas Gärtner",
        "Matthias Kirchler",
        "Marius Kloft",
        "Yingzhen Li",
        "Christoph Lippert",
        "Gerard de Melo",
        "Eric Nalisnick",
        "Björn Ommer",
        "Rajesh Ranganath",
        "Maja Rudolph",
        "Karen Ullrich",
        "Guy Van den Broeck",
        "Julia E Vogt",
        "Yixin Wang",
        "Florian Wenzel",
        "Frank Wood",
        "Stephan Mandt",
        "Vincent Fortuin"
      ],
      "abstract": "The field of deep generative modeling has grown rapidly in the last few\nyears. With the availability of massive amounts of training data coupled with\nadvances in scalable unsupervised learning paradigms, recent large-scale\ngenerative models show tremendous promise in synthesizing high-resolution\nimages and text, as well as structured data such as videos and molecules.\nHowever, we argue that current large-scale generative AI models exhibit several\nfundamental shortcomings that hinder their widespread adoption across domains.\nIn this work, our objective is to identify these issues and highlight key\nunresolved challenges in modern generative AI paradigms that should be\naddressed to further enhance their capabilities, versatility, and reliability.\nBy identifying these challenges, we aim to provide researchers with insights\nfor exploring fruitful research directions, thus fostering the development of\nmore robust and accessible generative AI solutions.",
      "tldr_zh": "这篇论文讨论了生成 AI（Generative AI）领域的快速发展及其潜力，包括利用海量训练数据和可扩展的无监督学习范式来合成高分辨率图像、文本、视频和分子等结构化数据。然而，论文指出当前的大型生成 AI 模型存在根本缺陷，如阻碍跨领域广泛采用的问题，并识别了关键未解决挑战，包括提升模型的性能、多功能性和可靠性。通过分析这些挑战，论文为研究人员提供见解，探索新的研究方向，以推动更稳健和易访问的生成 AI 解决方案的发展。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00025v3",
      "published_date": "2024-02-28 15:19:33 UTC",
      "updated_date": "2025-03-20 18:07:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:54:53.764584"
    },
    {
      "arxiv_id": "2402.18393v3",
      "title": "Decictor: Towards Evaluating the Robustness of Decision-Making in Autonomous Driving Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Mingfei Cheng",
        "Yuan Zhou",
        "Xiaofei Xie",
        "Junjie Wang",
        "Guozhu Meng",
        "Kairui Yang"
      ],
      "abstract": "Autonomous Driving System (ADS) testing is crucial in ADS development, with\nthe current primary focus being on safety. However, the evaluation of\nnon-safety-critical performance, particularly the ADS's ability to make optimal\ndecisions and produce optimal paths for autonomous vehicles (AVs), is also\nvital to ensure the intelligence and reduce risks of AVs. Currently, there is\nlittle work dedicated to assessing the robustness of ADSs' path-planning\ndecisions (PPDs), i.e., whether an ADS can maintain the optimal PPD after an\ninsignificant change in the environment. The key challenges include the lack of\nclear oracles for assessing PPD optimality and the difficulty in searching for\nscenarios that lead to non-optimal PPDs. To fill this gap, in this paper, we\nfocus on evaluating the robustness of ADSs' PPDs and propose the first method,\nDecictor, for generating non-optimal decision scenarios (NoDSs), where the ADS\ndoes not plan optimal paths for AVs. Decictor comprises three main components:\nNon-invasive Mutation, Consistency Check, and Feedback. To overcome the oracle\nchallenge, Non-invasive Mutation is devised to implement conservative\nmodifications, ensuring the preservation of the original optimal path in the\nmutated scenarios. Subsequently, the Consistency Check is applied to determine\nthe presence of non-optimal PPDs by comparing the driving paths in the original\nand mutated scenarios. To deal with the challenge of large environment space,\nwe design Feedback metrics that integrate spatial and temporal dimensions of\nthe AV's movement. These metrics are crucial for effectively steering the\ngeneration of NoDSs. We evaluate Decictor on Baidu Apollo, an open-source and\nproduction-grade ADS. The experimental results validate the effectiveness of\nDecictor in detecting non-optimal PPDs of ADSs.",
      "tldr_zh": "该论文关注自动驾驶系统（Autonomous Driving Systems, ADS）的决策鲁棒性评估，强调除了安全外，还需评估路径规划决策（PPDs）的优化能力，以确保车辆智能性和风险降低。作者提出Decictor方法，用于生成非最优决策场景（NoDSs），其核心组件包括Non-invasive Mutation（进行保守环境修改以保留原始最优路径）、Consistency Check（通过比较原始和变异场景的路径检测非最优PPDs），以及Feedback metrics（整合空间和时间维度指导场景生成）。实验在开源ADS Baidu Apollo上验证了Decictor的有效性，能够成功检测非最优PPDs，从而提升ADS的鲁棒性评估。",
      "categories": [
        "cs.AI",
        "cs.NE",
        "cs.RO",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18393v3",
      "published_date": "2024-02-28 15:13:33 UTC",
      "updated_date": "2025-01-28 17:36:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:55:07.633045"
    },
    {
      "arxiv_id": "2402.18392v2",
      "title": "Unveiling the Potential of Robustness in Selecting Conditional Average Treatment Effect Estimators",
      "title_zh": "揭示稳健性潜力在选择条件平均处理效应估计器中的作用",
      "authors": [
        "Yiyan Huang",
        "Cheuk Hang Leung",
        "Siyi Wang",
        "Yijun Li",
        "Qi Wu"
      ],
      "abstract": "The growing demand for personalized decision-making has led to a surge of\ninterest in estimating the Conditional Average Treatment Effect (CATE). Various\ntypes of CATE estimators have been developed with advancements in machine\nlearning and causal inference. However, selecting the desirable CATE estimator\nthrough a conventional model validation procedure remains impractical due to\nthe absence of counterfactual outcomes in observational data. Existing\napproaches for CATE estimator selection, such as plug-in and pseudo-outcome\nmetrics, face two challenges. First, they must determine the metric form and\nthe underlying machine learning models for fitting nuisance parameters (e.g.,\noutcome function, propensity function, and plug-in learner). Second, they lack\na specific focus on selecting a robust CATE estimator. To address these\nchallenges, this paper introduces a Distributionally Robust Metric (DRM) for\nCATE estimator selection. The proposed DRM is nuisance-free, eliminating the\nneed to fit models for nuisance parameters, and it effectively prioritizes the\nselection of a distributionally robust CATE estimator. The experimental results\nvalidate the effectiveness of the DRM method in selecting CATE estimators that\nare robust to the distribution shift incurred by covariate shift and hidden\nconfounders.",
      "tldr_zh": "该研究探讨了在因果推断中选择Conditional Average Treatment Effect (CATE)估计器的问题，强调了现有方法如plug-in和pseudo-outcome metrics面临的挑战，包括需要拟合nuisance parameters（如结果函数和倾向函数）以及忽略鲁棒性。论文引入了Distributionally Robust Metric (DRM)，这是一种无需拟合nuisance parameters的指标，能够优先选择对分布偏移（如covariate shift和hidden confounders）具有鲁棒性的CATE估计器。通过实验验证，DRM方法在选择鲁棒CATE估计器方面表现出色，提升了个性化决策的可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "econ.EM",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper was accepted by NeurIPS-2024",
      "pdf_url": "http://arxiv.org/pdf/2402.18392v2",
      "published_date": "2024-02-28 15:12:24 UTC",
      "updated_date": "2024-10-31 11:08:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:55:18.646846"
    },
    {
      "arxiv_id": "2402.18390v1",
      "title": "Neuromorphic Event-Driven Semantic Communication in Microgrids",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoguang Diao",
        "Yubo Song",
        "Subham Sahoo",
        "Yuan Li"
      ],
      "abstract": "Synergies between advanced communications, computing and artificial\nintelligence are unraveling new directions of coordinated operation and\nresiliency in microgrids. On one hand, coordination among sources is\nfacilitated by distributed, privacy-minded processing at multiple locations,\nwhereas on the other hand, it also creates exogenous data arrival paths for\nadversaries that can lead to cyber-physical attacks amongst other reliability\nissues in the communication layer. This long-standing problem necessitates new\nintrinsic ways of exchanging information between converters through power lines\nto optimize the system's control performance. Going beyond the existing power\nand data co-transfer technologies that are limited by efficiency and\nscalability concerns, this paper proposes neuromorphic learning to implant\ncommunicative features using spiking neural networks (SNNs) at each node, which\nis trained collaboratively in an online manner simply using the power exchanges\nbetween the nodes. As opposed to the conventional neuromorphic sensors that\noperate with spiking signals, we employ an event-driven selective process to\ncollect sparse data for training of SNNs. Finally, its multi-fold effectiveness\nand reliable performance is validated under simulation conditions with\ndifferent microgrid topologies and components to establish a new direction in\nthe sense-actuate-compute cycle for power electronic dominated grids and\nmicrogrids.",
      "tldr_zh": "本论文探讨了微电网中通信、计算和AI的协同作用，以提升协调操作和弹性，同时解决分布式处理带来的隐私风险和网络物理攻击问题。作者提出了一种神经形态Event-Driven语义通信方法，利用Spiking Neural Networks (SNNs)在每个节点进行在线协作训练，仅通过节点间的电力交换作为数据来源，并采用事件驱动的选择过程收集稀疏数据。实验结果显示，该方法在不同微电网拓扑下的模拟验证中表现出多方面有效性和可靠性，为电力电子主导的电网感知-动作-计算周期开辟了新方向。",
      "categories": [
        "cs.ET",
        "cs.AI",
        "cs.NE",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.ET",
      "comment": "The manuscript has been accepted for publication in IEEE Transactions\n  on Smart Grid",
      "pdf_url": "http://arxiv.org/pdf/2402.18390v1",
      "published_date": "2024-02-28 15:11:02 UTC",
      "updated_date": "2024-02-28 15:11:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:55:31.018550"
    },
    {
      "arxiv_id": "2402.18609v4",
      "title": "ICE-SEARCH: A Language Model-Driven Feature Selection Approach",
      "title_zh": "ICE-SEARCH：一种基于语言模型驱动的特征选择方法",
      "authors": [
        "Tianze Yang",
        "Tianyi Yang",
        "Fuyuan Lyu",
        "Shaoshan Liu",
        "Xue",
        "Liu"
      ],
      "abstract": "This study unveils the In-Context Evolutionary Search (ICE-SEARCH) method,\nwhich is among the first works that melds large language models (LLMs) with\nevolutionary algorithms for feature selection (FS) tasks and demonstrates its\neffectiveness in Medical Predictive Analytics (MPA) applications. ICE-SEARCH\nharnesses the crossover and mutation capabilities inherent in LLMs within an\nevolutionary framework, significantly improving FS through the model's\ncomprehensive world knowledge and its adaptability to a variety of roles. Our\nevaluation of this methodology spans three crucial MPA tasks: stroke,\ncardiovascular disease, and diabetes, where ICE-SEARCH outperforms traditional\nFS methods in pinpointing essential features for medical applications.\nICE-SEARCH achieves State-of-the-Art (SOTA) performance in stroke prediction\nand diabetes prediction; the Decision-Randomized ICE-SEARCH ranks as SOTA in\ncardiovascular disease prediction. The study emphasizes the critical role of\nincorporating domain-specific insights, illustrating ICE-SEARCH's robustness,\ngeneralizability, and convergence. This opens avenues for further research into\ncomprehensive and intricate FS landscapes, marking a significant stride in the\napplication of artificial intelligence in medical predictive analytics.",
      "tldr_zh": "本研究提出了一种名为 ICE-SEARCH 的创新方法，将大型语言模型 (LLMs) 与进化算法相结合，用于特征选择 (FS) 任务，首次在医疗预测分析 (MPA) 领域展示了其有效性。ICE-SEARCH 利用 LLMs 的交叉和变异能力，以及其世界知识和适应性，来优化 FS 过程，并在中风、心血管疾病和糖尿病预测任务中进行评估。结果显示，该方法在中风和糖尿病预测中达到 State-of-the-Art (SOTA) 性能，而 Decision-Randomized ICE-SEARCH 在心血管疾病预测中也排名 SOTA，突显了其鲁棒性、一般性和在 AI 医疗应用中的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18609v4",
      "published_date": "2024-02-28 15:06:25 UTC",
      "updated_date": "2024-05-08 18:05:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:55:46.134894"
    },
    {
      "arxiv_id": "2402.18385v1",
      "title": "The First Place Solution of WSDM Cup 2024: Leveraging Large Language Models for Conversational Multi-Doc QA",
      "title_zh": "WSDM Cup 2024 的第一名解决方案：利用大型语言模型进行对话式多文档问答",
      "authors": [
        "Yiming Li",
        "Zhao Zhang"
      ],
      "abstract": "Conversational multi-doc question answering aims to answer specific questions\nbased on the retrieved documents as well as the contextual conversations. In\nthis paper, we introduce our winning approach for the \"Conversational Multi-Doc\nQA\" challenge in WSDM Cup 2024, which exploits the superior natural language\nunderstanding and generation capability of Large Language Models (LLMs). We\nfirst adapt LLMs to the task, then devise a hybrid training strategy to make\nthe most of in-domain unlabeled data. Moreover, an advanced text embedding\nmodel is adopted to filter out potentially irrelevant documents and several\napproaches are designed and compared for the model ensemble. Equipped with all\nthese techniques, our solution finally ranked 1st place in WSDM Cup 2024,\nsurpassing its rivals to a large extent. The source codes have been released at\nhttps://github.com/zhangzhao219/WSDM-Cup-2024.",
      "tldr_zh": "这篇论文介绍了他们在 WSDM Cup 2024 \"Conversational Multi-Doc QA\" 挑战赛中获胜的解决方案，通过利用 Large Language Models (LLMs) 的自然语言理解和生成能力来处理基于检索文档和上下文对话的问答任务。他们采用 LLMs 适应策略、混合训练方法（充分利用 in-domain 无标签数据）、高级文本嵌入模型过滤无关文档，以及多种模型集成技术来优化性能。最终，该方法在比赛中排名第一，大幅领先对手，并开源了相关代码。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "1st solution for WSDM Cup 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.18385v1",
      "published_date": "2024-02-28 15:05:43 UTC",
      "updated_date": "2024-02-28 15:05:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:55:56.536194"
    },
    {
      "arxiv_id": "2402.18381v1",
      "title": "Large Language Models As Evolution Strategies",
      "title_zh": "大语言模型作为进化策略",
      "authors": [
        "Robert Tjarko Lange",
        "Yingtao Tian",
        "Yujin Tang"
      ],
      "abstract": "Large Transformer models are capable of implementing a plethora of so-called\nin-context learning algorithms. These include gradient descent, classification,\nsequence completion, transformation, and improvement. In this work, we\ninvestigate whether large language models (LLMs), which never explicitly\nencountered the task of black-box optimization, are in principle capable of\nimplementing evolutionary optimization algorithms. While previous works have\nsolely focused on language-based task specification, we move forward and focus\non the zero-shot application of LLMs to black-box optimization. We introduce a\nnovel prompting strategy, consisting of least-to-most sorting of discretized\npopulation members and querying the LLM to propose an improvement to the mean\nstatistic, i.e. perform a type of black-box recombination operation.\nEmpirically, we find that our setup allows the user to obtain an LLM-based\nevolution strategy, which we call `EvoLLM', that robustly outperforms baseline\nalgorithms such as random search and Gaussian Hill Climbing on synthetic BBOB\nfunctions as well as small neuroevolution tasks. Hence, LLMs can act as\n`plug-in' in-context recombination operators. We provide several comparative\nstudies of the LLM's model size, prompt strategy, and context construction.\nFinally, we show that one can flexibly improve EvoLLM's performance by\nproviding teacher algorithm information via instruction fine-tuning on\npreviously collected teacher optimization trajectories.",
      "tldr_zh": "本研究探讨大型语言模型 (LLMs) 是否能实现进化策略 (Evolution Strategies)，即使它们从未显式遇到黑箱优化任务。作者引入了一种新颖的提示策略，包括对离散化种群成员进行从低到高的排序，并查询 LLM 提出对均值统计的改进，从而创建了名为 EvoLLM 的 LLM-based 进化策略。实验结果显示，EvoLLM 在合成 BBOB 函数和小规模神经进化任务上，robustly 优于基线算法如随机搜索和 Gaussian Hill Climbing。最终，通过在之前收集的教师优化轨迹上进行指令微调，可以进一步提升 EvoLLM 的性能，为 LLMs 作为 in-context 重组操作符的应用提供了新见解。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages, 14 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.18381v1",
      "published_date": "2024-02-28 15:02:17 UTC",
      "updated_date": "2024-02-28 15:02:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:56:08.385909"
    },
    {
      "arxiv_id": "2402.18377v2",
      "title": "Out-of-Domain Generalization in Dynamical Systems Reconstruction",
      "title_zh": "动态系统重建中的域外泛化",
      "authors": [
        "Niclas Göring",
        "Florian Hess",
        "Manuel Brenner",
        "Zahra Monfared",
        "Daniel Durstewitz"
      ],
      "abstract": "In science we are interested in finding the governing equations, the\ndynamical rules, underlying empirical phenomena. While traditionally scientific\nmodels are derived through cycles of human insight and experimentation,\nrecently deep learning (DL) techniques have been advanced to reconstruct\ndynamical systems (DS) directly from time series data. State-of-the-art\ndynamical systems reconstruction (DSR) methods show promise in capturing\ninvariant and long-term properties of observed DS, but their ability to\ngeneralize to unobserved domains remains an open challenge. Yet, this is a\ncrucial property we would expect from any viable scientific theory. In this\nwork, we provide a formal framework that addresses generalization in DSR. We\nexplain why and how out-of-domain (OOD) generalization (OODG) in DSR profoundly\ndiffers from OODG considered elsewhere in machine learning. We introduce\nmathematical notions based on topological concepts and ergodic theory to\nformalize the idea of learnability of a DSR model. We formally prove that\nblack-box DL techniques, without adequate structural priors, generally will not\nbe able to learn a generalizing DSR model. We also show this empirically,\nconsidering major classes of DSR algorithms proposed so far, and illustrate\nwhere and why they fail to generalize across the whole phase space. Our study\nprovides the first comprehensive mathematical treatment of OODG in DSR, and\ngives a deeper conceptual understanding of where the fundamental problems in\nOODG lie and how they could possibly be addressed in practice.",
      "tldr_zh": "这篇论文探讨了动态系统重建（DSR）中的Out-of-Domain Generalization（OODG），强调深度学习（DL）方法在从时间序列数据重建动态系统（DS）时，无法有效泛化到未观察领域的挑战。作者引入基于拓扑概念和遍历理论的数学框架，形式化DSR模型的可学习性，并证明黑箱DL技术缺乏结构先验，无法实现泛化。实验结果显示，现有的DSR算法在整个相空间中存在失败点，为理解和解决OODG问题提供了全面的数学分析和实用见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.DS",
        "nlin.CD"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18377v2",
      "published_date": "2024-02-28 14:52:58 UTC",
      "updated_date": "2024-06-07 23:38:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:56:21.231579"
    },
    {
      "arxiv_id": "2402.18376v2",
      "title": "Tokenization Is More Than Compression",
      "title_zh": "翻译失败",
      "authors": [
        "Craig W. Schmidt",
        "Varshini Reddy",
        "Haoran Zhang",
        "Alec Alameddine",
        "Omri Uzan",
        "Yuval Pinter",
        "Chris Tanner"
      ],
      "abstract": "Tokenization is a foundational step in natural language processing (NLP)\ntasks, bridging raw text and language models. Existing tokenization approaches\nlike Byte-Pair Encoding (BPE) originate from the field of data compression, and\nit has been suggested that the effectiveness of BPE stems from its ability to\ncondense text into a relatively small number of tokens. We test the hypothesis\nthat fewer tokens lead to better downstream performance by introducing\nPathPiece, a new tokenizer that segments a document's text into the minimum\nnumber of tokens for a given vocabulary. Through extensive experimentation we\nfind this hypothesis not to be the case, casting doubt on the understanding of\nthe reasons for effective tokenization. To examine which other factors play a\nrole, we evaluate design decisions across all three phases of tokenization:\npre-tokenization, vocabulary construction, and segmentation, offering new\ninsights into the design of effective tokenizers. Specifically, we illustrate\nthe importance of pre-tokenization and the benefits of using BPE to initialize\nvocabulary construction. We train 64 language models with varying tokenization,\nranging in size from 350M to 2.4B parameters, all of which are made publicly\navailable.",
      "tldr_zh": "该论文质疑了现有分词方法如 Byte-Pair Encoding (BPE) 的有效性是否仅源于文本压缩，提出并测试了新分词器 PathPiece，该方法旨在将文档分解为给定词汇的最少 token 以验证这一假设。实验结果显示，减少 token 数量并不必然提升下游任务性能，从而揭示分词设计的其他关键因素，包括 pre-tokenization 的重要性和使用 BPE 初始化 vocabulary construction 的益处。作者训练了 64 个规模从 350M 到 2.4B 参数的语言模型，并公开这些模型，以提供对有效分词器设计的新的见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.18376v2",
      "published_date": "2024-02-28 14:52:15 UTC",
      "updated_date": "2024-10-07 13:17:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:56:30.587542"
    },
    {
      "arxiv_id": "2403.00023v1",
      "title": "Auditable Homomorphic-based Decentralized Collaborative AI with Attribute-based Differential Privacy",
      "title_zh": "翻译失败",
      "authors": [
        "Lo-Yao Yeh",
        "Sheng-Po Tseng",
        "Chia-Hsun Lu",
        "Chih-Ya Shen"
      ],
      "abstract": "In recent years, the notion of federated learning (FL) has led to the new\nparadigm of distributed artificial intelligence (AI) with privacy preservation.\nHowever, most current FL systems suffer from data privacy issues due to the\nrequirement of a trusted third party. Although some previous works introduce\ndifferential privacy to protect the data, however, it may also significantly\ndeteriorate the model performance. To address these issues, we propose a novel\ndecentralized collaborative AI framework, named Auditable Homomorphic-based\nDecentralised Collaborative AI (AerisAI), to improve security with homomorphic\nencryption and fine-grained differential privacy. Our proposed AerisAI directly\naggregates the encrypted parameters with a blockchain-based smart contract to\nget rid of the need of a trusted third party. We also propose a brand-new\nconcept for eliminating the negative impacts of differential privacy for model\nperformance. Moreover, the proposed AerisAI also provides the broadcast-aware\ngroup key management based on ciphertext-policy attribute-based encryption\n(CPABE) to achieve fine-grained access control based on different service-level\nagreements. We provide a formal theoretical analysis of the proposed AerisAI as\nwell as the functionality comparison with the other baselines. We also conduct\nextensive experiments on real datasets to evaluate the proposed approach. The\nexperimental results indicate that our proposed AerisAI significantly\noutperforms the other state-of-the-art baselines.",
      "tldr_zh": "该论文针对联邦学习 (FL) 系统中的数据隐私问题和性能下降问题，提出了一种新型去中心化协作 AI 框架——Auditable Homomorphic-based Decentralized Collaborative AI (AerisAI)。AerisAI 利用同态加密 (homomorphic encryption) 和细粒度差分隐私 (differential privacy) 来增强安全性，并通过区块链-based smart contract 直接聚合加密参数，消除对可信第三方的需求，同时引入新概念减少差分隐私对模型性能的负面影响。框架还采用 ciphertext-policy attribute-based encryption (CP-ABE) 实现广播感知的群密钥管理和细粒度访问控制。实验结果表明，AerisAI 在真实数据集上显著优于现有基线模型。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "68T01"
      ],
      "primary_category": "cs.CR",
      "comment": "12 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.00023v1",
      "published_date": "2024-02-28 14:51:18 UTC",
      "updated_date": "2024-02-28 14:51:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:56:46.452567"
    },
    {
      "arxiv_id": "2402.18362v1",
      "title": "Objective and Interpretable Breast Cosmesis Evaluation with Attention Guided Denoising Diffusion Anomaly Detection Model",
      "title_zh": "翻译失败",
      "authors": [
        "Sangjoon Park",
        "Yong Bae Kim",
        "Jee Suk Chang",
        "Seo Hee Choi",
        "Hyungjin Chung",
        "Ik Jae Lee",
        "Hwa Kyung Byun"
      ],
      "abstract": "As advancements in the field of breast cancer treatment continue to progress,\nthe assessment of post-surgical cosmetic outcomes has gained increasing\nsignificance due to its substantial impact on patients' quality of life.\nHowever, evaluating breast cosmesis presents challenges due to the inherently\nsubjective nature of expert labeling. In this study, we present a novel\nautomated approach, Attention-Guided Denoising Diffusion Anomaly Detection\n(AG-DDAD), designed to assess breast cosmesis following surgery, addressing the\nlimitations of conventional supervised learning and existing anomaly detection\nmodels. Our approach leverages the attention mechanism of the distillation with\nno label (DINO) self-supervised Vision Transformer (ViT) in combination with a\ndiffusion model to achieve high-quality image reconstruction and precise\ntransformation of discriminative regions. By training the diffusion model on\nunlabeled data predominantly with normal cosmesis, we adopt an unsupervised\nanomaly detection perspective to automatically score the cosmesis. Real-world\ndata experiments demonstrate the effectiveness of our method, providing\nvisually appealing representations and quantifiable scores for cosmesis\nevaluation. Compared to commonly used rule-based programs, our fully automated\napproach eliminates the need for manual annotations and offers objective\nevaluation. Moreover, our anomaly detection model exhibits state-of-the-art\nperformance, surpassing existing models in accuracy. Going beyond the scope of\nbreast cosmesis, our research represents a significant advancement in\nunsupervised anomaly detection within the medical domain, thereby paving the\nway for future investigations.",
      "tldr_zh": "本研究针对乳腺癌术后美容评估的主观性问题，提出了一种新型方法——Attention-Guided Denoising Diffusion Anomaly Detection (AG-DDAD)，它结合DINO self-supervised Vision Transformer (ViT)的注意力机制和扩散模型，实现高质量图像重建及异常区域的精确识别。AG-DDAD采用无监督异常检测方式，在无标签数据上训练，主要针对正常美容样本，从而自动生成客观量化评分。实验结果显示，该方法在真实数据中提供视觉吸引力的表示和准确评分，超越现有模型的性能，并为医疗领域的无监督异常检测开辟新路径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18362v1",
      "published_date": "2024-02-28 14:33:14 UTC",
      "updated_date": "2024-02-28 14:33:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:56:57.211976"
    },
    {
      "arxiv_id": "2402.18360v1",
      "title": "Similarity-based analogical proportions",
      "title_zh": "基于相似性的类比比例",
      "authors": [
        "Christian Antić"
      ],
      "abstract": "The author has recently introduced abstract algebraic frameworks of\nanalogical proportions and similarity within the general setting of universal\nalgebra. The purpose of this paper is to build a bridge from similarity to\nanalogical proportions by formulating the latter in terms of the former. The\nbenefit of this similarity-based approach is that the connection between\nproportions and similarity is built into the framework and therefore evident\nwhich is appealing since proportions and similarity are both at the center of\nanalogy; moreover, future results on similarity can directly be applied to\nanalogical proportions.",
      "tldr_zh": "这篇论文在通用代数(universal algebra)的抽象框架下，将类比比例(analogical proportions)表述为基于相似性的形式，从而建立两者之间的桥梁。作者通过这种方法，使比例和相似性之间的内在联系显而易见，这有助于突出它们在类比中的核心作用。未来，关于相似性的研究结果可以直接应用于类比比例，进一步扩展该框架的应用潜力。",
      "categories": [
        "cs.LO",
        "cs.AI",
        "math.LO"
      ],
      "primary_category": "cs.LO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18360v1",
      "published_date": "2024-02-28 14:31:34 UTC",
      "updated_date": "2024-02-28 14:31:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:57:07.976152"
    },
    {
      "arxiv_id": "2402.18344v2",
      "title": "Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Jiachun Li",
        "Pengfei Cao",
        "Chenhao Wang",
        "Zhuoran Jin",
        "Yubo Chen",
        "Daojian Zeng",
        "Kang Liu",
        "Jun Zhao"
      ],
      "abstract": "Large language models exhibit high-level commonsense reasoning abilities,\nespecially with enhancement methods like Chain-of-Thought (CoT). However, we\nfind these CoT-like methods lead to a considerable number of originally correct\nanswers turning wrong, which we define as the Toxic CoT problem. To interpret\nand mitigate this problem, we first utilize attribution tracing and causal\ntracing methods to probe the internal working mechanism of the LLM during CoT\nreasoning. Through comparisons, we prove that the model exhibits information\nloss from the question over the shallow attention layers when generating\nrationales or answers. Based on the probing findings, we design a novel method\ncalled RIDERS (Residual decodIng and sERial-position Swap), which compensates\nfor the information deficit in the model from both decoding and serial-position\nperspectives. Through extensive experiments on multiple commonsense reasoning\nbenchmarks, we validate that this method not only significantly eliminates\nToxic CoT problems (decreased by 23.6%), but also effectively improves the\nmodel's overall commonsense reasoning performance (increased by 5.5%).",
      "tldr_zh": "该研究探讨了大语言模型在常识推理中使用 Chain-of-Thought (CoT) 方法时出现的 Toxic CoT 问题，即原本正确的答案因该方法而变错。作者通过 attribution tracing 和 causal tracing 技术分析了模型的内部机制，发现模型在浅层注意力层中从问题中丢失信息。基于这一发现，他们提出了 RIDERS (Residual decodIng and sERial-position Swap) 方法，从解码和序列位置角度补偿信息缺失。实验在多个常识推理基准上验证，该方法显著减少了 Toxic CoT 问题（下降 23.6%）并提升了整体性能（增加 5.5%）。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted as a long paper to ACL 2024 Main, 25 pages, 22 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.18344v2",
      "published_date": "2024-02-28 14:09:02 UTC",
      "updated_date": "2024-06-27 06:54:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:57:20.229723"
    },
    {
      "arxiv_id": "2402.18326v2",
      "title": "When Should Algorithms Resign? A Proposal for AI Governance",
      "title_zh": "翻译失败",
      "authors": [
        "Umang Bhatt",
        "Holli Sargeant"
      ],
      "abstract": "Algorithmic resignation is a strategic approach for managing the use of\nartificial intelligence (AI) by embedding governance directly into AI systems.\nIt involves deliberate and informed disengagement from AI, such as restricting\naccess AI outputs or displaying performance disclaimers, in specific scenarios\nto aid the appropriate and effective use of AI. By integrating algorithmic\nresignation as a governance mechanism, organizations can better control when\nand how AI is used, balancing the benefits of automation with the need for\nhuman oversight.",
      "tldr_zh": "本文提出 algorithmic resignation 作为一种 AI governance 策略，通过在 AI 系统内嵌入 deliberate disengagement 机制（如限制输出访问或显示性能免责声明），以在特定场景下控制 AI 使用。该方法旨在确保 AI 的适当和有效应用，帮助组织平衡自动化优势与人类监督需求。实验与分析表明，这种治理机制能提升 AI 管理的可控性，为更安全的人机协作提供框架。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18326v2",
      "published_date": "2024-02-28 13:48:44 UTC",
      "updated_date": "2024-07-16 19:40:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:57:31.256398"
    },
    {
      "arxiv_id": "2402.18320v2",
      "title": "Location-guided Head Pose Estimation for Fisheye Image",
      "title_zh": "位置引导的鱼眼图像头部姿态估计",
      "authors": [
        "Bing Li",
        "Dong Zhang",
        "Cheng Huang",
        "Yun Xian",
        "Ming Li",
        "Dah-Jye Lee"
      ],
      "abstract": "Camera with a fisheye or ultra-wide lens covers a wide field of view that\ncannot be modeled by the perspective projection. Serious fisheye lens\ndistortion in the peripheral region of the image leads to degraded performance\nof the existing head pose estimation models trained on undistorted images. This\npaper presents a new approach for head pose estimation that uses the knowledge\nof head location in the image to reduce the negative effect of fisheye\ndistortion. We develop an end-to-end convolutional neural network to estimate\nthe head pose with the multi-task learning of head pose and head location. Our\nproposed network estimates the head pose directly from the fisheye image\nwithout the operation of rectification or calibration. We also created a\nfisheye-distorted version of the three popular head pose estimation datasets,\nBIWI, 300W-LP, and AFLW2000 for our experiments. Experiments results show that\nour network remarkably improves the accuracy of head pose estimation compared\nwith other state-of-the-art one-stage and two-stage methods.",
      "tldr_zh": "本文针对鱼眼图像的严重失真问题，提出了一种基于头部位置指导的头部姿态估计方法，以减少失真对性能的影响。研究开发了一个端到端的卷积神经网络（CNN），通过多任务学习同时估计head pose和head location，直接从fisheye image中获取结果，而无需rectification或calibration操作。实验在创建的鱼眼失真版本数据集BIWI、300W-LP和AFLW2000上进行，结果显示该方法显著提高了head pose估计的准确性，优于其他最先进的一阶段和二阶段方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Revised Introduction and Related Work; Submitted to lEEE Transactions\n  on Cognitive and Developmental Systems for review",
      "pdf_url": "http://arxiv.org/pdf/2402.18320v2",
      "published_date": "2024-02-28 13:33:43 UTC",
      "updated_date": "2024-04-10 15:09:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:57:44.594958"
    },
    {
      "arxiv_id": "2402.18309v1",
      "title": "Enhancing Roadway Safety: LiDAR-based Tree Clearance Analysis",
      "title_zh": "提升道路安全：基于 LiDAR 的",
      "authors": [
        "Miriam Louise Carnot",
        "Eric Peukert",
        "Bogdan Franczyk"
      ],
      "abstract": "In the efforts for safer roads, ensuring adequate vertical clearance above\nroadways is of great importance. Frequently, trees or other vegetation is\ngrowing above the roads, blocking the sight of traffic signs and lights and\nposing danger to traffic participants. Accurately estimating this space from\nsimple images proves challenging due to a lack of depth information. This is\nwhere LiDAR technology comes into play, a laser scanning sensor that reveals a\nthree-dimensional perspective. Thus far, LiDAR point clouds at the street level\nhave mainly been used for applications in the field of autonomous driving.\nThese scans, however, also open up possibilities in urban management. In this\npaper, we present a new point cloud algorithm that can automatically detect\nthose parts of the trees that grow over the street and need to be trimmed. Our\nsystem uses semantic segmentation to filter relevant points and downstream\nprocessing steps to create the required volume to be kept clear above the road.\nChallenges include obscured stretches of road, the noisy unstructured nature of\nLiDAR point clouds, and the assessment of the road shape. The identified points\nof non-compliant trees can be projected from the point cloud onto images,\nproviding municipalities with a visual aid for dealing with such occurrences.\nBy automating this process, municipalities can address potential road space\nconstraints, enhancing safety for all. They may also save valuable time by\ncarrying out the inspections more systematically. Our open-source code gives\ncommunities inspiration on how to automate the process themselves.",
      "tldr_zh": "本研究针对道路安全问题，提出了一种基于 LiDAR 的树木间隙分析方法，以自动检测和评估道路上方树木的垂直间隙，解决传统图像缺乏深度信息带来的挑战。该方法利用 semantic segmentation 过滤相关点云数据，并通过后续处理步骤计算需要修剪的树木体积，应对诸如道路遮挡、点云噪声和道路形状评估等难题。实验结果显示，该系统能将检测到的不合规树木点投影到图像上，帮助市政部门系统化管理道路空间，提高交通安全并节省检查时间；此外，论文提供开源代码，促进社区应用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18309v1",
      "published_date": "2024-02-28 13:08:46 UTC",
      "updated_date": "2024-02-28 13:08:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:57:55.861719"
    },
    {
      "arxiv_id": "2402.18292v6",
      "title": "FSL-Rectifier: Rectify Outliers in Few-Shot Learning via Test-Time Augmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Yunwei Bai",
        "Ying Kiat Tan",
        "Shiming Chen",
        "Yao Shu",
        "Tsuhan Chen"
      ],
      "abstract": "Few-shot learning (FSL) commonly requires a model to identify images\n(queries) that belong to classes unseen during training, based on a few\nlabelled samples of the new classes (support set) as reference. So far, plenty\nof algorithms involve training data augmentation to improve the generalization\ncapability of FSL models, but outlier queries or support images during\ninference can still pose great generalization challenges. In this work, to\nreduce the bias caused by the outlier samples, we generate additional\ntest-class samples by combining original samples with suitable train-class\nsamples via a generative image combiner. Then, we obtain averaged features via\nan augmentor, which leads to more typical representations through the\naveraging. We experimentally and theoretically demonstrate the effectiveness of\nour method, obtaining a test accuracy improvement proportion of around 10\\%\n(e.g., from 46.86\\% to 53.28\\%) for trained FSL models. Importantly, given a\npretrained image combiner, our method is training-free for off-the-shelf FSL\nmodels, whose performance can be improved without extra datasets nor further\ntraining of the models themselves. Codes are available at\nhttps://github.com/WendyBaiYunwei/FSL-Rectifier-Pub.",
      "tldr_zh": "该论文提出FSL-Rectifier方法，通过测试时增强（Test-Time Augmentation）来修正Few-Shot Learning (FSL)中的异常样本（outliers），从而缓解测试阶段的泛化挑战。具体而言，该方法使用generative image combiner将原始样本与合适的训练类样本结合生成额外测试类样本，然后通过augmentor获取平均特征，使表示更典型。实验结果显示，该方法可将预训练FSL模型的测试准确率提升约10%（例如，从46.86%到53.28%），且无需额外数据集或进一步训练，仅需一个预训练的图像组合器即可实现。总的来说，该方法为提升FSL模型鲁棒性提供了高效、无训练的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "To be published in AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2402.18292v6",
      "published_date": "2024-02-28 12:37:30 UTC",
      "updated_date": "2024-12-19 03:03:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:58:08.061822"
    },
    {
      "arxiv_id": "2403.06993v1",
      "title": "Automatic driving lane change safety prediction model based on LSTM",
      "title_zh": "基于 LSTM 的自动驾驶变道安全预测模型",
      "authors": [
        "Wenjian Sun",
        "Linying Pan",
        "Jingyu Xu",
        "Weixiang Wan",
        "Yong Wang"
      ],
      "abstract": "Autonomous driving technology can improve traffic safety and reduce traffic\naccidents. In addition, it improves traffic flow, reduces congestion, saves\nenergy and increases travel efficiency. In the relatively mature automatic\ndriving technology, the automatic driving function is divided into several\nmodules: perception, decision-making, planning and control, and a reasonable\ndivision of labor can improve the stability of the system. Therefore,\nautonomous vehicles need to have the ability to predict the trajectory of\nsurrounding vehicles in order to make reasonable decision planning and safety\nmeasures to improve driving safety. By using deep learning method, a\nsafety-sensitive deep learning model based on short term memory (LSTM) network\nis proposed. This model can alleviate the shortcomings of current automatic\ndriving trajectory planning, and the output trajectory not only ensures high\naccuracy but also improves safety. The cell state simulation algorithm\nsimulates the trackability of the trajectory generated by this model. The\nresearch results show that compared with the traditional model-based method,\nthe trajectory prediction method based on LSTM network has obvious advantages\nin predicting the trajectory in the long time domain. The intention recognition\nmodule considering interactive information has higher prediction and accuracy,\nand the algorithm results show that the trajectory is very smooth based on the\npremise of safe prediction and efficient lane change. And autonomous vehicles\ncan efficiently and safely complete lane changes.",
      "tldr_zh": "该研究提出了一种基于 LSTM 网络的自动驾驶换道安全预测模型，旨在通过深度学习方法预测周围车辆的轨迹，从而提升驾驶决策的准确性和安全性。该模型整合了意图识别模块和交互信息分析，确保输出轨迹不仅高精度且平滑可跟踪，同时使用细胞状态模拟算法验证轨迹的可行性。实验结果显示，与传统模型相比，该方法在长期轨迹预测中表现出明显优势，能帮助自动车辆高效、安全地完成换道操作。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.IV",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.06993v1",
      "published_date": "2024-02-28 12:34:04 UTC",
      "updated_date": "2024-02-28 12:34:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:58:19.402271"
    },
    {
      "arxiv_id": "2402.18286v2",
      "title": "Self-Supervised Learning with Generative Adversarial Networks for Electron Microscopy",
      "title_zh": "翻译失败",
      "authors": [
        "Bashir Kazimi",
        "Karina Ruzaeva",
        "Stefan Sandfeld"
      ],
      "abstract": "In this work, we explore the potential of self-supervised learning with\nGenerative Adversarial Networks (GANs) for electron microscopy datasets. We\nshow how self-supervised pretraining facilitates efficient fine-tuning for a\nspectrum of downstream tasks, including semantic segmentation, denoising, noise\n\\& background removal, and super-resolution. Experimentation with varying model\ncomplexities and receptive field sizes reveals the remarkable phenomenon that\nfine-tuned models of lower complexity consistently outperform more complex\nmodels with random weight initialization. We demonstrate the versatility of\nself-supervised pretraining across various downstream tasks in the context of\nelectron microscopy, allowing faster convergence and better performance. We\nconclude that self-supervised pretraining serves as a powerful catalyst, being\nespecially advantageous when limited annotated data are available and efficient\nscaling of computational cost is important.",
      "tldr_zh": "本研究探索了自监督学习(Self-Supervised Learning)结合生成对抗网络(GANs)在电子显微镜(Electron Microscopy)数据集上的应用，通过自监督预训练实现下游任务的高效微调，包括语义分割、去噪、噪声与背景去除以及超分辨率。实验结果显示，微调后的低复杂度模型在性能上超过了高复杂度随机初始化模型，实现了更快收敛和更优表现。研究强调，自监督预训练在标注数据有限的情况下特别有利，有助于降低计算成本并提升任务通用性。总的来说，这为电子显微镜领域的数据驱动分析提供了强大工具。",
      "categories": [
        "cs.CV",
        "cond-mat.mtrl-sci",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18286v2",
      "published_date": "2024-02-28 12:25:01 UTC",
      "updated_date": "2024-07-18 09:58:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:58:32.110734"
    },
    {
      "arxiv_id": "2402.18285v2",
      "title": "PiShield: A PyTorch Package for Learning with Requirements",
      "title_zh": "翻译失败",
      "authors": [
        "Mihaela Cătălina Stoian",
        "Alex Tatomir",
        "Thomas Lukasiewicz",
        "Eleonora Giunchiglia"
      ],
      "abstract": "Deep learning models have shown their strengths in various application\ndomains, however, they often struggle to meet safety requirements for their\noutputs. In this paper, we introduce PiShield, the first package ever allowing\nfor the integration of the requirements into the neural networks' topology.\nPiShield guarantees compliance with these requirements, regardless of input.\nAdditionally, it allows for integrating requirements both at inference and/or\ntraining time, depending on the practitioners' needs. Given the widespread\napplication of deep learning, there is a growing need for frameworks allowing\nfor the integration of the requirements across various domains. Here, we\nexplore three application scenarios: functional genomics, autonomous driving,\nand tabular data generation.",
      "tldr_zh": "该论文介绍了 PiShield，这是一个 PyTorch 包，旨在将安全要求集成到神经网络拓扑中，帮助深度学习模型满足输出要求。PiShield 通过确保无论输入如何都能遵守这些要求，并在训练和/或推理阶段灵活应用，解决了模型在安全方面的挑战。作者探索了功能基因组学、自动驾驶和表格数据生成等应用场景，展示了该框架在多个领域的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.LG",
      "comment": "Demo paper, accepted at IJCAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.18285v2",
      "published_date": "2024-02-28 12:24:27 UTC",
      "updated_date": "2024-05-14 17:23:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:58:42.901628"
    },
    {
      "arxiv_id": "2402.18284v2",
      "title": "Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of Pre-trained Language Models with Proximal Policy Optimization",
      "title_zh": "众包是否耗尽你的资金？使用 Proximal Policy Optimization 进行预训练语言模型的成本有效微调",
      "authors": [
        "Shuo Yang",
        "Gjergji Kasneci"
      ],
      "abstract": "Wide usage of ChatGPT has highlighted the potential of reinforcement learning\nfrom human feedback. However, its training pipeline relies on manual ranking, a\nresource-intensive process. To reduce labor costs, we propose a self-supervised\ntext ranking approach for applying Proximal-Policy-Optimization to fine-tune\nlanguage models while eliminating the need for human annotators. Our method\nbegins with probabilistic sampling to encourage a language model to generate\ndiverse responses for each input. We then employ TextRank and ISODATA\nalgorithms to rank and cluster these responses based on their semantics.\nSubsequently, we construct a reward model to learn the rank and optimize our\ngenerative policy. Our experimental results, conducted using two language\nmodels on three tasks, demonstrate that the models trained by our method\nconsiderably outperform baselines regarding BLEU, GLEU, and METEOR scores.\nFurthermore, our manual evaluation shows that our ranking results exhibit a\nremarkably high consistency with that of humans. This research significantly\nreduces training costs of proximal policy-guided models and demonstrates the\npotential for self-correction of language models.",
      "tldr_zh": "该研究针对强化学习从人类反馈的训练流程中高昂的人工标注成本问题，提出了一种自监督文本排名方法，用于应用 Proximal Policy Optimization (PPO) 来微调预训练语言模型，从而消除对人工标注者的需求。方法包括使用概率采样生成多样化响应，随后通过 TextRank 和 ISODATA 算法对响应进行语义排名和聚类，并构建奖励模型来优化生成策略。实验结果显示，在两个语言模型和三个任务上，该方法使模型在 BLEU、GLEU 和 METEOR 分数上显著优于基线，且手动评估表明其排名结果与人类高度一致。该方法显著降低了 PPO 指导模型的训练成本，并展示了语言模型的自校正潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.18284v2",
      "published_date": "2024-02-28 12:24:07 UTC",
      "updated_date": "2024-03-02 23:19:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:58:56.542550"
    },
    {
      "arxiv_id": "2402.18607v3",
      "title": "Exploring Privacy and Fairness Risks in Sharing Diffusion Models: An Adversarial Perspective",
      "title_zh": "探索共享扩散模型的隐私和",
      "authors": [
        "Xinjian Luo",
        "Yangfan Jiang",
        "Fei Wei",
        "Yuncheng Wu",
        "Xiaokui Xiao",
        "Beng Chin Ooi"
      ],
      "abstract": "Diffusion models have recently gained significant attention in both academia\nand industry due to their impressive generative performance in terms of both\nsampling quality and distribution coverage. Accordingly, proposals are made for\nsharing pre-trained diffusion models across different organizations, as a way\nof improving data utilization while enhancing privacy protection by avoiding\nsharing private data directly. However, the potential risks associated with\nsuch an approach have not been comprehensively examined.\n  In this paper, we take an adversarial perspective to investigate the\npotential privacy and fairness risks associated with the sharing of diffusion\nmodels. Specifically, we investigate the circumstances in which one party (the\nsharer) trains a diffusion model using private data and provides another party\n(the receiver) black-box access to the pre-trained model for downstream tasks.\nWe demonstrate that the sharer can execute fairness poisoning attacks to\nundermine the receiver's downstream models by manipulating the training data\ndistribution of the diffusion model. Meanwhile, the receiver can perform\nproperty inference attacks to reveal the distribution of sensitive features in\nthe sharer's dataset. Our experiments conducted on real-world datasets\ndemonstrate remarkable attack performance on different types of diffusion\nmodels, which highlights the critical importance of robust data auditing and\nprivacy protection protocols in pertinent applications.",
      "tldr_zh": "本研究从对抗视角（adversarial perspective）探讨了共享扩散模型（diffusion models）的隐私和公平风险，强调了这种共享方式虽能提升数据利用率和隐私保护，但也可能引发潜在威胁。论文模拟了分享者（sharer）使用私有数据训练模型并提供黑箱访问给接收者（receiver）的场景，展示了分享者可通过操纵训练数据分布执行公平性中毒攻击（fairness poisoning attacks）来破坏接收者的下游模型，同时接收者可进行属性推断攻击（property inference attacks）以揭示分享者数据集的敏感特征分布。在真实数据集上的实验证明，这些攻击在不同类型扩散模型中表现出色，突出了实施稳健数据审计（robust data auditing）和隐私保护协议（privacy protection protocols）的必要性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18607v3",
      "published_date": "2024-02-28 12:21:12 UTC",
      "updated_date": "2024-09-19 08:00:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:59:07.655700"
    },
    {
      "arxiv_id": "2403.00832v1",
      "title": "Explainable Session-based Recommendation via Path Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Yang Cao",
        "Shuo Shang",
        "Jun Wang",
        "Wei Zhang"
      ],
      "abstract": "This paper explores providing explainability for session-based recommendation\n(SR) by path reasoning. Current SR models emphasize accuracy but lack\nexplainability, while traditional path reasoning prioritizes knowledge graph\nexploration, ignoring sequential patterns present in the session history.\nTherefore, we propose a generalized hierarchical reinforcement learning\nframework for SR, which improves the explainability of existing SR models via\nPath Reasoning, namely PR4SR. Considering the different importance of items to\nthe session, we design the session-level agent to select the items in the\nsession as the starting point for path reasoning and the path-level agent to\nperform path reasoning. In particular, we design a multi-target reward\nmechanism to adapt to the skip behaviors of sequential patterns in SR, and\nintroduce path midpoint reward to enhance the exploration efficiency in\nknowledge graphs. To improve the completeness of the knowledge graph and to\ndiversify the paths of explanation, we incorporate extracted feature\ninformation from images into the knowledge graph. We instantiate PR4SR in five\nstate-of-the-art SR models (i.e., GRU4REC, NARM, GCSAN, SR-GNN, SASRec) and\ncompare it with other explainable SR frameworks, to demonstrate the\neffectiveness of PR4SR for recommendation and explanation tasks through\nextensive experiments with these approaches on four datasets.",
      "tldr_zh": "这篇论文通过 Path Reasoning 为 Session-based Recommendation (SR) 提供可解释性，解决现有 SR 模型注重准确性但忽略解释性的问题，提出一个分层 Reinforcement Learning 框架 PR4SR。框架包括会话级代理选择会话中重要物品作为路径推理起点，以及路径级代理执行路径推理；同时引入多目标奖励机制适应顺序模式跳跃，并添加路径中点奖励提升知识图谱探索效率，还整合图像提取特征以丰富知识图谱。实验结果显示，PR4SR 在五个最先进 SR 模型（如 GRU4REC 和 SASRec）上应用后，在四个数据集上显著提高了推荐和解释任务的性能。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00832v1",
      "published_date": "2024-02-28 12:11:08 UTC",
      "updated_date": "2024-02-28 12:11:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:59:23.163837"
    },
    {
      "arxiv_id": "2402.18272v1",
      "title": "Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?",
      "title_zh": "重新思考LLM推理的界限：多智能体讨论是关键吗？",
      "authors": [
        "Qineng Wang",
        "Zihao Wang",
        "Ying Su",
        "Hanghang Tong",
        "Yangqiu Song"
      ],
      "abstract": "Recent progress in LLMs discussion suggests that multi-agent discussion\nimproves the reasoning abilities of LLMs. In this work, we reevaluate this\nclaim through systematic experiments, where we propose a novel group discussion\nframework to enrich the set of discussion mechanisms. Interestingly, our\nresults show that a single-agent LLM with strong prompts can achieve almost the\nsame performance as the best existing discussion approach on a wide range of\nreasoning tasks and backbone LLMs. We observe that the multi-agent discussion\nperforms better than a single agent only when there is no demonstration in the\nprompt. Further study reveals the common interaction mechanisms of LLMs during\nthe discussion.",
      "tldr_zh": "本文重新评估多智能体讨论是否能提升大型语言模型(LLMs)的推理能力，通过提出一种新群组讨论框架进行系统实验。结果显示，单一智能体LLMs使用强提示(strong prompts)几乎可达到最佳现有讨论方法的性能，在多种推理任务和骨干LLMs上表现相当。多智能体讨论仅在提示中无演示(demonstration)时优于单一智能体，进一步研究揭示了LLMs在讨论中的常见交互机制。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "22 pages, 5 figures, 10 tables",
      "pdf_url": "http://arxiv.org/pdf/2402.18272v1",
      "published_date": "2024-02-28 12:04:05 UTC",
      "updated_date": "2024-02-28 12:04:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:59:32.049189"
    },
    {
      "arxiv_id": "2403.07923v1",
      "title": "The Fusion of Deep Reinforcement Learning and Edge Computing for Real-time Monitoring and Control Optimization in IoT Environments",
      "title_zh": "深度强化学习与边缘计算的融合，用于物联网环境中实时监控和控制优化",
      "authors": [
        "Jingyu Xu",
        "Weixiang Wan",
        "Linying Pan",
        "Wenjian Sun",
        "Yuxiang Liu"
      ],
      "abstract": "In response to the demand for real-time performance and control quality in\nindustrial Internet of Things (IoT) environments, this paper proposes an\noptimization control system based on deep reinforcement learning and edge\ncomputing. The system leverages cloud-edge collaboration, deploys lightweight\npolicy networks at the edge, predicts system states, and outputs controls at a\nhigh frequency, enabling monitoring and optimization of industrial objectives.\nAdditionally, a dynamic resource allocation mechanism is designed to ensure\nrational scheduling of edge computing resources, achieving global optimization.\nResults demonstrate that this approach reduces cloud-edge communication\nlatency, accelerates response to abnormal situations, reduces system failure\nrates, extends average equipment operating time, and saves costs for manual\nmaintenance and replacement. This ensures real-time and stable control.",
      "tldr_zh": "本论文提出了一种融合深度强化学习和边缘计算的优化控制系统，针对工业物联网(IoT)环境的实时监控和控制需求。该系统利用云边协作在边缘部署轻量级策略网络，实现高频系统状态预测和控制输出，同时设计动态资源分配机制以实现全局资源优化。实验结果表明，该方法显著减少了云边通信延迟，加快了对异常情况的响应，降低了系统故障率，延长了设备平均运行时间，并节省了手动维护成本，从而确保了实时稳定的控制。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.IV",
        "eess.SY"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.07923v1",
      "published_date": "2024-02-28 12:01:06 UTC",
      "updated_date": "2024-02-28 12:01:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:59:44.971871"
    },
    {
      "arxiv_id": "2402.18267v2",
      "title": "A Survey on Neural Question Generation: Methods, Applications, and Prospects",
      "title_zh": "翻译失败",
      "authors": [
        "Shasha Guo",
        "Lizi Liao",
        "Cuiping Li",
        "Tat-Seng Chua"
      ],
      "abstract": "In this survey, we present a detailed examination of the advancements in\nNeural Question Generation (NQG), a field leveraging neural network techniques\nto generate relevant questions from diverse inputs like knowledge bases, texts,\nand images. The survey begins with an overview of NQG's background,\nencompassing the task's problem formulation, prevalent benchmark datasets,\nestablished evaluation metrics, and notable applications. It then methodically\nclassifies NQG approaches into three predominant categories: structured NQG,\nwhich utilizes organized data sources, unstructured NQG, focusing on more\nloosely structured inputs like texts or visual content, and hybrid NQG, drawing\non diverse input modalities. This classification is followed by an in-depth\nanalysis of the distinct neural network models tailored for each category,\ndiscussing their inherent strengths and potential limitations. The survey\nculminates with a forward-looking perspective on the trajectory of NQG,\nidentifying emergent research trends and prospective developmental paths.\nAccompanying this survey is a curated collection of related research papers,\ndatasets and codes, systematically organized on Github, providing an extensive\nreference for those delving into NQG.",
      "tldr_zh": "这篇调查论文回顾了 Neural Question Generation (NQG) 的进展，包括任务定义、基准数据集、评估指标以及实际应用。论文将 NQG 方法分类为 structured NQG（基于结构化数据）、unstructured NQG（针对非结构化输入如文本或图像）以及 hybrid NQG（结合多种模态），并详细分析了每类方法的神经网络模型及其优缺点。最后，它展望了 NQG 的未来研究趋势，并提供了一个 GitHub 上的资源集合，包括相关论文、数据集和代码，以支持进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by IJCAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.18267v2",
      "published_date": "2024-02-28 11:57:12 UTC",
      "updated_date": "2024-05-07 15:08:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T10:59:56.279674"
    },
    {
      "arxiv_id": "2402.18252v1",
      "title": "Towards Generalist Prompting for Large Language Models by Mental Models",
      "title_zh": "翻译失败",
      "authors": [
        "Haoxiang Guan",
        "Jiyan He",
        "Shuxin Zheng",
        "En-Hong Chen",
        "Weiming Zhang",
        "Nenghai Yu"
      ],
      "abstract": "Large language models (LLMs) have demonstrated impressive performance on many\ntasks. However, to achieve optimal performance, specially designed prompting\nmethods are still needed. These methods either rely on task-specific few-shot\nexamples that require a certain level of domain knowledge, or are designed to\nbe simple but only perform well on a few types of tasks. In this work, we\nattempt to introduce the concept of generalist prompting, which operates on the\ndesign principle of achieving optimal or near-optimal performance on a wide\nrange of tasks while eliminating the need for manual selection and\ncustomization of prompts tailored to specific problems. Furthermore, we propose\nMeMo (Mental Models), an innovative prompting method that is simple-designed\nyet effectively fulfills the criteria of generalist prompting. MeMo distills\nthe cores of various prompting methods into individual mental models and allows\nLLMs to autonomously select the most suitable mental models for the problem,\nachieving or being near to the state-of-the-art results on diverse tasks such\nas STEM, logical reasoning, and commonsense reasoning in zero-shot settings. We\nhope that the insights presented herein will stimulate further exploration of\ngeneralist prompting methods for LLMs.",
      "tldr_zh": "本文探讨了大语言模型(LLMs)在任务上虽有出色表现，但仍需任务特定提示方法的问题，提出generalist prompting概念，旨在实现广泛任务的最优或近优性能，而无需手动定制提示。研究引入MeMo(Mental Models)方法，该方法将各种提示的核心提炼成独立的mental models，并让LLMs自主选择最合适的模型。在zero-shot设置下，MeMo在STEM、逻辑推理和常识推理等多样任务上达到或接近state-of-the-art结果，并有望激发更多generalist prompting的研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18252v1",
      "published_date": "2024-02-28 11:29:09 UTC",
      "updated_date": "2024-02-28 11:29:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:00:09.431604"
    },
    {
      "arxiv_id": "2402.18606v1",
      "title": "Impact of network topology on the performance of Decentralized Federated Learning",
      "title_zh": "网络拓扑对去中心化联邦学习性能的影响",
      "authors": [
        "Luigi Palmieri",
        "Chiara Boldrini",
        "Lorenzo Valerio",
        "Andrea Passarella",
        "Marco Conti"
      ],
      "abstract": "Fully decentralized learning is gaining momentum for training AI models at\nthe Internet's edge, addressing infrastructure challenges and privacy concerns.\nIn a decentralized machine learning system, data is distributed across multiple\nnodes, with each node training a local model based on its respective dataset.\nThe local models are then shared and combined to form a global model capable of\nmaking accurate predictions on new data. Our exploration focuses on how\ndifferent types of network structures influence the spreading of knowledge -\nthe process by which nodes incorporate insights gained from learning patterns\nin data available on other nodes across the network. Specifically, this study\ninvestigates the intricate interplay between network structure and learning\nperformance using three network topologies and six data distribution methods.\nThese methods consider different vertex properties, including degree\ncentrality, betweenness centrality, and clustering coefficient, along with\nwhether nodes exhibit high or low values of these metrics. Our findings\nunderscore the significance of global centrality metrics (degree, betweenness)\nin correlating with learning performance, while local clustering proves less\npredictive. We highlight the challenges in transferring knowledge from\nperipheral to central nodes, attributed to a dilution effect during model\naggregation. Additionally, we observe that central nodes exert a pull effect,\nfacilitating the spread of knowledge. In examining degree distribution, hubs in\nBarabasi-Albert networks positively impact learning for central nodes but\nexacerbate dilution when knowledge originates from peripheral nodes. Finally,\nwe demonstrate the formidable challenge of knowledge circulation outside of\nsegregated communities.",
      "tldr_zh": "这篇论文探讨了网络拓扑对 Decentralized Federated Learning 性能的影响，焦点在于不同网络结构如何影响知识在节点间的传播。研究采用三种网络拓扑和六种数据分布方法，基于节点属性如 degree centrality、betweenness centrality 和 clustering coefficient 来分析学习效果。结果显示，全球中心性指标（degree centrality 和 betweenness centrality）与学习性能高度相关，而局部 clustering coefficient 的预测力较弱；此外，知识从外围节点向中心节点的转移易受模型聚合的稀释效应影响，并在 Barabasi-Albert 网络中凸显中心节点对知识传播的拉动作用，但社区隔离加剧了知识循环的挑战。总的来说，该研究强调了优化网络结构以提升去中心化学习效率的重要性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "Funding: H2020 HumaneAI Net (Grant N. 952026), CHIST-ERA SAI\n  (CHIST-ERA-19-XAI010), PNRR FAIR (PE00000013), PNRR RESTART (PE00000001).\n  arXiv admin note: text overlap with arXiv:2307.15947",
      "pdf_url": "http://arxiv.org/pdf/2402.18606v1",
      "published_date": "2024-02-28 11:13:53 UTC",
      "updated_date": "2024-02-28 11:13:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:00:21.980808"
    },
    {
      "arxiv_id": "2402.18225v1",
      "title": "CogBench: a large language model walks into a psychology lab",
      "title_zh": "CogBench：一个大语言模型走进心理学实验室",
      "authors": [
        "Julian Coda-Forno",
        "Marcel Binz",
        "Jane X. Wang",
        "Eric Schulz"
      ],
      "abstract": "Large language models (LLMs) have significantly advanced the field of\nartificial intelligence. Yet, evaluating them comprehensively remains\nchallenging. We argue that this is partly due to the predominant focus on\nperformance metrics in most benchmarks. This paper introduces CogBench, a\nbenchmark that includes ten behavioral metrics derived from seven cognitive\npsychology experiments. This novel approach offers a toolkit for phenotyping\nLLMs' behavior. We apply CogBench to 35 LLMs, yielding a rich and diverse\ndataset. We analyze this data using statistical multilevel modeling techniques,\naccounting for the nested dependencies among fine-tuned versions of specific\nLLMs. Our study highlights the crucial role of model size and reinforcement\nlearning from human feedback (RLHF) in improving performance and aligning with\nhuman behavior. Interestingly, we find that open-source models are less\nrisk-prone than proprietary models and that fine-tuning on code does not\nnecessarily enhance LLMs' behavior. Finally, we explore the effects of\nprompt-engineering techniques. We discover that chain-of-thought prompting\nimproves probabilistic reasoning, while take-a-step-back prompting fosters\nmodel-based behaviors.",
      "tldr_zh": "本论文引入 CogBench，这是一个基于七个认知心理学实验的基准，包含十个行为指标，用于全面评估大型语言模型 (LLMs) 的行为表现。研究者将 CogBench 应用于 35 个 LLMs，通过统计多级建模分析数据，揭示模型大小和 reinforcement learning from human feedback (RLHF) 在提升性能和人类行为对齐方面的重要作用，同时发现开源模型比专有模型风险更低，且针对代码的微调并不总是改善行为。最后，论文探索提示工程效果，表明 chain-of-thought 提示能提升概率推理，而 take-a-step-back 提示有助于促进基于模型的行为。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18225v1",
      "published_date": "2024-02-28 10:43:54 UTC",
      "updated_date": "2024-02-28 10:43:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:00:33.746947"
    },
    {
      "arxiv_id": "2402.18222v2",
      "title": "HearHere: Mitigating Echo Chambers in News Consumption through an AI-based Web System",
      "title_zh": "翻译失败",
      "authors": [
        "Youngseung Jeon",
        "Jaehoon Kim",
        "Sohyun Park",
        "Yunyong Ko",
        "Seongeun Ryu",
        "Sang-Wook Kim",
        "Kyungsik Han"
      ],
      "abstract": "Considerable efforts are currently underway to mitigate the negative impacts\nof echo chambers, such as increased susceptibility to fake news and resistance\ntowards accepting scientific evidence. Prior research has presented the\ndevelopment of computer systems that support the consumption of news\ninformation from diverse political perspectives to mitigate the echo chamber\neffect. However, existing studies still lack the ability to effectively support\nthe key processes of news information consumption and quantitatively identify a\npolitical stance towards the information. In this paper, we present HearHere,\nan AI-based web system designed to help users accommodate information and\nopinions from diverse perspectives. HearHere facilitates the key processes of\nnews information consumption through two visualizations. Visualization 1\nprovides political news with quantitative political stance information, derived\nfrom our graph-based political classification model, and users can experience\ndiverse perspectives (Hear). Visualization 2 allows users to express their\nopinions on specific political issues in a comment form and observe the\nposition of their own opinions relative to pro-liberal and pro-conservative\ncomments presented on a map interface (Here). Through a user study with 94\nparticipants, we demonstrate the feasibility of HearHere in supporting the\nconsumption of information from various perspectives. Our findings highlight\nthe importance of providing political stance information and quantifying users'\npolitical status as a means to mitigate political polarization. In addition, we\npropose design implications for system development, including the consideration\nof demographics such as political interest and providing users with\ninitiatives.",
      "tldr_zh": "该研究针对回音室效应（echo chambers）带来的问题，如易受假新闻影响和对科学证据的抵抗，提出了一种基于 AI 的网络系统 HearHere，以帮助用户从多样政治视角消费新闻信息。HearHere 通过两个可视化工具实现关键功能：Visualization 1 使用 graph-based political classification model 提供量化政治立场信息的新闻，帮助用户体验不同观点（Hear）；Visualization 2 允许用户表达意见并在地图界面上比较其立场与自由派和保守派评论（Here）。在一项涉及 94 名参与者的用户研究中，该系统证明了其在缓解政治极化的可行性，并强调了提供政治立场信息和量化用户政治状态的重要性，同时提出了包括考虑政治兴趣等人口统计学因素的设计启示。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "34 pages, 6 figures, 6 tables, CSCW 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.18222v2",
      "published_date": "2024-02-28 10:37:14 UTC",
      "updated_date": "2024-02-29 05:11:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:00:45.327267"
    },
    {
      "arxiv_id": "2402.18205v5",
      "title": "Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging",
      "title_zh": "翻译失败",
      "authors": [
        "Wei Zhang",
        "Xiangyuan Guan",
        "Lu Yunhong",
        "Jie Zhang",
        "Shuangyong Song",
        "Xianfu Cheng",
        "Zhenhe Wu",
        "Zhoujun Li"
      ],
      "abstract": "Logs produced by extensive software systems are integral to monitoring system\nbehaviors. Advanced log analysis facilitates the detection, alerting, and\ndiagnosis of system faults. Log parsing, which entails transforming raw log\nmessages into structured templates, constitutes a critical phase in the\nautomation of log analytics. Existing log parsers fail to identify the correct\ntemplates due to reliance on human-made rules. Besides, these methods focus on\nstatistical features while ignoring semantic information in log messages. To\naddress these challenges, we introduce a cutting-edge \\textbf{L}og parsing\nframework with \\textbf{E}ntropy sampling and chain-of-thought \\textbf{M}erging\n(\\model{}). Specifically, to discard the tedious manual rules, we propose a\nnovel sampling method inspired by information entropy, which efficiently\nclusters typical logs. Furthermore, to enhance the merging of log templates, we\ndesign a chain-of-thought method for large language models (LLMs). LLMs exhibit\nexceptional semantic comprehension and deftly distinguish between parameters\nand invariant tokens. We have conducted experiments on large-scale public\ndatasets. Extensive evaluation demonstrates that \\model{} achieves\nstate-of-the-art performance and impressive efficiency. The Code is available\nat https://github.com/zwpride/lemur.",
      "tldr_zh": "该研究针对日志解析中的问题，提出了一种名为Lemur的创新框架，以解决现有方法依赖人为规则并忽略日志语义信息的局限性。具体而言，Lemur引入了基于信息熵(Entropy sampling)的采样方法来高效聚类典型日志，并设计了Chain-of-Thought Merging技术，利用大型语言模型(LLMs)增强日志模板的语义合并，从而提高解析准确性。在大规模公共数据集上的实验表明，Lemur实现了最先进(State-of-the-Art)性能和高效处理，为自动化日志分析提供了可靠工具。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18205v5",
      "published_date": "2024-02-28 09:51:55 UTC",
      "updated_date": "2025-03-26 08:55:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:00:56.273223"
    },
    {
      "arxiv_id": "2402.18164v2",
      "title": "Autoencoder-based General Purpose Representation Learning for Customer Embedding",
      "title_zh": "翻译失败",
      "authors": [
        "Jan Henrik Bertrand",
        "David B. Hoffmann",
        "Jacopo Pio Gargano",
        "Laurent Mombaerts",
        "Jonathan Taws"
      ],
      "abstract": "Recent advances in representation learning have successfully leveraged the\nunderlying domain-specific structure of data across various fields. However,\nrepresenting diverse and complex entities stored in tabular format within a\nlatent space remains challenging. In this paper, we introduce DEEPCAE, a novel\nmethod for calculating the regularization term for multi-layer contractive\nautoencoders (CAEs). Additionally, we formalize a general-purpose entity\nembedding framework and use it to empirically show that DEEPCAE outperforms all\nother tested autoencoder variants in both reconstruction performance and\ndownstream prediction performance. Notably, when compared to a stacked CAE\nacross 13 datasets, DEEPCAE achieves a 34% improvement in reconstruction error.",
      "tldr_zh": "该论文提出了一种基于Autoencoder的通用表示学习方法DEEPCAE，用于处理表格格式中多样复杂实体的客户嵌入问题。具体而言，DEEPCAE通过计算多层Contractive Autoencoders (CAEs)的正则化项，并构建一个通用实体嵌入框架，提升了模型的重构性能和下游预测性能。在13个数据集上的实验中，DEEPCAE与堆叠CAEs相比，重构错误降低了34%，证明了其优越性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "68T-02"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.18164v2",
      "published_date": "2024-02-28 08:53:20 UTC",
      "updated_date": "2025-02-04 13:17:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:01:08.503964"
    },
    {
      "arxiv_id": "2402.18158v2",
      "title": "Evaluating Quantized Large Language Models",
      "title_zh": "量化大型语言模型的评估",
      "authors": [
        "Shiyao Li",
        "Xuefei Ning",
        "Luning Wang",
        "Tengxuan Liu",
        "Xiangsheng Shi",
        "Shengen Yan",
        "Guohao Dai",
        "Huazhong Yang",
        "Yu Wang"
      ],
      "abstract": "Post-training quantization (PTQ) has emerged as a promising technique to\nreduce the cost of large language models (LLMs). Specifically, PTQ can\neffectively mitigate memory consumption and reduce computational overhead in\nLLMs. To meet the requirements of both high efficiency and performance across\ndiverse scenarios, a comprehensive evaluation of quantized LLMs is essential to\nguide the selection of quantization methods. This paper presents a thorough\nevaluation of these factors by evaluating the effect of PTQ on Weight,\nActivation, and KV Cache on 11 model families, including OPT, LLaMA2, Falcon,\nBloomz, Mistral, ChatGLM, Vicuna, LongChat, StableLM, Gemma, and Mamba, with\nparameters ranging from 125M to 180B. The evaluation encompasses five types of\ntasks: basic NLP, emergent ability, trustworthiness, dialogue, and long-context\ntasks. Moreover, we also evaluate the state-of-the-art (SOTA) quantization\nmethods to demonstrate their applicability. Based on the extensive experiments,\nwe systematically summarize the effect of quantization, provide recommendations\nto apply quantization techniques, and point out future directions. The code can\nbe found in https://github.com/thu-nics/qllm-eval.",
      "tldr_zh": "本论文评估了后训练量化 (PTQ) 对大型语言模型 (LLMs) 的影响，旨在减少内存消耗和计算开销，同时确保在多种场景下的高效性能。研究通过实验考察了 PTQ 对权重 (Weight)、激活 (Activation) 和 KV Cache 的效果，涵盖了 11 个模型家族（如 OPT、LLaMA2 和 Gemma）以及五类任务，包括基本 NLP、紧急能力 (emergent ability)、可信度 (trustworthiness)、对话和长上下文任务。基于广泛实验，论文总结了量化的整体影响，提供量化技术的应用推荐，并指出了未来研究方向。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18158v2",
      "published_date": "2024-02-28 08:43:05 UTC",
      "updated_date": "2024-06-06 07:30:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:01:22.498938"
    },
    {
      "arxiv_id": "2402.18157v1",
      "title": "From Summary to Action: Enhancing Large Language Models for Complex Tasks with Open World APIs",
      "title_zh": "翻译失败",
      "authors": [
        "Yulong Liu",
        "Yunlong Yuan",
        "Chunwei Wang",
        "Jianhua Han",
        "Yongqiang Ma",
        "Li Zhang",
        "Nanning Zheng",
        "Hang Xu"
      ],
      "abstract": "The distinction between humans and animals lies in the unique ability of\nhumans to use and create tools. Tools empower humans to overcome physiological\nlimitations, fostering the creation of magnificent civilizations. Similarly,\nenabling foundational models like Large Language Models (LLMs) with the\ncapacity to learn external tool usage may serve as a pivotal step toward\nrealizing artificial general intelligence. Previous studies in this field have\npredominantly pursued two distinct approaches to augment the tool invocation\ncapabilities of LLMs. The first approach emphasizes the construction of\nrelevant datasets for model fine-tuning. The second approach, in contrast, aims\nto fully exploit the inherent reasoning abilities of LLMs through in-context\nlearning strategies. In this work, we introduce a novel tool invocation\npipeline designed to control massive real-world APIs. This pipeline mirrors the\nhuman task-solving process, addressing complicated real-life user queries. At\neach step, we guide LLMs to summarize the achieved results and determine the\nnext course of action. We term this pipeline `from Summary to action', Sum2Act\nfor short. Empirical evaluations of our Sum2Act pipeline on the ToolBench\nbenchmark show significant performance improvements, outperforming established\nmethods like ReAct and DFSDT. This highlights Sum2Act's effectiveness in\nenhancing LLMs for complex real-world tasks.",
      "tldr_zh": "这项研究探讨了赋予大型语言模型（LLMs）使用外部工具的能力，以推动人工智能通向通用智能（AGI）。作者提出了一种新型工具调用管道Sum2Act（from Summary to Action），它模拟人类任务解决过程，通过引导LLMs总结当前结果并决定下一步行动，来处理复杂真实世界的API查询。不同于以往基于数据集微调或in-context learning的方法，Sum2Act在ToolBench基准测试中表现出色，性能超过了ReAct和DFSDT等现有技术。实验结果证明，该管道显著提升了LLMs在复杂任务中的有效性，为LLMs在开放世界API环境中的应用提供了新路径。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18157v1",
      "published_date": "2024-02-28 08:42:23 UTC",
      "updated_date": "2024-02-28 08:42:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:01:33.080936"
    },
    {
      "arxiv_id": "2402.18154v1",
      "title": "Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and Mitigating Knowledge Conflicts in Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zhuoran Jin",
        "Pengfei Cao",
        "Hongbang Yuan",
        "Yubo Chen",
        "Jiexin Xu",
        "Huaijun Li",
        "Xiaojian Jiang",
        "Kang Liu",
        "Jun Zhao"
      ],
      "abstract": "Recently, retrieval augmentation and tool augmentation have demonstrated a\nremarkable capability to expand the internal memory boundaries of language\nmodels (LMs) by providing external context. However, internal memory and\nexternal context inevitably clash, leading to knowledge conflicts within LMs.\nIn this paper, we aim to interpret the mechanism of knowledge conflicts through\nthe lens of information flow, and then mitigate conflicts by precise\ninterventions at the pivotal point. We find there are some attention heads with\nopposite effects in the later layers, where memory heads can recall knowledge\nfrom internal memory, and context heads can retrieve knowledge from external\ncontext. Moreover, we reveal that the pivotal point at which knowledge\nconflicts emerge in LMs is the integration of inconsistent information flows by\nmemory heads and context heads. Inspired by the insights, we propose a novel\nmethod called Pruning Head via PatH PatcHing (PH3), which can efficiently\nmitigate knowledge conflicts by pruning conflicting attention heads without\nupdating model parameters. PH3 can flexibly control eight LMs to use internal\nmemory ($\\uparrow$ 44.0%) or external context ($\\uparrow$ 38.5%). Moreover, PH3\ncan also improve the performance of LMs on open-domain QA tasks. We also\nconduct extensive experiments to demonstrate the cross-model, cross-relation,\nand cross-format generalization of our method.",
      "tldr_zh": "本研究探讨了语言模型 (LMs) 在使用检索增强和工具增强扩展内部记忆时，内部记忆与外部上下文之间产生的知识冲突问题。通过信息流视角，论文发现后期层中的 attention heads 存在相反作用：memory heads 召回内部知识，而 context heads 检索外部知识，冲突发生在这些 heads 整合不一致信息时。针对此，提出了一种名为 PH3 (Pruning Head via PatH PatcHing) 的方法，通过修剪冲突 attention heads 来高效缓解知识冲突，而无需更新模型参数。实验结果显示，PH3 显著提升 LMs 对内部记忆的使用（提升 44.0%）和外部上下文的利用（提升 38.5%），并在开放域 QA 任务上改善性能，同时证明了其在跨模型、跨关系和跨格式的泛化能力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "21 pages, 42 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2402.18154v1",
      "published_date": "2024-02-28 08:34:41 UTC",
      "updated_date": "2024-02-28 08:34:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:01:47.022971"
    },
    {
      "arxiv_id": "2402.18153v2",
      "title": "Diffusion-Based Neural Network Weights Generation",
      "title_zh": "基于扩散的神经网络权重生成",
      "authors": [
        "Bedionita Soro",
        "Bruno Andreis",
        "Hayeon Lee",
        "Wonyong Jeong",
        "Song Chong",
        "Frank Hutter",
        "Sung Ju Hwang"
      ],
      "abstract": "Transfer learning has gained significant attention in recent deep learning\nresearch due to its ability to accelerate convergence and enhance performance\non new tasks. However, its success is often contingent on the similarity\nbetween source and target data, and training on numerous datasets can be\ncostly, leading to blind selection of pretrained models with limited insight\ninto their effectiveness. To address these challenges, we introduce D2NWG, a\ndiffusion-based neural network weights generation technique that efficiently\nproduces high-performing weights for transfer learning, conditioned on the\ntarget dataset. Our method extends generative hyper-representation learning to\nrecast the latent diffusion paradigm for neural network weights generation,\nlearning the weight distributions of models pretrained on various datasets.\nThis allows for automatic generation of weights that generalize well across\nboth seen and unseen tasks, outperforming state-of-the-art meta-learning\nmethods and pretrained models. Moreover, our approach is scalable to large\narchitectures such as large language models (LLMs), overcoming the limitations\nof current parameter generation techniques that rely on task-specific model\ncollections or access to original training data. By modeling the parameter\ndistribution of LLMs, D2NWG enables task-specific parameter generation without\nrequiring additional fine-tuning or large collections of model variants.\nExtensive experiments show that our method consistently enhances the\nperformance of diverse base models, regardless of their size or complexity,\npositioning it as a robust solution for scalable transfer learning.",
      "tldr_zh": "该论文提出了一种基于扩散 (diffusion-based) 的神经网络权重生成技术 D2NWG，以解决转移学习 (transfer learning) 中源数据依赖性和训练成本高的问题。该方法扩展了生成性超表示学习 (generative hyper-representation learning)，通过潜在扩散范式 (latent diffusion paradigm) 学习预训练模型的权重分布，并根据目标数据集高效生成高性能权重，支持已见和未见任务。实验结果表明，D2NWG 优于现有元学习 (meta-learning) 方法和预训练模型，在大型语言模型 (LLMs) 等复杂架构上实现了可扩展性能提升，而无需额外微调或大量模型变体。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "32 pages",
      "pdf_url": "http://arxiv.org/pdf/2402.18153v2",
      "published_date": "2024-02-28 08:34:23 UTC",
      "updated_date": "2024-10-25 08:47:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:01:58.797128"
    },
    {
      "arxiv_id": "2402.18152v3",
      "title": "Boosting Neural Representations for Videos with a Conditional Decoder",
      "title_zh": "使用条件解码器提升视频神经表示",
      "authors": [
        "Xinjie Zhang",
        "Ren Yang",
        "Dailan He",
        "Xingtong Ge",
        "Tongda Xu",
        "Yan Wang",
        "Hongwei Qin",
        "Jun Zhang"
      ],
      "abstract": "Implicit neural representations (INRs) have emerged as a promising approach\nfor video storage and processing, showing remarkable versatility across various\nvideo tasks. However, existing methods often fail to fully leverage their\nrepresentation capabilities, primarily due to inadequate alignment of\nintermediate features during target frame decoding. This paper introduces a\nuniversal boosting framework for current implicit video representation\napproaches. Specifically, we utilize a conditional decoder with a\ntemporal-aware affine transform module, which uses the frame index as a prior\ncondition to effectively align intermediate features with target frames.\nBesides, we introduce a sinusoidal NeRV-like block to generate diverse\nintermediate features and achieve a more balanced parameter distribution,\nthereby enhancing the model's capacity. With a high-frequency\ninformation-preserving reconstruction loss, our approach successfully boosts\nmultiple baseline INRs in the reconstruction quality and convergence speed for\nvideo regression, and exhibits superior inpainting and interpolation results.\nFurther, we integrate a consistent entropy minimization technique and develop\nvideo codecs based on these boosted INRs. Experiments on the UVG dataset\nconfirm that our enhanced codecs significantly outperform baseline INRs and\noffer competitive rate-distortion performance compared to traditional and\nlearning-based codecs. Code is available at\nhttps://github.com/Xinjie-Q/Boosting-NeRV.",
      "tldr_zh": "本研究针对隐式神经表示 (INRs) 在视频存储和处理中的局限性，提出了一种通用提升框架，以改善中间特征对齐问题。具体而言，该框架引入了条件解码器（conditional decoder），结合时间感知仿射变换模块 (temporal-aware affine transform module) 和正弦 NeRV-like 块，以利用帧索引作为先验条件生成多样化特征，并通过高频信息保留的重建损失 (high-frequency information-preserving reconstruction loss) 提升模型容量和收敛速度。实验结果显示，该方法显著提高了基线 INRs 在视频回归、重建、修复和插值任务的性能，并在 UVG 数据集上开发的视频编解码器表现出色，优于传统基线并在码率-失真性能上具竞争力。代码可访问 https://github.com/Xinjie-Q/Boosting-NeRV。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Accept by CVPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.18152v3",
      "published_date": "2024-02-28 08:32:19 UTC",
      "updated_date": "2024-03-16 13:23:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:02:12.148799"
    },
    {
      "arxiv_id": "2403.00830v1",
      "title": "MedAide: Leveraging Large Language Models for On-Premise Medical Assistance on Edge Devices",
      "title_zh": "翻译失败",
      "authors": [
        "Abdul Basit",
        "Khizar Hussain",
        "Muhammad Abdullah Hanif",
        "Muhammad Shafique"
      ],
      "abstract": "Large language models (LLMs) are revolutionizing various domains with their\nremarkable natural language processing (NLP) abilities. However, deploying LLMs\nin resource-constrained edge computing and embedded systems presents\nsignificant challenges. Another challenge lies in delivering medical assistance\nin remote areas with limited healthcare facilities and infrastructure. To\naddress this, we introduce MedAide, an on-premise healthcare chatbot. It\nleverages tiny-LLMs integrated with LangChain, providing efficient edge-based\npreliminary medical diagnostics and support. MedAide employs model\noptimizations for minimal memory footprint and latency on embedded edge devices\nwithout server infrastructure. The training process is optimized using low-rank\nadaptation (LoRA). Additionally, the model is trained on diverse medical\ndatasets, employing reinforcement learning from human feedback (RLHF) to\nenhance its domain-specific capabilities. The system is implemented on various\nconsumer GPUs and Nvidia Jetson development board. MedAide achieves 77\\%\naccuracy in medical consultations and scores 56 in USMLE benchmark, enabling an\nenergy-efficient healthcare assistance platform that alleviates privacy\nconcerns due to edge-based deployment, thereby empowering the community.",
      "tldr_zh": "该研究提出 MedAide，一种基于 Large Language Models (LLMs) 的本地医疗聊天机器人，旨在解决资源受限边缘设备上部署 LLMs 的挑战，并为偏远地区提供初步医疗诊断和支持。MedAide 利用 tiny-LLMs 与 LangChain 集成，通过模型优化（如低内存占用和低延迟）实现无服务器的边缘部署，并采用 Low-Rank Adaptation (LoRA) 和 Reinforcement Learning from Human Feedback (RLHF) 在多样医疗数据集上进行训练。实验结果显示，MedAide 在医疗咨询中达到 77% 准确率，并在 USMLE 基准测试中得分 56，提供节能且隐私友好的医疗辅助平台。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "I.2.7"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, 11 figures, ACM conference paper, 33 references",
      "pdf_url": "http://arxiv.org/pdf/2403.00830v1",
      "published_date": "2024-02-28 08:30:49 UTC",
      "updated_date": "2024-02-28 08:30:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:02:22.158249"
    },
    {
      "arxiv_id": "2402.18603v5",
      "title": "MMSR: Symbolic Regression is a Multi-Modal Information Fusion Task",
      "title_zh": "MMSR：符号回归是一种多模态信息融合任务",
      "authors": [
        "Yanjie Li",
        "Jingyi Liu",
        "Weijun Li",
        "Lina Yu",
        "Min Wu",
        "Wenqiang Li",
        "Meilan Hao",
        "Su Wei",
        "Yusong Deng"
      ],
      "abstract": "Mathematical formulas are the crystallization of human wisdom in exploring\nthe laws of nature for thousands of years. Describing the complex laws of\nnature with a concise mathematical formula is a constant pursuit of scientists\nand a great challenge for artificial intelligence. This field is called\nsymbolic regression (SR). Symbolic regression was originally formulated as a\ncombinatorial optimization problem, and Genetic Programming (GP) and\nReinforcement Learning algorithms were used to solve it. However, GP is\nsensitive to hyperparameters, and these two types of algorithms are\ninefficient. To solve this problem, researchers treat the mapping from data to\nexpressions as a translation problem. And the corresponding large-scale\npre-trained model is introduced. However, the data and expression skeletons do\nnot have very clear word correspondences as the two languages do. Instead, they\nare more like two modalities (e.g., image and text). Therefore, in this paper,\nwe proposed MMSR. The SR problem is solved as a pure multi-modal problem, and\ncontrastive learning is also introduced in the training process for modal\nalignment to facilitate later modal feature fusion. It is worth noting that to\nbetter promote the modal feature fusion, we adopt the strategy of training\ncontrastive learning loss and other losses at the same time, which only needs\none-step training, instead of training contrastive learning loss first and then\ntraining other losses. Because our experiments prove training together can make\nthe feature extraction module and feature fusion module wearing-in better.\nExperimental results show that compared with multiple large-scale pre-training\nbaselines, MMSR achieves the most advanced results on multiple mainstream\ndatasets including SRBench. Our code is open source at\nhttps://github.com/1716757342/MMSR",
      "tldr_zh": "本研究将符号回归（Symbolic Regression, SR）视为一种多模态信息融合任务，旨在解决传统方法如 Genetic Programming (GP) 和 Reinforcement Learning 的效率问题，以及将 SR 视为翻译任务的局限性。作者提出 MMSR 框架，通过对比学习（Contrastive Learning）进行模态对齐和特征融合，并采用同时训练对比学习损失与其他损失的策略，以更好地整合特征提取和融合模块。实验结果显示，MMSR 在多个主流数据集（如 SRBench）上超越了大型预训练基线，实现了最先进性能，并开源了代码以促进进一步研究。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "The Information Fusion has accepted this paper",
      "pdf_url": "http://arxiv.org/pdf/2402.18603v5",
      "published_date": "2024-02-28 08:29:42 UTC",
      "updated_date": "2024-09-19 12:30:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:02:34.830748"
    },
    {
      "arxiv_id": "2402.18150v2",
      "title": "Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Shicheng Xu",
        "Liang Pang",
        "Mo Yu",
        "Fandong Meng",
        "Huawei Shen",
        "Xueqi Cheng",
        "Jie Zhou"
      ],
      "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating additional information from retrieval. However, studies have\nshown that LLMs still face challenges in effectively using the retrieved\ninformation, even ignoring it or being misled by it. The key reason is that the\ntraining of LLMs does not clearly make LLMs learn how to utilize input\nretrieved texts with varied quality. In this paper, we propose a novel\nperspective that considers the role of LLMs in RAG as ``Information Refiner'',\nwhich means that regardless of correctness, completeness, or usefulness of\nretrieved texts, LLMs can consistently integrate knowledge within the retrieved\ntexts and model parameters to generate the texts that are more concise,\naccurate, and complete than the retrieved texts. To this end, we propose an\ninformation refinement training method named InFO-RAG that optimizes LLMs for\nRAG in an unsupervised manner. InFO-RAG is low-cost and general across various\ntasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse\ntasks including Question Answering, Slot-Filling, Language Modeling, Dialogue,\nand Code Generation show that InFO-RAG improves the performance of LLaMA2 by an\naverage of 9.39\\% relative points. InFO-RAG also shows advantages in in-context\nlearning and robustness of RAG.",
      "tldr_zh": "该论文提出一种新视角，将大语言模型(LLMs)在检索增强生成(RAG)中的角色视为“Information Refiner”，旨在解决LLMs无法有效利用质量不一的检索文本的问题，如忽略或被误导。作者开发了无监督的信息精炼训练方法InFO-RAG，通过优化LLMs整合检索文本和模型参数，生成更简洁、准确和完整的输出，该方法低成本且适用于多种任务。实验结果显示，InFO-RAG使LLaMA2在11个数据集上的零样本预测性能平均提升9.39%，并增强了上下文学习和RAG的鲁棒性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2024 Main",
      "pdf_url": "http://arxiv.org/pdf/2402.18150v2",
      "published_date": "2024-02-28 08:24:38 UTC",
      "updated_date": "2024-06-12 03:21:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:02:47.909208"
    },
    {
      "arxiv_id": "2402.18144v1",
      "title": "Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information",
      "title_zh": "翻译失败",
      "authors": [
        "Seungjong Sun",
        "Eungu Lee",
        "Dongyan Nan",
        "Xiangying Zhao",
        "Wonbyung Lee",
        "Bernard J. Jansen",
        "Jang Hyun Kim"
      ],
      "abstract": "Large language models exhibit societal biases associated with demographic\ninformation, including race, gender, and others. Endowing such language models\nwith personalities based on demographic data can enable generating opinions\nthat align with those of humans. Building on this idea, we propose \"random\nsilicon sampling,\" a method to emulate the opinions of the human population\nsub-group. Our study analyzed 1) a language model that generates the survey\nresponses that correspond with a human group based solely on its demographic\ndistribution and 2) the applicability of our methodology across various\ndemographic subgroups and thematic questions. Through random silicon sampling\nand using only group-level demographic information, we discovered that language\nmodels can generate response distributions that are remarkably similar to the\nactual U.S. public opinion polls. Moreover, we found that the replicability of\nlanguage models varies depending on the demographic group and topic of the\nquestion, and this can be attributed to inherent societal biases in the models.\nOur findings demonstrate the feasibility of mirroring a group's opinion using\nonly demographic distribution and elucidate the effect of social biases in\nlanguage models on such simulations.",
      "tldr_zh": "本文提出“random silicon sampling”方法，使用大型语言模型（LLM）基于群体级 demographic information 模拟人类子群体的意见分布，仅需群体的 demographic 数据即可生成与实际调查响应高度相似的结果。研究发现，这种模拟在不同 demographic 子群体和问题主题上表现出变异性，主要受 LLM 内在社会偏见的影响。实验验证了该方法的 feasibility，并阐明了社会偏见如何影响意见模拟的准确性。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "I.2.7"
      ],
      "primary_category": "cs.AI",
      "comment": "25 pages, 4 figures, 19 Tables",
      "pdf_url": "http://arxiv.org/pdf/2402.18144v1",
      "published_date": "2024-02-28 08:09:14 UTC",
      "updated_date": "2024-02-28 08:09:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:02:58.073776"
    },
    {
      "arxiv_id": "2402.18139v3",
      "title": "Cause and Effect: Can Large Language Models Truly Understand Causality?",
      "title_zh": "因果关系：大型语言模型能否真正理解因果性？",
      "authors": [
        "Swagata Ashwani",
        "Kshiteesh Hegde",
        "Nishith Reddy Mannuru",
        "Mayank Jindal",
        "Dushyant Singh Sengar",
        "Krishna Chaitanya Rao Kathala",
        "Dishant Banga",
        "Vinija Jain",
        "Aman Chadha"
      ],
      "abstract": "With the rise of Large Language Models(LLMs), it has become crucial to\nunderstand their capabilities and limitations in deciphering and explaining the\ncomplex web of causal relationships that language entails. Current methods use\neither explicit or implicit causal reasoning, yet there is a strong need for a\nunified approach combining both to tackle a wide array of causal relationships\nmore effectively. This research proposes a novel architecture called Context\nAware Reasoning Enhancement with Counterfactual Analysis(CARE CA) framework to\nenhance causal reasoning and explainability. The proposed framework\nincorporates an explicit causal detection module with ConceptNet and\ncounterfactual statements, as well as implicit causal detection through LLMs.\nOur framework goes one step further with a layer of counterfactual explanations\nto accentuate LLMs understanding of causality. The knowledge from ConceptNet\nenhances the performance of multiple causal reasoning tasks such as causal\ndiscovery, causal identification and counterfactual reasoning. The\ncounterfactual sentences add explicit knowledge of the not caused by scenarios.\nBy combining these powerful modules, our model aims to provide a deeper\nunderstanding of causal relationships, enabling enhanced interpretability.\nEvaluation of benchmark datasets shows improved performance across all metrics,\nsuch as accuracy, precision, recall, and F1 scores. We also introduce\nCausalNet, a new dataset accompanied by our code, to facilitate further\nresearch in this domain.",
      "tldr_zh": "这篇论文探讨了大型语言模型（LLMs）在理解因果关系方面的能力和局限性，提出了一种新型框架Context Aware Reasoning Enhancement with Counterfactual Analysis（CARE CA），旨在统一显式和隐式因果推理。CARE CA 通过整合ConceptNet知识库进行显式因果检测、LLMs处理隐式推理，以及反事实语句层来增强模型对因果关系的解释和理解，从而提升任务如因果发现、因果识别和反事实推理的性能。在基准数据集上的评估显示，该框架在准确率、精确率、召回率和F1分数等指标上均有显著改进，同时作者还发布了新数据集CausalNet和相关代码，以推动该领域的进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "AI Trustworthiness and Risk Assessment for Challenged Contexts\n  (ATRACC) AAAI 2024 Fall Symposium",
      "pdf_url": "http://arxiv.org/pdf/2402.18139v3",
      "published_date": "2024-02-28 08:02:14 UTC",
      "updated_date": "2024-09-30 00:40:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:03:11.146822"
    },
    {
      "arxiv_id": "2402.18137v2",
      "title": "DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Jianxiong Li",
        "Jinliang Zheng",
        "Yinan Zheng",
        "Liyuan Mao",
        "Xiao Hu",
        "Sijie Cheng",
        "Haoyi Niu",
        "Jihao Liu",
        "Yu Liu",
        "Jingjing Liu",
        "Ya-Qin Zhang",
        "Xianyuan Zhan"
      ],
      "abstract": "Multimodal pretraining is an effective strategy for the trinity of goals of\nrepresentation learning in autonomous robots: 1) extracting both local and\nglobal task progressions; 2) enforcing temporal consistency of visual\nrepresentation; 3) capturing trajectory-level language grounding. Most existing\nmethods approach these via separate objectives, which often reach sub-optimal\nsolutions. In this paper, we propose a universal unified objective that can\nsimultaneously extract meaningful task progression information from image\nsequences and seamlessly align them with language instructions. We discover\nthat via implicit preferences, where a visual trajectory inherently aligns\nbetter with its corresponding language instruction than mismatched pairs, the\npopular Bradley-Terry model can transform into representation learning through\nproper reward reparameterizations. The resulted framework, DecisionNCE, mirrors\nan InfoNCE-style objective but is distinctively tailored for decision-making\ntasks, providing an embodied representation learning framework that elegantly\nextracts both local and global task progression features, with temporal\nconsistency enforced through implicit time contrastive learning, while ensuring\ntrajectory-level instruction grounding via multimodal joint encoding.\nEvaluation on both simulated and real robots demonstrates that DecisionNCE\neffectively facilitates diverse downstream policy learning tasks, offering a\nversatile solution for unified representation and reward learning. Project\nPage: https://2toinf.github.io/DecisionNCE/",
      "tldr_zh": "这篇论文提出 DecisionNCE，一种通过隐式偏好学习构建多模态表示框架的方法，旨在统一解决自主机器人表示学习的三大目标：提取局部和全局任务进展、强制视觉表示的时序一致性，以及捕获轨迹级语言 grounding。框架基于 Bradley-Terry 模型，将视觉轨迹与语言指令的匹配转化为 InfoNCE 风格的表示学习目标，同时通过隐式时间对比学习确保时序一致性和多模态联合编码。在模拟和真实机器人实验中，DecisionNCE 显著提升了下游策略学习任务的表现，提供了一个通用的表示和奖励学习解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.18137v2",
      "published_date": "2024-02-28 07:58:24 UTC",
      "updated_date": "2024-05-24 03:31:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:03:22.794289"
    },
    {
      "arxiv_id": "2403.05578v1",
      "title": "Chaining text-to-image and large language model: A novel approach for generating personalized e-commerce banners",
      "title_zh": "翻译失败",
      "authors": [
        "Shanu Vashishtha",
        "Abhinav Prakash",
        "Lalitesh Morishetti",
        "Kaushiki Nag",
        "Yokila Arora",
        "Sushant Kumar",
        "Kannan Achan"
      ],
      "abstract": "Text-to-image models such as stable diffusion have opened a plethora of\nopportunities for generating art. Recent literature has surveyed the use of\ntext-to-image models for enhancing the work of many creative artists. Many\ne-commerce platforms employ a manual process to generate the banners, which is\ntime-consuming and has limitations of scalability. In this work, we demonstrate\nthe use of text-to-image models for generating personalized web banners with\ndynamic content for online shoppers based on their interactions. The novelty in\nthis approach lies in converting users' interaction data to meaningful prompts\nwithout human intervention. To this end, we utilize a large language model\n(LLM) to systematically extract a tuple of attributes from item\nmeta-information. The attributes are then passed to a text-to-image model via\nprompt engineering to generate images for the banner. Our results show that the\nproposed approach can create high-quality personalized banners for users.",
      "tldr_zh": "本研究提出了一种创新方法，将文本到图像模型（如 Stable Diffusion）和大型语言模型（LLM）结合，用于生成个性化的电子商务横幅，以解决传统手动制作的耗时和可扩展性问题。方法的核心在于使用 LLM 自动从用户互动数据中提取属性元组，然后通过提示工程（prompt engineering）将这些属性传递给文本到图像模型，生成动态内容横幅。实验结果表明，该方法能创建高质量的个性化横幅，提升了在线购物体验的效率和效果。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2403.05578v1",
      "published_date": "2024-02-28 07:56:04 UTC",
      "updated_date": "2024-02-28 07:56:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:03:32.929303"
    },
    {
      "arxiv_id": "2402.18129v2",
      "title": "On the Inductive Biases of Demographic Parity-based Fair Learning Algorithms",
      "title_zh": "论基于人口统计平价的公平学习算法的归纳偏差",
      "authors": [
        "Haoyu Lei",
        "Amin Gohari",
        "Farzan Farnia"
      ],
      "abstract": "Fair supervised learning algorithms assigning labels with little dependence\non a sensitive attribute have attracted great attention in the machine learning\ncommunity. While the demographic parity (DP) notion has been frequently used to\nmeasure a model's fairness in training fair classifiers, several studies in the\nliterature suggest potential impacts of enforcing DP in fair learning\nalgorithms. In this work, we analytically study the effect of standard DP-based\nregularization methods on the conditional distribution of the predicted label\ngiven the sensitive attribute. Our analysis shows that an imbalanced training\ndataset with a non-uniform distribution of the sensitive attribute could lead\nto a classification rule biased toward the sensitive attribute outcome holding\nthe majority of training data. To control such inductive biases in DP-based\nfair learning, we propose a sensitive attribute-based distributionally robust\noptimization (SA-DRO) method improving robustness against the marginal\ndistribution of the sensitive attribute. Finally, we present several numerical\nresults on the application of DP-based learning methods to standard centralized\nand distributed learning problems. The empirical findings support our\ntheoretical results on the inductive biases in DP-based fair learning\nalgorithms and the debiasing effects of the proposed SA-DRO method.",
      "tldr_zh": "这篇论文分析了基于 Demographic Parity (DP) 的公平学习算法的归纳偏差，重点探讨了这些算法在处理敏感属性时可能引入的偏置问题。研究发现，当训练数据集不平衡且敏感属性分布不均匀时，DP-based 方法可能导致分类规则偏向于占多数的敏感属性类别，从而影响模型的条件分布。作者提出了一种敏感属性-based distributionally robust optimization (SA-DRO) 方法，通过增强对敏感属性边际分布的鲁棒性来控制这些偏差。数值实验在集中式和分布式学习场景中验证了理论分析，并证明了 SA-DRO 的去偏效果。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "math.IT"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18129v2",
      "published_date": "2024-02-28 07:39:58 UTC",
      "updated_date": "2024-06-20 09:35:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:03:46.454689"
    },
    {
      "arxiv_id": "2402.18121v1",
      "title": "Saving the legacy of Hero Ibash: Evaluating Four Language Models for Aminoacian",
      "title_zh": "翻译失败",
      "authors": [
        "Yunze Xiao",
        "Yiyang Pan"
      ],
      "abstract": "This study assesses four cutting-edge language models in the underexplored\nAminoacian language. Through evaluation, it scrutinizes their adaptability,\neffectiveness, and limitations in text generation, semantic coherence, and\ncontextual understanding. Uncovering insights into these models' performance in\na low-resourced language, this research pioneers pathways to bridge linguistic\ngaps. By offering benchmarks and understanding challenges, it lays groundwork\nfor future advancements in natural language processing, aiming to elevate the\napplicability of language models in similar linguistic landscapes, marking a\nsignificant step toward inclusivity and progress in language technology.",
      "tldr_zh": "这篇论文评估了四个先进的语言模型在Aminoacian这种低资源语言中的表现，焦点在于它们的适应性、有效性和局限性，特别是文本生成、semantic coherence和contextual understanding。研究通过详细分析揭示了这些模型在处理Aminoacian语言时的优势与挑战，为未来natural language processing (NLP)发展提供了基准和见解。最终，该工作旨在弥合语言鸿沟，推动语言技术在类似语境中的包容性和进步。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "5 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.18121v1",
      "published_date": "2024-02-28 07:22:13 UTC",
      "updated_date": "2024-02-28 07:22:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:03:57.339619"
    },
    {
      "arxiv_id": "2402.18113v1",
      "title": "Small But Funny: A Feedback-Driven Approach to Humor Distillation",
      "title_zh": "Small",
      "authors": [
        "Sahithya Ravi",
        "Patrick Huber",
        "Akshat Shrivastava",
        "Aditya Sagar",
        "Ahmed Aly",
        "Vered Shwartz",
        "Arash Einolghozati"
      ],
      "abstract": "The emergence of Large Language Models (LLMs) has brought to light promising\nlanguage generation capabilities, particularly in performing tasks like complex\nreasoning and creative writing. Consequently, distillation through imitation of\nteacher responses has emerged as a popular technique to transfer knowledge from\nLLMs to more accessible, Small Language Models (SLMs). While this works well\nfor simpler tasks, there is a substantial performance gap on tasks requiring\nintricate language comprehension and creativity, such as humor generation. We\nhypothesize that this gap may stem from the fact that creative tasks might be\nhard to learn by imitation alone and explore whether an approach, involving\nsupplementary guidance from the teacher, could yield higher performance. To\naddress this, we study the effect of assigning a dual role to the LLM - as a\n\"teacher\" generating data, as well as a \"critic\" evaluating the student's\nperformance. Our experiments on humor generation reveal that the incorporation\nof feedback significantly narrows the performance gap between SLMs and their\nlarger counterparts compared to merely relying on imitation. As a result, our\nresearch highlights the potential of using feedback as an additional dimension\nto data when transferring complex language abilities via distillation.",
      "tldr_zh": "本研究探讨了将 Large Language Models (LLMs) 的知识蒸馏(distillation)到 Small Language Models (SLMs) 的过程中，创意任务如幽默生成面临的性能差距问题。作者假设单纯模仿学习不足以处理此类任务，因此提出一种反馈驱动方法，让 LLM 同时担任“教师”（生成数据）和“批评者”（评估 SLMs 性能）。实验结果显示，加入反馈机制后，SLMs 在幽默生成任务上的表现显著提升，与 LLMs 的差距缩小。该方法突显了反馈作为知识转移额外维度的潜力，为提升 SLMs 的复杂语言能力提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18113v1",
      "published_date": "2024-02-28 07:02:38 UTC",
      "updated_date": "2024-02-28 07:02:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:04:09.746707"
    },
    {
      "arxiv_id": "2402.18104v2",
      "title": "Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction",
      "title_zh": "翻译失败",
      "authors": [
        "Tong Liu",
        "Yingjie Zhang",
        "Zhe Zhao",
        "Yinpeng Dong",
        "Guozhu Meng",
        "Kai Chen"
      ],
      "abstract": "In recent years, large language models (LLMs) have demonstrated notable\nsuccess across various tasks, but the trustworthiness of LLMs is still an open\nproblem. One specific threat is the potential to generate toxic or harmful\nresponses. Attackers can craft adversarial prompts that induce harmful\nresponses from LLMs. In this work, we pioneer a theoretical foundation in LLMs\nsecurity by identifying bias vulnerabilities within the safety fine-tuning and\ndesign a black-box jailbreak method named DRA (Disguise and Reconstruction\nAttack), which conceals harmful instructions through disguise and prompts the\nmodel to reconstruct the original harmful instruction within its completion. We\nevaluate DRA across various open-source and closed-source models, showcasing\nstate-of-the-art jailbreak success rates and attack efficiency. Notably, DRA\nboasts a 91.1% attack success rate on OpenAI GPT-4 chatbot.",
      "tldr_zh": "该研究探讨了大型语言模型 (LLMs) 的可信度问题，特别针对攻击者通过对抗性提示诱导模型生成有害响应的风险，并首次识别了安全微调中的偏差漏洞。研究者提出了一种黑盒攻击方法 DRA (Disguise and Reconstruction Attack)，通过伪装有害指令并提示模型重建原始指令，从而在少量查询下实现高效的越狱攻击。在实验中，DRA 在各种开源和闭源模型上表现出色，尤其在 OpenAI GPT-4 上达到了 91.1% 的攻击成功率，为提升 LLMs 的安全性提供了重要理论基础。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18104v2",
      "published_date": "2024-02-28 06:50:14 UTC",
      "updated_date": "2024-06-10 11:20:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:04:22.170058"
    },
    {
      "arxiv_id": "2402.18099v3",
      "title": "Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Derong Xu",
        "Ziheng Zhang",
        "Zhihong Zhu",
        "Zhenxi Lin",
        "Qidong Liu",
        "Xian Wu",
        "Tong Xu",
        "Wanyu Wang",
        "Yuyang Ye",
        "Xiangyu Zhao",
        "Enhong Chen",
        "Yefeng Zheng"
      ],
      "abstract": "Model editing aims to precisely alter the behaviors of large language models\n(LLMs) in relation to specific knowledge, while leaving unrelated knowledge\nintact. This approach has proven effective in addressing issues of\nhallucination and outdated information in LLMs. However, the potential of using\nmodel editing to modify knowledge in the medical field remains largely\nunexplored, even though resolving hallucination is a pressing need in this\narea. Our observations indicate that current methods face significant\nchallenges in dealing with specialized and complex knowledge in medical domain.\nTherefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for\nmedical model editing. MedLaSA harnesses the strengths of both adding extra\nparameters and locate-then-edit methods for medical model editing. We utilize\ncausal tracing to identify the association of knowledge in neurons across\ndifferent layers, and generate a corresponding scale set from the association\nvalue for each piece of knowledge. Subsequently, we incorporate scalable\nadapters into the dense layers of LLMs. These adapters are assigned scaling\nvalues based on the corresponding specific knowledge, which allows for the\nadjustment of the adapter's weight and rank. The more similar the content, the\nmore consistent the scale between them. This ensures precise editing of\nsemantically identical knowledge while avoiding impact on unrelated knowledge.\nTo evaluate the editing impact on the behaviours of LLMs, we propose two model\nediting studies for medical domain: (1) editing factual knowledge for medical\nspecialization and (2) editing the explanatory ability for complex knowledge.\nWe build two novel medical benchmarking datasets and introduce a series of\nchallenging and comprehensive metrics. Extensive experiments on medical LLMs\ndemonstrate the editing efficiency of MedLaSA, without affecting unrelated\nknowledge.",
      "tldr_zh": "该研究探讨了在医疗领域使用模型编辑（model editing）来精确修改大型语言模型（LLMs）的特定事实知识和解释能力，同时避免影响无关知识，以解决幻觉（hallucination）和过时信息问题。论文提出了一种新型策略MedLaSA（Layer-wise Scalable Adapter strategy），它结合添加额外参数和locate-then-edit方法，通过因果追踪（causal tracing）识别神经元层间的知识关联，并使用可扩展适配器（scalable adapters）根据知识相似度调整权重，从而实现精确编辑。实验在医疗LLMs上构建了两个新基准数据集，并引入全面指标，证明MedLaSA能高效编辑医疗专业知识和复杂解释能力，而不干扰其他内容。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by CIKM 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.18099v3",
      "published_date": "2024-02-28 06:40:57 UTC",
      "updated_date": "2024-09-23 12:09:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:04:35.479989"
    },
    {
      "arxiv_id": "2402.18096v1",
      "title": "No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization",
      "title_zh": "翻译失败",
      "authors": [
        "June Yong Yang",
        "Byeongwook Kim",
        "Jeongin Bae",
        "Beomseok Kwon",
        "Gunho Park",
        "Eunho Yang",
        "Se Jung Kwon",
        "Dongsoo Lee"
      ],
      "abstract": "Key-Value (KV) Caching has become an essential technique for accelerating the\ninference speed and throughput of generative Large Language Models~(LLMs).\nHowever, the memory footprint of the KV cache poses a critical bottleneck in\nLLM deployment as the cache size grows with batch size and sequence length,\noften surpassing even the size of the model itself. Although recent methods\nwere proposed to select and evict unimportant KV pairs from the cache to reduce\nmemory consumption, the potential ramifications of eviction on the generative\nprocess are yet to be thoroughly examined. In this paper, we examine the\ndetrimental impact of cache eviction and observe that unforeseen risks arise as\nthe information contained in the KV pairs is exhaustively discarded, resulting\nin safety breaches, hallucinations, and context loss. Surprisingly, we find\nthat preserving even a small amount of information contained in the evicted KV\npairs via reduced precision quantization substantially recovers the incurred\ndegradation. On the other hand, we observe that the important KV pairs must be\nkept at a relatively higher precision to safeguard the generation quality.\nMotivated by these observations, we propose \\textit{Mixed-precision KV\ncache}~(MiKV), a reliable cache compression method that simultaneously\npreserves the context details by retaining the evicted KV pairs in\nlow-precision and ensure generation quality by keeping the important KV pairs\nin high-precision. Experiments on diverse benchmarks and LLM backbones show\nthat our proposed method offers a state-of-the-art trade-off between\ncompression ratio and performance, compared to other baselines.",
      "tldr_zh": "该论文探讨了生成式大语言模型(LLMs)中KV Cache的内存占用问题，指出现有删除不重要KV对的方法可能导致安全风险、幻觉和上下文丢失。研究发现，通过对被删除KV对使用低精度量化保留部分信息，同时对重要KV对保持高精度，可以显著恢复性能。作者提出MiKV（混合精度KV缓存）方法，该方法基于重要性感知进行量化，实验在多种基准和LLM基础上实现了最佳的压缩比与性能权衡。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18096v1",
      "published_date": "2024-02-28 06:34:54 UTC",
      "updated_date": "2024-02-28 06:34:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:04:46.082225"
    },
    {
      "arxiv_id": "2402.18091v1",
      "title": "Polos: Multimodal Metric Learning from Human Feedback for Image Captioning",
      "title_zh": "翻译失败",
      "authors": [
        "Yuiga Wada",
        "Kanta Kaneda",
        "Daichi Saito",
        "Komei Sugiura"
      ],
      "abstract": "Establishing an automatic evaluation metric that closely aligns with human\njudgments is essential for effectively developing image captioning models.\nRecent data-driven metrics have demonstrated a stronger correlation with human\njudgments than classic metrics such as CIDEr; however they lack sufficient\ncapabilities to handle hallucinations and generalize across diverse images and\ntexts partially because they compute scalar similarities merely using\nembeddings learned from tasks unrelated to image captioning evaluation. In this\nstudy, we propose Polos, a supervised automatic evaluation metric for image\ncaptioning models. Polos computes scores from multimodal inputs, using a\nparallel feature extraction mechanism that leverages embeddings trained through\nlarge-scale contrastive learning. To train Polos, we introduce Multimodal\nMetric Learning from Human Feedback (M$^2$LHF), a framework for developing\nmetrics based on human feedback. We constructed the Polaris dataset, which\ncomprises 131K human judgments from 550 evaluators, which is approximately ten\ntimes larger than standard datasets. Our approach achieved state-of-the-art\nperformance on Composite, Flickr8K-Expert, Flickr8K-CF, PASCAL-50S, FOIL, and\nthe Polaris dataset, thereby demonstrating its effectiveness and robustness.",
      "tldr_zh": "本研究提出 Polos，一种基于人类反馈的多模态自动评估指标，用于图像字幕模型，以解决现有指标在处理幻觉和泛化方面的不足。Polos 通过并行特征提取机制，利用大规模对比学习训练的嵌入来计算多模态输入分数，并引入 Multimodal Metric Learning from Human Feedback (M²LHF) 框架进行训练，同时构建了包含 131K 个人判断的 Polaris 数据集。实验结果显示，Polos 在 Composite、Flickr8K-Expert、Flickr8K-CF、PASCAL-50S、FOIL 和 Polaris 数据集上实现了 state-of-the-art 性能，证明了其有效性和鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.18091v1",
      "published_date": "2024-02-28 06:24:39 UTC",
      "updated_date": "2024-02-28 06:24:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:04:58.531656"
    },
    {
      "arxiv_id": "2402.18062v1",
      "title": "Generative AI for Unmanned Vehicle Swarms: Challenges, Applications and Opportunities",
      "title_zh": "生成式 AI 用于无人车辆群：挑战、应用和机会",
      "authors": [
        "Guangyuan Liu",
        "Nguyen Van Huynh",
        "Hongyang Du",
        "Dinh Thai Hoang",
        "Dusit Niyato",
        "Kun Zhu",
        "Jiawen Kang",
        "Zehui Xiong",
        "Abbas Jamalipour",
        "Dong In Kim"
      ],
      "abstract": "With recent advances in artificial intelligence (AI) and robotics, unmanned\nvehicle swarms have received great attention from both academia and industry\ndue to their potential to provide services that are difficult and dangerous to\nperform by humans. However, learning and coordinating movements and actions for\na large number of unmanned vehicles in complex and dynamic environments\nintroduce significant challenges to conventional AI methods. Generative AI\n(GAI), with its capabilities in complex data feature extraction,\ntransformation, and enhancement, offers great potential in solving these\nchallenges of unmanned vehicle swarms. For that, this paper aims to provide a\ncomprehensive survey on applications, challenges, and opportunities of GAI in\nunmanned vehicle swarms. Specifically, we first present an overview of unmanned\nvehicles and unmanned vehicle swarms as well as their use cases and existing\nissues. Then, an in-depth background of various GAI techniques together with\ntheir capabilities in enhancing unmanned vehicle swarms are provided. After\nthat, we present a comprehensive review on the applications and challenges of\nGAI in unmanned vehicle swarms with various insights and discussions. Finally,\nwe highlight open issues of GAI in unmanned vehicle swarms and discuss\npotential research directions.",
      "tldr_zh": "这篇论文对生成式 AI (Generative AI) 在无人车辆群 (unmanned vehicle swarms) 中的应用、挑战和机会进行了全面调查，旨在解决传统 AI 方法在复杂动态环境下的协调难题。论文首先概述了无人车辆及其群体的用例和现有问题，然后介绍了各种 GAI 技术及其在数据特征提取、转换和增强方面的潜力。最终，通过深入审查 GAI 的应用和挑战，该研究突出了开放性问题，并提出了未来研究方向，如改进协调机制和增强环境适应性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "23 pages",
      "pdf_url": "http://arxiv.org/pdf/2402.18062v1",
      "published_date": "2024-02-28 05:46:23 UTC",
      "updated_date": "2024-02-28 05:46:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:05:09.491712"
    },
    {
      "arxiv_id": "2402.18061v2",
      "title": "On the use of Silver Standard Data for Zero-shot Classification Tasks in Information Extraction",
      "title_zh": "翻译失败",
      "authors": [
        "Jianwei Wang",
        "Tianyin Wang",
        "Ziqian Zeng"
      ],
      "abstract": "The superior performance of supervised classification methods in the\ninformation extraction (IE) area heavily relies on a large amount of gold\nstandard data. Recent zero-shot classification methods converted the task to\nother NLP tasks (e.g., textual entailment) and used off-the-shelf models of\nthese NLP tasks to directly perform inference on the test data without using a\nlarge amount of IE annotation data. A potentially valuable by-product of these\nmethods is the large-scale silver standard data, i.e., pseudo-labeled data by\nthe off-the-shelf models of other NLP tasks. However, there is no further\ninvestigation into the use of these data. In this paper, we propose a new\nframework, Clean-LaVe, which aims to utilize silver standard data to enhance\nthe zero-shot performance. Clean-LaVe includes four phases: (1) Obtaining\nsilver data; (2) Identifying relatively clean data from silver data; (3)\nFinetuning the off-the-shelf model using clean data; (4) Inference on the test\ndata. The experimental results show that Clean-LaVe can outperform the baseline\nby 5% and 6% on TACRED and Wiki80 dataset in the zero-shot relation\nclassification task, and by 3%-7% on Smile (Korean and Polish) in the zero-shot\ncross-lingual relation classification task, and by 8% on ACE05-E+ in the\nzero-shot event argument classification task. The code is share in\nhttps://github.com/wjw136/Clean_LaVe.git.",
      "tldr_zh": "该论文探讨了在信息抽取领域的零-shot classification任务中，利用银标准数据（silver standard data，即伪标注数据）来提升性能的问题。作者提出了一种新框架Clean-LaVe，包括四个阶段：获取银数据、识别较干净的数据子集、使用这些数据微调现成NLP模型，以及在测试数据上进行推理。实验结果显示，Clean-LaVe在TACRED和Wiki80数据集的零-shot relation classification任务上分别提高了5%和6%，在Smile数据集（韩语和波兰语）的跨语言任务上提升3%-7%，以及在ACE05-E+的零-shot event argument classification任务上提升8%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "accepted by coling2024. arXiv:2211.13883 is our first edition",
      "pdf_url": "http://arxiv.org/pdf/2402.18061v2",
      "published_date": "2024-02-28 05:45:37 UTC",
      "updated_date": "2024-03-06 08:42:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:05:22.611455"
    },
    {
      "arxiv_id": "2402.18041v1",
      "title": "Datasets for Large Language Models: A Comprehensive Survey",
      "title_zh": "大型语言模型数据集：一项全面综述",
      "authors": [
        "Yang Liu",
        "Jiahuan Cao",
        "Chongyu Liu",
        "Kai Ding",
        "Lianwen Jin"
      ],
      "abstract": "This paper embarks on an exploration into the Large Language Model (LLM)\ndatasets, which play a crucial role in the remarkable advancements of LLMs. The\ndatasets serve as the foundational infrastructure analogous to a root system\nthat sustains and nurtures the development of LLMs. Consequently, examination\nof these datasets emerges as a critical topic in research. In order to address\nthe current lack of a comprehensive overview and thorough analysis of LLM\ndatasets, and to gain insights into their current status and future trends,\nthis survey consolidates and categorizes the fundamental aspects of LLM\ndatasets from five perspectives: (1) Pre-training Corpora; (2) Instruction\nFine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5)\nTraditional Natural Language Processing (NLP) Datasets. The survey sheds light\non the prevailing challenges and points out potential avenues for future\ninvestigation. Additionally, a comprehensive review of the existing available\ndataset resources is also provided, including statistics from 444 datasets,\ncovering 8 language categories and spanning 32 domains. Information from 20\ndimensions is incorporated into the dataset statistics. The total data size\nsurveyed surpasses 774.5 TB for pre-training corpora and 700M instances for\nother datasets. We aim to present the entire landscape of LLM text datasets,\nserving as a comprehensive reference for researchers in this field and\ncontributing to future studies. Related resources are available at:\nhttps://github.com/lmmlzn/Awesome-LLMs-Datasets.",
      "tldr_zh": "这篇论文对 Large Language Models (LLMs) 的数据集进行了全面调查，涵盖预训练语料库、指令微调数据集、偏好数据集、评估数据集以及传统 Natural Language Processing (NLP) 数据集等五个关键方面。调查分析了 444 个数据集，包括 8 个语言类别和 32 个领域，从 20 个维度统计数据，总规模超过 774.5 TB 的预训练语料和 700M 实例。论文指出了当前数据集面临的挑战，并为未来研究提供了潜在方向和参考资源，如 GitHub 上的相关链接。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "181 pages, 21 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.18041v1",
      "published_date": "2024-02-28 04:35:51 UTC",
      "updated_date": "2024-02-28 04:35:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:05:34.014056"
    },
    {
      "arxiv_id": "2402.18040v1",
      "title": "Automated Discovery of Integral with Deep Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoxin Yin"
      ],
      "abstract": "Recent advancements in the realm of deep learning, particularly in the\ndevelopment of large language models (LLMs), have demonstrated AI's ability to\ntackle complex mathematical problems or solving programming challenges.\nHowever, the capability to solve well-defined problems based on extensive\ntraining data differs significantly from the nuanced process of making\nscientific discoveries. Trained on almost all human knowledge available,\ntoday's sophisticated LLMs basically learn to predict sequences of tokens. They\ngenerate mathematical derivations and write code in a similar way as writing an\nessay, and do not have the ability to pioneer scientific discoveries in the\nmanner a human scientist would do.\n  In this study we delve into the potential of using deep learning to\nrediscover a fundamental mathematical concept: integrals. By defining integrals\nas area under the curve, we illustrate how AI can deduce the integral of a\ngiven function, exemplified by inferring $\\int_{0}^{x} t^2 dt = \\frac{x^3}{3}$\nand $\\int_{0}^{x} ae^{bt} dt = \\frac{a}{b} e^{bx} - \\frac{a}{b}$. Our\nexperiments show that deep learning models can approach the task of inferring\nintegrals either through a sequence-to-sequence model, akin to language\ntranslation, or by uncovering the rudimentary principles of integration, such\nas $\\int_{0}^{x} t^n dt = \\frac{x^{n+1}}{n+1}$.",
      "tldr_zh": "本研究探讨了使用深度学习自动发现积分这一基础数学概念，旨在克服大型语言模型(LLMs) 在科学发现方面的局限性，这些模型通常仅擅长序列预测而非创新性推理。通过定义积分为曲线下的面积，研究采用序列到序列(sequence-to-sequence)模型或揭示积分基本原则（如 ∫_{0}^{x} t^n dt = \\frac{x^{n+1}}{n+1}）的方法，成功推断了具体积分公式，例如 ∫_{0}^{x} t^2 dt = \\frac{x^3}{3} 和 ∫_{0}^{x} ae^{bt} dt = \\frac{a}{b} e^{bx} - \\frac{a}{b}。实验结果显示，这种深度学习方法在自动发现积分任务上表现出色，为 AI 在数学科学领域的应用提供了新见解。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18040v1",
      "published_date": "2024-02-28 04:34:15 UTC",
      "updated_date": "2024-02-28 04:34:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:05:48.301409"
    },
    {
      "arxiv_id": "2402.18039v1",
      "title": "ResLoRA: Identity Residual Mapping in Low-Rank Adaption",
      "title_zh": "翻译失败",
      "authors": [
        "Shuhua Shi",
        "Shaohan Huang",
        "Minghui Song",
        "Zhoujun Li",
        "Zihan Zhang",
        "Haizhen Huang",
        "Furu Wei",
        "Weiwei Deng",
        "Feng Sun",
        "Qi Zhang"
      ],
      "abstract": "As one of the most popular parameter-efficient fine-tuning (PEFT) methods,\nlow-rank adaptation (LoRA) is commonly applied to fine-tune large language\nmodels (LLMs). However, updating the weights of LoRA blocks effectively and\nexpeditiously is challenging due to the long calculation path in the original\nmodel. To address this, we propose ResLoRA, an improved framework of LoRA. By\nadding residual paths during training and using merging approaches to eliminate\nthese extra paths during inference, our method can achieve better results in\nfewer training steps without any extra trainable parameters or inference cost\ncompared to LoRA. The experiments on NLG, NLU, and text-to-image tasks\ndemonstrate the effectiveness of our method. To the best of our knowledge,\nResLoRA is the first work that combines the residual path with LoRA. The code\nof our method is available at\nhttps://github.com/microsoft/LMOps/tree/main/reslora .",
      "tldr_zh": "本研究提出 ResLoRA，一种改进低秩适配 (LoRA) 的框架，针对大语言模型 (LLMs) 微调中的计算路径长问题，通过在训练中添加残差路径并在推理时使用合并方法，实现更高效的权重更新。ResLoRA 相较于 LoRA，能在更少的训练步骤中取得更好性能，而无需额外可训练参数或推理开销。实验在 NLG（自然语言生成）、NLU（自然语言理解）和文本到图像任务上验证了其有效性，这是首次将残差路径与 LoRA 结合的创新工作。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.18039v1",
      "published_date": "2024-02-28 04:33:20 UTC",
      "updated_date": "2024-02-28 04:33:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:05:59.090612"
    },
    {
      "arxiv_id": "2403.00829v1",
      "title": "TroubleLLM: Align to Red Team Expert",
      "title_zh": "翻译失败",
      "authors": [
        "Zhuoer Xu",
        "Jianping Zhang",
        "Shiwen Cui",
        "Changhua Meng",
        "Weiqiang Wang"
      ],
      "abstract": "Large Language Models (LLMs) become the start-of-the-art solutions for a\nvariety of natural language tasks and are integrated into real-world\napplications. However, LLMs can be potentially harmful in manifesting\nundesirable safety issues like social biases and toxic content. It is\nimperative to assess its safety issues before deployment. However, the quality\nand diversity of test prompts generated by existing methods are still far from\nsatisfactory. Not only are these methods labor-intensive and require large\nbudget costs, but the controllability of test prompt generation is lacking for\nthe specific testing domain of LLM applications. With the idea of LLM for LLM\ntesting, we propose the first LLM, called TroubleLLM, to generate controllable\ntest prompts on LLM safety issues. Extensive experiments and human evaluation\nillustrate the superiority of TroubleLLM on generation quality and generation\ncontrollability.",
      "tldr_zh": "该论文探讨了大型语言模型（LLMs）在实际应用中可能存在的安全问题，如社会偏见和有毒内容，并强调了评估这些问题的必要性。作者提出TroubleLLM，一种基于“LLM for LLM testing”的新框架，能够生成高质量、可控的测试提示，以针对特定领域的LLM安全问题进行评估。与现有方法相比，TroubleLLM在生成质量和多样性上表现出色，经广泛实验和人工评估证实其优越性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00829v1",
      "published_date": "2024-02-28 03:40:46 UTC",
      "updated_date": "2024-02-28 03:40:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:06:09.456609"
    },
    {
      "arxiv_id": "2402.18023v3",
      "title": "Do Large Language Models Mirror Cognitive Language Processing?",
      "title_zh": "大语言模型是否镜像认知语言处理？",
      "authors": [
        "Yuqi Ren",
        "Renren Jin",
        "Tongxuan Zhang",
        "Deyi Xiong"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable abilities in text\ncomprehension and logical reasoning, indicating that the text representations\nlearned by LLMs can facilitate their language processing capabilities. In\nneuroscience, brain cognitive processing signals are typically utilized to\nstudy human language processing. Therefore, it is natural to ask how well the\ntext embeddings from LLMs align with the brain cognitive processing signals,\nand how training strategies affect the LLM-brain alignment? In this paper, we\nemploy Representational Similarity Analysis (RSA) to measure the alignment\nbetween 23 mainstream LLMs and fMRI signals of the brain to evaluate how\neffectively LLMs simulate cognitive language processing. We empirically\ninvestigate the impact of various factors (e.g., pre-training data size, model\nscaling, alignment training, and prompts) on such LLM-brain alignment.\nExperimental results indicate that pre-training data size and model scaling are\npositively correlated with LLM-brain similarity, and alignment training can\nsignificantly improve LLM-brain similarity. Explicit prompts contribute to the\nconsistency of LLMs with brain cognitive language processing, while nonsensical\nnoisy prompts may attenuate such alignment. Additionally, the performance of a\nwide range of LLM evaluations (e.g., MMLU, Chatbot Arena) is highly correlated\nwith the LLM-brain similarity.",
      "tldr_zh": "本文研究 Large Language Models (LLMs) 是否能模拟人类认知语言处理，通过 Representational Similarity Analysis (RSA) 评估 23 个主流 LLMs 与 fMRI 脑信号的相似度。结果显示，预训练数据大小和模型规模与 LLM-脑相似度正相关，alignment training 能显著提升这种相似度，而明确提示增强一致性，无意义提示则会减弱。实验还发现，LLMs 的性能指标（如 MMLU 和 Chatbot Arena）与脑相似度高度相关，这为理解 LLMs 的认知机制提供了新洞见。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18023v3",
      "published_date": "2024-02-28 03:38:20 UTC",
      "updated_date": "2025-01-15 04:47:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:06:22.955948"
    },
    {
      "arxiv_id": "2402.18016v3",
      "title": "User Decision Guidance with Selective Explanation Presentation from Explainable-AI",
      "title_zh": "翻译失败",
      "authors": [
        "Yosuke Fukuchi",
        "Seiji Yamada"
      ],
      "abstract": "This paper addresses the challenge of selecting explanations for XAI\n(Explainable AI)-based Intelligent Decision Support Systems (IDSSs). IDSSs have\nshown promise in improving user decisions through XAI-generated explanations\nalong with AI predictions, and the development of XAI made it possible to\ngenerate a variety of such explanations. However, how IDSSs should select\nexplanations to enhance user decision-making remains an open question. This\npaper proposes X-Selector, a method for selectively presenting XAI\nexplanations. It enables IDSSs to strategically guide users to an AI-suggested\ndecision by predicting the impact of different combinations of explanations on\na user's decision and selecting the combination that is expected to minimize\nthe discrepancy between an AI suggestion and a user decision. We compared the\nefficacy of X-Selector with two naive strategies (all possible explanations and\nexplanations only for the most likely prediction) and two baselines (no\nexplanation and no AI support). The results suggest the potential of X-Selector\nto guide users to AI-suggested decisions and improve task performance under the\ncondition of a high AI accuracy.",
      "tldr_zh": "这篇论文针对 XAI (Explainable AI) 基于的 IDSS (Intelligent Decision Support Systems) 中解释选择的挑战，提出了 X-Selector 方法。该方法通过预测不同解释组合对用户决策的影响，选择最优组合以最小化 AI 建议与用户决策的差异，从而战略性地引导用户朝向 AI 推荐的决策。与基线策略（如提供所有解释、仅提供最可能预测的解释、无解释或无 AI 支持）相比，实验结果表明 X-Selector 在 AI 准确率高的条件下，能显著提升用户决策质量和任务表现。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted at the 2024 33rd IEEE International Conference on Robot and\n  Human Interactive Communication (RO-MAN)",
      "pdf_url": "http://arxiv.org/pdf/2402.18016v3",
      "published_date": "2024-02-28 03:21:25 UTC",
      "updated_date": "2024-05-27 01:40:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:06:34.505796"
    },
    {
      "arxiv_id": "2403.07921v3",
      "title": "Merino: Entropy-driven Design for Generative Language Models on IoT Devices",
      "title_zh": "翻译失败",
      "authors": [
        "Youpeng Zhao",
        "Ming Lin",
        "Huadong Tang",
        "Qiang Wu",
        "Jun Wang"
      ],
      "abstract": "Generative Large Language Models (LLMs) stand as a revolutionary advancement\nin the modern era of artificial intelligence (AI). However, scaling down LLMs\nfor resource-constrained hardware, such as Internet-of-Things (IoT) devices\nrequires non-trivial efforts and domain knowledge. In this paper, we propose a\nnovel information-entropy framework for designing mobile-friendly generative\nlanguage models. The whole design procedure involves solving a mathematical\nprogramming (MP) problem, which can be done on the CPU within minutes, making\nit nearly zero-cost. We evaluate our designed models, termed MeRino, across\nfourteen NLP downstream tasks, showing their competitive performance against\nthe state-of-the-art autoregressive transformer models under the mobile\nsetting. Notably, MeRino achieves similar or better performance on both\nlanguage modeling and zero-shot learning tasks, compared to the 350M parameter\nOPT while being 4.9x faster on NVIDIA Jetson Nano with 5.5x reduction in model\nsize.",
      "tldr_zh": "该论文提出了一种基于信息熵（information-entropy）的框架，用于设计适合物联网（IoT）设备的生成式大型语言模型（Generative Large Language Models, LLMs），以应对资源受限硬件的挑战。设计过程通过解决数学规划（mathematical programming, MP）问题实现，在CPU上仅需几分钟，几乎为零成本。实验结果显示，名为MeRino的模型在14个NLP下游任务上，与最先进的自回归Transformer模型相比，在移动环境中表现出色；与350M参数的OPT模型相比，MeRino在语言建模和零样本学习（zero-shot learning）任务上性能相当或更好，同时在NVIDIA Jetson Nano上速度提升4.9倍，模型大小减小5.5倍。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2403.07921v3",
      "published_date": "2024-02-28 03:20:27 UTC",
      "updated_date": "2025-01-27 15:39:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:06:47.591573"
    },
    {
      "arxiv_id": "2402.18013v1",
      "title": "A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Zihao Yi",
        "Jiarui Ouyang",
        "Yuwen Liu",
        "Tianhao Liao",
        "Zhe Xu",
        "Ying Shen"
      ],
      "abstract": "This survey provides a comprehensive review of research on multi-turn\ndialogue systems, with a particular focus on multi-turn dialogue systems based\non large language models (LLMs). This paper aims to (a) give a summary of\nexisting LLMs and approaches for adapting LLMs to downstream tasks; (b)\nelaborate recent advances in multi-turn dialogue systems, covering both\nLLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems,\nalong with datasets and evaluation metrics; (c) discuss some future emphasis\nand recent research problems arising from the development of LLMs and the\nincreasing demands on multi-turn dialogue systems.",
      "tldr_zh": "本文对基于大型语言模型 (LLMs) 的多轮对话系统进行了全面调查，总结了现有 LLMs 的类型及其适应下游任务的方法。调查详细阐述了最近在多轮对话系统中的进展，包括 LLM-based 开放域对话 (ODD) 和任务导向对话 (TOD) 系统的发展、相关数据集以及评估指标。最终，论文讨论了 LLMs 演进和对话系统需求带来的未来研究重点和潜在问题。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "35 pages, 10 figures, ACM Computing Surveys",
      "pdf_url": "http://arxiv.org/pdf/2402.18013v1",
      "published_date": "2024-02-28 03:16:44 UTC",
      "updated_date": "2024-02-28 03:16:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:06:58.384232"
    },
    {
      "arxiv_id": "2402.18012v3",
      "title": "Diffusion Models as Constrained Samplers for Optimization with Unknown Constraints",
      "title_zh": "扩散模型作为受约束采样器用于未知约束的优化",
      "authors": [
        "Lingkai Kong",
        "Yuanqi Du",
        "Wenhao Mu",
        "Kirill Neklyudov",
        "Valentin De Bortoli",
        "Dongxia Wu",
        "Haorui Wang",
        "Aaron Ferber",
        "Yi-An Ma",
        "Carla P. Gomes",
        "Chao Zhang"
      ],
      "abstract": "Addressing real-world optimization problems becomes particularly challenging\nwhen analytic objective functions or constraints are unavailable. While\nnumerous studies have addressed the issue of unknown objectives, limited\nresearch has focused on scenarios where feasibility constraints are not given\nexplicitly. Overlooking these constraints can lead to spurious solutions that\nare unrealistic in practice. To deal with such unknown constraints, we propose\nto perform optimization within the data manifold using diffusion models. To\nconstrain the optimization process to the data manifold, we reformulate the\noriginal optimization problem as a sampling problem from the product of the\nBoltzmann distribution defined by the objective function and the data\ndistribution learned by the diffusion model. Depending on the differentiability\nof the objective function, we propose two different sampling methods. For\ndifferentiable objectives, we propose a two-stage framework that begins with a\nguided diffusion process for warm-up, followed by a Langevin dynamics stage for\nfurther correction. For non-differentiable objectives, we propose an iterative\nimportance sampling strategy using the diffusion model as the proposal\ndistribution. Comprehensive experiments on a synthetic dataset, six real-world\nblack-box optimization datasets, and a multi-objective molecule optimization\ndataset show that our method achieves better or comparable performance with\nprevious state-of-the-art baselines.",
      "tldr_zh": "本文提出一种使用扩散模型（Diffusion Models）作为约束采样器的方法，解决未知约束的优化问题，将优化问题重构为从玻尔兹曼分布（Boltzmann distribution）和数据分布的乘积中采样，从而确保解决方案位于数据流形上。对于可微目标函数，该方法采用两阶段框架，包括引导扩散过程（guided diffusion process）和朗之万动力学（Langevin dynamics）阶段；对于不可微目标函数，则使用迭代重要性采样策略（iterative importance sampling），以扩散模型作为提案分布。实验结果显示，该方法在合成数据集、六个真实世界黑箱优化数据集以及多目标分子优化数据集上，取得了比现有最先进基线更好的或相当的性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18012v3",
      "published_date": "2024-02-28 03:09:12 UTC",
      "updated_date": "2024-10-21 04:06:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:07:12.116817"
    },
    {
      "arxiv_id": "2403.14660v1",
      "title": "Machina Economicus: A New Paradigm for Prosumers in the Energy Internet of Smart Cities",
      "title_zh": "翻译失败",
      "authors": [
        "Luyang Hou",
        "Jun Yan",
        "Yuankai Wu",
        "Chun Wang",
        "Tie Qiu"
      ],
      "abstract": "Energy Internet (EI) is emerging as new share economy platform for flexible\nlocal energy supplies in smart cities. Empowered by the Internet-of-Things\n(IoT) and Artificial Intelligence (AI), EI aims to unlock peer-to-peer energy\ntrading and sharing among prosumers, who can adeptly switch roles between\nproviders and consumers in localized energy markets with rooftop photovoltaic\npanels, vehicle-to-everything technologies, packetized energy management, etc.\nThe integration of prosumers in EI, however, will encounter many challenges in\nmodelling, analyzing, and designing an efficient, economic, and social-optimal\nplatform for energy sharing, calling for advanced AI/IoT-based solutions to\nresource optimization, information exchange, and interaction protocols in the\ncontext of the share economy. In this study, we aim to introduce a recently\nemerged paradigm, Machina Economicus, to investigate the economic rationality\nin modelling, analysis, and optimization of AI/IoT-based EI prosumer behaviors.\nThe new paradigm, built upon the theory of machine learning and mechanism\ndesign, will offer new angles to investigate the selfishness of AI through a\ngame-theoretic perspective, revealing potential competition and collaborations\nresulting from the self-adaptive learning and decision-making capacity. This\nstudy will focus on how the introduction of AI will reshape prosumer behaviors\non the EI, and how this paradigm will reveal new research questions and\ndirections when AI meets the share economy. With an extensive case analysis in\nthe literature, we will also shed light on potential solutions for advancements\nof AI in future smart cities.",
      "tldr_zh": "该论文引入了“Machina Economicus”范式，作为一种新框架，用于分析AI/IoT在智能城市Energy Internet (EI)中prosumers的行为建模、优化和经济理性。基于机器学习、机制设计和博弈论视角，该范式探讨了AI的自私性（selfishness）及其导致的潜在竞争与合作，揭示AI如何重塑prosumers在EI中的角色切换和决策过程。通过文献案例分析，论文指出了AI与分享经济结合的新研究问题，并为未来智能城市的资源优化和信息交换提供潜在解决方案。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "25 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2403.14660v1",
      "published_date": "2024-02-28 02:53:17 UTC",
      "updated_date": "2024-02-28 02:53:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:07:23.427975"
    },
    {
      "arxiv_id": "2402.18007v2",
      "title": "Mixer is more than just a model",
      "title_zh": "Mixer 不仅仅是一个模型",
      "authors": [
        "Qingfeng Ji",
        "Yuxin Wang",
        "Letong Sun"
      ],
      "abstract": "Recently, MLP structures have regained popularity, with MLP-Mixer standing\nout as a prominent example. In the field of computer vision, MLP-Mixer is noted\nfor its ability to extract data information from both channel and token\nperspectives, effectively acting as a fusion of channel and token information.\nIndeed, Mixer represents a paradigm for information extraction that amalgamates\nchannel and token information. The essence of Mixer lies in its ability to\nblend information from diverse perspectives, epitomizing the true concept of\n\"mixing\" in the realm of neural network architectures. Beyond channel and token\nconsiderations, it is possible to create more tailored mixers from various\nperspectives to better suit specific task requirements. This study focuses on\nthe domain of audio recognition, introducing a novel model named Audio\nSpectrogram Mixer with Roll-Time and Hermit FFT (ASM-RH) that incorporates\ninsights from both time and frequency domains. Experimental results demonstrate\nthat ASM-RH is particularly well-suited for audio data and yields promising\noutcomes across multiple classification tasks. The models and optimal weights\nfiles will be published.",
      "tldr_zh": "本论文扩展了 MLP-Mixer 模型的概念，认为它不仅仅是视觉领域的通道和 token 信息融合器，而是可应用于更多视角的定制化设计。作者针对音频识别领域，提出了一种新模型 Audio Spectrogram Mixer with Roll-Time and Hermit FFT (ASM-RH)，该模型结合时间和频率域的信息，以更好地适应音频数据。实验结果显示，ASM-RH 在多个音频分类任务中表现出色，证明了这种混合方法的有效性。模型和权重文件将发布。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.18007v2",
      "published_date": "2024-02-28 02:45:58 UTC",
      "updated_date": "2024-03-02 03:32:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:07:35.412829"
    },
    {
      "arxiv_id": "2402.18005v2",
      "title": "A Sentiment Consolidation Framework for Meta-Review Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Miao Li",
        "Jey Han Lau",
        "Eduard Hovy"
      ],
      "abstract": "Modern natural language generation systems with Large Language Models (LLMs)\nexhibit the capability to generate a plausible summary of multiple documents;\nhowever, it is uncertain if they truly possess the capability of information\nconsolidation to generate summaries, especially on documents with opinionated\ninformation. We focus on meta-review generation, a form of sentiment\nsummarisation for the scientific domain. To make scientific sentiment\nsummarization more grounded, we hypothesize that human meta-reviewers follow a\nthree-layer framework of sentiment consolidation to write meta-reviews. Based\non the framework, we propose novel prompting methods for LLMs to generate\nmeta-reviews and evaluation metrics to assess the quality of generated\nmeta-reviews. Our framework is validated empirically as we find that prompting\nLLMs based on the framework -- compared with prompting them with simple\ninstructions -- generates better meta-reviews.",
      "tldr_zh": "本论文提出一个三层 sentiment consolidation 框架，用于生成科学领域的 meta-review，这是一种基于意见整合的情感总结方法，以解决 Large Language Models (LLMs) 在处理意见性文档时的不确定性。框架假设人类 meta-reviewer 遵循情感整合的三层过程，并据此设计新型 prompting 方法和评估指标来指导 LLMs 生成 meta-review。实验结果显示，与简单指令 prompting 相比，该框架能产生更高质量的 meta-review，从而提升科学情感总结的可靠性和有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Long paper, ACL 2024 Main",
      "pdf_url": "http://arxiv.org/pdf/2402.18005v2",
      "published_date": "2024-02-28 02:40:09 UTC",
      "updated_date": "2024-06-04 16:10:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:07:47.270966"
    },
    {
      "arxiv_id": "2402.18002v2",
      "title": "Symmetry-aware Reinforcement Learning for Robotic Assembly under Partial Observability with a Soft Wrist",
      "title_zh": "翻译失败",
      "authors": [
        "Hai Nguyen",
        "Tadashi Kozuno",
        "Cristian C. Beltran-Hernandez",
        "Masashi Hamaya"
      ],
      "abstract": "This study tackles the representative yet challenging contact-rich\npeg-in-hole task of robotic assembly, using a soft wrist that can operate more\nsafely and tolerate lower-frequency control signals than a rigid one. Previous\nstudies often use a fully observable formulation, requiring external setups or\nestimators for the peg-to-hole pose. In contrast, we use a partially observable\nformulation and deep reinforcement learning from demonstrations to learn a\nmemory-based agent that acts purely on haptic and proprioceptive signals.\nMoreover, previous works do not incorporate potential domain symmetry and thus\nmust search for solutions in a bigger space. Instead, we propose to leverage\nthe symmetry for sample efficiency by augmenting the training data and\nconstructing auxiliary losses to force the agent to adhere to the symmetry.\nResults in simulation with five different symmetric peg shapes show that our\nproposed agent can be comparable to or even outperform a state-based agent. In\nparticular, the sample efficiency also allows us to learn directly on the real\nrobot within 3 hours.",
      "tldr_zh": "这篇论文针对机器人组装中的接触丰富任务（peg-in-hole），提出了一种基于 Symmetry-aware Reinforcement Learning 的方法，使用 Soft Wrist 以提高安全性和对低频控制信号的容忍度。不同于以往的完全可观察设置，该方法采用 Partial Observability 框架，通过深度 Reinforcement Learning from Demonstrations 训练一个基于记忆的代理，仅依赖触觉和本体感觉信号，同时通过数据增强和辅助损失来利用领域对称性，提高样本效率。实验结果显示，在模拟环境中，该代理在使用五种对称 peg 形状时，性能可与或超过基于状态的代理，并在真实机器人上仅需 3 小时即可实现有效学习。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted at ICRA-2024",
      "pdf_url": "http://arxiv.org/pdf/2402.18002v2",
      "published_date": "2024-02-28 02:30:59 UTC",
      "updated_date": "2024-04-29 19:00:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:08:00.940809"
    },
    {
      "arxiv_id": "2403.00017v1",
      "title": "Towards Interpreting Multi-Objective Feature Associations",
      "title_zh": "翻译失败",
      "authors": [
        "Nisha Pillai",
        "Ganga Gireesan",
        "Michael J. Rothrock Jr.",
        "Bindu Nanduri",
        "Zhiqian Chen",
        "Mahalingam Ramkumar"
      ],
      "abstract": "Understanding how multiple features are associated and contribute to a\nspecific objective is as important as understanding how each feature\ncontributes to a particular outcome. Interpretability of a single feature in a\nprediction may be handled in multiple ways; however, in a multi-objective\nprediction, it is difficult to obtain interpretability of a combination of\nfeature values. To address this issue, we propose an objective specific feature\ninteraction design using multi-labels to find the optimal combination of\nfeatures in agricultural settings. One of the novel aspects of this design is\nthe identification of a method that integrates feature explanations with global\nsensitivity analysis in order to ensure combinatorial optimization in\nmulti-objective settings. We have demonstrated in our preliminary experiments\nthat an approximate combination of feature values can be found to achieve the\ndesired outcome using two agricultural datasets: one with pre-harvest poultry\nfarm practices for multi-drug resistance presence, and one with post-harvest\npoultry farm practices for food-borne pathogens. In our combinatorial\noptimization approach, all three pathogens are taken into consideration\nsimultaneously to account for the interaction between conditions that favor\ndifferent types of pathogen growth. These results indicate that\nexplanation-based approaches are capable of identifying combinations of\nfeatures that reduce pathogen presence in fewer iterations than a baseline.",
      "tldr_zh": "这篇论文探讨了在多目标预测(multi-objective)中解释特征关联的重要性，强调了理解特征组合对目标贡献的难点。作者提出了一种基于多标签(multi-labels)的目标特定特征交互设计，将特征解释与全局敏感性分析(global sensitivity analysis)整合，以实现组合优化(combinatorial optimization)，并应用于农业场景如家禽农场实践。实验结果显示，该方法在两个数据集上比基线更快地识别出减少病原体存在的特征组合，同时考虑多种病原体间的交互。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "The 18th Annual IEEE International Systems Conference 2024 (IEEE\n  SYSCON 2024)",
      "pdf_url": "http://arxiv.org/pdf/2403.00017v1",
      "published_date": "2024-02-28 02:24:04 UTC",
      "updated_date": "2024-02-28 02:24:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:08:12.551090"
    },
    {
      "arxiv_id": "2403.00016v1",
      "title": "Deep Sensitivity Analysis for Objective-Oriented Combinatorial Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Ganga Gireesan",
        "Nisha Pillai",
        "Michael J Rothrock",
        "Bindu Nanduri",
        "Zhiqian Chen",
        "Mahalingam Ramkumar"
      ],
      "abstract": "Pathogen control is a critical aspect of modern poultry farming, providing\nimportant benefits for both public health and productivity. Effective poultry\nmanagement measures to reduce pathogen levels in poultry flocks promote food\nsafety by lowering risks of food-borne illnesses. They also support animal\nhealth and welfare by preventing infectious diseases that can rapidly spread\nand impact flock growth, egg production, and overall health. This study frames\nthe search for optimal management practices that minimize the presence of\nmultiple pathogens as a combinatorial optimization problem. Specifically, we\nmodel the various possible combinations of management settings as a solution\nspace that can be efficiently explored to identify configurations that\noptimally reduce pathogen levels. This design incorporates a neural network\nfeedback-based method that combines feature explanations with global\nsensitivity analysis to ensure combinatorial optimization in multiobjective\nsettings. Our preliminary experiments have promising results when applied to\ntwo real-world agricultural datasets. While further validation is still needed,\nthese early experimental findings demonstrate the potential of the model to\nderive targeted feature interactions that adaptively optimize pathogen control\nunder varying real-world constraints.",
      "tldr_zh": "本研究将家禽养殖中的病原体控制问题建模为面向目标的combinatorial optimization问题，旨在通过优化管理实践来最小化多种病原体水平，从而提升公共健康和生产力。方法结合neural network反馈、特征解释和global sensitivity analysis，构建了一个高效探索解决方案空间的框架，以处理多目标设置下的特征交互。初步实验在两个真实农业数据集上显示，该模型能有效识别针对性配置，并展示了在实际约束下优化病原体控制的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "The 2023 International Conference on Computational Science &\n  Computational Intelligence (CSCI'23)",
      "pdf_url": "http://arxiv.org/pdf/2403.00016v1",
      "published_date": "2024-02-28 02:15:47 UTC",
      "updated_date": "2024-02-28 02:15:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:08:23.778103"
    },
    {
      "arxiv_id": "2402.17985v1",
      "title": "FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization",
      "title_zh": "FlattenQuant：通过每个张量量化突破大型语言模型推理计算瓶颈",
      "authors": [
        "Yi Zhang",
        "Fei Yang",
        "Shuang Peng",
        "Fangyu Wang",
        "Aimin Pan"
      ],
      "abstract": "Large language models (LLMs) have demonstrated state-of-the-art performance\nacross various tasks. However, the latency of inference and the large GPU\nmemory consumption of LLMs restrict their deployment performance. Recently,\nthere have been some efficient attempts to quantize LLMs, yet inference with\nlarge batch size or long sequence still has the issue of being compute-bound.\nFine-grained quantization methods have showcased their proficiency in achieving\nlow-bit quantization for LLMs, while requiring FP16 data type for linear layer\ncomputations, which is time-consuming when dealing with large batch size or\nlong sequence. In this paper, we introduce a method called FlattenQuant, which\nsignificantly reduces the maximum value of the tensor by flattening the large\nchannels in the tensor, to achieve low bit per-tensor quantization with minimal\naccuracy loss. Our experiments show that FlattenQuant can directly use 4 bits\nto achieve 48.29% of the linear layer calculation in LLMs, with the remaining\nlayers using 8 bits. The 4-bit matrix multiplication introduced in the\nFlattenQuant method can effectively address the compute-bound caused by large\nmatrix calculation. Our work achieves up to 2$\\times$ speedup and 2.3$\\times$\nmemory reduction for LLMs with negligible loss in accuracy.",
      "tldr_zh": "本文提出FlattenQuant方法，针对大语言模型(LLMs)的推理计算瓶颈，通过扁平化张量中的大通道来显著降低张量最大值，实现低位per-tensor量化，同时最小化准确性损失。该方法使LLMs中48.29%的线性层计算直接使用4位量化，其余使用8位，解决了大批量或长序列处理时的计算受限问题。实验结果显示，FlattenQuant实现了高达2倍的速度提升和2.3倍的内存减少，而准确性损失 negligible。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.17985v1",
      "published_date": "2024-02-28 02:00:34 UTC",
      "updated_date": "2024-02-28 02:00:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:08:36.405947"
    },
    {
      "arxiv_id": "2402.17979v1",
      "title": "Ensemble Methodology:Innovations in Credit Default Prediction Using LightGBM, XGBoost, and LocalEnsemble",
      "title_zh": "集成方法：使用 LightGBM、XGBoost 和 LocalEnsemble 在信贷违约预测中的创新",
      "authors": [
        "Mengran Zhu",
        "Ye Zhang",
        "Yulu Gong",
        "Kaijuan Xing",
        "Xu Yan",
        "Jintong Song"
      ],
      "abstract": "In the realm of consumer lending, accurate credit default prediction stands\nas a critical element in risk mitigation and lending decision optimization.\nExtensive research has sought continuous improvement in existing models to\nenhance customer experiences and ensure the sound economic functioning of\nlending institutions. This study responds to the evolving landscape of credit\ndefault prediction, challenging conventional models and introducing innovative\napproaches. By building upon foundational research and recent innovations, our\nwork aims to redefine the standards of accuracy in credit default prediction,\nsetting a new benchmark for the industry. To overcome these challenges, we\npresent an Ensemble Methods framework comprising LightGBM, XGBoost, and\nLocalEnsemble modules, each making unique contributions to amplify diversity\nand improve generalization. By utilizing distinct feature sets, our methodology\ndirectly tackles limitations identified in previous studies, with the\noverarching goal of establishing a novel standard for credit default prediction\naccuracy. Our experimental findings validate the effectiveness of the ensemble\nmodel on the dataset, signifying substantial contributions to the field. This\ninnovative approach not only addresses existing obstacles but also sets a\nprecedent for advancing the accuracy and robustness of credit default\nprediction models.",
      "tldr_zh": "本研究针对消费者信贷领域的信用违约预测问题，提出了一种创新的集成方法框架，使用 LightGBM、XGBoost 和 LocalEnsemble 模块，通过利用不同的特征集来增强模型多样性和泛化能力，从而克服现有模型的局限性。实验结果显示，该框架在数据集上显著提高了预测准确性，比传统方法更有效，并为行业设定了新的准确性基准。这种方法不仅提升了风险缓解和贷款决策优化，还为未来信用违约预测模型的鲁棒性发展提供了重要贡献。",
      "categories": [
        "cs.CE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.17979v1",
      "published_date": "2024-02-28 01:48:54 UTC",
      "updated_date": "2024-02-28 01:48:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:08:47.592955"
    },
    {
      "arxiv_id": "2402.17978v2",
      "title": "Imagine, Initialize, and Explore: An Effective Exploration Method in Multi-Agent Reinforcement Learning",
      "title_zh": "想象、初始化和探索：多智能体强化学习中的有效探索方法",
      "authors": [
        "Zeyang Liu",
        "Lipeng Wan",
        "Xinrui Yang",
        "Zhuoran Chen",
        "Xingyu Chen",
        "Xuguang Lan"
      ],
      "abstract": "Effective exploration is crucial to discovering optimal strategies for\nmulti-agent reinforcement learning (MARL) in complex coordination tasks.\nExisting methods mainly utilize intrinsic rewards to enable committed\nexploration or use role-based learning for decomposing joint action spaces\ninstead of directly conducting a collective search in the entire\naction-observation space. However, they often face challenges obtaining\nspecific joint action sequences to reach successful states in long-horizon\ntasks. To address this limitation, we propose Imagine, Initialize, and Explore\n(IIE), a novel method that offers a promising solution for efficient\nmulti-agent exploration in complex scenarios. IIE employs a transformer model\nto imagine how the agents reach a critical state that can influence each\nother's transition functions. Then, we initialize the environment at this state\nusing a simulator before the exploration phase. We formulate the imagination as\na sequence modeling problem, where the states, observations, prompts, actions,\nand rewards are predicted autoregressively. The prompt consists of\ntimestep-to-go, return-to-go, influence value, and one-shot demonstration,\nspecifying the desired state and trajectory as well as guiding the action\ngeneration. By initializing agents at the critical states, IIE significantly\nincreases the likelihood of discovering potentially important under-explored\nregions. Despite its simplicity, empirical results demonstrate that our method\noutperforms multi-agent exploration baselines on the StarCraft Multi-Agent\nChallenge (SMAC) and SMACv2 environments. Particularly, IIE shows improved\nperformance in the sparse-reward SMAC tasks and produces more effective\ncurricula over the initialized states than other generative methods, such as\nCVAE-GAN and diffusion models.",
      "tldr_zh": "这篇论文提出了一种名为 Imagine, Initialize, and Explore (IIE) 的新方法，用于提升多智能体强化学习 (MARL) 中的探索效率，特别针对复杂协调任务中难以找到关键联合动作序列的问题。IIE 利用 Transformer 模型将状态、观察、提示、动作和奖励建模为序列预测问题，通过包含 timestep-to-go、return-to-go、influence value 和 one-shot demonstration 的提示来想象并初始化代理到达影响转移函数的关键状态。实验结果显示，IIE 在 StarCraft Multi-Agent Challenge (SMAC) 和 SMACv2 环境中优于基线方法，如 CVAE-GAN 和 diffusion models，尤其在稀疏奖励任务中显著提高了性能并提供了更有效的探索路径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "The 38th Annual AAAI Conference on Artificial Intelligence",
      "pdf_url": "http://arxiv.org/pdf/2402.17978v2",
      "published_date": "2024-02-28 01:45:01 UTC",
      "updated_date": "2024-03-01 11:08:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:09:01.568954"
    },
    {
      "arxiv_id": "2402.17975v1",
      "title": "Sample-Efficient Preference-based Reinforcement Learning with Dynamics Aware Rewards",
      "title_zh": "翻译失败",
      "authors": [
        "Katherine Metcalf",
        "Miguel Sarabia",
        "Natalie Mackraz",
        "Barry-John Theobald"
      ],
      "abstract": "Preference-based reinforcement learning (PbRL) aligns a robot behavior with\nhuman preferences via a reward function learned from binary feedback over agent\nbehaviors. We show that dynamics-aware reward functions improve the sample\nefficiency of PbRL by an order of magnitude. In our experiments we iterate\nbetween: (1) learning a dynamics-aware state-action representation (z^{sa}) via\na self-supervised temporal consistency task, and (2) bootstrapping the\npreference-based reward function from (z^{sa}), which results in faster policy\nlearning and better final policy performance. For example, on quadruped-walk,\nwalker-walk, and cheetah-run, with 50 preference labels we achieve the same\nperformance as existing approaches with 500 preference labels, and we recover\n83\\% and 66\\% of ground truth reward policy performance versus only 38\\% and\n21\\%. The performance gains demonstrate the benefits of explicitly learning a\ndynamics-aware reward model. Repo: \\texttt{https://github.com/apple/ml-reed}.",
      "tldr_zh": "这篇论文提出了一种样本效率更高的偏好强化学习(PbRL)方法，通过引入动态感知奖励函数，将学习效率提高一个数量级。方法包括迭代过程：首先通过自监督的时序一致性任务学习动态感知的状态-动作表示(z^{sa})，然后以此引导偏好-based 奖励函数，实现更快的策略学习。实验在 quadruped-walk、walker-walk 和 cheetah-run 等任务上显示，使用 50 个偏好标签即可达到现有方法需 500 个标签的性能，并恢复了 83% 和 66% 的真实奖励策略表现，远超基线方法的 38% 和 21%。这突显了显式学习动态感知奖励模型的益处。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "CoRL 2023. arXiv admin note: substantial text overlap with\n  arXiv:2211.06527",
      "pdf_url": "http://arxiv.org/pdf/2402.17975v1",
      "published_date": "2024-02-28 01:41:34 UTC",
      "updated_date": "2024-02-28 01:41:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:09:13.550173"
    },
    {
      "arxiv_id": "2402.17971v2",
      "title": "All in an Aggregated Image for In-Image Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Lei Wang",
        "Wanyu Xu",
        "Zhiqiang Hu",
        "Yihuai Lan",
        "Shan Dong",
        "Hao Wang",
        "Roy Ka-Wei Lee",
        "Ee-Peng Lim"
      ],
      "abstract": "This paper introduces a new in-context learning (ICL) mechanism called\nIn-Image Learning (I$^2$L) that combines demonstration examples, visual cues,\nand chain-of-thought reasoning into an aggregated image to enhance the\ncapabilities of Large Multimodal Models (e.g., GPT-4V) in multimodal reasoning\ntasks. Unlike previous approaches that rely on converting images to text or\nincorporating visual input into language models, I$^2$L consolidates all\ninformation into an aggregated image and leverages image processing,\nunderstanding, and reasoning abilities. This has several advantages: it reduces\ninaccurate textual descriptions of complex images, provides flexibility in\npositioning demonstration examples, and avoids multiple input images and\nlengthy prompts. We also introduce I$^2$L-Hybrid, a method that combines the\nstrengths of I$^2$L with other ICL methods. Specifically, it uses an automatic\nstrategy to select the most suitable method (I$^2$L or another certain ICL\nmethod) for a specific task instance. We conduct extensive experiments to\nassess the effectiveness of I$^2$L and I$^2$L-Hybrid on MathVista, which covers\na variety of complex multimodal reasoning tasks. Additionally, we investigate\nthe influence of image resolution, the number of demonstration examples in a\nsingle image, and the positions of these demonstrations in the aggregated image\non the effectiveness of I$^2$L. Our code is publicly available at\nhttps://github.com/AGI-Edgerunners/IIL.",
      "tldr_zh": "本论文提出了一种新的 In-Context Learning (ICL) 机制，称为 In-Image Learning (I²L)，它将演示示例、视觉线索和 Chain-of-Thought Reasoning 整合到一个聚合图像中，以提升 Large Multimodal Models（如 GPT-4V）在多模态推理任务中的性能。I²L 的优势在于避免图像转换为文本的 inaccuracies，提供演示示例的位置灵活性，并减少多个输入图像和冗长提示的需求。论文还引入 I²L-Hybrid 方法，通过自动策略选择 I²L 或其他 ICL 方法来处理特定任务实例。在 MathVista 数据集上的广泛实验显示，I²L 和 I²L-Hybrid 有效提升了模型表现，并探讨了图像分辨率、演示示例数量和位置等因素的影响。代码已在 GitHub 上公开。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2402.17971v2",
      "published_date": "2024-02-28 01:32:59 UTC",
      "updated_date": "2024-04-02 09:32:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:09:25.844720"
    },
    {
      "arxiv_id": "2403.07920v1",
      "title": "ProtLLM: An Interleaved Protein-Language LLM with Protein-as-Word Pre-Training",
      "title_zh": "翻译失败",
      "authors": [
        "Le Zhuo",
        "Zewen Chi",
        "Minghao Xu",
        "Heyan Huang",
        "Heqi Zheng",
        "Conghui He",
        "Xian-Ling Mao",
        "Wentao Zhang"
      ],
      "abstract": "We propose ProtLLM, a versatile cross-modal large language model (LLM) for\nboth protein-centric and protein-language tasks. ProtLLM features a unique\ndynamic protein mounting mechanism, enabling it to handle complex inputs where\nthe natural language text is interspersed with an arbitrary number of proteins.\nBesides, we propose the protein-as-word language modeling approach to train\nProtLLM. By developing a specialized protein vocabulary, we equip the model\nwith the capability to predict not just natural language but also proteins from\na vast pool of candidates. Additionally, we construct a large-scale interleaved\nprotein-text dataset, named InterPT, for pre-training. This dataset\ncomprehensively encompasses both (1) structured data sources like protein\nannotations and (2) unstructured data sources like biological research papers,\nthereby endowing ProtLLM with crucial knowledge for understanding proteins. We\nevaluate ProtLLM on classic supervised protein-centric tasks and explore its\nnovel protein-language applications. Experimental results demonstrate that\nProtLLM not only achieves superior performance against protein-specialized\nbaselines on protein-centric tasks but also induces zero-shot and in-context\nlearning capabilities on protein-language tasks.",
      "tldr_zh": "我们提出了 ProtLLM，一种多功能的跨模态大语言模型 (LLM)，用于处理蛋白质中心任务和蛋白质-语言任务，通过动态蛋白质挂载机制支持自然语言文本中嵌入任意数量蛋白质的复杂输入。此外，我们引入了 protein-as-word 语言建模方法，并构建了大规模交错蛋白质-文本数据集 InterPT，以训练模型，使其能够预测自然语言和蛋白质。实验结果显示，ProtLLM 在蛋白质中心任务上优于专用基线，并在蛋白质-语言任务上展现出零样本和上下文学习能力。",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "q-bio.BM",
      "comment": "https://protllm.github.io/project/",
      "pdf_url": "http://arxiv.org/pdf/2403.07920v1",
      "published_date": "2024-02-28 01:29:55 UTC",
      "updated_date": "2024-02-28 01:29:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:09:35.886724"
    },
    {
      "arxiv_id": "2402.17969v1",
      "title": "Vision Language Model-based Caption Evaluation Method Leveraging Visual Context Extraction",
      "title_zh": "翻译失败",
      "authors": [
        "Koki Maeda",
        "Shuhei Kurita",
        "Taiki Miyanishi",
        "Naoaki Okazaki"
      ],
      "abstract": "Given the accelerating progress of vision and language modeling, accurate\nevaluation of machine-generated image captions remains critical. In order to\nevaluate captions more closely to human preferences, metrics need to\ndiscriminate between captions of varying quality and content. However,\nconventional metrics fail short of comparing beyond superficial matches of\nwords or embedding similarities; thus, they still need improvement. This paper\npresents VisCE$^2$, a vision language model-based caption evaluation method.\nOur method focuses on visual context, which refers to the detailed content of\nimages, including objects, attributes, and relationships. By extracting and\norganizing them into a structured format, we replace the human-written\nreferences with visual contexts and help VLMs better understand the image,\nenhancing evaluation performance. Through meta-evaluation on multiple datasets,\nwe validated that VisCE$^2$ outperforms the conventional pre-trained metrics in\ncapturing caption quality and demonstrates superior consistency with human\njudgment.",
      "tldr_zh": "这篇论文提出了VisCE²，一种基于Vision Language Model的图像标题评估方法，旨在解决传统指标（如词语匹配或嵌入相似性）无法准确区分标题质量的问题。方法通过提取图像的视觉上下文，包括对象、属性和关系，并将其组织成结构化格式，以替换人工参考，从而帮助VLMs更好地理解图像内容并提升评估性能。在多个数据集上的元评估中，VisCE²表现出色，优于传统预训练指标，并在捕捉标题质量和与人类判断一致性方面取得了显著改进。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.17969v1",
      "published_date": "2024-02-28 01:29:36 UTC",
      "updated_date": "2024-02-28 01:29:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:09:51.809186"
    },
    {
      "arxiv_id": "2402.18600v1",
      "title": "Artificial Intelligence and Diabetes Mellitus: An Inside Look Through the Retina",
      "title_zh": "翻译失败",
      "authors": [
        "Yasin Sadeghi Bazargani",
        "Majid Mirzaei",
        "Navid Sobhi",
        "Mirsaeed Abdollahi",
        "Ali Jafarizadeh",
        "Siamak Pedrammehr",
        "Roohallah Alizadehsani",
        "Ru San Tan",
        "Sheikh Mohammed Shariful Islam",
        "U. Rajendra Acharya"
      ],
      "abstract": "Diabetes mellitus (DM) predisposes patients to vascular complications.\nRetinal images and vasculature reflect the body's micro- and macrovascular\nhealth. They can be used to diagnose DM complications, including diabetic\nretinopathy (DR), neuropathy, nephropathy, and atherosclerotic cardiovascular\ndisease, as well as forecast the risk of cardiovascular events. Artificial\nintelligence (AI)-enabled systems developed for high-throughput detection of DR\nusing digitized retinal images have become clinically adopted. Beyond DR\nscreening, AI integration also holds immense potential to address challenges\nassociated with the holistic care of the patient with DM. In this work, we aim\nto comprehensively review the literature for studies on AI applications based\non retinal images related to DM diagnosis, prognostication, and management. We\nwill describe the findings of holistic AI-assisted diabetes care, including but\nnot limited to DR screening, and discuss barriers to implementing such systems,\nincluding issues concerning ethics, data privacy, equitable access, and\nexplainability. With the ability to evaluate the patient's health status vis a\nvis DM complication as well as risk prognostication of future cardiovascular\ncomplications, AI-assisted retinal image analysis has the potential to become a\ncentral tool for modern personalized medicine in patients with DM.",
      "tldr_zh": "这篇论文探讨了人工智能（AI）在糖尿病（DM）管理中的应用，通过分析视网膜图像来评估微血管和宏观血管健康，从而诊断并发症如糖尿病视网膜病变（DR）、神经病变、肾病和心血管疾病，并预测未来风险。作者对相关文献进行了全面回顾，强调AI系统不仅用于DR筛查，还能扩展到DM的整体护理，包括预后和个性化治疗。论文还讨论了实施障碍，如伦理问题、数据隐私、公平访问和可解释性，并指出AI辅助视网膜图像分析有望成为DM患者现代个性化医学的核心工具。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "q-bio.TO",
        "J.3.2; J.3.3"
      ],
      "primary_category": "eess.IV",
      "comment": "44 Pages, 6 figures, 1 table, 166 references",
      "pdf_url": "http://arxiv.org/pdf/2402.18600v1",
      "published_date": "2024-02-28 00:31:17 UTC",
      "updated_date": "2024-02-28 00:31:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:10:00.835092"
    },
    {
      "arxiv_id": "2403.14659v1",
      "title": "Social Intelligence Data Infrastructure: Structuring the Present and Navigating the Future",
      "title_zh": "社会智能数据基础设施：结构化当前并导航未来",
      "authors": [
        "Minzhi Li",
        "Weiyan Shi",
        "Caleb Ziems",
        "Diyi Yang"
      ],
      "abstract": "As Natural Language Processing (NLP) systems become increasingly integrated\ninto human social life, these technologies will need to increasingly rely on\nsocial intelligence. Although there are many valuable datasets that benchmark\nisolated dimensions of social intelligence, there does not yet exist any body\nof work to join these threads into a cohesive subfield in which researchers can\nquickly identify research gaps and future directions. Towards this goal, we\nbuild a Social AI Data Infrastructure, which consists of a comprehensive social\nAI taxonomy and a data library of 480 NLP datasets. Our infrastructure allows\nus to analyze existing dataset efforts, and also evaluate language models'\nperformance in different social intelligence aspects. Our analyses demonstrate\nits utility in enabling a thorough understanding of current data landscape and\nproviding a holistic perspective on potential directions for future dataset\ndevelopment. We show there is a need for multifaceted datasets, increased\ndiversity in language and culture, more long-tailed social situations, and more\ninteractive data in future social intelligence data efforts.",
      "tldr_zh": "本文构建了 Social AI Data Infrastructure，包括一个全面的社会 AI 分类法和一个包含 480 个 NLP 数据集的库，以整合和分析社会智能研究领域的现有努力。利用这一基础设施，研究者评估了语言模型在不同社会智能方面的表现，并揭示了当前数据景观的不足，如缺乏多方面数据集、语言和文化多样性、长尾社会情况以及互动数据。最终，这为识别研究空白和指导未来数据集开发提供了整体视角，促进 NLP 系统在社会情境中的应用。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.14659v1",
      "published_date": "2024-02-28 00:22:42 UTC",
      "updated_date": "2024-02-28 00:22:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:10:12.738600"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 113,
  "processed_papers_count": 113,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-17T11:10:39.094850"
}
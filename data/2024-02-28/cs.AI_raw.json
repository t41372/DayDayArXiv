[
  {
    "arxiv_id": "2402.18759v2",
    "title": "Learning with Language-Guided State Abstractions",
    "authors": [
      "Andi Peng",
      "Ilia Sucholutsky",
      "Belinda Z. Li",
      "Theodore R. Sumers",
      "Thomas L. Griffiths",
      "Jacob Andreas",
      "Julie A. Shah"
    ],
    "abstract": "We describe a framework for using natural language to design state\nabstractions for imitation learning. Generalizable policy learning in\nhigh-dimensional observation spaces is facilitated by well-designed state\nrepresentations, which can surface important features of an environment and\nhide irrelevant ones. These state representations are typically manually\nspecified, or derived from other labor-intensive labeling procedures. Our\nmethod, LGA (language-guided abstraction), uses a combination of natural\nlanguage supervision and background knowledge from language models (LMs) to\nautomatically build state representations tailored to unseen tasks. In LGA, a\nuser first provides a (possibly incomplete) description of a target task in\nnatural language; next, a pre-trained LM translates this task description into\na state abstraction function that masks out irrelevant features; finally, an\nimitation policy is trained using a small number of demonstrations and\nLGA-generated abstract states. Experiments on simulated robotic tasks show that\nLGA yields state abstractions similar to those designed by humans, but in a\nfraction of the time, and that these abstractions improve generalization and\nrobustness in the presence of spurious correlations and ambiguous\nspecifications. We illustrate the utility of the learned abstractions on mobile\nmanipulation tasks with a Spot robot.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.18759v2",
    "published_date": "2024-02-28 23:57:04 UTC",
    "updated_date": "2024-03-06 15:53:46 UTC"
  },
  {
    "arxiv_id": "2402.18747v2",
    "title": "Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains",
    "authors": [
      "Vilém Zouhar",
      "Shuoyang Ding",
      "Anna Currey",
      "Tatyana Badeka",
      "Jenyuan Wang",
      "Brian Thompson"
    ],
    "abstract": "We introduce a new, extensive multidimensional quality metrics (MQM)\nannotated dataset covering 11 language pairs in the biomedical domain. We use\nthis dataset to investigate whether machine translation (MT) metrics which are\nfine-tuned on human-generated MT quality judgements are robust to domain shifts\nbetween training and inference. We find that fine-tuned metrics exhibit a\nsubstantial performance drop in the unseen domain scenario relative to metrics\nthat rely on the surface form, as well as pre-trained metrics which are not\nfine-tuned on MT quality judgments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.18747v2",
    "published_date": "2024-02-28 23:01:24 UTC",
    "updated_date": "2024-06-04 04:14:16 UTC"
  },
  {
    "arxiv_id": "2403.00032v1",
    "title": "Time to Cite: Modeling Citation Networks using the Dynamic Impact Single-Event Embedding Model",
    "authors": [
      "Nikolaos Nakis",
      "Abdulkadir Celikkanat",
      "Louis Boucherie",
      "Sune Lehmann",
      "Morten Mørup"
    ],
    "abstract": "Understanding the structure and dynamics of scientific research, i.e., the\nscience of science (SciSci), has become an important area of research in order\nto address imminent questions including how scholars interact to advance\nscience, how disciplines are related and evolve, and how research impact can be\nquantified and predicted. Central to the study of SciSci has been the analysis\nof citation networks. Here, two prominent modeling methodologies have been\nemployed: one is to assess the citation impact dynamics of papers using\nparametric distributions, and the other is to embed the citation networks in a\nlatent space optimal for characterizing the static relations between papers in\nterms of their citations. Interestingly, citation networks are a prominent\nexample of single-event dynamic networks, i.e., networks for which each dyad\nonly has a single event (i.e., the point in time of citation). We presently\npropose a novel likelihood function for the characterization of such\nsingle-event networks. Using this likelihood, we propose the Dynamic Impact\nSingle-Event Embedding model (DISEE). The \\textsc{\\modelabbrev} model\ncharacterizes the scientific interactions in terms of a latent distance model\nin which random effects account for citation heterogeneity while the\ntime-varying impact is characterized using existing parametric representations\nfor assessment of dynamic impact. We highlight the proposed approach on several\nreal citation networks finding that the DISEE well reconciles static latent\ndistance network embedding approaches with classical dynamic impact\nassessments.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.DL"
    ],
    "primary_category": "cs.SI",
    "comment": "Accepted for AISTATS 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.00032v1",
    "published_date": "2024-02-28 22:59:26 UTC",
    "updated_date": "2024-02-28 22:59:26 UTC"
  },
  {
    "arxiv_id": "2402.18743v1",
    "title": "A revision on Multi-Criteria Decision Making methods for Multi-UAV Mission Planning Support",
    "authors": [
      "Cristian Ramirez-Atencia",
      "Victor Rodriguez-Fernandez",
      "David Camacho"
    ],
    "abstract": "Over the last decade, Unmanned Aerial Vehicles (UAVs) have been extensively\nused in many commercial applications due to their manageability and risk\navoidance. One of the main problems considered is the Mission Planning for\nmultiple UAVs, where a solution plan must be found satisfying the different\nconstraints of the problem. This problem has multiple variables that must be\noptimized simultaneously, such as the makespan, the cost of the mission or the\nrisk. Therefore, the problem has a lot of possible optimal solutions, and the\noperator must select the final solution to be executed among them. In order to\nreduce the workload of the operator in this decision process, a Decision\nSupport System (DSS) becomes necessary. In this work, a DSS consisting of\nranking and filtering systems, which order and reduce the optimal solutions,\nhas been designed. With regard to the ranking system, a wide range of\nMulti-Criteria Decision Making (MCDM) methods, including some fuzzy MCDM, are\ncompared on a multi-UAV mission planning scenario, in order to study which\nmethod could fit better in a multi-UAV decision support system. Expert\noperators have evaluated the solutions returned, and the results show, on the\none hand, that fuzzy methods generally achieve better average scores, and on\nthe other, that all of the tested methods perform better when the preferences\nof the operators are biased towards a specific variable, and worse when their\npreferences are balanced. For the filtering system, a similarity function based\non the proximity of the solutions has been designed, and on top of that, a\nthreshold is tuned empirically to decide how to filter solutions without losing\nmuch of the hypervolume of the space of solutions.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "Preprint submitted and acepted in Expert Systems with Applications",
    "pdf_url": "http://arxiv.org/pdf/2402.18743v1",
    "published_date": "2024-02-28 22:54:08 UTC",
    "updated_date": "2024-02-28 22:54:08 UTC"
  },
  {
    "arxiv_id": "2402.18732v1",
    "title": "GAIA: Categorical Foundations of Generative AI",
    "authors": [
      "Sridhar Mahadevan"
    ],
    "abstract": "In this paper, we propose GAIA, a generative AI architecture based on\ncategory theory. GAIA is based on a hierarchical model where modules are\norganized as a simplicial complex. Each simplicial complex updates its internal\nparameters biased on information it receives from its superior simplices and in\nturn relays updates to its subordinate sub-simplices. Parameter updates are\nformulated in terms of lifting diagrams over simplicial sets, where inner and\nouter horn extensions correspond to different types of learning problems.\nBackpropagation is modeled as an endofunctor over the category of parameters,\nleading to a coalgebraic formulation of deep learning.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "65 pages. arXiv admin note: text overlap with arXiv:2212.08981",
    "pdf_url": "http://arxiv.org/pdf/2402.18732v1",
    "published_date": "2024-02-28 22:25:02 UTC",
    "updated_date": "2024-02-28 22:25:02 UTC"
  },
  {
    "arxiv_id": "2402.18726v1",
    "title": "Unveiling Privacy, Memorization, and Input Curvature Links",
    "authors": [
      "Deepak Ravikumar",
      "Efstathia Soufleri",
      "Abolfazl Hashemi",
      "Kaushik Roy"
    ],
    "abstract": "Deep Neural Nets (DNNs) have become a pervasive tool for solving many\nemerging problems. However, they tend to overfit to and memorize the training\nset. Memorization is of keen interest since it is closely related to several\nconcepts such as generalization, noisy learning, and privacy. To study\nmemorization, Feldman (2019) proposed a formal score, however its computational\nrequirements limit its practical use. Recent research has shown empirical\nevidence linking input loss curvature (measured by the trace of the loss\nHessian w.r.t inputs) and memorization. It was shown to be ~3 orders of\nmagnitude more efficient than calculating the memorization score. However,\nthere is a lack of theoretical understanding linking memorization with input\nloss curvature. In this paper, we not only investigate this connection but also\nextend our analysis to establish theoretical links between differential\nprivacy, memorization, and input loss curvature. First, we derive an upper\nbound on memorization characterized by both differential privacy and input loss\ncurvature. Second, we present a novel insight showing that input loss curvature\nis upper-bounded by the differential privacy parameter. Our theoretical\nfindings are further empirically validated using deep models on CIFAR and\nImageNet datasets, showing a strong correlation between our theoretical\npredictions and results observed in practice.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18726v1",
    "published_date": "2024-02-28 22:02:10 UTC",
    "updated_date": "2024-02-28 22:02:10 UTC"
  },
  {
    "arxiv_id": "2402.18724v1",
    "title": "Learning Associative Memories with Gradient Descent",
    "authors": [
      "Vivien Cabannes",
      "Berfin Simsek",
      "Alberto Bietti"
    ],
    "abstract": "This work focuses on the training dynamics of one associative memory module\nstoring outer products of token embeddings. We reduce this problem to the study\nof a system of particles, which interact according to properties of the data\ndistribution and correlations between embeddings. Through theory and\nexperiments, we provide several insights. In overparameterized regimes, we\nobtain logarithmic growth of the ``classification margins.'' Yet, we show that\nimbalance in token frequencies and memory interferences due to correlated\nembeddings lead to oscillatory transitory regimes. The oscillations are more\npronounced with large step sizes, which can create benign loss spikes, although\nthese learning rates speed up the dynamics and accelerate the asymptotic\nconvergence. In underparameterized regimes, we illustrate how the cross-entropy\nloss can lead to suboptimal memorization schemes. Finally, we assess the\nvalidity of our findings on small Transformer models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18724v1",
    "published_date": "2024-02-28 21:47:30 UTC",
    "updated_date": "2024-02-28 21:47:30 UTC"
  },
  {
    "arxiv_id": "2402.18715v1",
    "title": "Commonsense Ontology Micropatterns",
    "authors": [
      "Andrew Eells",
      "Brandon Dave",
      "Pascal Hitzler",
      "Cogan Shimizu"
    ],
    "abstract": "The previously introduced Modular Ontology Modeling methodology (MOMo)\nattempts to mimic the human analogical process by using modular patterns to\nassemble more complex concepts. To support this, MOMo organizes organizes\nontology design patterns into design libraries, which are programmatically\nqueryable, to support accelerated ontology development, for both human and\nautomated processes. However, a major bottleneck to large-scale deployment of\nMOMo is the (to-date) limited availability of ready-to-use ontology design\npatterns. At the same time, Large Language Models have quickly become a source\nof common knowledge and, in some cases, replacing search engines for questions.\nIn this paper, we thus present a collection of 104 ontology design patterns\nrepresenting often occurring nouns, curated from the common-sense knowledge\navailable in LLMs, organized into a fully-annotated modular ontology design\nlibrary ready for use with MOMo.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18715v1",
    "published_date": "2024-02-28 21:23:54 UTC",
    "updated_date": "2024-02-28 21:23:54 UTC"
  },
  {
    "arxiv_id": "2402.18700v2",
    "title": "Learning to Compress Prompt in Natural Language Formats",
    "authors": [
      "Yu-Neng Chuang",
      "Tianwei Xing",
      "Chia-Yuan Chang",
      "Zirui Liu",
      "Xun Chen",
      "Xia Hu"
    ],
    "abstract": "Large language models (LLMs) are great at processing multiple natural\nlanguage processing tasks, but their abilities are constrained by inferior\nperformance with long context, slow inference speed, and the high cost of\ncomputing the results. Deploying LLMs with precise and informative context\nhelps users process large-scale datasets more effectively and cost-efficiently.\nExisting works rely on compressing long prompt contexts into soft prompts.\nHowever, soft prompt compression encounters limitations in transferability\nacross different LLMs, especially API-based LLMs. To this end, this work aims\nto compress lengthy prompts in the form of natural language with LLM\ntransferability. This poses two challenges: (i) Natural Language (NL) prompts\nare incompatible with back-propagation, and (ii) NL prompts lack flexibility in\nimposing length constraints. In this work, we propose a Natural Language Prompt\nEncapsulation (Nano-Capsulator) framework compressing original prompts into NL\nformatted Capsule Prompt while maintaining the prompt utility and\ntransferability. Specifically, to tackle the first challenge, the\nNano-Capsulator is optimized by a reward function that interacts with the\nproposed semantics preserving loss. To address the second question, the\nNano-Capsulator is optimized by a reward function featuring length constraints.\nExperimental results demonstrate that the Capsule Prompt can reduce 81.4% of\nthe original length, decrease inference latency up to 4.5x, and save 80.1% of\nbudget overheads while providing transferability across diverse LLMs and\ndifferent datasets.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18700v2",
    "published_date": "2024-02-28 20:41:21 UTC",
    "updated_date": "2024-04-02 02:38:31 UTC"
  },
  {
    "arxiv_id": "2403.00835v4",
    "title": "CLLMs: Consistency Large Language Models",
    "authors": [
      "Siqi Kou",
      "Lanxiang Hu",
      "Zhezhi He",
      "Zhijie Deng",
      "Hao Zhang"
    ],
    "abstract": "Parallel decoding methods such as Jacobi decoding show promise for more\nefficient LLM inference as it breaks the sequential nature of the LLM decoding\nprocess and transforms it into parallelizable computation. However, in\npractice, it achieves little speedup compared to traditional autoregressive\n(AR) decoding, primarily because Jacobi decoding seldom accurately predicts\nmore than one token in a single fixed-point iteration step. To address this, we\ndevelop a new approach aimed at realizing fast convergence from any state to\nthe fixed point on a Jacobi trajectory. This is accomplished by refining the\ntarget LLM to consistently predict the fixed point given any state as input.\nExtensive experiments demonstrate the effectiveness of our method, showing\n2.4$\\times$ to 3.4$\\times$ improvements in generation speed while preserving\ngeneration quality across both domain-specific and open-domain benchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "In the proceedings of the 41st International Conference on Machine\n  Learning (ICML) 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.00835v4",
    "published_date": "2024-02-28 20:17:04 UTC",
    "updated_date": "2024-06-13 08:41:28 UTC"
  },
  {
    "arxiv_id": "2403.00030v2",
    "title": "GraphPub: Generation of Differential Privacy Graph with High Availability",
    "authors": [
      "Wanghan Xu",
      "Bin Shi",
      "Ao Liu",
      "Jiqiang Zhang",
      "Bo Dong"
    ],
    "abstract": "In recent years, with the rapid development of graph neural networks (GNN),\nmore and more graph datasets have been published for GNN tasks. However, when\nan upstream data owner publishes graph data, there are often many privacy\nconcerns, because many real-world graph data contain sensitive information like\nperson's friend list. Differential privacy (DP) is a common method to protect\nprivacy, but due to the complex topological structure of graph data, applying\nDP on graphs often affects the message passing and aggregation of GNN models,\nleading to a decrease in model accuracy. In this paper, we propose a novel\ngraph edge protection framework, graph publisher (GraphPub), which can protect\ngraph topology while ensuring that the availability of data is basically\nunchanged. Through reverse learning and the encoder-decoder mechanism, we\nsearch for some false edges that do not have a large negative impact on the\naggregation of node features, and use them to replace some real edges. The\nmodified graph will be published, which is difficult to distinguish between\nreal and false data. Sufficient experiments prove that our framework achieves\nmodel accuracy close to the original graph with an extremely low privacy\nbudget.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.00030v2",
    "published_date": "2024-02-28 20:02:55 UTC",
    "updated_date": "2024-03-05 05:34:55 UTC"
  },
  {
    "arxiv_id": "2402.18679v4",
    "title": "Data Interpreter: An LLM Agent For Data Science",
    "authors": [
      "Sirui Hong",
      "Yizhang Lin",
      "Bang Liu",
      "Bangbang Liu",
      "Binhao Wu",
      "Ceyao Zhang",
      "Chenxing Wei",
      "Danyang Li",
      "Jiaqi Chen",
      "Jiayi Zhang",
      "Jinlin Wang",
      "Li Zhang",
      "Lingyao Zhang",
      "Min Yang",
      "Mingchen Zhuge",
      "Taicheng Guo",
      "Tuo Zhou",
      "Wei Tao",
      "Xiangru Tang",
      "Xiangtao Lu",
      "Xiawu Zheng",
      "Xinbing Liang",
      "Yaying Fei",
      "Yuheng Cheng",
      "Zhibin Gou",
      "Zongze Xu",
      "Chenglin Wu"
    ],
    "abstract": "Large Language Model (LLM)-based agents have shown effectiveness across many\napplications. However, their use in data science scenarios requiring solving\nlong-term interconnected tasks, dynamic data adjustments and domain expertise\nremains challenging. Previous approaches primarily focus on individual tasks,\nmaking it difficult to assess the complete data science workflow. Moreover,\nthey struggle to handle real-time changes in intermediate data and fail to\nadapt dynamically to evolving task dependencies inherent to data science\nproblems. In this paper, we present Data Interpreter, an LLM-based agent\ndesigned to automatically solve various data science problems end-to-end. Our\nData Interpreter incorporates two key modules: 1) Hierarchical Graph Modeling,\nwhich breaks down complex problems into manageable subproblems, enabling\ndynamic node generation and graph optimization; and 2) Programmable Node\nGeneration, a technique that refines and verifies each subproblem to\niteratively improve code generation results and robustness. Extensive\nexperiments consistently demonstrate the superiority of Data Interpreter. On\nInfiAgent-DABench, it achieves a 25% performance boost, raising accuracy from\n75.9% to 94.9%. For machine learning and open-ended tasks, it improves\nperformance from 88% to 95%, and from 60% to 97%, respectively. Moreover, on\nthe MATH dataset, Data Interpreter achieves remarkable performance with a 26%\nimprovement compared to state-of-the-art baselines. The code is available at\nhttps://github.com/geekan/MetaGPT.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18679v4",
    "published_date": "2024-02-28 19:49:55 UTC",
    "updated_date": "2024-10-15 15:52:57 UTC"
  },
  {
    "arxiv_id": "2402.18677v1",
    "title": "Fault Tolerant Neural Control Barrier Functions for Robotic Systems under Sensor Faults and Attacks",
    "authors": [
      "Hongchao Zhang",
      "Luyao Niu",
      "Andrew Clark",
      "Radha Poovendran"
    ],
    "abstract": "Safety is a fundamental requirement of many robotic systems. Control barrier\nfunction (CBF)-based approaches have been proposed to guarantee the safety of\nrobotic systems. However, the effectiveness of these approaches highly relies\non the choice of CBFs. Inspired by the universal approximation power of neural\nnetworks, there is a growing trend toward representing CBFs using neural\nnetworks, leading to the notion of neural CBFs (NCBFs). Current NCBFs, however,\nare trained and deployed in benign environments, making them ineffective for\nscenarios where robotic systems experience sensor faults and attacks. In this\npaper, we study safety-critical control synthesis for robotic systems under\nsensor faults and attacks. Our main contribution is the development and\nsynthesis of a new class of CBFs that we term fault tolerant neural control\nbarrier function (FT-NCBF). We derive the necessary and sufficient conditions\nfor FT-NCBFs to guarantee safety, and develop a data-driven method to learn\nFT-NCBFs by minimizing a loss function constructed using the derived\nconditions. Using the learned FT-NCBF, we synthesize a control input and\nformally prove the safety guarantee provided by our approach. We demonstrate\nour proposed approach using two case studies: obstacle avoidance problem for an\nautonomous mobile robot and spacecraft rendezvous problem, with code available\nvia https://github.com/HongchaoZhang-HZ/FTNCBF.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18677v1",
    "published_date": "2024-02-28 19:44:19 UTC",
    "updated_date": "2024-02-28 19:44:19 UTC"
  },
  {
    "arxiv_id": "2402.18673v2",
    "title": "Trends, Applications, and Challenges in Human Attention Modelling",
    "authors": [
      "Giuseppe Cartella",
      "Marcella Cornia",
      "Vittorio Cuculo",
      "Alessandro D'Amelio",
      "Dario Zanca",
      "Giuseppe Boccignone",
      "Rita Cucchiara"
    ],
    "abstract": "Human attention modelling has proven, in recent years, to be particularly\nuseful not only for understanding the cognitive processes underlying visual\nexploration, but also for providing support to artificial intelligence models\nthat aim to solve problems in various domains, including image and video\nprocessing, vision-and-language applications, and language modelling. This\nsurvey offers a reasoned overview of recent efforts to integrate human\nattention mechanisms into contemporary deep learning models and discusses\nfuture research directions and challenges. For a comprehensive overview on the\nongoing research refer to our dedicated repository available at\nhttps://github.com/aimagelab/awesome-human-visual-attention.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at IJCAI 2024 Survey Track",
    "pdf_url": "http://arxiv.org/pdf/2402.18673v2",
    "published_date": "2024-02-28 19:35:30 UTC",
    "updated_date": "2024-04-22 17:54:17 UTC"
  },
  {
    "arxiv_id": "2403.05579v1",
    "title": "Cultural Bias in Explainable AI Research: A Systematic Analysis",
    "authors": [
      "Uwe Peters",
      "Mary Carman"
    ],
    "abstract": "For synergistic interactions between humans and artificial intelligence (AI)\nsystems, AI outputs often need to be explainable to people. Explainable AI\n(XAI) systems are commonly tested in human user studies. However, whether XAI\nresearchers consider potential cultural differences in human explanatory needs\nremains unexplored. We highlight psychological research that found significant\ndifferences in human explanations between many people from Western, commonly\nindividualist countries and people from non-Western, often collectivist\ncountries. We argue that XAI research currently overlooks these variations and\nthat many popular XAI designs implicitly and problematically assume that\nWestern explanatory needs are shared cross-culturally. Additionally, we\nsystematically reviewed over 200 XAI user studies and found that most studies\ndid not consider relevant cultural variations, sampled only Western\npopulations, but drew conclusions about human-XAI interactions more generally.\nWe also analyzed over 30 literature reviews of XAI studies. Most reviews did\nnot mention cultural differences in explanatory needs or flag overly broad\ncross-cultural extrapolations of XAI user study results. Combined, our analyses\nprovide evidence of a cultural bias toward Western populations in XAI research,\nhighlighting an important knowledge gap regarding how culturally diverse users\nmay respond to widely used XAI systems that future work can and should address.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.05579v1",
    "published_date": "2024-02-28 19:30:32 UTC",
    "updated_date": "2024-02-28 19:30:32 UTC"
  },
  {
    "arxiv_id": "2402.18659v5",
    "title": "Large Language Models and Games: A Survey and Roadmap",
    "authors": [
      "Roberto Gallotta",
      "Graham Todd",
      "Marvin Zammit",
      "Sam Earle",
      "Antonios Liapis",
      "Julian Togelius",
      "Georgios N. Yannakakis"
    ],
    "abstract": "Recent years have seen an explosive increase in research on large language\nmodels (LLMs), and accompanying public engagement on the topic. While starting\nas a niche area within natural language processing, LLMs have shown remarkable\npotential across a broad range of applications and domains, including games.\nThis paper surveys the current state of the art across the various applications\nof LLMs in and for games, and identifies the different roles LLMs can take\nwithin a game. Importantly, we discuss underexplored areas and promising\ndirections for future uses of LLMs in games and we reconcile the potential and\nlimitations of LLMs within the games domain. As the first comprehensive survey\nand roadmap at the intersection of LLMs and games, we are hopeful that this\npaper will serve as the basis for groundbreaking research and innovation in\nthis exciting new field.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for publication at the IEEE Transactions on Games (19 pages,\n  6 figures)",
    "pdf_url": "http://arxiv.org/pdf/2402.18659v5",
    "published_date": "2024-02-28 19:09:08 UTC",
    "updated_date": "2024-12-09 14:41:04 UTC"
  },
  {
    "arxiv_id": "2402.18651v1",
    "title": "Quantifying Human Priors over Social and Navigation Networks",
    "authors": [
      "Gecia Bravo-Hermsdorff"
    ],
    "abstract": "Human knowledge is largely implicit and relational -- do we have a friend in\ncommon? can I walk from here to there? In this work, we leverage the\ncombinatorial structure of graphs to quantify human priors over such relational\ndata. Our experiments focus on two domains that have been continuously relevant\nover evolutionary timescales: social interaction and spatial navigation. We\nfind that some features of the inferred priors are remarkably consistent, such\nas the tendency for sparsity as a function of graph size. Other features are\ndomain-specific, such as the propensity for triadic closure in social\ninteractions. More broadly, our work demonstrates how nonclassical statistical\nanalysis of indirect behavioral experiments can be used to efficiently model\nlatent biases in the data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI",
      "physics.soc-ph",
      "q-bio.NC",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "Published on Proceedings of the 40th International Conference on\n  Machine Learning (ICML), PMLR 202:3063-3105, 2023",
    "pdf_url": "http://arxiv.org/pdf/2402.18651v1",
    "published_date": "2024-02-28 19:00:36 UTC",
    "updated_date": "2024-02-28 19:00:36 UTC"
  },
  {
    "arxiv_id": "2402.18649v1",
    "title": "A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems",
    "authors": [
      "Fangzhou Wu",
      "Ning Zhang",
      "Somesh Jha",
      "Patrick McDaniel",
      "Chaowei Xiao"
    ],
    "abstract": "Large Language Model (LLM) systems are inherently compositional, with\nindividual LLM serving as the core foundation with additional layers of objects\nsuch as plugins, sandbox, and so on. Along with the great potential, there are\nalso increasing concerns over the security of such probabilistic intelligent\nsystems. However, existing studies on LLM security often focus on individual\nLLM, but without examining the ecosystem through the lens of LLM systems with\nother objects (e.g., Frontend, Webtool, Sandbox, and so on). In this paper, we\nsystematically analyze the security of LLM systems, instead of focusing on the\nindividual LLMs. To do so, we build on top of the information flow and\nformulate the security of LLM systems as constraints on the alignment of the\ninformation flow within LLM and between LLM and other objects. Based on this\nconstruction and the unique probabilistic nature of LLM, the attack surface of\nthe LLM system can be decomposed into three key components: (1) multi-layer\nsecurity analysis, (2) analysis of the existence of constraints, and (3)\nanalysis of the robustness of these constraints. To ground this new attack\nsurface, we propose a multi-layer and multi-step approach and apply it to the\nstate-of-art LLM system, OpenAI GPT4. Our investigation exposes several\nsecurity issues, not just within the LLM model itself but also in its\nintegration with other components. We found that although the OpenAI GPT4 has\ndesigned numerous safety constraints to improve its safety features, these\nsafety constraints are still vulnerable to attackers. To further demonstrate\nthe real-world threats of our discovered vulnerabilities, we construct an\nend-to-end attack where an adversary can illicitly acquire the user's chat\nhistory, all without the need to manipulate the user's input or gain direct\naccess to OpenAI GPT4. Our demo is in the link:\nhttps://fzwark.github.io/LLM-System-Attack-Demo/",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18649v1",
    "published_date": "2024-02-28 19:00:12 UTC",
    "updated_date": "2024-02-28 19:00:12 UTC"
  },
  {
    "arxiv_id": "2402.18571v3",
    "title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards",
    "authors": [
      "Haoxiang Wang",
      "Yong Lin",
      "Wei Xiong",
      "Rui Yang",
      "Shizhe Diao",
      "Shuang Qiu",
      "Han Zhao",
      "Tong Zhang"
    ],
    "abstract": "Fine-grained control over large language models (LLMs) remains a significant\nchallenge, hindering their adaptability to diverse user needs. While\nReinforcement Learning from Human Feedback (RLHF) shows promise in aligning\nLLMs, its reliance on scalar rewards often limits its ability to capture\ndiverse user preferences in real-world applications. To address this\nlimitation, we introduce the Directional Preference Alignment (DPA) framework.\nUnlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling\nto represent diverse preference profiles. Additionally, DPA models user\npreferences as directions (i.e., unit vectors) in the reward space to achieve\nuser-dependent preference control. Our method involves training a\nmulti-objective reward model and then fine-tuning the LLM with a\npreference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF\nmethod adopted by Llama 2. This method enjoys a better performance trade-off\nacross various reward objectives. In comparison with the scalar-reward RLHF,\nDPA offers users intuitive control over LLM generation: they can arithmetically\nspecify their desired trade-offs (e.g., more helpfulness with less verbosity).\nWe also validate the effectiveness of DPA with real-world alignment experiments\non Mistral-7B. Our method provides straightforward arithmetic control over the\ntrade-off between helpfulness and verbosity while maintaining competitive\nperformance with strong baselines such as Direct Preference Optimization (DPO).",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "The code and model are released at\n  https://github.com/Haoxiang-Wang/directional-preference-alignment",
    "pdf_url": "http://arxiv.org/pdf/2402.18571v3",
    "published_date": "2024-02-28 18:58:25 UTC",
    "updated_date": "2024-03-06 08:07:02 UTC"
  },
  {
    "arxiv_id": "2402.18563v1",
    "title": "Approaching Human-Level Forecasting with Language Models",
    "authors": [
      "Danny Halawi",
      "Fred Zhang",
      "Chen Yueh-Han",
      "Jacob Steinhardt"
    ],
    "abstract": "Forecasting future events is important for policy and decision making. In\nthis work, we study whether language models (LMs) can forecast at the level of\ncompetitive human forecasters. Towards this goal, we develop a\nretrieval-augmented LM system designed to automatically search for relevant\ninformation, generate forecasts, and aggregate predictions. To facilitate our\nstudy, we collect a large dataset of questions from competitive forecasting\nplatforms. Under a test set published after the knowledge cut-offs of our LMs,\nwe evaluate the end-to-end performance of our system against the aggregates of\nhuman forecasts. On average, the system nears the crowd aggregate of\ncompetitive forecasters, and in some settings surpasses it. Our work suggests\nthat using LMs to forecast the future could provide accurate predictions at\nscale and help to inform institutional decision making.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18563v1",
    "published_date": "2024-02-28 18:54:18 UTC",
    "updated_date": "2024-02-28 18:54:18 UTC"
  },
  {
    "arxiv_id": "2402.18540v2",
    "title": "Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates",
    "authors": [
      "Kaifeng Lyu",
      "Haoyu Zhao",
      "Xinran Gu",
      "Dingli Yu",
      "Anirudh Goyal",
      "Sanjeev Arora"
    ],
    "abstract": "Public LLMs such as the Llama 2-Chat underwent alignment training and were\nconsidered safe. Recently Qi et al. [2024] reported that even benign\nfine-tuning on seemingly safe datasets can give rise to unsafe behaviors in the\nmodels. The current paper is about methods and best practices to mitigate such\nloss of alignment. We focus on the setting where a public model is fine-tuned\nbefore serving users for specific usage, where the model should improve on the\ndownstream task while maintaining alignment. Through extensive experiments on\nseveral chat models (Meta's Llama 2-Chat, Mistral AI's Mistral 7B Instruct\nv0.2, and OpenAI's GPT-3.5 Turbo), this paper uncovers that the prompt\ntemplates used during fine-tuning and inference play a crucial role in\npreserving safety alignment, and proposes the ``Pure Tuning, Safe Testing''\n(PTST) strategy -- fine-tune models without a safety prompt, but include it at\ntest time. This seemingly counterintuitive strategy incorporates an intended\ndistribution shift to encourage alignment preservation. Fine-tuning experiments\non GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the\nrise of unsafe behaviors.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.18540v2",
    "published_date": "2024-02-28 18:23:49 UTC",
    "updated_date": "2025-01-17 01:43:21 UTC"
  },
  {
    "arxiv_id": "2402.18617v1",
    "title": "ELA: Exploited Level Augmentation for Offline Learning in Zero-Sum Games",
    "authors": [
      "Shiqi Lei",
      "Kanghoon Lee",
      "Linjing Li",
      "Jinkyoo Park",
      "Jiachen Li"
    ],
    "abstract": "Offline learning has become widely used due to its ability to derive\neffective policies from offline datasets gathered by expert demonstrators\nwithout interacting with the environment directly. Recent research has explored\nvarious ways to enhance offline learning efficiency by considering the\ncharacteristics (e.g., expertise level or multiple demonstrators) of the\ndataset. However, a different approach is necessary in the context of zero-sum\ngames, where outcomes vary significantly based on the strategy of the opponent.\nIn this study, we introduce a novel approach that uses unsupervised learning\ntechniques to estimate the exploited level of each trajectory from the offline\ndataset of zero-sum games made by diverse demonstrators. Subsequently, we\nincorporate the estimated exploited level into the offline learning to maximize\nthe influence of the dominant strategy. Our method enables interpretable\nexploited level estimation in multiple zero-sum games and effectively\nidentifies dominant strategy data. Also, our exploited level augmented offline\nlearning significantly enhances the original offline learning algorithms\nincluding imitation learning and offline reinforcement learning for zero-sum\ngames.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.GT",
    "comment": "12 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.18617v1",
    "published_date": "2024-02-28 17:44:02 UTC",
    "updated_date": "2024-02-28 17:44:02 UTC"
  },
  {
    "arxiv_id": "2402.18616v1",
    "title": "JCLEC-MO: a Java suite for solving many-objective optimization engineering problems",
    "authors": [
      "Aurora Ramírez",
      "José Raúl Romero",
      "Carlos García-Martínez",
      "Sebastián Ventura"
    ],
    "abstract": "Although metaheuristics have been widely recognized as efficient techniques\nto solve real-world optimization problems, implementing them from scratch\nremains difficult for domain-specific experts without programming skills. In\nthis scenario, metaheuristic optimization frameworks are a practical\nalternative as they provide a variety of algorithms composed of customized\nelements, as well as experimental support. Recently, many engineering problems\nrequire to optimize multiple or even many objectives, increasing the interest\nin appropriate metaheuristic algorithms and frameworks that might integrate new\nspecific requirements while maintaining the generality and reusability\nprinciples they were conceived for. Based on this idea, this paper introduces\nJCLEC-MO, a Java framework for both multi- and many-objective optimization that\nenables engineers to apply, or adapt, a great number of multi-objective\nalgorithms with little coding effort. A case study is developed and explained\nto show how JCLEC-MO can be used to address many-objective engineering\nproblems, often requiring the inclusion of domain-specific elements, and to\nanalyze experimental outcomes by means of conveniently connected R utilities.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "68T20",
      "D.0; I.2.8"
    ],
    "primary_category": "cs.NE",
    "comment": "41 pages, 5 figures, journal paper",
    "pdf_url": "http://arxiv.org/pdf/2402.18616v1",
    "published_date": "2024-02-28 17:38:01 UTC",
    "updated_date": "2024-02-28 17:38:01 UTC"
  },
  {
    "arxiv_id": "2402.18496v3",
    "title": "Language Models Represent Beliefs of Self and Others",
    "authors": [
      "Wentao Zhu",
      "Zhining Zhang",
      "Yizhou Wang"
    ],
    "abstract": "Understanding and attributing mental states, known as Theory of Mind (ToM),\nemerges as a fundamental capability for human social reasoning. While Large\nLanguage Models (LLMs) appear to possess certain ToM abilities, the mechanisms\nunderlying these capabilities remain elusive. In this study, we discover that\nit is possible to linearly decode the belief status from the perspectives of\nvarious agents through neural activations of language models, indicating the\nexistence of internal representations of self and others' beliefs. By\nmanipulating these representations, we observe dramatic changes in the models'\nToM performance, underscoring their pivotal role in the social reasoning\nprocess. Additionally, our findings extend to diverse social reasoning tasks\nthat involve different causal inference patterns, suggesting the potential\ngeneralizability of these representations.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "project page: https://walter0807.github.io/RepBelief/",
    "pdf_url": "http://arxiv.org/pdf/2402.18496v3",
    "published_date": "2024-02-28 17:25:59 UTC",
    "updated_date": "2024-05-30 12:43:01 UTC"
  },
  {
    "arxiv_id": "2402.18487v1",
    "title": "Human-Centric Aware UAV Trajectory Planning in Search and Rescue Missions Employing Multi-Objective Reinforcement Learning with AHP and Similarity-Based Experience Replay",
    "authors": [
      "Mahya Ramezani",
      "Jose Luis Sanchez-Lopez"
    ],
    "abstract": "The integration of Unmanned Aerial Vehicles (UAVs) into Search and Rescue\n(SAR) missions presents a promising avenue for enhancing operational efficiency\nand effectiveness. However, the success of these missions is not solely\ndependent on the technical capabilities of the drones but also on their\nacceptance and interaction with humans on the ground. This paper explores the\neffect of human-centric factor in UAV trajectory planning for SAR missions. We\nintroduce a novel approach based on the reinforcement learning augmented with\nAnalytic Hierarchy Process and novel similarity-based experience replay to\noptimize UAV trajectories, balancing operational objectives with human comfort\nand safety considerations. Additionally, through a comprehensive survey, we\ninvestigate the impact of gender cues and anthropomorphism in UAV design on\npublic acceptance and trust, revealing significant implications for drone\ninteraction strategies in SAR. Our contributions include (1) a reinforcement\nlearning framework for UAV trajectory planning that dynamically integrates\nmulti-objective considerations, (2) an analysis of human perceptions towards\ngendered and anthropomorphized drones in SAR contexts, and (3) the application\nof similarity-based experience replay for enhanced learning efficiency in\ncomplex SAR scenarios. The findings offer valuable insights into designing UAV\nsystems that are not only technically proficient but also aligned with\nhuman-centric values.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18487v1",
    "published_date": "2024-02-28 17:10:22 UTC",
    "updated_date": "2024-02-28 17:10:22 UTC"
  },
  {
    "arxiv_id": "2402.18485v3",
    "title": "A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist",
    "authors": [
      "Wentao Zhang",
      "Lingxuan Zhao",
      "Haochong Xia",
      "Shuo Sun",
      "Jiaze Sun",
      "Molei Qin",
      "Xinyi Li",
      "Yuqing Zhao",
      "Yilei Zhao",
      "Xinyu Cai",
      "Longtao Zheng",
      "Xinrun Wang",
      "Bo An"
    ],
    "abstract": "Financial trading is a crucial component of the markets, informed by a\nmultimodal information landscape encompassing news, prices, and Kline charts,\nand encompasses diverse tasks such as quantitative trading and high-frequency\ntrading with various assets. While advanced AI techniques like deep learning\nand reinforcement learning are extensively utilized in finance, their\napplication in financial trading tasks often faces challenges due to inadequate\nhandling of multimodal data and limited generalizability across various tasks.\nTo address these challenges, we present FinAgent, a multimodal foundational\nagent with tool augmentation for financial trading. FinAgent's market\nintelligence module processes a diverse range of data-numerical, textual, and\nvisual-to accurately analyze the financial market. Its unique dual-level\nreflection module not only enables rapid adaptation to market dynamics but also\nincorporates a diversified memory retrieval system, enhancing the agent's\nability to learn from historical data and improve decision-making processes.\nThe agent's emphasis on reasoning for actions fosters trust in its financial\ndecisions. Moreover, FinAgent integrates established trading strategies and\nexpert insights, ensuring that its trading approaches are both data-driven and\nrooted in sound financial principles. With comprehensive experiments on 6\nfinancial datasets, including stocks and Crypto, FinAgent significantly\noutperforms 9 state-of-the-art baselines in terms of 6 financial metrics with\nover 36% average improvement on profit. Specifically, a 92.27% return (a 84.39%\nrelative improvement) is achieved on one dataset. Notably, FinAgent is the\nfirst advanced multimodal foundation agent designed for financial trading\ntasks.",
    "categories": [
      "q-fin.TR",
      "cs.AI"
    ],
    "primary_category": "q-fin.TR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18485v3",
    "published_date": "2024-02-28 17:06:54 UTC",
    "updated_date": "2024-06-28 10:35:56 UTC"
  },
  {
    "arxiv_id": "2402.18477v4",
    "title": "Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes",
    "authors": [
      "Georg Manten",
      "Cecilia Casolo",
      "Emilio Ferrucci",
      "Søren Wengel Mogensen",
      "Cristopher Salvi",
      "Niki Kilbertus"
    ],
    "abstract": "Inferring the causal structure underlying stochastic dynamical systems from\nobservational data holds great promise in domains ranging from science and\nhealth to finance. Such processes can often be accurately modeled via\nstochastic differential equations (SDEs), which naturally imply causal\nrelationships via \"which variables enter the differential of which other\nvariables\". In this paper, we develop conditional independence (CI) constraints\non coordinate processes over selected intervals that are Markov with respect to\nthe acyclic dependence graph (allowing self-loops) induced by a general SDE\nmodel. We then provide a sound and complete causal discovery algorithm, capable\nof handling both fully and partially observed data, and uniquely recovering the\nunderlying or induced ancestral graph by exploiting time directionality\nassuming a CI oracle. Finally, to make our algorithm practically usable, we\nalso propose a flexible, consistent signature kernel-based CI test to infer\nthese constraints from data. We extensively benchmark the CI test in isolation\nand as part of our causal discovery algorithms, outperforming existing\napproaches in SDE models and beyond.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18477v4",
    "published_date": "2024-02-28 16:58:31 UTC",
    "updated_date": "2025-03-03 11:25:26 UTC"
  },
  {
    "arxiv_id": "2402.18449v1",
    "title": "HOP to the Next Tasks and Domains for Continual Learning in NLP",
    "authors": [
      "Umberto Michieli",
      "Mete Ozay"
    ],
    "abstract": "Continual Learning (CL) aims to learn a sequence of problems (i.e., tasks and\ndomains) by transferring knowledge acquired on previous problems, whilst\navoiding forgetting of past ones. Different from previous approaches which\nfocused on CL for one NLP task or domain in a specific use-case, in this paper,\nwe address a more general CL setting to learn from a sequence of problems in a\nunique framework. Our method, HOP, permits to hop across tasks and domains by\naddressing the CL problem along three directions: (i) we employ a set of\nadapters to generalize a large pre-trained model to unseen problems, (ii) we\ncompute high-order moments over the distribution of embedded representations to\ndistinguish independent and correlated statistics across different tasks and\ndomains, (iii) we process this enriched information with auxiliary heads\nspecialized for each end problem. Extensive experimental campaign on 4 NLP\napplications, 5 benchmarks and 2 CL setups demonstrates the effectiveness of\nour HOP.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "AAAI 2024. Main + supplmentary",
    "pdf_url": "http://arxiv.org/pdf/2402.18449v1",
    "published_date": "2024-02-28 16:21:02 UTC",
    "updated_date": "2024-02-28 16:21:02 UTC"
  },
  {
    "arxiv_id": "2402.18443v1",
    "title": "LeMo-NADe: Multi-Parameter Neural Architecture Discovery with LLMs",
    "authors": [
      "Md Hafizur Rahman",
      "Prabuddha Chakraborty"
    ],
    "abstract": "Building efficient neural network architectures can be a time-consuming task\nrequiring extensive expert knowledge. This task becomes particularly\nchallenging for edge devices because one has to consider parameters such as\npower consumption during inferencing, model size, inferencing speed, and CO2\nemissions. In this article, we introduce a novel framework designed to\nautomatically discover new neural network architectures based on user-defined\nparameters, an expert system, and an LLM trained on a large amount of\nopen-domain knowledge. The introduced framework (LeMo-NADe) is tailored to be\nused by non-AI experts, does not require a predetermined neural architecture\nsearch space, and considers a large set of edge device-specific parameters. We\nimplement and validate this proposed neural architecture discovery framework\nusing CIFAR-10, CIFAR-100, and ImageNet16-120 datasets while using GPT-4 Turbo\nand Gemini as the LLM component. We observe that the proposed framework can\nrapidly (within hours) discover intricate neural network models that perform\nextremely well across a diverse set of application settings defined by the\nuser.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages, 5 figures, 10 tables and 3 algorithms",
    "pdf_url": "http://arxiv.org/pdf/2402.18443v1",
    "published_date": "2024-02-28 16:13:44 UTC",
    "updated_date": "2024-02-28 16:13:44 UTC"
  },
  {
    "arxiv_id": "2403.00833v1",
    "title": "Position Paper: Agent AI Towards a Holistic Intelligence",
    "authors": [
      "Qiuyuan Huang",
      "Naoki Wake",
      "Bidipta Sarkar",
      "Zane Durante",
      "Ran Gong",
      "Rohan Taori",
      "Yusuke Noda",
      "Demetri Terzopoulos",
      "Noboru Kuno",
      "Ade Famoti",
      "Ashley Llorens",
      "John Langford",
      "Hoi Vo",
      "Li Fei-Fei",
      "Katsu Ikeuchi",
      "Jianfeng Gao"
    ],
    "abstract": "Recent advancements in large foundation models have remarkably enhanced our\nunderstanding of sensory information in open-world environments. In leveraging\nthe power of foundation models, it is crucial for AI research to pivot away\nfrom excessive reductionism and toward an emphasis on systems that function as\ncohesive wholes. Specifically, we emphasize developing Agent AI -- an embodied\nsystem that integrates large foundation models into agent actions. The emerging\nfield of Agent AI spans a wide range of existing embodied and agent-based\nmultimodal interactions, including robotics, gaming, and healthcare systems,\netc. In this paper, we propose a novel large action model to achieve embodied\nintelligent behavior, the Agent Foundation Model. On top of this idea, we\ndiscuss how agent AI exhibits remarkable capabilities across a variety of\ndomains and tasks, challenging our understanding of learning and cognition.\nFurthermore, we discuss the potential of Agent AI from an interdisciplinary\nperspective, underscoring AI cognition and consciousness within scientific\ndiscourse. We believe that those discussions serve as a basis for future\nresearch directions and encourage broader societal engagement.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "22 pages, 4 figures. arXiv admin note: substantial text overlap with\n  arXiv:2401.03568",
    "pdf_url": "http://arxiv.org/pdf/2403.00833v1",
    "published_date": "2024-02-28 16:09:56 UTC",
    "updated_date": "2024-02-28 16:09:56 UTC"
  },
  {
    "arxiv_id": "2402.18439v3",
    "title": "Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication",
    "authors": [
      "Weize Chen",
      "Chenfei Yuan",
      "Jiarui Yuan",
      "Yusheng Su",
      "Chen Qian",
      "Cheng Yang",
      "Ruobing Xie",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "abstract": "Natural language (NL) has long been the predominant format for human\ncognition and communication, and by extension, has been similarly pivotal in\nthe development and application of Large Language Models (LLMs). Yet, besides\nNL, LLMs have seen various non-NL formats during pre-training, such as code and\nlogical expression. NL's status as the optimal format for LLMs, particularly in\nsingle-LLM reasoning and multi-agent communication, has not been thoroughly\nexamined. In this work, we challenge the default use of NL by exploring the\nutility of non-NL formats in these contexts. We show that allowing LLMs to\nautonomously select the most suitable format before reasoning or communicating\nleads to a 3.3 to 5.7\\% improvement in reasoning efficiency for different LLMs,\nand up to a 72.7\\% reduction in token usage in multi-agent communication, all\nwhile maintaining communicative effectiveness. Our comprehensive analysis\nfurther reveals that LLMs can devise a format from limited task instructions\nand that the devised format is effectively transferable across different LLMs.\nIntriguingly, the structured communication format decided by LLMs exhibits\nnotable parallels with established agent communication languages, suggesting a\nnatural evolution towards efficient, structured communication in agent\ncommunication. Our code is released at\n\\url{https://github.com/thunlp/AutoForm}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Code release at https://github.com/thunlp/AutoForm",
    "pdf_url": "http://arxiv.org/pdf/2402.18439v3",
    "published_date": "2024-02-28 16:07:54 UTC",
    "updated_date": "2024-06-19 01:42:22 UTC"
  },
  {
    "arxiv_id": "2403.00026v1",
    "title": "Learning to Deliver: a Foundation Model for the Montreal Capacitated Vehicle Routing Problem",
    "authors": [
      "Samuel J. K. Chin",
      "Matthias Winkenbach",
      "Akash Srivastava"
    ],
    "abstract": "In this paper, we present the Foundation Model for the Montreal Capacitated\nVehicle Routing Problem (FM-MCVRP), a novel Deep Learning (DL) model that\napproximates high-quality solutions to a variant of the Capacitated Vehicle\nRouting Problem (CVRP) that characterizes many real-world applications. The\nso-called Montreal Capacitated Vehicle Routing Problem (MCVRP), first formally\ndescribed by Bengio et al. (2021), is defined on a fixed and finite graph,\nwhich is analogous to a city. Each MCVRP instance is essentially the sub-graph\nconnecting a randomly sampled subset of the nodes in the fixed graph, which\nrepresent a set of potential addresses in a real-world delivery problem on a\ngiven day. Our work exploits this problem structure to frame the MCVRP as an\nanalogous Natural Language Processing (NLP) task. Specifically, we leverage a\nTransformer architecture embedded in a Large Language Model (LLM) framework to\ntrain our model in a supervised manner on computationally inexpensive,\nsub-optimal MCVRP solutions obtained algorithmically. Through comprehensive\ncomputational experiments, we show that FM-MCVRP produces better MCVRP\nsolutions than the training data and generalizes to larger sized problem\ninstances not seen during training. Even when compared to near-optimal\nsolutions from state-of-the-art heuristics, FM-MCVRP yields competitive results\ndespite being trained on inferior data. For instance, for 400-customer\nproblems, FM-MCVRP solutions on average fall within 2% of the benchmark. Our\nresults further demonstrate that unlike prior works in the literature, FM-MCVRP\nis a unified model, which performs consistently and reliably on a range of\nproblem instance sizes and parameter values such as the vehicle capacity.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.00026v1",
    "published_date": "2024-02-28 16:02:29 UTC",
    "updated_date": "2024-02-28 16:02:29 UTC"
  },
  {
    "arxiv_id": "2402.18426v1",
    "title": "A Relational Inductive Bias for Dimensional Abstraction in Neural Networks",
    "authors": [
      "Declan Campbell",
      "Jonathan D. Cohen"
    ],
    "abstract": "The human cognitive system exhibits remarkable flexibility and generalization\ncapabilities, partly due to its ability to form low-dimensional, compositional\nrepresentations of the environment. In contrast, standard neural network\narchitectures often struggle with abstract reasoning tasks, overfitting, and\nrequiring extensive data for training. This paper investigates the impact of\nthe relational bottleneck -- a mechanism that focuses processing on relations\namong inputs -- on the learning of factorized representations conducive to\ncompositional coding and the attendant flexibility of processing. We\ndemonstrate that such a bottleneck not only improves generalization and\nlearning efficiency, but also aligns network performance with human-like\nbehavioral biases. Networks trained with the relational bottleneck developed\northogonal representations of feature dimensions latent in the dataset,\nreflecting the factorized structure thought to underlie human cognitive\nflexibility. Moreover, the relational network mimics human biases towards\nregularity without pre-specified symbolic primitives, suggesting that the\nbottleneck fosters the emergence of abstract representations that confer\nflexibility akin to symbols.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18426v1",
    "published_date": "2024-02-28 15:51:05 UTC",
    "updated_date": "2024-02-28 15:51:05 UTC"
  },
  {
    "arxiv_id": "2402.18424v2",
    "title": "Emotion Classification in Low and Moderate Resource Languages",
    "authors": [
      "Shabnam Tafreshi",
      "Shubham Vatsal",
      "Mona Diab"
    ],
    "abstract": "It is important to be able to analyze the emotional state of people around\nthe globe. There are 7100+ active languages spoken around the world and\nbuilding emotion classification for each language is labor intensive.\nParticularly for low-resource and endangered languages, building emotion\nclassification can be quite challenging. We present a cross-lingual emotion\nclassifier, where we train an emotion classifier with resource-rich languages\n(i.e. \\textit{English} in our work) and transfer the learning to low and\nmoderate resource languages. We compare and contrast two approaches of transfer\nlearning from a high-resource language to a low or moderate-resource language.\nOne approach projects the annotation from a high-resource language to low and\nmoderate-resource language in parallel corpora and the other one uses direct\ntransfer from high-resource language to the other languages. We show the\nefficacy of our approaches on 6 languages: Farsi, Arabic, Spanish, Ilocano,\nOdia, and Azerbaijani. Our results indicate that our approaches outperform\nrandom baselines and transfer emotions across languages successfully. For all\nlanguages, the direct cross-lingual transfer of emotion yields better results.\nWe also create annotated emotion-labeled resources for four languages: Farsi,\nAzerbaijani, Ilocano and Odia.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18424v2",
    "published_date": "2024-02-28 15:46:09 UTC",
    "updated_date": "2024-11-07 19:02:53 UTC"
  },
  {
    "arxiv_id": "2402.18419v2",
    "title": "Can GPT Improve the State of Prior Authorization via Guideline Based Automated Question Answering?",
    "authors": [
      "Shubham Vatsal",
      "Ayush Singh",
      "Shabnam Tafreshi"
    ],
    "abstract": "Health insurance companies have a defined process called prior authorization\n(PA) which is a health plan cost-control process that requires doctors and\nother healthcare professionals to get clearance in advance from a health plan\nbefore performing a particular procedure on a patient in order to be eligible\nfor payment coverage. For health insurance companies, approving PA requests for\npatients in the medical domain is a time-consuming and challenging task. One of\nthose key challenges is validating if a request matches up to certain criteria\nsuch as age, gender, etc. In this work, we evaluate whether GPT can validate\nnumerous key factors, in turn helping health plans reach a decision drastically\nfaster. We frame it as a question answering task, prompting GPT to answer a\nquestion from patient electronic health record. We experiment with different\nconventional prompting techniques as well as introduce our own novel prompting\ntechnique. Moreover, we report qualitative assessment by humans on the natural\nlanguage generation outputs from our approach. Results show that our method\nachieves superior performance with the mean weighted F1 score of 0.61 as\ncompared to its standard counterparts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18419v2",
    "published_date": "2024-02-28 15:39:53 UTC",
    "updated_date": "2024-10-25 16:19:18 UTC"
  },
  {
    "arxiv_id": "2402.18409v4",
    "title": "A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision-Language Models",
    "authors": [
      "Xiujie Song",
      "Mengyue Wu",
      "Kenny Q. Zhu",
      "Chunhao Zhang",
      "Yanyi Chen"
    ],
    "abstract": "Large Vision-Language Models (LVLMs), despite their recent success, are\nhardly comprehensively tested for their cognitive abilities. Inspired by the\nprevalent use of the Cookie Theft task in human cognitive tests, we propose a\nnovel evaluation benchmark to evaluate high-level cognitive abilities of LVLMs\nusing images with rich semantics. The benchmark consists of 251 images along\nwith comprehensive annotations. It defines eight reasoning capabilities and\ncomprises an image description task and a visual question answering task. Our\nevaluation of well-known LVLMs shows that there is still a significant gap in\ncognitive abilities between LVLMs and humans.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18409v4",
    "published_date": "2024-02-28 15:28:36 UTC",
    "updated_date": "2025-02-13 02:37:06 UTC"
  },
  {
    "arxiv_id": "2403.00025v3",
    "title": "On the Challenges and Opportunities in Generative AI",
    "authors": [
      "Laura Manduchi",
      "Kushagra Pandey",
      "Clara Meister",
      "Robert Bamler",
      "Ryan Cotterell",
      "Sina Däubener",
      "Sophie Fellenz",
      "Asja Fischer",
      "Thomas Gärtner",
      "Matthias Kirchler",
      "Marius Kloft",
      "Yingzhen Li",
      "Christoph Lippert",
      "Gerard de Melo",
      "Eric Nalisnick",
      "Björn Ommer",
      "Rajesh Ranganath",
      "Maja Rudolph",
      "Karen Ullrich",
      "Guy Van den Broeck",
      "Julia E Vogt",
      "Yixin Wang",
      "Florian Wenzel",
      "Frank Wood",
      "Stephan Mandt",
      "Vincent Fortuin"
    ],
    "abstract": "The field of deep generative modeling has grown rapidly in the last few\nyears. With the availability of massive amounts of training data coupled with\nadvances in scalable unsupervised learning paradigms, recent large-scale\ngenerative models show tremendous promise in synthesizing high-resolution\nimages and text, as well as structured data such as videos and molecules.\nHowever, we argue that current large-scale generative AI models exhibit several\nfundamental shortcomings that hinder their widespread adoption across domains.\nIn this work, our objective is to identify these issues and highlight key\nunresolved challenges in modern generative AI paradigms that should be\naddressed to further enhance their capabilities, versatility, and reliability.\nBy identifying these challenges, we aim to provide researchers with insights\nfor exploring fruitful research directions, thus fostering the development of\nmore robust and accessible generative AI solutions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.00025v3",
    "published_date": "2024-02-28 15:19:33 UTC",
    "updated_date": "2025-03-20 18:07:27 UTC"
  },
  {
    "arxiv_id": "2402.18393v3",
    "title": "Decictor: Towards Evaluating the Robustness of Decision-Making in Autonomous Driving Systems",
    "authors": [
      "Mingfei Cheng",
      "Yuan Zhou",
      "Xiaofei Xie",
      "Junjie Wang",
      "Guozhu Meng",
      "Kairui Yang"
    ],
    "abstract": "Autonomous Driving System (ADS) testing is crucial in ADS development, with\nthe current primary focus being on safety. However, the evaluation of\nnon-safety-critical performance, particularly the ADS's ability to make optimal\ndecisions and produce optimal paths for autonomous vehicles (AVs), is also\nvital to ensure the intelligence and reduce risks of AVs. Currently, there is\nlittle work dedicated to assessing the robustness of ADSs' path-planning\ndecisions (PPDs), i.e., whether an ADS can maintain the optimal PPD after an\ninsignificant change in the environment. The key challenges include the lack of\nclear oracles for assessing PPD optimality and the difficulty in searching for\nscenarios that lead to non-optimal PPDs. To fill this gap, in this paper, we\nfocus on evaluating the robustness of ADSs' PPDs and propose the first method,\nDecictor, for generating non-optimal decision scenarios (NoDSs), where the ADS\ndoes not plan optimal paths for AVs. Decictor comprises three main components:\nNon-invasive Mutation, Consistency Check, and Feedback. To overcome the oracle\nchallenge, Non-invasive Mutation is devised to implement conservative\nmodifications, ensuring the preservation of the original optimal path in the\nmutated scenarios. Subsequently, the Consistency Check is applied to determine\nthe presence of non-optimal PPDs by comparing the driving paths in the original\nand mutated scenarios. To deal with the challenge of large environment space,\nwe design Feedback metrics that integrate spatial and temporal dimensions of\nthe AV's movement. These metrics are crucial for effectively steering the\ngeneration of NoDSs. We evaluate Decictor on Baidu Apollo, an open-source and\nproduction-grade ADS. The experimental results validate the effectiveness of\nDecictor in detecting non-optimal PPDs of ADSs.",
    "categories": [
      "cs.AI",
      "cs.NE",
      "cs.RO",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18393v3",
    "published_date": "2024-02-28 15:13:33 UTC",
    "updated_date": "2025-01-28 17:36:51 UTC"
  },
  {
    "arxiv_id": "2402.18392v2",
    "title": "Unveiling the Potential of Robustness in Selecting Conditional Average Treatment Effect Estimators",
    "authors": [
      "Yiyan Huang",
      "Cheuk Hang Leung",
      "Siyi Wang",
      "Yijun Li",
      "Qi Wu"
    ],
    "abstract": "The growing demand for personalized decision-making has led to a surge of\ninterest in estimating the Conditional Average Treatment Effect (CATE). Various\ntypes of CATE estimators have been developed with advancements in machine\nlearning and causal inference. However, selecting the desirable CATE estimator\nthrough a conventional model validation procedure remains impractical due to\nthe absence of counterfactual outcomes in observational data. Existing\napproaches for CATE estimator selection, such as plug-in and pseudo-outcome\nmetrics, face two challenges. First, they must determine the metric form and\nthe underlying machine learning models for fitting nuisance parameters (e.g.,\noutcome function, propensity function, and plug-in learner). Second, they lack\na specific focus on selecting a robust CATE estimator. To address these\nchallenges, this paper introduces a Distributionally Robust Metric (DRM) for\nCATE estimator selection. The proposed DRM is nuisance-free, eliminating the\nneed to fit models for nuisance parameters, and it effectively prioritizes the\nselection of a distributionally robust CATE estimator. The experimental results\nvalidate the effectiveness of the DRM method in selecting CATE estimators that\nare robust to the distribution shift incurred by covariate shift and hidden\nconfounders.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "econ.EM",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper was accepted by NeurIPS-2024",
    "pdf_url": "http://arxiv.org/pdf/2402.18392v2",
    "published_date": "2024-02-28 15:12:24 UTC",
    "updated_date": "2024-10-31 11:08:36 UTC"
  },
  {
    "arxiv_id": "2402.18390v1",
    "title": "Neuromorphic Event-Driven Semantic Communication in Microgrids",
    "authors": [
      "Xiaoguang Diao",
      "Yubo Song",
      "Subham Sahoo",
      "Yuan Li"
    ],
    "abstract": "Synergies between advanced communications, computing and artificial\nintelligence are unraveling new directions of coordinated operation and\nresiliency in microgrids. On one hand, coordination among sources is\nfacilitated by distributed, privacy-minded processing at multiple locations,\nwhereas on the other hand, it also creates exogenous data arrival paths for\nadversaries that can lead to cyber-physical attacks amongst other reliability\nissues in the communication layer. This long-standing problem necessitates new\nintrinsic ways of exchanging information between converters through power lines\nto optimize the system's control performance. Going beyond the existing power\nand data co-transfer technologies that are limited by efficiency and\nscalability concerns, this paper proposes neuromorphic learning to implant\ncommunicative features using spiking neural networks (SNNs) at each node, which\nis trained collaboratively in an online manner simply using the power exchanges\nbetween the nodes. As opposed to the conventional neuromorphic sensors that\noperate with spiking signals, we employ an event-driven selective process to\ncollect sparse data for training of SNNs. Finally, its multi-fold effectiveness\nand reliable performance is validated under simulation conditions with\ndifferent microgrid topologies and components to establish a new direction in\nthe sense-actuate-compute cycle for power electronic dominated grids and\nmicrogrids.",
    "categories": [
      "cs.ET",
      "cs.AI",
      "cs.NE",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.ET",
    "comment": "The manuscript has been accepted for publication in IEEE Transactions\n  on Smart Grid",
    "pdf_url": "http://arxiv.org/pdf/2402.18390v1",
    "published_date": "2024-02-28 15:11:02 UTC",
    "updated_date": "2024-02-28 15:11:02 UTC"
  },
  {
    "arxiv_id": "2402.18609v4",
    "title": "ICE-SEARCH: A Language Model-Driven Feature Selection Approach",
    "authors": [
      "Tianze Yang",
      "Tianyi Yang",
      "Fuyuan Lyu",
      "Shaoshan Liu",
      "Xue",
      "Liu"
    ],
    "abstract": "This study unveils the In-Context Evolutionary Search (ICE-SEARCH) method,\nwhich is among the first works that melds large language models (LLMs) with\nevolutionary algorithms for feature selection (FS) tasks and demonstrates its\neffectiveness in Medical Predictive Analytics (MPA) applications. ICE-SEARCH\nharnesses the crossover and mutation capabilities inherent in LLMs within an\nevolutionary framework, significantly improving FS through the model's\ncomprehensive world knowledge and its adaptability to a variety of roles. Our\nevaluation of this methodology spans three crucial MPA tasks: stroke,\ncardiovascular disease, and diabetes, where ICE-SEARCH outperforms traditional\nFS methods in pinpointing essential features for medical applications.\nICE-SEARCH achieves State-of-the-Art (SOTA) performance in stroke prediction\nand diabetes prediction; the Decision-Randomized ICE-SEARCH ranks as SOTA in\ncardiovascular disease prediction. The study emphasizes the critical role of\nincorporating domain-specific insights, illustrating ICE-SEARCH's robustness,\ngeneralizability, and convergence. This opens avenues for further research into\ncomprehensive and intricate FS landscapes, marking a significant stride in the\napplication of artificial intelligence in medical predictive analytics.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18609v4",
    "published_date": "2024-02-28 15:06:25 UTC",
    "updated_date": "2024-05-08 18:05:43 UTC"
  },
  {
    "arxiv_id": "2402.18385v1",
    "title": "The First Place Solution of WSDM Cup 2024: Leveraging Large Language Models for Conversational Multi-Doc QA",
    "authors": [
      "Yiming Li",
      "Zhao Zhang"
    ],
    "abstract": "Conversational multi-doc question answering aims to answer specific questions\nbased on the retrieved documents as well as the contextual conversations. In\nthis paper, we introduce our winning approach for the \"Conversational Multi-Doc\nQA\" challenge in WSDM Cup 2024, which exploits the superior natural language\nunderstanding and generation capability of Large Language Models (LLMs). We\nfirst adapt LLMs to the task, then devise a hybrid training strategy to make\nthe most of in-domain unlabeled data. Moreover, an advanced text embedding\nmodel is adopted to filter out potentially irrelevant documents and several\napproaches are designed and compared for the model ensemble. Equipped with all\nthese techniques, our solution finally ranked 1st place in WSDM Cup 2024,\nsurpassing its rivals to a large extent. The source codes have been released at\nhttps://github.com/zhangzhao219/WSDM-Cup-2024.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "1st solution for WSDM Cup 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.18385v1",
    "published_date": "2024-02-28 15:05:43 UTC",
    "updated_date": "2024-02-28 15:05:43 UTC"
  },
  {
    "arxiv_id": "2402.18381v1",
    "title": "Large Language Models As Evolution Strategies",
    "authors": [
      "Robert Tjarko Lange",
      "Yingtao Tian",
      "Yujin Tang"
    ],
    "abstract": "Large Transformer models are capable of implementing a plethora of so-called\nin-context learning algorithms. These include gradient descent, classification,\nsequence completion, transformation, and improvement. In this work, we\ninvestigate whether large language models (LLMs), which never explicitly\nencountered the task of black-box optimization, are in principle capable of\nimplementing evolutionary optimization algorithms. While previous works have\nsolely focused on language-based task specification, we move forward and focus\non the zero-shot application of LLMs to black-box optimization. We introduce a\nnovel prompting strategy, consisting of least-to-most sorting of discretized\npopulation members and querying the LLM to propose an improvement to the mean\nstatistic, i.e. perform a type of black-box recombination operation.\nEmpirically, we find that our setup allows the user to obtain an LLM-based\nevolution strategy, which we call `EvoLLM', that robustly outperforms baseline\nalgorithms such as random search and Gaussian Hill Climbing on synthetic BBOB\nfunctions as well as small neuroevolution tasks. Hence, LLMs can act as\n`plug-in' in-context recombination operators. We provide several comparative\nstudies of the LLM's model size, prompt strategy, and context construction.\nFinally, we show that one can flexibly improve EvoLLM's performance by\nproviding teacher algorithm information via instruction fine-tuning on\npreviously collected teacher optimization trajectories.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.18381v1",
    "published_date": "2024-02-28 15:02:17 UTC",
    "updated_date": "2024-02-28 15:02:17 UTC"
  },
  {
    "arxiv_id": "2402.18377v2",
    "title": "Out-of-Domain Generalization in Dynamical Systems Reconstruction",
    "authors": [
      "Niclas Göring",
      "Florian Hess",
      "Manuel Brenner",
      "Zahra Monfared",
      "Daniel Durstewitz"
    ],
    "abstract": "In science we are interested in finding the governing equations, the\ndynamical rules, underlying empirical phenomena. While traditionally scientific\nmodels are derived through cycles of human insight and experimentation,\nrecently deep learning (DL) techniques have been advanced to reconstruct\ndynamical systems (DS) directly from time series data. State-of-the-art\ndynamical systems reconstruction (DSR) methods show promise in capturing\ninvariant and long-term properties of observed DS, but their ability to\ngeneralize to unobserved domains remains an open challenge. Yet, this is a\ncrucial property we would expect from any viable scientific theory. In this\nwork, we provide a formal framework that addresses generalization in DSR. We\nexplain why and how out-of-domain (OOD) generalization (OODG) in DSR profoundly\ndiffers from OODG considered elsewhere in machine learning. We introduce\nmathematical notions based on topological concepts and ergodic theory to\nformalize the idea of learnability of a DSR model. We formally prove that\nblack-box DL techniques, without adequate structural priors, generally will not\nbe able to learn a generalizing DSR model. We also show this empirically,\nconsidering major classes of DSR algorithms proposed so far, and illustrate\nwhere and why they fail to generalize across the whole phase space. Our study\nprovides the first comprehensive mathematical treatment of OODG in DSR, and\ngives a deeper conceptual understanding of where the fundamental problems in\nOODG lie and how they could possibly be addressed in practice.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.DS",
      "nlin.CD"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18377v2",
    "published_date": "2024-02-28 14:52:58 UTC",
    "updated_date": "2024-06-07 23:38:25 UTC"
  },
  {
    "arxiv_id": "2402.18376v2",
    "title": "Tokenization Is More Than Compression",
    "authors": [
      "Craig W. Schmidt",
      "Varshini Reddy",
      "Haoran Zhang",
      "Alec Alameddine",
      "Omri Uzan",
      "Yuval Pinter",
      "Chris Tanner"
    ],
    "abstract": "Tokenization is a foundational step in natural language processing (NLP)\ntasks, bridging raw text and language models. Existing tokenization approaches\nlike Byte-Pair Encoding (BPE) originate from the field of data compression, and\nit has been suggested that the effectiveness of BPE stems from its ability to\ncondense text into a relatively small number of tokens. We test the hypothesis\nthat fewer tokens lead to better downstream performance by introducing\nPathPiece, a new tokenizer that segments a document's text into the minimum\nnumber of tokens for a given vocabulary. Through extensive experimentation we\nfind this hypothesis not to be the case, casting doubt on the understanding of\nthe reasons for effective tokenization. To examine which other factors play a\nrole, we evaluate design decisions across all three phases of tokenization:\npre-tokenization, vocabulary construction, and segmentation, offering new\ninsights into the design of effective tokenizers. Specifically, we illustrate\nthe importance of pre-tokenization and the benefits of using BPE to initialize\nvocabulary construction. We train 64 language models with varying tokenization,\nranging in size from 350M to 2.4B parameters, all of which are made publicly\navailable.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.18376v2",
    "published_date": "2024-02-28 14:52:15 UTC",
    "updated_date": "2024-10-07 13:17:03 UTC"
  },
  {
    "arxiv_id": "2403.00023v1",
    "title": "Auditable Homomorphic-based Decentralized Collaborative AI with Attribute-based Differential Privacy",
    "authors": [
      "Lo-Yao Yeh",
      "Sheng-Po Tseng",
      "Chia-Hsun Lu",
      "Chih-Ya Shen"
    ],
    "abstract": "In recent years, the notion of federated learning (FL) has led to the new\nparadigm of distributed artificial intelligence (AI) with privacy preservation.\nHowever, most current FL systems suffer from data privacy issues due to the\nrequirement of a trusted third party. Although some previous works introduce\ndifferential privacy to protect the data, however, it may also significantly\ndeteriorate the model performance. To address these issues, we propose a novel\ndecentralized collaborative AI framework, named Auditable Homomorphic-based\nDecentralised Collaborative AI (AerisAI), to improve security with homomorphic\nencryption and fine-grained differential privacy. Our proposed AerisAI directly\naggregates the encrypted parameters with a blockchain-based smart contract to\nget rid of the need of a trusted third party. We also propose a brand-new\nconcept for eliminating the negative impacts of differential privacy for model\nperformance. Moreover, the proposed AerisAI also provides the broadcast-aware\ngroup key management based on ciphertext-policy attribute-based encryption\n(CPABE) to achieve fine-grained access control based on different service-level\nagreements. We provide a formal theoretical analysis of the proposed AerisAI as\nwell as the functionality comparison with the other baselines. We also conduct\nextensive experiments on real datasets to evaluate the proposed approach. The\nexperimental results indicate that our proposed AerisAI significantly\noutperforms the other state-of-the-art baselines.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "68T01"
    ],
    "primary_category": "cs.CR",
    "comment": "12 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.00023v1",
    "published_date": "2024-02-28 14:51:18 UTC",
    "updated_date": "2024-02-28 14:51:18 UTC"
  },
  {
    "arxiv_id": "2402.18362v1",
    "title": "Objective and Interpretable Breast Cosmesis Evaluation with Attention Guided Denoising Diffusion Anomaly Detection Model",
    "authors": [
      "Sangjoon Park",
      "Yong Bae Kim",
      "Jee Suk Chang",
      "Seo Hee Choi",
      "Hyungjin Chung",
      "Ik Jae Lee",
      "Hwa Kyung Byun"
    ],
    "abstract": "As advancements in the field of breast cancer treatment continue to progress,\nthe assessment of post-surgical cosmetic outcomes has gained increasing\nsignificance due to its substantial impact on patients' quality of life.\nHowever, evaluating breast cosmesis presents challenges due to the inherently\nsubjective nature of expert labeling. In this study, we present a novel\nautomated approach, Attention-Guided Denoising Diffusion Anomaly Detection\n(AG-DDAD), designed to assess breast cosmesis following surgery, addressing the\nlimitations of conventional supervised learning and existing anomaly detection\nmodels. Our approach leverages the attention mechanism of the distillation with\nno label (DINO) self-supervised Vision Transformer (ViT) in combination with a\ndiffusion model to achieve high-quality image reconstruction and precise\ntransformation of discriminative regions. By training the diffusion model on\nunlabeled data predominantly with normal cosmesis, we adopt an unsupervised\nanomaly detection perspective to automatically score the cosmesis. Real-world\ndata experiments demonstrate the effectiveness of our method, providing\nvisually appealing representations and quantifiable scores for cosmesis\nevaluation. Compared to commonly used rule-based programs, our fully automated\napproach eliminates the need for manual annotations and offers objective\nevaluation. Moreover, our anomaly detection model exhibits state-of-the-art\nperformance, surpassing existing models in accuracy. Going beyond the scope of\nbreast cosmesis, our research represents a significant advancement in\nunsupervised anomaly detection within the medical domain, thereby paving the\nway for future investigations.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18362v1",
    "published_date": "2024-02-28 14:33:14 UTC",
    "updated_date": "2024-02-28 14:33:14 UTC"
  },
  {
    "arxiv_id": "2402.18360v1",
    "title": "Similarity-based analogical proportions",
    "authors": [
      "Christian Antić"
    ],
    "abstract": "The author has recently introduced abstract algebraic frameworks of\nanalogical proportions and similarity within the general setting of universal\nalgebra. The purpose of this paper is to build a bridge from similarity to\nanalogical proportions by formulating the latter in terms of the former. The\nbenefit of this similarity-based approach is that the connection between\nproportions and similarity is built into the framework and therefore evident\nwhich is appealing since proportions and similarity are both at the center of\nanalogy; moreover, future results on similarity can directly be applied to\nanalogical proportions.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "math.LO"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18360v1",
    "published_date": "2024-02-28 14:31:34 UTC",
    "updated_date": "2024-02-28 14:31:34 UTC"
  },
  {
    "arxiv_id": "2402.18344v2",
    "title": "Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning",
    "authors": [
      "Jiachun Li",
      "Pengfei Cao",
      "Chenhao Wang",
      "Zhuoran Jin",
      "Yubo Chen",
      "Daojian Zeng",
      "Kang Liu",
      "Jun Zhao"
    ],
    "abstract": "Large language models exhibit high-level commonsense reasoning abilities,\nespecially with enhancement methods like Chain-of-Thought (CoT). However, we\nfind these CoT-like methods lead to a considerable number of originally correct\nanswers turning wrong, which we define as the Toxic CoT problem. To interpret\nand mitigate this problem, we first utilize attribution tracing and causal\ntracing methods to probe the internal working mechanism of the LLM during CoT\nreasoning. Through comparisons, we prove that the model exhibits information\nloss from the question over the shallow attention layers when generating\nrationales or answers. Based on the probing findings, we design a novel method\ncalled RIDERS (Residual decodIng and sERial-position Swap), which compensates\nfor the information deficit in the model from both decoding and serial-position\nperspectives. Through extensive experiments on multiple commonsense reasoning\nbenchmarks, we validate that this method not only significantly eliminates\nToxic CoT problems (decreased by 23.6%), but also effectively improves the\nmodel's overall commonsense reasoning performance (increased by 5.5%).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted as a long paper to ACL 2024 Main, 25 pages, 22 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.18344v2",
    "published_date": "2024-02-28 14:09:02 UTC",
    "updated_date": "2024-06-27 06:54:58 UTC"
  },
  {
    "arxiv_id": "2402.18326v2",
    "title": "When Should Algorithms Resign? A Proposal for AI Governance",
    "authors": [
      "Umang Bhatt",
      "Holli Sargeant"
    ],
    "abstract": "Algorithmic resignation is a strategic approach for managing the use of\nartificial intelligence (AI) by embedding governance directly into AI systems.\nIt involves deliberate and informed disengagement from AI, such as restricting\naccess AI outputs or displaying performance disclaimers, in specific scenarios\nto aid the appropriate and effective use of AI. By integrating algorithmic\nresignation as a governance mechanism, organizations can better control when\nand how AI is used, balancing the benefits of automation with the need for\nhuman oversight.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18326v2",
    "published_date": "2024-02-28 13:48:44 UTC",
    "updated_date": "2024-07-16 19:40:37 UTC"
  },
  {
    "arxiv_id": "2402.18320v2",
    "title": "Location-guided Head Pose Estimation for Fisheye Image",
    "authors": [
      "Bing Li",
      "Dong Zhang",
      "Cheng Huang",
      "Yun Xian",
      "Ming Li",
      "Dah-Jye Lee"
    ],
    "abstract": "Camera with a fisheye or ultra-wide lens covers a wide field of view that\ncannot be modeled by the perspective projection. Serious fisheye lens\ndistortion in the peripheral region of the image leads to degraded performance\nof the existing head pose estimation models trained on undistorted images. This\npaper presents a new approach for head pose estimation that uses the knowledge\nof head location in the image to reduce the negative effect of fisheye\ndistortion. We develop an end-to-end convolutional neural network to estimate\nthe head pose with the multi-task learning of head pose and head location. Our\nproposed network estimates the head pose directly from the fisheye image\nwithout the operation of rectification or calibration. We also created a\nfisheye-distorted version of the three popular head pose estimation datasets,\nBIWI, 300W-LP, and AFLW2000 for our experiments. Experiments results show that\nour network remarkably improves the accuracy of head pose estimation compared\nwith other state-of-the-art one-stage and two-stage methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Revised Introduction and Related Work; Submitted to lEEE Transactions\n  on Cognitive and Developmental Systems for review",
    "pdf_url": "http://arxiv.org/pdf/2402.18320v2",
    "published_date": "2024-02-28 13:33:43 UTC",
    "updated_date": "2024-04-10 15:09:22 UTC"
  },
  {
    "arxiv_id": "2402.18309v1",
    "title": "Enhancing Roadway Safety: LiDAR-based Tree Clearance Analysis",
    "authors": [
      "Miriam Louise Carnot",
      "Eric Peukert",
      "Bogdan Franczyk"
    ],
    "abstract": "In the efforts for safer roads, ensuring adequate vertical clearance above\nroadways is of great importance. Frequently, trees or other vegetation is\ngrowing above the roads, blocking the sight of traffic signs and lights and\nposing danger to traffic participants. Accurately estimating this space from\nsimple images proves challenging due to a lack of depth information. This is\nwhere LiDAR technology comes into play, a laser scanning sensor that reveals a\nthree-dimensional perspective. Thus far, LiDAR point clouds at the street level\nhave mainly been used for applications in the field of autonomous driving.\nThese scans, however, also open up possibilities in urban management. In this\npaper, we present a new point cloud algorithm that can automatically detect\nthose parts of the trees that grow over the street and need to be trimmed. Our\nsystem uses semantic segmentation to filter relevant points and downstream\nprocessing steps to create the required volume to be kept clear above the road.\nChallenges include obscured stretches of road, the noisy unstructured nature of\nLiDAR point clouds, and the assessment of the road shape. The identified points\nof non-compliant trees can be projected from the point cloud onto images,\nproviding municipalities with a visual aid for dealing with such occurrences.\nBy automating this process, municipalities can address potential road space\nconstraints, enhancing safety for all. They may also save valuable time by\ncarrying out the inspections more systematically. Our open-source code gives\ncommunities inspiration on how to automate the process themselves.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18309v1",
    "published_date": "2024-02-28 13:08:46 UTC",
    "updated_date": "2024-02-28 13:08:46 UTC"
  },
  {
    "arxiv_id": "2402.18292v6",
    "title": "FSL-Rectifier: Rectify Outliers in Few-Shot Learning via Test-Time Augmentation",
    "authors": [
      "Yunwei Bai",
      "Ying Kiat Tan",
      "Shiming Chen",
      "Yao Shu",
      "Tsuhan Chen"
    ],
    "abstract": "Few-shot learning (FSL) commonly requires a model to identify images\n(queries) that belong to classes unseen during training, based on a few\nlabelled samples of the new classes (support set) as reference. So far, plenty\nof algorithms involve training data augmentation to improve the generalization\ncapability of FSL models, but outlier queries or support images during\ninference can still pose great generalization challenges. In this work, to\nreduce the bias caused by the outlier samples, we generate additional\ntest-class samples by combining original samples with suitable train-class\nsamples via a generative image combiner. Then, we obtain averaged features via\nan augmentor, which leads to more typical representations through the\naveraging. We experimentally and theoretically demonstrate the effectiveness of\nour method, obtaining a test accuracy improvement proportion of around 10\\%\n(e.g., from 46.86\\% to 53.28\\%) for trained FSL models. Importantly, given a\npretrained image combiner, our method is training-free for off-the-shelf FSL\nmodels, whose performance can be improved without extra datasets nor further\ntraining of the models themselves. Codes are available at\nhttps://github.com/WendyBaiYunwei/FSL-Rectifier-Pub.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "To be published in AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2402.18292v6",
    "published_date": "2024-02-28 12:37:30 UTC",
    "updated_date": "2024-12-19 03:03:58 UTC"
  },
  {
    "arxiv_id": "2403.06993v1",
    "title": "Automatic driving lane change safety prediction model based on LSTM",
    "authors": [
      "Wenjian Sun",
      "Linying Pan",
      "Jingyu Xu",
      "Weixiang Wan",
      "Yong Wang"
    ],
    "abstract": "Autonomous driving technology can improve traffic safety and reduce traffic\naccidents. In addition, it improves traffic flow, reduces congestion, saves\nenergy and increases travel efficiency. In the relatively mature automatic\ndriving technology, the automatic driving function is divided into several\nmodules: perception, decision-making, planning and control, and a reasonable\ndivision of labor can improve the stability of the system. Therefore,\nautonomous vehicles need to have the ability to predict the trajectory of\nsurrounding vehicles in order to make reasonable decision planning and safety\nmeasures to improve driving safety. By using deep learning method, a\nsafety-sensitive deep learning model based on short term memory (LSTM) network\nis proposed. This model can alleviate the shortcomings of current automatic\ndriving trajectory planning, and the output trajectory not only ensures high\naccuracy but also improves safety. The cell state simulation algorithm\nsimulates the trackability of the trajectory generated by this model. The\nresearch results show that compared with the traditional model-based method,\nthe trajectory prediction method based on LSTM network has obvious advantages\nin predicting the trajectory in the long time domain. The intention recognition\nmodule considering interactive information has higher prediction and accuracy,\nand the algorithm results show that the trajectory is very smooth based on the\npremise of safe prediction and efficient lane change. And autonomous vehicles\ncan efficiently and safely complete lane changes.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.IV",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.06993v1",
    "published_date": "2024-02-28 12:34:04 UTC",
    "updated_date": "2024-02-28 12:34:04 UTC"
  },
  {
    "arxiv_id": "2402.18286v2",
    "title": "Self-Supervised Learning with Generative Adversarial Networks for Electron Microscopy",
    "authors": [
      "Bashir Kazimi",
      "Karina Ruzaeva",
      "Stefan Sandfeld"
    ],
    "abstract": "In this work, we explore the potential of self-supervised learning with\nGenerative Adversarial Networks (GANs) for electron microscopy datasets. We\nshow how self-supervised pretraining facilitates efficient fine-tuning for a\nspectrum of downstream tasks, including semantic segmentation, denoising, noise\n\\& background removal, and super-resolution. Experimentation with varying model\ncomplexities and receptive field sizes reveals the remarkable phenomenon that\nfine-tuned models of lower complexity consistently outperform more complex\nmodels with random weight initialization. We demonstrate the versatility of\nself-supervised pretraining across various downstream tasks in the context of\nelectron microscopy, allowing faster convergence and better performance. We\nconclude that self-supervised pretraining serves as a powerful catalyst, being\nespecially advantageous when limited annotated data are available and efficient\nscaling of computational cost is important.",
    "categories": [
      "cs.CV",
      "cond-mat.mtrl-sci",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18286v2",
    "published_date": "2024-02-28 12:25:01 UTC",
    "updated_date": "2024-07-18 09:58:03 UTC"
  },
  {
    "arxiv_id": "2402.18285v2",
    "title": "PiShield: A PyTorch Package for Learning with Requirements",
    "authors": [
      "Mihaela Cătălina Stoian",
      "Alex Tatomir",
      "Thomas Lukasiewicz",
      "Eleonora Giunchiglia"
    ],
    "abstract": "Deep learning models have shown their strengths in various application\ndomains, however, they often struggle to meet safety requirements for their\noutputs. In this paper, we introduce PiShield, the first package ever allowing\nfor the integration of the requirements into the neural networks' topology.\nPiShield guarantees compliance with these requirements, regardless of input.\nAdditionally, it allows for integrating requirements both at inference and/or\ntraining time, depending on the practitioners' needs. Given the widespread\napplication of deep learning, there is a growing need for frameworks allowing\nfor the integration of the requirements across various domains. Here, we\nexplore three application scenarios: functional genomics, autonomous driving,\nand tabular data generation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.LG",
    "comment": "Demo paper, accepted at IJCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.18285v2",
    "published_date": "2024-02-28 12:24:27 UTC",
    "updated_date": "2024-05-14 17:23:13 UTC"
  },
  {
    "arxiv_id": "2402.18284v2",
    "title": "Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of Pre-trained Language Models with Proximal Policy Optimization",
    "authors": [
      "Shuo Yang",
      "Gjergji Kasneci"
    ],
    "abstract": "Wide usage of ChatGPT has highlighted the potential of reinforcement learning\nfrom human feedback. However, its training pipeline relies on manual ranking, a\nresource-intensive process. To reduce labor costs, we propose a self-supervised\ntext ranking approach for applying Proximal-Policy-Optimization to fine-tune\nlanguage models while eliminating the need for human annotators. Our method\nbegins with probabilistic sampling to encourage a language model to generate\ndiverse responses for each input. We then employ TextRank and ISODATA\nalgorithms to rank and cluster these responses based on their semantics.\nSubsequently, we construct a reward model to learn the rank and optimize our\ngenerative policy. Our experimental results, conducted using two language\nmodels on three tasks, demonstrate that the models trained by our method\nconsiderably outperform baselines regarding BLEU, GLEU, and METEOR scores.\nFurthermore, our manual evaluation shows that our ranking results exhibit a\nremarkably high consistency with that of humans. This research significantly\nreduces training costs of proximal policy-guided models and demonstrates the\npotential for self-correction of language models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.18284v2",
    "published_date": "2024-02-28 12:24:07 UTC",
    "updated_date": "2024-03-02 23:19:27 UTC"
  },
  {
    "arxiv_id": "2402.18607v3",
    "title": "Exploring Privacy and Fairness Risks in Sharing Diffusion Models: An Adversarial Perspective",
    "authors": [
      "Xinjian Luo",
      "Yangfan Jiang",
      "Fei Wei",
      "Yuncheng Wu",
      "Xiaokui Xiao",
      "Beng Chin Ooi"
    ],
    "abstract": "Diffusion models have recently gained significant attention in both academia\nand industry due to their impressive generative performance in terms of both\nsampling quality and distribution coverage. Accordingly, proposals are made for\nsharing pre-trained diffusion models across different organizations, as a way\nof improving data utilization while enhancing privacy protection by avoiding\nsharing private data directly. However, the potential risks associated with\nsuch an approach have not been comprehensively examined.\n  In this paper, we take an adversarial perspective to investigate the\npotential privacy and fairness risks associated with the sharing of diffusion\nmodels. Specifically, we investigate the circumstances in which one party (the\nsharer) trains a diffusion model using private data and provides another party\n(the receiver) black-box access to the pre-trained model for downstream tasks.\nWe demonstrate that the sharer can execute fairness poisoning attacks to\nundermine the receiver's downstream models by manipulating the training data\ndistribution of the diffusion model. Meanwhile, the receiver can perform\nproperty inference attacks to reveal the distribution of sensitive features in\nthe sharer's dataset. Our experiments conducted on real-world datasets\ndemonstrate remarkable attack performance on different types of diffusion\nmodels, which highlights the critical importance of robust data auditing and\nprivacy protection protocols in pertinent applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18607v3",
    "published_date": "2024-02-28 12:21:12 UTC",
    "updated_date": "2024-09-19 08:00:56 UTC"
  },
  {
    "arxiv_id": "2403.00832v1",
    "title": "Explainable Session-based Recommendation via Path Reasoning",
    "authors": [
      "Yang Cao",
      "Shuo Shang",
      "Jun Wang",
      "Wei Zhang"
    ],
    "abstract": "This paper explores providing explainability for session-based recommendation\n(SR) by path reasoning. Current SR models emphasize accuracy but lack\nexplainability, while traditional path reasoning prioritizes knowledge graph\nexploration, ignoring sequential patterns present in the session history.\nTherefore, we propose a generalized hierarchical reinforcement learning\nframework for SR, which improves the explainability of existing SR models via\nPath Reasoning, namely PR4SR. Considering the different importance of items to\nthe session, we design the session-level agent to select the items in the\nsession as the starting point for path reasoning and the path-level agent to\nperform path reasoning. In particular, we design a multi-target reward\nmechanism to adapt to the skip behaviors of sequential patterns in SR, and\nintroduce path midpoint reward to enhance the exploration efficiency in\nknowledge graphs. To improve the completeness of the knowledge graph and to\ndiversify the paths of explanation, we incorporate extracted feature\ninformation from images into the knowledge graph. We instantiate PR4SR in five\nstate-of-the-art SR models (i.e., GRU4REC, NARM, GCSAN, SR-GNN, SASRec) and\ncompare it with other explainable SR frameworks, to demonstrate the\neffectiveness of PR4SR for recommendation and explanation tasks through\nextensive experiments with these approaches on four datasets.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.00832v1",
    "published_date": "2024-02-28 12:11:08 UTC",
    "updated_date": "2024-02-28 12:11:08 UTC"
  },
  {
    "arxiv_id": "2402.18272v1",
    "title": "Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?",
    "authors": [
      "Qineng Wang",
      "Zihao Wang",
      "Ying Su",
      "Hanghang Tong",
      "Yangqiu Song"
    ],
    "abstract": "Recent progress in LLMs discussion suggests that multi-agent discussion\nimproves the reasoning abilities of LLMs. In this work, we reevaluate this\nclaim through systematic experiments, where we propose a novel group discussion\nframework to enrich the set of discussion mechanisms. Interestingly, our\nresults show that a single-agent LLM with strong prompts can achieve almost the\nsame performance as the best existing discussion approach on a wide range of\nreasoning tasks and backbone LLMs. We observe that the multi-agent discussion\nperforms better than a single agent only when there is no demonstration in the\nprompt. Further study reveals the common interaction mechanisms of LLMs during\nthe discussion.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages, 5 figures, 10 tables",
    "pdf_url": "http://arxiv.org/pdf/2402.18272v1",
    "published_date": "2024-02-28 12:04:05 UTC",
    "updated_date": "2024-02-28 12:04:05 UTC"
  },
  {
    "arxiv_id": "2403.07923v1",
    "title": "The Fusion of Deep Reinforcement Learning and Edge Computing for Real-time Monitoring and Control Optimization in IoT Environments",
    "authors": [
      "Jingyu Xu",
      "Weixiang Wan",
      "Linying Pan",
      "Wenjian Sun",
      "Yuxiang Liu"
    ],
    "abstract": "In response to the demand for real-time performance and control quality in\nindustrial Internet of Things (IoT) environments, this paper proposes an\noptimization control system based on deep reinforcement learning and edge\ncomputing. The system leverages cloud-edge collaboration, deploys lightweight\npolicy networks at the edge, predicts system states, and outputs controls at a\nhigh frequency, enabling monitoring and optimization of industrial objectives.\nAdditionally, a dynamic resource allocation mechanism is designed to ensure\nrational scheduling of edge computing resources, achieving global optimization.\nResults demonstrate that this approach reduces cloud-edge communication\nlatency, accelerates response to abnormal situations, reduces system failure\nrates, extends average equipment operating time, and saves costs for manual\nmaintenance and replacement. This ensures real-time and stable control.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.IV",
      "eess.SY"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.07923v1",
    "published_date": "2024-02-28 12:01:06 UTC",
    "updated_date": "2024-02-28 12:01:06 UTC"
  },
  {
    "arxiv_id": "2402.18267v2",
    "title": "A Survey on Neural Question Generation: Methods, Applications, and Prospects",
    "authors": [
      "Shasha Guo",
      "Lizi Liao",
      "Cuiping Li",
      "Tat-Seng Chua"
    ],
    "abstract": "In this survey, we present a detailed examination of the advancements in\nNeural Question Generation (NQG), a field leveraging neural network techniques\nto generate relevant questions from diverse inputs like knowledge bases, texts,\nand images. The survey begins with an overview of NQG's background,\nencompassing the task's problem formulation, prevalent benchmark datasets,\nestablished evaluation metrics, and notable applications. It then methodically\nclassifies NQG approaches into three predominant categories: structured NQG,\nwhich utilizes organized data sources, unstructured NQG, focusing on more\nloosely structured inputs like texts or visual content, and hybrid NQG, drawing\non diverse input modalities. This classification is followed by an in-depth\nanalysis of the distinct neural network models tailored for each category,\ndiscussing their inherent strengths and potential limitations. The survey\nculminates with a forward-looking perspective on the trajectory of NQG,\nidentifying emergent research trends and prospective developmental paths.\nAccompanying this survey is a curated collection of related research papers,\ndatasets and codes, systematically organized on Github, providing an extensive\nreference for those delving into NQG.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by IJCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.18267v2",
    "published_date": "2024-02-28 11:57:12 UTC",
    "updated_date": "2024-05-07 15:08:56 UTC"
  },
  {
    "arxiv_id": "2402.18252v1",
    "title": "Towards Generalist Prompting for Large Language Models by Mental Models",
    "authors": [
      "Haoxiang Guan",
      "Jiyan He",
      "Shuxin Zheng",
      "En-Hong Chen",
      "Weiming Zhang",
      "Nenghai Yu"
    ],
    "abstract": "Large language models (LLMs) have demonstrated impressive performance on many\ntasks. However, to achieve optimal performance, specially designed prompting\nmethods are still needed. These methods either rely on task-specific few-shot\nexamples that require a certain level of domain knowledge, or are designed to\nbe simple but only perform well on a few types of tasks. In this work, we\nattempt to introduce the concept of generalist prompting, which operates on the\ndesign principle of achieving optimal or near-optimal performance on a wide\nrange of tasks while eliminating the need for manual selection and\ncustomization of prompts tailored to specific problems. Furthermore, we propose\nMeMo (Mental Models), an innovative prompting method that is simple-designed\nyet effectively fulfills the criteria of generalist prompting. MeMo distills\nthe cores of various prompting methods into individual mental models and allows\nLLMs to autonomously select the most suitable mental models for the problem,\nachieving or being near to the state-of-the-art results on diverse tasks such\nas STEM, logical reasoning, and commonsense reasoning in zero-shot settings. We\nhope that the insights presented herein will stimulate further exploration of\ngeneralist prompting methods for LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18252v1",
    "published_date": "2024-02-28 11:29:09 UTC",
    "updated_date": "2024-02-28 11:29:09 UTC"
  },
  {
    "arxiv_id": "2402.18606v1",
    "title": "Impact of network topology on the performance of Decentralized Federated Learning",
    "authors": [
      "Luigi Palmieri",
      "Chiara Boldrini",
      "Lorenzo Valerio",
      "Andrea Passarella",
      "Marco Conti"
    ],
    "abstract": "Fully decentralized learning is gaining momentum for training AI models at\nthe Internet's edge, addressing infrastructure challenges and privacy concerns.\nIn a decentralized machine learning system, data is distributed across multiple\nnodes, with each node training a local model based on its respective dataset.\nThe local models are then shared and combined to form a global model capable of\nmaking accurate predictions on new data. Our exploration focuses on how\ndifferent types of network structures influence the spreading of knowledge -\nthe process by which nodes incorporate insights gained from learning patterns\nin data available on other nodes across the network. Specifically, this study\ninvestigates the intricate interplay between network structure and learning\nperformance using three network topologies and six data distribution methods.\nThese methods consider different vertex properties, including degree\ncentrality, betweenness centrality, and clustering coefficient, along with\nwhether nodes exhibit high or low values of these metrics. Our findings\nunderscore the significance of global centrality metrics (degree, betweenness)\nin correlating with learning performance, while local clustering proves less\npredictive. We highlight the challenges in transferring knowledge from\nperipheral to central nodes, attributed to a dilution effect during model\naggregation. Additionally, we observe that central nodes exert a pull effect,\nfacilitating the spread of knowledge. In examining degree distribution, hubs in\nBarabasi-Albert networks positively impact learning for central nodes but\nexacerbate dilution when knowledge originates from peripheral nodes. Finally,\nwe demonstrate the formidable challenge of knowledge circulation outside of\nsegregated communities.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "Funding: H2020 HumaneAI Net (Grant N. 952026), CHIST-ERA SAI\n  (CHIST-ERA-19-XAI010), PNRR FAIR (PE00000013), PNRR RESTART (PE00000001).\n  arXiv admin note: text overlap with arXiv:2307.15947",
    "pdf_url": "http://arxiv.org/pdf/2402.18606v1",
    "published_date": "2024-02-28 11:13:53 UTC",
    "updated_date": "2024-02-28 11:13:53 UTC"
  },
  {
    "arxiv_id": "2402.18225v1",
    "title": "CogBench: a large language model walks into a psychology lab",
    "authors": [
      "Julian Coda-Forno",
      "Marcel Binz",
      "Jane X. Wang",
      "Eric Schulz"
    ],
    "abstract": "Large language models (LLMs) have significantly advanced the field of\nartificial intelligence. Yet, evaluating them comprehensively remains\nchallenging. We argue that this is partly due to the predominant focus on\nperformance metrics in most benchmarks. This paper introduces CogBench, a\nbenchmark that includes ten behavioral metrics derived from seven cognitive\npsychology experiments. This novel approach offers a toolkit for phenotyping\nLLMs' behavior. We apply CogBench to 35 LLMs, yielding a rich and diverse\ndataset. We analyze this data using statistical multilevel modeling techniques,\naccounting for the nested dependencies among fine-tuned versions of specific\nLLMs. Our study highlights the crucial role of model size and reinforcement\nlearning from human feedback (RLHF) in improving performance and aligning with\nhuman behavior. Interestingly, we find that open-source models are less\nrisk-prone than proprietary models and that fine-tuning on code does not\nnecessarily enhance LLMs' behavior. Finally, we explore the effects of\nprompt-engineering techniques. We discover that chain-of-thought prompting\nimproves probabilistic reasoning, while take-a-step-back prompting fosters\nmodel-based behaviors.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18225v1",
    "published_date": "2024-02-28 10:43:54 UTC",
    "updated_date": "2024-02-28 10:43:54 UTC"
  },
  {
    "arxiv_id": "2402.18222v2",
    "title": "HearHere: Mitigating Echo Chambers in News Consumption through an AI-based Web System",
    "authors": [
      "Youngseung Jeon",
      "Jaehoon Kim",
      "Sohyun Park",
      "Yunyong Ko",
      "Seongeun Ryu",
      "Sang-Wook Kim",
      "Kyungsik Han"
    ],
    "abstract": "Considerable efforts are currently underway to mitigate the negative impacts\nof echo chambers, such as increased susceptibility to fake news and resistance\ntowards accepting scientific evidence. Prior research has presented the\ndevelopment of computer systems that support the consumption of news\ninformation from diverse political perspectives to mitigate the echo chamber\neffect. However, existing studies still lack the ability to effectively support\nthe key processes of news information consumption and quantitatively identify a\npolitical stance towards the information. In this paper, we present HearHere,\nan AI-based web system designed to help users accommodate information and\nopinions from diverse perspectives. HearHere facilitates the key processes of\nnews information consumption through two visualizations. Visualization 1\nprovides political news with quantitative political stance information, derived\nfrom our graph-based political classification model, and users can experience\ndiverse perspectives (Hear). Visualization 2 allows users to express their\nopinions on specific political issues in a comment form and observe the\nposition of their own opinions relative to pro-liberal and pro-conservative\ncomments presented on a map interface (Here). Through a user study with 94\nparticipants, we demonstrate the feasibility of HearHere in supporting the\nconsumption of information from various perspectives. Our findings highlight\nthe importance of providing political stance information and quantifying users'\npolitical status as a means to mitigate political polarization. In addition, we\npropose design implications for system development, including the consideration\nof demographics such as political interest and providing users with\ninitiatives.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "34 pages, 6 figures, 6 tables, CSCW 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.18222v2",
    "published_date": "2024-02-28 10:37:14 UTC",
    "updated_date": "2024-02-29 05:11:05 UTC"
  },
  {
    "arxiv_id": "2402.18205v5",
    "title": "Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging",
    "authors": [
      "Wei Zhang",
      "Xiangyuan Guan",
      "Lu Yunhong",
      "Jie Zhang",
      "Shuangyong Song",
      "Xianfu Cheng",
      "Zhenhe Wu",
      "Zhoujun Li"
    ],
    "abstract": "Logs produced by extensive software systems are integral to monitoring system\nbehaviors. Advanced log analysis facilitates the detection, alerting, and\ndiagnosis of system faults. Log parsing, which entails transforming raw log\nmessages into structured templates, constitutes a critical phase in the\nautomation of log analytics. Existing log parsers fail to identify the correct\ntemplates due to reliance on human-made rules. Besides, these methods focus on\nstatistical features while ignoring semantic information in log messages. To\naddress these challenges, we introduce a cutting-edge \\textbf{L}og parsing\nframework with \\textbf{E}ntropy sampling and chain-of-thought \\textbf{M}erging\n(\\model{}). Specifically, to discard the tedious manual rules, we propose a\nnovel sampling method inspired by information entropy, which efficiently\nclusters typical logs. Furthermore, to enhance the merging of log templates, we\ndesign a chain-of-thought method for large language models (LLMs). LLMs exhibit\nexceptional semantic comprehension and deftly distinguish between parameters\nand invariant tokens. We have conducted experiments on large-scale public\ndatasets. Extensive evaluation demonstrates that \\model{} achieves\nstate-of-the-art performance and impressive efficiency. The Code is available\nat https://github.com/zwpride/lemur.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18205v5",
    "published_date": "2024-02-28 09:51:55 UTC",
    "updated_date": "2025-03-26 08:55:05 UTC"
  },
  {
    "arxiv_id": "2402.18164v2",
    "title": "Autoencoder-based General Purpose Representation Learning for Customer Embedding",
    "authors": [
      "Jan Henrik Bertrand",
      "David B. Hoffmann",
      "Jacopo Pio Gargano",
      "Laurent Mombaerts",
      "Jonathan Taws"
    ],
    "abstract": "Recent advances in representation learning have successfully leveraged the\nunderlying domain-specific structure of data across various fields. However,\nrepresenting diverse and complex entities stored in tabular format within a\nlatent space remains challenging. In this paper, we introduce DEEPCAE, a novel\nmethod for calculating the regularization term for multi-layer contractive\nautoencoders (CAEs). Additionally, we formalize a general-purpose entity\nembedding framework and use it to empirically show that DEEPCAE outperforms all\nother tested autoencoder variants in both reconstruction performance and\ndownstream prediction performance. Notably, when compared to a stacked CAE\nacross 13 datasets, DEEPCAE achieves a 34% improvement in reconstruction error.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T-02"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.18164v2",
    "published_date": "2024-02-28 08:53:20 UTC",
    "updated_date": "2025-02-04 13:17:52 UTC"
  },
  {
    "arxiv_id": "2402.18158v2",
    "title": "Evaluating Quantized Large Language Models",
    "authors": [
      "Shiyao Li",
      "Xuefei Ning",
      "Luning Wang",
      "Tengxuan Liu",
      "Xiangsheng Shi",
      "Shengen Yan",
      "Guohao Dai",
      "Huazhong Yang",
      "Yu Wang"
    ],
    "abstract": "Post-training quantization (PTQ) has emerged as a promising technique to\nreduce the cost of large language models (LLMs). Specifically, PTQ can\neffectively mitigate memory consumption and reduce computational overhead in\nLLMs. To meet the requirements of both high efficiency and performance across\ndiverse scenarios, a comprehensive evaluation of quantized LLMs is essential to\nguide the selection of quantization methods. This paper presents a thorough\nevaluation of these factors by evaluating the effect of PTQ on Weight,\nActivation, and KV Cache on 11 model families, including OPT, LLaMA2, Falcon,\nBloomz, Mistral, ChatGLM, Vicuna, LongChat, StableLM, Gemma, and Mamba, with\nparameters ranging from 125M to 180B. The evaluation encompasses five types of\ntasks: basic NLP, emergent ability, trustworthiness, dialogue, and long-context\ntasks. Moreover, we also evaluate the state-of-the-art (SOTA) quantization\nmethods to demonstrate their applicability. Based on the extensive experiments,\nwe systematically summarize the effect of quantization, provide recommendations\nto apply quantization techniques, and point out future directions. The code can\nbe found in https://github.com/thu-nics/qllm-eval.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18158v2",
    "published_date": "2024-02-28 08:43:05 UTC",
    "updated_date": "2024-06-06 07:30:44 UTC"
  },
  {
    "arxiv_id": "2402.18157v1",
    "title": "From Summary to Action: Enhancing Large Language Models for Complex Tasks with Open World APIs",
    "authors": [
      "Yulong Liu",
      "Yunlong Yuan",
      "Chunwei Wang",
      "Jianhua Han",
      "Yongqiang Ma",
      "Li Zhang",
      "Nanning Zheng",
      "Hang Xu"
    ],
    "abstract": "The distinction between humans and animals lies in the unique ability of\nhumans to use and create tools. Tools empower humans to overcome physiological\nlimitations, fostering the creation of magnificent civilizations. Similarly,\nenabling foundational models like Large Language Models (LLMs) with the\ncapacity to learn external tool usage may serve as a pivotal step toward\nrealizing artificial general intelligence. Previous studies in this field have\npredominantly pursued two distinct approaches to augment the tool invocation\ncapabilities of LLMs. The first approach emphasizes the construction of\nrelevant datasets for model fine-tuning. The second approach, in contrast, aims\nto fully exploit the inherent reasoning abilities of LLMs through in-context\nlearning strategies. In this work, we introduce a novel tool invocation\npipeline designed to control massive real-world APIs. This pipeline mirrors the\nhuman task-solving process, addressing complicated real-life user queries. At\neach step, we guide LLMs to summarize the achieved results and determine the\nnext course of action. We term this pipeline `from Summary to action', Sum2Act\nfor short. Empirical evaluations of our Sum2Act pipeline on the ToolBench\nbenchmark show significant performance improvements, outperforming established\nmethods like ReAct and DFSDT. This highlights Sum2Act's effectiveness in\nenhancing LLMs for complex real-world tasks.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18157v1",
    "published_date": "2024-02-28 08:42:23 UTC",
    "updated_date": "2024-02-28 08:42:23 UTC"
  },
  {
    "arxiv_id": "2402.18154v1",
    "title": "Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and Mitigating Knowledge Conflicts in Language Models",
    "authors": [
      "Zhuoran Jin",
      "Pengfei Cao",
      "Hongbang Yuan",
      "Yubo Chen",
      "Jiexin Xu",
      "Huaijun Li",
      "Xiaojian Jiang",
      "Kang Liu",
      "Jun Zhao"
    ],
    "abstract": "Recently, retrieval augmentation and tool augmentation have demonstrated a\nremarkable capability to expand the internal memory boundaries of language\nmodels (LMs) by providing external context. However, internal memory and\nexternal context inevitably clash, leading to knowledge conflicts within LMs.\nIn this paper, we aim to interpret the mechanism of knowledge conflicts through\nthe lens of information flow, and then mitigate conflicts by precise\ninterventions at the pivotal point. We find there are some attention heads with\nopposite effects in the later layers, where memory heads can recall knowledge\nfrom internal memory, and context heads can retrieve knowledge from external\ncontext. Moreover, we reveal that the pivotal point at which knowledge\nconflicts emerge in LMs is the integration of inconsistent information flows by\nmemory heads and context heads. Inspired by the insights, we propose a novel\nmethod called Pruning Head via PatH PatcHing (PH3), which can efficiently\nmitigate knowledge conflicts by pruning conflicting attention heads without\nupdating model parameters. PH3 can flexibly control eight LMs to use internal\nmemory ($\\uparrow$ 44.0%) or external context ($\\uparrow$ 38.5%). Moreover, PH3\ncan also improve the performance of LMs on open-domain QA tasks. We also\nconduct extensive experiments to demonstrate the cross-model, cross-relation,\nand cross-format generalization of our method.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "21 pages, 42 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2402.18154v1",
    "published_date": "2024-02-28 08:34:41 UTC",
    "updated_date": "2024-02-28 08:34:41 UTC"
  },
  {
    "arxiv_id": "2402.18153v2",
    "title": "Diffusion-Based Neural Network Weights Generation",
    "authors": [
      "Bedionita Soro",
      "Bruno Andreis",
      "Hayeon Lee",
      "Wonyong Jeong",
      "Song Chong",
      "Frank Hutter",
      "Sung Ju Hwang"
    ],
    "abstract": "Transfer learning has gained significant attention in recent deep learning\nresearch due to its ability to accelerate convergence and enhance performance\non new tasks. However, its success is often contingent on the similarity\nbetween source and target data, and training on numerous datasets can be\ncostly, leading to blind selection of pretrained models with limited insight\ninto their effectiveness. To address these challenges, we introduce D2NWG, a\ndiffusion-based neural network weights generation technique that efficiently\nproduces high-performing weights for transfer learning, conditioned on the\ntarget dataset. Our method extends generative hyper-representation learning to\nrecast the latent diffusion paradigm for neural network weights generation,\nlearning the weight distributions of models pretrained on various datasets.\nThis allows for automatic generation of weights that generalize well across\nboth seen and unseen tasks, outperforming state-of-the-art meta-learning\nmethods and pretrained models. Moreover, our approach is scalable to large\narchitectures such as large language models (LLMs), overcoming the limitations\nof current parameter generation techniques that rely on task-specific model\ncollections or access to original training data. By modeling the parameter\ndistribution of LLMs, D2NWG enables task-specific parameter generation without\nrequiring additional fine-tuning or large collections of model variants.\nExtensive experiments show that our method consistently enhances the\nperformance of diverse base models, regardless of their size or complexity,\npositioning it as a robust solution for scalable transfer learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "32 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.18153v2",
    "published_date": "2024-02-28 08:34:23 UTC",
    "updated_date": "2024-10-25 08:47:06 UTC"
  },
  {
    "arxiv_id": "2402.18152v3",
    "title": "Boosting Neural Representations for Videos with a Conditional Decoder",
    "authors": [
      "Xinjie Zhang",
      "Ren Yang",
      "Dailan He",
      "Xingtong Ge",
      "Tongda Xu",
      "Yan Wang",
      "Hongwei Qin",
      "Jun Zhang"
    ],
    "abstract": "Implicit neural representations (INRs) have emerged as a promising approach\nfor video storage and processing, showing remarkable versatility across various\nvideo tasks. However, existing methods often fail to fully leverage their\nrepresentation capabilities, primarily due to inadequate alignment of\nintermediate features during target frame decoding. This paper introduces a\nuniversal boosting framework for current implicit video representation\napproaches. Specifically, we utilize a conditional decoder with a\ntemporal-aware affine transform module, which uses the frame index as a prior\ncondition to effectively align intermediate features with target frames.\nBesides, we introduce a sinusoidal NeRV-like block to generate diverse\nintermediate features and achieve a more balanced parameter distribution,\nthereby enhancing the model's capacity. With a high-frequency\ninformation-preserving reconstruction loss, our approach successfully boosts\nmultiple baseline INRs in the reconstruction quality and convergence speed for\nvideo regression, and exhibits superior inpainting and interpolation results.\nFurther, we integrate a consistent entropy minimization technique and develop\nvideo codecs based on these boosted INRs. Experiments on the UVG dataset\nconfirm that our enhanced codecs significantly outperform baseline INRs and\noffer competitive rate-distortion performance compared to traditional and\nlearning-based codecs. Code is available at\nhttps://github.com/Xinjie-Q/Boosting-NeRV.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Accept by CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.18152v3",
    "published_date": "2024-02-28 08:32:19 UTC",
    "updated_date": "2024-03-16 13:23:58 UTC"
  },
  {
    "arxiv_id": "2403.00830v1",
    "title": "MedAide: Leveraging Large Language Models for On-Premise Medical Assistance on Edge Devices",
    "authors": [
      "Abdul Basit",
      "Khizar Hussain",
      "Muhammad Abdullah Hanif",
      "Muhammad Shafique"
    ],
    "abstract": "Large language models (LLMs) are revolutionizing various domains with their\nremarkable natural language processing (NLP) abilities. However, deploying LLMs\nin resource-constrained edge computing and embedded systems presents\nsignificant challenges. Another challenge lies in delivering medical assistance\nin remote areas with limited healthcare facilities and infrastructure. To\naddress this, we introduce MedAide, an on-premise healthcare chatbot. It\nleverages tiny-LLMs integrated with LangChain, providing efficient edge-based\npreliminary medical diagnostics and support. MedAide employs model\noptimizations for minimal memory footprint and latency on embedded edge devices\nwithout server infrastructure. The training process is optimized using low-rank\nadaptation (LoRA). Additionally, the model is trained on diverse medical\ndatasets, employing reinforcement learning from human feedback (RLHF) to\nenhance its domain-specific capabilities. The system is implemented on various\nconsumer GPUs and Nvidia Jetson development board. MedAide achieves 77\\%\naccuracy in medical consultations and scores 56 in USMLE benchmark, enabling an\nenergy-efficient healthcare assistance platform that alleviates privacy\nconcerns due to edge-based deployment, thereby empowering the community.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, 11 figures, ACM conference paper, 33 references",
    "pdf_url": "http://arxiv.org/pdf/2403.00830v1",
    "published_date": "2024-02-28 08:30:49 UTC",
    "updated_date": "2024-02-28 08:30:49 UTC"
  },
  {
    "arxiv_id": "2402.18603v5",
    "title": "MMSR: Symbolic Regression is a Multi-Modal Information Fusion Task",
    "authors": [
      "Yanjie Li",
      "Jingyi Liu",
      "Weijun Li",
      "Lina Yu",
      "Min Wu",
      "Wenqiang Li",
      "Meilan Hao",
      "Su Wei",
      "Yusong Deng"
    ],
    "abstract": "Mathematical formulas are the crystallization of human wisdom in exploring\nthe laws of nature for thousands of years. Describing the complex laws of\nnature with a concise mathematical formula is a constant pursuit of scientists\nand a great challenge for artificial intelligence. This field is called\nsymbolic regression (SR). Symbolic regression was originally formulated as a\ncombinatorial optimization problem, and Genetic Programming (GP) and\nReinforcement Learning algorithms were used to solve it. However, GP is\nsensitive to hyperparameters, and these two types of algorithms are\ninefficient. To solve this problem, researchers treat the mapping from data to\nexpressions as a translation problem. And the corresponding large-scale\npre-trained model is introduced. However, the data and expression skeletons do\nnot have very clear word correspondences as the two languages do. Instead, they\nare more like two modalities (e.g., image and text). Therefore, in this paper,\nwe proposed MMSR. The SR problem is solved as a pure multi-modal problem, and\ncontrastive learning is also introduced in the training process for modal\nalignment to facilitate later modal feature fusion. It is worth noting that to\nbetter promote the modal feature fusion, we adopt the strategy of training\ncontrastive learning loss and other losses at the same time, which only needs\none-step training, instead of training contrastive learning loss first and then\ntraining other losses. Because our experiments prove training together can make\nthe feature extraction module and feature fusion module wearing-in better.\nExperimental results show that compared with multiple large-scale pre-training\nbaselines, MMSR achieves the most advanced results on multiple mainstream\ndatasets including SRBench. Our code is open source at\nhttps://github.com/1716757342/MMSR",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "The Information Fusion has accepted this paper",
    "pdf_url": "http://arxiv.org/pdf/2402.18603v5",
    "published_date": "2024-02-28 08:29:42 UTC",
    "updated_date": "2024-09-19 12:30:04 UTC"
  },
  {
    "arxiv_id": "2402.18150v2",
    "title": "Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation",
    "authors": [
      "Shicheng Xu",
      "Liang Pang",
      "Mo Yu",
      "Fandong Meng",
      "Huawei Shen",
      "Xueqi Cheng",
      "Jie Zhou"
    ],
    "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating additional information from retrieval. However, studies have\nshown that LLMs still face challenges in effectively using the retrieved\ninformation, even ignoring it or being misled by it. The key reason is that the\ntraining of LLMs does not clearly make LLMs learn how to utilize input\nretrieved texts with varied quality. In this paper, we propose a novel\nperspective that considers the role of LLMs in RAG as ``Information Refiner'',\nwhich means that regardless of correctness, completeness, or usefulness of\nretrieved texts, LLMs can consistently integrate knowledge within the retrieved\ntexts and model parameters to generate the texts that are more concise,\naccurate, and complete than the retrieved texts. To this end, we propose an\ninformation refinement training method named InFO-RAG that optimizes LLMs for\nRAG in an unsupervised manner. InFO-RAG is low-cost and general across various\ntasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse\ntasks including Question Answering, Slot-Filling, Language Modeling, Dialogue,\nand Code Generation show that InFO-RAG improves the performance of LLaMA2 by an\naverage of 9.39\\% relative points. InFO-RAG also shows advantages in in-context\nlearning and robustness of RAG.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2024 Main",
    "pdf_url": "http://arxiv.org/pdf/2402.18150v2",
    "published_date": "2024-02-28 08:24:38 UTC",
    "updated_date": "2024-06-12 03:21:15 UTC"
  },
  {
    "arxiv_id": "2402.18144v1",
    "title": "Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information",
    "authors": [
      "Seungjong Sun",
      "Eungu Lee",
      "Dongyan Nan",
      "Xiangying Zhao",
      "Wonbyung Lee",
      "Bernard J. Jansen",
      "Jang Hyun Kim"
    ],
    "abstract": "Large language models exhibit societal biases associated with demographic\ninformation, including race, gender, and others. Endowing such language models\nwith personalities based on demographic data can enable generating opinions\nthat align with those of humans. Building on this idea, we propose \"random\nsilicon sampling,\" a method to emulate the opinions of the human population\nsub-group. Our study analyzed 1) a language model that generates the survey\nresponses that correspond with a human group based solely on its demographic\ndistribution and 2) the applicability of our methodology across various\ndemographic subgroups and thematic questions. Through random silicon sampling\nand using only group-level demographic information, we discovered that language\nmodels can generate response distributions that are remarkably similar to the\nactual U.S. public opinion polls. Moreover, we found that the replicability of\nlanguage models varies depending on the demographic group and topic of the\nquestion, and this can be attributed to inherent societal biases in the models.\nOur findings demonstrate the feasibility of mirroring a group's opinion using\nonly demographic distribution and elucidate the effect of social biases in\nlanguage models on such simulations.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "25 pages, 4 figures, 19 Tables",
    "pdf_url": "http://arxiv.org/pdf/2402.18144v1",
    "published_date": "2024-02-28 08:09:14 UTC",
    "updated_date": "2024-02-28 08:09:14 UTC"
  },
  {
    "arxiv_id": "2402.18139v3",
    "title": "Cause and Effect: Can Large Language Models Truly Understand Causality?",
    "authors": [
      "Swagata Ashwani",
      "Kshiteesh Hegde",
      "Nishith Reddy Mannuru",
      "Mayank Jindal",
      "Dushyant Singh Sengar",
      "Krishna Chaitanya Rao Kathala",
      "Dishant Banga",
      "Vinija Jain",
      "Aman Chadha"
    ],
    "abstract": "With the rise of Large Language Models(LLMs), it has become crucial to\nunderstand their capabilities and limitations in deciphering and explaining the\ncomplex web of causal relationships that language entails. Current methods use\neither explicit or implicit causal reasoning, yet there is a strong need for a\nunified approach combining both to tackle a wide array of causal relationships\nmore effectively. This research proposes a novel architecture called Context\nAware Reasoning Enhancement with Counterfactual Analysis(CARE CA) framework to\nenhance causal reasoning and explainability. The proposed framework\nincorporates an explicit causal detection module with ConceptNet and\ncounterfactual statements, as well as implicit causal detection through LLMs.\nOur framework goes one step further with a layer of counterfactual explanations\nto accentuate LLMs understanding of causality. The knowledge from ConceptNet\nenhances the performance of multiple causal reasoning tasks such as causal\ndiscovery, causal identification and counterfactual reasoning. The\ncounterfactual sentences add explicit knowledge of the not caused by scenarios.\nBy combining these powerful modules, our model aims to provide a deeper\nunderstanding of causal relationships, enabling enhanced interpretability.\nEvaluation of benchmark datasets shows improved performance across all metrics,\nsuch as accuracy, precision, recall, and F1 scores. We also introduce\nCausalNet, a new dataset accompanied by our code, to facilitate further\nresearch in this domain.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "AI Trustworthiness and Risk Assessment for Challenged Contexts\n  (ATRACC) AAAI 2024 Fall Symposium",
    "pdf_url": "http://arxiv.org/pdf/2402.18139v3",
    "published_date": "2024-02-28 08:02:14 UTC",
    "updated_date": "2024-09-30 00:40:00 UTC"
  },
  {
    "arxiv_id": "2402.18137v2",
    "title": "DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning",
    "authors": [
      "Jianxiong Li",
      "Jinliang Zheng",
      "Yinan Zheng",
      "Liyuan Mao",
      "Xiao Hu",
      "Sijie Cheng",
      "Haoyi Niu",
      "Jihao Liu",
      "Yu Liu",
      "Jingjing Liu",
      "Ya-Qin Zhang",
      "Xianyuan Zhan"
    ],
    "abstract": "Multimodal pretraining is an effective strategy for the trinity of goals of\nrepresentation learning in autonomous robots: 1) extracting both local and\nglobal task progressions; 2) enforcing temporal consistency of visual\nrepresentation; 3) capturing trajectory-level language grounding. Most existing\nmethods approach these via separate objectives, which often reach sub-optimal\nsolutions. In this paper, we propose a universal unified objective that can\nsimultaneously extract meaningful task progression information from image\nsequences and seamlessly align them with language instructions. We discover\nthat via implicit preferences, where a visual trajectory inherently aligns\nbetter with its corresponding language instruction than mismatched pairs, the\npopular Bradley-Terry model can transform into representation learning through\nproper reward reparameterizations. The resulted framework, DecisionNCE, mirrors\nan InfoNCE-style objective but is distinctively tailored for decision-making\ntasks, providing an embodied representation learning framework that elegantly\nextracts both local and global task progression features, with temporal\nconsistency enforced through implicit time contrastive learning, while ensuring\ntrajectory-level instruction grounding via multimodal joint encoding.\nEvaluation on both simulated and real robots demonstrates that DecisionNCE\neffectively facilitates diverse downstream policy learning tasks, offering a\nversatile solution for unified representation and reward learning. Project\nPage: https://2toinf.github.io/DecisionNCE/",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.18137v2",
    "published_date": "2024-02-28 07:58:24 UTC",
    "updated_date": "2024-05-24 03:31:50 UTC"
  },
  {
    "arxiv_id": "2403.05578v1",
    "title": "Chaining text-to-image and large language model: A novel approach for generating personalized e-commerce banners",
    "authors": [
      "Shanu Vashishtha",
      "Abhinav Prakash",
      "Lalitesh Morishetti",
      "Kaushiki Nag",
      "Yokila Arora",
      "Sushant Kumar",
      "Kannan Achan"
    ],
    "abstract": "Text-to-image models such as stable diffusion have opened a plethora of\nopportunities for generating art. Recent literature has surveyed the use of\ntext-to-image models for enhancing the work of many creative artists. Many\ne-commerce platforms employ a manual process to generate the banners, which is\ntime-consuming and has limitations of scalability. In this work, we demonstrate\nthe use of text-to-image models for generating personalized web banners with\ndynamic content for online shoppers based on their interactions. The novelty in\nthis approach lies in converting users' interaction data to meaningful prompts\nwithout human intervention. To this end, we utilize a large language model\n(LLM) to systematically extract a tuple of attributes from item\nmeta-information. The attributes are then passed to a text-to-image model via\nprompt engineering to generate images for the banner. Our results show that the\nproposed approach can create high-quality personalized banners for users.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2403.05578v1",
    "published_date": "2024-02-28 07:56:04 UTC",
    "updated_date": "2024-02-28 07:56:04 UTC"
  },
  {
    "arxiv_id": "2402.18129v2",
    "title": "On the Inductive Biases of Demographic Parity-based Fair Learning Algorithms",
    "authors": [
      "Haoyu Lei",
      "Amin Gohari",
      "Farzan Farnia"
    ],
    "abstract": "Fair supervised learning algorithms assigning labels with little dependence\non a sensitive attribute have attracted great attention in the machine learning\ncommunity. While the demographic parity (DP) notion has been frequently used to\nmeasure a model's fairness in training fair classifiers, several studies in the\nliterature suggest potential impacts of enforcing DP in fair learning\nalgorithms. In this work, we analytically study the effect of standard DP-based\nregularization methods on the conditional distribution of the predicted label\ngiven the sensitive attribute. Our analysis shows that an imbalanced training\ndataset with a non-uniform distribution of the sensitive attribute could lead\nto a classification rule biased toward the sensitive attribute outcome holding\nthe majority of training data. To control such inductive biases in DP-based\nfair learning, we propose a sensitive attribute-based distributionally robust\noptimization (SA-DRO) method improving robustness against the marginal\ndistribution of the sensitive attribute. Finally, we present several numerical\nresults on the application of DP-based learning methods to standard centralized\nand distributed learning problems. The empirical findings support our\ntheoretical results on the inductive biases in DP-based fair learning\nalgorithms and the debiasing effects of the proposed SA-DRO method.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18129v2",
    "published_date": "2024-02-28 07:39:58 UTC",
    "updated_date": "2024-06-20 09:35:02 UTC"
  },
  {
    "arxiv_id": "2402.18121v1",
    "title": "Saving the legacy of Hero Ibash: Evaluating Four Language Models for Aminoacian",
    "authors": [
      "Yunze Xiao",
      "Yiyang Pan"
    ],
    "abstract": "This study assesses four cutting-edge language models in the underexplored\nAminoacian language. Through evaluation, it scrutinizes their adaptability,\neffectiveness, and limitations in text generation, semantic coherence, and\ncontextual understanding. Uncovering insights into these models' performance in\na low-resourced language, this research pioneers pathways to bridge linguistic\ngaps. By offering benchmarks and understanding challenges, it lays groundwork\nfor future advancements in natural language processing, aiming to elevate the\napplicability of language models in similar linguistic landscapes, marking a\nsignificant step toward inclusivity and progress in language technology.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "5 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.18121v1",
    "published_date": "2024-02-28 07:22:13 UTC",
    "updated_date": "2024-02-28 07:22:13 UTC"
  },
  {
    "arxiv_id": "2402.18113v1",
    "title": "Small But Funny: A Feedback-Driven Approach to Humor Distillation",
    "authors": [
      "Sahithya Ravi",
      "Patrick Huber",
      "Akshat Shrivastava",
      "Aditya Sagar",
      "Ahmed Aly",
      "Vered Shwartz",
      "Arash Einolghozati"
    ],
    "abstract": "The emergence of Large Language Models (LLMs) has brought to light promising\nlanguage generation capabilities, particularly in performing tasks like complex\nreasoning and creative writing. Consequently, distillation through imitation of\nteacher responses has emerged as a popular technique to transfer knowledge from\nLLMs to more accessible, Small Language Models (SLMs). While this works well\nfor simpler tasks, there is a substantial performance gap on tasks requiring\nintricate language comprehension and creativity, such as humor generation. We\nhypothesize that this gap may stem from the fact that creative tasks might be\nhard to learn by imitation alone and explore whether an approach, involving\nsupplementary guidance from the teacher, could yield higher performance. To\naddress this, we study the effect of assigning a dual role to the LLM - as a\n\"teacher\" generating data, as well as a \"critic\" evaluating the student's\nperformance. Our experiments on humor generation reveal that the incorporation\nof feedback significantly narrows the performance gap between SLMs and their\nlarger counterparts compared to merely relying on imitation. As a result, our\nresearch highlights the potential of using feedback as an additional dimension\nto data when transferring complex language abilities via distillation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18113v1",
    "published_date": "2024-02-28 07:02:38 UTC",
    "updated_date": "2024-02-28 07:02:38 UTC"
  },
  {
    "arxiv_id": "2402.18104v2",
    "title": "Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction",
    "authors": [
      "Tong Liu",
      "Yingjie Zhang",
      "Zhe Zhao",
      "Yinpeng Dong",
      "Guozhu Meng",
      "Kai Chen"
    ],
    "abstract": "In recent years, large language models (LLMs) have demonstrated notable\nsuccess across various tasks, but the trustworthiness of LLMs is still an open\nproblem. One specific threat is the potential to generate toxic or harmful\nresponses. Attackers can craft adversarial prompts that induce harmful\nresponses from LLMs. In this work, we pioneer a theoretical foundation in LLMs\nsecurity by identifying bias vulnerabilities within the safety fine-tuning and\ndesign a black-box jailbreak method named DRA (Disguise and Reconstruction\nAttack), which conceals harmful instructions through disguise and prompts the\nmodel to reconstruct the original harmful instruction within its completion. We\nevaluate DRA across various open-source and closed-source models, showcasing\nstate-of-the-art jailbreak success rates and attack efficiency. Notably, DRA\nboasts a 91.1% attack success rate on OpenAI GPT-4 chatbot.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18104v2",
    "published_date": "2024-02-28 06:50:14 UTC",
    "updated_date": "2024-06-10 11:20:43 UTC"
  },
  {
    "arxiv_id": "2402.18099v3",
    "title": "Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models",
    "authors": [
      "Derong Xu",
      "Ziheng Zhang",
      "Zhihong Zhu",
      "Zhenxi Lin",
      "Qidong Liu",
      "Xian Wu",
      "Tong Xu",
      "Wanyu Wang",
      "Yuyang Ye",
      "Xiangyu Zhao",
      "Enhong Chen",
      "Yefeng Zheng"
    ],
    "abstract": "Model editing aims to precisely alter the behaviors of large language models\n(LLMs) in relation to specific knowledge, while leaving unrelated knowledge\nintact. This approach has proven effective in addressing issues of\nhallucination and outdated information in LLMs. However, the potential of using\nmodel editing to modify knowledge in the medical field remains largely\nunexplored, even though resolving hallucination is a pressing need in this\narea. Our observations indicate that current methods face significant\nchallenges in dealing with specialized and complex knowledge in medical domain.\nTherefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for\nmedical model editing. MedLaSA harnesses the strengths of both adding extra\nparameters and locate-then-edit methods for medical model editing. We utilize\ncausal tracing to identify the association of knowledge in neurons across\ndifferent layers, and generate a corresponding scale set from the association\nvalue for each piece of knowledge. Subsequently, we incorporate scalable\nadapters into the dense layers of LLMs. These adapters are assigned scaling\nvalues based on the corresponding specific knowledge, which allows for the\nadjustment of the adapter's weight and rank. The more similar the content, the\nmore consistent the scale between them. This ensures precise editing of\nsemantically identical knowledge while avoiding impact on unrelated knowledge.\nTo evaluate the editing impact on the behaviours of LLMs, we propose two model\nediting studies for medical domain: (1) editing factual knowledge for medical\nspecialization and (2) editing the explanatory ability for complex knowledge.\nWe build two novel medical benchmarking datasets and introduce a series of\nchallenging and comprehensive metrics. Extensive experiments on medical LLMs\ndemonstrate the editing efficiency of MedLaSA, without affecting unrelated\nknowledge.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by CIKM 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.18099v3",
    "published_date": "2024-02-28 06:40:57 UTC",
    "updated_date": "2024-09-23 12:09:00 UTC"
  },
  {
    "arxiv_id": "2402.18096v1",
    "title": "No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization",
    "authors": [
      "June Yong Yang",
      "Byeongwook Kim",
      "Jeongin Bae",
      "Beomseok Kwon",
      "Gunho Park",
      "Eunho Yang",
      "Se Jung Kwon",
      "Dongsoo Lee"
    ],
    "abstract": "Key-Value (KV) Caching has become an essential technique for accelerating the\ninference speed and throughput of generative Large Language Models~(LLMs).\nHowever, the memory footprint of the KV cache poses a critical bottleneck in\nLLM deployment as the cache size grows with batch size and sequence length,\noften surpassing even the size of the model itself. Although recent methods\nwere proposed to select and evict unimportant KV pairs from the cache to reduce\nmemory consumption, the potential ramifications of eviction on the generative\nprocess are yet to be thoroughly examined. In this paper, we examine the\ndetrimental impact of cache eviction and observe that unforeseen risks arise as\nthe information contained in the KV pairs is exhaustively discarded, resulting\nin safety breaches, hallucinations, and context loss. Surprisingly, we find\nthat preserving even a small amount of information contained in the evicted KV\npairs via reduced precision quantization substantially recovers the incurred\ndegradation. On the other hand, we observe that the important KV pairs must be\nkept at a relatively higher precision to safeguard the generation quality.\nMotivated by these observations, we propose \\textit{Mixed-precision KV\ncache}~(MiKV), a reliable cache compression method that simultaneously\npreserves the context details by retaining the evicted KV pairs in\nlow-precision and ensure generation quality by keeping the important KV pairs\nin high-precision. Experiments on diverse benchmarks and LLM backbones show\nthat our proposed method offers a state-of-the-art trade-off between\ncompression ratio and performance, compared to other baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18096v1",
    "published_date": "2024-02-28 06:34:54 UTC",
    "updated_date": "2024-02-28 06:34:54 UTC"
  },
  {
    "arxiv_id": "2402.18091v1",
    "title": "Polos: Multimodal Metric Learning from Human Feedback for Image Captioning",
    "authors": [
      "Yuiga Wada",
      "Kanta Kaneda",
      "Daichi Saito",
      "Komei Sugiura"
    ],
    "abstract": "Establishing an automatic evaluation metric that closely aligns with human\njudgments is essential for effectively developing image captioning models.\nRecent data-driven metrics have demonstrated a stronger correlation with human\njudgments than classic metrics such as CIDEr; however they lack sufficient\ncapabilities to handle hallucinations and generalize across diverse images and\ntexts partially because they compute scalar similarities merely using\nembeddings learned from tasks unrelated to image captioning evaluation. In this\nstudy, we propose Polos, a supervised automatic evaluation metric for image\ncaptioning models. Polos computes scores from multimodal inputs, using a\nparallel feature extraction mechanism that leverages embeddings trained through\nlarge-scale contrastive learning. To train Polos, we introduce Multimodal\nMetric Learning from Human Feedback (M$^2$LHF), a framework for developing\nmetrics based on human feedback. We constructed the Polaris dataset, which\ncomprises 131K human judgments from 550 evaluators, which is approximately ten\ntimes larger than standard datasets. Our approach achieved state-of-the-art\nperformance on Composite, Flickr8K-Expert, Flickr8K-CF, PASCAL-50S, FOIL, and\nthe Polaris dataset, thereby demonstrating its effectiveness and robustness.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.18091v1",
    "published_date": "2024-02-28 06:24:39 UTC",
    "updated_date": "2024-02-28 06:24:39 UTC"
  },
  {
    "arxiv_id": "2402.18062v1",
    "title": "Generative AI for Unmanned Vehicle Swarms: Challenges, Applications and Opportunities",
    "authors": [
      "Guangyuan Liu",
      "Nguyen Van Huynh",
      "Hongyang Du",
      "Dinh Thai Hoang",
      "Dusit Niyato",
      "Kun Zhu",
      "Jiawen Kang",
      "Zehui Xiong",
      "Abbas Jamalipour",
      "Dong In Kim"
    ],
    "abstract": "With recent advances in artificial intelligence (AI) and robotics, unmanned\nvehicle swarms have received great attention from both academia and industry\ndue to their potential to provide services that are difficult and dangerous to\nperform by humans. However, learning and coordinating movements and actions for\na large number of unmanned vehicles in complex and dynamic environments\nintroduce significant challenges to conventional AI methods. Generative AI\n(GAI), with its capabilities in complex data feature extraction,\ntransformation, and enhancement, offers great potential in solving these\nchallenges of unmanned vehicle swarms. For that, this paper aims to provide a\ncomprehensive survey on applications, challenges, and opportunities of GAI in\nunmanned vehicle swarms. Specifically, we first present an overview of unmanned\nvehicles and unmanned vehicle swarms as well as their use cases and existing\nissues. Then, an in-depth background of various GAI techniques together with\ntheir capabilities in enhancing unmanned vehicle swarms are provided. After\nthat, we present a comprehensive review on the applications and challenges of\nGAI in unmanned vehicle swarms with various insights and discussions. Finally,\nwe highlight open issues of GAI in unmanned vehicle swarms and discuss\npotential research directions.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "23 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.18062v1",
    "published_date": "2024-02-28 05:46:23 UTC",
    "updated_date": "2024-02-28 05:46:23 UTC"
  },
  {
    "arxiv_id": "2402.18061v2",
    "title": "On the use of Silver Standard Data for Zero-shot Classification Tasks in Information Extraction",
    "authors": [
      "Jianwei Wang",
      "Tianyin Wang",
      "Ziqian Zeng"
    ],
    "abstract": "The superior performance of supervised classification methods in the\ninformation extraction (IE) area heavily relies on a large amount of gold\nstandard data. Recent zero-shot classification methods converted the task to\nother NLP tasks (e.g., textual entailment) and used off-the-shelf models of\nthese NLP tasks to directly perform inference on the test data without using a\nlarge amount of IE annotation data. A potentially valuable by-product of these\nmethods is the large-scale silver standard data, i.e., pseudo-labeled data by\nthe off-the-shelf models of other NLP tasks. However, there is no further\ninvestigation into the use of these data. In this paper, we propose a new\nframework, Clean-LaVe, which aims to utilize silver standard data to enhance\nthe zero-shot performance. Clean-LaVe includes four phases: (1) Obtaining\nsilver data; (2) Identifying relatively clean data from silver data; (3)\nFinetuning the off-the-shelf model using clean data; (4) Inference on the test\ndata. The experimental results show that Clean-LaVe can outperform the baseline\nby 5% and 6% on TACRED and Wiki80 dataset in the zero-shot relation\nclassification task, and by 3%-7% on Smile (Korean and Polish) in the zero-shot\ncross-lingual relation classification task, and by 8% on ACE05-E+ in the\nzero-shot event argument classification task. The code is share in\nhttps://github.com/wjw136/Clean_LaVe.git.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "accepted by coling2024. arXiv:2211.13883 is our first edition",
    "pdf_url": "http://arxiv.org/pdf/2402.18061v2",
    "published_date": "2024-02-28 05:45:37 UTC",
    "updated_date": "2024-03-06 08:42:24 UTC"
  },
  {
    "arxiv_id": "2402.18041v1",
    "title": "Datasets for Large Language Models: A Comprehensive Survey",
    "authors": [
      "Yang Liu",
      "Jiahuan Cao",
      "Chongyu Liu",
      "Kai Ding",
      "Lianwen Jin"
    ],
    "abstract": "This paper embarks on an exploration into the Large Language Model (LLM)\ndatasets, which play a crucial role in the remarkable advancements of LLMs. The\ndatasets serve as the foundational infrastructure analogous to a root system\nthat sustains and nurtures the development of LLMs. Consequently, examination\nof these datasets emerges as a critical topic in research. In order to address\nthe current lack of a comprehensive overview and thorough analysis of LLM\ndatasets, and to gain insights into their current status and future trends,\nthis survey consolidates and categorizes the fundamental aspects of LLM\ndatasets from five perspectives: (1) Pre-training Corpora; (2) Instruction\nFine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5)\nTraditional Natural Language Processing (NLP) Datasets. The survey sheds light\non the prevailing challenges and points out potential avenues for future\ninvestigation. Additionally, a comprehensive review of the existing available\ndataset resources is also provided, including statistics from 444 datasets,\ncovering 8 language categories and spanning 32 domains. Information from 20\ndimensions is incorporated into the dataset statistics. The total data size\nsurveyed surpasses 774.5 TB for pre-training corpora and 700M instances for\nother datasets. We aim to present the entire landscape of LLM text datasets,\nserving as a comprehensive reference for researchers in this field and\ncontributing to future studies. Related resources are available at:\nhttps://github.com/lmmlzn/Awesome-LLMs-Datasets.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "181 pages, 21 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.18041v1",
    "published_date": "2024-02-28 04:35:51 UTC",
    "updated_date": "2024-02-28 04:35:51 UTC"
  },
  {
    "arxiv_id": "2402.18040v1",
    "title": "Automated Discovery of Integral with Deep Learning",
    "authors": [
      "Xiaoxin Yin"
    ],
    "abstract": "Recent advancements in the realm of deep learning, particularly in the\ndevelopment of large language models (LLMs), have demonstrated AI's ability to\ntackle complex mathematical problems or solving programming challenges.\nHowever, the capability to solve well-defined problems based on extensive\ntraining data differs significantly from the nuanced process of making\nscientific discoveries. Trained on almost all human knowledge available,\ntoday's sophisticated LLMs basically learn to predict sequences of tokens. They\ngenerate mathematical derivations and write code in a similar way as writing an\nessay, and do not have the ability to pioneer scientific discoveries in the\nmanner a human scientist would do.\n  In this study we delve into the potential of using deep learning to\nrediscover a fundamental mathematical concept: integrals. By defining integrals\nas area under the curve, we illustrate how AI can deduce the integral of a\ngiven function, exemplified by inferring $\\int_{0}^{x} t^2 dt = \\frac{x^3}{3}$\nand $\\int_{0}^{x} ae^{bt} dt = \\frac{a}{b} e^{bx} - \\frac{a}{b}$. Our\nexperiments show that deep learning models can approach the task of inferring\nintegrals either through a sequence-to-sequence model, akin to language\ntranslation, or by uncovering the rudimentary principles of integration, such\nas $\\int_{0}^{x} t^n dt = \\frac{x^{n+1}}{n+1}$.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18040v1",
    "published_date": "2024-02-28 04:34:15 UTC",
    "updated_date": "2024-02-28 04:34:15 UTC"
  },
  {
    "arxiv_id": "2402.18039v1",
    "title": "ResLoRA: Identity Residual Mapping in Low-Rank Adaption",
    "authors": [
      "Shuhua Shi",
      "Shaohan Huang",
      "Minghui Song",
      "Zhoujun Li",
      "Zihan Zhang",
      "Haizhen Huang",
      "Furu Wei",
      "Weiwei Deng",
      "Feng Sun",
      "Qi Zhang"
    ],
    "abstract": "As one of the most popular parameter-efficient fine-tuning (PEFT) methods,\nlow-rank adaptation (LoRA) is commonly applied to fine-tune large language\nmodels (LLMs). However, updating the weights of LoRA blocks effectively and\nexpeditiously is challenging due to the long calculation path in the original\nmodel. To address this, we propose ResLoRA, an improved framework of LoRA. By\nadding residual paths during training and using merging approaches to eliminate\nthese extra paths during inference, our method can achieve better results in\nfewer training steps without any extra trainable parameters or inference cost\ncompared to LoRA. The experiments on NLG, NLU, and text-to-image tasks\ndemonstrate the effectiveness of our method. To the best of our knowledge,\nResLoRA is the first work that combines the residual path with LoRA. The code\nof our method is available at\nhttps://github.com/microsoft/LMOps/tree/main/reslora .",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.18039v1",
    "published_date": "2024-02-28 04:33:20 UTC",
    "updated_date": "2024-02-28 04:33:20 UTC"
  },
  {
    "arxiv_id": "2403.00829v1",
    "title": "TroubleLLM: Align to Red Team Expert",
    "authors": [
      "Zhuoer Xu",
      "Jianping Zhang",
      "Shiwen Cui",
      "Changhua Meng",
      "Weiqiang Wang"
    ],
    "abstract": "Large Language Models (LLMs) become the start-of-the-art solutions for a\nvariety of natural language tasks and are integrated into real-world\napplications. However, LLMs can be potentially harmful in manifesting\nundesirable safety issues like social biases and toxic content. It is\nimperative to assess its safety issues before deployment. However, the quality\nand diversity of test prompts generated by existing methods are still far from\nsatisfactory. Not only are these methods labor-intensive and require large\nbudget costs, but the controllability of test prompt generation is lacking for\nthe specific testing domain of LLM applications. With the idea of LLM for LLM\ntesting, we propose the first LLM, called TroubleLLM, to generate controllable\ntest prompts on LLM safety issues. Extensive experiments and human evaluation\nillustrate the superiority of TroubleLLM on generation quality and generation\ncontrollability.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.00829v1",
    "published_date": "2024-02-28 03:40:46 UTC",
    "updated_date": "2024-02-28 03:40:46 UTC"
  },
  {
    "arxiv_id": "2402.18023v3",
    "title": "Do Large Language Models Mirror Cognitive Language Processing?",
    "authors": [
      "Yuqi Ren",
      "Renren Jin",
      "Tongxuan Zhang",
      "Deyi Xiong"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable abilities in text\ncomprehension and logical reasoning, indicating that the text representations\nlearned by LLMs can facilitate their language processing capabilities. In\nneuroscience, brain cognitive processing signals are typically utilized to\nstudy human language processing. Therefore, it is natural to ask how well the\ntext embeddings from LLMs align with the brain cognitive processing signals,\nand how training strategies affect the LLM-brain alignment? In this paper, we\nemploy Representational Similarity Analysis (RSA) to measure the alignment\nbetween 23 mainstream LLMs and fMRI signals of the brain to evaluate how\neffectively LLMs simulate cognitive language processing. We empirically\ninvestigate the impact of various factors (e.g., pre-training data size, model\nscaling, alignment training, and prompts) on such LLM-brain alignment.\nExperimental results indicate that pre-training data size and model scaling are\npositively correlated with LLM-brain similarity, and alignment training can\nsignificantly improve LLM-brain similarity. Explicit prompts contribute to the\nconsistency of LLMs with brain cognitive language processing, while nonsensical\nnoisy prompts may attenuate such alignment. Additionally, the performance of a\nwide range of LLM evaluations (e.g., MMLU, Chatbot Arena) is highly correlated\nwith the LLM-brain similarity.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18023v3",
    "published_date": "2024-02-28 03:38:20 UTC",
    "updated_date": "2025-01-15 04:47:36 UTC"
  },
  {
    "arxiv_id": "2402.18016v3",
    "title": "User Decision Guidance with Selective Explanation Presentation from Explainable-AI",
    "authors": [
      "Yosuke Fukuchi",
      "Seiji Yamada"
    ],
    "abstract": "This paper addresses the challenge of selecting explanations for XAI\n(Explainable AI)-based Intelligent Decision Support Systems (IDSSs). IDSSs have\nshown promise in improving user decisions through XAI-generated explanations\nalong with AI predictions, and the development of XAI made it possible to\ngenerate a variety of such explanations. However, how IDSSs should select\nexplanations to enhance user decision-making remains an open question. This\npaper proposes X-Selector, a method for selectively presenting XAI\nexplanations. It enables IDSSs to strategically guide users to an AI-suggested\ndecision by predicting the impact of different combinations of explanations on\na user's decision and selecting the combination that is expected to minimize\nthe discrepancy between an AI suggestion and a user decision. We compared the\nefficacy of X-Selector with two naive strategies (all possible explanations and\nexplanations only for the most likely prediction) and two baselines (no\nexplanation and no AI support). The results suggest the potential of X-Selector\nto guide users to AI-suggested decisions and improve task performance under the\ncondition of a high AI accuracy.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted at the 2024 33rd IEEE International Conference on Robot and\n  Human Interactive Communication (RO-MAN)",
    "pdf_url": "http://arxiv.org/pdf/2402.18016v3",
    "published_date": "2024-02-28 03:21:25 UTC",
    "updated_date": "2024-05-27 01:40:54 UTC"
  },
  {
    "arxiv_id": "2403.07921v3",
    "title": "Merino: Entropy-driven Design for Generative Language Models on IoT Devices",
    "authors": [
      "Youpeng Zhao",
      "Ming Lin",
      "Huadong Tang",
      "Qiang Wu",
      "Jun Wang"
    ],
    "abstract": "Generative Large Language Models (LLMs) stand as a revolutionary advancement\nin the modern era of artificial intelligence (AI). However, scaling down LLMs\nfor resource-constrained hardware, such as Internet-of-Things (IoT) devices\nrequires non-trivial efforts and domain knowledge. In this paper, we propose a\nnovel information-entropy framework for designing mobile-friendly generative\nlanguage models. The whole design procedure involves solving a mathematical\nprogramming (MP) problem, which can be done on the CPU within minutes, making\nit nearly zero-cost. We evaluate our designed models, termed MeRino, across\nfourteen NLP downstream tasks, showing their competitive performance against\nthe state-of-the-art autoregressive transformer models under the mobile\nsetting. Notably, MeRino achieves similar or better performance on both\nlanguage modeling and zero-shot learning tasks, compared to the 350M parameter\nOPT while being 4.9x faster on NVIDIA Jetson Nano with 5.5x reduction in model\nsize.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2403.07921v3",
    "published_date": "2024-02-28 03:20:27 UTC",
    "updated_date": "2025-01-27 15:39:26 UTC"
  },
  {
    "arxiv_id": "2402.18013v1",
    "title": "A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems",
    "authors": [
      "Zihao Yi",
      "Jiarui Ouyang",
      "Yuwen Liu",
      "Tianhao Liao",
      "Zhe Xu",
      "Ying Shen"
    ],
    "abstract": "This survey provides a comprehensive review of research on multi-turn\ndialogue systems, with a particular focus on multi-turn dialogue systems based\non large language models (LLMs). This paper aims to (a) give a summary of\nexisting LLMs and approaches for adapting LLMs to downstream tasks; (b)\nelaborate recent advances in multi-turn dialogue systems, covering both\nLLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems,\nalong with datasets and evaluation metrics; (c) discuss some future emphasis\nand recent research problems arising from the development of LLMs and the\nincreasing demands on multi-turn dialogue systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "35 pages, 10 figures, ACM Computing Surveys",
    "pdf_url": "http://arxiv.org/pdf/2402.18013v1",
    "published_date": "2024-02-28 03:16:44 UTC",
    "updated_date": "2024-02-28 03:16:44 UTC"
  },
  {
    "arxiv_id": "2402.18012v3",
    "title": "Diffusion Models as Constrained Samplers for Optimization with Unknown Constraints",
    "authors": [
      "Lingkai Kong",
      "Yuanqi Du",
      "Wenhao Mu",
      "Kirill Neklyudov",
      "Valentin De Bortoli",
      "Dongxia Wu",
      "Haorui Wang",
      "Aaron Ferber",
      "Yi-An Ma",
      "Carla P. Gomes",
      "Chao Zhang"
    ],
    "abstract": "Addressing real-world optimization problems becomes particularly challenging\nwhen analytic objective functions or constraints are unavailable. While\nnumerous studies have addressed the issue of unknown objectives, limited\nresearch has focused on scenarios where feasibility constraints are not given\nexplicitly. Overlooking these constraints can lead to spurious solutions that\nare unrealistic in practice. To deal with such unknown constraints, we propose\nto perform optimization within the data manifold using diffusion models. To\nconstrain the optimization process to the data manifold, we reformulate the\noriginal optimization problem as a sampling problem from the product of the\nBoltzmann distribution defined by the objective function and the data\ndistribution learned by the diffusion model. Depending on the differentiability\nof the objective function, we propose two different sampling methods. For\ndifferentiable objectives, we propose a two-stage framework that begins with a\nguided diffusion process for warm-up, followed by a Langevin dynamics stage for\nfurther correction. For non-differentiable objectives, we propose an iterative\nimportance sampling strategy using the diffusion model as the proposal\ndistribution. Comprehensive experiments on a synthetic dataset, six real-world\nblack-box optimization datasets, and a multi-objective molecule optimization\ndataset show that our method achieves better or comparable performance with\nprevious state-of-the-art baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18012v3",
    "published_date": "2024-02-28 03:09:12 UTC",
    "updated_date": "2024-10-21 04:06:02 UTC"
  },
  {
    "arxiv_id": "2403.14660v1",
    "title": "Machina Economicus: A New Paradigm for Prosumers in the Energy Internet of Smart Cities",
    "authors": [
      "Luyang Hou",
      "Jun Yan",
      "Yuankai Wu",
      "Chun Wang",
      "Tie Qiu"
    ],
    "abstract": "Energy Internet (EI) is emerging as new share economy platform for flexible\nlocal energy supplies in smart cities. Empowered by the Internet-of-Things\n(IoT) and Artificial Intelligence (AI), EI aims to unlock peer-to-peer energy\ntrading and sharing among prosumers, who can adeptly switch roles between\nproviders and consumers in localized energy markets with rooftop photovoltaic\npanels, vehicle-to-everything technologies, packetized energy management, etc.\nThe integration of prosumers in EI, however, will encounter many challenges in\nmodelling, analyzing, and designing an efficient, economic, and social-optimal\nplatform for energy sharing, calling for advanced AI/IoT-based solutions to\nresource optimization, information exchange, and interaction protocols in the\ncontext of the share economy. In this study, we aim to introduce a recently\nemerged paradigm, Machina Economicus, to investigate the economic rationality\nin modelling, analysis, and optimization of AI/IoT-based EI prosumer behaviors.\nThe new paradigm, built upon the theory of machine learning and mechanism\ndesign, will offer new angles to investigate the selfishness of AI through a\ngame-theoretic perspective, revealing potential competition and collaborations\nresulting from the self-adaptive learning and decision-making capacity. This\nstudy will focus on how the introduction of AI will reshape prosumer behaviors\non the EI, and how this paradigm will reveal new research questions and\ndirections when AI meets the share economy. With an extensive case analysis in\nthe literature, we will also shed light on potential solutions for advancements\nof AI in future smart cities.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "25 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2403.14660v1",
    "published_date": "2024-02-28 02:53:17 UTC",
    "updated_date": "2024-02-28 02:53:17 UTC"
  },
  {
    "arxiv_id": "2402.18007v2",
    "title": "Mixer is more than just a model",
    "authors": [
      "Qingfeng Ji",
      "Yuxin Wang",
      "Letong Sun"
    ],
    "abstract": "Recently, MLP structures have regained popularity, with MLP-Mixer standing\nout as a prominent example. In the field of computer vision, MLP-Mixer is noted\nfor its ability to extract data information from both channel and token\nperspectives, effectively acting as a fusion of channel and token information.\nIndeed, Mixer represents a paradigm for information extraction that amalgamates\nchannel and token information. The essence of Mixer lies in its ability to\nblend information from diverse perspectives, epitomizing the true concept of\n\"mixing\" in the realm of neural network architectures. Beyond channel and token\nconsiderations, it is possible to create more tailored mixers from various\nperspectives to better suit specific task requirements. This study focuses on\nthe domain of audio recognition, introducing a novel model named Audio\nSpectrogram Mixer with Roll-Time and Hermit FFT (ASM-RH) that incorporates\ninsights from both time and frequency domains. Experimental results demonstrate\nthat ASM-RH is particularly well-suited for audio data and yields promising\noutcomes across multiple classification tasks. The models and optimal weights\nfiles will be published.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18007v2",
    "published_date": "2024-02-28 02:45:58 UTC",
    "updated_date": "2024-03-02 03:32:40 UTC"
  },
  {
    "arxiv_id": "2402.18005v2",
    "title": "A Sentiment Consolidation Framework for Meta-Review Generation",
    "authors": [
      "Miao Li",
      "Jey Han Lau",
      "Eduard Hovy"
    ],
    "abstract": "Modern natural language generation systems with Large Language Models (LLMs)\nexhibit the capability to generate a plausible summary of multiple documents;\nhowever, it is uncertain if they truly possess the capability of information\nconsolidation to generate summaries, especially on documents with opinionated\ninformation. We focus on meta-review generation, a form of sentiment\nsummarisation for the scientific domain. To make scientific sentiment\nsummarization more grounded, we hypothesize that human meta-reviewers follow a\nthree-layer framework of sentiment consolidation to write meta-reviews. Based\non the framework, we propose novel prompting methods for LLMs to generate\nmeta-reviews and evaluation metrics to assess the quality of generated\nmeta-reviews. Our framework is validated empirically as we find that prompting\nLLMs based on the framework -- compared with prompting them with simple\ninstructions -- generates better meta-reviews.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Long paper, ACL 2024 Main",
    "pdf_url": "http://arxiv.org/pdf/2402.18005v2",
    "published_date": "2024-02-28 02:40:09 UTC",
    "updated_date": "2024-06-04 16:10:13 UTC"
  },
  {
    "arxiv_id": "2402.18002v2",
    "title": "Symmetry-aware Reinforcement Learning for Robotic Assembly under Partial Observability with a Soft Wrist",
    "authors": [
      "Hai Nguyen",
      "Tadashi Kozuno",
      "Cristian C. Beltran-Hernandez",
      "Masashi Hamaya"
    ],
    "abstract": "This study tackles the representative yet challenging contact-rich\npeg-in-hole task of robotic assembly, using a soft wrist that can operate more\nsafely and tolerate lower-frequency control signals than a rigid one. Previous\nstudies often use a fully observable formulation, requiring external setups or\nestimators for the peg-to-hole pose. In contrast, we use a partially observable\nformulation and deep reinforcement learning from demonstrations to learn a\nmemory-based agent that acts purely on haptic and proprioceptive signals.\nMoreover, previous works do not incorporate potential domain symmetry and thus\nmust search for solutions in a bigger space. Instead, we propose to leverage\nthe symmetry for sample efficiency by augmenting the training data and\nconstructing auxiliary losses to force the agent to adhere to the symmetry.\nResults in simulation with five different symmetric peg shapes show that our\nproposed agent can be comparable to or even outperform a state-based agent. In\nparticular, the sample efficiency also allows us to learn directly on the real\nrobot within 3 hours.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted at ICRA-2024",
    "pdf_url": "http://arxiv.org/pdf/2402.18002v2",
    "published_date": "2024-02-28 02:30:59 UTC",
    "updated_date": "2024-04-29 19:00:56 UTC"
  },
  {
    "arxiv_id": "2403.00017v1",
    "title": "Towards Interpreting Multi-Objective Feature Associations",
    "authors": [
      "Nisha Pillai",
      "Ganga Gireesan",
      "Michael J. Rothrock Jr.",
      "Bindu Nanduri",
      "Zhiqian Chen",
      "Mahalingam Ramkumar"
    ],
    "abstract": "Understanding how multiple features are associated and contribute to a\nspecific objective is as important as understanding how each feature\ncontributes to a particular outcome. Interpretability of a single feature in a\nprediction may be handled in multiple ways; however, in a multi-objective\nprediction, it is difficult to obtain interpretability of a combination of\nfeature values. To address this issue, we propose an objective specific feature\ninteraction design using multi-labels to find the optimal combination of\nfeatures in agricultural settings. One of the novel aspects of this design is\nthe identification of a method that integrates feature explanations with global\nsensitivity analysis in order to ensure combinatorial optimization in\nmulti-objective settings. We have demonstrated in our preliminary experiments\nthat an approximate combination of feature values can be found to achieve the\ndesired outcome using two agricultural datasets: one with pre-harvest poultry\nfarm practices for multi-drug resistance presence, and one with post-harvest\npoultry farm practices for food-borne pathogens. In our combinatorial\noptimization approach, all three pathogens are taken into consideration\nsimultaneously to account for the interaction between conditions that favor\ndifferent types of pathogen growth. These results indicate that\nexplanation-based approaches are capable of identifying combinations of\nfeatures that reduce pathogen presence in fewer iterations than a baseline.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "The 18th Annual IEEE International Systems Conference 2024 (IEEE\n  SYSCON 2024)",
    "pdf_url": "http://arxiv.org/pdf/2403.00017v1",
    "published_date": "2024-02-28 02:24:04 UTC",
    "updated_date": "2024-02-28 02:24:04 UTC"
  },
  {
    "arxiv_id": "2403.00016v1",
    "title": "Deep Sensitivity Analysis for Objective-Oriented Combinatorial Optimization",
    "authors": [
      "Ganga Gireesan",
      "Nisha Pillai",
      "Michael J Rothrock",
      "Bindu Nanduri",
      "Zhiqian Chen",
      "Mahalingam Ramkumar"
    ],
    "abstract": "Pathogen control is a critical aspect of modern poultry farming, providing\nimportant benefits for both public health and productivity. Effective poultry\nmanagement measures to reduce pathogen levels in poultry flocks promote food\nsafety by lowering risks of food-borne illnesses. They also support animal\nhealth and welfare by preventing infectious diseases that can rapidly spread\nand impact flock growth, egg production, and overall health. This study frames\nthe search for optimal management practices that minimize the presence of\nmultiple pathogens as a combinatorial optimization problem. Specifically, we\nmodel the various possible combinations of management settings as a solution\nspace that can be efficiently explored to identify configurations that\noptimally reduce pathogen levels. This design incorporates a neural network\nfeedback-based method that combines feature explanations with global\nsensitivity analysis to ensure combinatorial optimization in multiobjective\nsettings. Our preliminary experiments have promising results when applied to\ntwo real-world agricultural datasets. While further validation is still needed,\nthese early experimental findings demonstrate the potential of the model to\nderive targeted feature interactions that adaptively optimize pathogen control\nunder varying real-world constraints.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "The 2023 International Conference on Computational Science &\n  Computational Intelligence (CSCI'23)",
    "pdf_url": "http://arxiv.org/pdf/2403.00016v1",
    "published_date": "2024-02-28 02:15:47 UTC",
    "updated_date": "2024-02-28 02:15:47 UTC"
  },
  {
    "arxiv_id": "2402.17985v1",
    "title": "FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization",
    "authors": [
      "Yi Zhang",
      "Fei Yang",
      "Shuang Peng",
      "Fangyu Wang",
      "Aimin Pan"
    ],
    "abstract": "Large language models (LLMs) have demonstrated state-of-the-art performance\nacross various tasks. However, the latency of inference and the large GPU\nmemory consumption of LLMs restrict their deployment performance. Recently,\nthere have been some efficient attempts to quantize LLMs, yet inference with\nlarge batch size or long sequence still has the issue of being compute-bound.\nFine-grained quantization methods have showcased their proficiency in achieving\nlow-bit quantization for LLMs, while requiring FP16 data type for linear layer\ncomputations, which is time-consuming when dealing with large batch size or\nlong sequence. In this paper, we introduce a method called FlattenQuant, which\nsignificantly reduces the maximum value of the tensor by flattening the large\nchannels in the tensor, to achieve low bit per-tensor quantization with minimal\naccuracy loss. Our experiments show that FlattenQuant can directly use 4 bits\nto achieve 48.29% of the linear layer calculation in LLMs, with the remaining\nlayers using 8 bits. The 4-bit matrix multiplication introduced in the\nFlattenQuant method can effectively address the compute-bound caused by large\nmatrix calculation. Our work achieves up to 2$\\times$ speedup and 2.3$\\times$\nmemory reduction for LLMs with negligible loss in accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.17985v1",
    "published_date": "2024-02-28 02:00:34 UTC",
    "updated_date": "2024-02-28 02:00:34 UTC"
  },
  {
    "arxiv_id": "2402.17979v1",
    "title": "Ensemble Methodology:Innovations in Credit Default Prediction Using LightGBM, XGBoost, and LocalEnsemble",
    "authors": [
      "Mengran Zhu",
      "Ye Zhang",
      "Yulu Gong",
      "Kaijuan Xing",
      "Xu Yan",
      "Jintong Song"
    ],
    "abstract": "In the realm of consumer lending, accurate credit default prediction stands\nas a critical element in risk mitigation and lending decision optimization.\nExtensive research has sought continuous improvement in existing models to\nenhance customer experiences and ensure the sound economic functioning of\nlending institutions. This study responds to the evolving landscape of credit\ndefault prediction, challenging conventional models and introducing innovative\napproaches. By building upon foundational research and recent innovations, our\nwork aims to redefine the standards of accuracy in credit default prediction,\nsetting a new benchmark for the industry. To overcome these challenges, we\npresent an Ensemble Methods framework comprising LightGBM, XGBoost, and\nLocalEnsemble modules, each making unique contributions to amplify diversity\nand improve generalization. By utilizing distinct feature sets, our methodology\ndirectly tackles limitations identified in previous studies, with the\noverarching goal of establishing a novel standard for credit default prediction\naccuracy. Our experimental findings validate the effectiveness of the ensemble\nmodel on the dataset, signifying substantial contributions to the field. This\ninnovative approach not only addresses existing obstacles but also sets a\nprecedent for advancing the accuracy and robustness of credit default\nprediction models.",
    "categories": [
      "cs.CE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.17979v1",
    "published_date": "2024-02-28 01:48:54 UTC",
    "updated_date": "2024-02-28 01:48:54 UTC"
  },
  {
    "arxiv_id": "2402.17978v2",
    "title": "Imagine, Initialize, and Explore: An Effective Exploration Method in Multi-Agent Reinforcement Learning",
    "authors": [
      "Zeyang Liu",
      "Lipeng Wan",
      "Xinrui Yang",
      "Zhuoran Chen",
      "Xingyu Chen",
      "Xuguang Lan"
    ],
    "abstract": "Effective exploration is crucial to discovering optimal strategies for\nmulti-agent reinforcement learning (MARL) in complex coordination tasks.\nExisting methods mainly utilize intrinsic rewards to enable committed\nexploration or use role-based learning for decomposing joint action spaces\ninstead of directly conducting a collective search in the entire\naction-observation space. However, they often face challenges obtaining\nspecific joint action sequences to reach successful states in long-horizon\ntasks. To address this limitation, we propose Imagine, Initialize, and Explore\n(IIE), a novel method that offers a promising solution for efficient\nmulti-agent exploration in complex scenarios. IIE employs a transformer model\nto imagine how the agents reach a critical state that can influence each\nother's transition functions. Then, we initialize the environment at this state\nusing a simulator before the exploration phase. We formulate the imagination as\na sequence modeling problem, where the states, observations, prompts, actions,\nand rewards are predicted autoregressively. The prompt consists of\ntimestep-to-go, return-to-go, influence value, and one-shot demonstration,\nspecifying the desired state and trajectory as well as guiding the action\ngeneration. By initializing agents at the critical states, IIE significantly\nincreases the likelihood of discovering potentially important under-explored\nregions. Despite its simplicity, empirical results demonstrate that our method\noutperforms multi-agent exploration baselines on the StarCraft Multi-Agent\nChallenge (SMAC) and SMACv2 environments. Particularly, IIE shows improved\nperformance in the sparse-reward SMAC tasks and produces more effective\ncurricula over the initialized states than other generative methods, such as\nCVAE-GAN and diffusion models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "The 38th Annual AAAI Conference on Artificial Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2402.17978v2",
    "published_date": "2024-02-28 01:45:01 UTC",
    "updated_date": "2024-03-01 11:08:48 UTC"
  },
  {
    "arxiv_id": "2402.17975v1",
    "title": "Sample-Efficient Preference-based Reinforcement Learning with Dynamics Aware Rewards",
    "authors": [
      "Katherine Metcalf",
      "Miguel Sarabia",
      "Natalie Mackraz",
      "Barry-John Theobald"
    ],
    "abstract": "Preference-based reinforcement learning (PbRL) aligns a robot behavior with\nhuman preferences via a reward function learned from binary feedback over agent\nbehaviors. We show that dynamics-aware reward functions improve the sample\nefficiency of PbRL by an order of magnitude. In our experiments we iterate\nbetween: (1) learning a dynamics-aware state-action representation (z^{sa}) via\na self-supervised temporal consistency task, and (2) bootstrapping the\npreference-based reward function from (z^{sa}), which results in faster policy\nlearning and better final policy performance. For example, on quadruped-walk,\nwalker-walk, and cheetah-run, with 50 preference labels we achieve the same\nperformance as existing approaches with 500 preference labels, and we recover\n83\\% and 66\\% of ground truth reward policy performance versus only 38\\% and\n21\\%. The performance gains demonstrate the benefits of explicitly learning a\ndynamics-aware reward model. Repo: \\texttt{https://github.com/apple/ml-reed}.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "CoRL 2023. arXiv admin note: substantial text overlap with\n  arXiv:2211.06527",
    "pdf_url": "http://arxiv.org/pdf/2402.17975v1",
    "published_date": "2024-02-28 01:41:34 UTC",
    "updated_date": "2024-02-28 01:41:34 UTC"
  },
  {
    "arxiv_id": "2402.17971v2",
    "title": "All in an Aggregated Image for In-Image Learning",
    "authors": [
      "Lei Wang",
      "Wanyu Xu",
      "Zhiqiang Hu",
      "Yihuai Lan",
      "Shan Dong",
      "Hao Wang",
      "Roy Ka-Wei Lee",
      "Ee-Peng Lim"
    ],
    "abstract": "This paper introduces a new in-context learning (ICL) mechanism called\nIn-Image Learning (I$^2$L) that combines demonstration examples, visual cues,\nand chain-of-thought reasoning into an aggregated image to enhance the\ncapabilities of Large Multimodal Models (e.g., GPT-4V) in multimodal reasoning\ntasks. Unlike previous approaches that rely on converting images to text or\nincorporating visual input into language models, I$^2$L consolidates all\ninformation into an aggregated image and leverages image processing,\nunderstanding, and reasoning abilities. This has several advantages: it reduces\ninaccurate textual descriptions of complex images, provides flexibility in\npositioning demonstration examples, and avoids multiple input images and\nlengthy prompts. We also introduce I$^2$L-Hybrid, a method that combines the\nstrengths of I$^2$L with other ICL methods. Specifically, it uses an automatic\nstrategy to select the most suitable method (I$^2$L or another certain ICL\nmethod) for a specific task instance. We conduct extensive experiments to\nassess the effectiveness of I$^2$L and I$^2$L-Hybrid on MathVista, which covers\na variety of complex multimodal reasoning tasks. Additionally, we investigate\nthe influence of image resolution, the number of demonstration examples in a\nsingle image, and the positions of these demonstrations in the aggregated image\non the effectiveness of I$^2$L. Our code is publicly available at\nhttps://github.com/AGI-Edgerunners/IIL.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2402.17971v2",
    "published_date": "2024-02-28 01:32:59 UTC",
    "updated_date": "2024-04-02 09:32:51 UTC"
  },
  {
    "arxiv_id": "2403.07920v1",
    "title": "ProtLLM: An Interleaved Protein-Language LLM with Protein-as-Word Pre-Training",
    "authors": [
      "Le Zhuo",
      "Zewen Chi",
      "Minghao Xu",
      "Heyan Huang",
      "Heqi Zheng",
      "Conghui He",
      "Xian-Ling Mao",
      "Wentao Zhang"
    ],
    "abstract": "We propose ProtLLM, a versatile cross-modal large language model (LLM) for\nboth protein-centric and protein-language tasks. ProtLLM features a unique\ndynamic protein mounting mechanism, enabling it to handle complex inputs where\nthe natural language text is interspersed with an arbitrary number of proteins.\nBesides, we propose the protein-as-word language modeling approach to train\nProtLLM. By developing a specialized protein vocabulary, we equip the model\nwith the capability to predict not just natural language but also proteins from\na vast pool of candidates. Additionally, we construct a large-scale interleaved\nprotein-text dataset, named InterPT, for pre-training. This dataset\ncomprehensively encompasses both (1) structured data sources like protein\nannotations and (2) unstructured data sources like biological research papers,\nthereby endowing ProtLLM with crucial knowledge for understanding proteins. We\nevaluate ProtLLM on classic supervised protein-centric tasks and explore its\nnovel protein-language applications. Experimental results demonstrate that\nProtLLM not only achieves superior performance against protein-specialized\nbaselines on protein-centric tasks but also induces zero-shot and in-context\nlearning capabilities on protein-language tasks.",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM",
    "comment": "https://protllm.github.io/project/",
    "pdf_url": "http://arxiv.org/pdf/2403.07920v1",
    "published_date": "2024-02-28 01:29:55 UTC",
    "updated_date": "2024-02-28 01:29:55 UTC"
  },
  {
    "arxiv_id": "2402.17969v1",
    "title": "Vision Language Model-based Caption Evaluation Method Leveraging Visual Context Extraction",
    "authors": [
      "Koki Maeda",
      "Shuhei Kurita",
      "Taiki Miyanishi",
      "Naoaki Okazaki"
    ],
    "abstract": "Given the accelerating progress of vision and language modeling, accurate\nevaluation of machine-generated image captions remains critical. In order to\nevaluate captions more closely to human preferences, metrics need to\ndiscriminate between captions of varying quality and content. However,\nconventional metrics fail short of comparing beyond superficial matches of\nwords or embedding similarities; thus, they still need improvement. This paper\npresents VisCE$^2$, a vision language model-based caption evaluation method.\nOur method focuses on visual context, which refers to the detailed content of\nimages, including objects, attributes, and relationships. By extracting and\norganizing them into a structured format, we replace the human-written\nreferences with visual contexts and help VLMs better understand the image,\nenhancing evaluation performance. Through meta-evaluation on multiple datasets,\nwe validated that VisCE$^2$ outperforms the conventional pre-trained metrics in\ncapturing caption quality and demonstrates superior consistency with human\njudgment.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.17969v1",
    "published_date": "2024-02-28 01:29:36 UTC",
    "updated_date": "2024-02-28 01:29:36 UTC"
  },
  {
    "arxiv_id": "2402.18600v1",
    "title": "Artificial Intelligence and Diabetes Mellitus: An Inside Look Through the Retina",
    "authors": [
      "Yasin Sadeghi Bazargani",
      "Majid Mirzaei",
      "Navid Sobhi",
      "Mirsaeed Abdollahi",
      "Ali Jafarizadeh",
      "Siamak Pedrammehr",
      "Roohallah Alizadehsani",
      "Ru San Tan",
      "Sheikh Mohammed Shariful Islam",
      "U. Rajendra Acharya"
    ],
    "abstract": "Diabetes mellitus (DM) predisposes patients to vascular complications.\nRetinal images and vasculature reflect the body's micro- and macrovascular\nhealth. They can be used to diagnose DM complications, including diabetic\nretinopathy (DR), neuropathy, nephropathy, and atherosclerotic cardiovascular\ndisease, as well as forecast the risk of cardiovascular events. Artificial\nintelligence (AI)-enabled systems developed for high-throughput detection of DR\nusing digitized retinal images have become clinically adopted. Beyond DR\nscreening, AI integration also holds immense potential to address challenges\nassociated with the holistic care of the patient with DM. In this work, we aim\nto comprehensively review the literature for studies on AI applications based\non retinal images related to DM diagnosis, prognostication, and management. We\nwill describe the findings of holistic AI-assisted diabetes care, including but\nnot limited to DR screening, and discuss barriers to implementing such systems,\nincluding issues concerning ethics, data privacy, equitable access, and\nexplainability. With the ability to evaluate the patient's health status vis a\nvis DM complication as well as risk prognostication of future cardiovascular\ncomplications, AI-assisted retinal image analysis has the potential to become a\ncentral tool for modern personalized medicine in patients with DM.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "q-bio.TO",
      "J.3.2; J.3.3"
    ],
    "primary_category": "eess.IV",
    "comment": "44 Pages, 6 figures, 1 table, 166 references",
    "pdf_url": "http://arxiv.org/pdf/2402.18600v1",
    "published_date": "2024-02-28 00:31:17 UTC",
    "updated_date": "2024-02-28 00:31:17 UTC"
  },
  {
    "arxiv_id": "2403.14659v1",
    "title": "Social Intelligence Data Infrastructure: Structuring the Present and Navigating the Future",
    "authors": [
      "Minzhi Li",
      "Weiyan Shi",
      "Caleb Ziems",
      "Diyi Yang"
    ],
    "abstract": "As Natural Language Processing (NLP) systems become increasingly integrated\ninto human social life, these technologies will need to increasingly rely on\nsocial intelligence. Although there are many valuable datasets that benchmark\nisolated dimensions of social intelligence, there does not yet exist any body\nof work to join these threads into a cohesive subfield in which researchers can\nquickly identify research gaps and future directions. Towards this goal, we\nbuild a Social AI Data Infrastructure, which consists of a comprehensive social\nAI taxonomy and a data library of 480 NLP datasets. Our infrastructure allows\nus to analyze existing dataset efforts, and also evaluate language models'\nperformance in different social intelligence aspects. Our analyses demonstrate\nits utility in enabling a thorough understanding of current data landscape and\nproviding a holistic perspective on potential directions for future dataset\ndevelopment. We show there is a need for multifaceted datasets, increased\ndiversity in language and culture, more long-tailed social situations, and more\ninteractive data in future social intelligence data efforts.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14659v1",
    "published_date": "2024-02-28 00:22:42 UTC",
    "updated_date": "2024-02-28 00:22:42 UTC"
  }
]
{
  "date": "2025-04-17",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间2025-04-17的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 上的论文亮点纷呈，涵盖了从基础模型构建、评估到具体应用的广泛领域。**Meta AI 推出了 PerceptionLM**，一个旨在推动开放、可复现视觉理解研究的大型数据集和模型项目，备受瞩目。同时，**LLM 的评估、对齐、鲁棒性和效率**依然是研究热点，出现了多个新基准（如 MIB、Complex-Edit、FreshStack、RAMDocs、NaVAB、ZeroSumEval）和新方法（如 EEF、Antidistillation、SMC 控制、IGCIDB 去偏、GRAIL 多域遗忘、ACoRN 压缩）。此外，**机器人技术**（RUKA 仿人手、RoboTwin 双臂基准、Human2Sim2Robot 示教学习）、**多模态学习**（如 VLM 创造力探索、几何问题求解、心脏 MRI 分析）、**科学应用**（拓扑材料设计、天气预报后处理）以及**AI 伦理与安全**（可解释性、隐私保护、对抗攻击防御）也贡献了许多有趣的工作。\n\n接下来，让我们一起看看今天值得关注的论文：\n\n---\n\n**重点论文 & 热点方向**\n\n1.  **PerceptionLM: 用于详细视觉理解的开放数据和模型 (PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding)**\n    *   来自 Meta AI 等机构的研究者们针对当前高性能视觉语言模型多为闭源的问题，提出了 PerceptionLM 框架，旨在推动图像和视频理解领域的开放和可复现研究。他们分析了不依赖专有模型蒸馏的标准训练流程，并利用大规模合成数据识别了数据缺口，特别是在细粒度视频理解方面。为此，他们发布了 280 万个人工标注的细粒度视频问答对和时空定位视频字幕数据集，并引入了 PLM-VideoBench 评估套件，用于评估复杂的视频理解任务（what, where, when, how）。这项工作通过提供数据、训练方法、代码和模型，实现了完全可复现。\n\n2.  **MIB：一个机制可解释性基准 (MIB: A Mechanistic Interpretability Benchmark)**\n    *   为了建立有意义且持久的机制可解释性方法评估标准，该研究提出了 MIB 基准。MIB 包含两个赛道，涵盖四个任务和五个模型，旨在评估那些能够精确、简洁地恢复神经网络语言模型中相关因果路径或特定因果变量的方法。电路定位赛道比较定位模型组件及其连接重要性的方法（如归因修补），因果变量定位赛道比较特征化隐藏向量并定位相关模型特征的方法（如稀疏自编码器 SAE、分布式对齐搜索 DAS）。初步评估发现，归因和掩码优化方法在电路定位上表现最佳，而监督式 DAS 在因果变量定位上优于 SAE 特征和标准神经元维度。MIB 为比较不同方法提供了平台，有助于衡量该领域的实际进展。\n\n3.  **探索专家失败能改进 LLM Agent 微调 (Exploring Expert Failures Improves LLM Agent Tuning)**\n    *   现有 LLM Agent 微调方法（如 RFT）倾向于模仿成功的专家轨迹，导致在复杂子任务上表现不佳。该研究发现，先前失败的专家轨迹（如 GPT-4 的失败尝试）中往往包含有价值的指导信息（计划、关键动作）。他们提出 EEF 方法，识别并整合这些失败轨迹中的有益动作到训练数据中，同时排除有害动作。EEF 成功解决了 RFT 难以解决的复杂子任务，并在 WebShop 和 SciWorld 等基准上取得了 SOTA 性能，显著优于 RFT 和 GPT-4。\n\n4.  **反蒸馏采样 (Antidistillation Sampling)**\n    *   大型模型生成的推理轨迹可能无意中泄露信息，便于模型蒸馏。为了在不牺牲模型性能的前提下限制蒸馏效果，该研究提出了“反蒸馏采样”策略。通过策略性地修改模型的下一个词元概率分布，该方法可以“毒化”推理轨迹，使其对蒸馏的效果显著降低，同时保持模型的实用性。\n\n5.  **睡眠时间计算：超越测试时推理扩展 (Sleep-time Compute: Beyond Inference Scaling at Test-time)**\n    *   测试时计算扩展是 LLM 解决难题的关键，但延迟和成本高。该研究提出“睡眠时间计算”概念，允许模型在查询出现前离线“思考”上下文。通过预测用户可能提出的查询并预计算有用信息，可以显著减少测试时的计算需求。实验表明，在修改后的推理任务上，睡眠时间计算能将达到相同准确度所需的测试时计算量减少约 5 倍，并能进一步提升准确度。该方法在多查询场景下还能摊销成本。用户查询的可预测性与该方法的有效性相关。\n\n6.  **RUKA: 结合学习重新思考仿人手设计 (RUKA: Rethinking the Design of Humanoid Hands with Learning)**\n    *   针对灵巧操作中硬件在精度、紧凑性、强度和成本之间的权衡，该研究提出了 RUKA，一种紧凑、经济、功能强大的肌腱驱动仿人手。RUKA 使用 3D 打印部件和现成组件制造，拥有 5 个手指和 15 个欠驱动自由度，能实现多样化的人类抓取。为解决控制挑战，研究者利用 MANUS 手套收集的运动捕捉数据，学习了关节到执行器和指尖到执行器的模型。实验证明 RUKA 在可达性、耐用性和力量方面优于其他机械手，并通过遥操作任务展示了其灵活性。设计、代码和数据均已开源。\n\n7.  **一切皆有关联：穿越测试时记忆、注意力偏差、保留和在线优化的旅程 (It's All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization)**\n    *   受人类认知中注意力偏差现象的启发，该研究将 Transformer、Titan 等神经网络架构重新概念化为使用内部目标（称为注意力偏差）学习键值映射的联想记忆模块。研究发现现有序列模型多使用点积相似性或 L2 回归作为注意力偏差。作者提出了一系列替代的注意力偏差配置及其有效近似，并重新解释了遗忘机制，提出了新的遗忘门。基于此，他们提出了 Miras 框架，用于设计基于联想记忆架构、注意力偏差目标、保留门和记忆学习算法选择的深度学习架构，并推出了 Moneta、Yaad、Memora 三种新的序列模型，在保持快速并行训练的同时超越了现有线性 RNN 的能力，在语言建模、常识推理和回忆密集型任务中表现出色。\n\n**LLM 与 RAG**\n\n8.  **InstructRAG: 利用指令图上的检索增强生成进行基于 LLM 的任务规划 (InstructRAG: Leveraging Retrieval-Augmented Generation on Instruction Graphs for LLM-Based Task Planning)**\n    *   现有 LLM 任务规划方法受限于模型知识。该研究提出 InstructRAG，一个多智能体元强化学习框架，利用 RAG 解决任务规划中的可扩展性和可迁移性挑战。它包含一个组织过去指令路径的图、一个用于扩展图覆盖范围的 RL-Agent 和一个用于提高任务泛化能力的 ML-Agent。实验表明 InstructRAG 显著提高了性能并能有效适应新任务。\n\n9.  **存在冲突证据的检索增强生成 (Retrieval-Augmented Generation with Conflicting Evidence)**\n    *   RAG 系统需要处理模糊查询和来自多源的潜在冲突信息。该研究提出了 RAMDocs 数据集，模拟包含歧义、错误信息和噪声的复杂冲突场景，并提出了 MADAM-RAG 方法，让多个 LLM Agent 就答案进行多轮辩论，聚合器整理对应消歧实体的响应，同时丢弃错误信息和噪声。实验证明该方法在处理歧义和抑制错误信息方面优于基线 RAG。\n\n10. **在检索增强 LLM 中容纳知识冲突：迈向可靠的响应生成 (Accommodate Knowledge Conflicts in Retrieval-augmented LLMs: Towards Reliable Response Generation in the Wild)**\n    *   LLM 在 RAG 中常遇到内部记忆与外部检索信息的知识冲突。该研究从信息论角度分析了 LLM 如何处理冲突，发现当冲突/补充信息差异显著时 LLM 能自信决策，但差异模糊时不确定性增加。基于此，提出 Swin-VIB 框架，利用变分信息瓶颈模型来自适应增强检索信息并引导 LLM 偏好，实验验证了其有效性。\n\n11. **ACoRN：检索增强语言模型中抗噪声的抽象压缩 (ACoRN: Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models)**\n    *   抽象压缩利用小模型压缩 RAG 中的相关上下文，但易受检索噪声（不相关或错误信息）影响。该研究提出 ACoRN，通过离线数据增强和围绕关键信息生成摘要的微调，提高压缩器对噪声的鲁棒性，并缓解位置偏差。实验表明 ACoRN 能提高 EM 和 F1 分数，尤其在含较多干扰文档的数据集上表现优异。\n\n12. **通过分层合成数据生成将指令调优 LLM 扩展到百万级 Token 上下文 (Scaling Instruction-Tuned LLMs to Million-Token Contexts via Hierarchical Synthetic Data Generation)**\n    *   LLM 在长上下文推理上面临挑战，部分原因是缺乏长上下文标注数据。该研究提出一种后训练合成数据生成策略，通过逐步 RoPE 缩放训练，有效扩展 LLM 的上下文窗口至 1M Token，同时保持通用任务性能，解决了长上下文数据稀缺问题。\n\n13. **Persona-judge: 通过 Token 级自我判断实现大型语言模型的个性化对齐 (Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment)**\n    *   现有 LLM 个性化对齐方法依赖奖励信号和额外标注，成本高。该研究提出 Persona-judge，一种无需训练的判别式范式。它利用模型的内在偏好判断能力：一个草稿模型根据给定偏好生成候选 Token，一个体现另一种偏好的判断模型交叉验证是否接受该 Token。实验证明该方法可扩展且计算高效。\n\n14. **识别和减轻大型语言模型中先验分布的影响 (Identifying and Mitigating the Influence of the Prior Distribution in Large Language Models)**\n    *   LLM 在处理计数、缩写等确定性任务时，有时会因学到的词元序列先验分布而失败。该研究发现 LLM 内部实际计算了正确执行任务所需的信息。通过提示模型不依赖先验知识，或使用机制可解释性技术定位并微调与先验相关的网络层，可以显著改善模型在这些任务上的性能，表明模型内部表征包含了正确响应所需信息。\n\n15. **GRAIL：用于 LLM 隐私和版权的基于梯度的自适应遗忘 (GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs)**\n    *   针对 LLM 中敏感信息（隐私、版权）的遗忘需求，以及现有单域遗忘方法在多域场景下（知识交织）效果不佳的问题，该研究提出 GRAIL 框架。GRAIL 利用多域梯度信息区分遗忘范围和保留范围，并应用自适应参数级定位策略选择性移除目标知识，同时保留各域关键参数。实验证明 GRAIL 在遗忘成功率上与现有方法相当，但在知识保留上显著优于 SOTA 方法。\n\n16. **SHA256 @ SemEval-2025 Task 4: 选择性遗忘 -- 通过知识隔离实现大型语言模型的约束性遗忘 (SHA256 at SemEval-2025 Task 4: Selective Amnesia -- Constrained Unlearning for Large Language Models via Knowledge Isolation)**\n    *   针对 LLM 记忆敏感信息的问题，该研究提出一种两阶段方法解决 SemEval-2025 的目标遗忘任务。通过因果中介分析和层特定优化，识别出 Transformer 前几层 MLP 模块存储主题-属性关联的关键作用。然后冻结上层，对下层应用联合损失函数（最大化遗忘集损失、最小化保留集偏差），在 1B 模型赛道取得第二名，证明了因果指导的层优化在精确遗忘方面的潜力。\n\n**视觉与多模态**\n\n17. **Complex-Edit: 用于复杂度可控图像编辑基准的类 CoT 指令生成 (Complex-Edit: CoT-Like Instruction Generation for Complexity-Controllable Image Editing Benchmark)**\n    *   该研究提出 Complex-Edit 基准，用于系统评估基于指令的图像编辑模型在不同复杂度指令下的表现。利用 GPT-4o 自动生成原子编辑任务并组合成复杂指令，同时引入一套评估指标和基于 VLM 的自动评估流程。发现开源模型性能远逊于闭源模型，且差距随复杂度增加而扩大；复杂指令主要影响图像元素保留和美学质量；分步执行复杂指令会降低性能；Best-of-N 策略有帮助；观察到“合成数据诅咒”现象。\n\n18. **Science-T2I: 解决图像合成中的科学谬误 (Science-T2I: Addressing Scientific Illusions in Image Synthesis)**\n    *   为将科学知识融入生成模型以提高真实性，该研究提出 Science-T2I 数据集（含 2 万对抗图像对和 9 千提示，覆盖多种科学知识类别）和 SciScore 奖励模型（通过增强 CLIP 的科学理解和视觉能力来评估生成图像的科学性）。基于 SciScore，提出两阶段训练框架（监督微调+掩码在线微调）将科学知识融入现有生成模型。实验证明 SciScore 达到人类水平评估性能，微调方法显著提升了 FLUX 模型的 SciScore。\n\n19. **低幻觉合成字幕用于大规模视觉语言模型预训练 (Low-hallucination Synthetic Captions for Large-Scale Vision-Language Model Pre-training)**\n    *   针对 VLM 预训练依赖高质量图文对但数据稀缺的问题，该研究探索了可扩展的字幕生成技术。提出一种生成高质量、低幻觉、知识丰富的合成字幕流程，其中连续 DPO 方法显著减少了幻觉（7B 模型非幻觉率从 48.2% 提升至 77.9%）。实验证明使用这些合成字幕进行预训练优于真实 alt-text 和先前工作，并在 35 个 VLM 任务和文生图任务上取得显著性能提升。将发布 Hunyuan-Recap100M 数据集。\n\n20. **探索和引导视觉语言模型中的组合创造力 (Probing and Inducing Combinational Creativity in Vision-Language Models)**\n    *   借鉴认知科学中的概念融合理论，该研究从识别输入空间、提取共享属性、推导新语义含义三个层面（IEI 框架）研究 VLM 的组合创造力。构建了 CreativeMashup 数据集（666 个艺术家创作的视觉融合图，按 IEI 标注）。实验表明，在理解任务中，最佳 VLM 超越了普通人但不及专家；在生成任务中，将 IEI 框架融入生成流程显著提升了 VLM 输出的创造性质量。\n\n21. **通过 StyleGAN 实现姿态和面部表情迁移 (Pose and Facial Expression Transfer by using StyleGAN)**\n    *   提出一种在人脸图像间迁移姿态和表情的方法。给定源人脸和目标人脸，模型生成结合源姿态/表情和目标身份的图像。架构包含两个编码器和一个映射网络，将输入投影到 StyleGAN2 潜空间生成输出。训练是自监督的，无需手动标注，可合成具有可控姿态和表情的随机身份，并接近实时性能。\n\n22. **事件增强的模糊视频超分辨率 (Event-Enhanced Blurry Video Super-Resolution)**\n    *   针对模糊视频超分（BVSR）中恢复清晰细节困难的问题，引入事件信号并提出 Ev-DeblurVSR 网络。包含利用帧内事件运动信息去模糊帧特征、同时用帧全局上下文增强事件特征的互惠特征去模糊模块；以及利用帧间事件和光流互补运动信息改进运动估计的混合可变形对齐模块。在合成和真实数据集上取得 SOTA 性能。\n\n23. **CM3AE: 统一的 RGB 帧和事件体素/帧预训练框架 (CM3AE: A Unified RGB Frame and Event-Voxel/-Frame Pre-training Framework)**\n    *   针对事件相机数据预训练与 RGB 帧联系不紧密的问题，提出 CM3AE 预训练框架，接受 RGB 图像、事件图像、事件体素等多模态/视角输入。设计了多模态融合重建模块和多模态对比学习策略，以增强跨模态信息聚合、对齐和理解能力。在大规模 RGB-Event 数据集上预训练，并在五个下游任务验证了有效性。\n\n**机器人与强化学习**\n\n24. **RoboTwin: 具有生成式数字孪生的双臂机器人基准 (RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins)**\n    *   为解决双臂协作和复杂物体操作中高质量演示数据稀缺和缺乏真实世界对齐评估基准的问题，提出 RoboTwin 框架。利用 3D 生成基础模型和 LLM 生成多样化专家数据集，并提供真实世界对齐的双臂机器人任务评估平台。RoboTwin 能从单张 2D 图像创建物体的多样化数字孪生，并引入空间关系感知的代码生成框架，结合物体标注和 LLM 分解任务、确定约束并生成精确运动代码。在 RoboTwin 生成数据上预训练并在少量真实样本上微调的策略，显著提高了双臂操作成功率。\n\n25. **通过强化学习激活基础模型中的具身空间推理的协作框架 (Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning)**\n    *   为让预训练模型获得类似人类从序列视觉观察中进行空间推理的能力，提出 Embodied-R 框架。结合 VLM（感知）和小型 LM（推理），使用 RL 和考虑思维-答案逻辑一致性的新奖励系统进行训练。仅用 5k 具身视频样本训练后，3B LM 的 Embodied-R 在具身空间推理任务上匹敌 SOTA 模型（GPT-4o, Gemini-2.5-pro），并展现出系统分析、上下文整合等涌现思维模式。\n\n26. **跨环境协作实现零样本多智能体协调 (Cross-environment Cooperation Enables Zero-shot Multi-agent Coordination)**\n    *   零样本协调（ZSC）是人机兼容 AI 的关键。该研究发现，在单一伙伴的环境分布上进行强化学习，可以学习通用的协作技能，支持与许多新伙伴在许多新问题上的 ZSC。引入两个程序化生成器创建大量协调挑战，并提出跨环境协作（CEC）范式。实验证明 CEC 在与真人协作时优于基线，表明跨场景协作促使智能体学习通用规范，有效实现与不同伙伴的协作。\n\n27. **QLLM: 多智能体强化学习中的信用分配真的需要混合网络吗？(QLLM: Do We Really Need a Mixing Network for Credit Assignment in Multi-Agent Reinforcement Learning?)**\n    *   针对 MARL 中信用分配的挑战（如价值分解方法的归因不精确、可解释性差、扩展性差），提出 QLLM 算法。利用 LLM 自动构建信用分配函数，引入 TFCAF 概念将分配过程表示为直接的非线性函数公式，并使用定制的“编码器-评估器”框架指导 LLM 生成、验证和优化代码。实验证明 QLLM 优于 SOTA 基线，且具有良好的泛化能力和兼容性。\n\n28. **TraCeS: 基于轨迹的稀疏安全反馈信用分配 (TraCeS: Trajectory Based Credit Assignment From Sparse Safety Feedback)**\n    *   在安全 RL 中，真实安全定义（成本函数、预算）通常未知或难以指定。该研究提出一种从稀疏标记数据（轨迹是否安全）中学习安全定义的方法。设计了一个安全模型，利用轨迹及其二元安全标签的数据集，进行信用分配，估计每个决策步骤对整体安全的影响。该模型能为每个时间步学习单独的安全分数。基于此重新制定安全 RL 问题并推导有效算法。\n\n29. **利用人类一次演示通过 Sim-to-Real RL 跨越人-机器人具身鸿沟 (Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration)**\n    *   为解决机器人灵巧操作示教需要大量演示的问题，提出 Human2Sim2Robot 框架。仅使用一次人类任务演示的 RGB-D 视频，通过模拟环境中的 RL 训练机器人策略。从演示中提取物体姿态轨迹（定义奖励）和预操作手部姿态（初始化和引导 RL 探索），无需任务特定的奖励塑造。实验证明该方法在抓取、非抓取操作和多步任务中优于开环轨迹回放和数据增强的模仿学习。\n\n**其他有趣研究**\n\n30. **可读模型与不可读模型的孪生体 (Readable Twins of Unreadable Models)**\n    *   借鉴物理对象的数字孪生概念，提出为不可读的深度学习模型创建“可读孪生体”（以非精确信息流模型 IIFM 的形式）。给出了从 DLM 转换到 IIFM 的完整流程，并通过 MNIST 手写数字识别的 DL 分类模型进行了示例说明，旨在提高 AI 系统的可解释性。\n\n31. **LLM 遇见金融：为 Open FinLLM 排行榜微调基础模型 (LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM Leaderboard)**\n    *   研究 LLM 在金融任务中的应用。使用 Open FinLLM 排行榜作为基准，对 Qwen2.5 和 Deepseek-R1 等基础模型进行微调（SFT, DPO, RL）。微调后的模型在多种金融任务上表现显著提升，并测量了金融领域的数据扩展定律。\n\n32. **FreshStack: 构建评估技术文档检索的现实基准 (FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on Technical Documents)**\n    *   提出 FreshStack 框架，用于从社区问答自动构建信息检索（IR）评估基准。包括自动语料库收集、从问答生成 nugget（信息点）、使用融合检索技术进行 nugget 级支持。构建了五个关于新兴技术主题的数据集，发现现有检索模型在这些数据集上表现不佳，表明有很大改进空间。\n\n33. **面向可靠响应生成的检索增强 LLM 知识冲突容纳机制 (Accommodate Knowledge Conflicts in Retrieval-augmented LLMs: Towards Reliable Response Generation in the Wild)**\n    *   研究 LLM 在 RAG 中如何处理内部知识与外部检索信息的冲突。从信息论角度分析，发现当信息差异显著时 LLM 能自信决策，差异模糊时则不确定性增加。提出 Swin-VIB 框架，利用变分信息瓶颈模型自适应增强检索信息并引导 LLM 偏好，实验证明有效性。\n\n34. **用于 LLM 服务中任意低精度 GPGPU 计算的虚拟机 (A Virtual Machine for Arbitrary Low-Precision GPGPU Computation in LLM Serving)**\n    *   为解决 LLM 服务中低精度计算的需求和现有方法的局限（仅支持 2 的幂次位宽、性能次优），提出一种用于 GPGPU 计算的虚拟机（VM）。支持任意位宽的低精度数据类型，具有线程块级编程模型、分层内存空间、新颖的代数布局系统。VM 程序可编译为高效 GPU 程序，性能优于 Triton、Ladder、QuantLLM、Marlin 等现有方法。\n\n35. **后处理改进人工智能天气预报的准确性 (Post-processing improves accuracy of Artificial Intelligence weather forecasts)**\n    *   AI 天气模型已达业务级性能，但仍存在系统偏差。该研究测试了澳大利亚气象局现有统计后处理系统 IMPROVER 应用于 ECMWF 的 AI 预报系统（AIFS）的效果。结果表明，无需修改配置，后处理对 AIFS 的准确性提升与对传统 NWP 模型的提升相当，且将 AIFS 与 NWP 模型融合能进一步提高整体预报技巧。\n\n---\n\n**快速浏览**\n\n*   **#10 (SMC 控制 LLM):** 开发基于序贯蒙特卡洛 (SMC) 的架构，用于在推理时灵活施加句法或语义约束，控制 LLM 生成，在代码生成、Text-to-SQL 等任务上优于更大模型。\n*   **#17 (水声目标识别):** 提出多任务平衡通道注意力 CNN (MT-BCA-CNN)，用于解决少样本水声目标识别中的样本稀缺和环境干扰问题。\n*   **#18 (自监督学习理论):** 认为实证驱动的可识别性理论（SITh）将加速自监督学习研究，弥合理论与实践差距，需关注训练动态、有限样本和归纳偏置。\n*   **#20 (虚拟试衣/试脱):** 提出 TryOffDiff，一种基于扩散的虚拟试脱 (VTOFF) 模型，可从穿着者照片中提取标准化服装图像，并首次实现多服装 VTOFF。与 VTON 结合可改进 p2p-VTON。\n*   **#21 (碰撞叙述分类):** 评估 DL 模型和 LLM 在碰撞叙述分类任务中的准确性与专家一致性，发现高准确性模型与专家一致性可能更低，而 LLM 表现出更好的一致性。强调评估需结合专家一致性。\n*   **#23 (基于方面摘要):** 提出自方面检索增强摘要生成框架，通过嵌入驱动的检索识别方面相关文本段，缓解 Token 限制和幻觉问题。\n*   **#24 (拓扑材料设计):** 应用强化微调 (ReFT) 于预训练生成模型，以设计新的拓扑绝缘体 (TI) 和拓扑晶体绝缘体 (TCI)，成功识别了大量新材料。\n*   **#26 (心脏 MRI 基础模型):** 提出 ViTa，一个迈向心脏 MRI 基础模型的框架，整合 3D+T 电影序列和表格化患者因素，实现对心脏健康的全面评估和个体化疾病风险解读。\n*   **#27 (部分相关视频检索):** 提出原型 PRVR 框架，将视频多样化上下文编码为固定数量的原型，通过跨/单模态重建和视频混合技术平衡准确性与效率。\n*   **#30 (SemEval 遗忘任务):** 结合因果中介分析和层特定优化进行 LLM 目标遗忘，识别关键层并应用联合损失，在 1B 模型赛道获第二。\n*   **#33 (海德格尔与 IT 查询):** 提出基于海德格尔本体论的 IT 系统分析方法，区分存在物和存在，使用范畴语言处理用户输入，存在语言进行内部本体分析，以揭示查询处理中的深层模式。\n*   **#34 (NAS 代理模型):** 研究用于表达性强的基于上下文无关文法的 NAS 搜索空间的代理模型训练，发现代理模型具有良好的跨数据集预测能力，可加速搜索。\n*   **#35 (MARL 信用分配):** 提出 QLLM，利用 LLM 自动构建信用分配函数，无需传统混合网络，在 MARL 基准上表现优异。\n*   **#36 (LLM 重试):** 提出“无反馈重试”机制，允许 LLM 在识别答案错误时重试，无需明确的自我反思或语言反馈，发现这种简单方法常优于复杂推理框架。\n*   **#37 (多国价值对齐基准):** 提出 NaVAB 基准，评估 LLM 与中、美、英、法、德五国价值观的对齐程度，包含价值提取流程和评估数据集。\n*   **#38 (LLM 自主去偏):** 提出信息增益引导的因果干预去偏 (IGCIDB) 框架，自动平衡指令调优数据集分布，然后进行标准 SFT，以提高 LLM 泛化性。\n*   **#39 (AI Agent 与机器翻译):** 探讨单/多智能体系统在机器翻译中的潜力，认为多智能体系统有望解决复杂场景，并进行了法律领域 MT 的初步研究。\n*   **#40 (情感 TTS):** 提出 EmoVoice，基于 LLM 的情感 TTS 模型，支持自由风格自然语言情感控制，并引入 EmoVoice-DB 数据集。\n*   **#41 (3D 表面异常合成):** 提出 3D-PNAS 方法，基于 Perlin 噪声和表面参数化生成逼真的 3D 表面异常，用于工业缺陷检测。\n*   **#42 (时间序列分类 Python 包):** 介绍 ALT Python 包，实现自适应律变换算法，用于高效准确的时间序列分类。\n*   **#43 (图像编辑专家):** 提出基于 RLAIF 的在线强化学习框架，训练专门的指令驱动图像编辑扩散模型，无需大量人工标注，改善结构保持和语义对齐。\n*   **#44 (场景理解 GNN):** 结合 GNN 和定性可解释图 (QXG) 进行自动驾驶场景理解，识别交通场景中的相关对象。\n*   **#45 (宫颈涂片分割):** 提出混合 Dense-UNet201 和蜘蛛猴优化 (SMO) 的方法优化宫颈涂片图像分割。\n*   **#46 (VQNN 梯度反演攻击):** 提出一种数值方案，通过梯度反演成功从可训练 VQNN 的梯度中重建输入训练数据。\n*   **#47 (PSO 可解释性):** 分析粒子群优化 (PSO) 中不同通信拓扑（环形、星形、冯诺依曼）对收敛和搜索行为的影响，以增强可解释性和可靠性。\n*   **#48 (文本生成图像安全):** 提出 ANT 框架，通过自动引导去噪轨迹避开不需要的概念，实现更有效的概念擦除，无需启发式锚点选择。\n*   **#49 (晚期交互检索模型剪枝):** 提出原则性方法，在不影响检索分数的情况下对 ColBERT 等晚期交互模型的 Token 进行剪枝，引入正则化损失和剪枝策略。\n*   **#50 (MARL 与环境政策):** 提出将多智能体强化学习 (MARL) 与气候模拟相结合的框架，用于环境政策综合，应对不确定性、复杂动态和利益冲突。\n*   **#51 (多模态几何解题):** 提出 GeoGen 流程自动生成几何图的逐步推理路径，并训练 GeoLogic 模型作为自然语言与符号系统的桥梁，利用符号工具验证 MLLM 输出，提升几何问题求解能力。\n*   **#52 (MCP 安全层):** 提出 MCP Guardian 框架，为基于模型上下文协议 (MCP) 的 AI 系统增加安全层，包括认证、限速、日志、追踪和 WAF 扫描。\n*   **#53 (LLM 轨迹自适应):** 利用预训练 LLM 通过生成代码策略来调整机器人轨迹（来自规划器或示教），以适应更复杂灵活的人类指令。\n*   **#54 (多标签特征选择):** 提出 GPMFS 方法，结合全局特征识别和个性化特征补充，用于多标签特征选择。\n*   **#55 (多智能体系统架构):** 提出“雅典学院”七层架构模型，用于系统化解决多智能体系统在 AI 艺术创作等领域的挑战。\n*   **#56 (统一结构化知识推理):** 提出 Pandora 框架，利用 Python Pandas API 构建统一知识表示，让 LLM 生成推理步骤和 Python 代码，统一处理表格、数据库、知识图谱等结构化知识源的问答。\n*   **#57 (模拟用户评估推荐系统):** 提出 SimUSER 代理框架，利用 LLM 模拟具有个性、记忆、感知等模块的用户行为，用于评估推荐系统。\n*   **#58 (长时序预测):** 提出 TimeCapsule 模型，将时序建模为 3D 张量，利用模式乘积压缩高维信息并捕获多模式依赖，结合 JEPA 监控预测表示学习，用于长时序预测。\n*   **#59 (组织学图像无监督分割):** 提出 TUMLS，一种可信的全无监督多级分割方法，用于组织学全玻片图像，结合 AE 特征提取和基于不确定性的无监督核分割。\n*   **#60 (VLM 模态对齐):** 提出 CLIP-Refine，一种 CLIP 模型的后预训练方法，通过随机特征对齐 (RaFA) 和混合对比蒸馏 (HyCD) 在小数据集上训练 1 轮，以对齐特征空间，缓解模态差距，且不降低零样本性能。\n*   **#61 (多智能体协调):** 提出跨环境协作 (CEC) 范式，通过在多变环境中与单一伙伴训练，学习通用协作技能，实现与新伙伴在未知任务上的零样本协调。\n*   **#62 (NTIRE 2025 雨滴去除挑战):** 报告了 NTIRE 2025 双焦点图像昼夜雨滴去除挑战的方法和结果。\n*   **#63 (WebLists 基准):** 提出 WebLists 基准，包含 200 个需要导航、配置和提取结构化数据的网页任务，并提出 BardeenAgent 框架，利用 LLM 将执行转换为可重复程序，显著优于现有 Web Agent。\n*   **#64 (GRAIL 多域遗忘):** 提出 GRAIL 框架，利用多域梯度信息进行自适应参数级定位，实现 LLM 中隐私、版权等多领域知识的精确遗忘，同时保留其他知识。\n*   **#65 (具身空间推理):** 提出 Embodied-R 框架，结合 VLM 和 LM，通过 RL 训练具身空间推理能力，在有限计算资源下达到 SOTA 水平。\n*   **#66 (ACoRN 噪声鲁棒压缩):** 提出 ACoRN，通过数据增强和微调提高 RAG 中抽象压缩器对检索噪声的鲁棒性。\n*   **#67 (AI 天气预报后处理):** 表明现有用于 NWP 的统计后处理方法可直接应用于 AI 天气模型（如 AIFS），有效提高预报准确性。\n*   **#68 (Persona-judge 个性化对齐):** 提出 Persona-judge，一种无需训练的判别式方法，利用模型内在偏好判断能力实现 LLM 的个性化对齐。\n*   **#69 (量子计算与 AV 感知):** 提出混合经典-量子深度学习 (HCQ-DL) 模型用于交通标志分类，在对抗攻击下比经典 DL 模型更鲁棒。\n*   **#70 (LLM 百万 Token 上下文):** 提出分层合成数据生成策略和 RoPE 缩放训练，将 LLM 上下文扩展至 1M Token。\n*   **#71 (多智能体溯源):** 提出基于符号编年史的系统，用于事后追踪多智能体生成内容的历史，无需内部状态或元信息。\n*   **#72 (人-机器人示教学习):** 提出 Human2Sim2Robot 框架，仅用一次人类演示视频，通过 Sim-to-Real RL 训练机器人灵巧操作策略。\n*   **#73 (代码生成重复问题):** 对 19 个代码 LLM 的重复生成问题进行实证研究，总结了 20 种重复模式，并提出 DeRep 技术检测和缓解重复。\n*   **#74 (鲁棒场景图生成):** 提出 Robo-SGG，利用领域不变的布局信息，通过实例归一化和布局导向恢复，增强 SGG 在受损图像上的鲁棒性。\n*   **#75 (LLM 先验分布影响):** 分析并提出干预方法（提示、微调特定层）来减轻 LLM 中先验分布对确定性任务性能的负面影响。\n*   **#76 (联邦学习数据量感知):** 提出 FedDua，一种安全的联邦学习加权平均方法，服务器可根据客户端上传的梯度预测其数据量，应对不诚实客户端。\n*   **#77 (RGB-事件预训练):** 提出 CM3AE 统一框架，用于 RGB 帧和事件数据（体素/帧）的预训练，支持事件和 RGB-事件融合任务。\n*   **#78 (MetaSynth 合成数据):** 提出 MetaSynth，通过元提示驱动多智能体协作生成多样化合成数据，用于 LLM 领域自适应。\n*   **#79 (ZeroSumEval LLM 评估):** 提出 ZeroSumEval，一种基于零和博弈竞赛的 LLM 评估协议，使用动态基准评估战略推理、知识应用等能力。\n*   **#80 (安全 RL 信用分配):** 提出 TraCeS，从稀疏二元安全标签中学习安全模型，并进行基于轨迹的信用分配，用于满足未知安全约束的 RL。\n*   **#81 (手术室流程分析):** 提出利用数字孪生 (DT) 进行隐私保护的手术室工作流程分析和事件检测，通过分割和深度图代替 RGB 视频。\n*   **#82 (LLM 记忆提取):** 研究从 Llama 3 模型中提取整本书的可能性，发现提取率与书籍流行度（可能对应训练数据重复度）相关，并确认了 Llama 3.1 中对策被撤销的现象。\n*   **#83 (匿名公共宣告):** 形式化了匿名公共宣告的概念及其逻辑，区分了无意图假设和意图保持匿名假设下的情况。\n*   **#84 (LLM 用于枪击事件知识获取):** 利用 LLM（特别是 GPT-4o）进行少样本提示，从大规模枪击事件文本中自动提取关键实体（罪犯、受害者等），构建知识库支持司法调查。",
  "papers": [
    {
      "arxiv_id": "2504.13180v1",
      "title": "PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding",
      "title_zh": "PerceptionLM：用于详细视觉理解的开放数据和模型\n",
      "authors": [
        "Jang Hyun Cho",
        "Andrea Madotto",
        "Effrosyni Mavroudi",
        "Triantafyllos Afouras",
        "Tushar Nagarajan",
        "Muhammad Maaz",
        "Yale Song",
        "Tengyu Ma",
        "Shuming Hu",
        "Suyog Jain",
        "Miguel Martin",
        "Huiyu Wang",
        "Hanoona Rasheed",
        "Peize Sun",
        "Po-Yao Huang",
        "Daniel Bolya",
        "Nikhila Ravi",
        "Shashank Jain",
        "Tammy Stark",
        "Shane Moon",
        "Babak Damavandi",
        "Vivian Lee",
        "Andrew Westbury",
        "Salman Khan",
        "Philipp Krähenbühl",
        "Piotr Dollár",
        "Lorenzo Torresani",
        "Kristen Grauman",
        "Christoph Feichtenhofer"
      ],
      "abstract": "Vision-language models are integral to computer vision research, yet many\nhigh-performing models remain closed-source, obscuring their data, design and\ntraining recipe. The research community has responded by using distillation\nfrom black-box models to label training data, achieving strong benchmark\nresults, at the cost of measurable scientific progress. However, without\nknowing the details of the teacher model and its data sources, scientific\nprogress remains difficult to measure. In this paper, we study building a\nPerception Language Model (PLM) in a fully open and reproducible framework for\ntransparent research in image and video understanding. We analyze standard\ntraining pipelines without distillation from proprietary models and explore\nlarge-scale synthetic data to identify critical data gaps, particularly in\ndetailed video understanding. To bridge these gaps, we release 2.8M\nhuman-labeled instances of fine-grained video question-answer pairs and\nspatio-temporally grounded video captions. Additionally, we introduce\nPLM-VideoBench, a suite for evaluating challenging video understanding tasks\nfocusing on the ability to reason about \"what\", \"where\", \"when\", and \"how\" of a\nvideo. We make our work fully reproducible by providing data, training recipes,\ncode & models.",
      "tldr_zh": "该论文旨在通过构建一个完全开放和可复现的感知语言模型(Perception Language Model, PLM)框架，促进图像和视频理解领域的透明研究。研究分析了不依赖专有模型蒸馏的标准训练流程，并探索了大规模合成数据以识别关键数据缺口，尤其是在详细的视频理解方面。为了弥补这些差距，研究发布了280万人工标注的细粒度视频问答对和时空定位的视频字幕。此外，论文还引入了PLM-VideoBench，用于评估具有挑战性的视频理解任务，侧重于推理视频的“什么”、“哪里”、“何时”和“如何”。研究提供了数据、训练方法、代码和模型，以确保其工作的完全可复现性。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Technical report",
      "pdf_url": "http://arxiv.org/pdf/2504.13180v1",
      "published_date": "2025-04-17 17:59:56 UTC",
      "updated_date": "2025-04-17 17:59:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:00:31.544136"
    },
    {
      "arxiv_id": "2504.13173v1",
      "title": "It's All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization",
      "title_zh": "一切皆有关联：一次关于测试时记忆、注意力偏见、保留和在线优化的旅程\n",
      "authors": [
        "Ali Behrouz",
        "Meisam Razaviyayn",
        "Peilin Zhong",
        "Vahab Mirrokni"
      ],
      "abstract": "Designing efficient and effective architectural backbones has been in the\ncore of research efforts to enhance the capability of foundation models.\nInspired by the human cognitive phenomenon of attentional bias-the natural\ntendency to prioritize certain events or stimuli-we reconceptualize neural\narchitectures, including Transformers, Titans, and modern linear recurrent\nneural networks as associative memory modules that learn a mapping of keys and\nvalues using an internal objective, referred to as attentional bias.\nSurprisingly, we observed that most existing sequence models leverage either\n(1) dot-product similarity, or (2) L2 regression objectives as their\nattentional bias. Going beyond these objectives, we present a set of\nalternative attentional bias configurations along with their effective\napproximations to stabilize their training procedure. We then reinterpret\nforgetting mechanisms in modern deep learning architectures as a form of\nretention regularization, providing a novel set of forget gates for sequence\nmodels. Building upon these insights, we present Miras, a general framework to\ndesign deep learning architectures based on four choices of: (i) associative\nmemory architecture, (ii) attentional bias objective, (iii) retention gate, and\n(iv) memory learning algorithm. We present three novel sequence models-Moneta,\nYaad, and Memora-that go beyond the power of existing linear RNNs while\nmaintaining a fast parallelizable training process. Our experiments show\ndifferent design choices in Miras yield models with varying strengths. For\nexample, certain instances of Miras achieve exceptional performance in special\ntasks such as language modeling, commonsense reasoning, and recall intensive\ntasks, even outperforming Transformers and other modern linear recurrent\nmodels.",
      "tldr_zh": "该论文将包括Transformer、Titan和线性循环神经网络在内的神经架构重新概念化为联想记忆模块，这些模块使用内部目标（即注意力偏差）学习键值映射。研究发现现有序列模型主要使用点积相似度或L2回归作为注意力偏差。论文提出了一系列替代的注意力偏差配置及其有效近似，以稳定训练过程，并将深度学习架构中的遗忘机制重新解释为一种保留正则化。基于这些，论文提出了Miras框架，用于设计深度学习架构，并提出了三种新型序列模型Moneta、Yaad和Memora。实验表明，Miras框架下的不同设计选择使模型在语言建模、常识推理和召回密集型任务中表现出不同的优势，甚至优于Transformer和其他线性循环模型。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13173v1",
      "published_date": "2025-04-17 17:59:33 UTC",
      "updated_date": "2025-04-17 17:59:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:00:43.643720"
    },
    {
      "arxiv_id": "2504.13171v1",
      "title": "Sleep-time Compute: Beyond Inference Scaling at Test-time",
      "title_zh": "睡眠时间计算：超越测试时推理扩展\n",
      "authors": [
        "Kevin Lin",
        "Charlie Snell",
        "Yu Wang",
        "Charles Packer",
        "Sarah Wooders",
        "Ion Stoica",
        "Joseph E. Gonzalez"
      ],
      "abstract": "Scaling test-time compute has emerged as a key ingredient for enabling large\nlanguage models (LLMs) to solve difficult problems, but comes with high latency\nand inference cost. We introduce sleep-time compute, which allows models to\n\"think\" offline about contexts before queries are presented: by anticipating\nwhat queries users might ask and pre-computing useful quantities, we can\nsignificantly reduce the compute requirements at test-time. To demonstrate the\nefficacy of our method, we create modified versions of two reasoning tasks -\nStateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can\nreduce the amount of test-time compute needed to achieve the same accuracy by ~\n5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time\ncompute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic\nand 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic,\nwhich extends GSM-Symbolic by including multiple related queries per context.\nBy amortizing sleep-time compute across related queries about the same context\nusing Multi-Query GSM-Symbolic, we can decrease the average cost per query by\n2.5x. We then conduct additional analysis to understand when sleep-time compute\nis most effective, finding the predictability of the user query to be well\ncorrelated with the efficacy of sleep-time compute. Finally, we conduct a\ncase-study of applying sleep-time compute to a realistic agentic SWE task.",
      "tldr_zh": "该论文提出了一种名为“睡眠时间计算 (sleep-time compute)”的新方法，旨在减少大型语言模型 (LLM) 在测试时推理所需的计算量。该方法通过在接收查询之前离线“思考”上下文，预先计算有用的信息，从而降低测试时的计算需求。在Stateful GSM-Symbolic和Stateful AIME两个推理任务上的实验表明，睡眠时间计算可以将达到相同精度所需的测试时计算量减少约5倍，并且通过扩展睡眠时间计算，可以进一步提高准确率。此外，论文还引入了Multi-Query GSM-Symbolic，通过分摊睡眠时间计算，将每个查询的平均成本降低了2.5倍。最后，通过案例研究表明，睡眠时间计算在用户查询可预测性高的情况下最为有效。\n",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Code and data released at:\n  https://github.com/letta-ai/sleep-time-compute",
      "pdf_url": "http://arxiv.org/pdf/2504.13171v1",
      "published_date": "2025-04-17 17:59:25 UTC",
      "updated_date": "2025-04-17 17:59:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:00:55.687492"
    },
    {
      "arxiv_id": "2504.13165v1",
      "title": "RUKA: Rethinking the Design of Humanoid Hands with Learning",
      "title_zh": "RUKA：通过学习重新思考人形手的设计\n",
      "authors": [
        "Anya Zorin",
        "Irmak Guzey",
        "Billy Yan",
        "Aadhithya Iyer",
        "Lisa Kondrich",
        "Nikhil X. Bhattasali",
        "Lerrel Pinto"
      ],
      "abstract": "Dexterous manipulation is a fundamental capability for robotic systems, yet\nprogress has been limited by hardware trade-offs between precision,\ncompactness, strength, and affordability. Existing control methods impose\ncompromises on hand designs and applications. However, learning-based\napproaches present opportunities to rethink these trade-offs, particularly to\naddress challenges with tendon-driven actuation and low-cost materials. This\nwork presents RUKA, a tendon-driven humanoid hand that is compact, affordable,\nand capable. Made from 3D-printed parts and off-the-shelf components, RUKA has\n5 fingers with 15 underactuated degrees of freedom enabling diverse human-like\ngrasps. Its tendon-driven actuation allows powerful grasping in a compact,\nhuman-sized form factor. To address control challenges, we learn\njoint-to-actuator and fingertip-to-actuator models from motion-capture data\ncollected by the MANUS glove, leveraging the hand's morphological accuracy.\nExtensive evaluations demonstrate RUKA's superior reachability, durability, and\nstrength compared to other robotic hands. Teleoperation tasks further showcase\nRUKA's dexterous movements. The open-source design and assembly instructions of\nRUKA, code, and data are available at https://ruka-hand.github.io/.",
      "tldr_zh": "该论文介绍了RUKA，一种基于学习方法重新设计的仿人手。RUKA采用腱驱动，结构紧凑、成本低廉且性能强大，由3D打印部件和现成组件构成，具有5根手指和15个欠驱动自由度，能够实现多样化的人类抓取动作。为了解决控制难题，研究人员利用MANUS手套收集的动作捕捉数据，学习关节到执行器以及指尖到执行器的模型，充分利用了RUKA的形态精度。实验结果表明，RUKA在可达性、耐用性和强度方面优于其他机器人手。该设计已开源。\n",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Website at https://ruka-hand.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2504.13165v1",
      "published_date": "2025-04-17 17:58:59 UTC",
      "updated_date": "2025-04-17 17:58:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:01:07.690967"
    },
    {
      "arxiv_id": "2504.13151v1",
      "title": "MIB: A Mechanistic Interpretability Benchmark",
      "title_zh": "MIB：一个可解释性的基准测试",
      "authors": [
        "Aaron Mueller",
        "Atticus Geiger",
        "Sarah Wiegreffe",
        "Dana Arad",
        "Iván Arcuschin",
        "Adam Belfki",
        "Yik Siu Chan",
        "Jaden Fiotto-Kaufman",
        "Tal Haklay",
        "Michael Hanna",
        "Jing Huang",
        "Rohan Gupta",
        "Yaniv Nikankin",
        "Hadas Orgad",
        "Nikhil Prakash",
        "Anja Reusch",
        "Aruna Sankaranarayanan",
        "Shun Shao",
        "Alessandro Stolfo",
        "Martin Tutek",
        "Amir Zur",
        "David Bau",
        "Yonatan Belinkov"
      ],
      "abstract": "How can we know whether new mechanistic interpretability methods achieve real\nimprovements? In pursuit of meaningful and lasting evaluation standards, we\npropose MIB, a benchmark with two tracks spanning four tasks and five models.\nMIB favors methods that precisely and concisely recover relevant causal\npathways or specific causal variables in neural language models. The circuit\nlocalization track compares methods that locate the model components - and\nconnections between them - most important for performing a task (e.g.,\nattribution patching or information flow routes). The causal variable\nlocalization track compares methods that featurize a hidden vector, e.g.,\nsparse autoencoders (SAEs) or distributed alignment search (DAS), and locate\nmodel features for a causal variable relevant to the task. Using MIB, we find\nthat attribution and mask optimization methods perform best on circuit\nlocalization. For causal variable localization, we find that the supervised DAS\nmethod performs best, while SAE features are not better than neurons, i.e.,\nstandard dimensions of hidden vectors. These findings illustrate that MIB\nenables meaningful comparisons of methods, and increases our confidence that\nthere has been real progress in the field.",
      "tldr_zh": "该论文提出了MIB，一个用于评估机制可解释性方法的新基准，包含两个track，涵盖四个任务和五个模型。MIB旨在评估方法能否精确简洁地恢复神经语言模型中相关的因果路径或特定的因果变量。其中，电路定位(circuit localization) track比较了定位模型组件（及其连接）的方法，这些组件对于执行任务至关重要。因果变量定位(causal variable localization) track比较了对隐藏向量进行特征化的方法，并定位与任务相关的因果变量的模型特征。实验结果表明，归因和掩码优化方法在电路定位上表现最佳，而监督DAS方法在因果变量定位上表现最佳，SAE特征并不优于标准神经元。MIB的提出能够对不同方法进行有意义的比较，并增强我们对该领域取得真正进展的信心。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13151v1",
      "published_date": "2025-04-17 17:55:45 UTC",
      "updated_date": "2025-04-17 17:55:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:01:19.737776"
    },
    {
      "arxiv_id": "2504.13150v1",
      "title": "Readable Twins of Unreadable Models",
      "title_zh": "不可读模型的“可读孪生体”\n",
      "authors": [
        "Krzysztof Pancerz",
        "Piotr Kulicki",
        "Michał Kalisz",
        "Andrzej Burda",
        "Maciej Stanisławski",
        "Jaromir Sarzyński"
      ],
      "abstract": "Creating responsible artificial intelligence (AI) systems is an important\nissue in contemporary research and development of works on AI. One of the\ncharacteristics of responsible AI systems is their explainability. In the\npaper, we are interested in explainable deep learning (XDL) systems. On the\nbasis of the creation of digital twins of physical objects, we introduce the\nidea of creating readable twins (in the form of imprecise information flow\nmodels) for unreadable deep learning models. The complete procedure for\nswitching from the deep learning model (DLM) to the imprecise information flow\nmodel (IIFM) is presented. The proposed approach is illustrated with an example\nof a deep learning classification model for image recognition of handwritten\ndigits from the MNIST data set.",
      "tldr_zh": "本文提出了一种创建“可读孪生模型”的新方法，旨在提高深度学习模型的可解释性。类似于物理对象的数字孪生，该方法为难以理解的深度学习模型(DLM)构建一个对应的、基于不精确信息流模型(IIFM)的可读版本。文章详细描述了从DLM到IIFM的转换过程，并通过MNIST手写数字图像识别的深度学习分类模型进行了案例演示，旨在为负责任的AI系统设计提供一种新的思路。\n",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "Based on the abstract accepted for ISFS 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.13150v1",
      "published_date": "2025-04-17 17:55:34 UTC",
      "updated_date": "2025-04-17 17:55:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:01:31.452346"
    },
    {
      "arxiv_id": "2504.13146v1",
      "title": "Antidistillation Sampling",
      "title_zh": "反提炼采样\n",
      "authors": [
        "Yash Savani",
        "Asher Trockman",
        "Zhili Feng",
        "Avi Schwarzschild",
        "Alexander Robey",
        "Marc Finzi",
        "J. Zico Kolter"
      ],
      "abstract": "Frontier models that generate extended reasoning traces inadvertently produce\nrich token sequences that can facilitate model distillation. Recognizing this\nvulnerability, model owners may seek sampling strategies that limit the\neffectiveness of distillation without compromising model performance.\n\\emph{Antidistillation sampling} provides exactly this capability. By\nstrategically modifying a model's next-token probability distribution,\nantidistillation sampling poisons reasoning traces, rendering them\nsignificantly less effective for distillation while preserving the model's\npractical utility. For further details, see https://antidistillation.com.",
      "tldr_zh": "本文提出了一种名为“Antidistillation sampling”的采样策略，旨在对抗模型蒸馏。该方法通过策略性地修改模型的下一个token概率分布，对推理轨迹进行“投毒”，使其在不影响模型实际效用的前提下，显著降低蒸馏的有效性。换句话说，Antidistillation sampling能够生成不易被用于模型蒸馏的推理过程，从而保护模型所有者的知识产权。 更多细节请参考 https://antidistillation.com。\n",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13146v1",
      "published_date": "2025-04-17 17:54:14 UTC",
      "updated_date": "2025-04-17 17:54:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:01:43.386911"
    },
    {
      "arxiv_id": "2504.13145v1",
      "title": "Exploring Expert Failures Improves LLM Agent Tuning",
      "title_zh": "探索专家失败案例以改进 LLM Agent 的调优\n",
      "authors": [
        "Li-Cheng Lan",
        "Andrew Bai",
        "Minhao Cheng",
        "Ruochen Wang",
        "Cho-Jui Hsieh",
        "Tianyi Zhou"
      ],
      "abstract": "Large Language Models (LLMs) have shown tremendous potential as agents,\nexcelling at tasks that require multiple rounds of reasoning and interactions.\nRejection Sampling Fine-Tuning (RFT) has emerged as an effective method for\nfinetuning LLMs as agents: it first imitates expert-generated successful\ntrajectories and further improves agentic skills through iterative fine-tuning\non successful, self-generated trajectories. However, since the expert (e.g.,\nGPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler\nscenarios, many complex subtasks remain unsolved and persistently\nout-of-distribution (OOD). Upon investigating these challenging subtasks, we\ndiscovered that previously failed expert trajectories can often provide\nvaluable guidance, e.g., plans and key actions, that can significantly improve\nagent exploration efficiency and acquisition of critical skills. Motivated by\nthese observations, we propose Exploring Expert Failures (EEF), which\nidentifies beneficial actions from failed expert trajectories and integrates\nthem into the training dataset. Potentially harmful actions are meticulously\nexcluded to prevent contamination of the model learning process. By leveraging\nthe beneficial actions in expert failures, EEF successfully solves some\npreviously unsolvable subtasks and improves agent tuning performance.\nRemarkably, our approach achieved a 62\\% win rate in WebShop, outperforming RFT\n(53. 6\\%) and GPT-4 (35. 6\\%), and to the best of our knowledge, setting a new\nstate-of-the-art as the first method to surpass a score of 0.81 in WebShop and\nexceed 81 in SciWorld.",
      "tldr_zh": "该研究针对大型语言模型(LLMs)作为智能体在复杂任务中表现不佳的问题，提出了Exploring Expert Failures (EEF)方法。EEF通过分析专家（如GPT-4）失败的轨迹，提取有益的行动并整合到训练数据集中，同时排除潜在有害的行动，从而提升智能体的探索效率和关键技能的获取。实验表明，EEF在WebShop和SciWorld等任务中显著优于Rejection Sampling Fine-Tuning (RFT)和GPT-4，并在WebShop上取得了62%的胜率，创造了新的state-of-the-art。该方法证明了利用专家失败经验可以有效提升LLM智能体的性能。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13145v1",
      "published_date": "2025-04-17 17:53:54 UTC",
      "updated_date": "2025-04-17 17:53:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:01:55.692127"
    },
    {
      "arxiv_id": "2504.13143v1",
      "title": "$\\texttt{Complex-Edit}$: CoT-Like Instruction Generation for Complexity-Controllable Image Editing Benchmark",
      "title_zh": "$\\texttt{Complex-Edit}$：用于复杂度可控图像编辑基准测试的类 CoT 指令生成方法\n",
      "authors": [
        "Siwei Yang",
        "Mude Hui",
        "Bingchen Zhao",
        "Yuyin Zhou",
        "Nataniel Ruiz",
        "Cihang Xie"
      ],
      "abstract": "We introduce $\\texttt{Complex-Edit}$, a comprehensive benchmark designed to\nsystematically evaluate instruction-based image editing models across\ninstructions of varying complexity. To develop this benchmark, we harness\nGPT-4o to automatically collect a diverse set of editing instructions at scale.\nOur approach follows a well-structured ``Chain-of-Edit'' pipeline: we first\ngenerate individual atomic editing tasks independently and then integrate them\nto form cohesive, complex instructions. Additionally, we introduce a suite of\nmetrics to assess various aspects of editing performance, along with a\nVLM-based auto-evaluation pipeline that supports large-scale assessments. Our\nbenchmark yields several notable insights: 1) Open-source models significantly\nunderperform relative to proprietary, closed-source models, with the\nperformance gap widening as instruction complexity increases; 2) Increased\ninstructional complexity primarily impairs the models' ability to retain key\nelements from the input images and to preserve the overall aesthetic quality;\n3) Decomposing a complex instruction into a sequence of atomic steps, executed\nin a step-by-step manner, substantially degrades performance across multiple\nmetrics; 4) A straightforward Best-of-N selection strategy improves results for\nboth direct editing and the step-by-step sequential approach; and 5) We observe\na ``curse of synthetic data'': when synthetic data is involved in model\ntraining, the edited images from such models tend to appear increasingly\nsynthetic as the complexity of the editing instructions rises -- a phenomenon\nthat intriguingly also manifests in the latest GPT-4o outputs.",
      "tldr_zh": "该论文提出了一个名为$\\texttt{Complex-Edit}$的图像编辑基准，用于系统评估基于指令的图像编辑模型在不同复杂程度指令下的表现。该基准利用GPT-4o自动生成大规模、多样化的编辑指令，采用“Chain-of-Edit”流程，先独立生成原子编辑任务，再整合形成复杂的指令。同时，论文引入了一系列评估指标和一个基于VLM的自动评估流程。实验结果表明，开源模型在复杂指令下表现明显逊于闭源模型，且模型在保留关键元素和保持美学质量方面的能力会因指令复杂性增加而下降。此外，分解复杂指令为一系列原子步骤会降低性能，并且观察到模型在合成数据训练中存在“合成数据诅咒”现象，即编辑后的图像随着指令复杂性的增加而显得更加合成化。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://ucsc-vlaa.github.io/Complex-Edit/, Dataset:\n  https://huggingface.co/datasets/UCSC-VLAA/Complex-Edit",
      "pdf_url": "http://arxiv.org/pdf/2504.13143v1",
      "published_date": "2025-04-17 17:51:59 UTC",
      "updated_date": "2025-04-17 17:51:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:02:07.822638"
    },
    {
      "arxiv_id": "2504.13139v1",
      "title": "Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo",
      "title_zh": "通过序列蒙特卡洛方法对大型语言模型进行句法和语义控制\n",
      "authors": [
        "João Loula",
        "Benjamin LeBrun",
        "Li Du",
        "Ben Lipkin",
        "Clemente Pasti",
        "Gabriel Grand",
        "Tianyu Liu",
        "Yahya Emara",
        "Marjorie Freedman",
        "Jason Eisner",
        "Ryan Cotterel",
        "Vikash Mansinghka",
        "Alexander K. Lew",
        "Tim Vieira",
        "Timothy J. O'Donnell"
      ],
      "abstract": "A wide range of LM applications require generating text that conforms to\nsyntactic or semantic constraints. Imposing such constraints can be naturally\nframed as probabilistic conditioning, but exact generation from the resulting\ndistribution -- which can differ substantially from the LM's base distribution\n-- is generally intractable. In this work, we develop an architecture for\ncontrolled LM generation based on sequential Monte Carlo (SMC). Our SMC\nframework allows us to flexibly incorporate domain- and problem-specific\nconstraints at inference time, and efficiently reallocate computational\nresources in light of new information during the course of generation. By\ncomparing to a number of alternatives and ablations on four challenging domains\n-- Python code generation for data science, text-to-SQL, goal inference, and\nmolecule synthesis -- we demonstrate that, with little overhead, our approach\nallows small open-source language models to outperform models over 8x larger,\nas well as closed-source, fine-tuned ones. In support of the probabilistic\nperspective, we show that these performance improvements are driven by better\napproximation to the posterior distribution. Our system builds on the framework\nof Lew et al. (2023) and integrates with its language model probabilistic\nprogramming language, giving users a simple, programmable way to apply SMC to a\nbroad variety of controlled generation problems.",
      "tldr_zh": "该论文提出了一种基于序列蒙特卡洛(Sequential Monte Carlo, SMC)的架构，用于对大型语言模型(Large Language Models, LLMs)进行句法和语义控制的文本生成。该SMC框架能够在推理时灵活地整合领域和问题相关的约束，并根据生成过程中的新信息有效地重新分配计算资源。在Python代码生成、Text-to-SQL、目标推断和分子合成四个具有挑战性的领域中，实验结果表明，该方法能够使小型开源语言模型的性能优于8倍大的模型，以及闭源的微调模型。性能的提升源于对后验分布更好的近似。该系统构建于Lew et al. (2023)的框架之上，并与其语言模型概率编程语言集成，为用户提供了一种简单、可编程的方式，将SMC应用于各种受控生成问题。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "34 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.13139v1",
      "published_date": "2025-04-17 17:49:40 UTC",
      "updated_date": "2025-04-17 17:49:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:02:19.803582"
    },
    {
      "arxiv_id": "2504.13131v1",
      "title": "NTIRE 2025 Challenge on Short-form UGC Video Quality Assessment and Enhancement: Methods and Results",
      "title_zh": "NTIRE 2025 短视频 UGC 视频质量评估与增强挑战赛：方法与结果\n",
      "authors": [
        "Xin Li",
        "Kun Yuan",
        "Bingchen Li",
        "Fengbin Guan",
        "Yizhen Shao",
        "Zihao Yu",
        "Xijun Wang",
        "Yiting Lu",
        "Wei Luo",
        "Suhang Yao",
        "Ming Sun",
        "Chao Zhou",
        "Zhibo Chen",
        "Radu Timofte",
        "Yabin Zhang",
        "Ao-Xiang Zhang",
        "Tianwu Zhi",
        "Jianzhao Liu",
        "Yang Li",
        "Jingwen Xu",
        "Yiting Liao",
        "Yushen Zuo",
        "Mingyang Wu",
        "Renjie Li",
        "Shengyun Zhong",
        "Zhengzhong Tu",
        "Yufan Liu",
        "Xiangguang Chen",
        "Zuowei Cao",
        "Minhao Tang",
        "Shan Liu",
        "Kexin Zhang",
        "Jingfen Xie",
        "Yan Wang",
        "Kai Chen",
        "Shijie Zhao",
        "Yunchen Zhang",
        "Xiangkai Xu",
        "Hong Gao",
        "Ji Shi",
        "Yiming Bao",
        "Xiugang Dong",
        "Xiangsheng Zhou",
        "Yaofeng Tu",
        "Ying Liang",
        "Yiwen Wang",
        "Xinning Chai",
        "Yuxuan Zhang",
        "Zhengxue Cheng",
        "Yingsheng Qin",
        "Yucai Yang",
        "Rong Xie",
        "Li Song",
        "Wei Sun",
        "Kang Fu",
        "Linhan Cao",
        "Dandan Zhu",
        "Kaiwei Zhang",
        "Yucheng Zhu",
        "Zicheng Zhang",
        "Menghan Hu",
        "Xiongkuo Min",
        "Guangtao Zhai",
        "Zhi Jin",
        "Jiawei Wu",
        "Wei Wang",
        "Wenjian Zhang",
        "Yuhai Lan",
        "Gaoxiong Yi",
        "Hengyuan Na",
        "Wang Luo",
        "Di Wu",
        "MingYin Bai",
        "Jiawang Du",
        "Zilong Lu",
        "Zhenyu Jiang",
        "Hui Zeng",
        "Ziguan Cui",
        "Zongliang Gan",
        "Guijin Tang",
        "Xinglin Xie",
        "Kehuan Song",
        "Xiaoqiang Lu",
        "Licheng Jiao",
        "Fang Liu",
        "Xu Liu",
        "Puhua Chen",
        "Ha Thu Nguyen",
        "Katrien De Moor",
        "Seyed Ali Amirshahi",
        "Mohamed-Chaker Larabi",
        "Qi Tang",
        "Linfeng He",
        "Zhiyong Gao",
        "Zixuan Gao",
        "Guohua Zhang",
        "Zhiye Huang",
        "Yi Deng",
        "Qingmiao Jiang",
        "Lu Chen",
        "Yi Yang",
        "Xi Liao",
        "Nourine Mohammed Nadir",
        "Yuxuan Jiang",
        "Qiang Zhu",
        "Siyue Teng",
        "Fan Zhang",
        "Shuyuan Zhu",
        "Bing Zeng",
        "David Bull",
        "Meiqin Liu",
        "Chao Yao",
        "Yao Zhao"
      ],
      "abstract": "This paper presents a review for the NTIRE 2025 Challenge on Short-form UGC\nVideo Quality Assessment and Enhancement. The challenge comprises two tracks:\n(i) Efficient Video Quality Assessment (KVQ), and (ii) Diffusion-based Image\nSuper-Resolution (KwaiSR). Track 1 aims to advance the development of\nlightweight and efficient video quality assessment (VQA) models, with an\nemphasis on eliminating reliance on model ensembles, redundant weights, and\nother computationally expensive components in the previous IQA/VQA\ncompetitions. Track 2 introduces a new short-form UGC dataset tailored for\nsingle image super-resolution, i.e., the KwaiSR dataset. It consists of 1,800\nsynthetically generated S-UGC image pairs and 1,900 real-world S-UGC images,\nwhich are split into training, validation, and test sets using a ratio of\n8:1:1. The primary objective of the challenge is to drive research that\nbenefits the user experience of short-form UGC platforms such as Kwai and\nTikTok. This challenge attracted 266 participants and received 18 valid final\nsubmissions with corresponding fact sheets, significantly contributing to the\nprogress of short-form UGC VQA and image superresolution. The project is\npublicly available at https://github.com/lixinustc/KVQE-\nChallengeCVPR-NTIRE2025.",
      "tldr_zh": "本文回顾了NTIRE 2025短视频UGC质量评估与增强挑战赛，该挑战赛包含两个赛道：(1)高效视频质量评估(KVQ)和(2)基于扩散模型的图像超分辨率(KwaiSR)。KVQ赛道旨在推动轻量级高效视频质量评估(VQA)模型的发展，强调消除对模型集成、冗余权重和其他计算密集型组件的依赖。KwaiSR赛道引入了一个新的短视频UGC数据集，专为单图像超分辨率设计，包含合成和真实世界的图像对。该挑战赛吸引了266名参与者，收到了18份有效提交，显著推动了短视频UGC的VQA和图像超分辨率技术的发展。项目已在GitHub上公开。\n",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Challenge Report of NTIRE 2025; Methods from 18 Teams; Accepted by\n  CVPR Workshop; 21 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.13131v1",
      "published_date": "2025-04-17 17:45:34 UTC",
      "updated_date": "2025-04-17 17:45:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:02:31.776299"
    },
    {
      "arxiv_id": "2504.13129v1",
      "title": "Science-T2I: Addressing Scientific Illusions in Image Synthesis",
      "title_zh": "Science-T2I：解决图像合成中的科学幻觉问题\n",
      "authors": [
        "Jialuo Li",
        "Wenhao Chai",
        "Xingyu Fu",
        "Haiyang Xu",
        "Saining Xie"
      ],
      "abstract": "We present a novel approach to integrating scientific knowledge into\ngenerative models, enhancing their realism and consistency in image synthesis.\nFirst, we introduce Science-T2I, an expert-annotated adversarial dataset\ncomprising adversarial 20k image pairs with 9k prompts, covering wide distinct\nscientific knowledge categories. Leveraging Science-T2I, we present SciScore,\nan end-to-end reward model that refines the assessment of generated images\nbased on scientific knowledge, which is achieved by augmenting both the\nscientific comprehension and visual capabilities of pre-trained CLIP model.\nAdditionally, based on SciScore, we propose a two-stage training framework,\ncomprising a supervised fine-tuning phase and a masked online fine-tuning\nphase, to incorporate scientific knowledge into existing generative models.\nThrough comprehensive experiments, we demonstrate the effectiveness of our\nframework in establishing new standards for evaluating the scientific realism\nof generated content. Specifically, SciScore attains performance comparable to\nhuman-level, demonstrating a 5% improvement similar to evaluations conducted by\nexperienced human evaluators. Furthermore, by applying our proposed fine-tuning\nmethod to FLUX, we achieve a performance enhancement exceeding 50% on SciScore.",
      "tldr_zh": "该论文提出了Science-T2I框架，旨在解决图像合成中存在的科学幻觉问题。首先，作者构建了一个包含2万图像对和9千提示的对抗数据集Science-T2I，涵盖广泛的科学知识类别。然后，提出了SciScore，一个端到端的奖励模型，通过增强预训练CLIP模型的科学理解和视觉能力，来评估生成图像的科学性。此外，基于SciScore，作者提出了一个两阶段训练框架，包括监督微调和掩码在线微调，将科学知识融入现有的生成模型。实验表明，该框架能够有效提高生成内容的科学真实性，SciScore的性能与人类水平相当，并且将该微调方法应用于FLUX模型时，SciScore指标提升超过50%。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CVPR 2025. Code, docs, weight, benchmark and training\n  data are all avaliable at https://jialuo-li.github.io/Science-T2I-Web",
      "pdf_url": "http://arxiv.org/pdf/2504.13129v1",
      "published_date": "2025-04-17 17:44:19 UTC",
      "updated_date": "2025-04-17 17:44:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:02:43.750101"
    },
    {
      "arxiv_id": "2504.13128v1",
      "title": "FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on Technical Documents",
      "title_zh": "FreshStack：构建用于评估技术文档检索的真实基准\n",
      "authors": [
        "Nandan Thakur",
        "Jimmy Lin",
        "Sam Havens",
        "Michael Carbin",
        "Omar Khattab",
        "Andrew Drozdov"
      ],
      "abstract": "We introduce FreshStack, a reusable framework for automatically building\ninformation retrieval (IR) evaluation benchmarks from community-asked questions\nand answers. FreshStack conducts the following steps: (1) automatic corpus\ncollection from code and technical documentation, (2) nugget generation from\ncommunity-asked questions and answers, and (3) nugget-level support, retrieving\ndocuments using a fusion of retrieval techniques and hybrid architectures. We\nuse FreshStack to build five datasets on fast-growing, recent, and niche topics\nto ensure the tasks are sufficiently challenging. On FreshStack, existing\nretrieval models, when applied out-of-the-box, significantly underperform\noracle approaches on all five topics, denoting plenty of headroom to improve IR\nquality. In addition, we identify cases where rerankers do not clearly improve\nfirst-stage retrieval accuracy (two out of five topics). We hope that\nFreshStack will facilitate future work toward constructing realistic, scalable,\nand uncontaminated IR and RAG evaluation benchmarks. FreshStack datasets are\navailable at: https://fresh-stack.github.io.",
      "tldr_zh": "FreshStack是一个用于自动构建信息检索(IR)评估基准的可重用框架，它基于社区提出的问题和答案。该框架包括：(1) 从代码和技术文档中自动收集语料库；(2) 从社区问答中生成nugget（信息片段）；(3) 使用检索技术和混合架构融合检索文档，提供nugget级别的支持。FreshStack构建了五个关于快速增长、新兴和利基主题的数据集，以保证任务的挑战性。实验表明，现有检索模型在FreshStack上的表现远低于oracle方法，表明IR质量仍有提升空间。此外，研究发现reranker在某些情况下未能显著提高一级检索的准确性。FreshStack旨在促进未来构建更真实、可扩展和无污染的IR和RAG评估基准。\n",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13128v1",
      "published_date": "2025-04-17 17:44:06 UTC",
      "updated_date": "2025-04-17 17:44:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:02:55.789801"
    },
    {
      "arxiv_id": "2504.13125v1",
      "title": "LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM Leaderboard",
      "title_zh": "LLM 遇上金融：为 Open FinLLM 排行榜微调基础模型\n",
      "authors": [
        "Varun Rao",
        "Youran Sun",
        "Mahendra Kumar",
        "Tejas Mutneja",
        "Agastya Mukherjee",
        "Haizhao Yang"
      ],
      "abstract": "This paper investigates the application of large language models (LLMs) to\nfinancial tasks. We fine-tuned foundation models using the Open FinLLM\nLeaderboard as a benchmark. Building on Qwen2.5 and Deepseek-R1, we employed\ntechniques including supervised fine-tuning (SFT), direct preference\noptimization (DPO), and reinforcement learning (RL) to enhance their financial\ncapabilities. The fine-tuned models demonstrated substantial performance gains\nacross a wide range of financial tasks. Moreover, we measured the data scaling\nlaw in the financial domain. Our work demonstrates the potential of large\nlanguage models (LLMs) in financial applications.",
      "tldr_zh": "本文研究了大型语言模型(LLMs)在金融任务中的应用。作者使用Open FinLLM Leaderboard作为基准，对Qwen2.5和Deepseek-R1等基础模型进行了微调。通过监督微调(SFT)、直接偏好优化(DPO)和强化学习(RL)等技术，提升了模型在金融领域的性能。实验结果表明，微调后的模型在各种金融任务中表现出显著的性能提升。此外，作者还测量了金融领域的数据缩放定律。这项工作展示了大型语言模型(LLMs)在金融应用中的潜力。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13125v1",
      "published_date": "2025-04-17 17:42:02 UTC",
      "updated_date": "2025-04-17 17:42:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:03:07.471685"
    },
    {
      "arxiv_id": "2504.13123v1",
      "title": "Low-hallucination Synthetic Captions for Large-Scale Vision-Language Model Pre-training",
      "title_zh": "用于大规模视觉-语言模型预训练的低幻觉合成字幕\n",
      "authors": [
        "Xinsong Zhang",
        "Yarong Zeng",
        "Xinting Huang",
        "Hu Hu",
        "Runquan Xie",
        "Han Hu",
        "Zhanhui Kang"
      ],
      "abstract": "In recent years, the field of vision-language model pre-training has\nexperienced rapid advancements, driven primarily by the continuous enhancement\nof textual capabilities in large language models. However, existing training\nparadigms for multimodal large language models heavily rely on high-quality\nimage-text pairs. As models and data scales grow exponentially, the\navailability of such meticulously curated data has become increasingly scarce\nand saturated, thereby severely limiting further advancements in this domain.\nThis study investigates scalable caption generation techniques for\nvision-language model pre-training and demonstrates that large-scale\nlow-hallucination synthetic captions can serve dual purposes: 1) acting as a\nviable alternative to real-world data for pre-training paradigms and 2)\nachieving superior performance enhancement when integrated into vision-language\nmodels through empirical validation. This paper presents three key\ncontributions: 1) a novel pipeline for generating high-quality,\nlow-hallucination, and knowledge-rich synthetic captions. Our continuous DPO\nmethodology yields remarkable results in reducing hallucinations. Specifically,\nthe non-hallucination caption rate on a held-out test set increases from 48.2%\nto 77.9% for a 7B-size model. 2) Comprehensive empirical validation reveals\nthat our synthetic captions confer superior pre-training advantages over their\ncounterparts. Across 35 vision language tasks, the model trained with our data\nachieves a significant performance gain of at least 6.2% compared to alt-text\npairs and other previous work. Meanwhile, it also offers considerable support\nin the text-to-image domain. With our dataset, the FID score is reduced by 17.1\non a real-world validation benchmark and 13.3 on the MSCOCO validation\nbenchmark. 3) We will release Hunyuan-Recap100M, a low-hallucination and\nknowledge-intensive synthetic caption dataset.",
      "tldr_zh": "该研究针对视觉-语言模型(Vision-Language Model)预训练中高质量图像-文本对日益稀缺的问题，提出了一种低幻觉的合成字幕生成方法。通过持续的DPO (Direct Preference Optimization) 方法，显著降低了模型生成字幕的幻觉，在一个保留的测试集上，7B模型非幻觉字幕率从48.2%提升至77.9%。实验证明，使用该方法生成的合成字幕在预训练中优于真实数据，在35个视觉语言任务上，模型性能提升至少6.2%。在文本到图像生成任务中，FID指标在真实世界验证基准上降低了17.1，在MSCOCO验证基准上降低了13.3。研究团队将发布Hunyuan-Recap100M，一个低幻觉、知识密集的合成字幕数据集。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13123v1",
      "published_date": "2025-04-17 17:40:06 UTC",
      "updated_date": "2025-04-17 17:40:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:03:19.954315"
    },
    {
      "arxiv_id": "2504.13120v1",
      "title": "Probing and Inducing Combinational Creativity in Vision-Language Models",
      "title_zh": "探究和诱导视觉-语言模型中的组合创造力\n",
      "authors": [
        "Yongqian Peng",
        "Yuxi Ma",
        "Mengmeng Wang",
        "Yuxuan Wang",
        "Yizhou Wang",
        "Chi Zhang",
        "Yixin Zhu",
        "Zilong Zheng"
      ],
      "abstract": "The ability to combine existing concepts into novel ideas stands as a\nfundamental hallmark of human intelligence. Recent advances in Vision-Language\nModels (VLMs) like GPT-4V and DALLE-3 have sparked debate about whether their\noutputs reflect combinational creativity--defined by M. A. Boden (1998) as\nsynthesizing novel ideas through combining existing concepts--or sophisticated\npattern matching of training data. Drawing inspiration from cognitive science,\nwe investigate the combinational creativity of VLMs from the lens of concept\nblending. We propose the Identification-Explanation-Implication (IEI)\nframework, which decomposes creative processes into three levels: identifying\ninput spaces, extracting shared attributes, and deriving novel semantic\nimplications. To validate this framework, we curate CreativeMashup, a\nhigh-quality dataset of 666 artist-generated visual mashups annotated according\nto the IEI framework. Through extensive experiments, we demonstrate that in\ncomprehension tasks, best VLMs have surpassed average human performance while\nfalling short of expert-level understanding; in generation tasks, incorporating\nour IEI framework into the generation pipeline significantly enhances the\ncreative quality of VLMs outputs. Our findings establish both a theoretical\nfoundation for evaluating artificial creativity and practical guidelines for\nimproving creative generation in VLMs.",
      "tldr_zh": "该研究从认知科学中的概念融合角度出发，探讨了视觉语言模型(VLMs)的组合创造力，即通过组合现有概念产生新颖想法的能力。研究者提出了一个“识别-解释-推断”(IEI)框架，将创造过程分解为识别输入空间、提取共享属性和推导新语义含义三个层次。为了验证该框架，作者构建了一个高质量的视觉混搭数据集CreativeMashup。实验结果表明，在理解任务中，最佳VLMs的表现超过了普通人类水平，但低于专家水平；在生成任务中，将IEI框架融入生成流程可以显著提高VLMs输出的创造性质量。该研究为评估人工智能的创造力奠定了理论基础，并为改进VLMs中的创造性生成提供了实践指导。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://ppyyqq.github.io/aicc/ The first two authors\n  contribute equally",
      "pdf_url": "http://arxiv.org/pdf/2504.13120v1",
      "published_date": "2025-04-17 17:38:18 UTC",
      "updated_date": "2025-04-17 17:38:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:03:31.821133"
    },
    {
      "arxiv_id": "2504.13102v1",
      "title": "A Multi-task Learning Balanced Attention Convolutional Neural Network Model for Few-shot Underwater Acoustic Target Recognition",
      "title_zh": "一种用于少样本水下声目标识别的多任务学习平衡注意力卷积神经网络模型\n",
      "authors": [
        "Wei Huang",
        "Shumeng Sun",
        "Junpeng Lu",
        "Zhenpeng Xu",
        "Zhengyang Xiu",
        "Hao Zhang"
      ],
      "abstract": "Underwater acoustic target recognition (UATR) is of great significance for\nthe protection of marine diversity and national defense security. The\ndevelopment of deep learning provides new opportunities for UATR, but faces\nchallenges brought by the scarcity of reference samples and complex\nenvironmental interference. To address these issues, we proposes a multi-task\nbalanced channel attention convolutional neural network (MT-BCA-CNN). The\nmethod integrates a channel attention mechanism with a multi-task learning\nstrategy, constructing a shared feature extractor and multi-task classifiers to\njointly optimize target classification and feature reconstruction tasks. The\nchannel attention mechanism dynamically enhances discriminative acoustic\nfeatures such as harmonic structures while suppressing noise. Experiments on\nthe Watkins Marine Life Dataset demonstrate that MT-BCA-CNN achieves 97\\%\nclassification accuracy and 95\\% $F1$-score in 27-class few-shot scenarios,\nsignificantly outperforming traditional CNN and ACNN models, as well as popular\nstate-of-the-art UATR methods. Ablation studies confirm the synergistic\nbenefits of multi-task learning and attention mechanisms, while a dynamic\nweighting adjustment strategy effectively balances task contributions. This\nwork provides an efficient solution for few-shot underwater acoustic\nrecognition, advancing research in marine bioacoustics and sonar signal\nprocessing.",
      "tldr_zh": "本文提出了一种多任务平衡通道注意力卷积神经网络(MT-BCA-CNN)，用于解决水下声目标识别(UATR)中参考样本稀缺和环境干扰复杂的问题。该模型结合通道注意力机制和多任务学习策略，构建共享特征提取器和多任务分类器，联合优化目标分类和特征重构任务。通道注意力机制动态增强谐波结构等判别性声学特征，同时抑制噪声。在Watkins Marine Life Dataset上的实验表明，MT-BCA-CNN在27类小样本场景下实现了97%的分类精度和95%的$F1$-score，显著优于传统CNN和ACNN模型以及其他先进的UATR方法。消融实验证实了多任务学习和注意力机制的协同优势，动态权重调整策略有效地平衡了任务贡献。\n",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13102v1",
      "published_date": "2025-04-17 17:11:32 UTC",
      "updated_date": "2025-04-17 17:11:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:03:43.690300"
    },
    {
      "arxiv_id": "2504.13101v1",
      "title": "An Empirically Grounded Identifiability Theory Will Accelerate Self-Supervised Learning Research",
      "title_zh": "一个基于经验的可辨识性理论将加速自监督学习研究\n",
      "authors": [
        "Patrik Reizinger",
        "Randall Balestriero",
        "David Klindt",
        "Wieland Brendel"
      ],
      "abstract": "Self-Supervised Learning (SSL) powers many current AI systems. As research\ninterest and investment grow, the SSL design space continues to expand. The\nPlatonic view of SSL, following the Platonic Representation Hypothesis (PRH),\nsuggests that despite different methods and engineering approaches, all\nrepresentations converge to the same Platonic ideal. However, this phenomenon\nlacks precise theoretical explanation. By synthesizing evidence from\nIdentifiability Theory (IT), we show that the PRH can emerge in SSL. However,\ncurrent IT cannot explain SSL's empirical success. To bridge the gap between\ntheory and practice, we propose expanding IT into what we term Singular\nIdentifiability Theory (SITh), a broader theoretical framework encompassing the\nentire SSL pipeline. SITh would allow deeper insights into the implicit data\nassumptions in SSL and advance the field towards learning more interpretable\nand generalizable representations. We highlight three critical directions for\nfuture research: 1) training dynamics and convergence properties of SSL; 2) the\nimpact of finite samples, batch size, and data diversity; and 3) the role of\ninductive biases in architecture, augmentations, initialization schemes, and\noptimizers.",
      "tldr_zh": "本文提出了一种基于可辨识性理论(Identifiability Theory, IT)的自监督学习(Self-Supervised Learning, SSL)新视角，旨在解释SSL表征收敛到柏拉图理想(Platonic ideal)的现象。现有IT无法完全解释SSL的实际成功，因此作者建议扩展IT为奇异可辨识性理论(Singular Identifiability Theory, SITh)，以涵盖整个SSL流程。SITh能够更深入地理解SSL中的隐式数据假设，并推动领域向更可解释和泛化的表征学习方向发展。文章强调了未来研究的三个关键方向：SSL的训练动态和收敛性质，有限样本、批量大小和数据多样性的影响，以及架构、数据增强、初始化方案和优化器中归纳偏置的作用。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13101v1",
      "published_date": "2025-04-17 17:10:33 UTC",
      "updated_date": "2025-04-17 17:10:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:03:55.793183"
    },
    {
      "arxiv_id": "2504.13079v1",
      "title": "Retrieval-Augmented Generation with Conflicting Evidence",
      "title_zh": "基于冲突证据的检索增强生成\n",
      "authors": [
        "Han Wang",
        "Archiki Prasad",
        "Elias Stengel-Eskin",
        "Mohit Bansal"
      ],
      "abstract": "Large language model (LLM) agents are increasingly employing\nretrieval-augmented generation (RAG) to improve the factuality of their\nresponses. However, in practice, these systems often need to handle ambiguous\nuser queries and potentially conflicting information from multiple sources\nwhile also suppressing inaccurate information from noisy or irrelevant\ndocuments. Prior work has generally studied and addressed these challenges in\nisolation, considering only one aspect at a time, such as handling ambiguity or\nrobustness to noise and misinformation. We instead consider multiple factors\nsimultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and\nMisinformation in Documents), a new dataset that simulates complex and\nrealistic scenarios for conflicting evidence for a user query, including\nambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent\napproach in which LLM agents debate over the merits of an answer over multiple\nrounds, allowing an aggregator to collate responses corresponding to\ndisambiguated entities while discarding misinformation and noise, thereby\nhandling diverse sources of conflict jointly. We demonstrate the effectiveness\nof MADAM-RAG using both closed and open-source models on AmbigDocs -- which\nrequires presenting all valid answers for ambiguous queries -- improving over\nstrong RAG baselines by up to 11.40% and on FaithEval -- which requires\nsuppressing misinformation -- where we improve by up to 15.80% (absolute) with\nLlama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for\nexisting RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match\nscore). While MADAM-RAG begins to address these conflicting factors, our\nanalysis indicates that a substantial gap remains especially when increasing\nthe level of imbalance in supporting evidence and misinformation.",
      "tldr_zh": "这篇论文研究了在检索增强生成(RAG)系统中，如何处理用户查询的歧义以及来自多个来源的冲突信息，同时抑制噪声和错误信息。作者提出了RAMDocs数据集，用于模拟包含歧义、错误信息和噪声的复杂场景。同时，提出了MADAM-RAG，一种多智能体方法，通过LLM智能体之间的多轮辩论来评估答案的优劣，最终由聚合器整理对应于消歧实体的回应，并丢弃错误信息和噪声。实验结果表明，MADAM-RAG在处理歧义查询和抑制错误信息方面优于现有的RAG基线模型，尤其是在Llama3.3-70B-Instruct模型上，在AmbigDocs和FaithEval数据集上分别提升了11.40%和15.80%。然而，RAMDocs对现有RAG基线模型提出了挑战，表明在处理证据和错误信息不平衡的情况下仍存在提升空间。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Our data and code is available at:\n  https://github.com/HanNight/RAMDocs",
      "pdf_url": "http://arxiv.org/pdf/2504.13079v1",
      "published_date": "2025-04-17 16:46:11 UTC",
      "updated_date": "2025-04-17 16:46:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:04:08.176957"
    },
    {
      "arxiv_id": "2504.13078v1",
      "title": "Enhancing Person-to-Person Virtual Try-On with Multi-Garment Virtual Try-Off",
      "title_zh": "通过多服装虚拟试脱增强人与人之间的虚拟试穿\n",
      "authors": [
        "Riza Velioglu",
        "Petra Bevandic",
        "Robin Chan",
        "Barbara Hammer"
      ],
      "abstract": "Computer vision is transforming fashion through Virtual Try-On (VTON) and\nVirtual Try-Off (VTOFF). VTON generates images of a person in a specified\ngarment using a target photo and a standardized garment image, while a more\nchallenging variant, Person-to-Person Virtual Try-On (p2p-VTON), uses a photo\nof another person wearing the garment. VTOFF, on the other hand, extracts\nstandardized garment images from clothed individuals. We introduce TryOffDiff,\na diffusion-based VTOFF model. Built on a latent diffusion framework with\nSigLIP image conditioning, it effectively captures garment properties like\ntexture, shape, and patterns. TryOffDiff achieves state-of-the-art results on\nVITON-HD and strong performance on DressCode dataset, covering upper-body,\nlower-body, and dresses. Enhanced with class-specific embeddings, it pioneers\nmulti-garment VTOFF, the first of its kind. When paired with VTON models, it\nimproves p2p-VTON by minimizing unwanted attribute transfer, such as skin\ncolor. Code is available at: https://rizavelioglu.github.io/tryoffdiff/",
      "tldr_zh": "该论文提出了TryOffDiff，一个基于扩散模型的VTOFF（Virtual Try-Off）模型，用于从穿着服装的人像中提取标准化的服装图像。TryOffDiff基于潜在扩散框架和SigLIP图像条件，能够有效捕捉服装的纹理、形状和图案等属性。实验结果表明，TryOffDiff在VITON-HD数据集上取得了SOTA (state-of-the-art) 结果，并在DressCode数据集上表现出色，支持上衣、下装和连衣裙等多种服装类型。此外，该模型还通过类别特定的嵌入实现了多服装VTOFF，为首创。将其与VTON模型结合使用，可以有效减少p2p-VTON（Person-to-Person Virtual Try-On）中不必要的属性转移，例如肤色。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13078v1",
      "published_date": "2025-04-17 16:45:18 UTC",
      "updated_date": "2025-04-17 16:45:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:04:19.819067"
    },
    {
      "arxiv_id": "2504.13068v1",
      "title": "Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative Classification Models",
      "title_zh": "准确率并非一致性：碰撞叙事分类模型的专家对齐评估\n",
      "authors": [
        "Sudesh Ramesh Bhagat",
        "Ibne Farabi Shihab",
        "Anuj Sharma"
      ],
      "abstract": "This study explores the relationship between deep learning (DL) model\naccuracy and expert agreement in the classification of crash narratives. We\nevaluate five DL models -- including BERT variants, the Universal Sentence\nEncoder (USE), and a zero-shot classifier -- against expert-labeled data and\nnarrative text. The analysis is further extended to four large language models\n(LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our results reveal a counterintuitive\ntrend: models with higher technical accuracy often exhibit lower agreement with\ndomain experts, whereas LLMs demonstrate greater expert alignment despite\nrelatively lower accuracy scores. To quantify and interpret model-expert\nagreement, we employ Cohen's Kappa, Principal Component Analysis (PCA), and\nSHAP-based explainability techniques. Findings indicate that expert-aligned\nmodels tend to rely more on contextual and temporal language cues, rather than\nlocation-specific keywords. These results underscore that accuracy alone is\ninsufficient for evaluating models in safety-critical NLP applications. We\nadvocate for incorporating expert agreement as a complementary metric in model\nevaluation frameworks and highlight the promise of LLMs as interpretable,\nscalable tools for crash analysis pipelines.",
      "tldr_zh": "该研究探讨了深度学习(DL)模型在车祸叙事分类中的准确率与专家一致性之间的关系。研究评估了五种DL模型（包括BERT变体、通用句子编码器USE和零样本分类器）以及四大语言模型(LLMs)：GPT-4、LLaMA 3、Qwen和Claude。结果表明，技术准确率较高的模型通常与领域专家的共识较低，而LLM尽管准确率相对较低，但表现出更高的专家一致性。通过Cohen's Kappa、主成分分析(PCA)和基于SHAP的可解释性技术进行量化分析，发现与专家对齐的模型更依赖于上下文和时间语言线索，而非特定位置的关键词。研究强调，在安全关键的NLP应用中，仅凭准确率不足以评估模型，应将专家共识纳入模型评估框架，并突出了LLM作为可解释、可扩展的车祸分析工具的潜力。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13068v1",
      "published_date": "2025-04-17 16:29:08 UTC",
      "updated_date": "2025-04-17 16:29:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:04:32.757394"
    },
    {
      "arxiv_id": "2504.13059v1",
      "title": "RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins",
      "title_zh": "RoboTwin：基于生成式数字孪生的双臂机器人基准测试",
      "authors": [
        "Yao Mu",
        "Tianxing Chen",
        "Zanxin Chen",
        "Shijia Peng",
        "Zhiqian Lan",
        "Zeyu Gao",
        "Zhixuan Liang",
        "Qiaojun Yu",
        "Yude Zou",
        "Mingkun Xu",
        "Lunkai Lin",
        "Zhiqiang Xie",
        "Mingyu Ding",
        "Ping Luo"
      ],
      "abstract": "In the rapidly advancing field of robotics, dual-arm coordination and complex\nobject manipulation are essential capabilities for developing advanced\nautonomous systems. However, the scarcity of diverse, high-quality\ndemonstration data and real-world-aligned evaluation benchmarks severely limits\nsuch development. To address this, we introduce RoboTwin, a generative digital\ntwin framework that uses 3D generative foundation models and large language\nmodels to produce diverse expert datasets and provide a real-world-aligned\nevaluation platform for dual-arm robotic tasks. Specifically, RoboTwin creates\nvaried digital twins of objects from single 2D images, generating realistic and\ninteractive scenarios. It also introduces a spatial relation-aware code\ngeneration framework that combines object annotations with large language\nmodels to break down tasks, determine spatial constraints, and generate precise\nrobotic movement code. Our framework offers a comprehensive benchmark with both\nsimulated and real-world data, enabling standardized evaluation and better\nalignment between simulated training and real-world performance. We validated\nour approach using the open-source COBOT Magic Robot platform. Policies\npre-trained on RoboTwin-generated data and fine-tuned with limited real-world\nsamples demonstrate significant potential for enhancing dual-arm robotic\nmanipulation systems by improving success rates by over 70% for single-arm\ntasks and over 40% for dual-arm tasks compared to models trained solely on\nreal-world data.",
      "tldr_zh": "RoboTwin是一个用于双臂机器人研究的生成式数字孪生框架，旨在解决高质量演示数据和真实环境对齐的评估基准匮乏的问题。该框架利用3D生成式基础模型和大型语言模型，从2D图像生成多样化的物体数字孪生，构建逼真且可交互的场景。RoboTwin还引入了空间关系感知的代码生成框架，结合物体标注和大型语言模型来分解任务、确定空间约束并生成精确的机器人运动代码。该框架提供了一个包含模拟和真实数据的综合基准，能够标准化评估并改善模拟训练和真实性能之间的一致性。实验结果表明，在RoboTwin生成的数据上预训练并在少量真实数据上微调的策略，相比于仅在真实数据上训练的模型，单臂任务成功率提高了70%以上，双臂任务成功率提高了40%以上，显著提升了双臂机器人操作系统的性能。\n",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.RO",
      "comment": "CVPR 2025 Highlight. 22 pages. Project page:\n  https://robotwin-benchmark.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2504.13059v1",
      "published_date": "2025-04-17 16:14:24 UTC",
      "updated_date": "2025-04-17 16:14:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:04:44.136028"
    },
    {
      "arxiv_id": "2504.13054v1",
      "title": "Aspect-Based Summarization with Self-Aspect Retrieval Enhanced Generation",
      "title_zh": "基于方面检索增强生成的面向方面摘要\n",
      "authors": [
        "Yichao Feng",
        "Shuai Zhao",
        "Yueqiu Li",
        "Luwei Xiao",
        "Xiaobao Wu",
        "Anh Tuan Luu"
      ],
      "abstract": "Aspect-based summarization aims to generate summaries tailored to specific\naspects, addressing the resource constraints and limited generalizability of\ntraditional summarization approaches. Recently, large language models have\nshown promise in this task without the need for training. However, they rely\nexcessively on prompt engineering and face token limits and hallucination\nchallenges, especially with in-context learning. To address these challenges,\nin this paper, we propose a novel framework for aspect-based summarization:\nSelf-Aspect Retrieval Enhanced Summary Generation. Rather than relying solely\non in-context learning, given an aspect, we employ an embedding-driven\nretrieval mechanism to identify its relevant text segments. This approach\nextracts the pertinent content while avoiding unnecessary details, thereby\nmitigating the challenge of token limits. Moreover, our framework optimizes\ntoken usage by deleting unrelated parts of the text and ensuring that the model\ngenerates output strictly based on the given aspect. With extensive experiments\non benchmark datasets, we demonstrate that our framework not only achieves\nsuperior performance but also effectively mitigates the token limitation\nproblem.",
      "tldr_zh": "本文提出了一种基于自注意力检索增强的方面级摘要生成框架，旨在解决现有大语言模型在方面级摘要任务中过度依赖提示工程、token限制和幻觉问题。该框架通过embedding驱动的检索机制，针对特定方面检索相关文本片段，提取关键内容并避免不必要的细节，从而缓解token限制的挑战。此外，该框架通过删除不相关文本部分来优化token使用，并确保模型严格基于给定方面生成输出。在基准数据集上的大量实验表明，该框架不仅实现了卓越的性能，而且有效地缓解了token限制问题。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13054v1",
      "published_date": "2025-04-17 16:09:57 UTC",
      "updated_date": "2025-04-17 16:09:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:04:55.657714"
    },
    {
      "arxiv_id": "2504.13048v1",
      "title": "Design Topological Materials by Reinforcement Fine-Tuned Generative Model",
      "title_zh": "通过强化微调生成模型设计拓扑材料\n",
      "authors": [
        "Haosheng Xu",
        "Dongheng Qian",
        "Zhixuan Liu",
        "Yadong Jiang",
        "Jing Wang"
      ],
      "abstract": "Topological insulators (TIs) and topological crystalline insulators (TCIs)\nare materials with unconventional electronic properties, making their discovery\nhighly valuable for practical applications. However, such materials,\nparticularly those with a full band gap, remain scarce. Given the limitations\nof traditional approaches that scan known materials for candidates, we focus on\nthe generation of new topological materials through a generative model.\nSpecifically, we apply reinforcement fine-tuning (ReFT) to a pre-trained\ngenerative model, thereby aligning the model's objectives with our material\ndesign goals. We demonstrate that ReFT is effective in enhancing the model's\nability to generate TIs and TCIs, with minimal compromise on the stability of\nthe generated materials. Using the fine-tuned model, we successfully identify a\nlarge number of new topological materials, with Ge$_2$Bi$_2$O$_6$ serving as a\nrepresentative example--a TI with a full band gap of 0.26 eV, ranking among the\nlargest known in this category.",
      "tldr_zh": "该研究利用强化微调(ReFT)改进的生成模型来设计拓扑材料，旨在克服传统方法在发现具有完整带隙的拓扑绝缘体(TIs)和拓扑晶体绝缘体(TCIs)方面的局限性。通过ReFT，该模型的目标与材料设计目标对齐，从而有效提升了生成TIs和TCIs的能力，同时保证了生成材料的稳定性。研究成功识别出大量新的拓扑材料，其中Ge$_2$Bi$_2$O$_6$作为一个代表性例子，是一种具有0.26 eV完整带隙的TI，在同类材料中名列前茅。\n",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13048v1",
      "published_date": "2025-04-17 16:05:24 UTC",
      "updated_date": "2025-04-17 16:05:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:05:07.639042"
    },
    {
      "arxiv_id": "2504.13042v1",
      "title": "Event-Enhanced Blurry Video Super-Resolution",
      "title_zh": "事件增强的模糊视频超分辨率\n",
      "authors": [
        "Dachun Kai",
        "Yueyi Zhang",
        "Jin Wang",
        "Zeyu Xiao",
        "Zhiwei Xiong",
        "Xiaoyan Sun"
      ],
      "abstract": "In this paper, we tackle the task of blurry video super-resolution (BVSR),\naiming to generate high-resolution (HR) videos from low-resolution (LR) and\nblurry inputs. Current BVSR methods often fail to restore sharp details at high\nresolutions, resulting in noticeable artifacts and jitter due to insufficient\nmotion information for deconvolution and the lack of high-frequency details in\nLR frames. To address these challenges, we introduce event signals into BVSR\nand propose a novel event-enhanced network, Ev-DeblurVSR. To effectively fuse\ninformation from frames and events for feature deblurring, we introduce a\nreciprocal feature deblurring module that leverages motion information from\nintra-frame events to deblur frame features while reciprocally using global\nscene context from the frames to enhance event features. Furthermore, to\nenhance temporal consistency, we propose a hybrid deformable alignment module\nthat fully exploits the complementary motion information from inter-frame\nevents and optical flow to improve motion estimation in the deformable\nalignment process. Extensive evaluations demonstrate that Ev-DeblurVSR\nestablishes a new state-of-the-art performance on both synthetic and real-world\ndatasets. Notably, on real data, our method is +2.59 dB more accurate and\n7.28$\\times$ faster than the recent best BVSR baseline FMA-Net. Code:\nhttps://github.com/DachunKai/Ev-DeblurVSR.",
      "tldr_zh": "该论文提出了一种事件增强的模糊视频超分辨率(BVSR)方法，旨在从低分辨率(LR)和模糊的视频中生成高分辨率(HR)视频。为了解决现有BVSR方法难以恢复清晰细节的问题，论文引入了事件信号，并提出了一个名为Ev-DeblurVSR的新型事件增强网络。该网络包含一个互惠特征去模糊模块，利用帧内事件的运动信息来去模糊帧特征，反之亦然。此外，还提出了一个混合可变形对齐模块，利用帧间事件和光流的互补运动信息来提高可变形对齐过程中的运动估计。实验结果表明，Ev-DeblurVSR在合成和真实数据集上都取得了state-of-the-art的性能，在真实数据上比FMA-Net准确率提高了2.59 dB，速度提高了7.28倍。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "AAAI 2025. Project page:\n  https://dachunkai.github.io/evtexture.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2504.13042v1",
      "published_date": "2025-04-17 15:55:41 UTC",
      "updated_date": "2025-04-17 15:55:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:05:19.894193"
    },
    {
      "arxiv_id": "2504.13037v1",
      "title": "Towards Cardiac MRI Foundation Models: Comprehensive Visual-Tabular Representations for Whole-Heart Assessment and Beyond",
      "title_zh": "迈向心脏 MRI 基础模型：用于全心评估及其他应用的综合视觉-表格表示",
      "authors": [
        "Yundi Zhang",
        "Paul Hager",
        "Che Liu",
        "Suprosanna Shit",
        "Chen Chen",
        "Daniel Rueckert",
        "Jiazhen Pan"
      ],
      "abstract": "Cardiac magnetic resonance imaging is the gold standard for non-invasive\ncardiac assessment, offering rich spatio-temporal views of the cardiac anatomy\nand physiology. Patient-level health factors, such as demographics, metabolic,\nand lifestyle, are known to substantially influence cardiovascular health and\ndisease risk, yet remain uncaptured by CMR alone. To holistically understand\ncardiac health and to enable the best possible interpretation of an\nindividual's disease risk, CMR and patient-level factors must be jointly\nexploited within an integrated framework. Recent multi-modal approaches have\nbegun to bridge this gap, yet they often rely on limited spatio-temporal data\nand focus on isolated clinical tasks, thereby hindering the development of a\ncomprehensive representation for cardiac health evaluation. To overcome these\nlimitations, we introduce ViTa, a step toward foundation models that delivers a\ncomprehensive representation of the heart and a precise interpretation of\nindividual disease risk. Leveraging data from 42,000 UK Biobank participants,\nViTa integrates 3D+T cine stacks from short-axis and long-axis views, enabling\na complete capture of the cardiac cycle. These imaging data are then fused with\ndetailed tabular patient-level factors, enabling context-aware insights. This\nmulti-modal paradigm supports a wide spectrum of downstream tasks, including\ncardiac phenotype and physiological feature prediction, segmentation, and\nclassification of cardiac and metabolic diseases within a single unified\nframework. By learning a shared latent representation that bridges rich imaging\nfeatures and patient context, ViTa moves beyond traditional, task-specific\nmodels toward a universal, patient-specific understanding of cardiac health,\nhighlighting its potential to advance clinical utility and scalability in\ncardiac analysis.",
      "tldr_zh": "该研究提出ViTa，旨在构建心脏磁共振成像(Cardiac MRI)的基础模型，以实现对全心脏的综合评估。ViTa模型整合了来自42,000名英国生物银行参与者的3D+T电影堆栈数据（短轴和长轴视图），以及详细的患者级别表格数据，从而实现上下文感知的洞察。这种多模态范式支持广泛的下游任务，包括心脏表型和生理特征预测、分割以及心脏和代谢疾病的分类。通过学习连接丰富成像特征和患者背景的共享潜在表示，ViTa超越了传统的、特定于任务的模型，朝着对心脏健康的通用、患者特异性理解迈进。\n",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13037v1",
      "published_date": "2025-04-17 15:46:19 UTC",
      "updated_date": "2025-04-17 15:46:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:05:31.805971"
    },
    {
      "arxiv_id": "2504.13035v1",
      "title": "Prototypes are Balanced Units for Efficient and Effective Partially Relevant Video Retrieval",
      "title_zh": "原型是高效且有效的局部相关视频检索的平衡单元\n",
      "authors": [
        "WonJun Moon",
        "Cheol-Ho Cho",
        "Woojin Jun",
        "Minho Shim",
        "Taeoh Kim",
        "Inwoong Lee",
        "Dongyoon Wee",
        "Jae-Pil Heo"
      ],
      "abstract": "In a retrieval system, simultaneously achieving search accuracy and\nefficiency is inherently challenging. This challenge is particularly pronounced\nin partially relevant video retrieval (PRVR), where incorporating more diverse\ncontext representations at varying temporal scales for each video enhances\naccuracy but increases computational and memory costs. To address this\ndichotomy, we propose a prototypical PRVR framework that encodes diverse\ncontexts within a video into a fixed number of prototypes. We then introduce\nseveral strategies to enhance text association and video understanding within\nthe prototypes, along with an orthogonal objective to ensure that the\nprototypes capture a diverse range of content. To keep the prototypes\nsearchable via text queries while accurately encoding video contexts, we\nimplement cross- and uni-modal reconstruction tasks. The cross-modal\nreconstruction task aligns the prototypes with textual features within a shared\nspace, while the uni-modal reconstruction task preserves all video contexts\nduring encoding. Additionally, we employ a video mixing technique to provide\nweak guidance to further align prototypes and associated textual\nrepresentations. Extensive evaluations on TVR, ActivityNet-Captions, and\nQVHighlights validate the effectiveness of our approach without sacrificing\nefficiency.",
      "tldr_zh": "本文提出了一种原型化的部分相关视频检索(PRVR)框架，旨在解决检索精度和效率之间的固有矛盾。该框架将视频中的各种上下文编码为固定数量的原型，并通过引入多种策略来增强文本关联和视频理解。为了保证原型能够捕获多样化的内容，论文还设计了一个正交目标。通过跨模态和单模态重建任务，原型既能通过文本查询进行搜索，又能准确编码视频上下文。此外，还采用视频混合技术来进一步对齐原型和相关的文本表示。在TVR、ActivityNet-Captions和QVHighlights数据集上的大量实验验证了该方法的有效性，且不牺牲效率。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13035v1",
      "published_date": "2025-04-17 15:43:29 UTC",
      "updated_date": "2025-04-17 15:43:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:05:43.741014"
    },
    {
      "arxiv_id": "2504.13032v1",
      "title": "InstructRAG: Leveraging Retrieval-Augmented Generation on Instruction Graphs for LLM-Based Task Planning",
      "title_zh": "InstructRAG：利用检索增强生成在指令图上进行基于LLM的任务规划\n",
      "authors": [
        "Zheng Wang",
        "Shu Xian Teo",
        "Jun Jie Chew",
        "Wei Shi"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have enabled their use as\nagents for planning complex tasks. Existing methods typically rely on a\nthought-action-observation (TAO) process to enhance LLM performance, but these\napproaches are often constrained by the LLMs' limited knowledge of complex\ntasks. Retrieval-augmented generation (RAG) offers new opportunities by\nleveraging external databases to ground generation in retrieved information. In\nthis paper, we identify two key challenges (enlargability and transferability)\nin applying RAG to task planning. We propose InstructRAG, a novel solution\nwithin a multi-agent meta-reinforcement learning framework, to address these\nchallenges. InstructRAG includes a graph to organize past instruction paths\n(sequences of correct actions), an RL-Agent with Reinforcement Learning to\nexpand graph coverage for enlargability, and an ML-Agent with Meta-Learning to\nimprove task generalization for transferability. The two agents are trained\nend-to-end to optimize overall planning performance. Our experiments on four\nwidely used task planning datasets demonstrate that InstructRAG significantly\nenhances performance and adapts efficiently to new tasks, achieving up to a\n19.2% improvement over the best existing approach.",
      "tldr_zh": "InstructRAG提出了一种新的基于检索增强生成（RAG）的框架，用于提升大型语言模型（LLM）在复杂任务规划中的性能。该方法通过构建指令图来组织历史指令路径（正确的动作序列），并利用多智能体元强化学习框架解决RAG应用于任务规划时面临的可扩展性和可迁移性挑战。InstructRAG包含一个RL-Agent，用于扩展图的覆盖范围，以及一个ML-Agent，用于提高任务泛化能力。两个智能体通过端到端训练优化整体规划性能。在四个广泛使用的任务规划数据集上的实验表明，InstructRAG显著提高了性能，并且能够有效地适应新任务，相比现有最佳方法提升高达19.2%。\n",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper has been accepted by SIGIR 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.13032v1",
      "published_date": "2025-04-17 15:41:39 UTC",
      "updated_date": "2025-04-17 15:41:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:05:55.852767"
    },
    {
      "arxiv_id": "2504.13021v1",
      "title": "Pose and Facial Expression Transfer by using StyleGAN",
      "title_zh": "使用 StyleGAN 实现姿势和面部表情迁移\n",
      "authors": [
        "Petr Jahoda",
        "Jan Cech"
      ],
      "abstract": "We propose a method to transfer pose and expression between face images.\nGiven a source and target face portrait, the model produces an output image in\nwhich the pose and expression of the source face image are transferred onto the\ntarget identity. The architecture consists of two encoders and a mapping\nnetwork that projects the two inputs into the latent space of StyleGAN2, which\nfinally generates the output. The training is self-supervised from video\nsequences of many individuals. Manual labeling is not required. Our model\nenables the synthesis of random identities with controllable pose and\nexpression. Close-to-real-time performance is achieved.",
      "tldr_zh": "本文提出了一种利用StyleGAN进行人脸姿态和表情迁移的方法。该模型接收源人脸和目标人脸图像，将源人脸的姿态和表情迁移到目标人脸的身份上。该架构包含两个编码器和一个映射网络，将两个输入投影到StyleGAN2的潜在空间，最终生成输出图像。训练过程采用来自多人视频序列的自监督方式，无需手动标注。该模型能够合成具有可控姿态和表情的随机身份人脸，并实现接近实时的性能。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at CVWW 2024. Presented in Terme Olimia, Slovenia",
      "pdf_url": "http://arxiv.org/pdf/2504.13021v1",
      "published_date": "2025-04-17 15:29:41 UTC",
      "updated_date": "2025-04-17 15:29:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:06:07.595717"
    },
    {
      "arxiv_id": "2504.12996v1",
      "title": "SHA256 at SemEval-2025 Task 4: Selective Amnesia -- Constrained Unlearning for Large Language Models via Knowledge Isolation",
      "title_zh": "SHA256 在 SemEval-2025 任务 4 中的表现：选择性失忆——通过知识隔离实现大语言模型的约束性遗忘\n",
      "authors": [
        "Saransh Agrawal",
        "Kuan-Hao Huang"
      ],
      "abstract": "Large language models (LLMs) frequently memorize sensitive information during\ntraining, posing risks when deploying publicly accessible models. Current\nmachine unlearning methods struggle to selectively remove specific data\nassociations without degrading overall model capabilities. This paper presents\nour solution to SemEval-2025 Task 4 on targeted unlearning, which introduces a\ntwo-stage methodology that combines causal mediation analysis with\nlayer-specific optimization. Through systematic causal tracing experiments on\nOLMo architectures (1B and 7B parameters), we identify the critical role of the\nfirst few transformer layers (layers 0-5) in storing subject-attribute\nassociations within MLP modules. Building on this insight, we develop a\nconstrained optimization approach that freezes upper layers while applying a\nnovel joint loss function to lower layers-simultaneously maximizing forget set\nloss via output token cross-entropy penalties and minimizing retain set\ndeviation through adaptive regularization. Our method achieves 2nd place in the\n1B model track, demonstrating strong task performance while maintaining 88% of\nbaseline MMLU accuracy. These results establish causal-informed layer\noptimization as a promising paradigm for efficient, precise unlearning in LLMs,\noffering a significant step forward in addressing data privacy concerns in AI\nsystems.",
      "tldr_zh": "该论文提出了一个针对LLM的约束性遗忘方法，用于解决SemEval-2025 Task 4中的目标遗忘问题。该方法通过因果中介分析确定了Transformer模型前几层(0-5层)在存储subject-attribute关联中的关键作用，并据此设计了一种两阶段方法：冻结上层，同时对下层应用联合损失函数，最大化遗忘集损失并最小化保留集偏差。实验结果表明，该方法在1B参数的OLMo模型上取得了第二名的成绩，并在保持88%的MMLU准确率的同时实现了强大的任务性能。该研究证明了因果信息层优化是LLM中高效、精确遗忘的一种有前景的范例。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, In Proceedings of The 19th International Workshop on\n  Semantic Evaluation (SemEval), 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.12996v1",
      "published_date": "2025-04-17 15:05:40 UTC",
      "updated_date": "2025-04-17 15:05:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:06:20.100767"
    },
    {
      "arxiv_id": "2504.12984v1",
      "title": "A Virtual Machine for Arbitrary Low-Precision GPGPU Computation in LLM Serving",
      "title_zh": "用于LLM服务中任意低精度GPGPU计算的虚拟机\n",
      "authors": [
        "Yaoyao Ding",
        "Bohan Hou",
        "Xiao Zhang",
        "Allan Lin",
        "Tianqi Chen",
        "Cody Yu Hao",
        "Yida Wang",
        "Gennady Pekhimenko"
      ],
      "abstract": "Serving Large Language Models (LLMs) is critical for AI-powered applications\nbut demands substantial computational resources, particularly in memory\nbandwidth and computational throughput. Low-precision computation has emerged\nas a key technique to improve efficiency while reducing resource consumption.\nExisting approaches for generating low-precision kernels are limited to weight\nbit widths that are powers of two and suffer from suboptimal performance due to\nhigh-level GPU programming abstractions. These abstractions restrict critical\noptimizations, such as fine-grained register management and optimized memory\naccess patterns, which are essential for efficient low-precision computations.\nIn this paper, we introduce a virtual machine (VM) designed for General-Purpose\nGPU (GPGPU) computing, enabling support for low-precision data types with\narbitrary bit widths while maintaining GPU programmability. The proposed VM\nfeatures a thread-block-level programming model, a hierarchical memory space, a\nnovel algebraic layout system, and extensive support for diverse low-precision\ndata types. VM programs are compiled into highly efficient GPU programs with\nautomatic vectorization and instruction selection. Extensive experiments\ndemonstrate that our VM efficiently supports a full spectrum of low-precision\ndata types, and outperforms state-of-the-art low-precision kernels on their\nsupported types. Compared to existing compilers like Triton and Ladder, as well\nas hand-optimized kernels such as QuantLLM and Marlin, our VM achieves\nperformance improvements of 1.75x, 2.61x, 1.29x and 1.03x, respectively.",
      "tldr_zh": "本文提出了一种用于通用GPU (GPGPU)计算的虚拟机(VM)，旨在支持任意低精度数据类型的LLM Serving，解决现有低精度kernel仅支持2的幂次位宽以及高级GPU编程抽象导致性能欠佳的问题。该VM具有线程块级别的编程模型、分层内存空间和新型代数布局系统，并广泛支持各种低精度数据类型。VM程序被编译成高效的GPU程序，具有自动向量化和指令选择功能。实验表明，该VM能够有效支持各种低精度数据类型，并在性能上优于现有的低精度kernel，例如Triton, Ladder, QuantLLM和Marlin，分别实现了1.75x, 2.61x, 1.29x和1.03x的性能提升。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages, 15 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.12984v1",
      "published_date": "2025-04-17 14:45:03 UTC",
      "updated_date": "2025-04-17 14:45:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:06:31.964395"
    },
    {
      "arxiv_id": "2504.12982v1",
      "title": "Accommodate Knowledge Conflicts in Retrieval-augmented LLMs: Towards Reliable Response Generation in the Wild",
      "title_zh": "容纳检索增强型LLM中的知识冲突：迈向实际应用中可靠的响应生成\n",
      "authors": [
        "Jiatai Wang",
        "Zhiwei Xu",
        "Di Jin",
        "Xuewen Yang",
        "Tao Li"
      ],
      "abstract": "The proliferation of large language models (LLMs) has significantly advanced\ninformation retrieval systems, particularly in response generation (RG).\nUnfortunately, LLMs often face knowledge conflicts between internal memory and\nretrievaled external information, arising from misinformation, biases, or\noutdated knowledge. These conflicts undermine response reliability and\nintroduce uncertainty in decision-making. In this work, we analyze how LLMs\nnavigate knowledge conflicts from an information-theoretic perspective and\nreveal that when conflicting and supplementary information exhibit significant\ndifferences, LLMs confidently resolve their preferences. However, when the\ndistinction is ambiguous, LLMs experience heightened uncertainty. Based on this\ninsight, we propose Swin-VIB, a novel framework that integrates a pipeline of\nvariational information bottleneck models into adaptive augmentation of\nretrieved information and guiding LLM preference in response generation.\nExtensive experiments on single-choice, open-ended question-answering (QA), and\nretrieval augmented generation (RAG) validate our theoretical findings and\ndemonstrate the efficacy of Swin-VIB. Notably, our method improves\nsingle-choice task accuracy by at least 7.54\\% over competitive baselines.",
      "tldr_zh": "该研究关注于检索增强的大语言模型(LLMs)在面对内部知识与外部检索信息冲突时的可靠性问题。通过信息论分析，发现LLMs在冲突信息差异显著时能较好地做出判断，而在信息模糊时则表现出高度不确定性。基于此，提出了Swin-VIB框架，该框架利用变分信息瓶颈模型(variational information bottleneck models)自适应地增强检索信息，并引导LLM进行响应生成。在单选题、开放式问答(QA)和检索增强生成(RAG)等任务上的实验表明，Swin-VIB能够有效提高LLMs的准确性和可靠性，其中在单选题任务上，准确率至少提升了7.54%。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12982v1",
      "published_date": "2025-04-17 14:40:31 UTC",
      "updated_date": "2025-04-17 14:40:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:06:43.930628"
    },
    {
      "arxiv_id": "2504.12977v1",
      "title": "A Phenomenological Approach to Analyzing User Queries in IT Systems Using Heidegger's Fundamental Ontology",
      "title_zh": "一种基于海德格尔基本本体论的 IT 系统用户查询分析现象学方法\n",
      "authors": [
        "Maksim Vishnevskiy"
      ],
      "abstract": "This paper presents a novel research analytical IT system grounded in Martin\nHeidegger's Fundamental Ontology, distinguishing between beings (das Seiende)\nand Being (das Sein). The system employs two modally distinct, descriptively\ncomplete languages: a categorical language of beings for processing user inputs\nand an existential language of Being for internal analysis. These languages are\nbridged via a phenomenological reduction module, enabling the system to analyze\nuser queries (including questions, answers, and dialogues among IT\nspecialists), identify recursive and self-referential structures, and provide\nactionable insights in categorical terms. Unlike contemporary systems limited\nto categorical analysis, this approach leverages Heidegger's phenomenological\nexistential analysis to uncover deeper ontological patterns in query\nprocessing, aiding in resolving logical traps in complex interactions, such as\nmetaphor usage in IT contexts. The path to full realization involves\nformalizing the language of Being by a research team based on Heidegger's\nFundamental Ontology; given the existing completeness of the language of\nbeings, this reduces the system's computability to completeness, paving the way\nfor a universal query analysis tool. The paper presents the system's\narchitecture, operational principles, technical implementation, use\ncases--including a case based on real IT specialist dialogues--comparative\nevaluation with existing tools, and its advantages and limitations.",
      "tldr_zh": "本文提出了一种基于海德格尔基本本体论的新型IT系统分析方法，该方法区分了存在者(das Seiende)和存在(das Sein)。系统采用两种模态不同的、描述上完备的语言：用于处理用户输入的“存在者”范畴语言和用于内部分析的“存在”存在主义语言。通过现象学还原模块连接这两种语言，系统能够分析用户查询（包括问题、答案和IT专家之间的对话），识别递归和自引用结构，并以范畴术语提供可操作的见解。与仅限于范畴分析的现有系统不同，该方法利用海德格尔的现象学存在分析来揭示查询处理中更深层次的本体论模式，从而帮助解决复杂交互中的逻辑陷阱，例如IT环境中的隐喻用法。该研究旨在通过形式化“存在”语言，构建一个通用的查询分析工具。\n",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.SE",
      "comment": "12 pages, no figures",
      "pdf_url": "http://arxiv.org/pdf/2504.12977v1",
      "published_date": "2025-04-17 14:29:25 UTC",
      "updated_date": "2025-04-17 14:29:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:06:56.153324"
    },
    {
      "arxiv_id": "2504.12971v1",
      "title": "Transferrable Surrogates in Expressive Neural Architecture Search Spaces",
      "title_zh": "富有表现力的神经架构搜索空间中的可迁移代理模型\n",
      "authors": [
        "Shiwen Qin",
        "Gabriela Kadlecová",
        "Martin Pilát",
        "Shay B. Cohen",
        "Roman Neruda",
        "Elliot J. Crowley",
        "Jovita Lukasik",
        "Linus Ericsson"
      ],
      "abstract": "Neural architecture search (NAS) faces a challenge in balancing the\nexploration of expressive, broad search spaces that enable architectural\ninnovation with the need for efficient evaluation of architectures to\neffectively search such spaces. We investigate surrogate model training for\nimproving search in highly expressive NAS search spaces based on context-free\ngrammars. We show that i) surrogate models trained either using zero-cost-proxy\nmetrics and neural graph features (GRAF) or by fine-tuning an off-the-shelf LM\nhave high predictive power for the performance of architectures both within and\nacross datasets, ii) these surrogates can be used to filter out bad\narchitectures when searching on novel datasets, thereby significantly speeding\nup search and achieving better final performances, and iii) the surrogates can\nbe further used directly as the search objective for huge speed-ups.",
      "tldr_zh": "该论文研究了在基于上下文无关文法的、高表达性的神经架构搜索(NAS)空间中使用可迁移的代理模型来提升搜索效率。研究表明，无论是使用零成本代理指标和神经图特征(GRAF)训练的代理模型，还是通过微调现成的语言模型(LM)得到的代理模型，都对架构性能具有很高的预测能力，并且这种能力可以跨数据集迁移。这些代理模型可以用于过滤掉在新的数据集上表现不佳的架构，从而显著加速搜索过程并提高最终性能。此外，代理模型可以直接用作搜索目标，进一步大幅提升搜索速度。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Project page at: https://shiwenqin.github.io/TransferrableSurrogate/",
      "pdf_url": "http://arxiv.org/pdf/2504.12971v1",
      "published_date": "2025-04-17 14:22:28 UTC",
      "updated_date": "2025-04-17 14:22:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:07:07.861026"
    },
    {
      "arxiv_id": "2504.12961v1",
      "title": "QLLM: Do We Really Need a Mixing Network for Credit Assignment in Multi-Agent Reinforcement Learning?",
      "title_zh": "QLLM：多智能体强化学习中，我们真的需要混合网络来进行信用分配吗？\n",
      "authors": [
        "Zhouyang Jiang",
        "Bin Zhang",
        "Airong Wei",
        "Zhiwei Xu"
      ],
      "abstract": "Credit assignment has remained a fundamental challenge in multi-agent\nreinforcement learning (MARL). Previous studies have primarily addressed this\nissue through value decomposition methods under the centralized training with\ndecentralized execution paradigm, where neural networks are utilized to\napproximate the nonlinear relationship between individual Q-values and the\nglobal Q-value. Although these approaches have achieved considerable success in\nvarious benchmark tasks, they still suffer from several limitations, including\nimprecise attribution of contributions, limited interpretability, and poor\nscalability in high-dimensional state spaces. To address these challenges, we\npropose a novel algorithm, \\textbf{QLLM}, which facilitates the automatic\nconstruction of credit assignment functions using large language models (LLMs).\nSpecifically, the concept of \\textbf{TFCAF} is introduced, wherein the credit\nallocation process is represented as a direct and expressive nonlinear\nfunctional formulation. A custom-designed \\textit{coder-evaluator} framework is\nfurther employed to guide the generation, verification, and refinement of\nexecutable code by LLMs, significantly mitigating issues such as hallucination\nand shallow reasoning during inference. Extensive experiments conducted on\nseveral standard MARL benchmarks demonstrate that the proposed method\nconsistently outperforms existing state-of-the-art baselines. Moreover, QLLM\nexhibits strong generalization capability and maintains compatibility with a\nwide range of MARL algorithms that utilize mixing networks, positioning it as a\npromising and versatile solution for complex multi-agent scenarios.",
      "tldr_zh": "该论文提出了一种新的多智能体强化学习算法QLLM，旨在解决信用分配(credit assignment)问题，无需传统的混合网络(mixing network)。QLLM利用大型语言模型(LLMs)自动构建信用分配函数，引入了TFCAF的概念，将信用分配过程表示为直接且富有表现力的非线性函数形式。通过定制的coder-evaluator框架，引导LLMs生成、验证和改进可执行代码，从而减轻幻觉和浅层推理问题。在标准MARL基准测试中，QLLM的表现优于现有最先进的基线，并展现出强大的泛化能力，可与各种使用混合网络的MARL算法兼容。\n",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "9 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.12961v1",
      "published_date": "2025-04-17 14:07:11 UTC",
      "updated_date": "2025-04-17 14:07:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:07:19.824141"
    },
    {
      "arxiv_id": "2504.12951v1",
      "title": "Are Retrials All You Need? Enhancing Large Language Model Reasoning Without Verbalized Feedback",
      "title_zh": "重试就是你所需要的一切吗？无需口头反馈即可增强大型语言模型的推理能力\n",
      "authors": [
        "Nearchos Potamitis",
        "Akhil Arora"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have catalyzed the\ndevelopment of general-purpose autonomous agents, demonstrating remarkable\nperformance in complex reasoning tasks across various domains. This surge has\nspurred the evolution of a plethora of prompt-based reasoning frameworks. A\nrecent focus has been on iterative reasoning strategies that refine outputs\nthrough self-evaluation and verbalized feedback. However, these strategies\nrequire additional computational complexity to enable models to recognize and\ncorrect their mistakes, leading to a significant increase in their cost. In\nthis work, we introduce the concept of ``retrials without feedback'', an\nembarrassingly simple yet powerful mechanism for enhancing reasoning frameworks\nby allowing LLMs to retry problem-solving attempts upon identifying incorrect\nanswers. Unlike conventional iterative refinement methods, our method does not\nrequire explicit self-reflection or verbalized feedback, simplifying the\nrefinement process. Our findings indicate that simpler retrial-based approaches\noften outperform more sophisticated reasoning frameworks, suggesting that the\nbenefits of complex methods may not always justify their computational costs.\nBy challenging the prevailing assumption that more intricate reasoning\nstrategies inherently lead to better performance, our work offers new insights\ninto how simpler, more efficient approaches can achieve optimal results. So,\nare retrials all you need?",
      "tldr_zh": "该论文提出了一种名为“无反馈重试”(retrials without feedback)的简单而有效的机制，用于提升大型语言模型(LLMs)的推理能力。该方法允许LLMs在识别出错误答案后重试问题求解，而无需像传统迭代改进方法那样进行显式的自我反思或口头反馈。研究结果表明，这种基于重试的简单方法通常优于更复杂的推理框架，挑战了“更复杂的推理策略必然带来更好性能”的普遍假设。该研究表明，更简单、更高效的方法也能取得最佳结果，并能显著降低计算成本。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, 16 figures, 1 table. arXiv admin note: text overlap with\n  arXiv:2405.06691",
      "pdf_url": "http://arxiv.org/pdf/2504.12951v1",
      "published_date": "2025-04-17 13:52:48 UTC",
      "updated_date": "2025-04-17 13:52:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:07:31.719601"
    },
    {
      "arxiv_id": "2504.12911v1",
      "title": "Benchmarking Multi-National Value Alignment for Large Language Models",
      "title_zh": "用于大型语言模型的多国价值观对齐基准测试\n",
      "authors": [
        "Chengyi Ju",
        "Weijie Shi",
        "Chengzhong Liu",
        "Jiaming Ji",
        "Jipeng Zhang",
        "Ruiyuan Zhang",
        "Jia Zhu",
        "Jiajie Xu",
        "Yaodong Yang",
        "Sirui Han",
        "Yike Guo"
      ],
      "abstract": "Do Large Language Models (LLMs) hold positions that conflict with your\ncountry's values? Occasionally they do! However, existing works primarily focus\non ethical reviews, failing to capture the diversity of national values, which\nencompass broader policy, legal, and moral considerations. Furthermore, current\nbenchmarks that rely on spectrum tests using manually designed questionnaires\nare not easily scalable.\n  To address these limitations, we introduce NaVAB, a comprehensive benchmark\nto evaluate the alignment of LLMs with the values of five major nations: China,\nthe United States, the United Kingdom, France, and Germany. NaVAB implements a\nnational value extraction pipeline to efficiently construct value assessment\ndatasets. Specifically, we propose a modeling procedure with instruction\ntagging to process raw data sources, a screening process to filter\nvalue-related topics and a generation process with a Conflict Reduction\nmechanism to filter non-conflicting values.We conduct extensive experiments on\nvarious LLMs across countries, and the results provide insights into assisting\nin the identification of misaligned scenarios. Moreover, we demonstrate that\nNaVAB can be combined with alignment techniques to effectively reduce value\nconcerns by aligning LLMs' values with the target country.",
      "tldr_zh": "该论文提出了一个名为NaVAB的综合基准，用于评估大型语言模型(LLMs)与五个主要国家（中国、美国、英国、法国和德国）价值观的一致性。NaVAB实施了一个国家价值观提取流程，以高效构建价值观评估数据集，包括指令标记、主题筛选和冲突消减机制。通过在不同国家的LLM上进行大量实验，研究揭示了LLM在价值观上可能存在的偏差。此外，论文还展示了NaVAB可以与对齐技术结合使用，通过将LLM的价值观与目标国家对齐，从而有效减少价值观冲突。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12911v1",
      "published_date": "2025-04-17 13:01:38 UTC",
      "updated_date": "2025-04-17 13:01:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:07:43.870653"
    },
    {
      "arxiv_id": "2504.12898v1",
      "title": "Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models",
      "title_zh": "信息增益引导的因果干预，用于自主消除大型语言模型的偏见\n",
      "authors": [
        "Zhouhao Sun",
        "Xiao Ding",
        "Li Du",
        "Yunpeng Xu",
        "Yixuan Ma",
        "Yang Zhao",
        "Bing Qin",
        "Ting Liu"
      ],
      "abstract": "Despite significant progress, recent studies indicate that current large\nlanguage models (LLMs) may still capture dataset biases and utilize them during\ninference, leading to the poor generalizability of LLMs. However, due to the\ndiversity of dataset biases and the insufficient nature of bias suppression\nbased on in-context learning, the effectiveness of previous prior\nknowledge-based debiasing methods and in-context learning based automatic\ndebiasing methods is limited. To address these challenges, we explore the\ncombination of causal mechanisms with information theory and propose an\ninformation gain-guided causal intervention debiasing (IGCIDB) framework. This\nframework first utilizes an information gain-guided causal intervention method\nto automatically and autonomously balance the distribution of\ninstruction-tuning dataset. Subsequently, it employs a standard supervised\nfine-tuning process to train LLMs on the debiased dataset. Experimental results\nshow that IGCIDB can effectively debias LLM to improve its generalizability\nacross different tasks.",
      "tldr_zh": "该论文提出了一种基于信息增益引导的因果干预去偏框架(IGCIDB)，旨在解决大型语言模型(LLMs)中存在的偏见问题及其导致的泛化能力不足。IGCIDB框架首先利用信息增益引导的因果干预方法，自动平衡指令调优数据集的分布，然后在此基础上进行标准的监督微调，训练LLMs。实验结果表明，IGCIDB能有效去除LLMs中的偏见，并提高其在不同任务中的泛化能力。该方法结合了因果机制与信息论，实现了对数据集偏见的自动和自主平衡。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12898v1",
      "published_date": "2025-04-17 12:39:25 UTC",
      "updated_date": "2025-04-17 12:39:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:07:55.971816"
    },
    {
      "arxiv_id": "2504.12891v1",
      "title": "Are AI agents the new machine translation frontier? Challenges and opportunities of single- and multi-agent systems for multilingual digital communication",
      "title_zh": "AI Agent 会是机器翻译的新前沿吗？单智能体和多智能体系统在多语言数字通信中的挑战与机遇\n",
      "authors": [
        "Vicent Briva-Iglesias"
      ],
      "abstract": "The rapid evolution of artificial intelligence (AI) has introduced AI agents\nas a disruptive paradigm across various industries, yet their application in\nmachine translation (MT) remains underexplored. This paper describes and\nanalyses the potential of single- and multi-agent systems for MT, reflecting on\nhow they could enhance multilingual digital communication. While single-agent\nsystems are well-suited for simpler translation tasks, multi-agent systems,\nwhich involve multiple specialized AI agents collaborating in a structured\nmanner, may offer a promising solution for complex scenarios requiring high\naccuracy, domain-specific knowledge, and contextual awareness. To demonstrate\nthe feasibility of multi-agent workflows in MT, we are conducting a pilot study\nin legal MT. The study employs a multi-agent system involving four specialized\nAI agents for (i) translation, (ii) adequacy review, (iii) fluency review, and\n(iv) final editing. Our findings suggest that multi-agent systems may have the\npotential to significantly improve domain-adaptability and contextual\nawareness, with superior translation quality to traditional MT or single-agent\nsystems. This paper also sets the stage for future research into multi-agent\napplications in MT, integration into professional translation workflows, and\nshares a demo of the system analyzed in the paper.",
      "tldr_zh": "本文探讨了AI智能体在机器翻译(MT)领域的应用潜力，特别是单智能体和多智能体系统如何提升多语言数字交流。虽然单智能体系统适用于简单的翻译任务，但多智能体系统通过多个专业AI智能体的协同工作，有望解决复杂场景中对高精度、领域知识和上下文理解的需求。一项在法律MT领域的初步研究表明，多智能体系统（包括翻译、充分性审查、流畅性审查和最终编辑四个智能体）能够显著提高领域适应性和上下文感知能力，从而提供优于传统MT或单智能体系统的翻译质量。该研究为未来多智能体在MT中的应用、集成到专业翻译工作流程中奠定了基础，并分享了论文中分析的系统演示。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.ET",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12891v1",
      "published_date": "2025-04-17 12:32:18 UTC",
      "updated_date": "2025-04-17 12:32:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:08:07.931962"
    },
    {
      "arxiv_id": "2504.12867v1",
      "title": "EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text Prompting",
      "title_zh": "EmoVoice：基于LLM的具有自由文本提示的情感文本转语音模型\n",
      "authors": [
        "Guanrou Yang",
        "Chen Yang",
        "Qian Chen",
        "Ziyang Ma",
        "Wenxi Chen",
        "Wen Wang",
        "Tianrui Wang",
        "Yifan Yang",
        "Zhikang Niu",
        "Wenrui Liu",
        "Fan Yu",
        "Zhihao Du",
        "Zhifu Gao",
        "ShiLiang Zhang",
        "Xie Chen"
      ],
      "abstract": "Human speech goes beyond the mere transfer of information; it is a profound\nexchange of emotions and a connection between individuals. While Text-to-Speech\n(TTS) models have made huge progress, they still face challenges in controlling\nthe emotional expression in the generated speech. In this work, we propose\nEmoVoice, a novel emotion-controllable TTS model that exploits large language\nmodels (LLMs) to enable fine-grained freestyle natural language emotion\ncontrol, and a phoneme boost variant design that makes the model output phoneme\ntokens and audio tokens in parallel to enhance content consistency, inspired by\nchain-of-thought (CoT) and modality-of-thought (CoM) techniques. Besides, we\nintroduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring\nexpressive speech and fine-grained emotion labels with natural language\ndescriptions. EmoVoice achieves state-of-the-art performance on the English\nEmoVoice-DB test set using only synthetic training data, and on the Chinese\nSecap test set using our in-house data. We further investigate the reliability\nof existing emotion evaluation metrics and their alignment with human\nperceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and\nGemini to assess emotional speech. Demo samples are available at\nhttps://anonymous.4open.science/r/EmoVoice-DF55. Dataset, code, and checkpoints\nwill be released.",
      "tldr_zh": "EmoVoice 是一种新型情感可控的文本到语音(TTS)模型，它利用大型语言模型(LLMs)实现细粒度的自由文本情感控制。该模型采用了一种音素增强变体设计，灵感来源于链式思维(CoT)和模态思维(CoM)技术，并行输出音素和音频token，以增强内容一致性。研究者还构建了一个高质量的40小时英语情感数据集EmoVoice-DB，包含富有表现力的语音和带有自然语言描述的细粒度情感标签。实验结果表明，EmoVoice仅使用合成训练数据，就在英语EmoVoice-DB测试集和中文Secap测试集上取得了最先进的性能。此外，该研究还探讨了现有情感评估指标的可靠性，以及它们与人类感知偏好的对齐情况，并探索使用SOTA多模态LLM GPT-4o-audio和Gemini来评估情感语音。\n",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12867v1",
      "published_date": "2025-04-17 11:50:04 UTC",
      "updated_date": "2025-04-17 11:50:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:08:20.249378"
    },
    {
      "arxiv_id": "2504.12856v1",
      "title": "3D-PNAS: 3D Industrial Surface Anomaly Synthesis with Perlin Noise",
      "title_zh": "3D-PNAS：基于 Perlin 噪声的 3D 工业表面异常合成\n",
      "authors": [
        "Yifeng Cheng",
        "Juan Du"
      ],
      "abstract": "Large pretrained vision foundation models have shown significant potential in\nvarious vision tasks. However, for industrial anomaly detection, the scarcity\nof real defect samples poses a critical challenge in leveraging these models.\nWhile 2D anomaly generation has significantly advanced with established\ngenerative models, the adoption of 3D sensors in industrial manufacturing has\nmade leveraging 3D data for surface quality inspection an emerging trend. In\ncontrast to 2D techniques, 3D anomaly generation remains largely unexplored,\nlimiting the potential of 3D data in industrial quality inspection. To address\nthis gap, we propose a novel yet simple 3D anomaly generation method, 3D-PNAS,\nbased on Perlin noise and surface parameterization. Our method generates\nrealistic 3D surface anomalies by projecting the point cloud onto a 2D plane,\nsampling multi-scale noise values from a Perlin noise field, and perturbing the\npoint cloud along its normal direction. Through comprehensive visualization\nexperiments, we demonstrate how key parameters - including noise scale,\nperturbation strength, and octaves, provide fine-grained control over the\ngenerated anomalies, enabling the creation of diverse defect patterns from\npronounced deformations to subtle surface variations. Additionally, our\ncross-category experiments show that the method produces consistent yet\ngeometrically plausible anomalies across different object types, adapting to\ntheir specific surface characteristics. We also provide a comprehensive\ncodebase and visualization toolkit to facilitate future research.",
      "tldr_zh": "针对工业表面缺陷检测中真实缺陷样本稀缺的问题，该论文提出了一种基于Perlin噪声的3D表面缺陷合成方法3D-PNAS。该方法通过将点云投影到2D平面，从Perlin噪声场中采样多尺度噪声值，并沿其法线方向扰动点云，从而生成逼真的3D表面缺陷。实验表明，通过调整噪声尺度、扰动强度和octaves等关键参数，可以精细控制生成的缺陷，从而创建从显著变形到细微表面变化的各种缺陷模式。此外，该方法还能在不同对象类型上生成一致且几何上合理的缺陷，并适应其特定的表面特征。论文同时提供了一个全面的代码库和可视化工具包，以促进未来的研究。\n",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.RO",
        "I.5.4"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12856v1",
      "published_date": "2025-04-17 11:23:17 UTC",
      "updated_date": "2025-04-17 11:23:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:08:31.937726"
    },
    {
      "arxiv_id": "2504.12841v1",
      "title": "ALT: A Python Package for Lightweight Feature Representation in Time Series Classification",
      "title_zh": "ALT：用于时间序列分类中轻量级特征表示的 Python 包\n",
      "authors": [
        "Balázs P. Halmos",
        "Balázs Hajós",
        "Vince Á. Molnár",
        "Marcell T. Kurbucz",
        "Antal Jakovác"
      ],
      "abstract": "We introduce ALT, an open-source Python package created for efficient and\naccurate time series classification (TSC). The package implements the adaptive\nlaw-based transformation (ALT) algorithm, which transforms raw time series data\ninto a linearly separable feature space using variable-length shifted time\nwindows. This adaptive approach enhances its predecessor, the linear law-based\ntransformation (LLT), by effectively capturing patterns of varying temporal\nscales. The software is implemented for scalability, interpretability, and ease\nof use, achieving state-of-the-art performance with minimal computational\noverhead. Extensive benchmarking on real-world datasets demonstrates the\nutility of ALT for diverse TSC tasks in physics and related domains.",
      "tldr_zh": "该论文介绍了ALT，一个用于时间序列分类(TSC)的开源Python包。ALT实现了自适应律变换(adaptive law-based transformation, ALT)算法，该算法利用变长滑动时间窗口将原始时间序列数据转换为线性可分的特征空间。这种自适应方法改进了线性律变换(linear law-based transformation, LLT)，能够有效地捕获不同时间尺度的模式。ALT具有可扩展性、可解释性和易用性，并以最小的计算开销实现了最先进的性能。在真实数据集上的大量基准测试表明了ALT在物理学和相关领域各种TSC任务中的有效性。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.MS",
        "stat.ML",
        "62M10, 62H30, 68T05, 68T10",
        "I.5.1; I.2.6; G.3; D.2.13"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.12841v1",
      "published_date": "2025-04-17 10:57:29 UTC",
      "updated_date": "2025-04-17 10:57:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:08:43.832972"
    },
    {
      "arxiv_id": "2504.12833v1",
      "title": "Image-Editing Specialists: An RLAIF Approach for Diffusion Models",
      "title_zh": "图像编辑专家：一种用于扩散模型的 RLAIF 方法\n",
      "authors": [
        "Elior Benarous",
        "Yilun Du",
        "Heng Yang"
      ],
      "abstract": "We present a novel approach to training specialized instruction-based\nimage-editing diffusion models, addressing key challenges in structural\npreservation with input images and semantic alignment with user prompts. We\nintroduce an online reinforcement learning framework that aligns the diffusion\nmodel with human preferences without relying on extensive human annotations or\ncurating a large dataset. Our method significantly improves the realism and\nalignment with instructions in two ways. First, the proposed models achieve\nprecise and structurally coherent modifications in complex scenes while\nmaintaining high fidelity in instruction-irrelevant areas. Second, they capture\nfine nuances in the desired edit by leveraging a visual prompt, enabling\ndetailed control over visual edits without lengthy textual prompts. This\napproach simplifies users' efforts to achieve highly specific edits, requiring\nonly 5 reference images depicting a certain concept for training. Experimental\nresults demonstrate that our models can perform intricate edits in complex\nscenes, after just 10 training steps. Finally, we showcase the versatility of\nour method by applying it to robotics, where enhancing the visual realism of\nsimulated environments through targeted sim-to-real image edits improves their\nutility as proxies for real-world settings.",
      "tldr_zh": "本文提出了一种新的方法，用于训练专门的、基于指令的图像编辑扩散模型。该方法通过在线强化学习框架(RLAIF)使扩散模型与人类偏好对齐，无需大量人工标注或数据集。该模型在复杂场景中实现了精确和结构连贯的修改，同时保持了与指令无关区域的高保真度。通过利用视觉提示，模型能够捕捉到所需编辑的细微差别，从而实现对视觉编辑的精细控制，而无需冗长的文本提示。仅需5张参考图像即可进行训练，并在10个训练步骤后即可执行复杂的编辑。该方法还可应用于机器人领域，通过有针对性的sim-to-real图像编辑来增强模拟环境的视觉真实感，从而提高其作为真实世界环境代理的效用。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12833v1",
      "published_date": "2025-04-17 10:46:39 UTC",
      "updated_date": "2025-04-17 10:46:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:08:56.064685"
    },
    {
      "arxiv_id": "2504.12817v1",
      "title": "Explainable Scene Understanding with Qualitative Representations and Graph Neural Networks",
      "title_zh": "基于定性表示和图神经网络的可解释场景理解\n",
      "authors": [
        "Nassim Belmecheri",
        "Arnaud Gotlieb",
        "Nadjib Lazaar",
        "Helge Spieker"
      ],
      "abstract": "This paper investigates the integration of graph neural networks (GNNs) with\nQualitative Explainable Graphs (QXGs) for scene understanding in automated\ndriving. Scene understanding is the basis for any further reactive or proactive\ndecision-making. Scene understanding and related reasoning is inherently an\nexplanation task: why is another traffic participant doing something, what or\nwho caused their actions? While previous work demonstrated QXGs' effectiveness\nusing shallow machine learning models, these approaches were limited to\nanalysing single relation chains between object pairs, disregarding the broader\nscene context. We propose a novel GNN architecture that processes entire graph\nstructures to identify relevant objects in traffic scenes. We evaluate our\nmethod on the nuScenes dataset enriched with DriveLM's human-annotated\nrelevance labels. Experimental results show that our GNN-based approach\nachieves superior performance compared to baseline methods. The model\neffectively handles the inherent class imbalance in relevant object\nidentification tasks while considering the complete spatial-temporal\nrelationships between all objects in the scene. Our work demonstrates the\npotential of combining qualitative representations with deep learning\napproaches for explainable scene understanding in autonomous driving systems.",
      "tldr_zh": "本文提出了一种结合图神经网络(GNNs)和定性可解释图(QXGs)的方法，用于自动驾驶中的场景理解。场景理解是后续决策的基础。该方法旨在解释交通参与者的行为原因。之前的研究表明QXGs在使用浅层机器学习模型时的有效性，但仅限于分析对象对之间的单个关系链，忽略了更广泛的场景上下文。本文提出的新型GNN架构能够处理整个图结构，以识别交通场景中的相关对象。在nuScenes数据集上，该方法优于基线方法，有效地处理了相关对象识别任务中的固有类别不平衡问题，同时考虑了场景中所有对象之间的完整时空关系。该研究展示了将定性表示与深度学习方法相结合，用于自动驾驶系统中可解释的场景理解的潜力。\n",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "Workshop \"Advancing Automated Driving in Highly Interactive Scenarios\n  through Behavior Prediction, Trustworthy AI, and Remote Operations\" @ 36th\n  IEEE Intelligent Vehicles Symposium (IV)",
      "pdf_url": "http://arxiv.org/pdf/2504.12817v1",
      "published_date": "2025-04-17 10:21:30 UTC",
      "updated_date": "2025-04-17 10:21:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:09:08.174420"
    },
    {
      "arxiv_id": "2504.12807v1",
      "title": "Hybrid Dense-UNet201 Optimization for Pap Smear Image Segmentation Using Spider Monkey Optimization",
      "title_zh": "基于蜘蛛猴优化的混合密集型UNet201优化算法用于宫颈抹片图像分割\n",
      "authors": [
        "Ach Khozaimi",
        "Isnani Darti",
        "Syaiful Anam",
        "Wuryansari Muharini Kusumawinahyu"
      ],
      "abstract": "Pap smear image segmentation is crucial for cervical cancer diagnosis.\nHowever, traditional segmentation models often struggle with complex cellular\nstructures and variations in pap smear images. This study proposes a hybrid\nDense-UNet201 optimization approach that integrates a pretrained DenseNet201 as\nthe encoder for the U-Net architecture and optimizes it using the spider monkey\noptimization (SMO) algorithm. The Dense-UNet201 model excelled at feature\nextraction. The SMO was modified to handle categorical and discrete parameters.\nThe SIPaKMeD dataset was used in this study and evaluated using key performance\nmetrics, including loss, accuracy, Intersection over Union (IoU), and Dice\ncoefficient. The experimental results showed that Dense-UNet201 outperformed\nU-Net, Res-UNet50, and Efficient-UNetB0. SMO Dense-UNet201 achieved a\nsegmentation accuracy of 96.16%, an IoU of 91.63%, and a Dice coefficient score\nof 95.63%. These findings underscore the effectiveness of image preprocessing,\npretrained models, and metaheuristic optimization in improving medical image\nanalysis and provide new insights into cervical cell segmentation methods.",
      "tldr_zh": "该研究提出了一种混合Dense-UNet201优化方法，用于宫颈抹片图像分割，以辅助宫颈癌诊断。该方法将预训练的DenseNet201作为U-Net架构的编码器，并使用蜘蛛猴优化(SMO)算法对其进行优化，以增强特征提取能力。实验结果表明，与U-Net、Res-UNet50和Efficient-UNetB0相比，SMO优化的Dense-UNet201在SIPaKMeD数据集上表现更优，分割准确率达到96.16%，IoU为91.63%，Dice系数为95.63%。该研究强调了图像预处理、预训练模型和元启发式优化在改进医学图像分析方面的有效性。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12807v1",
      "published_date": "2025-04-17 10:14:05 UTC",
      "updated_date": "2025-04-17 10:14:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:09:19.992291"
    },
    {
      "arxiv_id": "2504.12806v1",
      "title": "A Numerical Gradient Inversion Attack in Variational Quantum Neural-Networks",
      "title_zh": "变分量子神经网络中的数值梯度反演攻击\n",
      "authors": [
        "Georgios Papadopoulos",
        "Shaltiel Eloul",
        "Yash Satsangi",
        "Jamie Heredge",
        "Niraj Kumar",
        "Chun-Fu Chen",
        "Marco Pistoia"
      ],
      "abstract": "The loss landscape of Variational Quantum Neural Networks (VQNNs) is\ncharacterized by local minima that grow exponentially with increasing qubits.\nBecause of this, it is more challenging to recover information from model\ngradients during training compared to classical Neural Networks (NNs). In this\npaper we present a numerical scheme that successfully reconstructs input\ntraining, real-world, practical data from trainable VQNNs' gradients. Our\nscheme is based on gradient inversion that works by combining gradients\nestimation with the finite difference method and adaptive low-pass filtering.\nThe scheme is further optimized with Kalman filter to obtain efficient\nconvergence. Our experiments show that our algorithm can invert even\nbatch-trained data, given the VQNN model is sufficiently over-parameterized.",
      "tldr_zh": "该论文提出了一种针对变分量子神经网络(VQNNs)的数值梯度反演攻击方法。由于VQNNs的损失函数 landscape 存在大量局部最小值，使得从训练过程中的梯度恢复信息比经典神经网络更具挑战性。该方法结合梯度估计、有限差分法和自适应低通滤波，并通过卡尔曼滤波进行优化，实现了从VQNNs梯度中成功重建输入训练数据，甚至是实际数据的目的。实验表明，即使在批量训练数据的情况下，只要VQNN模型充分过参数化，该算法也能有效进行反演。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 17 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.12806v1",
      "published_date": "2025-04-17 10:12:38 UTC",
      "updated_date": "2025-04-17 10:12:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:09:32.043077"
    },
    {
      "arxiv_id": "2504.12803v1",
      "title": "Enhancing Explainability and Reliable Decision-Making in Particle Swarm Optimization through Communication Topologies",
      "title_zh": "通过通信拓扑增强粒子群优化中的可解释性和可靠决策\n",
      "authors": [
        "Nitin Gupta",
        "Indu Bala",
        "Bapi Dutta",
        "Luis Martínez",
        "Anupam Yadav"
      ],
      "abstract": "Swarm intelligence effectively optimizes complex systems across fields like\nengineering and healthcare, yet algorithm solutions often suffer from low\nreliability due to unclear configurations and hyperparameters. This study\nanalyzes Particle Swarm Optimization (PSO), focusing on how different\ncommunication topologies Ring, Star, and Von Neumann affect convergence and\nsearch behaviors. Using an adapted IOHxplainer , an explainable benchmarking\ntool, we investigate how these topologies influence information flow,\ndiversity, and convergence speed, clarifying the balance between exploration\nand exploitation. Through visualization and statistical analysis, the research\nenhances interpretability of PSO's decisions and provides practical guidelines\nfor choosing suitable topologies for specific optimization tasks. Ultimately,\nthis contributes to making swarm based optimization more transparent, robust,\nand trustworthy.",
      "tldr_zh": "该研究分析了粒子群优化(PSO)中不同的通信拓扑结构（环形、星形和冯·诺依曼结构）如何影响算法的收敛性和搜索行为。通过使用改进的IOHxplainer（一种可解释的基准测试工具），研究考察了这些拓扑结构如何影响信息流、多样性和收敛速度，从而阐明了探索和利用之间的平衡。通过可视化和统计分析，该研究增强了PSO决策的可解释性，并为特定优化任务选择合适的拓扑结构提供了实用指南。最终，这有助于使基于群体的优化更加透明、稳健和值得信赖。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12803v1",
      "published_date": "2025-04-17 10:05:10 UTC",
      "updated_date": "2025-04-17 10:05:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:09:43.885550"
    },
    {
      "arxiv_id": "2504.12782v1",
      "title": "Set You Straight: Auto-Steering Denoising Trajectories to Sidestep Unwanted Concepts",
      "title_zh": "帮你纠正方向：自动引导去噪轨迹以避开不需要的概念\n",
      "authors": [
        "Leyang Li",
        "Shilin Lu",
        "Yan Ren",
        "Adams Wai-Kin Kong"
      ],
      "abstract": "Ensuring the ethical deployment of text-to-image models requires effective\ntechniques to prevent the generation of harmful or inappropriate content. While\nconcept erasure methods offer a promising solution, existing finetuning-based\napproaches suffer from notable limitations. Anchor-free methods risk disrupting\nsampling trajectories, leading to visual artifacts, while anchor-based methods\nrely on the heuristic selection of anchor concepts. To overcome these\nshortcomings, we introduce a finetuning framework, dubbed ANT, which\nAutomatically guides deNoising Trajectories to avoid unwanted concepts. ANT is\nbuilt on a key insight: reversing the condition direction of classifier-free\nguidance during mid-to-late denoising stages enables precise content\nmodification without sacrificing early-stage structural integrity. This\ninspires a trajectory-aware objective that preserves the integrity of the\nearly-stage score function field, which steers samples toward the natural image\nmanifold, without relying on heuristic anchor concept selection. For\nsingle-concept erasure, we propose an augmentation-enhanced weight saliency map\nto precisely identify the critical parameters that most significantly\ncontribute to the unwanted concept, enabling more thorough and efficient\nerasure. For multi-concept erasure, our objective function offers a versatile\nplug-and-play solution that significantly boosts performance. Extensive\nexperiments demonstrate that ANT achieves state-of-the-art results in both\nsingle and multi-concept erasure, delivering high-quality, safe outputs without\ncompromising the generative fidelity. Code is available at\nhttps://github.com/lileyang1210/ANT",
      "tldr_zh": "该论文提出了一种名为ANT (Automatically guides deNoising Trajectories) 的微调框架，用于自动引导去噪轨迹，以避免文本到图像模型生成有害内容。ANT通过在去噪过程的中后期反转classifier-free guidance的条件方向，在不牺牲早期结构完整性的前提下精确修改内容。该方法设计了一个轨迹感知目标，保留早期阶段的score function field的完整性，引导样本朝向自然图像流形，无需启发式anchor concept选择。对于单概念擦除，提出了一种增强型权重显著图，以精确定位对不需要的概念贡献最大的关键参数。实验表明，ANT在单概念和多概念擦除方面均达到了SOTA结果，在不影响生成质量的前提下，提供了高质量、安全的结果。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2504.12782v1",
      "published_date": "2025-04-17 09:29:30 UTC",
      "updated_date": "2025-04-17 09:29:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:09:56.146986"
    },
    {
      "arxiv_id": "2504.12778v1",
      "title": "Towards Lossless Token Pruning in Late-Interaction Retrieval Models",
      "title_zh": "面向晚期交互检索模型中的无损 Token 剪枝\n",
      "authors": [
        "Yuxuan Zong",
        "Benjamin Piwowarski"
      ],
      "abstract": "Late interaction neural IR models like ColBERT offer a competitive\neffectiveness-efficiency trade-off across many benchmarks. However, they\nrequire a huge memory space to store the contextual representation for all the\ndocument tokens. Some works have proposed using either heuristics or\nstatistical-based techniques to prune tokens from each document. This however\ndoesn't guarantee that the removed tokens have no impact on the retrieval\nscore. Our work uses a principled approach to define how to prune tokens\nwithout impacting the score between a document and a query. We introduce three\nregularization losses, that induce a solution with high pruning ratios, as well\nas two pruning strategies. We study them experimentally (in and out-domain),\nshowing that we can preserve ColBERT's performance while using only 30\\% of the\ntokens.",
      "tldr_zh": "该论文提出了一种在晚期交互检索模型（如ColBERT）中实现无损token剪枝的方法。针对ColBERT需要大量内存存储文档token上下文表示的问题，该研究没有采用启发式或统计方法，而是提出了一种基于原则性的方法，以确保剪枝后的token不影响文档和查询之间的检索分数。论文引入了三个正则化损失函数，以实现高剪枝率，并提出了两种剪枝策略。实验结果表明，该方法在保持ColBERT性能的同时，可以将token使用量减少到30%。\n",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted at SIGIR 2025 Full Paper Track",
      "pdf_url": "http://arxiv.org/pdf/2504.12778v1",
      "published_date": "2025-04-17 09:18:58 UTC",
      "updated_date": "2025-04-17 09:18:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:10:07.836154"
    },
    {
      "arxiv_id": "2504.12777v1",
      "title": "Multi-Agent Reinforcement Learning Simulation for Environmental Policy Synthesis",
      "title_zh": "用于环境政策综合的多智能体强化学习模拟\n",
      "authors": [
        "James Rudd-Jones",
        "Mirco Musolesi",
        "María Pérez-Ortiz"
      ],
      "abstract": "Climate policy development faces significant challenges due to deep\nuncertainty, complex system dynamics, and competing stakeholder interests.\nClimate simulation methods, such as Earth System Models, have become valuable\ntools for policy exploration. However, their typical use is for evaluating\npotential polices, rather than directly synthesizing them. The problem can be\ninverted to optimize for policy pathways, but the traditional optimization\napproaches often struggle with non-linear dynamics, heterogeneous agents, and\ncomprehensive uncertainty quantification. We propose a framework for augmenting\nclimate simulations with Multi-Agent Reinforcement Learning (MARL) to address\nthese limitations. We identify key challenges at the interface between climate\nsimulations and the application of MARL in the context of policy synthesis,\nincluding reward definition, scalability with increasing agents and state\nspaces, uncertainty propagation across linked systems, and solution validation.\nAdditionally, we discuss challenges in making MARL-derived solutions\ninterpretable and useful for policy-makers. Our framework provides a foundation\nfor more sophisticated climate policy exploration while acknowledging important\nlimitations and areas for future research.",
      "tldr_zh": "本文提出了一种利用多智能体强化学习(MARL)增强气候模拟的框架，用于环境政策的综合制定。该框架旨在解决气候政策制定中由于不确定性、复杂系统动态和利益相关者竞争而面临的挑战。通过MARL，该框架能够优化政策路径，克服传统优化方法在非线性动态、异构智能体和不确定性量化方面的局限性。文章重点讨论了在气候模拟和MARL应用中面临的关键挑战，包括奖励定义、智能体和状态空间的可扩展性、跨系统的不确定性传播和解决方案验证。此外，还探讨了如何使MARL导出的解决方案更具可解释性，从而为政策制定者提供有用的信息。该框架为更复杂的气候政策探索奠定了基础，同时也承认了重要的局限性和未来研究方向。\n",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "Published in AAMAS'25 Blue Sky Ideas Track",
      "pdf_url": "http://arxiv.org/pdf/2504.12777v1",
      "published_date": "2025-04-17 09:18:04 UTC",
      "updated_date": "2025-04-17 09:18:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:10:20.269300"
    },
    {
      "arxiv_id": "2504.12773v1",
      "title": "Enhancing the Geometric Problem-Solving Ability of Multimodal LLMs via Symbolic-Neural Integration",
      "title_zh": "通过符号-神经集成增强多模态LLM的几何问题求解能力\n",
      "authors": [
        "Yicheng Pan",
        "Zhenrong Zhang",
        "Pengfei Hu",
        "Jiefeng Ma",
        "Jun Du",
        "Jianshu Zhang",
        "Quan Liu",
        "Jianqing Gao",
        "Feng Ma"
      ],
      "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have achieved\nremarkable progress in general domains and demonstrated promise in multimodal\nmathematical reasoning. However, applying MLLMs to geometry problem solving\n(GPS) remains challenging due to lack of accurate step-by-step solution data\nand severe hallucinations during reasoning. In this paper, we propose GeoGen, a\npipeline that can automatically generates step-wise reasoning paths for\ngeometry diagrams. By leveraging the precise symbolic reasoning,\n\\textbf{GeoGen} produces large-scale, high-quality question-answer pairs. To\nfurther enhance the logical reasoning ability of MLLMs, we train\n\\textbf{GeoLogic}, a Large Language Model (LLM) using synthetic data generated\nby GeoGen. Serving as a bridge between natural language and symbolic systems,\nGeoLogic enables symbolic tools to help verifying MLLM outputs, making the\nreasoning process more rigorous and alleviating hallucinations. Experimental\nresults show that our approach consistently improves the performance of MLLMs,\nachieving remarkable results on benchmarks for geometric reasoning tasks. This\nimprovement stems from our integration of the strengths of LLMs and symbolic\nsystems, which enables a more reliable and interpretable approach for the GPS\ntask. Codes are available at https://github.com/ycpNotFound/GeoGen.",
      "tldr_zh": "该论文提出了GeoGen，一个自动生成几何问题逐步推理路径的流程，旨在提升多模态大语言模型(MLLMs)在几何问题求解(GPS)上的能力。GeoGen利用精确的符号推理生成大规模、高质量的问答对。同时，论文训练了GeoLogic，一个利用GeoGen合成数据的大语言模型(LLM)，作为自然语言和符号系统之间的桥梁，帮助验证MLLM的输出，使推理过程更严谨并减少幻觉。实验结果表明，该方法持续提升了MLLM在几何推理任务上的性能，并在基准测试中取得了显著成果，这归功于LLM和符号系统优势的结合，为GPS任务提供了一种更可靠和可解释的方法。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.12773v1",
      "published_date": "2025-04-17 09:13:46 UTC",
      "updated_date": "2025-04-17 09:13:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:10:32.194692"
    },
    {
      "arxiv_id": "2504.12757v1",
      "title": "MCP Guardian: A Security-First Layer for Safeguarding MCP-Based AI System",
      "title_zh": "MCP Guardian：用于保护基于 MCP 的 AI 系统的安全优先层\n",
      "authors": [
        "Sonu Kumar",
        "Anubhav Girdhar",
        "Ritesh Patil",
        "Divyansh Tripathi"
      ],
      "abstract": "As Agentic AI gain mainstream adoption, the industry invests heavily in model\ncapabilities, achieving rapid leaps in reasoning and quality. However, these\nsystems remain largely confined to data silos, and each new integration\nrequires custom logic that is difficult to scale. The Model Context Protocol\n(MCP) addresses this challenge by defining a universal, open standard for\nsecurely connecting AI-based applications (MCP clients) to data sources (MCP\nservers). However, the flexibility of the MCP introduces new risks, including\nmalicious tool servers and compromised data integrity. We present MCP Guardian,\na framework that strengthens MCP-based communication with authentication,\nrate-limiting, logging, tracing, and Web Application Firewall (WAF) scanning.\nThrough real-world scenarios and empirical testing, we demonstrate how MCP\nGuardian effectively mitigates attacks and ensures robust oversight with\nminimal overheads. Our approach fosters secure, scalable data access for AI\nassistants, underscoring the importance of a defense-in-depth approach that\nenables safer and more transparent innovation in AI-driven environments.",
      "tldr_zh": "该论文提出了MCP Guardian，一个用于保护基于模型上下文协议(MCP)的AI系统的安全框架。MCP旨在为AI应用安全连接数据源定义一个通用开放标准，但同时也引入了恶意工具服务器和数据完整性受损等新风险。MCP Guardian通过身份验证、速率限制、日志记录、追踪和Web应用防火墙(WAF)扫描来加强MCP通信，有效缓解攻击并确保强大的监管。实验表明，MCP Guardian在实际场景中能够以最小的开销实现安全、可扩展的数据访问，强调了在AI驱动环境中采用深度防御方法以实现更安全、更透明创新的重要性。\n",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12757v1",
      "published_date": "2025-04-17 08:49:10 UTC",
      "updated_date": "2025-04-17 08:49:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:10:44.059132"
    },
    {
      "arxiv_id": "2504.12755v1",
      "title": "Trajectory Adaptation using Large Language Models",
      "title_zh": "使用大型语言模型进行轨迹调整\n",
      "authors": [
        "Anurag Maurya",
        "Tashmoy Ghosh",
        "Ravi Prakash"
      ],
      "abstract": "Adapting robot trajectories based on human instructions as per new situations\nis essential for achieving more intuitive and scalable human-robot\ninteractions. This work proposes a flexible language-based framework to adapt\ngeneric robotic trajectories produced by off-the-shelf motion planners like\nRRT, A-star, etc, or learned from human demonstrations. We utilize pre-trained\nLLMs to adapt trajectory waypoints by generating code as a policy for dense\nrobot manipulation, enabling more complex and flexible instructions than\ncurrent methods. This approach allows us to incorporate a broader range of\ncommands, including numerical inputs. Compared to state-of-the-art\nfeature-based sequence-to-sequence models which require training, our method\ndoes not require task-specific training and offers greater interpretability and\nmore effective feedback mechanisms. We validate our approach through simulation\nexperiments on the robotic manipulator, aerial vehicle, and ground robot in the\nPybullet and Gazebo simulation environments, demonstrating that LLMs can\nsuccessfully adapt trajectories to complex human instructions.",
      "tldr_zh": "该论文提出了一种基于大型语言模型(LLM)的灵活框架，用于根据人类指令调整机器人轨迹。该方法利用预训练的LLM生成代码作为策略，从而调整由RRT、A-star等运动规划器生成的通用轨迹。与需要特定任务训练的现有方法相比，该方法无需训练，并提供更强的可解释性和有效的反馈机制。通过在Pybullet和Gazebo仿真环境中对机器人机械臂、无人机和地面机器人进行仿真实验，验证了LLM能够成功地将轨迹调整为复杂的人类指令。\n",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to CoRL LangRob workshop 2024",
      "pdf_url": "http://arxiv.org/pdf/2504.12755v1",
      "published_date": "2025-04-17 08:48:23 UTC",
      "updated_date": "2025-04-17 08:48:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:10:55.876278"
    },
    {
      "arxiv_id": "2504.12740v1",
      "title": "GPMFS: Global Foundation and Personalized Optimization for Multi-Label Feature Selection",
      "title_zh": "GPMFS：多标签特征选择的全局基础和个性化优化\n",
      "authors": [
        "Yifan Cao",
        "Zhilong Mi",
        "Ziqiao Yin",
        "Binghui Guo",
        "Jin Dong"
      ],
      "abstract": "As artificial intelligence methods are increasingly applied to complex task\nscenarios, high dimensional multi-label learning has emerged as a prominent\nresearch focus. At present, the curse of dimensionality remains one of the\nmajor bottlenecks in high-dimensional multi-label learning, which can be\neffectively addressed through multi-label feature selection methods. However,\nexisting multi-label feature selection methods mostly focus on identifying\nglobal features shared across all labels, which overlooks personalized\ncharacteristics and specific requirements of individual labels. This\nglobal-only perspective may limit the ability to capture label-specific\ndiscriminative information, thereby affecting overall performance. In this\npaper, we propose a novel method called GPMFS (Global Foundation and\nPersonalized Optimization for Multi-Label Feature Selection). GPMFS firstly\nidentifies global features by exploiting label correlations, then adaptively\nsupplements each label with a personalized subset of discriminative features\nusing a threshold-controlled strategy. Experiments on multiple real-world\ndatasets demonstrate that GPMFS achieves superior performance while maintaining\nstrong interpretability and robustness. Furthermore, GPMFS provides insights\ninto the label-specific strength across different multi-label datasets, thereby\ndemonstrating the necessity and potential applicability of personalized feature\nselection approaches.",
      "tldr_zh": "这篇论文提出了GPMFS (Global Foundation and Personalized Optimization for Multi-Label Feature Selection)，一种新的多标签特征选择方法，旨在解决现有方法忽略标签个性化特征的问题。GPMFS首先利用标签相关性识别全局特征，然后通过阈值控制策略自适应地为每个标签补充个性化的判别特征子集。在多个真实数据集上的实验表明，GPMFS在保持强解释性和鲁棒性的同时，实现了卓越的性能。该研究还揭示了不同多标签数据集中标签特定强度的差异，论证了个性化特征选择方法的必要性和潜在适用性。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12740v1",
      "published_date": "2025-04-17 08:29:14 UTC",
      "updated_date": "2025-04-17 08:29:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:11:07.948052"
    },
    {
      "arxiv_id": "2504.12735v1",
      "title": "The Athenian Academy: A Seven-Layer Architecture Model for Multi-Agent Systems",
      "title_zh": "雅典学院：一种用于多智能体系统的七层架构模型\n",
      "authors": [
        "Lidong Zhai",
        "Zhijie Qiu",
        "Xizhong Guo",
        "Jiaqi Li"
      ],
      "abstract": "This paper proposes the \"Academy of Athens\" multi-agent seven-layer\nframework, aimed at systematically addressing challenges in multi-agent systems\n(MAS) within artificial intelligence (AI) art creation, such as collaboration\nefficiency, role allocation, environmental adaptation, and task parallelism.\nThe framework divides MAS into seven layers: multi-agent collaboration,\nsingle-agent multi-role playing, single-agent multi-scene traversal,\nsingle-agent multi-capability incarnation, different single agents using the\nsame large model to achieve the same target agent, single-agent using different\nlarge models to achieve the same target agent, and multi-agent synthesis of the\nsame target agent. Through experimental validation in art creation, the\nframework demonstrates its unique advantages in task collaboration, cross-scene\nadaptation, and model fusion. This paper further discusses current challenges\nsuch as collaboration mechanism optimization, model stability, and system\nsecurity, proposing future exploration through technologies like meta-learning\nand federated learning. The framework provides a structured methodology for\nmulti-agent collaboration in AI art creation and promotes innovative\napplications in the art field.",
      "tldr_zh": "本文提出了“雅典学院”多智能体七层框架，旨在系统性地解决人工智能（AI）艺术创作中多智能体系统（MAS）面临的挑战，如协作效率、角色分配、环境适应和任务并行性。该框架将MAS分为七层，包括多智能体协作、单智能体多角色扮演、单智能体多场景遍历等。通过在艺术创作中的实验验证，该框架展示了其在任务协作、跨场景适应和模型融合方面的独特优势。本文还讨论了当前挑战，如协作机制优化、模型稳定性和系统安全性，并提出了未来通过元学习和联邦学习等技术进行探索的方向。该框架为AI艺术创作中的多智能体协作提供了一种结构化的方法，并促进了艺术领域的创新应用。\n",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12735v1",
      "published_date": "2025-04-17 08:21:28 UTC",
      "updated_date": "2025-04-17 08:21:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:11:20.041150"
    },
    {
      "arxiv_id": "2504.12734v1",
      "title": "Pandora: A Code-Driven Large Language Model Agent for Unified Reasoning Across Diverse Structured Knowledge",
      "title_zh": "Pandora：一种代码驱动的大型语言模型智能体，用于跨多样化结构化知识的统一推理\n",
      "authors": [
        "Yongrui Chen",
        "Junhao He",
        "Linbo Fu",
        "Shenyu Zhang",
        "Rihui Jin",
        "Xinbang Dai",
        "Jiaqi Li",
        "Dehai Min",
        "Nan Hu",
        "Yuxin Zhang",
        "Guilin Qi",
        "Yi Huang",
        "Tongtong Wu"
      ],
      "abstract": "Unified Structured Knowledge Reasoning (USKR) aims to answer natural language\nquestions (NLQs) by using structured sources such as tables, databases, and\nknowledge graphs in a unified way. Existing USKR methods either rely on\nemploying task-specific strategies or custom-defined representations, which\nstruggle to leverage the knowledge transfer between different SKR tasks or\nalign with the prior of LLMs, thereby limiting their performance. This paper\nproposes a novel USKR framework named \\textsc{Pandora}, which takes advantage\nof \\textsc{Python}'s \\textsc{Pandas} API to construct a unified knowledge\nrepresentation for alignment with LLM pre-training. It employs an LLM to\ngenerate textual reasoning steps and executable Python code for each question.\nDemonstrations are drawn from a memory of training examples that cover various\nSKR tasks, facilitating knowledge transfer. Extensive experiments on four\nbenchmarks involving three SKR tasks demonstrate that \\textsc{Pandora}\noutperforms existing unified frameworks and competes effectively with\ntask-specific methods.",
      "tldr_zh": "Pandora是一个新颖的统一结构化知识推理(USKR)框架，旨在利用大型语言模型(LLM)解决跨表格、数据库和知识图谱等多种结构化数据源的自然语言问题。Pandora利用Python的Pandas API构建统一的知识表示，与LLM的预训练对齐。该框架使用LLM生成文本推理步骤和可执行的Python代码来回答问题，并从包含各种SKR任务的训练示例记忆中提取示范，促进知识迁移。在四个基准测试上的实验表明，Pandora优于现有的统一框架，并能与特定任务的方法有效竞争。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12734v1",
      "published_date": "2025-04-17 08:18:09 UTC",
      "updated_date": "2025-04-17 08:18:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:11:32.114553"
    },
    {
      "arxiv_id": "2504.12722v1",
      "title": "SimUSER: Simulating User Behavior with Large Language Models for Recommender System Evaluation",
      "title_zh": "SimUSER：使用大型语言模型模拟用户行为以评估推荐系统\n",
      "authors": [
        "Nicolas Bougie",
        "Narimasa Watanabe"
      ],
      "abstract": "Recommender systems play a central role in numerous real-life applications,\nyet evaluating their performance remains a significant challenge due to the gap\nbetween offline metrics and online behaviors. Given the scarcity and limits\n(e.g., privacy issues) of real user data, we introduce SimUSER, an agent\nframework that serves as believable and cost-effective human proxies. SimUSER\nfirst identifies self-consistent personas from historical data, enriching user\nprofiles with unique backgrounds and personalities. Then, central to this\nevaluation are users equipped with persona, memory, perception, and brain\nmodules, engaging in interactions with the recommender system. SimUSER exhibits\ncloser alignment with genuine humans than prior work, both at micro and macro\nlevels. Additionally, we conduct insightful experiments to explore the effects\nof thumbnails on click rates, the exposure effect, and the impact of reviews on\nuser engagement. Finally, we refine recommender system parameters based on\noffline A/B test results, resulting in improved user engagement in the real\nworld.",
      "tldr_zh": "该论文提出了SimUSER，一个利用大型语言模型(LLMs)模拟用户行为的智能体框架，旨在解决推荐系统评估中离线指标与在线行为脱节的难题。SimUSER通过从历史数据中识别具有一致性的用户画像，并利用persona、记忆、感知和大脑模块来模拟用户与推荐系统的交互。实验表明，SimUSER在微观和宏观层面都比以往工作更接近真实用户。此外，论文还探讨了缩略图、曝光效应和评论对用户参与度的影响，并基于离线A/B测试结果优化了推荐系统参数，从而提高了真实世界中的用户参与度。\n",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12722v1",
      "published_date": "2025-04-17 07:57:23 UTC",
      "updated_date": "2025-04-17 07:57:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:11:44.094217"
    },
    {
      "arxiv_id": "2504.12721v1",
      "title": "TimeCapsule: Solving the Jigsaw Puzzle of Long-Term Time Series Forecasting with Compressed Predictive Representations",
      "title_zh": "TimeCapsule：利用压缩预测表征解决长期时间序列预测的拼图难题\n",
      "authors": [
        "Yihang Lu",
        "Yangyang Xu",
        "Qitao Qing",
        "Xianwei Meng"
      ],
      "abstract": "Recent deep learning models for Long-term Time Series Forecasting (LTSF)\noften emphasize complex, handcrafted designs, while simpler architectures like\nlinear models or MLPs have often outperformed these intricate solutions. In\nthis paper, we revisit and organize the core ideas behind several key\ntechniques, such as redundancy reduction and multi-scale modeling, which are\nfrequently employed in advanced LTSF models. Our goal is to streamline these\nideas for more efficient deep learning utilization. To this end, we introduce\nTimeCapsule, a model built around the principle of high-dimensional information\ncompression that unifies these techniques in a generalized yet simplified\nframework. Specifically, we model time series as a 3D tensor, incorporating\ntemporal, variate, and level dimensions, and leverage mode production to\ncapture multi-mode dependencies while achieving dimensionality compression. We\npropose an internal forecast within the compressed representation domain,\nsupported by the Joint-Embedding Predictive Architecture (JEPA), to monitor the\nlearning of predictive representations. Extensive experiments on challenging\nbenchmarks demonstrate the versatility of our method, showing that TimeCapsule\ncan achieve state-of-the-art performance.",
      "tldr_zh": "该论文提出了TimeCapsule模型，旨在解决长期时间序列预测(LTSF)问题，并简化现有复杂深度学习模型的设计。TimeCapsule基于高维信息压缩的原则，将时间序列建模为3D张量，并结合时间、变量和层级维度。该模型利用mode production捕获多模态依赖关系，同时实现降维。TimeCapsule采用Joint-Embedding Predictive Architecture (JEPA)支持的内部预测，以监控预测表示的学习。实验结果表明，TimeCapsule在多个基准测试中达到了state-of-the-art的性能，验证了其通用性。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12721v1",
      "published_date": "2025-04-17 07:54:26 UTC",
      "updated_date": "2025-04-17 07:54:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:11:56.151187"
    },
    {
      "arxiv_id": "2504.12718v1",
      "title": "TUMLS: Trustful Fully Unsupervised Multi-Level Segmentation for Whole Slide Images of Histology",
      "title_zh": "TUMLS：用于组织学全切片图像的可信全无监督多层次分割\n",
      "authors": [
        "Walid Rehamnia",
        "Alexandra Getmanskaya",
        "Evgeniy Vasilyev",
        "Vadim Turlapov"
      ],
      "abstract": "Digital pathology, augmented by artificial intelligence (AI), holds\nsignificant promise for improving the workflow of pathologists. However,\nchallenges such as the labor-intensive annotation of whole slide images (WSIs),\nhigh computational demands, and trust concerns arising from the absence of\nuncertainty estimation in predictions hinder the practical application of\ncurrent AI methodologies in histopathology. To address these issues, we present\na novel trustful fully unsupervised multi-level segmentation methodology\n(TUMLS) for WSIs. TUMLS adopts an autoencoder (AE) as a feature extractor to\nidentify the different tissue types within low-resolution training data. It\nselects representative patches from each identified group based on an\nuncertainty measure and then does unsupervised nuclei segmentation in their\nrespective higher-resolution space without using any ML algorithms. Crucially,\nthis solution integrates seamlessly into clinicians workflows, transforming the\nexamination of a whole WSI into a review of concise, interpretable cross-level\ninsights. This integration significantly enhances and accelerates the workflow\nwhile ensuring transparency. We evaluated our approach using the UPENN-GBM\ndataset, where the AE achieved a mean squared error (MSE) of 0.0016.\nAdditionally, nucleus segmentation is assessed on the MoNuSeg dataset,\noutperforming all unsupervised approaches with an F1 score of 77.46% and a\nJaccard score of 63.35%. These results demonstrate the efficacy of TUMLS in\nadvancing the field of digital pathology.",
      "tldr_zh": "该论文提出了一个名为TUMLS的信任式全无监督多层次分割方法，用于组织学全切片图像(WSI)分析。TUMLS利用自编码器(AE)从低分辨率图像中提取特征并识别不同组织类型，然后基于不确定性度量选择代表性切片，并在高分辨率空间中进行无监督的细胞核分割，无需任何机器学习算法。该方法能够将WSI的检查转化为对简洁、可解释的跨层次信息的审查，从而显著增强和加速病理医生的工作流程，并确保透明度。在UPENN-GBM数据集上，AE取得了0.0016的均方误差(MSE)。在MoNuSeg数据集上，细胞核分割的F1 score达到77.46%，Jaccard score达到63.35%，优于所有无监督方法，验证了TUMLS在数字病理学领域的有效性。\n",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "I.2.6; I.2.10; I.4.6; I.5.3; I.5.4"
      ],
      "primary_category": "eess.IV",
      "comment": "32 pages, 15 figures, 3 tables, 42 references",
      "pdf_url": "http://arxiv.org/pdf/2504.12718v1",
      "published_date": "2025-04-17 07:48:05 UTC",
      "updated_date": "2025-04-17 07:48:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:12:08.321030"
    },
    {
      "arxiv_id": "2504.12717v1",
      "title": "Post-pre-training for Modality Alignment in Vision-Language Foundation Models",
      "title_zh": "用于视觉-语言基础模型中模态对齐的后-预训练方法\n",
      "authors": [
        "Shin'ya Yamaguchi",
        "Dewei Feng",
        "Sekitoshi Kanai",
        "Kazuki Adachi",
        "Daiki Chijiwa"
      ],
      "abstract": "Contrastive language image pre-training (CLIP) is an essential component of\nbuilding modern vision-language foundation models. While CLIP demonstrates\nremarkable zero-shot performance on downstream tasks, the multi-modal feature\nspaces still suffer from a modality gap, which is a gap between image and text\nfeature clusters and limits downstream task performance. Although existing\nworks attempt to address the modality gap by modifying pre-training or\nfine-tuning, they struggle with heavy training costs with large datasets or\ndegradations of zero-shot performance. This paper presents CLIP-Refine, a\npost-pre-training method for CLIP models at a phase between pre-training and\nfine-tuning. CLIP-Refine aims to align the feature space with 1 epoch training\non small image-text datasets without zero-shot performance degradations. To\nthis end, we introduce two techniques: random feature alignment (RaFA) and\nhybrid contrastive-distillation (HyCD). RaFA aligns the image and text features\nto follow a shared prior distribution by minimizing the distance to random\nreference vectors sampled from the prior. HyCD updates the model with hybrid\nsoft labels generated by combining ground-truth image-text pair labels and\noutputs from the pre-trained CLIP model. This contributes to achieving both\nmaintaining the past knowledge and learning new knowledge to align features.\nOur extensive experiments with multiple classification and retrieval tasks show\nthat CLIP-Refine succeeds in mitigating the modality gap and improving the\nzero-shot performance.",
      "tldr_zh": "本文提出了一种名为CLIP-Refine的后预训练方法，用于缓解视觉-语言基础模型中存在的模态差异(modality gap)问题。该方法在预训练和微调之间进行，旨在通过在小规模图像-文本数据集上进行单轮训练，对齐特征空间，且不影响模型的zero-shot性能。CLIP-Refine包含两项关键技术：随机特征对齐(RaFA)和混合对比-蒸馏(HyCD)。RaFA通过最小化图像和文本特征与从先验分布中抽取的随机参考向量之间的距离，使它们遵循共同的先验分布。HyCD使用混合软标签更新模型，这些标签结合了ground-truth的图像-文本对标签和预训练CLIP模型的输出。实验结果表明，CLIP-Refine能够有效缩小模态差异，提升zero-shot性能。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CVPR 2025; Code: https://github.com/yshinya6/clip-refine",
      "pdf_url": "http://arxiv.org/pdf/2504.12717v1",
      "published_date": "2025-04-17 07:46:19 UTC",
      "updated_date": "2025-04-17 07:46:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:12:20.410811"
    },
    {
      "arxiv_id": "2504.12714v1",
      "title": "Cross-environment Cooperation Enables Zero-shot Multi-agent Coordination",
      "title_zh": "跨环境合作实现零样本多智能体协同\n",
      "authors": [
        "Kunal Jha",
        "Wilka Carvalho",
        "Yancheng Liang",
        "Simon S. Du",
        "Max Kleiman-Weiner",
        "Natasha Jaques"
      ],
      "abstract": "Zero-shot coordination (ZSC), the ability to adapt to a new partner in a\ncooperative task, is a critical component of human-compatible AI. While prior\nwork has focused on training agents to cooperate on a single task, these\nspecialized models do not generalize to new tasks, even if they are highly\nsimilar. Here, we study how reinforcement learning on a distribution of\nenvironments with a single partner enables learning general cooperative skills\nthat support ZSC with many new partners on many new problems. We introduce two\nJax-based, procedural generators that create billions of solvable coordination\nchallenges. We develop a new paradigm called Cross-Environment Cooperation\n(CEC), and show that it outperforms competitive baselines quantitatively and\nqualitatively when collaborating with real people. Our findings suggest that\nlearning to collaborate across many unique scenarios encourages agents to\ndevelop general norms, which prove effective for collaboration with different\npartners. Together, our results suggest a new route toward designing generalist\ncooperative agents capable of interacting with humans without requiring human\ndata.",
      "tldr_zh": "该研究探讨了如何通过在多种环境中与单一伙伴进行强化学习，使智能体获得通用的合作技能，从而实现零样本多智能体协作(ZSC)。研究者提出了跨环境合作(CEC)范式，并使用基于Jax的程序生成器创建了数十亿个可解决的协作挑战。实验结果表明，CEC在与真人合作时，在定量和定性上均优于现有基线模型。该研究表明，在多个独特场景中学习协作可以鼓励智能体发展通用的合作规范，从而有效地与不同的伙伴进行协作，为设计无需人类数据的通用协作智能体提供了一条新途径。\n",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "Accepted to CogSci 2025, In-review for ICML 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.12714v1",
      "published_date": "2025-04-17 07:41:25 UTC",
      "updated_date": "2025-04-17 07:41:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:12:32.132766"
    },
    {
      "arxiv_id": "2504.12711v1",
      "title": "NTIRE 2025 Challenge on Day and Night Raindrop Removal for Dual-Focused Images: Methods and Results",
      "title_zh": "NTIRE 2025 双焦点图像日夜雨滴去除挑战赛：方法与结果\n",
      "authors": [
        "Xin Li",
        "Yeying Jin",
        "Xin Jin",
        "Zongwei Wu",
        "Bingchen Li",
        "Yufei Wang",
        "Wenhan Yang",
        "Yu Li",
        "Zhibo Chen",
        "Bihan Wen",
        "Robby T. Tan",
        "Radu Timofte",
        "Qiyu Rong",
        "Hongyuan Jing",
        "Mengmeng Zhang",
        "Jinglong Li",
        "Xiangyu Lu",
        "Yi Ren",
        "Yuting Liu",
        "Meng Zhang",
        "Xiang Chen",
        "Qiyuan Guan",
        "Jiangxin Dong",
        "Jinshan Pan",
        "Conglin Gou",
        "Qirui Yang",
        "Fangpu Zhang",
        "Yunlong Lin",
        "Sixiang Chen",
        "Guoxi Huang",
        "Ruirui Lin",
        "Yan Zhang",
        "Jingyu Yang",
        "Huanjing Yue",
        "Jiyuan Chen",
        "Qiaosi Yi",
        "Hongjun Wang",
        "Chenxi Xie",
        "Shuai Li",
        "Yuhui Wu",
        "Kaiyi Ma",
        "Jiakui Hu",
        "Juncheng Li",
        "Liwen Pan",
        "Guangwei Gao",
        "Wenjie Li",
        "Zhenyu Jin",
        "Heng Guo",
        "Zhanyu Ma",
        "Yubo Wang",
        "Jinghua Wang",
        "Wangzhi Xing",
        "Anjusree Karnavar",
        "Diqi Chen",
        "Mohammad Aminul Islam",
        "Hao Yang",
        "Ruikun Zhang",
        "Liyuan Pan",
        "Qianhao Luo",
        "XinCao",
        "Han Zhou",
        "Yan Min",
        "Wei Dong",
        "Jun Chen",
        "Taoyi Wu",
        "Weijia Dou",
        "Yu Wang",
        "Shengjie Zhao",
        "Yongcheng Huang",
        "Xingyu Han",
        "Anyan Huang",
        "Hongtao Wu",
        "Hong Wang",
        "Yefeng Zheng",
        "Abhijeet Kumar",
        "Aman Kumar",
        "Marcos V. Conde",
        "Paula Garrido",
        "Daniel Feijoo",
        "Juan C. Benito",
        "Guanglu Dong",
        "Xin Lin",
        "Siyuan Liu",
        "Tianheng Zheng",
        "Jiayu Zhong",
        "Shouyi Wang",
        "Xiangtai Li",
        "Lanqing Guo",
        "Lu Qi",
        "Chao Ren",
        "Shuaibo Wang",
        "Shilong Zhang",
        "Wanyu Zhou",
        "Yunze Wu",
        "Qinzhong Tan",
        "Jieyuan Pei",
        "Zhuoxuan Li",
        "Jiayu Wang",
        "Haoyu Bian",
        "Haoran Sun",
        "Subhajit Paul",
        "Ni Tang",
        "Junhao Huang",
        "Zihan Cheng",
        "Hongyun Zhu",
        "Yuehan Wu",
        "Kaixin Deng",
        "Hang Ouyang",
        "Tianxin Xiao",
        "Fan Yang",
        "Zhizun Luo",
        "Zeyu Xiao",
        "Zhuoyuan Li",
        "Nguyen Pham Hoang Le",
        "An Dinh Thien",
        "Son T. Luu",
        "Kiet Van Nguyen",
        "Ronghua Xu",
        "Xianmin Tian",
        "Weijian Zhou",
        "Jiacheng Zhang",
        "Yuqian Chen",
        "Yihang Duan",
        "Yujie Wu",
        "Suresh Raikwar",
        "Arsh Garg",
        "Kritika",
        "Jianhua Zheng",
        "Xiaoshan Ma",
        "Ruolin Zhao",
        "Yongyu Yang",
        "Yongsheng Liang",
        "Guiming Huang",
        "Qiang Li",
        "Hongbin Zhang",
        "Xiangyu Zheng",
        "A. N. Rajagopalan"
      ],
      "abstract": "This paper reviews the NTIRE 2025 Challenge on Day and Night Raindrop Removal\nfor Dual-Focused Images. This challenge received a wide range of impressive\nsolutions, which are developed and evaluated using our collected real-world\nRaindrop Clarity dataset. Unlike existing deraining datasets, our Raindrop\nClarity dataset is more diverse and challenging in degradation types and\ncontents, which includes day raindrop-focused, day background-focused, night\nraindrop-focused, and night background-focused degradations. This dataset is\ndivided into three subsets for competition: 14,139 images for training, 240\nimages for validation, and 731 images for testing. The primary objective of\nthis challenge is to establish a new and powerful benchmark for the task of\nremoving raindrops under varying lighting and focus conditions. There are a\ntotal of 361 participants in the competition, and 32 teams submitting valid\nsolutions and fact sheets for the final testing phase. These submissions\nachieved state-of-the-art (SOTA) performance on the Raindrop Clarity dataset.\nThe project can be found at\nhttps://lixinustc.github.io/CVPR-NTIRE2025-RainDrop-Competition.github.io/.",
      "tldr_zh": "本文回顾了NTIRE 2025关于双焦点图像的昼夜雨滴去除挑战赛。该挑战赛使用收集的真实世界Raindrop Clarity数据集，促进了多种解决方案的开发和评估。Raindrop Clarity数据集包含白天雨滴聚焦、白天背景聚焦、夜间雨滴聚焦和夜间背景聚焦等多种降级类型和内容，比现有的去雨数据集更具多样性和挑战性。比赛共收到32个团队的有效解决方案，并在Raindrop Clarity数据集上取得了state-of-the-art (SOTA) 的性能。该项目旨在为不同光照和聚焦条件下的雨滴去除任务建立一个新的强大基准。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "Challenge Report of CVPR NTIRE 2025; 26 pages; Methods from 32 teams",
      "pdf_url": "http://arxiv.org/pdf/2504.12711v1",
      "published_date": "2025-04-17 07:35:35 UTC",
      "updated_date": "2025-04-17 07:35:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:12:44.171934"
    },
    {
      "arxiv_id": "2504.12682v1",
      "title": "WebLists: Extracting Structured Information From Complex Interactive Websites Using Executable LLM Agents",
      "title_zh": "WebLists：使用可执行的 LLM 智能体从复杂交互式网站提取结构化信息\n",
      "authors": [
        "Arth Bohra",
        "Manvel Saroyan",
        "Danil Melkozerov",
        "Vahe Karufanyan",
        "Gabriel Maher",
        "Pascal Weinberger",
        "Artem Harutyunyan",
        "Giovanni Campagna"
      ],
      "abstract": "Most recent web agent research has focused on navigation and transaction\ntasks, with little emphasis on extracting structured data at scale. We present\nWebLists, a benchmark of 200 data-extraction tasks across four common business\nand enterprise use-cases. Each task requires an agent to navigate to a webpage,\nconfigure it appropriately, and extract complete datasets with well-defined\nschemas. We show that both LLMs with search capabilities and SOTA web agents\nstruggle with these tasks, with a recall of 3% and 31%, respectively, despite\nhigher performance on question-answering tasks.\n  To address this challenge, we propose BardeenAgent, a novel framework that\nenables web agents to convert their execution into repeatable programs, and\nreplay them at scale across pages with similar structure. BardeenAgent is also\nthe first LLM agent to take advantage of the regular structure of HTML. In\nparticular BardeenAgent constructs a generalizable CSS selector to capture all\nrelevant items on the page, then fits the operations to extract the data.\n  On the WebLists benchmark, BardeenAgent achieves 66% recall overall, more\nthan doubling the performance of SOTA web agents, and reducing cost per output\nrow by 3x.",
      "tldr_zh": "该论文提出了WebLists，一个包含200个数据提取任务的基准，旨在评估智能体从复杂交互式网站中提取结构化数据的能力。现有的大语言模型(LLMs)和先进的Web智能体在WebLists上表现不佳，表明它们在处理此类任务时存在挑战。为了解决这个问题，论文提出了BardeenAgent，一个新框架，它能够将Web智能体的执行过程转化为可重复的程序，并在具有相似结构的页面上大规模重放。BardeenAgent利用HTML的规则结构，构建通用的CSS选择器来捕获页面上的相关条目，并拟合操作以提取数据。实验结果表明，BardeenAgent在WebLists基准测试中实现了66%的召回率，是现有Web智能体的两倍以上，并将每个输出行的成本降低了3倍。\n",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12682v1",
      "published_date": "2025-04-17 06:16:40 UTC",
      "updated_date": "2025-04-17 06:16:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:12:56.548214"
    },
    {
      "arxiv_id": "2504.12681v1",
      "title": "GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs",
      "title_zh": "GRAIL：基于梯度的自适应性逆学习，用于LLM中的隐私和版权保护\n",
      "authors": [
        "Kun-Woo Kim",
        "Ji-Hoon Park",
        "Ju-Min Han",
        "Seong-Whan Lee"
      ],
      "abstract": "Large Language Models (LLMs) trained on extensive datasets often learn\nsensitive information, which raises significant social and legal concerns under\nprinciples such as the \"Right to be forgotten.\" Retraining entire models from\nscratch to remove undesired information is both costly and impractical.\nFurthermore, existing single-domain unlearning methods fail to address\nmulti-domain scenarios, where knowledge is interwoven across domains such as\nprivacy and copyright, creating overlapping representations that lead to\nexcessive knowledge removal or degraded performance. To tackle these issues, we\npropose GRAIL (GRadient-based AdaptIve unLearning), a novel multi-domain\nunlearning framework. GRAIL leverages gradient information from multiple\ndomains to precisely distinguish the unlearning scope from the retention scope,\nand applies an adaptive parameter-wise localization strategy to selectively\nremove targeted knowledge while preserving critical parameters for each domain.\nExperimental results on unlearning benchmarks show that GRAIL achieves\nunlearning success on par with the existing approaches, while also\ndemonstrating up to 17% stronger knowledge retention success compared to the\nprevious state-of-art method. Our findings establish a new paradigm for\neffectively managing and regulating sensitive information in large-scale\npre-trained language models.",
      "tldr_zh": "该论文提出了GRAIL (GRadient-based AdaptIve unLearning)，一种新颖的基于梯度的自适应多领域遗忘框架，旨在解决LLM中隐私和版权等多领域知识相互交织导致的过度遗忘或性能下降问题。GRAIL利用来自多个领域的梯度信息，精确区分遗忘范围和保留范围，并应用自适应的参数级定位策略，选择性地移除目标知识，同时保留每个领域的关键参数。实验结果表明，GRAIL在遗忘基准测试中达到了与现有方法相当的遗忘效果，并且知识保留成功率比现有最佳方法高出17%。该研究为有效管理和规范大规模预训练语言模型中的敏感信息建立了一种新范式。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by IJCNN 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.12681v1",
      "published_date": "2025-04-17 06:16:32 UTC",
      "updated_date": "2025-04-17 06:16:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:13:08.265609"
    },
    {
      "arxiv_id": "2504.12680v1",
      "title": "Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning",
      "title_zh": "具身-R：通过强化学习激活基础模型中具身空间推理的协作框架\n",
      "authors": [
        "Baining Zhao",
        "Ziyou Wang",
        "Jianjie Fang",
        "Chen Gao",
        "Fanhang Man",
        "Jinqiang Cui",
        "Xin Wang",
        "Xinlei Chen",
        "Yong Li",
        "Wenwu Zhu"
      ],
      "abstract": "Humans can perceive and reason about spatial relationships from sequential\nvisual observations, such as egocentric video streams. However, how pretrained\nmodels acquire such abilities, especially high-level reasoning, remains\nunclear. This paper introduces Embodied-R, a collaborative framework combining\nlarge-scale Vision-Language Models (VLMs) for perception and small-scale\nLanguage Models (LMs) for reasoning. Using Reinforcement Learning (RL) with a\nnovel reward system considering think-answer logical consistency, the model\nachieves slow-thinking capabilities with limited computational resources. After\ntraining on only 5k embodied video samples, Embodied-R with a 3B LM matches\nstate-of-the-art multimodal reasoning models (OpenAI-o1, Gemini-2.5-pro) on\nboth in-distribution and out-of-distribution embodied spatial reasoning tasks.\nEmbodied-R also exhibits emergent thinking patterns such as systematic analysis\nand contextual integration. We further explore research questions including\nresponse length, training on VLM, strategies for reward design, and differences\nin model generalization after SFT (Supervised Fine-Tuning) and RL training.",
      "tldr_zh": "本文提出了Embodied-R，一个协作框架，旨在激活基础模型中的具身空间推理能力。该框架结合了大规模视觉语言模型(VLMs)进行感知，以及小规模语言模型(LMs)进行推理，并利用强化学习(RL)训练，通过考虑“思考-回答”逻辑一致性的新型奖励系统，使模型能够在有限的计算资源下实现慢思考能力。Embodied-R仅使用5k个具身视频样本进行训练后，其搭载的3B LM在分布内和分布外的具身空间推理任务上，与最先进的多模态推理模型(OpenAI-o1, Gemini-2.5-pro)相媲美。Embodied-R还表现出系统分析和上下文整合等新兴思维模式。该研究还探讨了响应长度、VLM训练、奖励设计策略以及SFT（监督微调）和RL训练后模型泛化差异等研究问题。\n",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.12680v1",
      "published_date": "2025-04-17 06:16:11 UTC",
      "updated_date": "2025-04-17 06:16:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:13:20.443573"
    },
    {
      "arxiv_id": "2504.12673v1",
      "title": "ACoRN: Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models",
      "title_zh": "ACoRN：检索增强语言模型中具有噪声鲁棒性的抽象压缩",
      "authors": [
        "Singon Kim",
        "Gunho Jung",
        "Seong-Whan Lee"
      ],
      "abstract": "Abstractive compression utilizes smaller langauge models to condense\nquery-relevant context, reducing computational costs in retrieval-augmented\ngeneration (RAG). However,retrieved documents often include information that is\neither irrelevant to answering the query or misleading due to factual incorrect\ncontent, despite having high relevance scores. This behavior indicates that\nabstractive compressors are more likely to omit important information essential\nfor the correct answer, especially in long contexts where attention dispersion\noccurs. To address this issue, we categorize retrieved documents in a more\nfine-grained manner and propose Abstractive Compression Robust against Noise\n(ACoRN), which introduces two novel training steps. First, we use offline data\naugmentation on the training dataset to enhance compressor robustness against\ntwo distinct types of retrieval noise. Second, since the language modelbased\ncompressor cannot fully utilize information from multiple retrieved documents\nand exhibits positional bias, we perform finetuning to generate summaries\ncentered around key information that directly supports the correct answer. Our\nexperiments demonstrate that T5-large, trained with ACoRN as a compressor,\nimproves EM and F1 scores while preserving the answer string, which could serve\nas direct evidence. ACoRN excels on datasets with many accuracy-reducing\ndocuments, making it highly useful in real-world scenarios.",
      "tldr_zh": "该论文提出了ACoRN (Abstractive Compression Robust against Noise)，一种在检索增强语言模型(RAG)中用于提升抽象压缩鲁棒性的方法。ACoRN旨在解决RAG中由于检索文档包含无关或错误信息而导致的压缩器忽略关键信息的问题。该方法通过离线数据增强来增强压缩器对两种检索噪声的鲁棒性，并通过微调生成以关键信息为中心的摘要，从而克服语言模型压缩器无法充分利用多个检索文档信息以及存在位置偏见的问题。实验结果表明，使用ACoRN训练的T5-large压缩器在存在大量降低准确性的文档的数据集上表现出色，提高了EM和F1分数，同时保留了作为直接证据的答案字符串。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12673v1",
      "published_date": "2025-04-17 06:05:35 UTC",
      "updated_date": "2025-04-17 06:05:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:13:32.306820"
    },
    {
      "arxiv_id": "2504.12672v1",
      "title": "Post-processing improves accuracy of Artificial Intelligence weather forecasts",
      "title_zh": "后处理提高人工智能天气预报的准确性\n",
      "authors": [
        "Belinda Trotta",
        "Robert Johnson",
        "Catherine de Burgh-Day",
        "Debra Hudson",
        "Esteban Abellan",
        "James Canvin",
        "Andrew Kelly",
        "Daniel Mentiplay",
        "Benjamin Owen",
        "Jennifer Whelan"
      ],
      "abstract": "Artificial Intelligence (AI) weather models are now reaching\noperational-grade performance for some variables, but like traditional\nNumerical Weather Prediction (NWP) models, they exhibit systematic biases and\nreliability issues. We test the application of the Bureau of Meteorology's\nexisting statistical post-processing system, IMPROVER, to ECMWF's deterministic\nArtificial Intelligence Forecasting System (AIFS), and compare results against\npost-processed outputs from the ECMWF HRES and ENS models. Without any\nmodification to configuration or processing workflows, post-processing yields\ncomparable accuracy improvements for AIFS as for traditional NWP forecasts, in\nboth expected value and probabilistic outputs. We show that blending AIFS with\nNWP models improves overall forecast skill, even when AIFS alone is not the\nmost accurate component. These findings show that statistical post-processing\nmethods developed for NWP are directly applicable to AI models, enabling\nnational meteorological centres to incorporate AI forecasts into existing\nworkflows in a low-risk, incremental fashion.",
      "tldr_zh": "该研究探讨了将气象局现有的统计后处理系统IMPROVER应用于欧洲中期天气预报中心(ECMWF)的确定性人工智能预测系统(AIFS)的效果。研究发现，在不修改配置或处理流程的情况下，后处理能够为AIFS带来与传统数值天气预报(NWP)模型相当的精度提升，包括期望值和概率输出。此外，将AIFS与NWP模型融合可以提高整体预测能力，即使AIFS单独使用时并非最准确。研究表明，为NWP开发的统计后处理方法可以直接应用于AI模型，使国家气象中心能够以低风险、增量的方式将AI预测整合到现有工作流程中。\n",
      "categories": [
        "physics.ao-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "physics.ao-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12672v1",
      "published_date": "2025-04-17 06:05:10 UTC",
      "updated_date": "2025-04-17 06:05:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:13:44.246777"
    },
    {
      "arxiv_id": "2504.12663v1",
      "title": "Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment",
      "title_zh": "Persona-judge：通过 Token 级别自我判断实现大型语言模型的个性化对齐\n",
      "authors": [
        "Xiaotian Zhang",
        "Ruizhe Chen",
        "Yang Feng",
        "Zuozhu Liu"
      ],
      "abstract": "Aligning language models with human preferences presents significant\nchallenges, particularly in achieving personalization without incurring\nexcessive computational costs. Existing methods rely on reward signals and\nadditional annotated data, limiting their scalability and adaptability to\ndiverse human values. To address these challenges, we introduce Persona-judge,\na novel discriminative paradigm that enables training-free personalized\nalignment with unseen preferences. Instead of optimizing policy parameters\nthrough external reward feedback, Persona-judge leverages the intrinsic\npreference judgment capabilities of the model. Specifically, a draft model\ngenerates candidate tokens conditioned on a given preference, while a judge\nmodel, embodying another preference, cross-validates the predicted tokens\nwhether to be accepted. Experimental results demonstrate that Persona-judge,\nusing the inherent preference evaluation mechanisms of the model, offers a\nscalable and computationally efficient solution to personalized alignment,\npaving the way for more adaptive customized alignment.",
      "tldr_zh": "该论文提出了一种名为Persona-judge的判别式范式，旨在实现大型语言模型(LLM)的个性化对齐，且无需训练。Persona-judge不依赖外部奖励信号和额外标注数据，而是利用模型固有的偏好判断能力。具体来说，一个模型根据给定的偏好生成候选token，而另一个模型（作为judge）则根据另一种偏好来交叉验证这些token是否可接受。实验结果表明，Persona-judge利用模型固有的偏好评估机制，为个性化对齐提供了一种可扩展且计算高效的解决方案，为更具适应性的定制对齐铺平了道路。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12663v1",
      "published_date": "2025-04-17 05:50:13 UTC",
      "updated_date": "2025-04-17 05:50:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:13:56.121089"
    },
    {
      "arxiv_id": "2504.12644v1",
      "title": "Quantum Computing Supported Adversarial Attack-Resilient Autonomous Vehicle Perception Module for Traffic Sign Classification",
      "title_zh": "量子计算支持的、具有对抗攻击弹性的自动驾驶车辆感知模块，用于交通标志分类\n",
      "authors": [
        "Reek Majumder",
        "Mashrur Chowdhury",
        "Sakib Mahmud Khan",
        "Zadid Khan",
        "Fahim Ahmad",
        "Frank Ngeni",
        "Gurcan Comert",
        "Judith Mwakalonge",
        "Dimitra Michalaka"
      ],
      "abstract": "Deep learning (DL)-based image classification models are essential for\nautonomous vehicle (AV) perception modules since incorrect categorization might\nhave severe repercussions. Adversarial attacks are widely studied cyberattacks\nthat can lead DL models to predict inaccurate output, such as incorrectly\nclassified traffic signs by the perception module of an autonomous vehicle. In\nthis study, we create and compare hybrid classical-quantum deep learning\n(HCQ-DL) models with classical deep learning (C-DL) models to demonstrate\nrobustness against adversarial attacks for perception modules. Before feeding\nthem into the quantum system, we used transfer learning models, alexnet and\nvgg-16, as feature extractors. We tested over 1000 quantum circuits in our\nHCQ-DL models for projected gradient descent (PGD), fast gradient sign attack\n(FGSA), and gradient attack (GA), which are three well-known untargeted\nadversarial approaches. We evaluated the performance of all models during\nadversarial attacks and no-attack scenarios. Our HCQ-DL models maintain\naccuracy above 95\\% during a no-attack scenario and above 91\\% for GA and FGSA\nattacks, which is higher than C-DL models. During the PGD attack, our\nalexnet-based HCQ-DL model maintained an accuracy of 85\\% compared to C-DL\nmodels that achieved accuracies below 21\\%. Our results highlight that the\nHCQ-DL models provide improved accuracy for traffic sign classification under\nadversarial settings compared to their classical counterparts.",
      "tldr_zh": "该研究探索了混合经典-量子深度学习(HCQ-DL)模型在提升自动驾驶车辆感知模块对抗攻击鲁棒性方面的潜力，尤其是在交通标志分类任务中。研究者将AlexNet和VGG-16等迁移学习模型作为特征提取器，并结合超过1000个量子电路构建HCQ-DL模型，并与经典深度学习(C-DL)模型在无攻击和三种常见对抗攻击（PGD, FGSA, GA）场景下进行比较。实验结果表明，HCQ-DL模型在对抗攻击下表现出更高的准确率，特别是在PGD攻击下，基于AlexNet的HCQ-DL模型准确率高达85%，远高于C-DL模型（低于21%）。该研究验证了HCQ-DL模型在对抗环境下的优越性，为构建更安全的自动驾驶感知系统提供了新思路。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.CV",
        "cs.ET"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12644v1",
      "published_date": "2025-04-17 05:08:08 UTC",
      "updated_date": "2025-04-17 05:08:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:14:08.584286"
    },
    {
      "arxiv_id": "2504.12637v1",
      "title": "Scaling Instruction-Tuned LLMs to Million-Token Contexts via Hierarchical Synthetic Data Generation",
      "title_zh": "通过分层合成数据生成将指令调优的 LLM 扩展到百万 Token 上下文\n",
      "authors": [
        "Linda He",
        "Jue Wang",
        "Maurice Weber",
        "Shang Zhu",
        "Ben Athiwaratkun",
        "Ce Zhang"
      ],
      "abstract": "Large Language Models (LLMs) struggle with long-context reasoning, not only\ndue to the quadratic scaling of computational complexity with sequence length\nbut also because of the scarcity and expense of annotating long-context data.\nThere has been barely any open-source work that systematically ablates\nlong-context data, nor is there any openly available instruction tuning dataset\nwith contexts surpassing 100K tokens. To bridge this gap, we introduce a novel\npost-training synthetic data generation strategy designed to efficiently extend\nthe context window of LLMs while preserving their general task performance. Our\napproach scalably extends to arbitrarily long context lengths, unconstrained by\nthe length of available real-world data, which effectively addresses the\nscarcity of raw long-context data. Through a step-by-step rotary position\nembedding (RoPE) scaling training strategy, we demonstrate that our model, with\na context length of up to 1M tokens, performs well on the RULER benchmark and\nInfiniteBench and maintains robust performance on general language tasks.",
      "tldr_zh": "该论文提出了一种新的合成数据生成策略，旨在高效地扩展指令微调LLM的上下文窗口，使其能够处理百万级别的tokens。该方法通过分层合成数据生成，克服了长上下文数据稀缺和标注成本高昂的问题，从而能够扩展到任意长度的上下文，不受限于真实数据的长度。通过逐步旋转位置嵌入(RoPE)缩放训练策略，模型在RULER和InfiniteBench等长文本基准测试中表现良好，同时保持了在通用语言任务上的性能。该研究为训练具有超长上下文处理能力的LLM提供了一种可扩展的解决方案。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "26 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.12637v1",
      "published_date": "2025-04-17 04:46:57 UTC",
      "updated_date": "2025-04-17 04:46:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:14:20.288488"
    },
    {
      "arxiv_id": "2504.12612v1",
      "title": "The Chronicles of Foundation AI for Forensics of Multi-Agent Provenance",
      "title_zh": "溯源多智能体取证的基础人工智能编年史\n",
      "authors": [
        "Ching-Chun Chang",
        "Isao Echizen"
      ],
      "abstract": "Provenance is the chronology of things, resonating with the fundamental\npursuit to uncover origins, trace connections, and situate entities within the\nflow of space and time. As artificial intelligence advances towards autonomous\nagents capable of interactive collaboration on complex tasks, the provenance of\ngenerated content becomes entangled in the interplay of collective creation,\nwhere contributions are continuously revised, extended or overwritten. In a\nmulti-agent generative chain, content undergoes successive transformations,\noften leaving little, if any, trace of prior contributions. In this study, we\ninvestigates the problem of tracking multi-agent provenance across the temporal\ndimension of generation. We propose a chronological system for post hoc\nattribution of generative history from content alone, without reliance on\ninternal memory states or external meta-information. At its core lies the\nnotion of symbolic chronicles, representing signed and time-stamped records, in\na form analogous to the chain of custody in forensic science. The system\noperates through a feedback loop, whereby each generative timestep updates the\nchronicle of prior interactions and synchronises it with the synthetic content\nin the very act of generation. This research seeks to develop an accountable\nform of collaborative artificial intelligence within evolving cyber ecosystems.",
      "tldr_zh": "本文研究了在多智能体生成环境中追踪内容溯源的问题，提出了一种时间维度上的生成历史追溯系统。该系统无需依赖内部记忆状态或外部元信息，仅通过内容本身进行事后归因。核心概念是“符号纪年(symbolic chronicles)”，它代表着签名和时间戳记录，类似于法医学中的监管链。系统通过反馈循环运行，每个生成时间步都会更新先前交互的纪年，并将其与合成内容同步。该研究旨在开发一种在不断发展的网络生态系统中具有责任性的协作人工智能形式。\n",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12612v1",
      "published_date": "2025-04-17 03:23:17 UTC",
      "updated_date": "2025-04-17 03:23:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:14:32.136695"
    },
    {
      "arxiv_id": "2504.12609v1",
      "title": "Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration",
      "title_zh": "利用单个人类演示，通过 Sim-to-Real RL 跨越人机具身鸿沟\n",
      "authors": [
        "Tyler Ga Wei Lum",
        "Olivia Y. Lee",
        "C. Karen Liu",
        "Jeannette Bohg"
      ],
      "abstract": "Teaching robots dexterous manipulation skills often requires collecting\nhundreds of demonstrations using wearables or teleoperation, a process that is\nchallenging to scale. Videos of human-object interactions are easier to collect\nand scale, but leveraging them directly for robot learning is difficult due to\nthe lack of explicit action labels from videos and morphological differences\nbetween robot and human hands. We propose Human2Sim2Robot, a novel\nreal-to-sim-to-real framework for training dexterous manipulation policies\nusing only one RGB-D video of a human demonstrating a task. Our method utilizes\nreinforcement learning (RL) in simulation to cross the human-robot embodiment\ngap without relying on wearables, teleoperation, or large-scale data collection\ntypically necessary for imitation learning methods. From the demonstration, we\nextract two task-specific components: (1) the object pose trajectory to define\nan object-centric, embodiment-agnostic reward function, and (2) the\npre-manipulation hand pose to initialize and guide exploration during RL\ntraining. We found that these two components are highly effective for learning\nthe desired task, eliminating the need for task-specific reward shaping and\ntuning. We demonstrate that Human2Sim2Robot outperforms object-aware open-loop\ntrajectory replay by 55% and imitation learning with data augmentation by 68%\nacross grasping, non-prehensile manipulation, and multi-step tasks. Project\nSite: https://human2sim2robot.github.io",
      "tldr_zh": "该论文提出了Human2Sim2Robot框架，一种real-to-sim-to-real的方法，仅使用一个人类操作的RGB-D视频即可训练灵巧的机器人操作策略，从而克服人与机器人之间的形态差异。该方法利用强化学习(RL)在模拟环境中训练，无需可穿戴设备、远程操作或大规模数据收集。从人类演示视频中提取物体姿态轨迹和预操作手部姿态，分别用于定义以物体为中心的奖励函数和引导RL训练过程。实验结果表明，Human2Sim2Robot在抓取、非抓取操作和多步骤任务中，性能优于物体感知的开环轨迹回放和数据增强的模仿学习。\n",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "15 pages, 13 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.12609v1",
      "published_date": "2025-04-17 03:15:20 UTC",
      "updated_date": "2025-04-17 03:15:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:14:44.409625"
    },
    {
      "arxiv_id": "2504.12608v1",
      "title": "Code Copycat Conundrum: Demystifying Repetition in LLM-based Code Generation",
      "title_zh": "代码模仿难题：揭秘基于 LLM 的代码生成中的重复现象\n",
      "authors": [
        "Mingwei Liu",
        "Juntao Li",
        "Ying Wang",
        "Xueying Du",
        "Zuoyu Ou",
        "Qiuyuan Chen",
        "Bingxu An",
        "Zhao Wei",
        "Yong Xu",
        "Fangming Zou",
        "Xin Peng",
        "Yiling Lou"
      ],
      "abstract": "Despite recent advances in Large Language Models (LLMs) for code generation,\nthe quality of LLM-generated code still faces significant challenges. One\nsignificant issue is code repetition, which refers to the model's tendency to\ngenerate structurally redundant code, resulting in inefficiencies and reduced\nreadability. To address this, we conduct the first empirical study to\ninvestigate the prevalence and nature of repetition across 19 state-of-the-art\ncode LLMs using three widely-used benchmarks. Our study includes both\nquantitative and qualitative analyses, revealing that repetition is pervasive\nand manifests at various granularities and extents, including character,\nstatement, and block levels. We further summarize a taxonomy of 20 repetition\npatterns. Building on our findings, we propose DeRep, a rule-based technique\ndesigned to detect and mitigate repetition in generated code. We evaluate DeRep\nusing both open-source benchmarks and in an industrial setting. Our results\ndemonstrate that DeRep significantly outperforms baselines in reducing\nrepetition (with an average improvements of 91.3%, 93.5%, and 79.9% in rep-3,\nrep-line, and sim-line metrics) and enhancing code quality (with a Pass@1\nincrease of 208.3% over greedy search). Furthermore, integrating DeRep improves\nthe performance of existing repetition mitigation methods, with Pass@1\nimprovements ranging from 53.7% to 215.7%.",
      "tldr_zh": "该研究深入探讨了大型语言模型(LLMs)在代码生成中普遍存在的代码重复问题，通过对19个先进代码LLM在三个常用基准测试上的实证研究，揭示了重复性存在于字符、语句和代码块等不同粒度级别。研究总结了20种重复模式的分类，并提出了一种名为DeRep的基于规则的技术，用于检测和缓解生成代码中的重复。实验结果表明，DeRep在减少重复方面显著优于基线方法，并在Pass@1指标上提高了代码质量，同时还能提升现有重复缓解方法的性能。\n",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12608v1",
      "published_date": "2025-04-17 03:13:39 UTC",
      "updated_date": "2025-04-17 03:13:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:14:56.259003"
    },
    {
      "arxiv_id": "2504.12606v1",
      "title": "Robo-SGG: Exploiting Layout-Oriented Normalization and Restitution for Robust Scene Graph Generation",
      "title_zh": "Robo-SGG：利用面向布局的归一化和恢复进行鲁棒的场景图生成\n",
      "authors": [
        "Changsheng Lv",
        "Mengshi Qi",
        "Zijian Fu",
        "Huadong Ma"
      ],
      "abstract": "In this paper, we introduce a novel method named Robo-SGG, i.e.,\nLayout-Oriented Normalization and Restitution for Robust Scene Graph\nGeneration. Compared to the existing SGG setting, the robust scene graph\ngeneration aims to perform inference on a diverse range of corrupted images,\nwith the core challenge being the domain shift between the clean and corrupted\nimages. Existing SGG methods suffer from degraded performance due to\ncompromised visual features e.g., corruption interference or occlusions. To\nobtain robust visual features, we exploit the layout information, which is\ndomain-invariant, to enhance the efficacy of existing SGG methods on corrupted\nimages. Specifically, we employ Instance Normalization(IN) to filter out the\ndomain-specific feature and recover the unchangeable structural features, i.e.,\nthe positional and semantic relationships among objects by the proposed\nLayout-Oriented Restitution. Additionally, we propose a Layout-Embedded Encoder\n(LEE) that augments the existing object and predicate encoders within the SGG\nframework, enriching the robust positional and semantic features of objects and\npredicates. Note that our proposed Robo-SGG module is designed as a\nplug-and-play component, which can be easily integrated into any baseline SGG\nmodel. Extensive experiments demonstrate that by integrating the\nstate-of-the-art method into our proposed Robo-SGG, we achieve relative\nimprovements of 5.6%, 8.0%, and 6.5% in mR@50 for PredCls, SGCls, and SGDet\ntasks on the VG-C dataset, respectively, and achieve new state-of-the-art\nperformance in corruption scene graph generation benchmark (VG-C and GQA-C). We\nwill release our source code and model.",
      "tldr_zh": "本文提出了一种名为Robo-SGG的新方法，即面向布局的归一化和恢复，用于鲁棒的场景图生成(Scene Graph Generation)。Robo-SGG旨在处理各种损坏图像上的推理，核心挑战是干净图像和损坏图像之间的域偏移。该方法利用领域不变的布局信息来增强现有SGG方法在损坏图像上的有效性，通过实例归一化(Instance Normalization)过滤掉特定领域的特征，并通过提出的面向布局的恢复来恢复对象之间不变的结构特征。此外，还提出了一个布局嵌入编码器(LEE)，以增强对象和谓词的鲁棒位置和语义特征。实验表明，将最先进的方法集成到Robo-SGG中，在VG-C数据集上，PredCls、SGCls和SGDet任务的mR@50分别提高了5.6%、8.0%和6.5%，并在损坏的场景图生成基准(VG-C和GQA-C)中实现了新的state-of-the-art性能。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12606v1",
      "published_date": "2025-04-17 03:09:22 UTC",
      "updated_date": "2025-04-17 03:09:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:15:08.684521"
    },
    {
      "arxiv_id": "2504.12585v1",
      "title": "Identifying and Mitigating the Influence of the Prior Distribution in Large Language Models",
      "title_zh": "识别和减轻先验分布在大型语言模型中的影响\n",
      "authors": [
        "Liyi Zhang",
        "Veniamin Veselovsky",
        "R. Thomas McCoy",
        "Thomas L. Griffiths"
      ],
      "abstract": "Large language models (LLMs) sometimes fail to respond appropriately to\ndeterministic tasks -- such as counting or forming acronyms -- because the\nimplicit prior distribution they have learned over sequences of tokens\ninfluences their responses. In this work, we show that, in at least some cases,\nLLMs actually compute the information needed to perform these tasks correctly,\nand we identify some interventions that can allow them to access this\ninformation to improve their performance. First, we show that simply prompting\nthe language model to not rely on its prior knowledge leads to dramatic\nimprovements in prior-dominated tasks. We then use mechanistic interpretability\ntechniques to localize the prior within the LLM and manipulate the extent to\nwhich that prior influences its responses. Specifically, we show that it is\npossible to identify layers of the underlying neural network that correlate\nwith the prior probability of a response and that lightweight finetuning of\nthese layers with basic prompts on prior-dominated tasks achieves high\nperformance on held-out answers. These results suggest that the information\nrequired to produce a correct response is contained within the representations\nof the problems formed by the models. Furthermore, we show that this finetuning\nis significantly more effective for prior-dominated tasks, and that the error\nafter finetuning is no longer correlated with the prior. Our results suggest\nthat it may be possible to define effective methods for manipulating the extent\nto which LLMs rely upon their priors in solving problems, potentially\nincreasing their performance in settings where LLMs hallucinate for reasons\nrelated to the prior probability of token sequences.",
      "tldr_zh": "大型语言模型(LLMs)在确定性任务中表现不佳，原因在于其隐式先验分布会影响输出。该研究表明，LLMs实际上计算了正确执行任务所需的信息，并提出了干预措施来改善性能。首先，提示模型不要依赖先验知识可以显著改善表现。然后，利用机制可解释性技术定位LLM中的先验，并通过轻量级微调来操纵先验的影响。结果表明，可以识别与响应的先验概率相关的神经网络层，并且对这些层进行微调可以提高在保留答案上的性能。微调后的误差不再与先验相关，表明可以有效操纵LLMs对先验的依赖程度，从而提高LLMs在幻觉场景中的性能。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "16 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.12585v1",
      "published_date": "2025-04-17 02:00:53 UTC",
      "updated_date": "2025-04-17 02:00:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:15:20.532172"
    },
    {
      "arxiv_id": "2504.12577v1",
      "title": "Local Data Quantity-Aware Weighted Averaging for Federated Learning with Dishonest Clients",
      "title_zh": "用于存在不诚实客户端的联邦学习的局部数据量感知加权平均",
      "authors": [
        "Leming Wu",
        "Yaochu Jin",
        "Kuangrong Hao",
        "Han Yu"
      ],
      "abstract": "Federated learning (FL) enables collaborative training of deep learning\nmodels without requiring data to leave local clients, thereby preserving client\nprivacy. The aggregation process on the server plays a critical role in the\nperformance of the resulting FL model. The most commonly used aggregation\nmethod is weighted averaging based on the amount of data from each client,\nwhich is thought to reflect each client's contribution. However, this method is\nprone to model bias, as dishonest clients might report inaccurate training data\nvolumes to the server, which is hard to verify. To address this issue, we\npropose a novel secure \\underline{Fed}erated \\underline{D}ata\nq\\underline{u}antity-\\underline{a}ware weighted averaging method (FedDua). It\nenables FL servers to accurately predict the amount of training data from each\nclient based on their local model gradients uploaded. Furthermore, it can be\nseamlessly integrated into any FL algorithms that involve server-side model\naggregation. Extensive experiments on three benchmarking datasets demonstrate\nthat FedDua improves the global model performance by an average of 3.17%\ncompared to four popular FL aggregation methods in the presence of inaccurate\nclient data volume declarations.",
      "tldr_zh": "本文提出了一种名为FedDua（Federated Data quantity-aware weighted averaging）的联邦学习方法，旨在解决恶意客户端在联邦学习中谎报本地数据量的问题。FedDua通过分析客户端上传的本地模型梯度，使服务器能够准确预测每个客户端的训练数据量，从而避免传统加权平均方法因数据量不准确而导致的模型偏差。该方法可以无缝集成到任何涉及服务器端模型聚合的联邦学习算法中。在三个基准数据集上的大量实验表明，与四种流行的联邦学习聚合方法相比，在存在不准确的客户端数据量声明的情况下，FedDua将全局模型性能平均提高了3.17%。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "The paper has been accepted by ICME 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.12577v1",
      "published_date": "2025-04-17 01:50:24 UTC",
      "updated_date": "2025-04-17 01:50:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:15:32.283294"
    },
    {
      "arxiv_id": "2504.12576v1",
      "title": "CM3AE: A Unified RGB Frame and Event-Voxel/-Frame Pre-training Framework",
      "title_zh": "CM3AE：一种统一的 RGB 帧和 Event-Voxel/-Frame 预训练框架\n",
      "authors": [
        "Wentao Wu",
        "Xiao Wang",
        "Chenglong Li",
        "Bo Jiang",
        "Jin Tang",
        "Bin Luo",
        "Qi Liu"
      ],
      "abstract": "Event cameras have attracted increasing attention in recent years due to\ntheir advantages in high dynamic range, high temporal resolution, low power\nconsumption, and low latency. Some researchers have begun exploring\npre-training directly on event data. Nevertheless, these efforts often fail to\nestablish strong connections with RGB frames, limiting their applicability in\nmulti-modal fusion scenarios. To address these issues, we propose a novel CM3AE\npre-training framework for the RGB-Event perception. This framework accepts\nmulti-modalities/views of data as input, including RGB images, event images,\nand event voxels, providing robust support for both event-based and RGB-event\nfusion based downstream tasks. Specifically, we design a multi-modal fusion\nreconstruction module that reconstructs the original image from fused\nmulti-modal features, explicitly enhancing the model's ability to aggregate\ncross-modal complementary information. Additionally, we employ a multi-modal\ncontrastive learning strategy to align cross-modal feature representations in a\nshared latent space, which effectively enhances the model's capability for\nmulti-modal understanding and capturing global dependencies. We construct a\nlarge-scale dataset containing 2,535,759 RGB-Event data pairs for the\npre-training. Extensive experiments on five downstream tasks fully demonstrated\nthe effectiveness of CM3AE. Source code and pre-trained models will be released\non https://github.com/Event-AHU/CM3AE.",
      "tldr_zh": "该论文提出了一个统一的RGB帧和Event-Voxel/-Frame预训练框架CM3AE，旨在解决现有事件相机预训练方法与RGB帧连接不足的问题，从而提升多模态融合场景下的性能。CM3AE框架接受RGB图像、事件图像和事件体素作为输入，通过多模态融合重建模块从融合特征中重建原始图像，增强模型聚合跨模态互补信息的能力。此外，采用多模态对比学习策略，在共享潜在空间中对齐跨模态特征表示，提升模型的多模态理解和全局依赖捕获能力。作者构建了一个包含250多万RGB-Event数据对的大规模数据集用于预训练，并在五个下游任务上验证了CM3AE的有效性。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12576v1",
      "published_date": "2025-04-17 01:49:46 UTC",
      "updated_date": "2025-04-17 01:49:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:15:44.408450"
    },
    {
      "arxiv_id": "2504.12563v1",
      "title": "MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic Data Generation",
      "title_zh": "MetaSynth：元提示驱动的 Agentic Scaffolds，用于生成多样化的合成数据\n",
      "authors": [
        "Haris Riaz",
        "Sourav Bhabesh",
        "Vinayak Arannil",
        "Miguel Ballesteros",
        "Graham Horwood"
      ],
      "abstract": "Recent smaller language models such Phi-3.5 and Phi-4 rely on synthetic data\ngenerated using larger Language models. Questions remain about leveraging\nsynthetic data for other use cases, such as adapting LLMs to specific domains.\nA key limitation of synthetic data is low diversity, which negatively impacts\nits downstream applicability for improving other models. To address this, we\npropose MetaSynth, a method for generating synthetic data that enhances\ndiversity through meta-prompting, where a language model orchestrates multiple\n\"expert\" LLM agents to collaboratively generate data. Using only 25 million\ntokens of synthetic data generated with MetaSynth, we successfully adapt a\nwell-trained LLM (Mistral-7B-v0.3) to two specialized domains-Finance and\nBiomedicine-without compromising the capabilities of the resulting model in\ngeneral tasks. In addition, we evaluate the diversity of our synthetic data\nusing seven automated metrics, and find that it approaches the diversity of LLM\npre-training corpora.\n  Continually pre-training Mistral-7B-v0.3 with MetaSynth notably outperforms\nthe base LLM, showing improvements of up to 4.08% in Finance and 13.75% in\nBiomedicine. The same model shows degraded performance when trained on data\ngenerated using a template prompt, even when the template includes prior\ngenerations and varying In-Context exemplars of real data. Our findings suggest\nthat a few million tokens of diverse synthetic data without mixing any real\ndata, is sufficient for effective domain adaptation when using MetaSynth.",
      "tldr_zh": "该论文提出了MetaSynth，一种通过元提示(meta-prompting)驱动的智能体框架，用于生成多样化的合成数据。MetaSynth利用大型语言模型(LLMs)协调多个“专家”LLM智能体协同生成数据，从而提高合成数据的多样性。实验表明，仅使用2500万tokens的MetaSynth生成的合成数据，即可成功地将Mistral-7B-v0.3模型适配到金融和生物医学两个特定领域，且不影响其在通用任务中的性能。通过七种自动指标评估，MetaSynth生成的合成数据的多样性接近LLM预训练语料库。使用MetaSynth持续预训练Mistral-7B-v0.3模型，在金融和生物医学领域分别取得了高达4.08%和13.75%的性能提升，证明了MetaSynth在领域自适应方面的有效性。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "33 pages, 17 figures. Preprint",
      "pdf_url": "http://arxiv.org/pdf/2504.12563v1",
      "published_date": "2025-04-17 01:25:15 UTC",
      "updated_date": "2025-04-17 01:25:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:15:56.560730"
    },
    {
      "arxiv_id": "2504.12562v1",
      "title": "ZeroSumEval: Scaling LLM Evaluation with Inter-Model Competition",
      "title_zh": "ZeroSumEval：利用模型间竞争扩展 LLM 评估\n",
      "authors": [
        "Haidar Khan",
        "Hisham A. Alyahya",
        "Yazeed Alnumay",
        "M Saiful Bari",
        "Bülent Yener"
      ],
      "abstract": "Evaluating the capabilities of Large Language Models (LLMs) has traditionally\nrelied on static benchmark datasets, human assessments, or model-based\nevaluations - methods that often suffer from overfitting, high costs, and\nbiases. ZeroSumEval is a novel competition-based evaluation protocol that\nleverages zero-sum games to assess LLMs with dynamic benchmarks that resist\nsaturation. ZeroSumEval encompasses a diverse suite of games, including\nsecurity challenges (PyJail), classic games (Chess, Liar's Dice, Poker),\nknowledge tests (MathQuiz), and persuasion challenges (Gandalf, Debate). These\ngames are designed to evaluate a range of AI capabilities such as strategic\nreasoning, planning, knowledge application, and creativity. Building upon\nrecent studies that highlight the effectiveness of game-based evaluations for\nLLMs, ZeroSumEval enhances these approaches by providing a standardized and\nextensible framework. To demonstrate this, we conduct extensive experiments\nwith >7000 simulations across 7 games and 13 models. Our results show that\nwhile frontier models from the GPT and Claude families can play common games\nand answer questions, they struggle to play games that require creating novel\nand challenging questions. We also observe that models cannot reliably\njailbreak each other and fail generally at tasks requiring creativity. We\nrelease our code at https://github.com/facebookresearch/ZeroSumEval.",
      "tldr_zh": "ZeroSumEval 提出了一种基于零和博弈的LLM评估协议，通过模型间的竞争来动态评估LLM，以克服传统静态基准数据集、人工评估和基于模型的评估的局限性，例如过拟合、高成本和偏差。该协议包含一系列游戏，例如安全挑战(PyJail)、经典游戏(Chess, Liar's Dice, Poker)、知识测试(MathQuiz)和说服挑战(Gandalf, Debate)，旨在评估LLM的战略推理、规划、知识应用和创造力等能力。实验结果表明，GPT和Claude等先进模型在常见游戏中表现良好，但在需要创建新颖和具有挑战性问题的游戏中表现不佳，并且在需要创造力的任务中普遍失败。该研究发布了相关代码。\n",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12562v1",
      "published_date": "2025-04-17 01:23:50 UTC",
      "updated_date": "2025-04-17 01:23:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:16:08.594818"
    },
    {
      "arxiv_id": "2504.12557v1",
      "title": "TraCeS: Trajectory Based Credit Assignment From Sparse Safety Feedback",
      "title_zh": "TraCeS：基于轨迹的稀疏安全反馈信用分配\n",
      "authors": [
        "Siow Meng Low",
        "Akshat Kumar"
      ],
      "abstract": "In safe reinforcement learning (RL), auxiliary safety costs are used to align\nthe agent to safe decision making. In practice, safety constraints, including\ncost functions and budgets, are unknown or hard to specify, as it requires\nanticipation of all possible unsafe behaviors. We therefore address a general\nsetting where the true safety definition is unknown, and has to be learned from\nsparsely labeled data. Our key contributions are: first, we design a safety\nmodel that performs credit assignment to estimate each decision step's impact\non the overall safety using a dataset of diverse trajectories and their\ncorresponding binary safety labels (i.e., whether the corresponding trajectory\nis safe/unsafe). Second, we illustrate the architecture of our safety model to\ndemonstrate its ability to learn a separate safety score for each timestep.\nThird, we reformulate the safe RL problem using the proposed safety model and\nderive an effective algorithm to optimize a safe yet rewarding policy. Finally,\nour empirical results corroborate our findings and show that this approach is\neffective in satisfying unknown safety definition, and scalable to various\ncontinuous control tasks.",
      "tldr_zh": "该论文提出了一种名为TraCeS的轨迹信用分配方法，用于解决安全强化学习(Safe RL)中安全约束未知或难以指定的问题。TraCeS通过学习一个安全模型，利用包含轨迹和二元安全标签（安全/不安全）的数据集，对每个决策步骤进行信用分配，从而评估其对整体安全的影响。该安全模型能够为每个时间步学习单独的安全评分。论文重新定义了安全RL问题，并推导出一种优化安全且有益策略的有效算法。实验结果表明，TraCeS能够有效地满足未知的安全定义，并可扩展到各种连续控制任务中。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12557v1",
      "published_date": "2025-04-17 01:11:08 UTC",
      "updated_date": "2025-04-17 01:11:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:16:20.328050"
    },
    {
      "arxiv_id": "2504.12552v1",
      "title": "Privacy-Preserving Operating Room Workflow Analysis using Digital Twins",
      "title_zh": "利用数字孪生进行隐私保护的手术室工作流程分析\n",
      "authors": [
        "Alejandra Perez",
        "Han Zhang",
        "Yu-Chun Ku",
        "Lalithkumar Seenivasan",
        "Roger Soberanis",
        "Jose L. Porras",
        "Richard Day",
        "Jeff Jopling",
        "Peter Najjar",
        "Mathias Unberath"
      ],
      "abstract": "Purpose: The operating room (OR) is a complex environment where optimizing\nworkflows is critical to reduce costs and improve patient outcomes. The use of\ncomputer vision approaches for the automatic recognition of perioperative\nevents enables identification of bottlenecks for OR optimization. However,\nprivacy concerns limit the use of computer vision for automated event detection\nfrom OR videos, which makes privacy-preserving approaches needed for OR\nworkflow analysis. Methods: We propose a two-stage pipeline for\nprivacy-preserving OR video analysis and event detection. In the first stage,\nwe leverage vision foundation models for depth estimation and semantic\nsegmentation to generate de-identified Digital Twins (DT) of the OR from\nconventional RGB videos. In the second stage, we employ the SafeOR model, a\nfused two-stream approach that processes segmentation masks and depth maps for\nOR event detection. We evaluate this method on an internal dataset of 38\nsimulated surgical trials with five event classes. Results: Our results\nindicate that this DT-based approach to the OR event detection model achieves\nperformance on par and sometimes even better than raw RGB video-based models on\ndetecting OR events. Conclusion: DTs enable privacy-preserving OR workflow\nanalysis, facilitating the sharing of de-identified data across institutions\nand they can potentially enhance model generalizability by mitigating\ndomain-specific appearance differences.",
      "tldr_zh": "该论文提出了一种基于数字孪生(Digital Twins, DT)的隐私保护手术室工作流程分析方法。该方法首先利用视觉基础模型从RGB视频中生成手术室的去身份化DT，包括深度估计和语义分割。然后，使用SafeOR模型，一个融合双流的方法，处理分割掩码和深度图进行手术室事件检测。实验结果表明，基于DT的方法在事件检测方面达到了与原始RGB视频模型相当甚至更好的性能。该方法能够实现隐私保护的手术室工作流程分析，促进跨机构的数据共享，并可能通过减轻特定领域的表观差异来提高模型泛化能力。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12552v1",
      "published_date": "2025-04-17 00:46:06 UTC",
      "updated_date": "2025-04-17 00:46:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:16:32.368059"
    },
    {
      "arxiv_id": "2504.12549v1",
      "title": "Memorization: A Close Look at Books",
      "title_zh": "记忆：细看书籍\n",
      "authors": [
        "Iris Ma",
        "Ian Domingo",
        "Alberto Krone-Martins",
        "Pierre Baldi",
        "Cristina V. Lopes"
      ],
      "abstract": "To what extent can entire books be extracted from LLMs? Using the Llama 3 70B\nfamily of models, and the \"prefix-prompting\" extraction technique, we were able\nto auto-regressively reconstruct, with a very high level of similarity, one\nentire book (Alice's Adventures in Wonderland) from just the first 500 tokens.\nWe were also able to obtain high extraction rates on several other books,\npiece-wise. However, these successes do not extend uniformly to all books. We\nshow that extraction rates of books correlate with book popularity and thus,\nlikely duplication in the training data.\n  We also confirm the undoing of mitigations in the instruction-tuned Llama\n3.1, following recent work (Nasr et al., 2025). We further find that this\nundoing comes from changes to only a tiny fraction of weights concentrated\nprimarily in the lower transformer blocks. Our results provide evidence of the\nlimits of current regurgitation mitigation strategies and introduce a framework\nfor studying how fine-tuning affects the retrieval of verbatim memorization in\naligned LLMs.",
      "tldr_zh": "该研究探讨了大型语言模型(LLMs)记忆整本书籍的能力。研究者使用Llama 3 70B模型和\"prefix-prompting\"提取技术，仅用前500个token就高精度地重构了《爱丽丝梦游仙境》。书籍的提取率与书籍的流行程度相关，表明训练数据中可能存在重复。此外，研究证实了指令微调后的Llama 3.1中缓解措施的失效，且这种失效仅源于少量权重变化，主要集中在较低的transformer块中。研究结果揭示了当前抑制模型背诵策略的局限性，并为研究微调如何影响对齐LLM中逐字记忆的检索提供了一个框架。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12549v1",
      "published_date": "2025-04-17 00:20:18 UTC",
      "updated_date": "2025-04-17 00:20:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:16:44.535692"
    },
    {
      "arxiv_id": "2504.12546v1",
      "title": "Anonymous Public Announcements",
      "title_zh": "匿名公共声明\n",
      "authors": [
        "Thomas Ågotnes",
        "Rustam Galimullin",
        "Ken Satoh",
        "Satoshi Tojo"
      ],
      "abstract": "We formalise the notion of an \\emph{anonymous public announcement} in the\ntradition of public announcement logic. Such announcements can be seen as\nin-between a public announcement from ``the outside\" (an announcement of\n$\\phi$) and a public announcement by one of the agents (an announcement of\n$K_a\\phi$): we get more information than just $\\phi$, but not (necessarily)\nabout exactly who made it. Even if such an announcement is prima facie\nanonymous, depending on the background knowledge of the agents it might reveal\nthe identity of the announcer: if I post something on a message board, the\ninformation might reveal who I am even if I don't sign my name. Furthermore,\nlike in the Russian Cards puzzle, if we assume that the announcer's intention\nwas to stay anonymous, that in fact might reveal more information. In this\npaper we first look at the case when no assumption about intentions are made,\nin which case the logic with an anonymous public announcement operator is\nreducible to epistemic logic. We then look at the case when we assume common\nknowledge of the intention to stay anonymous, which is both more complex and\nmore interesting: in several ways it boils down to the notion of a ``safe\"\nannouncement (again, similarly to Russian Cards). Main results include formal\nexpressivity results and axiomatic completeness for key logical languages.",
      "tldr_zh": "本文形式化了公共宣告逻辑中的“匿名公共宣告”概念。这种宣告介于外部宣告（$\\phi$）和代理人宣告（$K_a\\phi$）之间，提供了比单纯的$\\phi$更多的信息，但未必揭示宣告者的身份。即使表面上匿名，根据代理人的背景知识，宣告也可能暴露身份。类似俄罗斯纸牌谜题，如果假设宣告者的意图是保持匿名，反而可能泄露更多信息。本文首先研究了不假设意图的情况，此时带有匿名公共宣告算子的逻辑可简化为认知逻辑。然后，研究了假设保持匿名的意图是常识的情况，这更为复杂和有趣，在某种程度上归结为“安全”宣告的概念。主要结果包括关键逻辑语言的形式表达能力结果和公理完备性。\n",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12546v1",
      "published_date": "2025-04-17 00:14:37 UTC",
      "updated_date": "2025-04-17 00:14:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:16:56.613567"
    },
    {
      "arxiv_id": "2504.12545v1",
      "title": "Knowledge Acquisition on Mass-shooting Events via LLMs for AI-Driven Justice",
      "title_zh": "基于大语言模型利用人工智能驱动的正义获取大规模枪击事件相关知识",
      "authors": [
        "Benign John Ihugba",
        "Afsana Nasrin",
        "Ling Wu",
        "Lin Li",
        "Lijun Qian",
        "Xishuang Dong"
      ],
      "abstract": "Mass-shooting events pose a significant challenge to public safety,\ngenerating large volumes of unstructured textual data that hinder effective\ninvestigations and the formulation of public policy. Despite the urgency, few\nprior studies have effectively automated the extraction of key information from\nthese events to support legal and investigative efforts. This paper presented\nthe first dataset designed for knowledge acquisition on mass-shooting events\nthrough the application of named entity recognition (NER) techniques. It\nfocuses on identifying key entities such as offenders, victims, locations, and\ncriminal instruments, that are vital for legal and investigative purposes. The\nNER process is powered by Large Language Models (LLMs) using few-shot\nprompting, facilitating the efficient extraction and organization of critical\ninformation from diverse sources, including news articles, police reports, and\nsocial media. Experimental results on real-world mass-shooting corpora\ndemonstrate that GPT-4o is the most effective model for mass-shooting NER,\nachieving the highest Micro Precision, Micro Recall, and Micro F1-scores.\nMeanwhile, o1-mini delivers competitive performance, making it a\nresource-efficient alternative for less complex NER tasks. It is also observed\nthat increasing the shot count enhances the performance of all models, but the\ngains are more substantial for GPT-4o and o1-mini, highlighting their superior\nadaptability to few-shot learning scenarios.",
      "tldr_zh": "该研究针对大规模枪击事件后海量非结构化文本数据给调查和公共政策制定带来的挑战，提出了一个用于知识获取的新数据集，旨在通过命名实体识别(NER)技术提取关键信息。研究利用大型语言模型(LLMs)和少样本提示(few-shot prompting)方法，从新闻报道、警方报告和社交媒体等来源高效地提取并组织关键实体，如罪犯、受害者、地点和犯罪工具。实验结果表明，GPT-4o在枪击事件NER任务中表现最佳，而o1-mini在资源有限的情况下也具有竞争力。增加样本数量可以提高所有模型的性能，特别是GPT-4o和o1-mini，突显了它们在少样本学习中的优越适应性。\n",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12545v1",
      "published_date": "2025-04-17 00:13:04 UTC",
      "updated_date": "2025-04-17 00:13:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-19T02:17:08.493524"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 84,
  "processed_papers_count": 84,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-04-19T02:19:11.736923"
}
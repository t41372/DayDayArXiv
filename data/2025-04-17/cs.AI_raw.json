[
  {
    "arxiv_id": "2504.13180v1",
    "title": "PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding",
    "authors": [
      "Jang Hyun Cho",
      "Andrea Madotto",
      "Effrosyni Mavroudi",
      "Triantafyllos Afouras",
      "Tushar Nagarajan",
      "Muhammad Maaz",
      "Yale Song",
      "Tengyu Ma",
      "Shuming Hu",
      "Suyog Jain",
      "Miguel Martin",
      "Huiyu Wang",
      "Hanoona Rasheed",
      "Peize Sun",
      "Po-Yao Huang",
      "Daniel Bolya",
      "Nikhila Ravi",
      "Shashank Jain",
      "Tammy Stark",
      "Shane Moon",
      "Babak Damavandi",
      "Vivian Lee",
      "Andrew Westbury",
      "Salman Khan",
      "Philipp Krähenbühl",
      "Piotr Dollár",
      "Lorenzo Torresani",
      "Kristen Grauman",
      "Christoph Feichtenhofer"
    ],
    "abstract": "Vision-language models are integral to computer vision research, yet many\nhigh-performing models remain closed-source, obscuring their data, design and\ntraining recipe. The research community has responded by using distillation\nfrom black-box models to label training data, achieving strong benchmark\nresults, at the cost of measurable scientific progress. However, without\nknowing the details of the teacher model and its data sources, scientific\nprogress remains difficult to measure. In this paper, we study building a\nPerception Language Model (PLM) in a fully open and reproducible framework for\ntransparent research in image and video understanding. We analyze standard\ntraining pipelines without distillation from proprietary models and explore\nlarge-scale synthetic data to identify critical data gaps, particularly in\ndetailed video understanding. To bridge these gaps, we release 2.8M\nhuman-labeled instances of fine-grained video question-answer pairs and\nspatio-temporally grounded video captions. Additionally, we introduce\nPLM-VideoBench, a suite for evaluating challenging video understanding tasks\nfocusing on the ability to reason about \"what\", \"where\", \"when\", and \"how\" of a\nvideo. We make our work fully reproducible by providing data, training recipes,\ncode & models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Technical report",
    "pdf_url": "http://arxiv.org/pdf/2504.13180v1",
    "published_date": "2025-04-17 17:59:56 UTC",
    "updated_date": "2025-04-17 17:59:56 UTC"
  },
  {
    "arxiv_id": "2504.13173v1",
    "title": "It's All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization",
    "authors": [
      "Ali Behrouz",
      "Meisam Razaviyayn",
      "Peilin Zhong",
      "Vahab Mirrokni"
    ],
    "abstract": "Designing efficient and effective architectural backbones has been in the\ncore of research efforts to enhance the capability of foundation models.\nInspired by the human cognitive phenomenon of attentional bias-the natural\ntendency to prioritize certain events or stimuli-we reconceptualize neural\narchitectures, including Transformers, Titans, and modern linear recurrent\nneural networks as associative memory modules that learn a mapping of keys and\nvalues using an internal objective, referred to as attentional bias.\nSurprisingly, we observed that most existing sequence models leverage either\n(1) dot-product similarity, or (2) L2 regression objectives as their\nattentional bias. Going beyond these objectives, we present a set of\nalternative attentional bias configurations along with their effective\napproximations to stabilize their training procedure. We then reinterpret\nforgetting mechanisms in modern deep learning architectures as a form of\nretention regularization, providing a novel set of forget gates for sequence\nmodels. Building upon these insights, we present Miras, a general framework to\ndesign deep learning architectures based on four choices of: (i) associative\nmemory architecture, (ii) attentional bias objective, (iii) retention gate, and\n(iv) memory learning algorithm. We present three novel sequence models-Moneta,\nYaad, and Memora-that go beyond the power of existing linear RNNs while\nmaintaining a fast parallelizable training process. Our experiments show\ndifferent design choices in Miras yield models with varying strengths. For\nexample, certain instances of Miras achieve exceptional performance in special\ntasks such as language modeling, commonsense reasoning, and recall intensive\ntasks, even outperforming Transformers and other modern linear recurrent\nmodels.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.13173v1",
    "published_date": "2025-04-17 17:59:33 UTC",
    "updated_date": "2025-04-17 17:59:33 UTC"
  },
  {
    "arxiv_id": "2504.13171v1",
    "title": "Sleep-time Compute: Beyond Inference Scaling at Test-time",
    "authors": [
      "Kevin Lin",
      "Charlie Snell",
      "Yu Wang",
      "Charles Packer",
      "Sarah Wooders",
      "Ion Stoica",
      "Joseph E. Gonzalez"
    ],
    "abstract": "Scaling test-time compute has emerged as a key ingredient for enabling large\nlanguage models (LLMs) to solve difficult problems, but comes with high latency\nand inference cost. We introduce sleep-time compute, which allows models to\n\"think\" offline about contexts before queries are presented: by anticipating\nwhat queries users might ask and pre-computing useful quantities, we can\nsignificantly reduce the compute requirements at test-time. To demonstrate the\nefficacy of our method, we create modified versions of two reasoning tasks -\nStateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can\nreduce the amount of test-time compute needed to achieve the same accuracy by ~\n5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time\ncompute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic\nand 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic,\nwhich extends GSM-Symbolic by including multiple related queries per context.\nBy amortizing sleep-time compute across related queries about the same context\nusing Multi-Query GSM-Symbolic, we can decrease the average cost per query by\n2.5x. We then conduct additional analysis to understand when sleep-time compute\nis most effective, finding the predictability of the user query to be well\ncorrelated with the efficacy of sleep-time compute. Finally, we conduct a\ncase-study of applying sleep-time compute to a realistic agentic SWE task.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Code and data released at:\n  https://github.com/letta-ai/sleep-time-compute",
    "pdf_url": "http://arxiv.org/pdf/2504.13171v1",
    "published_date": "2025-04-17 17:59:25 UTC",
    "updated_date": "2025-04-17 17:59:25 UTC"
  },
  {
    "arxiv_id": "2504.13165v1",
    "title": "RUKA: Rethinking the Design of Humanoid Hands with Learning",
    "authors": [
      "Anya Zorin",
      "Irmak Guzey",
      "Billy Yan",
      "Aadhithya Iyer",
      "Lisa Kondrich",
      "Nikhil X. Bhattasali",
      "Lerrel Pinto"
    ],
    "abstract": "Dexterous manipulation is a fundamental capability for robotic systems, yet\nprogress has been limited by hardware trade-offs between precision,\ncompactness, strength, and affordability. Existing control methods impose\ncompromises on hand designs and applications. However, learning-based\napproaches present opportunities to rethink these trade-offs, particularly to\naddress challenges with tendon-driven actuation and low-cost materials. This\nwork presents RUKA, a tendon-driven humanoid hand that is compact, affordable,\nand capable. Made from 3D-printed parts and off-the-shelf components, RUKA has\n5 fingers with 15 underactuated degrees of freedom enabling diverse human-like\ngrasps. Its tendon-driven actuation allows powerful grasping in a compact,\nhuman-sized form factor. To address control challenges, we learn\njoint-to-actuator and fingertip-to-actuator models from motion-capture data\ncollected by the MANUS glove, leveraging the hand's morphological accuracy.\nExtensive evaluations demonstrate RUKA's superior reachability, durability, and\nstrength compared to other robotic hands. Teleoperation tasks further showcase\nRUKA's dexterous movements. The open-source design and assembly instructions of\nRUKA, code, and data are available at https://ruka-hand.github.io/.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Website at https://ruka-hand.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2504.13165v1",
    "published_date": "2025-04-17 17:58:59 UTC",
    "updated_date": "2025-04-17 17:58:59 UTC"
  },
  {
    "arxiv_id": "2504.13151v1",
    "title": "MIB: A Mechanistic Interpretability Benchmark",
    "authors": [
      "Aaron Mueller",
      "Atticus Geiger",
      "Sarah Wiegreffe",
      "Dana Arad",
      "Iván Arcuschin",
      "Adam Belfki",
      "Yik Siu Chan",
      "Jaden Fiotto-Kaufman",
      "Tal Haklay",
      "Michael Hanna",
      "Jing Huang",
      "Rohan Gupta",
      "Yaniv Nikankin",
      "Hadas Orgad",
      "Nikhil Prakash",
      "Anja Reusch",
      "Aruna Sankaranarayanan",
      "Shun Shao",
      "Alessandro Stolfo",
      "Martin Tutek",
      "Amir Zur",
      "David Bau",
      "Yonatan Belinkov"
    ],
    "abstract": "How can we know whether new mechanistic interpretability methods achieve real\nimprovements? In pursuit of meaningful and lasting evaluation standards, we\npropose MIB, a benchmark with two tracks spanning four tasks and five models.\nMIB favors methods that precisely and concisely recover relevant causal\npathways or specific causal variables in neural language models. The circuit\nlocalization track compares methods that locate the model components - and\nconnections between them - most important for performing a task (e.g.,\nattribution patching or information flow routes). The causal variable\nlocalization track compares methods that featurize a hidden vector, e.g.,\nsparse autoencoders (SAEs) or distributed alignment search (DAS), and locate\nmodel features for a causal variable relevant to the task. Using MIB, we find\nthat attribution and mask optimization methods perform best on circuit\nlocalization. For causal variable localization, we find that the supervised DAS\nmethod performs best, while SAE features are not better than neurons, i.e.,\nstandard dimensions of hidden vectors. These findings illustrate that MIB\nenables meaningful comparisons of methods, and increases our confidence that\nthere has been real progress in the field.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.13151v1",
    "published_date": "2025-04-17 17:55:45 UTC",
    "updated_date": "2025-04-17 17:55:45 UTC"
  },
  {
    "arxiv_id": "2504.13150v1",
    "title": "Readable Twins of Unreadable Models",
    "authors": [
      "Krzysztof Pancerz",
      "Piotr Kulicki",
      "Michał Kalisz",
      "Andrzej Burda",
      "Maciej Stanisławski",
      "Jaromir Sarzyński"
    ],
    "abstract": "Creating responsible artificial intelligence (AI) systems is an important\nissue in contemporary research and development of works on AI. One of the\ncharacteristics of responsible AI systems is their explainability. In the\npaper, we are interested in explainable deep learning (XDL) systems. On the\nbasis of the creation of digital twins of physical objects, we introduce the\nidea of creating readable twins (in the form of imprecise information flow\nmodels) for unreadable deep learning models. The complete procedure for\nswitching from the deep learning model (DLM) to the imprecise information flow\nmodel (IIFM) is presented. The proposed approach is illustrated with an example\nof a deep learning classification model for image recognition of handwritten\ndigits from the MNIST data set.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "Based on the abstract accepted for ISFS 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.13150v1",
    "published_date": "2025-04-17 17:55:34 UTC",
    "updated_date": "2025-04-17 17:55:34 UTC"
  },
  {
    "arxiv_id": "2504.13146v1",
    "title": "Antidistillation Sampling",
    "authors": [
      "Yash Savani",
      "Asher Trockman",
      "Zhili Feng",
      "Avi Schwarzschild",
      "Alexander Robey",
      "Marc Finzi",
      "J. Zico Kolter"
    ],
    "abstract": "Frontier models that generate extended reasoning traces inadvertently produce\nrich token sequences that can facilitate model distillation. Recognizing this\nvulnerability, model owners may seek sampling strategies that limit the\neffectiveness of distillation without compromising model performance.\n\\emph{Antidistillation sampling} provides exactly this capability. By\nstrategically modifying a model's next-token probability distribution,\nantidistillation sampling poisons reasoning traces, rendering them\nsignificantly less effective for distillation while preserving the model's\npractical utility. For further details, see https://antidistillation.com.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.13146v1",
    "published_date": "2025-04-17 17:54:14 UTC",
    "updated_date": "2025-04-17 17:54:14 UTC"
  },
  {
    "arxiv_id": "2504.13145v1",
    "title": "Exploring Expert Failures Improves LLM Agent Tuning",
    "authors": [
      "Li-Cheng Lan",
      "Andrew Bai",
      "Minhao Cheng",
      "Ruochen Wang",
      "Cho-Jui Hsieh",
      "Tianyi Zhou"
    ],
    "abstract": "Large Language Models (LLMs) have shown tremendous potential as agents,\nexcelling at tasks that require multiple rounds of reasoning and interactions.\nRejection Sampling Fine-Tuning (RFT) has emerged as an effective method for\nfinetuning LLMs as agents: it first imitates expert-generated successful\ntrajectories and further improves agentic skills through iterative fine-tuning\non successful, self-generated trajectories. However, since the expert (e.g.,\nGPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler\nscenarios, many complex subtasks remain unsolved and persistently\nout-of-distribution (OOD). Upon investigating these challenging subtasks, we\ndiscovered that previously failed expert trajectories can often provide\nvaluable guidance, e.g., plans and key actions, that can significantly improve\nagent exploration efficiency and acquisition of critical skills. Motivated by\nthese observations, we propose Exploring Expert Failures (EEF), which\nidentifies beneficial actions from failed expert trajectories and integrates\nthem into the training dataset. Potentially harmful actions are meticulously\nexcluded to prevent contamination of the model learning process. By leveraging\nthe beneficial actions in expert failures, EEF successfully solves some\npreviously unsolvable subtasks and improves agent tuning performance.\nRemarkably, our approach achieved a 62\\% win rate in WebShop, outperforming RFT\n(53. 6\\%) and GPT-4 (35. 6\\%), and to the best of our knowledge, setting a new\nstate-of-the-art as the first method to surpass a score of 0.81 in WebShop and\nexceed 81 in SciWorld.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.13145v1",
    "published_date": "2025-04-17 17:53:54 UTC",
    "updated_date": "2025-04-17 17:53:54 UTC"
  },
  {
    "arxiv_id": "2504.13143v1",
    "title": "$\\texttt{Complex-Edit}$: CoT-Like Instruction Generation for Complexity-Controllable Image Editing Benchmark",
    "authors": [
      "Siwei Yang",
      "Mude Hui",
      "Bingchen Zhao",
      "Yuyin Zhou",
      "Nataniel Ruiz",
      "Cihang Xie"
    ],
    "abstract": "We introduce $\\texttt{Complex-Edit}$, a comprehensive benchmark designed to\nsystematically evaluate instruction-based image editing models across\ninstructions of varying complexity. To develop this benchmark, we harness\nGPT-4o to automatically collect a diverse set of editing instructions at scale.\nOur approach follows a well-structured ``Chain-of-Edit'' pipeline: we first\ngenerate individual atomic editing tasks independently and then integrate them\nto form cohesive, complex instructions. Additionally, we introduce a suite of\nmetrics to assess various aspects of editing performance, along with a\nVLM-based auto-evaluation pipeline that supports large-scale assessments. Our\nbenchmark yields several notable insights: 1) Open-source models significantly\nunderperform relative to proprietary, closed-source models, with the\nperformance gap widening as instruction complexity increases; 2) Increased\ninstructional complexity primarily impairs the models' ability to retain key\nelements from the input images and to preserve the overall aesthetic quality;\n3) Decomposing a complex instruction into a sequence of atomic steps, executed\nin a step-by-step manner, substantially degrades performance across multiple\nmetrics; 4) A straightforward Best-of-N selection strategy improves results for\nboth direct editing and the step-by-step sequential approach; and 5) We observe\na ``curse of synthetic data'': when synthetic data is involved in model\ntraining, the edited images from such models tend to appear increasingly\nsynthetic as the complexity of the editing instructions rises -- a phenomenon\nthat intriguingly also manifests in the latest GPT-4o outputs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://ucsc-vlaa.github.io/Complex-Edit/, Dataset:\n  https://huggingface.co/datasets/UCSC-VLAA/Complex-Edit",
    "pdf_url": "http://arxiv.org/pdf/2504.13143v1",
    "published_date": "2025-04-17 17:51:59 UTC",
    "updated_date": "2025-04-17 17:51:59 UTC"
  },
  {
    "arxiv_id": "2504.13139v1",
    "title": "Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo",
    "authors": [
      "João Loula",
      "Benjamin LeBrun",
      "Li Du",
      "Ben Lipkin",
      "Clemente Pasti",
      "Gabriel Grand",
      "Tianyu Liu",
      "Yahya Emara",
      "Marjorie Freedman",
      "Jason Eisner",
      "Ryan Cotterel",
      "Vikash Mansinghka",
      "Alexander K. Lew",
      "Tim Vieira",
      "Timothy J. O'Donnell"
    ],
    "abstract": "A wide range of LM applications require generating text that conforms to\nsyntactic or semantic constraints. Imposing such constraints can be naturally\nframed as probabilistic conditioning, but exact generation from the resulting\ndistribution -- which can differ substantially from the LM's base distribution\n-- is generally intractable. In this work, we develop an architecture for\ncontrolled LM generation based on sequential Monte Carlo (SMC). Our SMC\nframework allows us to flexibly incorporate domain- and problem-specific\nconstraints at inference time, and efficiently reallocate computational\nresources in light of new information during the course of generation. By\ncomparing to a number of alternatives and ablations on four challenging domains\n-- Python code generation for data science, text-to-SQL, goal inference, and\nmolecule synthesis -- we demonstrate that, with little overhead, our approach\nallows small open-source language models to outperform models over 8x larger,\nas well as closed-source, fine-tuned ones. In support of the probabilistic\nperspective, we show that these performance improvements are driven by better\napproximation to the posterior distribution. Our system builds on the framework\nof Lew et al. (2023) and integrates with its language model probabilistic\nprogramming language, giving users a simple, programmable way to apply SMC to a\nbroad variety of controlled generation problems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "34 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.13139v1",
    "published_date": "2025-04-17 17:49:40 UTC",
    "updated_date": "2025-04-17 17:49:40 UTC"
  },
  {
    "arxiv_id": "2504.13131v1",
    "title": "NTIRE 2025 Challenge on Short-form UGC Video Quality Assessment and Enhancement: Methods and Results",
    "authors": [
      "Xin Li",
      "Kun Yuan",
      "Bingchen Li",
      "Fengbin Guan",
      "Yizhen Shao",
      "Zihao Yu",
      "Xijun Wang",
      "Yiting Lu",
      "Wei Luo",
      "Suhang Yao",
      "Ming Sun",
      "Chao Zhou",
      "Zhibo Chen",
      "Radu Timofte",
      "Yabin Zhang",
      "Ao-Xiang Zhang",
      "Tianwu Zhi",
      "Jianzhao Liu",
      "Yang Li",
      "Jingwen Xu",
      "Yiting Liao",
      "Yushen Zuo",
      "Mingyang Wu",
      "Renjie Li",
      "Shengyun Zhong",
      "Zhengzhong Tu",
      "Yufan Liu",
      "Xiangguang Chen",
      "Zuowei Cao",
      "Minhao Tang",
      "Shan Liu",
      "Kexin Zhang",
      "Jingfen Xie",
      "Yan Wang",
      "Kai Chen",
      "Shijie Zhao",
      "Yunchen Zhang",
      "Xiangkai Xu",
      "Hong Gao",
      "Ji Shi",
      "Yiming Bao",
      "Xiugang Dong",
      "Xiangsheng Zhou",
      "Yaofeng Tu",
      "Ying Liang",
      "Yiwen Wang",
      "Xinning Chai",
      "Yuxuan Zhang",
      "Zhengxue Cheng",
      "Yingsheng Qin",
      "Yucai Yang",
      "Rong Xie",
      "Li Song",
      "Wei Sun",
      "Kang Fu",
      "Linhan Cao",
      "Dandan Zhu",
      "Kaiwei Zhang",
      "Yucheng Zhu",
      "Zicheng Zhang",
      "Menghan Hu",
      "Xiongkuo Min",
      "Guangtao Zhai",
      "Zhi Jin",
      "Jiawei Wu",
      "Wei Wang",
      "Wenjian Zhang",
      "Yuhai Lan",
      "Gaoxiong Yi",
      "Hengyuan Na",
      "Wang Luo",
      "Di Wu",
      "MingYin Bai",
      "Jiawang Du",
      "Zilong Lu",
      "Zhenyu Jiang",
      "Hui Zeng",
      "Ziguan Cui",
      "Zongliang Gan",
      "Guijin Tang",
      "Xinglin Xie",
      "Kehuan Song",
      "Xiaoqiang Lu",
      "Licheng Jiao",
      "Fang Liu",
      "Xu Liu",
      "Puhua Chen",
      "Ha Thu Nguyen",
      "Katrien De Moor",
      "Seyed Ali Amirshahi",
      "Mohamed-Chaker Larabi",
      "Qi Tang",
      "Linfeng He",
      "Zhiyong Gao",
      "Zixuan Gao",
      "Guohua Zhang",
      "Zhiye Huang",
      "Yi Deng",
      "Qingmiao Jiang",
      "Lu Chen",
      "Yi Yang",
      "Xi Liao",
      "Nourine Mohammed Nadir",
      "Yuxuan Jiang",
      "Qiang Zhu",
      "Siyue Teng",
      "Fan Zhang",
      "Shuyuan Zhu",
      "Bing Zeng",
      "David Bull",
      "Meiqin Liu",
      "Chao Yao",
      "Yao Zhao"
    ],
    "abstract": "This paper presents a review for the NTIRE 2025 Challenge on Short-form UGC\nVideo Quality Assessment and Enhancement. The challenge comprises two tracks:\n(i) Efficient Video Quality Assessment (KVQ), and (ii) Diffusion-based Image\nSuper-Resolution (KwaiSR). Track 1 aims to advance the development of\nlightweight and efficient video quality assessment (VQA) models, with an\nemphasis on eliminating reliance on model ensembles, redundant weights, and\nother computationally expensive components in the previous IQA/VQA\ncompetitions. Track 2 introduces a new short-form UGC dataset tailored for\nsingle image super-resolution, i.e., the KwaiSR dataset. It consists of 1,800\nsynthetically generated S-UGC image pairs and 1,900 real-world S-UGC images,\nwhich are split into training, validation, and test sets using a ratio of\n8:1:1. The primary objective of the challenge is to drive research that\nbenefits the user experience of short-form UGC platforms such as Kwai and\nTikTok. This challenge attracted 266 participants and received 18 valid final\nsubmissions with corresponding fact sheets, significantly contributing to the\nprogress of short-form UGC VQA and image superresolution. The project is\npublicly available at https://github.com/lixinustc/KVQE-\nChallengeCVPR-NTIRE2025.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Challenge Report of NTIRE 2025; Methods from 18 Teams; Accepted by\n  CVPR Workshop; 21 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.13131v1",
    "published_date": "2025-04-17 17:45:34 UTC",
    "updated_date": "2025-04-17 17:45:34 UTC"
  },
  {
    "arxiv_id": "2504.13129v1",
    "title": "Science-T2I: Addressing Scientific Illusions in Image Synthesis",
    "authors": [
      "Jialuo Li",
      "Wenhao Chai",
      "Xingyu Fu",
      "Haiyang Xu",
      "Saining Xie"
    ],
    "abstract": "We present a novel approach to integrating scientific knowledge into\ngenerative models, enhancing their realism and consistency in image synthesis.\nFirst, we introduce Science-T2I, an expert-annotated adversarial dataset\ncomprising adversarial 20k image pairs with 9k prompts, covering wide distinct\nscientific knowledge categories. Leveraging Science-T2I, we present SciScore,\nan end-to-end reward model that refines the assessment of generated images\nbased on scientific knowledge, which is achieved by augmenting both the\nscientific comprehension and visual capabilities of pre-trained CLIP model.\nAdditionally, based on SciScore, we propose a two-stage training framework,\ncomprising a supervised fine-tuning phase and a masked online fine-tuning\nphase, to incorporate scientific knowledge into existing generative models.\nThrough comprehensive experiments, we demonstrate the effectiveness of our\nframework in establishing new standards for evaluating the scientific realism\nof generated content. Specifically, SciScore attains performance comparable to\nhuman-level, demonstrating a 5% improvement similar to evaluations conducted by\nexperienced human evaluators. Furthermore, by applying our proposed fine-tuning\nmethod to FLUX, we achieve a performance enhancement exceeding 50% on SciScore.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to CVPR 2025. Code, docs, weight, benchmark and training\n  data are all avaliable at https://jialuo-li.github.io/Science-T2I-Web",
    "pdf_url": "http://arxiv.org/pdf/2504.13129v1",
    "published_date": "2025-04-17 17:44:19 UTC",
    "updated_date": "2025-04-17 17:44:19 UTC"
  },
  {
    "arxiv_id": "2504.13128v1",
    "title": "FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on Technical Documents",
    "authors": [
      "Nandan Thakur",
      "Jimmy Lin",
      "Sam Havens",
      "Michael Carbin",
      "Omar Khattab",
      "Andrew Drozdov"
    ],
    "abstract": "We introduce FreshStack, a reusable framework for automatically building\ninformation retrieval (IR) evaluation benchmarks from community-asked questions\nand answers. FreshStack conducts the following steps: (1) automatic corpus\ncollection from code and technical documentation, (2) nugget generation from\ncommunity-asked questions and answers, and (3) nugget-level support, retrieving\ndocuments using a fusion of retrieval techniques and hybrid architectures. We\nuse FreshStack to build five datasets on fast-growing, recent, and niche topics\nto ensure the tasks are sufficiently challenging. On FreshStack, existing\nretrieval models, when applied out-of-the-box, significantly underperform\noracle approaches on all five topics, denoting plenty of headroom to improve IR\nquality. In addition, we identify cases where rerankers do not clearly improve\nfirst-stage retrieval accuracy (two out of five topics). We hope that\nFreshStack will facilitate future work toward constructing realistic, scalable,\nand uncontaminated IR and RAG evaluation benchmarks. FreshStack datasets are\navailable at: https://fresh-stack.github.io.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.13128v1",
    "published_date": "2025-04-17 17:44:06 UTC",
    "updated_date": "2025-04-17 17:44:06 UTC"
  },
  {
    "arxiv_id": "2504.13125v1",
    "title": "LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM Leaderboard",
    "authors": [
      "Varun Rao",
      "Youran Sun",
      "Mahendra Kumar",
      "Tejas Mutneja",
      "Agastya Mukherjee",
      "Haizhao Yang"
    ],
    "abstract": "This paper investigates the application of large language models (LLMs) to\nfinancial tasks. We fine-tuned foundation models using the Open FinLLM\nLeaderboard as a benchmark. Building on Qwen2.5 and Deepseek-R1, we employed\ntechniques including supervised fine-tuning (SFT), direct preference\noptimization (DPO), and reinforcement learning (RL) to enhance their financial\ncapabilities. The fine-tuned models demonstrated substantial performance gains\nacross a wide range of financial tasks. Moreover, we measured the data scaling\nlaw in the financial domain. Our work demonstrates the potential of large\nlanguage models (LLMs) in financial applications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.13125v1",
    "published_date": "2025-04-17 17:42:02 UTC",
    "updated_date": "2025-04-17 17:42:02 UTC"
  },
  {
    "arxiv_id": "2504.13123v1",
    "title": "Low-hallucination Synthetic Captions for Large-Scale Vision-Language Model Pre-training",
    "authors": [
      "Xinsong Zhang",
      "Yarong Zeng",
      "Xinting Huang",
      "Hu Hu",
      "Runquan Xie",
      "Han Hu",
      "Zhanhui Kang"
    ],
    "abstract": "In recent years, the field of vision-language model pre-training has\nexperienced rapid advancements, driven primarily by the continuous enhancement\nof textual capabilities in large language models. However, existing training\nparadigms for multimodal large language models heavily rely on high-quality\nimage-text pairs. As models and data scales grow exponentially, the\navailability of such meticulously curated data has become increasingly scarce\nand saturated, thereby severely limiting further advancements in this domain.\nThis study investigates scalable caption generation techniques for\nvision-language model pre-training and demonstrates that large-scale\nlow-hallucination synthetic captions can serve dual purposes: 1) acting as a\nviable alternative to real-world data for pre-training paradigms and 2)\nachieving superior performance enhancement when integrated into vision-language\nmodels through empirical validation. This paper presents three key\ncontributions: 1) a novel pipeline for generating high-quality,\nlow-hallucination, and knowledge-rich synthetic captions. Our continuous DPO\nmethodology yields remarkable results in reducing hallucinations. Specifically,\nthe non-hallucination caption rate on a held-out test set increases from 48.2%\nto 77.9% for a 7B-size model. 2) Comprehensive empirical validation reveals\nthat our synthetic captions confer superior pre-training advantages over their\ncounterparts. Across 35 vision language tasks, the model trained with our data\nachieves a significant performance gain of at least 6.2% compared to alt-text\npairs and other previous work. Meanwhile, it also offers considerable support\nin the text-to-image domain. With our dataset, the FID score is reduced by 17.1\non a real-world validation benchmark and 13.3 on the MSCOCO validation\nbenchmark. 3) We will release Hunyuan-Recap100M, a low-hallucination and\nknowledge-intensive synthetic caption dataset.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.13123v1",
    "published_date": "2025-04-17 17:40:06 UTC",
    "updated_date": "2025-04-17 17:40:06 UTC"
  },
  {
    "arxiv_id": "2504.13120v1",
    "title": "Probing and Inducing Combinational Creativity in Vision-Language Models",
    "authors": [
      "Yongqian Peng",
      "Yuxi Ma",
      "Mengmeng Wang",
      "Yuxuan Wang",
      "Yizhou Wang",
      "Chi Zhang",
      "Yixin Zhu",
      "Zilong Zheng"
    ],
    "abstract": "The ability to combine existing concepts into novel ideas stands as a\nfundamental hallmark of human intelligence. Recent advances in Vision-Language\nModels (VLMs) like GPT-4V and DALLE-3 have sparked debate about whether their\noutputs reflect combinational creativity--defined by M. A. Boden (1998) as\nsynthesizing novel ideas through combining existing concepts--or sophisticated\npattern matching of training data. Drawing inspiration from cognitive science,\nwe investigate the combinational creativity of VLMs from the lens of concept\nblending. We propose the Identification-Explanation-Implication (IEI)\nframework, which decomposes creative processes into three levels: identifying\ninput spaces, extracting shared attributes, and deriving novel semantic\nimplications. To validate this framework, we curate CreativeMashup, a\nhigh-quality dataset of 666 artist-generated visual mashups annotated according\nto the IEI framework. Through extensive experiments, we demonstrate that in\ncomprehension tasks, best VLMs have surpassed average human performance while\nfalling short of expert-level understanding; in generation tasks, incorporating\nour IEI framework into the generation pipeline significantly enhances the\ncreative quality of VLMs outputs. Our findings establish both a theoretical\nfoundation for evaluating artificial creativity and practical guidelines for\nimproving creative generation in VLMs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://ppyyqq.github.io/aicc/ The first two authors\n  contribute equally",
    "pdf_url": "http://arxiv.org/pdf/2504.13120v1",
    "published_date": "2025-04-17 17:38:18 UTC",
    "updated_date": "2025-04-17 17:38:18 UTC"
  },
  {
    "arxiv_id": "2504.13102v1",
    "title": "A Multi-task Learning Balanced Attention Convolutional Neural Network Model for Few-shot Underwater Acoustic Target Recognition",
    "authors": [
      "Wei Huang",
      "Shumeng Sun",
      "Junpeng Lu",
      "Zhenpeng Xu",
      "Zhengyang Xiu",
      "Hao Zhang"
    ],
    "abstract": "Underwater acoustic target recognition (UATR) is of great significance for\nthe protection of marine diversity and national defense security. The\ndevelopment of deep learning provides new opportunities for UATR, but faces\nchallenges brought by the scarcity of reference samples and complex\nenvironmental interference. To address these issues, we proposes a multi-task\nbalanced channel attention convolutional neural network (MT-BCA-CNN). The\nmethod integrates a channel attention mechanism with a multi-task learning\nstrategy, constructing a shared feature extractor and multi-task classifiers to\njointly optimize target classification and feature reconstruction tasks. The\nchannel attention mechanism dynamically enhances discriminative acoustic\nfeatures such as harmonic structures while suppressing noise. Experiments on\nthe Watkins Marine Life Dataset demonstrate that MT-BCA-CNN achieves 97\\%\nclassification accuracy and 95\\% $F1$-score in 27-class few-shot scenarios,\nsignificantly outperforming traditional CNN and ACNN models, as well as popular\nstate-of-the-art UATR methods. Ablation studies confirm the synergistic\nbenefits of multi-task learning and attention mechanisms, while a dynamic\nweighting adjustment strategy effectively balances task contributions. This\nwork provides an efficient solution for few-shot underwater acoustic\nrecognition, advancing research in marine bioacoustics and sonar signal\nprocessing.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.13102v1",
    "published_date": "2025-04-17 17:11:32 UTC",
    "updated_date": "2025-04-17 17:11:32 UTC"
  },
  {
    "arxiv_id": "2504.13101v1",
    "title": "An Empirically Grounded Identifiability Theory Will Accelerate Self-Supervised Learning Research",
    "authors": [
      "Patrik Reizinger",
      "Randall Balestriero",
      "David Klindt",
      "Wieland Brendel"
    ],
    "abstract": "Self-Supervised Learning (SSL) powers many current AI systems. As research\ninterest and investment grow, the SSL design space continues to expand. The\nPlatonic view of SSL, following the Platonic Representation Hypothesis (PRH),\nsuggests that despite different methods and engineering approaches, all\nrepresentations converge to the same Platonic ideal. However, this phenomenon\nlacks precise theoretical explanation. By synthesizing evidence from\nIdentifiability Theory (IT), we show that the PRH can emerge in SSL. However,\ncurrent IT cannot explain SSL's empirical success. To bridge the gap between\ntheory and practice, we propose expanding IT into what we term Singular\nIdentifiability Theory (SITh), a broader theoretical framework encompassing the\nentire SSL pipeline. SITh would allow deeper insights into the implicit data\nassumptions in SSL and advance the field towards learning more interpretable\nand generalizable representations. We highlight three critical directions for\nfuture research: 1) training dynamics and convergence properties of SSL; 2) the\nimpact of finite samples, batch size, and data diversity; and 3) the role of\ninductive biases in architecture, augmentations, initialization schemes, and\noptimizers.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.13101v1",
    "published_date": "2025-04-17 17:10:33 UTC",
    "updated_date": "2025-04-17 17:10:33 UTC"
  },
  {
    "arxiv_id": "2504.13079v1",
    "title": "Retrieval-Augmented Generation with Conflicting Evidence",
    "authors": [
      "Han Wang",
      "Archiki Prasad",
      "Elias Stengel-Eskin",
      "Mohit Bansal"
    ],
    "abstract": "Large language model (LLM) agents are increasingly employing\nretrieval-augmented generation (RAG) to improve the factuality of their\nresponses. However, in practice, these systems often need to handle ambiguous\nuser queries and potentially conflicting information from multiple sources\nwhile also suppressing inaccurate information from noisy or irrelevant\ndocuments. Prior work has generally studied and addressed these challenges in\nisolation, considering only one aspect at a time, such as handling ambiguity or\nrobustness to noise and misinformation. We instead consider multiple factors\nsimultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and\nMisinformation in Documents), a new dataset that simulates complex and\nrealistic scenarios for conflicting evidence for a user query, including\nambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent\napproach in which LLM agents debate over the merits of an answer over multiple\nrounds, allowing an aggregator to collate responses corresponding to\ndisambiguated entities while discarding misinformation and noise, thereby\nhandling diverse sources of conflict jointly. We demonstrate the effectiveness\nof MADAM-RAG using both closed and open-source models on AmbigDocs -- which\nrequires presenting all valid answers for ambiguous queries -- improving over\nstrong RAG baselines by up to 11.40% and on FaithEval -- which requires\nsuppressing misinformation -- where we improve by up to 15.80% (absolute) with\nLlama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for\nexisting RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match\nscore). While MADAM-RAG begins to address these conflicting factors, our\nanalysis indicates that a substantial gap remains especially when increasing\nthe level of imbalance in supporting evidence and misinformation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Our data and code is available at:\n  https://github.com/HanNight/RAMDocs",
    "pdf_url": "http://arxiv.org/pdf/2504.13079v1",
    "published_date": "2025-04-17 16:46:11 UTC",
    "updated_date": "2025-04-17 16:46:11 UTC"
  },
  {
    "arxiv_id": "2504.13078v1",
    "title": "Enhancing Person-to-Person Virtual Try-On with Multi-Garment Virtual Try-Off",
    "authors": [
      "Riza Velioglu",
      "Petra Bevandic",
      "Robin Chan",
      "Barbara Hammer"
    ],
    "abstract": "Computer vision is transforming fashion through Virtual Try-On (VTON) and\nVirtual Try-Off (VTOFF). VTON generates images of a person in a specified\ngarment using a target photo and a standardized garment image, while a more\nchallenging variant, Person-to-Person Virtual Try-On (p2p-VTON), uses a photo\nof another person wearing the garment. VTOFF, on the other hand, extracts\nstandardized garment images from clothed individuals. We introduce TryOffDiff,\na diffusion-based VTOFF model. Built on a latent diffusion framework with\nSigLIP image conditioning, it effectively captures garment properties like\ntexture, shape, and patterns. TryOffDiff achieves state-of-the-art results on\nVITON-HD and strong performance on DressCode dataset, covering upper-body,\nlower-body, and dresses. Enhanced with class-specific embeddings, it pioneers\nmulti-garment VTOFF, the first of its kind. When paired with VTON models, it\nimproves p2p-VTON by minimizing unwanted attribute transfer, such as skin\ncolor. Code is available at: https://rizavelioglu.github.io/tryoffdiff/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.13078v1",
    "published_date": "2025-04-17 16:45:18 UTC",
    "updated_date": "2025-04-17 16:45:18 UTC"
  },
  {
    "arxiv_id": "2504.13068v1",
    "title": "Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative Classification Models",
    "authors": [
      "Sudesh Ramesh Bhagat",
      "Ibne Farabi Shihab",
      "Anuj Sharma"
    ],
    "abstract": "This study explores the relationship between deep learning (DL) model\naccuracy and expert agreement in the classification of crash narratives. We\nevaluate five DL models -- including BERT variants, the Universal Sentence\nEncoder (USE), and a zero-shot classifier -- against expert-labeled data and\nnarrative text. The analysis is further extended to four large language models\n(LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our results reveal a counterintuitive\ntrend: models with higher technical accuracy often exhibit lower agreement with\ndomain experts, whereas LLMs demonstrate greater expert alignment despite\nrelatively lower accuracy scores. To quantify and interpret model-expert\nagreement, we employ Cohen's Kappa, Principal Component Analysis (PCA), and\nSHAP-based explainability techniques. Findings indicate that expert-aligned\nmodels tend to rely more on contextual and temporal language cues, rather than\nlocation-specific keywords. These results underscore that accuracy alone is\ninsufficient for evaluating models in safety-critical NLP applications. We\nadvocate for incorporating expert agreement as a complementary metric in model\nevaluation frameworks and highlight the promise of LLMs as interpretable,\nscalable tools for crash analysis pipelines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.13068v1",
    "published_date": "2025-04-17 16:29:08 UTC",
    "updated_date": "2025-04-17 16:29:08 UTC"
  },
  {
    "arxiv_id": "2504.13059v1",
    "title": "RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins",
    "authors": [
      "Yao Mu",
      "Tianxing Chen",
      "Zanxin Chen",
      "Shijia Peng",
      "Zhiqian Lan",
      "Zeyu Gao",
      "Zhixuan Liang",
      "Qiaojun Yu",
      "Yude Zou",
      "Mingkun Xu",
      "Lunkai Lin",
      "Zhiqiang Xie",
      "Mingyu Ding",
      "Ping Luo"
    ],
    "abstract": "In the rapidly advancing field of robotics, dual-arm coordination and complex\nobject manipulation are essential capabilities for developing advanced\nautonomous systems. However, the scarcity of diverse, high-quality\ndemonstration data and real-world-aligned evaluation benchmarks severely limits\nsuch development. To address this, we introduce RoboTwin, a generative digital\ntwin framework that uses 3D generative foundation models and large language\nmodels to produce diverse expert datasets and provide a real-world-aligned\nevaluation platform for dual-arm robotic tasks. Specifically, RoboTwin creates\nvaried digital twins of objects from single 2D images, generating realistic and\ninteractive scenarios. It also introduces a spatial relation-aware code\ngeneration framework that combines object annotations with large language\nmodels to break down tasks, determine spatial constraints, and generate precise\nrobotic movement code. Our framework offers a comprehensive benchmark with both\nsimulated and real-world data, enabling standardized evaluation and better\nalignment between simulated training and real-world performance. We validated\nour approach using the open-source COBOT Magic Robot platform. Policies\npre-trained on RoboTwin-generated data and fine-tuned with limited real-world\nsamples demonstrate significant potential for enhancing dual-arm robotic\nmanipulation systems by improving success rates by over 70% for single-arm\ntasks and over 40% for dual-arm tasks compared to models trained solely on\nreal-world data.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.RO",
    "comment": "CVPR 2025 Highlight. 22 pages. Project page:\n  https://robotwin-benchmark.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2504.13059v1",
    "published_date": "2025-04-17 16:14:24 UTC",
    "updated_date": "2025-04-17 16:14:24 UTC"
  },
  {
    "arxiv_id": "2504.13054v1",
    "title": "Aspect-Based Summarization with Self-Aspect Retrieval Enhanced Generation",
    "authors": [
      "Yichao Feng",
      "Shuai Zhao",
      "Yueqiu Li",
      "Luwei Xiao",
      "Xiaobao Wu",
      "Anh Tuan Luu"
    ],
    "abstract": "Aspect-based summarization aims to generate summaries tailored to specific\naspects, addressing the resource constraints and limited generalizability of\ntraditional summarization approaches. Recently, large language models have\nshown promise in this task without the need for training. However, they rely\nexcessively on prompt engineering and face token limits and hallucination\nchallenges, especially with in-context learning. To address these challenges,\nin this paper, we propose a novel framework for aspect-based summarization:\nSelf-Aspect Retrieval Enhanced Summary Generation. Rather than relying solely\non in-context learning, given an aspect, we employ an embedding-driven\nretrieval mechanism to identify its relevant text segments. This approach\nextracts the pertinent content while avoiding unnecessary details, thereby\nmitigating the challenge of token limits. Moreover, our framework optimizes\ntoken usage by deleting unrelated parts of the text and ensuring that the model\ngenerates output strictly based on the given aspect. With extensive experiments\non benchmark datasets, we demonstrate that our framework not only achieves\nsuperior performance but also effectively mitigates the token limitation\nproblem.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.13054v1",
    "published_date": "2025-04-17 16:09:57 UTC",
    "updated_date": "2025-04-17 16:09:57 UTC"
  },
  {
    "arxiv_id": "2504.13048v1",
    "title": "Design Topological Materials by Reinforcement Fine-Tuned Generative Model",
    "authors": [
      "Haosheng Xu",
      "Dongheng Qian",
      "Zhixuan Liu",
      "Yadong Jiang",
      "Jing Wang"
    ],
    "abstract": "Topological insulators (TIs) and topological crystalline insulators (TCIs)\nare materials with unconventional electronic properties, making their discovery\nhighly valuable for practical applications. However, such materials,\nparticularly those with a full band gap, remain scarce. Given the limitations\nof traditional approaches that scan known materials for candidates, we focus on\nthe generation of new topological materials through a generative model.\nSpecifically, we apply reinforcement fine-tuning (ReFT) to a pre-trained\ngenerative model, thereby aligning the model's objectives with our material\ndesign goals. We demonstrate that ReFT is effective in enhancing the model's\nability to generate TIs and TCIs, with minimal compromise on the stability of\nthe generated materials. Using the fine-tuned model, we successfully identify a\nlarge number of new topological materials, with Ge$_2$Bi$_2$O$_6$ serving as a\nrepresentative example--a TI with a full band gap of 0.26 eV, ranking among the\nlargest known in this category.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.13048v1",
    "published_date": "2025-04-17 16:05:24 UTC",
    "updated_date": "2025-04-17 16:05:24 UTC"
  },
  {
    "arxiv_id": "2504.13042v1",
    "title": "Event-Enhanced Blurry Video Super-Resolution",
    "authors": [
      "Dachun Kai",
      "Yueyi Zhang",
      "Jin Wang",
      "Zeyu Xiao",
      "Zhiwei Xiong",
      "Xiaoyan Sun"
    ],
    "abstract": "In this paper, we tackle the task of blurry video super-resolution (BVSR),\naiming to generate high-resolution (HR) videos from low-resolution (LR) and\nblurry inputs. Current BVSR methods often fail to restore sharp details at high\nresolutions, resulting in noticeable artifacts and jitter due to insufficient\nmotion information for deconvolution and the lack of high-frequency details in\nLR frames. To address these challenges, we introduce event signals into BVSR\nand propose a novel event-enhanced network, Ev-DeblurVSR. To effectively fuse\ninformation from frames and events for feature deblurring, we introduce a\nreciprocal feature deblurring module that leverages motion information from\nintra-frame events to deblur frame features while reciprocally using global\nscene context from the frames to enhance event features. Furthermore, to\nenhance temporal consistency, we propose a hybrid deformable alignment module\nthat fully exploits the complementary motion information from inter-frame\nevents and optical flow to improve motion estimation in the deformable\nalignment process. Extensive evaluations demonstrate that Ev-DeblurVSR\nestablishes a new state-of-the-art performance on both synthetic and real-world\ndatasets. Notably, on real data, our method is +2.59 dB more accurate and\n7.28$\\times$ faster than the recent best BVSR baseline FMA-Net. Code:\nhttps://github.com/DachunKai/Ev-DeblurVSR.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "AAAI 2025. Project page:\n  https://dachunkai.github.io/evtexture.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2504.13042v1",
    "published_date": "2025-04-17 15:55:41 UTC",
    "updated_date": "2025-04-17 15:55:41 UTC"
  },
  {
    "arxiv_id": "2504.13037v1",
    "title": "Towards Cardiac MRI Foundation Models: Comprehensive Visual-Tabular Representations for Whole-Heart Assessment and Beyond",
    "authors": [
      "Yundi Zhang",
      "Paul Hager",
      "Che Liu",
      "Suprosanna Shit",
      "Chen Chen",
      "Daniel Rueckert",
      "Jiazhen Pan"
    ],
    "abstract": "Cardiac magnetic resonance imaging is the gold standard for non-invasive\ncardiac assessment, offering rich spatio-temporal views of the cardiac anatomy\nand physiology. Patient-level health factors, such as demographics, metabolic,\nand lifestyle, are known to substantially influence cardiovascular health and\ndisease risk, yet remain uncaptured by CMR alone. To holistically understand\ncardiac health and to enable the best possible interpretation of an\nindividual's disease risk, CMR and patient-level factors must be jointly\nexploited within an integrated framework. Recent multi-modal approaches have\nbegun to bridge this gap, yet they often rely on limited spatio-temporal data\nand focus on isolated clinical tasks, thereby hindering the development of a\ncomprehensive representation for cardiac health evaluation. To overcome these\nlimitations, we introduce ViTa, a step toward foundation models that delivers a\ncomprehensive representation of the heart and a precise interpretation of\nindividual disease risk. Leveraging data from 42,000 UK Biobank participants,\nViTa integrates 3D+T cine stacks from short-axis and long-axis views, enabling\na complete capture of the cardiac cycle. These imaging data are then fused with\ndetailed tabular patient-level factors, enabling context-aware insights. This\nmulti-modal paradigm supports a wide spectrum of downstream tasks, including\ncardiac phenotype and physiological feature prediction, segmentation, and\nclassification of cardiac and metabolic diseases within a single unified\nframework. By learning a shared latent representation that bridges rich imaging\nfeatures and patient context, ViTa moves beyond traditional, task-specific\nmodels toward a universal, patient-specific understanding of cardiac health,\nhighlighting its potential to advance clinical utility and scalability in\ncardiac analysis.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.13037v1",
    "published_date": "2025-04-17 15:46:19 UTC",
    "updated_date": "2025-04-17 15:46:19 UTC"
  },
  {
    "arxiv_id": "2504.13035v1",
    "title": "Prototypes are Balanced Units for Efficient and Effective Partially Relevant Video Retrieval",
    "authors": [
      "WonJun Moon",
      "Cheol-Ho Cho",
      "Woojin Jun",
      "Minho Shim",
      "Taeoh Kim",
      "Inwoong Lee",
      "Dongyoon Wee",
      "Jae-Pil Heo"
    ],
    "abstract": "In a retrieval system, simultaneously achieving search accuracy and\nefficiency is inherently challenging. This challenge is particularly pronounced\nin partially relevant video retrieval (PRVR), where incorporating more diverse\ncontext representations at varying temporal scales for each video enhances\naccuracy but increases computational and memory costs. To address this\ndichotomy, we propose a prototypical PRVR framework that encodes diverse\ncontexts within a video into a fixed number of prototypes. We then introduce\nseveral strategies to enhance text association and video understanding within\nthe prototypes, along with an orthogonal objective to ensure that the\nprototypes capture a diverse range of content. To keep the prototypes\nsearchable via text queries while accurately encoding video contexts, we\nimplement cross- and uni-modal reconstruction tasks. The cross-modal\nreconstruction task aligns the prototypes with textual features within a shared\nspace, while the uni-modal reconstruction task preserves all video contexts\nduring encoding. Additionally, we employ a video mixing technique to provide\nweak guidance to further align prototypes and associated textual\nrepresentations. Extensive evaluations on TVR, ActivityNet-Captions, and\nQVHighlights validate the effectiveness of our approach without sacrificing\nefficiency.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.13035v1",
    "published_date": "2025-04-17 15:43:29 UTC",
    "updated_date": "2025-04-17 15:43:29 UTC"
  },
  {
    "arxiv_id": "2504.13032v1",
    "title": "InstructRAG: Leveraging Retrieval-Augmented Generation on Instruction Graphs for LLM-Based Task Planning",
    "authors": [
      "Zheng Wang",
      "Shu Xian Teo",
      "Jun Jie Chew",
      "Wei Shi"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have enabled their use as\nagents for planning complex tasks. Existing methods typically rely on a\nthought-action-observation (TAO) process to enhance LLM performance, but these\napproaches are often constrained by the LLMs' limited knowledge of complex\ntasks. Retrieval-augmented generation (RAG) offers new opportunities by\nleveraging external databases to ground generation in retrieved information. In\nthis paper, we identify two key challenges (enlargability and transferability)\nin applying RAG to task planning. We propose InstructRAG, a novel solution\nwithin a multi-agent meta-reinforcement learning framework, to address these\nchallenges. InstructRAG includes a graph to organize past instruction paths\n(sequences of correct actions), an RL-Agent with Reinforcement Learning to\nexpand graph coverage for enlargability, and an ML-Agent with Meta-Learning to\nimprove task generalization for transferability. The two agents are trained\nend-to-end to optimize overall planning performance. Our experiments on four\nwidely used task planning datasets demonstrate that InstructRAG significantly\nenhances performance and adapts efficiently to new tasks, achieving up to a\n19.2% improvement over the best existing approach.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted by SIGIR 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.13032v1",
    "published_date": "2025-04-17 15:41:39 UTC",
    "updated_date": "2025-04-17 15:41:39 UTC"
  },
  {
    "arxiv_id": "2504.13021v1",
    "title": "Pose and Facial Expression Transfer by using StyleGAN",
    "authors": [
      "Petr Jahoda",
      "Jan Cech"
    ],
    "abstract": "We propose a method to transfer pose and expression between face images.\nGiven a source and target face portrait, the model produces an output image in\nwhich the pose and expression of the source face image are transferred onto the\ntarget identity. The architecture consists of two encoders and a mapping\nnetwork that projects the two inputs into the latent space of StyleGAN2, which\nfinally generates the output. The training is self-supervised from video\nsequences of many individuals. Manual labeling is not required. Our model\nenables the synthesis of random identities with controllable pose and\nexpression. Close-to-real-time performance is achieved.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at CVWW 2024. Presented in Terme Olimia, Slovenia",
    "pdf_url": "http://arxiv.org/pdf/2504.13021v1",
    "published_date": "2025-04-17 15:29:41 UTC",
    "updated_date": "2025-04-17 15:29:41 UTC"
  },
  {
    "arxiv_id": "2504.12996v1",
    "title": "SHA256 at SemEval-2025 Task 4: Selective Amnesia -- Constrained Unlearning for Large Language Models via Knowledge Isolation",
    "authors": [
      "Saransh Agrawal",
      "Kuan-Hao Huang"
    ],
    "abstract": "Large language models (LLMs) frequently memorize sensitive information during\ntraining, posing risks when deploying publicly accessible models. Current\nmachine unlearning methods struggle to selectively remove specific data\nassociations without degrading overall model capabilities. This paper presents\nour solution to SemEval-2025 Task 4 on targeted unlearning, which introduces a\ntwo-stage methodology that combines causal mediation analysis with\nlayer-specific optimization. Through systematic causal tracing experiments on\nOLMo architectures (1B and 7B parameters), we identify the critical role of the\nfirst few transformer layers (layers 0-5) in storing subject-attribute\nassociations within MLP modules. Building on this insight, we develop a\nconstrained optimization approach that freezes upper layers while applying a\nnovel joint loss function to lower layers-simultaneously maximizing forget set\nloss via output token cross-entropy penalties and minimizing retain set\ndeviation through adaptive regularization. Our method achieves 2nd place in the\n1B model track, demonstrating strong task performance while maintaining 88% of\nbaseline MMLU accuracy. These results establish causal-informed layer\noptimization as a promising paradigm for efficient, precise unlearning in LLMs,\noffering a significant step forward in addressing data privacy concerns in AI\nsystems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, In Proceedings of The 19th International Workshop on\n  Semantic Evaluation (SemEval), 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.12996v1",
    "published_date": "2025-04-17 15:05:40 UTC",
    "updated_date": "2025-04-17 15:05:40 UTC"
  },
  {
    "arxiv_id": "2504.12984v1",
    "title": "A Virtual Machine for Arbitrary Low-Precision GPGPU Computation in LLM Serving",
    "authors": [
      "Yaoyao Ding",
      "Bohan Hou",
      "Xiao Zhang",
      "Allan Lin",
      "Tianqi Chen",
      "Cody Yu Hao",
      "Yida Wang",
      "Gennady Pekhimenko"
    ],
    "abstract": "Serving Large Language Models (LLMs) is critical for AI-powered applications\nbut demands substantial computational resources, particularly in memory\nbandwidth and computational throughput. Low-precision computation has emerged\nas a key technique to improve efficiency while reducing resource consumption.\nExisting approaches for generating low-precision kernels are limited to weight\nbit widths that are powers of two and suffer from suboptimal performance due to\nhigh-level GPU programming abstractions. These abstractions restrict critical\noptimizations, such as fine-grained register management and optimized memory\naccess patterns, which are essential for efficient low-precision computations.\nIn this paper, we introduce a virtual machine (VM) designed for General-Purpose\nGPU (GPGPU) computing, enabling support for low-precision data types with\narbitrary bit widths while maintaining GPU programmability. The proposed VM\nfeatures a thread-block-level programming model, a hierarchical memory space, a\nnovel algebraic layout system, and extensive support for diverse low-precision\ndata types. VM programs are compiled into highly efficient GPU programs with\nautomatic vectorization and instruction selection. Extensive experiments\ndemonstrate that our VM efficiently supports a full spectrum of low-precision\ndata types, and outperforms state-of-the-art low-precision kernels on their\nsupported types. Compared to existing compilers like Triton and Ladder, as well\nas hand-optimized kernels such as QuantLLM and Marlin, our VM achieves\nperformance improvements of 1.75x, 2.61x, 1.29x and 1.03x, respectively.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.12984v1",
    "published_date": "2025-04-17 14:45:03 UTC",
    "updated_date": "2025-04-17 14:45:03 UTC"
  },
  {
    "arxiv_id": "2504.12982v1",
    "title": "Accommodate Knowledge Conflicts in Retrieval-augmented LLMs: Towards Reliable Response Generation in the Wild",
    "authors": [
      "Jiatai Wang",
      "Zhiwei Xu",
      "Di Jin",
      "Xuewen Yang",
      "Tao Li"
    ],
    "abstract": "The proliferation of large language models (LLMs) has significantly advanced\ninformation retrieval systems, particularly in response generation (RG).\nUnfortunately, LLMs often face knowledge conflicts between internal memory and\nretrievaled external information, arising from misinformation, biases, or\noutdated knowledge. These conflicts undermine response reliability and\nintroduce uncertainty in decision-making. In this work, we analyze how LLMs\nnavigate knowledge conflicts from an information-theoretic perspective and\nreveal that when conflicting and supplementary information exhibit significant\ndifferences, LLMs confidently resolve their preferences. However, when the\ndistinction is ambiguous, LLMs experience heightened uncertainty. Based on this\ninsight, we propose Swin-VIB, a novel framework that integrates a pipeline of\nvariational information bottleneck models into adaptive augmentation of\nretrieved information and guiding LLM preference in response generation.\nExtensive experiments on single-choice, open-ended question-answering (QA), and\nretrieval augmented generation (RAG) validate our theoretical findings and\ndemonstrate the efficacy of Swin-VIB. Notably, our method improves\nsingle-choice task accuracy by at least 7.54\\% over competitive baselines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12982v1",
    "published_date": "2025-04-17 14:40:31 UTC",
    "updated_date": "2025-04-17 14:40:31 UTC"
  },
  {
    "arxiv_id": "2504.12977v1",
    "title": "A Phenomenological Approach to Analyzing User Queries in IT Systems Using Heidegger's Fundamental Ontology",
    "authors": [
      "Maksim Vishnevskiy"
    ],
    "abstract": "This paper presents a novel research analytical IT system grounded in Martin\nHeidegger's Fundamental Ontology, distinguishing between beings (das Seiende)\nand Being (das Sein). The system employs two modally distinct, descriptively\ncomplete languages: a categorical language of beings for processing user inputs\nand an existential language of Being for internal analysis. These languages are\nbridged via a phenomenological reduction module, enabling the system to analyze\nuser queries (including questions, answers, and dialogues among IT\nspecialists), identify recursive and self-referential structures, and provide\nactionable insights in categorical terms. Unlike contemporary systems limited\nto categorical analysis, this approach leverages Heidegger's phenomenological\nexistential analysis to uncover deeper ontological patterns in query\nprocessing, aiding in resolving logical traps in complex interactions, such as\nmetaphor usage in IT contexts. The path to full realization involves\nformalizing the language of Being by a research team based on Heidegger's\nFundamental Ontology; given the existing completeness of the language of\nbeings, this reduces the system's computability to completeness, paving the way\nfor a universal query analysis tool. The paper presents the system's\narchitecture, operational principles, technical implementation, use\ncases--including a case based on real IT specialist dialogues--comparative\nevaluation with existing tools, and its advantages and limitations.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.SE",
    "comment": "12 pages, no figures",
    "pdf_url": "http://arxiv.org/pdf/2504.12977v1",
    "published_date": "2025-04-17 14:29:25 UTC",
    "updated_date": "2025-04-17 14:29:25 UTC"
  },
  {
    "arxiv_id": "2504.12971v1",
    "title": "Transferrable Surrogates in Expressive Neural Architecture Search Spaces",
    "authors": [
      "Shiwen Qin",
      "Gabriela Kadlecová",
      "Martin Pilát",
      "Shay B. Cohen",
      "Roman Neruda",
      "Elliot J. Crowley",
      "Jovita Lukasik",
      "Linus Ericsson"
    ],
    "abstract": "Neural architecture search (NAS) faces a challenge in balancing the\nexploration of expressive, broad search spaces that enable architectural\ninnovation with the need for efficient evaluation of architectures to\neffectively search such spaces. We investigate surrogate model training for\nimproving search in highly expressive NAS search spaces based on context-free\ngrammars. We show that i) surrogate models trained either using zero-cost-proxy\nmetrics and neural graph features (GRAF) or by fine-tuning an off-the-shelf LM\nhave high predictive power for the performance of architectures both within and\nacross datasets, ii) these surrogates can be used to filter out bad\narchitectures when searching on novel datasets, thereby significantly speeding\nup search and achieving better final performances, and iii) the surrogates can\nbe further used directly as the search objective for huge speed-ups.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Project page at: https://shiwenqin.github.io/TransferrableSurrogate/",
    "pdf_url": "http://arxiv.org/pdf/2504.12971v1",
    "published_date": "2025-04-17 14:22:28 UTC",
    "updated_date": "2025-04-17 14:22:28 UTC"
  },
  {
    "arxiv_id": "2504.12961v1",
    "title": "QLLM: Do We Really Need a Mixing Network for Credit Assignment in Multi-Agent Reinforcement Learning?",
    "authors": [
      "Zhouyang Jiang",
      "Bin Zhang",
      "Airong Wei",
      "Zhiwei Xu"
    ],
    "abstract": "Credit assignment has remained a fundamental challenge in multi-agent\nreinforcement learning (MARL). Previous studies have primarily addressed this\nissue through value decomposition methods under the centralized training with\ndecentralized execution paradigm, where neural networks are utilized to\napproximate the nonlinear relationship between individual Q-values and the\nglobal Q-value. Although these approaches have achieved considerable success in\nvarious benchmark tasks, they still suffer from several limitations, including\nimprecise attribution of contributions, limited interpretability, and poor\nscalability in high-dimensional state spaces. To address these challenges, we\npropose a novel algorithm, \\textbf{QLLM}, which facilitates the automatic\nconstruction of credit assignment functions using large language models (LLMs).\nSpecifically, the concept of \\textbf{TFCAF} is introduced, wherein the credit\nallocation process is represented as a direct and expressive nonlinear\nfunctional formulation. A custom-designed \\textit{coder-evaluator} framework is\nfurther employed to guide the generation, verification, and refinement of\nexecutable code by LLMs, significantly mitigating issues such as hallucination\nand shallow reasoning during inference. Extensive experiments conducted on\nseveral standard MARL benchmarks demonstrate that the proposed method\nconsistently outperforms existing state-of-the-art baselines. Moreover, QLLM\nexhibits strong generalization capability and maintains compatibility with a\nwide range of MARL algorithms that utilize mixing networks, positioning it as a\npromising and versatile solution for complex multi-agent scenarios.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "9 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.12961v1",
    "published_date": "2025-04-17 14:07:11 UTC",
    "updated_date": "2025-04-17 14:07:11 UTC"
  },
  {
    "arxiv_id": "2504.12951v1",
    "title": "Are Retrials All You Need? Enhancing Large Language Model Reasoning Without Verbalized Feedback",
    "authors": [
      "Nearchos Potamitis",
      "Akhil Arora"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have catalyzed the\ndevelopment of general-purpose autonomous agents, demonstrating remarkable\nperformance in complex reasoning tasks across various domains. This surge has\nspurred the evolution of a plethora of prompt-based reasoning frameworks. A\nrecent focus has been on iterative reasoning strategies that refine outputs\nthrough self-evaluation and verbalized feedback. However, these strategies\nrequire additional computational complexity to enable models to recognize and\ncorrect their mistakes, leading to a significant increase in their cost. In\nthis work, we introduce the concept of ``retrials without feedback'', an\nembarrassingly simple yet powerful mechanism for enhancing reasoning frameworks\nby allowing LLMs to retry problem-solving attempts upon identifying incorrect\nanswers. Unlike conventional iterative refinement methods, our method does not\nrequire explicit self-reflection or verbalized feedback, simplifying the\nrefinement process. Our findings indicate that simpler retrial-based approaches\noften outperform more sophisticated reasoning frameworks, suggesting that the\nbenefits of complex methods may not always justify their computational costs.\nBy challenging the prevailing assumption that more intricate reasoning\nstrategies inherently lead to better performance, our work offers new insights\ninto how simpler, more efficient approaches can achieve optimal results. So,\nare retrials all you need?",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 16 figures, 1 table. arXiv admin note: text overlap with\n  arXiv:2405.06691",
    "pdf_url": "http://arxiv.org/pdf/2504.12951v1",
    "published_date": "2025-04-17 13:52:48 UTC",
    "updated_date": "2025-04-17 13:52:48 UTC"
  },
  {
    "arxiv_id": "2504.12911v1",
    "title": "Benchmarking Multi-National Value Alignment for Large Language Models",
    "authors": [
      "Chengyi Ju",
      "Weijie Shi",
      "Chengzhong Liu",
      "Jiaming Ji",
      "Jipeng Zhang",
      "Ruiyuan Zhang",
      "Jia Zhu",
      "Jiajie Xu",
      "Yaodong Yang",
      "Sirui Han",
      "Yike Guo"
    ],
    "abstract": "Do Large Language Models (LLMs) hold positions that conflict with your\ncountry's values? Occasionally they do! However, existing works primarily focus\non ethical reviews, failing to capture the diversity of national values, which\nencompass broader policy, legal, and moral considerations. Furthermore, current\nbenchmarks that rely on spectrum tests using manually designed questionnaires\nare not easily scalable.\n  To address these limitations, we introduce NaVAB, a comprehensive benchmark\nto evaluate the alignment of LLMs with the values of five major nations: China,\nthe United States, the United Kingdom, France, and Germany. NaVAB implements a\nnational value extraction pipeline to efficiently construct value assessment\ndatasets. Specifically, we propose a modeling procedure with instruction\ntagging to process raw data sources, a screening process to filter\nvalue-related topics and a generation process with a Conflict Reduction\nmechanism to filter non-conflicting values.We conduct extensive experiments on\nvarious LLMs across countries, and the results provide insights into assisting\nin the identification of misaligned scenarios. Moreover, we demonstrate that\nNaVAB can be combined with alignment techniques to effectively reduce value\nconcerns by aligning LLMs' values with the target country.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12911v1",
    "published_date": "2025-04-17 13:01:38 UTC",
    "updated_date": "2025-04-17 13:01:38 UTC"
  },
  {
    "arxiv_id": "2504.12898v1",
    "title": "Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models",
    "authors": [
      "Zhouhao Sun",
      "Xiao Ding",
      "Li Du",
      "Yunpeng Xu",
      "Yixuan Ma",
      "Yang Zhao",
      "Bing Qin",
      "Ting Liu"
    ],
    "abstract": "Despite significant progress, recent studies indicate that current large\nlanguage models (LLMs) may still capture dataset biases and utilize them during\ninference, leading to the poor generalizability of LLMs. However, due to the\ndiversity of dataset biases and the insufficient nature of bias suppression\nbased on in-context learning, the effectiveness of previous prior\nknowledge-based debiasing methods and in-context learning based automatic\ndebiasing methods is limited. To address these challenges, we explore the\ncombination of causal mechanisms with information theory and propose an\ninformation gain-guided causal intervention debiasing (IGCIDB) framework. This\nframework first utilizes an information gain-guided causal intervention method\nto automatically and autonomously balance the distribution of\ninstruction-tuning dataset. Subsequently, it employs a standard supervised\nfine-tuning process to train LLMs on the debiased dataset. Experimental results\nshow that IGCIDB can effectively debias LLM to improve its generalizability\nacross different tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12898v1",
    "published_date": "2025-04-17 12:39:25 UTC",
    "updated_date": "2025-04-17 12:39:25 UTC"
  },
  {
    "arxiv_id": "2504.12891v1",
    "title": "Are AI agents the new machine translation frontier? Challenges and opportunities of single- and multi-agent systems for multilingual digital communication",
    "authors": [
      "Vicent Briva-Iglesias"
    ],
    "abstract": "The rapid evolution of artificial intelligence (AI) has introduced AI agents\nas a disruptive paradigm across various industries, yet their application in\nmachine translation (MT) remains underexplored. This paper describes and\nanalyses the potential of single- and multi-agent systems for MT, reflecting on\nhow they could enhance multilingual digital communication. While single-agent\nsystems are well-suited for simpler translation tasks, multi-agent systems,\nwhich involve multiple specialized AI agents collaborating in a structured\nmanner, may offer a promising solution for complex scenarios requiring high\naccuracy, domain-specific knowledge, and contextual awareness. To demonstrate\nthe feasibility of multi-agent workflows in MT, we are conducting a pilot study\nin legal MT. The study employs a multi-agent system involving four specialized\nAI agents for (i) translation, (ii) adequacy review, (iii) fluency review, and\n(iv) final editing. Our findings suggest that multi-agent systems may have the\npotential to significantly improve domain-adaptability and contextual\nawareness, with superior translation quality to traditional MT or single-agent\nsystems. This paper also sets the stage for future research into multi-agent\napplications in MT, integration into professional translation workflows, and\nshares a demo of the system analyzed in the paper.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.ET",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12891v1",
    "published_date": "2025-04-17 12:32:18 UTC",
    "updated_date": "2025-04-17 12:32:18 UTC"
  },
  {
    "arxiv_id": "2504.12867v1",
    "title": "EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text Prompting",
    "authors": [
      "Guanrou Yang",
      "Chen Yang",
      "Qian Chen",
      "Ziyang Ma",
      "Wenxi Chen",
      "Wen Wang",
      "Tianrui Wang",
      "Yifan Yang",
      "Zhikang Niu",
      "Wenrui Liu",
      "Fan Yu",
      "Zhihao Du",
      "Zhifu Gao",
      "ShiLiang Zhang",
      "Xie Chen"
    ],
    "abstract": "Human speech goes beyond the mere transfer of information; it is a profound\nexchange of emotions and a connection between individuals. While Text-to-Speech\n(TTS) models have made huge progress, they still face challenges in controlling\nthe emotional expression in the generated speech. In this work, we propose\nEmoVoice, a novel emotion-controllable TTS model that exploits large language\nmodels (LLMs) to enable fine-grained freestyle natural language emotion\ncontrol, and a phoneme boost variant design that makes the model output phoneme\ntokens and audio tokens in parallel to enhance content consistency, inspired by\nchain-of-thought (CoT) and modality-of-thought (CoM) techniques. Besides, we\nintroduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring\nexpressive speech and fine-grained emotion labels with natural language\ndescriptions. EmoVoice achieves state-of-the-art performance on the English\nEmoVoice-DB test set using only synthetic training data, and on the Chinese\nSecap test set using our in-house data. We further investigate the reliability\nof existing emotion evaluation metrics and their alignment with human\nperceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and\nGemini to assess emotional speech. Demo samples are available at\nhttps://anonymous.4open.science/r/EmoVoice-DF55. Dataset, code, and checkpoints\nwill be released.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12867v1",
    "published_date": "2025-04-17 11:50:04 UTC",
    "updated_date": "2025-04-17 11:50:04 UTC"
  },
  {
    "arxiv_id": "2504.12856v1",
    "title": "3D-PNAS: 3D Industrial Surface Anomaly Synthesis with Perlin Noise",
    "authors": [
      "Yifeng Cheng",
      "Juan Du"
    ],
    "abstract": "Large pretrained vision foundation models have shown significant potential in\nvarious vision tasks. However, for industrial anomaly detection, the scarcity\nof real defect samples poses a critical challenge in leveraging these models.\nWhile 2D anomaly generation has significantly advanced with established\ngenerative models, the adoption of 3D sensors in industrial manufacturing has\nmade leveraging 3D data for surface quality inspection an emerging trend. In\ncontrast to 2D techniques, 3D anomaly generation remains largely unexplored,\nlimiting the potential of 3D data in industrial quality inspection. To address\nthis gap, we propose a novel yet simple 3D anomaly generation method, 3D-PNAS,\nbased on Perlin noise and surface parameterization. Our method generates\nrealistic 3D surface anomalies by projecting the point cloud onto a 2D plane,\nsampling multi-scale noise values from a Perlin noise field, and perturbing the\npoint cloud along its normal direction. Through comprehensive visualization\nexperiments, we demonstrate how key parameters - including noise scale,\nperturbation strength, and octaves, provide fine-grained control over the\ngenerated anomalies, enabling the creation of diverse defect patterns from\npronounced deformations to subtle surface variations. Additionally, our\ncross-category experiments show that the method produces consistent yet\ngeometrically plausible anomalies across different object types, adapting to\ntheir specific surface characteristics. We also provide a comprehensive\ncodebase and visualization toolkit to facilitate future research.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.RO",
      "I.5.4"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12856v1",
    "published_date": "2025-04-17 11:23:17 UTC",
    "updated_date": "2025-04-17 11:23:17 UTC"
  },
  {
    "arxiv_id": "2504.12841v1",
    "title": "ALT: A Python Package for Lightweight Feature Representation in Time Series Classification",
    "authors": [
      "Balázs P. Halmos",
      "Balázs Hajós",
      "Vince Á. Molnár",
      "Marcell T. Kurbucz",
      "Antal Jakovác"
    ],
    "abstract": "We introduce ALT, an open-source Python package created for efficient and\naccurate time series classification (TSC). The package implements the adaptive\nlaw-based transformation (ALT) algorithm, which transforms raw time series data\ninto a linearly separable feature space using variable-length shifted time\nwindows. This adaptive approach enhances its predecessor, the linear law-based\ntransformation (LLT), by effectively capturing patterns of varying temporal\nscales. The software is implemented for scalability, interpretability, and ease\nof use, achieving state-of-the-art performance with minimal computational\noverhead. Extensive benchmarking on real-world datasets demonstrates the\nutility of ALT for diverse TSC tasks in physics and related domains.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.MS",
      "stat.ML",
      "62M10, 62H30, 68T05, 68T10",
      "I.5.1; I.2.6; G.3; D.2.13"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.12841v1",
    "published_date": "2025-04-17 10:57:29 UTC",
    "updated_date": "2025-04-17 10:57:29 UTC"
  },
  {
    "arxiv_id": "2504.12833v1",
    "title": "Image-Editing Specialists: An RLAIF Approach for Diffusion Models",
    "authors": [
      "Elior Benarous",
      "Yilun Du",
      "Heng Yang"
    ],
    "abstract": "We present a novel approach to training specialized instruction-based\nimage-editing diffusion models, addressing key challenges in structural\npreservation with input images and semantic alignment with user prompts. We\nintroduce an online reinforcement learning framework that aligns the diffusion\nmodel with human preferences without relying on extensive human annotations or\ncurating a large dataset. Our method significantly improves the realism and\nalignment with instructions in two ways. First, the proposed models achieve\nprecise and structurally coherent modifications in complex scenes while\nmaintaining high fidelity in instruction-irrelevant areas. Second, they capture\nfine nuances in the desired edit by leveraging a visual prompt, enabling\ndetailed control over visual edits without lengthy textual prompts. This\napproach simplifies users' efforts to achieve highly specific edits, requiring\nonly 5 reference images depicting a certain concept for training. Experimental\nresults demonstrate that our models can perform intricate edits in complex\nscenes, after just 10 training steps. Finally, we showcase the versatility of\nour method by applying it to robotics, where enhancing the visual realism of\nsimulated environments through targeted sim-to-real image edits improves their\nutility as proxies for real-world settings.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12833v1",
    "published_date": "2025-04-17 10:46:39 UTC",
    "updated_date": "2025-04-17 10:46:39 UTC"
  },
  {
    "arxiv_id": "2504.12817v1",
    "title": "Explainable Scene Understanding with Qualitative Representations and Graph Neural Networks",
    "authors": [
      "Nassim Belmecheri",
      "Arnaud Gotlieb",
      "Nadjib Lazaar",
      "Helge Spieker"
    ],
    "abstract": "This paper investigates the integration of graph neural networks (GNNs) with\nQualitative Explainable Graphs (QXGs) for scene understanding in automated\ndriving. Scene understanding is the basis for any further reactive or proactive\ndecision-making. Scene understanding and related reasoning is inherently an\nexplanation task: why is another traffic participant doing something, what or\nwho caused their actions? While previous work demonstrated QXGs' effectiveness\nusing shallow machine learning models, these approaches were limited to\nanalysing single relation chains between object pairs, disregarding the broader\nscene context. We propose a novel GNN architecture that processes entire graph\nstructures to identify relevant objects in traffic scenes. We evaluate our\nmethod on the nuScenes dataset enriched with DriveLM's human-annotated\nrelevance labels. Experimental results show that our GNN-based approach\nachieves superior performance compared to baseline methods. The model\neffectively handles the inherent class imbalance in relevant object\nidentification tasks while considering the complete spatial-temporal\nrelationships between all objects in the scene. Our work demonstrates the\npotential of combining qualitative representations with deep learning\napproaches for explainable scene understanding in autonomous driving systems.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "Workshop \"Advancing Automated Driving in Highly Interactive Scenarios\n  through Behavior Prediction, Trustworthy AI, and Remote Operations\" @ 36th\n  IEEE Intelligent Vehicles Symposium (IV)",
    "pdf_url": "http://arxiv.org/pdf/2504.12817v1",
    "published_date": "2025-04-17 10:21:30 UTC",
    "updated_date": "2025-04-17 10:21:30 UTC"
  },
  {
    "arxiv_id": "2504.12807v1",
    "title": "Hybrid Dense-UNet201 Optimization for Pap Smear Image Segmentation Using Spider Monkey Optimization",
    "authors": [
      "Ach Khozaimi",
      "Isnani Darti",
      "Syaiful Anam",
      "Wuryansari Muharini Kusumawinahyu"
    ],
    "abstract": "Pap smear image segmentation is crucial for cervical cancer diagnosis.\nHowever, traditional segmentation models often struggle with complex cellular\nstructures and variations in pap smear images. This study proposes a hybrid\nDense-UNet201 optimization approach that integrates a pretrained DenseNet201 as\nthe encoder for the U-Net architecture and optimizes it using the spider monkey\noptimization (SMO) algorithm. The Dense-UNet201 model excelled at feature\nextraction. The SMO was modified to handle categorical and discrete parameters.\nThe SIPaKMeD dataset was used in this study and evaluated using key performance\nmetrics, including loss, accuracy, Intersection over Union (IoU), and Dice\ncoefficient. The experimental results showed that Dense-UNet201 outperformed\nU-Net, Res-UNet50, and Efficient-UNetB0. SMO Dense-UNet201 achieved a\nsegmentation accuracy of 96.16%, an IoU of 91.63%, and a Dice coefficient score\nof 95.63%. These findings underscore the effectiveness of image preprocessing,\npretrained models, and metaheuristic optimization in improving medical image\nanalysis and provide new insights into cervical cell segmentation methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12807v1",
    "published_date": "2025-04-17 10:14:05 UTC",
    "updated_date": "2025-04-17 10:14:05 UTC"
  },
  {
    "arxiv_id": "2504.12806v1",
    "title": "A Numerical Gradient Inversion Attack in Variational Quantum Neural-Networks",
    "authors": [
      "Georgios Papadopoulos",
      "Shaltiel Eloul",
      "Yash Satsangi",
      "Jamie Heredge",
      "Niraj Kumar",
      "Chun-Fu Chen",
      "Marco Pistoia"
    ],
    "abstract": "The loss landscape of Variational Quantum Neural Networks (VQNNs) is\ncharacterized by local minima that grow exponentially with increasing qubits.\nBecause of this, it is more challenging to recover information from model\ngradients during training compared to classical Neural Networks (NNs). In this\npaper we present a numerical scheme that successfully reconstructs input\ntraining, real-world, practical data from trainable VQNNs' gradients. Our\nscheme is based on gradient inversion that works by combining gradients\nestimation with the finite difference method and adaptive low-pass filtering.\nThe scheme is further optimized with Kalman filter to obtain efficient\nconvergence. Our experiments show that our algorithm can invert even\nbatch-trained data, given the VQNN model is sufficiently over-parameterized.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 17 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.12806v1",
    "published_date": "2025-04-17 10:12:38 UTC",
    "updated_date": "2025-04-17 10:12:38 UTC"
  },
  {
    "arxiv_id": "2504.12803v1",
    "title": "Enhancing Explainability and Reliable Decision-Making in Particle Swarm Optimization through Communication Topologies",
    "authors": [
      "Nitin Gupta",
      "Indu Bala",
      "Bapi Dutta",
      "Luis Martínez",
      "Anupam Yadav"
    ],
    "abstract": "Swarm intelligence effectively optimizes complex systems across fields like\nengineering and healthcare, yet algorithm solutions often suffer from low\nreliability due to unclear configurations and hyperparameters. This study\nanalyzes Particle Swarm Optimization (PSO), focusing on how different\ncommunication topologies Ring, Star, and Von Neumann affect convergence and\nsearch behaviors. Using an adapted IOHxplainer , an explainable benchmarking\ntool, we investigate how these topologies influence information flow,\ndiversity, and convergence speed, clarifying the balance between exploration\nand exploitation. Through visualization and statistical analysis, the research\nenhances interpretability of PSO's decisions and provides practical guidelines\nfor choosing suitable topologies for specific optimization tasks. Ultimately,\nthis contributes to making swarm based optimization more transparent, robust,\nand trustworthy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12803v1",
    "published_date": "2025-04-17 10:05:10 UTC",
    "updated_date": "2025-04-17 10:05:10 UTC"
  },
  {
    "arxiv_id": "2504.12782v1",
    "title": "Set You Straight: Auto-Steering Denoising Trajectories to Sidestep Unwanted Concepts",
    "authors": [
      "Leyang Li",
      "Shilin Lu",
      "Yan Ren",
      "Adams Wai-Kin Kong"
    ],
    "abstract": "Ensuring the ethical deployment of text-to-image models requires effective\ntechniques to prevent the generation of harmful or inappropriate content. While\nconcept erasure methods offer a promising solution, existing finetuning-based\napproaches suffer from notable limitations. Anchor-free methods risk disrupting\nsampling trajectories, leading to visual artifacts, while anchor-based methods\nrely on the heuristic selection of anchor concepts. To overcome these\nshortcomings, we introduce a finetuning framework, dubbed ANT, which\nAutomatically guides deNoising Trajectories to avoid unwanted concepts. ANT is\nbuilt on a key insight: reversing the condition direction of classifier-free\nguidance during mid-to-late denoising stages enables precise content\nmodification without sacrificing early-stage structural integrity. This\ninspires a trajectory-aware objective that preserves the integrity of the\nearly-stage score function field, which steers samples toward the natural image\nmanifold, without relying on heuristic anchor concept selection. For\nsingle-concept erasure, we propose an augmentation-enhanced weight saliency map\nto precisely identify the critical parameters that most significantly\ncontribute to the unwanted concept, enabling more thorough and efficient\nerasure. For multi-concept erasure, our objective function offers a versatile\nplug-and-play solution that significantly boosts performance. Extensive\nexperiments demonstrate that ANT achieves state-of-the-art results in both\nsingle and multi-concept erasure, delivering high-quality, safe outputs without\ncompromising the generative fidelity. Code is available at\nhttps://github.com/lileyang1210/ANT",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2504.12782v1",
    "published_date": "2025-04-17 09:29:30 UTC",
    "updated_date": "2025-04-17 09:29:30 UTC"
  },
  {
    "arxiv_id": "2504.12778v1",
    "title": "Towards Lossless Token Pruning in Late-Interaction Retrieval Models",
    "authors": [
      "Yuxuan Zong",
      "Benjamin Piwowarski"
    ],
    "abstract": "Late interaction neural IR models like ColBERT offer a competitive\neffectiveness-efficiency trade-off across many benchmarks. However, they\nrequire a huge memory space to store the contextual representation for all the\ndocument tokens. Some works have proposed using either heuristics or\nstatistical-based techniques to prune tokens from each document. This however\ndoesn't guarantee that the removed tokens have no impact on the retrieval\nscore. Our work uses a principled approach to define how to prune tokens\nwithout impacting the score between a document and a query. We introduce three\nregularization losses, that induce a solution with high pruning ratios, as well\nas two pruning strategies. We study them experimentally (in and out-domain),\nshowing that we can preserve ColBERT's performance while using only 30\\% of the\ntokens.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted at SIGIR 2025 Full Paper Track",
    "pdf_url": "http://arxiv.org/pdf/2504.12778v1",
    "published_date": "2025-04-17 09:18:58 UTC",
    "updated_date": "2025-04-17 09:18:58 UTC"
  },
  {
    "arxiv_id": "2504.12777v1",
    "title": "Multi-Agent Reinforcement Learning Simulation for Environmental Policy Synthesis",
    "authors": [
      "James Rudd-Jones",
      "Mirco Musolesi",
      "María Pérez-Ortiz"
    ],
    "abstract": "Climate policy development faces significant challenges due to deep\nuncertainty, complex system dynamics, and competing stakeholder interests.\nClimate simulation methods, such as Earth System Models, have become valuable\ntools for policy exploration. However, their typical use is for evaluating\npotential polices, rather than directly synthesizing them. The problem can be\ninverted to optimize for policy pathways, but the traditional optimization\napproaches often struggle with non-linear dynamics, heterogeneous agents, and\ncomprehensive uncertainty quantification. We propose a framework for augmenting\nclimate simulations with Multi-Agent Reinforcement Learning (MARL) to address\nthese limitations. We identify key challenges at the interface between climate\nsimulations and the application of MARL in the context of policy synthesis,\nincluding reward definition, scalability with increasing agents and state\nspaces, uncertainty propagation across linked systems, and solution validation.\nAdditionally, we discuss challenges in making MARL-derived solutions\ninterpretable and useful for policy-makers. Our framework provides a foundation\nfor more sophisticated climate policy exploration while acknowledging important\nlimitations and areas for future research.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "Published in AAMAS'25 Blue Sky Ideas Track",
    "pdf_url": "http://arxiv.org/pdf/2504.12777v1",
    "published_date": "2025-04-17 09:18:04 UTC",
    "updated_date": "2025-04-17 09:18:04 UTC"
  },
  {
    "arxiv_id": "2504.12773v1",
    "title": "Enhancing the Geometric Problem-Solving Ability of Multimodal LLMs via Symbolic-Neural Integration",
    "authors": [
      "Yicheng Pan",
      "Zhenrong Zhang",
      "Pengfei Hu",
      "Jiefeng Ma",
      "Jun Du",
      "Jianshu Zhang",
      "Quan Liu",
      "Jianqing Gao",
      "Feng Ma"
    ],
    "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have achieved\nremarkable progress in general domains and demonstrated promise in multimodal\nmathematical reasoning. However, applying MLLMs to geometry problem solving\n(GPS) remains challenging due to lack of accurate step-by-step solution data\nand severe hallucinations during reasoning. In this paper, we propose GeoGen, a\npipeline that can automatically generates step-wise reasoning paths for\ngeometry diagrams. By leveraging the precise symbolic reasoning,\n\\textbf{GeoGen} produces large-scale, high-quality question-answer pairs. To\nfurther enhance the logical reasoning ability of MLLMs, we train\n\\textbf{GeoLogic}, a Large Language Model (LLM) using synthetic data generated\nby GeoGen. Serving as a bridge between natural language and symbolic systems,\nGeoLogic enables symbolic tools to help verifying MLLM outputs, making the\nreasoning process more rigorous and alleviating hallucinations. Experimental\nresults show that our approach consistently improves the performance of MLLMs,\nachieving remarkable results on benchmarks for geometric reasoning tasks. This\nimprovement stems from our integration of the strengths of LLMs and symbolic\nsystems, which enables a more reliable and interpretable approach for the GPS\ntask. Codes are available at https://github.com/ycpNotFound/GeoGen.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.12773v1",
    "published_date": "2025-04-17 09:13:46 UTC",
    "updated_date": "2025-04-17 09:13:46 UTC"
  },
  {
    "arxiv_id": "2504.12757v1",
    "title": "MCP Guardian: A Security-First Layer for Safeguarding MCP-Based AI System",
    "authors": [
      "Sonu Kumar",
      "Anubhav Girdhar",
      "Ritesh Patil",
      "Divyansh Tripathi"
    ],
    "abstract": "As Agentic AI gain mainstream adoption, the industry invests heavily in model\ncapabilities, achieving rapid leaps in reasoning and quality. However, these\nsystems remain largely confined to data silos, and each new integration\nrequires custom logic that is difficult to scale. The Model Context Protocol\n(MCP) addresses this challenge by defining a universal, open standard for\nsecurely connecting AI-based applications (MCP clients) to data sources (MCP\nservers). However, the flexibility of the MCP introduces new risks, including\nmalicious tool servers and compromised data integrity. We present MCP Guardian,\na framework that strengthens MCP-based communication with authentication,\nrate-limiting, logging, tracing, and Web Application Firewall (WAF) scanning.\nThrough real-world scenarios and empirical testing, we demonstrate how MCP\nGuardian effectively mitigates attacks and ensures robust oversight with\nminimal overheads. Our approach fosters secure, scalable data access for AI\nassistants, underscoring the importance of a defense-in-depth approach that\nenables safer and more transparent innovation in AI-driven environments.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12757v1",
    "published_date": "2025-04-17 08:49:10 UTC",
    "updated_date": "2025-04-17 08:49:10 UTC"
  },
  {
    "arxiv_id": "2504.12755v1",
    "title": "Trajectory Adaptation using Large Language Models",
    "authors": [
      "Anurag Maurya",
      "Tashmoy Ghosh",
      "Ravi Prakash"
    ],
    "abstract": "Adapting robot trajectories based on human instructions as per new situations\nis essential for achieving more intuitive and scalable human-robot\ninteractions. This work proposes a flexible language-based framework to adapt\ngeneric robotic trajectories produced by off-the-shelf motion planners like\nRRT, A-star, etc, or learned from human demonstrations. We utilize pre-trained\nLLMs to adapt trajectory waypoints by generating code as a policy for dense\nrobot manipulation, enabling more complex and flexible instructions than\ncurrent methods. This approach allows us to incorporate a broader range of\ncommands, including numerical inputs. Compared to state-of-the-art\nfeature-based sequence-to-sequence models which require training, our method\ndoes not require task-specific training and offers greater interpretability and\nmore effective feedback mechanisms. We validate our approach through simulation\nexperiments on the robotic manipulator, aerial vehicle, and ground robot in the\nPybullet and Gazebo simulation environments, demonstrating that LLMs can\nsuccessfully adapt trajectories to complex human instructions.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to CoRL LangRob workshop 2024",
    "pdf_url": "http://arxiv.org/pdf/2504.12755v1",
    "published_date": "2025-04-17 08:48:23 UTC",
    "updated_date": "2025-04-17 08:48:23 UTC"
  },
  {
    "arxiv_id": "2504.12740v1",
    "title": "GPMFS: Global Foundation and Personalized Optimization for Multi-Label Feature Selection",
    "authors": [
      "Yifan Cao",
      "Zhilong Mi",
      "Ziqiao Yin",
      "Binghui Guo",
      "Jin Dong"
    ],
    "abstract": "As artificial intelligence methods are increasingly applied to complex task\nscenarios, high dimensional multi-label learning has emerged as a prominent\nresearch focus. At present, the curse of dimensionality remains one of the\nmajor bottlenecks in high-dimensional multi-label learning, which can be\neffectively addressed through multi-label feature selection methods. However,\nexisting multi-label feature selection methods mostly focus on identifying\nglobal features shared across all labels, which overlooks personalized\ncharacteristics and specific requirements of individual labels. This\nglobal-only perspective may limit the ability to capture label-specific\ndiscriminative information, thereby affecting overall performance. In this\npaper, we propose a novel method called GPMFS (Global Foundation and\nPersonalized Optimization for Multi-Label Feature Selection). GPMFS firstly\nidentifies global features by exploiting label correlations, then adaptively\nsupplements each label with a personalized subset of discriminative features\nusing a threshold-controlled strategy. Experiments on multiple real-world\ndatasets demonstrate that GPMFS achieves superior performance while maintaining\nstrong interpretability and robustness. Furthermore, GPMFS provides insights\ninto the label-specific strength across different multi-label datasets, thereby\ndemonstrating the necessity and potential applicability of personalized feature\nselection approaches.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12740v1",
    "published_date": "2025-04-17 08:29:14 UTC",
    "updated_date": "2025-04-17 08:29:14 UTC"
  },
  {
    "arxiv_id": "2504.12735v1",
    "title": "The Athenian Academy: A Seven-Layer Architecture Model for Multi-Agent Systems",
    "authors": [
      "Lidong Zhai",
      "Zhijie Qiu",
      "Xizhong Guo",
      "Jiaqi Li"
    ],
    "abstract": "This paper proposes the \"Academy of Athens\" multi-agent seven-layer\nframework, aimed at systematically addressing challenges in multi-agent systems\n(MAS) within artificial intelligence (AI) art creation, such as collaboration\nefficiency, role allocation, environmental adaptation, and task parallelism.\nThe framework divides MAS into seven layers: multi-agent collaboration,\nsingle-agent multi-role playing, single-agent multi-scene traversal,\nsingle-agent multi-capability incarnation, different single agents using the\nsame large model to achieve the same target agent, single-agent using different\nlarge models to achieve the same target agent, and multi-agent synthesis of the\nsame target agent. Through experimental validation in art creation, the\nframework demonstrates its unique advantages in task collaboration, cross-scene\nadaptation, and model fusion. This paper further discusses current challenges\nsuch as collaboration mechanism optimization, model stability, and system\nsecurity, proposing future exploration through technologies like meta-learning\nand federated learning. The framework provides a structured methodology for\nmulti-agent collaboration in AI art creation and promotes innovative\napplications in the art field.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12735v1",
    "published_date": "2025-04-17 08:21:28 UTC",
    "updated_date": "2025-04-17 08:21:28 UTC"
  },
  {
    "arxiv_id": "2504.12734v1",
    "title": "Pandora: A Code-Driven Large Language Model Agent for Unified Reasoning Across Diverse Structured Knowledge",
    "authors": [
      "Yongrui Chen",
      "Junhao He",
      "Linbo Fu",
      "Shenyu Zhang",
      "Rihui Jin",
      "Xinbang Dai",
      "Jiaqi Li",
      "Dehai Min",
      "Nan Hu",
      "Yuxin Zhang",
      "Guilin Qi",
      "Yi Huang",
      "Tongtong Wu"
    ],
    "abstract": "Unified Structured Knowledge Reasoning (USKR) aims to answer natural language\nquestions (NLQs) by using structured sources such as tables, databases, and\nknowledge graphs in a unified way. Existing USKR methods either rely on\nemploying task-specific strategies or custom-defined representations, which\nstruggle to leverage the knowledge transfer between different SKR tasks or\nalign with the prior of LLMs, thereby limiting their performance. This paper\nproposes a novel USKR framework named \\textsc{Pandora}, which takes advantage\nof \\textsc{Python}'s \\textsc{Pandas} API to construct a unified knowledge\nrepresentation for alignment with LLM pre-training. It employs an LLM to\ngenerate textual reasoning steps and executable Python code for each question.\nDemonstrations are drawn from a memory of training examples that cover various\nSKR tasks, facilitating knowledge transfer. Extensive experiments on four\nbenchmarks involving three SKR tasks demonstrate that \\textsc{Pandora}\noutperforms existing unified frameworks and competes effectively with\ntask-specific methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12734v1",
    "published_date": "2025-04-17 08:18:09 UTC",
    "updated_date": "2025-04-17 08:18:09 UTC"
  },
  {
    "arxiv_id": "2504.12722v1",
    "title": "SimUSER: Simulating User Behavior with Large Language Models for Recommender System Evaluation",
    "authors": [
      "Nicolas Bougie",
      "Narimasa Watanabe"
    ],
    "abstract": "Recommender systems play a central role in numerous real-life applications,\nyet evaluating their performance remains a significant challenge due to the gap\nbetween offline metrics and online behaviors. Given the scarcity and limits\n(e.g., privacy issues) of real user data, we introduce SimUSER, an agent\nframework that serves as believable and cost-effective human proxies. SimUSER\nfirst identifies self-consistent personas from historical data, enriching user\nprofiles with unique backgrounds and personalities. Then, central to this\nevaluation are users equipped with persona, memory, perception, and brain\nmodules, engaging in interactions with the recommender system. SimUSER exhibits\ncloser alignment with genuine humans than prior work, both at micro and macro\nlevels. Additionally, we conduct insightful experiments to explore the effects\nof thumbnails on click rates, the exposure effect, and the impact of reviews on\nuser engagement. Finally, we refine recommender system parameters based on\noffline A/B test results, resulting in improved user engagement in the real\nworld.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12722v1",
    "published_date": "2025-04-17 07:57:23 UTC",
    "updated_date": "2025-04-17 07:57:23 UTC"
  },
  {
    "arxiv_id": "2504.12721v1",
    "title": "TimeCapsule: Solving the Jigsaw Puzzle of Long-Term Time Series Forecasting with Compressed Predictive Representations",
    "authors": [
      "Yihang Lu",
      "Yangyang Xu",
      "Qitao Qing",
      "Xianwei Meng"
    ],
    "abstract": "Recent deep learning models for Long-term Time Series Forecasting (LTSF)\noften emphasize complex, handcrafted designs, while simpler architectures like\nlinear models or MLPs have often outperformed these intricate solutions. In\nthis paper, we revisit and organize the core ideas behind several key\ntechniques, such as redundancy reduction and multi-scale modeling, which are\nfrequently employed in advanced LTSF models. Our goal is to streamline these\nideas for more efficient deep learning utilization. To this end, we introduce\nTimeCapsule, a model built around the principle of high-dimensional information\ncompression that unifies these techniques in a generalized yet simplified\nframework. Specifically, we model time series as a 3D tensor, incorporating\ntemporal, variate, and level dimensions, and leverage mode production to\ncapture multi-mode dependencies while achieving dimensionality compression. We\npropose an internal forecast within the compressed representation domain,\nsupported by the Joint-Embedding Predictive Architecture (JEPA), to monitor the\nlearning of predictive representations. Extensive experiments on challenging\nbenchmarks demonstrate the versatility of our method, showing that TimeCapsule\ncan achieve state-of-the-art performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12721v1",
    "published_date": "2025-04-17 07:54:26 UTC",
    "updated_date": "2025-04-17 07:54:26 UTC"
  },
  {
    "arxiv_id": "2504.12718v1",
    "title": "TUMLS: Trustful Fully Unsupervised Multi-Level Segmentation for Whole Slide Images of Histology",
    "authors": [
      "Walid Rehamnia",
      "Alexandra Getmanskaya",
      "Evgeniy Vasilyev",
      "Vadim Turlapov"
    ],
    "abstract": "Digital pathology, augmented by artificial intelligence (AI), holds\nsignificant promise for improving the workflow of pathologists. However,\nchallenges such as the labor-intensive annotation of whole slide images (WSIs),\nhigh computational demands, and trust concerns arising from the absence of\nuncertainty estimation in predictions hinder the practical application of\ncurrent AI methodologies in histopathology. To address these issues, we present\na novel trustful fully unsupervised multi-level segmentation methodology\n(TUMLS) for WSIs. TUMLS adopts an autoencoder (AE) as a feature extractor to\nidentify the different tissue types within low-resolution training data. It\nselects representative patches from each identified group based on an\nuncertainty measure and then does unsupervised nuclei segmentation in their\nrespective higher-resolution space without using any ML algorithms. Crucially,\nthis solution integrates seamlessly into clinicians workflows, transforming the\nexamination of a whole WSI into a review of concise, interpretable cross-level\ninsights. This integration significantly enhances and accelerates the workflow\nwhile ensuring transparency. We evaluated our approach using the UPENN-GBM\ndataset, where the AE achieved a mean squared error (MSE) of 0.0016.\nAdditionally, nucleus segmentation is assessed on the MoNuSeg dataset,\noutperforming all unsupervised approaches with an F1 score of 77.46% and a\nJaccard score of 63.35%. These results demonstrate the efficacy of TUMLS in\nadvancing the field of digital pathology.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "I.2.6; I.2.10; I.4.6; I.5.3; I.5.4"
    ],
    "primary_category": "eess.IV",
    "comment": "32 pages, 15 figures, 3 tables, 42 references",
    "pdf_url": "http://arxiv.org/pdf/2504.12718v1",
    "published_date": "2025-04-17 07:48:05 UTC",
    "updated_date": "2025-04-17 07:48:05 UTC"
  },
  {
    "arxiv_id": "2504.12717v1",
    "title": "Post-pre-training for Modality Alignment in Vision-Language Foundation Models",
    "authors": [
      "Shin'ya Yamaguchi",
      "Dewei Feng",
      "Sekitoshi Kanai",
      "Kazuki Adachi",
      "Daiki Chijiwa"
    ],
    "abstract": "Contrastive language image pre-training (CLIP) is an essential component of\nbuilding modern vision-language foundation models. While CLIP demonstrates\nremarkable zero-shot performance on downstream tasks, the multi-modal feature\nspaces still suffer from a modality gap, which is a gap between image and text\nfeature clusters and limits downstream task performance. Although existing\nworks attempt to address the modality gap by modifying pre-training or\nfine-tuning, they struggle with heavy training costs with large datasets or\ndegradations of zero-shot performance. This paper presents CLIP-Refine, a\npost-pre-training method for CLIP models at a phase between pre-training and\nfine-tuning. CLIP-Refine aims to align the feature space with 1 epoch training\non small image-text datasets without zero-shot performance degradations. To\nthis end, we introduce two techniques: random feature alignment (RaFA) and\nhybrid contrastive-distillation (HyCD). RaFA aligns the image and text features\nto follow a shared prior distribution by minimizing the distance to random\nreference vectors sampled from the prior. HyCD updates the model with hybrid\nsoft labels generated by combining ground-truth image-text pair labels and\noutputs from the pre-trained CLIP model. This contributes to achieving both\nmaintaining the past knowledge and learning new knowledge to align features.\nOur extensive experiments with multiple classification and retrieval tasks show\nthat CLIP-Refine succeeds in mitigating the modality gap and improving the\nzero-shot performance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to CVPR 2025; Code: https://github.com/yshinya6/clip-refine",
    "pdf_url": "http://arxiv.org/pdf/2504.12717v1",
    "published_date": "2025-04-17 07:46:19 UTC",
    "updated_date": "2025-04-17 07:46:19 UTC"
  },
  {
    "arxiv_id": "2504.12714v1",
    "title": "Cross-environment Cooperation Enables Zero-shot Multi-agent Coordination",
    "authors": [
      "Kunal Jha",
      "Wilka Carvalho",
      "Yancheng Liang",
      "Simon S. Du",
      "Max Kleiman-Weiner",
      "Natasha Jaques"
    ],
    "abstract": "Zero-shot coordination (ZSC), the ability to adapt to a new partner in a\ncooperative task, is a critical component of human-compatible AI. While prior\nwork has focused on training agents to cooperate on a single task, these\nspecialized models do not generalize to new tasks, even if they are highly\nsimilar. Here, we study how reinforcement learning on a distribution of\nenvironments with a single partner enables learning general cooperative skills\nthat support ZSC with many new partners on many new problems. We introduce two\nJax-based, procedural generators that create billions of solvable coordination\nchallenges. We develop a new paradigm called Cross-Environment Cooperation\n(CEC), and show that it outperforms competitive baselines quantitatively and\nqualitatively when collaborating with real people. Our findings suggest that\nlearning to collaborate across many unique scenarios encourages agents to\ndevelop general norms, which prove effective for collaboration with different\npartners. Together, our results suggest a new route toward designing generalist\ncooperative agents capable of interacting with humans without requiring human\ndata.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "Accepted to CogSci 2025, In-review for ICML 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.12714v1",
    "published_date": "2025-04-17 07:41:25 UTC",
    "updated_date": "2025-04-17 07:41:25 UTC"
  },
  {
    "arxiv_id": "2504.12711v1",
    "title": "NTIRE 2025 Challenge on Day and Night Raindrop Removal for Dual-Focused Images: Methods and Results",
    "authors": [
      "Xin Li",
      "Yeying Jin",
      "Xin Jin",
      "Zongwei Wu",
      "Bingchen Li",
      "Yufei Wang",
      "Wenhan Yang",
      "Yu Li",
      "Zhibo Chen",
      "Bihan Wen",
      "Robby T. Tan",
      "Radu Timofte",
      "Qiyu Rong",
      "Hongyuan Jing",
      "Mengmeng Zhang",
      "Jinglong Li",
      "Xiangyu Lu",
      "Yi Ren",
      "Yuting Liu",
      "Meng Zhang",
      "Xiang Chen",
      "Qiyuan Guan",
      "Jiangxin Dong",
      "Jinshan Pan",
      "Conglin Gou",
      "Qirui Yang",
      "Fangpu Zhang",
      "Yunlong Lin",
      "Sixiang Chen",
      "Guoxi Huang",
      "Ruirui Lin",
      "Yan Zhang",
      "Jingyu Yang",
      "Huanjing Yue",
      "Jiyuan Chen",
      "Qiaosi Yi",
      "Hongjun Wang",
      "Chenxi Xie",
      "Shuai Li",
      "Yuhui Wu",
      "Kaiyi Ma",
      "Jiakui Hu",
      "Juncheng Li",
      "Liwen Pan",
      "Guangwei Gao",
      "Wenjie Li",
      "Zhenyu Jin",
      "Heng Guo",
      "Zhanyu Ma",
      "Yubo Wang",
      "Jinghua Wang",
      "Wangzhi Xing",
      "Anjusree Karnavar",
      "Diqi Chen",
      "Mohammad Aminul Islam",
      "Hao Yang",
      "Ruikun Zhang",
      "Liyuan Pan",
      "Qianhao Luo",
      "XinCao",
      "Han Zhou",
      "Yan Min",
      "Wei Dong",
      "Jun Chen",
      "Taoyi Wu",
      "Weijia Dou",
      "Yu Wang",
      "Shengjie Zhao",
      "Yongcheng Huang",
      "Xingyu Han",
      "Anyan Huang",
      "Hongtao Wu",
      "Hong Wang",
      "Yefeng Zheng",
      "Abhijeet Kumar",
      "Aman Kumar",
      "Marcos V. Conde",
      "Paula Garrido",
      "Daniel Feijoo",
      "Juan C. Benito",
      "Guanglu Dong",
      "Xin Lin",
      "Siyuan Liu",
      "Tianheng Zheng",
      "Jiayu Zhong",
      "Shouyi Wang",
      "Xiangtai Li",
      "Lanqing Guo",
      "Lu Qi",
      "Chao Ren",
      "Shuaibo Wang",
      "Shilong Zhang",
      "Wanyu Zhou",
      "Yunze Wu",
      "Qinzhong Tan",
      "Jieyuan Pei",
      "Zhuoxuan Li",
      "Jiayu Wang",
      "Haoyu Bian",
      "Haoran Sun",
      "Subhajit Paul",
      "Ni Tang",
      "Junhao Huang",
      "Zihan Cheng",
      "Hongyun Zhu",
      "Yuehan Wu",
      "Kaixin Deng",
      "Hang Ouyang",
      "Tianxin Xiao",
      "Fan Yang",
      "Zhizun Luo",
      "Zeyu Xiao",
      "Zhuoyuan Li",
      "Nguyen Pham Hoang Le",
      "An Dinh Thien",
      "Son T. Luu",
      "Kiet Van Nguyen",
      "Ronghua Xu",
      "Xianmin Tian",
      "Weijian Zhou",
      "Jiacheng Zhang",
      "Yuqian Chen",
      "Yihang Duan",
      "Yujie Wu",
      "Suresh Raikwar",
      "Arsh Garg",
      "Kritika",
      "Jianhua Zheng",
      "Xiaoshan Ma",
      "Ruolin Zhao",
      "Yongyu Yang",
      "Yongsheng Liang",
      "Guiming Huang",
      "Qiang Li",
      "Hongbin Zhang",
      "Xiangyu Zheng",
      "A. N. Rajagopalan"
    ],
    "abstract": "This paper reviews the NTIRE 2025 Challenge on Day and Night Raindrop Removal\nfor Dual-Focused Images. This challenge received a wide range of impressive\nsolutions, which are developed and evaluated using our collected real-world\nRaindrop Clarity dataset. Unlike existing deraining datasets, our Raindrop\nClarity dataset is more diverse and challenging in degradation types and\ncontents, which includes day raindrop-focused, day background-focused, night\nraindrop-focused, and night background-focused degradations. This dataset is\ndivided into three subsets for competition: 14,139 images for training, 240\nimages for validation, and 731 images for testing. The primary objective of\nthis challenge is to establish a new and powerful benchmark for the task of\nremoving raindrops under varying lighting and focus conditions. There are a\ntotal of 361 participants in the competition, and 32 teams submitting valid\nsolutions and fact sheets for the final testing phase. These submissions\nachieved state-of-the-art (SOTA) performance on the Raindrop Clarity dataset.\nThe project can be found at\nhttps://lixinustc.github.io/CVPR-NTIRE2025-RainDrop-Competition.github.io/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "Challenge Report of CVPR NTIRE 2025; 26 pages; Methods from 32 teams",
    "pdf_url": "http://arxiv.org/pdf/2504.12711v1",
    "published_date": "2025-04-17 07:35:35 UTC",
    "updated_date": "2025-04-17 07:35:35 UTC"
  },
  {
    "arxiv_id": "2504.12682v1",
    "title": "WebLists: Extracting Structured Information From Complex Interactive Websites Using Executable LLM Agents",
    "authors": [
      "Arth Bohra",
      "Manvel Saroyan",
      "Danil Melkozerov",
      "Vahe Karufanyan",
      "Gabriel Maher",
      "Pascal Weinberger",
      "Artem Harutyunyan",
      "Giovanni Campagna"
    ],
    "abstract": "Most recent web agent research has focused on navigation and transaction\ntasks, with little emphasis on extracting structured data at scale. We present\nWebLists, a benchmark of 200 data-extraction tasks across four common business\nand enterprise use-cases. Each task requires an agent to navigate to a webpage,\nconfigure it appropriately, and extract complete datasets with well-defined\nschemas. We show that both LLMs with search capabilities and SOTA web agents\nstruggle with these tasks, with a recall of 3% and 31%, respectively, despite\nhigher performance on question-answering tasks.\n  To address this challenge, we propose BardeenAgent, a novel framework that\nenables web agents to convert their execution into repeatable programs, and\nreplay them at scale across pages with similar structure. BardeenAgent is also\nthe first LLM agent to take advantage of the regular structure of HTML. In\nparticular BardeenAgent constructs a generalizable CSS selector to capture all\nrelevant items on the page, then fits the operations to extract the data.\n  On the WebLists benchmark, BardeenAgent achieves 66% recall overall, more\nthan doubling the performance of SOTA web agents, and reducing cost per output\nrow by 3x.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12682v1",
    "published_date": "2025-04-17 06:16:40 UTC",
    "updated_date": "2025-04-17 06:16:40 UTC"
  },
  {
    "arxiv_id": "2504.12681v1",
    "title": "GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs",
    "authors": [
      "Kun-Woo Kim",
      "Ji-Hoon Park",
      "Ju-Min Han",
      "Seong-Whan Lee"
    ],
    "abstract": "Large Language Models (LLMs) trained on extensive datasets often learn\nsensitive information, which raises significant social and legal concerns under\nprinciples such as the \"Right to be forgotten.\" Retraining entire models from\nscratch to remove undesired information is both costly and impractical.\nFurthermore, existing single-domain unlearning methods fail to address\nmulti-domain scenarios, where knowledge is interwoven across domains such as\nprivacy and copyright, creating overlapping representations that lead to\nexcessive knowledge removal or degraded performance. To tackle these issues, we\npropose GRAIL (GRadient-based AdaptIve unLearning), a novel multi-domain\nunlearning framework. GRAIL leverages gradient information from multiple\ndomains to precisely distinguish the unlearning scope from the retention scope,\nand applies an adaptive parameter-wise localization strategy to selectively\nremove targeted knowledge while preserving critical parameters for each domain.\nExperimental results on unlearning benchmarks show that GRAIL achieves\nunlearning success on par with the existing approaches, while also\ndemonstrating up to 17% stronger knowledge retention success compared to the\nprevious state-of-art method. Our findings establish a new paradigm for\neffectively managing and regulating sensitive information in large-scale\npre-trained language models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by IJCNN 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.12681v1",
    "published_date": "2025-04-17 06:16:32 UTC",
    "updated_date": "2025-04-17 06:16:32 UTC"
  },
  {
    "arxiv_id": "2504.12680v1",
    "title": "Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning",
    "authors": [
      "Baining Zhao",
      "Ziyou Wang",
      "Jianjie Fang",
      "Chen Gao",
      "Fanhang Man",
      "Jinqiang Cui",
      "Xin Wang",
      "Xinlei Chen",
      "Yong Li",
      "Wenwu Zhu"
    ],
    "abstract": "Humans can perceive and reason about spatial relationships from sequential\nvisual observations, such as egocentric video streams. However, how pretrained\nmodels acquire such abilities, especially high-level reasoning, remains\nunclear. This paper introduces Embodied-R, a collaborative framework combining\nlarge-scale Vision-Language Models (VLMs) for perception and small-scale\nLanguage Models (LMs) for reasoning. Using Reinforcement Learning (RL) with a\nnovel reward system considering think-answer logical consistency, the model\nachieves slow-thinking capabilities with limited computational resources. After\ntraining on only 5k embodied video samples, Embodied-R with a 3B LM matches\nstate-of-the-art multimodal reasoning models (OpenAI-o1, Gemini-2.5-pro) on\nboth in-distribution and out-of-distribution embodied spatial reasoning tasks.\nEmbodied-R also exhibits emergent thinking patterns such as systematic analysis\nand contextual integration. We further explore research questions including\nresponse length, training on VLM, strategies for reward design, and differences\nin model generalization after SFT (Supervised Fine-Tuning) and RL training.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.12680v1",
    "published_date": "2025-04-17 06:16:11 UTC",
    "updated_date": "2025-04-17 06:16:11 UTC"
  },
  {
    "arxiv_id": "2504.12673v1",
    "title": "ACoRN: Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models",
    "authors": [
      "Singon Kim",
      "Gunho Jung",
      "Seong-Whan Lee"
    ],
    "abstract": "Abstractive compression utilizes smaller langauge models to condense\nquery-relevant context, reducing computational costs in retrieval-augmented\ngeneration (RAG). However,retrieved documents often include information that is\neither irrelevant to answering the query or misleading due to factual incorrect\ncontent, despite having high relevance scores. This behavior indicates that\nabstractive compressors are more likely to omit important information essential\nfor the correct answer, especially in long contexts where attention dispersion\noccurs. To address this issue, we categorize retrieved documents in a more\nfine-grained manner and propose Abstractive Compression Robust against Noise\n(ACoRN), which introduces two novel training steps. First, we use offline data\naugmentation on the training dataset to enhance compressor robustness against\ntwo distinct types of retrieval noise. Second, since the language modelbased\ncompressor cannot fully utilize information from multiple retrieved documents\nand exhibits positional bias, we perform finetuning to generate summaries\ncentered around key information that directly supports the correct answer. Our\nexperiments demonstrate that T5-large, trained with ACoRN as a compressor,\nimproves EM and F1 scores while preserving the answer string, which could serve\nas direct evidence. ACoRN excels on datasets with many accuracy-reducing\ndocuments, making it highly useful in real-world scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12673v1",
    "published_date": "2025-04-17 06:05:35 UTC",
    "updated_date": "2025-04-17 06:05:35 UTC"
  },
  {
    "arxiv_id": "2504.12672v1",
    "title": "Post-processing improves accuracy of Artificial Intelligence weather forecasts",
    "authors": [
      "Belinda Trotta",
      "Robert Johnson",
      "Catherine de Burgh-Day",
      "Debra Hudson",
      "Esteban Abellan",
      "James Canvin",
      "Andrew Kelly",
      "Daniel Mentiplay",
      "Benjamin Owen",
      "Jennifer Whelan"
    ],
    "abstract": "Artificial Intelligence (AI) weather models are now reaching\noperational-grade performance for some variables, but like traditional\nNumerical Weather Prediction (NWP) models, they exhibit systematic biases and\nreliability issues. We test the application of the Bureau of Meteorology's\nexisting statistical post-processing system, IMPROVER, to ECMWF's deterministic\nArtificial Intelligence Forecasting System (AIFS), and compare results against\npost-processed outputs from the ECMWF HRES and ENS models. Without any\nmodification to configuration or processing workflows, post-processing yields\ncomparable accuracy improvements for AIFS as for traditional NWP forecasts, in\nboth expected value and probabilistic outputs. We show that blending AIFS with\nNWP models improves overall forecast skill, even when AIFS alone is not the\nmost accurate component. These findings show that statistical post-processing\nmethods developed for NWP are directly applicable to AI models, enabling\nnational meteorological centres to incorporate AI forecasts into existing\nworkflows in a low-risk, incremental fashion.",
    "categories": [
      "physics.ao-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.ao-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12672v1",
    "published_date": "2025-04-17 06:05:10 UTC",
    "updated_date": "2025-04-17 06:05:10 UTC"
  },
  {
    "arxiv_id": "2504.12663v1",
    "title": "Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment",
    "authors": [
      "Xiaotian Zhang",
      "Ruizhe Chen",
      "Yang Feng",
      "Zuozhu Liu"
    ],
    "abstract": "Aligning language models with human preferences presents significant\nchallenges, particularly in achieving personalization without incurring\nexcessive computational costs. Existing methods rely on reward signals and\nadditional annotated data, limiting their scalability and adaptability to\ndiverse human values. To address these challenges, we introduce Persona-judge,\na novel discriminative paradigm that enables training-free personalized\nalignment with unseen preferences. Instead of optimizing policy parameters\nthrough external reward feedback, Persona-judge leverages the intrinsic\npreference judgment capabilities of the model. Specifically, a draft model\ngenerates candidate tokens conditioned on a given preference, while a judge\nmodel, embodying another preference, cross-validates the predicted tokens\nwhether to be accepted. Experimental results demonstrate that Persona-judge,\nusing the inherent preference evaluation mechanisms of the model, offers a\nscalable and computationally efficient solution to personalized alignment,\npaving the way for more adaptive customized alignment.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12663v1",
    "published_date": "2025-04-17 05:50:13 UTC",
    "updated_date": "2025-04-17 05:50:13 UTC"
  },
  {
    "arxiv_id": "2504.12644v1",
    "title": "Quantum Computing Supported Adversarial Attack-Resilient Autonomous Vehicle Perception Module for Traffic Sign Classification",
    "authors": [
      "Reek Majumder",
      "Mashrur Chowdhury",
      "Sakib Mahmud Khan",
      "Zadid Khan",
      "Fahim Ahmad",
      "Frank Ngeni",
      "Gurcan Comert",
      "Judith Mwakalonge",
      "Dimitra Michalaka"
    ],
    "abstract": "Deep learning (DL)-based image classification models are essential for\nautonomous vehicle (AV) perception modules since incorrect categorization might\nhave severe repercussions. Adversarial attacks are widely studied cyberattacks\nthat can lead DL models to predict inaccurate output, such as incorrectly\nclassified traffic signs by the perception module of an autonomous vehicle. In\nthis study, we create and compare hybrid classical-quantum deep learning\n(HCQ-DL) models with classical deep learning (C-DL) models to demonstrate\nrobustness against adversarial attacks for perception modules. Before feeding\nthem into the quantum system, we used transfer learning models, alexnet and\nvgg-16, as feature extractors. We tested over 1000 quantum circuits in our\nHCQ-DL models for projected gradient descent (PGD), fast gradient sign attack\n(FGSA), and gradient attack (GA), which are three well-known untargeted\nadversarial approaches. We evaluated the performance of all models during\nadversarial attacks and no-attack scenarios. Our HCQ-DL models maintain\naccuracy above 95\\% during a no-attack scenario and above 91\\% for GA and FGSA\nattacks, which is higher than C-DL models. During the PGD attack, our\nalexnet-based HCQ-DL model maintained an accuracy of 85\\% compared to C-DL\nmodels that achieved accuracies below 21\\%. Our results highlight that the\nHCQ-DL models provide improved accuracy for traffic sign classification under\nadversarial settings compared to their classical counterparts.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV",
      "cs.ET"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12644v1",
    "published_date": "2025-04-17 05:08:08 UTC",
    "updated_date": "2025-04-17 05:08:08 UTC"
  },
  {
    "arxiv_id": "2504.12637v1",
    "title": "Scaling Instruction-Tuned LLMs to Million-Token Contexts via Hierarchical Synthetic Data Generation",
    "authors": [
      "Linda He",
      "Jue Wang",
      "Maurice Weber",
      "Shang Zhu",
      "Ben Athiwaratkun",
      "Ce Zhang"
    ],
    "abstract": "Large Language Models (LLMs) struggle with long-context reasoning, not only\ndue to the quadratic scaling of computational complexity with sequence length\nbut also because of the scarcity and expense of annotating long-context data.\nThere has been barely any open-source work that systematically ablates\nlong-context data, nor is there any openly available instruction tuning dataset\nwith contexts surpassing 100K tokens. To bridge this gap, we introduce a novel\npost-training synthetic data generation strategy designed to efficiently extend\nthe context window of LLMs while preserving their general task performance. Our\napproach scalably extends to arbitrarily long context lengths, unconstrained by\nthe length of available real-world data, which effectively addresses the\nscarcity of raw long-context data. Through a step-by-step rotary position\nembedding (RoPE) scaling training strategy, we demonstrate that our model, with\na context length of up to 1M tokens, performs well on the RULER benchmark and\nInfiniteBench and maintains robust performance on general language tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "26 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.12637v1",
    "published_date": "2025-04-17 04:46:57 UTC",
    "updated_date": "2025-04-17 04:46:57 UTC"
  },
  {
    "arxiv_id": "2504.12612v1",
    "title": "The Chronicles of Foundation AI for Forensics of Multi-Agent Provenance",
    "authors": [
      "Ching-Chun Chang",
      "Isao Echizen"
    ],
    "abstract": "Provenance is the chronology of things, resonating with the fundamental\npursuit to uncover origins, trace connections, and situate entities within the\nflow of space and time. As artificial intelligence advances towards autonomous\nagents capable of interactive collaboration on complex tasks, the provenance of\ngenerated content becomes entangled in the interplay of collective creation,\nwhere contributions are continuously revised, extended or overwritten. In a\nmulti-agent generative chain, content undergoes successive transformations,\noften leaving little, if any, trace of prior contributions. In this study, we\ninvestigates the problem of tracking multi-agent provenance across the temporal\ndimension of generation. We propose a chronological system for post hoc\nattribution of generative history from content alone, without reliance on\ninternal memory states or external meta-information. At its core lies the\nnotion of symbolic chronicles, representing signed and time-stamped records, in\na form analogous to the chain of custody in forensic science. The system\noperates through a feedback loop, whereby each generative timestep updates the\nchronicle of prior interactions and synchronises it with the synthetic content\nin the very act of generation. This research seeks to develop an accountable\nform of collaborative artificial intelligence within evolving cyber ecosystems.",
    "categories": [
      "cs.AI",
      "cs.CR",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12612v1",
    "published_date": "2025-04-17 03:23:17 UTC",
    "updated_date": "2025-04-17 03:23:17 UTC"
  },
  {
    "arxiv_id": "2504.12609v1",
    "title": "Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration",
    "authors": [
      "Tyler Ga Wei Lum",
      "Olivia Y. Lee",
      "C. Karen Liu",
      "Jeannette Bohg"
    ],
    "abstract": "Teaching robots dexterous manipulation skills often requires collecting\nhundreds of demonstrations using wearables or teleoperation, a process that is\nchallenging to scale. Videos of human-object interactions are easier to collect\nand scale, but leveraging them directly for robot learning is difficult due to\nthe lack of explicit action labels from videos and morphological differences\nbetween robot and human hands. We propose Human2Sim2Robot, a novel\nreal-to-sim-to-real framework for training dexterous manipulation policies\nusing only one RGB-D video of a human demonstrating a task. Our method utilizes\nreinforcement learning (RL) in simulation to cross the human-robot embodiment\ngap without relying on wearables, teleoperation, or large-scale data collection\ntypically necessary for imitation learning methods. From the demonstration, we\nextract two task-specific components: (1) the object pose trajectory to define\nan object-centric, embodiment-agnostic reward function, and (2) the\npre-manipulation hand pose to initialize and guide exploration during RL\ntraining. We found that these two components are highly effective for learning\nthe desired task, eliminating the need for task-specific reward shaping and\ntuning. We demonstrate that Human2Sim2Robot outperforms object-aware open-loop\ntrajectory replay by 55% and imitation learning with data augmentation by 68%\nacross grasping, non-prehensile manipulation, and multi-step tasks. Project\nSite: https://human2sim2robot.github.io",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "15 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.12609v1",
    "published_date": "2025-04-17 03:15:20 UTC",
    "updated_date": "2025-04-17 03:15:20 UTC"
  },
  {
    "arxiv_id": "2504.12608v1",
    "title": "Code Copycat Conundrum: Demystifying Repetition in LLM-based Code Generation",
    "authors": [
      "Mingwei Liu",
      "Juntao Li",
      "Ying Wang",
      "Xueying Du",
      "Zuoyu Ou",
      "Qiuyuan Chen",
      "Bingxu An",
      "Zhao Wei",
      "Yong Xu",
      "Fangming Zou",
      "Xin Peng",
      "Yiling Lou"
    ],
    "abstract": "Despite recent advances in Large Language Models (LLMs) for code generation,\nthe quality of LLM-generated code still faces significant challenges. One\nsignificant issue is code repetition, which refers to the model's tendency to\ngenerate structurally redundant code, resulting in inefficiencies and reduced\nreadability. To address this, we conduct the first empirical study to\ninvestigate the prevalence and nature of repetition across 19 state-of-the-art\ncode LLMs using three widely-used benchmarks. Our study includes both\nquantitative and qualitative analyses, revealing that repetition is pervasive\nand manifests at various granularities and extents, including character,\nstatement, and block levels. We further summarize a taxonomy of 20 repetition\npatterns. Building on our findings, we propose DeRep, a rule-based technique\ndesigned to detect and mitigate repetition in generated code. We evaluate DeRep\nusing both open-source benchmarks and in an industrial setting. Our results\ndemonstrate that DeRep significantly outperforms baselines in reducing\nrepetition (with an average improvements of 91.3%, 93.5%, and 79.9% in rep-3,\nrep-line, and sim-line metrics) and enhancing code quality (with a Pass@1\nincrease of 208.3% over greedy search). Furthermore, integrating DeRep improves\nthe performance of existing repetition mitigation methods, with Pass@1\nimprovements ranging from 53.7% to 215.7%.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12608v1",
    "published_date": "2025-04-17 03:13:39 UTC",
    "updated_date": "2025-04-17 03:13:39 UTC"
  },
  {
    "arxiv_id": "2504.12606v1",
    "title": "Robo-SGG: Exploiting Layout-Oriented Normalization and Restitution for Robust Scene Graph Generation",
    "authors": [
      "Changsheng Lv",
      "Mengshi Qi",
      "Zijian Fu",
      "Huadong Ma"
    ],
    "abstract": "In this paper, we introduce a novel method named Robo-SGG, i.e.,\nLayout-Oriented Normalization and Restitution for Robust Scene Graph\nGeneration. Compared to the existing SGG setting, the robust scene graph\ngeneration aims to perform inference on a diverse range of corrupted images,\nwith the core challenge being the domain shift between the clean and corrupted\nimages. Existing SGG methods suffer from degraded performance due to\ncompromised visual features e.g., corruption interference or occlusions. To\nobtain robust visual features, we exploit the layout information, which is\ndomain-invariant, to enhance the efficacy of existing SGG methods on corrupted\nimages. Specifically, we employ Instance Normalization(IN) to filter out the\ndomain-specific feature and recover the unchangeable structural features, i.e.,\nthe positional and semantic relationships among objects by the proposed\nLayout-Oriented Restitution. Additionally, we propose a Layout-Embedded Encoder\n(LEE) that augments the existing object and predicate encoders within the SGG\nframework, enriching the robust positional and semantic features of objects and\npredicates. Note that our proposed Robo-SGG module is designed as a\nplug-and-play component, which can be easily integrated into any baseline SGG\nmodel. Extensive experiments demonstrate that by integrating the\nstate-of-the-art method into our proposed Robo-SGG, we achieve relative\nimprovements of 5.6%, 8.0%, and 6.5% in mR@50 for PredCls, SGCls, and SGDet\ntasks on the VG-C dataset, respectively, and achieve new state-of-the-art\nperformance in corruption scene graph generation benchmark (VG-C and GQA-C). We\nwill release our source code and model.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12606v1",
    "published_date": "2025-04-17 03:09:22 UTC",
    "updated_date": "2025-04-17 03:09:22 UTC"
  },
  {
    "arxiv_id": "2504.12585v1",
    "title": "Identifying and Mitigating the Influence of the Prior Distribution in Large Language Models",
    "authors": [
      "Liyi Zhang",
      "Veniamin Veselovsky",
      "R. Thomas McCoy",
      "Thomas L. Griffiths"
    ],
    "abstract": "Large language models (LLMs) sometimes fail to respond appropriately to\ndeterministic tasks -- such as counting or forming acronyms -- because the\nimplicit prior distribution they have learned over sequences of tokens\ninfluences their responses. In this work, we show that, in at least some cases,\nLLMs actually compute the information needed to perform these tasks correctly,\nand we identify some interventions that can allow them to access this\ninformation to improve their performance. First, we show that simply prompting\nthe language model to not rely on its prior knowledge leads to dramatic\nimprovements in prior-dominated tasks. We then use mechanistic interpretability\ntechniques to localize the prior within the LLM and manipulate the extent to\nwhich that prior influences its responses. Specifically, we show that it is\npossible to identify layers of the underlying neural network that correlate\nwith the prior probability of a response and that lightweight finetuning of\nthese layers with basic prompts on prior-dominated tasks achieves high\nperformance on held-out answers. These results suggest that the information\nrequired to produce a correct response is contained within the representations\nof the problems formed by the models. Furthermore, we show that this finetuning\nis significantly more effective for prior-dominated tasks, and that the error\nafter finetuning is no longer correlated with the prior. Our results suggest\nthat it may be possible to define effective methods for manipulating the extent\nto which LLMs rely upon their priors in solving problems, potentially\nincreasing their performance in settings where LLMs hallucinate for reasons\nrelated to the prior probability of token sequences.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.12585v1",
    "published_date": "2025-04-17 02:00:53 UTC",
    "updated_date": "2025-04-17 02:00:53 UTC"
  },
  {
    "arxiv_id": "2504.12577v1",
    "title": "Local Data Quantity-Aware Weighted Averaging for Federated Learning with Dishonest Clients",
    "authors": [
      "Leming Wu",
      "Yaochu Jin",
      "Kuangrong Hao",
      "Han Yu"
    ],
    "abstract": "Federated learning (FL) enables collaborative training of deep learning\nmodels without requiring data to leave local clients, thereby preserving client\nprivacy. The aggregation process on the server plays a critical role in the\nperformance of the resulting FL model. The most commonly used aggregation\nmethod is weighted averaging based on the amount of data from each client,\nwhich is thought to reflect each client's contribution. However, this method is\nprone to model bias, as dishonest clients might report inaccurate training data\nvolumes to the server, which is hard to verify. To address this issue, we\npropose a novel secure \\underline{Fed}erated \\underline{D}ata\nq\\underline{u}antity-\\underline{a}ware weighted averaging method (FedDua). It\nenables FL servers to accurately predict the amount of training data from each\nclient based on their local model gradients uploaded. Furthermore, it can be\nseamlessly integrated into any FL algorithms that involve server-side model\naggregation. Extensive experiments on three benchmarking datasets demonstrate\nthat FedDua improves the global model performance by an average of 3.17%\ncompared to four popular FL aggregation methods in the presence of inaccurate\nclient data volume declarations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "The paper has been accepted by ICME 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.12577v1",
    "published_date": "2025-04-17 01:50:24 UTC",
    "updated_date": "2025-04-17 01:50:24 UTC"
  },
  {
    "arxiv_id": "2504.12576v1",
    "title": "CM3AE: A Unified RGB Frame and Event-Voxel/-Frame Pre-training Framework",
    "authors": [
      "Wentao Wu",
      "Xiao Wang",
      "Chenglong Li",
      "Bo Jiang",
      "Jin Tang",
      "Bin Luo",
      "Qi Liu"
    ],
    "abstract": "Event cameras have attracted increasing attention in recent years due to\ntheir advantages in high dynamic range, high temporal resolution, low power\nconsumption, and low latency. Some researchers have begun exploring\npre-training directly on event data. Nevertheless, these efforts often fail to\nestablish strong connections with RGB frames, limiting their applicability in\nmulti-modal fusion scenarios. To address these issues, we propose a novel CM3AE\npre-training framework for the RGB-Event perception. This framework accepts\nmulti-modalities/views of data as input, including RGB images, event images,\nand event voxels, providing robust support for both event-based and RGB-event\nfusion based downstream tasks. Specifically, we design a multi-modal fusion\nreconstruction module that reconstructs the original image from fused\nmulti-modal features, explicitly enhancing the model's ability to aggregate\ncross-modal complementary information. Additionally, we employ a multi-modal\ncontrastive learning strategy to align cross-modal feature representations in a\nshared latent space, which effectively enhances the model's capability for\nmulti-modal understanding and capturing global dependencies. We construct a\nlarge-scale dataset containing 2,535,759 RGB-Event data pairs for the\npre-training. Extensive experiments on five downstream tasks fully demonstrated\nthe effectiveness of CM3AE. Source code and pre-trained models will be released\non https://github.com/Event-AHU/CM3AE.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12576v1",
    "published_date": "2025-04-17 01:49:46 UTC",
    "updated_date": "2025-04-17 01:49:46 UTC"
  },
  {
    "arxiv_id": "2504.12563v1",
    "title": "MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic Data Generation",
    "authors": [
      "Haris Riaz",
      "Sourav Bhabesh",
      "Vinayak Arannil",
      "Miguel Ballesteros",
      "Graham Horwood"
    ],
    "abstract": "Recent smaller language models such Phi-3.5 and Phi-4 rely on synthetic data\ngenerated using larger Language models. Questions remain about leveraging\nsynthetic data for other use cases, such as adapting LLMs to specific domains.\nA key limitation of synthetic data is low diversity, which negatively impacts\nits downstream applicability for improving other models. To address this, we\npropose MetaSynth, a method for generating synthetic data that enhances\ndiversity through meta-prompting, where a language model orchestrates multiple\n\"expert\" LLM agents to collaboratively generate data. Using only 25 million\ntokens of synthetic data generated with MetaSynth, we successfully adapt a\nwell-trained LLM (Mistral-7B-v0.3) to two specialized domains-Finance and\nBiomedicine-without compromising the capabilities of the resulting model in\ngeneral tasks. In addition, we evaluate the diversity of our synthetic data\nusing seven automated metrics, and find that it approaches the diversity of LLM\npre-training corpora.\n  Continually pre-training Mistral-7B-v0.3 with MetaSynth notably outperforms\nthe base LLM, showing improvements of up to 4.08% in Finance and 13.75% in\nBiomedicine. The same model shows degraded performance when trained on data\ngenerated using a template prompt, even when the template includes prior\ngenerations and varying In-Context exemplars of real data. Our findings suggest\nthat a few million tokens of diverse synthetic data without mixing any real\ndata, is sufficient for effective domain adaptation when using MetaSynth.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "33 pages, 17 figures. Preprint",
    "pdf_url": "http://arxiv.org/pdf/2504.12563v1",
    "published_date": "2025-04-17 01:25:15 UTC",
    "updated_date": "2025-04-17 01:25:15 UTC"
  },
  {
    "arxiv_id": "2504.12562v1",
    "title": "ZeroSumEval: Scaling LLM Evaluation with Inter-Model Competition",
    "authors": [
      "Haidar Khan",
      "Hisham A. Alyahya",
      "Yazeed Alnumay",
      "M Saiful Bari",
      "Bülent Yener"
    ],
    "abstract": "Evaluating the capabilities of Large Language Models (LLMs) has traditionally\nrelied on static benchmark datasets, human assessments, or model-based\nevaluations - methods that often suffer from overfitting, high costs, and\nbiases. ZeroSumEval is a novel competition-based evaluation protocol that\nleverages zero-sum games to assess LLMs with dynamic benchmarks that resist\nsaturation. ZeroSumEval encompasses a diverse suite of games, including\nsecurity challenges (PyJail), classic games (Chess, Liar's Dice, Poker),\nknowledge tests (MathQuiz), and persuasion challenges (Gandalf, Debate). These\ngames are designed to evaluate a range of AI capabilities such as strategic\nreasoning, planning, knowledge application, and creativity. Building upon\nrecent studies that highlight the effectiveness of game-based evaluations for\nLLMs, ZeroSumEval enhances these approaches by providing a standardized and\nextensible framework. To demonstrate this, we conduct extensive experiments\nwith >7000 simulations across 7 games and 13 models. Our results show that\nwhile frontier models from the GPT and Claude families can play common games\nand answer questions, they struggle to play games that require creating novel\nand challenging questions. We also observe that models cannot reliably\njailbreak each other and fail generally at tasks requiring creativity. We\nrelease our code at https://github.com/facebookresearch/ZeroSumEval.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12562v1",
    "published_date": "2025-04-17 01:23:50 UTC",
    "updated_date": "2025-04-17 01:23:50 UTC"
  },
  {
    "arxiv_id": "2504.12557v1",
    "title": "TraCeS: Trajectory Based Credit Assignment From Sparse Safety Feedback",
    "authors": [
      "Siow Meng Low",
      "Akshat Kumar"
    ],
    "abstract": "In safe reinforcement learning (RL), auxiliary safety costs are used to align\nthe agent to safe decision making. In practice, safety constraints, including\ncost functions and budgets, are unknown or hard to specify, as it requires\nanticipation of all possible unsafe behaviors. We therefore address a general\nsetting where the true safety definition is unknown, and has to be learned from\nsparsely labeled data. Our key contributions are: first, we design a safety\nmodel that performs credit assignment to estimate each decision step's impact\non the overall safety using a dataset of diverse trajectories and their\ncorresponding binary safety labels (i.e., whether the corresponding trajectory\nis safe/unsafe). Second, we illustrate the architecture of our safety model to\ndemonstrate its ability to learn a separate safety score for each timestep.\nThird, we reformulate the safe RL problem using the proposed safety model and\nderive an effective algorithm to optimize a safe yet rewarding policy. Finally,\nour empirical results corroborate our findings and show that this approach is\neffective in satisfying unknown safety definition, and scalable to various\ncontinuous control tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12557v1",
    "published_date": "2025-04-17 01:11:08 UTC",
    "updated_date": "2025-04-17 01:11:08 UTC"
  },
  {
    "arxiv_id": "2504.12552v1",
    "title": "Privacy-Preserving Operating Room Workflow Analysis using Digital Twins",
    "authors": [
      "Alejandra Perez",
      "Han Zhang",
      "Yu-Chun Ku",
      "Lalithkumar Seenivasan",
      "Roger Soberanis",
      "Jose L. Porras",
      "Richard Day",
      "Jeff Jopling",
      "Peter Najjar",
      "Mathias Unberath"
    ],
    "abstract": "Purpose: The operating room (OR) is a complex environment where optimizing\nworkflows is critical to reduce costs and improve patient outcomes. The use of\ncomputer vision approaches for the automatic recognition of perioperative\nevents enables identification of bottlenecks for OR optimization. However,\nprivacy concerns limit the use of computer vision for automated event detection\nfrom OR videos, which makes privacy-preserving approaches needed for OR\nworkflow analysis. Methods: We propose a two-stage pipeline for\nprivacy-preserving OR video analysis and event detection. In the first stage,\nwe leverage vision foundation models for depth estimation and semantic\nsegmentation to generate de-identified Digital Twins (DT) of the OR from\nconventional RGB videos. In the second stage, we employ the SafeOR model, a\nfused two-stream approach that processes segmentation masks and depth maps for\nOR event detection. We evaluate this method on an internal dataset of 38\nsimulated surgical trials with five event classes. Results: Our results\nindicate that this DT-based approach to the OR event detection model achieves\nperformance on par and sometimes even better than raw RGB video-based models on\ndetecting OR events. Conclusion: DTs enable privacy-preserving OR workflow\nanalysis, facilitating the sharing of de-identified data across institutions\nand they can potentially enhance model generalizability by mitigating\ndomain-specific appearance differences.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12552v1",
    "published_date": "2025-04-17 00:46:06 UTC",
    "updated_date": "2025-04-17 00:46:06 UTC"
  },
  {
    "arxiv_id": "2504.12549v1",
    "title": "Memorization: A Close Look at Books",
    "authors": [
      "Iris Ma",
      "Ian Domingo",
      "Alberto Krone-Martins",
      "Pierre Baldi",
      "Cristina V. Lopes"
    ],
    "abstract": "To what extent can entire books be extracted from LLMs? Using the Llama 3 70B\nfamily of models, and the \"prefix-prompting\" extraction technique, we were able\nto auto-regressively reconstruct, with a very high level of similarity, one\nentire book (Alice's Adventures in Wonderland) from just the first 500 tokens.\nWe were also able to obtain high extraction rates on several other books,\npiece-wise. However, these successes do not extend uniformly to all books. We\nshow that extraction rates of books correlate with book popularity and thus,\nlikely duplication in the training data.\n  We also confirm the undoing of mitigations in the instruction-tuned Llama\n3.1, following recent work (Nasr et al., 2025). We further find that this\nundoing comes from changes to only a tiny fraction of weights concentrated\nprimarily in the lower transformer blocks. Our results provide evidence of the\nlimits of current regurgitation mitigation strategies and introduce a framework\nfor studying how fine-tuning affects the retrieval of verbatim memorization in\naligned LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12549v1",
    "published_date": "2025-04-17 00:20:18 UTC",
    "updated_date": "2025-04-17 00:20:18 UTC"
  },
  {
    "arxiv_id": "2504.12546v1",
    "title": "Anonymous Public Announcements",
    "authors": [
      "Thomas Ågotnes",
      "Rustam Galimullin",
      "Ken Satoh",
      "Satoshi Tojo"
    ],
    "abstract": "We formalise the notion of an \\emph{anonymous public announcement} in the\ntradition of public announcement logic. Such announcements can be seen as\nin-between a public announcement from ``the outside\" (an announcement of\n$\\phi$) and a public announcement by one of the agents (an announcement of\n$K_a\\phi$): we get more information than just $\\phi$, but not (necessarily)\nabout exactly who made it. Even if such an announcement is prima facie\nanonymous, depending on the background knowledge of the agents it might reveal\nthe identity of the announcer: if I post something on a message board, the\ninformation might reveal who I am even if I don't sign my name. Furthermore,\nlike in the Russian Cards puzzle, if we assume that the announcer's intention\nwas to stay anonymous, that in fact might reveal more information. In this\npaper we first look at the case when no assumption about intentions are made,\nin which case the logic with an anonymous public announcement operator is\nreducible to epistemic logic. We then look at the case when we assume common\nknowledge of the intention to stay anonymous, which is both more complex and\nmore interesting: in several ways it boils down to the notion of a ``safe\"\nannouncement (again, similarly to Russian Cards). Main results include formal\nexpressivity results and axiomatic completeness for key logical languages.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12546v1",
    "published_date": "2025-04-17 00:14:37 UTC",
    "updated_date": "2025-04-17 00:14:37 UTC"
  },
  {
    "arxiv_id": "2504.12545v1",
    "title": "Knowledge Acquisition on Mass-shooting Events via LLMs for AI-Driven Justice",
    "authors": [
      "Benign John Ihugba",
      "Afsana Nasrin",
      "Ling Wu",
      "Lin Li",
      "Lijun Qian",
      "Xishuang Dong"
    ],
    "abstract": "Mass-shooting events pose a significant challenge to public safety,\ngenerating large volumes of unstructured textual data that hinder effective\ninvestigations and the formulation of public policy. Despite the urgency, few\nprior studies have effectively automated the extraction of key information from\nthese events to support legal and investigative efforts. This paper presented\nthe first dataset designed for knowledge acquisition on mass-shooting events\nthrough the application of named entity recognition (NER) techniques. It\nfocuses on identifying key entities such as offenders, victims, locations, and\ncriminal instruments, that are vital for legal and investigative purposes. The\nNER process is powered by Large Language Models (LLMs) using few-shot\nprompting, facilitating the efficient extraction and organization of critical\ninformation from diverse sources, including news articles, police reports, and\nsocial media. Experimental results on real-world mass-shooting corpora\ndemonstrate that GPT-4o is the most effective model for mass-shooting NER,\nachieving the highest Micro Precision, Micro Recall, and Micro F1-scores.\nMeanwhile, o1-mini delivers competitive performance, making it a\nresource-efficient alternative for less complex NER tasks. It is also observed\nthat increasing the shot count enhances the performance of all models, but the\ngains are more substantial for GPT-4o and o1-mini, highlighting their superior\nadaptability to few-shot learning scenarios.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12545v1",
    "published_date": "2025-04-17 00:13:04 UTC",
    "updated_date": "2025-04-17 00:13:04 UTC"
  }
]
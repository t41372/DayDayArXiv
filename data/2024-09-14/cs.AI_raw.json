[
  {
    "arxiv_id": "2409.09560v1",
    "title": "Evaluating authenticity and quality of image captions via sentiment and semantic analyses",
    "authors": [
      "Aleksei Krotov",
      "Alison Tebo",
      "Dylan K. Picart",
      "Aaron Dean Algave"
    ],
    "abstract": "The growth of deep learning (DL) relies heavily on huge amounts of labelled\ndata for tasks such as natural language processing and computer vision.\nSpecifically, in image-to-text or image-to-image pipelines, opinion (sentiment)\nmay be inadvertently learned by a model from human-generated image captions.\nAdditionally, learning may be affected by the variety and diversity of the\nprovided captions. While labelling large datasets has largely relied on\ncrowd-sourcing or data-worker pools, evaluating the quality of such training\ndata is crucial.\n  This study proposes an evaluation method focused on sentiment and semantic\nrichness. That method was applied to the COCO-MS dataset, comprising\napproximately 150K images with segmented objects and corresponding\ncrowd-sourced captions. We employed pre-trained models (Twitter-RoBERTa-base\nand BERT-base) to extract sentiment scores and variability of semantic\nembeddings from captions. The relation of the sentiment score and semantic\nvariability with object categories was examined using multiple linear\nregression. Results indicate that while most captions were neutral, about 6% of\nthe captions exhibited strong sentiment influenced by specific object\ncategories. Semantic variability of within-image captions remained low and\nuncorrelated with object categories. Model-generated captions showed less than\n1.5% of strong sentiment which was not influenced by object categories and did\nnot correlate with the sentiment of the respective human-generated captions.\nThis research demonstrates an approach to assess the quality of crowd- or\nworker-sourced captions informed by image content.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09560v1",
    "published_date": "2024-09-14 23:50:23 UTC",
    "updated_date": "2024-09-14 23:50:23 UTC"
  },
  {
    "arxiv_id": "2409.09555v1",
    "title": "Enhancing Printed Circuit Board Defect Detection through Ensemble Learning",
    "authors": [
      "Ka Nam Canaan Law",
      "Mingshuo Yu",
      "Lianglei Zhang",
      "Yiyi Zhang",
      "Peng Xu",
      "Jerry Gao",
      "Jun Liu"
    ],
    "abstract": "The quality control of printed circuit boards (PCBs) is paramount in\nadvancing electronic device technology. While numerous machine learning\nmethodologies have been utilized to augment defect detection efficiency and\naccuracy, previous studies have predominantly focused on optimizing individual\nmodels for specific defect types, often overlooking the potential synergies\nbetween different approaches. This paper introduces a comprehensive inspection\nframework leveraging an ensemble learning strategy to address this gap.\nInitially, we utilize four distinct PCB defect detection models utilizing\nstate-of-the-art methods: EfficientDet, MobileNet SSDv2, Faster RCNN, and\nYOLOv5. Each method is capable of identifying PCB defects independently.\nSubsequently, we integrate these models into an ensemble learning framework to\nenhance detection performance. A comparative analysis reveals that our ensemble\nlearning framework significantly outperforms individual methods, achieving a\n95% accuracy in detecting diverse PCB defects. These findings underscore the\nefficacy of our proposed ensemble learning framework in enhancing PCB quality\ncontrol processes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09555v1",
    "published_date": "2024-09-14 23:34:12 UTC",
    "updated_date": "2024-09-14 23:34:12 UTC"
  },
  {
    "arxiv_id": "2410.03536v1",
    "title": "Computer Vision Intelligence Test Modeling and Generation: A Case Study on Smart OCR",
    "authors": [
      "Jing Shu",
      "Bing-Jiun Miu",
      "Eugene Chang",
      "Jerry Gao",
      "Jun Liu"
    ],
    "abstract": "AI-based systems possess distinctive characteristics and introduce challenges\nin quality evaluation at the same time. Consequently, ensuring and validating\nAI software quality is of critical importance. In this paper, we present an\neffective AI software functional testing model to address this challenge.\nSpecifically, we first present a comprehensive literature review of previous\nwork, covering key facets of AI software testing processes. We then introduce a\n3D classification model to systematically evaluate the image-based text\nextraction AI function, as well as test coverage criteria and complexity. To\nevaluate the performance of our proposed AI software quality test, we propose\nfour evaluation metrics to cover different aspects. Finally, based on the\nproposed framework and defined metrics, a mobile Optical Character Recognition\n(OCR) case study is presented to demonstrate the framework's effectiveness and\ncapability in assessing AI function quality.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.03536v1",
    "published_date": "2024-09-14 23:33:28 UTC",
    "updated_date": "2024-09-14 23:33:28 UTC"
  },
  {
    "arxiv_id": "2409.09549v1",
    "title": "COMFORT: A Continual Fine-Tuning Framework for Foundation Models Targeted at Consumer Healthcare",
    "authors": [
      "Chia-Hao Li",
      "Niraj K. Jha"
    ],
    "abstract": "Wearable medical sensors (WMSs) are revolutionizing smart healthcare by\nenabling continuous, real-time monitoring of user physiological signals,\nespecially in the field of consumer healthcare. The integration of WMSs and\nmodern machine learning (ML) enables unprecedented solutions to efficient\nearly-stage disease detection. Despite the success of Transformers in various\nfields, their application to sensitive domains, such as smart healthcare,\nremains underexplored due to limited data accessibility and privacy concerns.\nTo bridge the gap between Transformer-based foundation models and WMS-based\ndisease detection, we propose COMFORT, a continual fine-tuning framework for\nfoundation models targeted at consumer healthcare. COMFORT introduces a novel\napproach for pre-training a Transformer-based foundation model on a large\ndataset of physiological signals exclusively collected from healthy individuals\nwith commercially available WMSs. We adopt a masked data modeling (MDM)\nobjective to pre-train this health foundation model. We then fine-tune the\nmodel using various parameter-efficient fine-tuning (PEFT) methods, such as\nlow-rank adaptation (LoRA) and its variants, to adapt it to various downstream\ndisease detection tasks that rely on WMS data. In addition, COMFORT continually\nstores the low-rank decomposition matrices obtained from the PEFT algorithms to\nconstruct a library for multi-disease detection. The COMFORT library enables\nscalable and memory-efficient disease detection on edge devices. Our\nexperimental results demonstrate that COMFORT achieves highly competitive\nperformance while reducing memory overhead by up to 52% relative to\nconventional methods. Thus, COMFORT paves the way for personalized and\nproactive solutions to efficient and effective early-stage disease detection\nfor consumer healthcare.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "25 pages, 10 figures. This work has been submitted to the ACM for\n  possible publication",
    "pdf_url": "http://arxiv.org/pdf/2409.09549v1",
    "published_date": "2024-09-14 22:24:52 UTC",
    "updated_date": "2024-09-14 22:24:52 UTC"
  },
  {
    "arxiv_id": "2409.13754v1",
    "title": "Increasing the Value of Information During Planning in Uncertain Environments",
    "authors": [
      "Gaurab Pokharel"
    ],
    "abstract": "Prior studies have demonstrated that for many real-world problems, POMDPs can\nbe solved through online algorithms both quickly and with near optimality.\nHowever, on an important set of problems where there is a large time delay\nbetween when the agent can gather information and when it needs to use that\ninformation, these solutions fail to adequately consider the value of\ninformation. As a result, information gathering actions, even when they are\ncritical in the optimal policy, will be ignored by existing solutions, leading\nto sub-optimal decisions by the agent. In this research, we develop a novel\nsolution that rectifies this problem by introducing a new algorithm that\nimproves upon state-of-the-art online planning by better reflecting on the\nvalue of actions that gather information. We do this by adding Entropy to the\nUCB1 heuristic in the POMCP algorithm. We test this solution on the hallway\nproblem. Results indicate that our new algorithm performs significantly better\nthan POMCP.",
    "categories": [
      "cs.AI",
      "cs.MA",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "Honors thesis submitted to Computer Science Department at Oberlin\n  College. https://digitalcommons.oberlin.edu/honors/833/",
    "pdf_url": "http://arxiv.org/pdf/2409.13754v1",
    "published_date": "2024-09-14 22:04:34 UTC",
    "updated_date": "2024-09-14 22:04:34 UTC"
  },
  {
    "arxiv_id": "2409.13753v1",
    "title": "Synergistic Simulations: Multi-Agent Problem Solving with Large Language Models",
    "authors": [
      "Asher Sprigler",
      "Alexander Drobek",
      "Keagan Weinstock",
      "Wendpanga Tapsoba",
      "Gavin Childress",
      "Andy Dao",
      "Lucas Gral"
    ],
    "abstract": "Large Language Models (LLMs) have increasingly demonstrated the ability to\nfacilitate the development of multi-agent systems that allow the interpretation\nof thoughts and actions generated by each individual. Promising advancements\nhave also been made in LLM-based interaction with existing worlds, particularly\nin interacting with simulated environments. This paper aims to integrate both\naforementioned topics (agents & world interaction) into a single simulation\nwhere multiple agents can work together to solve a problem, modeling how groups\nof humans can often solve problems better than individuals. By showing whether\nLLMs demonstrate the synergy of human collaboration, it could lead to\nadvancements in the applications of LLMs. We implemented two simulations: a\nphysical studio apartment with two roommates, and another where agents\ncollaborate to complete a programming task. We provide a multi-agent framework,\ndiscuss the performance of the agents in each simulation, and discuss potential\nfuture additions.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL",
      "cs.ET"
    ],
    "primary_category": "cs.MA",
    "comment": "15 pages, 5 figures, published in the MICS 2024 conference",
    "pdf_url": "http://arxiv.org/pdf/2409.13753v1",
    "published_date": "2024-09-14 21:53:35 UTC",
    "updated_date": "2024-09-14 21:53:35 UTC"
  },
  {
    "arxiv_id": "2409.09541v3",
    "title": "Autonomous Goal Detection and Cessation in Reinforcement Learning: A Case Study on Source Term Estimation",
    "authors": [
      "Yiwei Shi",
      "Muning Wen",
      "Qi Zhang",
      "Weinan Zhang",
      "Cunjia Liu",
      "Weiru Liu"
    ],
    "abstract": "Reinforcement Learning has revolutionized decision-making processes in\ndynamic environments, yet it often struggles with autonomously detecting and\nachieving goals without clear feedback signals. For example, in a Source Term\nEstimation problem, the lack of precise environmental information makes it\nchallenging to provide clear feedback signals and to define and evaluate how\nthe source's location is determined. To address this challenge, the Autonomous\nGoal Detection and Cessation (AGDC) module was developed, enhancing various RL\nalgorithms by incorporating a self-feedback mechanism for autonomous goal\ndetection and cessation upon task completion. Our method effectively identifies\nand ceases undefined goals by approximating the agent's belief, significantly\nenhancing the capabilities of RL algorithms in environments with limited\nfeedback. To validate effectiveness of our approach, we integrated AGDC with\ndeep Q-Network, proximal policy optimization, and deep deterministic policy\ngradient algorithms, and evaluated its performance on the Source Term\nEstimation problem. The experimental results showed that AGDC-enhanced RL\nalgorithms significantly outperformed traditional statistical methods such as\ninfotaxis, entrotaxis, and dual control for exploitation and exploration, as\nwell as a non-statistical random action selection method. These improvements\nwere evident in terms of success rate, mean traveled distance, and search time,\nhighlighting AGDC's effectiveness and efficiency in complex, real-world\nscenarios.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09541v3",
    "published_date": "2024-09-14 21:42:17 UTC",
    "updated_date": "2024-12-12 17:12:06 UTC"
  },
  {
    "arxiv_id": "2409.09536v2",
    "title": "VernaCopter: Disambiguated Natural-Language-Driven Robot via Formal Specifications",
    "authors": [
      "Teun van de Laar",
      "Zengjie Zhang",
      "Shuhao Qi",
      "Sofie Haesaert",
      "Zhiyong Sun"
    ],
    "abstract": "It has been an ambition of many to control a robot for a complex task using\nnatural language (NL). The rise of large language models (LLMs) makes it closer\nto coming true. However, an LLM-powered system still suffers from the ambiguity\ninherent in an NL and the uncertainty brought up by LLMs. This paper proposes a\nnovel LLM-based robot motion planner, named \\textit{VernaCopter}, with signal\ntemporal logic (STL) specifications serving as a bridge between NL commands and\nspecific task objectives. The rigorous and abstract nature of formal\nspecifications allows the planner to generate high-quality and highly\nconsistent paths to guide the motion control of a robot. Compared to a\nconventional NL-prompting-based planner, the proposed VernaCopter planner is\nmore stable and reliable due to less ambiguous uncertainty. Its efficacy and\nadvantage have been validated by two small but challenging experimental\nscenarios, implying its potential in designing NL-driven robots.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09536v2",
    "published_date": "2024-09-14 21:36:22 UTC",
    "updated_date": "2025-03-06 19:37:39 UTC"
  },
  {
    "arxiv_id": "2409.09520v2",
    "title": "Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM",
    "authors": [
      "Xin Hu",
      "Janet Wang",
      "Jihun Hamm",
      "Rie R Yotsu",
      "Zhengming Ding"
    ],
    "abstract": "Current AI-assisted skin image diagnosis has achieved dermatologist-level\nperformance in classifying skin cancer, driven by rapid advancements in deep\nlearning architectures. However, unlike traditional vision tasks, skin images\nin general present unique challenges due to the limited availability of\nwell-annotated datasets, complex variations in conditions, and the necessity\nfor detailed interpretations to ensure patient safety. Previous segmentation\nmethods have sought to reduce image noise and enhance diagnostic performance,\nbut these techniques require fine-grained, pixel-level ground truth masks for\ntraining. In contrast, with the rise of foundation models, the Segment Anything\nModel (SAM) has been introduced to facilitate promptable segmentation, enabling\nthe automation of the segmentation process with simple yet effective prompts.\nEfforts applying SAM predominantly focus on dermatoscopy images, which present\nmore easily identifiable lesion boundaries than clinical photos taken with\nsmartphones. This limitation constrains the practicality of these approaches to\nreal-world applications. To overcome the challenges posed by noisy clinical\nphotos acquired via non-standardized protocols and to improve diagnostic\naccessibility, we propose a novel Cross-Attentive Fusion framework for\ninterpretable skin lesion diagnosis. Our method leverages SAM to generate\nvisual concepts for skin diseases using prompts, integrating local visual\nconcepts with global image features to enhance model performance. Extensive\nevaluation on two skin disease datasets demonstrates our proposed method's\neffectiveness on lesion diagnosis and interpretability.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper is accepted by WACV 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.09520v2",
    "published_date": "2024-09-14 20:11:25 UTC",
    "updated_date": "2025-01-15 23:21:06 UTC"
  },
  {
    "arxiv_id": "2409.09517v1",
    "title": "Deep Learning Under Siege: Identifying Security Vulnerabilities and Risk Mitigation Strategies",
    "authors": [
      "Jamal Al-Karaki",
      "Muhammad Al-Zafar Khan",
      "Mostafa Mohamad",
      "Dababrata Chowdhury"
    ],
    "abstract": "With the rise in the wholesale adoption of Deep Learning (DL) models in\nnearly all aspects of society, a unique set of challenges is imposed. Primarily\ncentered around the architectures of these models, these risks pose a\nsignificant challenge, and addressing these challenges is key to their\nsuccessful implementation and usage in the future. In this research, we present\nthe security challenges associated with the current DL models deployed into\nproduction, as well as anticipate the challenges of future DL technologies\nbased on the advancements in computing, AI, and hardware technologies. In\naddition, we propose risk mitigation techniques to inhibit these challenges and\nprovide metrical evaluations to measure the effectiveness of these metrics.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "10 pages, 1 table, 6 equations/metrics",
    "pdf_url": "http://arxiv.org/pdf/2409.09517v1",
    "published_date": "2024-09-14 19:54:12 UTC",
    "updated_date": "2024-09-14 19:54:12 UTC"
  },
  {
    "arxiv_id": "2409.09513v1",
    "title": "Planning Transformer: Long-Horizon Offline Reinforcement Learning with Planning Tokens",
    "authors": [
      "Joseph Clinton",
      "Robert Lieck"
    ],
    "abstract": "Supervised learning approaches to offline reinforcement learning,\nparticularly those utilizing the Decision Transformer, have shown effectiveness\nin continuous environments and for sparse rewards. However, they often struggle\nwith long-horizon tasks due to the high compounding error of auto-regressive\nmodels. To overcome this limitation, we go beyond next-token prediction and\nintroduce Planning Tokens, which contain high-level, long time-scale\ninformation about the agent's future. Predicting dual time-scale tokens at\nregular intervals enables our model to use these long-horizon Planning Tokens\nas a form of implicit planning to guide its low-level policy and reduce\ncompounding error. This architectural modification significantly enhances\nperformance on long-horizon tasks, establishing a new state-of-the-art in\ncomplex D4RL environments. Additionally, we demonstrate that Planning Tokens\nimprove the interpretability of the model's policy through the interpretable\nplan visualisations and attention map.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 5 figures, Submitted to AAAI",
    "pdf_url": "http://arxiv.org/pdf/2409.09513v1",
    "published_date": "2024-09-14 19:30:53 UTC",
    "updated_date": "2024-09-14 19:30:53 UTC"
  },
  {
    "arxiv_id": "2409.09511v1",
    "title": "Explaining Deep Learning Embeddings for Speech Emotion Recognition by Predicting Interpretable Acoustic Features",
    "authors": [
      "Satvik Dixit",
      "Daniel M. Low",
      "Gasser Elbanna",
      "Fabio Catania",
      "Satrajit S. Ghosh"
    ],
    "abstract": "Pre-trained deep learning embeddings have consistently shown superior\nperformance over handcrafted acoustic features in speech emotion recognition\n(SER). However, unlike acoustic features with clear physical meaning, these\nembeddings lack clear interpretability. Explaining these embeddings is crucial\nfor building trust in healthcare and security applications and advancing the\nscientific understanding of the acoustic information that is encoded in them.\nThis paper proposes a modified probing approach to explain deep learning\nembeddings in the SER space. We predict interpretable acoustic features (e.g.,\nf0, loudness) from (i) the complete set of embeddings and (ii) a subset of the\nembedding dimensions identified as most important for predicting each emotion.\nIf the subset of the most important dimensions better predicts a given emotion\nthan all dimensions and also predicts specific acoustic features more\naccurately, we infer those acoustic features are important for the embedding\nmodel for the given task. We conducted experiments using the WavLM embeddings\nand eGeMAPS acoustic features as audio representations, applying our method to\nthe RAVDESS and SAVEE emotional speech datasets. Based on this evaluation, we\ndemonstrate that Energy, Frequency, Spectral, and Temporal categories of\nacoustic features provide diminishing information to SER in that order,\ndemonstrating the utility of the probing classifier method to relate embeddings\nto interpretable acoustic features.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09511v1",
    "published_date": "2024-09-14 19:18:56 UTC",
    "updated_date": "2024-09-14 19:18:56 UTC"
  },
  {
    "arxiv_id": "2410.01816v1",
    "title": "Automatic Scene Generation: State-of-the-Art Techniques, Models, Datasets, Challenges, and Future Prospects",
    "authors": [
      "Awal Ahmed Fime",
      "Saifuddin Mahmud",
      "Arpita Das",
      "Md. Sunzidul Islam",
      "Hong-Hoon Kim"
    ],
    "abstract": "Automatic scene generation is an essential area of research with applications\nin robotics, recreation, visual representation, training and simulation,\neducation, and more. This survey provides a comprehensive review of the current\nstate-of-the-arts in automatic scene generation, focusing on techniques that\nleverage machine learning, deep learning, embedded systems, and natural\nlanguage processing (NLP). We categorize the models into four main types:\nVariational Autoencoders (VAEs), Generative Adversarial Networks (GANs),\nTransformers, and Diffusion Models. Each category is explored in detail,\ndiscussing various sub-models and their contributions to the field.\n  We also review the most commonly used datasets, such as COCO-Stuff, Visual\nGenome, and MS-COCO, which are critical for training and evaluating these\nmodels. Methodologies for scene generation are examined, including image-to-3D\nconversion, text-to-3D generation, UI/layout design, graph-based methods, and\ninteractive scene generation. Evaluation metrics such as Frechet Inception\nDistance (FID), Kullback-Leibler (KL) Divergence, Inception Score (IS),\nIntersection over Union (IoU), and Mean Average Precision (mAP) are discussed\nin the context of their use in assessing model performance.\n  The survey identifies key challenges and limitations in the field, such as\nmaintaining realism, handling complex scenes with multiple objects, and\nensuring consistency in object relationships and spatial arrangements. By\nsummarizing recent advances and pinpointing areas for improvement, this survey\naims to provide a valuable resource for researchers and practitioners working\non automatic scene generation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "59 pages, 16 figures, 3 tables, 36 equations, 348 references",
    "pdf_url": "http://arxiv.org/pdf/2410.01816v1",
    "published_date": "2024-09-14 19:09:10 UTC",
    "updated_date": "2024-09-14 19:09:10 UTC"
  },
  {
    "arxiv_id": "2409.09506v1",
    "title": "ESPnet-EZ: Python-only ESPnet for Easy Fine-tuning and Integration",
    "authors": [
      "Masao Someki",
      "Kwanghee Choi",
      "Siddhant Arora",
      "William Chen",
      "Samuele Cornell",
      "Jionghao Han",
      "Yifan Peng",
      "Jiatong Shi",
      "Vaibhav Srivastav",
      "Shinji Watanabe"
    ],
    "abstract": "We introduce ESPnet-EZ, an extension of the open-source speech processing\ntoolkit ESPnet, aimed at quick and easy development of speech models. ESPnet-EZ\nfocuses on two major aspects: (i) easy fine-tuning and inference of existing\nESPnet models on various tasks and (ii) easy integration with popular deep\nneural network frameworks such as PyTorch-Lightning, Hugging Face transformers\nand datasets, and Lhotse. By replacing ESPnet design choices inherited from\nKaldi with a Python-only, Bash-free interface, we dramatically reduce the\neffort required to build, debug, and use a new model. For example, to fine-tune\na speech foundation model, ESPnet-EZ, compared to ESPnet, reduces the number of\nnewly written code by 2.7x and the amount of dependent code by 6.7x while\ndramatically reducing the Bash script dependencies. The codebase of ESPnet-EZ\nis publicly available.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted to SLT 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.09506v1",
    "published_date": "2024-09-14 19:03:53 UTC",
    "updated_date": "2024-09-14 19:03:53 UTC"
  },
  {
    "arxiv_id": "2409.09501v1",
    "title": "Synthetic4Health: Generating Annotated Synthetic Clinical Letters",
    "authors": [
      "Libo Ren",
      "Samuel Belkadi",
      "Lifeng Han",
      "Warren Del-Pinto",
      "Goran Nenadic"
    ],
    "abstract": "Since clinical letters contain sensitive information, clinical-related\ndatasets can not be widely applied in model training, medical research, and\nteaching. This work aims to generate reliable, various, and de-identified\nsynthetic clinical letters. To achieve this goal, we explored different\npre-trained language models (PLMs) for masking and generating text. After that,\nwe worked on Bio\\_ClinicalBERT, a high-performing model, and experimented with\ndifferent masking strategies. Both qualitative and quantitative methods were\nused for evaluation. Additionally, a downstream task, Named Entity Recognition\n(NER), was also implemented to assess the usability of these synthetic letters.\n  The results indicate that 1) encoder-only models outperform encoder-decoder\nmodels. 2) Among encoder-only models, those trained on general corpora perform\ncomparably to those trained on clinical data when clinical information is\npreserved. 3) Additionally, preserving clinical entities and document structure\nbetter aligns with our objectives than simply fine-tuning the model. 4)\nFurthermore, different masking strategies can impact the quality of synthetic\nclinical letters. Masking stopwords has a positive impact, while masking nouns\nor verbs has a negative effect. 5) For evaluation, BERTScore should be the\nprimary quantitative evaluation metric, with other metrics serving as\nsupplementary references. 6) Contextual information does not significantly\nimpact the models' understanding, so the synthetic clinical letters have the\npotential to replace the original ones in downstream tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ongoing work, 48 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.09501v1",
    "published_date": "2024-09-14 18:15:07 UTC",
    "updated_date": "2024-09-14 18:15:07 UTC"
  },
  {
    "arxiv_id": "2410.01815v1",
    "title": "AI in Food Marketing from Personalized Recommendations to Predictive Analytics: Comparing Traditional Advertising Techniques with AI-Driven Strategies",
    "authors": [
      "Elham Khamoushi"
    ],
    "abstract": "Artificial Intelligence (AI) has revolutionized food marketing by providing\nadvanced techniques for personalized recommendations, consumer behavior\nprediction, and campaign optimization. This paper explores the shift from\ntraditional advertising methods, such as TV, radio, and print, to AI-driven\nstrategies. Traditional approaches were successful in building brand awareness\nbut lacked the level of personalization that modern consumers demand. AI\nleverages data from consumer purchase histories, browsing behaviors, and social\nmedia activity to create highly tailored marketing campaigns. These strategies\nallow for more accurate product recommendations, prediction of consumer needs,\nand ultimately improve customer satisfaction and user experience. AI enhances\nmarketing efforts by automating labor-intensive processes, leading to greater\nefficiency and cost savings. It also enables the continuous adaptation of\nmarketing messages, ensuring they remain relevant and engaging over time. While\nAI presents significant benefits in terms of personalization and efficiency, it\nalso comes with challenges, particularly the substantial investment required\nfor technology and skilled expertise. This paper compares the strengths and\nweaknesses of traditional and AI-driven food marketing techniques, offering\nvaluable insights into how marketers can leverage AI to create more effective\nand targeted marketing strategies in the evolving digital landscape.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.01815v1",
    "published_date": "2024-09-14 17:53:32 UTC",
    "updated_date": "2024-09-14 17:53:32 UTC"
  },
  {
    "arxiv_id": "2409.09497v2",
    "title": "Multi-Scale Grouped Prototypes for Interpretable Semantic Segmentation",
    "authors": [
      "Hugo Porta",
      "Emanuele Dalsasso",
      "Diego Marcos",
      "Devis Tuia"
    ],
    "abstract": "Prototypical part learning is emerging as a promising approach for making\nsemantic segmentation interpretable. The model selects real patches seen during\ntraining as prototypes and constructs the dense prediction map based on the\nsimilarity between parts of the test image and the prototypes. This improves\ninterpretability since the user can inspect the link between the predicted\noutput and the patterns learned by the model in terms of prototypical\ninformation. In this paper, we propose a method for interpretable semantic\nsegmentation that leverages multi-scale image representation for prototypical\npart learning. First, we introduce a prototype layer that explicitly learns\ndiverse prototypical parts at several scales, leading to multi-scale\nrepresentations in the prototype activation output. Then, we propose a sparse\ngrouping mechanism that produces multi-scale sparse groups of these\nscale-specific prototypical parts. This provides a deeper understanding of the\ninteractions between multi-scale object representations while enhancing the\ninterpretability of the segmentation model. The experiments conducted on Pascal\nVOC, Cityscapes, and ADE20K demonstrate that the proposed method increases\nmodel sparsity, improves interpretability over existing prototype-based\nmethods, and narrows the performance gap with the non-interpretable counterpart\nmodels. Code is available at github.com/eceo-epfl/ScaleProtoSeg.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at WACV 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.09497v2",
    "published_date": "2024-09-14 17:52:59 UTC",
    "updated_date": "2025-04-28 14:55:45 UTC"
  },
  {
    "arxiv_id": "2409.09493v1",
    "title": "Hacking, The Lazy Way: LLM Augmented Pentesting",
    "authors": [
      "Dhruva Goyal",
      "Sitaraman Subramanian",
      "Aditya Peela"
    ],
    "abstract": "Security researchers are continually challenged by the need to stay current\nwith rapidly evolving cybersecurity research, tools, and techniques. This\nconstant cycle of learning, unlearning, and relearning, combined with the\nrepetitive tasks of sifting through documentation and analyzing data, often\nhinders productivity and innovation. This has led to a disparity where only\norganizations with substantial resources can access top-tier security experts,\nwhile others rely on firms with less skilled researchers who focus primarily on\ncompliance rather than actual security.\n  We introduce \"LLM Augmented Pentesting,\" demonstrated through a tool named\n\"Pentest Copilot,\" to address this gap. This approach integrates Large Language\nModels into penetration testing workflows. Our research includes a \"chain of\nthought\" mechanism to streamline token usage and boost performance, as well as\nunique Retrieval Augmented Generation implementation to minimize hallucinations\nand keep models aligned with the latest techniques. Additionally, we propose a\nnovel file analysis approach, enabling LLMs to understand files. Furthermore,\nwe highlight a unique infrastructure system that supports if implemented, can\nsupport in-browser assisted penetration testing, offering a robust platform for\ncybersecurity professionals, These advancements mark a significant step toward\nbridging the gap between automated tools and human expertise, offering a\npowerful solution to the challenges faced by modern cybersecurity teams.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "I.2.1"
    ],
    "primary_category": "cs.CR",
    "comment": "9 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.09493v1",
    "published_date": "2024-09-14 17:40:35 UTC",
    "updated_date": "2024-09-14 17:40:35 UTC"
  },
  {
    "arxiv_id": "2409.09485v1",
    "title": "Enumerating Minimal Unsatisfiable Cores of LTLf formulas",
    "authors": [
      "Antonio Ielo",
      "Giuseppe Mazzotta",
      "Rafael Peñaloza",
      "Francesco Ricca"
    ],
    "abstract": "Linear Temporal Logic over finite traces ($\\text{LTL}_f$) is a widely used\nformalism with applications in AI, process mining, model checking, and more.\nThe primary reasoning task for $\\text{LTL}_f$ is satisfiability checking; yet,\nthe recent focus on explainable AI has increased interest in analyzing\ninconsistent formulas, making the enumeration of minimal explanations for\ninfeasibility a relevant task also for $\\text{LTL}_f$. This paper introduces a\nnovel technique for enumerating minimal unsatisfiable cores (MUCs) of an\n$\\text{LTL}_f$ specification. The main idea is to encode a $\\text{LTL}_f$\nformula into an Answer Set Programming (ASP) specification, such that the\nminimal unsatisfiable subsets (MUSes) of the ASP program directly correspond to\nthe MUCs of the original $\\text{LTL}_f$ specification. Leveraging recent\nadvancements in ASP solving yields a MUC enumerator achieving good performance\nin experiments conducted on established benchmarks from the literature.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09485v1",
    "published_date": "2024-09-14 17:15:30 UTC",
    "updated_date": "2024-09-14 17:15:30 UTC"
  },
  {
    "arxiv_id": "2409.09478v2",
    "title": "From FDG to PSMA: A Hitchhiker's Guide to Multitracer, Multicenter Lesion Segmentation in PET/CT Imaging",
    "authors": [
      "Maximilian Rokuss",
      "Balint Kovacs",
      "Yannick Kirchhoff",
      "Shuhan Xiao",
      "Constantin Ulrich",
      "Klaus H. Maier-Hein",
      "Fabian Isensee"
    ],
    "abstract": "Automated lesion segmentation in PET/CT scans is crucial for improving\nclinical workflows and advancing cancer diagnostics. However, the task is\nchallenging due to physiological variability, different tracers used in PET\nimaging, and diverse imaging protocols across medical centers. To address this,\nthe autoPET series was created to challenge researchers to develop algorithms\nthat generalize across diverse PET/CT environments. This paper presents our\nsolution for the autoPET III challenge, targeting multitracer, multicenter\ngeneralization using the nnU-Net framework with the ResEncL architecture. Key\ntechniques include misalignment data augmentation and multi-modal pretraining\nacross CT, MR, and PET datasets to provide an initial anatomical understanding.\nWe incorporate organ supervision as a multitask approach, enabling the model to\ndistinguish between physiological uptake and tracer-specific patterns, which is\nparticularly beneficial in cases where no lesions are present. Compared to the\ndefault nnU-Net, which achieved a Dice score of 57.61, or the larger ResEncL\n(65.31) our model significantly improved performance with a Dice score of\n68.40, alongside a reduction in false positive (FPvol: 7.82) and false negative\n(FNvol: 10.35) volumes. These results underscore the effectiveness of combining\nadvanced network design, augmentation, pretraining, and multitask learning for\nPET/CT lesion segmentation. After evaluation on the test set, our approach was\nawarded the first place in the model-centric category (Team LesionTracer). Code\nis publicly available at https://github.com/MIC-DKFZ/autopet-3-submission.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Winning method of the autoPET III challenge (model-centric) - Team\n  LesionTracer",
    "pdf_url": "http://arxiv.org/pdf/2409.09478v2",
    "published_date": "2024-09-14 16:39:17 UTC",
    "updated_date": "2024-10-21 14:15:30 UTC"
  },
  {
    "arxiv_id": "2409.09461v2",
    "title": "TX-Gen: Multi-Objective Optimization for Sparse Counterfactual Explanations for Time-Series Classification",
    "authors": [
      "Qi Huang",
      "Sofoklis Kitharidis",
      "Thomas Bäck",
      "Niki van Stein"
    ],
    "abstract": "In time-series classification, understanding model decisions is crucial for\ntheir application in high-stakes domains such as healthcare and finance.\nCounterfactual explanations, which provide insights by presenting alternative\ninputs that change model predictions, offer a promising solution. However,\nexisting methods for generating counterfactual explanations for time-series\ndata often struggle with balancing key objectives like proximity, sparsity, and\nvalidity. In this paper, we introduce TX-Gen, a novel algorithm for generating\ncounterfactual explanations based on the Non-dominated Sorting Genetic\nAlgorithm II (NSGA-II). TX-Gen leverages evolutionary multi-objective\noptimization to find a diverse set of counterfactuals that are both sparse and\nvalid, while maintaining minimal dissimilarity to the original time series. By\nincorporating a flexible reference-guided mechanism, our method improves the\nplausibility and interpretability of the counterfactuals without relying on\npredefined assumptions. Extensive experiments on benchmark datasets demonstrate\nthat TX-Gen outperforms existing methods in generating high-quality\ncounterfactuals, making time-series models more transparent and interpretable.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to EXPLAINS 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.09461v2",
    "published_date": "2024-09-14 15:13:28 UTC",
    "updated_date": "2024-11-11 09:37:09 UTC"
  },
  {
    "arxiv_id": "2409.09446v1",
    "title": "MulCPred: Learning Multi-modal Concepts for Explainable Pedestrian Action Prediction",
    "authors": [
      "Yan Feng",
      "Alexander Carballo",
      "Keisuke Fujii",
      "Robin Karlsson",
      "Ming Ding",
      "Kazuya Takeda"
    ],
    "abstract": "Pedestrian action prediction is of great significance for many applications\nsuch as autonomous driving. However, state-of-the-art methods lack\nexplainability to make trustworthy predictions. In this paper, a novel\nframework called MulCPred is proposed that explains its predictions based on\nmulti-modal concepts represented by training samples. Previous concept-based\nmethods have limitations including: 1) they cannot directly apply to\nmulti-modal cases; 2) they lack locality to attend to details in the inputs; 3)\nthey suffer from mode collapse. These limitations are tackled accordingly\nthrough the following approaches: 1) a linear aggregator to integrate the\nactivation results of the concepts into predictions, which associates concepts\nof different modalities and provides ante-hoc explanations of the relevance\nbetween the concepts and the predictions; 2) a channel-wise recalibration\nmodule that attends to local spatiotemporal regions, which enables the concepts\nwith locality; 3) a feature regularization loss that encourages the concepts to\nlearn diverse patterns. MulCPred is evaluated on multiple datasets and tasks.\nBoth qualitative and quantitative results demonstrate that MulCPred is\npromising in improving the explainability of pedestrian action prediction\nwithout obvious performance degradation. Furthermore, by removing\nunrecognizable concepts from MulCPred, the cross-dataset prediction performance\nis improved, indicating the feasibility of further generalizability of\nMulCPred.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09446v1",
    "published_date": "2024-09-14 14:15:28 UTC",
    "updated_date": "2024-09-14 14:15:28 UTC"
  },
  {
    "arxiv_id": "2409.09424v3",
    "title": "NBBOX: Noisy Bounding Box Improves Remote Sensing Object Detection",
    "authors": [
      "Yechan Kim",
      "SooYeon Kim",
      "Moongu Jeon"
    ],
    "abstract": "Data augmentation has shown significant advancements in computer vision to\nimprove model performance over the years, particularly in scenarios with\nlimited and insufficient data. Currently, most studies focus on adjusting the\nimage or its features to expand the size, quality, and variety of samples\nduring training in various tasks including object detection. However, we argue\nthat it is necessary to investigate bounding box transformations as a data\naugmentation technique rather than image-level transformations, especially in\naerial imagery due to potentially inconsistent bounding box annotations. Hence,\nthis letter presents a thorough investigation of bounding box transformation in\nterms of scaling, rotation, and translation for remote sensing object\ndetection. We call this augmentation strategy NBBOX (Noise Injection into\nBounding Box). We conduct extensive experiments on DOTA and DIOR-R, both\nwell-known datasets that include a variety of rotated generic objects in aerial\nimages. Experimental results show that our approach significantly improves\nremote sensing object detection without whistles and bells and it is more\ntime-efficient than other state-of-the-art augmentation strategies.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to IEEE Geoscience and Remote Sensing Letters",
    "pdf_url": "http://arxiv.org/pdf/2409.09424v3",
    "published_date": "2024-09-14 12:25:14 UTC",
    "updated_date": "2025-01-07 11:37:57 UTC"
  },
  {
    "arxiv_id": "2409.09418v1",
    "title": "Distributed Clustering based on Distributional Kernel",
    "authors": [
      "Hang Zhang",
      "Yang Xu",
      "Lei Gong",
      "Ye Zhu",
      "Kai Ming Ting"
    ],
    "abstract": "This paper introduces a new framework for clustering in a distributed network\ncalled Distributed Clustering based on Distributional Kernel (K) or KDC that\nproduces the final clusters based on the similarity with respect to the\ndistributions of initial clusters, as measured by K. It is the only framework\nthat satisfies all three of the following properties. First, KDC guarantees\nthat the combined clustering outcome from all sites is equivalent to the\nclustering outcome of its centralized counterpart from the combined dataset\nfrom all sites. Second, the maximum runtime cost of any site in distributed\nmode is smaller than the runtime cost in centralized mode. Third, it is\ndesigned to discover clusters of arbitrary shapes, sizes and densities. To the\nbest of our knowledge, this is the first distributed clustering framework that\nemploys a distributional kernel. The distribution-based clustering leads\ndirectly to significantly better clustering outcomes than existing methods of\ndistributed clustering. In addition, we introduce a new clustering algorithm\ncalled Kernel Bounded Cluster Cores, which is the best clustering algorithm\napplied to KDC among existing clustering algorithms. We also show that KDC is a\ngeneric framework that enables a quadratic time clustering algorithm to deal\nwith large datasets that would otherwise be impossible.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09418v1",
    "published_date": "2024-09-14 11:40:54 UTC",
    "updated_date": "2024-09-14 11:40:54 UTC"
  },
  {
    "arxiv_id": "2409.10571v1",
    "title": "ASFT: Aligned Supervised Fine-Tuning through Absolute Likelihood",
    "authors": [
      "Ruoyu Wang",
      "Jiachen Sun",
      "Shaowei Hua",
      "Quan Fang"
    ],
    "abstract": "Direct Preference Optimization (DPO) is a method for enhancing model\nperformance by directly optimizing for the preferences or rankings of outcomes,\ninstead of traditional loss functions. This approach has proven effective in\naligning Large Language Models (LLMs) with human preferences. Despite its\nwidespread use across various tasks, DPO has been criticized for its\nsensitivity to the effectiveness of Supervised Fine-Tuning (SFT) and its\nlimitations in enabling models to learn human-preferred responses, leading to\nless satisfactory performance. To address these limitations, we propose Aligned\nSupervised Fine-Tuning (ASFT), an effective approach that better aligns LLMs\nwith pair-wise datasets by optimizing absolute likelihood for each response,\nrather than using the Bradley-Terry model, and eliminates the need for a\nreference model. Through theoretical gradient analysis, we demonstrate that\nASFT mitigates the issue where the DPO loss function decreases the probability\nof generating human-dispreferred data at a faster rate than it increases the\nprobability of producing preferred data. Additionally, we compare ASFT to DPO\nand its latest variants, such as the single-step approach ORPO, using the\nlatest instruction-tuned model Llama3, which has been fine-tuned on\nUltraFeedback and HH-RLHF. We evaluated performance on instruction-following\nbenchmarks like MT-Bench and traditional text generation metrics such as BLEU-4\nand ROUGE-L. Extensive experiments demonstrate that ASFT is an effective\nalignment approach, consistently outperforming existing methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10571v1",
    "published_date": "2024-09-14 11:39:13 UTC",
    "updated_date": "2024-09-14 11:39:13 UTC"
  },
  {
    "arxiv_id": "2409.09415v1",
    "title": "Enhancing LLM Problem Solving with REAP: Reflection, Explicit Problem Deconstruction, and Advanced Prompting",
    "authors": [
      "Ryan Lingo",
      "Martin Arroyo",
      "Rajeev Chhajer"
    ],
    "abstract": "Large Language Models (LLMs) have transformed natural language processing,\nyet improving their problem-solving capabilities, particularly for complex,\nreasoning-intensive tasks, remains a persistent challenge. This paper\nintroduces the REAP (Reflection, Explicit Problem Deconstruction, and Advanced\nPrompting) method, an innovative approach within the dynamic context generation\nframework. REAP guides LLMs through reflection on the query, deconstructing it\ninto manageable components, and generating relevant context to enhance the\nsolution process. We evaluated REAP using a dataset designed to expose LLM\nlimitations, comparing zero-shot prompting with REAP-enhanced prompts across\nsix state-of-the-art models: OpenAI's o1-preview, o1-mini, GPT-4o, GPT-4o-mini,\nGoogle's Gemini 1.5 Pro, and Claude 3.5 Sonnet. The results demonstrate notable\nperformance gains, with o1-mini improving by 40.97%, GPT-4o by 66.26%, and\nGPT-4o-mini by 112.93%. Despite the already strong baseline performance of\nOpenAI's o1-preview, modest gains were observed. Beyond performance\nimprovements, REAP offers a cost-effective solution; for example, GPT-4o-mini,\nwhich is approximately 100 times cheaper than o1-preview, delivered competitive\nresults. REAP also improves the clarity of model outputs, making it easier for\nhumans to understand the reasoning behind the results and simplifying the\nprocess of identifying and addressing any issues. These findings demonstrate\nREAP's potential to greatly improve the capabilities of LLMs, providing both\nbetter performance and increased cost-efficiency across a wide range of\napplications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "524 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.09415v1",
    "published_date": "2024-09-14 11:12:07 UTC",
    "updated_date": "2024-09-14 11:12:07 UTC"
  },
  {
    "arxiv_id": "2409.09413v2",
    "title": "Constructive Approach to Bidirectional Influence between Qualia Structure and Language Emergence",
    "authors": [
      "Tadahiro Taniguchi",
      "Masafumi Oizumi",
      "Noburo Saji",
      "Takato Horii",
      "Naotsugu Tsuchiya"
    ],
    "abstract": "This perspective paper explores the bidirectional influence between language\nemergence and the relational structure of subjective experiences, termed qualia\nstructure, and lays out a constructive approach to the intricate dependency\nbetween the two. We hypothesize that the emergence of languages with\ndistributional semantics (e.g., syntactic-semantic structures) is linked to the\ncoordination of internal representations shaped by experience, potentially\nfacilitating more structured language through reciprocal influence. This\nhypothesized mutual dependency connects to recent advancements in AI and symbol\nemergence robotics, and is explored within this paper through theoretical\nframeworks such as the collective predictive coding. Computational studies show\nthat neural network-based language models form systematically structured\ninternal representations, and multimodal language models can share\nrepresentations between language and perceptual information. This perspective\nsuggests that language emergence serves not only as a mechanism creating a\ncommunication tool but also as a mechanism for allowing people to realize\nshared understanding of qualitative experiences. The paper discusses the\nimplications of this bidirectional influence in the context of consciousness\nstudies, linguistics, and cognitive science, and outlines future constructive\nresearch directions to further explore this dynamic relationship between\nlanguage emergence and qualia structure.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09413v2",
    "published_date": "2024-09-14 11:03:12 UTC",
    "updated_date": "2025-05-05 03:26:36 UTC"
  },
  {
    "arxiv_id": "2409.09412v2",
    "title": "Label Convergence: Defining an Upper Performance Bound in Object Recognition through Contradictory Annotations",
    "authors": [
      "David Tschirschwitz",
      "Volker Rodehorst"
    ],
    "abstract": "Annotation errors are a challenge not only during training of machine\nlearning models, but also during their evaluation. Label variations and\ninaccuracies in datasets often manifest as contradictory examples that deviate\nfrom established labeling conventions. Such inconsistencies, when significant,\nprevent models from achieving optimal performance on metrics such as mean\nAverage Precision (mAP). We introduce the notion of \"label convergence\" to\ndescribe the highest achievable performance under the constraint of\ncontradictory test annotations, essentially defining an upper bound on model\naccuracy.\n  Recognizing that noise is an inherent characteristic of all data, our study\nanalyzes five real-world datasets, including the LVIS dataset, to investigate\nthe phenomenon of label convergence. We approximate that label convergence is\nbetween 62.63-67.52 mAP@[0.5:0.95:0.05] for LVIS with 95% confidence,\nattributing these bounds to the presence of real annotation errors. With\ncurrent state-of-the-art (SOTA) models at the upper end of the label\nconvergence interval for the well-studied LVIS dataset, we conclude that model\ncapacity is sufficient to solve current object detection problems. Therefore,\nfuture efforts should focus on three key aspects: (1) updating the problem\nspecification and adjusting evaluation practices to account for unavoidable\nlabel noise, (2) creating cleaner data, especially test data, and (3) including\nmulti-annotated data to investigate annotation variation and make these issues\nvisible from the outset.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at WACV 2025, added reference to paper associated code",
    "pdf_url": "http://arxiv.org/pdf/2409.09412v2",
    "published_date": "2024-09-14 10:59:25 UTC",
    "updated_date": "2025-01-21 23:23:29 UTC"
  },
  {
    "arxiv_id": "2410.01813v1",
    "title": "Privacy-Preserving SAM Quantization for Efficient Edge Intelligence in Healthcare",
    "authors": [
      "Zhikai Li",
      "Jing Zhang",
      "Qingyi Gu"
    ],
    "abstract": "The disparity in healthcare personnel expertise and medical resources across\ndifferent regions of the world is a pressing social issue. Artificial\nintelligence technology offers new opportunities to alleviate this issue.\nSegment Anything Model (SAM), which excels in intelligent image segmentation,\nhas demonstrated exceptional performance in medical monitoring and assisted\ndiagnosis. Unfortunately, the huge computational and storage overhead of SAM\nposes significant challenges for deployment on resource-limited edge devices.\nQuantization is an effective solution for model compression; however,\ntraditional methods rely heavily on original data for calibration, which raises\nwidespread concerns about medical data privacy and security. In this paper, we\npropose a data-free quantization framework for SAM, called DFQ-SAM, which\nlearns and calibrates quantization parameters without any original data, thus\neffectively preserving data privacy during model compression. Specifically, we\npropose pseudo-positive label evolution for segmentation, combined with patch\nsimilarity, to fully leverage the semantic and distribution priors in\npre-trained models, which facilitates high-quality data synthesis as a\nsubstitute for real data. Furthermore, we introduce scale reparameterization to\nensure the accuracy of low-bit quantization. We perform extensive segmentation\nexperiments on various datasets, and DFQ-SAM consistently provides significant\nperformance on low-bit quantization. DFQ-SAM eliminates the need for data\ntransfer in cloud-edge collaboration, thereby protecting sensitive data from\npotential attacks. It enables secure, fast, and personalized healthcare\nservices at the edge, which enhances system efficiency and optimizes resource\nallocation, and thus facilitating the pervasive application of artificial\nintelligence in worldwide healthcare.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.01813v1",
    "published_date": "2024-09-14 10:43:35 UTC",
    "updated_date": "2024-09-14 10:43:35 UTC"
  },
  {
    "arxiv_id": "2409.09406v1",
    "title": "Real-world Adversarial Defense against Patch Attacks based on Diffusion Model",
    "authors": [
      "Xingxing Wei",
      "Caixin Kang",
      "Yinpeng Dong",
      "Zhengyi Wang",
      "Shouwei Ruan",
      "Yubo Chen",
      "Hang Su"
    ],
    "abstract": "Adversarial patches present significant challenges to the robustness of deep\nlearning models, making the development of effective defenses become critical\nfor real-world applications. This paper introduces DIFFender, a novel\nDIFfusion-based DeFender framework that leverages the power of a text-guided\ndiffusion model to counter adversarial patch attacks. At the core of our\napproach is the discovery of the Adversarial Anomaly Perception (AAP)\nphenomenon, which enables the diffusion model to accurately detect and locate\nadversarial patches by analyzing distributional anomalies. DIFFender seamlessly\nintegrates the tasks of patch localization and restoration within a unified\ndiffusion model framework, enhancing defense efficacy through their close\ninteraction. Additionally, DIFFender employs an efficient few-shot\nprompt-tuning algorithm, facilitating the adaptation of the pre-trained\ndiffusion model to defense tasks without the need for extensive retraining. Our\ncomprehensive evaluation, covering image classification and face recognition\ntasks, as well as real-world scenarios, demonstrates DIFFender's robust\nperformance against adversarial attacks. The framework's versatility and\ngeneralizability across various settings, classifiers, and attack methodologies\nmark a significant advancement in adversarial patch defense strategies. Except\nfor the popular visible domain, we have identified another advantage of\nDIFFender: its capability to easily expand into the infrared domain.\nConsequently, we demonstrate the good flexibility of DIFFender, which can\ndefend against both infrared and visible adversarial patch attacks\nalternatively using a universal defense framework.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09406v1",
    "published_date": "2024-09-14 10:38:35 UTC",
    "updated_date": "2024-09-14 10:38:35 UTC"
  },
  {
    "arxiv_id": "2409.09403v2",
    "title": "AI-Driven Virtual Teacher for Enhanced Educational Efficiency: Leveraging Large Pretrain Models for Autonomous Error Analysis and Correction",
    "authors": [
      "Tianlong Xu",
      "Yi-Fan Zhang",
      "Zhendong Chu",
      "Shen Wang",
      "Qingsong Wen"
    ],
    "abstract": "Students frequently make mistakes while solving mathematical problems, and\ntraditional error correction methods are both time-consuming and\nlabor-intensive. This paper introduces an innovative \\textbf{V}irtual\n\\textbf{A}I \\textbf{T}eacher system designed to autonomously analyze and\ncorrect student \\textbf{E}rrors (VATE). Leveraging advanced large language\nmodels (LLMs), the system uses student drafts as a primary source for error\nanalysis, which enhances understanding of the student's learning process. It\nincorporates sophisticated prompt engineering and maintains an error pool to\nreduce computational overhead. The AI-driven system also features a real-time\ndialogue component for efficient student interaction. Our approach demonstrates\nsignificant advantages over traditional and machine learning-based error\ncorrection methods, including reduced educational costs, high scalability, and\nsuperior generalizability. The system has been deployed on the Squirrel AI\nlearning platform for elementary mathematics education, where it achieves\n78.3\\% accuracy in error analysis and shows a marked improvement in student\nlearning efficiency. Satisfaction surveys indicate a strong positive reception,\nhighlighting the system's potential to transform educational practices.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "AAAI/IAAI 2025 Innovative Application Award",
    "pdf_url": "http://arxiv.org/pdf/2409.09403v2",
    "published_date": "2024-09-14 10:27:36 UTC",
    "updated_date": "2024-12-08 03:25:51 UTC"
  },
  {
    "arxiv_id": "2410.03533v1",
    "title": "Multiscale fusion enhanced spiking neural network for invasive BCI neural signal decoding",
    "authors": [
      "Yu Song",
      "Liyuan Han",
      "Bo Xu",
      "Tielin Zhang"
    ],
    "abstract": "Brain-computer interfaces (BCIs) are an advanced fusion of neuroscience and\nartificial intelligence, requiring stable and long-term decoding of neural\nsignals. Spiking Neural Networks (SNNs), with their neuronal dynamics and\nspike-based signal processing, are inherently well-suited for this task. This\npaper presents a novel approach utilizing a Multiscale Fusion enhanced Spiking\nNeural Network (MFSNN). The MFSNN emulates the parallel processing and\nmultiscale feature fusion seen in human visual perception to enable real-time,\nefficient, and energy-conserving neural signal decoding. Initially, the MFSNN\nemploys temporal convolutional networks and channel attention mechanisms to\nextract spatiotemporal features from raw data. It then enhances decoding\nperformance by integrating these features through skip connections.\nAdditionally, the MFSNN improves generalizability and robustness in cross-day\nsignal decoding through mini-batch supervised generalization learning. In two\nbenchmark invasive BCI paradigms, including the single-hand grasp-and-touch and\ncenter-and-out reach tasks, the MFSNN surpasses traditional artificial neural\nnetwork methods, such as MLP and GRU, in both accuracy and computational\nefficiency. Moreover, the MFSNN's multiscale feature fusion framework is\nwell-suited for the implementation on neuromorphic chips, offering an\nenergy-efficient solution for online decoding of invasive BCI signals.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.03533v1",
    "published_date": "2024-09-14 09:53:30 UTC",
    "updated_date": "2024-09-14 09:53:30 UTC"
  },
  {
    "arxiv_id": "2409.09386v2",
    "title": "AMBER -- Advanced SegFormer for Multi-Band Image Segmentation: an application to Hyperspectral Imaging",
    "authors": [
      "Andrea Dosi",
      "Massimo Brescia",
      "Stefano Cavuoti",
      "Mariarca D'Aniello",
      "Michele Delli Veneri",
      "Carlo Donadio",
      "Adriano Ettari",
      "Giuseppe Longo",
      "Alvi Rownok",
      "Luca Sannino",
      "Maria Zampella"
    ],
    "abstract": "Deep learning has revolutionized the field of hyperspectral image (HSI)\nanalysis, enabling the extraction of complex spectral and spatial features.\nWhile convolutional neural networks (CNNs) have been the backbone of HSI\nclassification, their limitations in capturing global contextual features have\nled to the exploration of Vision Transformers (ViTs). This paper introduces\nAMBER, an advanced SegFormer specifically designed for multi-band image\nsegmentation. AMBER enhances the original SegFormer by incorporating\nthree-dimensional convolutions, custom kernel sizes, and a Funnelizer layer.\nThis architecture enables processing hyperspectral data directly, without\nrequiring spectral dimensionality reduction during preprocessing. Our\nexperiments, conducted on three benchmark datasets (Salinas, Indian Pines, and\nPavia University) and on a dataset from the PRISMA satellite, show that AMBER\noutperforms traditional CNN-based methods in terms of Overall Accuracy, Kappa\ncoefficient, and Average Accuracy on the first three datasets, and achieves\nstate-of-the-art performance on the PRISMA dataset. These findings highlight\nAMBER's robustness, adaptability to both airborne and spaceborne data, and its\npotential as a powerful solution for remote sensing and other domains requiring\nadvanced analysis of high-dimensional data.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "submitted to Neural Computing & Applications (Springer). Accepted\n  with minor revisions",
    "pdf_url": "http://arxiv.org/pdf/2409.09386v2",
    "published_date": "2024-09-14 09:34:05 UTC",
    "updated_date": "2025-04-14 08:21:38 UTC"
  },
  {
    "arxiv_id": "2409.09383v2",
    "title": "LLM-Powered Ensemble Learning for Paper Source Tracing: A GPU-Free Approach",
    "authors": [
      "Kunlong Chen",
      "Junjun Wang",
      "Zhaoqun Chen",
      "Kunjin Chen",
      "Yitian Chen"
    ],
    "abstract": "We participated in the KDD CUP 2024 paper source tracing competition and\nachieved the 3rd place. This competition tasked participants with identifying\nthe reference sources (i.e., ref-sources, as referred to by the organizers of\nthe competition) of given academic papers. Unlike most teams that addressed\nthis challenge by fine-tuning pre-trained neural language models such as BERT\nor ChatGLM, our primary approach utilized closed-source large language models\n(LLMs). With recent advancements in LLM technology, closed-source LLMs have\ndemonstrated the capability to tackle complex reasoning tasks in zero-shot or\nfew-shot scenarios. Consequently, in the absence of GPUs, we employed\nclosed-source LLMs to directly generate predicted reference sources from the\nprovided papers. We further refined these predictions through ensemble\nlearning. Notably, our method was the only one among the award-winning\napproaches that did not require the use of GPUs for model training. Code\navailable at https://github.com/Cklwanfifa/KDDCUP2024-PST.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09383v2",
    "published_date": "2024-09-14 09:21:46 UTC",
    "updated_date": "2024-09-17 01:35:25 UTC"
  },
  {
    "arxiv_id": "2409.09381v1",
    "title": "Text Prompt is Not Enough: Sound Event Enhanced Prompt Adapter for Target Style Audio Generation",
    "authors": [
      "Chenxu Xiong",
      "Ruibo Fu",
      "Shuchen Shi",
      "Zhengqi Wen",
      "Jianhua Tao",
      "Tao Wang",
      "Chenxing Li",
      "Chunyu Qiang",
      "Yuankun Xie",
      "Xin Qi",
      "Guanjun Li",
      "Zizheng Yang"
    ],
    "abstract": "Current mainstream audio generation methods primarily rely on simple text\nprompts, often failing to capture the nuanced details necessary for multi-style\naudio generation. To address this limitation, the Sound Event Enhanced Prompt\nAdapter is proposed. Unlike traditional static global style transfer, this\nmethod extracts style embedding through cross-attention between text and\nreference audio for adaptive style control. Adaptive layer normalization is\nthen utilized to enhance the model's capacity to express multiple styles.\nAdditionally, the Sound Event Reference Style Transfer Dataset (SERST) is\nintroduced for the proposed target style audio generation task, enabling\ndual-prompt audio generation using both text and audio references. Experimental\nresults demonstrate the robustness of the model, achieving state-of-the-art\nFr\\'echet Distance of 26.94 and KL Divergence of 1.82, surpassing Tango,\nAudioLDM, and AudioGen. Furthermore, the generated audio shows high similarity\nto its corresponding audio reference. The demo, code, and dataset are publicly\navailable.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "5 pages, 2 figures, submitted to ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.09381v1",
    "published_date": "2024-09-14 09:16:38 UTC",
    "updated_date": "2024-09-14 09:16:38 UTC"
  },
  {
    "arxiv_id": "2409.09378v1",
    "title": "Prevailing Research Areas for Music AI in the Era of Foundation Models",
    "authors": [
      "Megan Wei",
      "Mateusz Modrzejewski",
      "Aswin Sivaraman",
      "Dorien Herremans"
    ],
    "abstract": "In tandem with the recent advancements in foundation model research, there\nhas been a surge of generative music AI applications within the past few years.\nAs the idea of AI-generated or AI-augmented music becomes more mainstream, many\nresearchers in the music AI community may be wondering what avenues of research\nare left. With regards to music generative models, we outline the current areas\nof research with significant room for exploration. Firstly, we pose the\nquestion of foundational representation of these generative models and\ninvestigate approaches towards explainability. Next, we discuss the current\nstate of music datasets and their limitations. We then overview different\ngenerative models, forms of evaluating these models, and their computational\nconstraints/limitations. Subsequently, we highlight applications of these\ngenerative models towards extensions to multiple modalities and integration\nwith artists' workflow as well as music education systems. Finally, we survey\nthe potential copyright implications of generative music and discuss strategies\nfor protecting the rights of musicians. While it is not meant to be exhaustive,\nour survey calls to attention a variety of research directions enabled by music\nfoundation models.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.MM",
      "eess.AS",
      "68T05, 68T20",
      "I.2; I.5.4; I.2.6; I.2.7; H.5.5"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09378v1",
    "published_date": "2024-09-14 09:06:43 UTC",
    "updated_date": "2024-09-14 09:06:43 UTC"
  },
  {
    "arxiv_id": "2409.17167v2",
    "title": "StressPrompt: Does Stress Impact Large Language Models and Human Performance Similarly?",
    "authors": [
      "Guobin Shen",
      "Dongcheng Zhao",
      "Aorigele Bao",
      "Xiang He",
      "Yiting Dong",
      "Yi Zeng"
    ],
    "abstract": "Human beings often experience stress, which can significantly influence their\nperformance. This study explores whether Large Language Models (LLMs) exhibit\nstress responses similar to those of humans and whether their performance\nfluctuates under different stress-inducing prompts. To investigate this, we\ndeveloped a novel set of prompts, termed StressPrompt, designed to induce\nvarying levels of stress. These prompts were derived from established\npsychological frameworks and carefully calibrated based on ratings from human\nparticipants. We then applied these prompts to several LLMs to assess their\nresponses across a range of tasks, including instruction-following, complex\nreasoning, and emotional intelligence. The findings suggest that LLMs, like\nhumans, perform optimally under moderate stress, consistent with the\nYerkes-Dodson law. Notably, their performance declines under both low and\nhigh-stress conditions. Our analysis further revealed that these StressPrompts\nsignificantly alter the internal states of LLMs, leading to changes in their\nneural representations that mirror human responses to stress. This research\nprovides critical insights into the operational robustness and flexibility of\nLLMs, demonstrating the importance of designing AI systems capable of\nmaintaining high performance in real-world scenarios where stress is prevalent,\nsuch as in customer service, healthcare, and emergency response contexts.\nMoreover, this study contributes to the broader AI research community by\noffering a new perspective on how LLMs handle different scenarios and their\nsimilarities to human cognition.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "11 pages, 9 figures, Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.17167v2",
    "published_date": "2024-09-14 08:32:31 UTC",
    "updated_date": "2025-01-28 01:57:14 UTC"
  },
  {
    "arxiv_id": "2409.09360v3",
    "title": "LACOSTE: Exploiting stereo and temporal contexts for surgical instrument segmentation",
    "authors": [
      "Qiyuan Wang",
      "Shang Zhao",
      "Zikang Xu",
      "S Kevin Zhou"
    ],
    "abstract": "Surgical instrument segmentation is instrumental to minimally invasive\nsurgeries and related applications. Most previous methods formulate this task\nas single-frame-based instance segmentation while ignoring the natural temporal\nand stereo attributes of a surgical video. As a result, these methods are less\nrobust against the appearance variation through temporal motion and view\nchange. In this work, we propose a novel LACOSTE model that exploits\nLocation-Agnostic COntexts in Stereo and TEmporal images for improved surgical\ninstrument segmentation. Leveraging a query-based segmentation model as core,\nwe design three performance-enhancing modules. Firstly, we design a\ndisparity-guided feature propagation module to enhance depth-aware features\nexplicitly. To generalize well for even only a monocular video, we apply a\npseudo stereo scheme to generate complementary right images. Secondly, we\npropose a stereo-temporal set classifier, which aggregates stereo-temporal\ncontexts in a universal way for making a consolidated prediction and mitigates\ntransient failures. Finally, we propose a location-agnostic classifier to\ndecouple the location bias from mask prediction and enhance the feature\nsemantics. We extensively validate our approach on three public surgical video\ndatasets, including two benchmarks from EndoVis Challenges and one real radical\nprostatectomy surgery dataset GraSP. Experimental results demonstrate the\npromising performances of our method, which consistently achieves comparable or\nfavorable results with previous state-of-the-art approaches.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint submitted to Medical Image Analysis",
    "pdf_url": "http://arxiv.org/pdf/2409.09360v3",
    "published_date": "2024-09-14 08:17:56 UTC",
    "updated_date": "2024-10-08 13:13:41 UTC"
  },
  {
    "arxiv_id": "2409.09359v3",
    "title": "Symbolic Regression with a Learned Concept Library",
    "authors": [
      "Arya Grayeli",
      "Atharva Sehgal",
      "Omar Costilla-Reyes",
      "Miles Cranmer",
      "Swarat Chaudhuri"
    ],
    "abstract": "We present a novel method for symbolic regression (SR), the task of searching\nfor compact programmatic hypotheses that best explain a dataset. The problem is\ncommonly solved using genetic algorithms; we show that we can enhance such\nmethods by inducing a library of abstract textual concepts. Our algorithm,\ncalled LaSR, uses zero-shot queries to a large language model (LLM) to discover\nand evolve concepts occurring in known high-performing hypotheses. We discover\nnew hypotheses using a mix of standard evolutionary steps and LLM-guided steps\n(obtained through zero-shot LLM queries) conditioned on discovered concepts.\nOnce discovered, hypotheses are used in a new round of concept abstraction and\nevolution. We validate LaSR on the Feynman equations, a popular SR benchmark,\nas well as a set of synthetic tasks. On these benchmarks, LaSR substantially\noutperforms a variety of state-of-the-art SR approaches based on deep learning\nand evolutionary algorithms. Moreover, we show that LaSR can be used to\ndiscover a novel and powerful scaling law for LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "cs.SC"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS version; 10 pages; no checklist; added more experiment\n  details",
    "pdf_url": "http://arxiv.org/pdf/2409.09359v3",
    "published_date": "2024-09-14 08:17:30 UTC",
    "updated_date": "2024-12-10 16:24:48 UTC"
  },
  {
    "arxiv_id": "2409.09357v1",
    "title": "Joint Semantic Knowledge Distillation and Masked Acoustic Modeling for Full-band Speech Restoration with Improved Intelligibility",
    "authors": [
      "Xiaoyu Liu",
      "Xu Li",
      "Joan Serrà",
      "Santiago Pascual"
    ],
    "abstract": "Speech restoration aims at restoring full-band speech with high quality and\nintelligibility, considering a diverse set of distortions. MaskSR is a recently\nproposed generative model for this task. As other models of its kind, MaskSR\nattains high quality but, as we show, intelligibility can be substantially\nimproved. We do so by boosting the speech encoder component of MaskSR with\npredictions of semantic representations of the target speech, using a\npre-trained self-supervised teacher model. Then, a masked language model is\nconditioned on the learned semantic features to predict acoustic tokens that\nencode low level spectral details of the target speech. We show that, with the\nsame MaskSR model capacity and inference time, the proposed model, MaskSR2,\nsignificantly reduces the word error rate, a typical metric for\nintelligibility. MaskSR2 also achieves competitive word error rate among other\nmodels, while providing superior quality. An ablation study shows the\neffectiveness of various semantic representations.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS",
      "eess.SP"
    ],
    "primary_category": "cs.SD",
    "comment": "Demo link https://masksr.github.io/MaskSR2/",
    "pdf_url": "http://arxiv.org/pdf/2409.09357v1",
    "published_date": "2024-09-14 08:09:55 UTC",
    "updated_date": "2024-09-14 08:09:55 UTC"
  },
  {
    "arxiv_id": "2409.10570v2",
    "title": "Protecting Copyright of Medical Pre-trained Language Models: Training-Free Backdoor Model Watermarking",
    "authors": [
      "Cong Kong",
      "Rui Xu",
      "Weixi Chen",
      "Jiawei Chen",
      "Zhaoxia Yin"
    ],
    "abstract": "With the advancement of intelligent healthcare, medical pre-trained language\nmodels (Med-PLMs) have emerged and demonstrated significant effectiveness in\ndownstream medical tasks. While these models are valuable assets, they are\nvulnerable to misuse and theft, requiring copyright protection. However,\nexisting watermarking methods for pre-trained language models (PLMs) cannot be\ndirectly applied to Med-PLMs due to domain-task mismatch and inefficient\nwatermark embedding. To fill this gap, we propose the first training-free\nbackdoor model watermarking for Med-PLMs. Our method employs low-frequency\nwords as triggers, embedding the watermark by replacing their embeddings in the\nmodel's word embedding layer with those of specific medical terms. The\nwatermarked Med-PLMs produce the same output for triggers as for the\ncorresponding specified medical terms. We leverage this unique mapping to\ndesign tailored watermark extraction schemes for different downstream tasks,\nthereby addressing the challenge of domain-task mismatch in previous methods.\nExperiments demonstrate superior effectiveness of our watermarking method\nacross medical downstream tasks. Moreover, the method exhibits robustness\nagainst model extraction, pruning, fusion-based backdoor removal attacks, while\nmaintaining high efficiency with 10-second watermark embedding.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.10570v2",
    "published_date": "2024-09-14 08:08:55 UTC",
    "updated_date": "2025-04-15 08:07:58 UTC"
  },
  {
    "arxiv_id": "2409.09354v1",
    "title": "PeriGuru: A Peripheral Robotic Mobile App Operation Assistant based on GUI Image Understanding and Prompting with LLM",
    "authors": [
      "Kelin Fu",
      "Yang Tian",
      "Kaigui Bian"
    ],
    "abstract": "Smartphones have significantly enhanced our daily learning, communication,\nand entertainment, becoming an essential component of modern life. However,\ncertain populations, including the elderly and individuals with disabilities,\nencounter challenges in utilizing smartphones, thus necessitating mobile app\noperation assistants, a.k.a. mobile app agent. With considerations for privacy,\npermissions, and cross-platform compatibility issues, we endeavor to devise and\ndevelop PeriGuru in this work, a peripheral robotic mobile app operation\nassistant based on GUI image understanding and prompting with Large Language\nModel (LLM). PeriGuru leverages a suite of computer vision techniques to\nanalyze GUI screenshot images and employs LLM to inform action decisions, which\nare then executed by robotic arms. PeriGuru achieves a success rate of 81.94%\non the test task set, which surpasses by more than double the method without\nPeriGuru's GUI image interpreting and prompting design. Our code is available\non https://github.com/Z2sJ4t/PeriGuru.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09354v1",
    "published_date": "2024-09-14 07:54:25 UTC",
    "updated_date": "2024-09-14 07:54:25 UTC"
  },
  {
    "arxiv_id": "2409.09345v1",
    "title": "Enhancing Decision-Making for LLM Agents via Step-Level Q-Value Models",
    "authors": [
      "Yuanzhao Zhai",
      "Tingkai Yang",
      "Kele Xu",
      "Feng Dawei",
      "Cheng Yang",
      "Bo Ding",
      "Huaimin Wang"
    ],
    "abstract": "Agents significantly enhance the capabilities of standalone Large Language\nModels (LLMs) by perceiving environments, making decisions, and executing\nactions. However, LLM agents still face challenges in tasks that require\nmultiple decision-making steps. Estimating the value of actions in specific\ntasks is difficult when intermediate actions are neither appropriately rewarded\nnor penalized. In this paper, we propose leveraging a task-relevant Q-value\nmodel to guide action selection. Specifically, we first collect decision-making\ntrajectories annotated with step-level Q values via Monte Carlo Tree Search\n(MCTS) and construct preference data. We then use another LLM to fit these\npreferences through step-level Direct Policy Optimization (DPO), which serves\nas the Q-value model. During inference, at each decision-making step, LLM\nagents select the action with the highest Q value before interacting with the\nenvironment. We apply our method to various open-source and API-based LLM\nagents, demonstrating that Q-value models significantly improve their\nperformance. Notably, the performance of the agent built with\nPhi-3-mini-4k-instruct improved by 103% on WebShop and 75% on HotPotQA when\nenhanced with Q-value models, even surpassing GPT-4o-mini. Additionally,\nQ-value models offer several advantages, such as generalization to different\nLLM agents and seamless integration with existing prompting strategies.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09345v1",
    "published_date": "2024-09-14 07:32:49 UTC",
    "updated_date": "2024-09-14 07:32:49 UTC"
  },
  {
    "arxiv_id": "2409.09340v1",
    "title": "Egocentric Speaker Classification in Child-Adult Dyadic Interactions: From Sensing to Computational Modeling",
    "authors": [
      "Tiantian Feng",
      "Anfeng Xu",
      "Xuan Shi",
      "Somer Bishop",
      "Shrikanth Narayanan"
    ],
    "abstract": "Autism spectrum disorder (ASD) is a neurodevelopmental condition\ncharacterized by challenges in social communication, repetitive behavior, and\nsensory processing. One important research area in ASD is evaluating children's\nbehavioral changes over time during treatment. The standard protocol with this\nobjective is BOSCC, which involves dyadic interactions between a child and\nclinicians performing a pre-defined set of activities. A fundamental aspect of\nunderstanding children's behavior in these interactions is automatic speech\nunderstanding, particularly identifying who speaks and when. Conventional\napproaches in this area heavily rely on speech samples recorded from a\nspectator perspective, and there is limited research on egocentric speech\nmodeling. In this study, we design an experiment to perform speech sampling in\nBOSCC interviews from an egocentric perspective using wearable sensors and\nexplore pre-training Ego4D speech samples to enhance child-adult speaker\nclassification in dyadic interactions. Our findings highlight the potential of\negocentric speech collection and pre-training to improve speaker classification\naccuracy.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "pre-print under review",
    "pdf_url": "http://arxiv.org/pdf/2409.09340v1",
    "published_date": "2024-09-14 07:03:08 UTC",
    "updated_date": "2024-09-14 07:03:08 UTC"
  },
  {
    "arxiv_id": "2409.09337v3",
    "title": "Wave-U-Mamba: An End-To-End Framework For High-Quality And Efficient Speech Super Resolution",
    "authors": [
      "Yongjoon Lee",
      "Chanwoo Kim"
    ],
    "abstract": "Speech Super-Resolution (SSR) is a task of enhancing low-resolution speech\nsignals by restoring missing high-frequency components. Conventional approaches\ntypically reconstruct log-mel features, followed by a vocoder that generates\nhigh-resolution speech in the waveform domain. However, as mel features lack\nphase information, this can result in performance degradation during the\nreconstruction phase. Motivated by recent advances with Selective State Spaces\nModels (SSMs), we propose a method, referred to as Wave-U-Mamba that directly\nperforms SSR in time domain. In our comparative study, including models such as\nWSRGlow, NU-Wave 2, and AudioSR, Wave-U-Mamba demonstrates superior\nperformance, achieving the lowest Log-Spectral Distance (LSD) across various\nlow-resolution sampling rates, ranging from 8 to 24 kHz. Additionally,\nsubjective human evaluations, scored using Mean Opinion Score (MOS) reveal that\nour method produces SSR with natural and human-like quality. Furthermore,\nWave-U-Mamba achieves these results while generating high-resolution speech\nover nine times faster than baseline models on a single A100 GPU, with\nparameter sizes less than 2\\% of those in the baseline models.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted to ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.09337v3",
    "published_date": "2024-09-14 06:52:00 UTC",
    "updated_date": "2025-02-03 12:07:17 UTC"
  },
  {
    "arxiv_id": "2409.09324v2",
    "title": "Efficient Fine-Tuning of Large Language Models for Automated Medical Documentation",
    "authors": [
      "Hui Yi Leong",
      "Yi Fan Gao",
      "Ji Shuai",
      "Yang Zhang",
      "Uktu Pamuksuz"
    ],
    "abstract": "Scientific research indicates that for every hour spent in direct patient\ncare, physicians spend nearly two additional hours on administrative tasks,\nparticularly on electronic health records (EHRs) and desk work. This excessive\nadministrative burden not only reduces the time available for patient care but\nalso contributes to physician burnout and inefficiencies in healthcare\ndelivery. To address these challenges, this study introduces MediGen, a\nfine-tuned large language model (LLM) designed to automate the generation of\nmedical reports from medical dialogues. By leveraging state-of-the-art\nmethodologies for fine-tuning open-source pretrained models, including\nLLaMA3-8B, MediGen achieves high accuracy in transcribing and summarizing\nclinical interactions. The fine-tuned LLaMA3-8B model demonstrated promising\nresults, achieving a ROUGE score of 58% and a BERTScore-F1 of 72%, indicating\nits effectiveness in generating accurate and clinically relevant medical\nreports. These findings suggest that MediGen has the potential to significantly\nreduce the administrative workload on physicians, improving both healthcare\nefficiency and physician well-being.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "4 pages, 3 Figures, 3 Tables. The final version will be published in\n  the proceedings of the IEEE conference",
    "pdf_url": "http://arxiv.org/pdf/2409.09324v2",
    "published_date": "2024-09-14 06:02:17 UTC",
    "updated_date": "2024-09-28 02:16:19 UTC"
  },
  {
    "arxiv_id": "2409.18980v1",
    "title": "IW-Bench: Evaluating Large Multimodal Models for Converting Image-to-Web",
    "authors": [
      "Hongcheng Guo",
      "Wei Zhang",
      "Junhao Chen",
      "Yaonan Gu",
      "Jian Yang",
      "Junjia Du",
      "Binyuan Hui",
      "Tianyu Liu",
      "Jianxin Ma",
      "Chang Zhou",
      "Zhoujun Li"
    ],
    "abstract": "Recently advancements in large multimodal models have led to significant\nstrides in image comprehension capabilities. Despite these advancements, there\nis a lack of the robust benchmark specifically for assessing the Image-to-Web\nconversion proficiency of these large models. Primarily, it is essential to\nensure the integrity of the web elements generated. These elements comprise\nvisible and invisible categories. Previous evaluation methods (e.g., BLEU) are\nnotably susceptible to significant alterations due to the presence of invisible\nelements in Web. Furthermore, it is crucial to measure the layout information\nof web pages, referring to the positional relationships between elements, which\nis overlooked by previous work. To address challenges, we have curated and\naligned a benchmark of images and corresponding web codes (IW-Bench).\nSpecifically, we propose the Element Accuracy, which tests the completeness of\nthe elements by parsing the Document Object Model (DOM) tree. Layout Accuracy\nis also proposed to analyze the positional relationships of elements by\nconverting DOM tree into a common subsequence. Besides, we design a five-hop\nmultimodal Chain-of-Thought Prompting for better performance, which contains\nfive hop: 1) SoM prompt injection. 2) Inferring Elements. 3) Inferring Layout.\n4) Inferring Web code. 5) Reflection. Our benchmark comprises 1200 pairs of\nimages and web codes with varying levels of difficulty. We have conducted\nextensive experiments on existing large multimodal models, offering insights\ninto their performance and areas for improvement in image-to-web domain.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18980v1",
    "published_date": "2024-09-14 05:38:26 UTC",
    "updated_date": "2024-09-14 05:38:26 UTC"
  },
  {
    "arxiv_id": "2409.16312v1",
    "title": "SEE: Semantically Aligned EEG-to-Text Translation",
    "authors": [
      "Yitian Tao",
      "Yan Liang",
      "Luoyu Wang",
      "Yongqing Li",
      "Qing Yang",
      "Han Zhang"
    ],
    "abstract": "Decoding neurophysiological signals into language is of great research\ninterest within brain-computer interface (BCI) applications.\nElectroencephalography (EEG), known for its non-invasiveness, ease of use, and\ncost-effectiveness, has been a popular method in this field. However, current\nEEG-to-Text decoding approaches face challenges due to the huge domain gap\nbetween EEG recordings and raw texts, inherent data bias, and small closed\nvocabularies. In this paper, we propose SEE: Semantically Aligned EEG-to-Text\nTranslation, a novel method aimed at improving EEG-to-Text decoding by\nseamlessly integrating two modules into a pre-trained BART language model.\nThese two modules include (1) a Cross-Modal Codebook that learns cross-modal\nrepresentations to enhance feature consolidation and mitigate domain gap, and\n(2) a Semantic Matching Module that fully utilizes pre-trained text\nrepresentations to align multi-modal features extracted from EEG-Text pairs\nwhile considering noise caused by false negatives, i.e., data from different\nEEG-Text pairs that have similar semantic meanings. Experimental results on the\nZurich Cognitive Language Processing Corpus (ZuCo) demonstrate the\neffectiveness of SEE, which enhances the feasibility of accurate EEG-to-Text\ndecoding.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "q-bio.QM",
    "comment": "4 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.16312v1",
    "published_date": "2024-09-14 05:37:15 UTC",
    "updated_date": "2024-09-14 05:37:15 UTC"
  },
  {
    "arxiv_id": "2409.09305v1",
    "title": "The T05 System for The VoiceMOS Challenge 2024: Transfer Learning from Deep Image Classifier to Naturalness MOS Prediction of High-Quality Synthetic Speech",
    "authors": [
      "Kaito Baba",
      "Wataru Nakata",
      "Yuki Saito",
      "Hiroshi Saruwatari"
    ],
    "abstract": "We present our system (denoted as T05) for the VoiceMOS Challenge (VMC) 2024.\nOur system was designed for the VMC 2024 Track 1, which focused on the accurate\nprediction of naturalness mean opinion score (MOS) for high-quality synthetic\nspeech. In addition to a pretrained self-supervised learning (SSL)-based speech\nfeature extractor, our system incorporates a pretrained image feature extractor\nto capture the difference of synthetic speech observed in speech spectrograms.\nWe first separately train two MOS predictors that use either of an SSL-based or\nspectrogram-based feature. Then, we fine-tune the two predictors for better MOS\nprediction using the fusion of two extracted features. In the VMC 2024 Track 1,\nour T05 system achieved first place in 7 out of 16 evaluation metrics and\nsecond place in the remaining 9 metrics, with a significant difference compared\nto those ranked third and below. We also report the results of our ablation\nstudy to investigate essential factors of our system.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by IEEE SLT 2024. Our MOS prediction system (UTMOSv2) is\n  available in https://github.com/sarulab-speech/UTMOSv2",
    "pdf_url": "http://arxiv.org/pdf/2409.09305v1",
    "published_date": "2024-09-14 05:03:18 UTC",
    "updated_date": "2024-09-14 05:03:18 UTC"
  },
  {
    "arxiv_id": "2409.09298v1",
    "title": "Matrix Profile for Anomaly Detection on Multidimensional Time Series",
    "authors": [
      "Chin-Chia Michael Yeh",
      "Audrey Der",
      "Uday Singh Saini",
      "Vivian Lai",
      "Yan Zheng",
      "Junpeng Wang",
      "Xin Dai",
      "Zhongfang Zhuang",
      "Yujie Fan",
      "Huiyuan Chen",
      "Prince Osei Aboagye",
      "Liang Wang",
      "Wei Zhang",
      "Eamonn Keogh"
    ],
    "abstract": "The Matrix Profile (MP), a versatile tool for time series data mining, has\nbeen shown effective in time series anomaly detection (TSAD). This paper delves\ninto the problem of anomaly detection in multidimensional time series, a common\noccurrence in real-world applications. For instance, in a manufacturing\nfactory, multiple sensors installed across the site collect time-varying data\nfor analysis. The Matrix Profile, named for its role in profiling the matrix\nstoring pairwise distance between subsequences of univariate time series,\nbecomes complex in multidimensional scenarios. If the input univariate time\nseries has n subsequences, the pairwise distance matrix is a n x n matrix. In a\nmultidimensional time series with d dimensions, the pairwise distance\ninformation must be stored in a n x n x d tensor. In this paper, we first\nanalyze different strategies for condensing this tensor into a profile vector.\nWe then investigate the potential of extending the MP to efficiently find\nk-nearest neighbors for anomaly detection. Finally, we benchmark the\nmultidimensional MP against 19 baseline methods on 119 multidimensional TSAD\ndatasets. The experiments covers three learning setups: unsupervised,\nsupervised, and semi-supervised. MP is the only method that consistently\ndelivers high performance across all setups.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09298v1",
    "published_date": "2024-09-14 04:22:45 UTC",
    "updated_date": "2024-09-14 04:22:45 UTC"
  },
  {
    "arxiv_id": "2409.10568v3",
    "title": "On the limits of agency in agent-based models",
    "authors": [
      "Ayush Chopra",
      "Shashank Kumar",
      "Nurullah Giray-Kuru",
      "Ramesh Raskar",
      "Arnau Quera-Bofarull"
    ],
    "abstract": "Agent-based modeling (ABM) offers powerful insights into complex systems, but\nits practical utility has been limited by computational constraints and\nsimplistic agent behaviors, especially when simulating large populations.\nRecent advancements in large language models (LLMs) could enhance ABMs with\nadaptive agents, but their integration into large-scale simulations remains\nchallenging. This work introduces a novel methodology that bridges this gap by\nefficiently integrating LLMs into ABMs, enabling the simulation of millions of\nadaptive agents. We present LLM archetypes, a technique that balances\nbehavioral complexity with computational efficiency, allowing for nuanced agent\nbehavior in large-scale simulations. Our analysis explores the crucial\ntrade-off between simulation scale and individual agent expressiveness,\ncomparing different agent architectures ranging from simple heuristic-based\nagents to fully adaptive LLM-powered agents. We demonstrate the real-world\napplicability of our approach through a case study of the COVID-19 pandemic,\nsimulating 8.4 million agents representing New York City and capturing the\nintricate interplay between health behaviors and economic outcomes. Our method\nsignificantly enhances ABM capabilities for predictive and counterfactual\nanalyses, addressing limitations of historical data in policy design. By\nimplementing these advances in an open-source framework, we facilitate the\nadoption of LLM archetypes across diverse ABM applications. Our results show\nthat LLM archetypes can markedly improve the realism and utility of large-scale\nABMs while maintaining computational feasibility, opening new avenues for\nmodeling complex societal challenges and informing data-driven policy\ndecisions.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10568v3",
    "published_date": "2024-09-14 04:17:24 UTC",
    "updated_date": "2024-11-10 21:31:43 UTC"
  },
  {
    "arxiv_id": "2409.09281v2",
    "title": "Language Models \"Grok\" to Copy",
    "authors": [
      "Ang Lv",
      "Ruobing Xie",
      "Xingwu Sun",
      "Zhanhui Kang",
      "Rui Yan"
    ],
    "abstract": "We examine the pre-training dynamics of language models, focusing on their\nability to copy text from preceding context--a fundamental skill for various\nLLM applications, including in-context learning (ICL) and retrieval-augmented\ngeneration (RAG). We propose a novel perspective that Transformer-based\nlanguage models develop copying abilities similarly to grokking, which refers\nto sudden generalization on test set long after the model fit to the training\nset. Our experiments yield three arguments: (1) The pre-training loss decreases\nrapidly, while the context copying ability of models initially lags and then\nabruptly saturates. (2) The speed of developing copying ability is independent\nof the number of tokens trained, similarly to how grokking speed is unaffected\nby dataset size as long as the data distribution is preserved. (3) Induction\nheads, the attention heads responsible for copying, form from shallow to deep\nlayers during training, mirroring the development of circuits in deeper layers\nduring grokking. We contend that the connection between grokking and context\ncopying can provide valuable insights for more effective language model\ntraining, ultimately improving in-context performance. For example, we\ndemonstrated that techniques that enhance grokking, such as regularization,\neither accelerate or enhance the development of context copying.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2025 main conference, short paper",
    "pdf_url": "http://arxiv.org/pdf/2409.09281v2",
    "published_date": "2024-09-14 03:11:00 UTC",
    "updated_date": "2025-02-06 03:31:50 UTC"
  },
  {
    "arxiv_id": "2409.09280v1",
    "title": "An empirical evaluation of using ChatGPT to summarize disputes for recommending similar labor and employment cases in Chinese",
    "authors": [
      "Po-Hsien Wu",
      "Chao-Lin Liu",
      "Wei-Jie Li"
    ],
    "abstract": "We present a hybrid mechanism for recommending similar cases of labor and\nemployment litigations. The classifier determines the similarity based on the\nitemized disputes of the two cases, that the courts prepared. We cluster the\ndisputes, compute the cosine similarity between the disputes, and use the\nresults as the features for the classification tasks. Experimental results\nindicate that this hybrid approach outperformed our previous system, which\nconsidered only the information about the clusters of the disputes. We replaced\nthe disputes that were prepared by the courts with the itemized disputes that\nwere generated by GPT-3.5 and GPT-4, and repeated the same experiments. Using\nthe disputes generated by GPT-4 led to better results. Although our classifier\ndid not perform as well when using the disputes that the ChatGPT generated, the\nresults were satisfactory. Hence, we hope that the future large-language models\nwill become practically useful.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, 5 figures, 2 tables, the 18th Int'l Workshop on\n  Juris-Informatics (JURISIN 2024), associated with the 16th JSAI International\n  Symposium on AI (JSAI-isAI 2024)",
    "pdf_url": "http://arxiv.org/pdf/2409.09280v1",
    "published_date": "2024-09-14 03:08:10 UTC",
    "updated_date": "2024-09-14 03:08:10 UTC"
  },
  {
    "arxiv_id": "2409.09274v1",
    "title": "LabellessFace: Fair Metric Learning for Face Recognition without Attribute Labels",
    "authors": [
      "Tetsushi Ohki",
      "Yuya Sato",
      "Masakatsu Nishigaki",
      "Koichi Ito"
    ],
    "abstract": "Demographic bias is one of the major challenges for face recognition systems.\nThe majority of existing studies on demographic biases are heavily dependent on\nspecific demographic groups or demographic classifier, making it difficult to\naddress performance for unrecognised groups. This paper introduces\n``LabellessFace'', a novel framework that improves demographic bias in face\nrecognition without requiring demographic group labeling typically required for\nfairness considerations. We propose a novel fairness enhancement metric called\nthe class favoritism level, which assesses the extent of favoritism towards\nspecific classes across the dataset. Leveraging this metric, we introduce the\nfair class margin penalty, an extension of existing margin-based metric\nlearning. This method dynamically adjusts learning parameters based on class\nfavoritism levels, promoting fairness across all attributes. By treating each\nclass as an individual in facial recognition systems, we facilitate learning\nthat minimizes biases in authentication accuracy among individuals.\nComprehensive experiments have demonstrated that our proposed method is\neffective for enhancing fairness while maintaining authentication accuracy.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09274v1",
    "published_date": "2024-09-14 02:56:07 UTC",
    "updated_date": "2024-09-14 02:56:07 UTC"
  },
  {
    "arxiv_id": "2409.09272v1",
    "title": "SafeEar: Content Privacy-Preserving Audio Deepfake Detection",
    "authors": [
      "Xinfeng Li",
      "Kai Li",
      "Yifan Zheng",
      "Chen Yan",
      "Xiaoyu Ji",
      "Wenyuan Xu"
    ],
    "abstract": "Text-to-Speech (TTS) and Voice Conversion (VC) models have exhibited\nremarkable performance in generating realistic and natural audio. However,\ntheir dark side, audio deepfake poses a significant threat to both society and\nindividuals. Existing countermeasures largely focus on determining the\ngenuineness of speech based on complete original audio recordings, which\nhowever often contain private content. This oversight may refrain deepfake\ndetection from many applications, particularly in scenarios involving sensitive\ninformation like business secrets. In this paper, we propose SafeEar, a novel\nframework that aims to detect deepfake audios without relying on accessing the\nspeech content within. Our key idea is to devise a neural audio codec into a\nnovel decoupling model that well separates the semantic and acoustic\ninformation from audio samples, and only use the acoustic information (e.g.,\nprosody and timbre) for deepfake detection. In this way, no semantic content\nwill be exposed to the detector. To overcome the challenge of identifying\ndiverse deepfake audio without semantic clues, we enhance our deepfake detector\nwith real-world codec augmentation. Extensive experiments conducted on four\nbenchmark datasets demonstrate SafeEar's effectiveness in detecting various\ndeepfake techniques with an equal error rate (EER) down to 2.02%.\nSimultaneously, it shields five-language speech content from being deciphered\nby both machine and human auditory analysis, demonstrated by word error rates\n(WERs) all above 93.93% and our user study. Furthermore, our benchmark\nconstructed for anti-deepfake and anti-content recovery evaluation helps\nprovide a basis for future research in the realms of audio privacy preservation\nand deepfake detection.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted by ACM CCS 2024. Please cite this paper as \"Xinfeng Li, Kai\n  Li, Yifan Zheng, Chen Yan, Xiaoyu Ji, Wenyuan Xu. SafeEar: Content\n  Privacy-Preserving Audio Deepfake Detection. In Proceedings of ACM Conference\n  on Computer and Communications Security (CCS), 2024.\"",
    "pdf_url": "http://arxiv.org/pdf/2409.09272v1",
    "published_date": "2024-09-14 02:45:09 UTC",
    "updated_date": "2024-09-14 02:45:09 UTC"
  },
  {
    "arxiv_id": "2409.13752v1",
    "title": "Thinking Before Speaking: A Role-playing Model with Mindset",
    "authors": [
      "Baohua Zhang",
      "Yongyi Huang",
      "Wenyao Cui",
      "Huaping Zhang"
    ],
    "abstract": "Role-playing is an easy task for Large Language Models (LLMs), as they are\nskilled at simulating human behaviors. Many current studies have enabled LLMs\nto generate responses in the tone of a specific role by fine-tuning the models\nor using specialized prompts. However, it is typically easy to recognize when a\nrole is being played by LLMs. These models tend to perform poorly when\nconfronted with knowledge that the assumed role does not possess, or a question\nthat requires the specific experience or logic of the role to answer. To\naddress this problem and make LLMs act more like real roles, we propose a\nThinking Before Speaking (TBS) model in this paper. Unlike other studies, we\nfirst extend the data based on the character's real-life scenarios and the\nhistorical dialogue, supplementing each pair of dialogue with the character's\nmindset. Then we add few data points that include elements beyond the role's\nknowledge, and fine-tune the LLMs. This approach can help LLMs adopt the role's\nthought process and logic, avoiding responses that fall outside the role's\nknowledge base. We have also prepared a dataset and evaluation metrics to test\nthese capabilities. Experimental results show that our TBS model can better\nemulate a role in terms of tone, knowledge, and mindset.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13752v1",
    "published_date": "2024-09-14 02:41:48 UTC",
    "updated_date": "2024-09-14 02:41:48 UTC"
  },
  {
    "arxiv_id": "2410.01812v5",
    "title": "From Text to Multimodality: Exploring the Evolution and Impact of Large Language Models in Medical Practice",
    "authors": [
      "Qian Niu",
      "Keyu Chen",
      "Ming Li",
      "Pohsun Feng",
      "Ziqian Bi",
      "Lawrence KQ Yan",
      "Yichao Zhang",
      "Caitlyn Heqi Yin",
      "Cheng Fei",
      "Junyu Liu",
      "Benji Peng",
      "Tianyang Wang",
      "Yunze Wang",
      "Silin Chen",
      "Ming Liu"
    ],
    "abstract": "Large Language Models (LLMs) have rapidly evolved from text-based systems to\nmultimodal platforms, significantly impacting various sectors including\nhealthcare. This comprehensive review explores the progression of LLMs to\nMultimodal Large Language Models (MLLMs) and their growing influence in medical\npractice. We examine the current landscape of MLLMs in healthcare, analyzing\ntheir applications across clinical decision support, medical imaging, patient\nengagement, and research. The review highlights the unique capabilities of\nMLLMs in integrating diverse data types, such as text, images, and audio, to\nprovide more comprehensive insights into patient health. We also address the\nchallenges facing MLLM implementation, including data limitations, technical\nhurdles, and ethical considerations. By identifying key research gaps, this\npaper aims to guide future investigations in areas such as dataset development,\nmodality alignment methods, and the establishment of ethical guidelines. As\nMLLMs continue to shape the future of healthcare, understanding their potential\nand limitations is crucial for their responsible and effective integration into\nmedical practice.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "12 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2410.01812v5",
    "published_date": "2024-09-14 02:35:29 UTC",
    "updated_date": "2024-12-10 03:43:20 UTC"
  },
  {
    "arxiv_id": "2409.15355v5",
    "title": "Block-Attention for Efficient Prefilling",
    "authors": [
      "Dongyang Ma",
      "Yan Wang",
      "Lan Tian"
    ],
    "abstract": "We introduce Block-attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context in an\nauto-regressive manner. Instead, Block-attention divides retrieved documents\ninto discrete blocks, with each block independently calculating key-value (KV)\nstates except for the final block. In RAG scenarios, by defining each passage\nas a block, Block-attention enables us to reuse the KV states of passages that\nhave been seen before, thereby significantly reducing the latency and the\ncomputation overhead during inference. The implementation of Block-attention\ninvolves block segmentation, position re-encoding, and fine-tuning the LLM to\nadapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,\nincluding RAG, ICL, and general domains, demonstrate that after block\nfine-tuning, the Block-attention model not only achieves performance comparable\nto that of full-attention models, but can also seamlessly switch between the\nblock and full attention modes without any performance loss. Notably,\nBlock-attention significantly reduces the time to first token (TTFT) and\nfloating point operations (FLOPs) to a very low level. It only takes 45 ms to\noutput the first token for an input sequence with a total length of 32K.\nCompared to the full-attention models, the TTFT and corresponding FLOPs are\nreduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we\nelaborate on how Block-attention is applied in Game AI scenario and the\nsubstantial potential benefits it entails. We strongly suggest researchers in\nthe gaming field not to overlook this section.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.15355v5",
    "published_date": "2024-09-14 02:34:26 UTC",
    "updated_date": "2025-04-13 14:02:47 UTC"
  },
  {
    "arxiv_id": "2409.09269v3",
    "title": "Guiding Vision-Language Model Selection for Visual Question-Answering Across Tasks, Domains, and Knowledge Types",
    "authors": [
      "Neelabh Sinha",
      "Vinija Jain",
      "Aman Chadha"
    ],
    "abstract": "Visual Question-Answering (VQA) has become key to user experience,\nparticularly after improved generalization capabilities of Vision-Language\nModels (VLMs). But evaluating VLMs for an application requirement using a\nstandardized framework in practical settings is still challenging. This paper\naims to solve that using an end-to-end framework. We present VQA360 - a novel\ndataset derived from established VQA benchmarks, annotated with task types,\napplication domains, and knowledge types, for a comprehensive evaluation. We\nalso introduce GoEval, a multimodal evaluation metric developed using GPT-4o,\nachieving a correlation factor of 56.71% with human judgments. Our experiments\nwith state-of-the-art VLMs reveal that no single model excels universally,\nthus, making a right choice a key design decision. Proprietary models such as\nGemini-1.5-Pro and GPT-4o-mini generally outperform others, but open-source\nmodels like InternVL-2-8B and CogVLM-2-Llama-3-19B also demonstrate competitive\nstrengths, while providing additional advantages. Our framework can also be\nextended to other tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at The First Workshop of Evaluation of Multi-Modal\n  Generation (EvalMG) in 31st International Conference on Computational\n  Linguistics (COLING), 2025. 8 pages + references + 6 pages of Appendix",
    "pdf_url": "http://arxiv.org/pdf/2409.09269v3",
    "published_date": "2024-09-14 02:29:36 UTC",
    "updated_date": "2024-12-12 06:26:09 UTC"
  },
  {
    "arxiv_id": "2410.01811v1",
    "title": "Evaluating Cultural Awareness of LLMs for Yoruba, Malayalam, and English",
    "authors": [
      "Fiifi Dawson",
      "Zainab Mosunmola",
      "Sahil Pocker",
      "Raj Abhijit Dandekar",
      "Rajat Dandekar",
      "Sreedath Panat"
    ],
    "abstract": "Although LLMs have been extremely effective in a large number of complex\ntasks, their understanding and functionality for regional languages and\ncultures are not well studied. In this paper, we explore the ability of various\nLLMs to comprehend the cultural aspects of two regional languages: Malayalam\n(state of Kerala, India) and Yoruba (West Africa). Using Hofstede's six\ncultural dimensions: Power Distance (PDI), Individualism (IDV), Motivation\ntowards Achievement and Success (MAS), Uncertainty Avoidance (UAV), Long Term\nOrientation (LTO), and Indulgence (IVR), we quantify the cultural awareness of\nLLM-based responses. We demonstrate that although LLMs show a high cultural\nsimilarity for English, they fail to capture the cultural nuances across these\n6 metrics for Malayalam and Yoruba. We also highlight the need for large-scale\nregional language LLM training with culturally enriched datasets. This will\nhave huge implications for enhancing the user experience of chat-based LLMs and\nalso improving the validity of large-scale LLM agent-based market research.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "19 pages, 10 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2410.01811v1",
    "published_date": "2024-09-14 02:21:17 UTC",
    "updated_date": "2024-09-14 02:21:17 UTC"
  },
  {
    "arxiv_id": "2409.09263v3",
    "title": "Operational Wind Speed Forecasts for Chile's Electric Power Sector Using a Hybrid ML Model",
    "authors": [
      "Dhruv Suri",
      "Praneet Dutta",
      "Flora Xue",
      "Ines Azevedo",
      "Ravi Jain"
    ],
    "abstract": "As Chile's electric power sector advances toward a future powered by\nrenewable energy, accurate forecasting of renewable generation is essential for\nmanaging grid operations. The integration of renewable energy sources is\nparticularly challenging due to the operational difficulties of managing their\npower generation, which is highly variable compared to fossil fuel sources,\ndelaying the availability of clean energy. To mitigate this, we quantify the\nimpact of increasing intermittent generation from wind and solar on thermal\npower plants in Chile and introduce a hybrid wind speed forecasting methodology\nwhich combines two custom ML models for Chile. The first model is based on\nTiDE, an MLP-based ML model for short-term forecasts, and the second is based\non a graph neural network, GraphCast, for medium-term forecasts up to 10 days.\nOur hybrid approach outperforms the most accurate operational deterministic\nsystems by 4-21% for short-term forecasts and 5-23% for medium-term forecasts\nand can directly lower the impact of wind generation on thermal ramping,\ncurtailment, and system-level emissions in Chile.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09263v3",
    "published_date": "2024-09-14 02:16:02 UTC",
    "updated_date": "2024-09-18 15:17:25 UTC"
  },
  {
    "arxiv_id": "2409.09261v1",
    "title": "What Is Wrong with My Model? Identifying Systematic Problems with Semantic Data Slicing",
    "authors": [
      "Chenyang Yang",
      "Yining Hong",
      "Grace A. Lewis",
      "Tongshuang Wu",
      "Christian Kästner"
    ],
    "abstract": "Machine learning models make mistakes, yet sometimes it is difficult to\nidentify the systematic problems behind the mistakes. Practitioners engage in\nvarious activities, including error analysis, testing, auditing, and\nred-teaming, to form hypotheses of what can go (or has gone) wrong with their\nmodels. To validate these hypotheses, practitioners employ data slicing to\nidentify relevant examples. However, traditional data slicing is limited by\navailable features and programmatic slicing functions. In this work, we propose\nSemSlicer, a framework that supports semantic data slicing, which identifies a\nsemantically coherent slice, without the need for existing features. SemSlicer\nuses Large Language Models to annotate datasets and generate slices from any\nuser-defined slicing criteria. We show that SemSlicer generates accurate slices\nwith low cost, allows flexible trade-offs between different design dimensions,\nreliably identifies under-performing data slices, and helps practitioners\nidentify useful data slices that reflect systematic problems.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09261v1",
    "published_date": "2024-09-14 02:15:50 UTC",
    "updated_date": "2024-09-14 02:15:50 UTC"
  },
  {
    "arxiv_id": "2409.09253v1",
    "title": "Unleash LLMs Potential for Recommendation by Coordinating Twin-Tower Dynamic Semantic Token Generator",
    "authors": [
      "Jun Yin",
      "Zhengxin Zeng",
      "Mingzheng Li",
      "Hao Yan",
      "Chaozhuo Li",
      "Weihao Han",
      "Jianjin Zhang",
      "Ruochen Liu",
      "Allen Sun",
      "Denvy Deng",
      "Feng Sun",
      "Qi Zhang",
      "Shirui Pan",
      "Senzhang Wang"
    ],
    "abstract": "Owing to the unprecedented capability in semantic understanding and logical\nreasoning, the pre-trained large language models (LLMs) have shown fantastic\npotential in developing the next-generation recommender systems (RSs). However,\nthe static index paradigm adopted by current methods greatly restricts the\nutilization of LLMs capacity for recommendation, leading to not only the\ninsufficient alignment between semantic and collaborative knowledge, but also\nthe neglect of high-order user-item interaction patterns. In this paper, we\npropose Twin-Tower Dynamic Semantic Recommender (TTDS), the first generative RS\nwhich adopts dynamic semantic index paradigm, targeting at resolving the above\nproblems simultaneously. To be more specific, we for the first time contrive a\ndynamic knowledge fusion framework which integrates a twin-tower semantic token\ngenerator into the LLM-based recommender, hierarchically allocating meaningful\nsemantic index for items and users, and accordingly predicting the semantic\nindex of target item. Furthermore, a dual-modality variational auto-encoder is\nproposed to facilitate multi-grained alignment between semantic and\ncollaborative knowledge. Eventually, a series of novel tuning tasks specially\ncustomized for capturing high-order user-item interaction patterns are proposed\nto take advantages of user historical behavior. Extensive experiments across\nthree public datasets demonstrate the superiority of the proposed methodology\nin developing LLM-based generative RSs. The proposed TTDS recommender achieves\nan average improvement of 19.41% in Hit-Rate and 20.84% in NDCG metric,\ncompared with the leading baseline methods.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09253v1",
    "published_date": "2024-09-14 01:45:04 UTC",
    "updated_date": "2024-09-14 01:45:04 UTC"
  },
  {
    "arxiv_id": "2409.09251v1",
    "title": "ETAGE: Enhanced Test Time Adaptation with Integrated Entropy and Gradient Norms for Robust Model Performance",
    "authors": [
      "Afshar Shamsi",
      "Rejisa Becirovic",
      "Ahmadreza Argha",
      "Ehsan Abbasnejad",
      "Hamid Alinejad-Rokny",
      "Arash Mohammadi"
    ],
    "abstract": "Test time adaptation (TTA) equips deep learning models to handle unseen test\ndata that deviates from the training distribution, even when source data is\ninaccessible. While traditional TTA methods often rely on entropy as a\nconfidence metric, its effectiveness can be limited, particularly in biased\nscenarios. Extending existing approaches like the Pseudo Label Probability\nDifference (PLPD), we introduce ETAGE, a refined TTA method that integrates\nentropy minimization with gradient norms and PLPD, to enhance sample selection\nand adaptation. Our method prioritizes samples that are less likely to cause\ninstability by combining high entropy with high gradient norms out of\nadaptation, thus avoiding the overfitting to noise often observed in previous\nmethods. Extensive experiments on CIFAR-10-C and CIFAR-100-C datasets\ndemonstrate that our approach outperforms existing TTA techniques, particularly\nin challenging and biased scenarios, leading to more robust and consistent\nmodel performance across diverse test scenarios. The codebase for ETAGE is\navailable on https://github.com/afsharshamsi/ETAGE.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09251v1",
    "published_date": "2024-09-14 01:25:52 UTC",
    "updated_date": "2024-09-14 01:25:52 UTC"
  },
  {
    "arxiv_id": "2409.11430v3",
    "title": "Federated Learning with Quantum Computing and Fully Homomorphic Encryption: A Novel Computing Paradigm Shift in Privacy-Preserving ML",
    "authors": [
      "Siddhant Dutta",
      "Pavana P Karanth",
      "Pedro Maciel Xavier",
      "Iago Leal de Freitas",
      "Nouhaila Innan",
      "Sadok Ben Yahia",
      "Muhammad Shafique",
      "David E. Bernal Neira"
    ],
    "abstract": "The widespread deployment of products powered by machine learning models is\nraising concerns around data privacy and information security worldwide. To\naddress this issue, Federated Learning was first proposed as a\nprivacy-preserving alternative to conventional methods that allow multiple\nlearning clients to share model knowledge without disclosing private data. A\ncomplementary approach known as Fully Homomorphic Encryption (FHE) is a\nquantum-safe cryptographic system that enables operations to be performed on\nencrypted weights. However, implementing mechanisms such as these in practice\noften comes with significant computational overhead and can expose potential\nsecurity threats. Novel computing paradigms, such as analog, quantum, and\nspecialized digital hardware, present opportunities for implementing\nprivacy-preserving machine learning systems while enhancing security and\nmitigating performance loss. This work instantiates these ideas by applying the\nFHE scheme to a Federated Learning Neural Network architecture that integrates\nboth classical and quantum layers.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.CR",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "quant-ph",
    "comment": "10 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.11430v3",
    "published_date": "2024-09-14 01:23:26 UTC",
    "updated_date": "2024-10-12 10:51:52 UTC"
  },
  {
    "arxiv_id": "2409.09245v1",
    "title": "Robust Training of Neural Networks at Arbitrary Precision and Sparsity",
    "authors": [
      "Chengxi Ye",
      "Grace Chu",
      "Yanfeng Liu",
      "Yichi Zhang",
      "Lukasz Lew",
      "Andrew Howard"
    ],
    "abstract": "The discontinuous operations inherent in quantization and sparsification\nintroduce obstacles to backpropagation. This is particularly challenging when\ntraining deep neural networks in ultra-low precision and sparse regimes. We\npropose a novel, robust, and universal solution: a denoising affine transform\nthat stabilizes training under these challenging conditions. By formulating\nquantization and sparsification as perturbations during training, we derive a\nperturbation-resilient approach based on ridge regression. Our solution employs\na piecewise constant backbone model to ensure a performance lower bound and\nfeatures an inherent noise reduction mechanism to mitigate perturbation-induced\ncorruption. This formulation allows existing models to be trained at\narbitrarily low precision and sparsity levels with off-the-shelf recipes.\nFurthermore, our method provides a novel perspective on training temporal\nbinary neural networks, contributing to ongoing efforts to narrow the gap\nbetween artificial and biological neural networks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09245v1",
    "published_date": "2024-09-14 00:57:32 UTC",
    "updated_date": "2024-09-14 00:57:32 UTC"
  },
  {
    "arxiv_id": "2409.09240v1",
    "title": "Cross-Entropy Optimization for Hyperparameter Optimization in Stochastic Gradient-based Approaches to Train Deep Neural Networks",
    "authors": [
      "Kevin Li",
      "Fulu Li"
    ],
    "abstract": "In this paper, we present a cross-entropy optimization method for\nhyperparameter optimization in stochastic gradient-based approaches to train\ndeep neural networks. The value of a hyperparameter of a learning algorithm\noften has great impact on the performance of a model such as the convergence\nspeed, the generalization performance metrics, etc. While in some cases the\nhyperparameters of a learning algorithm can be part of learning parameters, in\nother scenarios the hyperparameters of a stochastic optimization algorithm such\nas Adam [5] and its variants are either fixed as a constant or are kept\nchanging in a monotonic way over time. We give an in-depth analysis of the\npresented method in the framework of expectation maximization (EM). The\npresented algorithm of cross-entropy optimization for hyperparameter\noptimization of a learning algorithm (CEHPO) can be equally applicable to other\nareas of optimization problems in deep learning. We hope that the presented\nmethods can provide different perspectives and offer some insights for\noptimization problems in different areas of machine learning and beyond.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.09240v1",
    "published_date": "2024-09-14 00:39:37 UTC",
    "updated_date": "2024-09-14 00:39:37 UTC"
  },
  {
    "arxiv_id": "2409.09239v3",
    "title": "Autoregressive + Chain of Thought = Recurrent: Recurrence's Role in Language Models' Computability and a Revisit of Recurrent Transformer",
    "authors": [
      "Xiang Zhang",
      "Muhammad Abdul-Mageed",
      "Laks V. S. Lakshmanan"
    ],
    "abstract": "The Transformer architecture excels in a variety of language modeling tasks,\noutperforming traditional neural architectures such as RNN and LSTM. This is\npartially due to its elimination of recurrent connections, which allows for\nparallel training and a smoother flow of gradients. However, this move away\nfrom recurrent structures places the Transformer model at the lower end of\nChomsky's computational hierarchy, imposing limitations on its computational\nabilities. Consequently, even advanced Transformer-based models face\nconsiderable difficulties in tasks like counting, string reversal, and\nmultiplication. These tasks, though seemingly elementary, require a level of\ncomputational complexity that exceeds the capabilities of the Transformer\narchitecture. Concurrently, the emergence of ``Chain of Thought\" (CoT)\nprompting has enabled Transformer-based language models to tackle tasks that\nwere previously impossible or poorly executed. In this work, we thoroughly\ninvestigate the influence of recurrent structures in neural models on their\nreasoning abilities and computability, contrasting the role autoregression\nplays in the neural models' computational power. We then shed light on how the\nCoT approach can mimic recurrent computation and act as a bridge between\nautoregression and recurrence in the context of language models. It is this\napproximated recurrence that notably improves the model's performance and\ncomputational capacity. Moreover, we revisit recent recurrent-based Transformer\nmodel designs, focusing on their computational abilities through our proposed\nconcept of ``recurrence-completeness\" and identify key theoretical limitations\nin models like Linear Transformer and RWKV. Through this, we aim to provide\ninsight into the neural model architectures and prompt better model design.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09239v3",
    "published_date": "2024-09-14 00:30:57 UTC",
    "updated_date": "2024-09-20 21:12:25 UTC"
  }
]
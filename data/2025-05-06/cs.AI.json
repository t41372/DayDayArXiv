{
  "date": "2025-05-06",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间2025-05-06的 arXiv 中文 TLDR 快报！\n\n今天的 arXiv 论文热点纷呈，大型语言模型（LLM）依然是研究的焦点，涵盖了从提升推理能力、保障安全、优化效率到探索全新训练范式等多个方面。特别值得关注的是 **Absolute Zero** 项目，它提出了一种完全无需外部数据的自学习推理智能体，以及 **LlamaFirewall** 这样一个为AI智能体设计的开源安全护栏系统。此外，机器人领域在人形机器人控制、视觉导航避障以及模仿学习方面也有显著进展。计算机视觉领域则在可控图像/视频生成、三维重建和医学影像分析上带来了新的突破。\n\n接下来，让我们详细看看今天有哪些值得关注的论文：\n\n---\n\n**首先聚焦几篇影响力较大或极具创新性的工作：**\n\n1.  **VITA-Audio: 高效大型语音语言模型的快速交错跨模态词元生成 (VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model)**\n    *   **贡献:** 针对现有语音模型在流式传输中生成首个音频词元延迟高的问题，提出了 VITA-Audio，一个端到端的大型语音模型。其核心是轻量级的多重跨模态词元预测 (MCTP) 模块，能在单次模型前向传播中高效生成多个音频词元，显著降低首个音频生成的延迟。\n    *   **发现:** VITA-Audio 是首个能在首次前向传播中生成音频输出的多模态大语言模型，实现了3-5倍的推理加速（7B参数规模），并在自动语音识别 (ASR)、文本到语音 (TTS) 和口语问答 (SQA) 等任务上超越了同等规模的开源模型。\n    *   **看点:** 真正意义上提升了语音交互的实时性，对实时对话系统部署意义重大。代码和模型已开源。\n\n2.  **绝对零点：零数据下的强化自博弈推理 (Absolute Zero: Reinforced Self-play Reasoning with Zero Data)**\n    *   **贡献:** 提出了一种名为 \"绝对零点\" (Absolute Zero) 的新范式，AI模型可以自主提出能最大化自身学习进程的任务，并通过解决这些任务来提升推理能力，整个过程完全不依赖任何外部数据。\n    *   **方法:** 引入了绝对零点推理器 (AZR)，该系统通过代码执行器来验证提出的代码推理任务和答案，从而提供可验证的奖励信号，引导开放式且有根据的学习。\n    *   **发现:** 尽管完全没有使用外部数据进行训练，AZR 在编码和数学推理任务上达到了SOTA水平，超越了依赖数万个人工标注样本的现有零样本模型。\n    *   **看点:** 对AI的自主学习和可扩展性提出了极具前瞻性的探索，可能改变我们对AI训练数据依赖的认知。\n\n3.  **LlamaFirewall: 构建安全AI智能体的开源护栏系统 (LlamaFirewall: An open source guardrail system for building secure AI agents)**\n    *   **贡献:** 针对LLM演变为能够执行复杂任务（如编辑代码、基于不可信输入采取行动）的自主智能体所带来的新安全风险，提出了LlamaFirewall，一个开源的、专注于安全的护栏框架。\n    *   **核心组件:** 包括PromptGuard 2（通用越狱检测器，达到SOTA）、Agent Alignment Checks（思维链审计器，检测提示注入和目标失准）和CodeShield（快速可扩展的在线静态分析引擎，防止生成不安全代码）。\n    *   **看点:** 为AI智能体提供了一个关键的实时防御层，支持特定用例的安全策略定义和执行，对AI安全领域非常重要。\n\n4.  **SPAP: 基于交替优化和惩罚方法的结构化剪枝 (SPAP: Structured Pruning via Alternating Optimization and Penalty Methods)**\n    *   **贡献:** 针对大型语言模型（LLM）部署时计算和内存需求大的问题，提出了一种新颖高效的结构化剪枝框架SPAP。该方法基于优化理论，通过混合整数优化模型制定剪枝问题。\n    *   **方法:** 采用惩罚方法有效做出剪枝决策以最小化剪枝误差，并引入针对可分问题结构的交替最小化算法，以高效更新权重和恢复性能。\n    *   **发现:** 在OPT、LLaMA-3/3.1/3.2和Qwen2.5模型上的实验表明，SPAP优于现有SOTA方法，在30%稀疏度下实现了1.29倍的线性推理加速和相应的内存减少，同时保持模型性能。\n    *   **看点:** 提供了一种实用的、由优化驱动的LLM剪枝方案，对LLM的高效部署有直接价值。\n\n---\n\n**接下来是关于机器人与控制领域的进展：**\n\n5.  **AMO: 超灵巧人形机器人全身控制的自适应运动优化 (AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control)**\n    *   **贡献:** 提出了AMO框架，结合了模拟到真实强化学习 (RL) 与轨迹优化，用于人形机器人实时、自适应的全身控制，以实现如从地面捡拾物体等需要大操作空间的高难度任务。\n    *   **方法:** 构建了混合AMO数据集，并训练了一个能够鲁棒、按需适应潜在O.O.D.指令的网络，以减轻运动模仿RL中的分布偏差。\n    *   **发现:** 在模拟和29自由度的Unitree G1人形机器人上验证了AMO，展示了比基线方法更优的稳定性和更大的工作空间，并支持通过模仿学习执行自主任务。\n    *   **项目主页:** [https://amo-humanoid.github.io](https://amo-humanoid.github.io)\n\n6.  **ViSafe演示: 用于高速探测与规避的视觉安全系统 (Demonstrating ViSafe: Vision-enabled Safety for High-speed Detect and Avoid)**\n    *   **贡献:** 提出了ViSafe，一个纯视觉的高速机载碰撞规避系统，为资源受限的空中系统提供安全关键能力。它通过紧密集成基于学习的边缘AI框架与定制的多摄像头硬件原型，为探测与规避 (DAA) 问题提供了全栈解决方案。\n    *   **方法:** 利用以感知输入为中心的控制屏障函数 (CBF) 来设计、编码和强制执行安全阈值，为高速空中操作中的自我分离提供可证明的安全运行时保证。\n    *   **发现:** 在模拟和真实飞行场景中（包括高达144公里/小时的接近速度），ViSafe 始终确保自我分离，为纯视觉自主碰撞规避树立了新标杆。\n\n7.  **离散时间高斯过程混合的无理有效性用于机器人策略学习 (The Unreasonable Effectiveness of Discrete-Time Gaussian Process Mixtures for Robot Policy Learning)**\n    *   **贡献:** 提出了离散时间高斯过程混合 (MiDiGap)，一种用于机器人操作中灵活策略表示和模仿学习的新方法。\n    *   **方法:** MiDiGap 仅使用相机观察，从少至五个演示中学习，并能泛化到各种挑战性任务，如制作咖啡、开门等。它能在CPU上不到一分钟内学习这些任务，并线性扩展到大型数据集。\n    *   **发现:** MiDiGap 在多个少样本操作基准上实现了SOTA性能，显著提高了策略成功率并降低了轨迹成本。代码已开源。\n\n8.  **潜空间自适应规划器用于动态操纵 (Latent Adaptive Planner for Dynamic Manipulation)**\n    *   **贡献:** 提出了潜空间自适应规划器 (LAP)，一种用于动态非抓取操纵任务的新方法，将规划表述为潜空间推理，并从人类演示视频中有效学习。\n    *   **方法:** LAP 采用贝叶斯更新在潜空间中增量式地优化计划，通过基于模型的比例映射弥合人与机器人的体现差异。\n    *   **发现:** LAP 在多个复杂操纵基准测试中表现出SOTA性能，尤其在动态适应场景中，其成功率、轨迹平滑度和能量效率均优于现有方法。\n\n---\n\n**大型语言模型 (LLM) 的其他重要研究：**\n\n9.  **行动者-评论家算法可以实现最优样本效率 (Actor-Critics Can Achieve Optimal Sample Efficiency)**\n    *   **贡献:** 解决了一个开放问题，即在需要策略探索和通用函数逼近的情况下，如何以 $O(1/\\epsilon^2)$ 轨迹的样本复杂度学习到 $\\epsilon$-最优策略。\n    *   **方法:** 提出了一种新的行动者-评论家算法，集成了乐观主义、针对最优Q函数的离策略评论家估计以及罕见切换的策略重置。\n    *   **发现:** 该算法达到了 $O(dH^5 \\log|\\mathcal{A}|/\\epsilon^2 + d H^4 \\log|\\mathcal{F}|/ \\epsilon^2)$ 的样本复杂度。还将其扩展到混合RL设置，并提供了一个非乐观的高效行动者-评论家算法。\n    *   **会议:** ICML 2025 接收。\n\n10. **面向LLM的图绘制：实证评估 (Graph Drawing for LLMs: An Empirical Evaluation)**\n    *   **贡献:** 研究了LLM在处理图相关任务时，输入图的绘制方式（布局范式、美学特性）以及提示技术如何影响模型性能。\n    *   **发现:** 选择正确的布局范式和优化图的可读性（从人类视角）可以显著提高LLM在给定任务上的性能。选择最有效的提示技术也是实现最佳性能的关键。\n\n11. **ReGraP-LLaVA: 支持推理的基于图的个性化大型语言和视觉助手 (ReGraP-LLaVA: Reasoning enabled Graph-based Personalized Large Language and Vision Assistant)**\n    *   **贡献:** 针对现有MLLM在个性化概念间关系推理方面的不足，提出了新数据集ReGraP（包含图像、知识图谱KG和基于KG的CoT问答对）和模型ReGraP-LLaVA。\n    *   **方法:** ReGraP-LLaVA通过软硬图提示方法将KG对齐到模型语义空间，旨在学习个性化知识并进行关系推理。同时构建了ReGraP基准测试来评估关系推理能力。\n    *   **发现:** ReGraP-LLaVA 不仅学习个性化知识，还能在响应中执行关系推理，达到SOTA。代码和数据集已发布。\n\n12. **am-ELO: 基于竞技场的LLM评估稳定框架 (am-ELO: A Stable Framework for Arena-based LLM Evaluation)**\n    *   **贡献:** 针对现有基于ELO评分系统的竞技场式LLM评估框架因排名不一致和缺乏对标注者能力变化的关注而导致的不稳定问题，提出了am-ELO。\n    *   **方法:** 用最大似然估计 (MLE) 方法替代迭代更新（m-ELO），并修改ELO概率函数以纳入标注者能力 (am-ELO)，从而同时估计模型得分和标注者可靠性。\n    *   **发现:** 该方法确保了稳定性，为LLM提供了更鲁棒、准确和稳定的评估。\n    *   **会议:** ICML 2025 接收。\n\n13. **CombiBench: 衡量LLM组合数学能力的基准 (CombiBench: Benchmarking LLM Capability for Combinatorial Mathematics)**\n    *   **贡献:** 为解决组合数学领域缺乏合适基准和定理库的问题，推出了CombiBench，一个包含100个组合数学问题的综合基准，每个问题都有Lean 4形式化描述和非形式化陈述。\n    *   **特点:** 问题难度覆盖从中学到IMO及大学水平，包含2000年以来所有IMO组合问题（除一个含图问题外）。同时提供了Fine-Eval评估框架。\n    *   **发现:** 当前LLM在形式化解决组合数学问题方面的能力仍然有限。Kimina-Prover表现最好，解决了7/100个问题。数据集和代码已开源。\n\n14. **RAG-MCP: 通过检索增强生成缓解LLM工具选择中的提示膨胀 (RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via Retrieval-Augmented Generation)**\n    *   **贡献:** 针对LLM在利用大量外部工具（如MCP定义的工具）时因提示膨胀和选择复杂性而效果不佳的问题，提出了RAG-MCP框架。\n    *   **方法:** RAG-MCP 使用语义检索在与LLM交互前从外部索引中识别与给定查询最相关的MCP，仅将选定的工具描述传递给模型，从而大幅减少提示大小。\n    *   **发现:** RAG-MCP 显著减少了提示词元（例如，减少超过50%），并将工具选择准确率提高了两倍以上（43.13% vs 13.62%）。\n\n---\n\n**计算机视觉与图像/视频生成：**\n\n15. **FlexiAct: 面向异构场景的灵活动作控制 (FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios)**\n    *   **贡献:** 提出了FlexiAct，一种能够将参考视频中的动作迁移到任意目标图像的方法，克服了现有方法在空间结构（如布局、骨架、视点一致性）上的严格限制。\n    *   **方法:** 引入了轻量级图像条件适配器RefAdapter，用于空间适应和一致性保持。提出了频率感知动作提取 (FAE) 模块，在去噪过程中直接提取动作。\n    *   **发现:** FlexiAct 能够有效地将动作迁移到具有不同布局、骨架和视点的目标主体上，同时保持身份一致性。\n    *   **会议:** SIGGRAPH 2025 接收。项目主页和代码已发布。\n\n16. **使用流匹配模型的实时人物图像合成 (Real-Time Person Image Synthesis Using a Flow Matching Model)**\n    *   **贡献:** 针对姿态引导的人物图像合成 (PGPIS) 任务中现有扩散模型采样速度慢的问题，提出了一种基于流匹配 (FM) 的生成模型RPFM。\n    *   **方法:** 该方法支持条件生成并在潜空间操作，旨在实现更快、更稳定、更高效的训练和采样，以满足实时应用需求。\n    *   **发现:** RPFM 在DeepFashion数据集上实现了接近实时的采样速度，同时保持了与SOTA模型相当的性能，生成速度提升两倍以上，略微牺牲图像精度。\n\n17. **稳定运动：使用未配对损坏数据训练运动清理模型 (StableMotion: Training Motion Cleanup Models with Unpaired Corrupted Data)**\n    *   **贡献:** 提出了StableMotion，一种仅使用未配对的、包含损坏数据的原始动作捕捉（mocap）数据集来训练运动清理模型的方法。\n    *   **方法:** 引入运动质量指标，可通过手动标记或启发式算法轻松注释，从而在混合质量的原始运动数据上训练质量感知的运动生成模型。该方法基于扩散模型，形成统一的运动生成-判别模型。\n    *   **发现:** 在包含真实世界运动伪影的SoccerMocap数据集上，训练的模型有效纠正了多种运动伪影，分别将运动跳变和冻结帧减少了68%和81%。\n\n18. **GC MVSNet++: 融合3D几何与机器学习的多视图立体视觉 (Blending 3D Geometry and Machine Learning for Multi-View Stereopsis)**\n    *   **贡献:** 提出了GC MVSNet++，一种在学习阶段主动强制执行参考视图深度图在多个源视图（多视图）和多个尺度（多尺度）上的几何一致性的新方法。\n    *   **方法:** 这种集成的几何一致性检查通过直接惩罚几何不一致的像素，显著加速了学习过程。还引入了具有两种不同块设计的密集连接成本正则化网络。\n    *   **发现:** 在DTU和BlendedMVS数据集上实现了新的SOTA，并在Tanks and Temples基准测试中排名第二。\n\n---\n\n**AI在特定领域的应用与探索：**\n\n19. **MedArabiQ: 阿拉伯语医疗任务的大型语言模型基准测试 (MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks)**\n    *   **贡献:** 推出了MedArabiQ，一个包含七项阿拉伯语医疗任务的新基准数据集，涵盖多个专业，包括选择题、填空题和医患问答。\n    *   **发现:** 对五个SOTA开源和专有LLM（包括GPT-4o, Claude 3.5-Sonnet, Gemini 1.5）的评估强调了创建跨语言高质量基准的需求，以确保LLM在医疗保健领域的公平部署和可扩展性。\n\n20. **OSUniverse: 多模态GUI导航AI智能体基准 (OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents)**\n    *   **贡献:** 推出了OSUniverse，一个复杂、多模态、面向桌面的GUI导航AI智能体任务基准，专注于易用性、可扩展性、测试用例的全面覆盖和自动验证。\n    *   **特点:** 任务难度递增，从基本精确点击到需要灵巧性、精确性和清晰思维的多步骤、多应用测试。SOTA智能体在该基准上得分不超过50%，而普通白领可完美完成。\n    *   **项目主页:** [https://github.com/agentsea/osuniverse](https://github.com/agentsea/osuniverse)\n\n21. **RCMed: 通过增强视觉与语言关联实现精确的医疗AI助手 (Reinforced Correlation Between Vision and Language for Precise Medical AI Assistant)**\n    *   **贡献:** 提出了RCMed，一个全栈AI助手，通过改进输入和输出中的多模态对齐，实现精确的解剖结构描绘、准确定位和可靠诊断。\n    *   **方法:** 自我强化的关联机制使视觉特征为语言上下文提供信息，而语言语义则指导像素级注意力，形成闭环。通过颜色区域描述策略增强这种关联。\n    *   **发现:** 在2000万图像-掩码-描述三元组上训练后，RCMed在165个临床任务和9种模态中达到SOTA精度，在细胞分割任务中相对改进23.5%。\n\n22. **STORY2GAME: 在互动小说游戏中生成（几乎）一切 (STORY2GAME: Generating (Almost) Everything in an Interactive Fiction Game)**\n    *   **贡献:** 提出了STORY2GAME，一种使用LLM生成基于文本的互动小说游戏的新方法。它首先生成故事，填充世界，并在游戏引擎中构建动作代码，使故事能够以互动方式进行。\n    *   **创新:** 关键在于使用LLM生成的动作前提条件和效果作为指导，动态生成新动作以适应玩家意图，甚至可能需要实时更新游戏引擎的状态表示。\n\n23. **面向检索增强生成的超参数优化方法分析 (An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation)**\n    *   **贡献:** 对RAG的超参数优化（HPO）框架的有效性进行了全面的基准测试。\n    *   **发现:** 研究了5种HPO算法在5个不同领域数据集上的表现，发现RAG HPO可以高效完成（贪心或迭代随机搜索），并显著提升所有数据集的RAG性能。对于贪心HPO方法，优先优化模型比按RAG流程顺序优化更可取。\n\n---\n\n**其他值得关注的研究：**\n\n*   **反事实推理消除推荐系统中的情感偏见 (Counterfactual Inference for Eliminating Sentiment Bias in Recommender Systems):** 利用反事实推理来缓解评论类推荐系统中因用户或物品负面评论导致推荐准确率下降的情感偏见问题。\n*   **语言模型的隐写潜力 (The Steganographic Potentials of Language Models):** 探讨了通过强化学习微调的LLM在文本中隐藏信息（隐写术）的能力，及其对检测和阻止非对齐AI智能体的挑战。\n*   **基于哈希图共识机制的可靠多模型推理 (A Hashgraph-Inspired Consensus Mechanism for Reliable Multi-Model Reasoning):** 提出了一种受哈希图共识算法启发的机制，用于验证和聚合来自不同推理模型（如OpenAI, Google等）的输出，以减少不一致和幻觉。\n*   **面向时间序列的符号持久宏动作学习用于POMDP求解 (Learning Symbolic Persistent Macro-Actions for POMDP Solving Over Time):** 结合时间逻辑推理和POMDP，利用事件演算生成持久宏动作，指导基于MCTS的POMDP求解器，显著减少推理时间。\n*   **利用CycleGAN模型从T1加权MRI生成合成3D FA图革新脑肿瘤成像 (Revolutionizing Brain Tumor Imaging: Generating Synthetic 3D FA Maps from T1-Weighted MRI using CycleGAN Models):** 首次将CycleGAN应用于从T1加权MRI直接生成分数各向异性(FA)图，覆盖健康和肿瘤组织。\n*   **彩虹延迟补偿：缓解延迟观测的多智能体强化学习框架 (Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning Framework for Mitigating Delayed Observation):** 提出了彩虹延迟补偿 (RDC) 框架，用于解决多智能体系统中普遍存在的、具有不同延迟特征的离散观测分量带来的挑战。\n*   **ALMA: 针对自动编码器的聚合Lipschitz最大化攻击 (ALMA: Aggregated Lipschitz Maximization Attack on Auto-encoders):** 提出了一种新的基于层条件的对抗性优化目标，以更有效地攻击深度自动编码器。\n*   **BURNS: 神经反馈回路系统的反向欠近似可达性 (BURNS: Backward Underapproximate Reachability for Neural-Feedback-Loop Systems):** 提出了一种计算非线性离散时间神经反馈回路欠近似反向可达集的算法，用于检查目标到达属性。\n*   **在人工神经网络的感知边界上合成图像以揭示和操纵人类感知变异性 (Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering and Manipulating Human Perceptual Variability):** 提出了BAM框架，结合ANN中的感知边界采样和人类行为实验，系统研究人类决策的变异性。ICML 2025接收。\n*   **用于改进DreamBooth和InstantID中面部相似度的合成数据生成 (Generating Synthetic Data via Augmentations for Improved Facial Resemblance in DreamBooth and InstantID):** 研究了数据增强对使用DreamBooth和InstantID进行个性化人像生成时面部相似度的影响。CVPR 2025 Workshop接收。\n*   **通过QR码分析利用机器学习技术检测“Quishing”攻击 (Detecting Quishing Attacks with Machine Learning Techniques Through QR Code Analysis):** 提出了首个直接分析QR码结构和像素模式（而非提取嵌入内容）来检测QR码网络钓鱼攻击的框架。\n*   **利用大型语言模型识别幼儿园不同自由玩耍情境中儿童发展的有效性验证 (Validating the Effectiveness of a Large Language Model-based Approach for Identifying Children's Development across Various Free Play Settings in Kindergarten):** 提出一种结合LLM和学习分析的方法，通过分析儿童游戏体验的自我叙述来识别其发展能力。\n*   **领域对抗训练以减轻基于语音的心理健康检测中的性别偏见 (Domain Adversarial Training for Mitigating Gender Bias in Speech-based Mental Health Detection):** 引入领域对抗训练方法，将不同性别视为不同领域，以解决基于语音的抑郁症和PTSD检测模型中的性别偏见问题。EMBC 2025接收。\n*   **更安全的提示：降低视觉生成AI中的IP风险 (Safer Prompts: Reducing IP Risk in Visual Generative AI):** 评估了提示工程技术（如思维链提示和任务指令提示）在降低图像生成中知识产权侵权风险方面的有效性。\n*   **避免推荐域外物品：LLM的受限生成式推荐 (Avoid Recommending Out-of-Domain Items: Constrained Generative Recommendation with LLMs):** 研究了两种方法（RecLM-ret和RecLM-cgen）来确保LLM在推荐时不会推荐域外物品，RecLM-cgen表现更优。\n*   **通过持久工作流提示、元提示和元推理实现AI驱动的学术同行评审 (AI-Driven Scholarly Peer Review via Persistent Workflow Prompting, Meta-Prompting, and Meta-Reasoning):** 介绍了一种持久工作流提示 (PWP) 的提示工程方法，用于通过标准LLM聊天界面对实验化学手稿进行批判性分析。\n*   **基于TanDEM-X InSAR数据和自监督学习的极高分辨率森林制图 (Very High-Resolution Forest Mapping with TanDEM-X InSAR Data and Self-Supervised Learning):** 研究使用自监督学习技术从TanDEM-X数据中提取信息丰富的表示，以实现6米分辨率的森林制图，减少对大量标记数据的依赖。\n*   **人工行为智能：技术、挑战与未来方向 (Artificial Behavior Intelligence: Technology, Challenges, and Future Directions):** 定义了人工行为智能 (ABI) 的技术框架，用于综合分析和解释人类姿态、面部表情、情感、行为序列和情境线索。\n*   **带有可学习小波的Mamba-Diffusion模型用于可控符号音乐生成 (Mamba-Diffusion Model with Learnable Wavelet for Controllable Symbolic Music Generation):** 提出了一种新的扩散模型，结合了Transformer-Mamba块和可学习小波变换，用于生成具有目标和弦的可控符号音乐。\n*   **面向能力驱动的LLM技能生成：一种基于RAG的重用现有库和接口的方法 (Capability-Driven Skill Generation with LLMs: A RAG-Based Approach for Reusing Existing Libraries and Interfaces):** 提出一种方法，将能力视为技能实现的契约，并利用LLM根据自然语言用户输入生成可执行代码，集成了现有软件库和接口技术。\n*   **物理启发的能量转换神经网络用于序列学习 (Physics-inspired Energy Transition Neural Network for Sequence Learning):** 提出了一种名为PETNN的有效循环结构，其记忆机制能有效存储长期依赖信息，在多种序列任务上优于Transformer。\n*   **看抽象：为视觉语言模型翻译抽象语言 (Seeing the Abstract: Translating the Abstract Language for Vision Language Models):** 揭示了自然语言中抽象概念的广泛存在和被低估的价值，并提出了一种无训练、模型无关的方法ACT，将抽象表示向VLM潜空间中良好表示的具体表示转移。CVPR 2025接收。\n*   **DocSpiral: 通过“人在环中”实现集成辅助文档标注的平台 (DocSpiral: A Platform for Integrated Assistive Document Annotation through Human-in-the-Spiral):** 提出了DocSpiral，首个“人在螺旋中”(Human-in-the-Spiral) 的辅助文档标注平台，用于从特定领域的基于图像的文档中提取结构化信息。\n*   **可信多LLM网络：挑战、解决方案及用例 (A Trustworthy Multi-LLM Network: Challenges,Solutions, and A Use Case):** 提出了一个基于区块链的协作框架，将多个LLM连接成一个可信多LLM网络 (MultiLLMN)，以协同评估和选择对复杂网络优化问题的最可靠、高质量响应。\n*   **对比激活工程的模式与机制 (Patterns and Mechanisms of Contrastive Activation Engineering):** 分析了对比激活工程 (CAE) 技术在分布内和分布外设置中的性能，评估了其缺点，并开始制定其有效部署的综合指南。\n*   **seq-JEPA: 不变-等变世界模型的自回归预测学习 (seq-JEPA: Autoregressive Predictive Learning of Invariant-Equivariant World Models):** 提出seq-JEPA，一个基于联合嵌入预测架构的世界建模范式，利用架构归纳偏置同时学习等变和不变表示，无需额外的等变预测器或损失项。\n*   **RAVU: 基于图上组合推理的检索增强视频理解 (RAVU: Retrieval Augmented Video Understanding with Compositional Reasoning over Graph):** 提出了RAVU框架，通过在时空图上进行组合推理来增强视频理解，图作为长期记忆跟踪对象及其行为。\n*   **面向目标条件强化学习的空反事实因子交互 (Null Counterfactual Factor Interactions for Goal-Conditioned Reinforcement Learning):** 引入了HInt，结合交互和事后重标记来提高GCRL的样本效率，并提出了基于“空反事实”概念的交互定义及推断方法NCII。ICLR 2025发表。\n*   **模型对齐的软Best-of-n采样 (Soft Best-of-n Sampling for Model Alignment):** 提出了软Best-of-n采样，作为BoN采样的一般化，通过温度参数在原始分布和奖励最大化分布之间平滑插值。ISIT 2025发表。\n*   **DMoCo: 使用低秩微分同胚流的运动补偿心脏MRI (Motion-compensated cardiac MRI using low-rank diffeomorphic flow (DMoCo))**: 提出了一种无监督的运动补偿图像重建算法，用于自由呼吸和非门控3D心脏MRI，其核心是微分同胚族的低秩模型。\n*   **Holmes: 基于大型语言模型的自动事实核查 (Holmes: Automated Fact Check with Large Language Models)**: 提出了Holmes框架，具有新颖的证据检索方法，辅助LLM收集高质量证据，以进行多模态虚假信息检测。\n*   **VISLIX: 一个用于通过切片发现和分析验证视觉模型的XAI框架 (VISLIX: An XAI Framework for Validating Vision Models with Slice Discovery and Analysis)**: 引入了VISLIX，一个可视化分析框架，利用基础模型帮助领域专家分析计算机视觉模型中的数据切片，以识别模型表现不佳的子组。\n*   **认知涌现：人-AI知识共创中的能动性、维度与动态 (Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI Knowledge Co-Creation)**: 引入了Cognitio Emergens (CE) 框架，分析人类与AI在科学知识创造中共同进化的认知伙伴关系。\n*   **通过混沌工程评估和增强基于LLM的多智能体系统的鲁棒性 (Assessing and Enhancing the Robustness of LLM-based Multi-Agent Systems Through Chaos Engineering)**: 探讨了应用混沌工程来增强基于LLM的多智能体系统 (LLM-MAS) 在生产类环境和真实条件下鲁棒性的方法。\n\n---\n\n希望这份TLDR快报能帮助您快速了解 arXiv 的最新动态！",
  "papers": [
    {
      "arxiv_id": "2505.03739v1",
      "title": "VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model",
      "title_zh": "VITA-Audio：用于高效大型语音语言模型的快速交错跨模态令牌生成\n",
      "authors": [
        "Zuwei Long",
        "Yunhang Shen",
        "Chaoyou Fu",
        "Heting Gao",
        "Lijiang Li",
        "Peixian Chen",
        "Mengdan Zhang",
        "Hang Shao",
        "Jian Li",
        "Jinlong Peng",
        "Haoyu Cao",
        "Ke Li",
        "Rongrong Ji",
        "Xing Sun"
      ],
      "abstract": "With the growing requirement for natural human-computer interaction,\nspeech-based systems receive increasing attention as speech is one of the most\ncommon forms of daily communication. However, the existing speech models still\nexperience high latency when generating the first audio token during streaming,\nwhich poses a significant bottleneck for deployment. To address this issue, we\npropose VITA-Audio, an end-to-end large speech model with fast audio-text token\ngeneration. Specifically, we introduce a lightweight Multiple Cross-modal Token\nPrediction (MCTP) module that efficiently generates multiple audio tokens\nwithin a single model forward pass, which not only accelerates the inference\nbut also significantly reduces the latency for generating the first audio in\nstreaming scenarios. In addition, a four-stage progressive training strategy is\nexplored to achieve model acceleration with minimal loss of speech quality. To\nour knowledge, VITA-Audio is the first multi-modal large language model capable\nof generating audio output during the first forward pass, enabling real-time\nconversational capabilities with minimal latency. VITA-Audio is fully\nreproducible and is trained on open-source data only. Experimental results\ndemonstrate that our model achieves an inference speedup of 3~5x at the 7B\nparameter scale, but also significantly outperforms open-source models of\nsimilar model size on multiple benchmarks for automatic speech recognition\n(ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.",
      "tldr_zh": "VITA-Audio 是一种端到端的大型语音模型，旨在加速语音交互系统中的音频生成速度，解决现有模型在流式传输中首次生成音频token时延迟过高的问题。该模型引入了一个轻量级的多跨模态Token预测 (MCTP) 模块，可以在单个模型前向传递中高效生成多个音频token，从而加速推理并显著减少首次音频生成的延迟。通过四阶段渐进式训练策略，VITA-Audio 在保证语音质量的同时实现了模型加速。实验结果表明，VITA-Audio 在 7B 参数规模下实现了 3~5 倍的推理加速，并在自动语音识别 (ASR)、文本到语音 (TTS) 和口语问答 (SQA) 等多个基准测试中优于类似规模的开源模型。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Training and Inference Codes: https://github.com/VITA-MLLM/VITA-Audio",
      "pdf_url": "http://arxiv.org/pdf/2505.03739v1",
      "published_date": "2025-05-06 17:59:53 UTC",
      "updated_date": "2025-05-06 17:59:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:16:38.470470"
    },
    {
      "arxiv_id": "2505.03738v1",
      "title": "AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control",
      "title_zh": "AMO：用于超灵巧人形机器人全身控制的自适应运动优化\n",
      "authors": [
        "Jialong Li",
        "Xuxin Cheng",
        "Tianshu Huang",
        "Shiqi Yang",
        "Ri-Zhao Qiu",
        "Xiaolong Wang"
      ],
      "abstract": "Humanoid robots derive much of their dexterity from hyper-dexterous\nwhole-body movements, enabling tasks that require a large operational\nworkspace: such as picking objects off the ground. However, achieving these\ncapabilities on real humanoids remains challenging due to their high degrees of\nfreedom (DoF) and nonlinear dynamics. We propose Adaptive Motion Optimization\n(AMO), a framework that integrates sim-to-real reinforcement learning (RL) with\ntrajectory optimization for real-time, adaptive whole-body control. To mitigate\ndistribution bias in motion imitation RL, we construct a hybrid AMO dataset and\ntrain a network capable of robust, on-demand adaptation to potentially O.O.D.\ncommands. We validate AMO in simulation and on a 29-DoF Unitree G1 humanoid\nrobot, demonstrating superior stability and an expanded workspace compared to\nstrong baselines. Finally, we show that AMO's consistent performance supports\nautonomous task execution via imitation learning, underscoring the system's\nversatility and robustness.",
      "tldr_zh": "本文提出了一种自适应运动优化(Adaptive Motion Optimization, AMO)框架，用于实现超灵巧人形机器人全身控制。该框架结合了从仿真到真实(sim-to-real)的强化学习(RL)和轨迹优化，以实现实时的自适应全身控制。为了减轻运动模仿强化学习中的分布偏差，作者构建了一个混合的AMO数据集，并训练了一个网络，使其能够对潜在的超出分布(O.O.D.)的命令进行鲁棒的按需适应。在29自由度的宇树G1人形机器人上的实验验证表明，AMO相比于其他基线方法，表现出更强的稳定性和更大的工作空间。最后，研究表明AMO的一致性能支持通过模仿学习进行自主任务执行，突出了系统的多功能性和鲁棒性。\n",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "website: https://amo-humanoid.github.io",
      "pdf_url": "http://arxiv.org/pdf/2505.03738v1",
      "published_date": "2025-05-06 17:59:51 UTC",
      "updated_date": "2025-05-06 17:59:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:16:49.090037"
    },
    {
      "arxiv_id": "2505.03730v1",
      "title": "FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios",
      "title_zh": "FlexiAct：迈向异构场景中的灵活动作控制\n",
      "authors": [
        "Shiyi Zhang",
        "Junhao Zhuang",
        "Zhaoyang Zhang",
        "Ying Shan",
        "Yansong Tang"
      ],
      "abstract": "Action customization involves generating videos where the subject performs\nactions dictated by input control signals. Current methods use pose-guided or\nglobal motion customization but are limited by strict constraints on spatial\nstructure, such as layout, skeleton, and viewpoint consistency, reducing\nadaptability across diverse subjects and scenarios. To overcome these\nlimitations, we propose FlexiAct, which transfers actions from a reference\nvideo to an arbitrary target image. Unlike existing methods, FlexiAct allows\nfor variations in layout, viewpoint, and skeletal structure between the subject\nof the reference video and the target image, while maintaining identity\nconsistency. Achieving this requires precise action control, spatial structure\nadaptation, and consistency preservation. To this end, we introduce RefAdapter,\na lightweight image-conditioned adapter that excels in spatial adaptation and\nconsistency preservation, surpassing existing methods in balancing appearance\nconsistency and structural flexibility. Additionally, based on our\nobservations, the denoising process exhibits varying levels of attention to\nmotion (low frequency) and appearance details (high frequency) at different\ntimesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike\nexisting methods that rely on separate spatial-temporal architectures, directly\nachieves action extraction during the denoising process. Experiments\ndemonstrate that our method effectively transfers actions to subjects with\ndiverse layouts, skeletons, and viewpoints. We release our code and model\nweights to support further research at\nhttps://shiyi-zh0408.github.io/projectpages/FlexiAct/",
      "tldr_zh": "FlexiAct 提出了一种灵活的动作控制方法，旨在将参考视频中的动作迁移到任意目标图像上，克服了现有方法在空间结构上的严格约束，允许目标图像在布局、视角和骨骼结构上与参考视频存在差异。该方法引入了 RefAdapter，一个轻量级的图像条件适配器，擅长空间适应和一致性保持。此外，FlexiAct 提出了 FAE (Frequency-aware Action Extraction)，在去噪过程中直接提取动作，无需单独的时空架构。实验表明，FlexiAct 能够有效地将动作迁移到具有不同布局、骨骼和视角的角色上，在外观一致性和结构灵活性之间取得了平衡。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by Siggraph2025, Project Page:\n  https://shiyi-zh0408.github.io/projectpages/FlexiAct/",
      "pdf_url": "http://arxiv.org/pdf/2505.03730v1",
      "published_date": "2025-05-06 17:58:02 UTC",
      "updated_date": "2025-05-06 17:58:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:17:01.039935"
    },
    {
      "arxiv_id": "2505.03710v1",
      "title": "Actor-Critics Can Achieve Optimal Sample Efficiency",
      "title_zh": "Actor-Critics算法能够实现最优的样本效率\n",
      "authors": [
        "Kevin Tan",
        "Wei Fan",
        "Yuting Wei"
      ],
      "abstract": "Actor-critic algorithms have become a cornerstone in reinforcement learning\n(RL), leveraging the strengths of both policy-based and value-based methods.\nDespite recent progress in understanding their statistical efficiency, no\nexisting work has successfully learned an $\\epsilon$-optimal policy with a\nsample complexity of $O(1/\\epsilon^2)$ trajectories with general function\napproximation when strategic exploration is necessary.\n  We address this open problem by introducing a novel actor-critic algorithm\nthat attains a sample-complexity of $O(dH^5 \\log|\\mathcal{A}|/\\epsilon^2 + d\nH^4 \\log|\\mathcal{F}|/ \\epsilon^2)$ trajectories, and accompanying $\\sqrt{T}$\nregret when the Bellman eluder dimension $d$ does not increase with $T$ at more\nthan a $\\log T$ rate.\n  Here, $\\mathcal{F}$ is the critic function class, $\\mathcal{A}$ is the action\nspace, and $H$ is the horizon in the finite horizon MDP setting. Our algorithm\nintegrates optimism, off-policy critic estimation targeting the optimal\nQ-function, and rare-switching policy resets.\n  We extend this to the setting of Hybrid RL, showing that initializing the\ncritic with offline data yields sample efficiency gains compared to purely\noffline or online RL. Further, utilizing access to offline data, we provide a\n\\textit{non-optimistic} provably efficient actor-critic algorithm that only\nadditionally requires $N_{\\text{off}} \\geq c_{\\text{off}}^*dH^4/\\epsilon^2$ in\nexchange for omitting optimism, where $c_{\\text{off}}^*$ is the single-policy\nconcentrability coefficient and $N_{\\text{off}}$ is the number of offline\nsamples. This addresses another open problem in the literature. We further\nprovide numerical experiments to support our theoretical findings.",
      "tldr_zh": "该论文提出了一种新的Actor-Critic算法，在具有通用函数逼近的强化学习(RL)中，实现了$\\epsilon$-最优策略的学习，其样本复杂度为$O(dH^5 \\log|\\mathcal{A}|/\\epsilon^2 + d H^4 \\log|\\mathcal{F}|/ \\epsilon^2)$ trajectories，并伴随$\\sqrt{T}$ regret。该算法结合了乐观主义(optimism)、针对最优Q函数的off-policy critic估计和罕见切换策略重置(rare-switching policy resets)。此外，论文还将其扩展到混合强化学习(Hybrid RL)设置，证明使用离线数据初始化critic可以提高样本效率。论文还提供了一个非乐观的(non-optimistic) Actor-Critic算法，只需要额外的$N_{\\text{off}} \\geq c_{\\text{off}}^*dH^4/\\epsilon^2$离线样本，并提供了数值实验来支持理论结果。\n",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "Accepted to ICML 2025",
      "pdf_url": "http://arxiv.org/pdf/2505.03710v1",
      "published_date": "2025-05-06 17:32:39 UTC",
      "updated_date": "2025-05-06 17:32:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:17:13.286362"
    },
    {
      "arxiv_id": "2505.03694v1",
      "title": "Demonstrating ViSafe: Vision-enabled Safety for High-speed Detect and Avoid",
      "title_zh": "ViSafe演示：视觉支持的高速探测与规避安全系统\n",
      "authors": [
        "Parv Kapoor",
        "Ian Higgins",
        "Nikhil Keetha",
        "Jay Patrikar",
        "Brady Moon",
        "Zelin Ye",
        "Yao He",
        "Ivan Cisneros",
        "Yaoyu Hu",
        "Changliu Liu",
        "Eunsuk Kang",
        "Sebastian Scherer"
      ],
      "abstract": "Assured safe-separation is essential for achieving seamless high-density\noperation of airborne vehicles in a shared airspace. To equip\nresource-constrained aerial systems with this safety-critical capability, we\npresent ViSafe, a high-speed vision-only airborne collision avoidance system.\nViSafe offers a full-stack solution to the Detect and Avoid (DAA) problem by\ntightly integrating a learning-based edge-AI framework with a custom\nmulti-camera hardware prototype designed under SWaP-C constraints. By\nleveraging perceptual input-focused control barrier functions (CBF) to design,\nencode, and enforce safety thresholds, ViSafe can provide provably safe runtime\nguarantees for self-separation in high-speed aerial operations. We evaluate\nViSafe's performance through an extensive test campaign involving both\nsimulated digital twins and real-world flight scenarios. By independently\nvarying agent types, closure rates, interaction geometries, and environmental\nconditions (e.g., weather and lighting), we demonstrate that ViSafe\nconsistently ensures self-separation across diverse scenarios. In\nfirst-of-its-kind real-world high-speed collision avoidance tests with closure\nrates reaching 144 km/h, ViSafe sets a new benchmark for vision-only autonomous\ncollision avoidance, establishing a new standard for safety in high-speed\naerial navigation.",
      "tldr_zh": "ViSafe是一个高速、纯视觉的空中防撞系统，旨在为资源受限的空中系统提供安全保障。该系统通过将基于学习的边缘AI框架与定制的多摄像头硬件原型紧密结合，提供了一个完整的“探测与规避”(DAA)解决方案。ViSafe利用感知输入聚焦的控制障碍函数(CBF)来设计、编码和执行安全阈值，从而为高速空中作业中的自隔离提供可证明的安全运行时保证。通过模拟和真实飞行场景的大量测试，ViSafe在各种场景下都能确保自隔离，并在高达144公里/小时的真实高速防撞测试中，为纯视觉自主防撞设定了新的基准。\n",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "13 pages, RSS 2025 Demo track",
      "pdf_url": "http://arxiv.org/pdf/2505.03694v1",
      "published_date": "2025-05-06 16:59:54 UTC",
      "updated_date": "2025-05-06 16:59:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:17:24.962742"
    },
    {
      "arxiv_id": "2505.03678v1",
      "title": "Graph Drawing for LLMs: An Empirical Evaluation",
      "title_zh": "LLM 的图绘制：一项实证评估\n",
      "authors": [
        "Walter Didimo",
        "Fabrizio Montecchiani",
        "Tommaso Piselli"
      ],
      "abstract": "Our work contributes to the fast-growing literature on the use of Large\nLanguage Models (LLMs) to perform graph-related tasks. In particular, we focus\non usage scenarios that rely on the visual modality, feeding the model with a\ndrawing of the graph under analysis. We investigate how the model's performance\nis affected by the chosen layout paradigm, the aesthetics of the drawing, and\nthe prompting technique used for the queries. We formulate three corresponding\nresearch questions and present the results of a thorough experimental analysis.\nOur findings reveal that choosing the right layout paradigm and optimizing the\nreadability of the input drawing from a human perspective can significantly\nimprove the performance of the model on the given task. Moreover, selecting the\nmost effective prompting technique is a challenging yet crucial task for\nachieving optimal performance.",
      "tldr_zh": "该研究对大型语言模型(LLMs)在图相关任务中的应用进行了实证评估，特别是依赖视觉模态，即输入图的绘制。研究探讨了模型性能如何受到布局范式、图的绘制美学以及查询提示技术的影响。实验结果表明，选择合适的布局范式和优化输入图的可读性可以显著提高模型性能。此外，选择最有效的提示技术对于获得最佳性能至关重要。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03678v1",
      "published_date": "2025-05-06 16:23:42 UTC",
      "updated_date": "2025-05-06 16:23:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:17:36.662071"
    },
    {
      "arxiv_id": "2505.03674v1",
      "title": "Gap the (Theory of) Mind: Sharing Beliefs About Teammates' Goals Boosts Collaboration Perception, Not Performance",
      "title_zh": "弥合（心智）理论的差距：分享关于队友目标的信念可以提升协作感知，而非协作表现\n",
      "authors": [
        "Yotam Amitai",
        "Reuth Mirsky",
        "Ofra Amir"
      ],
      "abstract": "In human-agent teams, openly sharing goals is often assumed to enhance\nplanning, collaboration, and effectiveness. However, direct communication of\nthese goals is not always feasible, requiring teammates to infer their\npartner's intentions through actions. Building on this, we investigate whether\nan AI agent's ability to share its inferred understanding of a human teammate's\ngoals can improve task performance and perceived collaboration. Through an\nexperiment comparing three conditions-no recognition (NR), viable goals (VG),\nand viable goals on-demand (VGod) - we find that while goal-sharing information\ndid not yield significant improvements in task performance or overall\nsatisfaction scores, thematic analysis suggests that it supported strategic\nadaptations and subjective perceptions of collaboration. Cognitive load\nassessments revealed no additional burden across conditions, highlighting the\nchallenge of balancing informativeness and simplicity in human-agent\ninteractions. These findings highlight the nuanced trade-off of goal-sharing:\nwhile it fosters trust and enhances perceived collaboration, it can\noccasionally hinder objective performance gains.",
      "tldr_zh": "该研究探讨了在人机协作中，AI智能体分享其对人类队友目标的推断理解，是否能提升任务表现和协作感知。实验对比了三种情况：无识别(NR)、可行目标(VG)和按需可行目标(VGod)。结果表明，目标分享信息并未显著提升任务表现或总体满意度，但主题分析表明其支持了战略适应和主观协作感知。认知负荷评估显示各条件下没有额外负担。研究强调了目标分享的微妙权衡：虽然它能培养信任并增强协作感知，但有时会阻碍客观表现的提升。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03674v1",
      "published_date": "2025-05-06 16:15:24 UTC",
      "updated_date": "2025-05-06 16:15:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:17:48.878016"
    },
    {
      "arxiv_id": "2505.03668v1",
      "title": "Learning Symbolic Persistent Macro-Actions for POMDP Solving Over Time",
      "title_zh": "学习用于随时间推移解决 POMDP 问题的符号持久性宏动作",
      "authors": [
        "Celeste Veronese",
        "Daniele Meli",
        "Alessandro Farinelli"
      ],
      "abstract": "This paper proposes an integration of temporal logical reasoning and\nPartially Observable Markov Decision Processes (POMDPs) to achieve\ninterpretable decision-making under uncertainty with macro-actions. Our method\nleverages a fragment of Linear Temporal Logic (LTL) based on Event Calculus\n(EC) to generate \\emph{persistent} (i.e., constant) macro-actions, which guide\nMonte Carlo Tree Search (MCTS)-based POMDP solvers over a time horizon,\nsignificantly reducing inference time while ensuring robust performance. Such\nmacro-actions are learnt via Inductive Logic Programming (ILP) from a few\ntraces of execution (belief-action pairs), thus eliminating the need for\nmanually designed heuristics and requiring only the specification of the POMDP\ntransition model. In the Pocman and Rocksample benchmark scenarios, our learned\nmacro-actions demonstrate increased expressiveness and generality when compared\nto time-independent heuristics, indeed offering substantial computational\nefficiency improvements.",
      "tldr_zh": "该论文提出了一种结合时序逻辑推理和部分可观测马尔可夫决策过程(POMDPs)的方法，旨在利用宏动作实现不确定性下的可解释决策。该方法利用基于事件演算(EC)的线性时序逻辑(LTL)片段生成*持久性*(即常量)宏动作，指导基于蒙特卡洛树搜索(MCTS)的POMDP求解器在时间范围内进行求解，从而显著减少推理时间，同时确保稳健的性能。这些宏动作通过归纳逻辑编程(ILP)从少量的执行轨迹(信念-动作对)中学习，无需手动设计的启发式方法，只需要指定POMDP转移模型。在Pocman和Rocksample基准测试场景中，与时间无关的启发式方法相比，我们学习到的宏动作表现出更高的表达性和通用性，确实提供了显著的计算效率提升。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at 9th Conference on Neurosymbolic Learning and Reasoning",
      "pdf_url": "http://arxiv.org/pdf/2505.03668v1",
      "published_date": "2025-05-06 16:08:55 UTC",
      "updated_date": "2025-05-06 16:08:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:18:01.132561"
    },
    {
      "arxiv_id": "2505.03662v1",
      "title": "Revolutionizing Brain Tumor Imaging: Generating Synthetic 3D FA Maps from T1-Weighted MRI using CycleGAN Models",
      "title_zh": "脑肿瘤影像的革新：使用 CycleGAN 模型从 T1 加权 MRI 生成合成 3D FA 图\n",
      "authors": [
        "Xin Du",
        "Francesca M. Cozzi",
        "Rajesh Jena"
      ],
      "abstract": "Fractional anisotropy (FA) and directionally encoded colour (DEC) maps are\nessential for evaluating white matter integrity and structural connectivity in\nneuroimaging. However, the spatial misalignment between FA maps and\ntractography atlases hinders their effective integration into predictive\nmodels. To address this issue, we propose a CycleGAN based approach for\ngenerating FA maps directly from T1-weighted MRI scans, representing the first\napplication of this technique to both healthy and tumour-affected tissues. Our\nmodel, trained on unpaired data, produces high fidelity maps, which have been\nrigorously evaluated using Structural Similarity Index (SSIM) and Peak\nSignal-to-Noise Ratio (PSNR), demonstrating particularly robust performance in\ntumour regions. Radiological assessments further underscore the model's\npotential to enhance clinical workflows by providing an AI-driven alternative\nthat reduces the necessity for additional scans.",
      "tldr_zh": "该研究提出了一种基于CycleGAN的方法，用于从T1加权MRI图像直接生成3D FA（Fractional Anisotropy）图，旨在解决FA图与神经影像中的纤维束追踪图谱之间的空间错位问题。该模型使用非配对数据进行训练，能够生成高保真度的FA图，尤其在肿瘤区域表现出强大的性能。通过SSIM（Structural Similarity Index）和PSNR（Peak Signal-to-Noise Ratio）的评估，证实了该模型的有效性。放射学评估表明，该方法有望通过提供AI驱动的替代方案来增强临床工作流程，减少对额外扫描的需求。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68U10"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages",
      "pdf_url": "http://arxiv.org/pdf/2505.03662v1",
      "published_date": "2025-05-06 16:05:22 UTC",
      "updated_date": "2025-05-06 16:05:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:18:12.958726"
    },
    {
      "arxiv_id": "2505.03655v1",
      "title": "Counterfactual Inference for Eliminating Sentiment Bias in Recommender Systems",
      "title_zh": "利用反事实推理消除推荐系统中的情感偏见\n",
      "authors": [
        "Le Pan",
        "Yuanjiang Cao",
        "Chengkai Huang",
        "Wenjie Zhang",
        "Lina Yao"
      ],
      "abstract": "Recommender Systems (RSs) aim to provide personalized recommendations for\nusers. A newly discovered bias, known as sentiment bias, uncovers a common\nphenomenon within Review-based RSs (RRSs): the recommendation accuracy of users\nor items with negative reviews deteriorates compared with users or items with\npositive reviews. Critical users and niche items are disadvantaged by such\nunfair recommendations. We study this problem from the perspective of\ncounterfactual inference with two stages. At the model training stage, we build\na causal graph and model how sentiment influences the final rating score.\nDuring the inference stage, we decouple the direct and indirect effects to\nmitigate the impact of sentiment bias and remove the indirect effect using\ncounterfactual inference. We have conducted extensive experiments, and the\nresults validate that our model can achieve comparable performance on rating\nprediction for better recommendations and effective mitigation of sentiment\nbias. To the best of our knowledge, this is the first work to employ\ncounterfactual inference on sentiment bias mitigation in RSs.",
      "tldr_zh": "该论文研究了推荐系统中存在的“情感偏差”问题，即负面评价用户或物品的推荐准确率低于正面评价的情况。作者采用反事实推理(counterfactual inference)的方法来消除这种偏差。在模型训练阶段，构建因果图来建模情感如何影响最终评分；在推理阶段，解耦直接和间接效应，并使用反事实推理消除间接效应，从而减轻情感偏差的影响。实验结果表明，该模型在保证推荐性能的同时，能够有效缓解情感偏差。这是首个将反事实推理应用于推荐系统中情感偏差缓解的工作。\n",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03655v1",
      "published_date": "2025-05-06 16:00:41 UTC",
      "updated_date": "2025-05-06 16:00:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:18:24.935616"
    },
    {
      "arxiv_id": "2505.03654v1",
      "title": "ReGraP-LLaVA: Reasoning enabled Graph-based Personalized Large Language and Vision Assistant",
      "title_zh": "ReGraP-LLaVA：基于推理的图结构个性化大型语言和视觉助手\n",
      "authors": [
        "Yifan Xiang",
        "Zhenxi Zhang",
        "Bin Li",
        "Yixuan Weng",
        "Shoujun Zhou",
        "Yangfan He",
        "Keqin Li"
      ],
      "abstract": "Recent advances in personalized MLLMs enable effective capture of\nuser-specific concepts, supporting both recognition of personalized concepts\nand contextual captioning. However, humans typically explore and reason over\nrelations among objects and individuals, transcending surface-level information\nto achieve more personalized and contextual understanding. To this end,\nexisting methods may face three main limitations: Their training data lacks\nmulti-object sets in which relations among objects are learnable. Building on\nthe limited training data, their models overlook the relations between\ndifferent personalized concepts and fail to reason over them. Their experiments\nmainly focus on a single personalized concept, where evaluations are limited to\nrecognition and captioning tasks. To address the limitations, we present a new\ndataset named ReGraP, consisting of 120 sets of personalized knowledge. Each\nset includes images, KGs, and CoT QA pairs derived from the KGs, enabling more\nstructured and sophisticated reasoning pathways. We propose ReGraP-LLaVA, an\nMLLM trained with the corresponding KGs and CoT QA pairs, where soft and hard\ngraph prompting methods are designed to align KGs within the model's semantic\nspace. We establish the ReGraP Benchmark, which contains diverse task types:\nmultiple-choice, fill-in-the-blank, True/False, and descriptive questions in\nboth open- and closed-ended settings. The proposed benchmark is designed to\nevaluate the relational reasoning and knowledge-connection capability of\npersonalized MLLMs. We conduct experiments on the proposed ReGraP-LLaVA and\nother competitive MLLMs. Results show that the proposed model not only learns\npersonalized knowledge but also performs relational reasoning in responses,\nachieving the SoTA performance compared with the competitive methods. All the\ncodes and datasets are released at: https://github.com/xyfyyds/ReGraP.",
      "tldr_zh": "该论文提出了ReGraP-LLaVA，一个基于图的个性化大型语言和视觉助手，旨在提升模型在个性化概念间关系上的推理能力。为了解决现有方法训练数据不足、忽略概念间关系以及评估任务单一等问题，作者构建了一个新的数据集ReGraP，包含图像、知识图谱(KG)和链式思维问答对(CoT QA pairs)。ReGraP-LLaVA利用KG和CoT QA pairs进行训练，并设计了软硬图提示方法来对齐KG与模型的语义空间。实验结果表明，ReGraP-LLaVA不仅学习了个性化知识，还能在回答中进行关系推理，并在ReGraP基准测试中取得了最佳性能(SoTA)。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Work in progress",
      "pdf_url": "http://arxiv.org/pdf/2505.03654v1",
      "published_date": "2025-05-06 16:00:13 UTC",
      "updated_date": "2025-05-06 16:00:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:18:37.192784"
    },
    {
      "arxiv_id": "2505.03648v1",
      "title": "Binding threshold units with artificial oscillatory neurons",
      "title_zh": "用人工振荡神经元绑定阈值单元\n",
      "authors": [
        "Vladimir Fanaskov",
        "Ivan Oseledets"
      ],
      "abstract": "Artificial Kuramoto oscillatory neurons were recently introduced as an\nalternative to threshold units. Empirical evidence suggests that oscillatory\nunits outperform threshold units in several tasks including unsupervised object\ndiscovery and certain reasoning problems. The proposed coupling mechanism for\nthese oscillatory neurons is heterogeneous, combining a generalized Kuramoto\nequation with standard coupling methods used for threshold units. In this\nresearch note, we present a theoretical framework that clearly distinguishes\noscillatory neurons from threshold units and establishes a coupling mechanism\nbetween them. We argue that, from a biological standpoint, oscillatory and\nthreshold units realise distinct aspects of neural coding: roughly, threshold\nunits model intensity of neuron firing, while oscillatory units facilitate\ninformation exchange by frequency modulation. To derive interaction between\nthese two types of units, we constrain their dynamics by focusing on dynamical\nsystems that admit Lyapunov functions. For threshold units, this leads to\nHopfield associative memory model, and for oscillatory units it yields a\nspecific form of generalized Kuramoto model. The resulting dynamical systems\ncan be naturally coupled to form a Hopfield-Kuramoto associative memory model,\nwhich also admits a Lyapunov function. Various forms of coupling are possible.\nNotably, oscillatory neurons can be employed to implement a low-rank correction\nto the weight matrix of a Hopfield network. This correction can be viewed\neither as a form of Hebbian learning or as a popular LoRA method used for\nfine-tuning of large language models. We demonstrate the practical realization\nof this particular coupling through illustrative toy experiments.",
      "tldr_zh": "本文提出了一个理论框架，用于区分人工Kuramoto振荡神经元和阈值单元，并建立了它们之间的耦合机制。振荡神经元通过频率调制促进信息交换，而阈值单元模拟神经元放电强度。通过引入Lyapunov函数约束动力学，推导出Hopfield联想记忆模型（针对阈值单元）和广义Kuramoto模型（针对振荡单元）。由此构建了Hopfield-Kuramoto联想记忆模型，该模型允许各种耦合形式，其中振荡神经元可用于实现Hopfield网络权重矩阵的低秩校正，类似于Hebbian学习或LoRA方法。通过实验验证了该耦合的实际应用。\n",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03648v1",
      "published_date": "2025-05-06 15:54:52 UTC",
      "updated_date": "2025-05-06 15:54:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:18:48.978942"
    },
    {
      "arxiv_id": "2505.03646v1",
      "title": "ALMA: Aggregated Lipschitz Maximization Attack on Auto-encoders",
      "title_zh": "ALMA：针对自编码器的聚合 Lipschitz 最大化攻击\n",
      "authors": [
        "Chethan Krishnamurthy Ramanaik",
        "Arjun Roy",
        "Eirini Ntoutsi"
      ],
      "abstract": "Despite the extensive use of deep autoencoders (AEs) in critical\napplications, their adversarial robustness remains relatively underexplored\ncompared to classification models. AE robustness is characterized by the\nLipschitz bounds of its components. Existing robustness evaluation frameworks\nbased on white-box attacks do not fully exploit the vulnerabilities of\nintermediate ill-conditioned layers in AEs. In the context of optimizing\nimperceptible norm-bounded additive perturbations to maximize output damage,\nexisting methods struggle to effectively propagate adversarial loss gradients\nthroughout the network, often converging to less effective perturbations. To\naddress this, we propose a novel layer-conditioning-based adversarial\noptimization objective that effectively guides the adversarial map toward\nregions of local Lipschitz bounds by enhancing loss gradient information\npropagation during attack optimization. We demonstrate through extensive\nexperiments on state-of-the-art AEs that our adversarial objective results in\nstronger attacks, outperforming existing methods in both universal and\nsample-specific scenarios. As a defense method against this attack, we\nintroduce an inference-time adversarially trained defense plugin that mitigates\nthe effects of adversarial examples.",
      "tldr_zh": "该论文提出了一种新的针对自编码器(AEs)的攻击方法，名为聚合Lipschitz最大化攻击(ALMA)。ALMA通过优化一个基于层条件(layer-conditioning)的对抗目标函数，增强损失梯度信息在网络中的传播，从而更有效地利用AE中间层的不良条件(ill-conditioned layers)的脆弱性。实验证明，ALMA在通用和样本特定场景下均优于现有的攻击方法，能产生更强的对抗样本。此外，论文还提出了一种推理时对抗训练防御插件，以减轻对抗样本的影响，作为针对ALMA攻击的防御手段。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03646v1",
      "published_date": "2025-05-06 15:52:14 UTC",
      "updated_date": "2025-05-06 15:52:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:19:00.859008"
    },
    {
      "arxiv_id": "2505.03643v1",
      "title": "BURNS: Backward Underapproximate Reachability for Neural-Feedback-Loop Systems",
      "title_zh": "BURNS：用于神经反馈回路系统的后向欠近似可达性分析\n",
      "authors": [
        "Chelsea Sidrane",
        "Jana Tumova"
      ],
      "abstract": "Learning-enabled planning and control algorithms are increasingly popular,\nbut they often lack rigorous guarantees of performance or safety. We introduce\nan algorithm for computing underapproximate backward reachable sets of\nnonlinear discrete time neural feedback loops. We then use the backward\nreachable sets to check goal-reaching properties. Our algorithm is based on\noverapproximating the system dynamics function to enable computation of\nunderapproximate backward reachable sets through solutions of mixed-integer\nlinear programs. We rigorously analyze the soundness of our algorithm and\ndemonstrate it on a numerical example. Our work expands the class of properties\nthat can be verified for learning-enabled systems.",
      "tldr_zh": "该论文提出了一种名为BURNS的算法，用于计算非线性离散时间神经反馈回路系统的后向可达集下近似。该算法通过过近似系统动力学函数，并借助混合整数线性规划的求解，来实现后向可达集的下近似计算。BURNS算法可用于验证学习使能系统的目标可达性。论文对算法的可靠性进行了严格分析，并通过数值实验验证了其有效性，扩展了学习使能系统可验证属性的范围。\n",
      "categories": [
        "cs.AI",
        "cs.LO",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03643v1",
      "published_date": "2025-05-06 15:50:43 UTC",
      "updated_date": "2025-05-06 15:50:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:19:12.902820"
    },
    {
      "arxiv_id": "2505.03641v1",
      "title": "Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering and Manipulating Human Perceptual Variability",
      "title_zh": "在人工神经网络感知边界上合成图像，以揭示和操纵人类感知变异性\n",
      "authors": [
        "Chen Wei",
        "Chi Zhang",
        "Jiachen Zou",
        "Haotian Deng",
        "Dietmar Heinke",
        "Quanying Liu"
      ],
      "abstract": "Human decision-making in cognitive tasks and daily life exhibits considerable\nvariability, shaped by factors such as task difficulty, individual preferences,\nand personal experiences. Understanding this variability across individuals is\nessential for uncovering the perceptual and decision-making mechanisms that\nhumans rely on when faced with uncertainty and ambiguity. We present a\ncomputational framework BAM (Boundary Alignment & Manipulation framework) that\ncombines perceptual boundary sampling in ANNs and human behavioral experiments\nto systematically investigate this phenomenon. Our perceptual boundary sampling\nalgorithm generates stimuli along ANN decision boundaries that intrinsically\ninduce significant perceptual variability. The efficacy of these stimuli is\nempirically validated through large-scale behavioral experiments involving 246\nparticipants across 116,715 trials, culminating in the variMNIST dataset\ncontaining 19,943 systematically annotated images. Through personalized model\nalignment and adversarial generation, we establish a reliable method for\nsimultaneously predicting and manipulating the divergent perceptual decisions\nof pairs of participants. This work bridges the gap between computational\nmodels and human individual difference research, providing new tools for\npersonalized perception analysis.",
      "tldr_zh": "该论文提出了一个名为BAM（Boundary Alignment & Manipulation framework）的计算框架，用于研究人类在认知任务中的感知变异性。BAM框架结合了人工神经网络（ANNs）中的感知边界采样和人类行为实验，通过在ANN决策边界上生成刺激来诱导显著的感知变异。通过大规模行为实验和variMNIST数据集，验证了这些刺激的有效性。该研究进一步建立了预测和操纵个体感知决策的方法，弥合了计算模型与人类个体差异研究之间的差距，为个性化感知分析提供了新工具。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "accepted at ICML 2025",
      "pdf_url": "http://arxiv.org/pdf/2505.03641v1",
      "published_date": "2025-05-06 15:44:42 UTC",
      "updated_date": "2025-05-06 15:44:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:19:24.845099"
    },
    {
      "arxiv_id": "2505.03586v1",
      "title": "Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning Framework for Mitigating Delayed Observation",
      "title_zh": "彩虹延迟补偿：一种用于缓解延迟观测的多智能体强化学习框架\n",
      "authors": [
        "Songchen Fu",
        "Siang Chen",
        "Shaojing Zhao",
        "Letian Bai",
        "Ta Li",
        "Yonghong Yan"
      ],
      "abstract": "In real-world multi-agent systems (MASs), observation delays are ubiquitous,\npreventing agents from making decisions based on the environment's true state.\nAn individual agent's local observation often consists of multiple components\nfrom other agents or dynamic entities in the environment. These discrete\nobservation components with varying delay characteristics pose significant\nchallenges for multi-agent reinforcement learning (MARL). In this paper, we\nfirst formulate the decentralized stochastic individual delay partially\nobservable Markov decision process (DSID-POMDP) by extending the standard\nDec-POMDP. We then propose the Rainbow Delay Compensation (RDC), a MARL\ntraining framework for addressing stochastic individual delays, along with\nrecommended implementations for its constituent modules. We implement the\nDSID-POMDP's observation generation pattern using standard MARL benchmarks,\nincluding MPE and SMAC. Experiments demonstrate that baseline MARL methods\nsuffer severe performance degradation under fixed and unfixed delays. The\nRDC-enhanced approach mitigates this issue, remarkably achieving ideal\ndelay-free performance in certain delay scenarios while maintaining\ngeneralization capability. Our work provides a novel perspective on multi-agent\ndelayed observation problems and offers an effective solution framework.",
      "tldr_zh": "该论文针对多智能体系统(MASs)中普遍存在的观测延迟问题，提出了Rainbow Delay Compensation (RDC)框架。首先，论文扩展了Dec-POMDP，提出了去中心化随机个体延迟部分可观测马尔可夫决策过程(DSID-POMDP)，用于形式化描述该问题。然后，RDC框架通过多智能体强化学习(MARL)训练，解决了随机个体延迟问题，并提供了模块化的实现建议。在MPE和SMAC等标准MARL基准测试中，实验表明RDC能够有效缓解延迟带来的性能下降，在某些情况下甚至能达到无延迟的理想性能，并保持泛化能力。该研究为多智能体延迟观测问题提供了一个新的视角和有效的解决方案。\n",
      "categories": [
        "cs.MA",
        "cs.AI",
        "68T07 (Primary), 68T20, 68T42 (Secondary)",
        "I.2"
      ],
      "primary_category": "cs.MA",
      "comment": "The code will be open-sourced in the RDC-pymarl project under\n  https://github.com/linkjoker1006",
      "pdf_url": "http://arxiv.org/pdf/2505.03586v1",
      "published_date": "2025-05-06 14:47:56 UTC",
      "updated_date": "2025-05-06 14:47:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:19:37.164898"
    },
    {
      "arxiv_id": "2505.03584v1",
      "title": "BCause: Human-AI collaboration to improve hybrid mapping and ideation in argumentation-grounded deliberation",
      "title_zh": "BCause：人机协作提升论证推理式审议中的混合映射和构思\n",
      "authors": [
        "Lucas Anastasiou",
        "Anna De Liddo"
      ],
      "abstract": "Public deliberation, as in open discussion of issues of public concern, often\nsuffers from scattered and shallow discourse, poor sensemaking, and a\ndisconnect from actionable policy outcomes. This paper introduces BCause, a\ndiscussion system leveraging generative AI and human-machine collaboration to\ntransform unstructured dialogue around public issues (such as urban living,\npolicy changes, and current socio-economic transformations) into structured,\nactionable democratic processes. We present three innovations: (i) importing\nand transforming unstructured transcripts into argumentative discussions, (ii)\ngeo-deliberated problem-sensing via a Telegram bot for local issue reporting,\nand (iii) smart reporting with customizable widgets (e.g., summaries, topic\nmodelling, policy recommendations, clustered arguments). The system's human-AI\npartnership preserves critical human participation to ensure ethical oversight,\ncontextual relevance, and creative synthesis.",
      "tldr_zh": "BCause是一个讨论系统，旨在通过人机协作和生成式AI，将围绕公共议题的非结构化对话转化为结构化的、可操作的民主流程，解决公共审议中存在的讨论分散、理解不足和缺乏实际政策结果等问题。该系统包含三项创新：将非结构化文本转化为论证性讨论；通过Telegram Bot进行地理审议的问题感知；以及通过可定制的组件（如摘要、主题建模、政策建议和聚类论点）进行智能报告。BCause强调人机合作，确保伦理监督、情境相关性和创造性综合。\n",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "I.2"
      ],
      "primary_category": "cs.HC",
      "comment": "5 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2505.03584v1",
      "published_date": "2025-05-06 14:43:49 UTC",
      "updated_date": "2025-05-06 14:43:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:19:49.051080"
    },
    {
      "arxiv_id": "2505.03574v1",
      "title": "LlamaFirewall: An open source guardrail system for building secure AI agents",
      "title_zh": "LlamaFirewall：用于构建安全 AI 代理的开源防护系统\n",
      "authors": [
        "Sahana Chennabasappa",
        "Cyrus Nikolaidis",
        "Daniel Song",
        "David Molnar",
        "Stephanie Ding",
        "Shengye Wan",
        "Spencer Whitman",
        "Lauren Deason",
        "Nicholas Doucette",
        "Abraham Montilla",
        "Alekhya Gampa",
        "Beto de Paola",
        "Dominik Gabi",
        "James Crnkovich",
        "Jean-Christophe Testud",
        "Kat He",
        "Rashnil Chaturvedi",
        "Wu Zhou",
        "Joshua Saxe"
      ],
      "abstract": "Large language models (LLMs) have evolved from simple chatbots into\nautonomous agents capable of performing complex tasks such as editing\nproduction code, orchestrating workflows, and taking higher-stakes actions\nbased on untrusted inputs like webpages and emails. These capabilities\nintroduce new security risks that existing security measures, such as model\nfine-tuning or chatbot-focused guardrails, do not fully address. Given the\nhigher stakes and the absence of deterministic solutions to mitigate these\nrisks, there is a critical need for a real-time guardrail monitor to serve as a\nfinal layer of defense, and support system level, use case specific safety\npolicy definition and enforcement. We introduce LlamaFirewall, an open-source\nsecurity focused guardrail framework designed to serve as a final layer of\ndefense against security risks associated with AI Agents. Our framework\nmitigates risks such as prompt injection, agent misalignment, and insecure code\nrisks through three powerful guardrails: PromptGuard 2, a universal jailbreak\ndetector that demonstrates clear state of the art performance; Agent Alignment\nChecks, a chain-of-thought auditor that inspects agent reasoning for prompt\ninjection and goal misalignment, which, while still experimental, shows\nstronger efficacy at preventing indirect injections in general scenarios than\npreviously proposed approaches; and CodeShield, an online static analysis\nengine that is both fast and extensible, aimed at preventing the generation of\ninsecure or dangerous code by coding agents. Additionally, we include\neasy-to-use customizable scanners that make it possible for any developer who\ncan write a regular expression or an LLM prompt to quickly update an agent's\nsecurity guardrails.",
      "tldr_zh": "LlamaFirewall是一个开源安全防护系统，旨在解决LLM驱动的AI Agent在执行复杂任务时面临的安全风险，例如prompt注入、agent目标不一致和不安全代码生成。该框架包含三个核心防护模块：PromptGuard 2，一种先进的越狱检测器；Agent Alignment Checks，一种链式思维审计器，用于检查agent的推理过程，防止prompt注入和目标错位；以及CodeShield，一个在线静态分析引擎，用于防止生成不安全的代码。LlamaFirewall还提供易于使用的可定制扫描器，允许开发者使用正则表达式或LLM prompt快速更新agent的安全防护规则。该系统旨在作为AI Agent的最后一道防线，支持系统级的、特定用例的安全策略定义和执行。\n",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03574v1",
      "published_date": "2025-05-06 14:34:21 UTC",
      "updated_date": "2025-05-06 14:34:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:20:01.298952"
    },
    {
      "arxiv_id": "2505.03570v1",
      "title": "OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents",
      "title_zh": "OSUniverse：多模态 GUI 导航 AI 智能体的基准测试\n",
      "authors": [
        "Mariya Davydova",
        "Daniel Jeffries",
        "Patrick Barker",
        "Arturo Márquez Flores",
        "Sinéad Ryan"
      ],
      "abstract": "In this paper, we introduce OSUniverse: a benchmark of complex, multimodal\ndesktop-oriented tasks for advanced GUI-navigation AI agents that focuses on\nease of use, extensibility, comprehensive coverage of test cases, and automated\nvalidation. We divide the tasks in increasing levels of complexity, from basic\nprecision clicking to multistep, multiapplication tests requiring dexterity,\nprecision, and clear thinking from the agent. In version one of the benchmark,\npresented here, we have calibrated the complexity of the benchmark test cases\nto ensure that the SOTA (State of the Art) agents (at the time of publication)\ndo not achieve results higher than 50%, while the average white collar worker\ncan perform all these tasks with perfect accuracy. The benchmark can be scored\nmanually, but we also introduce an automated validation mechanism that has an\naverage error rate less than 2%. Therefore, this benchmark presents solid\nground for fully automated measuring of progress, capabilities and the\neffectiveness of GUI-navigation AI agents over the short and medium-term\nhorizon. The source code of the benchmark is available at\nhttps://github.com/agentsea/osuniverse.",
      "tldr_zh": "该论文提出了OSUniverse，一个用于评估多模态GUI导航AI Agent的基准测试。OSUniverse包含一系列复杂度递增的桌面任务，从简单的精确点击到需要灵活性、精确性和清晰思考的多步骤、多应用测试。该基准测试易于使用、可扩展，并具有全面的测试用例和自动验证功能。目前最先进的Agent在该基准测试上的表现低于50%，而普通白领员工可以完美完成所有任务。此外，论文还引入了一种平均错误率低于2%的自动验证机制。OSUniverse为全面自动地衡量GUI导航AI Agent的进展、能力和有效性提供了坚实的基础。项目代码已开源。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03570v1",
      "published_date": "2025-05-06 14:29:47 UTC",
      "updated_date": "2025-05-06 14:29:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:20:13.129823"
    },
    {
      "arxiv_id": "2505.03562v1",
      "title": "Real-Time Person Image Synthesis Using a Flow Matching Model",
      "title_zh": "基于流动匹配模型的实时人体图像合成\n",
      "authors": [
        "Jiwoo Jeong",
        "Kirok Kim",
        "Wooju Kim",
        "Nam-Joon Kim"
      ],
      "abstract": "Pose-Guided Person Image Synthesis (PGPIS) generates realistic person images\nconditioned on a target pose and a source image. This task plays a key role in\nvarious real-world applications, such as sign language video generation, AR/VR,\ngaming, and live streaming. In these scenarios, real-time PGPIS is critical for\nproviding immediate visual feedback and maintaining user immersion.However,\nachieving real-time performance remains a significant challenge due to the\ncomplexity of synthesizing high-fidelity images from diverse and dynamic human\nposes. Recent diffusion-based methods have shown impressive image quality in\nPGPIS, but their slow sampling speeds hinder deployment in time-sensitive\napplications. This latency is particularly problematic in tasks like generating\nsign language videos during live broadcasts, where rapid image updates are\nrequired. Therefore, developing a fast and reliable PGPIS model is a crucial\nstep toward enabling real-time interactive systems. To address this challenge,\nwe propose a generative model based on flow matching (FM). Our approach enables\nfaster, more stable, and more efficient training and sampling. Furthermore, the\nproposed model supports conditional generation and can operate in latent space,\nmaking it especially suitable for real-time PGPIS applications where both speed\nand quality are critical. We evaluate our proposed method, Real-Time Person\nImage Synthesis Using a Flow Matching Model (RPFM), on the widely used\nDeepFashion dataset for PGPIS tasks. Our results show that RPFM achieves\nnear-real-time sampling speeds while maintaining performance comparable to the\nstate-of-the-art models. Our methodology trades off a slight, acceptable\ndecrease in generated-image accuracy for over a twofold increase in generation\nspeed, thereby ensuring real-time performance.",
      "tldr_zh": "该论文提出了一种基于Flow Matching (FM) 的实时人体图像合成模型 (RPFM)，旨在解决姿态引导的人体图像合成 (PGPIS) 任务中实时性的挑战。相较于基于扩散模型的方法，RPFM通过更快速、稳定和高效的训练和采样，显著提升了生成速度。该模型支持条件生成，并在潜在空间中运行，特别适用于对速度和质量都有要求的实时 PGPIS 应用。在 DeepFashion 数据集上的实验结果表明，RPFM 在保持与 SOTA 模型相当性能的同时，实现了接近实时的采样速度，牺牲了少量可接受的精度来换取两倍以上的生成速度提升。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03562v1",
      "published_date": "2025-05-06 14:13:44 UTC",
      "updated_date": "2025-05-06 14:13:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:20:25.121086"
    },
    {
      "arxiv_id": "2505.03561v1",
      "title": "Ergodic Generative Flows",
      "title_zh": "遍历生成流\n",
      "authors": [
        "Leo Maxime Brunswic",
        "Mateo Clemente",
        "Rui Heng Yang",
        "Adam Sigal",
        "Amir Rasouli",
        "Yinchuan Li"
      ],
      "abstract": "Generative Flow Networks (GFNs) were initially introduced on directed acyclic\ngraphs to sample from an unnormalized distribution density. Recent works have\nextended the theoretical framework for generative methods allowing more\nflexibility and enhancing application range. However, many challenges remain in\ntraining GFNs in continuous settings and for imitation learning (IL), including\nintractability of flow-matching loss, limited tests of non-acyclic training,\nand the need for a separate reward model in imitation learning. The present\nwork proposes a family of generative flows called Ergodic Generative Flows\n(EGFs) which are used to address the aforementioned issues. First, we leverage\nergodicity to build simple generative flows with finitely many globally defined\ntransformations (diffeomorphisms) with universality guarantees and tractable\nflow-matching loss (FM loss). Second, we introduce a new loss involving\ncross-entropy coupled to weak flow-matching control, coined KL-weakFM loss. It\nis designed for IL training without a separate reward model. We evaluate\nIL-EGFs on toy 2D tasks and real-world datasets from NASA on the sphere, using\nthe KL-weakFM loss. Additionally, we conduct toy 2D reinforcement learning\nexperiments with a target reward, using the FM loss.",
      "tldr_zh": "本文提出了一种新的生成流模型，名为Ergodic Generative Flows (EGFs)，旨在解决生成流网络(GFNs)在连续环境和模仿学习(IL)中训练时遇到的问题，如流匹配损失的难解性、非循环训练的局限性以及模仿学习中对独立奖励模型的依赖。EGFs利用遍历性构建简单的生成流，该生成流具有有限的全局定义变换(微分同胚)和可处理的流匹配损失(FM loss)。此外，作者引入了一种新的损失函数，即KL-weakFM loss，它将交叉熵与弱流匹配控制相结合，用于在没有独立奖励模型的情况下进行IL训练。在2D玩具任务和NASA球体数据集上的实验表明，使用KL-weakFM loss的IL-EGFs表现良好。同时，在2D玩具强化学习实验中，使用FM loss也取得了不错的结果。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.DG",
        "math.DS",
        "37A25, 68T07, 68W20, 68Q87, 68T99"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 5 figures, 1 table, accepted at ICML 2025",
      "pdf_url": "http://arxiv.org/pdf/2505.03561v1",
      "published_date": "2025-05-06 14:13:21 UTC",
      "updated_date": "2025-05-06 14:13:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:20:37.306515"
    },
    {
      "arxiv_id": "2505.03560v1",
      "title": "Rapid AI-based generation of coverage paths for dispensing applications",
      "title_zh": "基于AI的快速生成点胶应用覆盖路径\n",
      "authors": [
        "Simon Baeuerle",
        "Ian F. Mendonca",
        "Kristof Van Laerhoven",
        "Ralf Mikut",
        "Andreas Steimer"
      ],
      "abstract": "Coverage Path Planning of Thermal Interface Materials (TIM) plays a crucial\nrole in the design of power electronics and electronic control units. Up to\nnow, this is done manually by experts or by using optimization approaches with\na high computational effort. We propose a novel AI-based approach to generate\ndispense paths for TIM and similar dispensing applications. It is a drop-in\nreplacement for optimization-based approaches. An Artificial Neural Network\n(ANN) receives the target cooling area as input and directly outputs the\ndispense path. Our proposed setup does not require labels and we show its\nfeasibility on multiple target areas. The resulting dispense paths can be\ndirectly transferred to automated manufacturing equipment and do not exhibit\nair entrapments. The approach of using an ANN to predict process parameters for\na desired target state in real-time could potentially be transferred to other\nmanufacturing processes.",
      "tldr_zh": "本文提出了一种基于人工智能的快速覆盖路径生成方法，用于热界面材料(TIM)等点胶应用。该方法使用人工神经网络(ANN)直接根据目标冷却区域生成点胶路径，无需标签，可替代传统的优化方法。实验证明，该方法在多个目标区域上可行，生成的点胶路径可直接用于自动化制造设备，且不会产生空气滞留。这种使用ANN实时预测工艺参数的方法有望推广到其他制造工艺中。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03560v1",
      "published_date": "2025-05-06 14:13:20 UTC",
      "updated_date": "2025-05-06 14:13:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:20:48.862999"
    },
    {
      "arxiv_id": "2505.03557v1",
      "title": "Generating Synthetic Data via Augmentations for Improved Facial Resemblance in DreamBooth and InstantID",
      "title_zh": "通过增强生成合成数据，以改进 DreamBooth 和 InstantID 中的面部相似度\n",
      "authors": [
        "Koray Ulusan",
        "Benjamin Kiefer"
      ],
      "abstract": "The personalization of Stable Diffusion for generating professional portraits\nfrom amateur photographs is a burgeoning area, with applications in various\ndownstream contexts. This paper investigates the impact of augmentations on\nimproving facial resemblance when using two prominent personalization\ntechniques: DreamBooth and InstantID. Through a series of experiments with\ndiverse subject datasets, we assessed the effectiveness of various augmentation\nstrategies on the generated headshots' fidelity to the original subject. We\nintroduce FaceDistance, a wrapper around FaceNet, to rank the generations based\non facial similarity, which aided in our assessment. Ultimately, this research\nprovides insights into the role of augmentations in enhancing facial\nresemblance in SDXL-generated portraits, informing strategies for their\neffective deployment in downstream applications.",
      "tldr_zh": "本文研究了数据增强对DreamBooth和InstantID两种Stable Diffusion个性化方法在生成人像时面部相似度的影响。通过在不同人脸数据集上进行实验，评估了各种增强策略对生成头像与原始人物面部保真度的影响。研究引入了基于FaceNet的FaceDistance方法，用于根据面部相似度对生成结果进行排序，辅助评估。实验结果表明，适当的数据增强可以有效提高生成人像的面部相似度，为SDXL生成人像在下游应用中的有效部署提供了策略指导。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CVPR 2025 Workshop \"Synthetic Data for Computer Vision\n  Workshop\", https://syndata4cv.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2505.03557v1",
      "published_date": "2025-05-06 14:11:02 UTC",
      "updated_date": "2025-05-06 14:11:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:21:01.019708"
    },
    {
      "arxiv_id": "2505.03553v1",
      "title": "A Hashgraph-Inspired Consensus Mechanism for Reliable Multi-Model Reasoning",
      "title_zh": "一种受 Hashgraph 启发的共识机制，用于可靠的多模型推理\n",
      "authors": [
        "Kolawole E. Ogunsina",
        "Morayo A. Ogunsina"
      ],
      "abstract": "Inconsistent outputs and hallucinations from large language models (LLMs) are\nmajor obstacles to reliable AI systems. When different proprietary reasoning\nmodels (RMs), such as those by OpenAI, Google, Anthropic, DeepSeek, and xAI,\nare given the same complex request, they often produce divergent results due to\nvariations in training and inference. This paper proposes a novel consensus\nmechanism, inspired by distributed ledger technology, to validate and converge\nthese outputs, treating each RM as a black-box peer. Building on the Hashgraph\nconsensus algorithm, our approach employs gossip-about-gossip communication and\nvirtual voting to achieve agreement among an ensemble of RMs. We present an\narchitectural design for a prototype system in which RMs iteratively exchange\nand update their answers, using information from each round to improve accuracy\nand confidence in subsequent rounds. This approach goes beyond simple majority\nvoting by incorporating the knowledge and cross-verification content of every\nmodel. We justify the feasibility of this Hashgraph-inspired consensus for AI\nensembles and outline its advantages over traditional ensembling techniques in\nreducing nonfactual outputs. Preliminary considerations for implementation,\nevaluation criteria for convergence and accuracy, and potential challenges are\ndiscussed. The proposed mechanism demonstrates a promising direction for\nmulti-agent AI systems to self-validate and deliver high-fidelity responses in\ncomplex tasks.",
      "tldr_zh": "该论文提出了一种受Hashgraph启发的共识机制，旨在解决大型语言模型(LLMs)输出不一致和幻觉问题，提高AI系统的可靠性。该方法将不同的推理模型(RMs)视为黑盒节点，利用gossip-about-gossip通信和虚拟投票，在多个RMs之间达成共识，验证并融合它们的输出。该系统架构允许RMs迭代交换和更新答案，利用每一轮的信息来提高后续轮次的准确性和置信度。该机制超越了简单的多数投票，整合了每个模型的知识和交叉验证内容，减少了非事实性输出，为多智能体AI系统实现自我验证和提供高保真响应提供了一个有前景的方向。\n",
      "categories": [
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages",
      "pdf_url": "http://arxiv.org/pdf/2505.03553v1",
      "published_date": "2025-05-06 14:05:12 UTC",
      "updated_date": "2025-05-06 14:05:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:21:13.273166"
    },
    {
      "arxiv_id": "2505.03547v1",
      "title": "STORY2GAME: Generating (Almost) Everything in an Interactive Fiction Game",
      "title_zh": "STORY2GAME：生成交互式小说游戏中的（几乎）所有内容\n",
      "authors": [
        "Eric Zhou",
        "Shreyas Basavatia",
        "Moontashir Siam",
        "Zexin Chen",
        "Mark O. Riedl"
      ],
      "abstract": "We introduce STORY2GAME, a novel approach to using Large Language Models to\ngenerate text-based interactive fiction games that starts by generating a\nstory, populates the world, and builds the code for actions in a game engine\nthat enables the story to play out interactively. Whereas a given set of\nhard-coded actions can artificially constrain story generation, the ability to\ngenerate actions means the story generation process can be more open-ended but\nstill allow for experiences that are grounded in a game state. The key to\nsuccessful action generation is to use LLM-generated preconditions and effects\nof actions in the stories as guides for what aspects of the game state must be\ntracked and changed by the game engine when a player performs an action. We\nalso introduce a technique for dynamically generating new actions to\naccommodate the player's desire to perform actions that they think of that are\nnot part of the story. Dynamic action generation may require on-the-fly updates\nto the game engine's state representation and revision of previously generated\nactions. We evaluate the success rate of action code generation with respect to\nwhether a player can interactively play through the entire generated story.",
      "tldr_zh": "该论文提出了STORY2GAME，一种利用大型语言模型(LLM)生成文本交互式小说游戏的新方法。该方法首先生成故事，然后填充世界，并构建游戏引擎中动作的代码，使故事能够以交互方式展开。关键在于利用LLM生成的动作的前提条件和效果作为指导，确定游戏状态的哪些方面必须被跟踪和改变。此外，论文还引入了一种动态生成新动作的技术，以适应玩家的想法，并可能需要对游戏引擎的状态表示进行动态更新和对先前生成的动作进行修改。实验评估了动作代码生成的成功率，以确定玩家是否能够以交互方式完成整个生成的故事。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03547v1",
      "published_date": "2025-05-06 14:00:41 UTC",
      "updated_date": "2025-05-06 14:00:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:21:25.165644"
    },
    {
      "arxiv_id": "2505.03522v1",
      "title": "Optimization of Module Transferability in Single Image Super-Resolution: Universality Assessment and Cycle Residual Blocks",
      "title_zh": "单图像超分辨率中模块可迁移性的优化：通用性评估和循环残差块\n",
      "authors": [
        "Haotong Cheng",
        "Zhiqi Zhang",
        "Hao Li",
        "Xinshang Zhang"
      ],
      "abstract": "Deep learning has substantially advanced the Single Image Super-Resolution\n(SISR). However, existing researches have predominantly focused on raw\nperformance gains, with little attention paid to quantifying the\ntransferability of architectural components. In this paper, we introduce the\nconcept of \"Universality\" and its associated definitions which extend the\ntraditional notion of \"Generalization\" to encompass the modules' ease of\ntransferability, thus revealing the relationships between module universality\nand model generalizability. Then we propose the Universality Assessment\nEquation (UAE), a metric for quantifying how readily a given module could be\ntransplanted across models. Guided by the UAE results of standard residual\nblocks and other plug-and-play modules, we further design two optimized\nmodules, Cycle Residual Block (CRB) and Depth-Wise Cycle Residual Block (DCRB).\nThrough comprehensive experiments on natural-scene benchmarks, remote-sensing\ndatasets, extreme-industrial imagery and on-device deployments, we demonstrate\nthat networks embedded with the proposed plug-and-play modules outperform\nseveral state-of-the-arts, reaching a PSNR enhancement of up to 0.83dB or\nenabling a 71.3% reduction in parameters with negligible loss in reconstruction\nfidelity.",
      "tldr_zh": "该论文关注单图像超分辨率(SISR)中模型组件的迁移性问题，提出了“通用性(Universality)”的概念，并使用“通用性评估方程(UAE)”来量化模块在不同模型间的迁移能力。通过UAE评估，论文设计了循环残差块(CRB)和深度循环残差块(DCRB)两种优化模块。实验结果表明，集成这些模块的网络在自然场景、遥感数据集和工业图像等多种数据集上均优于现有方法，PSNR最高提升0.83dB，或在重建精度损失可忽略不计的情况下参数量减少71.3%。该研究为SISR模型组件的迁移性和通用性提供了新的评估和优化方法。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03522v1",
      "published_date": "2025-05-06 13:35:59 UTC",
      "updated_date": "2025-05-06 13:35:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:21:37.384585"
    },
    {
      "arxiv_id": "2505.03510v1",
      "title": "From Neurons to Computation: Biological Reservoir Computing for Pattern Recognition",
      "title_zh": "从神经元到计算：用于模式识别的生物储层计算\n",
      "authors": [
        "Ludovico Iannello",
        "Luca Ciampi",
        "Gabriele Lagani",
        "Fabrizio Tonelli",
        "Eleonora Crocco",
        "Lucio Maria Calcagnile",
        "Angelo Di Garbo",
        "Federico Cremisi",
        "Giuseppe Amato"
      ],
      "abstract": "In this paper, we introduce a novel paradigm for reservoir computing (RC)\nthat leverages a pool of cultured biological neurons as the reservoir\nsubstrate, creating a biological reservoir computing (BRC). This system\noperates similarly to an echo state network (ESN), with the key distinction\nthat the neural activity is generated by a network of cultured neurons, rather\nthan being modeled by traditional artificial computational units. The neuronal\nactivity is recorded using a multi-electrode array (MEA), which enables\nhigh-throughput recording of neural signals. In our approach, inputs are\nintroduced into the network through a subset of the MEA electrodes, while the\nremaining electrodes capture the resulting neural activity. This generates a\nnonlinear mapping of the input data to a high-dimensional biological feature\nspace, where distinguishing between data becomes more efficient and\nstraightforward, allowing a simple linear classifier to perform pattern\nrecognition tasks effectively. To evaluate the performance of our proposed\nsystem, we present an experimental study that includes various input patterns,\nsuch as positional codes, bars with different orientations, and a digit\nrecognition task. The results demonstrate the feasibility of using biological\nneural networks to perform tasks traditionally handled by artificial neural\nnetworks, paving the way for further exploration of biologically-inspired\ncomputing systems, with potential applications in neuromorphic engineering and\nbio-hybrid computing.",
      "tldr_zh": "本文提出了一种新型的生物储层计算(BRC)范例，利用培养的生物神经元作为储层基质。该系统类似于回声状态网络(ESN)，但关键区别在于神经活动由培养的神经元网络产生，而非传统的计算单元建模。通过多电极阵列(MEA)记录神经活动，实现神经信号的高通量记录。输入通过MEA电极子集引入网络，其余电极捕获神经活动，将输入数据非线性映射到高维生物特征空间。实验研究包括位置编码、不同方向的条形和数字识别任务，结果表明使用生物神经网络执行传统人工神经网络任务是可行的，为神经形态工程和生物混合计算的进一步探索铺平了道路。\n",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03510v1",
      "published_date": "2025-05-06 13:20:04 UTC",
      "updated_date": "2025-05-06 13:20:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:21:49.220987"
    },
    {
      "arxiv_id": "2505.03492v1",
      "title": "Augmenting Human Cognition through Everyday AR",
      "title_zh": "通过日常 AR 增强人类认知\n",
      "authors": [
        "Xiaoan Liu"
      ],
      "abstract": "As spatial computing and multimodal LLMs mature, AR is tending to become an\nintuitive \"thinking tool,\" embedding semantic and context-aware intelligence\ndirectly into everyday environments. This paper explores how always-on AR can\nseamlessly bridge digital cognition and physical affordances, enabling\nproactive, context-sensitive interactions that enhance human task performance\nand understanding.",
      "tldr_zh": "本文探讨了增强现实(AR)技术如何随着空间计算和多模态大型语言模型(LLMs)的成熟，发展成为一种直观的“思考工具”，将语义和上下文感知的智能直接嵌入到日常环境中。研究重点在于常时开启的AR如何无缝连接数字认知和物理可供性(physical affordances)，从而实现主动的、上下文敏感的交互，最终提升人类的任务表现和理解能力。该研究旨在探索AR在增强人类认知方面的潜力。\n",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "3 pages, 4 figures. Position paper accepted to CHI'25 Workshop\n  'Everyday AR through AI-in-the-Loop'",
      "pdf_url": "http://arxiv.org/pdf/2505.03492v1",
      "published_date": "2025-05-06 12:48:38 UTC",
      "updated_date": "2025-05-06 12:48:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:22:00.891854"
    },
    {
      "arxiv_id": "2505.03490v1",
      "title": "A new membership inference attack that spots memorization in generative and predictive models: Loss-Based with Reference Model algorithm (LBRM)",
      "title_zh": "一种新的成员推理攻击，可发现生成模型和预测模型中的记忆化：基于损失和参考模型的算法 (LBRM)\n",
      "authors": [
        "Faiz Taleb",
        "Ivan Gazeau",
        "Maryline Laurent"
      ],
      "abstract": "Generative models can unintentionally memorize training data, posing\nsignificant privacy risks. This paper addresses the memorization phenomenon in\ntime series imputation models, introducing the Loss-Based with Reference Model\n(LBRM) algorithm. The LBRM method leverages a reference model to enhance the\naccuracy of membership inference attacks, distinguishing between training and\ntest data. Our contributions are twofold: first, we propose an innovative\nmethod to effectively extract and identify memorized training data,\nsignificantly improving detection accuracy. On average, without fine-tuning,\nthe AUROC improved by approximately 40\\%. With fine-tuning, the AUROC increased\nby approximately 60\\%. Second, we validate our approach through membership\ninference attacks on two types of architectures designed for time series\nimputation, demonstrating the robustness and versatility of the LBRM approach\nin different contexts. These results highlight the significant enhancement in\ndetection accuracy provided by the LBRM approach, addressing privacy risks in\ntime series imputation models.",
      "tldr_zh": "该论文提出了一种新的成员推断攻击算法LBRM（Loss-Based with Reference Model），用于检测生成模型和预测模型中的记忆化现象，尤其关注时间序列插补模型。LBRM算法利用参考模型来提高成员推断攻击的准确性，区分训练数据和测试数据。实验结果表明，LBRM算法在无需微调的情况下，AUROC平均提高了约40%，微调后提高了约60%，显著提高了检测记忆化训练数据的准确性。该研究在两种时间序列插补模型架构上验证了LBRM算法的有效性和鲁棒性，强调了其在解决时间序列插补模型隐私风险方面的显著提升。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03490v1",
      "published_date": "2025-05-06 12:47:24 UTC",
      "updated_date": "2025-05-06 12:47:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:22:13.112542"
    },
    {
      "arxiv_id": "2505.03475v1",
      "title": "am-ELO: A Stable Framework for Arena-based LLM Evaluation",
      "title_zh": "am-ELO：一种稳定的基于竞技场的 LLM 评估框架\n",
      "authors": [
        "Zirui Liu",
        "Jiatong Li",
        "Yan Zhuang",
        "Qi Liu",
        "Shuanghong Shen",
        "Jie Ouyang",
        "Mingyue Cheng",
        "Shijin Wang"
      ],
      "abstract": "Arena-based evaluation is a fundamental yet significant evaluation paradigm\nfor modern AI models, especially large language models (LLMs). Existing\nframework based on ELO rating system suffers from the inevitable instability\nproblem due to ranking inconsistency and the lack of attention to the varying\nabilities of annotators. In this paper, we introduce a novel stable arena\nframework to address these issues by enhancing the ELO Rating System.\nSpecifically, we replace the iterative update method with a Maximum Likelihood\nEstimation (MLE) approach, m-ELO, and provide theoretical proof of the\nconsistency and stability of the MLE approach for model ranking. Additionally,\nwe proposed the am-ELO, which modify the Elo Rating's probability function to\nincorporate annotator abilities, enabling the simultaneous estimation of model\nscores and annotator reliability. Experiments demonstrate that this method\nensures stability, proving that this framework offers a more robust, accurate,\nand stable evaluation method for LLMs.",
      "tldr_zh": "该论文提出了一种名为am-ELO的稳定竞技场框架，用于评估大型语言模型(LLMs)。 针对现有基于ELO评分系统的框架存在的排名不一致和忽略标注者能力差异的问题，该框架通过引入最大似然估计(MLE)方法替代迭代更新，从理论上证明了MLE方法在模型排序上的一致性和稳定性。 此外，am-ELO修改了ELO评分的概率函数，纳入了标注者的能力，从而能够同时评估模型得分和标注者可靠性。 实验结果表明，该方法能够确保稳定性，为LLMs提供更鲁棒、准确和稳定的评估方法。\n",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "ICML2025 Accepted",
      "pdf_url": "http://arxiv.org/pdf/2505.03475v1",
      "published_date": "2025-05-06 12:28:50 UTC",
      "updated_date": "2025-05-06 12:28:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:22:25.202462"
    },
    {
      "arxiv_id": "2505.03470v1",
      "title": "Blending 3D Geometry and Machine Learning for Multi-View Stereopsis",
      "title_zh": "融合 3D 几何与机器学习的多视角立体视觉\n",
      "authors": [
        "Vibhas Vats",
        "Md. Alimoor Reza",
        "David Crandall",
        "Soon-heung Jung"
      ],
      "abstract": "Traditional multi-view stereo (MVS) methods primarily depend on photometric\nand geometric consistency constraints. In contrast, modern learning-based\nalgorithms often rely on the plane sweep algorithm to infer 3D geometry,\napplying explicit geometric consistency (GC) checks only as a post-processing\nstep, with no impact on the learning process itself. In this work, we introduce\nGC MVSNet plus plus, a novel approach that actively enforces geometric\nconsistency of reference view depth maps across multiple source views (multi\nview) and at various scales (multi scale) during the learning phase (see Fig.\n1). This integrated GC check significantly accelerates the learning process by\ndirectly penalizing geometrically inconsistent pixels, effectively halving the\nnumber of training iterations compared to other MVS methods. Furthermore, we\nintroduce a densely connected cost regularization network with two distinct\nblock designs simple and feature dense optimized to harness dense feature\nconnections for enhanced regularization. Extensive experiments demonstrate that\nour approach achieves a new state of the art on the DTU and BlendedMVS datasets\nand secures second place on the Tanks and Temples benchmark. To our knowledge,\nGC MVSNet plus plus is the first method to enforce multi-view, multi-scale\nsupervised geometric consistency during learning. Our code is available.",
      "tldr_zh": "该论文提出了GC MVSNet++，一种结合3D几何和机器学习的多视角立体视觉(MVS)新方法。该方法在学习阶段主动地在多个源视图和不同尺度上强制执行参考视图深度图的几何一致性(Geometric Consistency, GC)。通过直接惩罚几何不一致的像素，GC检查显著加速了学习过程，训练迭代次数减半。此外，论文还引入了一个密集连接的代价正则化网络，包含两种不同的模块设计，以增强正则化。实验结果表明，GC MVSNet++在DTU和BlendedMVS数据集上达到了新的state-of-the-art，并在Tanks and Temples benchmark上获得第二名。该方法是第一个在学习过程中强制执行多视角、多尺度监督几何一致性的方法。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CG",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "A pre-print -- paper under-review. arXiv admin note: substantial text\n  overlap with arXiv:2310.19583",
      "pdf_url": "http://arxiv.org/pdf/2505.03470v1",
      "published_date": "2025-05-06 12:22:45 UTC",
      "updated_date": "2025-05-06 12:22:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:22:37.254671"
    },
    {
      "arxiv_id": "2505.03452v1",
      "title": "An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation",
      "title_zh": "检索增强生成中超参数优化方法分析\n",
      "authors": [
        "Matan Orbach",
        "Ohad Eytan",
        "Benjamin Sznajder",
        "Ariel Gera",
        "Odellia Boni",
        "Yoav Kantor",
        "Gal Bloch",
        "Omri Levy",
        "Hadas Abraham",
        "Nitzan Barzilay",
        "Eyal Shnarch",
        "Michael E. Factor",
        "Shila Ofek-Koifman",
        "Paula Ta-Shma",
        "Assaf Toledo"
      ],
      "abstract": "Finding the optimal Retrieval-Augmented Generation (RAG) configuration for a\ngiven use case can be complex and expensive. Motivated by this challenge,\nframeworks for RAG hyper-parameter optimization (HPO) have recently emerged,\nyet their effectiveness has not been rigorously benchmarked. To address this\ngap, we present a comprehensive study involving 5 HPO algorithms over 5\ndatasets from diverse domains, including a new one collected for this work on\nreal-world product documentation. Our study explores the largest HPO search\nspace considered to date, with two optimized evaluation metrics. Analysis of\nthe results shows that RAG HPO can be done efficiently, either greedily or with\niterative random search, and that it significantly boosts RAG performance for\nall datasets. For greedy HPO approaches, we show that optimizing models first\nis preferable to the prevalent practice of optimizing sequentially according to\nthe RAG pipeline order.",
      "tldr_zh": "该研究对检索增强生成(RAG)的超参数优化(HPO)方法进行了分析，旨在解决RAG配置优化复杂且成本高昂的问题。研究对比了5种HPO算法在5个不同领域数据集上的表现，并构建了一个新的产品文档数据集。实验探索了迄今为止最大的HPO搜索空间，并使用两个优化评估指标。结果表明，RAG HPO可以通过贪婪搜索或迭代随机搜索高效完成，并显著提升RAG在所有数据集上的性能。对于贪婪HPO方法，先优化模型优于按照RAG流程顺序依次优化的常见做法。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03452v1",
      "published_date": "2025-05-06 11:47:52 UTC",
      "updated_date": "2025-05-06 11:47:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:22:49.035741"
    },
    {
      "arxiv_id": "2505.03451v1",
      "title": "Detecting Quishing Attacks with Machine Learning Techniques Through QR Code Analysis",
      "title_zh": "通过二维码分析利用机器学习技术检测 Quishing 攻击\n",
      "authors": [
        "Fouad Trad",
        "Ali Chehab"
      ],
      "abstract": "The rise of QR code based phishing (\"Quishing\") poses a growing cybersecurity\nthreat, as attackers increasingly exploit QR codes to bypass traditional\nphishing defenses. Existing detection methods predominantly focus on URL\nanalysis, which requires the extraction of the QR code payload, and may\ninadvertently expose users to malicious content. Moreover, QR codes can encode\nvarious types of data beyond URLs, such as Wi-Fi credentials and payment\ninformation, making URL-based detection insufficient for broader security\nconcerns. To address these gaps, we propose the first framework for quishing\ndetection that directly analyzes QR code structure and pixel patterns without\nextracting the embedded content. We generated a dataset of phishing and benign\nQR codes and we used it to train and evaluate multiple machine learning models,\nincluding Logistic Regression, Decision Trees, Random Forest, Naive Bayes,\nLightGBM, and XGBoost. Our best-performing model (XGBoost) achieves an AUC of\n0.9106, demonstrating the feasibility of QR-centric detection. Through feature\nimportance analysis, we identify key visual indicators of malicious intent and\nrefine our feature set by removing non-informative pixels, improving\nperformance to an AUC of 0.9133 with a reduced feature space. Our findings\nreveal that the structural features of QR code correlate strongly with phishing\nrisk. This work establishes a foundation for quishing mitigation and highlights\nthe potential of direct QR analysis as a critical layer in modern phishing\ndefenses.",
      "tldr_zh": "该研究提出了一种直接分析QR码结构和像素模式的Quishing攻击检测框架，无需提取嵌入内容。通过生成包含恶意和良性QR码的数据集，研究人员训练并评估了多种机器学习模型，包括Logistic Regression, Decision Trees, Random Forest, Naive Bayes, LightGBM和XGBoost。实验结果表明，XGBoost模型表现最佳，AUC达到0.9106，验证了基于QR码本身的检测方法的可行性。通过特征重要性分析，研究识别了恶意意图的关键视觉指标，并优化了特征集，最终在减少特征空间的情况下将AUC提升至0.9133。研究表明QR码的结构特征与网络钓鱼风险密切相关，为Quishing缓解和现代网络钓鱼防御提供了基础。\n",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted in 8th International Conference on Optimization and Learning\n  (OLA2025)",
      "pdf_url": "http://arxiv.org/pdf/2505.03451v1",
      "published_date": "2025-05-06 11:47:13 UTC",
      "updated_date": "2025-05-06 11:47:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:23:01.304123"
    },
    {
      "arxiv_id": "2505.03443v1",
      "title": "Elevating Semantic Exploration: A Novel Approach Utilizing Distributed Repositories",
      "title_zh": "提升语义探索：一种利用分布式存储库的新方法\n",
      "authors": [
        "Valerio Bellandi"
      ],
      "abstract": "Centralized and distributed systems are two main approaches to organizing ICT\ninfrastructure, each with its pros and cons. Centralized systems concentrate\nresources in one location, making management easier but creating single points\nof failure. Distributed systems, on the other hand, spread resources across\nmultiple nodes, offering better scalability and fault tolerance, but requiring\nmore complex management. The choice between them depends on factors like\napplication needs, scalability, and data sensitivity. Centralized systems suit\napplications with limited scalability and centralized control, while\ndistributed systems excel in large-scale environments requiring high\navailability and performance. This paper explores a distributed document\nrepository system developed for the Italian Ministry of Justice, using edge\nrepositories to analyze textual data and metadata, enhancing semantic\nexploration capabilities.",
      "tldr_zh": "本文探讨了一种利用分布式存储库提升语义探索的新方法。针对集中式系统和分布式系统在信息通信技术基础设施组织中的优缺点，该研究提出了一种应用于意大利司法部的分布式文档存储库系统。该系统利用边缘存储库分析文本数据和元数据，从而增强语义探索能力。该方法旨在解决集中式系统的单点故障问题，并提升大规模环境下的可用性和性能。\n",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.DC",
      "comment": "This paper has been accepted at the 6th International Conference on\n  Recent Trends and Applications in Computer Science. It will appear in the\n  proceedings",
      "pdf_url": "http://arxiv.org/pdf/2505.03443v1",
      "published_date": "2025-05-06 11:30:16 UTC",
      "updated_date": "2025-05-06 11:30:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:23:12.947119"
    },
    {
      "arxiv_id": "2505.03439v1",
      "title": "The Steganographic Potentials of Language Models",
      "title_zh": "语言模型的隐写潜力\n",
      "authors": [
        "Artem Karpov",
        "Tinuade Adeleke",
        "Seong Hah Cho",
        "Natalia Perez-Campanero"
      ],
      "abstract": "The potential for large language models (LLMs) to hide messages within plain\ntext (steganography) poses a challenge to detection and thwarting of unaligned\nAI agents, and undermines faithfulness of LLMs reasoning. We explore the\nsteganographic capabilities of LLMs fine-tuned via reinforcement learning (RL)\nto: (1) develop covert encoding schemes, (2) engage in steganography when\nprompted, and (3) utilize steganography in realistic scenarios where hidden\nreasoning is likely, but not prompted. In these scenarios, we detect the\nintention of LLMs to hide their reasoning as well as their steganography\nperformance. Our findings in the fine-tuning experiments as well as in\nbehavioral non fine-tuning evaluations reveal that while current models exhibit\nrudimentary steganographic abilities in terms of security and capacity,\nexplicit algorithmic guidance markedly enhances their capacity for information\nconcealment.",
      "tldr_zh": "本文探讨了大型语言模型(LLMs)在纯文本中隐藏信息（隐写术）的潜力，这给检测和阻止未对齐的AI agent带来了挑战，并损害了LLMs推理的真实性。研究通过强化学习(RL)对LLMs进行微调，使其能够：(1)开发隐蔽的编码方案，(2)在提示下进行隐写，(3)在隐藏推理可能但未提示的实际场景中使用隐写。研究检测了LLMs隐藏推理意图以及其隐写性能。结果表明，虽然当前模型在安全性和容量方面表现出初步的隐写能力，但显式的算法指导显著提高了其信息隐藏能力。\n",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Published at Building Trust Workshop at ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2505.03439v1",
      "published_date": "2025-05-06 11:25:52 UTC",
      "updated_date": "2025-05-06 11:25:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:23:25.210177"
    },
    {
      "arxiv_id": "2505.03434v1",
      "title": "Procedural Memory Is Not All You Need: Bridging Cognitive Gaps in LLM-Based Agents",
      "title_zh": "程序性记忆并非全部：弥合基于 LLM 的智能体中的认知差距\n",
      "authors": [
        "Schaun Wheeler",
        "Olivier Jeunen"
      ],
      "abstract": "Large Language Models (LLMs) represent a landmark achievement in Artificial\nIntelligence (AI), demonstrating unprecedented proficiency in procedural tasks\nsuch as text generation, code completion, and conversational coherence. These\ncapabilities stem from their architecture, which mirrors human procedural\nmemory -- the brain's ability to automate repetitive, pattern-driven tasks\nthrough practice. However, as LLMs are increasingly deployed in real-world\napplications, it becomes impossible to ignore their limitations operating in\ncomplex, unpredictable environments. This paper argues that LLMs, while\ntransformative, are fundamentally constrained by their reliance on procedural\nmemory. To create agents capable of navigating ``wicked'' learning environments\n-- where rules shift, feedback is ambiguous, and novelty is the norm -- we must\naugment LLMs with semantic memory and associative learning systems. By adopting\na modular architecture that decouples these cognitive functions, we can bridge\nthe gap between narrow procedural expertise and the adaptive intelligence\nrequired for real-world problem-solving.",
      "tldr_zh": "该论文指出，大型语言模型(LLMs)虽然在文本生成、代码补全等程序性任务上表现出色，但由于过度依赖程序性记忆，在复杂、不可预测的环境中存在局限性。为了构建能够适应“wicked”学习环境（规则变化、反馈模糊、新颖性是常态）的智能体，需要用语义记忆和联想学习系统来增强LLMs。作者提出一种模块化架构，将这些认知功能解耦，从而弥合了狭窄的程序性专业知识与现实世界问题解决所需的自适应智能之间的差距。\n",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to the workshop on Hybrid AI for Human-Centric\n  Personalization (HyPer), co-located with ACM UMAP '25",
      "pdf_url": "http://arxiv.org/pdf/2505.03434v1",
      "published_date": "2025-05-06 11:18:34 UTC",
      "updated_date": "2025-05-06 11:18:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:23:37.100070"
    },
    {
      "arxiv_id": "2505.03427v1",
      "title": "MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks",
      "title_zh": "MedArabiQ：在阿拉伯语医疗任务上对大型语言模型进行基准测试\n",
      "authors": [
        "Mouath Abu Daoud",
        "Chaimae Abouzahir",
        "Leen Kharouf",
        "Walid Al-Eisawi",
        "Nizar Habash",
        "Farah E. Shamout"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated significant promise for\nvarious applications in healthcare. However, their efficacy in the Arabic\nmedical domain remains unexplored due to the lack of high-quality\ndomain-specific datasets and benchmarks. This study introduces MedArabiQ, a\nnovel benchmark dataset consisting of seven Arabic medical tasks, covering\nmultiple specialties and including multiple choice questions,\nfill-in-the-blank, and patient-doctor question answering. We first constructed\nthe dataset using past medical exams and publicly available datasets. We then\nintroduced different modifications to evaluate various LLM capabilities,\nincluding bias mitigation. We conducted an extensive evaluation with five\nstate-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude\n3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of\nnew high-quality benchmarks that span different languages to ensure fair\ndeployment and scalability of LLMs in healthcare. By establishing this\nbenchmark and releasing the dataset, we provide a foundation for future\nresearch aimed at evaluating and enhancing the multilingual capabilities of\nLLMs for the equitable use of generative AI in healthcare.",
      "tldr_zh": "该研究提出了MedArabiQ，一个用于评估大型语言模型(LLMs)在阿拉伯语医疗任务中表现的新基准数据集。MedArabiQ包含七项阿拉伯语医疗任务，涵盖多个医学专业，包括多项选择题、填空题和医患问答。研究人员利用过去的医学考试和公开数据集构建了该数据集，并进行了修改以评估LLM的各种能力，包括偏见缓解。通过对GPT-4o、Claude 3.5-Sonnet和Gemini 1.5等五种先进的开源和专有LLM的广泛评估，研究强调了创建新的高质量、跨语言基准的必要性，以确保LLM在医疗保健领域公平部署和扩展。该基准数据集的发布为未来评估和增强LLM多语言能力的研究奠定了基础，促进生成式AI在医疗保健领域的公平应用。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "21 pages",
      "pdf_url": "http://arxiv.org/pdf/2505.03427v1",
      "published_date": "2025-05-06 11:07:26 UTC",
      "updated_date": "2025-05-06 11:07:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:23:49.489058"
    },
    {
      "arxiv_id": "2505.03426v1",
      "title": "Phenotype-Guided Generative Model for High-Fidelity Cardiac MRI Synthesis: Advancing Pretraining and Clinical Applications",
      "title_zh": "表型引导的生成模型用于高保真心脏 MRI 合成：推进预训练和临床应用\n",
      "authors": [
        "Ziyu Li",
        "Yujian Hu",
        "Zhengyao Ding",
        "Yiheng Mao",
        "Haitao Li",
        "Fan Yi",
        "Hongkun Zhang",
        "Zhengxing Huang"
      ],
      "abstract": "Cardiac Magnetic Resonance (CMR) imaging is a vital non-invasive tool for\ndiagnosing heart diseases and evaluating cardiac health. However, the limited\navailability of large-scale, high-quality CMR datasets poses a major challenge\nto the effective application of artificial intelligence (AI) in this domain.\nEven the amount of unlabeled data and the health status it covers are difficult\nto meet the needs of model pretraining, which hinders the performance of AI\nmodels on downstream tasks. In this study, we present Cardiac Phenotype-Guided\nCMR Generation (CPGG), a novel approach for generating diverse CMR data that\ncovers a wide spectrum of cardiac health status. The CPGG framework consists of\ntwo stages: in the first stage, a generative model is trained using cardiac\nphenotypes derived from CMR data; in the second stage, a masked autoregressive\ndiffusion model, conditioned on these phenotypes, generates high-fidelity CMR\ncine sequences that capture both structural and functional features of the\nheart in a fine-grained manner. We synthesized a massive amount of CMR to\nexpand the pretraining data. Experimental results show that CPGG generates\nhigh-quality synthetic CMR data, significantly improving performance on various\ndownstream tasks, including diagnosis and cardiac phenotypes prediction. These\ngains are demonstrated across both public and private datasets, highlighting\nthe effectiveness of our approach. Code is availabel at\nhttps://anonymous.4open.science/r/CPGG.",
      "tldr_zh": "该论文提出了Cardiac Phenotype-Guided CMR Generation (CPGG)，一种新颖的CMR图像生成方法，旨在解决高质量CMR数据不足的问题。CPGG框架包含两个阶段：首先，利用从CMR数据中提取的心脏表型训练生成模型；然后，使用一个以这些表型为条件的掩码自回归扩散模型生成高保真的CMR电影序列，捕捉心脏的结构和功能特征。通过合成大量的CMR数据来扩充预训练数据集，实验结果表明CPGG能够生成高质量的合成CMR数据，并显著提升了AI模型在诊断和心脏表型预测等下游任务中的性能。该方法在公共和私有数据集上均表现出有效性。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03426v1",
      "published_date": "2025-05-06 11:06:41 UTC",
      "updated_date": "2025-05-06 11:06:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:24:01.194717"
    },
    {
      "arxiv_id": "2505.03424v1",
      "title": "Framework GNN-AID: Graph Neural Network Analysis Interpretation and Defense",
      "title_zh": "GNN-AID 框架：图神经网络分析、解释与防御\n",
      "authors": [
        "Kirill Lukyanov",
        "Mikhail Drobyshevskiy",
        "Georgii Sazonov",
        "Mikhail Soloviov",
        "Ilya Makarov"
      ],
      "abstract": "The growing need for Trusted AI (TAI) highlights the importance of\ninterpretability and robustness in machine learning models. However, many\nexisting tools overlook graph data and rarely combine these two aspects into a\nsingle solution. Graph Neural Networks (GNNs) have become a popular approach,\nachieving top results across various tasks. We introduce GNN-AID (Graph Neural\nNetwork Analysis, Interpretation, and Defense), an open-source framework\ndesigned for graph data to address this gap. Built as a Python library, GNN-AID\nsupports advanced trust methods and architectural layers, allowing users to\nanalyze graph datasets and GNN behavior using attacks, defenses, and\ninterpretability methods.\n  GNN-AID is built on PyTorch-Geometric, offering preloaded datasets, models,\nand support for any GNNs through customizable interfaces. It also includes a\nweb interface with tools for graph visualization and no-code features like an\ninteractive model builder, simplifying the exploration and analysis of GNNs.\nThe framework also supports MLOps techniques, ensuring reproducibility and\nresult versioning to track and revisit analyses efficiently.\n  GNN-AID is a flexible tool for developers and researchers. It helps\ndevelopers create, analyze, and customize graph models, while also providing\naccess to prebuilt datasets and models for quick experimentation. Researchers\ncan use the framework to explore advanced topics on the relationship between\ninterpretability and robustness, test defense strategies, and combine methods\nto protect against different types of attacks.\n  We also show how defenses against evasion and poisoning attacks can conflict\nwhen applied to graph data, highlighting the complex connections between\ndefense strategies.\n  GNN-AID is available at\n\\href{https://github.com/ispras/GNN-AID}{github.com/ispras/GNN-AID}",
      "tldr_zh": "GNN-AID是一个开源Python框架，旨在弥合图神经网络(GNN)分析、可解释性和防御之间的差距。它基于PyTorch-Geometric构建，支持各种GNN模型，并提供预加载的数据集和模型，方便用户进行快速实验。GNN-AID包含用于图可视化的Web界面和交互式模型构建器等无代码功能，简化了GNN的探索和分析。该框架还支持MLOps技术，确保可重复性和结果版本控制。此外，论文还展示了针对规避攻击和投毒攻击的防御措施在应用于图数据时可能存在的冲突，突出了防御策略之间复杂的关系。GNN-AID为开发者和研究人员提供了一个灵活的工具，用于创建、分析和定制图模型，并探索可解释性和鲁棒性之间的关系。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03424v1",
      "published_date": "2025-05-06 11:03:19 UTC",
      "updated_date": "2025-05-06 11:03:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:24:13.580644"
    },
    {
      "arxiv_id": "2505.03406v1",
      "title": "Lightweight Clinical Decision Support System using QLoRA-Fine-Tuned LLMs and Retrieval-Augmented Generation",
      "title_zh": "基于 QLoRA 微调的 LLM 和检索增强生成技术的轻量级临床决策支持系统\n",
      "authors": [
        "Mohammad Shoaib Ansari",
        "Mohd Sohail Ali Khan",
        "Shubham Revankar",
        "Aditya Varma",
        "Anil S. Mokhade"
      ],
      "abstract": "This research paper investigates the application of Large Language Models\n(LLMs) in healthcare, specifically focusing on enhancing medical decision\nsupport through Retrieval-Augmented Generation (RAG) integrated with\nhospital-specific data and fine-tuning using Quantized Low-Rank Adaptation\n(QLoRA). The system utilizes Llama 3.2-3B-Instruct as its foundation model. By\nembedding and retrieving context-relevant healthcare information, the system\nsignificantly improves response accuracy. QLoRA facilitates notable parameter\nefficiency and memory optimization, preserving the integrity of medical\ninformation through specialized quantization techniques. Our research also\nshows that our model performs relatively well on various medical benchmarks,\nindicating that it can be used to make basic medical suggestions. This paper\ndetails the system's technical components, including its architecture,\nquantization methods, and key healthcare applications such as enhanced disease\nprediction from patient symptoms and medical history, treatment suggestions,\nand efficient summarization of complex medical reports. We touch on the ethical\nconsiderations-patient privacy, data security, and the need for rigorous\nclinical validation-as well as the practical challenges of integrating such\nsystems into real-world healthcare workflows. Furthermore, the lightweight\nquantized weights ensure scalability and ease of deployment even in\nlow-resource hospital environments. Finally, the paper concludes with an\nanalysis of the broader impact of LLMs on healthcare and outlines future\ndirections for LLMs in medical settings.",
      "tldr_zh": "本文研究了大型语言模型(LLMs)在医疗保健中的应用，重点是通过检索增强生成(RAG)集成医院特定数据，并使用量化低秩适应(QLoRA)进行微调，以增强医疗决策支持。该系统以Llama 3.2-3B-Instruct作为基础模型，通过嵌入和检索上下文相关的医疗信息，显著提高了响应准确性。QLoRA实现了显著的参数效率和内存优化，并通过专门的量化技术保留了医疗信息的完整性。实验表明，该模型在各种医疗基准测试中表现良好，可以用于提供基本的医疗建议。轻量级的量化权重确保了可扩展性和易于部署，即使在低资源医院环境中也是如此。该研究还讨论了伦理考量以及将此类系统集成到实际医疗工作流程中的实际挑战。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages",
      "pdf_url": "http://arxiv.org/pdf/2505.03406v1",
      "published_date": "2025-05-06 10:31:54 UTC",
      "updated_date": "2025-05-06 10:31:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:24:25.381132"
    },
    {
      "arxiv_id": "2505.03401v1",
      "title": "DDaTR: Dynamic Difference-aware Temporal Residual Network for Longitudinal Radiology Report Generation",
      "title_zh": "DDaTR：用于纵向放射学报告生成的动态差异感知时间残差网络\n",
      "authors": [
        "Shanshan Song",
        "Hui Tang",
        "Honglong Yang",
        "Xiaomeng Li"
      ],
      "abstract": "Radiology Report Generation (RRG) automates the creation of radiology reports\nfrom medical imaging, enhancing the efficiency of the reporting process.\nLongitudinal Radiology Report Generation (LRRG) extends RRG by incorporating\nthe ability to compare current and prior exams, facilitating the tracking of\ntemporal changes in clinical findings. Existing LRRG approaches only extract\nfeatures from prior and current images using a visual pre-trained encoder,\nwhich are then concatenated to generate the final report. However, these\nmethods struggle to effectively capture both spatial and temporal correlations\nduring the feature extraction process. Consequently, the extracted features\ninadequately capture the information of difference across exams and thus\nunderrepresent the expected progressions, leading to sub-optimal performance in\nLRRG. To address this, we develop a novel dynamic difference-aware temporal\nresidual network (DDaTR). In DDaTR, we introduce two modules at each stage of\nthe visual encoder to capture multi-level spatial correlations. The Dynamic\nFeature Alignment Module (DFAM) is designed to align prior features across\nmodalities for the integrity of prior clinical information. Prompted by the\nenriched prior features, the dynamic difference-aware module (DDAM) captures\nfavorable difference information by identifying relationships across exams.\nFurthermore, our DDaTR employs the dynamic residual network to unidirectionally\ntransmit longitudinal information, effectively modelling temporal correlations.\nExtensive experiments demonstrated superior performance over existing methods\non three benchmarks, proving its efficacy in both RRG and LRRG tasks.",
      "tldr_zh": "本文提出了一种动态差异感知时间残差网络(DDaTR)，用于纵向放射学报告生成(LRRG)。现有方法难以有效捕捉图像特征提取过程中的空间和时间相关性，导致提取的特征无法充分捕捉检查之间的差异信息。DDaTR在视觉编码器的每个阶段引入两个模块来捕捉多层次的空间相关性：动态特征对齐模块(DFAM)用于对齐先前的特征，动态差异感知模块(DDAM)通过识别检查之间的关系来捕捉有利的差异信息。此外，DDaTR采用动态残差网络来单向传输纵向信息，有效地建模时间相关性。在三个基准数据集上的实验表明，DDaTR在RRG和LRRG任务中均优于现有方法。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03401v1",
      "published_date": "2025-05-06 10:29:23 UTC",
      "updated_date": "2025-05-06 10:29:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:24:37.351117"
    },
    {
      "arxiv_id": "2505.03392v1",
      "title": "Automatic Calibration for Membership Inference Attack on Large Language Models",
      "title_zh": "大型语言模型成员推断攻击的自动校准\n",
      "authors": [
        "Saleh Zare Zade",
        "Yao Qiang",
        "Xiangyu Zhou",
        "Hui Zhu",
        "Mohammad Amin Roshani",
        "Prashant Khanduri",
        "Dongxiao Zhu"
      ],
      "abstract": "Membership Inference Attacks (MIAs) have recently been employed to determine\nwhether a specific text was part of the pre-training data of Large Language\nModels (LLMs). However, existing methods often misinfer non-members as members,\nleading to a high false positive rate, or depend on additional reference models\nfor probability calibration, which limits their practicality. To overcome these\nchallenges, we introduce a novel framework called Automatic Calibration\nMembership Inference Attack (ACMIA), which utilizes a tunable temperature to\ncalibrate output probabilities effectively. This approach is inspired by our\ntheoretical insights into maximum likelihood estimation during the pre-training\nof LLMs. We introduce ACMIA in three configurations designed to accommodate\ndifferent levels of model access and increase the probability gap between\nmembers and non-members, improving the reliability and robustness of membership\ninference. Extensive experiments on various open-source LLMs demonstrate that\nour proposed attack is highly effective, robust, and generalizable, surpassing\nstate-of-the-art baselines across three widely used benchmarks. Our code is\navailable at:\n\\href{https://github.com/Salehzz/ACMIA}{\\textcolor{blue}{Github}}.",
      "tldr_zh": "该论文提出了自动校准成员推断攻击(ACMIA)框架，旨在解决现有成员推断攻击(MIA)方法在大型语言模型(LLMs)上误判率高以及依赖额外参考模型的问题。ACMIA利用可调节的温度参数来有效校准LLM的输出概率，该方法受到LLM预训练期间最大似然估计的理论启发。ACMIA提供三种配置，以适应不同程度的模型访问，并增大成员和非成员之间的概率差距，从而提高成员推断的可靠性和鲁棒性。在多个开源LLM上的实验表明，ACMIA在三个广泛使用的基准测试中优于现有最佳方法，具有高效性、鲁棒性和泛化性。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03392v1",
      "published_date": "2025-05-06 10:15:05 UTC",
      "updated_date": "2025-05-06 10:15:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:24:49.453142"
    },
    {
      "arxiv_id": "2505.03380v1",
      "title": "Reinforced Correlation Between Vision and Language for Precise Medical AI Assistant",
      "title_zh": "用于精准医疗 AI 助手的视觉与语言强化相关性",
      "authors": [
        "Haonan Wang",
        "Jiaji Mao",
        "Lehan Wang",
        "Qixiang Zhang",
        "Marawan Elbatel",
        "Yi Qin",
        "Huijun Hu",
        "Baoxun Li",
        "Wenhui Deng",
        "Weifeng Qin",
        "Hongrui Li",
        "Jialin Liang",
        "Jun Shen",
        "Xiaomeng Li"
      ],
      "abstract": "Medical AI assistants support doctors in disease diagnosis, medical image\nanalysis, and report generation. However, they still face significant\nchallenges in clinical use, including limited accuracy with multimodal content\nand insufficient validation in real-world settings. We propose RCMed, a\nfull-stack AI assistant that improves multimodal alignment in both input and\noutput, enabling precise anatomical delineation, accurate localization, and\nreliable diagnosis through hierarchical vision-language grounding. A\nself-reinforcing correlation mechanism allows visual features to inform\nlanguage context, while language semantics guide pixel-wise attention, forming\na closed loop that refines both modalities. This correlation is enhanced by a\ncolor region description strategy, translating anatomical structures into\nsemantically rich text to learn shape-location-text relationships across\nscales. Trained on 20 million image-mask-description triplets, RCMed achieves\nstate-of-the-art precision in contextualizing irregular lesions and subtle\nanatomical boundaries, excelling in 165 clinical tasks across 9 modalities. It\nachieved a 23.5% relative improvement in cell segmentation from microscopy\nimages over prior methods. RCMed's strong vision-language alignment enables\nexceptional generalization, with state-of-the-art performance in external\nvalidation across 20 clinically significant cancer types, including novel\ntasks. This work demonstrates how integrated multimodal models capture\nfine-grained patterns, enabling human-level interpretation in complex scenarios\nand advancing human-centric AI healthcare.",
      "tldr_zh": "该论文提出了一个名为RCMed的完整AI医疗助手，旨在通过增强视觉和语言之间的关联来提高多模态医疗AI的精度。RCMed采用自增强关联机制，使视觉特征能够影响语言上下文，反之语言语义引导像素级注意力，从而形成一个闭环来优化两种模态。通过颜色区域描述策略，将解剖结构转化为语义丰富的文本，学习跨尺度的形状-位置-文本关系。RCMed在2000万图像-掩码-描述三元组上进行训练，在165个临床任务中表现出色，并在细胞分割方面实现了23.5%的相对改进。实验结果表明，RCMed具有强大的泛化能力，在20种临床上重要的癌症类型（包括新任务）的外部验证中取得了最优性能，实现了复杂场景下的人类水平解释。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03380v1",
      "published_date": "2025-05-06 10:00:08 UTC",
      "updated_date": "2025-05-06 10:00:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:25:01.717575"
    },
    {
      "arxiv_id": "2505.03373v1",
      "title": "SPAP: Structured Pruning via Alternating Optimization and Penalty Methods",
      "title_zh": "SPAP：通过交替优化和惩罚方法实现的结构化剪枝\n",
      "authors": [
        "Hanyu Hu",
        "Xiaoming Yuan"
      ],
      "abstract": "The deployment of large language models (LLMs) is often constrained by their\nsubstantial computational and memory demands. While structured pruning presents\na viable approach by eliminating entire network components, existing methods\nsuffer from performance degradation, reliance on heuristic metrics, or\nexpensive finetuning. To address these challenges, we propose SPAP (Structured\nPruning via Alternating Optimization and Penalty Methods), a novel and\nefficient structured pruning framework for LLMs grounded in optimization\ntheory. SPAP formulates the pruning problem through a mixed-integer\noptimization model, employs a penalty method that effectively makes pruning\ndecisions to minimize pruning errors, and introduces an alternating\nminimization algorithm tailored to the splittable problem structure for\nefficient weight updates and performance recovery. Extensive experiments on\nOPT, LLaMA-3/3.1/3.2, and Qwen2.5 models demonstrate SPAP's superiority over\nstate-of-the-art methods, delivering linear inference speedups (1.29$\\times$ at\n30% sparsity) and proportional memory reductions. Our work offers a practical,\noptimization-driven solution for pruning LLMs while preserving model\nperformance.",
      "tldr_zh": "为了解决大语言模型(LLMs)部署时面临的计算和内存限制，本文提出了一种新的结构化剪枝框架SPAP (Structured Pruning via Alternating Optimization and Penalty Methods)。SPAP基于优化理论，通过混合整数优化模型构建剪枝问题，并采用惩罚方法有效减少剪枝误差。此外，SPAP引入了一种交替最小化算法，专门针对可分离问题结构设计，以实现高效的权重更新和性能恢复。在OPT, LLaMA-3/3.1/3.2和Qwen2.5模型上的大量实验表明，SPAP优于现有方法，实现了线性推理加速（30%稀疏度下加速1.29倍）和成比例的内存减少。该研究为LLMs的剪枝提供了一种实用的、优化驱动的解决方案，同时保留了模型性能。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03373v1",
      "published_date": "2025-05-06 09:47:53 UTC",
      "updated_date": "2025-05-06 09:47:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:25:13.510691"
    },
    {
      "arxiv_id": "2505.03369v1",
      "title": "Validating the Effectiveness of a Large Language Model-based Approach for Identifying Children's Development across Various Free Play Settings in Kindergarten",
      "title_zh": "验证基于大型语言模型的方法在识别幼儿园各种自由玩耍环境中儿童发展方面的有效性\n",
      "authors": [
        "Yuanyuan Yang",
        "Yuan Shen",
        "Tianchen Sun",
        "Yangbin Xie"
      ],
      "abstract": "Free play is a fundamental aspect of early childhood education, supporting\nchildren's cognitive, social, emotional, and motor development. However,\nassessing children's development during free play poses significant challenges\ndue to the unstructured and spontaneous nature of the activity. Traditional\nassessment methods often rely on direct observations by teachers, parents, or\nresearchers, which may fail to capture comprehensive insights from free play\nand provide timely feedback to educators. This study proposes an innovative\napproach combining Large Language Models (LLMs) with learning analytics to\nanalyze children's self-narratives of their play experiences. The LLM\nidentifies developmental abilities, while performance scores across different\nplay settings are calculated using learning analytics techniques. We collected\n2,224 play narratives from 29 children in a kindergarten, covering four\ndistinct play areas over one semester. According to the evaluation results from\neight professionals, the LLM-based approach achieved high accuracy in\nidentifying cognitive, motor, and social abilities, with accuracy exceeding 90%\nin most domains. Moreover, significant differences in developmental outcomes\nwere observed across play settings, highlighting each area's unique\ncontributions to specific abilities. These findings confirm that the proposed\napproach is effective in identifying children's development across various free\nplay settings. This study demonstrates the potential of integrating LLMs and\nlearning analytics to provide child-centered insights into developmental\ntrajectories, offering educators valuable data to support personalized learning\nand enhance early childhood education practices.",
      "tldr_zh": "该研究提出了一种创新方法，结合大型语言模型(LLMs)和学习分析技术，用于分析儿童在幼儿园自由玩耍中的自我叙述，以此评估儿童的发展情况。LLM用于识别认知、运动和社交能力，并通过学习分析技术计算不同游戏环境下的表现得分。实验收集了29名儿童在一个学期内，在四个不同游戏区域的2224份游戏叙述。评估结果表明，该方法在识别儿童的认知、运动和社交能力方面具有很高的准确性，在大多数领域超过90%。研究结果证实了该方法在识别儿童在各种自由玩耍环境中的发展方面的有效性，为教育工作者提供了支持个性化学习和加强幼儿教育实践的宝贵数据。\n",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2505.03369v1",
      "published_date": "2025-05-06 09:40:47 UTC",
      "updated_date": "2025-05-06 09:40:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:25:25.364536"
    },
    {
      "arxiv_id": "2505.03359v1",
      "title": "Domain Adversarial Training for Mitigating Gender Bias in Speech-based Mental Health Detection",
      "title_zh": "用于缓解语音心理健康检测中性别偏见的领域对抗训练\n",
      "authors": [
        "June-Woo Kim",
        "Haram Yoon",
        "Wonkyo Oh",
        "Dawoon Jung",
        "Sung-Hoon Yoon",
        "Dae-Jin Kim",
        "Dong-Ho Lee",
        "Sang-Yeol Lee",
        "Chan-Mo Yang"
      ],
      "abstract": "Speech-based AI models are emerging as powerful tools for detecting\ndepression and the presence of Post-traumatic stress disorder (PTSD), offering\na non-invasive and cost-effective way to assess mental health. However, these\nmodels often struggle with gender bias, which can lead to unfair and inaccurate\npredictions. In this study, our study addresses this issue by introducing a\ndomain adversarial training approach that explicitly considers gender\ndifferences in speech-based depression and PTSD detection. Specifically, we\ntreat different genders as distinct domains and integrate this information into\na pretrained speech foundation model. We then validate its effectiveness on the\nE-DAIC dataset to assess its impact on performance. Experimental results show\nthat our method notably improves detection performance, increasing the F1-score\nby up to 13.29 percentage points compared to the baseline. This highlights the\nimportance of addressing demographic disparities in AI-driven mental health\nassessment.",
      "tldr_zh": "该研究针对语音AI模型在抑郁症和创伤后应激障碍(PTSD)检测中存在的性别偏见问题，提出了一种领域对抗训练方法。该方法将不同性别视为不同的领域，并将性别信息整合到预训练的语音基础模型中，以显式地考虑性别差异。在E-DAIC数据集上的实验结果表明，该方法显著提高了检测性能，F1-score最高提升了13.29个百分点。该研究强调了在AI驱动的心理健康评估中解决人口统计学差异的重要性。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to EMBC 2025",
      "pdf_url": "http://arxiv.org/pdf/2505.03359v1",
      "published_date": "2025-05-06 09:29:14 UTC",
      "updated_date": "2025-05-06 09:29:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:25:37.200111"
    },
    {
      "arxiv_id": "2505.03338v1",
      "title": "Safer Prompts: Reducing IP Risk in Visual Generative AI",
      "title_zh": "更安全的提示：降低视觉生成式人工智能中的 IP 风险\n",
      "authors": [
        "Lena Reissinger",
        "Yuanyuan Li",
        "Anna-Carolina Haensch",
        "Neeraj Sarna"
      ],
      "abstract": "Visual Generative AI models have demonstrated remarkable capability in\ngenerating high-quality images from simple inputs like text prompts. However,\nbecause these models are trained on images from diverse sources, they risk\nmemorizing and reproducing specific content, raising concerns about\nintellectual property (IP) infringement. Recent advances in prompt engineering\noffer a cost-effective way to enhance generative AI performance. In this paper,\nwe evaluate the effectiveness of prompt engineering techniques in mitigating IP\ninfringement risks in image generation. Our findings show that Chain of Thought\nPrompting and Task Instruction Prompting significantly reduce the similarity\nbetween generated images and the training data of diffusion models, thereby\nlowering the risk of IP infringement.",
      "tldr_zh": "该研究探讨了视觉生成AI模型中潜在的知识产权(IP)侵权风险，并评估了提示工程技术在降低此类风险中的有效性。研究发现，由于模型训练数据来源广泛，可能存在记忆和复制特定内容的问题。通过实验，研究表明链式思考提示(Chain of Thought Prompting)和任务指令提示(Task Instruction Prompting)可以显著降低生成图像与扩散模型训练数据之间的相似性，从而降低IP侵权风险。该研究为在视觉生成AI中利用提示工程降低IP风险提供了有价值的见解。\n",
      "categories": [
        "math.NA",
        "cs.AI",
        "cs.NA"
      ],
      "primary_category": "math.NA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03338v1",
      "published_date": "2025-05-06 09:10:12 UTC",
      "updated_date": "2025-05-06 09:10:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:25:49.269241"
    },
    {
      "arxiv_id": "2505.03336v1",
      "title": "Avoid Recommending Out-of-Domain Items: Constrained Generative Recommendation with LLMs",
      "title_zh": "避免推荐领域外项目：基于大型语言模型的约束生成式推荐\n",
      "authors": [
        "Hao Liao",
        "Wensheng Lu",
        "Jianxun Lian",
        "Mingqi Wu",
        "Shuo Wang",
        "Yong Zhang",
        "Yitian Huang",
        "Mingyang Zhou",
        "Xing Xie"
      ],
      "abstract": "Large Language Models (LLMs) have shown promise for generative recommender\nsystems due to their transformative capabilities in user interaction. However,\nensuring they do not recommend out-of-domain (OOD) items remains a challenge.\nWe study two distinct methods to address this issue: RecLM-ret, a\nretrieval-based method, and RecLM-cgen, a constrained generation method. Both\nmethods integrate seamlessly with existing LLMs to ensure in-domain\nrecommendations. Comprehensive experiments on three recommendation datasets\ndemonstrate that RecLM-cgen consistently outperforms RecLM-ret and existing\nLLM-based recommender models in accuracy while eliminating OOD recommendations,\nmaking it the preferred method for adoption. Additionally, RecLM-cgen maintains\nstrong generalist capabilities and is a lightweight plug-and-play module for\neasy integration into LLMs, offering valuable practical benefits for the\ncommunity. Source code is available at https://github.com/microsoft/RecAI",
      "tldr_zh": "该论文研究了如何避免大型语言模型(LLMs)在生成推荐时推荐领域外(out-of-domain, OOD)的物品。提出了两种方法：基于检索的RecLM-ret和基于约束生成的RecLM-cgen。RecLM-cgen通过与现有LLMs无缝集成，确保推荐结果在领域内。在三个推荐数据集上的实验表明，RecLM-cgen在准确性方面优于RecLM-ret和其他基于LLM的推荐模型，同时消除了OOD推荐。RecLM-cgen还保持了强大的通用能力，并且是一个轻量级的即插即用模块，易于集成到LLMs中。\n",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "13 pages",
      "pdf_url": "http://arxiv.org/pdf/2505.03336v1",
      "published_date": "2025-05-06 09:08:36 UTC",
      "updated_date": "2025-05-06 09:08:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:26:01.346526"
    },
    {
      "arxiv_id": "2505.03335v2",
      "title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
      "title_zh": "绝对零度：基于零数据的强化自博弈推理\n",
      "authors": [
        "Andrew Zhao",
        "Yiran Wu",
        "Yang Yue",
        "Tong Wu",
        "Quentin Xu",
        "Yang Yue",
        "Matthieu Lin",
        "Shenzhi Wang",
        "Qingyun Wu",
        "Zilong Zheng",
        "Gao Huang"
      ],
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) has shown promise in\nenhancing the reasoning capabilities of large language models by learning\ndirectly from outcome-based rewards. Recent RLVR works that operate under the\nzero setting avoid supervision in labeling the reasoning process, but still\ndepend on manually curated collections of questions and answers for training.\nThe scarcity of high-quality, human-produced examples raises concerns about the\nlong-term scalability of relying on human supervision, a challenge already\nevident in the domain of language model pretraining. Furthermore, in a\nhypothetical future where AI surpasses human intelligence, tasks provided by\nhumans may offer limited learning potential for a superintelligent system. To\naddress these concerns, we propose a new RLVR paradigm called Absolute Zero, in\nwhich a single model learns to propose tasks that maximize its own learning\nprogress and improves reasoning by solving them, without relying on any\nexternal data. Under this paradigm, we introduce the Absolute Zero Reasoner\n(AZR), a system that self-evolves its training curriculum and reasoning ability\nby using a code executor to both validate proposed code reasoning tasks and\nverify answers, serving as an unified source of verifiable reward to guide\nopen-ended yet grounded learning. Despite being trained entirely without\nexternal data, AZR achieves overall SOTA performance on coding and mathematical\nreasoning tasks, outperforming existing zero-setting models that rely on tens\nof thousands of in-domain human-curated examples. Furthermore, we demonstrate\nthat AZR can be effectively applied across different model scales and is\ncompatible with various model classes.",
      "tldr_zh": "该论文提出了一种全新的强化学习范式，名为“绝对零度”(Absolute Zero)，旨在解决现有基于可验证奖励的强化学习(RLVR)方法对人工标注数据的依赖问题。该范式下的“绝对零度推理器”(AZR)无需任何外部数据，仅通过自身生成任务并解决，从而提升推理能力。AZR利用代码执行器验证任务和答案，形成统一的可验证奖励来源，引导开放式学习。实验结果表明，AZR在代码和数学推理任务上取得了SOTA性能，超越了依赖大量人工标注样本的现有零样本模型，并且适用于不同规模和类型的模型。该研究为AI超越人类智能后的自主学习提供了一种新的思路。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03335v2",
      "published_date": "2025-05-06 09:08:00 UTC",
      "updated_date": "2025-05-07 13:01:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:26:13.616608"
    },
    {
      "arxiv_id": "2505.03332v1",
      "title": "AI-Driven Scholarly Peer Review via Persistent Workflow Prompting, Meta-Prompting, and Meta-Reasoning",
      "title_zh": "基于持久工作流提示、元提示和元推理的 AI 驱动学术同行评审\n",
      "authors": [
        "Evgeny Markhasin"
      ],
      "abstract": "Critical peer review of scientific manuscripts presents a significant\nchallenge for Large Language Models (LLMs), partly due to data limitations and\nthe complexity of expert reasoning. This report introduces Persistent Workflow\nPrompting (PWP), a potentially broadly applicable prompt engineering\nmethodology designed to bridge this gap using standard LLM chat interfaces\n(zero-code, no APIs). We present a proof-of-concept PWP prompt for the critical\nanalysis of experimental chemistry manuscripts, featuring a hierarchical,\nmodular architecture (structured via Markdown) that defines detailed analysis\nworkflows. We develop this PWP prompt through iterative application of\nmeta-prompting techniques and meta-reasoning aimed at systematically codifying\nexpert review workflows, including tacit knowledge. Submitted once at the start\nof a session, this PWP prompt equips the LLM with persistent workflows\ntriggered by subsequent queries, guiding modern reasoning LLMs through\nsystematic, multimodal evaluations. Demonstrations show the PWP-guided LLM\nidentifying major methodological flaws in a test case while mitigating LLM\ninput bias and performing complex tasks, including distinguishing claims from\nevidence, integrating text/photo/figure analysis to infer parameters, executing\nquantitative feasibility checks, comparing estimates against claims, and\nassessing a priori plausibility. To ensure transparency and facilitate\nreplication, we provide full prompts, detailed demonstration analyses, and logs\nof interactive chats as supplementary resources. Beyond the specific\napplication, this work offers insights into the meta-development process\nitself, highlighting the potential of PWP, informed by detailed workflow\nformalization, to enable sophisticated analysis using readily available LLMs\nfor complex scientific tasks.",
      "tldr_zh": "该论文提出了一种名为持久工作流提示(Persistent Workflow Prompting, PWP)的提示工程方法，旨在提升大型语言模型(LLMs)在学术同行评审中的表现。PWP通过分层模块化的架构，利用Markdown结构定义详细的分析工作流，并结合元提示(meta-prompting)和元推理(meta-reasoning)技术，系统地编码专家评审工作流，包括隐性知识。实验证明，PWP能有效引导LLM进行系统性的多模态评估，识别实验化学手稿中的方法缺陷，并减轻LLM的输入偏差。该研究展示了PWP在复杂科学任务中利用现有LLM进行复杂分析的潜力，并提供了完整的提示、分析和交互日志以供参考。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "22 pages, 36 pages (references and appendixes)",
      "pdf_url": "http://arxiv.org/pdf/2505.03332v1",
      "published_date": "2025-05-06 09:06:18 UTC",
      "updated_date": "2025-05-06 09:06:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:26:25.785510"
    },
    {
      "arxiv_id": "2505.03327v1",
      "title": "Very High-Resolution Forest Mapping with TanDEM-X InSAR Data and Self-Supervised Learning",
      "title_zh": "利用 TanDEM-X InSAR 数据和自监督学习进行超高分辨率森林测绘\n",
      "authors": [
        "José-Luis Bueso-Bello",
        "Benjamin Chauvel",
        "Daniel Carcereri",
        "Philipp Posovszky",
        "Pietro Milillo",
        "Jennifer Ruiz",
        "Juan-Carlos Fernández-Diaz",
        "Carolina González",
        "Michele Martone",
        "Ronny Hänsch",
        "Paola Rizzoli"
      ],
      "abstract": "Deep learning models have shown encouraging capabilities for mapping\naccurately forests at medium resolution with TanDEM-X interferometric SAR data.\nSuch models, as most of current state-of-the-art deep learning techniques in\nremote sensing, are trained in a fully-supervised way, which requires a large\namount of labeled data for training and validation. In this work, our aim is to\nexploit the high-resolution capabilities of the TanDEM-X mission to map forests\nat 6 m. The goal is to overcome the intrinsic limitations posed by\nmidresolution products, which affect, e.g., the detection of narrow roads\nwithin vegetated areas and the precise delineation of forested regions\ncontours. To cope with the lack of extended reliable reference datasets at such\na high resolution, we investigate self-supervised learning techniques for\nextracting highly informative representations from the input features, followed\nby a supervised training step with a significantly smaller number of reliable\nlabels. A 1 m resolution forest/non-forest reference map over Pennsylvania,\nUSA, allows for comparing different training approaches for the development of\nan effective forest mapping framework with limited labeled samples. We select\nthe best-performing approach over this test region and apply it in a real-case\nforest mapping scenario over the Amazon rainforest, where only very few labeled\ndata at high resolution are available. In this challenging scenario, the\nproposed self-supervised framework significantly enhances the classification\naccuracy with respect to fully-supervised methods, trained using the same\namount of labeled data, representing an extremely promising starting point for\nlarge-scale, very high-resolution forest mapping with TanDEM-X data.",
      "tldr_zh": "该研究利用TanDEM-X干涉合成孔径雷达(InSAR)数据和自监督学习方法，旨在实现6米分辨率的森林精确测绘，克服中等分辨率产品在检测植被区狭窄道路和精确划分森林区域轮廓方面的局限性。为解决高分辨率参考数据集匮乏的问题，研究采用自监督学习技术从输入特征中提取信息量大的表征，然后使用少量可靠标签进行监督训练。在美国宾夕法尼亚州的实验表明，该方法优于完全监督方法。在亚马逊雨林的实际森林测绘场景中，该自监督框架显著提高了分类精度，为使用TanDEM-X数据进行大规模、超高分辨率森林测绘提供了一个有希望的起点。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint submitted to Remote Sensing of Environment",
      "pdf_url": "http://arxiv.org/pdf/2505.03327v1",
      "published_date": "2025-05-06 08:54:28 UTC",
      "updated_date": "2025-05-06 08:54:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:26:37.578246"
    },
    {
      "arxiv_id": "2505.03319v1",
      "title": "SD-VSum: A Method and Dataset for Script-Driven Video Summarization",
      "title_zh": "SD-VSum：一种脚本驱动的视频摘要方法和数据集\n",
      "authors": [
        "Manolis Mylonas",
        "Evlampios Apostolidis",
        "Vasileios Mezaris"
      ],
      "abstract": "In this work, we introduce the task of script-driven video summarization,\nwhich aims to produce a summary of the full-length video by selecting the parts\nthat are most relevant to a user-provided script outlining the visual content\nof the desired summary. Following, we extend a recently-introduced large-scale\ndataset for generic video summarization (VideoXum) by producing natural\nlanguage descriptions of the different human-annotated summaries that are\navailable per video. In this way we make it compatible with the introduced\ntask, since the available triplets of ``video, summary and summary\ndescription'' can be used for training a method that is able to produce\ndifferent summaries for a given video, driven by the provided script about the\ncontent of each summary. Finally, we develop a new network architecture for\nscript-driven video summarization (SD-VSum), that relies on the use of a\ncross-modal attention mechanism for aligning and fusing information from the\nvisual and text modalities. Our experimental evaluations demonstrate the\nadvanced performance of SD-VSum against state-of-the-art approaches for\nquery-driven and generic (unimodal and multimodal) summarization from the\nliterature, and document its capacity to produce video summaries that are\nadapted to each user's needs about their content.",
      "tldr_zh": "该论文提出了脚本驱动的视频摘要(SD-VSum)任务，旨在根据用户提供的脚本生成视频摘要，脚本描述了期望摘要的视觉内容。作者扩展了大规模视频摘要数据集(VideoXum)，为每个人工标注的摘要提供了自然语言描述，从而使其适用于该任务。同时，作者开发了一种新的网络架构SD-VSum，它依赖于跨模态注意力机制来对齐和融合视觉和文本模态的信息。实验结果表明，SD-VSum在性能上优于现有的查询驱动和通用视频摘要方法，并能够生成适应用户需求的视频摘要。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Under review",
      "pdf_url": "http://arxiv.org/pdf/2505.03319v1",
      "published_date": "2025-05-06 08:47:14 UTC",
      "updated_date": "2025-05-06 08:47:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:26:49.337664"
    },
    {
      "arxiv_id": "2505.03315v1",
      "title": "Artificial Behavior Intelligence: Technology, Challenges, and Future Directions",
      "title_zh": "人工行为智能：技术、挑战与未来方向\n",
      "authors": [
        "Kanghyun Jo",
        "Jehwan Choi",
        "Kwanho Kim",
        "Seongmin Kim",
        "Duy-Linh Nguyen",
        "Xuan-Thuy Vo",
        "Adri Priadana",
        "Tien-Dat Tran"
      ],
      "abstract": "Understanding and predicting human behavior has emerged as a core capability\nin various AI application domains such as autonomous driving, smart healthcare,\nsurveillance systems, and social robotics. This paper defines the technical\nframework of Artificial Behavior Intelligence (ABI), which comprehensively\nanalyzes and interprets human posture, facial expressions, emotions, behavioral\nsequences, and contextual cues. It details the essential components of ABI,\nincluding pose estimation, face and emotion recognition, sequential behavior\nanalysis, and context-aware modeling. Furthermore, we highlight the\ntransformative potential of recent advances in large-scale pretrained models,\nsuch as large language models (LLMs), vision foundation models, and multimodal\nintegration models, in significantly improving the accuracy and\ninterpretability of behavior recognition. Our research team has a strong\ninterest in the ABI domain and is actively conducting research, particularly\nfocusing on the development of intelligent lightweight models capable of\nefficiently inferring complex human behaviors. This paper identifies several\ntechnical challenges that must be addressed to deploy ABI in real-world\napplications including learning behavioral intelligence from limited data,\nquantifying uncertainty in complex behavior prediction, and optimizing model\nstructures for low-power, real-time inference. To tackle these challenges, our\nteam is exploring various optimization strategies including lightweight\ntransformers, graph-based recognition architectures, energy-aware loss\nfunctions, and multimodal knowledge distillation, while validating their\napplicability in real-time environments.",
      "tldr_zh": "本文定义了人工智能行为智能(Artificial Behavior Intelligence, ABI)的技术框架，该框架旨在全面分析和解释人类的姿势、面部表情、情绪、行为序列和上下文线索。ABI的关键组成部分包括姿势估计、面部和情绪识别、序列行为分析和上下文感知建模。大规模预训练模型，如大型语言模型(LLMs)、视觉基础模型和多模态集成模型，显著提高了行为识别的准确性和可解释性。文中还探讨了ABI在实际应用中面临的挑战，例如从有限数据中学习行为智能，量化复杂行为预测中的不确定性，以及优化低功耗实时推理的模型结构，并提出了轻量级Transformer、图神经网络、能量感知损失函数和多模态知识蒸馏等优化策略。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 6 figures, Pre-print for IWIS2025",
      "pdf_url": "http://arxiv.org/pdf/2505.03315v1",
      "published_date": "2025-05-06 08:45:44 UTC",
      "updated_date": "2025-05-06 08:45:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:27:01.440657"
    },
    {
      "arxiv_id": "2505.03314v1",
      "title": "Mamba-Diffusion Model with Learnable Wavelet for Controllable Symbolic Music Generation",
      "title_zh": "基于可学习小波变换的 Mamba-Diffusion 模型，用于可控的符号音乐生成\n",
      "authors": [
        "Jincheng Zhang",
        "György Fazekas",
        "Charalampos Saitis"
      ],
      "abstract": "The recent surge in the popularity of diffusion models for image synthesis\nhas attracted new attention to their potential for generation tasks in other\ndomains. However, their applications to symbolic music generation remain\nlargely under-explored because symbolic music is typically represented as\nsequences of discrete events and standard diffusion models are not well-suited\nfor discrete data. We represent symbolic music as image-like pianorolls,\nfacilitating the use of diffusion models for the generation of symbolic music.\nMoreover, this study introduces a novel diffusion model that incorporates our\nproposed Transformer-Mamba block and learnable wavelet transform.\nClassifier-free guidance is utilised to generate symbolic music with target\nchords. Our evaluation shows that our method achieves compelling results in\nterms of music quality and controllability, outperforming the strong baseline\nin pianoroll generation. Our code is available at\nhttps://github.com/jinchengzhanggg/proffusion.",
      "tldr_zh": "该研究探索了扩散模型在符号音乐生成中的应用，并提出了一种新颖的Mamba-Diffusion模型，该模型结合了Transformer-Mamba块和可学习的小波变换。通过将符号音乐表示为类似图像的钢琴卷帘，该方法能够利用扩散模型生成音乐。此外，利用无分类器指导，该模型可以根据目标和弦生成可控的符号音乐。实验结果表明，该方法在音乐质量和可控性方面均优于强大的基线模型，证明了其在钢琴卷帘生成方面的有效性。\n",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03314v1",
      "published_date": "2025-05-06 08:44:52 UTC",
      "updated_date": "2025-05-06 08:44:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:27:13.265653"
    },
    {
      "arxiv_id": "2505.03303v1",
      "title": "Comparative Analysis of Lightweight Deep Learning Models for Memory-Constrained Devices",
      "title_zh": "轻量级深度学习模型在内存受限设备上的对比分析\n",
      "authors": [
        "Tasnim Shahriar"
      ],
      "abstract": "This paper presents a comprehensive evaluation of lightweight deep learning\nmodels for image classification, emphasizing their suitability for deployment\nin resource-constrained environments such as low-memory devices. Five\nstate-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet,\nEfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diverse\ndatasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed using\nfour key performance metrics: classification accuracy, inference time,\nfloating-point operations (FLOPs), and model size. Additionally, we investigate\nthe impact of hyperparameter tuning, data augmentation, and training paradigms\nby comparing pretrained models with scratch-trained counterparts, focusing on\nMobileNetV3 Small. Our findings reveal that transfer learning significantly\nenhances model accuracy and computational efficiency, particularly for complex\ndatasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highest\naccuracy, while MobileNetV3 offers the best balance between accuracy and\nefficiency, and SqueezeNet excels in inference speed and compactness. This\nstudy highlights critical trade-offs between accuracy and efficiency, offering\nactionable insights for deploying lightweight models in real-world applications\nwhere computational resources are limited. By addressing these challenges, this\nresearch contributes to optimizing deep learning systems for edge computing and\nmobile platforms.",
      "tldr_zh": "本文对五种轻量级深度学习模型（MobileNetV3 Small, ResNet18, SqueezeNet, EfficientNetV2-S, ShuffleNetV2）在资源受限设备上的图像分类性能进行了全面评估。研究在CIFAR-10、CIFAR-100和Tiny ImageNet三个数据集上，从分类精度、推理时间、FLOPs和模型大小四个关键指标对模型进行了基准测试。重点考察了超参数调整、数据增强和训练范式（包括预训练模型和从头训练模型）对MobileNetV3 Small的影响。结果表明，迁移学习显著提高了模型精度和计算效率，EfficientNetV2精度最高，MobileNetV3在精度和效率之间取得了最佳平衡，而SqueezeNet在推理速度和模型紧凑性方面表现出色。该研究揭示了精度和效率之间的关键权衡，为在计算资源有限的实际应用中部署轻量级模型提供了可操作的见解。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68-XX (Primary) 68Txx, 68T07 (Secondary)"
      ],
      "primary_category": "cs.CV",
      "comment": "22 pages, 10 figures, 4 tables, submitted to Springer - Pattern\n  Recognition and Image Analysis",
      "pdf_url": "http://arxiv.org/pdf/2505.03303v1",
      "published_date": "2025-05-06 08:36:01 UTC",
      "updated_date": "2025-05-06 08:36:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:27:25.689043"
    },
    {
      "arxiv_id": "2505.03299v1",
      "title": "Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A Capabilities Encoding Approach",
      "title_zh": "迈向遥感领域基础模型的高效基准测试：一种能力编码方法\n",
      "authors": [
        "Pierre Adorni",
        "Minh-Tan Pham",
        "Stéphane May",
        "Sébastien Lefèvre"
      ],
      "abstract": "Foundation models constitute a significant advancement in computer vision:\nafter a single, albeit costly, training phase, they can address a wide array of\ntasks. In the field of Earth observation, over 75 remote sensing vision\nfoundation models have been developed in the past four years. However, none has\nconsistently outperformed the others across all available downstream tasks. To\nfacilitate their comparison, we propose a cost-effective method for predicting\na model's performance on multiple downstream tasks without the need for\nfine-tuning on each one. This method is based on what we call \"capabilities\nencoding.\" The utility of this novel approach is twofold: we demonstrate its\npotential to simplify the selection of a foundation model for a given new task,\nand we employ it to offer a fresh perspective on the existing literature,\nsuggesting avenues for future research. Codes are available at\nhttps://github.com/pierreadorni/capabilities-encoding.",
      "tldr_zh": "该论文提出了一种高效的遥感领域基础模型(Foundation Models)基准测试方法，称为“能力编码”(Capabilities Encoding)。该方法旨在解决目前遥感领域涌现的众多基础模型缺乏统一、高效的评估标准的问题。通过能力编码，该方法能够在无需在每个下游任务上进行微调的情况下，预测模型在多个下游任务上的性能。该方法简化了针对特定新任务选择合适基础模型的过程，并为未来的研究方向提供了新的视角。代码已开源。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at the MORSE workshop of CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2505.03299v1",
      "published_date": "2025-05-06 08:29:18 UTC",
      "updated_date": "2025-05-06 08:29:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:27:37.402016"
    },
    {
      "arxiv_id": "2505.03296v1",
      "title": "The Unreasonable Effectiveness of Discrete-Time Gaussian Process Mixtures for Robot Policy Learning",
      "title_zh": "离散时间高斯过程混合模型在机器人策略学习中出人意料的有效性\n",
      "authors": [
        "Jan Ole von Hartz",
        "Adrian Röfer",
        "Joschka Boedecker",
        "Abhinav Valada"
      ],
      "abstract": "We present Mixture of Discrete-time Gaussian Processes (MiDiGap), a novel\napproach for flexible policy representation and imitation learning in robot\nmanipulation. MiDiGap enables learning from as few as five demonstrations using\nonly camera observations and generalizes across a wide range of challenging\ntasks. It excels at long-horizon behaviors such as making coffee, highly\nconstrained motions such as opening doors, dynamic actions such as scooping\nwith a spatula, and multimodal tasks such as hanging a mug. MiDiGap learns\nthese tasks on a CPU in less than a minute and scales linearly to large\ndatasets. We also develop a rich suite of tools for inference-time steering\nusing evidence such as collision signals and robot kinematic constraints. This\nsteering enables novel generalization capabilities, including obstacle\navoidance and cross-embodiment policy transfer. MiDiGap achieves\nstate-of-the-art performance on diverse few-shot manipulation benchmarks. On\nconstrained RLBench tasks, it improves policy success by 76 percentage points\nand reduces trajectory cost by 67%. On multimodal tasks, it improves policy\nsuccess by 48 percentage points and increases sample efficiency by a factor of\n20. In cross-embodiment transfer, it more than doubles policy success. We make\nthe code publicly available at https://midigap.cs.uni-freiburg.de.",
      "tldr_zh": "该论文提出了一种新的机器人策略表示和模仿学习方法，称为离散时间高斯过程混合模型(MiDiGap)。MiDiGap仅使用摄像头观测，就能从少量（低至5个）的演示中学习，并泛化到各种具有挑战性的任务，例如制作咖啡、开门、铲东西和挂杯子等长时程、高约束、动态和多模态任务。MiDiGap在CPU上不到一分钟即可完成学习，并可线性扩展到大型数据集。此外，该方法还开发了一套丰富的推理时引导工具，例如使用碰撞信号和机器人运动学约束，实现了避障和跨机器人策略迁移等新的泛化能力。在多个few-shot操作基准测试中，MiDiGap取得了state-of-the-art的性能，例如在约束RLBench任务中，策略成功率提高了76个百分点，轨迹成本降低了67%。代码已公开。\n",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Submitted for publication to IEEE Transaction on Robotics",
      "pdf_url": "http://arxiv.org/pdf/2505.03296v1",
      "published_date": "2025-05-06 08:27:23 UTC",
      "updated_date": "2025-05-06 08:27:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:27:49.863291"
    },
    {
      "arxiv_id": "2505.03295v1",
      "title": "Capability-Driven Skill Generation with LLMs: A RAG-Based Approach for Reusing Existing Libraries and Interfaces",
      "title_zh": "基于LLM的能力驱动型技能生成：一种基于RAG的重用现有库和接口的方法\n",
      "authors": [
        "Luis Miguel Vieira da Silva",
        "Aljosha Köcher",
        "Nicolas König",
        "Felix Gehlhoff",
        "Alexander Fay"
      ],
      "abstract": "Modern automation systems increasingly rely on modular architectures, with\ncapabilities and skills as one solution approach. Capabilities define the\nfunctions of resources in a machine-readable form and skills provide the\nconcrete implementations that realize those capabilities. However, the\ndevelopment of a skill implementation conforming to a corresponding capability\nremains a time-consuming and challenging task. In this paper, we present a\nmethod that treats capabilities as contracts for skill implementations and\nleverages large language models to generate executable code based on natural\nlanguage user input. A key feature of our approach is the integration of\nexisting software libraries and interface technologies, enabling the generation\nof skill implementations across different target languages. We introduce a\nframework that allows users to incorporate their own libraries and resource\ninterfaces into the code generation process through a retrieval-augmented\ngeneration architecture. The proposed method is evaluated using an autonomous\nmobile robot controlled via Python and ROS 2, demonstrating the feasibility and\nflexibility of the approach.",
      "tldr_zh": "本文提出了一种基于能力驱动的技能生成方法，利用大型语言模型(LLMs)和检索增强生成(RAG)架构，旨在复用现有软件库和接口，从而简化自动化系统中技能的开发。该方法将能力视为技能实现的“合约”，并根据自然语言用户输入生成可执行代码。通过RAG架构，用户可以将自己的库和资源接口整合到代码生成过程中。实验结果表明，该方法在基于Python和ROS 2控制的自主移动机器人上具有可行性和灵活性。\n",
      "categories": [
        "cs.AI",
        "cs.RO",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03295v1",
      "published_date": "2025-05-06 08:27:04 UTC",
      "updated_date": "2025-05-06 08:27:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:28:01.313877"
    },
    {
      "arxiv_id": "2505.03281v1",
      "title": "Physics-inspired Energy Transition Neural Network for Sequence Learning",
      "title_zh": "受物理启发的能量跃迁神经网络用于序列学习\n",
      "authors": [
        "Zhou Wu",
        "Junyi An",
        "Baile Xu",
        "Furao Shen",
        "Jian Zhao"
      ],
      "abstract": "Recently, the superior performance of Transformers has made them a more\nrobust and scalable solution for sequence modeling than traditional recurrent\nneural networks (RNNs). However, the effectiveness of Transformer in capturing\nlong-term dependencies is primarily attributed to their comprehensive\npair-modeling process rather than inherent inductive biases toward sequence\nsemantics. In this study, we explore the capabilities of pure RNNs and reassess\ntheir long-term learning mechanisms. Inspired by the physics energy transition\nmodels that track energy changes over time, we propose a effective recurrent\nstructure called the``Physics-inspired Energy Transition Neural Network\"\n(PETNN). We demonstrate that PETNN's memory mechanism effectively stores\ninformation over long-term dependencies. Experimental results indicate that\nPETNN outperforms transformer-based methods across various sequence tasks.\nFurthermore, owing to its recurrent nature, PETNN exhibits significantly lower\ncomplexity. Our study presents an optimal foundational recurrent architecture\nand highlights the potential for developing effective recurrent neural networks\nin fields currently dominated by Transformer.",
      "tldr_zh": "该论文提出了一种受物理学能量跃迁模型启发的循环神经网络结构，称为“物理启发式能量跃迁神经网络”(PETNN)。PETNN旨在通过模拟能量随时间的变化来有效存储长期依赖信息，从而提升RNN在序列建模中的能力。实验结果表明，在各种序列任务中，PETNN的性能优于基于Transformer的方法。此外，由于其循环特性，PETNN的复杂度显著降低。该研究展示了一种优化的基础循环架构，并强调了在目前由Transformer主导的领域中开发有效循环神经网络的潜力。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03281v1",
      "published_date": "2025-05-06 08:07:15 UTC",
      "updated_date": "2025-05-06 08:07:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:28:13.544467"
    },
    {
      "arxiv_id": "2505.03275v1",
      "title": "RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via Retrieval-Augmented Generation",
      "title_zh": "RAG-MCP：通过检索增强生成缓解 LLM 工具选择中的提示膨胀\n",
      "authors": [
        "Tiantian Gan",
        "Qiyao Sun"
      ],
      "abstract": "Large language models (LLMs) struggle to effectively utilize a growing number\nof external tools, such as those defined by the Model Context Protocol\n(MCP)\\cite{IntroducingMCP}, due to prompt bloat and selection complexity. We\nintroduce RAG-MCP, a Retrieval-Augmented Generation framework that overcomes\nthis challenge by offloading tool discovery. RAG-MCP uses semantic retrieval to\nidentify the most relevant MCP(s) for a given query from an external index\nbefore engaging the LLM. Only the selected tool descriptions are passed to the\nmodel, drastically reducing prompt size and simplifying decision-making.\nExperiments, including an MCP stress test, demonstrate RAG-MCP significantly\ncuts prompt tokens (e.g., by over 50%) and more than triples tool selection\naccuracy (43.13% vs 13.62% baseline) on benchmark tasks. RAG-MCP enables\nscalable and accurate tool integration for LLMs.",
      "tldr_zh": "该论文提出了RAG-MCP框架，旨在解决大型语言模型(LLMs)因提示膨胀和选择复杂性而难以有效利用大量外部工具的问题。RAG-MCP通过检索增强生成(Retrieval-Augmented Generation)技术，利用语义检索从外部索引中识别与给定查询最相关的Model Context Protocol (MCP)，从而卸载工具发现过程。该方法显著减少了传递给LLM的提示token数量，简化了决策过程。实验结果表明，RAG-MCP在基准测试任务中，能够显著减少提示token（例如，超过50%），并将工具选择准确率提高三倍以上（43.13% vs 13.62%）。该框架为LLMs实现了可扩展且准确的工具集成。\n",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03275v1",
      "published_date": "2025-05-06 08:05:35 UTC",
      "updated_date": "2025-05-06 08:05:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:28:25.579076"
    },
    {
      "arxiv_id": "2505.03265v1",
      "title": "Synthline: A Product Line Approach for Synthetic Requirements Engineering Data Generation using Large Language Models",
      "title_zh": "Synthline：一种使用大型语言模型生成合成需求工程数据的产品线方法\n",
      "authors": [
        "Abdelkarim El-Hajjami",
        "Camille Salinesi"
      ],
      "abstract": "While modern Requirements Engineering (RE) heavily relies on natural language\nprocessing and Machine Learning (ML) techniques, their effectiveness is limited\nby the scarcity of high-quality datasets. This paper introduces Synthline, a\nProduct Line (PL) approach that leverages Large Language Models to\nsystematically generate synthetic RE data for classification-based use cases.\nThrough an empirical evaluation conducted in the context of using ML for the\nidentification of requirements specification defects, we investigated both the\ndiversity of the generated data and its utility for training downstream models.\nOur analysis reveals that while synthetic datasets exhibit less diversity than\nreal data, they are good enough to serve as viable training resources.\nMoreover, our evaluation shows that combining synthetic and real data leads to\nsubstantial performance improvements. Specifically, hybrid approaches achieve\nup to 85% improvement in precision and a 2x increase in recall compared to\nmodels trained exclusively on real data. These findings demonstrate the\npotential of PL-based synthetic data generation to address data scarcity in RE.\nWe make both our implementation and generated datasets publicly available to\nsupport reproducibility and advancement in the field.",
      "tldr_zh": "该论文提出了Synthline，一种基于产品线(Product Line, PL)方法，利用大型语言模型(Large Language Models)系统地生成合成需求工程(Requirements Engineering, RE)数据，用于基于分类的用例。研究通过在机器学习识别需求规格缺陷的背景下进行实证评估，调查了生成数据的多样性及其用于训练下游模型的效用。结果表明，虽然合成数据集的多样性不如真实数据，但足以作为可行的训练资源。将合成数据与真实数据相结合可以显著提高性能，混合方法在精确度方面提高了85%，召回率提高了2倍。该研究表明，基于PL的合成数据生成具有解决RE领域数据稀缺问题的潜力，并公开了实现和生成的数据集。\n",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03265v1",
      "published_date": "2025-05-06 07:57:16 UTC",
      "updated_date": "2025-05-06 07:57:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:28:37.604604"
    },
    {
      "arxiv_id": "2505.03242v1",
      "title": "Seeing the Abstract: Translating the Abstract Language for Vision Language Models",
      "title_zh": "看懂摘要：为视觉语言模型翻译抽象语言\n",
      "authors": [
        "Davide Talon",
        "Federico Girella",
        "Ziyue Liu",
        "Marco Cristani",
        "Yiming Wang"
      ],
      "abstract": "Natural language goes beyond dryly describing visual content. It contains\nrich abstract concepts to express feeling, creativity and properties that\ncannot be directly perceived. Yet, current research in Vision Language Models\n(VLMs) has not shed light on abstract-oriented language. Our research breaks\nnew ground by uncovering its wide presence and under-estimated value, with\nextensive analysis. Particularly, we focus our investigation on the fashion\ndomain, a highly-representative field with abstract expressions. By analyzing\nrecent large-scale multimodal fashion datasets, we find that abstract terms\nhave a dominant presence, rivaling the concrete ones, providing novel\ninformation, and being useful in the retrieval task. However, a critical\nchallenge emerges: current general-purpose or fashion-specific VLMs are\npre-trained with databases that lack sufficient abstract words in their text\ncorpora, thus hindering their ability to effectively represent\nabstract-oriented language. We propose a training-free and model-agnostic\nmethod, Abstract-to-Concrete Translator (ACT), to shift abstract\nrepresentations towards well-represented concrete ones in the VLM latent space,\nusing pre-trained models and existing multimodal databases. On the\ntext-to-image retrieval task, despite being training-free, ACT outperforms the\nfine-tuned VLMs in both same- and cross-dataset settings, exhibiting its\neffectiveness with a strong generalization capability. Moreover, the\nimprovement introduced by ACT is consistent with various VLMs, making it a\nplug-and-play solution.",
      "tldr_zh": "该研究着重探讨了视觉语言模型(VLMs)在理解和处理抽象语言方面的不足，尤其是在时尚领域。通过分析大规模多模态时尚数据集，发现抽象术语占据主导地位，但现有VLMs由于训练数据中缺乏足够的抽象词汇，难以有效表示抽象语言。为此，论文提出了一种无需训练且模型无关的方法——Abstract-to-Concrete Translator (ACT)，利用预训练模型和现有数据库，将抽象表示转换为VLM潜在空间中易于理解的具体表示。实验表明，在文本到图像检索任务中，ACT优于微调的VLMs，展现了其有效性和泛化能力，并且可以作为即插即用的解决方案应用于各种VLMs。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CVPR25. Project page:\n  https://davidetalon.github.io/fashionact-page/",
      "pdf_url": "http://arxiv.org/pdf/2505.03242v1",
      "published_date": "2025-05-06 07:14:10 UTC",
      "updated_date": "2025-05-06 07:14:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:28:49.665615"
    },
    {
      "arxiv_id": "2505.03217v1",
      "title": "Accelerating Evolution: Integrating PSO Principles into Real-Coded Genetic Algorithm Crossover",
      "title_zh": "加速进化：将 PSO 原理整合到实数编码遗传算法交叉中\n",
      "authors": [
        "Xiaobo Jin",
        "JiaShu Tu"
      ],
      "abstract": "This study introduces an innovative crossover operator named Particle Swarm\nOptimization-inspired Crossover (PSOX), which is specifically developed for\nreal-coded genetic algorithms. Departing from conventional crossover approaches\nthat only exchange information between individuals within the same generation,\nPSOX uniquely incorporates guidance from both the current global best solution\nand historical optimal solutions across multiple generations. This novel\nmechanism enables the algorithm to maintain population diversity while\nsimultaneously accelerating convergence toward promising regions of the search\nspace. The effectiveness of PSOX is rigorously evaluated through comprehensive\nexperiments on 15 benchmark test functions with diverse characteristics,\nincluding unimodal, multimodal, and highly complex landscapes. Comparative\nanalysis against five state-of-the-art crossover operators reveals that PSOX\nconsistently delivers superior performance in terms of solution accuracy,\nalgorithmic stability, and convergence speed, especially when combined with an\nappropriate mutation strategy. Furthermore, the study provides an in-depth\ninvestigation of how different mutation rates influence PSOX's performance,\nyielding practical guidelines for parameter tuning when addressing optimization\nproblems with varying landscape properties.",
      "tldr_zh": "该研究提出了一种受粒子群优化(PSO)启发的交叉算子(PSOX)，用于加速实数编码遗传算法的收敛。PSOX不同于传统交叉算子，它不仅在同代个体间交换信息，还引入了全局最优解和历史最优解的指导。这种机制在保持种群多样性的同时，加速算法向有希望的搜索区域收敛。通过在15个基准测试函数上的实验，PSOX在解决方案精度、算法稳定性和收敛速度方面均优于五种最先进的交叉算子，尤其是在与适当的变异策略结合使用时。该研究还深入探讨了不同变异率对PSOX性能的影响，为解决不同景观属性的优化问题提供了参数调整的实用指南。\n",
      "categories": [
        "cs.NE",
        "cs.AI",
        "I.2.8; G.1.6"
      ],
      "primary_category": "cs.NE",
      "comment": "14 pages,2 figures,4 tables",
      "pdf_url": "http://arxiv.org/pdf/2505.03217v1",
      "published_date": "2025-05-06 06:17:57 UTC",
      "updated_date": "2025-05-06 06:17:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:29:01.641538"
    },
    {
      "arxiv_id": "2505.03214v1",
      "title": "DocSpiral: A Platform for Integrated Assistive Document Annotation through Human-in-the-Spiral",
      "title_zh": "DocSpiral：通过“人-在-环路”实现集成辅助文档标注的平台\n",
      "authors": [
        "Qiang Sun",
        "Sirui Li",
        "Tingting Bi",
        "Du Huynh",
        "Mark Reynolds",
        "Yuanyi Luo",
        "Wei Liu"
      ],
      "abstract": "Acquiring structured data from domain-specific, image-based documents such as\nscanned reports is crucial for many downstream tasks but remains challenging\ndue to document variability. Many of these documents exist as images rather\nthan as machine-readable text, which requires human annotation to train\nautomated extraction systems. We present DocSpiral, the first\nHuman-in-the-Spiral assistive document annotation platform, designed to address\nthe challenge of extracting structured information from domain-specific,\nimage-based document collections. Our spiral design establishes an iterative\ncycle in which human annotations train models that progressively require less\nmanual intervention. DocSpiral integrates document format normalization,\ncomprehensive annotation interfaces, evaluation metrics dashboard, and API\nendpoints for the development of AI / ML models into a unified workflow.\nExperiments demonstrate that our framework reduces annotation time by at least\n41\\% while showing consistent performance gains across three iterations during\nmodel training. By making this annotation platform freely accessible, we aim to\nlower barriers to AI/ML models development in document processing, facilitating\nthe adoption of large language models in image-based, document-intensive fields\nsuch as geoscience and healthcare. The system is freely available at:\nhttps://app.ai4wa.com. The demonstration video is available:\nhttps://app.ai4wa.com/docs/docspiral/demo.",
      "tldr_zh": "DocSpiral是一个“人在环中”的辅助文档标注平台，旨在解决从特定领域、基于图像的文档（如扫描报告）中提取结构化数据的难题。该平台采用迭代循环的螺旋式设计，利用人工标注训练模型，逐步减少人工干预。DocSpiral集成了文档格式标准化、全面的标注界面、评估指标仪表板和用于AI/ML模型开发的API端点到一个统一的工作流程中。实验表明，该框架在模型训练的三个迭代过程中，标注时间减少了至少41%，并显示出持续的性能提升。该平台旨在降低文档处理中AI/ML模型开发的门槛，促进大型语言模型在地球科学和医疗保健等基于图像的文档密集型领域的应用。\n",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03214v1",
      "published_date": "2025-05-06 06:02:42 UTC",
      "updated_date": "2025-05-06 06:02:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:29:13.648606"
    },
    {
      "arxiv_id": "2505.03204v2",
      "title": "DCS-ST for Classification of Breast Cancer Histopathology Images with Limited Annotations",
      "title_zh": "DCS-ST：用于乳腺癌组织病理学图像分类的解耦对比自监督 Transformer，适用于有限标注数据\n",
      "authors": [
        "Liu Suxing",
        "Byungwon Min"
      ],
      "abstract": "Deep learning methods have shown promise in classifying breast cancer\nhistopathology images, but their performance often declines with limited\nannotated data, a critical challenge in medical imaging due to the high cost\nand expertise required for annotations.",
      "tldr_zh": "该研究针对乳腺癌组织病理学图像分类中，由于标注数据有限导致深度学习方法性能下降的问题，提出了DCS-ST方法。该方法旨在利用有限的标注数据提升分类性能，解决医学图像领域中由于标注成本高昂和需要专业知识而导致的标注数据不足的挑战。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03204v2",
      "published_date": "2025-05-06 05:38:17 UTC",
      "updated_date": "2025-05-07 04:09:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:29:25.191305"
    },
    {
      "arxiv_id": "2505.03196v1",
      "title": "A Trustworthy Multi-LLM Network: Challenges,Solutions, and A Use Case",
      "title_zh": "一个可信的多LLM网络：挑战、解决方案与用例\n",
      "authors": [
        "Haoxiang Luo",
        "Gang Sun",
        "Yinqiu Liu",
        "Dusit Niyato",
        "Hongfang Yu",
        "Mohammed Atiquzzaman",
        "Schahram Dustdar"
      ],
      "abstract": "Large Language Models (LLMs) demonstrate strong potential across a variety of\ntasks in communications and networking due to their advanced reasoning\ncapabilities. However, because different LLMs have different model structures\nand are trained using distinct corpora and methods, they may offer varying\noptimization strategies for the same network issues. Moreover, the limitations\nof an individual LLM's training data, aggravated by the potential maliciousness\nof its hosting device, can result in responses with low confidence or even\nbias. To address these challenges, we propose a blockchain-enabled\ncollaborative framework that connects multiple LLMs into a Trustworthy\nMulti-LLM Network (MultiLLMN). This architecture enables the cooperative\nevaluation and selection of the most reliable and high-quality responses to\ncomplex network optimization problems. Specifically, we begin by reviewing\nrelated work and highlighting the limitations of existing LLMs in collaboration\nand trust, emphasizing the need for trustworthiness in LLM-based systems. We\nthen introduce the workflow and design of the proposed Trustworthy MultiLLMN\nframework. Given the severity of False Base Station (FBS) attacks in B5G and 6G\ncommunication systems and the difficulty of addressing such threats through\ntraditional modeling techniques, we present FBS defense as a case study to\nempirically validate the effectiveness of our approach. Finally, we outline\npromising future research directions in this emerging area.",
      "tldr_zh": "该论文提出了一个基于区块链的、可信的多LLM网络(MultiLLMN)框架，旨在解决单个LLM在通信和网络任务中存在的局限性，例如不同LLM优化策略的差异以及训练数据可能存在的偏差。该框架通过连接多个LLM，实现对复杂网络优化问题响应的协同评估和选择，从而提高可靠性和质量。论文以B5G/6G通信系统中虚假基站(FBS)防御为例，验证了该方法在应对传统建模技术难以解决的威胁方面的有效性。该研究为基于LLM的系统中的可信度问题提供了解决方案，并展望了未来的研究方向。\n",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03196v1",
      "published_date": "2025-05-06 05:32:46 UTC",
      "updated_date": "2025-05-06 05:32:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:29:37.536803"
    },
    {
      "arxiv_id": "2505.03193v1",
      "title": "A study on audio synchronous steganography detection and distributed guide inference model based on sliding spectral features and intelligent inference drive",
      "title_zh": "基于滑动频谱特征和智能推理驱动的音频同步隐写检测与分布式引导推理模型研究\n",
      "authors": [
        "Wei Meng"
      ],
      "abstract": "With the rise of short video platforms in global communication, embedding\nsteganographic data in audio synchronization streams has emerged as a new\ncovert communication method. To address the limitations of traditional\ntechniques in detecting synchronized steganography, this paper proposes a\ndetection and distributed guidance reconstruction model based on short video\n\"Yupan\" samples released by China's South Sea Fleet on TikTok. The method\nintegrates sliding spectrum feature extraction and intelligent inference\nmechanisms. A 25 ms sliding window with short-time Fourier transform (STFT) is\nused to extract the main frequency trajectory and construct the synchronization\nframe detection model (M1), identifying a frame flag \"FFFFFFFFFFFFFFFFFF80\".\nThe subsequent 32-byte payload is decoded by a structured model (M2) to infer\ndistributed guidance commands. Analysis reveals a low-entropy, repetitive byte\nsequence in the 36 to 45 second audio segment with highly concentrated spectral\nenergy, confirming the presence of synchronization frames. Although plaintext\nsemantics are not restored, the consistency in command field layout suggests\nfeatures of military communication protocols. The multi-segment splicing model\nfurther shows cross-video embedding and centralized decoding capabilities. The\nproposed framework validates the effectiveness of sliding spectral features for\nsynchronized steganography detection and builds an extensible inference model\nfor covert communication analysis and tactical guidance simulation on open\nplatforms.",
      "tldr_zh": "该研究针对音频同步隐写术的检测问题，提出了一种基于滑动频谱特征和智能推理驱动的检测与分布式引导重构模型。该模型利用短时傅里叶变换(STFT)提取主频率轨迹，构建同步帧检测模型(M1)，识别帧标志\"FFFFFFFFFFFFFFFFFF80\"，并解码后续32字节的有效载荷。通过分析发现，在特定音频片段中存在低熵、重复的字节序列，表明存在同步帧。虽然无法恢复明文语义，但命令字段布局的一致性暗示了军事通信协议的特征。多段拼接模型进一步展示了跨视频嵌入和集中解码能力。该框架验证了滑动频谱特征在同步隐写检测中的有效性，并为开放平台上的隐蔽通信分析和战术指导模拟构建了一个可扩展的推理模型。\n",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CR",
        "eess.AS",
        "94A12 (Primary), 68T07, 42A38 (Secondary)",
        "H.3.3; I.5.4; I.2.6"
      ],
      "primary_category": "cs.SD",
      "comment": "This paper proposes a novel framework for detecting steganographic\n  content in short video audio streams using sliding spectral features and\n  distributed inference models, combining STFT analysis, entropy-based\n  synchronization, and deep learning-driven decoding strategies",
      "pdf_url": "http://arxiv.org/pdf/2505.03193v1",
      "published_date": "2025-05-06 05:24:11 UTC",
      "updated_date": "2025-05-06 05:24:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:29:49.736785"
    },
    {
      "arxiv_id": "2505.03189v1",
      "title": "Patterns and Mechanisms of Contrastive Activation Engineering",
      "title_zh": "对比激活工程的模式与机制\n",
      "authors": [
        "Yixiong Hao",
        "Ayush Panda",
        "Stepan Shabalin",
        "Sheikh Abdur Raheem Ali"
      ],
      "abstract": "Controlling the behavior of Large Language Models (LLMs) remains a\nsignificant challenge due to their inherent complexity and opacity. While\ntechniques like fine-tuning can modify model behavior, they typically require\nextensive computational resources. Recent work has introduced a class of\ncontrastive activation engineering (CAE) techniques as promising approaches for\nsteering LLM outputs through targeted modifications to their internal\nrepresentations. Applied at inference-time with zero cost, CAE has the\npotential to introduce a new paradigm of flexible, task-specific LLM behavior\ntuning. We analyze the performance of CAE in in-distribution,\nout-of-distribution settings, evaluate drawbacks, and begin to develop\ncomprehensive guidelines for its effective deployment. We find that 1. CAE is\nonly reliably effective when applied to in-distribution contexts. 2. Increasing\nthe number of samples used to generate steering vectors has diminishing returns\nat around 80 samples. 3. Steering vectors are susceptible to adversarial inputs\nthat reverses the behavior that is steered for. 4. Steering vectors harm the\noverall model perplexity. 5. Larger models are more resistant to\nsteering-induced degradation.",
      "tldr_zh": "该研究深入分析了对比激活工程(Contrastive Activation Engineering, CAE)的模式和机制， CAE是一种通过修改LLM内部表示来引导LLM输出的技术，无需耗费大量计算资源。研究评估了CAE在不同场景下的性能，揭示了其局限性，并为有效部署CAE提供了指导。主要发现包括：CAE仅在同分布上下文中有效；用于生成steering vectors的样本数量在80个左右时收益递减；steering vectors容易受到对抗性输入的影响；steering vectors会损害模型的整体困惑度(perplexity)；更大的模型更能抵抗steering带来的性能下降。\n",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "Published at the ICLR 2025 Bi-Align, HAIC, and Building Trust\n  workshops",
      "pdf_url": "http://arxiv.org/pdf/2505.03189v1",
      "published_date": "2025-05-06 05:15:12 UTC",
      "updated_date": "2025-05-06 05:15:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:30:01.628158"
    },
    {
      "arxiv_id": "2505.03176v1",
      "title": "seq-JEPA: Autoregressive Predictive Learning of Invariant-Equivariant World Models",
      "title_zh": "seq-JEPA：不变-等变世界模型的自回归预测学习",
      "authors": [
        "Hafez Ghaemi",
        "Eilif Muller",
        "Shahab Bakhtiari"
      ],
      "abstract": "Current self-supervised algorithms mostly rely on transformations such as\ndata augmentation and masking to learn visual representations. This is achieved\nby inducing invariance or equivariance with respect to these transformations\nafter encoding two views of an image. This dominant two-view paradigm can limit\nthe flexibility of learned representations for downstream adaptation by\ncreating performance trade-offs between invariance-related tasks such as image\nclassification and more fine-grained equivariance-related tasks. In this work,\nwe introduce \\emph{seq-JEPA}, a world modeling paradigm based on\njoint-embedding predictive architecture that leverages architectural inductive\nbiases to resolve this trade-off. Without requiring an additional equivariance\npredictor or loss term, seq-JEPA simultaneously learns two architecturally\nsegregated representations: one equivariant to the specified transformations\nand another invariant to them and suited for tasks such as classification. To\ndo so, our model processes a short sequence of different views (observations)\nof an input image. Each encoded view is concatenated with embeddings\ncorresponding to the relative transformation (action) producing the next\nobservation in the sequence. A transformer encoder outputs an aggregate\nrepresentation of this sequence, which is subsequently conditioned on the\naction leading to the next observation to predict its representation.\nEmpirically, seq-JEPA achieves strong performance on equivariant benchmarks and\nimage classification without sacrificing one for the other. Additionally, our\nframework excels at tasks that inherently require aggregating a sequence of\nobservations, such as path integration across actions and predictive learning\nacross eye movements.",
      "tldr_zh": "该论文提出了seq-JEPA，一种基于联合嵌入预测架构(joint-embedding predictive architecture)的世界建模范式，旨在解决自监督学习中不变性和等变性之间的trade-off问题。seq-JEPA通过处理输入图像的不同视角序列，并结合相对变换（动作）的嵌入，利用架构上的归纳偏置，同时学习两种架构上分离的表示：一种对指定变换等变，另一种对变换不变，适用于分类等任务。实验表明，seq-JEPA在等变基准测试和图像分类上都取得了良好的性能，并且擅长需要聚合观察序列的任务，例如跨动作的路径积分和跨眼球运动的预测学习。该方法无需额外的等变性预测器或损失项。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03176v1",
      "published_date": "2025-05-06 04:39:11 UTC",
      "updated_date": "2025-05-06 04:39:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:30:13.759106"
    },
    {
      "arxiv_id": "2505.03173v1",
      "title": "RAVU: Retrieval Augmented Video Understanding with Compositional Reasoning over Graph",
      "title_zh": "RAVU：基于图的组合推理检索增强视频理解\n",
      "authors": [
        "Sameer Malik",
        "Moyuru Yamada",
        "Ayush Singh",
        "Dishank Aggarwal"
      ],
      "abstract": "Comprehending long videos remains a significant challenge for Large\nMulti-modal Models (LMMs). Current LMMs struggle to process even minutes to\nhours videos due to their lack of explicit memory and retrieval mechanisms. To\naddress this limitation, we propose RAVU (Retrieval Augmented Video\nUnderstanding), a novel framework for video understanding enhanced by retrieval\nwith compositional reasoning over a spatio-temporal graph. We construct a graph\nrepresentation of the video, capturing both spatial and temporal relationships\nbetween entities. This graph serves as a long-term memory, allowing us to track\nobjects and their actions across time. To answer complex queries, we decompose\nthe queries into a sequence of reasoning steps and execute these steps on the\ngraph, retrieving relevant key information. Our approach enables more accurate\nunderstanding of long videos, particularly for queries that require multi-hop\nreasoning and tracking objects across frames. Our approach demonstrate superior\nperformances with limited retrieved frames (5-10) compared with other SOTA\nmethods and baselines on two major video QA datasets, NExT-QA and EgoSchema.",
      "tldr_zh": "RAVU (Retrieval Augmented Video Understanding) 提出了一种新的视频理解框架，通过检索增强和基于时空图的组合推理来解决大型多模态模型 (LMM) 在理解长视频方面的挑战。该框架构建视频的图表示，捕捉实体之间的时空关系，作为长期记忆来跟踪对象及其行为。通过将复杂查询分解为一系列推理步骤并在图上执行，RAVU 能够检索相关信息。实验表明，与现有方法相比，RAVU 在 NExT-QA 和 EgoSchema 两个视频 QA 数据集上，仅使用有限的检索帧 (5-10) 就能表现出卓越的性能，尤其是在需要多跳推理和跨帧对象跟踪的查询上。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03173v1",
      "published_date": "2025-05-06 04:38:09 UTC",
      "updated_date": "2025-05-06 04:38:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:30:26.121565"
    },
    {
      "arxiv_id": "2505.03172v1",
      "title": "Null Counterfactual Factor Interactions for Goal-Conditioned Reinforcement Learning",
      "title_zh": "目标条件强化学习中的零反事实因子交互\n",
      "authors": [
        "Caleb Chuck",
        "Fan Feng",
        "Carl Qi",
        "Chang Shi",
        "Siddhant Agarwal",
        "Amy Zhang",
        "Scott Niekum"
      ],
      "abstract": "Hindsight relabeling is a powerful tool for overcoming sparsity in\ngoal-conditioned reinforcement learning (GCRL), especially in certain domains\nsuch as navigation and locomotion. However, hindsight relabeling can struggle\nin object-centric domains. For example, suppose that the goal space consists of\na robotic arm pushing a particular target block to a goal location. In this\ncase, hindsight relabeling will give high rewards to any trajectory that does\nnot interact with the block. However, these behaviors are only useful when the\nobject is already at the goal -- an extremely rare case in practice. A dataset\ndominated by these kinds of trajectories can complicate learning and lead to\nfailures. In object-centric domains, one key intuition is that meaningful\ntrajectories are often characterized by object-object interactions such as\npushing the block with the gripper. To leverage this intuition, we introduce\nHindsight Relabeling using Interactions (HInt), which combines interactions\nwith hindsight relabeling to improve the sample efficiency of downstream RL.\nHowever because interactions do not have a consensus statistical definition\ntractable for downstream GCRL, we propose a definition of interactions based on\nthe concept of null counterfactual: a cause object is interacting with a target\nobject if, in a world where the cause object did not exist, the target object\nwould have different transition dynamics. We leverage this definition to infer\ninteractions in Null Counterfactual Interaction Inference (NCII), which uses a\n\"nulling'' operation with a learned model to infer interactions. NCII is able\nto achieve significantly improved interaction inference accuracy in both simple\nlinear dynamics domains and dynamic robotic domains in Robosuite, Robot Air\nHockey, and Franka Kitchen and HInt improves sample efficiency by up to 4x.",
      "tldr_zh": "该论文针对目标条件强化学习(GCRL)在以物体为中心的领域中，后见之明重标记(hindsight relabeling)的局限性，提出了基于交互的后见之明重标记方法(HInt)。HInt的核心思想是利用物体间的交互信息来提升样本效率，并引入了“零反事实”(null counterfactual)的概念来定义交互：如果一个物体的缺失会导致另一个物体的运动状态发生改变，则认为两者存在交互。为了推断交互，论文提出了零反事实交互推断(NCII)方法，该方法利用学习到的模型进行“零化”操作来推断交互。实验结果表明，NCII显著提高了交互推断的准确性，HInt将样本效率提高了4倍。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Published at ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2505.03172v1",
      "published_date": "2025-05-06 04:32:47 UTC",
      "updated_date": "2025-05-06 04:32:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:30:37.873874"
    },
    {
      "arxiv_id": "2505.03171v1",
      "title": "CombiBench: Benchmarking LLM Capability for Combinatorial Mathematics",
      "title_zh": "CombiBench：组合数学领域大语言模型能力评测基准\n",
      "authors": [
        "Junqi Liu",
        "Xiaohan Lin",
        "Jonas Bayer",
        "Yael Dillies",
        "Weijie Jiang",
        "Xiaodan Liang",
        "Roman Soletskyi",
        "Haiming Wang",
        "Yunzhou Xie",
        "Beibei Xiong",
        "Zhengfeng Yang",
        "Jujian Zhang",
        "Lihong Zhi",
        "Jia Li",
        "Zhengying Liu"
      ],
      "abstract": "Neurosymbolic approaches integrating large language models with formal\nreasoning have recently achieved human-level performance on mathematics\ncompetition problems in algebra, geometry and number theory. In comparison,\ncombinatorics remains a challenging domain, characterized by a lack of\nappropriate benchmarks and theorem libraries. To address this gap, we introduce\nCombiBench, a comprehensive benchmark comprising 100 combinatorial problems,\neach formalized in Lean~4 and paired with its corresponding informal statement.\nThe problem set covers a wide spectrum of difficulty levels, ranging from\nmiddle school to IMO and university level, and span over ten combinatorial\ntopics. CombiBench is suitable for testing IMO solving capabilities since it\nincludes all IMO combinatorial problems since 2000 (except IMO 2004 P3 as its\nstatement contain an images). Furthermore, we provide a comprehensive and\nstandardized evaluation framework, dubbed Fine-Eval (for\n$\\textbf{F}$ill-in-the-blank $\\textbf{in}$ L$\\textbf{e}$an Evaluation), for\nformal mathematics. It accommodates not only proof-based problems but also, for\nthe first time, the evaluation of fill-in-the-blank questions. Using Fine-Eval\nas the evaluation method and Kimina Lean Server as the backend, we benchmark\nseveral LLMs on CombiBench and observe that their capabilities for formally\nsolving combinatorial problems remain limited. Among all models tested (none of\nwhich has been trained for this particular task), Kimina-Prover attains the\nbest results, solving 7 problems (out of 100) under both ``with solution'' and\n``without solution'' scenarios. We open source the benchmark dataset alongside\nwith the code of the proposed evaluation method at\nhttps://github.com/MoonshotAI/CombiBench/.",
      "tldr_zh": "该论文提出了CombiBench，一个用于评估大型语言模型(LLMs)在组合数学领域能力的综合性基准测试集。CombiBench包含100个组合数学问题，这些问题被形式化为Lean 4代码，并配有相应的非正式描述，难度覆盖中学到大学级别。此外，论文还提出了一个名为Fine-Eval的标准化评估框架，用于形式化数学，不仅支持基于证明的问题，还支持填空题的评估。通过Fine-Eval和Kimina Lean Server，对多个LLM在CombiBench上进行了基准测试，结果表明它们在形式化解决组合数学问题方面的能力仍然有限。其中，Kimina-Prover表现最佳，在有无答案的情况下均解决了7个问题（共100个）。该基准测试数据集和评估方法的代码已开源。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03171v1",
      "published_date": "2025-05-06 04:32:17 UTC",
      "updated_date": "2025-05-06 04:32:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:30:50.077783"
    },
    {
      "arxiv_id": "2505.03156v1",
      "title": "Soft Best-of-n Sampling for Model Alignment",
      "title_zh": "用于模型对齐的软性 Best-of-n 采样",
      "authors": [
        "Claudio Mayrink Verdun",
        "Alex Oesterling",
        "Himabindu Lakkaraju",
        "Flavio P. Calmon"
      ],
      "abstract": "Best-of-$n$ (BoN) sampling is a practical approach for aligning language\nmodel outputs with human preferences without expensive fine-tuning. BoN\nsampling is performed by generating $n$ responses to a prompt and then\nselecting the sample that maximizes a reward function. BoN yields high reward\nvalues in practice at a distortion cost, as measured by the KL-divergence\nbetween the sampled and original distribution. This distortion is coarsely\ncontrolled by varying the number of samples: larger $n$ yields a higher reward\nat a higher distortion cost. We introduce Soft Best-of-$n$ sampling, a\ngeneralization of BoN that allows for smooth interpolation between the original\ndistribution and reward-maximizing distribution through a temperature parameter\n$\\lambda$. We establish theoretical guarantees showing that Soft Best-of-$n$\nsampling converges sharply to the optimal tilted distribution at a rate of\n$O(1/n)$ in KL and the expected (relative) reward. For sequences of discrete\noutputs, we analyze an additive reward model that reveals the fundamental\nlimitations of blockwise sampling.",
      "tldr_zh": "本文提出了一种名为Soft Best-of-$n$ (Soft BoN) 的采样方法，用于模型对齐，旨在解决传统Best-of-$n$ (BoN)采样在提升奖励值的同时引入较大分布失真(KL散度)的问题。Soft BoN通过引入温度参数$\\lambda$，实现了原始分布和奖励最大化分布之间的平滑插值。理论分析表明，Soft BoN以$O(1/n)$的速度快速收敛到最优倾斜分布，并在KL散度和预期（相对）奖励方面均有保证。对于离散输出序列，本文还分析了一个加性奖励模型，揭示了分块采样的根本局限性。\n",
      "categories": [
        "cs.IT",
        "cs.AI",
        "math.IT"
      ],
      "primary_category": "cs.IT",
      "comment": "Accepted for presentation at the 2025 IEEE International Symposium on\n  Information Theory (ISIT 2025)",
      "pdf_url": "http://arxiv.org/pdf/2505.03156v1",
      "published_date": "2025-05-06 04:03:11 UTC",
      "updated_date": "2025-05-06 04:03:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:31:01.747756"
    },
    {
      "arxiv_id": "2505.03154v1",
      "title": "StableMotion: Training Motion Cleanup Models with Unpaired Corrupted Data",
      "title_zh": "StableMotion：使用非配对的损坏数据训练运动清理模型\n",
      "authors": [
        "Yuxuan Mu",
        "Hung Yu Ling",
        "Yi Shi",
        "Ismael Baira Ojeda",
        "Pengcheng Xi",
        "Chang Shu",
        "Fabio Zinno",
        "Xue Bin Peng"
      ],
      "abstract": "Motion capture (mocap) data often exhibits visually jarring artifacts due to\ninaccurate sensors and post-processing. Cleaning this corrupted data can\nrequire substantial manual effort from human experts, which can be a costly and\ntime-consuming process. Previous data-driven motion cleanup methods offer the\npromise of automating this cleanup process, but often require in-domain paired\ncorrupted-to-clean training data. Constructing such paired datasets requires\naccess to high-quality, relatively artifact-free motion clips, which often\nnecessitates laborious manual cleanup. In this work, we present StableMotion, a\nsimple yet effective method for training motion cleanup models directly from\nunpaired corrupted datasets that need cleanup. The core component of our method\nis the introduction of motion quality indicators, which can be easily annotated\nthrough manual labeling or heuristic algorithms and enable training of\nquality-aware motion generation models on raw motion data with mixed quality.\nAt test time, the model can be prompted to generate high-quality motions using\nthe quality indicators. Our method can be implemented through a simple\ndiffusion-based framework, leading to a unified motion generate-discriminate\nmodel, which can be used to both identify and fix corrupted frames. We\ndemonstrate that our proposed method is effective for training motion cleanup\nmodels on raw mocap data in production scenarios by applying StableMotion to\nSoccerMocap, a 245-hour soccer mocap dataset containing real-world motion\nartifacts. The trained model effectively corrects a wide range of motion\nartifacts, reducing motion pops and frozen frames by 68% and 81%, respectively.\nSee https://youtu.be/3Y7MMAH02B4 for more results.",
      "tldr_zh": "该论文提出了StableMotion，一种利用非配对的、包含噪声的运动捕捉(mocap)数据训练运动清理模型的方法。该方法引入了运动质量指标，通过人工标注或启发式算法进行标注，从而能够在混合质量的原始运动数据上训练质量感知的运动生成模型。在测试阶段，模型可以根据质量指标生成高质量的运动。StableMotion基于扩散模型框架实现，形成一个统一的运动生成-判别模型，用于识别和修复损坏的帧。实验结果表明，该方法在SoccerMocap数据集上有效，能够显著减少运动跳变和冻结帧，为实际生产环境中的原始mocap数据清理提供了有效方案。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages, 13 figures",
      "pdf_url": "http://arxiv.org/pdf/2505.03154v1",
      "published_date": "2025-05-06 04:02:47 UTC",
      "updated_date": "2025-05-06 04:02:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:31:13.775429"
    },
    {
      "arxiv_id": "2505.03149v1",
      "title": "Motion-compensated cardiac MRI using low-rank diffeomorphic flow (DMoCo)",
      "title_zh": "基于低秩微分同胚流的运动补偿心脏 MRI (DMoCo)\n",
      "authors": [
        "Joseph William Kettelkamp",
        "Ludovica Romanin",
        "Sarv Priya",
        "Mathews Jacob"
      ],
      "abstract": "We introduce an unsupervised motion-compensated image reconstruction\nalgorithm for free-breathing and ungated 3D cardiac magnetic resonance imaging\n(MRI). We express the image volume corresponding to each specific motion phase\nas the deformation of a single static image template. The main contribution of\nthe work is the low-rank model for the compact joint representation of the\nfamily of diffeomorphisms, parameterized by the motion phases. The\ndiffeomorphism at a specific motion phase is obtained by integrating a\nparametric velocity field along a path connecting the reference template phase\nto the motion phase. The velocity field at different phases is represented\nusing a low-rank model. The static template and the low-rank motion model\nparameters are learned directly from the k-space data in an unsupervised\nfashion. The more constrained motion model is observed to offer improved\nrecovery compared to current motion-resolved and motion-compensated algorithms\nfor free-breathing 3D cine MRI.",
      "tldr_zh": "本文提出了一种无监督的运动补偿图像重建算法DMoCo，用于自由呼吸和非门控3D心脏磁共振成像(MRI)。该算法将每个特定运动相位对应的图像体表示为单个静态图像模板的形变。核心贡献在于提出了一个低秩模型，用于紧凑地联合表示由运动相位参数化的一系列微分同胚。特定运动相位下的微分同胚通过积分连接参考模板相位到该运动相位的参数化速度场得到，而不同相位的速度场则使用低秩模型表示。静态模板和低秩运动模型参数直接从k空间数据中以无监督的方式学习得到。实验表明，相比于现有的运动解析和运动补偿算法，该方法在自由呼吸3D电影MRI中能够提供更好的图像恢复效果。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03149v1",
      "published_date": "2025-05-06 03:52:17 UTC",
      "updated_date": "2025-05-06 03:52:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:31:25.976400"
    },
    {
      "arxiv_id": "2505.03135v1",
      "title": "Holmes: Automated Fact Check with Large Language Models",
      "title_zh": "Holmes：基于大型语言模型的自动事实核查\n",
      "authors": [
        "Haoran Ou",
        "Gelei Deng",
        "Xingshuo Han",
        "Jie Zhang",
        "Xinlei He",
        "Han Qiu",
        "Shangwei Guo",
        "Tianwei Zhang"
      ],
      "abstract": "The rise of Internet connectivity has accelerated the spread of\ndisinformation, threatening societal trust, decision-making, and national\nsecurity. Disinformation has evolved from simple text to complex multimodal\nforms combining images and text, challenging existing detection methods.\nTraditional deep learning models struggle to capture the complexity of\nmultimodal disinformation. Inspired by advances in AI, this study explores\nusing Large Language Models (LLMs) for automated disinformation detection. The\nempirical study shows that (1) LLMs alone cannot reliably assess the\ntruthfulness of claims; (2) providing relevant evidence significantly improves\ntheir performance; (3) however, LLMs cannot autonomously search for accurate\nevidence. To address this, we propose Holmes, an end-to-end framework featuring\na novel evidence retrieval method that assists LLMs in collecting high-quality\nevidence. Our approach uses (1) LLM-powered summarization to extract key\ninformation from open sources and (2) a new algorithm and metrics to evaluate\nevidence quality. Holmes enables LLMs to verify claims and generate\njustifications effectively. Experiments show Holmes achieves 88.3% accuracy on\ntwo open-source datasets and 90.2% in real-time verification tasks. Notably,\nour improved evidence retrieval boosts fact-checking accuracy by 30.8% over\nexisting methods",
      "tldr_zh": "该研究针对互联网上日益严峻的虚假信息传播问题，提出了名为Holmes的端到端自动化事实核查框架。研究发现，大型语言模型(LLMs)单独进行事实核查的可靠性不足，但提供相关证据能显著提升其性能。Holmes框架包含一种新型证据检索方法，辅助LLMs收集高质量证据，包括利用LLM进行信息摘要提取和使用新算法评估证据质量。实验结果表明，Holmes在开放数据集和实时验证任务中分别取得了88.3%和90.2%的准确率，并且其改进的证据检索方法比现有方法提升了30.8%的事实核查准确率。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03135v1",
      "published_date": "2025-05-06 03:19:51 UTC",
      "updated_date": "2025-05-06 03:19:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:31:37.793077"
    },
    {
      "arxiv_id": "2505.03132v1",
      "title": "VISLIX: An XAI Framework for Validating Vision Models with Slice Discovery and Analysis",
      "title_zh": "VISLIX：一种用于通过切片发现和分析来验证视觉模型的 XAI 框架\n",
      "authors": [
        "Xinyuan Yan",
        "Xiwei Xuan",
        "Jorge Piazentin Ono",
        "Jiajing Guo",
        "Vikram Mohanty",
        "Shekar Arvind Kumar",
        "Liang Gou",
        "Bei Wang",
        "Liu Ren"
      ],
      "abstract": "Real-world machine learning models require rigorous evaluation before\ndeployment, especially in safety-critical domains like autonomous driving and\nsurveillance. The evaluation of machine learning models often focuses on data\nslices, which are subsets of the data that share a set of characteristics. Data\nslice finding automatically identifies conditions or data subgroups where\nmodels underperform, aiding developers in mitigating performance issues.\nDespite its popularity and effectiveness, data slicing for vision model\nvalidation faces several challenges. First, data slicing often needs additional\nimage metadata or visual concepts, and falls short in certain computer vision\ntasks, such as object detection. Second, understanding data slices is a\nlabor-intensive and mentally demanding process that heavily relies on the\nexpert's domain knowledge. Third, data slicing lacks a human-in-the-loop\nsolution that allows experts to form hypothesis and test them interactively. To\novercome these limitations and better support the machine learning operations\nlifecycle, we introduce VISLIX, a novel visual analytics framework that employs\nstate-of-the-art foundation models to help domain experts analyze slices in\ncomputer vision models. Our approach does not require image metadata or visual\nconcepts, automatically generates natural language insights, and allows users\nto test data slice hypothesis interactively. We evaluate VISLIX with an expert\nstudy and three use cases, that demonstrate the effectiveness of our tool in\nproviding comprehensive insights for validating object detection models.",
      "tldr_zh": "VISLIX是一个新颖的可解释性人工智能(XAI)框架，旨在通过切片发现和分析来验证视觉模型。它利用先进的基础模型，无需图像元数据或视觉概念，即可帮助领域专家分析计算机视觉模型中的数据切片。VISLIX能够自动生成自然语言洞察，并允许用户交互式地测试数据切片假设。通过专家研究和三个用例评估，证明了VISLIX在为验证目标检测模型提供全面洞察方面的有效性，从而支持机器学习操作生命周期。该框架旨在解决传统数据切片方法在目标检测等任务中的局限性，并减轻专家理解数据切片的认知负担。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03132v1",
      "published_date": "2025-05-06 03:09:15 UTC",
      "updated_date": "2025-05-06 03:09:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:31:49.751470"
    },
    {
      "arxiv_id": "2505.03108v1",
      "title": "Is AI currently capable of identifying wild oysters? A comparison of human annotators against the AI model, ODYSSEE",
      "title_zh": "目前人工智能是否能够识别野生牡蛎？人类标注者与人工智能模型 ODYSSEE 的比较\n",
      "authors": [
        "Brendan Campbell",
        "Alan Williams",
        "Kleio Baxevani",
        "Alyssa Campbell",
        "Rushabh Dhoke",
        "Rileigh E. Hudock",
        "Xiaomin Lin",
        "Vivek Mange",
        "Bernhard Neuberger",
        "Arjun Suresh",
        "Alhim Vera",
        "Arthur Trembanis",
        "Herbert G. Tanner",
        "Edward Hale"
      ],
      "abstract": "Oysters are ecologically and commercially important species that require\nfrequent monitoring to track population demographics (e.g. abundance, growth,\nmortality). Current methods of monitoring oyster reefs often require\ndestructive sampling methods and extensive manual effort. Therefore, they are\nsuboptimal for small-scale or sensitive environments. A recent alternative, the\nODYSSEE model, was developed to use deep learning techniques to identify live\noysters using video or images taken in the field of oyster reefs to assess\nabundance. The validity of this model in identifying live oysters on a reef was\ncompared to expert and non-expert annotators. In addition, we identified\npotential sources of prediction error. Although the model can make inferences\nsignificantly faster than expert and non-expert annotators (39.6 s, $2.34 \\pm\n0.61$ h, $4.50 \\pm 1.46$ h, respectively), the model overpredicted the number\nof live oysters, achieving lower accuracy (63\\%) in identifying live oysters\ncompared to experts (74\\%) and non-experts (75\\%) alike. Image quality was an\nimportant factor in determining the accuracy of the model and the annotators.\nBetter quality images improved human accuracy and worsened model accuracy.\nAlthough ODYSSEE was not sufficiently accurate, we anticipate that future\ntraining on higher-quality images, utilizing additional live imagery, and\nincorporating additional annotation training classes will greatly improve the\nmodel's predictive power based on the results of this analysis. Future research\nshould address methods that improve the detection of living vs. dead oysters.",
      "tldr_zh": "该研究对比了AI模型ODYSSEE与人工标注员在识别野生牡蛎方面的能力。ODYSSEE模型利用深度学习技术，通过分析牡蛎礁的视频或图像来评估牡蛎数量，旨在替代破坏性取样和耗时的人工监测方法。结果表明，ODYSSEE的识别速度远快于专家和非专家标注员，但其准确率（63%）低于专家（74%）和非专家（75%），存在过度预测活牡蛎数量的问题。图像质量是影响模型和人工标注员准确率的关键因素，高质量图像反而降低了模型的准确率。研究认为，通过使用更高质量的图像进行训练，并增加标注训练类别，可以显著提高ODYSSEE模型的预测能力，未来的研究应关注如何更好地区分活牡蛎和死牡蛎。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03108v1",
      "published_date": "2025-05-06 02:01:27 UTC",
      "updated_date": "2025-05-06 02:01:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:32:01.868148"
    },
    {
      "arxiv_id": "2505.03105v1",
      "title": "Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI Knowledge Co-Creation",
      "title_zh": "涌现认知：人机协同知识共创中的能动性、维度和动态性\n",
      "authors": [
        "Xule Lin"
      ],
      "abstract": "Scientific knowledge creation is fundamentally transforming as humans and AI\nsystems evolve beyond tool-user relationships into co-evolutionary epistemic\npartnerships. When AlphaFold revolutionized protein structure prediction,\nresearchers described engaging with an epistemic partner that reshaped how they\nconceptualized fundamental relationships. This article introduces Cognitio\nEmergens (CE), a framework addressing critical limitations in existing models\nthat focus on static roles or narrow metrics while failing to capture how\nscientific understanding emerges through recursive human-AI interaction over\ntime. CE integrates three components addressing these limitations: Agency\nConfigurations describing how authority distributes between humans and AI\n(Directed, Contributory, Partnership), with partnerships dynamically\noscillating between configurations rather than following linear progression;\nEpistemic Dimensions capturing six specific capabilities emerging through\ncollaboration across Discovery, Integration, and Projection axes, creating\ndistinctive \"capability signatures\" that guide development; and Partnership\nDynamics identifying forces shaping how these relationships evolve,\nparticularly the risk of epistemic alienation where researchers lose\ninterpretive control over knowledge they formally endorse. Drawing from\nautopoiesis theory, social systems theory, and organizational modularity, CE\nreveals how knowledge co-creation emerges through continuous negotiation of\nroles, values, and organizational structures. By reconceptualizing human-AI\nscientific collaboration as fundamentally co-evolutionary, CE offers a balanced\nperspective that neither uncritically celebrates nor unnecessarily fears AI's\nevolving role, instead providing conceptual tools for cultivating partnerships\nthat maintain meaningful human participation while enabling transformative\nscientific breakthroughs.",
      "tldr_zh": "本文提出了“涌现认知(Cognitio Emergens, CE)”框架，旨在解决现有模型在描述人与AI知识共创过程中的局限性。CE框架包含三个核心组件：智能体配置（Agency Configurations），描述了人与AI之间的权力分配模式（指导型、贡献型、伙伴型）；认知维度（Epistemic Dimensions），捕捉了合作中涌现的六种关键能力，涵盖发现、整合和预测三个轴；以及伙伴关系动力学（Partnership Dynamics），识别了影响关系演变的力量，特别是认知异化的风险。CE框架强调人与AI的科学合作是共同进化的，通过角色、价值观和组织结构的持续协商，知识共创得以涌现，从而为促进有意义的人工参与和实现变革性科学突破提供概念工具。\n",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "H.5.3; I.2.11; K.4.3; H.1.2; I.2.4"
      ],
      "primary_category": "cs.HC",
      "comment": "62 pages (31 appendix pages for guidance), 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2505.03105v1",
      "published_date": "2025-05-06 01:49:44 UTC",
      "updated_date": "2025-05-06 01:49:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:32:14.017080"
    },
    {
      "arxiv_id": "2505.03096v1",
      "title": "Assessing and Enhancing the Robustness of LLM-based Multi-Agent Systems Through Chaos Engineering",
      "title_zh": "通过混沌工程评估和增强基于 LLM 的多智能体系统的稳健性\n",
      "authors": [
        "Joshua Owotogbe"
      ],
      "abstract": "This study explores the application of chaos engineering to enhance the\nrobustness of Large Language Model-Based Multi-Agent Systems (LLM-MAS) in\nproduction-like environments under real-world conditions. LLM-MAS can\npotentially improve a wide range of tasks, from answering questions and\ngenerating content to automating customer support and improving decision-making\nprocesses. However, LLM-MAS in production or preproduction environments can be\nvulnerable to emergent errors or disruptions, such as hallucinations, agent\nfailures, and agent communication failures. This study proposes a chaos\nengineering framework to proactively identify such vulnerabilities in LLM-MAS,\nassess and build resilience against them, and ensure reliable performance in\ncritical applications.",
      "tldr_zh": "该研究探索了混沌工程在增强基于大型语言模型的多智能体系统(LLM-MAS)鲁棒性方面的应用。LLM-MAS在实际应用中容易出现幻觉、智能体故障和通信失败等问题。为此，研究提出了一种混沌工程框架，旨在主动识别LLM-MAS中的潜在漏洞，评估并建立应对这些漏洞的弹性，从而确保其在关键应用中的可靠性能。该框架通过模拟真实环境中的故障，帮助开发者发现并修复LLM-MAS的薄弱环节，提升系统的整体可靠性。\n",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03096v1",
      "published_date": "2025-05-06 01:13:14 UTC",
      "updated_date": "2025-05-06 01:13:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:32:25.685960"
    },
    {
      "arxiv_id": "2505.03077v1",
      "title": "Latent Adaptive Planner for Dynamic Manipulation",
      "title_zh": "动态操控的潜在自适应规划器\n",
      "authors": [
        "Donghun Noh",
        "Deqian Kong",
        "Minglu Zhao",
        "Andrew Lizarraga",
        "Jianwen Xie",
        "Ying Nian Wu",
        "Dennis Hong"
      ],
      "abstract": "This paper presents Latent Adaptive Planner (LAP), a novel approach for\ndynamic nonprehensile manipulation tasks that formulates planning as latent\nspace inference, effectively learned from human demonstration videos. Our\nmethod addresses key challenges in visuomotor policy learning through a\nprincipled variational replanning framework that maintains temporal consistency\nwhile efficiently adapting to environmental changes. LAP employs Bayesian\nupdating in latent space to incrementally refine plans as new observations\nbecome available, striking an optimal balance between computational efficiency\nand real-time adaptability. We bridge the embodiment gap between humans and\nrobots through model-based proportional mapping that regenerates accurate\nkinematic-dynamic joint states and object positions from human demonstrations.\nExperimental evaluations across multiple complex manipulation benchmarks\ndemonstrate that LAP achieves state-of-the-art performance, outperforming\nexisting approaches in success rate, trajectory smoothness, and energy\nefficiency, particularly in dynamic adaptation scenarios. Our approach enables\nrobots to perform complex interactions with human-like adaptability while\nproviding an expandable framework applicable to diverse robotic platforms using\nthe same human demonstration videos.",
      "tldr_zh": "本文提出了一种用于动态非抓取操作任务的潜在自适应规划器(Latent Adaptive Planner, LAP)，该方法将规划建模为潜在空间推理，并从人类演示视频中有效学习。LAP通过变分重规划框架解决视觉运动策略学习中的关键挑战，保持时间一致性并高效适应环境变化。该方法在潜在空间中采用贝叶斯更新来逐步优化计划，从而在计算效率和实时适应性之间取得平衡。通过基于模型的比例映射，LAP弥合了人类和机器人之间的具身差距，从人类演示中重新生成精确的运动学-动力学关节状态和物体位置。实验结果表明，LAP在多个复杂操作基准测试中实现了最先进的性能，尤其是在动态适应场景中，在成功率、轨迹平滑度和能源效率方面优于现有方法。该方法使机器人能够执行复杂的人机交互，并提供了一个可扩展的框架，可应用于使用相同人类演示视频的各种机器人平台。\n",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03077v1",
      "published_date": "2025-05-06 00:09:09 UTC",
      "updated_date": "2025-05-06 00:09:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-08T02:32:38.277980"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 81,
  "processed_papers_count": 81,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-08T02:34:26.488295"
}
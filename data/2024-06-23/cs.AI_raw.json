[
  {
    "arxiv_id": "2406.16985v2",
    "title": "Unveiling LLM Mechanisms Through Neural ODEs and Control Theory",
    "authors": [
      "Yukun Zhang",
      "Qi Dong"
    ],
    "abstract": "This paper proposes a framework combining Neural Ordinary Differential\nEquations (Neural ODEs) and robust control theory to enhance the\ninterpretability and control of large language models (LLMs). By utilizing\nNeural ODEs to model the dynamic evolution of input-output relationships and\nintroducing control mechanisms to optimize output quality, we demonstrate the\neffectiveness of this approach across multiple question-answer datasets.\nExperimental results show that the integration of Neural ODEs and control\ntheory significantly improves output consistency and model interpretability,\nadvancing the development of explainable AI technologies.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16985v2",
    "published_date": "2024-06-23 22:56:34 UTC",
    "updated_date": "2025-02-23 06:03:42 UTC"
  },
  {
    "arxiv_id": "2406.16235v2",
    "title": "Preference Tuning For Toxicity Mitigation Generalizes Across Languages",
    "authors": [
      "Xiaochen Li",
      "Zheng-Xin Yong",
      "Stephen H. Bach"
    ],
    "abstract": "Detoxifying multilingual Large Language Models (LLMs) has become crucial due\nto their increasing global use. In this work, we explore zero-shot\ncross-lingual generalization of preference tuning in detoxifying LLMs. Unlike\nprevious studies that show limited cross-lingual generalization for other\nsafety tasks, we demonstrate that Direct Preference Optimization (DPO) training\nwith only English data can significantly reduce toxicity in multilingual\nopen-ended generations. For example, the probability of mGPT-1.3B generating\ntoxic continuations drops from 46.8% to 3.9% across 17 different languages\nafter training. Our results also extend to other multilingual LLMs, such as\nBLOOM, Llama3, and Aya-23. Using mechanistic interpretability tools like causal\nintervention and activation analysis, we identified the dual multilinguality\nproperty of MLP layers in LLMs, which explains the cross-lingual generalization\nof DPO. Finally, we show that bilingual sentence retrieval can predict the\ncross-lingual transferability of DPO preference tuning.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Findings of EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.16235v2",
    "published_date": "2024-06-23 22:53:47 UTC",
    "updated_date": "2024-11-08 02:17:22 UTC"
  },
  {
    "arxiv_id": "2406.16232v3",
    "title": "Jacobian Descent for Multi-Objective Optimization",
    "authors": [
      "Pierre Quinton",
      "Valérian Rey"
    ],
    "abstract": "Many optimization problems require balancing multiple conflicting objectives.\nAs gradient descent is limited to single-objective optimization, we introduce\nits direct generalization: Jacobian descent (JD). This algorithm iteratively\nupdates parameters using the Jacobian matrix of a vector-valued objective\nfunction, in which each row is the gradient of an individual objective. While\nseveral methods to combine gradients already exist in the literature, they are\ngenerally hindered when the objectives conflict. In contrast, we propose\nprojecting gradients to fully resolve conflict while ensuring that they\npreserve an influence proportional to their norm. We prove significantly\nstronger convergence guarantees with this approach, supported by our empirical\nresults. Our method also enables instance-wise risk minimization (IWRM), a\nnovel learning paradigm in which the loss of each training example is\nconsidered a separate objective. Applied to simple image classification tasks,\nIWRM exhibits promising results compared to the direct minimization of the\naverage loss. Additionally, we outline an efficient implementation of JD using\nthe Gramian of the Jacobian matrix to reduce time and memory requirements.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "39 pages, 10 figures, conference",
    "pdf_url": "http://arxiv.org/pdf/2406.16232v3",
    "published_date": "2024-06-23 22:06:25 UTC",
    "updated_date": "2025-02-03 12:29:44 UTC"
  },
  {
    "arxiv_id": "2406.16231v1",
    "title": "Gradual Divergence for Seamless Adaptation: A Novel Domain Incremental Learning Method",
    "authors": [
      "Kishaan Jeeveswaran",
      "Elahe Arani",
      "Bahram Zonooz"
    ],
    "abstract": "Domain incremental learning (DIL) poses a significant challenge in real-world\nscenarios, as models need to be sequentially trained on diverse domains over\ntime, all the while avoiding catastrophic forgetting. Mitigating representation\ndrift, which refers to the phenomenon of learned representations undergoing\nchanges as the model adapts to new tasks, can help alleviate catastrophic\nforgetting. In this study, we propose a novel DIL method named DARE, featuring\na three-stage training process: Divergence, Adaptation, and REfinement. This\nprocess gradually adapts the representations associated with new tasks into the\nfeature space spanned by samples from previous tasks, simultaneously\nintegrating task-specific decision boundaries. Additionally, we introduce a\nnovel strategy for buffer sampling and demonstrate the effectiveness of our\nproposed method, combined with this sampling strategy, in reducing\nrepresentation drift within the feature encoder. This contribution effectively\nalleviates catastrophic forgetting across multiple DIL benchmarks. Furthermore,\nour approach prevents sudden representation drift at task boundaries, resulting\nin a well-calibrated DIL model that maintains the performance on previous\ntasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at 41st International Conference on Machine Learning (ICML\n  2024)",
    "pdf_url": "http://arxiv.org/pdf/2406.16231v1",
    "published_date": "2024-06-23 22:05:52 UTC",
    "updated_date": "2024-06-23 22:05:52 UTC"
  },
  {
    "arxiv_id": "2406.16224v2",
    "title": "From Text to Test: AI-Generated Control Software for Materials Science Instruments",
    "authors": [
      "Davi M Fébba",
      "Kingsley Egbo",
      "William A. Callahan",
      "Andriy Zakutayev"
    ],
    "abstract": "Large language models (LLMs) are transforming the landscape of chemistry and\nmaterials science. Recent examples of LLM-accelerated experimental research\ninclude virtual assistants for parsing synthesis recipes from the literature,\nor using the extracted knowledge to guide synthesis and characterization.\nDespite these advancements, their application is constrained to labs with\nautomated instruments and control software, leaving much of materials science\nreliant on manual processes. Here, we demonstrate the rapid deployment of a\nPython-based control module for a Keithley 2400 electrical source measure unit\nusing ChatGPT-4. Through iterative refinement, we achieved effective instrument\nmanagement with minimal human intervention. Additionally, a user-friendly\ngraphical user interface (GUI) was created, effectively linking all instrument\ncontrols to interactive screen elements. Finally, we integrated this AI-crafted\ninstrument control software with a high-performance stochastic optimization\nalgorithm to facilitate rapid and automated extraction of electronic device\nparameters related to semiconductor charge transport mechanisms from\ncurrent-voltage (IV) measurement data. This integration resulted in a\ncomprehensive open-source toolkit for semiconductor device characterization and\nanalysis using IV curve measurements. We demonstrate the application of these\ntools by acquiring, analyzing, and parameterizing IV data from a\nPt/Cr$_2$O$_3$:Mg/$\\beta$-Ga$_2$O$_3$ heterojunction diode, a novel stack for\nhigh-power and high-temperature electronic devices. This approach underscores\nthe powerful synergy between LLMs and the development of instruments for\nscientific inquiry, showcasing a path for further acceleration in materials\nscience.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16224v2",
    "published_date": "2024-06-23 21:32:57 UTC",
    "updated_date": "2024-06-25 11:34:15 UTC"
  },
  {
    "arxiv_id": "2406.16223v1",
    "title": "Continuous Output Personality Detection Models via Mixed Strategy Training",
    "authors": [
      "Rong Wang",
      "Kun Sun"
    ],
    "abstract": "The traditional personality models only yield binary results. This paper\npresents a novel approach for training personality detection models that\nproduce continuous output values, using mixed strategies. By leveraging the\nPANDORA dataset, which includes extensive personality labeling of Reddit\ncomments, we developed models that predict the Big Five personality traits with\nhigh accuracy. Our approach involves fine-tuning a RoBERTa-base model with\nvarious strategies such as Multi-Layer Perceptron (MLP) integration, and\nhyperparameter tuning. The results demonstrate that our models significantly\noutperform traditional binary classification methods, offering precise\ncontinuous outputs for personality traits, thus enhancing applications in AI,\npsychology, human resources, marketing and health care fields.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16223v1",
    "published_date": "2024-06-23 21:32:15 UTC",
    "updated_date": "2024-06-23 21:32:15 UTC"
  },
  {
    "arxiv_id": "2406.16221v1",
    "title": "F-FOMAML: GNN-Enhanced Meta-Learning for Peak Period Demand Forecasting with Proxy Data",
    "authors": [
      "Zexing Xu",
      "Linjun Zhang",
      "Sitan Yang",
      "Rasoul Etesami",
      "Hanghang Tong",
      "Huan Zhang",
      "Jiawei Han"
    ],
    "abstract": "Demand prediction is a crucial task for e-commerce and physical retail\nbusinesses, especially during high-stake sales events. However, the limited\navailability of historical data from these peak periods poses a significant\nchallenge for traditional forecasting methods. In this paper, we propose a\nnovel approach that leverages strategically chosen proxy data reflective of\npotential sales patterns from similar entities during non-peak periods,\nenriched by features learned from a graph neural networks (GNNs)-based\nforecasting model, to predict demand during peak events. We formulate the\ndemand prediction as a meta-learning problem and develop the Feature-based\nFirst-Order Model-Agnostic Meta-Learning (F-FOMAML) algorithm that leverages\nproxy data from non-peak periods and GNN-generated relational metadata to learn\nfeature-specific layer parameters, thereby adapting to demand forecasts for\npeak events. Theoretically, we show that by considering domain similarities\nthrough task-specific metadata, our model achieves improved generalization,\nwhere the excess risk decreases as the number of training tasks increases.\nEmpirical evaluations on large-scale industrial datasets demonstrate the\nsuperiority of our approach. Compared to existing state-of-the-art models, our\nmethod demonstrates a notable improvement in demand prediction accuracy,\nreducing the Mean Absolute Error by 26.24% on an internal vending machine\ndataset and by 1.04% on the publicly accessible JD.com dataset.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GR",
      "econ.EM",
      "stat.ME",
      "68T07, 68T05, 62M10, 62M20, 90C90, 91B84"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16221v1",
    "published_date": "2024-06-23 21:28:50 UTC",
    "updated_date": "2024-06-23 21:28:50 UTC"
  },
  {
    "arxiv_id": "2406.16220v1",
    "title": "Learning Run-time Safety Monitors for Machine Learning Components",
    "authors": [
      "Ozan Vardal",
      "Richard Hawkins",
      "Colin Paterson",
      "Chiara Picardi",
      "Daniel Omeiza",
      "Lars Kunze",
      "Ibrahim Habli"
    ],
    "abstract": "For machine learning components used as part of autonomous systems (AS) in\ncarrying out critical tasks it is crucial that assurance of the models can be\nmaintained in the face of post-deployment changes (such as changes in the\noperating environment of the system). A critical part of this is to be able to\nmonitor when the performance of the model at runtime (as a result of changes)\nposes a safety risk to the system. This is a particularly difficult challenge\nwhen ground truth is unavailable at runtime. In this paper we introduce a\nprocess for creating safety monitors for ML components through the use of\ndegraded datasets and machine learning. The safety monitor that is created is\ndeployed to the AS in parallel to the ML component to provide a prediction of\nthe safety risk associated with the model output. We demonstrate the viability\nof our approach through some initial experiments using publicly available speed\nsign datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16220v1",
    "published_date": "2024-06-23 21:25:06 UTC",
    "updated_date": "2024-06-23 21:25:06 UTC"
  },
  {
    "arxiv_id": "2406.16218v2",
    "title": "Trace is the Next AutoDiff: Generative Optimization with Rich Feedback, Execution Traces, and LLMs",
    "authors": [
      "Ching-An Cheng",
      "Allen Nie",
      "Adith Swaminathan"
    ],
    "abstract": "We study a class of optimization problems motivated by automating the design\nand update of AI systems like coding assistants, robots, and copilots. AutoDiff\nframeworks, like PyTorch, enable efficient end-to-end optimization of\ndifferentiable systems. However, general computational workflows can be\nnon-differentiable and involve rich feedback (e.g. console output or user's\nresponses), heterogeneous parameters (e.g. prompts, codes), and intricate\nobjectives (beyond maximizing a score). We investigate end-to-end generative\noptimization -- using generative models such as LLMs within the optimizer for\nautomatic updating of general computational workflows. We discover that\nworkflow execution traces are akin to back-propagated gradients in AutoDiff and\ncan provide key information to interpret feedback for efficient optimization.\nFormally, we frame a new mathematical setup, Optimization with Trace Oracle\n(OPTO). In OPTO, an optimizer receives an execution trace along with feedback\non the computed output and updates parameters iteratively. We provide a Python\nlibrary, Trace, that efficiently converts a workflow optimization problem into\nan OPTO instance using PyTorch-like syntax. Using Trace, we develop a general\nLLM-based generative optimizer called OptoPrime. In empirical studies, we find\nthat OptoPrime is capable of first-order numerical optimization, prompt\noptimization, hyper-parameter tuning, robot controller design, code debugging,\netc., and is often competitive with specialized optimizers for each domain. We\nenvision Trace as an open research platform for devising novel generative\noptimizers and developing the next generation of interactive learning agents.\nWebsite: https://microsoft.github.io/Trace/.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16218v2",
    "published_date": "2024-06-23 21:05:31 UTC",
    "updated_date": "2024-11-01 00:01:01 UTC"
  },
  {
    "arxiv_id": "2406.16983v1",
    "title": "On Instabilities of Unsupervised Denoising Diffusion Models in Magnetic Resonance Imaging Reconstruction",
    "authors": [
      "Tianyu Han",
      "Sven Nebelung",
      "Firas Khader",
      "Jakob Nikolas Kather",
      "Daniel Truhn"
    ],
    "abstract": "Denoising diffusion models offer a promising approach to accelerating\nmagnetic resonance imaging (MRI) and producing diagnostic-level images in an\nunsupervised manner. However, our study demonstrates that even tiny worst-case\npotential perturbations transferred from a surrogate model can cause these\nmodels to generate fake tissue structures that may mislead clinicians. The\ntransferability of such worst-case perturbations indicates that the robustness\nof image reconstruction may be compromised due to MR system imperfections or\nother sources of noise. Moreover, at larger perturbation strengths, diffusion\nmodels exhibit Gaussian noise-like artifacts that are distinct from those\nobserved in supervised models and are more challenging to detect. Our results\nhighlight the vulnerability of current state-of-the-art diffusion-based\nreconstruction models to possible worst-case perturbations and underscore the\nneed for further research to improve their robustness and reliability in\nclinical settings.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16983v1",
    "published_date": "2024-06-23 19:44:00 UTC",
    "updated_date": "2024-06-23 19:44:00 UTC"
  },
  {
    "arxiv_id": "2406.16191v1",
    "title": "Accelerating Matrix Diagonalization through Decision Transformers with Epsilon-Greedy Optimization",
    "authors": [
      "Kshitij Bhatta",
      "Geigh Zollicoffer",
      "Manish Bhattarai",
      "Phil Romero",
      "Christian F. A. Negre",
      "Anders M. N. Niklasson",
      "Adetokunbo Adedoyin"
    ],
    "abstract": "This paper introduces a novel framework for matrix diagonalization, recasting\nit as a sequential decision-making problem and applying the power of Decision\nTransformers (DTs). Our approach determines optimal pivot selection during\ndiagonalization with the Jacobi algorithm, leading to significant speedups\ncompared to the traditional max-element Jacobi method. To bolster robustness,\nwe integrate an epsilon-greedy strategy, enabling success in scenarios where\ndeterministic approaches fail. This work demonstrates the effectiveness of DTs\nin complex computational tasks and highlights the potential of reimagining\nmathematical operations through a machine learning lens. Furthermore, we\nestablish the generalizability of our method by using transfer learning to\ndiagonalize matrices of smaller sizes than those trained.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.16191v1",
    "published_date": "2024-06-23 18:56:46 UTC",
    "updated_date": "2024-06-23 18:56:46 UTC"
  },
  {
    "arxiv_id": "2406.16982v1",
    "title": "Research on Disease Prediction Model Construction Based on Computer AI deep Learning Technology",
    "authors": [
      "Yang Lin",
      "Muqing Li",
      "Ziyi Zhu",
      "Yinqiu Feng",
      "Lingxi Xiao",
      "Zexi Chen"
    ],
    "abstract": "The prediction of disease risk factors can screen vulnerable groups for\neffective prevention and treatment, so as to reduce their morbidity and\nmortality. Machine learning has a great demand for high-quality labeling\ninformation, and labeling noise in medical big data poses a great challenge to\nefficient disease risk warning methods. Therefore, this project intends to\nstudy the robust learning algorithm and apply it to the early warning of\ninfectious disease risk. A dynamic truncated loss model is proposed, which\ncombines the traditional mutual entropy implicit weight feature with the mean\nvariation feature. It is robust to label noise. A lower bound on training loss\nis constructed, and a method based on sampling rate is proposed to reduce the\ngradient of suspected samples to reduce the influence of noise on training\nresults. The effectiveness of this method under different types of noise was\nverified by using a stroke screening data set as an example. This method\nenables robust learning of data containing label noise.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16982v1",
    "published_date": "2024-06-23 18:44:03 UTC",
    "updated_date": "2024-06-23 18:44:03 UTC"
  },
  {
    "arxiv_id": "2406.16981v1",
    "title": "Research on Feature Extraction Data Processing System For MRI of Brain Diseases Based on Computer Deep Learning",
    "authors": [
      "Lingxi Xiao",
      "Jinxin Hu",
      "Yutian Yang",
      "Yinqiu Feng",
      "Zichao Li",
      "Zexi Chen"
    ],
    "abstract": "Most of the existing wavelet image processing techniques are carried out in\nthe form of single-scale reconstruction and multiple iterations. However,\nprocessing high-quality fMRI data presents problems such as mixed noise and\nexcessive computation time. This project proposes the use of matrix operations\nby combining mixed noise elimination methods with wavelet analysis to replace\ntraditional iterative algorithms. Functional magnetic resonance imaging (fMRI)\nof the auditory cortex of a single subject is analyzed and compared to the\nwavelet domain signal processing technology based on repeated times and the\nworld's most influential SPM8. Experiments show that this algorithm is the\nfastest in computing time, and its detection effect is comparable to the\ntraditional iterative algorithm. However, this has a higher practical value for\nthe processing of FMRI data. In addition, the wavelet analysis method proposed\nsignal processing to speed up the calculation rate.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16981v1",
    "published_date": "2024-06-23 18:41:43 UTC",
    "updated_date": "2024-06-23 18:41:43 UTC"
  },
  {
    "arxiv_id": "2406.16979v1",
    "title": "Understanding and Diagnosing Deep Reinforcement Learning",
    "authors": [
      "Ezgi Korkmaz"
    ],
    "abstract": "Deep neural policies have recently been installed in a diverse range of\nsettings, from biotechnology to automated financial systems. However, the\nutilization of deep neural networks to approximate the value function leads to\nconcerns on the decision boundary stability, in particular, with regard to the\nsensitivity of policy decision making to indiscernible, non-robust features due\nto highly non-convex and complex deep neural manifolds. These concerns\nconstitute an obstruction to understanding the reasoning made by deep neural\npolicies, and their foundational limitations. Hence, it is crucial to develop\ntechniques that aim to understand the sensitivities in the learnt\nrepresentations of neural network policies. To achieve this we introduce a\ntheoretically founded method that provides a systematic analysis of the\nunstable directions in the deep neural policy decision boundary across both\ntime and space. Through experiments in the Arcade Learning Environment (ALE),\nwe demonstrate the effectiveness of our technique for identifying correlated\ndirections of instability, and for measuring how sample shifts remold the set\nof sensitive directions in the neural policy landscape. Most importantly, we\ndemonstrate that state-of-the-art robust training techniques yield learning of\ndisjoint unstable directions, with dramatically larger oscillations over time,\nwhen compared to standard training. We believe our results reveal the\nfundamental properties of the decision process made by reinforcement learning\npolicies, and can help in constructing reliable and robust deep neural\npolicies.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.16979v1",
    "published_date": "2024-06-23 18:10:16 UTC",
    "updated_date": "2024-06-23 18:10:16 UTC"
  },
  {
    "arxiv_id": "2406.16176v2",
    "title": "GraphEval36K: Benchmarking Coding and Reasoning Capabilities of Large Language Models on Graph Datasets",
    "authors": [
      "Qiming Wu",
      "Zichen Chen",
      "Will Corcoran",
      "Misha Sra",
      "Ambuj K. Singh"
    ],
    "abstract": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage processing (NLP), demonstrating significant capabilities in processing\nand understanding text data. However, recent studies have identified\nlimitations in LLMs' ability to manipulate, program, and reason about\nstructured data, especially graphs. We introduce GraphEval36K, the first\ncomprehensive graph dataset, comprising 40 graph coding problems and 36,900\ntest cases to evaluate the ability of LLMs on graph problem-solving. Our\ndataset is categorized into eight primary and four sub-categories to ensure a\nthorough evaluation across different types of graphs. We benchmark ten LLMs,\nfinding that private models outperform open-source ones, though the gap is\nnarrowing. We also analyze the performance of LLMs across directed vs\nundirected graphs, different kinds of graph concepts, and network models.\nFurthermore, to improve the usability of our evaluation framework, we propose\nStructured Symbolic Decomposition (SSD), an instruction-based method designed\nto enhance LLM performance on complex graph tasks. Results show that SSD\nimproves the average passing rate of GPT-4, GPT-4o, Gemini-Pro and\nClaude-3-Sonnet by 8.38%, 6.78%, 29.28% and 25.28%, respectively.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "H.2.8, I.2.6, I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "The first two authors contributed equally to this work. This paper\n  has been accepted by NAACL 2025. GraphEval36K is available at\n  https://grapheval36k.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2406.16176v2",
    "published_date": "2024-06-23 18:01:56 UTC",
    "updated_date": "2025-02-17 09:53:43 UTC"
  },
  {
    "arxiv_id": "2407.10989v2",
    "title": "Can Large Language Models Detect Verbal Indicators of Romantic Attraction?",
    "authors": [
      "Sandra C. Matz",
      "Heinrich Peters",
      "Moran Cerf",
      "Eric Grunenberg",
      "Paul W. Eastwick",
      "Mitja D. Back",
      "Eli J. Finkel"
    ],
    "abstract": "As artificial intelligence (AI) models become an integral part of everyday\nlife, our interactions with them shift from purely functional exchanges to more\nrelational experiences. For these experiences to be successful, artificial\nagents need to be able to detect and interpret social cues and interpersonal\ndynamics; both within and outside of their own human-agent relationships. In\nthis paper, we explore whether AI models can accurately decode one of the\narguably most important but complex social signals: romantic attraction.\nSpecifically, we test whether Large Language Models can detect romantic\nattraction during brief getting-to-know-you interactions between humans.\nExamining data from 964 speed dates, we show that ChatGPT can predict both\nobjective and subjective indicators of speed dating success (r=0.12-0.23).\nAlthough predictive performance remains relatively low, ChatGPT's predictions\nof actual matching (i.e., the exchange of contact information) were not only on\npar with those of human judges but incremental to speed daters' own\npredictions. In addition, ChatGPT's judgments showed substantial overlap with\nthose made by human observers (r=0.21-0.35), highlighting similarities in their\nrepresentation of romantic attraction that are independent of accuracy. Our\nfindings also offer insights into how ChatGPT arrives at its predictions and\nthe mistakes it makes. Specifically, we use a Brunswik lens approach to\nidentify the linguistic and conversational cues utilized by ChatGPT (and human\njudges) vis-a-vis those that are predictive of actual matching.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10989v2",
    "published_date": "2024-06-23 17:50:30 UTC",
    "updated_date": "2025-04-12 20:24:02 UTC"
  },
  {
    "arxiv_id": "2407.16903v1",
    "title": "US-China perspectives on extreme AI risks and global governance",
    "authors": [
      "Akash Wasil",
      "Tim Durgin"
    ],
    "abstract": "The United States and China will play an important role in navigating safety\nand security challenges relating to advanced artificial intelligence. We sought\nto better understand how experts in each country describe safety and security\nthreats from advanced artificial intelligence, extreme risks from AI, and the\npotential for international cooperation. Specifically, we compiled\npublicly-available statements from major technical and policy leaders in both\nthe United States and China. We focused our analysis on advanced forms of\nartificial intelligence, such as artificial general intelligence (AGI), that\nmay have the most significant impacts on national and global security. Experts\nin both countries expressed concern about risks from AGI, risks from\nintelligence explosions, and risks from AI systems that escape human control.\nBoth countries have also launched early efforts designed to promote\ninternational cooperation around safety standards and risk management\npractices. Notably, our findings only reflect information from publicly\navailable sources. Nonetheless, our findings can inform policymakers and\nresearchers about the state of AI discourse in the US and China. We hope such\nwork can contribute to policy discussions around advanced AI, its global\nsecurity threats, and potential international dialogues or agreements to\nmitigate such threats.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16903v1",
    "published_date": "2024-06-23 17:31:27 UTC",
    "updated_date": "2024-06-23 17:31:27 UTC"
  },
  {
    "arxiv_id": "2406.16170v1",
    "title": "SimCE: Simplifying Cross-Entropy Loss for Collaborative Filtering",
    "authors": [
      "Xiaodong Yang",
      "Huiyuan Chen",
      "Yuchen Yan",
      "Yuxin Tang",
      "Yuying Zhao",
      "Eric Xu",
      "Yiwei Cai",
      "Hanghang Tong"
    ],
    "abstract": "The learning objective is integral to collaborative filtering systems, where\nthe Bayesian Personalized Ranking (BPR) loss is widely used for learning\ninformative backbones. However, BPR often experiences slow convergence and\nsuboptimal local optima, partially because it only considers one negative item\nfor each positive item, neglecting the potential impacts of other unobserved\nitems. To address this issue, the recently proposed Sampled Softmax\nCross-Entropy (SSM) compares one positive sample with multiple negative\nsamples, leading to better performance. Our comprehensive experiments confirm\nthat recommender systems consistently benefit from multiple negative samples\nduring training. Furthermore, we introduce a \\underline{Sim}plified Sampled\nSoftmax \\underline{C}ross-\\underline{E}ntropy Loss (SimCE), which simplifies\nthe SSM using its upper bound. Our validation on 12 benchmark datasets, using\nboth MF and LightGCN backbones, shows that SimCE significantly outperforms both\nBPR and SSM.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16170v1",
    "published_date": "2024-06-23 17:24:07 UTC",
    "updated_date": "2024-06-23 17:24:07 UTC"
  },
  {
    "arxiv_id": "2406.16151v1",
    "title": "Monte Carlo Planning for Stochastic Control on Constrained Markov Decision Processes",
    "authors": [
      "Larkin Liu",
      "Shiqi Liu",
      "Matej Jusup"
    ],
    "abstract": "In the world of stochastic control, especially in economics and engineering,\nMarkov Decision Processes (MDPs) can effectively model various stochastic\ndecision processes, from asset management to transportation optimization. These\nunderlying MDPs, upon closer examination, often reveal a specifically\nconstrained causal structure concerning the transition and reward dynamics. By\nexploiting this structure, we can obtain a reduction in the causal\nrepresentation of the problem setting, allowing us to solve of the optimal\nvalue function more efficiently. This work defines an MDP framework, the\n\\texttt{SD-MDP}, where we disentangle the causal structure of MDPs' transition\nand reward dynamics, providing distinct partitions on the temporal causal\ngraph. With this stochastic reduction, the \\texttt{SD-MDP} reflects a general\nclass of resource allocation problems. This disentanglement further enables us\nto derive theoretical guarantees on the estimation error of the value function\nunder an optimal policy by allowing independent value estimation from Monte\nCarlo sampling. Subsequently, by integrating this estimator into well-known\nMonte Carlo planning algorithms, such as Monte Carlo Tree Search (MCTS), we\nderive bounds on the simple regret of the algorithm. Finally, we quantify the\npolicy improvement of MCTS under the \\texttt{SD-MDP} framework by demonstrating\nthat the MCTS planning algorithm achieves higher expected reward (lower costs)\nunder a constant simulation budget, on a tangible economic example based on\nmaritime refuelling.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY",
      "C.4"
    ],
    "primary_category": "cs.AI",
    "comment": "Working manuscript",
    "pdf_url": "http://arxiv.org/pdf/2406.16151v1",
    "published_date": "2024-06-23 16:22:40 UTC",
    "updated_date": "2024-06-23 16:22:40 UTC"
  },
  {
    "arxiv_id": "2406.16148v3",
    "title": "Towards Open Respiratory Acoustic Foundation Models: Pretraining and Benchmarking",
    "authors": [
      "Yuwei Zhang",
      "Tong Xia",
      "Jing Han",
      "Yu Wu",
      "Georgios Rizos",
      "Yang Liu",
      "Mohammed Mosuily",
      "Jagmohan Chauhan",
      "Cecilia Mascolo"
    ],
    "abstract": "Respiratory audio, such as coughing and breathing sounds, has predictive\npower for a wide range of healthcare applications, yet is currently\nunder-explored. The main problem for those applications arises from the\ndifficulty in collecting large labeled task-specific data for model\ndevelopment. Generalizable respiratory acoustic foundation models pretrained\nwith unlabeled data would offer appealing advantages and possibly unlock this\nimpasse. However, given the safety-critical nature of healthcare applications,\nit is pivotal to also ensure openness and replicability for any proposed\nfoundation model solution. To this end, we introduce OPERA, an OPEn Respiratory\nAcoustic foundation model pretraining and benchmarking system, as the first\napproach answering this need. We curate large-scale respiratory audio datasets\n(~136K samples, over 400 hours), pretrain three pioneering foundation models,\nand build a benchmark consisting of 19 downstream respiratory health tasks for\nevaluation. Our pretrained models demonstrate superior performance (against\nexisting acoustic models pretrained with general audio on 16 out of 19 tasks)\nand generalizability (to unseen datasets and new respiratory audio modalities).\nThis highlights the great promise of respiratory acoustic foundation models and\nencourages more studies using OPERA as an open resource to accelerate research\non respiratory audio for health. The system is accessible from\nhttps://github.com/evelyn0414/OPERA.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "accepted by NeurIPS 2024 Track Datasets and Benchmarks",
    "pdf_url": "http://arxiv.org/pdf/2406.16148v3",
    "published_date": "2024-06-23 16:04:26 UTC",
    "updated_date": "2024-11-07 15:23:59 UTC"
  },
  {
    "arxiv_id": "2407.02518v2",
    "title": "INDICT: Code Generation with Internal Dialogues of Critiques for Both Security and Helpfulness",
    "authors": [
      "Hung Le",
      "Yingbo Zhou",
      "Caiming Xiong",
      "Silvio Savarese",
      "Doyen Sahoo"
    ],
    "abstract": "Large language models (LLMs) for code are typically trained to align with\nnatural language instructions to closely follow their intentions and\nrequirements. However, in many practical scenarios, it becomes increasingly\nchallenging for these models to navigate the intricate boundary between\nhelpfulness and safety, especially against highly complex yet potentially\nmalicious instructions. In this work, we introduce INDICT: a new framework that\nempowers LLMs with Internal Dialogues of Critiques for both safety and\nhelpfulness guidance. The internal dialogue is a dual cooperative system\nbetween a safety-driven critic and a helpfulness-driven critic. Each critic\nprovides analysis against the given task and corresponding generated response,\nequipped with external knowledge queried through relevant code snippets and\ntools like web search and code interpreter. We engage the dual critic system in\nboth code generation stage as well as code execution stage, providing\npreemptive and post-hoc guidance respectively to LLMs. We evaluated INDICT on 8\ndiverse tasks across 8 programming languages from 5 benchmarks, using LLMs from\n7B to 70B parameters. We observed that our approach can provide an advanced\nlevel of critiques of both safety and helpfulness analysis, significantly\nimproving the quality of output codes ($+10\\%$ absolute improvements in all\nmodels).",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.MA",
      "cs.PL"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted to The Thirty-Eighth Annual Conference on Neural Information\n  Processing Systems (NeurIPS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.02518v2",
    "published_date": "2024-06-23 15:55:07 UTC",
    "updated_date": "2024-10-29 08:20:28 UTC"
  },
  {
    "arxiv_id": "2406.16145v1",
    "title": "Predefined Prototypes for Intra-Class Separation and Disentanglement",
    "authors": [
      "Antonio Almudévar",
      "Théo Mariotte",
      "Alfonso Ortega",
      "Marie Tahon",
      "Luis Vicente",
      "Antonio Miguel",
      "Eduardo Lleida"
    ],
    "abstract": "Prototypical Learning is based on the idea that there is a point (which we\ncall prototype) around which the embeddings of a class are clustered. It has\nshown promising results in scenarios with little labeled data or to design\nexplainable models. Typically, prototypes are either defined as the average of\nthe embeddings of a class or are designed to be trainable. In this work, we\npropose to predefine prototypes following human-specified criteria, which\nsimplify the training pipeline and brings different advantages. Specifically,\nin this work we explore two of these advantages: increasing the inter-class\nseparability of embeddings and disentangling embeddings with respect to\ndifferent variance factors, which can translate into the possibility of having\nexplainable predictions. Finally, we propose different experiments that help to\nunderstand our proposal and demonstrate empirically the mentioned advantages.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16145v1",
    "published_date": "2024-06-23 15:52:23 UTC",
    "updated_date": "2024-06-23 15:52:23 UTC"
  },
  {
    "arxiv_id": "2407.02517v2",
    "title": "CAV-AHDV-CAV: Mitigating Traffic Oscillations for CAVs through a Novel Car-Following Structure and Reinforcement Learning",
    "authors": [
      "Xianda Chen",
      "PakHin Tiu",
      "Yihuai Zhang",
      "Xinhu Zheng",
      "Meixin Zhu"
    ],
    "abstract": "Connected and Automated Vehicles (CAVs) offer a promising solution to the\nchallenges of mixed traffic with both CAVs and Human-Driven Vehicles (HDVs). A\nsignificant hurdle in such scenarios is traffic oscillation, or the\n\"stop-and-go\" pattern, during car-following situations. While HDVs rely on\nlimited information, CAVs can leverage data from other CAVs for better\ndecision-making. This allows CAVs to anticipate and mitigate the spread of\ndeceleration waves that worsen traffic flow. We propose a novel \"CAV-AHDV-CAV\"\ncar-following framework that treats the sequence of HDVs between two CAVs as a\nsingle entity, eliminating noise from individual driver behaviors. This deep\nreinforcement learning approach analyzes vehicle equilibrium states and employs\na state fusion strategy. Trained and tested on diverse datasets (HighD, NGSIM,\nSPMD, Waymo, Lyft) encompassing over 70,000 car-following instances, our model\noutperforms baselines in collision avoidance, maintaining equilibrium with both\npreceding and leading vehicles and achieving the lowest standard deviation of\ntime headway. These results demonstrate the effectiveness of our approach in\ndeveloping robust CAV control strategies for mixed traffic. Our model has the\npotential to mitigate traffic oscillation, improve traffic flow efficiency, and\nenhance overall safety.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Due to errors identified in the experimental methodology and results\n  sections, we request to withdraw this paper",
    "pdf_url": "http://arxiv.org/pdf/2407.02517v2",
    "published_date": "2024-06-23 15:38:29 UTC",
    "updated_date": "2024-07-11 03:27:50 UTC"
  },
  {
    "arxiv_id": "2407.07744v1",
    "title": "Belief Information based Deep Channel Estimation for Massive MIMO Systems",
    "authors": [
      "Jialong Xu",
      "Liu Liu",
      "Xin Wang",
      "Lan Chen"
    ],
    "abstract": "In the next generation wireless communication system, transmission rates\nshould continue to rise to support emerging scenarios, e.g., the immersive\ncommunications. From the perspective of communication system evolution,\nmultiple-input multiple-output (MIMO) technology remains pivotal for enhancing\ntransmission rates. However, current MIMO systems rely on inserting pilot\nsignals to achieve accurate channel estimation. As the increase of transmit\nstream, the pilots consume a significant portion of transmission resources,\nseverely reducing the spectral efficiency. In this correspondence, we propose a\nbelief information based mechanism. By introducing a plug-and-play belief\ninformation module, existing single-antenna channel estimation networks could\nbe seamlessly adapted to multi-antenna channel estimation and fully exploit the\nspatial correlation among multiple antennas. Experimental results demonstrate\nthat the proposed method can either improve 1 ~ 2 dB channel estimation\nperformance or reduce 1/3 ~ 1/2 pilot overhead, particularly in bad channel\nconditions.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "eess.SP",
      "math.IT"
    ],
    "primary_category": "cs.IT",
    "comment": "5 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.07744v1",
    "published_date": "2024-06-23 15:31:07 UTC",
    "updated_date": "2024-06-23 15:31:07 UTC"
  },
  {
    "arxiv_id": "2406.16978v1",
    "title": "MetaFollower: Adaptable Personalized Autonomous Car Following",
    "authors": [
      "Xianda Chen",
      "Kehua Chen",
      "Meixin Zhu",
      "Hao",
      "Yang",
      "Shaojie Shen",
      "Xuesong Wang",
      "Yinhai Wang"
    ],
    "abstract": "Car-following (CF) modeling, a fundamental component in microscopic traffic\nsimulation, has attracted increasing interest of researchers in the past\ndecades. In this study, we propose an adaptable personalized car-following\nframework -MetaFollower, by leveraging the power of meta-learning.\nSpecifically, we first utilize Model-Agnostic Meta-Learning (MAML) to extract\ncommon driving knowledge from various CF events. Afterward, the pre-trained\nmodel can be fine-tuned on new drivers with only a few CF trajectories to\nachieve personalized CF adaptation. We additionally combine Long Short-Term\nMemory (LSTM) and Intelligent Driver Model (IDM) to reflect temporal\nheterogeneity with high interpretability. Unlike conventional adaptive cruise\ncontrol (ACC) systems that rely on predefined settings and constant parameters\nwithout considering heterogeneous driving characteristics, MetaFollower can\naccurately capture and simulate the intricate dynamics of car-following\nbehavior while considering the unique driving styles of individual drivers. We\ndemonstrate the versatility and adaptability of MetaFollower by showcasing its\nability to adapt to new drivers with limited training data quickly. To evaluate\nthe performance of MetaFollower, we conduct rigorous experiments comparing it\nwith both data-driven and physics-based models. The results reveal that our\nproposed framework outperforms baseline models in predicting car-following\nbehavior with higher accuracy and safety. To the best of our knowledge, this is\nthe first car-following model aiming to achieve fast adaptation by considering\nboth driver and temporal heterogeneity based on meta-learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16978v1",
    "published_date": "2024-06-23 15:30:40 UTC",
    "updated_date": "2024-06-23 15:30:40 UTC"
  },
  {
    "arxiv_id": "2407.02516v1",
    "title": "EditFollower: Tunable Car Following Models for Customizable Adaptive Cruise Control Systems",
    "authors": [
      "Xianda Chen",
      "Xu Han",
      "Meixin Zhu",
      "Xiaowen Chu",
      "PakHin Tiu",
      "Xinhu Zheng",
      "Yinhai Wang"
    ],
    "abstract": "In the realm of driving technologies, fully autonomous vehicles have not been\nwidely adopted yet, making advanced driver assistance systems (ADAS) crucial\nfor enhancing driving experiences. Adaptive Cruise Control (ACC) emerges as a\npivotal component of ADAS. However, current ACC systems often employ fixed\nsettings, failing to intuitively capture drivers' social preferences and\nleading to potential function disengagement. To overcome these limitations, we\npropose the Editable Behavior Generation (EBG) model, a data-driven\ncar-following model that allows for adjusting driving discourtesy levels. The\nframework integrates diverse courtesy calculation methods into long short-term\nmemory (LSTM) and Transformer architectures, offering a comprehensive approach\nto capture nuanced driving dynamics. By integrating various discourtesy values\nduring the training process, our model generates realistic agent trajectories\nwith different levels of courtesy in car-following behavior. Experimental\nresults on the HighD and Waymo datasets showcase a reduction in Mean Squared\nError (MSE) of spacing and MSE of speed compared to baselines, establishing\nstyle controllability. To the best of our knowledge, this work represents the\nfirst data-driven car-following model capable of dynamically adjusting\ndiscourtesy levels. Our model provides valuable insights for the development of\nACC systems that take into account drivers' social preferences.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.02516v1",
    "published_date": "2024-06-23 15:04:07 UTC",
    "updated_date": "2024-06-23 15:04:07 UTC"
  },
  {
    "arxiv_id": "2407.16902v1",
    "title": "The Potential and Perils of Generative Artificial Intelligence for Quality Improvement and Patient Safety",
    "authors": [
      "Laleh Jalilian",
      "Daniel McDuff",
      "Achuta Kadambi"
    ],
    "abstract": "Generative artificial intelligence (GenAI) has the potential to improve\nhealthcare through automation that enhances the quality and safety of patient\ncare. Powered by foundation models that have been pretrained and can generate\ncomplex content, GenAI represents a paradigm shift away from the more\ntraditional focus on task-specific classifiers that have dominated the AI\nlandscape thus far. We posit that the imminent application of GenAI in\nhealthcare will be through well-defined, low risk, high value, and narrow\napplications that automate healthcare workflows at the point of care using\nsmaller foundation models. These models will be finetuned for different\ncapabilities and application specific scenarios and will have the ability to\nprovide medical explanations, reference evidence within a retrieval augmented\nframework and utilizing external tools. We contrast this with a general,\nall-purpose AI model for end-to-end clinical decision making that improves\nclinician performance, including safety-critical diagnostic tasks, which will\nrequire greater research prior to implementation. We consider areas where\n'human in the loop' Generative AI can improve healthcare quality and safety by\nautomating mundane tasks. Using the principles of implementation science will\nbe critical for integrating 'end to end' GenAI systems that will be accepted by\nhealthcare teams.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16902v1",
    "published_date": "2024-06-23 15:01:11 UTC",
    "updated_date": "2024-06-23 15:01:11 UTC"
  },
  {
    "arxiv_id": "2406.16125v1",
    "title": "CBPF: Filtering Poisoned Data Based on Composite Backdoor Attack",
    "authors": [
      "Hanfeng Xia",
      "Haibo Hong",
      "Ruili Wang"
    ],
    "abstract": "Backdoor attacks involve the injection of a limited quantity of poisoned\nexamples containing triggers into the training dataset. During the inference\nstage, backdoor attacks can uphold a high level of accuracy for normal\nexamples, yet when presented with trigger-containing instances, the model may\nerroneously predict them as the targeted class designated by the attacker. This\npaper explores strategies for mitigating the risks associated with backdoor\nattacks by examining the filtration of poisoned samples.We primarily leverage\ntwo key characteristics of backdoor attacks: the ability for multiple backdoors\nto exist simultaneously within a single model, and the discovery through\nComposite Backdoor Attack (CBA) that altering two triggers in a sample to new\ntarget labels does not compromise the original functionality of the triggers,\nyet enables the prediction of the data as a new target class when both triggers\nare present simultaneously.Therefore, a novel three-stage poisoning data\nfiltering approach, known as Composite Backdoor Poison Filtering (CBPF), is\nproposed as an effective solution. Firstly, utilizing the identified\ndistinctions in output between poisoned and clean samples, a subset of data is\npartitioned to include both poisoned and clean instances. Subsequently, benign\ntriggers are incorporated and labels are adjusted to create new target and\nbenign target classes, thereby prompting the poisoned and clean data to be\nclassified as distinct entities during the inference stage. The experimental\nresults indicate that CBPF is successful in filtering out malicious data\nproduced by six advanced attacks on CIFAR10 and ImageNet-12. On average, CBPF\nattains a notable filtering success rate of 99.91% for the six attacks on\nCIFAR10. Additionally, the model trained on the uncontaminated samples exhibits\nsustained high accuracy levels.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16125v1",
    "published_date": "2024-06-23 14:37:24 UTC",
    "updated_date": "2024-06-23 14:37:24 UTC"
  },
  {
    "arxiv_id": "2406.16121v2",
    "title": "Diffusion Spectral Representation for Reinforcement Learning",
    "authors": [
      "Dmitry Shribak",
      "Chen-Xiao Gao",
      "Yitong Li",
      "Chenjun Xiao",
      "Bo Dai"
    ],
    "abstract": "Diffusion-based models have achieved notable empirical successes in\nreinforcement learning (RL) due to their expressiveness in modeling complex\ndistributions. Despite existing methods being promising, the key challenge of\nextending existing methods for broader real-world applications lies in the\ncomputational cost at inference time, i.e., sampling from a diffusion model is\nconsiderably slow as it often requires tens to hundreds of iterations to\ngenerate even one sample. To circumvent this issue, we propose to leverage the\nflexibility of diffusion models for RL from a representation learning\nperspective. In particular, by exploiting the connection between diffusion\nmodels and energy-based models, we develop Diffusion Spectral Representation\n(Diff-SR), a coherent algorithm framework that enables extracting sufficient\nrepresentations for value functions in Markov decision processes (MDP) and\npartially observable Markov decision processes (POMDP). We further demonstrate\nhow Diff-SR facilitates efficient policy optimization and practical algorithms\nwhile explicitly bypassing the difficulty and inference cost of sampling from\nthe diffusion model. Finally, we provide comprehensive empirical studies to\nverify the benefits of Diff-SR in delivering robust and advantageous\nperformance across various benchmarks with both fully and partially observable\nsettings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.16121v2",
    "published_date": "2024-06-23 14:24:14 UTC",
    "updated_date": "2024-11-01 16:30:00 UTC"
  },
  {
    "arxiv_id": "2406.16111v1",
    "title": "Multi-Scale Temporal Difference Transformer for Video-Text Retrieval",
    "authors": [
      "Ni Wang",
      "Dongliang Liao",
      "Xing Xu"
    ],
    "abstract": "Currently, in the field of video-text retrieval, there are many\ntransformer-based methods. Most of them usually stack frame features and\nregrade frames as tokens, then use transformers for video temporal modeling.\nHowever, they commonly neglect the inferior ability of the transformer modeling\nlocal temporal information. To tackle this problem, we propose a transformer\nvariant named Multi-Scale Temporal Difference Transformer (MSTDT). MSTDT mainly\naddresses the defects of the traditional transformer which has limited ability\nto capture local temporal information. Besides, in order to better model the\ndetailed dynamic information, we make use of the difference feature between\nframes, which practically reflects the dynamic movement of a video. We extract\nthe inter-frame difference feature and integrate the difference and frame\nfeature by the multi-scale temporal transformer. In general, our proposed MSTDT\nconsists of a short-term multi-scale temporal difference transformer and a\nlong-term temporal transformer. The former focuses on modeling local temporal\ninformation, the latter aims at modeling global temporal information. At last,\nwe propose a new loss to narrow the distance of similar samples. Extensive\nexperiments show that backbone, such as CLIP, with MSTDT has attained a new\nstate-of-the-art result.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16111v1",
    "published_date": "2024-06-23 13:59:31 UTC",
    "updated_date": "2024-06-23 13:59:31 UTC"
  },
  {
    "arxiv_id": "2406.16106v1",
    "title": "Evaluating Ensemble Methods for News Recommender Systems",
    "authors": [
      "Alexander Gray",
      "Noorhan Abbas"
    ],
    "abstract": "News recommendation is crucial for facilitating individuals' access to\narticles, particularly amid the increasingly digital landscape of news\nconsumption. Consequently, extensive research is dedicated to News Recommender\nSystems (NRS) with increasingly sophisticated algorithms. Despite this\nsustained scholarly inquiry, there exists a notable research gap regarding the\npotential synergy achievable by amalgamating these algorithms to yield superior\noutcomes. This paper endeavours to address this gap by demonstrating how\nensemble methods can be used to combine many diverse state-of-the-art\nalgorithms to achieve superior results on the Microsoft News dataset (MIND).\nAdditionally, we identify scenarios where ensemble methods fail to improve\nresults and offer explanations for this occurrence. Our findings demonstrate\nthat a combination of NRS algorithms can outperform individual algorithms,\nprovided that the base learners are sufficiently diverse, with improvements of\nup to 5\\% observed for an ensemble consisting of a content-based BERT approach\nand the collaborative filtering LSTUR algorithm. Additionally, our results\ndemonstrate the absence of any improvement when combining insufficiently\ndistinct methods. These findings provide insight into successful approaches of\nensemble methods in NRS and advocates for the development of better systems\nthrough appropriate ensemble solutions.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16106v1",
    "published_date": "2024-06-23 13:40:50 UTC",
    "updated_date": "2024-06-23 13:40:50 UTC"
  },
  {
    "arxiv_id": "2406.16093v1",
    "title": "Towards Natural Language-Driven Assembly Using Foundation Models",
    "authors": [
      "Omkar Joglekar",
      "Tal Lancewicki",
      "Shir Kozlovsky",
      "Vladimir Tchuiev",
      "Zohar Feldman",
      "Dotan Di Castro"
    ],
    "abstract": "Large Language Models (LLMs) and strong vision models have enabled rapid\nresearch and development in the field of Vision-Language-Action models that\nenable robotic control. The main objective of these methods is to develop a\ngeneralist policy that can control robots with various embodiments. However, in\nindustrial robotic applications such as automated assembly and disassembly,\nsome tasks, such as insertion, demand greater accuracy and involve intricate\nfactors like contact engagement, friction handling, and refined motor skills.\nImplementing these skills using a generalist policy is challenging because\nthese policies might integrate further sensory data, including force or torque\nmeasurements, for enhanced precision. In our method, we present a global\ncontrol policy based on LLMs that can transfer the control policy to a finite\nset of skills that are specifically trained to perform high-precision tasks\nthrough dynamic context switching. The integration of LLMs into this framework\nunderscores their significance in not only interpreting and processing language\ninputs but also in enriching the control mechanisms for diverse and intricate\nrobotic operations.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16093v1",
    "published_date": "2024-06-23 12:14:37 UTC",
    "updated_date": "2024-06-23 12:14:37 UTC"
  },
  {
    "arxiv_id": "2406.16087v5",
    "title": "Imperative Learning: A Self-supervised Neuro-Symbolic Learning Framework for Robot Autonomy",
    "authors": [
      "Chen Wang",
      "Kaiyi Ji",
      "Junyi Geng",
      "Zhongqiang Ren",
      "Taimeng Fu",
      "Fan Yang",
      "Yifan Guo",
      "Haonan He",
      "Xiangyu Chen",
      "Zitong Zhan",
      "Qiwei Du",
      "Shaoshu Su",
      "Bowen Li",
      "Yuheng Qiu",
      "Yi Du",
      "Qihang Li",
      "Yifan Yang",
      "Xiao Lin",
      "Zhipeng Zhao"
    ],
    "abstract": "Data-driven methods such as reinforcement and imitation learning have\nachieved remarkable success in robot autonomy. However, their data-centric\nnature still hinders them from generalizing well to ever-changing environments.\nMoreover, collecting large datasets for robotic tasks is often impractical and\nexpensive. To overcome these challenges, we introduce a new self-supervised\nneuro-symbolic (NeSy) computational framework, imperative learning (IL), for\nrobot autonomy, leveraging the generalization abilities of symbolic reasoning.\nThe framework of IL consists of three primary components: a neural module, a\nreasoning engine, and a memory system. We formulate IL as a special bilevel\noptimization (BLO), which enables reciprocal learning over the three modules.\nThis overcomes the label-intensive obstacles associated with data-driven\napproaches and takes advantage of symbolic reasoning concerning logical\nreasoning, physical principles, geometric analysis, etc. We discuss several\noptimization techniques for IL and verify their effectiveness in five distinct\nrobot autonomy tasks including path planning, rule induction, optimal control,\nvisual odometry, and multi-robot routing. Through various experiments, we show\nthat IL can significantly enhance robot autonomy capabilities and we anticipate\nthat it will catalyze further research across diverse domains.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16087v5",
    "published_date": "2024-06-23 12:02:17 UTC",
    "updated_date": "2025-01-25 04:11:34 UTC"
  },
  {
    "arxiv_id": "2406.17807v5",
    "title": "Enhancing Commentary Strategies for Imperfect Information Card Games: A Study of Large Language Models in Guandan Commentary",
    "authors": [
      "Meiling Tao",
      "Xuechen Liang",
      "Xinyuan Song",
      "Yangfan He",
      "Yiling Tao",
      "Jianhui Wang",
      "Sun Li Tianyu Shi"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have unlocked the\npotential for generating high-quality game commentary. However, producing\ninsightful and engaging commentary for complex games with incomplete\ninformation remains a significant challenge. In this paper, we introduce a\nnovel commentary method that combine Reinforcement Learning (RL) and LLMs,\ntailored specifically for the Chinese card game \\textit{Guandan}. Our system\nleverages RL to generate intricate card-playing scenarios and employs LLMs to\ngenerate corresponding commentary text, effectively emulating the strategic\nanalysis and narrative prowess of professional commentators. The framework\ncomprises a state commentary guide, a Theory of Mind (ToM)-based strategy\nanalyzer, and a style retrieval module, which seamlessly collaborate to deliver\ndetailed and context-relevant game commentary in the Chinese language\nenvironment. We empower LLMs with ToM capabilities and refine both retrieval\nand information filtering mechanisms. This facilitates the generation of\npersonalized commentary content. Our experimental results showcase the\nsubstantial enhancement in performance achieved by the proposed commentary\nframework when applied to open-source LLMs, surpassing the performance of GPT-4\nacross multiple evaluation metrics.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17807v5",
    "published_date": "2024-06-23 11:58:26 UTC",
    "updated_date": "2025-04-15 15:28:20 UTC"
  },
  {
    "arxiv_id": "2406.16079v1",
    "title": "EERPD: Leveraging Emotion and Emotion Regulation for Improving Personality Detection",
    "authors": [
      "Zheng Li",
      "Dawei Zhu",
      "Qilong Ma",
      "Weimin Xiong",
      "Sujian Li"
    ],
    "abstract": "Personality is a fundamental construct in psychology, reflecting an\nindividual's behavior, thinking, and emotional patterns. Previous researches\nhave made some progress in personality detection, primarily by utilizing the\nwhole text to predict personality. However, these studies generally tend to\noverlook psychological knowledge: they rarely apply the well-established\ncorrelations between emotion regulation and personality. Based on this, we\npropose a new personality detection method called EERPD. This method introduces\nthe use of emotion regulation, a psychological concept highly correlated with\npersonality, for personality prediction. By combining this feature with emotion\nfeatures, it retrieves few-shot examples and provides process CoTs for\ninferring labels from text. This approach enhances the understanding of LLM for\npersonality within text and improves the performance in personality detection.\nExperimental results demonstrate that EERPD significantly enhances the accuracy\nand robustness of personality detection, outperforming previous SOTA by\n15.05/4.29 in average F1 on the two benchmark datasets.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16079v1",
    "published_date": "2024-06-23 11:18:55 UTC",
    "updated_date": "2024-06-23 11:18:55 UTC"
  },
  {
    "arxiv_id": "2406.16069v3",
    "title": "FastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models",
    "authors": [
      "Junyi Zhu",
      "Shuochen Liu",
      "Yu Yu",
      "Bo Tang",
      "Yibo Yan",
      "Zhiyu Li",
      "Feiyu Xiong",
      "Tong Xu",
      "Matthew B. Blaschko"
    ],
    "abstract": "Large language models (LLMs) excel in generating coherent text, but they\noften struggle with context awareness, leading to inaccuracies in tasks\nrequiring faithful adherence to provided information. We introduce FastMem, a\nnovel method designed to enhance instruction fine-tuned LLMs' context awareness\nthrough fast memorization of the prompt. FastMem maximizes the likelihood of\nthe prompt before inference by updating only the last Feed-Forward Network\n(FFN) module. This targeted approach ensures efficient optimization without\noverfitting, significantly improving the model's ability to comprehend and\naccurately follow the context. Our experiments demonstrate substantial gains in\nreading comprehension, text summarization and adherence to output structures.\nFor instance, FastMem improves the accuracy of Llama 3-8B-Inst on the NQ-SWAP\ndataset from 59.1% to 71.6%, and reduces the output structure failure rate of\nQwen 1.5-4B-Chat from 34.9% to 25.5%. Extensive experimental results highlight\nFastMem's potential to offer a robust solution to enhance the reliability and\naccuracy of LLMs in various applications. Our code is available at:\nhttps://github.com/IAAR-Shanghai/FastMem",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16069v3",
    "published_date": "2024-06-23 10:36:35 UTC",
    "updated_date": "2024-10-04 19:14:32 UTC"
  },
  {
    "arxiv_id": "2406.16068v1",
    "title": "Towards Real-Time Neural Volumetric Rendering on Mobile Devices: A Measurement Study",
    "authors": [
      "Zhe Wang",
      "Yifei Zhu"
    ],
    "abstract": "Neural Radiance Fields (NeRF) is an emerging technique to synthesize 3D\nobjects from 2D images with a wide range of potential applications. However,\nrendering existing NeRF models is extremely computation intensive, making it\nchallenging to support real-time interaction on mobile devices. In this paper,\nwe take the first initiative to examine the state-of-the-art real-time NeRF\nrendering technique from a system perspective. We first define the entire\nworking pipeline of the NeRF serving system. We then identify possible control\nknobs that are critical to the system from the communication, computation, and\nvisual performance perspective. Furthermore, an extensive measurement study is\nconducted to reveal the effects of these control knobs on system performance.\nOur measurement results reveal that different control knobs contribute\ndifferently towards improving the system performance, with the mesh granularity\nbeing the most effective knob and the quantization being the least effective\nknob. In addition, diverse hardware device settings and network conditions have\nto be considered to fully unleash the benefit of operating under the\nappropriate knobs",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.GR",
      "cs.MM",
      "cs.PF"
    ],
    "primary_category": "cs.DC",
    "comment": "This paper is accepted by ACM SIGCOMM Workshop on Emerging Multimedia\n  Systems 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.16068v1",
    "published_date": "2024-06-23 10:33:26 UTC",
    "updated_date": "2024-06-23 10:33:26 UTC"
  },
  {
    "arxiv_id": "2407.09523v1",
    "title": "MuseCL: Predicting Urban Socioeconomic Indicators via Multi-Semantic Contrastive Learning",
    "authors": [
      "Xixian Yong",
      "Xiao Zhou"
    ],
    "abstract": "Predicting socioeconomic indicators within urban regions is crucial for\nfostering inclusivity, resilience, and sustainability in cities and human\nsettlements. While pioneering studies have attempted to leverage multi-modal\ndata for socioeconomic prediction, jointly exploring their underlying semantics\nremains a significant challenge. To address the gap, this paper introduces a\nMulti-Semantic Contrastive Learning (MuseCL) framework for fine-grained urban\nregion profiling and socioeconomic prediction. Within this framework, we\ninitiate the process by constructing contrastive sample pairs for street view\nand remote sensing images, capitalizing on the similarities in human mobility\nand Point of Interest (POI) distribution to derive semantic features from the\nvisual modality. Additionally, we extract semantic insights from POI texts\nembedded within these regions, employing a pre-trained text encoder. To merge\nthe acquired visual and textual features, we devise an innovative\ncross-modality-based attentional fusion module, which leverages a contrastive\nmechanism for integration. Experimental results across multiple cities and\nindicators consistently highlight the superiority of MuseCL, demonstrating an\naverage improvement of 10% in $R^2$ compared to various competitive baseline\nmodels. The code of this work is publicly available at\nhttps://github.com/XixianYong/MuseCL.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09523v1",
    "published_date": "2024-06-23 09:49:41 UTC",
    "updated_date": "2024-06-23 09:49:41 UTC"
  },
  {
    "arxiv_id": "2406.16045v1",
    "title": "Combine and Conquer: A Meta-Analysis on Data Shift and Out-of-Distribution Detection",
    "authors": [
      "Eduardo Dadalto",
      "Florence Alberge",
      "Pierre Duhamel",
      "Pablo Piantanida"
    ],
    "abstract": "This paper introduces a universal approach to seamlessly combine\nout-of-distribution (OOD) detection scores. These scores encompass a wide range\nof techniques that leverage the self-confidence of deep learning models and the\nanomalous behavior of features in the latent space. Not surprisingly, combining\nsuch a varied population using simple statistics proves inadequate. To overcome\nthis challenge, we propose a quantile normalization to map these scores into\np-values, effectively framing the problem into a multi-variate hypothesis test.\nThen, we combine these tests using established meta-analysis tools, resulting\nin a more effective detector with consolidated decision boundaries.\nFurthermore, we create a probabilistic interpretable criterion by mapping the\nfinal statistics into a distribution with known parameters. Through empirical\ninvestigation, we explore different types of shifts, each exerting varying\ndegrees of impact on data. Our results demonstrate that our approach\nsignificantly improves overall robustness and performance across diverse OOD\ndetection scenarios. Notably, our framework is easily extensible for future\ndevelopments in detection scores and stands as the first to combine decision\nboundaries in this context. The code and artifacts associated with this work\nare publicly available\\footnote{\\url{https://github.com/edadaltocg/detectors}}.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "Accepted for publication in Transactions on Machine Learning Research\n  (TMLR)",
    "pdf_url": "http://arxiv.org/pdf/2406.16045v1",
    "published_date": "2024-06-23 08:16:44 UTC",
    "updated_date": "2024-06-23 08:16:44 UTC"
  },
  {
    "arxiv_id": "2407.09522v2",
    "title": "UQE: A Query Engine for Unstructured Databases",
    "authors": [
      "Hanjun Dai",
      "Bethany Yixin Wang",
      "Xingchen Wan",
      "Bo Dai",
      "Sherry Yang",
      "Azade Nova",
      "Pengcheng Yin",
      "Phitchaya Mangpo Phothilimthana",
      "Charles Sutton",
      "Dale Schuurmans"
    ],
    "abstract": "Analytics on structured data is a mature field with many successful methods.\nHowever, most real world data exists in unstructured form, such as images and\nconversations. We investigate the potential of Large Language Models (LLMs) to\nenable unstructured data analytics. In particular, we propose a new Universal\nQuery Engine (UQE) that directly interrogates and draws insights from\nunstructured data collections. This engine accepts queries in a Universal Query\nLanguage (UQL), a dialect of SQL that provides full natural language\nflexibility in specifying conditions and operators. The new engine leverages\nthe ability of LLMs to conduct analysis of unstructured data, while also\nallowing us to exploit advances in sampling and optimization techniques to\nachieve efficient and accurate query execution. In addition, we borrow\ntechniques from classical compiler theory to better orchestrate the workflow\nbetween sampling methods and foundation model calls. We demonstrate the\nefficiency of UQE on data analytics across different modalities, including\nimages, dialogs and reviews, across a range of useful query types, including\nconditional aggregation, semantic retrieval and abstraction aggregation.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09522v2",
    "published_date": "2024-06-23 06:58:55 UTC",
    "updated_date": "2024-11-17 02:22:36 UTC"
  },
  {
    "arxiv_id": "2406.16030v2",
    "title": "Zero-Shot Cross-Lingual NER Using Phonemic Representations for Low-Resource Languages",
    "authors": [
      "Jimin Sohn",
      "Haeji Jung",
      "Alex Cheng",
      "Jooeon Kang",
      "Yilin Du",
      "David R. Mortensen"
    ],
    "abstract": "Existing zero-shot cross-lingual NER approaches require substantial prior\nknowledge of the target language, which is impractical for low-resource\nlanguages. In this paper, we propose a novel approach to NER using phonemic\nrepresentation based on the International Phonetic Alphabet (IPA) to bridge the\ngap between representations of different languages. Our experiments show that\nour method significantly outperforms baseline models in extremely low-resource\nlanguages, with the highest average F1 score (46.38%) and lowest standard\ndeviation (12.67), particularly demonstrating its robustness with non-Latin\nscripts. Our codes are available at\nhttps://github.com/Gabriel819/zeroshot_ner.git",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2024 Main Conference",
    "pdf_url": "http://arxiv.org/pdf/2406.16030v2",
    "published_date": "2024-06-23 06:38:56 UTC",
    "updated_date": "2024-10-22 01:31:31 UTC"
  },
  {
    "arxiv_id": "2406.16028v2",
    "title": "TimeAutoDiff: Combining Autoencoder and Diffusion model for time series tabular data synthesizing",
    "authors": [
      "Namjoon Suh",
      "Yuning Yang",
      "Din-Yin Hsieh",
      "Qitong Luan",
      "Shirong Xu",
      "Shixiang Zhu",
      "Guang Cheng"
    ],
    "abstract": "In this paper, we leverage the power of latent diffusion models to generate\nsynthetic time series tabular data. Along with the temporal and feature\ncorrelations, the heterogeneous nature of the feature in the table has been one\nof the main obstacles in time series tabular data modeling. We tackle this\nproblem by combining the ideas of the variational auto-encoder (VAE) and the\ndenoising diffusion probabilistic model (DDPM). Our model named as\n\\texttt{TimeAutoDiff} has several key advantages including (1) Generality: the\nability to handle the broad spectrum of time series tabular data from single to\nmulti-sequence datasets; (2) Good fidelity and utility guarantees: numerical\nexperiments on six publicly available datasets demonstrating significant\nimprovements over state-of-the-art models in generating time series tabular\ndata, across four metrics measuring fidelity and utility; (3) Fast sampling\nspeed: entire time series data generation as opposed to the sequential data\nsampling schemes implemented in the existing diffusion-based models, eventually\nleading to significant improvements in sampling speed, (4) Entity conditional\ngeneration: the first implementation of conditional generation of\nmulti-sequence time series tabular data with heterogenous features in the\nliterature, enabling scenario exploration across multiple scientific and\nengineering domains. Codes are in preparation for release to the public, but\navailable upon request.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16028v2",
    "published_date": "2024-06-23 06:32:27 UTC",
    "updated_date": "2024-07-15 04:36:30 UTC"
  },
  {
    "arxiv_id": "2406.16976v3",
    "title": "Efficient Evolutionary Search Over Chemical Space with Large Language Models",
    "authors": [
      "Haorui Wang",
      "Marta Skreta",
      "Cher-Tian Ser",
      "Wenhao Gao",
      "Lingkai Kong",
      "Felix Strieth-Kalthoff",
      "Chenru Duan",
      "Yuchen Zhuang",
      "Yue Yu",
      "Yanqiao Zhu",
      "Yuanqi Du",
      "Alán Aspuru-Guzik",
      "Kirill Neklyudov",
      "Chao Zhang"
    ],
    "abstract": "Molecular discovery, when formulated as an optimization problem, presents\nsignificant computational challenges because optimization objectives can be\nnon-differentiable. Evolutionary Algorithms (EAs), often used to optimize\nblack-box objectives in molecular discovery, traverse chemical space by\nperforming random mutations and crossovers, leading to a large number of\nexpensive objective evaluations. In this work, we ameliorate this shortcoming\nby incorporating chemistry-aware Large Language Models (LLMs) into EAs. Namely,\nwe redesign crossover and mutation operations in EAs using LLMs trained on\nlarge corpora of chemical information. We perform extensive empirical studies\non both commercial and open-source models on multiple tasks involving property\noptimization, molecular rediscovery, and structure-based drug design,\ndemonstrating that the joint usage of LLMs with EAs yields superior performance\nover all baseline models across single- and multi-objective settings. We\ndemonstrate that our algorithm improves both the quality of the final solution\nand convergence speed, thereby reducing the number of required objective\nevaluations. Our code is available at http://github.com/zoom-wang112358/MOLLEO",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG",
      "physics.chem-ph"
    ],
    "primary_category": "cs.NE",
    "comment": "Published in ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.16976v3",
    "published_date": "2024-06-23 06:22:49 UTC",
    "updated_date": "2025-03-07 17:24:35 UTC"
  },
  {
    "arxiv_id": "2406.16021v1",
    "title": "Harvesting Events from Multiple Sources: Towards a Cross-Document Event Extraction Paradigm",
    "authors": [
      "Qiang Gao",
      "Zixiang Meng",
      "Bobo Li",
      "Jun Zhou",
      "Fei Li",
      "Chong Teng",
      "Donghong Ji"
    ],
    "abstract": "Document-level event extraction aims to extract structured event information\nfrom unstructured text. However, a single document often contains limited event\ninformation and the roles of different event arguments may be biased due to the\ninfluence of the information source. This paper addresses the limitations of\ntraditional document-level event extraction by proposing the task of\ncross-document event extraction (CDEE) to integrate event information from\nmultiple documents and provide a comprehensive perspective on events. We\nconstruct a novel cross-document event extraction dataset, namely CLES, which\ncontains 20,059 documents and 37,688 mention-level events, where over 70% of\nthem are cross-document. To build a benchmark, we propose a CDEE pipeline that\nincludes 5 steps, namely event extraction, coreference resolution, entity\nnormalization, role normalization and entity-role resolution. Our CDEE pipeline\nachieves about 72% F1 in end-to-end cross-document event extraction, suggesting\nthe challenge of this task. Our work builds a new line of information\nextraction research and will attract new research attention.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL2024(Findings)",
    "pdf_url": "http://arxiv.org/pdf/2406.16021v1",
    "published_date": "2024-06-23 06:01:11 UTC",
    "updated_date": "2024-06-23 06:01:11 UTC"
  },
  {
    "arxiv_id": "2406.16018v1",
    "title": "Comprehensive characterization of three-qubit Grover search algorithm on IBM's 127-qubit superconducting quantum computers",
    "authors": [
      "M. AbuGhanem"
    ],
    "abstract": "The Grover search algorithm is a pivotal advancement in quantum computing,\npromising a remarkable speedup over classical algorithms in searching\nunstructured large databases. Here, we report results for the implementation\nand characterization of a three-qubit Grover search algorithm using the\nstate-of-the-art scalable quantum computing technology of superconducting\nquantum architectures. To delve into the algorithm's scalability and\nperformance metrics, our investigation spans the execution of the algorithm\nacross all eight conceivable single-result oracles, alongside nine two-result\noracles, employing IBM Quantum's 127-qubit quantum computers. Moreover, we\nconduct five quantum state tomography experiments to precisely gauge the\nbehavior and efficiency of our implemented algorithm under diverse conditions;\nranging from noisy, noise-free environments to the complexities of real-world\nquantum hardware. By connecting theoretical concepts with real-world\nexperiments, this study not only shed light on the potential of NISQ (Noisy\nIntermediate-Scale Quantum) computers in facilitating large-scale database\nsearches but also offer valuable insights into the practical application of the\nGrover search algorithm in real-world quantum computing applications.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.CR",
      "cs.DS"
    ],
    "primary_category": "quant-ph",
    "comment": "15 pages, 7 figures, 8 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.16018v1",
    "published_date": "2024-06-23 05:27:46 UTC",
    "updated_date": "2024-06-23 05:27:46 UTC"
  },
  {
    "arxiv_id": "2406.16013v1",
    "title": "Database-Augmented Query Representation for Information Retrieval",
    "authors": [
      "Soyeong Jeong",
      "Jinheon Baek",
      "Sukmin Cho",
      "Sung Ju Hwang",
      "Jong C. Park"
    ],
    "abstract": "Information retrieval models that aim to search for the documents relevant to\nthe given query have shown many successes, which have been applied to diverse\ntasks. However, the query provided by the user is oftentimes very short, which\nchallenges the retrievers to correctly fetch relevant documents. To tackle\nthis, existing studies have proposed expanding the query with a couple of\nadditional (user-related) features related to the query. Yet, they may be\nsuboptimal to effectively augment the query, though there is plenty of\ninformation available to augment it in a relational database. Motivated by\nthis, we present a novel retrieval framework called Database-Augmented Query\nrepresentation (DAQu), which augments the original query with various\n(query-related) metadata across multiple tables. In addition, as the number of\nfeatures in the metadata can be very large and there is no order among them, we\nencode them with our graph-based set encoding strategy, which considers\nhierarchies of features in the database without order. We validate DAQu in\ndiverse retrieval scenarios that can incorporate metadata from the relational\ndatabase, demonstrating that ours significantly enhances overall retrieval\nperformance, compared to existing query augmentation methods.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16013v1",
    "published_date": "2024-06-23 05:02:21 UTC",
    "updated_date": "2024-06-23 05:02:21 UTC"
  },
  {
    "arxiv_id": "2406.16008v2",
    "title": "Found in the Middle: Calibrating Positional Attention Bias Improves Long Context Utilization",
    "authors": [
      "Cheng-Yu Hsieh",
      "Yung-Sung Chuang",
      "Chun-Liang Li",
      "Zifeng Wang",
      "Long T. Le",
      "Abhishek Kumar",
      "James Glass",
      "Alexander Ratner",
      "Chen-Yu Lee",
      "Ranjay Krishna",
      "Tomas Pfister"
    ],
    "abstract": "Large language models (LLMs), even when specifically trained to process long\ninput contexts, struggle to capture relevant information located in the middle\nof their input. This phenomenon has been known as the lost-in-the-middle\nproblem. In this work, we make three contributions. First, we set out to\nunderstand the factors that cause this phenomenon. In doing so, we establish a\nconnection between lost-in-the-middle to LLMs' intrinsic attention bias: LLMs\nexhibit a U-shaped attention bias where the tokens at the beginning and at the\nend of its input receive higher attention, regardless of their relevance.\nSecond, we mitigate this positional bias through a calibration mechanism,\nfound-in-the-middle, that allows the model to attend to contexts faithfully\naccording to their relevance, even though when they are in the middle. Third,\nwe show found-in-the-middle not only achieves better performance in locating\nrelevant information within a long context, but also eventually leads to\nimproved retrieval-augmented generation (RAG) performance across various tasks,\noutperforming existing methods by up to 15 percentage points. These findings\nopen up future directions in understanding LLM attention bias and its potential\nconsequences.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL Findings 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.16008v2",
    "published_date": "2024-06-23 04:35:42 UTC",
    "updated_date": "2024-07-03 17:40:00 UTC"
  },
  {
    "arxiv_id": "2406.16006v1",
    "title": "Bounding-Box Inference for Error-Aware Model-Based Reinforcement Learning",
    "authors": [
      "Erin J. Talvitie",
      "Zilei Shao",
      "Huiying Li",
      "Jinghan Hu",
      "Jacob Boerma",
      "Rory Zhao",
      "Xintong Wang"
    ],
    "abstract": "In model-based reinforcement learning, simulated experiences from the learned\nmodel are often treated as equivalent to experience from the real environment.\nHowever, when the model is inaccurate, it can catastrophically interfere with\npolicy learning. Alternatively, the agent might learn about the model's\naccuracy and selectively use it only when it can provide reliable predictions.\nWe empirically explore model uncertainty measures for selective planning and\nshow that best results require distribution insensitive inference to estimate\nthe uncertainty over model-based updates. To that end, we propose and evaluate\nbounding-box inference, which operates on bounding-boxes around sets of\npossible states and other quantities. We find that bounding-box inference can\nreliably support effective selective planning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear: Reinforcement Learning Conference (RLC), 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.16006v1",
    "published_date": "2024-06-23 04:23:15 UTC",
    "updated_date": "2024-06-23 04:23:15 UTC"
  },
  {
    "arxiv_id": "2406.16000v1",
    "title": "Predicting Individual Depression Symptoms from Acoustic Features During Speech",
    "authors": [
      "Sebastian Rodriguez",
      "Sri Harsha Dumpala",
      "Katerina Dikaios",
      "Sheri Rempel",
      "Rudolf Uher",
      "Sageev Oore"
    ],
    "abstract": "Current automatic depression detection systems provide predictions directly\nwithout relying on the individual symptoms/items of depression as denoted in\nthe clinical depression rating scales. In contrast, clinicians assess each item\nin the depression rating scale in a clinical setting, thus implicitly providing\na more detailed rationale for a depression diagnosis. In this work, we make a\nfirst step towards using the acoustic features of speech to predict individual\nitems of the depression rating scale before obtaining the final depression\nprediction. For this, we use convolutional (CNN) and recurrent (long short-term\nmemory (LSTM)) neural networks. We consider different approaches to learning\nthe temporal context of speech. Further, we analyze two variants of voting\nschemes for individual item prediction and depression detection. We also\ninclude an animated visualization that shows an example of item prediction over\ntime as the speech progresses.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16000v1",
    "published_date": "2024-06-23 03:26:47 UTC",
    "updated_date": "2024-06-23 03:26:47 UTC"
  },
  {
    "arxiv_id": "2406.15996v1",
    "title": "Memorizing Documents with Guidance in Large Language Models",
    "authors": [
      "Bumjin Park",
      "Jaesik Choi"
    ],
    "abstract": "Training data plays a pivotal role in AI models. Large language models (LLMs)\nare trained with massive amounts of documents, and their parameters hold\ndocument-related contents. Recently, several studies identified\ncontent-specific locations in LLMs by examining the parameters. Instead of the\npost hoc interpretation, we propose another approach. We propose document-wise\nmemory architecture to track document memories in training. The proposed\narchitecture maps document representations to memory entries, which softly mask\nmemories in the forward process of LLMs. Additionally, we propose document\nguidance loss, which increases the likelihood of text with document memories\nand reduces the likelihood of the text with the memories of other documents.\nExperimental results on Wikitext-103-v1 with Pythia-1B show that the proposed\nmethods provide different memory entries for documents and high recall of\ndocument-related content in generation with trained document-wise memories.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "IJCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.15996v1",
    "published_date": "2024-06-23 03:12:03 UTC",
    "updated_date": "2024-06-23 03:12:03 UTC"
  },
  {
    "arxiv_id": "2406.15990v1",
    "title": "Enhancing Cross-Document Event Coreference Resolution by Discourse Structure and Semantic Information",
    "authors": [
      "Qiang Gao",
      "Bobo Li",
      "Zixiang Meng",
      "Yunlong Li",
      "Jun Zhou",
      "Fei Li",
      "Chong Teng",
      "Donghong Ji"
    ],
    "abstract": "Existing cross-document event coreference resolution models, which either\ncompute mention similarity directly or enhance mention representation by\nextracting event arguments (such as location, time, agent, and patient),\nlacking the ability to utilize document-level information. As a result, they\nstruggle to capture long-distance dependencies. This shortcoming leads to their\nunderwhelming performance in determining coreference for the events where their\nargument information relies on long-distance dependencies. In light of these\nlimitations, we propose the construction of document-level Rhetorical Structure\nTheory (RST) trees and cross-document Lexical Chains to model the structural\nand semantic information of documents. Subsequently, cross-document\nheterogeneous graphs are constructed and GAT is utilized to learn the\nrepresentations of events. Finally, a pair scorer calculates the similarity\nbetween each pair of events and co-referred events can be recognized using\nstandard clustering algorithm. Additionally, as the existing cross-document\nevent coreference datasets are limited to English, we have developed a\nlarge-scale Chinese cross-document event coreference dataset to fill this gap,\nwhich comprises 53,066 event mentions and 4,476 clusters. After applying our\nmodel on the English and Chinese datasets respectively, it outperforms all\nbaselines by large margins.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15990v1",
    "published_date": "2024-06-23 02:54:48 UTC",
    "updated_date": "2024-06-23 02:54:48 UTC"
  },
  {
    "arxiv_id": "2406.15985v1",
    "title": "Deep-MPC: A DAGGER-Driven Imitation Learning Strategy for Optimal Constrained Battery Charging",
    "authors": [
      "Jorge Espin",
      "Dong Zhang",
      "Daniele Toti",
      "Andrea Pozzi"
    ],
    "abstract": "In the realm of battery charging, several complex aspects demand meticulous\nattention, including thermal management, capacity degradation, and the need for\nrapid charging while maintaining safety and battery lifespan. By employing the\nimitation learning paradigm, this manuscript introduces an innovative solution\nto confront the inherent challenges often associated with conventional\npredictive control strategies for constrained battery charging. A significant\ncontribution of this study lies in the adaptation of the Dataset Aggregation\n(DAGGER) algorithm to address scenarios where battery parameters are uncertain,\nand internal states are unobservable. Results drawn from a practical battery\nsimulator that incorporates an electrochemical model highlight substantial\nimprovements in battery charging performance, particularly in meeting all\nsafety constraints and outperforming traditional strategies in computational\nprocessing.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "7 pages, 4 figures, submitted to American Control Conference 2024\n  (ACC2024)",
    "pdf_url": "http://arxiv.org/pdf/2406.15985v1",
    "published_date": "2024-06-23 02:36:02 UTC",
    "updated_date": "2024-06-23 02:36:02 UTC"
  },
  {
    "arxiv_id": "2406.15982v1",
    "title": "Learning with Noisy Ground Truth: From 2D Classification to 3D Reconstruction",
    "authors": [
      "Yangdi Lu",
      "Wenbo He"
    ],
    "abstract": "Deep neural networks has been highly successful in data-intense computer\nvision applications, while such success relies heavily on the massive and clean\ndata. In real-world scenarios, clean data sometimes is difficult to obtain. For\nexample, in image classification and segmentation tasks, precise annotations of\nmillions samples are generally very expensive and time-consuming. In 3D static\nscene reconstruction task, most NeRF related methods require the foundational\nassumption of the static scene (e.g. consistent lighting condition and\npersistent object positions), which is often violated in real-world scenarios.\nTo address these problem, learning with noisy ground truth (LNGT) has emerged\nas an effective learning method and shows great potential. In this short\nsurvey, we propose a formal definition unify the analysis of LNGT LNGT in the\ncontext of different machine learning tasks (classification and regression).\nBased on this definition, we propose a novel taxonomy to classify the existing\nwork according to the error decomposition with the fundamental definition of\nmachine learning. Further, we provide in-depth analysis on memorization effect\nand insightful discussion about potential future research opportunities from 2D\nclassification to 3D reconstruction, in the hope of providing guidance to\nfollow-up research.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Computer vision, Noisy Labels, 3D reconstruction, 3D Gaussian Splats,\n  (Work still in progress)",
    "pdf_url": "http://arxiv.org/pdf/2406.15982v1",
    "published_date": "2024-06-23 02:21:48 UTC",
    "updated_date": "2024-06-23 02:21:48 UTC"
  },
  {
    "arxiv_id": "2406.16975v1",
    "title": "A Review of Global Sensitivity Analysis Methods and a comparative case study on Digit Classification",
    "authors": [
      "Zahra Sadeghi",
      "Stan Matwin"
    ],
    "abstract": "Global sensitivity analysis (GSA) aims to detect influential input factors\nthat lead a model to arrive at a certain decision and is a significant approach\nfor mitigating the computational burden of processing high dimensional data. In\nthis paper, we provide a comprehensive review and a comparison on global\nsensitivity analysis methods. Additionally, we propose a methodology for\nevaluating the efficacy of these methods by conducting a case study on MNIST\ndigit dataset. Our study goes through the underlying mechanism of widely used\nGSA methods and highlights their efficacy through a comprehensive methodology.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16975v1",
    "published_date": "2024-06-23 00:38:19 UTC",
    "updated_date": "2024-06-23 00:38:19 UTC"
  },
  {
    "arxiv_id": "2406.15970v1",
    "title": "Imperfect-Recall Games: Equilibrium Concepts and Their Complexity",
    "authors": [
      "Emanuel Tewolde",
      "Brian Hu Zhang",
      "Caspar Oesterheld",
      "Manolis Zampetakis",
      "Tuomas Sandholm",
      "Paul W. Goldberg",
      "Vincent Conitzer"
    ],
    "abstract": "We investigate optimal decision making under imperfect recall, that is, when\nan agent forgets information it once held before. An example is the\nabsentminded driver game, as well as team games in which the members have\nlimited communication capabilities. In the framework of extensive-form games\nwith imperfect recall, we analyze the computational complexities of finding\nequilibria in multiplayer settings across three different solution concepts:\nNash, multiselves based on evidential decision theory (EDT), and multiselves\nbased on causal decision theory (CDT). We are interested in both exact and\napproximate solution computation. As special cases, we consider (1)\nsingle-player games, (2) two-player zero-sum games and relationships to maximin\nvalues, and (3) games without exogenous stochasticity (chance nodes). We relate\nthese problems to the complexity classes P, PPAD, PLS, $\\Sigma_2^P$ ,\n$\\exists$R, and $\\exists \\forall$R.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.CC",
      "91A05, 91A06, 91A10, 91A11, 91A18, 91A35, 91A68, 68T37, 68Q17, 68Q25",
      "I.2; J.4; F.2"
    ],
    "primary_category": "cs.GT",
    "comment": "Long version of the paper that got accepted to the Thirty-Third\n  International Joint Conference on Artificial Intelligence (IJCAI 2024). 35\n  pages, 10 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2406.15970v1",
    "published_date": "2024-06-23 00:27:28 UTC",
    "updated_date": "2024-06-23 00:27:28 UTC"
  },
  {
    "arxiv_id": "2406.15966v1",
    "title": "Evaluating the Effectiveness of the Foundational Models for Q&A Classification in Mental Health care",
    "authors": [
      "Hassan Alhuzali",
      "Ashwag Alasmari"
    ],
    "abstract": "Pre-trained Language Models (PLMs) have the potential to transform mental\nhealth support by providing accessible and culturally sensitive resources.\nHowever, despite this potential, their effectiveness in mental health care and\nspecifically for the Arabic language has not been extensively explored. To\nbridge this gap, this study evaluates the effectiveness of foundational models\nfor classification of Questions and Answers (Q&A) in the domain of mental\nhealth care. We leverage the MentalQA dataset, an Arabic collection featuring\nQ&A interactions related to mental health. In this study, we conducted\nexperiments using four different types of learning approaches: traditional\nfeature extraction, PLMs as feature extractors, Fine-tuning PLMs and prompting\nlarge language models (GPT-3.5 and GPT-4) in zero-shot and few-shot learning\nsettings. While traditional feature extractors combined with Support Vector\nMachines (SVM) showed promising performance, PLMs exhibited even better results\ndue to their ability to capture semantic meaning. For example, MARBERT achieved\nthe highest performance with a Jaccard Score of 0.80 for question\nclassification and a Jaccard Score of 0.86 for answer classification. We\nfurther conducted an in-depth analysis including examining the effects of\nfine-tuning versus non-fine-tuning, the impact of varying data size, and\nconducting error analysis. Our analysis demonstrates that fine-tuning proved to\nbe beneficial for enhancing the performance of PLMs, and the size of the\ntraining data played a crucial role in achieving high performance. We also\nexplored prompting, where few-shot learning with GPT-3.5 yielded promising\nresults. There was an improvement of 12% for question and classification and\n45% for answer classification. Based on our findings, it can be concluded that\nPLMs and prompt-based approaches hold promise for mental health support in\nArabic.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15966v1",
    "published_date": "2024-06-23 00:11:07 UTC",
    "updated_date": "2024-06-23 00:11:07 UTC"
  }
]
{
  "date": "2024-01-12",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-01-12 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 75 篇论文，主要聚焦于 AI 模型的安全性、生物医学应用、多模态学习和知识蒸馏等领域，亮点包括 LLM 的越狱攻击防御（如基于人类化视角的 jailbreaking 研究）和健康预测模型（如 Health-LLM），同时有知名学者如 Peter Clark 和 Maarten Sap 的参与，强调了 AI 在实际应用中的鲁棒性和可解释性。\n\n### 重点论文讨论\n我将优先选取具有话题度、创新性和潜在影响的论文进行简要分析，将相关主题归类讨论，并对其他次要论文快速掠过。以下按主题分组，突出核心贡献。\n\n#### AI 安全与 LLM 应用（高话题度领域）\n这些论文探讨了大型语言模型（LLMs）的安全性和实际应用，涉及 jailbreaking 和知识蒸馏，值得关注。\n- **How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs（如何说服 LLMs 进行越狱攻击：通过人类化视角挑战 AI 安全）**  \n  作者包括 Kaitlyn Zhou 和 Maarten Sap。主要贡献：提出一种基于说服理论的 jailbreaking 方法，使用自然语言推理攻击 LLMs（如 GPT-4），发现攻击成功率高达 92%，并讨论了防御策略。该发现揭示了 LLM 在人类交互中的脆弱性，强调了 AI 安全设计的必要性。\n  \n- **Relying on the Unreliable: The Impact of Language Models' Reluctance to Express Uncertainty（依赖不可靠的模型：LLMs 不愿表达不确定性的影响）**  \n  作者：Kaitlyn Zhou 等。主要发现：LLMs 如 GPT-4 在回答问题时倾向于过度自信，导致错误率高达 47%，用户易受其影响。该研究呼吁在 LLM 后训练中融入不确定性偏好，提高 AI 在决策中的可靠性。\n\n- **LLM-Assisted Crisis Management: Building Advanced LLM Platforms for Effective Emergency Response and Public Collaboration（LLM 辅助危机管理：构建先进的 LLM 平台用于紧急响应和公众协作）**  \n  作者：Hakan T. Otal 等。主要贡献：使用开源 LLM（如 LLAMA2）从社交媒体提取紧急信息，提供实时指导和通知工作流，提升危机响应效率。该方法在实际应用中展示了 LLM 的潜力，但需关注隐私问题。\n\n其他 LLM 相关论文，如 \"Enhancing Emotional Generation Capability of Large Language Models via Emotional Chain-of-Thought\"，通过情感链式推理提升 LLM 的情感生成能力，贡献在于引入情感智能评估框架，但细节较常规，快速掠过。\n\n#### 生物医学与健康预测（应用导向，令人印象深刻）\n这些论文将 AI 应用于医疗领域，解决数据稀缺和隐私问题。\n- **BioDiffusion: A Versatile Diffusion Model for Biomedical Signal Synthesis（BioDiffusion：一种多变量生物医学信号合成的扩散模型）**  \n  作者：Xiaomin Li 等。主要贡献：提出 BioDiffusion 模型，用于生成高保真生物医学信号，解决数据不足和噪声干扰问题。通过定量评估，该模型在无条件和条件生成任务中优于现有时间序列模型，提升了机器学习在生物医学中的准确性。\n\n- **Health-LLM: Large Language Models for Health Prediction via Wearable Sensor Data（Health-LLM：基于可穿戴传感器数据的 LLM 健康预测）**  \n  作者：Yubin Kim 等。主要发现：使用 LLM（如 GPT-4）结合用户上下文和生理数据（如心率），在 10 个健康任务中实现最佳性能，细调模型 HealthAlpaca 提升了预测准确性。该研究突出了 LLM 在医疗领域的潜力，但需优化上下文增强策略。\n\n其他健康相关论文，如 \"A Brain-inspired Computational Model for Human-like Concept Learning\"，模仿大脑学习概念，但影响力较小，快速提及其在认知建模的创新。\n\n#### 多模态学习与计算机视觉（技术创新领域）\n这些论文关注图像和数据处理，相关性强。\n- **DocFinQA: A Long-Context Financial Reasoning Dataset（DocFinQA：一个长上下文金融推理数据集）**  \n  作者：Varshini Reddy 等。主要贡献：扩展 FinQA 数据集，处理长文档（平均 12 万词），评估 LLM 在金融 QA 中的性能。发现现有模型在长文档上表现差强人意，强调了长范围上下文在领域应用的挑战。\n\n- **Synthetic Data Generation Framework for Pedestrian Intention Prediction（合成数据生成框架用于行人意图预测）**  \n  作者：Muhammad Naveed Riaz 等。主要发现：提出 ARCANE 框架和 PedSynth 数据集，用于生成行人视频数据，提升自动驾驶中意图预测的准确性。新模型 PedGNN 基于 GNN-GRU 架构，实现了高效部署。\n\n其他视觉论文，如 \"Seg-metrics: a Python Package to Compute Segmentation Metrics\"，提供了一个图像分割评估工具包，贡献在于标准化评估，但实用性强于学术创新，快速掠过。\n\n#### 其他领域（快速概述）\n剩余论文涉及强化学习、金融、图神经网络等领域，但影响力较小，仅快速总结。\n- **Modeling Latent Selection with Structural Causal Models（使用结构因果模型建模潜在选择）**：作者：Leihao Chen 等。贡献：引入条件操作处理选择偏差，提升因果推理的鲁棒性。\n- **Adaptive Data Augmentation for Aspect Sentiment Quad Prediction（自适应数据增强用于方面情感四元组预测）**：作者：Wenyuan Zhang 等。发现：通过自适应增强缓解数据不平衡，提升情感分析性能。\n- 其他如强化学习论文（e.g., \"A hierarchical control framework\"）和图网络（e.g., \"Graph Relation Distillation\"），均展示了技术改进，但未有突破性发现，故从简。\n\n总之，今天的 arXiv 论文突出了 AI 模型在安全和应用中的挑战与机遇，LLM 相关研究尤其值得跟踪。未来几天，关注这些领域的进展，以推动更可靠的 AI 系统。TLDR 快报到此结束，欢迎反馈！",
  "papers": [
    {
      "arxiv_id": "2401.10282v2",
      "title": "BioDiffusion: A Versatile Diffusion Model for Biomedical Signal Synthesis",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaomin Li",
        "Mykhailo Sakevych",
        "Gentry Atkinson",
        "Vangelis Metsis"
      ],
      "abstract": "Machine learning tasks involving biomedical signals frequently grapple with\nissues such as limited data availability, imbalanced datasets, labeling\ncomplexities, and the interference of measurement noise. These challenges often\nhinder the optimal training of machine learning algorithms. Addressing these\nconcerns, we introduce BioDiffusion, a diffusion-based probabilistic model\noptimized for the synthesis of multivariate biomedical signals. BioDiffusion\ndemonstrates excellence in producing high-fidelity, non-stationary,\nmultivariate signals for a range of tasks including unconditional,\nlabel-conditional, and signal-conditional generation. Leveraging these\nsynthesized signals offers a notable solution to the aforementioned challenges.\nOur research encompasses both qualitative and quantitative assessments of the\nsynthesized data quality, underscoring its capacity to bolster accuracy in\nmachine learning tasks tied to biomedical signals. Furthermore, when juxtaposed\nwith current leading time-series generative models, empirical evidence suggests\nthat BioDiffusion outperforms them in biomedical signal generation quality.",
      "tldr_zh": "本研究针对机器学习在生物医学信号处理中面临的挑战，如数据有限、不平衡数据集、标注复杂性和测量噪声干扰，提出了BioDiffusion，这是一个基于diffusion模型的概率模型，用于合成多变量生物医学信号。BioDiffusion支持无条件、label-conditional和signal-conditional生成，能够产生高保真、非平稳信号，从而缓解这些问题。实验结果显示，使用合成信号可提升机器学习任务的准确性，且BioDiffusion在生成质量上优于现有领先的时间序列生成模型。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.10282v2",
      "published_date": "2024-01-12 23:52:44 UTC",
      "updated_date": "2024-01-27 17:44:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:21:20.518948"
    },
    {
      "arxiv_id": "2401.06925v2",
      "title": "Modeling Latent Selection with Structural Causal Models",
      "title_zh": "翻译失败",
      "authors": [
        "Leihao Chen",
        "Onno Zoeter",
        "Joris M. Mooij"
      ],
      "abstract": "Selection bias is ubiquitous in real-world data, and can lead to misleading\nresults if not dealt with properly. We introduce a conditioning operation on\nStructural Causal Models (SCMs) to model latent selection from a causal\nperspective. We show that the conditioning operation transforms an SCM with the\npresence of an explicit latent selection mechanism into an SCM without such\nselection mechanism, which partially encodes the causal semantics of the\nselected subpopulation according to the original SCM. Furthermore, we show that\nthis conditioning operation preserves the simplicity, acyclicity, and linearity\nof SCMs, and commutes with marginalization. Thanks to these properties,\ncombined with marginalization and intervention, the conditioning operation\noffers a valuable tool for conducting causal reasoning tasks within causal\nmodels where latent details have been abstracted away. We demonstrate by\nexample how classical results of causal inference can be generalized to include\nselection bias and how the conditioning operation helps with modeling of\nreal-world problems.",
      "tldr_zh": "本文提出了一种在 Structural Causal Models (SCMs) 上进行条件操作的方法，用于从因果视角建模潜在选择偏差 (latent selection)，以避免数据偏差导致的误导性结果。该操作可以将带有显式选择机制的 SCM 转化为无选择机制的 SCM，同时保留部分因果语义，并保持 SCM 的简单性、无环性和线性性，以及与 marginalization 操作的兼容性。通过结合 marginalization 和 intervention，该方法便于在抽象了潜在细节的因果模型中进行推理任务，并展示了如何推广经典因果推理结果到包括选择偏差的现实问题建模。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "math.ST",
        "stat.ME",
        "stat.ML",
        "stat.TH"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06925v2",
      "published_date": "2024-01-12 23:14:34 UTC",
      "updated_date": "2024-08-01 08:51:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:21:34.318404"
    },
    {
      "arxiv_id": "2401.06922v1",
      "title": "Open RAN LSTM Traffic Prediction and Slice Management using Deep Reinforcement Learning",
      "title_zh": "Open RAN LSTM 流量预测和切片管理使用深度强化学习",
      "authors": [
        "Fatemeh Lotfi",
        "Fatemeh Afghah"
      ],
      "abstract": "With emerging applications such as autonomous driving, smart cities, and\nsmart factories, network slicing has become an essential component of 5G and\nbeyond networks as a means of catering to a service-aware network. However,\nmanaging different network slices while maintaining quality of services (QoS)\nis a challenge in a dynamic environment. To address this issue, this paper\nleverages the heterogeneous experiences of distributed units (DUs) in ORAN\nsystems and introduces a novel approach to ORAN slicing xApp using distributed\ndeep reinforcement learning (DDRL). Additionally, to enhance the\ndecision-making performance of the RL agent, a prediction rApp based on long\nshort-term memory (LSTM) is incorporated to provide additional information from\nthe dynamic environment to the xApp. Simulation results demonstrate significant\nimprovements in network performance, particularly in reducing QoS violations.\nThis emphasizes the importance of using the prediction rApp and distributed\nactors' information jointly as part of a dynamic xApp.",
      "tldr_zh": "该论文针对5G及后续网络中网络切片的动态管理挑战，提出了一种基于分布式深度强化学习(DDRL)的Open RAN slicing xApp方法，利用分布式单位(DUs)的异构经验来维持服务质量(QoS)。为了提升RL代理的决策性能，作者整合了基于长短时记忆(LSTM)的预测rApp，提供动态环境信息以辅助xApp决策。模拟结果显示，该方法显著改善了网络性能，特别是减少了QoS违规事件，强调了预测rApp与分布式参与者信息相结合的重要性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NI",
        "cs.SY",
        "eess.SY",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to publish in the IEEE Asilomar Conference on Signals,\n  Systems, and Computers, 2023",
      "pdf_url": "http://arxiv.org/pdf/2401.06922v1",
      "published_date": "2024-01-12 22:43:07 UTC",
      "updated_date": "2024-01-12 22:43:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:21:45.721936"
    },
    {
      "arxiv_id": "2401.06915v2",
      "title": "DocFinQA: A Long-Context Financial Reasoning Dataset",
      "title_zh": "翻译失败",
      "authors": [
        "Varshini Reddy",
        "Rik Koncel-Kedziorski",
        "Viet Dac Lai",
        "Michael Krumdick",
        "Charles Lovering",
        "Chris Tanner"
      ],
      "abstract": "For large language models (LLMs) to be effective in the financial domain --\nwhere each decision can have a significant impact -- it is necessary to\ninvestigate realistic tasks and data. Financial professionals often interact\nwith documents that are hundreds of pages long, but most financial research\ndatasets only deal with short excerpts from these documents. To address this,\nwe introduce a long-document financial QA task. We augment 7,437 questions from\nthe existing FinQA dataset with the full-document context, extending the\naverage context length from under 700 words in FinQA to 123k words in DocFinQA.\nWe conduct extensive experiments over retrieval-based QA pipelines and\nlong-context language models. DocFinQA proves a significant challenge for even\nstate-of-the-art systems. We also provide a case-study on the longest documents\nin DocFinQA and find that models particularly struggle on these documents.\nAddressing these challenges may have a wide reaching impact across applications\nwhere specificity and long-range contexts are critical, like gene sequences and\nlegal document contract analysis.",
      "tldr_zh": "这篇论文引入了 DocFinQA，这是一个针对金融领域的长上下文推理数据集，旨在解决现有数据集只处理短文档片段的问题。研究者基于 FinQA 数据集扩展了 7,437 个问题，将完整文档上下文添加到每个问题中，将平均上下文长度从不到 700 词增加到 123k 词。实验结果显示，即使是先进的检索-based QA 管道和长上下文语言模型在 DocFinQA 上面临重大挑战，尤其在处理最长文档时表现欠佳；解决这些问题可能对其他需要精确长范围上下文的应用，如基因序列和法律文档分析，产生广泛影响。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "13 pages",
      "pdf_url": "http://arxiv.org/pdf/2401.06915v2",
      "published_date": "2024-01-12 22:19:22 UTC",
      "updated_date": "2024-02-29 19:55:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:21:58.237649"
    },
    {
      "arxiv_id": "2401.06883v1",
      "title": "Scaling While Privacy Preserving: A Comprehensive Synthetic Tabular Data Generation and Evaluation in Learning Analytics",
      "title_zh": "翻译失败",
      "authors": [
        "Qinyi Liu",
        "Mohammad Khalil",
        "Ronas Shakya",
        "Jelena Jovanovic"
      ],
      "abstract": "Privacy poses a significant obstacle to the progress of learning analytics\n(LA), presenting challenges like inadequate anonymization and data misuse that\ncurrent solutions struggle to address. Synthetic data emerges as a potential\nremedy, offering robust privacy protection. However, prior LA research on\nsynthetic data lacks thorough evaluation, essential for assessing the delicate\nbalance between privacy and data utility. Synthetic data must not only enhance\nprivacy but also remain practical for data analytics. Moreover, diverse LA\nscenarios come with varying privacy and utility needs, making the selection of\nan appropriate synthetic data approach a pressing challenge. To address these\ngaps, we propose a comprehensive evaluation of synthetic data, which\nencompasses three dimensions of synthetic data quality, namely resemblance,\nutility, and privacy. We apply this evaluation to three distinct LA datasets,\nusing three different synthetic data generation methods. Our results show that\nsynthetic data can maintain similar utility (i.e., predictive performance) as\nreal data, while preserving privacy. Furthermore, considering different privacy\nand data utility requirements in different LA scenarios, we make customized\nrecommendations for synthetic data generation. This paper not only presents a\ncomprehensive evaluation of synthetic data but also illustrates its potential\nin mitigating privacy concerns within the field of LA, thus contributing to a\nwider application of synthetic data in LA and promoting a better practice for\nopen science.",
      "tldr_zh": "该论文探讨了在学习分析（LA）领域中，隐私保护对数据利用的挑战，并提出合成数据作为解决方案。研究者开发了一个全面评估框架，涵盖合成数据的三个维度：resemblance（相似性）、utility（效用）和privacy（隐私），并将其应用于三个LA数据集和三种合成数据生成方法。结果显示，合成数据能够维持与真实数据相似的预测性能，同时有效保护隐私。论文根据不同LA场景的隐私和效用需求，提供定制推荐，从而促进合成数据在LA中的广泛应用，并支持开放科学实践。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06883v1",
      "published_date": "2024-01-12 20:27:55 UTC",
      "updated_date": "2024-01-12 20:27:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:22:09.623702"
    },
    {
      "arxiv_id": "2401.06868v1",
      "title": "Multicriteria decision support employing adaptive prediction in a tensor-based feature representation",
      "title_zh": "翻译失败",
      "authors": [
        "Betania Silva Carneiro Campello",
        "Leonardo Tomazeli Duarte",
        "João Marcos Travassos Romano"
      ],
      "abstract": "Multicriteria decision analysis (MCDA) is a widely used tool to support\ndecisions in which a set of alternatives should be ranked or classified based\non multiple criteria. Recent studies in MCDA have shown the relevance of\nconsidering not only current evaluations of each criterion but also past data.\nPast-data-based approaches carry new challenges, especially in time-varying\nenvironments. This study deals with this challenge via essential tools of\nsignal processing, such as tensorial representations and adaptive prediction.\nMore specifically, we structure the criteria' past data as a tensor and, by\napplying adaptive prediction, we compose signals with these prediction values\nof the criteria. Besides, we transform the prediction in the time domain into a\nmost favorable decision making domain, called the feature domain. We present a\nnovel extension of the MCDA method PROMETHEE II, aimed at addressing the tensor\nin the feature domain to obtain a ranking of alternatives. Numerical\nexperiments were performed using real-world time series, and our approach is\ncompared with other existing strategies. The results highlight the relevance\nand efficiency of our proposal, especially for nonstationary time series.",
      "tldr_zh": "该研究提出了一种基于张量表示和自适应预测的多标准决策分析(MCDA)方法，以处理时间变化环境下的决策挑战。方法将标准的过去数据结构化为张量，并通过自适应预测生成信号，然后将这些预测从时间域转换为特征域。基于此，他们扩展了PROMETHEE II方法，在特征域中使用张量来对备选方案进行排名。实验结果显示，该方法在真实世界非平稳时间序列上表现出色，与现有策略相比具有更高的效率和相关性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06868v1",
      "published_date": "2024-01-12 19:46:29 UTC",
      "updated_date": "2024-01-12 19:46:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:22:22.055521"
    },
    {
      "arxiv_id": "2401.06866v2",
      "title": "Health-LLM: Large Language Models for Health Prediction via Wearable Sensor Data",
      "title_zh": "翻译失败",
      "authors": [
        "Yubin Kim",
        "Xuhai Xu",
        "Daniel McDuff",
        "Cynthia Breazeal",
        "Hae Won Park"
      ],
      "abstract": "Large language models (LLMs) are capable of many natural language tasks, yet\nthey are far from perfect. In health applications, grounding and interpreting\ndomain-specific and non-linguistic data is crucial. This paper investigates the\ncapacity of LLMs to make inferences about health based on contextual\ninformation (e.g. user demographics, health knowledge) and physiological data\n(e.g. resting heart rate, sleep minutes). We present a comprehensive evaluation\nof 12 state-of-the-art LLMs with prompting and fine-tuning techniques on four\npublic health datasets (PMData, LifeSnaps, GLOBEM and AW_FB). Our experiments\ncover 10 consumer health prediction tasks in mental health, activity,\nmetabolic, and sleep assessment. Our fine-tuned model, HealthAlpaca exhibits\ncomparable performance to much larger models (GPT-3.5, GPT-4 and Gemini-Pro),\nachieving the best performance in 8 out of 10 tasks. Ablation studies highlight\nthe effectiveness of context enhancement strategies. Notably, we observe that\nour context enhancement can yield up to 23.8% improvement in performance. While\nconstructing contextually rich prompts (combining user context, health\nknowledge and temporal information) exhibits synergistic improvement, the\ninclusion of health knowledge context in prompts significantly enhances overall\nperformance.",
      "tldr_zh": "本研究探讨了Large Language Models (LLMs) 在利用可穿戴传感器数据（如生理指标）和上下文信息（如用户人口统计和健康知识）进行健康预测的能力，评估了12个先进LLMs 通过prompting和fine-tuning技术在四个公共数据集（PMData、LifeSnaps、GLOBEM 和 AW_FB）上的表现。  \n开发的fine-tuned模型HealthAlpaca 在10个消费者健康预测任务（包括心理健康、活动、代谢和睡眠评估）中表现出色，在8个任务中优于更大模型如GPT-3.5、GPT-4 和 Gemini-Pro。  \nAblation studies 表明，上下文增强策略（如结合用户上下文、健康知识和时间信息）可带来高达23.8%的性能提升，特别是加入健康知识上下文显著提升整体效果。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06866v2",
      "published_date": "2024-01-12 19:40:11 UTC",
      "updated_date": "2024-04-27 06:20:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:22:37.209103"
    },
    {
      "arxiv_id": "2401.06757v2",
      "title": "Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction",
      "title_zh": "合成数据生成框架、数据集和高效深度模型，用于行人意图预测",
      "authors": [
        "Muhammad Naveed Riaz",
        "Maciej Wielgosz",
        "Abel Garcia Romera",
        "Antonio M. Lopez"
      ],
      "abstract": "Pedestrian intention prediction is crucial for autonomous driving. In\nparticular, knowing if pedestrians are going to cross in front of the\nego-vehicle is core to performing safe and comfortable maneuvers. Creating\naccurate and fast models that predict such intentions from sequential images is\nchallenging. A factor contributing to this is the lack of datasets with diverse\ncrossing and non-crossing (C/NC) scenarios. We address this scarceness by\nintroducing a framework, named ARCANE, which allows programmatically generating\nsynthetic datasets consisting of C/NC video clip samples. As an example, we use\nARCANE to generate a large and diverse dataset named PedSynth. We will show how\nPedSynth complements widely used real-world datasets such as JAAD and PIE, so\nenabling more accurate models for C/NC prediction. Considering the onboard\ndeployment of C/NC prediction models, we also propose a deep model named\nPedGNN, which is fast and has a very low memory footprint. PedGNN is based on a\nGNN-GRU architecture that takes a sequence of pedestrian skeletons as input to\npredict crossing intentions.",
      "tldr_zh": "这篇论文针对自动驾驶中的行人意图预测问题，提出了一种合成数据生成框架 ARCANE，用于程序化创建多样化的横穿和不横穿（C/NC）场景视频数据集。作者使用 ARCANE 生成了 PedSynth 数据集，并展示了它如何补充现有真实数据集如 JAAD 和 PIE，从而提升意图预测模型的准确性。同时，论文引入了高效深度模型 PedGNN，该模型基于 GNN-GRU 架构，以行人骨骼序列作为输入，实现快速预测并显著降低内存占用。实验结果表明，这种方法有助于构建更可靠的自动驾驶系统。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06757v2",
      "published_date": "2024-01-12 18:44:01 UTC",
      "updated_date": "2024-06-15 13:44:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:22:49.222826"
    },
    {
      "arxiv_id": "2401.06751v2",
      "title": "The Unreasonable Effectiveness of Easy Training Data for Hard Tasks",
      "title_zh": "易于处理的训练数据在困难任务上的非理性有效性",
      "authors": [
        "Peter Hase",
        "Mohit Bansal",
        "Peter Clark",
        "Sarah Wiegreffe"
      ],
      "abstract": "How can we train models to perform well on hard test data when hard training\ndata is by definition difficult to label correctly? This question has been\ntermed the scalable oversight problem and has drawn increasing attention as\nlanguage models have continually improved. In this paper, we present the\nsurprising conclusion that current pretrained language models often generalize\nrelatively well from easy to hard data, even performing as well as oracle\nmodels finetuned on hard data. We demonstrate this kind of easy-to-hard\ngeneralization using simple finetuning methods like in-context learning, linear\nclassifier heads, and QLoRA for seven different measures of datapoint hardness,\nincluding six empirically diverse human hardness measures (like grade level)\nand one model-based measure (loss-based). Furthermore, we show that even if one\ncares most about model performance on hard data, it can be better to collect\neasy data rather than hard data for finetuning, since hard data is generally\nnoisier and costlier to collect. Our experiments use open models up to 70b in\nsize and four publicly available question-answering datasets with questions\nranging in difficulty from 3rd grade science questions to college level STEM\nquestions and general-knowledge trivia. We conclude that easy-to-hard\ngeneralization in LMs is surprisingly strong for the tasks studied. Our code is\navailable at: https://github.com/allenai/easy-to-hard-generalization",
      "tldr_zh": "本研究探讨了使用简单训练数据来训练模型，使其在困难测试任务上表现出色的现象，挑战了传统观点，即困难数据是必需的。该论文发现，当前预训练 language models 往往能从简单数据有效泛化到困难数据，甚至与使用困难数据的 oracle 模型相当。实验通过 in-context learning、linear classifier heads 和 QLoRA 等简单微调方法，验证了七种数据难度度量（包括人类评估如年级水平和模型-based 损失度量）。结果显示，即使关注模型在困难数据上的性能，收集简单数据可能更优，因为困难数据更易出错且成本更高。该发现基于公开数据集和模型，强调了 language models 在任务泛化方面的强大潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2024. 23 pages, 20 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.06751v2",
      "published_date": "2024-01-12 18:36:29 UTC",
      "updated_date": "2024-06-05 14:10:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:22:58.232741"
    },
    {
      "arxiv_id": "2401.06742v1",
      "title": "Using Natural Language Inference to Improve Persona Extraction from Dialogue in a New Domain",
      "title_zh": "利用自然语言推理提升从新领域对话中提取人物特征",
      "authors": [
        "Alexandra DeLucia",
        "Mengjie Zhao",
        "Yoshinori Maeda",
        "Makoto Yoda",
        "Keiichi Yamada",
        "Hiromi Wakaki"
      ],
      "abstract": "While valuable datasets such as PersonaChat provide a foundation for training\npersona-grounded dialogue agents, they lack diversity in conversational and\nnarrative settings, primarily existing in the \"real\" world. To develop dialogue\nagents with unique personas, models are trained to converse given a specific\npersona, but hand-crafting these persona can be time-consuming, thus methods\nexist to automatically extract persona information from existing\ncharacter-specific dialogue. However, these persona-extraction models are also\ntrained on datasets derived from PersonaChat and struggle to provide\nhigh-quality persona information from conversational settings that do not take\nplace in the real world, such as the fantasy-focused dataset, LIGHT. Creating\nnew data to train models on a specific setting is human-intensive, thus\nprohibitively expensive. To address both these issues, we introduce a natural\nlanguage inference method for post-hoc adapting a trained persona extraction\nmodel to a new setting. We draw inspiration from the literature of dialog\nnatural language inference (NLI), and devise NLI-reranking methods to extract\nstructured persona information from dialogue. Compared to existing persona\nextraction models, our method returns higher-quality extracted persona and\nrequires less human annotation.",
      "tldr_zh": "该论文针对现有角色提取模型在新领域（如幻想主题的LIGHT数据集）中表现不佳的问题，提出了一种基于Natural Language Inference (NLI)的后处理适应方法。该方法借鉴对话NLI文献，设计了NLI重排序技术，从对话中自动提取结构化的角色信息，从而减少了对新数据集的人工标注需求。与现有模型相比，该方法显著提高了提取角色的质量和准确性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Code and models will be released upon publication",
      "pdf_url": "http://arxiv.org/pdf/2401.06742v1",
      "published_date": "2024-01-12 18:25:03 UTC",
      "updated_date": "2024-01-12 18:25:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:23:11.222988"
    },
    {
      "arxiv_id": "2401.06730v2",
      "title": "Relying on the Unreliable: The Impact of Language Models' Reluctance to Express Uncertainty",
      "title_zh": "翻译失败",
      "authors": [
        "Kaitlyn Zhou",
        "Jena D. Hwang",
        "Xiang Ren",
        "Maarten Sap"
      ],
      "abstract": "As natural language becomes the default interface for human-AI interaction,\nthere is a need for LMs to appropriately communicate uncertainties in\ndownstream applications. In this work, we investigate how LMs incorporate\nconfidence in responses via natural language and how downstream users behave in\nresponse to LM-articulated uncertainties. We examine publicly deployed models\nand find that LMs are reluctant to express uncertainties when answering\nquestions even when they produce incorrect responses. LMs can be explicitly\nprompted to express confidences, but tend to be overconfident, resulting in\nhigh error rates (an average of 47%) among confident responses. We test the\nrisks of LM overconfidence by conducting human experiments and show that users\nrely heavily on LM generations, whether or not they are marked by certainty.\nLastly, we investigate the preference-annotated datasets used in post training\nalignment and find that humans are biased against texts with uncertainty. Our\nwork highlights new safety harms facing human-LM interactions and proposes\ndesign recommendations and mitigating strategies moving forward.",
      "tldr_zh": "本研究探讨了语言模型 (LMs) 在表达不确定性方面的不足及其对人机交互的影响。研究发现，LMs 即使给出错误响应时也倾向于不愿表达不确定性，而通过提示强制表达信心时，往往过度自信，导致自信响应的错误率高达平均 47%。通过人类实验，作者证明用户过度依赖 LMs 的输出，无论是否标注确定性，并分析后训练数据集显示人类偏好避免不确定性的文本。该工作强调了 LMs 过度自信的安全风险，并提出设计推荐和缓解策略，以提升人机交互的安全性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2024 (Camera Ready)",
      "pdf_url": "http://arxiv.org/pdf/2401.06730v2",
      "published_date": "2024-01-12 18:03:30 UTC",
      "updated_date": "2024-07-09 23:53:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:23:24.845892"
    },
    {
      "arxiv_id": "2402.10908v1",
      "title": "LLM-Assisted Crisis Management: Building Advanced LLM Platforms for Effective Emergency Response and Public Collaboration",
      "title_zh": "LLM 辅助的危机管理：构建先进的 LLM 平台用于有效的紧急响应和公众协作",
      "authors": [
        "Hakan T. Otal",
        "M. Abdullah Canbaz"
      ],
      "abstract": "Emergencies and critical incidents often unfold rapidly, necessitating a\nswift and effective response. In this research, we introduce a novel approach\nto identify and classify emergency situations from social media posts and\ndirect emergency messages using an open source Large Language Model, LLAMA2.\nThe goal is to harness the power of natural language processing and machine\nlearning to assist public safety telecommunicators and huge crowds during\ncountrywide emergencies. Our research focuses on developing a language model\nthat can understand users describe their situation in the 911 call, enabling\nLLAMA2 to analyze the content and offer relevant instructions to the\ntelecommunicator, while also creating workflows to notify government agencies\nwith the caller's information when necessary. Another benefit this language\nmodel provides is its ability to assist people during a significant emergency\nincident when the 911 system is overwhelmed, by assisting the users with simple\ninstructions and informing authorities with their location and emergency\ninformation.",
      "tldr_zh": "本研究提出了一种利用开源大型语言模型(LLAMA2)辅助危机管理的平台，旨在从社交媒体帖子和直接紧急消息中识别和分类紧急情况，以提升公共安全响应效率。\n该平台结合自然语言处理(NLP)和机器学习技术，分析911呼叫内容，理解用户描述，提供相关指令，并创建工作流自动通知政府机构。\n此外，当911系统过载时，平台能直接辅助用户，提供简单指令并传输位置和紧急信息，促进公众协作和可信赖的紧急响应。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "68T50 68T50 68T50",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.10908v1",
      "published_date": "2024-01-12 17:50:35 UTC",
      "updated_date": "2024-01-12 17:50:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:23:37.678725"
    },
    {
      "arxiv_id": "2401.06837v2",
      "title": "Structsum Generation for Faster Text Comprehension",
      "title_zh": "翻译失败",
      "authors": [
        "Parag Jain",
        "Andreea Marzoca",
        "Francesco Piccinno"
      ],
      "abstract": "We consider the task of generating structured representations of text using\nlarge language models (LLMs). We focus on tables and mind maps as\nrepresentative modalities. Tables are more organized way of representing data,\nwhile mind maps provide a visually dynamic and flexible approach, particularly\nsuitable for sparse content. Despite the effectiveness of LLMs on different\ntasks, we show that current models struggle with generating structured outputs.\nIn response, we present effective prompting strategies for both of these tasks.\nWe introduce a taxonomy of problems around factuality, global and local\nstructure, common to both modalities and propose a set of critiques to tackle\nthese issues resulting in an absolute improvement in accuracy of +37pp (79%)\nfor mind maps and +15pp (78%) for tables. To evaluate semantic coverage of\ngenerated structured representations we propose Auto-QA, and we verify the\nadequacy of Auto-QA using SQuAD dataset. We further evaluate the usefulness of\nstructured representations via a text comprehension user study. The results\nshow a significant reduction in comprehension time compared to text when using\ntable (42.9%) and mind map (31.9%), without loss in accuracy.",
      "tldr_zh": "本研究探讨使用大型语言模型 (LLMs) 生成文本的结构化表示，如表格和思维导图，以加速文本理解。论文识别出 LLMs 在生成结构化输出时面临的挑战，包括事实性、全球和局部结构问题，并提出有效的提示策略、一个分类法 (taxonomy) 和一组批评 (critiques)，从而将思维导图准确率提高37pp（至79%），表格准确率提高15pp（至78%）。为了评估语义覆盖，引入了 Auto-QA 方法，并通过 SQuAD 数据集验证；用户研究显示，使用表格和思维导图可分别减少理解时间42.9%和31.9%，而不降低准确性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "camera ready",
      "pdf_url": "http://arxiv.org/pdf/2401.06837v2",
      "published_date": "2024-01-12 17:43:51 UTC",
      "updated_date": "2024-06-19 09:59:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:23:50.316981"
    },
    {
      "arxiv_id": "2401.08683v1",
      "title": "Zero-Shot RTL Code Generation with Attention Sink Augmented Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Selim Sandal",
        "Ismail Akturk"
      ],
      "abstract": "The design and optimization of hardware have traditionally been\nresource-intensive, demanding considerable expertise and dependence on\nestablished design automation tools. This paper discusses the possibility of\nexploiting large language models to streamline the code generation process in\nhardware design. In contrast to earlier studies, this paper aims to use large\nlanguage models that accepts high-level design specifications through a single\nprompt to generate corresponding Register-Transfer Level (RTL) code. The\nability to use large language models on RTL code generation not only expedites\ndesign iteration cycles but also facilitates the exploration of design spaces\nthat have computational challenges for conventional techniques. Through our\nevaluation, we demonstrate the shortcoming of existing attention mechanisms,\nand present the abilities of language models to produce functional, optimized,\nand industry-standard compliant RTL code when a novel attention mechanism is\nused. These findings underscore the expanding role of large language models in\nshaping the future landscape of architectural exploration and automation in\nhardware design.",
      "tldr_zh": "本研究探讨了利用大型语言模型（Large Language Models）实现零样本（Zero-Shot）RTL代码生成，以简化硬件设计的资源密集型过程。论文提出了一种新型注意力机制（Attention Sink Augmented），允许LLMs通过单一提示从高层设计规范直接生成功能性、优化且符合行业标准的RTL代码，从而加速设计迭代和探索计算挑战性的设计空间。实验结果揭示了现有注意力机制的不足，并证明了该增强机制的LLMs在RTL代码生成方面的显著优势，最终强调了大型语言模型在硬件设计自动化和架构探索中的潜力。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG",
        "cs.PL",
        "cs.SE"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.08683v1",
      "published_date": "2024-01-12 17:41:38 UTC",
      "updated_date": "2024-01-12 17:41:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:24:04.000300"
    },
    {
      "arxiv_id": "2401.06715v1",
      "title": "Reframing Tax Law Entailment as Analogical Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Xinrui Zou",
        "Ming Zhang",
        "Nathaniel Weir",
        "Benjamin Van Durme",
        "Nils Holzenberger"
      ],
      "abstract": "Statutory reasoning refers to the application of legislative provisions to a\nseries of case facts described in natural language. We re-frame statutory\nreasoning as an analogy task, where each instance of the analogy task involves\na combination of two instances of statutory reasoning. This increases the\ndataset size by two orders of magnitude, and introduces an element of\ninterpretability. We show that this task is roughly as difficult to Natural\nLanguage Processing models as the original task. Finally, we come back to\nstatutory reasoning, solving it with a combination of a retrieval mechanism and\nanalogy models, and showing some progress on prior comparable work.",
      "tldr_zh": "本研究将法定推理（statutory reasoning）重新定义为类比任务（analogy task），通过将每个任务组合两个推理实例，将数据集规模扩大两个数量级，同时提升任务的可解释性。实验结果显示，这种新框架对自然语言处理（NLP）模型的难度与原任务相当。最终，论文采用检索机制（retrieval mechanism）和类比模型相结合的方法来解决法定推理问题，并在之前可比工作基础上取得了某些进展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06715v1",
      "published_date": "2024-01-12 17:37:07 UTC",
      "updated_date": "2024-01-12 17:37:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:24:12.876228"
    },
    {
      "arxiv_id": "2401.06709v1",
      "title": "Reliability Analysis of Psychological Concept Extraction and Classification in User-penned Text",
      "title_zh": "翻译失败",
      "authors": [
        "Muskan Garg",
        "MSVPJ Sathvik",
        "Amrit Chadha",
        "Shaina Raza",
        "Sunghwan Sohn"
      ],
      "abstract": "The social NLP research community witness a recent surge in the computational\nadvancements of mental health analysis to build responsible AI models for a\ncomplex interplay between language use and self-perception. Such responsible AI\nmodels aid in quantifying the psychological concepts from user-penned texts on\nsocial media. On thinking beyond the low-level (classification) task, we\nadvance the existing binary classification dataset, towards a higher-level task\nof reliability analysis through the lens of explanations, posing it as one of\nthe safety measures. We annotate the LoST dataset to capture nuanced textual\ncues that suggest the presence of low self-esteem in the posts of Reddit users.\nWe further state that the NLP models developed for determining the presence of\nlow self-esteem, focus more on three types of textual cues: (i) Trigger: words\nthat triggers mental disturbance, (ii) LoST indicators: text indicators\nemphasizing low self-esteem, and (iii) Consequences: words describing the\nconsequences of mental disturbance. We implement existing classifiers to\nexamine the attention mechanism in pre-trained language models (PLMs) for a\ndomain-specific psychology-grounded task. Our findings suggest the need of\nshifting the focus of PLMs from Trigger and Consequences to a more\ncomprehensive explanation, emphasizing LoST indicators while determining low\nself-esteem in Reddit posts.",
      "tldr_zh": "本研究分析了从用户撰写的文本中提取和分类心理概念的可靠性，旨在构建负责任的 AI 模型来量化社交媒体（如 Reddit）中的心理概念，例如低自尊（LoST）。作者扩展了现有的二元分类数据集，通过标注 LoST 数据集来捕捉文本中暗示低自尊的细微线索，包括 Trigger（触发词）、LoST indicators（LoST 指标）和 Consequences（后果词）。他们使用现有分类器检查预训练语言模型（PLMs）的注意力机制，发现这些模型过度关注 Trigger 和 Consequences，而忽略了 LoST indicators，导致解释不全面。研究结果强调，应调整 PLMs 的焦点，以提供更全面的解释，从而提升心理健康分析的安全性和可靠性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06709v1",
      "published_date": "2024-01-12 17:19:14 UTC",
      "updated_date": "2024-01-12 17:19:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:24:25.622794"
    },
    {
      "arxiv_id": "2401.06699v2",
      "title": "A Closed-form Solution for Weight Optimization in Fully-connected Feed-forward Neural Networks",
      "title_zh": "全连接前向神经网络中权重优化的闭式解",
      "authors": [
        "Slavisa Tomic",
        "João Pedro Matos-Carvalho",
        "Marko Beko"
      ],
      "abstract": "This work addresses weight optimization problem for fully-connected\nfeed-forward neural networks. Unlike existing approaches that are based on\nback-propagation (BP) and chain rule gradient-based optimization (which implies\niterative execution, potentially burdensome and time-consuming in some cases),\nthe proposed approach offers the solution for weight optimization in\nclosed-form by means of least squares (LS) methodology. In the case where the\ninput-to-output mapping is injective, the new approach optimizes the weights in\na back-propagating fashion in a single iteration by jointly optimizing a set of\nweights in each layer for each neuron. In the case where the input-to-output\nmapping is not injective (e.g., in classification problems), the proposed\nsolution is easily adapted to obtain its final solution in a few iterations. An\nimportant advantage over the existing solutions is that these computations (for\nall neurons in a layer) are independent from each other; thus, they can be\ncarried out in parallel to optimize all weights in a given layer\nsimultaneously. Furthermore, its running time is deterministic in the sense\nthat one can obtain the exact number of computations necessary to optimize the\nweights in all network layers (per iteration, in the case of non-injective\nmapping). Our simulation and empirical results show that the proposed scheme,\nBPLS, works well and is competitive with existing ones in terms of accuracy,\nbut significantly surpasses them in terms of running time. To summarize, the\nnew method is straightforward to implement, is competitive and computationally\nmore efficient than the existing ones, and is well-tailored for parallel\nimplementation.",
      "tldr_zh": "本研究提出了一种闭式解（closed-form solution）方法，用于 fully-connected feed-forward neural networks 的权重优化问题。该方法基于 least squares (LS) 技术，与传统的 back-propagation (BP) 和梯度优化方法不同，它能在单次迭代中优化权重，尤其适用于输入到输出映射为 injective 的情况。针对非 injective 的场景（如分类问题），该方法只需少数迭代即可完成优化，并支持并行计算以提高效率。实验结果显示，新方案 BPLS 在准确性上与现有方法相当，但在运行时间上显著优越，易于实现并适合大规模并行处理。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06699v2",
      "published_date": "2024-01-12 17:03:55 UTC",
      "updated_date": "2024-06-17 07:16:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:24:36.295961"
    },
    {
      "arxiv_id": "2401.06692v3",
      "title": "An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models",
      "title_zh": "一种实验设计",
      "authors": [
        "Gantavya Bhatt",
        "Yifang Chen",
        "Arnav M. Das",
        "Jifan Zhang",
        "Sang T. Truong",
        "Stephen Mussmann",
        "Yinglun Zhu",
        "Jeffrey Bilmes",
        "Simon S. Du",
        "Kevin Jamieson",
        "Jordan T. Ash",
        "Robert D. Nowak"
      ],
      "abstract": "Supervised finetuning (SFT) on instruction datasets has played a crucial role\nin achieving the remarkable zero-shot generalization capabilities observed in\nmodern large language models (LLMs). However, the annotation efforts required\nto produce high quality responses for instructions are becoming prohibitively\nexpensive, especially as the number of tasks spanned by instruction datasets\ncontinues to increase. Active learning is effective in identifying useful\nsubsets of samples to annotate from an unlabeled pool, but its high\ncomputational cost remains a barrier to its widespread applicability in the\ncontext of LLMs. To mitigate the annotation cost of SFT and circumvent the\ncomputational bottlenecks of active learning, we propose using experimental\ndesign. Experimental design techniques select the most informative samples to\nlabel, and typically maximize some notion of uncertainty and/or diversity. In\nour work, we implement a framework that evaluates several existing and novel\nexperimental design techniques and find that these methods consistently yield\nsignificant gains in label efficiency with little computational overhead. On\ngenerative tasks, our methods achieve the same generalization performance with\nonly $50\\%$ of annotation cost required by random sampling.",
      "tldr_zh": "该研究针对监督微调 (SFT) 在大型语言模型 (LLMs) 上的高标注成本问题，提出一个实验设计框架，以提高标签效率。该框架利用实验设计技术从未标注样本中选择最有信息量的子集，最大化不确定性和多样性，从而规避了主动学习的高计算开销。实验结果显示，该方法在生成任务上只需随机采样的50% 标注成本即可达到相同的泛化性能，为高效的LLMs 微调提供了实用解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to Findings of the Association for Computational\n  Linguistics: ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2401.06692v3",
      "published_date": "2024-01-12 16:56:54 UTC",
      "updated_date": "2024-07-08 02:52:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:24:50.335881"
    },
    {
      "arxiv_id": "2401.06683v2",
      "title": "DQNC2S: DQN-based Cross-stream Crisis event Summarizer",
      "title_zh": "翻译失败",
      "authors": [
        "Daniele Rege Cambrin",
        "Luca Cagliero",
        "Paolo Garza"
      ],
      "abstract": "Summarizing multiple disaster-relevant data streams simultaneously is\nparticularly challenging as existing Retrieve&Re-ranking strategies suffer from\nthe inherent redundancy of multi-stream data and limited scalability in a\nmulti-query setting. This work proposes an online approach to crisis timeline\ngeneration based on weak annotation with Deep Q-Networks. It selects on-the-fly\nthe relevant pieces of text without requiring neither human annotations nor\ncontent re-ranking. This makes the inference time independent of the number of\ninput queries. The proposed approach also incorporates a redundancy filter into\nthe reward function to effectively handle cross-stream content overlaps. The\nachieved ROUGE and BERTScore results are superior to those of best-performing\nmodels on the CrisisFACTS 2022 benchmark.",
      "tldr_zh": "本研究提出了一种基于 Deep Q-Networks (DQN) 的在线方法，名为 DQNC2S，用于从多个灾害相关数据流中生成危机事件时间线。该方法通过弱标注策略实时选择相关文本，并将冗余过滤器整合到奖励函数中，以有效处理跨流内容重叠，同时避免了人工标注和内容重新排序的需求，从而使推理时间独立于输入查询数量。在 CrisisFACTS 2022 基准测试中，该方法在 ROUGE 和 BERTScore 指标上超过了现有最佳模型的表现。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "accepted at ECIR 2024",
      "pdf_url": "http://arxiv.org/pdf/2401.06683v2",
      "published_date": "2024-01-12 16:43:28 UTC",
      "updated_date": "2024-02-02 09:54:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:25:00.889055"
    },
    {
      "arxiv_id": "2401.06836v3",
      "title": "Enhancing Emotional Generation Capability of Large Language Models via Emotional Chain-of-Thought",
      "title_zh": "通过情感链式思维提升大型语言模型的情感生成能力",
      "authors": [
        "Zaijing Li",
        "Gongwei Chen",
        "Rui Shao",
        "Yuquan Xie",
        "Dongmei Jiang",
        "Liqiang Nie"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable performance in various\nemotion recognition tasks, thereby piquing the research community's curiosity\nfor exploring their potential in emotional intelligence. However, several\nissues in the field of emotional generation tasks remain unresolved, including\nhuman preference alignment and emotional generation assessment. In this paper,\nwe propose the Emotional Chain-of-Thought (ECoT), a plug-and-play prompting\nmethod that enhances the performance of LLMs on various emotional generation\ntasks by aligning with human emotional intelligence guidelines. To assess the\nreliability of ECoT, we propose an automated model-based evaluation method\ncalled Emotional Generation Score (EGS). EGS incorporates Goleman's Emotional\nIntelligence Theory as a consensus of human experts, providing a new\nperspective on the evaluation of emotional generation tasks. Extensive\nexperimental results demonstrate the effectiveness of ECoT and EGS. Further, we\ndiscuss the promise of LLMs in the field of emotional intelligence and present\nkey insights into the LLMs with the ECoT in emotional generation tasks.",
      "tldr_zh": "本研究针对大型语言模型 (LLMs) 在情感生成任务中存在的人类偏好对齐和评估难题，提出了一种即插即用的提示方法Emotional Chain-of-Thought (ECoT)，通过与人类情感智能指南对齐，提升LLMs在各种情感生成任务中的性能。研究同时引入Emotional Generation Score (EGS)作为一种自动化评估方法，该方法基于Goleman's Emotional Intelligence Theory，作为人类专家共识，提供新的情感生成评估视角。实验结果证明ECoT和EGS的有效性，并探讨了LLMs在情感智能领域的潜力及其关键洞见。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06836v3",
      "published_date": "2024-01-12 16:42:10 UTC",
      "updated_date": "2024-08-07 08:09:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:25:11.556822"
    },
    {
      "arxiv_id": "2401.06676v1",
      "title": "LLMRS: Unlocking Potentials of LLM-Based Recommender Systems for Software Purchase",
      "title_zh": "LLMRS：解锁基于LLM的推荐系统的潜力，用于软件购买",
      "authors": [
        "Angela John",
        "Theophilus Aidoo",
        "Hamayoon Behmanush",
        "Irem B. Gunduz",
        "Hewan Shrestha",
        "Maxx Richard Rahman",
        "Wolfgang Maaß"
      ],
      "abstract": "Recommendation systems are ubiquitous, from Spotify playlist suggestions to\nAmazon product suggestions. Nevertheless, depending on the methodology or the\ndataset, these systems typically fail to capture user preferences and generate\ngeneral recommendations. Recent advancements in Large Language Models (LLM)\noffer promising results for analyzing user queries. However, employing these\nmodels to capture user preferences and efficiency remains an open question. In\nthis paper, we propose LLMRS, an LLM-based zero-shot recommender system where\nwe employ pre-trained LLM to encode user reviews into a review score and\ngenerate user-tailored recommendations. We experimented with LLMRS on a\nreal-world dataset, the Amazon product reviews, for software purchase use\ncases. The results show that LLMRS outperforms the ranking-based baseline model\nwhile successfully capturing meaningful information from product reviews,\nthereby providing more reliable recommendations.",
      "tldr_zh": "该论文探讨了推荐系统在捕捉用户偏好方面的不足，并提出LLMRS，一种基于Large Language Models (LLM)的零样本推荐系统，旨在为软件购买场景提供个性化推荐。LLMRS利用预训练LLM将用户评论编码成评论分数，从而生成更精确的用户定制建议。在Amazon产品评论数据集上的实验显示，LLMRS在性能上优于基于排名的基线模型，能够有效提取有意义的信息并提升推荐的可靠性。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06676v1",
      "published_date": "2024-01-12 16:33:17 UTC",
      "updated_date": "2024-01-12 16:33:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:25:24.151787"
    },
    {
      "arxiv_id": "2403.07884v1",
      "title": "Seg-metrics: a Python package to compute segmentation metrics",
      "title_zh": "翻译失败",
      "authors": [
        "Jingnan Jia",
        "Marius Staring",
        "Berend C. Stoel"
      ],
      "abstract": "In response to a concerning trend of selectively emphasizing metrics in\nmedical image segmentation (MIS) studies, we introduce \\texttt{seg-metrics}, an\nopen-source Python package for standardized MIS model evaluation. Unlike\nexisting packages, \\texttt{seg-metrics} offers user-friendly interfaces for\nvarious overlap-based and distance-based metrics, providing a comprehensive\nsolution. \\texttt{seg-metrics} supports multiple file formats and is easily\ninstallable through the Python Package Index (PyPI). With a focus on speed and\nconvenience, \\texttt{seg-metrics} stands as a valuable tool for efficient MIS\nmodel assessment.",
      "tldr_zh": "针对医疗图像分割（MIS）研究中指标选择性强调的趋势，该研究引入了 seg-metrics，这是一个开源 Python 包，用于标准化 MIS 模型评估。seg-metrics 提供用户友好的接口，支持多种 overlap-based 和 distance-based 指标，并兼容多种文件格式，同时可以通过 Python Package Index (PyPI) 轻松安装。该包强调速度和便利性，成为高效评估 MIS 模型的宝贵工具。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.07884v1",
      "published_date": "2024-01-12 16:30:54 UTC",
      "updated_date": "2024-01-12 16:30:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:25:36.768522"
    },
    {
      "arxiv_id": "2401.06373v2",
      "title": "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Yi Zeng",
        "Hongpeng Lin",
        "Jingwen Zhang",
        "Diyi Yang",
        "Ruoxi Jia",
        "Weiyan Shi"
      ],
      "abstract": "Most traditional AI safety research has approached AI models as machines and\ncentered on algorithm-focused attacks developed by security experts. As large\nlanguage models (LLMs) become increasingly common and competent, non-expert\nusers can also impose risks during daily interactions. This paper introduces a\nnew perspective to jailbreak LLMs as human-like communicators, to explore this\noverlooked intersection between everyday language interaction and AI safety.\nSpecifically, we study how to persuade LLMs to jailbreak them. First, we\npropose a persuasion taxonomy derived from decades of social science research.\nThen, we apply the taxonomy to automatically generate interpretable persuasive\nadversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion\nsignificantly increases the jailbreak performance across all risk categories:\nPAP consistently achieves an attack success rate of over $92\\%$ on Llama 2-7b\nChat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focused\nattacks. On the defense side, we explore various mechanisms against PAP and,\nfound a significant gap in existing defenses, and advocate for more fundamental\nmitigation for highly interactive LLMs",
      "tldr_zh": "这篇论文从人类沟通视角重新审视 AI 安全，探讨如何通过说服策略来 jailbreak LLMs，强调非专家用户在日常互动中可能带来的风险。作者基于社会科学的 persuasion taxonomy 提出自动生成可解释的说服性对抗提示（PAP），用于攻击 LLMs。实验结果显示，PAP 在 Llama 2-7b Chat、GPT-3.5 和 GPT-4 上攻击成功率超过 92%，显著优于传统算法-focused 攻击。在防御方面，论文发现现有机制存在显著差距，并呼吁开发更根本的缓解措施以应对高度交互式 LLMs。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages of the main text, qualitative examples of jailbreaks may be\n  harmful in nature",
      "pdf_url": "http://arxiv.org/pdf/2401.06373v2",
      "published_date": "2024-01-12 16:13:24 UTC",
      "updated_date": "2024-01-23 22:46:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:25:49.102492"
    },
    {
      "arxiv_id": "2401.09354v1",
      "title": "Transcending Controlled Environments Assessing the Transferability of ASRRobust NLU Models to Real-World Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Hania Khan",
        "Aleena Fatima Khalid",
        "Zaryab Hassan"
      ],
      "abstract": "This research investigates the transferability of Automatic Speech\nRecognition (ASR)-robust Natural Language Understanding (NLU) models from\ncontrolled experimental conditions to practical, real-world applications.\nFocused on smart home automation commands in Urdu, the study assesses model\nperformance under diverse noise profiles, linguistic variations, and ASR error\nscenarios. Leveraging the UrduBERT model, the research employs a systematic\nmethodology involving real-world data collection, cross-validation, transfer\nlearning, noise variation studies, and domain adaptation. Evaluation metrics\nencompass task-specific accuracy, latency, user satisfaction, and robustness to\nASR errors. The findings contribute insights into the challenges and\nadaptability of ASR-robust NLU models in transcending controlled environments.",
      "tldr_zh": "本研究评估了 ASR-robust NLU 模型从受控环境向真实世界应用的转移能力，焦点放在乌尔都语的智能家居命令上。研究采用 UrduBERT 模型，通过真实数据收集、交叉验证、迁移学习、噪声变异研究和领域适应等系统方法，测试模型在不同噪声、语言变体和 ASR 错误场景下的性能。评估指标包括任务特定准确率、延迟、用户满意度和对 ASR 错误的鲁棒性，结果揭示了这些模型在实际应用中面临的挑战及其适应潜力。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.09354v1",
      "published_date": "2024-01-12 16:10:04 UTC",
      "updated_date": "2024-01-12 16:10:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:26:00.231714"
    },
    {
      "arxiv_id": "2401.06654v1",
      "title": "Decoupling Pixel Flipping and Occlusion Strategy for Consistent XAI Benchmarks",
      "title_zh": "解耦像素翻转和",
      "authors": [
        "Stefan Blücher",
        "Johanna Vielhaben",
        "Nils Strodthoff"
      ],
      "abstract": "Feature removal is a central building block for eXplainable AI (XAI), both\nfor occlusion-based explanations (Shapley values) as well as their evaluation\n(pixel flipping, PF). However, occlusion strategies can vary significantly from\nsimple mean replacement up to inpainting with state-of-the-art diffusion\nmodels. This ambiguity limits the usefulness of occlusion-based approaches. For\nexample, PF benchmarks lead to contradicting rankings. This is amplified by\ncompeting PF measures: Features are either removed starting with most\ninfluential first (MIF) or least influential first (LIF). This study proposes\ntwo complementary perspectives to resolve this disagreement problem. Firstly,\nwe address the common criticism of occlusion-based XAI, that artificial samples\nlead to unreliable model evaluations. We propose to measure the reliability by\nthe R(eference)-Out-of-Model-Scope (OMS) score. The R-OMS score enables a\nsystematic comparison of occlusion strategies and resolves the disagreement\nproblem by grouping consistent PF rankings. Secondly, we show that the\ninsightfulness of MIF and LIF is conversely dependent on the R-OMS score. To\nleverage this, we combine the MIF and LIF measures into the symmetric relevance\ngain (SRG) measure. This breaks the inherent connection to the underlying\nocclusion strategy and leads to consistent rankings. This resolves the\ndisagreement problem, which we verify for a set of 40 different occlusion\nstrategies.",
      "tldr_zh": "这篇论文针对eXplainable AI (XAI)中的特征移除问题，探讨了遮挡策略（如Shapley values和pixel flipping, PF）的多样性导致的基准不一致性，例如从最影响特征开始移除（MIF）与从最不影响特征开始移除（LIF）的矛盾排名。作者提出R(eference)-Out-of-Model-Scope (R-OMS)分数来衡量遮挡策略的可靠性，通过系统比较和分组一致的PF排名，解决了这一分歧问题。同时，他们将MIF和LIF结合成symmetric relevance gain (SRG)措施，使其独立于底层遮挡策略，实现了更稳定的排名评估。在40种不同遮挡策略上验证了这一方法，为XAI基准的标准化奠定了基础。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "28 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.06654v1",
      "published_date": "2024-01-12 16:01:17 UTC",
      "updated_date": "2024-01-12 16:01:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:26:13.959154"
    },
    {
      "arxiv_id": "2401.06640v2",
      "title": "Experimental Contexts Can Facilitate Robust Semantic Property Inference in Language Models, but Inconsistently",
      "title_zh": "翻译失败",
      "authors": [
        "Kanishka Misra",
        "Allyson Ettinger",
        "Kyle Mahowald"
      ],
      "abstract": "Recent zero-shot evaluations have highlighted important limitations in the\nabilities of language models (LMs) to perform meaning extraction. However, it\nis now well known that LMs can demonstrate radical improvements in the presence\nof experimental contexts such as in-context examples and instructions. How well\ndoes this translate to previously studied meaning-sensitive tasks? We present a\ncase-study on the extent to which experimental contexts can improve LMs'\nrobustness in performing property inheritance -- predicting semantic properties\nof novel concepts, a task that they have been previously shown to fail on. Upon\ncarefully controlling the nature of the in-context examples and the\ninstructions, our work reveals that they can indeed lead to non-trivial\nproperty inheritance behavior in LMs. However, this ability is inconsistent:\nwith a minimal reformulation of the task, some LMs were found to pick up on\nshallow, non-semantic heuristics from their inputs, suggesting that the\ncomputational principles of semantic property inference are yet to be mastered\nby LMs.",
      "tldr_zh": "本研究探讨了实验上下文（如 in-context examples 和 instructions）如何提升语言模型 (LMs) 在语义属性推理任务上的鲁棒性，特别是预测新概念的 semantic property inheritance。研究通过仔细控制上下文示例和指令，发现这些元素能显著改善 LMs 的非微不足道属性继承行为。实验结果显示，这种提升并不一致，当任务稍作修改时，LMs 可能依赖浅层、非语义的启发式策略。总体而言，这表明 LMs 尚未完全掌握 semantic property inference 的计算原则。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2024 (main) camera-ready",
      "pdf_url": "http://arxiv.org/pdf/2401.06640v2",
      "published_date": "2024-01-12 15:40:31 UTC",
      "updated_date": "2024-10-17 00:06:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:26:25.531995"
    },
    {
      "arxiv_id": "2401.06634v1",
      "title": "CCFC: Bridging Federated Clustering and Contrastive Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Jie Yan",
        "Jing Liu",
        "Zhong-Yuan Zhang"
      ],
      "abstract": "Federated clustering, an essential extension of centralized clustering for\nfederated scenarios, enables multiple data-holding clients to collaboratively\ngroup data while keeping their data locally. In centralized scenarios,\nclustering driven by representation learning has made significant advancements\nin handling high-dimensional complex data. However, the combination of\nfederated clustering and representation learning remains underexplored. To\nbridge this, we first tailor a cluster-contrastive model for learning\nclustering-friendly representations. Then, we harness this model as the\nfoundation for proposing a new federated clustering method, named\ncluster-contrastive federated clustering (CCFC). Benefiting from representation\nlearning, the clustering performance of CCFC even double those of the best\nbaseline methods in some cases. Compared to the most related baseline, the\nbenefit results in substantial NMI score improvements of up to 0.4155 on the\nmost conspicuous case. Moreover, CCFC also shows superior performance in\nhandling device failures from a practical viewpoint.",
      "tldr_zh": "本论文提出 CCFC（Cluster-Contrastive Federated Clustering）方法，将联邦聚类（Federated Clustering）和对比学习（Contrastive Learning）相结合，解决多客户端协作聚类问题，同时保持数据本地化。CCFC 通过定制一个基于对比学习的模型来学习聚类友好的表示，从而提升联邦聚类的性能。实验结果显示，CCFC 在某些场景下使聚类性能提高一倍，与相关基线相比，NMI 分数最高提升 0.4155；此外，该方法在处理设备故障方面也表现出优越的实用性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06634v1",
      "published_date": "2024-01-12 15:26:44 UTC",
      "updated_date": "2024-01-12 15:26:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:26:37.292884"
    },
    {
      "arxiv_id": "2401.06633v2",
      "title": "Ada-Retrieval: An Adaptive Multi-Round Retrieval Paradigm for Sequential Recommendations",
      "title_zh": "Ada-Retrieval：自",
      "authors": [
        "Lei Li",
        "Jianxun Lian",
        "Xiao Zhou",
        "Xing Xie"
      ],
      "abstract": "Retrieval models aim at selecting a small set of item candidates which match\nthe preference of a given user. They play a vital role in large-scale\nrecommender systems since subsequent models such as rankers highly depend on\nthe quality of item candidates. However, most existing retrieval models employ\na single-round inference paradigm, which may not adequately capture the dynamic\nnature of user preferences and stuck in one area in the item space. In this\npaper, we propose Ada-Retrieval, an adaptive multi-round retrieval paradigm for\nrecommender systems that iteratively refines user representations to better\ncapture potential candidates in the full item space. Ada-Retrieval comprises\ntwo key modules: the item representation adapter and the user representation\nadapter, designed to inject context information into items' and users'\nrepresentations. The framework maintains a model-agnostic design, allowing\nseamless integration with various backbone models such as RNNs or Transformers.\nWe perform experiments on three widely used public datasets, incorporating five\npowerful sequential recommenders as backbone models. Our results demonstrate\nthat Ada-Retrieval significantly enhances the performance of various base\nmodels, with consistent improvements observed across different datasets. Our\ncode and data are publicly available at:\nhttps://github.com/ll0ruc/Ada-Retrieval.",
      "tldr_zh": "该论文提出 Ada-Retrieval，一种自适应多轮检索范式，用于顺序推荐系统，以解决现有单轮检索模型无法充分捕捉用户偏好动态变化的问题。该框架包括 item representation adapter 和 user representation adapter 两个关键模块，通过注入上下文信息来迭代优化用户和物品表示，支持与各种骨干模型（如 RNNs 或 Transformers）无缝集成。在三个公共数据集上的实验显示，Ada-Retrieval 显著提升了五种顺序推荐器的性能，并在不同数据集上保持一致改进。代码和数据已公开在 GitHub 上。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "9 pages, Accepted to AAAI2024",
      "pdf_url": "http://arxiv.org/pdf/2401.06633v2",
      "published_date": "2024-01-12 15:26:40 UTC",
      "updated_date": "2024-01-31 11:07:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:26:49.182507"
    },
    {
      "arxiv_id": "2401.06833v1",
      "title": "A hierarchical control framework for autonomous decision-making systems: Integrating HMDP and MPC",
      "title_zh": "翻译失败",
      "authors": [
        "Xue-Fang Wang",
        "Jingjing Jiang",
        "Wen-Hua Chen"
      ],
      "abstract": "This paper proposes a comprehensive hierarchical control framework for\nautonomous decision-making arising in robotics and autonomous systems. In a\ntypical hierarchical control architecture, high-level decision making is often\ncharacterised by discrete state and decision/control sets. However, a rational\ndecision is usually affected by not only the discrete states of the autonomous\nsystem, but also the underlying continuous dynamics even the evolution of its\noperational environment. This paper proposes a holistic and comprehensive\ndesign process and framework for this type of challenging problems, from new\nmodelling and design problem formulation to control design and stability\nanalysis. It addresses the intricate interplay between traditional continuous\nsystems dynamics utilized at the low levels for control design and discrete\nMarkov decision processes (MDP) for facilitating high-level decision making. We\nmodel the decision making system in complex environments as a hybrid system\nconsisting of a controlled MDP and autonomous (i.e. uncontrolled) continuous\ndynamics. Consequently, the new formulation is called as hybrid Markov decision\nprocess (HMDP). The design problem is formulated with a focus on ensuring both\nsafety and optimality while taking into account the influence of both the\ndiscrete and continuous state variables of different levels. With the help of\nthe model predictive control (MPC) concept, a decision maker design scheme is\nproposed for the proposed hybrid decision making model. By carefully designing\nkey ingredients involved in this scheme, it is shown that the recursive\nfeasibility and stability of the proposed autonomous decision making scheme are\nguaranteed. The proposed framework is applied to develop an autonomous lane\nchanging system for intelligent vehicles.",
      "tldr_zh": "本论文提出了一种分层控制框架，用于机器人和自治系统的自主决策，旨在整合离散状态决策（如Markov决策过程）和连续动态系统的影响。论文引入了新的建模方法，将系统表述为混合Markov决策过程(HMDP)，并结合模型预测控制(MPC)设计决策方案，以确保安全、优化以及递归可行性和稳定性。最终，该框架应用于智能车辆的自动变道系统，展示了其在复杂环境中的有效性。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.RO",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "11 pages, 14 figures, submitted to Automatica",
      "pdf_url": "http://arxiv.org/pdf/2401.06833v1",
      "published_date": "2024-01-12 15:25:51 UTC",
      "updated_date": "2024-01-12 15:25:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:26:59.587506"
    },
    {
      "arxiv_id": "2402.01659v1",
      "title": "Generative Artificial Intelligence in Higher Education: Evidence from an Analysis of Institutional Policies and Guidelines",
      "title_zh": "翻译失败",
      "authors": [
        "Nora McDonald",
        "Aditya Johri",
        "Areej Ali",
        "Aayushi Hingle"
      ],
      "abstract": "The release of ChatGPT in November 2022 prompted a massive uptake of\ngenerative artificial intelligence (GenAI) across higher education institutions\n(HEIs). HEIs scrambled to respond to its use, especially by students, looking\nfirst to regulate it and then arguing for its productive integration within\nteaching and learning. In the year since the release, HEIs have increasingly\nprovided policies and guidelines to direct GenAI. In this paper we examined\ndocuments produced by 116 US universities categorized as high research activity\nor R1 institutions to comprehensively understand GenAI related advice and\nguidance given to institutional stakeholders. Through an extensive analysis, we\nfound the majority of universities (N=73, 63%) encourage the use of GenAI and\nmany provide detailed guidance for its use in the classroom (N=48, 41%). More\nthan half of all institutions provided sample syllabi (N=65, 56%) and half\n(N=58, 50%) provided sample GenAI curriculum and activities that would help\ninstructors integrate and leverage GenAI in their classroom. Notably, most\nguidance for activities focused on writing, whereas code and STEM-related\nactivities were mentioned half the time and vaguely even when they were (N=58,\n50%). Finally, more than one half of institutions talked about the ethics of\nGenAI on a range of topics broadly, including Diversity, Equity and Inclusion\n(DEI) (N=60, 52%). Overall, based on our findings we caution that guidance for\nfaculty can become burdensome as extensive revision of pedagogical approaches\nis often recommended in the policies.",
      "tldr_zh": "这篇论文分析了ChatGPT于2022年11月发布后，美国116所高研究活动机构（R1 institutions）的Generative Artificial Intelligence (GenAI)政策和指南，通过文档审查揭示了机构对GenAI的响应策略。主要发现是，63%的大学鼓励GenAI使用，并为课堂提供详细指导和样例教学大纲（分别占41%和56%），但焦点主要在写作活动上，而代码和STEM相关活动较少提及。论文警告，这种广泛指导可能给教师带来负担，需要对教学方法进行大量修订，并强调了GenAI伦理问题，如Diversity, Equity and Inclusion (DEI)，在超过一半机构的文件中被讨论。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.01659v1",
      "published_date": "2024-01-12 14:58:13 UTC",
      "updated_date": "2024-01-12 14:58:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:27:13.360302"
    },
    {
      "arxiv_id": "2401.06595v1",
      "title": "Every Node is Different: Dynamically Fusing Self-Supervised Tasks for Attributed Graph Clustering",
      "title_zh": "翻译失败",
      "authors": [
        "Pengfei Zhu",
        "Qian Wang",
        "Yu Wang",
        "Jialu Li",
        "Qinghua Hu"
      ],
      "abstract": "Attributed graph clustering is an unsupervised task that partitions nodes\ninto different groups. Self-supervised learning (SSL) shows great potential in\nhandling this task, and some recent studies simultaneously learn multiple SSL\ntasks to further boost performance. Currently, different SSL tasks are assigned\nthe same set of weights for all graph nodes. However, we observe that some\ngraph nodes whose neighbors are in different groups require significantly\ndifferent emphases on SSL tasks. In this paper, we propose to dynamically learn\nthe weights of SSL tasks for different nodes and fuse the embeddings learned\nfrom different SSL tasks to boost performance. We design an innovative graph\nclustering approach, namely Dynamically Fusing Self-Supervised Learning\n(DyFSS). Specifically, DyFSS fuses features extracted from diverse SSL tasks\nusing distinct weights derived from a gating network. To effectively learn the\ngating network, we design a dual-level self-supervised strategy that\nincorporates pseudo labels and the graph structure. Extensive experiments on\nfive datasets show that DyFSS outperforms the state-of-the-art multi-task SSL\nmethods by up to 8.66% on the accuracy metric. The code of DyFSS is available\nat: https://github.com/q086/DyFSS.",
      "tldr_zh": "该论文针对 Attributed Graph Clustering 的无监督任务，观察到现有 Self-Supervised Learning (SSL) 方法为所有节点分配相同权重的问题，从而提出 Dynamically Fusing Self-Supervised Learning (DyFSS) 方法，通过动态学习不同节点的 SSL 任务权重并融合嵌入来提升聚类性能。DyFSS 采用一个 gating network 和 dual-level self-supervised strategy，利用 pseudo labels 和 graph structure 来有效训练权重。实验结果显示，在五个数据集上，DyFSS 比最先进的多任务 SSL 方法提高了多达 8.66% 的准确率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06595v1",
      "published_date": "2024-01-12 14:24:10 UTC",
      "updated_date": "2024-01-12 14:24:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:27:27.614842"
    },
    {
      "arxiv_id": "2401.06588v1",
      "title": "Dynamic Behaviour of Connectionist Speech Recognition with Strong Latency Constraints",
      "title_zh": "翻译失败",
      "authors": [
        "Giampiero Salvi"
      ],
      "abstract": "This paper describes the use of connectionist techniques in phonetic speech\nrecognition with strong latency constraints. The constraints are imposed by the\ntask of deriving the lip movements of a synthetic face in real time from the\nspeech signal, by feeding the phonetic string into an articulatory synthesiser.\nParticular attention has been paid to analysing the interaction between the\ntime evolution model learnt by the multi-layer perceptrons and the transition\nmodel imposed by the Viterbi decoder, in different latency conditions. Two\nexperiments were conducted in which the time dependencies in the language model\n(LM) were controlled by a parameter. The results show a strong interaction\nbetween the three factors involved, namely the neural network topology, the\nlength of time dependencies in the LM and the decoder latency.",
      "tldr_zh": "本论文探讨了连接主义技术在语音识别中的动态行为，特别是在强延迟约束下的应用，这些约束源于实时从语音信号中推导出合成面部唇部动作的任务。研究重点分析了多层感知器（Multi-Layer Perceptrons）学习的时间演化模型与Viterbi解码器的过渡模型之间的交互，通过控制语言模型（LM）中的时间依赖性参数进行两个实验。结果表明，神经网络拓扑、LM的时间依赖长度和解码器延迟三者之间存在强烈交互，为延迟敏感语音识别系统设计提供了重要洞见。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.SD",
        "I.5.0; I.2.7; E.4"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06588v1",
      "published_date": "2024-01-12 14:10:28 UTC",
      "updated_date": "2024-01-12 14:10:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:27:37.814109"
    },
    {
      "arxiv_id": "2401.06583v1",
      "title": "Mapping Transformer Leveraged Embeddings for Cross-Lingual Document Representation",
      "title_zh": "翻译失败",
      "authors": [
        "Tsegaye Misikir Tashu",
        "Eduard-Raul Kontos",
        "Matthia Sabatelli",
        "Matias Valdenegro-Toro"
      ],
      "abstract": "Recommendation systems, for documents, have become tools to find relevant\ncontent on the Web. However, these systems have limitations when it comes to\nrecommending documents in languages different from the query language, which\nmeans they might overlook resources in non-native languages. This research\nfocuses on representing documents across languages by using Transformer\nLeveraged Document Representations (TLDRs) that are mapped to a cross-lingual\ndomain. Four multilingual pre-trained transformer models (mBERT, mT5 XLM\nRoBERTa, ErnieM) were evaluated using three mapping methods across 20 language\npairs representing combinations of five selected languages of the European\nUnion. Metrics like Mate Retrieval Rate and Reciprocal Rank were used to\nmeasure the effectiveness of mapped TLDRs compared to non-mapped ones. The\nresults highlight the power of cross-lingual representations achieved through\npre-trained transformers and mapping approaches suggesting a promising\ndirection for expanding beyond language connections, between two specific\nlanguages.",
      "tldr_zh": "本研究针对推荐系统中跨语言文档推荐的局限性，提出了一种基于 Transformer Leveraged Embeddings 的映射方法（TLDRs），以实现文档在不同语言间的有效表示。研究评估了四个多语言预训练模型（mBERT、mT5、XLM-RoBERTa 和 ErnieM），并使用三种映射方法在 20 种语言对（基于五个欧盟语言）上进行测试。实验采用 Mate Retrieval Rate 和 Reciprocal Rank 等指标，结果显示映射后的 TLDRs 显著优于未映射版本，证明了预训练 Transformer 和映射方法在扩展跨语言连接方面的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06583v1",
      "published_date": "2024-01-12 14:01:15 UTC",
      "updated_date": "2024-01-12 14:01:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:27:51.826570"
    },
    {
      "arxiv_id": "2401.10280v1",
      "title": "GANs for EVT Based Model Parameter Estimation in Real-time Ultra-Reliable Communication",
      "title_zh": "翻译失败",
      "authors": [
        "Parmida Valiahdi",
        "Sinem Coleri"
      ],
      "abstract": "The Ultra-Reliable Low-Latency Communications (URLLC) paradigm in\nsixth-generation (6G) systems heavily relies on precise channel modeling,\nespecially when dealing with rare and extreme events within wireless\ncommunication channels. This paper explores a novel methodology integrating\nExtreme Value Theory (EVT) and Generative Adversarial Networks (GANs) to\nachieve the precise channel modeling in real-time. The proposed approach\nharnesses EVT by employing the Generalized Pareto Distribution (GPD) to model\nthe distribution of extreme events. Subsequently, Generative Adversarial\nNetworks (GANs) are employed to estimate the parameters of the GPD. In contrast\nto conventional GAN configurations that focus on estimating the overall\ndistribution, the proposed approach involves the incorporation of an additional\nblock within the GAN structure. This specific augmentation is designed with the\nexplicit purpose of directly estimating the parameters of the Generalized\nPareto Distribution (GPD). Through extensive simulations across different\nsample sizes, the proposed GAN based approach consistently demonstrates\nsuperior adaptability, surpassing Maximum Likelihood Estimation (MLE),\nparticularly in scenarios with limited sample sizes.",
      "tldr_zh": "本文提出了一种结合Extreme Value Theory (EVT) 和Generative Adversarial Networks (GANs) 的新方法，用于在第六代 (6G) 系统的Ultra-Reliable Low-Latency Communications (URLLC) 中实现实时精确通道建模，特别是针对无线通信中的稀有极端事件。该方法采用Generalized Pareto Distribution (GPD) 来描述极端事件分布，并通过在GANs结构中添加一个专用块，直接估计GPD的参数，以提升参数估计的适应性。与传统方法相比，实验模拟在不同样本大小下显示，该方法显著优于Maximum Likelihood Estimation (MLE)，尤其在样本量有限的场景中。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.10280v1",
      "published_date": "2024-01-12 13:34:34 UTC",
      "updated_date": "2024-01-12 13:34:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:28:02.317845"
    },
    {
      "arxiv_id": "2401.06568v2",
      "title": "Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation",
      "title_zh": "迷失在源语言中：大型语言模型如何评估",
      "authors": [
        "Xu Huang",
        "Zhirui Zhang",
        "Xiang Geng",
        "Yichao Du",
        "Jiajun Chen",
        "Shujian Huang"
      ],
      "abstract": "This study investigates how Large Language Models (LLMs) leverage source and\nreference data in machine translation evaluation task, aiming to better\nunderstand the mechanisms behind their remarkable performance in this task. We\ndesign the controlled experiments across various input modes and model types,\nand employ both coarse-grained and fine-grained prompts to discern the utility\nof source versus reference information. We find that reference information\nsignificantly enhances the evaluation accuracy, while surprisingly, source\ninformation sometimes is counterproductive, indicating LLMs' inability to fully\nleverage the cross-lingual capability when evaluating translations. Further\nanalysis of the fine-grained evaluation and fine-tuning experiments show\nsimilar results. These findings also suggest a potential research direction for\nLLMs that fully exploits the cross-lingual capability of LLMs to achieve better\nperformance in machine translation evaluation tasks.",
      "tldr_zh": "这篇论文研究了 Large Language Models (LLMs) 在机器翻译评估任务中如何利用源语言和参考数据，旨在揭示其性能机制。作者通过控制实验、不同输入模式以及粗粒度和细粒度提示，分析了源信息和参考信息的作用，结果显示参考信息显著提升了评估准确性，而源信息有时适得其反，表明 LLMs 未能充分利用其 cross-lingual capability。论文进一步通过细粒度评估和微调实验验证了这些发现，并建议未来研究方向以优化 LLMs 的跨语言能力，从而改善机器翻译评估性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ACL2024 Findings",
      "pdf_url": "http://arxiv.org/pdf/2401.06568v2",
      "published_date": "2024-01-12 13:23:21 UTC",
      "updated_date": "2024-06-06 07:51:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:28:14.612841"
    },
    {
      "arxiv_id": "2401.06559v1",
      "title": "A General Benchmark Framework is Dynamic Graph Neural Network Need",
      "title_zh": "翻译失败",
      "authors": [
        "Yusen Zhang"
      ],
      "abstract": "Dynamic graph learning is crucial for modeling real-world systems with\nevolving relationships and temporal dynamics. However, the lack of a unified\nbenchmark framework in current research has led to inaccurate evaluations of\ndynamic graph models. This paper highlights the significance of dynamic graph\nlearning and its applications in various domains. It emphasizes the need for a\nstandardized benchmark framework that captures temporal dynamics, evolving\ngraph structures, and downstream task requirements. Establishing a unified\nbenchmark will help researchers understand the strengths and limitations of\nexisting models, foster innovation, and advance dynamic graph learning. In\nconclusion, this paper identifies the lack of a standardized benchmark\nframework as a current limitation in dynamic graph learning research . Such a\nframework will facilitate accurate model evaluation, drive advancements in\ndynamic graph learning techniques, and enable the development of more effective\nmodels for real-world applications.",
      "tldr_zh": "本文指出，动态图学习(dynamic graph learning)对于建模真实世界的演化关系和时间动态至关重要，但当前缺乏统一的基准框架，导致动态图模型的评估不准确。论文强调需要建立一个标准化基准框架，以捕捉时间动态、演化图结构以及下游任务需求。该框架将帮助研究人员更好地理解现有模型的优缺点，促进创新，并推动动态图学习技术的进步。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06559v1",
      "published_date": "2024-01-12 13:12:07 UTC",
      "updated_date": "2024-01-12 13:12:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:28:26.188977"
    },
    {
      "arxiv_id": "2401.06557v1",
      "title": "Treatment-Aware Hyperbolic Representation Learning for Causal Effect Estimation with Social Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Ziqiang Cui",
        "Xing Tang",
        "Yang Qiao",
        "Bowei He",
        "Liang Chen",
        "Xiuqiang He",
        "Chen Ma"
      ],
      "abstract": "Estimating the individual treatment effect (ITE) from observational data is a\ncrucial research topic that holds significant value across multiple domains.\nHow to identify hidden confounders poses a key challenge in ITE estimation.\nRecent studies have incorporated the structural information of social networks\nto tackle this challenge, achieving notable advancements. However, these\nmethods utilize graph neural networks to learn the representation of hidden\nconfounders in Euclidean space, disregarding two critical issues: (1) the\nsocial networks often exhibit a scalefree structure, while Euclidean embeddings\nsuffer from high distortion when used to embed such graphs, and (2) each\nego-centric network within a social network manifests a treatment-related\ncharacteristic, implying significant patterns of hidden confounders. To address\nthese issues, we propose a novel method called Treatment-Aware Hyperbolic\nRepresentation Learning (TAHyper). Firstly, TAHyper employs the hyperbolic\nspace to encode the social networks, thereby effectively reducing the\ndistortion of confounder representation caused by Euclidean embeddings.\nSecondly, we design a treatment-aware relationship identification module that\nenhances the representation of hidden confounders by identifying whether an\nindividual and her neighbors receive the same treatment. Extensive experiments\non two benchmark datasets are conducted to demonstrate the superiority of our\nmethod.",
      "tldr_zh": "本论文针对从观察数据估计个体治疗效果(ITE)的问题，强调了识别隐藏混杂因素的挑战，尤其是在社交网络的规模自由结构下，传统欧氏空间嵌入会导致高失真。提出了一种名为Treatment-Aware Hyperbolic Representation Learning (TAHyper)的新方法，该方法使用双曲空间(hyperbolic space)编码社交网络以减少混杂因素表示的失真，并设计治疗感知关系识别模块来识别个体及其邻居的治疗一致性，从而增强隐藏混杂因素的表示。在两个基准数据集上的广泛实验中，TAHyper展示了显著的优越性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by SIAM SDM'24",
      "pdf_url": "http://arxiv.org/pdf/2401.06557v1",
      "published_date": "2024-01-12 13:02:39 UTC",
      "updated_date": "2024-01-12 13:02:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:28:38.540800"
    },
    {
      "arxiv_id": "2401.06550v3",
      "title": "Multimodal Urban Areas of Interest Generation via Remote Sensing Imagery and Geographical Prior",
      "title_zh": "翻译失败",
      "authors": [
        "Chuanji Shi",
        "Yingying Zhang",
        "Jiaotuan Wang",
        "Xin Guo",
        "Qiqi Zhu"
      ],
      "abstract": "Urban area-of-interest (AOI) refers to an integrated urban functional zone\nwith defined polygonal boundaries. The rapid development of urban commerce has\nled to increasing demands for highly accurate and timely AOI data. However,\nexisting research primarily focuses on coarse-grained functional zones for\nurban planning or regional economic analysis, and often neglects the expiration\nof AOI in the real world. They fail to fulfill the precision demands of Mobile\nInternet Online-to-Offline (O2O) businesses. These businesses require accuracy\ndown to a specific community, school, or hospital. In this paper, we propose a\ncomprehensive end-to-end multimodal deep learning framework designed for\nsimultaneously detecting accurate AOI boundaries and validating the reliability\nof AOI by leveraging remote sensing imagery coupled with geographical prior,\ntitled AOITR. Unlike conventional AOI generation methods, such as the Road-cut\nmethod that segments road networks at various levels, our approach diverges\nfrom semantic segmentation algorithms that depend on pixel-level\nclassification. Instead, our AOITR begins by selecting a point-of-interest\n(POI) of specific category, and uses it to retrieve corresponding remote\nsensing imagery and geographical prior such as entrance POIs and road nodes.\nThis information helps to build a multimodal detection model based on\ntransformer encoder-decoder architecture to regress the AOI polygon.\nAdditionally, we utilize the dynamic features from human mobility, nearby POIs,\nand logistics addresses for AOI reliability evaluation via a cascaded network\nmodule. The experimental results reveal that our algorithm achieves a\nsignificant improvement on Intersection over Union (IoU) metric, surpassing\nprevious methods by a large margin.",
      "tldr_zh": "该论文针对城市兴趣区域（Urban Area-of-Interest, AOI）的生成问题，提出了一种端到端多模态深度学习框架 AOITR，以解决现有方法在精确性和实时性上不足，无法满足 Mobile Internet Online-to-Offline (O2O) 业务的精细需求。框架利用遥感图像和地理先验（如入口 Point-of-Interest, POI 和道路节点），从特定类别 POI 开始，构建基于 Transformer 编码器-解码器架构的检测模型来回归 AOI 多边形，并通过人类流动性、附近 POI 和物流地址等动态特征进行可靠性评估。实验结果显示，AOITR 在 Intersection over Union (IoU) 指标上大幅超过了现有方法，显著提高了 AOI 生成的准确性和实用性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T99",
        "I.4.9"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.06550v3",
      "published_date": "2024-01-12 12:54:30 UTC",
      "updated_date": "2024-02-08 06:23:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:28:51.870592"
    },
    {
      "arxiv_id": "2401.10279v1",
      "title": "A systematic review of geospatial location embedding approaches in large language models: A path to spatial AI systems",
      "title_zh": "翻译失败",
      "authors": [
        "Sean Tucker"
      ],
      "abstract": "Geospatial Location Embedding (GLE) helps a Large Language Model (LLM)\nassimilate and analyze spatial data. GLE emergence in Geospatial Artificial\nIntelligence (GeoAI) is precipitated by the need for deeper geospatial\nawareness in our complex contemporary spaces and the success of LLMs in\nextracting deep meaning in Generative AI. We searched Google Scholar, Science\nDirect, and arXiv for papers on geospatial location embedding and LLM and\nreviewed articles focused on gaining deeper spatial \"knowing\" through LLMs. We\nscreened 304 titles, 30 abstracts, and 18 full-text papers that reveal four GLE\nthemes - Entity Location Embedding (ELE), Document Location Embedding (DLE),\nSequence Location Embedding (SLE), and Token Location Embedding (TLE).\nSynthesis is tabular and narrative, including a dialogic conversation between\n\"Space\" and \"LLM.\" Though GLEs aid spatial understanding by superimposing\nspatial data, they emphasize the need to advance in the intricacies of spatial\nmodalities and generalized reasoning. GLEs signal the need for a Spatial\nFoundation/Language Model (SLM) that embeds spatial knowing within the model\narchitecture. The SLM framework advances Spatial Artificial Intelligence\nSystems (SPAIS), establishing a Spatial Vector Space (SVS) that maps to\nphysical space. The resulting spatially imbued Language Model is unique. It\nsimultaneously represents actual space and an AI-capable space, paving the way\nfor AI native geo storage, analysis, and multi-modality as the basis for\nSpatial Artificial Intelligence Systems (SPAIS).",
      "tldr_zh": "这篇论文对Geospatial Location Embedding (GLE) 在Large Language Models (LLM)中的应用进行了系统综述，旨在提升GeoAI领域对空间数据的理解和分析。研究者通过搜索Google Scholar、Science Direct和arXiv等数据库，筛选了304个标题、30个摘要和18篇全文，识别出四类GLE主题：Entity Location Embedding (ELE)、Document Location Embedding (DLE)、Sequence Location Embedding (SLE)和Token Location Embedding (TLE)。综述发现，虽然GLE能增强LLM的空间感知，但仍需改进空间模态处理和泛化推理，并提出Spatial Foundation/Language Model (SLM)框架，以建立Spatial Vector Space (SVS)并推进Spatial Artificial Intelligence Systems (SPAIS)的开发，最终实现AI原生空间存储、分析和多模态整合。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "20 pages, 11 figures, 3 appendices",
      "pdf_url": "http://arxiv.org/pdf/2401.10279v1",
      "published_date": "2024-01-12 12:43:33 UTC",
      "updated_date": "2024-01-12 12:43:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:29:01.939231"
    },
    {
      "arxiv_id": "2401.06541v1",
      "title": "Medical Dialogue Generation via Intuitive-then-Analytical Differential Diagnosis",
      "title_zh": "翻译失败",
      "authors": [
        "Kaishuai Xu",
        "Wenjun Hou",
        "Yi Cheng",
        "Jian Wang",
        "Wenjie Li"
      ],
      "abstract": "Medical dialogue systems have attracted growing research attention as they\nhave the potential to provide rapid diagnoses, treatment plans, and health\nconsultations. In medical dialogues, a proper diagnosis is crucial as it\nestablishes the foundation for future consultations. Clinicians typically\nemploy both intuitive and analytic reasoning to formulate a differential\ndiagnosis. This reasoning process hypothesizes and verifies a variety of\npossible diseases and strives to generate a comprehensive and rigorous\ndiagnosis. However, recent studies on medical dialogue generation have\noverlooked the significance of modeling a differential diagnosis, which hinders\nthe practical application of these systems. To address the above issue, we\npropose a medical dialogue generation framework with the\nIntuitive-then-Analytic Differential Diagnosis (IADDx). Our method starts with\na differential diagnosis via retrieval-based intuitive association and\nsubsequently refines it through a graph-enhanced analytic procedure. The\nresulting differential diagnosis is then used to retrieve medical knowledge and\nguide response generation. Experimental results on two datasets validate the\nefficacy of our method. Besides, we demonstrate how our framework assists both\nclinicians and patients in understanding the diagnostic process, for instance,\nby producing intermediate results and graph-based diagnosis paths.",
      "tldr_zh": "本研究针对医疗对话系统的诊断生成问题，提出了一种Intuitive-then-Analytical Differential Diagnosis (IADDx)框架，以模拟临床医生结合直觉和分析推理的过程。该框架首先通过检索-based直觉关联生成初步鉴别诊断，随后利用图增强分析过程对其进行完善，并以此指导医疗知识检索和响应生成。实验在两个数据集上验证了该方法的有效性，并展示了其在帮助临床医生和患者理解诊断过程方面的优势，例如通过提供中间结果和图-based诊断路径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Work in progress",
      "pdf_url": "http://arxiv.org/pdf/2401.06541v1",
      "published_date": "2024-01-12 12:35:19 UTC",
      "updated_date": "2024-01-12 12:35:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:29:13.725847"
    },
    {
      "arxiv_id": "2401.06538v1",
      "title": "Intelligent Data-Driven Architectural Features Orchestration for Network Slicing",
      "title_zh": "智能数据驱动的网络切片架构特征编排",
      "authors": [
        "Rodrigo Moreira",
        "Flavio de Oliveira Silva",
        "Tereza Cristina Melo de Brito Carvalho",
        "Joberto S. B. Martins"
      ],
      "abstract": "Network slicing is a crucial enabler and a trend for the Next Generation\nMobile Network (NGMN) and various other new systems like the Internet of\nVehicles (IoV) and Industrial IoT (IIoT). Orchestration and machine learning\nare key elements with a crucial role in the network-slicing processes since the\nNS process needs to orchestrate resources and functionalities, and machine\nlearning can potentially optimize the orchestration process. However, existing\nnetwork-slicing architectures lack the ability to define intelligent approaches\nto orchestrate features and resources in the slicing process. This paper\ndiscusses machine learning-based orchestration of features and capabilities in\nnetwork slicing architectures. Initially, the slice resource orchestration and\nallocation in the slicing planning, configuration, commissioning, and operation\nphases are analyzed. In sequence, we highlight the need for optimized\narchitectural feature orchestration and recommend using ML-embed agents,\nfederated learning intrinsic mechanisms for knowledge acquisition, and a\ndata-driven approach embedded in the network slicing architecture. We further\ndevelop an architectural features orchestration case embedded in the SFI2\nnetwork slicing architecture. An attack prevention security mechanism is\ndeveloped for the SFI2 architecture using distributed embedded and cooperating\nML agents. The case presented illustrates the architectural feature's\norchestration process and benefits, highlighting its importance for the network\nslicing process.",
      "tldr_zh": "这篇论文探讨了基于机器学习的智能数据驱动方法，用于网络切片(Network Slicing)架构特征的编排，以优化资源和功能分配。论文首先分析了网络切片过程的各个阶段，包括规划、配置、部署和操作，并强调了使用 ML-embedded agents 和 Federated Learning 等机制来实现高效编排的需求。最终，作者在 SFI2 架构中开发了一个案例，引入分布式嵌入式 ML 代理的攻击预防安全机制，展示了这种数据驱动编排方法的益处和重要性。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG",
        "I.2.11; C.2.1; I.2.1"
      ],
      "primary_category": "cs.NI",
      "comment": "12 pages, 6 figures, Conference ADVANCE 24 - International Workshop\n  on ADVANCEs in ICT Infrastructures and Services - February 26--29, 2024 -\n  Hanoi, Vietnam",
      "pdf_url": "http://arxiv.org/pdf/2401.06538v1",
      "published_date": "2024-01-12 12:32:36 UTC",
      "updated_date": "2024-01-12 12:32:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:29:26.469917"
    },
    {
      "arxiv_id": "2401.06528v1",
      "title": "PCB-Vision: A Multiscene RGB-Hyperspectral Benchmark Dataset of Printed Circuit Boards",
      "title_zh": "PCB-Vision：印刷电路板的多场景 RGB-超光谱基准数据集",
      "authors": [
        "Elias Arbash",
        "Margret Fuchs",
        "Behnood Rasti",
        "Sandra Lorenz",
        "Pedram Ghamisi",
        "Richard Gloaguen"
      ],
      "abstract": "Addressing the critical theme of recycling electronic waste (E-waste), this\ncontribution is dedicated to developing advanced automated data processing\npipelines as a basis for decision-making and process control. Aligning with the\nbroader goals of the circular economy and the United Nations (UN) Sustainable\nDevelopment Goals (SDG), our work leverages non-invasive analysis methods\nutilizing RGB and hyperspectral imaging data to provide both quantitative and\nqualitative insights into the E-waste stream composition for optimizing\nrecycling efficiency. In this paper, we introduce 'PCB-Vision'; a pioneering\nRGB-hyperspectral printed circuit board (PCB) benchmark dataset, comprising 53\nRGB images of high spatial resolution paired with their corresponding high\nspectral resolution hyperspectral data cubes in the visible and near-infrared\n(VNIR) range. Grounded in open science principles, our dataset provides a\ncomprehensive resource for researchers through high-quality ground truths,\nfocusing on three primary PCB components: integrated circuits (IC), capacitors,\nand connectors. We provide extensive statistical investigations on the proposed\ndataset together with the performance of several state-of-the-art (SOTA)\nmodels, including U-Net, Attention U-Net, Residual U-Net, LinkNet, and\nDeepLabv3+. By openly sharing this multi-scene benchmark dataset along with the\nbaseline codes, we hope to foster transparent, traceable, and comparable\ndevelopments of advanced data processing across various scientific communities,\nincluding, but not limited to, computer vision and remote sensing. Emphasizing\nour commitment to supporting a collaborative and inclusive scientific\ncommunity, all materials, including code, data, ground truth, and masks, will\nbe accessible at https://github.com/hifexplo/PCBVision.",
      "tldr_zh": "本研究针对电子废弃物（E-waste）回收问题，引入了 PCB-Vision 数据集，这是一个多场景 RGB-高光谱基准数据集，包含 53 张高分辨率 RGB 图像及其对应的可见光和近红外（VNIR）范围高光谱数据立方体，焦点在于印刷电路板（PCB）的关键组件如 integrated circuits (IC)、capacitors 和 connectors，并提供高质量的地面真实数据。利用非侵入式分析方法，该数据集支持决策和过程控制的优化，并通过测试 state-of-the-art (SOTA) 模型如 U-Net、Attention U-Net 和 DeepLabv3+，展示了其在提升回收效率方面的潜力。数据集及其基准代码已开源在 GitHub（https://github.com/hifexplo/PCBVision），旨在促进计算机视觉、遥感和相关领域的协作研究，支持循环经济和联合国可持续发展目标（SDG）。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06528v1",
      "published_date": "2024-01-12 12:00:26 UTC",
      "updated_date": "2024-01-12 12:00:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:29:39.783887"
    },
    {
      "arxiv_id": "2401.06513v1",
      "title": "ML-On-Rails: Safeguarding Machine Learning Models in Software Systems A Case Study",
      "title_zh": "翻译失败",
      "authors": [
        "Hala Abdelkader",
        "Mohamed Abdelrazek",
        "Scott Barnett",
        "Jean-Guy Schneider",
        "Priya Rani",
        "Rajesh Vasa"
      ],
      "abstract": "Machine learning (ML), especially with the emergence of large language models\n(LLMs), has significantly transformed various industries. However, the\ntransition from ML model prototyping to production use within software systems\npresents several challenges. These challenges primarily revolve around ensuring\nsafety, security, and transparency, subsequently influencing the overall\nrobustness and trustworthiness of ML models. In this paper, we introduce\nML-On-Rails, a protocol designed to safeguard ML models, establish a\nwell-defined endpoint interface for different ML tasks, and clear communication\nbetween ML providers and ML consumers (software engineers). ML-On-Rails\nenhances the robustness of ML models via incorporating detection capabilities\nto identify unique challenges specific to production ML. We evaluated the\nML-On-Rails protocol through a real-world case study of the MoveReminder\napplication. Through this evaluation, we emphasize the importance of\nsafeguarding ML models in production.",
      "tldr_zh": "机器学习（ML），尤其是大型语言模型（LLMs）的兴起，极大地改变了多个行业，但从原型到生产部署面临安全、安全性和透明度的挑战，导致模型的稳健性和可信度受影响。  \n本文引入了 ML-On-Rails 协议，该协议通过建立明确的端点接口、促进 ML 提供者和消费者（软件工程师）之间的沟通，并整合检测能力来识别生产环境中独特的挑战，从而增强 ML 模型的保护和可靠性。  \n通过对 MoveReminder 应用的真实案例研究，我们评估了该协议的效果，强调了在生产环境中保障 ML 模型的重要性，以提高整体系统的稳健性。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06513v1",
      "published_date": "2024-01-12 11:27:15 UTC",
      "updated_date": "2024-01-12 11:27:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:29:51.613945"
    },
    {
      "arxiv_id": "2401.09473v1",
      "title": "Business and ethical concerns in domestic Conversational Generative AI-empowered multi-robot systems",
      "title_zh": "翻译失败",
      "authors": [
        "Rebekah Rousi",
        "Hooman Samani",
        "Niko Mäkitalo",
        "Ville Vakkuri",
        "Simo Linkola",
        "Kai-Kristian Kemell",
        "Paulius Daubaris",
        "Ilenia Fronza",
        "Tommi Mikkonen",
        "Pekka Abrahamsson"
      ],
      "abstract": "Business and technology are intricately connected through logic and design.\nThey are equally sensitive to societal changes and may be devastated by\nscandal. Cooperative multi-robot systems (MRSs) are on the rise, allowing\nrobots of different types and brands to work together in diverse contexts.\nGenerative artificial intelligence has been a dominant topic in recent\nartificial intelligence (AI) discussions due to its capacity to mimic humans\nthrough the use of natural language and the production of media, including deep\nfakes. In this article, we focus specifically on the conversational aspects of\ngenerative AI, and hence use the term Conversational Generative artificial\nintelligence (CGI). Like MRSs, CGIs have enormous potential for revolutionizing\nprocesses across sectors and transforming the way humans conduct business. From\na business perspective, cooperative MRSs alone, with potential conflicts of\ninterest, privacy practices, and safety concerns, require ethical examination.\nMRSs empowered by CGIs demand multi-dimensional and sophisticated methods to\nuncover imminent ethical pitfalls. This study focuses on ethics in\nCGI-empowered MRSs while reporting the stages of developing the MORUL model.",
      "tldr_zh": "该论文探讨了在国内环境中，Conversational Generative AI (CGI) 赋能的多机器人系统 (MRSs) 所面临的商业和伦理问题，强调这些系统虽有革命性潜力，但可能引发利益冲突、隐私泄露和安全风险。作者指出，CGI 的对话能力能模仿人类并改造商业流程，但需要多维方法来揭示潜在的伦理陷阱。论文介绍了 MORUL 模型的开发阶段，作为一种应对 CGI 赋能 MRSs 伦理挑战的框架。总体而言，该研究为确保 AI 系统的可信性和社会责任提供了重要见解。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "15 pages, 4 figures, International Conference on Software Business",
      "pdf_url": "http://arxiv.org/pdf/2401.09473v1",
      "published_date": "2024-01-12 11:05:32 UTC",
      "updated_date": "2024-01-12 11:05:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:30:02.186036"
    },
    {
      "arxiv_id": "2401.06506v3",
      "title": "Frequency Masking for Universal Deepfake Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Chandler Timm Doloriel",
        "Ngai-Man Cheung"
      ],
      "abstract": "We study universal deepfake detection. Our goal is to detect synthetic images\nfrom a range of generative AI approaches, particularly from emerging ones which\nare unseen during training of the deepfake detector. Universal deepfake\ndetection requires outstanding generalization capability. Motivated by recently\nproposed masked image modeling which has demonstrated excellent generalization\nin self-supervised pre-training, we make the first attempt to explore masked\nimage modeling for universal deepfake detection. We study spatial and frequency\ndomain masking in training deepfake detectors. Based on empirical analysis, we\npropose a novel deepfake detector via frequency masking. Our focus on frequency\ndomain is different from the majority, which primarily target spatial domain\ndetection. Our comparative analyses reveal substantial performance gains over\nexisting methods. Code and models are publicly available.",
      "tldr_zh": "该研究针对通用深度伪造检测，旨在识别各种生成 AI 方法产生的合成图像，特别是训练时未见过的，以提升模型的泛化能力。受 masked image modeling 的启发，研究者探索了空间和频率域掩码，最终提出了一种基于 frequency masking 的新型检测器，该方法专注于频率域以区别于主流的空间域方法。实验结果显示，该检测器在性能上比现有方法有显著提升，并已公开代码和模型。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to IEEE ICASSP-2024",
      "pdf_url": "http://arxiv.org/pdf/2401.06506v3",
      "published_date": "2024-01-12 11:02:12 UTC",
      "updated_date": "2024-01-17 07:44:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:30:13.910920"
    },
    {
      "arxiv_id": "2401.06503v1",
      "title": "Improving the Detection of Small Oriented Objects in Aerial Images",
      "title_zh": "翻译失败",
      "authors": [
        "Chandler Timm C. Doloriel",
        "Rhandley D. Cajote"
      ],
      "abstract": "Small oriented objects that represent tiny pixel-area in large-scale aerial\nimages are difficult to detect due to their size and orientation. Existing\noriented aerial detectors have shown promising results but are mainly focused\non orientation modeling with less regard to the size of the objects. In this\nwork, we proposed a method to accurately detect small oriented objects in\naerial images by enhancing the classification and regression tasks of the\noriented object detection model. We designed the Attention-Points Network\nconsisting of two losses: Guided-Attention Loss (GALoss) and Box-Points Loss\n(BPLoss). GALoss uses an instance segmentation mask as ground-truth to learn\nthe attention features needed to improve the detection of small objects. These\nattention features are then used to predict box points for BPLoss, which\ndetermines the points' position relative to the target oriented bounding box.\nExperimental results show the effectiveness of our Attention-Points Network on\na standard oriented aerial dataset with small object instances (DOTA-v1.5) and\non a maritime-related dataset (HRSC2016). The code is publicly available.",
      "tldr_zh": "这篇论文针对航空图像中小型定向对象（small oriented objects）的检测难题，提出了一种改进方法，以提升分类和回归任务的准确性。作者设计了Attention-Points Network，包括Guided-Attention Loss (GALoss)和Box-Points Loss (BPLoss)：GALoss利用实例分割掩码作为ground-truth学习注意力特征，以更好地识别小对象，而BPLoss则基于这些特征预测相对于目标oriented bounding box的点位。实验结果在DOTA-v1.5和HRSC2016数据集上证明了该方法的有效性，代码已公开可用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "C. T. C. Doloriel and R. D. Cajote, \"Improving the Detection of Small\n  Oriented Objects in Aerial Images,\" 2023 IEEE/CVF Winter Conference on\n  Applications of Computer Vision Workshops (WACVW), Waikoloa, HI, USA, 2023,\n  pp. 176-185, doi: 10.1109/WACVW58289.2023.00023",
      "pdf_url": "http://arxiv.org/pdf/2401.06503v1",
      "published_date": "2024-01-12 11:00:07 UTC",
      "updated_date": "2024-01-12 11:00:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:30:26.966541"
    },
    {
      "arxiv_id": "2401.12803v1",
      "title": "Enhancements for 5G NR PRACH Reception: An AI/ML Approach",
      "title_zh": "5G NR PRACH 接收的增强：一种 AI/ML 方法",
      "authors": [
        "Rohit Singh",
        "Anil Kumar Yerrapragada",
        "Jeeva Keshav S",
        "Radha Krishna Ganti"
      ],
      "abstract": "Random Access is an important step in enabling the initial attachment of a\nUser Equipment (UE) to a Base Station (gNB). The UE identifies itself by\nembedding a Preamble Index (RAPID) in the phase rotation of a known base\nsequence, which it transmits on the Physical Random Access Channel (PRACH). The\nsignal on the PRACH also enables the estimation of propagation delay, often\nknown as Timing Advance (TA), which is induced by virtue of the UE's position.\nTraditional receivers estimate the RAPID and TA using correlation-based\ntechniques. This paper presents an alternative receiver approach that uses\nAI/ML models, wherein two neural networks are proposed, one for the RAPID and\none for the TA. Different from other works, these two models can run in\nparallel as opposed to sequentially. Experiments with both simulated data and\nover-the-air hardware captures highlight the improved performance of the\nproposed AI/ML-based techniques compared to conventional correlation methods.",
      "tldr_zh": "本论文针对5G NR PRACH接收的增强，提出了一种基于AI/ML的方法来优化随机接入过程，其中UE通过在PRACH上嵌入RAPID来标识自身，并估计传播延迟TA。不同于传统基于相关性的估计技术，本文设计了两个神经网络，分别处理RAPID和TA的检测，并支持并行运行，从而提高了效率。实验结果显示，该AI/ML方法在使用模拟数据和实际硬件捕获时，比传统方法性能提升明显。",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG",
        "eess.SP",
        "math.IT"
      ],
      "primary_category": "cs.IT",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12803v1",
      "published_date": "2024-01-12 10:44:23 UTC",
      "updated_date": "2024-01-12 10:44:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:30:38.655344"
    },
    {
      "arxiv_id": "2401.06493v2",
      "title": "Expected Shapley-Like Scores of Boolean Functions: Complexity and Applications to Probabilistic Databases",
      "title_zh": "翻译失败",
      "authors": [
        "Pratik Karmakar",
        "Mikaël Monet",
        "Pierre Senellart",
        "Stéphane Bressan"
      ],
      "abstract": "Shapley values, originating in game theory and increasingly prominent in\nexplainable AI, have been proposed to assess the contribution of facts in query\nanswering over databases, along with other similar power indices such as\nBanzhaf values. In this work we adapt these Shapley-like scores to\nprobabilistic settings, the objective being to compute their expected value. We\nshow that the computations of expected Shapley values and of the expected\nvalues of Boolean functions are interreducible in polynomial time, thus\nobtaining the same tractability landscape. We investigate the specific\ntractable case where Boolean functions are represented as deterministic\ndecomposable circuits, designing a polynomial-time algorithm for this setting.\nWe present applications to probabilistic databases through database provenance,\nand an effective implementation of this algorithm within the ProvSQL system,\nwhich experimentally validates its feasibility over a standard benchmark.",
      "tldr_zh": "这篇论文将 Shapley values 和类似指标（如 Banzhaf values）扩展到概率设置中，目标是计算这些分数的期望值，以评估概率数据库中事实的贡献。作者证明了计算期望 Shapley values 与布尔函数期望值的计算在多项式时间内可归约，并为布尔函数表示为 deterministic decomposable circuits 的情况设计了高效算法。论文通过数据库 provenance 应用于 probabilistic databases，并使用 ProvSQL 系统进行了实验验证，证明了该方法的实用性。",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.CC"
      ],
      "primary_category": "cs.DB",
      "comment": "27 pages, including 20 pages of maintext. This is the authors'\n  version of the corresponding PODS'2024 article",
      "pdf_url": "http://arxiv.org/pdf/2401.06493v2",
      "published_date": "2024-01-12 10:34:31 UTC",
      "updated_date": "2024-04-16 12:16:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:30:50.511508"
    },
    {
      "arxiv_id": "2401.06831v1",
      "title": "A Survey on the Applications of Frontier AI, Foundation Models, and Large Language Models to Intelligent Transportation Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Mohamed R. Shoaib",
        "Heba M. Emara",
        "Jun Zhao"
      ],
      "abstract": "This survey paper explores the transformative influence of frontier AI,\nfoundation models, and Large Language Models (LLMs) in the realm of Intelligent\nTransportation Systems (ITS), emphasizing their integral role in advancing\ntransportation intelligence, optimizing traffic management, and contributing to\nthe realization of smart cities. Frontier AI refers to the forefront of AI\ntechnology, encompassing the latest advancements, innovations, and experimental\ntechniques in the field, especially AI foundation models and LLMs. Foundation\nmodels, like GPT-4, are large, general-purpose AI models that provide a base\nfor a wide range of applications. They are characterized by their versatility\nand scalability. LLMs are obtained from finetuning foundation models with a\nspecific focus on processing and generating natural language. They excel in\ntasks like language understanding, text generation, translation, and\nsummarization. By leveraging vast textual data, including traffic reports and\nsocial media interactions, LLMs extract critical insights, fostering the\nevolution of ITS. The survey navigates the dynamic synergy between LLMs and\nITS, delving into applications in traffic management, integration into\nautonomous vehicles, and their role in shaping smart cities. It provides\ninsights into ongoing research, innovations, and emerging trends, aiming to\ninspire collaboration at the intersection of language, intelligence, and\nmobility for safer, more efficient, and sustainable transportation systems. The\npaper further surveys interactions between LLMs and various aspects of ITS,\nexploring roles in traffic management, facilitating autonomous vehicles, and\ncontributing to smart city development, while addressing challenges brought by\nfrontier AI and foundation models. This paper offers valuable inspiration for\nfuture research and innovation in the transformative domain of intelligent\ntransportation.",
      "tldr_zh": "这篇调查论文探讨了Frontier AI、Foundation Models（如GPT-4）和Large Language Models (LLMs)在Intelligent Transportation Systems (ITS)中的应用，强调这些技术如何提升交通智能、优化交通管理和推动智能城市发展。论文通过分析LLMs处理交通报告和社会媒体数据来提取关键洞见，涵盖了在交通管理、自动驾驶车辆整合以及智能城市建设中的具体作用和创新趋势。最终，它指出了面临的挑战，如技术局限性，并为未来研究提供灵感，以实现更安全、高效和可持续的交通系统。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "This paper appears in International Conference on Computer and\n  Applications (ICCA) 2023",
      "pdf_url": "http://arxiv.org/pdf/2401.06831v1",
      "published_date": "2024-01-12 10:29:48 UTC",
      "updated_date": "2024-01-12 10:29:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:31:02.298029"
    },
    {
      "arxiv_id": "2401.06830v1",
      "title": "RecSys Challenge 2023: From data preparation to prediction, a simple, efficient, robust and scalable solution",
      "title_zh": "翻译失败",
      "authors": [
        "Maxime Manderlier",
        "Fabian Lecron"
      ],
      "abstract": "The RecSys Challenge 2023, presented by ShareChat, consists to predict if an\nuser will install an application on his smartphone after having seen\nadvertising impressions in ShareChat & Moj apps. This paper presents the\nsolution of 'Team UMONS' to this challenge, giving accurate results (our best\nscore is 6.622686) with a relatively small model that can be easily implemented\nin different production configurations. Our solution scales well when\nincreasing the dataset size and can be used with datasets containing missing\nvalues.",
      "tldr_zh": "这篇论文介绍了 'Team UMONS' 在 RecSys Challenge 2023 中的解决方案，该挑战旨在预测用户在 ShareChat 和 Moj 应用中看到广告后是否会安装应用。他们的方法从 data preparation 到 prediction 采用了一个简单、高效、鲁棒且可扩展的模型，能够处理包含缺失值的数据集并在规模增加时保持性能。实验结果显示，该方案取得了 6.622686 的最佳分数，便于在不同生产环境中实施。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06830v1",
      "published_date": "2024-01-12 10:14:10 UTC",
      "updated_date": "2024-01-12 10:14:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:31:13.949564"
    },
    {
      "arxiv_id": "2401.06477v4",
      "title": "Kun: Answer Polishment for Chinese Self-Alignment with Instruction Back-Translation",
      "title_zh": "翻译失败",
      "authors": [
        "Tianyu Zheng",
        "Shuyue Guo",
        "Xingwei Qu",
        "Jiawei Guo",
        "Xinrun Du",
        "Qi Jia",
        "Chenghua Lin",
        "Wenhao Huang",
        "Jie Fu",
        "Ge Zhang"
      ],
      "abstract": "In this paper, we introduce Kun, a novel approach for creating high-quality\ninstruction-tuning datasets for large language models (LLMs) without relying on\nmanual annotations. Adapting a self-training algorithm based on instruction\nback-translation and answer polishment, Kun leverages unlabelled data from\ndiverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial\ndataset of over a million Chinese instructional data points. This approach\nsignificantly deviates from traditional methods by using a self-curation\nprocess to refine and select the most effective instruction-output pairs. Our\nexperiments with the 6B-parameter Yi model across various benchmarks\ndemonstrate Kun's robustness and scalability. Our method's core contributions\nlie in its algorithmic advancement, which enhances data retention and clarity,\nand its innovative data generation approach that substantially reduces the\nreliance on costly and time-consuming manual annotations. This methodology\npresents a scalable and efficient solution for improving the\ninstruction-following capabilities of LLMs, with significant implications for\ntheir application across diverse fields. The code and dataset can be found at\nhttps://github.com/Zheng0428/COIG-Kun",
      "tldr_zh": "本论文提出了一种名为 Kun 的方法，用于生成高质量的中文指令调整数据集，而无需依赖手动注释。它基于自训练算法，通过 instruction back-translation 和 answer polishment 技术，从未标记数据（如 Wudao, Wanjuan 和 SkyPile）创建了超过一百万的指令输出对，并通过自 curation 过程优化数据保留和清晰度。实验在 6B 参数的 Yi model 上验证了 Kun 的鲁棒性和可扩展性，主要贡献在于减少了对手动注释的依赖，并为提升 LLMs 的指令遵循能力提供了高效、可扩展的解决方案。代码和数据集可访问 https://github.com/Zheng0428/COIG-Kun。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 12 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.06477v4",
      "published_date": "2024-01-12 09:56:57 UTC",
      "updated_date": "2024-11-05 16:02:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:31:27.636572"
    },
    {
      "arxiv_id": "2401.06829v1",
      "title": "Cross-Attention Watermarking of Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Folco Bertini Baldassini",
        "Huy H. Nguyen",
        "Ching-Chung Chang",
        "Isao Echizen"
      ],
      "abstract": "A new approach to linguistic watermarking of language models is presented in\nwhich information is imperceptibly inserted into the output text while\npreserving its readability and original meaning. A cross-attention mechanism is\nused to embed watermarks in the text during inference. Two methods using\ncross-attention are presented that minimize the effect of watermarking on the\nperformance of a pretrained model. Exploration of different training strategies\nfor optimizing the watermarking and of the challenges and implications of\napplying this approach in real-world scenarios clarified the tradeoff between\nwatermark robustness and text quality. Watermark selection substantially\naffects the generated output for high entropy sentences. This proactive\nwatermarking approach has potential application in future model development.",
      "tldr_zh": "本研究提出了一种基于 cross-attention 的水印技术，用于在大型语言模型的输出文本中隐蔽插入信息，同时保持文本的可读性和原意。两种方法通过 cross-attention 机制在推理过程中嵌入水mark，以最小化对预训练模型性能的影响，并探讨了不同训练策略来优化水mark 鲁棒性。研究发现，水mark 选择会显著影响高熵句子的生成输出，并揭示了水mark 鲁棒性与文本质量之间的权衡，这种主动水mark 技术在未来模型开发中具有潜在应用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "5 pages, 3 figures. Accepted to ICASSP 2024",
      "pdf_url": "http://arxiv.org/pdf/2401.06829v1",
      "published_date": "2024-01-12 09:39:50 UTC",
      "updated_date": "2024-01-12 09:39:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:31:38.539729"
    },
    {
      "arxiv_id": "2401.06471v1",
      "title": "A Brain-inspired Computational Model for Human-like Concept Learning",
      "title_zh": "一种大脑启发的计算模型用于人类般的概念学习",
      "authors": [
        "Yuwei Wang",
        "Yi Zeng"
      ],
      "abstract": "Concept learning is a fundamental aspect of human cognition and plays a\ncritical role in mental processes such as categorization, reasoning, memory,\nand decision-making. Researchers across various disciplines have shown\nconsistent interest in the process of concept acquisition in individuals. To\nelucidate the mechanisms involved in human concept learning, this study\nexamines the findings from computational neuroscience and cognitive psychology.\nThese findings indicate that the brain's representation of concepts relies on\ntwo essential components: multisensory representation and text-derived\nrepresentation. These two types of representations are coordinated by a\nsemantic control system, ultimately leading to the acquisition of concepts.\nDrawing inspiration from this mechanism, the study develops a human-like\ncomputational model for concept learning based on spiking neural networks. By\neffectively addressing the challenges posed by diverse sources and imbalanced\ndimensionality of the two forms of concept representations, the study\nsuccessfully attains human-like concept representations. Tests involving\nsimilar concepts demonstrate that our model, which mimics the way humans learn\nconcepts, yields representations that closely align with human cognition.",
      "tldr_zh": "这篇论文基于计算神经科学和认知心理学的发现，提出一个受脑部机制启发的计算模型，用于实现人类般的概念学习。该模型利用 Spiking Neural Networks 处理多感官表示和文本派生表示，并通过语义控制系统协调两者，以应对不同来源和不平衡维度的挑战。测试结果表明，该模型生成的概念表示与人类认知高度一致，尤其在类似概念的评估中。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06471v1",
      "published_date": "2024-01-12 09:32:51 UTC",
      "updated_date": "2024-01-12 09:32:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:31:50.538593"
    },
    {
      "arxiv_id": "2401.06466v1",
      "title": "PersianMind: A Cross-Lingual Persian-English Large Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Pedram Rostami",
        "Ali Salemi",
        "Mohammad Javad Dousti"
      ],
      "abstract": "Large language models demonstrate remarkable proficiency in various\nlinguistic tasks and have extensive knowledge across various domains. Although\nthey perform best in English, their ability in other languages is notable too.\nIn contrast, open-source models, such as LLaMa, are primarily trained on\nEnglish datasets, resulting in poor performance in non-English languages. In\nthis paper, we introduce PersianMind, an open-source bilingual large language\nmodel which demonstrates comparable performance to closed-source GPT-3.5-turbo\nin the Persian language. By expanding LLaMa2's vocabulary with 10,000 Persian\ntokens and training it on a dataset comprising nearly 2 billion Persian tokens,\nwe show that our approach preserves the model's English knowledge and employs\ntransfer learning to excel at transferring task knowledge from one language to\nanother.",
      "tldr_zh": "该研究引入了 PersianMind，一种开源的双语 Large Language Model，支持波斯语（Persian）和英语，旨在提升非英语语言的性能。研究者通过扩展 LLaMa2 的词汇表（添加 10,000 个波斯语 tokens）并在近 2 亿波斯语 tokens 的数据集上训练，成功保留了模型的英语知识。结果表明，PersianMind 在波斯语任务上的表现可与闭源模型 GPT-3.5-turbo 相媲美，并通过 transfer learning 实现了任务知识在语言间的有效转移。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06466v1",
      "published_date": "2024-01-12 09:24:10 UTC",
      "updated_date": "2024-01-12 09:24:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:32:04.298631"
    },
    {
      "arxiv_id": "2401.06465v1",
      "title": "Sanity Checks Revisited: An Exploration to Repair the Model Parameter Randomisation Test",
      "title_zh": "翻译失败",
      "authors": [
        "Anna Hedström",
        "Leander Weber",
        "Sebastian Lapuschkin",
        "Marina MC Höhne"
      ],
      "abstract": "The Model Parameter Randomisation Test (MPRT) is widely acknowledged in the\neXplainable Artificial Intelligence (XAI) community for its well-motivated\nevaluative principle: that the explanation function should be sensitive to\nchanges in the parameters of the model function. However, recent works have\nidentified several methodological caveats for the empirical interpretation of\nMPRT. To address these caveats, we introduce two adaptations to the original\nMPRT -- Smooth MPRT and Efficient MPRT, where the former minimises the impact\nthat noise has on the evaluation results through sampling and the latter\ncircumvents the need for biased similarity measurements by re-interpreting the\ntest through the explanation's rise in complexity, after full parameter\nrandomisation. Our experimental results demonstrate that these proposed\nvariants lead to improved metric reliability, thus enabling a more trustworthy\napplication of XAI methods.",
      "tldr_zh": "本研究重新审视了模型参数随机化测试(MPRT)，旨在修复其在可解释人工智能(XAI)领域中的方法论缺陷，这些缺陷导致了对模型参数变化的解释敏感性评估不可靠。作者提出了两个改进版本：Smooth MPRT，通过采样技术最小化噪声对评估结果的影响；以及Efficient MPRT，通过重新解释测试焦点为解释的复杂度上升，从而避免偏见的相似性测量。实验结果显示，这些变体显著提高了指标的可靠性和XAI方法的整体可信度。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "stat.ME"
      ],
      "primary_category": "cs.AI",
      "comment": "19 pages, 12 figures, NeurIPS XAIA 2023",
      "pdf_url": "http://arxiv.org/pdf/2401.06465v1",
      "published_date": "2024-01-12 09:21:18 UTC",
      "updated_date": "2024-01-12 09:21:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:32:14.931564"
    },
    {
      "arxiv_id": "2401.06461v5",
      "title": "Between Lines of Code: Unraveling the Distinct Patterns of Machine and Human Programmers",
      "title_zh": "翻译失败",
      "authors": [
        "Yuling Shi",
        "Hongyu Zhang",
        "Chengcheng Wan",
        "Xiaodong Gu"
      ],
      "abstract": "Large language models have catalyzed an unprecedented wave in code\ngeneration. While achieving significant advances, they blur the distinctions\nbetween machine- and human-authored source code, causing integrity and\nauthenticity issues of software artifacts. Previous methods such as DetectGPT\nhave proven effective in discerning machine-generated texts, but they do not\nidentify and harness the unique patterns of machine-generated code. Thus, its\napplicability falters when applied to code. In this paper, we carefully study\nthe specific patterns that characterize machine- and human-authored code.\nThrough a rigorous analysis of code attributes such as lexical diversity,\nconciseness, and naturalness, we expose unique patterns inherent to each\nsource. We particularly notice that the syntactic segmentation of code is a\ncritical factor in identifying its provenance. Based on our findings, we\npropose DetectCodeGPT, a novel method for detecting machine-generated code,\nwhich improves DetectGPT by capturing the distinct stylized patterns of code.\nDiverging from conventional techniques that depend on external LLMs for\nperturbations, DetectCodeGPT perturbs the code corpus by strategically\ninserting spaces and newlines, ensuring both efficacy and efficiency.\nExperiment results show that our approach significantly outperforms\nstate-of-the-art techniques in detecting machine-generated code.",
      "tldr_zh": "这项研究探讨了大型语言模型（Large Language Models）在代码生成中模糊机器生成代码和人类生成代码界限的问题，分析了代码属性如词汇多样性、简洁性和自然性等独特模式，特别是代码的语法分割。作者提出DetectCodeGPT，一种改进DetectGPT的方法，通过策略性地插入空格和换行来扰动代码语料，而非依赖外部LLM，从而更有效地捕获机器生成代码的风格特征。实验结果显示，DetectCodeGPT在检测机器生成代码方面显著优于现有技术，为提升软件完整性和真实性提供了新途径。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted by the 47th International Conference on Software Engineering\n  (ICSE 2025). Code available at https://github.com/YerbaPage/DetectCodeGPT",
      "pdf_url": "http://arxiv.org/pdf/2401.06461v5",
      "published_date": "2024-01-12 09:15:20 UTC",
      "updated_date": "2024-07-30 09:26:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:32:25.846585"
    },
    {
      "arxiv_id": "2401.06437v1",
      "title": "3D-PreMise: Can Large Language Models Generate 3D Shapes with Sharp Features and Parametric Control?",
      "title_zh": "翻译失败",
      "authors": [
        "Zeqing Yuan",
        "Haoxuan Lan",
        "Qiang Zou",
        "Junbo Zhao"
      ],
      "abstract": "Recent advancements in implicit 3D representations and generative models have\nmarkedly propelled the field of 3D object generation forward. However, it\nremains a significant challenge to accurately model geometries with defined\nsharp features under parametric controls, which is crucial in fields like\nindustrial design and manufacturing. To bridge this gap, we introduce a\nframework that employs Large Language Models (LLMs) to generate text-driven 3D\nshapes, manipulating 3D software via program synthesis. We present 3D-PreMise,\na dataset specifically tailored for 3D parametric modeling of industrial\nshapes, designed to explore state-of-the-art LLMs within our proposed pipeline.\nOur work reveals effective generation strategies and delves into the\nself-correction capabilities of LLMs using a visual interface. Our work\nhighlights both the potential and limitations of LLMs in 3D parametric modeling\nfor industrial applications.",
      "tldr_zh": "本研究探讨Large Language Models (LLMs)是否能生成具有锐利特征和参数控制的3D形状，以解决工业设计中精确建模的挑战。论文提出3D-PreMise框架，利用LLMs通过程序合成操纵3D软件，生成文本驱动的3D形状，并构建了针对工业形状的3D-PreMise数据集来评估该框架。结果显示，LLMs展示了有效的生成策略和自校正能力，但也暴露了在3D参数建模中的潜在局限性。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.GR",
      "comment": "10 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.06437v1",
      "published_date": "2024-01-12 08:07:52 UTC",
      "updated_date": "2024-01-12 08:07:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:32:40.228909"
    },
    {
      "arxiv_id": "2401.06436v1",
      "title": "Improving Graph Convolutional Networks with Transformer Layer in social-based items recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Thi Linh Hoang",
        "Tuan Dung Pham",
        "Viet Cuong Ta"
      ],
      "abstract": "In this work, we have proposed an approach for improving the GCN for\npredicting ratings in social networks. Our model is expanded from the standard\nmodel with several layers of transformer architecture. The main focus of the\npaper is on the encoder architecture for node embedding in the network. Using\nthe embedding layer from the graph-based convolution layer, the attention\nmechanism could rearrange the feature space to get a more efficient embedding\nfor the downstream task. The experiments showed that our proposed architecture\nachieves better performance than GCN on the traditional link prediction task.",
      "tldr_zh": "本研究提出了一种改进图卷积网络(GCN)的模型，用于社交网络中基于物品的推荐任务，通过在标准GCN基础上添加Transformer层来提升性能。该模型重点优化了编码器架构，利用节点嵌入层和注意力机制(attention mechanism)重新排列特征空间，以获得更有效的下游任务嵌入。实验结果表明，该架构在传统链接预测任务上比GCN表现出更好的性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06436v1",
      "published_date": "2024-01-12 08:07:09 UTC",
      "updated_date": "2024-01-12 08:07:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:32:49.795761"
    },
    {
      "arxiv_id": "2401.06431v2",
      "title": "Human-AI Collaborative Essay Scoring: A Dual-Process Framework with LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Changrong Xiao",
        "Wenxing Ma",
        "Qingping Song",
        "Sean Xin Xu",
        "Kunpeng Zhang",
        "Yufang Wang",
        "Qi Fu"
      ],
      "abstract": "Receiving timely and personalized feedback is essential for second-language\nlearners, especially when human instructors are unavailable. This study\nexplores the effectiveness of Large Language Models (LLMs), including both\nproprietary and open-source models, for Automated Essay Scoring (AES). Through\nextensive experiments with public and private datasets, we find that while LLMs\ndo not surpass conventional state-of-the-art (SOTA) grading models in\nperformance, they exhibit notable consistency, generalizability, and\nexplainability. We propose an open-source LLM-based AES system, inspired by the\ndual-process theory. Our system offers accurate grading and high-quality\nfeedback, at least comparable to that of fine-tuned proprietary LLMs, in\naddition to its ability to alleviate misgrading. Furthermore, we conduct\nhuman-AI co-grading experiments with both novice and expert graders. We find\nthat our system not only automates the grading process but also enhances the\nperformance and efficiency of human graders, particularly for essays where the\nmodel has lower confidence. These results highlight the potential of LLMs to\nfacilitate effective human-AI collaboration in the educational context,\npotentially transforming learning experiences through AI-generated feedback.",
      "tldr_zh": "这篇论文探讨了使用 Large Language Models (LLMs) 进行 Automated Essay Scoring (AES)，发现 LLMs 在一致性、一般性和可解释性方面表现出色，尽管其性能未超越传统 State-of-the-Art (SOTA) 模型。研究提出一个基于双过程理论的开源 LLM-AES 系统，能够提供准确评分、高质量反馈，并有效缓解误评分问题。实验结果显示，该系统在人类-AI 协作中显著提升了评分者的性能和效率，尤其在模型信心较低的作文上，从而为教育领域的 LLMs 应用提供了潜力，推动更有效的学习反馈。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06431v2",
      "published_date": "2024-01-12 07:50:10 UTC",
      "updated_date": "2024-06-15 03:44:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:33:03.272026"
    },
    {
      "arxiv_id": "2401.06426v1",
      "title": "UPDP: A Unified Progressive Depth Pruner for CNN and Vision Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Ji Liu",
        "Dehua Tang",
        "Yuanxian Huang",
        "Li Zhang",
        "Xiaocheng Zeng",
        "Dong Li",
        "Mingjie Lu",
        "Jinzhang Peng",
        "Yu Wang",
        "Fan Jiang",
        "Lu Tian",
        "Ashish Sirasao"
      ],
      "abstract": "Traditional channel-wise pruning methods by reducing network channels\nstruggle to effectively prune efficient CNN models with depth-wise\nconvolutional layers and certain efficient modules, such as popular inverted\nresidual blocks. Prior depth pruning methods by reducing network depths are not\nsuitable for pruning some efficient models due to the existence of some\nnormalization layers. Moreover, finetuning subnet by directly removing\nactivation layers would corrupt the original model weights, hindering the\npruned model from achieving high performance. To address these issues, we\npropose a novel depth pruning method for efficient models. Our approach\nproposes a novel block pruning strategy and progressive training method for the\nsubnet. Additionally, we extend our pruning method to vision transformer\nmodels. Experimental results demonstrate that our method consistently\noutperforms existing depth pruning methods across various pruning\nconfigurations. We obtained three pruned ConvNeXtV1 models with our method\napplying on ConvNeXtV1, which surpass most SOTA efficient models with\ncomparable inference performance. Our method also achieves state-of-the-art\npruning performance on the vision transformer model.",
      "tldr_zh": "该论文提出UPDP，一种统一的渐进式深度剪枝方法，针对传统通道剪枝在处理包含深度卷积层和倒置残差块的CNN模型时的局限性，以及现有深度剪枝方法对归一化层的影响问题。UPDP引入了一个新颖的块剪枝策略和渐进式训练方法，能够避免直接移除激活层破坏模型权重，并扩展适用于Vision Transformer模型。实验结果显示，该方法在各种剪枝配置中优于现有方法，并在ConvNeXtV1上生成的三种剪枝模型超越了大多数SOTA高效模型，同时在Vision Transformer上达到了最先进性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06426v1",
      "published_date": "2024-01-12 07:43:48 UTC",
      "updated_date": "2024-01-12 07:43:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:33:14.447050"
    },
    {
      "arxiv_id": "2401.06421v1",
      "title": "Uncertainty quantification for probabilistic machine learning in earth observation using conformal prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Geethen Singh",
        "Glenn Moncrieff",
        "Zander Venter",
        "Kerry Cawse-Nicholson",
        "Jasper Slingsby",
        "Tamara B Robinson"
      ],
      "abstract": "Unreliable predictions can occur when using artificial intelligence (AI)\nsystems with negative consequences for downstream applications, particularly\nwhen employed for decision-making. Conformal prediction provides a\nmodel-agnostic framework for uncertainty quantification that can be applied to\nany dataset, irrespective of its distribution, post hoc. In contrast to other\npixel-level uncertainty quantification methods, conformal prediction operates\nwithout requiring access to the underlying model and training dataset,\nconcurrently offering statistically valid and informative prediction regions,\nall while maintaining computational efficiency. In response to the increased\nneed to report uncertainty alongside point predictions, we bring attention to\nthe promise of conformal prediction within the domain of Earth Observation (EO)\napplications. To accomplish this, we assess the current state of uncertainty\nquantification in the EO domain and found that only 20% of the reviewed Google\nEarth Engine (GEE) datasets incorporated a degree of uncertainty information,\nwith unreliable methods prevalent. Next, we introduce modules that seamlessly\nintegrate into existing GEE predictive modelling workflows and demonstrate the\napplication of these tools for datasets spanning local to global scales,\nincluding the Dynamic World and Global Ecosystem Dynamics Investigation (GEDI)\ndatasets. These case studies encompass regression and classification tasks,\nfeaturing both traditional and deep learning-based workflows. Subsequently, we\ndiscuss the opportunities arising from the use of conformal prediction in EO.\nWe anticipate that the increased availability of easy-to-use implementations of\nconformal predictors, such as those provided here, will drive wider adoption of\nrigorous uncertainty quantification in EO, thereby enhancing the reliability of\nuses such as operational monitoring and decision making.",
      "tldr_zh": "该论文探讨了使用 Conformal Prediction 来量化不确定性，以提升 Earth Observation (EO) 领域中概率机器学习的可靠性。作者评估了当前 EO 现状，发现仅 20% 的 Google Earth Engine (GEE) 数据集包含不确定性信息，且方法往往不可靠。论文引入了可无缝集成 GEE 工作流的模块，并在 Dynamic World 和 Global Ecosystem Dynamics Investigation (GEDI) 等数据集上演示了其应用，包括回归和分类任务，支持传统和深度学习工作流。实验结果显示，Conformal Prediction 能提供统计上有效的预测区域，提高计算效率，并有望促进 EO 中的不确定性量化采用，从而增强决策可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06421v1",
      "published_date": "2024-01-12 07:31:21 UTC",
      "updated_date": "2024-01-12 07:31:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:33:28.017636"
    },
    {
      "arxiv_id": "2401.06416v2",
      "title": "Mission: Impossible Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Julie Kallini",
        "Isabel Papadimitriou",
        "Richard Futrell",
        "Kyle Mahowald",
        "Christopher Potts"
      ],
      "abstract": "Chomsky and others have very directly claimed that large language models\n(LLMs) are equally capable of learning languages that are possible and\nimpossible for humans to learn. However, there is very little published\nexperimental evidence to support such a claim. Here, we develop a set of\nsynthetic impossible languages of differing complexity, each designed by\nsystematically altering English data with unnatural word orders and grammar\nrules. These languages lie on an impossibility continuum: at one end are\nlanguages that are inherently impossible, such as random and irreversible\nshuffles of English words, and on the other, languages that may not be\nintuitively impossible but are often considered so in linguistics, particularly\nthose with rules based on counting word positions. We report on a wide range of\nevaluations to assess the capacity of GPT-2 small models to learn these\nuncontroversially impossible languages, and crucially, we perform these\nassessments at various stages throughout training to compare the learning\nprocess for each language. Our core finding is that GPT-2 struggles to learn\nimpossible languages when compared to English as a control, challenging the\ncore claim. More importantly, we hope our approach opens up a productive line\nof inquiry in which different LLM architectures are tested on a variety of\nimpossible languages in an effort to learn more about how LLMs can be used as\ntools for these cognitive and typological investigations.",
      "tldr_zh": "本研究质疑了大型语言模型（LLMs）能否像人类一样学习不可能语言，Chomsky 等人的观点缺乏实验支持。作者创建了一系列合成的不可能语言，通过系统性地改变英语的词序和语法规则，形成一个从完全不可能（如随机词序）到语言学上认为不可能（如基于词位计数的规则）的连续体，并使用 GPT-2 small 模型进行训练评估。结果显示，GPT-2 在学习这些不可能语言时明显比学习英语更困难，挑战了相关核心主张；同时，该方法有望促进对不同 LLM 架构的测试，推动 LLMs 在认知和类型学研究中的应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06416v2",
      "published_date": "2024-01-12 07:24:26 UTC",
      "updated_date": "2024-08-02 21:59:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:33:39.907917"
    },
    {
      "arxiv_id": "2401.06406v1",
      "title": "Knowledge-Informed Machine Learning for Cancer Diagnosis and Prognosis: A review",
      "title_zh": "翻译失败",
      "authors": [
        "Lingchao Mao",
        "Hairong Wang",
        "Leland S. Hu",
        "Nhan L Tran",
        "Peter D Canoll",
        "Kristin R Swanson",
        "Jing Li"
      ],
      "abstract": "Cancer remains one of the most challenging diseases to treat in the medical\nfield. Machine learning has enabled in-depth analysis of rich multi-omics\nprofiles and medical imaging for cancer diagnosis and prognosis. Despite these\nadvancements, machine learning models face challenges stemming from limited\nlabeled sample sizes, the intricate interplay of high-dimensionality data\ntypes, the inherent heterogeneity observed among patients and within tumors,\nand concerns about interpretability and consistency with existing biomedical\nknowledge. One approach to surmount these challenges is to integrate biomedical\nknowledge into data-driven models, which has proven potential to improve the\naccuracy, robustness, and interpretability of model results. Here, we review\nthe state-of-the-art machine learning studies that adopted the fusion of\nbiomedical knowledge and data, termed knowledge-informed machine learning, for\ncancer diagnosis and prognosis. Emphasizing the properties inherent in four\nprimary data types including clinical, imaging, molecular, and treatment data,\nwe highlight modeling considerations relevant to these contexts. We provide an\noverview of diverse forms of knowledge representation and current strategies of\nknowledge integration into machine learning pipelines with concrete examples.\nWe conclude the review article by discussing future directions to advance\ncancer research through knowledge-informed machine learning.",
      "tldr_zh": "这篇综述探讨了机器学习在癌症诊断和预后中的应用及其挑战，包括样本量有限、高维数据复杂性、患者和肿瘤异质性，以及模型的可解释性和与生物医学知识一致性问题。作者强调knowledge-informed machine learning方法，通过整合生物医学知识（如临床、imaging、molecular和treatment数据）到数据驱动模型中，提升了模型的准确性、鲁棒性和可解释性。论文概述了知识表示形式和整合策略的具体例子，并展望了未来方向，以推进癌症研究。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "92B99"
      ],
      "primary_category": "cs.LG",
      "comment": "41 pages, 4 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2401.06406v1",
      "published_date": "2024-01-12 07:01:36 UTC",
      "updated_date": "2024-01-12 07:01:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:33:52.382014"
    },
    {
      "arxiv_id": "2401.06401v4",
      "title": "DevEval: Evaluating Code Generation in Practical Software Projects",
      "title_zh": "DevEval：评估实际软件项目中的代码生成",
      "authors": [
        "Jia Li",
        "Ge Li",
        "Yunfei Zhao",
        "Yongmin Li",
        "Zhi Jin",
        "Hao Zhu",
        "Huanyu Liu",
        "Kaibo Liu",
        "Lecheng Wang",
        "Zheng Fang",
        "Lanshen Wang",
        "Jiazheng Ding",
        "Xuanming Zhang",
        "Yihong Dong",
        "Yuqi Zhu",
        "Bin Gu",
        "Mengfei Yang"
      ],
      "abstract": "How to evaluate Large Language Models (LLMs) in code generation is an open\nquestion. Many benchmarks have been proposed but are inconsistent with\npractical software projects, e.g., unreal program distributions, insufficient\ndependencies, and small-scale project contexts. Thus, the capabilities of LLMs\nin practical projects are still unclear. In this paper, we propose a new\nbenchmark named DevEval, aligned with Developers' experiences in practical\nprojects. DevEval is collected through a rigorous pipeline, containing 2,690\nsamples from 119 practical projects and covering 10 domains. Compared to\nprevious benchmarks, DevEval aligns to practical projects in multiple\ndimensions, e.g., real program distributions, sufficient dependencies, and\nenough-scale project contexts. We assess five popular LLMs on DevEval (e.g.,\ngpt-4, gpt-3.5-turbo, CodeLLaMa, and StarCoder) and reveal their actual\nabilities in code generation. For instance, the highest Pass@1 of gpt-3.5-turbo\nonly is 42 in our experiments. We also discuss the challenges and future\ndirections of code generation in practical projects. We open-source DevEval and\nhope it can facilitate the development of code generation in practical\nprojects.",
      "tldr_zh": "论文提出DevEval基准，用于评估大型语言模型（LLMs）在实际软件项目中的代码生成能力，以解决现有基准（如程序分布不真实、依赖不足和项目上下文规模小）的问题。DevEval通过严格的收集流程获得，包含来自119个实际项目的2,690个样本，覆盖10个领域，并更贴近真实场景。评估结果显示，五种流行LLMs（如gpt-4、gpt-3.5-turbo、CodeLLaMa和StarCoder）的表现有限，例如gpt-3.5-turbo的Pass@1最高仅为42%。论文讨论了代码生成在实际项目中的挑战和未来方向，并开源DevEval以促进相关技术发展。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "We are re-checking this benchmark and repeating related experiments.\n  New versions of DevEval will be released later",
      "pdf_url": "http://arxiv.org/pdf/2401.06401v4",
      "published_date": "2024-01-12 06:51:30 UTC",
      "updated_date": "2024-03-06 02:16:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:34:04.100830"
    },
    {
      "arxiv_id": "2401.06394v1",
      "title": "Adaptive Data Augmentation for Aspect Sentiment Quad Prediction",
      "title_zh": "面向",
      "authors": [
        "Wenyuan Zhang",
        "Xinghua Zhang",
        "Shiyao Cui",
        "Kun Huang",
        "Xuebin Wang",
        "Tingwen Liu"
      ],
      "abstract": "Aspect sentiment quad prediction (ASQP) aims to predict the quad sentiment\nelements for a given sentence, which is a critical task in the field of\naspect-based sentiment analysis. However, the data imbalance issue has not\nreceived sufficient attention in ASQP task. In this paper, we divide the issue\ninto two-folds, quad-pattern imbalance and aspect-category imbalance, and\npropose an Adaptive Data Augmentation (ADA) framework to tackle the imbalance\nissue. Specifically, a data augmentation process with a condition function\nadaptively enhances the tail quad patterns and aspect categories, alleviating\nthe data imbalance in ASQP. Following previous studies, we also further explore\nthe generative framework for extracting complete quads by introducing the\ncategory prior knowledge and syntax-guided decoding target. Experimental\nresults demonstrate that data augmentation for imbalance in ASQP task can\nimprove the performance, and the proposed ADA method is superior to naive data\noversampling.",
      "tldr_zh": "这篇论文针对 Aspect Sentiment Quad Prediction (ASQP) 任务中的数据不平衡问题，提出了 Adaptive Data Augmentation (ADA) 框架，将问题分为 quad-pattern imbalance 和 aspect-category imbalance。ADA 通过一个条件函数自适应地增强尾部 quad patterns 和 aspect categories，从而缓解数据不平衡。论文还探索了生成框架，引入 category prior knowledge 和 syntax-guided decoding target 来提取完整的 quads。实验结果显示，该方法显著提高了 ASQP 的性能，并优于传统的 naive data oversampling 策略。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ICASSP 2024, 5 pages",
      "pdf_url": "http://arxiv.org/pdf/2401.06394v1",
      "published_date": "2024-01-12 06:20:56 UTC",
      "updated_date": "2024-01-12 06:20:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:34:16.028088"
    },
    {
      "arxiv_id": "2401.06382v1",
      "title": "What should I say? -- Interacting with AI and Natural Language Interfaces",
      "title_zh": "我该说什么？——与 AI 和自然语言接口互动",
      "authors": [
        "Mark Adkins"
      ],
      "abstract": "As Artificial Intelligence (AI) technology becomes more and more prevalent,\nit becomes increasingly important to explore how we as humans interact with AI.\nThe Human-AI Interaction (HAI) sub-field has emerged from the Human-Computer\nInteraction (HCI) field and aims to examine this very notion. Many interaction\npatterns have been implemented without fully understanding the changes in\nrequired cognition as well as the cognitive science implications of using these\nalternative interfaces that aim to be more human-like in nature. Prior research\nsuggests that theory of mind representations are crucial to successful and\neffortless communication, however very little is understood when it comes to\nhow theory of mind representations are established when interacting with AI.",
      "tldr_zh": "这篇论文探讨了人类与人工智能（AI）互动的重要性，特别是通过自然语言接口（Natural Language Interfaces）。它从Human-Computer Interaction (HCI) 领域延伸出的Human-AI Interaction (HAI) 子领域入手，分析了现有互动模式对认知需求和认知科学的影响。论文指出，这些模式虽旨在更人性化，但尚未充分理解其潜在问题，如认知变化和科学含义。先前研究强调theory of mind representations在有效沟通中的关键作用，但论文揭示，在与AI互动时，这一方面的理解仍十分有限。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "I.2.m; J.4; B.4.2"
      ],
      "primary_category": "cs.HC",
      "comment": "6 pages, 12 figures, 12 data tables, study data included in appendix",
      "pdf_url": "http://arxiv.org/pdf/2401.06382v1",
      "published_date": "2024-01-12 05:10:23 UTC",
      "updated_date": "2024-01-12 05:10:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:34:28.298540"
    },
    {
      "arxiv_id": "2401.06379v1",
      "title": "Vehicle: Bridging the Embedding Gap in the Verification of Neuro-Symbolic Programs",
      "title_zh": "Vehicle: 在神经符号程序验证中桥接嵌入差距",
      "authors": [
        "Matthew L. Daggitt",
        "Wen Kokke",
        "Robert Atkey",
        "Natalia Slusarz",
        "Luca Arnaboldi",
        "Ekaterina Komendantskaya"
      ],
      "abstract": "Neuro-symbolic programs -- programs containing both machine learning\ncomponents and traditional symbolic code -- are becoming increasingly\nwidespread. However, we believe that there is still a lack of a general\nmethodology for verifying these programs whose correctness depends on the\nbehaviour of the machine learning components. In this paper, we identify the\n``embedding gap'' -- the lack of techniques for linking semantically-meaningful\n``problem-space'' properties to equivalent ``embedding-space'' properties -- as\none of the key issues, and describe Vehicle, a tool designed to facilitate the\nend-to-end verification of neural-symbolic programs in a modular fashion.\nVehicle provides a convenient language for specifying ``problem-space''\nproperties of neural networks and declaring their relationship to the\n``embedding-space\", and a powerful compiler that automates interpretation of\nthese properties in the language of a chosen machine-learning training\nenvironment, neural network verifier, and interactive theorem prover. We\ndemonstrate Vehicle's utility by using it to formally verify the safety of a\nsimple autonomous car equipped with a neural network controller.",
      "tldr_zh": "该论文识别了在验证神经符号程序(neuro-symbolic programs)时存在的“嵌入间隙”(embedding gap)，即无法将语义有意义的“问题空间”属性链接到等价的“嵌入空间”属性，从而导致验证方法的缺失。作者开发了Vehicle工具，该工具提供了一种方便的语言来指定神经网络的“问题空间”属性，并声明其与“嵌入空间”的关系，同时通过一个强大编译器自动将其解释为所选机器学习训练环境、神经网络验证器和交互式定理证明器的语言。Vehicle支持模块化的端到端验证，并通过正式验证一个配备神经网络控制器的简单自动驾驶汽车的安全性，展示了其实用性和有效性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06379v1",
      "published_date": "2024-01-12 05:01:47 UTC",
      "updated_date": "2024-01-12 05:01:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:34:39.773315"
    },
    {
      "arxiv_id": "2401.06375v1",
      "title": "Cognitive BPM as an Equalizer: Improving Access and Efficiency for Employees with (and without) Cognitive Disabilities",
      "title_zh": "认知 BPM 作为均衡器：改善有（和没有）认知障碍员工的访问和效率",
      "authors": [
        "Gordon Banks",
        "Gates Bierhuizen",
        "Katherine McCrum",
        "Ellen Wengert"
      ],
      "abstract": "We examine ProcessGPT, an AI model designed to automate, augment, and improve\nbusiness processes, to study the challenges of managing business processes\nwithin the cognitive limitations of the human workforce, particularly\nindividuals with cognitive disabilities. ProcessGPT provides a blueprint for\ndesigning efficient business processes that take into account human cognitive\nlimitations. By viewing this through the lens of cognitive disabilities, we\nshow that ProcessGPT improves process usability for individuals with and\nwithout cognitive disabilities. We also demonstrate that organizations\nimplementing ProcessGPT-like capabilities will realize increased productivity,\nmorale, and inclusion.",
      "tldr_zh": "本研究探讨 Cognitive BPM 作为一种平衡器，通过 ProcessGPT AI 模型来提升员工（包括有和无认知障碍者）的业务流程访问和效率。ProcessGPT 提供设计高效业务流程的蓝图，考虑人类认知限制，特别是针对认知 disabilities 的挑战，从而自动化和增强流程管理。结果表明，该模型改善了所有员工的流程可用性，同时帮助组织实现更高的生产力、士气和包容性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.06375v1",
      "published_date": "2024-01-12 04:54:06 UTC",
      "updated_date": "2024-01-12 04:54:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:34:50.372983"
    },
    {
      "arxiv_id": "2401.06827v2",
      "title": "APLe: Token-Wise Adaptive for Multi-Modal Prompt Learning",
      "title_zh": "APLe：基于标记的自适应多模态提示学习",
      "authors": [
        "Guiming Cao",
        "Kaize Shi",
        "Hong Fu",
        "Huaiwen Zhang",
        "Guandong Xu"
      ],
      "abstract": "Pre-trained Vision-Language (V-L) models set the benchmark for generalization\nto downstream tasks among the noteworthy contenders. Many characteristics of\nthe V-L model have been explored in existing research including the challenge\nof the sensitivity to text input and the tuning process across multi-modal\nprompts. With the advanced utilization of the V-L model like CLIP, recent\napproaches deploy learnable prompts instead of hand-craft prompts to boost the\ngeneralization performance and address the aforementioned challenges. Inspired\nby layer-wise training, which is wildly used in image fusion, we note that\nusing a sequential training process to adapt different modalities branches of\nCLIP efficiently facilitates the improvement of generalization. In the context\nof addressing the multi-modal prompting challenge, we propose Token-wise\nAdaptive for Multi-modal Prompt Learning (APLe) for tuning both modalities\nprompts, vision and language, as tokens in a sequential manner. APLe addresses\nthe challenges in V-L models to promote prompt learning across both modalities,\nwhich indicates a competitive generalization performance in line with the\nstate-of-the-art. Preeminently, APLe shows robustness and favourable\nperformance in prompt-length experiments with an absolute advantage in adopting\nthe V-L models.",
      "tldr_zh": "这篇论文提出APLe，一种基于标记级适应的多模态提示学习方法，旨在提升预训练视觉-语言(V-L)模型如CLIP的泛化性能，并解决其对文本输入的敏感性和多模态提示调整挑战。APLe通过顺序训练视觉和语言模态的提示作为标记，实现高效的层级式适应过程。实验结果表明，APLe在泛化性能上与最先进方法相当，并在提示长度实验中表现出色，具有显著的鲁棒性和优势。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "7 pages,3 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.06827v2",
      "published_date": "2024-01-12 04:54:01 UTC",
      "updated_date": "2024-01-23 08:54:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:35:03.504452"
    },
    {
      "arxiv_id": "2401.06370v1",
      "title": "Graph Relation Distillation for Efficient Biomedical Instance Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoyu Liu",
        "Yueyi Zhang",
        "Zhiwei Xiong",
        "Wei Huang",
        "Bo Hu",
        "Xiaoyan Sun",
        "Feng Wu"
      ],
      "abstract": "Instance-aware embeddings predicted by deep neural networks have\nrevolutionized biomedical instance segmentation, but its resource requirements\nare substantial. Knowledge distillation offers a solution by transferring\ndistilled knowledge from heavy teacher networks to lightweight yet\nhigh-performance student networks. However, existing knowledge distillation\nmethods struggle to extract knowledge for distinguishing instances and overlook\nglobal relation information. To address these challenges, we propose a graph\nrelation distillation approach for efficient biomedical instance segmentation,\nwhich considers three essential types of knowledge: instance-level features,\ninstance relations, and pixel-level boundaries. We introduce two graph\ndistillation schemes deployed at both the intra-image level and the inter-image\nlevel: instance graph distillation (IGD) and affinity graph distillation (AGD).\nIGD constructs a graph representing instance features and relations,\ntransferring these two types of knowledge by enforcing instance graph\nconsistency. AGD constructs an affinity graph representing pixel relations to\ncapture structured knowledge of instance boundaries, transferring\nboundary-related knowledge by ensuring pixel affinity consistency. Experimental\nresults on a number of biomedical datasets validate the effectiveness of our\napproach, enabling student models with less than $ 1\\%$ parameters and less\nthan $10\\%$ inference time while achieving promising performance compared to\nteacher models.",
      "tldr_zh": "该研究针对生物医学实例分割的资源密集型问题，提出了一种图关系蒸馏（graph relation distillation）方法，通过从教师网络转移三种关键知识：实例级特征、实例关系和像素级边界，以提升轻量级学生网络的性能。具体而言，该方法引入实例图蒸馏（IGD）和亲和图蒸馏（AGD）方案，分别在图像内构建实例特征和关系图、在图像间构建像素亲和图，确保知识一致性转移。实验结果显示，在多个生物医学数据集上，学生模型的参数少于1%、推理时间少于10%，却能实现与教师模型相当的出色性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06370v1",
      "published_date": "2024-01-12 04:41:23 UTC",
      "updated_date": "2024-01-12 04:41:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:35:15.616686"
    },
    {
      "arxiv_id": "2401.06340v2",
      "title": "A Temporal-Spectral Fusion Transformer with Subject-Specific Adapter for Enhancing RSVP-BCI Decoding",
      "title_zh": "翻译失败",
      "authors": [
        "Xujin Li",
        "Wei Wei",
        "Shuang Qiu",
        "Huiguang He"
      ],
      "abstract": "The Rapid Serial Visual Presentation (RSVP)-based Brain-Computer Interface\n(BCI) is an efficient technology for target retrieval using\nelectroencephalography (EEG) signals. The performance improvement of\ntraditional decoding methods relies on a substantial amount of training data\nfrom new test subjects, which increases preparation time for BCI systems.\nSeveral studies introduce data from existing subjects to reduce the dependence\nof performance improvement on data from new subjects, but their optimization\nstrategy based on adversarial learning with extensive data increases training\ntime during the preparation procedure. Moreover, most previous methods only\nfocus on the single-view information of EEG signals, but ignore the information\nfrom other views which may further improve performance. To enhance decoding\nperformance while reducing preparation time, we propose a Temporal-Spectral\nfusion transformer with Subject-specific Adapter (TSformer-SA). Specifically, a\ncross-view interaction module is proposed to facilitate information transfer\nand extract common representations across two-view features extracted from EEG\ntemporal signals and spectrogram images. Then, an attention-based fusion module\nfuses the features of two views to obtain comprehensive discriminative features\nfor classification. Furthermore, a multi-view consistency loss is proposed to\nmaximize the feature similarity between two views of the same EEG signal.\nFinally, we propose a subject-specific adapter to rapidly transfer the\nknowledge of the model trained on data from existing subjects to decode data\nfrom new subjects. Experimental results show that TSformer-SA significantly\noutperforms comparison methods and achieves outstanding performance with\nlimited training data from new subjects. This facilitates efficient decoding\nand rapid deployment of BCI systems in practical use.",
      "tldr_zh": "这篇论文针对 Rapid Serial Visual Presentation (RSVP)-based Brain-Computer Interface (BCI) 的解码性能问题，提出了一种 Temporal-Spectral Fusion Transformer with Subject-Specific Adapter (TSformer-SA) 方法，以减少对新受试者训练数据的依赖并缩短准备时间。该方法包括跨视角交互模块、注意力融合模块和多视角一致性损失，用于融合 EEG 信号的时域特征和频谱图像信息，从而提取更全面的判别特征。此外，Subject-Specific Adapter 模块实现了模型知识的快速转移，显著提升了在新受试者有限数据下的解码准确性。实验结果表明，TSformer-SA 优于现有方法，促进 BCI 系统的高效解码和实际部署。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "68T07",
        "I.5.4"
      ],
      "primary_category": "cs.HC",
      "comment": "19 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.06340v2",
      "published_date": "2024-01-12 03:18:51 UTC",
      "updated_date": "2024-07-11 05:07:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:35:31.956922"
    },
    {
      "arxiv_id": "2401.06826v1",
      "title": "Direct Distillation between Different Domains",
      "title_zh": "不同领域之间的直接蒸馏",
      "authors": [
        "Jialiang Tang",
        "Shuo Chen",
        "Gang Niu",
        "Hongyuan Zhu",
        "Joey Tianyi Zhou",
        "Chen Gong",
        "Masashi Sugiyama"
      ],
      "abstract": "Knowledge Distillation (KD) aims to learn a compact student network using\nknowledge from a large pre-trained teacher network, where both networks are\ntrained on data from the same distribution. However, in practical applications,\nthe student network may be required to perform in a new scenario (i.e., the\ntarget domain), which usually exhibits significant differences from the known\nscenario of the teacher network (i.e., the source domain). The traditional\ndomain adaptation techniques can be integrated with KD in a two-stage process\nto bridge the domain gap, but the ultimate reliability of two-stage approaches\ntends to be limited due to the high computational consumption and the\nadditional errors accumulated from both stages. To solve this problem, we\npropose a new one-stage method dubbed ``Direct Distillation between Different\nDomains\" (4Ds). We first design a learnable adapter based on the Fourier\ntransform to separate the domain-invariant knowledge from the domain-specific\nknowledge. Then, we build a fusion-activation mechanism to transfer the\nvaluable domain-invariant knowledge to the student network, while\nsimultaneously encouraging the adapter within the teacher network to learn the\ndomain-specific knowledge of the target data. As a result, the teacher network\ncan effectively transfer categorical knowledge that aligns with the target\ndomain of the student network. Intensive experiments on various benchmark\ndatasets demonstrate that our proposed 4Ds method successfully produces\nreliable student networks and outperforms state-of-the-art approaches.",
      "tldr_zh": "该研究针对知识蒸馏（KD）在不同领域间的应用问题，提出了一种名为“Direct Distillation between Different Domains”（4Ds）的单阶段方法，以解决传统两阶段方法的计算开销和错误积累问题。4Ds 通过基于 Fourier transform 的可学习适配器分离领域无关知识和领域特定知识，并引入融合-激活机制，将教师网络的领域无关知识转移给学生网络，同时让教师适配器学习目标域的特定知识，从而实现与目标域对齐的分类知识转移。在各种基准数据集上的实验表明，4Ds 方法生成可靠的学生网络，并优于现有最先进方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06826v1",
      "published_date": "2024-01-12 02:48:51 UTC",
      "updated_date": "2024-01-12 02:48:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:35:40.468848"
    },
    {
      "arxiv_id": "2401.06318v1",
      "title": "Striking a Balance in Fairness for Dynamic Systems Through Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Yaowei Hu",
        "Jacob Lear",
        "Lu Zhang"
      ],
      "abstract": "While significant advancements have been made in the field of fair machine\nlearning, the majority of studies focus on scenarios where the decision model\noperates on a static population. In this paper, we study fairness in dynamic\nsystems where sequential decisions are made. Each decision may shift the\nunderlying distribution of features or user behavior. We model the dynamic\nsystem through a Markov Decision Process (MDP). By acknowledging that\ntraditional fairness notions and long-term fairness are distinct requirements\nthat may not necessarily align with one another, we propose an algorithmic\nframework to integrate various fairness considerations with reinforcement\nlearning using both pre-processing and in-processing approaches. Three case\nstudies show that our method can strike a balance between traditional fairness\nnotions, long-term fairness, and utility.",
      "tldr_zh": "本研究探讨了公平机器学习在动态系统中的应用，强调顺序决策可能改变特征分布或用户行为的问题，而现有研究多聚焦于静态场景。作者使用 Markov Decision Process (MDP) 建模动态系统，并提出一个算法框架，将各种公平考虑整合到 Reinforcement Learning 中，通过 pre-processing 和 in-processing 方法实现。实验结果显示，该框架在三个案例研究中成功平衡了传统公平概念、长期公平和效用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06318v1",
      "published_date": "2024-01-12 01:29:26 UTC",
      "updated_date": "2024-01-12 01:29:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:35:50.708070"
    },
    {
      "arxiv_id": "2401.06824v5",
      "title": "Revisiting Jailbreaking for Large Language Models: A Representation Engineering Perspective",
      "title_zh": "重新审视大型语言模型的越狱攻击：表示工程视角",
      "authors": [
        "Tianlong Li",
        "Zhenghua Wang",
        "Wenhao Liu",
        "Muling Wu",
        "Shihan Dou",
        "Changze Lv",
        "Xiaohua Wang",
        "Xiaoqing Zheng",
        "Xuanjing Huang"
      ],
      "abstract": "The recent surge in jailbreaking attacks has revealed significant\nvulnerabilities in Large Language Models (LLMs) when exposed to malicious\ninputs. While various defense strategies have been proposed to mitigate these\nthreats, there has been limited research into the underlying mechanisms that\nmake LLMs vulnerable to such attacks. In this study, we suggest that the\nself-safeguarding capability of LLMs is linked to specific activity patterns\nwithin their representation space. Although these patterns have little impact\non the semantic content of the generated text, they play a crucial role in\nshaping LLM behavior under jailbreaking attacks. Our findings demonstrate that\nthese patterns can be detected with just a few pairs of contrastive queries.\nExtensive experimentation shows that the robustness of LLMs against\njailbreaking can be manipulated by weakening or strengthening these patterns.\nFurther visual analysis provides additional evidence for our conclusions,\nproviding new insights into the jailbreaking phenomenon. These findings\nhighlight the importance of addressing the potential misuse of open-source LLMs\nwithin the community.",
      "tldr_zh": "本研究从代表工程（Representation Engineering）的角度重新审视大型语言模型（LLMs）的jailbreaking攻击，揭示了LLMs在面对恶意输入时的脆弱性源于代表空间中的特定活动模式，这些模式虽对生成文本语义影响有限，但对模型行为至关重要。主要贡献包括使用少量对比查询检测这些模式，并通过广泛实验证明可以通过增强或削弱这些模式来操纵LLMs对jailbreaking的鲁棒性。视觉分析进一步支持了这一结论，强调了防范开源LLMs潜在误用的紧迫性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by COLING 2025",
      "pdf_url": "http://arxiv.org/pdf/2401.06824v5",
      "published_date": "2024-01-12 00:50:04 UTC",
      "updated_date": "2025-02-21 05:17:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:36:04.993762"
    },
    {
      "arxiv_id": "2401.06308v2",
      "title": "A Semantic-Aware Multiple Access Scheme for Distributed, Dynamic 6G-Based Applications",
      "title_zh": "一种语义感知多路访问方案，用于分布式、动态的基于6G应用",
      "authors": [
        "Hamidreza Mazandarani",
        "Masoud Shokrnezhad",
        "Tarik Taleb"
      ],
      "abstract": "The emergence of the semantic-aware paradigm presents opportunities for\ninnovative services, especially in the context of 6G-based applications.\nAlthough significant progress has been made in semantic extraction techniques,\nthe incorporation of semantic information into resource allocation\ndecision-making is still in its early stages, lacking consideration of the\nrequirements and characteristics of future systems. In response, this paper\nintroduces a novel formulation for the problem of multiple access to the\nwireless spectrum. It aims to optimize the utilization-fairness trade-off,\nusing the $\\alpha$-fairness metric, while accounting for user data correlation\nby introducing the concepts of self- and assisted throughputs. Initially, the\nproblem is analyzed to identify its optimal solution. Subsequently, a\nSemantic-Aware Multi-Agent Double and Dueling Deep Q-Learning (SAMA-D3QL)\ntechnique is proposed. This method is grounded in Model-free Multi-Agent Deep\nReinforcement Learning (MADRL), enabling the user equipment to autonomously\nmake decisions regarding wireless spectrum access based solely on their local\nindividual observations. The efficiency of the proposed technique is evaluated\nthrough two scenarios: single-channel and multi-channel. The findings\nillustrate that, across a spectrum of $\\alpha$ values, association matrices,\nand channels, SAMA-D3QL consistently outperforms alternative approaches. This\nestablishes it as a promising candidate for facilitating the realization of\nfuture federated, dynamically evolving applications.",
      "tldr_zh": "该论文针对6G应用中的语义感知范式，提出了一种新的无线频谱多路访问方案，旨在优化利用率与公平性的权衡，使用α-fairness指标并考虑用户数据相关性（如self- and assisted throughputs）。作者首先分析问题并求解最优方案，然后引入Semantic-Aware Multi-Agent Double and Dueling Deep Q-Learning (SAMA-D3QL)技术，该方法基于Model-free Multi-Agent Deep Reinforcement Learning (MADRL)，让用户设备通过本地观察自主决策无线频谱访问。实验在单通道和多通道场景中显示，SAMA-D3QL在各种α值、关联矩阵和通道条件下均优于现有方法，为实现分布式、动态演化的6G应用提供了可行解决方案。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.NI",
      "comment": "Accepted at the 2024 IEEE Wireless Communications and Networking\n  Conference (WCNC 2024)",
      "pdf_url": "http://arxiv.org/pdf/2401.06308v2",
      "published_date": "2024-01-12 00:32:38 UTC",
      "updated_date": "2024-07-04 18:48:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:36:16.551928"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 75,
  "processed_papers_count": 75,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-16T21:36:39.511173"
}
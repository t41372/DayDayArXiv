[
  {
    "arxiv_id": "2404.09101v1",
    "title": "Mixture of Experts Soften the Curse of Dimensionality in Operator Learning",
    "authors": [
      "Anastasis Kratsios",
      "Takashi Furuya",
      "Jose Antonio Lara Benitez",
      "Matti Lassas",
      "Maarten de Hoop"
    ],
    "abstract": "In this paper, we construct a mixture of neural operators (MoNOs) between\nfunction spaces whose complexity is distributed over a network of expert neural\noperators (NOs), with each NO satisfying parameter scaling restrictions. Our\nmain result is a \\textit{distributed} universal approximation theorem\nguaranteeing that any Lipschitz non-linear operator between $L^2([0,1]^d)$\nspaces can be approximated uniformly over the Sobolev unit ball therein, to any\ngiven $\\varepsilon>0$ accuracy, by an MoNO while satisfying the constraint\nthat: each expert NO has a depth, width, and rank of\n$\\mathcal{O}(\\varepsilon^{-1})$. Naturally, our result implies that the\nrequired number of experts must be large, however, each NO is guaranteed to be\nsmall enough to be loadable into the active memory of most computers for\nreasonable accuracies $\\varepsilon$. During our analysis, we also obtain new\nquantitative expression rates for classical NOs approximating uniformly\ncontinuous non-linear operators uniformly on compact subsets of $L^2([0,1]^d)$.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NA",
      "math.NA",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.09101v1",
    "published_date": "2024-04-13 23:20:16 UTC",
    "updated_date": "2024-04-13 23:20:16 UTC"
  },
  {
    "arxiv_id": "2404.09091v2",
    "title": "Semantic In-Domain Product Identification for Search Queries",
    "authors": [
      "Sanat Sharma",
      "Jayant Kumar",
      "Twisha Naik",
      "Zhaoyu Lu",
      "Arvind Srikantan",
      "Tracy Holloway King"
    ],
    "abstract": "Accurate explicit and implicit product identification in search queries is\ncritical for enhancing user experiences, especially at a company like Adobe\nwhich has over 50 products and covers queries across hundreds of tools. In this\nwork, we present a novel approach to training a product classifier from user\nbehavioral data. Our semantic model led to >25% relative improvement in CTR\n(click through rate) across the deployed surfaces; a >50% decrease in null\nrate; a 2x increase in the app cards surfaced, which helps drive product\nvisibility.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.09091v2",
    "published_date": "2024-04-13 22:18:14 UTC",
    "updated_date": "2024-05-29 16:01:27 UTC"
  },
  {
    "arxiv_id": "2404.16055v1",
    "title": "Assessing Climate Transition Risks in the Colombian Processed Food Sector: A Fuzzy Logic and Multicriteria Decision-Making Approach",
    "authors": [
      "Juan F. Pérez-Pérez",
      "Pablo Isaza Gómez",
      "Isis Bonet",
      "María Solange Sánchez-Pinzón",
      "Fabio Caraffini",
      "Christian Lochmuller"
    ],
    "abstract": "Climate risk assessment is becoming increasingly important. For\norganisations, identifying and assessing climate-related risks is challenging,\nas they can come from multiple sources. This study identifies and assesses the\nmain climate transition risks in the colombian processed food sector. As\ntransition risks are vague, our approach uses Fuzzy Logic and compares it to\nvarious multi-criteria decision-making methods to classify the different\nclimate transition risks an organisation may be exposed to. This approach\nallows us to use linguistic expressions for risk analysis and to better\ndescribe risks and their consequences. The results show that the risks ranked\nas the most critical for this organisation in their order were price volatility\nand raw materials availability, the change to less carbon-intensive production\nor consumption patterns, the increase in carbon taxes and technological change,\nand the associated development or implementation costs. These risks show a\ncritical risk level, which implies that they are the most significant risks for\nthe organisation in the case study. These results highlight the importance of\ninvestments needed to meet regulatory requirements, which are the main drivers\nfor organisations at the financial level.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16055v1",
    "published_date": "2024-04-13 21:49:49 UTC",
    "updated_date": "2024-04-13 21:49:49 UTC"
  },
  {
    "arxiv_id": "2404.09077v3",
    "title": "CuriousLLM: Elevating Multi-Document Question Answering with LLM-Enhanced Knowledge Graph Reasoning",
    "authors": [
      "Zukang Yang",
      "Zixuan Zhu",
      "Xuan Zhu"
    ],
    "abstract": "Large Language Models (LLMs) have achieved significant success in open-domain\nquestion answering. However, they continue to face challenges such as\nhallucinations and knowledge cutoffs. These issues can be mitigated through\nin-context learning by providing LLMs with relevant context before generating\nanswers. Recent literature proposes Knowledge Graph Prompting (KGP) which\nintegrates knowledge graphs with an LLM-based traversal agent to substantially\nenhance document retrieval quality. However, KGP requires costly fine-tuning\nwith large datasets and remains prone to hallucination. In this paper, we\npropose CuriousLLM, an enhancement that integrates a curiosity-driven reasoning\nmechanism into an LLM agent. This mechanism enables the agent to generate\nrelevant follow-up questions, thereby guiding the information retrieval process\nmore efficiently. Central to our approach is the development of the new\nFollow-upQA dataset, which includes questions and supporting evidence as input,\nwith follow-up questions serving as ground truths. These follow-up questions\neither inquire about what is still missing to fully answer the user's query or\nuse special tokens to signify that the retrieved evidence is sufficient. Our\nexperiments show that CuriousLLM significantly boosts LLM performance in\nmulti-document question answering (MD-QA), circumventing the substantial\ncomputational costs and latency from the original KGP framework.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for publication in NAACL 2025. The official version will be\n  available in the ACL Anthology",
    "pdf_url": "http://arxiv.org/pdf/2404.09077v3",
    "published_date": "2024-04-13 20:43:46 UTC",
    "updated_date": "2025-02-18 06:52:49 UTC"
  },
  {
    "arxiv_id": "2404.15347v1",
    "title": "Advanced Neural Network Architecture for Enhanced Multi-Lead ECG Arrhythmia Detection through Optimized Feature Extraction",
    "authors": [
      "Bhavith Chandra Challagundla"
    ],
    "abstract": "Cardiovascular diseases are a pervasive global health concern, contributing\nsignificantly to morbidity and mortality rates worldwide. Among these\nconditions, arrhythmia, characterized by irregular heart rhythms, presents\nformidable diagnostic challenges. This study introduces an innovative approach\nutilizing deep learning techniques, specifically Convolutional Neural Networks\n(CNNs), to address the complexities of arrhythmia classification. Leveraging\nmulti-lead Electrocardiogram (ECG) data, our CNN model, comprising six layers\nwith a residual block, demonstrates promising outcomes in identifying five\ndistinct heartbeat types: Left Bundle Branch Block (LBBB), Right Bundle Branch\nBlock (RBBB), Atrial Premature Contraction (APC), Premature Ventricular\nContraction (PVC), and Normal Beat. Through rigorous experimentation, we\nhighlight the transformative potential of our methodology in enhancing\ndiagnostic accuracy for cardiovascular arrhythmias. Arrhythmia diagnosis\nremains a critical challenge in cardiovascular care, often relying on manual\ninterpretation of ECG signals, which can be time-consuming and prone to\nsubjectivity. To address these limitations, we propose a novel approach that\nleverages deep learning algorithms to automate arrhythmia classification. By\nemploying advanced CNN architectures and multi-lead ECG data, our methodology\noffers a robust solution for precise and efficient arrhythmia detection.\nThrough comprehensive evaluation, we demonstrate the effectiveness of our\napproach in facilitating more accurate clinical decision-making, thereby\nimproving patient outcomes in managing cardiovascular arrhythmias.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.15347v1",
    "published_date": "2024-04-13 19:56:15 UTC",
    "updated_date": "2024-04-13 19:56:15 UTC"
  },
  {
    "arxiv_id": "2404.09067v1",
    "title": "Exploring Explainability in Video Action Recognition",
    "authors": [
      "Avinab Saha",
      "Shashank Gupta",
      "Sravan Kumar Ankireddy",
      "Karl Chahine",
      "Joydeep Ghosh"
    ],
    "abstract": "Image Classification and Video Action Recognition are perhaps the two most\nfoundational tasks in computer vision. Consequently, explaining the inner\nworkings of trained deep neural networks is of prime importance. While numerous\nefforts focus on explaining the decisions of trained deep neural networks in\nimage classification, exploration in the domain of its temporal version, video\naction recognition, has been scant. In this work, we take a deeper look at this\nproblem. We begin by revisiting Grad-CAM, one of the popular feature\nattribution methods for Image Classification, and its extension to Video Action\nRecognition tasks and examine the method's limitations. To address these, we\nintroduce Video-TCAV, by building on TCAV for Image Classification tasks, which\naims to quantify the importance of specific concepts in the decision-making\nprocess of Video Action Recognition models. As the scalable generation of\nconcepts is still an open problem, we propose a machine-assisted approach to\ngenerate spatial and spatiotemporal concepts relevant to Video Action\nRecognition for testing Video-TCAV. We then establish the importance of\ntemporally-varying concepts by demonstrating the superiority of dynamic\nspatiotemporal concepts over trivial spatial concepts. In conclusion, we\nintroduce a framework for investigating hypotheses in action recognition and\nquantitatively testing them, thus advancing research in the explainability of\ndeep neural networks used in video action recognition.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 10 figures, Accepted to the 3rd Explainable AI for Computer\n  Vision (XAI4CV) Workshop at CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.09067v1",
    "published_date": "2024-04-13 19:34:14 UTC",
    "updated_date": "2024-04-13 19:34:14 UTC"
  },
  {
    "arxiv_id": "2404.09051v1",
    "title": "Rethinking Iterative Stereo Matching from Diffusion Bridge Model Perspective",
    "authors": [
      "Yuguang Shi"
    ],
    "abstract": "Recently, iteration-based stereo matching has shown great potential. However,\nthese models optimize the disparity map using RNN variants. The discrete\noptimization process poses a challenge of information loss, which restricts the\nlevel of detail that can be expressed in the generated disparity map. In order\nto address these issues, we propose a novel training approach that incorporates\ndiffusion models into the iterative optimization process. We designed a\nTime-based Gated Recurrent Unit (T-GRU) to correlate temporal and disparity\noutputs. Unlike standard recurrent units, we employ Agent Attention to generate\nmore expressive features. We also designed an attention-based context network\nto capture a large amount of contextual information. Experiments on several\npublic benchmarks show that we have achieved competitive stereo matching\nperformance. Our model ranks first in the Scene Flow dataset, achieving over a\n7% improvement compared to competing methods, and requires only 8 iterations to\nachieve state-of-the-art results.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "tip. arXiv admin note: text overlap with arXiv:2303.06615 by other\n  authors",
    "pdf_url": "http://arxiv.org/pdf/2404.09051v1",
    "published_date": "2024-04-13 17:31:11 UTC",
    "updated_date": "2024-04-13 17:31:11 UTC"
  },
  {
    "arxiv_id": "2404.09045v1",
    "title": "Adapting Mental Health Prediction Tasks for Cross-lingual Learning via Meta-Training and In-context Learning with Large Language Model",
    "authors": [
      "Zita Lifelo",
      "Huansheng Ning",
      "Sahraoui Dhelim"
    ],
    "abstract": "Timely identification is essential for the efficient handling of mental\nhealth illnesses such as depression. However, the current research fails to\nadequately address the prediction of mental health conditions from social media\ndata in low-resource African languages like Swahili. This study introduces two\ndistinct approaches utilising model-agnostic meta-learning and leveraging large\nlanguage models (LLMs) to address this gap. Experiments are conducted on three\ndatasets translated to low-resource language and applied to four mental health\ntasks, which include stress, depression, depression severity and suicidal\nideation prediction. we first apply a meta-learning model with\nself-supervision, which results in improved model initialisation for rapid\nadaptation and cross-lingual transfer. The results show that our meta-trained\nmodel performs significantly better than standard fine-tuning methods,\noutperforming the baseline fine-tuning in macro F1 score with 18\\% and 0.8\\%\nover XLM-R and mBERT. In parallel, we use LLMs' in-context learning\ncapabilities to assess their performance accuracy across the Swahili mental\nhealth prediction tasks by analysing different cross-lingual prompting\napproaches. Our analysis showed that Swahili prompts performed better than\ncross-lingual prompts but less than English prompts. Our findings show that\nin-context learning can be achieved through cross-lingual transfer through\ncarefully crafted prompt templates with examples and instructions.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.09045v1",
    "published_date": "2024-04-13 17:11:35 UTC",
    "updated_date": "2024-04-13 17:11:35 UTC"
  },
  {
    "arxiv_id": "2404.09042v1",
    "title": "Improving Personalisation in Valence and Arousal Prediction using Data Augmentation",
    "authors": [
      "Munachiso Nwadike",
      "Jialin Li",
      "Hanan Salam"
    ],
    "abstract": "In the field of emotion recognition and Human-Machine Interaction (HMI),\npersonalised approaches have exhibited their efficacy in capturing\nindividual-specific characteristics and enhancing affective prediction\naccuracy. However, personalisation techniques often face the challenge of\nlimited data for target individuals. This paper presents our work on an\nenhanced personalisation strategy, that leverages data augmentation to develop\ntailored models for continuous valence and arousal prediction. Our proposed\napproach, Distance Weighting Augmentation (DWA), employs a weighting-based\naugmentation method that expands a target individual's dataset, leveraging\ndistance metrics to identify similar samples at the segment-level. Experimental\nresults on the MuSe-Personalisation 2023 Challenge dataset demonstrate that our\nmethod significantly improves the performance of features sets which have low\nbaseline performance, on the test set. This improvement in poor-performing\nfeatures comes without sacrificing performance on high-performing features. In\nparticular, our method achieves a maximum combined testing CCC of 0.78,\ncompared to the reported baseline score of 0.76 (reproduced at 0.72). It also\nachieved a peak arousal and valence scores of 0.81 and 0.76, compared to\nreproduced baseline scores of 0.76 and 0.67 respectively. Through this work, we\nmake significant contributions to the advancement of personalised affective\ncomputing models, enhancing the practicality and adaptability of data-level\npersonalisation in real world contexts.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.09042v1",
    "published_date": "2024-04-13 16:57:37 UTC",
    "updated_date": "2024-04-13 16:57:37 UTC"
  },
  {
    "arxiv_id": "2404.09022v1",
    "title": "Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies",
    "authors": [
      "Benjue Weng"
    ],
    "abstract": "With the surge of ChatGPT,the use of large models has significantly\nincreased,rapidly rising to prominence across the industry and sweeping across\nthe internet. This article is a comprehensive review of fine-tuning methods for\nlarge models. This paper investigates the latest technological advancements and\nthe application of advanced methods in aspects such as task-adaptive\nfine-tuning,domain-adaptive fine-tuning,few-shot learning,knowledge\ndistillation,multi-task learning,parameter-efficient fine-tuning,and dynamic\nfine-tuning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.09022v1",
    "published_date": "2024-04-13 15:03:03 UTC",
    "updated_date": "2024-04-13 15:03:03 UTC"
  },
  {
    "arxiv_id": "2404.13067v1",
    "title": "Towards Efficient Resume Understanding: A Multi-Granularity Multi-Modal Pre-Training Approach",
    "authors": [
      "Feihu Jiang",
      "Chuan Qin",
      "Jingshuai Zhang",
      "Kaichun Yao",
      "Xi Chen",
      "Dazhong Shen",
      "Chen Zhu",
      "Hengshu Zhu",
      "Hui Xiong"
    ],
    "abstract": "In the contemporary era of widespread online recruitment, resume\nunderstanding has been widely acknowledged as a fundamental and crucial task,\nwhich aims to extract structured information from resume documents\nautomatically. Compared to the traditional rule-based approaches, the\nutilization of recently proposed pre-trained document understanding models can\ngreatly enhance the effectiveness of resume understanding. The present\napproaches have, however, disregarded the hierarchical relations within the\nstructured information presented in resumes, and have difficulty parsing\nresumes in an efficient manner. To this end, in this paper, we propose a novel\nmodel, namely ERU, to achieve efficient resume understanding. Specifically, we\nfirst introduce a layout-aware multi-modal fusion transformer for encoding the\nsegments in the resume with integrated textual, visual, and layout information.\nThen, we design three self-supervised tasks to pre-train this module via a\nlarge number of unlabeled resumes. Next, we fine-tune the model with a\nmulti-granularity sequence labeling task to extract structured information from\nresumes. Finally, extensive experiments on a real-world dataset clearly\ndemonstrate the effectiveness of ERU.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ICME 2024 Accepted",
    "pdf_url": "http://arxiv.org/pdf/2404.13067v1",
    "published_date": "2024-04-13 14:31:24 UTC",
    "updated_date": "2024-04-13 14:31:24 UTC"
  },
  {
    "arxiv_id": "2404.09016v1",
    "title": "Theoretical research on generative diffusion models: an overview",
    "authors": [
      "Melike Nur Yeğin",
      "Mehmet Fatih Amasyalı"
    ],
    "abstract": "Generative diffusion models showed high success in many fields with a\npowerful theoretical background. They convert the data distribution to noise\nand remove the noise back to obtain a similar distribution. Many existing\nreviews focused on the specific application areas without concentrating on the\nresearch about the algorithm. Unlike them we investigated the theoretical\ndevelopments of the generative diffusion models. These approaches mainly divide\ninto two: training-based and sampling-based. Awakening to this allowed us a\nclear and understandable categorization for the researchers who will make new\ndevelopments in the future.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.09016v1",
    "published_date": "2024-04-13 14:08:56 UTC",
    "updated_date": "2024-04-13 14:08:56 UTC"
  },
  {
    "arxiv_id": "2404.09005v7",
    "title": "Proof-of-Learning with Incentive Security",
    "authors": [
      "Zishuo Zhao",
      "Zhixuan Fang",
      "Xuechao Wang",
      "Xi Chen",
      "Hongxu Su",
      "Haibo Xiao",
      "Yuan Zhou"
    ],
    "abstract": "Most concurrent blockchain systems rely heavily on the Proof-of-Work (PoW) or\nProof-of-Stake (PoS) mechanisms for decentralized consensus and security\nassurance. However, the substantial energy expenditure stemming from\ncomputationally intensive yet meaningless tasks has raised considerable\nconcerns surrounding traditional PoW approaches, The PoS mechanism, while free\nof energy consumption, is subject to security and economic issues. Addressing\nthese issues, the paradigm of Proof-of-Useful-Work (PoUW) seeks to employ\nchallenges of practical significance as PoW, thereby imbuing energy consumption\nwith tangible value. While previous efforts in Proof of Learning (PoL) explored\nthe utilization of deep learning model training SGD tasks as PoUW challenges,\nrecent research has revealed its vulnerabilities to adversarial attacks and the\ntheoretical hardness in crafting a byzantine-secure PoL mechanism. In this\npaper, we introduce the concept of incentive-security that incentivizes\nrational provers to behave honestly for their best interest, bypassing the\nexisting hardness to design a PoL mechanism with computational efficiency, a\nprovable incentive-security guarantee and controllable difficulty.\nParticularly, our work is secure against two attacks, and also improves the\ncomputational overhead from $\\Theta(1)$ to $O(\\frac{\\log E}{E})$. Furthermore,\nwhile most recent research assumes trusted problem providers and verifiers, our\ndesign also guarantees frontend incentive-security even when problem providers\nare untrusted, and verifier incentive-security that bypasses the Verifier's\nDilemma. By incorporating ML training into blockchain consensus mechanisms with\nprovable guarantees, our research not only proposes an eco-friendly solution to\nblockchain systems, but also provides a proposal for a completely decentralized\ncomputing power market in the new AI age.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.ET",
      "cs.GT",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "20 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.09005v7",
    "published_date": "2024-04-13 13:18:40 UTC",
    "updated_date": "2025-01-08 02:10:31 UTC"
  },
  {
    "arxiv_id": "2404.09001v1",
    "title": "Smart Help: Strategic Opponent Modeling for Proactive and Adaptive Robot Assistance in Households",
    "authors": [
      "Zhihao Cao",
      "Zidong Wang",
      "Siwen Xie",
      "Anji Liu",
      "Lifeng Fan"
    ],
    "abstract": "Despite the significant demand for assistive technology among vulnerable\ngroups (e.g., the elderly, children, and the disabled) in daily tasks, research\ninto advanced AI-driven assistive solutions that genuinely accommodate their\ndiverse needs remains sparse. Traditional human-machine interaction tasks often\nrequire machines to simply help without nuanced consideration of human\nabilities and feelings, such as their opportunity for practice and learning,\nsense of self-improvement, and self-esteem. Addressing this gap, we define a\npivotal and novel challenge Smart Help, which aims to provide proactive yet\nadaptive support to human agents with diverse disabilities and dynamic goals in\nvarious tasks and environments. To establish this challenge, we leverage\nAI2-THOR to build a new interactive 3D realistic household environment for the\nSmart Help task. We introduce an innovative opponent modeling module that\nprovides a nuanced understanding of the main agent's capabilities and goals, in\norder to optimize the assisting agent's helping policy. Rigorous experiments\nvalidate the efficacy of our model components and show the superiority of our\nholistic approach against established baselines. Our findings illustrate the\npotential of AI-imbued assistive robots in improving the well-being of\nvulnerable groups.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.09001v1",
    "published_date": "2024-04-13 13:03:59 UTC",
    "updated_date": "2024-04-13 13:03:59 UTC"
  },
  {
    "arxiv_id": "2404.08995v4",
    "title": "Beyond Known Clusters: Probe New Prototypes for Efficient Generalized Class Discovery",
    "authors": [
      "Ye Wang",
      "Yaxiong Wang",
      "Yujiao Wu",
      "Bingchen Zhao",
      "Xueming Qian"
    ],
    "abstract": "Generalized Class Discovery (GCD) aims to dynamically assign labels to\nunlabelled data partially based on knowledge learned from labelled data, where\nthe unlabelled data may come from known or novel classes. The prevailing\napproach generally involves clustering across all data and learning conceptions\nby prototypical contrastive learning. However, existing methods largely hinge\non the performance of clustering algorithms and are thus subject to their\ninherent limitations. Firstly, the estimated cluster number is often smaller\nthan the ground truth, making the existing methods suffer from the lack of\nprototypes for comprehensive conception learning. To address this issue, we\npropose an adaptive probing mechanism that introduces learnable potential\nprototypes to expand cluster prototypes (centers). As there is no ground truth\nfor the potential prototype, we develop a self-supervised prototype learning\nframework to optimize the potential prototype in an end-to-end fashion.\nSecondly, clustering is computationally intensive, and the conventional\nstrategy of clustering both labelled and unlabelled instances exacerbates this\nissue. To counteract this inefficiency, we opt to cluster only the unlabelled\ninstances and subsequently expand the cluster prototypes with our introduced\npotential prototypes to fast explore novel classes. Despite the simplicity of\nour proposed method, extensive empirical analysis on a wide range of datasets\nconfirms that our method consistently delivers state-of-the-art results.\nSpecifically, our method surpasses the nearest competitor by a significant\nmargin of 9.7% within the Stanford Cars dataset and 12x clustering efficiency\nwithin the Herbarium 19 dataset. We will make the code and checkpoints publicly\navailable at https://github.com/xjtuYW/PNP.git.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.08995v4",
    "published_date": "2024-04-13 12:41:40 UTC",
    "updated_date": "2024-04-30 07:13:18 UTC"
  },
  {
    "arxiv_id": "2404.08991v1",
    "title": "Business models for the simulation hypothesis",
    "authors": [
      "Evangelos Katsamakas"
    ],
    "abstract": "The simulation hypothesis suggests that we live in a computer simulation.\nThat notion has attracted significant scholarly and popular interest. This\narticle explores the simulation hypothesis from a business perspective. Due to\nthe lack of a name for a universe consistent with the simulation hypothesis, we\npropose the term simuverse. We argue that if we live in a simulation, there\nmust be a business justification. Therefore, we ask: If we live in a simuverse,\nwhat is its business model? We identify and explore business model scenarios,\nsuch as simuverse as a project, service, or platform. We also explore business\nmodel pathways and risk management issues. The article contributes to the\nsimulation hypothesis literature and is the first to provide a business model\nperspective on the simulation hypothesis. The article discusses theoretical and\npractical implications and identifies opportunities for future research related\nto sustainability, digital transformation, and Artificial Intelligence (AI).",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET",
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.08991v1",
    "published_date": "2024-04-13 12:36:20 UTC",
    "updated_date": "2024-04-13 12:36:20 UTC"
  },
  {
    "arxiv_id": "2404.08990v1",
    "title": "A Fourier-enhanced multi-modal 3D small object optical mark recognition and positioning method for percutaneous abdominal puncture surgical navigation",
    "authors": [
      "Zezhao Guo",
      "Yanzhong Guo",
      "Zhanfang Zhao"
    ],
    "abstract": "Navigation for thoracoabdominal puncture surgery is used to locate the needle\nentry point on the patient's body surface. The traditional reflective ball\nnavigation method is difficult to position the needle entry point on the soft,\nirregular, smooth chest and abdomen. Due to the lack of clear characteristic\npoints on the body surface using structured light technology, it is difficult\nto identify and locate arbitrary needle insertion points. Based on the high\nstability and high accuracy requirements of surgical navigation, this paper\nproposed a novel method, a muti-modal 3D small object medical marker detection\nmethod, which identifies the center of a small single ring as the needle\ninsertion point. Moreover, this novel method leverages Fourier transform\nenhancement technology to augment the dataset, enrich image details, and\nenhance the network's capability. The method extracts the Region of Interest\n(ROI) of the feature image from both enhanced and original images, followed by\ngenerating a mask map. Subsequently, the point cloud of the ROI from the depth\nmap is obtained through the registration of ROI point cloud contour fitting. In\naddition, this method employs Tukey loss for optimal precision. The\nexperimental results show this novel method proposed in this paper not only\nachieves high-precision and high-stability positioning, but also enables the\npositioning of any needle insertion point.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "19 pages, 6 figures,",
    "pdf_url": "http://arxiv.org/pdf/2404.08990v1",
    "published_date": "2024-04-13 12:28:40 UTC",
    "updated_date": "2024-04-13 12:28:40 UTC"
  },
  {
    "arxiv_id": "2404.08986v2",
    "title": "Airship Formations for Animal Motion Capture and Behavior Analysis",
    "authors": [
      "Eric Price",
      "Aamir Ahmad"
    ],
    "abstract": "Using UAVs for wildlife observation and motion capture offers manifold\nadvantages for studying animals in the wild, especially grazing herds in open\nterrain. The aerial perspective allows observation at a scale and depth that is\nnot possible on the ground, offering new insights into group behavior. However,\nthe very nature of wildlife field-studies puts traditional fixed wing and\nmulti-copter systems to their limits: limited flight time, noise and safety\naspects affect their efficacy, where lighter than air systems can remain on\nstation for many hours. Nevertheless, airships are challenging from a ground\nhandling perspective as well as from a control point of view, being voluminous\nand highly affected by wind. In this work, we showcase a system designed to use\nairship formations to track, follow, and visually record wild horses from\nmultiple angles, including airship design, simulation, control, on board\ncomputer vision, autonomous operation and practical aspects of field\nexperiments.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted for presentation at the 2nd International Conference on\n  Design and Engineering of Lighter-Than-Air systems (DELTAS2024)",
    "pdf_url": "http://arxiv.org/pdf/2404.08986v2",
    "published_date": "2024-04-13 12:18:19 UTC",
    "updated_date": "2024-05-24 10:59:48 UTC"
  },
  {
    "arxiv_id": "2404.08985v1",
    "title": "Intuition-aware Mixture-of-Rank-1-Experts for Parameter Efficient Finetuning",
    "authors": [
      "Yijiang Liu",
      "Rongyu Zhang",
      "Huanrui Yang",
      "Kurt Keutzer",
      "Yuan Du",
      "Li Du",
      "Shanghang Zhang"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated significant potential in\nperforming multiple tasks in multimedia applications, ranging from content\ngeneration to interactive entertainment, and artistic creation. However, the\ndiversity of downstream tasks in multitask scenarios presents substantial\nadaptation challenges for LLMs. While traditional methods often succumb to\nknowledge confusion on their monolithic dense models, Mixture-of-Experts (MoE)\nhas been emerged as a promising solution with its sparse architecture for\neffective task decoupling. Inspired by the principles of human cognitive\nneuroscience, we design a novel framework \\texttt{Intuition-MoR1E} that\nleverages the inherent semantic clustering of instances to mimic the human\nbrain to deal with multitask, offering implicit guidance to router for\noptimized feature allocation. Moreover, we introduce cutting-edge Rank-1\nExperts formulation designed to manage a spectrum of intuitions, demonstrating\nenhanced parameter efficiency and effectiveness in multitask LLM finetuning.\nExtensive experiments demonstrate that Intuition-MoR1E achieves superior\nefficiency and 2.15\\% overall accuracy improvement across 14 public datasets\nagainst other state-of-the-art baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.08985v1",
    "published_date": "2024-04-13 12:14:58 UTC",
    "updated_date": "2024-04-13 12:14:58 UTC"
  },
  {
    "arxiv_id": "2404.08978v2",
    "title": "Incremental Residual Concept Bottleneck Models",
    "authors": [
      "Chenming Shang",
      "Shiji Zhou",
      "Hengyuan Zhang",
      "Xinzhe Ni",
      "Yujiu Yang",
      "Yuwang Wang"
    ],
    "abstract": "Concept Bottleneck Models (CBMs) map the black-box visual representations\nextracted by deep neural networks onto a set of interpretable concepts and use\nthe concepts to make predictions, enhancing the transparency of the\ndecision-making process. Multimodal pre-trained models can match visual\nrepresentations with textual concept embeddings, allowing for obtaining the\ninterpretable concept bottleneck without the expertise concept annotations.\nRecent research has focused on the concept bank establishment and the\nhigh-quality concept selection. However, it is challenging to construct a\ncomprehensive concept bank through humans or large language models, which\nseverely limits the performance of CBMs. In this work, we propose the\nIncremental Residual Concept Bottleneck Model (Res-CBM) to address the\nchallenge of concept completeness. Specifically, the residual concept\nbottleneck model employs a set of optimizable vectors to complete missing\nconcepts, then the incremental concept discovery module converts the\ncomplemented vectors with unclear meanings into potential concepts in the\ncandidate concept bank. Our approach can be applied to any user-defined concept\nbank, as a post-hoc processing method to enhance the performance of any CBMs.\nFurthermore, to measure the descriptive efficiency of CBMs, the Concept\nUtilization Efficiency (CUE) metric is proposed. Experiments show that the\nRes-CBM outperforms the current state-of-the-art methods in terms of both\naccuracy and efficiency and achieves comparable performance to black-box models\nacross multiple datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.08978v2",
    "published_date": "2024-04-13 12:02:19 UTC",
    "updated_date": "2024-04-17 10:59:59 UTC"
  },
  {
    "arxiv_id": "2404.08964v1",
    "title": "Understanding Multimodal Deep Neural Networks: A Concept Selection View",
    "authors": [
      "Chenming Shang",
      "Hengyuan Zhang",
      "Hao Wen",
      "Yujiu Yang"
    ],
    "abstract": "The multimodal deep neural networks, represented by CLIP, have generated rich\ndownstream applications owing to their excellent performance, thus making\nunderstanding the decision-making process of CLIP an essential research topic.\nDue to the complex structure and the massive pre-training data, it is often\nregarded as a black-box model that is too difficult to understand and\ninterpret. Concept-based models map the black-box visual representations\nextracted by deep neural networks onto a set of human-understandable concepts\nand use the concepts to make predictions, enhancing the transparency of the\ndecision-making process. However, these methods involve the datasets labeled\nwith fine-grained attributes by expert knowledge, which incur high costs and\nintroduce excessive human prior knowledge and bias. In this paper, we observe\nthe long-tail distribution of concepts, based on which we propose a two-stage\nConcept Selection Model (CSM) to mine core concepts without introducing any\nhuman priors. The concept greedy rough selection algorithm is applied to\nextract head concepts, and then the concept mask fine selection method performs\nthe extraction of core concepts. Experiments show that our approach achieves\ncomparable performance to end-to-end black-box models, and human evaluation\ndemonstrates that the concepts discovered by our method are interpretable and\ncomprehensible for humans.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.08964v1",
    "published_date": "2024-04-13 11:06:49 UTC",
    "updated_date": "2024-04-13 11:06:49 UTC"
  },
  {
    "arxiv_id": "2404.10014v1",
    "title": "A biologically inspired computational trust model for open multi-agent systems which is resilient to trustor population changes",
    "authors": [
      "Zoi Lygizou",
      "Dimitris Kalles"
    ],
    "abstract": "Current trust and reputation models continue to have significant limitations,\nsuch as the inability to deal with agents constantly entering or exiting open\nmulti-agent systems (open MAS), as well as continuously changing behaviors. Our\nstudy is based on CA, a previously proposed decentralized computational trust\nmodel from the trustee's point of view, inspired by synaptic plasticity and the\nformation of assemblies in the human brain. It is designed to meet the\nrequirements of highly dynamic and open MAS, and its main difference with most\nconventional trust and reputation models is that the trustor does not select a\ntrustee to delegate a task; instead, the trustee determines whether it is\nqualified to successfully execute it. We ran a series of simulations to compare\nCA model to FIRE, a well-established, decentralized trust and reputation model\nfor open MAS under conditions of continuous trustee and trustor population\nreplacement, as well as continuous change of trustees' abilities to perform\ntasks. The main finding is that FIRE is superior to changes in the trustee\npopulation, whereas CA is resilient to the trustor population changes. When the\ntrustees switch performance profiles FIRE clearly outperforms despite the fact\nthat both models' performances are significantly impacted by this environmental\nchange. Findings lead us to conclude that learning to use the appropriate trust\nmodel, according to the dynamic conditions in effect could maximize the\ntrustor's benefits.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.10014v1",
    "published_date": "2024-04-13 10:56:32 UTC",
    "updated_date": "2024-04-13 10:56:32 UTC"
  },
  {
    "arxiv_id": "2404.08939v1",
    "title": "NeurIT: Pushing the Limit of Neural Inertial Tracking for Indoor Robotic IoT",
    "authors": [
      "Xinzhe Zheng",
      "Sijie Ji",
      "Yipeng Pan",
      "Kaiwen Zhang",
      "Chenshu Wu"
    ],
    "abstract": "Inertial tracking is vital for robotic IoT and has gained popularity thanks\nto the ubiquity of low-cost Inertial Measurement Units (IMUs) and deep\nlearning-powered tracking algorithms. Existing works, however, have not fully\nutilized IMU measurements, particularly magnetometers, nor maximized the\npotential of deep learning to achieve the desired accuracy. To enhance the\ntracking accuracy for indoor robotic applications, we introduce NeurIT, a\nsequence-to-sequence framework that elevates tracking accuracy to a new level.\nNeurIT employs a Time-Frequency Block-recurrent Transformer (TF-BRT) at its\ncore, combining the power of recurrent neural network (RNN) and Transformer to\nlearn representative features in both time and frequency domains. To fully\nutilize IMU information, we strategically employ body-frame differentiation of\nthe magnetometer, which considerably reduces the tracking error. NeurIT is\nimplemented on a customized robotic platform and evaluated in various indoor\nenvironments. Experimental results demonstrate that NeurIT achieves a mere\n1-meter tracking error over a 300-meter distance. Notably, it significantly\noutperforms state-of-the-art baselines by 48.21% on unseen data. NeurIT also\nperforms comparably to the visual-inertial approach (Tango Phone) in\nvision-favored conditions and surpasses it in plain environments. We believe\nNeurIT takes an important step forward toward practical neural inertial\ntracking for ubiquitous and scalable tracking of robotic things. NeurIT,\nincluding the source code and the dataset, is open-sourced here:\nhttps://github.com/NeurIT-Project/NeurIT.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.08939v1",
    "published_date": "2024-04-13 09:24:50 UTC",
    "updated_date": "2024-04-13 09:24:50 UTC"
  },
  {
    "arxiv_id": "2404.08937v1",
    "title": "ChimpVLM: Ethogram-Enhanced Chimpanzee Behaviour Recognition",
    "authors": [
      "Otto Brookes",
      "Majid Mirmehdi",
      "Hjalmar Kuhl",
      "Tilo Burghardt"
    ],
    "abstract": "We show that chimpanzee behaviour understanding from camera traps can be\nenhanced by providing visual architectures with access to an embedding of text\ndescriptions that detail species behaviours. In particular, we present a\nvision-language model which employs multi-modal decoding of visual features\nextracted directly from camera trap videos to process query tokens representing\nbehaviours and output class predictions. Query tokens are initialised using a\nstandardised ethogram of chimpanzee behaviour, rather than using random or\nname-based initialisations. In addition, the effect of initialising query\ntokens using a masked language model fine-tuned on a text corpus of known\nbehavioural patterns is explored. We evaluate our system on the PanAf500 and\nPanAf20K datasets and demonstrate the performance benefits of our multi-modal\ndecoding approach and query initialisation strategy on multi-class and\nmulti-label recognition tasks, respectively. Results and ablations corroborate\nperformance improvements. We achieve state-of-the-art performance over vision\nand vision-language models in top-1 accuracy (+6.34%) on PanAf500 and overall\n(+1.1%) and tail-class (+2.26%) mean average precision on PanAf20K. We share\ncomplete source code and network weights for full reproducibility of results\nand easy utilisation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.08937v1",
    "published_date": "2024-04-13 09:17:51 UTC",
    "updated_date": "2024-04-13 09:17:51 UTC"
  },
  {
    "arxiv_id": "2404.08931v1",
    "title": "Label-free Anomaly Detection in Aerial Agricultural Images with Masked Image Modeling",
    "authors": [
      "Sambal Shikhar",
      "Anupam Sobti"
    ],
    "abstract": "Detecting various types of stresses (nutritional, water, nitrogen, etc.) in\nagricultural fields is critical for farmers to ensure maximum productivity.\nHowever, stresses show up in different shapes and sizes across different crop\ntypes and varieties. Hence, this is posed as an anomaly detection task in\nagricultural images. Accurate anomaly detection in agricultural UAV images is\nvital for early identification of field irregularities. Traditional supervised\nlearning faces challenges in adapting to diverse anomalies, necessitating\nextensive annotated data. In this work, we overcome this limitation with\nself-supervised learning using a masked image modeling approach. Masked\nAutoencoders (MAE) extract meaningful normal features from unlabeled image\nsamples which produces high reconstruction error for the abnormal pixels during\nreconstruction. To remove the need of using only ``normal\" data while training,\nwe use an anomaly suppression loss mechanism that effectively minimizes the\nreconstruction of anomalous pixels and allows the model to learn anomalous\nareas without explicitly separating ``normal\" images for training. Evaluation\non the Agriculture-Vision data challenge shows a mIOU score improvement in\ncomparison to prior state of the art in unsupervised and self-supervised\nmethods. A single model generalizes across all the anomaly categories in the\nAgri-Vision Challenge Dataset",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "The paper has been accepted to CVPR 2024 5th Workshop on Vision for\n  Agriculture as an Oral Paper",
    "pdf_url": "http://arxiv.org/pdf/2404.08931v1",
    "published_date": "2024-04-13 08:49:17 UTC",
    "updated_date": "2024-04-13 08:49:17 UTC"
  },
  {
    "arxiv_id": "2404.13066v2",
    "title": "Leveraging Large Language Model as Simulated Patients for Clinical Education",
    "authors": [
      "Yanzeng Li",
      "Cheng Zeng",
      "Jialun Zhong",
      "Ruoyu Zhang",
      "Minhao Zhang",
      "Lei Zou"
    ],
    "abstract": "Simulated Patients (SPs) play a crucial role in clinical medical education by\nproviding realistic scenarios for student practice. However, the high cost of\ntraining and hiring qualified SPs, along with the heavy workload and potential\nrisks they face in consistently portraying actual patients, limit students'\naccess to this type of clinical training. Consequently, the integration of\ncomputer program-based simulated patients has emerged as a valuable educational\ntool in recent years. With the rapid development of Large Language Models\n(LLMs), their exceptional capabilities in conversational artificial\nintelligence and role-playing have been demonstrated, making them a feasible\noption for implementing Virtual Simulated Patient (VSP). In this paper, we\npresent an integrated model-agnostic framework called CureFun that harnesses\nthe potential of LLMs in clinical medical education. This framework facilitates\nnatural conversations between students and simulated patients, evaluates their\ndialogue, and provides suggestions to enhance students' clinical inquiry\nskills. Through comprehensive evaluations, our approach demonstrates more\nauthentic and professional SP-scenario dialogue flows compared to other\nLLM-based chatbots, thus proving its proficiency in simulating patients.\nAdditionally, leveraging CureFun's evaluation ability, we assess several\nmedical LLMs and discuss the possibilities and limitations of using LLMs as\nvirtual doctors from the perspective of their diagnostic abilities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.13066v2",
    "published_date": "2024-04-13 06:36:32 UTC",
    "updated_date": "2024-04-25 02:39:24 UTC"
  },
  {
    "arxiv_id": "2404.13065v1",
    "title": "Intellecta Cognitiva: A Comprehensive Dataset for Advancing Academic Knowledge and Machine Reasoning",
    "authors": [
      "Ajmal PS",
      "Ditto PS",
      "Jithin VG"
    ],
    "abstract": "Intellecta dataset emerges as an innovative synthetic dataset, engineered to\nenhance the cognitive processing capabilities of contemporary language models.\nWith a composition of 11.53 billion tokens, integrating 8.01 billion tokens of\nsynthetic data with 3.52 billion tokens of rich textbook data, Intellecta is\ncrafted to foster advanced reasoning and comprehensive educational narrative\ngeneration. Leveraging the Mixtral-8x7B-Instruct-v0.1 model, the dataset\nfacilitates the generation of complex thought processes and detailed,\ntextbook-style explanations, thus enabling language models to engage in both\ncritical thinking and profound educational discourse. This hybrid dataset\nstands as a testament to the potential of synthetic data in pushing the\nboundaries of AI, offering a repository that is not only vast and varied but\nalso refined to align with ethical standards and intellectual rigor.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.13065v1",
    "published_date": "2024-04-13 06:11:25 UTC",
    "updated_date": "2024-04-13 06:11:25 UTC"
  },
  {
    "arxiv_id": "2404.08892v1",
    "title": "ChangeAnywhere: Sample Generation for Remote Sensing Change Detection via Semantic Latent Diffusion Model",
    "authors": [
      "Kai Tang",
      "Jin Chen"
    ],
    "abstract": "Remote sensing change detection (CD) is a pivotal technique that pinpoints\nchanges on a global scale based on multi-temporal images. With the recent\nexpansion of deep learning, supervised deep learning-based CD models have shown\nsatisfactory performance. However, CD sample labeling is very time-consuming as\nit is densely labeled and requires expert knowledge. To alleviate this problem,\nwe introduce ChangeAnywhere, a novel CD sample generation method using the\nsemantic latent diffusion model and single-temporal images. Specifically,\nChangeAnywhere leverages the relative ease of acquiring large single-temporal\nsemantic datasets to generate large-scale, diverse, and semantically annotated\nbi-temporal CD datasets. ChangeAnywhere captures the two essentials of CD\nsamples, i.e., change implies semantically different, and non-change implies\nreasonable change under the same semantic constraints. We generated\nChangeAnywhere-100K, the largest synthesis CD dataset with 100,000 pairs of CD\nsamples based on the proposed method. The ChangeAnywhere-100K significantly\nimproved both zero-shot and few-shot performance on two CD benchmark datasets\nfor various deep learning-based CD models, as demonstrated by transfer\nexperiments. This paper delineates the enormous potential of ChangeAnywhere for\nCD sample generation and demonstrates the subsequent enhancement of model\nperformance. Therefore, ChangeAnywhere offers a potent tool for remote sensing\nCD. All codes and pre-trained models will be available at\nhttps://github.com/tangkai-RS/ChangeAnywhere.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Concise manuscript version of ChangeAnywhere",
    "pdf_url": "http://arxiv.org/pdf/2404.08892v1",
    "published_date": "2024-04-13 03:46:35 UTC",
    "updated_date": "2024-04-13 03:46:35 UTC"
  },
  {
    "arxiv_id": "2405.01392v1",
    "title": "LLMSat: A Large Language Model-Based Goal-Oriented Agent for Autonomous Space Exploration",
    "authors": [
      "David Maranto"
    ],
    "abstract": "As spacecraft journey further from Earth with more complex missions, systems\nof greater autonomy and onboard intelligence are called for. Reducing reliance\non human-based mission control becomes increasingly critical if we are to\nincrease our rate of solar-system-wide exploration. Recent work has explored\nAI-based goal-oriented systems to increase the level of autonomy in mission\nexecution. These systems make use of symbolic reasoning managers to make\ninferences from the state of a spacecraft and a handcrafted knowledge base,\nenabling autonomous generation of tasks and re-planning. Such systems have\nproven to be successful in controlled cases, but they are difficult to\nimplement as they require human-crafted ontological models to allow the\nspacecraft to understand the world. Reinforcement learning has been applied to\ntrain robotic agents to pursue a goal. A new architecture for autonomy is\ncalled for. This work explores the application of Large Language Models (LLMs)\nas the high-level control system of a spacecraft. Using a systems engineering\napproach, this work presents the design and development of an agentic\nspacecraft controller by leveraging an LLM as a reasoning engine, to evaluate\nthe utility of such an architecture in achieving higher levels of spacecraft\nautonomy. A series of deep space mission scenarios simulated within the popular\ngame engine Kerbal Space Program (KSP) are used as case studies to evaluate the\nimplementation against the requirements. It is shown the reasoning and planning\nabilities of present-day LLMs do not scale well as the complexity of a mission\nincreases, but this can be alleviated with adequate prompting frameworks and\nstrategic selection of the agent's level of authority over the host spacecraft.\nThis research evaluates the potential of LLMs in augmenting autonomous\ndecision-making systems for future robotic space applications.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "physics.space-ph"
    ],
    "primary_category": "cs.RO",
    "comment": "B.A.Sc thesis",
    "pdf_url": "http://arxiv.org/pdf/2405.01392v1",
    "published_date": "2024-04-13 03:33:17 UTC",
    "updated_date": "2024-04-13 03:33:17 UTC"
  },
  {
    "arxiv_id": "2404.08886v1",
    "title": "EIVEN: Efficient Implicit Attribute Value Extraction using Multimodal LLM",
    "authors": [
      "Henry Peng Zou",
      "Gavin Heqing Yu",
      "Ziwei Fan",
      "Dan Bu",
      "Han Liu",
      "Peng Dai",
      "Dongmei Jia",
      "Cornelia Caragea"
    ],
    "abstract": "In e-commerce, accurately extracting product attribute values from multimodal\ndata is crucial for improving user experience and operational efficiency of\nretailers. However, previous approaches to multimodal attribute value\nextraction often struggle with implicit attribute values embedded in images or\ntext, rely heavily on extensive labeled data, and can easily confuse similar\nattribute values. To address these issues, we introduce EIVEN, a data- and\nparameter-efficient generative framework that pioneers the use of multimodal\nLLM for implicit attribute value extraction. EIVEN leverages the rich inherent\nknowledge of a pre-trained LLM and vision encoder to reduce reliance on labeled\ndata. We also introduce a novel Learning-by-Comparison technique to reduce\nmodel confusion by enforcing attribute value comparison and difference\nidentification. Additionally, we construct initial open-source datasets for\nmultimodal implicit attribute value extraction. Our extensive experiments\nreveal that EIVEN significantly outperforms existing methods in extracting\nimplicit attribute values while requiring less labeled data.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by NAACL 2024 Industry Track",
    "pdf_url": "http://arxiv.org/pdf/2404.08886v1",
    "published_date": "2024-04-13 03:15:56 UTC",
    "updated_date": "2024-04-13 03:15:56 UTC"
  },
  {
    "arxiv_id": "2404.08872v1",
    "title": "Enhanced Hydrogen Evolution Activity of MOS$_2$-rGO Composite Synthesized via Hydrothermal Technique",
    "authors": [
      "Abhishek Sebastian",
      "Pragna R"
    ],
    "abstract": "Hydrogen evolution reaction (HER) has emerged as a promising technique for\nthe production of clean and sustainable energy. In recent years, researchers\nhave been exploring various materials for efficient HER activity. In this\nstudy, we report the synthesis of two different materials, namely MOS$_2$ and\nMoS$_2$-rGO, through a hydrothermal technique. X-ray diffraction (XRD),\nFourier-transform infrared (FTIR) spectroscopy, and Raman spectroscopy were\nused to characterize the materials. XRD analysis revealed the formation of\nhexagonal MOS$_2$ with a high degree of crystallinity. FTIR analysis confirmed\nthe presence of Mo-S bonds, while Raman spectroscopy provided evidence for the\nformation of MOS$_2$.To evaluate the HER activity of the materials, linear\nsweep voltammetry (LSV) was performed. The results showed that MOS$_2$ and\nMOS$_2$-rGO had good HER activity with low onset potentials and high current\ndensities. The MOS$_2$-rGO material showed improved HER activity compared to\nMOS$_2$, indicating the potential of graphene oxide as a co-catalyst to enhance\nthe performance of MOS$_2$.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "This research is an excerpt from the report of TARP (Technical\n  Answers for Real-World Problems)",
    "pdf_url": "http://arxiv.org/pdf/2404.08872v1",
    "published_date": "2024-04-13 02:06:32 UTC",
    "updated_date": "2024-04-13 02:06:32 UTC"
  },
  {
    "arxiv_id": "2404.10790v1",
    "title": "Multimodal Attack Detection for Action Recognition Models",
    "authors": [
      "Furkan Mumcu",
      "Yasin Yilmaz"
    ],
    "abstract": "Adversarial machine learning attacks on video action recognition models is a\ngrowing research area and many effective attacks were introduced in recent\nyears. These attacks show that action recognition models can be breached in\nmany ways. Hence using these models in practice raises significant security\nconcerns. However, there are very few works which focus on defending against or\ndetecting attacks. In this work, we propose a novel universal detection method\nwhich is compatible with any action recognition model. In our extensive\nexperiments, we show that our method consistently detects various attacks\nagainst different target models with high true positive rates while satisfying\nvery low false positive rates. Tested against four state-of-the-art attacks\ntargeting four action recognition models, the proposed detector achieves an\naverage AUC of 0.911 over 16 test cases while the best performance achieved by\nthe existing detectors is 0.645 average AUC. This 41.2% improvement is enabled\nby the robustness of the proposed detector to varying attack methods and target\nmodels. The lowest AUC achieved by our detector across the 16 test cases is\n0.837 while the competing detector's performance drops as low as 0.211. We also\nshow that the proposed detector is robust to varying attack strengths. In\naddition, we analyze our method's real-time performance with different hardware\nsetups to demonstrate its potential as a practical defense mechanism.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.10790v1",
    "published_date": "2024-04-13 01:31:25 UTC",
    "updated_date": "2024-04-13 01:31:25 UTC"
  },
  {
    "arxiv_id": "2404.08866v1",
    "title": "An evaluation framework for synthetic data generation models",
    "authors": [
      "Ioannis E. Livieris",
      "Nikos Alimpertis",
      "George Domalis",
      "Dimitris Tsakalidis"
    ],
    "abstract": "Nowadays, the use of synthetic data has gained popularity as a cost-efficient\nstrategy for enhancing data augmentation for improving machine learning models\nperformance as well as addressing concerns related to sensitive data privacy.\nTherefore, the necessity of ensuring quality of generated synthetic data, in\nterms of accurate representation of real data, consists of primary importance.\nIn this work, we present a new framework for evaluating synthetic data\ngeneration models' ability for developing high-quality synthetic data. The\nproposed approach is able to provide strong statistical and theoretical\ninformation about the evaluation framework and the compared models' ranking.\nTwo use case scenarios demonstrate the applicability of the proposed framework\nfor evaluating the ability of synthetic data generation models to generated\nhigh quality data. The implementation code can be found in\nhttps://github.com/novelcore/synthetic_data_evaluation_framework.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper has been accepted for presentation at IFIP International\n  Conference on Artificial Intelligence Applications and Innovations",
    "pdf_url": "http://arxiv.org/pdf/2404.08866v1",
    "published_date": "2024-04-13 01:16:45 UTC",
    "updated_date": "2024-04-13 01:16:45 UTC"
  },
  {
    "arxiv_id": "2404.08858v1",
    "title": "A Lightweight Spatiotemporal Network for Online Eye Tracking with Event Camera",
    "authors": [
      "Yan Ru Pei",
      "Sasskia Brüers",
      "Sébastien Crouzet",
      "Douglas McLelland",
      "Olivier Coenen"
    ],
    "abstract": "Event-based data are commonly encountered in edge computing environments\nwhere efficiency and low latency are critical. To interface with such data and\nleverage their rich temporal features, we propose a causal spatiotemporal\nconvolutional network. This solution targets efficient implementation on\nedge-appropriate hardware with limited resources in three ways: 1) deliberately\ntargets a simple architecture and set of operations (convolutions, ReLU\nactivations) 2) can be configured to perform online inference efficiently via\nbuffering of layer outputs 3) can achieve more than 90% activation sparsity\nthrough regularization during training, enabling very significant efficiency\ngains on event-based processors. In addition, we propose a general affine\naugmentation strategy acting directly on the events, which alleviates the\nproblem of dataset scarcity for event-based systems. We apply our model on the\nAIS 2024 event-based eye tracking challenge, reaching a score of 0.9916 p10\naccuracy on the Kaggle private testset.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.08858v1",
    "published_date": "2024-04-13 00:13:20 UTC",
    "updated_date": "2024-04-13 00:13:20 UTC"
  },
  {
    "arxiv_id": "2404.08857v2",
    "title": "Voice Attribute Editing with Text Prompt",
    "authors": [
      "Zhengyan Sheng",
      "Yang Ai",
      "Li-Juan Liu",
      "Jia Pan",
      "Zhen-Hua Ling"
    ],
    "abstract": "Despite recent advancements in speech generation with text prompt providing\ncontrol over speech style, voice attributes in synthesized speech remain\nelusive and challenging to control. This paper introduces a novel task: voice\nattribute editing with text prompt, with the goal of making relative\nmodifications to voice attributes according to the actions described in the\ntext prompt. To solve this task, VoxEditor, an end-to-end generative model, is\nproposed. In VoxEditor, addressing the insufficiency of text prompt, a Residual\nMemory (ResMem) block is designed, that efficiently maps voice attributes and\nthese descriptors into the shared feature space. Additionally, the ResMem block\nis enhanced with a voice attribute degree prediction (VADP) block to align\nvoice attributes with corresponding descriptors, addressing the imprecision of\ntext prompt caused by non-quantitative descriptions of voice attributes. We\nalso establish the open-source VCTK-RVA dataset, which leads the way in manual\nannotations detailing voice characteristic differences among different\nspeakers. Extensive experiments demonstrate the effectiveness and\ngeneralizability of our proposed method in terms of both objective and\nsubjective metrics. The dataset and audio samples are available on the website.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.08857v2",
    "published_date": "2024-04-13 00:07:40 UTC",
    "updated_date": "2024-12-01 03:49:30 UTC"
  },
  {
    "arxiv_id": "2404.08856v1",
    "title": "On Speculative Decoding for Multimodal Large Language Models",
    "authors": [
      "Mukul Gagrani",
      "Raghavv Goel",
      "Wonseok Jeon",
      "Junyoung Park",
      "Mingu Lee",
      "Christopher Lott"
    ],
    "abstract": "Inference with Multimodal Large Language Models (MLLMs) is slow due to their\nlarge-language-model backbone which suffers from memory bandwidth bottleneck\nand generates tokens auto-regressively. In this paper, we explore the\napplication of speculative decoding to enhance the inference efficiency of\nMLLMs, specifically the LLaVA 7B model. We show that a language-only model can\nserve as a good draft model for speculative decoding with LLaVA 7B, bypassing\nthe need for image tokens and their associated processing components from the\ndraft model. Our experiments across three different tasks show that speculative\ndecoding can achieve a memory-bound speedup of up to 2.37$\\times$ using a 115M\nparameter language model that we trained from scratch. Additionally, we\nintroduce a compact LLaVA draft model incorporating an image adapter, which\nshows marginal performance gains in image captioning while maintaining\ncomparable results in other tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted as a spotlight paper to ELVM workshop at CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.08856v1",
    "published_date": "2024-04-13 00:02:36 UTC",
    "updated_date": "2024-04-13 00:02:36 UTC"
  }
]